Sentence Unit Detection without an Audio Signal

William Morgan∗

1 Introduction and motivation

2 CRFs

Sentence unit (SU) detection is the task of divid-
ing a sequence of words into individual sentences.
SU detection is a close relative of sentence bound-
ary detection, which has been a topic of study in
the computational linguistics community for over a
decade. (Palmer and Hearst, 1994; Reynar and Rat-
naparkhi, 1997)
SU detection is speciﬁc to the context of auto-
matic speech recognition (ASR) systems, which typ-
ically produce an unstructured sequence of words
from an audio signal, and must then recover latent
structural features in the signal such as word case
(“true-casing”) and sentence boundaries (“SU detec-
tion”) in order for the output to be ready for human
consumption. Recent efforts by the DARPA EARS
program (Ofﬁce, 2003) to improve ASR quality has
renewed interest in this problem.
Work on SU detection in modern ASR systems
typically takes in to account the full set of fea-
tures available from the audio signal. Features like
prosidy, voice quality, and even (in the case of multi-
modal systems) visual cues like gestures have all
been shown to be informative in deciding on sen-
tence boundaries. (Liu et al., 2005; Stolcke et al.,
2004)
In this study, we apply conditional random ﬁelds
(CRFs) to examine the feasibility of detecting sen-
tence boundaries directly from text, i.e. without the
corresponding audio signal. These results act as a
baseline, suggesting the true usefulness of features
extracted from the audio and video for SU detection.

Much work on sentence unit detection has used
Hidden Markov Models (HMMs) as the underly-
ing model. (Shriberg et al., 2000; Renals and Go-
toh, 2000; Christensen et al., 2001; Kim and Wood-
land, 2001) Recent experiments with CRFs, how-
ever, have shown they can exhibit better perfor-
mance on the SU detection task than HMMs or max-
imum entropy approaches. (Liu et al., 2005)
A CRF is an undirected graphical model of repre-
senting an event sequence E globally conditioned on
an observation sequence O . CRFs are naturally ap-
plicable to many problems to which HMMs have tra-
ditionally been applied applied, but unlike HMMs,
which maximize the joint distribution P (E , O),
CRFs directly maximize the posterior event proba-
bilities P (E |O).
The most likely sequence of events in a CRF is
given by

E ∗ = arg max
E

exp (Pk λkGk (E , O))
Zλ (O)

where Gk (E , O) are potential functions over the
events and observations, and Zλ a normalization
term. In general the Gk can be any functions, but
in many cases (including ours) it is computationally
beneﬁcial to restrict oneself to a ﬁrst-order CRFs, as
exempliﬁed by Figure 1.
In this paper we use the Stanford NLP CRF im-
plementation.

3 Data

∗No collaborators or advisors.

Our data was drawn from the NIST RT-03 evalua-
tion, LDC publication LDC2004T12. (NIST, 2003)

E

i−1

E

i

E

i+1

5 Metrics

O

i−1

O

i

O

i+1

Figure 1: A ﬁrst-order CRF. E represents the events
(sentence boundary or not) and O represent the ob-
servations (the words of ASR output).

This corpus consists of 474 human-edited transcrip-
tions, 172 drawn from broadcast news and 302
drawn from conversational speech, altogether com-
prising of 661k words. Of this, approximately 20%
was set aside as test data. The remainder was used
as training data.1

4 Feature extraction and modeling

The CRF “event” for SU detection was encoded as
a boolean value for each word in the training data,
specifying whether that word was at the beginning
of a sentence or not.
The transcript data contained, for each word of
speech,
the lexeme (written representation), start
time, duration, and speaker identiﬁcation. From
these attributes we extracted the following features:
the lexeme (lower-cased,
to prevent confounding
from case information), the duration, the delay since
the prior word (if any), whether a speaker change
had occurred.
information were
Two additional sources of
added. Part-of-speech tags for each word were de-
termined by the Stanford NLP part-of-speech tagger,
a bi-directional CMM tagger.
Additionally, a bigram language model was built
from a portion of the the LDC English Gigaword
corpus (LDC publication LDC2003T05). The lan-
guage model was based on 211 million words of
English text, and simply estimated, for each word
wi ,
the probability P (wi
is the ﬁrst word in a
sentence|wi+1 ).

1Broadcast news and conversational speech have different
characteristics; we mainly ignore this, but do try to control for
it by drawning samples from the training corpus proportionately
from both types.

In evaluating a system for SU detection, it is likely
that one is concerned both with type I and type II
errors. We measured system performance using pre-
cision and recall, which capture both these types of
errors. In our case, they are deﬁned as

P =

# correctly marked as SU
# marked as SU

and

.

R =

# correctly marked as SU
# SUs in the corpus
One pleasing feature of these two metrics is that
while either may be gamed individually, it is impos-
sible to game both simultaneously. In our case, pre-
cision may be gamed by tagging only the ﬁrst word
of the test set as a SU, and not tagging anything else,
and recall may be gamed by tagging every word as
an SU, but having high scores in both requires a tag-
ger of both high accuracy and large coverage.
It is often convenient to combine these two scores
into a single number. Typically this is done by the
harmonic mean, or f-measure:

F =

2P R
P + R
Intuitively, as P approaches R, F approaches
, and as they diverge, F approaches 0.
P +R
2

.

6 Results

Table 1 gives the result of the experiments. Some-
what surprisingly, neither the part-of-speech tags
(“+pos”) nor the language model (“+lm ”) signiﬁ-
cantly affected the performance of the system as
compared to the baseline. For comparison, three
systems which randomly assign SU or not-SU tags
to each word are given.
It is clear that while the
trained systems far exceed these in precision, the dif-
ference in recall scores is much lower.
Figure 2 shows the f-measure of the baseline sys-
tem on both training and test sets as a function of
training set size. Near-maximal scores are obtained
with fairly small training set sizes ( 50 documents),
suggesting that more work on feature extraction is
needed to improve scores.
Figures 3 and 4 show the precision, recall and f-
measure of the baseline system on the test and train-
ing sets respectively. Again, we see that by the time

0
0
8
8

0
0
6
6

0
0
4
4

0
0
2
2

e
r
u
s
a
e
m
−
f

0
0
0
9
9
9

0
0
0
8
8
8

0
0
0
7
7
7

0
0
0
6
6
6

c
c
c
i
i
i
r
r
r
t
t
t
e
e
e
m
m
m

training set
test set
rand (50%)
rand (10%)

precision
f−measure
recall

0
0

100
100

200
200

300
300

400
400

0
0
0

100
100
100

200
200
200

300
300
300

400
400
400

training set size (docs)

training set size (docs)

Figure 2: Training vs test set performance (f-
measure) of the baseline system.

Figure 4: Precision, recall and f-measure of the
baseline system on the training set.

c
c
c
i
i
i
r
r
r
t
t
t
e
e
e
m
m
m

0
0
0
8
8
8

0
0
0
7
7
7

0
0
0
6
6
6

0
0
0
5
5
5

0
0
0
4
4
4

precision
f−measure
recall

0
0
0

100
100
100

200
200
200

300
300
300

400
400
400

training set size (docs)

Figure 3: Precision, recall and f-measure of the
baseline system on the test set.

the training corpus contains 50 documents, we have
achieved the vast majority of our ﬁnal performance.
These graphs also highlight the fact that the system’s
errors are primarily those of omission, not of incor-
rect labelling.
For comparison purposes, Liu et al. (2005) re-
port a “boundary classiﬁcation” error rate of 5.43%
when training a CRF without using only textual fea-
tures; i.e., the system correctly labelled 94.57% of
the words as either SU or non-SU. Our error rate by
the same metric is 5.67%, which is roughly compa-
rable (they trained on the same corpus). They used
several other textual features (such as automatically-
derived word classes) that may account for the dif-
ference. When they added prosodic features, their
error rate went down to 3.47%.

7 Conclusion and Future Work

Baseline system performance was a respectable f-
measure of 71; speciﬁcally, approximately 80% of
the labels the system made were correct, and it de-

System
baseline
+pos
+lm
just 1 doc
random 50%
random 10%
random 1%

Precision Recall F-measure
71.05
64.49
79.10
78.96
65.45
71.57
71.67
65.76
78.74
60.39
49.82
76.65
11.89
49.40
19.16
10.74
9.80
11.87
11.40
0.95
1.75

Table 1: Results of training the CRF to perform SU
detection. The baseline system has neither part-of-
speech tags nor language model probabilities. “Just
1 doc” is identical to the baseline system but is only
trained on one document. The “random ” systems
simply assign a SU tag to each word by ﬂipping a
coin weighted with the respective probability.

tected about 65% of the possible SUs. However,
the fact that test set performance reached most of
its maximal value with only 50 documents suggests
a need for better modeling and feature extraction. It
is likely that features from the audio stream itself
would improve these scores signiﬁcantly.
It is somewhat surprising that neither the language
model nor the part-of-speech tagger had any effect
on the system performance. More work is needed to
understand this. One approach would be to exam-
ine individual errors and compare the features and
feature weights involved to see if the source of the
errors can be pinpointed.
One interesting question still unaswered is the
performance of the system on actual ASR output.
The data used in this study was exclusively human-
annotated and thus did not have the characteristic er-

rors of ASR output, which will likely have a negative
effect on system performance.

References
Heidi Christensen, Steve Renals, and Yoshihiko Gotoh.
2001. Punctuation annotation using statistical prosody
models.
In ISCA Workshop on Prosody in Speech
Recognition and Understanding, September 10.

Ji-hwan Kim and P. C. Woodland. 2001. The use of
prosody in A combined system for punctuation gener-
ation and speech recognition. In Proceedings of Eu-
rospeech 2001, pages 2757 –2760, November 22.

Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random ﬁelds for
sentence boundary detection in speech.
In Proceed-
ings of the 43rd Annula Meeting of the ACL, pages
451 –458, June 12.

RT-03F
work-
2003.
NIST.
presentations.
and
agenda
shop
http://www.nist.gov/speech/tests/rt/rt2003/fall/presentations/,
November.

DARPA Information Processing Ofﬁce.
Ef-
2003.
fective, affordable, reusable speech-to-text (EARS).
http://www.darpa.mil/ipto/programs/ears/.

D. D. Palmer and M. A. Hearst. 1994. Adaptive sen-
tence boundary disambiguation. In Proceedings of the
Fourth Applied Conference on NLP, pages 78 –83.

Steve Renals and Yoshihiko Gotoh. 2000. Sentence
boundary detection in broadcast speech transcripts.
In Proceedings of ISCA Workshop: ASR: Challenges
for the New Millenium ASR-2000, pages 228 –235,
July 19.

J. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the Fifth Applied Conference on NLP,
pages 16 –19.

Elizabeth Shriberg, Andreas Stolcke, Dilek Z. Hakkani-
T ¨ur, and G ¨okan T ¨ur.
2000. Prosody-based auto-
matic segmentation of speech into sentences and top-
ics. Speech Communications 32, pages 127 –154.

Andreas Stolcke, Elizabeth Shriberg, Mary Harper, and
Yang Liu. 2004. Comparing and combining genera-
tive and posterior probability models: Some advances
in sentence boundary detection in speech. In Proceed-
ings of the Conference on Empirical Methods in NLP,
June 12.


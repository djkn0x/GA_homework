Analysis of Spectral Kernel Design based
Semi-supervised Learning

Tong Zhang
Yahoo! Inc.
New York City, NY 10011

Rie Kubota Ando
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598

Abstract

We consider a framework for semi-supervised learning using spectral
decomposition based un-supervised kernel design. This approach sub-
sumes a class of previously proposed semi-supervised learning methods
on data graphs. We examine various theoretical properties of such meth-
ods. In particular, we derive a generalization performance bound, and
obtain the optimal kernel design by minimizing the bound. Based on
the theoretical analysis, we are able to demonstrate why spectral kernel
design based methods can often improve the predictive performance. Ex-
periments are used to illustrate the main consequences of our analysis.

1 Introduction

Spectral graph methods have been used both in clustering and in semi-supervised learning.
This paper focuses on semi-supervised learning, where a classiﬁer is constructed from both
labeled and unlabeled training examples. Although previous studies showed that this class
of methods work well for certain concrete problems (for example, see [1, 4, 5, 6]), there
is no satisfactory theory demonstrating why (and under what circumstances) such methods
should work.

The purpose of this paper is to develop a more complete theoretical understanding for graph
based semi-supervised learning. In Theorem 2.1, we present a transductive formulation of
kernel learning on graphs which is equivalent to supervised kernel learning. This new
kernel learning formulation includes some of the previous proposed graph semi-supervised
learning methods as special cases. A consequence is that we can view such graph-based
semi-supervised learning methods as kernel design methods that utilize unlabeled data; the
designed kernel is then used in the standard supervised learning setting. This insight allows
us to prove useful results concerning the behavior of graph based semi-supervised learning
from the more general view of spectral kernel design. Similar spectral kernel design ideas
also appeared in [2]. However, they didn’t present a graph-based learning formulation
(Theorem 2.1 in this paper); nor did they study the theoretical properties of such methods.
We focus on two issues for graph kernel learning formulations based on Theorem 2.1. First,
we establish the convergence of graph based semi-supervised learning (when the number
of unlabeled data increases). Second, we obtain a learning bound, which can be used to
compare the performance of different kernels. This analysis gives insights to what are good
kernels, and why graph-based spectral kernel design is often helpful in various applications.
Examples are given to justify the theoretical analysis. Due to the space limitations, proofs

will not be included in this paper.

2 Transductive Kernel Learning on Graphs

We shall start with notations for supervised learning. Consider the problem of predicting
a real-valued output Y based on its corresponding input vector X . In the standard ma-
chine learning formulation, we assume that the data (X, Y ) are drawn from an unknown
underlying distribution D . Our goal is to ﬁnd a predictor p(x) so that the expected true
loss of p given below is as small as possible: R(p(·)) = E(X,Y )∼D L(p(X ), Y ), where we
use E(X,Y )∼D to denote the expectation with respect to the true (but unknown) underlying
distribution D . Typically, one needs to restrict the hypothesis function family size so that a
stable estimate within the function family can be obtained from a ﬁnite number of samples.
We are interested in learning in Hilbert spaces. For notational simplicity, we assume that
there is a feature representation ψ(x) ∈ H, where H is a high (possibly in ﬁnity) dimen-
sional feature space. We denote ψ(x) by column vectors, so that the inner product in the
Hilbert-space H is the vector product. A linear classiﬁer p(x) on H can be represented by
a vector w ∈ H such that p(x) = wT ψ(x).
Let the training samples be (X1 , Y1 ), . . . , (Xn , Yn ). We consider the following regularized
linear prediction method on H:
n
w∈H " 1
L(wT ψ(Xi ), Yi ) + λwT w# .
Xi=1
ˆp(x) = ˆwT ψ(x),
ˆw = arg min
n
If H is an in ﬁnite dimensional space, then it is not be feasible to solve (1) directly. A
remedy is to use kernel methods. Given a feature representation ψ(x), we can de ﬁne kernel
It is well-known (the so-called representer theorem) that the
k(x, x0 ) = ψ(x)T ψ(x0 ).
solution of (1) can be represented as ˆp(x) = Pn
i=1 ˆαi k(Xi , x), where [ ˆαi ] is given by
[αi ]∈Rn 
αiαj k(Xi , Xj )
αj k(Xi , Xj ), Yi
L 
n
n
n
1
Xi,j=1
Xi=1
Xj=1
[ ˆαi ] = arg min
 .
 + λ


n
The above formulations of kernel methods are standard. In the following, we present an
equivalence of supervised kernel learning to a speciﬁc semi -supervised formulation. Al-
though this representation is implicit in some earlier papers, the explicit form of this method
is not well-known. As we shall see later, this new kernel learning formulation is critical for
analyzing a class of graph-based semi-supervised learning methods.
In this framework, the data graph consists of nodes that are the data points Xj . The edge
connecting two nodes Xi and Xj is weighted by k(Xi , Xj ). The following theorem, which
establishes the graph kernel learning formulation we will study in this paper, essentially
implies that graph-based semi-supervised learning is equivalent to the supervised learning
method which employs the same kernel.

(1)

(2)

Theorem 2.1 (Graph Kernel Learning) Consider labeled data {(Xi , Yi )}i=1,...,n and
unlabeled data Xj (j = n + 1, . . . , m). Consider real-valued vectors f = [f1 , . . . , fm ]T ∈
Rm , and the following semi-supervised learning method:
n
f ∈Rm " 1
L(fi , Yi ) + λf T K −1f # ,
ˆf = arg inf
Xi=1
n
where K (often called gram-matrix in kernel learning or afﬁnity mat rix in graph learning)
is an m × m matrix with Ki,j = k(Xi , Xj ) = ψ(Xi )T ψ(Xj ). Let ˆp be the solution of (1),
then ˆfj = ˆp(Xj ) for j = 1, . . . , m.

(3)

The kernel gram matrix K is always positive semi-de ﬁnite. However, if K is not full rank
(singular), then the correct interpretation of f T K −1f is limµ→0+ f T (K + µIm×m )−1 f ,
where Im×m is the m × m identity matrix.
If we start with a given kernel k and let
K = [k(Xi , Xj )], then a semi-supervised learning method of the form (3) is equivalent
to the supervised method (1). It follows that with a formulation like (3), the only way to
utilize unlabeled data is to replace K by a kernel ¯K in (3), or k by ¯k in (2), where ¯K (or
¯k) depends on the unlabeled data. In other words, the only bene ﬁt of unlabeled data in this
setting is to construct a good kernel based on unlabeled data.

Some of previous graph-based semi-supervised learning methods employ the same formu-
lation (3) with K −1 replaced by the graph Laplacian operator L (which we will describe
in Section 5). However, the equivalence of this formulation and supervised kernel learning
(with kernel matrix K = L−1 ) was not obtained in these earlier studies. This equivalence is
important for good theoretical understanding, as we will see later in this paper. Moreover,
by treating graph-based supervised learning as unsupervised kernel design (see Figure 1),
the scope of this paper is more general than graph Laplacian based methods.

Input: labeled data [(Xi , Yi )]i=1,...,n , unlabeled data Xj (j = n + 1, . . . , m)
shrinkage factors sj ≥ 0 (j = 1, . . . , m), kernel function k(·, ·),
Output: predictive values ˆf 0
j on Xj (j = 1, . . . , m)
Form the kernel matrix K = [k(Xi , Xj )] (i, j = 1, . . . , m)
Compute the kernel eigen-decomposition:
K = m Pm
j , where (µj , vj ) are eigenpairs of K (vT
j vj = 1)
j=1 µj vj vT
Modify the kernel matrix as: ¯K = m Pm
j=1 sj µj vj vT
(∗)
j
n Pn
Compute ˆf 0 = arg minf ∈Rm (cid:2) 1
i=1 L(fi , Yi ) + λf T ¯K −1f (cid:3).
Figure 1: Spectral kernel design based semi-supervised learning on graph

In Figure 1, we consider a general formulation of semi-supervised learning method on data
graph through spectral kernel design. This is the method we will analyze in the paper. As
a special case, we can let sj = g (µj ) in Figure 1, where g is a rational function, then
¯K = g (K/m)K . In this special case, we do not have to compute eigen-decomposition of
K . Therefore we obtain a simpler algorithm with the (∗) in Figure 1 replaced by
¯K = g (K/m)K.

(4)

As mentioned earlier, the idea of using spectral kernel design has appeared in [2] although
they didn’t base their method on the graph formulation (3). However, we believe our anal-
ysis also sheds lights to their methods. The semi-supervised learning method described in
Figure 1 is useful only when ˆf 0 is a better predictor than ˆf in Theorem 2.1 (which uses the
original kernel K ) – in other words, only when the new kernel ¯K is better than K .
In the next few sections, we will investigate the following issues concerning the theoretical
behavior of this algorithm: (a) the limiting behavior of ˆf 0 as m → ∞; that is, whether ˆf 0
j
converges for each j ; (b) the generalization performance of (3); (c) optimal Kernel design
by minimizing the generalization error, and its implications; (d) statistical models under
which spectral kernel design based semi-supervised learning is effective.

3 The Limiting Behavior of Graph-based Semi-supervised Learning

We want to show that as m → ∞, the semi-supervised algorithm in Figure 1 is well-
behaved. That is, ˆf 0
j converges as m → ∞. This is one of the most fundamental issues.

Using feature space representation, we have k(x, x0 ) = ψ(x)T ψ(x0 ). Therefore a change
of kernel can be regarded as a change of feature mapping. In particular, we consider a
feature transformation of the form ¯ψ(x) = S 1/2ψ(x), where S is an appropriate positive
semi-de ﬁnite operator on H. The following result establishes an equivalent feature space
formulation of the semi-supervised learning method in Figure 1.

w∈H " 1
ˆw0 = arg min
n

n
Xi=1

Theorem 3.1 Using notations in Figure 1. Assume k(x, x0 ) = ψ(x)T ψ(x0 ). Consider
j , where uj = Ψvj /√µj , Ψ = [ψ(X1 ), . . . , ψ(Xm )], then (µj , uj ) is
S = Pm
j=1 sj uj uT
an eigenpair of ΨΨT /m. Let
ˆp0 (x) = ˆw0T S 1/2ψ(x),

L(wT S 1/2ψ(Xi ), Yi ) + λwT w# .

Then ˆf 0
j = ˆp0 (Xj ) (j = 1, . . . , m).
The asymptotic behavior of Figure 1 when m → ∞ can be easily understood from
m Pm
In this case, we just replace ΨΨT /m = 1
j=1 ψ(Xj )ψ(Xj )T by
Theorem 3.1.
EX ψ(X )ψ(X )T . The spectral decomposition of EX ψ(X )ψ(X )T corresponds to the fea-
ture space PCA. It is clear that if S converges, then the feature space algorithm in Theo-
rem 3.1 also converges. In general, S converges if the eigenvectors uj converges and the
shrinkage factors sj are bounded. As a special case, we have the following result.

Theorem 3.2 Consider a sequence of data X1 , X2 , . . . drawn from a distribution, with
only the ﬁrst n points labeled. Assume when m → ∞, Pm
j=1 ψ(Xj )ψ(Xj )T /m con-
verges to EX ψ(X )ψ(X )T almost surely, and g is a continuous function in the spec-
tral range of EX ψ(X )ψ(X )T . Now in Figure 1 with (∗) given by (4) and kernel
k(x, x0 ) = ψ(x)T ψ(x0 ), ˆf 0
j converges almost surely for each ﬁxed j .

4 Generalization analysis on graph

We study the generalization behavior of graph based semi-supervised learning algorithm
(3), and use it to compare different kernels. We will then use this bound to justify the ker-
nel design method given in Section 2. To measure the sample complexity, we consider m
points (Xj , Yj ) for i = 1, . . . , m. We randomly pick n distinct integers i1 , . . . , in from
{1, . . . , m} uniformly (sample without replacement), and regard it as the n labeled train-
ing data. We obtain predictive values ˆfj on the graph using the semi-supervised learning
method (3) with the labeled data, and test it on the remaining m − n data points. We are
interested in the average predictive performance over all random draws.

Theorem 4.1 Consider (Xj , Yj ) for i = 1, . . . , m. Assume that we randomly pick n dis-
tinct integers i1 , . . . , in from {1, . . . , m} uniformly (sample without replacement), and de-
note it by Zn . Let ˆf (Zn ) be the semi-supervised learning method (3) using training data in
Zn : ˆf (Zn ) = arg minf ∈Rm (cid:2) 1
n Pi∈Zn L(fi , Yi ) + λf T K −1f (cid:3). If | ∂
∂ p L(p, y )| ≤ γ , and
L(p, y ) is convex with respect to p, then we have
2λnm 
f ∈Rm 
m
γ 2tr(K )
1
1
L( ˆfj (Zn ), Yj ) ≤ inf
m − n Xj /∈Zn
Xj=1
L(fj , Yj ) + λf T K −1f +
 .
EZn

m
The bound depends on the regularization parameter λ in addition to the kernel K . In order
to compare different kernels, it is reasonable to compare the bound with the optimal λ for

EZn

1
m

L(fj , Yj ) +

each K . That is, in addition to minimizing f , we also minimize over λ on the right hand of
the bound. Note that in practice, it is usually not difﬁcult t o ﬁnd a nearly-optimal λ through
cross validation, implying that it is reasonable to assume that we can choose the optimal λ
in the bound. With the optimal λ, we obtain:
√2n pR(f , K )
f ∈Rm 
m
1
γ
L( ˆfj (Zn ), Yj ) ≤ inf
Xj=1
m − n Xj /∈Zn
 ,

where R(f , K ) = tr(K/m) f T K −1f is the complexity of f with respect to kernel K .
¯K as in Figure 1, then the complexity of a function f with respect to ¯K is given
If we de ﬁne
j=1 sj µj )(Pm
by R(f , ¯K ) = (Pm
j /(sj µj )). If we believe that a good approximate
j=1 α2
target function f can be expressed as f = Pj αj vj with |αj | ≤ βj for some known βj ,
then based on this belief, the optimal choice of the shrinkage factor becomes sj = βj /µj .
That is, the kernel that optimizes the bound is ¯K = Pj βj vj vT
j , where vj are normalized
eigenvectors of K . In this case, we have R(f , ¯K ) ≤ (Pj βj )2 . The eigenvalues of the
optimal kernel is thus independent of K , but depends only on the spectral coefﬁcient’s
range βj of the approximate target function.
Since there is no reason to believe that the eigenvalues µj of the original kernel K are
proportional to the target spectral coefﬁcient range. If we have some guess of the spectral
coefﬁcients of the target, then one may use the knowledge to o btain a better kernel. This
justiﬁes why spectral kernel design based algorithm can be p otentially helpful (when we
have some information on the target spectral coefﬁcients).
In practice, it is usually difﬁ-
cult to have a precise guess of βj . However, for many application problems, we observe in
practice that the eigenvalues of kernel K decays more slowly than that of the target spectral
coefﬁcients. In this case, our analysis implies that we shou ld use an alternative kernel with
faster eigenvalue decay: for example, using K 2 instead of K . This has a dimension reduc-
tion effect. That is, we effectively project the data into the principal components of data.
The intuition is also quite clear: if the dimension of the target function is small (spectral
coefﬁcient decays fast), then we should project data to thos e dimensions by reducing the
remaining noisy dimensions (corresponding to fast kernel eigenvalue decay).

5 Spectral analysis: the effect of input noise

target function often decay
We provide a justiﬁcation on why spectral coefﬁcients of the
faster than the eigenvalues of a natural kernel K . In essence, this is due to the fact that
input vector X is often corrupted with noise. Together with results in the previous section,
we know that in order to achieve optimal performance, we need to use a kernel with faster
eigenvalue decay. We will demonstrate this phenomenon under a statistical model, and use
the feature space notation in Section 3. For simplicity, we assume that ψ(x) = x.
We consider a two-class classiﬁcation problem in R∞ (with the standard 2-norm inner-
product), where the label Y = ±1. We ﬁrst start with a noise free model, where the data can
be partitioned into p clusters. Each cluster ` is composed of a single center point ¯x` (having
zero variance) with label ¯y` = ±1. In this model, assume that the centers are well separated
so that there is a weight vector w∗ such that wT
∗ w∗ < ∞ and wT
∗ ¯x` = ¯y` . Without loss
of generality, we may assume that ¯x` and w∗ belong to a p-dimensional subspace Vp . Let
p be its orthogonal complement. Assume now that the observed input data are corrupted
V ⊥
with noise. We ﬁrst generate a center index `, and then noise δ (which may depend on
`). The observed input data is the corrupted data X = ¯x` + δ , and the observed output is
∗ ¯x` . In this model, let `(Xi ) be the center corresponding to Xi , the observation
Y = wT
can be decomposed as: Xi = ¯x`(Xi ) + δ(Xi ), and Yi = wT
∗ ¯x`(Xi ) . Given noise δ , we

decompose it as δ = δ1 + δ2 where δ1 is the orthogonal projection of δ in Vp , and δ2 is
the orthogonal projection of δ in V ⊥
p . We assume that δ1 is a small noise component; the
component δ2 can be large but has small variance in every direction.

Theorem 5.1 Consider the data generation model in this section, with observation X =
¯x` + δ and Y = wT
∗ ¯x` . Assume that δ is conditionally zero-mean given `: Eδ|` δ = 0.
Let EX X T = Pj µj uj uT
j be the spectral decomposition with decreasing eigenvalues µj
1 ≥ σ2
j uj = 1). Then the following claims are valid: let σ2
(uT
2 ≥ · · · be the eigenval-
ues of Eδ2 δT
2 , then µj ≥ σ2
j ; if kδ1k2 ≤ b/kw∗k2 , then |wT
∗ Xi − Yi | ≤ b; ∀t ≥ 0,
∗ uj )2µ−t
Pj≥1 (wT
∗ (E ¯x` ¯xT
j ≤ wT
` )−tw∗ .
Consider m points X1 , . . . , Xm . Let Ψ = [X1 , . . . , Xm ] and K = ΨT Ψ = m Pj µj vj vT
j
be the kernel spectral decomposition. Let uj = Ψvj /√mµj , fi = wT
∗ Xi , and f =
Pj αj vj . Then it is not difﬁcult to verify that αj = √mµj wT
∗ uj .
If we assume that
m Pm
asymptotically 1
i → EX X T , then we have the following consequences:
i=1 XiX T
∗ Xi is a good approximate target when b is small. In particular, if b < 1,
• fi = wT
then this function always gives the correct class label.
m Pm
j /µ1+t
• For all t > 0, the spectral coefﬁcient αj of f decays as 1
j=1 α2
j ≤
` )−tw∗ .
wT
∗ (E ¯x` ¯xT
• The eigenvalue µj decays slowly when the noise spectral decays slowly: µj ≥ σ2
j .
If the clean data are well behaved in that we can ﬁnd a weight ve ctor such that
`(X ) )−tw∗ is bounded for some t > 1, then when the data are corrupted
wT
∗ (EX ¯x`(X ) ¯xT
with noise, we can ﬁnd a good approximate target that has spec tral decay faster (on aver-
age) than that of the kernel eigenvalues. This analysis implies that if the feature represen-
tation associated with the original kernel is corrupted with noise, then it is often helpful
to use a kernel with faster spectral decay. For example, instead of using K , we may use
¯K = K 2 . However, it may not be easy to estimate the exact decay rate of the target spectral
coefﬁcients. In practice, one may use cross validation to op timize the kernel.

A kernel with fast spectral decay projects the data into the most prominent principal compo-
nents. Therefore we are interested in designing kernels which can achieve a dimension re-
duction effect. Although one may use direct eigenvalue computation, an alternative is to use
a function g (K/m)K for such an effect, as in (4). For example, we may consider a normal-
j where 0 ≤ uj ≤ 1. A standard normalization
ized kernel such that K/m = Pj µj uj uT
method is to use D−1/2KD−1/2 , where D is the diagonal matrix with each entry corre-
j . We are
sponding to the row sums of K . It follows that g (K/m)K = m Pj g (µj )µj uj uT
interested in a function g such that g (µ)µ ≈ 1 when µ ∈ [α, 1] for some α, and g (µ)µ ≈ 0
when µ < α (where α is close to 1). One such function is to let g (µ)µ = (1 − α)/(1 − αµ).
This is the function used in various graph Laplacian formulations with normalized Gaus-
sian kernel as the initial kernel K . For example, see [5]. Our analysis suggests that it is
the dimension reduction effect of this function that is important, rather than the connec-
tion to graph Laplacian. As we shall see in the empirical examples, other kernels such as
K 2 , which achieve similar dimension reduction effect (but has nothing to do with graph
Laplacian), also improve performance.

6 Empirical Examples

This section shows empirical examples to demonstrate some consequences of our theoret-
ical analysis. We use the MNIST data set (http://yann.lecun.com/exdb/mnist/), consisting

of hand-written digit images (representing 10 classes, from digit “0 ” to digit “9 ”). In the
following experiments, we randomly draw m = 2000 samples. We regard n = 100 of
them as labeled data, and the remaining m − n = 1900 as unlabeled test data.

Normalized 25NN, MNIST

Accuracy: 25NN, MNIST

s
t
n
e
i
c
i
f
f
e
o
c

 1000

 100

 10

 1

 0.1

 0.01

 0.001

Y avg
K
2
K
3
K
4
K
Inverse

y
c
a
r
u
c
c
a

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

Y
4
K
3
K
2
K
K
1,..1,0,..
Inverse
original K

 0  20  40  60  80 100 120 140 160 180 200

 0

 50

 100

 150

 200

dimension (d)

dimension (d)

Figure 2: Left: spectral coefﬁcients; right: classiﬁcatio

n accuracy.

Throughout the experiments, we use the least squares loss: L(p, y ) = (p − y )2 for simplic-
ity. We study the performance of various kernel design methods, by changing the spectral
coefﬁcients of the initial gram matrix K , as in Figure 1. Below we write ¯µj for the new
¯K : i.e., ¯K = Pm
spectral coefﬁcient of the new gram matrix
i . We study the
i=1 ¯µi vi vT
following kernel design methods (also see [2]), with a dimension cut off parameter d, so
that ¯µi = 0 when i > d. (a) [1, . . . , 1, 0, . . . , 0]: ¯µi = 1 if i ≤ d, and 0 otherwise. This
was used in spectral clustering [3]. (b) K : ¯µi = µi if i ≤ d; 0 otherwise. This method is
essentially kernel principal component analysis which keeps the d most signiﬁcant princi-
pal components of K . (c) K p : ¯µi = µp
i if i ≤ d; 0 otherwise. We set p = 2, 3, 4. This
accelerates the decay of eigenvalues of K . (d) Inverse: ¯µi = 1/(1 − ρµi ) if i ≤ d; 0
otherwise. ρ is a constant close to 1 (we used 0.999). This is essentially graph-Laplacian
based semi-supervised learning for normalized kernel (e.g. see [5]). Note that the standard
graph-Laplacian formulation sets d = m. (e) Y : ¯µi = |Y T vi | if i ≤ d; 0 otherwise. This is
the oracle kernel that optimizes our generalization bound. The purpose of testing this oracle
method is to validate our analysis by checking whether good kernel in our theory produces
good classiﬁcation performance on real data. Note that in th e experiments, we use averaged
Y over the ten classes. Therefore the resulting kernel will not be the best possible kernel
for each speciﬁc class, and thus its performance may not alwa ys be optimal.

Figure 2 shows the spectral coefﬁcients of the above mention ed kernel design methods
and the corresponding classiﬁcation performance. The init ial kernel is normalized 25-NN,
which is de ﬁned as K = D−1/2W D−1/2 (see previous section), where Wij = 1 if either
the i-th example is one of the 25 nearest neighbors of the j -th example or vice versa; and
0 otherwise. As expected, the results demonstrate that the target spectral coefﬁcients Y
decay faster than that of the original kernel K . Therefore it is useful to use kernel design
methods that accelerate the eigenvalue decay. The accuracy plot on the right is consistent
with our theory. The near oracle kernel ’Y’ performs well especially when the dimension
cut-off is large. With appropriate dimension d, all methods perform better than the super-
vised base-line (original K) which is below 65%. With appropriate dimension cut-off, all
methods perform similarly (over 80%). However, K p with (p = 2, 3, 4) is less sensitive to
the cut-off dimension d than the kernel principal component dimension reduction method
K . Moreover, the hard threshold method in spectral clustering ([1, . . . , 1, 0, . . . , 0]) is not
stable. Similar behavior can also be observed with other initial kernels. Figure 3 shows
the classiﬁcation accuracy with the standard Gaussian kern el as the initial kernel K , both
with and without normalization. We also used different bandwidth t to illustrate that the

behavior of different methods are similar with different t (in a reasonable range). The re-
sult shows that normalization is not critical for achieving high performance, at least for
this data. Again, we observe that the near oracle method performs extremely well. The
spectral clustering kernel is sensitive to the cut-off dimension, while K p with p = 2, 3, 4
are quite stable. The standard kernel principal component dimension reduction (method
K ) performs very well with appropriately chosen dimension cut-off. The experiments are
consistent with our theoretical analysis.

Accuracy: normalized Gaussian, MNIST

Accuracy: Gaussian, MNIST

y
c
a
r
u
c
c
a

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

Y
4
K
3
K
2
K
K
1,..1,0,..
Inverse
original K

y
c
a
r
u
c
c
a

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

1,..,1,0,..
K
2
K
3
K
4
K
Y
original K

 0

 50

 100

 150

 200

 0

 50

 100

 150

 200

dimension (d)

dimension (d)

Figure 3: Classiﬁcation accuracy with Gaussian kernel k(i, j ) = exp(−||xi − xj ||2
2/t).
Left: normalized Gaussian (t = 0.1); right: unnormalized Gaussian (t = 0.3).

7 Conclusion

We investigated a class of graph-based semi-supervised learning methods. By establishing
a graph-based formulation of kernel learning, we showed that this class of semi-supervised
learning methods is equivalent to supervised kernel learning with unsupervised kernel de-
sign (explored in [2]). We then obtained a generalization bound, which implies that the
eigenvalues of the optimal kernel should decay at the same rate of the target spectral co-
efﬁcients. Moreover, we showed that input noise can cause th e target spectral coefﬁcients
to decay faster than the kernel spectral coefﬁcients. The an alysis explains why it is often
helpful to modify the original kernel eigenvalues to achieve a dimension reduction effect.

References

[1] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian mani-
folds. Machine Learning, Special Issue on Clustering:209 –239, 2004.
[2] Olivier Chapelle, Jason Weston, and Bernhard Sch:olkopf. Cluster kernels for semi-
supervised learning. In NIPS, 2003.
[3] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis
and an algorithm. In NIPS, pages 849 –856, 2001.
[4] M. Szummer and T. Jaakkola. Partially labeled classiﬁca tion with Markov random
walks. In NIPS 2001, 2002.
[5] D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and B. Schlkopf. Learning with local and
global consistency. In NIPS 2003, pages 321 –328, 2004.
[6] Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using
Gaussian ﬁelds and harmonic functions. In ICML 2003, 2003.


A Probabilistic Interpretation of SVMs with an
Application to Unbalanced Classi ﬁcation

Yves Grandvalet ∗
Heudiasyc, CNRS/UTC
60205 Compi `egne cedex, France
grandval@utc.fr

Johnny Mari ´ethoz
Samy Bengio
IDIAP Research Institute
1920 Martigny, Switzerland
{marietho,bengio}@idiap.ch

Abstract

In this paper, we show that the hinge loss can be interpreted as the
neg-log-likelihood of a semi-parametric model of posterior probabilities.
From this point of view, SVMs represent the parametric component of a
semi-parametric model ﬁtted by a maximum a posteriori estim ation pro-
cedure. This connection enables to derive a mapping from SVM scores
to estimated posterior probabilities. Unlike previous proposals, the sug-
gested mapping is interval-valued, providing a set of posterior probabil-
ities compatible with each SVM score. This framework offers a new
way to adapt the SVM optimization problem to unbalanced classi ﬁca-
tion, when decisions result in unequal (asymmetric) losses. Experiments
show improvements over state-of-the-art procedures.

1

Introduction

In this paper, we show that support vector machines (SVMs) are the solution of a relaxed
maximum a posteriori (MAP) estimation problem. This relaxed problem results from ﬁtting
a semi-parametric model of posterior probabilities. This model is decomposed into two
components: the parametric component, which is a function of the SVM score, and the
non-parametric component which we call a nuisance function. Given a proper binding of
the nuisance function adapted to the considered problem, this decomposition enables to
concentrate on selected ranges of the probability spectrum. The estimation process can
thus allocate model capacity to the neighborhoods of decision boundaries.

The connection to semi-parametric models provides a probabilistic interpretation of SVM
scores, which may have several applications, such as estimating conﬁdences over the pre-
dictions, or dealing with unbalanced losses. (which occur in domains such as diagnosis,
intruder detection, etc). Several mappings relating SVM scores to probabilities have al-
ready been proposed (Sollich 2000, Platt 2000), but they are subject to arbitrary choices,
which are avoided here by their integration to the nuisance function.

The paper is organized as follows. Section 2 presents the semi-parametric modeling ap-
proach; Section 3 shows how we reformulate SVM in this framework; Section 4 proposes
several outcomes of this formulation, including a new method to handle unbalanced losses,
which is tested empirically in Section 5. Finally, Section 6 brieﬂy concludes the paper.

∗This work was supported in part by the IST Programme of the European Community, under the
PASCAL Network of Excellence IST-2002-506778. This publication only reﬂects the authors’ views.

2 Semi-Parametric Classiﬁcation

We address the binary classi ﬁcation problem of estimating a decision rule from a learning
set Ln = {(xi , yi )}n
i=1 , where the ith example is described by the pattern xi ∈ X and
the associated response yi ∈ {−1, 1}. In the framework of maximum likelihood estima-
tion, classi ﬁcation can be addressed either via generative models, i.e. models of the joint
distribution P (X, Y ), or via discriminative methods modeling the conditional P (Y |X ).

2.1 Complete and Marginal Likelihood, Nuisance Functions

Let p(1|x; θ) denote the model of P (Y = 1|X = x), p(x; ψ) the model of P (X ) and ti
the binary response variable such that ti = 1 when yi = 1 and ti = 0 when yi = −1.
Assuming independent examples, the complete log-likelihood can be decomposed as
L(θ , ψ ; Ln ) = Xi
ti log(p(1|xi ; θ)) + (1 − ti ) log(1 − p(1|xi ; θ)) + log(p(xi ; ψ)) , (1)
where the two ﬁrst terms of the right-hand side represent the marginal or conditional like-
lihood, that is, the likelihood of p(1|x; θ).
For classi ﬁcation purposes, the parameter ψ is not relevant, and may thus be quali ﬁed as a
nuisance parameter (Lindsay 1985). When θ can be estimated independently of ψ , maxi-
mizing the marginal likelihood provides the estimate returned by maximizing the complete
likelihood with respect to θ and ψ . In particular, when no assumption whatsoever is made
on P (X ), maximizing the conditional likelihood amounts to maximize the joint likelihood
(McLachlan 1992). The density of inputs is then considered as a nuisance function.

2.2 Semi-Parametric Models

Again, for classi ﬁcation purposes, estimating P (Y |X ) may be considered as too demand-
ing. Indeed, taking a decision only requires the knowledge of sign(2P (Y = 1|X = x)−1).
We may thus consider looking for the decision rule minimizing the empirical classi ﬁcation
error, but this problem is intractable for non-trivial models of discriminant functions.

ti log(p(1|xi ; θ)) + (1 − ti ) log(1 − p(1|xi ; θ))

Here, we brieﬂy explore how semi-parametric models (Oakes 1 988) may be used to re-
duce the modelization effort as compared to the standard likelihood approach. For this,
we consider a two-component semi-parametric model of P (Y = 1|X = x), deﬁned as
p(1|x; θ) = g(x; θ) + ε(x), where the parametric component g(x; θ) is the function of in-
terest, and where the non-parametric component ε is a constrained nuisance function. Then,
we address the maximum likelihood estimation of the semi-parametric model p(1|x; θ)
− Xi

min
θ ,ε
s. t. p(1|x; θ) = g(x; θ) + ε(x)
0 ≤ p(1|x; θ) ≤ 1

ε− (x) ≤ ε(x) ≤ ε+ (x)
where ε− and ε+ are user-deﬁned functions, which place constraints on the n on-parametric
component ε. According to these constraints, one pursues different objectives, which can
be interpreted as either weakened or focused versions of the original problem of estimating
precisely P (Y |X ) on the whole range [0, 1].
At the one extreme, when ε− = ε+ , one recovers a parametric maximum likelihood prob-
lem, where the estimate of posterior probabilities p(1|x; θ) is simply g(x; θ) shifted by the
baseline function ε. At the other extreme, when ε− (x) ≤ −g(x) and ε+ (x) ≥ 1 − g(x),
p(1|·; θ) perfectly explains (interpolates) any training sample for any θ , and the optimiza-
tion problem in θ is ill-posed. Note that the optimization problem in ε is always ill-posed,
but this is not of concern as we do not wish to estimate the nuisance function.

(2)

  1
1−ε

)
x
|
1
(
p

  ε
  0
  0   ε

)
x
(
+
ε
/
)
x
(

-
ε

+ε
 0
−ε

 0   ε 

1−ε 1 

g(x)

1−ε  1

g(x)

0.5

0

)
x
(
+
ε
/
)
x
(

-
ε

−0.5
0

1

)
x
|
1
(
p

0.5

1

0
0

0.5
g(x)

0.5
g(x)

1

Figure 1: Two examples of ε− (x) (dashed) and ε+ (x) (plain) vs. g(x) and resulting ǫ-tube
of possible values for the estimate of P (Y = 1|X = x) (gray zone) vs. g(x).

Generally, as ε is not estimated, the estimate of posterior probabilities p(1|x; θ) is only
known to lie within the interval [g(x; θ) + ε− (x), g(x; θ) + ε+ (x)]. In what follows, we
only consider functions ε− and ε+ expressed as functions of the argument g(x), for which
the interval can be recovered from g(x) alone. We also require ε− (x) ≤ 0 ≤ ε+ (x), in
order to ensure that g(x; θ) is an admissible value of p(1|x; θ).
Two simple examples are displayed in Figure 1. The two ﬁrst gr aphs represent ε− and ε+
designed to estimate posterior probabilities up to precision ǫ, and the corresponding ǫ-tube
of admissible estimates knowing g(x). The two last graphs represent the same functions
for ε− and ε+ deﬁned to focus on the only relevant piece of information reg arding decision:
estimating where P (Y |X ) is above 1/2. 1

2.3 Estimation of the Parametric Component

ti log(g(xi ; θ) + εi ) + (1 − ti ) log(1 − g(xi ; θ) − εi )

The deﬁnitions of ε− and ε+ affect the estimation of the parametric component. Regarding
θ , when the values of g(x; θ) + ε− (x) and g(x; θ) + ε+ (x) lie within [0, 1], problem (2)
is equivalent to the following relaxed maximum likelihood problem

− Xi
min
θ ,ε
s. t.
ε− (xi ) ≤ εi ≤ ε+ (xi )
i = 1, . . . , n

where ε is an n-dimensional vector of slack variables. The problem is quali ﬁed as relaxed
compared to the the maximum likelihood estimation of posterior probabilities by g(xi ; θ),
because modeling posterior probabilities by g(xi ; θ) + εi is a looser objective.
The monotonicity of the objective function with respect to εi implies that the constraints
ε− (xi ) ≤ εi and εi ≤ ε+ (xi ) are saturated at the solution of (3) for ti = 0 or ti = 1 re-
spectively. Thus, the loss in (3) is the neg-log-likelihood of the lower or the upper bound on
p(1|xi ; θ) respectively. Provided that g , ε− and ε+ are deﬁned such that ε− (x) ≤ ε+ (x),
0 ≤ g(x) + ε− (x) ≤ 1 and 0 ≤ g(x) + ε+ (x) ≤ 1, the optimization problem with respect
to θ reduces to
− Xi
min
θ
Figure 2 displays the losses for positive examples corresponding to the choices of ε− and
ε+ depicted in Figure 1 (the losses are symmetrical around 0.5 for negative examples).
Note that the convexity of the objective function with respect to g depends on the choices
of ε− and ε+ . One can show that, providing ε+ and ε− are respectively concave and convex
functions of g , then the loss (4) is convex in g .
When ε− (x) ≤ 0 ≤ ε+ (x), g(x) is an admissible estimate of P (Y = 1|x). However,
the relaxed loss (4) is optimistic, below the neg-log-likelihood of g . This optimism usually

ti log(g(xi ; θ) + ε+ (xi )) + (1 − ti ) log(1 − g(xi ; θ) − ε− (xi )) .

(3)

(4)

1Of course, this naive attempt to minimize the training classiﬁcation error is do omed to failure.
Reformulating the problem does not affect its convexity: it remains NP-hard.

)
1
,
)
x
(
g
(
L

 0 

1−ε  1 

g(x)

)
1
,
)
x
(
g
(
L

0

0.5
g(x)

1

Figure 2: Losses for positive examples (plain) and neg-log-likelihood of g(x) (dotted) vs.
g(x). Left: for the function ε+ displayed on the left-hand side of Figure 1; right: for the
function ε+ displayed on the right-hand side of Figure 1.

results in a non-consistent estimation of posterior probabilities (i.e g(x) does not converge
towards P (Y = 1|X = x) as the sample size goes to inﬁnity), a common situation in
semi-parametric modeling (Lindsay 1985). This lack of consistency should not be a con-
cern here, since the non-parametric component is purposely introduced to address a looser
estimation problem. We should therefore restrict consistency requirements to the primary
goal of having posterior probabilities in the ǫ-tube [g(x) + ε− (x), g(x) + ε+ (x)].

3 Semi-Parametric Formulation of SVMs

Several authors pointed the closeness of SVM and the MAP approach to Gaussian pro-
cesses (Sollich (2000) and references therein). However, this similarity does not provide a
proper mapping from SVM scores to posterior probabilities. Here, we resolve this difﬁculty
thanks to the additional degrees of freedom provided by semi-parametric modelling.

min
f ,b

[1 − yi (f (xi ) + b)]+ ,

3.1 SVMs and Gaussian Processes
In its primal Lagrangian formulation, the SVM optimization problem reads
1
H + C Xi
kf k2
2
where H is a reproducing kernel Hilbert space with norm k · kH , C is a regularization
parameter and [f ]+ = max(f , 0).
The penalization term in (5) can be interpreted as a Gaussian prior on f , with a covariance
function proportional to the reproducing kernel of H (Sollich 2000). Then, the interpreta-
tion of the hinge loss as a marginal log-likelihood requires to identify an afﬁne function of
the last term of (5) with the two ﬁrst terms of (1). We thus look for two constants c0 and
c1 6= 0, such that, for all values of f (x) + b, there exists a value 0 ≤ p(1|x) ≤ 1 such that
(cid:26)
p(1|x) = exp −(c0 + c1 [1 − (f (x) + b)]+ )
1 − p(1|x) = exp −(c0 + c1 [1 + (f (x) + b)]+ )
The system (6) has a solution over the whole range of possible values of f (x) + b if and
only if c0 = log(2) and c1 = 0. Thus, the SVM optimization problem does not implement
the MAP approach to Gaussian processes.

(5)

.

(6)

To proceed with a probabilistic interpretation of SVMs, Sollich (2000) proposed a nor-
malized probability model. The normalization functional was chosen arbitrarily, and the
consequences of this choice on the probabilistic interpretation was not evaluated. In what
follows, we derive an imprecise mapping, with interval-valued estimates of probabilities,
representing the set of all admissible semi-parametric formulations of SVM scores.

3.2 SVMs and Semi-Parametric Models
With the semi-parametric models of Section 2.2, one has to identify an afﬁne function of
the hinge loss with the two terms of (4). Compared to the previous situation, one has the

1

0.8

0.6

0.4

0.2

)
x
|
1
(
p

2.5

2

1.5

1

0.5

)
1
,
)
x
(
g
(
L

1

)
x
|
1
(
p

0.5

0
−6

−4

−2

2

4

6

0
f(x)+b

0
−6

−4

−2

2

4

6

0
f(x)+b

0
0

0.25

0.75

1

0.5
g(x)

Figure 3: Left:
lower (dashed) and upper (plain) posterior probabilities [g(x) +
ε− (x), g(x) + ε+ (x)] vs. SVM scores f (x) + b; center: corresponding neg-log-likelihood
of g(x) for positive examples vs. f (x)+ b. right: lower (dashed) and upper (plain) posterior
probabilities vs. g(x), for g deﬁned in (8).

.

(7)

freedom to deﬁne the slack functions ε− and ε+ . The identi ﬁcation problem is now
g(x) + ε+ (x) = exp −(c0 + c1 [1 − (f (x) + b)]+ )

1 − g(x) − ε− (x) = exp −(c0 + c1 [1 + (f (x) + b)]+ )
s.t. 0 ≤ g(x) + ε− (x) ≤ 1
0 ≤ g(x) + ε+ (x) ≤ 1

ε− (x) ≤ ε+ (x)
Provided c0 = 0 and 0 < c1 ≤ log(2), there are functions g , ε− and ε+ such that the
above problem has a solution. Hence, we obtain a set of probabilistic interpretations fully
compatible with SVM scores. The solutions indexed by c1 are nested, in the sense that, for
any x, the length of the uncertainty interval, ε+ (x) − ε− (x), is monotonically decreasing in
c1 : the interpretation of SVM scores as posterior probabilities gets tighter as c1 increases.
The most restricted subset of admissible interpretations, with the shortest uncertainty inter-
vals, obtained for c1 = log(2), is represented in the left-hand side of Figure 3. The loss
incurred by a positive example is represented on the central graph, where the gray zone rep-
resents the neg-log-likelihood of all admissible solutions of g(x). Note that the hinge loss
is proportional to the neg-log-likelihood of the upper posterior probability g(x) + ε+ (x),
which is the loss for positive examples in the semi-parametric model in (4). Conversely, the
hinge loss for negative examples is reached for g(x) + ε− (x). An important observation,
that will be useful in Section 4.2 is that the neg-log-likelihood of any admissible functions
g(x) is tangent to the hinge loss at f (x) + b = 0.
The solution is unique in terms of the admissible interval [g + ε− , g + ε+ ], but many
(ε− , ε+ , g) solve (7). For example, g may be deﬁned as
deﬁnitions of

2−[1−(f (x)+b)]+
2−[1+(f (x)+b)]+ + 2−[1−(f (x)+b)]+
which is essentially the posterior probability model proposed by Sollich (2000), represented
dotted in the ﬁrst two graphs of Figure 3.

g(x; θ) =

(8)

,

The last graph of Figure 3 displays the mapping from g(x) to admissible values of p(1|x)
which results from the choice described in (8). Although the interpretation of SVM scores
does not require to specify g , it may worth to list some features common to all options.
First, g(x) + ε− (x) = 0 for all g(x) below some threshold g0 > 0, and conversely, g(x) +
ε+ (x) = 1 for all g(x) above some threshold g1 < 1. These two features are responsible
for the sparsity of the SVM solution. Second, the estimation of posterior probabilities is
accurate at 0.5, and the length of the uncertainty interval on p(1|x) monotonically increases
in [g0 , 0.5] and then monotonically decreases in [0.5, g1 ]. Hence, the training objective of
SVMs is intermediate between the accurate estimation of posterior probabilities on the
whole range [0, 1] and the minimization of the classi ﬁcation risk.

4 Outcomes of the Probabilistic Interpretation

This section gives two consequences of our probabilistic interpretation of SVMs. Further
outcomes, still reserved for future research are listed in Section 6.

4.1 Pointwise Posterior Probabilities from SVM Scores

g(x; θ) =

Platt (2000) proposed to estimate posterior probabilities from SVM scores by ﬁtting a lo-
gistic function over the SVM scores. The only logistic function compatible with the most
stringent interpretation of SVMs in the semi-parametric framework,
1
1 + 4−(f (x)+b))
is identical to the model of Sollich (2000) (8) when f (x) + b lies in the interval [−1, 1].
Other logistic functions are compatible with the looser interpretations obtained by letting
c1 < log(2), but their use as pointwise estimates is questionable, since the associated
conﬁdence interval is wider.
In particular, the looser inte rpretations do not ensure that
f (x) + b = 0 corresponds to g(x) = 0.5. Then, the decision function based on the
estimated posterior probabilities by g(x) may differ from the SVM decision function.
Being based on an arbitrary choice of g(x), pointwise estimates of posterior probabilities
derived from SVM scores should be handled with caution. As discussed by Zhang (2004),
they may only be consistent at f (x) + b = 0, where they may converge towards 0.5.

(9)

,

4.2 Unbalanced Classi ﬁcation Losses

[1 + (f (xi ) + b)]+ , (10)

SVMs are known to perform well regarding misclassi ﬁcation e rror, but they provide skewed
decision boundaries for unbalanced classi ﬁcation losses, where the losses associated with
incorrect decisions differ according to the true label. The mainstream approach used to
address this problem consists in using different losses for positive and negative examples
(Morik et al. 1999, Veropoulos et al. 1999), i.e.
1
H + C + X{i|yi=1}
[1 − (f (xi ) + b)]+ + C − X{i|yi=−1}
kf k2
2
where the coefﬁcients C + and C − are constants, whose ratio is equal to the ratio of the
losses ℓFN and ℓFP pertaining to false negatives and false positives, respectively (Lin et al.
2002).2 Bayes’ decision theory deﬁnes the optimal decision rule by p ositive classi ﬁcation
when P (y = 1|x) > P0 , where P0 = ℓFP
. We may thus rewrite C + = C · (1 − P0 )
ℓFP+ℓFN
and C − = C · P0 . With such deﬁnitions, the optimization problem may be inte rpreted
as an upper-bound on the classi ﬁcation risk deﬁned from ℓFN and ℓFP . However, the ma-
chinery of Section 3.2 unveils a major problem: the SVM decision function provided by
sign(f (xi ) + b) is not consistent with the probabilistic interpretation of SVM scores.
We address this problem by deriving another criterion, by requiring that the neg-log-
likelihood of any admissible functions g(x) is tangent to the hinge loss at f (x) + b = 0.
This leads to the following problem:
H + C 
 X{i|yi=1}
kf k2
[− log(1 − P0 ) + P0 (f (xi ) + b)]+
X{i|yi=−1}
 .
2False negatives/positives respectively designate positive/negative examples incorrectly classiﬁed.

[− log(P0 ) − (1 − P0 )(f (xi ) + b)]+ +

min
f ,b

(11)

min
f ,b

1
2

)
x
|
1
(
p

1

0.8

0.6

0.4

0.2

0
−10

5

4

3

2

1

)
1
,
)
x
(
g
(
L

0
−10

0

10

20

f(x)+b

1

)
x
|
1
(
p

0.5

0

10

20

f(x)+b

0
0

0.25

0.75

1

0.5
g(x)

Figure 4: Left:
lower (dashed) and upper (plain) posterior probabilities [g(x) +
ε− (x), g(x) + ε+ (x)] vs. SVM scores f (x) + b obtained from (11) with P0 = 0.25; center:
corresponding neg-log-likelihood of g(x) for positive examples vs. f (x) + b. right: lower
(dashed) and upper (plain) posterior probabilities vs. g(x), for g deﬁned by ε+ (x) = 0 for
f (x) + b ≤ 0 and ε− (x) = 0 for f (x) + b ≥ 0.

This loss differs from (10), in the respect that the margin for positive examples is smaller
than the one for negative examples when P0 < 0.5. In particular, (10) does not affect
the SVM solution for separable problems, while in (11), the decision boundary moves
towards positive support vectors when P0 decreases. The analogue of Figure 3, displayed
on Figure 4, shows that one recovers the characteristics of the standard SVM loss, except
that the focus is now on the posterior probability P0 deﬁned by Bayes’ decision rule.

5 Experiments with Unbalanced Classiﬁcations Losses

It is straightforward to implement (11) in standard SVM packages. For experimenting with
difﬁcult unbalanced two-class problems, we used the Forest database, the largest available
UCI dataset (http://kdd.ics.uci.edu/databases/covertype/). We con-
sider the subproblem of discriminating the positive class Krummholz (20510 examples)
against the negative class Spruce/Fir (211840 examples). The ratio of negative to positive
examples is high, a feature commonly encountered with unbalanced classi ﬁcation losses.

The training set was built by random selection of size 11 000 (1000 and 10 000 examples
from the positive and negative class respectively); a validation set, of size 11 000 was drawn
identically among the other examples; ﬁnally, the test set, of size 99 000, was drawn among
the remaining examples.
The performance was measured by the weighted risk function R = 1
n (NFN ℓFN+NFP ℓFP ),
where NFN and NFP are the number of false negatives and false positives, respectively. The
loss ℓFP was set to one, and ℓFN was successively set to 1, 10 and 100, in order to penalize
more and more heavily errors from the under-represented class.

All approaches were tested using SVMs with a Gaussian kernel on normalized data. The
hyper-parameters were tuned on the validation set for each of the ℓFN values. We addition-
ally considered three tuning for the bias b: ˆb is the bias returned by the algorithm; ˆbv the
bias returned by minimizing R on the validation set, which is an optimistic estimate of the
bias that could be computed by cross-validation. We also provide results for b∗ , the optimal
bias computed on the test set. This “crystal ball ” tuning may
not represent an achievable
goal, but it shows how far we are from the optimum. Table 1 compares the risk R obtained
with the three approaches for the different values of ℓFN .
The ﬁrst line, with ℓFN = 1 corresponds to the standard classi ﬁcation error, where all
training criteria are equivalent in theory and in practice. The bias returned by the algorithm
is very close to the optimal one. For ℓFN = 10 and ℓFN = 100, the models obtained
by optimizing C + /C − (10) and P0 (11) achieve better results than the baseline with the
crystal ball bias. While the solutions returned by C + /C − can be signi ﬁcantly improved

Table 1: Errors for 3 different criteria and for 3 different models over the Forest database
P0 , problem (11)
Baseline, problem (5) C + /C − , problem (10)
ℓFN
ˆbv
ˆb
ˆbv
ˆb
ˆb
b∗
b∗
b∗
0.026
0.027
0.027
0.026
0.027
0.027
0.026
0.027
0.167
0.108
0.105
0.104
0.094
0.095
0.104
0.094
0.289
0.291
0.295
0.289
0.291
0.403
0.406
1.664

1
10
100

by tuning the bias, our criterion provides results that are very close to the optimum, in the
range of the performances obtained with the bias optimized on an independant validation
set. The new optimization criterion can thus outperform standard approaches for highly
unbalanced problems.

6 Conclusion

This paper introduced a semi-parametric model for classi ﬁc ation which provides an inter-
esting viewpoint on SVMs. The non-parametric component provides an intuitive means of
transforming the likelihood into a decision-oriented criterion. This framework was used
here to propose a new parameterization of the hinge loss, dedicated to unbalanced classi ﬁ-
cation problems, yielding signi ﬁcant improvements over th e classical procedure.

Among other prospectives, we plan to apply the same framework to investigate hinge-like
criteria for decision rules including a reject option, where the classi ﬁer abstains when a
pattern is ambiguous. We also aim at deﬁning losses encourag ing sparsity in probabilistic
models, such as kernelized logistic regression. We could thus build sparse probabilistic
classi ﬁers, providing an accurate estimation of posterior probabilities on a (limited) pre-
deﬁned range of posterior probabilities. In particular, we
could derive decision-oriented
criteria for multi-class probabilistic classi ﬁers. For ex ample, minimizing classi ﬁcation er-
ror only requires to ﬁnd the class with highest posterior pro bability, and this search does
not require precise estimates of probabilities outside the interval [1/K, 1/2], where K is
the number of classes.

References
Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classiﬁca tion in non-standard situations.
Machine Learning, 46:191–202, 2002.
B. G. Lindsay. Nuisance parameters. In S. Kotz, C. B. Read, and D. L. Banks, editors, Encyclopedia
of Statistical Sciences, volume 6. Wiley, 1985.
G. J. McLachlan. Discriminant analysis and statistical pattern recognition. Wiley, 1992.
K. Morik, P. Brockhausen, and T. Joachims. Combining statistical learning with a knowledge-based
approach - a case study in intensive care monitoring. In Proceedings of ICML, 1999.
D. Oakes. Semi-parametric models. In S. Kotz, C. B. Read, and D. L. Banks, editors, Encyclopedia
of Statistical Sciences, volume 8. Wiley, 1988.
J. C. Platt. Probabilities for SV machines. In A. J. Smola, P. L. Bartlett, B. Sch ¨olkopf, and D. Schu-
urmans, editors, Advances in Large Margin Classiﬁers , pages 61–74. MIT Press, 2000.
P. Sollich. Probabilistic methods for support vector machines. In S. A. Solla, T. K. Leen, and K.-R.
M ¨uller, editors, Advances in Neural Information Processing Systems 12, pages 349–355, 2000.
K. Veropoulos, C. Campbell, and N. Cristianini. Controlling the sensitivity of support vector ma-
chines. In T. Dean, editor, Proc. of the IJCAI, pages 55–60, 1999.
T. Zhang. Statistical behavior and consistency of classiﬁcation methods b ased on convex risk mini-
mization. Annals of Statistics, 32(1):56–85, 2004.


TD(0) Leads to Better Policies than
Approximate Value Iteration

Benjamin Van Roy
Management Science and Engineering and Electrical Engineering
Stanford University
Stanford, CA 94305
bvr@stanford.edu

Abstract

We consider approximate value iteration with a parameterized approxi-
mator in which the state space is partitioned and the optimal cost-to-go
function over each partition is approximated by a constant. We estab-
lish performance loss bounds for policies derived from approximations
associated with ﬁxed points. These bounds identify beneﬁts to having
projection weights equal to the invariant distribution of the resulting pol-
icy. Such projection weighting leads to the same ﬁxed points as TD(0).
Our analysis also leads to the ﬁrst performance loss bound for approxi-
mate value iteration with an average cost objective.

1 Preliminaries

Consider a discrete-time communicating Markov decision process (MDP) with a ﬁnite state
space S = {1, . . . , |S |}. At each state x ∈ S , there is a ﬁnite set Ux of admissible actions.
P
If the current state is x and an action u ∈ Ux is selected, a cost of gu (x) is incurred, and
the system transitions to a state y ∈ S with probability pxy (u). For any x ∈ S and u ∈ Ux ,
y∈S pxy (u) = 1. Costs are discounted at a rate of α ∈ (0, 1) per period. Each instance
of such an MDP is deﬁned by a quintuple (S , U , g , p, α).
A (stationary deterministic) policy is a mapping µ that assigns an action u ∈ Ux to each
state x ∈ S . If actions are selected based on a policy µ, the state follows a Markov process
with transition matrix Pµ , where each (x, y)th entry is equal to pxy (µ(x)). The restriction
to communicating MDPs ensures that it is possible to reach any state from any other state.
P∞
Each policy µ is associated with a cost-to-go function Jµ ∈ <|S | , deﬁned by Jµ =
(gu (x) + α P
µ gµ = (I − αPµ )−1 gµ , where, with some abuse of notation, gµ (x) = gµ(x) (x)
t=0 αtP t
for each x ∈ S . A policy µ is said to be greedy with respect to a function J if
µ(x) ∈ argmin
y∈S pxy (u)J (y)) for all x ∈ S .
u∈Ux
The optimal cost-to-go function J ∗ ∈ <|S | is deﬁned by J ∗ (x) = minµ Jµ (x), for all
x ∈ S . A policy µ∗ is said to be optimal if Jµ∗ = J ∗ . It is well-known that an optimal
policy exists. Further, a policy µ∗ is optimal if and only if it is greedy with respect to J ∗ .
Hence, given the optimal cost-to-go function, optimal actions can computed be minimizing
the right-hand side of the above inclusion.

α P
Value iteration generates a sequence J‘ converging to J ∗ according to J‘+1 = T J‘ ,
where T is the dynamic programming operator, deﬁned by (T J )(x) = minu∈Ux (gu (x) +
y∈S pxy (u)J (y)), for all x ∈ S and J ∈ <|S | . This sequence converges to J ∗ for any
initialization of J0 .

2 Approximate Value Iteration

The state spaces of relevant MDPs are typically so large that computation and storage of
a cost-to-go function is infeasible. One approach to dealing with this obstacle involves
partitioning the state space S into a manageable number K of disjoint subsets S1 , . . . , SK
and approximating the optimal cost-to-go function with a function that is constant over
each partition. This can be thought of as a form of state aggregation – all states within a
given partition are assumed to share a common optimal cost-to-go.
To represent an approximation, we deﬁne a matrix Φ ∈ <|S |×K such that each k th column
is an indicator function for the k th partition Sk . Hence, for any r ∈ <K , k , and x ∈ Sk ,
(Φr)(x) = rk . In this paper, we study variations of value iteration, each of which computes
a vector r so that Φr approximates J ∗ . The use of such a policy µr which is greedy with
respect to Φr is justiﬁed by the following result (see [10] for a proof):
Theorem 1 If µ is a greedy policy with respect to a function ˜J ∈ <|S | then
kJµ − J ∗ k∞ ≤ 2α
kJ ∗ − ˜J k∞ .
1 − α
One common way of approximating a function J ∈ <|S | with a function of the form Φr in-
norm: kJ k2,π = (cid:0)P
x∈S π(x)J 2 (x)(cid:1)1/2 . Here, π ∈ <|S |
volves projection with respect to a weighted Euclidean norm k ·kπ . The weighted Euclidean
+ is a vector of weights that assign
relative emphasis among states. The projection Ππ J is the function Φr that attains the min-
imum of kJ −Φrk2,π ; if there are multiple functions Φr that attain the minimum, they must
the k th partition, for any π , J , and x ∈ Sk , (Ππ J )(x) = P
π(y)J (y)/ P
form an afﬁne space, and the projection is taken to be the one with minimal norm kΦrk2,π .
Note that in our context, where each k th column of Φ represents an indicator function for
π(y).
y∈Sk
y∈Sk
Approximate value iteration begins with a function Φr(0) and generates a sequence accord-
ing to Φr(‘+1) = Ππ T Φr(‘) . It is well-known that the dynamic programming operator T is
a contraction mapping with respect to the maximum norm. Further, Ππ is maximum-norm
nonexpansive [16, 7, 8]. (This is not true for general Φ, but is true in our context in which
columns of Φ are indicator functions for partitions.) It follows that the composition Ππ T
is a contraction mapping. By the contraction mapping theorem, Ππ T has a unique ﬁxed
point Φ˜r , which is the limit of the sequence Φr(‘) . Further, the following result holds:

Theorem 2 For any MDP, partition, and weights π with support intersecting every parti-
tion, if Φ˜r = Ππ T Φ˜r then
kΦ˜r − J ∗ k∞ ≤ 2
1 − α

kJ ∗ − Φrk∞ ,

min
r∈<K

and

(1 − α)kJµ ˜r − J ∗ k∞ ≤ 4α
1 − α
The ﬁrst inequality of the theorem is an approximation error bound, established in [16, 7, 8]
for broader classes of approximators that include state aggregation as a special case. The

kJ ∗ − Φrk∞ .

min
r∈<K

second is a performance loss bound, derived by simply combining the approximation error
bound and Theorem 1.
Note that Jµ ˜r (x) ≥ J ∗ (x) for all x, so the left-hand side of the performance loss bound
is the maximal increase in cost-to-go, normalized by 1 − α. This normalization is natural,
since a cost-to-go function is a linear combination of expected future costs, with coefﬁcients
1, α, α2 , . . ., which sum to 1/(1 − α).
Our motivation of the normalizing constant begs the question of whether, for ﬁxed MDP
parameters (S , U , g , p) and ﬁxed Φ, minr kJ ∗ − Φrk∞ also grows with 1/(1 − α). It turns
out that minr kJ ∗ − Φrk∞ = O(1). To see why, note that for any µ,
1
Jµ = (I − αPµ )−1 gµ =
1 − α
where λµ (x) is the expected average cost if the process starts in state x and is controlled
τ −1X
by policy µ,
1
τ
t=0
and hµ is the discounted differential cost function
hµ = (I − αPµ )−1 (gµ − λµ ).
Both λµ and hµ converge to ﬁnite vectors as α approaches 1 [3]. For an optimal policy
µ∗ , limα↑1 λµ∗ (x) does not depend on x (in our context of a communicating MDP). Since
constant functions lie in the range of Φ,
kJ ∗ − Φrk∞ ≤ lim
min
lim
α↑1
α↑1
r∈<K

khµ∗ k∞ < ∞.

λµ + hµ ,

λµ = lim
τ →∞

P t
µ gµ ,

The performance loss bound still exhibits an undesirable dependence on α through the
coefﬁcient 4α/(1 − α). In most relevant contexts, α is close to 1; a representative value
might be 0.99. Consequently, 4α/(1 − α) can be very large. Unfortunately, the bound is
sharp, as expressed by the following theorem. We will denote by 1 the vector with every
component equal to 1.
Theorem 3 For any δ > 0, α ∈ (0, 1), and ∆ ≥ 0, there exists MDP parameters
(S , U , g , p) and a partition such that minr∈<K kJ ∗ − Φrk∞ = ∆ and, if Φ˜r = Ππ T Φ˜r
with π = 1,
(1 − α)kJµ ˜r − J ∗ k∞ ≥ 4α
kJ ∗ − Φrk∞ − δ.
1 − α
This theorem is established through an example in [22]. The choice of uniform weights
(π = 1) is meant to point out that even for such a simple, perhaps natural, choice of
weights, the performance loss bound is sharp.
Based on Theorems 2 and 3, one might expect that there exists MDP parameters (S , U , g , p)
(cid:18) 1
(cid:19)
and a partition such that, with π = 1,
kJ ∗ − Φrk∞
(1 − α)kJµ ˜r − J ∗ k∞ = Θ
min
1 − α
.
r∈<K
In other words, that the performance loss is both lower and upper bounded by 1/(1 − α)
times the smallest possible approximation error. It turns out that this is not true, at least
if we restrict to a ﬁnite state space. However, as the following theorem establishes, the
coefﬁcient multiplying minr∈<K kJ ∗ − Φrk∞ can grow arbitrarily large as α increases,
keeping all else ﬁxed.

min
r∈<K

Theorem 4 For any L and ∆ ≥ 0, there exists MDP parameters (S , U , g , p) and a parti-
tion such that limα↑1 minr∈<K kJ ∗ − Φrk∞ = ∆ and, if Φ˜r = Ππ T Φ˜r with π = 1,
(1 − α) (Jµ ˜r (x) − J ∗ (x)) ≥ L lim
kJ ∗ − Φrk∞ ,
min
lim inf
α↑1
α↑1
r∈<K

for all x ∈ S .
This Theorem is also established through an example [22].
For any µ and x,

((1 − α)Jµ (x) − λµ (x)) = lim
(1 − α)hµ (x) = 0.
lim
α↑1
α↑1
Combined with Theorem 4, this yields the following corollary.
Corollary 1 For any L and ∆ ≥ 0, there exists MDP parameters (S , U , g , p) and a parti-
tion such that limα↑1 minr∈<K kJ ∗ − Φrk∞ = ∆ and, if Φ˜r = Ππ T Φ˜r with π = 1,
(λµ ˜r (x) − λµ∗ (x)) ≥ L lim
kJ ∗ − Φrk∞ ,
lim inf
min
α↑1
α↑1
r∈<K

for all x ∈ S .

3 Using the Invariant Distribution

In the previous section, we considered an approximation Φ˜r that solves Ππ T Φ˜r = Φ˜r for
some arbitrary pre-selected weights π . We now turn to consider use of an invariant state
distribution π ˜r of Pµ ˜r as the weight vector.1 This leads to a circular deﬁnition: the weights
are used in deﬁning ˜r and now we are deﬁning the weights in terms of ˜r . What we are
really after here is a vector ˜r that satisﬁes Ππ ˜r T Φ˜r = Φ˜r . The following theorem captures
the associated beneﬁts. (Due to space limitations, we omit the proof, which is provided in
the full length version of this paper [22].)
Theorem 5 For any MDP and partition, if Φ˜r = Ππ ˜r T Φ˜r and π ˜r has support intersecting
every partition, (1 − α)πT
˜r (Jµ ˜r − J ∗ ) ≤ 2α minr∈<K kJ ∗ − Φrk∞ .
When α is close to 1, which is typical, the right-hand side of our new performance loss
bound is far less than that of Theorem 2. The primary improvement is in the omission of a
factor of 1 − α from the denominator. But for the bounds to be compared in a meaningful
way, we must also relate the left-hand-side expressions. A relation can be based on the fact
that for all µ, limα↑1 k(1 − α)Jµ − λµk∞ = 0, as explained in Section 2. In particular,
based on this, we have
πT (Jµ − J ∗ ),
(1 − α)kJµ − J ∗ k∞ = |λµ − λ∗ | = λµ − λ∗ = lim
lim
α↑1
α↑1
for all policies µ and probability distributions π . Hence, the left-hand-side expressions
from the two performance bounds become directly comparable as α approaches 1.
Another interesting comparison can be made by contrasting Corollary 1 against the follow-
ing immediate consequence of Theorem 5.
P
Corollary 2 For all MDP parameters (S , U , g , p) and partitions, if Φ˜r = Ππ ˜r T Φ˜r and
lim inf α↑1
π ˜r (x) > 0 for all k ,
x∈Sk
kJ ∗ − Φrk∞ .
kλµ ˜r − λµ∗ k∞ ≤ 2 lim
lim sup
α↑1
α↑1

min
r∈<K

The comparison suggests that solving Φ˜r = Ππ ˜r T Φ˜r is strongly preferable to solving
Φ˜r = Ππ T Φ˜r with π = 1.
1By an invariant state distribution of a transition matrix P , we mean any probability distribution
π such that πT P = πT . In the event that Pµ ˜r has multiple invariant distributions, π ˜r denotes an
arbitrary choice.

4 Exploration

µ(x, u) =

If a vector ˜r solves Φ˜r = Ππ ˜r T Φ˜r and the support of π ˜r intersects every partition, Theorem
5 promises a desirable bound. However, there are two signiﬁcant shortcomings to this
solution concept, which we will address in this section. First, in some cases, the equation
Ππ ˜r T Φ˜r = Φ˜r does not have a solution. It is easy to produce examples of this; though
no example has been documented for the particular class of approximators we are using
here, [2] offers an example involving a different linearly parameterized approximator that
captures the spirit of what can happen. Second, it would be nice to relax the requirement
that the support of π ˜r intersect every partition.
To address these shortcomings, we introduce stochastic policies. A stochastic policy µ
u ∈ Ux , and P
maps state-action pairs to probabilities. For each x ∈ S and u ∈ Ux , µ(x, u) is the
probability of taking action u when in state x. Hence, µ(x, u) ≥ 0 for all x ∈ S and
µ(x, u) = 1 for all x ∈ S .
u∈Ux
Given a scalar  > 0 and a function J , the -greedy Boltzmann exploration policy with
respect to J is deﬁned by
P
e−(Tu J )(x)(|Ux |−1)/e
e−(Tu J )(x)(|Ux |−1)/e
u∈Ux
For any  > 0 and r , let µ
r denote the -greedy Boltzmann exploration policy with respect
P
to Φr . Further, we deﬁne a modiﬁed dynamic programming operator that incorporates
Boltzmann exploration:
P
e−(Tu J )(x)(|Ux |−1)/e (TuJ )(x)
u∈Ux
e−(Tu J )(x)(|Ux |−1)/e
u∈Ux
As  approaches 0, -greedy Boltzmann exploration policies become greedy and the mod-
iﬁed dynamic programming operators become the dynamic programming operator. More
r (x, µr (x)) = 1 and lim↓1 T J = T J . These are
precisely, for all r , x, and J , lim↓0 µ
Lemma 1 For any n, v ∈ <n , mini vi +  ≥ P
i e−vi (n−1)/e vi / P
immediate consequences of the following result (see [4] for a proof).
i e−vi (n−1)/e ≥
mini vi .
Because we are only concerned with communicating MDPs, there is a unique invariant
state distribution associated with each -greedy Boltzmann exploration policy µ
r and the
support of this distribution is S . Let π 
r denote this distribution. We consider a vector ˜r that
solves Φ˜r = Ππ
T Φ˜r . For any  > 0, there exists a solution to this equation (this is an
˜r
immediate extension of Theorem 5.1 from [4]).
We have the following performance loss bound, which parallels Theorem 5 but with an
equation for which a solution is guaranteed to exist and without any requirement on the
resulting invariant distribution. (Again, we omit the proof, which is available in [22].)
T Φ˜r then (1 −

(T J )(x) =

Theorem 6 For any MDP, partition, and  > 0,
− J ∗ ) ≤ 2α minr∈<K kJ ∗ − Φrk∞ + .
˜r )T (Jµ
α)(π 
˜r

if Φ˜r = Ππ
˜r

.

.

5 Computation: TD(0)

Though computation is not a focus of this paper, we offer a brief discussion here. First,
we describe a simple algorithm from [16], which draws on ideas from temporal-difference
learning [11, 12] and Q-learning [23, 24] to solve Φ˜r = Ππ T Φ˜r .
It requires an abil-
ity to sample a sequence of states x(0) , x(1) , x(2) , . . ., each independent and identically

,

r(‘+1) = r(‘) + γ‘φ(x(‘) )

minu∈Ux (gu (x) + α P
distributed according to π . Also required is a way to efﬁciently compute (T Φr)(x) =
y∈S pxy (u)(Φr)(y)), for any given x and r . This is typically pos-
sible when the action set Ux and the support of px· (u) (i.e., the set of states that can follow
x if action u is selected) are not too large. The algorithm generates a sequence of vectors
(cid:17)
(cid:16)
r(‘) according to
(T Φr(‘) )(x(‘) ) − (Φr(‘) )(x(‘) )
where γ‘ is a step size and φ(x) denotes the column vector made up of components from
the xth row of Φ. In [16], using results from [15, 9], it is shown that under appropriate as-
sumptions on the step size sequence, r(‘) converges to a vector ˜r that solves Φ˜r = Ππ T Φ˜r .
The equation Φ˜r = Ππ T Φ˜r may have no solution. Further, the requirement that states
are sampled independently from the invariant distribution may be impractical. However, a
natural extension of the above algorithm leads to an easily implementable version of TD(0)
that aims at solving Φ˜r = Ππ
T Φ˜r . The algorithm requires simulation of a trajectory
x0 , x1 , x2 , . . . of the MDP, with each action ut ∈ Uxt generated by the -greedy Boltz-
˜r
mann exploration policy with respect to Φr(t) . The sequence of vectors r(t) is generated
(cid:16)
(cid:17)
according to
(T Φr(t) )(xt ) − (Φr(t) )(xt )
Under suitable conditions on the step size sequence, if this algorithm converges, the limit
satisﬁes Φ˜r = Ππ
T Φ˜r . Whether such an algorithm converges and whether there are
˜r
other algorithms that can effectively solve Φ˜r = Ππ
T Φ˜r for broad classes of relevant
˜r
problems remain open issues.

r(t+1) = r(t) + γtφ(xt )

.

6 Extensions and Open Issues

Our results demonstrate that weighting a Euclidean norm projection by the invariant dis-
tribution of a greedy (or approximately greedy) policy can lead to a dramatic performance
gain. It is intriguing that temporal-difference learning implicitly carries out such a pro-
jection, and consequently, any limit of convergence obeys the stronger performance loss
bound.
This is not the ﬁrst time that the invariant distribution has been shown to play a critical
role in approximate value iteration and temporal-difference learning. In prior work involv-
ing approximation of a cost-to-go function for a ﬁxed policy (no control) and a general
linearly parameterized approximator (arbitrary matrix Φ), it was shown that weighting by
the invariant distribution is key to ensuring convergence and an approximation error bound
[17, 18]. Earlier empirical work anticipated this [13, 14].
The temporal-difference learning algorithm presented in Section 5 is a version of TD(0),
This is a special case of TD(λ), which is parameterized by λ ∈ [0, 1]. It is not known
whether the results of this paper can be extended to the general case of λ ∈ [0, 1]. Prior
research has suggested that larger values of λ lead to superior results. In particular, an
example of [1] and the approximation error bounds of [17, 18], both of which are restricted
to the case of a ﬁxed policy, suggest that approximation error is ampliﬁed by a factor of
1/(1 − α) as λ is changed from 1 to 0. The results of Sections 3 and 4 suggest that
this factor vanishes if one considers a controlled process and performance loss rather than
approximation error.
Whether the results of this paper can be extended to accommodate approximate value it-
eration with general linearly parameterized approximators remains an open issue. In this
broader context, error and performance loss bounds of the kind offered by Theorem 2 are

unavailable, even when the invariant distribution is used to weight the projection. Such
error and performance bounds are available, on the other hand, for the solution to a certain
linear program [5, 6]. Whether a factor of 1/(1 − α) can similarly be eliminated from these
bounds is an open issue.
Our results can be extended to accommodate an average cost objective, assuming that the
MDP is communicating. With Boltzmann exploration, the equation of interest becomes
(T Φ˜r − ˜λ1).
Φ˜r = Ππ
˜r
The variables include an estimate ˜λ ∈ < of the minimal average cost λ∗ ∈ < and an
approximation Φ˜r of the optimal differential cost function h∗ . The discount factor α is set
to 1 in computing an -greedy Boltzmann exploration policy as well as T  . There is an
average-cost version of temporal-difference learning for which any limit of convergence
(˜λ, ˜r) satisﬁes this equation [19, 20, 21]. Generalization of Theorem 2 does not lead to a
useful result because the right-hand side of the bound becomes inﬁnite as α approaches 1.
On the other hand, generalization of Theorem 6 yields the ﬁrst performance loss bound for
approximate value iteration with an average-cost objective:

Theorem 7 For any communicating MDP with an average-cost objective, partition, and
(T Φ˜r − ˜λ1) then
 > 0, if Φ˜r = Ππ
˜r
− λ∗ ≤ 2 min
λµ
r∈<K
˜r
∈ < denotes the average cost under policy µ
Here, λµ
˜r , which is well-deﬁned because the
˜r
process is irreducible under an -greedy Boltzmann exploration policy. This theorem can be
proved by taking limits on the left and right-hand sides of the bound of Theorem 6. It is easy
− λ∗ . The limit of minr∈<K kJ ∗ − Φrk∞
to see that the limit of the left-hand side is λµ
on the right-hand side is minr∈<K kh∗ − Φrk∞ . (This follows from the analysis of [3].)
˜r

kh∗ − Φrk∞ + .

Acknowledgments

This material is based upon work supported by the National Science Foundation under
Grant ECS-9985229 and by the Ofﬁce of Naval Research under Grant MURI N00014-00-
1-0637. The author’s understanding of the topic beneﬁted from collaborations with Dimitri
Bertsekas, Daniela de Farias, and John Tsitsiklis. A full length version of this paper has
been submitted to Mathematics of Operations Research and has beneﬁted from a number
of useful comments and suggestions made by reviewers.

References
[1] D. P. Bertsekas. A counterexample to temporal-difference learning. Neural Compu-
tation, 7:270–279, 1994.
[2] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc,
Belmont, MA, 1996.
[3] D. Blackwell. Discrete dynamic programming. Annals of Mathematical Statistics,
33:719–726, 1962.
[4] D. P. de Farias and B. Van Roy. On the existence of ﬁxed points for approximate
value iteration and temporal-difference learning. Journal of Optimization Theory and
Applications, 105(3), 2000.
[5] D. P. de Farias and B. Van Roy. Approximate dynamic programming via linear pro-
gramming. In Advances in Neural Information Processing Systems 14. MIT Press,
2002.

[6] D. P. de Farias and B. Van Roy. The linear programming approach to approximate
dynamic programming. Operations Research, 51(6):850–865, 2003.
[7] G. J. Gordon. Stable function approximation in dynamic programming. Technical
Report CMU-CS-95-103, Carnegie Mellon University, 1995.
[8] G. J. Gordon. Stable function approximation in dynamic programming. In Machine
Learning: Proceedings of the Twelfth International Conference (ICML), San Fran-
cisco, CA, 1995.
[9] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the Convergence of Stochastic Iterative
Dynamic Programming Algorithms. Neural Computation, 6:1185–1201, 1994.
[10] S. P. Singh and R. C. Yee. An upper-bound on the loss from approximate optimal-
value functions. Machine Learning, 1994.
[11] R. S. Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis,
University of Massachusetts, Amherst, Amherst, MA, 1984.
[12] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine
Learning, 3:9–44, 1988.
[13] R. S. Sutton. On the virtues of linear learning and trajectory distributions. In Pro-
ceedings of the Workshop on Value Function Approximation, Machine Learning Con-
ference, 1995.
[14] R. S. Sutton. Generalization in reinforcement learning: Successful examples using
sparse coarse coding. In Advances in Neural Information Processing Systems 8, Cam-
bridge, MA, 1996. MIT Press.
[15] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine
Learning, 16:185–202, 1994.
[16] J. N. Tsitsiklis and B. Van Roy. Feature–based methods for large scale dynamic
programming. Machine Learning, 22:59–94, 1996.
[17] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal–difference learning with
function approximation. IEEE Transactions on Automatic Control, 42(5):674–690,
1997.
[18] J. N. Tsitsiklis and B. Van Roy. Analysis of temporal-difference learning with func-
tion approximation. In Advances in Neural Information Processing Systems 9, Cam-
bridge, MA, 1997. MIT Press.
[19] J. N. Tsitsiklis and B. Van Roy. Average cost temporal-difference learning. In Pro-
ceedings of the IEEE Conference on Decision and Control, 1997.
[20] J. N. Tsitsiklis and B. Van Roy. Average cost temporal-difference learning. Automat-
ica, 35(11):1799–1808, 1999.
[21] J. N. Tsitsiklis and B. Van Roy. On average versus discounted reward temporal-
difference learning. Machine Learning, 49(2-3):179–191, 2002.
[22] B. Van Roy. Performance loss bounds for approximate value iteration with state
aggregation. Under review with Mathematics of Operations Research, available at
www.stanford.edu/ bvr/psﬁles/aggregation.pdf, 2005.
[23] C. J. C. H. Watkins. Learning From Delayed Rewards. PhD thesis, Cambridge Uni-
versity, Cambridge, UK, 1989.
[24] C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279–292, 1992.


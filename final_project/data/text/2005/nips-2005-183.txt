Variational Bayesian Stochastic
Complexity of Mixture Models

Kazuho Watanabe∗
Department of Computational Intelligence
and Systems Science
Tokyo Institute of Technology
Mail Box:R2-5, 4259 Nagatsuta,
Midori-ku, Yokohama, 226-8503, Japan
kazuho23@pi.titech.ac.jp

Sumio Watanabe
P& I Lab.
Tokyo Institute of Technology
swatanab@pi.titech.ac.jp

Abstract

The Variational Bayesian framework has been widely used to ap-
proximate the Bayesian learning.
In various applications, it has
provided computational tractability and good generalization per-
formance. In this paper, we discuss the Variational Bayesian learn-
ing of the mixture of exponential families and provide some ad-
ditional theoretical support by deriving the asymptotic form of
the stochastic complexity. The stochastic complexity, which cor-
responds to the minimum free energy and a lower bound of the
marginal likelihood, is a key quantity for model selection. It also
enables us to discuss the eﬀect of hyperparameters and the accu-
racy of the Variational Bayesian approach as an approximation of
the true Bayesian learning.

1 Introduction

The Variational Bayesian (VB) framework has been widely used as an approxima-
tion of the Bayesian learning for models involving hidden (latent) variables such as
mixture models[2][4]. This framework provides computationally tractable posterior
distributions with only modest computational costs in contrast to Markov chain
Monte Carlo (MCMC) methods.
In many applications, it has performed better
generalization compared to the maximum likelihood estimation.
In spite of its tractability and its wide range of applications, little has been done
to investigate the theoretical properties of the Variational Bayesian learning itself.
For example, questions like how accurately it approximates the true one remained
unanswered until quite recently. To address these issues, the stochastic complexity
in the Variational Bayesian learning of gaussian mixture models was clariﬁed and
the accuracy of the Variational Bayesian learning was discussed[10].

∗

This work was supported by the Ministry of Education, Science, Sports and Culture,
Grant-in-Aid for JSPS Fellows 4637 and for Scientiﬁc Research 15500130, 2005.

In this paper, we focus on the Variational Bayesian learning of more general mix-
ture models, namely the mixtures of exponential families which include mixtures of
distributions such as gaussian, binomial and gamma. Mixture models are known to
be non-regular statistical models due to the non-identiﬁability of parameters caused
by their hidden variables[7]. In some recent studies, the Bayesian stochastic com-
plexities of non-regular models have been clariﬁed and it has been proven that they
become smaller than those of regular models[12][13]. This indicates an advantage
of the Bayesian learning when it is applied to non-regular models.
As our main results, the asymptotic upper and lower bounds are obtained for the
stochastic complexity or the free energy in the Variational Bayesian learning of the
mixture of exponential families. The stochastic complexity is important quantity
for model selection and giving the asymptotic form of it also contributes to the
following two issues. One is the accuracy of the Variational Bayesian learning as
an approximation method since the stochastic complexity shows the distance from
the variational posterior distribution to the true Bayesian posterior distribution in
the sense of Kullback information.
Indeed, we give the asymptotic form of the
stochastic complexity as F (n) (cid:1) λ log n where n is the sample size, by comparing
the coeﬃcient λ with that of the true Bayesian learning, we discuss the accuracy of
the VB approach. Another is the inﬂuence of the hyperparameter on the learning
process. Since the Variational Bayesian algorithm is a procedure of minimizing the
functional that ﬁnally gives the stochastic complexity, the derived bounds indicate
how the hyperparameters inﬂuence the process of the learning. Our results have
an implication for how to determine the hyperparameter values before the learning
process.
We consider the case in which the true distribution is contained in the learner model.
Analyzing the stochastic complexity in this case is most valuable for comparing the
Variational Bayesian learning with the true Bayesian learning. This is because the
advantage of the Bayesian learning is typical in this case[12]. Furthermore, this
analysis is necessary and essential for addressing the model selection problem and
hypothesis testing.
The paper is organized as follows. In Section 2, we introduce the mixture of expo-
nential family model. In Section 3, we describe the Bayesian learning. In Section
4, the Variational Bayesian framework is described and the variational posterior
distribution for the mixture of exponential family model is derived. In Section 5,
we present our main result. Discussion and conclusion follow in Section 6.

2 Mixture of Exponential Family
Denote by c(x|b) a density function of the input x ∈ RN given an M -dimensional
parameter vector b = (b(1) , b(2) , · · · , b(M ))T ∈ B where B is a subset of RM . The
general mixture model p(x|θ) with a parameter vector θ is deﬁned by
K(cid:1)
p(x|θ) =
ak c(x|bk ),
(cid:2)
k=1
where integer K is the number of components and {ak |ak ≥ 0,
k=1 ak = 1} is the
K
set of mixing proportions. The model parameter θ is {ak , bk }K
k=1.
A mixture model is called a mixture of exponential family (MEF) model or exponen-
tial family mixture model if the probability distribution c(x|b) for each component
is given by the following form,
c(x|b) = exp{b · f (x) + f0 (x) − g(b)},

(1)

ϕ(a) =

where b ∈ B is called the natural parameter, b · f (x) is its inner product with the
vector f (x) = (f1 (x), · · · , fM (x))T , f0 (x) and g(b) are real-valued functions of the
input x and the parameter b, respectively[3]. Suppose functions f1 , · · · , fM and a
constant function are linearly independent, which means the eﬀective number of
parameters in a single component distribution c(x|b) is M .
The conjugate prior distribution ϕ(θ) for the MEF model is given by the product
of the following two distributions on a = {ak }K
k=1 and b = {bk }K
k=1,
K(cid:3)
aφ0−1
Γ(K φ0 )
,
Γ(φ0 )K
k
K(cid:3)
K(cid:3)
exp{ξ0 (bk · ν0 − g(bk ))}
ϕ(bk ) =
C (ξ0 , ν0)
k=1
k=1
where ξ0 > 0, ν0 ∈ RM and φ0 > 0 are constants called hyperparameters and
(cid:4)
exp{ξ (µ · b − g(b))}db

C (ξ , µ) =
is a function of ξ ∈ R and µ ∈ RM .
The mixture model can be rewritten as follows by using a hidden variable y =
(y1 , · · · , yK ) ∈ {(1, 0, · · · , 0), (0, 1, · · · , 0), · · · , (0, 0, · · · , 1)},
(cid:5)
(cid:6)yk
K(cid:3)
p(x, y|θ) =
ak c(x|bk )

ϕ(b) =

(2)

(4)

k=1

,

(3)

.

k=1
If and only if the datum x is generated from the kth component, yk = 1.

3 The Bayesian Learning
Suppose n training samples X n = {x1 , · · · , xn} are independently and identically
taken from the true distribution p0 (x). In the Bayesian learning of a model p(x|θ)
whose parameter is θ, ﬁrst, the prior distribution ϕ(θ) on the parameter θ is set.
Then the posterior distribution p(θ|X n ) is computed from the given dataset and
the prior by
p(θ|X n ) =
exp(−nHn(θ))ϕ(θ),
1
Z (X n )
where Hn(θ) is the empirical Kullback information,
n(cid:1)
log p0(xi )
p(xi |θ) ,
i=1
and Z (X n ) is the normalization constant that is also known as the marginal like-
lihood or the evidence of the dataset X n [6]. The Bayesian predictive distribution
p(x|X n ) is given by averaging the model over the posterior distribution as follows,
(cid:4)
p(x|X n) =
p(x|θ)p(θ|X n )dθ.

Hn(θ) =

(6)

(7)

(5)

1
n

The stochastic complexity F (X n ) is deﬁned by
F (X n ) = − log Z (X n ),

(8)

which is also called the free energy and is important in most data modelling prob-
lems. Practically, it is used as a criterion by which the model is selected and the
hyperparameters in the prior are optimized[1][9].
(cid:8)
(cid:7)
Deﬁne the average stochastic complexity F (n) by
F (n) = EXn
F (X n)
(9)
,
where EXn [·] denotes the expectation value over all sets of training samples. Re-
cently, it was proved that F (n) has the following asymptotic form[12],
F (n) (cid:1) λ log n − (m − 1) log log n + O(1),
(10)
where λ and m are the rational number and the natural number respectively which
are determined by the singularities of the set of true parameters. In regular sta-
tistical models, 2λ is equal to the number of parameters and m = 1, whereas in
non-regular models such as mixture models, 2λ is not larger than the number of
parameters and m ≥ 1. This means an advantage of the Bayesian learning.
However, in the Bayesian learning, one computes the stochastic complexity or the
predictive distribution by integrating over the posterior distribution, which typically
cannot be performed analytically. As an approximation, the VB framework was
proposed[2][4].

4 The Variational Bayesian Learning

4.1 The Variational Bayesian Framework
In the VB framework, the Bayesian posterior p(Y n , θ|X n ) of the hidden variables
and the parameters is approximated by the variational posterior q(Y n , θ|X n ), which
factorizes as
q(Y n , θ|X n ) = Q(Y n |X n)r(θ|X n ),
(11)
where Q(Y n |X n ) and r(θ|X n ) are posteriors on the hidden variables and the pa-
rameters respectively. The variational posterior q(Y n , θ|X n) is chosen to minimize
(cid:4)
(cid:1)
the functional F [q ] deﬁned by
q(Y n , θ|X n) log q(Y n , θ|X n )p0 (X n )
F [q ] =
p(X n , Y n , θ)
Y n
= F (X n ) + K (q(Y n , θ|X n )||p(Y n , θ|X n )),
(13)
where K (q(Y n , θ|X n )||p(Y n , θ|X n )) is the Kullback information between the true
Bayesian posterior p(Y n , θ|X n ) and the variational posterior q(Y n , θ|X n) 1 . This
leads to the following theorem. The proof is well known[8].

(12)

dθ,

Theorem 1 If the functional F [q ] is minimized under the constraint (11) then the
variational posteriors, r(θ|X n ) and Q(Y n |X n ), satisfy
(cid:9)
(cid:10)
log p(X n , Y n |θ)
r(θ|X n ) =
1
ϕ(θ) exp
Q(Y n |X n ) ,
(cid:9)
(cid:10)
Cr
log p(X n , Y n |θ)
Q(Y n |X n ) =
1
exp
(15)
r(θ |X n ) ,
CQ
1K (q(x)||p(x)) denotes the Kullback information from a distribution q(x) to a distri-
(cid:4)
bution p(x), that is,

(14)

K (q(x)||p(x)) =

q(x) log

q(x)
p(x)

dx.

where Cr and CQ are the normalization constants2 .

We deﬁne the stochastic complexity in the VB learning F (X n ) by the minimum
value of the functional F [q ] , that is ,
F (X n ) = min
r,Q
which shows the accuracy of the VB approach as an approximation of the Bayesian
learning. F (X n ) is also used for model selection since it gives an upper bound of
the true Bayesian stochastic complexity F (X n ).

F [q ],

i f (xi ),
yk

,

,

(16)

(17)

r(b) =

r(bk ) =

i , and νk =
yk

· bk − g(bk ))},

r(a) =
K(cid:3)

4.2 Variational Posterior for MEF Model
In this subsection, we derive the variational posterior r(θ|X n ) for the MEF model
based on (14) and then deﬁne the variational parameter for this model.
Using the complete data {X n , Y n} = {(x1 , y1), · · · , (xn , yn)}, we put
n(cid:1)
n(cid:1)
i = (cid:4)yk
(cid:5)Q(Y n ) , nk =
1
yk
i
nk
i=1
i=1
where yk
i = 1 if and only if the ith datum xi is from the kth component. The
variable nk is the expected number of the data that are estimated to be from the
kth component. From (14) and the respective prior (2) and (3), the variational
posterior r(θ) is obtained as the product of the following two distributions3 ,
K(cid:3)
(cid:11)
ank+φ0 −1
Γ(n + K φ0 )
k
K
k=1 Γ(nk + φ0)
K(cid:3)
k=1
exp{γk (µk
1
C (γk , µk )
k=1
k=1
where µk = nkνk+ξ0 ν0
and γk = nk + ξ0 . Let
nk+ξ0
ak = (cid:4)ak (cid:5)r(a) = nk + φ0
n + K φ0
bk = (cid:4)bk (cid:5)r(bk ) =
∂ log C (γk , µk )
1
,
γk
∂µk
and deﬁne the variational parameter θ by θ = (cid:4)θ(cid:5)r(θ) = {ak , bk }K
k=1. Then it is
noted that the variational posterior r(θ) and CQ in (15) are parameterized by the
variational parameter θ . Therefore, we denote them as r(θ|θ ) and CQ (θ) henceforth.
We deﬁne the variational estimator θvb by the variational parameter θ that attains
the minimum value of the stochastic complexity F (X n ). Then, putting (15) into
(12), we obtain
{K (r(θ|θ )||ϕ(θ)) − (log CQ(θ) + S(X n ))},
F (X n ) = min
θ
= K (r(θ|θ vb )||ϕ(θ)) − (log CQ (θvb ) + S(X n )),
where S(X n ) = − (cid:2)
n
i=1 log p0 (x).
Therefore, our aim is to evaluate the minimum value of (20) as a function of the
variational parameter θ .
2 (cid:1)·(cid:2)p(x) denotes the expectation over p(x).
3Hereafter, we omit the condition X n of the variational posteriors, and abbreviate them
to q(Y n , θ), Q(Y n ) and r(θ).

(18)

(19)

(20)

(21)

5 Main Result

The average stochastic complexity F (n) in the VB learning is deﬁned by
F (n) = EXn [F (X n )].

(22)

∗
k

k=1

ak exp{bk · f (x) + f0 (x) − g(bk )},

We assume the following conditions.
(i) The true distribution p0(x) is an MEF model p(x|θ0) which has K0 com-
ponents and the parameter θ0 = {a
}K0
∗
∗
k=1,
k , b
K0(cid:1)
k
p(x|θ0) =
k exp{b
k )},
· f (x) + f0 (x) − g(b
∗
∗
∗
a
k
k=1
(cid:6)= b
j (k (cid:6)= j ). And suppose that the model p(x|θ)
∈ RM and b
∗
∗
where b
k
has K components,
K(cid:1)
p(x|θ) =
and K ≥ K0 holds.
(ii) The prior distribution of the parameters is ϕ(θ) = ϕ(a)ϕ(b) given by (2)
and (3) with ϕ(b) bounded.
(iii) Regarding the distribution c(x|b) of each component, the Fisher information
∂ b∂ b satisﬁes 0 < |I (b)| < +∞,
for arbitrary b ∈ B 4 . The
2
g(b)
matrix I (b) = ∂
function µ · b − g(b) has a stationary point at ˆb in the interior of B for each
µ ∈ { ∂ g(b)
|b ∈ B}.
∂ b
Under these conditions, we prove the following.
Theorem 2 (Main Result) Assume the conditions (i),(ii) and (iii). Then the
(cid:8)
(cid:7)
average stochastic complexity F (n) deﬁned by (22) satisﬁes
+ C1 ≤ F (n) ≤ λ log n + C2 ,
nHn(θvb )
(23)
λ log n + EXn
(cid:12)
(cid:12)
for an arbitrary natural number n, where C1 , C2 are constants independent of n and
(φ0 ≤ M +1
(K − K0 )φ0 + M K0+K0 −1
(K − 1)φ0 + M
),
2 ,
M K+K−1
M K+K−1
2
2
(φ0 > M +1
).
,
2
2
2

(24)

λ =

λ =

This theorem shows the asymptotic form of the average stochastic complexity in
the Variational Bayesian learning. The coeﬃcients λ, λ of the leading terms are
identiﬁed by K ,K0 , that are the numbers of components of the learner and the true
distribution, the number of parameters M of each component and the hyperparam-
In this theorem, nHn(θvb ) = − (cid:2)
i=1 log p(xi |θvb )−S(X n ), and − (cid:2)
eter φ0 of the conjugate prior given by (2).
i=1 log p(xi |θ vb )
n
n
(cid:8)
(cid:7)
is a training error which is computable during the learning.
If the term
is a bounded function of n, then it immediately follows from this
nHn(θvb )
EXn
theorem that
λ log n + O(1) ≤ F 0(n) ≤ λ log n + O(1),
(j) and | · | denotes the determinant
2
g(b)
∂
(i)
∂ b
∂ b

2
g(b)
4 ∂
∂ b∂ b denotes the matrix whose ij th entry is
of a matrix.

where O(1) is a bounded function of n. In certain cases, such as binomial mixtures
and mixtures of von-Mises distributions, it is actually a bounded function of n. In
the case of gaussian mixtures, if B = RN , it is conjectured that the minus likelihood
ratio minθ nHn(θ), a lower bound of nHn(θvb ), is at most of the order of log log n[5].
Since the dimension of the parameter θ is M K + K − 1, the average stochastic
complexity of regular statistical models, which coincides with the Bayesian infor-
mation criterion (BIC)[9] is given by λBIC log n where λBIC = M K+K−1
. Theorem
2 claims that the coeﬃcient λ of log n is smaller than λBIC when φ0 ≤ (M + 1)/2.
2
This implies that the advantage of non-regular models in the Bayesian learning still
remains in the VB learning.
(Outline of the proof of Theorem 2)
From the condition (iii), calculating C (γk , µk ) in (17) by the saddle point approxi-
mation, K (r(θ|θ )||ϕ(θ)) in (20) is evaluated as follows 5 ,
K (r(θ|θ )||ϕ(θ)) = G(a) − K(cid:1)
k=1
where the function G(a) of a = {ak }K
k=1 is given by
G(a) = M K + K − 1
− (φ0 − 1
log n + { M
2
2
2

log ϕ(bk ) + Op (1),

log ak .

(25)

)} K(cid:1)
k=1

(26)

where

Then log CQ (θ) in (20) is evaluated as follows.
nHn(θ) + Op(1) ≤ −(log CQ (θ) + S(X n )) ≤ nH n (θ) + Op (1)
n(cid:1)
p(xi |θ0)
(cid:13)−
(cid:14) ,
(cid:2)
1
H n(θ) =
log
k=1 ak c(xi |¯bk ) exp
C (cid:1)
K
n
nk+min{φ0 ,ξ0 }
i=1
(cid:2) is a constant. Thus, from (20), evaluating the right-hand sides of (25) and
and C
(27) at speciﬁc points near the true parameter θ0 , we obtain the upper bound in
(cid:2)
(23). The lower bound in (23) is obtained from (25) and (27) by Jensen’s inequality
k=1 ak = 1. (Q.E.D)
K
and the constraint

(27)

6 Discussion and Conclusion

In this paper, we showed the upper and lower bounds of the stochastic complexity
for the mixture of exponential family models in the VB learning.
Firstly, we compare the stochastic complexity shown in Theorem 2 with the one
in the true Bayesian learning. On the mixture models with M parameters in each
component, the following upper bound for the coeﬃcient of F (n) in (10) is known
(cid:12)
[13],
(K + K0 − 1)/2
λ ≤
(M = 1),
(K − K0 ) + (M K0 + K0 − 1)/2 (M ≥ 2).
By the certain conditions about the prior distribution under which the above bound
was derived, we can compare the stochastic complexity when φ0 = 1. Putting φ0 = 1
in (24), we have
λ = K − K0 + (M K0 + K0 − 1)/2.
5Op (1) denotes a random variable bounded in probability.

(29)

(28)

Since we obtain F (n) (cid:1) λ log n+O(1) under certain assumptions[11], let us compare
λ of the VB learning to λ in (28) of the true Bayesian learning. When M = 1, that
is, each component has one parameter, λ ≥ λ holds since K0 ≤ K . This means
that the more redundant components the model has, the more the VB learning
In this case, 2λ is equal to the number
diﬀers from the true Bayesian learning.
of the parameters of the model. Hence the BIC[9] corresponds to λ log n when
If M ≥ 2, the upper bound of λ is equal to λ. This implies that the
M = 1.
variational posterior is close to the true Bayesian posterior when M ≥ 2. More
precise discussion about the accuracy of the approximation can be done for models
on which tighter bounds or exact values of the coeﬃcient λ in (10) are given[10].
Secondly, we point out that Theorem 2 shows how the hyperparameter φ0 inﬂu-
ence the process of the VB learning. The coeﬃcient λ in (24) indicates that only
when φ0 ≤ (M + 1)/2, the prior distribution (2) works to eliminate the redundant
components that the model has and otherwise it works to use all the components.
And lastly, let us give examples of how to use the theoretical bounds in (23). One
can examine experimentally whether the actual iterative algorithm converges to the
optimal variational posterior instead of local minima by comparing the stochastic
complexity with our theoretical result. The theoretical bounds would also enable us
to compare the accuracy of the VB learning with that of the Laplace approximation
or the MCMC method. As mentioned in Section 4, our result will be important for
developing eﬀective model selection methods using F (X n ) in the future work.

References

[1] H.Akaike, “Likelihood and Bayes procedure,” Bayesian Statistics, (Bernald J.M. eds.)
University Press, Valencia, Spain, pp.143-166, 1980.

[2] H.Attias, ”Inferring parameters and structure of latent variable models by variational
bayes,” Proc. of UAI, 1999.
[3] L.D.Brown, “Fundamentals of statistical exponential families,” IMS Lecture Notes-
Monograph Series, 1986.
[4] Z.Ghahramani, M.J.Beal, “Graphical models and variational methods,” Advanced
Mean Field Methods , MIT Press, 2000.
[5] J.A.Hartigan, “A Failure of likelihood asymptotics for normal mixtures,” Proc. of the
Berkeley Conference in Honor of J.Neyman and J.Kiefer, Vol.2, 807-810, 1985.
[6] D.J. Mackay, “Bayesian interpolation,” Neural Computation, 4(2), pp.415-447, 1992.
[7] G.McLachlan, D.Peel,”Finite mixture models,” Wiley, 2000.
[8] M.Sato, “Online model selection based on the variational bayes,” Neural Computation,
13(7), pp.1649-1681, 2001.
[9] G.Schwarz, “Estimating the dimension of a model,” Annals of Statistics, 6(2), pp.461-
464, 1978.

[10] K.Watanabe, S.Watanabe, ”Lower bounds of stochastic complexities in variational
bayes learning of gaussian mixture models,” Proc. of IEEE CIS04, pp.99-104, 2004.
[11] K.Watanabe, S.Watanabe, ”Stochastic complexity for mixture of exponential families
in variational bayes,” Proc. of ALT05, pp.107-121, 2005.
[12] S.Watanabe,“Algebraic analysis for non-identiﬁable learning machines,” Neural Com-
putation, 13(4), pp.899-933, 2001.
[13] K.Yamazaki, S.Watanabe, ”Singularities in mixture models and upper bounds of
stochastic complexity,” Neural Networks, 16, pp.1029-1038, 2003.


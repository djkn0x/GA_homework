A General and Efﬁcient Multiple Kernel
Learning Algorithm

S ¨oren Sonnenburg∗
Fraunhofer FIRST
Kekul ´estr. 7
12489 Berlin
Germany
sonne@first.fhg.de

Gunnar R ¨atsch
Friedrich Miescher Lab
Max Planck Society
Spemannstr. 39
T ¨ubingen, Germany
raetsch@tue.mpg.de

Christin Sch ¨afer
Fraunhofer FIRST
Kekul ´estr. 7
12489 Berlin
Germany
christin@first.fhg.de

Abstract

While classical kernel-based learning algorithms are based on a single
kernel, in practice it is often desirable to use multiple kernels. Lankriet
et al. (2004) considered conic combinations of kernel matrices for classi-
ﬁcation, leading to a convex quadratically constraint quad ratic program.
We show that it can be rewritten as a semi-inﬁnite linear prog ram that
can be efﬁciently solved by recycling the standard SVM imple menta-
tions. Moreover, we generalize the formulation and our method to a
larger class of problems, including regression and one-class classi ﬁca-
tion. Experimental results show that the proposed algorithm helps for
automatic model selection, improving the interpretability of the learn-
ing result and works for hundred thousands of examples or hundreds of
kernels to be combined.

1
Introduction
Kernel based methods such as Support Vector Machines (SVMs) have proven to be pow-
erful for a wide range of different data analysis problems. They employ a so-called kernel
function k(xi , xj ) which intuitively computes the similarity between two examples xi and
xj . The result of SVM learning is a α-weighted linear combination of kernel elements and
the bias b:
f (x) = sign   N
αi yik(xi , x) + b! ,
Xi=1
where the xi ’s are N labeled training examples (yi ∈ {±1}).
Recent developments in the literature on the SVM and other kernel methods have shown
the need to consider multiple kernels. This provides ﬂexibi
lity, and also reﬂects the fact
that typical learning problems often involve multiple, heterogeneous data sources. While
this so-called “multiple kernel learning” (MKL) problem ca n in principle be solved via
cross-validation, several recent papers have focused on more efﬁcient methods for multiple
kernel learning [4, 5, 1, 7, 3, 9, 2].

(1)

One of the problems with kernel methods compared to other techniques is that the resulting
decision function (1) is hard to interpret and, hence, is difﬁcult to use in order to extract rel-

∗For more details, datasets and pseudocode see http://www.fml.tuebingen.mpg.de
/raetsch/projects/mkl silp.

evant knowledge about the problem at hand. One can approach this problem by considering
convex combinations of K kernels, i.e.
K
Xk=1
(2)
βk kk (xi , xj )
k(xi , xj ) =
with βk ≥ 0 and Pk βk = 1, where each kernel kk uses only a distinct set of features
of each instance. For appropriately designed sub-kernels kk , the optimized combination
coefﬁcients can then be used to understand which features of
the examples are of impor-
tance for discrimination:
if one would be able to obtain an accurate classi ﬁcation by a
sparse weighting βk , then one can quite easily interpret the resulting decision function. We
will illustrate that the considered MKL formulation provides useful insights and is at the
same time is very efﬁcient. This is an important property mis sing in current kernel based
algorithms.

We consider the framework proposed by [7], which results in a convex optimization prob-
lem - a quadratically-constrained quadratic program (QCQP). This problem is more chal-
lenging than the standard SVM QP, but it can in principle be solved by general-purpose
optimization toolboxes. Since the use of such algorithms will only be feasible for small
problems with few data points and kernels, [1] suggested an algorithm based on sequential
minimization optimization (SMO) [10]. While the kernel learning problem is convex, it
is also non-smooth, making the direct application of simple local descent algorithms such
as SMO infeasible. [1] therefore considered a smoothed version of the problem to which
SMO can be applied.

In this work we follow a different direction: We reformulate the problem as a semi-inﬁnite
linear program (SILP), which can be efﬁciently solved using an off-the-shelf LP solver and
a standard SVM implementation (cf. Section 2 for details). Using this approach we are
able to solve problems with more than hundred thousand examples or with several hundred
kernels quite efﬁciently. We have used it for the analysis of
sequence analysis problems
leading to a better understanding of the biological problem at hand [16, 13]. We extend
our previous work and show that the transformation to a SILP works with a large class of
convex loss functions (cf. Section 3). Our column-generation based algorithm for solving
the SILP works by repeatedly using an algorithm that can efﬁc iently solve the single kernel
problem in order to solve the MKL problem. Hence, if there exists an algorithm that solves
the simpler problem efﬁciently (like SVMs), then our new alg orithm can efﬁciently solve
the multiple kernel learning problem.

We conclude the paper by illustrating the usefulness of our algorithms in several examples
relating to the interpretation of results and to automatic model selection.

2 Multiple Kernel Learning for Classiﬁcation using SILP
In the Multiple Kernel Learning (MKL) problem for binary classi ﬁcation one is given N
data points (xi , yi ) (yi ∈ {±1}), where xi is translated via a mapping Φk (x) 7→ RDk , k =
1 . . . K from the input into K feature spaces (Φ1 (xi ), . . . , ΦK (xi )) where Dk denotes
the dimensionality of the k-th feature space. Then one solves the following optimization
problem [1], which is equivalent to the linear SVM for K = 1:1
βk kwk k2!2
2   K
N
1
Xi=1
Xk=1
ξi
+ C
yi   K
k Φk (xi ) + b! ≥ 1 − ξi and
K
Xk=1
Xk=1
βkw⊤
1 [1] used a slightly different but equivalent (assuming tr(Kk ) = 1, k = 1, . . . , K ) formulation
without the β ’s, which we introduced for illustration.

min
wk ∈RDk ,ξ∈RN
+ ,β∈RK
+ ,b∈R

s.t.

(3)

βk = 1.

Note that the ℓ1 -norm of β is constrained to one, while one is penalizing the ℓ2 -norm of
wk in each block k separately. The idea is that ℓ1 -norm constrained or penalized variables
tend to have sparse optimal solutions, while ℓ2 -norm penalized variables do not [11]. Thus
the above optimization problem offers the possibility to ﬁn d sparse solutions on the block
level with non-sparse solutions within the blocks.

γ

s.t.

min
γ∈R,1C≥α∈RN
+

Bach et al. [1] derived the dual for problem (3), which can be equivalently written as:
N
N
1
Xi=1
Xi,j=1
≤ γ and Xi=1
αi
αiαj yi yj kk (xi , xj ) −
2
|
{z
}
=:Sk (α)
for k = 1, . . . , K , where kk (xi , xj ) = (Φk (xi ), Φk (xj )). Note that we have one quadratic
constraint per kernel (Sk (α) ≤ γ ). In the case of K = 1, the above problem reduces to the
original SVM dual.

αi yi = 0 (4)

max
θ∈R,β∈RM
+

In order to solve (4), one may solve the following saddle point problem (Lagrangian):
K
Xk=1
(5)
βk (Sk (α) − γ )
L := γ +
+ , γ ∈ R (subject to α ≤ C 1 and Pi αi yi = 0) and maximized
minimized w.r.t. α ∈ RN
+ . Setting the derivative w.r.t. to γ to zero, one obtains the constraint Pk βk =
w.r.t. β ∈ RK
1 and (5) simpli ﬁes to: L = S (α, β) := PK
k=1 βk Sk (α) and leads to a min-max problem:
K
N
K
Xk=1
Xi=1
Xk=1
(6)
αi yi = 0 and
s.t.
min
max
βk = 1.
βk Sk (α)
1C≥α∈RN
β∈Rk
+
+
Assume α∗ would be the optimal solution, then θ∗ := S (α∗ , β) is minimal and, hence,
S (α, β) ≥ θ∗ for all α (subject to the above constraints). Hence, ﬁnding a saddle- point of
(5) is equivalent to solving the following semi-inﬁnite lin ear program:
K
Xk=1
s.t. Xk
βk = 1 and
θ
βk Sk (α) ≥ θ
for all α with 0 ≤ α ≤ C 1 and Xi
Note that this is a linear program, as θ and β are only linearly constrained. However
there are inﬁnitely many constraints: one for each α ∈ RN satisfying 0 ≤ α ≤ C and
PN
i=1 αi yi = 0. Both problems (6) and (7) have the same solution. To illustrate that,
consider β is ﬁxed and we maximize α in (6). Let α∗ be the solution that maximizes (6).
Then we can decrease the value of θ in (7) as long as no α-constraint (7) is violated, i.e.
down to θ = PK
k=1 βk Sk (α∗ ). Similarly, as we increase θ for a ﬁxed α the maximizing β
is found. We will discuss in Section 4 how to solve such semi inﬁnite linear programs.
3 Multiple Kernel Learning with General Cost Functions
In this section we consider the more general class of MKL problems, where one is given
an arbitrary strictly convex differentiable loss function, for which we derive its MKL SILP
formulation. We will then investigate in this general MKL SILP using different loss func-
tions, in particular the soft-margin loss, the ǫ-insensitive loss and the quadratic loss.
We deﬁne the MKL primal formulation for a strictly convex and differentiable loss function
L as: (for simplicity we omit a bias term)
kwk k!2
2   K
N
1
Xi=1
Xk=1

(Φk (xi ), wk )

K
Xk=1

L(f (xi ), yi )

min
wk ∈RDk

yiαi = 0

(7)

+

s.t.

f (xi ) =

(8)

min
γ∈R,α∈RN

2

(9)

2

.

Sk (α) = −

αi yi +

s.t. :

s.t.

max
θ∈R,β∈RK

θ

αiL′−1 (αi , yi )

where Sk (α) = −

In analogy to [1] we treat problem (8) as a second order cone program (SOCP)
(see Supplementary Website or
leading to the following dual
for details):
[17]
N
N
Xi=1
Xi=1
L(L′−1 (αi , yi ), yi ) +
γ −
2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
αiΦk (xi )(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
N
1
Xi=1
≤ γ , ∀k = 1 . . . K
2
To derive the SILP formulation we follow the same recipe as in Section 2: deriving the La-
grangian leads to a max-min problem formulation to be eventually reformulated to a SILP:
K
Xk=1
and PK
k=1 βk Sk (α) ≥ θ , ∀α ∈ RN ,
βk = 1
αiΦk (xi )(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
N
N
N
1
Xi=1
Xi=1
Xi=1
αiL′−1 (αi , yi ) +
L(L′−1 (αi , yi ), yi ) +
2
We assumed that L(x, y) is strictly convex and differentiable in x. Unfortunately, the soft
margin and ǫ-insensitive loss do not have these properties. We therefore consider them
separately in the sequel.
Soft Margin Loss We use the following loss in order to approximate the soft margin
loss: Lσ (x, y) = C
σ log(1 + exp((1 − xy)σ)). It is easy to verify that limσ→∞ Lσ (x, y) =
C (1 − xy)+ . Moreover, Lσ is strictly convex and differentiable for σ < ∞. Using this loss
and assuming yi ∈ {±1}, we obtain :
2 ‚‚‚‚‚
αiΦk (xi )‚‚‚‚‚
N
N
N
σ „log „ C yi
αi + C yi «« +
αi + C yi « + log „−
C
αi
1
Xi=1
Xi=1
Xi=1
2
If σ → ∞, then the ﬁrst two terms vanish provided that −C ≤ αi ≤ 0 if yi = 1 and
0 ≤ αi ≤ C if yi = −1. Substituting α = − ˜αi yi , we then obtain Sk ( ˜α) = − PN
i=1 ˜αi +
2 (cid:13)(cid:13)(cid:13)PN
i=1 ˜αi yiΦk (xi )(cid:13)(cid:13)(cid:13)
2
1
, with 0 ≤ ˜αi ≤ C (i = 1, . . . , N ), which is very similar to (4):
2
only the Pi αi yi = 0 constraint is missing, since we omitted the bias.
One-Class Soft Margin Loss The one-class SVM soft margin (e.g. [15]) is very similar
2 (cid:13)(cid:13)(cid:13)PN
i=1 αiΦk (xi )(cid:13)(cid:13)(cid:13)
2
to the two class case and leads to Sk (α) = 1
subject to 0 ≤ α ≤ 1
νN
2
and PN
i=1 αi = 1.
ǫ-insensitive Loss Using the same technique for the epsilon insensitive loss L(x, y) =
C (1 − |x − y |)+ , we obtain
2
2 (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i )Φk (xi )(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
N
N
N
1
Xi=1
Xi=1
Xi=1
(αi − α∗
2
with 0 ≤ α, α∗ ≤ C 1. When including a bias term, we additionally have the constraint
PN
i=1 (αi − α∗
i )yi = 0.
It is straightforward to derive the dual problem for other loss functions such as the quadratic
loss. Note that the dual SILP’s only differ in the deﬁnition o f Sk and the domains of the
α’s.
4 Algorithms to solve SILPs
The SILPs considered in this work all have the following form:
s.t. PK
k=1 βk = 1 and PM
k=1 βk Sk (α) ≥ θ for all α ∈ C (10)
max
θ
θ∈R,β∈RM
+

(αi + α∗
i )ǫ −

(αi − α∗
i )yi ,

2

.

1

Sk (α, α∗ ) =

−

(11)

for some appropriate Sk (α) and the feasible set C ⊆ RN of α depending on the choice of
the cost function. Using Theorem 5 in [12] one can show that the above SILP has a solution
if the corresponding primal is feasible and bounded. Moreover, there is no duality gap, if
M = co{[S1 (α), . . . , SK (α)]⊤ | α ∈ C } is a closed set. For all loss functions considered
in this paper this holds true. We propose to use a technique called Column Generation to
solve (10). The basic idea is to compute the optimal (β , θ) in (10) for a restricted subset of
constraints. It is called the restricted master problem. Then a second algorithm generates
a new constraint determined by α. In the best case the other algorithm ﬁnds the constraint
that maximizes the constraint violation for the given intermediate solution (β , θ), i.e.
α∈C Xk
αβ := argmin
βk Sk (α).
If αβ satis ﬁes the constraint PK
k=1 βk Sk (αβ ) ≥ θ , then the solution is optimal. Other-
wise, the constraint is added to the set of constraints.
Algorithm 1 is a special case of the set of SILP algorithms known as exchange methods.
These methods are known to converge (cf. Theorem 7.2 in [6]). However, no convergence
rates for such algorithm are so far known.2 Since it is often sufﬁcient to obtain an approxi-
mate solution, we have to deﬁne a suitable convergence crite rion. Note that the problem is
solved when all constraints are satis ﬁed. Hence, it is a natu ral choice to use the normalized
maximal constraint violation as a convergence criterion, i.e. ǫ := (cid:12)(cid:12)(cid:12)1 − PK
(cid:12)(cid:12)(cid:12) ,
k=1 β t
k Sk (αt )
θt
where (β t , θt ) is the optimal solution at iteration t − 1 and αt corresponds to the newly
found maximally violating constraint of the next iteration.
We need an algorithm to identify unsatis ﬁed constraints, wh ich, fortunately, turns out to be
particularly simple. Note that (11) is for all considered cases exactly the dual optimization
problem of the single kernel case for ﬁxed β . For instance for binary classi ﬁcation, (11)
reduces to the standard SVM dual using the kernel k(xi , xj ) = Pk βk kk (xi , xj ):
N
N
N
Xi=1
Xi=1
Xi,j=1
with
0 ≤ α ≤ C 1 and
αi yi = 0.
αi
αiαj yi yj k(xi , xj ) −
min
α∈RN
We can therefore use a standard SVM implementation in order to identify the most violated
constraint. Since there exist a large number of efﬁcient alg orithms to solve the single
kernel problems for all sorts of cost functions, we have therefore found an easy way to
extend their applicability to the problem of Multiple Kernel Learning. In some cases it
is possible to extend existing SMO based implementations to simultaneously optimize β
and α. In [16] we have considered such an algorithm for the binary classi ﬁcation case
that frequently recomputes the β ’s.3 Empirically it is a few times faster than the column
generation algorithm, but it is on the other hand much harder to implement.
5 Experiments
In this section we will discuss toy examples for binary classi ﬁcation and regression, demon-
strating that MKL can recover information about the problem at hand, followed by a brief
review on problems for which MKL has been successfully used.
5.1 Classi ﬁcations
In Figure 1 we consider a binary classi ﬁcation problem, wher e we used MKL-SVMs with
ﬁve RBF-kernels with different widths, to distinguish the d ark star-like shape from the

2 It has been shown that solving semi-inﬁnite problems like (7), using a meth od related to boosting
(e.g. [8]) one requires at most T = O(log(M )/ˆǫ2 ) iterations, where ˆǫ is the remaining constraint
violation and the constants may depend on the kernels and the number of examples N [11, 14]. At
least for not too small values of ˆǫ this technique produces reasonably fast good approximate solutions.
3Simplex based LP solvers often offer the possibility to efﬁcient restart the
computation when
adding only a few constraints.

Algorithm 1 The column generation algorithm employs a linear programming solver to
iteratively solve the semi-inﬁnite linear optimization pr oblem (10). The accuracy parameter
ǫ is a parameter of the algorithm. Sk (α) and C are determined by the cost function.
K for k = 1, . . . , K
S 0 = 1, θ1 = −∞, β 1
k = 1
for t = 1, 2, . . . do

Compute α

t = argmin
α∈C

K
Xk=1

β t
k Sk (α) by single kernel algorithm with K =

K
Xk=1

β t
kKk

S t =

t )

β t
k Sk (α

K
Xk=1
S t
θt | ≤ ǫ then break
if |1 −
(β t+1 , θt+1 ) = argmax θ

w.r.t. β ∈ R

K
+ , θ ∈ R with

end for

K
Xk=1

βk = 1 and

K
Xk=1

βk S r
k ≥ θ for r = 1, . . . , t

light star. (The distance between the stars increases from left to right.) Shown are the
obtained kernel weightings for the ﬁve kernels and the test e rror which quickly drops to
zero as the problem becomes separable. Note that the RBF kernel with largest width was
not appropriate and thus never chosen. Also with increasing distance between the stars
kernels with greater widths are used. This illustrates that MKL one can indeed recover
such tendencies.

5.2 Regression

We applied the newly derived MKL support vector regression formulation, to the task of
learning a sine function using three RBF-kernels with different widths. We then increased
the frequency of the sine wave. As can be seen in Figure 2, MKL-SV regression abruptly
switches to the width of the RBF-kernel ﬁtting the regressio n problem best. In another
regression experiment, we combined a linear function with two sine waves, one of lower
frequency and one of high frequency, i.e. f (x) = c · x + sin(ax) + sin(bx). Using ten
RBF-kernels of different width (see Figure 3) we trained a MKL-SVR and display the
learned weights (a column in the ﬁgure). The largest selecte d width (100) models the linear
component (since RBF with large widths are effectively linear) and the medium width (1)
corresponds to the lower frequency sine. We varied the frequency of the high frequency
sine wave from low to high (left to right in the ﬁgure). One obs erves that MKL determines

Figure 1: A 2-class toy problem where the dark grey star-like shape is to be distinguished
from the light grey star inside of the dark grey star. For details see text.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t
h
g
i
e
w
 
l
e
n
r
e
k

width 0.005
width 0.05
width 0.5
width 1
width 10

0

0

2
frequency
Figure 2: MKL-Support Vector Regression for the task of learning a sine wave (please see
text for details).

1

3

4

5

an appropriate combination of kernels of low and high widths, while decreasing the RBF-
kernel width with increased frequency. This shows that MKL can be more powerful than
cross-validation: To achieve a similar result with cross-validation one has to use 3 nested
loops to tune 3 RBF-kernel sigmas, e.g. train 10 · 9 · 8/6 = 120 SVMs, which in preliminary
experiments was much slower then using MKL (800 vs. 56 seconds).

h
t
d
i
w
 
l
e
n
r
e
k
 
F
B
R

0.001

0.005

0.01

0.05

0.1

1

10

50

100

1000

0.7

0.6

0.5

0.4

0.3

0.2

0.1

6

2

4

10
12
frequency
Figure 3: MKL support vector regression on a linear combination of three functions:
f (x) = c · x + sin(ax) + sin(bx). MKL recovers that the original function is a com-
bination of functions of low and high complexity. For more details see text.

14

16

18

20

8

0

5.3 Applications in the Real World

MKL has been successfully used on real-world datasets in the ﬁeld of computational biol-
ogy [7, 16]. It was shown to improve classi ﬁcation performan ce on the task of ribosomal
and membrane protein prediction, where a weighting over different kernels each corre-
sponding to a different feature set was learned. Random channels obtained low kernel
weights. Moreover, on a splice site recognition task we used MKL as a tool for interpreting
the SVM classi ﬁer [16], as is displayed in Figure 4. Using spe ci ﬁcally optimized string
kernels, we were able to solve the classi ﬁcation MKL SILP for N = 1.000.000 examples
and K = 20 kernels, as well as for N = 10.000 examples and K = 550 kernels.

0.05

0.045

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

0
−50

−20

−30

−40

−10 Exon
Start
Figure 4: The ﬁgure shows an importance weighting for each po sition in a DNA sequence
(around a so called splice site). MKL was used to learn these weights, each corresponding
to a sub-kernel which uses information at that position to discriminate true splice sites from
fake ones. Different peaks correspond to different biologically known signals (see [16] for
details). We used 65.000 examples for training with 54 sub-kernels.

+50

+10

+20

+30

+40

6 Conclusion
We have proposed a simple, yet efﬁcient algorithm to solve th e multiple kernel learning
problem for a large class of loss functions. The proposed method is able to exploit the
existing single kernel algorithms, whereby extending their applicability. In experiments we
have illustrated that the MKL for classi ﬁcation and regress ion can be useful for automatic
model selection and for obtaining comprehensible information about the learning problem
at hand. It is future work to evaluate MKL algorithms for unsupervised learning such as
Kernel PCA and one-class classi ﬁcation.

Acknowledgments
The authors gratefully acknowledge partial support from the PASCAL Network of Ex-
cellence (EU #506778), DFG grants JA 379 / 13-2 and MU 987/2-1. We thank Guido
Dornhege, Olivier Chapelle, Olaf Weiss, Joaquin Qui ˜no ˜nero Candela, Sebastian Mika and
K.-R. M ¨uller for great discussions.
References
[1] Francis R. Bach, Gert R. G. Lanckriet, and Michael I. Jordan. Multiple kernel learning, conic
duality, and the SMO algorithm. In Twenty- ﬁrst international conference on Machine learning .
ACM Press, 2004.
[2] Kristin P. Bennett, Michinari Momma, and Mark J. Embrechts. Mark: a boosting algorithm for
heterogeneous kernel models. KDD, pages 24–31, 2002.
[3] Jinbo Bi, Tong Zhang, and Kristin P. Bennett. Column-generation boosting methods for mixture
of kernels. KDD, pages 521–526, 2004.
[4] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for
support vector machines. Machine Learning, 46(1-3):131–159, 2002.
[5] I. Grandvalet and S. Canu. Adaptive scaling for feature selection in SVMs. In In Advances in
Neural Information Processing Systems, 2002.
[6] R. Hettich and K.O. Kortanek. Semi-inﬁnite programming: Theory, m ethods and applications.
SIAM Review, 3:380–429, September 1993.
[7] G.R.G. Lanckriet, T. De Bie, N. Cristianini, M.I. Jordan, and W.S. Noble. A statistical framework
for genomic data fusion. Bioinformatics, 2004.
[8] R. Meir and G. R ¨atsch. An introduction to boosting and leveraging.
In S. Mendelson and
A. Smola, editors, Proc. of the ﬁrst Machine Learning Summer School in Canberra , LNCS,
pages 119–184. Springer, 2003. in press.
[9] C.S. Ong, A.J. Smola, and R.C. Williamson. Hyperkernels. In In Advances in Neural Information
Processing Systems, volume 15, pages 495–502, 2003.
[10] J. Platt. Fast training of support vector machines using sequential minimal optimization. In
B. Sch ¨olkopf, C.J.C. Burges, and A.J. Smola, editors, Advances in Kernel Methods — Support
Vector Learning, pages 185–208, Cambridge, MA, 1999. MIT Press.
[11] G. R ¨atsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam,
Computer Science Dept., August-Bebel-Str. 89, 14482 Potsdam, Germany, 2001.
[12] G. R ¨atsch, A. Demiriz, and K. Bennett. Sparse regression ensembles in inﬁn ite and ﬁnite hy-
pothesis spaces. Machine Learning, 48(1-3):193–221, 2002. Special Issue on New Methods for
Model Selection and Model Combination. Also NeuroCOLT2 Technical Report NC-TR-2000-
085.
[13] G. R ¨atsch, S. Sonnenburg, and C. Sch ¨afer. Learning interpretable svms for biological sequence
classi ﬁcation. BMC Bioinformatics, Special Issue from NIPS workshop on New Problems and
Methods in Computational Biology Whistler, Canada, 18 December 2004, 7(Suppl. 1:S9), Febru-
ary 2006.
[14] G. R ¨atsch and M.K. Warmuth. Marginal boosting. NeuroCOLT2 Technical Report 97, Royal
Holloway College, London, July 2001.
[15] B. Sch ¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[16] S. Sonnenburg, G. R ¨atsch, and C. Sch ¨afer. Learning interpretable SVMs for biological se-
quence classiﬁcation. In RECOMB 2005, LNBI 3500, pages 389–407. Springer-Verlag Berlin
Heidelberg, 2005.
[17] S. Sonnenburg, G. R ¨atsch, S. Sch ¨afer, and B. Sch ¨olkopf. Large scale multiple kernel learning.
Journal of Machine Learning Research, 2006. accepted.


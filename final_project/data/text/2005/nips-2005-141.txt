Generalization to Unseen Cases

Teemu Roos
Helsinki Institute for Information Technology
P.O.Box 68, 00014 Univ. of Helsinki, Finland
teemu.roos@cs.helsinki.fi

Peter Gr ¨unwald
CWI, P.O.Box 94079, 1090 GB,
Amsterdam, The Netherlands
pdg@cwi.nl

Petri Myllym ¨aki
Helsinki Institute for Information Technology
P.O.Box 68, 00014 Univ of Helsinki, Finland
petri.myllymaki@cs.helsinki.fi

Henry Tirri
Nokia Research Center
P.O.Box 407 Nokia Group, Finland
henry.tirri@nokia.com

Abstract

We analyze classi(cid:2)cation error on unseen cases, i.e. cases that are differ-
ent from those in the training set. Unlike standard generalization error,
this off-training-set error may differ signi(cid:2)cantly from the empirical er-
ror with high probability even with large sample sizes. We derive a data-
dependent bound on the difference between off-training-set and standard
generalization error. Our result is based on a new bound on the missing
mass, which for small samples is stronger than existing bounds based
on Good-Turing estimators. As we demonstrate on UCI data-sets, our
bound gives nontrivial generalization guarantees in many practical cases.
In light of these results, we show that certain claims made in the No Free
Lunch literature are overly pessimistic.

1 Introduction

A large part of learning theory deals with methods that bound the generalization error of
hypotheses in terms of their empirical errors. The standard de(cid:2)nition of generalization
error allows overlap between the training sample and test cases. When such overlap is
not allowed, i.e., when considering off-training-set error [1](cid:150)[5] de(cid:2)ned in terms of only
previously unseen cases, usual generalization bounds do not apply. The off-training-set
error and the empirical error sometimes differ signi(cid:2)cantly with high probability even for
large sample sizes. In this paper, we show that in many practical cases, one can nevertheless
bound this difference. In particular, we show that with high probability, in the realistic
situation where the number of repeated cases, or duplicates, relative to the total sample size
is small, the difference between the off-training-set error and the standard generalization
error is also small. In this case any standard generalization error bound, no matter how it is
arrived at, transforms into a similar bound on the off-training-set error.

Our Contribution We show that with probability at least 1(cid:0)(cid:14) , if there are r repetitions in
the training sample, then the difference between the off-training-set error and the standard
generalization error is at most of order O (cid:16)q 1
(cid:14) + r log n(cid:1)(cid:17) (Thm. 2). Our main
n (cid:0)log 4

result (Corollary 1 of Thm. 1) gives a stronger non-asymptotic bound that can be evaluated
numerically. The proof of Thms. 1 and 2 is based on Lemma 2, which is of independent
interest, giving a new lower bound on the so-called missing mass, the total probability of as
yet unseen cases. For small samples and few repetitions, this bound is signi(cid:2)cantly stronger
than existing bounds based on Good-Turing estimators [6](cid:150)[8].
Properties of Our Bounds Our bounds hold (1) uniformly, are (2) distribution-free and
(3) data-dependent, yet (4) relevant for data-sets encountered in practice. Let us consider
these properties in turn. Our bounds hold uniformly in that they hold for all hypotheses
(functions from features to labels) at the same time. Thus, unlike many bounds on standard
generalization error, our bounds do not depend in any way on the richness of the hypothesis
class under consideration measured in terms of, for instance, its VC dimension, or the
margin of the selected hypothesis on the training sample, or any other property of the
mechanism with which the hypothesis is chosen. Our bounds are distribution-free in that
they hold no matter what the (unknown) data-generating distribution is. Our bounds depend
on the data: they are useful only if the number of repetitions in the training set is very small
compared to the training set size. However, in machine learning practice this is often the
case as demonstrated in Sec. 3 with several UCI data-sets.
Relevance Why are our results interesting? There are at least three reasons, the (cid:2)rst two of
which we discuss extensively in Sec. 4: (1) The use of off-training-set error is an essential
ingredient of the No Free Lunch (NFL) theorems [1](cid:150)[5]. Our results counter-balance some
of the overly pessimistic conclusions of this work. This is all the more relevant since the
NFL theorems have been quite in(cid:3)uential in shaping the thinking of both theoretical and
practical machine learning researchers (see, e.g., Sec. 9.2 of the well-known textbook [5]).
(2) The off-training-set error is an intuitive measure of generalization performance. Yet in
practice it differs from standard generalization error (even with continuous feature spaces).
Thus, we feel, it is worth studying. (3) Technically, we establish a surprising connection
between off-training-set error (a concept from classi(cid:2)cation) and missing mass (a concept
mostly applied in language modeling), and give a new lower bound on the missing mass.
The paper is organized as follows: In Sec. 2 we (cid:2)x notation, including the various error
functionals considered, and state some preliminary results. In Sec. 3 we state our bounds,
and we demonstrate their use on data-sets from the UCI machine learning repository. We
discuss the implications of our results in Sec. 4. Postponed proofs are in Appendix A.

2 Preliminaries and Notation

Let X be an arbitrary space of inputs, and let Y be a discrete space of labels. A learner
observes a random training sample, D , of size n, consisting of the values of a sequence
of input(cid:150)label pairs ((X1 ; Y1 ); :::; (Xn ; Yn )); where (Xi ; Yi ) 2 X (cid:2) Y . Based on the
sample, the learner outputs a hypothesis h : X ! Y that gives, for each possible input
value, a prediction of the corresponding label. The learner is successful if the produced
hypothesis has high probability of making a correct prediction when applied to a test case.
(Xn+1 ; Yn+1 ). Both the training sample and the test case are independently drawn from a
common generating distribution P (cid:3) . We use the following error functionals:
Deﬁnition 1 (errors). Given a training sample D of size n, the i.i.d., off-training-set, and
empirical error of a hypothesis h are given by
i.i.d. error;
Eiid (h)
:= Pr[Y 6= h(X )]
off-training-set error;
:= Pr[Y 6= h(X ) j X =2 XD ]
Eots (h; D)
n Pn
:= 1
empirical error;
Eemp (h; D)
Ifh(Xi )6=Yi g
i=1
where XD is the set of X -values occurring in sample D , and the indicator function If(cid:1)g
takes value one if its argument is true and zero otherwise.

The (cid:2)rst one of these is just the standard generalization error of learning theory. Following
[2], we call it i.i.d. error. For general input spaces and generating distributions Eots (h; D)
may be unde(cid:2)ned for some D . In either case, this is not a problem. First, if XD has measure
one, the off-training-set error is unde(cid:2)ned and we need not concern ourselves with it; the
relevant error measure is Eiid (h) and standard results apply1 . If, on the other hand, XD has
measure zero, the off-training-set error and the i.i.d. error are equivalent and our results (in
Sec. 3 below) hold trivially. Thus, if off-training-set error is relevant, our results hold.
Deﬁnition 2. Given a training sample D , the sample coverage p(XD ) is the probability
that a new X -value appears in D: p(XD ) := Pr[X 2 XD ], where XD is as in Def. 1. The
remaining probability, 1 (cid:0) p(XD ), is called the missing mass.
Lemma 1. For any training set D such that Eots (h; D) is deﬁned, we have
a)
jEots (h; D) (cid:0) Eiid (h)j (cid:20) p(XD ) ;
p(XD )
b) Eots (h; D) (cid:0) Eiid (h) (cid:20)
1 (cid:0) p(XD ) Eiid (h) :
Proof. Both bounds follow essentially from the following inequalities2 :
Pr[X =2 XD ] ^ 1 = Eiid (h)
Pr[Y 6= h(X )]
Pr[Y 6= h(X ); X =2 XD ]
(cid:20)
1 (cid:0) p(XD ) ^ 1
Eots (h; D) =
Pr[X =2 XD ]
= (cid:18) Eiid (h)
1 (cid:0) p(XD ) ^ 1(cid:19) (1 (cid:0) p(XD )) + (cid:18) Eiid (h)
1 (cid:0) p(XD ) ^ 1(cid:19) p(XD )
(cid:20) Eiid (h) + p(XD ) ;
where ^ denotes the minimum. This gives one direction of Lemma 1.a (an upper bound on
Eots (h; D)); the other direction is obtained by using analogous inequalities for the quantity
1 (cid:0) Eots (h; D), with Y 6= h(X ) replaced by Y = h(X ), which gives the upper bound
1 (cid:0) Eots (h; D) (cid:20) 1 (cid:0) Eiid (h) + p(XD ). Lemma 1.b follows from the (cid:2)rst line by ignoring
the upper bound 1, and subtracting Eiid (h) from both sides.
Given the value of (or an upper bound on) Eiid (h), the upper bound of Lemma 1.b may
be signi(cid:2)cantly stronger than that of Lemma 1.a. However, in this work we only use
Lemma 1.a for simplicity since it depends on p(XD ) alone. The lemma would be of little
use without a good enough upper bound on the sample coverage p(XD ), or equivalently, a
lower bound on the missing mass. In the next section we obtain such a bound.

3 An Off-training-set Error Bound

Good-Turing estimators [6], named after Irving J. Good, and Alan Turing, are widely used
in language modeling to estimate the missing mass. The known small bias of such estima-
tors, together with a rate of convergence, can be used to obtain lower and upper bound for
the missing mass [7, 8]. Unfortunately, for the sample sizes we are interested in, the lower
bounds are not quite tight enough (see Fig. 1 below). In this section we state a new lower
bound, not based on Good-Turing estimators, that is practically useful in our context. We
compare this bound to the existing ones after Thm. 2.
Let (cid:22)Xn (cid:26) X be the set consisting of the n most probable individual values of X . In case
there are several such subsets any one of them will do. In case X has less than n elements,
(cid:22)Xn := X . Denote for short (cid:22)pn := Pr[X 2 (cid:22)Xn ]. No assumptions are made regarding the
value of (cid:22)pn , it may or may not be zero. The reason for us being interested in (cid:22)pn is that
1Note however, that a continuous feature space does not necessarily imply this, see Sec. 4.
2This neat proof is due to Gilles Blanchard (personal communication).

it gives us an upper bound p(XD ) (cid:20) (cid:22)pn on the sample coverage that holds for all D . We
prove that when (cid:22)pn is large it is likely that a sample of size n will have several repeated X -
values so that the number of distinct X -values is less than n. This implies that if a sample
with a small number of repeated X -values is observed, it is safe to assume that (cid:22)pn is small
and therefore, the sample coverage p(XD ) must also be small.
Lemma 2. The probability of obtaining a sample of size n (cid:21) 1 with at most 0 (cid:20) r < n
repeated X -values is upper-bounded by Pr[(cid:147)at most r repetitions(cid:148)] (cid:20) (cid:1)(n; r; (cid:22)pn ) ; where
n
Xk=0 (cid:18)n
k(cid:19) (cid:22)pk
(1)
n (1 (cid:0) (cid:22)pn )n(cid:0)k f (n; r; k)
(cid:1)(n; r; (cid:22)pn ) :=
if k < r
:= (1
and f (n; r; k) is given by f (n; r; k)
(n(cid:0)k+r)! n(cid:0)(k(cid:0)r) ; 1(cid:17) if k (cid:21) r :
min (cid:16)(cid:0)k
n!
r (cid:1)
(cid:1)(n; r; (cid:22)pn ) is a non-increasing function of (cid:22)pn .
For a proof, see Appendix A. Given a (cid:2)xed con(cid:2)dence level 1 (cid:0) (cid:14) we can now de(cid:2)ne a
data-dependent upper bound on the sample coverage
(2)
B((cid:14); D) := arg min
p fp : (cid:1)(n; r; p) (cid:20) (cid:14)g ;
where r is the number of repeated X -values in D , and (cid:1)(n; r; p) is given by Eq. (1).
Theorem 1. For any 0 (cid:20) (cid:14) (cid:20) 1, the upper bound B((cid:14); D) on the sample coverage given
by Eq. (2) holds with at least probability 1 (cid:0) (cid:14) :
Pr [p(XD ) (cid:20) B((cid:14); D)] (cid:21) 1 (cid:0) (cid:14) :
Proof. Consider (cid:2)xed values of the con(cid:2)dence level 1 (cid:0) (cid:14) , sample size n, and probability
(cid:22)pn . Let R be the largest integer for which (cid:1)(n; R; (cid:22)pn ) (cid:20) (cid:14) . By Lemma 2 the probability of
obtaining at most R repetitions is upper-bounded by (cid:14) . Thus, it is suf(cid:2)cient that the bound
holds whenever the number of repetitions is greater than R. For any such r > R, we have
(cid:1)(n; r; (cid:22)pn ) > (cid:14) . By Lemma 2 the function (cid:1)(n; r; (cid:22)pn ) is non-increasing in (cid:22)pn , and hence
it must be that (cid:22)pn < arg minp fp : (cid:1)(n; r; p) (cid:20) (cid:14)g = B((cid:14); D). Since p(XD ) (cid:20) (cid:22)pn , the
bound then holds for all r > R.
Rather than the sample coverage p(XD ), the real interest is often in off-training-set error.
Using the relation between the two quantities, one gets the following corollary that follows
directly from Lemma 1.a and Thm. 1.
Corollary 1 (main result: off-training-set error bound). For any 0 (cid:20) (cid:14) (cid:20) 1, the differ-
ence between the i.i.d. error and the off-training-set error is bounded by
Pr [8h jEots (h; D) (cid:0) Eiid (h)j (cid:20) B((cid:14); D)] (cid:21) 1 (cid:0) (cid:14) :
Corollary 1 implies that the off-training-set error and the i.i.d. error are entangled, thus
transforming all distribution-free bounds on the i.i.d. error to similar bounds on the off-
training-set error. Since the probabilistic part of the result (Lemma 1) does not involve a
speci(cid:2)c hypothesis, Corollary 1 holds for all hypotheses at the same time, and does not
depend on the richness of the hypothesis class in terms of, for instance, its VC dimension.
Figure 1 illustrates the behavior of the bound (2) as the sample size grows. It can be seen
that for a small number of repetitions the bound is nontrivial already at moderate sample
sizes. Moreover, the effect of repetitions is tolerable, and it diminishes as the number of
repetitions grows. Table 1 lists values of the bound for a number of data-sets from the UCI
machine learning repository [9]. In many cases the bound is about 0.10(cid:150)0.20 or less.
Theorem 2 gives an upper bound on the rate with which the bound decreases as n grows.

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

)
D
;
(cid:14)
(
B

PSfrag replacements

G

-

T

r
=
1
0

r
=
1

r=
0

 1

 10

 1000

 100
sample size
Figure 1: Upper bound B((cid:14); D) given by Eq. (2) for samples with zero (r = 0) to ten
(r = 10) repeated X -values on the 95 % con(cid:2)dence level ((cid:14) = 0:05). The dotted curve
is an asymptotic version for r = 0 given by Thm. 2. The curve labeled ‘G-T’ (for r = 0)
is based on Good-Turing estimators (Thm. 3 in [7]). Asymptotically, it exceeds our r = 0
bound by a factor O(log n). Bound for the UCI data-sets in Table 1 are marked with small
triangles (5). Note the log-scale for sample size.

 10000

Theorem 2 (a weaker bound in closed-form). For all n and all (cid:22)pn , all r < n, the function
B((cid:14); D) has the upper bound B((cid:14); D) (cid:20) 3q 1
2n (cid:0)log 4
(cid:14) + 2r log n(cid:1):
For a proof, see Appendix A. Let us compare Thm. 2 to the existing bounds on B((cid:14); D)
based on Good-Turing estimators [7, 8]. For (cid:2)xed (cid:14) , Thm. 3 in [7] gives an upper bound
of O (r=n + log n=pn). The exact bound is drawn as the G-T curve in Fig. 1. In contrast,
our bound gives O (cid:0)pC + r log n=pn(cid:1) ; for a known constant C > 0. For (cid:2)xed r and
increasing n, this gives an improvement over the G-T bound of order O(log n) if r = 0,
and O(plog n) if r > 0. For r growing faster than O(plog n), asymptotically our bound
becomes uncompetitive3 . The real advantage of our bound is that, in contrast to G-T, it
gives nontrivial bounds for sample sizes and number of repetitions that typically occur in
classi(cid:2)cation problems. For practical applications in language modeling (large samples,
many repetitions), the existing G-T bound of [7] is probably preferable.
The developments in [8] are also relevant, albeit in a more indirect manner. In Thm. 10
of that paper, it is shown that the probability that the missing mass is larger than its ex-
pected value by an amount (cid:15) is bounded by e(cid:0)(e=2)n(cid:15)2 . In [7], Sec. 4, some techniques
are developed to bound the expected missing mass in terms of the number of repetitions in
the sample. One might conjecture that, combined with Thm. 10 of [8], these techniques
can be extended to yield an upper bound on B((cid:14); D) of order O(r=n + 1=pn) that would
be asymptotically stronger than the current bound. We plan to investigate this and other
potential ways to improve the bounds in future work. Any advance in this direction makes
the implications of our bounds even more compelling.

3 If data are i.i.d. according to a (cid:2)xed P (cid:3) , then, as follows from the strong law of large numbers,
r , considered as a function of n, will either remain zero for ever or will be larger than cn for some
c > 0, for all n larger than some n0 . In practice, our bound is still relevant because typical data-sets
often have r very small compared to n (see Table 1). This is possible because apparently n (cid:28) n 0 .

Table 1: Bounds on the difference between the i.i.d. error and the off-training-set error
given by Eq. (2) on con(cid:2)dence level 95% ((cid:14) = 0:05). A dash (-) indicates no repetitions.
Bounds greater than 0.5 are in parentheses.
DATA
SAMPLE SIZE
4177
Abalone
32562
Adult
798
Annealing
1000
Arti(cid:2)cial Characters
Breast Cancer (Diagnostic)
569
699
Breast Cancer (Original)
690
Credit Approval
542
Cylinder Bands
Housing
506
2385
Internet Advertisement
1332
Isolated Letter Speech Recogn.
20000
Letter Recognition
2000
Multiple Features
Musk
6598
5473
Page Blocks
527
Water Treatment Plant
Waveform
5000

REPETITIONS
-
25
8
34
-
236
-
-
-
441
-
1332
4
17
80
-
-

BOUND
0.0383
0.0959
0.3149
(0.5112)
0.1057
(1.0)
0.0958
0.1084
0.1123
(0.9865)
0.0685
(0.6503)
0.1563
0.1671
0.3509
0.1099
0.0350

4 Discussion – Implications of Our Results

The use of off-training-set error is an essential ingredient of the in(cid:3)uential No Free Lunch
theorems [1](cid:150)[5]. Our results imply that, while the NFL theorems themselves are valid,
some of the conclusions drawn from them are overly pessimistic, and should be recon-
sidered. For instance, it has been suggested that the tools of conventional learning theory
(dealing with standard generalization error) are (cid:147)ill-suited for investigating off-training-
set error(cid:148) [3]. With the help of the little add-on we provide in this paper (Corollary 1),
any bound on standard generalization error can be converted to a bound on off-training-set
error. Our empirical results on UCI data-sets show that the resulting bound is often not
essentially weaker than the original one. Thus, the conventional tools turn out not to be so
‘ill-suited’ after all. Secondly, contrary to what is sometimes suggested4 , we show that one
can relate performance on the training sample to performance on as yet unseen cases.
On the other side of the debate, it has sometimes been claimed that the off-training-set error
is irrelevant to much of modern learning theory where often the feature space is continuous.
This may seem to imply that off-training-set error coincides with standard generalization
error (see remark after Def. 1). However, this is true only if the associated distribution is
continuous: then the probability of observing the same X -value twice is zero. However,
in practice even when the feature space has continuous components, data-sets sometimes
contain repetitions (e.g., Adult, see Table 1), if only for the reason that continuous features
may be discretized or truncated. In practice repetitions occur in many data-sets, implying
that off-training-set error can be different from the standard i.i.d. error. Thus, off-training-
set error is relevant. Also, it measures a quantity that is in some ways close to the meaning
of ‘inductive generalization’ (cid:150) in dictionaries the words ‘induction’ and ‘generalization’
frequently refer to ‘unseen instances’. Thus, off-training-set error is not just relevant but
also intuitive. This makes it all the more interesting that standard generalization bounds
transfer to off-training-set error (cid:150) and that is the central implication of this paper.

4 For instance, (cid:147)if we are interested in the error for [unseen cases], the NFL theorems tell us that
(in the absence of prior assumptions) [empirical error] is meaningless(cid:148) [2].

Acknowledgments
We thank Gilles Blanchard for useful discussions. Part of this work was carried out while
the (cid:2)rst author was visiting CWI. This work was supported in part by the Academy of
Finland (Minos, Prima), Nuf(cid:2)c, and IST Programme of the European Community, under
the PASCAL Network, IST-2002-506778. This publication only re(cid:3)ects the authors’ views.

References
[1] Wolpert, D.H.: On the connection between in-sample testing and generalization error. Complex
Systems 6 (1992) 47(cid:150)94
[2] Wolpert, D.H.: The lack of a priori distinctions between learning algorithms. Neural Computa-
tion 8 (1996) 1341(cid:150)1390
[3] Wolpert, D.H.: The supervised learning no-free-lunch theorems. In: Proc. 6th Online World
Conf. on Soft Computing in Industrial Applications (2001).
[4] Schaffer, C.: A conservation law for generalization performance. In: Proc. 11th Int. Conf. on
Machine Learning (1994) 259(cid:150)265
[5] Duda, R.O., Hart, P.E., Stork, D.G.: Pattern Classiﬁcation , 2nd Edition. Wiley, 2001.
[6] Good, I.J.: The population frequencies of species and the estimation of population parameters.
Biometrika 40 (1953) 237(cid:150)264
[7] McAllester, D.A., Schapire, R.E.: On the convergence rate of Good-Turing estimators. In: Proc.
13th Ann. Conf. on Computational Learning Theory (2000) 1(cid:150)6
[8] McAllester, D.A., Ortiz L.: Concentration inequalities for the missing mass and for histogram
rule error. Journal of Machine Learning Research 4 (2003) 895(cid:150)911.
[9] Blake, C., and Merz, C.: UCI repository of machine learning databases. Univ. of California,
Dept. of Information and Computer Science (1998)

A Postponed Proofs

We (cid:2)rst state two propositions that are useful in the proof of Lemma 2.
Proposition 1. Let Xm be a domain of size m, and let P (cid:3)
Xm be an associated probability
distribution. The probability of getting no repetitions when sampling 1 (cid:20) k (cid:20) m items
with replacement from distribution P (cid:3)
Xm is upper-bounded by
Pr[(cid:147)no repetitions(cid:148) j k ] (cid:20)
m!
(m(cid:0)k)!mk :
Proof Sketch of Proposition 1. By way of contradiction it is possible to show that the prob-
ability of obtaining no repetitions is maximized when P (cid:3)
Xm is uniform. After this, it is
easily seen that the maximal probability equals the right-hand side of the inequality.
Proposition 2. Let Xm be a domain of size m, and let P (cid:3)
Xm be an associated probability
distribution. The probability of getting at most r (cid:21) 0 repeated values when sampling
1 (cid:20) k (cid:20) m items with replacement from distribution P (cid:3)
Xm is upper-bounded by
if k < r
Pr[(cid:147)at most r repetitions(cid:148) j k ] (cid:20) ( 1
min (cid:16)(cid:0)k
(m(cid:0)k+r)! m(cid:0)(k(cid:0)r) ; 1(cid:17)
m!
if k (cid:21) r .
r (cid:1)
Proof of Proposition 2. The case k < r is trivial. For k (cid:21) r , the event (cid:147)at most r repeti-
tions in k draws(cid:148) is equivalent to the event that there is at least one subset of size k (cid:0) r of
the X -variables fX1 ; : : : ; Xk g such that all variables in the subset take distinct values. For
a subset of size k (cid:0) r , Proposition 1 implies that the probability that all values are distinct
(m(cid:0)k+r)! m(cid:0)(k(cid:0)r) . Since there are (cid:0)k
is at most
r (cid:1) subsets of the X -variables of size k (cid:0) r ,
m!
the union bound implies that multiplying this by (cid:0)k
r (cid:1) gives the required result.

Proof of Lemma 2. The probability of getting at most r repeated X -values can be upper
bounded by considering repetitions in the maximally probable set (cid:22)Xn only. The probability
of no repetitions in (cid:22)Xn can be broken into n + 1 mutually exclusive cases depending on
how many X -values fall into the set (cid:22)Xn . Thus we get
n
Pr[(cid:147)at most r repetitions in (cid:22)Xn (cid:148)] =
Pr[(cid:147)at most r repetitions in (cid:22)Xn (cid:148) j k ] Pr[k ] ;
Xk=0
where Pr[(cid:1) j k ] denotes probability under the condition that k of the n cases fall into
(cid:22)Xn , and Pr[k ] denotes the probability of the latter occurring. Proposition 2 gives an up-
per bound on the conditional probability. The probability Pr[k ] is given by the binomial
distribution with parameter (cid:22)pn : Pr[k ] = Bin(k ; n; (cid:22)pn ) = (cid:0)n
n (1 (cid:0) (cid:22)pn )n(cid:0)k : Com-
k(cid:1) (cid:22)pk
bining these gives the formula for (cid:1)(n; r; (cid:22)pn ). Showing that (cid:1)(n; r; (cid:22)pn ) is non-increasing
in (cid:22)pn is tedious but uninteresting and we only sketch the proof: It can be checked that
the conditional probability given by Proposition 2 is non-increasing in k (the min opera-
tor is essential for this). From this the claim follows since for increasing (cid:22)pn the binomial
distribution puts more weight to terms with large k , thus not increasing the sum.

Proof of Thm. 2. The (cid:2)rst three factors in the de(cid:2)nition (1) of (cid:1)(n; r; (cid:22)pn ) are equal to a
binomial probability Bin(k ; n; (cid:22)pn ), and the expectation of k is thus n (cid:22)pn . By the Hoeffd-
ing bound, for all (cid:15) > 0, the probability of k < n( (cid:22)pn (cid:0) (cid:15)) is bounded by exp((cid:0)2n(cid:15)2 ).
3 (cid:22)pn is bounded by
Applying this bound with (cid:15) = (cid:22)pn =3 we get that the probability of k < 2
n ). Combined with (1) this gives the following upper bound on (cid:1)(n; r; (cid:22)pn ):
exp((cid:0) 2
9 n (cid:22)p2
f (n; r; k) (cid:20) exp (cid:0)(cid:0) 2
exp (cid:0)(cid:0) 2
9 n (cid:22)p2
9 n (cid:22)p2
n (cid:1) + max
n (cid:1) max
f (n; r; k)
f (n; r; k) + max
k(cid:21)n 2
k<n 2
k(cid:21)n 2
3 pn
3 pn
3 pn
(3)
where the maxima are taken over integer-valued k . In the last inequality we used the fact
that for all n; r; k , it holds that f (n; r; k) (cid:20) 1. Now note that for k (cid:21) r , we can bound
r(cid:19) k(cid:0)r(cid:0)1
r (cid:19) k
k
f (n; r; k) (cid:20) (cid:18)k
n (cid:20) (cid:18)n
n (cid:0) j
n
n (cid:0) j
Yj=0
Yj=0
Yj=k(cid:0)r
n (cid:0) j (cid:20)
n
k
r (cid:19) k
n (cid:0) k (cid:19)r+1
(cid:18)n
n (cid:18) n
n (cid:0) j
(cid:20) n2r n
n (cid:0) j
Yj=1
Yj=1
n (cid:0) k
n
If k < r , f (n; r; k) = 1 so that (4) holds in fact for all k with 1 (cid:20) k (cid:20) n. We bound
n further as follows. The average of the k factors of this product is
the last factor Qk
n(cid:0)j
j=1
2n . Since a product of k factors is always less than or
less than or equal to n(cid:0)k=2
n = 1 (cid:0) k
2n (cid:1)k
equal to the average of the factors to the power of k , we get the upper bound (cid:0)1 (cid:0) k
(cid:20)
2n (cid:17) ; where the (cid:2)rst inequality follows from 1 (cid:0) x (cid:20) exp((cid:0)x)
2n (cid:1) (cid:20) exp (cid:16)(cid:0) k2
exp (cid:0)(cid:0) k(cid:1)k
n(cid:0)k exp (cid:16)(cid:0) k2
2n (cid:17) : Plugging
for x < 1. Plugging this into (4) gives f (n; r; k) (cid:20) n2r n
3n2r exp (cid:16)(cid:0) k2
2n (cid:17) (cid:20)
this back into (3) gives (cid:1)(n; r; (cid:22)pn ) (cid:20) exp((cid:0) 2
9 n (cid:22)p2
n ) + maxk(cid:21)n 2
3 pn
exp((cid:0) 2
n ) (cid:20) 4n2r exp((cid:0) 2
n ) + 3n2r exp((cid:0) 2
9 n (cid:22)p2
9 n (cid:22)p2
9 n (cid:22)p2
n ):
Recall that B((cid:14); D) := arg minp fp : (cid:1)(n; r; p) (cid:20) (cid:14)g. Replacing (cid:1)(n; r; p) by the above
upper bound, makes the set of p satisfying the inequality smaller. Thus, the minimal mem-
ber of the reduced set is greater than or equal to the minimal member of the set with
(cid:1)(n; r; p) (cid:20) (cid:14) , giving the following bound on B((cid:14); D):
9 np2 (cid:1) (cid:20) (cid:14)(cid:9) = 3q 1
2n (cid:0)log 4
B((cid:14); D) (cid:20) arg minp (cid:8)p : 4n2r exp (cid:0)(cid:0) 2
(cid:14) + 2r log n(cid:1) :

(4)

:


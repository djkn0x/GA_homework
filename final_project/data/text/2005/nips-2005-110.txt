An Alternative Inﬁnite Mixture Of Gaussian
Process Experts

Edward Meeds and Simon Osindero
Department of Computer Science
University of Toronto
Toronto, M5S 3G4
fewm,osinderog@cs.toronto.edu
Abstract

We present an inﬁnite mixture model in which each component com-
prises a multivariate Gaussian distribution over an input space, and a
Gaussian Process model over an output space. Our model is neatly able
to deal with non-stationary covariance functions, discontinuities, multi-
modality and overlapping output signals. The work is similar to that by
Rasmussen and Ghahramani [1]; however, we use a full generative model
over input and output space rather than just a conditional model. This al-
lows us to deal with incomplete data, to perform inference over inverse
functional mappings as well as for regression, and also leads to a more
powerful and consistent Bayesian speciﬁcation of the effective ‘gating
network’ for the different experts.

1 Introduction
Gaussian process (GP) models are powerful tools for regression, function approximation,
and predictive density estimation. However, despite their power and ﬂe xibility, they suffer
from several limitations. The computational requirements scale cubically with the number
of data points, thereby necessitating a range of approximations for large datasets. Another
problem is that it can be difﬁcult
to specify priors and perform learning in GP models if we
require non-stationary covariance functions, multi-modal output, or discontinuities.

There have been several attempts to circumvent some of these lacunae, for example [2, 1].
In particular the Inﬁnite Mixture of Gaussian Process Experts (IMoGPE) model proposed
by Rasmussen and Ghahramani [1] neatly addresses the aforementioned key issues. In a
single GP model, an n by n matrix must be inverted during inference. However, if we use a
model composed of multiple GP’s, each responsible only for a subset of the data, then the
computational complexity of inverting an n by n matrix is replaced by several inversions
of smaller matrices —
for large datasets this can result in a substantial speed-up and may
allow one to consider large-scale problems that would otherwise be unwieldy. Furthermore,
by combining multiple stationary GP experts, we can easily accommodate non-stationary
covariance and noise levels, as well as distinctly multi-modal outputs. Finally, by placing a
Dirichlet process prior over the experts we can allow the data and our prior beliefs (which
may be rather vague) to automatically determine the number of components to use.

In this work we present an alternative in ﬁnite model that is strongly inspired by the work
in [1], but which uses a different formulation for the mixture of experts that is in the style
presented in, for example [3, 4]. This alternative approach effectively uses posterior re-

PSfrag replacements

PSfrag replacements

xi

zi

yi

N

zi

xi

yi

N

Figure 1: Left: Graphical model for the standard MoE model [6]. The expert indicators
fz(i) g are speciﬁed by a gating network applied to the inputs fx(i) g. Right: An alternative
view of MoE model using a full generative model [4]. The distribution of input locations is
now given by a mixture model, with components for each expert. Conditioned on the input
locations, the posterior responsibilities for each mixture component behave like a gating
network.

sponsibilities from a mixture distribution as the gating network. Even if the task at hand
is simply output density estimation or regression, we suggest a full generative model over
inputs and outputs might be preferable to a purely conditional model. The generative ap-
proach retains all the strengths of [1] and also has a number of potential advantages, such
as being able to deal with partially speciﬁed data (e.g. missing input co-ordinates) and
being able to infer inverse functional mappings (i.e. the input space given an output value).
The generative approach also affords us a richer and more consistent way of specifying
our prior beliefs about how the covariance structure of the outputs might vary as we move
within input space.

An example of the type of generative model which we propose is shown in ﬁgure 2. We
use a Dirichlet process prior over a countably in ﬁnite number of experts and each expert
comprises two parts: a density over input space describing the distribution of input points
associated with that expert, and a Gaussian Process model over the outputs associated with
that expert. In this preliminary exposition, we restrict our attention to experts whose in-
put space densities are given a single full covariance Gaussian. Even this simple approach
demonstrates interesting performance and capabilities. However, in a more elaborate setup
the input density associated with each expert might itself be an in ﬁnite mixture of sim-
pler distributions (for instance, an in ﬁnite mixture of Gaussians [5]) to allow for the most
ﬂe xible partitioning of input space amongst the experts.

The structure of the paper is as follows. We begin in section 2 with a brief overview of
two ways of thinking about Mixtures of Experts. Then, in section 3, we give the complete
speciﬁcation and graphical depiction of our generative model, and in section 4 we outline
the steps required to perform Monte Carlo inference and prediction. In section 5 we present
the results of several simple simulations that highlight some of the salient features of our
proposal, and ﬁnally in section 6, we discuss our work and place it in relation to similar
techniques.

2 Mixtures of Experts
In the standard mixture of experts (MoE) model [6], a gating network probabilistically
mixes regression components. One subtlety in using GP’s in a mixture of experts model is
that IID assumptions on the data no longer hold and we must specify joint distributions for
each possible assignment of experts to data. Let fx(i) g be the set of d-dimensional input
vectors, fy(i) g be the set of scalar outputs, and fz(i) g be the set of expert indicators which
assign data points to experts.

The likelihood of the outputs, given the inputs, is speciﬁed in equation 1, where (cid:18) GP
rep-
r
resents the GP parameters of the r th expert, (cid:18) g represents the parameters of the gating
network, and the summation is over all possible con ﬁgurations of indicator variables.

PSfrag replacements

b(cid:11)0

a(cid:11)0

(cid:11)0

fz(i) g

(cid:6)x

(cid:23)S fS

a(cid:23)c b(cid:23)c

(cid:23)0 f0

(cid:22)x

S

(cid:23)c

(cid:6)0

(cid:22)0

(cid:6)r

(cid:22)r

i = 1 : Nr

z r
i

x

r
(i)

Yr

v0r

v1r

a0 b0

a1 b1

wj r

aw bw

r = 1 : K

j = 1 : D

Figure 2: The graphical model representation of the alternative in ﬁnite mixture of GP
experts (AiMoGPE) model proposed in this paper. We have used xr
(i) to represent the
ith data point in the set of input data whose expert label is r , and Yr to represent the set of
all output data whose expert label is r . In other words, input data are IID given their expert
label, whereas the sets of output data are IID given their corresponding sets of input data.
The lightly shaded boxes with rounded corners represent hyper-hyper parameters that are
ﬁx ed ((cid:10) in the text). The DP concentration parameter (cid:11)0 , the expert indicators variables,
fz(i) g, the gate hyperparameters, (cid:30)x = f(cid:22)0 ; (cid:6)0 ; (cid:23)c ; S g, the gate component parameters,
r = f(cid:22)r ; (cid:6)r g, and the GP expert parameters, (cid:18)GP
r = fv0r ; v1r ; wj r g, are all updated for
 x
all r and j .

P (fy(i) : z(i) = rgjfx(i) : z(i) = rg; (cid:18)GP
r )

P (fz(i) gjfx(i) g; (cid:18)g ) Yr
P (fy(i) gjfx(i) g; (cid:18)) =XZ
(1)
There is an alternative view of the MoE model in which the experts also generate the inputs,
rather than simply being conditioned on them [3, 4] (see ﬁgure 1). This alternative view
employs a joint mixture model over input and output space, even though the objective
is still primarily that of estimating conditional densities i.e. outputs given inputs. The
gating network effectively gets speciﬁed by the posterior responsibilities of each of the
different components in the mixture. An advantage of this perspective is that it can easily
accommodate partially observed inputs and it also allows ‘reverse-conditioning’, should
we wish to estimate where in input space a given output value is likely to have originated.
For a mixture model using Gaussian Processes experts, the likelihood is given by
P (fx(i) g;fy(i) gj(cid:18)) = XZ
Yr
P (fy(i) : z(i) = rgjfx(i) : z(i) = rg; (cid:18)GP
r )P (fx(i) : z(i) = rgj(cid:18)g )
where the description of the density over input space is encapsulated in (cid:18) g .

P (fz(i) gj(cid:18)g )(cid:2)

(2)

3 Inﬁnite Mixture of Gaussian Processes: A Joint Generative Model
The graphical structure for our full generative model is shown in ﬁgure 2. Our generative
process does not produce IID data points and is therefore most simply formulated either as

a joint distribution over a dataset of a given size, or as a set of conditionals in which we
incrementally add data points.To construct a complete set of N sample points from the prior
(speciﬁed by top-level hyper-parameters (cid:10)) we would perform the following operations:
1. Sample Dirichlet process concentration variable (cid:11)0 given the top-level hyper-
parameters.
2. Construct a partition of N objects into at most N groups using a Dirichlet pro-
cess. This assignment of objects is denoted by using a set the indicator variables
i=1 .
fz(i) gN
3. Sample the gate hyperparameters (cid:30)x given the top-level hyperparameters.
4. For each grouping of indicators fz(i) : z(i) = rg, sample the input space param-
eters  x
r conditioned on (cid:30)x .  x
r deﬁnes
the density in input space, in our case a
full-covariance Gaussian.
5. Given the parameters  x
r for each group, sample the locations of the input points
Xr (cid:17) fx(i) : z(i) = rg.
6. For each group, sample the hyper-parameters for the GP expert associated with
that group, (cid:18)GP
r .
7. Using the input locations Xr and hyper-parameters (cid:18)GP
r for the individual groups,
formulate the GP output covariance matrix and sample the set of output values,
Yr (cid:17) fy(i) : z(i) = rg from this joint Gaussian distribution.
We write the full joint distribution of our model as follows.

r=1 ; f(cid:18)GP
r gN
r gN
i=1 ; fz(i) gN
P (fx(i) ; y(i) gN
r=1 ; (cid:11)0 ; (cid:30)x jN ; (cid:10)) =
i=1 ; f x
N
Yr=1 (cid:2)H N
r ; (cid:18)GP
r j(cid:10))P (Yr jXr ; (cid:18)GP
r )P ((cid:18)GP
r ) + (1 (cid:0) H N
r )(cid:3)
r j(cid:30)x )P (Xr j x
r )D0 ( x
r P ( x
(cid:2) P (fz(i) gN
(3)
i=1 jN ; (cid:11)0 )P ((cid:11)0 j(cid:10))P ((cid:30)x j(cid:10))

Where we have used the supplementary notation: H N
r = 0 if ffz(i) g : z(i) = rg is the
r ; (cid:18)GP
empty set and H N
r = 1 otherwise; and D0 ( x
r ) is a delta function on an (irrelevant)
dummy set of parameters to ensure proper normalisation.

For the GP components, we use a standard, stationary covariance function of the form
Q(x(i) ; x(h) ) = v0 exp (cid:18)(cid:0)
j (cid:19) + (cid:14)(i; h)v1
1
2 XD
j=1 (cid:0)x(i)j (cid:0) x(h)j (cid:1)2
=w2
The individual distributions in equation 3 are deﬁned as follows1 :

(4)

P ((cid:11)0 j(cid:10)) = G ((cid:11)0 ; a(cid:11)0 ; b(cid:11)0 )
P (fz(i) gN
i=1 jN ; (cid:10)) = P U ((cid:11)0 ; N )
P ((cid:30)x j(cid:10)) = N ((cid:22)0 ; (cid:22)x ; (cid:6)x =f0 )W ((cid:6)(cid:0)1
0 ; (cid:23)0 ; f0(cid:6)(cid:0)1
x =(cid:23)0 )
G ((cid:23)c ; a(cid:23)c ; b(cid:23)c )W (S(cid:0)1 ; (cid:23)S ; fS (cid:6)x =(cid:23)S )
r j(cid:10)) = N ((cid:22)r ; (cid:22)0 ; (cid:6)0 )W ((cid:6)(cid:0)1
r ; (cid:23)c ; S=(cid:23)c )
P ( x
r ) = N (Xr ; (cid:22)r ; (cid:6)r )
P (Xr j x
r j(cid:10)) = G (v0r ; a0 ; b0 )G (v1r ; a1 ; b1 ) YD
P ((cid:18)GP
j=1
P (Yr jXr ; (cid:18)GP
r ) = N (Yr ; (cid:22)Qr ; (cid:27)2
Qr )
1We use the notation N , W , G , and LN to represent the normal, the Wishart, the gamma, and the
log-normal distributions, respectively; we use the parameterizations found in [7] (Appendix A). The
notation P U refers to the Polya urn distribution [8].

LN (wj r ; aw ; bw ) (10)

(11)

(5)
(6)

(7)
(8)
(9)

In an approach similar to Rasmussen [5], we use the input data mean (cid:22)x and covariance
(cid:6)x to provide an automatic normalisation of our dataset. We also incorporate additional
hyperparameters f0 and fS , which allow prior beliefs about the variation in location of (cid:22)r
and size of (cid:6)r , relative to the data covariance.

4 Monte Carlo Updates
Almost all the integrals and summations required for inference and learning operations
within our model are analytically intractable, and therefore necessitate Monte Carlo ap-
proximations. Fortunately, all the necessary updates are relatively straightforward to carry
out using a Markov Chain Monte Carlo (MCMC) scheme employing Gibbs sampling and
Hybrid Monte Carlo. We also note that in our model the predictive density depends on
the entire set of test locations (in input space). This transductive behaviour follows from
the non-IID nature of the model and the in ﬂuence that test locations have on the posterior
distribution over mixture parameters. Consequently, the marginal predictive distribution at
a given location can depend on the other locations for which we are making simultaneous
predictions. This may or may not be desired. In some situations the ability to incorporate
the additional information about the input density at test time may be beneﬁcial. However,
it is also straightforward to effectively ‘ignore’ this new information and simply compute a
set of independent single location predictions.
Given a set of test locations fx
(t) g, along with training data pairs fx(i) ; y(i) g and top-level
(cid:3)
hyper-parameters (cid:10), we iterate through the following conditional updates to produce our
predictive distribution for unknown outputs fy (cid:3)
(t) g. The parameter updates are all conjugate
with the prior distributions, except where noted:
1. Update indicators fz(i) g by cycling through the data and sampling one indicator
variable at a time. We use algorithm 8 from [9] with m = 1 to explore new
experts.
2. Update input space parameters.
3. Update GP hyper-params using Hybrid Monte Carlo [10].
4. Update gate hyperparameters. Note that (cid:23)c is updated using slice sampling [11].
5. Update DP hyperparameter (cid:11)0 using the data augmentation technique of Escobar
and West [12].
6. Resample missing output values by cycling through the experts, and jointly sam-
pling the missing outputs associated with that GP.
We perform some preliminary runs to estimate the longest auto-covariance time, (cid:28)max for
our posterior estimates, and then use a burn-in period that is about 10 times this timescale
before taking samples every (cid:28)max iterations.2 For our simulations the auto-covariance time
was typically 40 complete update cycles, so we use a burn-in period of 500 iterations and
collect samples every 50.

5 Experiments
5.1 Samples From The Prior

In ﬁgure 3 (A) we give an example of data drawn from our model which is multi-modal
and non-stationary. We also use this arti ﬁcial dataset to con ﬁrm that our MCMC algorithm
performs well and is able recover sensible posterior distributions. Posterior histograms for
some of the inferred parameters are shown in ﬁgure 3 (B) and we see that they are well
clustered around the ‘true’ values.

2This is primarily for convenience. It would also be valid to use all the samples after the burn-in
period, and although they could not be considered independent, they could be used to obtain a more
accurate estimator.

40

30

20

10

0

−10

−20

−30

−40

−50

−60
−8

−6

−4

−2

0

2
(A)

4

6

8

10

t
n
u
o
c

15

10

5

0
−3

t
n
u
o
c

100

80

60

40

20

0

−2.5

−2

−1.5

a
−1
0

−0.5

0

0.5

1

3

4

k
(B)

5

6

Figure 3: (A) A set of samples from our model prior. The different marker styles are used
to indicate the sets of points from different experts. (B) The posterior distribution of log (cid:11)0
with its true value indicated by the dashed line (top) and the distribution of occupied experts
(bottom). We note that the posterior mass is located in the vicinity of the true values.

5.2 Inference On Toy Data

To illustrate some of the features of our model we constructed a toy dataset consisting of
4 continuous functions, to which we added different levels of noise. The functions used
were:
f1 (a1 ) = 0:25a2
Noise SD: 7 (12)
a1 2 (0 : : : 15)
1 (cid:0) 40
f2 (a2 ) = (cid:0)0:0625(a2 (cid:0) 18)2 + :5a2 + 20
Noise SD: 7 (13)
a2 2 (35 : : : 60)
f3 (a3 ) = 0:008(a3 (cid:0) 60)3 (cid:0) 70
Noise SD: 4 (14)
a3 2 (45 : : : 80)
Noise SD: 2 (15)
f4 (a4 ) = (cid:0) sin(0:25a4 ) (cid:0) 6
a4 2 (80 : : : 100)
The resulting data has non-stationary noise levels, non-stationary covariance, discontinu-
ities and signiﬁcant multi-modality. Figure 4 shows our results on this dataset along with
those from a single GP for comparison.

We see that in order to account for the entire data set with a single GP, we are forced to infer
an unnecessarily high level of noise in the function. Also, a single GP is unable to capture
the multi-modality or non-stationarity of the data distribution. In contrast, our model seems
much more able to deal with these challenges.

Since we have a full generative model over both input and output space, we are also able
to use our model to infer likely input locations given a particular output value. There
are a number of applications for which this might be relevant, for example if one wanted
to sample candidate locations at which to evaluate a function we are trying to optimise.
We provide a simple illustration of this in ﬁgure 4 (B). We choose three output levels
and conditioned on the output having these values, we sample for the input location. The
inference seems plausible and our model is able to suggest locations in input space for a
maximal output value (+40) that was not seen in the training data.

5.3 Regression on a simple “r eal-world” dataset

We also apply our model and algorithm to the motorcycle dataset of [13]. This is a com-
monly used dataset in the GP community and therefore serves as a useful basis for compar-
ison. In particular, it also makes it easy to see how our model compares with standard GP’s
and with the work of [1]. Figure 5 compares the performance of our model with that of a
single GP. In particular, we note that although the median of our model closely resembles
the mean of the single GP, our model is able to more accurately model the low noise level

80

60

40

20

0

−20

−40

−60

−80

−100

−120
−20

Training Data
AiMoGPE
Single GP

0

20

40
(A)

60

80

100

120

80

60

40

20

0

−20

−40

−60

−80

−100

−120
−20

0

20

40
(B)

60

80

100

120

Figure 4: Results on a toy dataset. (A) The training data is shown along with the predictive
mean of a stationary covariance GP and the median of the predictive distribution of our
model. (B) The small dots are samples from the model (160 samples per location) evaluated
at 80 equally spaced locations across the range (but plotted with a small amount of jitter
to aid visualisation). These illustrate the predictive density from our model. The solid the
lines show the (cid:6) 2 SD interval from a regular GP. The circular markers at ordinates of 40,
10 and (cid:0)100 show samples from ‘reverse-conditioning’ where we sample likely abscissa
locations given the test ordinate and the set of training data.

on the left side of the dataset. For the remainder of the dataset, the noise level modeled by
our model and a single GP are very similar, although our model is better able to capture
the behaviour of the data at around 30 ms. It is difﬁcult
to make an exact comparison to
[1], however we can speculate that our model is more realistically modeling the noise at
the beginning of the dataset by not inferring an overly “
ﬂat”
GP expert at that location. We
can also report that our expert adjacency matrix closely resembles that of [1].

6 Discussion
We have presented an alternative framework for an in ﬁnite mixture of GP experts. We feel
that our proposed model carries over the strengths of [1] and augments these with the sev-
eral desirable additional features. The pseudo-likelihood objective function used to adapt
the gating network deﬁned in [1] is not guaranteed to lead to a self-consistent distribution
and therefore the results may depend on the order in which the updates are performed; our
model incorporates a consistent Bayesian density formulation for both input and output
spaces by deﬁnition. Furthermore, in our most general framework we are more naturally
able to specify priors over the partitioning of space between different expert components.
Also, since we have a full joint model we can infer inverse functional mappings.
There should be considerable gains to be made by allowing the input density models be
more powerful. This would make it easier for arbitrary regions of space to share the same
covariance structures; at present the areas ‘controlled’ by a particular expert tend to be
local. Consequently, a potentially undesirable aspect of the current model is that strong
clustering in input space can lead us to infer several expert components even if a single GP
would do a good job of modelling the data. An elegant way of extending the model in this
way might be to use a separate in ﬁnite mixture distribution for the input density of each
expert, perhaps incorporating a hierarchical DP prior across the in ﬁnite set of experts to
allow information to be shared.

With regard to applications, it might be interesting to further explore our model’s capability
to infer inverse functional mappings; perhaps this could be useful in an optimisation or
active learning context. Finally, we note that although we have focused on rather small
examples so far, it seems that the inference techniques should scale well to larger problems

100

50

0

−50

−100

)
g
(
 
n
o
i
t
a
r
e
l
e
c
c
A

Training Data
AiMoGPE
SingleGP

100

50

0

−50

−100

)
g
(
 
n
o
i
t
a
r
e
l
e
c
c
A

−150

0

10

20

30
Time (ms)
(A)

40

50

60

−150

0

10

20

40

50

60

30
Time (ms)
(B)

Figure 5: (A) Motorcycle impact data together with the median of our model’s point-wise
predictive distribution and the predictive mean of a stationary covariance GP model. (B)
The small dots are samples from our model (160 samples per location) evaluated at 80
equally spaced locations across the range (but plotted with a small amount of jitter to aid
visualisation). The solid lines show the (cid:6) 2 SD interval from a regular GP.

and more practical tasks.

Acknowledgments
Thanks to Ben Marlin for sharing slice sampling code and to Carl Rasmussen for making
minimize.m available.

References
[1] C.E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. In Advances
in Neural Information Processing Systems 14, pages 881–888. MIT Press, 2002.
[2] V. Tresp. Mixture of Gaussian processes. In Advances in Neural Information Processing Sys-
tems, volume 13. MIT Press, 2001.
[3] Z. Ghahramani and M. I. Jordan. Supervised learning from incomplete data via an EM ap-
proach. In Advances in Neural Information Processing Systems 6, pages 120–127. Morgan-
Kaufmann, 1995.
[4] L. Xu, M. I. Jordan, and G. E. Hinton. An alternative model for mixtures of experts. In Advances
in Neural Information Processing Systems 7, pages 633–640. MIT Press, 1995.
[5] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information
Processing Systems, volume 12, pages 554–560. MIT Press, 2000.
[6] R.A. Jacobs, M.I. Jordan, and G.E. Hinton. Adaptive mixture of local experts. Neural Compu-
tation, 3, 1991.
[7] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman and
Hall, 2nd edition, 2004.
[8] D. Blackwell and J. B. MacQueen. Ferguson distributions via Polya urn schemes. The Annals
of Statistics, 1(2):353–355, 1973.
[9] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9:249–265, 2000.
[10] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report
CRG-TR-93-1, University of Toronto, 1993.
[11] R. M. Neal. Slice sampling (with discussion). Annals of Statistics, 31:705 –767, 2003.
[12] M. Escobar and M. West. Computing Bayesian nonparametric hierarchical models. In Prac-
tical Nonparametric and Semiparametric Bayesian Statistics, number 133 in Lecture Notes in
Statistics. Springer-Verlag, 1998.
[13] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression
curve ﬁtting.
J. Royal Stayt Society. Ser. B, 47:1 –52, 1985.


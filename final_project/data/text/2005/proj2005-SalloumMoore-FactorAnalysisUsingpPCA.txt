Wael Salloum
Brad Moore
Rajat Raina

1 Introduction

Final Project
16th December 2005
Page 1 of 5

Nearly 100 years ago, psychologist Charles Spearman, hypothesized incorrectly all dimensions of
mental ability can be explained solely by one underlying factor, which he dubbed as ’g’. Factor
analysis seeks to uncover if there are indeed underlying independent, and unmeasurable, ”factors”
that aﬀect the observed dependent variables. A typical factor analysis attempts to answer four
ma jor questions:
• How many factors are needed to explain relationship within data?
• Do these factors have real life meanings? If so, what?
• Do hypothesized factors explain the data well, and how much can the dimensionality be
reduced while still maintaining inter-data relationship?
• Can factors measure the amount of pure random or unique variance within each observed
variable?

Factor analysis is often trained using the Expectation-Maximization algorithms to try and identify
the mean, variance, and dimensional mapping that corresponds to the lower dimensional factors.
Professor Ng hypothesized that this algorithm could both be sped up and improved if ideas from
Probabilistic Principal Component Analysis are used. The rest of the 5 page paper will present
the new algorithm, discuss the data set and experiential methodology, present conclusions drawn
from the results, and then present selected results. Additional material such as derivation of the
algorithm, the basic factor analysis model, 4 selected scripts from the 24 code ﬁles, and supporting
graphs are left for the appendix. We will start by presenting the algorithms.

2 Algorithms

2.1 Factor pPCA

The established algorithms (EM, GDA, and PCA) can be seen in Appendix A see page 7. Here
we present the algorithm for factor analysis using probabilistic principal component analysis. The
derivation of this algorithm is fully described in Appendix B (see page 9).
Given k and X,

a.) Calculate sample mean µ and sample variance Σ.

b.) While not converged
i.) Calculate the k largest eigenvalues d = {λ1 , ..., λk } with corresponding (column) eigen-
vectors v = {v1 , ..., vk } of Ψ− 1
2 ΣΨ− 1
2 sorted in descending order.
(cid:189)
ii.) Let ˜d denote the number of elements of d > 1. Set t = min(k , ˜d)
(dii − 1) 1
i = j ≤ t
iii.) Set ∀i, j ≤ k , (cid:195)Lij =
2
o.w
0,

iv.) Set U:,i = vi .

2

Factor Analysis using pPCA

(cid:189)

(Σ − ΛΛT )ii
v.) Set ∀i, j, Ψij =
0,
2 ∗ U ∗ L
vi.) Set Λ = Ψ 1

i = j
i (cid:54)= j

2.2 Hypothesis

In approaching this analysis, we expected that Factor pPCA would be faster then Factor EM, but
not necessarily more accurate, as it may impose the restrictive assumptions of pPCA. Nor did we
expect it to maximize likelihood at every iteration, thus no longer guaranteeing convergence. Our
test were generated to test this hypothesis.

3 Experimental Methodology

3.1 Optimizing Factor EM

In order to ensure that the pPCA-Factor algorithm was properly tested, a lot of eﬀort was dedicated
to optimizing Factor EM and GDA for matlab, by leveraging compiled existing code and reducing
the number of for loops necessary for the code to execute. Everything was rewritten so that sums
can be replaced with matrix multiplications while maintaining accuracy of result. This provides a
stiﬀer test of accuracy and time eﬃciency of the pPCA-Factor algorithm.

3.2 Tests

We designed several layers of tests in order to ascertain the various properties of these algorithms.
We developed two main tests for time as that was our main criteria:

a.) Every time we executed the algorithm, we tested for how long it ran in order to measure
its time. We then plotted for each data set time versus dimensions. This gives us a direct
comparison between FactorEM and FactorpPCA. In each test, we ensured that we initialized
the algorithm with identical starting points and identical starting criteria.

b.) In order to ascertain why one algorithm took longer then the other, we had each algorithm
return the number of iterations it took to converge. Similarly, we plotted the number of
iterations versus dimensions, allowing us to determine if one algorithm took longer to converge
because of iteration or computational cost.

We also developed three main tests for accuracy:

a.) We trained the algorithm on the entire data set, and then plotted training error as a function
of dimension per algorithm. This allows us a direct comparison of error per dimension across
algorithms.

b.) We also trained the algorithm on 70% of the data set, and then tested the predicative ability
on the remaining 30%. This gives us an idea of the predicative ability of the algorithms.

c.) We also plotted the total log-likelihood per dimension per algorithm, in order to ascertain
which algorithm claimed to have a greater likelihood of explaining the data set.

We also developed several tests to ensure the algorithms were correct:

Factor Analysis using pPCA

3

a.) We tested if the algorithm were indeed correct by measuring if the log-likelihood was increasing
per iteration, as it should be in any EM algorithm. If so, we can conclude if its correct. This
was the problem– our EM algorithm does not monotonically increase.

b.) Wanting to ensure if initialization had any eﬀect on the algorithm, we initialized the algo-
rithms at 3 diﬀerent points, one where Ψ is the identity matrix I , one where Ψ is 0.1 * I, and
the last where Ψ is 10 * I. If the results were unaﬀected, we can conclude that the matrix is
invariant to scaling. Λ was always chosen to be random as its initialization only aﬀects EM.

Finally, in order to ensure that results were indeed at maxima points, we ran each of the above tests
on the algorithm initialized with the converged result of the other algorithm. It was suspected that
it won’t take many iterations before the algorithm converges. The last part of any experimental
methodology is of course, the data set selection.

3.3 Data Set

All of our data came from the UCI Machine Repository, see [1]. The data downloaded comprises
3 distinct sets.In ”Machine”, we attempt to predict if the computer is faster then the average
processor speed based on publicly released machine data. In ”Balance”, we attempt to analyze
psychological data, given the relative weighting and distance. Here there exists 3 classes, Left,
Right, and Balanced. In ”Votes”, we attempt to predict the party of the politician given his votes
in 16 key issues. Finally, we generated random data that remained consistent with both GDA and
Factor Analysis to see how well the algorithms performed. As we present the results, we will go into
more detail as to the implementation of the data sets. For complete detail, please see Appendix C,
(see page 12).

4 Conclusion

As the results are best interpreted in light of conclusions, we present the conclusions ﬁrst. It seems
that FactorpPCA is both faster per iteration and more accurate. It seems to fare better with a
convergence test based on likelihood rather then on global maxima on the matrices, as those may
be scaled or altered but have similar predictive capabilities. Initialization appears to be important,
as indicated by Machine, in that Ψ should be initialized to have similar magnitude to the data sets.
Otherwise, subtracting 1 tends to get swamped, and we get 0’s in the diagonal. The algorithm
as a result no longer guarantees convergence, and likelihood, as predicted, does not increase with
every iteration. Nevertheless, this looks to be a promising algorithm and there seems to be several
mechanisms that can be used to ﬁx these problems. First, the data set can be normalized. (This
wasn’t tried, but this may resolve problem of having the data sets of varying magnitudes.) Second,
there could be a further reﬁnement of the convergence criteria made. As there is far more plots
and data then can ﬁt in 5 pages, the secondary results and graphs are reproduced in the appendix.

5 Results

The following graphs are 3-d band graphs. EM or pPCA designates the standard algorithm. If
the name is followed by a 1, the algorithm is run with Psi initialized to 0.1 * I. With 10, it means
it was multiplied by 10. With ”init”, this means it was initialized with the results of the other
algorithm. The following graphs were generated with a convergence test on Ψ and a cap of number
of iterations of 1000.

4

Factor Analysis using pPCA

(a)

(b)

(c)

Figure 1: Iteration Figures- Random, Machine, Balance

Only three data sets are presented. The results of votes are included in Appendix E (see page 25).
Here, we clearly note that machine failed to converge within 1000 iterations, while it converged in
all other cases but one.

(a)

(b)

(c)

Figure 2: Time Figures- Random, Machine, Balance

It is clear that FactorpPCA outerperfomed FactorEM except when it failed to converge. Moreover,
it is interesting to note that in many cases, FactorpPCA required more iterations to converge, and
yet, still managed to outperform FactorEM. Thus, it was clear we needed to analyze our convergence
criteria.

11.522.533.5411.522.533.544.5501002003004005006007008009001000Algorithms(Full Set): # Iter Vs Dim Per AlgkNumber of IterationsEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.533.544.5501002003004005006007008009001000Algorithms(Full Set): # Iter Vs Dim Per AlgkNumber of IterationsEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.530102030405060Algorithms(Full Set): # Iter Vs Dim Per AlgkNumber of IterationsEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.533.544.5500.511.522.5Algorithms(Full Set): Time Vs Dim Per AlgkTIMEEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.533.544.55012345678Algorithms(Full Set): Time Vs Dim Per AlgkTIMEEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.5300.10.20.30.40.50.60.70.80.9Algorithms(Full Set): Time Vs Dim Per AlgkTIMEEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinitFactor Analysis using pPCA

5

(a)

(b)

(c)

Figure 3: Likelihood Figures-Random, Machine, Balance

Both EM, pPCA stagnates well before we terminate the algorithm and becomes cleared by looking
at the graphs in Appendix E. As a result, its possible (and likely) that the convergence on Psi is
overrestricting and does not increase predictive ability by any signiﬁcant amount.

(a)

(b)

(c)

Figure 4: Error Figures- Random, Machine, Balance

While speed is important, it is important to analyze classiﬁcation error. Test error is presented in
the appendix, as well as comparison against GDA. These graphs are of training error on the full
set and indicate that FactorpPCA is clearly the winner here. Thus, FactorpPCA is a promising
algorithm, but is susceptible to initialization, magnitude of the data (machine), and may require a
more reﬁned convergence test based on likelihood rather then on maxima of Psi. The ﬁrst ﬁgures
in the appendix test this convergence criteria. This concludes our pro ject.

1234567891011−200−195−190−185−180−175−170−165−160IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM02468101214−2900−2800−2700−2600−2500−2400−2300−2200−2100IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM11.522.533.544.55−2600−2500−2400−2300−2200−2100−2000−1900IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM11.522.533.5411.522.533.544.552021222324252627282930Algorithms(Full Set): Error Vs Dim Per AlgkPercent Classification ErrorEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.533.544.5505101520253035Algorithms(Full Set): Error Vs Dim Per AlgkPercent Classification ErrorEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5411.522.53810121416182022242628Algorithms(Full Set): Error Vs Dim Per AlgkPercent Classification ErrorEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit6

Factor Analysis using pPCA

6 Appendix A- Basic Models

6.1 Basic Factor Analysis Model
Factor analysis assumes x ∈ (cid:60)n is an observed random variable according to the following model:
z ∼ N (0, I )
x|z ∼ N (µ + Λz , Ψ)
where z ∈ (cid:60)k is the latent random variable.1
The parameters are: µ ∈ (cid:60)n , Λ ∈ (cid:60)n×k , and Ψ = diag(σ2
n ) ∈ (cid:60)n×n .
1 . . . σ2
(cid:184)(cid:182)
(cid:183)
(cid:184)
(cid:181)(cid:183)
(cid:184)
(cid:183)
The joint distribution for (z , x) is:

z
x

∼ N

0
µ

,

ΛT
I
Λ ΛΛT + Ψ

An important note is that ΛΛT + Ψ must be symmetric positive semideﬁnite, as it is the sum of
the ”square” of a matrix and a covariance matrix.
Given a training set {x(i)}m
m(cid:88)
i=1 , the log-likelihood of the parameters is:
(cid:96)(µ, Λ, Ψ) = C − m log |ΛΛT + Ψ| − 1
(x(i) − µ)T (ΛΛT + Ψ)−1 (x(i) − µ)
m(cid:88)
2
i=1
= C − m log |ΛΛT + Ψ| − (ΛΛT + Ψ)−1
2
i=1

(x(i) − µ)(x(i) − µ)T

1
m

where C is constant.

m(cid:88)
The ML estimator for µ is simply given by the mean of the data and can be calculated once:
i=1

1
m

µ =

x(i)

(cid:195)
Thus, Factor Analysis seeks to solve:
(cid:124)

argmaxΛ,Ψ

(cid:123)(cid:122)
− log |ΛΛT + Ψ| − tr((ΛΛT + Ψ)−1
L

(cid:80)m
i=1 (x(i) − µ)(x(i) − µ)T
m

(cid:33)
(cid:125)

)

(1)

6.2 Factor EM Model

As this is our primary model to test against, its important to restate the results:
(cid:81)m
The log likelihood of the model is as follows:
2 |ΛΛT +Ψ| e(− 1
2 (x(i)−µ)T (ΛΛT +Ψ)−1 (x(i)−µ))
1
n
i=1

l(µ, Λ, Ψ) = log

2π

The E-step is as follows:
1Note k > n yields the trivial optimal model by setting Λ = In×n |0n×(k−n) .

Factor Analysis using pPCA

7

µz (i) |x(i) = ΛT (ΛΛT + Ψ)−1 (x(i) − µ),
Σz (i) |x(i) = I − ΛT (ΛΛT + Ψ)−1Λ
(cid:33) (cid:195)
m(cid:88)
i=1
z (i) |x(i) ΛT − ΛµT
x(i)x(i)T − x(i)µT
z (i) |x(i) x(i)T + Λµz (i) |x(i) µz (i) |x(i)T + Σz (i) |x(i) ΛT ,

z (i) |x(i) + Σz (i) |x(i)
µz (i) |x(i) µT

(cid:33)−1

Λ =

The M-step is as follows:
(cid:195)
m(cid:88)
(x(i) − µ)µT
m(cid:88)
z (i) |x(i)
i=1
1
m
i=1
Ψii = Φii ,
Ψij = 0, if i (cid:54)= j

Φ =

6.3 Gaussian Discriminant Analysis Model
This model has parameters φj = P (y = j ), µj , and Σ. The MLE estimates are:
m(cid:88)
(cid:80)m
1
(cid:80)m
m
i=1
i=1 I y (i) = jx(i)
m(cid:88)
i=1 I y (i) = j
(x(i) − µy(i) )(x(i) − µy(i) )T
i=1

I {y (i) = j },

µj =

φj =

Σ =

,

1
m

8

Factor Analysis using pPCA

7 Appendix B- Derivation of Factor pPCA
(cid:80)m
7.1 Maximizing w.r.t. Λ with Ψ ﬁxed
i=1 (x(i)−µ)(x(i)−µ)T
m

For the sake of brevity, let Σ =

.

Therefore,

∇Λ (cid:195)L = −2ΛT (ΛΛT + Ψ)−T + 2ΛT (ΛΛT + Ψ)−T Σ(ΛΛT + Ψ)−T
Setting the gradient to zero and taking transposes, the extreme values of Λ must satisfy:
Λ = Σ(ΛΛT + Ψ)−1Λ
(2)
Lets only the consider when Λ (cid:54)= 0 and ΛΛT + Ψ (cid:54)= Σ as those cases trivially satisfy the above
constraint. The two other cases are considered at the end.

Assuming Ψ is non-singular2 , its singular value decomposition exists and is given by
2 Λ = U LV T , where U = (u1 , u2 , . . . , uk ) ∈ (cid:60)n×k is an column orthonormal matrix,
Ψ 1
L = [l1 , l2 , . . . , lk ] I ∈ (cid:60)k×k is a diagonal matrix of singular values, and V ∈ (cid:60)k×k is an orthogonal
matrix. Thus, we have the decomposition Λ = Ψ 1
2 U LV T .

Substituting this into Eqn. 2 for Λ yields:
2 (U L2U T + I )−1U L = Ψ 1
ΣΨ− 1
(3)
2 U L
If li = 0 for some i ∈ {1, . . . , k}, then the solution for U can have an arbitrary column ui . Thus,
reduce the system to only the constrained dimensions and invert L3 .
ΣΨ− 1
2 (U L2U T + I )−1U L = Ψ 1
2 U L
=⇒ ΣΨ− 1
2 (U L2U T + I )−1U = Ψ 1
2 U
2 U (I − Q) = Ψ 1
=⇒ ΣΨ− 1
2 U
=⇒ Ψ− 1
2 U (I − Q) = U
2 ΣΨ− 1
=⇒ Ψ− 1
2 ΣΨ− 1
2 U = U (I + L2 )
where Q = diag(q1 , . . . , qk ) ∈ (cid:60)k×k with each qi = l2
i /(1 + l2
i ).
Step 2 to 3 uses the matrix inversion lemma to yield: (U L2U T + I )−1 = I − U QU T . The last step
i ) the ith diagonal entry of I − Q must be 1/(1 + l2
follows from the observation that qi = l2
i /(1 + l2
i ).
For any li (cid:54)= 0, this equation can be written in terms of the column ui as:
2 ΣΨ− 1
Ψ− 1
2 ui = (1 + l2
i )ui
2 ΣΨ− 1
Thus, ui must be an eigenvector of Ψ− 1
√
2 .
λi − 1 which constrains λi > 1.
Denoting the corresponding eigenvalue by λi , this implies li =
2The assumption that Ψ is non-singular amounts to, within the factor analysis model, that the diagonal entries
are (cid:54)= 0. That is, the normally distributed error has non-zero variance along every dimension of X.
3Note that reducing L did not adversely aﬀect the solution as we can permute the rows and columns back by a
linear operator, then use block matrix notation to invert and consider only the segments we are interested in.

Factor Analysis using pPCA

9

Therefore,
• V can be any k × k orthogonal matrix. So let V = I.
• U and L are chosen together, column by column. Each column ui and diagonal entry li should
be picked using one of the following choices:
Choice 1: li = 0; Arbitrary ui ∈ (cid:60)n
√
λi − 1; ui corresponding eigenvector to eigenvalue λi .
Choice 2: li =

7.2 The global maximum
Without loss of generality, let l1 , l2 , . . . , lt represent the nonzero values picked using Choice 2
above, and let λa1 , λa2 , . . . , λat > 1 represent the corresponding eigenvalues of Ψ− 1
2 ΣΨ− 1
2 . Fur-
let U1:t ∈ (cid:60)n×t contain the t columns of U picked using Choice 2 above and let
thermore,
L1:t = [l1 , . . . , lt ] I ∈ (cid:60)t×t . Using Choice 1, lt+1 , . . . , lk = 0 with undetermined corresponding eigen-
(cid:113)
values.
i for i ∈ {1, 2, . . . , n}.
The eigenvalues of Ψ 1
2 are simply
σ2
The eigenvalues of U1:tL2
1:t are exactly the diagonal entries of L2
1:tU T
1:t . 4
(cid:161)
(cid:162)
Then,
Λ = Ψ 1
(cid:195)
(cid:33)
U1:tL1:t 0
V T
2
n(cid:89)
t(cid:89)
=⇒ ΛΛT + Ψ = Ψ 1
1:t + I )Ψ 1
2 (U1:tL2
1:tU T
2
i ·
=⇒ log |ΛΛT + Ψ| = log
σ2
i=1
i=1
1:tU1:t + I ) are the t values {λa1 , λa2 , . . . , λat } and (n − t) 1’s,
Since the eigenvalues of (U1:tL2
t(cid:88)
n(cid:88)
tr((ΛΛT + Ψ)−1S ) = tr((U1:tL2
1:tU1:t + I )−1 ˜S )
1 +
λai
(cid:195)
n(cid:88)
t(cid:88)
i=1
i=t+1
=⇒ (cid:195)L = − log |ΛΛT + Ψ| − tr(ΛΛT + Ψ−1Σ)
= −
n log σ2
i +
i=t+1
i=1
t(cid:88)
(1 + log λai ) +
i=1

log λai + t +
n(cid:88)
i=t+1

We therefore need to minimize

− (cid:195)L = n log σ2
i +

λai

(4)

(Block matrix multiplication)

λai

=

(cid:33)

λai

Since the following holds,
∀i ∈ {1, 2, . . . , t}
• λai > 1
• 1 + log(x) < x
∀x > 1
• f (x) = x − (1 + log x) is an increasing function of x for x > 1.
4 since this is explicitly in the form of an eigenvector decomposition.

10

Factor Analysis using pPCA

Eqn. 4 is minimized over {ai} and t by picking the largest t eigenvalues as λa1 , . . . , λat and also
picking t as high as possible:

t = min(k , number of eigenvalues of ˜S > 1).

7.3 Maximizing w.r.t. Ψ with Λ ﬁxed

Compute the gradient of (cid:195)L w.r.t. Ψ:
∇Ψ (cid:195)L = −(ΛΛT + Ψ)−1 + (ΛΛT + Ψ)−1Σ(ΛΛT + Ψ)−1
Setting the gradient to zero, the extreme values of Ψ must satisfy ∀i, j, i (cid:54)= j :
((ΛΛT + Ψ)−1Σ)ii = 1
Ψii = (Σ − ΛΛT )ii
=⇒
Ψij = 0

7.4 Cases

Case 1: Λ = 0. This is a minimum of (cid:195)L.
Case 2: ΛΛT + Ψ = Σ. This means ΛΛT = Σ − Ψ, which has a solution when up to k eigenvalues of
S − Ψ are nonnegative, and the rest are zero (i.e., S − Ψ must be positive semideﬁnite, meaning all
eigenvalues are nonnegative, but further conditions are required to guarantee a rectangular square
root. The solution is given in its SVD form as Λ = U LV T where U = (u1 , . . . , uk ) ∈ (cid:60)n×k has
eigenvectors of S − Ψ in its columns, L ∈ (cid:60)k×k is a diagonal matrix with the square roots of the
corresponding eigenvalues on the diagonal, and V ∈ (cid:60)k×k is an arbitrary orthogonal matrix.5 This
case seems unlikely in real data because it requires, for example, the sample covariance S to be of
the form S = (rank k matrix + diagonal matrix).

5For an explanation, refer to the deﬁnition of the square-root of a symmetric positive deﬁnite matrix. The ma jor
diﬀerence in our case is that Λ is rectangular, whereas the standard deﬁnition refers to a square matrix as the square-
root. The bridge follows from the following two facts: ﬁrst, the regular square matrix square-root can be obtained by
appending (n − k) zero columns to the rectangular Λ; second, we can have a rectangular square root (only) because
of the condition on zero eigenvalues of S − Ψ, namely, that this latter matrix has rank at most k for a rectangular
decomposition of this form.

Factor Analysis using pPCA

11

8 Appendix C- Data Sets

8.1 Random

Random Data was generated using a natural dimensions of 2. Various higher dimensions and
number of training samples were used, with results being consistent across any such variation. So
the results that were presented were with a total training sample of 60, and n = 5. This allowed
us to have at least one data set that terminated relatively quickly, allowing us to generate quick
results.

8.2 Voting

Below is the voting data set information as taken from the names section of the repository.

Relevant Information:
This data set includes votes for each of the U.S. House of
Representatives Congressmen on the 16 key votes identified by the
CQA.
The CQA lists nine different types of votes: voted for, paired
for, and announced for (these three simplified to yea), voted
against, paired against, and announced against (these three
simplified to nay), voted present, voted present to avoid conflict
of interest, and did not vote or otherwise make a position known
(these three simplified to an unknown disposition).

5. Number of Instances: 435 (267 democrats, 168 republicans)

Nays were treated as -1, Yea’s as 1, Abstaining as 0, and the goal was to predict, based on voting
record, was someone a democrat or a republican.

8.3 Machine

Below is the machine data set information as taken from the names section of the repository.

7. Attribute Information:
1. vendor name: 30
(adviser, amdahl,apollo, basf, bti, burroughs, c.r.d, cambex, cdc, dec,
dg, formation, four-phase, gould, honeywell, hp, ibm, ipl, magnuson,
microdata, nas, ncr, nixdorf, perkin-elmer, prime, siemens, sperry,
sratus, wang)
2. Model Name: many unique symbols
3. MYCT: machine cycle time in nanoseconds (integer)
4. MMIN: minimum main memory in kilobytes (integer)
5. MMAX: maximum main memory in kilobytes (integer)
6. CACH: cache memory in kilobytes (integer)
7. CHMIN: minimum channels in units (integer)
8. CHMAX: maximum channels in units (integer)
9. PRP: published relative performance (integer)
10. ERP: estimated relative performance from the original article (integer)

12

Factor Analysis using pPCA

As there were 8 diﬀerent classes formed by binning, and we needed results per class, we decided
to combine these into 2 classes– 1 above the mean of the global data set, and 1 below. We then
tested if it was above the mean or below.

8.4 Balance

Below is the balance data set information as taken from the names section of the repository.

4. Relevant Information:
This data set was generated to model psychological
experimental results.
Each example is classified as having the
balance scale tip to the right, tip to the left, or be
balanced.
The attributes are the left weight, the left
distance, the right weight, and the right distance.
The
correct way to find the class is the greater of
(left-distance * left-weight) and (right-distance *
If they are equal, it is balanced.
right-weight).

5. Number of Instances: 625 (49 balanced, 288 left, 288 right)

The classes ’L’, ’B’, and ’R’, were converted directly into ascii representation. As the classes are
not used in anyway in the calculation except as denotation, it doesn’t matter – as long as they are
distinct. The goal was to predict the correct class.

Factor Analysis using pPCA

13

9 Appendix D- Code

This section contains only the four relevant scripts used to ascertain our results. Nearly 24 scripts
were written that execute 5 diﬀerent variations of the algorithms discussed. They will be provided
if requested.

9.1 Basic Factor EM

Below is the code for the basic (unoptimized) factor EM algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CS 229 Factor Analysis / EM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Note: This problem asssumes each column is a training sample.

function [mu, Lambda, Psi, lik, numiter] = factorEM(X, k, Psi,
Lambda)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Defining Parameters we need for E-M Factor Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Parameters For E-M Algorithm
[n m] = size(X);
Mu = zeros(k,m);
Sigma = eye(k);
I = eye(k);
error = 0.01;

% Dimensions of X
% For each dimension, for each sample
% A single n x n covariance matrix
% K x K identity
% Error theshold

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Defining our predicitions for E-M Factor Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%What we are trying to predict
mu = mean(X, 2);

% n x 1

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%E-M Factor Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
numiter = 0; PsiOld = Psi + 2 * error; while ((max(abs(diag(Psi -
PsiOld))) > error) && (numiter < 1000))
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Storing Old Value of Psi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
PsiOld = Psi;
numiter = numiter + 1;

%%%%%%%%%%%%%%%%%%%

14

Factor Analysis using pPCA

% E-Step
%%%%%%%%%%%%%%%%%%%
for i = 1:m
Mu(:,i) = Lambda’ * inv(Lambda * Lambda’ + Psi) *(X(:,i) - mu);

end
Sigma = I - Lambda’ * inv(Lambda * Lambda’ + Psi)

* Lambda;

%%%%%%%%%%%%%%%%%%%
% M-Step
%%%%%%%%%%%%%%%%%%%
term1 = 0; term2 = 0;
for i = 1:m
term1 = term1 + (X(:,i) - mu) * Mu(:,i)’;
term2 = term2 + Mu(:,i) * Mu(:,i)’ + Sigma;

end
LambdaNew = term1 * inv(term2);

Phi = 0;
for i = 1:m
Phi = Phi + X(:,i) * X(:,i)’ - X(:,i) * Mu(:,i)’ * Lambda’ - Lambda * Mu(:,i) * X(:,i)’ + Lambda*(Mu(:,i) * Mu(:,i)’ + Sigma)*Lambda’;

end
Psi = diag(diag(Phi/m));
Lambda = LambdaNew;
[l, lik(numiter)] = factorLik(X, mu, Lambda * Lambda’ + Psi);

end

9.2 FactorpPCA with Likelihood convergence test

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CS 229 Factor Analysis / pPCA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Note: This problem asssumes each column is a training sample.

function [mu, Lambda, Psi, lik, numiter] = factorpPCA(X, k, Psi)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Defining Parameters we need for pPCA Factor Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Parameters For pPCA
[n m] = size(X);
I = eye(n);
error = 0.1;

% n x n identity
% Error theshold

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Defining predictions for pPCA Factor Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Factor Analysis using pPCA

15

mu = mean(X, 2);
Lambda = zeros(n,k);

% n x 1
%Just so we know we are "predicting it"

%Calculate sample variance and "old Psi"
S = 0; Xminusmu = X - mu*ones(1,m); for i = 1:m
S = S + Xminusmu(:,i) * Xminusmu(:,i)’;
end
%Sample Variance
S = S / m;
%To Store old Psi Value
PsiOld = Psi - 2 * error;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%pPCA Factor Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
numiter = 0; likNew = 1; likOld = 0;
%Setting initial paramters
while((max(abs(likNew - likOld)) > error) && (numiter < 1000))

%To remove unnecesary repeated matrix computation
numiter = numiter + 1;
PsiSqrt = Psi^0.5; PsiNegSqrt = inv(PsiSqrt);

%Get the largest number of valid eigenvectors/values
eig(PsiNegSqrt * S * PsiNegSqrt); %Get eig vectors/values
[v, d] =
diagD = diag(d);
%Convert d into a vector of lambdas
t = min(k, sum(diagD > 1));
%Find the valid t
[sorted, indices] = sort(diagD, ’descend’); %Find indices with largest eig 1st
validLambda = indices(1:t);
%Get indices of t largest Lambda’s
L = diag(diagD(validLambda) - 1)^0.5;
%Create L for valid Lambda’s
%Create U for valid Lambda’s
U = v(:, validLambda);

%Storing old value of Psi, and calculating Lambda/Psi
PsiOld = Psi;
Psi = diag(diag(S - Lambda * Lambda’));
Lambda = PsiSqrt * cat(2, U * L, zeros(n, k - t));
[l, lik(numiter)] = factorLik(X, mu, Lambda * Lambda’ + Psi);

%Convergence Test
if numiter > 1
likOld = lik(numiter - 1);
likNew = lik(numiter);

end

end

9.3 Test Code

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CS 229 Classification test
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

16

Factor Analysis using pPCA

% Note: This problem places assumes each column is a training sample.
% This also "KNOWS" that factorEM and factorpPCA have a cap of 1000

function [] = test(X, y, t)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% As I didn’t want to restrict this to a particular prespecified dimension,
% I go for each possible classifictaion, caluclate the values, and then
% also calculate the likihood of that value, and classify it accordingly.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparing FactorEM to FactorpPCA on Real Data Sets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%General Paramaters
class = unique(y);
numClass = length(class);
[n m] = size(X);
k = 1:2:n;

%getting classifications
%Number of classes
%numDimensions, numSamples
%Define dimensions to consider

%Iteration Paramters
pPCAiter = zeros(1, length(k));
pPCA10iter = zeros(1, length(k));
pPCA1iter = zeros(1, length(k));
pPCAinititer = zeros(1, length(k));
EMiter = zeros(1,length(k));
EM10iter = zeros(1,length(k));
EM1iter = zeros(1,length(k));
EMinititer = zeros(1,length(k));

%Time Paramters
pPCAtime = zeros(1, length(k));
pPCA10time = zeros(1, length(k));
pPCA1time = zeros(1, length(k));
pPCAinittime = zeros(1, length(k));
EMtime = zeros(1, length(k));
EM10time = zeros(1, length(k));
EM1time = zeros(1, length(k));
EMinittime = zeros(1, length(k));

%Likelihood Parameters
pPCAlik = [];
pPCA10lik = [];
pPCA1lik = [];
pPCAinitlik = [];
pPCATestlik = [];
EMlik = [];

%0 zero average iterations to date
%0 zero average iterations to date
%0 zero average iterations to date
%0 zero average iterations to date
%0 zero average iterations to date
%0 zero average iterations to date
%0 zero average iterations to date
%0 zero average iterations to date

%0 average time used so far
%0 average time used so far
%0 average time used so far
%0 average time used so far
%0 average time used so far
%0 average time used so far
%0 average time used so far
%0 average time used so far

%Initializing to empty
%Initializing to empty
%Initializing to empty
%Initializing to empty
%Initializing to empty
%Initializing to empty

Factor Analysis using pPCA

17

EM10lik = [];
EM1lik = [];
EMinitlik = [];
EMTestlik = [];

%Initializing to empty
%Initializing to empty
%Initializing to empty
%Initializing to empty

%Likelihood Array Parameters
PlikIter = ones(length(k), 1001, numClass);
ElikIter = ones(length(k), 1001, numClass);

%Likelihood array
%Lkelihood array

%Parameters for Test Error
indices = randperm(m);
numTrain = round(m * 0.7);
XTrain = X(:, indices(1:numTrain));
yTrain = y(indices(1:numTrain));
XTest = X(:, indices((numTrain+1):m));
yTest = y(indices((numTrain+1):m));

%Random Permutation of Indices
%Number in Training Sample
%Pick 1st 70% of random index

%Pick last 30% of random index

%Executing Test for each dimension, classification, test/training, etc.
for i = 1:length(k)
%For each dimension
%For each potential classification
for j = 1:numClass

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Training Error
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%FactorpPCA
t = cputime;
%Get Current time and execute
[mu, Lambda, Psi, lik, iter] = factorpPCA(X(:, y == class(j)),k(i), eye(n));
%Getting time difference
pPCAtime(i) = pPCAtime(i) + (cputime - t)/numClass;
pPCAiter(i) = pPCAiter(i) + iter/numClass;
%Summing number of iterations
[pPCAlik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi);
%lik(i,j) = loglik of ith sample in jth class
%Storing likelihood
PlikIter(i, 1:iter, j) = lik;

%Factor EM with FactopPCA initialization
t = cputime;
%Get Current time and execute
[mu, Lambda, Psi, lik, iter] = factorEM(X(:, y == class(j)),k(i), Psi, Lambda);
EMinittime(i) = EMinittime(i) + (cputime - t);
%Getting time difference
EMinititer(i) = EMinititer(i) + iter/numClass;
%Summing number of iterations
[EMinitlik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi); %lik(i,j) = prob of ith sample in jth class

%FactorppCA with Psi * 10
t = cputime;
%Get Current time and execute
[mu, Lambda, Psi, lik, iter] = factorpPCA(X(:, y == class(j)),k(i), eye(n) * 10);
%Getting time difference
pPCA10time(i) = pPCA10time(i) + (cputime - t)/numClass;
pPCA10iter(i) = pPCA10iter(i) + iter/numClass;
%Summing number of iterations
%lik(i,j) = loglik of ith sample in jth class
[pPCA10lik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi);

%FactorppCA with Psi * 0.1

18

Factor Analysis using pPCA

%Get Current time and execute
t = cputime;
[mu, Lambda, Psi, lik, iter] = factorpPCA(X(:, y == class(j)),k(i), eye(n) * 0.1);
%Getting time difference
pPCA1time(i) = pPCA1time(i) + (cputime - t)/numClass;
%Summing number of iterations
pPCA1iter(i) = pPCA1iter(i) + iter/numClass;
[pPCA1lik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi);
%lik(i,j) = loglik of ith sample in jth class

%FactorEM
%Get Current time and execute
t = cputime;
[mu, Lambda, Psi, lik, iter] = factorEM(X(:, y == class(j)),k(i), eye(n), rand(n,k(i)));
%Getting time difference
EMtime(i) = EMtime(i) + (cputime - t);
EMiter(i) = EMiter(i) + iter/numClass;
%Summing number of iterations
[EMlik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi); %lik(i,j) = prob of ith sample in jth class
%Storing likelihood
ElikIter(i, 1:iter, j) = lik;

%FactorpPCA with EM initilization
t = cputime;
%Get Current time and execute
[mu, Lambda, Psi, lik, iter] = factorpPCA(X(:, y == class(j)),k(i), Psi);
%Getting time difference
pPCAinittime(i) = pPCAinittime(i) + (cputime - t)/numClass;
pPCAinititer(i) = pPCAinititer(i) + iter/numClass;
%Summing number of iterations
%lik(i,j) = loglik of ith sample in jth class
[pPCAinitlik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi);

%FactorEM with Psi * 10
%Get Current time and execute
t = cputime;
[mu, Lambda, Psi, lik, iter] = factorEM(X(:, y == class(j)),k(i), eye(n) * 10, rand(n,k(i)));
%Getting time difference
EM10time(i) = EM10time(i) + (cputime - t);
EM10iter(i) = EM10iter(i) + iter/numClass;
%Summing number of iterations
[EM10lik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi); %lik(i,j) = prob of ith sample in jth class

%FactorEM with Psi * 0.1
t = cputime;
%Get Current time and execute
[mu, Lambda, Psi, lik, iter] = factorEM(X(:, y == class(j)),k(i), eye(n) * 0.1, rand(n,k(i)));
%Getting time difference
EM1time(i) = EM1time(i) + (cputime - t);
EM1iter(i) = EM1iter(i) + iter/numClass;
%Summing number of iterations
[EM1lik(:,j), l] = factorLik(X, mu, Lambda * Lambda’ + Psi); %lik(i,j) = prob of ith sample in jth class

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Training Versus Test
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[mu, Lambda, Psi, lik, iter] = factorEM(XTrain(:, yTrain == class(j)),k(i), eye(n), rand(n,k(i)));
[EMTestlik(:,j), l] = factorLik(XTest, mu, Lambda * Lambda’ + Psi); %lik(i,j) = prob of ith sample in jth class
[mu, Lambda, Psi, lik, iter] = factorpPCA(XTrain(:, yTrain == class(j)),k(i), eye(n));
[pPCATestlik(:,j), l] = factorLik(XTest, mu, Lambda * Lambda’ + Psi); %lik(i,j) = prob of ith sample in jth class

end

%Classification error on training samples
pPCAerror(i) = classifyError(pPCAlik, y);
pPCA1error(i) = classifyError(pPCA1lik, y);
pPCA10error(i) = classifyError(pPCA10lik, y);

Factor Analysis using pPCA

19

pPCAiniterror(i) = classifyError(pPCAinitlik, y);
pPCATesterror(i) = classifyError(pPCATestlik, yTest);
EMerror(i) = classifyError(EMlik, y);
EM1error(i) = classifyError(EM1lik, y);
EM10error(i) = classifyError(EM10lik, y);
EMiniterror(i) = classifyError(EMinitlik, y);
EMTesterror(i) = classifyError(EMTestlik, yTest);

end

%Computing GDA
[GDAerror, mu, Sigma] = GDA(XTrain, yTrain, XTest, yTest); GDAerror
= GDAerror * ones(1, length(k));

%Plots
X = ones(1,length(k)); %Defining X axess of plots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% UnNormalized
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Plotting Time it takes
hold off figure(1) plot3(X, k, EMtime, ’-.bo’, X, k, pPCAtime,
’-ro’, X + 1, k, EM1time, ’-.kd’, X + 1, k, pPCA1time, ’-gd’); hold
on plot3(X + 2, k, EM10time, ’-.bx’, X + 2, k, pPCA10time, ’-rx’, X
+ 3, k, EMinittime, ’-.ks’, X + 3, k, pPCAinittime, ’-gs’);
xlabel(’Algorithms’); ylabel(’k’); zlabel(’TIME’); title(’(Full
Set): Time Vs Dim Per Alg’); legend(’EM’, ’pPCA’, ’EM1’, ’pPCA1’,
’EM10’, ’pPCA10’, ’EMinit’, ’pPCAinit’); view(3);

%Plotting number of iterations
hold off figure(2) plot3(X, k, EMiter, ’-.bo’, X, k, pPCAiter,
’-ro’, X + 1, k, EM1iter, ’-.kd’, X + 1, k, pPCA1iter, ’-gd’); hold
on plot3(X + 2, k, EM10iter,
’-.bx’, X + 2, k, pPCA10iter, ’-rx’, X
+ 3, k, EMinititer, ’-.ks’, X + 3, k, pPCAinititer, ’-gs’);
xlabel(’Algorithms’); ylabel(’k’); zlabel(’Number of Iterations’);
title(’(Full Set): # Iter Vs Dim Per Alg’); legend(’EM’, ’pPCA’,
’EM1’, ’pPCA1’, ’EM10’, ’pPCA10’, ’EMinit’, ’pPCAinit’); view(3);

%Plotting error per dimension
hold off figure(3) plot3(X, k, EMerror, ’-.bo’, X, k, pPCAerror,
’-ro’, X + 1, k, EM1error, ’-.kd’,
X + 1, k, pPCA1error, ’-gd’);
hold on plot3(X + 2, k, EM10error, ’-.bx’, X + 2, k, pPCA10error,
’-rx’, X + 3, k, EMiniterror, ’-.ks’, X + 3, k, pPCAiniterror,
’-gs’); xlabel(’Algorithms’); ylabel(’k’); zlabel(’Percent
Classification Error’); title(’(Full Set): Error Vs Dim Per Alg’);
legend(’EM’, ’pPCA’, ’EM1’, ’pPCA1’, ’EM10’, ’pPCA10’, ’EMinit’,
’pPCAinit’); view(3);

20

Factor Analysis using pPCA

%Plotting Test Error
hold off figure(4) plot(k, GDAerror, ’--kx’, k, EMTesterror, ’-.bo’,
k, pPCATesterror, ’-rd’); xlabel(’k’); ylabel(’Percent
Classification Error’);
title(’Test Set- 30%’);
legend(’GDA’, ’EM’, ’pPCA’);

%Plotting likelihood per dimension per class
for i = 1:length(k)
for j = 1:numClass
figure((i-1) * numClass + j + 4)
%Finding data points for pPCA/EM
Pmax = find(PlikIter(i, :, j) > 0);
Emax = find(ElikIter(i, :, j) > 0);
X = 1:(min(Pmax(1), Emax(1)) -1);
PY = PlikIter(i, X, j);
EY = ElikIter(i, X, j);

%Figure N + 4

%Find when it ends
%Find when it ends
%Find the minimum
%Pull out values
%Pull out Values

%Plotting them
plot(X, PY, ’-ro’, X, EY, ’-bd’);
xlabel(’Iteration’);
ylabel(’Likelihood of Training Data’);
title([’k = ’, num2str(k(i)), ’, (Full Set): Sum of LogLik per Iteration’, ’class = ’, num2str(j)]);
legend(’FactorpPCA’, ’EM’);

end

end

9.4 Data Format

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CS 229 Real Data Sets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Note: This problem places in each column a training sample.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Turning off warnings
%%%%%%%%%%%%%%%%%%%%%%%%%%%
s = warning(’off’, ’all’);

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preprocess Data for Badges
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear
MAX = 30;
X = [];
fid = fopen(’badges.data’, ’rt’);

%Clear Variables
%Maximum dimension to consider
%Defining X
%open file

Factor Analysis using pPCA

21

%skip first whie space
fgetl(fid);
%currently at 0th element
i = 0;
%Currently max dimension is 0
maxDim = 0;
%until end of file
while feof(fid) == 0
%go to next element
i = i + 1;
%read 1 line
tline = fgetl(fid);
%Set y = 1 if +, 0 if -
y(i, 1) = (tline(1) == ’+’);
%Whats this dimension (if < MAX)?
dim = min(length(tline) - 2, MAX);
maxDim = max(maxDim, dim);
%Whats largest size dimension?
X = [X , [double(tline((1:dim) + 2))’; zeros(MAX - dim,1)]];
%Concatenate X with new input

end
X = X(1:maxDim, :);
fclose(fid);
test(X, y, ’Badges’);

%Shorten X to valid dimensions
%Close file
%Testing it on read data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preprocess Data for Flags
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Clear Variables
clear
%Defining X
X = [];
%currently at 0th element
i = 0;
fid = fopen(’flag.data’, ’rt’);
%open file
%until end of file
while feof(fid) == 0
%Go to next element
i = i + 1;
tline = fgetl(fid);
%read 1 line
mat = regexp(tline, ’(?<=,\s*)\w*’, ’match’);
y(i, 1) = mat(6);
%Storing Classification
X(:,i) = str2double(mat([1:5,7:16,18:27])); %Dropping Class, text -> numeric
end
fclose(fid);
test(X, str2double(y), ’flags’);

%Close file
%Testing it on read data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preprocess Data for Balance Scale
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Clear Variables
clear
X = [];
%Defining X
%currently at 0th element
i = 0;
fid = fopen(’balance-scale.data’, ’rt’);
%open file
%until end of file
while feof(fid) == 0
i = i + 1;
%Go to next element
tline = fgetl(fid);
%read 1 line
mat = regexp(tline, ’(?<=,\s*)\w*’, ’match’);
y(i, 1) = double(tline(1));
%Storing Classification
X(:,i) = str2double(mat);
%text -> numeric
end
fclose(fid);

%Close file

22

Factor Analysis using pPCA

test(X, y, ’Balance Scale’);

%Testing it on read data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preprocess Data for Machine
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Clear Variables
clear
%Defining X
X = [];
i = 0;
%currently at 0th element
%open file
fid = fopen(’machine.data’, ’rt’);
%until end of file
while feof(fid) == 0
%Go to next element
i = i + 1;
tline = fgetl(fid);
%read 1 line
mat = regexp(tline, ’(?<=,\s*)\w*’, ’match’);
%Storing Classification
y(i, 1) = mat(9);
X(:,i) = str2double(mat(2:7));
%text -> numeric
end
fclose(fid);
y = str2double(y);
test(X, y > mean(y), ’Machine’);

%Close file
%Converting to numeric
%Testing it on read data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preprocess Data for House Votes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Clear Variables
clear
%Defining X
X = [];
j = 0;
%currently at 0th element
fid = fopen(’house-votes-84.data’, ’rt’);
%open file
%until end of file
while feof(fid) == 0
%Go to next element
j = j + 1;
tline = fgetl(fid);
%read 1 line
mat = regexp(tline, ’(?<=,\s*)\S’, ’match’);
%for each dimension
for i = 1:length(mat)
%if voted no
if strcmp(mat(i),’n’)
X(i,j) = -1;
%store -1
%otherwise either ? (0) or Y (1)
else
X(i,j) = strcmp(mat(i),’y’);
%store 1 if yes
or 0 o.w

end

end
y(j,1) = strcmp(tline(1),’r’);
end
fclose(fid);
test(X, y, ’Votes’);

%1 for republican, 0 democrat

%Close file
%Testing it on read data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preprocess Data for Random test Sample
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Clear Variables
clear
n = 5;
%Number of dimensions

Factor Analysis using pPCA

23

m = 30;
%Number of samples
k = 2;
%Smaller dimensions
Psi = diag(rand(n,1));
%Random Psi
Lambda = rand(n,k);
%Random Lambda
mu = rand(n,1);
%Mean for positive class
X = mu * ones(1,m) + Lambda * randn(k, m) + Psi^0.5* randn(n, m);
mu = -1 * rand(n,1);
%Mean for negative class
X = [X, mu * ones(1,m) + Lambda * randn(k, m) + Psi^0.5* randn(n,
m)];
y = [ones(m,1); zeros(m,1)];
test(X, y, ’Random’);

%Inputting Classification
%Testing it on read data

24

Factor Analysis using pPCA

10 Appendix E - More Results

These results are the plots (similar to above of likelihoods)

(a)

(b)

Figure 5: Time convergence- Random, Machine

The rest of the plots associated with this test are not replicated below for sake of alleviating
redundancy as they are similar to the plots below. They are available upon request however as
they reﬂect the beneﬁts of the new convergence test.

11.522.533.5411.522.533.544.5500.511.522.53Algorithms(Full Set): Time Vs Dim Per AlgkTIMEEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.5405101501002003004005006007008009001000Algorithms(Full Set): # Iter Vs Dim Per AlgkNumber of IterationsEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinitFactor Analysis using pPCA

25

The plots below are the results of the other tests we executed on these algorithms. They are
reproduced here for readers critique.

(a)

(b)

(c)

Figure 6: Tested on random data set

(a)

(b)

(c)

Figure 7: Tested on random data set

11.522.533.544.5522242628303234kPercent Classification ErrorTest Set− 30%GDAEMpPCA1234567−205−200−195−190−185−180−175−170−165IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM11.522.533.544.555.56−205−200−195−190−185−180−175−170−165IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM024681012141618−200−195−190−185−180−175−170−165−160IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM1234567−205−200−195−190−185−180−175−170−165IterationLikelihood of Training Datak = 5, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM02468101214−200−195−190−185−180−175−170−165−160IterationLikelihood of Training Datak = 5, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM26

Factor Analysis using pPCA

(a)

(b)

(c)

Figure 8: Tested on machine data sample

(a)

(b)

(c)

Figure 9: Tested on machine data sample

11.522.533.544.554681012141618202224kPercent Classification ErrorTest Set− 30%GDAEMpPCA0102030405060708090−7200−7000−6800−6600−6400−6200−6000−5800−5600−5400IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM01020304050607080−12−10−8−6−4−20x 106IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM0246810121416−7000−6500−6000−5500−5000−4500−4000−3500−3000−2500−2000IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM01020304050607080−12−10−8−6−4−20x 106IterationLikelihood of Training Datak = 5, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM0246810121416−4−3.5−3−2.5−2−1.5−1−0.50x 105IterationLikelihood of Training Datak = 5, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEMFactor Analysis using pPCA

27

(a)

(b)

(c)

Figure 10: Tested on votes data sample

(a)

(b)

(c)

Figure 11: Tested on votes data sample

11.522.533.5405101501002003004005006007008009001000Algorithms(Full Set): # Iter Vs Dim Per AlgkNumber of IterationsEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit11.522.533.540510152345678910Algorithms(Full Set): Error Vs Dim Per AlgkPercent Classification ErrorEMpPCAEM1pPCA1EM10pPCA10EMinitpPCAinit05101555.566.577.588.5kPercent Classification ErrorTest Set− 30%GDAEMpPCA1234567−5500−5400−5300−5200−5100−5000−4900−4800−4700−4600IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM1234567−3200−3100−3000−2900−2800−2700−2600−2500−2400−2300IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM1234567−5400−5300−5200−5100−5000−4900−4800−4700−4600−4500IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM28

Factor Analysis using pPCA

(a)

(b)

(c)

Figure 12: Tested on votes data sample

(a)

(b)

(c)

Figure 13: Tested on votes data sample

(a)

(b)

(c)

Figure 14: Tested on votes data sample

123456789−3200−3100−3000−2900−2800−2700−2600−2500−2400−2300IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM123456789−5400−5300−5200−5100−5000−4900−4800−4700−4600−4500IterationLikelihood of Training Datak = 5, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM12345678910−3400−3200−3000−2800−2600−2400−2200IterationLikelihood of Training Datak = 5, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM12345678910−3400−3200−3000−2800−2600−2400−2200IterationLikelihood of Training Datak = 7, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM02468101214−5600−5400−5200−5000−4800−4600−4400IterationLikelihood of Training Datak = 9, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM12345678910−3400−3200−3000−2800−2600−2400−2200IterationLikelihood of Training Datak = 9, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM02468101214−5600−5400−5200−5000−4800−4600−4400IterationLikelihood of Training Datak = 11, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM1234567891011−3400−3200−3000−2800−2600−2400−2200IterationLikelihood of Training Datak = 11, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM1234567891011−5600−5400−5200−5000−4800−4600−4400IterationLikelihood of Training Datak = 13, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEMFactor Analysis using pPCA

29

(a)

(b)

(c)

Figure 15: Tested on votes data sample

(a)

Figure 16: Tested on votes data sample

(a)

(b)

(c)

Figure 17: Tested on balance data sample

02468101214−3400−3200−3000−2800−2600−2400−2200IterationLikelihood of Training Datak = 13, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM1234567−5800−5600−5400−5200−5000−4800−4600−4400IterationLikelihood of Training Datak = 15, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM024681012−3400−3200−3000−2800−2600−2400−2200IterationLikelihood of Training Datak = 15, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM024681012−5400−5300−5200−5100−5000−4900−4800−4700−4600−4500IterationLikelihood of Training Datak = 7, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM11.21.41.61.822.22.42.62.8351015202530354045kPercent Classification ErrorTest Set− 30%GDAEMpPCA11.522.533.544.55−440−420−400−380−360−340−320IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM11.522.533.544.55−2600−2500−2400−2300−2200−2100−2000−1900IterationLikelihood of Training Datak = 1, (Full Set): Sum of LogLik per Iterationclass = 3FactorpPCAEM30

Factor Analysis using pPCA

(a)

(b)

(c)

Figure 18: Tested on balance data sample

References

[1] Uci machine learning repository.

[2] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University
Press, Baltimore, MD, USA, 1989.

[3] Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge University Press, New
York, NY, USA, 1986.

[4] Andrew Y. Ng. Factor analysis cs229 lecture notes.

[5] M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal
Statistical Society, Series B, 61(Part 3):611–622, 1997.

11.522.533.544.55−440−420−400−380−360−340−320−300IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 1FactorpPCAEM11.522.533.544.55−2600−2500−2400−2300−2200−2100−2000−1900−1800IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 2FactorpPCAEM11.522.533.544.55−2600−2500−2400−2300−2200−2100−2000−1900−1800IterationLikelihood of Training Datak = 3, (Full Set): Sum of LogLik per Iterationclass = 3FactorpPCAEM
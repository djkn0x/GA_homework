Estimating the “wrong” Markov random ﬁeld:
Beneﬁts in the computation-limited setting

Martin J. Wainwright
Department of Statistics, and
Department of Electrical Engineering and Computer Science
UC Berkeley, Berkeley CA 94720
wainwrig@{stat,eecs}.berkeley.edu

Abstract

Consider the problem of joint parameter estimation and prediction in a Markov
random ﬁeld: i.e., the model parameters are estimated on the basis of an ini-
tial set of data, and then the ﬁtted model is used to perform prediction (e.g .,
smoothing, denoising, interpolation) on a new noisy observation. Working in the
computation-limited setting, we analyze a joint method in which the same convex
variational relaxation is used to construct an M-estimator for ﬁtting parameters,
and to perform approximate marginalization for the prediction step. The key re-
sult of this paper is that in the computation-limited setting, using an inconsistent
parameter estimator (i.e., an estimator that returns the “wrong” model e ven in
the inﬁnite data limit) is provably beneﬁcial, since the resulting errors can par
-
tially compensate for errors made by using an approximate prediction technique.
En route to this result, we analyze the asymptotic properties of M-estimators
based on convex variational relaxations, and establish a Lipschitz stability prop-
erty that holds for a broad class of variational methods. We show that joint esti-
mation/prediction based on the reweighted sum-product algorithm substantially
outperforms a commonly used heuristic based on ordinary sum-product. 1

Keywords: Markov random ﬁelds; variational method; message-passing algorithm s; sum-product;
belief propagation; parameter estimation; learning.

1

Introduction

Consider the problem of joint learning (parameter estimation) and prediction in a Markov
random ﬁeld (MRF): in the learning phase, an initial collect
ion of data is used to esti-
mate parameters, and the ﬁtted model is then used to perform p rediction (e.g., smoothing,
interpolation, denoising) on a new noisy observation. Disregarding computational cost,
there exist optimal methods for solving this problem (Route A in Figure 1). For general
MRFs, however, optimal methods are computationally intractable; consequently, many re-
searchers have examined various types of message-passing methods for learning and pre-
diction problems, including belief propagation [3, 6, 7, 14], expectation propagation [5],
linear response [4], as well as reweighted message-passing algorithms [10, 13]. Accord-
ingly, it is of considerable interest to understand and quantify the performance loss incurred

1Work partially supported by Intel Corporation Equipment Grant 22978, an Alfred P. Sloan Foun-
dation Fellowship, and NSF Grant DMS-0528488.

by using computationally tractable methods versus exact methods (i.e., Route B versus A
in Figure 1).

ROUTE A

OPTIMAL PARAMETER
        ESTIMATION

θ∗

OPTIMAL
PREDICTION

PREDICTION  
bz (y , µ; θ∗ )

DATA SOURCE
{xi }

ROUTE B

NEW OBSERVATIONS
y

Error

APPROXIMATE
PREDICTION

PREDICTION  
bz (y , τ ; bθ)

APPROXIMATE  PARAMETER
          ESTIMATION

bθ
Figure 1. Route A: computationally intractable combination of parameter estimation and
prediction. Route B: computationally efﬁcient combination of approximate p arameter esti-
mation and prediction.

It is now well known that many message-passing algorithms —in cluding mean ﬁeld, (gen-
eralized) belief propagation, expectation propagation and various convex relaxations —can
be understood from a variational perspective; in particular, all of these message-passing al-
gorithms are iterative methods solving relaxed forms of an exact variational principle [12].
This paper focuses on the analysis of variational methods based convex relaxations, which
includes a broad range of extant algorithms —among them the tr
ee-reweighted sum-product
algorithm [11], reweighted forms of generalized belief propagation [13], and semideﬁnite
relaxations [12]. Moreover, it is straightforward to modify other message-passing methods
(e.g., expectation propagation [5]) so as to “convexify” th em. At a high level, the key idea
of this paper is the following: given that approximate methods can lead to errors at both
the estimation and prediction phases, it is natural to speculate that these sources of error
might be arranged to partially cancel one another. Our theoretical analysis conﬁrms this
intuition: we show that with respect to end-to-end performance, it is in fact beneﬁcial, even
in the inﬁnite data limit, to learn the “wrong” the model by us
ing an inconsistent parameter
estimator.

More speci ﬁcally, we show how any convex variational method can be used to deﬁne a
surrogate likelihood function. We then investigate the asymptotic properties of parameter
estimators based maximizing such surrogate likelihoods, and establish that they are asymp-
totically normal but inconsistent in general. We then prove that any variational method that
is based on a strongly concave entropy approximation is globally Lipschitz stable. Finally,
focusing on prediction for a coupled mixture of Gaussians, we prove upper bounds on
the increase in MSE of our computationally efﬁcient method,
relative to the unachievable
Bayes optimum. We provide experimental results using the tree-reweighted (TRW) sum-
product algorithm that conﬁrm the stability of our methods, and demonstrate its superior
performance to a heuristic method based on standard sum-product.

2 Background

We begin with necessary notation and background on multinomial Markov random ﬁelds,
as well as variational representations and methods.

Markov random ﬁelds:
Given an undirected graph G = (V , E ) with N = |V | vertices,
we associate to each vertex s ∈ V a discrete random variable Xs , taking values in Xs =
{0, 1 . . . , m − 1}. We assume that the vector X = {Xs | s ∈ V } has a distribution that is

Markov with respect to the graph G, so that its distribution can be represented in the form
p(x; θ) = exp{Xs∈V
θs (xs ) + X(s,t)∈E
(1)
θst (xs , xt ) − A(θ)}
Here A(θ) := log Px∈X N exp (cid:8) Ps∈V θs (xs ) + P(s,t)∈E θst (xs , xt )(cid:9) is the cumulant
generating function that normalizes the distribution, and θs (·) and θst (·, ·) are potential
functions. In particular, we make use of the parameterization θs (xs ) := Pj∈Xs
θs;j I j [xs ],
where I j [xs ] is an indicator function for the event {xs = j }; the quantity θst is deﬁned
analogously. Overall, the family of MRFs (1) is an exponential family with canonical
parameter θ ∈ Rd . Note that the elements of the canonical parameters are associated with
vertices {θs;j , s ∈ V , j ∈ Xs} and edges {θst;jk , (s, t) ∈ E , (j, k) ∈ Xs × Xt} of the
underlying graph.

Variational representation: We now describe how the cumulant generating function can
be represented as the solution of an optimization problem. The constraint set is given
by MARG(G; φ) := (cid:8)µ ∈ Rd | µ = Px∈X N p(x)φ(x) for some p(·)(cid:9), consisting
of all globally realizable singleton µs (·) and pairwise µst (· , ·) marginal distributions on
the graph G. For any µ ∈ MARG(G; φ), we deﬁne A∗ (µ) = − maxp H (p), where
the maximum is taken over all distributions that have mean parameters µ. With these
deﬁnitions, it can be shown [12] that A has the variational representation
µ∈MARG(G;φ) (cid:8)θT µ − A∗ (µ)(cid:9).
max
3 From convex surrogates to joint estimation/prediction

A(θ) =

(2)

:=

B (θ)

In general, solving the variational problem (2) is intractable for two reasons: (i) the con-
straint set MARG(G; φ) is extremely difﬁcult to characterize; and (ii) the dual fun ction
A∗ lacks a closed-form representation. These challenges motivate approximations to A∗
and MARG(G; φ); the resulting relaxed optimization problem deﬁnes a conve x surrogate
to the cumulant generating function.
Convex surrogates: Let REL(G; φ) be a compact and convex outer bound to the marginal
polytope MARG(G; φ), and let B ∗ be a strictly convex and twice continuously differen-
tiable approximation to the dual function A∗ . We use these approximations to deﬁne a
convex surrogate B via the relaxed optimization problem
τ ∈REL(G;φ) (cid:8)θT τ − B ∗ (τ )(cid:9).
max
The function B so deﬁned has several desirable properties. First, since B is deﬁned by the
maximum of a collection of functions linear in θ , it is convex [1]. Moreover, by the strict
convexity of B ∗ and compactness of REL(G; φ), the optimum is uniquely attained at some
τ (θ). Finally, an application of Danskin’s theorem [1] yields that B is differentiable, and
that ∇B (θ) = τ (θ). Since τ (θ) has a natural interpretation as a pseudomarginal, this last
property of B is analogous to the well-known cumulant generating property of A—namely,
∇A(θ) = µ(θ).
One example of such a convex surrogate is the tree-reweighted Bethe free energy consid-
ered in our previous work [11]. For this surrogate, the relaxed constraint set REL(G; φ)
takes the form LOCAL(G; φ) := (cid:8)τ ∈ Rd
+ | Pxs
τs (xs ) = 1, Pxt
τst (xs , xt ) =
τs (xs )(cid:9), whereas the entropy approximation B ∗ is of the “convexi ﬁed” Bethe form
−B ∗ (τ ) = Xs∈V
Hs (τs ) − X(s,t)∈E
ρst Ist (τst ).

(3)

(4)

Here Hs and Ist are the singleton entropy and edge-based mutual information, respectively,
and the weights ρst are derived from the graph structure so as to ensure convexity (see [11]
for more details). Analogous convex variational formulations underlie the reweighted gen-
eralized BP algorithm [13], as well as a log-determinant relaxation [12].
Approximate parameter estimation using surrogate likelihoods: Consider the prob-
lem of estimating the parameter θ using i.i.d. samples {x1 , . . . , xn }. For an MRF of
the form (1), the maximum likelihood estimate (MLE) is speci ﬁed using the vector bµ of
empirical marginal distributions (singleton bµs and pairwise bµst ). Since the likelihood is
intractable to optimize (due to the cumulant generating function A), it is natural to use the
convex surrogate B to deﬁne an alternative estimator obtained by maximizing th e regular-
ized surrogate likelihood:
θ∈Rd (cid:8)θT bµ − B (θ) − λnR(θ)(cid:9). (5)
θ∈Rd (cid:8)LB (θ ; bµ) − λnR(θ)(cid:9) = arg max
bθn
:= arg max
Here R : Rd → R+ is a regularization function (e.g., R(θ) = kθk2 ), whereas λn > 0
is a regularization coefﬁcient. For the tree-reweighted Be the surrogate, we have shown in
previous work [10] that in the absence of regularization, the optimal parameter estimates
bθn have a very simple closed-form solution, speci ﬁed in terms o f the weights ρst and the
empirical marginals bµ. If a regularizing term is added, these estimates no longer have a
closed-form solution, but the optimization problem (5) can still be solved efﬁciently by
message-passing methods.
Joint estimation/prediction: Using such an estimator, we now consider the joint ap-
proach to estimation and prediction illustrated in Figure 2. Using an initial set of i.i.d. sam-
ples, we ﬁrst use the surrogate likelihood (5) to construct a parameter estimate bθn . Given
a new noisy or incomplete observation y , we wish to perform near-optimal prediction or
data fusion using the ﬁtted model (e.g., for smoothing or int erpolation of a noisy image).
In order to do so, we ﬁrst incorporate the new observation int o the model, and then use
the message-passing algorithm associated with the convex surrogate B in order to compute
approximate pseudomarginals τ . These pseudomarginals can then be used to construct a
prediction bz (y ; τ ), where the speci ﬁcs of the prediction depend on the observat
ion model.
We provide a concrete illustration in Section 5 using a mixture-of-Gaussians observation
model.
4 Analysis

Asymptotics of estimator: We begin by considering the asymptotic behavior of the param-
eter estmiator bθn deﬁned by the surrogate likelihood (5). Since this paramete r estimator is
a particular type of M -estimator, the following result follows from standard techniques [8]:
Proposition 1. For a general graph with cycles, bθn converges in probability to some ﬁxed
bθ 6= θ∗ ; moreover, √n[bθn − bθ ] is asymptotically normal.
bθ differs
A key property of the estimator is its inconsistency—i.e., the estimated model
from the true model θ∗ even in the limit of large data. Despite this inconsistency, we will
see that bθn is useful for performing prediction.
Algorithmic stability: A desirable property of any algorithm—particularly one appl
ied
stability with respect to its
to statistical data—is that it exhibit an appropriate form of
inputs. Not all message-passing algorithms have such stability properties. For instance,
the standard BP algorithm, although stable for relatively weakly coupled MRFs [3, 6],
can be highly unstable due to phase transitions. Previous experimental work has shown
that methods based on convex relaxations, including reweighted belief propagation [10],

Generic algorithm for joint parameter estimation and prediction:
1. Estimate parameters bθn from initial data x1 , . . . , xn by maximizing surrogate likeli-
hood LB .
2. Given a new set of observations y , incorporate them into the model:

eθs ( · ; ys ) = bθ
n
s ( · ) + log p(ys | · ).

(6)

3. Compute approximate marginals τ by using the message-passing algorithm associated
with the convex surrogate B . Use approximate marginals to construct prediction bz (y ; τ )
of z based on the observation y and pseudomarginals τ .

Figure 2. Algorithm for joint parameter estimation and prediction. Both the learning and
prediction steps are approximate, but the key is that they are both based on the same under-
lying convex surrogate B . Such a construction yields a provably beneﬁcial cancellation of
the two sources of error (learning and prediction).

reweighted generalized BP [13], and log-determinant relaxations [12] appear to be very
stable. Here we provide theoretical support for these empirical observations: in particular,
we prove that, in sharp contrast to non-convex methods, any variational method based on a
strongly convex entropy approximation is globally stable.
A function f : Rn → R is strongly convex if there exists a constant c > 0 such that
f (y) ≥ f (x) + ∇f (x)T (cid:0)y − x) + c
2 ky − xk2 for all x, y ∈ Rn . For a twice continuously
differentiable function, this condition is equivalent to the eigenspectrum of the Hessian
∇2 f (x) being uniformly bounded away from zero by c. With this deﬁnition, we have:
Proposition 2. Consider any variational method based on a strongly concave entropy ap-
proximation −B ∗ ; moreover, for any parameter θ ∈ Rd , let τ (θ) denote the associated
set of pseudomarginals. If the optimum is attained interior of the constraint set, then there
exists a constant R < +∞ such that
for all θ , δ ∈ Rd .
kτ (θ + δ) − τ (θ)k ≤ Rkδk
Proof. By our construction of the convex surrogate B , we have τ (θ) = ∇B (θ), so that the
statement is equivalent to the assertion that the gradient ∇B is a Lipschitz function. Apply-
ing the mean value theorem to ∇B , we can write ∇B (θ + δ) − ∇B (θ) = ∇2B (θ + tδ)δ
where t ∈ [0, 1]. Consequently, in order to establish the Lipschitz condition, it sufﬁces
to show that the spectral norm of ∇2B (γ ) is uniformly bounded above over all γ ∈ Rd .
Differentiating the relation ∇B (θ) = τ (θ) yields ∇2B (θ) = ∇τ (θ). Now standard sen-
sitivity analysis results [1] yield that ∇τ (θ) = [∇2B ∗ (τ (θ)]−1 . Finally, our assumption
of strong convexity of B ∗ yields that the spectral norm of ∇2B ∗ (τ ) is uniformly bounded
away from zero, which yields the claim.

Many existing entropy approximations, including the convexifed Bethe entropy (4), can be
shown to be strongly concave [9].

5 Bounds on performance loss

We now turn to theoretical analysis of the joint method for parameter estimation and pre-
diction illustrated in Figure 2. Note that given our setting of limited computation, the
Bayes optimum is unattainable for two reasons: (a) it has knowledge of the exact parame-
ter value θ∗ ; and (b) the prediction step (7) involves computing exact marginal probabilities
µ. Therefore, our ultimate goal is to bound the performance loss of our method relative to
the unachievable Bayes optimum. So as to obtain a concrete result, we focus on the spe-
cial case of joint learning/prediction for a mixture-of-Gaussians; however, the ideas and
techniques described here are more generally applicable.

:=

(7)

Prediction for mixture of Gaussians: Suppose that the discrete random vector is a label
vector for the components in a ﬁnite mixture of Gaussians: i. e., for each s ∈ V , the random
variable Zs is speci ﬁed by p(Zs = zs | Xs = j ; θ∗ ) ∼ N (νj , σ2
j ), for j ∈ {0, 1, . . . , m −
1}. Such models are widely used in statistical signal and image processing [2]. Suppose
Ys = αZs + √1 − α2Ws , where
that we observe a noise-corrupted version of Zs —namely
Ws ∼ N (0, 1) is additive Gaussian noise, and the parameter α ∈ [0, 1] speci ﬁes the signal-
to-noise ratio (SNR) of the observation model. (Here α = 0 corresponds to pure noise,
whereas α = 1 corresponds to completely uncorrupted observations.)
With this set-up, it is straightforward to show that the optimal Bayes least squares estimator
(BLSE) of Z takes the form
µs (j ; θ∗ )(cid:20)ωj (α)(cid:0)ys − νj (cid:1) + νj (cid:21),
m−1Xj=0
bzs (y ; µ)
where µs (j ; θ∗ ) is the exact marginal of the distribution p(y | x)p(x; θ∗ ); and ωj (α) :=
ασ2
j +(1−α2 ) is the usual BLSE weighting for a Gaussian with variance σj . For this set-up,
j
α2 σ2
the approximate predictor bzs (y ; τ ) deﬁned by our joint procedure in Figure 2 corresponds
to replacing the exact marginals µ with the pseudomarginals τs (j ; eθ) obtained by solving
the variational problem with eθ .
Bounds on performance loss: We now turn to a comparison of the mean-squared error
(MSE) of the Bayes optimal predictor bz (Y ; µ) to the MSE of the surrogate-based predictor
bz (Y ; τ ). More speci ﬁcally, we provide an upper bound on the increase in MSE, where the
bound is speci ﬁed in terms of the coupling strength and the SN R parameter α. Although
results of this nature can be derived more generally, for simplicity we focus on the case
of two mixture components (m = 2), and consider the asymptotic setting, in which the
number of data samples n → +∞, so that the law of large numbers [8] ensures that the
empirical marginals bµn converge to the exact marginal distributions µ∗ . Consequently, the
MLE converges to the true parameter value θ∗ , whereas Proposition 1 guarantees that our
approximate parameter estimate bθn converges to the ﬁxed quantity bθ . By construction, we
have the relations ∇B (bθ) = µ∗ = ∇A(θ∗ ).
An important factor in our bound is the quantity
σmax (cid:0)∇2A(θ∗ + δ) − ∇2B (bθ + δ)(cid:1),
L(θ∗ ; bθ)
(8)
:= sup
δ∈Rd
where σmax denotes the maximal singular value. Following the argument in the proof of
Proposition 2, it can be seen that L(θ∗ ; bθ) is ﬁnite. Two additional quantities that play a
role in our bound are the differences
and ∆ν (α) := [1 − ω1 (α)]ν1 − [1 − ω0 (α)]ν0 ,
∆ω (α) := ω1 (α) − ω0 (α),
where ν0 , ν1 are the means of the two Gaussian components. Finally, we deﬁ ne γ (Y ; α) ∈
Rd with components log p(Ys |Xs=1)
p(Ys |Xs=0) for s ∈ V , and zeroes otherwise. With this notation,
we state the following result (see the technical report [9] for the proof):
Theorem 1. Let MSE(τ ) and MSE(µ) denote the mean-squared prediction errors of the
surrogate-based predictor bz (y ; τ ), and the Bayes optimal estimate bz (y ; µ) respectively. The
N (cid:2) MSE(τ ) − MSE(µ)(cid:3) is upper bounded by
MSE increase I (α) := 1
I (α) ≤ E(Ω2 (α)∆2
N i)
ω (α)r Ps Y 4
+ 2|∆ν (α)| |∆ω (α)|r Ps Y 2
ν (α) + Ω(α)h∆2
s
s
N
where Ω(α) := min{1, L(θ∗ ; bθ)k γ (Y ;α)
N k}.

IND

BP

TRW

s
s
o
l
 
e
c
n
a
m
r
o
f
r
e
P

50

40

30

20

10

0
0

s
s
o
l
 
e
c
n
a
m
r
o
f
r
e
P

5

4

3

2

1

0
0

0.5

SNR

0.5

1 0

Edge strength

(a)
IND

s
s
o
l
 
e
c
n
a
m
r
o
f
r
e
P

50

40

30

20

10

0
0

s
s
o
l
 
e
c
n
a
m
r
o
f
r
e
P

5

4

3

2

1

0
0

1

1

0.5

SNR

0.5

1 0

Edge strength

(b)
BP

s
s
o
l
 
e
c
n
a
m
r
o
f
r
e
P

50

40

30

20

10

0
0

s
s
o
l
 
e
c
n
a
m
r
o
f
r
e
P

5

4

3

2

1

0
0

1

1

0.5

SNR

0.5

1 0

Edge strength

(c)
TRW

1

1

0.5

SNR

0.5

1 0

Edge strength

0.5

SNR

0.5

1 0

Edge strength

0.5

SNR

0.5

1 0

Edge strength

(f)
(e)
(d)
Figure 3. Surface plots of the percentage increase in MSE relative to Bayes optimum
for different methods as a function of observation SNR and coupling strength. Top row:
Gaussian mixture with components (ν0 , σ2
0 ) = (−1, 0.5) and (ν1 , σ2
1 ) = (1, 0.5). Bot-
tom row: Gaussian mixture with components (ν0 , σ2
0 ) = (0, 1) and (ν0 , σ2
1 ) = (0, 9).
Left column: independence model (IND). Center column: ordinary belief propagation
(BP). Right column: tree-reweighted algorithm (TRW).

It can be seen that I (α) → 0 as α → 0+ and as α → 1− , so that the surrogate-based
method is asymptotically optimal for both low and high SNR. The behavior of the bound
in the intermediate regime is controlled by the balance between these two terms.
Experimental results: In order to test our joint estimation/prediction procedure, we have
applied it to coupled Gaussian mixture models on different graphs, coupling strengths,
observation SNRs, and mixture distributions. Although our methods are more generally
applicable, here we show representative results for m = 2 components, and two differ-
ent mixture types. The ﬁrst ensemble, constructed with mean and variance components
1 ) = (0, 9), mimics heavy-tailed behavior. The second en-
0 ) = (0, 1) and (ν1 , σ2
(ν0 , σ2
0 ) = (−1, 0.5) and (ν1 , σ2
semble is bimodal, with components (ν0 , σ2
1 ) = (1, 0.5). In
both cases, each mixture component is equally weighted. Here we show results for a 2-D
grid with N = 64 nodes. Since the mixture variables have m = 2 states, the coupling
distribution can be written as p(x; θ) ∝ exp (cid:8) Ps∈V θsxs + P(s,t)∈E θstxsxt(cid:9). where
x ∈ {−1, +1}N are spin variables indexing the mixture components. In all trials, we chose
θs = 0 for all nodes s ∈ V , which ensures uniform marginal distributions p(xs ; θ) at each
node. For each coupling strength γ ∈ [0, 1], we chose edge parameters as θst ∼ U [0, γ ],
and we varied the SNR parameter α controlling the observation model in [0, 1]. We eval-
uated the following three methods based on their increase in mean-squared error (MSE)
over the Bayes optimal predictor (7): (a) As a baseline, we used the independence model
for the mixture components: parameters are estimated θs (xs ) = log bµs (xs ), and setting
coupling terms θst (xs , xt ) equal to zero. The prediction step reduces to performing BLSE
at each node independently. (b) The standard belief propagation (BP) approach is based on
estimating parameters (see step (1) of Figure 2) using ρst = 1 for all edges (s, t), and using
BP to compute the pseudomarginals. (c) The tree-reweighted method (TRW) is based on
estimating parameters using the tree-reweighted surrogate [10] with weights ρst = 1
2 for all
edges (s, t), and using the TRW sum-product algorithm to compute the pseudomarginals.

Shown in Figure 3 are 2-D surface plots of the average percentage increase in MSE, taken
over 100 trials, as a function of the coupling strength γ ∈ [0, 1] and the observation SNR
parameter α ∈ [0, 1] for the independence model (left column), BP approach (middle col-
umn) and TRW method (right column). For weak coupling (γ ≈ 0), all three methods —
including the independence model —perform quite well, as sho uld be expected given the
weak dependency. Although not clear in these plots, BP outperforms TRW for weak cou-
pling; however, both methods lose than than 1% in this regime. As the coupling is in-
creased, the BP method eventually deteriorates quite seriously; indeed, for large enough
coupling and low/intermediate SNR, its performance can be worse than the independence
model. Looking at alternative models (in which phase transitions are known), we have
found that this rapid degradation co-incides with the appearance of multiple ﬁxed points.
In contrast, the behavior of the TRW method is extremely stable, consistent with our theory.

6 Conclusion

We have described and analyzed joint methods for parameter estimation and predic-
tion/smoothing using variational methods that are based on convex surrogates to the cu-
mulant generating function. Our results —both theoretical a nd experimental —conﬁrm the
intuition that in the computation-limited setting, in which errors arise from approximations
made both during parameter estimation and subsequent prediction, it is provably beneﬁcial
to use an inconsistent parameter estimator. Our experimental results on the coupled mixture
of Gaussian model conﬁrm the theory: the tree-reweighted su m-product algorithm yields
prediction results close to the Bayes optimum, and substantially outperforms an analogous
but heuristic method based on standard belief propagation.

References

[1] D. Bertsekas. Nonlinear programming. Athena Scientiﬁc, Belmont, MA, 1995.
[2] M. Crouse, R. Nowak, and R. Baraniuk. Wavelet-based statistical signal processing using
hidden Markov models. IEEE Trans. Signal Processing, 46:886–902, April 1998.
[3] A. Ihler, J. Fisher, and A. S. Willsky. Loopy belief propagation: Convergence and effects of
message errors. Journal of Machine Learning Research, 6:905–936, May 2005.
[4] M. A. R. Leisink and H. J. Kappen. Learning in higher order Boltzmann machines using linear
response. Neural Networks, 13:329–335, 2000.
[5] T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT,
January 2001.
[6] S. Tatikonda and M. I. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncer-
tainty in Arti ﬁcial Intelligence , volume 18, pages 493–500, August 2002.
[7] Y. W. Teh and M. Welling. On improving the efﬁciency of the iterative pr oportional ﬁtting
procedure. In Workshop on Artiﬁcial Intelligence and Statistics , 2003.
[8] A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, Cambridge, UK, 1998.
[9] M. J. Wainwright. Joint estimation and prediction in Markov random ﬁeld s: Beneﬁts of incon-
sistency in the computation-limited regime. Technical Report 690, Department of Statistics,
UC Berkeley, 2005.
[10] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. Tree-reweighted belief propagation algo-
rithms and approximate ML estimation by pseudomoment matching. In Workshop on Artiﬁcial
Intelligence and Statistics, January 2003.
[11] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log
partition function. IEEE Trans. Info. Theory, 51(7):2313–2335, July 2005.
[12] M. J. Wainwright and M. I. Jordan. A variational principle for graphical models.
Directions in Statistical Signal Processing. MIT Press, Cambridge, MA, 2005.
[13] W. Wiegerinck. Approximations with reweighted generalized belief propagation. In Workshop
on Artiﬁcial Intelligence and Statistics , January 2005.
[14] J. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free energy approximations and gener-
alized belief propagation algorithms. IEEE Trans. Info. Theory, 51(7):2282–2312, July 2005.

In New


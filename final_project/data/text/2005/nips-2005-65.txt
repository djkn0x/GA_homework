Response Analysis of Neuronal Population with
Synaptic Depression

Wentao Huang
Institute of Intelligent Information
Processing, Xidian University,
Xi’an 710071, China
wthuang@mail.xidian.edu.cn

Licheng Jiao
Institute of Intelligent Information
Processing, Xidian University,
Xi’an 710071, China
lchjiao@mail.xidian.edu.cn

Shan Tan
Institute of Intelligent Information
Processing, Xidian University,
Xi’an 710071, China
shtan@mail.xidian.edu.cn

Maoguo Gong
Institute of Intelligent Information
Processing, Xidian University,
Xi’an 710071, China
mggong@mail.xidian.edu.cn

Abstract

In this paper, we aim at analyzing the characteristic of neuronal popula-
tion responses to instantaneous or time-dependent inputs and the role of
synapses in neural information processing. We have derived an evolu-
tion equation of the membrane potential density function with synaptic
depression, and obtain the formulas for analytic computing the response
of instantaneous (cid:2)re rate. Through a technical analysis, we arrive at sev-
eral signi(cid:2)cant conclusions: The background inputs play an important
role in information processing and act as a switch betwee temporal inte-
gration and coincidence detection. the role of synapses can be regarded
as a spatio-temporal (cid:2)lter; it is important in neural information process-
ing for the spatial distribution of synapses and the spatial and temporal
relation of inputs. The instantaneous input frequency can affect the re-
sponse amplitude and phase delay.

1 Introduction

Noise has an important impact on information processing of the nervous system in vivo. It
is signi(cid:2)cance for us to study the stimulus-and-response behavior of neuronal populations,
especially to transients or time-dependent inputs in noisy environment, viz. given this sto-
chastic environment, the neuronal output is typically characterized by the instantaneous
(cid:2)ring rate. It has come in for a great deal of attention in recent years[1-4]. Moreover, it
is revealed recently that synapses have a more active role in information processing[5-7].
The synapses are highly dynamic and show use-dependent plasticity over a wide range
of time scales. Synaptic short-term depression is one of the most common expressions
of plasticity. At synapses with this type of modulation, pre-synaptic activity produces a
decrease in synaptic. The present work is concerned with the processes underlying in-
vestigating the collectivity dynamics of neuronal population with synaptic depression and

the instantaneous response to time-dependence inputs. First, we deduce a one-dimension
Fokker-Planck (FP) equation via reducing the high-dimension FP equations. Then, we de-
rive the stationary solution and the response of instantaneous (cid:2)re rate from it. Finally, the
models are analyzed and discussed in theory and some conclusions are presented.

2 Models and Methods

2.1 Single Neuron Models and Density Evolution Equations

(cid:28) v

Our approach is based on the integrate-and-(cid:2)re(IF) neurons. The population density based
on the integrate-and-(cid:2)re neuronal model is low-dimensional and thus can be computed
ef(cid:2)ciently, although the approach could be generalized to other neuron models. It is com-
pletely characterized by its membrane potential below threshold. Details of the generation
of an action potential above the threshold are ignored. Synaptic and external inputs are
summed until it reaches a threshold where a spike is emitted. The general form of the
dynamics of the membrane potential v in IF model can be written as
NXk=1
dv(t)
Jk (t)(cid:14)(t (cid:0) tsp
= (cid:0)v(t) + Se (t) + (cid:28) v
k );
dt
where 0 (cid:20) v (cid:20) 1, (cid:28) v is the membrane time constant, Se (t) is an external current directly
injected in the neuron, N is the number of synaptic connections, tsp
k is occurring time of
the (cid:2)ring of a presynaptic neuron k and obeys a Poisson distribution with mean (cid:21)k , Jk (t)
is the ef(cid:2)cacy of synapse k . The transmembrane potential, v , has been normalized so that
v = 0 marks the rest state, and v = 1 the threshold for (cid:2)ring. When the latter is achieved, v
is reset to zero. Jk (t) = ADk (t), where A is a constant representing the absolute synaptic
ef(cid:2)cacy corresponding to the maximal postsynaptic response obtained if all the synaptic
resources are released at once, and Dk (t) act in accordance with complex dynamics rule.
We use the phenomenological model by Tsodyks & Markram [7] to simulate short-term
synaptic depression:

(1)

(cid:28) v

dv(t)
dt

(1 (cid:0) Dk (t))
dDk (t)
(2)
(cid:0) UkDk (t)(cid:14)(t (cid:0) tsp
k );
=
dt
(cid:28) d
where Dk is a ‘depression’ variable, Dk 2 [0; 1], (cid:28) d is the recovery time constant, Uk is a
constant determining the step decrease in Dk . Using the diffusion approximation, we can
get from (1) and (2)
NXk=1
ADk ((cid:21)k + p(cid:21)k (cid:24) k (t));
= (cid:0)v(t) + Se (t) + (cid:28) v
(cid:0) UkDk ((cid:21)k + p(cid:21)k (cid:24) k (t)):
dDk (t)
(1 (cid:0) Dk )
(cid:28) d
dt
The Fokker-Planck equation of equations (3) is
NXk=1
NXk=1
@
@Dk
NXk=1
((cid:21)kU 2
k D2
(cid:21)kA2D2
k p) +
k p)g;
KDk =

( (cid:0)v + Kv
(cid:28) v
NXk=1
@ 2
@ v2 (
NXk=1
(cid:28) v (cid:21)kADk ;

@ p(t; v ; D)
@ t

(1 (cid:0) Dk )
(cid:28) d

(cid:0) (cid:21)kUkDk :

((cid:21)kAUkD2
k p)

@
@ v

= (cid:0)

+

1
2 f

Kv = Se +

p) (cid:0)

(KDk p) (cid:0)

@
@ v@Dk

(3)

(4)

=

@ 2
@D2
k

where D = (D1 ; D2 ; :::DN ), and

Z 1
pd (t; Djv)dD = 1:
p(t; v ; D) = pd (t; Djv)pv (t; v);
(cid:0)1
We assume that D1 ; D2 ; :::DN are uncorrelated, then we have
NYk=1
~pk
pd (t; Djv) =
d (t; Dk jv);
where ~pk
d (t; Dk jv) is the conditional probability density. Moreover, we can assume
d (t; Dk jv) (cid:25) pk
~pk
d (t; Dk ):

pv

(AUkD2
k (cid:21)k pv pd )+

(cid:28) v

Substituting (5) into (4), we get
@
@ pv
@ pd
@ v
@ t
@ t

+ pv

pd

( (cid:0)v + Kv
(cid:28) v

where

@ 2
@D2
k

@ 2pv (t; v)
@ v2

pv pd )(cid:0)
NXk=1
(cid:21)kA2D2
k pv pd ) +

= (cid:0)
NXk=1
@
@
(KDk pd ) (cid:0)
@Dk
@ v@Dk
NXk=1
NXk=1
@ 2
1
@ v2 (
2 f
Integrating Eqation (8) over D, we get
@
@ pv (t; v)
((cid:0)v + ~Kv )pv (t; v) +
= (cid:0)
@ t
@ v
~Kv = Z Kv pddD =Se +
NXk=1
NXk=1
(cid:28) v (cid:21)kAmk ; Qv =
(cid:13) k = Z D2
mk = Z Dk pk
k pk
d (t; Dk )dDk ;
d (t; Dk )dDk ;
d (t; Dk ) satis(cid:2)es the following equation Fokker-Planck equation
and pk
@ pk
@ 2
@
1
d
= (cid:0)
@D2
@Dk
@ t
2
k
From (10) and (11), we can get
dmk
dt
d(cid:13) k
dt

1
(cid:28) d
+ (2U (cid:0) U 2 )(cid:21)k )(cid:13) k +

= (cid:0)(
= (cid:0)(

k (cid:21)k pk
k D2
(U 2
d ):

(KDk pk
d ) +

+ U (cid:21)k )mk +

1
(cid:28) d
2
(cid:28) d

2mk
(cid:28) d

Qv
2

;

;

:

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

k D2
((cid:21)kU 2
k pv pd )g:

(cid:28) v (cid:21)kA2(cid:13) k ;

Let

Qv
2(cid:28) v

)pv (t; v) (cid:0)

Jv (t; v) = ( (cid:0)v + ~Kv
(cid:28) v
(13)
r(t) = Jv (t; 1);
where Jv (t; v) is the probability (cid:3)ux of pv , r(t) is the (cid:2)re rate. The boundary conditions of
equation (9) are
Z 1
0

@ pv (t; v)
@ v

r(t) = Jv (t; 0):

pv (t; v)dv = 1;

pv (t; 1) = 0;

(14)

;

2.2 Stationary Solution and Response Analysis

exp[

When the system is in the stationary states, @ pv =@ t = 0; dmk =dt = 0; d(cid:13) k =dt = 0;
k and (cid:21)k (t) = (cid:21)0
k . are time-
pv (t; v) = p0
v (v); r(t) = r0 ; mk (t) = m0
k ; (cid:13) k (t) = (cid:13) 0
independent. From (9), (12), (13) and (14), we get
] Z 1
(v 0 (cid:0) ~K 0
(v (cid:0) ~K 0
v )2
v )2
2(cid:28) v r0
]dv 0 ; 0 (cid:20) v (cid:20) 1;
p0
v (v) =
exp[(cid:0)
Q0
Q0
Q0
v
v
v
v
) + erf (u)]du1CA
r0 = 0B@(cid:28) vp(cid:25) Z 1(cid:0) ~K0
(cid:0)1
vpQ0
~K 0
vpQ0
exp(u2 )[erf (
v
(cid:0) ~K0
vpQ0
v
v
NXk=1
NXk=1
~K 0
(cid:28) v A(cid:21)0
(cid:28) v A2(cid:21)0
Q0
km0
k (cid:13) 0
v = Se +
k ;
k ;
v =
2m0
1
k
k )(cid:21)0
1 + Uk (cid:28) d(cid:21)0
2 + (cid:28) d (2Uk (cid:0) U 2
k
k
Sometimes, we are more interested in the instantaneous response to time-dependence ran-
dom (cid:3)uctuation inputs. The inputs take the form:
k (1 + "k (cid:21)1
(cid:21)k = (cid:21)0
k (t));
where "k (cid:28) 1. Then mk and (cid:13) k have the forms, i.e.,
k (t) + O("2
k (1 + "km1
mk = m0
k ));
(cid:13) k = (cid:13) 0
k (1 + "k (cid:13) 1
k (t) + O("2
k ));

m0
k =

(cid:13) 0
k =

(16)

(15)

(17)

;

;

:

and ~Kv and Qv are

Qv =

k ((cid:21)1
"k (cid:28) v A(cid:21)0
k )) + O("2
k + m1
km0
k );

NXk=1
k ((cid:21)1
"k (cid:28) v A2(cid:21)0
k ) + O("2
k + (cid:13) 1
k (cid:13) 0
k ):

NXk=1
(cid:28) v A2(cid:21)0
k (cid:13) 0
k +

~Kv = Se +
(cid:28) v A(cid:21)0
km0
k +
NXk=1
NXk=1
Substituting (17) into (12), and ignoring the high order item, it yields:
dm1
k
dt
d(cid:13) 1
k
dt

k (cid:21)1
k (cid:0) Uk (cid:21)0
+ Uk (cid:21)0
k )m1
k (t);
k )(cid:21)0
k )(cid:13) 1
+ (2Uk (cid:0) U 2
k +

2m1
k (cid:21)1
k )(cid:21)0
(cid:28) d (cid:0) (2Uk (cid:0) U 2
k
k (t):

1
(cid:28) d
2
(cid:28) d

= (cid:0)(

= (cid:0)(
With the de(cid:2)nitions

v + (cid:15) ~K 1
~Kv = ~K 0
v (t) + O((cid:15)2 );
v + (cid:15)Q1
Qv = Q0
v (t) + O((cid:15)2 );
pv = p0
v + (cid:15)p1 (t) + O((cid:15)2 );
r = r0 + (cid:15)r1 (t) + O((cid:15)2 );
where (cid:15) (cid:28) 1; and boundary conditions of p1
Z 1
p1 (t; 1) = 0;
0

p1 (t; v)dv = 0;

(18)

(19)

(20)

(21)

using the perturbative expansion in powers of (cid:15); we can get

;

;

;

(cid:28) v

Q0
v
2

@
@ v
@
@ v

Qv
2
@ 2 p1
@ v2 (cid:0)

@ 2p0
v (v)
@ v2
@ f0 (t; v)
@ v

((cid:0)v + ~K 0
v )p0
v (v) +
0 = (cid:0)
@ p1
((cid:0)v + ~K 0
v )p1 +
= (cid:0)
@ t
@ p0
Q1
v (t)
f0 (t; v) = ~K 1
v
v (t)p0
v (cid:0)
@ v
2
Q1
Q0
@ p0
v (t)
@ p1 (t; 1)
v (1)
v
r1 = (cid:0)
(cid:0)
2(cid:28) v
@ v
2(cid:28) v
@ v
For the oscillatory inputs ~K 1
v (t) = k(!)ej!t , Q1
v (t) = q(!)ej!t , the output has the same
frequency and takes the forms p1 (t; v) = p! (! ; v)ej!t ; @ p1 =@ t = j!p1 .
For inputs that vary on a slow enough time scale, satisfy (cid:28) v ! (cid:28) 1; we de(cid:2)ne
(cid:15)l = (cid:28) v ! ;
1 + O((cid:15)2
1 + (cid:15)l p1
p1 = p0
l );
1 + O((cid:15)2
1 + (cid:15)l r1
r1 = r0
l ):
Using the perturbative expansion in powers of (cid:15)l ; we get

(22)

(23)

:

]dv 0 ;

rn
1 =

pn
1 =

Fn exp[

]dv 0 dv ;

Q0
v
2
Q0
v
2

@ 2 p0
1
@ v2 ;
@ 2 p1
1
@ v2 :

@ f0 (t; v)
@ v

2
exp[(cid:0)
Q0
v
v Z 1
2r0
Q0
0
F0 = f0 (t; v);

@
((cid:0)v + ~K 0
v )p0
= (cid:0)
1 +
@ v
@
((cid:0)v + ~K 0
j p0
v )p1
1 = (cid:0)
1 +
@ v
The solutions of equtions (24) are
] Z 1
(v 0 (cid:0) ~K 0
(v (cid:0) ~K 0
v )2
v )2
((cid:28) v rn
1 (cid:0) Fn ) exp[
Q0
Q0
v
v
v
] Z 1
(v 0 (cid:0) ~K 0
(v (cid:0) ~K 0
v )2
v )2
exp[(cid:0)
Q0
Q0
v
v
v
F1 = j Z v
1 (v 0 )dv 0 ;
n = 0; 1.
p0
0
v (t), then we have
In general, Q1
v (t) (cid:28) ~K 1
F0 = f0 (t; v) (cid:25) ~K 1
v (t)p0
v :
From (23), (25) and (26), we can get
] Z 1
v (t) Z 1
(v 0 (cid:0) ~K 0
(v (cid:0) ~K 0
v )2
v )2
2r0
]dv 0 dv + j!(cid:28) v(cid:2)
~K 1
r1 (cid:25)
exp[(cid:0)
Q0
Q0
Q0
v
0
v
v
v
[Z v 0
] Z 1
v Z 1
(v 0 (cid:0) ~K 0
(v (cid:0) ~K 0
v )2
v )2
2r0
1 (v 00 )dv 00 ] exp[
p0
exp[(cid:0)
Q0
Q0
Q0
v
0
0
v
v
In the limit of high frequency inputs, i.e. 1=(cid:28) v ! (cid:28) 1; with the de(cid:2)nitions
1
(cid:15)h =
;
(cid:28) v !
h + O((cid:15)2
h + (cid:15)hp1
p1 = p0
h );

p0
v exp[

]dv 0 dv :

(24)

(25)

(26)

(27)

(28)

;

+ O((cid:15)2
h )

we obtain

Q0
v
2(cid:28) v

p1
h = j

p0
h = 0;

@ f0 (t; v)
@ v
Q1
@ 2 f0 (t; 1)
@ p0
v (t)
v (1)
r1 = (cid:0)
@ v (cid:0) j (cid:15)h
@ v2
2(cid:28) v
Q1
@ 2p0
Q0
v (t)
v (1)
( ~K 1
v
r0 (cid:0) j (cid:15)h
v (t)
@ v2 (cid:0)
2(cid:28) v
Q0
(cid:18)(1 (cid:0) ~K 0
2j (cid:15)h ~K 1
Q1
v (t)r0
v (t)r0
v ) (cid:0)
v (cid:0)
Q0
Q0
v
When Q1
v (t), we have
v (t) (cid:28) ~K 1
r1 (cid:25)

2j ~K 1
v (t)r0
(cid:28) v !Q0
v

Q1
v (t)r0
v (cid:0)
Q0

(cid:25)

=

Q1
@ 3 p0
v (t)
v
@ v3 )
2
v (cid:17)(cid:19) :
v (cid:16)1 (cid:0) ~K 0
Q1
v (t)
v (cid:0) Q0
~K 1
v (t)Q0

(1 (cid:0) ~K 0
v )(1 (cid:0)

Q1
v (t)
~K 1
v (t)Q0
v

);

(29)

(30)

3 Discussion

;

:

(31)

Q0
v (cid:25)

v re(cid:3)ects the
v re(cid:3)ects the average intensity of background inputs and Q0
In equation (15), ~K 0
intensity of background noise. When 1 (cid:28) (cid:28) dUk (cid:21)0
k , we have
NXk=1
(cid:28) v A
~K 0
v (cid:25) Se +
(cid:28) dUk
NXk=1
(cid:28) v A2
(cid:28) dUk (1 + (cid:28) dUk (cid:21)0
k (1 (cid:0) Uk =2))
k has little in(cid:3)uence on ~K 0
From (31), we can know the change of background inputs (cid:21)0
v
v which decreases
which is dominated by parameter (cid:28) v A=(cid:28) dUk , but more in(cid:3)uence on Q0
k increasing.
with (cid:21)0
In the low input frequency regime, from (27), we can know that the input frequency !
increasing will result in the response amplitude and the phase delay increasing. However,
in the high input frequency limit regime, from (30), we can know the input frequency !
increasing will result in the response amplitude and the phase delay decreasing. More-
over, from (27) and (30), we know the stationary background (cid:2)re rate r0 play an important
part in response to changes in (cid:3)uctuation outputs. The instantaneous response r1 increases
monotonically with background (cid:2)re rate r0 :But the background (cid:2)re rate r0 is a function
v (cid:13)(cid:13)(cid:13) re(cid:3)ects the response amplitude,
v : In equation (27), (cid:13)(cid:13)(cid:13)r1 = ~K 1
of the background noise Q0
and in equation (30), r0 =Q0
v re(cid:3)ects the response amplitude. As Figure 1 (A) and (B) show
that (cid:13)(cid:13)(cid:13)r1 = ~K 1
v (cid:13)(cid:13)(cid:13) and r0 =Q0
v changes with variables Q0
v and ~K 0
v respectively. We can know,
for the subthreshold regime ( ~K 0
v < 1), they increase monotonically with Q0
v when ~K 0
v is a
constant. However, for the suprathreshold regime ( ~K 0
v > 1), they decrease monotonically
with Q0
v when ~K 0
v is a constant. When inputs remain, if the instantaneous response ampli-
tude increases, then we can take for the role of neurons are more like coincidence detection
than temporal integration. And from this viewpoint, it suggests that the background in-
puts play an important role in information processing and act as a switch between temporal
integration and coincidence detection.
In equation (16), if the inputs take the oscillatory form, (cid:21)1
k (t) = ej!t ; according to (19),

v (cid:13)(cid:13)(cid:13) (for equation (27))
(A) (cid:13)(cid:13)(cid:13)r1 = ~K 1
v and eK 0
Figure 1: Response amplitude versus Q0
v .
v and ~K 0
changes with Q0
v (for equation (30)) changes with Q0
v . (B) r0 =Q0
v and ~K 0
v .

we get

;

(32)

m1
k = (cid:0)

(cid:28) dUk (cid:21)0
k ej (!t(cid:0)(cid:18)m )
q((cid:28) d!)2 + (1 + (cid:28) dUk (cid:21)0
k )2
k =q((cid:28) d!)2 + (1 + (cid:28) dUk (cid:21)0
where (cid:18)m =arctg(
) is the phase delay, (cid:28) dUk (cid:21)0
k )2 is
(cid:28) d!
1+(cid:28) dUk (cid:21)0
k
the amplitude. The minus shows it is a ‘depression’ response amplitude. The phase delay
increases with the input frequency ! and decreases with the background input (cid:21)0
k . The
‘depression’ response amplitude decrease with the input frequency ! and increase with the
k . The equations (15) (18), (12), (19), (27), (30) and (32) show us a
background input (cid:21)0
point of view that the synapses can be regarded as a time-dependent external (cid:2)eld which
impacts on the neuronal population through the time-dependent mean and variance. We
assume the inputs are composed of two parts, viz. (cid:21)1
2 ej!t ; then we can
k1 (t) = (cid:21)1
k2 (t) = 1
get m1
and m1
. However, in general m1
, this suggest for us that the
k 6= m1
+ m1
k1
k2
k1
k2
spatial distribution of synapses and inputs is important on neural information processing.
In conclusion, the role of synapses can be regarded as a spatio-temporal (cid:2)lter. Figure 2 is
the results of simulation of a network of 2000 neurons and the analytic solution for equation
(15) and equation (27) in different conditions.

4 Summary

In this paper, we deal with the model of the integrate-and-(cid:2)re neurons with synaptic cur-
rent dynamics and synaptic depression. In Section 2, (cid:2)rst, using the membrane potential
equation (1) and combining the synaptic depression equation (2), we derive the evolution
equation (4) of the joint distribution density function. Then, we give an approach to cut
the evolution equation of the high dimensional function down to one dimension, and get
equation (9). Finally, we give the stationary solution and the response of instantaneous (cid:2)re
rate to time-dependence random (cid:3)uctuation inputs. In Section 3, the analysis and discus-
sion of the model is given and several signi(cid:2)cant conclusions are presented. This paper can
only investigate the IF neuronal model without internal connection. We can also extend to
other models, such as the non-linear IF neuronal models of sparsely connected networks of
excitatory and inhibitory neurons.

Figure 2: Simulation of a network of 2000 neurons (thin solid line) and the analytic solution
(thick solid line) for equation (15) and equation (27), with (cid:28) v = 15(ms), (cid:28) d = 1(s),
A = 0:5, Uk = 0:5, N = 30, ! = 6:28(Hz); (cid:21)1
k = sin(!t), "k (cid:21)0
k = 10(Hz), (cid:21)0
k = 70(Hz)
(A and C) and 100(Hz) (B and D), Se = 0:5(A and B) and 0:8(C and D). The horizontal
axis is time (0-2s), and the longitudinal axis is the (cid:2)re rate.

References

[1] Fourcaud N. & Brunel, N. (2005) Dynamics of the Instantaneous Firing Rate in Response to
Changes in Input Statistics. Journal of Computational Neuroscience 18(3):311-321.
[2] Fourcaud, N. & Brunel, N. (2002) Dynamics of the Firing Probability of Noisy Integrate-and-Fire
Neurons. Neural Computation 14(9):2057-2110.
[3] Gerstner, W. (2000) Population Dynamics of Spiking Neurons: Fast Transients, Asynchronous
States, and Locking. Neural Computation 12(1):43-89.
[4] Silberberg, G., Bethge, M., Markram, H., Pawelzik, K. & Tsodyks, M. (2004) Dynamics of
Population Rate Codes in Ensembles of Neocortical Neurons. J Neurophysiol 91(2):704-709.
[5] Abbott, L.F. & Regehr, W.G. (2004) Synaptic Computation. Nature 431(7010):796-803.
[6] Destexhe, A. & Marder, E. (2004) Plasticity in Single Neuron and Circuit Computations. Nature
431(7010):789-795.
[7] Markram, H., Wang, Y. & Tsodyks, M. (1998) Differential Signaling Via the Same Axon of
Neocortical Pyramidal Neurons. Proc Natl Acad Sci USA 95(9):5323-5328.


Rodeo: Sparse Nonparametric Regression in
High Dimensions

John Lafferty
School of Computer Science
Carnegie Mellon University

Larry Wasserman
Department of Statistics
Carnegie Mellon University

Abstract

We present a method for nonparametric regression that performs band-
width selection and variable selection simultaneously. The approach is
based on the technique of incrementally decreasing the bandwidth in di-
rections where the gradient of the estimator with respect to bandwidth
is large. When the unknown function satis ﬁes a sparsity condi
tion, our
approach avoids the curse of dimensionality, achieving the optimal mini-
max rate of convergence, up to logarithmic factors, as if the relevant vari-
rodeo (regularization
ables were known in advance. The method—called
of derivative expectation operator) —conducts a sequence of
hypothesis
tests, and is easy to implement. A modi ﬁed version that repla ces hard
with soft thresholding effectively solves a sequence of lasso problems.

1

Introduction

Estimating a high dimensional regression function is notoriously difﬁcult due to the
“curse of dimensionality.” Minimax theory precisely chara cterizes the curse. Let Yi =
i = 1, . . . , n where Xi = (Xi (1), . . . , Xi (d)) ∈ Rd is a d-dimensional
m(Xi ) + ǫi ,
covariate, m : Rd → R is the unknown function to estimate, and ǫi ∼ N (0, σ2 ). Then if
m is in W2 (c), the d-dimensional Sobolev ball of order two and radius c, it is well known
that
n4/(4+d) inf
(1)
m∈W2 (c) R( bmn , m) > 0 ,
sup
lim inf
n→∞
bmn
where R( bmn , m) = Em R ( bmn (x) − m(x))2 dx is the risk of the estimate bmn constructed
on a sample of size n (Gy ¨or ﬁ et al. 2002). Thus, the best rate of convergence is n−4/(4+d) ,
which is impractically slow if d is large.
However, for some applications it is reasonable to expect that the true function only depends
on a small number of the total covariates. Suppose that m satis ﬁes such a sparseness
condition, so that m(x) = m(xR ) where xR = (xj : j ∈ R), R ⊂ {1, . . . , d} is a subset
of the d covariates, of size r = |R| ≪ d. We call {xj }j∈R the relevant variables. Under
this sparseness assumption we can hope to achieve the better minimax convergence rate of
n−4/(4+r) if the r relevant variables can be isolated. Thus, we are faced with the problem
of variable selection in nonparametric regression.

A large body of previous work has addressed this fundamental problem, which has led
to a variety of methods to combat the curse of dimensionality. Many of these are based

on very clever, though often heuristic techniques. For additive models of the form
f (x) = Pj fj (xj ), standard methods like stepwise selection, Cp and AIC can be used
(Hastie et al. 2001). For spline models, Zhang et al. (2005) use likelihood basis pur-
suit, essentially the lasso adapted to the spline setting. CART (Breiman et al. 1984) and
MARS (Friedman 1991) effectively perform variable selection as part of their function ﬁt-
ting. More recently, Li et al. (2005) use independence testing for variable selection and
B ¨uhlmann and Yu (2005) introduced a boosting approach. While these methods have met
with varying degrees of empirical success, they can be challenging to implement and de-
manding computationally. Moreover, these methods are typically difﬁcult to analyze the-
oretically, and so often come with no formal guarantees. Indeed, the theoretical analysis
of sparse parametric estimators such as the lasso (Tibshirani 1996) is difﬁcult,
and only
recently has signi ﬁcant progress been made on this front (Do noho 2004; Fu and Knight
2000).

In this paper we present a new approach to sparse nonparametric function estimation that
is both computationally simple and amenable to theoretical analysis. We call the general
framework rodeo, for regularization of derivative expectation operator. It is based on the
idea that bandwidth and variable selection can be simultaneously performed by computing
the inﬁnitesimal change in a nonparametric estimator as a fu nction of the smoothing pa-
rameters, and then thresholding these derivatives to effectively get a sparse estimate. As
a simple version of this principle we use hard thresholding, effectively carrying out a se-
quence of hypothesis tests. A modi ﬁed version that replaces
testing with soft thresholding
effectively solves a sequence of lasso problems. The potential appeal of this approach is
that it can be based on relatively simple and theoretically well understood nonparametric
techniques such as local linear smoothing, leading to methods that are simple to implement
and can be used in high dimensional problems. Moreover, we show that the rodeo can
achieve near optimal minimax rates of convergence, and therefore circumvents the curse of
dimensionality when the true function is indeed sparse. When applied in one dimension,
our method yields a locally optimal bandwidth. We present experiments on both synthetic
and real data that demonstrate the effectiveness of the new approach.

2 Rodeo: The Main Idea
The key idea in our approach is as follows. Fix a point x and let bmh (x) denote an estimator
of m(x) based on a vector of smoothing parameters h = (h1 , . . . , hd ). If c is a scalar,
then we write h = c to mean h = (c, . . . , c). Let M (h) = E( bmh (x)) denote the mean of
bmh (x). For now, assume that xi is one of the observed data points and that bm0 (x) = Yi .
In that case, m(x) = M (0) = E(Yi ). If P = (h(t) : 0 ≤ t ≤ 1) is a smooth path through
the set of smoothing parameters with h(0) = 0 and h(1) = 1 (or any other ﬁxed, large
bandwidth) then
m(x) = M (0) = M (1) − Z 1
ds = M (1) − Z 1
0 (cid:10)D(s), ˙h(s)(cid:11)ds
dM (h(s))
ds
0
where D(h) = ∇M (h) = (cid:16) ∂M
∂hj (cid:17)T
is the gradient of M (h) and ˙h(s) = dh(s)
, . . . , ∂M
is
∂hj
ds
the derivative of h(s) along the path. A biased, low variance estimator of M (1) is bm1 (x).
An unbiased estimator of D(h) is
Z (h) = (cid:18) ∂ bmh (x)
∂hd (cid:19)T
∂ bmh (x)
(2)
, . . . ,
.
∂h1
The naive estimator
bm(x) = bm1 (x) − Z 1
0 (cid:10)Z (s), ˙h(s)(cid:11)ds

(3)

h2

Start

Rodeo path

Ideal path

Figure 1: The bandwidths for the relevant
variables (h2 ) are shrunk, while the band-
widths for the irrelevant variables (h1 ) are
kept relatively large. The simplest rodeo al-
gorithm shrinks the bandwidths in discrete
steps 1, β , β 2 , . . . for some 0 < β < 1.

Optimal
bandwidth
h1
is identically equal to bm0 (x) = Yi , which has poor risk since the variance of Z (h) is large
for small h. However, our sparsity assumption on m suggests that there should be paths for
which D(h) is also sparse. Along such a path, we replace Z (h) with an estimator bD(h)
that makes use of the sparsity assumption. Our estimate of m(x) is then
em(x) = bm1 (x) − Z 1
0 (cid:10) bD(s), ˙h(s)(cid:11)ds .
(4)
To implement this idea we need to do two things: (i) we need to ﬁ nd a sparse path and (ii)
we need to take advantage of this sparseness when estimating D along that path.
The key observation is that if xj is irrelevant, then we expect that changing the bandwidth
hj for that variable should cause only a small change in the estimator bmh (x). Conversely,
if xj is relevant, then we expect that changing the bandwidth hj for that variable should
cause a large change in the estimator. Thus, Zj = ∂ bmh (x)/∂hj should discriminate be-
tween relevant and irrelevant covariates. To simplify the procedure, we can replace the
continuum of bandwidths with a discrete set where each hj ∈ B = {h0 , βh0 , β 2h0 , . . .}
for some 0 < β < 1. Moreover, we can proceed in a greedy fashion by estimating D(h)
sequentially with hj ∈ B and setting bDj (h) = 0 when hj < bhj , where bhj is the ﬁrst h
such that |Zj (h)| < λj (h) for some threshold λj . This greedy version, coupled with the
hard threshold estimator, yields em(x) = bmbh (x). A conceptual illustration of the idea is
shown in Figure 1. This idea can be implemented using a greedy algorithm, coupled with
the hard threshold estimator, to yield a bandwidth selection procedure based on testing.
This approach to bandwidth selection is similar to that of Lepski et al. (1997), which uses a
more reﬁned test leads to estimators that achieve good spati al adaptation over large function
classes. Our approach is also similar to a method of Ruppert (1997) that uses a sequence of
decreasing bandwidths and then estimates the optimal bandwidth by estimating the mean
squared error as a function of bandwidth. Our greedy approach tests whether an inﬁnites-
imal change in the bandwidth from its current setting leads to a signi ﬁcant change in the
estimate, and is more easily extended to a practical method in higher dimensions. Related
work of Hristache et al. (2001) focuses on variable selection in multi-index models rather
than on bandwidth estimation.

3 Rodeo using Local Linear Regression

We now present the multivariate case in detail, using local linear smoothing as the basic
method since it is known to have many good properties. Let x = (x(1), . . . , x(d)) be some
point at which we want to estimate m. Let bmH (x) denote the local linear estimator of

(5)

(7)

(8)

=

Gj (Xi , x, h)Yi

Zj =

m(x) using bandwidth matrix H . Thus,
Xx = 

(X1 − x)T
1
...
...
x WxXx )−1X T
1 (X T
bmH (x) = eT
x WxY ,
(Xn − x)T
1
where e1 = (1, 0, . . . , 0)T , and Wx is the diagonal matrix with (i, i) element KH (Xi − x)
and KH (u) = |H |−1K (H −1u). The estimator bmH can be written as bmH (x) =
Pn
i=1 G(Xi , x, h) Yi where
x WxXx )−1 (cid:18)
(u − x)T (cid:19) KH (u − x)
1
G(u, x, h) = eT
1 (X T
(6)
is called the effective kernel. We assume that the covariates are random with sampling den-
sity f (x), and make the same assumptions as Ruppert and Wand (1994) in their analysis
of the bias and variance of local linear regression. In particular, (i) the kernel K has com-
pact support with zero odd moments and R uu⊤K (u) du = ν2 (K )I and (ii) the sampling
density f (x) is continuously differentiable and strictly positive. In the version of the algo-
rithm that follows, we take K to be a product kernel and H to be diagonal with elements
h = (h1 , . . . , hd ).
Our method is based on the statistic
nXi=1
∂ bmh (x)
∂hj
where Gj (u, x, h) = ∂G(u,x,h)
. Straightforward calculations show that
∂hj
∂Wx
∂ bmh (x)
x WxXx )−1X ⊤
1 (X ⊤
= = e⊤
(Y − Xx bα)
Zj =
x
∂hj
∂hj
where bα = (X ⊤
x WxY is the coefﬁcient vector for the local linear ﬁt. Note
x WxXx )−1X ⊤
that the factor |H |−1 = Qd
i=1 1/hi in the kernel cancels in the expression for bm, and
therefore we can ignore it in our calculation of Zj . Assuming a product kernel we have
K ((Xnj − xj )/hj )
Wx = diag 
dYj=1
dYj=1
(9)
K ((X1j − xj )/hj ), . . . ,
and ∂Wx /∂hj = WxDj where
Dj = diag (cid:18) ∂ log K ((X1j − xj )/hj )
(cid:19)
∂ log K ((Xnj − xj )/hj )
(10)
, . . . ,
∂hj
∂hj
x WxDj (Y − Xx bα). For example, with the Gaussian
and thus Zj = e⊤
x WxXx )−1X ⊤
1 (X ⊤
diag (cid:0)(X1j − xj )2 , . . . , (Xnj − xj )2 (cid:1).
kernel K (u) = exp(−u2 /2) we have Dj = 1
h3
j
Let
nXi=1
(11)
Gj (Xi , x, h)m(Xi )
µj ≡ µj (h) = E(Zj |X1 , . . . , Xn ) =
nXi=1
s2
j ≡ s2
j (h) = V(Zj |X1 , . . . , Xn ) = σ2
Then the hard thresholding version of the rodeo algorithm is given in Figure 2.
The algorithm requires that we insert an estimate bσ of σ in (12). One estimate of σ can
be obtained by generalizing a method of Rice (1984). For i < ℓ, let diℓ = kXi − Xℓ k.
Fix an integer J and let E denote the set of pairs (i, ℓ) corresponding to the J smallest
2J Pi,ℓ∈E (Yi − Yℓ )2 . Then E(bσ2 ) = σ2 + bias where
values of diℓ . Now deﬁne bσ2 = 1

Gj (Xi , x, h)2 .

(12)

Rodeo: Hard thresholdingversion
1. Select parameter 0 < β < 1 and initial bandwidth h0 slowly decreasing to zero,
with h0 = Ω (cid:0)1/√log log n(cid:1). Let cn = Ω(1) be a sequence satisfying dcn =
Ω(log n).
2. Initialize the bandwidths, and activate all covariates:
(a) hj = h0 , j = 1, 2, . . . , d.
(b) A = {1, 2, . . . , d}
3. While A is nonempty, do for each j ∈ A:
(a) Compute the estimated derivative expectation: Zj (equation 7) and sj (equa-
tion 12).
(b) Compute the threshold λj = sjp2 log(dcn ).
(c) If |Zj | ≥ λj , then set hj ← βhj , otherwise remove j from A.
4. Output bandwidths h⋆ = (h1 , . . . , hd ) and estimator em(x) = bmh⋆ (x).
Figure 2: The hard thresholding version of the rodeo, which can be applied using the
derivatives Zj of any nonparametric smoother.
bias ≤ D supx Pj∈R (cid:12)(cid:12)(cid:12) ∂ f (x)
∂xj (cid:12)(cid:12)(cid:12) with D = maxi,ℓ∈E kXi − Xℓ k. There is a bias-variance
tradeoff: large J makes bσ2 positively biased, and small J makes bσ2 highly variable. Note
however that the bias is mitigated by sparsity (small r). This is the estimator used in our
examples.
4 Analysis

In this section we present some results on the properties of the resulting estimator. For-
mally, we use a triangular array approach so that f (x), m(x), d and r can all change as n
changes. For convenience of notation we assume that the covariates are numbered such that
the relevant variables xj correspond to 1 ≤ j ≤ r , and the irrelevant variables to j > r . To
begin, we state the following technical lemmas on the mean and variance of Zj .

Lemma 4.1 . SupposethatK isaproductkernelwithbandwidthvectorh = (h1 , . . . , hd ).
Ifthesamplingdensity f isuniform,then µj = 0 for all j ∈ Rc. Moregenerally,assuming
that r isbounded,wehave thefollowingwhen hj → 0: If j ∈ Rc thederivativeof thebias
is
∂
2 (∇j log f (x))2 hj + oP (hj )
E[ bmH (x) − m(x)] = −tr(HRHR ) ν 2
(13)
µj =
∂hj
0 (cid:19) and HR = diag(h2
where theHessian of m(x) is H = (cid:18) HR 0
r ). For j ∈ R
1 , . . . , h2
0
wehave
∂
µj =
E[ bmH (x) − m(x)] = hj ν2mj j (x) + oP (hj ).
∂hj
Lemma 4.2 . Let C = (cid:16) σ2R(K )
4m(x) (cid:17) where R(K ) = R K (u)2 du. Then, if hj = o(1),
hk ! (cid:18)1 + oP (1)(cid:19).
j   dYk=1
1
C
s2
j = Var(Zj |X1 , . . . , Xn ) =
nh2

(14)

(15)

These lemmas parallel the calculations of Ruppert and Wand (1994) except for the dif-
ference that the irrelevant variables have different leading terms in the expansions than
relevant variables.

Our main theoretical result characterizes the asymptotic running time, selected bandwidths,
and risk of the algorithm. In order to get a practical algorithm, we need to make assump-
tions on the functions m and f .

where

j ≤ r
j > r

(A2) For each j ≤ r ,

(A1) For some constant k > 0, each j > r satis ﬁes
n1/4 !
∇j log f (x) = O   logk n
mj j (x) 6= 0 .
Explanation of the Assumptions. To give the intuition behind these assumptions, recall
from Lemma 4.1 that
µj = (cid:26) Aj hj + oP (hj )
Bj hj + oP (hj )
Aj = ν2mj j (x), Bj = −tr(HH)ν 2
2 (∇j log f (x))2 .
(19)
Moreover, µj = 0 when the sampling density f is uniform or the data are on a regular
grid. Consider assumption (A1). If f is uniform then this assumption is automatically
satis ﬁed since then µj (s) = 0 for j > r . More generally, µj is approximately proportional
to (∇j log f (x))2 for j > r which implies that |µj | ≈ 0 for irrelevant variables if f
is sufﬁciently smooth in the variable xj . Hence, assumption (A1) can be interpreted as
requiring that f is sufﬁciently smooth in the irrelevant dimensions.
Now consider assumption (A2).
Equation (18) ensures that µj is proportional
to
hj |mj j (x)| for small hj . Since we take the initial bandwidth h0 to be decreasingly slowly
with n, (A2) implies that |µj (h)| ≥ chj |mj j (x)| for some constant c > 0, for sufﬁciently
large n.
In the following we write Yn = eOP (an ) to mean that Yn = OP (bnan ) where bn is loga-
rithmic in n; similarly, an = eΩ(bn ) if an = Ω(bn cn ) where cn is logarithmic in n.
Theorem 4.3 . Supposeassumptions(A1)and(A2)hold. Inaddition,supposethat dmin =
minj≤r |mj j (x)| = eΩ(1) and dmax = maxj≤r |mj j (x)| = eO(1). Then the number of
iterations Tn until the rodeo stops satisﬁes
P (cid:18) 1
log1/β (nbn )(cid:19) −→ 1
1
(20)
log1/β (nan ) ≤ Tn ≤
4 + r
4 + r
where an = eΩ(1) and bn = eO(1). Moreover, the algorithm outputs bandwidths h⋆ that
satisfy
for all j > r(cid:19) −→ 1
P (cid:18)h⋆
1
(21)
j ≥
logk n
and
P (cid:16)h0 (nbn )−1/(4+r) ≤ h⋆
j ≤ h0 (nan )−1/(4+r) for all j ≤ r(cid:17) −→ 1 .
Corollary 4.4 . Under theconditionsofTheorem4.3, therisk R(h⋆ ) of therodeoestima-
tor satisﬁes
R(h⋆ ) = eOP (cid:16)n−4/(4+r)(cid:17) .
(23)

(16)

(17)

(18)

(22)

In the one-dimensional case, this result shows that the algorithm recovers the locally op-
timal bandwidth, giving an adaptive estimator, and in general attains the optimal (up to
logarithmic factors) minimax rate of convergence.

The proofs of these results are given in the full version of the paper.

5 Some Examples and Extensions

Figure 3 illustrates the rodeo on synthetic and real data. The left plot shows the bandwidths
obtained on a synthetic dataset with n = 500 points of dimension d = 20. The covariates
are generated as xi ∼ Uniform(0, 1), the true function is m(x) = 2(x1 + 1)2 + 2 sin(10x2 ),
and σ = 1. The results are averaged over 50 randomly generated data sets; note that the dis-
played bandwidth paths are not monotonic because of this averaging. The plot shows how
the bandwidths of the relevant variables shrink toward zero, while the bandwidths of the ir-
relevant variables remain large. Simulations on other synthetic data sets, not included here,
are similar and indicate that the algorithm’s performance is consistent with our theoretical
analysis.

The framework introduced here has many possible generalizations. While we have fo-
cused on estimation of m locally at a point x, the idea can be extended to carry out global
bandwidth and variable selection by averaging over multiple evaluation points x1 , . . . , xk .
These could be points interest for estimation, could be randomly chosen, or could be taken
to be identical to the observed Xi s. In addition, it is possible to consider more general
paths, for example using soft thresholding or changing only the bandwidth corresponding
to the largest |Zj |/λj .
Such a version of the rodeo can be seen as a nonparametric counterpart to least angle
regression (LARS) (Efron et al. 2004), a reﬁnement of forwar d stagewise regression in
which one adds the covariate most correlated with the residuals of the current ﬁt, in small,
incremental steps. Note ﬁrst that Zj is essentially the correlation between the Yi s and the
Gj (Xi , x, h)s (the change in the effective kernel). Reducing the bandwidth is like adding in
more of that variable. Suppose now that we make the following modi ﬁcations to the rodeo:
j = |Zj |/λj , (ii) reduce
(i) change the bandwidths one at a time, based on the largest Z ∗
the bandwidth continuously, rather than in discrete steps, until the largest Z ∗
j is equal to the
next largest. Figure 3 (right) shows the result of running this greedy version of the rodeo on
the diabetes dataset used to illustrate LARS. The algorithm averages Z ∗
j over a randomly
chosen set of k = 100 data points. The resulting variable ordering is seen to be very similar
to, but different from, the ordering obtained from the parametric LARS ﬁt.

Acknowledgments

We thank the reviewers for their helpful comments. Research supported in part by NSF
grants IIS-0312814, IIS-0427206, and DMS-0104016, and NIH grants R01-CA54852-07
and MH57881.

References

L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and regression trees .
Wadsworth Publishing Co Inc, 1984.
P. B ¨uhlmann and B. Yu. Boosting, model selection, lasso and nonnegative garrote. Technical report,
Berkeley, 2005.

h
t
d
i
w
d
n
a
B
 
e
g
a
r
e
v
A

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

12

11
6
16
8
34
15
1819
5
7
10
13
17
20
9
14

1

2

h
t
d
i
w
d
n
a
B

5
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

3

9

7

4

1

2

8

5

10

Rodeo Step

15

0

20

40

60

80

100

Greedy Rodeo Step

Figure 3: Left: Average bandwidth output by the rodeo for a function with r = 2 relevant variables
in d = 20 dimensions (n = 500, with 50 trials). Covariates are generated as xi ∼ Uniform(0, 1),
the true function is m(x) = 2(x1 + 1)3 + 2 sin(10x2 ), and σ = 1, ﬁt at the test point x =
2 ). The variance is greater for large step sizes since the rodeo runs that long for fewer data
2 , . . . , 1
( 1
sets. Right: Greedy rodeo on the diabetes data, used to illustrate LARS (Efron et al. 2004). A set of
k = 100 of the total n = 442 points were sampled (d = 10), and the bandwidth for the variable
with largest average |Zj |/λj was reduced in each step. The variables were selected in the order 3
(body mass index), 9 (serum), 7 (serum), 4 (blood pressure), 1 (age), 2 (sex), 8 (serum), 5 (serum),
10 (serum), 6 (serum). The parametric LARS algorithm adds variables in the order 3, 9, 4, 7, 2, 10,
5, 8, 6, 1. One notable difference is in the position of the age variable.

D. Donoho. For most large underdetermined systems of equations, the minimal ℓ1 -norm near-solution
approximates the sparest near-solution. Technical report, Stanford, 2004.
B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of Statistics,
32:407–499, 2004.
J. H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics, 19:1–67, 1991.
W. Fu and K. Knight. Asymptotics for lasso type estimators. The Annals of Statistics, 28:1356–1378,
2000.
L. Gy ¨or ﬁ, M. Kohler, A. Krzy ˙zak, and H. Walk. A Distribution-Free Theory of Nonparametric
Regression. Springer-Verlag, 2002.
T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction. Springer-Verlag, 2001.
M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension
reduction. Ann. Statist., 29:1537–1566, 2001.
O. V. Lepski, E. Mammen, and V. G. Spokoiny. Optimal spatial adaptation to inhomogeneous
smoothness: An approach based on kernel estimates with variable bandwidth selectors. The Annals
of Statistics, 25:929–947, 1997.
L. Li, R. D. Cook, and C. Nachsteim. Model-free variable selection. J. R. Statist. Soc. B., 67:285–299,
2005.
J. Rice. Bandwidth choice for nonparametric regression. The Annals of Statistics, 12:1215–1230,
1984.
D. Ruppert. Empirical-bias bandwidths for local polynomial nonparametric regression and density
estimation. Journal of the American Statistical Association, 92:1049–1062, 1997.
D. Ruppert and M. P. Wand. Multivariate locally weighted least squares regression. The Annals of
Statistics, 22:1346–1370, 1994.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, Methodological, 58:267–288, 1996.
H. Zhang, G. Wahba, Y. Lin, M. Voelker, R. K. Ferris, and B. Klein. Variable selection and model
building via likelihood basis pursuit. J. of the Amer. Stat. Assoc., 99(467):659–672, 2005.


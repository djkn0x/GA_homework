Rate Distortion Codes in Sensor Networks:
A System-level Analysis

Tatsuto Murayama and Pete r Davis
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
“Keihanna Science City”, Kyoto 619-0237, Japan
{murayama,davis}@cslab.kecl.ntt.co.jp

Abstract

This paper provides a system-level analysis of a scalable distributed sens-
ing model for networked sensors. In our system model, a data center ac-
quires data from a bunch of L sensors which each independently encode
their noisy observations of an original binary sequence, and transmit their
encoded data sequences to the data center at a combined rate R, which
is limited. Supposing that the sensors use independent LDGM rate dis-
tortion codes, we show that the system performance can be evaluated for
any given ﬁnite R when the number of sensors L goes to inﬁnity. The
analysis shows how the optimal strategy for the distributed sensing prob-
lem changes at critical values of the data rate R or the noise level.

1

Introduction

Device and sensor networks are shaping many activities in our society. These networks are
being deployed in a growing number of applications as diverse as agricultural management,
industrial controls, crime watch, and military applications. Indeed, sensor networks can be
considered as a promising technology with a wide range of potential future markets [1].
Still, for all the promise, it is often difﬁcult to integrate the individual components of a sen-
sor network in a smart way. Although we see many breakthroughs in component devices,
advanced software, and power managem ents, system-level understanding of the emerging
technology is still weak. It requires a shift in our notion of “what to look for”. It requires
a study of collective behavior and resulting trade-offs. This is the issue that we address in
this article. We demonstrate the usefulness of adopting new approaches by considering the
following scenario.
Consider that a data center is interested in the data sequence, {X (t)}∞
t=1 , which cannot be
observed directly. Therefore, the data center deploys a bunch of L sensors which each inde-
pendently encodes its noisy observation of the sequence, {Y i (t)}∞
t=1 , without sharing any
information, i.e., the sensors are not permitted to communicate and decide what to send to
the data center beforehand. The data center collects separate samples from all the L sensors
and uses them to recove r the original sequence. However, since {X (t)}∞
t=1 is not the only
pressing matter which the data center must consider, the combined data rate R at which
the sensors can communicate with it is strictly limited. A formulation of decentralized
communication with estimation task, the “CEO problem”, was ﬁrst proposed by Berger

and Zhang [2], providing a new theoretical framework for large scale sensing systems. In
this outstanding work, some interesting properties of such systems have been revealed. If
the sensors were permitted to communicate on the basis of their pooled observations, then
they would be able to smooth out their independent observation noises entirely as L goes
to inﬁnity. Therefore, the data center can achieve an arbitrary ﬁdelity D(R), where D(·)
denotes the distortion rate function of {X (t)}. In particular, the data center recove rs almost
complete information if R exce eds the entropy rate of {X (t)}. However, if the sensors are
not allowed to communicate with each other, there does not exist a ﬁnite value of R for
which even inﬁnitely many sensors can make D arbitrarily small [2].

In this paper, we introduce a new analytical model for a massive sensing system with a
ﬁnite data rate R. More speciﬁca lly, we assume that the sensors use LDGM codes for rate
distortion coding, while the data center recovers the original sequence by using optimal
“majority vote” estimation [3]. We consider the distributed sensing problem of deciding
the optimal number of sensors L given the combined data rate R. Our asymptotic anal-
ysis succes sfully provides the performance of the whole sensing system when L goes to
inﬁnity, where the data rate for an individual sensor information vanishes. Here, we exploit
statistical methods which have recently been developed in the ﬁeld of disordered statistical
systems, in particular, the spin glass theory. The paper is organized as follows. In Sec-
tion 2, we introduce a system model for the sensor network. Section 3 summarize s the
results of our approach, where the following section provides the outline of our analysis.
Conclusions are given in the last section.

W (yi |x)

2 System Model
Let P (x) be a probability distribution common to {X (t)} ∈ X , and W (y|x) be a stochastic
matrix deﬁned on X × Y , with Y denotes the common alphabet of {Y i(t)}, where i =
1, · · · , L and t ≥ 1. In the general setup, we assume that the instantaneous joint probability
L(cid:1)
distribution in the form

Pr[x, y1 , · · · , yL ] = P (x)
i=1
for the temporally memoryless source {X (t)}∞
t=1 . Here, the random variables Yi (t)
are conditionally independent when X (t) is given, and the conditional probabilities
W [yi (t)|x(t)] are identical for all i and t. In this paper, we impose the binary assump-
tions to the problem, i.e., the data sequence {X (t)} and its noisy observations {Y i(t)} are
(cid:2)
all assumed to be binary sequences. Therefore, the stochastic matrix can be parameterized
as
1 − p,
W (y|x) =
if y = x
,
otherwise
p,
where p ∈ [0, 1] represents the observation noise. Note also that the alphabets have been
selected as X = Y . Furthermore, for simplicity, we also assume that P (x) = 1/2 always
holds, implying that a purely random source is observed.
i = [yi(1), · · · , yi (n)]T of length n
At the encoding stage, a sensor i encodes a block y
from the noisy observation {y i (t)}∞
t=1 , into a block z i = [zi(1), · · · , zi (m)]T of length
m deﬁned on Z . Hereafter, we take the Boolean representation of the binary alphabet
X = {0, 1}, therefore Y = Z = {0, 1} as well. Let ˆy
i be a reproduction sequence for the
block, and we have a known integer m < n. Then, making use of a Boolean matrix Ai of
dimensionality n×m, we are to ﬁnd an m bit codeword sequence z i = [zi(1), · · · , zi (m)]T
which satisﬁes

ˆy

i = Aiz i

(mod 2) ,

(1)

where the ﬁdelity criterion

1
i , ˆy
dH(y
D =
i )
(2)
n
holds [4]. Here the Hamming distance dH(·, ·) is used for the distortion mea sure. Note
that we have applied modulo-2 arithmetic for the additive operation in (1). Let A i be
characterized by K ones per row and C per column. The ﬁnite, and usually small, numbers
K and C deﬁne a particular LDGM code family. The data center then collects the L
codeword sequences , z1 , · · · , zL . Since all the L codewords are of the same length m,
the combined data rate will be R = L × m/n. Therefore, in our scena rio, the data center
1 , · · · , ˆy
deploys exchangeable sensors with ﬁxed quality reproductions, ˆy
(cid:2)
L . Lastly, the tth
symbol of the estimate, ˆx = [ ˆx(1), · · · , ˆx(n)]T , is to be calculated by majority vote [3],
if ˆy1 (t) + · · · + ˆyL (t) ≤ L/2
0,
1, otherwise
Therefore, overall performance of the system can be measured by the expected bit error
frequency for decisions by the majority vote (3), P e = Pr[x (cid:4)= ˆx].
In this paper, we consider two limit case s of decentralization levels; (1) The extreme situa-
tion of L → ∞, and (2) the case of L = R. The former case means that the data rate for
an individual sensor information vanishes, while the latter case results in the transmission
without coding techniques. In general, it is difﬁcult to determine which level is optimal for
the estimation, i.e., which scenario results in the smaller value of P e . Indeed, by using the
rate distortion codes, the data center could use as many sensors as possible for a given R.
However, the quality of the individual reproduction would be less informative. The best
choice seems to depend largely on R, as well as p.

ˆx(t) =

(3)

.

(cid:8)

− σ
2√
α

Pe(p, R) =

dr N(0, 1)

3 Main Results
For simplicity, we consider the following two solvable cases ; K = 2 for C ≥ K and
the optimal case of K → ∞. Let p be a given observation noise level, and R the ﬁnite
real value of a given combined data rate. Letting L → ∞, we ﬁnd the expected bit error
(cid:3) −(1−2p)cg
frequency to be
√
R
−∞
(cid:7)
(cid:6) √
α
2

(cid:5) √
(cid:4)
with the constant value
−
1√
2 + 2 ln 2√
α
√
(K = 2)
cg =
α
(K → ∞)
2
2 ln 2
(cid:10)
(cid:9)
where the rescaled variance σ2 = α (cid:7) ˆx2 (cid:8) ˆπ ( ˆx) and the ﬁrst step RSB enforcement
(cid:7)tanh2 x (1 + 2x csch x sech x)(cid:8)π(x) = 0
− 1
− σ2
1
2
2
2
α
α
holds. Here N(X, Y ) denotes the normal distribution with the mean X and the variance Y .
(cid:12)
(cid:11)
(cid:3) ∞
The resca led variance σ2 and the scale invariant parameter α is determined numerically,
where we use the following notations.
dx√
(cid:7) · (cid:8)π(x) =
(cid:3)
−∞
2πσ2
d ˆx√
(cid:7) · (cid:8) ˆπ ( ˆx) =
+1
−1
2πσ2

− x2
2σ2
(1 − ˆx2 )
−1 exp

−1 ˆx)2
− (tanh
2σ2

(cid:7)tanh2 x(cid:8)π(x)

( · ) ,
(cid:11)

(4)

(5)

(cid:12)

( · ) .

+

ln 2 +

exp

(a) Narrow Band
2

)
R
,
p
(
)
B
d
(
e
P

1

0

−1

−2
0

R = 1

R = 2
R = 10

0.1

0.2

p

0.3

0.4

0.5

(b) Broadband

150
100
)
50
R
,
p
0
(
)
B
−50
d
(
e
P
−100
−150
0

R = 100
R = 500
R = 1000

0.1

0.2

p

0.3

0.4

0.5

Figure 1: P (dB)
e

(p, R) for K = 2. (a) Narrow band (b) Broadband

P (0)
e

(6)

P (dB)
e

(p, R) =

Therefore, it is straightforward to evaluate (4) with (5) for given parameters, p and R.
For a given ﬁnite value of R, we see what happens to the quality of the estimate when the
(cid:4)(cid:13)
(cid:14)
(cid:15)
noise level p varies. Fig. 1 and Fig. 2 shows the typical behavior of the bit error frequency,
Pe(p, R), in decibel (dB), where the reference level is chosen as
(cid:14)
(cid:15)
(cid:14)
(cid:15)
(cid:13)
(1 − p)l pR−l ,
(R−1)/2
R
(R is odd)
(1 − p)l pR−l + 1
(1 − p)R/2pR/2
R/2−1
l=0
l
R
R
(R is even)
l=0
2
R/2
l
for a given integer R. The reference (6) denotes Pe for the case of L = R, i.e., the case
when the sensors are not allowed to compress their observations. Here, in decibel, we have
(p, R) = 10 log Pe (p, R)
P (0)
(p, R)
e
where the log is to base 10. Note that the zero level in decibel occurs when the measured
error frequency Pe (p, R) is equal to the reference level. Therefore, it is also possible to
have negative levels, which would mean an expected bit error frequency much smaller than
the reference level. In the case of small combined data rate R, the narrow band case, the
numerical results in Fig. 1 (a) and Fig. 2 (a) show that the quality of the estimate is sensitive
to the parity of the integer R. In particular, the R = 2 case has the lowest threshold level,
pc = 0.0921 for Fig. 1 (a) and pc = 0.082 for Fig. 2 (a) respectively, beyond which
the L → ∞ scenario outperforms the L = R scenario, while the R = 1 case does not
have such a threshold. In contrast, if the bandwidth is wide enough, the difference of the
expected bit error probabilities in decibel, P (dB)
(p, R), is proved to have similar qualitative
e
characteristics as shown in Fig. 1 (b) and Fig. 2 (b). Moreover, our preliminary experiments
for larger systems also indicate that the threshold p c seems to converge to the value, 0.165
and 0.146 respectively, as L goes to inﬁnity; we are currently working on the theoretical
derivation.

,

4 Outline of Derivation
Since the predetermined matrices A1 , · · · , AL are selected randomly, it is quite natural to
say that the instantaneous series, deﬁned by ˆy(t) = [ ˆy1 (t), · · · , ˆyL(t)]T , can be modeled

)
R
,
p
(
)
B
d
(
e
P

(a) Narrow Band

2

1

0

−1

−2
0

0.1

R = 1

R = 2

R = 10
0.2

p

0.3

0.4

0.5

(b) Broadband

150
100
)
50
R
,
p
0
(
)
B
−50
d
(
e
P
−100
−150
0

R = 100
R = 500
R = 1000

0.1

0.2

p

0.3

0.4

0.5

Figure 2: P (dB)
e

(p, R) for K → ∞. (a) Narrow band (b) Broadband

with

(cid:3)
B(L

(8)

(cid:2)
using the Bernoulli trials. Here, the reproduction problem reduces to a channel model,
where the stochastic matrix is deﬁned as
W ( ˆy |x) =
if ˆy = x
q ,
1 − q , otherwise
(7)
,
where q denotes the quality of the reproductions, i.e., Pr[x (cid:4)= ˆy i ] = 1 − q for i = 1, · · · , L.
(cid:2)
Letting the channel model (7) for the reproduction problem be valid, the expected bit error
frequency can be well captured by using the cumulative probability distributions
B( L−1
Pe = Pr[x (cid:4)= ˆx] =
: L, q),
if L is odd
− 1 : L, q) + 1
2
2 b( L
2 : L, q) otherwise
B( L
(cid:10)
(cid:9)
2
(cid:1)(cid:16)
L
L
l

q l (1 − q)L−l ,

b(l : L, q) =

b(l : L, q) ,

: L, q) =

l=0
where an integer l be the total number of non-ﬂipped elements in ˆy(t), and the second term
(1/2)b(L/2 : L, q) represents random guessing with l = L/2. Note that the reproduction
quality q can be easily obtained by the simple algebra q = pD + (1 − p)(1 − D), where D
is the distortion with respect to coding.
Since the error probability (8) is given by a function of q , we ﬁrstly derive an analytical
solution for the quality q in the limit L → ∞, keeping R ﬁnite. In this approach, we apply
the method of statistical mechanics to evaluate the typical performance of the codes [4].
As a ﬁrst step, we translate the Boolean alphabets Z = {0, 1} to the “Ising ” ones,
S =
{+1, −1}. Consequently, we need to translate the additive operations, such as, z i (s) +
(cid:3) ) (mod 2) into their multiplicative representations, σ i (s) × σi(s
(cid:3) ) ∈ S for s, s
(cid:3) =
zi (s
1, · · · , m. Similarly, we translate the Boolean y i (t)s into the Ising J i (t)s. For simplicity,
we omit the subscript i, which labels the L agents, in the rest of this section. Following the
(cid:16)
prescription of Sourlas [5], we exam ine the Gibbs-Boltzmann distribution
exp [−βH (σ |J )]
−βH (σ |J ) ,
e
Z (J )
σ

with Z (J ) =

Pr[σ ] =

(9)

(10)

(cid:16)
where the Hamiltonian of the Ising system is deﬁned as
As1 ...sK Ji [t(s1 , . . . , sK )]σ(s1 ) . . . σ(sK ) .
H (σ |J ) = −
s1<···<sK
The observation index t(s1 , . . . , sK ) speciﬁes the proper value of
t given the set
s1 , . . . , sK , so that it corresponds to the parity check equation (1). Here the elements
of the symmetric tensor As1 ...sK , representing dilution, is either zero or one depending
(cid:13)
on the set of indices (s1 , . . . , sK ). Since there are C non-zero elements randomly chosen
Ass2 ...sK = C . The code rate is R/L = K/C
for any given index s, we ﬁnd
s2 ,...,sK
because a reproduction sequence has C bits per index s and carries K bits of the code-
word.
It is easy to see that the Hamiltonian (10) is counting the reproduction errors,
[1 − Jt(s1,...,sK ) · σ(s1 ) . . . σ(sK )]/2.
Moreover, according to the statistical mechanics, we can easily derive the “observable”
quantities using the free energy deﬁned as
f = − 1
(cid:7)ln Z (J )(cid:8)A,J
β
which carries all information about the statistics of the system. Here, β denotes an “inverse
temperature” for the Gibbs-Boltzmann distribution (9), and (cid:7)·(cid:8) A,J represents the con ﬁgura-
tional average . Therefore, we have to average the logarithm of the partition function Z (J )
over the given distribution (cid:7)·(cid:8) A,J after the calculation of the partition function. Finally,
(cid:17)
to perform such a program, the replica trick is used [6]. The theory of replica symmetry
breaking can provide the free energy resulting in the expression
f = − 1
ln cosh β − K (cid:7)ln [1 + tanh(βx) tanh(β ˆx)](cid:8)
(cid:19)(cid:20)
(cid:17)
(cid:18) (cid:16)
K(cid:1)
π(x), ˆπ( ˆx)
βn
tanh(βxl )
1 + tanh(βJ )
ln
(cid:20)
(cid:19)
(cid:18)
C(cid:1)
(cid:16)
J =±1
+ C
[1 + σ tanh(β ˆxl )]
K
σ=±1
l=1
ˆπ ( ˆx)
where (cid:7)·(cid:8)π(x) denotes the averaging over p(x l )s and so on. The variation of (11) by π(x)
(cid:18)
(cid:17)
(cid:18)
(cid:20)
(cid:19)(cid:20)
and ˆπ( ˆx) under the condition of normalization gives the saddle point condition
x − C−1(cid:16)
(cid:16)
δ [ ˆx − µ(x1 , . . . , xK−1 ; J )]
δ
J =±1
l=1
(cid:19)
K−1(cid:1)
tanh(βxl )

1
2
(cid:17)
tanh(βJ )

µ(x1 , . . . , xK−1 ; J ) =

π(x) =

ˆπ( ˆx) =

where

+

1
2

(11)

,

ˆπ( ˆx)

ˆxl

l=1

π(x)

,

π(x)

ln

,

−1

tanh

1
β

.

l=1

We now investigate the case of K = 2. Applying the central limit theorem to π(x) [7], we
get

1√
2πC σ2
where σ2 is the variance of ˆπ( ˆx). Here the resulting distribution (12) is a even function.
The leading contribution to µ is then given by µ(x; J ) ∼ J · tanh(βx) as β goes to zero;

− x
2
e
2Cσ

π(x) =

2 ,

(12)

ˆπ( ˆx) =

+

Pe =

∂f
∂β

π(x)

(13)

ln 2 +

β

+

ln 2 +

˜π( ˜x) with

˜π( ˜x) =

− ˜x
2
2
2β
e
Cσ

2 ,

The expression is valid in the asymptotic region L (cid:11) 1 for a ﬁxed R. Then, the formula
(cid:20)
(cid:18)
(cid:10)(cid:21)(cid:21)(cid:21)(cid:21)−1
(cid:12) (cid:21)(cid:21)(cid:21)(cid:21)ρ
(cid:9)
(cid:11)
for the delta function yields [8]
x − 1
1
−1 ˆx
−1 ˆx; ˆx
(cid:3)
tanh
tanh
(cid:12)
(cid:11)
δ
β
β
(cid:22)
(1 − ˆx2)−1
−1 ˆx)2
− (tanh
=
exp
,
2β2C σ2
2πβ2C σ2
(cid:11)
(cid:3)
where we have used ρ(x; ˆx) = ˆx − tanh(βx). Therefore, we have
d ˆx(cid:22)
−1 ˆx)2
− (tanh
σ2 = (cid:7) ˆx2 (cid:8) ˆπ ( ˆx) =
+1
ˆx2
1 − ˆx2 exp
2β2 C σ2
−1
2πβ2C σ2
(cid:24)
(cid:23)
for given β 2C . Inserting (12), (13) into (11), we get
1(cid:22)
1 − 2σ2
− R
f = − β
tanh2 ˜x
2
2
2πβ2C σ2
β
where we rewrite ˜x = βx. The theory of replica symmetry breaking tells us that relevant
value of β should not be smaller than the “freezing point”
β g , which implies the vanishing
(cid:24)
(cid:23)
entropy condition:
1 − 2σ2
= − 1
tanh2 ˜x (1 + 2 ˜x csch ˜x sech ˜x)
˜π ( ˜x) = 0 .
2
2

2
β2
g C
Accordingly, it is convenient for us to deﬁne a scaling invariant parameter α = β 2
g C , and to
rewrite the variance ˜σ2 = ασ2 for simplicity. Introducing these newly deﬁned parameters,
(cid:25)
(cid:25)
(cid:25)
(cid:25)
(cid:12)
(cid:10)
(cid:9)
(cid:11)
the above results could be summarized as follows. Given R and L, we ﬁnd
(cid:7)tanh2 ˜x(cid:8) ˜π ( ˜x)
− ˜σ2
− ln 2
− 1
2
α
R
f =
2
2
α
α
L
(cid:10) (cid:23)
(cid:9)
with ˜σ 2 = α (cid:7) ˆx2 (cid:8) ˆπ ( ˆx) , where the condition
(cid:24)
− 1
− ˜σ2
2
1
tanh2 ˜x (1 + 2 ˜x csch ˜x sech ˜x)
ln 2 +
2
2
α
α
(cid:12)
(cid:11)
(cid:3) ∞
holds. Here we denote
d ˜x√
− ˜x2
(cid:7) · (cid:8) ˜π ( ˜x) =
(cid:3)
−∞
2 ˜σ2
2π ˜σ2
d ˆx√
(1 − ˆx2 )
(cid:7) · (cid:8) ˆπ ( ˆx) =
+1
−1 exp
−1
2π ˜σ2
(cid:10)
(cid:9)
(cid:3)
L/2(cid:16)
Lastly, by using the cumulative probability distribution, we get
q l (1 − q)L−l ∼
dr N(Lq , Lq(1 − q)) .
L
l
(cid:22)
(cid:22)
l=0
It is easy to see that (15) can be converted to a standard normal distribution by changing
variables to ˜r = (r − Lq)/
Lq(1 − q) [7], so d ˜r = dr/
Lq(1 − q), yielding
(cid:3)
Pe ∼
˜rg
−√
L

−1 ˆx)2
− (tanh
2 ˜σ2

( · ) ,
(cid:11)

α
2

1
2

˜π( ˜x) = 0

(14)

(cid:12)

( · ) .

L/2

0

d ˜r N(0, 1)

+

exp

(cid:12)

(15)

with

(cid:10)

=

+

(cid:12)

.

(cid:9)
α

√

(cid:9)
√
D − 1
L(1 − 2p)
(cid:25)
(cid:11)
˜rg = 2
2
√
α − 2 ln 2√
(cid:7)tanh2 ˜x(cid:8) ˜π ( ˜x)
(1 − 2p)
− 1
− ˜σ2
1
R
2
2
2
α
α
Note that the relation D = (1 + f )/2 holds at the vanishing entropy condition (14) [4].
Finally, we obtain the main result (4) in Section 3 in the limit L → ∞, when we use proper
notations for the variables and the name of the function.
We can investigate the asymptotic case of K → ∞ in a similar way. Since the leading
(cid:8)(cid:27)
(cid:26)
(cid:5)
√
contribution to ˆπ( ˆx) comes from the value of x in the vicinity of
C σ2 , we ﬁnd the ex-
pression ˆπ( ˆx) ≈
ˆx − yβK (C σ2 ) K
by using the power counting. Therefore, within
δ
2
√
the Parisi RSB scheme , one obtain a set of equations
√
ln 2 , − 1
Lf = −
− R√
αc
2
2
αc
with the scale-invariant α c = β2L. This results in cg =

R
αc
2 ln 2, as is mentioned before.

ln 2 = 0

(cid:10)

+
√

5 Conclusion

This paper provides a system-level perspective for massive sensor networks. The decen-
tralized sensing problem argued in this paper was ﬁrst addressed by Berger and his col-
laborators. However, this paper is the ﬁrst work that gives a scheme to analyze practically
tractable codes in the given ﬁnite data rate, and shows the existence of threshold level of
noise of which the optimal levels of decentralization changes. Future work includes the
theoretical derivation of the threshold level p c where R goes to inﬁnity, as well as the
implementation problem.

Acknowledgments

The authors thank Jun Muramatsu and Naonori Ueda for useful discussions. This work was
supported by the Ministry of Education, Science, Sports and Culture (MEXT) of Japan,
under the Grant-in-Aid for Young Scientists (B), 15760288.

References
[1] (2005) Intel@Mote. [Online]. Available: http://www.intel.com/research/exploratory/motes.htm
[2] T. Berger, Z. Zhang, and H. Viswanathan, “The CEO problem,”
IEEE Trans. Inform. Theory,
vol. 42, pp. 887–902, May 1996.
[3] D. J. C. MacKay, Information Theory, Inference and Learning Algorithms. Cambridge, UK:
Cambridge University Press, 2003.
[4] T. Murayama and M. Okada, “Rate distortion function in the spin glass state: a toy model,” in
Advances in Neural Information Processing Systems 15 (NIPS’02) , Denver, USA, Dec. 2002, pp.
423–430.
[5] N. Sourlas, “Spin-glass models as error-correcting codes,” Nature, vol. 339, pp. 693–695, June
1989.
[6] V. Dotsenko, Introduction to the Replica Theory of Disordered Statistical Systems. Cambridge,
UK: Cambridge University Press, 2001.
[7] W. Hays, Statistics (5th Edition). Belmont, CA: Wadsworth Publishing, 1994.
[8] C. W. Wong, Introduction to Mathematical Physics: Methods and Concepts. Oxford, UK:
Oxford University Press, 1991.


Non-Local Manifold Parzen Windows

Yoshua Bengio, Hugo Larochelle and Pascal Vincent
Dept. IRO, Universit ´e de Montr ´eal
P.O. Box 6128, Downtown Branch, Montreal, H3C 3J7, Qc, Canada
{bengioy,larocheh,vincentp}@iro.umontreal.ca

Abstract

To escape from the curse of dimensionality, we claim that one can learn
non-local functions, in the sense that the value and shape of the learned
function at x must be inferred using examples that may be far from x.
With this objective, we present a non-local non-parametric density esti-
mator. It builds upon previously proposed Gaussian mixture models with
regularized covariance matrices to take into account the local shape of
the manifold. It also builds upon recent work on non-local estimators of
the tangent plane of a manifold, which are able to generalize in places
with little training data, unlike traditional, local, non-parametric models.

1 Introduction

A central objective of statistical machine learning is to discover structure in the joint dis-
tribution between random variables, so as to be able to make predictions about new com-
binations of values of these variables. A central issue in obtaining generalization is how
information from the training examples can be used to make predictions about new exam-
ples and, without strong prior assumptions (i.e.
in non-parametric models), this may be
fundamentally difﬁcult, as illustrated by the curse of dime nsionality.

(Bengio, Delalleau and Le Roux, 2005) and (Bengio and Monperrus, 2005) present sev-
eral arguments illustrating some fundamental limitations of modern kernel methods due
to the curse of dimensionality, when the kernel is local (like the Gaussian kernel). These
arguments are all based on the locality of the estimators, i.e., that very important informa-
tion about the predicted function at x is derived mostly from the near neighbors of x in the
training set. This analysis has been applied to supervised learning algorithms such as SVMs
as well as to unsupervised manifold learning algorithms and graph-based semi-supervised
learning. The analysis in (Bengio, Delalleau and Le Roux, 2005) highlights intrinsic limita-
tions of such local learning algorithms, that can make them fail when applied on problems
where one has to look beyond what happens locally in order to overcome the curse of di-
mensionality, or more precisely when the function to be learned has many variations while
there exist more compact representations of these variations than a simple enumeration.
This strongly suggests to investigate non-local learning methods, which can in principle
generalize at x using information gathered at training points xi that are far from x. We
present here such a non-local learning algorithm, in the realm of density estimation.

The proposed non-local non-parametric density estimator builds upon the Manifold Parzen
density estimator (Vincent and Bengio, 2003) that associates a regularized Gaussian with

each training point, and upon recent work on non-local estimators of the tangent plane of
a manifold (Bengio and Monperrus, 2005). The local covariance matrix characterizing the
density in the immediate neighborhood of a data point is learned as a function of that data
point, with global parameters. This allows to potentially generalize in places with little or
no training data, unlike traditional, local, non-parametric models. Here, the implicit as-
sumption is that there is some kind of regularity in the shape of the density, such that learn-
ing about its shape in one region could be informative of the shape in another region that
is not adjacent. Note that the smoothness assumption typically underlying non-parametric
models relies on a simple form of such transfer, but only for neighboring regions, which is
not very helpful when the intrinsic dimension of the data (the dimension of the manifold
on which or near which it lives) is high or when the underlying density function has many
variations (Bengio, Delalleau and Le Roux, 2005). The proposed model is also related to
the Neighborhood Component Analysis algorithm (Goldberger et al., 2005), which learns
a global covariance matrix for use in the Mahalanobis distance within a non-parametric
classiﬁer. Here we generalize this global matrix to one that
is a function of the datum x.

2 Manifold Parzen Windows

In the Parzen Windows estimator, one puts a spherical (isotropic) Gaussian around each
training point xi , with a single shared variance hyper-parameter. One approach to improve
on this estimator, introduced in (Vincent and Bengio, 2003), is to use not just the presence
of xi and its neighbors but also their geometry, trying to infer the principal characteristics of
the local shape of the manifold (where the density concentrates), which can be summarized
in the covariance matrix of the Gaussian, as illustrated in Figure 1. If the data concentrates
in certain directions around xi , we want that covariance matrix to be “
ﬂat” (near zero
variance) in the orthogonal directions.

ˆp(y ) =

N (y ; xi + µ(xi ), S (xi ))

S (xi ) = σ2
noise (xi )I +

One way to achieve this is to parametrize each of these covariance matrices in terms of
“principal directions” (which correspond to the tangent ve
ctors of the manifold, if the data
concentrates on a manifold). In this way we do not need to specify individually all the
entries of the covariance matrix. The only required assumption is that the “noise directions”
orthogonal to the “principal directions” all have the same v ariance.
n
1
X
n
i=1
where N (y ; xi + µ(xi ), S (xi )) is a Gaussian density at y , with mean vector xi + µ(xi ) and
covariance matrix S (xi ) represented compactly by
d
X
j=1
where s2
j (xi ) and σ2
noise (xi ) are scalars, and vj (xi ) denotes a “principal” direction with
variance s2
noise (xi ) is the noise variance (the variance in all the
noise (xi ), while σ2
j (xi ) + σ2
other directions). vj (xi )′ denotes the transpose of vj (xi ).
0 is a global hyper-
In (Vincent and Bengio, 2003), µ(xi ) = 0, and σ2
noise (xi ) = σ2
parameter, while (λj (xi ), vj ) = (s2
noise (xi ), vj (xi )) are the leading (eigen-
j (xi ) + σ2
value,eigenvector) pairs from the eigen-decomposition of a locally weighted covariance
matrix (e.g. the empirical covariance of the vectors xl − xi , with xl a near neighbor of xi ).
The “noise level” hyper-parameter σ2
0 must be chosen such that the principal eigenvalues
are all greater than σ2
0 . Another hyper-parameter is the number d of principal components
to keep. Alternatively, one can choose σ2
noise (xi ) to be the (d + 1)th eigenvalue, which
guarantees that λj (xi ) > σ2
noise (xi ), and gets rid of a hyper-parameter. This very simple
model was found to be consistently better than the ordinary Parzen density estimator in
numerical experiments in which all hyper-parameters are chosen by cross-validation.

s2
j (xi )vj (xi )vj (xi )′

(1)

(2)

3 Non-Local Manifold Tangent Learning

In (Bengio and Monperrus, 2005) a manifold learning algorithm was introduced in which
the tangent plane of a d-dimensional manifold at x is learned as a function of x ∈ RD ,
using globally estimated parameters. The output of the predictor function F (x) is a d × D
matrix whose d rows are the d (possibly non-orthogonal) vectors that span the tangent
plane. The training information about the tangent plane is obtained by considering pairs of
near neighbors xi and xj in the training set. Consider the predicted tangent plane of the
manifold at xi , characterized by the rows of F (xi ). For a good predictor we expect the
vector (xi − xj ) to be close to its projection on the tangent plane, with local coordinates
w ∈ Rd . w can be obtained analytically by solving a linear system of dimension d.
The training criterion chosen in (Bengio and Monperrus, 2005) then minimizes the sum
over such (xi , xj ) of the sinus of the projection angle, i.e. ||F ′ (xi )w − (xj − xi )||2 /||xj −
xi ||2 . It is a heuristic criterion, which will be replaced in our new algorithm by one de-
rived from the maximum likelihood criterion, considering that F (xi ) indirectly provides
the principal eigenvectors of the local covariance matrix at xi . Both criteria gave similar
results experimentally, but the model proposed here yields a complete density estimator. In
both cases F (xi ) can be interpreted as specifying the directions in which one expects to
see the most variations when going from xi to one of its near neighbors in a ﬁnite sample.

 

 


xi

µ

qs2
1

2
+ σ
noise

v1

σnoise

tangent
plane

Figure 1: Illustration of the local parametrization of local or Non-Local Manifold Parzen.
The examples around training point xi are modeled by a Gaussian. µ(xi ) speciﬁes the
center of that Gaussian, which should be non-zero when xi is off the manifold. vk ’s are
principal directions of the Gaussian and are tangent vectors of the manifold. σnoise repre-
sents the thickness of the manifold.

4 Proposed Algorithm: Non-Local Manifold Parzen Windows

In equations (1) and (2) we wrote µ(xi ) and S (xi ) as if they were functions of xi rather
than simply using indices µi and Si . This is because we introduce here a non-local ver-
sion of Manifold Parzen Windows inspired from the non-local manifold tangent learning
algorithm, i.e., in which we can share information about the density across different
regions of space. In our experiments we use a neural network of nhid hidden neurons,
with xi in input to predict µ(xi ), σ2
noise (xi ), and the s2
j (xi ) and vj (xi ). The vectors com-
puted by the neural network do not need to be orthonormal: we only need to consider the
j (xi ), instead
subspace that they span. Also, the vectors’ squared norm is used to infer s2
of having a separate output for them. We will note F (xi ) the matrix whose rows are the
j (xi ) and vj (xi ) by perform-
vectors output of the neural network. From it we obtain the s2
ing a singular value decomposition, i.e. F ′F = Pd
j . Moreover, to make sure
j=1 s2
j vj v ′
noise does not get too small, which could make the optimization unstable, we impose
σ2
0 , where snoise (·) is an output of the neural network and σ2
0 is
σ2
noise (xi ) = s2
noise (xi ) + σ2
a ﬁxed constant.

Imagine that the data were lying near a lower dimensional manifold. Consider a training
example xi near the manifold. The Gaussian centered near xi tells us how neighbors of

vj (xi ) span the tangent of the
xi are expected to differ from xi . Its “principal” vectors
manifold near xi . The Gaussian center variation µ(xi ) tells us how xi is located with
noise (xi ) tells us how far
respect to its projection on the manifold. The noise variance σ2
from the manifold to expect neighbors, and the directional variances s2
j (xi ) + σ2
noise (xi )
tell us how far to expect neighbors on the different local axes of the manifold, near xi ’s
projection on the manifold. Figure 1 illustrates this in 2 dimensions.

The important element of this model is that the parameters of the predictive neural network
can potentially represent non-local structure in the density, i.e., they allow to potentially
discover shared structure among the different covariance matrices in the mixture. Here is
the pseudo code algorithm for training Non-Local Manifold Parzen (NLMP):

Algorithm NLMP::Train(X, d, k , kµ , µ(·), S (·), σ2
0 )
Input: training set X , chosen number of principal directions d, chosen number of
neighbors k and kµ , initial functions µ(·) and S (·), and regularization hyper-parameter
0 .
σ2
(1) For xi ∈ X
(2) Collect max(k ,kµ) nearest neighbors of xj .
Below, call yj one of the k nearest neighbors, yµ
j one of the kµ nearest neighbors.
(3) Perform a stochastic gradient step on parameters of S (·) and µ(·),
using the negative log-likelihood error signal on the yj , with a Gaussian
of mean xi + µ(xi ) and of covariance matrix S (xi ).

The approximate gradients are:

∂C (yµ
j ,xi )
j ) S (xi )−1 (yµ
∂µ(xi ) = − 1
j − xi − µ(xi ))
nkµ (yµ
∂C (yj ,xi )
1
nk (yj ) (cid:0)T r(S (xi )−1 ) − ||(yj − xi − µ(xi ))′S (xi )−1 ||2 (cid:1)
noise (xi ) = 0.5
∂σ2
∂C (yj ,xi )
∂F (xi ) = 1
nk (yj ) F (xi )S (xi )−1 (cid:0)I − (yj − xi − µ(xi ))(yj − xi − µ(xi ))′S (xi )−1 (cid:1)
where nk (y ) = |Nk (y )| is the number of points in the training set that
have y among their k nearest neighbors.
(4) Go to (1) until a given criterion is satisﬁed (e.g. average NLL of NLMP density
estimation on a validation set stops decreasing)
Result: trained µ(·) and S (·) functions, with corresponding σ2
0 .

Deriving the gradient formula (the derivative of the log-likelihood with respect to the neural
network outputs) is lengthy but straightforward. The main trick is to do a Singular Value
Decomposition of the basis vectors computed by the neural network, and to use known
simplifying formulas for the derivative of the inverse of a matrix and of the determinant of
a matrix. Details on the gradient derivation and on the optimization of the neural network
are given in the technical report (Bengio and Larochelle, 2005).

5 Computationally Efﬁcient Extension: Test-Centric NLMP

While the NLMP algorithm appears to perform very well, one of its main practical lim-
itation for density estimation, that it shares with Manifold Parzen, is the large amount of
computation required upon testing: for each test point x, the complexity of the computation
is O(n.d.D) (where D is the dimensionality of input space RD ).
However there may be a different and cheaper way to compute an estimate of the density
at x. We build here on an idea suggested in (Vincent, 2003), which yields an estimator that

does not exactly integrate to one, but this is not an issue if the estimator is to be used for
applications such as classiﬁcation. Note that in our presen tation of NLMP, we are using
“hard ” neighborhoods (i.e. a local weighting kernel that as
signs a weight of 1 to the k
nearest neighbors and 0 to the rest) but it could easily be generalized to “soft” weighting,
as in (Vincent, 2003).
Let us decompose the true density at x as: p(x) = p(x|x ∈ Bk (x))P (Bk (x)), where
Bk (x) represents the spherical ball centered on x and containing the k nearest neighbors
of x (i.e., the ball with radius kx − Nk (x)k where Nk (x) is the k-th neighbor of x in the
training set).

It can be shown that the above NLMP learning procedure looks for functions µ(·) and S (·)
that best characterize the distribution of the k training-set nearest neighbors of x as the
normal N (·; x + µ(x), S (x)). If we trust this locally normal (unimodal) approximation of
the neighborhood distribution to be appropriate then we can approximate p(x|x ∈ Bk (x))
by N (x; x + µ(x), S (x)). The approximation should be good when Bk (x) is small and
p(x) is continuous. Moreover as Bk (x) contains k points among n we can approximate
P (Bk (x)) by k
n .
This yields the estimator ˆp(x) = N (x; x+µ(x), S (x)) k
n , which requires only O(d.D) time
to evaluate at a test point. We call this estimator Test-centric NLMP, since it considers only
the Gaussian predicted at the test point, rather than a mixture of all the Gaussians obtained
at the training points.

6 Experimental Results

We have performed comparative experiments on both toy and real-world data, on density
estimation and classiﬁcation tasks. All hyper-parameters are selected by cross-validation,
and the costs on a large test set is used to compare ﬁnal perfor mance of all algorithms.
Experiments on toy 2D data. To understand and validate the non-local algorithm we
tested it on toy 2D data where it is easy to understand what is being learned. The sinus
data set includes examples sampled around a sinus curve. In the spiral data set examples
are sampled near a spiral. Respectively, 57 and 113 examples are used for training, 23 and
48 for validation (hyper-parameter selection), and 920 and 3839 for testing. The following
algorithms were compared:
• Non-Local Manifold Parzen Windows. The hyper-parameters are the number of princi-
pal directions (i.e., the dimension of the manifold), the number of nearest neighbors k and
kµ , the minimum constant noise variance σ2
0 and the number of hidden units of the neural
network.
• Gaussian mixture with full but regularized covariance matrices. Regularization is done
0 to the eigenvalues of the Gaussians. It is trained
by setting a minimum constant value σ2
by EM and initialized using the k-means algorithm. The hyper-parameter is σ2
0 , and early
stopping of EM iterations is done with the validation set.
• Parzen Windows density estimator, with a spherical Gaussian kernel. The hyper-
parameter is the spread of the Gaussian kernel.
• Manifold Parzen density estimator. The hyper-parameters are the number of principal
0 .
components, k of the nearest neighbor kernel and the minimum eigenvalue σ2
Note that, for these experiments, the number of principal directions (or components) was
ﬁxed to 1 for both NLMP and Manifold Parzen.

Density estimation results are shown in table 1. To help understand why Non-Local Mani-
fold Parzen works well on these data, ﬁgure 2 illustrates the learned densities for the sinus
and spiral data. Basically, it works better here because it yields an estimator that is less sen-
sitive to the speciﬁc samples around each test point, thanks
to its ability to share structure

Algorithm
Non-Local MP
Manifold Parzen
Gauss Mix Full
Parzen Windows

sinus
1.144
1.345
1.567
1.841

spiral
-1.346
-0.914
-0.857
-0.487

Table 1: Average out-of-sample negative log-
likelihood on two toy problems, for Non-Local
Manifold Parzen, a Gaussian mixture with full
covariance, Manifold Parzen, and Parzen Win-
dows. The non-local algorithm dominates all
the others.

across the whole training set.

Algorithm
Non-Local MP
Manifold Parzen
Parzen Windows

Valid.
-73.10
65.21
77.87

Test
-76.03
58.33
65.94

Table 2: Average Negative Log-Likelihood on
the digit rotation experiment, when testing on
a digit class (1’s) not used during training, for
Non-Local Manifold Parzen, Manifold Parzen,
and Parzen Windows. The non-local algorithm
is clearly superior.

Figure 2: Illustration of the learned densities (sinus on top, spiral on bottom) for four com-
pared models. From left to right: Non-Local Manifold Parzen, Gaussian mixture, Parzen
Windows, Manifold Parzen. Parzen Windows wastes probability mass in the spheres around
each point, while leaving many holes. Gaussian mixtures tend to choose too few compo-
nents to avoid overﬁtting. The Non-Local Manifold Parzen ex ploits global structure to yield
the best estimator.

Experiments on rotated digits. The next experiment is meant to show both qualitatively
and quantitatively the power of non-local learning, by using 9 classes of rotated digit images
(from 729 ﬁrst examples of the USPS training set) to learn abo ut the rotation manifold and
testing on the left-out class (digit 1), not used for training. Each training digit was rotated
by 0.1 and 0.2 radians and all these images were used as training data. We used NLMP
for training, and for testing we formed an augmented mixture with Gaussians centered not
only on the training examples, but also on the original unrotated 1 digits. We tested our
estimator on the rotated versions of each of the 1 digits. We compared this to Manifold
Parzen trained on the training data containing both the original and rotated images of the
training class digits and the unrotated 1 digits. The objective of the experiment was to see
if the model was able to infer the density correctly around the original unrotated images,
i.e., to predict a high probability for the rotated versions of these images. In table 2 we see
quantitatively that the non-local estimator predicts the rotated images much better.

As qualitative evidence, we used small steps in the principal direction predicted by Test-
centric NLMP to rotate an image of the digit 1. To make this task even more illustrative of
the generalization potential of non-local learning, we followed the tangent in the direction
opposite to the rotations of the training set.
It can be seen in ﬁgure 3 that the rotated

Figure 3: From left to right: original image of a digit 1; rotated analytically by −0.2
radians; Rotation predicted using Non-Local MP; rotation predicted using MP. Rotations
are obtained by following the tangent vector in small steps.

digit obtained is quite similar to the same digit analytically rotated. For comparison, we
tried to apply the same rotation technique to that digit, but by using the principal direction,
computed by Manifold Parzen, of its nearest neighbor’s Gaussian component in the training
set. This clearly did not work, and hence shows how crucial non-local learning is for this
task.

In this experiment, to make sure that NLMP focusses on the tangent plane of the rotation
manifold, we ﬁxed the number of principal directions d = 1 and the number of nearest
neighbors k = 1, and also imposed µ(·) = 0. The same was done for Manifold Parzen.
Experiments on Classiﬁcation by Density Estimation.
The USPS data set was used
to perform a classiﬁcation experiment. The original traini ng set (7291) was split into a
training ( ﬁrst 6291) and validation set (last 1000), used to tune hyper-parameters. One
density estimator for each of the 10 digit classes is estimated. For comparison we also
show the results obtained with a Gaussian kernel Support Vector Machine (already used
in (Vincent and Bengio, 2003)). Non-local MP* refers to the variation described in (Bengio
and Larochelle, 2005), which attemps to train faster the components with larger variance.
The t-test statistic for the null hypothesis of no difference in the average classiﬁcation
error on the test set of 2007 examples between Non-local MP and the strongest competitor
(Manifold Parzen) is shown in parenthesis. Figure 4 also shows some of the invariant
transformations learned by Non-local MP for this task.

Note that better SVM results (about 3% error) can be obtained using prior knowledge about
image invariances, e.g. with virtual support vectors (Decoste and Scholkopf, 2002). How-
ever, as far as we know the NLMP performance is the best on the original USPS dataset
among algorithms that do not use prior knowledge about images.
Algorithm
Valid.
Test
Hyper-Parameters
C = 100, σ = 8
4.68%
1.2%
SVM
5.08%
1.8%
Parzen Windows
σ = 0.8
d = 11, k = 11, σ2
0.9%
Manifold Parzen
4.08%
0 = 0.1
Non-local MP
0.6% 3.64% (-1.5218)
d = 7, k = 10, kµ = 10,
0 = 0.05, nhid = 70
σ2
d = 7, k = 10, kµ = 4,
0 = 0.05, nhid = 30
σ2

0.6% 3.54% (-1.9771)

Non-local MP*

Table 3: Classiﬁcation error obtained on USPS with SVM, Parzen Windo ws and Local and
Non-Local Manifold Parzen Windows classiﬁers. The hyper-p arameters shown are those
selected with the validation set.

7 Conclusion

We have proposed a non-parametric density estimator that, unlike its predecessors, is able
to generalize far from the training examples by capturing global structural features of the

Figure 4: Tranformations learned by Non-local MP. The top row shows digits taken from
the USPS training set, and the two following rows display the results of steps taken by one
of the 7 principal directions learned by Non-local MP, the third one corresponding to more
steps than the second one.

density. It does so by learning a function with global parameters that successfully predicts
the local shape of the density, i.e., the tangent plane of the manifold along which the density
concentrates. Three types of experiments showed that this idea works, yields improved
density estimation and reduced classiﬁcation error compar ed to its local predecessors.

Acknowledgments
The authors would like to thank the following funding organizations for support: NSERC,
MITACS, and the Canada Research Chairs. The authors are also grateful for the feedback
and stimulating exchanges that helped to shape this paper, with Sam Roweis and Olivier
Delalleau.

References

Bengio, Y., Delalleau, O., and Le Roux, N. (2005). The curse of dimensionality for local
kernel machines. Technical Report 1258, D ´epartement d’in formatique et recherche
op ´erationnelle, Universit ´e de Montr ´eal.
Bengio, Y. and Larochelle, H. (2005). Non-local manifold parzen windows. Technical re-
port, D ´epartement d’informatique et recherche op ´eratio nnelle, Universit ´e de Montr ´eal.
Bengio, Y. and Monperrus, M. (2005). Non-local manifold tangent learning. In Saul, L.,
Weiss, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems
17. MIT Press.
Decoste, D. and Scholkopf, B. (2002). Training invariant support vector machines. Ma-
chine Learning, 46:161 –190.
Goldberger, J., Roweis, S., Hinton, G., and Salakhutdinov, R. (2005). Neighbourhood
component analysis.
In Saul, L., Weiss, Y., and Bottou, L., editors, Advances in
Neural Information Processing Systems 17. MIT Press.
Vincent, P. (2003). Mod `eles `a Noyaux `a Structure Locale. PhD thesis, Universit ´e de
Montr ´eal, D ´epartement d’informatique et recherche op ´e rationnelle, Montreal, Qc.,
Canada.
Vincent, P. and Bengio, Y. (2003). Manifold parzen windows. In Becker, S., Thrun, S.,
and Obermayer, K., editors, Advances in Neural Information Processing Systems 15,
Cambridge, MA. MIT Press.


Two view learning: SVM-2K, Theory and
Practice

Jason D.R. Farquhar
jdrf99r@ecs.soton.ac.uk

David R. Hardoon
drh@ecs.soton.ac.uk

Hongying Meng
hongying@cs.york.ac.uk

John Shawe-Taylor
jst@ecs.soton.ac.uk

Sandor Szedmak
ss03v@ecs.soton.ac.uk

School of Electronics and Computer Science,
University of Southampton, Southampton, England

Abstract

Kernel methods make it relatively easy to deﬁne complex high -
dimensional feature spaces. This raises the question of how we can
identify the relevant subspaces for a particular learning task. When two
views of the same phenomenon are available kernel Canonical Correla-
tion Analysis (KCCA) has been shown to be an effective preprocessing
step that can improve the performance of classi ﬁcation algo rithms such
as the Support Vector Machine (SVM). This paper takes this observa-
tion to its logical conclusion and proposes a method that combines this
two stage learning (KCCA followed by SVM) into a single optimisation
termed SVM-2K. We present both experimental and theoretical analysis
of the approach showing encouraging results and insights.

1

Introduction

Kernel methods enable us to work with high dimensional feature spaces by deﬁning weight
vectors implicitly as linear combinations of the training examples. This even makes it
practical to learn in inﬁnite dimensional spaces as for exam ple when using the Gaussian
kernel. The Gaussian kernel is an extreme example, but techniques have been developed to
deﬁne kernels for a range of different datatypes, in many cas es characterised by very high
dimensionality. Examples are the string kernels for text, graph kernels for graphs, marginal
kernels, kernels for image data, etc.
With this plethora of high dimensional representations it is frequently helpful to assist
learning algorithms by preprocessing the feature space in projecting the data into a low
dimensional subspace that contains the relevant information for the learning task. Methods
of performing this include principle components analysis (PCA) [7], partial least squares
[8], kernel independent component analysis (KICA) [1] and kernel canonical correlation
analysis (KCCA) [5].

The last method requires two views of the data both of which contain all of the relevant
information for the learning task, but which individually contain representation speci ﬁc
details that are different and irrelevant. Perhaps the simplest example of this situation is a
paired document corpus in which we have the same information in two languages. KCCA
attempts to isolate feature space directions that correlate between the two views and hence
might be expected to represent the common relevant information. Hence, one can view this
preprocessing as a denoising of the individual representations through cross-correlating
them.
Experiments have shown how using this as a preprocessing step can improve subsequent
analysis in for example classi ﬁcation experiments using a s upport vector machine (SVM)
[6]. This is explained by the fact that the signal to noise ratio has improved in the identi ﬁed
subspace.
Though the combination of KCCA and SVM seems effective, there appears no guarantee
that the directions identi ﬁed by KCCA will be best suited to t he classi ﬁcation task. This
paper therefore looks at the possibility of combining the two distinct stages of KCCA and
SVM into a single optimisation that will be termed SVM-2K.
The next section introduces the new algorithm and discusses its structure. Experiments are
then given showing the performance of the algorithm on an image classi ﬁcation task.
Though the performance is encouraging it is in many ways counter-intuitive, leading to
speculation about why an improvement is seen. To investigate this question an analysis of
its generalisation properties is given in the following two sections, before drawing conclu-
sions.

2 SVM-2K Algorithm

We assume that we are given two views of the same data, one expressed through a feature
projection φA with corresponding kernel κA and the other through a feature projection φB
with kernel κB . A paired data set is then given by a set

S = {(φA (x1 ), φB (x1 )), . . . , (φA (xℓ ), φB (xℓ ))},

where for example φA could be the feature vector associated with one language and φB
that associated with a second language. For a classi ﬁcation task each data item would also
include a label.
The KCCA algorithm looks for directions in the two feature spaces such that when the
training data is projected onto those directions the two vectors (one for each view) of values
obtained are maximally correlated. One can also characterise these directions as those that
minimise the two norm between the two vectors under the constraint that they both have
norm 1 [5].
We can think of this as constraining the choice of weight vectors in the two spaces. KCCA
would typically ﬁnd a sequence of projection directions of d imension anywhere between
50 and 500 that can then be used as the feature space for training an SVM [6].
An SVM can be thought of as a 1-dimensional projection followed by thresholding, so
SVM-2K combines the two steps by introducing the constraint of similarity between two
1-dimensional projections identifying two distinct SVMs one in each of the two feature
spaces. The extra constraint is chosen slightly differently from the 2-norm that charac-
terises KCCA. We rather take an ǫ-insensitive 1-norm using slack variables to measure the
amount by which points fail to meet ǫ similarity:

|hwA , φA (xi )i + bA − hwB , φB (xi )i − bB | ≤ ηi + ǫ,

where wA , bA (wB , bB ) are the weight and threshold of the ﬁrst (second) SVM.
Combining this constraint with the usual 1-norm SVM constraints and allowing different

regularisation constants gives the following optimisation:

such that

1
2

1
2

ξB
i + D

ηi

min L =

ξA
i + C B

kwA k2 +

ℓ
Xi=1

kwB k2 + C A

ℓ
ℓ
Xi=1
Xi=1
|hwA , φA (xi )i + bA − hwB , φB (xi )i − bB | ≤ ηi + ǫ
yi (hwA , φA (xi )i + bA ) ≥ 1 − ξA
i
yi (hwB , φB (xi )i + bB ) ≥ 1 − ξB
i
ξA
ξB
ηi ≥ 0 all for
i ≥ 0,
i ≥ 0,
Let ˆwA , ˆwB , ˆbA , ˆbB be the solution to this optimisation problem. The ﬁnal SVM-2 K
decision function is then h(x) = sign(f (x)), where
f (x) = 0.5 (cid:16)h ˆwA , φA (x)i + ˆbA + h ˆwB , φB (x)i + ˆbB (cid:17) = 0.5 (fA (x) + fB (x)) .
Applying the usual Lagrange multiplier techniques we arrive at the following dual problem:

1 ≤ i ≤ ℓ.

max W = −

such that

1
2

ℓ
ℓ
Xi=1
Xi,j=1 (cid:0)gA
i + αB
(αA
i gB
j κA (xi , xj ) + gB
i gA
j κB (xi , xj )(cid:1) +
i )
i − β−
i yi + β+
i + β−
i yi − β+
gB
i = αB
gA
i = αA
i ,
i ,
ℓ
ℓ
Xi=1
Xi=1
0 ≤ αA/B
≤ C A/B
i
0 ≤ β+/−
i + β−
β+
i ≤ D
i

gA
i = 0 =

gB
i ,

,

with the functions

fA/B (x) =

ℓ
Xi=1

gA/B
i

κA/B (xi , x) + bA/B .

3 Experimental results

Figure 1: Typical example images from the PASCAL VOC challenge database. Classes
are; Bikes (top-left), People (top-right), Cars (bottom-left) and Motorbikes (bottom-right).

The performance of the algorithms developed in this paper we evaluated on PASCAL Vi-
sual Object Classes (VOC) challenge dataset test11 . This is a new dataset consisting of
four object classes in realistic scenes. The object classes are, motorbikes (M), bicycles (B),
people (P) and cars (C) with the dataset containing 684 training set images consisting of
(214, 114, 84, 272) images in each class and 689 test set images with (216, 114, 84, 275)
for each class. As can be seen in Figure 1 this is a very challenging dataset with objects of
widely varying type, pose, illumination, occlusion, background, etc.
The task is to classify the image according to whether it contains a given object type. We
tested the images containing the object (i.e. categories M, B, C and P) against non-object
images from the database (i.e. category N). The training set contained 100 positive and 100
negative images. The tests are carried out on 100 new images, half belonging to the learned
class and half not.
Like many other successful methods [3, 4] we take a “set-of-p atches ” approach to this
problem. These methods represent an image in terms of the features of a set of small
image patches. By carefully choosing the patches and their features this representation can
be made largely robust to the common types of image transformation, e.g. scale, rotation,
perspective, occlusion.
Two views were provided of each image through the use of different patch types. One was
from afﬁne invariant interest point detectors with a moment
invariant descriptor calculated
for each interest point. The second were key point features from SIFT detectors. For one
image, several hundred characteristic patches were detected according to the complexity
of the images. These were then clustered around K = 400 centres for each feature space.
Each image is then represented as a histogram over these centres. So ﬁnally, for one image
there are two feature vectors of length 400 that provide the two views.

SVM 1
SVM 2
KCCA + SVM
SVM 2K

Motorbike Bicycle
91.58
94.05
91.15
91.15
90.28
94.19
94.34
93.47

People
91.58
90.57
90.57
92.74

Car
87.95
86.21
88.68
90.13

Table 1: Results for 4 datasets showing test accuracy of the individual SVMs and SVM-2K.

Figure 1 show the results of the test errors obtained for the different categories for the
individual SVMs and the SVM-2K. There is a clear improvement in performance of the
SVM-2K over the two individual SVMs in all four categories.
If we examine the structure of the optimisation, the restriction that the output of the two
linear functions be similar seems to be an arbitrary restriction particularly for points that are
far from the margin or are misclassi ﬁed. Intuitively it woul d appear better to take advantage
of the abilities of the different representations to better ﬁt the data.
In order to understand this apparent contradiction we now consider a theoretical analysis
of the generalisation of the SVM-2K using the framework provided by Rademacher com-
plexity bounds.

4 Background theory

We begin with the deﬁnitions required for Rademacher comple xity, see for example Bartlett
and Mendelson [2] (see also [9] for an introductory exposition).
Deﬁnition 1. For a sample S = {x1 , · · · , xℓ } generated by a distribution D on a set

1Available
from
http://www.pascal-network.org/challenges/VOC/voc/
160305 VOCdata.tar.gz

2
ℓ

X and a real-valued function class F with a domain X , the empirical Rademacher
complexity of F is the random variable
ℓ
ˆRℓ (F ) = Eσ " sup
x1 , · · · , xℓ#
σi f (xi ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f ∈F (cid:12)(cid:12)(cid:12)
Xi=1
where σ = {σ1 , · · · , σℓ } are independent uniform {±1}-valued Rademacher random vari-
ables. The Rademacher complexity of F is
σi f (xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f ∈F (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ℓ
#
Rℓ (F ) = ES h ˆRℓ (F )i = ESσ " sup
2
Xi=1
ℓ
We use ED to denote expectation with respect to a distribution D and ES when the distri-
bution is the uniform (empirical) distribution on a sample S .
Theorem 1. Fix δ ∈ (0, 1) and let F be a class of functions mapping from S to [0, 1].
Let (xi )ℓ
i=1 be drawn independently according to a probability distribution D . Then with
probability at least 1 − δ over random draws of samples of size ℓ, every f ∈ F satis ﬁes
ED [f (x)] ≤ ES [f (x)] + Rℓ (F ) + 3q ln(2/δ)
2ℓ
≤ ES [f (x)] + ˆRℓ (F ) + 3q ln(2/δ)
2ℓ
Given a training set S the class of functions that we will primarily be considering are linear
functions with bounded norm
i=1 αiκ (xi , x) : α′K α ≤ B 2o
nx → Pℓ
⊆ {x → hw, φ (x)i : kwk ≤ B } = FB
where φ is the feature mapping corresponding to the kernel κ and K is the corresponding
kernel matrix for the sample S . The following result bounds the Rademacher complexity
of linear function classes.
Theorem 2. [2] If κ : X × X → R is a kernel, and S = {x1 , · · · , xℓ } is a sample of
point from X, then the empirical Rademacher complexity of the class FB satis ﬁes
ℓ vuut
ℓ
2B
Xi=1
For SVM-2K, the two feature sets from the same objects are (φA (xi ))ℓ
i=1 and (φB (xi ))ℓ
i=1
respectively. We assume the notation and optimisation of SVM-2K given in section 2,
equation (1).
First observe that an application of Theorem 1 shows that

2B
ℓ ptr (K )

4.1 Analysing SVM-2K

κ (xi , xi ) =

ˆRℓ (F ) ≤

ES [|fA (x) − fB (x)|] ≤ ES [|h ˆwA , φA (x)i + ˆbA − h ˆwB , φB (x)i − ˆbB |]
ℓ
ℓ ptr(KA ) + tr(KB ) + 3r ln(2/δ)
2C
Xi=1
2ℓ
with probability at least 1−δ . We have assumed that kwA k2 + b2
A ≤ C 2 and kwB k2 + b2
B ≤
C 2 for some preﬁxed C . Hence, the class of functions we are considering when applying

≤ ǫ +

=: D

ηi +

1
ℓ

SVM-2K to this problem can be restricted to
f : x → 0.5   ℓ
i κB (xi , x)(cid:3) + bA + bB ! ,
FC,D = (f (cid:12)(cid:12)(cid:12)(cid:12)
Xi=1 (cid:2)gA
i κA (xi , x) + gB
B ≤ C 2 , ES [|fA (x) − fB (x)|] ≤ D(cid:27)
A ≤ C 2 , gB ′KB gB + b2
gA ′KA gA + b2
The class FC,D is clearly closed under negation.
Applying the usual Rademacher techniques for margin bounds on generalisation we obtain
the following result.
Theorem 3. Fix δ ∈ (0, 1) and let FC,D be the class of functions described above. Let
(xi )ℓ
i=1 be drawn independently according to a probability distribution D . Then with prob-
ability at least 1 − δ over random draws of samples of size ℓ, every f ∈ FC,D satis ﬁes
ℓ
i ) + ˆRℓ (FC,D ) + 3r ln(2/δ)
Xi=1
i + ξB
(ξA
2ℓ
It therefore remains to compute the empirical Rademacher complexity of FC,D , which
is the critical discriminator between the bounds for the individual SVMs and that of the
SVM-2K.

P(x,y)∼D (sign(f (x)) 6= y) ≤

0.5
ℓ

.

4.2 Empirical Rademacher complexity of FC,D
We now deﬁne an auxiliary function of two weight vectors wA and wB ,

D(wA , wB ) := ED [|hwA , φA (x)i + bA − hwB , φB (x)i − bB |]

With this notation we can consider computing the Rademacher complexity of the class
FC,D .

2
ℓ

#

f ∈FC,D (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
σi f (xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ℓ
#
ˆRℓ (FC,D ) = Eσ " sup
Xi=1
ℓ
= Eσ "
(cid:12)(cid:12)(cid:12)(cid:12)
σi [hwA , φA (xi )i + bA + hwB , φB (xi )i + bB ] (cid:12)(cid:12)(cid:12)(cid:12)
1
Xi=1
sup
ℓ
kwA k≤C, kwB k≤C
D(wA ,wB )≤D
Our next observation follows from a reversed version of the basic Rademacher complexity
theorem reworked to reverse the roles of the empirical and true expectations:
Theorem 4. Fix δ ∈ (0, 1) and let F be a class of functions mapping from S to [0, 1].
Let (xi )ℓ
i=1 be drawn independently according to a probability distribution D . Then with
probability at least 1 − δ over random draws of samples of size ℓ, every f ∈ F satis ﬁes
ES [f (x)] ≤ ED [f (x)] + Rℓ (F ) + 3q ln(2/δ)
2ℓ
≤ ED [f (x)] + ˆRℓ (F ) + 3q ln(2/δ)
2ℓ
The proof tracks that of Theorem 1 but is omitted through lack of space.
For weight vectors wA and wB satisfying D(wA , wB ) ≤ D , an application of Theorem 4

shows that with probability at least 1 − δ we have

ˆD(wA , wB )

=: ˆD

≤ D +

≤ ǫ +

1
ℓ

:= ES [|hwA , φA (x)i + bA − hwB , φB (x)i − bB |]
ℓ ptr(KA ) + tr(KB ) + 3r ln(2/δ)
2C
2ℓ
ℓ
ℓ ptr(KA ) + tr(KB ) + 6r ln(2/δ)
4C
Xi=1
ηi +
2ℓ
We now return to bounding the Rademacher complexity of FC,D . The above result shows
that with probability greater than 1 − δ
ˆRℓ (cid:0)FC,D (cid:1)
i=1 σi [hwA , φA (xi )i + bA + hwB , φB (xi )i + bB ] (cid:12)(cid:12)(cid:12)(cid:21)
≤ Eσ (cid:20) sup kwA k≤C
ˆD(wA ,wB )≤ ˆD (cid:12)(cid:12)(cid:12)
ℓ Pℓ
1
kwB k≤C
First note that the expression in square brackets is concentrated under the uniform distribu-
tion of Rademacher variables. Hence, we can estimate the complexity for a ﬁxed instantia-
tion ˆσ of the the Rademacher variables σ . We now must ﬁnd the value of wA and wB that
maximises the expression
ˆσi#(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ℓ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ℓ
ℓ
ˆσiφA (xi )+ + bA
"*wA ,
ˆσi + *wB ,
1
Xi=1
Xi=1
1
ℓ (cid:12)(cid:12) ˆσ ′KA gA + ˆσ ′KB gB + (bA + bB ) ˆσ ′ j(cid:12)(cid:12)
=
subject to the constraints gA ′KA gA ≤ C 2 , gB ′KB gB ≤ C 2 , and
1
1′ abs(KA gA − KB gB + (bA − bB )1) ≤ ˆD
ℓ
where 1 is the all ones vector and abs(u) is the vector obtained by applying the abs function
to u component-wise. The resulting value of the objective function is the estimate of the
Rademacher complexity. This is the optimisation solved in the brief experiments described
below.

ˆσiφB (xi )+ + bB

ℓ
Xi=1

ℓ
Xi=1

4.3 Experiments with Rademacher complexity

We computed the Rademacher complexity for the problems considered in the experimental
section above. We wished to verify that the Rademacher complexity of the space FC,D ,
where C and D are determined by applying the SVM-2K, are indeed signi ﬁcan tly lower
than that obtained for the SVMs in each space individually.

SVM 1
Rad 1
SVM 2
Rad 2
SVM 2K
Rad 2K

Motorbike Bicycle
91.58
94.05
1.65
0.93
91.15
91.15
1.48
1.72
93.47
94.34
1.26
1.28

People
91.58
0.91
90.57
0.87
92.74
0.82

Car
87.95
1.60
86.21
1.64
90.13
1.26

Table 2: Results for 4 datasets showing test accuracy and Rademacher complexity (Rad) of
the individual SVMs and SVM-2K.

Table 2 shows the results for the motorbike, bicycle, people and car datasets. We show
the Rademacher complexities for the individual SVMs and for the SVM-2K along with
the generalisation results already given in Table 1. In the case of SVM-2K we sampled
the Rademacher variables 10 times and give the corresponding standard deviation. As pre-
dicted the Rademacher complexity is signi ﬁcantly smaller f or SVM-2K, hence conﬁrming
the intuition that led to the introduction of the approach, namely that the complexity of the
class is reduced by restricting the weight vectors to align on the training data. Provided
both representations contain the necessary data we can therefore expect an improvement in
generalisation as observed in the reported experiments.

5 Conclusions

With the plethora of data now being collected in a wide range of ﬁelds there is frequently
the luxury of having two views of the same phenomenon. The simplest example is paired
corpora of documents in different languages, but equally we can think of examples from
bioinformatics, machine vision, etc. Frequently it is also reasonable to assume that both
views contain all of the relevant information required for a classi ﬁcation task.
We have demonstrated that in such cases it can be possible to leaver the correlation be-
tween the two views to improve classi ﬁcation accuracy. This has been demonstrated in
experiments with a machine vision task. Furthermore, we have undertaken a theoretical
analysis to illuminate the source and extent of the advantage that can be obtained, show-
ing in the cases considered a signi ﬁcant reduction in the Rad emacher complexity of the
corresponding function classes.

References

[1] Francis R. Bach and Michael I. Jordan. Kernel independent component analysis. Jour-
nal of Machine Learning Research, 3:1–48, 2002.
[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds
and structural results. Journal of Machine Learning Research, 3:463–482, 2002.
[3] G. Csurka, C. Bray, C. Dance, and L. Fan. Visual categorization with bags of keypoints.
In XRCE Research Reports, XEROX. The 8th European Conference on Computer Vi-
sion - ECCV, Prague, 2004.
[4] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised
scale-invariant learning. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2003.
[5] David Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation
analysis: An overview with application to learning methods. Neural Computation,
16:2639–2664, 2004.
[6] Yaoyong Li and John Shawe-Taylor. Using kcca for japanese-english cross-language
information retrieval and classi ﬁcation.
to appear in Journal of Intelligent Information
Systems, 2005.
[7] S. Mika, B. Sch ¨olkopf, A. Smola, K.-R. M ¨uller, M. Scholz, and G. R ¨atsch. Kernel
PCA and de-noising in feature spaces. In Advances in Neural Information Processing
Systems 11, 1998.
[8] R. Rosipal and L. J. Trejo. Kernel partial least squares regression in reproducing kernel
hilbert space. Journal of Machine Learning Research, 2:97–123, 2001.
[9] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge
University Press, Cambridge, UK, 2004.


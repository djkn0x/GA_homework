Radial Basis Function Network for Multi-task
Learning

Xuejun Liao
Department of ECE
Duke University
Durham, NC 27708-0291, USA
xjliao@ee.duke.edu

Lawrence Carin
Department of ECE
Duke University
Durham, NC 27708-0291, USA
lcarin@ee.duke.edu

Abstract

We extend radial basis function (RBF) networks to the scenario in which
multiple correlated tasks are learned simultaneously, and present the cor-
responding learning algorithms. We develop the algorithms for learn-
ing the network structure, in either a supervised or unsupervised manner.
Training data may also be actively selected to improve the network’s gen-
eralization to test data. Experimental results based on real data demon-
strate the advantage of the proposed algorithms and support our conclu-
sions.

1

Introduction

In practical applications, one is frequently confronted with situations in which multiple
tasks must be solved. Often these tasks are not independent, implying what is learned from
one task is transferable to another correlated task. By making use of this transferability,
each task is made easier to solve. In machine learning, the concept of explicitly exploiting
the transferability of expertise between tasks, by learning the tasks simultaneously under a
uni ﬁed representation, is formally referred to as “multi-t
ask learning” [1].

In this paper we extend radial basis function (RBF) networks [4,5] to the scenario of multi-
task learning and present the corresponding learning algorithms. Our primary interest is to
learn the regression model of several data sets, where any given data set may be correlated
with some other sets but not necessarily with all of them. The advantage of multi-task
learning is usually manifested when the training set of each individual task is weak, i.e., it
does not generalize well to the test data. Our algorithms intend to enhance, in a mutually
beneﬁcial way, the weak training sets of multiple tasks, by l earning them simultaneously.
Multi-task learning becomes super ﬂuous when the data sets a ll come from the same gen-
erating distribution, since in that case we can simply take the union of them and treat the
union as a single task. In the other extreme, when all the tasks are independent, there is no
correlation to utilize and we learn each task separately.

The paper is organized as follows. We deﬁne the structure of m ulti-task RBF network
in Section 2 and present the supervised learning algorithm in Section 3. In Section 4 we
show how to learn the network structure in an unsupervised manner, and based on this
we demonstrate how to actively select the training data, with the goal of improving the

generalization to test data. We perform experimental studies in Section 5 and conclude the
paper in Section 6.

2 Multi-Task Radial Basis Function Network

Figure 1 schematizes the radial basis function (RBF) network structure customized to mul-
titask learning. The network consists of an input layer, a hidden layer, and an output layer.
The input layer receives a data point x = [x1 , · · · , xd ]T ∈ Rd and submits it to the hidden
layer. Each node at the hidden layer has a localized activation φn (x) = φ(||x − cn ||, σn ),
n = 1, · · · , N , where || · || denotes the vector norm and φn (·) is a radial basis function
(RBF) localized around cn with the degree of localization parameterized by σn . Choos-
ing φ(z , σ) = exp(− z2
2σ2 ) gives the Gaussian RBF. The activations of all hidden nodes
are weighted and sent to the output layer. Each output node represents a unique task
and has its own hidden-to-output weights. The weighted activations of the hidden nodes
are summed at each output node to produce the output for the associated task. Denoting
wk = [w0k , w1k , · · · , wN k ]T as the weights connecting hidden nodes to the k-th output
node, then the output for the k-th task, in response to input x, takes the form

(1)
fk (x) = wT
k φ(x)
where φ(x) = (cid:2)φ0 (x), φ1 (x), . . . , φN (x)(cid:3)T is a column containing N + 1 basis functions
with φ0 (x) ≡ 1 a dummy basis accounting for the bias in Figure 1.

f1(x)

f2(x)

fK(x)

Network response

Task
1

w
1

Task
2

w
2

…

Task
K

Output layer

(Specified by the 
number of tasks)

w
K

Hidden-to-output weights

(cid:73)0 (cid:123)1

Bias

(cid:73)1(x) (cid:73)2(x)

…

(cid:73)N(x)

Hidden layer (basis functions)
(To be learned by algorithms)

…

Input layer

(Specified by data 
dimensionality)

x = [

x1

x2

…

xd

]T

Input data point

Figure 1: A multi-task structure of RBF Network. Each of the output nodes represents a unique
task. Each task has its own hidden-to-output weights but all the tasks share the same hidden nodes.
The activation of hidden node n is characterized by a basis function φn (x) = φ(||x − cn ||, σn ). A
2
2σ2 ), which gives the Gaussian RBF.
typical choice of φ is φ(z , σ) = exp(− z

3 Supervised Learning

the k-th task is Dk =
Suppose we have K tasks and the data set of
{(x1k , y1k ), · · · , (xJk k , yJk k )}, where yik is the target (desired output) of xik . By deﬁ-
nition, a given data point xik is said to be supervised if the associated target yik is provided
and unsupervised if yik is not provided. The deﬁnition extends similarly to a set of d ata

Table 1: Learning Algorithm of Multi-Task RBF Network
Input: {(x1k , y2k ), · · · , (xJk ,k , yJk ,k )}k=1:K , φ(·, σ), σ , and ρ; Output: φ(·) and
k=1 .
{wk }K

1. For m = 1 : K , For n = 1 : Jm , For k = 1 : K , For i = 1 : Jk
Compute bφnm
ik = φ(||xnm − xik ||, σ);
i=1 yik )2 i;
k=1 hPJk
ik − (Jk + ρ)−1 (PJk
2. Let N = 0, φ(·) = 1, e0 = PK
i=1 y2
For k = 1 : K , compute Ak = Jk + ρ, wk = (Jk + ρ)−1PJk
i=1 yik ;
3. For m = 1 : K , For n = 1 : Jm
If bφnm is not marked as “deleted”
For k = 1 : K , compute
qk = PJk
ck = PJk
i=1 ( bφnm
i=1 φik bφnm
ik ,
ik )2 + ρ − cT
k
If there exists k such that qk = 0, mark bφnm as “deleted”;
else, compute δe(φ, bφnm ) using (5).
4. If { bφik }i=1:Jk ,k=1:K are all marked as “deleted”, go to 10.
5. Let (n∗, m∗) = arg max bφnm not marked as “deleted” δe(φ, bφnm); Mark bφn∗m∗ as “deleted”.
6. Tune RBF parameter σN +1 = arg max σ δe(φ, φ(|| · −xn∗m∗ ||, σ))
7. Let φN +1 (·) = φ(|| · −xn∗m∗ ||, σN +1 ); Update φ(·) ← [φT (·), φN +1 (·)]T ;
8. For k = 1 : K
respectively by (A-1) and (A-3) in the appendix; Up-
and wnew
Compute Anew
k
k
date Ak ← Anew
, wk ← wnew
k
k
9. Let eN +1 = eN − δe(φ, φN +1 );
If the sequence {en }n=0:(N +1) is converged, go
to 10, else update N ← N + 1 and go back to 3.
10. Exit and output φ(·) and {wk }K
k=1 .

A−1
k

ck ;

(2)

points. We are interested in learning the functions fk (x) for the K tasks, based on ∪K
k=1Dk .
The learning is based on minimizing the squared error
+ ρ ||wk ||2o
k=1 nPJk
i=1 (cid:0)wT
k φik − yik (cid:1)2
e (φ, w) = PK
where φik = φ(xik ) for notational simplicity. The regularization terms ρ ||wk ||2 , k =
1, · · · , K , are used to prevent singularity of the A matrices deﬁned in (3), and ρ is typically
set to a small positive number. For ﬁxed φ’s, the w’s are solved by minimizing e(φ, w)
with respect to w, yielding
and Ak = PJk
k PJk
wk = A−1
i=1φikφT
ik + ρ I,
i=1 yikφik
In a multi-task RBF network, the input layer and output layer are respectively speci ﬁed by
the data dimensionality and the number of tasks. We now discuss how to determine the
hidden layer (basis functions φ). Substituting the solutions of the w’s in (3) into (2) gives
i=1 (cid:0)y2
k φik (cid:1)
e(φ) = PK
k=1 PJk
ik − yikwT
where e(φ) is a function of φ only because w’s are now functions of φ as given by
(3). By minimizing e(φ), we can determine φ. Recalling that φik is an abbreviation of
φ(xik ) = (cid:2)1, φ1 (xik ), . . . , φN (xik )(cid:3)T , this amounts to determining N , the number of ba-
sis functions, and the functional form of each basis function φn (·), n = 1, . . . , N . Consider
the candidate functions {φnm (x) = φ(||x − xnm ||, σ) : n = 1, · · · , Jm , m = 1, · · · , K }.
We learn the RBF network structure by selecting φ(·) from these candidate functions such
that e(φ) in (4) is minimized. The following theorem tells us how to perform the selection
in a sequential way; the proof is given in the Appendix.

k = 1, · · · , K (3)

(4)

Theorem 1 Let φ(x) = [1, φ1 (x), . . . , φN (x)]T and φN +1 (x) be a single basis function.
Assume the A matrices corresponding to φ and [φ, φN +1 ]T are all non-degenerate. Then
k=1 (cid:0)cT
(cid:1)2
wk − PJk
δe(φ, φN +1 ) = e(φ) − e([φ, φN +1 ]T ) = PK
i=1 yik φN +1
k
ik
where φN +1
ik = φN +1 (φik ), wk and A are the same as in (3), and
dk = PJk
ck = PJk
A−1
i=1 (φN +1
i=1φik φN +1
)2 + ρ,
qk = dk − cT
,
k
k
ik
ik
is full rank and hence it is positive deﬁnite by con-
By the conditions of the theorem Anew
k
struction. By (A-2) in the Appendix, q−1
)−1 , therefore q−1
is a diagonal element of (Anew
k
k
k
is positive and by (5) δe(φ, φN +1 ) > 0, which means adding φN +1 to φ generally makes
the squared error decrease. The decrease δe(φ, φN +1 ) depends on φN +1 . By sequentially
selecting basis functions that bring the maximum error reduction, we achieve the goal of
maximizing e(φ). The details of the learning algorithm are summarized in Table 1.

q−1
k

(5)

ck

(6)

4 Active Learning

In the previous section, the data in Dk are supervised (provided with the targets). In this
section, we assume the data in Dk are initially unsupervised (only x is available without
access to the associated y ) and we select a subset from Dk to be supervised (targets ac-
quired) such that the resulting network generalizes well to the remaining data in Dk . The
approach is generally known as active learning [6]. We ﬁrst l earn the basis functions φ
from the unsupervised data, and based on φ select data to be supervised. Both of these
steps are based on the following theorem, the proof of which is given in the Appendix.
Theorem 2 Let there be K tasks and the data set of the k-th task is Dk ∪ eDk where
i=1 and eDk = {(xik , yik )}Jk+ eJk
Dk = {(xik , yik )}Jk
i=Jk+1 . Let there be two multi-task RBF
networks, whose output nodes are characterized by fk (·) and f ∼
k (·), respectively, for task
k = 1, . . . , K . The two networks have the same given basis functions (hidden nodes)
φ(·) = [1, φ1 (·), · · · , φN (·)]T , but different hidden-to-output weights. The weights of fk (·)
k (·) are trained using eDk . Then for
are trained with Dk ∪ eDk , while the weights of f ∼
k = 1, · · · , K , the square errors committed on Dk by fk (·) and f ∼
k (·) are related by
k (xik ))2 (cid:3)−1PJk
max,k ≤(cid:2)PJk
i=1(yik −fk (xik ))2≤ λ−1
0 ≤ [det Γk ]−1≤ λ−1
i=1(yik −f ∼
min,k ≤ 1 (7)
where Γk = (cid:2)I + ΦT
k )−1Φk (cid:3)2 with Φ = (cid:2)φ(x1k ), . . . , φ(xJk k )(cid:3) and eΦ =
T
k (ρ I + eΦk eΦ
(cid:2)φ(xJk+1,k ), . . . , φ(x
Jk+ eJk ,k )(cid:3), and λmax,k and λmin,k are respectively the largest and
smallest eigenvalues of Γk .
Specializing Theorem 2 to the case eJk = 0, we have
Corollary 1 Let there be K tasks and the data set of the k-th task is Dk = {(xik , yik )}Jk
i=1 .
Let the RBF network, whose output nodes are characterized by fk (·) for task k =
1, . . . , K , have given basis functions (hidden nodes) φ(·) = [1, φ1 (·), · · · , φN (·)]T and
the hidden-to-output weights of task k be trained with Dk . Then for k = 1, · · · , K , the
squared error committed on Dk by fk (·) is bounded as 0 ≤ [det Γk ]−1 ≤ λ−1
max,k ≤
min,k ≤ 1, where Γk = (cid:0)I + ρ−1ΦT
ik (cid:3)−1PJk
Φk (cid:1)2 with
(cid:2)PJk
i=1 (yik − fk (xik ))2 ≤ λ−1
i=1 y2
k
Φ = (cid:2)φ(x1,k ), . . . , φ(xJk ,k )(cid:3), and λmax,k and λmin,k are respectively the largest and
smallest eigenvalues of Γk .
It is evident from the properties of matrix determinant [7] and the deﬁnition of Φ that
det Γk = (cid:2)det(ρI + ΦkΦT
k )(cid:3)2
[det(ρ I)]−2 = (cid:2)det(ρI + PJk
ik )(cid:3)2
i=1 φikφT
[det(ρ I)]−2 .

Using (3) we write succinctly det Γk = [det A2
k ][det(ρ I)]−2 . We are interested in se-
lecting the basis functions φ that minimize the error, before seeing y ’s. By Corollary 1
k ][det(ρ I)]−2 , the squared error is lower bounded by
and the equation det Γk = [det A2
PJk
ik [det(ρ I)]2 [det Ak ]−2 . Instead of minimizing the error directly, we minimize its
i=1 y2
lower bound. As [det(ρ I)]2PLk
ik does not depend on φ, this amounts to selecting φ to
i=1 y2
minimize (det Ak )−2 . To minimize the errors for all tasks k = 1 · · · , K , we select φ to
minimize QK
k=1 (det Ak )−2 .
The selection proceeds in a sequential manner. Suppose we have selected basis func-
tions φ = [1, φ1 , · · · , φN ]T . The associated A matrices are Ak = PJk
i=1 φikφT
ik +
ρ I(N +1)×(N +1) , k = 1, · · · , K . Augmenting basis functions to [φT , φN +1 ]T , the A
= PJk
ik , φN +1
ik , φN +1
i=1 [φT
]T [φT
matrices change to Anew
] + ρ I(N +2)×(N +2) . Us-
k
ik
ik
ing the determinant formula of block matrices [7], we get QK
)−2 =
k=1 (det Anew
k
QK
k=1 (qk det Ak )−2 , where qk is the same as in (6). As Ak does not depend on φN +1 ,
the left-hand side is minimized by maximizing QK
k . The selection is easily imple-
k=1 q2
mented by making the following two minor modi ﬁcations in Tab le 1: (a) in step 2, compute
e0 = PK
k=1 ln(Jk + ρ)−2 ; in step 3, compute δe(φ, bφnm ) = PK
k . Employing the
k=1 ln q2
logarithm is for gaining additivity and it does not affect the maximization.
Based on the basis functions φ determined above, we proceed to selecting data to be su-
pervised and determining the hidden-to-output weights w from the supervised data using
the equations in (3). The selection of data is based on an iterative use of the following
corollary, which is a specialization of Theorem 2 and was originally given in [8].

Corollary 2 Let there be K tasks and the data set of the k-th task is Dk = {(xik , yik )}Jk
i=1 .
Let there be two RBF networks, whose output nodes are characterized by fk (·) and
f +
k (·), respectively, for task k = 1, . . . , K . The two networks have the same given
basis functions φ(·) = [1, φ1 (·), · · · , φN (·)]T , but different hidden-to-output weights.
The weights of fk (·) are trained with Dk , while the weights of f +
k (·) are trained us-
ing D+
k = Dk ∪ {(xJk+1,k , yJk+1,k )}. Then for k = 1, · · · , K , the squared errors
k (·) are related by (cid:2)f +
committed on (xJk+1,k , yJk+1,k ) by fk (·) and f +
k (xJk+1,k ) −
= (cid:2)γ (xJk+1,k )(cid:3)−1 (cid:2)fk (xJk+1,k ) − yJk+1,k (cid:3)2 , where γ (xJk+1,k ) = (cid:2)1 +
yJk+1,k (cid:3)2
i=1 (cid:2)ρI + φ(xik )φT (xik )(cid:3) is the same
k φ(xJk+1,k )(cid:3)2
≥ 1 and Ak = PJk
φT (xJk+1,k )A−1
as in (3).
Two observations are made from Corollary 2. First, if γ (xJk+1,k ) ≈ 1, seeing yJk+1,k
does not effect the error on xJk+1,k , indicating Dk already contain sufﬁcient information
about (xJk+1,k , yJk+1,k ). Second, if γ (xi ) ≫ 1, seeing yJk+1,k greatly decrease the er-
ror on xJk+1,k , indicating xJk+1,k is signi ﬁcantly dissimilar (novel) to Dk and xJk+1,k
must be supervised to reduce the error. Based on Corollary 2, the selection proceeds se-
quentially. Suppose we have selected data Dk = {(xik , yik )}Jk
i=1 , from which we com-
pute Ak . We select the next data point as xJk+1,k = arg max i>Jk , k=1,··· ,K γ (xik ) =
arg max i>Jk k=1,··· ,K (cid:2)1 + φT(xik )A−1
k φ(xik )(cid:3)2 . After xJk+1,k is selected, the Ak is
updated and the next selection begins. As the iteration advances γ will decrease until it
reaches convergence. We use (3) to compute w from the selected x and their associated
targets y , completing learning of the RBF network.

5 Experimental Results

In this section we compare the multi-task RBF network against single-task RBF networks
via experimental studies. We consider three types of RBF networks to learn K tasks, each

K tasks
with its data set Dk . In the ﬁrst, which we call “one RBF network”, we let the
share both basis functions φ (hidden nodes) and hidden-to output weights w, thus we do
not distinguish the K tasks and design a single RBF network to learn a union of them. The
second is the multi-task RBF network, where the K tasks share the same φ but each has its
own w. In the third, we have K independent networks, each designed for a single task.

We use a school data set from the Inner London Education Authority, consisting of ex-
amination records of 15362 students from 139 secondary schools. The data are available
at http://multilevel.ioe.ac.uk/intro/datasets.html. This data set was originally used to study
the effectiveness of schools and has recently been used to evaluate multi-task algorithms
[2,3]. The goal is to predict the exam scores of the students based on 9 variables: year of
exam (1985, 1986, or 1987), school code (1-139), FSM (percentage of students eligible for
free school meals), VR1 band (percentage of students in school in VR band one), gender,
VR band of student (3 categories), ethnic group of student (11 categories), school gender
(male, female, or mixed), school denomination (3 categories). We consider each school a
task, leading to 139 tasks in total. The remaining 8 variables are used as inputs to the RBF
network. Following [2,3], we converted each categorical variable to a number of binary
variables, resulting in a total number of 27 input variables, i.e., x ∈ R27 . The exam score
is the target to be predicted.

The three types of RBF networks as deﬁned above are designed a s follows. The multi-task
RBF network is implemented as the structure as shown in Figure 1 and trained with the
learning algorithm in Table 1. The “one RBF network” is imple mented as a special case
of Figure 1, with a single output node and trained using the union of supervised data from
all 139 schools. We design 139 independent RBF networks, each of which is implemented
with a single output node and trained using the supervised data from a single school. We
use the Gaussian RBF φn (x) = exp(− ||x−cn ||2
), where the cn ’s are selected from training
2σ2
data points and σn ’s are initialized as 20 and optimized as described in Table 1. The main
role of the regularization parameter ρ is to prevent the A matrices from being singular and
it does not affect the results seriously. In the results reported here, ρ is set to 10−6 .
Following [2-3], we randomly take 75% of the 15362 data points as training (supervised)
data and the remaining 25% as test data. The generalization performance is measured by
the squared error (fk (xik ) − yik )2 averaged over all test data xik of tasks k = 1, · · · , K .
We made 10 independent trials to randomly split the data into training and test sets and the
squared error averaged over the test data of all the 139 schools and the trials are shown in
Table 2, for the three types of RBF networks.

Table 2: Squared error averaged over the test data of all 139 schools and the 10 independent trials
for randomly splitting the school data into training (75%) and testing (25%) sets.
Multi-task RBF network
Independent RBF networks One RBF network

109.89 ± 1.8167

136.41 ± 7.0081

149.48 ± 2.8093

Table 2 clearly shows the multi-task RBF network outperforms the other two types of RBF
networks by a considerable margin. The “one RBF network” ign ores the difference be-
tween the tasks and the independent RBF networks ignore the tasks’ correlations, therefore
they both perform inferiorly. The multi-task RBF network uses the shared hidden nodes
(basis functions) to capture the common internal representation of the tasks and meanwhile
uses the independent hidden-to-output weights to learn the statistics speci ﬁc to each task.

We now demonstrate the results of active learning. We use the method in Section 4 to ac-
tively split the data into training and test sets using a two-step procedure. First we learn the
basis functions φ of multi-task RBF network using all 15362 data (unsupervised). Based
on the φ, we then select the data to be supervised and use them as training data to learn

the hidden-to-output weights w. To make the results comparable, we use the same training
data to learn the other two types of RBF networks (including learning their own φ and w).
The networks are then tested on the remaining data.

Figure 2 shows the results of active learning. Each curve is the squared error averaged over
the test data of all 139 schools, as a function of number of training data. It is clear that
the multi-task RBF network maintains its superior performance all the way down to 5000
training data points, whereas the independent RBF networks have their performances de-
graded seriously as the training data diminish. This demonstrates the increasing advantage
of multi-task learning as the number of training data decreases. The “one RBF network”
seems also insensitive to the number of training data, but it ignores the inherent dissimilar-
ity between the tasks, which makes its performance inferior.

Multi−task RBF network
Independent RBF networks
One RBF network

260

240

220

200

180

160

140

120

a
t
a
d
 
t
s
e
t
 
r
e
v
o
 
d
e
g
a
r
e
v
a
 
r
o
r
r
e
 
d
e
r
a
u
q
S

100
5000

6000

7000
10000
9000
8000
Number of training (supervised) data

11000

12000

Figure 2: Squared error averaged over the test data of all 139 schools, as a function of the number
of training (supervised) data. The data are split into training and test sets via active learning.

6 Conclusions

We have presented the structure and learning algorithms for multi-task learning with the
radial basis function (RBF) network. By letting multiple tasks share the basis functions
(hidden nodes) we impose a common internal representation for correlated tasks. Exploit-
ing the inter-task correlation yields a more compact network structure that has enhanced
generalization ability. Unsupervised learning of the network structure enables us to actively
split the data into training and test sets. As the data novel to the previously selected ones are
selected next, what ﬁnally remain unselected and to be teste d are all similar to the selected
data which constitutes the training set. This improves the generalization of the resulting
network to the test data. These conclusions are substantiated via results on real multi-task
data.

References

[1] R. Caruana. (1997) Multitask learning. Machine Learning, 28, p. 41-75, 1997.

[2] B. Bakker and T. Heskes (2003). Task clustering and gating for Bayesian multitask learning.
Journal of Machine Learning Research, 4: 83-99, 2003

[3] T. Evgeniou, C. A. Micchelli, and M. Pontil (2005). Learning Multiple Tasks with Kernel Meth-
ods. Journal of Machine Learning Research, 6: 615637, 2005

[4] Powell M. (1987), Radial basis functions for multivariable interpolation : A review, J.C. Mason
and M.G. Cox, eds, Algorithms for Approximation, pp.143-167.

[5] Chen, F. Cowan, and P. Grant (1991), Orthogonal least squares learning algorithm for radial basis
function networks, IEEE Transactions on Neural Networks, Vol. 2, No. 2, 302-309, 1991

[6] Cohn, D. A., Ghahramani, Z., and Jordan, M. I. (1995). Active learning with statistical models.
Advances in Neural Information Processing Systems, 7, 705-712.

[7] V. Fedorov (1972), Theory of Optimal Experiments, Academic Press, 1972

[8] M. Stone (1974), Cross-validatory choice and assessment of statistical predictions, Journal of the
Royal Statistical Society, Series B, 36, pp. 111-147, 1974.

Appendix

(A-1)

are all

(A-2)

(A-3)

(Anew
k

Proof of Theorem 1:. Let φnew = [φ, φN +1 ]T . By (3), the A matrices corresponding to φnew are
(cid:3) + ρ I(N +2)×(N +2) = h Ak
dk i
i (cid:2) φT
i=1 h φik
k = PJk
ck
Anew
ik φN +1
φN +1
cT
ik
k
ik
where ck and dk are as in (6). By the conditions of the theorem, the matrices Ak and Anew
k
non-degenerate. Using the block matrix inversion formula [7] we get
(cid:21)
)−1=(cid:20)A−1
−A−1
k ck q−1
k + A−1
k ck q−1
k A−1
k cT
k
k
q−1
k A−1
−q−1
k cT
k
k
corresponding to [φT , φN +1 ]T are
where qk is as in (6). By (3), the weights wnew
k
(cid:21)
(cid:21) = (cid:20) wk + A−1
)−1 (cid:20) PJk
k ck q−1
k gk
i=1 yikφik
k = (Anew
wnew
PJk
−q−1
k
i=1 yik φN +1
k gk
ik
ikwk + (cid:0)φT
k wk − PJk
i=1 yik φN +1
ikA−1
with gk = cT
. Hence, (φnew
ik )T wnew
= φT
k ck −
k
ik
(cid:3) =
i=1 (cid:2)y2
(cid:1)gk q−1
k , which is put into (4) to get e(φnew = PK
k=1PJk
φN +1
ik )T wnew
ik − yik (φnew
k
ik
k (cid:3) = e(φ) − PK
k=1 (cid:0)cT
i=1 (cid:2)y2
(cid:1)gk q−1
ikwk − yik (cid:0)φT
PK
k=1PJk
k ck − φN +1
ikA−1
ik − yikφT
k wk −
ik
(cid:1)2
PJk
q−1
i=1 yik φN +1
k , where in arriving the last equality we have used (3) and (4) and gk =
ik
k wk − PJk
i=1 yik φN +1
. The theorem is proved.
cT
(cid:3)
ik
Proof of Theorem 2: The proof applies to k = 1, · · · , K . For any given k , deﬁne Φ =
(cid:2)φ(x1k ), . . . , φ(xJk k )(cid:3), eΦ = (cid:2)φ(xJk +1,k ), . . . , φ(xJk + eJk ,k )(cid:3), yk = [y1k , . . . , yJk k ]T , eyk =
[yJk +1,k , . . . , yJk + eJk ,k ]T , fk = [f (x1k ), . . . , f (xJk k )]T , f ∼
k (xJk k )]T ,
k = [f ∼
k (x1k ), . . . , f ∼
k (cid:0) eAk +
T
and eAk = ρI + eΦk eΦ
k . By (1), (3), and the conditions of the theorem, fk = ΦT
k (cid:1)−1 (Φk yk + eΦeyk )
= (cid:2)ΦT
k −(cid:0)ΦT
k Φk +I−I(cid:1)(cid:0)I+ΦT
k Φk (cid:1)−1
k (cid:3)(cid:2)Φk yk+
(a)
k eA−1
k eA−1
k eA−1
k eA−1
ΦkΦT
ΦT
k + (cid:0)I +
k Φk (cid:1)−1
k (cid:3)(cid:2) eΦk eyk + Φk yk (cid:3) (b)
= (cid:0)I + ΦT
k Φk (cid:1)−1
eΦk eyk (cid:3) = (cid:2)(cid:0)I + ΦT
k eA−1
k eA−1
k eA−1
ΦT
f ∼
k − yk (cid:1), where equa-
k Φk (cid:1)−1 (cid:0)f ∼
k Φk + I − I(cid:1)yk = yk + (cid:0)I + ΦT
k Φk (cid:1)−1 (cid:0)ΦT
k eA−1
k eA−1
k eA−1
ΦT
tion (a) is due to the Sherman-Morrison-Woodbury formula and equation (b) results because
k − yk (cid:1), which gives
k Φk (cid:1)−1 (cid:0)f ∼
k eΦk eyk . Hence, fk − yk = (cid:0)I + ΦT
k eA−1
k eA−1
k = ΦT
f ∼
k − yk (cid:1)T
k (cid:0)f ∼
k − yk (cid:1)
i=1 (yik − fk (xik ))2 = (fk − yk )T (fk − yk ) = (cid:0)f ∼
PJk
Γ−1
k Φk (cid:3)2 = (cid:2)I + ΦT
where Γk = (cid:2)I + ΦT
k )−1Φk (cid:3)2 .
T
k eA−1
k (ρ I + eΦk eΦ
k diag[λ1k , · · · , λJk k ]Ek with
By construction, Γk has all its eigenvalues no less than 1, i.e., Γk = ET
k Ek = I and λ1k , · · · , λJk k ≥ 1, which makes the ﬁrst, second, and last inequality in (7) hold.
ET
Using this expansion of Γk in (A-4) we get
Jk k ](cid:0)f ∼
k − yk (cid:1)
i=1 (fk (xik ) − yik )2 = (cid:0)f ∼
k − yk (cid:1)T
PJk
1k , . . . , σ−1
k diag[σ−1
ET
k − yk (cid:1) = λ−1
min,k I(cid:3)Ek (cid:0)f ∼
k (cid:2)λ−1
≤ (cid:0)f ∼
k − yk (cid:1)T
min,kPJk
k (xik ) − yik )2
ET
(A-5)
i=1(f ∼
where the inequality results because λmin,k = min(λ1,k , · · · , λJk ,k ). From (A-5) follows the
fourth inequality in (7). The third inequality in (7) can be proven in in a similar way.
(cid:3)

(A-4)


Learning Multiple Related Tasks using Latent
Independent Component Analysis

Jian Zhangy, Zoubin Ghahramaniyz, Yiming Yangy

z Gatsby Computational Neuroscience Unit
y School of Computer Science
University College London
Cargenie Mellon University
London WC1N 3AR, UK
Pittsburgh, PA 15213
fjian.zhang, zoubin, yimingg@cs.cmu.edu

Abstract

We propose a probabilistic model based on Independent Component
Analysis for learning multiple related tasks. In our model the task pa-
rameters are assumed to be generated from independent sources which
account for the relatedness of the tasks. We use Laplace distributions
to model hidden sources which makes it possible to identify the hidden,
independent components instead of just modeling correlations. Further-
more, our model enjoys a sparsity property which makes it both parsimo-
nious and robust. We also propose efﬁcient algorithms for both empiri-
cal Bayes method and point estimation. Our experimental results on two
multi-label text classiﬁcation data sets show that the proposed approach
is promising.

1 Introduction

An important problem in machine learning is how to generalize between multiple related
tasks. This problem has been called “multi-task learning ”,
“learning to learn ”, or in some
cases “predicting multivariate responses”. Multi-task learning has many potential practical
applications. For example, given a newswire story, predicting its subject categories as
well as the regional categories of reported events based on the same input text is such
a problem. Given the mass tandem spectra of a sample protein mixture, identifying the
individual proteins as well as the contained peptides is another example.

Much attention in machine learning research has been placed on how to effectively learn
multiple tasks, and many approaches have been proposed[1][2][3][4 ][5][6][10][11]. Exist-
ing approaches share the basic assumption that tasks are related to each other. Under this
general assumption, it would be beneﬁcial
to learn all tasks jointly and borrow information
from each other rather than learn each task independently. Previous approaches can be
roughly summarized based on how the “relatedness”
among tasks is modeled, such as IID
tasks[2], a Bayesian prior over tasks[2][6][11], linear mixing factors[5][10], rotation plus
shrinkage[3] and structured regularization in kernel methods[4].

Like previous approaches, the basic assumption in this paper is that the multiple tasks are
related to each other. Consider the case where there are K tasks and each task is a binary

classiﬁcation problem from the same input space (e.g., multiple simultaneous classiﬁca-
tions of text documents). If we were to separately learn a classiﬁer , with parameters (cid:18)k for
each task k , we would be ignoring relevant information from the other classiﬁers. The as-
sumption that the tasks are related suggests that the (cid:18)k for different tasks should be related
to each other. It is therefore natural to consider different statistical models for how the (cid:18)k ’s
might be related.

We propose a model for multi-task learning based on Independent Component Analysis
(ICA)[9]. In this model, the parameters (cid:18)k for different classiﬁers
are assumed to have
been generated from a sparse linear combination of a small set of basic classiﬁers. Both the
coefﬁcients of the sparse combination (the factors or sources) and the basic classiﬁers are
learned from the data. In the multi-task learning context, the relatedness of multiple tasks
can be explained by the fact that they share certain number of hidden, independent compo-
nents. By controlling the model complexity in terms of those independent components we
are able to achieve better generalization capability. Furthermore, by using distributions like
Laplace we are able to enjoy a sparsity property, which makes the model both parsimonious
and robust in terms of identifying the connections with independent sources. Our model
can be combined with many popular classiﬁers,
and as an indispensable part we present
scalable algorithms for both empirical Bayes method and point estimation, with the later
being able to solve high-dimensional tasks. Finally, being a probabilistic model it is always
convenient to obtain probabilistic scores and con ﬁdence which are very helpful in making
statistical decisions. Further discussions on related work are given in Section 5.

2 Latent Independent Component Analysis

The model we propose for solving multiple related tasks, namely the Latent Independent
Component Analysis (LICA) model, is a hierarchical Bayesian model based on the tra-
ditional Independent Component Analysis. ICA[9] is a promising technique from signal
processing and designed to solve the blind source separation problem, whose goal is to
extract independent sources given only observed data that are linear combinations of the
unknown sources. ICA has been successfully applied to blind source separation problem
and shows great potential in that area. With the help of non-Gaussianity and higher-order
statistics it can correctly identify the independent sources, as opposed to technique like
Factor Analysis which is only able to remove the correlation in the data due to the intrinsic
Gaussian assumption in the corresponding model.

In order to learn multiple related tasks more effectively, we transform the joint learning
problem into learning a generative probabilistic model for our tasks (or more precisely,
task parameters), which precisely explains the relatedness of multiple tasks through the
latent, independent components. Unlike the standard Independent Component Analysis
where we use observed data to estimate the hidden sources, in LICA the “observ ed data”
for ICA are actually task parameters. Consequently, they are latent and themselves need
to be learned from the training data of each individual task. Below we give the precise
deﬁnition of the probabilistic model for LICA.
Suppose we use (cid:18)1 ; (cid:18)2 ; : : : ; (cid:18)K to represent the model parameters of K tasks where (cid:18)k 2
RF (cid:2)1 can be thought as the parameter vector of the k -th individual task. Consider the
following generative model for the K tasks:

(cid:18)k = (cid:3)sk + ek
sk (cid:24) p(sk j (cid:8))
ek (cid:24) N (0; (cid:9))

(1)

where sk 2 RH(cid:2)1 are the hidden source models with (cid:8) denotes its distribution param-
eters; (cid:3) 2 RF (cid:2)H is a linear transformation matrix; and the noise vector ek 2 RF (cid:2)1

Figure 1: Graphical Model for Latent Independent Component Analysis
is usually assumed to be a multivariate Gaussian with diagonal covariance matrix (cid:9) =
diag( 11 ; : : : ;  F F ) or even (cid:9) = (cid:27) 2 I. This is essentially assuming that the hidden sources
s are responsible for all the dependencies among (cid:18)k ’s, and conditioned on them all (cid:18)k ’s are
independent. Generally speaking we can use any member of the exponential families as
p(ek ), but in most situations the noise is taken to be a multivariate Gaussian which is con-
venient. The graphical model for equation (1) is shown as the upper level in Figure 1,
whose lower part will be described in the following.

2.1 Probabilistic Discriminative Classiﬁers

One building block in the LICA is the probabilistic model for learning each individual
task, and in this paper we focus on classiﬁcation tasks. We will use the following notation
to describe a probabilistic discriminative classiﬁer
for task k , and for notation simplicity we
omit the task index k below. Suppose we have training data D = f(x1 ; y1 ); : : : ; (xN ; yN )g
where xi 2 RF (cid:2)1 is the input data vector and yi 2 f0; 1g is the binary class label, our goal
is to seek a probabilistic classiﬁer whose prediction is based on the conditional probability
4
= f (x) 2 [0; 1]. We further assume that the discriminative function to have
p(y = 1jx)
a linear form f (x) = (cid:22)((cid:18)T x), which can be easily generalized to non-linear functions by
some feature mapping. The output class label y can be thought as randomly generated from
a Bernoulli distribution with parameter (cid:22)((cid:18)T x), and the overall model can be summarized
as follows:
yi (cid:24) B((cid:22)((cid:18)T xi ))
(cid:22)(t) = Z t
(cid:0)1
where B(:) denotes the Bernoulli distribution and p(z ) is the probability density function
of some random variable Z . By changing the deﬁnition of random variable Z we are able
to specialize the above model into a variety of popular learning methods. For example,
when p(z ) is standard logistic distribution we will get logistic regression classiﬁer; when
p(z ) is standard Gaussian we get the probit regression. In principle any member belonging
to the above class of classiﬁers can be plugged in our LICA, or even generative classiﬁers
like Naive Bayes. We take logistic regression as the basic classiﬁer , and this choice should
not affect the main point in this paper. Also note that it is straightforward to extend the
framework for regression tasks whose likelihood function y i (cid:24) N ((cid:18)T xi ; (cid:27)2 ) can be solved
by simple and efﬁcient algorithms. Finally we would like to point out that although shown
in the graphical model that all training instances share the same input vector x, this is
mainly for notation simplicity and there is indeed no such restriction in our model. This is
convenient since in reality we may not be able to obtain all the task responses for the same
training instance.

p(z )dz

(2)

3 Learning and Inference for LICA

The basic idea of the inference algorithm for the LICA is to iteratively estimate the task
parameters (cid:18)k , hidden sources sk , and the mixing matrix (cid:3) and noise covariance (cid:9). Here
we present two algorithms, one for the empirical Bayes method, and the other for point
estimation which is more suitable for high-dimensional tasks.

3.1 Empirical Bayes Method

The graphical model shown in Figure 1 is an example of a hierarchical Bayesian model,
where the upper levels of the hierarchy model the relation between the tasks. We can use
an empirical Bayes approach and learn the parameters (cid:10) = f(cid:8); (cid:3); (cid:9)g from the data while
treating the variables Z = f(cid:18)k ; sk gK
k=1 as hidden, random variables. To get around the
unidentiﬁability caused by the interaction between (cid:3) and s we assume (cid:8) is of standard
parametric form (e.g. zero mean and unit variance) and thus remove it from (cid:10). The goal
is to learn point estimators ^(cid:3) and ^(cid:9) as well as obtain posterior distributions over hidden
variables given training data.
The log-likelihood of incomplete data log p(D j (cid:10)) 1 can be calculated by integrating out
hidden variables
j xi ; (cid:18)k ) (cid:18)Z p((cid:18)k j sk ; (cid:3); (cid:9))p(sk j(cid:8))dsk(cid:19) d(cid:18)k)
log (Z
K
N
Xk=1
Yi=1
for which the maximization over parameters (cid:10) = f(cid:3); (cid:9)g involves two complicated inte-
grals over (cid:18)k and sk , respectively. Furthermore, for classiﬁcation tasks the likelihood func-
tion p(y jx; (cid:18)) is typically non-exponential and thus exact calculation becomes intractable.
However, we can approximate the solution by applying the EM algorithm to decouple it
into a series of simpler E-steps and M-steps as follows:

log p(Dj(cid:10)) =

p(y (k)
i

1. E-step: Given the parameter (cid:10)t(cid:0)1 = f(cid:3); (cid:9)gt(cid:0)1 from the (t (cid:0) 1)-th step, compute
the distribution of hidden variables given (cid:10)t(cid:0)1 and D: p(Z j (cid:10)t(cid:0)1 ; D)
2. M-step: Maximizing the expected log-likelihood of complete data (Z ; D), where
the expectation is taken over the distribution of hidden variables obtained in the
E-step: (cid:10)t = arg max(cid:10) Ep(Z j(cid:10)t(cid:0)1 ;D) [log p(D; Z j (cid:10))]

log p(y (k)
i

log p(D; Z j (cid:10)) =

The log-likelihood of complete data can be written as
Xk=1 ( N
j xi ; (cid:18)k ) + log p((cid:18)k j sk ; (cid:3); (cid:9)) + log p(sk j (cid:8)))
K
Xi=1
where the ﬁrst and third item do not depend on (cid:10). After some simpliﬁcation the M-step
can be summarized as f ^(cid:3); ^(cid:9)g = arg max(cid:3);(cid:9) PK
E[log p((cid:18)k j sk ; (cid:3); (cid:9))] which leads to
k=1
the following updating equations:
(cid:0)1
k ]) ^(cid:3)T !
K   K
k ]!
k ]!   K
^(cid:3) =   K
K
1
; ^(cid:9) =
Xk=1
Xk=1
Xk=1
Xk=1
E[(cid:18)k sT
E[sk sT
E[(cid:18)k sT
In the E-step we need to calculate the posterior distribution p(Z j D; (cid:10)) given the pa-
rameter (cid:10) calculated in previous M-step. Essentially only the ﬁrst
and second order

E[(cid:18)k (cid:18)T
k ] (cid:0) (

1Here with a little abuse of notation we ignore the difference of discriminative and generative at
the classiﬁer level and use p(D j (cid:18)k ) to denote the likelihood in general.

Algorithm 1 Variational Bayes for the E-step (subscript k is removed for simplicity)
1. Initialize q(s) with some standard distribution (Laplace distribution in our case):
q(s) = QH
h=1 L(0; 1).
2. Solve the following Bayesian logistic regression (or other Bayesian classiﬁer):
d(cid:18))
q((cid:18)) (Z q((cid:18)) log
N ((cid:18); (cid:3)E[s]; (cid:9)) QN
i=1 p(yi j(cid:18); xi )
q((cid:18))   arg max
q((cid:18))

3. Update q(s):
q(s)(cid:26)Z q(s) (cid:20)log
Tr (cid:0)(cid:9)(cid:0)1 (E[(cid:18)(cid:18)T ]+(cid:3)ssT (cid:3)T (cid:0) 2E[(cid:18)]((cid:3)s)T )(cid:1)(cid:21) ds(cid:27)
q(s) arg max
4. Repeat steps 2-5 until convergence conditions are satisﬁed.

p(s)
q(s)

(cid:0)

1
2

moments are needed, namely: E[(cid:18)k ], E[sk ], E[(cid:18)k (cid:18)T
k ] and E[(cid:18)k sT
k ], E[sk sT
k ]. Since ex-
act calculation is intractable we will approximate p(Z j D; (cid:10)) with q(Z ) belonging to
the exponential family such that certain distance measure (can be asymmetric) between
p(Z jD; (cid:10)) and q(Z )) is minimized. In our case we apply the variational Bayes method
which applies KL (q(Z )jjp(D; Z j (cid:10))) as the distance measure. The central idea is to
lower bound the log-likelihood using Jensen’s inequality: log p(D) = log R p(D; Z )dZ (cid:21)
R q(Z ) log p(D ;Z )
q(Z ) dZ . The RHS of the above equation is what we want to maximize, and
it is straightforward to show that maximizing this lower bound is equivalent to minimize
the KL-divergence KL(q(Z )jjp(Z jD)). Since given (cid:10) the K tasks are decoupled, we can
conduct inference for each task respectively. We further assume q((cid:18)k ; sk ) = q((cid:18)k )q(sk ),
which in general is a reasonable simplifying assumption and allows us to do the optimiza-
tion iteratively. The details for the E-step are shown in Algorithm 1.

We would like to comment on several things in Algorithm 1. First, we assume the form of
q((cid:18)) to be multivariate Gaussian, which is a reasonable choice especially considering the
fact that only the ﬁrst
and second moments are needed in the M-step. Second, the prior
choice of p(s) in step 3 is signiﬁcant
since for each s we only have one associated “data
point” (cid:18). In particular using the Laplace distribution will lead to a more sparse solution of
E[s], and this will be made more clear in Section 3.2. Finally, we take the parametric form
of q(s) to be the product of Laplace distributions with unit variance but known mean, where
the ﬁx ed variance is intended to remove the unidentiﬁability issue caused by the interaction
between scales of s and (cid:3). Although using a full covariance Gaussian for q(s) is another
choice, again due to unidentiﬁability reason caused by rotations of s and (cid:3) we could make
it a diagonal Gaussian. As a result, we argue that the product of Laplaces is better than the
product of Gaussians since it has the same parametric form as the prior p(s).

3.1.1 Variational Method for Bayesian Logistic Regression

We present an efﬁcient algorithm based on the variational method proposed in[7] to solve
step 2 in Algorithm 1, which is guaranteed to converge and known to be efﬁcient
for this
problem. Given a Gaussian prior N (m0 ; V0 ) over the parameter (cid:18) and a training set 2
D = f(x1 ; y1); : : : ; (xN ; yN )g, we want to obtain an approximation N (m; V) to the true
posterior distribution p((cid:18)jD). Taking one data point (x; y) as an example, the basic idea
is to use an exponential function to approximate the non-exponential likelihood function
p(y jx; (cid:18)) = (1 + exp((cid:0)y(cid:18)T x))(cid:0)1 which in turn makes the Bayes formula tractable.

2Again we omit the task index k and use y 2 f(cid:0)1; 1g instead of y 2 f0; 1g to simplify notation.

By using the inequality p(y jx; (cid:18)) (cid:21) g((cid:24) ) exp (cid:8)(yxT (cid:18) (cid:0) (cid:24) )=2 (cid:0) (cid:21)((cid:24) )((xT (cid:18))2 (cid:0) (cid:24) 2 )(cid:9) 4
=
p(y jx; (cid:18); (cid:24) ) where g(z ) = 1=(1 + exp((cid:0)z )) is the logistic function and (cid:21)((cid:24) ) =
tanh((cid:24)=2)=4(cid:24) , we can maximize the lower bound of p(y jx) = R p((cid:18))p(y jx; (cid:18))d(cid:18) (cid:21)
R p((cid:18))p(y jx; (cid:18); (cid:24) )d(cid:18). An EM algorithm can be formulated by treating (cid:24) as the parame-
ter and (cid:18) as the hidden variable:
(cid:15) E-step: Q((cid:24) ; (cid:24) t ) = E [log fp((cid:18))p(y jx; (cid:18); (cid:24) )g j x; y ; (cid:24) t ]
(cid:15) M-step: (cid:24) t+1 = arg max(cid:24) Q((cid:24) ; (cid:24) t )

Vpost = V (cid:0)

Due to the Gaussianity assumption the E-step can be thought as updating the sufﬁcient
statistics (mean and covariance) of q((cid:18)). Finally by using the Woodbury formula the EM
iterations can be unraveled and we get the efﬁcient one-shot E-step updating without in-
volving matrix inversion (due to space limitation we skip the derivation):
2(cid:21)((cid:24) )
Vx(Vx)T
1 + 2(cid:21)((cid:24) )c
2(cid:21)((cid:24) )
y
y
2(cid:21)((cid:24) )
2
1 + 2(cid:21)((cid:24) )c
2
1 + 2(cid:21)((cid:24) )c
where c = xT Vx, and (cid:24) is calculated ﬁrst
from the M-step which is reduced to ﬁnd the
ﬁx ed point of the following one-dimensional problem and can be solved efﬁciently:
c2(cid:19)2
c2 + (cid:18)xT m (cid:0)
2(cid:21)((cid:24) )
2(cid:21)((cid:24) )
y
y
2(cid:21)((cid:24) )
(cid:24) 2 = c (cid:0)
cxT m +
c (cid:0)
1 + 2(cid:21)((cid:24) )c
1 + 2(cid:21)((cid:24) )c
2
2
1 + 2(cid:21)((cid:24) )c
And this process will be performed for each data point to get the ﬁnal approximation q((cid:18)).

mpost = m (cid:0)

VxxT m +

Vx (cid:0)

cVx

3.2 Point Estimation

Although the empirical Bayes method is efﬁcient
for medium-sized problem, both its com-
putational cost and memory requirement grow as the number of data instances or features
increases. For example, it can easily happen in text or image domain where the number
of features can be more than ten thousand, so we need faster methods. We can obtain the
k=1 , by treating it as a limiting case of the previous algorithm.
point estimation of f(cid:18)k ; sk gK
To be more speciﬁc, by letting q((cid:18)) and q(s) converging to the Dirac delta function, step
2 in Algorithm 1 can thought as ﬁnding the MAP estimation of (cid:18) and step 4 becomes the
following lasso-like optimization problem (ms denotes the point estimation of s):
s (cid:3)T (cid:9)(cid:0)1(cid:3)ms (cid:0) 2mT
s (cid:3)T (cid:9)(cid:0)1E[(cid:18)](cid:9)
ms (cid:8)2jjms jj1 + mT
^ms = arg min
which can be solved numerically. Furthermore, the solution of the above optimization
is sparse in ms . This is a particularly nice property since we would only like to consider
hidden sources for which the association with tasks are signi ﬁcantly supported by evidence.

4 Experimental Results

The LICA model will work most effectively if the tasks we want to learn are very related.
In our experiments we apply the LICA model to multi-label text classiﬁcation problems,
which are the case for many existing text collections including the most popular ones like
Reuters-21578 and the new RCV1 corpus. Here each individual task is to classify a given
document to a particular category, and it is assumed that the multi-label property implies
that some of the tasks are related through some latent sources (semantic topics).

For Reuters-21578 we choose nine categories out of ninety categories, which is based on
fact that those categories are often correlated by previous studies[8]. After some pre-
processing3 we get 3,358 unique features/words, and empirical Bayes method is used to

3We do stemming, remove stopwords and rare words (words that occur less than three times).

1
F
−
o
r
c
a
M

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

Individual
LICA

Individual
LICA

Reuters−21578

RCV1

0.6

0.5

1
F
o−
r
c
i
M

0.4

0.3

0.2

0.1

50

100

200

500

100

200

500

1000

Training Set Size
Training Set Size
Figure 2: Multi-label Text Classiﬁcation Results on Reuters-21578 and RCV1

solve this problem. On the other hand, if we include all the 116 TOPIC categories in RCV1
corpus we get a much larger vocabulary size: 47,236 unique features. Bayesian inference
is intractable for this high-dimensional case since memory requirement itself is O(F 2 ) to
store the full covariance matrix V[(cid:18)]. As a result we take the point estimation approach
which reduces the memory requirement to O(F ). For both data sets we use the standard
training/test split, but for RCV1 since the test part of corpus is huge (around 800k docu-
ments) we only randomly sample 10k as our test set. Since the effectiveness of learning
multiple related tasks jointly should be best demonstrated when we have limited resources,
we evaluate our LICA by varying the size of training set. Each setting is repeated ten times
and the results are summarized in Figure 2.

In Figure 2 the result “indi vidual ”
is obtained by using regularized logistic regression for
each category individually. The number of tasks K is equal to 9 and 116 for the Reuters-
21578 and the RCV1 respectively, and we set H (the dimension of hidden source) to be
the same as K in our experiments. We use F1 measure which is preferred to error rate
in text classiﬁcation due to the very unbalanced positive/negative document ratio. For the
Reuters-21578 collection we report the Macro-F1 results because this corpus is easier and
thus Micro-F1 are almost the same for both methods. For the RCV1 collection we only
report Micro-F1 due to space limitation, and in fact we observed similar trend in Macro-F1
although values are much lower due to the large number of rare categories. Furthermore,
we achieved a sparse solution for the point estimation method. In particular, we obtained
less than 5 non-zero sources out of 116 for most of the tasks for the RCV1 collection.

5 Discussions on Related Work

By viewing multitask learning as predicting multivariate responses, Breiman and Fried-
man[3] proposed a method called “Curds and Whey” for regression problems. The intuition
is to apply shrinkage in a rotated basis instead of the original task basis so that information
can be borrowed among tasks.

By treating tasks as IID generated from some probability space, empirical process the-
ory[2] has been applied to study the bounds and asymptotics of multiple task learning,
similar to the case of standard learning. On the other hand, from the general Bayesian per-
spective[2][6] we could treat the problem of learning multiple tasks as learning a Bayesian
prior over the task space. Despite the generality of above two principles, it is often nec-
essary to assume some speciﬁc
structure or parametric form of the task space since the
functional space is usually of higher or in ﬁnite dimension compared to the input space.

Our model is related to the recently proposed Semiparametric Latent Factor Model (SLFM)
for regression by Teh et. al.[10]. It uses Gaussian Processes (GP) to model regression
through a latent factor analysis. Besides the difference between FA and ICA, its advantage

is that GP is non-parametric and works on the instance space; the disadvantage of that
model is that training instances need to be shared for all tasks. Furthermore, it is not clear
how to explore different task structures in this instance-space viewpoint. As pointed out
earlier, the exploration of different source models is important in learning related tasks as
the prior often plays a more important role than it does in standard learning.

6 Conclusion and Future Work

In this paper we proposed a probabilistic framework for learning multiple related tasks,
which tries to identify the shared latent independent components that are responsible for
the relatedness among those tasks. We also presented the corresponding empirical Bayes
method as well as point estimation algorithms for learning the model. Using non-Gaussian
distributions for hidden sources makes it possible to identify independent components in-
stead of just decorrelation, and in particular we enjoyed the sparsity by modeling hidden
sources with Laplace distribution. Having the sparsity property makes the model not only
parsimonious but also more robust since the dependence on latent, independent sources will
be shrunk toward zero unless signi ﬁcantly supported by evidence from the data. By learn-
ing those related tasks jointly, we are able to get a better estimation of the latent independent
sources and thus achieve a better generalization capability compared to conventional ap-
proaches where the learning of each task is done independently. Our experimental results
in multi-label text classiﬁcation problems show evidence to support our claim.

Our approach assumes that the underlying structure in the task space is a linear subspace,
which can usually capture important information about independent sources. However, it is
possible to achieve better results if we can incorporate speciﬁc domain knowledge about the
relatedness of those tasks into the model and obtain a reliable estimation of the structure.
For future research, we would like to consider more ﬂe xible source models as well as
incorporate domain speciﬁc knowledge to specify and learn the underlying structure.

References

[1] Ando, R. and Zhang, T. A Framework for Learning Predicative Structures from Multiple Tasks
and Unlabeled Data. Technical Rerport RC23462, IBM T.J. Watson Research Center, 2004.

[2] Baxter, J. A Model for Inductive Bias Learning. J. of Artiﬁcial

Intelligence Research, 2000.

[3] Breiman, L. and Friedman J. Predicting Multivariate Responses in Multiple Linear Regression. J.
Royal Stat. Society B, 59:3-37, 1997.

[4] Evgeniou, T., Micchelli, C. and Pontil, M. Learning Multiple Tasks with Kernel Methods. J. of
Machine Learning Research, 6:615-637, 2005.

[5] Ghosn, J. and Bengio, Y. Bias Learning, Knowledge Sharing. IEEE Transaction on Neural Net-
works, 14(4):748-765, 2003.

[6] Heskes, T. Empirical Bayes for Learning to Learn. In Proc. of the 17th ICML, 2000.

[7] Jaakkola, T. and Jordan, M. A Variational Approach to Bayesian Logistic Regression Models and
Their Extensions. In Proc. of the Sixth Int. Workshop on AI and Statistics, 1997.

[8] Koller, D. and Sahami, M. Hierarchically Classifying Documents using Very Few Words. In Proc.
of the 14th ICML, 1997.

[9] Roberts, S. and Everson, R. (editors). Independent Component Analysis: Principles and Practice,
Cambridge University Press, 2001.

[10] Teh, Y.-W., Seeger, M. and Jordan, M. Semiparametric Latent Factor Models. In Z. Ghahramani
Intelligence and Statistics 10, 2005.
and R. Cowell, editors, Workshop on Artiﬁcial

[11] Yu, K., Tresp, V. and Schwaighofer, A. Learning Gaussian Processes from Multiple Tasks. In
Proc. of the 22nd ICML, 2005.


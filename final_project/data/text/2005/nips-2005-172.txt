Predicting EMG Data from M1 Neurons
with Variational Bayesian Least Squares

Jo-Anne Ting1 , Aaron D’Souza1
Kenji Yamamoto3 , Toshinori Yoshioka2 , Donna Hoﬀman3
Shinji Kakei4 , Lauren Sergio6 , John Kalaska5
Mitsuo Kawato2 , Peter Strick3 , Stefan Schaal1,2

1Comp. Science & Neuroscience, U.of S. California, Los Angeles, CA 90089, USA
2ATR Computational Neuroscience Laboratories, Kyoto 619-0288, Japan
3University of Pittsburgh, Pittsburgh, PA 15261, USA
4Tokyo Metropolitan Institute for Neuroscience, Tokyo 183-8526, Japan
5University of Montreal, Montreal, Canada H3C-3J7
6York University, Toronto, Ontario, Canada M3J1P3

Abstract

An increasing number of pro jects in neuroscience requires the sta-
tistical analysis of high dimensional data sets, as, for instance, in
predicting behavior from neural ﬁring or in operating artiﬁcial de-
vices from brain recordings in brain-machine interfaces. Linear
analysis techniques remain prevalent in such cases, but classical
linear regression approaches are often numerically too fragile in
high dimensions. In this paper, we address the question of whether
EMG data collected from arm movements of monkeys can be faith-
fully reconstructed with linear approaches from neural activity in
primary motor cortex (M1). To achieve robust data analysis, we
develop a full Bayesian approach to linear regression that auto-
matically detects and excludes irrelevant features in the data, reg-
ularizing against overﬁtting.
In comparison with ordinary least
squares, stepwise regression, partial least squares, LASSO regres-
sion and a brute force combinatorial search for the most predictive
input features in the data, we demonstrate that the new Bayesian
method oﬀers a superior mixture of characteristics in terms of reg-
ularization against overﬁtting, computational eﬃciency and ease of
use, demonstrating its potential as a drop-in replacement for other
linear regression techniques. As neuroscientiﬁc results, our anal-
yses demonstrate that EMG data can be well predicted from M1
neurons, further opening the path for possible real-time interfaces
between brains and machines.

1 Introduction

In recent years, there has been growing interest in large scale analyses of brain ac-
tivity with respect to associated behavioral variables. For instance, pro jects can be
found in the area of brain-machine interfaces, where neural ﬁring is directly used
to control an artiﬁcial system like a robot [1, 2], to control a cursor on a computer
screen via non-invasive brain signals [3] or to classify visual stimuli presented to

a sub ject [4, 5]. In these pro jects, the brain signals to be processed are typically
high dimensional, on the order of hundreds or thousands of inputs, with large num-
bers of redundant and irrelevant signals. Linear modeling techniques like linear
regression are among the primary analysis tools [6, 7] for such data. However, the
computational problem of data analysis involves not only data ﬁtting, but requires
that the model extracted from the data has good generalization properties. This is
crucial for predicting behavior from future neural recordings, e.g., for continual on-
line interpretation of brain activity to control prosthetic devices or for longitudinal
scientiﬁc studies of information processing in the brain. Surprisingly, robust linear
modeling of high dimensional data is non-trivial as the danger of ﬁtting noise and
encountering numerical problems is high. Classical techniques like ridge regression,
stepwise regression or partial least squares regression are known to be prone to
overﬁtting and require careful human supervision to ensure useful results.
In this paper, we will focus on how to improve linear data analysis for the high di-
mensional scenarios described above, with a view towards developing a statistically
robust “black box” approach that automatically detects the most relevant input
dimensions for generalization and excludes other dimensions in a statistically sound
way. For this purpose, we investigate a full Bayesian treatment of linear regres-
sion with automatic relevance detection [8]. Such an algorithm, called Variational
Bayesian Least Squares (VBLS), can be formulated in closed form with the help of
a variational Bayesian approximation and turns out to be computationally highly
eﬃcient. We apply VBLS to the reconstruction of EMG data from motor cortical
ﬁring, using data sets collected by [9] and [10, 11]. This data analysis addresses
important neuroscientiﬁc questions in terms of whether M1 neurons can directly
predict EMG traces [12], whether M1 has a muscle-based topological organization
and whether information in M1 should be used to predict behavior in future brain-
machine interfaces. Our main focus in this paper, however, will be on the robust
statistical analysis of these kinds of data. Comparisons with classical linear analy-
sis techniques and a brute force combinatorial model search on a cluster computer
demonstrate that our VBLS algorithm achieves the “black box” quality of a robust
statistical analysis technique without any tunable parameters.
In the following sections, we will ﬁrst sketch the derivation of Variational Bayesian
Least Squares and subsequently perform extensive comparative data analysis of this
technique in the context of prediction EMG data from M1 neural ﬁring.

2 High Dimensional Regression

Before developing our VBLS algorithm, let us brieﬂy revisit classical linear regres-
sion techniques. The standard model for linear regression is:
dX

y =

bmxm + 

(1)

m=1
where b is the regression vector composed of bm components, d is the number of
input dimensions,  is additive mean-zero noise, x are the inputs and y are the
outputs. The Ordinary Least Squares (OLS) estimate of the regression vector is
(cid:2)−1 XT y. The main problem with OLS regression in high dimensional
(cid:1)
XT X
b =
(cid:1)
(cid:2)−1 is often violated due to
XT X
input spaces is that the full rank assumption of
underconstrained data sets. Ridge regression can “ﬁx” such problems numerically,
but introduces uncontrolled bias. Additionally, if the input dimensionality exceeds
around 1000 dimensions, the matrix inversion can become prohibitively computa-
tionally expensive.
Several ideas exist how to improve over OLS. First, stepwise regression [13] can
be employed. However, it has been strongly criticized for its potential for overﬁt-
ting and its inconsistency in the presence of collinearity in the input data [14]. To

xi1

b1

yi

xi1

zi1

yi

xid

zid

xid

i=1..N
(a) Linear regression

bd
i=1..N
(b) Probabilistic backﬁtting

α1

αd

b1

bd

xi1

xid

zi1

zid

(c) VBLS

yi

i=1..N

Figure 1: Graphical Models for Linear Regression. Random variables are in circular
nodes, observed random variables are in double circles and point estimated parameters are
in square nodes.

deal with such collinearity directly, dimensionality reduction techniques like Prin-
cipal Components Regression (PCR) and Factor Regression (FR) [15] are useful.
These methods retain components in input space with large variance, regardless of
whether these components inﬂuence the prediction [16], and can even eliminate low
variance inputs that may have high predictive power for the outputs [17]. Another
class of linear regression methods are pro jection regression techniques, most notably
Partial Least Squares Regression (PLS) [18]. PLS performs computationally inex-
pensive O(d) univariate regressions along pro jection directions, chosen according to
the correlation between inputs and outputs. While slightly heuristic in nature, PLS
is a surprisingly successful algorithm for ill-conditioned and high-dimensional re-
gression problems, although it also has a tendency towards overﬁtting [16]. LASSO
(Least Absolute Shrinkage and Selection Operator) regression [19] shrinks certain
regression coeﬃcients to 0, giving interpretable models that are sparse. However, a
tuning parameter needs to be set, which can be done using n-fold cross-validation
or manual hand-tuning. Finally, there are also more eﬃcient methods for matrix
inversion [20, 21], which, however, assume a well-condition regression problem a
priori and degrade in the presence of collinearities in inputs.
In the following section, we develop a linear regression algorithm in a Bayesian
framework that automatically regularizes against problems of overﬁtting. Moreover,
the iterative nature of the algorithm, due to its formulation as an Expectation-
Maximization problem [22], avoids the computational cost and numerical problems
of matrix inversions. Thus, it addresses the two ma jor problems of high-dimensional
OLS simultaneously. Conceptually, the algorithm can be interpreted as a Bayesian
version of either backﬁtting or partial least squares regression.

3 Variational Bayesian Least Squares

Figure 1 illustrates the progression of graphical models that we need in order to
develop a robust Bayesian version of linear regression. Figure 1a depicts the stan-
dard linear regression model. In the spirit of PLS, if we knew an optimal pro jection
direction of the input data, then the entire regression problem could be solved by
a univariate regression between the pro jected data and the outputs. This optimal
pro jection direction is simply the true gradient between inputs and outputs. In the
tradition of EM algorithms [22], we encode this pro jection direction as a hidden
variable, as shown in Figure 1b. The unobservable variables zim (where i = 1..N
denotes the index into the data set of N data points) are the results of each input
being multiplied with its corresponding component of the pro jection vector (i.e.
bm ). Then, the zim are summed up to form a predicted output yi .
More formally, the linear regression model in Eq. (1) is modiﬁed to become:
dX

zim = bmxim

yi =

zim + 

m=1

For a probabilistic treatment with EM, we make a standard normal assumption of
”
“
all distributions in form of:
zim |xi ∼ Normal (zim ; bmxim , ψzm )
yi |zi ∼ Normal
yi ; 1T zi , ψy
where 1 = [1, 1, .., 1]T . While this model is still identical to OLS, notice that in the
graphical model, the regression coeﬃcients bm are behind the fan-in to the outputs
yi . Given the data D = {xi , yi }N
i=1 , we can view this new regression model as an
EM problem and maximize the incomplete log likelihood log p(y|X) by maximizing
the expected complete log likelihood (cid:1)log p(y, Z|X)(cid:2):
´2 − N
`
Pd
PN
log p(y, Z|X) = − N
2 log ψy − 1
yi − 1T zi
− Pd
m=1 log ψzm
i=1
2ψy
2
2ψzm (zim − bmxim )
2
1
+ const
m=1
where Z denotes the N by d matrix of all zim . The resulting EM updates require
standard manipulations of normal distributions and result in:
“Pd
” h
“Pd
E-step :
M-step :
1 − 1
PN
i=1 (cid:1)zim (cid:2)xim
1T Σz 1 =
´
`
`
yi − 1T (cid:2)zi (cid:3)´
PN
m=1 ψzm
m=1 ψzm
PN
i=1 x2
s
im
1 − 1
´
`
PN
σ2
2 + 1T Σz 1
ψy = 1
ψzm
zm = ψzm
i=1
s
N
i=1 ((cid:2)zim (cid:3) − bmxim )
yi − bT xi
(cid:2)zim (cid:3) = bmxi + 1
2
+ σ2
ψzm = 1
Pd
ψxm
zm
s
N
m=1 ψxm and Σz = Cov(z|y, X). It is very important to
where we deﬁne s = ψy +
note that one EM update has a computationally complexity of O(d), where d is the
number of input dimensions, instead of the O(d3 ) associated with OLS regression.
This eﬃciency comes at the cost of an iterative solution, instead of a one-shot
solution for b as in OLS. It can be proved that this EM version of least squares
regression is guaranteed to converge to the same solution as OLS [23].
This new EM algorithm appears to only replace the matrix inversion in OLS by an
iterative method, as others have done with alternative algorithms [20, 21], although
the convergence guarantees of EM are an improvement over previous approaches.
The true power of this probabilistic formulation, though, becomes apparent when we
add a Bayesian layer that achieves the desired robustness in face of ill-conditioned
data.

bm =

(2)

”i

3.1 Automatic Relevance Determination

From a Bayesian point of view, the parameters bm should be treated probabilistically
so that we can integrate them out to safeguard against overﬁtting. For this purpose,
as shown in Figure 1c, we introduce precision variables αm over each regression
´ 1
`
¯
˘− αm
Qd
parameter bm :
p(b|α) =
Qd
b2
αm
2 exp
m
m=1
2π
2
exp {−bααm }
α(aα−1)
baα
p(α) =
α
m
m=1
Gamma(aα )
where α is the vector of all αm . In order to obtain a tractable posterior distribution
over all hidden variables b, zim and α, we use a factorial variational approximation
of the true posterior Q(α, b, Z) = Q(α, b)Q(Z). Note that the connection from
the αm to the corresponding zim in Figure 1c is an intentional design. Under this
graphical model, the marginal distribution of bm becomes a Student t-distribution
that allows traditional hypothesis testing [24]. The minimal factorization of the
posterior into Q(α, b)Q(Z) would not be possible without this special design.
The resulting augmented model has the following distributions:
bm |αm ∼ N (wbm ; 0, 1/αm )
yi |zi ∼ N (yi ; 1T zi , ψy )
zim |bm , αmxim ∼ N (zim ; bmxim , ψzm /αm )
αm ∼ Gamma(αm ; aα , bα )

(3)

”

ˆaα = aα +

We now have a mechanism that infers the signiﬁcance of each dimension’s contribu-
tion to the observed output y . Since bm is zero mean, a very large αm (equivalent
to a very small variance of bm ) suggests that bm is very close to 0 and has no contri-
bution to the output. An EM-like algorithm [25] can be used to ﬁnd the posterior
updates of all distributions. We omit the EM update equations due to space con-
straints as they are similar to the EM update above and only focus on the posterior
”−1
“PN
update for bm and α:
σ2
bm |αm = ψzm
”−1 “PN
“PN
i=1 x2
im + ψzm
αm
(cid:2)bm |αm (cid:3) =
i=1 (cid:2)zim (cid:3) xim
i=1 x2
im + ψzm
N
jPN
”−1 “PN
“PN
˙
¸ −
2
i=1 (cid:2)zim (cid:3) xim
ˆb(m)
i=1 x2
z 2
α = bα + 1
im + ψzm
im
i=1
2ψzm
Note that the update equation for (cid:1)bm |αm (cid:2) can be rewritten as:
”
“ PN
PN
i=1 (yi−(cid:1)b|α (cid:2)(n)T xi )xim
(cid:2)bm |αm (cid:3)(n)
(cid:2)bm |αm (cid:3)(n+1)
i=1 x2
+ ψzm
im
PN
PN
i=1 x2
i=1 x2
sαm
im+ψzm
im+ψzm
Eq.
(5) demonstrates that in the absence of a correlation between the current
input dimension and the residual error, the ﬁrst term causes the current regression
coeﬃcient to decay. The resulting regression solution regularizes over the number of
retained inputs in the ﬁnal regression vector, performing a functionality similar to
Automatic Relevance Determination (ARD) [8]. The update equations’ algorithmic
complexity remains O(d). One can further show that the marginal distribution of all
bm is a t-distribution with t = (cid:1)bm |αm (cid:2) /σbm |αm and 2ˆaα degrees of freedom, which
allows a principled way of determining whether a regression coeﬃcient was excluded
by means of standard hypothesis testing. Thus, Variational Bayesian Least Squares
(VBLS) regression is a full Bayesian treatment of the linear regression problem.

(5)

(4)

ﬀ

”2

=

4 Evaluation

We now turn to the application and evaluation of VBLS in the context of predict-
ing EMG data from neural data recorded in M1 of monkeys. The key questions
addressed in this application were i) whether EMG data can be reconstructed ac-
curately with good generalization, ii) how many neurons contribute to the recon-
struction of each muscle and iii) how well the VBLS algorithm compares to other
analysis techniques. The underlying assumption of this analysis is that the rela-
tionship between neural ﬁring and muscle activity is approximately linear.

4.1 Data sets

We investigated data from two diﬀerent experiments. In the ﬁrst experiment by
Sergio & Kalaska [9], the monkey moved a manipulandum in a center-out task in
eight diﬀerent directions, equally spaced in a horizontal planar circle of 8cm radius.
A variation of this experiment held the manipulandum rigidly in place, while the
monkey applied isometric forces in the same eight directions. In both conditions,
movement or force, feedback was given through visual display on a monitor. Neural
activity for 71 M1 neurons was recorded in all conditions (2400 data points for each
neuron), along with the EMG outputs of 11 muscles.
The second experiment by Kakei et al. [10] involved a monkey trained to perform
eight diﬀerent combinations of wrist ﬂexion-extension and radial-ulnar movements
while in three diﬀerent arm postures (pronated, supinated and midway between the
two). The data set consisted of neural data of 92 M1 neurons that were recorded

OLS
STEP
PLS
LASSO
VBLS
ModelSearch

3

2.5

2

E
S
M
n

1.5

1

0.5

0

nMSE Train
nMSE Test 
(a) Sergio & Kalaska [9] data

3

2.5

2

E
S
M
n

1.5

1

0.5

0

OLS
STEP
PLS
LASSO
VBLS
ModelSearch

nMSE Train
nMSE Test 
(b) Kakei et al. [10] data

Figure 2: Normalized mean squared error for Cross-validation Sets (6-fold for [10] and
8-fold for [9])

Sergio & Kalaska data set
Kakei et al. data set

STEP LASSO
VBLS PLS
93.6% 7.44% 8.71%
8.42%
76.3%
87.1% 40.1% 72.3%

Table 1: Percentage neuron matches between baseline and all other algorithms, averaged
over all muscles in the data set
at all three wrist postures (producing 2664 data points for each neuron) and the
EMG outputs of 7 contributing muscles. In all experiments, the neural data was
represented as average ﬁring rates and was time aligned with EMG data based on
analyses that are outside of the scope of this paper.
4.2 Methods

For the Sergio & Kalaska data set, a baseline comparison of good EMG reconstruction
was obtained through a limited combinatorial search over possible regression models. A
particular model is characterized by a subset of neurons that is used to predict the EMG
data. Given 71 neurons, theoretically 271 possible models exist. This value is too large
for an exhaustive search. Therefore, we considered only possible combinations of up to 20
neurons, which required several weeks of computation on a 30-node cluster computer. The
optimal predictive subset of neurons was determined from an 8-fold cross validation. This
baseline study served as a comparison for PLS, stepwise regression, LASSO regression,
OLS and VBLS. The ﬁve other algorithms used the same validation sets employed in the
baseline study. The number of PLS pro jections for each data ﬁt was found by leave-one-
out cross-validation. Stepwise regression used Matlab’s “stepwiseﬁt” function. LASSO
regression was implemented, manually choosing the optimal tuning parameter over all
cross-validation sets. OLS was implemented using a small ridge regression parameter of
−10 in order to avoid ill-conditioned matrix inversions.
10

d
n
u
o
F
 
s
n
o
r
u
e
N
 
f
o
 
#
 
e
v
A

90

80

70

60

50

40

30

20

10

0

STEP

PLS

LASSO

VBLS

ModelSearch

5

4

3

1

2

6
Muscle
(a) Sergio & Kalaska [9] data

10

7

8

9

d
n
u
o
F
 
s
n
o
r
u
e
N
 
f
o
 
#
 
e
v
A

90

80

70

60

50

40

30

20

10

0

11

STEP

PLS

LASSO

VBLS

ModelSearch

3

1

2

4
Muscle
(b) Kakei et al. [10] data

5

6

7

Figure 3: Average Number of Relevant Neurons found over Cross-validation Sets (6-fold
for [10] and 8-fold for [9])

The average number of relevant neurons was calculated over all 8 cross-validation sets
and a ﬁnal set of relevant neurons was reached for each algorithm by taking the common
neurons found to be relevant over the 8 cross-validation sets. Inference of relevant neurons
in PLS was based on the subspace spanned by the PLS pro jections, while relevant neurons
in VBLS were inferred from t-tests on the regression parameters, using a signiﬁcance of
p < 0.05. Stepwise regression and LASSO regression determined the number of relevant
neurons from the inputs that were included in the ﬁnal model. Note that since OLS
retained all input dimensions, this algorithm was omitted in relevant neuron comparisons.

Analogous to the ﬁrst data set, a combinatorial analysis was performed on the Kakei et al.
data set in order to determine the optimal set of neurons contributing to each muscle (i.e.
producing the lowest possible prediction error) in a 6-fold cross-validation. PLS, stepwise
regression, LASSO regression, OLS and VBLS were applied using the same cross-validation
sets, employing the same procedure described for the ﬁrst data set.

4.3 Results

Figure 2 shows that, in general, EMG traces seem to be well predictable from M1 neural
ﬁring. VBLS resulted in a generalization error comparable to that produced by the base-
line study. In the Kakei et al. dataset, all algorithms performed similarly, with LASSO
regression performing a little better than the rest. However, OLS, stepwise regression,
LASSO regression and PLS performed far worse on the Sergio & Kalaska dataset, with
OLS regression attaining the worst error. Such performance is typical for traditional linear
regression methods on ill-conditioned high dimensional data, motivating the development
of VBLS. The average number of relevant neurons found by VBLS was slightly higher
than the baseline study, as seen in Figure 3. This result is not surprising as the baseline
study did not consider all possible combination of neurons. Given the good generalization
results of VBLS, it seems that the Bayesian approach regularized the participating neu-
rons suﬃciently so that no overﬁtting occurred. Note that the results for muscle 6 and 7
in Figure 3b seem to be due to some irregularities in the data and should be considered
outliers. Table 1 demonstrates that the relevant neurons identiﬁed by VBLS coincided at
a very high percentage with those of the baseline results, while PLS, stepwise regression
and LASSO regression had inferior outcomes.

Thus, in general, VBLS achieved comparable performance with the baseline study when
reconstructing EMG data from M1 neurons. While VBLS is an iterative statistical method,
which performs slower than classical “one-shot” linear least squares methods (i.e., on the
order of several minutes for the data sets in our analyses), it achieved comparable results
with our combinatorial model search, which took weeks on a cluster computer.

5 Discussion

This paper addressed the problem of analyzing high dimensional data with linear regression
techniques, as encountered in neuroscience and the new ﬁeld of brain-machine interfaces.
To achieve robust statistical results, we introduced a novel Bayesian technique for linear
regression analysis with automatic feature detection, called Variational Bayesian Least
Squares. Comparisons with classical linear regression methods and a “gold standard”
obtained from a brute force search over all possible linear models demonstrate that VBLS
performs very well without any manual parameter tuning, such that it has the quality of
a “black box” statistical analysis technique.

A point of concern against the VBLS algorithm is how the variational approximation in
this algorithm aﬀects the quality of function approximation. It is known that factorial
approximations to a joint distribution create more peaked distributions, such that one
could potentially assume that VBLS might tend to overﬁt. However, in the case of VBLS,
a more peaked distribution over bm pushes the regression parameter closer to zero. Thus,
VBLS will be on the slightly pessimistic side of function ﬁtting and is unlikely to overﬁt.
Future evaluations and comparisons with Markov Chain Monte Carlo methods will reveal
more details of the nature of the variational approximation. Regardless, it appears that
VBLS could become a useful drop-in replacement for various classical regression methods.
It lends itself to incremental implementation as would be needed in real-time analyses of
brain information.

Acknowledgments

This research was supported in part by National Science Foundation grants ECS-0325383, IIS-0312802,
IIS-0082995, ECS-0326095, ANI-0224419, a NASA grant AC#98 − 516, an AFOSR grant on Intelligent
Control, the ERATO Kawato Dynamic Brain Pro ject funded by the Japanese Science and Technology
Agency, the ATR Computational Neuroscience Laboratories and by funds from the Veterans Adminis-
tration Medical Research Service.

References

[1] M.A. Nicolelis. Actions from thoughts. Nature, 409:403–407, 2001.

[2] D.M. Taylor, S.I. Tillery, and A.B. Schwartz. Direct cortical control of 3d neuroprosthetic devices.
Science, 296:1829–1932, 2002.

[3] J.R. Wolpaw and D.J. McFarland. Control of a two-dimensional movement signal by a noninvasive
brain-computer interface in humans. Proceedings of the National Academy of Sciences, 101:17849–
17854, 2004.

[4] Y. Kamitani and F. Tong. Decoding the visual and sub jective contents of the human brain. Nature
Neuroscience, 8:679, 2004.

[5] J.D. Haynes and G. Rees. Predicting the orientation of invisible stimuli from activity in human
primary visual cortex. Nature Neuroscience, 8:686, 2005.

[6] J. Wessberg and M.A. Nicolelis. Optimizing a linear algorithm for real-time robotic control using
chronic cortical ensemble recordings in monkeys. Journal of Cognitive Neuroscience, 16:1022–1035,
2004.

[7] S. Musallam, B.D. Corneil, B. Greger, H. Scherberger, and R.A. Andersen. Cognitive control signals
for neural prosthetics. Science, 305:258–262, 2004.

[8] R.M. Neal. Bayesian learning for neural networks. PhD thesis, Dept. of Computer Science,
University of Toronto, 1994.

[9] L.E. Sergio and J.F. Kalaska. Changes in the temporal pattern of primary motor cortex activity in a
directional isometric force versus limb movement task. Journal of Neurophysiology, 80:1577–1583,
1998.

[10] S. Kakei, D.S. Hoﬀman, and P.L. Strick. Muscle and movement representations in the primary
motor cortex. Science, 285:2136–2139, 1999.

[11] S. Kakei, D.S. Hoﬀman, and P.L. Strick. Direction of action is represented in the ventral premotor
cortex. Nature Neuroscience, 4:1020–1025, 2001.

[12] E. Todorov. Direct cortical control of muscle activation in voluntary arm movements: a model.
Nature Neuroscience, 3:391–398, 2000.

[13] N. R. Draper and H. Smith. Applied Regression Analysis. Wiley, 1981.

[14] S. Derksen and H.J. Keselman. Backward, forward and stepwise automated subset selection algo-
rithms: Frequency of obtaining authentic and noise variables. British Journal of Mathematical
and Statistical Psychology, 45:265–282, 1992.

[15] W.F. Massey. Principal component regression in exploratory statistical research. Journal of the
American Statistical Association, 60:234–246, 1965.

[16] S. Schaal, S. Vijayakumar, and C.G. Atkeson. Local dimensionality reduction. In M.I. Jordan, M.J.
Kearns, and S.A. Solla, editors, Advances in Neural Information Processing Systems. MIT Press,
1998.

[17] I.E. Frank and J.H. Friedman. A statistical view of some chemometric regression tools. Techno-
metrics, 35:109–135, 1993.

[18] H. Wold. Soft modeling by latent variables: The nonlinear iterative partial least squares approach.
In J. Gani, editor, Perspectives in probability and statistics, papers in honor of M. S. Bartlett.
Academic Press, 1975.

[19] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of Royal Statistical Society,
Series B, 58(1):267–288, 1996.

[20] V. Strassen. Gaussian elimination is not optimal. Num Mathematik, 13:354–356, 1969.

[21] T. J. Hastie and R. J. Tibshirani. Generalized additive models. Number 43 in Monographs on
Statistics and Applied Probability. Chapman and Hall, 1990.

[22] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of Royal Statistical Society. Series B, 39(1):1–38, 1977.

[23] A. D’Souza, S. Vijayakumar, and S. Schaal. The bayesian backﬁtting relevance vector machine. In
Proceedings of the 21st International Conference on Machine Learning. ACM Press, 2004.

[24] A. Gelman, J. Carlin, H.S. Stern, and D.B. Rubin. Bayesian Data Analaysis. Chapman and Hall,
2000.

[25] Z. Ghahramani and M.J. Beal. Graphical models and variational methods. In D. Saad and M. Opper,
editors, Advanced Mean Field Methods - Theory and Practice. MIT Press, 2000.


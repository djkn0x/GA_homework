Assessing Approximations for
Gaussian Process Classiﬁcation

Malte Kuss and Carl Edward Rasmussen
Max Planck Institute for Biological Cybernetics
Spemannstra ße 38, 72076 T ¨ubingen, Germany
{kuss,carl}@tuebingen.mpg.de

Abstract

Gaussian processes are attractive models for probabilistic classiﬁcation
but unfortunately exact inference is analytically intractable. We com-
pare Laplace’s method and Expectation Propagation (EP) focusing on
marginal likelihood estimates and predictive performance. We explain
theoretically and corroborate empirically that EP is superior to Laplace.
We also compare to a sophisticated MCMC scheme and show that EP is
surprisingly accurate.

In recent years models based on Gaussian process (GP) priors have attracted much atten-
tion in the machine learning community. Whereas inference in the GP regression model
with Gaussian noise can be done analytically, probabilistic classiﬁcation using GPs is an-
alytically intractable. Several approaches to approximate Bayesian inference have been
suggested, including Laplace’s approximation, Expectation Propagation (EP), variational
approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in con-
junction with generalisation bounds, online learning schemes and sparse approximations.

Despite the abundance of recent work on probabilistic GP classiﬁers, most experimental
studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when
and why which algorithm should be preferred. Thus, from a practitioners point of view
probabilistic GP classi ﬁcation remains a jungle. In this paper, we set out to understand and
compare two of the most wide-spread approximations: Laplace’s method and Expectation
Propagation (EP). We also compare to a sophisticated, but computationally demanding
MCMC scheme to examine how close the approximations are to ground truth.

We examine two aspects of the approximation schemes: Firstly the accuracy of approxi-
mations to the marginal likelihood which is of central importance for model selection and
model comparison. In any practical application of GPs in classiﬁcation (usually multiple)
parameters of the covariance function (hyperparameters) have to be handled. Bayesian
model selection provides a consistent framework for setting such parameters. Therefore, it
is essential to evaluate the accuracy of the marginal likelihood approximations as a function
of the hyperparameters, in order to assess the practical usefulness of the approach

Secondly, we need to assess the quality of the approximate probabilistic predictions. In the
past, the probabilistic nature of the GP predictions have not received much attention, the
focus being mostly on classiﬁcation error
rates. This unfortunate state of affairs is caused
primarily by typical benchmarking problems being considered outside of a realistic con-
text. The ability of a classiﬁer to produce class probabilities or conﬁdences, have obvious

relevance in most areas of application, eg. medical diagnosis. We evaluate the predictive
distributions of the approximate methods, and compare to the MCMC gold standard.

1 The Gaussian Process Model for Binary Classiﬁcation
Let y ∈ {−1, 1} denote the class label of an input x. Gaussian process classiﬁcation (GPC)
is discriminative in modelling p(y |x) for given x by a Bernoulli distribution. The proba-
bility of success p(y = 1|x) is related to an unconstrained latent function f (x) which is
mapped to the unit interval by a sigmoid transformation, eg. the logit or the probit. For rea-
sons of analytic convenience we exclusively use the probit model p(y = 1|x) = Φ(f (x)),
where Φ denotes the cumulative density function of the standard Normal distribution.
In the GPC model Bayesian inference is performed about the latent function f in the light
of observed data D = {(yi , xi )|i = 1, . . . , m}. Let fi = f (xi ) and f = [f1 , . . . , fm ]>
be shorthand for the values of the latent function and y = [y1 , . . . , ym ]> and X =
[x1 , . . . , xm ]> collect the class labels and inputs respectively. Given the latent function
mY
mY
the class labels are independent Bernoulli variables, so the joint likelihood factories:
p(y|f ) =
p(yi |fi ) =
i=1
i=1
and depends on f only through its value at the observed inputs. We use a zero-mean
Gaussian process prior over the latent function f with a covariance function k(x, x0 |θ),
which may depend on hyperparameters θ [1]. The functional form and parameters of
the covariance function encodes assumptions about the latent function, and adaptation of
these is part of the inference. The posterior distribution over latent function values f at the
Z
mY
observed X for given hyperparameters θ becomes:
N (f |0, K)
p(f |D , θ) =
p(D |θ)
i=1
denotes the marginal likelihood. Unfortunately neither the marginal likelihood, nor the
posterior itself, or predictions can be computed analytically, so approximations are needed.

p(y|f )p(f |X, θ)df ,

Φ(yi fi ),

Φ(yi fi ), where

p(D |θ) =

2 Approximate Bayesian Inference

For the GPC model approximations are either based on a Gaussian approximation to the
posterior p(f |D , θ) ≈ q(f |D , θ) = N (f |m, A) or involve Markov chain Monte Carlo
(MCMC) sampling [2]. We compare Laplace’s method and Expectation Propagation (EP)
which are two alternative approaches to ﬁnding parameters m and A of the Gaussian
q(f |D , θ). Both methods also allow approximate evaluation of the marginal likelihood,
which is useful for ML-II hyperparameter optimisation.

Laplace’s approximation (LA) is found by making a second order Taylor approximation of
the (un-normalised) log posterior [3]. The mean m is placed at the mode (MAP) and the
covariance A equals the negative inverse Hessian of the log posterior density at m.
The EP approximation [4] also gives a Gaussian approximation to the posterior. The pa-
rameters m and A are found in an iterative scheme by matching the approximate marginal
moments of p(fi |D , θ) by the marginals of the approximation N (fi |mi , Aii ). Although
we cannot prove the convergence of EP, we conjecture that it always converges for GPC
with probit likelihood, and have never encountered an exception.

A key insight is that a Gaussian approximation to the GPC posterior is equivalent to a GP
approximation to the posterior distribution over latent functions. For a test input x∗ the

(b)
(a)
Figure 1: Panel (a) provides a one-dimensional illustration of the approximations. The
prior N (f |0, 52 ) combined with the probit likelihood (y = 1) results in a skewed posterior.
The likelihood uses the right axis, all other curves use the left axis. Laplace’s approximation
peaks at the posterior mode, but places far too much mass over negative values of f and
too little at large positive values. The EP approximation matches the ﬁrst two posterior
moments, which results in a larger mean and a more accurate placement of probability mass
compared to Laplace’s approximation. In Panel (b) we caricature a high dimensional zero-
mean Gaussian prior as an ellipse. The gray shadow indicates that for a high dimensional
Gaussian most of the mass lies in a thin shell. For large latent signals (large entries in K),
the likelihood essentially cuts off regions which are incompatible with the training labels
(hatched area), leaving the upper right orthant as the posterior. The dot represents the mode
of the posterior, which remains close to the origin.
p
approximate predictive latent and class probabilities are:
q(f∗ |D , θ , x∗ ) = N (µ∗ , σ2∗ ),
q(y∗ = 1|D , x∗ ) = Φ(µ∗ /
1 + σ2∗ ),
and
∗ K−1m and σ2∗ = k(x∗ , x∗ )−k>
∗ (K−1 − K−1AK−1 )k∗ , where the vector
where µ∗ = k>
k∗ = [k(x1 , x∗ ), . . . , k(xm , x∗ )]> collects covariances between x∗ and training inputs X.
MCMC sampling has the advantage that it becomes exact in the limit of long runs and so
provides a gold standard by which to measure the two analytic methods described above.
Although MCMC methods can in principle be used to do inference over f and θ jointly [5],
we compare to methods using ML-II optimisation over θ , thus we use MCMC to integrate
over f only. Good marginal likelihood estimates are notoriously difﬁcult to obtain; in our
experiments we use Annealed Importance Sampling (AIS) [6], combining several Thermo-
dynamic Integration runs into a single (unbiased) estimate of the marginal likelihood.
Both analytic approximations have a computational complexity which is cubic O(m3 ) as
common among non-sparse GP models due to inversions m × m matrices. In our imple-
mentations LA and EP need similar running times, on the order of a few minutes for several
hundred data-points. Making AIS work efﬁciently requires some ﬁne-tuning and a single
estimate of p(D |θ) can take several hours for data sets of a few hundred examples, but this
could conceivably be improved upon.

3 Structural Properties of the Posterior and its Approximations

Structural properties of the posterior can best be understood by examining its construction.
The prior is a correlated m-dimensional Gaussian N (f |0, K) centred at the origin. Each
likelihood term p(yi |fi ) softly truncates the half-space from the prior that is incompatible
with the observed label, see Figure 1. The resulting posterior is unimodal and skewed,
similar to a multivariate Gaussian truncated to the orthant containing y. The mode of

−404800.020.040.060.080.10.120.140.16p(f|y)−404800.20.40.60.81fp(y|f)Likelihood p(y|f)Prior p(f)Posterior p(f|y)Laplace q(f|y)EP q(f|y)fifj.the posterior remains close to the origin, while the mass is placed in accordance with the
observed class labels. Additionally, high dimensional Gaussian distributions exhibit the
property that most probability mass is contained in a thin ellipsoidal shell – depending on
the covariance structure – away from the mean [7, ch. 29.2]. Intuitively this occurs since in
high dimensions the volume grows extremely rapidly with the radius. As an effect the mode
becomes less representative (typical) for the prior distribution as the dimension increases.
For the GPC posterior this property persists: the mode of the posterior distribution stays
relatively close to the origin, still being unrepresentative for the posterior distribution, while
the mean moves to the mass of the posterior making mean and mode differ signiﬁcantly.

We cannot generally assume the posterior to be close to Gaussian, as in the often studied
limit of low-dimensional parametric models with large amounts of data. Therefore in GPC
we must be aware of making a Gaussian approximation to a non-Gaussian posterior. From
the properties of the posterior it can be expected that Laplace’s method places m in the right
orthant but too close to the origin, such that the approximation will overlap with regions
having practically zero posterior mass. As an effect the amplitude of the approximate latent
posterior GP will be underestimated systematically, leading to overly cautious predictive
distributions. The EP approximation does not rely on a local expansion, but assumes that
the marginal distributions can be well approximated by Gaussians. This assumption will
be examined empirically below.

4 Experiments

In this section we compare and inspect approximations for GPC using various benchmark
data sets. The primary focus is not to optimise the absolute performance of GPC models
but to compare the relative accuracy of approximations and to validate the arguments given
k(x, x0 |θ) = σ2 exp (cid:0) − 1
2 kx − x0 k2 /‘2 (cid:1),
in the previous section. In all experiments we use a covariance function of the form:
such that θ = [σ, ‘]. We refer to σ2 as the signal variance and to ‘ as the characteristic
length-scale. Note that for many classiﬁcation tasks it may be reasonable to use an in-
dividual length scale parameter for every input dimension (ARD) or a different kind of
covariance function. Nevertheless, for the sake of presentability we use the above covari-
ance function and we believe the conclusions about the accuracy of approximations to be
independent of this choice, since it relies on arguments which are independent of the form
of the covariance function.

(1)

As measure of the accuracy of predictive probabilities we use the average information in
bits of the predictions about the test targets in excess of that of random guessing. Let
p∗ = p(y∗ = 1|D , θ , x∗ ) be the model’s prediction, then we average:
log2 (1 − p∗
i ) + 1−yi
log2 (p∗
I (p∗
i ) + H
i , yi ) = yi+1
2
2
over all test cases, where H is the entropy of the training labels. The error rate E is equal
to the percentage of erroneous class assignments if prediction is understood as a decision
problem with symmetric costs.

(2)

For the ﬁrst set of experiments presented here the well-known USPS digits and the Iono-
sphere data set were used. A binary sub-problem from the USPS digits is deﬁned by only
considering 3’s vs. 5’s (which is probably the hardest of the binary sub-problems) and di-
viding the data into 767 cases for training and 773 for testing. The Ionosphere data is split
into 200 training and 151 test cases. We do an exhaustive investigation on a ﬁne regular
grid of values for the log hyperparameters. For each θ on the grid we compute the approxi-
mated log marginal likelihood by LA, EP and AIS. Additionally we compute the respective
predictive performance (2) on the test set. Results are shown in Figure 2.

(1a)

(1b)

(1c)

(2a)

(2b)

(2c)

(3a)

(3b)

(3c)

(4c)
(4b)
(4a)
Figure 2: Comparison of marginal likelihood approximations and predictive performances
of different approximation techniques for USPS 3s vs. 5s (upper half) and the Ionosphere
data (lower half). The columns correspond to LA (a), EP (b), and MCMC (c). The rows
show estimates of the log marginal likelihood (rows 1 & 3) and the corresponding predictive
performance (2) on the test set (rows 2 & 4) respectively.

−200−200−150−150−130−130−115−115−105−100log lengthscale, log(l)log magnitude, log(σf)Log marginal likelihood2345012345−200−200−160−160−130−115−105−105−100−95−92log lengthscale, log(l)log magnitude, log(σf)Log marginal likelihood23450123452345012345log lengthscale, log(l)log magnitude, log(σf)Log marginal likelihood−92−95−100−105−105−115−130−160−160−200−2000.250.250.50.50.70.70.80.80.84log lengthscale, log(l)log magnitude, log(σf)Information about test targets in bits23450123450.250.50.70.70.80.80.840.840.860.860.880.89log lengthscale, log(l)log magnitude, log(σf)Information about test targets in bits23450123450.250.50.70.70.80.840.840.860.860.880.89log lengthscale, log(l)log magnitude, log(σf)Information about test targets in bits2345012345−200−150−120−120−100−100−90−80−75−70log lengthscale, log(l)log magnitude, log(σf)Log marginal likelihood−1012345−1012345−120−120−100−100−90−90−80−75−75−70−65log lengthscale, log(l)log magnitude, log(σf)Log marginal likelihood−1012345−1012345−120−120−100−100−90−90−80−75−75−70−65log lengthscale, log(l)log magnitude, log(σf)Log marginal likelihood−1012345−1012345000.10.10.20.20.30.40.50.55log lengthscale, log(l)log magnitude, log(σf)Information about test targets in bits−1012345−1012345000.10.10.20.20.30.30.40.50.50.550.6log lengthscale, log(l)log magnitude, log(σf)Information about test targets in bits−1012345−1012345000.10.10.20.20.30.30.40.50.50.50.50.550.6log lengthscale, log(l)log magnitude, log(σf)Information about test targets in bits−1012345−1012345(a)

(c)

(b)
Figure 3: Panel (a) and (b) show two marginal distributions p(fi |D , θ) from a GPC poste-
rior and its approximations. The true posterior is approximated by a normalised histogram
of 9000 samples of fi obtained by MCMC sampling. Panel (c) shows a histogram of sam-
ples of a marginal distribution of a truncated high-dimensional Gaussian. The line describes
a Gaussian with mean and variance estimated from the samples.

For all three approximation techniques we see an agreement between marginal likelihood
estimates and test performance, which justiﬁes the use of ML-II parameter estimation. But
the shape of the contours and the values differ between the methods. The contours for
Laplace’s method appear to be slanted compared to EP. The marginal likelihood estimates
of EP and AIS agree surprisingly well1 , given that the marginal likelihood comes as a 767
respectively 200 dimensional integral. The EP predictions contain as much information
about the test cases as the MCMC predictions and signiﬁcantly more than for LA. Note
that for small signal variances (roughly ln(σ2 ) < 1) LA and EP give very similar results.
A possible explanation is that for small signal variances the likelihood does not truncate
the prior but only down-weights the tail that disagrees with the observation. As an effect
the posterior will be less skewed and both approximations will lead to similar results.
For the USPS 3’s vs. 5’s we now inspect the marginal distributions p(fi |D , θ) of single
latent function values under the posterior approximations for a given value of θ . We have
chosen the values ln(σ) = 3.35 and ln(‘) = 2.85 which are between the ML-II esti-
mates of EP and LA. Hybrid MCMC was used to generate 9000 samples from the posterior
p(f |D , θ). For LA and EP the approximate marginals are q(fi |D , θ) = N (fi |mi , Aii )
where m and A are found by the respective approximation techniques.
In general we observe that the marginal distributions of MCMC samples agree very well
with the respective marginal distributions of the EP approximation. For Laplace’s approx-
imation we ﬁnd the mean to be underestimated and the marginal distributions to overlap
with zero far more than the EP approximations. Figure (3a) displays the marginal dis-
tribution and its approximations for which the MCMC samples show maximal skewness.
Figure (3b) shows a typical example where the EP approximation agrees very well with the
MCMC samples. We show this particular example because under the EP approximation
p(yi = 1|D , θ) < 0.1% but LA gives a wrong p(yi = 1|D , θ) ≈ 18%.
In the experiment we saw that the marginal distributions of the posterior often agree very

1Note that the agreement between the two seems to be limited by the accuracy of the MCMC
runs, as judged by the regularity of the contour lines; the tolerance is less than one unit on a (natural)
log scale.

−16−14−12−10−8−6−4−202400.050.10.150.2fMCMC samplesLaplace p(f|D)EP p(f|D)−40−35−30−25−20−15−10−505101500.020.040.06fMCMC samplesLaplace p(f|D)EP p(f|D)024600.050.10.150.20.250.30.350.40.45xip(xi)well with a Gaussian approximation. This seems to contradict the description given in the
previous section were we argued that the posterior is skewed by construction. In order to
inspect the marginals of a truncated high-dimensional multivariate Gaussian distribution
we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian
N (x|0, C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and
all other eigenvalues are 1. We then truncate this distribution such that all xi ≥ 0. Note
that the mode of the truncated Gaussian is still at zero, whereas the mean moves towards
the remaining mass. Figure (3c) shows a normalised histogram of samples from a marginal
distribution of one xi . The samples agree very well with a Gaussian approximation. In
the previous section we described the somewhat surprising property, that for a truncated
high-dimensional Gaussian, resembling the posterior, the mode (used by LA) may not be
particularly representative of the distribution. Although the marginal is also truncated, it
is still exceptionally well modelled by a Gaussian – however, the Laplace approximation
centred on the origin would be completely inappropriate.

In a second set of experiments we compare the predictive performance of LA and EP for
GPC on several well known benchmark problems. Each data set is randomly split into 10
folds of which one at a time is left out as a test set to measure the predictive performance
of a model trained (or selected) on the remaining nine folds. All performance measures are
averages over the 10 folds. For GPC we implement model selection by ML-II hyperparam-
eter estimation, reporting results given the θ that maximised the respective approximate
marginal likelihoods p(D |θ).
In order to get a better picture of the absolute performance we also compare to results
obtained by C-SVM classiﬁcation. The kernel we used is equivalent to the covariance
function (1) without the signal variance parameter. For each fold the parameters C and
‘ are found in an inner loop of 5-fold cross-validation, in which the parameter grids are
reﬁned until the performance stabilises. Predictive probabilities for test cases are obtained
by mapping the unthresholded output of the SVM to [0, 1] using a sigmoid function [8].
Results are summarised in Table 1. Comparing Laplace’s method to EP the latter shows
to be more accurate both in terms of error rate and information. While the error rates are
relatively similar the predictive distribution obtained by EP shows to be more informative
about the test targets. Note that for GPC the error rate only depends of the sign of the
mean µ∗ of the approximated posterior over latent functions and not the entire posterior
predictive distribution. As to be expected, the length of the mean vector kmk shows much
larger values for the EP approximations. Comparing EP and SVMs the results are mixed.
For the Crabs data set all methods show the same error rate but the information content of
the predictive distributions differs dramatically. For some test cases the SVM predicts the
wrong class with large certainty.

5 Summary & Conclusions

Our experiments reveal serious differences between Laplace’s method and EP when used in
GPC models. From the structural properties of the posterior we described why LA system-
atically underestimates the mean m. The resulting posterior GP over latent functions will
have too small amplitude, although the sign of the mean function will be mostly correct. As
an effect LA gives over-conservative predictive probabilities, and diminished information
about the test labels. This effect has been show empirically on several real world exam-
ples. Large resulting discrepancies in the actual posterior probabilities were found, even at
the training locations, which renders the predictive class probabilities produced under this
approximation grossly inaccurate. Note, the difference becomes less dramatic if we only
consider the classiﬁcation error rates obtained by thresholding p∗ at 1/2. For this particular
task, we’ve seen the the sign of the latent function tends to be correct (at least at the training
locations).

SVM
EP
Laplace
kmk
kmk
E%
I
E%
I
E%
I
n
m
Data Set
5.69 0.681
124.94
7.99 0.661
49.96
8.84 0.591
34
351
Ionosphere
84.95
3.21 0.795
62.62
3.21 0.805
9
3.21 0.804
683
Wisconsin
47.49 23.01 0.232
29.05 22.63 0.253
8 22.77 0.252
768
Pima Indians
2552.97
2.0 0.047
2.0 0.682 112.34
7
2.0 0.908
200
Crabs
26.86 13.85 0.537 15678.55 11.14 0.567
60 15.36 0.439
Sonar
208
USPS 3 vs 5 1540 256
2.27 0.849 163.05
2.21 0.902 22011.70
2.01 0.918

Table 1: Results for benchmark data sets. The ﬁrst three columns give the name of the data
set, number of observations m and dimension of inputs n. For Laplace’s method and EP
the table reports the average error rate E%, the average information I (2) and the average
length kmk of the mean vector of the Gaussian approximation. For SVMs the error rate
and the average information about the test targets are reported. Note that for the Crabs data
set we use the sex (not the colour) of the crabs as class label.

The EP approximation has shown to give results very close to MCMC both in terms of
predictive distributions and marginal likelihood estimates. We have shown and explained
why the marginal distributions of the posterior can be well approximated by Gaussians.

Further, the marginal likelihood values obtained by LA and EP differ systematically which
will lead to different results of ML-II hyperparameter estimation. The discrepancies are
similar for different tasks. Using AIS we were able to show the accuracy of marginal
likelihood estimates, which to the best of our knowledge has never been done before.

In summary, we found that EP is the method of choice for approximate inference in bi-
nary GPC models, when the computational cost of MCMC is prohibitive. In contrast, the
Laplace approximation is so inaccurate that we advise against its use, especially when
predictive probabilities are to be taken seriously. Further experiments and a detailed de-
scription of the approximation schemes can be found in [2].

Acknowledgements Both authors acknowledge support by the German Research Foun-
dation (DFG) through grant RA 1030/1. This work was supported in part by the IST Pro-
gramme of the European Community, under the PASCAL Network of Excellence, IST-
2002-506778. This publication only reﬂects the authors’ views.

References

[1] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In David S. Touretzky,
Michael C. Mozer, and Michael E. Hasselmo, editors, NIPS 8, pages 514–520. MIT Press, 1996.
[2] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process
classiﬁcation.
Journal of Machine Learning Research, 6:1679–1704, 2005.
[3] C. K. I. Williams and D. Barber. Bayesian classi ﬁcation with Gaussian processes.
actions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.
[4] T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Depart-
ment of Electrical Engineering and Computer Science, MIT, 2001.
[5] R. M. Neal. Regression and classiﬁcation using Gaussian process priors. In J. M. Bernardo, J. O.
Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475–501. Oxford
University Press, 1998.
[6] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001.
[7] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. CUP, 2003.
[8] J. C. Platt. Probabilities for SV machines. In Advances in Large Margin Classiﬁers , pages 61–73.
The MIT Press, 2000.

IEEE Trans-


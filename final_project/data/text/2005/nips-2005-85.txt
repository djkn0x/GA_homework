Generalization in Clustering with Unobserved
Features

Eyal Krupka and Naftali Tishby
School of Computer Science and Engineering,
Interdisciplinary Center for Neural Computation
The Hebrew University Jerusalem, 91904, Israel
{eyalkr,tishby}@cs.huji.ac.il

Abstract

We argue that when objects are characterized by many attributes, clus-
tering them on the basis of a relatively small random subset of these
attributes can capture information on the unobserved attributes as well.
Moreover, we show that under mild technical conditions, clustering the
objects on the basis of such a random subset performs almost as well as
clustering with the full attribute set. We prove a ﬁnite samp le general-
ization theorems for this novel learning scheme that extends analogous
results from the supervised learning setting. The scheme is demonstrated
for collaborative ﬁltering of users with movies rating as at
tributes.

1

Introduction

Data clustering is unsupervised classi ﬁcation of objects i nto groups based on their similar-
ity [1]. Often, it is desirable to have the clusters to match some labels that are unknown
to the clustering algorithm. In this context, a good data clustering is expected to have ho-
mogeneous labels in each cluster, under some constraints on the number or complexity of
the clusters. This can be quanti ﬁed by mutual information (s ee e.g. [2]) between the ob-
jects’ cluster identity and their (unknown) labels, for a given complexity of clusters. Since
the clustering algorithm has no access to the labels, it is unclear how the algorithm can
optimize the quality of the clustering. Even worse, the clustering quality depends on the
speci ﬁc choice of the unobserved labels. For example a good d ocuments clustering with
respect to topics is very different from a clustering with respect to authors.

In our setting, instead of trying to cluster by some “arbitra ry” labels, we try to predict
unobserved features from observed ones. In this sense our target “labels ” are yet other
features that “happened” to be unobserved. For example, whe n clustering fruits based on
their observed features, such as shape, color and size, the target of clustering is to match
unobserved features, such as nutritional value and toxicity.

In order to theoretically analyze and quantify this new learning scheme, we make the fol-
lowing assumptions. Consider an inﬁnite set of features, an d assume that we observe only
a random subset of n features, called observed features. The other features are called un-
observed features. We assume that the random selection of features is done uniformly and
independently.

Training set
Test set
Learning algorithm
Hypothesis class
Min generalization error
ERM
Good generalization

Table 1: Analogy with supervised learning
n randomly selected features (observed features)
Unobserved features
Cluster the instances into k clusters
All possible partitions of m instances into k clusters
Max expected information on unobserved features
Observed Information Maximization (OIM)
Mean observed and unobserved information are similar

The clustering algorithm has access only to the observed features of m instances. After the
clustering, one of the unobserved features is randomly and uniformly selected to be a target
label, i.e. clustering performance is measured with respect to this feature. Obviously, the
clustering algorithm cannot be directly optimized for this speci ﬁc feature.

The question is whether we can optimize the expected performance on the unobserved
feature, based on the observed features alone. The expectation is over the random selection
of the target feature. In other words, can we ﬁnd clusters tha t match as many unobserved
features as possible? Perhaps surprisingly, for large enough number of observed features,
the answer is yes. We show that for any clustering algorithm, the average performance of
the clustering with respect to the observed and unobserved features, is similar. Hence we
can indirectly optimize clustering performance with respect to the unobserved features, in
analogy to generalization in supervised learning. These results are universal and do not
require any additional assumptions such as underling model or a distribution that created
the instances.

In order to quantify these results, we deﬁne two terms:
the av erage observed informa-
tion and the expected unobserved information. Let T be the variable which represents the
cluster for each instance, and {X1 , ..., X∞ } the set of random variables which denotes the
features. The average observed information, denoted by Iob , is the average mutual informa-
tion between T and each of the observed features. In other words, if the observed features
n Pn
are {X1 , ..., Xn } then Iob = 1
j=1 I (T ; Xj ). The expected unobserved information,
denoted by Iun , is the expected value of the mutual information between T and a randomly
selected unobserved feature, i.e. Ej {I (T ; Xj )}. Note that whereas Iob can be measured
directly, this paper deals with the question of how to infer and maximize Iun .
Our main results consist of two theorems. The ﬁrst is a genera lization theorem. It gives
an upper bound on the probability of large difference between Iob and Iun for all possible
clusterings. It also states a uniform convergence in probability of |Iob − Iun | as the num-
ber of observed features increases. Conceptually, the observed mean information, Iob , is
analogous to the training error in standard supervised learning [3], whereas the unobserved
information, Iun , is similar to the generalization error.
The second theorem states that under constraint on the number of clusters, and large enough
number of observed features, one can achieve nearly the best possible performance, in
terms of Iun . Analogous to the principle of Empirical Risk Minimization (ERM) in statis-
tical learning theory [3], this is done by maximizing Iob .
Table 1 summarizes the correspondence of our setting to that of supervised learning. The
key difference is that in supervised learning, the set of features is ﬁxed and the training
instances (samples) are assumed to be randomly drawn from some distribution.
In our
setting, the set of instances is ﬁxed, but the set of observed features is assumed to be
randomly selected.

Our new theorems are evaluated empirically in section 3, on a data set of movie ratings.

This empirical test also suggests one future research direction: use the framework sug-
gested in this paper for collaborative ﬁltering. Our main po int in this paper, however, is the
new conceptual framework and not a speci ﬁc algorithm or expe rimental performance.
Related work The idea of an information tradeoff between complexity and information
on target variables is similar to the idea of the information bottleneck [4]. But unlike the
bottleneck method, here we are trying to maximize information on unobserved variables,
using ﬁnite samples.

In the framework of learning with labeled and unlabeled data [5], a fundamental issue is the
link between the marginal distribution P (x) over examples x and the conditional P (y |x)
for the label y [6]. From this point of view our approach assumes that y is a feature in itself.

2 Mathematical Formulation and Analysis

Consider a set of discrete random variables {X1 , ..., XL }, where L is very large (L →
∞). We randomly, uniformly and independently select n << √L variables from this set.
These variables are the observed features and their indexes are denoted by {q1 , ..., qn }. The
remaining L − n variables are the unobserved features. A clustering algorithm has access
only to the observed features over m instances {x[1], ..., x[m]}. The algorithm assigns a
cluster label ti ∈ {1, ..., k} for each instance x[i], where k is the number of clusters. Let T
denote the cluster label assigned by the algorithm.

Shannon’s mutual information between two variables is a function of their joint distribu-
P (t, xj ) log (cid:16) P (t,xj )
P (t)P (xj ) (cid:17). Since we are dealing with a
tion, deﬁned as I (T ; Xj ) = Pt,xj
ﬁnite number of samples, m, the distribution P is taken as the empirical joint distribution
of (T , Xj ), for every j . For a random j , this empirical mutual information is a random
variable on its own.
n Pn
Iob = 1
The average observed information, Iob , is now deﬁned as
i=1 I (T ; Xqi ). In
general, Iob is higher when clusters are more coherent, i.e. elements within each cluster
have many similar attributes. The expected unobserved information, Iun , is deﬁned as
Iun = Ej {I (T ; Xj )}. We can assume that the unobserved feature is with high probability
from the unobserved set. Equivalently, Iun can be the mean mutual information between
the clusters and each of the unobserved features, Iun = 1
L−n Pj /∈{q1 ,...,qn } I (T ; Xj ).
The goal of the clustering algorithm is to ﬁnd cluster labels {t1 , ..., tm }, that maximize
Iun , subject to a constraint on their complexity - henceforth considered as the number of
clusters (k ≤ D) for simplicity, where D is an integer bound.
Before discussing how to maximize Iun , we consider ﬁrst the problem of estimating it.
Similar to the generalization error in supervised learning, Iun cannot be estimated directly
in the learning algorithm, but we may be able to bound the difference between the observed
information Iob - our “training error ” - and
Iun - the “generalization error ”. To obtain gen-
eralization this bound should be uniform over all possible clusterings with a high proba-
bility over the randomly selected features. The following lemma argues that such uniform
convergence in probability of Iob to Iun always occurs.

Lemma 1 With the deﬁnitions above,

Pr ( sup
{t1 ,...,tm } |Iob − Iun | > ǫ) ≤ 2e−2nǫ2 /(log k)2+m log k ∀ǫ > 0
where the probability is over the random selection of the observed features.

Proof: For ﬁxed cluster labels, {t1 , ..., tm }, and a random feature j , the mutual infor-
mation I (T ; Xj ) is a function of the random variable j , and hence I (T ; Xj ) is a random
variable in itself. Iob is the average of n such independent random variables and Iun is its
expected value. Clearly, for all j , 0 ≤ I (T ; Xj ) ≤ log k . Using Hoeffding’s inequality [7],
Pr {|Iob − Iun | > ǫ} ≤ 2e−2nǫ2 /(log k)2 . Since there are at most km possible partitions,
the union bound is sufﬁcient to prove the lemma 1.
Note that for any ǫ > 0, the probability that |Iob − Iun | > ǫ goes to zero, as n → ∞. The
convergence rate of Iob to Iun is bounded by O(log n/√n). As expected, this upper bound
decreases as the number of clusters, k , decreases.
Unlike the standard bounds in supervised learning, this bound increases with the number
of instances (m), and decreases with increasing number of observed features (n). This
is because in our scheme the training size is not the number of instances, but rather the
number of observed features (See Table 1). However, in the next theorem we obtain an
upper bound that is independent of m, and hence is tighter for large m.

Theorem 1 (Generalization Theorem) With the deﬁnitions above,

{t1 ,...,tm } |Iob − Iun | > ǫ) ≤ 8(log k)e
Pr ( sup
− nǫ2
8(log k)2 +
the number of different values it can
where |Xj | denotes the alphabet size of Xj (i.e.
obtain). Again, the probability is over the random selection of the observed features.
The convergence rate here is bounded by O(log n/3√n). However, for relatively large n
one can use the bound in lemma 1, which converge faster.

∀ǫ > 0

4k maxj |Xj |
ǫ

log k−log ǫ

A detailed proof of theorem 1 can be found in [8]. Here we provide the outline of the proof.
Proof outline: From the given m instances and any given cluster labels {t1 , ..., tm }, draw
uniformly and independently m′ instances (repeats allowed) and denote their indexes by
{i1 , ..., im′ }. We can estimate I (T ; Xj ) from the empirical distribution of (T , Xj ) over
the m′ instances. This distribution is denoted by ˆP (t, xj ) and the corresponding mutual
information is denoted by I ˆP (T ; Xj ). Theorem 1 is build up from the following upper
bounds, which are independent of m, but depend on the choice of m′ . The ﬁrst bound is on
E (cid:8)(cid:12)(cid:12)I (T ; Xj ) − I ˆP (T ; Xj )(cid:12)(cid:12)(cid:9), where the expectation is over random selection of the m′
instances. From this bound we derive upper bounds on |Iob − E ( ˆIob )| and |Iun − E ( ˆIun )|,
where ˆIob , ˆIun are the estimated values of Iob , Iun based on the subset of m′ instances.
The last required bound is on the probability that sup{t1 ,...,tm } |E ( ˆI ob ) − E ( ˆIun )| > ǫ1 ,
for any ǫ1 > 0. This bound is obtained from lemma 1. The choice of m′ is independent on
m. Its value should be large enough for the estimations ˆIob , ˆIun to be accurate, but not too
large, so as to limit the number of possible clusterings over the m′ instances.
We now describe the above mentioned upper bounds in more details. Using Paninski [9]
(proposition 1) it is easy to show that the bias between I (T ; Xj ) and its maximum likeli-
hood estimation, based on ˆP (t, xj ) is bounded as follows.
E{i1 ,...,im′ } (cid:8)(cid:12)(cid:12)I (T ; Xj ) − I ˆP (T ; Xj )(cid:12)(cid:12)(cid:9) ≤ log (cid:18)1 +
From this equation we obtain,
|Iob − E{i1 ,...,im′ } ( ˆIob )|, |Iun − E{i1 ,...,im′ } ( ˆIun )| ≤ k max
j

k |Xj | − 1
m′

|Xj |/m′

(cid:19) ≤

k |Xj |
m′

(1)

(2)

Using lemma 1 we have an upper bound on the probability that sup{t1 ,...,tm } | ˆIob − ˆIun | > ǫ
over the random selection of features, as a function of m′ . However, the upper bound
we need is on the probability that sup{t1 ,...,tm } |E ( ˆI ob ) − E ( ˆIun )| > ǫ1 . Note that the
expectations E ( ˆIob ), E ( ˆIun ) are done over random selection of the subset of m′ instances,
for a set of features that were randomly selected once. In order to link between these two
probabilities, we need the following lemma.

4 log k
ǫ1

−
e

nǫ2
2(log k)2 +m′ log k
1

Lemma 2 Consider a function f of two independent random variables (Y , Z ). We assume
that f (y , z ) ≤ c, ∀y , z , where c is some constant. If Pr {f (Y , Z ) > ˜ǫ} ≤ δ , then
c − ˜ǫ
Pr
Z {Ey (f (y , Z )) ≥ ǫ} ≤
δ ∀ǫ > ˜ǫ
ǫ − ˜ǫ
The proof of this lemma is rather standard and is given in [8]. From lemmas 1 and 2 it is
easy to show that
! > ǫ1) ≤
Pr (E{i1 ,...,im′ }   sup
ˆIob − ˆIun (cid:12)(cid:12)(cid:12)
{t1 ,...,tm } (cid:12)(cid:12)(cid:12)
Lemma 2 is used, where Z represents the random selection of features, Y represents the
random selection of m′ instances, f (y , z ) = sup{t1 ,...,tm } | ˆIob − ˆIun |, c = log k , and
˜ǫ = ǫ1 /2. From eq. 2 and 3 it can be shown that
Pr ( sup
) ≤
{t1 ,...,tm } |Iob − Iun | > ǫ1 +
By selecting ǫ1 = ǫ/2, m′ = 4k maxj |Xj |/ǫ, we obtain theorem 1.
Note that the selection of m′ depends on k maxj |Xj |. This reﬂects the fact that in order
to accurately estimate I (T , Xj ), we need a number of instances, m′ , which is much larger
than the product of the alphabet sizes of T , Xj .
We can now return to the problem of specifying a clustering that maximizes Iun , using only
the observed features. For a reference, we will ﬁrst deﬁne
Iun of the best possible clusters.

2k maxj |Xj |
m′

4 log k
ǫ1

−
e

(3)

nǫ2
2(log k)2 +m′ log k
1

Deﬁnition 1 Maximally achievable unobserved information: Let I ∗
un,D be the maximum
value of Iun that can be achieved by any clustering {t1 , ..., tm }, subject to the constraint
k ≤ D , for some constant D

I ∗
un,D =

sup
{{t1 ,...,tm }:k≤D}

Iun

The clustering that achieves this value is called the best clustering. The average observed
information of this clustering is denoted by I ∗
ob,D .

Deﬁnition 2 Observed information maximization algorithm: Let IobMax be any cluster-
ing algorithm that, based on the values of observed features alone, selects the cluster labels
{t1 , ..., tm } having the maximum possible value of Iob , subject to the constraint k ≤ D .
Let ˜Iob,D be the average observed information achieved by IobMax algorithm. Let ˜Iun,D
be the expected unobserved information achieved by the IobMax algorithm.

The next theorem states that IobMax not only maximizes Iob , but also Iun .

8k maxj |Xj |
ǫ

log k−log(ǫ/2)

Theorem 2 With the deﬁnitions above,
− nǫ2
Pr n ˜Iun,D ≤ I ∗
un,D − ǫo ≤ 8(log k)e
32(log k)2 +
where the probability is over the random selection of the observed features.
Proof: We now deﬁne a bad clustering as a clustering whose expected unobserved infor-
un,D − ǫ. Using Theorem 1, the probability that |Iob − Iun | > ǫ/2
mation satis ﬁes Iun ≤ I ∗
for any of the clusterings is upper bounded by the right term of equation 4. If for all clus-
un,D − ǫ/2 (see Deﬁnition 1) and Iob of
terings |Iob − Iun | ≤ ǫ/2, then surely I ∗
ob,D ≥ I ∗
all bad clusterings satis ﬁes Iob ≤ I ∗
un,D − ǫ/2. Hence the probability that a bad clustering
has a higher average observed information than the best clustering is upper bounded as in
Theorem 2.

∀ǫ > 0 (4)

As a result of this theorem, when n is large enough, even an algorithm that knows the value
of all the features (observed and unobserved) cannot ﬁnd a cluster ing with the same com-
plexity (k) which is signi ﬁcantly better than the clustering found by I obM ax algorithm.

3 Empirical Evaluation

In this section we describe an experimental evaluation of the generalization properties of
the IobMax algorithm for a ﬁnite large number of features. We examine th e difference
between Iob and Iun as function of the number of observed features and the number of
clusters used. We also compare the value of Iun achieved by IobMax algorithm to the
maximum achievable I ∗
un,D (See deﬁnition 1).
Our evaluation uses a data set typically used for collaborative ﬁltering. Collaborative ﬁl-
tering refers to methods of making predictions about a user’s preferences, by collecting
preferences of many users. For example, collaborative ﬁlte ring for movie ratings could
make predictions about rating of movies by a user, given a partial list of ratings from this
user and many other users. Clustering methods are used for collaborative ﬁltering by cluster
users based on the similarity of their ratings (see e.g. [10]).

In our setting, each user is described as a vector of movie ratings. The rating of each movie
is regarded as a feature. We cluster users based on the set of observed features, i.e. rated
movies. In our context, the goal of the clustering is to maximize the information between
the clusters and unobserved features, i.e. movies that have not yet been rated by any of the
users. By Theorem 2, given large enough number of rated movies, we can achieve the best
possible clustering of users with respect to unseen movies. In this region, no additional
information (such as user age, taste, rating of more movies) beyond the observed features
can improve Iun by more than some small ǫ.
The purpose of this section is not to suggest a new algorithm for collaborative ﬁltering or
compare it to other methods, but simply to illustrate our new theorems on empirical data.
Dataset. We used MovieLens (www.movielens.umn.edu), which is a movie rating data
set. It was collected distributed by GroupLens Research at the University of Minnesota. It
contains approximately 1 million ratings for 3900 movies by 6040 users. Ratings are on
a scale of 1 to 5. We used only a subset consisting of 2400 movies by 4000 users. In our
setting, each instance is a vector of ratings (x1 , ..., x2400 ) by speci ﬁc user. Each movie is
viewed as a feature, where the rating is the value of the feature.
Experimental Setup. We randomly split the 2400 movies into two groups, denoted by
“A” and “B”, of 1200 movies (features) each. We used a subset o
f the movies from group
“A” as observed features and all movies from group “B” as the u
nobserved features. The
experiment was repeated with 10 random splits and the results averaged. We estimated Iun
by the mean information between the clusters and ratings of movies from group “B”.

0.025

0. 02

0.015

0. 01

I*
un

0.005

0

0

0.025

0. 02

0.015

I*
un

0. 01

0.005

Iob

Iun

0.015

0. 01

0.005

Iob

Iun

Iob

Iun

1000
800
600
400
200
Numbe r o f obse rved  fea tures  (movi es ) (n)
(a) 2 Clusters

1200

0

0

200
1000
800
600
400
Numbe r o f obse rved  fea tures  (movi es ) (n)
(b) 6 Clusters

1200

0

2

3

4
Numbe r o f cluste rs  (k)
(c) Fixed n (1200)

5

6

Figure 1: Iob , Iun and I ∗
un per number of training movies and clusters. In (a) and (b) the
number of movies is variable, and the number of clusters is ﬁx ed. In (c) The number of
observed movies is ﬁxed (1200), and the number of clusters is variable. The overall mean
information is low, since the rating matrix is sparse.

Handling Missing Values. In this data set, most of the values are missing (not rated). We
handle this by deﬁning the feature variable as 1,2,...,5 for
the ratings and 0 for missing
value. We maximize the mutual information based on the empirical distribution of values
that are present, and weight it by the probability of presence for this feature. Hence, Iob =
Pn
j=1 P (Xj 6= 0)I (T ; Xj |Xj 6= 0) and Iun = Ej {P (Xj 6= 0)I (T ; Xj |Xj 6= 0)}. The
weighting prevents ’over ﬁtting’ to movies with few ratings . Since the observed features
were selected at random, the statistics of missing values of the observed and unobserved
features are the same. Hence, all theorems are applicable to these deﬁnitions of Iob and Iun
as well.

Greedy IobMax Algorithm

We cluster the users using a simple greedy clustering algorithm . The input to the algorithm
is all users, represented solely by the observed features. Since this algorithm can only ﬁnd
a local maximum of Iob , we ran the algorithm 10 times (each used a different random
initialization) and selected the results that had a maximum value of Iob . More details about
this algorithm can be found in [8].
In order to estimate I ∗
un,D (see deﬁnition 1), we also ran the same algorithm, where all t he
features are available to the algorithm (i.e. also features from group “B”). The algorithm
ﬁnds clusters that maximize the mean mutual information on f eatures from group "B".

Results

The results are shown in Figure 1. As n increases, Iob decreases and Iun increases, until
they converge to each other. For small n, the clustering ’over ﬁts’ to the observed features.
This is similar to training and test errors in supervised learning. For large n, Iun approaches
un,D , which means the I obM ax algorithm found nearly the best possible clustering -
to I ∗
as expected from the theorem 2. As the number of clusters increases, both Iob and Iun
increase, but the difference between them also increases.

4 Discussion and Summary

We introduce a new learning paradigm: clustering based on observed features that gen-
eralizes to unobserved features. Our results are summarized by two theorems that tell us
how, without knowing the value of the unobserved features, one can estimate and maximize
information between the clusters and the unobserved features.

The key assumption that enables us to prove the theorems is the random independent selec-
tion of the observed features. Another interpretation of the generalization theorem, without
using this assumption, might be combinatorial. The difference between the observed and
unobserved information is large only for a small portion of all possible partitions into ob-
served and unobserved features. This means that almost any arbitrary partition generalizes
well.

The importance of clustering which preserves information on unobserved features is that
it enables us to learn new - previously unobserved - attributes from a small number of
examples. Suppose that after clustering fruits based on their observed features, we eat a
chinaberry1 and thus, we ”observe” (by getting sick), the previously uno bserved attribute of
toxicity. Assuming that in each cluster, all fruits have similar unobserved attributes, we can
conclude that all fruits in the same cluster, i.e. all chinaberries, are likely to be poisonous.

We can even relate the IobMax principle to cognitive clustering in sensory information
processing. In general, a symbolic representation (e.g. assigning object names in language)
may be based on a similar principle - ﬁnd a representation (cl usters) that contain signi ﬁcant
information on as many observed features as possible, while still remaining simple. Such
representations are expected to contain information on other rarely viewed salient features.

Acknowledgments

We thank Amir Globerson, Ran Bachrach, Amir Navot, Oren Shriki, Avner Dor and Ilan
Sutskover for helpful discussions. We also thank the GroupLens Research Group at the
University of Minnesota for use of the MovieLens data set. Our work is partly supported
by grant from the Israeli Academy of Science.

References

[1] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Computing Surveys,
31(3):264–323, September 1999.
[2] T. M. Cover and J. A. Thomas. Elements Of Information Theory. Wiley Interscience, 1991.
[3] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.
[4] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. Proc. 37th Allerton
Conf. on Communication and Computation, 1999.
[5] M. Seeger. Learning with labeled and unlabeled data. Technical report, University of Edinburgh,
2002.
[6] M. Szummer and T. Jaakkola. Information regularization with partially labeled data. In NIPS,
2003.
[7] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58:13–30, 1963.
[8] E. Krupka and N. Tishby. Generalization in clustering with unobserved features. Technical
report, Hebrew University, 2005. http://www.cs.huji.ac.il/~tishby/nips2005tr.pdf.
[9] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1101–
1253, 2003.
[10] B. Marlin. Collaborative ﬁltering: A machine learning perspective. Master’s thesis, University
of Toronto, 2004.

1Chinaberries are the fruits of the Melia azedarach tree, and are poisonous.


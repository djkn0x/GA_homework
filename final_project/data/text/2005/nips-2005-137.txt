Off-policy Learning with Options and
Recognizers

Doina Precup
McGill University
Montreal, QC, Canada

Richard S. Sutton
University of Alberta
Edmonton, AB, Canada

Cosmin Paduraru
University of Alberta
Edmonton, AB, Canada

Anna Koop
University of Alberta
Edmonton, AB, Canada

Satinder Singh
University of Michigan
Ann Arbor, MI, USA

Abstract

We introduce a new algorithm for off-policy temporal-difference learn-
ing with function approximation that has lower variance and requires less
knowledge of the behavior policy than prior methods. We develop the no-
tion of a recognizer, a ﬁlter on actions that distorts the behavior policy to
produce a related target policy with low-variance importance-sampling
corrections. We also consider target policies that are deviations from
the state distribution of the behavior policy, such as potential temporally
abstract options, which further reduces variance. This paper introduces
recognizers and their potential advantages, then develops a full algorithm
for linear function approximation and proves that its updates are in the
same direction as on-policy TD updates, which implies asymptotic con-
vergence. Even though our algorithm is based on importance sampling,
we prove that it requires absolutely no knowledge of the behavior policy
for the case of state-aggregation function approximators.

Off-policy learning is learning about one way of behaving while actually behaving in an-
other way. For example, Q-learning is an off- policy learning method because it learns
about the optimal policy while taking actions in a more exploratory fashion, e.g., according
to an e-greedy policy. Off-policy learning is of interest because only one way of selecting
actions can be used at any time, but we would like to learn about many different ways of
behaving from the single resultant stream of experience. For example, the options frame-
work for temporal abstraction involves considering a variety of different ways of selecting
actions. For each such option one would like to learn a model of its possible outcomes suit-
able for planning and other uses. Such option models have been proposed as fundamental
building blocks of grounded world knowledge (Sutton, Precup & Singh, 1999; Sutton,
Rafols & Koop, 2005). Using off-policy learning, one would be able to learn predictive
models for many options at the same time from a single stream of experience.

Unfortunately, off-policy learning using temporal-difference methods has proven problem-
atic when used in conjunction with function approximation. Function approximation is
essential in order to handle the large state spaces that are inherent in many problem do-

mains. Q-learning, for example, has been proven to converge to an optimal policy in the
tabular case, but is unsound and may diverge in the case of linear function approximation
(Baird, 1996). Precup, Sutton, and Dasgupta (2001) introduced and proved convergence for
the ﬁrst off-policy learning algorithm with linear function approximation. They addressed
the problem of learning the expected value of a target policy based on experience generated
using a different behavior policy. They used importance sampling techniques to reduce the
off-policy case to the on-policy case, where existing convergence theorems apply (Tsitsik-
lis & Van Roy, 1997; Tadic, 2001). There are two important difﬁculties with that approach.
First, the behavior policy needs to be stationary and known, because it is needed to compute
the importance sampling corrections. Second, the importance sampling weights are often
ill-conditioned. In the worst case, the variance could be inﬁnite and convergence would not
occur. The conditions required to prevent this were somewhat awkward and, even when
they applied and asymptotic convergence was assured, the variance could still be high and
convergence could be slow.

In this paper we address both of these problems in the context of off-policy learning for
options. We introduce the notion of a recognizer. Rather than specifying an explicit target
policy (for instance, the policy of an option), about which we want to make predictions, a
recognizer speci ﬁes a condition on the actions that are selected. For example, a recognizer
for the temporally extended action of picking up a cup would not specify which hand is to
be used, or what the motion should be at all different positions of the cup. The recognizer
would recognize a whole variety of directions of motion and poses as part of picking the
cup. The advantage of this strategy is not that one might prefer a multitude of different
behaviors, but that the behavior may be based on a variety of different strategies, all of
which are relevant, and we would like to learn from any of them. In general, a recognizer
is a function that recognizes or accepts a space of different ways of behaving and thus, can
learn from a wider range of data.

Recognizers have two advantages over direct speciﬁcation of a target policy: 1) they are
a natural and easy way to specify a target policy for which importance sampling will be
well conditioned, and 2) they do not require the behavior policy to be known. The latter is
important because in many cases we may have little knowledge of the behavior policy, or a
stationary behavior policy may not even exist. We show that for the case of state aggrega-
tion, even if the behavior policy is unknown, convergence to a good model is achieved.

1 Non-sequential example

The beneﬁts of using recognizers in off-policy learning can be most easily seen in a non-
sequential context with a single continuous action. Suppose you are given a sequence of
sample actions ai ∈ [0, 1], selected i.i.d. according to probability density b : [0, 1] 7→ ´+
(the behavior density). For example, suppose the behavior density is of the oscillatory
form shown as a red line in Figure 1. For each each action, ai , we observe a corresponding
outcome, zi ∈ ´, a random variable whose distribution depends only on ai . Thus the be-
havior density induces an outcome density. The on-policy problem is to estimate the mean
mb of the outcome density. This problem can be solved simply by averaging the sample
outcomes: ˆmb = (1/n) (cid:229)n
i=1 zi . The off-policy problem is to use this same data to learn what
the mean would be if actions were selected in some way other than b, for example, if the
actions were restricted to a designated range, such as between 0.7 and 0.9.

There are two natural ways to pose this off-policy problem. The most straightforward way
is to be equally interested in all actions within the designated region. One professes to be
interested in actions selected according to a target density p : [0, 1] 7→ ´+ , which in the
example would be 5.0 between 0.7 and 0.9, and zero elsewhere, as in the dashed line in

Figure 1: The left panel shows the behavior policy and the target policies for the formula-
tions of the problem with and without recognizers. The right panel shows empirical esti-
mates of the variances for the two formulations as a function of the number sample actions.
The lowest line is for the formulation using empirically-estimated recognition probabilities.

Figure 1 (left). The importance- sampling estimate of the mean outcome is
p(ai )
n(cid:229)
p = 1
ˆm
b(ai ) zi .
n
i=1
This approach is problematic if there are parts of the region of interest where the behavior
density is zero or very nearly so, such as near 0.72 and 0.85 in the example. Here the
importance sampling ratios are exceedingly large and the estimate is poorly conditioned
(large variance). The upper curve in Figure 1 (right) shows the empirical variance of this
estimate as a function of the number of samples. The spikes and uncertain decline of the
empirical variance indicate that the distribution is very skewed and that the estimates are
very poorly conditioned.

(1)

The second way to pose the problem uses recognizers. One professes to be interested in
actions to the extent that they are both selected by b and within the designated region. This
leads to the target policy shown in blue in the left panel of Figure 1 (it is taller because it
still must sum to 1). For this problem, the variance of (1) is much smaller, as shown in
the lower two lines of Figure 1 (right). To make this way of posing the problem clear, we
introduce the notion of a recognizer function c : A 7→ ´+ . The action space in the example
is A = [0, 1] and the recognizer is c(a) = 1 for a between 0.7 and 0.9 and is zero elsewhere.
The target policy is deﬁned in general by
= c(a)b(a)
p(a) = c(a)b(a)
(2)
.
(cid:229)x c(x)b(x)
where µ = (cid:229)x c(x)b(x) is a constant, equal to the probability of recognizing an action from
the behavior policy. Given p, ˆmp from (1) can be rewritten in terms of the recognizer as
p(ai )
n(cid:229)
n(cid:229)
n(cid:229)
c(ai )b(ai )
c(ai )
= 1
= 1
p = 1
1
ˆm
b(ai )
b(ai )
n
n
n
i=1
i=1
i=1
Note that the target density does not appear at all in the last expression and that the be-
havior distribution appears only in µ , which is independent of the sample action. If this
constant is known, then this estimator can be computed with no knowledge of p or b. The
constant µ can easily be estimated as the fraction of recognized actions in the sample. The
lowest line in Figure 1 (right) shows the variance of the estimator using this fraction in
place of the recognition probability. Its variance is low, no worse than that of the exact
algorithm, and apparently slightly lower. Because this algorithm does not use the behavior
density, it can be applied when the behavior density is unknown or does not even exist. For
example, suppose actions were selected in some deterministic, systematic way that in the
long run produced an empirical distribution like b. This would be problematic for the other
algorithms but would require no modiﬁcation of the recognition-fraction algorithm.

(3)

zi

zi

zi

00.70.91Action012Probabilitydensityfunctions101002003004005000.511.5Empirical variances(average of 200 sample variances)without recognizerwith recognizerNumber of sample actionsTargetpolicy with recognizerBehavior policyTarget policy w/o recognizerµ
µ
µ
2 Recognizers improve conditioning of off-policy learning
The main use of recognizers is in formulating a target density p about which we can suc-
cessfully learn predictions, based on the current behavior being followed. Here we formal-
ize this intuition.
Theorem 1 Let A = {a1 , . . . ak } ⊆ A be a subset of all the possible actions. Consider a
ﬁxed behavior policy b and let pA be the class of policies that only choose actions from A,
i.e., if p(a) > 0 then a ∈ A. Then the policy induced by b and the binary recognizer cA is the
"(cid:18) p(ai )
(cid:19)2#
policy with minimum-variance one-step importance sampling corrections, among those in
pA :
p as given by (2) = arg minp∈pA
Eb
b(ai )
Proof: Denote p(ai ) = pi , b(ai ) = bi . Then the expected variance of the one-step impor-
"(cid:18) pi
(cid:19)2#
(cid:19)2 − 1 = (cid:229)
(cid:18) pi
(cid:19)(cid:21)
(cid:20)(cid:18) pi
tance sampling corrections is:
p2
= (cid:229)
− 1,
− E 2
i
Eb
bi
b
bi
bi
bi
bi
i
i
where the summation (here and everywhere below) is such that the action ai ∈ A. We
want to ﬁnd pi that minimizes this expression, subject to the constraint that (cid:229)i pi = 1.
This is a constrained optimization problem. To solve it, we write down the corresponding
Lagrangian:

(4)

p2
− 1 + b((cid:229)
L(pi , b) = (cid:229)
pi − 1)
i
bi
i
i
We take the partial derivatives wrt pi and b and set them to 0:
+ b = 0 ⇒ pi = − bbi
¶L
2
= pi
¶pi
2
bi
¶L
¶b = (cid:229)
pi − 1 = 0
i
By taking (5) and plugging into (6), we get the following expression for b:
− b
(cid:229)
bi = 1 ⇒ b = − 2
(cid:229)i bi
2
i
By substituting b into (5) we obtain:

(5)

(6)

pi = bi
(cid:229)i bi
This is exactly the policy induced by the recognizer deﬁned by c(ai ) = 1 iff ai ∈ A. (cid:5)
We also note that it is advantageous, from the point of view of minimizing the variance of
the updates, to have recognizers that accept a broad range of actions:

Theorem 2 Consider two binary recognizers c1 and c2 , such that µ 1 > µ 2 . Then the im-
portance sampling corrections for c1 have lower variance than the importance sampling
corrections for c2 .
(cid:19)2 1
(cid:18) bi
Proof: From the previous theorem, we have the variance of a recognizer cA :
p2
− 1 = (cid:229)
Var = (cid:229)
− 1 = 1
− 1
− 1 =
i
(cid:229) j∈A b j
bi
bi
i
i

1
(cid:229) j∈A b j

(cid:5)

µ
3 Formal framework for sequential problems
We turn now to the full case of learning about sequential decision processes with function
approximation. We use the standard framework in which an agent interacts with a stochas-
tic environment. At each time step t , the agent receives a state st and chooses an action at .
We assume for the moment that actions are selected according to a ﬁxed behavior policy,
b : S × A → [0, 1] where b(s, a) is the probability of selecting action a in state s. The behav-
ior policy is used to generate a sequence of experience (observations, actions and rewards).
The goal is to learn, from this data, predictions about different ways of behaving. In this
paper we focus on learning predictions about expected returns, but other predictions can be
tackled as well (for instance, predictions of transition models for options (Sutton, Precup
& Singh, 1999), or predictions speciﬁed by a TD-network (Sutton & Tanner, 2005; Sutton,
Rafols & Koop, 2006)). We assume that the state space is large or continuous, and function
approximation must be used to compute any values of interest. In particular, we assume a
space of feature vectors F and a mapping f : S → F. We denote by fs the feature vector
associated with s.
An option is deﬁned as a triple o = hI , p, bi where I ⊆ S is the set of states in which the
option can be initiated, p is the internal policy of the option and b : S → [0, 1] is a stochastic
termination condition. In the option work (Sutton, Precup & Singh, 1999), each of these
elements has to be explicitly speciﬁed and ﬁxed in order for an option to be well deﬁned.
Here, we will instead deﬁne options implicitly, using the notion of a recognizer.
A recognizer is deﬁned as a function c : S × A → [0, 1], where c(s, a) indicates to what
extent the recognizer allows action a in state s. An important special case, which we treat in
this paper, is that of binary recognizers. In this case, c is an indicator function, specifying
a subset of actions that are allowed, or recognized, given a particular state. Note that
recognizers do not specify policies; instead, they merely give restrictions on the policies
that are allowed or recognized.
A recognizer c together with a behavior policy b generates a target policy p, where:
p(s, a) = b(s, a)c(s, a)
= b(s, a)c(s, a)
(7)
(cid:229)x b(s, x)c(s, x)
µ (s)
The denominator of this fraction, µ (s) = (cid:229)x b(s, x)c(s, x), is the recognition probability at s,
i.e., the probability that an action will be accepted at s when behavior is generated according
to b. The policy p is only deﬁned at states for which µ (s) > 0. The numerator gives the
probability that action a is produced by the behavior and recognized in s. Note that if the
recognizer accepts all state-action pairs, i.e. c(s, a) = 1, ∀s, a, then p is the same as b.
Since a recognizer and a behavior policy can specify together a target policy, we can use
recognizers as a way to specify policies for options, using (7). An option can only be
initiated at a state for which at least one action is recognized, so µ (s) > 0, ∀s ∈ I . Similarly,
the termination condition of such an option, b, is deﬁned as b(s) = 1 if µ (s) = 0. In other
words, the option must terminate if no actions are recognized at a given state. At all other
states, b can be deﬁned between 0 and 1 as desired.
We will focus on computing the reward model of an option o, which represents the expected
total return. The expected values of different features at the end of the option can be
estimated similarly. The quantity that we want to compute is
Eo{R(s)} = E {r1 + r2 + . . . + rT |s0 = s, p, b}
where s ∈ I , experience is generated according to the policy of the option, p, and T denotes
the random variable representing the time step at which the option terminates according to
b. We assume that linear function approximation is used to represent these values, i.e.
Eo{R(s)} ≈ qT fs

where q is a vector of parameters.

4 Off-policy learning algorithm
In this section we present an adaptation of the off-policy learning algorithm of Precup,
Sutton & Dasgupta (2001) to the case of learning about options. Suppose that an option’s
policy p was used to generate behavior. In this case, learning the reward model of the
option is a special case of temporal-difference learning of value functions. The forward
view of this algorithm is as follows. Let ¯R(n)
denote the truncated n-step return starting at
t
time step t and let yt denote the 0-step truncated return, ¯R(0)
. By the de ﬁnition of the n-step
t
truncated return, we have:
t = rt+1 + (1 − bt+1 ) ¯R(n−1)
¯R(n)
.
t+1
This is similar to the case of value functions, but it accounts for the possibility of terminat-
ing the option at time step t + 1. The l-return is deﬁned in the usual way:
¥(cid:229)
ln−1 ¯R(n)
t = (1 − l)
l
¯R
.
t
n=1
h ¯R
i (cid:209)q yt (1 − b1 ) · · · (1 − bt ).
The parameters of the linear function approximator are updated on every time step propor-
tionally to:
t − yt
l
D ¯qt =
In our case, however, trajectories are generated according to the behavior policy b. The
main idea of the algorithm is to use importance sampling corrections in order to account
for the difference in the state distribution of the two policies.
Let rt = p(st ,at )
b(st ,at ) be the importance sampling ratio at time step t . The truncated n-step return,
R(n)
, satisﬁes:
t
t = rt [rt+1 + (1 − bt+1 )R(n−1)
R(n)
].
h
i (cid:209)q yt r0 (1 − b1 ) · · · rt−1 (1 − bt ).
t+1
The update to the parameter vector is proportional to:
t − yt
l
Dqt =
R
The following result shows that the expected updates of the on-policy and off-policy algo-
rithms are the same.
Theorem 3 For every time step t ≥ 0 and any initial state s,
Eb [Dqt |s] = Ep [D ¯qt |s].
Proof: First we will show by induction that Eb{R(n)
|s}, ∀n (which implies
|s} = Ep{ ¯R(n)
t
t
t |s}). For n = 0, the statement is trivial. Assuming that it is true for
that Eb{Rl
t |s} = Ep ( ¯Rl
|s0oi
n
h
n
o
n − 1, we have
R(n−1)
b(s, a)(cid:229)
= (cid:229)
ss0 + (1 − b(s0 ))Eb
|s
ss0 r(s, a)
R(n)
n ¯R(n−1)
|s0oi
h
ra
Pa
Eb
t+1
t
s0
a
p(s, a)
(cid:229)
= (cid:229)
ss0 + (1 − b(s0 ))Ep
o
n ¯R(n)
|s0oi
n ¯R(n−1)
h
Pa
ra
ss0 b(s, a)
t+1
b(s, a)
s0
a
p(s, a)(cid:229)
= (cid:229)
ss0 + (1 − b(s0 ))Ep
|s
ra
Pa
= Ep
ss0
t+1
t
s0
a
Now we are ready to prove the theorem’s main statement. De ﬁning Wt to be the set of all
t − yt )(cid:209)q yt |wo t−1(cid:213)
n
trajectory components up to state st , we have:
Eb {Dqt |s} = (cid:229)
Pb (w|s)Eb
l
(R
w∈Wt
i=0

ri (1 − bi+1 )

.

 t−1(cid:213)
! h
i (cid:209)q yt
o − yt
n
t−1(cid:213)
pi
= (cid:229)
t |st
(1 − bi+1 )
 t−1(cid:213)
! h
l
biPai
Eb
R
n ¯R
i (cid:209)q yt (1 − b1 )...(1 − bt )
o − yt
si si+1
bi
w∈Wt
i=0
i=0
= (cid:229)
t |st
n
t − yt )(cid:209)q yt |wo
l
piPai
(1 − b1 )...(1 − bt ) = Ep (cid:8)D ¯qt |s(cid:9) .
Ep
si si+1
w∈Wt
i=0
= (cid:229)
Pp (w|s)Ep
l
( ¯R
w∈Wt
Note that we are able to use st and w interchangeably because of the Markov property. (cid:5)
Since we have shown that Eb [Dqt |s] = Ep [D ¯qt |s] for any state s, it follows that the expected
updates will also be equal for any distribution of the initial state s. When learning the model
of options with data generated from the behavior policy b, the starting state distribution with
respect to which the learning is performed, I0 is determined by the stationary distribution
of the behavior policy, as well as the initiation set of the option I . We note also that the
importance sampling corrections only have to be performed for the trajectory since the
initiation of the updates for the option. No corrections are required for the experience prior
to this point. This should generate updates that have signiﬁcantly lower variance than in
the case of learning values of policies (Precup, Sutton & Dasgupta, 2001).
Because of the termination condition of the option, b, Dq can quickly decay to zero. To
avoid this problem, we can use a restart function g : S → [0, 1], such that g(st ) speciﬁes
the extent to which the updating episode is considered to start at time t . Adding restarts
generates a new forward update:
t − yt )(cid:209)q yt
l
Dqt = (R

t(cid:229)
i=0
where Rl
is the same as above. With an adaptation of the proof in Precup, Sutton &
t
Dasgupta (2001), we can show that we get the same expected value of updates by applying
this algorithm from the original starting distribution as we would by applying the algorithm
without restarts from a starting distribution de ﬁned by I0 and g. We can turn this forward
algorithm into an incremental, backward view algorithm in the following way:
• Initialize k0 = g0 , e0 = k0(cid:209)q y0
• At every time step t :

dt = rt (rt+1 + (1 − bt+1 )yt+1 ) − yt
qt+1 = qt + adt et
kt+1 = rt kt (1 − bt+1 ) + gt+1
et+1 = lrt (1 − bt+1 )et + kt+1(cid:209)q yt+1
Using a similar technique to that of Precup, Sutton & Dasgupta (2001) and Sutton & Barto
(1998), we can prove that the forward and backward algorithm are equivalent (omitted due
to lack of space). This algorithm is guaranteed to converge if the variance of the updates is
ﬁnite (Precup, Sutton & Dasgupta, 2001). In the case of options, the termination condition
b can be used to ensure that this is the case.
5 Learning when the behavior policy is unknown
In this section, we consider the case in which the behavior policy is unknown. This case
is generally problematic for importance sampling algorithms, but the use of recognizers
will allow us to deﬁne importance sampling corrections, as well as a convergent algorithm.
Recall that when using a recognizer, the target policy of the option is deﬁned as:
p(s, a) = c(s, a)b(s, a)
µ (s)

giri ...rt−1 (1 − bi+1 )...(1 − bt ),

(8)

and the recognition probability becomes:
p(s, a)
= c(s, a)
r(s, a) =
µ (s)
b(s, a)
Of course, µ (s) depends on b. If b is unknown, instead of µ (s), we will use a maximum like-
: S → [0, 1]. The structure used to compute ˆ
lihood estimate ˆ
µ will have to be compatible
with the feature space used to represent the reward model. We will make this more precise
below. Likewise, the recognizer c(s, a) will have to be de ﬁned in terms of the features used
to represent the model. We will then de ﬁne the importance sampling corrections as:
ˆr(s, a) = c(s, a)
µ (s)

We consider the case in which the function approximator used to model the option is actu-
ally a state aggregator. In this case, we will de ﬁne recognizers which behave consistently in
each partition, i.e., c(s, a) = c( p, a), ∀s ∈ p. This means that an action is either recognized
or not recognized in all states of the partition. The recognition probability ˆ
µ will have one
entry for every partition p of the state space. Its value will be:
µ ( p) = N ( p, c = 1)
N ( p)
where N ( p) is the number of times partition p was visited, and N ( p, c = 1) is the num-
ber of times the action taken in p was recognized.
In the limit, w.p.1,
converges to
(cid:229)s d b (s| p) (cid:229)a c( p, a)b(s, a) where d b (s| p) is the probability of visiting state s from parti-
tion p under the stationary distribution of b. At this limit, ˆp(s, a) = ˆr(s, a)b(s, a) will be a
well-deﬁned policy (i.e., (cid:229)a ˆp(s, a) = 1). Using Theorem 3, off-policy updates using im-
portance sampling corrections ˆr will have the same expected value as on-policy updates
using ˆp. Note though that the learning algorithm never uses ˆp; the only quantities needed
are ˆr, which are learned incrementally from data.
For the case of general linear function approximation, we conjecture that a similar idea
can be used, where the recognition probability is learned using logistic regression. The
development of this part is left for future work.
Acknowledgements

The authors gratefully acknowledge the ideas and encouragement they have received in this
work from Eddie Rafols, Mark Ring, Lihong Li and other members of the rlai.net group.
We thank Csaba Szepesvari and the reviewers of the paper for constructive comments. This
research was supported in part by iCore, NSERC, Alberta Ingenuity, and CFI.
References

Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In
Proceedings of ICML.

Precup, D., Sutton, R. S. and Dasgupta, S. (2001). Off-policy temporal-difference learning with
function approximation. In Proceedings of ICML.

Sutton, R.S., Precup D. and Singh, S (1999). Between MDPs and semi-MDPs: A framework for
temporal abstraction in reinforcement learning. Artiﬁcial Intelligence , vol . 112, pp. 181–211.

Sutton,, R.S. and Tanner, B. (2005). Temporal-difference networks. In Proceedings of NIPS-17.

Sutton R.S., Raffols E. and Koop, A. (2006). Temporal abstraction in temporal-difference networks”.
In Proceedings of NIPS-18.

Tadic, V. (2001). On the convergence of temporal-difference learning with linear function approxi-
mation. In Machine learning vol. 42, pp. 241-267.

Tsitsiklis, J. N., and Van Roy, B. (1997). An analysis of temporal-difference learning with function
approximation. IEEE Transactions on Automatic Control 42:674–690.

µ
ˆ
ˆ
ˆ
µ

Generalization error bounds for classi ﬁers
trained with interdependent data

Nicolas Usunier, Massih-Reza Amini, Patrick Gallinari
Department of Computer Science, University of Paris VI
8, rue du Capitaine Scott, 75015 Paris France
{usunier, amini, gallinari}@poleia.lip6.fr

Abstract

In this paper we propose a general framework to study the generalization
properties of binary classiﬁers trained with data which may be depen-
dent, but are deterministically generated upon a sample of independent
examples. It provides generalization bounds for binary classiﬁcation and
some cases of ranking problems, and clariﬁes the relationship between
these learning tasks.

1

Introduction

Many machine learning (ML) applications deal with the problem of bipartite ranking where
the goal is to ﬁnd a function which orders relevant elements over irrelevant ones. Such
problems appear for example in Information Retrieval, where the system returns a list of
documents, ordered by relevancy to the user’s demand. The criterion widely used to mea-
sure the ranking quality is the Area Under the ROC Curve (AUC) [6]. Given a training set
p=1 with yp ∈ {±1}, its optimization over a class of real valued functions G
S = ((xp , yp ))n
(cid:2) ) = sign(g(x) − g(x
(cid:2) )), g ∈ G
can be carried out by ﬁnding a classiﬁer of the form cg (x, x
, −1) in S [6]. More
(cid:2)
which minimizes the error rate over pairs of examples (x, 1) and (x
generally, it is well-known that the learning of scoring functions can be expressed as a
classiﬁcation task over pairs of examples [7, 5].

The study of the generalization properties of ranking problems is a challenging task, since
the pairs of examples violate the central i.i.d. assumption of binary classiﬁcation. Using
task-speciﬁc studies, this issue has recently been the focus of a large amount of work. [2]
showed that SVM-like algorithms optimizing the AUC have good generalization guaran-
tees, and [11] showed that maximizing the margin of the pairs, deﬁned by the quantity
g(x) − g(x
(cid:2) ), leads to the minimization of the generalization error. While these results
suggest some similarity between the classiﬁcation of the pairs of examples and the clas-
siﬁcation of independent data, no common framework has been established. As a major
drawback, it is not possible to directly deduce results for ranking from those obtained in
classiﬁcation.

In this paper, we present a new framework to study the generalization properties of clas-
siﬁers over data which can exhibit a suitable dependency structure. Among others, the
problems of binary classiﬁcation, bipartite ranking, and the ranking risk deﬁned in [5] are
special cases of our study. It shows that it is possible to infer generalization bounds for clas-

siﬁers trained over interdependent examples using generalization results known for binary
classiﬁcation. We illustrate this property by proving a new margin-based, data-dependent
bound for SVM-like algorithms optimizing the AUC. This bound derives straightforwardly
from the same kind of bounds for SVMs for classi ﬁcation given in [12]. Since learning al-
gorithms aim at minimizing the generalization error of their chosen hypothesis, our results
suggest that the design of bipartite ranking algorithms can follow the design of standard
classiﬁcation learning systems.

The remainder of this paper is as follows. In section 2, we give the formal deﬁnition of
our framework and detail the progression of our analysis over the paper. In section 3, we
present a new concentration inequality which allows to extend the notion of Rademacher
complexity (section 4), and, in section 5, we prove generalization bounds for binary clas-
siﬁcation and bipartite ranking tasks under our framework. Finally, the missing proofs are
given in a longer version of the paper [13].

2 Formal framework

We distinguish between the input and the training data. The input data S = (sp )n
p=1 is
a set of n independent examples, while the training data Z = (zi )N
i=1 is composed of
N binary classiﬁed elements where each zi is in Xtr × {−1, +1}, with Xtr the space
of characteristics. For example, in the general case of bipartite ranking, the input data
is the set of elements to be ordered, while the training data is constituted by the pairs of
examples to be classiﬁed. The purpose of this work is the study of generalization properties
of classiﬁers trained using a possibly dependent
training data, but in the special case where
(cid:2)
the latter is deterministically generated from the input data. The aim here is to select a
hypothesis h ∈ H = {hθ : Xtr → {−1, 1}|θ ∈ Θ} which optimizes the empirical risk
i=1 (cid:3)(h, zi ), (cid:3) being the instantaneous loss of h, over the training set Z .
L(h, Z ) = 1
N
N
Deﬁnition 1 (Classiﬁers trained with interdependent data) . A classiﬁcation algorithm over
interdependent training data takes as input data a set S = (sp )n
p=1 supposed to be drawn
according to an unknown product distribution ⊗n
p=1Dp over a product sample space S n 1 ,
outputs a binary classiﬁer chosen in a hypothesis space H : {h : Xtr → {+1, −1}}, and
has a two-step learning process. In a ﬁrst step, the learner applies to its input data S a
ﬁxed function ϕ : S n → (Xtr × {−1, 1})N to generate a vector Z = (zi )N
i=1 = ϕ(S ) of
N training examples zi ∈ Xtr × {−1, 1}, i = 1, ..., N . In the second step, the learner runs
a classiﬁcation algorithm in order to obtain h which minimizes the empirical classi ﬁcation
loss L(h, Z ), over its training data Z = ϕ(S ).
Examples Using the notations above, when S = Xtr × {±1}, n = N , ϕ is the identity
function and S is drawn i.i.d. according to an unknown distribution D , we recover the
classical deﬁnition of a binary classiﬁcation algorithm. Another example is the ranking task
described in [5] where S = X ×R, Xtr = X 2 , N = n(n− 1) and, given S = ((xp , yp ))n
p=1
drawn i.i.d. according to a ﬁxed D , ϕ generates all the pairs ((xk , xl ), sign( yk−yl
)), k (cid:5)= l.
2
In the remaining of the paper, we will prove generalization error bounds of the selected
hypothesis by upper bounding
h∈H L(h) − L(h, ϕ(S ))
sup
with high conﬁdence over S , where L(h) = ES L(h, ϕ(S )). To this end we decompose
Z = ϕ(S ) using the dependency graph of the random variables composing Z with a tech-
nique similar to the one proposed by [8]. We go towards this result by ﬁrst bounding

(1)

1 It is equivalent to say that the input data is a vector of independent, but not necessarilly identically
distributed random variables.

(cid:4)
(cid:3)
(cid:2)
(cid:2)
i=1 q(ϕ( ˜S )i ) − 1
i=1 q(ϕ(S )i )
sup
1
N
N
q∈Q
with high con ﬁdence over samples
E ˜S
N
N
S , where ˜S is also drawn according to ⊗n
p=1Dp , Q is a class of functions taking values
in [0, 1], and ϕ(S )i denotes the i-th training example (Theorem 4). This bound uses an
extension of the Rademacher complexity [3], the fractional Rademacher complexity (FRC)
(deﬁnition 3), which is a weighted sum of Rademacher complexities over independent sub-
sets of the training data. We show that the FRC of an arbitrary class of real-valued functions
can be trivially computed given the Rademacher complexity of this class of functions and
ϕ (theorem 6). This theorem shows that generalization error bounds for classes of classi-
ﬁers over interdependent data (in the sense of deﬁnition 1) trivially follows from the same
kind of bounds for the same class of classiﬁers trained over i.i.d. data. Finally, we show
an example of the derivation of a margin-based, data-dependent generalization error bound
(i.e. a bound on equation (1) which can be computed on the training data) for the bipartite
ranking case when H = {(x, x
(cid:2) ) (cid:6)→ sign(K (θ , x) − K (θ , x
(cid:2) ))|K (θ , θ) ≤ B 2}, assuming
that the input examples are drawn i.i.d. according to a distribution D over X × {±1},
X ⊂ Rd and K is a kernel over X 2 .

Notations Throughout the paper, we will use the notations of the preceding subsection,
i=1 , which will denote an arbitrary element of (Xtr × {−1, 1})N . In
except for Z = (zi )N
order to obtain the dependency graph of the random variables ϕ(S )i , we will consider, for
each 1 ≤ i ≤ N , a set [i] ⊂ {1, ..., n} such that ϕ(S )i depends only on the variables sp ∈ S
for which p ∈ [i]. Using these notations, if we consider two indices k , l in {1, ..., N }, we
can notice that the two random variables ϕ(S )k and ϕ(S )l are independent if and only if
[k ] ∩ [l] = ∅. The dependency graph of the ϕ(S )i s follows, by constructing the graph Γ(ϕ),
with the set of vertices V = {1, ..., N }, and with an edge between k and l if and only if
[k ] ∩ [l] (cid:5)= ∅. The following deﬁnitions, taken from [8], will enable us to separate the set of
partly dependent variables into sets of independent variables:
• A subset A of V is independent if all the elements in A are independent.
(cid:5)
• A sequence C = (Cj )m
j=1 of subsets of V is a proper cover of V if, for all j , Cj is
j Cj = V
independent, and
(cid:2)
• A sequence C = (Cj , wj )m
j=1 is a proper, exact fractional cover of Γ if wj > 0
for all j , and, for each i ∈ V ,
(i) = 1, where ICj is the indicator
m
j=1 wj ICj
(cid:2)
function of Cj .
• The fractional chromatic number of Γ, noted χ(Γ), is equal to the minimum of
j wj over all proper, exact fractional cover.
It is to be noted that from lemma 3.2 of [8], the existence of proper, exact fractional covers
(cid:2)
is ensured. Since Γ is fully determined by the function ϕ, we will note χ(Γ) = χ(ϕ).
Moreover, we will denote by C (ϕ) = (Cj , wj )κ
j=1 a proper, exact fractional cover of Γ
j wj = χ(ϕ). Finally, for a given C (ϕ), we denote by κj the number of
such that
elements in Cj , and we ﬁx the notations: Cj = {Cj 1 , ..., Cjκj
}. It is to be noted that if
i=1 ∈ RN , and C (ϕ) = (Cj , wj )κ
(ti )N
N(cid:6)
κ(cid:6)
κj(cid:6)
j=1 , lemma 3.1 of [8] states that:
j=1
i=1
k=1

wj Tj , where Tj =

ti =

tCj k

(2)

3 A new concentration inequality

Concentration inequalities bound the probability that a random variable deviates too much
from its expectation (see [4] for a survey). They play a major role in learning theory as

they can be used for example to bound the probability of deviation of the expected loss of a
function from its empirical value estimated over a sample set. A well-known inequality is
McDiarmid’s theorem [9] for independent random variables, which bounds the probability
of deviation from its expectation of an arbitrary function with bounded variations over
each one of its parameters. While this theorem is very general, [8] proved a large deviation
bound for sums of partly random variables where the dependency structure of the variables
is known, which can be tighter in some cases. Since we also consider variables with known
dependency structure, using such results may lead to tighter bounds. However, we will
bound functions like in equation (1), which do not write as a sum of partly dependent
variables. Thus, we need a result on more general functions than sums of random variables,
but which also takes into account the known dependency structure of the variables.
Theorem 2. Let ϕ : X n → X (cid:2)N . Using the notations deﬁned above, let C (ϕ) =
j=1 . Let f : X (cid:2)N → R such that:
(Cj , wj )κ
(cid:2)
1. There exist κ functions fj : X (cid:2) κj → R which satisfy ∀Z = (z1 , ..., zN ) ∈ X (cid:2)N ,
f (Z ) =
j wj fj (zCj1 , ..., zCjκj
).
∈ X (cid:2) κj such that Zj and Z k
2. There exist β1 , ..., βN ∈ R+ such that ∀j, ∀Zj , Z k
)| ≤ βCjk .
differ only in the k-th dimension, |fj (Zj ) − fj (Z k
j
j
j
Let ﬁnally D1 , ..., Dn be n probability distributions over X . Then, we have:
(cid:2)
22
(f ◦ ϕ(X ) − Ef ◦ ϕ > ) ≤ exp(−
PX∼⊗n
i=1Di
N
i=1 β 2
i
and the same holds for P(Ef ◦ ϕ − f ◦ ϕ > ).
The proof of this theorem (given in [13]) is a variation of the demonstrations in [8] and
McDiarmid’s theorem. The main idea of this theorem is to allow the decomposition of
f , which will take as input partly dependent random variables when applied to ϕ(S ), into
a sum of functions which, when considering f ◦ ϕ(S ), will be functions of independent
(cid:2)
variables. As we will see, this theorem will be the major tool in our analysis. It is to be
noted that when X = X (cid:2)
, N = n and ϕ is the identity function of X n , the theorem 2 is
i=1 qi (zi ) with
N
exactly McDiarmid’s theorem. On the other hand, when f takes the form
for all z ∈ X (cid:2)
, a ≤ qi (z ) ≤ a + βi with a ∈ R, then theorem 2 reduces to a particular case
of the large deviation bound of [8].

χ(ϕ)

)

(3)

4 The fractional Rademacher complexity
i=1 ∈ Z N . If Z is supposed to be drawn i.i.d. according to a distribution DZ
(cid:2)
Let Z = (zi )N
over Z , for a class F of functions from Z to R, the Rademacher complexity of F is deﬁned
by [10] RN (F ) = EZ∼DZ RN (F , Z ), where RN (F , Z ) = Eσ sup
i=1 σi f (zi ) is
N
f ∈F
the empirical Rademacher complexity of F on Z , and σ = (σi )n
i=1 is a sequence of in-
dependent Rademacher variables, i.e. ∀i, P(σi = 1) = P(σi = −1) = 1
2 . This quan-
tity has been extensively used to measure the complexity of function classes in previous
bounds for binary classiﬁcation [3, 10]. In particular, if we consider a class of functions
Q = {q : Z (cid:6)→ [0, 1]}, it can be shown (theorem 4.9 in [12]) that with probability at least
1 − δ over Z , all q ∈ Q verify the following inequality, which serves as a preliminary result
(cid:7)
N(cid:6)
to show data-dependent bounds for SVMs in [12]:
EZ∼DZ q(z ) ≤ 1
q(zi ) + RN (Q) +
N
i=1

ln(1/δ)
2N

(4)

R

(cid:2)
In this section, we generalize equation (4) to our case with theorem 4, using the following
i=1 q(ϕ(S )i ) and λ(q) = ES λ(q , ϕ(S ))):
deﬁnition 2 (we denote λ(q , ϕ(S )) = 1
N
N
Deﬁnition 3. Let Q, be class of functions from a set Z to R, Let ϕ : X n → Z N and S
a sample of size n drawn according to a product distribution ⊗n
p=1Dp over X n . Then, we
deﬁne the empirical fractional Rademacher complexity 3 of Q given ϕ as:
(cid:6)
(cid:6)
2
(Q, S, ϕ) =
∗
σi q(ϕ(S )i )
wj sup
Eσ
q∈Q
n
N
i∈Cj
j
As well as the fractional Rademacher complexity of Q as R
(Q, ϕ) = ES R
(Q, S, ϕ)
∗
∗
(cid:8)
n
n
Theorem 4. Let Q be a class of functions from Z to [0, 1]. Then, with probability at least
1 − δ over the samples S drawn according to
p=1 Dp , for all q ∈ Q:
(cid:7)
n
N(cid:6)
χ(ϕ) ln(1/δ)
λ(q) − 1
(Q, ϕ) +
q(ϕi (S )) ≤ R
∗
(cid:7)
2N
N(cid:6)
n
N
i=1
(Q, S, ϕ) + 3
q(ϕi (S )) ≤ R
i=1

And: λ(q) − 1
N

χ(ϕ) ln(2/δ)
2N

∗
n

In the deﬁnition of the fractional Rademacher complexity (FRC), if ϕ is the identity func-
tion, we recover the standard Rademacher complexity, and theorem 4 reduces to equation
(4). These results are therefore extensions of equation (4), and show that the generalization
error bounds for the tasks falling in our framework will follow from a unique approach.
(cid:10)
(cid:9)
Proof. In order to ﬁnd a bound for all q in Q of λ(q) − λ(q , ϕ(S )), we write:
N(cid:6)
N(cid:6)
1
q(ϕ( ˜S )i ) − 1
λ(q) − λ(q , ϕ(S )) ≤ sup
⎡
⎤
q(ϕ(S )i )
E ˜S
q∈Q
(cid:6)
(cid:6)
(cid:6)
N
N
i=1
i=1
⎣E ˜S
⎦
≤ 1
q(ϕ( ˜S )i ) −
wj sup
q(ϕ(S )i )
q∈Q
N
i∈Cj
i∈Cj
j
(cid:2)
k=1 q(ϕ( ˜S )Cj k ) − (cid:2)
Where we have used equation (2). Now, consider, for each j , fj : Z κj → R such that, for
(cid:2)
all z (j ) ∈ Z κj , fj (z (j ) ) = 1
(j )
sup
k=1 q(z
). It is clear
κj
κj
q∈Q E ˜S
that if f : Z N → R is deﬁned by: for all Z ∈ Z N , f (Z ) =
k
N
),
j=1 wj fj (zC j 1 , ..., zC jκj
N
then the right side of equation (5) is equal to f ◦ ϕ(S ), and that f satis ﬁes all the conditions
of theorem 2 with, for all i ∈ {1, ..., N }, βi = 1
(cid:8)
(cid:2)
N . Therefore, with a direct application
of theorem 2, we can claim that, with probability at least 1 − δ over samples S drawn
p=1 Dp (we denote λj (q , ϕ(S )) = 1
(cid:7)
i∈Cj q(ϕ(S )i )):
(cid:3)
(cid:4)
n
(cid:6)
according to
N
E ˜S λj (q , ϕ( ˜S )) − λj (q , ϕ(S ))
λ(q) − λ(q , ϕ(S )) ≤ ES
+
wj sup
(cid:7)
(cid:6)
(cid:6)
q∈Q
j
i∈Cj

[q(ϕ( ˜S )i ) − q(ϕ(S )i )] +

χ(ϕ) ln(1/δ)
2N

χ(ϕ) ln(1/δ)
2N

≤ E
S, ˜S

(5)

wj
N

sup
q∈Q

j

(6)
2The fractional Rademacher complexity depends on the cover C (ϕ) chosen, since it is not unique.
However in practice, our bounds only depend on χ(ϕ) (see section 4.1).
3 this denomination stands as it is a sum of Rademacher averages over independent parts of ϕ(S ).

(7)

E
S, ˜S

σi [q(ϕ( ˜S )i ) − q(ϕ(S )i )]

Now ﬁx j , and consider σ = (σi )N
i=1 , a sequence of N independent Rademacher variables.
(cid:6)
(cid:6)
For a given realization of σ , we have that
[q(ϕ( ˜S )i ) − q(ϕ(S )i )] = E
sup
sup
S, ˜S
q∈Q
q∈Q
i∈Cj
i∈Cj
because, for each σi considered, σi = −1 simply corresponds to permutating, in S, ˜S , the
two sequences S[i] and ˜S[i] (where S[i] denotes the subset of S ϕ(S )i really depends on)
which have the same distribution (even though the sp ’s are not identically distributed), and
are independent from the other S[k] and ˜S[k] since we are considering i, k ∈ Cj . Therefore,
taking the expection over S, ˜S is the same with the elements permuted this way as if they
were not permuted. Then, from equation (6), the ﬁrst inequality of the theorem follow. The
(Q, S, ϕ).
∗
second inequality is due to an application of theorem 2 to R
n
Remark 5. The symmetrization performed in equation (7) requires the variables ϕ(S )i
appearing in the same sum to be independent. Thus, the generalization of Rademacher
complexities could only be performed using a decomposition in independent sets, and the
cover C assures some optimality of the decomposition. Moreover, even though McDi-
armid’s theorem could be applied each time we used theorem 2, the derivation of the real
numbers bounding the differences is not straightforward, and may not lead to the same
result. The creation of the dependency graph of ϕ and theorem 2 are therefore necessary
tools for obtaining theorem 4.

Properties of the fractional Rademacher complexity
Theorem 6. Let Q be a class of functions from a set Z to R, and ϕ : X n → Z N . For
S ∈ X n , the following results are true.
(φ ◦ Q, S, ϕ) ≤ LR
(Q, S, ϕ)
1. Let φ : R → R, an L-Lipschitz function. Then R
∗
∗
(cid:15)
n
n
2. If there exist M > 0 such that for every k , and samples Sk of size k Rk (Q, Sk ) ≤
(Q, S, ϕ) ≤ M
∗
(cid:16)
M√
χ(ϕ)
, then R
n
N
k
3. Let K be a kernel over Z , B > 0, denote ||x||K =
K (x, x) and deﬁne HK,B =
(cid:17)(cid:18)(cid:18)(cid:19) N(cid:6)
{hθ : Z → R, hθ (x) = K (θ , x)|||θ ||K ≤ B }. Then:
(cid:16)
χ(ϕ)
N
i=1

(HK,B , S, ϕ) ≤ 2B

||ϕ(S )i ||2
K

∗
n

R

The ﬁrst point of this theorem is a direct consequence of a Rademacher process comparison
theorem, namely theorem 7 of [10], and will enable the obtention of margin-based bounds.
The second and third points show that the results regarding the Rademacher complexity
can be used to immediately deduce bounds on the FRC. This result, as well as theorem 4
show that binary classiﬁers of i.i.d. data and classiﬁers of interdependent data will have
generalization bounds of the same form, but with different convergence rate depending on
the dependency structure imposed by ϕ.
(cid:2)
(cid:2)
elements of proof. The second point results from Jensen ’s inequality, using the facts that
j wj |Cj | = N . The third point is based
j wj = χ(ϕ) and, from equation (2),
(cid:15)(cid:2)
[3]), if Sk = ((xp , yp ))k
p=1 , then
on the same calculations by noting that (see e.g.
Rk (HK,B , Sk ) ≤ 2B
p=1 ||xp ||2
k
K .
k

5 Data-dependent bounds

i

))

The fact that classiﬁers trained on interdependent data will ”inherit” the generalization
bound of the same classiﬁer trained on i.i.d. data suggests simple ways of obtaining bi-
partite ranking algorithms. Indeed, suppose we want to learn a linear ranking function, for
example a function h ∈ HK,B as deﬁned in theorem 6, where K is a linear kernel, and
consider a sample S ∈ (X × {−1, 1})n with X ⊂ Rd , drawn i.i.d. according to some D .
, −1) in S , h(x) − h(x
(cid:2) ) = h(x − x
(cid:2)
(cid:2) ).
Then we have, for input examples (x, 1) and (x
Therefore, we can learn a bipartite ranking function by applying an SVM algorithm to the
, −1)) in S , each pair being represented by x − x
(cid:2)
(cid:2)
pairs ((x, 1), (x
, and our framework
allows to immediately obtain generalization bounds for this learning process based on the
generalization bounds for SVMs. We show these bounds in theorem 7.
the 1-Lipschitz function deﬁned by φ(x) =
To derive the bounds, we consider φ,
min(1, max(1 − x, 0)) ≥ [[x ≤ 0]]4 . Given a training example z , we denote by
(cid:2)
(cid:8)
z l its label and z f its feature representation. With an abuse of notation, we denote
p=1 Dp , we have,
φ(h, Z ) = 1
i h(z f
i=1 φ(z l
)). For a sample S drawn according to
N
n
(cid:6)
(cid:6)
for all h in some function class H:
i
N
1
1
(cid:3)(h, zi ) ≤ ES
(cid:7)
i h(z f
φ(z l
ES
(cid:6)
(cid:6)
i
N
N
i
χ(ϕ) ln(2/δ)
2wj
≤ φ(h, Z ) + Eσ
sup
)) + 3
σiφ(z l
i h(z f
2N
h∈H
i
N
i∈Cj
j
) ≤ 0]], and the last inequality holds with probability at least
where (cid:3)(h, zi ) = [[z l
i h(z f
1 − δ over samples S from theorem 4. Notice that when σCjk is a Rademacher variable, it
i
∈ {−1, 1}. Thus, using the ﬁrst result of
Cjk σCjk since z l
has the same distribution as z l
(cid:7)
theorem 6 we have that with probability 1 − δ over the samples S , all h in H satisfy:
Cjk
(cid:6)
(cid:6)
χ(ϕ) ln(2/δ)
1
(cid:3)(h, zi ) ≤ 1
(H, S, ϕ) + 3
∗
φ(z lh(z f )) + R
(8)
ES
2N
n
N
N
i
i
Now putting in equation (8) the third point of theorem 6, with H = HK,B as deﬁned in
theorem 6 with Z = X , we obtain the following theorem:
Theorem 7. Let S ∈ (X × {−1, 1})n be a sample of size n drawn i.i.d. according to an
(cid:17)(cid:18)(cid:18)(cid:19) n(cid:6)
unknown distribution D . Then, with probability at least 1 − δ , all h ∈ HK,B verify:
(cid:7)
n(cid:6)
ln(2/δ)
2B
ES [[yih(xi ) ≤ 0]] ≤ 1
||xi ||2
+ 3
2n
K
n
n
+(cid:6)
nS−(cid:6)
i=1
i=1
nS
(cid:2) = −1} ≤ 1
(cid:2) )]]|y = 1, y
And E{[[h(x) ≤ h(x
(cid:17)(cid:18)(cid:18)(cid:18)(cid:19) nS
(cid:15)
+nS−
nS
+(cid:6)
nS−(cid:6)
j=1
i=1
max(nS
+ , nS− )
+nS−
nS
j=1
i=1
+ , nS− are the number of positive and negative instances in S , and σ and ν also
Where nS
depend on S , and are such that xσ(i) is the i-th positive instance in S and ν (j ) the j -th
negative instance.

φ(h(xσ(i) ) − h(xν (j ) ))
(cid:20)
+ 3

ln(2/δ)
2 min(nS
+ , nS− )

φ(yih(xi )) +

2B

+

||xσ(i) − xν (j ) ||2
K

4 remark that φ is upper bounded by the slack variables of the SVM optimization problem (see
e.g. [12]).

It is to be noted that when h ∈ HK,B with a non linear kernel, the same bounds apply, with,
for the case of bipartite ranking, ||xσ(i) − xν (j ) ||2
−
+ ||xν (j ) ||2
K replaced by ||xσ(i) ||2
K
K
2K (xσ(i) , xν (j ) ).
For binary classiﬁcation, we recover the bounds of [12], since our framework is a gen-
eralization of their approach. As expected, the bounds suggest that kernel machines will
generalize well for bipartite ranking. Thus, we recover the results of [2] obtained in a spe-
ciﬁc framework of algorithmic stability. However, our bound suggests that the convergence
rate is controlled by 1/ min(nS
+ , nS− ), while their results suggested 1/nS
+ + 1/nS− . The full
proof, in which we follow the approach of [1], is given in [13].

6 Conclusion

We have shown a general framework for classi ﬁers trained with interdependent data, and
provided the necessary tools to study their generalization properties. It gives a new insight
on the close relationship between the binary classiﬁcation task and the bipartite ranking,
and allows to prove the ﬁrst data-dependent bounds for this latter case. Moreover, the
framework could also yield comparable bounds on other learning tasks.

Acknowledgments

This work was supported in part by the IST Programme of the European Community, under
the PASCAL Network of Excellence, IST-2002-506778. This publication only reﬂects the
authors views.

References

[1] Agarwal S., Graepel T., Herbrich R., Har-Peled S., Roth D. (2005) Generalization Error Bounds
for the Area Under the ROC curve, Journal of Machine Learning Research.

[2] Agarwal S., Niyogi P. (2005) Stability and generalization of bipartite ranking algorithms, Confer-
ence on Learning Theory 18.

[3] Bartlett P., Mendelson S. (2002) Rademacher and Gaussian Complexities: Risk Bounds and Struc-
tural Results, Journal of Machine Learning Research 3, pp. 463-482.

[4] Boucheron S., Bousquet O., Lugosi G. (2004) Concentration inequalities, in O. Bousquet, U.v.
Luxburg, and G. Rtsch (editors), Advanced Lectures in Machine Learning, Springer, pp. 208-240.

[5] Clemenc¸ on S., Lugosi G., Vayatis N. (2005) Ranking and scoring using empirical risk minimiza-
tion, Conference on Learning Theory 18.

[6] Cortes C., Mohri M. (2004) AUC optimization vs error rate miniminzation NIPS 2003,

[7] Freund Y., Iyer R.D., Schapire R.E., Singer Y. (2003) An Efﬁcient Boosting Algorithm for Com-
bining Preferences, Journal of Machine Learning Research 4, pp. 933-969.

[8] Janson S. (2004) Large deviations for sums of partly dependent random variables, Random Struc-
tures and Algorithms 24, pp. 234-248.

[9] McDiarmid C. (1989) On the method of bounded differences, Surveys in Combinatorics.

[10] Meir R., Zhang T. (2003) Generalization Error Bounds for Bayesian Mixture Algorithms, Jour-
nal of Machine Learning Research 4, pp. 839-860.

[11] Rudin C., Cortes C., Mohri M., Schapire R.E. (2005) Margin-Based Ranking meets Boosting in
the middle, Conference on Learning Theory 18.

[12] Shawe-Taylor J., Cristianini N. (2004) Kernel Methods for Pattern Analysis, Cambridge U. Prs.

[13] Long version of this paper, Available at http://www-connex.lip6.fr/ ˜usunier/nips05-lv.pdf


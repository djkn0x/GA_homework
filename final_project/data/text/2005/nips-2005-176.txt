Consistency of one-class SVM and related
algorithms

R ´egis Vert
Laboratoire de Recherche en Informatique
Universit ´e Paris-Sud
91405, Orsay Cedex, France
Masagroup
24 Bd de l’H ˆopital
75005, Paris, France
Regis.Vert@lri.fr

Jean-Philippe Vert
Geostatistics Center
Ecole des Mines de Paris - ParisTech
77300 Fontainebleau, France
Jean-Philippe.Vert@ensmp.fr

Abstract

We determine the asymptotic limit of the function computed by support
vector machines (SVM) and related algorithms that minimize a regu-
larized empirical convex loss function in the reproducing kernel Hilbert
space of the Gaussian RBF kernel, in the situation where the number of
examples tends to inﬁnity, the bandwidth of the Gaussian ker nel tends
to 0, and the regularization parameter is held ﬁxed. Non-asympt otic con-
vergence bounds to this limit in the L2 sense are provided, together with
upper bounds on the classi ﬁcation error that is shown to conv erge to the
Bayes risk, therefore proving the Bayes-consistency of a variety of meth-
ods although the regularization term does not vanish. These results are
particularly relevant to the one-class SVM, for which the regularization
can not vanish by construction, and which is shown for the ﬁrs t time to
be a consistent density level set estimator.

1

Introduction

Given n i.i.d. copies (X1 , Y1 ), . . . , (Xn , Yn ) of a random variable (X, Y ) ∈ Rd × {−1, 1},
we study in this paper the limit and consistency of learning algorithms that solve the fol-
lowing problem:
f ∈Hσ ( 1
Hσ ) ,
nXi=1
φ (Yi f (Xi )) + λk f k2
arg min
n
where φ : R → R is a convex loss function and Hσ is the reproducing kernel Hilbert space
(RKHS) of the normalized Gaussian radial basis function kernel (denoted simply Gaussian
kernel below):
(cid:0)√2πσ(cid:1)d exp (cid:18) −k x − x′ k2
(cid:19) , σ > 0 .
1
2σ2
This framework encompasses in particular the classical support vector machine (SVM) [1]
when φ(u) = max(1 − u, 0). Recent years have witnessed important theoretical advances

kσ (x, x′ ) =

(1)

(2)

aimed at understanding the behavior of such regularized algorithms when n tends to inﬁnity
and λ decreases to 0. In particular the consistency and convergence rates of the two-class
SVM (see, e.g., [2, 3, 4] and references therein) have been studied in detail, as well as the
shape of the asymptotic decision function [5, 6]. All results published so far study the case
where λ decreases as the number of points tends to inﬁnity (or, equiv alently, where λσ−d
converges to 0 if one uses the classical non-normalized version of the Gaussian kernel in-
stead of (2)). Although it seems natural to reduce regularization as more and more training
data are available –even more than natural, it is the spirit o f regularization [7, 8] –, there
is at least one important situation where λ is typically held ﬁxed: the one-class SVM [9].
In that case, the goal is to estimate an α-quantile, that is, a subset of the input space X of
given probability α with minimum volume. The estimation is performed by thresholding
the function output by the one-class SVM, that is, the SVM (1) with only positive exam-
ples; in that case λ is supposed to determine the quantile level1 . Although it is known that
the fraction of examples in the selected region converges to the desired quantile level α [9],
it is still an open question whether the region converges towards a quantile, that is, a region
of minimum volume. Besides, most theoretical results about the consistency and conver-
gence rates of two-class SVM with vanishing regularization constant do not translate to the
one-class case, as we are precisely in the seldom situation where the SVM is used with a
regularization term that does not vanish as the sample size increases.

The main contribution of this paper is to show that Bayes consistency can be obtained for
algorithms that solve (1) without decreasing λ, if instead the bandwidth σ of the Gaussian
kernel decreases at a suitable rate. We prove upper bounds on the convergence rate of the
classi ﬁcation error towards the Bayes risk for a variety of f unctions φ and of distributions P ,
in particular for SVM (Theorem 6). Moreover, we provide an explicit description of the
function asymptotically output by the algorithms, and establish converge rates towards this
limit for the L2 norm (Theorem 7). In particular, we show that the decision function out-
put by the one-class SVM converges towards the density to be estimated, truncated at the
level 2λ (Theorem 8); we ﬁnally show that this implies the consistenc y of one-class SVM
as a density level estimator for the excess-mass functional [10] (Theorem 9).

Due to lack of space we limit ourselves in this extended abstract to the statement of the main
results (Section 2) and sketch the proof of the main theorem (Theorem 3) that underlies all
other results in Section 3. All detailed proofs are available in the companion paper [11].

2 Notations and main results

Let (X, Y ) be a pair of random variables taking values in Rd × {−1, 1}, with distribu-
tion P . We assume throughout this paper that the marginal distribution of X is absolutely
continuous with respect to Lebesgue measure with density ρ : Rd → R, and that is has
a support included in a compact set X ⊂ Rd . We denote η : Rd → [0, 1] a measurable
version of the conditional distribution of Y = 1 given X .
The normalized Gaussian radial basis function (RBF) kernel kσ with bandwidth parame-
ter σ > 0 is deﬁned for any (x, x′ ) ∈ Rd × Rd by:
(cid:19) ,
(cid:0)√2πσ(cid:1)d exp (cid:18) −k x − x′ k2
1
kσ (x, x′ ) =
2σ2
and the corresponding reproducing kernel Hilbert space (RKHS) is denoted by Hσ . We
note κσ = (cid:0)√2πσ(cid:1)−d the normalizing constant that ensures that the kernel integrates to 1.
1While the original formulation of the one-class SVM involves a parameter ν , there is asymptoti-
cally a one-to-one correspondance between λ and ν

where,

(true) risk of f , when Y is

Denoting by M the set of measurable real-valued functions on Rd , we deﬁne several risks
for functions f ∈ M:
• The classi ﬁcation error rate, usually refered to as
predicted by the sign of f (X ), is denoted by
R (f ) = P (sign (f (X )) 6= Y ) .
• For a scalar λ > 0 ﬁxed throughout this paper and a convex function φ : R → R,
the φ-risk regularized by the RKHS norm is deﬁned, for any σ > 0 and f ∈ Hσ ,
by
Rφ,σ (f ) = EP [φ (Y f (X ))] + λk f k2
Hσ
Furthermore, for any real r ≥ 0, we denote by L (r) the Lipschitz constant of the
restriction of φ to the interval [−r, r]. For example, for the hinge loss φ(u) =
max(0, 1 − u) one can take L(r) = 1, and for the squared hinge loss φ(u) =
max(0, 1 − u)2 one can take L(r) = 2(r + 1).
• Finally, the L2 -norm regularized φ-risk is, for any f ∈ M:
Rφ,0 (f ) = EP [φ (Y f (X ))] + λk f k2
L2
L2 = ZRd
k f k2
f (x)2dx ∈ [0, +∞].
The minima of the three risk functionals deﬁned above over th eir respective domains are
denoted by R∗ , R∗φ,σ and R∗φ,0 respectively. Each of these risks has an empirical counter-
part where the expectation with respect to P is replaced by an average over an i.i.d. sam-
ple T = {(X1 , Y1 ) , . . . , (Xn , Yn )}. In particular, the following empirical version of Rφ,σ
will be used
nXi=1
bRφ,σ (f ) =
The main focus of this paper is the analysis of learning algorithms that minimize the em-
pirical φ-risk regularized by the RKHS norm bRφ,σ , and their limit as the number of points
tends to inﬁnity and the kernel width σ decreases to 0 at a suitable rate when n tends
to ∞, λ being kept ﬁxed. Roughly speaking, our main result shows tha t in this situation,
if φ is a convex loss function, the minimization of bRφ,σ asymptotically amounts to minimiz-
ing Rφ,0 . This stems from the fact that the empirical average term in the deﬁnition of bRφ,σ
converges to its corresponding expectation, while the norm in Hσ of a function f decreases
to its L2 norm when σ decreases to zero. To turn this intuition into a rigorous statement, we
need a few more assumptions about the minimizer of Rφ,0 and about P . First, we observe
that the minimizer of Rφ,0 is indeed well-deﬁned and can often be explicitly computed:
Lemma 1 For any x ∈ Rd , let
α∈R (cid:8)ρ(x) [η(x)φ(α) + (1 − η)φ(−α)] + λα2(cid:9) .
fφ,0 (x) = arg min
Then fφ,0 is measurable and satis ﬁes:
Rφ,0 (f )
Rφ,0 (fφ,0 ) = inf
f ∈M

φ (Yi f (Xi )) + λk f k2
Hσ

∀σ > 0, f ∈ Hσ ,

1
n

.

Second, we provide below a general result that shows how to control the excess Rφ,0 -risk of
the empirical minimizer of the Rφ,σ -risk, for which we need to recall the notion of modulus
of continuity [12].

Deﬁnition 2 (Modulus of continuity) Let f be a Lebesgue measurable function from Rd
to R. Then its modulus of continuity in the L1 -norm is deﬁned for any δ ≥ 0 as follows
ω(f , δ) = sup
0≤k t k≤δ k f (. + t) − f (.) kL1 ,
where k t k is the Euclidian norm of t ∈ Rd .
Our main result can now be stated as follows:

(3)

Theorem 3 (Main Result) Let σ1 > σ > 0, 0 < p < 2, δ > 0, and let ˆfφ,σ denote a
minimizer of the bRφ,σ risk over Hσ . Assume that the marginal density ρ is bounded, and
let M = supx∈Rd ρ(x). Then there exist constants (Ki )i=1...4 (depending only on p, δ , λ, d,
and M ) such that, for any x > 0, the following holds with probability greater than 1 − e−x
over the draw of the training data:
Rφ,0 ( ˆfφ,σ ) − R∗φ,0 ≤ K1L  r κσ φ (0)
λ ! 4
2+p (cid:18) 1
n (cid:19) 2
σ (cid:19) [2+(2−p)(1+δ)]d
(cid:18) 1
2+p
2+p
λ !2 (cid:18) 1
+ K2L  r κσ φ (0)
σ (cid:19)d x
n
σ2
+ K3
σ2
1
+ K4ω(fφ,0 , σ1 ) .

(4)

The ﬁrst two terms in the r.h.s. of (4) bound the estimation er ror associated with the
gaussian RKHS, which naturally tends to be small when the number of training data
increases and when the RKHS is ’small’, i.e., when σ is large. As is usually the case in
such variance/bias splitings, the variance term here depends on the dimension d of the
input space. Note that it is also parametrized by both p and δ . The third term measures
the error due to penalizing the L2 -norm of a ﬁxed function in Hσ1 by its k . kHσ -norm,
with 0 < σ < σ1 . This is a price to pay to get a small estimation error. As for the fourth
term, it is a bound on the approximation error of the Gaussian RKHS. Note that, once λ
and σ have been ﬁxed, σ1 remains a free variable parameterizing the bound itself.

In order to highlight the type of convergence rates one can obtain from Theorem 3, let us
assume that the φ loss function is Lipschitz on R (e.g., take the hinge loss), and suppose
that for some 0 ≤ β ≤ 1, c1 > 0, and for any h ≥ 0, the function fφ,0 satis ﬁes the
following inequality
ω(fφ,0 , h) ≤ c1hβ .
(5)
Then we can optimize the right hand side of (4) w.r.t. σ1 , σ , p and δ by balancing the four
terms. This eventually leads to:
Rφ,0 (cid:16) ˆfφ,σ (cid:17) − R∗φ,0 = OP  (cid:18) 1
4β+(2+β)d −ǫ! ,
n (cid:19) 2β
for any ǫ > 0. This rate is achieved by choosing
n (cid:19)
σ1 = (cid:18) 1
2
4β+(2+β)d − ǫ
β
n (cid:19) 2+β
1 = (cid:18) 1
4β+(2+β)d − ǫ(2+β)
2β
2+β
σ = σ
2

(6)

,

(7)

(8)

,

p = 2 and δ as small as possible (that is why an arbitray small quantity ǫ appears in the
rate).
Theorem 3 shows that minimizing the bRφ,σ risk for well-chosen width σ is a an algorithm
consistant for the Rφ,0 -risk. In order to relate this consistency with more traditional mea-
sures of performance of learning algorithms, the next theorem shows that under a simple
additionnal condition on φ, Rφ,0 -risk-consistency implies Bayes consistency:
Theorem 4 If φ is convex, differentiable at 0, with φ′ (0) < 0, then for every sequence of
functions (fi )i≥1 ∈ M,
lim
i→+∞

Rφ,0 (fi ) = R∗φ,0 =⇒ lim
i→+∞

R (fi ) = R∗

This theorem results from a more general quantitative analysis of the relationship between
the excess Rφ,0 -risk and the excess R-risk, in the spirit of [13]. In order to state a reﬁned
version in the particular case of the support vector machine algorithm, we ﬁrst need the
following deﬁnition:

Deﬁnition 5 We say that a distribution P with ρ as marginal density of X w.r.t. Lebesgue
measure has a low density exponent γ ≥ 0 if there exists (c2 , ǫ0 ) ∈ (0, +∞)2 such that
∀ǫ ∈ [0, ǫ0 ], P (cid:0)(cid:8)x ∈ Rd : ρ(x) ≤ ǫ(cid:9)(cid:1) ≤ c2 ǫγ .
We are now in position to state a quantitative relationship between the excess Rφ,0 -risk and
the excess R-risk in the case of support vector machines:

Theorem 6 Let φ1 (α) = max (1 − α, 0) be the hinge loss function, and φ2 (α) =
max (1 − α, 0)2 , be the squared hinge loss function. Then for any distribution P with
low density exponent γ , there exist constant (K1 , K2 , r1 , r2 ) ∈ (0, +∞)4 such that for
any f ∈ M with an excess Rφ1 ,0 -risk upper bounded by r1 the following holds:
R(f ) − R∗ ≤ K1 (cid:0)Rφ1 ,0 (f ) − R∗φ1 ,0 (cid:1) γ
2γ+1 ,
and if the excess regularized Rφ2 ,0 -risk upper bounded by r2 the following holds:
R(f ) − R∗ ≤ K2 (cid:0)Rφ2 ,0 (f ) − R∗φ2 ,0 (cid:1) γ
2γ+1 ,
This result can be extended to any loss function through the introduction of variational
arguments, in the spirit of [13]; we do not further explore this direction, but the reader is
invited to consult [11] for more details. Hence we have proved the consistency of SVM,
together with upper bounds on the convergence rates, in a situation where the effect of
regularization does not vanish asymptotically.
Another consequence of the Rφ,0 -consistency of an algorithm is the L2 -convergence of the
function output by the algorithm to the minimizer of the Rφ,0 -risk:
Lemma 7 For any f ∈ M, the following holds:
λ (cid:0)Rφ,0 (f ) − R∗φ,0 (cid:1) .
1
k f − fφ,0 k2
L2 ≤
This result is particularly relevant to study algorithms whose objective are not binary clas-
si ﬁcation. Consider for example the one-class SVM algorith m, which served as the initial
motivation for this paper. Then we claim the following:

Theorem 8 Let ρλ denote the density truncated as follows:
ρλ (x) = ( ρ(x)
if ρ(x) ≤ 2λ,
2λ
otherwise.
1
Let ˆfσ denote the function output by the one-class SVM, that is the function that solves (1)
in the case φ is the hinge-loss function and Yi = 1 for all i ∈ {1, . . . , n}. Then, under the
general conditions of Theorem 3, for σ choosen as in Equation (8),
n→+∞ k ˆfσ − ρλ kL2 = 0 .
lim
An interesting by-product of this theorem is the consistency of the one-class SVM algo-
rithm for density level set estimation:

(9)

lim
n→+∞
for σ choosen as in Equation (8).

Theorem 9 Let 0 < µ < 2λ < M , let Cµ be the level set of the density function ρ at
level µ, and bCµ be the level set of 2λ ˆfσ at level µ, where ˆfσ is still the function outptut by
the one-class SVM. For any distribution Q, for any subset C of Rd , deﬁne the excess-mass
of C with respect to Q as follows:
(10)
HQ (C ) = Q (C ) − µLeb (C ) ,
where Leb is the Lebesgue measure. Then, under the general assumptions of Theorem 3,
we have
HP (Cµ ) − HP (cid:16) bCµ(cid:17) = 0 ,
The excess-mass functional was ﬁrst introduced in [10] to as sess the quality of density
level set estimators. It is maximized by the true density level set Cµ and acts as a risk
functional in the one-class framework. The proof ef Theorem 9 is based on the following
result: if ˆρ is a density estimator converging to the true density ρ in the L2 sense, then
for any ﬁxed 0 < µ < sup {ρ}, the excess mass of the level set of ˆρ at level µ converges
to the excess mass of Cµ . In other words, as is the case in the classi ﬁcation framewor k,
plug-in estimators built on L2 -consistent density estimators are consistent with respect to
the excess mass.

(11)

3 Proof of Theorem 3 (sketch)

In this section we sketch the proof of the main learning theorem of this contribution, which
underlies most other results stated in Section 2 The proof of Theorem 3 is based on the
following decomposition of the excess Rφ,0 -risk for the minimizer ˆfφ,σ of bRφ,σ , valid for
any 0 < σ < √2σ1 and any sample (xi , yi )i=1,...,n :
Rφ,0 ( ˆfφ,σ ) − R∗φ,0 = hRφ,0 (cid:16) ˆfφ,σ (cid:17) − Rφ,σ (cid:16) ˆfφ,σ (cid:17)i
+ hRφ,σ ( ˆfφ,σ ) − R∗φ,σ i
+ (cid:2)R∗φ,σ − Rφ,σ (kσ1 ∗ fφ,0 )(cid:3)
+ [Rφ,σ (kσ1 ∗ fφ,0 ) − Rφ,0 (kσ1 ∗ fφ,0 )]
+ (cid:2)Rφ,0 (kσ1 ∗ fφ,0 ) − R∗φ,0 (cid:3) .
It can be shown that kσ1 ∗ fφ,0 ∈ H√2σ1 ⊂ Hσ ⊂ L2(Rd ) which justi ﬁes the introduction
of Rφ,σ (kσ1 ∗fφ,0 ) and Rφ,0 (kσ1 ∗fφ,0 ). By studying the relationship between the Gaussian
RKHS norm and the L2 norm, it can be shown that
Hσ (cid:17) ≤ 0,
Rφ,0 (cid:16) ˆfφ,σ (cid:17) − Rφ,σ (cid:16) ˆfφ,σ (cid:17) = λ (cid:16)k ˆfφ,σ k2
L2 − k ˆfφ,σ k2

(12)

while the following stems from the deﬁnition of R∗φ,σ :
R∗φ,σ − Rφ,σ (kσ1 ∗ fφ,0 ) ≤ 0.
Hence, controlling Rφ,0 ( ˆfφ,σ ) − R∗φ,0 boils down to controlling each of the remaining three
terms in (12).
• The second term in (12) is usually referred to as the sample error or estimation
error. The control of such quantities has been the topic of much research recently,
including for example [14, 15, 16, 17, 18, 4]. Using estimates of local Rademacher
complexities through covering numbers for the Gaussian RKHS due to [4], the
following result can be shown:
Lemma 10 For any σ > 0 small enough, let ˆfφ,σ be the minimizer of the bRφ,σ -
risk on a sample of size n, where φ is a convex loss function. For any 0 < p <
2, δ > 0, and x ≥ 1, the following holds with probability at least 1 − ex over the
draw of the sample:
Rφ,σ ( ˆfφ,σ ) − Rφ,σ (fφ,σ ) ≤ K1L  r κσ φ (0)
λ ! 4
2+p (cid:18) 1
n (cid:19) 2
σ (cid:19) [2+(2−p)(1+δ)]d
(cid:18) 1
2+p
2+p
λ !2 (cid:18) 1
+ K2L  r κσ φ (0)
σ (cid:19)d x
,
n
where K1 and K2 are positive constants depending neither on σ , nor on n.
• In order to upper bound the fourth term in (12), the analysis of the convergence of
the Gaussian RKHS norm towards the L2 norm when the bandwidth of the kernel
tends to 0 leads to:
Hσ − k kσ1 ∗ fφ,0 k2
Rφ,σ (kσ1 ∗ fφ,0 ) − Rφ,0 (kσ1 ∗ fφ,0 ) = k kσ1 ∗ fφ,0 k2
L2
σ2
1 k fφ,0 k2
≤
L2
2σ2
φ (0) σ2
≤
.
2λσ2
1
• The ﬁfth term in (12) corresponds to the approximation error . It can be shown that
for any bounded function in L1 (Rd ) and all σ > 0, the following holds:
k kσ ∗ f − f kL1 ≤ (1 + √d)ω(f , σ) ,
(13)
where ω(f , .) denotes the modulus of continuity of f in the L1 norm. From this
the following inequality can be derived:
Rφ,0 (kσ1 ∗ fφ,0 ) − Rφ,0 (fφ,0 )
≤ (2λk fφ,0 kL∞ + L (k fφ,0 kL∞ ) M ) (cid:16)1 + √d(cid:17) ω (fφ,0 , σ1 ) .

4 Conclusion

We have shown that consistency of learning algorithms that minimize a regularized empir-
ical risk can be obtained even when the so-called regularization term does not asymptoti-
cally vanish, and derived the consistency of one-class SVM as a density level set estimator.
Our method of proof is based on an unusual decomposition of the excess risk due to the
presence of the regularization term, which plays an important role in the determination of
the asymptotic limit of the function that minimizes the empirical risk. Although the upper
bounds on the convergence rates we obtain are not optimal, they provide a ﬁrst step toward
the analysis of learning algorithms in this context.

Acknowledgments

The authors are grateful to St ´ephane Boucheron, Pascal Massart and Ingo Steinwart for
fruitful discussions. This work was supported by the ACI “No uvelles interfaces des
Math ´ematiques ” of the French Ministry for Research, and by the IS T Program of the Eu-
ropean Community, under the Pascal Network of Excellence, IST-2002-506778.

References

[1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers.
In Proceedings of the 5th annual ACM workshop on Computational Learning Theory, pages
144–152. ACM Press, 1992.
[2] I. Steinwart. Support vector machines are universally consistent. J. Complexity, 18:768–791,
2002.
[3] T. Zhang. Statistical behavior and consistency of classiﬁcation metho ds based on convex risk
minimization. Ann. Stat., 32:56–134, 2004.
[4] I. Steinwart and C. Scovel. Fast rates for support vector machines using gaussian kernels.
Technical report, Los Alamos National Laboratory, 2004. submitted to Annals of Statistics.
[5] I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071–1105, 2003.
[6] P. L. Bartlett and A. Tewari. Sparseness vs estimating conditional probabilities: Some asymp-
totic results. In Lecture Notes in Computer Science, volume 3120, pages 564–578. Springer,
2004.
[7] A.N. Tikhonov and V.Y. Arsenin. Solutions of ill-posed problems. W.H. Winston, Washington,
D.C., 1977.
[8] B. W. Silverman. On the estimation of a probability density function by the maximum penalized
likelihood method. Ann. Stat., 10:795–810, 1982.
[9] B. Sch ¨olkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the
support of a high-dimensional distribution. Neural Comput., 13:1443–1471, 2001.
[10] J. A. Hartigan. Estimation of a convex density contour in two dimensions. J. Amer. Statist.
Assoc., 82(397):267–270, 1987.
[11] R. Vert and J.-P. Vert. Consistency and convergence rates of one-class svm and related algo-
rithms. J. Mach. Learn. Res., 2006. To appear.
[12] R. A. DeVore and G. G. Lorentz. Constructive Approximation. Springer Grundlehren der
Mathematischen Wissenschaften. Springer Verlag, 1993.
[13] P.I. Bartlett, M.I. Jordan, and J.D. McAuliffe. Convexity, classiﬁcation and risk bounds. Tech-
nical Report 638, UC Berkeley Statistics, 2003.
[14] A. B. Tsybakov. On nonparametric estimation of density level sets. Ann. Stat., 25:948–969,
June 1997.
[15] E. Mammen and A. Tsybakov. Smooth discrimination analysis. Ann. Stat., 27(6):1808–1829,
1999.
[16] P. Massart. Some applications of concentration inequalities to statistics. Ann. Fac. Sc. Toulouse,
IX(2):245–303, 2000.
[17] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. Annals of
Statistics, 2005. To appear.
[18] V. Koltchinskii. Localized rademacher complexities. Manuscript, september 2003.


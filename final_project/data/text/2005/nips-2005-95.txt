A Criterion for the Convergence of Learning
with Spike Timing Dependent Plasticity

Robert Legenstein and Wolfgang Maass
Institute for Theoretical Computer Science
Technische Universitaet Graz
A-8010 Graz, Austria
flegi,maassg@igi.tugraz.at

Abstract

We investigate under what conditions a neuron can learn by experimen-
tally supported rules for spike timing dependent plasticity (STDP) to pre-
dict the arrival times of strong “teacher
inputs”
to the same neuron. It
turns out that in contrast to the famous Perceptron Convergence Theo-
rem, which predicts convergence of the perceptron learning rule for a
simpliﬁed neuron model whenever a stable solution exists, no equally
strong convergence guarantee can be given for spiking neurons with
STDP. But we derive a criterion on the statistical dependency structure of
input spike trains which characterizes exactly when learning with STDP
will converge on average for a simple model of a spiking neuron. This
criterion is reminiscent of the linear separability criterion of the Percep-
tron Convergence Theorem, but it applies here to the rows of a correlation
matrix related to the spike inputs. In addition we show through computer
simulations for more realistic neuron models that the resulting analyti-
cally predicted positive learning results not only hold for the common
interpretation of STDP where STDP changes the weights of synapses,
but also for a more realistic interpretation suggested by experimental
data where STDP modulates the initial release probability of dynamic
synapses.

1 Introduction

Numerous experimental data show that STDP changes the value wold of a synaptic weight
after pairing of the ﬁring of the presynaptic neuron at time tpre with a ﬁring of the postsy-
naptic neuron at time tpost = tpre + (cid:1)t to wnew = wold + (cid:1)w according to the rule
if (cid:1)t > 0
wnew = (cid:26) minfwmax ; wold + W+ (cid:1) e(cid:0)(cid:1)t=(cid:28)+ g
;
if (cid:1)t (cid:20) 0 ;
maxf0; wold (cid:0) W(cid:0) (cid:1) e(cid:1)t=(cid:28)(cid:0) g
;
with some parameters W+ ; W(cid:0) ; (cid:28)+ ; (cid:28)(cid:0) > 0 (see [1]). If during training a teacher induces
ﬁring of the postsynaptic neuron, this rule becomes somewhat analogous to the well-known
perceptron learning rule for McCulloch-Pitts neurons (= “perceptrons”).
The Perceptron
Convergence Theorem states that this rule enables a perceptron to learn, starting from any
initial weights, after ﬁnitely many errors any transformation that it could possibly imple-
ment. However, we have constructed examples of input spike trains and teacher spike trains

(1)

(omitted in this abstract) such that although a weight vector exists which produces the de-
sired ﬁring and which is stable under STDP, learning with STDP does not converge to a
stable solution. On the other hand experiments in vivo have shown that neurons can be
taught by suitable teacher input to adopt a given ﬁring response [2, 3] (although the spike-
timing dependence is not exploited there). We show in section 2 that such convergence of
learning can be explained by STDP in the average case, provided that a certain criterion is
met for the statistical dependence among Poisson spike inputs. The validity of the proposed
criterion is tested in section 3 for more realistic models for neurons and synapses.

2 An analytical criterion for the convergence of STDP

The average case analysis in this section is based on the linear Poisson neuron model (see
[4, 5]). This neuron model outputs a spike train S post (t) which is a realization of a Poisson
process with the underlying instantaneous ﬁring rate Rpost (t). We represent a spike train
S (t) as a sum of Dirac-(cid:14) functions S (t) = Pk (cid:14)(t (cid:0) tk ), where tk is the k th spike time of
the spike train. The effect of an input spike at input i at time t0 is modeled by an increase
in the instantaneous ﬁring rate of an amount wi (t0 )(cid:15)(t (cid:0) t0 ), where (cid:15) is a response kernel
and wi (t0 ) is the synaptic efﬁcac y of synapse i at time t0 . We assume (cid:15)(s) = 0 for s < 0
(causality), R 1
0 ds (cid:15)(s) = 1 (normalization of the response kernel), and (cid:15)(s) (cid:21) 0 for all s
as well as wi (cid:21) 0 for all i (excitatory inputs). In the linear model, the contributions of all
inputs are summed up linearly:

(2)

(3)

Rpost (t) =

ds wj (t (cid:0) s) (cid:15)(s) Sj (t (cid:0) s) ,

n
Xj=1 Z 1
0
where S1 ; : : : ; Sn are the n presynaptic spike trains. Note that in this spike generation
process, the generation of an output spike is independent of previous output spikes.
The STDP-rule (1) avoids the growth of weights beyond bounds 0 and wmax by simple
clipping. Alternatively one can make the weight update dependent on the actual weight
value. In [5] a general rule is suggested where the weight dependence has the form of a
power law with a non-negative exponent (cid:22). This weight update rule is deﬁned by
if (cid:1)t > 0
(cid:1)w = (cid:26) W+ (cid:1) (1 (cid:0) w)(cid:22) (cid:1) e(cid:0)(cid:1)t=(cid:28)+
;
,
if (cid:1)t (cid:20) 0
(cid:0)W(cid:0) (cid:1) w(cid:22) (cid:1) e(cid:1)t=(cid:28)(cid:0)
;
where we assumed for simplicity that wmax = 1.
input
Instead of looking at speciﬁc
spike trains, we consider the average behavior of the weight vector for (possibly correlated)
homogeneous Poisson input spike trains. Hence, the change (cid:1)wi is a random variable with
a mean drift and ﬂuctuations
around it. We will in the following focus on the drift by
assuming that individual weight changes are very small and only averaged quantities enter
the learning dynamics, see [6]. Let Si be the spike train of input i and let S (cid:3) be the output
spike train of the neuron. The mean drift of synapse i at time t can be approximated as
i Z 0
_wi (t) = W+ (1 (cid:0) wi )(cid:22) Z 1
ds e(cid:0)s=(cid:28) Ci (s; t) (cid:0) W(cid:0)w(cid:22)
0
(cid:0)1
where Ci (s; t) = hSi (t)S (cid:3) (t + s)iE is the ensemble averaged correlation function between
input i and the output of the neuron (see [5, 6]). For the linear Poisson neuron model, input-
output correlations can be described by means of correlations in the inputs. We deﬁne the
normalized cross correlation between input spike trains Si and Sj with a common rate
r > 0 as

ds es=(cid:28) Ci (s; t)

hSi (t) Sj (t + s)iE
C 0
ij (s) =
r2
which assumes value 0 for uncorrelated Poisson spike trains. We assume in this article that
ij is constant over time. In our setup, the output of the neuron during learning is clamped
C 0

(cid:0) 1 ,

(5)

,

(4)

to the teacher spike train S (cid:3) which is the output of a neuron with the target weight vector
w(cid:3) . Therefore, the input-output correlations Ci (s; t) are also constant over time and we
denote them by Ci (s) in the following. In our neuron model, correlations are shaped by the
response kernel (cid:15)(s) and they enter the learning equation (4) with respect to the learning
window. This motivates the deﬁnition of window correlations c+
ij and c(cid:0)
ij for the positive
and negative learning window respectively:
(cid:28) Z 1
ds e(cid:0)s=(cid:28) Z 1
1
0
0
We call the matrices C (cid:6) = fc(cid:6)
ij gi;j=1;:::;n the window correlation matrices. Note that
window correlations are non-negative and that for homogeneous Poisson input spike trains
and for a non-negative response kernel, they are positive. For soft weight bounds and
(cid:22) > 0, a synaptic weight can converge to a value arbitrarily close to 0 or 1, but not to one
of these values directly. This motivates the following deﬁnition of learnability.

ds0 (cid:15)(s0 )C 0
ij ((cid:6)s (cid:0) s0 )

c(cid:6)
ij = 1 +

(6)

.

Deﬁnition 2.1 We say that a target weight vector w(cid:3) 2 f0; 1gn can approximately be
learned in a supervised paradigm by STDP with soft weight bounds on homogeneous Pois-
son input spike trains (short: “ w(cid:3) can be learned ”)
if and only if there exist W+ ; W(cid:0) > 0,
such that for (cid:22) ! 0 the ensemble averaged weight vector hw(t)iE with learning dynamics
given by Equation 4 converges to w(cid:3) for any initial weight vector w(0) 2 [0; 1]n .

We are now ready to formulate an analytical criterion for learnability:

Theorem 2.1 A weight vector w(cid:3) can be learned (when being teached with S (cid:3) ) for ho-
mogeneous Poisson input spike trains with window correlation matrices C + and C (cid:0) to a
linear Poisson neuron with non-negative response kernel if and only if w (cid:3) 6= 0 and
> Pn
k c+
Pn
k=1 w(cid:3)
k c+
k=1 w(cid:3)
jk
ik
Pn
Pn
k c(cid:0)
k c(cid:0)
k=1 w(cid:3)
k=1 w(cid:3)
ik
jk
i = 1 and w(cid:3)
for all pairs hi; j i 2 f1; : : : ; ng2 with w(cid:3)
j = 0.

W+ (1 (cid:0) wi )(cid:22)

Ci (s) = hSi (t) S (cid:3) (t + s)iE =

ds0 (cid:15)(s0 ) hSi (t) Sj (t + s (cid:0) s0 )iE .

Proof idea: The correlation between an input and the teacher induced output is (by Eq. 2):
n
j Z 1
Xj=1
w(cid:3)
0
Substitution of this equation into Eq. 4 yields the synaptic drift
ij 3
_wi = (cid:28) r2 2
n
n
Xj=1
Xj=1
j c(cid:0)
w(cid:3)
5
4
We ﬁnd the equilibrium points w(cid:22)i of synapse i by setting _wi = 0 in Eq. 7. This
i (cid:19)(cid:0)1
yields w(cid:22)i = (cid:18)1 + 1
Pn
j c+
j=1 w(cid:3)
, where (cid:3)i denotes W+
ij
(cid:3)1=(cid:22)
Pn
W(cid:0)
j c(cid:0)
j=1 w(cid:3)
ij
zero if w(cid:3) = 0 which implies that w(cid:3) = 0 cannot be learned. For w(cid:3) 6= 0, one can
show that w(cid:22) = (w(cid:22)1 ; : : : ; w(cid:22)n ) is the only equilibrium point of the system and that it
is stable. Since the system decomposes into n independent one-dimensional systems,
convergence to w(cid:3) is guaranteed for all initial conditions. Furthermore, one sees that
lim(cid:22)!0 w(cid:22)i = 1 if and only if (cid:3)i > 1, and lim(cid:22)!0 w(cid:22)i = 0 if and only if (cid:3)i < 1.
Therefore, lim(cid:22)!0 w(cid:22) = w(cid:3) holds if and only if (cid:3)i > 1 for all i with w(cid:3)
i = 1 and (cid:3)i < 1
i = 0. The theorem follows from the deﬁnition of (cid:3)i .
for all i with w(cid:3)

. Note that the drift is

ij (cid:0) W(cid:0)w(cid:22)
j c+
w(cid:3)
i

.

(7)

For a wide class of cross-correlation functions, one can establish a relationship between
learnability by STDP and the well-known concept of linear separability from linear alge-
bra.1 Because of synaptic delays, the response of a spiking neuron to an input spike is
delayed by some time t0 . One can model such a delay in the response kernel by the restric-
tion (cid:15)(s) = 0 for all s (cid:20) t0 . In the following Corollary we consider the case where input
correlations C 0
ij (s) appear only in a time window smaller than the delay:

Corollary 2.1 If there exists a t0 (cid:21) 0 such that (cid:15)(s) = 0 for all s (cid:20) t0 and C 0
ij (s) = 0 for
all s < (cid:0)t0 ; i; j 2 f1; : : : ; ng, then the following holds for the case of homogeneous
Poisson input spike trains to a linear Poisson neuron with positive response kernel (cid:15):

A weight vector w(cid:3) can be learned if and only if w(cid:3) 6= 0 and w(cid:3) linearly separates the list
n ii, where c+
L = hhc+
n are the rows of C + .
1 ; : : : ; c+
1 i; : : : ; hc+
n ; w(cid:3)
1 ; w(cid:3)

Proof idea: From the assumptions of the corollary it follows that c(cid:0)
ij = 1. In this case, the
condition in Theorem 2.1 is equivalent to the statement that w(cid:3) linearly separates the list
L = hhc+
n ii.
1 i; : : : ; hc+
n ; w(cid:3)
1 ; w(cid:3)

Corollary 2.1 can be viewed as an analogon of the Perceptron Convergence Theorem for the
average case analysis of STDP. Its formulation is tight in the sense that linear separability of
the list L alone (as opposed to linear separability by the target vector w (cid:3) ) is not sufﬁcient
to imply learnability. For uncorrelated input spike trains of rate r > 0, the normalized
ij (s) = (cid:14)ij
cross correlation functions are given by C 0
r (cid:14)(s), where (cid:14)ij is the Kronecker
delta function. The positive window correlation matrix C + is therefore essentially a scaled
version of the identity matrix. The following corollary then follows from Corollary 2.1:

Corollary 2.2 A target weight vector w(cid:3) 2 f0; 1gn can be learned in the case of uncorre-
lated Poisson input spike trains to a linear Poisson neuron with positive response kernel (cid:15)
such that (cid:15)(s) = 0 for all s (cid:20) 0 if and only if w(cid:3) 6= 0.

3 Computer simulations of supervised learning with STDP

In order to make a theoretical analysis feasible, we needed to make in section 2 a number of
simplifying assumptions on the neuron model and the synapse model. In addition a number
of approximations had to be used in order to simplify the estimates. We consider in this
section the more realistic integrate-and-ﬁre model2 for neurons and a model for synapses
which are subject to paired-pulse depression and paired-pulse facilitation, in addition to the
long term plasticity induced by STDP [7]. This model describes synapses with parameters
U (initial release probability), D (depression time constant), and F (facilitation time con-
stant) in addition to the synaptic weight w . The parameters U , D , and F were randomly

1Let c1 ; : : : ; cm 2 Rn and y1 ; : : : ; ym 2 f0; 1g. We say that a vector w 2 Rn linearly separates
the list hhc1 ; y1 i; : : : ; hcm ; ym ii if there exists a threshold (cid:2) such that yi = sign(ci (cid:1) w (cid:0) (cid:2))
for i = 1; : : : ; m. We deﬁne sign(z ) = 1 if z (cid:21) 0 and sign(z ) = 0 otherwise.
2The membrane potential Vm of the neuron is given by (cid:28)m
dVm
dt = (cid:0)(Vm (cid:0) Vresting ) + Rm (cid:1)
(Isyn (t) + Ibackground + Iinject (t)) where (cid:28)m = Cm (cid:1) Rm = 30ms is the membrane time constant,
Rm = 1M (cid:10) is the membrane resistance, Isyn (t) is the current supplied by the synapses, Ibackground
is a constant background current, and Iinject (t) represents currents induced by a “teacher”. If V m
exceeds the threshold voltage Vthresh it is reset to Vreset = 14:2mV and held there for the length
Tref ract = 3ms of the absolute refractory period.Neuron parameters: Vresting = 0V , Ibackground
randomly chosen for each trial from the interval [13:5nA; 14:5nA]. Vthresh was set such that each
neuron spiked at a rate of about 25 Hz. This resulted in a threshold voltage slightly above 15mV .
Synaptic parameters: Synaptic currents were modeled as exponentially decaying currents with decay
time constants (cid:28)S = 3ms ((cid:28)S = 6ms) for excitatory (inhibitory) synapses.

chosen from Gaussian distributions that were based on empirically found data for such con-
nections. We also show that in some cases a less restrictive teacher forcing sufﬁces,
that
tolerates undesired ﬁring of the neuron during training. The results of section 2 predict that
the temporal structure of correlations has a strong in ﬂuence on the outcome of a learning
experiment. We used input spike trains with cross correlations that decay exponentially
with a correlation decay constant (cid:28)cc .3 In experiment 1 we consider temporal correlations
with (cid:28)cc=10ms. Since such “broader”
correlations are not problematic for STDP, sharper
correlations ((cid:28)cc=6ms) are considered in experiment 2.
Experiment 1 (correlated input with (cid:28)cc=10ms): In this experiment, a leaky integrate-
and-ﬁre neuron received inputs from 100 dynamic synapses. 90% of these synapses were
excitatory and 10% were inhibitory. For each excitatory synapse, the maximal efﬁcac y
wmax was chosen from a Gaussian distribution with mean 54 and SD 10:8, bounded by
54 (cid:6) 3SD . The 90 excitatory inputs were divided into 9 groups of 10 synapses per group.
Spike trains were correlated within groups with correlation coefﬁcients between 0 and 0:8,
whereas there were virtually no correlations between spike trains of different groups.4 Tar-
get weight vectors w(cid:3) were chosen in the most adverse way: half of the weights of w(cid:3)
within each group was set to 0, the other half to its maximal value wmax (see Fig. 1C).

A

target

trained

B

1

0.8

0.6

0.4

0.2

0

0

0

0.2

0.4

0.6

0.8

angular error [rad]
spike correlation [s=5ms]

500

1000
1500
time [sec]

2000

2500

1
time [sec]

C

w

60

40

20

0

0

D

w

60

40

20

0

0

1.2

1.4

1.6

1.8

2

target

20

40
Synapse

60

80

trained

20

40
Synapse

60

80

Figure 1: Learning a target weight vector w
(cid:3) on correlated Poisson inputs. A) Output spike train on
test data after one hour of training (trained) compared to the target output (target). B) Evolution of
(cid:3) that implements F in radiant (angular error,
the angle between weight vector w(t) and the vector w
solid line), and spike correlation (dashed line). C) Target weight vector w
(cid:3) consisting of elements
with value 0 or the value wmax assigned to that synapse. D) Corresponding weights of the learned
vector w(t) after 40 minutes of training. (All time data refer to simulated biological time)

Before training, the weights of all excitatory synapses were initialized by randomly chosen
small values. Weights of inhibitory synapses remained ﬁx ed throughout the experiment.
Information about the target weight vector w(cid:3) was given to the neuron only in the form of
short current injections (1 (cid:22)A for 0.2 ms) at those times when the neuron with the weight
vector w(cid:3) would have produced a spike. Learning was implemented as standard STDP
(see rule 1) with parameters (cid:28)+ = (cid:28)(cid:0) = 20ms, W+ = 0:45, W(cid:0) =W+ = 1:05. Additional
inhibitory input was given to the neuron during training that reduced the occurrence of non-

3We constructed input spike trains with normalized cross correlations (see Equation 5) approxi-
ij (s) = ccij
2(cid:28)cc r e(cid:0)jsj=(cid:28)cc between inputs i and j for a mean input rate of r = 20Hz,
mately given by C 0
a correlation coefﬁcient cij , and a correlation decay constant of (cid:28)cc = 10ms.
4The correlation coefﬁcient cij for spike trains within group k consisting of 10 spike trains was
set to cij = cck = 0:1 (cid:3) (k (cid:0) 1) for k = 1; : : : ; 9.

teacher-induced ﬁring of the neuron (see text below).5 Two different performance measures
were used for analyzing the learning progress. The “spik e correlation” measures for test
inputs that were not used for training (but had been generated by the same process) the
deviation between the output spike train produced by the target weight vector w (cid:3) for this
input, and the output spike train produced for the same input by the neuron with the current
weight vector w(t)6 . The angular error measures the angle between the current weight
vector w(t) and the target weight vector w(cid:3) . The results are shown in Fig. 1. One can
see that the deviation of the learned weight vector shown in panel D from the target weight
vector w(cid:3) (panel C) is very small, even for highly correlated groups of synapses with het-
erogeneous target weights. No signi ﬁcant changes in the results were observed for longer
simulations (4 hours simulated biological time), showing stability of learning. On 20 trials
(each with a new random distribution of maximal weights wmax , different initializations
w(0) of the weight vector before learning, and new Poisson spike trains), a spike correla-
tion of 0:83 (cid:6) 0:06 was achieved (angular error 6:8 (cid:6) 4:7 degrees). Note that learning is not
only based on teacher spikes but also on non teacher-induced ﬁring. Therefore, strongly
correlated groups of inputs tend to cause autonomous (i.e., not teacher-induced) ﬁring of
the neuron which results in weight increases for all weights within the corresponding group
of synapses according to well-known results for STDP [8, 5]. Obviously this effect makes
it quite hard to learn a target weight vector w(cid:3) where half of the weights for each corre-
lated group have value 0. The effect is reduced by the additional inhibitory input during
training which reduces undesired ﬁring. However, without this input a spike correlation of
0:79 (cid:6) 0:09 could still be achieved (angular error 14:1 (cid:6) 10 degrees).

A

n
o
i
t
a
l
e
r
r
o
c
 
e
k
i
p
s

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
input correlation cc

B

n
o
i
t
a
l
e
r
r
o
c
 
e
k
i
p
s
−
1

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

predicted to be learnable
predicted to be not learnable

5

10

15

weight error [ °]

Figure 2: A) Spike correlation achieved for correlated inputs (solid line). Some inputs were cor-
related with cc plotted on the x-axis. Also, as a control the spike correlation achieved by randomly
drawn weight vectors is shown (dashed line, where half of the weights were set to wmax and the other
weights were set to 0). B) Comparison between theory and simulation results for a leaky integrate-
and-ﬁre neuron and input correlations between 0:1 and 0:5 ((cid:28)cc = 6ms). Each cross (open circle)
marks a trial where the target vector was learnable (not learnable) according to Theorem 2.1. The
actual learning performance of STDP is plotted for each trial in terms of the weight error (x-axis) and
1 minus the spike correlation (y-axis).

Experiment 2 (testing the theoretical predictions for (cid:28)cc=6ms): In order to evaluate the
dependence of correlation among inputs we proceeded in a setup similar to experiment
1. 4 input groups consisting each of 10 input spike trains were constructed for which the
correlations within each group had the same value cc while the input spike train to the
other 50 excitatory synapses were uncorrelated. Again, half of the weights of w (cid:3) within

5We added 30 inhibitory synapses with weights drawn from a gamma distribution with mean 25
and standard deviation 7:5, that received additional 30 uncorrelated Poisson spike trains at 20 Hz.
6For that purpose each spike in these two output spike trains was replaced by a Gaussian function
with an SD of 5 ms. The spike correlation between both output spike trains was deﬁned as the
correlation between the resulting smooth functions of time (for segments of length 100 s).

each correlated group (and within the uncorrelated group) was set to 0, the other half to a
randomly chosen maximal value. The learning performance after 1 hour of training for 20
trials is plotted in Fig. 2A for 7 different values of the correlation cc ((cid:28)cc = 6ms) that is
applied in 4 of the input groups (solid line).

In order to test the approximate validity of Theorem 2.1 for leaky integrate-and-ﬁre neu-
rons and dynamic synapses, we repeated the above experiment for input correlations
cc = 0:1; 0:2; 0:3; 0:4, and 0:5. For each correlation value, 20 learning trials (with different
target vectors) were simulated. For each trial we ﬁrst checked whether the (randomly cho-
sen) target vector w(cid:3) was learnable according to the condition given in Theorem 2:1 (65%
of the 100 learning trials were classiﬁed as being learnable).7 The actual performance of
learning with STDP was evaluated after 50 minutes of training.8 The result is shown in Fig.
2B. It shows that the theoretical prediction of learnability or non-learnability for the case
of simpler neuron models and synapses from Theorem 2.1 translates in a biologically more
realistic scenario into a quantitative grading of the learning performance that can ultimately
be achieved with STDP.

A

1

0.8

0.6

0.4

0.2

0
0

angular error [rad]
weight deviation
spike correlation

500

1000 1500 2000 2500
time [sec]

B

U

C

U

0.2

0.1

0

0.2

0.1

0

target

5

5

15

20

10
Synapse
trained

10
Synapse

15

20

Figure 3: Results of modulation of initial release probabilities U . A) Performance of U -learning for
a generic learning task (see text). B) Twenty values of the target U vector (each component assumes
its maximal possible value or the value 0). C) Corresponding U values after 42 minutes of training.

Experiment 3 (Modulation of initial release probabilities U by STDP): Experimental
data from [9] suggest that synaptic plasticity does not change the uniform scaling of the
amplitudes of EPSPs resulting from a presynaptic spike train (i.e., the parameter w), but
rather redistributes the sum of their amplitudes. If one assumes that STDP changes the pa-
rameter U that determines the synaptic release probability for the ﬁrst spike in a spike train,
whereas the weight w remains unchanged, then the same experimental data that support the
classical rule for STDP, support the following rule for changing U :
Unew = (cid:26) minfUmax ; Uold + U+ (cid:1) e(cid:0)(cid:1)t=(cid:28)+ g
maxf0; Uold (cid:0) U(cid:0) (cid:1) e(cid:1)t=(cid:28)(cid:0) g
with suitable nonnegative parameters Umax ; U+ ; U(cid:0) ; (cid:28)+ ; (cid:28)(cid:0) .
Fig. 3 shows results of an experiment where U was modulated with rule (8) (similar to
experiment 1, but with uncorrelated inputs). 20 repetitions of this experiment yielded after
42 minutes of training the following results: spike correlation 0:88 (cid:6) 0:036, angular error
27:9 (cid:6) 3:7 degrees, for U+ = 0:0012, U(cid:0)=U+ = 1:055. Apparently the output spike train
is less sensitive to changes in the values of U than to changes in w . Consequently, since

if (cid:1)t > 0
if (cid:1)t (cid:20) 0 ;

;
;

(8)

7We had chosen a response kernel of the form (cid:15)(s) = 1
(e(cid:0)s=(cid:28)1 (cid:0) e(cid:0)s=(cid:28)2 ) with (cid:28)1 = 2ms
(cid:28)1(cid:0)(cid:28)2
and (cid:28)2 = 1ms (Least mean squares ﬁt of the double exponential to the peri-stimulus-time histogram
(PSTH) of the neuron, which reﬂectsthe probability of spiking as a function of time s since an input
ij and c(cid:0)
spike), and calculated the window correlations c+
ij numerically.
8To guarantee the best possible performance for each learning trial, training was performed on 27
different values for W(cid:0) =W+ between 1:02 and 1:15.

only the behavior of a neuron with vector U(cid:3) but not the vector U(cid:3) is made available to
the neuron during training, the resulting correlation between target- and actual output spike
trains is quite high, whereas angular error between U(cid:3) and U(t), as well as the average
deviation in U , remain rather large.
We also repeated experiment 1 (correlated Poisson inputs) with rule (8) for U -learning.
20 repetitions with different target weights and different initial conditions yielded after 35
minutes of training: spike correlation 0:75 (cid:6) 0:08, angular error 39:3 (cid:6) 4:8 degrees, for
U+ = 8 (cid:1) 10(cid:0)4 , U(cid:0)=U+ = 1:09.

4 Discussion

The main conclusion of this article is that for many common distributions of input spikes
a spiking neuron can learn with STDP and teacher-induced input currents any map from
input spike trains to output spike trains that it could possibly implement in a stable manner.

We have shown in section 2 that a mathematical average case analysis can be carried out
for supervised learning with STDP. This theoretical analysis produces the ﬁrst
criterion
that allows us to predict whether supervised learning with STDP will succeed in spite of
correlations among Poisson input spike trains. For the special case of “sharp correlations”
(i.e. when the cross correlations vanish for time shifts larger than the synaptic delay) this
criterion can be formulated in terms of linear separability of the rows of a correlation matrix
related to the spike input, and its mathematical form is therefore reminiscent of the well-
known condition for learnability in the case of perceptron learning. In this sense Corollary
2.1 can be viewed as an analogon of the Perceptron Convergence Theorem for spiking
neurons with STDP.

Furthermore we have shown that an alternative interpretation of STDP where one assumes
that it modulates the initial release probabilities U of dynamic synapses, rather than their
scaling factors w , gives rise to very satisfactory convergence results for learning.
Acknowledgment: We would like to thank Yves Fregnac, Wulfram Gerstner, and espe-
cially Henry Markram for inspiring discussions.

References
[1] L. F. Abbott and S. B. Nelson. Synaptic plasticity: taming the beast. Nature Neurosci., 3:1178 –
1183, 2000.
[2] Y. Fregnac, D. Shulz, S. Thorpe, and E. Bienenstock. A cellular analogue of visual cortical
plasticity. Nature, 333(6171):367 –370, 1988.
[3] D. Debanne, D. E. Shulz, and Y. Fregnac. Activity dependent regulation of on- and off-responses
in cat visual cortical receptive ﬁelds. Journal of Physiology, 508:523–548, 1998.
[4] R. Kempter, W. Gerstner, and J. L. van Hemmen. Intrinsic stabilization of output rates by spike-
based hebbian learning. Neural Computation, 13:2709–2741, 2001.
[5] R. G ¨utig, R. Aharonov, S. Rotter, and H. Sompolinsky. Learning input correlations through
non-linear temporally asymmetric hebbian plasticity. Journal of Neurosci., 23:3697–3714, 2003.
[6] R. Kempter, W. Gerstner, and J. L. van Hemmen. Hebbian learning and spiking neurons. Phys.
Rev. E, 59(4):4498–4514, 1999.
[7] H. Markram, Y. Wang, and M. Tsodyks. Differential signaling via the same axon of neocortical
pyramidal neurons. PNAS, 95:5323–5328, 1998.
[8] S. Song, K. D. Miller, and L. F. Abbott. Competitive hebbian learning through spike-timing
dependent synaptic plasticity. Nature Neuroscience, 3:919–926, 2000.
[9] H. Markram and M. Tsodyks. Redistribution of synaptic efﬁcac y between neocortical pyramidal
neurons. Nature, 382:807–810, 1996.


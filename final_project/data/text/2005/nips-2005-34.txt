Generalized Nonnegative Matrix
Approximations with Bregman Divergences

Inderjit S. Dhillon
Suvrit Sra
Dept. of Computer Sciences
The Univ. of Texas at Austin
Austin, TX 78712.
{inderjit,suvrit}@cs.utexas.edu

Abstract

Nonnegative matrix approximation (NNMA) is a recent technique for di-
mensionality reduction and data analysis that yields a parts based, sparse
nonnegative representation for nonnegative input data. NNMA has found
a wide variety of applications, including text analysis, document cluster-
ing, face/image recognition, language modeling, speech processing and
many others. Despite these numerous applications, the algorithmic de-
velopment for computing the NNMA factors has been relatively deﬁ-
cient. This paper makes algorithmic progress by modeling and solving
(using multiplicative updates) new generalized NNMA problems that
minimize Bregman divergences between the input matrix and its low-
rank approximation. The multiplicative update formulae in the pioneer-
ing work by Lee and Seung [11] arise as a special case of our algorithms.
In addition, the paper shows how to use penalty functions for incorporat-
ing constraints other than nonnegativity into the problem. Further, some
interesting extensions to the use of “link” functions for mo deling non-
linear relationships are also discussed.

1

Introduction

Nonnegative matrix approximation (NNMA) is a method for dimensionality reduction and
data analysis that has gained favor over the past few years. NNMA has previously been
called positive matrix factorization [13] and nonnegative matrix factorization1 [12]. As-
sume that a1 , . . . , aN are N nonnegative input (M -dimensional) vectors. We organize
these vectors as the columns of a nonnegative data matrix
A , £a1 a2
. . . aN ¤.
NNMA seeks a small set of K nonnegative representative vectors b1 , . . . , bK that can be
nonnegatively (or conically) combined to approximate the input vectors ai . That is,
K
Xk=1
1We use the word approximation instead of factorization to emphasize the inexactness of the
process since, the input A is approximated by BC .

1 ≤ n ≤ N ,

an ≈

cknbk ,

ckn are restricted to be nonnegative. If ckn and bk are
where the combining coefﬁcients
unrestricted, and we minimize Pn kan − Bcnk2 , the Truncated Singular Value Decompo-
sition (TSVD) of A yields the optimal bk and ckn values. If the bk are unrestricted, but the
coefﬁcient vectors cn are restricted to be indicator vectors, then we obtain the problem of
hard-clustering (See [16, Chapter 8] for related discussion regarding different constraints
on cn and bk ).
In this paper we consider problems where all involved matrices are nonnegative. For many
practical problems nonnegativity is a natural requirement. For example, color intensities,
chemical concentrations, frequency counts etc., are all nonnegative entities, and approxi-
mating their measurements by nonnegative representations leads to greater interpretability.
NNMA has found a signi ﬁcant number of applications, not only due to increased inter-
pretability, but also because admitting only nonnegative combinations of the bk leads to
sparse representations.

This paper contributes to the algorithmic advancement of NNMA by generalizing the prob-
lem signi ﬁcantly, and by deriving efﬁcient algorithms base
d on multiplicative updates for
the generalized problems. The scope of this paper is primarily on generic methods for
NNMA, rather than on speci ﬁc applications. The multiplicat
ive update formulae in the pi-
oneering work by Lee and Seung [11] arise as a special case of our algorithms, which seek
to minimize Bregman divergences between the nonnegative input A and its approxima-
tion. In addition, we discuss the use penalty functions for incorporating constraints other
than nonnegativity into the problem. Further, we illustrate an interesting extension of our
algorithms for handling non-linear relationships through the use of “link” functions.
2 Problems
Given a nonnegative matrix A as input, the classical NNMA problem is to approximate it
by a lower rank nonnegative matrix of the form BC , where B = [b1 , ..., bK ] and C =
[c1 , ..., cN ] are themselves nonnegative. That is, we seek the approximation,
where B , C ≥ 0.
A ≈ BC ,
We judge the goodness of the approximation in (2.1) by using a general class of distortion
measures called Bregman divergences. For any strictly convex function ϕ : S ⊆ R → R
that has a continuous ﬁrst derivative, the corresponding Bregman divergence Dϕ : S ×
int(S ) → R
+ is deﬁned as Dϕ (x, y) , ϕ(x) − ϕ(y) − ∇ϕ(y)(x − y), where int(S )
is the interior of set S [1, 2]. Bregman divergences are nonnegative, convex in the ﬁ rst
argument and zero if and only if x = y . These divergences play an important role in
convex optimization [2]. For the sequel we consider only separable Bregman divergences,
i.e., Dϕ (X , Y ) = Pij Dϕ (xij , yij ). We further require xij , yij ∈ domϕ ∩ R
+ .
Formally, the resulting generalized nonnegative matrix approximation problems are:
min
Dϕ (BC , A) + α(B ) + β (C ),
B , C≥0
min
B , C≥0
The functions α and β serve as penalty functions, and they allow us to enforce regulariza-
tion (or other constraints) on B and C . We consider both (2.2) and (2.3) since Bregman
divergences are generally asymmetric. Table 1 gives a small sample of NNMA problems
to illustrate the breadth of our formulation.

Dϕ (A, BC ) + α(B ) + β (C ).

(2.1)

(2.2)

(2.3)

3 Algorithms

In this section we present algorithms that seek to optimize (2.2) and (2.3). Our algorithms
are iterative in nature, and are directly inspired by the efﬁ cient algorithms of Lee and Seung
[11]. Appealing properties include ease of implementation and computational efﬁciency.

Divergence Dϕ
ϕ
1
2 x2
kA − BC k2
F
1
kA − BC k2
2 x2
F
1
kW ⊙ (A − BC )k2
2 x2
F
KL(A, BC )
x log x
KL(A, W BC )
x log x
KL(A, BC )
x log x
Dϕ (A, W1BCW2 ) ϕ(x)

α
0

0

0

0

β
0
λ1T C 1
0

0

0
0
c1B T B1 −c′kC k2
F
α(B )
β (C )

Remarks
Lee and Seung [11, 12]
Hoyer [10]
Paatero and Tapper [13]
Lee and Seung [11]
Guillamet et al. [9]
Feng et al. [8]
Weighted NNMA (new)

Table 1: Some example NNMA problems that may be obtained from (2.3). The correspond-
ing asymmetric problem (2.2) has not been previously treated in the literature. KL(x, y )
denotes the generalized KL-Divergence = Pi xi log xi
yi − xi + yi (also called I-divergence).
Note that the problems (2.2) and (2.3) are not jointly convex in B and C , so it is not easy
to obtain globally optimal solutions in polynomial time. Our iterative procedures start by
initializing B and C randomly or otherwise. Then, B and C are alternately updated until
there is no further appreciable change in the objective function value.

3.1 Algorithms for (2.2)
We utilize the concept of auxiliary functions [11] for our derivations. It is sufﬁcient to
illustrate our methods using a single column of C (or row of B ), since our divergences are
separable.
Deﬁnition 3.1 (Auxiliary function). A function G(c, c′ ) is called an auxiliary function
for F (c) if:

1. G(c, c) = F (c), and
2. G(c, c′ ) ≥ F (c) for all c′ .
Auxiliary functions turn out to be useful due to the following lemma.
Lemma 3.2 (Iterative minimization). If G(c, c′ ) is an auxiliary function for F (c), then
F is non-increasing under the update

ct+1 = argminc G(c, ct ).

Proof. F (ct+1 ) ≤ G(ct+1 , ct ) ≤ G(ct , ct ) = F (ct ).
As can be observed, the sequence formed by the iterative application of Lemma 3.2 leads to
a monotonic decrease in the objective function value F (c). For an algorithm that iteratively
updates c in its quest to minimize F (c), the method for proving convergence boils down to
the construction of an appropriate auxiliary function. Auxiliary functions have been used
in many places before, see for example [5, 11].

We now construct simple auxiliary functions for (2.2) that yield multiplicative updates. To
avoid clutter we drop the functions α and β from (2.2), noting that our methods can easily
be extended to incorporate these functions.

Suppose B is ﬁxed and we wish to compute an updated column of C . We wish to minimize

F (c) = Dϕ (Bc, a),

(3.1)

where a is the column of A corresponding to the column c of C . The lemma below shows
how to construct an auxiliary function for (3.1). For convenience of notation we use ψ to
denote ∇ϕ for the rest of this section.

(3.2)

Lemma 3.3 (Auxiliary function). The function
λij ϕµ bij cj
λij ¶ − µXi
ϕ(ai ) + ψ(ai )¡(Bc)i − ai ¢¶,
G(c, c′ ) = Xij
l ), is an auxiliary function for (3.1). Note that by deﬁnition
with λij = (bij c′
j )/(Pl bil c′
Pj λij = 1, and as both bij and c′
j are nonnegative, λij ≥ 0.
Proof. It is easy to verify that G(c, c) = F (c), since Pj λij = 1. Using the convexity of
ϕ, we conclude that if Pj λij = 1 and λij ≥ 0, then
bij cj ¶ − ϕ(ai ) − ψ(ai )¡(Bc)i − ai ¢
ϕµXj
F (c) = Xi
λij ϕµ bij cj
λij ¶ − µXi
ϕ(ai ) + ψ(ai )¡(Bc)i − ai ¢¶
≤ Xij
= G(c, c′ ).

∂G
∂ cp

c. Let ψ(x) denote the vector

To obtain the update, we minimize G(c, c′ ) w.r.t.
[ψ(x1 ), . . . , ψ(xn )]T . We compute the partial derivative
λip ¶ bip
λipψµ bip cp
= Xi
λip − Xi
bipψ(ai )
(Bc′ )i¶ − (B T ψ(a))p .
bipψµ cp
= Xi
c′
p
We need to solve (3.3) for cp by setting ∂G/∂ cp = 0. Solving this equation analytically
is not always possible. However, for a broad class of functions, we can obtain an analytic
solution. For example, if ψ is multiplicative (i.e., ψ(xy) = ψ(x)ψ(y)) we obtain the
following iterative update relations for b and c (see [7])
bp ← bp · ψ−1³ [ψ(aT )C T ]p
[ψ(bT C )C T ]p ´,
cp ← cp · ψ−1³ [B T ψ(a)]p
[B T ψ(Bc)]p ´.
It turns out that when ϕ is a convex function of Legendre type, then ψ−1 can be obtained
by the derivative of the conjugate function ϕ∗ of ϕ, i.e., ψ−1 = ∇ϕ∗ [14].
Note. (3.4) & (3.5) coincide with updates derived by Lee and Seung [11], if ϕ(x) = 1
2 x2 .

(3.3)

(3.4)

(3.5)

3.1.1 Examples of New NNMA Problems

We illustrate the power of our generic auxiliary functions given above for deriving algo-
rithms with multiplicative updates for some speci ﬁc intere sting problems.

First we consider the problem that seeks to minimize the divergence,
KL(Bc, a) = Xi

(Bc)i
ai − (Bc)i + ai ,

(Bc)i log

B , c ≥ 0.

(3.6)

bip log ai = 0,

Let ϕ(x) = x log x − x. Then, ψ(x) = log x, and as ψ(xy) = ψ(x) + ψ(y), upon
substituting in (3.3), and setting the resultant to zero we obtain
∂G
= Xi
p ) − Xi
bip log(cp (Bc′ )i /c′
∂ cp
cp
= [B T log a − B T log(Bc′ )]p
=⇒ (B T 1)p log
c′
p
p · expÃ [B T log¡a/(Bc′ )¢]p
!.
=⇒ cp = c′
[B T 1]p
The update for b can be derived similarly.
Constrained NNMA. Next we consider NNMA problems that have additional constraints.
We illustrate our ideas on a problem with linear constraints.

(3.7)

Dϕ (Bc, a)
min
x
s.t. P c ≤ 0,
c ≥ 0.
We can solve (3.7) problem using our method by making use of an appropriate (differen-
tiable) penalty function that enforces P c ≤ 0. We consider,
F (c) = Dϕ (Bc, a) + ρk max(0, P c)k2 ,
where ρ > 0 is some penalty constant. Assuming multiplicative ψ and following the
auxiliary function technique described above, we obtain the following updates for c,
ck ← ck · ψ−1µ [B T ψ(a)]k − ρ[P T (P c)+ ]k
¶,
[B T ψ(Bc)]k
where (P c)+ = max(0, P c). Note that care must be taken to ensure that the addition of
this penalty term does not violate the nonnegativity of c, and to ensure that the argument
of ψ−1 lies in its domain.

(3.8)

Remarks.
Incorporating additional constraints into (3.6) is however easier, since the ex-
ponential updates ensure nonnegativity. Given a = 1, with appropriate penalty functions,
our solution to (3.6) can be utilized for maximizing entropy of Bc subject to linear or
non-linear constraints on c.
Nonlinear models with “link” functions.
If A ≈ h(BC ), where h is a “link” function
that models a nonlinear relationship between A and the approximant BC , we may wish
to minimize Dϕ (h(BC ), A). We can easily extend our methods to handle this case for
appropriate h. Recall that the auxiliary function that we used, depended upon the convexity
of ϕ. Thus, if (ϕ ◦ h) is a convex function, whose derivative ∇(ϕ ◦ h) is “factorizable,” then
we can easily derive algorithms for this problem with link functions. We exclude explicit
examples for lack of space and refer the reader to [7] for further details.

3.2 Algorithms using KKT conditions
(2.3), and these updates turn out
We now derive efﬁcient multiplicative update relations for
to be simpler than those for (2.2). To avoid clutter, we describe our methods with α ≡ 0,
and β ≡ 0, noting that if α and β are differentiable, then it is easy to incorporate them in
our derivations. For convenience we use ζ (x) to denote ∇2 (x) for the rest of this section.
Using matrix algebra, one can show that the gradients of Dϕ (A, BC ) w.r.t. B and C are,
∇B Dϕ (A, BC ) =¡ζ (BC ) ⊙ (BC − A)¢C T
∇C Dϕ (A, BC ) =B T ¡ζ (BC ) ⊙ (BC − A)¢,

∇C Dϕ (A, BC ) = Ω,

where ⊙ denotes the elementwise or Hadamard product, and ζ is applied elementwise to
BC . According to the KKT conditions, there exist Lagrange multiplier matrices Λ ≥ 0
and Ω ≥ 0 such that
(3.9a)
∇BDϕ (A, BC ) = Λ,
(3.9b)
λmk bmk = ωkn ckn = 0.
Writing out the gradient ∇B Dϕ (A, BC ) elementwise, multiplying by bmk , and making
use of (3.9a,b), we obtain
£¡ζ (BC ) ⊙ (BC − A)¢C T ¤mk bmk = λmk bmk = 0,
which suggests the iterative scheme
bmk ← bmk £¡ζ (BC ) ⊙ A¢C T ¤mk
£¡ζ (BC ) ⊙ BC ¢C T ¤mk
Proceeding in a similar fashion we obtain a similar iterative formula for ckn , which is
[B T ¡ζ (BC ) ⊙ A¢]kn
[B T ¡ζ (BC ) ⊙ BC ¢]kn
3.2.1 Examples of New and Old NNMA Problems as Special Cases
We now illustrate the power of our approach by showing how one can easily obtain iterative
update relations for many NNMA problems, including known and new problems. For more
examples and further generalizations we refer the reader to [7].
Lee and Seung’s Algorithms. Let α ≡ 0, β ≡ 0. Now if we set ϕ(x) = 1
2 x2 or
ϕ(x) = x log x, then (3.10) and (3.11) reduce to the Frobenius norm and KL-Divergence
update rules originally derived by Lee and Seung [11].
Elementwise weighted distortion. Here we wish to minimize kW ⊙ (A−BC )k2
F . Using
X ← √W ⊙ X , and A ← √W ⊙ A in (3.10) and (3.11) one obtains
B T (W ⊙ A)
(W ⊙ A)C T
C ← C ⊙
(W ⊙ (BC ))C T ,
B ← B ⊙
B T (W ⊙ (BC ))
These iterative updates are signi ﬁcantly simpler than the P MF algorithms of [13].
The Multifactor NNMA Problem (new). The above ideas can be extended to the multi-
factor NNMA problem that seeks to minimize the following divergence (see [7])

ckn ← ckn

.

.

(3.10)

(3.11)

.

Dϕ (A, B1B2 . . . BR ),

where all matrices involved are nonnegative. A typical usage of multifactor NNMA prob-
lem would be to obtain a three-factor NNMA, namely A ≈ RBC . Such an approximation
is closely tied to the problem of co-clustering [3], and can be used to produce relaxed co-
clustering solutions [7].
Weighted NNMA Problem (new). We can follow the same derivation method as above
(based on KKT conditions) for obtaining multiplicative updates for the weighted NNMA
problem:

min Dϕ (A, W1BCW2 ),
where W1 and W2 are nonnegative (and nonsingular) weight matrices. The work of [9] is
a special case as mentioned in Table 1. Please refer to [7] for more details.

4 Experiments and Discussion

We have looked at generic algorithms for minimizing Bregman divergences between the
input and its approximation. One important question arises: Which Bregman divergence
should one use for a given problem? Consider the following factor analytic model

A = BC + N ,

where N represents some additive noise present in the measurements A, and the aim is to
recover B and C . If we assume that the noise is distributed according to some member
of the exponential family, then minimizing the corresponding Bregman divergence [1] is
appropriate. For e.g., if the noise is modeled as i.i.d. Gaussian noise, then the Frobenius
norm based problem is natural.

Another question is: Which version of the problem we should use, (2.2) or (2.3)? For
2 x2 , both problems coincide. For other ϕ, the choice between (2.2) and (2.3) can
ϕ(x) = 1
be guided by computation issues or sparsity patterns of A. Clearly, further work is needed
for answering this question in more detail.

Some other open problems involve looking at the class of minimization problems to which
the iterative methods of Section 3.2 may be applied. For example, determining the class
of functions h, for which these methods may be used to minimize Dϕ (A, h(BC )). Other
possible methods for solving both (2.2) and (2.3), such as the use of alternating projections
(AP) for NNMA, also merit a study.

Our methods for (2.2) decreased the objective function monotonically (by construction).
However, we did not demonstrate such a guarantee for the updates (3.10) & (3.11). Figure 1
offers encouraging empirical evidence in favor of a monotonic behavior of these updates.
It is still an open problem to formally prove this monotonic decrease. Preliminary results
that yield new monotonicity proofs for the Frobenius norm and KL-divergence NNMA
problems may be found in [7].

PMF Objective

3

2.9

2.8

2.7

2.6

2.5

2.4

2.3

e
u
l
a
v
 
n
o
i
t
c
n
u
f
 
e
v
i
t
c
e
j
b
O

ϕ(x) = − log x

28

26

24

22

20

18

16

14

12

10

e
u
l
a
v
 
n
o
i
t
c
n
u
f
 
e
v
i
t
c
e
j
b
O

19

18

17

16

15

14

13

12

e
u
l
a
v
 
n
o
i
t
c
n
u
f
 
e
v
i
t
c
e
j
b
O

ϕ(x) = x log x − x

2.2

0

10

20

30

40
60
50
Number of iterations

70

80

90

100

8

0

10

20

30

40
60
50
Number of iterations

70

80

90

100

11

0

10

20

30

40
60
50
Number of iterations

70

80

90

100

Figure 1: Objective function values over 100 iterations for different NNMA problems. The input
matrix A was random 20 × 8 nonnegative matrix. Matrices B and C were 20 × 4, 4 × 8, respectively.

NNMA has been used in a large number of applications, a fact that attests to its importance
and appeal. We believe that special cases of our generalized problems will prove to be
useful for applications in data mining and machine learning.
5 Related Work
Paatero and Tapper [13] introduced NNMA as positive matrix factorization, and they aimed
to minimize kW ⊙ (A − BC )kF , where W was a ﬁxed nonnegative matrix of weights.
NNMA remained conﬁned to applications in Environmetrics an d Chemometrics before
pioneering papers of Lee and Seung [11, 12] popularized the problem. Lee and Seung [11]
provided simple and efﬁcient algorithms for the NNMA proble ms that sought to minimize

kA − BC kF and KL(A, BC ). Lee & Seung called these problems nonnegative matrix
factorization (NNMF), and their algorithms have inspired our generalizations.

NNMA was applied to a host of applications including text analysis, face/image recogni-
tion, language modeling, and speech processing amongst others. We refer the reader to [7]
for pointers to the literature on various applications of NNMA.

Srebro and Jaakola [15] discuss elementwise weighted low-rank approximations without
any nonnegativity constraints. Collins et al. [6] discuss algorithms for obtaining a low rank
approximation of the form A ≈ BC , where the loss functions are Bregman divergences,
however, there is no restriction on B and C . More recently, Cichocki et al. [4] presented
schemes for NNMA with Csisz ´ar’s ϕ-divergeneces, though rigorous convergence proofs
seem to be unavailable. Our approach of Section 3.2 also yields heuristic methods for
minimizing Csisz ´ar’s divergences.

Acknowledgments

This research was supported by NSF grant CCF-0431257, NSF Career Award ACI-
0093404, and NSF-ITR award IIS-0325116.
References
[1] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with Bregman Divergences. In
SIAM International Conf. on Data Mining, Lake Buena Vista, Florida, April 2004. SIAM.
[2] Y. Censor and S. A. Zenios. Parallel Optimization: Theory, Algorithms, and Applications.
Numerical Mathematics and Scientiﬁc Computation. Oxford University Pre ss, 1997.
[3] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minimum Sum Squared Residue based Co-clustering
of Gene Expression data. In Proc. 4th SIAM International Conference on Data Mining (SDM),
pages 114–125, Florida, 2004. SIAM.
[4] A. Cichocki, R. Zdunek, and S. Amari. Csisz ´ar’s Divergences for Non-Negative Matrix Factor-
ization: Family of New Algorithms. In 6th Int. Conf. ICA & BSS, USA, March 2006.
[5] M. Collins, R. Schapire, and Y. Singer. Logistic regression, adaBoost, and Bregman distances.
In Thirteenth annual conference on COLT, 2000.
[6] M. Collins, S. Dasgupta, and R. E. Schapire. A Generalization of Principal Components Anal-
ysis to the Exponential Family. In NIPS 2001, 2001.
[7] I. S. Dhillon and S. Sra. Generalized nonnegative matrix approximations. Technical report,
Computer Sciences, University of Texas at Austin, 2005.
[8] T. Feng, S. Z. Li, H-Y. Shum, and H. Zhang. Local nonnegative matrix factorization as a
visual representation. In Proceedings of the 2nd International Conference on Development and
Learning, pages 178–193, Cambridge, MA, June 2002.
[9] D. Guillamet, M. Bressan, and J. Vitri `a. A weighted nonnegative matrix factorization for local
representations. In CVPR. IEEE, 2001.
[10] P. O. Hoyer. Non-negative sparse coding. In Proc. IEEE Workshop on Neural Networks for
Signal Processing, pages 557–565, 2002.
[11] D. D. Lee and H. S. Seung. Algorithms for nonnegative matrix factorization. In NIPS, pages
556–562, 2000.
[12] D. D. Lee and H. S. Seung. Learning the parts of objects by nonnegative matrix factorization.
Nature, 401:788–791, October 1999.
[13] P. Paatero and U. Tapper. Positive matrix factorization: A nonnegative factor model with opti-
mal utilization of error estimates of data values. Environmetrics, 5(111–126), 1994.
[14] R. T. Rockafellar. Convex Analysis. Princeton Univ. Press, 1970.
[15] N. Srebro and T. Jaakola. Weighted low-rank approximations. In Proc. of 20th ICML, 2003.
[16] J. A. Tropp. Topics in Sparse Approximation. PhD thesis, The Univ. of Texas at Austin, 2004.


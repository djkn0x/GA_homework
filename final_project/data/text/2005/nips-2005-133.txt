Nonparametric inference of prior probabilities
from Bayes-optimal behavior

Liam Paninski∗
Department of Statistics, Columbia University
liam@stat.columbia.edu; http://www.stat.columbia.edu/∼liam
Abstract

We discuss a method for obtaining a subject’s a priori beliefs from
his/her behavior in a psychophysics context, under the assumption that
the behavior is (nearly) optimal from a Bayesian perspective. The
method is nonparametric in the sense that we do not assume that the
prior belongs to any ﬁxed class of distributions (e.g., Gaus sian). Despite
this increased generality, the method is relatively simple to implement,
being based in the simplest case on a linear programming algorithm, and
more generally on a straightforward maximum likelihood or maximum
a posteriori formulation, which turns out to be a convex optimization
problem (with no non-global local maxima) in many important cases. In
addition, we develop methods for analyzing the uncertainty of these esti-
mates. We demonstrate the accuracy of the method in a simple simulated
coin- ﬂipping setting; in particular, the method is able to p recisely track
the evolution of the subject’s posterior distribution as more and more data
are observed. We close by brieﬂy discussing an interesting c onnection to
recent models of neural population coding.

Introduction

Bayesian methods have become quite popular in psychophysics and neuroscience (1–5 ); in
particular, a recent trend has been to interpret observed biases in perception and/or behavior
as optimal, in a Bayesian (average) sense, under ecologically-determined prior distributions
on the stimuli or behavioral contexts under study. For example, (2) interpret visual motion
illusions in terms of a prior weighted towards slow, smooth movements of objects in space.

In an experimental context, it is clearly desirable to empirically obtain estimates of the
prior the subject is operating under; the idea would be to then compare these experimental
estimates of the subject’s prior with the ecological prior he or she “should” have been
using. Conversely, such an approach would have the potential to establish that the subject
is not behaving Bayes-optimally under any prior, but rather is in fact using a different, non-
Bayesian strategy. Such tools would also be quite useful in the context of studies of learning
and generalization, in which we would like to track the time course of a subject’s adaptation
to an experimentally-chosen prior distribution (5). Such estimates of the subject’s prior
have in the past been rather qualitative, and/or limited to simple parametric families (e.g.,

∗We thank N. Daw, P. Hoyer, S. Inati, K. Koerding, I. Nemenman, E. Simoncelli, A. Stocker, and D.
Wolpert for helpful suggestions, and in particular P. Dayan for pointing out the connection to neural
population coding models. This work was supported by funding from the Howard Hughes Medical
Institute, Gatsby Charitable Trust, and by a Royal Society International Fellowship.

the width of a Gaussian may be ﬁt to the experimental data, but
of the prior is not examined systematically).

the actual Gaussian identity

We present a more quantitative method here. We ﬁrst discuss t he method in the general case
of an arbitrarily-chosen loss function (the “cost ” which we
assume the subject is attempting
to minimize, on average), then examine a few special important cases (e.g., mean-square
and mean-absolute error) in which the technique may be simpli ﬁed somewhat. The algo-
rithms for determining the subject’s prior distributions turn out to be surprisingly quick and
easy to code: the basic idea is that each observed stimulus-response pair provides a set of
constraints on what the actual prior could be. In the simplest case, these constraints are
linear, and the resulting algorithm is simply a version of linear programming, for which
very efﬁcient algorithms exist. More generally, the constr aints are probabilistic, and we
discuss likelihood-based methods for combining these noisy constraints (and in particular
when the resulting maximum likelihood, or maximum a posteriori, problem can be solved
efﬁciently via ascent methods, without fear of getting trap ped in non-global local maxima).
Finally, we discuss Bayesian methods for representing the uncertainty in our estimates.

We should point out that related problems have appeared in the statistics literature, par-
ticularly under the subject of elicitation of expert opinion (6–8 ); in the machine learn-
ing literature, most recently in the area of “inverse reinfo rcement learning”
( 9); and in
the economics/ game theory literature on utility learning (10). The experimental eco-
nomics literature in particular is quite vast (where the relevance to gambling, price setting,
— expected utility-
etc. is discussed at length, particularly in settings in which “rational ”
maximizing — behavior seems to break down); see, e.g. Wakker
’s recent bibliography
(www1.fee.uva.nl/creed/wakker/refs/rfrncs.htm) for further references. Finally, it is worth
noting that the question of determining a subject’s (or more precisely, an opponent’s) pri-
ors in a gambling context — in particular, in the binary case o f whether or not an opponent
will accept a bet, given a ﬁxed table of outcomes vs. payoffs —
has received attention
going back to the foundations of decision theory, most prominently in the discussions of
de Finetti and Savage. Nevertheless, we are unaware of any previous application of simi-
lar techniques (both for estimating a subject’s true prior and for analyzing the uncertainty
associated with these estimates) in the psychophysical or neuroscience literature.

General case

Our technique for determining the subject’s prior is based on several assumptions (some of
which will be relaxed below). To begin, we assume that the subject is behaving optimally
in a Bayesian sense. To be precise, we have four ingredients: a prior distribution on some
hidden parameter θ ; observed input (stimulus) data, dependent in some probabilistic way
on θ ; the subject’s corresponding output estimates of the underlying θ , given the input
data; and ﬁnally a loss function D(., .) that penalizes bad estimates for θ . The fundamental
assumption is that, on each trial i, the subject is choosing the estimate ˆθi of the underlying
parameter, given data xi , to minimize the posterior average error
Z p(θ |xi )D( ˆθi , θ)dθ ∼ Z p(θ)p(xi |θ)D( ˆθi , θ)dθ ,
where p(θ) is the prior on hidden parameters (the unknown object the experimenter is trying
to estimate), and p(xi |θ) is the likelihood of data xi given θ . For example, in the visual
motion example, θ could be the true underlying velocity of an object moving through space,
the observed data xi could be a short, noise-contaminated movie of the object’s motion, and
the subject would be asked to estimate the true motion θ given the data xi and any prior
conceptions, p(θ), of how one expects objects to move. Note that we have also implicitly
assumed, in this simplest case, that both the loss D(., .) and likelihood functions p(xi |θ)
are known, both to the subject and to the experimenter (perhaps from a preceding set of

(1)

“learning” trials).

So how can the experimenter actually estimate p(θ), given the likelihoods p(x|θ), the loss
function D(., .), and some set of data {xi } with corresponding estimates { ˆθi} minimizing
the posterior expected loss (1)? This turns out to be a linear programming problem (11),
for which very efﬁcient algorithms exist (e.g., “linprog.m ” in Matlab). To see why, ﬁrst
note that the right hand side of expression (1) is linear in the prior p(θ). Second, we have a
large collection of linear constraints on p(θ): we know that

∀θ

p(θ) ≥ 0
Z p(θ)dθ = 1
Z p(θ)p(xi |θ)(cid:20)D( ˆθi , θ) − D(z , θ)(cid:21)dθ ≤ 0
where (2-3) are satis ﬁed by any proper prior distribution an d (4) is the maximizer condition
(1) expressed in slightly different language.
(See also (10), who noted the same linear
programming structure in an application to cost function estimation, rather than the prior
estimation examined here.)

(3)

(2)

(4)

∀z

The solution to the linear programming problem deﬁned by (2- 4) isn’t necessarily unique; it
corresponds to an intersection of half-spaces, which is convex in general. To come up with
a unique solution, we could maximimize a concave “regulariz ing” function on this convex
set; possible such functions include, e.g., the entropy of p(θ), or its negative mean-square
derivative (this function is strictly concave on the space of all functions whose integral is
held ﬁxed, as is the case here given constraint (3)); more gen erally, if we have some prior
information on the form of the priors the subject might be using, and this information can
be expressed in the “energy” form

P [p(θ)] ∼ eq [p(θ)] ,

for a concave functional q [.], we could use the log of this “prior on priors ” P . An alternative
solution would be to modify constraint (4) to
Z p(θ)p(xi |θ)(cid:20)D( ˆθi , θ) − D(z , θ)(cid:21) ≤ −ǫ
where we can then adjust the slack variable ǫ until the contraint set shrinks to a single
point. This leads directly to another linear programming problem (where we want to make
the linear function ǫ as large as possible, under the above constraints). Note that for this
last approach to work — for the linear programming problem to
have a solution — we need
to ensure that the set deﬁned by the constraints (2-4) is comp act; this basically means that
the constraint set (4) needs to be sufﬁciently rich, which, i n turn, means that sufﬁcient data
(or sufﬁciently strong prior constraints) are required. We will return to this point below.

∀z ,

Finally, what if our primary assumption is not met? That is, what if subjects are not quite
behaving optimally with respect to p(θ)? It is possible to detect this situation in the above
framework, for example if the slack variable ǫ above is found to be negative. However, a
different, more probabilistic viewpoint can be taken. Assume the value of the choice ˆθi is
optimal under some “comparison” noise, that is,
Z p(θ)p(xi |θ)(cid:20)D( ˆθi , θ) − D(z , θ)(cid:21) ≤ σηi (z )
with ηi (z ) a random variable of scale σ > 0 (assume η to be i.i.d. for now, although this
may be generalized). If we assume this decision noise η has a log-concave density (i.e.,
the log of the density is a concave function; e.g., Gaussian, or exponential), then so does

∀z ,

dp(η),

ui (z ) ≡

its integral (12), and the resulting maximum likelihood problem has no non-global local
maxima and is therefore solvable by ascent methods. To see this, write the log-likelihood
of (p, σ) given data {xi , ˆθi } as
L{xi , ˆθi } (p, σ) = X log Z ui (z)
−∞
with the sum over the set of all the constraints in (4) and
σ Z p(θ)p(xi |θ)(cid:20)D( ˆθi , θ) − D(z , θ)(cid:21).
1
L is the sum of concave functions in ui , and hence is concave itself, and has no non-global
local maxima in these variables; since σ and p are linearly related through ui (and (p, σ)
live in a convex set), L has no non-global local maxima in (p, σ), either. Once again, this
maximum likelihood problem may be regularized by prior information1 , maximizing the
a posteriori likelihood L(p) − q [p] instead of L(p); this problem is similarly tractable by
ascent methods, by the concavity of −q [.] (note that this “soft-constraint ” problem reduces
exactly to the “hard” constraint problem (4) as the noise σ → 0)2 .
Note that the estimated value of the noise scale σ plays a similar role to that of the slack
variable ǫ, above, with the difference that ǫ can be much more sensitive to the worst trial
(that is, the trial on which the subject behaves most suboptimally); we can use either of
these slack variables to go back and ask about how close to optimally the subjects were
actually performing — large values of
σ , for example, imply sub-optimal performance. An
additional interesting idea is to use the computed value of η as a kind of outlier test; η large
implies the trial was particularly suboptimal.

Special cases

Maximum a posteriori estimation: The maximum a posteriori (MAP) estimator corre-
sponds to the Hamming distance loss function,

this implies that the constraints (4) have the simple form

D(i, j ) = 1(i 6= j );

p( ˆθi ) − p(z )L( ˆθi , z ) ≥ 0,

with L( ˆθi , z ) deﬁned as the largest observed likelihood ratio for ˆθi and z , that is,

L( ˆθi , z ) ≡ max
xi

p(xi |z )
p(xi | ˆθi )

,

1Over ﬁtting here is a symptom of the fact that in some cases — particularly wh
en few data samples
have been observed — many priors (even highly implausible priors) can
explain the observed data
fairly well; in this case, it is often quite useful to penalize these “implausible” p riors, thus effectively
regularizing our estimates. Similar observations have appeared in the context of medical applications
of Markov random ﬁeld methods ( 13).
2Another possible application of this regularization idea is as follows. We may incorporate im-
proper priors — that is, priors which may not integrate to unity (such prior
s frequently arise in the
analysis of reparameterization-invariant decision procedures, for example) — without any major con-
ceptual modiﬁcation in our analysis, simply by removing the normalization co ntraint (3). However,
a problem arises: the zero measure, p(θ) ≡ 0, will always trivially satisfy the remaining constraints
(2) and (4). This problem could potentially be ameliorated by introducing a convex regularizing term
(or equivalently, a log-concave prior) on the total mass R p(θ)dθ .

with the maximum taken over all xi which led to the estimate ˆθi . This setup is perhaps
most appropriate for a two-alternative forced choice situation, where the problem is one of
classi ﬁcation or discrimination, not estimation.
Mean-square and absolute-error regression: Our discussion assumes an even simpler
form when the loss function D(., .) is taken to be squared error, D(x, y) = (x − y)2 , or
absolute error, D(x, y) = |x − y |. In this case it is convenient to work with a slightly
different noise model than the classi ﬁcation noise discuss ed above; instead, we may model
the subject’s responses as optimal plus estimation noise. For squared-error, the optimal
ˆθi is known to be uniquely deﬁned as the conditional mean of θ given xi . Thus we may
replace the collection of linear inequality constraints (4) with a much smaller set of linear
equalities (a single equality per trial, instead of a single inequality per trial per z ):
Z (cid:18)p(xi |θ)(θ − ˆθi )(cid:19)p(θ)dθ = σηi ;
the corresponding likelihood, again, has no non-global local maxima if η has a log-concave
density.
In the simplest case of Gaussian η , the maximum likelihood problem may be
solved by standard nonnegative least-squares (e.g., “lsqn onneg” or “quadprog” in Matlab).
In the absolute error case, the optimal ˆθi is given by the conditional median of θ given
xi (although recall that the median is not necessarily unique here); thus, the inequality
constraints (4) may again be replaced by equalities which are linear in p(θ):
p(θ)p(xi |θ) − Z ∞
Z
ˆθi
−∞
again, for Gaussian η this may be solved via standard nonnegative regression, albeit with a
different constraint matrix. In each case, ηi retains its utility as an outlier score.

p(θ)p(xi |θ) = σηi ;

(5)

ˆθi

A worked example: learning the fairness of a coin

In this section we will work through a concrete example, to show how to put the ideas
discussed above into practice. We take perhaps the simplest possible example, for clarity:
the subject observes some number N of independent, identically distributed coin ﬂips, and
on each trial i tells us his/her probability of observing tails on the next trial, given that
i trials3 . Here the likelihood functions p(xi |θ)
t = t(i) tails were observed in the ﬁrst
take the standard binomial form p(t(i)|ptails ) = (cid:0)i
tails (1 − ptails )i−t (note that it is
t(cid:1)pt
reasonable to assume that these likelihoods are known to the subject, at least approximately,
due to the ubiquity of binomial data).
Under our assumptions, the subject’s estimates ˆptails,i are given as the posterior mean
of ptails given the number of tails observed up to trial i. This puts us directly in the
mean-square framework discussed in equation (5); we assume Gaussian estimation noise η ,
construct a regression matrix A of N rows, with the i-th row given by p(t(i)|ptails )(ptails −
ˆptails,i ). To regularize our estimates, we add a small square-difference penalty of the form
q [p(θ)] = R |dp(θ)/dθ |2dθ . Finally, we estimate
min
ˆp(θ) = arg
p≥0;R 1
0 p(θ)dθ=1

||Ap||2
2 + ǫq [p],

for ǫ ≈ 10−7 ; this estimate is equivalent to MAP estimation under a (weak) Gaussian prior
on the function p(θ) (truncated so that p(θ) ≥ 0), and is computed using quadprog.m.

3We note in passing that this simple binomial paradigm has potential applications to ideal-
observer analysis of classical neuroscientiﬁc tasks (e.g., synaptic r elease detection, or photon count-
ing in retina) in addition to potential applications in psychophysics.

r
o
i
r
p
 
e
u
r
t

2

1

0

150

100

50

)
i
(
 
#
 
l
a
i
r
t

0

8
6
4
2
0

3

2

1

0

6

4

2

0

s
d
o
o
h
i
l
e
k
i
l

r
o
i
r
p
 
t
s
e

r
o
i
r
p
 
t
s
e

# tails/i
estimate

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

i=0
50
100

0.1

0.2

0.3
0.6
0.5
0.4
<−−−−−−−all heads  ...  all tails−−−−−−−−>

0.7

0.8

0.9

Figure 1: Learning the fairness of a coin (numerical simulation). Top panel: True prior distribution
on coin fairness. The bimodal nature of this prior indicates that the subject expects coins to be
unfair (skewed towards heads, ptails < .5, or tails, ptails > .5) more often than fair (ptails =
.5). Second: Observed data. Open circles indicate the fraction of observed tails t = t(i) as a
function of trial number i (the maximum likelihood estimate, MLE, of the fairness and a minimal
sufﬁcient statistic for this problem); + symbols indicate the subject’s estimate of the coin’s fairness,
assumed to correspond to the posterior mean of the fairness under the subject’s prior. Note the
systematic deviations of the subject’s estimate from the MLE; these deviations shrink as i increases
and the strength of the prior relative to the likelihood term decreases. Third: Binomial likelihood
terms `i
tails (1 − ptails )i−t . Color of trace correponds to trial number i, as indicated in previous
t´pt
panel (traces are normalized for clarity). Fourth: Estimate of prior given 150 trials. Black trace
indicates true prior (as in top panel); red indicates estimate ±1 posterior standard error (computed
via importance sampling). Bottom: Tracking the evolution of the posterior. Black traces indicate
the subject’s true posterior after observing 0 (thin trace), 50 (medium trace), and 100 (thick trace)
sample coin ﬂips; as more data are observed, the subject becomes mor e and more conﬁdent about
the true fairness of the coin (p = .5), and the posteriors match the likelihood terms (c.f. third panel)
more closely. Red traces indicate the estimated posterior given the full 150 or just the last 100 or 50
trials, respectively (errorbars omitted for visibility). Note that the procedure tracks the evolution of
the subject’s posterior quite accurately, given relatively few trials.

To place Bayesian conﬁdence intervals around our estimate, we sample from the corre-
sponding (truncated) Gaussian posterior distribution on p(θ) (via importance sampling with
a suitably shifted, rescaled truncated Gaussian proposal density; similar methods are ap-
plicable more generally in the non-Gaussian case via the usual posterior approximation
techniques, e.g. Laplace approximation). Figs. 1-2 demonstrate the accuracy of the es-
timated ˆp(θ); in particular, the bottom panels show that the method accurately tracks the
evolution of the model subjects’ posteriors as an increasing amount of data are observed.

r
o
i
r
p
 
e
u
r
t

3

2

1

0

50

0

10

)
i
(
 
#
 
l
a
i
r
t

s
d
o
o
h
i
l
e
k
i
l

150

100

# tails/i
estimate

5

0

4

2

0

6

4

2

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

i=0
50
100

0.1

0.2

0.3
0.6
0.5
0.4
<−−−−−−−all heads  ...  all tails−−−−−−−−>

0.7

0.8

0.9

r
o
i
r
p
 
t
s
e

r
o
i
r
p
 
t
s
e

Figure 2: Learning an unfair coin (ptails = .25). Conventions as in Fig. 1.

Connection to neural population coding

It is interesting to note a connection to the neural population coding model studied in (14)
(with more recent work reviewed in (15)). The basic idea is that neural populations encode
not just stimuli, but probability distributions over stimuli (where the distribution describes
the uncertainty in the state of the encoded object). Here the experimentally observed data
are neural ﬁring rates, which provide constraints on the und erlying encoded “prior ” distri-
bution in terms of the individual tuning function of each cell in the observed population.
The simplest model is as follows: the observed spikes ni from the i-th cell are Poisson-
distributed, with rate a nonlinear function of a linear functional of some prior distribution,
ni ∼ Poiss (cid:18)g (cid:18)Z p(θ)f (xi , θ)(cid:19)(cid:19) ,
where the kernel f is considered as the cell’s “tuning function”; the log-conc avity of the
likelihood of p is preserved for any nonlinearity g that is convex and log-concave, a class
including the linear recti ﬁers, exponentials, and power-l aws (and studied more extensively
in (16)). Alternately, a simpli ﬁed model is often used, e.g.:
ni ∼ q (cid:18) ni − R p(θ)f (xi , θ)
(cid:19) ,
σ
with q a log-concave density (typically Gaussian) to preserve the concavity of the log-
likelihood; in this case, the scale σ of the noise does not vary with the mean ﬁring rate,

as it does in the Poisson model. In both cases, the observed ﬁr ing rates act as constraints
oriented linearly with respect to p; in the latter case, the noise scale σ sets the strength, or
conﬁdence, of each such constraint ( 2, 3). Thus, under this framework, given the simul-
taneously recorded activity of many cells {ni } and some model for the tuning functions
f (xi , θ), we can infer p(θ) (and represent the uncertainty in these estimates) using meth-
ods quite similar to those developed above.

Directions

The obvious open avenue for future research (aside from application to experimental data)
is to relax the assumptions: that the likelihood and cost function are both known, and that
the data are observed directly (without any noise).
It seems fair to conjecture that the
subject can learn the likelihood and cost functions given enough data, but one would like to
test this directly, e.g. by estimating D(., .) and p together, perhaps under restrictions on the
form of D(., .). As emphasized above, the utility estimation problem has received a great
deal of attention, and it is plausible to expect that the methods proposed here for estimation
of the prior might be combined with previously-studied methods for utility elicitation and
estimation.
It is also interesting to consider these elicitation methods in the context of
experimental design (8, 17, 18), in which we might actively seek stimuli xi to maximally
constrain the possible form of the prior and/or cost function.

References

1. D. Knill, W. Richards, eds., Perception as Bayesian Inference (Cambridge University Press,
1996).
2. Y. Weiss, E. Simoncelli, E. Adelson, Nature Neuroscience 5, 598 (2002).
3. Y. Weiss, D. Fleet, Statistical Theories of the Cortex (MIT Press, 2002), chap. Velocity likeli-
hoods in biological and machine vision, pp. 77–96.
4. D. Kersten, P. Mamassian, A. Yuille, Annual Review of Psychology 55, 271 (2004).
5. K. Koerding, D. Wolpert, Nature 427, 244 (2004).
6. R. Hogarth, Journal of the American Statistical Association 70, 271 (1975).
7. J. Oakley, A. O’Hagan, Biometrika under review (2003).
8. P. Garthwaite, J. Kadane, A. O’Hagan, Handbook of Statistics (2004), chap. Elicitation.
9. A. Ng, S. Russell, ICML-17 (2000).
10. J. Blythe, AAAI02 (2002).
11. G. Strang, Linear algebra and its applications (Harcourt Brace, New York, 1988).
12. Y. Rinott, Annals of Probability 4, 1020 (1976).
13. M. Henrion, et al., Why is diagnosis using belief networks insensitive to imprecision in proba-
bilities?, Tech. Rep. SMI-96-0637 , Stanford (1996).
14. R. Zemel, P. Dayan, A. Pouget, Neural Computation 10, 403 (1998).
15. A. Pouget, P. Dayan, R. Zemel, Annual Reviews of Neuroscience 26, 381 (2003).
16. L. Paninski, Network: Computation in Neural Systems 15, 243 (2004).
17. K. Chaloner, I. Verdinelli, Statistical Science 10, 273 (1995).
18. L. Paninski, Advances in Neural Information Processing Systems 16 (2003).


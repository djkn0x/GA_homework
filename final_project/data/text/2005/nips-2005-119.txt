Diffusion Maps, Spectral Clustering and
Eigenfunctions of Fokker-Planck Operators

Boaz Nadler∗ St ´ephane Lafon Ronald R. Coifman
Department of Mathematics, Yale University, New Haven, CT 06520.
{boaz.nadler,stephane.lafon,ronald.coifman}@yale.edu

Ioannis G. Kevrekidis
Department of Chemical Engineering and Program in Applied Mathematics
Princeton University, Princeton, NJ 08544
yannis@princeton.edu

Abstract

This paper presents a diffusion based probabilistic interpretation of
spectral clustering and dimensionality reduction algorithms that use the
eigenvectors of the normalized graph Laplacian. Given the pairwise adja-
cency matrix of all points, we deﬁne a diffusion distance bet ween any two
data points and show that the low dimensional representation of the data
by the ﬁrst few eigenvectors of the corresponding Markov mat rix is opti-
mal under a certain mean squared error criterion. Furthermore, assuming
that data points are random samples from a density p(x) = e−U (x) we
identify these eigenvectors as discrete approximations of eigenfunctions
of a Fokker-Planck operator in a potential 2U (x) with reﬂecting bound-
ary conditions. Finally, applying known results regarding the eigenvalues
and eigenfunctions of the continuous Fokker-Planck operator, we provide
a mathematical justi ﬁcation for the success of spectral clu stering and di-
mensional reduction algorithms based on these ﬁrst few eige nvectors.
This analysis elucidates, in terms of the characteristics of diffusion pro-
cesses, many empirical ﬁndings regarding spectral cluster ing algorithms.

Keywords: Algorithms and architectures, learning theory.

1

Introduction

Clustering and low dimensional representation of high dimensional data are important
problems in many diverse ﬁelds. In recent years various spec tral methods to perform these
tasks, based on the eigenvectors of adjacency matrices of graphs on the data have been
developed, see for example [1]-[10] and references therein. In the simplest version, known
as the normalized graph Laplacian, given n data points {xi }n
i=1 where each xi ∈ Rp , we
deﬁne a pairwise similarity matrix between points, for exam ple using a Gaussian kernel

∗Corresponding author.
Currently at Weizmann Institute of Science, Rehovot,
http://www.wisdom.weizmann.ac.il/∼nadler

Israel.

with width ε,

Li,j = k(xi , xj ) = exp (cid:18)− kxi − xj k2
(cid:19)
(1)
2ε
and a diagonal normalization matrix Di,i = Pj Li,j . Many works propose to use the
ﬁrst few eigenvectors of the normalized eigenvalue problem Lφ = λDφ, or equivalently
of the matrix M = D−1L, either as a low dimensional representation of data or as good
coordinates for clustering purposes. Although eq. (1) is based on a Gaussian kernel, other
kernels are possible. While for actual datasets the choice of a kernel k(xi , xj ) is crucial, it
does not qualitatively change our asymptotic analysis [11].

The use of the ﬁrst few eigenvectors of M as good coordinates is typically justi ﬁed with
heuristic arguments or as a relaxation of a discrete clustering problem [3]. In [4, 5] Belkin
and Niyogi showed that when data is uniformly sampled from a low dimensional manifold
of Rp the ﬁrst few eigenvectors of M are discrete approximations of the eigenfunctions of
the Laplace-Beltrami operator on the manifold, thus providing a mathematical justi ﬁcation
for their use in this case. A different theoretical analysis of the eigenvectors of the matrix
M , based on the fact that M is a stochastic matrix representing a random walk on the graph
was described by Meil ˇa and Shi [12], who considered the case of piecewise constant eigen-
vectors for speci ﬁc lumpable matrix structures. Additiona l notable works that considered
the random walk aspects of spectral clustering are [8, 13], where the authors suggest clus-
tering based on the average commute time between points, and [14] which considered the
relaxation process of this random walk.

In this paper we provide a uni ﬁed probabilistic framework wh ich combines these results
and extends them in two different directions. First, in section 2 we deﬁne a distance func-
tion between any two points based on the random walk on the graph, which we naturally
denote the diffusion distance. We then show that the low dimensional description of the
data by the ﬁrst few eigenvectors, denoted as the diffusion map, is optimal under a mean
squared error criterion based on this distance. In section 3 we consider a statistical model,
in which data points are iid random samples from a probability density p(x) in a smooth
bounded domain Ω ⊂ Rp and analyze the asymptotics of the eigenvectors as the number of
data points tends to inﬁnity. This analysis shows that the ei genvectors of the ﬁnite matrix
M are discrete approximations of the eigenfunctions of a Fokker-Planck (FP) operator with
reﬂecting boundary conditions. This observation, coupled with known results regarding the
eigenvalues and eigenfunctions of the FP operator provide new insights into the properties
of these eigenvectors and on the performance of spectral clustering algorithms, as described
in section 4.

2 Diffusion Distances and Diffusion Maps

The starting point of our analysis, as also noted in other works, is the observation that the
matrix M is adjoint to a symmetric matrix
Ms = D1/2M D−1/2 .
(2)
Thus, M and Ms share the same eigenvalues. Moreover, since Ms is symmetric it is diag-
onalizable and has a set of n real eigenvalues {λj }n−1
j=0 whose corresponding eigenvectors
{v j } form an orthonormal basis of Rn . The left and right eigenvectors of M , denoted φj
and ψj are related to those of Ms according to
ψj = v j D−1/2
φj = v j D1/2 ,
(3)
Since the eigenvectors v j are orthonormal under the standard dot product in Rn , it follows
that the vectors φj and ψk are bi-orthonormal
hφi , ψj i = δi,j

(4)

(5)

where hu, vi is the standard dot product between two vectors in Rn . We now utilize the
fact that by construction M is a stochastic matrix with all row sums equal to one, and can
thus be interpreted as deﬁning a random walk on the graph. Und er this view, Mi,j denotes
the transition probability from the point xi to the point xj in one time step. Furthermore,
based on the similarity of the Gaussian kernel (1) to the fundamental solution of the heat
equation, we deﬁne our time step as ∆t = ε. Therefore,
Pr{x(t + ε) = xj | x(t) = xi} = Mi,j
Note that ε has therefore a dual interpretation in this framework. The ﬁrst is that ε is the
(squared) radius of the neighborhood used to infer local geometric and density information
for the construction of the adjacency matrix, while the second is that ε is the discrete time
step at which the random walk jumps from point to point.
We denote by p(t, y |x) the probability distribution of a random walk landing at location
y at time t, given a starting location x at time t = 0. For t = k ε, p(t, y |xi ) = eiM k ,
where ei is a row vector of zeros with a single one at the i-th coordinate. For ε large
enough, all points in the graph are connected so that M has a unique eigenvalue equal
to 1. The other eigenvalues form a non-increasing sequence of non-negative numbers:
λ0 = 1 > λ1 ≥ λ2 ≥ . . . ≥ λn−1 ≥ 0. Then, regardless of the initial starting point x,
p(t, y |x) = φ0 (y)
lim
t→∞
where φ0 is the left eigenvector of M with eigenvalue λ0 = 1, explicitly given by
Di,i
Pj
Dj,j
This eigenvector also has a dual interpretation. The ﬁrst is
that φ0 is the stationary proba-
bility distribution on the graph, while the second is that φ0 (x) is a density estimate at the
point x. Note that for a general shift invariant kernel K (x − y) and for the Gaussian kernel
in particular, φ0 is simply the well known Parzen window density estimator.
For any ﬁnite time t, we decompose the probability distribution in the eigenbasis {φj }
p(t, y |x) = φ0 (y) + Xj≥1
aj (x)λt
j φj (y)
where the coefﬁcients aj depend on the initial location x. Using the bi-orthonormality
condition (4) gives aj (x) = ψj (x), with a0 (x) = ψ0 (x) = 1 already implicit in (8).
Given the deﬁnition of the random walk on the graph it is only n atural to quantify the sim-
ilarity between any two points according to the evolution of their probability distributions.
Speci ﬁcally, we consider the following distance measure at
time t,
t (x0 , x1 ) = kp(t, y |x0 ) − p(t, y |x1 )k2
D2
w
= Xy
(p(t, y |x0 ) − p(t, y |x1 ))2w(y)
with the speci ﬁc choice w(y) = 1/φ0 (y) for the weight function, which takes into account
the (empirical) local density of the points.

φ0 (xi ) =

(6)

(7)

(8)

(9)

Since this distance depends on the random walk on the graph, we quite naturally denote it
as the diffusion distance at time t. We also denote the mapping between the original space
and the ﬁrst k eigenvectors as the diffusion map
2ψ2 (x), . . . , λt
1ψ1 (x), λt
Ψt (x) = (cid:0)λt
k ψk (x)(cid:1)
The following theorem relates the diffusion distance and the diffusion map.

(10)

Theorem: The diffusion distance (9) is equal to Euclidean distance in the diffusion map
space with all (n − 1) eigenvectors.
j (ψj (x0 ) − ψj (x1 ))2 = kΨt (x0 ) − Ψt (x1 )k2
t (x0 , x1 ) = Xj≥1
D2
λ2t
Proof: Combining (8) and (9) gives
j (ψj (x0 ) − ψj (x1 ))φj (y)(cid:17)2
t (x0 , x1 ) = Xy (cid:16) Xj
λt
D2
Expanding the brackets, exchanging the order of summation and using relations (3) and (4)
between φj and ψj yields the required result. Note that the weight factor 1/φ0 is essential
for the theorem to hold.
(cid:3).

1/φ0 (y)

(11)

(12)

This theorem provides a justi ﬁcation for using Euclidean di stance in the diffusion map
space for spectral clustering purposes. Therefore, geometry in diffusion space is meaning-
ful and can be interpreted in terms of the Markov chain. In particular, as shown in [18],
quantizing this diffusion space is equivalent to lumping the random walk. Moreover, since
in many practical applications the spectrum of the matrix M has a spectral gap with only
a few eigenvalues close to one and all additional eigenvalues much smaller than one, the
diffusion distance at a large enough time t can be well approximated by only the ﬁrst few
k eigenvectors ψ1 (x), . . . , ψk (x), with a negligible error of the order of O((λk+1 /λk )t ).
This observation provides a theoretical justi ﬁcation for d imensional reduction with these
eigenvectors. In addition, the following theorem shows that this k-dimensional approxima-
tion is optimal under a certain mean squared error criterion.
Theorem: Out of all k-dimensional approximations of the form

aj (t, x)wj (y)

ˆp(t, y |x) = φ0 (y) +

k
Xj=1
for the probability distribution at time t, the one that minimizes the mean squared error
x {kp(t, y |x) − ˆp(t, y |x)k2
w }
where averaging over initial points x is with respect to the stationary density φ0 (x), is
j ψj (x). Therefore, the optimal k-dimensional
given by wj (y) = φj (y) and aj (t, x) = λt
approximation is given by the truncated sum

E

ˆp(y , t|x) = φ0 (y) +

k
Xj=1
Proof: The proof is a consequence of a weighted principal component analysis applied to
the matrix M , taking into account the biorthogonality of the left and right eigenvectors.
We note that the ﬁrst few eigenvectors are also optimal under other criteria, for example for
data sampled from a manifold as in [4], or for multiclass spectral clustering [15].

λt
j ψj (x)φj (y)

(13)

3 The Asymptotics of the Diffusion Map

The analysis of the previous section provides a mathematical explanation for the success of
the diffusion maps for dimensionality reduction and spectral clustering. However, it does
not provide any information regarding the structure of the computed eigenvectors.

To this end, and similar to the framework of [16], we introduce a statistical model and
assume that the data points {xi } are i.i.d. random samples from a probability density p(x)

(14)

p(x, t)

conﬁned to a compact connected subset Ω ⊂ Rp with smooth boundary ∂Ω. Following
the statistical physics notation, we write the density in Boltzmann form, p(x) = e−U (x) ,
where U (x) is the (dimensionless) potential or energy of the conﬁgurat
ion x.
As shown in [11], in the limit n → ∞ the random walk on the discrete graph converges
to a random walk on the continuous space Ω. Then, it is possible to deﬁne forward and
backward operators Tf and Tb as follows,
Tf [φ](x) = ZΩ
M (x|y)φ(y)p(y)dy , Tb [ψ ](x) = ZΩ
M (y |x)ψ(y)p(y)dy
where M (x|y) = exp(−kx − yk2 /2ε)/D(y ) is the transition probability from y to x in
time ε, and D(y) = R exp(−kx − yk2 /2ε)p(x)dx.
The two operators Tf and Tb have probabilistic interpretations. If φ(x) is a probability
distribution on the graph at time t = 0, then Tf [φ] is the probability distribution at time
t = ε. Similarly, Tb [ψ ](x) is the mean of the function ψ at time t = ε, for a random walk
that started at location x at time t = 0. The operators Tf and Tb are thus the continuous
analogues of the left and right multiplication by the ﬁnite m atrix M .
We now take this analysis one step further and consider the limit ε → 0. This is possible,
since when n = ∞ each data point contains an inﬁnite number of nearby neighbo rs. In
this limit, since ε also has the interpretation of a time step, the random walk converges to a
diffusion process, whose probability density evolves continuously in time, according to
∂ p(x, t)
p(x, t + ε) − p(x, t)
Tf − I
= lim
= lim
∂ t
ε
ε
ε→0
ε→0
in which case it is customary to study the inﬁnitesimal gener ators (propagators)
Tb − I
Tf − I
(16)
Hb = lim
,
Hf = lim
ε
ε
ε→0
ε→0
Clearly, the eigenfunctions of Tf and Tb converge to those of Hf and Hb , respectively.
As shown in [11], the backward generator is given by the following Fokker-Planck operator
(17)
Hbψ = ∆ψ − 2∇ψ · ∇U
which corresponds to a diffusion process in a potential ﬁeld of 2U (x)
˙x(t) = −∇(2U ) + √2D ˙w(t)
(18)
where w(t) is standard Brownian motion in p dimensions and D is the diffusion coefﬁcient,
equal to one in equation (17). The Langevin equation (18) is a common model to describe
stochastic dynamical systems in physics, chemistry and biology [19, 20]. As such, its
characteristics as well as those of the corresponding FP equation have been extensively
studied, see [19]-[22] and many others. The term ∇ψ · ∇U in (17) is interpreted as a drift
term towards low energy (high-density) regions, and as discussed in the next section, may
play a crucial part in the deﬁnition of clusters.
Note that when data is uniformly sampled from Ω, ∇U = 0 so the drift term vanishes and
we recover the Laplace-Beltrami operator on Ω. The connection between the discrete ma-
trix M and the (weighted) Laplace-Beltrami or Fokker-Planck operator, as well as rigorous
convergence proofs of the eigenvalues and eigenvectors of M to those of the integral opera-
tor Tb or inﬁnitesimal generator Hb were considered in many recent works [4, 23, 17, 9, 24].
However, it seems that the important issue of boundary conditions was not considered.
Since (17) is deﬁned in the bounded domain Ω, the eigenvalues and eigenfunctions of Hb
depend on the boundary conditions imposed on ∂Ω. As shown in [9], in the limit ε → 0,
the random walk satis ﬁes reﬂecting boundary conditions on
∂Ω, which translate into
∂ψ(x)
∂n (cid:12)(cid:12)(cid:12)∂Ω

(15)

= 0

(19)

Table 1: Random Walks and Diffusion Processes
Stochastic Process
Operator
Case
ﬁnite n × n
R.W. discrete in space
ε > 0
discrete in time
matrix M
n < ∞
operators
R.W. in continuous space
ε > 0
discrete in time
n → ∞
Tf , Tb
diffusion process
inﬁnitesimal
ε → 0
n = ∞ generator Hf
continuous in time & space

where n is a unit normal vector at the point x ∈ ∂Ω.
To conclude, the left and right eigenvectors of the ﬁnite mat rix M can be viewed as discrete
approximations to those of the operators Tf and Tb , which in turn can be viewed as approx-
imations to those of Hf and Hb . Therefore, if there are enough data points for accurate
statistical sampling, the structure and characteristics of the eigenvalues and eigenfunctions
of Hb are similar to the corresponding eigenvalues and discrete eigenvectors of M . For
convenience, the three different stochastic processes are shown in table 1.

4 Fokker-Planck eigenfunctions and spectral clustering

According to (16), if λε is an eigenvalue of the matrix M or of the integral operator Tb based
on a kernel with parameter ε, then the corresponding eigenvalue of Hb is µ ≈ (λε − 1)/ε.
Therefore the largest eigenvalues of M correspond to the smallest eigenvalues of Hb . These
eigenvalues and their corresponding eigenfunctions have been extensively studied in the
literature under various settings. In general, the eigenvalues and eigenfunctions depend
both on the geometry of the domain Ω and on the proﬁle of the potential U (x). For clarity
and due to lack of space we brieﬂy analyze here two extreme cas es. In the ﬁrst case Ω = Rp
so geometry plays no role, while in the second U (x) = const so density plays no role.
Yet we show that in both cases there can still be well deﬁned cl usters, with the unifying
probabilistic concept being that the mean exit time from one cluster to another is much
larger than the characteristic equilibration time inside each cluster.
Case I: Consider diffusion in a smooth potential U (x) in Ω = Rp , where U has a few local
minima, and U (x) → ∞ as kxk → ∞ fast enough so that R e−U dx = 1 < ∞. Each such
local minimum thus deﬁnes a metastable state, with transiti ons between metastable states
being relatively rare events, depending on the barrier heights separating them. As shown
in [21, 22] (and in many other works) there is an intimate connection between the smallest
eigenvalues of Hb and mean exit times out of these metastable states. Speci ﬁca lly, in the
asymptotic limit of small noise D (cid:28) 1, exit times are exponentially distributed and the ﬁrst
non-trivial eigenvalue (after µ0 = 0) is given by µ1 = 1/ ¯τ where ¯τ is the mean exit time to
overcome the highest potential barrier on the way to the deepest potential well. For the case
of two potential wells, for example, the corresponding eigenfunction is roughly constant
in each well with a sharp transition near the saddle point between the wells. In general,
in the case of k local minima there are asymptotically only k eigenvalues very close to
zero. Apart from µ0 = 0, each of the other k − 1 eigenvalues corresponds to the mean
exit time from one of the wells into the deepest one, with the corresponding eigenfunctions
being almost constant in each well. Therefore, for a ﬁnite da taset the presence of only k
eigenvalues close to 1 with a spectral gap, e.g. a large difference between λk and λk+1
is indicative of k well deﬁned global clusters. In ﬁgure 1 (left) an example of this case is
shown, where p(x) is the sum of two well separated Gaussian clouds leading to a double
well potential. Indeed there are only two eigenvalues close or equal to 1 with a distinct
spectral gap and the ﬁrst eigenfunction being almost piecew ise constant in each well.

1

0

−1

−2

−1

0

1

1

0.8

0.6

1

0

−1

2

4

6

−2

−1

0

1

0.8
0.6
0.4
0.2

5

0

−5
−5

1
0
−1
−2−1 0 1

0

−5

5

2

4

6

−1

0

1

2

4

6

0

−1

1

0.9

0.8

1

0

−1

2 Gaussians

3 Gaussians

Uniform density
1

0

5

−1

0

1

Figure 1: Diffusion map results on different datasets. Top - the datasets. Middle - the
eigenvalues. Bottom - the ﬁrst eigenvector vs. x1 or the ﬁrst and second eigenvectors for
the case of three Gaussians.

In stochastic dynamical systems a spectral gap corresponds to a separation of time scales
between long transition times from one well or metastable state to another as compared
to short equilibration times inside each well. Therefore, clustering and identi ﬁcation of
metastable states are very similar tasks, and not surprisingly algorithms similar to the nor-
malized graph Laplacian have been independently developed in the literature [25].

The above mentioned results are asymptotic in the small noise limit. In practical datasets,
there can be clusters of different scales, where a global analysis with a single ε is not suit-
able. As an example consider the second dataset in ﬁgure 1, wi
th three clusters. While
the ﬁrst eigenvector distinguishes between the large clust er and the two smaller ones, the
second eigenvector captures the equilibration inside the large cluster instead of further dis-
tinguishing the two small clusters. While a theoretical explanation is beyond the scope of
this paper, a possible solution is to choose a location dependent ε, as proposed in [26].
Case II: Consider a uniform density in a region Ω ⊂ R3 composed of two large con-
tainers connected by a narrow circular tube, as in the top right frame in ﬁgure 1. In this
case U (x) = const, so the second term in (17) vanishes. As shown in [27], the second
eigenvalue of the FP operator is extremely small, of the order of a/V where a is the radius
of the connecting tube and V is the volume of the containers, thus showing an interesting
connection to the Cheeger constant on graphs. The corresponding eigenfunction is almost
piecewise constant in each container with a sharp transition in the connecting tube. Even
though in this case the density is uniform, there still is a spectral gap with two well deﬁned
clusters (the two containers), deﬁned entirely by the geome try of Ω. An example of such a
case and the results of the diffusion map are shown in ﬁgure 1 ( right).

In summary the eigenfunctions and eigenvalues of the FP operator, and thus of the cor-
responding ﬁnite Markov matrix, depend on both geometry and density. The diffusion
distance and its close relation to mean exit times between different clusters is the quantity
that incorporates these two features. This provides novel insight into spectral clustering
algorithms, as well as a theoretical justi ﬁcation for the al gorithm in [13], which deﬁnes
clusters according to mean travel times between points on the graph. A similar analysis
could also be applied to semi-supervised learning based on spectral methods [28]. Finally,
these eigenvectors may be used to design better search and data collection protocols [29].
Acknowledgments: The authors thank Mikhail Belkin and Partha Niyogi for interesting
discussions. This work was partially supported by DARPA through AFOSR.

References
[1] B. Sch ¨olkopf, A. Smola and K.R. M ¨uller. Nonlinear component analysis as a kernel eigenvalue
problem, Neural Computation 10, 1998.
[2] Y. Weiss. Segmentation using eigenvectors: a unifying view. ICCV 1999.
[3] J. Shi and J. Malik. Normalized cuts and image segmentation, PAMI, Vol. 22, 2000.
[4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering, NIPS Vol. 14, 2002.
[5] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-
sentation, Neural Computation 15:1373-1396, 2003.
[6] A.Y. Ng, M. Jordan and Y. Weiss. On spectral clustering, analysis and an algorithm, NIPS Vol.
14, 2002.
[7] X. Zhu, Z. Ghahramani, J. Lafferty, Semi-supervised learning using Gaussian ﬁelds and har-
monic functions, Proceedings of the 20th international conference on machine learning, 2003.
[8] M. Saerens, F. Fouss, L. Yen and P. Dupont, The principal component analysis of a graph and
its relationships to spectral clustering. ECML 2004.
[9] R.R. Coifman, S. Lafon, Diffusion Maps, to appear in Appl. Comp. Harm. Anal.
[10] R.R. Coifman & al., Geometric diffusion as a tool for harmonic analysis and structure deﬁn i-
tion of data, parts I and II, Proc. Nat. Acad. Sci., 102(21):7426-37 (2005).
[11] B. Nadler, S. Lafon, R.R. Coifman, I. G. Kevrekidis, Diffusion maps, spectral clustering,
and the reaction coordinates of dynamical systems, to appear in Appl. Comp. Harm. Anal.,
available at http://arxiv.org/abs/math.NA/0503445.
[12] M. Meila, J. Shi. A random walks view of spectral segmentation, AI and Statistics, 2001.
[13] L. Yen L., Vanvyve D., Wouters F., Fouss F., Verleysen M. and Saerens M. , Clustering using
a random-walk based distance measure. ESANN 2005, pp 317-324.
[14] N. Tishby, N. Slonim, Data Clustering by Markovian Relaxation and the information bottle-
neck method, NIPS, 2000.
[15] S. Yu and J. Shi. Multiclass spectral clustering. ICCV 2003.
[16] Y. Bengio et. al, Learning eigenfunctions links spectral embedding and kernel PCA, Neural
Computation, 16:2197-2219 (2004).
[17] U. von Luxburg, O. Bousquet, M. Belkin, On the convergence of spectral clustering on random
samples: the normalized case, NIPS, 2004.
[18] S. Lafon, A.B. Lee, Diffusion maps: A uniﬁed framework for d imension reduction, data par-
titioning and graph subsampling, submitted.
[19] C.W. Gardiner, Handbook of stochastic methods, third edition, Springer NY, 2004.
[20] H. Risken, The Fokker Planck equation, 2nd edition, Springer NY, 1999.
[21] B.J. Matkowsky and Z. Schuss, Eigenvalues of the Fokker-Planck operator and the approach
to equilibrium for diffusions in potential ﬁelds, SIAM J. App. Math. 40(2):242-254 (1981).
[22] M. Eckhoff, Precise asymptotics of small eigenvalues of reversible diffusions in the metastable
regime, Annals of Prob. 33:244-299, 2005.
[23] M. Belkin and P. Niyogi, Towards a theoeretical foundation for Laplacian-based manifold
methods, COLT 2005 (to appear).
[24] M. Hein, J. Audibert, U. von Luxburg, From graphs to manifolds - weak and strong pointwise
consistency of graph Laplacians, COLT 2005 (to appear).
[25] W. Huisinga, C. Best, R. Roitzsch, C. Sch ¨utte, F. Cordes, From simulation data to conforma-
tional ensembles, structure and dynamics based methods, J. Comp. Chem. 20:1760-74, 1999.
[26] L. Zelnik-Manor, P. Perona, Self-Tuning spectral clustering, NIPS, 2004.
[27] A. Singer, Z. Schuss, D. Holcman and R.S. Eisenberg, narrow escape, part I, submitted.
[28] D. Zhou & al., Learning with local and global consistency, NIPS Vol. 16, 2004.
[29] I.G. Kevrekidis, C.W. Gear, G. Hummer, Equation-free: The computer-aided analysis of com-
plex multiscale systems, Aiche J. 50:1346-1355, 2004.


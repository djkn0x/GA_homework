Computing the Solution Path for the
Regularized Support Vector Regression

Lacey Gunter
Department of Statistics
University of Michigan
Ann Arbor, MI 48109
lgunter@umich.edu

Ji Zhu∗
Department of Statistics
University of Michigan
Ann Arbor, MI 48109
jizhu@umich.edu

Abstract

In this paper we derive an algorithm that computes the entire solu-
tion path of the support vector regression, with essentially the same
computational cost as ﬁtting one SVR model. We also propose an
unbiased estimate for the degrees of freedom of the SVR model,
which allows convenient selection of the regularization parameter.

1 Introduction

The support vector regression (SVR) is a popular tool for function estimation prob-
lems, and it has been widely used on many real applications in the past decade, for
example, time series prediction [1], signal processing [2] and neural decoding [3].
In this paper, we focus on the regularization parameter of the SVR, and propose
an eﬃcient algorithm that computes the entire regularized solution path; we also
propose an unbiased estimate for the degrees of freedom of the SVR, which allows
convenient selection of the regularization parameter.
Suppose we have a set of training data (x1 , y1 ), . . . , (xn , yn ), where the input xi ∈
Rp and the output yi ∈ R. Many researchers have noted that the formulation for
n(cid:2)
(cid:3)(cid:3)yi − β0 − βTxi
(cid:3)(cid:3)
the linear -SVR can be written in a loss + penalty form [4]:
 + λ
min
2 βTβ
β0 ,β
(cid:4)
i=1
where |ξ | is the so called -insensitive loss function:
if |ξ | ≤ 
|ξ | =
0
|ξ | − 
otherwise
The idea is to disregard errors as long as they are less than . Figure 1 plots the loss
function. Notice that it has two non-diﬀerentiable points at ±. The regularization
parameter λ controls the trade-oﬀ between the -insensitive loss and the complexity
of the ﬁtted model.
∗

(1)

To whom the correspondence should be addressed.

5
.
2

0
.
2

5
.
1

s
s
o
L

0
.
1

5
.
0

0
.
0

−3

Left

Right

Center

Elbow L

Elbow R

−2

−1

0

y−f
Figure 1: The -insensitive loss function.

1

2

3

min
β0 ,θ

In practice, one often maps x into a high (often inﬁnite) dimensional reproducing
n(cid:2)
n(cid:2)
n(cid:2)
kernel Hilbert space (RKHS), and ﬁts a nonlinear kernel SVR model [4]:
|yi − f (xi )|
1
(cid:5)
 +
θi θi(cid:2) K (xi , xi(cid:2) )
(2)
2λ
i(cid:2)=1
i=1
i=1
i=1 θiK (x, xi ), and K (·, ·) is a positive-deﬁnite reproducing
n
where f (x) = β0 + 1
λ
kernel that generates a RKHS. Notice that we write f (x) in a way that involves λ
explicitly, and we will see later that θi ∈ [−1, 1].
Both (1) and (2) can be transformed into a quadratic programming problem, hence
most commercially available packages can be used to solve the SVR. In the past
years, many speciﬁc algorithms for the SVR have also been developed, for example,
interior point algorithms [4-5], subset selection algorithms [6–7], and sequential
minimal optimization [4, 8–9]. All these algorithms solve the SVR for a pre-ﬁxed
regularization parameter λ, and it is well known that an appropriate value of λ is
crucial for achieving small prediction error of the SVR.
In this paper, we show that the solution θ(λ) is piecewise linear as a function of
λ, which allows us to derive an eﬃcient algorithm that computes the exact entire
solution path {θ(λ), 0 ≤ λ ≤ ∞}. We acknowledge that this work was inspired by
one of the authors’ earlier work on the SVM setting [10].
Before delving into the technical details, we illustrate the concept of piecewise linear-
ity of the solution path with a simple example. We generate 10 training observations
using the famous sinc(·) function:
+ e, where x ∼ U (−2π , 2π) and e ∼ N (0, 0.192 )
sin(πx)
πx
We use the SVR with a 1-dimensional spline kernel
) − k4 (|x − x
(cid:3) |)
(cid:3)
(cid:3)
(cid:3)
K (x, x
(3)
) + k2 (x)k2 (x
) = 1 + k1 (x)k1 (x
where k1 (·) = · − 1/2, k2 = (k2
− k2
− 1/12)/2, k4 = (k4
1 /2 + 7/240)/24. Figure 2
1
1
shows a subset of the piecewise linear solution path θ(λ) as a function of λ.
In section 2, we describe the algorithm that computes the entire solution path of
the SVR. In section 3, we propose an unbiased estimate for the degrees of freedom
of the SVR, which can be used to select the regularization parameter λ. In section
4, we present numerical results on simulation data. We conclude the paper with a
discussion section.

y =

0
.
1

5
.
0

θ

0
.
0

5
.
0

0
.
1

1

2

3
λ

4

5

Figure 2: A subset of the solution path θ (λ) as a function of λ.

2 Algorithm

LP :

ξi , δi ≥ 0;

2.1 Problem Setup

For simplicity in notation, we describe the problem setup using the linear SVR, and
the algorithm using the kernel SVR.
n(cid:2)
The linear -SVR (1) can be re-written in an equivalent way:
(ξi + δi ) + λ
min
2 βTβ
β0 ,β
i=1
−(δi + ) ≤ yi − f (xi ) ≤ (ξi + ),
sub ject to
n(cid:2)
n(cid:2)
f (xi ) = β0 + βTxi ,
i = 1, . . . n
This gives us the Lagrangian primal function
αi (yi − f (xi ) − ξi − ) −
(ξi + δi ) + λ
ρi ξi − n(cid:2)
γi (yi − f (xi ) + δi + ) − n(cid:2)
n(cid:2)
2 βTβ +
i=1
i=1
n(cid:2)
i=1
i=1
i=1
Setting the derivatives to zero we arrive at:
(αi − γi )xi
1
∂
n(cid:2)
n(cid:2)
β =
∂β
λ
i=1
i=1
i=1
αi = 1 − ρi
∂
∂ ξi
γi = 1 − τi
∂
∂ δi
where the Karush-Kuhn-Tucker conditions are
αi (yi − f (xi ) − ξi − ) = 0
γi (yi − f (xi ) + δi + ) = 0
ρi ξi = 0
τi δi = 0

(8)
(9)
(10)
(11)

∂
∂β0

:

:

:

:

αi =

(4)

(5)

(6)

(7)

τi δi .

γi

−
−
Along with the constraint that our Lagrange multipliers must be non-negative, we
can conclude from (6) and (7) that both 0 ≤ αi ≤ 1 and 0 ≤ γi ≤ 1. We also see
from (8) and (9) that if αi is positive, then γi must be zero, and vice versa. These
lead to the following relationships:
yi − f (xi ) > 
⇒ αi = 1,
⇒ αi = 0,
yi − f (xi ) < −
yi − f (xi ) ∈ (−, ) ⇒ αi = 0,
yi − f (xi ) = 
⇒ αi ∈ [0, 1],
yi − f (xi ) = −
⇒ αi = 0,

γi = 0,
γi = 1,
γi = 0,
γi = 0,
γi ∈ [0, 1],

ξi > 0,
ξi = 0,
ξi = 0,
ξi = 0,
ξi = 0,

δi = 0;
δi > 0;
δi = 0;
δi = 0;
δi = 0.

Using these relationships, we deﬁne the following sets that will be used later on
when we are calculating the regularization path of the SVR:
• R = {i : yi − f (xi ) > , αi = 1, γi = 0} (Right of the elbows)
• ER = {i : yi − f (xi ) = , 0 ≤ αi ≤ 1, γi = 0} (Right elbow)
• C = {i : − < yi − f (xi ) < , αi = 0, γi = 0} (Center)
• EL = {i : yi − f (xi ) = −, αi = 0, 0 ≤ γi ≤ 0} (Left elbow)
• L = {i : yi − f (xi ) < −, αi = 0, γi = 1} (Left of the elbows)
Notice from (4) that for every λ, β is fully determined by the values of αi and γi .
For points in R, L and C , the values of αi and γi are known; therefore, the algorithm
will focus on points resting at the two elbows ER and EL .

Initialization
2.2
Initially, when λ = ∞ we can see from (4) that β = 0. We can determine the value
of β0 via a simple 1-dimensional optimization. For lack of space, we focus on the
case that all the values of yi are distinct, and furthermore, the initial sets ER and
EL have at most one point combined (which is the usual situation). In this case β0
will not be unique and each of the αi and γi will be either 0 or 1.
Since β0 is not unique, we can focus on one particular solution path, for example,
by always setting β0 equal to one of its boundary values (thus keeping one point
at an elbow). As λ decreases, the range of β0 shrinks toward zero and reaches zero
when we have two points at the elbows, and the algorithm proceeds from there.

2.3 The Path

The formalized setup above can be easily modiﬁed to accommodate non-linear ker-
nels; in fact, θi in (2) is equal to αi − γi . For the remaining portion of the algorithm
we will use the kernel notation.
The algorithm focuses on the sets of points ER and EL . These points have either
f (xi ) = yi −  with αi ∈ [0, 1], or f (xi ) = yi +  with γi ∈ [0, 1]. As we follow the
path we will examine these sets until one or both of them change, at which point
we will say an event has occurred. Thus events can be categorized as:

1. The initial event, for which two points must enter the elbow(s)
2. A point from R has just entered ER , with αi initially 1
3. A point from L has just entered EL , with γi initially 1
4. A point from C has just entered ER , with αi initially 0

f (x) =

f (x) =

=

5. A point from C has just entered EL , with γi initially 0
6. One or more points in ER and/or EL have just left the elbow(s) to join
either R, L, or C , with αi and γi initially 0 or 1
Until another event has occurred, all sets will remain the same. As a point passes
through ER or EL , its respective αi or γi must change from 0 → 1 or 1 → 0. Relying
on the fact that f (xi ) = yi − or f (xi ) = yi + for all points in ER or EL respectively,
we can calculate αi and γi for these points.
We use the subscript (cid:14) to index the sets above immediately after the (cid:14)th event has
occurred, and let α(cid:5)
0 and λ(cid:5) be the parameter values immediately after the
i , β (cid:5)
i , γ (cid:5)
(cid:7)
(cid:6)
(cid:14)th event. Also let f (cid:5) be the function at this point. We deﬁne for convenience
β0,λ = λ · β0 and hence β (cid:5)
0,λ = λ(cid:5) · β (cid:5)
n(cid:2)
0 . Then since
(αi − γi )K (x, xi ) + β0,λ
1
λ
(cid:9)
i=1
f (cid:5) (x)

(cid:8)
for λ(cid:5)+1 < λ < λ(cid:5) we can write
⎡⎣ (cid:2)
⎤⎦ ,
f (x) − λ(cid:5)
+ λ(cid:5)
(cid:2)
f (cid:5) (x)
λ
λ
νiK (x, xi ) −
1
ωj K (x, xj ) + ν0 + λ(cid:5) f (cid:5) (x)
λ
i∈E (cid:2)R
j∈E (cid:2)L
j and ν0 = β0,λ − β (cid:5)
i , ωj = γj − γ (cid:5)
where νi = αi − α(cid:5)
0,λ , and we can do the reduction
in the second line since the αi and γi are ﬁxed for all points in R(cid:5) , L(cid:5) , and C (cid:5) and
all points remain in their respective sets. Suppose |E (cid:5)R | = n(cid:5)
R and |E (cid:5)L | = n(cid:5)
⎡⎣ (cid:2)
⎤⎦ = λ − λ(cid:5) , ∀k ∈ E (cid:5)R
L , so for
(cid:2)
R + n(cid:5)
L points staying at the elbows we have (after some algebra) that
the n(cid:5)
1
⎡⎣ (cid:2)
⎤⎦ = λ − λ(cid:5) , ∀m ∈ E (cid:5)L
yk − 
(cid:2)
j∈E (cid:2)L
i∈E (cid:2)R
νiK (xm , xi ) −
1
ym + 
i∈E (cid:2)R
j∈E (cid:2)L
Also, by condition (5) we have that(cid:2)
i∈E (cid:2)R

ωj K (xm , xj ) + ν0
(cid:2)
j∈E (cid:2)L

νiK (xk , xi ) −

ωj K (xk , xj ) + ν0

νi −

ωj = 0

This gives us n(cid:5)
R + n(cid:5)
L + 1 linear equations we can use to solve for each of the
L + 1 unknown variables νi , ωj and ν0 . Notice this system is linear in λ − λ(cid:5) ,
R + n(cid:5)
n(cid:5)
which implies that αi , γj and β0,λ change linearly in λ − λ(cid:5) . So we can write:
i + (λ − λ(cid:5) )bi ∀i ∈ E (cid:5)R
αi = α(cid:5)
j + (λ − λ(cid:5) )bj ∀j ∈ E (cid:5)L
γj = γ (cid:5)
(cid:14)
(cid:15)
0,λ + (λ − λ(cid:5) )b0
β0,λ = β (cid:5)
f (cid:5) (x) − h(cid:5) (x)
f (x) = λ(cid:5)
λ

(12)
(13)
(14)

+ h(cid:5) (x)

(15)

(cid:2)
(cid:2)
where (bi , bj , b0 ) is the solution when λ − λ(cid:5) is equal to 1, and
biK (x, xi ) −
bj K (x, xj ) + b0 .
h(cid:5) (x) =
j∈E (cid:2)L
i∈E (cid:2)R
Given λ(cid:5) , equations (12), (13) and (15) allow us to compute the λ at which the next
event will occur, λ(cid:5)+1 . This will be the largest λ less than λ(cid:5) , such that either αi
for i ∈ E (cid:5)R reaches 0 or 1, or γj for j ∈ E (cid:5)L reaches 0 or 1, or one of the points in R,
L or C reaches an elbow.
We terminate the algorithm either when the sets R and L become empty, or when
In the later case we must have f (cid:5) − h(cid:5)
λ has become suﬃciently close to zero.
suﬃciently small as well.

2.4 Computational cost

The ma jor computational cost for updating the solutions at any event (cid:14) involves two
things: solving the system of (n(cid:5)
R + n(cid:5)
L ) linear equations, and computing h(cid:5) (x). The
R + n(cid:5)
L )2 ) calculations by using inverse updating and downdating
former takes O((n(cid:5)
since the elbow sets usually diﬀer by only one point between consecutive events,
and the latter requires O(n(n(cid:5)
R + n(cid:5)
L )) computations.
(cid:17)
(cid:16)
According to our experience, the total number of steps taken by the algorithm is on
average some small multiple of n. Letting m be the average size of E (cid:5)R ∪ E (cid:5)L , then
, which is
cn2m + nm2
the approximate computational cost of the algorithm is O
comparable to a single SVR ﬁtting algorithm that uses quadratic programming.

3 The Degrees of Freedom

The degrees of freedom is an informative measure of the complexity of a ﬁtted model.
In this section, we propose an unbiased estimate for the degrees of freedom of the
SVR, which allows convenient selection of the regularization parameter λ.
Since the usual goal of regression analysis is to minimize the predicted squared-error
loss, we study the degrees of freedom using Stein’s unbiased risk estimation (SURE)
theory [11]. Given x, assuming y is generated according to a homoskedastic model:
y ∼ (μ(x), σ2 )
where μ is the true mean and σ2 is the common variance. Then the degrees of
n(cid:2)
freedom of a ﬁtted model f (x) can be deﬁned as
(cid:5)
cov(f (xi ), yi )/σ2
(cid:5)
i=1
n
i=1 ∂ fi /∂ yi is an unbiased estimate of
Stein showed that under mild conditions,
n
df (f ). It turns out that for the SVR model, for every ﬁxed λ,
i=1 ∂ fi /∂ yi has an
(cid:18)df ≡ n(cid:2)
extremely simple formula:
= |ER | + |EL |
∂ fi
(16)
∂ yi
i=1
Therefore, |ER | + |EL | is a convenient unbiased estimate for the degrees of freedom
of f (x). Due to the space restriction, we omit the proof here, but make a note that
the proof relies on our SVR algorithm.

df (f ) =

criterion [12] for model selection:(cid:5)
In applying (16) to select the regularization parameter λ, we plug it into the GCV
(n − (cid:18)df )2
i=1 (yi − f (xi ))2
n
The advantages of this criterion are that it does not assume a known σ2 , and it
avoids cross-validation, which is computationally intensive. In practice, we can ﬁrst
use our eﬃcient algorithm to compute the entire solution path, then identify the
appropriate value of λ that minimizes the GCV criterion.

4 Numerical Results

To demonstrate our algorithm and the selection of λ using the GCV criterion, we
show numerical results on simulated data. We consider both additive and multi-
p(cid:2)
p(cid:19)
plicative kernels using the 1-dimensional spline kernel (3), which are respectively
j=1
j=1

(cid:3)
K (xj , x
j )

(cid:3)
K (xj , x
j )

and K (x, x

) =

(cid:3)

(cid:3)

K (x, x

) =

Simulations were based on the following four functions [13]:
πx + e1 , x ∈ (−2π , 2π)
1. f (x) = sin(πx)
(cid:21)
(cid:20)
(cid:17)
(cid:16)
−20(x2−.5) + 3x3 + 2x4 + x5 + e2 , x ∈ (0, 1)2
2. f (x) = 0.1e4x1 +
1
1+e
(cid:20)
(cid:21)
1/2
2
+ e3 ,
3. f (R, ω , L, C ) =
R2 +
ωL + 1
ωC
4. f (R, ω , L, C ) = tan−1
ωL+ 1
+ e4 ,
ωC
R
where (R, ω , L, C ) ∈ (0, 100) × (2π(20, 280)) × (0, 1) × (1, 11)
ei are distributed as N (0, σ2
i ), where σ1 = 0.19, σ2 = 1, σ3 = 218.5, σ4 = 0.18.
We generated 300 training observations from each function along with 10,000 vali-
dation observations and 10,000 test observations. For the ﬁrst two simulations we
used the additive 1-dimensional spline kernel and for the second two simulations
the multiplicative 1-dimensional spline kernel. We then found the λ that minimized
the GCV criterion. The validation set was used to select the gold standard λ which
minimized the prediction MSE. Using these λ’s we calculated the prediction MSE
with the test data for each criterion. After repeating this for 20 times, the average
MSE and standard deviation for the MSE can be seen in Table 1, which indicates
the GCV criterion performs closely to optimal.

Table 1: Simulation results of λ selection for SVR
f (x) MSE-Gold Standard MSE-GCV
0.0389 (0.0011)
0.0385 (0.0011)
1
2
1.1120 (0.0382)
1.0999 (0.0367)
3
50095 (1358)
50982 (2205)
4
0.0471 (0.0028)
0.0459 (0.0023)

5 Discussion

In this paper, we have proposed an eﬃcient algorithm that computes the entire reg-
ularization path of the SVR. We have also proposed the GCV criterion for selecting
the best λ given the entire path. The GCV criterion seems to work suﬃciently well
on the simulation data. However, we acknowledge that according to our experience
on real data sets (not shown here due to lack of the space), the GCV criterion
sometimes tends to over-ﬁt the model. We plan to explore this issue further.
Due to the diﬃculty of also selecting the best  for the SVR, an alternate algorithm
exists that automatically adjusts the value of , called the ν -SVR [4].
In this
scenario,  is treated as another free parameter. Using arguments similar to those
for β0 in our above algorithm, one can show that  is piecewise linear in 1/λ and
its path can be calculated similarly.

Acknowledgments

We would like to thank Saharon Rosset for helpful comments. Gunter and Zhu are
partially supported by grant DMS-0505432 from the National Science Foundation.

References

[1] M¨uler K, Smola A, R¨atsch G, Sch¨olkopf B, Kohlmorgen J & Vapnik V (1997) Predicting
time series with support vector machines. Artiﬁcial Neural Networks, 999-1004.

[2] Vapnik V, Golowich S & Smola A (1997) Support vector method for function approxi-
mation, regression estimation, and signal processing. NIPS 9.

[3] Shpigelman L, Crammer K, Paz R, Vaadia E & Singer Y (2004) A temporal kernel-based
model for tracking hand movements from neural activities. NIPS 17, 1273-1280.

[4] Smola A & Sch¨olkopf B (2004) A tutorial on support vector regression. Statistics and
Computing 14: 199-222.

[5] Vanderbei, R. (1994) LOQO: An interior point code for quadratic programming. Tech-
nical Report SOR-94-15, Princeton University.

[6] Osuna E, Freund R & Girosi F (1997) An improved training algorithm for support
vector machines. Neural Networks for Signal Processing, 276-284.

[7] Joachims T (1999) Making large-scale SVM learning practical. Advances in Kernel
Methods – Support Vector Learning, 169-184.

[8] Platt J (1999) Fast training of support vector machines using sequential minimal opti-
mization. Advances in Kernel Methods – Support Vector Learning, 185-208.

[9] Keerthi S, Shevade S, Bhattacharyya C & Murthy K (1999) Improvements to Platt’s
SMO algorithm for SVM classiﬁer design. Technical Report CD-99-14, NUS.

[10] Hastie, T., Rosset, S., Tibshirani, R. & Zhu, J. (2004) The Entire Regularization Path
for the Support Vector Machine. JMLR, 5, 1391-1415.

[11] Stein, C. (1981) Estimation of the mean of a multivariate normal distribution. Annals
of Statistics 9: 1135-1151.

[12] Craven, P. & Wahba, G. (1979) Smoothing noisy data with spline function. Numerical
Mathematics 31: 377-403.

[13] Friedman, J. (1991) Multivariate Adaptive Regression Splines. Annals of Statistics
19: 1-67.


Divergences, surrogate loss functions and
experimental design

XuanLong Nguyen
University of California
Berkeley, CA 94720
xuanlong@cs.berkeley.edu

Martin J. Wainwright
University of California
Berkeley, CA 94720
wainwrig@eecs.berkeley.edu

Michael I. Jordan
University of California
Berkeley, CA 94720
jordan@cs.berkeley.edu

Abstract

In this paper, we provide a general theorem that establishes a correspon-
dence between surrogate loss functions in classiﬁcation and the family
of f -divergences. Moreover, we provide constructive procedures for
determining the f -divergence induced by a given surrogate loss, and
conversely for ﬁnding all surrogate loss functions that realize a given
f -divergence. Next we introduce the notion of universal equivalence
among loss functions and corresponding f -divergences, and provide nec-
essary and sufﬁcient conditions for universal equivalence to hold. These
ideas have applications to classiﬁcation problems that also involve a com-
ponent of experiment design; in particular, we leverage our results to
prove consistency of a procedure for learning a classiﬁer under decen-
tralization requirements.

1 Introduction

A unifying theme in the recent literature on classiﬁcation is the notion of a surrogate loss
function—a
convex upper bound on the 0-1 loss. Many practical classiﬁcation algorithms
can be formulated in terms of the minimization of surrogate loss functions; well-known
examples include the support vector machine (hinge loss) and Adaboost (exponential loss).
Signiﬁcant progress has been made on the theoretical front by analyzing the general statis-
tical consequences of using surrogate loss functions [e.g., 2, 10, 13].

These recent developments have an interesting historical antecedent. Working in the con-
text of experimental design, researchers in the 1960’s recast the (intractable) problem of
minimizing the probability of classiﬁcation error in terms of the maximization of various
surrogate functions [e.g., 5, 8]. Examples of experimental design include the choice of a
quantizer as a preprocessor for a classiﬁer
[12], or the choice of a “signal set ”
for a radar
system [5]. The surrogate functions that were used included the Hellinger distance and var-
ious forms of KL divergence; maximization of these functions was proposed as a criterion
for the choice of a design. Theoretical support for this approach was provided by a classical
theorem on the comparison of experiments due to Blackwell [3]. An important outcome
of this line of work was the deﬁnition of a general family of “ f -divergences”
(also known
as “ Ali-Silvey distances”), which includes Hellinger distance and KL divergence as special
cases [1, 4].

In broad terms, the goal of the current paper is to bring together these two literatures, in
particular by establishing a correspondence between the family of surrogate loss functions
and the family of f -divergences. Several speciﬁc goals motivate us in this regard: (1)
different f -divergences are related by various well-known inequalities [11], so that a cor-
respondence between loss functions and f -divergences would allow these inequalities to
be harnessed in analyzing surrogate loss functions; (2) a correspondence could allow the
deﬁnition of interesting equivalence classes of losses or divergences; and (3) the problem
of experimental design, which motivated the classical research on f -divergences, provides
new venues for applying the loss function framework from machine learning. In particular,
one natural extension—and
one which we explore towards the end of this paper—is
in re-
quiring consistency not only in the choice of an optimal discriminant function but also in
the choice of an optimal experiment design.

The main technical contribution of this paper is to state and prove a general theorem relat-
ing surrogate loss functions and f -divergences. 1 We show that the correspondence is quite
strong: any surrogate loss induces a corresponding f -divergence, and any f -divergence
satisfying certain conditions corresponds to a family of surrogate loss functions. Moreover,
exploiting tools from convex analysis, we provide a constructive procedure for ﬁnding loss
functions from f -divergences. We also introduce and analyze a notion of universal equiva-
lence among loss functions (and corresponding f -divergences). Finally, we present an ap-
plication of these ideas to the problem of proving consistency of classiﬁcation algorithms
with an additional decentralization requirement.

2 Background and elementary results

Consider a covariate X 2 X , where X is a compact topological space, and a random
variable Y 2 Y := f¡1; +1g. The space (X £ Y ) is assumed to be endowed with a
Borel regular probability measure P . In this paper, we consider a variant of the standard
classiﬁcation problem, in which the decision-maker, rather than having direct access to X ,
only observes some variable Z 2 Z that is obtained via conditional probability Q(Z jX ).
The stochastic map Q is referred to as an experiment in statistics; in the signal processing
literature, where Z is generally taken to be discrete, it is referred to as a quantizer. We let
Q denote the space of all stochastic Q and let Q0 denote its deterministic subset.
Given a ﬁx ed experiment Q, we can formulate a standard binary classiﬁcation problem as
one of ﬁnding a measurable function (cid:176) 2 ¡ := fZ ! Rg that minimizes the Bayes risk
P (Y 6= sign((cid:176) (Z ))). Our focus is the broader question of determining both the classiﬁer
(cid:176) 2 ¡, as well as the experiment choice Q 2 Q so as to minimize the Bayes risk.
The Bayes risk corresponds to the expectation of the 0-1 loss. Given the non-convexity of
this loss function, it is natural to consider a surrogate loss function ` that we optimize in
place of the 0-1 loss. We refer to the quantity R` ((cid:176) ; Q) := E`(Y (cid:176) (Z )) as the `-risk. For
each ﬁx ed quantization rule Q, the optimal ` risk (as a function of Q) is deﬁned as follows:
(1)

R` ((cid:176) ; Q):

R` (Q) := inf
(cid:176)2¡

Given priors q = P (Y = ¡1) and p = P (Y = 1), deﬁne nonnegative measures „ and … :
„(z ) = P (Y = 1; Z = z ) = p Zx
Q(z jx)dP (xjY = 1)
…(z ) = P (Y = ¡1; Z = z ) = q Zx
Q(z jx)dP (xjY = ¡1):
1Proofs are omitted from this manuscript for lack of space; see the long version of the paper [7]
for proofs of all of our results.

(2)

As a consequence of Lyapunov’s theorem, the space of f(„; …)g obtained by varying Q 2
Q (or Q0 ) is both compact and convex (see [12] for details). For simplicity, we assume that
the space Q of Q is restricted such that both „ and … are strictly positive measures.
One approach to choosing Q is to deﬁne an f -divergence between „ and … ; indeed this is
the classical approach referred to earlier [e.g., 8]. Rather than following this route, however,
we take an alternative path, setting up the problem in terms of `-risk and optimizing out
the discriminant function (cid:176) . Note in particular that the `-risk can be represented in terms
of the measures „ and … as follows:
R` ((cid:176) ; Q) = Xz
`((cid:176) (z ))„(z ) + `(¡(cid:176) (z ))…(z ):
This representation allows us to compute the optimal value for (cid:176) (z ) for all z 2 Z , as well
as the optimal ` risk for a ﬁx ed Q. We illustrate this calculation with several examples:
0-1 loss. If ` is 0-1 loss, then (cid:176) (z ) = sign(„(z ) ¡ …(z )). Thus the optimal Bayes risk given
a ﬁx ed Q takes the form: Rbayes (Q) = Pz2Z minf„(z ); …(z )g = 1
2 ¡ 1
2 Pz2Z j„(z ) ¡
…(z )j =: 1
2 (1 ¡ V („; …)), where V („; …) denotes the variational distance between two
measures „ and … .
Hinge loss. Let `hinge (y(cid:176) (z )) = (1 ¡ y(cid:176) (z ))+ . In this case (cid:176) (z ) = sign(„(z ) ¡ …(z ))
and the optimal risk takes the form: Rhinge (Q) = Pz2Z 2 minf„(z ); …(z )g = 1 ¡
Pz2Z j„(z ) ¡ …(z )j = 1 ¡ V („; …) = 2Rbayes (Q).
Least squares loss. Letting `sqr (y(cid:176) (z )) = (1 ¡ y(cid:176) (z ))2 , we have (cid:176) (z ) = „(z)¡…(z)
„(z)+…(z) . The
(„(z)¡…(z))2
4„(z)…(z)
optimal risk takes the form: Rsqr (Q) = Pz2Z
„(z)+…(z) = 1 ¡ Pz2Z
„(z)+…(z) =:
1 ¡ ¢(„; …), where ¢(„; …) denotes the triangular discrimination distance.
Logistic loss. Letting `log (y(cid:176) (z )) := log ¡1 + exp¡y(cid:176) (z) ¢, we have (cid:176) (z ) = log „(z)
…(z) .
The optimal risk for logistic loss takes the form: Rlog (Q) = Pz2Z „(z ) log „(z)+…(z)
„(z) +
…(z ) log „(z)+…(z)
= log 2 ¡ K L(„jj „+…
2 ) ¡ K L(… jj „+…
2 ) =: log 2 ¡ C („; …), where
…(z)
C (U; V ) denotes the capacitory discrimination distance.
2 log „(z)
Exponential loss. Letting `exp (y(cid:176) (z )) = exp(¡y(cid:176) (z )), we have (cid:176) (z ) = 1
…(z) .
The optimal risk for exponential loss takes the form: Rexp (Q) = Pz2Z 2p„(z )…(z ) =
1 ¡ Pz2Z (p„(z ) ¡ p…(z ))2 = 1 ¡ 2h2 („; …), where h(„; …) denotes the Hellinger
distance between measures „ and … .
All of the distances given above (e.g., variational, Hellinger) are all particular instances of
f -divergences. This fact points to an interesting correspondence between optimized `-risks
and f -divergences. How general is this correspondence?

3 The correspondence between loss functions and f -divergences

In order to resolve this question, we begin with precise deﬁnitions of f -divergences, and
surrogate loss functions. A f -divergence functional is deﬁned as follows [1, 4]:
Deﬁnition 1. Given any continuous convex function f : [0; +1) ! R [ f+1g, the
…(z) ¶.
f -divergence between measures „ and … is given by If („; …) := Pz …(z )f (cid:181) „(z)
For instance, the variational distance is given by f (u) = ju ¡ 1j, KL divergence by f (u) =
u log u, triangular discrimination by f (u) = (u ¡ 1)2 =(u + 1), and Hellinger distance by
2 (pu ¡ 1)2 .
f (u) = 1

inf
ﬁ

Surrogate loss `. First, we require that any surrogate loss function ` is continuous and
convex. Second, the function ` must be classi ﬁcation-calibr ated [2], meaning that for
any a; b ‚ 0 and a 6= b, inf ﬁ:ﬁ(a¡b)<0 `(ﬁ)a + `(¡ﬁ)b > inf ﬁ2R `(ﬁ)a + `(¡ﬁ)b. It
can be shown [2] that in the convex case ` is classiﬁcation-calibrated if and only if it is
differentiable at 0 and `0 (0) < 0. Lastly, let ﬁ⁄ = inf ﬁ f`(ﬁ) = inf `g. If ﬁ⁄ < +1,
then for any – > 0, we require that `(ﬁ⁄ ¡ –) ‚ `(ﬁ⁄ + –). The interpretation of the last
assumption is that one should penalize deviations away from ﬁ⁄ in the negative direction
at least as strongly as deviations in the positive direction; this requirement is intuitively
reasonable given the margin-based interpretation of ﬁ.
From `-risk to f -divergence. We begin with a simple result that formalizes how any `-
risk induces a corresponding f -divergence. More precisely, the following lemma proves
that the optimal ` risk for a ﬁx ed Q can be written as the negative of an f divergence.
Lemma 2. For each ﬁxed Q, let (cid:176)Q denote the optimal decision rule. The ` risk for (Q; (cid:176)Q )
is an f -divergence between „ and … for some convex function f :
R` (Q) = ¡If („; …):
Proof. The optimal ` risk takes the form:
ﬁ (cid:181)`(¡ﬁ) + `(ﬁ)
…(z ) ¶:
„(z )
(`(ﬁ)„(z ) + `(¡ﬁ)…(z )) = Xz
R` (Q) = Xz2Z
…(z ) inf
For each z let u = „(z)
…(z) , then inf ﬁ (`(¡ﬁ) + `(ﬁ)u) is a concave function of u (since
minimization over a set of linear function is a concave function). Thus, the claim follows
by deﬁning (for u 2 R)

(`(¡ﬁ) + `(ﬁ)u):
f (u) := ¡ inf
ﬁ
From f -divergence to `-risk. In the remainder of this section, we explore the converse
of Lemma 2. Given a divergence If („; …) for some convex function f , does there exist a
loss function ` for which R` (Q) = ¡If („; …)? In the following, we provide a precise
characterization of the set of f -divergences that can be realized in this way, as well as a
constructive procedure for determining all ` that realize a given f -divergence.
Our method requires the introduction of several intermediate functions. First, let us deﬁne,
for each ﬂ , the inverse mapping `¡1 (ﬂ ) := inf fﬁ : `(ﬁ) • ﬂ g, where inf ; := +1.
Using the function `¡1 , we then deﬁne a new function “ : R ! R by
if `¡1 (ﬂ ) 2 R;
:= ‰`(¡`¡1 (ﬂ ))
“(ﬂ )
otherwise.
+1
Note that the domain of “ is Dom(“) = fﬂ 2 R : `¡1 (ﬂ ) 2 Rg. Deﬁne
ﬂ1 := inf fﬂ : “(ﬂ ) < +1g and ﬂ2 := inf fﬂ : “(ﬂ ) = inf “g:
(6)
It is simple to check that inf ` = inf “ = `(ﬁ⁄ ), and ﬂ1 = `(ﬁ⁄ ), ﬂ2 = `(¡ﬁ⁄ ).
Furthermore, “(ﬂ2 ) = `(ﬁ⁄ ) = ﬂ1 , “(ﬂ1 ) = `(¡ﬁ⁄ ) = ﬂ2 . With this set-up, the
following lemma captures several important properties of “:
Lemma 3.
(a) “ is strictly decreasing in (ﬂ1 ; ﬂ2 ). If ` is decreasing, then “ is also
decreasing in (¡1; +1). In addition, “(ﬂ ) = +1 for ﬂ < ﬂ1 .
(b) “ is convex in (¡1; ﬂ2 ]. If ` is decreasing, then “ is convex in (¡1; +1).
(c) “ is lower semi-continuous, and continuous in its domain.
(d) There exists u⁄ 2 (ﬂ1 ; ﬂ2 ) such that “(u⁄ ) = u⁄ .

(3)

(4)

(5)

(7)

(e) There holds “(“(ﬂ )) = ﬂ for all ﬂ 2 (ﬂ1 ; ﬂ2 ).
The connection between “ and an f -divergence arises from the following fact. Given the
deﬁnition (5) of “, it is possible to show that
(¡ﬂu ¡ “(ﬂ )) = “⁄ (¡u);
f (u) = sup
ﬂ2R
where “⁄ denotes the conjugate dual of the function “. Hence, if “ is a lower semicon-
tinuous convex function, it is possible to recover “ from f by means of convex duality [9]:
“(ﬂ ) = f ⁄ (¡ﬂ ). Thus, equation (5) provides means for recovering a loss function ` from
“. Indeed, the following theorem provides a constructive procedure for ﬁnding all such `
when “ satisﬁes necessary conditions speciﬁed in Lemma 3:
Theorem 4. (a) Given a lower semicontinuous convex function f : R ! R, deﬁne:
“(ﬂ ) = f ⁄ (¡ﬂ ):
If “ is a decreasing function satisfying the properties speciﬁed in parts (c), (d) and (e) of
Lemma 3, then there exist convex continuous loss function ` for which (3) and (4) hold.
(b) More precisely, all such functions ` are of the form: For any ﬁ ‚ 0,
and `(¡ﬁ) = g(ﬁ + u⁄ );
`(ﬁ) = “(g(ﬁ + u⁄ ));
(9)
where u⁄ satisﬁes “(u⁄ ) = u⁄ for some u⁄ 2 (ﬂ1 ; ﬂ2 ) and g : [u⁄ ; +1) ! R is any
increasing continuous convex function such that g(u⁄ ) = u⁄ . Moreover, g is differentiable
at u⁄+ and g 0 (u⁄+) > 0.

(8)

One interesting consequence of Theorem 4 that any realizable f -divergence can in fact be
obtained from a fairly large set of ` loss functions. More precisely, examining the statement
of Theorem 4(b) reveals that for ﬁ • 0, we are free to choose a function g that must satisfy
only mild conditions; given a choice of g , then ` is speciﬁed for ﬁ > 0 accordingly by
equation (9). We describe below how the Hellinger distance, for instance, is realized not
only by the exponential loss (as described earlier), but also by many other surrogate loss
functions. Additional examples can be found in [7].
Illustrative examples. Consider Hellinger distance, which is an f -divergence2 with
f (u) = ¡2pu. Augment the domain of f with f (u) = +1 for u < 0. Following
the prescription of Theorem 4(a), we ﬁrst
recover “ from f :
when ﬂ > 0
(¡ﬂu ¡ f (u)) = ‰1=ﬂ
“(ﬂ ) = f ⁄ (¡ﬂ ) = sup
+1 otherwise.
u2R
Clearly, u⁄ = 1. Now if we choose g(u) = eu¡1 , then we obtain the exponential loss
`(ﬁ) = exp(¡ﬁ). However, making the alternative choice g(u) = u, we obtain the
function `(ﬁ) = 1=(ﬁ + 1) and `(¡ﬁ) = ﬁ + 1, which also realizes the Hellinger distance.
Recall that we have shown previously that the 0-1 loss induces the variational distance,
which can be expressed as an f -divergence with fvar (u) = ¡2 min(u; 1) for u ‚ 0. It
is thus of particular interest to determine other loss functions that also lead to variational
distance. If we augment the function fvar by deﬁning fvar (u) = +1 for u < 0, then we
can recover “ from fvar as follows:
(¡ﬂu ¡ fvar (u)) = ‰(2 ¡ ﬂ )+ when ﬂ ‚ 0
“(ﬂ ) = f ⁄
var (¡ﬂ ) = sup
when ﬂ < 0:
+1
u2R
2We consider f -divergences for two convex functions f1 and f2 to be equivalent if f1 and f2 are
related by a linear term, i.e., f1 = cf2 + au + b for some constants c > 0; a; b, because then If1 and
If2 are different by a constant.

Clearly u⁄ = 1. Choosing g(u) = u leads to the hinge loss `(ﬁ) = (1 ¡ ﬁ)+ , which is
consistent with our earlier ﬁndings. Making the alternative choice g(u) = eu¡1 leads to a
rather different loss—namely , `(ﬁ) = (2 ¡ eﬁ )+ for ﬁ ‚ 0 and `(ﬁ) = e¡ﬁ for ﬁ < 0—
that also realizes the variational distance.

Using Theorem 4 it can be shown that an f -divergence is realizable by a margin-based sur-
rogate loss if and only if it is symmetric [7]. Hence, the list of non-realizable f -divergences
includes the K L divergence K L(„jj…) (as well as K L(… jj„)). The symmetric KL diver-
gence K L(„jj…) + K L(… jj„) is a realizable f -divergence. Theorem 4 allows us to con-
struct all ` losses that realize it. One of them turns out to have the simple closed-form
`(ﬁ) = e¡ﬁ ¡ ﬁ, but obtaining it requires some non-trivial calculations [7].
4 On comparison of loss functions and quantization schemes

The previous section was devoted to study of the correspondence between f -divergences
and the optimal `-risk R` (Q) for a ﬁx ed experiment Q. Our ultimate goal, however, is that
of choosing an optimal Q, a problem known as experimental design in the statistics litera-
ture [3]. One concrete application is the design of quantizers for performing decentralized
detection [12, 6] in a sensor network.

In this section, we address the experiment design problem via the joint optimization of `-
risk (or more precisely, its empirical version) over both the decision (cid:176) and the choice of
experiment Q (hereafter referred to as a quantizer). This procedure raises the natural the-
oretical question: for what loss functions ` does such joint optimization lead to minimum
Bayes risk? Note that the minimum here is taken over both the decision rule (cid:176) and the
space of experiments Q, so that this question is not covered by standard consistency re-
sults [13, 10, 2]. Here we describe how the results of the previous section can be leveraged
to resolve this issue of consistency.

4.1 Universal equivalence

The connection between f -divergences and 0-1 loss can be traced back to seminal work
on the comparison of experiments [3]. Formally, we say that the quantization scheme Q1
dominates than Q2 if Rbayes (Q1 ) • Rbayes (Q2 ) for any prior probabilities q 2 (0; 1). We
have the following theorem [3] (see also [7] for a short proof):
Theorem 5. Q1 dominates Q2 iff If („Q1 ; …Q1 ) ‚ If („Q2 ; …Q2 ), for all convex functions
f . The superscripts denote the dependence of „ and … on the quantizer rules Q1 ; Q2 .

Using Lemma 2, we can establish the following:
Corollary 6. Q1 dominates Q2 iff R` (Q1 ) • R` (Q2 ) for any surrogate loss `.
One implication of Corollary 6 is that if R` (Q1 ) • R` (Q2 ) for some loss function `, then
Rbayes (Q1 ) • Rbayes (Q2 ) for some set of prior probabilities on the labels Y . This fact
justiﬁes
the use of a surrogate `-loss as a proxy for the 0-1 loss, at least for a certain subset
of prior probabilities. Typically, however, the goal is to select the optimal experiment Q
for a pre-speciﬁed set of priors, in which context this implication is of limited use. We
are thus motivated to consider a different method of determining which loss functions (or
equivalently, f -divergences) lead to the same optimal experimental design as the 0-1 loss
(respectively the variational distance). More generally, we are interested in comparing two
arbitrary loss function `1 and `2 , with corresponding divergences induced by f1 and f2
respectively:
Deﬁnition 7. The surrogate loss functions `1 and `2 are universally equivalent, denoted
u
u
… f2 ), if for any P (X; Y ) and quantization rules Q1 ; Q2 , there holds:
… `2 (and f1
by `1
(10)
R`1 (Q1 ) • R`1 (Q2 ) , R`2 (Q1 ) • R`2 (Q2 ):

The following result provides necessary and sufﬁcient conditions for universal equivalence:
Theorem 8. Suppose that f1 and f2 are differentiable a.e., convex functions that map
u
… f2 if and only if f1 (u) = cf2 (u) + au + b for some constants
[0; +1) to R. Then f1
a; b 2 R and c > 0.
If we restrict our attention to convex and differentiable a.e. functions f , then it follows that
all f -divergences univerally equivalent to the variational distance must have the form
with c > 0:
f (u) = ¡c min(u; 1) + au + b
As a consequence, the only `-loss functions universally equivalent to 0-1 loss are those that
induce an f -divergence of this form (11). One well-known example of such a function is
the hinge loss; more generally, Theorem 4 allows us to construct all such `.

(11)

4.2 Consistency in experimental design

The notion of universal equivalence might appear quite restrictive because condition (10)
must hold for any underlying probability measure P (X; Y ). However, this is precisely
what we need when P (X; Y ) is unknown. Assume that the knowledge about P (X; Y )
comes from an empirical data sample (xi ; yi )n
i=1 .
Consider any algorithm (such as that proposed by Nguyen et al. [6]) that involves choosing
a classiﬁer -quantizer pair ((cid:176) ; Q) 2 ¡ £ Q by minimizing an empirical version of `-risk:
n
1
^R` ((cid:176) ; Q) :=
Xi=1 Xz
`(yi(cid:176) (z ))Q(z jxi ):
n
More formally, suppose that (Cn ; Dn ) is a sequence of increasing compact function classes
such that C1 (cid:181) C2 (cid:181) : : : (cid:181) ¡ and D1 (cid:181) D2 (cid:181) : : : (cid:181) Q. Let ((cid:176) ⁄
n ) be an optimal
n ; Q⁄
^R` ((cid:176) ; Q), and let R⁄
solution to the minimization problem min((cid:176) ;Q)2(Cn ;Dn )
bayes denote
the minimum Bayes risk achieved over the space of decision rules ((cid:176) ; Q) 2 (¡; Q). We
bayes the Bayes error of our estimation procedure. We say that
call Rbayes ((cid:176) ⁄
n ; Q⁄
n ) ¡ R⁄
such a procedure is universally consistent if the Bayes error tends to 0 as n ! 1, i.e., for
any (unknown) Borel probability measure P on X £ Y ,
n ) ¡ R⁄
n ; Q⁄
Rbayes ((cid:176) ⁄
bayes = 0 in probability:
lim
n!1
When the surrogate loss ` is universally equivalent to 0-1 loss, we can prove that suit-
able learning procedures are indeed universally consistent. Our approach is based on the
framework developed by various authors [13, 10, 2] for the case of ordinary classiﬁca-
tion, and using the strategy of decomposing the Bayes error into a combination of (a)
approximation error introduced by the bias of the function classes Cn (cid:181) ¡: E0 (Cn ; Dn ) =
` , where R⁄
` := inf ((cid:176) ;Q)2(¡;Q) R` ((cid:176) ; Q); and (b) esti-
inf ((cid:176) ;Q)2(Cn ;Dn ) R` ((cid:176) ; Q) ¡ R⁄
sample size n, E1 (Cn ; Dn ) =
mation error introduced by the variance of using ﬁnite
E sup((cid:176) ;Q)2(Cn ;Dn ) j ^R` ((cid:176) ; Q) ¡ R` ((cid:176) ; Q)j, where the expectation is taken with respect
to the (unknown) probability measure P (X; Y ).
Assumptions. Assume that the loss function ` is universally equivalent to the 0-1
loss. From Theorem 8, the corresponding f -divergence must be of the form f (u) =
¡c min(u; 1) + au + b, for a; b 2 R and c > 0. Finally, we also assume that
(a ¡ b)(p ¡ q) ‚ 0 and `(0) ‚ 0.3
In addition, for each n = 1; 2; : : :, suppose that
Mn := supy ;z sup((cid:176) ;Q)2(Cn ;Dn ) j`(y(cid:176) (z ))j < +1.
3These technical conditions are needed so that the approximation error due to varying Q domi-
nates the approximation error due to varying (cid:176) . Setting a = b is sufﬁcient.

The following lemma plays a key role in our proof: it links the excess `-risk to the Bayes
error when performing joint minimization:
Lemma 9. For any ((cid:176) ; Q), we have c
2 (Rbayes ((cid:176) ; Q) ¡ R⁄
bayes ) • R` ((cid:176) ; Q) ¡ R⁄
` :
Finally, we can relate the Bayes error to the approximation error and estimation error, and
provide general conditions for universal consistency:
Theorem 10. (a) For any Borel probability measure P , with probability at least 1¡ – , there
bayes • 2
c (2E1 (Cn ; Dn ) + E0 (Cn ; Dn ) + 2Mnp2 ln(2=–)=n):
holds: Rbayes ((cid:176) ⁄
n ) ¡ R⁄
n ; Q⁄
n=1Cn is dense in ¡ so
n=1Dn is dense in Q and if [1
(b) (Universal Consistency) If [1
that limn!1 E0 (Cn ; Dn ) = 0, and if the sequence of function classes (Cn ; Dn ) grows
sufﬁciently slowly enough so that limn!1 E1 (Cn ; Dn ) = limn!1 Mnpln n=n = 0, there
holds limn!1 Rbayes ((cid:176) ⁄
bayes = 0 in probability.
n ; Q⁄
n ) ¡ R⁄

5 Conclusions

We have presented a general theoretical connection between surrogate loss functions and
f -divergences. As illustrated by our application to decentralized detection, this connec-
tion can provide new domains of application for statistical learning theory. We also expect
that this connection will provide new applications for f -divergences within learning the-
ory; note in particular that bounds among f -divergences (of which many are known; see,
e.g., [11]) induce corresponding bounds among loss functions.

References
[1] S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from
another. J. Royal Stat. Soc. Series B, 28:131 –142, 1966.
[2] P. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation and risk bounds. Journal
of the American Statistical Association, 2005. To appear.
[3] D. Blackwell. Equivalent comparisons of experiments. Annals of Statistics, 24(2):265 –272,
1953.
[4] I. Csisz ´ar.
Information-type measures of difference of probability distributions and indirect
observation. Studia Sci. Math. Hungar, 2:299–318, 1967.
[5] T. Kailath. The divergence and Bhattacharyya distance measures in signal selection.
Trans. on Communication Technology, 15(1):52–60, 1967.
[6] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Nonparametric decentralized detection using
kernel methods. IEEE Transactions on Signal Processing, 53(11):4053–4066, 2005.
[7] X. Nguyen, M. J. Wainwright, and M. I. Jordan. On divergences, surrogate loss functions
and decentralized detection. Technical Report 695, Department of Statistics, University of
California at Berkeley, September 2005.
[8] H. V. Poor and J. B. Thomas. Applications of Ali-Silvey distance measures in the design of
generalized quantizers for binary decision systems. IEEE Trans. on Communications, 25:893 –
900, 1977.
[9] G. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970.
[10] I. Steinwart. Consistency of support vector machines and other regularized kernel machines.
IEEE Trans. Info. Theory, 51:128 –142, 2005.
[11] F. Topsoe. Some inequalities for information divergence and related measures of discrimination.
IEEE Transactions on Information Theory, 46:1602–1609, 2000.
[12] J. Tsitsiklis. Extremal properties of likelihood-ratio quantizers. IEEE Trans. on Communication,
41(4):550–558, 1993.
[13] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk
minimization. Annal of Statistics, 53:56–134, 2004.

IEEE


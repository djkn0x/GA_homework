Interpolating Between Types and Tokens
by Estimating Power-Law Generators ∗

Sharon Goldwater
Thomas L. Grif ﬁths
Mark Johnson
Department of Cognitive and Linguistic Sciences
Brown University, Providence RI 02912, USA
{sharon goldwater,tom griffiths,mark johnson}@brown.edu

Abstract
Standard statistical models of language fail to capture one of the most
striking properties of natural languages: the power-law di stribution in
the frequencies of word tokens. We present a framework for developing
statistical models that generically produce power-laws, augmenting stan-
dard generative models with an adaptor that produces the appropriate
pattern of token frequencies. We show that taking a particular stochastic
process – the Pitman-Yor process – as an adaptor justiﬁes the
appearance
of type frequencies in formal analyses of natural language, and improves
the performance of a model for unsupervised learning of morp hology.

1 Introduction

In general it is important for models used in unsupervised learning to be able to describe
the gross statistical properties of the data they are intend ed to learn from, otherwise these
properties may distort inferences about the parameters of t he model. One of the most strik-
ing statistical properties of natural languages is that the distribution of word frequencies is
closely approximated by a power-law. That is, the probabili ty that a word w will occur with
w . This observation, which
frequency nw in a sufﬁciently large corpus is proportional to n−g
is usually attributed to Zipf [1] but enjoys a long and detailed history [2], stimulated intense
research in the 1950s (e.g., [3]) but has largely been ignored in modern computational lin-
guistics. By developing models that generically exhibit power-laws, it may be possible to
improve methods for unsupervised learning of linguistic st ructure.

In this paper, we introduce a framework for developing gener ative models for language
that produce power-law distributions. Our framework is bas ed upon the idea of specifying
language models in terms of two components: a generator, an underlying generative model
for words which need not (and usually does not) produce a powe r-law distribution, and an
adaptor, which transforms the stream of words produced by the genera tor into one whose
frequencies obey a power law distribution. This framework i s extremely general: any gen-
erative model for language can be used as a generator, with th e power-law distribution
being produced as the result of making an appropriate choice for the adaptor.

In our framework, estimation of the parameters of the genera tor will be affected by assump-
tions about the form of the adaptor. We show that use of a particular adaptor, the Pitman-
Yor process [4, 5, 6], sheds light on a tension exhibited by formal approaches to natural
language: whether explanations should be based upon the types of words that languages

∗This work was partially supported by NSF awards IGERT 9870676 and ITR 0085940 and NIMH
award 1R0-IMH60922-01A2

exhibit, or the frequencies with which tokens of those words occur. One place where this
tension manifests is in accounts of morphology, where forma l linguists develop accounts of
why particular words appear in the lexicon (e.g., [7]), while computational linguists focus
on statistical models of the frequencies of tokens of those words (e.g., [8]). The tension
between types and tokens also appears within computational linguistics. For example, one
of the most successful forms of smoothing used in statistical language models, Kneser-Ney
smoothing, explicitly interpolates between type and token frequencies [9, 10, 11].

The plan of the paper is as follows. Section 2 discusses stochastic processes that can pro-
duce power-law distributions, including the Pitman-Yor process. Section 3 speciﬁes a two-
stage language model that uses the Pitman-Yor process as an adaptor, and examines some
properties of this model: Section 3.1 shows that estimation based on type and token fre-
quencies are special cases of this two-stage language model, and Section 3.2 uses these
results to provide a novel justiﬁcation for the use of Kneser -Ney smoothing. Section 4
describes a model for unsupervised learning of the morpholo gical structure of words that
uses our framework, and demonstrates that its performance improves as we move from
estimation based upon tokens to types. Section 5 concludes the paper.

2 Producing power-law distributions

P (zi = k | z−i ) = a

Assume we want to generate a sequence of N outcomes, z = {z1 , . . . , zN } with each
outcome zi being drawn from a set of (possibly unbounded) size Z . Many of the stochastic
processes that produce power-laws are based upon the princi ple of preferential attachment,
where the probability that the ith outcome, zi , takes on a particular value k depends upon
the frequency of k in z−i = {z1 , . . . , zi−1 } [2]. For example, one of the earliest and most
widely used preferential attachment schemes [3] chooses zi according to the distribution
n(z
−i )
1
k
i − 1
Z
where n(z
−i )
is the number of times k occurs in z−i . This “rich-get-richer” process means
k
that a few outcomes appear with very high frequency in z – the key attribute of a power-law
distribution. In this case, the power-law has parameter g = 1/(1 − a).
One problem with these classical models is that they assume a ﬁxed ordering on the out-
comes z. While this may be appropriate for some settings, the assump tion of a temporal
ordering restricts the contexts in which such models can be applied. In particular, it is
much more restrictive than the assumption of independent sampling that underlies most
statistical language models. Consequently, we will focus on a different preferential attach-
ment scheme, based upon the two-parameter species sampling model [4, 5] known as the
Pitman-Yor process [6]. Under this scheme outcomes follow a power-law distribution, but
remain exchangeable: the probability of a set of outcomes is not affected by their ordering.

+ (1 − a)

(1)

The Pitman-Yor process can be viewed as a generalization of the Chinese restaurant process
[6]. Assume that N customers enter a restaurant with in ﬁnitely many tables, ea ch with
in ﬁnite seating capacity. Let zi denote the table chosen by the ith customer. The ﬁrst
customer sits at the ﬁrst table, z1 = 1. The ith customer chooses table k with probability
P (zi = k | z−i ) = 
−i )
(z
n
−a
k
i−1+b
K (z
−i )a+b

i−1+b
where a and b are the two parameters of the process and K (z−i ) is the number of tables
that are currently occupied.
The Pitman-Yor process satisﬁes our need for a process that p roduces power-laws while
retaining exchangeability. Equation 2 is clearly a preferential attachment scheme. When

k ≤ K (z−i )
k = K (z−i ) + 1

(2)

(a)

Generator

Adaptor

(b)

Generator

Adaptor

θ

ℓ

z

w

c

f

t

ℓ

z

w

Figure 1: Graphical models showing dependencies among variables in (a) the simple two-
stage model, and (b) the morphology model. Shading of the node containing w re ﬂects the
fact that this variable is observed. Dotted lines delimit the generator and adaptor.

a = 0 and b > 0, it reduces to the standard Chinese restaurant process [12, 4] used in
Dirichlet process mixture models [13]. When 0 < a < 1, the number of people seated at
each table follows a power-law distribution with g = 1 + a [5]. It is straightforward to
show that the customers are exchangeable: the probability o f a partition of customers into
sets seated at different tables is unaffected by the order in which the customers were seated.

3 A two-stage language model

We can use the Pitman-Yor process as the foundation for a language model that generi-
cally produces power-law distributions. We will de ﬁne a two -stage model by extending the
restaurant metaphor introduced above. Imagine that each table k is labelled with a word ℓk
from a vocabulary of (possibly unbounded) size W . The ﬁrst stage is to generate these la-
bels, sampling ℓk from a generative model for words that we will refer to as the generator.
For example, we could choose to draw the labels from a multinomial distribution θ. The
second stage is to generate the actual sequence of words itself. This is done by allowing a
sequence of customers to enter the restaurant. Each custome r chooses a table, producing a
seating arrangement, z, and says the word used to label that the table, producing a sequence
of words, w. The process by which customers choose tables, which we will refer to as the
adaptor, de ﬁnes a probability distribution over the sequence of wor ds w produced by the
customers, determining the frequency with which tokens of the different types occur. The
statistical dependencies among the variables in one such model are shown in Figure 1 (a).
Given the discussion in the previous section, the Pitman-Yor process is a natural choice
for an adaptor. The result is technically a Pitman-Yor mixtu re model, with zi indicating
the “class” responsible for generating the
ith word, and ℓk determining the multinomial
distribution over words associated with class k , with P (wi = w | zi = k , ℓk ) = 1 if
ℓk = w, and 0 otherwise. Under this model the probability that the ith customer produces
word w given previously produced words w−i and current seating arrangement z−i is
X
P (wi = w | w−i , z−i , θ) = X
k
ℓk

P (wi = w | zi = k, ℓk )P (ℓk | w−i , z−i , θ)P (zi = k | z−i )

=

θw

(3)

−i )

(z
− a
n
k
i − 1 + b

−i )
K (z
X
k=1
where I (·) is an indicator function, being 1 when its argument is true and 0 otherwise. If
θ is uniform over all W words, then the distribution over w reduces to the Pitman-Yor
process as W → ∞. Otherwise, multiple tables can receive the same label, increasing the
frequency of the corresponding word and producing a distribution with g < 1 + a. Again,
it is straightforward to show that words are exchangeable un der this distribution.

K (z−i )a + b
i − 1 + b

I (ℓk = w) +

3.1 Types and tokens

The use of the Pitman-Yor process as an adaptor provides a jus tiﬁcation for the role of word
types in formal analyses of natural language. This can be seen by considering the question
of how to estimate the parameters of the multinomial distribution used as a generator, θ.1
In general, the parameters of generators can be estimated us ing Markov chain Monte Carlo
methods, as we demonstrate in Section 4. In this section, we will show that estimation
schemes based upon type and token frequencies are special cases of our language model,
corresponding to the extreme values of the parameter a. Values of a between these extremes
identify estimation methods that interpolate between types and tokens.

Taking a multinomial distribution with parameters θ as a generator and the Pitman-Yor
process as an adaptor, the probability of a sequence of words w given θ is
K (z)
Γ(1 − a) !
Yk=1  θℓk ((k − 1)a + b)
Γ(n(z)
k − a)
P (w | θ) = Xz,ℓ
P (w, z, ℓ | θ) = Xz,ℓ
where in the last sum z and ℓ are constrained such that ℓzi = wi for all i. In the case where
b = 0, this simpliﬁes to

Γ(b)
Γ(N + b)

(4)

Γ(K (z))
Γ(N )

θℓk 

Γ(1 − a) 
· aK (z)−1 · 
K (z)
K (z)
Γ(n(z)
k − a)
Yk=1
P (w | θ) = Xz,ℓ
Yk=1
 ·



The distribution P (w | θ) determines how the data w in ﬂuence estimates of θ, so we will
consider how P (w | θ) changes under different limits of a.
In the limit as a approaches 1, estimation of θ is based upon word tokens. When a → 1,
is 1 for n(z)
k = 1 but approaches 0 for n(z)
Γ(nz
−a)
k > 1. Consequently, all terms in the
k
Γ(1−a)
sum over (z, ℓ) go to zero, except that in which every word token has its own table. In this
case, K (z) = N and ℓk = wk . It follows that lima→1 P (w | θ) = QN
k=1 θwk . Any form of
estimation using P (w | θ) will thus be based upon the frequencies of word tokens in w.
In the limit as a approaches 0, estimation of θ is based upon word types. The appearance
of aK (z)−1 in Equation 4 means that as a → 0, the sum over z is dominated by the seating
arrangement that minimizes the total number of tables. Unde r the constraint that ℓzi = wi
for all i, this minimal con ﬁguration is the one in which every word typ e receives a single
table. Consequently, lima→0 P (w | θ) is dominated by a term in which there is a single
instance of θw for each word w that appears in w.2 Any form of estimation using P (w | θ)
will thus be based upon a single instance of each word type in w.

3.2 Predictions and smoothing

In addition to providing a justiﬁcation for the role of types
in formal analyses of language
in general, use of the Pitman-Yor process as an adaptor can be used to explain the assump-
tions behind a speciﬁc scheme for combining token and type fr equencies: Kneser-Ney
smoothing. Smoothing methods are schemes for regularizing empirical estimates of the
probabilities of words, with the goal of improving the predictive performance of language
models. The Kneser-Ney smoother estimates the probability of a word by combining type
and token frequencies, and has proven particularly effective for n-gram models [9, 10, 11].

1Under the interpretation of this model as a Pitman-Yor proce ss mixture model, this is analogous
to estimating the base measure G0 in a Dirichlet process mixture model (e.g. [13]).
2Despite the fact that P (w | θ) approaches 0 in this limit, aK (z)−1 will be constant across all
choices of θ . Consequently, estimation schemes that depend only on the non-constant terms in
P (w | θ), such as maximum-likelihood or Bayesian inference, will remain well deﬁned.

P (wN +1 = w | w) =

w − I (n(w)
n(w)
w > D)D
N

(5)

To use an n-gram language model, we need to estimate the probability distribution over
words given their history, i.e. the n preceding words. Assume we are given a vector of N
words w that all share a common history, and want to predict the next word, wN +1 , that will
occur with that history. Assume that we also have vectors of words from H other histories,
w(1) , . . . , w(H ) . The interpolated Kneser-Ney smoother [11] makes the prediction
+ Pw I (n(w)
w > D)D
N

(h) )
Ph I (w ∈ w
Pw Ph I (w ∈ w(h) )
where we have suppressed the dependence on w(1) , . . . , w(H ) , D is a “discount factor”
speciﬁed as a parameter of the model, and the sum over h includes w.
We can de ﬁne a two-stage model appropriate for this setting b y assuming that the sets of
words for all histories are produced by the same adaptor and g enerator. Under this model,
the probability of word wN +1 given w and θ is
P (wN +1 = w | w, θ) = Xz
where P (wN +1 = w|w, z, θ) is given by Equation 3. Assuming b = 0, this becomes
+ Pw Ez [Kw (z)] a
nw
w − Ez [Kw (z)] a
(6)
P (wN +1 = w | w, θ) =
θw
N
N
where Ez [Kw (z)] = Pz Kw (z)P (z|w, θ), and Kw (z) is the number of tables with label
w under the seating assignment z. The other histories enter into this expression via θ.
Since the words associated with each history is assumed to be produced from a single set
of parameters θ, the maximum-likelihood estimate of θw will approach
θw = Ph I (w ∈ w(h) )
Pw Ph I (w ∈ w(h) )
as a approaches 0, since only a single instance of each word type in each context will
contribute to the estimate of θ. Substituting this value of θw into Equation 6 reveals the
correspondence to the Kneser-Ney smoother (Equation 5). The only difference is that the
constant discount factor D is replaced by aEz [Kw (z)], which will increase slowly as nw
increases. This difference might actually lead to an improved smoother: the Kneser-Ney
smoother seems to produce better performance when D increases as a function of nw [11].

P (wN +1 = w|w, z, θ)P (z|w, θ)

4 Types and tokens in modeling morphology

Our attempt to develop statistical models of language that generically produce power-law
distributions was motivated by the possibility that models that account for this statistical
regularity might be able to learn linguistic information be tter than those that do not. Our
two-stage language modeling framework allows us to create exactly these sorts of mod-
els, with the generator producing individual lexical items , and the adaptor producing the
power-law distribution over words. In this section, we show that taking a generative model
for morphology as the generator and varying the parameters o f the adaptor results in an
improvement in unsupervised learning of the morphological structure of English.

4.1 A generative model for morphology

Many languages contain words built up of smaller units of meaning, or morphemes. These
units can contain lexical information (as stems) or grammat ical information (as afﬁxes).
For example, the English word walked can be parsed into the stem walk and the past-tense
sufﬁx ed. Knowledge of morphological structure enables language learners to understand
and produce novel wordforms, and facilitates tasks such as s temming (e.g., [14]).

I (w = t.f )P (ck = c)P (tk = t | ck = c)P (fk = f | ck = c)

As a basic model of morphology, we assume that each word consists of a single stem
and sufﬁx, and belongs to some in ﬂectional class. Each class
is associated with a stem
distribution and a sufﬁx distribution. We assume that stems and sufﬁxes are independent
given the class, so we have
P (ℓk = w) = Xc,t,f
where ck , tk , and fk are the class, stem, and sufﬁx associated with ℓk , and t.f indicates
the concatenation of t and f . In other words, we generate a label by ﬁrst drawing a class,
then drawing a stem and a sufﬁx conditioned on the class. Each of these draws is from a
multinomial distribution, and we will assume that these multinomials are in turn generated
from symmetric Dirichlet priors, with parameters κ, τ , and φ respectively. The resulting
generative model can be used as the generator in a two-stage language model, providing a
more structured replacement for the multinomial distribut ion, θ. As before, we will use the
Pitman-Yor process as an adaptor, setting b = 0. Figure 1 (b) illustrates the dependencies
between the variables in this model.

(7)

Our morphology model is similar to that used by Goldsmith in h is unsupervised morpho-
logical learning system [8], with two important differences. First, Goldsmith’s model is
recursive, i.e. a word stem can be further split into a smalle r stem plus sufﬁx. Second,
Goldsmith’s model assumes that all occurrences of each word type have the same analysis,
whereas our model allows different tokens of the same type to have different analyses.

4.2 Inference by Gibbs sampling

Our goal in de ﬁning this morphology model is to be able to auto matically infer the morpho-
logical structure of a language. This can be done using Gibbs sampling, a standard Markov
chain Monte Carlo (MCMC) method [15]. In MCMC, variables in the model are repeatedly
sampled, with each sample conditioned on the current values of all other variables in the
model. This process de ﬁnes a Markov chain whose stationary d istribution is the posterior
distribution over model variables given the input data.

Rather than sampling all the variables in our two-stage mode l simultaneously, our Gibbs
sampler alternates between sampling the variables in the generator and those in the adaptor.
Fixing the assignment of words to tables, we sample ck , tk , and fk for each table from

·

·

(8)

= I (ℓk = tk .fk ) ·

P (ck = c, tk = t, fk = f | c−k , t−k , f−k , ℓ)
∝ I (ℓk = tk .fk ) P (ck = c | c−k ) P (tk = t | t−k , c) P (fk = f | f−k , c)
nc + κ
nc,f + φ
nc,t + τ
K (z) − 1 + κC
nc + τ T
nc + φF
where nc is the number of other labels assigned to class c, nc,t and nc,f are the number of
other labels in class c with stem t and sufﬁx f , respectively, and C , T , and F , are the total
number of possible classes, stems, and sufﬁxes, which are ﬁx
ed. We use the notation c−k
here to indicate all members of c except for ck . Equation 8 is obtained by integrating over
the multinomial distributions speciﬁed in Equation 7, expl oiting the conjugacy between
multinomial and Dirichlet distributions.
Fixing the morphological analysis (c, t, f), we sample the table zi for each word token from
n(z
−i )
P (zi = k | z−i , w, c, t, f ) ∝ ( I (ℓk = wi )(n(z
−i )
− a)
k
k
P (ℓk = wi )(K (z−i )a + b) n(z
−i )
k
where P (ℓk = wi ) is found using Equation 7, with P (c), P (t), and P (f ) replaced with
the corresponding conditional distributions from Equation 8.

(9)

> 0

= 0

(a)

NULL

e

ed

d

ing

s

es

n

en

other

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
(true dist)

a
 
f
o
 
e
u
l
a
V

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
(true dist)

a
 
f
o
 
e
u
l
a
V

 
0

0

 

1

1

(b)

s
e
p
y
T
 
e
u
r
T

s
n
e
k
o
T
 
e
u
r
T

oth.
en
n
es
s
ing
d
ed
e
NU.
NU.e ed d ing s es n enoth.
Found Types

oth.
en
n
es
s
ing
d
ed
e
NU.
NU.e ed d ing s es n enoth.
Found Tokens

0.2
0.8
0.6
0.4
Proportion of types with each suffix

0.2
0.4
0.6
0.8
Proportion of tokens with each suffix

Figure 2: (a) Results for the morphology model, varying a. (b) Confusion matrices for the
morphology model with a = 0. The area of a square at location (i, j ) is proportional to the
number of word types (top) or tokens (bottom) with true sufﬁx i and found sufﬁx j .

4.3 Experiments

We applied our model to a data set consisting of all the verbs in the training section of
the Penn Wall Street Journal treebank (137,997 tokens belonging to 7,761 types). This
simple test case using only a single part of speech makes our results easy to analyze. We
determined the true sufﬁx of each word using simple heuristi cs based on the part-of-speech
tag and spelling of the word.3 We then ran a Gibbs sampler using 6 classes, and compared
the results of our learning algorithm to the true sufﬁxes fou nd in the corpus.

As noted above, the Gibbs sampler does not converge to a single analysis of the data, but
rather to a distribution over analyses. For evaluation, we u sed a single sample taken after
1000 iterations. Figure 2 (a) shows the distribution of sufﬁ xes found by the model for
various values of a, as well as the true distribution. We analyzed the results in two ways:
by counting each sufﬁx once for each word type it was associat ed with, and by counting
once for each word token (thus giving more weight to the resul ts for frequent words).

The most salient aspect of our results is that, regardless of whether we evaluate on types or
tokens, it is clear that low values of a are far more effective for learning morphology than
higher values. With higher values of a, the system has too strong a preference for empty
sufﬁxes. This observation seems to support the linguists’ v iew of type-based generalization.

It is also worth explaining why our morphological learner ﬁn ds so many e and es sufﬁxes.
This problem is common to other morphological learning systems with similar models (e.g.
[8]) and is due to the spelling rule in English that deletes stem- ﬁnal e before certain sufﬁxes.
Since the system has no knowledge of spelling rules, it tends to hypothesize analyses such
as {stat.e, stat.ing, stat.ed, stat.es}, where the e and es sufﬁxes take the place of NULL
and s. This effect can be seen clearly in the confusion matrices shown in Figure 2 (b). The
remaining errors seen in the confusion matrices are those wh ere the system hypothesized an
empty sufﬁx when in fact a non-empty sufﬁx was present. Analy
sis of our results showed
that these cases were mostly words where no other form with th e same stem was present in

3The part-of-speech tags distinguish between past tense, past participle, progressive, 3rd person
present singular, and inﬁnitive/unmarked verbs, and there fore roughly correlate with actual sufﬁxes.

the corpus. There was therefore no reason for the system to pr efer a non-empty sufﬁx.

5 Conclusion

We have shown that statistical language models that exhibit one of the most striking prop-
erties of natural languages – power-law distributions – can
be de ﬁned by breaking the pro-
cess of generating words into two stages, with a generator producing a set of words, and an
adaptor determining their frequencies. Our morphology mod el and the Pitman-Yor process
are particular choices for a generator and an adaptor. These choices produce empirical and
theoretical results that justify the role of word types in fo rmal analyses of natural language.
However, the greatest strength of this framework lies in its generality: we anticipate that
other choices of generators and adaptors will yield similar ly interesting results.

References
[1] G. Zipf. Selective Studies and the Principle of Relative Frequency in Language. Harvard
University Press, Cambridge, MA, 1932.
[2] M. Mitzenmacher. A brief history of generative models for power law and lognormal distribu-
tions. Internet Mathematics, 1(2):226–251, 2003.
[3] H.A. Simon. On a class of skew distribution functions. Biometrika, 42(3/4):425–440, 1955.
[4] J. Pitman. Exchangeable and partially exchangeable random partitions. Probability Theory and
Related Fields, 102:145–158, 1995.
[5] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855–900, 1997.
[6] H. Ishwaran and L. F. James. Generalized weighted Chinese restaurant processes for species
sampling mixture models. Statistica Sinica, 13:1211–1235, 2003.
[7] J. B. Pierrehumbert. Probabilistic phonology: discrim ination and robustness. In R. Bod, J. Hay,
and S. Jannedy, editors, Probabilistic linguistics. MIT Press, Cambridge, MA, 2003.
[8] J. Goldsmith. Unsupervised learning of the morphology of a natural language. Computational
Linguistics, 27:153–198, 2001.
[9] H. Ney, U. Essen, and R. Kneser. On structuring probabilistic dependences in stochastic lan-
guage modeling. Computer, Speech, and Language, 8:1–38, 1994.
[10] R. Kneser and H. Ney. Improved backing-off for n-gram language modeling. In Proceedings
of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1995.
[11] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard
University, 1998.
[12] D. Aldous. Exchangeability and related topics. In ´Ecole d’ ´et ´e de probabilit ´es de Saint-Flour,
XIII—1983 , pages 1–198. Springer, Berlin, 1985.
[13] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9:249–265, 2000.
[14] L. Larkey, L. Ballesteros, and M. Connell.
Improving stemming for arabic information re-
trieval: Light stemming and co-occurrence analysis. In Proceedings of the 25th International
Conference on Research and Development in Information Retrieval (SIGIR), 2002.
[15] W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo in
Practice. Chapman and Hall, Suffolk, 1996.


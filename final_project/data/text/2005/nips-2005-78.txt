Hyperparameter and Kernel Learning for
Graph Based Semi-Supervised Classiﬁcation

Ashish Kapoory , Yuan (Alan) Qiz , Hyungil Ahny and Rosalind W. Picardy
yMIT Media Laboratory, Cambridge, MA 02139
fkapoor, hiahn, picardg@media.mit.edu
zMIT CSAIL, Cambridge, MA 02139
alanqi@csail.mit.edu

Abstract

There have been many graph-based approaches for semi-supervised clas-
siﬁcation. One problem is that of hyperparameter learning: performance
depends greatly on the hyperparameters of the similarity graph, trans-
formation of the graph Laplacian and the noise model. We present a
Bayesian framework for learning hyperparameters for graph-based semi-
supervised classiﬁcation. Given some labeled data, which can contain
inaccurate labels, we pose the semi-supervised classiﬁcation as an in-
ference problem over the unknown labels. Expectation Propagation is
used for approximate inference and the mean of the posterior is used for
classiﬁcation. The hyperparameters are learned using EM for evidence
maximization. We also show that the posterior mean can be written in
terms of the kernel matrix, providing a Bayesian classiﬁer
to classify new
points. Tests on synthetic and real datasets show cases where there are
improvements in performance over the existing approaches.
signiﬁcant

1 Introduction

A lot of recent work on semi-supervised learning is based on regularization on graphs [5].
The basic idea is to ﬁrst create a graph with the labeled and unlabeled data points as the
vertices and with the edge weights encoding the similarity between the data points. The aim
is then to obtain a labeling of the vertices that is both smooth over the graph and compatible
with the labeled data. The performance of most of these algorithms depends upon the edge
weights of the graph. Often the smoothness constraints on the labels are imposed using a
transformation of the graph Laplacian and the parameters of the transformation affect the
performance. Further, there might be other parameters in the model, such as parameters
to address label noise in the data. Finding a right set of parameters is a challenge, and
usually the method of choice is cross-validation, which can be prohibitively expensive for
real-world problems and problematic when we have few labeled data points.

Most of the methods ignore the problem of learning hyperparameters that determine the
similarity graph and there are only a few approaches that address this problem. Zhu et al.
[8] propose learning non-parametric transformation of the graph Laplacians using semidef-
inite programming. This approach assumes that the similarity graph is already provided;
thus, it does not address the learning of edge weights. Other approaches include label

entropy minimization [7] and evidence-maximization using the Laplace approximation [9].

This paper provides a new way to learn the kernel and hyperparameters for graph based
semi-supervised classiﬁcation, while adhering to a Bayesian framework. The semi-
supervised classiﬁcation is posed as a Bayesian inference. We use the evidence to si-
multaneously tune the hyperparameters that deﬁne
the structure of the similarity graph,
the parameters that determine the transformation of the graph Laplacian, and any other
parameters of the model. Closest to our work is Zhu et al. [9], where they proposed a
Laplace approximation for learning the edge weights. We use Expectation Propagation
(EP), a technique for approximate Bayesian inference that provides better approximations
than Laplace. An additional contribution is a new EM algorithm to learn the hyperparam-
eters for the edge weights, the parameters of the transformation of the graph spectrum.
More importantly, we explicitly model the level of label noise in the data, while [9] does
comparison of hyperparameter learning with
not do. We provide what may be the ﬁrst
cross-validation on state-of-the-art algorithms (LLGC [6] and harmonic ﬁelds
[7]).

2 Bayesian Semi-Supervised Learning

We assume that we are given a set of data points X = fx1 ; ::; xn+m g, of which XL =
fx1 ; ::; xn g are labeled as tL = ft1 ; ::; tn g and XU = fxn+1 ; ::; xn+m g are unlabeled.
Throughout this paper we limit ourselves to two-way classiﬁcation,
thus t 2 f(cid:0)1; 1g. Our
model assumes that the hard labels ti depend upon hidden soft-labels yi for all i. Given
the dataset D = [fXL ; tL g; XU ], the task of semi-supervised learning is then to infer the
posterior p(tU jD), where tU = [tn+1 ; ::; tn+m ]. The posterior can be written as:
p(tU jD) = Zy
In this paper, we propose to ﬁrst
approximate the posterior p(yjD) and then use (1) to
classify the unlabeled data. Using the Bayes rule we can write:

p(tU jy)p(yjD)

(1)

p(yjD) = p(yjX; tL ) / p(yjX)p(tL jy)
The term, p(yjX) is the prior. It enforces a smoothness constraint and depends upon the
underlying data manifold. Similar to the spirit of graph regularization [5] we use similarity
graphs and their transformed Laplacian to induce priors on the soft labels y. The second
term, p(tL jy) is the likelihood that incorporates the information provided by the labels.
In this paper, p(yjD) is inferred using Expectation Propagation, a technique for approxi-
mate Bayesian inference [3]. In the following subsections ﬁrst we describe the prior and
the likelihood in detail and then we show how evidence maximization can be used to learn
hyperparameters and other parameters in the model.

2.1 Priors and Regularization on Graphs

The prior plays a signiﬁcant
role in semi-supervised learning, especially when there is only
a small amount of labeled data. The prior imposes a smoothness constraint and should be
such that it gives higher probability to the labelings that respect the similarity of the graph.

The prior, p(yjX), is constructed by ﬁrst
forming an undirected graph over the data points.
The data points are the nodes of the graph and edge-weights between the nodes are based
on similarity. This similarity is usually captured using a kernel. Examples of kernels
include RBF, polynomial etc. Given the data points and a kernel, we can construct an
(n + m) (cid:2) (n + m) kernel matrix K , where Kij = k(xi ; xj ) for all i 2 f1; ::; n + mg.
Lets consider the matrix ~K , which is same as the matrix K , except that the diagonals are set
~Kij , then we can construct the
to zero. Further, if G is a diagonal matrix such that Gii = Pj

2 ~KG(cid:0) 1
combinatorial Laplacian ((cid:1) = G(cid:0) ~K ) or the normalized Laplacian ( ~(cid:1) = I (cid:0)G(cid:0) 1
2 )
of the graph. For brevity, in the text we use (cid:1) as a notation for both the Laplacians. Both
the Laplacians are symmetric and positive semideﬁnite. Consider the eigen decomposition
of (cid:1) where fvi g denote the eigenvectors and f(cid:21)i g the corresponding eigenvalues; thus, we
can write (cid:1) = Pn+m
i . Usually, a transformation r((cid:1)) = Pn+m
that
i=1 (cid:21)ivivT
i=1 r((cid:21)i )vivT
i
modiﬁes
the spectrum of (cid:1) is used as a regularizer. Speciﬁcally , the smoothness imposed
by this regularizer prefers soft labeling for which the norm yT r((cid:1))y is small. Equivalently,
we can interpret this probabilistically as following:

2 yT r((cid:1))y = N (0; r((cid:1))(cid:0)1 )
p(yjX) / e(cid:0) 1

(2)

Where r((cid:1))(cid:0)1 denotes the pseudo-inverse if the inverse does not exist. Equation (2) sug-
gests that the labelings with the small value of yT r((cid:1))y are more probable than the others.
Note, that when r((cid:1)) is not invertible the prior is improper. The fact that the prior can
be written as a Gaussian is advantageous as techniques for approximate inference can be
easily applied. Also, different choices of transformation functions lead to different semi-
supervised learning algorithms. For example, the approach based on Gaussian ﬁelds and
harmonic functions (Harmonic) [7] can be thought of as using the transformation r((cid:21)) = (cid:21)
on the combinatorial Laplacian without any noise model. Similarly, the approach based in
local and global consistency (LLGC) [6] can be thought of as using the same transforma-
tion but on the normalized Laplacian and a Gaussian likelihood. Therefore, it is easy to see
that most of these algorithms can exploit the proposed evidence maximization framework.
In the following we focus only on the parametric linear transformation r((cid:21)) = (cid:21) + (cid:14) . Note
that this transformation removes zero eigenvalues from the spectrum of (cid:1).

2.2 The Likelihood

Assuming conditional independence of the observed labels given the hidden soft labels, the
likelihood p(tL jy) can be written as p(tL jy) = Qn
i=1 p(ti jyi ). The likelihood models the
probabilistic relation between the observed label ti and the hidden label yi . Many real-
world datasets contain hand-labeled data and can often have labeling errors. While most
people tend to model label errors with a linear or quadratic slack in the likelihood, it has
been noted that such an approach does not address the cases where label errors are far from
the decision boundary [2]. The ﬂipping likelihood can handle errors even when they are far
from the decision boundary and can be written as:

p(ti jyi ) = (cid:15)(1 (cid:0) (cid:8)(yi (cid:1) ti )) + (1 (cid:0) (cid:15))(cid:8)(yi (cid:1) ti ) = (cid:15) + (1 (cid:0) 2(cid:15))(cid:8)(yi (cid:1) ti )

(3)

Here, (cid:8) is the step function, (cid:15) is the labeling error rate and the model admits possibility of
errors in labeling with a probability (cid:15). This likelihood has been earlier used in the context
of Gaussian process classiﬁcation [2][4]. The above described likelihood explicitly models
the labeling error rate; thus, the model should be more robust to the presence of label noise
in the data. The experiments in this paper use the ﬂipping noise likelihood shown in (3).

2.3 Approximate Inference

In this paper, we use EP to obtain a Gaussian approximation of the posterior p(yjD).
Although, the prior derived in section 2.1 is a Gaussian distribution, the exact posterior
is not a Gaussian due to the form of the likelihood. We use EP to approximate the posterior
as a Gaussian and then equation (1) can be used to classify unlabeled data points. EP has
been previously used [3] to train a Bayes Point Machine, where EP starts with a Gaussian
prior over the classiﬁers and produces a Gaussian posterior. Our task is very similar and we
use the same algorithm. In our case, EP starts with the prior deﬁned in (2) and incorporates
likelihood to approximate the posterior p(yjD) (cid:24) N ( (cid:22)y; (cid:6)y ).

2.4 Hyperparameter Learning

We use evidence maximization to learn the hyperparameters. Denote the parameters of
the kernel as (cid:2)K and the parameters of transformation of the graph Laplacian as (cid:2)T .
Let (cid:2) = f(cid:2)K ; (cid:2)T ; (cid:15)g, where (cid:15) is the noise hyperparameter. The goal is to solve ^(cid:2) =
arg max(cid:2) log[p(tL jX; (cid:2))].
Non-linear optimization techniques, such as gradient descent or Expectation Maximization
(EM) can be used to optimize the evidence. When the parameter space is small then the
Matlab function fminbnd, based on golden section search and parabolic interpolation,
can be used. The main challenge is that the gradient of evidence is not easy to compute.
Previously, an EM algorithm for hyperparameter learning [2] has been derived for Gaus-
sian Process classiﬁcation. Using similar ideas we can derive an EM algorithm for semi-
supervised learning. In the E-step EP is used to infer the posterior q(y) over the soft labels.
The M-step consists of maximizing the lower bound:
p(yjX; (cid:2))p(tL jy; (cid:2))
F = Zy
q(y)
= (cid:0) Zy
q(y) log q(y) + Zy
n
Zyi
Xi=1
The EM procedure alternates between the E-step and the M-step until convergence.

q(yi ) log ((cid:15) + (1 (cid:0) 2(cid:15))(cid:8)(yi (cid:1) ti )) (cid:20) p(tL jX; (cid:2))

q(y) log N (y; 0; r((cid:1))(cid:0)1 )

q(y) log

+

(cid:15) E-Step: Given the current parameters (cid:2)i , approximate the posterior q(y) (cid:24)
N ( (cid:22)y; (cid:6)y ) by EP.
(cid:15) M-Step: Update
(cid:2)i+1 = arg max(cid:2) Ry q(y) log p(yjX;(cid:2))p(tL jy;(cid:2))
q(y)
In the M-step the maximization with respect to the (cid:2) cannot be computed in a closed
form, but can be solved using gradient descent. For maximizing the lower bound, we used
gradient based projected BFGS method using Armijo rule and simple line search. When
using the linear transformation r((cid:21)) = (cid:21) + (cid:14) on the Laplacian (cid:1), the prior p(yjX; (cid:2)) can
be written as N (0; ((cid:1) + (cid:14)I )(cid:0)1 ). Deﬁne Z = (cid:1) + (cid:14)I then, the gradients of the lower bound
with respect to the parameters are as follows:
1
1
tr(Z(cid:0)1 @(cid:1)
@F
@(cid:2)K
2
@(cid:2)K
2
1
1
@F
2
@(cid:2)T
2
n
Xi=1

1 (cid:0) 2(cid:8)(ti (cid:1) (cid:22)yi )
(cid:15) + (1 (cid:0) 2(cid:15))(cid:8)(ti (cid:1) (cid:22)yi )

where: (cid:22)yi = Zy

(cid:22)yT @(cid:1)
@(cid:2)K

tr(Z(cid:0)1 ) (cid:0)

@(cid:1)
@(cid:2)K

(cid:22)y (cid:0)

tr(

(cid:22)yT (cid:22)y (cid:0)

=

=

(cid:25)

1
2

tr((cid:6)y )

@F
@ (cid:15)

) (cid:0)

(cid:6)y )

1
2

yi q(y)

It is easy to show that the provided approximation of the derivative @F
@ (cid:15) equals zero, when
n , where k is the number of labeled data points differing in sign from their posterior
(cid:15) = k
means. The EM procedure described here is susceptible to local minima and in a few cases
might be too slow to converge. Especially, when the evidence curve is ﬂat and the initial
values are far from the optimum, we found that the EM algorithm provided very small
steps, thus, taking a long time to converge.

Whenever we encountered this problem in the experiments, we used an approximate gradi-
ent search to ﬁnd a good value of initial parameters for the EM algorithm. Essentially as the
gradients of the evidence are hard to compute, they can be approximated by the gradients
of the lower bound and can be used in any gradient ascent procedure.


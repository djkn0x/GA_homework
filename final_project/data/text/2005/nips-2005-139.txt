Preconditioner Approximations for
Probabilistic Graphical Models

Pradeep Ravikumar
John Lafferty
School of Computer Science
Carnegie Mellon University

Abstract

We present a family of approximation techniques for probabilistic graph-
ical models, based on the use of graphical preconditioners developed in
the scienti ﬁc computing literature. Our framework yields r igorous upper
and lower bounds on event probabilities and the log partition function
of undirected graphical models, using non-iterative procedures that have
low time complexity. As in mean ﬁeld approaches, the approxi mations
are built upon tractable subgraphs; however, we recast the problem of op-
timizing the tractable distribution parameters and approximate inference
in terms of the well-studied linear systems problem of obtaining a good
matrix preconditioner. Experiments are presented that compare the new
approximation schemes to variational methods.

1

Introduction

Approximate inference techniques are enabling sophisticated new probabilistic models to
be developed and applied to a range of practical problems. One of the primary uses of
approximate inference is to estimate the partition function and event probabilities for undi-
rected graphical models, which are natural tools in many domains, from image processing
to social network modeling. A central challenge is to improve the accuracy of existing ap-
proximation methods, and to derive rigorous rather than heuristic bounds on probabilities in
such graphical models. In this paper, we present a simple new approach to the approximate
inference problem, based upon non-iterative procedures that have low time complexity. We
follow the variational mean ﬁeld intuition of focusing on tr actable subgraphs, however we
recast the problem of optimizing the tractable distribution parameters as a generalized lin-
ear system problem. In this way, the task of deriving a tractable distribution conveniently
reduces to the well-studied problem of obtaining a good preconditioner for a matrix (Bo-
man and Hendrickson, 2003). This framework has the added advantage that tighter bounds
can be obtained by reducing the sparsity of the preconditioners, at the expense of increasing
the time complexity for computing the approximation.

In the following section we establish some notation and background.
In Section 3, we
outline the basic idea of our proposed framework, and explain how to use preconditioners
for deriving tractable approximate distributions. In Sections 3.1 and 4, we then describe
the underlying theory, which we call the generalized support theory for graphical models.
In Section 5 we present experiments that compare the new approximation schemes to some
of the standard variational and optimization based methods.

2 Notation and Background

(1)

Consider a graph G = (V , E ), where V denotes the set of nodes and E denotes the set
of edges. Let Xi be a random variable associated with node i, for i ∈ V , yielding a
random vector X = {X1 , . . . , Xn }. Let φ = {φα , α ∈ I } denote the set of potential
functions or sufﬁcient statistics , for a set I of cliques in G. Associated with φ is a vector of
parameters θ = {θα , α ∈ I }. With this notation, the exponential family of distributions of
X , associated with φ and G, is given by
p(x; θ) = exp  Xα
θαφα − Ψ(θ)! .
For traditional reasons through connections with statistical physics, Z = exp Ψ(θ) is called
the partition function. As discussed in (Yedidia et al., 2001), at the expense in increasing
the state space one can assume without loss of generality that the graphical model is a
pairwise Markov random ﬁeld,
i.e., the set of cliques I is the set of edges {(s, t) ∈ E }.
We shall assume a pairwise random ﬁeld, and thus can express t he potential function and
parameter vectors in more compact form as matrices:
 (2)
 Φ(x) := 
Θ := 
In the following we will denote the trace of the product of two matrices A and B by the in-
ner product hhA, B ii. Assuming that each Xi is ﬁnite-valued, the partition function Z (Θ) is
then given by Z (Θ) = Px∈χ exp hhΘ, Φ(x)ii. The computation of Z (Θ) has a complex-
ity exponential in the tree-width of the graph G and hence is intractable for large graphs.
Our goal is to obtain rigorous upper and lower bounds for this partition function, which can
then be used to obtain rigorous upper and lower bounds for general event probabilities; this
is discussed further in (Ravikumar and Lafferty, 2004).

. . . φ1n (x1 , xn )
...
...
. . . φnn (xn , xn )

φ11 (x1 , x1 )
...
φn1 (xn , x1 )

θ11
...
θn1

. . .
...
. . .

θ1n
...
θnn

2.1 Preconditioners in Linear Systems

Consider a linear system, Ax = c, where the variable x is n dimensional, and A is an
n × n matrix with m non-zero entries. Solving for x via direct methods such as Gaussian
elimination has a computational complexity O(n3 ), which is impractical for large values
of n. Multiplying both sides of the linear system by the inverse of an invertible matrix
B , we get an equivalent “preconditioned” system, B−1Ax = B−1 c. If B is similar to A,
B−1A is in turn similar to I , the identity matrix, making the preconditioned system easier
to solve. Such an approximating matrix B is called a preconditioner.
The computational complexity of preconditioned conjugate gradient is given by
ǫ (cid:19)
T (A) = pκ(A, B ) (m + T (B )) log (cid:18) 1
where T (A) is the time required for an ǫ-approximate solution; κ(A, B ) is the condition
number of A and B which intuitively corresponds to the quality of the approximation B ,
and T (B ) is the time required to solve B y = c.
Recent developments in the theory of preconditioners are in part based on support graph
theory, where the linear system matrix is viewed as the Laplacian of a graph, and graph-
based techniques can be used to obtain good approximations. While these methods re-
quire diagonally dominant matrices (Aii ≥ Pj 6=i |Aij |), they yield “ultra-sparse” (tree
plus a constant number of edges) preconditioners with a low condition number.
In our

(3)

experiments, we use two elementary tree-based preconditioners in this family, Vaidya’s
Spanning Tree preconditioner Vaidya (1990), and Gremban-Miller’s Support Tree precon-
ditioner Gremban (1996).

3 Graphical Model Preconditioners

Our proposed framework follows the generalized mean ﬁeld in tuition of looking at sparse
graph approximations of the original graph, but solving a different optimization problem.
We begin by outlining the basic idea, and then develop the underlying theory.

Consider the graphical model with graph G, potential-function matrix Φ(x), and parameter
matrix Θ. For purposes of intuition, think of the graphical model “en ergy” hhΘ, Φ(x)ii as
the matrix norm x⊤Θx. We would like to obtain a sparse approximation B for Θ. If B
approximates Θ well, then the condition number κ is small:
x⊤Bx (cid:30) min
x⊤Θx
x
This suggests the following procedure for approximate inference. First, choose a matrix B
that minimizes the condition number with Θ (rather than KL divergence as in mean- ﬁeld).
Then, scale B appropriately, as detailed in the following sections. Finally, use the scaled
matrix B as the parameter matrix for approximate inference. Note that if B corresponds to
a tree, approximate inference has linear time complexity.

= λmax (Θ, B ) /λmin (Θ, B )

κ(Θ, B ) = max
x

x⊤Θx
x⊤Bx

(4)

3.1 Generalized Eigenvalue Bounds

Given a graphical model with graph G, potential-function matrix Φ(x), and parameter
matrix Θ, our goal is to obtain parameter matrices ΘU and ΘL , corresponding to sparse
graph approximations of G, such that

(5)
Z (ΘL ) ≤ Z (Θ) ≤ Z (ΘU ).
That is, the partition functions of the sparse graph parameter matrices ΘU and ΘL are upper
and lower bounds, respectively, of the partition function of the original graph. However,
we will instead focus on a seemingly much stronger condition; in particular, we will look
for ΘL and ΘU that satisfy
(6)
hhΘL , Φ(x)ii ≤ hhΘ, Φ(x)ii ≤ hhΘU , Φ(x)ii
for all x. By monotonicity of exp, this stronger condition implies condition (5) on the
partition function, by summing over the values of X . However, this stronger condition will
give us greater ﬂexibility, and rigorous bounds for general event probabilities since then

exp hhΘL , Φ(x)ii
exp hhΘU , Φ(x)ii
Z (ΘU )
Z (ΘL )
In contrast, while variational methods give bounds on the log partition function, the derived
bounds on general event probabilities via the variational parameters are only heuristic.

≤ p(x; Θ) ≤

(7)

.

Let S be a set of sparse graphs; for example, S may be the set of all trees. Focusing on the
upper bound, we for now would like to obtain a graph G′ ∈ S with parameter matrix B ,
which approximates G, and whose partition function upper bounds the partition function
of the original graph. Following (6), we require,
(8)
hhΘ, Φ(x)ii ≤ hhB , Φ(x)ii , such that G(B ) ∈ S
where G(B ) denotes the graph corresponding to the parameter matrix B . Now, we would
like the distribution corresponding to B to be as close as possible to the distribution corre-
sponding to Θ; that is, hhB , Φ(x)ii should not only upper bound hhΘ, Φ(x)ii but should be

close to it. The distance measure we use for this is the minimax distance. In other words,
while the upper bound requires that

hhΘ, Φ(x)ii
hhB , Φ(x)ii

≤ 1,

(9)

we would like

hhΘ, Φ(x)ii
min
hhB , Φ(x)ii
x
to be as high as possible. Expressing these desiderata in the form of an optimization prob-
lem, we have

(10)

B ⋆ = arg max
B : G(B )∈S

min
x

hhΘ,Φ(x)ii
hhB ,Φ(x)ii , such that

hhΘ,Φ(x)ii
hhB ,Φ(x)ii ≤ 1.

Before solving this problem, we ﬁrst make some deﬁnitions, w hich are generalized versions
of standard concepts in linear systems theory.

Deﬁnition 3.1. For a pairwiseMarkov random ﬁeldwith potential functionma trix Φ(x);
thegeneralized eigenvaluesof apairofparametermatrices (A, B ) aredeﬁned as

λΦ
max(A, B ) =

max
x: hhB ,Φ(x)ii6=0

λΦ
min(A, B ) =

min
x: hhB ,Φ(x)ii6=0

hhA, Φ(x)ii
hhB , Φ(x)ii
hhA, Φ(x)ii
hhB , Φ(x)ii

.

Note that

λΦ
max (A, αB ) =

hhA, Φ(x)ii
hhαB , Φ(x)ii
1
hhA, Φ(x)ii
hhB , Φ(x)ii
α
We state the basic properties of the generalized eigenvalues in the following lemma.

= α−1λΦ
max (A, B ).

max
x: hhαB ,Φ(x)ii 6=0

max
x: hhB ,Φ(x)ii 6=0

=

Lemma 3.2. Thegeneralized eigenvalues satisfy

≤ λΦ
max(A, B )

λΦ
min(A, B ) ≤

hhA, Φ(x)ii
hhB , Φ(x)ii
λΦ
max(A, αB ) = α−1λΦ
max(A, B )
min(A, αB ) = α−1λΦ
λΦ
min(A, B )
1
λΦ
min(A, B ) =
λΦ
max(B , A)

.

(11)

(12)

(13)

(14)

(15)

(16)
(17)

(18)

In the following, we will use A to generically denote the parameter matrix Θ of the model.
We can now rewrite the optimization problem for the upper bound in equation (11) as
min (A, B ), such that λΦ
λΦ
max (A, B ) ≤ 1

max
B : G(B )∈S
We shall express the optimal solution of Problem Λ1 in terms of the optimal solution of a
companion problem. Towards that end, consider the optimization problem

(Problem Λ1 )

(19)

λΦ
max (A, C )
λΦ
min (A, C )
The following proposition shows the sense in which these problems are equivalent.

(Problem Λ2 )

min
C : G(C )∈S

.

(20)

≤

(21)

(22)

(23)

λΦ
min (A, B ) ≤

Proposition 3.3. If bC attainstheoptimuminProblem Λ2, then eC = λΦ
max(A, bC ) bC attains
theoptimumofProblem Λ1.
Proof. For any feasible solution B of Problem Λ1 , we have
λΦ
min (A, B )
(since λΦ
max (A, B ) ≤ 1)
λΦ
max (A, B )
min (A, bC )
λΦ
(since bC is the optimum of Problem Λ2 )
max (A, bC )
λΦ
max (A, bC ) bC (cid:17) (from Lemma 3.2)
min (cid:16)A, λΦ
= λΦ
min (A, eC ).
= λΦ
(24)
Thus, eC upper bounds all feasible solutions in Problem Λ1 . However, it itself is a feasible
solution, since
max (A, bC ) bC (cid:17) =
max (cid:16)A, λΦ
1
max (A, eC ) = λΦ
max (A, bC ) = 1
λΦ
λΦ
(25)
max (A, bC )
λΦ
from Lemma 3.2. Thus, eC attains the maximum in the upper bound Problem Λ1 . (cid:3)
The analysis for obtaining an upper bound parameter matrix B for a given parameter matrix
A carries over for the lower bound; we need to replace a maximin problem with a minimax
problem. For the lower bound, we want a matrix B such that
hhA, Φ(x)ii
max
min
hhB , Φ(x)ii
{x: hhB ,Φ(x)ii6=0}
B : G(B )∈S
This leads to the following lower bound optimization problem.
λΦ
max (A, B ), such that λΦ
(Problem Λ3 )
min (A, B ) ≥ 1.

hhA, Φ(x)ii
hhB , Φ(x)ii

, such that

≥ 1 (26)

(27)

B⋆ =

min
B : G(B )∈S

The proof of the following statement closely parallels the proof of Proposition 3.3.

min(A, ˆC ) ˆC attains
Proposition 3.4. If ˆC attains theoptimum inProblem Λ2, then C = λΦ
theoptimumof the lowerboundProblem Λ3.

(28)

Finally, we state the following basic lemma, whose proof is easily veri ﬁed.
Lemma 3.5. For anypairofparameter-matrices (A, B ),wehave
min(A, B )B , Φ(x)(cid:11)(cid:11) ≤ hhA, Φ(x)ii ≤ (cid:10)(cid:10)λΦ
max(A, B )B , Φ(x)(cid:11)(cid:11) .
(cid:10)(cid:10)λΦ
3.2 Main Procedure
We now have in place the machinery necessary to describe the procedure for solving the
main problem in equation (6), to obtain upper and lower bound matrices for a graphical
model. Lemma 3.5 shows how to obtain upper and lower bound parameter matrices with
respect to any matrix B , given a parameter matrix A, by solving a generalized eigenvalue
problem. Propositions 3.3 and 3.4 tell us, in principle, how to obtain the optimal such
upper and lower bound matrices. We thus have the following procedure. First, obtain a
min (Θ, C ). Then
parameter matrix C such that G(C ) ∈ S , which minimizes λΦ
max (Θ, C )/λΦ
max (Θ, C ) C gives the optimal upper bound parameter matrix and λΦ
min (Θ, C ) C gives the
λΦ
optimal lower bound parameter matrix. However, as things stand, this recipe appears to
be even more challenging to work with than the generalized mean ﬁeld procedures. The
difﬁculty lies in obtaining the matrix C .
In the following section we offer a series of
relaxations that help to simplify this task.

4 Generalized Support Theory for Graphical Models

In what follows, we begin by assuming that the potential function matrix is positive semi-
deﬁnite, Φ(x) (cid:23) 0, and later extend our results to general Φ.

Deﬁnition 4.1. For a pairwiseMRF with potential functionmatrix Φ(x) (cid:23) 0, the gener-
alized support number of apairofparametermatrices (A, B ),where B (cid:23) 0, is
σΦ (A, B ) = min {τ ∈ R | hhτ B , Φ(x)ii ≥ hhA, Φ(x)ii for all x}

(29)

The generalized support number can be thought of as the “numb er of copies ” τ of B re-
quired to “support ” A so that hhτ B − A, Φ(x)ii ≥ 0. The usefulness of this deﬁnition is
demonstrated by the following result.

Proposition 4.2. If B (cid:23) 0 then λΦ
max(A, B ) ≤ σΦ (A, B ).
Proof. From the deﬁnition of the generalized support number for a g raphical model,
we have that (cid:10)(cid:10)σΦ (A, B )B − A, Φ(x)(cid:11)(cid:11) ≥ 0. Now, since we assume that Φ(x) (cid:23) 0, if
also B (cid:23) 0 then hhB , Φ(x)ii ≥ 0. Therefore, it follows that hhA,Φ(x)ii
hhB ,Φ(x)ii ≤ σΦ (A, B ), and
thus
hhA, Φ(x)ii
≤ σΦ (A, B )
λΦ
(30)
max (A, B ) = max
hhB , Φ(x)ii
x
giving the statement of the proposition. (cid:3)

This leads to our ﬁrst relaxation of the generalized eigenva lue bound for a model. From
Lemma 3.2 and Proposition 4.2 we see that
λΦ
max (A, B )
λΦ
min (A, B )
Thus, this result suggests that to approximate the graphical model (Θ, Φ) we can search for
a parameter matrix B ⋆ , with corresponding simple graph G(B ⋆ ) ∈ S , such that
σΦ (Θ, B )σΦ (B , Θ)

= λΦ
max (A, B )λΦ
max (B , A) ≤ σΦ (A, B )σΦ (B , A)

(31)

B ⋆ = arg min
B

(32)

While this relaxation may lead to effective bounds, we will now go further, to derive an
additional relaxation that relates our generalized graphical model support number to the
“classical ” support number.

Proposition 4.3. For a potential functionmatrix Φ(x) (cid:23) 0, σΦ (A, B ) ≤ σ(A, B ),where
σ(A, B ) = min{τ | (τ B − A) (cid:23) 0}.

Proof. Since σ(A, B )B −A (cid:23) 0 by deﬁnition and Φ(x) (cid:23) 0 by assumption, we have
that hhσ(A, B )B − A, Φ(x)ii ≥ 0. Therefore, σΦ (A, B ) ≤ σ(A, B ) from the deﬁnition of
generalized support number. (cid:3)

The above result reduces the problem of approximating a graphical model to the problem
of minimizing classical support numbers, the latter problem being well-studied in the sci-
enti ﬁc computing literature (Boman and Hendrickson, 2003; Bern et al., 2001), where the
expression σ(A, C )σ(C, A) is called the condition number, and a matrix that minimizes
it within a simple family of graphs is called a preconditioner. We can thus plug in any
algorithm for ﬁnding a sparse preconditioner for Θ, carrying out the optimization
B ⋆ = arg min
B

σ(Θ, B ) σ(B , Θ)

(33)

and then use that matrix B ⋆ in our basic procedure.
One example is Vaidya’s preconditioner Vaidya (1990), which is essentially the maximum
spanning tree of the graph. Another is the support tree of Gremban (1996), which intro-
duces Steiner nodes, in this case auxiliary nodes introduced via a recursive partitioning
of the graph. We present experiments with these basic preconditioners in the following
section.

Before turning to the experiments, we comment that our generalized support number anal-
ysis assumed that the potential function matrix Φ(x) was positive semi-deﬁnite. The case
when it is not can be handled as follows. We ﬁrst add a large pos itive diagonal matrix D
so that Φ′ (x) = Φ(x) + D (cid:23) 0. Then, for a given parameter matrix Θ, we use the above
machinery to get an upper bound parameter matrix B such that

hhA, Φ(x) + Dii ≤ hhB , Φ(x) + Dii ⇒ hhA, Φ(x)ii ≤ hhB , Φ(x)ii + hhB − A, Dii .
(34)
Exponentiating and summing both sides over x, we then get the required upper bound for
the parameter matrix A; the same can be done for the lower bound.

5 Experiments

As the previous sections detailed, the preconditioner based bounds are in principle quite
easy to compute—we compute a sparse preconditioner for the pa
rameter matrix (typi-
cally O(n) to O(n3 )) and use the preconditioner as the parameter matrix for the bound
computation (which is linear if the preconditioner matrix corresponds to a tree). This
yields a simple, non-iterative deterministic procedure as compared to the more complex
propagation-based or iterative update procedures. In this section we evaluate these bounds
on small graphical models for which exact answers can be readily computed, and compare
the bounds to variational approximations.

We show simulation results averaged over a randomly generated set of graphical models.
The graphs used were 2D grid graphs, and the edge potentials were selected according to a
uniform distribution Uniform(−2dcoup , 0) for various coupling strengths dcoup . We report
the relative error, (bound − log-partition-function)/log-partition-function.
As a baseline, we use the mean ﬁeld and structured mean ﬁeld me
thods for the lower bound,
and the Wainwright et al. (2003) tree-reweighted belief propagation approximation for the
upper bound. For the preconditioner based bounds, we use two very simple precondition-
ers, (a) Vaidya’s maximum spanning tree preconditioner (Vaidya, 1990), which assumes the
input parameter matrix to be a Laplacian, and (b) Gremban (1996)’s support tree precon-
ditioner, which also gives a sparse parameter matrix corresponding to a tree, with Steiner
(auxiliary) nodes. To compute bounds over these larger graphs with Steiner nodes we aver-
age an internal node over its children; this is the technique used with such preconditioners
for solving linear systems. We note that these preconditioners are quite basic, and the use
of better preconditioners (yielding a better condition number) has the potential to achieve
much better bounds, as shown in Propositions 3.3 and 3.4. We also reiterate that while our
approach can be used to derive bounds on event probabilities, the variational methods yield
bounds only for the partition function, and only apply heuristically to estimating simple
event probabilities such as marginals.

As the plots in Figure 1 show, even for the simple preconditioners used, the new bounds
are quite close to the actual values, outperforming the mean ﬁeld method and giving com-
parable results to the tree-reweighted belief propagation method. The spanning tree pre-
conditioner provides a good lower bound, while the support tree preconditioner provides a
good upper bound, however not as tight as the bound obtained using tree-reweighted be-
lief propagation. Although we cannot compute the exact solution for large graphs, we can

Spanning Tree Preconditioner
Structured Mean Field
Support Tree Preconditioner
Mean Field

 

0.6

0.8

1.0
Coupling strength

1.4

2.0

Spanning tree Preconditioner
Structured Mean Field
Support Tree Preconditioner
Mean Field

r
o
r
r
e
 
e
v
i
t
a
l
e
r
 
e
g
a
r
e
v
A

1.4

1.2

1

0.8

0.6

0.4

0.2

0

 

n
o
i
t
c
n
u
f
 
n
o
i
t
i
t
r
a
p
 
n
o
 
d
n
u
o
b
 
r
e
w
o
L

0
0
5
1

0
0
0
1

0
0
5

0

r
o
r
r
e
 
e
v
i
t
a
l
e
r
 
e
g
a
r
e
v
A

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

 

Support Tree Preconditioner
Tree BP

 

0.6

0.8

1.0
Coupling strength

1.4

2.0

Figure 1:
lower
Comparison of
bounds (top left), and upper bounds
(top right) for small grid graphs, and
lower bounds for grid graphs of in-
creasing size (left).

200

400

600

800

Number of nodes in graph

compare bounds. The bottom plot of Figure 1 compares lower bounds for graphs with up
to 900 nodes; a larger bound is necessarily tighter, and the preconditioner bounds are seen
to outperform mean ﬁeld.

Acknowledgments

We thank Gary Miller for helpful discussions. Research supported in part by NSF grants
IIS-0312814 and IIS-0427206.

References

M. Bern, J. R. Gilbert, B. Hendrickson, N. Nguyen, and S. Toledo. Support-graph preconditioners.
Submitted to SIAM J. Matrix Anal. Appl., 2001.
E. G. Boman and B. Hendrickson. Support theory for preconditioning. SIAM Journal on Matrix
Analysis and Applications, 25, 2003.
K. Gremban. Combinatorial preconditioners for sparse, symmetric, diagonally dominant linear sys-
tems. Ph.D. Thesis, Carnegie Mellon University, 1996, 1996.
P. Ravikumar and J. Lafferty. Variational Chernoff bounds for graphical models. Proceedings of
Uncertainty in Artiﬁcial Intelligence (UAI) , 2004.
P. M. Vaidya. Solving linear equations with symmetric diagonally dominant matrices by constructing
good preconditioners. 1990. Unpublished manuscript, UIUC.
M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Tree-reweighted belief propagation and approx-
imate ML estimation by pseudo-moment matching. 9th Workshop on Artiﬁcial Intelligence and
Statistics, 2003.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations.
IJCAI 2001 Distinguished Lecture track, 2001.


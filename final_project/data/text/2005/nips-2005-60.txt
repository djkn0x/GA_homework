Large-Scale Multiclass Transduction

Thomas G ¨artner
Fraunhofer AIS.KD, 53754 Sankt Augustin, Thomas.Gaertner@ais.fraunhofer.de
Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan
Statistical Machine Learning Program, NICTA and ANU, Canberra, ACT
{Quoc.Le, Simon.Burton, Alex.Smola, SVN.Vishwanathan}@nicta.com.au

Abstract

We present a method for performing transductive inference on very large
datasets. Our algorithm is based on multiclass Gaussian processes and is
effective whenever the multiplication of the kernel matrix or its inverse
with a vector can be computed sufﬁciently fast. This holds, f or instance,
for certain graph and string kernels. Transduction is achieved by varia-
tional inference over the unlabeled data subject to a balancing constraint.

1

Introduction

While obtaining labeled data remains a time and labor consuming task, acquisition and
storage of unlabelled data is becoming increasingly cheap and easy. This development
has driven machine learning research into exploring algorithms that make extensive use of
unlabelled data at training time in order to obtain better generalization performance.

A common problem of many transductive approaches is that they scale badly with the
amount of unlabeled data, which prohibits the use of massive sets of unlabeled data. Our
algorithm shows improved scaling behavior, both for standard Gaussian Process classi ﬁca-
tion and transduction. We perform classi ﬁcation on a datase t consisting of a digraph with
75, 888 vertices and 508, 960 edges. To the best of our knowledge it has so far not been
possible to perform transduction on graphs of this size in reasonable time (with standard
hardware). On standard data our method shows competitive or better performance.
Existing Transductive Approaches for SVMs use nonlinear programming [2] or EM-style
iterations for binary classi ﬁcation [4]. Moreover, on grap hs various methods for unsuper-
vised learning have been proposed [12, 11], all of which are mainly concerned with com-
puting the kernel matrix on training and test set jointly. Other formulations impose that the
label assignment on the test set be consistent with the assumption of conﬁdent classi ﬁcation
[8]. Yet others impose that training and test set have similar marginal distributions [4].

The present paper uses all three properties.
It is particularly efﬁcient whenever K α or
K −1α can be computed in linear time, where K ∈ Rm×m is the kernel matrix and α ∈ Rm .
• We require consistency of training and test marginals. This avoids problems with
overly large majority classes and small training sets.
• Kernels (or their inverses) are computed on training and test set simultaneously.
On graphs this can lead to considerable computational savings.
• Self consistency of the estimates is achieved by a variational approach. This al-
lows us to make use of Gaussian Process multiclass formulations.

2 Multiclass Classiﬁcation

We begin with a brief overview over Gaussian Process multiclass classi ﬁcation [10] recast
in terms of exponential families. Denote by X × Y with Y = {1..n} the domain of obser-
vations and labels. Moreover let X := {x1 , . . . , xm } and Y := {y1 , . . . , ym } be the set of
observations. It is our goal to estimate y |x via
p(y |x, θ) = exp (hφ(x, y), θi − g(θ |x)) where g(θ |x) = log Xy∈Y
φ(x, y) are the joint sufﬁcient statistics of x and y and g(θ |x) is the log-partition function
which takes care of the normalization. We impose a normal prior on θ , leading to the
following negative joint likelihood in θ and Y :
m
1
Xi=1
2σ2 kθk2 + const.
For transduction purposes p(θ , Y |X ) will prove more useful than p(θ |Y , X ). Note that a
normal prior on θ with variance σ21 implies a Gaussian process on the random variable
t(x, y) := hφ(x, y), θi with covariance kernel
Cov [t(x, y), t(x′ , y ′ )] = σ2 hφ(x, y), φ(x′ , y ′ )i =: σ2k((x, y), (x′ , y ′ )).

[g(θ |xi ) − hφ(xi , yi ), θi] +

exp (hφ(x, y), θi) . (1)

P := − log p(θ , Y |X ) =

(2)

(3)

(4)

kθy k2 .

hφ(x, y), θi = hφ(x), θy i and kθk2 =

Parametric Optimization Problem In the following we assume isotropy among the
class labels, that is hφ(x, y), φ(x′ , y ′ )i = δy ,y ′ hφ(x), φ(x′ )i (this is not a necessary re-
quirement for the efﬁciency of our algorithm, however it gre atly simpli ﬁes the presenta-
tion). This allows us to decompose θ into θ1 , . . . , θn such that
n
Xy=1
Applying the representer theorem allows us to expand θ in terms of φ(xi , yi ) as θ =
Pm
i=1 Pn
y=1 αiy φ(xi , y). In conjunction with (4) we have
m
Xi=1
Let µ ∈ Rm×n with µij = 1 if yi = j and µij = 0 otherwise, and K ∈ Rm×m with
Kij = hφ(xi ), φ(xj )i. Here joint log-likelihood (2) in terms of α and K yields
n
m
1
Xy=1
Xi=1
2σ2 tr α⊤K α + const.
Equivalently we could expand (2) in terms of t := K α. This is commonly done in Gaussian
process literature and we will use both formulations, depending on the problem we need to
solve: if K α can be computed effectively, as is the case with string kernels [9], we use the
α-parameterization. Conversely, if K −1α is cheap, as for example with graph kernels [7],
we use the t-parameterization.

αiy φ(xi ) where α ∈ Rm×n .

exp ([K α]iy ) − tr µ⊤K α +

θy =

log

(5)

(6)

Derivatives Second order methods such as Conjugate Gradient require the computation
of derivatives of − log p(θ , Y |X ) with respect to θ in terms of α or t. Using the shorthand
π ∈ Rm×n with πij := p(y = j |xi , θ) we have
∂αP = K (π − µ + σ−2α) and ∂tP = π − µ + σ−2K −1 t.
(7)
To avoid spelling out tensors of fourth order for the second derivatives (since α ∈ Rm×n )
we state the action of the latter as bilinear forms on vectors β , γ , u, v ∈ Rm×n . For con-
venience we use the “Matlab” notation of ’
.∗’ to denote element-wise multiplication of
matrices:

αP[β , γ ] = tr(K γ )⊤ (π . ∗ (K β )) − tr(π . ∗ K γ )⊤ (π . ∗ (K β )) + σ−2 tr γ⊤K β (8a)
∂ 2
∂ 2
t P[u, v ] = tr u⊤ (π . ∗ v) − tr(π . ∗ u)⊤ (π . ∗ v) + σ−2 tr u⊤K −1 v .
(8b)
Let L · n be the computational time required to compute K α and K −1 t respectively. One
may check that L = O(m) implies that each conjugate gradient (CG) descent step can
be performed in O(m) time. Combining this with rates of convergence for Newton-type
or nonlinear CG solver strategies yields overall time costs in the order of O(m log m) to
O(m2 ) worst case, a signi ﬁcant improvement over conventional O(m3 ) methods.

3 Transductive Inference by Variational Methods

As we are interested in transduction, the labels Y (and analogously the data X ) decompose
as Y = Ytrain ∪ Ytest . To directly estimate p(Ytest |X, Ytrain ) we would need to integrat-
ing out θ , which is usually intractable. Instead, we now aim at estimating the mode of
p(θ |X, Ytrain ) by variational means. With the KL-divergence D and an arbitrary distribu-
tion q the well-known bound (see e.g. [5])

− log p(θ |X, Ytrain ) ≤ − log p(θ |X, Ytrain ) + D(q(Ytest )kp(Ytest |X, Ytrain , θ))
= − XYtest
holds. This bound (10) can be minimized with respect to θ and q in an iterative fashion. The
key trick is that while using a factorizing approximation for q we restrict the latter to dis-
tributions which satisfy balancing constraints. That is, we require them to yield marginals
on the unlabeled data which are comparable with the labeled observations.

(log p(Ytest , θ |X, Ytrain ) − log q(Ytest )) q(Ytest )

(10)

(9)

Decomposing the Variational Bound To simplify (10) observe that

(11)

p(Ytest , θ |X, Ytrain ) = p(Ytrain , Ytest , θ |X )/p(Ytrain |X ).
In other words, the ﬁrst term in (10) equals (6) up to a constan t independent of θ or Ytest .
With qij := q(yi = j ) we deﬁne µij (q) = qij for all i > mtrain and µij (q) = 1 if yi = 1
and 0 otherwise for all i ≤ mtrain . In other words, we are taking the expectation in µ over
all unobserved labels Ytest with respect to the distribution q(Ytest ). We have
XYtest
n
m
Xj=1
Xi=1
For ﬁxed q the optimization over θ proceeds as in Section 2. Next we discuss q .

1
2σ2 tr α⊤K α + const.

exp ([K α]ij ) − tr µ(q)⊤K α +

q(Ytest ) log p(Ytest , θ |X, Ytrain )

(12)

=

log

q(Ytest ) log q(Ytest ) =

Optimization over q The second term in (10) is the negative entropy of q . Since q fac-
torizes we have
m
Xi=mtrain+1
XYtest
It is unreasonable to assume that q may be chosen freely from all factorizing distributions
(the latter would lead to a straightforward EM algorithm for transductive inference): if we
observe a certain distribution of labels on the training set, e.g., for binary classi ﬁcation we
see 45% positive and 55% negative labels, then it is very unlikely that the label distribution
on the test set deviates signi ﬁcantly. Hence we should make u se of this information.

qij log qij .

(13)

r−
j ≤

qij ≤ r+
j for all j ∈ Y and

qij = 1 for all i ∈ {mtrain ..m} .

If m ≫ mtrain , however, a naive application of the variational bound can lead to cases
where q is concentrated on one class — the increase in likelihood for
a resulting very sim-
ple classi ﬁer completely outweighs any balancing constrai nts implicit in the data. This is
conﬁrmed by experimental results. It is, incidentally, als o the reason why SVM transduc-
tion optimization codes [4] impose a balancing constraint on the assignment of test labels.
We impose the following conditions:
n
m
Xj=1
Xi=mtrain+1
j = pemp (y = j ) − ǫ and r+
Here the constraints r−
j = pemp (y = j ) + ǫ are chosen such
as to correspond to conﬁdence intervals given by ﬁnite sampl
e size tail bounds. In other
train Pmtrain
words we set pemp (y = j ) = m−1
i=1 {yi = j } and ǫ such as to satisfy
Pr ((cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
i (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
> ǫ) ≤ δ
mtrain
mtest
Xi=1
Xi=1
m−1
ξi − m−1
ξ ′
test
train
for iid {0, 1} random variables ξi and ξ ′
i with mean p. This is a standard ghost-sample
inequality. It follows directly from [3, Eq. (2.7)] after application of a union bound over
the class labels that ǫ ≤ plog(2n/δ)m/ (2mtrainmtest ).
4 Graphs, Strings and Vectors

(14)

We now discuss the two main applications where computational savings can be achieved:
graphs and strings. In the case of graphs, the advantage arises from the fact that K −1 is
sparse, whereas for texts we can use fast string kernels [9] to compute K α in linear time.
Graphs Denote by G(V , E ) the graph given by vertices V and edges E where each edge is
a set of two vertices. Then W ∈ R|V |×|V | denotes the adjacency matrix of the graph, where
Wij > 0 only if edge {i, j } ∈ E . We assume that the graph G, and thus also the adjacency
matrix W , is sparse. Now denote by 1 the identity matrix and by D the diagonal matrix of
vertex degrees, i.e., Dii = Pj Wij . Then the graph Laplacian and the normalized graph
Laplacian of G are given by
˜L := 1 − D− 1
2 W D− 1
L := D − W and
(15)
2 ,
respectively. Many kernels K (or their inverse) on G are given by low-degree polynomials
of the Laplacian or the adjacency matrix of G, such as the following:

K =

ciW 2i , K =

(1 − ci ˜L), or K −1 = ˜L + ǫ1.

l
l
Yi=1
Xi=1
In all three cases we assumed ci , ǫ ≥ 0 and l ∈ N. The ﬁrst kernel arises from an l-step
random walk, the third case is typically referred to as regularized graph Laplacian. In these
cases K α or K −1 t can be computed using L = l(|V | + |E |) operations. This means
that if the average degree of the graph does not increase with the number of observations,
L = O(m) as m = |V | for inference on graphs.

(16)

From Graphs to Graphical Models Graphs are one of the examples where transduction
actually improves computational cost: Assume that we are given the inverse kernel matrix
K −1 on training and test set and we wish to perform induction only. In this case we need
to compute the kernel matrix (or its inverse) restricted to the training set. Let K −1 =
(cid:20) A B
B⊤ C (cid:21), then the upper left hand corner (representing the training set part only) of

K is given by the Schur complement (cid:0)A − B⊤C −1B (cid:1)−1 . Computing the latter is costly.
Moreover, neither the Schur complement nor its inverse are typically sparse.
Here we have a nice connection between graphical models and graph kernels. Assume that
t is a normal random variable with conditional independence properties. In this case the
inverse covariance matrix has nonzero entries only for variables with a direct dependency
structure. This follows directly from an application of the Clifford-Hammersley theorem to
Gaussian random variables [6]. In other words, if we are given a graphical model of normal
random variables, their conditional independence structure is reﬂected by K −1 .
In the same way as in graphical models marginalization may induce dependencies, com-
puting the kernel matrix on the training set only, may lead to dense matrices, even when
the inverse kernel on training and test data combined is sparse. The bottom line is there are
cases where it is computationally cheaper to take both training and test set into account and
optimize over a larger set of variables rather than dealing with a smaller dense matrix.
Strings: Efﬁcient computation of string kernels using sufﬁx trees wa
s described in [9]. In
particular, it was observed that expansions of the form Pm
i=1 αik(xi , x) can be evaluated
in linear time in the length of x, provided some preprocessing for the coefﬁcients α and
observations xi is performed. This preprocessing is independent of x and can be computed
in O(Pi |xi |) time. The efﬁcient computation scheme covers all kernels of
type
k(x, x′ ) = Xs
ws#s (x)#s (x′ )
for arbitrary ws ≥ 0. Here, #s (x) denotes the number of occurrences of s in x and the
sum is carried out over all substrings of x. This means that computation time for evaluating
K α is again O(Pi |xi |) as we need to evaluate the kernel expansion for all x ∈ X . Since
the average string length is independent of m this yields an O(m) algorithm for K α.
Vectors: If k(x, x′ ) = φ(x)⊤φ(x′ ) and φ(x) ∈ Rd for d ≪ m, it is possible to carry
out matrix vector multiplications in O(md) time. This is useful for cases where we have a
sparse matrix with a small number of low-rank updates (e.g. from low rank dense ﬁll-ins).

(17)

5 Optimization

Optimization in α and t: P is convex in α (and in t since t = K α). This means that a com-
bination of Conjugate-Gradient and Newton-Raphson (NR) can be used for optimization.

−1∂αP via
• Compute updates α ←− α − η∂ 2
αP
– Solve the linear system approximately by Conjugate Gradient iterations.
– Find optimal η by line search.
• Repeat until the norm of the gradient is sufﬁciently small.
Key is the fact that the arising linear system is only solved approximately, which can be
done using very few CG iterations. Since each of them is O(m) for fast kernel-vector
computations the overall cost is a sub-quadratic function of m.
Optimization in q is somewhat less straightforward: we need to ﬁnd the optimal q in terms
of KL-divergence subject to the marginal constraint. Denote by τ the part of K α pertaining
to test data, or more formally τ ∈ Rmtest×n with τij = [K α]i+mtrain ,j . We have:
tr q⊤ τ + Xi,j
j ≤ Xi
j , qij ≥ 0 and Xi
subject to q−
qij ≤ q+

qli = 1 for all j ∈ Y, l ∈ {1..mtest }

(18)

minimize
q

qij log qij

Table 1: Error rates on some benchmark datasets (mostly from UCI). The last column is
the error rates reported in [1]
#IN S T #AT TR
DATA S E T
cancer
9
699
cancer (progn.)
569
30
heart (cleave.)
13
297
housing
13
506
ionosphere
34
351
pima
769
8
sonar
60
208
glass
10
214
wine
13
178
tictactoe
9
958
cmc
1473
10
USPS
256
9298

IND . GP
TRAN SD . GP
3.4%±4.1%
2.1%±4.7%
6.1%±3.7%
6.0%±3.7%
15.0%±5.6% 13.0%±6.3%
6.8%±0.9%
7.0%±1.0%
8.6%±6.3%
6.1%±3.4%
19.6%±8.1% 17.6%±8.0%
10.5%±5.1%
8.6%±3.4%
20.5%±1.6% 17.3%±4.5%
19.4%±5.7% 15.6%±4.2%
3.9%±0.7%
3.3%±0.6%
32.5%±7.1% 28.9%±7.5%
4.8%
5.9%

S3 VMM I P
3.4%
3.3%
16.0%
15.1%
10.6%
22.2%
21.9%

1

This is a convex optimization problem. Using Lagrange multipliers one can show that q
needs to satisfy qij = exp(−τij )bi cj where bi , cj ≥ 0. Solving for Pn
j qij = 1 yields
qij = exp(−τij )cj
. This means that instead of an optimization problem in mtest × n
Pn
l=1 exp(−τil )cl
variables we now only need to optimize over n variables subject to 2n constraints.
i = q−
Note that the exact matching constraint where q+
i amounts to a maximum likelihood
problem for a shifted exponential family model where qij = exp(τij ) exp(γi − gj (γi )).
It can be shown that the approximate matching problem is equivalent to a maximum a
posteriori optimization problem using the norm dual to expectation constraints on qij . We
are currently working on extending this setting

In summary, the optimization now only depends on n variables. It can be solved by standard
second order methods. As initialization we choose γi such that the per class averages match
the marginal constraint while ignoring the per sample balance. After that a small number
Newton steps sufﬁces for optimization.

6 Experiments

Unfortunately, we are not aware of other multiclass transductive learning algorithms. To
still be able to compare our approach to other transductive learning algorithms we per-
formed experiments on some benchmark datasets. To investigate the performance of our
algorithm in classifying vertices of a graph, we choose the WebKB dataset.
Benchmark datasets Table 1 reports results on some benchmark datasets. To be able to
compare the error rates of the transductive multiclass Gaussian Process classi ﬁer proposed
in this paper, we also report error rates from [2] and an inductive multiclass Gaussian
Process classi ﬁer. The reported error rates are for 10-fold crossvalidations. Parameters
were chosen by crossvalidation inside the training folds.
Graph Mining To illustrate the effectiveness of our approach on graphs we performed
experiments on the well known WebKB dataset. This dataset consists of 8275 webpages
classi ﬁed into 7 classes. Each webpage contains textual content and/or links to other web-
pages. As we are using this dataset to evaluate our graph mining algorithm, we ignore the
text on each webpage and consider the dataset as a labelled directed graph. To have the data

1 In [2] only subsets of USPS were considered due to the size of this problem.

—
—
—
—
—
Table 2: Results on WebKB for ‘inverse’ 10-fold crossvalidation
ERROR
DATA S E T
DATA S E T
|E |
|V |
|E |
|V |
Misc
Cornell
4462
4113
10%
1793
867
Texas
all
8%
1683
827
14370
8275
10% Universities
Washington
4162
9591
1205
2368
Wisconsin
15%
3678
1263

ERROR
66%
53%
12%

set as large as possible, we did not remove any webpages, opposed to most other work.

Table 2 reports the results of our algorithm on different subsets of the WebKB data as
well as on the full data. We use the co-linkage graph and report results for ‘inverse’ 10-
fold strati ﬁed crossvalidations, i.e., we use 1 fold as training data and 9 folds as test data.
Parameters are the same for all reported experiments and were found by experimenting with
a few parametersets on the ‘Cornell’ subset only. It turned out that the class membership
probabilities are not well-calibrated on this dataset. To overcome this, we predict on the
test set as follows: For each class the instances that are most likely to be in this class are
picked (if they haven’t been picked for a class with lower index) such that the fraction of
instances assigned to this class is the same on the training and test set. We will investigate
the reason for this in future work.

The setting most similar to ours is probably the one described in [11]. Although a di-
rected graph approach outperforms there an undirected approach, we resorted to kernels
for undirected graphs, as those are computationally more attractive. We will investigate
computationally attractive digraph kernels in future work and expect similar beneﬁts as re-
ported by [11]. Though we are using more training data than [11] we are also considering
a more difﬁcult learning problem (multiclass without remov ing various instances). To in-
vestigate the behaviour of our algorithm with less training data, we performed a 20-fold
inverse crossvalidation on the ‘wisconsin’ subset and observed an error rate of 17% there.
To further strengthen our results and show that the runtime performance of our algorithm
is sufﬁcient for classifying the vertices of massive graphs , we also performed initial ex-
periments on the Epinions dataset collected by Mathew Richardson and Pedro Domingos.
The dataset is a social network consisting of 75, 888 people connected by 508, 960 ‘trust’
edges. Additionally the dataset comes with a list of 185 ‘topreviewers’ for 25 topic areas.
We tried to predict these but only got 12% of the topreviewers correct. As we are not aware
of any predictive results on this task, we suppose this low accuracy is inherent to this task.
However, the experiments show that the algorithm can be run on very large graph datasets.

7 Discussion and Extensions

We presented an efﬁcient method for performing transductio n on multiclass estimation
problems with Gaussian Processes. It performs particularly well whenever the kernel ma-
trix has special numerical properties which allow fast matrix vector multiplication. That
said, also on standard dense problems we observed very good improvements (typically a
10% reduction of the training error) over standard induction.
Structured Labels and Conditional Random Fields are a clear area where to extend
the transductive setting. The key obstacle to overcome in this context is to ﬁnd a suitable
marginal distribution: with increasing structure of the labels the conﬁdence bounds per
subclass decrease dramatically. A promising strategy is to use only partial marginals on
maximal cliques and enforce them directly similarly to an unconditional Markov network.

Applications to Document Analysis require efﬁcient small-memory-footprint sufﬁx tree
implementations. We are currently working on this, which will allow GP classi ﬁcation to
perform estimation on large document collections. We believe it will be possible to use
out-of-core storage in conjunction with annotation to work on sequences of 108 characters.
Other Marginal Constraints than matching marginals are worth exploring. In particular,
constraints derived from exchangeable distributions such as those used by Latent Dirichlet
Allocation are a promising area to consider. This may also lead to connections between GP
classi ﬁcation and clustering.
Sparse O(m1.3 ) Solvers for Graphs have recently been proposed by the theoretical com-
puter science community. It is worthwhile exploring their use for inference on graphs.
Acknowledgements The authors thank Mathew Richardson and Pedro Domingos for col-
lecting the Epinions data and Deepayan Chakrabarti and Christos Faloutsos for providing
a preprocessed version. Parts of this work were carried out when TG was visiting NICTA.
National ICT Australia is funded through the Australian Government’s Backing Australia’s
Ability initiative, in part through the Australian Research Council. This work was supported
by grants of the ARC and by the Pascal Network of Excellence.

References
[1] K. Bennett. Combining support vector and mathematical programming methods for
classi ﬁcation. In Advances in Kernel Methods - -Support Vector Learning, pages 307
– 326. MIT Press, 1998.
[2] K. Bennett. Combining support vector and mathematical programming methods for
induction. In B. Sch ¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in
Kernel Methods - -SV Learning, pages 307 – 326, Cambridge, MA, 1999. MIT Press.
[3] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal
of the American Statistical Association, 58:13 – 30, 1963.
[4] T. Joachims. Learning to Classify Text Using Support Vector Machines: Methods,
Theory, and Algorithms. The Kluwer International Series In Engineering And Com-
puter Science. Kluwer Academic Publishers, Boston, May 2002. ISBN 0 - 7923 -
7679-X.
[5] M. I. Jordan, Z. Ghahramani, Tommi S. Jaakkola, and L. K. Saul. An introduction to
variational methods for graphical models. Machine Learning, 37(2):183 – 233, 1999.
[6] S. L. Lauritzen. Graphical Models. Oxford University Press, 1996.
[7] A. J. Smola and I. R. Kondor. Kernels and regularization on graphs. In B. Sch ¨olkopf
and M. K. Warmuth, editors, Proceedings of the Annual Conference on Computa-
tional Learning Theory, Lecture Notes in Computer Science. Springer, 2003.
[8] V. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.
[9] S. V. N. Vishwanathan and A. J. Smola. Fast kernels for string and tree matching.
In K. Tsuda, B. Sch ¨olkopf, and J.P. Vert, editors, Kernels and Bioinformatics, Cam-
bridge, MA, 2004. MIT Press.
[10] C. K. I. Williams and D. Barber. Bayesian classi ﬁcation with Gaussian processes.
IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI, 20(12):1342
– 1351, 1998.
[11] D. Zhou, J. Huang, and B. Sch ¨olkopf. Learning from labeled and unlabeled data on a
directed graph. In International Conference on Machine Learning, 2005.
[12] X. Zhu, J. Lafferty, and Z. Ghahramani. Semi-supervised learning using gaussian
ﬁelds and harmonic functions.
In International Conference on Machine Learning
ICML’03, 2003.


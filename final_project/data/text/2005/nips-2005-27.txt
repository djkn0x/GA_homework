Learning from Data of Variable Quality

Koby Crammer, Michael Kearns, Jennifer Wortman
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19103
{crammer,mkearns,wortmanj}@cis.upenn.edu

Abstract

We initiate the study of learning from multiple sources of limited data,
each of which may be corrupted at a different rate. We develop a com-
plete theory of which data sources should be used for two fundamental
problems: estimating the bias of a coin, and learning a classi ﬁer in the
presence of label noise. In both cases, efﬁcient algorithms are provided
for computing the optimal subset of data.

1

Introduction

In many natural machine learning settings, one is not only faced with data that may be cor-
rupted or deﬁcient in some way (classi ﬁcation noise or other
label errors, missing attributes,
and so on), but with data that is not uniformly corrupted. In other words, we might be pre-
sented with data of variable quality — perhaps some small amount of entirely “clean” data,
another amount of slightly corrupted data, yet more that is signi ﬁcantly corrupted, and so
on. Furthermore, in such circumstances we may often know at least an upper bound on the
rate and type of corruption in each pile of data. An extreme example is the recent interest in
settings where one has a very limited set of correctly labeled examples, and an effectively
unlimited set of entirely unlabeled examples, as naturally arises in problems such as clas-
sifying web pages [1]. Another general category of problems that falls within our interest
is when multiple piles of data are drawn from processes that differ perhaps slightly and in
varying amounts from the process we wish to estimate. For example, we might wish to
estimate a conditional distribution P (X |Y = y) but have only a small number of observa-
tions in which Y = y , but a larger number of observations in which Y = y ′ for values of
y ′ “near ” to
y . In such circumstances it might make sense to base our model on a larger
number of observations, at least for those y ′ closest to y .
While there is a large body of learning theory both for uncorrupted data and for data that
is uniformly corrupted in some way [2, 3], there is no general framework and theory for
learning from data of variable quality. In this paper we introduce such a framework, and
develop its theory, for two basic problems: estimating a bias from corrupted coins, and
learning a classi ﬁer in the presence of varying amounts of la bel noise. For the corrupted
coins case we provide an upper bound on the error that is expressed as a trade-off between
weighted approximation errors and larger amounts of data. This bound provides a building
block for the classi ﬁcation noise setting, in which we are ab le to give a bound on the
generalization error of empirical risk minimization that speci ﬁes the optimal subset of the

data to use. Both bounds can computed by simple and efﬁcient a lgorithms. We illustrate
both problems and our algorithms with numerical simulations.

2 Estimating the Bias from Corrupted Coins

We begin by considering perhaps the simplest possible instance of the general class of
problems in which we are interested — namely, the problem of e
stimating the unknown
bias of a coin. In this version of the variable quality model, we will have access to different
amounts of data from “corrupted” coins whose bias differs fr om the one we wish to esti-
mate. We use our solution for this simple problem as a building block for the classi ﬁcation
noise setting in Section 3.

2.1 Problem Description

Suppose we wish to estimate the bias β of a coin given K piles of training observations
N1 , ..., NK . Each pile Ni contains ni outcomes of ﬂips of a coin with bias βi , where the
only information we are provided is that βi ∈ [β − ǫi , β + ǫi ], and 0 ≤ ǫ1 ≤ ǫ2 ≤ ... ≤ ǫK .
We refer to the ǫi as bounds on the approximation errors of the corrupted coins. We denote
by hi the number of heads observed in the ith pile. Our immediate goal is to determine
which piles should be considered in order to obtain the best estimate of the true bias β .
We consider estimates for β obtained by merging some subset of the data into a single
uni ﬁed pile, and computing the maximum likelihood estimate for β , which is simply the
fraction of times heads appears as an outcome in the uni ﬁed pi
le. Although one can con-
sider using any subset of the data, it can be proved (and is intuitively obvious) that an
optimal estimate (in the sense that will be deﬁned shortly) a lways uses a preﬁx of the data,
i.e. all data from the piles indexed 1 to k for some k ≤ K , and possibly a subset of the data
from pile k + 1. In fact, it will be shown that only complete piles need to be considered.
Therefore, from this point on we restrict ourselves to estimates of this form, and identify
them by the maximal index k of the piles used. The associated estimate is then simply
h1 + . . . + hk
n1 + . . . + nk
We denote the expectation of this estimate by
n1β1 + . . . + nk βk
¯βk = E h ˆβk i =
n1 + . . . + nk
To simplify the presentation we denote by ni,j the number of outcomes in piles Ni , . . . , Nj ,
that is, ni,j = Pj
m=i nm .
We now bound the deviation of the estimate ˆβk from the true bias of the coin β using the
expectation ¯βk :

ˆβk =

.

.

|β − ˆβk | = |β − ¯βk + ¯βk − ˆβk |
≤ |β − ¯βk | + | ¯βk − ˆβk |
k
Xi=1
The ﬁrst inequality follows from the triangle inequality an d the second from our assump-
tions. Using the Hoeffding inequality we can bound the second term and ﬁnd that with high
probability for an appropriate choice of δ we have
ǫi + s log(2K/δ)
k
Xi=1
2n1,k

ǫi + | ¯βk − ˆβk |

|β − ˆβk | ≤

ni
n1,k

(1)

≤

ni
n1,k

.

To summarize, we have proved the following theorem.

≤

ni
n1,k

Theorem 1 Let ˆβk be the estimate obtained by using only the data from the ﬁrst k piles.
Then for any δ > 0, with probability ≥ 1 − δ we have
ǫi + s log(2K/δ)
k
(cid:12)(cid:12)(cid:12)
β − ˆβk (cid:12)(cid:12)(cid:12)
Xi=1
2n1,k
simultaneously for all k = 1, . . . , K .
Two remarks are in place here. First, the theorem is data-independent since it does not take
into account the actual outcomes of the experiments h1 , . . . , hK . Second, the two terms in
the bound reﬂect the well-known trade-off between bias (app roximation error) and variance
(estimation error). The ﬁrst term bounds the approximation error of replacing the true coin
β with the average ¯βk . The second term corresponds to the estimation error which arises
as a result of our ﬁnite sample size.

ni
n1,k

This theorem implies a natural algorithm to choose the number of piles k∗ as is the mini-
mizer of the bound over the number of piles used:
ǫi + s log(2K/δ)
k∈{1,...,K } ( k
2n1,k ) .
Xi=1
k∗ = argmin
To conclude this section we argue that our choice of using a preﬁx of piles is optimal. First,
note that by adding a new pile with a corruption level ǫ smaller then the current corruption
level, we can always reduce the bounds. Thus it is optimal to use preﬁx of the piles and
not to ignore piles with low corruption levels. Second, we need to show that if we decide
to use a pile, it will be optimal to use all of it. Note that we can choose to view each coin
toss as a separate pile with a single observation, thus yielding n1,K piles of size 1. The
following technical lemma states that under this view of singleton piles, once we decide to
add a pile with some corruption level, it will be optimal to use all singleton piles with the
same corruption level. The proof of this lemma is omitted due to lack of space.

ni
n1,k+p

ni
n1,k

k
Xi=1
ni
n1,k+p+1

Lemma 1 Assume that all the piles are of size ni = 1 and that ǫk ≤ ǫp+k = ǫp+k+1 . Then
the following two inequalities cannot hold simultaneously:
ǫi + s log(2n1,K /δ)
2n1,k
ǫi + s log(2n1,K /δ)
2n1,k+p+1

k+p
Xi=1
k+p
k+p+1
Xi=1
Xi=1
In other words, if the bound on |β − ˆβk+p | is smaller than the bound on |β − ˆβk |, then
the bound on |β − ˆβk+p+1 | must be smaller than both unless ǫk+p+1 > ǫk+p . Thus if the
pth and p+1th samples are from the same original pile (and ǫk+p+1 = ǫk+p ), then once we
decide to use samples through p, we will always want to include sample p + 1. It follows
that we must only consider using complete piles of data.

ǫi + s log(2n1,K /δ)
2n1,k+p
ǫi + s log(2n1,K /δ)
2n1,k+p

>

≥

ni
n1,k+p

.

2.2 Corrupted Coins Simulations

The theory developed so far can be nicely illustrated via some simple simulations. We
brieﬂy describe just one such experiment in which there were K = 8 piles. The tar-
get coin was fair: β = 0.5. The approximation errors of the corrupted coins were

Actual Error
Bound
Singeltons Bound

e4 + B(4, k)

e6

e5

e4

e3

i∗
k

= 2

e2

e1

r
o
r
r
E

1

0.8

0.6

0.4

0.2

0

Error Bound
Actual Error
Achieved Error

2

4

6
8
Number of Piles Used

10

12

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
o
r
r
E

101

102
103
Number of Examples Used

104

Figure 1: Left: Illustration of the actual error and our error bounds for estimating the bias
of a coin. The error bars show one standard deviation. Center: Illustration of the interval
construction. Right: Illustration of actual error of a 20 dimensional classi ﬁcat
ion problem
and the error bounds found using our methods.

~ǫ = (0.001, 0.01, 0.02, 0.03, 0.04, 0.2, 0.3, 0.5), and number of outcomes in the corre-
sponding piles were ~n = (10, 50, 100, 500, 1500, 2000, 3000, 10000). The following pro-
cess was repeated 1, 000 times. We set the probability of the ith coin to be βi = β + ǫi
and sampled ni times from it. We then used all possible preﬁxes 1, . . . , k of piles to es-
timate β . For each k , we computed the bound for the estimate using piles 1, . . . , k using
the theory developed in the previous section. To illustrate Lemma 1 we also computed the
bound using partial piles. This bound is slightly higher than the suggested bound since we
use effectively more piles (n1,K instead of K ). As the lemma predicts, it is not valuable to
use subsets of piles. Simulations with other values of K , ~ǫ and ~n yield similar qualitative
behavior. We note that a strength of the theory developed is its generality, as it provides
bounds for any model parameters.

The leftmost panel of Figure 1 summarizes the simulation results. Empirically, the best
estimate of the target coin is using the ﬁrst four piles, whil e our algorithm suggests using
the ﬁrst ﬁve piles. However, the empirical difference in qua
lity between the two estimates
is negligible, so the theory has given near-optimal guidance in this case. We note that while
our bounds have essentially the right shape (which is what matters for the computation
of k∗ ), numerically they are quite loose compared to the true behavior. There are various
limits to the numerical precision we should expect without increasing the complexity of the
theory — for example, the precision is limited by accuracy of
constants in the Hoeffding
inequality and the use of the union bound.

3 Classiﬁcation with Label Noise

We next explore the problem of classi ﬁcation in the presence of multiple data sets with
varying amounts of label noise. The setting is as follows. We assume there is a ﬁxed and
unknown binary function f : X → {0, 1} and a ﬁxed and unknown distribution P on the
inputs X to f . We are presented again with K piles of data, N1 , ..., NK . Now each pile
Ni contains ni labeled examples (x, y) that are generated from the target function f with
label noise at rate ηi , where 0 ≤ η1 < η2 < ... < ηK . In other words, for each example
(x, y) in pile Ni , y = f (x) with probability 1 − ηi and y = ¬f (x) with probability ηi .
The goal is to decide which piles of data to use in order to choose a function h from a set
of hypothesis functions H with minimal generalization (true) error e(h) with respect to f
and P . As before, for any preﬁx of piles N1 , . . . , Nk , we examine the most basic estimator
based on this data, namely the hypothesis minimizing the observed or training error:

ˆhk = argmin
h∈H

{ˆek (h)}

where ˆek (h) is the fraction of times h(x) 6= y over all (x, y) ∈ N1 ∪ · · · ∪ Nk . Thus we
examine the standard empirical risk minimization framework [2]. Generalizing from the
biased coin setting, we are interested in three primary questions: what can we say about
the deviation |e(ˆhk ) − ˆe(ˆhk )|, which is the gap between the true and observed error of the
estimator ˆhk ; what is the optimal value of k ; and how can we compute the corresponding
bounds?

We note that the classi ﬁcation noise setting can naturally b e viewed as a special case of
a more general and challenging “agnostic” classi ﬁcation se
tting that we discuss brieﬂy in
Section 4. Here we provide a more specialized solution that exploits particular properties
of class label noise.
We begin by observing that for any ﬁxed function h, the question of how ˆek (h) is related
to e(h) bears great similarity to the biased coin setting. More precisely, the expected clas-
si ﬁcation error of h on pile Ni only is

(1 − ηi )e(h) + ηi (1 − e(h)) = e(h) + ηi (1 − 2e(h)) .

Thus if we set

(2)
ǫi = ηi |1 − 2e(h)|
β = e(h),
and if we were only concerned with making the best use of the data in estimating e(h), we
could attempt to apply the theory developed in Section 2 using the reduction above. There
are two distinct and obvious difﬁculties. The ﬁrst difﬁcult
y is that even restricting attention
to estimating e(h) for a ﬁxed h, the values for ǫi above (and thus the bounds computed
by the methods of Section 2) depend on e(h), which is exactly the unknown quantity we
would like to estimate. The second difﬁculty is that in order
to bound the performance of
empirical error minimization within H, we must say something about the probability of
any h ∈ H being selected. We address each of these difﬁculties in turn .

3.1 Computing the Error Bound Matrix

For now we assume that {e(h) : h ∈ H} is a ﬁnite set containing M values e1 < . . . < eM .
This assumption clearly holds if |H| is ﬁnite, and can be removed entirely by discretizing
the values in {e(h) : h ∈ H}. For convenience we assume that for all levels ei there exists
a function h ∈ H such that e(h) = ei . This assumption can also be removed (details of
both omitted due to space considerations). We deﬁne a matrix B of estimation errors as
follows. Each row i of B represents one possible value of e(h) = ei , while each column
k represents the use of only piles N1 , . . . , Nk of noisy labeled examples of the target f .
The entry B(i, k) will contain a bound on |e(h) − ˆek (h)| that is valid simultaneously for all
h ∈ H with e(h) = ei . In other words, for any such h, with high probability ˆek (h) falls in
the range [ei − B(i, k), ei + B(i, k)]. It is crucial to note that we do not need to know which
functions h ∈ H satisfy e(h) = ei in order to either compute or use the bound B(i, k), as
we shall see shortly. Rather, it is enough to know that for each h ∈ H, some row of B will
provide estimation error bounds for each k .
The values in B can be now be calculated using the settings provided by Eq. (2) and the
bound in Eq. (1). However, since Eq. (1) applies to the case of a single biased coin and here
we have many (essentially one for each function at a given generalization error ei ), we must
modify it slightly. We can (pessimistically) bound the VC dimension of all functions with
error rate e(h) = ei by the VC dimension d of the entire class H. Formally, we replace the
square root term in Eq. (1) with the following expression, which is a simple application of
VC theory [2, 3]:

O  s 1
δ (cid:19)(cid:19)! .
d (cid:17) + log (cid:18) KM
n1,k (cid:18)d log (cid:16) n1,k

(3)

We note that in cases where we have more information on the structure of the generalization
errors in H, an accordingly modi ﬁed equation can be used, which may yiel d considerably
improved bounds. For example, in the statistical physics theory of learning curves[4] it is
common to posit knowledge of the density or number of functions in H at a given gener-
alization error ei . In such a case we could clearly substitute the VC dimension d by the
(potentially much smaller) VC dimension di of just this subclass.
In a moment we describe how the matrix B can be used to choose the number k of piles
to use, and to compute a bound on the generalization error of ˆhk . We ﬁrst formalize the
development above as an intermediate result.

Lemma 2 Suppose H is a set of binary functions with VC dimension d. Let M be the
number of noise levels and K be the number of piles. Then for all δ > 0, with probability
at least 1 − δ , for all i ∈ {1, . . . , M }, for all h ∈ H with e(h) = ei , and for all k ∈
{1, . . . , K } we have

The matrix B can be computed in time linear in its size O(KM ).

|e(h) − ˆek (h)| ≤ B(i, k) .

3.2 Putting It All Together

By Lemma 2, the matrix B gives, for each possible generalization error ei and each k , an
upper bound on the deviation between observed and true errors for functions of true error
ei when using piles N1 , . . . , Nk . It is thus natural to try to use column k of B to bound the
error of ˆhk , the function minimizing the observed error on these piles.
Suppose we ﬁx the number of piles used to be k . The observed error of any function
with true generalization error ei must, with high probability, lie in the interval Ii,k =
[ei − B(i, k), ei + B(i, k)]. By simultaneously considering these intervals for all values of
ei , we can put a bound on the generalization error of the best function in the hypothesis
class. This process is best illustrated by an example.

Consider a hypothesis space in which the generalization error of the available functions can
take on the discrete values 0, 0.1, 0.2, 0.3, 0.4, and 0.5. Suppose the matrix B has been
calculated as above and the k th column is (0.16, 0.05, 0.08, 0.14, 0.07, 0.1). We know, for
example, that all functions with true generalization error e2 = 0.1 will show an error in
the range I2,k = [0.05, 0.15], and that all functions with true generalization error e4 = 0.3
will show an error in the range I4,k = [0.16, 0.44]. The center panel of Figure 1 illustrates
the span of each interval.
Examining this diagram, it becomes clear that the function ˆhk minimizing the error on
N1 ∪ · · · ∪ Nk could not possibly be a function with true error e4 or higher as long as
H contains at least one function with true error e2 since the observed error of the latter
would necessarily be lower (with high probability). Likewise, it would not be possible
for a function with true error e5 or e6 to be chosen. However, a function with true error e3
could produce a lower observed error than one with true error e1 or e2 (since e3 − B(3, k) <
e2 + B(2, k) and e3 − B(3, k) < e1 + B(1, k)), and thus could be chosen as ˆhk . Therefore,
the smallest bound we can place on the true error of ˆhk in this example is e3 = 0.2.
In general, we know that ˆhk will have true error corresponding to the midpoint of an a inter-
val which overlaps with the interval with the least upper bound (I2,k in this example). This
leads to an intuitive procedure for calculating a bound on the true error of ˆhk . First, we de-
k = argmini {ei + B(i, k)}. Consider
termine the interval with the smallest upper bound, i∗
the set of intervals which overlap with i∗
k , namely Jk = {i : ei −B(i, k) ≤ ei∗
k , k)}.
k +B(i∗
It is possible for the smallest observed error to come from a function corresponding to any

of the intervals in Jk . Thus, a bound on the true error of ˆhk can be obtained by taking the
def
maximum e(h) value for any function in Jk , i.e. C (k)
= maxi∈Jk {ei }.
Our overall algorithm for bounding e(ˆhk ) and choosing k∗ can thus be summarized:

1. Compute the matrix B as described in Section 3.1 .
2. Compute the vector C described above.
3. Output k∗ = argmink {C (k)}.

We have established the following theorem.

Theorem 2 Suppose H is a set of binary functions with VC dimension d. Let M be the
number of noise levels and K be the number of piles. For all k = 1, ..., K , let ˆhk =
argminh {ˆek (h)} be the function in H with the lowest empirical error evaluated using the
ﬁrst k piles of data. Then for all δ > 0, with probability at least 1 − δ ,

e(ˆhk ) ≤ C (k)

The suggested choice of k is thus k∗ = argmink {C (k)}.

3.3 Classi ﬁcation Noise Simulations

In order to illustrate the methodology described in this section, simulations were run on a
classi ﬁcation problem in which samples ~x ∈ {0, 1}20 were chosen uniformly at random,
and the target function f (~x) was 1 if and only if P20
i=1 xi > 10.
Classi ﬁcation models were created for k = 1, ..., K by training using the ﬁrst k piles of data
using logistic regression with a learning rate of 0.0005 for a maximum of 5, 000 iterations.
The generalization error for each model was determined by testing on a noise-free sample
of 500 examples drawn from the same uniform distribution. Bounds were calculated using
the algorithm described above with functions binned into 101 evenly spaced error values
~e = (0, 0.01, 0.02, ..., 1) with δ = 0.001.
The right panel of Figure 1 shows an example of the bounds found with K = 12 piles,
noise levels ~η = (0.001, 0.002, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5), and
sample sizes ~n = (20, 150, 300, 400, 500, 600, 700, 1000, 1500, 2000, 3000, 5000). The
algorithm described above correctly predicts that the eighth pile should be chosen as the
cutoff, yielding an optimal error value of 0.018. It is interesting to note that although the
error bounds shown are signi ﬁcantly higher than the actual e rror, the shapes of the curves
are similar. This phenomena is common to many uniform convergence bounds.

Further experimentation has shown that the algorithm described here works well in general
when there are small piles of low noise data and large piles of high noise data. Its predic-
tions are more useful in higher dimensional space, since it is relatively easy to get good
predictions without much available data in lower dimensions.

4 Further Research

In research subsequent to the results presented here [5], we examine a considerably more
general “agnostic” classi ﬁcation setting [6]. As before, w e assume there is a ﬁxed and
unknown binary function f : X → {0, 1} and a ﬁxed and unknown distribution P on the
inputs X to f . We are presented again with K piles of data, N1 , ..., NK . Now each pile Ni
contains ni labeled examples (x, y) that are generated from an unknown function hi such
that e(hi ) = e(hi , f ) = PrP [hi (x) 6= f (x)] ≤ ǫi for given values ǫ1 ≤ . . . ≤ ǫK . Thus

we are provided piles of labeled examples of unknown functions “nearby” the unknown
target f , where “nearby” is quanti ﬁed by the sequence of
ǫi .
In forthcoming work [5] we show that with high probability, for any k ≤ K
n1,k (cid:19) ǫi+O  s 1
δ (cid:19)(cid:19)!
k
d (cid:17) + log (cid:18) K
n1,k (cid:18)d log (cid:16) n1,k
Xi=1 (cid:18) ni
This result again allows us to express the optimal number of piles as a trade-off between
weighted approximation errors and increasing sample size. We suspect the result can be
extended to a wider class of loss functions that just classi ﬁ cation.

e(ˆhk , f ) ≤ min
h∈H

{e(f , h)}+2

References
[1] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings
of the Eleventh Annual Conference on Computational Learning Theory, pages 92–100, 1998.
[2] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.
[3] M. J. Kearns and U. V. Vazirani. An Introduction to Computational Learning Theory. MIT Press,
1994.
[4] D. Haussler, M. Kearns, H.S. Seung, and N. Tishby. Rigorous learning curve bounds from
statistical mechanics. In Proceedings of the Seventh Annual ACM Conference on Computational
Learning Theory, pages 76–87, 1994.
[5] K. Crammer, M. Kearns, and J. Wortman. Forthcoming. 2006.
[6] M. Kearns, R. Schapire, and L. Sellie. Towards efﬁcient agnostic learning. Machine Learning,
17:115–141, 1994.


Dual-Tree Fast Gauss Transforms

Dongryeol Lee
Computer Science
Carnegie Mellon Univ.
dongryel@cmu.edu

Alexander Gray
Computer Science
Carnegie Mellon Univ.
agray@cs.cmu.edu

Andrew Moore
Computer Science
Carnegie Mellon Univ.
awm@cs.cmu.edu

Abstract

In previous work we presented an efﬁcient approach to comput ing ker-
nel summations which arise in many machine learning methods such as
kernel density estimation. This approach, dual-tree recursion with ﬁnite-
difference approximation, generalized existing methods for similar prob-
lems arising in computational physics in two ways appropriate for sta-
tistical problems: toward distribution sensitivity and general dimension,
partly by avoiding series expansions. While this proved to be the fastest
practical method for multivariate kernel density estimation at the optimal
bandwidth, it is much less efﬁcient at larger-than-optimal bandwidths.
In this work, we explore the extent to which the dual-tree approach can
be integrated with multipole-like Hermite expansions in order to achieve
reasonable efﬁciency across all bandwidth scales, though o nly for low di-
mensionalities. In the process, we derive and demonstrate the ﬁrst truly
hierarchical fast Gauss transforms, effectively combining the best tools
from discrete algorithms and continuous approximation theory.

1 Fast Gaussian Summation

e

i.e. where the ker-

Kernel summations are fundamental in both statistics/learning and computational physics.
NRPr=1
−||xq −xr ||2
This paper will focus on the common form G(xq ) =
2h2
nel is the Gaussian kernel with scaling parameter, or bandwidth h, there are NR reference
points xr , and we desire the sum for NQ different query points xq . Such kernel summations
appear in a wide array of statistical/learning methods [5], perhaps most obviously in kernel
density estimation [11], the most widely used distribution-free method for the fundamental
task of density estimation, which will be our main example. Understanding kernel summa-
tion algorithms from a recently developed uniﬁed perspecti ve [5] begins with the picture of
Figure 1, then separately considers the discrete and continuous aspects.
Discrete/geometric aspect. In terms of discrete algorithmic structure, the dual-tree frame-
work of [5], in the context of kernel summation, generalizes all of the well-known algo-
rithms. 1 It was applied to the problem of kernel density estimation in [7] using a simple

1These include the Barnes-Hut algorithm [2], the Fast Multipole Method [8], Appel’s algorithm
[1], and the WSPD [4]: the dual-tree method is a node-node algorithm (considers query regions rather
than points), is fully recursive, can use distribution-sensitive data structures such as kd-trees, and is
bichromatic (can specialize for differing query and reference sets).

Figure 1: The basic idea is to approximate the kernel sum contribution of some subset of the ref-
erence points XR , lying in some compact region of space R with centroid xR , to a query point. In
more efﬁcient schemes a query region is considered,
i.e. the approximate contribution is made to an
entire subset of the query points XQ lying in some region of space Q, with centroid xQ .

ﬁnite-difference approximation, which is tantamount to a c entroid approximation. Partially
by avoiding series expansions, which depend explicitly on the dimension, the result was
the fastest such algorithm for general dimension, when operating at the optimal bandwidth.
Unfortunately, when performing cross-validation to determine the (initially unknown) op-
timal bandwidth, both suboptimally small and large bandwidths must be evaluated. The
ﬁnite-difference-based dual-tree method tends to be efﬁci
ent at or below the optimal band-
width, and at very large bandwidths, but for intermediately-large bandwidths it suffers.
Continuous/approximation aspect. This motivates investigating a multipole-like series
approximation which is appropriate for the Gaussian kernel, as introduced by [9], which
can be shown the generalize the centroid approximation. We de ﬁne the Hermite functions
hn (t) by hn (t) = e−t2
Hn (t), where the Hermite polynomials Hn (t) are de ﬁned by the
Rodrigues formula: Hn (t) = (−1)n et2
Dn e−t2
, t ∈ R1 . After scaling and shifting the ar-
gument t appropriately, then taking the product of univariate functions for each dimension,
we obtain the multivariate Hermite expansion
α! (cid:18) xr − xR√2h2 (cid:19)α
hα (cid:18) xq − xR√2h2 (cid:19)
NRXr=1
NRXr=1 Xα≥0
1
where we’ve adopted the usual multi-index notation as in [9]. This can be re-written as
hα (cid:18) xr − xQ√2h2 (cid:19) (cid:18) xq − xQ√2h2 (cid:19)α
NRXr=1 Xα≥0
NRXr=1
to express the sum as a Taylor (local) expansion about a nearby representative centroid xQ
in the query region. We will be using both types of expansions simultaneously.

−||xq −xr ||2
2h2

G(xq ) =

G(xq ) =

−||xq −xr ||2
2h2

e

=

e

=

1
α!

(1)

(2)

Since series approximations only hold locally, Greengard and Rokhlin [8] showed that it
is useful to think in terms of a set of three ‘translation operators’ for converting between
expansions centered at different points, in order to create their celebrated hierarchical algo-
rithm. This was done in the context of the Coulombic kernel, but the Gaussian kernel has
importantly different mathematical properties. The original Fast Gauss Transform (FGT)
[9] was based on a ﬂat grid, and thus provided only one operato r ( “H2L” of the next sec-
tion), with an associated error bound (which was unfortunately incorrect). The Improved
Fast Gauss Transform (IFGT) [14] was based on a ﬂat set of clus ters and provided no op-
erators with a rearranged series approximation, which intended to be more favorable in
higher dimensions but had an incorrect error bound. We will show the derivations of all
the translation operators and associated error bounds needed to obtain, for the ﬁrst time, a
hierarchical algorithm for the Gaussian kernel.

2 Translation Operators and Error Bounds

The ﬁrst operator converts a multipole expansion of a refere nce node to form a local expan-
sion centered at the centroid of the query node, and is our main approximation workhorse.
Lemma 2.1. Hermite-to-local (H2L) translation operator for Gaussian kernel (as pre-
sented in Lemma 2.2 in [9, 10]): Given a reference node XR , a query node XQ , and the
Aαhα (cid:16) xq−xR√2h2 (cid:17), the
Hermite expansion centered at a centroid xR of XR : G(xq ) = Pα≥0
Taylor expansion of the Hermite expansion at the centroid xQ of the query node XQ is
Aα hα+β (cid:16) xQ−xR√2h2 (cid:17).
Bβ (cid:16) xq −xQ√2h2 (cid:17)β
β ! Pα≥0
given by G(xq ) = Pβ≥0
where Bβ = (−1)|β |
Proof. (sketch) The proof consists of replacing the Hermite function portion of the expan-
sion with its Taylor series.
Note that we can rewrite G(xq ) = Pα≥0 (cid:20) NRPr=1
α! (cid:16) xr−xR√2h2 (cid:17)α (cid:21) hα (cid:16) xq−xR√2h2 (cid:17) by interchanging
1
the summation order, such that the term in the brackets depends only on the reference
points, and can thus be computed indepedent of any query location – we will call such
terms Hermite moments. The next operator allows the efﬁcien t pre-computation of the
Hermite moments in the reference tree in a bottom-up fashion from its children.
Lemma 2.2. Hermite-to-Hermite (H2H) translation operator for Gaussian kernel:
in a reference node XR′ :
Given the Hermite expansion centered at a centroid xR′
√2h2 (cid:17), this same Hermite expansion shifted to a new loca-
A′α hα (cid:16) xq−xR′
G(xq ) = Pα≥0
Aγ hγ (cid:16) xq −xR√2h2 (cid:17) where
tion xR of the parent node of XR is given by G(xq ) = Pγ≥0
(γ−α)! A′α (cid:16) xR′ −xR√2h2 (cid:17)γ−α
Aγ = P0≤α≤γ
1
.
Proof. We simply replace the Hermite function part of the expansion by a new Taylor
series, as follows:
√2h2 «
αhα „ xq − xR′
G(xq ) = Xα≥0
A′
√2h2 «
√2h2 «β
(−1)|β |hα+β „ xq − xR
β ! „ xR − xR′
α Xβ≥0
= Xα≥0
1
A′
√2h2 «
√2h2 «β
(−1)|β |hα+β „ xq − xR
β ! „ xR − xR′
= Xα≥0 Xβ≥0
1
A′
α
√2h2 «
√2h2 «β
hα+β „ xq − xR
β ! „ xR′ − xR
= Xα≥0 Xβ≥0
1
A′
α
√2h2 «γ−α35 hγ „ xq − xR
24 X0≤α≤γ
√2h2 «
α „ xR′ − xR
= Xγ≥0
1
A′
(γ − α)!
where γ = α + β .

The next operator acts as a “clean-up ” routine in a hierarchi cal algorithm. Since we can
approximate at different scales in the query tree, we must somehow combine all the ap-
proximations at the end of the computation. By performing a breadth- ﬁrst traversal of the
query tree, the L2L operator shifts a node’s local expansion to the centroid of each child.
Lemma 2.3. Local-to-local
(L2L)
translation operator
for Gaussian ker-
nel: Given a Taylor expansion centered at a centroid xQ′ of a query node
Bβ (cid:16) xq−xQ′
√2h2 (cid:17)β
XQ′ : G(xq ) = Pβ≥0
the Taylor expansion obtained by shift-
,
ing this expansion to the new centroid xQ of
Pα≥0 " Pβ≥α
√2h2 (cid:17)β−α# (cid:16) xq −xQ√2h2 (cid:17)α
α!(β−α)! Bβ (cid:16) xQ−xQ′
β !
Proof. Applying the multinomial theorem to to expand about the new center xQ yields:
√2h2 «β
Bβ „ xq − xQ′
G(xq ) = Xβ≥0
α!(β − α)! „ xQ − xQ′
√2h2 «β−α „ xq − xQ
√2h2 «α
= Xβ≥0 Xα≤β
β !
Bβ
whose summation order can be interchanged to achieve the result.

the child node XQ is G(xq ) =

.

.

. For any query point xq , the
D−1Pk=0 (cid:0)D
k (cid:1)(1 −

Because the Hermite and the Taylor expansion are truncated after taking pD terms, we incur
an error in approximation. The original error bounds for the Gaussian kernel in [9, 10] were
wrong and corrections were shown in [3]. Here, we will present all necessary three error
bounds incurred in performing translation operators. We note that these error bounds place
limits on the size of the query node and the reference node. 2
Lemma 2.4. Error Bound for Truncating an Hermite Expansion (as presented in [3]):
Suppose we are given an Hermite expansion of a reference node XR about its centroid xR :
Aαhα (cid:16) xq−xR√2h2 (cid:17) where Aα =
α! (cid:16) xr−xR√2h2 (cid:17)α
NRPr=1
G(xq ) = Pα≥0
1
error due to truncating the series after the ﬁrst pD term is |ǫM (p)| ≤ NR
(1−r)D
rp )k (cid:16) rp
√p! (cid:17)D−k
where ∀xr ∈ XR satisﬁes ||xr − xR ||∞ < rh for r < 1.
Proof. (sketch) We expand the Hermite expansion as a product of one-dimensional Her-
mite functions, and utilize a bound on one-dimensional Hermite functions due to [13]:
n
2
−x
1
n! |hn (x)| ≤ 2
2 , n ≥ 0, x ∈ R1 .
2
e
√n!
Lemma 2.5. Error Bound for Truncating a Taylor Expansion Converted from an
Hermite Expansion of Inﬁnite Order:
Suppose we are given the following Taylor ex-
Bβ (cid:16) xq−xQ√2h2 (cid:17)β
pansion about the centroid xQ of a query node G(xq ) = Pβ≥0
2Strain [12] proposed the interesting idea of using Stirling’s formula (for any non-negative integer
e ´n
n: ` n+1
≤ n!) to lift the node size constraint; one might imagine that this could allow approxi-
mation of larger regions in a tree-based algorithm. Unfortunately, the error bounds developed in [12]
were also incorrect. We have derived the three necessary corrected error bounds based on the tech-
niques in [3]. However, due to space, and because using these bounds actually degraded performance
slightly, we do not include those lemmas here.

where

Use e

.

and

(−1)ni
ni !

(−1)ni
ni !

−||xq −xr ||2
2h2

e

vp (xqi , xri , xQi ) =

Aαhα+β (cid:16) xQ−xR√2h2 (cid:17) and Aα ’s are the coefﬁcients of the Hermite ex-
β ! Pα≥0
Bβ = (−1)|β |
pansion centered at the reference node centroid xR . Then, truncating the series after
√p! (cid:17)D−k
k (cid:1)(1 − rp )k (cid:16) rp
D−1Pk=0 (cid:0)D
pD terms satisﬁes the error bound |ǫL (p)| ≤ NR
where
(1−r)D
||xq − xQ ||∞ < rh for r < 1, ∀xq ∈ XQ .
Proof. Taylor expansion of the Hermite function yields
√2h2 «β
√2h2 « „ xq − xQ
√2h2 «α
hα+β „ xQ − xR
α! „ xr − xR
β ! Xα≥0
= Xβ≥0
(−1)|β |
1
α! „ xR − xr
√2h2 «α
√2h2 « „ xq − xQ
√2h2 «β
(−1)|α|hα+β „ xQ − xR
= Xβ≥0
β ! Xα≥0
(−1)|β |
1
√2h2 « „ xq − xQ
√2h2 «β
hβ „ xQ − xr
= Xβ≥0
(−1)|β |
β !
DQi=1
−||xq −xr ||2
(up (xqi , xri , xQi ) + vp (xqi , xri , xQi )) for 1 ≤ i ≤ D , where
=
2h2
hni „ xQi − xri
√2h2 « „ xqi − xQi
√2h2 «ni
p−1Xni =0
up (xqi , xri , xQi ) =
√2h2 « „ xqi − xQi
√2h2 «ni
hni „ xQi − xri
∞Xni =p
These univariate functions respectively satisfy up(xqi , xri , xQi ) ≤ 1−rp
1−r
rp
vp (xqi , xri , xQi ) ≤ 1√p!
1−r , for 1 ≤ i ≤ D , achieving the multivariate bound.
Lemma 2.6. Error Bound for Truncating a Taylor Expansion Converted from an Al-
ready Truncated Hermite Expansion: A truncated Hermite expansion centered about
Aα hα (cid:16) xq −xR√2h2 (cid:17) has the following
the centroid xR of a reference node G(xq ) = Pα<p
Cβ (cid:16) xq −xQ√2h2 (cid:17)β
Taylor expansion about the centroid xQ of a query node: G(xq ) = Pβ≥0
Aαhα+β (cid:16) xQ−xR√2h2 (cid:17). Truncat-
β ! Pα<p
where the coefﬁcients Cβ are given by Cβ = (−1)|β |
D−1Pk=0 (cid:0)D
k (cid:1)((1 −
NR
ing the series after pD terms satisﬁes the error bound |ǫL (p)| ≤
(1−2r)2D
(cid:17)D−k
(2r)p )2 )k (cid:16) ((2r)p )(2−(2r)p )
for a query node XQ for which ||xq − xQ ||∞ < rh, and
√p!
a reference node XR for which ||xr − xR ||∞ < rh for r < 1
2 , ∀xq ∈ XQ , ∀xr ∈ XR .
Proof. We de ﬁne upi = up (xqi , xri , xQi , xRi ), vpi = vp (xqi , xri , xQi , xRi ), wpi =
wp (xqi , xri , xQi , xRi ) for 1 ≤ i ≤ D:
(−1)nj hni +nj „ xQi − xRi
√2h2 « „ xqi − xQi
√2h2 «nj
√2h2 «ni
nj ! „ xRi − xri
p−1Xnj =0
p−1Xni =0
(−1)ni
1
ni !
(−1)nj hni+nj „ xQi − xRi
√2h2 « „ xqi − xQi
√2h2 «nj
√2h2 «ni
nj ! „ xRi − xri
p−1Xni=0
∞Xnj =p
1

(−1)ni
ni !

upi =

vpi =

wpi =

−||xq −xr ||2
2h2

−

vpi ≤

1
√p!

(−1)ni
ni !

∞Xnj =0
−||xq −xr ||2
2h2

∞Xni=p
Note that e

nj ! „ xRi − xri
(−1)nj hni +nj „ xQi − xRi
√2h2 « „ xqi − xQi
√2h2 «nj
√2h2 «ni
1
DQi=1
(upi + vpi + wpi ) for 1 ≤ i ≤ D . Using the bound for
=
Hermite functions and the property of geometric series, we obtain the following upper
bounds:
1 − 2r «2
(2r)ni (2r)nj = „ 1 − (2r)p )
p−1Xnj =0
p−1Xni =0
upi ≤
√p! „ 1 − (2r)p
1 − 2r « „ (2r)p
1 − 2r «
p−1Xni=0
∞Xnj =p
1
(2r)ni (2r)nj =
1 − 2r « „ (2r)p
1 − 2r «
√p! „ 1
∞Xni=p
∞Xnj =0
1
1
(2r)ni (2r)nj =
wpi ≤
√p!
upi ˛˛˛˛˛ ≤ (1 − 2r)−2D
k !((1 − (2r)p )2 )k „ ((2r)p )(2 − (2r)p )
D−1Xk=0  D
«D−k
DYi=1
√p!
√2h2 «β ˛˛˛˛˛˛
˛˛˛˛˛˛
D−1
k ”((1 − (2r)p )2 )k „ ((2r)p )(2 − (2r)p )
«D−k
Cβ „ xq − xQ
NR
Xk=0 “D
G(xq ) − Xβ<p
≤
√p!
(1 − 2r)2D
3 Algorithm and Results

Therefore,
˛˛˛˛˛
e

Algorithm.
algorithm mainly consists of making the
function call
The
DFGT(Q.root,R.root),
calling the recursive function DFGT() with the root
i.e.
nodes of the query tree and reference tree. After the DFGT() routine is completed, the
pre-order traversal of the query tree implied by the L2L operator is performed. Before the
DFGT() routine is called, the reference tree could be initialized with Hermite coefﬁcients
stored in each node using the H2H translation operator, but instead we will compute
them as needed on the ﬂy.
It adaptively chooses among three po ssible methods for
approximating the summation contribution of the points in node R to the queries in node
Q , a running
Q, which are self-explanatory, based on crude operation count estimates. Gmin
lower bound on the kernel sum G(xq ) for any xq ∈ XQ , is used to ensure locally that
the global relative error is ǫ or less. This automatic mechanism allows the user to specify
only an error tolerance ǫ rather than other tweak parameters. Upon approximation, the
upper and lower bounds on G for Q and all its children are updated; the latter can be
done in an O(1) delayed fashion as in [7]. The remainder of the routine implements the
characteristic four-way dual-tree recursion. We also tested a hybrid method (DFGTH)
which approximates if either of the DFD or DFGT approximation criteria are met.
Experimental results. We empirically studied the runtime 3 performance of ﬁve algo-
rithms on ﬁve real-world datasets for kernel density estima tion at every query point with a
range of bandwidths, from 3 orders of magnitude smaller than optimal to three orders larger
than optimal, according to the standard least-squares cross-validation score [11]. The naive

3All times include all preprocessing costs including any data structure construction. Times are
measured in CPU seconds on a dual-processor AMD Opteron 242 machine with 8 Gb of main mem-
ory and 1 Mb of CPU cache. All the codes that we have written and obtained are written in C and
C++, and was compiled under -O6 -funroll-loops ﬂags on Linux kernel 2.4.26.

algorithm computes the sum explicitly and thus exactly. We have limited all datasets to
50K points so that true relative error, i.e. (cid:16)| bG(xq ) − Gtrue (xq )|(cid:17) /Gtrue (xq ), can be eval-
uated, and set the tolerance at 1% relative error for all query points. When any method fails
to achieve the error tolerance in less time than twice that of the naive method, we give up.
Codes for the FGT [9] and for the IFGT [14] were obtained from the authors’ websites.
Note that both of these methods require the user to tweak parameters, while the others are
automatic. 4 DFD refers to the depth- ﬁrst dual-tree ﬁnite-difference me
thod [7].

DFGT(Q, R)
pDH = pDL = pH 2L = ∞
if R.maxside < 2h, pDH = the smallest p ≥ 1 such that
√p! (cid:17)D−k
k (cid:1)(1 − rp )k (cid:16) rp
D−1Pk=0 (cid:0)D
NR
Q .
< ǫGmin
(1−r)D
if Q.maxside < 2h, pDL = the smallest p ≥ 1 such that
√p! (cid:17)D−k
k (cid:1)(1 − rp )k (cid:16) rp
D−1Pk=0 (cid:0)D
NR
Q .
< ǫGmin
(1−r)D
if max(Q.maxside,R.maxside) < h, pH 2L = the smallest p ≥ 1 such that
(cid:17)D−k
k (cid:1)((1 − (2r)p )2 )k (cid:16) ((2r)p )(2−(2r)p )
D−1Pk=0 (cid:0)D
NR
Q .
< ǫGmin
√p!
(1−2r)2D
DLNR . cH 2L = DpD+1
H 2L . cDirect = DNQNR .
cDH = pD
DH NQ . cDL = pD
if no Hermite coefﬁcient of order pDH exists for XR ,
Compute it. cDH = cDH + pD
DH NR .
if no Hermite coefﬁcient of order pH 2L exists for XR ,
Compute it. cH 2L = cH 2L + pD
H 2LNR .

c = min(cDH , cDL , cH 2L , cDirect ).
if c = cDH < ∞, (Direct Hermite)
Evaluate each xq at the Hermite series of order pDH centered about xR of XR
using Equation 1.
if c = cDL < ∞, (Direct Local)
Accumulate each xr ∈ XR as the Taylor series of order pDL about the center
xQ of XQ using Equation 2.
if c = cH 2L < ∞, (Hermite-to-Local)
Convert the Hermite series of order pH 2L centered about xR of XR to the Taylor
series of the same order centered about xQ of XQ using Lemma 2.1.
if c 6= cDirect ,
Update Gmin and Gmax in Q and all its children. return.

if leaf(Q) and leaf(R),
Perform the naive algorithm on every pair of points in Q and R.
else
DFGT(Q.left, R.left). DFGT(Q.left, R.right).
DFGT(Q.right, R.left). DFGT(Q.right, R.right).
4For the FGT, note that the algorithm only ensures: ˛˛˛ bG(xq ) − Gtrue (xq )˛˛˛ ≤ τ . Therefore, we
ﬁrst set
τ = ǫ, halving τ until the error tolerance ǫ was met. For the IFGT, which has multiple
parameters that must be tweaked simultaneously, an automatic scheme was created, based on the
recommendations given in the paper and software documentation: For D = 2, use p = 8; for D = 3,
use p = 6; set ρx = 2.5; start with K = √N and double K until the error tolerance is met. When this
failed to meet the tolerance, we resorted to additional trial and error by hand. The costs of parameter
selection for these methods in both computer and human time is not included in the table.

Algorithm \ scale

0.001

1000

301.696
0.114430
7.55986
3.604753
3.5638
0.627554

Naive
FGT
IFGT
DFD
DFGT
DFGTH

Naive
FGT
IFGT
DFD
DFGT
DFGTH

301.696
0.183616
7.576783
1.551019
2.532401
0.68471

0.01
100
10
1
0.1
sj2-50000-2 (astronomy: positions), D = 2, N = 50000, h∗ = 0.00139506
301.696
301.696
301.696
301.696
301.696
301.696
0.319538
2.01846
3.892312
out of RAM
out of RAM
out of RAM
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
151.590062
62.077669
6.018158
1.658592
1.087066
0.837724
2.777454
18.450387
72.435177
4.599235
1.11567
0.849935
1.10654
0.846294
1.683913
6.265131
5.063365
1.036626
colors50k (astronomy: colors), D = 2, N = 50000, h∗ = 0.0016911
301.696
301.696
301.696
301.696
301.696
301.696
0.475281
> 2×Naive
> 2×Naive
out of RAM
out of RAM
out of RAM
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
81.373053
280.633106
30.294007
2.802112
1.469454
1.095838
5.336602
12.886239
285.719266
29.231309
1.983888
1.099828
1.081216
1.78648
1.47692
2.855083
24.598749
7.142465
edsgc-radec-rnd (astronomy: angles), D = 2, N = 50000, h∗ = 0.00466204
301.696
301.696
301.696
301.696
301.696
301.696
0.210799
1.768738
2.859245
out of RAM
out of RAM
out of RAM
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
357.099354
63.849361
5.860172
1.682261
1.083528
0.812462
3.424304
21.652047
73.036687
4.346061
1.120015
0.84023
0.821672
1.104545
1.737799
6.037217
5.7398
1.883216
mockgalaxy-D-1M-rnd (cosmology: positions), D = 3, N = 50000, h∗ = 0.000768201
354.868751
354.868751
354.868751
354.868751
354.868751
354.868751
> 2×Naive
> 2×Naive
out of RAM
out of RAM
out of RAM
out of RAM
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
42.022605
1.086608
0.843451
0.761524
0.701547
0.70054
125.059911
50.619588
0.999316
0.799711
0.733638
0.73007
0.724004
0.719951
0.789002
0.877564
1.265064
22.6106
bio5-rnd (biology: drug activity), D = 5, N = 50000, h∗ = 0.000567161
364.439228
364.439228
364.439228
364.439228
364.439228
364.439228
364.439228
Naive
out of RAM
out of RAM
out of RAM
out of RAM
out of RAM
out of RAM
out of RAM
FGT
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
IFGT
412.39142
94.345003
12.065697
4.70948
2.4958865
2.249868
107.675935
DFD
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
DFGT
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
> 2×Naive
DFGTH
Discussion. The experiments indicate that the DFGTH method is able to achieve rea-
sonable performance across all bandwidth scales. Unfortunately none of the series
approximation-based methods do well on the 5-dimensional data, as expected, highlight-
ing the main weakness of the approach presented. Pursuing corrections to the error bounds
necessary to use the intriguing series form of [14] may allow an increase in dimensionality.

354.868751
> 2×Naive
> 2×Naive
383.12048
109.353701
87.488392

301.696
0.059664
7.585585
0.743045
1.977302
0.436596

Naive
FGT
IFGT
DFD
DFGT
DFGTH

Naive
FGT
IFGT
DFD
DFGT
DFGTH

References
[1] A. W. Appel. An Efﬁcient Program for Many-Body Simulatio ns. SIAM Journal on Scientiﬁc and Statistical Computing ,
6(1):85–103, 1985.
[2] J. Barnes and P. Hut. A Hierarchical O(N logN ) Force-Calculation Algorithm. Nature, 324, 1986.
[3] B. Baxter and G. Roussos. A new error estimate of the fast gauss transform. SIAM Journal on Scientiﬁc Computing ,
24(1):257–259, 2002.
[4] P. Callahan and S. Kosaraju. A decomposition of multidimensional point sets with applications to k-nearest-neighbors and
n-body potential ﬁelds.
Journal of the ACM, 62(1):67–90, January 1995.
[5] A. Gray and A. W. Moore. N-Body Problems in Statistical Learning. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors,
Advances in Neural Information Processing Systems 13 (December 2000). MIT Press, 2001.
[6] A. G. Gray. Bringing Tractability to Generalized N-Body Problems in Statistical and Scientiﬁc Computation . PhD thesis,
Carnegie Mellon University, 2003.
[7] A. G. Gray and A. W. Moore. Rapid Evaluation of Multiple Density Models. In Artiﬁcial Intelligence and Statistics 2003 ,
2003.
[8] L. Greengard and V. Rokhlin. A Fast Algorithm for Particle Simulations. Journal of Computational Physics, 73, 1987.
[9] L. Greengard and J. Strain. The fast gauss transform. SIAM Journal on Scientiﬁc and Statistical Computing , 12(1):79–94,
1991.
[10] L. Greengard and X. Sun. A new version of the fast gauss transform. Documenta Mathematica, Extra Volume ICM(III):575–
584, 1998.
[11] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall, 1986.
[12] J. Strain. The fast gauss transform with variable scales. SIAM Journal on Scientiﬁc and Statistical Computing , 12:1131–
1139, 1991.
[13] O. Sz ´asz. On the relative extrema of the hermite orthog onal functions. J. Indian Math. Soc., 15:129–134, 1951.
[14] C. Yang, R. Duraiswami, N. A. Gumerov, and L. Davis. Improved fast gauss transform and efﬁcient kernel density estim a-
tion. International Conference on Computer Vision, 2003.


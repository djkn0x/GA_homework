Tensor Subspace Analysis

Partha Niyogi1
Deng Cai2
Xiaofei He1
1 Department of Computer Science, University of Chicago
{xiaofei, niyogi}@cs.uchicago.edu
2 Department of Computer Science, University of Illinois at Urbana-Champaign
dengcai2@uiuc.edu

Abstract

Previous work has demonstrated that the image variations of many ob-
jects (human faces in particular) under variable lighting can be effec-
tively modeled by low dimensional linear spaces. The typical linear sub-
space learning algorithms include Principal Component Analysis (PCA),
Linear Discriminant Analysis (LDA), and Locality Preserving Projec-
tion (LPP). All of these methods consider an n1 × n2 image as a high
dimensional vector in Rn1×n2 , while an image represented in the plane
is intrinsically a matrix. In this paper, we propose a new algorithm called
Tensor Subspace Analysis (TSA). TSA considers an image as the sec-
ond order tensor in Rn1 ⊗ Rn2 , where Rn1 and Rn2 are two vector
spaces. The relationship between the column vectors of the image ma-
trix and that between the row vectors can be naturally characterized by
TSA. TSA detects the intrinsic local geometrical structure of the tensor
space by learning a lower dimensional tensor subspace. We compare our
proposed approach with PCA, LDA and LPP methods on two standard
databases. Experimental results demonstrate that TSA achieves better
recognition rate, while being much more efﬁcient.

1

Introduction

There is currently a great deal of interest in appearance-based approaches to face recogni-
tion [1], [5], [8]. When using appearance-based approaches, we usually represent an image
of size n1 × n2 pixels by a vector in Rn1×n2 . Throughout this paper, we denote by face
space the set of all the face images. The face space is generally a low dimensional mani-
fold embedded in the ambient space [6], [7], [10]. The typical linear algorithms for learning
such a face manifold for recognition include Principal Component Analysis (PCA), Linear
Discriminant Analysis (LDA) and Locality Preserving Projection (LPP) [4].

Most of previous works on statistical image analysis represent an image by a vector in
high-dimensional space. However, an image is intrinsically a matrix, or the second or-
der tensor. The relationship between the rows vectors of the matrix and that between the
column vectors might be important for ﬁnding a projection, e specially when the number
of training samples is small. Recently, multilinear algebra, the algebra of higher-order
tensors, was applied for analyzing the multifactor structure of image ensembles [9], [11],
[12]. Vasilescu and Terzopoulos have proposed a novel face representation algorithm called
Tensorface [9]. Tensorface represents the set of face images by a higher-order tensor and

extends Singular Value Decomposition (SVD) to higher-order tensor data. In this way, the
multiple factors related to expression, illumination and pose can be separated from different
dimensions of the tensor.

In this paper, we propose a new algorithm for image (human faces in particular) represen-
tation based on the considerations of multilinear algebra and differential geometry. We call
it Tensor Subspace Analysis (TSA). For an image of size n1 × n2 , it is represented as the
second order tensor (or, matrix) in the tensor space Rn1 ⊗ Rn2 . On the other hand, the face
space is generally a submanifold embedded in Rn1 ⊗ Rn2 . Given some images sampled
from the face manifold, we can build an adjacency graph to model the local geometrical
structure of the manifold. TSA ﬁnds a projection that respec ts this graph structure. The
obtained tensor subspace provides an optimal linear approximation to the face manifold
in the sense of local isometry. Vasilescu shows how to extend SVD(PCA) to higher order
tensor data. We extend Laplacian based idea to tensor data.

It is worthwhile to highlight several aspects of the proposed approach here:

1. While traditional linear dimensionality reduction algorithms like PCA, LDA and
LPP ﬁnd a map from Rn to Rl (l < n), TSA ﬁnds a map from Rn1 ⊗ Rn2 to
Rl1 ⊗ Rl2 (l1 < n1 , l2 < n2 ). This leads to structured dimensionality reduction.
2. TSA can be performed in either supervised, unsupervised, or semi-supervised
manner. When label information is available, it can be easily incorporated into
the graph structure. Also, by preserving neighborhood structure, TSA is less sen-
sitive to noise and outliers.
3. The computation of TSA is very simple. It can be obtained by solving two eigen-
vector problems. The matrices in the eigen-problems are of size n1×n1 or n2×n2 ,
which are much smaller than the matrices of size n × n (n = n1 × n2 ) in PCA,
LDA and LPP. Therefore, TSA is much more computationally efﬁ cient in time
and storage. There are few parameters that are independently estimated, so per-
formance in small data sets is very good.
4. TSA explicitly takes into account the manifold structure of the image space. The
local geometrical structure is modeled by an adjacency graph.
5. This paper is primarily focused on the second order tensors (or, matrices). How-
ever, the algorithm and analysis presented here can also be applied to higher order
tensors.

2 Tensor Subspace Analysis

In this section, we introduce a new algorithm called Tensor Subspace Analysis for learning a
tensor subspace which respects the geometrical and discriminative structures of the original
data space.

2.1 Laplacian based Dimensionality Reduction

Problems of dimensionality reduction has been considered. One general approach is based
on graph Laplacian [2]. The objective function of Laplacian eigenmap is as follows:
f Xij
(f (xi ) − f (xj ))2 Sij
min
where S is a similarity matrix. These optimal functions are nonlinear but may be expensive
to compute.

A class of algorithms may be optimized by restricting problem to more tractable families
of functions. One natural approach restricts to linear function giving rise to LPP [4]. In this

paper we will consider a more structured subset of linear functions that arise out of tensor
analysis. This provided greater computational beneﬁts.

2.2 The Linear Dimensionality Reduction Problem in Tensor Space

The generic problem of linear dimensionality reduction in the second order tensor space
is the following. Given a set of data points X1 , · · · , Xm in Rn1 ⊗ Rn2 , ﬁnd two trans-
formation matrices U of size n1 × l1 and V of size n2 × l2 that maps these m points to a
set of points Y1 , · · · , Ym ∈ Rl1 ⊗ Rl2 (l1 < n1 , l2 < n2 ), such that Yi “represents ” Xi ,
where Yi = U T XiV . Our method is of particular applicability in the special case where
X1 , · · · , Xm ∈ M and M is a nonlinear submanifold embedded in Rn1 ⊗ Rn2 .

2.3 Optimal Linear Embeddings

e−

,

(1)

As we described previously, the face space is probably a nonlinear submanifold embedded
in the tensor space. One hopes then to estimate geometrical and topological properties of
the submanifold from random points ( “scattered data”) lyin g on this unknown submanifold.
In this section, we consider the particular question of ﬁndi ng a linear subspace approxima-
tion to the submanifold in the sense of local isometry. Our method is fundamentally based
on LPP [4].
Given m data points X = {X1 , · · · , Xm} sampled from the face submanifold M ∈ Rn1 ⊗
Rn1 , one can build a nearest neighbor graph G to model the local geometrical structure of
M. Let S be the weight matrix of G . A possible deﬁnition of S is as follows:
kXi−Xj k2
Sij = 
if Xi is among the k nearest
t
neighbors of Xj , or Xj is among
the k nearest neighbors of Xi ;

otherwise.
0,
where t is a suitable constant. The function exp(−kXi − Xj k2 /t) is the so called heat
kernel which is intimately related to the manifold structure. k · k is the Frobenius norm of
matrix, i.e. kAk = qPi Pj a2
ij . When the label information is available, it can be easily
incorporated into the graph as follows:
Sij = ( e−
kXi−Xj k2
t
0,
Let U and V be the transformation matrices. A reasonable transformation respecting the
graph structure can be obtained by solving the following objective functions:
U,V Xij
kU T XiV − U T Xj V k2Sij
min
The objective function incurs a heavy penalty if neighboring points Xi and Xj are mapped
far apart. Therefore, minimizing it is an attempt to ensure that if Xi and Xj are “close”
then U T XiV and U T Xj V are “close” as well. Let
Yi = U T XiV . Let D be a diagonal
matrix, Dii = Pj Sij . Since kAk2 = tr(AAT ), we see that:
1
1
2 Xij
2 Xij
kU T XiV − U T Xj V k2Sij =
tr (cid:0)(Yi − Yj )(Yi − Yj )T (cid:1) Sij
1
2 Xij
j − YiY T
tr (cid:0)YiY T
i + Yj Y T
j − Yj Y T
i (cid:1) Sij
=
j (cid:17)
= tr(cid:16) Xi
i − Xij
Sij YiY T
DiiYiY T

if Xi and Xj share the same label;
otherwise.

,

(2)

(3)

=

= tr(cid:16) Xi
j U (cid:17)
i U − Xij
DiiU T XiV V T X T
Sij U T XiV V T X T
j (cid:1)U (cid:17)
= tr(cid:16)U T (cid:0) Xi
i − Xij
Sij XiV V T X T
DiiXiV V T X T
.
= tr (cid:0)U T (DV − SV ) U (cid:1)
where DV = Pi DiiXiV V T X T
i and SV = Pij Sij XiV V T X T
j . Similarly, kAk2 =
tr(AT A), so we also have
1
2 Xij
kU T XiV − U T Xj V k2Sij
1
2 Xij
tr (cid:0)(Yi − Yj )T (Yi − Yj )(cid:1) Sij
1
2 Xij
i Yj − Y T
tr (cid:0)Y T
i Yi + Y T
j Yj − Y T
j Yi (cid:1) Sij
=
i Yj (cid:17)
= tr(cid:16) Xi
i Yi − Xij
Sij Y T
DiiY T
i U U T Xj (cid:1)V (cid:17)
= tr(cid:16)V T (cid:0) Xi
i U U T Xi − Xij
X T
DiiX T
.
= tr (cid:0)V T (DU − SU ) V (cid:1)
i U U T Xj . Therefore, we should
i U U T Xi and SU = Pij Sij X T
where DU = Pi DiiX T
simultaneously minimize tr (cid:0)U T (DV − SV ) U (cid:1) and tr (cid:0)V T (DU − SU ) V (cid:1).
In addition to preserving the graph structure, we also aim at maximizing the global variance
on the manifold. Recall that the variance of a random variable x can be written as follows:
var(x) = ZM
(x − µ)2dP (x), µ = ZM
where M is the data manifold, µ is the expected value of x and dP is the probability
measure on the manifold. By spectral graph theory [3], dP can be discretely estimated by
the diagonal matrix D(Dii = Pj Sij ) on the sample points. Let Y = U T X V denote
the random variable in the tensor subspace and suppose the data points have a zero mean.
Thus, the weighted variance can be estimated as follows:
i Yi )Dii = Xi
kYi k2Dii = Xi
var(Y ) = Xi
i U U T XiV )Dii
tr(V T X T
tr(Y T
= tr  V T  Xi
i U U T Xi! V ! = tr (cid:0)V T DU V (cid:1)
DiiX T
Similarly, kYik2 = tr(YiY T
i ), so we also have:
i ! U ! = tr (cid:0)U T DV U (cid:1)
i )Dii = tr  U T  Xi
var(Y ) = Xi
DiiXiV V T X T
tr(YiY T
Finally, we get the following optimization problems:
tr (cid:0)U T (DV − SV ) U (cid:1)
tr (U T DV U )

xdP (x)

min
U,V

(4)

tr (cid:0)V T (DU − SU ) V (cid:1)
min
tr (V T DU V )
U,V
The above two minimization problems (4) and (5) depends on each other, and hence can not
be solved independently. In the following subsection, we describe a simple computational
method to solve these two optimization problems.

(5)

2.4 Computation

In this subsection, we discuss how to solve the optimization problems (4) and (5). It is easy
to see that the optimal U should be the generalized eigenvectors of (DV − SV , DV ) and the
optimal V should be the generalized eigenvectors of (DU − SU , DU ). However, it is difﬁ-
cult to compute the optimal U and V simultaneously since the matrices DV , SV , DU , SU
are not ﬁxed. In this paper, we compute U and V iteratively as follows. We ﬁrst ﬁx U , then
V can be computed by solving the following generalized eigenvector problem:
(DU − SU )v = λDU v
Once V is obtained, U can be updated by solving the following generalized eigenvector
problem:

(DV − SV )u = λDV u
(7)
Thus, the optimal U and V can be obtained by iteratively computing the generalized eigen-
vectors of (6) and (7). In our experiments, U is initially set to the identity matrix. It is easy
to show that the matrices DU , DV , DU − SU , and DV − SV are all symmetric and positive
semi-deﬁnite.

(6)

3 Experimental Results

In this section, several experiments are carried out to show the efﬁciency and effectiveness
of our proposed algorithm for face recognition. We compare our algorithm with the Eigen-
face (PCA) [8], Fisherface (LDA) [1], and Laplacianface (LPP) [5] methods, three of the
most popular linear methods for face recognition.

Two face databases were used. The ﬁrst one is the PIE (Pose, Il
lumination, and Experience)
database from CMU, and the second one is the ORL database.
In all the experiments,
preprocessing to locate the faces was applied. Original images were normalized (in scale
and orientation) such that the two eyes were aligned at the same position. Then, the facial
areas were cropped into the ﬁnal images for matching. The siz e of each cropped image in all
the experiments is 32×32 pixels, with 256 gray levels per pixel. No further preprocessing is
done. For the Eigenface, Fisherface, and Laplacianface methods, the image is represented
as a 1024-dimensional vector, while in our algorithm the image is represented as a (32 ×
32)-dimensional matrix, or the second order tensor. The nearest neighbor classi ﬁer is used
for classi ﬁcation for its simplicity.

In short, the recognition process has three steps. First, we calculate the face subspace from
the training set of face images; then the new face image to be identi ﬁed is projected into
d-dimensional subspace (PCA, LDA, and LPP) or (d × d)-dimensional tensor subspace
(TSA); ﬁnally, the new face image is identi ﬁed by nearest nei
ghbor classi ﬁer. In our TSA
algorithm, the number of iterations is taken to be 3.

3.1 Experiments on PIE Database

The CMU PIE face database contains 68 subjects with 41,368 face images as a whole. The
face images were captured by 13 synchronized cameras and 21 ﬂ ashes, under varying pose,
illumination and expression. We choose the ﬁve near frontal poses (C05, C07, C09, C27,

 

70

)
%
(
 
e
t
a
r
 
r
o
r
r
E

60

50

40

30

0

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

 

60

50
)
%
(
 
e
40
t
a
r
 
r
o
r
r
E
30

20

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

 

50

)
%
(
 
e
t
a
r
 
r
o
r
r
E

40

30

20

10

 

35

30

25
)
%
(
 
e
20
t
a
r
 
r
o
r
r
15
E

10

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

TSA
Laplac ianfaces  (PCA+LPP )
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

0

300

400

200
600
400
200
600
400
200
400
100
200
Dims  d (d·d  for TSA)
Dims  d (d·d  for TSA)
Dims  d (d·d  for TSA)
Dims  d (d·d  for TSA)
(a) 5 Train
(d) 30 Train
(c) 20 Train
(b) 10 Train
Figure 1: Error rate vs. dimensionality reduction on PIE database

1000

600

800

0

0

5

800

1000

time(s)
-
0.907

Baseline
Eigenfaces
Fisherfaces
Laplacianfaces
TSA

Table 1: Performance comparison on PIE database
10 Train
5 Train
Method
error
error
dim
dim
69.9% 1024
55.7% 1024
55.7% 654
69.9% 338
22.4%
31.5%
67
67
21.1% 134
30.8%
67
16.9% 132
27.9% 112
20 Train
30 Train
error
error
dim
dim
27.9% 1024
38.2% 1024
27.9% 990
38.1% 889
7.77%
15.4%
67
67
14.1% 146
7.13% 131
6.88% 122
9.64% 132

Method
Baseline
Eigenfaces
Fisherfaces
Laplacianfaces
TSA

time(s)
-
14.328

39.172
7.125

2.375
0.594

35.828

1.843

time(s)
-
5.297

9.609

11.516
2.063

time(s)
-
15.453

38.406

47.610
15.688

C29) and use all the images under different illuminations and expressions, thus we get 170
images for each individual. For each individual, l(= 5, 10, 20, 30) images are randomly
selected for training and the rest are used for testing.

The training set is utilized to learn the subspace representation of the face manifold by using
Eigenface, Fisherface, Laplacianface and our algorithm. The testing images are projected
into the face subspace in which recognition is then performed. For each given l, we average
the results over 20 random splits. It would be important to note that the Laplacianface
algorithm and our algorithm share the same graph structure as deﬁned in Eqn. (2).

Figure 1 shows the plots of error rate versus dimensionality reduction for the Eigenface,
Fisherface, Laplacianface, TSA and baseline methods. For the baseline method, the recog-
nition is simply performed in the original 1024-dimensional image space without any di-
mensionality reduction. Note that, the upper bound of the dimensionality of Fisherface is
c − 1 where c is the number of individuals. For our TSA algorithm, we only show its per-
formance in the (d × d)-dimensional tensor subspace, say, 1, 4, 9, etc. As can be seen, the
performance of the Eigenface, Fisherface, Laplacianface, and TSA algorithms varies with
the number of dimensions. We show the best results obtained by them in Table 1 and the
corresponding face subspaces are called optimal face subspace for each method.

It is found that our method outperforms the other four methods with different numbers
of training samples (5, 10, 20, 30) per individual. The Eigenface method performs the
worst. It does not obtain any improvement over the baseline method. The Fisherface and
Laplacianface methods perform comparatively to each each. The dimensions of the optimal
subspaces are also given in Table 1.

As we have discussed, TSA can be implemented very efﬁciently . We show the running
time in seconds for each method in Table 1. As can be seen, TSA is much faster than the

 

55

50

45

)
%
(
 
e
t
a
r
 
r
o
r
r
E

40

35

30

25

20

15

0

10

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

 

)
%
(
 
e
t
a
r
 
r
o
r
r
E

50

40

30

20

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

 

45

40

35

30

25

20

)
%
(
 
e
t
a
r
 
r
o
r
r
E

15

10

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

 

)
%
(
 
e
t
a
r
 
r
o
r
r
E

40

30

20

10

TSA
Laplac ianfaces  (PCA+LPP)
Fisherfaces  (PCA+LDA)
E igenfaces  (PCA)
Baseline

60

70

10

0

10

50

20

30
40
Dims  d
(a) 2 Train

50

20

30
40
Dims  d
(b) 3 Train

60

70

5

0

10

50

20

30
40
Dims  d
(c) 4 Train

60

70

0

0

10

50

20

30
40
Dims  d
(d) 5 Train

60

70

Figure 2: Error rate vs. dimensionality reduction on ORL database

Baseline
Eigenfaces
Fisherfaces
Laplacianfaces
TSA

time
-
38.13
60.32
62.65
65.00

Table 2: Performance comparison on ORL database
3 Train
2 Train
Method
error
error
dim
dim
30.2% 1024
22.4% 1024
113
22.3%
79
30.2%
39
13.1%
23
25.2%
12.5%
22.2%
39
39
10.7% 112
20.0% 102
4 Train
5 Train
error
error
dim
dim
11.7% 1024
16.0% 1024
182
11.6%
122
15.9%
9.17%
39
6.55%
39
5.45%
8.54%
40
39
7.12% 102
4.75% 102

time
-
141.72
212.82
248.90
201.40

Method
Baseline
Eigenfaces
Fisherfaces
Laplacianfaces
TSA

time
-
85.16
119.69
136.25
135.93

time
-
224.69
355.63
410.78
302.97

Eigenface, Fisherface and Laplacianface methods. All the algorithms were implemented in
Matlab 6.5 and run on a Intel P4 2.566GHz PC with 1GB memory.

3.2 Experiments on ORL Database

The ORL (Olivetti Research Laboratory) face database is used in this test. It consists of a
total of 400 face images, of a total of 40 people (10 samples per person). The images were
captured at different times and have different variations including expressions (open or
closed eyes, smiling or non-smiling) and facial details (glasses or no glasses). The images
were taken with a tolerance for some tilting and rotation of the face up to 20 degrees. For
each individual, l(= 2, 3, 4, 5) images are randomly selected for training and the rest are
used for testing.

The experimental design is the same as that in the last subsection. For each given l, we
average the results over 20 random splits. Figure 3.2 shows the plots of error rate versus
dimensionality reduction for the Eigenface, Fisherface, Laplacianface, TSA and baseline
methods. Note that, the presentation of the performance of the TSA algorithm is different
from that in the last subsection. Here, for a given d, we show its performance in the (d × d)-
dimensional tensor subspace. The reason is for better comparison, since the Eigenface and
Laplacianface methods start to converge after 70 dimensions and there is no need to show
their performance after that. The best result obtained in the optimal subspace and the
running time (millisecond) of computing the eigenvectors for each method are shown in
Table 2.

As can be seen, our TSA algorithm performed the best in all the cases. The Fisherface
and Laplacianface methods performed comparatively to our method, while the Eigenface
method performed poorly.

4 Conclusions and Future Work

Tensor based face analysis (representation and recognition) is introduced in this paper in
order to detect the underlying nonlinear face manifold structure in the manner of tensor
subspace learning. The manifold structure is approximated by the adjacency graph com-
puted from the data points. The optimal tensor subspace respecting the graph structure is
then obtained by solving an optimization problem. We call this Tensor Subspace Analysis
method.

Most of traditional appearance based face recognition methods (i.e. Eigenface, Fisherface,
and Laplacianface) consider an image as a vector in high dimensional space. Such repre-
sentation ignores the spacial relationships between the pixels in the image. In our work, an
image is naturally represented as a matrix, or the second order tensor. Tensor representation
makes our algorithm much more computationally efﬁcient tha n PCA, LDA, and LPP. Ex-
perimental results on PIE and ORL databases demonstrate the efﬁciency and effectiveness
of our method.

TSA is linear. Therefore, if the face manifold is highly nonlinear, it may fail to discover
the intrinsic geometrical structure.
It remains unclear how to generalize our algorithm
to nonlinear case. Also, in our algorithm, the adjacency graph is induced from the local
geometry and class information. Different graph structures lead to different projections. It
remains unclear how to deﬁne the optimal graph structure in t he sense of discrimination.

References

[1] P.N. Belhumeur, J.P. Hepanha, and D.J. Kriegman, “Eigenfac es vs. ﬁsherfaces: recognition
using class speciﬁc linear projection,”
IEEE. Trans. Pattern Analysis and Machine Intelligence,
vol. 19, no. 7, pp. 711-720, July 1997.
[2] M. Belkin and P. Niyogi, “Laplacian Eigenmaps and Spectral Techniq ues for Embedding and
Clustering ,” Advances in Neural Information Processing Systems 14, 2001.
[3] Fan R. K. Chung, Spectral Graph Theory, Regional Conference Series in Mathematics, number
92, 1997.
“Locality Preserving Projections,” Advance in Neural Information
[4] X. He and P. Niyogi,
Processing Systems 16, Vancouver, Canada, December 2003.
[5] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang,
“Face Recogn ition using Laplacian-
faces,” IEEE. Trans. Pattern Analysis and Machine Intelligence, vol. 27, No. 3, 2005.
[6] S. Roweis, and L. K. Saul, “Nonlinear Dimensionality Reduction by Loc ally Linear Embed-
ding,” Science, vol 290, 22 December 2000.
[7] J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A Global Geo metric Framework for Non-
linear Dimensionality Reduction,” Science, vol 290, 22 December 2000.
[8] M. Turk and A. Pentland, “Eigenfaces for recognition,”
Journal of Cognitive Neuroscience,
3(1):71-86, 1991.
[9] M. A. O. Vasilescu and D. Terzopoulos, “Multilinear Subspace Analy sis for Image Ensembles,”
IEEE Conference on Computer Vision and Pattern Recognition, 2003.
[10] K. Q. Weinberger and L. K. Saul, “Unsupervised Learning of Im age Manifolds by SemiDeﬁnite
Programming,”
IEEE Conference on Computer Vision and Pattern Recognition, Washington,
DC, 2004.
[11] J. Yang, D. Zhang, A. Frangi, and J. Yang,
“Two-dimensional PCA: a new approach to
appearance-based face representation and recognition,” IEEE. Trans. Pattern Analysis and Ma-
chine Intelligence, vol. 26, No. 1, 2004.
[12] J. Ye, R. Janardan, Q. Li, “Two-Dimensional Linear Discriminan t Analysis ,” Advances in
Neural Information Processing Systems 17, 2004.


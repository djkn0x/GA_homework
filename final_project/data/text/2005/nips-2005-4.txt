Maximum Margin Semi-Supervised
Learning for Structured Variables

Y. Altun, D. McAllester
TTI at Chicago
Chicago, IL 60637
altun,mcallester@tti-c.org

M. Belkin
Department of Computer Science
University of Chicago
Chicago, IL 60637
misha@cs.uchicago.edu

Abstract

Many real-world classiﬁcation problems involve the prediction of
multiple inter-dependent variables forming some structural depen-
dency. Recent progress in machine learning has mainly focused on
supervised classiﬁcation of such structured variables. In this paper,
we investigate structured classiﬁcation in a semi-supervised setting.
We present a discriminative approach that utilizes the intrinsic ge-
ometry of input patterns revealed by unlabeled data points and we
derive a maximum-margin formulation of semi-supervised learning
for structured variables. Unlike transductive algorithms, our for-
mulation naturally extends to new test points.

1 Introduction

Discriminative methods, such as Boosting and Support Vector Machines have sig-
niﬁcantly advanced the state of the art for classiﬁcation. However, traditionally
these methods do not exploit dependencies between class labels where more than
one label is predicted. Many real-world classiﬁcation problems, on the other hand,
involve sequential or structural dependencies between multiple labels. For example
labeling the words in a sentence with their part-of-speech tags involves sequential
dependency between part-of-speech tags; ﬁnding the parse tree of a sentence in-
volves a structural dependency among the labels in the parse tree. Recently, there
has been a growing interest in generalizing kernel methods to predict structured
and inter-dependent variables in a supervised learning setting, such as dual percep-
tron [7], SVMs [2, 15, 14] and kernel logistic regression [1, 11]. These techniques
combine the eﬃciency of dynamic programming methods with the advantages of
the state-of-the-art learning methods. In this paper, we investigate classiﬁcation of
structured ob jects in a semi-supervised setting.

The goal of semi-supervised learning is to leverage the learning process from a
small sample of labeled inputs with a large sample of unlabeled data. This idea has
recently attracted a considerable amount of interest due to ubiquity of unlabeled
data. In many applications from data mining to speech recognition it is easy to
produce large amounts of unlabeled data, while labeling is often manual and expen-
sive. This is also the case for many structured classiﬁcation problems. A variety

of methods ranging from Naive Bayes [12], Cotraining [4], to Transductive SVM [9]
to Cluster Kernels [6] and graph-based approaches [3] and references therein, have
been proposed. The intuition behind many of these methods is that the classiﬁca-
tion/regression function should be smooth with respect to the geometry of the data,
i. e. the labels of two inputs x and ¯x are likely to be the same if x and ¯x are similar.
This idea is often represented as the cluster assumption or the manifold assumption.
The unlabeled points reveal the intrinsic structure, which is then utilized by the
classiﬁcation algorithm. A discriminative approach to semi-supervised learning was
developed by Belkin, Sindhwani and Niyogi [3, 13], where the Laplacian operator
associated with unlabeled data is used as an additional penalty (regularizer) on the
space of functions in a Reproducing Kernel Hilbert Space. The additional regular-
ization from the unlabeled data can be represented as a new kernel — a “graph
regularized” kernel.

In this paper, building on [3, 13], we present a discriminative semi-supervised learn-
ing formulation for problems that involve structured and inter-dependent outputs
and give experimental results on max-margin semi-supervised structured classiﬁ-
cation using graph-regularized kernels. The solution of the optimization problem
that utilizes both labeled and unlabeled data is a linear combination of the graph
regularized kernel evaluated at the parts of the labeled inputs only, leading to a
large reduction in the number of parameters.
It is important to note that our
classiﬁcation function is deﬁned on all input points whereas some previous work is
only deﬁned for the input points in the (labeled and unlabeled) training sample, as
they use standard graph kernels, which are restricted to in-sample data points by
deﬁnition.

There is an the extensive literature on semi-supervised learning and the growing
number of studies on learning structured and inter-dependent variables. Delaleau
et. al. [8] propose a semi-supervised learning method for standard classiﬁcation that
extends to out-of-sample points. Brefeld et. al. [5] is one of the ﬁrst studies investi-
gating semi-supervised structured learning problem in a discriminative framework.
The most relevant previous work is the transductive structured learning proposed
by Laﬀerty et. al. [11].

2 Supervised Learning for Structured Variables

In structured learning, the goal is to learn a mapping h : X → Y from structured
inputs to structured response values, where the inputs and response values form a
dependency structure. For each input x, there is a set of feasible outputs, Y (x) ⊆ Y .
For simplicity, let us assume that Y (x) is ﬁnite for all x ∈ X , which is the case in
many real world problems and in all our examples. We denote the set of feasible
input-output pairs by Z ⊆ X × Y .

It is common to construct a discriminant function F : Z → < which maps the
feasible input-output pairs to a compatibility score of the pair. To make a prediction
for x, this score is maximized over the set of feasible outputs,

h(x) = argmax
y∈Y (x)

F (x, y).

(1)

The score of an hx, yi pair is computed from local fragments, or “parts”, of hx, yi.
In Markov random ﬁelds, x is a graph, y is a labeling of the nodes of x and a
local fragment (a part) of hx, yi is a clique in x and its labeling y . In parsing with
probabilistic context free grammars, a local fragment (a part) of hx, yi consist of
a branch of the tree y , where a branch is an internal node in y together with its

children, plus all pairs of a leaf node in y with the word in x labeled by that node.
Note that a given branch structure, such as NP → Det N, can occur more than
once in a given parse tree.

In general, we let P be a set of (all possible) parts. We assume a “counting function”,
c, such that for p ∈ P and hx, yi ∈ Z , c(p, hx, yi) gives the number of times that
the part p occurs in the pair hx, yi (the count of p in hx, yi). For a Mercer kernel
k : P × P → < on P , there is an associated RHKS Hk of functions f : P → <,
where f measures the goodness of a part p. For any f ∈ Hk , we deﬁne a function
Ff on Z as

Ff (x, y) = X
p∈P

c(p, hx, yi)f (p).

(2)

Consider a simple chain example. Let Γ be a set of possible observations and Σ
be a set of possible hidden states. We take the input x to be a sequence x1 , . . . , x`
with xi ∈ Γ and we take Y (x) to be the set of all sequences y1 , . . . , y` with the same
length as x and with yi ∈ Σ. We can take P to be the set of all pairs hs, ¯si plus all
pairs hs, ui with s, ¯s ∈ Σ and u ∈ Γ. Often Σ is taken to be a ﬁnite set of “states”
and Γ = <d is a set of possible feature vectors. k(p, p0 ) is commonly deﬁned as
k(hs, ¯si, hs0 , ¯s0 i) = δ(s, s0 )δ(¯s, ¯s0 ),
k(hs, ui, hs0 , u0 i) = δ(s, s0 )ko (u, u0 ),
where δ(w, w 0 ) denotes the Kronecker-δ . Note that in this example there are two
types of parts — pairs of hidden states and pairs of a hidden state and an observa-
tion. Here we take k(p, p0 ) to be 0 if p and p0 are of diﬀerent types.
In the supervised learning scenario, we are given a sample S of ` pairs (hx1 , y1 i, . . .,
hx` , y ` i) drawn i. i. d.
from an unknown but ﬁxed probability distribution P on
Z . The goal is to learn a function f on the local parts P with small expected loss
EP [L(x, y , f )] where L is a prescribed loss function. This is commonly realized by
learning f that minimizes the regularized loss functional

(3)
(4)

f ∗ = argmin
f ∈Hk

`
X
i=1

L(xi , y i , f ) + λkf k2
k ,

(5)

where k.kk is the norm corresponding to Hk measuring the complexity of f . A vari-
ety of loss functions L have been considered in the literature. In kernel conditional
random ﬁelds (CRFs) [11], the loss function is given by
L(x, y , f ) = −Ff (x, y) + log X
ˆy∈Y (x)

exp(Ff (x, ˆy))

In structured Support Vector Machines (SVM), the loss function is given by

L(x, y , f ) = max
ˆy∈Y (x)

∆(x, y , ˆy) + Ff (x, ˆy) − Ff (x, y),

(6)

where ∆(x, y , ˆy) is some measure of distance between y and ˆy for a given observation
x. A natural choice for ∆ is to take ∆(x, y , ˆy) to be the indicator 1[y 6= ˆy ] [2]. Another
choice is to take ∆(x, y , ˆy) to be the size of the symmetric diﬀerence between the
sets P (hx, yi) and P (hx, ˆyi) [14].

Let P (x) ⊆ P be the set of parts having nonzero count in some pair hx, yi for
y ∈ Y (x). Let P (S ) be the union of all sets P (xi ) for xi in the sample. Then, we
have following straightforward variant of the Representer Theorem [10], which was
also presented in [11].

Deﬁnition: A loss L is local if L(x, y , f ) is determined by the value of f on the
set P (x), i.e., for f , g : P → < we have that if f (p) = g(p) for all p ∈ P (x) then
L(x, y , f ) = L(x, y , g).
Theorem 1. For any local loss function L and sample S there exist weights αp for
p ∈ P (S ) such that f ∗ as deﬁned by (5) can be written as fol lows.
f ∗ (p) = X
p0∈P (S )

αp0 k(p0 , p)

(7)

Thus, even though the set of feasible outputs for x generally scales exponentially
with the size of output, the solution can be represented in terms of the parts of
the sample, which commonly scales polynomially. This is true for any loss function
that partitions into parts, which is the case for loss functions discussed above.

3 A Semi-Supervised Learning Approach to Structured
Variables

In semi-supervised learning, we are given a sample S consisting of l input-output
pairs {(x1 , y1 ), . . . , (x` , y ` )} drawn i. i. d.
from the probability distribution P on
Z and u unlabeled input patterns {x`+1 , . . . , x`+u } drawn i. i. d from the marginal
distribution PX , where usually l < u. Let X (S ) be the set {x1 , . . . , x`+u} and let
Z (S ) be the set of all pairs hx, yi with x ∈ X (S ) and y ∈ Y (x).

If the true classiﬁcation function is smooth wrt the underlying marginal distribution,
one can utilize unlabeled data points to favor functions that are smooth in this sense.
Belkin et. al. [3] implement this assumption by introducing a new regularizer to the
standard RHKS optimization framework (as opposed to introducing a new kernel
as discussed in Section 5)

f ∗ = argmin
f ∈Hk

`
X
i=1

k + λ2 ||f ||2
L(xi , y i , f ) + λ1 ||f ||2
kS

,

(8)

where kS is a kernel representing the intrinsic measure of the marginal distribution.
Sindhwani et. al.[13] prove that the minimizer of (8) is in the span of a new kernel
function (details below) evaluated at labeled data only. Here, we generalize this
framework to structured variables and give a simpliﬁed derivation of the new kernel.

The smoothness assumption in the structured setting states that f should be smooth
on the underlying density on the parts P , thus we enforce f to assign similar
goodness scores to two parts p and p0 , if p and p0 are similar, for all parts of Z (S ).
Let P (S ) be the union of all sets P (z ) for z ∈ Z (S ) and let W be symmetric matrix
where Wp,p0 represents the similarity of p and p0 for p, p0 ∈ P (S ).

f ∗ = argmin
f ∈Hk

= argmin
f ∈Hk

`
X
i=1

`
X
i=1

k + λ2 X
L(xi , y i , f ) + λ1 ||f ||2
p,p0∈P (S )

Wp,p0 (f (p) − f (p0 ))2

k + λ2 f T Lf
L(xi , y i , f ) + λ1 ||f ||2

(9)

Here W is a similarity matrix (like a nearest neighbor graph) and L is the Laplacian
of W , L = D − W , where D is a diagonal matrix deﬁned by Dp,p = Pp0 Wp,p0 . f
denotes the vector of f (p) for all p ∈ P (S ). Note that the last term depends only
on the value of f on the parts in the set P (S ). Then, for any local loss L(x, y , f ),

we immediately have the following Representer Theorem for the semi-supervised
structured case where S includes the labeled and the unlabeled data.
α (p) = X
f ∗
αp0 k(p0 , p)
p0∈P (S )

(10)

Substituting (10) into (9) leads to the following optimization problem

α∗ = argmin
α

`
X
i=1
where Q = λ1K + λ2K LK , K is the matrix of k(p, p0 ) for all p, p0 ∈ P (S ) and fα ,
as a vector in the space Hk , is a linear function of the vector α. Note that (11)
applies to any local loss function and if L(x, y , f ) is convex in f , as in the case for
logistic or hinge loss, then (11) is convex in α.

L(xi , y i , fα ) + αT Qα,

(11)

We now have a loss function over labeled data regularized by the L2 norm (wrt
the inner product Q), for which we can re-evoke the Representer Theorem. Let S `
be the set of labeled inputs {x1 , . . . , x` }, Z (S ` ) be the set of all pairs hx, yi with
x ∈ X (S ` ) and y ∈ Y (x) and P (S ` ) be the set of al parts having nonzero count for
some pair in Z (S ` ). Let δp be a vector whose pth component is 1 and 0 elsewhere.
Using the standard orthogonality argument, let α∗ decompose into two: the vector
in the span of γp = δpKQ−1 for all p ∈ P (S ` ), and the vector in the orthogonal
component (under the inner product Q).
α = X
p∈P (S ` )

βpγp + α⊥

α⊥ can only increase the quadratic term in the optimization problem. Notice that
the ﬁrst term in (11) depends only on fα (p) for p ∈ P (S ` ),
fα (p) = δpK α = (δpKQ−1 )Qα = γpQα.
Since γpQα⊥ = 0, we conclude that the optimal solution to (11) is given by
α∗ = X
p∈P (S ` )

βpγp = βKQ−1 ,

(12)

where β is required to be sparse, such that only parts from the labeled data are
nonzero. Plugging this into original equations we get
˜k(p, p0 ) = kpQ−1kp0
βp ˜k(p, p0 )
fβ (p0 ) = X
p∈P (S ` )
β ∗ = argmin
β
where kp is the vector of k(p, p0 ) for all p0 ∈ P (S ) and ˜K is the matrix of ˜k(p, p0 )
for all p, p0 in P (S ` ). ˜k is the same as in [13].
We call ˜k the graph-regularized kernel, in which unlabeled data points are used to
augment the base kernel k wrt the standard graph kernel to take the underlying
density on parts into account. This kernel is deﬁned over the complete part space,
where as standard graph kernels are restricted to P (S ) only.

L(S ` , fβ ) + β T ˜K β

(14)

(13)

(15)

Given the graph-regularized kernel, the semi-supervised structured learning problem
is reduced to supervised structured learning. Since in semi-supervised learning
problems, in general, labeled data points are far fewer than unlabeled data, the
dimensionality of the optimization problems is greatly reduced by this reduction.

4 Structured Max-Margin Learning

We now investigate optimizing the hinge loss as deﬁned by (6) using graph-
regularized kernel ˜k . Deﬁning γ x,y to be the vector where γ x,y
p = c(p, hx, yi) is
the count of p in hx, yi, the linear discriminant can be written in matrix notation
for x ∈ S ` as

Ffβ (x, y) = β T ˜K γ x,y .
Then, the optimization problem for margin maximization is

β ∗ = argmin
β

min
ξ

ξi ≥ max
ˆy∈Y (xi )

ξi + β T ˜K β

l
X
i=1
4( ˆy , y i ) − β T ˜K (cid:16)γ xi ,yi

− γ xi , ˆy (cid:17)

∀i ≤ l.

This gives a convex quadratic program over the vectors indexed by P (S ), a poly-
nomial size problem in terms of the size of the structures. Following [2], we replace
the convex constraints by linear constraints for all y ∈ Y (x) and using Lagrangian
duality techniques, we get the following dual Quadratic program:
θ∗ = argmin
θT dR θ − ∆T θ
θ
θ(xi ,y) ≥ 0, X
y∈Y (x)

∀y ∈ Y (xi ),

θ(xi ,y) = 1,

∀i ≤ l,

(16)

where ∆ is a vector of 4(y , ˆy) for all y ∈ Y (x) of all labeled observations x, dγ is a
matrix whose (xi , y)th column dγ ., (xi , y) = γ xi ,yi
− γ xi ,y and dR = dγ T ˜K dγ . Due
to the sparse structure of the constraint matrix, even though this is an exponential
sized QP, the algorithm proposed in [2] is proven to solve (16) to η proximity in
polynomial time in P (S l ) and 1
η [15].

5 Semi-Supervised vs Transductive Learning

Since one ma jor contribution of this paper is learning a classiﬁer for structured ob-
jects that is deﬁned over the complete part space P , we now examine the diﬀerences
of semi-supervised and transductive learning in more detail. The most common ap-
proach to realize the smoothness assumption is to construct a data dependent kernel
kS derived from the graph Laplacian on a nearest neighbor graph on the labeled and
unlabeled input patterns in the sample S . Thus, kS is not deﬁned on observations
that are out of the sample. Given kS , one can construct a function ˜f ∗ on S as
`
X
i=1

L(xi , y i , f ) + λ||f ||2
kS

˜f ∗ = argmin
f ∈HkS

(17)

.

It is well known that kernels can be combined linearly to yield new kernels. This
observation in the transductive setting leads to the following optimization problem,
when the kernel of the optimization problem is taken to be a linear combination of
a graph kernel kS and a standard kernel k restricted to P (S ).

¯f ∗ =

argmin
f ∈H(µ1 k+µ2 kS )

`
X
i=1

L(xi , y i , f ) + λ||f ||2
(µ1 k+µ2 kS )

(18)

A structured semi-supervised algorithm based on (18) has been evaluated in [11].
The kernel is (18) is the weighted mean of k and kS , whereas the graph-regularized

kernel, resulting from weighted mean of two regularizers, is the harmonic mean of k
and kS [16]. An important distinction between ¯f ∗ and f ∗ in (8), the optimization
performed in this paper, is that ¯f ∗ is only deﬁned on P (S ) (only on observations
in the training data) while f ∗ is deﬁned on all of P and can be used for novel (out
of sample) inputs x. We note that in general P is inﬁnite. Out-of-sample extension
is already a serious limitation for transductive learning, but it is even more severe
in the structured case where parts of P can be composed of multiple observation
tokens.

6 Experiments

Similarity Graph: We build the similarity matrix W over P (S ) using K-nearest
neighborhood relationship. Wp,p0 is 0 if p and p0 are not in the K-nearest neighbor-
hood of each other or if p and p0 are of diﬀerent types. Otherwise, the similarity is
given by a heat kernel. In our applications, the structure is a simple chain, therefore
the cliques involved single observation label pairs,

Wp,p0 = δ(y(up ), y(u0
p0 ))e

kup−u0
p0 k2
t

,

(19)

where up denotes the observation part of p and y(u) denotes the labeling of u 1 . In
cases where k(p, p0 ) = Wp,p0 = 0 for p, p0 of diﬀerent types, as in our experiments, the
Gram matrix K and the Laplacian L can be presented as block diagonal matrices,
which signiﬁcantly reduces the computational complexity, the computation of Q−1
in particular.

Applications: We performed experiments using a simple chain model for pitch
accent (PA) prediction and OCR. In PA prediction, Y (x) = {0, 1}T with T = |x|
and xt ∈ <31 , ∀t. In OCR, xt ∈ {0, 1}128 and |Σ| = 15.

U:0
65.92
-
65.81
-

U:80
68.83
69.94
70.28
70.72

PA

SVM

STR

U:0
70.34
-
72.15
-

U:80 U:200
71.27
73.68
73.11
72.00
76.37
74.92
75.66
77.45

We ran experiments com-
paring
semi-supervised
structured (referred as STR)
and unstructured (referred
as SVM) max-margin opti-
mization. For both SVM and
STR, we used RBF kernel as
the base kernel ko in (4) and
a 5-nearest neighbor graph
to construct the Laplacian.
We chose the width of the RBF kernel by cross-validation on SVM and used the
same value for STR. Following [3], we ﬁxed λ1 : λ2 ratio at 1 : 9. We report
the average results of experiments with 5 random selection of labeled sequences
in Table 1 and 2, with number of labeled sequences 4 on the left side of Table
1, 40 on the right side, and 10 in Table 2. We varied the number of unlabeled
sequences and reported the per-label accuracy of test sequences (on top of each
cell) and of unlabeled sequences (bottom) (when U > 0). The results in pitch
accent prediction shows the advantage of a sequence model over a non-structured

Table 1: Per-label accuracy for Pitch Accent.

1For more complicated parts, diﬀerent measures can apply. For example, in sequence
classiﬁcation, if the classiﬁer is evaluated wrt the correctly classiﬁed individual labels in
the sequence, W can be s. t. Wp,p0 = Pu∈p,u0∈p0 δ(y(u), y(u0 ))˜s(u, u0 ) where ˜s denotes
some similarity measure such as the heat kernel.
If the evaluation is over segments of
the sequence, the similarity can be Wp,p0 = δ(y(p), y 0 (p0 )) Pu∈p,u0∈p0 ˜s(u, u0 ) where y(p)
denotes all the label nodes in the part p.

model, where STR consistently performs better than SVM. We also observe the
usefulness of unlabeled data both in the structured and unstructured models, where
as U increases, so does the accuracy. The improvement from unlabeled data and
from structured classiﬁcation can be considered as additive. The small diﬀerence
between the accuracy of in-sample unlabeled data and the test data indicates the
natural extension of our framework to new data points.

In OCR, on the other hand, STR does not improve
over SVM. Even though unlabeled data improves ac-
curacy, performing sequence classiﬁcation is not help-
ful due to the sparsity of structural information. Since
|Σ| = 15 and there are only 10 labeled sequences with
average length 8.3, the statistics of label-label depen-
dency is quite noisy.

7 Conclusions

OCR

SVM

STR

U:0
43.62
-
49.25
-

U:412
49.96
47.56
49.91
49.65

Table 2: OCR

We presented a discriminative approach to semi-supervised learning of structured
and inter-dependent response variables.
In this framework, we derived a maxi-
mum margin formulation and presented experiments for a simple chain model. Our
approach naturally extends to the classiﬁcation of unobserved structured inputs
and this is supported by our empirical results which showed similar accuracy on
in-sample unlabeled data and out-of-sample test data.

References

[1] Y. Altun, T. Hofmann, and A. Smola. Gaussian process classiﬁcation for segmenting
and annotating sequences. In ICML, 2004.
[2] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector ma-
chines. In ICML, 2003.
[3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: a geometric frame-
work for learning from examples. Technical Report 06, UChicago CS, 2004.
[4] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-
training. In COLT, 1998.
[5] U. Brefeld, C. B¨uscher, and T. Scheﬀer. Multi-view discriminative sequential learning.
In (ECML), 2005.
[6] O. Chappelle, J. Weston, and B. Scholkopf. Cluster kernels for semi-supervised learn-
ing. In (NIPS), 2002.
[7] M. Collins and N.l Duﬀy. Convolution kernels for natural language. In (NIPS), 2001.
[8] Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux. Eﬃcient non-parametric
function induction in semi-supervised learning. In Proceedings of AISTAT, 2005.
[9] Thorsten Joachims. Transductive inference for text classiﬁcation using support vector
machines. In (ICML), pages 200–209, 1999.
[10] G. Kimeldorf and G. Wahba. Some results on tchebychean spline functions. Journal
of Mathematics Analysis and Applications, 33:82–95, 1971.
[11] John Laﬀerty, Yan Liu, and Xiao jin Zhu. Kernel conditional random ﬁelds: Repre-
sentation, clique selection, and semi-supervised learning. In (ICML), 2004.
[12] K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mitchell. Learning to classify text
from labeled and unlabeled documents. In Proceedings of AAAI-98, pages 792–799,
Madison, US, 1998.
[13] V. Sindhwani, P. Niyogi, and M. Belkin. Beyond the point cloud: from transductive
to semi-supervised learning. In (ICML), 2005.
[14] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2004.
[15] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine
learning for interdependent and structured output spaces. In (ICML), 2004.
[16] T. Zhang. personal communication.


A Domain Decomposition Method for
Fast Manifold Learning

Zhenyue Zhang
Department of Mathematics
Zhejiang University, Yuquan Campus,
Hangzhou, 310027, P. R. China
zyzhang@zju.edu.cn

Hongyuan Zha
Department of Computer Science
Pennsylvania State University
University Park, PA 16802
zha@cse.psu.edu

Abstract

We propose a fast manifold learning algorithm based on the methodol-
ogy of domain decomposition. Starting with the set of sample points
partitioned into two subdomains, we develop the solution of the inter-
face problem that can glue the embeddings on the two subdomains into
an embedding on the whole domain. We provide a detailed analysis to
assess the errors produced by the gluing process using matrix perturba-
tion theory. Numerical examples are given to illustrate the efﬁcienc y and
effectiveness of the proposed methods.

1 Introduction

The setting of manifold learning we consider is the following. We are given a parameter-
ized manifold of dimension d deﬁned by a mapping f : (cid:10) ! Rm , where d < m, and
(cid:10) open and connected in Rd . We assume the manifold is well-behaved, it is smooth and
contains no self-intersections etc. Suppose we have a set of points x1 ; (cid:1) (cid:1) (cid:1) ; xN , sampled
possibly with noise from the manifold, i.e.,

xi = f ((cid:28)i ) + (cid:15)i ;

i = 1; : : : ; N ;

(1.1)

where (cid:15)i ’s represent noise. The goal of manifold learning is to recover the parameters (cid:28) i ’s
and/or the mapping f ((cid:1)) from the sample points xi ’s [2, 6, 9, 12]. The general framework of
manifold learning methods involves imposing a connectivity structure such as a k-nearest-
neighbor graph on the set of sample points and then turn the embedding problem into the
solution of an eigenvalue problem. Usually constructing the graph dominates the computa-
tional cost of a manifold learning algorithm, but for large data sets, the computational cost
of the eigenvalue problem can be substantial as well.

The focus of this paper is to explore the methodology of domain decomposition for de-
veloping fast algorithms for manifold learning. Domain decomposition by now is a well-
established ﬁeld in scientiﬁc computing and has been successfully applied in many science
and engineering ﬁelds
in connection with numerical solutions of partial differential equa-
tions. One class of domain decomposition methods partitions the solution domain into
subdomains, solves the problem on each subdomain and glue the partial solutions on the
subdomains by solving an interface problem [7, 10]. This is the general approach we will

follow in this paper. In particular, in section 3, we consider the case where the given set
of sample points x1 ; : : : ; xN are partitioned into two subdomains. On each of the subdo-
main, we can use a manifold learning method such as LLE [6], LTSA [12] or any other
manifold learning methods to construct an embedding for the subdomain in question. We
will then formulate the interface problem the solution of which will allow us to combine
the embeddings on the two subdomains together to obtain an embedding over the whole
domain. However, it is not always feasible to carry out the procedure described above. In
section 2, we give necessary and sufﬁcient conditions under which the embedding on the
whole domain can be constructed from the embeddings on the subdomains. In section 4,
we analyze the errors produced by the gluing process using matrix perturbation theory. In
section 5, we brieﬂy mention how the partitioning of the set of sample points into subdo-
mains can be accomplished by some graph partitioning algorithms. Section 6 is devoted to
numerical experiments.

NOTAT ION . We use e to denote a column vector of all 1’s the dimension of which should
be clear from the context. N ((cid:1)) and R((cid:1)) denote the null space and range space of a
matrix, respectively. For an index set I = [i1 ; : : : ; ik ], A(:; I ) denotes the submatrix of
A consisting of columns of A with indices in I with a similar deﬁnition for the rows of a
matrix. We use k (cid:1) k to denote the spectral norm of a matrix.

2 A Basic Theorem

Let X = [x1 ; (cid:1) (cid:1) (cid:1) ; xN ] with xi = f ((cid:28)i ) + (cid:15)i ; i = 1; : : : ; N : Assume that the whole sample
domain X is divided into two subdomains X1 = fxi j i 2 I1g and X2 = fxi j i 2 I2g.
Here I1 and I2 denote the index sets such that I1 [ I2 = f1; : : : ; N g and I1 \ I2 is not
empty. Suppose we have obtained the two low-dimensional embeddings T1 and T2 of the
sub-domains X1 and X2 , respectively. The domain decomposition method attempts to
recover the overall embedding T = f(cid:28)1 ; : : : ; (cid:28)N g from the embeddings T1 and T2 on the
subdomains.
In general, the recovered sub-embedding Tj ; j = 1; 2; may not be exactly the subset
f(cid:28)i j i 2 Ij g of T . For example, it is often the case that the recovered embeddings Tj
are approximately afﬁnely equal to f(cid:28)i j i 2 Ij g, i.e., up to certain approximation errors,
there is an afﬁne transformation such that

Tj = fFj (cid:28)i + cj j i 2 Ij g;
where Fj is a nonsingular matrix and cj a column vector. Thus a domain decomposition
method for manifold learning should be invariant to afﬁne transformation on the embed-
dings Tj obtained from subdomains. In that case, we can assume that Tj is just the subset
of T , i.e., Tj = f(cid:28)i j i 2 Ij g. With an abuse of notation, we also denote by T and Tj the
matrices of the column vectors in the set T and Tj , for example, we write T = [(cid:28)1 ; : : : ; (cid:28)N ].
Let (cid:8)j be an orthogonal projection with N ((cid:8)j ) = span([e; T T
j ]). Then Tj can be recov-
ered by computing the eigenvectors of (cid:8)j corresponding to its zero eigenvalues. To recover
the whole T we need to construct a matrix (cid:8) with N ((cid:8)) = span([e; T T ]) [11].
To this end, for each Tj , let (cid:8)j = Qj QT
j 2 RNj (cid:2)Nj , where Qj is an orthonormal basis
matrix of N ([e; T T
j ]T ) and Nj is the column-size of Tj . To construct a (cid:8) matrix, Let
Sj 2 RN (cid:2)Nj be the 0-1 selection matrix deﬁned as Sj = IN (:; Ij ), where IN is the
identity matrix of order N . Let ^(cid:8)j = Sj (cid:8)j S T
j . We then simply take (cid:8) = ^(cid:8)1 + ^(cid:8)2 ;
or more ﬂe xibly, (cid:8) = w1 ^(cid:8)1 + w2 ^(cid:8)2 ; where w1 and w2 are the weights: wi > 0 and
w1 + w2 = 1. Obviously k(cid:8)k (cid:20) 1 since k(cid:8)j k = 1. The following theorem gives the
necessary and sufﬁcient conditions under which the null space of (cid:8) is just spanf[e; T T ]g.
(In the theorem, we only require the (cid:8)j to positive semideﬁnite.)

Theorem 2.1 Let (cid:8)i be two positive semideﬁnite matrices such that N ((cid:8)i ) =
2 ] are of full
1 ] and [e; T T
i ]), i = 1; 2, and T0 = T1 \ T2 . Assume that [e; T T
span([e; T T
0 ] is of full column-rank.
column-rank. Then N ((cid:8)) = span([e; T T ]) if and only if [e; T T

Proof. We ﬁrst
prove the necessity by contradiction. Assume that N ([e; T T
0 ]) 6=
0 ]y = 0 and [e; T T (:; I2 )]y 6= 0. De-
2 ]), then there is y 6= 0 such that [e; T T
N ([e; T T
note by I c
1 the complement of I1 , i.e., the index set of i’s which do not belong to I1 . Then
1 )]y 6= 0. Now we construct a vector x as
[e; T T (:; I c
x(I c
x(I1 ) = [e; T T
1 ) = 0:
1 ]y ;
Clearly x(I2 ) = 0 and hence x 2 N ((cid:8)). By the condition N ((cid:8)) = span([e; T T ]), we can
1 ]z . Note
write x in the form x = [e; T T ]z for a column vector z . Specially, x(I1 ) = [e; T T
that we also have x(I1 ) = [e; T T
1 ]y by deﬁnition.
It implies that z = y because [e; T T
1 ] is
of full rank. Therefor,

[e; T T (:; I c
1 )]y = [e; T T (:; I c
1 )]z = x(I c
1 ) = 0:
Using it together with [e; T T
0 ]y = 0 we have [e; T T (:; I2 )]y = 0, a contradiction.
Now we prove the sufﬁcienc y. Let Q be a basis matrix of N ((cid:8)). we have
w1G1QT ^(cid:8)1Q + w2G2QT ^(cid:8)2Q = QT (cid:8)Q = 0;
which implies (cid:8)iQ(I1 ; :) = 0, i = 1; 2, because ^(cid:8)i is positive semideﬁnite. So
Q(Ii ; :) = [e; T T
i ]Gi ;
i = 1; 2:
Taking the overlap part Q(I0 ; :) of Q with the different representations
Q(I0 ; :) = [e; Ti (:; I0 )T ]Gi = [e; T T
0 ]Gi ;
0 ] is of full column rank,
0 ](G1 (cid:0) G2 ) = 0: So G1 = G2 because [e; T T
we obtain [e; T T
giving rise to Q = [e; T T ]G1 , i.e., N ((cid:8)) (cid:26) span([e; T T ]). It follows together with the
obvious result span([e; T T ]) (cid:26) N ((cid:8)) that N ((cid:8)) = span([e; T T ]).
0 ] is of
The above result states that when the overlapping is large enough such that [e; T T
full column-rank (which is generically true when T0 contains d + 1 points or more), the
embedding over the whole domain can be recovered from the embeddings over the two
subdomains. However, to follow Theorem 2.1, it seems that we will need to compute
the null space of (cid:8). In the next section, we will show this can done much cheaply by
considering an interface problem which is of much smaller dimension.

(2.2)

3 Computing the Null Space of (cid:8)

In this section, we formulate the interface problem and show how to solve it to glue the
embeddings from the two subdomains to obtain an embedding over the whole domain. To
simplify notations, we re-denote by T (cid:3) the actual embedding over the whole domain and
j the subsets of T (cid:3) corresponding to subdomains. We then use Tj to denote afﬁnely
T (cid:3)
transformed versions of T (cid:3)
j obtained by LTSA for example, i.e., T (cid:3)
j = cj eT + Fj Tj : Here
cj is a constant column vector in Rd and Fj is a nonsingular matrix. Denote by T0j the
overlapping part of Tj corresponding to I0 = I1 \ I2 as in the proof of Theorem 2.1. We
consider the overlapping parts T (cid:3)
0j of T (cid:3)
j ,
c1 eT + F1T01 = T (cid:3)
02 = c2 eT + F2T02 :
01 = T (cid:3)

(3.3)

Or equivalently,

02 ]i (cid:20) (c1 ; F1 )T
(c2 ; F2 )T (cid:21) = 0:
h[e; T T
01 ]; (cid:0)[e; T T

Therefore, if we take an orthonormal basis G of the null space of h[e; T T
02 ]i
01 ]; (cid:0)[e; T T
02 ]G2 : Let Aj =
and partition G = [GT
2 ]T conformally, then [e; T T
1 ; GT
01 ]G1 = [e; T T
j = 1; 2: Deﬁne the matrix A such that A(:; Ij ) = Aj . Then since (cid:8)iAT
GT
j [e; T T
j ]T ;
i =
0, the well-deﬁned matrix AT is a basis of N ((cid:8)),
1 + S2(cid:8)2AT
2 AT = S1(cid:8)1AT
1 AT + S2(cid:8)2S T
(cid:8)AT = S1(cid:8)1S T
2 = 0:
Therefore, we can use AT to recover the global embedding T .
A simpler alternative way is use a one-sided afﬁne transformation, i.e., ﬁx one of T i and
afﬁnely transform the other; the afﬁne matrix is obtained by ﬁxing one of ~T0i and trans-
forming the other. For example, we can determine c and F such that

(3.4)
T01 = ceT + F T02 ;
and transform T2 to ^T2 = ceT + F T2 . Clearly, for the overlapping part, ^T02 = T01 . Then
we can construct a larger matrix T by T (:; I1 ) = T1 , T (:; I2 ) = ceT + F T2 : One can also
readily verify that T T is a basis matrix of N ((cid:8)).
In the noisy case, a least squares formulation will be needed. For example, for the simul-
2 ]T to be an orthonormal matrix in
taneous afﬁne
transformation, we take G = [GT
1 ; GT
R2(d+1)(cid:2)(d+1) such that

01 ]G1 (cid:0) [e; T T
k[e; T T
02 ]G2 k = min :
It is known that the minimum G is given by the right singular vector matrix correspond-
ing to the d + 1 smallest singular values of W = h[e; T T
02 ]i, and the residual
01 ]; (cid:0)[e; T T
02 ]G2(cid:13)(cid:13) = (cid:27)d+2 (W ). For the one-side approach (3.4), [c; F ] can be a
(cid:13)(cid:13)[e; T T
01 ]G1 (cid:0) [e; T T
solution to the least squares problem
F (cid:13)(cid:13)(T01 (cid:0) t01 eT ) (cid:0) F (T02 (cid:0) t02 eT )(cid:13)(cid:13) ;
c; F (cid:13)(cid:13)T01 (cid:0) (cid:0)ceT + F T02 (cid:1)(cid:13)(cid:13) = min
min
where t0j is the column mean of T0j . The minimum is achieved at F = (T01(cid:0)t01 eT )(T02(cid:0)
t02 eT )+ ; c = t01 (cid:0) F t02 : Clearly, the residual now reads as
(T01 (cid:0) t01 eT )(cid:16)I (cid:0) (T02 (cid:0) t02 eT )+ (T02 (cid:0) t02 eT )(cid:17)(cid:13)(cid:13)(cid:13)
c; F (cid:13)(cid:13)T01 (cid:0) (cid:0)ceT + F T02 (cid:1)(cid:13)(cid:13) = (cid:13)(cid:13)(cid:13)
min
Notice that the overlapping parts in the two afﬁnely transformed subsets are not exactly
equal to each other in the noisy case. There are several possible choices for setting A(:; I0 )
or ^T (:; I0 ). For example, one choice is to set T (:; I0 ) by a convex combination of T0j ’s,
T (:; I0 ) = (cid:11)T01 + (1 (cid:0) (cid:11)) ^T02 :

:

with (cid:11) = 1=2 for example.
We summarize discussions above in the following two algorithms for gluing the two sub-
domains T1 and T2 .
Algorithm I. [Simultaneously afﬁne transformation]

1. Compute the right singular vector matrix G corresponding to the d + 1 smallest
singular values of h[e; T T
02 ]i.
01 ]; (cid:0)[e; T T
2. Partition G = [GT
2 ]T and set Ai = GT
i ]T , i = 1; 2, and
1 ; GT
i [e; T T
A(:; I1 nI0 ) = A11 ; A(:; I0 ) = (cid:11)A01 + (1 (cid:0) (cid:11))A02 ; A(:; I2 nI0 ) = A12 ;
where A0j is the overlap part of Aj and A1j is the Aj with A0j deleted.

3 Compute the column mean a of A, and an orthogonal basis U of N (aT ).
4. Set T = U T A.

Algorithm II. [One-side afﬁne transformation]

1. Compute the least squares problem minW kT01 (cid:0) W [e; T T
02 ]T kF .
2. Afﬁnely transform T2 to ^T2 = W [e; T T
2 ]T .
3. Set the global coordinate matrix T by
T (:; I1 nI0 ) = T11 ; T (:; I0 ) = (cid:11)T01 + (1 (cid:0) (cid:11)) ^T02 ; T (:; I2 nI0 ) = ^T12 :

4 Error Analysis

As we mentioned before, the computation of Tj ; j = 1; 2 using a manifold learning algo-
rithm such as LTSA involves errors. In this section, we assess the impact of those errors on
the accuracy of the gluing process. Two issues are considered for the error analysis. One is
the perturbation analysis of N ((cid:8)(cid:3) ) when the computation of (cid:8)(cid:3)
i is subject to error. In this
case, N ((cid:8)(cid:3) ) will be approximated by the smallest (d + 1)-dimensional eigenspace V of an
approximation (cid:8) (cid:25) (cid:8)(cid:3) (Theorem 4.1). The other issue is the error estimation of V when
a basis matrix of V is approximately constructed by afﬁnely transformed local embeddings
as described in section 3 (Theorem 4.2). Because of space limit, we will not present the
details of the proofs of the results.

The distance of two linear subspaces X and Y are deﬁned by dist(X ; Y ) = kPX (cid:0) PY k;
where PX and PY are the orthogonal projection onto X and Y , respectively. Let
i k; where (cid:8)(cid:3)
i and (cid:8)i are the orthogonal projectors onto the range spaces
(cid:15)i = k(cid:8)i (cid:0) (cid:8)(cid:3)
i )T ]) and span([e; (Ti )T ]), respectively. Clearly, if (cid:8)(cid:3) = w1(cid:8)(cid:3)
2 and
span([e; (T (cid:3)
1 + w2(cid:8)(cid:3)
(cid:8) = w1(cid:8)1 + w2(cid:8)2 , then
dist(cid:16)span([e; (T (cid:3) )T ]); span([e; T T ])(cid:17) = k(cid:8) (cid:0) (cid:8)(cid:3) k (cid:20) w1 (cid:15)1 + w2 (cid:15)2 (cid:17) (cid:15):
Theorem 4.1 Let (cid:27) be the smallest nonzero eigenvalue of (cid:8)(cid:3) and V the subspace spanned
by the eigenvectors of (cid:8) corresponding to the d + 1 smallest eigenvalues. If (cid:15) < (cid:27)=4, and
4(cid:15)2 (k(cid:8)(cid:3) k (cid:0) (cid:27) + 2(cid:15)) < ((cid:27) (cid:0) 2(cid:15))3 , then

dist(V ; N ((cid:8)(cid:3) )) (cid:20)

(cid:15)
p((cid:27)=2 (cid:0) (cid:15))2 + (cid:15)2
Theorem 4.2 Let (cid:27) and (cid:15) be deﬁned in Theorem 4.1. A is the matrix computed by the
simultaneous afﬁne transformation (Algorithm I in section 3) Let (cid:27)i ((cid:1)) be the i-th smallest
singular value of a matrix. Denote
1
01 ]; (cid:0)[e; T T
(cid:27)d+2 ((cid:2)[e; T T
02 ](cid:3));
2

(cid:22)
(cid:27)min (A)

(cid:22) =

(cid:17) =

:

:

If (cid:15) < (cid:27)=4, then

dist(V ; span(A)) (cid:20)

1
(cid:27)d+2 ((cid:8)) (cid:16)(cid:17) +
From Theorems 4.1 and 4.2 we conclude directly that
1
(cid:15)(cid:27)=2
((cid:27)=2 (cid:0) (cid:15))2 (cid:17) +
(cid:27)d+2 ((cid:8)) (cid:16)(cid:17) +

dist(span(A); N ((cid:8)(cid:3) )) (cid:20)

(cid:15)(cid:27)=2
((cid:27)=2 (cid:0) (cid:15))2 (cid:17)

2(cid:15)
p((cid:27) (cid:0) 2(cid:15))2 + 4(cid:15)2

:

5 Partitioning the Domains

To apply the domain decomposition methods, we need to partition the given set of data
points into several domains making use of the k nearest neighbor graph imposed on the
data points. This reduces the problem to a graph partition problem and many techniques
such as spectral graph partitioning and METIS [3, 5] can be used. In our experiments, we
have used a particularly simple approach: we use the reverse Cuthill-McKee method [4]
to order the vertices of the k-NN graph and then partition the vertices into domains (for
details see Test 2 in the next section).

Once we have partitioned the whole domain into multiple overlapping subdomains we can
use the following two approaches to glue them together.
Successive gluing. Here we glue the subdomains one by one as follows.
Initially set
T (1) = T1 and I (1) = I1 , and then glue the patch Tk to T (k(cid:0)1) and obtain the larger one
T (k) for k = 2; : : : ; K , and so on. The index set of T (k) is given by I (k) = I (k(cid:0)1) [ Ik .
Clearly the overlapping set of T (k(cid:0)1) and Tk is I (k)
0 = I (k(cid:0)1) \ Ik .
Recursive gluing. Here at the leaf level, we divide the subdomains into several pairs,
2i ), 1 = 1; 2; : : :. Then glue each pair to be a larger subdomain T (1)
say (T (0)
2i(cid:0)1 ; T (0)
and
i
continue. The recursive gluing method is obviously parallelizable.

6 Numerical Experiments

In this section we report numerical experiments for the proposed domain decomposition
methods for manifold learning. This efﬁcienc y and effectiveness of the methods clearly
depend on the accuracy of the computed embeddings for subdomains, the sizes of the
subdomains, and the sizes of the overlaps of the subdomains.
Test 1. Our ﬁrst

test data set is sampled from a Swiss-roll as follows

(6.5)
xi = [ti cos(ti ); hi ; ti sin(ti )]T ;
i = 1; : : : ; N = 2000;
2 ; 9(cid:25)
where ti and hi are uniformly randomly chosen in the intervals [ 3(cid:25)
2 ] and [0; 21], re-
spectively. Let (cid:28)i be the arc length of the corresponding spiral curve [t cos(t); t sin(t)]T
from t0 = 3(cid:25)
2 to ti . (cid:28)max = maxi (cid:28)i . To compare the CPU time of the domain decompo-
sition methods, we simply partition the (cid:28) -interval [0; (cid:28)max ] into k(cid:28) subintervals (ai(cid:0)1 ; ai ]
with equal length and also partition the h-interval into kh subintervals (bj(cid:0)1 ; bj ]. Let
Dij = (ai(cid:0)1 ; ai ] (cid:2) (bj(cid:0)1 ; bj ] and Sij (r) be the balls centered at (ai ; bj ) with radius r . We
set the subdomains as

Xij = fxk j ((cid:28)k ; hk ) 2 Dij [ Sij (r)g:

Clearly r determines the size of overlapping parts of Xij with Xi+1;j ; Xi;j+1 ; Xi+1;j+1 .
The submatrices Xij are ordered as X1;1 ; X1;2 ; : : : ; X1;kh ; X2;1 ; : : : and denoted as Xk ,
k = 1; : : : ; K = k(cid:28) kh . We ﬁrst
compute the K local 2-D embeddings T1 ; : : : ; TK by
applying LTSA on the sample data sets Xk for the subdomains. Then those local coordinate
embeddings Tk are aligned by the successive one-sided afﬁne transformation algorithm by
adding subdomain Tk one by one.
Table 1 lists the total CPU time for the successive domain decomposition algorithm, in-
cluding the time for computing the embeddings fTk g for the subdomains, for different
parameters k(cid:28) and kh with the parameter r = 5. In Table 2, we list the CPU time for the
recursive gluing approach taking into account the parallel procedure. As a comparison, the
CPU time of LTSA applying to the whole data points is 6:23 seconds.

Table 1: CPU Time (seconds) of the successive domain decomposition algorithm.
6
4
3
kh=2
5
1.64
1.61
1.64
1.70
1.89
1.77
1.70
1.61
1.67
167
1.66
1.59
1.67
1.78
1.86
2.09
1.89
1.75
1.66
163
2.23
2.02
1.84
1.70
1.59
2.44
2.22
1.94
1.80
1.58
2.66
2.31
2.06
1.83
1.63
1.63
1.86
2.38
2.56
2.94

k(cid:28) = 3
4
5
6
7
8
9
10

Table 2: CPU Time (seconds) of the parallel recursive domain decomposition.
kh=2
3
4
5
6
017
0.19
0.27
0.34
0.52
0.13
0.17
0.20
0.23
0.53
0.14
0.17
0.19
0.17
0.31
0.14
0.13
0.16
0.19
0.25
0.20
0.16
0.14
0.14
0.11
0.14
0.14
0.16
0.17
0.20
0.14
0.14
0.14
0.16
0.19
0.19
0.16
0.17
0.19
0.13

k(cid:28) = 3
4
5
6
7
8
9
10

Test 2. The symmetric reverse Cuthill-McKee permutation (symrcm) is an algorithm for
ordering the rows and columns of a symmetric sparse matrix [4]. It tends to move the
nonzero elements of the sparse matrix towards the main diagonals of the matrix. We use
Matlab’s symrcm to the adjacency matrix of the k-nearest-neighbor graph of the data points
to reorder them. Denote by X the reordered data set. We then partition the whole sample
points into K = 16 subsets Xi = X (:; si
: ei ) with si = maxf1; (i (cid:0) 1)m (cid:0) 20g,
ei = minfim + 20; N g, and m = N=K = 125.
It is known that the t-h parameters in (6.5) represent an isometric parametrization of the
swiss-roll surface. We have shown that within the errors made in computing the local
embeddings, LTSA can recover the isometric parametrization up to an afﬁne transformation
[11]. We denote by ~T (k) = ceT + F T (k) the optimal approximation to T (cid:3) (:; I (k) ) within
afﬁne transformations,
kT (cid:3) (:; I (k) ) (cid:0) ~T (k) kF = min
c;F
We denote by (cid:17)k the average of relative errors
kT (cid:3) (:; i) (cid:0) ~T (k) (:; i)k2
kT (cid:3) (:; i)k2

1
jI (k) j Xi2I (k)
In the left panel of Figure 1 we plot the initial embedding errors for the subdomains (blue
bar), the error of LTSA applied to the whole data set (red bar), and the errors (cid:17)k of the
successive gluing (red line). The successive gluing method gives an embedding with an
acceptable accuracy comparing with the accuracy obtained by applying LTSA to the whole
data set. As shown in the error analysis, the errors in successive gluing will increase when
the initial errors for the subdomains increase. To show it more clearly, we also plot the (cid:17)k
for the recursive gluing method in the right panel of Figure 1.
Acknowledgment.
author was supported in part by by NSFC
The work of ﬁrst
(project 60372033), the Special Funds for Major State Basic Research Projects (project

kT (cid:3) (:; I (k) ) (cid:0) (ceT + F T (k) )kF :

(cid:17)k =

:

x 10−3

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

successive alignment
subdomains
whole domain

2

4

6

8

10

12

14

16

18

k

x 10−3

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

root 4
root 3
root 2
root 1
subdomains
whole domain

2

4

6

8

10

12

14

16

18

k

Figure 1: Relative errors for the successive (left) and recursive (right) approaches.

G19990328), and NSF grant CCF-0305879. The work of second author was supported in
part by NSF grants DMS-0311800 and CCF-0430349.

References

[1] M. Brand. Charting a manifold. Advances in Neural Information Processing Systems
15, MIT Press, 2003.
[2] D. Donoho and C. Grimes. Hessian Eigenmaps: new tools for nonlinear dimension-
ality reduction. Proceedings of National Academy of Science, 5591-5596, 2003.
[3] M. Fiedler. A property of eigenvectors of nonnegative symmetric matrices and its
application to graph theory. Czech. Math. J. 25:619–637, 1975.
[4] A. George and J. W. Liu. Computer Solution of Large Sparse Positive Deﬁnite Matri-
ces. Prentice Hall, 1981.
[5] METIS. http://www-users.cs.umn.edu/(cid:24)karypis/metis/.
[6] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. Science, 290: 2323–2326, 2000.
[7] B. Smith, P. Bjorstad and W. Gropp Domain Decomposition, Parallel Multilevel
Methods for Elliptic Partial Differential Equations. Cambridge University Press,
1996.
[8] G.W. Stewart and J.G. Sun. Matrix Perturbation Theory. Academic Press, New York,
1990.
[9] J. Tenenbaum, V. De Silva and J. Langford. A global geometric framework for non-
linear dimension reduction. Science, 290:2319 –2323, 2000.
[10] A. Toselli and O. Widlund. Domain Decomposition Methods - Algorithms and The-
ory. Springer, 2004.
[11] H. Zha and Z. Zhang. Spectral analysis of alignment in manifold learning. Proceed-
ings of IEEE International Conference on Acoustics, Speech, and Signal Processing,
(ICASSP), 2005.
[12] Z. Zhang and H. Zha. Principal manifolds and nonlinear dimensionality reduction via
tangent space alignment. SIAM J. Scienti ﬁc Computing. 26:313-338, 2005.


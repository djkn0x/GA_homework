Efﬁcient Unsupervised Learning for Localization
and Detection in Object Categories

Nicolas Loeff, Himanshu Arora
ECE Department
University of Illinois at
Urbana-Champaign
{loeff,harora1}@uiuc.edu

Alexander Sorokin, David Forsyth
Computer Science Department
University of Illinois at
Urbana-Champaign
{sorokin2,daf}@uiuc.edu

Abstract

We describe a novel method for learning templates for recognition and
localization of objects drawn from categories. A generative model repre-
sents the conﬁguration of multiple object parts with respec t to an object
coordinate system; these parts in turn generate image features. The com-
plexity of the model in the number of features is low, meaning our model
is much more efﬁcient to train than comparative methods. Mor eover,
a variational approximation is introduced that allows learning to be or-
ders of magnitude faster than previous approaches while incorporating
many more features. This results in both accuracy and localization im-
provements. Our model has been carefully tested on standard datasets;
we compare with a number of recent template models. In particular, we
demonstrate state-of-the-art results for detection and localization.

1

Introduction

Building appropriate object models is central to object recognition, which is a fundamental
problem in computer vision. Desirable characteristics of a model include good represen-
tation of objects, fast and efﬁcient learning algorithms th at require as little supervised in-
formation as possible. We believe an appropriate representation of an object should allow
for both detection of its presence and localization (‘where is it?’). So far the quality of
object recognition in the literature has been measured by its detection performance only.
Viola and Jones [1] present a fast object detection system boosting Haar ﬁlter responses.
Another effective discriminative approach is that of a bag of keypoints [2, 3]. It is based on
clustering image patches using appearance only, disregarding geometric information. The
performance for detection in this algorithm is among the state of the art. However as no
geometry cues are used during training, features that do not belong to the object can be
incorporated into the object model. This is similar to classic over ﬁtting and typically leads
to problems in object localization.

Weber et. al. [4] represent an object as a constellation of parts. Fergus et. al. [5] extend
the model to account for variability in appearance. The model encodes a template as a
set of feature-generating parts. Each part generates at most one feature. As a result the
complexity is determined by hardness of part-feature assignment. Heuristic search is used
to approximate the solution, but feasible problems are limited to 7 parts with 30 features.

Agarwal and Roth [6] learn using SNoW a classi ﬁer on a sparse r epresentation of patches
extracted around interesting points in the image. In [7], Leibe and Schiele use a voting
scheme to predict object conﬁguration from locations of ind ividual patches. Both ap-
proaches provide localization, but require manually localizing the objects in training im-
ages. Hillel et. al. [8] independently proposed an approach similar to ours. Their model
however has higher learning complexity and inferior detection performance despite being
of discriminative nature.

In this paper, we present a generative probabilistic model for detection and localization of
objects that can be efﬁciently learnt with minimal supervis ion. The ﬁrst crucial property
of the model is that it represents the conﬁguration of multip le object parts with respect to
an unobserved, abstract object root (unlike [9, 10], where an “object root ” is chosen
as
one of the visible parts of the object). This simpli ﬁes local
ization and allows our model to
overcome occlusion and errors in feature extraction. The model also becomes symmetric
with respect to visible parts. The second crucial assumption of the model is that a single
part can generate multiple features in the image (or none). This may seem counterintuitive,
but keypoint detectors generally detects several features around interesting areas. This
hypothesis also makes an explicit model for part occlusion unnecessary: instead occlusion
of a part means implicitly that no feature in the image is produced by it.

These assumptions allow us to model all features in the image as being emitted indepen-
dently conditioned on the object center. As a result the complexity of inference in our
model is linear in the number of parts of the model and the number of features in the im-
age, obviating the exponential complexity of combinatoric assignments in other approaches
[4, 5, 11]. This means our model is much easier than constellation models to train using
Expectation Maximization (EM), which enables the use of more features and more com-
plex models with resulting improvements in both accuracy and localization. Furthermore
we introduce a variational (mean- ﬁeld) approximation duri ng learning that allows it to be
hundreds of times faster than previous approaches, with no substantial loss of accuracy.

2 Model

Our model of an object category is a template that generates features in the image. Each
image is represented as a set {fj } of F features extracted with the scale-saliency point
detector [13]. Each feature is described by its location and appearance. Feature ex-
traction and representation will be detailed in section 3. As described in the introduc-
tion, we hypothesize that given the object center all features are generated independently:
P (oc ) Qj p(fj |oc ). The abstract object center - which does not
pobj (f1 , .., fF ) = Poc
generate any features - is represented by a hidden random variable oc . For simplicity it
takes values in a discrete grid of size Nx × Ny inside the image and oc is assumed to be a
priori uniformly distributed in its domain.

Conditioned on the object center, each feature is generated by a mixture of P parts plus a
background part. A set of hidden variables {ωij } represents which part (i) produced feature
fj . These variables ωij then take values {0, 1} restricted to PP +1
i=1 ωij = 1. In other words,
ωij = 1 means feature j was produced by part i; each part can produce multiple features,
each feature is produced by only one part. The distribution of a feature conditioned on the
object center is then p(fj |oc ) = Pi p(fj , wij = 1|oc ) = Pi p(fj |wij = 1, oc )πi , where
πi is the prior emission probability of part i. πi is subject to PP +1
i=1 πi = 1.
Each part has a location distribution with respect to the object center corresponding to a two
L (x|oc ). The appearance (see section 3 for details)
dimensional full covariance Gaussian, pi
of a part does not depend on the conﬁguration of the object; we consider two models :

Gaussian Model (G) Appearance pi
A is modeled as a k dimensional diagonal covariance
Gaussian distribution.
Local Topic Model (LT) Appearance pi
A is modeled as a multinomial distribution on a
previously learnt k-word image patch dictionary. This can be considered as a
local topic model.

P obj
θ

Let θ denote the set of parameters. The complete data likelihood (joint distribution) for
image n in the object model is then,
[oc=o′
c ]
c )

A (fj )πi(cid:9)[ωij =1]
Yj,i (cid:8)pi
({ωij }, oc , {fj }) = Yo′
c )pi
P (o′
L (fj |o′


c
where [expr] is one if expr is true and zero otherwise. Marginalizing, the probability of
the observed image in the object model is then,
P (oc ) Yj ′ (Xi
P (fj ′ , ωij ′ = 1|oc ))
({fj }) = Xoc
The background model assumes all features are produced independently, with uniform lo-
cation on the image.
In the G model of appearance, the appearance is modeled with a
k dimensional full covariance matrix Gaussian distribution. In the LT model, we use a
multinomial distribution on the k-word image patch dictionary to model the appearance.

P obj
θ

(2)

(1)

2.1 Learning

The maximum-likelihood solution for the parameters of the above model does not have a
closed form. In order to train the model the parameters are computed numerically using the
approach of [14], minimizing a free-energy Fe associated with the model that is an upper
bound on the negative log-likelihood. Following [14], we denote v = {fj } as the set of
visible and h = {oc , ωij } as the set of hidden variables. Let DKL be the K-L divergence:
Fe (Q, θ) = DKL(cid:8)Q(h)(cid:12)(cid:12)(cid:12)(cid:12)Pθ (h|v)(cid:9) − log Pθ (v) = Zh
In this bound, Q(h) can be a simpler approximation of the posterior probability Pθ (h|v),
that is used to compute estimates and update parameters. Minimizing eq. 3 with respect to
Q and θ under different restrictions, produces a range of algorithms including exact EM,
variational learning and others [14]. Table 2.1 shows sample updates and complexity of
these algorithms and comparison to other relevant work.

Q(h)
Pθ (h, v)

Q(h) log

(3)

dh

The background model is learnt before the object model is trained. As assumed earlier, for
Gaussian appearance model the background appearance model is a single gaussian, whose
mean and variance are estimated as the sample mean and covariance. For the Local Topic
model, the multinomial distribution is estimated as the sample histogram. The model for
background feature location is uniform and does not have any parameters.
EM Learning for the Object model: In the E-step, the set of parameters θ is ﬁxed and
Fe is minimized with respect to Q(h) without restrictions. This is equivalent to com-
puting the actual posteriors in EM [14, 15]. In this case the optimal solution factorizes
as Q(h) = Q(oc )Q(ωij |oc ) = P (oc |v)P (ωij |oc , v). In the M-step, Fe is minimized with
respect to the parameters θ using the current estimate of Q. Due to the conditional indepen-
dence introduced in the model, inference is tractable and thus the E-step can be computed
efﬁciently. The overall complexity of inference is O(F P · NxNy ).

Model
Fergus et al.

Model (EM)

µi
L ←

(Variational)

µi
L ←

Update for µi
L
N/A
j
Q(oc ) Pj Q(ωj i |oc ){x
L−oc }
Pn Poc
Q(oc ) Pj Q(ωj i |oc )
Pn Poc
j
Q(oc )oc }
Pn {Pj Q(ωj i )x
L−Poc
Q(oc ) Pj Q(ωj i )
Pn Poc

Complexity
F P

F P · NxNy

Time (F,P)
36 hrs (30, 7)

3 hrs (50, 30)

F P + NxNy

3 mins (100, 30)

Table 1: An example of an update, overall complexity and convergence time for our models and [5],
for different number of features per image (F ) and number of parts in the object model (P ). There is
an increase in speed of several orders of magnitude with respect to [5] on similar hardware.
Variational Learning: In this approach a mean ﬁeld approximation of Q is considered;
in the E-step the parameters θ are ﬁxed and F is minimized with respect to Q under the
restriction that it factorizes as Q(h) = Q(oc )Q(wij ). This corresponds to a decoupling of
location (oc ) and part-feature assignment (wij ) in the approximation (Q) of the posterior
Pθ (h|v). In the M-step θ is ﬁxed and the free energy Fe is minimized with respect to this
(mean ﬁeld) version of Q. A comparison between EM and Variational updates of the mean
in location µi
L of a part is shown in table 2.1. The overall complexity of inference is now
O(F P ) + O(NxNy ); this represents orders of magnitude of speedup with respect to the
already efﬁcient EM learning. The impact on performance of t he variational approximation
is discussed in section 4.

2.2 Detection and localization

For detection of object presence, a natural decision rule is the likelihood ratio test. After the
models are learnt, for each test image P obj
({fj })/P bg ({fj }) is compared to a threshold to
θ
make the decision. Once the presence of the object is established, the most likely location
is given by the MAP estimate of oc . We assign parts in the model to the object if they ex-
hibit consistent appearance and location. To remove model parts representing background
we use a threshold on the entropy of the appearance distribution for the LT model (the
determinant of the covariance in location for the G model). The MAP estimate of which
features in the image are assigned (marginalizing over the object center) to parts in the
model determines the support of the object. Bounding boxes include all keypoints assigned
to the object and means of all model parts belonging to the object even if no keypoint is
observed to be produced by such part. This explicitly handles occlusion ( ﬁg. 1).

3 Experimental setup

The performance of the method depends on the feature detector making consistent extrac-
tion in different instances of objects of the same type. We use the scale-saliency interest
point detector proposed in [13]. This method selects regions exhibiting unpredictable char-
acteristics over both location and scale. The F regions with highest saliency over the image
provide the features for learning and recognition. After the keypoints are detected, patches
are extracted around this points and scale-normalized. A SIFT descriptor [16] (without
orientation) is obtained from these patches. For model G, due to the high dimensionality
of resulting space, PCA is performed choosing k = 15 components to represent the ap-
pearance of a feature. For model LT, we instead cluster the appearance of features in the
original SIFT space with a gaussian mixture model with k = 250 components and use the
most likely cluster as feature appearance representation.

For all experiments we use P = 30 parts. The number of features is F = 50 for G model
and F = 100 for LT model, Nx × Ny = 238. We test our approach on the Caltech 5
dataset: faces, motorbikes, airplanes, spotted cats vs. Caltech background and cars rear
2001 vs. cars background [5]. We initialize appearance and location of the parts with P
randomly chosen features from the training set. The stopping criterion is the change in Fe .

Figure 1: Local Topic model for faces, motorbikes and airplanes datasets [5]. In (a) the most likely
location of the object center is plotted as a black circle. With respect to this reference, the spatial
In (b) the
distribution (2D gaussian) of each part associated with the object is plotted in green.
centers of all features extracted are depicted. Blue ones are assigned by the model to the object, and
red ones to the background. The bounding box is plotted in blue. Image (c) shows how many features
in the image are assigned to the same part (a property of our model, not shared by [5]): six parts are
chosen, their spatial distribution is plotted (green), and the features assigned to them are depicted in
blue. Eyes (4,5), mouth (3) and left ear (6) have multiple assignments each. For each these parts,
image (d) image shows the best matches in features extracted from the dataset. Note that the local
topic model can learn parts uniform in appearance (i.e. eyes) but also more complex parts (i.e. the
mouth part includes moustaches, beards and chins). The G appearance model and [5] do not have
this property. The images (e) show the robustness of the method in cases with occlusion, missed
detections and one caricature of a face. Images (f) and (g) show plots for motorbikes, and (h) and (i)
for airplanes.

4 Results

Detection: Although we believe that localization is an essential performance criterion, it is
useless if the approach cannot detect objects. Figure 2 depicts equal error rate detection per-
formance for our models and [5, 3, 8]. We can not compare our range of performance (for
train/test splits), shown on the plot, because this data is not available for other approaches.
Our method is robust to initialization (the variance for starting points is negligible com-
pared to train/test split variance). The results show higher detection performance of all our
algorithms compared to the generative model presented in [5]. The local topic (LT) model
performs better than the model presented in [8]. The purely discriminative approach pre-
sented in [3] shows higher detection performance with different ( “optimal combination”)
features, but performs worse for the features we are using. The LT model showed con-
sistently higher detection performance than the Gaussian (G) model. For both LT and G
models the variational approximations showed similar discriminative power to that of the
respective exact models. Unlike [5, 3], our model currently is not scale invariant. Never-
theless the probabilistic nature of the model allows for some tolerance to scale changes.

In datasets of manageable size, it is inevitable that the background is correlated with the
object. The result is that most modern methods that infer the template form partially su-
pervised data can tend to model some background parts as lying on the object (see ﬁgure
4). Doing so tends to increase detection performance. It is reasonable to expect this in-
crease will not persist in the face of a dramatic change in background. One symptom of
this phenomenon (as in classical over ﬁtting) is that method s that detect very well may be
bad at localization, because they cannot separate the object from background. We are able
to avoid this difﬁculty by predicting object extent conditi oned on detection using only a
subset of parts known to have relatively low variance in location or appearance, given the
object center. We do not yet have an estimate of the increase in detection rate resulting
from over ﬁtting . This is a topic of ongoing research. In our opinion, if a method can detect
but performs poorly at localization, the reason may be over ﬁ tting.
Localization: Previous work on localization required aligned images (bounding boxes)
or segmentation masks [7, 6]. A novel property of our model is that it learns to localize
the object and determine its spatial extent without supervision. Figure 1 shows learned
models and examples of localization. There is no standard measure to evaluate localization
performance in an unsupervised setting. In such a case, the object center can be learnt at
any position in the image, provided that this position is consistent across all images. We
thus use as our performance measure, the standard deviation of estimated object centers
and bounding boxes (obtained as in §2.2), after normalizing the estimates of each image to
a coordinate system in which the ground truth bounding box is a unit square (0, 0) − (1, 1).
As a baseline we use the recti ﬁed center of the image. All obje cts of interest in both
airplane and motorbike datasets are centered in the image. As a result the baseline is a
good predictor of the object center and is hard to beat. However in the faces dataset there is
much more variation in location; then the advantage of our approach becomes clear. Figure
3 shows the scatterplot of normalized object centers and bounding boxes. The table in
ﬁgure 2 shows the localization performance results using th e proposed metric.
Variational approximation comparison: Unusually for a variational approximation it is
possible to compare it to the exact model; the results are excellent especially for the G
model. This is consistent with our observation that during learning the variational approx-
imation is good in this case (the free energy bound appears tight). On the other hand, for
the LT model, the variational bound is loose during learning and localization performance
is equivalent, but slightly lower than that of exact LT model. This may be explained by the
fact that gaussian appearance model is less ﬂexible then the topic model and thus G model
can better tolerate decoupling of location and appearance.

Spotted Cats
98

LV

B

LT

Model

LT

LV

96

G GV

94

92

90

88

86

84

C

DLc

G
GV
LT
LV
BL

LT
BL

LT
BL

GV

G

C

Obj. center(%)
horz
vert

8.88
8.64
8.17
7.86
-

4.58
4.47
3.92
3.76
4.50

Bbox(%)
horz
vert
Faces
21.88
16.10
13.16
18.62
-
Airplanes
10.06
9.09
-
10.37
Motorbikes
4.93
7.33
-
5.11

19.30
-

8.41
-

16.59
16.10
6.45
11.04
24.71

4.42
4.47

4.65
2.01

Airplanes

Motorbikes

Faces

Cars rear

99

98

97

96

95

94

93

92

91

100
DLc

99

98

97

96

LT

LV

DL

GV

G

B

95

94

93

C

100
DLc

99

98

97

DL

LT

LV

GV

G

96

B

95

94

C

LV

LT

GV

G

100

DLc
98

96

DL

94

C

92

90

B

88

DL

92
93
86
90
82
Figure 2: Plots on the left show detection performance on Caltech 5 datasets [5]. Equal error rate
is reported. The original performance of constellation model [5] is denoted by C. We denote by DLc
the performance (best in literature) reported by [3] using an optimal combination of feature types,
and by DL the performance using our features. The performance of [8] is denoted by B. We show
performance for our G model (G), LT model (L) and their variational approximations (GV) and (LV)
respectively. We report median performance (×) over 20 runs and performance range excluding 10%
best and 10% worst runs. On the right we show localization performance for all models on Faces
dataset and performance of the best model (LT) on all datasets. Standard deviation is reported in
percentage units with respect to the ground truth bounding box. For bounding boxes we average the
standard deviation in each direction. BL denotes baseline performance.

Figure 3: The airplane and motorbike datasets are aligned. Thus the image center baseline (b), (d)
performs well there. Our localization performs similarly (a), (c). There is more variation in location
in faces dataset. Scatterplot (f) shows the baseline performance and (g) shows the performance of
our model. (e) shows the bounding boxes computed by our approach (LT model). Object centers and
bounding boxes are recti ﬁed using the ground truth bounding boxes (blu e). No information about
location or spatial extent of the object is given to the algorithm.

Figure 4: Approaches like [3] do not use geometric constraints during learning. Therefore, corre-
lation between background and object in the dataset is incorporated into the object model. In this
case the ellipses represent the features that are used by the algorithm in [3] to decide the presence
of a face and motorbike (left images taken from [3]). On the other hand, our model (right images)
can estimate the location and support of the object, even though no information about it is provided
during learning. Blue circles represent the features assigned by the model to the face, the red points
are centers of features assigned to background (plot for Local Topic Model).

5 Conclusions and future work

We have presented a novel model for object categories. Our model allows efﬁcient unsu-
pervised learning, bringing the learning time to a few hours for full models and to minutes
for variational approximations. The signi ﬁcant reduction in complexity allows to handle
many more parts and features than comparable algorithms. The detection performance of
our approach compares favorably to the state of the art even when compared to purely dis-
criminative approaches. Also our model is capable of learning the spatial extent of the
objects without supervision, with good results.

This combination of fast learning and ability to localize is required to tackle challenging
problems in computer vision. Among the most interesting applications we see unsupervised
segmentation, learning, detection and localization of multiple object categories, deformable
objects and objects with varying aspects.

References

In Proc. of

[1] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. Proc.
of CVPR, pages 511–518, 2001.
[2] G. Csurka, C. Dance, L. Fan, and C. Bray. Visual Categorization with Bags of Keypoints. In
Workshop on Stat. Learning in Comp. Vision, ECCV, pages 1–22, 2004.
[3] G. Dork ´o and C. Schmid. Object class recognition using discriminative local features. Submit-
ted to IEEE trans. on PAMI, 2004.
[4] M. Weber, M. Welling, and P. Perona. Unsupervised Learning of Models for Recognition. Proc.
of ECCV (1), pages 18–32, 2000.
[5] R. Fergus, P. Perona, and A. Zisserman. Object Class Recognition by Unsupervised Scale-
Invariant Learning. Proc. of CVPR, pages 264–271, 2003.
[6] S. Agarwal and D. Roth. Learning a sparse representation for object detection.
ECCV, volume 4, pages 113–130, Copenhagen, Denmark, May 2002.
[7] B. Leibe, A. Leonardis, and B. Schiele. Combined object categorization and segmentation with
an implicit shape model. In Workshop on Stat. Learning in Comp. Vision, pages 17–32, May
2004.
[8] A. B. Hillel, T. Hertz, and D. Weinshall. Efﬁcient learning of relational object class models. In
Proc. of ICCV, pages 1762–1769, October 2005.
[9] R. Fergus, P. Perona, and A. Zisserman. A sparse object category model for efﬁcient learning
and exhaustive recognition. In Proc. of CVPR, pages 380–387, june 2005.
[10] D. Crandall, P. Felzenszwalb, and D. Huttenlocher. Spatial Priors for Part-Based Recognition
using Statistical Models. In Proc. of CVPR, pages 10–17, 2005.
[11] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training
examples an incremental bayesian approach tested on 101 object categories. In Workshop on
Generative-Model Based Vision, Washington, DC, June 2004.
[12] A. Opelt, M. Fussenegger, A. Pinz, and P. Auer. Generic object recognition with boosting.
Technical Report TR-EMT-2004-01, EMT, TU Graz, Austria, 2004. Submitted to the IEEE
Trans. on PAMI.
[13] T. Kadir and M. Brady. Saliency, Scale and Image Description. IJCV, 45(2):83–105, 2001.
[14] B. Frey and N. Jojic. A Comparison of Algorithms for Inference and Learning in Probabilistic
Graphical Models. IEEE Trans. on PAMI, 27(9):1392–1416, 2005.
[15] R. Neal and G. Hinton. A view of the EM algorithm that justiﬁes incremen tal, sparse, and other
variants.
In M. I. Jordan, editor, Learning in graphical models, pages 355–368. MIT Press,
Cambridge, MA, USA, 1999.
[16] D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.


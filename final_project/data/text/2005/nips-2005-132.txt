Variational EM Algorithms for
Non-Gaussian Latent Variable Models

J. A. Palmer, D. P. Wipf, K. Kreutz-Delgado, and B. D. Rao
Department of Electrical and Computer Engineering
University of California San Diego, La Jolla, CA 92093
{japalmer,dwipf,kreutz,brao}@ece.ucsd.edu

Abstract

We consider criteria for variational representations of non-Gaussian la-
tent variables, and derive variational EM algorithms in general form. We
establish a general equivalence among convex bounding methods, evi-
dence based methods, and ensemble learning/Variational Bayes methods,
which has previously been demonstrated only for particular cases.

1 Introduction

Probabilistic methods have become well-established in the analysis of learning algorithms
over the past decade, drawing largely on classical Gaussian statistical theory [21, 2, 28].
More recently, variational Bayes and ensemble learning methods [22, 13] have been pro-
posed. In addition to the evidence and VB methods, variational methods based on convex
bounding have been proposed for dealing with non-gaussian latent variables [18, 14]. We
concentrate here on the theory of the linear model, with direct application to ICA [14],
factor analysis [2], mixture models [13], kernel regression [30, 11, 32], and linearization
approaches to nonlinear models [15]. The methods can likely be applied in other contexts.

In Mackay’s evidence framework, ”hierarchical priors” are employed on the latent vari-
ables, using Gamma priors on the inverse variances, which has the effect of making the
marginal distribution of the latent variable prior the non-Gaussian Student’s t [30]. Based
on Mackay’s framework, Tipping proposed the Relevance Vector Machine (RVM) [30] for
estimation of sparse solutions in the kernel regression problem. A relationship between the
evidence framework and ensemble/VB methods has been noted in [22, 6] for the partic-
ular case of the RVM with t hyperprior. Figueiredo [11] proposed EM algorithms based
on hyperprior representations of the Laplacian and Jeffrey’s priors. In [14], Girolami em-
ployed the convex variational framework of [16] to derive a different type of variational
EM algorithm using a convex variational representation of the Laplacian prior. Wipf et al.
[32] demonstrated the equivalence between the variational approach of [16, 14] and the ev-
idence based RVM for the case of t priors, and thus via [6], the equivalence of the convex
variational method and the ensemble/VB methods for the particular case of the t prior.

In this paper we consider these methods from a unifying viewpoint, deriving algorithms in
more general form and establishing a more general relationship among the methods than
has previously been shown. In §2, we deﬁne the model and estimation problems we shall
be concerned with, and in §3 we discuss criteria for variational representations. In §4 we
consider the relationships among these methods.

2 The Bayesian linear model
(cid:81)
Throughout we shall consider the following model,
y = Ax + ν ,
(1)
where A ∈ Rm×n , x ∼ p(x) =
i p(xi ), and ν ∼ N (0, Σν ), with x and ν independent.
The important thing to note for our purposes is that the xi are non-Gaussian.
We consider two types of variational representation of the non-Gaussian priors p(xi ), which
we shall call convex type and integral type. In the convex type of variational representation,
the density is represented as a supremum over Gaussian functions of varying scale,
N (x; 0, ξ−1 ) ϕ(ξ ) .
p(x) = sup
ξ>0
The essential property of “concavity in x2 ” leading to this representation was used in [29,
17, 16, 18, 6] to represent the Logistic link function. A convex type representation of the
Laplace density was applied to learning overcomplete representations in [14].
(cid:90) ∞
In the integral type of representation, the density p(x) is represented as an integral over the
scale parameter of the density, with respect to some positive measure µ,
N (x; 0, ξ−1 ) dµ(ξ ) .
0
Such representations with a general kernel are referred to as scale mixtures [19]. Gaussian
scale mixtures were discussed in the examples of Dempster, Laird, and Rubin’s original
EM paper [9], and treated more extensively in [10]. The integral representation has been
used, sometimes implicitly, for kernel-based estimation [30, 11] and ICA [20]. The distinc-
tion between MAP estimation of components and estimation of hyperparameters has been
discussed in [23] and [30] for the case of Gamma distributed inverse variance.

p(x) =

(3)

(2)

We shall be interested in variational EM algorithms for solving two basic problems, cor-
responding essentially to the two methods of handling hyperparameters discussed in [23]:
the MAP estimate of the latent variables
p(x|y)
ˆx = arg max
x
and the MAP estimate of the hyperparameters,
ˆξ = arg max
ξ
The following section discusses the criteria for and relationship between the two types of
In §4, we discuss algorithms for each problem based on the
variational representation.
two types of variational representations, and determine when these are equivalent. We also
discuss the approximation of the likelihood p(y; A) using the ensemble learning or VB
method, which approximates the posterior p(x, ξ |y) by a factorial density q(x|y)q(ξ |y).
We show that the ensemble method is equivalent to the hyperparameter MAP method.

p(ξ |y) .

(4)

(5)

3 Variational representations of super-Gaussian densities

In this section we discuss the criteria for the convex and integral type representations.

3.1 Convex variational bounds
log N (cid:161)
x ; 0, ξ−1 (cid:162)
We wish to determine when a symmetric, unimodal density p(x) can be represented in the
form (2) for some function ϕ(ξ ). Equivalently, when,
− log p(x) = − sup
ϕ(ξ ) = inf
ξ>0
ξ>0

2 x2 ξ − log ξ
1

1
2 ϕ(ξ )

√
for all x > 0. The last formula says that − log p(
x) is the concave conjugate of (the
2 ϕ(ξ ) [27, §12]. This is possible if and only
√
1
closure of the convex hull of) the function, log ξ
if − log p(
x) is closed, increasing and concave on (0, ∞). Thus we have the following.
Theorem 1. A symmetric probability density p(x) ≡ exp(−g(x2 )) can be represented in
the convex variational form,

N (x; 0, ξ−1 ) ϕ(ξ )
p(x) = sup
√
ξ>0
(cid:112)
(cid:161)
(cid:162)
if and only if g(x) ≡ − log p(
x) is increasing and concave on (0, ∞). In this case we can
use the function,
g∗(ξ/2)
2π/ξ exp
ϕ(ξ ) =
where g∗ is the concave conjugate of g .
(i) Generalized Gaussian ∝
Examples of densities satisfying this criterion include:
(ii) Logistic ∝ 1/ cosh2 (x/2),
exp(−|x|β ), 0 < β ≤ 2,
(iii) Student’s t ∝
(1 + x2 /ν )−(ν+1)/2 , ν > 0, and (iv) symmetric α-stable densities (having characteristic
function exp(−|ω |α ), 0 < α ≤ 2).
The convex variational representation motivates the following deﬁnition.
√
√
Deﬁnition 1. A symmetric probability density p(x) is strongly super-gaussian if p(
x) is
log-convex on (0, ∞), and strongly sub-gaussian if p(
x) is log-concave on (0, ∞).
p(x) = exp(−f (x))
An equivalent deﬁnition is given in [5, pp. 60-61], which deﬁnes
to be sub-gaussian (super-gaussian) if f (cid:48) (x)/x is increasing (decreasing) on (0, ∞). This
condition is equivalent to f (x) = g(x2 ) with g concave, i.e. g (cid:48) decreasing. The property of
being strongly sub- or super-gaussian is independent of scale.

,

3.2 Scale mixtures

We now wish to determine when a probability density p(x) can be represented in the form
(3) for some µ(ξ ) non-decreasing on (0, ∞). A fundamental result dealing with integral
representations was given by Bernstein and Widder (see [31]). It uses the following deﬁni-
tion.
Deﬁnition 1. A function f (x) is completely monotonic on (a, b) if,
(−1)n f (n) (x) ≥ 0 , n = 0, 1, . . .

for every x ∈ (a, b).
That is, f (x) is completely monotonic if it is positive, decreasing, convex, and so on.
Bernstein’s theorem [31, Thm. 12b] states:
(cid:90) ∞
Theorem 2. A necessary and sufﬁcient condition that p(x) should be completely monotonic
on (0, ∞) is that,
0

e−txdα(t) ,
p(x) =
where α(t) is non-decreasing on (0, ∞).
(cid:90) ∞
Thus for p(x) to be a Gaussian scale mixture,
p(x) = e−f (x) = e−g(x2 ) =
e− 1
2 tx2
dα(t) ,
√
0
x) = e−g(x) be completely monotonic for
a necessary and sufﬁcient condition is that p(
0 < x < ∞, and we have the following (see also [19, 1]),
√
Theorem 3. A function p(x) can be represented as a Gaussian scale mixture if and only if
x) is completely monotonic on (0, ∞).
p(

3.3 Relationship between convex and integral type representations

We now consider the relationship between the convex and integral types of variational
representation. Let p(x) = exp(−g(x2 )). We have seen that p(x) can be represented in the
form (2) if and only if g(x) is symmetric and concave on (0, ∞). And we have seen that
√
x) = exp(−g(x)) is completely
√
p(x) can be represented in the form (3) if and only if p(
√
monotonic. We shall consider now whether or not complete monotonicity of p(
x) implies
the concavity of g(x) = − log p(
x), that is whether representability in the integral form
implies representability in the convex form.
Complete monotonicity of a function q(x) implies that q ≥ 0, q (cid:48) ≤ 0, q (cid:48)(cid:48) ≥ 0, etc. For
√
dx2 e−g(x) = e−g(x) (cid:161)
(cid:162) ≥ 0 .
x) is completely monotonic, then,
example, if p(
√
d2
x) = d2
g (cid:48) (x)2 − g (cid:48)(cid:48) (x)
dx2 p(
√
Thus if g (cid:48)(cid:48) ≤ 0, then p(
√
x) is convex, but the converse does not necessarily hold. That
x), as the latter only requires that
is, concavity of g does not follow from convexity of p(
g (cid:48)(cid:48) ≤ g (cid:48) 2 .
√
(cid:82)
Concavity of g does follow however from the complete monotonicity of p(
ple, we can use the following result [8, §3.5.2].
Theorem 4. If the functions ft (x), t ∈ D , are convex, then
D eft (x)dt is convex.
Thus completely monotonic functions, being scale mixtures of the log convex function
e−x by Theorem 2, are also log convex. We thus see that any function representable in the
integral variational form (3) is also representable in the convex variational form (2).

x). For exam-

In fact, a stronger result holds. The following theorem [7, Thm. 4.1.5] establishes the
equivalence between q(x) and g (cid:48) (x) = d/dx − log q(x) in terms of complete monotonicity.
Theorem 5. If g(x) > 0, then e−ug(x) is completely monotonic for every u > 0, if and
only if g (cid:48) (x) is completely monotonic.
√
In particular, it holds that q(x) ≡ p(
x) = exp(−g(x)) is convex only if g (cid:48)(cid:48) (x) ≤ 0.
To summarize, let p(x) = e−g(x2 ) . If g is increasing and concave for x > 0, then p(x) ad-
mits the convex type of variational representation (2). If, in addition, the higher derivatives
satisfy g (3) (x) ≥ 0, g (4) (x) ≤ 0, g (5) (x) ≥ 0, etc., then p(x) also admits the Gaussian
scale mixture representation (3).

4 General equivalences among Variational methods

4.1 MAP estimation of components

Consider ﬁrst the MAP estimate of the latent variables (4).

4.1.1 Component MAP – Integral case
(cid:90) ∞
(cid:90) ∞
Following [10]1 , consider an EM algorithm to estimate x when the p(xi ) are independent
Gaussian scale mixtures as in (3). Differentiating inside the integral gives,
(cid:90) ∞
p(x|ξ )p(ξ )dξ = −
p(cid:48) (x) = d
dx
0
0
= −xp(x)
ξp(ξ |x) dξ .
0
1 In [10], the xi in (1) are actually estimated as non-random parameters, with the noise ν being
non-gaussian, but the underlying theory is essentially the same.

ξ xp(x, ξ ) dξ

(cid:90) ∞
Thus, with p(x) ≡ exp(−f (x)), we see that,
= f (cid:48)(xi )
ξi p(ξi |xi ) dξi = − p(cid:48) (xi )
E (ξi |xi ) =
(6)
.
xi p(xi )
xi
0
The EM algorithm alternates setting ˆξi to the posterior mean, E (ξi |xi ) = f (cid:48) (xi )/xi , and
setting x to minimize,
ν Ax − yT Σ−1
− log p(y|x)p(x| ˆξ ) = 1
2 xTAT Σ−1
ν Ax + 1
2 xTΛx + const.,
(7)
where Λ = diag( ˆξ )−1 . At iteration k , we put ξ k
i , and Λk = diag(ξ k )−1 , and
i = f (cid:48)(xk
i )/xk
xk+1 = ΛkAT (AΛkAT + Σν )−1y .
4.1.2 Component MAP – Convex case

Again consider the MAP estimate of x. For strongly super-gaussian priors, p(xi ), we have,
p(y|x)p(x) = arg max
p(x|y) = arg max
p(y|x)p(x; ξ )ϕ(ξ )
max
arg max
n(cid:88)
x
x
x
ξ
Now since,
i ξi − g∗ (ξi/2) ,
ν Ax − yT Σ−1
− log p(y|x)p(x; ξ )ϕ(ξ ) = 1
2 xTAT Σ−1
ν Ax +
2 x2
1
i=1
the MAP estimate can be improved iteratively by alternately maximizing x and ξ ,
i ) = f (cid:48)(xk
i )
i = 2 g∗(cid:48)−1(xk 2
i ) = 2 g (cid:48)(xk 2
ξ k
(8)
,
xk
i
with x updated as in §4.1.1. We thus see that this algorithm is equivalent to the MAP
algorithm derived in §4.1.1 for Gaussian scale mixtures. That is, for direct MAP estimation
of latent variable x, the EM Gaussian scale mixture method and the variational bounding
method yield the same algorithm.

This algorithm has also been derived in the image restoration literature [12] as the “half-
quadratic” algorithm, and it is the basis for the FOCUSS algorithms derived in [26, 25].
The regression algorithm given in [11] for the particular cases of Laplacian and Jeffrey’s
priors is based on the theory in §4.1.1, and is in fact equivalent to the FOCUSS algorithm
derived in [26].

4.2 MAP estimate of variational parameters

Now consider MAP estimation of the (random) variational hyperparameters ξ .

4.2.1 Hyperparameter MAP – Integral case

Consider an EM algorithm to ﬁnd the MAP estimate of the hyperparameters ξ in the integral
representation (Gaussian scale mixture) case, where the latent variables x are hidden. For
the complete likelihood, we have,
(cid:88)
(cid:112)
p(ξ , x|y) ∝ p(y|x, ξ )p(x|ξ )p(ξ ) = p(y|x)p(x|ξ )p(ξ ) .
(cid:173)− log p(x|ξ )p(ξ )
(cid:174)
The function to be minimized over ξ is then,
2 (cid:104)x2
i (cid:105) ξi − log
x =
1
√
i (cid:105)(cid:162)
ξi = h∗(cid:48) (cid:161)
i
If we deﬁne h(ξ ) ≡ log
ξi p(ξi ), and assume that this function is concave, then the
optimal value of ξ is given by,
2 (cid:104)x2
1
.
This algorithm converges to a local maximum of p(ξ |y), ˆξ , which then yields an estimate
of x by taking ˆx = E (x|y, ˆξ ). Alternative algorithms result from using this method to ﬁnd
the MAP estimate of different functions of the scale random variable ξ .

ξi p(ξi ) + const.

(9)

p(y) =

4.2.2 Hyperparameter MAP – Convex case
(cid:90)
(cid:90)
In the convex representation, the ξ parameters do not actually represent a probabilistic
quantity, but rather arise as parameters in a variational inequality. Speci ﬁcally, we write,
(cid:90)
p(y|x) p(x|ξ ) ϕ(ξ ) dx
max
p(y, x) dx =
ξ
N (cid:161)
(cid:162)
p(y|x) p(x|ξ ) ϕ(ξ ) dx
≥ max
ξ
y; 0, AΛAT + Σν
ϕ(ξ ) .
= max
˜p(y; ξ ) ≡ N (cid:161)
(cid:162)
ξ
Now we deﬁne the function,
ϕ(ξ )
y; 0, AΛAT + Σν
(cid:90)
and try to ﬁnd ˆξ = arg max ˜p(y; ξ ). We maximize ˜p by EM, marginalizing over x,
p(y|x) p(x|ξ ) ϕ(ξ ) dx .
˜p(y; ξ ) =
The algorithm is then equivalent to that in §4.1.2 except that the expectation is taken of x2
as the E step, and the diagonal weighting matrix becomes,
ξi = f (cid:48) (σi )
(cid:112)
,
σi
i |y; ξi ). Although ˜p is not a true probability density function, the proof
where σi =
E (x2
of convergence for EM does not assume unit normalization. This theory is the basis for the
algorithm presented in [14] for the particular case of a Laplacian prior (where in addition
A in the model (1) is updated according to the standard EM update.)

4.3 Ensemble learning

In the ensemble learning approach (also Variational Bayes [4, 3, 6]) the idea is to ﬁnd the
(cid:90)
(cid:175)(cid:175)(cid:175)(cid:175) p(z|y)
(cid:161)
(cid:162)
approximate separable posterior that minimizes the KL divergence from the true posterior,
using the following decomposition of the log likelihood,
q(z|y) log p(z, y)
q(z|y)
log p(y) =
q(z|y) dz + D
≡ −F (q) + D(q ||p) .
The term F (q) is commonly called the variational free energy [29, 24]. Minimizing the F
over q is equivalent to minimizing D over q . The posterior approximating distribution is
taken to be factorial,
q(z|y) = q(x, ξ |y) = q(x|y)q(ξ |y) .
(cid:90) (cid:90)
(cid:179)
(cid:180)
(cid:175)(cid:175)(cid:175)(cid:175) e(cid:104)log p(x,ξ |y)(cid:105)ξ
For ﬁxed q(ξ |y), the free energy F is given by,
q(x|y)q(ξ |y) log p(x, ξ |y)
−
q(x|y)
q(x|y)q(ξ |y) dξ dx = D
+ const.,
(cid:161)
(cid:162)
(10)
where (cid:104)· (cid:105)ξ denotes expectation with respect to q(ξ |y), and the constant is the entropy,
(cid:173)
(cid:174)
(cid:173)
(cid:174)
q(ξ |y)
. The minimum of the KL divergence in (10) is attained if and only if
H
ξ ∝ p(y|x) exp
log p(x|ξ )
q(x|y) ∝ exp
log p(x, ξ |y)
(cid:173)
(cid:174)
(cid:173)
(cid:174)
ξ
almost surely. An identical derivation yields the optimal
log p(x|ξ )
x ∝ p(ξ ) exp
q(ξ |y) ∝ exp
log p(x, ξ |y)
x

when q(x|y) is ﬁxed. The ensemble (or VB) algorithm consists of alternately updating the
parameters of these approximating marginal distributions.

,

In the linear model with Gaussian scale mixture latent variables, the complete likelihood is
again,
p(y, x, ξ ) = p(y|x)p(x|ξ )p(ξ ) .
(cid:180)
(cid:179)
(cid:175)(cid:175) xi = (cid:104)x2
The optimal approximate posteriors are given by,
q(x|y) = N (x; µx|y , Σx|y ) ,
q(ξi |y) = p
i (cid:105)1/2
ξi
where, letting Λ = diag((cid:104)ξ (cid:105))−1 , the posterior moments are given by,
µx|y ≡ ΛAT (AΛAT + Σν )−1y
Σx|y ≡ (AT Σ−1
ν A + Λ−1 )−1 = Λ − ΛAT (AΛAT + Σν )−1AΛ .
(cid:90)
(cid:90)
(cid:180)
(cid:179)
The only relevant fact about q(ξ |y) that we need is (cid:104)ξ (cid:105), for which we have, using (6),
dξi = f (cid:48) (σi )
(cid:112)
(cid:104)ξi (cid:105) =
ξi q(ξi |y) dξi =
ξi | xi = (cid:104)x2
i (cid:105)1/2
ξip
,
σi
i |y; ξi ). We thus see that the ensemble learning algorithm is equivalent
where σi =
E (x2
to the approximate hyperparameter MAP algorithm of §4.2.2. Note also that this shows that
the VB methods can be applied to any Gaussian scale mixture density, using only the form
of the latent variable prior p(x), without needing the marginal hyperprior p(ξ ) in closed
form. This is particularly important in the case of the Generalized Gaussian and Logistic
densities, whose scale parameter densities are α-Stable and Kolmogorov [1] respectively.

5 Conclusion

In this paper, we have discussed criteria for variational representations of non-Gaussian
latent variables, and derived general variational EM algorithms based on these represen-
tations. We have shown a general equivalence between the two representations in MAP
estimation taking hyperparameters as hidden, and we have shown the general equivalence
between the variational convex approximate MAP estimate of hyperparameters and the
ensemble learning or VB method.

References

[1] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. J. Roy. Statist. Soc.
Ser. B, 36:99–102, 1974.
[2] H. Attias. Independent factor analysis. Neural Computation, 11:803–851, 1999.
[3] H. Attias. A variational Bayesian framework for graphical models.
In Advances in Neural
Information Processing Systems 12. MIT Press, 2000.
[4] M. J. Beal and Z. Ghahrarmani. The variational Bayesian EM algorithm for incomplete data:
with application to scoring graphical model structures. In Bayesian Statistics 7, pages 453–464.
University of Oxford Press, 2002.
[5] A. Benveniste, M. M ´etivier, and P. Priouret. Adaptive algorithms and stochastic approxima-
tions. Springer-Verlag, 1990.
[6] C. M. Bishop and M. E. Tipping. Variational relevance vector machines. In C. Boutilier and
M. Goldszmidt, editors, Proceedings of the 16th Conference on Uncertainty in Artiﬁcial Intel-
ligence, pages 46–53. Morgan Kaufmann, 2000.
[7] S. Bochner. Harmonic analysis and the theory of probability. University of California Press,
Berkeley and Los Angeles, 1960.

[8] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.
[10] A. P. Dempster, N. M. Laird, and D. B. Rubin. Iteratively reweighted least squares for linear
regression when errors are Normal/Independent distributed. In P. R. Krishnaiah, editor, Multi-
variate Analysis V, pages 35–57. North Holland Publishing Company, 1980.
[11] M. Figueiredo. Adaptive sparseness using Jeffreys prior. In T. G. Dietterich, S. Becker, and
Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge,
MA, 2002. MIT Press.
[12] D. Geman and G. Reynolds. Constrained restoration and the recovery of discontinuities. IEEE
Trans. Pattern Analysis and Machine Intelligence, 14(3):367–383, 1992.
[13] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixtures of factor analysers.
In Advances in Neural Information Processing Systems 12. MIT Press, 2000.
[14] M. Girolami. A variational method for learning sparse and overcomplete representations. Neu-
ral Computation, 13:2517–2532, 2001.
[15] A. Honkela and H. Valpola. Unsupervised variational Bayesian learning of nonlinear models.
In Advances in Neural Information Processing Systems 17. MIT Press, 2005.
[16] T. S. Jaakkola. Variational Methods for Inference and Estimation in Graphical Models. PhD
thesis, Massachusetts Institute of Technology, 1997.
[17] T. S. Jaakkola and M. I. Jordan. A variational approach to Bayesian logistic regression models
and their extensions.
In Proceedings of the 1997 Conference on Artiﬁcial Intelligence and
Statistics, 1997.
[18] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational
methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer
Academic Publishers, 1998.
[19] J. Keilson and F. W. Steutel. Mixtures of distributions, moment inequalities, and measures of
exponentiality and Normality. The Annals of Probability, 2:112–130, 1974.
[20] H. Lappalainen. Ensemble learning for independent component analysis. In Proceedings of the
First International Workshop on Independent Component Analysis, 1999.
[21] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992.
[22] D. J. C. MacKay. Ensemble learning and evidence maximization. Unpublished manuscript,
1995.
[23] D. J. C. Mackay. Comparison of approximate methods for handling hyperparameters. Neural
Computation, 11(5):1035–1068, 1999.
[24] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and
other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. Kluwer,
1998.
[25] B. D. Rao, K. Engan, S. F. Cotter, J. Palmer, and K. Kreutz-Delgado. Subset selection in noise
based on diversity measure minimization. IEEE Trans. Signal Processing, 51(3), 2003.
[26] B. D. Rao and I. F. Gorodnitsky. Sparse signal reconstruction from limited data using FOCUSS:
a re-weighted minimum norm algorithm. IEEE Trans. Signal Processing, 45:600–616, 1997.
[27] R. T. Rockafellar. Convex Analysis. Princeton, 1970.
[28] Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models. Neural
Computation, 11(5):305–345, 1999.
[29] L. K. Saul, T. S. Jaakkola, and M. I. Jordan. Mean ﬁeld theory for sigmoid belief networks.
Journal of Artiﬁcial Intelligence Research , 4:61–76, 1996.
[30] M. E. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. Journal of Ma-
chine Learning Research, 1:211–244, 2001.
[31] D. V. Widder. The Laplace Transform. Princeton University Press, 1946.
[32] D. Wipf, J. Palmer, and B. Rao. Perspectives on sparse bayesian learning. In S. Thrun, L. Saul,
and B. Sch ¨olkopf, editors, Advances in Neural Information Processing Systems 16, Cambridge,
MA, 2003. MIT Press.


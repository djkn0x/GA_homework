Products of “Edge-perts”

Peter Gehler
Max Planck Institute for Biological Cybernetics
Spemannstraße 38, 72076 T ¨ubingen, Germany
pgehler@tuebingen.mpg.de

Max Welling
Department of Computer Science
University of California Irvine
welling@ics.uci.edu

Abstract

Images represent an important and abundant source of data. Understand-
ing their statistical structure has important applications such as image
compression and restoration. In this paper we propose a particular kind
of probabilistic model, dubbed the “products of edge-perts model” to de-
scribe the structure of wavelet transformed images. We develop a prac-
tical denoising algorithm based on a single edge-pert and show state-of-
the-art denoising performance on benchmark images.

1 Introduction

Images, when represented as a collection of pixel values, exhibit a high degree of redun-
dancy. Wavelet transforms, which capture most of the second order dependencies, form the
basis of many successful image processing applications such as image compression (e.g.
JPEG2000) or image restoration (e.g. wavelet coring). However, the higher order depen-
dencies can not be ﬁltered out by these linear transforms. In particular, the absolute values
of neighboring wavelet coefﬁcients (but not their signs) are mutually dependent. This kind
of dependency is caused by the presence of edges that induce clustering of wavelet activity.
Our philosophy is that by modelling this clustering effect we can potentially improve the
performance of some important image processing tasks.

Our model builds on earlier work in the image processing literature.
In particular, the
PoEdges models that we discuss in this paper can be viewed as generalizations of the mod-
els proposed in [1] and [2]. The state-of-art in this area is the joint model discussed in [3]
based on the “Gaussian scale mixture” model (GSM). While the GSM falls in the cate-
gory of directed graphical models and has a top-down structure, the PoEdges model is best
classiﬁed as an (undirected) Markov random ﬁeld model and follows bottom-up semantics.

The main contributions of this paper are 1) a new model to describe the higher order sta-
tistical dependencies among wavelet coefﬁcients (section 2), 2) an efﬁcient estimation pro-
cedure to ﬁt the parameters of a single edge-pert model and a new technique to estimate
the wavelet coefﬁcients that participate in each such (local) model (section 3.1) and 3) a
new “iterated Wiener denoising algorithm” (section 3.2). In section 4 we report on a num-
ber of experiments to compare performance of our algorithm with several methods in the
literature and with the GSM-based method in particular.

(Ia)

(Ib)

(IIa)

(IIb)

Figure 1: Estimated (Ia) and modelled (Ib) conditional distribution of a wavelet coefﬁcient given
its upper left neighbor. The statistics were collected from the vertical subband at the lowest level of
a Haar ﬁlter wavelet decomposition of the ”Lena ” image. Note that the “bow-tie” dependencies are
captured by the PoEdges model. (IIa) Bottom up network interpretation of “products of edge-perts”
model. (IIb) Top-down generative Gaussian scale mixture model.

“Product of Edge-perts ”
2
It has long been recognized in the image processing community that wavelet transforms
form an excellent basis for representation of images. Within the class of linear trans-
forms, it represents a compromise between many conﬂicting but desirable properties
of image representation such as multi-scale and multi-orientation representation, local-
ity both in space and frequency, and orthogonality resulting in decorrelation. A par-
ticularly suitable wavelet transform which forms the basis of the best denoising algo-
rithms today is the over-complete steerable wavelet pyramid [4] freely downloadable from
http://www.cns.nyu.edu/∼lcv/software.html. In our experiments we have conﬁrmed that the best
results were obtained using this wavelet pyramid.

P (z) =

exp

, βj > 0, αi ∈ (0, 1], Wij ≥ 0

In the following we will describe a model for the statistical dependencies between wavelet
coefﬁcients. This model was inspired by recent studies of these dependencies (see e.g.
[1, 5]). It also represents a generalization of the bivariate Laplacian model proposed in
h − X
(cid:16) X
(cid:17)αi i
[2]. The probability distribution of the “product of edge-pert” model (PoEdges) over the
wavelet coefﬁcients z has the following form,
1
Wij |ˆaT
j z|βj
Z
j
i
where the normalization constant Z depends on all
the parameters in the model
{Wij , ˆaj , βj , αi } and where ˆa indicates an unit-length vector.
In ﬁgure 2 we show the effect of changing some parameters for a single edge-pert model
(i.e. set i = 1 in Eqn.1 above). The parameters {βj } control the shape of the contours: for
β = 2 we have elliptical contours, for β = 1 the contours are straight lines while for β < 1
the contours curve inwards. The parameters {αi } control the rate at which the distribution
decays, i.e. the distance between iso-probability contours. The unit vectors {ˆai } determine
the orientation of basis vectors. If the {ˆai } are axis-aligned (as in ﬁgure 2), the distribution
is symmetric w.r.t. reﬂections of any subset of the {zi } in the origin, which implies that the
wavelet coefﬁcients are necessarily decorrelated (although higher order dependencies may
still remain). Finally, the weights {Wij } model the scale (inverse variance) of the wavelet
coefﬁcients. We mention that it is possible to entertain a larger number of bases vectors
than wavelet coefﬁcients (a so-called “over-complete basis”), which seems appropriate for
some of the empirical joint histograms shown in [1].

This model describes two important statistical properties which have been observed for
wavelet coefﬁcients: 1) its marginal distributions p(zi ) are peaked and have heavy tails
(high kurtosis) and 2) the conditional distributions p(zi |zj ) display “bow-tie” dependencies
which are indicative of clustering of wavelet coefﬁcients (neighboring wavelet coefﬁcient

center componentupper left component−15−10−5051015−15−10−5051015W = [8.64,8.63], α = 0.28center componentupper left component−15−10−5 0  5  10 15 −15−10−5 0  5  10 15 WZUUα|Z|βΣZU(a)

(b)

(c)

(d)

Figure 2: Contour plots for a single edge-pert model with (a) β1,2 = 0.5, α = 0.5, (b) β1,2 =
1, α = 0.5, (c) β1,2 = 2, α = 0.5, (d) β1,2 = 2, α = 0.3. For all ﬁgures W1 = 1 and W2 = 0.8.

are often active together). This phenomenon is shown in ﬁgure 1Ia,b. To better understand
the qualitative behavior of our model we provide the following network interpretation (see
ﬁgure 1IIa,b. Input to the model (i.e. the wavelet coefﬁcients) undergo a nonlinear trans-
formation zi → |zi |βi → u = W |z|β → uα . The output of this network, uα , can be
interpreted as a “penalty” for the input: the larger this penalty is, the more unlikely this
input becomes under the probabilistic model. This process is most naturally understood
[6] as enforcing constraints of the form u = W |z|β ≈ 0, by penalizing violations of these
constraints with uα .

What is the reason that the PoEdges model captures the clustering of wavelet activities?
Consider a local model describing the statistical structure of a patch of wavelet coefﬁcients
and recall that the weighted sum of these activities is penalized. At a ﬁxed position the
activities are typically very small across images. However, when an edge happens to fall
within the window of the model, most coefﬁcients become active jointly. This “sparse”
pattern of activity incurs less penalty than for instance the same amount1 of activity dis-
tributed equally over all images because of the concave shape of the penalty function, i.e.
(act)α < ( 1
2 act)α + ( 1
2 act)α where “act” is the activity level and α < 1.

2.1 Related Work

Early wavelet denoising techniques were based on the observation that the marginal dis-
tribution of a wavelet coefﬁcient is highly kurtotic (peaked and heavy tails). It was found
that the generalized Gaussian density represents a very good ﬁt to the empirical histograms
[1, 7],
exp [−(w|z |)α ] , α > 0, w > 0.
p(z ) = αw
2Γ( 1
α )
This has lead to the successful wavelet coring and shrinkage methods. A bivariate gener-
alization of that model describing a wavelet coefﬁcient zc and its “parent”
zp at a higher
(cid:16)−q
(cid:17)
level in the pyramid jointly, was proposed in [2]. The probability density,
p(zc , zp ) = w
p )
c + z 2
w(z 2
2π
is easily seen to be a special case of the PoEdges model proposed here. This model, un-
like the univariate model, captures the bow-tie dependencies described above resulting a
signi ﬁcant gain in denoising performance.

exp

(1)

(2)

“Gaussian scale mixtures” (GSM) have been proposed to model even larger neighborhoods
of wavelet coefﬁcients.
In particular, very good denoising results have been obtained
by including within subband neighborhoods of size 3 × 3 in addition to the parent of a
√
wavelet coefﬁcient [3]. A GSM is deﬁned in terms of a precision variable
u, the square-
p(z) = R du Nz [0, uΣ] p(u). Here, p(u) is the prior distribution for the precision variable.
u y, y ∼ N [0, Σ],
root of which multiplies a multivariate Gaussian variable: z =
resulting in the following expression for the distribution over the wavelet coefﬁcients:
Hence, the GSM represents an example of a generative model with top-down semantics.
1We assume the total amount of variance in wavelet activity is ﬁxed in this comparison.

−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468This in contrast to the PoEdges model which is better interpreted as a bottom-up network
with log-probability proportional to its output. This difference is contrasted in ﬁgure 1IIa,b.

3 Edge-pert Denoising
Based on the PoEdges model discussed in the previous sections we now introduce a simpli-
ﬁed model that forms the basis for a practical denoising algorithm. Recent progress in the
ﬁeld has indicated that it is important to model the higher order dependencies which exist
between wavelet coefﬁcients [2, 3]. This can be realized through the estimation of a joint
model on a small cluster of wavelet coefﬁcients around each coefﬁcient. Ideally, we would
p(z) ∝ exp (cid:2) − (cid:0) X
(cid:0) ˆaj
T z(cid:1)2 (cid:1)α (cid:3).
like to use the full PoEdges model, but training these models from data is cumbersome.
Therefore, in order to keep computations tractable, we proceed with a simpliﬁed model,
j
Compared to the full PoEdges model we use only one edge-pert and we have set βj = 2 ∀j .

(3)

wj

3.1 Model Estimation

Our next task is to estimate the parameters of this model efﬁciently. We will learn sepa-
rate models for each wavelet coefﬁcient jointly with a small neighborhood of dependent
coefﬁcients. Each such model is estimated in three steps: I) determine the coefﬁcients that
participate in each model, II) transform each model into a decorrelated domain (this implic-
itly estimates the {ˆaj }) and III) estimate the remaining parameters w, α in the decorrelated
domain using moment matching. Below we will describe these steps in more detail.
By zi , ˜zi we will denote the clean and noisy wavelet coefﬁcients respectively. With yi , ˜yi
we denote the decorrelated clean and noisy wavelet coefﬁcients while ni denotes the
Gaussian noise random variable in the wavelet domain, i.e. ˜zi = zi + ni . Both due to
the details of the wavelet decomposition and due to the properties of the noise itself we
assume the noise to be correlated and zero mean: E[ni ] = 0, E[ninj ] = Σij . In this paper
we further assume that we know the noise covariance in the image domain from which
one can easily compute the noise covariance in the wavelet domain, however only minor
changes are needed to estimate it from the noisy image itself.
Step I: We start with a 7 × 7 neighborhood from which we will adaptively select the best
candidates to include in the model. In addition, we will always include the parent coef-
ﬁcient in the subband of a coarser scale if it exists (this is done by ﬁrst up-sampling this
band, see [3]). The coefﬁcients that participate in a model are selected by estimating their
dependencies relative to the center coefﬁcient. Anticipating that (second order) correla-
tions will be removed by sphering we are only interested in higher order dependencies, in
particular dependencies between the variances. The following cumulant is used to obtain
these estimates,
j ] − 2E[ ˜zc ˜zj ]2 − E[ ˜z 2
c ]E[ ˜z 2
Hcj = E[ ˜z 2
c ˜z 2
j ]
(4)
where c is the center coefﬁcient which will be denoised. The necessary averages E[·] are
computed by collecting samples within each subband, assuming that the statistics are lo-
cation invariant. It can be shown that this cumulant is invariant under addition of possibly
correlated Gaussian noise, i.e. it’s value is the same for {zi } and { ˜zi }. Effectively, we mea-
sure the (higher order) dependencies between squared wavelet coefﬁcients after subtraction
of all correlations. Finally, we select the participants of a model centered at coefﬁcient ˜zc by
ranking the positive Hcj and picking all the ones which satisfy: Hci > 0.7 × maxj 6=c Hcj .
Step II: For each model (with varying number of participants) we estimate the covariance,
Cij = E[zi , zj ] = E[ ˜zi ˜zj ] − Σij
(5)

(7)

/ Γ

p Γ
N 2

and correct it by setting to zero all negative eigenvalues in such a way that the sum of
the eigenvalues is invariant (see [3]). Statistics are again collected by sampling within a
subband. Then, we perform a linear transformation to a new basis onto which Σ = I and
C are diagonal. This can be accomplished by the following procedure,
RRT = Σ ⇒ U ΛU T = R−1CR−T ⇒ ˜y = (RU )−1 ˜z.
(6)
In this new space (which is different for every wavelet coefﬁcient) we can now assume
ˆaj = ej , the axis aligned basis vector.
Step III: In the decorrelated space we estimate the single edge-pert model by moment
(cid:16) Np + 2‘
j )‘ i
Eh
(cid:17)
(cid:16) Np
(cid:17)
NpX
matching. The moments of the edge-pert model in this space are easily computed using
= Γ
(
wj y2
2α
2α
j=1
where Np is the number of participating coefﬁcients in the model. We note that E[ ˜y2
(cid:16) Np+4
(cid:16) Np
(cid:17)
(cid:17)
i ] =
1 + E[y2
NpX
NpX
i ]. This leads to the following equation for α
(cid:16) Np+2
(cid:17)2
i ] − 6E[ ˜y2
Γ
E[ ˜y4
i ] + 3
2α
2α
i ] − 1)2
(E[ ˜y2
i 6=j
Γ
i=1
2α
(8)
Thus we can estimate α by a line search and approximate the second term on the right hand
side with Np (Np − 1) to simplify the calculations. By further noting that the model (Eqn.3)
(cid:16)
(cid:1)(cid:17)
i ] − 1) Γ(cid:0) Np
wj = Γ(cid:0) Np+2
(cid:1) /
is symmetric w.r.t. permutations of the variables uj = wj y2
j we ﬁnd
Np (E[ ˜y2
(9)
.
2α
2α
A common strategy in the wavelet literature is to estimate the averages E[·] by collecting
samples in a local neighborhood around the coefﬁcient under consideration. The advantage
is that the estimates are adapting to the local statistics in the image. We have adopted
this strategy and used a 11 × 11 box around each coefﬁcient to collect 121 samples in the
decorrelated wavelet domain. Coefﬁcients for which E[ ˜y2
i ] < 1 are set to zero and removed
from consideration. The estimation of α depends on the fourth moment and is thus very
sensitive to outliers, which is a commonly known problem with the moment matching
method. We encounter the same problem so whenever we ﬁnd no estimate of α in [0, 1]
using Eqn.8 we simply set it to 0.5.

i ] − E[ ˜y2
j ] − E[ ˜y2
E[ ˜y2
i ˜y2
j ] + 1
i ] − 1)(E[ ˜y2
j ] − 1)
(E[ ˜y2

.

=

+

3.2 The Iterated Wiener Filter
(cid:0) log p(˜z|z) + log p(z)(cid:1).
To infer a wavelet coefﬁcient given its noisy observation in the decorrelated wavelet do-
main, we maximize the a posteriori probability of our joint model. This is equivalent to,
z∗ = argmax
(10)
(cid:1)α(cid:17)
(cid:16) 1
2 (z − ˜z)T K (z − ˜z) + (cid:0) X
z
When we assume Gaussian pixel noise, this translates into,
z∗ = argmin
z
j
where J is the (linear) wavelet transform ˜z = J x, K = J #T Σ−1
n J # with J # =
(J T J )−1J T the pseudo-inverse of J (i.e. J #J = I ) and Σn the noise covariance ma-
trix. In the decorrelated wavelet domain we simply set K = I.
(cid:1) α
f α ≤ γ f + (1 − α)(cid:0) γ
One can now construct an upper bound on this objective by using,
α−1
α

α < 1.

wj z 2
j

(11)

(12)

Figure 3: Output PSNR as a function of input PSNR for various methods on Lena (left) and Barbara
(right) images. GSM: Gaussian scale mixture (3 × 3+p)[3], EP: edge-pert, BIV: Bivariate adaptive
shrinkage [2], LiOr: results from [8], LM: 5 × 5 LAWMAP results from [9]. Dashed lines indicate
results copied from the literature, while solid lines indicate that the values were (re)produced on our
computer.
This bound is saturated for γ = αf α−1 , and hence we can construct the following iterative
K ˜z ⇔ γ t+1 = α(cid:0) X
zt+1 = (cid:0)K + Diag[2γ tw](cid:1)−1
)2 (cid:1)α−1
algorithm that is guaranteed to converge to a local minimum,
j
This algorithm has a natural interpretation as an “iterated Wiener ﬁlter” (IWF), since the
ﬁrst step (left hand side) is an ordinary Wiener ﬁlter while the second step (right hand side)
adapts the variance of the ﬁlter. A summary of the complete algorithm is provided below.

wj (z t+1
j

.

(13)

Edge-per t Denoising Algorithm
1. Decompose image into subbands.
2. For each subband (except low-pass residual):
2i. Determine coefﬁcients participating in joint model by using Eqn.4 (includes parent).
2ii. Compute noise covariance Σ.
2iii. Compute signal covariance using Eqn.5.
3. For each coefﬁcient in a subband:
3i.
Transform coefﬁcients into the decorrelated domain using Eqn.6.
Estimate parameters {α, wi } on a local neighborhood using Eqn.8 and Eqn.9.
3ii.
3iii. Denoise all wavelet coefﬁcients in the neighborhood using IWF from section 3.2.
3iv. Transform denoised cluster back to the wavelet domain and retain the “center coefﬁcient” only.
4. Reconstruct denoised image by inverting the wavelet transform.

4 Experiments
Denoising experiments were run on the steerable wavelet pyramid with oriented high-
pass residual bands (FSpyr) using 8 orientations as described in [3]. Results are re-
ported on six images:
“Lena”,
“Barbara”,
“Boat”,
“Fingerprint”,
“House” and “Pep-
pers” and averaged over 5 experiments.
In each experiment an image was artiﬁcially
contaminated with independent Gaussian pixel noise of some predetermined variance
and denoised using 20 iterations of the proposed algorithm. To reduce artifacts at the
boundaries we used “reﬂective boundary extensions”. The images were obtained from
http://decsai.ugr.es/∼javier/denoise/index.html to ensure comparison on the same set of images.
In table 1 we compare performance between the PoEdges and GSM based denoising algo-
rithms on six test images and ten different noise levels. In ﬁgure 3 we compare results on

202224262830313233343536Input PSNR [dB]Output PSNR [dB]LenaGSM: 35.59, 33.89, 32.67, 31.68EP   : 35.60, 33.89, 32.62, 31.64BiV  : 35.35, 33.67, 32.40, 31.40LiOr : 34.96, 33.05, 31.72, 30.64LM   : 34.31, 32.36, 31.01, 29.982022242628272829303132333435Input PSNR [dB]Output PSNR [dB]BarbaraGSM: 34.03, 31.87, 30.31, 29.12EP   : 34.40, 32.32, 30.86, 29.69BiV  : 33.35, 31.31, 29.80, 28.61LiOr : 33.35, 31.10, 29.44, 28.23LM   : 32.57, 30.19, 28.59, 27.42σ
Lena

Barbara

Boat

Fingerprint

House

Peppers

1
48.65
EP
GSM 48.46
48.70
EP
GSM 48.37
48.46
EP
GSM 48.44
EP
48.44
GSM 48.46
49.06
EP
GSM 48.85
48.50
EP
GSM 48.38

2
43.53
43.23
43.59
43.29
43.09
42.99
43.02
43.05
44.32
44.07
43.20
43.00

5
38.51
38.49
38.06
37.79
37.05
36.97
36.66
36.68
39.00
38.65
37.40
37.31

10
35.60
35.61
34.40
34.03
33.49
33.58
32.35
32.45
35.54
35.35
33.79
33.77

15
33.89
33.90
32.32
31.86
31.58
31.70
30.02
30.14
33.67
33.64
31.74
31.74

20
32.62
32.66
30.86
30.32
30.28
30.38
28.42
28.60
32.37
32.39
30.29
30.31

25
31.64
31.69
29.69
29.13
29.24
29.37
27.31
27.45
31.33
31.40
29.13
29.21

50
28.58
28.61
26.12
25.48
26.27
26.38
24.15
24.16
28.15
28.26
25.69
25.90

75
26.74
26.84
24.12
23.65
24.64
24.79
22.45
22.40
26.12
26.41
23.85
24.00

100
25.53
25.64
22.90
22.61
23.56
23.75
21.28
21.22
24.84
25.11
22.50
22.66

Table 1: Comparison of image denoising results between PoEdges (EP above) and its closest com-
petitor (GSM). All results are averaged over 5 noise samples. The GSM results are copied from [3].
Details of the PoEdges algorithm are described in main text. Note that PoEdges outperforms GSM
for low noise levels while the GSM performs better at high noise levels. Also, PoEdges performs best
at all noise levels on the Barbara image, while GSM is superior on the boat image.

FSpyr against various methods published in the literature [3, 2, 9] on the images “Lena”
and “Barbara”.

These experiments lead to some interesting conclusions. In comparing PoEdges with GSM
the general trend seems to be that PoEdges performs superior at lower noise levels while
the reverse is true for higher noise levels. We observe that the PoEdges give signiﬁcantly
better results on the ”Barbara” image than any other published method (by a large magin).
According to the ﬁndings of the authors of [3] 2 this stems mainly from the fact that the
parameters are estimated locally which is particularly suited for this image. Increasing the
estimation window in step 3ii of the algorithm let the denoising results drop down to the
GSM solution (not reported here). Comparing the quality of restored images in detail (as in
ﬁgure 3) we conclude that the GSM produces slightly sharper edges at the expense of more
artifacts. Denoising a 512 × 512 pixel sized image on a pentium 4 2.8GH z PC for our
adaptive neighborhood selection model took 26 seconds for the QMF9 and 440 seconds for
the FSpyr.

We also compared GSM and EP using a separable orthonormal pyramid (QMF9). Using
this simpler orthonormal decomposition we found that the EP model outperforms GSM in
all experiments described above. However the results are signiﬁcantly inferior because the
wavelet representation plays a prominent role for denoising performance. These results and
our matlab implementation of the algorithm are available online3 .

5 Discussion
We have proposed a general “product of edge-perts” model to capture the dependency
structure in wavelet coefﬁcients. This was turned into a practical denoising algorithm by
simplifying to a single edge-pert and choosing βj = 2 ∀j . The parameters of this model
can be adapted based on the noisy observation of the image. In comparison with the closest
competitor (GSM [3]) we found superior performance at low noise levels while the reverse
is true for high noise levels. Also, the PoEdges model performs better than any competitor
on the Barbara image, but consistency less well than GSM on the boat image.

The GSM model aims at capturing the same statistical regularities as the PoEdges but using
a very different modelling paradigm: where PoEdges is best interpreted as a bottom-up con-
straint satisfaction model, the GSM is a causal generative model with top-down semantics.
We have found that these two modelling paradigms exhibit different denoising accuracies

2Personal communication
3 http://www.kyb.mpg.de/∼pgehler

(d)
(c)
(b)
(a)
Figure 4: Comparison between (c) GSM with 3 × 3+parent [3] (PSNR 29.13) and (d) edge-pert
denoiser with parameter settings as described in the text (PSNR 29.69) on Barbara image (cropped
to 150 × 150 to enhance artifacts). Noisy image (b) has PSNR 20.17. Although the results turn out
very similar, the GSM seems to be slightly less blurry at the expense of introducing more artifacts.

on some types of images implying an opportunity for further study and improvement.

The model in Eqn.3 can be extended in a number of ways. For example, we can lift the
restriction on βj = 2, allow more basis-vectors ˆaj than coefﬁcients or extend the neighbor-
hood selection to subbands of different scales and/or orientations. More substantial perfor-
mance gains are expected if we can extend the single edge-pert case to a multi edge-pert
model. However, approximations in the estimation of these models will become necessary
to keep the denoising algorithm practical. The adaptation of α relies on empirical esti-
mations of the fourth moment and is therefore very sensitive to outliers. We are currently
investigating more robust estimators to ﬁt α.

Further performance gains may still be expected through the development of new wavelet
pyramids and through modelling of new dependency structures such as the phenomenon of
phase alignment at the edges.

Acknowledgments We would like to thank the authors of [2] and [3] for making their
code available online.

References
[1] J. Huang and D. Mumford. Statistics of natural images and models. In Proc. of the Conf. on
Computer Vision and Pattern Recognition, pages 1541–1547, Ft. Collins, CO, USA, 1999.
[2] L. Sendur and I.W. Selesnick. Bivariate shrinkage with local variance estimation. IEEE Signal
Processing Letters, 9(12):438–441, 2002.
[3] J. Portilla, V. Strela, M. Wainwright, and E. P. Simoncelli. Image denoising using scale mixtures
of Gaussians in the wavelet domain. IEEE Trans Image Processing, 12(11):1338–1351, 2003.
[4] E.P. Simoncelli and W.T. Freeman. A ﬂexible architecture for multi-scale derivative computation.
In IEEE Second Int’l Conf on Image Processing, Washington DC, 1995.
[5] E.P. Simoncelli. Modeling the joint statistics of images in the wavelet domain. In Proc SPIE,
44th Annual Meeting, volume 3813, pages 188–195, Denver, 1999.
[6] G.E. Hinton and Y.W. Teh. Discovering multiple constraints that are frequently approximately
satisﬁed. In Proc. of the Conf. on Uncertainty in Artiﬁcial Intelligence , pages 227–234, 2001.
[7] E.P. Simoncelli and E.H. Adelson. Noise removal via bayesian wavelet coring. In 3rd IEEE Int’l
Conf on Image Processing, Laussanne Switzerland, 1996.
[8] X. Li and M.T. Orchard. Spatially adaptive image denoising under over-complete expansion. In
IEEE Int’l. conf. on Image Processing, Vancouver, BC, 2000.
[9] M. Kivanc, I. Kozintsev, K. Ramchandran, and P. Moulin. Low-complexity image denoising
based on statistical modeling of wavelet coefﬁcients.
IEEE Signal Proc. Letters, 6:300–303,
1999.


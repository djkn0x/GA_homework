Recovery of Jointly Sparse Signals
from Few Random Projections

Michael B. Wakin
ECE Department
Rice University
wakin@rice.edu

Marco F. Duarte
ECE Department
Rice University
duarte@rice.edu

Shriram Sarvotham
ECE Department
Rice University
shri@rice.edu

Dror Baron
ECE Department
Rice University
drorb@rice.edu

Richard G. Baraniuk
ECE Department
Rice University
richb@rice.edu

Abstract

Compressed sensing is an emerging ﬁeld based on the revelati on that a small group
of linear projections of a sparse signal contains enough information for reconstruc-
tion. In this paper we introduce a new theory for distributed compressed sensing
(DCS) that enables new distributed coding algorithms for multi-signal ensembles
that exploit both intra- and inter-signal correlation structures. The DCS theory rests
on a new concept that we term the joint sparsity of a signal ensemble. We study
three simple models for jointly sparse signals, propose algorithms for joint recov-
ery of multiple signals from incoherent projections, and characterize theoretically
and empirically the number of measurements per sensor required for accurate re-
construction. In some sense DCS is a framework for distributed compression of
sources with memory, which has remained a challenging problem in information
theory for some time. DCS is immediately applicable to a range of problems in
sensor networks and arrays.

1
Introduction
Distributed communication, sensing, and computing [13, 17] are emerging ﬁelds with nu-
merous promising applications. In a typical setup, large groups of cheap and individu-
ally unreliable nodes may collaborate to perform a variety of data processing tasks such
as sensing, data collection, classi ﬁcation, modeling, tra cking, and so on. As individual
nodes in such a network are often battery-operated, power consumption is a limiting fac-
tor, and the reduction of communication costs is crucial.
In such a setting, distributed
source coding [8, 13, 14, 17] may allow the sensors to save on communication costs. In the
Slepian-Wolf framework for lossless distributed coding [8, 14], the availability of corre-
lated side information at the decoder enables the source encoder to communicate losslessly
at the conditional entropy rate, rather than the individual entropy. Because sensor networks
and arrays rely on data that often exhibit strong spatial correlations [13, 17], distributed
compression can reduce the communication costs substantially, thus enhancing battery life.
Unfortunately, distributed compression schemes for sources with memory are not yet ma-
ture [8, 13, 14, 17].

We propose a new approach for distributed coding of correlated sources whose signal cor-
relations take the form of a sparse structure. Our approach is based on another emerging
ﬁeld known as compressed sensing (CS) [4, 9]. CS builds upon the groundbreaking work
of Cand `es et al. [4] and Donoho [9], who showed that signals that are sparse relative to a
known basis can be recovered from a small number of nonadaptive linear projections onto
a second basis that is incoherent with the ﬁrst. (A random bas is provides such incoherence
with high probability. Hence CS with random projections is universal — the signals can
be reconstructed if they are sparse relative to any known basis.) The implications of CS
for signal acquisition and compression are very promising. With no a priori knowledge of
a signal’s structure, a sensor node could simultaneously acquire and compress that signal,
preserving the critical information that is extracted only later at a fusion center.
In our framework for distributed compressed sensing (DCS), this advantage is particularly
compelling. In a typical DCS scenario, a number of sensors measure signals that are each
individually sparse in some basis and also correlated from sensor to sensor. Each sensor
independently encodes its signal by projecting it onto another, incoherent basis (such as a
random one) and then transmits just a few of the resulting coefﬁcients to a single collection
point. Under the right conditions, a decoder at the collection point can reconstruct each of
the signals precisely. The DCS theory rests on a concept that we term the joint sparsity of a
signal ensemble. We study in detail three simple models for jointly sparse signals, propose
tractable algorithms for joint recovery of signal ensembles from incoherent projections, and
characterize theoretically and empirically the number of measurements per sensor required
for reconstruction. While the sensors operate entirely without collaboration, joint decoding
can recover signals using far fewer measurements per sensor than would be required for
separable CS recovery. This paper presents our speci ﬁc resu lts for one of the three models;
the other two are highlighted in our papers [1, 2, 11].

2 Sparse Signal Recovery from Incoherent Projections
In the traditional CS setting, we consider a single signal x ∈ RN , which we assume to be
sparse in a known orthonormal basis or frame Ψ = [ψ1 , ψ2 , . . . , ψN ]. That is, x = Ψθ
for some θ , where kθk0 = K holds.1 The signal x is observed indirectly via an M ×
N measurement matrix Φ, where M < N . We let y = Φx be the observation vector,
consisting of the M inner products of the measurement vectors against the signal. The M
rows of Φ are the measurement vectors, against which the signal is projected. These rows
are chosen to be incoherent with Ψ — that is, they each have non-sparse expansions in
the basis Ψ [4, 9]. In general, Φ meets the necessary criteria when its entries are drawn
randomly, for example independent and identically distributed (i.i.d.) Gaussian.
Although the equation y = Φx is underdetermined, it is possible to recover x from y under
certain conditions. In general, due to the incoherence between Φ and Ψ, θ can be recovered
by solving the `0 optimization problem
bθ = arg min kθk0
In principle, remarkably few random measurements are required to recover a K -sparse
signal via `0 minimization. Clearly, more than K measurements must be taken to avoid
ambiguity; in theory, K + 1 random measurements will sufﬁce [2]. Unfortunately, solvi ng
this `0 optimization problem appears to be NP-hard [6], requiring a combinatorial enumer-
ation of the (cid:0)N
K (cid:1) possible sparse subspaces for θ .
The amazing revelation that supports the CS theory is that a much simpler problem yields
an equivalent solution (thanks again to the incoherence of the bases): we need only solve

s.t. y = ΦΨθ .

1The `0 “norm”
kθk0 merely counts the number of nonzero entries in the vector θ . CS theory also
applies to signals for which kθkp ≤ K , where 0 < p ≤ 1; such extensions for DCS are a topic of
ongoing research.

s.t. y = ΦΨθ .

for the `1 -sparsest vector θ that agrees with the observed coefﬁcients y [4, 9]
bθ = arg min kθk1
This optimization problem, known also as Basis Pursuit (BP) [7], is signi ﬁcantly more
tractable and can be solved with traditional linear programming techniques. There is no
free lunch, however; more than K + 1 measurements will be required in order to recover
sparse signals. In general, there exists a constant oversampling factor c = c(K, N ) such
that cK measurements sufﬁce to recover x with very high probability [4, 9]. Commonly
quoted as c = O(log(N )), we have found that c ≈ log2 (1 + N/K ) provides a useful
rule-of-thumb [2]. At the expense of slightly more measurements, greedy algorithms have
also been developed to recover x from y . One example, known as Orthogonal Matching
Pursuit (OMP) [15], requires c ≈ 2 ln(N ). We exploit both BP and greedy algorithms for
recovering jointly sparse signals.

3
Joint Sparsity Models
In this section, we generalize the notion of a signal being sparse in some basis to the
notion of an ensemble of signals being jointly sparse. We consider three different joint
sparsity models (JSMs) that apply in different situations. In most cases, each signal is itself
sparse, and so we could use the CS framework from above to encode and decode each one
separately. However, there also exists a framework wherein a joint representation for the
ensemble uses fewer total vectors.
We use the following notation for our signal ensembles and measurement model. Denote
the signals in the ensemble by xj , j ∈ {1, 2, . . . , J }, and assume that each signal xj ∈ RN .
We assume that there exists a known sparse basis Ψ for RN in which the xj can be sparsely
represented. Denote by Φj the measurement matrix for signal j ; Φj is Mj × N and, in
general, the entries of Φj are different for each j . Thus, yj = Φj xj consists of Mj < N
incoherent measurements of xj .
JSM-1: Sparse common component + innovations. In this model, all signals share a
common sparse component while each individual signal contains a sparse innovation com-
ponent; that is,

xj = zC + zj ,

j ∈ {1, 2, . . . , J }

with

zj = Ψθj , kθj k0 = Kj .
zC = ΨθC , kθC k0 = K
and
Thus, the signal zC is common to all of the xj and has sparsity K in basis Ψ. The signals
zj are the unique portions of the xj and have sparsity Kj in the same basis. A practical
situation well-modeled by JSM-1 is a group of sensors measuring temperatures at a number
of outdoor locations throughout the day. The temperature readings xj have both temporal
(intra-signal) and spatial (inter-signal) correlations. Global factors, such as the sun and
prevailing winds, could have an effect zC that is both common to all sensors and structured
enough to permit sparse representation. More local factors, such as shade, water, or ani-
mals, could contribute localized innovations zj that are also structured (and hence sparse).
Similar scenarios could be imagined for a network of sensors recording other phenomena
that change smoothly in time and in space and thus are highly correlated.
JSM-2: Common sparse supports. In this model, all signals are constructed from the
same sparse set of basis vectors, but with different coefﬁci ents; that is,

xj = Ψθj ,
where each θj is supported only on the same Ω ⊂ {1, 2, . . . , N } with |Ω| = K . Hence,
all signals have `0 sparsity of K , and all are constructed from the same K basis elements,
but with arbitrarily different coefﬁcients. A practical si
tuation well-modeled by JSM-2
is where multiple sensors acquire the same signal but with phase shifts and attenuations

j ∈ {1, 2, . . . , J },

caused by signal propagation. In many cases it is critical to recover each one of the sensed
signals, such as in many acoustic localization and array processing algorithms. Another
useful application for JSM-2 is MIMO communication [16].
JSM-3: Nonsparse common + sparse innovations. This model extends JSM-1 so that
the common component need no longer be sparse in any basis; that is,

xj = zC + zj ,

j ∈ {1, 2, . . . , J }

with

zj = Ψθj , kθj k0 = Kj ,
and
zC = ΨθC
but zC is not necessarily sparse in the basis Ψ. We also consider the case where the supports
of the innovations are shared for all signals, which extends JSM-2. A practical situation
well-modeled by JSM-3 is where several sources are recorded by different sensors together
with a background signal that is not sparse in any basis. Consider, for example, a computer
vision-based veri ﬁcation system in a device production pla nt. Cameras acquire snapshots
of components in the production line; a computer system then checks for failures in the
devices for quality control purposes. While each image could be extremely complicated,
the ensemble of images will be highly correlated, since each camera is observing the same
device with minor (sparse) variations. JSM-3 could also be useful in some non-distributed
scenarios. For example, it motivates the compression of data such as video, where the
innovations or differences between video frames may be sparse, even though a single frame
may not be very sparse. In general, JSM-3 may be invoked for ensembles with signi ﬁcant
inter-signal correlations but insigni ﬁcant intra-signal correlations.

4 Recovery of Jointly Sparse Signals
In a setting where a network or array of sensors may encounter a collection of jointly
sparse signals, and where a centralized reconstruction algorithm is feasible, the number
of incoherent measurements required by each sensor can be reduced. For each JSM, we
propose algorithms for joint signal recovery from incoherent projections and characterize
theoretically and empirically the number of measurements per sensor required for accurate
reconstruction. We focus in particular on JSM-3 in this paper but also overview our results
for JSMs 1 and 2, which are discussed in further detail in our papers [1, 2, 11].

4.1
JSM-1: Sparse common component + innovations
For this model (see also [1, 2]), we have proposed an analytical framework inspired by the
principles of information theory. This allows us to characterize the measurement rates Mj
required to jointly reconstruct the signals xj . The measurement rates relate directly to the
signals’ conditional sparsities, in parallel with the Slepian-Wolf theory. More speci ﬁcall y,
we have formalized the following intuition. Consider the simple case of J = 2 signals. By
employing the CS machinery, we might expect that (i) (K + K1 )c coefﬁcients sufﬁce to
reconstruct x1 , (ii) (K +K2 )c coefﬁcients sufﬁce to reconstruct
x2 , yet only (iii) (K +K1 +
K2 )c coefﬁcients should sufﬁce to reconstruct both
x1 and x2 , since we have K + K1 + K2
nonzero elements in x1 and x2 . In addition, given the (K + K1 )c measurements for x1
as side information, and assuming that the partitioning of x1 into zC and z1 is known,
cK2 measurements that describe z2 should allow reconstruction of x2 . Formalizing these
arguments allows us to establish theoretical lower bounds on the required measurement
rates at each sensor; Fig.1(a) shows such a bound for the case of J = 2 signals.
We have also established upper bounds on the required measurement rates Mj by proposing
a speci ﬁc algorithm for reconstruction [1]. The algorithm u ses carefully designed measure-
ment matrices Φj (in which some rows are identical and some differ) so that the resulting
measurements can be combined to allow step-by-step recovery of the sparse components.
The theoretical rates Mj are below those required for separable CS recovery of each signal
xj (see Fig. 1(a)). We also proposed a reconstruction technique based on a single exe-
cution of a linear program, which seeks the sparsest components [zC ; z1 ;
. . . zJ ] that

1

0.9

0.8

0.7

0.6

2
R

0.5

0.4

0.3

0.2

0.1

Simulation

Converse
Anticipated
Achievable
Separate

n = 50, k = 5

32
16

8

4

2

1

2

4

8

16

32

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
c
u
r
t
s
n
o
c
e
r
 
t
c
a
x
e
 
f
o
 
.
b
o
r
P

0
0

0.2

0.4

0.6

0.8

0
0

1

5

25

10
15
20
(a)
(b)
R
Number of measurements per sensor
1
Figure 1: (a) Converse bounds and achievable measurement rates for J = 2 signals with common
sparse component and sparse innovations (JSM-1). We ﬁx signal len gths N = 1000 and sparsities
K = 200, K1 = K2 = 50. The measurement rates Rj := Mj /N reﬂect the number of measure-
mentsnormalizedby the signal length. Blue curves indicateour theoretical and anticipated converse
bounds; red indicates aprovably achievable region, andpinkdenotes the rates required for separable
CS signal reconstruction. (b)Reconstructing a signal ensemblewith common sparse supports (JSM-
2). We plot the probability of perfect reconstruction via DCS-SOMP (solid lines) and independent
CS reconstruction (dashed lines) as a function of the number ofmeasurements per signal M and the
number of signals J . We ﬁx the signal length to N = 50 and the sparsity to K = 5. An oracle
encoder thatknows thepositionsof the large coefﬁcientswoulduse 5 measurementsper signal.

30

account for the observed measurements. Numerical simulations support such an approach
(see Fig.1(a)). Future work will extend JSM-1 to `p -compressible signals, 0 < p ≤ 1.

4.2

JSM-2: Common sparse supports

Under the JSM-2 signal ensemble model (see also [2, 11]), independent recovery of each
signal via `1 minimization would require cK measurements per signal. However, algo-
rithms inspired by conventional greedy pursuit algorithms (such as OMP [15]) can sub-
stantially reduce this number.
In the single-signal case, OMP iteratively constructs the
sparse support set Ω; decisions are based on inner products between the columns of ΦΨ
and a residual. In the multi-signal case, there are more clues available for determining the
elements of Ω.
To establish a theoretical justi ﬁcation for our approach, w e ﬁrst proposed a simple One-
Step Greedy Algorithm (OSGA) [11] that combines all of the measurements and seeks the
largest correlations with the columns of the Φj Ψ. We established that, assuming that Φj
has i.i.d. Gaussian entries and that the nonzero coefﬁcient s in the θj are i.i.d. Gaussian, then
with M ≥ 1 measurements per signal, OSGA recovers Ω with probability approaching 1
as J → ∞. Moreover, with M ≥ K measurements per signal, OSGA recovers all xj with
probability approaching 1 as J → ∞. This meets the theoretical lower bound for Mj .
In practice, OSGA can be improved using an iterative greedy algorithm. We proposed a
simple variant of Simultaneous Orthogonal Matching Pursuit (SOMP) [16] that we term
DCS-SOMP [11]. For this algorithm, Fig. 1(b) plots the performance as the number of
sensors varies from J = 1 to 32. We ﬁx the signal lengths at N = 50 and the sparsity of
each signal to K = 5. With DCS-SOMP, for perfect reconstruction of all signals the aver-
age number of measurements per signal decreases as a function of J . The trend suggests
that, for very large J , close to K measurements per signal should sufﬁce. On the contrary,
with independent CS reconstruction, for perfect reconstruction of all signals the number of
measurements per sensor increases as a function of J . This surprise is due to the fact that
each signal will experience an independent probability p ≤ 1 of successful reconstruction;
therefore the overall probability of complete success is pJ . Consequently, each sensor must
compensate by making additional measurements.

JSM-3: Nonsparse common + sparse innovations
4.3
The JSM-3 signal ensemble model provides a particularly compelling motivation for joint
recovery. Under this model, no individual signal xj is sparse, and so separate signal recov-
ery would require fully N measurements per signal. As in the other JSMs, however, the
commonality among the signals makes it possible to substantially reduce this number.
Our recovery algorithms are based on the observation that if the common component zC
were known, then each innovation zj could be estimated using the standard single-signal
CS machinery on the adjusted measurements yj − Φj zC = Φj zj . While zC is not known in
advance, it can be estimated from the measurements. In fact, across all J sensors, a total of
Pj Mj random projections of zC are observed (each corrupted by a contribution from one
of the zj ). Since zC is not sparse, it cannot be recovered via CS techniques, but when the
number of measurements is sufﬁciently large ( Pj Mj (cid:29) N ), zC can be estimated using
standard tools from linear algebra. A key requirement for such a method to succeed in
recovering zC is that each Φj be different, so that their rows combine to span all of RN . In
the limit, zC can be recovered while still allowing each sensor to operate at the minimum
measurement rate dictated by the {zj }. A prototype algorithm, which we name Transpose
Estimation of Common Component (TECC), is listed below, where we assume that each
measurement matrix Φj has i.i.d. N (0, σ2
j ) entries.
TECC Algorithm for JSM-3
1. Estimate common component: Deﬁne the matrix bΦ as the concatenation of the regu-
Φj , that is, bΦ = [ bΦ1 , bΦ2 , . . . , bΦJ ].
larized individual measurement matrices bΦj = 1
Mj σ2
j
J bΦT y .
Calculate the estimate of the common component as czC = 1
2. Estimate measurements generated by innovations: Using the previous estimate, sub-
tract the contribution of the common part on the measurements and generate estimates
for the measurements caused by the innovations for each signal: byj = yj − Φj czC .
3. Reconstruct innovations: Using a standard single-signal CS reconstruction algorithm,
obtain estimates of the innovations bzj from the estimated innovation measurements byj .
4. Obtain signal estimates: Sum the above estimates, letting bxj = czC + bzj .
The following theorem shows that asymptotically, by using the TECC algorithm, each
sensor need only measure at the rate dictated by the sparsity Kj .

Theorem 1 [2] Assume that the nonzero expansion coefﬁcients of the sparse innovations
zj are i.i.d. Gaussian random variables and that their locations are uniformly distributed
on {1, 2, ..., N }. Then the following statements hold:
1. Let the measurement matrices Φj contain i.i.d. N (0, σ2
j ) entries with Mj ≥ Kj +
1. Then each signal xj can be recovered using the TECC algorithm with probability
approaching 1 as J → ∞.
2. Let Φj be a measurement matrix with Mj ≤ Kj for some j ∈ {1, 2, ..., J }. Then with
probability 1, the signal xj cannot be uniquely recovered by any algorithm for any J .

For large J , the measurement rates permitted by Statement 1 are the lowest possible for any
reconstruction strategy on JSM-3 signals, even neglecting the presence of the nonsparse
component. Thus, Theorem 1 provides a tight achievable and converse for JSM-3 signals.
The CS technique employed in Theorem 1 involves combinatorial searches for estimating
the innovation components. More efﬁcient techniques could also be employed (including
several proposed for CS in the presence of noise [3, 5, 7, 10, 12]).
While Theorem 1 suggests the theoretical gains from joint recovery as J → ∞, practical
gains can also be realized with a moderate number of sensors. For example, suppose in
the TECC algorithm that the initial estimate czC is not accurate enough to enable correct

identi ﬁcation of the sparse innovation supports {Ωj }. In such a case, it may still be possible
for a rough approximation of the innovations {zj } to help reﬁne the estimate czC . This in
turn could help to reﬁne the estimates of the innovations. Si nce each component helps to
estimate the others, we propose an iterative algorithm for JSM-3 recovery. The Alternating
Common and Innovation Estimation (ACIE) algorithm exploits the observation that once
the basis vectors comprising the innovation zj have been identi ﬁed in the index set Ωj ,
their effect on the measurements yj can be removed to aid in estimating zC .
ACIE Algorithm for JSM-3
1. Initialize: Set bΩj = ∅ for each j . Set the iteration counter ` = 1.
be the Mj × | bΩj | submatrix obtained
2. Estimate common component: Let Φj, bΩj
by sampling the columns bΩj from Φj and construct an Mj × (Mj − | bΩj |) matrix
Qj = [qj,1 . . . qj,Mj −| bΩj | ] having orthonormal columns that span the orthogonal com-
plement of colspan(Φj, bΩj
). Remove the projection of the measurements into the afore-
mentioned span to obtain measurements caused exclusively by vectors not in bΩj , letting
j Φj . Use the modi ﬁed measurements eY = (cid:2)eyT
J (cid:3)T
j yj and eΦj = QT
eyj = QT
1 eyT
2 . . . eyT
J iT
and modi ﬁed holographic basis eΦ = h eΦT
2 . . . eΦT
1 eΦT
to reﬁne the estimate of the
measurements caused by the common part of the signal, setting fzC = eΦ† eY , where
A† = (AT A)−1AT denotes the pseudoinverse of matrix A.
3. Estimate innovation supports: For each signal j , subtract fzC from the measurements,
byj = yj − Φj fzC , and estimate the sparse support of each innovation bΩj .
4. Iterate: If ` < L, a preset number of iterations, then increment ` and return to Step 2.
Otherwise proceed to Step 5.
5. Estimate innovation coef ﬁcients: For each signal j , estimate the coefﬁcients for the
(yj − Φj fzC ), where bθj, bΩj
indices in bΩj , setting bθj, bΩj
= Φ†
is a sampled version of
j, bΩj
the innovation’s sparse coefﬁcient vector estimate bθj .
6. Reconstruct signals: Estimate each signal as bxj = fzC + bzj = fzC + Φj bθj .
In the case where the innovation support estimate is correct ( bΩj = Ωj ), the measurements
eyj will describe only the common component zC . If this is true for every signal j and the
number of remaining measurements Pj Mj −K J ≥ N , then zC can be perfectly recovered
in Step 2. Because it may be difﬁcult to correctly obtain all Ωj in the ﬁrst iteration, we ﬁnd
it preferable to run the algorithm for several iterations.
Fig. 2(a) shows that, for sufﬁciently large J , we can recover all of the signals with signi ﬁ-
cantly fewer than N measurements per signal. We note the following behavior in the graph.
First, as J grows, it becomes more difﬁcult to perfectly reconstruct al
l J signals. We be-
lieve this is inevitable, because even if zC were known without error, then perfect ensemble
recovery would require the successful execution of J independent runs of OMP. Second,
for small J , the probability of success can decrease at high values of M . We believe this is
due to the fact that initial errors in estimating zC may tend to be somewhat sparse (since czC
roughly becomes an average of the signals {xj }), and these sparse errors can mislead the
subsequent OMP processes. For more moderate M , it seems that the errors in estimating
zC (though greater) tend to be less sparse. We expect that a more sophisticated algorithm
could alleviate such a problem, and we note that the problem is also mitigated at higher J .
Fig. 2(b) shows that when the sparse innovations share common supports we see an even
greater savings. As a point of reference, a traditional approach to signal encoding would
require 1600 total measurements to reconstruct these J = 32 nonsparse signals of length
N = 50. Our approach requires only about 10 per sensor for a total of 320 measurements.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
c
u
r
t
s
n
o
c
e
R
 
t
c
a
x
E
 
f
o
 
y
t
i
l
i
b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
c
u
r
t
s
n
o
c
e
R
 
t
c
a
x
E
 
f
o
 
y
t
i
l
i
b
a
b
o
r
P

 8
16
32

 8
16
32

0
0

50

10
20
30
40
Number of Measurements per Signal, M

(a)
(b)
Figure 2: Reconstructing a signal ensemble with nonsparse common component and sparse inno-
vations (JSM-3) usingACIE. (a) Reconstruction usingOMP independently on each signal in Step 3
of the ACIE algorithm (innovations have arbitrary supports). (b) Reconstruction using DCS-SOMP
jointly on all signals in Step 3 of the ACIE algorithm (innovations have identical supports). Signal
length N = 50, sparsity K = 5. The common structure exploited byDCS-SOMP enables dramatic
savings in thenumberofmeasurements. We averageover1000 simulation runs.

35

0
0

5

10
15
20
25
Number of Measurements per Signal, M

30

Acknowledgments: Thanks to Emmanuel Cand `es, Hyeokho Choi, and Joel Tropp for in-
formative and inspiring conversations.

References

[1] D. Baron, M. F. Duarte, S. Sarvotham, M. B. Wakin, and R. G. Baraniuk. An information-
theoretic approach to distributed compressed sensing. In Allerton Conf. Comm., Control, Com-
put., Sept. 2005.
[2] D. Baron, M. B. Wakin, M. F. Duarte, S. Sarvotham, and R. G. Baraniuk. Distributed com-
pressed sensing. 2005. Preprint. Available at www.dsp.rice.edu/cs.
[3] E. Cand `es, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate
measurements. Comm. Pure Applied Mathematics, 2005. To appear.
[4] E. Cand `es and T. Tao. Near optimal signal recovery from random projections and universal
encoding strategies. 2004. Preprint.
[5] E. Cand `es and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than
n. 2005. Preprint.
[6] E. Cand `es and T. Tao. Error correction via linear programming. 2005. Preprint.
[7] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal
on Scientiﬁc Computing , 20(1):33–61, 1998.
[8] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, New York, 1991.
[9] D. Donoho. Compressed sensing. 2004. Preprint.
[10] D. Donoho and Y. Tsaig. Extensions of compressed sensing. 2004. Preprint.
[11] M. F. Duarte, S. Sarvotham, D. Baron, M. B. Wakin, and R. G. Baraniuk. Distributed com-
pressed sensing of jointly sparse signals. In Asilomar Conf. Signals, Sys., Comput., Nov. 2005.
[12] J. Haupt and R. Nowak. Signal reconstruction from noisy random projections. 2005. Preprint.
[13] S. Pradhan and K. Ramchandran. Distributed source coding using syndromes (DISCUS): De-
sign and construction. IEEE Trans. Inform. Theory, 49:626–643, March 2003.
[14] D. Slepian and J. K. Wolf. Noiseless coding of correlated information sources. IEEE Trans.
Inform. Theory, 19:471–480, July 1973.
[15] J. Tropp and A. C. Gilbert. Signal recovery from partial information via orthogonal matching
pursuit. 2005. Preprint.
[16] J. Tropp, A. C. Gilbert, and M. J. Strauss. Simulataneous sparse approximation via greedy
pursuit. In IEEE 2005 Int. Conf. Acoustics, Speech, Signal Processing, March 2005.
[17] Z. Xiong, A. Liveris, and S. Cheng. Distributed source coding for sensor networks. IEEE Signal
Proc. Mag., 21:80–94, September 2004.


A Theoretical Analysis of Robust Coding over
Noisy Overcomplete Channels

Eizaburo Doi1 , Doru C. Balcan2 , & Michael S. Lewicki1,2
1Center for the Neural Basis of Cognition,
2Computer Science Department,
Carnegie Mellon University, Pittsburgh, PA 15213
{edoi,dbalcan,lewicki}@cnbc.cmu.edu

Abstract

Biological sensory systems are faced with the problem of encoding a
high-ﬁdelity sensory signal with a population of noisy, low-ﬁdelity neu-
rons. This problem can be expressed in information theoretic terms as
coding and transmitting a multi-dimensional, analog signal over a set of
noisy channels. Previously, we have shown that robust, overcomplete
codes can be learned by minimizing the reconstruction error with a con-
straint on the channel capacity. Here, we present a theoretical analysis
that characterizes the optimal linear coder and decoder for one- and two-
dimensional data. The analysis allows for an arbitrary number of coding
units, thus including both under- and over-complete representations, and
provides a number of important insights into optimal coding strategies.
In particular, we show how the form of the code adapts to the number
of coding units and to different data and noise conditions to achieve ro-
bustness. We also report numerical solutions for robust coding of high-
dimensional image data and show that these codes are substantially more
robust compared against other image codes such as ICA and wavelets.

1 Introduction

In neural systems, the representational capacity of a single neuron is estimated to be as
low as 1 bit/spike [1, 2]. The characteristics of the optimal coding strategy under such
conditions, however, remains an open question. Recent efﬁcient coding models for sensory
coding such as sparse coding and ICA have provided many insights into visual sensory
coding (for a review, see [3]), but those models made the implicit assumption that the
representational capacity of individual neurons was inﬁnite. Intuitively, such a limit on
representational precision should strongly inﬂuence the form of the optimal code. In par-
ticular, it should be possible to increase the number of limited capacity units in a population
to form a more precise representation of the sensory signal. However, to the best of our
knowledge, such a code has not been characterized analytically, even in the simplest case.
Here we present a theoretical analysis of this problem for one- and two-dimensional data
for arbitrary numbers of units. For simplicity, we assume that the encoder and decoder
are both linear, and that the goal is to minimize the mean squared error (MSE) of the
reconstruction.
In contrast to our previous report, which examined noisy overcomplete

representations [4], the cost function does not contain a sparsity prior. This simpliﬁcation
makes the cost depend up to second order statistics, making it analytically tractable while
preserving the robustness to noise.

2 The model

To deﬁne our model, we assume that the data is N -dimensional, has zero mean and covari-
ance matrix Σx , and deﬁne two matrices W ∈ RM ×N and A ∈ RN ×M . For each data
point x, its representation r in the model is the linear transform of x through matrix W,
perturbed by the additive noise (i.e., channel noise) n ∼ N (0, σ2
n IM ):
r = Wx + n = u + n.
(1)
We refer to W as the encoding matrix and its row vectors as encoding vectors. The recon-
struction of a data point from its representation is simply the linear transform of the latter,
using matrix A:

ˆx = Ar = AWx + An.
(2)
We refer to A as the decoding matrix and its column vectors as decoding vectors. The term
AWx in eq. 2 determines how the reconstruction depends on the data, while An reﬂects
the channel noise in the reconstruction. When there is no channel noise (n = 0), AW = I
is equivalent to perfect reconstruction. A graphical description of this system is shown in
Fig. 1.

Figure 1: Diagram of the model.

The goal of the system is to form an accurate representation of the data that is robust
to the presence of channel noise. We quantify the accuracy of the reconstruction by the
mean squared error (MSE) over a set of data. The error of each sample is  = x − ˆx =
(IN − AW)x − An, and the MSE is expressed in matrix form:
E (A, W) = tr{(IN − AW)Σx (IN − AW)T } + σ2
n tr{AAT },
(3)
where we used E = hT i = tr(hT i). Note that, due to the MSE objective along with
the zero-mean assumptions, the optimal solution depends solely on second-order statistics
of the data and the noise.
Since the SNR is limited in the neural representation [1, 2], we assume that each coding
unit has a limited variance hu2
i i = σ2
u so that the SNR is limited to the same constant value
2 ln(γ 2 + 1),
γ 2 = σ2
n . As the channel capacity of information is deﬁned by C = 1
u /σ2
this is equivalent to limiting the capacity of each unit to the same level. We will call this
constraint as channel capacity constraint.
Now our problem is to minimize eq. 3 under the channel capacity constraint. To solve
it, we will include this constraint in the parametrization of W. Let Σx = EDET be
√
√
the eigenvalue decomposition of the data covariance matrix, and denote S = D 1
2 =
λM ), where λi ≡ Dii are the Σx ’s eigenvalues. As we will see shortly,
λ1 , · · · ,
diag(
it is convenient to deﬁne V ≡ WES/σu , then the condition hu2
i i = σ2
u implies that
VVT = Cu = huuT i/σ2
(4)
u ,
where Cu is the correlation matrix of the representation u. Now the problem is formulated
as a constrained optimization: ﬁnding the parameters that satisfy eq. 4 and minimize E .

xWurAx^nEncoderDecoderChannel NoiseDataNoisyRepresentationNoiselessRepresentationReconstruction3 The optimal solutions and their characteristics

In this section we analyze the optimal solutions in some simple cases, namely for 1-
dimensional (1-D) and 2-dimensional (2-D) data.

3.1 1-D data

In the 1-D case the MSE (eq. 3) is expressed as
n kak2
x (1 − aw)2 + σ2
E = σ2
(5)
2 ,
x = Σx ∈ R, a = A ∈ R1×M and w = W ∈ RM ×1 . By solving the necessary
where σ2
condition for the minimum, ∂ E /∂a = 0, with the channel capacity constraint (eq. 4), the
entries of the optimal solutions are
wi = ± σu
σx
and the smallest value of the MSE is
E =

γ 2
M · γ 2 + 1 ,

, ai =

1
wi

(6)

(7)

·

σ2
x
M · γ 2 + 1 .

This minimum depends on the SNR (γ 2 ) and on the number of units (M ), and it is monoton-
ically decreasing with respect to both. Furthermore, we can compensate for a decrease in
SNR by an increase of the number of units. Note that ai are responsible for this adaptive
behavior as wi do not vary with either γ 2 or M , in the 1-D case. The second term in eq. 5
leads the optimal a into having as small norm as possible, while the ﬁrst term prevents it
from being arbitrarily small. The optimum is given by the best trade-off between them.

3.2 2-D data

(8)

V =

 cos θ1
 ,
In the 2-D case, the channel capacity constraint (eq. 4) restricts V such that the row vectors
of V should be on the unit circle. Therefore V can be parameterized as
sin θ1
...
...
cos θM sin θM
where θi ∈ [0, 2π) is the angle between i-th row of V and the principal eigenvector of
the data e1 (E = [e1 , e2 ], λ1 ≥ λ2 > 0). The necessary condition for the minimum
∂ E /∂A = O implies
n IM )−1 .
uVVT + σ2
A = σuESVT (σ2
M γ 2 + 1(cid:1) − γ 2
(λ1 + λ2 ) (cid:0) 2
Using eqs. 8 and 9, the MSE can be expressed as
(cid:0) M
2 γ 2 + 1(cid:1)2 − 1
2 (λ1 − λ2 ) Re(Z )
E =
4 γ 4 |Z |2
k=1 zk = PM
Z = PM
where by deﬁnition
k=1 [cos(2θk ) + i sin(2θk )].
(11)
Now the problem has been reduced to ﬁnding simply a complex number Z that minimizes
E . Note that Z deﬁnes θk in V, which in turn deﬁnes W (by deﬁnition; see eq. 4) and A
(eq. 9). In the following we analyze the problem in two complementary cases: when the
data variance is isotropic (i.e., λ1 = λ2 ), and when it is anisotropic (λ1 > λ2 ). As we will
see, the solutions are qualitatively different in these two cases.

(10)

(9)

,

E =

3.2.1 Isotropic case
Isotropy of the data variance implies λ1 = λ2 ≡ σ2
x , and (without loss of generality) E = I,
(cid:0)1 + M
2 γ 2 (cid:1)
which simpliﬁes the MSE (eq. 10) as
M γ 2 + 1(cid:1)2 − 1
(cid:0) 2
2σ2
x
4 γ 4 |Z |2
Therefore, E is minimized whenever |Z |2 is minimized.
If M = 1, |Z |2 = |z1 |2 is always 1 by deﬁnition (eq. 11), yielding the optimal solutions
γ 2
W = σu
V, A = σx
(13)
γ 2 + 1
σu
σx
where V = V(θ1 ), ∀ θ1 ∈ [0, 2π). Eq. 13 means that the orientation of the encoding and
decoding vectors is arbitrary, and that the length of those vectors is adjusted exactly as in
the 1-D case (eq. 6 with M = 1; Fig. 2). The minimum MSE is given by

VT ,

(12)

.

·

E = σ2
x
γ 2 + 1

+ σ2
x .

(14)

The ﬁrst term is the same as in the 1-D case (eq. 7 with M = 1), corresponding to the error
component along the axis that the encoding/decoding vectors represent, while the second
term is the whole data variance along the axis orthogonal to the encoding/decoding vectors,
along which no reconstruction is made.
If M ≥ 2, there exists a set of angles θk for which |Z |2 is 0. This can be veriﬁed by repre-
senting Z in the complex plane (Z-diagram in Fig. 2) and observing that there is always a
conﬁguration of connected, unit-length bars that starts from, and ends up at the origin, thus
indicating that Z = |Z |2 = 0. Accordingly, the optimal solution is

·

VT ,

γ 2
W = σu
V, A = σx
2 γ 2 + 1
M
σx
σu
where the optimal V = V(θ1 , · · · , θM ) is given by such θ1 , . . . , θM for which Z = 0.
Speciﬁcally, if M = 2, then z1 and z2 must be antiparallel but are not otherwise constrained,
making the pair of decoding vectors (and that of encoding vectors) orthogonal, yet free to
rotate. Note that both the encoding and the decoding vectors are parallel to the rows of
V (eq. 15), and the angle of zk from the real axis is twice as large as that of ak (or wk ).
Likewise, if M = 3, the decoding vectors should be evenly distributed yet still free to rotate;
if M = 4, the four vectors should just be two pairs of orthogonal vectors (not necessarily
evenly distributed); if M ≥ 5, there is no obvious regularity. With Z = 0, the MSE is
minimized as

(15)

.

E =

2σ2
x
2 γ 2 + 1
M
The minimum MSE (eq. 16) depends on the SNR (γ 2 ) and overcompleteness ratio (M /N )
exactly in the same manner as explained in the 1-D case (eq. 7), considering that in both
cases the numerator is the data variance, tr(Σx ). We present examples in Fig 2: given
M = 2, the reconstruction gets worse by lowering the SNR from 10 to 1; however, the
reconstruction can be improved by increasing the number of units for a ﬁxed SNR (γ 2 = 1).
Just as in the 1-D case, the norm of the decoding vectors gets smaller by increasing M or
decreasing γ 2 , which is explicitly described by eq. 15.

(16)

Figure 2: The optimal solutions for isotropic data. M is the number of units and γ 2 is the
SNR in the representation. “Variance” shows the variance ellipses for the data (gray) and
the reconstruction (magenta). For perfect reconstruction, the two ellipses should overlap.
“Encoding” and “Decoding” show encoding vectors (red) and decoding vectors (blue), re-
spectively. The gray vectors show the principal axes of the data, e1 and e2 . “Z-Diagram”
represents Z = Σk zk (eq. 11) in the complex plane, where each unit length bar corresponds
to a zk , and the end point indicated by “×” represents the coordinates of Z . The set of green
dots in a plot corresponds to optimal values of Z ; when this set reduces to a single dot, the
optimal Z is unique. In general there could be multiple conﬁgurations of bars for a single
Z , implying multiple equivalent solutions of A and W for a given Z . For M = 2 and
γ 2 = 10, we drew with gray dotted bars an example of Z that is not optimal (corresponding
encoding and decoding vectors not shown).

E =

3.2.2 Anisotropic case
In the anisotropic condition λ1 > λ2 , the MSE (eq. 10) is minimized when Z = Re(Z ) ≥ 0
for a ﬁxed value of |Z |2 . Therefore, the problem is reduced to seeking a real value Z =
2 γ 2 + 1(cid:1) − γ 2
(λ1 + λ2 ) (cid:0) M
y ∈ [0, M ] that minimizes
(cid:0) M
2 γ 2 + 1(cid:1)2 − 1
2 (λ1 − λ2 ) y
4 γ 4 y2
If M = 1, then y = cos 2θ1 from eq. 11, and therefore, E in eq. 17 is minimized iff θ1 = 0,
yielding the optimal solutions
√
γ 2
W = σu√
λ1
γ 2 + 1
σu
λ1
In contrast to the isotropic case with M = 1, the encoding and decoding vectors are speci-
ﬁed along the principal axis (e1 ) as illustrated in Fig. 3. The minimum MSE is
E = λ1
γ 2 + 1
This is the same form as in the isotropic case (eq. 14) except that the ﬁrst term is now related
to the variance along the principal axis, λ1 , by which the encoding/decoding vectors can

1 , A =
eT

+ λ2 .

·

e1 .

.

(17)

(18)

(19)

VarianceDecodingEncodingZ-DiagramEncodingM=1M=2M=3M=4M=5γ2=1γ2=10γ2=1γ2=1γ2=1γ2=1M +

y =

,

λ2

W = V

λ1

(20)

(21)

(22)

M +

2
γ 2

− y

= 0.

EVT ,

(23)

ET , A =

most effectively be utilized for representing the data, while the second term is speciﬁed as
the data variance along the minor axis, λ2 , by which the loss of reconstruction is mostly
minimized. Note that it is a similar mechanism of dimensionality reduction as using PCA.
If M ≥ 2, then we can derive the optimal y from the necessary condition for the minimum,
(cid:20) √
(cid:21)
(cid:19)
(cid:21)(cid:20) √
(cid:19)
(cid:18)
(cid:18)
dE /dy = 0, which yields
√
λ1 − √
λ1 +
2
√
λ1 − √
√
√
− y
λ2
λ2
λ1 +
γ 2
λ2
λ2
c = (pλ1 /λ2 − 1)/M .
Let γ 2
c denote the SNR critical point, where
γ 2
(cid:19)
(cid:18) 2
If γ 2 ≥ γ 2
c , then eq. 20 has a root within its domain [0, M ],
λ1 − √
√
√
√
λ2
γ 2 + M
λ1 +
λ2
(cid:18) σu /
(cid:19)
with y = M if γ 2 = γ 2
c . Accordingly the optimal solutions are given by
√
√
√
√
λ1 +
0
γ 2
·
0
2σu
2 γ 2 + 1
λ2
σu /
M
where the optimal V = V(θ1 , · · · , θM ) is given by the Z-diagram as illustrated in Fig. 3,
which we will describe shortly. The minimum MSE is given by
√
√
(
E =

1
2 γ 2 + 1
M
Note that eqs. 23–24 are reduced to eqs. 15–16 if λ1 = λ2 .
c , then dE /dy = 0 does not have a root within the domain.
If the SNR is smaller than γ 2
However, dE /dy is always negative, and hence, E decreases monotonically on [0, M ]. The
minimum is therefore obtained when y = M , yielding the optimal solutions
√
γ 2
W = σu√
·
λ1
1 , A =
1M eT
e11T
M ,
M γ 2 + 1
σu
λ1
where 1M = (1, · · · , 1)T ∈ RM , and the minimum is given by
E =
λ1
+ λ2 .
(26)
M γ 2 + 1
Note that E takes the same form as in M = 1 (eq. 19) except that we can now decrease the
error by increasing the number of units. To summarize, if the representational resource is
too limited either by M or γ 2 , the best strategy is to represent only the principal axis.
Now we describe the optimal solutions using the Z-diagram (Fig. 3). First, the optimal so-
c , the optimal Z is a certain point between 0
lutions differ depending on the SNR. If γ 2 > γ 2
and M on the real axis. Speciﬁcally, for M = 2 the optimal conﬁguration of the unit-length
connected bars is unique (up to ﬂipping about x-axis), meaning that the encoding/decoding
vectors are symmetric about the principal axis; for M ≥ 3, there are inﬁnitely many conﬁg-
urations of the bars starting from the origin and ending at the optimal Z , and nothing can be
added about their regularity. If γ 2 ≤ γ 2
c , the optimal Z is M , and the optimal conﬁguration
is obtained only when all the bars align on the real axis. In this case, encoding/decoding
vectors are all parallel to the principal axis (e1 ), as described by eq. 25. Such a degenerate
representation is unique for the anisotropic case and is determined by γ 2
c (eq. 21). We can

λ1 +
2

λ2 )2

.

(24)

(25)

Figure 3: The optimal solutions for anisotropic data. Notations are as in Fig. 2. We set
c holds for all M ≥ 2 but the one with M= 2 and γ 2 = 1.
λ1 = 1.87 and λ2 = 0.13. γ 2 > γ 2

avoid the degeneration either by increasing the SNR (e.g., Fig. 3, M = 2 with different γ 2 )
or by increasing the number of units (γ 2 = 1 with different M ).
Also, the optimal solutions for the overcomplete representation are, in general, not obtained
by simple replication (except in the degenerate case). For example, for γ 2 = 1 in Fig. 3,
the optimal solution for M = 8 is not identical to the replication of the optimal solution for
M = 2, and we can formally prove it by using eq. 22.
For M = 1 and for the degenerate case, where only one axis in two dimensional space is
represented, the optimal strategy is to preserve information along the principal axis at the
cost of losing all information along the minor axis. Such a biased representation is also
found for the non-degenerate case. We can see in Fig. 3 that the data along the principal
axis is more accurately reconstructed than that along the minor axis; if there is no bias, the
√
√
ellipse for the reconstruction should be similar to that of the data. More precisely, we can
λ2 :
prove that the error ratio along e1 is smaller than that along e2 at the ratio of
λ1
(note the switch of the subscripts), which describes the representation bias toward the main
axis.

4 Application to image coding

In the case of high-dimensional data we can employ an algorithm similar to the one in [4],
to numerically compute optimal solutions that minimizes the MSE subject to the channel
capacity constraint. Fig. 4 presents the performance of our model when applied to image
coding in the presence of channel noise. The data were 8 × 8 pixel blocks taken from a
large image, and for comparison we considered representations with M = 64 (“1×”) and
respectively, 512 (“8×”) units. As for the channel capacity, each unit has 1.0 bit precision
as in the neural representation [1]. The robust coding model shows a dramatic reduction
in the reconstruction error, when compared to alternatives such as ICA and wavelet codes.
This underscores the importance of taking into account the channel capacity constraint for
better understanding the neural representation.

M=1M=2M=3M=8γ2=10γ2=10γ2=2γ2=1γ2=1γ2=1γ2=1VarianceDecodingZ-DiagramEncodingFigure 4: Reconstruction using one bit channel capacity representations. To ensure that
all models had the same precision of 1.0 bit for each coefﬁcient, we added Gaussian noise
to the coefﬁcients of the ICA and “Daubechies 9/7” wavelet codes as in the robust coding.
For each representation, we displayed percentage error of the reconstruction. The results
are consistent using other images, block size, or wavelet ﬁlters.

I (x, ˆx) =

(27)

ln(γ 2 + 1),

5 Discussion
In this study we measured the accuracy of the reconstruction by the MSE. An alternative
measure could be, as in [5, 3], mutual information I (x, ˆx) between the data and the recon-
struction. However, we can prove that this measure does not yield optimal solutions for the
robust coding problem. Assuming the data is Gaussian and the representation is complete,
we can prove that the mutual information is upper-bounded,
1
ln det(γ 2VVT + IN ) ≤ N
2
2
with equality iff VVT = I, i.e., when the representation u is whitened (see eq. 4). This re-
sult holds even for anisotropic data, which is different from the optimal MSE code that can
employ correlated, or even degenerate, representation. As ICA is one form of whitening,
the results in Fig. 4 demonstrate the suboptimality of whitening in the MSE sense.
The optimal MSE code over noisy channels was examined previously in [6] for N -
dimensional data. However, the capacity constraint was deﬁned for a population and only
examined the case of undercomplete codes. In the model studied here, motivated by the
neural representation, the capacity constraint is imposed for individual units. Furthermore,
the model allows for arbitrary number of units, which provides a way to arbitrarily im-
prove the robustness of the code using a population code. The theoretical analysis for one-
and two-dimensional cases quantiﬁes the amount of error reduction as a function of the
SNR and the number of units along with the data covariance matrix. Finally, our numeri-
cal results for higher-dimensional image data demonstrate a dramatic improvement in the
robustness of the code over both conventional transforms such as wavelets and also repre-
sentations optimized for statistical efﬁciency such as ICA.

Information theory and neural coding. Nature Neuroscience,

References
[1] A. Borst and F. E. Theunissen.
2:947–957, 1999.
[2] N. K. Dhingra and R. G. Smith. Spike generator limits efﬁciency of information transfer in a
retinal ganglion cell. Journal of Neuroscience, 24:2914–2922, 2004.
[3] A. Hyvarinen, J. Karhunen, and E. Oja. Independent Component Analysis. Wiley, 2001.
[4] E. Doi and M. S. Lewicki. Sparse coding of natural images using an overcomplete set of limited
capacity units. In Advances in NIPS, volume 17, pages 377–384. MIT Press, 2005.
[5] J. J. Atick and A. N. Redlich. What does the retina know about natural scenes? Neural Compu-
tation, 4:196–210, 1992.
[6] K. I. Diamantaras, K. Hornik, and M. G. Strintzis. Optimal linear compression under unreliable
representation and robust PCA neural models. IEEE Trans. Neur. Netw., 10(5):1186–1195, 1999.

OriginalRobustCoding(1x)RobustCoding(8x)Wavelets34.8%ICA32.5%3.8%0.6%
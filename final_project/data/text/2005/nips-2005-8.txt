Combining Graph Laplacians for
Semi –Supervised Learning

Andreas Argyriou, Mark Herbster, Massimiliano Pontil
Department of Computer Science
University College London
Gower Street, London WC1E 6BT, England, UK
{a.argyriou, m.herbster, m.pontil}@cs.ucl.ac.uk

Abstract

A foundational problem in semi-supervised learning is the construction
of a graph underlying the data. We propose to use a method which op-
timally combines a number of differently constructed graphs. For each
of these graphs we associate a basic graph kernel. We then compute
an optimal combined kernel. This kernel solves an extended regulariza-
tion problem which requires a joint minimization over both the data and
the set of graph kernels. We present encouraging results on different
OCR tasks where the optimal combined kernel is computed from graphs
constructed with a variety of distances functions and the ‘k’ in nearest
neighbors.

1

Introduction

Semi-supervised learning has received signi ﬁcant attenti on in machine learning in recent
years, see, for example, [2, 3, 4, 8, 9, 16, 17, 18] and references therein. The deﬁning insight
of semi-supervised methods is that unlabeled data may be used to improve the performance
of learners in a supervised task. One of the key semi-supervised learning methods builds
on the assumption that the data is situated on a low dimensional manifold within the am-
bient space of the data and that this manifold can be approximated by a weighted discrete
graph whose vertices are identi ﬁed with the empirical (labe led and unlabeled) data, [3, 17].
Graph construction consists of two stages, ﬁrst selection o f a distance function and then
application of it to determine the graph’s edges (or weights thereof). For example, in this
paper we consider distances between images based on the Euclidean distance, Euclidean
distance combined with image transformations, and the related tangent distance [6]; we
determine the edge set of the graph with k-nearest neighbors. Another common choice is
to weight edges by a decreasing function of the distance d such as e−βd2 .
Although a surplus of unlabeled data may improve the quality of the empirical approxima-
tion of the manifold (via the graph) leading to improved performances, practical experience
with these methods indicates that their performance signi ﬁ cantly depends on how the graph
is constructed. Hence, the model selection problem must consider both the selection of the
distance function and the parameters k or β used in the graph building process described
above. A diversity of methods have been proposed for graph construction; in this paper

we do not advocate selecting a single graph but, rather we propose combining a number
of graphs. Our solution implements a method based on regularization which builds upon
the work in [1]. For a given dataset each combination of distance functions and edge set
speci ﬁcations from the distance will lead to a speci ﬁc graph
. Each of these graphs may then
be associated with a kernel. We then apply regularization to select the best convex com-
bination of these kernels; the minimizing function will trade off its ﬁt to the data against
its norm. What is unique about this regularization is that the minimization is not over a
single kernel space but rather over a space corresponding to all convex combinations of
kernels. Thus all data (labeled vertices) may be conserved for training rather than reduced
by cross-validation which is not an appealing option when the number of labeled vertices
per class is very small.

Figure 3 in Section 4 illustrates our algorithm on a simple example. There, three different
distances for 400 images of the digits ‘six’ and ‘nine’ are depicted, namely, the Euclidean
distance, a distance invariant under small centered image rotations from [−10◦ , 10◦ ] and
a distance invariant under rotations from [−180◦ , 180◦ ]. Clearly, the last distance is prob-
lematic as sixes become similar to nines. The performance of our graph regularization
learning algorithm discussed in Section 2.2 with these distances is reported below each
plot; as expected, this performance is much lower in the case that the third distance is used.
The paper is constructed as follows. In Section 2 we discuss how regularization may be
applied to single graphs. First, we review regularization in the context of reproducing ker-
nel Hilbert spaces (Section 2.1); then in Section 2.2 we specialize our discussion to Hilbert
spaces of functions deﬁned over a graph. Here we review the (n ormalized) Laplacian of
the graph and a kernel which is the pseudoinverse of the graph Laplacian. In Section 3 we
detail our algorithm for learning an optimal convex combination of Laplacian kernels. Fi-
nally, in Section 4 we present experiments on the USPS dataset with our algorithm trained
over different classes of Laplacian kernels.

2 Background on graph regularization

In this section we review graph regularization [2, 9, 14] from the perspective of reproducing
kernel Hilbert spaces, see e.g. [12].

2.1 Reproducing kernel Hilbert spaces
Let X be a set and K : X × X → IR a kernel function. We say that HK is a reproducing
kernel Hilbert space (RKHS) of functions f : X → IR if (i): for every x ∈ X , K (x, ·) ∈
HK and (ii): the reproducing kernel property f (x) = hf , K (x, ·)iK holds for every f ∈
HK and x ∈ X , where h·, ·iK is the inner product on HK . In particular, (ii) tells us that for
x, t ∈ X , K (x, t) = hK (x, ·), K (t, ·)iK , implying that the n × n matrix (K (ti , tj ) : i, j ∈
INp ) is symmetric and positive semi-deﬁnite for any set of inputs {ti : i ∈ INp} ⊆ X ,
p ∈ IN, where we use the notation INp := {1, . . . , p}.
Regularization in an RKHS learns a function f ∈ HK on the basis of available input/output
examples {(xi , yi ) : i ∈ INℓ } by solving the variational problem
Eγ (K ) := min ( ℓXi=1
K : f ∈ HK )
V (yi , f (xi )) + γ kf k2
where V : IR × IR → [0, ∞) is a loss function and γ a positive parameter. Moreover, if f
is a solution to problem (2.1) then it has the form
ℓXi=1

ciK (xi , x), x ∈ X

f (x) =

(2.1)

(2.2)

for some real vector of coefﬁcients c = (ci : i ∈ INℓ )⊤ , see, for example, [12], where “ ⊤ ”
denotes transposition. This vector can be found by replacing f by the right hand side of
equation (2.2) in equation (2.1) and then optimizing with respect to c. However, in many
practical situations it is more convenient to compute c by solving the dual problem to (2.1),
namely
−Eγ (K ) := min ( 1
V ∗ (yi , ci ) : c ∈ IRℓ)
ℓXi=1
c⊤ eKc +
4γ
where eK = (K (xi , xj ))ℓ
i,j=1 and the function V ∗ : IR × IR → IR ∪ {+∞} is the conjugate
of the loss function V which is deﬁned, for every z , α ∈ IR, as V ∗ (z , α) := sup{λα −
V (z , λ) : λ ∈ IR}, see, for example, [1] for a discussion. The choice of the loss function
V leads to different learning methods among which the most prominent are square loss
regularization and support vector machines, see, for example [15].

(2.3)

2.2 Graph regularization

Let G be an undirected graph with m vertices and an m × m adjacency matrix A such that
Aij = 1 if there is an edge connecting vertices i and j and zero otherwise1 . The graph
Laplacian L is the m × m matrix deﬁned as L := D − A, where D = diag(di : i ∈ INm )
and di is the degree of vertex i, that is di = Pm
j=1 Aij .
We identify the linear space of real-valued functions deﬁne d on the graph with IRm and
introduce on it the semi-inner product
hu, vi := u⊤Lv, u, v ∈ IRm .
The induced semi-norm is kvk := phv, vi, v ∈ IRm . It is a semi-norm since kvk = 0 if
2 Pm
v is a constant vector, as can be veri ﬁed by noting that kvk2 = 1
i,j=1 (vi − vj )2Aij .
We recall that G has r connected components if and only if L has r eigenvectors with
zero eigenvalues. Those eigenvectors are piece-wise constant on the connected compo-
nents of the graph. In particular, G is connected if and only if the constant vector is the
only eigenvector of L with zero eigenvalue [5]. We let {σi , ui }m
i=1 be a system of eigen-
values/vectors of L where the eigenvalues are non-decreasing in order, σi = 0, i ∈ INr ,
and deﬁne the linear subspace H(G) of IRm which is orthogonal to the eigenvectors with
zero eigenvalue, that is,

H(G) := {v : v⊤ui = 0, i ∈ INr }.

Within this framework, we wish to learn a function v ∈ H(G) on the basis of a set of
labeled vertices. Without loss of generality we assume that the ﬁrst
ℓ ≤ m vertices are
labeled and let y1 , ..., yℓ ∈ {−1, 1} be the corresponding labels. Following [2] we prescribe
a loss function V and compute the function v by solving the optimization problem
min ( ℓXi=1
V (yi , vi ) + γ kvk2 : v ∈ H(G)) .
We note that a similar approach is presented in [17] where v is (essentially) obtained as the
minimal norm interpolant in H(G) to the labeled vertices. The functional (2.4) balances the
error on the labeled points with a smoothness term measuring the complexity of v on the
graph. Note that this last term contains the information of both the labeled and unlabeled
vertices via the graph Laplacian.

(2.4)

1The ideas we discuss below naturally extend to weighted graphs.

Method (2.4) is a special case of problem (2.1). Indeed, the restriction of the semi-norm k ·k
on H(G) is a norm. Moreover, the pseudoinverse of the Laplacian, L+ , is the reproducing
kernel of H(G), see, for example, [7] for a proof. This means that for every v ∈ H(G) and
i , vi, where L+
i ∈ INm there holds the reproducing kernel property vi = hL+
is the i-th
i
column of L+ . Hence, by setting X ≡ INm , f (i) = vi and K (i, j ) = L+
ij , i, j ∈ INm , we
see that HK ≡ H(G). We note that the above analysis naturally extends to the case that L
is replaced by any positive semideﬁnite matrix. In particul ar, in our experiments below we
2 LD− 1
will use the normalized Laplacian matrix given by D− 1
2 .
Typically, problem (2.4) is solved by optimizing over v = (vi : i ∈ INm ). In particular, for
square loss regularization [2] and minimal norm interpolation [17] this requires solving a
squared linear system of m and m − ℓ equations respectively. On the contrary, in this paper
we use the representer theorem to express v as
v = (cid:16) ℓXj=1
ij cj : i ∈ INm(cid:17).
L+
This approach is advantageous if L+ can be computed off-line because, typically, ℓ ≪ m.
A further advantage of this approach is that multiple problems may be solved with the same
ci are obtained by solving problem (2.3) with eK =
Laplacian kernel. The coefﬁcients
(L+
i,j=1 . For example, for square loss regularization the computation of the parameter
ij )ℓ
vector c = (ci : i ∈ INℓ ) involves solving a linear system of ℓ equations, namely
( eK + γ I)c = y.
(2.5)
3 Learning a convex combination of Laplacian kernels
We now describe our framework for learning with multiple graph Laplacians. We assume
that we are given n graphs G(q) , q ∈ INn , all having m vertices, with corresponding
Laplacians L(q) , kernels K (q) = (L(q) )+ , Hilbert spaces H(q) := H(G(q) ) and norms
q := v⊤L(q)v, v ∈ H(q) . We propose to learn an optimal convex combination of
kvk2
graph kernels, that is, we solve the optimization problem
ρ = min ( ℓXi=1
K (λ) : λ ∈ Λ, v ∈ HK (λ))
V (yi , vi ) + γ kvk2
where we have deﬁned the set Λ := {λ ∈ IRn : λq ≥ 0, Pn
q=1 λq = 1} and, for each
λ ∈ Λ, the kernel K (λ) := Pn
q=1 λqK (q) . The above problem is motivated by observing
that
ρ ≤ min (cid:8)Eγ (K (q) ) : q ∈ INn(cid:9).
Hence an optimal convex combination of kernels has a smaller right hand side than that of
any individual kernel, motivating the expectation of improved performance. Furthermore,
large values of the components of the minimizing λ identify the most relevant kernels.

(3.1)

Problem (3.1) is a special case of the problem of jointly minimizing functional (2.1) over
v ∈ HK and K ∈ co(K), the convex hull of kernels in a prescribed set K. This prob-
lem is discussed in detail in [1, 12], see also [10, 11] where the case that K is ﬁnite is
considered. Practical experience with this method [1, 10, 11] indicates that it can enhance
the performance of the learning algorithm and, moreover, it is computationally efﬁcient to
solve. When solving problem (3.1) it is important to require that the kernels K (q) satisfy
a normalization condition such as that they all have the same trace or the same Frobenius
norm, see [10] for a discussion.

Initialization: Choose K (1) ∈ co{K (q) : q ∈ INn }
For t = 1 to T :
1. compute c(t) to be the solution of problem (2.3) with K = K (t) ;
ﬁnd q ∈ INn : (c(t) , K (q) c(t) ) > (c(t) , K (t) c(t) ). If such q does not exist
2.
terminate;
3. compute ˆp = argmin nEγ (pK (q) + (1 − p)K (t) ) : p ∈ (0, 1]o;
4. set K (t+1) = ˆpK (q) + (1 − ˆp)K (t) .

Figure 1: Algorithm to compute an optimal convex combination of kernels in the set
co{K (q) : q ∈ INn}.

(3.2)

Using the dual problem formulation discussed above (see equation (2.3)) in the inner min-
imum in (3.1) we can rewrite this problem as
V ∗ (yi , ci ) : c ∈ IRℓo : λ ∈ Λ) .
−ρ = max (min n 1
ℓXi=1
c⊤ eK(λ)c +
4γ
The variational problem (3.2) expresses the optimal convex combination of the kernels as
the solution to a saddle point problem. This problem is simpler to solve than the original
problem (3.1) since its objective function is linear in λ, see [1] for a discussion. Several
algorithms can be used for computing a saddle point (ˆc, ˆλ) ∈ IRℓ × Λ. Here we adapt
an algorithm from [1] which alternately optimizes over c and λ. For reproducibility of the
algorithm, it is reported in Figure 1. Note that once ˆλ is computed ˆc is given by a minimizer
of problem (2.3) for K = K (λ). In particular, for square loss regularization this requires
solving the equation (2.5) with eK = (Kij ( ˆλ) : i, j ∈ INℓ ).
4 Experiments
In this section we present our experiments on optical character recognition. We observed
the following. First, the optimal convex combination of kernels computed by our algorithm
is competitive with the best base kernels. Second, by observing the ‘weights’ of the convex
combination we can distinguish the strong from the weak candidate kernels. We proceed
by discussing the details of the experimental design interleaved with our results.
We used the USPS dataset2 of 16×16 images of handwritten digits with pixel values rang-
ing between -1 and 1. We present the results for 5 pairwise classi ﬁcation tasks of varying
difﬁculty and for odd vs. even digit classi ﬁcation. For pair wise classi ﬁcation, the training
set consisted of the ﬁrst 200 images for each digit in the USPS training set and the number
of labeled points was chosen to be 4, 8 or 12 (with equal numbers for each digit). For odd
vs. even digit classi ﬁcation, the training set consisted of
the ﬁrst 80 images per digit in the
USPS training set and the number of labeled points was 10, 20 or 30, with equal numbers
for each digit. Performance was averaged over 30 random selections, each with the same
number of labeled points.

In each experiment, we constructed n = 30 graphs G(q) (q ∈ INn ) by combining k-nearest
neighbors (k ∈ IN10 ) with three different distances. Then, n corresponding Laplacians
were computed together with their associated kernels. We chose as the loss function V
the square loss. Since kernels obtained from different types of graphs can vary widely, it
was necessary to renormalize them. Hence, we chose to normalize each kernel during the

2Available at: http://www-stat-class.stanford.edu/∼tibs/ElemStatLearn/data.html

Task \ Labels %

1 vs. 7

2 vs. 3

2 vs. 7

3 vs. 8

4 vs. 7

Labels

Odd vs. Even

Euclidean (10 kernels)

Transf. (10 kernels)

Tangent dist. (10 kernels)

All (30 kernels)

1%

1.55

0.08

3.08

0.85

4.46

1.17

7.33

1.67

2.90

0.77

10

18.6

3.98

2%

1.53

0.05

3.34

1.21

4.04

1.21

7.30

1.49

2.64

0.78

20

15.5

2.40

3%

1.50

0.15

3.38

1.29

3.56

0.82

7.03

1.43

2.25

0.77

30

13.4

2.67

1%

1.45

0.10

0.80

0.40

3.27

1.16

6.98

1.57

1.81

0.26

10

15.7

4.40

2%

1.45

0.11

0.85

0.38

2.92

1.26

6.87

1.77

1.82

0.42

20

11.7

3.14

3%

1.38

0.12

0.82

0.32

2.96

1.08

6.50

1.78

1.69

0.45

30

8.52

1.32

1%

1.01

0.00

0.73

0.93

2.95

1.79

4.43

1.21

0.88

0.17

10

2%

1.00

0.09

0.19

0.51

2.30

0.76

4.22

1.36

0.90

0.20

20

14.66

4.37

10.50

2.30

3%

1.00

0.11

0.03

0.09

2.14

0.53

3.96

1.25

0.90

0.20

30

8.38

1.90

1%

1.28

0.28

0.79

0.93

3.51

1.92

4.80

1.57

1.04

0.37

10

2%

1.24

0.27

0.25

0.61

2.54

0.97

4.32

1.46

1.14

0.42

20

17.07

4.38

10.98

2.61

3%

1.20

0.22

0.10

0.21

2.41

0.89

4.20

1.53

1.13

0.39

30

8.74

2.39

Table 1: Misclassi ﬁcation error percentage ( top) and standard deviation (bottom) for the
best convex combination of kernels on different handwritten digit recognition tasks, using
different distances. See text for description.

training process by the Frobenius norm of its submatrix corresponding to the labeled data.
We also observed that similar results were obtained when normalizing with the trace of
this submatrix. The regularization parameter was set to 10−5 in all algorithms. For convex
minimization, as the starting kernel in the algorithm in Figure 1 we always used the average
of the n kernels and as the maximum number of iterations T = 100.
Table 1 shows the results obtained using three distances as combined with k-NN (k ∈
IN10 ). The ﬁrst distance is the Euclidean distance between images. The second method is
transformation, where the distance between two images is given by the smallest Euclidean
distance between any pair of transformed images as determined by applying a number of
afﬁne transformations and a thickness transformation 3 , see [6] for more information. The
third distance is tangent distance, as described in [6], which is a ﬁrst-order approximation
to the above transformations. For the ﬁrst three columns in t he table the Euclidean distance
was used, for columns 4–6 the image transformation distance was used, for columns 7–9
the tangent distance was used. Finally, in the last three columns all three methods were
jointly compared. As the results indicate, when combining different types of kernels, the
algorithm tends to select the most effective ones (in this case the tangent distance kernels
and to a lesser degree the transformation distance kernels which did not work very well
because of the Matlab optimization routine we used). We also noted that within each of
the methods the performance of the convex combination is comparable to that of the best
kernels. Figure 2 reports the weight of each individual kernel learned by our algorithm
when 2% labels are used in the pairwise tasks and 20 labels are used for odd vs. even.
With the exception of the easy 1 vs. 7 task, the large weights are associated with the
graphs/kernels built with the tangent distance.

The effectiveness of our algorithm in selecting the good graphs/kernels is better demon-
strated in Figure 3, where the Euclidean and the transformation kernels are combined with
a “low-quality” kernel. This “low-quality” kernel is induc
ed by considering distances in-
variant over rotation in the range [−180◦ , 180◦ ], so that the image of a 6 can easily have
a small distance from an image of a 9, that is, if x and t are two images and Tθ (x) is the
image obtained by rotating x by θ degrees, we set

d(x, t) = min{kTθ (x) − Tθ ′ (t)k : θ , θ ′ ∈ [−180◦ , 180◦ ]}.

3This distance was approximated using Matlab’s constrained minimization function.

The ﬁgure shows the distance matrix on the set of labeled and u nlabeled data for the Eu-
clidean, transformation and “low-quality distance” respe ctively. The best error among 15
different values of k within each method, the error of the learned convex combination and
the total learned weights for each method are shown below each plot. It is clear that the
solution of the algorithm is dominated by the good kernels and is not inﬂuenced by the
ones with low performance. As a result, the error of the convex combination is comparable
to that of the Euclidean and transformation methods. The ﬁna l experiment (see Figure 4)
demonstrates that unlabeled data improves the performance of our method.

0.12

0.1

0.08

0.06

0.04

0.02

0
0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

1 vs. 7

5

10

15

20

25

30

3 vs. 8

5

10

15

20

25

30

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
0

2 vs. 3

5

10

15

20

25

30

4 vs. 7

5

10

15

20

25

30

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
0

0.25

0.2

0.15

0.1

0.05

0
0

2 vs. 7

5

10

15

20

25

30

odd−even

5

10

15

20

25

30

Figure 2: Kernel weights for Euclidean ( ﬁrst 10), Transform ation (middle 10) and Tangent
(last 10). See text for more information.

Euclidean

Transformation

Low−quality distance

0

50

100

150

200

250

300

350

400
0

100

200

300

400

0

50

100

150

200

250

300

350

400
0

100

200

300

400

0

50

100

150

200

250

300

350

400
0

100

200

300

400

error = 17.47%
error = 0.24%
error = 0.24%
P15
P30
P45
i=1 λi = 0.553
i=16 λi = 0.406
i=31 λi = 0.041
convex combination error = 0.26%
Figure 3: Similarity matrices and corresponding learned coefﬁcients of the convex combi-
nation for the 6 vs. 9 task. See text for description.

5 Conclusion

We have presented a method for computing an optimal kernel within the framework of regu-
larization over graphs. The method consists of a minimax problem which can be efﬁciently
solved by using an algorithm from [1]. When tested on optical character recognition tasks,
the method exhibits competitive performance and is able to select good graph structures.
Future work will focus on out-of-sample extensions of this algorithm and on continuous
optimization versions of it. In particular, we may consider a continuous family of graphs
each corresponding to a different weight matrix and study graph kernel combinations over
this class.

0.28

0.27

0.26

0.25

0.24

0.23

0.22

0.21

0.2

0.19

0.18
0

Euclidean
transformation
tang. dist.

500

1000

1500

2000

0.22

0.2

0.18

0.16

0.14

0.12

0.1
0

Euclidean
transformation
tang. dist.

500

1000

1500

2000

Figure 4: Misclassi ﬁcation error vs. number of training poi nts for odd vs. even classi ﬁca-
tion. The number of labeled points is 10 on the left and 20 on the right.

References

[1] A. Argyriou, C.A. Micchelli and M. Pontil. Learning convex combinations of continuously
parameterized basic kernels. Proc. 18-th Conf. on Learning Theory, 2005.
[2] M. Belkin, I. Matveeva and P. Niyogi. Regularization and semi-supervised learning on large
graphs. Proc. of 17–th Conf. Learning Theory (COLT), 2004.
[3] M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Mach. Learn.,
56: 209–239, 2004.
[4] A. Blum and S. Chawla. Learning from Labeled and Unlabeled Data using Graph Mincuts,
Proc. of 18–th International Conf. on Learning Theory, 2001.
[5] F.R. Chung. Spectral Graph Theory. Regional Conference Series in Mathematics, Vol. 92,
1997.
[6] T. Hastie and P. Simard. Models and Metrics for Handwritten Character Recognition. Statistical
Science, 13(1): 54–65, 1998.
[7] M. Herbster, M. Pontil, L. Wainer. Online learning over graphs. Proc. 22-nd Int. Conf. Machine
Learning, 2005.
[8] T. Joachims. Transductive Learning via Spectral Graph Partitioning. Proc. of the Int. Conf.
Machine Learning (ICML), 2003.
[9] R.I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. Proc.
19-th Int. Conf. Machine Learning, 2002.
[10] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, M. I. Jordan. Learning the kernel
matrix with semideﬁnite programming. J. Machine Learning Research, 5: 27–72, 2004.
[11] Y. Lin and H.H. Zhang. Component selection and smoothing in smoothing spline analysis of
variance models – COSSO. Institute of Statistics Mimeo Series 2556, NCSU, J anuary 2003.
[12] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization, J. Machine
Learning Research, 6: 1099–1125, 2005.
[13] C.S. Ong, A.J. Smola, and R.C. Williamson. Hyperkernels. Advances in Neural Information
Processing Systems, 15, S. Becker et al. (Eds.), MIT Press, Cambridge, MA, 2003.
[14] A.J. Smola and R.I Kondor. Kernels and regularization on graphs. Proc. of 16–th Conf. Learn-
ing Theory (COLT), 2003.
[15] V.N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.
[16] D. Zhou, O. Bousquet, T.N. Lal, J. Weston and B. Scholkopf. Learning with local and global
consistency. Advances in Neural Information Processing Systems, 16, S. Thrun et al. (Eds.),
MIT Press, Cambridge, MA, 2004.
[17] X. Zhu, Z. Ghahramani and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and
harmonic functions. Proc. 20–th Int. Conf. Machine Learning, 2003.
[18] X. Zhu, J. Kandola, Z, Ghahramani, J. Lafferty. Nonparametric transforms of graph kernels for
semi-supervised learning. Advances in Neural Information Processing Systems, 17, L.K. Saul
et al. (Eds.), MIT Press, Cambridge, MA, 2005.


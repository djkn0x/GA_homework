Learning Minimum Volume Sets

Clayton Scott
Statistics Department
Rice University
Houston, TX 77005
cscott@rice.edu

Robert Nowak
Electrical and Computer Engineering
University of Wisconsin
Madison, WI 53706
nowak@engr.wisc.edu

Abstract

Given a probability measure P and a reference measure µ, one is
often interested in the minimum µ-measure set with P -measure at
least α. Minimum volume sets of this type summarize the regions of
greatest probability mass of P , and are useful for detecting anoma-
lies and constructing conﬁdence regions. This paper addresses the
problem of estimating minimum volume sets based on independent
samples distributed according to P . Other than these samples, no
other information is available regarding P , but the reference mea-
sure µ is assumed to be known. We introduce rules for estimating
minimum volume sets that parallel the empirical risk minimization
and structural risk minimization principles in classiﬁcation. As
in classiﬁcation, we show that the performances of our estimators
are controlled by the rate of uniform convergence of empirical to
true probabilities over the class from which the estimator is drawn.
Thus we obtain ﬁnite sample size performance bounds in terms of
VC dimension and related quantities. We also demonstrate strong
universal consistency and an oracle inequality. Estimators based
on histograms and dyadic partitions illustrate the proposed rules.

1 Introduction

Given a probability measure P and a reference measure µ, the minimum volume
set (MV-set) with mass at least 0 < α < 1 is
G∗
α = arg min{µ(G) : P (G) ≥ α, G measurable}.
MV-sets summarize regions where the mass of P is most concentrated. For example,
if P is a multivariate Gaussian distribution and µ is the Lebesgue measure, then the
MV-sets are ellipsoids (see also Figure 1). Applications of minimum volume sets
include outlier/anomaly detection, determining highest posterior density or multi-
variate conﬁdence regions, tests for multimodality, and clustering. In comparison
to the closely related problem of density level set estimation [1, 2], the minimum
volume approach seems preferable in practice because the mass α is more easily
speciﬁed than a level of a density. See [3, 4, 5] for further discussion of MV-sets.

This paper considers the problem of MV-set estimation using a training sample
drawn from P , which in most practical settings is the only information one has

Figure 1: Gaussian mixture data, 500 samples, α = 0.9. (Left and Middle) Mini-
mum volume set estimates based on recursive dyadic partitions, discussed in Section
6. (Right) True MV set.

about P . The speciﬁcations to the estimation process are the signiﬁcance level α,
the reference measure µ, and a collection of candidate sets G . All proofs, as well as
additional results and discussion, may be found in [6] . To our knowledge, ours is
the ﬁrst work to establish ﬁnite sample bounds, an oracle inequality, and universal
consistency for the MV-set estimation problem.

The methods proposed herein are primarily of theoretical interest, although they
may be implemented eﬀeciently for certain partition-based estimators as discussed
later. As a more practical alternative, the MV-set problem may be reduced to
Neyman-Pearson classiﬁcation [7, 8] by simulating realizations from.

1.1 Notation

Let (X , B) be a measure space with X ⊂ Rd . Let X be a random variable taking
values in X with distribution P . Let S = (X1 , . . . , Xn ) be an independent and
identically distributed (IID) sample drawn according to P . Let G denote a subset
of X , and let G be a collection of such subsets. Let bP denote the empirical measure
based on S : bP (G) = (1/n) Pn
i=1 I (Xi ∈ G). Here I (·) is the indicator function. Set
µ∗
α = inf
{µ(G) : P (G) ≥ α},
(1)
G
where the inf is over all measurable sets. A minimum volume set, G∗
α , is a minimizer
of (1), when it exists. Let G be a class of sets. Given α ∈ (0, 1), denote Gα = {G ∈
G : P (G) ≥ α}, the collection of all sets in G with mass at least alpha. Deﬁne
µG ,α = inf {µ(G) : G ∈ Gα} and GG ,α = arg min{µ(G) : G ∈ Gα } when it exists.
Thus GG ,α is the best approximation to the MV-set G∗
α from G . Existence and
uniqueness of these and related quantities are discussed in [6] .

2 Minimum Volume Sets and Empirical Risk Minimization

In this section we introduce a procedure inspired by the empirical risk minimization
(ERM) principle for classiﬁcation. In classiﬁcation, ERM selects a classiﬁer from a
ﬁxed set of classiﬁers by minimizing the empirical error (risk) of a training sample.
Vapnik and Chervonenkis established the basic theoretical properties of ERM (see
[9, 10]), and we ﬁnd similar properties in the minimum volume setting. In this and
the next section we do not assume P has a density with respect to µ.

Let φ(G, S, δ) be a function of G ∈ G , the training sample S , and a conﬁdence

(2)

parameter δ ∈ (0, 1). Set bGα = {G ∈ G : bP (G) ≥ α − φ(G, S, δ)} and
bGG ,α = arg min{µ(G) : G ∈ bGα }.
We refer to the rule in (2) as MV-ERM because of the analogy with empirical risk
minimization in classiﬁcation. The quantity φ acts as a kind of “tolerance” by which
the empirical mass estimate may deviate from the targeted value of α. Throughout
this paper we assume that φ satisﬁes the following.
Deﬁnition 1. We say φ is a (distribution free) complexity penalty for G if and
only if for al l distributions P and al l δ ∈ (0, 1),
G∈G (cid:16)(cid:12)(cid:12)(cid:12)P (G) − bP (G)(cid:12)(cid:12)(cid:12) − φ(G, S, δ)(cid:17) > 0(cid:27)(cid:19) ≤ δ.
P n (cid:18)(cid:26)S : sup
Thus, φ controls the rate of uniform convergence of bP (G) to P (G) for G ∈ G . It
is well known that the performance of ERM (for binary classiﬁcation) relative to
the performance of the best classiﬁer in the given class is controlled by the uniform
convergence of true to empirical probabilities. A similar result holds for MV-ERM.
Theorem 1. If φ is a complexity penalty for G , then
P n (cid:16)(cid:16)P ( bGG ,α ) < α − 2φ( bGG ,α , S, δ)(cid:17) or (cid:16)µ( bGG ,α ) > µG ,α(cid:17)(cid:17) ≤ δ.
Proof. Consider the sets
ΘP = {S : P ( bGG ,α ) < α − 2φ( bGG ,α , S, δ)},
Θµ = {S : µ( bGG ,α ) > µ(GG ,α )},
G∈G (cid:16)(cid:12)(cid:12)(cid:12)P (G) − bP (G)(cid:12)(cid:12)(cid:12) − φ(G, S, δ)(cid:17) > 0(cid:27) .
ΩP = (cid:26)S : sup
The result follows easily from the following lemma.
Lemma 1. With ΘP , Θµ , and ΩP deﬁned as above and bGG ,α as deﬁned in (2) we
have ΘP ∪ Θµ ⊂ ΩP .
The proof of this lemma (see [6] ) follows closely the proof of Lemma 1 in [7]. This
result may be understood by analogy with the result from classiﬁcation that says
R( bf ) − inf f ∈F R(f ) ≤ 2 supf ∈F |R(f ) − bR(f )| (see [10], Ch. 8). Here R and bR are
the true and empirical risks, bf is the empirical risk minimizer, and F is a set of
classiﬁers. Just as this result relates uniform convergence bounds to empirical risk
minimization in classiﬁcation, so does Lemma 1 relate uniform convergence to the
performance of MV-ERM.

The theorem above allows direct translation of uniform convergence results into
performance guarantees for MV-ERM. Fortunately, many penalties (uniform con-
vergence results) are known. We now give to important examples, although many
others, such as the Rademacher penalty, are possible.

2.1 Example: VC Classes

Let G be a class of sets with VC dimension V , and deﬁne
φ(G, S, δ) = r32

V log n + log(8/δ)
n

.

(3)

By a version of the VC inequality [10], we know that φ is a complexity penalty for G ,
and therefore Theorem 1 applies. To view this result in perhaps a more recognizable
way, let  > 0 and choose δ such that 2φ(G, S, δ) = . By inverting the relationship
between δ and , we have the following.
Corollary 1. With the notation deﬁned above,
P n (cid:16)(cid:16)P ( bGG ,α ) < α − (cid:17) or (cid:16)µ( bGG ,α ) > µG ,α(cid:17)(cid:17) ≤ 8nV e−n2 /128 .
Thus, for any ﬁxed  > 0, the probability of being within  of the target mass α
and being less than the target volume µG ,α approaches one exponentially fast as
the sample size increases. This result may also be used to calculate a distribution
free upper bound on the sample size needed to be within a given tolerance  of α
and with a given conﬁdence 1 − δ . In particular, the sample size will grow no faster
than a polynomial in 1/ and 1/δ , paralleling results for classiﬁcation.

2.2 Example: Countable Classes

Suppose G is a countable class of sets. Assume that to every G ∈ G a number JGK
is assigned such that PG∈G 2−JGK ≤ 1. In light of the Kraft inequality for preﬁx
codes, JGK may be deﬁned as the codelength of a codeword for G in a preﬁx code
for G . Let δ > 0 and deﬁne
φ(G, S, δ) = r JGK log 2 + log(2/δ)
2n
By Chernoﬀ ’s bound together with the union bound, φ is a penalty for G . Therefore
Theorem 1 applies and we have obtained a result analogous to the Occam’s Razor
bound for classiﬁcation.

(4)

.

As a special case, suppose G is ﬁnite and take JGK = log2 |G |. Setting 2φ(G, S, δ) = 
and inverting the relationship between δ and , we have
Corollary 2. For the MV-ERM estimate bGG ,α from a ﬁnite class G
P n (cid:16)(cid:16)P ( bGG ,α ) < α − (cid:17) or (cid:16)µ( bGG ,α ) > µG ,α(cid:17)(cid:17) ≤ 2|G |e−n2 /2 .
3 Consistency

A minimum volume set estimator is consistent if its volume and mass tend to the
optimal values µ∗
α and α as n → ∞. Formally, deﬁne the error quantity
M(G) := (µ(G) − µ∗
α )+ + (α − P (G))+ ,
where (x)+ = max(x, 0). (Note that without the (·)+ operator, this would not be
a meaningful error since one term could be negative and cause M to tend to zero,
even if the other error term does not go to zero.) We are interested in MV-set
estimators such that M( bGG ,α ) tends to zero as n → ∞.
Deﬁnition 2. A learning rule bGG ,α is strongly consistent if limn→∞ M( bGG ,α ) = 0
with probability 1. If bGG ,α is strongly consistent for every possible distribution of
X , then bGG ,α is strongly universally consistent.
To see how consistency might result from MV-ERM, it helps to rewrite Theorem
1 as follows. Let G be ﬁxed and let φ(G, S, δ) be a penalty for G . Then with
probability at least 1 − δ , both
µ( bGG ,α ) − µ∗
α ≤ µ(GG ,α ) − µ∗
α

(5)

and

α − P ( bGG ,α ) ≤ 2φ( bGG ,α , S, δ)
(6)
hold. We refer to the left-hand side of (5) as the excess volume of the class G and
the left-hand side of (6) as the missing mass of bGG ,α . The upper bounds on the
right-hand sides are an approximation error and a stochastic error, respectively.
The idea is to let G grow with n so that both errors tend to zero as n → ∞. If G
does not change with n, universal consistency is impossible.

To have both stochastic and approximation errors tend to zero, we apply MV-ERM
to a class G k from a sequence of classes G 1 , G 2 , . . ., where k = k(n) grows with the
sample size. Consider the estimator bGG k ,α .
Theorem 2. Choose k = k(n) and δ = δ(n) such that k(n) → ∞ as n → ∞ and
P∞
n=1 δ(n) < ∞. Assume the sequence of sets G k and penalties φk satisfy
µ(G) = µ∗
inf
lim
α
k→∞
G∈G k
α
and

(7)

φk (G, S, δ(n)) = o(1).
lim
sup
n→∞
G∈G k
α
Then bGG k ,α is strongly universal ly consistent.
The proof combines the Borel-Cantelli lemma and the distribution-free result of
Theorem 1 with the stated assumptions. Examples satisfying the hypotheses of the
theorem include families of VC classes with arbitrary approximating power (e.g.,
generalized linear discriminant rules with appropriately chosen basis functions and
neural networks), and histogram rules. See [6]
for further discussion.

(8)

4 Structural Risk Minimization and an Oracle Inequality

In the previous section the rate of convergence of the two errors to zero is determined
by the choice of k = k(n), which must be chosen a priori. Hence it is possible that
the excess volume decays much more quickly than the missing mass, or vice versa.
In this section we introduce a new rule called MV-SRM, inspired by the principle of
structural risk minimization (SRM) from the theory of classiﬁcation [11, 12], that
automatically balances the two errors.

The result in this section is not distribution free. We assume

A1 P has a density f with respect to µ.
α exists and P (G∗
A2 G∗
α ) = α.

Under these assumptions (see [6] ) there exists γα > 0 such that for any MV-set
α , {x : f (x) > γα} ⊂ G∗
G∗
α ⊂ {x : f (x) ≥ γα}.
Let G be a class of sets. Conceptualize G as a collection of sets of varying capacities,
such as a union of VC classes or a union of ﬁnite classes. Let φ(G, S, δ) be a penalty
for G . The MV-SRM principle selects the set
nµ(G) + φ(G, S, δ) : bP (G) ≥ α − φ(G, S, δ)o .
bGG ,α = arg min
G∈G
Note that MV-SRM is diﬀerent from MV-ERM because it minimizes a complexity
penalized volume instead of simply the volume. We have the following.1

(9)

1Although the value of 1/γα is in practice unknown, it can be bounded by 1/γα ≤
(1 − µ∗
α )/(1 − α) ≤ 1/(1 − α). This follows from the bound 1 − α ≤ γα · (1 − µ∗
α ) on the
mass outside the minimum volume set.

Theorem 3. Let bGG ,α be the MV-set estimator in (9). With probability at least
1 − δ over the training sample S ,
γα (cid:19) inf
M( bGG ,α ) ≤ (cid:18)1 +
α + 2φ(G, S, δ) o .
G∈Gα n µ(G) − µ∗
1
(10)
Sketch of proof: The proof is similar in some respects to oracle inequalities for
classiﬁcation. The key diﬀerence is in the form of the error term M(G) =
(µ(G) − µ∗
α )+ + (α − P (G))+ . In classiﬁcation both approximation and stochastic
errors are positive, whereas with MV-sets the excess volume µ(G) − µ∗
α or missing
mass α − P (G) could be negative. This necessitates the (·)+ operators, without
which the error would not be meaningful as mentioned earlier. The proof considers
α and P ( bGG ,α ) < α, (2) µ( bGG ,α ) ≥ µ∗
three cases separately: (1) µ( bGG ,α ) ≥ µ∗
α and
P ( bGG ,α ) ≥ α, and (3) µ( bGG ,α ) < µ∗
α and P ( bGG ,α ) < α.
In the ﬁrst case, both
volume and mass errors are positive and the argument follows standard lines. The
second case can be seen to follow easily from the ﬁrst. The third case (which oc-
curs most frequently in practice) is most involved and requires use of the fact that
µ∗
α − µ∗
α− ≤ /γα for  > 0, which can be deduced from basic properties of MV and
density level sets.

The oracle inequality says that MV-SRM performs about as well as the set chosen
by an oracle to optimize the tradeoﬀ between the stochastic and approximation
errors. To illustrate the power of the oracle inequality, in [6] we demonstrate that
MV-SRM applied to recursive dyadic partition-based estimators adapts optimally
to the number of relevant features (unknown a priori).

5 Damping the Penalty

In Theorem 1, the reader may have noticed that MV-ERM does not equitably bal-
ance the volume error with the mass error. Indeed, with high probability, µ( bGG ,α )
is less than µ(GG ,α ), while P ( bGG ,α ) is only guaranteed to be within φ( bGG ,α ) of
α. The net eﬀect is that MV-ERM (and MV-SRM) underestimates the MV-set.
Experimental comparisons have conﬁrmed this to be the case [6] .
A minor modiﬁcation of MV-ERM and MV-SRM leads to a more equitable distribu-
tion of error between the volume and mass, instead of having all the error reside in
the mass term. The idea is simple: scale the penalty in the constraint by a damping
factor ν < 1. In the case of MV-SRM, the penalty in the ob jective function also
needs to be scaled by 1 + ν . Moreover, the theoretical properties of these estimators
stated above are retained (the statements, omitted here, are slightly more involved
[6] ). Notice that in the case ν = 1 we recover the original estimators. Also note
that the above theorem encompasses the generalized quantile estimate of [3], which
corresponds to ν = 0. Thus we have ﬁnite sample size guarantees for that estimator
to match Polonik’s asymptotic analysis.

6 Experiments: Histograms and Trees

To gain some insight into the basic properties of our estimators, we devised some
simple numerical experiments. In the case of histograms, MV-SRM can be imple-
mented in a two step process. First, compute the MV-ERM estimate (a very simple
procedure) for each G k , k = 1, . . . , K , where 1/k is the bin-width. Second, choose
the ﬁnal estimate by minimizing the penalized volume of the MV-ERM estimates.

Error as a function of sample size

occam
rademacher

n = 10000, k = 20, ν=0

0.12

0.1

0.08

0.06

0.04

0.02

0
100

1000

10000

100000

1000000

Figure 2: Results for histograms. (Left) A typical MV-ERM estimate with bin-
width 1/20, ν = 0, and based on 10000 points. True MV-set indicated by solid line.
(Right) The error of the MV-SRM estimate M( bGG ,α ) as a function of sample size
when ν = 0. The results indicated that the Occam’s Razor bound is tighter and
yields better performance than Rademacher.

We consider two penalties: one based on an Occam style bound, the other on the
(conditional) Rademacher average. As a data set we consider X = [0, 1]2 , the unit
square, and data generated by a two-dimensional truncated Gaussian distribution,
centered at the point (1/2, 1/2) and having spherical variance with parameter σ =
0.15. Other parameter settings are α = 0.8, K = 40, and δ = 0.05. All experiments
were conducted at nine diﬀerent sample sizes, logarithmically spaced from 100 to
1000000, and repeated 100 times. Results are summarized in Figure 2.

To illustrate the potential improvement oﬀered by spatially adaptive partitioning
methods, we consider a minimum volume set estimator based on recursive dyadic
(quadsplit) partitions. We employ a penalty that is additive over the cells A of the
partition. The precise form of the penalty φ(A) for each cell is given in [6] , but
loosely speaking it is proportional to the square-root of the ratio of the empirical
mass of the cell to the sample size n. In this case, MV-SRM with ν = 0 is
G∈GL XA
sub ject to XA bP (A)`(A) ≥ α
min
where G L is the collection of all partitions with dyadic cell sidelengths no smaller
than 2−L and `(A) = 1 if A belongs to the candidate set and `(A) = 0 otherwise
(see [6] for further details). Although directly optimization appears formidable, an
eﬃcient alternative is to consider the Lagrangian and conduct a bisection search over
the Lagrange multiplier until the mass constraint is nearly achieved with equality
(10 iterations is suﬃcient in practice). For each iteration, minimization of the
Lagrangian can be performed very rapidly using standard tree pruning techniques.

[µ(A)`(A) + φ(A)]

(11)

An experimental demonstration of the dyadic partition estimator is depicted in Fig-
ure 1. In the experiments we employed a dyadic quadtree structure with L = 8 (i.e.,
cell sidelengths no smaller than 2−8 ) and pruned according to the theoretical penalty
φ(A) formally deﬁned in [6] weighted by a factor of 1/30 (in practice the optimal
weight could be found via cross-validation or other techniques). Figure 1 shows
the results with data distributed according to a two-component Gaussian mixture
distribution. This ﬁgure (middle image) additionally illustrates the improvement
possible by “voting” over shifted partitions, which in principle is equivalent to con-
structing 2L × 2L diﬀerent trees, each based on a partition oﬀset by an integer
multiple of the base sidelength 2−L , and taking a ma jority vote over all the result-

ing set estimates to form the ﬁnal estimate. This strategy mitigates the “blocky”
structure due to the underlying dyadic partitions, and can be computed almost as
rapidly as a single tree estimate (within a factor of L) due to the large amount of
redundancy among trees. The actual running time was one to two seconds.

7 Conclusions

In this paper we propose two rules, MV-ERM and MV-SRM, for estimation of
minimum volume sets. Our theoretical analysis is made possible by relating the
performance of these rules to the uniform convergence properties of the class of sets
from which the estimate is taken. Ours are the ﬁrst known results to feature ﬁnite
sample bounds, an oracle inequality, and universal consistency.

Acknowledgements

The authors thank Ercan Yildiz and Rebecca Willett for their assistance with the experi-
ments involving dyadic trees.

References

[1] I. Steinwart, D. Hush, and C. Scovel, “A classiﬁcation framework for anomaly detec-
tion,” J. Machine Learning Research, vol. 6, pp. 211–232, 2005.

[2] S. Ben-David and M. Lindenbaum, “Learning distributions by their density levels – a
paradigm for learning without a teacher,” Journal of Computer and Systems Sciences,
vol. 55, no. 1, pp. 171–182, 1997.

[3] W. Polonik, “Minimum volume sets and generalized quantile processes,” Stochastic
Processes and their Applications, vol. 69, pp. 1–24, 1997.

[4] G. Walther, “Granulometric smoothing,” Ann. Stat., vol. 25, pp. 2273–2299, 1997.

[5] B. Sch¨olkopf, J. Platt, J. Shawe-Taylor, A. Smola, and R. Williamson, “Estimating
the support of a high-dimensional distribution,” Neural Computation, vol. 13, no. 7,
pp. 1443–1472, 2001.

[6] C. Scott and R. Nowak, “Learning minimum volume sets,” UW-Madison, Tech. Rep.
ECE-05-2, 2005. [Online]. Available: http://www.stat.rice.edu/∼cscott

[7] A. Cannon, J. Howse, D. Hush, and C. Scovel, “Learning with the Neyman-Pearson
and min-max criteria,” Los Alamos National Laboratory, Tech. Rep. LA-UR 02-2951,
2002.
[Online]. Available: http://www.c3.lanl.gov/∼kelly/ml/pubs/2002 minmax/
paper.pdf

[8] C. Scott and R. Nowak, “A Neyman-Pearson approach to statistical learning,” IEEE
Trans. Inform. Theory, 2005, (in press).

[9] V. Vapnik, Statistical Learning Theory. New York: Wiley, 1998.

[10] L. Devroye, L. Gy¨orﬁ, and G. Lugosi, A Probabilistic Theory of Pattern Recognition.
New York: Springer, 1996.

[11] V. Vapnik, Estimation of Dependencies Based on Empirical Data.
Springer-Verlag, 1982.

New York:

[12] G. Lugosi and K. Zeger, “Concept learning using complexity regularization,” IEEE
Trans. Inform. Theory, vol. 42, no. 1, pp. 48–54, 1996.


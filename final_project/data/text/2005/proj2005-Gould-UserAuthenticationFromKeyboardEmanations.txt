CS229 MACHINE LEARNING FINAL PROJECT - GOULD

1

A Novel Approach to User Authentication Through
Machine Learning of Keyboard Acoustic
Emanations
Stephen Gould
sgould@stanford.edu

[1] and
Abstract — Recent work by Asonov and Agrawal
Zhuang et al. [2] has shown that acoustic emanations from
keyboards can be used to reconstruct the text that is being
typed. This, in theory, allows for the retrieval of con ﬁdent ial
information such as passwords by covertly recording sound from
the keyboard, and thus poses a signi ﬁcant security threat.
Their techniques work because different keys produce different
acoustic signatures when struck by the typist. Classi ﬁers c an be
built to detect these differences and language models help in
establishing priors for the various key combinations.
This paper looks at applying machine learning techniques to
the identi ﬁcation and authentication of users based solely on the
acoustic waveform generated as the user types on a keyboard.
This is possible because, not only do different keys produce
different sounds, but the same key sequence can produce different
acoustic waveforms when typed by a different user. Classi ﬁc ation
accuracies of up to 98% are reported on a small set of trial users.
In one novel application, this kind of system can be used
to biometrically harden users’ passwords by incorporating each
user’s typing behavior as part of the authentication procedure
(see Monrose et al. [3]).

Index Terms— Machine Learning, User Authentication, Key-
board Acoustic Emanations, Computer Security

I . INTRODUCT ION
T HIS report discusses the outcome of research conducted
into the identiﬁcation and authentication of computer
system users from keyboard acoustic emanations.
Most users are comfortable with the familiar clicking sounds
emanating from their keyboard as they type. However, these
sounds, known as keyboard acoustic emanations, have been
shown to be a signiﬁcant security risk in allowing the recon-
struction of text from covert audio recordings ([1] and [2]).
A side channel attack in computer and communications
security is the ability for an adversary to obtain secret in-
formation through indirect observations of the system. For a
long time, researchers have known that private information can
be retrieved by monitoring electromagnetic radiation emitted
from different types of electronic equipment. For example,
data transmitted over older modems can be read by monitoring
the activity LED on the modem which is highly correlated
with the data being sent. Side channel attacks result from
weaknesses in a system’s physical implementation rather than
underlying algorithms or protocols.
Keyboard acoustic emanations are therefore a type of side
channel attack. Asonov and Agrawal [1] introduce the idea of
training a classiﬁer to recognize keys based on their unique

acoustic properties. More recently, Zhuang et al. [2] extended
this work by showing that it is possible to reconstruct text
that the user is typing after recording only 10 minutes of
data. Their results improve accuracy by overlaying a language
model which provides priors for keystroke bigrams.
The work outlined in this report shows that it is also possible
to identify the users themselves. This can then be used as
a novel way to improve computer security, for example, by
hardening passwords with biometric information. Monrose et
al. [3] have shown that password hardening is possible by
direct measurements of user’s typing characteristics (through
modiﬁcation of the computer system’s device drivers). The
approach described below does not rely on direct measure-
ments of keyboard timing, but rather extracts this information
as well as other features from the recorded sound. It allows
for the incorporation of features such as how hard the user
strikes each key, which has been identiﬁed by Bergadano et
al. [4] as valuable in distinguishing between users. This can
be done without the need for specialized computer hardware -
the computer’s microphone is all that is required. Finally, the
work here has other novel applications outside of password
hardening which are discussed at the end of the paper.

Fig. 1. Screen capture of Windows GUI application developed for capturing
acoustic data samples.

I I . DATA COL L ECT ION
Since standard datasets of acoustic keystroke data are not
readily available, the ﬁrst stage of this project was to deve lop
a tool for recording and labeling the sound of users typing
on keyboards. A Windows GUI application was written to

CS229 MACHINE LEARNING FINAL PROJECT - GOULD

2

record and save sample waveforms of different user’s typing
patterns along with some associated meta-information such as
keystroke timing.
The GUI samples the audio signal at a rate of 22.05kHz with
16-bit quantization. In order to facilitate easy processing in
Matlab, each sample waveform (training instance) is saved in
a separate .WAV ﬁle with meta-information in a correspondin g
.TXT ﬁle.
A total of 150 training examples were recorded from six
different users under similar environmental conditions (i.e.
ambient noise). Each user was asked to type “username ” and
“p4ssw0rd ” as if logging on to a computer system. All the data
samples (including those with misspellings) were included in
the training set1 . Basic statistics of the training set are shown
in the table below.

TABLE I
T RA I N I NG DATA S TAT I S T I C S

Fig. 2.
Captured audio signal for four keystrokes showing the energy
envelope used to detect each key press. Also clearly visible are the touch,
hit and release peaks of each keystroke.

Number of
Samples
20
20
20
30
20
40

Avg. Length
(sec.)
4.35
4.95
6.45
4.80
4.55
5.18

Avg.
Keystrokes
18.5
18.0
17.5
18.1
18.2
17.5

Chris
Kendra
Konstantin
Naomi
Ross
Stephen

All data was collected on a Dell Latitude D400 laptop using
the built-in microphone.

I I I . DATA ANALY S I S

A. Overview
The acoustic signal produced by users as they type on
computer keyboards contains a suprisingly large amount of
information. Fig. 2 shows a sample audio waveform of four
consecutive keystrokes. Clearly present in the waveform are
the (i) touch, (ii) hit, and (iii) release peaks corresponding to
(i) when the user’s ﬁnger ﬁrst touches the key, (ii) when the
key reaches the bottom of its stroke, and (iii) when the key
rebounds after being released, respectively. This is consistent
with the ﬁndings of Zhuang et al. [2]. The touch and hit peaks
are sometimes referred to collectively as the push peak.
There is typically 100-150ms between a key being pushed
and that same key being released, and 200-300ms between
consecutive keystrokes corresponding to an average typing
speed of 200-300 characters per minute. However, there can
be signiﬁcant shortening in the duration between keystroke s
especially when a user is typing familiar text (such as his
username and password). In these cases, the hit peak from
a subsequent keystroke may coincide with the release peak
from the previous keystroke. Although trivialized by Zhuang et
al. [2], this overlapping makes it extermely difﬁcult to rel iably
detect every keystroke as discussed in the next section.
It is quite obvious that each key click contains rich spectral
information that distinguishes it from background noise (see

1 In a real implementation of an authentication system, the login software
would, as usual, reject attempts with invalid usernames or passwords before
invoking any biometric checking

Fig. 3. Keyboard audio signal and corresponding frequency spectrogram
showing four seconds of continuous typing.

Fig. 3). The critical
insight of Asonov and Agrawal [1]
is that due to differing mechanical properties, each key on
a keyboard will produce a different sound. They use that
difference to reconstruct text. Our proposition is that different
typing patterns (corresponding, for example, to different users)
will produce different sounds for the same key or phrase, and
can therefore be used to identify users.

B. Keystroke Detection

Although seemingly simple, keystroke detection required a
signiﬁcant amount of work to get right. Our ﬁrst attempt was t
o
replicate the procedure outlined by Zhuang et al. [2] and detect
keystrokes by applying a threshold to the energy envelope of
the signal (computed by summing Fourier coefﬁcients from
400Hz to 12kHz). Zhuang et al. [2] admit that this sometimes
makes mistakes which slightly affects the quality of their
results. Our performance was, however, considerably lower
than theirs, which is most likely due to the use of the quieter
laptop keyboard over standard PC keyboards.

CS229 MACHINE LEARNING FINAL PROJECT - GOULD

3

Early experimentation found the method as described to
be inadequate for obtaining accurate keystroke detection. The
procedure was improved by employing some general tech-
niques described in the speech processing literature [5]. The
most successful techniques were:
• High-pass ﬁltering the signal.
• Clipping the audio signal to within two standard devi-
ations of the mean to prevent excessivley high sample
values.
• Raising the signal to a large odd power to accentuate
peaks.
• Post ﬁltering the detected keystrokes to disqualify any
keystroke within 50ms of a previous keystroke.
While these steps gave better results than using the raw
signal, the results obtained still contained about a 5% detection
error2 , and is something that would require investigation in any
further work.

IV. FEATURE EXTRACT ION
After detecting the keystrokes, a window of 20-50ms sur-
rounding each keystroke was used to extract features from the
waveform for use by the classiﬁers. Among the many features
tried, four main classes of feature were found to be effective.
1) Inter-key Timing: Inter-key timing, or latency, measures
the time duration between consecutive keystrokes. Monrose et
al. [3], Bergadano et al. [4] and Lau et al. [6] have all identiﬁed
this as one of the main features in discriminating between
different users’ typing behavior. However, due to inaccuracies
in keystroke detection from the audio signal3 and the short
input sequences being used for classiﬁcation, this feature was
not as critical as one would think, as will be shown in the
results below.
The inter-key time was associated with the second key in
each keystroke pair, so another issue to consider was that of
boundary conditions, i.e. how to deal with the ﬁrst key which
has no preceding keystroke. Three policies were investigated,
all producing similar results:
• Drop the ﬁrst sample altogether, leaving m − 1 feature
vectors for classifying the user from m keystrokes.
• Assume some nominal constant value for
the ﬁrst
keystroke, for example 0ms.
• Set the inter-key time for the ﬁrst keystroke to be the
average of the inter-key intervals for the entire sample.
In the results reported below, the ﬁrst option was used when
inter-key timing was the only feature used, while the second
option was used when inter-key timing was used as part of a
larger feature vector.
2) Keystroke Energy: Bergadano et al. [4] make the obser-
vation that keystroke energy provides good biometric infor-
mation since users strike keys with different pressure, but that
special-purpose keyboards would be required to make such

2By either missing actual keystrokes or falsely detecting non-existent
keystrokes.
3 In fact, even the system proposed by Lau which measures key hit
times directly suffers from inaccuracies caused by poor resolution of the
software timers provided by the operating system and latency intorduced
by intermediate software layers. The best approach would be to replace the
keyboard devcie driver with a custom module to record key times precisely.

measurements. The audio signal produced for a key press is
strongly correlated to the energy imparted by the user, and
therefore makes a good surrogate for keystroke energy. This
feature is derived by summing the squared signal values in a
small region surrounding the hit peak.
If the total keystroke energy is a
3) Fourier Coefﬁcients:
good feature for distinguishing between users, then the energy
in different frequency bands is also worth exploring. The
Fourier coefﬁcients feature computes the energy component
in
equally spaced ﬁlter banks over the 11kHz sample space. The
features are then normalize by the total energy to compensate
for background noise effects.
4) Mel-Frequency Cepstral Coefﬁcients (MFCC): Research
in digital signal processing of speech signals has long identi-
ﬁed MFCCs as critical for automated speech recognition (see
Deller et al. [5]), and are now used extensively in speech and
audio processing system. The Mel-frequency scale provides
a mapping between real frequency (measured in Hz) and
perceived frequency, and was originally used to describe the
human auditory perception. There are a number of “standard ”
scales cited in the literature. The scale used in this work is
de ﬁned as,

700 (cid:19)
fH z

fmel = 1127 ln (cid:18)1 +
The MFCCs are then computed as,
mf cc = F−1 
Hi,j log kSj k
N −1
Xj=0


i=1
where S is the Fourier transform of the waveform, and Hi is
the i-th Mel-frequency ﬁlter bank.

n

V. PROBAB IL I S T IC MODE L S
A. Naive Bayes Model
Let each training example be de ﬁned by a matrix con-
sisting of a sequence of m(i)
feature vectors, X (i) =
h ˜x(i)
m(i) i, each feature vector representing
˜x(i)
˜x(i)
. . .
1
2
a single keystorke. The probability of a particular user, k , given
the data is then,

1
Z

p (cid:16)X (i) |y (i) = k(cid:17)
p (cid:16)y (i) = k |X (i)(cid:17) =
where we have assumed equal priors on the y (i) , and Z is the
partition function. Now, under the naive Bayes assumption that
each keystroke is independent, we have,

m(i)
p (cid:16) ˜x(i)
j |y (i) = k(cid:17)
p (cid:16)y (i) = k |X (i)(cid:17) =
Yj=1
The probability of each feature vector given the user was
modeled as a multivariate Gaussian,

1
Z

p (cid:16) ˜x(i)
j |y (i) = k(cid:17) =

1
2 |Σk | 1
(2π) n
2

e

2 “ ˜x
− 1

(i)
j

−µk ”T

k “ ˜x
(i)
Σ−1
j

−µk ”

CS229 MACHINE LEARNING FINAL PROJECT - GOULD

4

Fig. 4. Hidden Markov Model for single keystroke acoustics incorporating
both hit and release peaks.

B. Hidden Markov Model
As discussed above, the keystroke detection algorithm will
sometimes fail to detect actual keystrokes and ocassionally
produces erroneous keystrokes. In an attempt to remove the
keystroke detection algorithm from the classiﬁcation proc e-
dure, a left-to-right hidden Markov model (HMM) with states
for silence, hit peak, silence, and release peak was designed.
The HMM can then automatically detect keystrokes given a
sequence of feature vectors from the entire acoustic waveform.
The HMM has the added bene ﬁt of modeling temporal infor-
mation (such as inter-key timing).
Each state of the HMM contained a single multivariate
Gaussian to model the observation probability distribution
given that state. The hit and release states were initialized
with parameter estimates derived from feature vectors at the
estimated position of the keystrokes, while the silence states
were initialized with feature vectors extracted from the mid-
point between two keystrokes. One HMM per user was then
trained using the entire sequence of feature vectors (each
generated from windows 5ms apart) using the Baum-Welch
re-estimation algorithm.

V I . RE SULT S

A. Evaluating Classiﬁcation Performance
Classiﬁcation performance was evaluated using leave-one-
out cross-validation (LOOCV) on the entire training set of 150
samples. A separate model was trained for each user, and the
test sample applied to each model. The predicted user label
was taken to be that of the model with highest posterior, in
the usual way,

arg max
k

p (cid:16) ˆy (test) = k |X (test) (cid:17)
The confusion matrices for the naive Bayes and hidden
Markov model classiﬁers using feature vector consisting of 10
MFCCs are shown below. The naive Bayes classiﬁer achieves
96% accuracy while the HMM attains 98%. Results for other
features combinations are summarized in table IV.

B. Evaluating Authentication Performance
The user authentication protocol evaluated works as follows.
A username/passowrd combination is authenticated as being
typed by the real user by comparing a user-speciﬁc statistic al
model of the keystroke biometrics with a population model
(where the models are either naive Bayes or HMMs as
described earlier). Although in this work, a separate population

TABLE II
NA I V E BAY E S G AU S S I AN D I S C R IM I NAN T C LA S S I FIER (U S I NG 5 0M S
W I NDOW AND 1 0 M FCC S ) LOOCV CON F U S I ON M ATR I X

C
20
-
-
1
-
-

Ke
-
20
-
-
1
-

Ko
-
-
20
1
1
-

N
-
-
-
28
-
1

R
-
-
-
-
18
-

S
-
-
-
-
-
39

C
Ke
Ko
N
R
S

TABLE III
H I DD EN M A RKOV M OD E L LOOCV CON F U S I ON M ATR I X

C
20
-
-
-
-
-

Ke
-
20
-
-
-
-

Ko
-
-
20
-
3
-

N
-
-
-
30
-
-

R
-
-
-
-
17
-

S
-
-
-
-
-
40

C
Ke
Ko
N
R
S

model was trained for each real user, a single population model
could be envisioned for all users in a large system.
Authentication performance was evaluated by assigning
each of the six users in the training set the role of imposter,
yimposter , in turn. Then, for each user remaining, yreal
6=
yimposter ,
train a user model on (cid:8)X (i) ; y (i) = yreal(cid:9) and
a population model on (cid:8)X (i) ; y (i) 6= yreal , yimposter (cid:9). On
each iteration we keep track of the number of true/false
positives/negatives,

1 (cid:26) p(X (i) ;θu )
> t(cid:27)
ntp = Pi:y (i) =yreal
p(X (i) ;θp )
1 (cid:26) p(X (i) ;θu )
> t(cid:27)
nf p = Pi:y (i)=yimposter
p(X (i) ;θp )
1 (cid:26) p(X (i) ;θu )
≤ t(cid:27)
ntn = Pi:y (i)=yimposter
p(X (i) ;θp )
≤ t(cid:27)
1 (cid:26) p(X (i) ;θu )
nf n = Pi:y (i)=yreal
p(X (i) ;θp )
where t, the acceptance threshold, can be adjusted to trade-
off between FAR and FRR, and θ are the appropriate model
parameters.
The false acceptance rate (FAR) and false rejection rate
(FRR) are then computed as,

F AR = nf p
nf p+ntn

and F RR = nf n
nf n+ntp

In most authentication schemes, it is desirable to keep the
FRR as low as possible to prevent denying access to legitimate
users, while maintaining a sufﬁciently small FRR to be of
practical bene ﬁt. Results for FAR and FRR for various featur e
combinations are shown in table IV.

C. Summary of Results
The following table summarizes the results of a number of
experiments varying the window size and feature selection.
Due to the prohibitively expensive cross-validation process
involved in evaluating the HMM performance, most of the

CS229 MACHINE LEARNING FINAL PROJECT - GOULD

5

The work of Zhuang et al. [2] would also bene ﬁt from
being able to model artifacts introduced by a user’s typing
behavior versus those necessary for reconstruction of text. In
deed, monitoring a user’s typing behavior may lead to yet
other novel applications such as context-sensitive assistance.
Finally, although this research has shown that it is possible
to identify a user based on keyboard acoustic eminations,
there are still a lot of practical issues that would need to be
resolved in order to actually deploy a robust system. Such
issues include ﬁltering out background noise, dealing with
differences between keyboards, and tracking temporary and
permanent changes in typing patterns (for example when a user
injures a hand or ﬁnger). That said, this work has highlighte d
what is possible and provides some insights that can be applied
in a range of similar research areas involving biometrics and
emanation security.

ACKNOW L EDGMENT
The authors would like to thank the members of the CS229
class who volunteered the biometric typing samples for used
in this study. We are also grateful to Dan Boneh in discussions
regarding this subject matter.

RE F ERENCE S
[1] D. Asonov and R. Agrawal, “Keyboard acoustic emanations .” in IEEE
Symposium on Security and Privacy, 2004, pp. 3–11.
[2] L. Zhuang, F. Zhou, and J. D. Tygar, “Keyboard acoustic em anations
revisited,”
To appear in Proceedings of
the 12th ACM Conference
on Computer and Communications Security, November 2005. [Online].
Available: http://trust.eecs.berkeley.edu/pubs/3.html
[3] F. Monrose, M. K. Reiter, and S. Wetzel, “Password harden ing based on
keystroke dynamics,” in CCS ’99: Proceedings of the 6th ACM conference
on Computer and communications security. New York, NY, USA: ACM
Press, 1999, pp. 73–82.
[4] F. Bergadano, D. Gunetti, and C. Picardi, “User authenti cation through
keystroke dynamics,” ACM Trans. Inf. Syst. Secur., vol. 5, no. 4, pp. 367–
397, 2002.
[5] J. John R. Deller, J. G. Proakis, and J. H. Hansen, Discrete Time
Processing of Speech Signals. Upper Saddle River, NJ, USA: Prentice
Hall PTR, 1993.
[6] e. a. Lau, E., “Enhanced user authentication through key stroke biomet-
rics,” 6.857 Computer and Network Security ﬁnal project report , 2004.

experiments were conducted using the naive Bayes Gaussian
discriminant classiﬁer, and only the most promising featur e
combinations used with the HMM classiﬁer.

TABLE IV
S UMMA RY O F R E S U LT S

Model

Naive Bayes
(inter-key timing)
Naive Bayes
(key energy, inter-key
timing)
Naive Bayes
(20ms window
normalized Fourier)
Naive Bayes
(20ms window MFCCs)
Naive Bayes
(50ms window MFCCs)
Naive Bayes
(20ms window MFCCs,
inter-key timing)
HMM
(50ms window MFCCs,
5ms increments)

Classiﬁcation
Rate (%)
32.7

Authentication
(% FAR / FRR)
13.1 / 1.6

41.1

69.3

90.7

96.7

94.0

98.0

13.1 / 1.6

25.1 / 8.0

16.0 / 2.8

16.3 / 1.3

12.9 / 1.6

21.1 / 7.9

It should be noted that since there were six users in the
training set, a random algorithm would achieve an accuracy
of 16.7% in the identiﬁcation task, and FAR and FRR of 50%
in the authentication task. The performance numbers of all
classiﬁers discussed in this report are signiﬁcantly bette
r than
random.

V I I . D I SCU S S ION AND FURTHER WORK

The results above clearly show that biometric information
can be extracted from keyboard acoustic emanations and used
to identify users with very high accuracy. The Mel-frequency
cepstral coefﬁcients provide robust features for classiﬁc
ation
with both naive Bayes and HMM classiﬁers, the naive Bayes
classiﬁers being much easier to implement and quicker to tra in.
The authentication results suffer from unacceptable large FAR
to be used in any practical application, but are still signiﬁ cantly
better than random.
Keystroke detection, while error prone, did not seem affect
overall performance of the classiﬁcation task. This is most
likely due to the user prediction being determined by as
the product of many individual feature vector posteriors, and
therefore tolerant to a small number of erroneous data points.
Further work would still be bene ﬁcial in improving keystrok e
detection and determining its affect on the authentication task.
One method for improved keystroke detection can include
a trained HMM. The optimal state sequence when given a
series of feature vectors can then be used to determine which
feature vectors pertain to hit and release peaks, and which are
background noise.
Other work that follows from this project includes analyzing
the effect of increasing the size of the user space as well as
identiﬁcation from free-form typing instead of ﬁxed userna me
and password. Furthermore, the incorporation of other biomet-
rics such as mouse movement patterns can be studied.


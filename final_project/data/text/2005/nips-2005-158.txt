Conditional Visual Tracking in Kernel Space

Cristian Sminchisescu1;2;3 Atul Kanujia3 Zhiguo Li3 Dimitris Metaxas3
1TTI-C, 1497 East 50th Street, Chicago, IL, 60637, USA
2University of Toronto, Department of Computer Science, Canada
3Rutgers University, Department of Computer Science, USA
crismin@cs.toronto.edu, fkanaujia,zhli,dnmg@cs.rutgers.edu

Abstract

We present a conditional temporal probabilistic framework for recon-
structing 3D human motion in monocular video based on descriptors en-
coding image silhouette observations. For computational efﬁcienc y we
restrict visual inference to low-dimensional kernel induced non-linear
state spaces. Our methodology (kBME) combines kernel PCA-based
non-linear dimensionality reduction (kPCA) and Conditional Bayesian
Mixture of Experts (BME) in order to learn complex multivalued pre-
dictors between observations and model hidden states. This is necessary
for accurate, inverse, visual perception inferences, where several proba-
ble, distant 3D solutions exist due to noise or the uncertainty of monoc-
ular perspective projection. Low-dimensional models are appropriate
because many visual processes exhibit strong non-linear correlations in
both the image observations and the target, hidden state variables. The
learned predictors are temporally combined within a conditional graphi-
cal model in order to allow a principled propagation of uncertainty. We
study several predictors and empirically show that the proposed algo-
rithm positively compares with techniques based on regression, Kernel
Dependency Estimation (KDE) or PCA alone, and gives results competi-
tive to those of high-dimensional mixture predictors at a fraction of their
computational cost. We show that the method successfully reconstructs
the complex 3D motion of humans in real monocular video sequences.

1 Introduction and Related Work

We consider the problem of inferring 3D articulated human motion from monocular video.
This research topic has applications for scene understanding including human-computer in-
terfaces, markerless human motion capture, entertainment and surveillance. A monocular
approach is relevant because in real-world settings the human body parts are rarely com-
pletely observed even when using multiple cameras. This is due to occlusions form other
people or objects in the scene. A robust system has to necessarily deal with incomplete,
ambiguous and uncertain measurements. Methods for 3D human motion reconstruction
can be classiﬁed as generative and discriminative. They both require a state representation,
namely a 3D human model with kinematics (joint angles) or shape (surfaces or joint po-
sitions) and they both use a set of image features as observations for state inference. The
computational goal in both cases is the conditional distribution for the model state given

image observations.

Generative model-based approaches [6, 16, 14, 13] have been demonstrated to ﬂe xibly re-
construct complex unknown human motions and to naturally handle problem constraints.
However it is difﬁcult
to construct reliable observation likelihoods due to the complexity
of modeling human appearance. This varies widely due to different clothing and defor-
mation, body proportions or lighting conditions. Besides being somewhat indirect, the
generative approach further imposes strict conditional independence assumptions on the
temporal observations given the states in order to ensure computational tractability. Due
to these factors inference is expensive and produces highly multimodal state distributions
[6, 16, 13]. Generative inference algorithms require complex annealing schedules [6, 13]
or systematic non-linear search for local optima [16] in order to ensure continuing tracking.

These difﬁculties motivate the advent of a complementary class of discriminative algo-
rithms [10, 12, 18, 2], that approximate the state conditional directly, in order to simplify
inference. However, inverse, observation-to-state multivalued mappings are difﬁcult
to
learn (see e.g. ﬁg. 1a) and a probabilistic temporal setting is necessary. In an earlier paper
[15] we introduced a probabilistic discriminative framework for human motion reconstruc-
tion. Because the method operates in the originally selected state and observation spaces
that can be task generic, therefore redundant and often high-dimensional, inference is more
expensive and can be less robust. To summarize, reconstructing 3D human motion in a

Figure 1: (a, Left) Example of 180o ambiguity in predicting 3D human poses from sil-
houette image features (center). It is essential that multiple plausible solutions (e.g. F1 and
F2 ) are correctly represented and tracked over time. A single state predictor will either
average the distant solutions or zig-zag between them, see also tables 1 and 2. (b, Right) A
conditional chain model. The local distributions p(yt jyt(cid:0)1 ; zt ) or p(yt jzt ) are learned as
in ﬁg. 2. For inference, the predicted local state conditional is recursively combined with
the ﬁltered prior c.f . (1).

conditional temporal framework poses the following difﬁculties:
(i) The mapping between
temporal observations and states is multivalued (i.e. the local conditional distributions to be
learned are multimodal), therefore it cannot be accurately represented using global function
approximations. (ii) Human models have multivariate, high-dimensional continuous states
of 50 or more human joint angles. The temporal state conditionals are multimodal which
makes efﬁcient Kalman ﬁltering algorithms inapplicable. General inference methods (par-
ticle ﬁlters, mixtures) have to be used instead, but these are expensive for high-dimensional
models (e.g. when reconstructing the motion of several people that operate in a joint state
space). (iii) The components of the human state and of the silhouette observation vector ex-
hibit strong correlations, because many repetitive human activities like walking or running
have low intrinsic dimensionality. It appears wasteful to work with high-dimensional states
of 50+ joint angles. Even if the space were truly high-dimensional, predicting correlated
state dimensions independently may still be suboptimal.

In this paper we present a conditional temporal estimation algorithm that restricts visual
inference to low-dimensional, kernel induced state spaces. To exploit correlations among
observations and among state variables, we model the local, temporal conditional distri-
butions using ideas from Kernel PCA [11, 19] and conditional mixture modeling [7, 5],
here adapted to produce multiple probabilistic predictions. The corresponding predictor is

referred to as a Conditional Bayesian Mixture of Low-dimensional Kernel-Induced Experts
(kBME). By integrating it within a conditional graphical model framework (ﬁg. 1b), we
can exploit temporal constraints probabilistically. We demonstrate that this methodology is
effective for reconstructing the 3D motion of multiple people in monocular video. Our con-
tribution w.r.t. [15] is a probabilistic conditional inference framework that operates over a
non-linear, kernel-induced low-dimensional state spaces, and a set of experiments (on both
real and artiﬁcial
image sequences) that show how the proposed framework positively com-
pares with powerful predictors based on KDE, PCA, or with the high-dimensional models
of [15] at a fraction of their cost.

2 Probabilistic Inference in a Kernel Induced State Space

We work with conditional graphical models with a chain structure [9], as shown in ﬁg. 1b,
These have continuous temporal states yt , t = 1 : : : T , observations zt . For compactness,
we denote joint states Yt = (y1 ; y2 ; : : : ; yt ) or joint observations Zt = (z1 ; : : : ; zt ).
Learning and inference are based on local conditionals: p(yt jzt ) and p(yt jyt(cid:0)1 ; zt ), with
yt and zt being low-dimensional, kernel induced representations of some initial model
having state xt and observation rt . We obtain zt ; yt from rt , xt using kernel PCA [11, 19].
Inference is performed in a low-dimensional, non-linear, kernel induced latent state space
(see ﬁg. 1b and ﬁg. 2 and (1)). For display or error reporting, we compute the original
conditional p(xjr), or a temporally ﬁltered version p(xt jRt ); Rt = (r1 ; r2 ; : : : ; rt ), using
a learned pre-image state map [3].

2.1 Density Propagation for Continuous Conditional Chains

p(yt jyt(cid:0)1 ; zt )p(yt(cid:0)1 jZt(cid:0)1 )

For online ﬁltering, we compute the optimal distribution p(yt jZt ) for the state yt , con-
ditioned by observations Zt up to time t. The ﬁltered density can be recursively derived
as:
p(yt jZt ) = Zyt(cid:0)1
We compute using a conditional mixture for p(yt jyt(cid:0)1 ; zt ) (a Bayesian mixture of experts
c.f . x2.2) and the prior p(yt(cid:0)1 jZt(cid:0)1 ), each having, say M components. We integrate M 2
pairwise products of Gaussians analytically. The means of the expanded posterior are clus-
tered and the centers are used to initialize a reduced M -component Kullback-Leibler ap-
proximation that is reﬁned using gradient descent [15]. The propagation rule (1) is similar
to the one used for discrete state labels [9], but here we work with multivariate continuous
state spaces and represent the local multimodal state conditionals using kBME (ﬁg. 2), and
not log-linear models [9] (these would require intractable normalization). This complex
continuous model rules out inference based on Kalman ﬁltering or dynamic programming
[9].

(1)

2.2 Learning Bayesian Mixtures over Kernel Induced State Spaces (kBME)

In order to model conditional mappings between low-dimensional non-linear spaces we
rely on kernel dimensionality reduction and conditional mixture predictors. The authors of
KDE [19] propose a powerful structured unimodal predictor. This works by decorrelating
the output using kernel PCA and learning a ridge regressor between the input and each
decorrelated output dimension.

Our procedure is also based on kernel PCA but takes into account the structure of the
studied visual problem where both inputs and outputs are likely to be low-dimensional and
the mapping between them multivalued. The output variables xi are projected onto the
column vectors of the principal space in order to obtain their principal coordinates y i . A

z 2 P (Fr )

p(yjz)

kP CA

(cid:8)r (r) (cid:26) Fr

/ y 2 P (Fx )
Q
Q
Q
Q
Q
Q
Q
Q
Q
Q
Q
(Q
Q
x (cid:25) PreImage(y)

(cid:8)x (x) (cid:26) Fx

kP CA

(cid:8)r

(cid:8)x

r 2 R (cid:26) Rr

x 2 X (cid:26) Rx

p(xjr) (cid:25) p(xjy)

Figure 2: The learned low-dimensional predictor, kBME, for computing p(xjr) (cid:17)
p(xt jrt ); 8t. (We similarly learn p(xt jxt(cid:0)1 ; rt ), with input (x; r) instead of r – here we
illustrate only p(xjr) for clarity.) The input r and the output x are decorrelated using Ker-
nel PCA to obtain z and y respectively. The kernels used for the input and output are (cid:8)r
and (cid:8)x , with induced feature spaces Fr and Fx , respectively. Their principal subspaces
obtained by kernel PCA are denoted by P (Fr ) and P (Fx ), respectively. A conditional
Bayesian mixture of experts p(yjz) is learned using the low-dimensional representation
(z; y). Using learned local conditionals of the form p(yt jzt ) or p(yt jyt(cid:0)1 ; zt ), tempo-
ral inference can be efﬁciently performed in a low-dimensional kernel induced state space
(see e.g. (1) and ﬁg. 1b). For visualization and error measurement, the ﬁltered density, e.g.
p(yt jZt ), can be mapped back to p(xt jRt ) using the pre-image c.f . (3).

similar procedure is performed on the inputs ri to obtain zi . In order to relate the reduced
feature spaces of z and y (P (Fr ) and P (Fx )), we estimate a probability distribution over
mappings from training pairs (zi ; yi ). We use a conditional Bayesian mixture of experts
(BME) [7, 5] in order to account for ambiguity when mapping similar, possibly identical
reduced feature inputs to very different feature outputs, as common in our problem (ﬁg. 1a).
This gives a model that is a conditional mixture of low-dimensional kernel-induced experts
(kBME):

p(yjz) =

g(zj(cid:14) j )N (yjWj z; (cid:6)j )

M
Xj=1
where g(zj(cid:14) j ) is a softmax function parameterized by (cid:14) j and (Wj ; (cid:6)j ) are the parame-
ters and the output covariance of expert j , here a linear regressor. As in many Bayesian
settings [17, 5], the weights of the experts and of the gates, Wj and (cid:14) j , are controlled by
hierarchical priors, typically Gaussians with 0 mean, and having inverse variance hyperpa-
rameters controlled by a second level of Gamma distributions. We learn this model using
a double-loop EM and employ ML-II type approximations [8, 17] with greedy (weight)
subset selection [17, 15].

(2)

Finally, the kBME algorithm requires the computation of pre-images in order to recover
the state distribution x from it’s image y 2 P (Fx ). This is a closed form computation
for polynomial kernels of odd degree. For more general kernels optimization or learning
(regression based) methods are necessary [3]. Following [3, 19], we use a sparse Bayesian
kernel regressor to learn the pre-image. This is based on training data (xi ; yi ):

p(xjy) = N (xjA(cid:8)y (y); (cid:10))

(3)

with parameters and covariances (A; (cid:10)). Since temporal inference is performed in
the low-dimensional kernel induced state space,
the pre-image function needs to be
calculated only for visualizing results or for the purpose of error reporting. Propa-
gating the result from the reduced feature space P (Fx ) to the output space X pro-

/
(
O
O
O
O
(cid:15)
(cid:15)
O
O
O
O
duces a Gaussian mixture with M elements, having coefﬁcients g(zj(cid:14) j ) and components
A> + (cid:10)), where J(cid:8)y is the Jacobian of the mapping (cid:8)y .
N (xjA(cid:8)y (Wj z); AJ(cid:8)y (cid:6)j J>
(cid:8)y

3 Experiments

We run experiments on both real image sequences (ﬁg. 5 and ﬁg. 6) and on sequences where
silhouettes were artiﬁcially rendered. The prediction error is reported in degrees (for mix-
ture of experts, this is w.r.t. the most probable one, but see also ﬁg. 4a), and normalized per
joint angle, per frame. The models are learned using standard cross-validation. Pre-images
are learned using kernel regressors and have average error 1:7o .

Training Set and Model State Representation: For training we gather pairs of 3D human
poses together with their image projections, here silhouettes, using the graphics package
Maya. We use realistically rendered computer graphics human surface models which we
animate using human motion capture [1]. Our original human representation (x) is based
on articulated skeletons with spherical joints and has 56 skeletal d.o.f. including global
translation. The database consists of 8000 samples of human activities including walking,
running, turns, jumps, gestures in conversations, quarreling and pantomime.

Image Descriptors: We work with image silhouettes obtained using statistical background
subtraction (with foreground and background models). Silhouettes are informative for pose
estimation although prone to ambiguities (e.g. the left / right limb assignment in side views)
or occasional lack of observability of some of the d.o.f. (e.g. 180o ambiguities in the global
azimuthal orientation for frontal views, e.g. ﬁg. 1a). These are multiplied by intrinsic for-
ward / backward monocular ambiguities [16]. As observations r, we use shape contexts
extracted on the silhouette [4] (5 radial, 12 angular bins, size range 1/8 to 3 on log scale).

The features are computed at different scales and sizes for points sampled on the silhou-
ette. To work in a common coordinate system, we cluster all features in the training set
into K = 50 clusters. To compute the representation of a new shape feature (a point on the
silhouette), we ‘project’ onto the common basis by (inverse distance) weighted voting into
the cluster centers. To obtain the representation (r) for a new silhouette we regularly sam-
ple 200 points on it and add all their feature vectors into a feature histogram. Because the
representation uses overlapping features of the observation the elements of the descriptor
are not independent. However, a conditional temporal framework (ﬁg. 1b) ﬂe xibly accom-
modates this.

For experiments, we use Gaussian kernels for the joint angle feature space and dot product
kernels for the observation feature space. We learn state conditionals for p(yt jzt ) and
p(yt jyt(cid:0)1 ; zt ) using 6 dimensions for the joint angle kernel induced state space and 25
dimensions for the observation induced feature space, respectively. In ﬁg. 3b) we show
an evaluation of the efﬁcac y of our kBME predictor for different dimensions in the joint
angle kernel induced state space (the observation feature space dimension is here 50). On
the analyzed dancing sequence, that involves complex motions of the arms and the legs,
the non-linear model signiﬁcantly outperforms alternative PCA methods and gives good
predictions for compact, low-dimensional models.1
In tables 1 and 2, as well as ﬁg. 4, we perform quantitative experiments on arti ﬁcially
rendered silhouettes. 3D ground truth joint angles are available and this allows a more

1Running times: On a Pentium 4 PC (3 GHz, 2 GB RAM), a full dimensional BME model with
5 experts takes 802s to train p(xt jxt(cid:0)1 ; rt ), whereas a kBME (including the pre-image) takes 95s to
train p(yt jyt(cid:0)1 ; zt ). The prediction time is 13.7s for BME and 8.7s (including the pre-image cost
1.04s) for kBME. The integration in (1) takes 2.67s for BME and 0.31s for kBME. The speed-up for
kBME is signiﬁcant and likely to increase with original models having higher dimensionality.

s
r
e
t
s
u
l
C
 
f
o
 
r
e
b
m
u
N

1000

100

10

1

100

10

1

r
o
r
r
E
 
n
o
i
t
c
i
d
e
r
P

kBME
KDE_RVM
PCA_BME
PCA_RVM

1 2 3 4 5 6 7 8
Degree of Multimodality

0

20
40
Number of Dimensions

60

Figure 3: (a, Left) Analysis of ‘multimodality’ for a training set. The input zt dimension
is 25, the output yt dimension is 6, both reduced using kPCA. We cluster independently in
(yt(cid:0)1 ; zt ) and yt using many clusters (2100) to simulate small input perturbations and we
histogram the yt clusters falling within each cluster in (yt(cid:0)1 ; zt ). This gives intuition on
the degree of ambiguity in modeling p(yt jyt(cid:0)1 ; zt ), for small perturbations in the input. (b,
Right) Evaluation of dimensionality reduction methods for an arti ﬁcial dancing sequence
(models trained on 300 samples). The kBME is our model x2.2, whereas the KDE-RVM
is a KDE model learned with a Relevance Vector Machine (RVM) [17] feature space map.
PCA-BME and PCA-RVM are models where the mappings between feature spaces (ob-
tained using PCA) is learned using a BME and a RVM. The non-linearity is signi ﬁcant.
Kernel-based methods outperform PCA and give low prediction error for 5-6d models.

systematic evaluation. Notice that the kernelized low-dimensional models generally out-
perform the PCA ones. At the same time, they give results competitive to the ones of
high-dimensional BME predictors, while being lower-dimensional and therefore signi ﬁ-
cantly less expensive for inference, e.g. the integral in (1).

In ﬁg. 5 and ﬁg. 6 we show human motion reconstruction results for two real image se-
quences. Fig. 5 shows the good quality reconstruction of a person performing an agile
jump. (Given the missing observations in a side view, 3D inference for the occluded body
parts would not be possible without using prior knowledge!) For this sequence we do infer-
ence using conditionals having 5 modes and reduced 6d states. We initialize tracking using
p(yt jzt ), whereas for inference we use p(yt jyt(cid:0)1 ; zt ) within (1). In the second sequence
in ﬁg. 6, we simultaneously reconstruct the motion of two people mimicking domestic ac-
tivities, namely washing a window and picking an object. Here we do inference over a
product, 12-dimensional state space consisting of the joint 6d state of each person. We
obtain good 3D reconstruction results, using only 5 hypotheses. Notice however, that the
results are not perfect, there are small errors in the elbow and the bending of the knee for
the subject at the l.h.s., and in the different wrist orientations for the subject at the r.h.s.
This reﬂects
the bias of our training set.

Walk and turn
Conversation
Run and turn left

KDE-RR RVM KDE-RVM BME kBME
4.69
4.27
7.57
4.95
10.46
4.79
4.15
6.31
4.96
7.95
5.22
5.02
6.25
5.01
4.92

Table 1: Comparison of average joint angle prediction error for different models. All
kPCA-based models use 6 output dimensions. Testing is done on 100 video frames for
each sequence, the inputs are artiﬁcially generated silhouettes, not in the training set. 3D
joint angle ground truth is used for evaluation. KDE-RR is a KDE model with ridge regres-
sion (RR) for the feature space mapping, KDE-RVM uses an RVM. BME uses a Bayesian
mixture of experts with no dimensionality reduction. kBME is our proposed model. kPCA-
based methods use kernel regressors to compute pre-images.

h
t
u
r
t
 
d
n
u
o
r
g
 
o
t
 
e
s
o
l
C
 
−
 
y
c
n
e
u
q
e
r
F

30

25

20

15

10

5

0

Expert Prediction

1

2

3
Expert Number

4

5

h
t
u
r
t
 
d
n
u
o
r
G
 
o
t
 
t
s
e
s
o
l
C
 
 
−
 
y
c
n
e
u
q
e
r
F

14

12

10

8

6

4

2

0

1st Probable Prev Output
2nd Probable Prev Output
3rd Probable Prev Output
4th Probable Prev Output
5th Probable Prev Output

1

2

3
Current Expert

4

5

Figure 4: (a, Left) Histogram showing the accuracy of various expert predictors: how
many times the expert ranked as the k-th most probable by the model (horizontal axis) is
closest to the ground truth. The model is consistent (the most probable expert indeed is
the most accurate most frequently), but occasionally less probable experts are better. (b,
Right) Histograms show the dynamics of p(yt jyt(cid:0)1 ; zt ), i.e. how the probability mass is
redistributed among experts between two successive time steps, in a conversation sequence.

Walk and turn back
Run and turn

KDE-RR RVM KDE-RVM BME kBME
7.59
6.9
7.15
3.6
3.72
8.01
8.2
16.08
16.8
17.7

Table 2: Joint angle prediction error computed for two complex sequences with walks, runs
and turns, thus more ambiguity (100 frames). Models have 6 state dimensions. Unimodal
predictors average competing solutions. kBME has signi ﬁcantly lower error.

Figure 5: Reconstruction of a jump (selected frames). Top: original image sequence. Mid-
dle: extracted silhouettes. Bottom: 3D reconstruction seen from a synthetic viewpoint.

4 Conclusion

We have presented a probabilistic framework for conditional inference in latent kernel-
induced low-dimensional state spaces. Our approach has the following properties: (a)

Figure 6: Reconstructing the activities of 2 people operating in an 12-d state space (each
person has its own 6d state). Top: original image sequence. Bottom: 3D reconstruction
seen from a synthetic viewpoint.

Accounts for non-linear correlations among input or output variables, by using kernel non-
linear dimensionality reduction (kPCA); (b) Learns probability distributions over mappings
between low-dimensional state spaces using conditional Bayesian mixture of experts, as re-
quired for accurate prediction. In the resulting low-dimensional kBME predictor ambigu-
ities and multiple solutions common in visual, inverse perception problems are accurately
represented. (c) Works in a continuous, conditional temporal probabilistic setting and of-
fers a formal management of uncertainty. We show comparisons that demonstrate how the
proposed approach outperforms regression, PCA or KDE alone for reconstructing the 3D
human motion in monocular video. Future work we will investigate scaling aspects for
large training sets and alternative structured prediction methods.

References
[1] CMU Human Motion DataBase. Online at http://mocap.cs.cmu.edu/search.html, 2003.
[2] A. Agarwal and B. Triggs. 3d human pose from silhouettes by Relevance Vector Regression.
In CVPR, 2004.
[3] G. Bakir, J. Weston, and B. Scholkopf. Learning to ﬁnd pre-images. In NIPS, 2004.
[4] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape
contexts. PAMI, 24, 2002.
[5] C. Bishop and M. Svensen. Bayesian mixtures of experts. In UAI, 2003.
[6] J. Deutscher, A. Blake, and I. Reid. Articulated Body Motion Capture by Annealed Particle
Filtering. In CVPR, 2000.
[7] M. Jordan and R. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural
Computation, (6):181–214, 1994.
[8] D. Mackay. Bayesian interpolation. Neural Computation, 4(5):720–736, 1992.
[9] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information
extraction and segmentation. In ICML, 2000.
[10] R. Rosales and S. Sclaroff. Learning Body Pose Via Specialized Maps. In NIPS, 2002.
[11] B. Sch ¨olkopf, A. Smola, and K. M ¨uller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Computation, 10:1299–1319, 1998.
[12] G. Shakhnarovich, P. Viola, and T. Darrell. Fast Pose Estimation with Parameter Sensitive
Hashing. In ICCV, 2003.
[13] L. Sigal, S. Bhatia, S. Roth, M. Black, and M. Isard. Tracking Loose-limbed People. In CVPR,
2004.
[14] C. Sminchisescu and A. Jepson. Generative Modeling for Continuous Non-Linearly Embedded
Visual Inference. In ICML, pages 759–766, Banff, 2004.
[15] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Discriminative Density Propagation for
3D Human Motion Estimation. In CVPR, 2005.
[16] C. Sminchisescu and B. Triggs. Kinematic Jump Processes for Monocular 3D Human Tracking.
In CVPR, volume 1, pages 69–76, Madison, 2003.
[17] M. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. JMLR, 2001.
[18] C. Tomasi, S. Petrov, and A. Sastry. 3d tracking = classiﬁcation + interpolation. In ICCV, 2003.
[19] J. Weston, O. Chapelle, A. Elisseeff, B. Scholkopf, and V. Vapnik. Kernel dependency estima-
tion. In NIPS, 2002.


The Curse of Highly Variable Functions for
Local Kernel Machines

Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux
Dept. IRO, Universit ´e de Montr ´eal
P.O. Box 6128, Downtown Branch, Montreal, H3C 3J7, Qc, Canada
fbengioy,delallea,lerouxnig@iro.umontreal.ca

Abstract

We present a series of theoretical arguments supporting the claim that a
large class of modern learning algorithms that rely solely on the smooth-
ness prior – with similarity between examples expressed with a local
kernel – are sensitive to the curse of dimensionality, or more precisely
to the variability of the target. Our discussion covers supervised, semi-
supervised and unsupervised learning algorithms. These algorithms are
found to be local in the sense that crucial properties of the learned func-
tion at x depend mostly on the neighbors of x in the training set. This
makes them sensitive to the curse of dimensionality, well studied for
classical non-parametric statistical learning. We show in the case of the
Gaussian kernel that when the function to be learned has many variations,
these algorithms require a number of training examples proportional to
the number of variations, which could be large even though there may ex-
ist short descriptions of the target function, i.e. their Kolmogorov com-
plexity may be low. This suggests that there exist non-local learning
algorithms that at least have the potential to learn about such structured
but apparently complex functions (because locally they have many vari-
ations), while not using very speciﬁc prior domain knowledge.

1 Introduction
A very large fraction of the recent work in statistical machine learning has been focused
on non-parametric learning algorithms which rely solely, explicitly or implicitely, on the
smoothness prior, which says that we prefer as solution functions f such that when x (cid:25) y ,
f (x) (cid:25) f (y). Additional prior knowledge is expressed by choosing the space of the
data and the particular notion of similarity between examples (typically expressed as a
kernel function). This class of learning algorithms therefore includes most of the ker-
nel machine algorithms (Sch ¨olkopf, Burges and Smola, 1999), such as Support Vector
Machines (SVMs) (Boser, Guyon and Vapnik, 1992; Cortes and Vapnik, 1995) or Gaus-
sian processes (Williams and Rasmussen, 1996), but also unsupervised learning algorithms
that attempt to capture the manifold structure of the data, such as Locally Linear Embed-
ding (Roweis and Saul, 2000), Isomap (Tenenbaum, de Silva and Langford, 2000), ker-
nel PCA (Sch ¨olkopf, Smola and M ¨uller, 1998), Laplacian Eigenmaps (Belkin and Niyogi,
2003), Manifold Charting (Brand, 2003), and spectral clustering algorithms (see (Weiss,
1999) for a review). More recently, there has also been much interest in non-parametric
semi-supervised learning algorithms, such as (Zhu, Ghahramani and Lafferty, 2003; Zhou

et al., 2004; Belkin, Matveeva and Niyogi, 2004; Delalleau, Bengio and Le Roux, 2005),
which also fall in this category, and share many ideas with manifold learning algorithms.

Since this is a very large class of algorithms and it is attracting so much attention, it is
worthwhile to investigate its limitations, and this is the main goal of this paper. Since
these methods share many characteristics with classical non-parametric statistical learning
algorithms (such as the k-nearest neighbors and the Parzen windows regression and density
estimation algorithms (Duda and Hart, 1973)), which have been shown to suffer from the
so-called curse of dimensionality, it is logical to investigate the following question: to what
extent do these modern kernel methods suffer from a similar problem?

f (x) = b +

(cid:11)iKD (x; xi )

In this paper, we focus on algorithms in which the learned function is expressed in terms
of a linear combination of kernel functions applied on the training examples:
n
Xi=1
where optionally a bias term b is added, D = fz1 ; : : : ; zn g are training examples (zi = xi
for unsupervised learning, zi = (xi ; yi ) for supervised learning, and yi can take a special
“missing”
value for semi-supervised learning). The (cid:11)i ’s are scalars chosen by the learning
algorithm using D , and KD ((cid:1); (cid:1)) is the kernel function, a symmetric function (sometimes
expected to be positive deﬁnite), which may be chosen by taking into account all the x i ’s.
A typical kernel function is the Gaussian kernel,
(cid:27)2 jju(cid:0)v jj2
K(cid:27) (u; v) = e(cid:0) 1
with the width (cid:27) controlling how local the kernel is. See (Bengio et al., 2004) to see that
LLE, Isomap, Laplacian eigenmaps and other spectral manifold learning algorithms such
as spectral clustering can be generalized to be written as in eq. 1 for a test point x.

;

(1)

(2)

One obtains consistency of classical non-parametric estimators by appropriately varying
the hyper-parameter that controls the locality of the estimator as n increases. Basically, the
kernel should be allowed to become more and more local, so that statistical bias goes to
zero, but the “ef fective number of examples”
involved in the estimator at x (equal to k for
the k-nearest neighbor estimator) should increase as n increases, so that statistical variance
is also driven to 0. For a wide class of kernel regression estimators, the unconditional
variance and squared bias can be shown to be written as follows (H ¨ardle et al., 2004):

expected error =

C1
n(cid:27)d + C2(cid:27)4 ;
with C1 and C2 not depending on n nor on the dimension d. Hence an optimal bandwidth is
(cid:0)1
4+d , and the resulting generalization error (not counting the noise)
chosen proportional to n
converges in n(cid:0)4=(4+d) , which becomes very slow for large d. Consider for example the
increase in number of examples required to get the same level of error, in 1 dimension
versus d dimensions. If n1 is the number of examples required to get a level of error e,
to get the same level of error in d dimensions requires on the order of n(4+d)=5
examples,
1
i.e. the required number of examples is exponential in d. For the k-nearest neighbor
classiﬁer , a similar result is obtained (Snapp and Venkatesh, 1998):
1
Xj=2
where E1 is the asymptotic error, d is the dimension and n the number of examples.
Note however that, if the data distribution is concentrated on a lower dimensional manifold,
it is the manifold dimension that matters. Indeed, for data on a smooth lower-dimensional
manifold, the only dimension that say a k-nearest neighbor classiﬁer
sees is the dimension

expected error = E1 +

cj n(cid:0)j=d

of the manifold, since it only uses the Euclidean distances between the near neighbors, and
if they lie on such a manifold then the local Euclidean distances approach the local geodesic
distances on the manifold (Tenenbaum, de Silva and Langford, 2000).

2 Minimum Number of Bases Required

In this section we present results showing the number of required bases (hence of training
examples) of a kernel machine with Gaussian kernel may grow linearly with the “v aria-
tions” of the target function that must be captured in order to achieve a given error level.

2.1 Result for Supervised Learning

The following theorem informs us about the number of sign changes that a Gaussian kernel
machine can achieve, when it has k bases (i.e. k support vectors, or at least k training
examples).
Theorem 2.1 (Theorem 2 of (Schmitt, 2002)). Let f : R ! R computed by a Gaussian
kernel machine (eq. 1) with k bases (non-zero (cid:11)i ’s). Then f has at most 2k zeros.

We would like to say something about kernel machines in Rd , and we can do this simply by
considering a straight line in Rd and the number of sign changes that the solution function
f can achieve along that line.
Corollary 2.2. Suppose that the learning problem is such that in order to achieve a given
error level for samples from a distribution P with a Gaussian kernel machine (eq. 1), then
f must change sign at least 2k times along some straight line (i.e., in the case of a classi ﬁer ,
the decision surface must be crossed at least 2k times by that straight line). Then the kernel
machine must have at least k bases (non-zero (cid:11)i ’s).

Proof. Let the straight line be parameterized by x(t) = u + tw , with t 2 R and kwk = 1
without loss of generality. Deﬁne g : R ! R by
g(t) = f (u + tw):

If f is a Gaussian kernel classiﬁer with k 0 bases, then g can be written
k0
(t (cid:0) ti )2
(cid:12)i exp (cid:18)(cid:0)
2(cid:27)2 (cid:19)
Xi=1
where u + tiw is the projection of xi on the line Du;w = fu + tw; t 2 Rg, and (cid:12)i 6= 0.
The number of bases of g is k 00 (cid:20) k 0 , as there may exist xi 6= xj such that ti = tj . Since
g must change sign at least 2k times, thanks to theorem 2.1, we can conclude that g has at
least k bases, i.e. k (cid:20) k 00 (cid:20) k 0 .

g(t) = b +

The above theorem tells us that if we are trying to represent a function that locally varies a
lot (in the sense that its sign along a straight line changes many times), then we need many
training examples to do so with a Gaussian kernel machine. Note that it says nothing about
the dimensionality of the space, but we might expect to have to learn functions that vary
more when the data is high-dimensional. The next theorem con ﬁrms
this suspicion in the
special case of the d-bits parity function:
parity : (b1 ; : : : ; bd ) 2 f0; 1gd 7! (cid:26) 1 if Pd
i=1 bi is even
(cid:0)1 otherwise
We will show that learning this apparently simple function with Gaussians centered on
points in f0; 1gd is difﬁcult,
in the sense that it requires a number of Gaussians exponential
in d (for a ﬁx ed Gaussian width). Note that our corollary 2.2 does not apply to the d-bits

parity function, so it represents another type of local variation (not along a line). However,
we are also able to prove a strong result about that case. We will use the following notations:
Xd = f0; 1gd = fx1 ; x2 ; : : : ; x2d g
H 0
d = f(b1 ; : : : ; bd ) 2 Xd j bd = 0g
H 1
d = f(b1 ; : : : ; bd ) 2 Xd j bd = 1g

(3)

(4)
We say that a decision function f : Rd ! R solves the parity problem if sign(f (xi )) =
parity(xi ) for all i in f1; : : : ; 2d g.
Lemma 2.3. Let f (x) = P2d
i=1 (cid:11)iK(cid:27) (xi ; x) be a linear combination of Gaussians
with same width (cid:27) centered on points xi 2 Xd .
If f solves the parity problem, then
(cid:11)iparity(xi ) > 0 for all i.
Proof. We prove this lemma by induction on d. If d = 1 there are only 2 points. Obviously
one Gaussian is not enough to classify correctly x1 and x2 , so both (cid:11)1 and (cid:11)2 are non-
zero, and (cid:11)1(cid:11)2 < 0 (otherwise f is of constant sign). Without loss of generality, assume
parity(x1 ) = 1 and parity(x2 ) = (cid:0)1. Then f (x1 ) > 0 > f (x2 ), which implies (cid:11)1 (1 (cid:0)
K(cid:27) (x1 ; x2 )) > (cid:11)2 (1 (cid:0) K(cid:27) (x1 ; x2 )) and (cid:11)1 > (cid:11)2 since K(cid:27) (x1 ; x2 ) < 1. Thus (cid:11)1 > 0
and (cid:11)2 < 0, i.e. (cid:11)iparity(xi ) > 0 for i 2 f1; 2g.
Suppose now lemma 2.3 is true for d = d0 (cid:0) 1, and consider the case d = d0 . We denote
in the expansion of f (see eq. 3 for the
i their coefﬁcient
d and by (cid:11)0
i the points in H 0
by x0
deﬁnition of H 0
d ). For x0
d , we denote by x1
d its projection on H 1
d (obtained by
i 2 H 0
i 2 H 1
d we
d and x1
i . For any x 2 H 0
setting its last bit to 1), whose coefﬁcient
in f is (cid:11)1
j 2 H 1
have:
j ; x) = exp  (cid:0)
2(cid:27)2 ! = exp (cid:18)(cid:0)
2(cid:27)2 (cid:19) exp  (cid:0)
2(cid:27)2 !
kx1
j (cid:0) xk2
kx0
j (cid:0) xk2
1
K(cid:27) (x1
= (cid:13)K(cid:27) (x0
j ; x)
where (cid:13) = exp (cid:0)(cid:0) 1
2(cid:27)2 (cid:1) 2 (0; 1). Thus f (x) for x 2 H 0
d can be written
i ; x) + Xx1
f (x) = Xx0
(cid:11)1
j (cid:13)K(cid:27) (x0
(cid:11)0
i K(cid:27) (x0
j ; x)
j 2H 1
i 2H 0
d
d
= Xx0
i (cid:1) K(cid:27) (x0
d (cid:0)(cid:11)0
i + (cid:13)(cid:11)1
i ; x):
i 2H 0
d implicitely deﬁnes a function
d is isomorphic to Xd(cid:0)1 , the restriction of f to H 0
Since H 0
d is 0, the parity is not
over Xd(cid:0)1 that solves the parity problem (because the last bit in H 0
modiﬁed). Using our induction hypothesis, we have that for all x0
d :
i 2 H 0
i (cid:1) parity(x0
i + (cid:13)(cid:11)1
(cid:0)(cid:11)0
(5)
i ) > 0:
A similar reasoning can be made if we switch the roles of H 0
d and H 1
d . One has to be careful
d and its mapping to Xd(cid:0)1 (because the last bit in H 1
that the parity is modiﬁed between H 1
d
is 1). Thus we obtain that the restriction of ((cid:0)f ) to H 1
d deﬁnes a function over Xd(cid:0)1 that
d :
solves the parity problem, and the induction hypothesis tells us that for all x1
j 2 H 1
j (cid:1)(cid:1) (cid:0)(cid:0)parity(x1
j + (cid:13)(cid:11)0
(cid:0)(cid:0) (cid:0)(cid:11)1
(6)
j )(cid:1) > 0:
d and its projection
and the two negative signs cancel out. Now consider any x0
i 2 H 0
d . Without loss of generality, assume parity(x0
i ) = 1 (and thus parity(x1
i ) = (cid:0)1).
x1
i 2 H 1
Using eq. 5 and 6 we obtain:

i + (cid:13)(cid:11)1
(cid:11)0
i > 0
i + (cid:13)(cid:11)0
(cid:11)1
i < 0

It is obvious that for these two equations to be simultaneously veriﬁed, we need (cid:11)0
i and
i to be non-zero and of opposite sign. Moreover, because (cid:13) 2 (0; 1), (cid:11)0
i + (cid:13)(cid:11)1
(cid:11)1
i > 0 >
i , which implies (cid:11)0
i > 0 and (cid:11)1
i < 0, i.e. (cid:11)0
i ) > 0 and
(cid:11)1
i + (cid:13)(cid:11)0
i ) (cid:11)0
i > (cid:11)1
i parity(x0
i ) > 0. Since this is true for all x0
i in H 0
d , we have proved lemma 2.3.
(cid:11)1
i parity(x1
Theorem 2.4. Let f (x) = b + P2d
i=1 (cid:11)iK(cid:27) (xi ; x) be an afﬁne combination of Gaussians
with same width (cid:27) centered on points xi 2 Xd . If f solves the parity problem, then there
are at least 2d(cid:0)1 non-zero coefﬁcients (cid:11)i .
Proof. We begin with two preliminary results. First, given any xi 2 Xd , the number of
points in Xd that differ from xi by exactly k bits is (cid:0)d
k(cid:1). Thus,
d
k2
2(cid:27)2 (cid:19) = c(cid:27) :
Xk=0 (cid:18)d
k(cid:19) exp (cid:18)(cid:0)
Xxj 2Xd
K(cid:27) (xi ; xj ) =
Second, it is possible to ﬁnd a linear combination (i.e. without bias) of Gaussians g such
that g(xi ) = f (xi ) for all xi 2 Xd . Indeed, let
g(x) = f (x) (cid:0) b + Xxj 2Xd
(cid:12)j K(cid:27) (xj ; xi ) = b, i.e. the vector (cid:12) satisﬁes the linear
g veriﬁes g(xi ) = f (xi ) iff Pxj 2Xd
system M(cid:27) (cid:12) = b1, where M(cid:27) is the kernel matrix whose element (i; j ) is K(cid:27) (xi ; xj )
and 1 is a vector of ones. It is well known that M(cid:27) is invertible as long as the xi are all
1 is the only solution
different, which is the case here (Micchelli, 1986). Thus (cid:12) = bM (cid:0)1
(cid:27)
to the system.
We now proceed to the proof of the theorem. By contradiction, suppose f solves the parity
problem with less than 2d(cid:0)1 non-zero coefﬁcients (cid:11)i . Then there exist two points xs and xt
in Xd such that (cid:11)s = (cid:11)t = 0 and parity(xs ) = 1 = (cid:0)parity(xt ). Consider the function
g deﬁned as in eq. 8 with (cid:12) = bM (cid:0)1
1. Since g(xi ) = f (xi ) for all xi 2 Xd , g solves
(cid:27)
the parity problem with a linear combination of Gaussians centered points in Xd . Thus,
applying lemma 2.3, we have in particular that (cid:12)sparity(xs ) > 0 and (cid:12)tparity(xt ) > 0
(because (cid:11)s = (cid:11)t = 0), so that (cid:12)s(cid:12)t < 0. But, because of eq. 7, M(cid:27) 1 = c(cid:27) 1, which means
1 is an eigenvector of M(cid:27) with eigenvalue c(cid:27) > 0. Consequently, 1 is also an eigenvector
of M (cid:0)1
(cid:27) > 0, and (cid:12) = bM (cid:0)1
(cid:27) with eigenvalue c(cid:0)1
1, which is in contradiction
1 = bc(cid:0)1
(cid:27)
(cid:27)
with (cid:12)s(cid:12)t < 0: f must therefore have at least 2d(cid:0)1 non-zero coefﬁcients.

(cid:12)j K(cid:27) (xj ; x):

(7)

(8)

The bound in theorem 2.4 is tight, since it is possible to solve the parity problem with
exactly 2d(cid:0)1 Gaussians and a bias, for instance by using a negative bias and putting a
positive weight on each example satisfying parity(xi ) = 1. When trained to learn the
parity function, a SVM may learn a function that looks like the opposite of the parity on
test points (while still performing optimally on training points), but it is an artefact of the
speciﬁc geometry of the problem, and only occurs when the training set size is appropriate
compared to jXd j = 2d (see (Bengio, Delalleau and Le Roux, 2005) for details). Note that
if the centers of the Gaussians are not restricted anymore to be points in Xd , it is possible
to solve the parity problem with only d + 1 Gaussians and no bias (Bengio, Delalleau and
Le Roux, 2005).

One may argue that parity is a simple discrete toy problem of little interest. But even if
we have to restrict the analysis to discrete samples in f0; 1gd for mathematical reasons, the
parity function can be extended to a smooth function on the [0; 1]d hypercube depending
only on the continuous sum b1 + : : : + bd . Theorem 2.4 is thus a basis to argue that the
number of Gaussians needed to learn a function with many variations in a continuous space
may scale linearly with these variations, and thus possibly exponentially in the dimension.

2.2 Results for Semi-Supervised Learning

In this section we focus on algorithms of the type described in recent papers (Zhu, Ghahra-
mani and Lafferty, 2003; Zhou et al., 2004; Belkin, Matveeva and Niyogi, 2004; Delalleau,
Bengio and Le Roux, 2005), which are graph-based non-parametric semi-supervised learn-
ing algorithms. Note that transductive SVMs, which are another class of semi-supervised
algorithms, are already subject to the limitations of corollary 2.2. The graph-based algo-
rithms we consider here can be seen as minimizing the following cost function, as shown
in (Delalleau, Bengio and Le Roux, 2005):
C ( ^Y ) = k ^Yl (cid:0) Yl k2 + (cid:22) ^Y >L ^Y + (cid:22)(cid:15)k ^Y k2
(9)
with ^Y = ( ^y1 ; : : : ; ^yn ) the estimated labels on both labeled and unlabeled data, and L the
(un-normalized) graph Laplacian derived from a similarity function W between points such
that Wij = W (xi ; xj ) corresponds to the weights of the edges in the graph. Here, ^Yl =
( ^y1 ; : : : ; ^yl ) is the vector of estimated labels on the l labeled examples, whose known labels
are given by Yl = (y1 ; : : : ; yl ), and one may constrain ^Yl = Yl as in (Zhu, Ghahramani and
Lafferty, 2003) by letting (cid:22) ! 0. We deﬁne a region with constant label as a connected
subset of the graph where all nodes xi have the same estimated label (sign of ^yi ), and such
that no other node can be added while keeping these properties.
Proposition 2.5. After running a label propagation algorithm minimizing the cost of eq. 9,
the number of regions with constant estimated label is less than (or equal to) the number
of labeled examples.

C ( ^yl+1 ; : : : ; ^yl+q ) =

Proof. By contradiction, if this proposition is false, then there exists a region with constant
estimated label that does not contain any labeled example. Without loss of generality,
consider the case of a positive constant label, with xl+1 ; : : : ; xl+q the q samples in this
region. The part of the cost of eq. 9 depending on their labels is
l+q
Xi;j=l+1
Wij ( ^yi (cid:0) ^yj )2
0
Wij ( ^yi (cid:0) ^yj )21
l+q
l+q
Xi=l+1
@ Xj =2fl+1;:::;l+qg
Xi=l+1
A + (cid:22)(cid:15)
The second term is stricly positive, and because the region we consider is maximal (by
deﬁnition) all samples xj outside of the region such that Wij > 0 verify ^yj < 0 (for xi a
sample in the region). Since all ^yi are stricly positive for i 2 fl + 1; : : : ; l + qg, this means
this second term can be stricly decreased by setting all ^yi to 0 for i 2 fl + 1; : : : ; l + qg.
This also sets the ﬁrst and third terms to zero (i.e. their minimum), showing that the set of
labels ^yi are not optimal, which conﬂicts with their deﬁnition as labels minimizing C .

^y2
i :

(cid:22)
2

+ (cid:22)

This means that if the class distributions are such that there are many distinct regions with
constant labels (either separated by low-density regions or regions with samples from the
other class), we will need at least the same number of labeled samples as there are such
regions (assuming we are using a sparse local kernel such as the k-nearest neighbor kernel,
or a thresholded Gaussian kernel). But this number could grow exponentially with the
dimension of the manifold(s) on which the data lie, for instance in the case of a labeling
function varying highly along each dimension, even if the label variations are “simple”
in
a non-local sense, e.g. if they alternate in a regular fashion. When the kernel is not sparse
(e.g. Gaussian kernel), obtaining such a result is less obvious. However, there often exists
a sparse approximation of the kernel. Thus we conjecture the same kind of result holds for
dense weight matrices, if the weighting function is local in the sense that it is close to zero
when applied to a pair of examples far from each other.

3 Extensions and Conclusions

In (Bengio, Delalleau and Le Roux, 2005) we present additional results that apply to unsu-
pervised learning algorithms such as non-parametric manifold learning algorithms (Roweis
and Saul, 2000; Tenenbaum, de Silva and Langford, 2000; Sch ¨olkopf, Smola and M ¨uller,
1998; Belkin and Niyogi, 2003). We ﬁnd that when the underlying manifold varies a lot
in the sense of having high curvature in many places, then a large number of examples is
required. Note that the tangent plane is deﬁned by the derivatives of the kernel machine
function f , for such algorithms. The core result is that the manifold tangent plane at x
is mostly deﬁned by the near neighbors of x in the training set (more precisely it is con-
strained to be in the span of the vectors x (cid:0) xi , with xi a neighbor of x). Hence one needs
to cover the manifold with small enough linear patches with at least d + 1 examples per
patch (where d is the dimension of the manifold).

In the same paper, we present a conjecture that generalizes the results presented here for
Gaussian kernel classiﬁers
to a larger class of local kernels, using the same notion of local-
ity of the derivative summarized above for manifold learning algorithms. In that case the
derivative of f represents the normal of the decision surface, and we ﬁnd that at x it mostly
depends on the neighbors of x in the training set.

It could be argued that if a function has many local variations (hence is not very smooth),
then it is not learnable unless having strong prior knowledge at hand. However, this is not
true. For example consider functions that have low Kolmogorov complexity, i.e. can be
described by a short string in some language. The only prior we need in order to quickly
learn such functions (in terms of number of examples needed) is that functions that are
simple to express in that language (e.g. a programming language) are preferred. For ex-
ample, the functions g(x) = sin(x) or g(x) = parity(x) would be easy to learn using
the C programming language to deﬁne the prior, even though the number of variations of
g(x) can be chosen to be arbitrarily large (hence also the number of required training ex-
amples when using only the smoothness prior), while keeping the Kolmogorov complexity
constant. We do not propose to necessarily focus on the Kolmogorov complexity to design
new learning algorithms, but we use this example to illustrate that it is possible to learn
apparently complex functions (because they vary a lot), as long as one uses a “non-local”
learning algorithm, corresponding to a broad prior, not solely relying on the smoothness
prior. Of course, if additional domain knowledge about the task is available, it should be
used, but without abandoning research on learning algorithms that can address a wider
scope of problems. We hope that this paper will stimulate more research into such learning
algorithms, since we expect local learning algorithms (that only rely on the smoothness
to make signi ﬁcant progress on complex problems such as those
prior) will be insufﬁcient
raised by research on Artiﬁcial
Intelligence.

Acknowledgments

The authors would like to thank the following funding organizations for support: NSERC,
MITACS, and the Canada Research Chairs. The authors are also grateful for the feedback
and stimulating exchanges that helped shape this paper, with Yann Le Cun and L ´eon Bottou,
as well as for the anonymous reviewers’ helpful comments.

References

Belkin, M., Matveeva, I., and Niyogi, P. (2004). Regularization and semi-supervised learn-
ing on large graphs. In Shawe-Taylor, J. and Singer, Y., editors, COLT’2004. Springer.
Belkin, M. and Niyogi, P. (2003). Using manifold structure for partially labeled classi-
ﬁcation.
In Becker, S., Thrun, S., and Obermayer, K., editors, Advances in Neural

Information Processing Systems 15, Cambridge, MA. MIT Press.
Bengio, Y., Delalleau, O., and Le Roux, N. (2005). The curse of dimensionality for local
kernel machines. Technical Report 1258, D ´epartement d’informatique et recherche
op ´erationnelle, Universit ´e de Montr ´eal.
Bengio, Y., Delalleau, O., Le Roux, N., Paiement, J.-F., Vincent, P., and Ouimet, M. (2004).
Learning eigenfunctions links spectral embedding and kernel PCA. Neural Computa-
tion, 16(10):2197–2219.
Boser, B., Guyon, I., and Vapnik, V. (1992). A training algorithm for optimal margin
classiﬁers.
In Fifth Annual Workshop on Computational Learning Theory, pages 144–
152, Pittsburgh.
Brand, M. (2003). Charting a manifold.
In Becker, S., Thrun, S., and Obermayer, K.,
editors, Advances in Neural Information Processing Systems 15. MIT Press.
Cortes, C. and Vapnik, V. (1995). Support vector networks. Machine Learning, 20:273–
297.
Delalleau, O., Bengio, Y., and Le Roux, N. (2005). Efﬁcient non-parametric function
In Cowell, R. and Ghahramani, Z., editors,
induction in semi-supervised learning.
Proceedings of the Tenth International Workshop on Artiﬁcial
Intelligence and Statis-
tics, Jan 6-8, 2005, Savannah Hotel, Barbados, pages 96 –103. Society for Artiﬁcial
Intelligence and Statistics.
Duda, R. and Hart, P. (1973). Pattern Classiﬁcation and Scene Analysis. Wiley, New York.
H ¨ardle, W., M ¨uller, M., Sperlich, S., and Werwatz, A. (2004). Nonparametric and Semi-
parametric Models. Springer, http://www.xplore-stat.de/ebooks/ebooks.html.
Micchelli, C. A. (1986). Interpolation of scattered data: distance matrices and condition-
ally positive deﬁnite functions. Constructive Approximation, 2:11–22.
Roweis, S. and Saul, L. (2000). Nonlinear dimensionality reduction by locally linear em-
bedding. Science, 290(5500):2323–2326.
Schmitt, M. (2002). Descartes’ rule of signs for radial basis function neural networks.
Neural Computation, 14(12):2997 –3011.
Sch ¨olkopf, B., Burges, C. J. C., and Smola, A. J. (1999). Advances in Kernel Methods —
Support Vector Learning. MIT Press, Cambridge, MA.
Sch ¨olkopf, B., Smola, A., and M ¨uller, K.-R. (1998). Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation, 10:1299–1319.
Snapp, R. R. and Venkatesh, S. S. (1998). Asymptotic derivation of the ﬁnite-sample risk of
the k nearest neighbor classiﬁer . Technical Report UVM-CS-1998-0101, Department
of Computer Science, University of Vermont.
Tenenbaum, J., de Silva, V., and Langford, J. (2000). A global geometric framework for
nonlinear dimensionality reduction. Science, 290(5500):2319 –2323.
Weiss, Y. (1999). Segmentation using eigenvectors: a unifying view. In Proceedings IEEE
International Conference on Computer Vision, pages 975–982.
Williams, C. and Rasmussen, C. (1996). Gaussian processes for regression. In Touretzky,
D., Mozer, M., and Hasselmo, M., editors, Advances in Neural Information Process-
ing Systems 8, pages 514–520. MIT Press, Cambridge, MA.
Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., and Sch ¨olkopf, B. (2004). Learning
with local and global consistency. In Thrun, S., Saul, L., and Sch ¨olkopf, B., editors,
Advances in Neural Information Processing Systems 16, Cambridge, MA. MIT Press.
Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-supervised learning using Gaussian
ﬁelds and harmonic functions. In ICML’2003.


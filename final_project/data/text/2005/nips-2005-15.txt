Convex Neural Networks

Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte
Dept. IRO, Universit ´e de Montr ´eal
P.O. Box 6128, Downtown Branch, Montreal, H3C 3J7, Qc, Canada
fbengioy,lerouxni,vincentp,delallea,marcotteg@iro.umontreal.ca

Abstract
Convexity has recently received a lot of attention in the machine learning
community, and the lack of convexity has been seen as a major disad-
vantage of many learning algorithms, such as multi-layer artiﬁcial neural
networks. We show that training multi-layer neural networks in which the
number of hidden units is learned can be viewed as a convex optimization
problem. This problem involves an inﬁnite number of variables, but can be
solved by incrementally inserting a hidden unit at a time, each time ﬁnding
that minimizes a weighted sum of errors.
a linear classiﬁer

1 Introduction
The objective of this paper is not to present yet another learning algorithm, but rather to point
to a previously unnoticed relation between multi-layer neural networks (NNs),Boosting (Fre-
und and Schapire, 1997) and convex optimization. Its main contributions concern the mathe-
matical analysis of an algorithm that is similar to previously proposed incremental NNs, with
L1 regularization on the output weights. This analysis helps to understand the underlying
convex optimization problem that one is trying to solve.

This paper was motivated by the unproven conjecture (based on anecdotal experience) that
when the number of hidden units is “lar ge”,
the resulting average error is rather insensitive to
the random initialization of the NN parameters. One way to justify this assertion is that to re-
ally stay stuck in a local minimum, one must have second derivatives positive simultaneously
in all directions. When the number of hidden units is large, it seems implausible for none of
them to offer a descent direction. Although this paper does not prove or disprove the above
conjecture, in trying to do so we found an interesting characterization of the optimization
problem for NNs as a convex program if the output loss function is convex in the NN out-
put and if the output layer weights are regularized by a convex penalty. More speciﬁcally ,
if the regularization is the L1 norm of the output layer weights, then we show that a “rea-
sonable” solution exists, involving a ﬁnite number of hidden units (no more than the number
of examples, and in practice typically much less). We present a theoretical algorithm that
is reminiscent of Column Generation (Chv ´atal, 1983), in which hidden neurons are inserted
one at a time. Each insertion requires solving a weighted classiﬁcation problem, very much
like in Boosting (Freund and Schapire, 1997) and in particular Gradient Boosting (Mason
et al., 2000; Friedman, 2001).
Neural Networks, Gradient Boosting, and Column Generation

Denote ~x 2 Rd+1 the extension of vector x 2 Rd with one element with value 1. What
we call “Neural Network”
(NN) here is a predictor for supervised learning of the form
^y(x) = Pm
i=1 wihi (x) where x is an input vector, hi (x) is obtained from a linear dis-
criminant function hi (x) = s(vi (cid:1) ~x) with e.g. s(a) = sign(a), or s(a) = tanh(a) or
s(a) = 1
1+e(cid:0)a . A learning algorithm must specify how to select m, the wi ’s and the vi ’s.

The classical solution (Rumelhart, Hinton and Williams, 1986) involves (a) selecting a loss
function Q( ^y ; y) that speciﬁes how to penalize for mismatches between ^y(x) and the ob-
served y ’s (target output or target class), (b) optionally selecting a regularization penalty that
favors “small”
parameters, and (c) choosing a method to approximately minimize the sum of
the losses on the training data D = f(x1 ; y1 ); : : : ; (xn ; yn )g plus the regularization penalty.
Note that in this formulation, an output non-linearity can still be used, by inserting it in the
loss function Q. Examples of such loss functions are the quadratic loss jj ^y (cid:0) y jj2 , the hinge
loss max(0; 1 (cid:0) y ^y) (used in SVMs), the cross-entropy loss (cid:0)y log ^y (cid:0) (1 (cid:0) y) log(1 (cid:0) ^y)
(used in logistic regression), and the exponential loss e(cid:0)y ^y (used in Boosting).
Gradient Boosting has been introduced in (Friedman, 2001) and (Mason et al., 2000) as a
non-parametric greedy-stagewise supervised learning algorithm in which one adds a function
at a time to the current solution ^y(x), in a steepest-descent fashion, to form an additive model
as above but with the functions hi typically taken in other kinds of sets of functions, such as
those obtained with decision trees. In a stagewise approach, when the (m + 1)-th basis hm+1
is added, only wm+1 is optimized (by a line search), like in matching pursuit algorithms.Such
a greedy-stagewise approach is also at the basis of Boosting algorithms (Freund and Schapire,
1997), which is usually applied using decision trees as bases and Q the exponential loss.
to minimize exactly for wm+1 and hm+1 when the previous bases and
It may be difﬁcult
weights are ﬁx ed, so (Friedman, 2001) proposes to “follo w the gradient”
in function space,
i.e., look for a base learner hm+1 that is best correlated with the gradient of the average
loss on the ^y(xi ) (that would be the residue ^y(xi ) (cid:0) yi in the case of the square loss). The
algorithm analyzed here also involves maximizing the correlation between Q 0 (the derivative
argument, evaluated on the training predictions) and the next
of Q with respect to its ﬁrst
basis hm+1 . However, we follow a “stepwise”,
less greedy, approach, in which all the output
weights are optimized at each step, in order to obtain convergence guarantees.

Our approach adapts the Column Generation principle (Chv ´atal, 1983), a decomposition
technique initially proposed for solving linear programs with many variables and few con-
straints. In this framework, active variables, or “columns”,
are only generated as they are
required to decrease the objective. In several implementations, the column-generation sub-
problem is frequently a combinatorial problem for which efﬁcient algorithms are available.
linear classiﬁer .
In our case, the subproblem corresponds to determining an “optimal”

2 Core Ideas
Informally, consider the set H of all possible hidden unit functions (i.e., of all possible hidden
unit weight vectors vi ). Imagine a NN that has all the elements in this set as hidden units. We
might want to impose precision limitations on those weights to obtain either a countable or
even a ﬁnite set. For such a NN, we only need to learn the output weights. If we end up with
a ﬁnite number of non-zero output weights, we will have at the end an ordinary feedforward
NN. This can be achieved by using a regularization penalty on the output weights that yields
sparse solutions, such as the L1 penalty. If in addition the loss function is convex in the output
layer weights (which is the case of squared error, hinge loss, (cid:15)-tube regression loss, and
logistic or softmax cross-entropy), then it is easy to show that the overall training criterion
is convex in the parameters (which are now only the output weights). The only problem is
that there are as many variables in this convex program as there are elements in the set H,
which may be very large (possibly inﬁnite). However, we ﬁnd that with L1 regularization,
a ﬁnite solution is obtained, and that such a solution can be obtained by greedily inserting
one hidden unit at a time. Furthermore, it is theoretically possible to check that the global
optimum has been reached.
Deﬁnition 2.1. Let H be a set of functions from an input space X to R. Elements of H
can be understood as “hidden units”
in a NN. Let W be the Hilbert space of functions from
H to R, with an inner product denoted by a (cid:1) b for a; b 2 W . An element of W can be
understood as the output weights vector in a neural network. Let h(x) : H ! R the function
that maps any element hi of H to hi (x). h(x) can be understood as the vector of activations

of hidden units when input x is observed. Let w 2 W represent a parameter (the output
weights). The NN prediction is denoted ^y(x) = w (cid:1) h(x). Let Q : R (cid:2) R ! R be a
cost function convex in its ﬁr st argument that takes a scalar prediction ^y(x) and a scalar
target value y and returns a scalar cost. This is the cost to be minimized on example pair
(x; y). Let D = f(xi ; yi ) : 1 (cid:20) i (cid:20) ng a training set. Let (cid:10) : W ! R be a convex
regularization functional that penalizes for the choice of more “comple x” parameters (e.g.,
(cid:10)(w) = (cid:21)jjwjj1 according to a 1-norm in W , if H is countable). We deﬁne the convex NN
criterion C (H; Q; (cid:10); D ; w) with parameter w as follows:
n
Xt=1
The following is a trivial lemma, but it is conceptually very important as it is the basis for the
rest of the analysis in this paper.
Lemma 2.2. The convex NN cost C (H; Q; (cid:10); D ; w) is a convex function of w .

C (H; Q; (cid:10); D ; w) = (cid:10)(w) +

Q(w (cid:1) h(xt ); yt ):

(1)

Proof. Q(w (cid:1) h(xt ); yt ) is convex in w and (cid:10) is convex in w , by the above construction. C
is additive in Q(w (cid:1) h(xt ); yt ) and additive in (cid:10). Hence C is convex in w .

Note that there are no constraints in this convex optimization program, so that at the global
minimum all the partial derivatives of C with respect to elements of w cancel.
it is not obvious that an optimal
Let jHj be the cardinality of the set H. If it is not ﬁnite,
solution can be achieved in ﬁnitely many iterations.

Lemma 2.2 says that training NNs from a very large class (with one or more hidden layer)
can be seen as convex optimization problems, usually in a very high dimensional space, as
long as we allow the number of hidden units to be selected by the learning algorithm.
By choosing a regularizer that promotes sparse solutions, we obtain a solution that has a
ﬁnite number of “acti ve” hidden units (non-zero entries in the output weights vector w).
This assertion is proven below, in theorem 3.1, for the case of the hinge loss.

However, even if the solution involves a ﬁnite number of active hidden units, the convex
optimization problem could still be computationally intractable because of the large number
of variables involved. One approach to this problem is to apply the principles already suc-
cessfully embedded in Gradient Boosting, but more speciﬁcally in Column Generation (an
optimization technique for very large scale linear programs), i.e., add one hidden unit at a
time in an incremental fashion. The important ingredient here is a way to know that we
have reached the global optimum, thus not requiring to actually visit all the possible
hidden units. We show that this can be achieved as long as we can solve the sub-problem
of ﬁnding a linear classiﬁer
that minimizes the weighted sum of classiﬁcation errors. This
can be done exactly only on low dimensional data sets but can be well approached using
weighted linear SVMs, weighted logistic regression, or Perceptron-type algorithms.
Another idea (not followed up here) would be to consider ﬁrst a smaller set H1 , for which
the convex problem can be solved in polynomial time, and whose solution can theoretically
be selected as initialization for minimizing the criterion C (H2 ; Q; (cid:10); D ; w), with H1 (cid:26) H2 ,
and where H2 may have inﬁnite cardinality (countable or not). In this way we could show
that we can ﬁnd a solution whose cost satisﬁes C (H2 ; Q; (cid:10); D ; w) (cid:20) C (H1 ; Q; (cid:10); D ; w),
i.e., is at least as good as the solution of a more restricted convex optimization problem. The
second minimization can be performed with a local descent algorithm, without the necessity
to guarantee that the global optimum will be found.
3 Finite Number of Hidden Neurons
In this section we consider the special case with Q( ^y ; y) = max(0; 1 (cid:0) y ^y) the hinge loss,
and L1 regularization, and we show that the global optimum of the convex cost involves at
most n + 1 hidden neurons, using an approach already exploited in (R ¨atsch, Demiriz and
Bennett, 2002) for L1 -loss regression Boosting with L1 regularization of output weights.

(P ) :

(cid:21)t

min
w;(cid:24)

L(w; (cid:24) ) = K kwk1 +

(cid:24)t

(C1 )
(C2 )

max (0; 1 (cid:0) ytw (cid:1) h(xt )). Let us rewrite

The training criterion is C (w) = K kwk1 +

n
Xt=1
this cost function as the constrained optimization problem:
n
s.t. (cid:26)
yt [w (cid:1) h(xt )] (cid:21) 1 (cid:0) (cid:24)t
Xt=1
and
(cid:24)t (cid:21) 0; t = 1; : : : ; n
Using a standard technique, the above program can be recast as a linear program. Deﬁn-
ing (cid:21) = ((cid:21)1 ; : : : ; (cid:21)n ) the vector of Lagrangian multipliers for the constraints C1 , its dual
problem (P ) takes the form (in the case of a ﬁnite number J of base learners):
n
s.t. (cid:26)
(cid:21) (cid:1) Zi (cid:0) K (cid:20) 0; i 2 I
Xt=1
max
and (cid:21)t (cid:20) 1; t = 1; : : : ; n
(cid:21)
with (Zi )t = ythi (xt ). In the case of a ﬁnite number J of base learners, I = f1; : : : ; J g. If
the number of hidden units is uncountable, then I is a closed bounded interval of R.
Such an optimization problem satisﬁes
all the conditions needed for using Theorem 4.2
from (Hettich and Kortanek, 1993). Indeed:
(cid:15) I is compact (as a closed bounded interval of R);
(cid:15) F : (cid:21) 7! Pn
t=1 (cid:21)t is a concave function (it is even a linear function);
(cid:15) g : ((cid:21); i) 7! (cid:21) (cid:1) Zi (cid:0) K is convex in (cid:21) (it is actually linear in (cid:21));
(cid:15) (cid:23) (P ) (cid:20) n (therefore ﬁnite)
((cid:23) (P ) is the largest value of F satisfying the constraints);
(cid:15) for every set of n + 1 points i0 ; : : : ; in 2 I , there exists ~(cid:21) such that g(~(cid:21); ij ) < 0 for
j = 0; : : : ; n (one can take ~(cid:21) = 0 since K > 0).
Then, from Theorem 4.2 from (Hettich and Kortanek, 1993), the following theorem holds:
Theorem 3.1. The solution of (P ) can be attained with constraints C 0
2 and only n + 1 con-
1 giving rise to the same maximum
1 (i.e., there exists a subset of n+ 1 constraints C 0
straints C 0
as when using the whole set of constraints). Therefore, the primal problem associated is the
minimization of the cost function of a NN with n + 1 hidden neurons.

4 Incremental Convex NN Algorithm
In this section we present a stepwise algorithm to optimize a NN, and show that there is a cri-
terion that allows to verify whether the global optimum has been reached. This is a specializa-
tion of minimizing C (H; Q; (cid:10); D ; w), with (cid:10)(w) = (cid:21)jjwjj1 and H = fh : h(x) = s(v (cid:1) ~x)g
is the set of soft or hard linear classiﬁers
(depending on choice of s((cid:1))).

Algorithm ConvexNN(D ,Q,(cid:21),s)
Input: training set D = f(x1 ; y1 ); : : : ; (xn ; yn )g, convex loss function Q, and scalar
regularization penalty (cid:21). s is either the sign function or the tanh function.
(1) Set v1 = (0; 0; : : : ; 1) and select w1 = argminw1 Pt Q(w1 s(1); yt ) + (cid:21)jw1 j.
(2) Set i = 2.
(3) Repeat
Let qt = Q0 (Pi(cid:0)1
(4)
j=1 wj hj (xt ); yt )
(5)
If s = sign
(5a)
train linear classiﬁer hi (x) = sign(vi (cid:1) ~x) with examples f(xt ; sign(qt ))g
and errors weighted by jqt j, t = 1 : : : n (i.e., maximize Pt qthi (xt ))
else (s = tanh)
train linear classiﬁer hi (x) = tanh(vi (cid:1) ~x) to maximize Pt qthi (xt ).
If Pt qthi (xt ) < (cid:21), stop.
Select w1 ; : : : ; wi (and optionally v2 ; : : : ; vi ) minimizing (exactly or
approximately) C = Pt Q(Pi
j=1 wj hj (xt ); yt ) + (cid:21) Pj=1 jwj j
such that @C
= 0 for j = 1 : : : i.
@wj
(8) Return the predictor ^y(x) = Pi
j=1 wj hj (x).

(5b)
(5c)
(6)
(7)

A key property of the above algorithm is that, at termination, the global optimum is reached,
i.e., no hidden unit (linear classiﬁer) can improve the objective. In the case where s = sign,
we obtain a Boosting-like algorithm, i.e., it involves ﬁnding a classiﬁer which minimizes the
weighted cost Pt qt sign(v (cid:1) ~xt ).
Theorem 4.1. Algorithm ConvexNN stops when it reaches the global optimum of
C (w) = Pt Q(w (cid:1) h(xt ); yt ) + (cid:21)jjwjj1 .
Proof. Let w be the output weights vector when the algorithm stops. Because the set of
hidden units H we consider is such that when h is in H, (cid:0)h is also in H, we can assume
all weights to be non-negative. By contradiction, if w 0
6= w is the global optimum, with
C (w 0 ) < C (w), then, since C is convex in the output weights, for any (cid:15) 2 (0; 1), we have
C ((cid:15)w 0 + (1 (cid:0) (cid:15))w) (cid:20) (cid:15)C (w 0 ) + (1 (cid:0) (cid:15))C (w) < C (w). Let w(cid:15) = (cid:15)w 0 + (1 (cid:0) (cid:15))w . For
(cid:15) small enough, we can assume all weights in w that are strictly positive to be also strictly
positive in w(cid:15) . Let us denote by Ip the set of strictly positive weights in w (and w(cid:15) ), by Iz
the set of weights set to zero in w but to a non-zero value in w(cid:15) , and by (cid:14)(cid:15)k the difference
w(cid:15);k (cid:0) wk in the weight of hidden unit hk between w and w(cid:15) . We can assume (cid:14)(cid:15)j < 0 for
j 2 Iz , because instead of setting a small positive weight to hj , one can decrease the weight
of (cid:0)hj by the same amount, which will give either the same cost, or possibly a lower one
when the weight of (cid:0)hj is positive. With o((cid:15)) denoting a quantity such that (cid:15)(cid:0)1 o((cid:15)) ! 0
when (cid:15) ! 0, the difference (cid:1)(cid:15) (w) = C (w(cid:15) ) (cid:0) C (w) can now be written:
(cid:1)(cid:15) (w) = (cid:21) (kw(cid:15) k1 (cid:0) kwk1 ) + Xt
(Q(w(cid:15) (cid:1) h(xt ); yt ) (cid:0) Q(w (cid:1) h(xt ); yt ))
(cid:0)(cid:14)(cid:15)j 1
= (cid:21) 0
(cid:14)(cid:15)i + Xj2Iz
A + Xt Xk
@Xi2Ip
(Q0 (w (cid:1) h(xt ); yt )(cid:14)(cid:15)k hk (xt )) + o((cid:15))
= Xi2Ip  (cid:21)(cid:14)(cid:15)i + Xt
qt (cid:14)(cid:15)ihi (xt )! + Xj2Iz  (cid:0)(cid:21)(cid:14)(cid:15)j + Xt
qt (cid:14)(cid:15)j hj (xt )! + o((cid:15))
(w) + Xj2Iz  (cid:0)(cid:21)(cid:14)(cid:15)j + Xt
qt (cid:14)(cid:15)j hj (xt )! + o((cid:15))
@C
= Xi2Ip
(cid:14)(cid:15)i
@wi
qt (cid:14)(cid:15)j hj (xt )! + o((cid:15))
= 0 + Xj2Iz  (cid:0)(cid:21)(cid:14)(cid:15)j + Xt
since for i 2 Ip , thanks to step (7) of the algorithm, we have @C
(w) = 0. Thus the
@wi
inequality (cid:15)(cid:0)1(cid:1)(cid:15) (w) < 0 rewrites into
qthj (xt )! + (cid:15)(cid:0)1 o((cid:15)) < 0
(cid:15)(cid:0)1 (cid:14)(cid:15)j  (cid:0)(cid:21) + Xt
Xj2Iz
which, when (cid:15) ! 0, yields (note that (cid:15)(cid:0)1 (cid:14)(cid:15)j does not depend on (cid:15) since (cid:14)(cid:15)j is linear in (cid:15)):
qthj (xt )! (cid:20) 0
(cid:15)(cid:0)1 (cid:14)(cid:15)j  (cid:0)(cid:21) + Xt
Xj2Iz
But, hi being the optimal classiﬁer chosen in step (5a) or (5c), all hidden units hj verify
Pt qthj (xt ) (cid:20) Pt qthi (xt ) < (cid:21) and 8j 2 Iz , (cid:15)(cid:0)1 (cid:14)(cid:15)j ((cid:0)(cid:21) + Pt qthj (xt )) > 0 (since
(cid:14)(cid:15)j < 0), contradicting eq. 2.
(Mason et al., 2000) prove a related global convergence result for the AnyBoost algorithm,
a non-parametric Boosting algorithm that is also similar to Gradient Boosting (Friedman,
2001). Again, this requires solving as a sub-problem an exact minimization to ﬁnd a function
hi 2 H that is maximally correlated with the gradient Q0 on the output. We now show a
simple procedure to select a hyperplane with the best weighted classiﬁcation error.
Exact Minimization

(2)

In step (5a) we are required to ﬁnd a linear classiﬁer
that minimizes the weighted sum of
classiﬁcation errors. Unfortunately, this is an NP-hard problem (w.r.t. d, see theorem 4
in (Marcotte and Savard, 1992)). However, an exact solution can be easily found in O(n3 )
computations for d = 2 inputs.
Proposition 4.2. Finding a linear classiﬁer that minimizes the weighted sum of classiﬁcation
error can be achieved in O(n3 ) steps when the input dimension is d = 2.
Proof. We want to maximize Pi ci sign(u (cid:1) xi + b) with respect to u and b, the ci ’s being
in R. Consider u ﬁxed and sort the xi ’s according to their dot product with u and denote r
the function which maps i to r(i) such that xr(i) is in i-th position in the sort. Depending on
the value of b, we will have n + 1 possible sums, respectively (cid:0) Pk
i=1 cr(i) + Pn
i=k+1 cr(i) ,
k = 0; : : : ; n. It is obvious that those sums only depend on the order of the products u (cid:1) x i ,
i = 1; : : : ; n. When u varies smoothly on the unit circle, as the dot product is a continuous
function of its arguments, the changes in the order of the dot products will occur only when
there is a pair (i; j ) such that u (cid:1) xi = u (cid:1) xj . Therefore, there are at most as many order
changes as there are pairs of different points, i.e., n(n (cid:0) 1)=2. In the case of d = 2, we
can enumerate all the different angles for which there is a change, namely a1 ; : : : ; az with
z (cid:20) n(n(cid:0)1)
. We then need to test at least one u = [cos((cid:18)); sin((cid:18))] for each interval a i <
2
(cid:18) < ai+1 , and also one u for (cid:18) < a1 , which makes a total of n(n(cid:0)1)
possibilities.
2

It is possible to generalize this result in higher dimensions, and as shown in (Marcotte and
Savard, 1992), one can achieve O(log(n)nd ) time.
Algorithm 1 Optimal linear classiﬁer search
Maximizing Pn
i=1 ci (cid:14)(sign(w (cid:1) xi ); yi ) in dimension 2
(1) for i = 1; : : : ; n for j = i + 1; : : : ; n
(3)
(cid:2)i;j = (cid:18)(xi ; xj ) + (cid:25)
2 where (cid:18)(xi ; xj ) is the angle between xi and xj
(6) sort the (cid:2)i;j in increasing order
(7) w0 = (1; 0)
(8) for k = 1; : : : ; n(n(cid:0)1)
2
wk = (cos (cid:2)i;j ; sin (cid:2)i;j ), uk = wk+wk(cid:0)1
(9)
2
(10)
sort the xi according to the value of uk (cid:1) xi
compute S (uk ) = Pn
(11)
i=1 ci (cid:14)(uk (cid:1) xi ); yi )
(12) output: argmaxuk S
Approximate Minimization

For data in higher dimensions, the exact minimization scheme to ﬁnd the optimal linear
classiﬁer
is not practical. Therefore it is interesting to consider approximate schemes for
obtaining a linear classiﬁer with weighted costs. Popular schemes for doing so are the linear
SVM (i.e., linear classiﬁer with hinge loss), the logistic regression classiﬁer , and variants of
the Perceptron algorithm. In that case, step (5c) of the algorithm is not an exact minimization,
and one cannot guarantee that the global optimum will be reached. However, it might be
reasonable to believe that ﬁnding a linear classiﬁer by minimizing a weighted hinge loss
should yield solutions close to the exact minimization. Unfortunately, this is not generally
true, as we have found out on a simple toy data set described below. On the other hand,
if in step (7) one performs an optimization not only of the output weights wj (j (cid:20) i) but
also of the corresponding weight vectors vj , then the algorithm ﬁnds a solution close to the
global optimum (we could only verify this on 2-D data sets, where the exact solution can be
computed easily). It means that at the end of each stage, one ﬁrst performs a few training
iterations of the whole NN (for the hidden units j (cid:20) i) with an ordinary gradient descent
mechanism (we used conjugate gradients but stochastic gradient descent would work too),
optimizing the wj ’s and the vj ’s, and then one ﬁx es the vj ’s and obtains the optimal wj ’s for
these vj ’s (using a convex optimization procedure). In our experiments we used a quadratic

Q, for which the optimization of the output weights can be done with a neural network, using
the outputs of the hidden layer as inputs.
Let us consider now a bit more carefully what it means to tune the vj ’s in step (7). Indeed,
changing the weight vector vj of a selected hidden neuron to decrease the cost is equivalent
to a change in the output weights w’s. More precisely, consider the step in which the
value of vj becomes v 0
j . This is equivalent to the following operation on the w’s, when wj
is the corresponding output weight value: the output weight associated with the value v j of
a hidden neuron is set to 0, and the output weight associated with the value v 0
j of a hidden
neuron is set to wj . This corresponds to an exchange between two variables in the convex
program. We are justiﬁed to take any such step as long as it allows us to decrease the cost
C (w). The fact that we are simultaneously making such exchanges on all the hidden units
when we tune the vj ’s allows us to move faster towards the global optimum.
Extension to multiple outputs

The multiple outputs case is more involved than the single-output case because it is not
enough to check the condition Pt ht qt > (cid:21). Consider a new hidden neuron whose output is
hi when the input is xi . Let us also denote (cid:11) = [(cid:11)1 ; : : : ; (cid:11)no ]0 the vector of output weights
between the new hidden neuron and the no output neurons. The gradient with respect to (cid:11)j
is gj = @C
= Pt ht qtj (cid:0) (cid:21)sign((cid:11)j ) with qtj the value of the j-th output neuron with input
@(cid:11)j
xt . This means that if, for a given j , we have j Pt ht qtj j < (cid:21), moving (cid:11)j away from 0 can
only increase the cost. Therefore, the right quantity to consider is (j Pt ht qtj j (cid:0) (cid:21))+ .
We must therefore ﬁnd argmaxv Pj (j Pt ht qtj j (cid:0) (cid:21))2
+ . As before, this sub-problem is not
convex, but it is not as obvious how to approximate it by a convex problem. The stopping
criterion becomes: if there is no j such that j Pt ht qtj j > (cid:21), then all weights must remain
equal to 0 and a global minimum is reached.
Experimental Results
We performed experiments on the 2-D double moon toy dataset (as used in (Delalleau, Ben-
gio and Le Roux, 2005)), to be able to compare with the exact version of the algorithm. In
these experiments, Q(w (cid:1) h(xt ); yt ) = [w (cid:1) h(xt ) (cid:0) yt ]2 . The set-up is the following:
(cid:15) Select a new linear classiﬁer , either (a) the optimal one or (b) an approximate using logistic
regression.
(cid:15) Optimize the output weights using a convex optimizer.
(cid:15) In case (b), tune both input and output weights by conjugate gradient descent on C and
ﬁnally re-optimize the output weights using LASSO regression.
(cid:15) Optionally, remove neurons whose output weight has been set to 0.
Using the approximate algorithm yielded for 100 training examples an average penalized
((cid:21) = 1) squared error of 17.11 (over 10 runs), an average test classiﬁcation error of 3.68%
and an average number of neurons of 5.5 . The exact algorithm yielded a penalized squared
error of 8.09, an average test classiﬁcation error of 5.3%, and required 3 hidden neurons. A
penalty of (cid:21) = 1 was nearly optimal for the exact algorithm whereas a smaller penalty further
improved the test classiﬁcation error of the approximate algorithm. Besides, when running
the approximate algorithm for a long time, it converges to a solution whose quadratic error is
extremely close to the one of the exact algorithm.

5 Conclusion
We have shown that training a NN can be seen as a convex optimization problem, and have
analyzed an algorithm that can exactly or approximately solve this problem. We have shown
that the solution with the hinge loss involved a number of non-zero weights bounded by
the number of examples, and much smaller in practice. We have shown that there exists a
stopping criterion to verify if the global optimum has been reached, but it involves solving a
sub-learning problem involving a linear classiﬁer with weighted errors, which can be com-

putationally hard if the exact solution is sought, but can be easily implemented for toy data
sets (in low dimension), for comparing exact and approximate solutions.

The above experimental results are in agreement with our initial conjecture: when there are
many hidden units we are much less likely to stall in the optimization procedure, because
there are many more ways to descend on the convex cost C (w). They also suggest, based
on experiments in which we can compare with the exact sub-problem minimization, that
applying Algorithm ConvexNN with an approximate minimization for adding each hidden
unit while continuing to tune the previous hidden units tends to lead to fast convergence
to the global minimum. What can get us stuck in a “local minimum” (in the traditional sense,
i.e., of optimizing w’s and v ’s together) is simply the inability to ﬁnd a new hidden unit
weight vector that can improve the total cost (ﬁt and regularization term) even if there
exists one.
Note that as a side-effect of the results presented here, we have a simple way to train neural
networks with hard-threshold hidden units, since increasing Pt Q0 ( ^y(xt ); yt )sign(vixt )
can be either achieved exactly (at great price) or approximately (e.g. by using a cross-entropy
or hinge loss on the corresponding linear classiﬁer).
Acknowledgments

The authors thank the following for support: NSERC, MITACS, and the Canada Research
Chairs. They are also grateful for the feedback and stimulating exchanges with Sam Roweis,
Nathan Srebro, and Aaron Courville.

References

Chv ´atal, V. (1983). Linear Programming. W.H. Freeman.
Delalleau, O., Bengio, Y., and Le Roux, N. (2005). Efﬁcient non-parametric function induction
in semi-supervised learning. In Cowell, R. and Ghahramani, Z., editors, Proceedings of AIS-
TATS’2005, pages 96–103.
Freund, Y. and Schapire, R. E. (1997). A decision theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Science, 55(1):119 –139.
Friedman, J. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statis-
tics, 29:1180.
Hettich, R. and Kortanek, K. (1993). Semi-inﬁnite programming: theory, methods, and applications.
SIAM Review, 35(3):380–429.
Marcotte, P. and Savard, G. (1992). Novel approaches to the discrimination problem. Zeitschrift fr
Operations Research (Theory), 36:517 –545.
Mason, L., Baxter, J., Bartlett, P. L., and Frean, M. (2000). Boosting algorithms as gradient descent.
In Advances in Neural Information Processing Systems 12, pages 512–518.
R ¨atsch, G., Demiriz, A., and Bennett, K. P. (2002). Sparse regression ensembles in inﬁnite and ﬁnite
hypothesis spaces. Machine Learning.
Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning representations by back-propagating
errors. Nature, 323:533–536.


Learning Topology with the Generative Gaussian
Graph and the EM Algorithm

Micha ¤el Aupetit
CEA - DASE
BP 12 - 91680
Bruy (cid:30)eres-le-Ch (cid:136)atel, France
aupetit@dase.bruyeres.cea.fr

Abstract

Given a set of points and a set of prototypes representing them, how to
create a graph of the prototypes whose topology accounts for that of the
points? This problem had not yet been explored in the framework of sta-
tistical learning theory.
In this work, we propose a generative model
based on the Delaunay graph of the prototypes and the Expectation-
Maximization algorithm to learn the parameters. This work is a (cid:2)rst
step towards the construction of a topological model of a set of points
grounded on statistics.

1

Introduction

1.1 Topology what for?

Given a set of points in a high-dimensional euclidean space, we intend to extract the topol-
ogy of the manifolds from which they are drawn. There are several reasons for this among
which:
increasing our knowledge about this set of points by measuring its topological
features (connectedness, intrinsic dimension, Betti numbers (number of voids, holes, tun-
nels. . . )) in the context of exploratory data analysis [1], allowing to compare two sets of
points wrt their topological characteristics or to (cid:2)nd clusters as connected components in
the context of pattern recognition [2], or (cid:2)nding shortest path along manifolds in the context
of robotics [3].
There are two families of approaches which deal with (cid:148)topology(cid:148) : on one hand, the (cid:148)topol-
ogy preserving(cid:148) approaches based on nonlinear projection of the data in lower dimensional
spaces with a constrained topology to allow visualization [4, 5, 6, 7, 8]; on the other hand,
the (cid:148)topology modelling(cid:148) approaches based on the construction of a structure whose topol-
ogy is not constrained a priori, so it is expected to better account for that of the data
[9, 10, 11] at the expense of the visualisability. Much work has been done about the for-
mer problem also called (cid:148)manifold learning(cid:148), from Generative Topographic Mapping [4]
to Multi-Dimensional Scaling and its variants [5, 6], Principal Curves [7] and so on. In
all these approaches, the intrinsic dimension of the model is (cid:2)xed a priori which eases the
visualization but arbitrarily forces the topology of the model. And when the dimension
is not (cid:2)xed as in the mixture of Principal Component Analyzers [8], the connectedness is
lost. The latter problem we deal with had never been explored in the statistical learning

perspective. Its aim is not to project and visualize a high-dimensional set of points, but to
extract the topological information from it directly in the high-dimensional space, so that
the model must be freed as much as possible from any a priori topological constraint.

1.2 Learning topology: a state of the art

As we may learn a complicated function combining simple basis functions, we shall learn
a complicated manifold1 combining simple basis manifolds. A simplicial complex2 is such
a model based on the combination of simplices, each with its own dimension (a 1-simplex
is a line segment, a 2-simplex is a triangle. . . a k-simplex is the convex hull of a set of k + 1
points). In a simplicial complex, the simplices are exclusively connected by their vertices or
their faces. Such a structure is appealing because it is possible to extract from it topological
information like Betti numbers, connectedness and intrinsic dimension [10]. A particular
simplicial complex is the Delaunay complex de(cid:2)ned as the set of simplices whose Vorono¤(cid:17)
cells3 of the vertices are adjacent assuming general position for the vertices. The Delaunay
graph is made of vertices and edges of the Delaunay complex [12].
All the previous work about topology modelling is grounded on the result of Edelsbrunner
and Shah [13] which prove that given a manifold M (cid:26) RD and a set of N0 vector proto-
types w 2 (RD )N0 nearby M, it exists a simplicial subcomplex of the Delaunay complex
of w which has the same topology as M under what we call the (cid:148)ES-conditions(cid:148).
In the present work, the manifold M is not known but through a (cid:2)nite set of M data points
v 2 MM . Martinetz and Schulten proposed to build a graph of the prototypes with an
algorithm called (cid:148)Competitive Hebbian Learning(cid:148) (CHL)[11] to tackle this problem. Their
approach has been extended to simplicial complexes by De Silva and Carlsson with the
de(cid:2)nition of (cid:148)weak witnesses(cid:148) [10]. In both cases, the ES-conditions about M are weak-
ened so they can be veri(cid:2)ed by a (cid:2)nite sample v of M, so that the graph or the simplicial
complex built over w is proved to have the same topology as M if v is a suf(cid:2)ciently dense
sampling of M.
The CHL consists in connecting two prototypes in w if they are the (cid:2)rst and the second
closest neighbors to a point of v (closeness wrt the Euclidean norm). Each point of v leads
to an edge, and is called a (cid:148)weak witness(cid:148) of the connected prototypes [10]. The topology
representing graph obtained is a subgraph of the Delaunay graph. The region of RD in
which any data point would connect the same prototypes, is the (cid:148)region of in(cid:3)uence(cid:148) (ROI)
of this edge (see Figure 2 d-f). This principle is extended to create k-simplices connecting
k + 1 prototypes, which are part of the Delaunay simplicial-complex of w [10].
Therefore, the model obtained is based on regions of in(cid:3)uence: a simplex exists in the
model if there is at least one datum in its ROI. Hence, the capacity of this model to correctly
represent the topology of a set of points, strongly depends on the shape and location of the
ROI wrt the points, and on the presence of noise in the data. Moreover, as far as N0 > 2,
it cannot exist an isolated prototype allowing to represent an isolated bump in the data
distribution, because any datum of this bump will have two closest prototypes to connect to
each other. An aging process has been proposed by Martinetz and Schulten to (cid:2)lter out the
noise, which works roughly such that edges with fewer data than a threshold in there ROI
are pruned from the graph. This looks like a (cid:2)lter based on the probability density of the
data distribution, but no statistical criterion is proposed to tune the parameters. Moreover
the area of the ROI may be intractable in high dimension and is not trivially related to the

1 For simplicity, we call (cid:148)manifold(cid:148) what can be actually a set of manifolds connected or not to
each other with possibly various intrinsic dimensions.
2The terms (cid:148)simplex(cid:148) or (cid:148)graph(cid:148) denote both the abstract object and its geometrical realization.
3Given a set of points w in RD , Vi = fv 2 RD j(v (cid:0) wi )2 (cid:20) (v (cid:0) wj )2 ; 8j g de(cid:2)nes the Vorono¤(cid:17)
cell associated to wi 2 w .

corresponding line segment, so measuring the frequency over such a region is not relevant
to de(cid:2)ne a useful probability density. At last, the line segment associated to an edge of
the graph is not part of the model: data are not projected on it, data drawn from such a
line segment may not give rise to the corresponding edge, and the line segment may not
intersect at all its associated ROI. In other words, the model is not self-consistent, that is
the geometrical realization of the graph is not always a good model of its own topology
whatever the density of the sampling.
We proposed to de(cid:2)ne Vorono¤(cid:17) cells of line segments as ROI for the edges and de(cid:2)ned a
criterion to cut edges with a lower density of data projecting on their middle than on their
borders [9]. This solves some of the CHL limits but it still remains one important problem
common to both approaches: they rely on the visual control of their quality, i.e. no criterion
allows to assess the quality of the model especially in dimension greater than 3.

1.3 Emerging topology from a statistical generative model

For all the above reasons, we propose another way for modelling topology. The idea is to
construct a (cid:148)good(cid:148) statistical generative model of the data taking the noise into account,
and to assume that its topology is therefore a (cid:148)good(cid:148) model of the topology of the manifold
which generated the data. The only constraint we impose on this generative model is that
its topology must be as (cid:148)(cid:3)exible(cid:148) as possible and must be (cid:148)extractible(cid:148). (cid:148)Flexible(cid:148) to
avoid at best any a priori constraint on the topology so as to allow the modelling of any
one. (cid:148)Extractible(cid:148) to get a (cid:148)white box(cid:148) model from which the topological characteristics
are tractable in terms of computation. So we propose to de(cid:2)ne a (cid:148)generative simplicial
complex(cid:148). However, this work being preliminary, we expose here the simpler case of
de(cid:2)ning a (cid:148)generative graph(cid:148) (a simplicial complex made only of vertices and edges) and
tuning its parameters. This allows to demonstrate the feasibility of this approach and to
foresee future dif(cid:2)culties when it is extended to simplicial complexes.
It works as follows. Given a set of prototypes located over the data distribution using
e.g. Vector Quantization [14], the Delaunay graph (DG) of the prototypes is constructed
[15]. Then, each edge and each vertex of the graph is the basis of a generative model so
that the graph generates a mixture of gaussian density functions. The maximization of the
likelihood of the data wrt the model, using Expectation-Maximization, allows to tune the
weights of this mixture and leads to the emergence of the expected topology representing
graph through the edges with non-negligible weights that remain after the optimization
process.
We (cid:2)rst present the framework and the algorithm we use in section 2. Then we test it on
arti(cid:2)cial data in section 3 before the discussion and conclusion in section 4.

2 A Generative Gaussian Graph to learn topology

2.1 The Generative Gaussian Graph

In this work, M is the support of the probability density function (pdf) p from which are
drawn the data v . In fact, this is not the topology of M which is of interest, but the topology
of manifolds Mprin called (cid:148)principal manifolds(cid:148) of the distribution p (in reference to the
de(cid:2)nition of Tibshirani [7]) which can be viewed as the manifold M without the noise. We
assume the data have been generated by some set of points and segments constituting the
set of manifolds Mprin which have been corrupted with additive spherical gaussian noise
with mean 0 and unknown variance (cid:27) 2
noise . Then, we de(cid:2)ne a gaussian mixture model
to account for the observed data, which is based on both gaussian kernels that we call
(cid:148)gaussian-points(cid:148), and what we call (cid:148)gaussian-segments(cid:148), forming a (cid:148)Generative Gaussian
Graph(cid:148) (GGG).

The value at point vj 2 v of a normalized gaussian-point centered on a prototype wi 2 w
with variance (cid:27) 2 is de(cid:2)ned as: g 0 (vj ; wi ; (cid:27)) = (2(cid:25)(cid:27) 2 )(cid:0)D=2 exp((cid:0) (vj (cid:0)wi )2
)
2(cid:27)2
A normalized gaussian-segment is de(cid:2)ned as the sum of an in(cid:2)nite number of gaussian-
points evenly spread on a line segment. Thus, this is the integral of a gaussian-point along
a line segment. The value at point vj of the gaussian-segment [wai wbi ] associated to the
ith edge fai ; bi g in DG with variance (cid:27) 2 is:
g1 (vj ; fwai ; wbi g; (cid:27)) = Z wbi
wai

exp (cid:18)(cid:0)
(2(cid:25)(cid:27)2 )

(vj (cid:0)w)2
2(cid:27)2 (cid:19) dw
D
2 Lai bi
j
erf  Q
(cid:27)p2 !(cid:0)erf  Q
ai bi
2Lai bi

(cid:1)

(1)

j
ai bi (cid:0)Lai bi
(cid:27)p2

!

exp(cid:18)(cid:0)
(2(cid:25)(cid:27)2 )

j
)2
2(cid:27)2 (cid:19)
(vj (cid:0)q
i
D(cid:0)1
2

=

Qj
= hvj (cid:0)wai jwbi (cid:0)wai i
where Lai bi = kwbi (cid:0) wai k, Qj
and q j
ai bi
i = wai + (wbi (cid:0) wai )
ai bi
Lai bi
Lai bi
is the orthogonal projection of vj on the straight line passing through wai and wbi . In the
case where wai = wbi , we set g 1 (vj ; fwai ; wbi g; (cid:27)) = g 0 (vj ; wai ; (cid:27)).
The left part of the dot product accounts for the gaussian noise orthogonal to the line seg-
ment, and the right part for the gaussian noise integrated along the line segment. The
functions g 0 and g1 are positive and we can prove that: RRD g0 (v ; wi ; (cid:27))dv = 1 and
RRD g1 (v ; fwa ; wb g; (cid:27))dv = 1, so they are both probability density functions. A gaussian-
point is associated to each prototype in w and a gaussian-segment to each edge in DG.
The gaussian mixture is obtained by a weighting sum of the N0 gaussian-points and N1
gaussian-segments, such that the weights (cid:25) sum to 1 and are non-negative:

p(vj j(cid:25) ; w ; (cid:27); DG) =

Nk
1
Xi=1
Xk=0
with P1
k=0 PNk
i = 1 and 8i; k ; (cid:25)k
i (cid:21) 0, where s0
i = wi and s1
i = fwai ; wbi g such
i=1 (cid:25)k
that fai ; bi g is the ith edge in DG. The weight (cid:25) 0
i (resp. (cid:25)1
i ) is the probability that a datum v
was drawn from the gaussian-point associated to wi (resp. the gaussian-segment associated
to the ith edge of DG).

(cid:25)k
i gk (vj ; sk
i ; (cid:27))

(2)

2.2 Measure of quality

The function p(vj j(cid:25) ; w ; (cid:27); DG) is the probability density at vj given the parameters of the
model. We measure the likelihood P of the data v wrt the parameters of the GGG model:

M
Yj=1
2.3 The Expectation-Maximization algorithm

P = P ((cid:25) ; w ; (cid:27); DG) =

p(vj j(cid:25) ; w ; (cid:27); DG)

(3)

In order to maximize the likelihood P or equivalently to minimize the negative log-
likelihood L = (cid:0)log(P ) wrt (cid:25) and (cid:27) , we use the Expectation-Maximization algorithm.

We refer to [2] (pages 59 (cid:0) 73) and [16] for further details. The minimization of the neg-
ative log-likelihood consists in tmax iterative steps updating (cid:25) and (cid:27) which ensure the
decrease of L. The updating rules take into account the constraints about positivity or sum
to unity of the parameters:

(cid:25)k[new]
i

M PM
= 1
j=1 P (k ; ijvj )
DM PM
j=1 [PN0
(cid:27)2[new] = 1
i=1 P (0; ijvj )(vj (cid:0) wi )2
j
)2
(vj (cid:0)q
)(I1 [(vj (cid:0)qj
i )2+(cid:27)2 ]+I2 )
(2(cid:25)(cid:27)2 )(cid:0)D=2 exp((cid:0)
i
+ PN1
2(cid:27)2
i=1 P (1; ijvj )
Lai bi (cid:1)g1 (vj ;fwai ;wbi g;(cid:27))

]

(4)

where

) (cid:0) erf (

Qj
ai bi
(cid:27)p2
(cid:0)Lai bi ) exp((cid:0)

I1 = (cid:27)p (cid:25)
2 (erf (
I2 = (cid:27)2 (cid:18)(Qj
)(cid:19)
ai bi
and P (k ; ijvj ) = (cid:25)k
i gk (vj ;sk
i ;(cid:27))
p(vj j(cid:25) ;w;(cid:27);DG) is the posterior probability that the datum vj was gener-
ated by the component associated to (k ; i).

Qj
ai bi (cid:0)Lai bi
))
(cid:27)p2
(Qj
ai bi(cid:0)Lai bi )2
2(cid:27)2

)(cid:0)Qj
ai bi

(5)

exp((cid:0)

)2

(Qj
ai bi
2(cid:27)2

2.4 Emerging topology by maximizing the likelihood

Finally, to get the topology representing graph from the generative model, the core idea is
to prune from the initial DG the edges for which there is probability (cid:15) they generated the
data. The complete algorithm is the following:

1. Initialize the location of the prototypes w using vector quantization [14].
2. Construct the Delaunay graph DG of the prototypes.
3. Initialize the weights (cid:25) to 1=(N0 + N1 ) to give equiprobability to each
vertices and edges.
4. Given w and DG, use updating rules (4) to (cid:2)nd (cid:27) 2(cid:3) and (cid:25)(cid:3) maximizing
the likelihood P .
5. Prune the edges fai bi g of DG associated to the gaussian segments with
i 2 (cid:25)(cid:3) .
probability (cid:25) 1
i (cid:20) (cid:15) where (cid:25) 1

The topology representing graph emerges from the edges with probabilities (cid:25) (cid:3) > (cid:15). It is the
graph which best models the topology of the data in the sense of the maximum likelihood
wrt (cid:25) , (cid:27) , (cid:15) and the set of prototypes w and their Delaunay graph.

3 Experiments

In these experiments, given a set of points and a set of prototypes located thanks to vector
quantization [14], we want to verify the relevance of the GGG to learn the topology in
various noise conditions. The principle of the GGG is shown in the Figure 1. In the Figure
2, we show the comparison of the GGG to a CHL for which we (cid:2)lter out edges which have
a number of hits lower than a threshold T . The data and prototypes are the same for both
algorithms. We set T (cid:3) such that the graph obtained matches visually as close as possible
the expected solution. We optimize (cid:27) and (cid:25) using (4) for tmax = 100 steps and (cid:15) = 0:001.
Conditions and conclusions of the experiments are given in the captions.

1.2

1

0.8

0.6

0.4

0.2

0

1.2

1

0.8

0.6

0.4

0.2

0

−0.2
−0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

−0.2
−0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

(a)

(b)

(c)

(d)

Figure 1: Principle of the Generative Gaussian Graph: (a) Data drawn from an oblique segment,
an horizontal one and an isolated point with respective density f0:25; 0:5; 0:25g. The prototypes are
located at the extreme points of the segments, and at the isolated point. They are connected with
edges from the Delaunay graph. (b) The corresponding initial Generative Gaussian Graph. (c) The
optimal GGG obtained after optimization of the likelihood according to (cid:27) and (cid:25) . (d) The edges of
the optimal GGG associated to non-negligible probabilities model the topology of the data.

4 Discussion

We propose that the problem of learning the topology of a set of points can be posed
as a statistical learning problem: we assume that the topology of a statistical generative
model of a set of points is an estimator of the topology of the principal manifold of this
set. From this assumption, we de(cid:2)ne a topologically (cid:3)exible statistical generative mixture
model that we call Generative Gaussian Graph from which we can extract the topology. The
(cid:2)nal topology representing graph emerges from the edges with non-negligible probability.
We propose to use the Delaunay graph as an initial graph assuming it is rich enough to
contain as a subgraph a good topological model of the data. The use of the likelihood
criterion makes possible cross-validation to select the best generative model hence the best
topological model in terms of generalization capacities.
The GGG allows to avoid the limits of the CHL for modelling topology. In particular, it
allows to take into account the noise and to model isolated bumps. Moreover, the likelihood
of the data wrt the GGG is maximized during the learning, allowing to measure the quality
of the model even when no visualization is possible. For some particular data distribu-
tions where all the data lie on the Delaunay line segments, no maximum of the likelihood
exists. This case is not a problem because (cid:27) = 0 effectively de(cid:2)nes a good solution (no
noise in a data set drawn from a graph). If only some of the data lie exactly on the line
segments, a maximum of the likelihood still exists because (cid:27) 2 de(cid:2)nes the variance for all
the generative gaussian points and segments at the same time so it cannot vanish to 0. The
computing time complexity of the GGG is o(D(N0 + N1 )M tmax ) plus the time O(DN 3
0 )
[15] needed to build the Delaunay graph which dominates the overall worst time complex-
ity. The Competitive Hebbian Learning is in time o(DN0M ). As in general, the CHL
builds too much edges than needed to model the topology, it would be interesting to use the
Delaunay subgraph obtained with the CHL as a starting point for the GGG model.
The Generative Gaussian Graph can be viewed as a generalization of gaussian mixtures to
points and segments: a gaussian mixture is a GGG with no edge. GGG provides at the
same time an estimation of the data distribution density more accurate than the gaussian
mixture based on the same set of prototypes and the same noise isovariance hypothesis (be-
cause it adds gaussian-segments to the pool of gaussian-points), and intrinsically an explicit
model of the topology of the data set which provides most of the topological information
at once. In contrast, other generative models do not provide any insight about the topol-
ogy of the data, except the Generative Topographic Map (GTM) [4], the revisited Principal
Manifolds [7] or the mixture of Probabilistics Principal Component Analysers (PPCA) [8].
However, in the two former cases, the intrinsic dimension of the model is (cid:2)xed a priori and

(cid:27)noise = 0:05

(cid:27)noise = 0:15

(cid:27)noise = 0:2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8
−1

1

0.5

0

−0.5

−1

1

0.5

0

−0.5

−1

−0.5

0

0.5

1

−1

−0.5

0

0.5

1

1.5

−1

−0.5

0

0.5

1

1.5

(a) GGG: (cid:27)(cid:3) = 0:06

(b) GGG: (cid:27) (cid:3) = 0:17

(c) GGG: (cid:27) (cid:3) = 0:21

−0.5

0

0.5

1

(d) CHL: T = 0

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

1

0.5

0

−0.5

−1

1

0.5

0

−0.5

−1

1

0.5

0

−0.5

−1

−0.5

0

0.5

1

−1

−0.5

0

0.5

1

(e) CHL: T = 0

(f) CHL: T = 0

1

0.5

0

−0.5

−1

−0.5

0

0.5

1

−0.5

0

0.5

1

−1

−0.5

0

0.5

1

(g) CHL: T (cid:3) = 60

(h) CHL: T (cid:3) = 65

(i) CHL: T (cid:3) = 58

Figure 2: Learning the topology of a data set: 600 data drawn from a spirale and an isolated point
noise . Prototypes are located by
corrupted with additive gaussian noise with mean 0 and variance (cid:27) 2
vector quantization [14]. (a-c) The edges of the GGG with weights greater than (cid:15) allow to recover the
topology of the principal manifolds except for large noise variance (c) where a triangle was created
at the center of the spirale. (cid:27) (cid:3) over-estimates (cid:27)noise because the model is piecewise linear while
the true manifolds are non-linear. (d-f) The CHL without threshold (T=0) is not able to recover the
true topology of the data for even small (cid:27)noise . In particular, the isolated bump cannot be recovered.
The grey cells correspond to ROI of the edges (darker cells contain more data). It shows these cells
are not intuitively related to the edges they are associated to (e.g.
they may have very tiny areas
(e), and may partly (d) or never (f) contain the corresponding line segment). (g-h) The CHL with
a threshold T allows to recover the topology of the data only for small noise variance (g) (Notice
T1 < T2 ) DGCHL (T2 ) (cid:18) DGCHL (T1 )). Moreover, setting T requires visual control and is not
associated to the optimum of any energy function which prevents its use in higher dimensional space.

not learned from the data, while in the latter the local intrinsic dimension is learned but the
connectedness between the local models is not.
One obvious way to follow to extend this work is considering a simplicial complex in place
of the graph to get the full topological information extractible. Some other interesting
questions arise about the curse of the dimension, the selection of the number of prototypes
and the threshold (cid:15), the theoretical grounding of the connection between the likelihood and
some topological measure of accuracy, the possibility to devise a (cid:148)universal topology esti-
mator(cid:148), the way to deal with data sets with multi-scale structures or background noise. . .
This preliminary work is an attempt to bridge the gap between Statistical Learning The-
ory [17] and Computational Topology [18][19]. We wish it to cross-fertilize and to open
new perspectives in both (cid:2)elds.

References
[1] M. Aupetit and T. Catz. High-dimensional labeled data analysis with topology representing
graphs. Neurocomputing, Elsevier, 63:139(cid:150)169, 2005.
[2] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford Univ. Press, New York, 1995.
[3] M. Zeller, R. Sharma, and K. Schulten. Topology representing network for sensor-based robot
motion planning. World Congress on Neural Networks, INNS Press, pages 100(cid:150)103, 1996.
[4] C. M. Bishop, M. Svens ·en, and C. K. I. Williams. Gtm: the generative topographic mapping.
Neural Computation, MIT Press, 10(1):215(cid:150)234, 1998.
[5] V. de Silva and J. B. Tenenbaum. Global versus local methods for nonlinear dimensionality
reduction. In S. Becker, S. Thrun, K. Obermayer (Eds) Advances in Neural Information Pro-
cessing Systems, MIT Press,Cambridge, MA, 15:705(cid:150)712, 2003.
[6] J. A. Lee, A. Lendasse, and M. Verleysen. Curvilinear distance analysis versus isomap. Europ.
Symp. on Art. Neural Networks, Bruges (Belgium), d-side eds., pages 185(cid:150)192, 2002.
[7] R. Tibshirani. Principal curves revisited. Statistics and Computing, (2):183(cid:150)190, 1992.
[8] M. E. Tipping and C. M. Bishop. Mixtures of probabilistic principal component analysers.
Neural Computation, 11(2):443(cid:150)482, 1999.
[9] M. Aupetit. Robust topology representing networks. European Symp. on Arti(cid:2)cial Neural
Networks, Bruges (Belgium), d-side eds., pages 45(cid:150)50, 2003.
[10] V. de Silva and G. Carlsson. Topological estimation using witness complexes.
In M.
Alexa and S. Rusinkiewicz (Eds) Eurographics Symposium on Point-Based Graphics, ETH,
Z ¤urich,Switzerland, June 2-4, 2004.
[11] T. M. Martinetz and K. J. Schulten. Topology representing networks. Neural Networks, Elsevier
London, 7:507(cid:150)522, 1994.
[12] A. Okabe, B. Boots, and K. Sugihara. Spatial tessellations: concepts and applications of
Vorono¤(cid:17) diagrams. John Wiley, Chichester, 1992.
[13] H. Edelsbrunner and N. R. Shah. Triangulating topological spaces. International Journal on
Computational Geometry and Applications, 7:365(cid:150)378, 1997.
[14] T. M. Martinetz, S. G. Berkovitch, and K. J. Schulten. (cid:147)neural-gas(cid:148) network for vector quanti-
zation and its application to time-series prediction. IEEE Trans. on NN, 4(4):558(cid:150)569, 1993.
[15] E. Agrell. A method for examining vector quantizer structures. Proceedings of IEEE Interna-
tional Symposium on Information Theory, San Antonio, TX, page 394, 1993.
[16] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1(cid:150)38, 1977.
[17] V.N. Vapnik. Statistical Learning Theory. John Wiley, 1998.
[18] T. Dey, H. Edelsbrunner, and S. Guha. Computational topology. In B. Chazelle, J. Goodman
and R. Pollack, editors, Advances in Discrete and Computational Geometry. American Math.
Society, Princeton, NJ, 1999.
[19] V. Robins, J. Abernethy, N. Rooney, and E. Bradley. Topology and intelligent data analysis.
IDA-03 (International Symposium on Intelligent Data Analysis), Berlin, 2003.


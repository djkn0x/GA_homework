Cue Integration for Figure/Ground Labeling

Xiaofeng Ren, Charless C. Fowlkes and Jitendra Malik
Computer Science Division, University of California, Berkeley, CA 94720
{xren,fowlkes,malik}@cs.berkeley.edu

Abstract

We present a model of edge and region grouping using a conditional
random ﬁeld built over a scale-invariant representation of images to inte-
grate multiple cues. Our model includes potentials that capture low-level
similarity, mid-level curvilinear continuity and high-level object shape.
Maximum likelihood parameters for the model are learned from human
labeled groundtruth on a large collection of horse images using belief
propagation. Using held out test data, we quantify the information gained
by incorporating generic mid-level cues and high-level shape.

1 Introduction

Figure/ground organization, the binding of contours to surfaces, is a classical problem in
vision. In the 1920s, Edgar Rubin pointed to several generic properties, such as closure,
which governed the perception of ﬁgure/ground. However, it is clear that in the context of
natural scenes, such processing must be closely intertwined with many low- and mid-level
grouping cues as well as a priori object knowledge [10].

In this paper, we study a simpliﬁed task of ﬁgure/ground labeling in which the goal is
to label every pixel as belonging to either a ﬁgural object or background. Our goal is to
understand the role of different cues in this process, including low-level cues, such as edge
contrast and texture similarity; mid-level cues, such as curvilinear continuity; and high-
level cues, such as characteristic shape or texture of the object. We develop a conditional
random ﬁeld model [7] over edges, regions and objects to integrate these cues. We train
the model from human-marked groundtruth labels and quantify the relative contributions
of each cue on a large collection of horse images[2].

In computer vision, the work of Geman and Geman [3] inspired a whole subﬁeld of work
on Markov Random Fields in relation to segmentation and denoising. More recently, Con-
ditional Random Fields (CRF) have been applied to low-level segmentation [6, 12, 4] and
have shown performance superior to traditional MRFs. However, most of the existing
MRF/CRF models focus on pixel-level labeling, requiring inferences over millions of pix-
els. Being tied to the pixel resolution, they are also unable to deal with scale change or
explicitly capture mid-level cues such as junctions. Our approach overcomes these difﬁ-
culties by utilizing a scale-invariant representation of image contours and regions where
each variable in our model can correspond to hundreds of pixels. It is also quite straightfor-
ward to design potentials which capture complicated relationships between these mid-level
tokens in a transparent way.

Interest in combining object knowledge with segmentation has grown quickly over the

(1)

(2)

(3)

(4)

Figure 1: A scale-invariant representation of images: Given the input (1), we estimate the
local probability of boundary P b based on gradients (2). We then build a piecewise linear
approximation of the edge map and complete it with Constrained Delaunay Triangulation
(CDT). The black edges in (3) are gradient edges detected in (2); the green edges are
potential completions generated by CDT. (4) We perform inference in a probabilistic model
built on top of this representation and extract marginal distributions on edges X , triangular
regions Y and object pose Z .

last few years [2, 16, 14]. Our probabilistic approach is similar in spirit to [14] however
we focus on learning parameters of a discriminative model and quantify our performance
on test data. Compared to previous techniques which rely heavily on top-down template
matching [2, 5], our approach has three major advantages: (1) We are able to use mid-
level grouping cues including junctions and continuity. Our results show these cues make
quantitatively signiﬁcant contributions. (2) We combine cues in a probabilistic framework
where the relative weighting of cues is learned from training data resulting in weights that
are easy to interpret. (3) The role of different cues can be easily studied by ”surgically
removing” them reﬁtting the remaining parameters.

2 A conditional random ﬁeld for ﬁgure/ground labeling

Figure 1 provides an overview of our technique for building a discrete, scale-independent
representation of image boundaries from a low-level detector. First we compute an edge
map using the boundary detector of [9] which utilizes both brightness and texture contrast
to estimate the probability of boundary, P b at each pixel. Next we use Canny’s hystere-
sis thresholding to trace the P b boundaries and then recursively split the boundaries using
angles, a scale-invariant measure, until each segment is approximately linear. Finally we
utilize the Constrained Delaunay Triangulation [13] to complete the piecewise linear ap-
proximations. CDT often completes gaps in object boundaries where local gradient infor-
mation is absent. More details about this construction can be found in [11].

Let G be the resulting CDT graph. The edges and triangles in G are natural entities for
ﬁgure/ground labeling. We introduce the following random variables:
• Edges: Xe is 1 if edge e in the CDT is a true boundary and 0 otherwise.
• Regions: Yt is 1 if triangle t corresponds to ﬁgure and 0 otherwise.
• Pose: Z encodes the ﬁgural object’s pose in the scene. We use a very simple
Z which considers a discrete conﬁguration space given by a grid of 25 possible
image locations. Z is easily augmented to include an indicator of object category
or aspect as well as location.
We now describe a conditional random ﬁeld model on {X, Y , Z } used to integrate multiple
grouping cues. The model takes the form of a log-linear combination of features which are
functions of variables and image measurements. We consider Z a latent variable which is

YtXeZYs~M1 (XV |I )

L1 (Xe |I ) − ~β ·

marginalized out by assuming a uniform distribution over aspects and locations.
1
P (X, Y |Z, I , Θ) =
Z (I , Θ) e−E (X,Y |Z,I ,Θ)
(cid:88)
(cid:88)
(cid:88)
where the energy E of a conﬁguration is linear in the parameters Θ = {α, ~β , ~δ , γ , ~η , κ, ~ν }
and given by
~L2 (Ys , Yt |I ) − ~δ ·
E = − α
(cid:88)
(cid:88)
(cid:88)
(cid:88)
hs,ti
e
V
M2 (Ys , Yt , Xe ) − ~η ·
H2 (Yt |Z, I ) − ~ν ·
hs,ti
e
t
t
The table below gives a summary of each potential. The next section ﬁlls in details.
L1 (Xe |I )
Edge energy along e
L2 (Ys , Yt |I )
Brightness/Texture similarity between s and t
Collinearity and junction frequency at vertex V M1 (XV |I )
M2 (Ys , Yt , Xe )
Consistency of edge and adjoining regions
H1 (Yt |I )
Similarity of region t to exemplar texture
H2 (Yt |Z, I )
Compatibility of region shape with pose
H3 (Xe |Z, I )
Compatibility of local edge shape with pose

~H1 (Yt |I ) − κ

~H3 (Xe |Z, I )

−γ

Similarity

Continuity
Closure

Familiarity

3 Cues for ﬁgure/ground labeling

3.1 Low-level Cues: Similarity of Brightness and Texture

To capture the locally measured edge contrast, we assign a sin-
gleton edge potential whose energy is
L1 (Xe |I ) = log(P be )Xe
where P be is the average P b recorded over the pixels corre-
sponding to edge e.

Since the triangular regions have larger support than the local
edge detector, we also include a pairwise, region-based similar-
ity cue, computed as
~β · ~L2 (Ys , Yt |I ) = (βB log(f (|Is − It |)) + βT log(g(χ2 (hs , ht ))))1{Ys=Yt }
where f predicts the likelihood of s and t belonging to the same group given the difference
of average image brightness and g makes a similar prediction based on the χ2 difference
between histograms of vector quantized ﬁlter responses (referred to as textons [8]) which
describe the texture in the two regions.

3.2 Mid-level Cues: Curvilinear Continuity and Closure

There are two types of edges in the CDT graph, gradient-edges
(detected by P b) and completed-edges (ﬁlled in by the trian-
gulation). Since true boundaries are more commonly marked
by a gradient, we keep track of these two types of edges sepa-
(cid:88)
rately when modeling junctions. To capture continuity and the
frequency of different junction types, we assign energy:
~δ · ~M1 (XV |I ) =
δi,j 1{degg (V )=i,degc (V )=j}
i,j
+ δC 1{degg (V )+degc (V )=2} log(h(θ))

YtXeL1L2YsYtXeM2YsM1where XV = {Xe1 , Xe2 , . . .} is the set of edge variables incident on V , degg (V ) is the
number of gradient-edges at vertex V for which Xe = 1. Similarly degc (V ) is the number
2, δC weights
of completed-edges that are “turned on”. When the total degree of a vertex is
the continuity of the two edges. h is the output of a logistic function ﬁt to |θ | and the
It is smooth and symmetric around θ = 0 and falls of as
probability of continuation.
θ → π . If the angle between the two edges is close to 0, they form a good continuation,
f (θ) is large, and they are more likely to both be turned on.
In order to assert the duality between segments and boundaries, we use a compatibility term
M2 (Ys , Yt , Xe ) = 1{Ys=Yt ,Xe=0} + 1{Ys 6=Yt ,Xe=1}
which simply counts when the label of s and t is consistent with that of e.

3.3 High-level Cues: Familiarity of Shape and Texture

We are interested in encoding high-level knowledge about object categories. In this paper
we experiment with a single object category, horses, but we believe our high-level cues will
scale to multiple objects in a natural way.
We compute texton histograms ht for each triangular region (as
in L1 ). From the set of training images, we use k-medoids
{hF
10 } for the
1 , . . . , hF
to ﬁnd 10 representative histograms
collection of segments labeled as ﬁgure and 10 histograms
{hG
l0 } for the set of background segments. Each seg-
l , . . . , hG
(cid:181)
(cid:182)
ment in a test image is compared to the set of exemplar his-
tograms using the χ2 histogram difference. We use the energy
term

H1 (Yt |I ) = log

mini χ2 (ht , hF
i )
mini χ2 (ht , hG
i )

Yt

to capture the cue of texture familiarity.
We describe the global shape of the object using a template T (x, y) generated by averag-
ing the groundtruth object segmentation masks. This yields a silhouette with quite fuzzy
boundaries due to articulations and scale variation. Figure 3.3(a) shows the template ex-
tracted from our training data. Let O(Z, t) be the normalized overlap between template
centered at Z = (x0 , y0 ) with the triangular region corresponding to Yt . This is computed
as the integral of T (x, y) over the triangle t divided by the area of t. We then use energy
~η · ~H2 (Yt |Z ) = ηF log(O(Z, t))Yt + ηG log(1 − O(Z, t))(1 − Yt )
In the case of multiple objects or aspects of a single object, we use multiple templates and
augment Z with an indicator of the aspect Z = (x, y , a). In our experiments on the dataset
considered here, we found that the variability is too small (all horses facing left) to see a
signiﬁcant impact on performance from adding multiple aspects.

Lastly, we would like to capture the spatial layout of articulated structures such as the horses
legs and head. To describe characteristic conﬁguration of edges, we utilize the geometric
blur[1] descriptor applied to the output of the P b boundary detector. The geometric blur
centered at location x, GBx (y), is a linear operator applied to P b(x, y) whose value is
another image given by the “convolution” of P b(x, y) with a spatially varying Gaussian.
Geometric blur is motivated by the search for a linear operator which will respond strongly
to a particular object feature and is invariant to some set of transformations of the image.
We use the geometric blur computed at the set of image edges (P b > 0.05) to build a library
of 64 prototypical ”shapemes ” from the training data by vector quantization. For each edge
Xe which expresses a particular shapeme we would like to know whether Xe should be

YtXeYsH2H3ZH1(a)

(b)

(c)

(d)

(e)

Figure 2: Using a priori shape knowledge: (a) average horse template. (b) one shapeme,
capturing long horizontal curves. Shown here is the average shape in this shapeme cluster.
(c) on a horse, this shapeme occurs at horse back and stomach. Shown here is the density
of the shapeme M ON overlayed with a contour plot of the average mask.
(d) another
shapeme, capturing parallel vertical lines. (e) on a horse, this shapeme occurs at legs.

(x, y) and
“turned on”. This is estimated from training data by building spatial maps M ON
i
(x, y) for each shapeme relative to the object center which record the frequency of
M OF F
i
a true/false boundary expressing shapeme i. Figure 3.3(b-e) shows two example shapemes
and their corresponding M ON map. Let Se,i (x, y) be the indicator of the set of pixels on
(cid:88)
(cid:88)
(cid:88)
edge e which express shapeme i. For an object in pose Z = (x0 , y0 ) we use the energy
1
(cid:88)
~ν ·
~H3 (Xe |Z, I ) =
(x − x0 , y − y0 ))Se,i (x, y)Xe+
|e| (νON
e
e
i,x,y
log(M OF F
i
i,x,y

(x − x0 , y − y0 ))Se,i (x, y)(1 − Xe ))

log(M ON
i

νOF F

4 Learning cue integration

We carry out approximate inference using loopy belief propagation [15] which appears to
converge quickly to a reasonable solution for the graphs and potentials in question.

∂
∂ δ0

=

=

To ﬁt parameters of the model, we maximize the joint likelihood over X, Y , Z taking each
image as an iid sample. Since our model is log-linear in the parameters Θ, partial deriva-
tives always yield the difference between the empirical expectation of a feature given by
the training data and the expected value given the model parameters. For example, the
derivative with respect to the continuation parameter δ0 for a single training image/ground
truth labeling, (I , X, Y , Z ) is:
(cid:88)
− log P (X, Y |Z, I , Θ)
(cid:43)
(cid:42)(cid:88)
log Z (In , Θ) −
{δ01{degg (V )+degc (V )=2} log(f (θ))}
∂
(cid:88)
∂ δ0
V
V
V
where the expectation is taken with respect to P (X, Y |Z, I , Θ).
Given this estimate, we optimize the parameters by gradient descent. We have also used
the difference of the energy and the Bethe free energy given by the beliefs as an estimate
of the log likelihood in order to support line-search in conjugate gradient or quasi-newton
routines. For our model, we ﬁnd that gradient descent with momentum is efﬁcient enough.

1{degg (V )+degc (V )=2} log(f (θ))

1{degg (V )+degc (V )=2} log(f (θ))

∂
∂ δ0

−

deg=0
weight=2.4607

deg=1
weight=0.8742

deg=2
weight=1.1458

deg=3
weight=0.0133

Figure 3: Learning about junctions: (a) deg=0, no boundary detected; the most common
case. (b) line endings. (c) continuations of contours, more common than line endings. (d)
T-junctions, very rare for the horse dataset. Compare with hand set potentials of Geman
and Geman [3].

5 Experiments

In our experiments we use 344 grayscale images of the horse dataset of Borenstein et al [2].
Half of the images are used for training and half for testing. Human-marked segmentations
are used1 for both training and evaluation.
Training: loopy belief propagation on a typical CDT graph converges in about 1 second.
The gradient descent learning described above converges within 1000 iterations. To under-
stand the weights given by the learning procedure, Figure 3 shows some of the junction
types in M1 and their associated weights δ .
Testing: we evaluate the performance of our model on both edge and region labels. We
present the results using a precision-recall curve which shows the trade-off between false
positives and missed detections. For each edge e, we assign the marginal probability E [Xe ]
to all pixels (x, y) belonging to e. Then for each threshold r , pixels above r are matched
to human-marked boundaries H . The precision P = P (H (x, y) = 1|PE (x, y) > r) and
recall R = P (PE (x, y) > r |H (x, y) = 1) are recorded. Similarly, each pixel in a triangle
t is assigned the marginal probability E [Yt ] and the precision and recall of the ground-truth
ﬁgural pixels computed.

The evaluations are shown in Figure 4 for various combinations of cues. Figure 5 shows
our results on some of the test images.

6 Conclusion

We have introduced a conditional random ﬁeld model on a triangulated representation of
images for ﬁgure/ground labeling. We have measured the contributions of mid- and high-
level cues by quantitative evaluations on held out test data. Our ﬁndings suggest that mid-
level cues provide useful information, even in the presence of high-level shape cues. In
future work we plan to extend this model to multiple object categories.

References
[1] A. Berg and J. Malik. Geometric blur for template matching. In CVPR, 2001.
[2] E. Borenstein and S. Ullman. Class-speciﬁc, top-down segmentation. In Proc. 7th Europ. Conf.
Comput. Vision, volume 2, pages 109–124, 2002.
[3] S. Geman and D. Geman. Stochastic relaxation, gibbs distribution, and the bayesian retoration
of images. IEEE Trans. Pattern Analysis and Machine Intelligence, 6:721–41, Nov. 1984.

1 From the human segmentations on pixel-grid, we use two simple techniques to establish
groundtruth labels on the CDT edges Xe and triangles Yt . For Xe , we run a maximum-cardinality
bipartite matching between the human marked boundaries and the CDT edges. We label Xe = 1 if
75% of the pixels lying under the edge e are matched to human boundaries. For Yt , we label Yt = 1
if at least half of the pixels within the triangle are ﬁgural pixels in the human segmentation.

Figure 4: Performance evaluation: (a) precision-recall curves for horse boundaries, models
with low-level cues only (P b), low- plus mid-level cues (P b+M ), low- plus high-level cues
(P b + H ), and all three classes of cues combined (P b + M + H ). The F-measure recorded
in the legend is the maximal harmonic mean of precision and recall and provides an overall
ranking. Using high-level cues greatly improves the boundary detection performance. Mid-
level continuity cues are useful with or without high-level cues. (b) precision-recall for
regions. The poor performance of the baseline L + M model indicates the ambiguity of
ﬁgure/ground labeling at low-level despite successful boundary detection. High-level shape
knowledge is the key, consistent with evidence from psychophysics [10]. In both boundary
and region cases, the groundtruth labels on CDTs are nearly perfect, indicating that the
CDT graphs preserve most of the image structure.

[4] X. He, R. Zemel, and M. Carreira-Perpinan. Multiscale conditional random ﬁelds for image
labelling. In IEEE Conference on Computer Vision and Pattern Recognition, 2004.
[5] M. P. Kumar, P. H. S. Torr, and A. Zisserman. OBJ CUT. In CVPR, 2005.
[6] S. Kumar and M. Hebert. Discriminative random ﬁelds: A discriminative framework for con-
textual interaction in classiﬁcation. In ICCV, 2003.
[7] John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Proba-
bilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on
Machine Learning, 2001.
[8] J. Malik, S. Belongie, J. Shi, and T. Leung. Textons, contours and regions: Cue integration in
image segmentation. In Proc. 7th Int’l. Conf. Computer Vision, pages 918–925, 1999.
[9] D. Martin, C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using bright-
ness and texture. In Advances in Neural Information Processing Systems 15, 2002.
[10] M. A. Peterson and B. S. Gibson. Object recognition contributions to ﬁgure-ground organiza-
tion. Perception and Psychophysics, 56:551–564, 1994.
[11] X. Ren, C. Fowlkes, and J. Malik. Mid-level cues improve boundary detection. Technical
Report UCB//CSD-05-1382, UC Berkeley, January 2005.
[12] N. Shental, A. Zomet, T. Hertz, and Y. Weiss. Pairwise clustering and graphical models. In
NIPS 2003, 2003.
[13] J. Shewchuk. Triangle: Engineering a 2d quality mesh generator and delaunay triangulator. In
First Workshop on Applied Computational Geometry, pages 124–133, 1996.
[14] Z.W. Tu, X.R. Chen, A.L Yuille, and S.C. Zhu. Image parsing: segmentation, detection, and
recognition. In ICCV, 2003.
[15] Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural
Computation, 2000.
[16] S. Yu, R. Gross, and J. Shi. Concurrent object segmentation and recognition with graph parti-
tioning. In Advances in Neural Information Processing Systems 15, 2002.

00.250.50.75100.250.50.751RecallPrecisionBoundariesPb [F=0.54]Pb + M [F=0.56]Pb + H [F=0.62]Pb + M + H [F=0.66]Ground Truth [F=0.80]00.250.50.75100.250.50.751RecallPrecisionRegionsL+M [F=0.66]L+H [F=0.82]L+M+H [F=0.83]Ground Truth [F=0.95](a)

(b)

(c)

(d)

Figure 5: Sample results. (a) the input grayscale images. (b) the low-level boundary map
output by P b. (c) the edge marginals under our full model and (d) the image masked by
the output region marginals. A red cross in (d) indicates the most probably object center.
By combining relatively simple low-/mid-/high-level cues in a learning framework, We are
able to ﬁnd and segment horses under varying conditions with only a simple object mode.
The boundary maps show the model is capable of suppressing strong gradients in the scene
background while boosting low-contrast edges between ﬁgure and ground. (Row 3) shows
an example of an unusual pose. In (Row 5) we predict a correct off-center object location
and (Row 8) demonstrates grouping together ﬁgure with non-homogeneous appearance.


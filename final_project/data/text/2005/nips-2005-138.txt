Estimation of Intrinsic Dimensionality Using
High-Rate Vector Quantization

Maxim Raginsky and Svetlana Lazebnik
Beckman Institute, University of Illinois
405 N Mathews Ave, Urbana, IL 61801
{maxim,slazebni}@uiuc.edu

Abstract

We introduce a technique for dimensionality estimation based on the no-
tion of quantization dimension, which connects the asymptotic optimal
quantization error for a probability distribution on a manifold to its intrin-
sic dimension. The deﬁnition of quantization dimension yie lds a family
of estimation algorithms, whose limiting case is equivalent to a recent
method based on packing numbers. Using the formalism of high-rate
vector quantization, we address issues of statistical consistency and ana-
lyze the behavior of our scheme in the presence of noise.

1. Introduction

The goal of nonlinear dimensionality reduction (NLDR) [1, 2, 3] is to ﬁnd low-dimensional
manifold descriptions of high-dimensional data. Most NLDR schemes require a good es-
timate of the intrinsic dimensionality of the data to be available in advance. A number
of existing methods for estimating the intrinsic dimension (e.g., [3, 4, 5]) rely on the fact
that, for data uniformly distributed on a d-dimensional compact smooth submanifold of
IRD , the probability of a small ball of radius ǫ around any point on the manifold is Θ(ǫd ).
In this paper, we connect this argument with the notion of quantization dimension [6, 7],
which relates the intrinsic dimension of a manifold (a topological property) to the asymp-
totic optimal quantization error for distributions on the manifold (an operational property).
Quantization dimension was originally introduced as a theoretical tool for studying “non-
standard” signals, such as singular distributions [6] or fr actals [7]. However, to the best
of our knowledge, it has not been previously used for dimension estimation in manifold
learning. The deﬁnition of quantization dimension leads to a family of dimensionality esti-
mation algorithms, parametrized by the distortion exponent r ∈ [1, ∞), yielding in the limit
of r = ∞ a scheme equivalent to K ´egl’s recent technique based on packing numbers [4].
To date, many theoretical aspects of intrinsic dimensionality estimation remain poorly un-
derstood. For instance, while the estimator bias and variance are assessed either heuris-
tically [4] or exactly [5], scant attention is paid to robustness of each particular scheme
against noise. Moreover, existing schemes do not fully utilize the potential for statistical
consistency afforded by ergodicity of i.i.d. data: they compute the dimensionality estimate
from a ﬁxed training sequence (typically, the entire datase t of interest), whereas we show
that an independent test sequence is necessary to avoid over ﬁtting. In addition, using the
framework of high-rate vector quantization allows us to analyze the performance of our
scheme in the presence of noise.

2. Quantization-based estimation of intrinsic dimension

Let us begin by introducing the deﬁnitions and notation used in the rest of the paper. A
D-dimensional k-point vector quantizer [6] is a measurable map Qk : IRD → C , where
C = {y1 , . . . , yk } ⊂ IRD is called the codebook and the yi ’s are called the codevec-
tors. The number log2 k is called the rate of the quantizer, in bits per vector. The sets
= {x ∈ IRD : Qk (x) = yi}, 1 ≤ i ≤ k , are called the quantizer cells (or par-
△
Ri
tition regions). The quantizer performance on a random vector X distributed according
to a probability distribution µ (denoted X ∼ µ) is measured by the average rth-power
△
= Eµ kX − Qk (X )kr , r ∈ [1, ∞), where k · k is the Euclidean
distortion δr (Qk |µ)
norm on IRD .
In the sequel, we will often ﬁnd it more convenient to work wi
th the
△
= δr (Qk |µ)1/r . Let Qk denote the set of all D-dimensional
quantizer error er (Qk |µ)
k-point quantizers. Then the performance achieved by an optimal k-point quantizer on X
△
△
r (k |µ)1/r .
= inf Qk ∈Qk δr (Qk |µ) or equivalently, e∗
is δ∗
= δ∗
r (k |µ)
r (k |µ)
2.1. Quantization dimension

The dimensionality estimation method presented in this paper exploits the connection be-
tween the intrinsic dimension d of a smooth compact manifold M ⊂ IRD (from now on,
simply referred to as “manifold”) and the asymptotic optima l quantization error for a reg-
ular probability distribution1 on M . When the quantizer rate is high, the partition cells can
be well approximated by D-dimensional balls around the codevectors. Then the regularity
of µ ensures that the probability of such a ball of radius ǫ is Θ(ǫd ), and it can be shown
r (k |µ) = Θ(k−1/d ). This is referred to as the high-rate (or high-resolution)
[7, 6] that e∗
approximation, and motivates the deﬁnition of quantization dimension of order r:

.

dr (µ)

log k
△
= − lim
log e∗
r (k |µ)
k→∞
The theory of high-rate quantization conﬁrms that, for a reg ular µ supported on the mani-
fold M , dr (µ) exists for all 1 ≤ r ≤ ∞ and equals the intrinsic dimension of M [7, 6].
(The r = ∞ limit will be treated in Sec. 2.2.)
This deﬁnition immediately suggests an empirical procedur e for estimating the intrinsic di-
mension of a manifold from a set of samples. Let X n = (X1 , . . . , Xn ) be n i.i.d. samples
from an unknown regular distribution µ on the manifold. We also ﬁx some r ∈ [1, ∞).
Brieﬂy, we select a range k1 ≤ k ≤ k2 of codebook sizes for which the high-rate approxi-
mation holds (see Sec. 3 for implementation details), and design a sequence of quantizers
{ ˆQk }k2
that give us good approximations ˆer (k |µ) to the optimal error e∗
r (k |µ) over the
k=k1
chosen range of k . Then an estimate of the intrinsic dimension is obtained by plotting log k
vs. − log ˆer (k |µ) and measuring the slope of the plot over the chosen range of k (because
the high-rate approximation holds, the plot is linear).

r (k |µ). Let us explain how
This method hinges on estimating reliably the optimal errors e∗
this can be achieved. The ideal quantizer for each k should minimize the training error
kXi − Qk (Xi )kr!1/r
n
er (Qk |µtrain ) =   1
Xi=1
n
1A probability distribution µ on IRD is regular of dimension d [6] if it has compact support and
≤ µ(B (a, ǫ)) ≤ cǫd for all a ∈ supp(µ) and all
if there exist constants c, ǫ0 > 0, such that c−1 ǫd
ǫ ∈ (0, ǫ0 ), where B (a, ǫ) is the open ball of radius ǫ centered at a. If M ⊂ IRD is a d-dimensional
smooth compact manifold, then any µ with M = supp(µ) that possesses a smooth, strictly positive
density w.r.t. the normalized surface measure on M is regular of dimension d.

,

where µtrain is the corresponding empirical distribution. However, ﬁnd ing this empirically
optimal quantizer is, in general, an intractable problem, so in practice we merely strive to
produce a quantizer ˆQk whose error er ( ˆQk |µtrain ) is a good approximation to the minimal
△
= inf Qk ∈Qk er (Qk |µtrain ) (the issue of quantizer design is
empirical error e∗
r (k |µtrain )
discussed in Sec. 3). However, while minimizing the training error is necessary for obtain-
ing a statistically consistent approximation to an optimal quantizer for µ, the training error
itself is an optimistically biased estimate of e∗
r (k |µ) [8]: intuitively, this is due to the fact
that an empirically designed quantizer over ﬁts the trainin g set. A less biased estimate is
given by the performance of ˆQk on a test sequence independent from the training set. Let
Z m = (Z1 , . . . , Zm ) be m i.i.d. samples from µ, independent from X n . Provided m is
sufﬁciently large, the law of large numbers guarantees that
the empirical average
kZi − ˆQk (Zi )kr!1/r
m
er ( ˆQk |µtest ) =   1
Xi=1
m
will be a good estimate of the test error er ( ˆQk |µ). Using learning-theoretic formalism [8],
one can show that the test error of an empirically optimal quantizer is a strongly consistent
r (k |µ), i.e., it converges almost surely to e∗
r (k |µ) as n → ∞. Thus, we
estimate of e∗
take ˆer (k |µ) = er ( ˆQk |µtest ). In practice, therefore, the proposed scheme is statistically
consistent to the extent that ˆQk is close to the optimum.
2.2. The r = ∞ limit and packing numbers
If the support of µ is compact (which is the case with all probability distributions considered
in this paper), then the limit e∞ (Qk |µ) = limr→∞ er (Qk |µ) exists and gives the “worst-
case” quantization error of X by Qk :
e∞ (Qk |µ) = max
x∈supp(µ) kx − Qk (x)k.
∞ (k |µ) = inf Qk ∈Qk e∞ (Qk |µ) has an interesting interpretation as the
The optimum e∗
smallest covering radius of the most parsimonious covering of supp(µ) by k or fewer balls
of equal radii [6]. Let us describe how the r = ∞ case is equivalent to dimensionality esti-
mation using packing numbers [4]. The covering number NM (ǫ) of a manifold M ⊂ IRD
is deﬁned as the size of the smallest covering of M by balls of radius ǫ > 0, while the pack-
ing number PM (ǫ) is the cardinality of the maximal set S ⊂ M with kx − yk ≥ ǫ for all
distinct x, y ∈ S . If d is the dimension of M , then NM (ǫ) = Θ(ǫ−d ) for small enough ǫ,
log NM (ǫ)
△
leading to the deﬁnition of the capacity dimension: dcap (M )
. If this
= − limǫ→0
log ǫ
limit exists, then it equals the intrinsic dimension of M . Alternatively, K ´egl [4] suggests
using the easily proved inequality NM (ǫ) ≤ PM (ǫ) ≤ NM (ǫ/2) to express the capacity
log PM (ǫ)
dimension in terms of packing numbers as dcap (M ) = − limǫ→0
.
log ǫ
Now, a simple geometric argument shows that, for any µ supported on M , PM (e∗
∞ (k |µ)) >
k [6]. On the other hand, NM (e∗
∞ (k |µ)) ≤ k , which implies that PM (2e∗
∞ (k |µ)) ≤ k . Let
∞ (k |µ). Let k0 be
{ǫk } be a sequence of positive reals converging to zero, such that ǫk = e∗
such that log ǫk < 0 for all k ≥ k0 . Then it is not hard to show that
log PM (ǫk )
log k
log PM (2ǫk )
k ≥ k0 .
,
log 2ǫk − 1 ≤ −
< −
−
log e∗
∞ (k |µ)
log ǫk
In other words, there exists a decreasing sequence {ǫk }, such that for sufﬁciently large
values of k (i.e., in the high-rate regime) the ratio − log k/ log e∗
∞ (k |µ) can be approx-
imated increasingly ﬁnely both from below and from above by q uantities involving the
packing numbers PM (ǫk ) and PM (2ǫk ) and converging to the common value dcap (M ).

This demonstrates that the r = ∞ case of our scheme is numerically equivalent to K ´egl’s
method based on packing numbers.
For a ﬁnite training set, the
r = ∞ case requires us to ﬁnd an empirically optimal k-
point quantizer w.r.t. the worst-case ℓ2 error — a task that is much more computationally
complex than for the r = 2 case (see Sec. 3 for details). In addition to computational
efﬁciency, other important practical considerations incl ude sensitivity to sampling density
and noise. In theory, this worst-case quantizer is completely insensitive to variations in
sampling density, since the optimal error e∗
∞ (k |µ) is the same for all µ with the same
support. However, this advantage is offset in practice by the increased sensitivity of the
r = ∞ scheme to noise, as explained next.
2.3. Estimation with noisy data

Random noise transforms “clean” data distributed accordin g to µ into “noisy” data dis-
tributed according to some other distribution ν . This will cause the empirically de-
signed quantizer to be matched to the noisy distribution ν , whereas our aim is to esti-
mate optimal quantizer performance on the original clean data. To do this, we make
△
use of the rth-order Wasserstein distance [6] between µ and ν , deﬁned as
¯ρr (µ, ν )
=
inf X∼µ,Y ∼ν (E kX − Y kr )1/r , r ∈ [1, ∞), where the inﬁmum is taken over all pairs
(X, Y ) of jointly distributed random variables with the respective marginals µ and ν . It
is a natural measure of quantizer mismatch, i.e., the difference in performance that results
from using a quantizer matched to ν on data distributed according to µ [9]. Let νn denote
the empirical distribution of n i.i.d. samples of ν . It is possible to show (details omitted
for lack of space) that for an empirically optimal k-point quantizer Q∗
k,r trained on n sam-
ples of ν , |er (Q∗
r (k |µ)| ≤ 2 ¯ρr (νn , ν ) + ¯ρr (µ, ν ). Moreover, νn converges to ν
k,r |ν ) − e∗
in the Wasserstein sense [6]: limn→∞ ¯ρr (νn , ν ) = 0. Thus, provided the training set is
sufﬁciently large, the distortion estimation error is cont rolled by ¯ρr (µ, ν ).
Consider the case of isotropic additive Gaussian noise. Let W be a D-dimensional zero-
mean Gaussian with covariance matrix K = σ2 ID , where ID is the D × D identity matrix.
The noisy data are described by the random variable X + W = Y ∼ ν , and
(cid:21)1/r
√2σ (cid:20) Γ((r + D)/2)
¯ρr (µ, ν ) ≤
,
Γ(D/2)
where Γ is the gamma function. In particular, ¯ρ2 (µ, ν ) ≤ σ√D . The magnitude of the
bound, and hence the worst-case sensitivity of the estimation procedure to noise, is con-
trolled by the noise variance, by the extrinsic dimension, and by the distortion exponent.
The factor involving the gamma functions grows without bound both as D → ∞ and as
r → ∞, which suggests that the susceptibility of our algorithm to noise increases with the
extrinsic dimension of the data and with the distortion exponent.
3. Experimental results

We have evaluated our quantization-based scheme for two choices of the distortion expo-
nent, r = 2 and r = ∞. For r = 2, we used the k-means algorithm to design the quantizers.
For r = ∞, we have implemented a Lloyd-type algorithm, which alternates two steps: (1)
the minimum-distortion encoder, where each sample Xi is mapped to its nearest neighbor
in the current codebook, and (2) the centroid decoder, where the center of each region is
recomputed as the center of the minimum enclosing ball of the samples assigned to that
region. It is clear that the decoder step locally minimizes the worst-case error (the largest
distance of any sample from the center). Using a simple randomized algorithm, the mini-
mum enclosing ball can be found in O((D + 1)!(D + 1)N ) time, where N is the number
of samples in the region [10]. Because of this dependence on D , the running time of the
Lloyd algorithm becomes prohibitive in high dimensions, and even for D < 10 it is an

Training error

Test error

10

r
o
r
r
e
 
g
n
i
n
i
a
r
T

8

6

4

2

0

r=2
r=∞ Lloyd
r=∞ greedy

2000

500
1500
1000
Codebook size (k)
Training error

10

r
o
r
r
e
 
t
s
e
T

8

6

4

2

0

r=2
r=∞ Lloyd
r=∞ greedy

2000

500
1500
1000
Codebook size (k)
Test error

Figure 1: Training and test error vs. codebook size on the swiss roll (Figure 2 (a)). Dashed line:
r = 2 (k-means), dash-dot: r = ∞ (Lloyd-type), solid: r = ∞ (greedy).
2.5
11

40
20

−10

−5

0

10

5

0

5

10

−5

−10
(a)

0.5
0
−0.5

4

2

0

−2

0

−2

−4

−4

4

2

(d)

10

)
k
 
g
o
l
(
 
e
t
a
R

9

8

7

6

5

)
k
 
g
o
l
(
 
e
t
a
R

11

10

9 

8 

7 

6 

Training error
Test error
Training fit
Test fit

−2

−1

1

0
−log(Error)
(b)

Training error
Test error

1

2

4

5

3
−log(Error)
(e)

e
t
a
m
i
t
s
E
 
.
m
i
D

2.25

2

1.75

1.5

1.25

1

2.25

2

1.75

1.5

1.25

1

0.75

e
t
a
m
i
t
s
E
 
.
m
i
D

Training estimate
Test estimate

6

7

8
Rate
(c)

9

10

Training estimate
Test estimate

6

7

8
Rate
(f)

9

10

Figure 2: (a) The swiss roll (20,000 samples). (b) Plot of rate vs. negative log of the quantizer
error (log-log curves), together with parametric curves ﬁtted using linea r least squares (see text). (c)
Slope (dimension) estimates: 1.88 (training) and 2.04 (test). (d) Toroidal spiral (20,000 samples). (e)
Log-log curves, exhibiting two distinct linear parts. (f) Dimension estimates: 1.04 (training), 2.02
(test) in the low-rate region, 0.79 (training), 1.11 (test) in the high-rate region.

order of magnitude slower than k-means. Thus, we were compelled to also implement a
greedy algorithm reminiscent of K ´egl’s algorithm for estimating the packing number [4]:
supposing that k − 1 codevectors have already been selected, the k th one is chosen to be the
sample point with the largest distance from the nearest codevector. Because this is the point
that gives the worst-case error for codebook size k − 1, adding it to the codebook lowers the
error. We generate several codebooks, initialized with different random samples, and then
choose the one with the smallest error. For the experiment shown in Figure 3, the training
error curves produced by this greedy algorithm were on average 21% higher than those of
the Lloyd algorithm, but the test curves were only 8% higher. In many cases, the two test
curves are visually almost coincident (Figure 1). Therefore, in the sequel, we report only
the results for the greedy algorithm for the r = ∞ case.
embedded in IR3 [2]. We split
Our ﬁrst synthetic dataset (Fig. 2 (a)) is the 2D “swiss roll ”
the samples into 4 equal parts and use each part in turn for training and the rest for testing.
This cross-validation setup produces four sets of error curves, which we average to obtain
an improved estimate. We sample quantizer rates in increments of 0.1 bits. The lowest rate
is 5 bits, and the highest rate is chosen as log(n/2), where n is the size of the training set.

The high-rate approximation suggests the asymptotic form Θ(k−1/d ) for the quantizer error

as a function of codebook size k . To validate this approximation, we use linear least squares
to ﬁt curves of the form a + b k−1/2 to the r = 2 training and test distortion curves for the
the swiss roll. The ﬁtting procedure yields estimates of −0.22 + 29.70k−1/2 and 0.10 +
28.41k−1/2 for the training and test curves, respectively. These estimates ﬁt the observed
data well, as shown in Fig. 2(b), a plot of rate vs. the negative logarithm of the training
and test error ( “log-log curves ” in the following). Note tha t the additive constant for the
training error is negative, reﬂecting the fact that the trai ning error of the empirical quantizer
is identically zero when n = k (each sample becomes a codevector). On the other hand,
the test error has a positive additive constant as a consequence of quantizer suboptimality.
n/k → 1, as the average number of training samples
Signi ﬁcantly, the ﬁt deteriorates as
per quantizer cell becomes too small to sustain the exponentially slow decay required for
the high-rate approximation.

Fig. 2(c) shows the slopes of the training and test log-log curves, obtained by ﬁtting a line to
each successive set of 10 points. These slopes are, in effect, rate-dependent dimensionality
estimates for the dataset. Note that the training slope is always below the test slope; this is
“pessimism” of the test error
a consequence of the “optimism” of the training error and the
). The shapes of the two slope
(as reﬂected in the additive constants of the parametric ﬁts
curves are typical of many “well-behaved” datasets. At low r
ates, both the training and the
test slopes are close to the extrinsic dimension, reﬂecting the global geometry of the dataset.
As rate increases, the local manifold structure is revealed, and the slope yields its intrinsic
dimension. However, as n/k → 1, the quantizer begins to “see” isolated samples instead
of the manifold structure. Thus, the training slope begins to fall to zero, and the test slope
rises, reﬂecting the failure of the quantizer to generalize to the test set. For most datasets
in our experiments, a good intrinsic dimensionality estimate is given by the ﬁrst minimum
of the test slope where the line- ﬁtting residual is sufﬁcien
tly low (marked by a diamond in
Fig. 2(c)). For completeness, we also report the slope of the training curve at the same rate
(note that the training curve may not have local minima because of its tendency to fall as
the rate increases). Interestingly, some datasets yield several well-deﬁned dimensionality
estimates at different rates. Fig. 2(d) shows a toroidal spiral embedded in IR3 , which at
larger scales “looks ” like a torus, while at smaller scales t he 1D curve structure becomes
more apparent. Accordingly, the log-log plot of the test error (Fig. 2(e)) has two distinct
linear parts, yielding dimension estimates of 2.02 and 1.11, respectively (Fig. 2(f)).

Recall from Sec. 2.1 that the high-rate approximation for regular probability distributions
is based on the assumption that the intersection of each quantizer cell with the manifold is a
d-dimensional neighborhood of that manifold. Because we compute our dimensionality es-
timate at a rate for which this approximation is valid, we know that the empirically optimal
quantizer at this rate partitions the data into clusters that are locally d-dimensional. Thus,
our dimensionality estimation procedure is also useful for ﬁnding a clustering of the data
that respects the intrinsic neighborhood structure of the manifold from which it is sampled.
As an expample, for the toroidal spiral of Fig. 2(c), we obtain two distinct dimensionality
estimates of 2 and 1 at rates 6.6 and 9.4, respectively (Fig. 2(f)). Accordingly, quantizing
the spiral at the lower (resp. higher) rate yields clusters that are locally two-dimensional
(resp. one-dimensional).

To ascertain the effect of noise and extrinsic dimension on our method, we have embedded
the swiss roll in dimensions 4 to 8 by zero-padding the coordinates and applying a random
orthogonal matrix, and added isotropic zero-mean Gaussian noise in the high-dimensional
space, with σ = 0.2, 0.4, . . . , 1. First, we have veri ﬁed that the r = 2 estimator behaves in
agreement with the Wasserstein bound from Sec. 2.3. The top part of Fig. 3(a) shows the
maximum differences between the noisy and the noiseless test error curves for each combi-
nation of D and σ , and the bottom part shows the corresponding values of the Wasserstein
bound σ√D for comparison. For each value of σ , the test error of the empirically designed
quantizer differs from the noiseless case by O(√D), while, for a ﬁxed D , the difference

r = 2 training

r = ∞ training

7

6
D

5

4

1

0.8

0.4

0.2

0.6
σ

3

0

r = 2 test

7

6
D

5

4

1

0.8

0.4

0.6
σ

0.2

3
0
r = ∞ test

e
t
a
m
i
t
s
e
 
d

3.5

3

2.5

2

8

e
t
a
m
i
t
s
e
 
d

3.5

3

2.5

2

8

3

2.5

2

1.5

1

0.5

e
c
n
e
r
e
f
f
i
d
 
l
a
c
i
r
i
p
m
e

0
3

3

2.5

2

d
n
u
o
b

1.5

1

0.5

4

5

D

6

7

8

σ = 1.0
σ = 0.8
σ = 0.6

σ = 0.4

σ = 0.2
σ = 0.0

σ = 1.0

σ = 0.8

σ = 0.6

σ = 0.4

σ = 0.2

e
t
a
m
i
t
s
e
 
d

3.5

3

2.5

2

8

e
t
a
m
i
t
s
e
 
d

3.5

3

2.5

2

8

7

5

1

0.8

7

1

0.8

0
3

6

7

8

6
D

4

5

5

4

0.4

0.2

0.4

0.6
σ

0.6
σ

σ = 0.0

3
0
(b) r = 2

D
(a) Noise bounds

6
D
4
0.2
3
0
(c) r = ∞
Figure 3: (a) Top: empirically observed differences between noisy and noiseless test curves; bottom:
theoretically derived bound `σ√D´. (b) Height plot of dimension estimates for the r = 2 algorithm
as a function of D and σ . Top: training estimates, bottom: test estimates. (c) Dimension estimates
for r = ∞. Top: training, bottom: test. Note that the training estimates are consistently lower than
the test estimates: the average difference is 0.17 (resp. 0.28) for the r = 2 (resp. r = ∞) case.
of the noisy and noiseless test errors grows as O(σ). As predicted by the bound, the ad-
ditive constant in the parametric form of the test error increases with σ , resulting in larger
slopes of the log-log curve and therefore higher dimension estimates. This is reﬂected in
Figs. 3(b) and (c), which show training and test dimensionality estimates for r = 2 and
r = ∞, respectively. The r = ∞ estimates are much less stable than those for r = 2 be-
cause the r = ∞ (worst-case) error is controlled by outliers and often stays constant over a
range of rates. The piecewise-constant shape of the test error curves (see Fig. 1) results in
log-log plots with unstable slopes.
Table 1 shows a comparative evaluation on the MNIST handwritten digits database2 and a
face video.3 The MNIST database contains 70,000 images at resolution 28× 28 (D = 784),
and the face video has 1965 frames at resolution 28 × 20 (D = 560). For each of the re-
sulting 11 datasets (taking each digit separately), we used half the samples for training
and half for testing. The ﬁrst row of the table shows dimensio n estimates obtained using
a baseline regression method [3]: for each sample point, a local estimate is given by the
d log ℓ
ﬁrst local minimum of the curve
d log ǫ(ℓ) , where ǫ(ℓ) is the distance from the point to its
ℓth nearest neighbor, and a global estimate is then obtained by averaging the local esti-
mates. The rest of the table shows the estimates obtained from the training and test curves
of the r = 2 quantizer and the (greedy) r = ∞ quantizer. Comparative examination of
the results shows that the r = ∞ estimates tend to be fairly low, which is consistent with
the experimental ﬁndings of K ´egl [4]. By contrast, the r = 2 estimates seem to be most
resistant to negative bias. The relatively high values of the dimension estimates reﬂect the
many degrees of freedom found in handwritten digits, including different scale, slant and
thickness of the strokes, as well as the presence of topological features (i.e., loops in 2’s or
extra horizontal bars in 7’s). The lowest dimensionality is found for 1’s, while the highest
is found for 8’s, reﬂecting the relative complexities of dif ferent digits. For the face dataset,
the different dimensionality estimates range from 4.25 to 8.30. This dataset certainly con-
tains enough degrees of freedom to justify such high estimates, including changes in pose

2http://yann.lecun.com/exdb/mnist/
3http://www.cs.toronto.edu/ ˜roweis/data.html , B. Frey and S. Roweis.

Table 1: Performance on the MNIST dataset and on the Frey faces dataset.
Handwritten digits (MNIST data set)

Faces

# samples
Regression
r = 2 train
r = 2 test
r = ∞ train
r = ∞ test

6903
11.14
12.39
15.47
10.33
9.02

7877
7.86
6.51
7.11
8.19
6.61

6990
12.79
16.04
20.89
10.15
13.98

7141
13.39
15.38
19.78
12.63
12.21

6824
11.98
13.22
16.79
9.87
7.26

6313
13.05
14.63
19.80
8.49
10.46

6876
11.19
12.05
16.02
9.85
9.08

7293
10.42
12.32
16.02
8.10
9.92

6825
13.79
19.80
20.07
10.88
14.03

6958
11.26
13.44
17.46
7.40
9.59

1965
5.63
5.70
8.30
4.25
6.39

and facial expression, as well as camera jitter.4 Finally, for both the digits and the faces,
signi ﬁcant noise in the dataset additionally inﬂated the es
timates.
4. Discussion

We have demonstrated an approach to intrinsic dimensionality estimation based on high-
rate vector quantization. A crucial distinguishing feature of our method is the use of an
independent test sequence to ensure statistical consistency and avoid underestimating the
dimension. Many existing methods are well-known to exhibit a negative bias in high di-
mensions [4, 5]. This can have serious implications in practice, as it may result in low-
dimensional representations that lose essential features of the data. Our results raise the
possibility that this negative bias may be indicative of over ﬁtting. In the future we plan to
integrate our proposed method into a uni ﬁed package of quant
ization-based algorithms for
estimating the intrinsic dimension of the data, obtaining its dimension-reduced manifold
representation, and compressing the low-dimensional data [11].

Acknowledgments

Maxim Raginsky was supported by the Beckman Institute Postdoctoral Fellowship. Svet-
lana Lazebnik was partially supported by the National Science Foundation grants IIS-
0308087 and IIS-0535152.

References
[1] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
Science, 290:2323–2326, December 2000.
[2] J.B. Tenenbaum, V. de Silva, and J.C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290:2319–2323, December 2000.
[3] M. Brand. Charting a manifold. In NIPS 15, pages 977–984, Cambridge, MA, 2003. MIT Press.
[4] B. K ´egl.
In NIPS 15, volume 15,
Intrinsic dimension estimation using packing numbers.
Cambridge, MA, 2003. MIT Press.
[5] E. Levina and P.J. Bickel. Maximum likelihood estimation of intrinsic dimension. In NIPS 17,
Cambridge, MA, 2005. MIT Press.
[6] S. Graf and H. Luschgy. Foundations of Quantization for Probability Distributions. Springer-
Verlag, Berlin, 2000.
[7] P.L. Zador. Asymptotic quantization error of continuous signals and the quantization dimension.
IEEE Trans. Inform. Theory, IT-28:139–149, March 1982.
[8] T. Linder. Learning-theoretic methods in vector quantization. In L. Gy ¨or ﬁ, editor, Principles of
Nonparametric Learning. Springer-Verlag, New York, 2001.
[9] R.M. Gray and L.D. Davisson. Quantizer mismatch. IEEE Trans. Commun., 23:439–443, 1975.
[10] E. Welzl. Smallest enclosing disks (balls and ellipsoids). In New Results and New Trends in
Computer Science, volume 555 of LNCS, pages 359–370. Springer, 1991.
[11] M. Raginsky. A complexity-regularized quantization approach to nonlinear dimensionality re-
duction. Proc. 2005 IEEE Int. Symp. Inform. Theory, pages 352–356.

4 Interestingly, Brand [3] reports an intrinsic dimension estimate of 3 for this data set. However,
he used only a 500-frame subsequence and introduced additional mirror symmetry.


A Matching Pursuit Approach to
Sparse Gaussian Process Regression

S. Sathiya Keerthi
Yahoo! Research Labs
210 S. DeLacey Avenue
Pasadena, CA 91105
selvarak@yahoo-inc.com

Wei Chu
Gatsby Computational Neuroscience Unit
University College London
London, WC1N 3AR, UK
chuwei@gatsby.ucl.ac.uk

Abstract

In this paper we propose a new basis selection criterion for building
sparse GP regression models that provides promising gains in accuracy
as well as efﬁciency over previous methods. Our algorithm is much faster
than that of Smola and Bartlett, while, in generalization it greatly outper-
forms the information gain approach proposed by Seeger et al, especially
on the quality of predictive distributions.

1

Introduction

Bayesian Gaussian processes provide a promising probabilistic kernel approach to super-
vised learning tasks. The advantage of Gaussian process (GP) models over non-Bayesian
kernel methods, such as support vector machines, comes from the explicit probabilistic for-
mulation that yields predictive distributions for test instances and allows standard Bayesian
techniques for model selection. The cost of training GP models is O(n3 ) where n is the
number of training instances, which results in a huge computational cost for large data sets.
Furthermore, when predicting a test case, a GP model requires O(n) cost for computing the
mean and O(n2 ) cost for computing the variance. These heavy scaling properties obstruct
the use of GPs in large scale problems.

Recently, sparse GP models which bring down the complexity of training as well as test-
ing have attracted considerable attention. Williams and Seeger (2001) applied the Nystr ¨om
method to calculate a reduced rank approximation of the original n×n kernel matrix. Csat ´o
and Opper (2002) developed an on-line algorithm to maintain a sparse representation of the
GP models. Smola and Bartlett (2001) proposed a forward selection scheme to approximate
the log posterior probability. Candela (2004) suggested a promising alternative criterion by
maximizing the approximate model evidence. Seeger et al. (2003) presented a very fast
greedy selection method for building sparse GP regression models. All of these meth-
ods make efforts to select an informative subset of the training instances for the predictive
model. This subset is usually referred to as the set of basis vectors, denoted as I . The
maximal size of I is usually limited by a value dmax . As dmax (cid:1) n, the sparseness greatly
alleviates the computational burden in both training and prediction of the GP models. The
performance of the resulting sparse GP models crucially depends on the criterion used in
the basis vector selection. Motivated by the ideas of Matching Pursuit (Vincent and Ben-
gio, 2002), we propose a new criterion of greedy forward selection for sparse GP models.

Our algorithm is closely related to that of Smola and Bartlett (2001), but the criterion we
propose is much more efﬁcient. Compared with the information gain method of Seeger
et al. (2003) our approach yields clearly better generalization performance, while essen-
tially having the same algorithm complexity. We focus only on regression in this paper, but
the main ideas are applicable to other supervised learning tasks.

The paper is organized as follows: in Section 2 we present the probabilistic framework
for sparse GP models; in Section 3 we describe our method of greedy forward selection
after motivating it via the previous methods; in Section 4 we discuss some issues in model
adaptation; in Section 5 we report results of numerical experiments that demonstrate the
effectiveness of our new method.

2 Sparse GPs for regression

In regression problems, we are given a training data set composed of n samples. Each
sample is a pair of an input vector xi ∈ Rm and its corresponding target yi ∈ R. The
true function value at xi is represented as an unobservable latent variable f (xi ) and the
target yi is a noisy measurement of f (xi ). The goal is to construct a predictive model that
estimates the relationship x (cid:3)→ f (x).
Gaussian process regression. In standard GPs for regression, the latent variables {f (xi )}
are random variables in a zero mean Gaussian process indexed by {xi }. The prior distrib-
ution of {f (xi )} is a multivariate joint Gaussian, denoted as P (f ) = N (f ; 0, K), where
f = [f (x1 ), . . . , f (xn )]T and K is the n × n covariance matrix whose ij -th element is
K(xi , xj ), K being the kernel function. The likelihood is essentially a model of the mea-
surement noise, which is usually evaluated as a product of independent Gaussian noises,
P (y |f ) = N (y ; f , σ2 I), where y = [y1 , . . . , yn ]T and σ2 is the noise variance. The
posterior distribution P (f |y) ∝ P (y |f )P (f ) is also exactly a Gaussian:
P (f |y) = N (f ; Kα(cid:1) , σ2K(K + σ2 I)
−1 )
(1)
where α(cid:1) = (K + σ2 I)−1y . For any test instance x, the predictive distribution is
x = K(x, x) − kT (K +
N (f (x); µx , σ2
x ) where µx = kT (K + σ2 I)−1y = kT α(cid:1) , σ2
σ2 I)−1k, and k = [K(x1 , x), . . . , K(xn , x)]T . The computational cost of training is
O(n3 ), which mainly comes from the need to invert the matrix (K + σ2 I) and obtain the
vector α(cid:1) . For doing predictions of a test instance the cost is O(n) to compute the mean
and O(n2 ) for computing the variance. This heavy scaling with respect to n makes the use
of standard GP computationally prohibitive on large datasets.
Projected latent variables. Seeger et al. (2003) gave a neat method for working with a
reduced number of latent variables, laying the foundation for forming sparse GP models.
In this section we review their ideas. Instead of assuming n latent variables for all the
training instances, sparse GP models assume only d latent variables placed at some chosen
basis vectors { ˜xi }, denoted as a column vector f I = [f ( ˜x1 ), . . . , f ( ˜xd )]T . The prior
distribution of the sparse GP is a joint Gaussian over f I only, i.e.,
P (f I ) = N (f I ; 0, KI )
(2)
where KI is the d × d covariance matrix of the basis vectors whose ij -th element is
K( ˜xi , ˜xj ).
These latent variables are then projected to all the training instances. Under the imposed
joint Gaussian prior, the conditional mean at the training instances is KTI ,· K−1I f I , where
KI ,· is a d × n matrix of the covariance functions between the basis vectors and all the
training instances. The likelihood can be evaluated by these projected latent variables as
follows
P (y |f I ) = N (y ; KTI ,· K−1I f I , σ2 I)

(3)

The posterior is P (f I |y) = N (f I ; KI α(cid:1)I , σ2KI (σ2KI + KI ,· KTI ,· )−1KI ), where
α(cid:1)I = (σ2KI + KI ,· KTI ,· )−1KI ,· y . The predictive distribution at any test instance x is
x = K(x, x) − ˜kT K−1I ˜kT + σ2 ˜kT (σ2KI +
N (f (x); ˜µx , ˜σ2
x ), where ˜µx = ˜kT α(cid:1)I , ˜σ2
KI ,· KTI ,· )−1 ˜k, and ˜k is a column vector of the covariance functions between the basis
vectors and the test instance x, i.e. ˜k = [K( ˜x1 , x), . . . , K( ˜xd , x)]T .
While the cost of training the full GP model is O(n3 ), the training complexity of sparse
GP models is only O(nd2
max ). This corresponds to the cost of forming K−1I , (σ2KI +
KI ,· KTI ,· )−1 and α(cid:1)I . Thus, if dmax is not big, learning on large datasets is feasible via
sparse GP models. Also, for these sparse models, prediction for each test instance costs
O(dmax ) for the mean and O(d2
max ) for the variance.
Generally the basis vectors can be placed anywhere in the input space Rm . Since training
instances usually cover the input space of interest quite well, it is quite reasonable to select
basis vectors from just the set of training instances. For a given problem dmax is chosen
to be as large as possible subject to constraints on computational time in training and/or
testing. Then we use some basis selection method to ﬁnd I of size dmax . This important
step is taken up in section 3.
A Useful optimization formulation. As pointed out by Smola and Bartlett (2001), it is
useful to view the determination of the mean of the posterior as coming from an optimiza-
tion problem. This viewpoint helps in the selection of basis vectors. The mean of the
posterior distribution is exactly the maximum a posteriori (MAP) estimate, and it is pos-
sible to give an equivalent parametric representation of the latent variables as f = Kα,
where α = [α1 , . . . , αn ]T . The MAP estimate of the full GP is equivalent to minimizing
the negative logarithm of the posterior (1):
αT (σ2K + KT K) α − yT K α
1
min
π(α) :=
(4)
2
α
Similarly, using f I = KI αI for sparse GP models, the MAP estimate of the sparse GP is
equivalent to minimizing the negative logarithm of the posterior, P (f I |y):
αTI (σ2KI + KI ,·KTI ,· ) αI − yT KTI ,· αI
1
˜π(αI ) :=
min
(5)
2
αI
Suppose α in (4) is composed of two parts, α = [αI ; αR ] where I denotes the set of basis
vectors and R denotes the remaining instances. Interestingly, as pointed out by Seeger et al.
(2003), the optimization problem (5) is same as minimizing π(α) in (4) using αI only, i.e.,
with the constraint, αR = 0. In other words, the basis vectors of the sparse GPs can be
selected to minimize the negative log-posterior of the full GPs, π(α) deﬁned as in (4).

3 Selection of basis functions

The most crucial element of the sparse GP approach of the previous section is the choice of
I , the set of basis vectors, which we take to be a subset of the training vectors. The cheapest
method is to select the basis vectors at random from the training data set. But, such a choice
will not work well when dmax is much smaller than n. A principled approach is to select
I that makes the corresponding sparse GP approximate well, the posterior distribution of
the full GP. The optimization formulation of the previous section is useful here. It would
be ideal to choose, among all subsets, I of size dmax , the one that gives the best value of
˜π in (5). But, this requires a combinatorial search that is infeasible for large problems. A
practical approach is to do greedy forward selection. This is the approach used in previous
methods as well as in our method of this paper.

Before we go into the details of the methods, let us give a brief discussion of the time com-
plexities associated with forward selection. There are two costs involved. (1) There is a

basic cost associated with updating of the sparse GP solution, given a sequence of chosen
basis functions. Let us refer to this cost as Tbasic . This cost is the same for all forward
selection methods, and is O(nd2
max ). (2) Then, depending on the basis selection method,
there is the cost associated with basis selection. We will refer to the accumulated value of
this cost for choosing all dmax basis functions as Tselection . Forward basis selection meth-
ods differ in the way they choose effective basis functions while keeping Tselection small.
It is useful to note that the total cost associated with the random basis selection method
mentioned earlier is just Tbasic = O(nd2
max ). This cost forms a baseline for comparison.
Smola and Bartlett’s method. Consider the typical situation in forward selection where
we have a current working set I and we are interested in choosing the next basis vector,
xi . The method of Smola and Bartlett (2001) evaluates each given xi /∈ I by trying its
complete inclusion, i.e., set I (cid:1) = I ∪ {xi } and optimize π(α) using αI (cid:1) = [αI ; αi ].
Thus, their selection criterion for the instance xi /∈ I is the decrease in π(α) that can be
obtained by allowing both αI and αi as variables to be non-zero. The minimal value of
π(α) can be obtained by solving minαI (cid:1) ˜π(αI (cid:1) ) deﬁned in (5). This costs O(nd) time for
each candidate, xi , where d is the size of the current set, I . If all xi /∈ I need to be tried,
it will lead to O(n2d) cost. Accumulated till dmax basis functions are added, this leads to
a Tselection that has O(n2d2
max ) complexity, which is disproportionately higher than Tbasic .
Therefore, Smola and Bartlett (2001) resorted to a randomized scheme by considering only
κ basis elements randomly chosen from outside I during one basis selection. They used a
value of κ = 59. For this randomized method, the complexity of Tselection is O(κnd2
max ).
Although, from a complexity viewpoint, Tbasic and Tselection are same, it should be noted
that the overall cost of the method is about 60 times that of Tbasic .
Seeger et al’s information gain method. Seeger et al. (2003) proposed a novel and very
cheap heuristic criterion for basis selection. The “informativeness ” of an input vector xi /∈
I is scored by the information gain between the true posterior distribution, P (f I (cid:1) |y) and
a posterior approximation, Q(f I (cid:1) |y), where I (cid:1)
denotes the new set of basis vectors after
including a new element xi into the current set I . The posterior approximation Q(f I (cid:1) |y)
ignores the dependencies between the latent variable f (xi ) and the targets other than yi .
Due to this simpliﬁcation, this value of information gain is computed in O(1) time, given
the current predictive model represented by I . Thus, the scores of all instances outside
I can be efﬁciently evaluated in O(n) time, which makes this algorithm almost as fast as
using random selection! The potential weakness of this algorithm might be the non-use of
the correlation in the remaining instances {xi : xi /∈ I }.
Post-backﬁtting approach. The two methods presented above are extremes in efﬁciency:
in Smola and Bartlett’s method Tselection is disproportionately larger than Tbasic while,
in Seeger et al’s method Tselection is very much smaller than Tbasic . In this section we
introduce a moderate method that is effective and whose complexity is in between the two
earlier methods. Our method borrows an idea from kernel matching pursuit.

Kernel Matching Pursuit (Vincent and Bengio, 2002) is a sparse method for ordinary least
squares that consists of two general greedy sparse approximation schemes, called pre-
backﬁtting and post-backﬁtting . It is worth pointing out that the same methods were also
considered much earlier in Adler et al. (1996). Both methods can be generalized to select
the basis vectors for sparse GPs. The pre-backﬁtting approach is very similar in spirit to
Smola and Bartlett ’s method. Our method is an efﬁcient selection criterion that is based
on the post-backﬁtting idea. Recall that, given the current I , the minimal value of π(α)
when it is optimized using only αI as variables is equivalent to minαI ˜π(αI ) as in (5).
The minimizer, denoted as α(cid:1)I , is given by
−1KI ,· y
α(cid:1)I = (σ2KI + KI ,· KTI ,· )
(6)
Our scoring criterion for an instance xi /∈ I is based on optimizing π(α) by ﬁxing αI =
α(cid:1)I and changing αi only. The one-dimensional minimizer can be easily found as

i,· (y − KTI ,·α(cid:1)I ) − σ2 ˜kT
KT
α(cid:1)I
α∗
i
σ2K(xi , xi ) + KT
i =
(7)
i,·Ki,·
where Ki,· is the n × 1 matrix of covariance functions between xi and all the training data,
and ˜ki is a d dimensional vector having K(xj , xi ), xj ∈ I . The selection score of the
instance xi is the decrease in π(α) achieved by the one dimensional optimization of αi ,
which can be written in closed form as
(cid:2)
(cid:1)
σ2K(xi , xi ) + KT
1
(α∗
i,·Ki,·
i )2
∆i =
(8)
2
where α∗
i is deﬁned as in (7). Note that a full kernel column Ki,· is required and so it
costs O(n) time to compute (8). In contrast, for scoring one instance, Smola and Bartlett’s
method requires O(nd) time and Seeger et al’s method requires O(1) time.
Ideally we would like to run over all xi /∈ I and choose the instance which gives the
largest decrease. This will need O(n2 ) effort. Summing the cost till dmax basis vectors are
selected, we get an overall complexity of O(n2dmax ), which is much higher than Tbasic .
To restrict the overall complexity of Tselection to O(nd2
max ), we resort to a randomization
scheme that selects a relatively good one rather than the best. Since it costs only O(n)
time to evaluate our selection criterion in (8) for one instance, we can choose the next basis
vector from a set of dmax instances randomly selected from outside of I . Such a scheme
keeps the overall complexity of Tselection to O(nd2
max ). But, from a practical point of view
the scheme is expensive because the selection criterion (8) requires computing a full kernel
row Ki,· for each instance to be evaluated. As kernel evaluations could be very expensive,
we propose a modiﬁed scheme to keep the number of such evaluations small.
Let us maintain a matrix cache, C of size c × n, that contains c rows of the full kernel matrix
K. At the beginning of the algorithm (when I is empty) we initialize C by randomly
choosing c training instances, computing the full kernel row, Ki,· for the chosen i’s and
putting them in the rows of C . Each step corresponding to a new basis vector selection
proceeds as follows. First we compute ∆i for the c instances corresponding to the rows of
C and select the instance with the highest score for inclusion in I . Let xj denote the chosen
basis vector. Then we sort the remaining instances (that deﬁne C ) according to their ∆i
values. Finally, we randomly select κ fresh instances (from outside of I and the vectors
that deﬁne C ) to replace xj and the κ − 1 cached instances with the lowest score. Thus,
in each basis selection step, we compute the criterion scores for c instances, but evaluate
full kernel rows only for κ fresh instances. An important advantage of the above scheme
is that, those basis elements which have very good scores, but are overtaken by another
better element in a particular step, continue to remain in C and probably get to be selected
in future basis selection steps. Like in Smola and Bartlett’s method we use κ = 59. The
value of c can be set to be any integer between κ and dmax . For any c in this range, the
complexity of Tselection remains at most O(nd2
max ). The above cache scheme is special to
our method and cannot be used with Smola and Bartlett’s method without unduly increasing
its complexity. If available, it is also useful to have an extra cache for storing kernel rows of
instances which get discarded in one step, but which get to be considered again in a future
step. Smola and Bartlett’s method can also gain from such a cache.

4 Model adaptation

In this section we address the problem of model adaptation for a given number of basis
functions, dmax . Seeger (2003) and Seeger et al. (2003) give the details together with
a very good discussion of various issues associated with gradient based model adapta-
tion. Since the same ideas hold for all basis selection methods, we will not discuss them
in detail. The sparse GP model is conditional on the parameters in the kernel function
and the Gaussian noise level σ2 , which can all be collected together in θ , the hyperpa-
rameter vector. The optimal values of θ can be inferred by minimizing the negative log

of the marginal likelihood, φ(θ) = − log P (y |θ) using gradient based techniques, where
(cid:3) P (y |f I )P (f I )df I = N (y |0, σ2 I + KTI ,· K−1I KI ,· ). One of the problems
P (y |θ) =
in doing this is the dependence of I on θ that makes φ a non-differentiable function. This
problem can be handled by repeating the following alternating steps: (1) ﬁx θ and select I
by the given basis selection algorithm; and (2) ﬁx I and do a (short) gradient based adap-
tation of θ . For the cache-based post-backﬁtting method of basis selection we also do the
following for adding some stability to the model adaptation process. After we do step (2)
using some I and obtain a θ we set the initial kernel cache, C using the rows of KI ,· at θ .

5 Numerical experiments

In this section, we compare our method against other sparse GP methods to verify the use-
fulness of our algorithm. To evaluate generalization performance, we utilize Normalized
(cid:4)
(yi−µi )2
Mean Square Error (NM S E) given by 1
t
and Negative Logarithm of Pre-
i=1
(cid:4)
Var(y)
t
i=1 − log P (yi |µi , σ2
1
t
i ) where t is the number
dictive Distribution (N L PD) deﬁned as
t
of test cases, yi , µi and σ2
i are, respectively, the target, the predictive mean and the pre-
dictive variance of the i-th test case. NM S E uses only the mean while N L PD measures
the quality of predictive distributions as it penalizes over-conﬁdent predictions as well as
(cid:2)
(cid:1)(cid:4)
under-conﬁdent ones. For all experiments, we use the ARD Gaussian kernel deﬁned by
K(xi , xj ) = υ0 exp
− x(cid:2)
j )2
m
+ υb where υ0 , υ(cid:2) , υb > 0 and x(cid:2)
(cid:2)=1 υ(cid:2) (x(cid:2)
i denotes the
(cid:8)-th element of xi . The ARD parameters {υ(cid:2) } give variable weights to input features that
i
leads to a type of feature selection.
Quality of Basis Selection in KIN40K Data Set. We use the KIN40K data set,1 composed
of 40,000 samples, to evaluate and compare the performance of the various basis selection
criteria. We ﬁrst trained a full GPR model with the ARD Gaussian kernel on a subset of
2000 samples randomly selected in the dataset. The optimal values of the hyperparameters
that we obtained were ﬁxed and used for all the sparse GP models in this experiment. We
compare the following ﬁve basis selection methods:
1. the baseline algorithm (RAND) that selects I at random;
2. the information gain algorithm (IN FO) proposed by Seeger et al. (2003);
3. our algorithm described in Section 3 with cache size c = κ = 59 (KA P PA) in
which we evaluate the selection scores of κ instances at each step;
4. our algorithm described in Section 3 with cache size c = dmax (DMAX);
5. the algorithm (SB) proposed by Smola and Bartlett (2001) with κ = 59.

We randomly selected 10,000 samples for training, and kept the remaining 30,000 samples
as test cases. For the purpose of studying variability the methods were run on ten such
random partitions. We varied dmax from 100 to 1200. The test performances of the ﬁve
methods are presented in Figure 1. From the upper plot of Figure 1 we can see that IN FO
yields much worse NM S E results than KA P PA , DMAX and SB, when dmax is less than 600.
When the size is around 100, IN FO is even worse than RAND. DMAX is always better than
KA P PA . Interestingly, DMAX is even slightly better than SB when dmax is less than 200.
This is probably because DMAX has a bigger set of basis functions to choose from, than
SB. SB generally yields slightly better results than KA P PA . From the middle plot of Figure
1 we can note that IN FO always gives poor N L PD results, even worse than RAND. The
performances of KA P PA , DMAX and SB are close.
The lower plot of Figure 1 gives the CPU time consumed by the ﬁve algorithms for training,
as a function of dmax , in log − log scale. The scaling exponents of RAND, IN FO and SB are
1The dataset is available at http://www.igi.tugraz.at/aschwaig/data.html.

E
S
M
N

0.3

0.2

0.1

0

−0.4
−0.6
−0.8
−1
−1.2
−1.4
−1.6
−1.8

105

104

103

102

101

D
P
L
N

E
M
I
T
 
U
P
C

100 

200 

300 

400 

500 

600 

700 

800 

900  1000 1100 1200

100 

200 

300 

400 

500 

600 

700 

800 

900 

1000

1100

1200

SB 
DMAX 
KAPPA 

INFO
RAND 

100
100
200
300
500
1000
2000
Figure 1: The variations of test set NM S E , test set N L PD and CPU time (in seconds) for
training of the ﬁve algorithms as a function of dmax . In the NM S E and N L PD plots, at each
value of dmax , the results of the ﬁve algorithms are presented as a boxplot group. From left
to right, they are RAND(blue), IN FO(red), KA P PA(green), DMAX(black), and SB(magenta).
Note that the CPU time plot is on a log − log scale.

around 2.0 (i.e., cost is proportional to d2
max ), which is consistent with our analysis. IN FO
is almost as fast as RAND, while SB is about 60 times slower than IN FO . The gap between
KA P PA and IN FO is the O(κndmax ) time in computing the score (8) for κ candidates.2 As
dmax increases, the cost of KA P PA asymptotically gets close to IN FO . The gap between
DMAX and KA P PA is the O(nd2
max − κndmax ) cost in computing the score (8) for the ad-
ditional (dmax − κ) instances. Thus, as dmax increases, the curve of DMAX asymptotically
becomes parallel to the curve of IN FO . Asymptotically, the ratio of the computational times
of DMAX and IN FO is only about 3. Thus, unlike SB, which is about 60 times slower than
IN FO , DMAX is only about 3 times slower than IN FO . Thus DMAX is an excellent method
for achieving excellent generalization while also being quite efﬁcient.
Model Adaptation on Benchmark Data Sets. Next, we compare model adaptation abili-
ties of the following three algorithms for dmax = 500.

1. The SB algorithm is applied to build a sparse GPR model with ﬁxed hyperparame-
ters (FIX ED - SB). The values of these hyperparameters were obtained by training
via a standard full GPR model on a manageable subset of 2000 samples randomly
selected from the training data. FIX ED - SB serves as a baseline.
2. The model adaptation scheme is coupled with the IN FO basis selection algorithm
(ADA P T- IN FO).
3. The model adaptation scheme is coupled with our DMAX basis selection algorithm
(ADA P T-DMAX).
2 If we want to take kernel evaluations also into account, the cost of KA P PA is O(mκndmax )
where m is the number of input variables. Note that IN FO does not require any kernel evaluations for
computing its selection criterion.

Table 1: Test results of the three algorithms on the seven benchmark regression datasets.
The results are the averages over 20 trials, along with the standard deviation. d denotes the
number of input features, ntrg denotes the training data size and ntst denotes the test data
size. We use bold face to indicate the lowest average value among the results of the three
algorithms. The symbol (cid:9) is used to indicate the cases signiﬁcantly worse than the winning
entry; a p-value threshold of 0.01 in Wilcoxon rank sum test was used to decide this.

N M S E
N L P D
DATA S E T
ADA P T-DMAX
ADA P T- IN FO ADA P T-DMAX
FIX ED - SB
ADA P T- IN FO
FIX ED - SB
d ntrg ntst
0.67 ± 0.53
3 .56 ± 0 .09 3 .11 ± 0 .65(cid:1) 1 .37 ± 0 .34(cid:1)
3 .54 ± 0 .08
3.52 ± 0.08
BANK8 FM 8 4500 3692
BANK32NH 32 4500 3692 48 .08 ± 2 .92 49 .04 ± 1 .34(cid:1) 47.41 ± 1.35 −1.02 ± 0.21 −0 .79 ± 0 .06(cid:1) −0 .88 ± 0 .03(cid:1)
2 .45 ± 0 .16
2.45 ± 0.15
2 .46 ± 0 .14 5 .18 ± 0 .61(cid:1) 3 .70 ± 0 .46(cid:1)
3.04 ± 0.17
C PU SMA L L 12 4500 3692
1.58 ± 0.13
1 .61 ± 0 .14
1 .61 ± 0 .11 4 .49 ± 0 .26(cid:1) 3 .68 ± 0 .40(cid:1)
3.09 ± 0.20
21 4500 3692
C PUAC T
13.03 ± 0.30
CA LHOU S E 8 10000 10640 22 .58 ± 0 .34(cid:1) 22 .82 ± 0 .46(cid:1) 20.02 ± 0.88 31 .83 ± 3 .35(cid:1) 21 .20 ± 1 .47(cid:1)
8 10000 12784 42 .27 ± 2 .14(cid:1) 37 .30 ± 1 .29(cid:1) 35.87 ± 0.94 12 .06 ± 0 .67 12 .06 ± 0 .07(cid:1)
11.71 ± 0.03
HOU S E8 L
HOU S E16H 16 10000 12784 53 .45 ± 7 .05(cid:1) 45 .72 ± 1 .15(cid:1) 44.29 ± 0.76 12 .72 ± 1 .69 12 .48 ± 0 .06(cid:1)
12.13 ± 0.04

We selected seven large regression datasets.3 Each of them is randomly partitioned into
training/test splits. For the purpose of analyzing statistical signiﬁcance, the partition was
repeated 20 times independently. Test set performances (NM S E and N L PD) of the three
methods on the seven datasets are presented in Table 1. On the four datasets with 4500
training instances, the NM S E results of the three methods are quite comparable. ADA P T-
DMAX yields signiﬁcantly better N L PD results on three of those four datasets. On the three
larger datasets with 10,000 training instances, ADA P T-DMAX is signiﬁcantly better than
ADA P T- IN FO on both NM S E and N L PD .

We also tested our algorithm on the Outaouais dataset, which consists of 29000 training
samples and 20000 test cases whose targets are held by the organizers of the “Evaluating
Predictive Uncertainty Challenge”. 4 The results of NM S E and N L PD we obtained in this
blind test are 0.014 and −1.037 respectively, which are much better than the results of
other participants.

References
Adler, J., B. D. Rao, and K. Kreutz-Delgado. Comparison of basis selection methods. In
Proceedings of the 30th Asilomar conference on signals, systems and computers, pages
252–257, 1996.
Candela, J. Q. Learning with uncertainty - Gaussian processes and relevance vector ma-
chines. PhD thesis, Technical University of Denmark, 2004.
Csat ´o, L. and M. Opper. Sparse online Gaussian processes. Neural Computation, The MIT
Press, 14:641–668, 2002.
Seeger, M. Bayesian Gaussian process models: PAC-Bayesian generalisation error bounds
and sparse approximations. PhD thesis, University of Edinburgh, July 2003.
Seeger, M., C. K. I. Williams, and N. Lawrence. Fast forward selection to speed up sparse
Gaussian process regression. In Workshop on AI and Statistics 9, 2003.
Smola, A. J. and P. Bartlett. Sparse greedy Gaussian process regression. In Leen, T. K.,
T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Sys-
tems 13, pages 619–625. MIT Press, 2001.
Vincent, P. and Y. Bengio. Kernel matching pursuit. Machine Learning, 48:165–187, 2002.
Williams, C. K. I. and M. Seeger. Using the Nystr ¨om method to speed up kernel machines.
In Leen, T. K., T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information
Processing Systems 13, pages 682–688. MIT Press, 2001.
3These datasets are vailable at http://www.liacc.up.pt/∼ltorgo/Regression/DataSets.html.
4The dataset and the results contributed by other participants can be found at the web site of the
challenge http://predict.kyb.tuebingen.mpg.de/.


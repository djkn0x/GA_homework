Transfer learning for text classiﬁcation

Chuong B. Do
Computer Science Department
Stanford University
Stanford, CA 94305

Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305

Abstract

Linear text classiﬁcation algorithms work by computing an inner prod-
uct between a test document vector and a parameter vector. In many such
algorithms, including naive Bayes and most TFIDF variants, the parame-
ters are determined by some simple, closed-form, function of training set
statistics; we call this mapping mapping from statistics to parameters, the
parameter function. Much research in text classiﬁcation over the last few
decades has consisted of manual efforts to identify better parameter func-
tions. In this paper, we propose an algorithm for automatically learning
this function from related classiﬁcation problems. The parameter func-
tion found by our algorithm then deﬁnes
a new learning algorithm for
text classiﬁcation, which we can apply to novel classiﬁcation tasks. We
ﬁnd that our learned classi ﬁer outperforms existing methods on a variety
of multiclass text classiﬁcation tasks.

1 Introduction
In the multiclass text classiﬁcation task, we are given a training set of documents, each
labeled as belonging to one of K disjoint classes, and a new unlabeled test document.
Using the training set as a guide, we must predict the most likely class for the test doc-
ument.
“Bag-of-w ords”
linear text classiﬁers
represent a document as a vector x of
word counts, and predict the class whose score (a linear function of x) is highest, i.e.,
arg maxk2f1;:::;K g Pn
i=1 (cid:18)kixi . Choosing parameters f(cid:18)ki g which give high classiﬁcation
accuracy on test data, thus, is the main challenge for linear text classiﬁcation algorithms.
In this paper, we focus on linear text classiﬁcation algorithms in which the parameters are
pre-speciﬁed functions of training set statistics; that is, each (cid:18)ki is a function (cid:18)ki := g(uki )
of some ﬁxed statistics uki of the training set. Unlike discriminative learning methods, such
as logistic regression [1] or support vector machines (SVMs) [2], which use numerical op-
timization to pick parameters, the learners we consider perform no optimization. Rather, in
our technique, parameter learning involves tabulating statistics vectors fuki g and applying
the closed-form function g to obtain parameters. We refer to g , this mapping from statistics
to parameters, as the parameter function.
the multinomial and multivariate
Many common text classiﬁcation methods—including
Bernoulli event models for naive Bayes [3], the vector space-based TFIDF classiﬁer
[4],
and its probabilistic variant, PrTFIDF [5]—belong
to this class of algorithms. Here, picking
a good text classiﬁer
from this class is equivalent to ﬁnding the right parameter function
for the available statistics.
In practice, researchers often develop text classiﬁcation
algorithms by trial-and-error,
guided by empirical testing on real-world classiﬁcation tasks (cf. [6, 7]). Indeed, one could

argue that much of the 30-year history of information retrieval has consisted of manually
trying TFIDF formula variants (i.e. adjusting the parameter function g ) to optimize perfor-
mance [8]. Even though this heuristic process can often lead to good parameter functions,
such a laborious task requires much human ingenuity, and risks failing to ﬁnd algorithm
variations not considered by the designer.
In this paper, we consider the task of automatically learning a parameter function g for
text classiﬁcation. Given a set of example text classiﬁcation problems, we wish to “meta-
learn” a new learning algorithm (as speciﬁed by the parameter function g ), which may then
be applied new classiﬁcation problems. The meta-learning technique we propose, which
leverages data from a variety of related classiﬁcation tasks to obtain a good classiﬁer
for
new tasks, is thus an instance of transfer learning; speciﬁcally , our framework automates
the process of ﬁnding a good parameter function for text classiﬁers,
replacing hours of
hand-tweaking with a straightforward, globally-convergent, convex optimization problem.
Our experiments demonstrate the effectiveness of learning classiﬁer
forms. In low training
data classiﬁcation tasks, the learning algorithm given by our automatically learned parame-
ter function consistently outperforms human-designed parameter functions based on naive
Bayes and TFIDF, as well as existing discriminative learning approaches.
2 Preliminaries
Let V = fw1 ; : : : ; wn g be a ﬁx ed vocabulary of words, and let X = Zn and Y =
f1; : : : ; K g be the input and output spaces for our classiﬁcation problem. A labeled docu-
ment is a pair (x; y) 2 X (cid:2) Y , where x is an n-dimensional vector with xi indicating the
number of occurrences of word wi in the document, and y is the document’s class label. A
classiﬁcation problem is a tuple hD ; S; (xtest ; ytest )i, where D is a distribution over X (cid:2) Y ,
i=1 is a set of M training examples, (xtest ; ytest ) is a single test example, and
S = f(xi ; yi )gM
all M + 1 examples are drawn iid from D . Given a training set S and a test input vector
xtest , we must predict the value of the test class label ytest .
In linear classiﬁcation algorithms, we evaluate the score fk (xtest ) := Pi (cid:18)kixtest i for as-
signing xtest to each class k 2 f1; : : : ; K g and pick the class y = arg maxk fk (xtest ) with
the highest score. In our meta-learning setting, we deﬁne each (cid:18)ki as the component-wise
evaluation of the parameter function g on some vector of training set statistics uki :
(cid:18)k1
g(uk1 )
2
2
3
3
g(uk2 )
(cid:18)k2
664
775
664
775
...
...
(cid:18)kn
g(ukn )
Here, each uki 2 Rq (k = 1; : : : ; K , i = 1; : : : ; n) is a vector whose components are
computed from the training set S (we will provide speciﬁc examples later). Furthermore,
g : Rq ! R is the parameter function mapping from uki to its corresponding parameter
(cid:18)ki . To illustrate these deﬁnitions, we show that two speciﬁc cases of the naive Bayes and
TFIDF classiﬁcation methods belong to the class of algorithms described above.

(1)

:=

:

In the multinomial variant of the naive Bayes classiﬁcation algorithm,1 the
Naive Bayes:
score for assigning a document x to class k is
f NB
k (x) := log ^p(y = k) + Pn
(2)
i=1 xi log ^p(wi j y = k):
over document classes, and
term, ^p(y = k), corresponds to a “prior”
The ﬁrst
the second term, ^p(wi j y = k), is the (smoothed) relative frequency of word

1Despite naive Bayes’ overly strong independence assumptions and thus its shortcomings as a
probabilistic model for text documents, we can nonetheless view naive Bayes as simply an algorithm
which makes predictions by computing certain functions of the training set. This view has proved
useful for analysis of naive Bayes even when none of its probabilistic assumptions hold [9]; here, we
adopt this view, without attaching any particular probabilistic meaning to the empirical frequencies
^p((cid:1)) that happen to be computed by the algorithm.

term is
wi in training documents of class k . For balanced training sets, the ﬁrst
irrelevant. Therefore, we have f NB
k (x) = Pi (cid:18)kixi where (cid:18)ki = gNB (uki ),
number of times wi appears in documents of class k
uki1
2
3
2
3
number of documents of class k containing wi
uki2
6664
7775
6664
7775
total number of words in documents of class k
uki3
total number of documents of class k
uki4
total number of documents
uki5

uki :=

; (3)

=

and

(4)

(5)

TFIDF:

gNB (uki ) := log

uki1 + "
uki3 + n"
where " is a smoothing parameter. (" = 1 gives Laplace smoothing.)
In the unnormalized TFIDF classiﬁer , the score for assigning x to class k is
i=1 (cid:16)xi jy=k (cid:1) log
^p(xi>0) (cid:17) ;
^p(xi>0) (cid:17) (cid:16)xi (cid:1) log
f TFIDF
(x) := Pn
1
1
k
where xi jy=k (sometimes called the average term frequency of wi ) is the average
ith component of all document vectors of class k , and ^p(xi > 0) (sometimes
called the document frequency of wi ) is the proportion of all documents containing
wi .2 As before, we write f TFIDF
(x) = Pi (cid:18)kixi with (cid:18)ki = gTFIDF (uki ). The
k
statistics vector is again deﬁned as in (3), but this time,
uki2 (cid:19)2
uki4 (cid:18)log
uki5
uki1
(6)
gTFIDF (uki ) :=
:
Space constraints preclude a detailed discussion, but many other classiﬁcation algorithms
can similarly be expressed in this framework, using other deﬁnitions of the statistics vec-
tors fuki g. These include most other variants of TFIDF based on different TF and IDF
terms [7], PrTFIDF [5], and various heuristically modi ﬁed versions of naive Bayes [6].
3 Learning the parameter function
In the last section, we gave two examples of algorithms that obtain their parameters (cid:18)ki
by applying a function g to a statistics vector uki . In each case, the parameter function
was hand-designed, either from probabilistic (in the case of naive Bayes [3]) or geometric
(in the case of TFIDF [4]) considerations. We now consider the problem of automatically
learning a parameter function from example classiﬁcation tasks. In the sequel, we assume
ﬁx ed statistics vectors fuki g and focus on ﬁnding an optimal parameter function g .
In the standard supervised learning setting, we are given a training set of examples sam-
pled from some unknown distribution D , and our goal is to use the training set to make a
prediction on a new test example also sampled from D . By using the training examples to
understand the statistical regularities in D , we hope to predict y test from xtest with low error.
Analogously, the problem of meta-learning g is again a supervised learning task; here, how-
ever, the training “e xamples” are now classiﬁcation problems sampled from a distribution
D over classiﬁcation problems.3 By seeing many instances of text classiﬁcation problems

2Note that (5) implicitly deﬁnes fTFIDF
(x) as a dot product of two vectors, each of whose com-
k
ponents consist of a product of two terms.
In the normalized TFIDF classiﬁer , both vectors are
normalized to unit length before computing the dot product, a modiﬁcation that makes the algorithm
more stable for documents of varying length. This too can be represented within our framework by
considering appropriately normalized statistics vectors.
3Note that in our meta-learning problem, the output of our algorithm is a parameter function
g mapping statistics to parameters. Our training data, however, do not explicitly indicate the best
parameter function g (cid:3) for each example classiﬁcation problem. Effectively then, in the meta-learning
task, the central problem is to ﬁt g to some unseen g(cid:3) , based on test examples in each training
classiﬁcation problem.

drawn from D , we hope to learn a parameter function g that exploits the statistical regulari-
ties in problems from D . Formally, let S = fhD (j ) ; S (j ) ; (x(j ) ; y (j ) )igm
j=1 be a collection
of m classiﬁcation problems sampled iid from D . For a new, test classiﬁcation problem
hDtest ; Stest ; (xtest ; ytest )i sampled independently from D , we desire that our learned g cor-
rectly classify xtest with high probability.
To achieve our goal, we ﬁrst
restrict our attention to parameter functions g that are linear
in their inputs. Using the linearity assumption, we pose a convex optimization problem
for ﬁnding a parameter function g that achieves small loss on test examples in the training
collection. Finally, we generalize our method to the non-parametric setting via the “k ernel
trick,”
thus allowing us to learn complex, highly non-linear functions of the input statistics.

3.1 Softmax learning

;

;

(7)

k = 1; : : : ; K;

p(y = k j x; f(cid:18)ki g) :=

Recall that in softmax regression, the class probabilities p(y j x) are modeled as
exp(Pi (cid:18)kixi )
Pk0 exp(Pi (cid:18)k0 ixi )
where the parameters f(cid:18)ki g are learned from the training data S by maximizing the con-
ditional log likelihood of the data. In this approach, a total of K n parameters are trained
jointly using numerical optimization. Here, we consider an alternative approach in which
each of the K n parameters is some function of the prespeciﬁed statistics vectors; in partic-
ular, (cid:18)ki := g(uki ). Our goal is to learn an appropriate g .
To pose our optimization problem, we start by learning the linear form g(uki ) = (cid:12)T uki .
Under this parameterization, the conditional likelihood of an example (x; y) is
exp(Pi (cid:12)T ukixi )
Pk0 exp(Pi (cid:12)T uk0 ixi )
In this setup, one natural approach for learning a linear function g is to maximize the
(regularized) conditional log likelihood ‘((cid:12) : S ) for the entire collection S :
‘((cid:12) : S ) := Pm
j=1 log p(y (j ) j x(j ) ; (cid:12)) (cid:0) C jj(cid:12) jj2
exp (cid:16)(cid:12)T Pi
i (cid:17)
y(j ) ix(j )
(j )
1
log 0
m
u
Xj=1
A (cid:0) C jj(cid:12) jj2 :
=
@
Pk exp (cid:16)(cid:12)T Pi
i (cid:17)
(j )
ki x(j )
In (9), the latter term corresponds to a Gaussian prior on the parameters (cid:12) , which provides
a means for controlling the complexity of the learned parameter function g . The maximiza-
tion of (9) is similar to softmax regression training except that here, instead of optimizing
over the parameters f(cid:18)ki g directly, we optimize over the choice of (cid:12) .

p(y = k j x; (cid:12)) =

k = 1; : : : ; K:

(9)

(8)

u

3.2 Nonparametric function learning

In this section, we generalize the technique of the previous section to nonlinear g . By the
Representer Theorem [10], there exists a maximizing solution to (9) for which the optimal
parameter vector (cid:12) (cid:3) is a linear combination of training set statistics:
ki x(j )
(j )
(cid:12)(cid:3) = Pm
j=1 Pk (cid:11)(cid:3)
jk Pi
i
From this, we reparameterize the original optimization over (cid:12) in (9) as an equivalent opti-
mization over training example weights f(cid:11)jk g. For notational convenience, let
i x(j 0 )
K(j; j 0 ; k ; k 0 ) := Pi Pi0 x(j )
i0

(j )
ki )T u

(j 0 )
k0 i0 :

(10)

(11)

(u

u

:

1

0.8

0.6

0.4

0.2

)
2
k
u
(
p
x
e

0

0

0.2

(a)

0.4
0.6
exp(uk1)

0.8

1

(b)

0

−5

−10

2
k
u

−15

−20

−25

−30
−35

−30

−25

−20

−15
uk1

−10

−5

0

Figure 1: Distribution of unnormalized uki vectors in dmoz data (a) with and (b) without
applying the log transformation in (15). In principle, one could alternatively use a feature
vector representation using these frequencies directly, as in (a). However, applying the log
transformation yields a feature space with fewer isolated points in R2 , as in (b). When using
the Gaussian kernel, a feature space with few isolated points is important as the topology
of the feature space establishes locality of in ﬂuence for support vectors.

m
Xj 0=1

(cid:0) C

1
A

‘(f(cid:11)jk g : S ) :=

Substituting (10) and (11) into (9), we obtain
exp (cid:16)Pm
j=1 Pk (cid:11)jkK(j; j 0 ; k ; y (j 0 ) )(cid:17)
log 0
@
Pk0 exp (cid:16)Pm
j=1 Pk (cid:11)jkK(j; j 0 ; k ; k 0 )(cid:17)
m
m
Xj 0=1 Xk Xk0
Xj=1
(cid:11)jk (cid:11)j 0 k0 K(j; j 0 ; k ; k 0 ):
Note that (12) is concave and differentiable, so we can train the model using any standard
numerical gradient optimization procedure, such as conjugate gradient or L-BFGS [11].
The assumption that g is a linear function of uki , however, places a severe restriction on
the class of learnable parameter functions. Noting that the statistics vectors appear only as
an inner product in (11), we apply the “k ernel trick”
to obtain
(j 0 )
i x(j 0 )
(j )
K(j; j 0 ; k ; k 0 ) := Pi Pi0 x(j )
(13)
ki ; u
k0 i0 );
i0 K (u
where the kernel function K (u; v) = (cid:10)(cid:8)(u); (cid:8)(v)(cid:11) deﬁnes
the inner product of some
high-dimensional mapping (cid:8)((cid:1)) of its inputs.4 In particular, choosing a Gaussian (RBF)
kernel, K (u; v) := exp((cid:0)(cid:13) jju (cid:0) vjj2 ), gives a non-parametric representation for g :
(j )
j=1 Pk Pi (cid:11)jk x(j )
g(uki ) = (cid:12)T (cid:8)(uki ) = Pm
ki (cid:0) uki jj2 ):
exp((cid:0)(cid:13) jju
i
Thus, g(uki ) is a weighted combination of the values f(cid:11)jk x(j )
i g, where the weights depend
(j )
exponentially on the squared ‘2 -distance of uki to each of the statistics vectors fu
ki g. As a
result, we can approximate any sufﬁciently smooth bounded function of u arbitrarily well,
given sufﬁciently many training classiﬁcation problems.
4 Experiments
To validate our method, we evaluated its ability to learn parameter functions on a variety
of email and webpage classiﬁcation tasks in which the number of classes, K , was large
(K = 10), and the number of number of training examples per class, m=K , was small
(m=K = 2). We used the dmoz Open Directory Project hierarchy,5 the 20 Newsgroups
dataset,6 the Reuters-21578 dataset,7 and the Industry Sector dataset8 .

(12)

(14)

4Note also that as a consequence of our kernelization, K itself can be considered a “kernel”
between all statistics vectors from two entire documents.
5 http://www.dmoz.org
6 http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.tar.gz
7 http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz
8 http://www.cs.umass.edu/ ˜mccallum/data/sector .tar.gz

Table 1: Test set accuracy on dmoz categories. Columns 2-4 give the proportion of correct
classiﬁcations using non-discriminative methods: the learned g , Naive Bayes, and TFIDF,
respectively. Columns 5-7 give the corresponding values for the discriminative methods:
softmax regression, 1-vs-all SVMs, and multiclass SVMs. The best accuracy in each row
is shown in bold.

Category
Arts
Business
Computers
Games
Health
Home
Kids and Teens
News
Recreation
Reference
Regional
Science
Shopping
Society
Sports
World
Average

g
0.421
0.456
0.467
0.411
0.479
0.640
0.252
0.349
0.663
0.635
0.438
0.363
0.612
0.435
0.619
0.531
0.486

gNB
0.296
0.283
0.304
0.288
0.282
0.470
0.205
0.222
0.487
0.415
0.268
0.256
0.456
0.308
0.432
0.491
0.341

gTFIDF
0.286
0.286
0.327
0.240
0.337
0.454
0.142
0.212
0.529
0.458
0.258
0.246
0.556
0.285
0.285
0.352
0.328

softmax
0.352
0.336
0.344
0.279
0.382
0.501
0.202
0.382
0.477
0.602
0.329
0.353
0.483
0.379
0.507
0.329
0.390

1VA-SVM MC-SVM
0.367
0.203
0.340
0.233
0.217
0.387
0.330
0.240
0.337
0.213
0.440
0.333
0.173
0.167
0.397
0.270
0.590
0.353
0.543
0.383
0.357
0.260
0.223
0.340
0.550
0.373
0.377
0.213
0.527
0.267
0.303
0.277
0.264
0.397

The dmoz project is a hierarchical collection of webpage links organized by subject matter.
The top level of the hierarchy consists of 16 major categories, each of which contains sev-
eral subcategories. To perform cross-validated testing, we obtained classiﬁcation problems
from each of the top-level categories by retrieving webpages from each of their respec-
tive subcategories. For the 20 Newsgroups, Reuters-21578, and Industry Sector datasets,
we performed similar preprocessing.9 Given a dataset of documents, we sampled 10-class
2-training-examples-per-class classiﬁcation problems by randomly selecting 10 different
classes within the dataset, picking 2 training examples within each class, and choosing one
test example from a randomly chosen class.

4.1 Choice of features
Theoretically, for the method described in this paper, any sufﬁciently rich set of features
could be used to learn a parameter function for classiﬁcation. For simplicity, we reduced
the feature vector in (3) to the following two-dimensional representation,10
uki = (cid:20)log(proportion of wi among words from documents of class k)
(cid:21) :
log(proportion of documents containing wi )
Note that up to the log transformation, the components of uki correspond to the relative
term frequency and document frequency of a word relative to class k (see Figure 1).

(15)

4.2 Generalization performance
We tested our meta-learning algorithm on classiﬁcation problems taken from each of the
16 top-level dmoz categories. For each top-level category, we built a collection of 300
classiﬁcation problems from that category; results reported here are averages over these

9For the Reuters data, we associated each article with its hand-annotated “topic”label and dis-
carded any articles with more than one topic annotation. For each dataset, we discarded all categories
with fewer than 50 examples, and selected a 500-word vocabulary based on information gain.
10Features were rescaled to have zero mean and unit variance over the training set.

Table 2: Cross corpora classiﬁcation accuracy, using classiﬁers
corpora. The best accuracy in each row is shown in bold.

trained on each of the four

Dataset
gdmoz
gnews
gTFIDF
gNB
gindu
greut
0.471 0.475 0.473 0.365 0.352
dmoz
n/a
0.371 0.369 0.223 0.184
20 Newsgroups 0.369
n/a
0.619 0.463 0.475
Reuters-21578
n/a
0.567 0.567
Industry Sector 0.438 0.459 0.446
n/a
0.374 0.274

softmax 1VA-SVM MC-SVM
0.412
0.283
0.381
0.217
0.206
0.248
0.481
0.308
0.463
0.376
0.271
0.375

problems. To assess the accuracy of our meta-learning algorithm for a particular test cate-
gory, we used the g learned from a set of 450 classiﬁcation problems drawn from the other
15 top-level categories.11 This ensured no overlap of training and testing data. In 15 out
of 16 categories, the learned parameter function g outperforms naive Bayes and TFIDF in
addition to the discriminative methods we tested (softmax regression, 1-vs-all SVMs [12],
and multiclass SVMs [13]12 ; see Table 1).13
Next, we assessed the ability of g to transfer across even more dissimilar corpora. Here, for
each of the four corpora (dmoz, 20 Newsgroups, Reuters-21578, Industry Sector), we con-
structed independent training and testing datasets of 480 random classiﬁcation problems.
After training separate classiﬁers
(gdmoz , gnews , greut , and gindu ) using data from each of the
four corpora, we tested the performance of each learned classiﬁer on the remaining three
corpora (see Table 2). Again, the learned parameter functions compare favorably to the
other methods. Moreover, these tests show that a single parameter function may give an ac-
curate classiﬁcation algorithm for many different corpora, demonstrating the effectiveness
of our approach for achieving transfer across related learning tasks.
5 Discussion and Related Work
In this paper, we presented an algorithm based on softmax regression for learning a pa-
rameter function g from example classiﬁcation problems. Once learned, g deﬁnes a new
learning algorithm that can be applied to novel classiﬁcation tasks.
Another approach for learning g is to modify the multiclass support vector machine formu-
lation of Crammer and Singer [13] in a manner analagous to the modi ﬁcation of softmax
regression in Section 3.1, giving the following quadratic program:
1
2 jj(cid:12) jj2 + C Pj (cid:24)j
minimize
(cid:12)2Rn ;(cid:24)2Rm
(j )
sub ject to (cid:12)T Pi x(j )
(j )
ki ) (cid:21) Ifk 6=y(j )g (cid:0) (cid:24)j ; 8k , 8j :
i (u
y(j ) i (cid:0) u
As usual, taking the dual leads naturally to an SMO-like procedure for optimization. We
implemented this method and found that the learned g , like in the softmax formulation,
outperforms naive Bayes, TFIDF, and the other discriminative methods.
The techniques described in this paper give one approach for achieving inductive transfer in
classiﬁer design—using
labeled data from related example classiﬁcation problems to solve
a particular classiﬁcation problem [16, 17]. Bennett et al. [18] also consider the issue of
knowledge transfer in text classiﬁcation in the context of ensemble classiﬁers, and propose
a system for using related classiﬁcation problems to learn the reliability of individual clas-
siﬁers within the ensemble. Unlike their approach, which attempts to meta-learn properties

11For each execution of the learning algorithm, (C; (cid:13) ) parameters were determined via grid search
using a small holdout set of 160 classiﬁcation problems. The same holdout set was used to select
regularization parameters for the discriminative learning algorithms.
12We used LIBSVM [14] to assess 1VA-SVMs and SVM-Light [15] for multiclass SVMs.
13For larger values of m=K (e.g. m=K = 10), softmax and multiclass SVMs consistently out-
perform naive Bayes and TFIDF; nevertheless, the learned g achieves a performance on par with dis-
criminative methods, despite being constrained to parameters which are explicit functions of training
data statistics. This result is consistent with a previous study in which a heuristically hand-tuned
version of Naive Bayes attained near-SVM text classiﬁcation performance for large datasets [6].

of algorithms, our method uses meta-learning to construct a new classiﬁcation algorithm.
Though not directly applied to text classiﬁcation, Teevan and Karger [19] consider the
problem of automatically learning term distributions for use in information retrieval.
Finally, Thrun and O’Sullivan [20] consider the task of classiﬁcation in a mobile robot do-
main. In this work, the authors describe a task-clustering (TC) algorithm in which learning
tasks are grouped via a nearest neighbors algorithm, as a means of facilitating knowledge
transfer. A similar concept is implicit in the kernelized parameter function learned by our
algorithm, where the Gaussian kernel facilitates transfer between similar statistics vectors.
Acknowledgments
We thank David Vickrey and Pieter Abbeel for useful discussions, and the anonymous
referees for helpful comments. CBD was supported by an NDSEG fellowship. This work
was supported by DARPA under contract number FA8750-05-2-0249.

In

References
[1] K. Nigam, J. Lafferty, and A. McCallum. Using maximum entropy for text classiﬁcation.
IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61 –67, 1999.
[2] T. Joachims. Text categorization with support vector machines: Learning with many relevant
features. In Machine Learning: ECML-98, pages 137–142, 1998.
[3] A. McCallum and K. Nigam. A comparison of event models for Naive Bayes text classiﬁcation.
In AAAI-98 Workshop on Learning for Text Categorization, 1998.
[4] G. Salton and C. Buckley. Term weighting approaches in automatic text retrieval. Information
Processing and Management, 29(5):513–523, 1988.
[5] T. Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categoriza-
tion. In Proceedings of ICML-97, pages 143–151, 1997.
[6] J. D. Rennie, L. Shih, J. Teevan, and D. R. Karger. Tackling the poor assumptions of naive
Bayes text classiﬁers. In ICML, pages 616–623, 2003.
[7] A. Moffat and J. Zobel. Exploring the similarity space. In ACM SIGIR Forum 32, 1998.
[8] C. Manning and H. Schutze. Foundations of statistical natural language processing, 1999.
[9] A. Ng and M. Jordan. On discriminative vs. generative classiﬁers: a comparison of logistic
regression and naive Bayes. In NIPS 14, 2002.
[10] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. J. Math. Anal.
Appl., 33:82–95, 1971.
[11] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.
[12] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. J. Mach. Learn. Res., 5:101–
141, 2004.
[13] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based
vector machines. J. Mach. Learn. Res., 2:265–292, 2001.
[14] C-C. Chang and C-J. Lin. LIBSVM: a library for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm .
[15] T. Joachims. Making large-scale support vector machine learning practical. In Advances in
Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA, 1998.
[16] S. Thrun. Lifelong learning: A case study. CMU tech report CS-95-208, 1995.
[17] R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.
[18] P. N. Bennett, S. T. Dumais, and E. Horvitz.
Inductive transfer for text classiﬁcation using
generalized reliability indicators. In Proceedings of ICML Workshop on The Continuum from
Labeled to Unlabeled Data in Machine Learning and Data Mining, 2003.
[19] J. Teevan and D. R. Karger. Empirical development of an exponential probabilistic model for
text retrieval: Using textual analysis to build a better model. In SIGIR ’03, 2003.
[20] S. Thrun and J. O’Sullivan. Discovering structure in multiple learning tasks: The TC algorithm.
In International Conference on Machine Learning, pages 489–497, 1996.


Learning Rankings via Convex Hull Separation

Glenn Fung, R ´omer Rosales, Balaji Krishnapuram
Computer Aided Diagnosis, Siemens Medical Solutions USA, Malvern, PA 19355
{glenn.fung, romer.rosales, balaji.krishnapuram}@siemens.com

Abstract

We propose efﬁcient algorithms for learning ranking functi ons from or-
der constraints between sets —
i.e. classes —of training samples. Our al-
gorithms may be used for maximizing the generalized Wilcoxon Mann
Whitney statistic that accounts for the partial ordering of the classes: spe-
cial cases include maximizing the area under the ROC curve for binary
classi ﬁcation and its generalization for ordinal regressi on. Experiments
on public benchmarks indicate that: (a) the proposed algorithm is at least
as accurate as the current state-of-the-art; (b) computationally, it is sev-
eral orders of magnitude faster and—unlike current methods —i
t is easily
able to handle even large datasets with over 20,000 samples.

1

Introduction

Many machine learning applications depend on accurately ordering the elements of a set
based on the known ordering of only some of its elements. In the literature, variants of this
problem have been referred to as ordinal regression, ranking, and learning of preference
relations. Formally, we want to ﬁnd a function f : ℜn → ℜ such that, for a set of test
samples {xk ∈ ℜn }, the output of the function f (xk ) can be sorted to obtain a ranking. In
order to learn such a function we are provided with training data, A, containing S sets (or
i }mj
classes) of training samples: A = SS
j=1 (Aj = {xj
i=1 ), where the j -th set Aj contains
mj samples, so that we have a total of m = PS
j=1 mj samples in A. Further, we are also
provided with a directed order graph G = (S , E ) each of whose vertices corresponds to a
class Aj , and the existence of a directed edge EP Q —corresponding to AP → AQ —means
that all training samples xp ∈ AP should be ranked higher than any sample xq ∈ AQ : i.e.
∀ (xp∈ AP , xq∈ AQ ), f (xp ) ≤ f (xq ).

In general the number of constraints on the ranking function grows as O(m2 ) so that naive
solutions are computationally infeasible even for moderate sized training sets with a few
thousand samples. Hence, we propose a more stringent problem with a larger (inﬁnite) set
of constraints, that is nevertheless much more tractably solved. In particular, we modify
the constraints to: ∀ (xp ∈ CH (AP ), xq ∈ CH (AQ )), f (xp ) ≤ f (xq ), where CH (Aj )
denotes the set of all points in the convex hull of Aj .
We show how this leads to: (a) a family of approximations to the original problem; and (b)
considerably more efﬁcient solutions that still enforce al
l of the original inter-group order
constraints. Notice that, this formulation subsumes the standard ranking problem (e.g. [4])
as a special case when each set Aj is reduced to a singleton and the order graph is equal to

{v,w,x}

{v,w}

{v,w}

{v}

{w}

{v,w,x}

{v,w}

{x}

{y,z}

(a)

{x}

{y,z}

{x}

{y,z}

{y}

(b)

(c)

{x}

(d)

{z}

{y}

{z}

{y}

{z}

(e)

(f)

Figure 1: Various instances of the proposed ranking problem consistent with the training set
{v , w, x, y , z} satisfying v > w > x > y > z . Each problem instance is deﬁned by an order
graph. (a-d) A succession of order graphs with an increasing number of constraints (e-f) Two order
graphs deﬁning the same partial ordering but different problem instan ces.

a full graph. However, as illustrated in Figure 1, the formulation is more general and does
not require a total ordering of the sets of training samples Aj , i.e. it allows any order graph
G to be incorporated into the problem.

1.1 Generalized Wilcoxon-Mann-Whitney Statistics

A distinction is usually made between classi ﬁcation and ord inal regression methods on
one hand, and ranking on the other. In particular, the loss functions used for classi ﬁcation
and ordinal regression evaluate whether each test sample is correctly classi ﬁed: in other
words, the loss functions that are used to evaluate these algorithms —
e.g. the 0– 1 loss for
binary classi ﬁcation—are computed for every sample individ
ually, and then averaged over
the training or test set.

By contrast, bipartite ranking solutions are evaluated using the Wilcoxon-Mann-Whitney
(WMW) statistic which measures the (sample averaged) probability that any pair of sam-
ples is ordered correctly; intuitively, the WMW statistic may be interpreted as the area
under the ROC curve (AUC). We deﬁne a slight generalization of the WMW statistic t hat
accounts for our notion of class-ordering:
l=1 δ (cid:16)f (xi
l )(cid:17)
k=1 Pmj
Pmi
k ) < f (xj
W M W (f , A) = XEij
k=1 Pmj
Pmi
l=1 1
Hence, if a sample is individually misclassi ﬁed because it f alls on the wrong side of the
decision boundary between classes it incurs a penalty in ordinal regression, whereas, in
ranking, it may be possible that it is still correctly ordered with respect to every other test
sample, and thus it may incur no penalty in the WMW statistic.

.

1.2 Previous Work

Ordinal regression and methods for handling structured output classes: For a classic
description of generalized linear models for ordinal regression, see [11]. A non-parametric
Bayesian model for ordinal regression based on Gaussian processes (GP) was deﬁned
[1]. Several recent machine learning papers consider structured output classes: e.g. [13]
presents SVM based algorithms for handling structured and interdependent output spaces,
and [5] discusses automatic document categorization into pre-deﬁned hierarchies or tax-
onomies of topics.
Learning Rankings: The problem of learning rankings was ﬁrst treated as a classi
ﬁcation
problem on pairs of objects by Herbrich [4] and subsequently used on a web page ranking
task by Joachims [6]; a variety of authors have investigated this approach recently. The
major advantage of this approach is that it considers a more explicit notion of ordering—
However, the naive optimization strategy proposed there suffers from the O(m2 ) growth

in the number of constraints mentioned in the previous section. This computational bur-
den renders these methods impractical even for medium sized datasets with a few thousand
samples. In other related work, boosting methods have been proposed for learning prefer-
ences [3], and a combinatorial structure called the ranking poset was used for conditional
modeling of partially ranked data[8], in the context of combining ranked sets of web pages
produced by various web-page search engines. Another, less related, approach is [2].
Relationship to the proposed work: Our algorithm penalizes wrong ordering of pairs of
training instances in order to learn ranking functions (similar to [4]), but in addition, it can
also utilize the notion of a structured class order graph. Nevertheless, using a formula-
tion based on constraints over convex hulls of the training classes, our method avoids the
prohibitive computational complexity of the previous algorithms for ranking.

1.3 Notation and Background

In the following, vectors will be assumed to be column vectors unless transposed to a row
vector by a prime superscript ′ . For a vector x in the n-dimensional real space ℜn , the
cardinality of a set A will be denoted by #(A). The scalar (inner) product of two vectors x
and y in the n-dimensional real space ℜn will be denoted by x′ y and the 2-norm of x will
be denoted by kxk. For a matrix A ∈ ℜm×n , Ai is the ith row of A which is a row vector in
ℜn , while A·j is the j th column of A. A column vector of ones of arbitrary dimension will
be denoted by e. For A ∈ ℜm×n and B ∈ ℜn×k , the kernel K (A, B ) maps ℜm×n × ℜn×k
into ℜm×k . In particular, if x and y are column vectors in ℜn then, K (x′ , y) is a real
number, K (x′ , A′ ) is a row vector in ℜm and K (A, A′ ) is an m × m matrix. The identity
matrix of arbitrary dimension will be denoted by I .

2 Convex Hull formulation

We are interested in learning a ranking function f : ℜn → ℜ given known ranking rela-
tionships between some training instances Ai , Aj ⊂ A. Let the ranking relationships be
speci ﬁed by a set E = {(i, j )|Ai ≺ Aj }
To begin with, let us consider the linearly separable binary ranking case which is equivalent
to the problem of classifying m points in the n-dimensional real space ℜn , represented by
the m × n matrix A, according to membership of each point x = Ai in the class A+ or A−
as speci ﬁed by a given vector of labels d. In others words, for binary classi ﬁers, we want a
linear ranking function fw (x) = w ′x that satis ﬁes the following constraints:
∀ (x+∈ A+ , x−∈ A− ), f (x− ) ≤ f (x+ ) ⇒ f (x− )− f (x+ ) = w ′x−− w ′x+ ≤ −1 ≤ 0.
(1)

Clearly, the number of constraints grows as O(m+m− ), which is roughly quadratic in
the number of training samples (unless we have severe class imbalance). While easily
overcome–based on additional insights –in the separable pr
oblem, in the non-separable
case, the quadratic growth in the number of constraints poses huge computational burdens
on the optimization algorithm; indeed direct optimization with these constraints is infeasi-
ble even for moderate sized problems. We overcome this computational problem based on
three key insights that are explained below.
First, notice that (by negation) the feasibility constraints in (1) can also be deﬁned as:
∀ (x+∈ A+ , x−∈ A− ), w ′x−−w ′x+ ≤ −1 ⇔ ∄(x+∈ A+ , x−∈ A− ), w ′x−−w ′x+ > −1.
In other words, a solution w is feasible iff there exist no pair of samples from the two
classes such that fw ((cid:5)) orders them incorrectly.
Second, we will make the constraints in (1) more stringent: instead of requiring that equa-
(x+∈ A+ , x−∈ A− ) in the training set, we will
tion (1) be satis ﬁed for each possible pair

Figure 2: Example binary problem where points belonging to the A+ and A− sets are represented
by blue circles and red triangles respectively. Note that two elements xi and xj of the set A−
are not correctly ordered and hence generate positive values of the corresponding slack variables
yi and yj . Note that the point xk (hollow triangle) is in the convex hull of the set A− and hence
the corresponding yk error can be writen as a convex combination (yk = λk
j yj ) of the two
i yi + λk
nonzero errors corresponding to points of A−

λ−− w ′A+′

λ+ ≤ −1.(2)

∀(λ+ , λ− )

(x+∈ CH (A+ ), x−∈ CH (A− )), where CH (Ai )
require (1) to be satis ﬁed for each pair
denotes the convex hull of the set Ai [12]. Thus, our constraints become:
such that (cid:26) 0 ≤ λ+ ≤ 1, P λ+ = 1
0 ≤ λ− ≤ 1, P λ− = 1 (cid:27) , w ′A−′
Next, notice that all the linear inequality and equality constraints on (λ+ , λ− ) may be
conveniently grouped together as Bλ ≤ b, where,
b+ = " 0+
b− = " 0−
#
λ = (cid:20) λ−
m+×1
m−×1
λ+ (cid:21)m×1
1
1
−1
−1
(m++2)×1
B+ = " 0 −Im+
B− = " −Im−
0
0 −e′ #
0 #
e′
e′
0
0
−e′
Thus, our constraints on w can be written as:
∀λ s.t. Bλ ≤ b, w ′A−′
λ−− w ′A+′
λ+ ≤ −1
λ−− w ′A+′
⇔ ∄λ s.t. Bλ ≤ b, w ′A−′
λ+ > −1
⇔ ∃u s.t. B ′u− w ′ [A−′
− A+′
] = 0, b′u ≤ −1, u ≥ 0,
Where the second equivalent form of the constraints was obtained by negation (as before),
and the third equivalent form results from our third key insight: the application of Farka’s
theorem of alternatives[9]. The resulting linear system of m equalities and m + 5 inequal-
ities in m + n + 4 variables can be used while minimizing any regularizer (such as kwk2 )
to obtain the linear ranking function that satis ﬁes (1); not
ice, however, that we avoid the
O(m2 ) scaling in constraints.

b = (cid:20) b+
b− (cid:21)
(m−+2)×1
(3)
B = (cid:20) B−
B+ (cid:21)(m+4)×m
(4)

(m++2)×m

(m−+2)×m

#

(5)

(6)

(7)

2.1 The binary non-separable case
In the non-separable case, CH (A+ ) T CH (A− ) 6= ∅ so the requirements have to be re-
laxed by introducing slack variables. To this end, we allow one slack variable yi ≥ 0
for each training sample xi , and consider the slack for any point inside the convex hull
CH (Aj ) to also be a convex combination of y (see Fig. 2). For example, this implies that

if only a subset of training samples have non-zero slacks yi> 0 (i.e. they are possibly mis-
classi ﬁed), then the slacks of any points inside the convex h ull also only depend on those
yi . Thus, our constraints now become:
∀λ s.t. Bλ ≤ b, w ′A−′
λ−− w ′A+′
λ+ ≤ −1 + (λ− y−+ λ+ y+ ), y+≥ 0, y−≥ 0.
Applying Farka’s theorem of alternatives, we get:
−A+w (cid:21) + (cid:20) y−
(2) ⇔ ∃u s.t. B ′u − (cid:20) A−w
y+ (cid:21) = 0, b′u ≤ −1, u ≥ 0
Replacing B from equation (4) and deﬁning u′ = [u−′
u+′
] ≥ 0 we get the constraints:
B+′
u+ + A+w + y+ = 0,
(10)
B−′
u− − A−w + y− = 0,
(11)
b+u+ + b−u− ≤ −1, u ≥ 0
(12)

(9)

(8)

2.2 The general ranking problem

Now we can extend the idea presented in the previous section for any given arbitrary di-
rected order graph G = (S , E ), as stated in the introduction, each of whose vertices corre-
sponds to a class Aj and the existence of a directed edge Eij means that all training samples
xi ∈ Ai should be ranked higher than any sample xj ∈ Aj , that is:
f (xj ) ≤ f (xi ) ⇒ f (xj ) − f (xi ) = w ′xj − w ′xi ≤ −1 ≤ 0
(13)
Analogously we obtain the following set of equations that enforced the ordering between
sets Ai and Aj :

B i′
uij + Aiw + y i = 0
(14)
B j ′
ˆuij − Aj w + y j = 0
(15)
biuij + bj ˆuij ≤ −1
(16)
uij , ˆuij ≥ 0
(17)
It can be shown that using the deﬁnitions of B i ,B j ,bi ,bj and the fact that uij , ˆuij ≥ 0,
equations (14) can be rewritten in the following way:
γ ij + Aiw + y i ≥ 0
(18)
ˆγ ij − Aj w + y j ≥ 0
(19)
γ ij + ˆγ ij ≤ −1
(20)
y i , y j ≥ 0
(21)
where γ ij = biuij and ˆγ ij = bj ˆuij . Note that enforcing the constraints deﬁned above
indeed implies the desired ordering, since we have:
Aiw + y i ≥ −γ ij ≥ ˆγ ij + 1 ≥ ˆγ ij ≥ Aj w − y j
It is also important to note the connection with Support Vector Machines (SVM) formu-
lation [10, 14] for the binary case. If we impose the extra constraints −γ ij = γ + 1 and
ˆγ ij = γ − 1, then equations (18) imply the constraints included in the standard primal SVM
formulation. To obtain a more general formulation,we can “k ernelize” equations (14) by
making a transformation of the variable w as: w = A′ v , where v can be interpreted as an
arbitrary variable in Rm ,This transformation can be motivated by duality theory [10], then
equations (14) become:

γ ij + AiA′ v + y i ≥ 0
ˆγ ij − Aj A′ v + y j ≥ 0
γ ij + ˆγ ij ≤ −1
y i , y j ≥ 0

(22)
(23)
(24)
(25)

If we now replace the linear kernels AiA′ and AiA′ by more general kernels K (Ai , A′ )
and K (Aj , A′ ) we obtain a “kernelized” version of equations (14)
γ ij + K (Ai , A′ )v + y i ≥ 0

Eij ≡ 
ˆγ ij − K (Aj , A′ )v + y j ≥ 0
γ ij + ˆγ ij
≤ −1


y i , y j
≥ 0
Given a graph G = (V , E ) representing the ordering of the training data and using equa-
tions (26) , we present next, a general mathematical programming formulation the ranking
problem:

(26)

ν ǫ(y) + R(v)

min
{v ,yi ,γ ij | (i,j )∈E }
s.t.
Eij ∀(i, j ) ∈ E
Where ǫ is a given loss function for the slack variables y i and R(v) represents a regularizer
on the normal to the hyperplane v . For an arbitrary kernel K (x, x′ ) the number of variables
of formulation (27) is 2 ∗ m + 2#(E ) and the number of linear equations(excluding the
nonnegativity constraints) is m#(E ) + #(E ) = #(E )(m + 1). for a linear kernel i.e.
K (x, x′ ) = xx′ the number of variables of formulation (27) becomes m + n + 2#(E )
and the number of linear equations remains the same. When using a linear kernel and
using ǫ(x) = R(x) = kxk2
2 , the optimization problem (27) becomes a linearly constrained
quadratic optimization problem for which a unique solution exists due to the convexity of
the objective function:

(27)

ν kyk2
2 + 1
2 w ′w
min
{w,yi ,γ ij | (i,j )∈E }
s.t.
Eij ∀(i, j ) ∈ E
Unlike other SVM-like methods for ranking that need a O(m2 ) number of slack variables
y our formulation only require one slack variable for example, only m slack variables
are used, giving our formulation computational advantage over ranking methods. Next,
we demonstrate the effectiveness of our algorithm by comparing it to two state-of-the-art
algorithms.

(28)

3 Experimental Evaluation

We test tested our approach in a set of nine publicly available datasets 1 shown in Tab. 1
(several large datasets are not reported since only the algorithm presented in this paper was
able to run them). These datasets have been frequently used as a benchmark for ordinal
regression methods (e.g. [1]). Here we use them for evaluating ranking performance. We
compare our method against SVM for ranking (e.g. [4, 6]) using the SVM-light package 2
and an efﬁcient Gaussian process method (the informative ve ctor machine) 3 [7].
These datasets were originally designed for regression, thus the continuous target values
for each dataset were discretized into ﬁve equal size bins. W e use these bins to deﬁne
our ranking constraints: all the datapoints with target value falling in the same bin were
grouped together. Each dataset was divided into 10% for testing and 90% for training.
Thus, the input to all of the algorithms tested was, for each point in the training set: (1) a
vector in ℜn (where n is different for each set) and (2) a value from 1 to 5 denoting the
rank of the group to which it belongs.

Performance is deﬁned in terms of the Wilcoxon statistic. Si nce we do not employ informa-
tion about the ranking of the elements within each group, order constraints within a group

1Available at http:\\www.liacc.up.pt\˜ltorgo\Regression\DataSets.html
2http:\\www.cs.cornell.edu\People\tj\svm light\
3http:\\www.dcs.shef.ac.uk\ neil\ivm\

Table 1: Benchmark Datasets
Name
n
m

Name

1 Abalone
2 Airplane Comp.
3 Auto-MPG
4 CA Housing
5 Housing-Boston

4177
950
392
20640
506

9
10
8
9
14

6 Machine-CPU
7 Pyrimidines
8 Triazines
9 WI Breast Cancer

m

209
74
186
194

n

7
28
61
33

Accuracy

Run time

103

102

101

100

10−1

10−2

)
e
l
a
c
s
−
g
o
L
(
 
e
m
i
t
 
n
u
R

SVM−light
IVM
Proposed (full−graph)
Proposed (chain−graph)

1

2

3

4
6
5
Dataset number

7

8

9

1

2

3

4
6
5
Dataset number

7

8

9

)
C
U
A
(
 
c
i
t
s
i
t
a
t
s
 
n
o
x
o
c
l
i
W
 
d
e
z
i
l
a
r
e
n
e
G

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Figure 3: Experimental comparison of the ranking SVM, IVM and the proposed method on nine
benchmark datasets. Along with the mean values in 10 fold cross-validation, the entire range of vari-
ation is indicated in the error-bars. (a) The overall accuracy for all the three methods is comparable.
(b) The proposed method has a much lower run time than the other methods, even for the full graph
case for medium to large size datasets. NOTE: Both SVM-light and IVM ran out of memory and
crashed on dataset 4; on dataset 1, SVM-light failed to complete even one fold after more than 24
hours of run time, so its results could not be compiled in time for submission.

cannot be veri ﬁed. Letting b(m) = m(m − 1)/2, the total number of order constraints is
equal to b(m) − Pi b(mi ), where mi is the number of instances in group i.
The results for all of the algorithms are shown in Fig.3. Our formulation was tested employ-
ing two order graphs, the full directed acyclic graph and the chain graph. The performance
for all datasets is generally comparable or signi ﬁcantly be tter for our algorithm (when us-
ing a chain order graph). Note that the performance for the full graph is consistently lower
than that for the chain graph. Thus, interestingly enforcing more order constraints does not
necessarily imply better performance. We suspect that this is due to the role that the slack
variables play in both formulations, since the number of slack variables remains the same
while the number of constraints increases. Adding more slack variables may positively
affect performance in the full graph, but this comes at a computational cost. An interesting
problem is to ﬁnd the right compromise. A different but poten tially related problem is that
of ﬁnding good order graph given a dataset. Note also that the chain graph is much more
stable regarding performance overall. Regarding run-time, our algorithm runs an order of
magnitude faster than current implementations of state-of-the-art methods, even approxi-
mate ones (like IVM).

4 Discussions and future work

We propose a general method for learning a ranking function from structured order con-
straints on sets of training samples. The proposed algorithm was illustrated on benchmark
ranking problems with two different constraint graphs: (a) a chain graph; and (b) a full

ordering graph. Although a chain graph was more accurate in the experiments shown in
Figure 3, with either type of graph structure, the proposed method is at least as accurate (in
terms of the WMW statistic for ordinal regression) as state-of-the-art algorithms such as
the ranking-SVM and Gaussian Processes for ordinal regression.

Besides being accurate, the computational requirements of our algorithm scale much more
favorably with the number of training samples as compared to other state-of-the-art meth-
ods. Indeed it was the only algorithm capable of handling several large datasets, while the
other methods either crashed due to lack of memory or ran for so long that they were not
practically feasible. While our experiments illustrate only speci ﬁc order graphs, we stress
that the method is general enough to handle arbitrary constraint relationships.

While the proposed formulation reduces the computational complexity of enforcing or-
der constraints, it is entirely independent of the regularizer that is minimized (under these
constraints) while learning the optimal ranking function. Though we have used a simple
margin regularization (via kwk2 in (28), and RKHS regularization in (27) in order to learn
in a supervised setting, we can just as easily easily use a graph-Laplacian based regular-
izer that exploits unlabeled data, in order to learn in semi-supervised settings. We plan to
explore this in future work.

References

[1] W. Chu and Z. Ghahramani, Gaussian processes for ordinal regression, Tech. report, University
College London, 2004.
[2] K. Crammer and Y. Singer, Pranking with ranking, Neural Info. Proc. Systems, 2002.
[3] Y. Freund, R. Iyer, and R. Schapire, An efﬁcient boosting algorithm for combining preferences ,
Journal of Machine Learning Research 4 (2003), 933–969.
[4] R. Herbrich, T. Graepel, and K. Obermayer, Large margin rank boundaries for ordinal regres-
sion, Advances in Large Margin Classiﬁers (2000), 115–132.
[5] T. Hofmann, L. Cai, and M. Ciaramita, Learning with taxonomies: Classifying documents and
words, (NIPS) Workshop on Syntax, Semantics, and Statistics, 2003.
[6] T. Joachims, Optimizing search engines using clickthrough data, Proc. ACM Conference on
Knowledge Discovery and Data Mining (KDD), 2002.
[7] N. Lawrence, M. Seeger, and R. Herbrich, Fast sparse gaussian process methods: The informa-
tive vector machine, Neural Info. Proc. Systems, 2002.
[8] G. Lebanon and J. Lafferty, Conditional models on the ranking poset, Neural Info. Proc. Sys-
tems, 2002.
[9] O. L. Mangasarian, Nonlinear programming, McGraw–Hill, New York, 1969, Reprint: SIAM
Classic in Applied Mathematics 10, 1994, Philadelphia.
, Generalized support vector machines, Advances in Large Margin Classiﬁers, 2000,
pp. 135–146.
[11] P. McCullagh and J. Nelder, Generalized linear models, Chapman & Hall, 1983.
[12] R. T. Rockafellar, Convex analysis, Princeton University Press, Princeton, New Jersey, 1970.
[13] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, Support vector machine learning for
interdependent and structured output spaces, Int.Conf. on Machine Learning, 2004.
[14] V. N. Vapnik, The nature of statistical learning theory, second ed., Springer, New York, 2000.

[10]


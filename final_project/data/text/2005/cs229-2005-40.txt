CS229 REPORT, DECEMBER 2005

1

Statistical clustering and Mineral Spectral Unmixing
in Aviris Hyperspectral Image of Cuprite, NV
Mario Parente, Argyris Zymnis

I . IN TRODUC T ION

Hyperspectral Imaging is a technique for obtaining a spec-
trum in each position of a large array of spatial positions
so that a recognizable image is obtained at each of a set of
discrete wavelengths. The images might be of a rock in the
laboratory, a ﬁeld study site from an aircraft or a rover camera,
or a whole planet from a spacecraft or Earth-based telescope.
By analyzing the spectral features (generally neighborhoods
of local minima in the spectra) one can map materials. A
simplistic explanation for this being is that speciﬁc chemical
bonds in different materials manifest themselves as absorption
features at different wavelengths and by mapping where those
bonds occur in the spectra one can uniquely identify what
is called the unique spectral signature of the material . The
factors affecting spectra of natural materials and the causes
of absorption features are several and combine in complex
ways. They are not the focus of this paper but a comprehensive
tutorial can be found in [3]. Spectral unmixing is the procedure
by which the measured spectrum of a pixel is decomposed
into a collection of constituent spectra, or endmembers, and a
set of corresponding fractions, or abundances, that indicate the
proportion of each endmember present in the pixel. In the case
of rocks or soils the endmembers can be consistent with the
minerals present in the geologic surface observed. In this work
we present a novel technique of endmember selection from
a database of minerals based on simple convex optimization
techniques.
Spectral unmixing can assume linear or nonlinear combi-
nation of the endmembers depending on the nature of the
surface observed [9]. Unfortunately nonlinear schemes can be
impractical for hyperspectral imaging because multiple views
from different angles of the same scene are required [11].
Linear Spectral Unmixing is based on the assumption
that the spectrum of each pixel of the scene is a convex
combination of the spectra of its component minerals [13].
Deterministic modeling of the mixture lacks the ability to
explain the statistical variability of the spectra within a class
due for example to illumination differences, altimetry, grain
size of the material and other causes. Several attempts have
been made to correct this problem: these approaches allow the
endmembers of the mixture to be random variables (mostly
Gaussians) [6], [14].
The present work assumes a mixture of multidimensional
pdf ’s for the statistical distribution of the spectra of the
single pixels composing the scene. Each pdf (a multinomial
Gaussian) represents the likelihood of a certain mineral mix-
ture in the scene. We make use of the Gauss Mixture Vector

Quantization algorithm [1], [7] as an alternative to the EM
algorithm [5]to learn the mixture parameters. We also explore
cluster analysis with correlation distance [8].
The clustering stage is useful to select only a few centroids
each to be representative of the image and to perform only
on them the unmixing as opposed to the whole image. This
decreases the computational cost of the processing, especially
when a large number of images is acquired.
Each gaussian mean produced by the clustering stage,
thought to be representative of a speciﬁc mineral mixture, is
unmixed in this work by a constrained least square algorithm
which can be cast as a quadratic program.

I I . AV IR I S CU PR I TE DATA S ET
Spectral data collected over Cuprite, Nevada, USA, have
been widely used to evaluate remote sensing technology and
for spectral unmixing [4], [15] and references therein.
For the purpose of our study 50 out of the Aviris bands (172
- 221, or 1.99 to 2.48 µm) have been selected because of the
better discrimination of mineral signatures in that range.
In ﬁgure 1(b) there is a plot of the spectra that represent
the spectral variability in the image. Those were obtained by
considering the image as a 50-dimensional data cloud and
capturing its corners.

I I I . C LU S TER ING
Our goal in clustering is to pick up as much as possible of
the spectral variability in the image. We tried some statistical
measures of cluster validation such as the Gap statistics. By
polling several experts in mineral identiﬁcation we assessed
that the statistical optimal number of clusters is too low to
capture the variability in the data that those experts would
consider if they scanned the pixel spectra.
Since this is a completely unsupervised classiﬁcation and
for the reasons just mentioned we consider capturing the most
spectra of ﬁgure 1(b) as a reasonable goal for the clustering
stage and to select the number of clusters.
We experimented with 3 different setups: K-means, GMVQ
and cluster analysis with the correlation measure.
We try for each k (number of clusters) 5 runs with random
initial point and choose the run with minimum value for
objective function (distortion or distance).

A. Lloyd Clustering Algorithm for Gauss Mixture design
The Gauss Mixture Vector Quantization algorithm can be
seen as an alternative to the EM algorithm for ﬁtting a ﬁnite

CS229 REPORT, DECEMBER 2005

2

(a) RGB composite of bands 183, 193 and 207

(b) spectral variability

Fig. 1. Aviris hyperspectral image of Cuprite, NV

+

αc+1 (xi ) = arg min
m

Gauss mixture {pm , gm} to a training set {x1 , x2 , . . . , xN }
(see [1] section 4 and [10].
The design of a Gauss mixture implemented by GMVQ is
as follows [1]:
Minimum Distortion (Nearest Neighbor) Step: For i =
1, . . . , N encode each training vector xi
into the index
αc+1 (xi ) corresponding to the minimum distortion Gaussian
(cid:19)
(cid:18)
model fm (xi |θc
m ), that is [1],
− ln fm (xi |θc
m ) + λ ln
1
ln |K c
m | +
= arg min
(
2
m
1
m ) − λ ln pc
m )−1 (x − µc
(x − µc
m )T (K c
m ).
2
m are the current estimates of the mean and
m and K c
where µc
covariance of the distribution of the samples.
the vectors {x1 , x2 , . . . , xNm }
Centroid Step: Given all
belonging to the m-th partition determine a gaussian density
(cid:20) 1
(cid:21)
NmX
m ) so as to minimize
m ) fm (xi θc
m and K c
(i.e. its parameters µc
1
m )−1 (xi − µc
m | +
ln |K c
(xi − µc
m )
m )T (K c
2
2
i=1
The minimization can be performed by alternatively ﬁxing µc
m
or K c
m and minimizing over the other one [1]. The solutions
NmX
are the sample mean in the m-th partition
m =
µc+1
i=1
NmX
and the sample covariance in the m-th partition
i=1
The algorithm includes one more step to calculate the
length function ln 1
,given the partition and the
optimal
pc
m
centroids [1].
We proposed a variation on the GMVQ algorithm that
discards the penalty term λ ln 1
in the distortion measure (
pc
m

m )(xi − µc
(xi − µc
m )T

m =
K c+1

(4)

(5)

1
Nm

xi

1
Nm

1
pc
m

(1)

(2)

(3)

by considering λ = 0 ) because we observed from simulations
that algorithms that use measures of membership probability
of each cluster ( like the prior for each cluster in EM and
optimal length function for GMVQ) penalize too much the
clusters with fewer assigned sample.
1) Euclidean Distance:
the covariance term
If we set
Km , ∀m equal to the identity, the distortion measure becomes
the Euclidean distance and the GMVQ Algorithm reduces to
the well known K-means (or vanilla Vector Quantization).
The Euclidean distance is invariant under orthogonal trans-
formation of the data but it does not take the correlation of
the variables into account.
The results in ﬁgure 2 (left) show that the euclidean distor-
tion is able to pick up the relatively very big (in norm) and very
small clusters in ﬁgure 1(b) which stand out as solitary but
only a few of the spectra in the central range are distinguished.
The reason being that those spectra differ mostly in shape.
We also found that the segmentation map was a little bit too
fragmented.
2) Mahalanobis Distance: If the only constraint is λ =
0 then the distance measure is similar to the Mahalanobis
distance (with an additional term that considers the volume
of the gaussian cluster).
The classiﬁer becomes quadratic. The increased ﬂexibility
of the boundary is traded off by the decreasing of the between-
cluster variance/within-cluster variance ratio.
From ﬁgure 3 (left) we see results similar to the previous
case. The reason might rely on the fact that the covariances
of the clusters are very similar and we know that in case
the classiﬁcation boundary is linear. We also notice that the
GMVQ estimation for the mean is a simple average like
in k-means. The fragmentation of the segmentation map is
somewhat improved by the use of the second order statistics.

B. Correlation-based Cluster Analysis
The correlation-based distance between a pixel xi and a
centroid µ is:
dc (xi , µ) = 1 − ρ(xi , µ) = 1 − (xi − ¯xi1)T (µ − ¯µ1)
kxi − ¯xi1k2 kµ − ¯µ1k2

,

CS229 REPORT, DECEMBER 2005

3

Fig. 2. Cluster centroids (left) and Cluster Map (right) for Euclidean Distance.

Fig. 3. Cluster centroids (left) and Cluster Map (right) for Mahalanobis Distance.

where ¯xi , ¯µ are the vector means of xi and µ respectively.
This is a measure that is invariant to scaling and shifting
(vertically) of the expression values. We tried this setup to take
into account the shape of the spectra considered as signals.
The drawback is that the actual magnitudes of the spectra are
ignored.
If the inputs are standardized, then the above distance is
equivalent to the Euclidean distance. Another drawback of the
distance is that dc (xi , µ) = 0 only implies linear relationship
between xi and µ.Furthermore the centroids are not obvious
to interpret.
We actually obtained the best results with the correlation-
based distance, as we can see from ﬁgure 4 (left). The obvoius
limitation of the measure is that the high norm cluster in ﬁgure
1(b) is misplaced because the measure is normalized. We on
the other hand get almost the full variability in the data. In
a development of this project we will explore a clustering
algorithm based on Shape and Gain Vector Quantization
that takes into account correlation (shape) and norm (gain)

simultaneously. The segmentation map seems less fragmented.

v2

. . .

vn

IV. M IN ERA L ID EN T I FICAT ION AND UNM IX ING
We assume that a dictionary of mineral spectra is available
to us. For this particular dataset, we extracted the dictionary
D = (cid:2) v1
(cid:3) ,
from [3] and [14]. Suppose that the mineral dictionary is given
in D
where vi for i = 1, . . . , n are the individual mineral spectra.
We want to ﬁnd the abundances a(j ) , such that, for each cluster
µj ≈ nX
centroid (mean) µj , we have:
i=1
in a least squares sense.
Since it is unreasonable to assume that a given spectrum
is the linear combination of a large number of dictionary

a(j )
i vi = Da(j )

(6)

22.052.12.152.22.252.32.352.42.450.150.20.250.30.350.40.45WavelengthReflectance  22.052.12.152.22.252.32.352.42.450.150.20.250.30.350.40.45WavelengthReflectance  CS229 REPORT, DECEMBER 2005

4

Fig. 4. Cluster Means (left) and Cluster Map (right) for Correlation Distance.

spectra, we want to impose a limit on the number of non-
zero abundance coefﬁcients. We can view this problem as
selecting a small number of regressors out of a given set, in
order to approximate (in a least-squares sense) a given vector.
Speciﬁcally we would like to solve the following problem, for
each cluster centroid:

kDa(j ) − µj k2
minimize
subject to Card(a(j ) ) ≤ r
a(j ) ≥ 0

(7)

Here Card(a(j ) ) denotes the cardinality of a(j ) ,
i.e.
the
number of nonzero elements in a(j ) , or in other words the
sparsity structure of the abundance vector.
For our particular problem, the dictionary contains n = 117
minerals. We would like to express each centroid as a linear
combination of approximately r = 5 of those minerals.
Problem 7 reduces to a quadratic program (QP) if the
cardinality constraint is removed. However, with this constraint
present, it turns out that this problem is combinatorial and is
thus very hard to solve. Speciﬁcally, if we wanted to ﬁnd the
global optimum of 7 we would have to solve n!/r !(n − r)!
quadratic programs. Each of these QPs would correspond to a
different sparsity structure in the abundance vector. Obviously
solving such a number of problems is intractable, even for a
modest value of r .
There exist, however, efﬁcient heuristics for ﬁnding approx-
imate solutions to this problem. As explained in [2] section
6.3.2, one method that works satisfactorily is to ﬁrst solve the
following problem, for a range of values of λ:
minimize kDa(j ) − µj k2 + λka(j ) k1

(8)

By increasing the value of λ, we are in essence putting more
weight on minimizing the l-1 norm of the abundance vector
a(j ) . This causes the solution of 8 to be sparser. We can then
use the sparsity pattern given by this problem to solve the
original problem.

(9)

It turns out that problem 8 is equivalent to the following
problem, for an appropriate choice of :
kDa(j ) − µj k2
minimize
subject to ka(j ) k1 ≤ 
This problem can be expressed as a QP with a simple
transformation in the variables.
The parameter  puts a limit on the maximum allowable l-1
norm of a(j ) . In particular, if we choose  to be large, then
the problem essentially becomes unconstrained. On the other
hand, if  is less than the l-1 norm of the optimal solution of
the unconstrained least-squares problem, then the constraint
in 9 will be tight. In other words if we choose a small ,
then we can be certain that the solution a(j )∗
of 9 will have
ka(j )∗ k1 = . Thus, since an l-1 norm constraint on a(j ) will
change its sparsity structure, we can change  until we get the
desired cardinality on the solution a(j )∗ .
Now suppose we obtain an acceptable solution a(j )∗
to
9. We then construct the matrix ˜D , which consists of the
columns of D which correspond to non-zero entries in a(j )∗ . We
then proceed to solve the following problem for each cluster
centroid:
k ˜D˜a(j ) − µj k2
minimize
˜a(j ) ≥ 0
subject to
Thus, the solution to 10, for a centroid µj will give us the
abundances (weights) for that given cluster corresponding to
equation 6. In order to express these in terms of percentages,
we then have to normalize the vector a(j ) .

(10)

V. UNM IX ING R E SU LT S
The results of ﬁgure 5 show the estimated abundance
maps for three minerals, whose presence in this region is
unanimously agreed on by experts (i.e. [4]). The maximum
in the scale is 60% (dark red). We found reference for quan-
titative data for mineral abundances for this dataset, namely
[15]. For the most common minerals our results our method
produced abundance maps which are qualitatively similar to

22.052.12.152.22.252.32.352.42.450.150.20.250.30.35WavelengthReflectance  CS229 REPORT, DECEMBER 2005

5

those obtained used there. The values of the abundances are
in broad accordance but we don’t have a deﬁnitive answer of
what are the most accurate for the lack of ground truth data
on mineral abundances.

V I . CONC LU S ION AND FU TUR E WORK
In this work we explored clustering techniques on a well-
known hyperspectral image. We assessed that cluster analysis
with use of the correlation distortion measure is a technique
that picks up most of the variability in the dataset. Despite the
lack of quantitative reference data for mineral abundances for
this dataset, our results were qualitatively in accordance with
other studies.
In future studies we will devise reliable performance mea-
sures for cluster validation and mineral unmixing. We will also
explore the clustering performance of an algorithm that takes
into account both shape and norm as an improvement of our
clustering stage.

R E F ER ENC E S
[1] A. Aiyer, K. Pyun, Y. Huang, D. O’Brien and R.M. Gray, Lloyd Clustering
of Gauss Mixture Models for Image Compression and Classiﬁcation, in
Image Communication, Vol 20, pp. 459-485 (2005).
[2] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge Univer-
sity Press, (2004).
[3] R.N. Clark, Spectroscopy of Rocks and Minerals, and Principles of
Spectroscopy, in Manual of Remote Sensing, John Wiley and Sons, A.
Rencz Editor, New York, (1999).
[4] R. N. Clark, G. A. Swayze, K. E. Livo, R. F. Kokaly, S. J. Sutley, J. B.
Dalton, R. R.McDougal, and C. A. Gent, Imaging Spectroscopy: Earth
and Planetary Remote Sensing with the USGS Tetracorder and Expert
Systems, Journal of Geophysical Research, Vol. 108, No. E12, , p. 5-1-
44, (December 2003).
[5] A.P. Dempster, N.M. Laird and D.B. Rubin, Maximum-likelihood from
incomplete data via the EM algorithm, Journal of the Royal Statistical
Society, Ser. B, 39, (1977).
[6] M.T. Eissmann and R.C. Hardle, Stochastic spectral unmixing with
enhanced endmember class separation, Applied Optics, Vol.43, No. 36,
(December 2004).
[7] R.M. Gray, Gauss Mixture Vector Quantization, Proceedings of IEEE
International Cnference on Acoustics, Speech and Signal Processing,
(May 2001).
[8] T. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical
Learning, Springer (2001)
[9] N. Keshava and J.F. Mustard, Spectral unmixing, IEEE Signal Processing
Magazine, (January 2002).
[10] M. Parente, An investigation of
the Properties of Expectation-
Maximization and Gauss Mixture Vector Quantization in Density Esti-
mation and Clustering, EE391 Report, Stanford University, (September
2004).
[11] M. Petrou, Mixed pixel classiﬁcation: an overview, submitted to World
Scientiﬁc, (1998).
[12] R. Redner and H. Walker, Mixture densities, maximum likelihood and
the EM algorithm, SIAM Review, 26(2), pages 195-239, (April 1984).
[13] J.J Settle and N.A. Drake, Linear mixing and the estimation of ground
cover proportions, International Journal of Remote Sensing, 14, pp.1159-
1177, (1993).
[14] D. Stein, Application of the Normal Compositional Model to the analysis
of Hyperspectral Imagery, IEEE, (2004).
[15] D.W. Stein, The Normal Compositional Model with Applications to
Hyperspectral Image Analysis, MIT Lincoln Laboratory, Project Report
NGA-8 , (March 2005).

Fig. 5. Alunite HS295.3B (left), Kaolinite CM9 (middle) and Muscovite
GDS113 (right) abundance maps.

  102030405060  102030405060  102030405060
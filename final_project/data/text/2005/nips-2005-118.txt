Nested sampling for Potts models

Iain Murray
Gatsby Computational Neuroscience Unit
University College London
i.murray@gatsby.ucl.ac.uk

David J.C. MacKay
Cavendish Laboratory
University of Cambridge
mackay@mrao.cam.ac.uk

Zoubin Ghahramani
Gatsby Computational Neuroscience Unit
University College London
zoubin@gatsby.ucl.ac.uk

John Skilling
Maximum Entropy
Data Consultants Ltd.
skilling@eircom.net

Abstract

Nested sampling is a new Monte Carlo method by Skilling [1] in-
tended for general Bayesian computation. Nested sampling pro-
vides a robust alternative to annealing-based methods for comput-
ing normalizing constants. It can also generate estimates of other
quantities such as posterior expectations. The key technical re-
quirement is an ability to draw samples uniformly from the prior
sub ject to a constraint on the likelihood. We provide a demonstra-
tion with the Potts model, an undirected graphical model.

1 Introduction

(1)

L(θ)π(θ) dθ ,

The computation of normalizing constants plays an important role in statistical
Z
Z
inference. For example, Bayesian model comparison needs the evidence, or marginal
likelihood of a model M
p(D |θ , M)p(θ |M) dθ ≡
Z = p(D |M) =
where the model has prior π and likelihood L over parameters θ after observing data
D . This integral is usually intractable for models of interest. However, given its
importance in Bayesian model comparison, many approaches—both sampling-based
and deterministic—have been proposed for estimating it.
Often the evidence cannot be obtained using samples drawn from either the prior
π , or the posterior p(θ |D , M) ∝ L(θ)π(θ). Practical Monte Carlo methods need
to sample from a sequence of distributions, possibly at diﬀerent “temperatures”
p(θ |β ) ∝ L(θ)β π(θ) (see Gelman and Meng [2] for a review). These methods are
sometimes cited as a gold standard for comparison with other approximate tech-
niques, e.g. Beal and Ghahramani [3]. However, care is required in choosing inter-
mediate distributions; appropriate temperature-based distributions may be diﬃcult
or impossible to ﬁnd. Nested sampling provides an alternate standard, which makes
no use of temperature and does not require tuning of intermediate distributions or
other large sets of parameters.

Figure 1: (a) Elements of parame-
ter space (top) are sorted by likeli-
hood and arranged on the x-axis. An
eighth of the prior mass is inside the
innermost likelihood contour in this
ﬁgure.
(b) Point xi is drawn from
the prior inside the likelihood con-
tour deﬁned by xi−1 . Li is identiﬁed
and p({xi }) is known, but exact val-
ues of xi are not known. (c) With N
particles, the least likely one sets the
likelihood contour and is replaced by
a new point inside the contour ({Li }
and p({xi }) are still known).

L(θ)π(θ) dθ =

L(θ(x)) dx.

(2)

Z =

(a)
(b)
(c)
Nested sampling uses a natural deﬁnition of Z , a sum over prior mass. The
weighted sum over likelihood elements is expressed as the area under a monotonic
Z 1
Z
one-dimensional curve “L vs x” (ﬁgure 1(a)), where:
0
This is a change of variables dx(θ) = π(θ)dθ , where each volume element of the prior
in the original θ-vector space is mapped onto a scalar element on the one-dimensional
x-axis. The ordering of the elements on the x-axis is chosen to sort the prior mass
in decreasing order of likelihood values (x1 < x2 ⇒ L(θ(x1 )) > L(θ(x2 ))). See
appendix A for dealing with elements with identical likelihoods.
Given some points {(xi , Li )}I
i=1 ordered such that xi > xi+1 , the area under the
curve (2) is easily approximated. We denote by ˆZ estimates obtained using a
trapezoidal rule. Rectangle rules upper and lower bound the error ˆZ − Z .
Points with known x-coordinates are unavailable in general. Instead we generate
points, {θi }, such that the distribution p(x) is known (where x ≡ {xi }), and ﬁnd
their associated {Li }. A simple algorithm to draw I points is algorithm 1, see also
ﬁgure 1(b).
Algorithm 1
Initial point: draw θ1 ∼ π(θ).
for i = 2 to I: draw θi ∼ ˘π(θ |L(θi−1 )),
(cid:26)π(θ) L(θ) > L(θi−1 )
where
˘π(θ |L(θi−1 )) ∝
0
otherwise.
(3)

Algorithm 2
Initialize: draw N points θ(n) ∼ π(θ)
for i = 2 to I:
• m = argminn L(θ(n) )
• θi−1 = θ(m)
• draw θm ∼ ˘π(θ |L(θi−1 )), given
by equation (3)

We know p(x1 ) = Uniform(0, 1), because x is a cumulative sum of prior mass.
Similarly p(xi |xi−1 ) = Uniform(0, xi−1 ), as every point is drawn from the prior
sub ject to L(θi ) > L(θi−1 ) ⇒ xi < xi−1 . This recursive relation allows us to
compute p(x).
A simple generalization, algorithm 2, uses multiple θ particles; at each step the least
likely is replaced with a draw from a constrained prior (ﬁgure 1(c)). Now p(x1 |N ) =
and subsequent points have p(xi /xi−1 |xi−1 , N ) = N (xi /xi−1 )N −1 . This
N xN −1
1

θ1θ2x1121418L(x)θ1θ2x1L(x)x1x2x3θ1θ2x1L(x)x1Figure 2: The arithmetic and geometric means of xi against iteration number, i, for
algorithm 2 with N = 8. Error bars on the geometric mean show exp(−i/N ± √
i/N ).
Samples of p(x|N ) are superimposed (i = 1600 . . . 1800 omitted for clarity).
Z
distribution over x combined with observations {Li } gives a distribution over ˆZ :
p( ˆZ |{Li }, N ) ≈
δ( ˆZ (x) − ˆZ )p(x|N ) dx.
(4)
Samples from the posterior over θ are also available, see Skilling [1] for details.
Nested sampling was introduced by Skilling [1]. The key idea is that samples from
the prior, sub ject to a nested sequence of constraints (3), give a probabilistic re-
alization of the curve, ﬁgure 1(a). Related work can be found in McDonald and
Singer [4]. Explanatory notes and some code are available online1 . In this paper we
present some new discussion of important issues regarding the practical implemen-
tation of nested sampling and provide the ﬁrst application to a challenging problem.
This leads to the ﬁrst cluster-based method for Potts models with ﬁrst-order phase
transitions of which we are aware.

2 Implementation issues

2.1 MCMC approximations
The nested sampling algorithm assumes obtaining samples from ˘π(θ |L(θi−1 )), equa-
tion (3), is possible. Rejection sampling using π would slow down exponentially with
iteration number i. We explore approximate sampling from ˘π using Markov chain
Monte Carlo (MCMC) methods.
In high-dimensional problems it is likely that the ma jority of ˘π ’s mass is typically in
a thin shell at the contour surface [5, p37]. This suggests ﬁnding eﬃcient chains that
sample at constant likelihood, a microcanonical distribution. In order to complete
an ergodic MCMC method, we also need transition operators that can alter the
likelihood (within the constraint). A simple Metropolis method may suﬃce.
We must initialize the Markov chain for each new sample somewhere. One possibil-
ity is to start at the position of the deleted point, θi−1 , on the contour constraint,
which is independent of the other points and not far from the bulk of the required
uniform distribution. However, if the Markov chain mixes slowly amongst modes,
the new point starting at θi−1 may be trapped in an insigniﬁcant mode. In this
case it would be better to start at one of the other N − 1 existing points inside the
contour constraint. They are all draws from the correct distribution, ˘π(θ |L(θi−1 )),
so represent modes fairly. However, this method may also require many Markov
chain steps, this time to make the new point eﬀectively independent of the point it
cloned.
1 http://www.inference.phy.cam.ac.uk/bayesys/

1e-1201e-1001e-801e-601e-401e-2010200400600800100012001400160018002000xiihxiexp(hlogxi)errorbars1e-1151e-1101e-1051e-1001e-951e-901e-851e-801e-7515001550160016501700175018001850190019502000xiihxiexp(hlogxi)errorbarsFigure 3: Histograms of errors in the point estimate log( ˜Z )
over 1000 random experiments for diﬀerent approximations.
The test system was a 40-dimensional hypercube of length
100 with uniform prior centered on the origin. The log-
likelihood was L = −θ> θ/2. Nested sampling used N = 10,
I = 2000. (a) Monte Carlo estimation (equation (5)) using
S = 12 sampled tra jectories (b) S = 1200 sampled tra jec-
tories.
(c) Deterministic approximation using the geomet-
ric mean tra jectory. In this example perfect integration over
p(x|N ) gives a distribution of width ≈ 3 over log( ˆZ ). There-
fore, improvements over (c) for approximating equation (5)
are unwarranted.

(a)

(b)

(c)

log(Z ) ≈

(5)

log( ˆZ (x(s) ))

x(s) ∼ p(x|N ).

Integrating out x
2.2
To estimate quantities of interest, we average over p(x|N ), as in equation (4). The
Z
mean of a distribution over log( ˆZ ) can be found by simple Monte Carlo estimation:
SX
log( ˆZ (x))p(x|N ) dx ≈ 1
S
s=1
This scheme is easily implemented for any expectation under p(x|N ), including
error bars from the variance of log( ˆZ ). To reduce noise in comparisons between
runs it is advisable to reuse the same samples from p(x|N ) (e.g. clamp the seed
used to generate them).
A simple deterministic approximation is useful for understanding, and also pro-
exp(R p(xi |N ) log xi dxi ) = e−i/N , follows the path of typical settings of x. Us-
vides fast to compute,
low variance estimators. Figure 2 shows sampled tra-
jectories of xi as the algorithm progresses. The geometric mean path, xi ≈
ing this single x setting is a reasonable and very cheap alternative to averaging over
settings (equation 5); see ﬁgure 3.
Typically the trapezoidal estimate of the integral, ˆZ , is dominated by a small
number of trapezoids, around iteration i∗ say. Considering uncertainty on just
log xi∗ = −i∗ /N ± √
i∗ /N provides reasonable and convenient error bars.

3 Potts Models

(cid:18) X
(cid:19)
The Potts model, an undirected graphical model, deﬁnes a probability distribution
over discrete variables s = (s1 , . . . , sn ), each taking on one of q distinct “colors”:
1
J (δsi sj − 1)
P (s|J, q) =
exp
ZP (J, q)
(ij )∈E
The variables exist as nodes on a graph where (ij ) ∈ E means that nodes i and j
are linked by an edge. The Kronecker delta, δsi sj is one when si and sj are the
same color and zero otherwise. Neighboring nodes pay an “energy penalty” of J
when they are diﬀerent colors. Here we assume identical positive couplings J > 0
on each edge (section 4 discusses the extension to diﬀerent Jij ). The Ising model
and Boltzmann machine are both special cases of the Potts model with q = 2.
Our goal is to compute the normalization constant ZP (J, q), where the discrete
variables s are the θ variables that need to be integrated (i.e. summed) over.

(6)

.

−5050102030−5050100200300−5050501001502002503.1 Swendsen–Wang sampling

P (s, d) =

p ≡ (1 − e−J ).

We will take advantage of the “Fortuin-Kasteleyn-Swendsen-Wang” (FKSW) joint
distribution identiﬁed explicitly in Edwards and Sokal [6] over color variables s and
Y
(cid:3) ,
(cid:2)(1 − p)δdij ,0 + pδdij ,1 δsi ,sj
a bond variable for each edge in E , dij ∈ {0, 1}:
1
ZP (J, q)
(ij )∈E
The marginal distribution over s in the FKSW model is the Potts distribution,
equation (6). The marginal distribution over the bonds is the random cluster model
of Fortuin and Kasteleyn [7]:
1
1
pD (1 − p)|E |−D qC (d) =
P (d) =
ZP (J, q)
ZP (J, q)
dij = 1, and D = P
where C (d) is the number of connected components in a graph with edges wherever
(ij )∈E dij . As the partition functions of equations 6, 7 and 8 are
identical, we should consider using any of these distributions to compute ZP (J, q).
The algorithm of Swendsen and Wang [8] performs block Gibbs sampling on the
joint model by alternately sampling from P (dij |s) and P (s|dij ). This can convert
a sample from any of the three distributions into a sample from one of the others.

exp(D log(eJ − 1))e−J |E | qC (d) , (8)

(7)

3.2 Nested Sampling

(9)

qC (d) .

π(d) =

A simple approximate nested sampler uses a ﬁxed number of Gibbs sampling up-
dates of ˘π . Cluster-based updates are also desirable in these models. Focusing on
the random cluster model, we rewrite equation (8):
1
P (d) =
L(d)π(d) where
ZN
ZP (J, q)
1
exp(J |E |), L(d) = exp(D log(eJ − 1)),
ZN =
Zπ
Zπ
Likelihood thresholds are thresholds on the total number of bonds D. Many states
have identical D, which requires careful treatment, see appendix A. Nested sampling
on this system will give the ratio of ZP/Zπ . The prior normalization, Zπ , can be
found from the partition function of a Potts system at J = log(2).
The following steps give two MCMC operators to change the bonds d → d0 :
2. Count sites that allow bonds, E = P
1. Create a random coloring, s, uniformly from the qC (d) colorings satisfying
3. Either, operator 1: record the number of bonds D 0 = P
the bond constraints d, as in the Swendsen–Wang algorithm.
(ij )∈E δsi ,sj .
(cid:1).
Or, operator 2: draw D 0 from Q(D 0 |E (s)) ∝ (cid:0)E (s)
4. Throw away the old bonds, d, and pick uniformly from one of the (cid:0)E (s)
(cid:1)
(ij )∈E dij
D0
D0
ways of setting D 0 bonds in the E available sites.
The probability of proposing a particular coloring and new setting of the bonds is
(cid:1) Q(D 0 |E (s))
1(cid:0)E (s)
1
Q(s, d0 |d) = Q(d0 |s, D 0 )Q(D 0 |E (s))Q(s|d) =
qC (d)
D0
(cid:1)
s Q(D|s)/(cid:0)E (s)
P
P
Summing over colorings, the correct Metropolis-Hastings acceptance ratio is:
P
s Q(D 0 |s)/(cid:0)E (s)
(cid:1) = 1,
P
s Q(s, d|d0 )
a = π(d0 )
= qC (d0 )
· qC (d)
·
D
s Q(s, d0 |d)
qC (d0 )
π(d)
qC (d)
D0

(10)

.

(11)

Table 1: Partition function results for 16× 16 Potts systems (see text for details).

Method
Gibbs AIS
Swendsen–Wang AIS
Gibbs nested sampling
Random-cluster nested sampling
Acceptance ratio

q = 2 (Ising), J = 1
7.1 ± 1.1
7.4 ± 0.1
7.1 ± 1.0
7.1 ± 0.7
7.3

q = 10, J = 1.477
(1.5)
(1.2)
12.2 ± 2.4
14.1 ± 1.8
11.2

regardless of the choice in step 3. The simple ﬁrst choice solves the diﬃcult problem
of navigating at constant D. The second choice deﬁnes an ergodic chain2 .

4 Results

Table 1 shows results on two example systems: an Ising model, q = 1, and a q = 10
Potts model in an diﬃcult parameter regime. We tested nested samplers using
Gibbs sampling and the cluster-based algorithm, annealed importance sampling
(AIS) [9] using both Gibbs sampling and Swendsen–Wang cluster updates. We also
developed an acceptance ratio method [10] based on our representation in equation
(9), which we ran extensively and should give nearly correct results.
Annealed importance sampling (AIS) was run 100 times, with a geometric spacing
of 104 settings of J as the annealing schedule. Nested sampling used N = 100
particles and 100 full-system MCMC updates to approximate each draw from ˘π .
Each Markov chain was initialized at one of the N−1 particles satisfying the current
constraint.
In trials using the other alternative (section 2.1) the Gibbs nested
sampler could get stuck permanently in a local maximum of the likelihood, while
the cluster method gave erroneous answers for the Ising system.
AIS performed very well on the Ising system. We took advantage of its performance
in easy parameter regimes to compute Zπ for use in the cluster-based nested sam-
pler. However, with a “temperature-based” annealing schedule, AIS was unable to
give useful answers for the q = 10 system. While nested sampling appears to be
correct within its error bars.
It is known that even the eﬃcient Swendsen–Wang algorithm mixes slowly for Potts
models with q > 4 near critical values of J [11], see ﬁgure 4. Typical Potts model
states are either entirely disordered or ordered; disordered states contain a jumble
of small regions with diﬀerent colors (e.g. ﬁgure 4(b)), in ordered states the system
is predominantly one color (e.g. ﬁgure 4(d)). Moving between these two phases
is diﬃcult; deﬁning a valid MCMC method that moves between distinct phases
requires knowledge of the relative probability of the whole collections of states in
those phases.
Temperature-based annealing algorithms explore the model for a range of settings
of J and fail to capture the correct behavior near the transition. Despite using
closely related Markov chains to those used in AIS, nested sampling can work in all
parameter regimes. Figure 4(e) shows how nested sampling can explore a mixture
of ordered and disordered phases. By moving steadily through these states, nested
sampling is able to estimate the prior mass associated with each likelihood value.
2Proof: with ﬁnite probability all si are given the same color, then any allowable D 0 is
possible, in turn all allowable d0 have ﬁnite probability.

⇒

⇒

(a)

(b)

(c)

(d)

(e)

Figure 4: Two 256 × 256, q = 10 Potts models with starting states (a) and (c) were
simulated with 5 × 106 full-system Swendsen–Wang updates with J = 1.42577. The
corresponding results, (b) and (d) are typical of all the intermediate samples: Swendsen–
Wang is unable to take (a) into an ordered phase, or (c) into a disordered phase, although
both phases are typical at this J . (e) in contrast shows an intermediate state of nested
sampling, which succeeds in bridging the phases.

This behaviour is not possible in algorithms that use J as a control parameter.
The potentials on every edge of the Potts model in this paper were the same. Much
of the formalism above generalizes to allow diﬀerent edge weights Jij on each edge,
and non-zero biases on each variable. Indeed Edwards and Sokal [6] gave a general
procedure for constructing such auxiliary-variable joint distributions. This gener-
alization would make the model more relevant to MRFs used in other ﬁelds (e.g.
computer vision). The challenge for nested sampling remains the invention of eﬀec-
tive sampling schemes that keep a system at or near constant energy. Generalizing
step 4 in section 3.2 would be the diﬃcult step.
Other temperatureless Monte Carlo methods exist, e.g. Berg and Neuhaus [12] study
the Potts model using the multicanonical ensemble. Nested sampling has some
unique properties compared to the established method. Formally it has only one
free parameter, N the number of particles. Unless problems with multiple modes
demand otherwise, N = 1 often reveals useful information, and if the error bars on
Z are too large further runs with larger N may be performed.

5 Conclusions

We have applied nested sampling to compute the normalizing constant of a system
that is challenging for many Monte Carlo methods.
• Nested sampling’s key technical requirement, an ability to draw samples
uniformly from a constrained prior, is largely solved by eﬃcient MCMC
methods.
• No complex schedules are required; steady progress towards compact re-
gions of large likelihood is controlled by a single free parameter, N , the
number of particles.
• Multiple particles, a built-in feature of this algorithm, are often necessary
to obtain accurate results.
• Nested sampling has no special diﬃculties on systems with ﬁrst order phase-
transitions, whereas all temperature-based methods fail.
We believe that nested sampling’s unique properties will be found useful in a variety
of statistical applications.

A Degenerate likelihoods

The description in section 1 assumed that the likelihood function provides a total
ordering of elements of the parameter space. However, distinct elements dx and
dx0 could have the same likelihood, either because the parameters are discrete, or
because the likelihood is degenerate.
One way to break degeneracies is through a joint model with variables of interest θ
and an independent variable m ∈ [0, 1]:
Z L(θ)π(θ) · 1
1
P (θ , m) = P (θ) · P (m) =
L(m)π(m)
(12)
Zm
where L(m) = 1 + (m − 0.5), π(m) = 1 and Zm = 1. We choose  such that log()
is smaller than the smallest diﬀerence in log(L(θ)) allowed by machine precision.
Standard nested sampling is now possible. Assuming we have a likelihood constraint
(cid:26)π(θ 0 )π(m0 ) L(θ 0 )L(m0 ) > Li ,
Li , we need to be able to draw from
P (θ 0 , m0 |θ , m, Li ) ∝
0
otherwise.
The additional variable can be ignored except for L(θ 0 ) = L(θi ), then only m0 > m
are possible. Therefore, the probability of states with likelihood L(θi ) are weighted
by (1 − m0 ).

(13)

References

[1] John Skilling. Nested sampling.
In R. Fischer, R. Preuss, and U. von Toussaint,
editors, Bayesian inference and maximum entropy methods in science and engineering,
AIP Conference Proceeedings 735, pages 395–405, 2004.
[2] Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: from impor-
tance sampling to bridge sampling to path sampling. Statist. Sci., 13(2):163–185,
1998.
[3] Matthew J. Beal and Zoubin Ghahramani. The variational Bayesian EM algorithm
for incomplete data: with application to scoring graphical model structures. Bayesian
Statistics, 7:453–464, 2003.
[4] I. R. McDonald and K. Singer. Machine calculation of thermodynamic properties of
a simple ﬂuid at supercritical temperatures. J. Chem. Phys., 47(11):4766–4772, 1967.
[5] David J.C. MacKay. Information Theory, Inference, and Learning Algorithms. CUP,
2003. www.inference.phy.cam.ac.uk/mackay/itila/.
[6] Robert G. Edwards and Alan D. Sokal. Generalization of the Fortuin-Kasteleyn-
Swendsen-Wang representation and Monte Carlo algorithm. Phys.Rev. D, 38(6), 1988.
[7] C. M. Fortuin and P. W. Kasteleyn. On the random-cluster model. I. Introduction
and relation to other models. Physica, 57:536–564, 1972.
[8] R. H. Swendsen and J. S. Wang. Nonuniversal critical dynamics in Monte Carlo
simulations. Phys. Rev. Lett., 58(2):86–88, January 1987.
[9] Radford M. Neal. Annealed importance sampling. Statistics and Computing, 11:
125–139, 2001.
[10] Charles H. Bennett. Eﬃcient estimation of free energy diﬀerences from Monte Carlo
data. Journal of Computational Physics, 22(2):245–268, October 1976.
[11] Vivek K. Gore and Mark R. Jerrum. The Swendsen-Wang process does not always
mix rapidly. In 29th ACM Symposium on Theory of Computing, pages 674–681, 1997.
[12] Bernd A. Berg and Thomas Neuhaus. Multicanonical ensemble: A new approach to
simulate ﬁrst-order phase transitions. Phys. Rev. Lett., 68(1):9–12, January 1992.


In ﬁnite Latent Feature Models
and the Indian Buffet Process

Zoubin Ghahramani
Thomas L. Grif ﬁths
Cognitive and Linguistic Sciences Gatsby Computational Neuroscience Unit
University College London, London
Brown University, Providence RI
tom griffiths@brown.edu
zoubin@gatsby.ucl.ac.uk

Abstract

We deﬁne a probability distribution over equivalence class es of binary
matrices with a ﬁnite number of rows and an unbounded number o f
columns. This distribution is suitable for use as a prior in probabilistic
models that represent objects using a potentially inﬁnite a rray of features.
We identify a simple generative process that results in the same distribu-
tion over equivalence classes, which we call the Indian buffet process.
We illustrate the use of this distribution as a prior in an inﬁ nite latent fea-
ture model, deriving a Markov chain Monte Carlo algorithm for inference
in this model and applying the algorithm to an image dataset.

1

Introduction

The statistical models typically used in unsupervised learning draw upon a relatively small
repertoire of representations. The simplest representation, used in mixture models, asso-
ciates each object with a single latent class. This approach is appropriate when objects
can be partitioned into relatively homogeneous subsets. However, the properties of many
objects are better captured by representing each object using multiple latent features. For
instance, we could choose to represent each object as a binary vector, with entries indicat-
ing the presence or absence of each feature [1], allow each feature to take on a continuous
value, representing objects with points in a latent space [2], or deﬁne a factorial model, in
which each feature takes on one of a discrete set of values [3, 4].
A critical question in all of these approaches is the dimensionality of the representation:
how many classes or features are needed to express the latent structure expressed by a
set of objects. Often, determining the dimensionality of the representation is treated as a
model selection problem, with a particular dimensionality being chosen based upon some
measure of simplicity or generalization performance. This assumes that there is a single,
ﬁnite-dimensional representation that correctly charact erizes the properties of the observed
objects. An alternative is to assume that the true dimensionality is unbounded, and that the
observed objects manifest only a ﬁnite subset of classes or f eatures [5]. This alternative
is pursued in nonparametric Bayesian models, such as Dirichlet process mixture models
[6, 7, 8, 9]. In a Dirichlet process mixture model, each object is assigned to a latent class,
and each class is associated with a distribution over observable properties. The prior dis-
tribution over assignments of objects to classes is deﬁned i n such a way that the number
of classes used by the model is bounded only by the number of objects, making Dirichlet
process mixture models “inﬁnite” mixture models [10].
The prior distribution assumed in a Dirichlet process mixture model can be speci ﬁed in

terms of a sequential process called the Chinese restaurant process (CRP) [11, 12]. In the
CRP, N customers enter a restaurant with inﬁnitely many tables, ea ch with inﬁnite seating
capacity. The ith customer chooses an already-occupied table k with probability mk
i−1+α ,
where mk is the number of current occupants, and chooses a new table with probability
α
i−1+α . Customers are exchangeable under this process: the probability of a particular
seating arrangement depends only on the number of people at each table, and not the order
in which they enter the restaurant.
If we replace customers with objects and tables with classes, the CRP speci ﬁes a distribu-
tion over partitions of objects into classes. A partition is a division of the set of N objects
into subsets, where each object belongs to a single subset and the ordering of the subsets
does not matter. Two assignments of objects to classes that result in the same division of
objects correspond to the same partition. For example, if we had three objects, the class
assignments {c1 , c2 , c3 } = {1, 1, 2} would correspond to the same partition as {2, 2, 1},
since all that differs between these two cases is the labels of the classes. A partition thus
deﬁnes an equivalence class of assignment vectors.
The distribution over partitions implied by the CRP can be derived by taking the limit of
the probability of the corresponding equivalence class of assignment vectors in a model
where class assignments are generated from a multinomial distribution with a Dirichlet
prior [9, 10]. In this paper, we derive an inﬁnitely exchange able distribution over inﬁnite
binary matrices by pursuing this strategy of taking the limit of a ﬁnite model. We also de-
scribe a stochastic process (the Indian buffet process, akin to the CRP) which generates this
distribution. Finally, we demonstrate how this distribution can be used as a prior in statisti-
cal models in which each object is represented by a sparse subset of an unbounded number
of features. Further discussion of the properties of this distribution, some generalizations,
and additional experiments, are available in the longer version of this paper [13].

2 A distribution on in ﬁnite binary matrices

In a latent feature model, each object is represented by a vector of latent feature values f i ,
and the observable properties of that object xi are generated from a distribution determined
by its latent features. Latent feature values can be continuous, as in principal component
analysis (PCA) [2], or discrete, as in cooperative vector quantization (CVQ) [3, 4]. In the
remainder of this section, we will assume that feature values are continuous. Using the ma-
N (cid:3)T to indicate the latent feature values for all N objects, the model
trix F = (cid:2)f T
2 · · · f T
f T
1
is speci ﬁed by a prior over features, p(F), and a distribution over observed property ma-
trices conditioned on those features, p(X|F), where p(·) is a probability density function.
These distributions can be dealt with separately: p(F) speci ﬁes the number of features and
the distribution over values associated with each feature, while p(X|F) determines how
these features relate to the properties of objects. Our focus will be on p(F), showing how
such a prior can be deﬁned without limiting the number of feat ures.
We can break F into two components: a binary matrix Z indicating which features are pos-
sessed by each object, with zik = 1 if object i has feature k and 0 otherwise, and a matrix
V indicating the value of each feature for each object. F is the elementwise product of Z
and V, F = Z ⊗ V, as illustrated in Figure 1. In many latent feature models (e.g., PCA)
objects have non-zero values on every feature, and every entry of Z is 1. In sparse latent
feature models (e.g., sparse PCA [14, 15]) only a subset of features take on non-zero values
for each object, and Z picks out these subsets. A prior on F can be deﬁned by specifying
priors for Z and V, with p(F) = P (Z)p(V), where P (·) is a probability mass function.
We will focus on deﬁning a prior on Z, since the effective dimensionality of a latent feature
model is determined by Z. Assuming that Z is sparse, we can deﬁne a prior for inﬁnite la-
tent feature models by deﬁning a distribution over inﬁnite b
inary matrices. Our discussion
of the Chinese restaurant process provides two desiderata for such a distribution: objects

(a)

K features

(b)

K features

(c)

K features

s
t
c
e
j
b
o

N

s
t
c
e
j
b
o

N

0.9

1.4

0

−3.2

0

0.9

−0.3

0

0

0

0.2 −2.8

1.8

0

−0.1

4

0

0

0

3

4

3

0

1

0

s
t
c
e
j
b
o

N

1

5

0

2

5

Figure 1: A binary matrix Z, as shown in (a), indicates which features take non-zero values.
Elementwise multiplication of Z by a matrix V of continuous values produces a represen-
tation like (b). If V contains discrete values, we obtain a representation like (c).

should be exchangeable, and posterior inference should be tractable.
It also suggests a
method by which these desiderata can be satis ﬁed: start with a model that assumes a ﬁnite
number of features, and consider the limit as the number of features approaches inﬁnity.

2.1 A ﬁnite feature model

P (zik |πk ) =

πmk
k (1 − πk )N −mk ,

We have N objects and K features, and the possession of feature k by object i is indicated
by a binary variable zik . The zik form a binary N × K feature matrix, Z. Assume that
each object possesses feature k with probability πk , and that the features are generated
independently. Under this model, the probability of Z given π = {π1 , π2 , . . . , πK }, is
K
N
K
Yk=1
Yi=1
Yk=1
P (Z|π) =
where mk = PN
i=1 zik is the number of objects possessing feature k . We can deﬁne a prior
on π by assuming that each πk follows a beta distribution, to give
πk | α ∼ Beta( α
K , 1)
zik | πk ∼ Bernoulli(πk )
Each zik is independent of all other assignments, conditioned on πk , and the πk are gener-
ated independently. We can integrate out π to obtain the probability of Z, which is
K
α
K Γ(mk + α
K )Γ(N − mk + 1)
Yk=1
Γ(N + 1 + α
K )
This distribution is exchangeable, since mk is not affected by the ordering of the objects.

(1)

P (Z) =

.

(2)

2.2 Equivalence classes

tion 2 as K → ∞, we need to
In order to ﬁnd the limit of the distribution speci ﬁed by Equa
deﬁne equivalence classes of binary matrices – the analogue
of partitions for class assign-
ments. Our equivalence classes will be deﬁned with respect t o a function on binary matri-
ces, lof (·). This function maps binary matrices to left-ordered binary matrices. lof (Z) is
obtained by ordering the columns of the binary matrix Z from left to right by the magnitude
of the binary number expressed by that column, taking the ﬁrs t row as the most signi ﬁcant
bit. The left-ordering of a binary matrix is shown in Figure 2. In the ﬁrst row of the left-
ordered matrix, the columns for which z1k = 1 are grouped at the left. In the second row,
the columns for which z2k = 1 are grouped at the left of the sets for which z1k = 1. This
grouping structure persists throughout the matrix.
The history of feature k at object i is deﬁned to be (z1k , . . . , z(i−1)k ). Where no object is
speci ﬁed, we will use history to refer to the full history of feature k , (z1k , . . . , zN k ). We

lof

Figure 2: Left-ordered form. A binary matrix is transformed into a left-ordered binary
matrix by the function lof (·). The entries in the left-ordered matrix were generated from
the Indian buffet process with α = 10. Empty columns are omitted from both matrices.

will individuate the histories of features using the decimal equivalent of the binary numbers
corresponding to the column entries. For example, at object 3, features can have one of four
histories: 0, corresponding to a feature with no previous assignments, 1, being a feature for
which z2k = 1 but z1k = 0, 2, being a feature for which z1k = 1 but z2k = 0, and 3, being
a feature possessed by both previous objects were assigned. Kh will denote the number of
features possessing the history h, with K0 being the number of features for which mk = 0
and K+ = P2N
−1
h=1 Kh being the number of features for which mk > 0, so K = K0 + K+ .
Two binary matrices Y and Z are lof -equivalent if lof (Y) = lof (Z). The lof -
equivalence class of a binary matrix Z, denoted [Z], is the set of binary matrices that are
lof -equivalent to Z. lof -equivalence classes play the role for binary matrices that parti-
tions play for assignment vectors: they collapse together all binary matrices (assignment
vectors) that differ only in column ordering (class labels). lof -equivalence classes are pre-
served through permutation of the rows or the columns of a matrix, provided the same
permutations are applied to the other members of the equivalence class. Performing infer-
ence at the level of lof -equivalence classes is appropriate in models where feature order
is not identi ﬁable, with p(X|F) being unaffected by the order of the columns of F. Any
model in which the probability of X is speci ﬁed in terms of a linear function of F, such
as PCA or CVQ, has this property. The cardinality of the lof -equivalence class [Z] is
−1 (cid:17) =
(cid:16)
K !
, where Kh is the number of columns with full history h.
K
K0 ...K2N
Q2N
−1
h=0 Kh !
2.3 Taking the inﬁnite limit

P (Z) =

.

(3)

α
K Γ(mk + α
K )Γ(N − mk + 1)
Γ(N + 1 + α
K )

Under the distribution deﬁned by Equation 2, the probabilit y of a particular lof -equivalence
class of binary matrices, [Z], is
K
K !
Yk=1
P ([Z]) = XZ∈[Z]
Q2N −1
h=0 Kh !
Rearranging terms, and using the fact that Γ(x) = (x − 1)Γ(x − 1) for x > 1, we can
compute the limit of P ([Z]) as K approaches inﬁnity
K ) !K
K+
·  
αK+
N !
Yk=1
QN
Q2N −1
j=1 (j + α
h=1 Kh !
K+
αK+
Yk=1
=
·
1
·
exp{−αHN }
Q2N −1
h=1 Kh !
where HN is the N th harmonic number, HN = PN
1
j . This distribution is inﬁnitely
j=1
exchangeable, since neither Kh nor mk are affected by the ordering on objects. Technical
details of this limit are provided in [13].

(N − mk )! Qmk−1
j=1
N !
(N − mk )!(mk − 1)!
N !

(j + α
K )

·

K !
K0 ! K K+

lim
K→∞

,

(4)

·

·

2.4 The Indian buffet process

The probability distribution deﬁned in Equation 4 can be der ived from a simple stochastic
process. Due to the similarity to the Chinese restaurant process, we will also use a culinary
metaphor, appropriately adjusted for geography. Indian restaurants in London offer buffets
with an apparently inﬁnite number of dishes. We will deﬁne a d
istribution over inﬁnite
binary matrices by specifying how customers (objects) choose dishes (features).
In our Indian buffet process (IBP), N customers enter a restaurant one after another. Each
customer encounters a buffet consisting of inﬁnitely many d ishes arranged in a line. The
ﬁrst customer starts at the left of the buffet and takes a serv ing from each dish, stopping
after a Poisson(α) number of dishes. The ith customer moves along the buffet, sampling
dishes in proportion to their popularity, taking dish k with probability mk
i , where mk is the
number of previous customers who have sampled that dish. Having reached the end of all
previous sampled dishes, the ith customer then tries a Poisson( α
i ) number of new dishes.
We can indicate which customers chose which dishes using a binary matrix Z with N rows
and inﬁnitely many columns, where zik = 1 if the ith customer sampled the k th dish.
Using K (i)
to indicate the number of new dishes sampled by the ith customer, the proba-
1
bility of any particular matrix being produced by the IBP is
K+
αK+
Yk=1
i=1 K (i)
QN
1 !
The matrices produced by this process are generally not in left-ordered form. These ma-
trices are also not ordered arbitrarily, because the Poisson draws always result in choices
of new dishes that are to the right of the previously sampled dishes. Customers are not
exchangeable under this distribution, as the number of dishes counted as K (i)
depends
1
upon the order in which the customers make their choices. However, if we only pay at-
tention to the lof -equivalence classes of the matrices generated by this process, we obtain
i=1 K (i)
the inﬁnitely exchangeable distribution P ([Z]) given by Equation 4: QN
1 !
matrices
Q2N
−1
h=1 Kh !
generated via this process map to the same left-ordered form, and P ([Z]) is obtained by
multiplying P (Z) from Equation 5 by this quantity. A similar but slightly more compli-
cated process can be deﬁned to produce left-ordered matrice s directly [13].

(N − mk )!(mk − 1)!
N !

exp{−αHN }

P (Z) =

(5)

.

2.5 Conditional distributions

To deﬁne a Gibbs sampler for models using the IBP, we need to kn ow the conditional
distribution on feature assignments, P (zik = 1|Z−(ik) ). In the ﬁnite model, where P (Z)
is given by Equation 2, it is straightforward to compute this conditional distribution for any
zik . Integrating over πk gives

P (zik = 1|z−i,k ) =

m−i,k + α
K
N + α
K
where z−i,k is the set of assignments of other objects, not including i, for feature k , and
m−i,k is the number of objects possessing feature k , not including i. We need only condi-
tion on z−i,k rather than Z−(ik) because the columns of the matrix are independent.
In the inﬁnite case, we can derive the conditional distribut
ion from the (exchangeable) IBP.
Choosing an ordering on objects such that the ith object corresponds to the last customer
to visit the buffet, we obtain

(6)

,

m−i,k
N
for any k such that m−i,k > 0. The same result can be obtained by taking the limit of
Equation 6 as K → ∞. The number of new features associated with object i should be

P (zik = 1|z−i,k ) =

(7)

,

drawn from a Poisson( α
N ) distribution. This can also be derived from Equation 6, using the
same kind of limiting argument as that presented above.

3 A linear-Gaussian binary latent feature model

To illustrate how the IBP can be used as a prior in models for unsupervised learning, we
derived and tested a linear-Gaussian latent feature model in which the features are binary.
In this case the feature matrix F reduces to the binary matrix Z. As above, we will start
with a ﬁnite model and then consider the inﬁnite limit.
In our ﬁnite model, the D-dimensional vector of properties of an object i, xi is generated
from a Gaussian distribution with mean ziA and covariance matrix ΣX = σ2
I, where
X
zi is a K -dimensional binary vector, and A is a K × D matrix of weights.
In matrix
notation, E [X] = ZA. If Z is a feature matrix, this is a form of binary factor analysis. The
distribution of X given Z, A, and σX is matrix Gaussian with mean ZA and covariance
X I , where I is the identity matrix. The prior on A is also matrix Gaussian, with
matrix σ2
A I . Integrating out A, we have
mean 0 and covariance matrix σ2
1
A |ZT Z + σ2
σKD
X
σ2
A

(2π)N D/2σ (N −K )D
X

p(X|Z, σX , σA ) =

I|D/2

exp{−

tr(XT (I − Z(ZT Z +

σ2
1
X
2σ2
σ2
X
A
This result is intuitive: the exponentiated term is the difference between the inner product
of X and its projection onto the space spanned by Z, regularized to an extent determined
by the ratio of the variance of the noise in X to the variance of the prior on A. It follows
that p(X|Z, σX , σA ) depends only on the non-zero columns of Z, and thus remains well-
deﬁned when we take the limit as K → ∞ (for more details see [13]).
We can deﬁne a Gibbs sampler for this model by computing the fu ll conditional distribution

I)−1ZT )X)}. (8)

(9)

P (zik |X, Z−(i,k) , σX , σA ) ∝ p(X|Z, σX , σA )P (zik |z−i,k ).
The two terms on the right hand side can be evaluated using Equations 8 and 7 respectively.
The Gibbs sampler is then straightforward. Assignments for features for which m−i,k > 0
are drawn from the distribution speci ﬁed by Equation 9. The d istribution over the number
of new features for each object can be approximated by truncation, computing probabilities
for a range of values of K (i)
1 up to an upper bound. For each value, p(X|Z, σX , σA ) can
be computed from Equation 8, and the prior on the number of new features is Poisson( α
N ).
We will demonstrate this Gibbs sampler for the inﬁnite binar y linear-Gaussian model on a
dataset consisting of 100 240 × 320 pixel images. We represented each image, xi , using
a 100-dimensional vector corresponding to the weights of the mean image and the ﬁrst 99
principal components. Each image contained up to four everyday objects – a $20 bill, a
Klein bottle, a prehistoric handaxe, and a cellular phone. Each object constituted a single
latent feature responsible for the observed pixel values. The images were generated by
sampling a feature vector, zi , from a distribution under which each feature was present
with probability 0.5, and then taking a photograph containing the appropriate objects using
a LogiTech digital webcam. Sample images are shown in Figure 3 (a).
The Gibbs sampler was initialized with K+ = 1, choosing the feature assignments for
the ﬁrst column by setting zi1 = 1 with probability 0.5. σA , σX , and α were initially
set to 0.5, 1.7, and 1 respectively, and then sampled by adding Metropolis steps to the
MCMC algorithm. Figure 3 shows trace plots for the ﬁrst 1000 i
terations of MCMC for the
number of features used by at least one object, K+ , and the model parameters σA , σX , and
α. All of these quantities stabilized after approximately 100 iterations, with the algorithm

(a)

(b)

(c)

+
K
 

α

10

5

0

4

2

0

2

0

0

X
σ

1

0

0

2

A
σ

1

0

0

(Positive)

(Negative)

(Negative)

(Negative)

0  0  0  0

0  1  0  0

1  1  1  0

1  0  1  1

100

200

300

400

500

600

700

800

900

1000

100

200

300

400

500

600

700

800

900

1000

100

200

300

400

500

600

700

800

900

1000

100

200

300

400

500
Iteration

600

700

800

900

1000

Figure 3: Data and results for the demonstration of the inﬁni
te linear-Gaussian binary
latent feature model. (a) Four sample images from the 100 in the dataset. Each image
had 320 × 240 pixels, and contained from zero to four everyday objects. (b) The posterior
mean of the weights (A) for the four most frequent binary features from the 1000th sample.
Each image corresponds to a single feature. These features perfectly indicate the presence
or absence of the four objects. The ﬁrst feature indicates th e presence of the $20 bill,
the other three indicate the absence of the Klein bottle, the handaxe, and the cellphone.
(c) Reconstructions of the images in (a) using the binary codes inferred for those images.
These reconstructions are based upon the posterior mean of A for the 1000th sample. For
example, the code for the ﬁrst image indicates that the $20 bi
ll is absent, while the other
three objects are not. The lower panels show trace plots for the dimensionality of the
representation (K+ ) and the parameters α, σX , and σA over 1000 iterations of sampling.
The values of all parameters stabilize after approximately 100 iterations.

ﬁnding solutions with approximately seven latent features . The four most common features
perfectly indicated the presence and absence of the four objects (shown in Figure 3 (b)), and
three less common features coded for slight differences in the locations of those objects.

4 Conclusion

We have shown that the methods that have been used to deﬁne inﬁ
nite latent class models
[6, 7, 8, 9, 10, 11, 12] can be extended to models in which objects are represented in
terms of a set of latent features, deriving a distribution on inﬁnite binary matrices that can
be used as a prior for such models. While we derived this prior as the inﬁnite limit of
a simple distribution on ﬁnite binary matrices, we have show n that the same distribution
can be speci ﬁed in terms of a simple stochastic process – the I
ndian buffet process. This
distribution satis ﬁes our two desiderata for a prior for inﬁ
nite latent feature models: objects
are exchangeable, and inference remains tractable. Our success in transferring the strategy
of taking the limit of a ﬁnite model from latent classes to lat ent features suggests that a
similar approach could be applied with other representations, expanding the forms of latent
structure that can be recovered through unsupervised learning.

References
[1] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. In Advances in Neural
Information Processing Systems 15, Cambridge, 2003. MIT Press.
[2] I. T. Jolliffe. Principal component analysis. Springer, New York, 1986.
[3] R. S. Zemel and G. E. Hinton. Developing population codes by minimizing description length.
In Advances in Neural Information Processing Systems 6. Morgan Kaufmann, San Francisco,
CA, 1994.
[4] Z. Ghahramani. Factorial learning and the EM algorithm. In Advances in Neural Information
Processing Systems 7. Morgan Kaufmann, San Francisco, CA, 1995.
[5] C. E. Rasmussen and Z. Ghahramani. Occam’s razor.
In Advances in Neural Information
Processing Systems 13. MIT Press, Cambridge, MA, 2001.
[6] C. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric
problems. The Annals of Statistics, 2:1152–1174, 1974.
[7] M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal
of the American Statistical Association, 90:577–588, 1995.
[8] T. S. Ferguson. Bayesian density estimation by mixtures of normal distributions. In M. Rizvi,
J. Rustagi, and D. Siegmund, editors, Recent advances in statistics, pages 287–302. Academic
Press, New York, 1983.
[9] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9:249–265, 2000.
[10] C. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information Pro-
cessing Systems 12. MIT Press, Cambridge, MA, 2000.
[11] D. Aldous. Exchangeability and related topics. In ´Ecole d’ ´et ´e de probabilit ´es de Saint-Flour,
XIII —1983 , pages 1–198. Springer, Berlin, 1985.
[12] J. Pitman. Combinatorial stochastic processes, 2002. Notes for Saint Flour Summer School.
[13] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models an
d the Indian buffet process.
Technical Report 2005-001, Gatsby Computational Neuroscience Unit, 2005.
[14] A. d’Aspremont, L. El Ghaoui, I. Jordan, and G. R. G. Lanckriet. A direct formulation for
sparse PCA using semideﬁnite programming. In Advances in Neural Information Processing
Systems 17. MIT Press, Cambridge, MA, 2005.
[15] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Compu-
tational and Graphical Statistics, in press.


Stimulus Evoked Independent Factor Analysis of
MEG Data with Large Background Activity

S.S. Nagarajan
Biomagnetic Imaging Laboratory
Department of Radiology
University of California, San Francisco
San Francisco, CA 94122
sri@radiology.ucsf.edu

H.T. Attias
Golden Metallic, Inc.
P.O. Box 475608
San Francisco, CA 94147
htattias@goldenmetallic.com

K.E. Hild
Biomagnetic Imaging Laboratory
Department of Radiology
University of California, San Francisco
San Francisco, CA 94122
hild@mrsc.ucsf.edu

K. Sekihara
Dept. of Systems Design and Engineering
Tokyo Metropolitan University
Asahigaoka 6-6, Hino, Tokyo 191-0065
ksekiha@cc.tmit.ac.jp

Abstract

This paper presents a novel technique for analyzing electromagnetic
imaging data obtained using the stimulus evoked experimental paradigm.
The technique is based on a probabilistic graphical model, which de-
scribes the data in terms of underlying evoked and interference sources,
and explicitly models the stimulus evoked paradigm. A variational
Bayesian EM algorithm infers the model from data, suppresses interfer-
ence sources, and reconstructs the activity of separated individual brain
sources. The new algorithm outperforms existing techniques on two real
datasets, as well as on simulated data.

1 Introduction
Electromagnetic source imaging, the reconstruction of the spatiotemporal activation of
brain sources from MEG and EEG data, is currently being used in numerous studies of
human cognition, both in normal and in various clinical populations [1]. A major advan-
tage of MEG/EEG over other noninvasive functional brain imaging techniques, such as
fMRI, is the ability to obtain valuable information about neural dynamics with high tempo-
ral resolution on the order of milliseconds. An experimental paradigm that is very popular
in imaging studies is the stimulus evoked paradigm. In this paradigm, a stimulus, e.g., a
tone at a particular frequency and duration, is presented to the subject at a series of equally
spaced time points. Each presentation (or trial) produces activity in a set of brain sources,
which generates an electromagnetic ﬁeld captured by the sensor array. These data con-
stitute the stimulus evoked response, and analyzing them can help to gain insights into the
mechanism used by the brain to process the stimulus and similar sensory inputs. This paper
presents a new technique for analyzing stimulus evoked electromagnetic imaging data.

An important problem in analyzing such data is that MEG/EEG signals, which are captured
by sensors located outside the brain, contain not only signals generated by brain sources
evoked by the stimulus, but also interference signals, generated by other sources such as
spontaneous brain activity, eye blinks and other biological and non-biological sources of
artifacts. Interference signals overlap spatially and temporally with the stimulus evoked
signals, making it difﬁcult to obtain accurate reconstructions of evoked brain sources. A
related problem is that signals from different evoked sources themselves overlap with each
other, making it difﬁcult to localize individual sources and reconstruct their separate re-
sponses.

Many approaches have been taken to the
problem of suppressing interference signals.
One method is averaging over multiple trials,
which reduces the contributions from inter-
ference sources, assuming that they are un-
correlated with the stimulus and that their au-
tocorrelation time scale is shorter than the
trial length. However, a successful applica-
tion of this method requires a large number of
trials, effectively limiting the number of stim-
ulus conditions per experiment.
It usually
also requires manual rejection of trials con-
taining conspicuous artifacts. A set of meth-
ods termed subspace techniques computes a
projection of the sensor data onto the signal
subspace, which corresponds to brain sources
of interest. However, these methods rely on
thresholding to determine the noise level, and tend to discard information below threshold.
Consequently, those methods perform well only when the interference level is low.

Figure 1: Simulation example (see text)

Independent component analysis (ICA) techniques [4-8], introduced more recently, attempt
to decompose the sensor data into a set of signals that are mutually statistically indepen-
dent. Artifacts such as eye blinks are independent of brain source activity and ICA has been
able in many cases to successfully separate the two types of signals into distinct groups of
output variables. However, ICA techniques have several shortcomings. First, they require
pre-processing the sensor data to reduce dimensionality from, which causes loss of infor-
mation on brain sources with relatively low amplitude. This is because, for K sensors,
ICA must learn a square K × K unmixing matrix from N data points; typical values such
as K = 275, N = 700 can lead to poor performance due to local maxima, overﬁtting,
and slow convergence. Second, ICA assumes L + M = K 0 , where L, M are the num-
ber of evoked and interference sources and K 0 < K is the reduced input dimensionality.
However, many cases have L + M > K 0 , which leads to suboptimal and sometime failed
separation. Third, ICA requires post-processing of its output signals, usually via manual
examination by experts (though sometime by thresholding), to determine which signals
correspond evoked brain sources of interest.

The fourth drawback of ICA techniques is that, by design, they cannot exploit the advantage
offered by the evoked stimulus paradigm. Whereas interference sources are continuously
active, evoked sources become active at each trial only near the time of stimulus presenta-
tion, termed stimulus onset time. Hence, knowledge of the onset times can help separate
the evoked sources. However, the onset times, which are determined by the experimental
design and available during data analysis, are ignored by ICA.

In this paper we present a novel technique for suppressing interference signals and separat-
ing signals from individual evoked sources. The technique is based on a new probabilistic
graphical model termed stimulus evoked independent factor analysis (SEIFA). This model,

an extension of [2], describes the observed sensor data in terms of two sets of independent
variables, termed factors, which are not directly observable. The factors in the ﬁrst set
represent evoked sources, and the factors in the second set represent interference sources.
The sensor data are generated by linearly combining the factors in the two sets using two
mixing matrices, followed by adding sensor noise. The mixing matrices and the precision
matrix of the sensor noise constitute the SEIFA model parameters, and are inferred from
data using a variational Bayesian EM algorithm [3], which computes their posterior dis-
tribution. Separation of the evoked sources is achieved in the course of processing by the
algorithm.

The SEIFA model is free from the above four
shortcomings. It can be applied directly to the sen-
sor data without dimensionality reduction, there-
fore no information is lost. Rather than learn-
ing a square K × K unmixing matrix, it learns
a K × (L + M ) mixing matrix, where the num-
ber of interference factors M is minimized us-
ing automatic Bayesian model selection which is
part of the algorithm. In addition, SEIFA is de-
signed to explicitly model the stimulus evoked
paradigm, hence it optimally exploits the knowl-
edge of stimulus onset
times. Consequently,
evoked sources are automatically identiﬁed and no
post-processing is required.

2 SEIFA
Probabilistic Graphical Model
This section presents the SEIFA probabilistic
graphical model, which is the focus of this paper.
The SEIFA model describes observed MEG sen-
sor data in terms of three types of underlying, un-
observed signals: (1) signals arising from stimu-
lus evoked sources, (2) signals arising from inter-
ference sources, and (2) sensor noise signals. The
model is inferred from data by an algorithm pre-
sented in the next section. Following inference,
the model is used to separate the evoked source
signals from those of the interference sources and
from sensor noise, thus providing a clean version
of the evoked response. The model further sep-
arates the evoked response into statistically inde-
pendent factors. In addition, it produces a regularized correlation matrix of the clean evoked
response and of each independent factors, which facilitates localization.
Let yin denote the signal recorded by sensor i = 1 : K at time n = 1 : N . We assume that
these signals arise from L evoked factors and M interference factors that are combined
linearly. Let xjn denote the signal of evoked factor j = 1 : L, and let ujn denote the
signal of interference factor j = 1 : M , both at time n. We use the term factor rather than
source for a reason explained below. Let Aij denote the evoked mixing matrix, and let Bij
denote the interference mixing matrix. Those matrices contain the coefﬁcients of the linear
combination of the factors that produces the data. They are analogous to the factor loading
matrix in the factor analysis model. Let vin denote the noise signal on sensor i.
We use an evoked stimulus paradigm, where a stimulus is presented at a speciﬁc time,
termed the stimulus onset time, and is absent beforehand. The stimulus onset time is de-

Figure 2: Performance on simulated
data (see text)

yn =

ﬁned as n = N0 + 1. The period preceding the onset n = 1 : N0 is termed pre-stimulus
period, and the period following the onset n = N0 + 1 : N is termed post-stimulus period.
(cid:26) Bun + vn ,
We assume the evoked factors are active only post stimulus and satisfy xjn = 0 before its
onset. Hence
n = 1 : N0
Axn + Bun + vn , n = N0 + 1 : N
To turn (1) into a probabilistic model, each signal must be modelled by a probability dis-
tribution. Here, each evoked factor is modelled by a mixture of Gaussian (MOG) distribu-
SjX
LY
tions. For factor j we have a MOG model with Sj components, also termed states,
sj =1
j=1
State sj is a Gaussian with mean µj,sj and precision νj,sj , and its probability is πj,sj . We
model the factors as mutually statistically independent.

N (xjn | µj,sj , νj,sj )πj,sj

p(xjn ) =

p(xn ) =

p(xjn ) ,

(1)

(2)

There are three reasons for using MOG distributions, rather than Gaussians, to describe the
evoked factors. First, evoked brain sources are often characterized by spikes or by mod-
ulated harmonic functions, leading to non-Gaussian distributions. Second, previous work
on ICA has shown that independent Gaussian sources that are linearly mixed cannot be
separated. Since we aim to separate the evoked response into contributions from individual
factors, we must therefore use independent non-Gaussian factor distributions. Third, as is
well known, a MOG model with a suitably chosen number of states can describe arbitrary
distributions at the desired level of accuracy.
For interference signals and sensor noise we employ a Gaussian model. Each interference
MY
factor is modelled by an independent, zero-mean Gaussian distribution with unit precision,
j=1

N (ujn | 0, 1) = N (un | 0, I )

p(un ) =

(3)

The Gaussian model implies that we exploit only second order statistics of the interference
signals. This contrasts with the evoked signals, whose MOG model facilitates exploiting
higher order statistics, leading to more accurate reconstruction and to separation.

The sensor noise is modelled by a zero-mean Gaussian distribution with a diagonal preci-
sion matrix λ, p(vn ) = N (vn | 0, λ). From (1) we obtain p(yn | xn , un ) = p(vn ) where
we substitute vn = yn − Axn − Bun with xn = 0 for n = 1 : N0 . Hence, we obtain the
(cid:26) N (yn | Bun , λ),
distribution of the sensor signals conditioned on the evoked and interference factors,
n = 1 : N0
p(yn | xn , un , A, B ) =
N (yn | Axn + Bun , λ), n = N0 + 1 : N
independent. Hence p(y , x, u | A, B ) = Q
SEIFA also makes an i.i.d. assumption, meaning the signals at different time points are
n p(yn | xn , un , A, B )p(xn )p(un ). where
y , x, u denote collectively the signals yn , xn , un at all time points. The i.i.d. assumption is
made for simplicity, and implies that the algorithm presented below can exploit the spatial
statistics of the data but not their temporal statistics.
To complete the deﬁnition of SEIFA, we must specify prior distributions over the model
parameters. For the noise precision matrix λ we choose a ﬂat prior, p(λ) = const. For the
p(B ) = Y
p(A) = Y
mixing matrices A, B we choose to use a conjugate prior
N (Aij | 0, λiαj ) ,
ij
ij

N (Bij | 0, λiβj )

(5)

(4)

where all matrix elements are independent zero-mean Gaussians and the precision of the
ij th matrix element is proportional to the noise precision λi on sensor i. It is the λ depen-
dence which makes this prior conjugate. The proportionality constants αj and βj constitute
the parameters of the prior, a.k.a. hyperparameters. Eqs. (2,3,4,5) fully deﬁne the SEIFA
model.

3
Inferring the SEIFA Model from Data: A VB-EM Algorithm
This section presents an algorithm that infers the SEIFA model from data. SEIFA is a
probabilistic model with hidden variables, since the evoked and interference factors are
not directly observable, hence it must be treated in the EM framework. We use varia-
tional Bayesian EM (VB-EM), which has two relevant advantages over standard EM. First,
it is more robust to overﬁtting, which can be a signiﬁcant problem when working with
high-dimensional but relatively short time series (here we analyze N < 1000 point long,
K = 275 dimensional data sequences). To achieve this robustness, VB-EM computes (us-
ing a variational approximation) a full posterior distribution over model parameters, rather
than a single MAP estimate. This means that VB-EM considers all possible parameters
values, and computes the probability of each value conditioned on the observed data. It
also performs automatic model order selection by optimizing the hyperparameters, and
consequently uses the minimum number of parameters needed to explain the data. Second,
VB-EM produces automatically regularized estimators for the evoked response correlation
matrices (required for source localization), where standard EM produces poorly condi-
tioned ones. This is also a result of computing a parameter posterior.

VB-EM is an iterative algorithm, where each iteration consists of an E- and an M-step.
E-step. For the pre-stimulus period n = 1 : N0 we compute the posterior over the interfer-
ence factors un only. It is a Gaussian distribution with posterior mean ¯un and covariance
Φ = (cid:0) ¯B T λ ¯B + I + KΨBB
(cid:1)−1
Φ given by
¯un = Φ ¯B T λyn ,
(6)
where ¯B are ΨBB are the posterior mean and covariance of the interference mixing matrix
B computed in the M-step below (more precisely, the posterior covariance of the ith row
of B is ΨBB /λi ).
For the post-stimulus period n = N0 + 1 : N we compute the posterior over the evoked
state of evoked factor j at time n. The total number of collective states is S = Q
and interference factors xn , un , and the collective state sn of the evoked factors. The latter
is deﬁned by the L-dimensional vector sn = (s1n , s2n , ..., sLn ), where sjn = 1 : Sj is the
j Sj .
To simplify the notation, we combine the evoked and interference factors into a single
L0 × 1 vector x0
n = (xn , un ), where L0 = L + M , and their mixing matrices into a single
K × L0 matrix A0 = (A, B ). Now, at time n, let r run over all the S collective states. For
each r , the posterior over the factors conditioned on sn = r is Gaussian, with posterior
Γr = (cid:0) ¯A0T λ ¯A0 + ν 0
(cid:0) ¯A0T λyn + ν 0
(cid:1) ,
r + KΨ(cid:1)−1
mean ¯xrn , ¯urn and covariance Γr given by
r µ0
¯x0
rn = Γr
r
rn = ( ¯xrn , ¯urn ) and ¯A0 = ( ¯A, ¯B ). The L × 1 vector µ0
¯x0
r and the
We have deﬁned
diagonal L × L matrix ν 0
r contain the means and precisions of the individual states (see (2))
composing r . The posterior mean and covariance ¯A0 , Ψ are computed in the M-step. Next,
(cid:18)
(cid:19)
p| νr || Γr | exp
compute the posterior probability that sn = r by
r νr µr − 1
1
− 1
1
n λyn +
2 µT
2 yT
2
zn
where zn is a normalization constant and µr , νr , πr are the MOG parameters of (2).

r ¯x0
rnΓ−1
¯x0
rn

(8)

(7)

¯πrn =

πr

(10)

M-step. We divide the model parameters into two sets. The ﬁrst set includes the mixing
matrices A, B , for which we compute full posterior distributions. The second set includes
the noise precision λ and the diagonal hyperparameters matrices α, β , for which we com-
(cid:18) Rxx + α
(cid:19)−1
pute MAP estimates. The posterior over A, B is Gaussian factorized over their rows, where
the mean is
¯A = RyxΨ
Rxu
¯B = RyuΨ , Ψ =
(9)
Ruu + β
RT
xu
and where the ith row of A0 = (A, B ) has covariance Ψ/λi . The hyperparameters
Ryx = P
n hynxn i, Rxx = P
αj , βj are diagonal entries of diagonal matrices α, β . Ryx , Ryu , Rxx , Rxu , Ruu are poste-
rior correlations between the factors and the data and among the factors themselves, e.g.,
n hxnxn i, where h·i denotes posterior averaging. They are
easily computed in terms of the E-step quantities ¯un , ¯x0
rn , Φ, Γr , ¯πrn and are omitted.
β−1 = diag (cid:0) ¯B T λ ¯B /K + ΨBB
(cid:1)
(cid:1) ,
α−1 = diag (cid:0) ¯AT λ ¯A/K + ΨAA
Next, the hyperparameter matrices α, β are updated by
and the noise precision matrix by λ−1 = diag(Ryy − ¯ART
yx − ¯BRT
yu )/N . ΨAA and
ΨBB are the appropriate blocks of Ψ in (9). The interference mixing matrix and the noise
precision are initialized from pre-stimulus data. We used MOG parameters corresponding
to peaky (super-Gaussian) distributions.
in = hAij xjn i denote the
Estimating and Localizing Clean Evoked Responses. Let z j
inferred individual contribution from evoked factor j to sensor signal i.
It is given via
posterior averaging by
where ¯xn = P
in = ¯Aij ¯xjn
¯z j
(11)
r ¯πr ¯xrn . Computing this estimate amounts to obtaining a clean version
of the individual contribution from each factor and of their combined contribution, and
removing contributions from interference factors and sensor noise.
The localization of individual evoked factors using sensor signals z j
n can be achieved by
spatial resolution and non-zero localization bias [6]. Let C j = P
many algorithms. In this paper, we use adaptive spatial ﬁlters that take data correlation
matrices as inputs for localization, because these methods have been shown to have superior
n hz j
n )T i denote the
n (z j
(cid:3) (Rxx )j j
C j = (cid:2) ¯Aj ( ¯Aj )T + λ−1 (ΨAA )j j
inferred sensor data correlation matrix corresponding to the individual contribution from
evoked factor j . Then,
(12)
where ¯Aj is a K × 1 vector denoting the j th column of ¯A. Notice that the VB-EM approach
has produced a correlation matrix that is automatically regularized (due to the ΨAA term)
and can be used for localization in its current form. In contrast, computing it from the signal
estimates obtained by other methods, such as PCA or ICA, yields a poorly conditioned
matrix that requires post-processing.

4 Experiments on Real and Simulated Data
Simulations. Fig. 1 shows a simulation with two evoked sources and three interference
sources with N = 10000, signal-to-interference (SIR) of 0 dB and signal-to-sensor-noise
(SNR) of 5dB. The true locations of the evoked sources, each of which is denoted by •, and
the true locations of the background sources, each of which is denoted by × are shown in
the top left panel. The right column in the top row shows the time courses of the evoked
sources as they appear at the sensors. The time courses of the actual sensor signals, which
also include the effects of background sources and sensor noise, are shown in the middle
row (right column). The bottom row shows the localization and time-course of cleaned

Figure 3: Estimating auditory-evoked responses from small trial averages (see text)

evoked sources estimated using SEIFA, which agrees with the true location and time-
course. Fig. 2 shows the mean performance as a function of SIR, across 50 Monte Carlo
trials for N = 1000 and SNR of 10 dB, for different locations of evoked and interference
sources. Denoising performance is quantiﬁed by the output signal-to-(noise+interference)
ratio (SNIR) and shown in the top panel. SEIFA outperforms both our benchmark meth-
ods, providing a 5-10 dB improvement over JADE [7] and SVD. Separation performance
of individual evoked factors is quantiﬁed by (separated-signal)-to-(noise+interference) ra-
tio (SSNIR) (deﬁnition omitted) and is shown in the middle panel. SEIFA far outperforms
JADE for this set of examples. JADE is able to separate the background sources from the
evoked sources (hence gives good denoising performance), but it is not always able to sep-
arate the evoked sources from each other. The Infomax algorithm [4] (results not shown)
exhibited poor separation performance similar to JADE. Finally, localization performance
is quantiﬁed by the mean distance in cm between the true evoked source locations and the
estimated locations, as shown in the bottom panel. Here too, SEIFA far outperforms all
other methods, especially for low SIR. Notably, SEIFA performance appears to be quite
robust to the i.i.d. assumption of the evoked and background sources, because in these
simulations evoked sources were assumed to be damped sinusoids and interference sources
were sinusoids.

4.1 Real Data
Denoising averages from small number of trials. Auditory evoked responses from a
particular subject obtained by averaging different number of trials are shown in ﬁgure 3
(left panel). SEIFA is able to clearly recover responses even from small trial averages.
To quantify the performance of the different methods, a ﬁltered version of the raw data
for Navg = 250 was assumed as “ground-truth”, and is shown in the inset of the right
panel. The output SNIR as a function of Navg is also shown in ﬁgure 3 (right panel).SEIFA
exhibits the best performance especially for small trial averages.
Separation of evoked sources. To highlight SEIFA’s ability to separately localize evoked
sources, we conducted an experiment involving simultaneous presentation of auditory and
somatosensory stimuli. We expected the activation of contralateral auditory and somatosen-
sory cortices to overlap in time. A pure tone (400ms duration, 1kHz, 5 ms ramp up/down)
was presented binaurally with a delay of 50 ms following a pneumatic tap on the left in-
dex ﬁnger. Averaging is performed over Navg = 100 trials triggered on the onset of the
tap. Results from SEIFA for this experiment are shown in Figure 4. In these ﬁgures, one
panel shows a contour map that shows the polarity and magnitude of the denoised and raw
sensor signals in sensor space. The contour plot of the magnetic ﬁeld on the sensor array,
corresponding to the mapping of three-dimensional sensor surface array to points within a

circle, shows the magnetic ﬁeld pro ﬁle at a particular instant of time relative to the stimu-
lus presentation. Other panels show localization of a particular evoked factor overlaid on
the subjects’ MRI. Three orthogonal projections - axial, sagittal and coronal MRI slices,
that highlight all voxels having activity that is > 80% of maximum are shown. Results
are based on the right hemisphere channels above contralateral somatosensory and audi-
tory cortices. Localization of time-course of the ﬁrst two factors estimated by SEIFA are
shown in left and middle panels of ﬁgure 4. The ﬁrst two factors localize to primary so-
matosensory cortex (SI), however with differential latencies. The ﬁrst factor shows a peak
response at a latency of 50 ms, whereas the second factor shows the response at a later
latency. Interestingly, the third factor localizes to auditory cortex and the extracted time-
course corresponds well to an auditory evoked response that is well-separated from the
somatosensory response (ﬁgure 3 right panels).

Figure 4: Estimated SEIFA factors for auditory-somatosensory experiment

5 Extensions
Whereas this paper uses ﬁxed values for the number of evoked and interference sources
L, M (though the effective number of interference sources was determined via optimizing
the hyperparameter β ), VB-EM facilitates inferring them from data, and we plan to inves-
tigate the effectiveness of this procedure. We also plan to infer the distribution of evoked
sources (MOG parameters) from data rather than using a ﬁxed distribution. Another exten-
sion that could enhance performance is exploiting temporal correlation in the data. We plan
to do it by incorporating temporal (e.g., autoregressive) models into the source distributions
and infer their parameters from data.
References
[1] S. Baillet, J. C. Mosher, and R. M. Leahy. Electromagnetic brain mapping.Signal Processing
Magazine, 18:14-30, 2001.
[2] H. Attias (1999). Independent Factor Analysis. Neur. Comp. 11, 803-851.
[3] H. Attias (2000). A variational Bayes framework for graphical models. Adv. Neur. Info. Proc.
Sys. 12, 209-215.
[4] T.-P. Jung, S. Makeig, M. Westerﬁeld, J. Townsend, E. Courchesne, T.J. Sejnowski (2000). Re-
moval of eye artifacts from visual event related potentials in normal and clinical subjects. J. Clin.
Neurophys. 40, 516-520.
[5] S. Makeig, S. Debener, J. Onton, A. Delorme (2004). Mining event related brain dynamics.
Trends Cog. Sci. 8, 204-210.
[6] K. Sekihara, S. Nagarajan, D. Poeppel, A. Marantz, Y. Miyashita (2001). Reconstructing spatio-
temporal activities of neural sources using a MEG vector beamformer technique.
IEEE Trans.
Biomed. Eng. 48, 760-771.
[7] J.F.Cardoso (1999) High-order contrasts for independent component analysis, Neural Computa-
tion, 11(1):157–192.
[8] R. Vigario, J. Sarela, V. Jousmaki, M. Hamalainen, E. Oja (2000). Independent component ap-
proach to the analysis of EEG and MEG recordings. IEEE Trans. Biomed. Eng. 47, 589-593.


Convergence and Consistency of
Regularized Boosting Algorithms with
Stationary β -Mixing Observations

Aur ´elie C. Lozano
Department of Electrical Engineering
Princeton University
Princeton, NJ 08544
alozano@princeton.edu

Sanjeev R. Kulkarni
Department of Electrical Engineering
Princeton University
Princeton, NJ 08544
kulkarni@princeton.edu

Robert E. Schapire
Department of Computer Science
Princeton University
Princeton, NJ 08544
schapire@cs.princeton.edu

Abstract

We study the statistical convergence and consistency of regularized
Boosting methods, where the samples are not independent and identi-
cally distributed (i.i.d.) but come from empirical processes of stationary
β -mixing sequences. Utilizing a technique that constructs a sequence of
independent blocks close in distribution to the original samples, we prove
the consistency of the composite classiﬁers resulting from a regulariza-
tion achieved by restricting the 1-norm of the base classiﬁers’ weights.
When compared to the i.i.d. case, the nature of sampling manifests in the
consistency result only through generalization of the original condition
on the growth of the regularization parameter.

1 Introduction

A signiﬁcant development in machine learning for classiﬁcation has been the emergence
of boosting algorithms [1]. Simply put, a boosting algorithm is an iterative procedure that
combines weak prediction rules to produce a composite classi ﬁer, the idea being that one
can obtain very precise prediction rules by combining rough ones. It was shown in [2] that
AdaBoost, the most popular Boosting algorithm, can be seen as stage-wise ﬁtting of addi-
tive models under the exponential loss function and it effectively minimizes an empirical
loss function that differs from the probability of incorrect prediction. From this perspec-
tive, boosting can be seen as performing a greedy stage-wise minimization of various loss
functions empirically. The question of whether boosting achieves Bayes-consistency then
arises, since minimizing an empirical loss function does not necessarily imply minimizing
the generalization error. When run a very long time, the AdaBoost algorithm, though resis-
tant to overﬁtting, is not immune to it [2, 3]. There also exist cases where running Adaboost

forever leads to a prediction error larger than the Bayes error in the limit of inﬁnite sample
size. Consequently, one approach for the study of consistency is to modify the original Ad-
aboost algorithm by imposing some constraints on the weights of the composite classiﬁer
to avoid overﬁtting. In this regularized version of Adaboost, the 1-norm of the weights of
the base classiﬁers is restricted to a ﬁxed value. The minimization of the loss function is
performed over the restricted class [4, 5].

In this paper, we examine the convergence and consistency of regularized boosting algo-
rithms with samples that are no longer i.i.d. but come from empirical processes of station-
ary weakly dependent sequences. A practical motivation for our study of non i.i.d. sam-
pling is that in many learning applications observations are intrinsically temporal and hence
often weakly dependent. Ignoring this dependency could seriously undermine the perfor-
mance of the learning process (for instance, information related to the time-dependent or-
dering of samples would be lost). Recognition of this issue has led to several studies of non
i.i.d. sampling [6, 7, 8, 9, 10, 11, 12].

To cope with weak dependence we apply mixing theory which, through its deﬁnition of
mixing coefﬁcients, offers a powerful approach to extend results for the traditional i.i.d.
observations to the case of weakly dependent or mixing sequences. We consider the β -
mixing coefﬁcients, whose mathematical deﬁnition is deferred to Sec. 2.1. Intuitively, they
provide a “measure” of how fast the dependence between the observations diminishes as
the distance between them increases. If certain conditions on the mixing coefﬁcients are
satisﬁed to reﬂect a sufﬁciently fast decline in the dependence between observations as
their distance grows, counterparts to results for i.i.d. random processes can be established.
A comprehensive review of mixing theory results is provided in [13].

Our principal ﬁnding is that consistency of regularized Boosting methods can be established
in the case of non-i.i.d. samples coming from empirical sequences of stationary β -mixing
sequences. Among the conditions that guarantee consistency, the mixing nature of sam-
pling appears only through a generalization of the one on the growth of the regularization
parameter originally stated for the i.i.d. case [4].

2 Background and Setup

2.1 Mixing Sequences

Let W = (Wi )i≥1 be a strictly stationary sequence of random variables, each having the
same distribution P on D ⊂ Rd . Let σ l
1 = σ (W1 , W2 , . . . , Wl ) be the σ -ﬁeld generated
by W1 , . . . , Wl . Similarly, let σ∞
l+k = σ (Wl+k , Wl+k+1 , . . . , ) . The following mixing
(cid:162) − P (A) | : A ∈ σ∞
(cid:170)
(cid:169)|P
(cid:161)
coefﬁcients characterize how close to independent a sequence W is.
Deﬁnition 1. For any sequence W , the β -mixing1 coefﬁcient is deﬁned by
A|σk
βW (n) = supk E sup
1
k+n
where the expectation is taken w.r.t. σk
1 .
Hence βW (n) quantiﬁes the degree of dependence between ’future’ observations and ’past’
ones separated by a distance of at least n. In this study, we will assume that the sequences

,

1To gain insight into the notion of β -mixing, it is useful to think of the σ -ﬁeld generated by a ran-
dom variable X as the “body of information” carried by X . This leads to the following interpretation
of β -mixing. Suppose that the index i in Wi is the time index. Let A be an event happening in the
future within the period of time between t = k + n and t = ∞. |P (A|σk
1 ) − P (A)| is the absolute
difference between the probability that event A occurs, given the knowledge of the information gener-
ated by the past up to t = k , and the probability of event A occurring without this knowledge. Then,
1 (the information generated by (W1 , . . . , Wk )) and σ∞
the greater the dependence between σk
k+n (the
information generated by (Wk+n , . . . , W∞ )), the larger the coefﬁcient βW (n).

we consider are algebraically β -mixing. This property implies that the dependence between
observations decreases fast enough as the distance between them increases.
limn→∞ βW (n) = 0. Further, it is
Deﬁnition 2. A sequence W is called β -mixing if
algebraically β -mixing if there is a positive constant rβ such that βW (n) = O (n−rβ ) .

The choice of β -mixing appears appropriate given previous results that showed “uniform
convergence of empirical means uniformly in probability” and “probably approximately
correct” properties to be preserved for β -mixing inputs [11]. Some examples of β -mixing
sequences that ﬁt naturally in a learning scenario are certain Markov processes and Hidden
Markov Models [11]. In practice, if the mixing properties are unknown, they need to be
estimated. Although it is difﬁcult to ﬁnd them in general, there exist simple methods to
determine the mixing rates for various classes of random processes (e.g. Gaussian, Markov,
ARMA, ARCH, GARCH). Hence the assumption of a known mixing rate is reasonable and
has been adopted by many studies [6, 7, 8, 9, 10, 12].

2.2 Classi ﬁcation with Stationary β -Mixing Training Data

In the standard binary classiﬁcation problem, the training data consist of a set Sn =
{(X1 , Y1 ) , . . . , (Xn , Yn )}, where Xk belongs to some measurable space X , and Yk is
in {−1, 1}. Using Sn , a classiﬁer hn : X → {−1, 1} is built to predict the label Y of an
unlabeled observation X . Traditionally, the samples are assumed to be i.i.d., and to our
knowledge, this assumption is made by all the studies on boosting consistency. In this pa-
per, we suppose that the sampling is no longer i.i.d. but corresponds to an empirical process
of stationary β -mixing sequences. More precisely, let D = X × Y , where Y = {−1, +1}.
Let Wi = (Xi , Yi ). We suppose that W = (Wi )i≥1 is a strictly stationary sequence of
random variables, each having the same distribution P on D and that W is β -mixing (see
Deﬁnition 2). This setup is in line with [7]. We assume that the unlabeled observation is
such that (X, Y ) is independent of Sn but with the same marginal.

3 Statistical Convergence and Consistency of Regularized Boosting
for Stationary β -Mixing Sequences

αj hj (X ) : t ∈ N, α1 , . . . , αt ≥ 0,

3.1 Regularized Boosting
We adopt the framework of [4] which we now recall. Let H denote the class of base
classiﬁers h : X → {−1, 1}, which usually consists of simple rules (for instance decision
stumps). This class is required to have ﬁnite VC-dimension. Call F , the class of functions
(cid:111)
(cid:110)
t(cid:88)
t(cid:88)
f : X → [−1, 1] obtained as convex combinations of the classi ﬁers in H:
F =
f (X ) =
.
j=1
j=1
(cid:80)n
(1)
Each fn ∈ F deﬁnes a classiﬁer
hfn = sign (fn ) and for simplicity the generalization
error L (hfn ) is denoted by L (fn ). Then the training error is denoted by Ln (fn ) =
i=1 I[hfn (Xi )(cid:54)=Yi ] . Deﬁne Z (f ) = −f (X ) Y and Zi (f ) = −f (Xi ) Yi . Instead of
1/n
minimizing the indicator of misclassiﬁcation ( I[−f (X )Y >0] ), boosting methods are shown
to effectively minimize a smooth convex cost function of Z (f ). For instance, Adaboost
(cid:80)n
is based on the exponential function. Consider a positive, differentiable, strictly in-
creasing, and strictly convex function φ : R → R+ and assume that φ (0) = 1 and
that limx→−∞ φ (x) = 0. The corresponding cost function and empirical cost func-
tion are respectively C (f ) = Eφ (Z (f )) and Cn (f ) = 1/n
i=1 φ (Zi (f )) . Note that
L (f ) ≤ C (f ), since I[x>0] ≤ φ (x).

αj = 1, h1 , . . . , ht ∈ H

The iterative aspect of boosting methods is ignored to consider only their performing an
(cid:80)n
(approximate) minimization of the empirical cost function or, as we shall see, a series of
cost functions. To avoid overﬁtting, the following regularization procedure is developed for
the choice of the cost functions. Deﬁne φλ such that ∀λ > 0 φλ (x) = φ (λx) . The cor-
n (f ) = 1
i=1 φλ (Zi (f ))
responding empirical and expected cost functions become C λ
n
and C λ (f ) = Eφλ (Z (f )) . The minimization of a series of cost functions C λ over the
convex hull of H is then analyzed.

3.2 Statistical Convergence

The nature of the sampling intervenes in the following two lemmas that relate the empirical
n (f ) and true cost C λ (f ).
cost C λ
Lemma 1. Suppose that for any n, the training data (X1 , Y1 ) , . . . (Xn , Yn ) comes from
(cid:180)
(cid:179)
a stationary algebraically β -mixing sequence with β -mixing coefﬁcients β (m) satisfying
β (m) = O (m−rβ ), m ∈ N and rβ a positive constant. Then for any λ > 0 and b ∈ [0, 1),
2
1
|C λ (f ) − C λ
n (f ) | ≤ 4λφ(cid:48) (λ)
c1
E sup
+
+ 2φ (λ)
.
n1−b
n(1−b)/2
nb(1+rβ )−1
f ∈F
Lemma 2. Let the training data be as in Lemma 1. For any b ∈ [0, 1), and α ∈ (0, 1 − b),
(cid:162) ≤ exp(−4c2nα ) + O(n1−b(rβ +1) ).
(cid:161)
let n = 3(2c1 + nα/2 )λφ(cid:48) (λ)/n(1−b)/2 . Then for any λ > 0
|C λ (f ) − C λ
n (f ) | > n
P
sup
f ∈F
The constants c1 and c2 in the above lemmas are given in the proofs of Lemma 1 (Sec-
tion 4.2) and Lemma 2 (Section 4.3) respectively.

(3)

(2)

3.3 Consistency Result

The following summarizes the assumptions that are made to prove consistency.
Assumption 1.
I- Properties of the sample sequence: The samples (X1 , Y1 ) , . . . , (Xn , Yn ) are assumed
to come from a stationary algebraically β -mixing sequence with β -mixing coefﬁcients
βX,Y (n) = O (n−rβ ), rβ being a positive constant.
II- Properties of the cost function φ: φ is assumed to be a differentiable, strictly convex,
strictly increasing cost function such that φ (0) = 1 and limx→−∞ φ (x) = 0.
III- Properties of the base hypothesis space: H has ﬁnite VC dimension. The distri-
bution of (X, Y ) and the class H are such that limλ→∞ inf f ∈λF C (f ) = C ∗ , where
λF = {λf : f ∈ F } and C ∗ = inf C (f ) over all measurable functions f : X → R.
c ∈ (cid:161)
(cid:162)
IV- Properties of the smoothing parameter: We assume that λ1 , λ2 , . . . is a sequence
of positive numbers satisfying λn → ∞ as n → ∞, and that there exists a constant
such that λnφ(cid:48) (λn ) /n(1−c)/2 → 0 as n → ∞.
, 1
1
(cid:80)n
1+rβ
n the function in F which approximatively minimizes C λ
Call ˆf λ
n (f ), i.e. ˆf λ
n is such that
n ) ≤ inf f ∈F C λ
i=1 φλ (Zi (f )) + n , with n → 0 as
n ( ˆf λ
n (f ) + n = inf f ∈F 1
C λ
n → ∞. The main result is the following.
n
Theorem 1. Consistency of regularized boosting methods for stationary β -mixing se-
n ∈ F , where ˆf λn
quences. Let fn = ˆf λn
n (f ) . Un-
(approximatively) minimizes C λn
der Assumption 1, limn→∞ L (hfn = sign (fn )) = L∗ almost surely and hfn is strongly
n
Bayes-risk consistent.

Cost functions satisfying Assumption 1.II include the exponential function and the logit
function log2 (1 + ex ). Regarding Assumption 1.II, the reader is referred to [4](Remark on

(denseness assumption)). In Assumption 1.IV, notice that the nature of sampling leads to
a generalization of the condition on the growth of λnφ(cid:48) (λn ) already present in the i.i.d.
setting [4]. More precisely, the nature of sampling manifests through parameter c, which is
limited by rβ . The assumption that rβ is known is quite strict but cannot be avoided (for
instance this assumption is widely made in the ﬁeld of time series analysis). On a positive
note, if unknown, rβ can be determined for various classes of processes as mentioned
Section 2.1.

4 Proofs
4.1 Preparation to the Proofs: the Blocking Technique
(cid:175)(cid:175)(cid:175),
(cid:175)(cid:175)(cid:175)1/n
n(cid:88)
(cid:175)(cid:175) = sup
(cid:175)(cid:175)C λ
The key issue resides in upper bounding
n (f ) − C λ (f )
φ (−λf (Xi ) Yi ) − Eφ (−λf (X1 ) Y1 )
sup
f ∈F
f ∈F
i=1
where F is given by (1). Let W = (X, Y ), Wi = (Xi , Yi ). Deﬁne the function gλ by
(cid:175)(cid:175)(cid:175).
(cid:175)(cid:175)(cid:175)n−1 (cid:80)n
(cid:175)(cid:175) = supgλ∈Gλ
(cid:175)(cid:175)C λ
gλ (W ) = gλ (X, Y ) = φ (−λf (X ) Y ) and the class Gλ by Gλ = {gλ : gλ (X, Y ) =
φ (−λf (X ) Y ) , f ∈ F } . Then (4) can be rewritten as
n (f ) − C λ (f )
i=1 gλ (Wi ) − Egλ (W1 )
supf ∈F
Note that the class Gλ is uniformly bounded by φ (λ). Besides, if H is a class of measurable
functions, then Gλ is also a class of measurable functions, by measurability of F .
As the Wi ’s are not i.i.d, we propose to use the blocking technique developed in [12, 14] to
construct i.i.d blocks of observations which are close in distribution to the original sequence
W1 , . . . , Wn . This enables us to work on the sequence of independent blocks instead of the
original sequence. We use the same notation as in [12]. The protocol is the following. Let
(bn , µn ) be a pair of integers, such that
(n − 2bn ) ≤ 2bnµn ≤ n.
(5)
Divide the segment W1 = (X1 , Y1 ) , . . . , Wn = (Xn , Yn ) of the mixing sequence into
2µn blocks of size bn , followed by a remaining block (of size at most 2bn ). Con-
sider the odd blocks only.
If their size bn is large enough, the dependence between
(cid:161)
(cid:162)
them is weak, since two odd blocks are separated by an even block of the same size
bn . Therefore, the odd blocks can be approximated by a sequence of independent blocks
(cid:161)
(cid:162)
(cid:161)
(cid:162)
with the same within-block structure. The same holds if we consider the even blocks.
Let (ξ1 , . . . , ξbn ) , (ξbn+1 , . . . , ξ2bn ) , . . . ,
be independent blocks
ξ(2µn−1)bn , . . . , ξ2µn bn
(cid:80)j bn
(cid:80)j bn
, for j = 0, . . . , µn − 1.
=D
such that
ξj bn+1 , . . . , ξ(j+1)bn
Wj bn+1 , . . . , W(j+1)bn
For j = 1, . . . , 2µn , and any g ∈ Gλ , deﬁne
(cid:161)
(cid:162) · ξ(2j−2)bn+i,2 , where ξk,1 and ξk,2
i=(j−1)bn+1 g (Wi ) − bnEg (W1 ) .
i=(j−1)bn+1 g (ξi ) − bnEg (ξ1 ) , ˜Zj,g :=
Zj,g :=
Let Oµn = {1, 3, . . . , 2µn − 1} and Eµn = {2, 4, . . . , 2µn }.
Deﬁne Zi,j (f ) as Zi,j (f ) := −f
ξ(2j−2)bn+i,1
are respectively the 1st and 2nd coordinate of the vector ξk . These correspond to the
Zk (f ) = −f (Xk ) Yk for k in the odd blocks 1, ..., bn , 2bn + 1, ..., 3bn, ....
4.2 Proof sketch of Lemma 1
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175)+ φ (λ)
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) ≤ 2E sup
(cid:179)
n(cid:88)
(cid:88)
A. Working with Independent Blocks. We show that
g (Wi )−Eg (W1 )
g∈Gλ
n
n
j∈Oµn
i=1

µnβW (bn )+

E sup
g∈Gλ

2bn
n

Zj,g

(4)

(cid:180)

.

(6)

(cid:180) (cid:175)(cid:175)(cid:175), where R
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) = E supg
(cid:175)(cid:175)(cid:175) 1
(cid:179)(cid:80)
(cid:80)n
(cid:80)
Proof. Without loss of generality, assume that Eg (W1 ) = Eg (ξ1 ) = 0.
(cid:80)n
(cid:80)
˜Zj,g + R
˜Zj,g +
Then, E supg
i=1 g (Wi )
Oµn
Eµn
n
n
is the remainder term consisting of a sum of at most 2bn terms. Noting that ∀g ∈
(cid:80)
i=1 g (Wi ) | ≤ E(supg | 1
Gλ , |g | ≤ φ (λ), it follows that E supg | 1
˜Zj,g |) +
Oµn
n
n
˜Zj,g |) + φ(λ)(2bn )
E(supg | 1
(cid:101)Q the
. We use the following intermediary lemma.
Eµn
n
n
4.1). Call Q the
3
Lemma
Lemma
from [15],
(adapted
distribu-
bound H , |Qh (W1 , . . .) − (cid:101)Qh (ξ1 , . . .) | ≤ H (µn − 1) βW (bn ) . The same result holds
(W1 , . . . , Wbn , W2bn+1 , . . . , W3bn , . . .)
tion
distribution
of
and
of
For any measurable function h on Rbn µn with
(ξ1 , . . . , ξbn , ξ2bn+1 , . . . , ξ3bn , . . .).
(cid:80)
(cid:80)
for (Wbn+1 , . . . , W2bn , W3bn+1 , . . . , W4bn . . .).
(cid:80)n
˜Zj,g |
˜Zj,g | and h(Wbn+1 , . . .) = supg | 1
Using this with h(W1 , . . .) = supg | 1
(cid:80)
(cid:80)
Oµn
Eµn
n
n
respectively, and noting that H = φ (λ) /2, we have E supg | 1
i=1 g (Wi ) | ≤
n
E supg | 1
Zj,g | + φ(λ)
2 µnβW (bn ) + E supg | 1
Zj,g | + φ(λ)
2 µnβW (bn ) + φ(λ)(2bn )
.
Eµn
Oµn
(cid:117)(cid:116)
n
n
n
As the Zj,g ’s from odd and even blocks have the same distribution, we obtain (6).

(7)

B. Symmetrization. The odd blocks Zj,g ’s being independent, we can use the standard
symmetrization techniques. Let Z (cid:48)
j,g ’s be i.i.d. copies of the Zj,g ’s. Let Z (cid:48)
i,j (f )’s be the
corresponding copies of the Zi,j (f ). Let (σi ) be a Rademacher sequence, i.e. a sequence
of independent random variables taking the values ±1 with probability 1/2. Then by [16],
(cid:162) (cid:175)(cid:175)(cid:175).
(cid:175)(cid:175)(cid:175) ≤ E sup
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) 1
(cid:88)
(cid:88)
(cid:161)
Lemma 6.3 (Proof is omitted due to space constraints), we have
Zj,g − Z (cid:48)
E sup
σj
Zj,g
j,g
n
n
j∈Oµn
j∈Oµm
g
g
(cid:175)(cid:175)(cid:175).
(cid:175)(cid:175)(cid:175) ≤ 2 · bnλφ(cid:48) (λ) E sup
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) 1
(cid:88)
µn(cid:88)
C. Contraction Principle. We now show that
(cid:80)bn
E sup
σj Z1,j (f )
(8)
Zj,g
g∈Gλ
f ∈F
n
n
(cid:175)(cid:175) ≤ E supg
(cid:175)(cid:175) 1
(cid:162)(cid:162) (cid:175)(cid:175) ≤
(cid:175)(cid:175) 1
j∈Oµn
(cid:161)
(cid:161)
(cid:80)µn
(cid:80)bn
(cid:80)
j=1
(cid:175)(cid:175). By applying the “Comparison Theorem”, The-
(cid:175)(cid:175)1
(cid:80)µn
i=1 φλ (Zi,j (f )), and the Zi,j (f )’s and Z (cid:48)
i,j (f )’s are i.i.d., with (7)
Proof. As Zj,g =
φλ (Zi,j (f )) − φλ
Z (cid:48)
E supg
i,j (f )
Zj,g
j=1 σj
j∈Oµn
i=1
n
n
j=1 σj (φλ (Z1,j (f ))− 1)
2bnE supg
(cid:117)(cid:116)
orem 7 in [17], to the contraction ψ (x) = (1/λφ(cid:48) (λ)) (φλ (x) − 1), we obtain (8).
n
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) ≤ c1
µn(cid:88)
D. Maximal Inequality. We show that there exists a constant c1 > 0 such that
√
µn
(cid:80)µn
E sup
σj Z1,j (f )
(9)
f ∈F
n
n
(cid:161)
(cid:162) |. Since
1 ∈HN supα1 ,...,αN | (cid:80)µn
(cid:80)N
j=1
1 . One can write E supf ∈F | 1
j=1 σj Z1,j (f )| =
Proof. Denote (h1 , . . . , hN ) by hN
n
(cid:162)(cid:162)
(cid:161)
(cid:161)
n E supN ≥1 suphN
1
ξ(2j−2)bn+1,1
k=1 αk σj ξ(1,j ),2hk
j=1
(cid:161)
(cid:161)
(cid:162)(cid:162)
ξ(2j−2)bn+1,2 and ξ(2j (cid:48)−2)bn+1,2 are i.i.d. for all j (cid:54)= j (cid:48) (they come from different blocks),
and (σj ) is a Rademacher sequence, then
ξ(2j−2)bn+1,1
σj ξ(2j−2)bn+1,2hk
(cid:162) (cid:175)(cid:175)(cid:175)(cid:175).
(cid:175)(cid:175)(cid:175)(cid:175) =
(cid:175)(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175)(cid:175) µn(cid:88)
j=1,...,µn
N(cid:88)
µn(cid:88)
(cid:161)
has the same distribution as
. Hence
ξ(2j−2)bn+1,1
σj hk
j=1,...,µn
ξ(2j−2)bn+1,1
sup
σj Z1,j (f )
n
1 ∈HN
hN
j=1
j=1
k=1
By the same argument as used in [4], p.53 on the maximum of a linear function over
a convex polygon, the supremum is achieved when αk = 1 for some k . Hence we get

sup
α1 ,...,αN

E sup
f ∈F

.

1
n

E sup
N ≥1

σj αk hk

1
n

σj h

σj h

E sup
h∈H

ξ(2j−2)bn+1,1

(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) = 1
(cid:175)(cid:175)(cid:175) (cid:80)µn
(cid:162) (cid:175)(cid:175)(cid:175). Noting that for all
(cid:161)
(cid:80)µn
E supf ∈F
n E suph∈H
j=1 σj Z1,j (f )
j=1 σj h
ξ(1,j ),1
n
j (cid:54)= j (cid:48) , h(ξ(2j−2)bn+1,1 ) and h(ξ(2j (cid:48)−2)bn+1,1 ) are i.i.d. and that Rademacher processes
(cid:175)(cid:175)(cid:175)(cid:175) µn(cid:88)
(cid:162) (cid:175)(cid:175)(cid:175)(cid:175) ≤ 1
(cid:175)(cid:175)(cid:175)(cid:175) µn(cid:88)
(cid:162) (cid:175)(cid:175)(cid:175)(cid:175)
(cid:161)
(cid:161)
are sub-gaussian, we have by [18], Corollary 2.2.8
(cid:90) ∞
E sup
h∈H∪{0}
n
≤ c(cid:48)√
j=1
j=1
N (, ρ2,Pn , H ∪ {0}))1/2 d,
µn
(log sup
n
P
0
where c(cid:48) is a constant and N (, ρ2,Pn , H ∪ {0}) is the empirical L2 covering number.
(cid:82) ∞
As H has ﬁnite VC-dimension (see Assumption 1.III), there exists a positive constant
w such that supP N (, ρ2,Pn , H ∪ {0}) = OP (−w )(see [18], Theorem 2.6.1). Hence
(log supPn N (, ρ2,Pn , H ∪ {0}))1/2d < ∞. and (9) follows.
(cid:117)(cid:116)
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) ≤ 4bnλφ(cid:48) (λ) c1
(cid:162)
(cid:161)
(cid:80)n
0
E. Establishing (2). Combining (6),(8), and (9), we have
√
i=1 g (Wi ) − Eg (W1 )
E supg∈Gλ
µn
µnβW (bn )+ 2bn
n + φ (λ)
.
(cid:161)
(cid:162)
n
n
Take bn = nb , with 0 ≤ b < 1. By (5), we obtain µn ≤ n1−b /2. Besides, as we assumed
that the sequence W is algebraically β -mixing (see Deﬁnition 2), βW (n) = O (n−rβ ).
n1−b(1+rβ )
Then µnβW (bn ) = O
, and we arrive at (2).

ξ(2j−2)bn+1,1

(10)

4.3 Proof Sketch of Lemma 2
A. Working with Independent Blocks and Symmetrization. For any b ∈ [0, 1), α ∈
(0, 1 − b), let
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) > n
(cid:175)(cid:175)(cid:175) > n/3
(cid:175)(cid:175)(cid:175) 1
(cid:179)
(cid:180)
(cid:180)
(cid:179)
n(cid:88)
(cid:88)
n = 3(2c1 + nα/2 )λφ(cid:48) (λ)/n(1−b)/2 .
We show
g (Wi )−Eg (W1 )
≤ 2P
+O(n1−b(1+rβ ) ).
P
sup
sup
Zj,g
g∈Gλ
g∈Gλ
n
n
j∈Oµn
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) > n
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) 1
(cid:179)
(cid:180)
(cid:179)
i=1
(cid:80)
(cid:80)n
(11)
(cid:180)
Proof. By [12], Lemma 3.1, we have that for any n such that φ(λ)bn = o(nn ),
i=1 g (Wi ) − Eg (W1 )
≤ 2P
P
supg∈Gλ
supg∈Gλ
Zj,g
j∈Oµn
n
n
+ 4µnβW (bn ). Set bn = nb , with 0 ≤ b < 1. Then µnβW (bn ) = O(n1−b(1+rβ ) )
n/3
(for the same reasons as in Section 4.2 E.). With n as in (10), and since Assumption 1.II
implies that λφ(cid:48) (λ) ≥ φ(λ) − 1, we automatically obtain φ(λ)bn = o(nn ).
(cid:117)(cid:116)
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) > n /3
(cid:179)
(cid:180)
(cid:88)
B. McDiarmid’s Bounded Difference Inequality. For n as in (10), there exists a constant
c2 > 0 such that,
sup
(cid:80)
g∈Gλ
n
j∈Oµn
Proof. The Zj,g ’s of the odd block being independent, we can apply McDiarmid’s bounded
| 1
Zj,g |
difference inequality ([19], Theorem 9.2 p.136) on the function supg∈Gλ
j∈Oµn
n
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) + 
(cid:175)(cid:175)(cid:175) > E supg∈Gλ
which depends of Z1,g , Z3,g . . . , Z2µn−1,g . Noting that changing the value of one variable
(cid:180)
(cid:179)
(cid:179)
(cid:180)
(cid:175)(cid:175) 1
(cid:80)
(cid:80)
does not change the value of the function by more that bnφ (λ) /n,we obtain with bn = nb
that for all  > 0,
(cid:175)(cid:175)(cid:175) 1
(cid:175)(cid:175)(cid:175) ≤ 2λφ(cid:48) (λ) C/n(1−b)/2 . With  = nα/2λφ(cid:48) (λ)/n(1−b)/2 , we
≤ exp
−42 n1−b
(cid:80)
P
supg∈Gλ
.
Zj,g
Zj,g
j∈Oµn
j∈Oµn
φ(λ)2
n
n
Combining (8) and (9) from the proof of Lemma 1, and with bn = nb , we have
E supg∈Gλ
Zj,g
j∈Oµn
n
obtain n as in (10). Pick λ0 such that 0 < λ0 < λ. Then, since λφ(cid:48) (λ) ≥ φ(λ) − 1, (12)
follows with c2 = (1 − 1/φ(λ0 ))2 .
(cid:117)(cid:116)

≤ exp(−4c2nα ).

(12)

Zj,g

P

C. Establishing (3). Combining (11) and (12) we obtain (3).

4.4 Proof Sketch of Theorem 1
Let ¯fλ a function in F minimizing C λ . With fn = ˆf λn
n , we have
(cid:161) ¯fλn
(cid:162) ≤ 2 supf ∈F |C λn (f ) −
C (λn fn ) − C ∗ = (C λn ( ˆf λn
n ) − C λn ( ¯fλn )) + (inf f ∈λnF C (f ) − C ∗ ).
Since λn → ∞, the second term on the right-hand side converges to zero by Assump-
n ) − C λn
tion 1.III. By [19], Lemma 8.2, we have C λn ( ˆf λn
n (f ) |. By Lemma 2, supf ∈F |C λn (f ) − C λn
n (f ) | → 0 with probability 1 if, as
C λn
n → ∞, λnφ(cid:48) (λn ) n(α+b−1)/2 → 0 and b > 1/(1 + rβ ). Hence if Assumption 1.IV
holds, C (λn fn ) → C ∗ with probability 1. By [4], Lemma 5, the theorem follows.

References

[1] Schapire, R.E.: The Boosting Approach to Machine Learning An Overview. In Proc. of the MSRI
Workshop on Nonlinear Estimation and Classiﬁcation (2002)
[2] Friedman, J., Hastie T., Tibshirani, R.: Additive logistic regression: A statistical view of boost-
ing. Ann. Statist. 38 (2000) 337–374
[3] Jiang, W.: Does Boosting Overﬁt:Views From an Exact Solution. Technical Report 00-03 De-
partment of Statistics, Northwestern University (2000)
[4] Lugosi, G., Vayatis, N.: On the Bayes-risk consistency of boosting methods. Ann. Statist. 32
(2004) 30–55
[5] Zhang, T.: Statistical Behavior and Consistency of Classiﬁcation Methods based on Convex Risk
Minimization. Ann. Statist. 32 (2004) 56–85
[6] Gy ¨orﬁ, L., H ¨ardle, W., Sarda, P., and Vieu, P.: Nonparametric Curve Estimation from Time
Series. Lecture Notes in Statistics. Springer-Verlag, Berlin. (1989)
[7] Irle, A.: On the consistency in nonparametric estimation under mixing assumptions. J. Multivari-
ate Anal. 60 (1997) 123–147
[8] Meir, R.: Nonparametric Time Series Prediction Through Adaptative Model Selection. Machine
Learning 39 (2000) 5–34
[9] Modha, D., Masry, E.: Memory-Universal Prediction of Stationary Random Processes. IEEE
Trans. Inform. Theory 44 (1998) 117–133
[10] Roussas, G.G.: Nonparametric estimation in mixing sequences of random variables. J. Statist.
Plan. Inference. 18 (1988) 135–149
[11] Vidyasagar, M.: A Theory of Learning and Generalization: With Applications to Neural Net-
works and Control Systems. Second Edition. Springer-Verlag, London (2002)
[12] Yu, B.: Density estimation in the L∞ norm for dependent data with applications. Ann. Statist.
21 (1993) 711–735
[13] Doukhan, P.: Mixing Properties and Examples. Springer-Verlag, New York (1995)
[14] Yu, B.: Some Results on Empirical Processes and Stochastic Complexity. Ph.D. Thesis, Dept
of Statistics, U.C. Berkeley (Apr. 1990)
[15] Yu, B.: Rate of convergence for empirical processes of stationary mixing sequences. Ann.
Probab. 22 (1994) 94–116.
[16] Ledoux, M., Talagrand, N.: Probability in Banach Spaces. Springer, New York (1991)
[17] Meir, R., Zhang, T.:Generalization error bounds for Bayesian mixture algorithms. J. Machine
Learning Research (2003)
[18] van der Vaart, A.W., Wellner, J.A.: Weak convergence and empirical processes. Springer Series
in Statistics. Springer-Verlag, New York (1996)
[19] Devroye, L., Gy ¨orﬁ L., Lugosi, G.: A Probabilistic Theory of Pattern Recognition. Springer,
New York (1996)


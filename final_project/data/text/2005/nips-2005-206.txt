On the Convergence of Eigenspaces in Kernel
Principal Component Analysis

Laurent Zwald
D ´epartement de Math ´ematiques,
Universit ´e Paris-Sud,
B ˆat. 425, F-91405 Orsay, France
Laurent.Zwald@math.u-psud.fr

Gilles Blanchard
Fraunhofer First (IDA),
K ´ekul ´estr. 7, D-12489 Berlin, Germany
blanchar@first.fhg.de

Abstract

This paper presents a non-asymptotic statistical analysis of Kernel-PCA
with a focus different from the one proposed in previous work on this
topic. Here instead of considering the reconstruction error of KPCA we
are interested in approximation error bounds for the eigenspaces them-
selves. We prove an upper bound depending on the spacing between
eigenvalues but not on the dimensionality of the eigenspace. As a conse-
quence this allows to infer stability results for these estimated spaces.

1 Introduction.

Principal Component Analysis (PCA for short in the sequel) is a widely used tool for data
dimensionality reduction. It consists in ﬁnding the most relevant lower-dimension projec-
tion of some data in the sense that the projection should keep as much of the variance of
the original data as possible. If the target dimensionality of the projected data is ﬁx ed in
advance, say D – an assumption that we will make throughout the present paper –
the solu-
tion of this problem is obtained by considering the projection on the span SD of the ﬁrst D
eigenvectors of the covariance matrix. Here by ’ﬁrst D eigenvectors’ we mean eigenvec-
tors associated to the D largest eigenvalues counted with multiplicity; hereafter with some
abuse the span of the ﬁrst D eigenvectors will be called “ D-eigenspace”
for short when
there is no risk of confusion.

The introduction of the ’Kernel trick’ has allowed to extend this methodology to data
mapped in a kernel feature space, then called KPCA [8]. The interest of this extension
is that, while still linear in feature space, it gives rise to nonlinear interpretation in original
space – vectors in the kernel feature space can be interpreted as nonlinear functions on the
original space.

For PCA as well as KPCA, the true covariance matrix (resp. covariance operator) is not
known and has to be estimated from the available data, an procedure which in the case of
Kernel spaces is linked to the so-called Nystr ¨om approximation [13]. The subspace given
as an output is then obtained as D-eigenspace bSD of the empirical covariance matrix or
operator. An interesting question from a statistical or learning theoretical point of view is
then, how reliable this estimate is.
This question has already been studied [10, 2] from the point of view of the reconstruction

error of the estimated subspace. What this means is that (assuming the data is centered in
Kernel space for simplicity) the average reconstruction error (square norm of the distance to
the projection) of bSD converges to the (optimal) reconstruction error of SD and that bounds
are known about the rate of convergence. However, this does not tell us much about the
convergence of SD to bSD –
since two very different subspaces can have a very similar
reconstruction error, in particular when some eigenvalues are very close to each other (the
gap between the eigenvalues will actually appear as a central point of the analysis to come).
In the present work, we set to study the behavior of these D-eigenspaces themselves: we
provide ﬁnite
sample bounds describing the closeness of the D-eigenspaces of the em-
pirical covariance operator to the true one. There are several broad motivations for this
analysis. First, the reconstruction error alone is a valid criterion only if one really plans to
perform dimensionality reduction of the data and stop there. However, PCA is often used
merely as a preprocessing step and the projected data is then submitted to further process-
ing (which could be classiﬁcation,
regression or something else). In particular for KPCA,
the projection subspace in the kernel space can be interpreted as a subspace of functions on
the original space; one then expects these functions to be relevant for the data at hand and
for some further task (see e.g. [3]). In these cases, if we want to analyze the full proce-
dure (from a learning theoretical sense), it is desirable to have a more precise information
on the selected subspace than just its reconstruction error. In particular, from a learning
complexity point of view, it is important to ensure that functions used for learning stay in
a set of limited complexity, which is ensured if the selected subspace is stable (which is a
consequence of its convergence).

The approach we use here is based on perturbation bounds and we essentially walk in the
steps pioneered by Kolchinskii and Gin ´e [7] (see also [4]) using tools of operator perturba-
tion theory [5]. Similar methods have been used to prove consistency of spectral clustering
[12, 11]. An important difference here is that we want to study directly the convergence of
the whole subspace spanned by the ﬁrst D eigenvectors instead of the separate convergence
of the individual eigenvectors; in particular we are interested in how D acts as a complexity
parameter. The important point in our main result is that it does not: only the gap between
the D-th and the (D + 1)-th eigenvalue comes into account. This means that there in no
increase in complexity (as far as this bound is concerned: of course we cannot exclude that
better bounds can be obtained in the future) between estimating the D-th eigenvector alone
or the span of the ﬁrst D eigenvectors.
Our contribution in the present work is thus

(cid:15) to adapt the operator perturbation result of [7] to D-eigenspaces.
(cid:15) to get non-asymptotic bounds on the approximation error of Kernel-PCA
eigenspaces thanks to the previous tool.

In section 2 we introduce shortly the notation, explain the main ingredients used and obtain
a ﬁrst bound based on controlling separately the ﬁrst D eigenvectors, and depending on the
dimension D . In section 3 we explain why the ﬁrst bound is actually suboptimal and derive
an improved bound as a consequence of an operator perturbation result that is more adapted
to our needs and deals directly with the D-eigenspace as a whole. Section 4 concludes and
discusses the obtained results. Mathematical proofs are found in the appendix.

2 First result.

Notation. The interest variable X takes its values in some measurable space X , following
the distribution P . We consider KPCA and are therefore primarily interested in the map-
ping of X into a reproducing kernel Hilbert space H with kernel function k through the

feature mapping ’(x) = k(x; (cid:1)). The objective of the kernel PCA procedure is to recover a
D-dimensional subspace SD of H such that the projection of ’(X ) on SD has maximum
averaged squared norm.

All operators considered in what follows are Hilbert-Schmidt and the norm considered for
these operators will be the Hilbert-Schmidt norm unless precised otherwise. Furthermore
we only consider symmetric nonnegative operators, so that they can be diagonalized and
have a discrete spectrum.

Let C denote the covariance operator of variable ’(X ). To simplify notation we assume
that nonzero eigenvalues (cid:21)1 > (cid:21)2 > : : : of C are all simple (This is for convenience only.
In the conclusion we discuss what changes have to be made if this is not the case). Let
(cid:30)1 ; (cid:30)2 ; : : : be the associated eigenvectors. It is well-known that the optimal D-dimensional
reconstruction space is SD = spanf(cid:30)1 ; : : : ; (cid:30)D g. The KPCA procedure approximates this
objective by considering the empirical covariance operator, denoted Cn , and the subspace
bSD spanned by its ﬁrst D eigenvectors. We denote PSD ; P bSD
the orthogonal projectors on
these spaces.
A ﬁrst bound. Broadly speaking, the main steps required to obtain the type of result we
are interested in are

1. A non-asympotic bound on the (Hilbert-Schmidt) norm of the difference between
the empirical and the true covariance operators;
2. An operator perturbation result bounding the difference between spectral projec-
tors of two operators by the norm of their difference.

The combination of these two steps leads to our goal. The ﬁrst step consists in the following
Lemma coming from [9]:
Lemma 1 (Corollary 5 of [9]) Supposing that supx2X k(x; x) (cid:20) M , with probability
greater than 1 (cid:0) e(cid:0)(cid:24) ,
pn  1 + r (cid:24)
2 ! :
2M
As for the second step, [7] provides the following perturbation bound (see also e.g. [12]):

kCn (cid:0) C k (cid:20)

Theorem 2 (Simpliﬁed Version of [7], Theorem 5.2 ) Let A be a symmetric positive
Hilbert-Schmidt operator of the Hilbert space H with simple positive eigenvalues (cid:21)1 >
(cid:21)2 > : : : For an integer r such that (cid:21)r > 0, let e(cid:14)r = (cid:14)r ^ (cid:14)r(cid:0)1 where (cid:14)r = 1
2 ((cid:21)r (cid:0) (cid:21)r+1 ).
Let B 2 H S (H) be another symmetric operator such that kB k < e(cid:14)r =2 and (A + B ) is
still a positive operator with simple nonzero eigenvalues.
Let Pr (A) (resp. Pr (A + B )) denote the orthogonal projector onto the subspace spanned
by the r-th eigenvector of A (resp. (A + B )). Then, these projectors satisfy:
2kB k
e(cid:14)r
Remark about the Approximation Error of the Eigenvectors:
let us recall that a con-
trol over the Hilbert-Schmidt norm of the projections onto eigenspaces imply a control on
the approximation errors of the eigenvectors themselves. Indeed, let (cid:30)r ;  r denote the (nor-
malized) r-th eigenvectors of the operators above with signs chosen so that h(cid:30)r ;  r i > 0.
Then
kP(cid:30)r (cid:0) P r k2 = 2(1 (cid:0) h(cid:30)r ;  r i2 ) (cid:21) 2(1 (cid:0) h(cid:30)r ;  r i) = k(cid:30)r (cid:0)  r k2 :

kPr (A) (cid:0) Pr (A + B )k (cid:20)

:

Now, the orthogonal projector on the direct sum of the ﬁrst D eigenspaces is the sum
PD
r=1 Pr . Using the triangle inequality, and combining Lemma 1 and Theorem 2, we
conclude that with probability at least 1 (cid:0) e(cid:0)(cid:24) the following holds:
2 ! ;
pn  1 + r (cid:24)
(cid:13)(cid:13)(cid:13)PSD (cid:0) P bSD (cid:13)(cid:13)(cid:13) (cid:20)   DXr=1 e(cid:14)(cid:0)1
r ! 4M
provided that n (cid:21) 16M 2 (cid:18)1 + q (cid:24)
2 (cid:19)2
(sup1(cid:20)r(cid:20)D e(cid:14)(cid:0)2
r ) . The disadvantage of this bound
is that we are penalized on the one hand by the (inverse) gaps between the eigenvalues, and
on the other by the dimension D (because we have to sum the inverse gaps from 1 to D).
In the next section we improve the operator perturbation bound to get an improved result
where only the gap (cid:14)D enters into account.

3 Improved Result.

We ﬁrst prove the following variant on the operator perturbation property which better cor-
responds to our needs by taking directly into account the projection on the ﬁrst D eigen-
vectors at once. The proof uses the same kind of techniques as in [7].

:

(1)

Theorem 3 Let A be a symmetric positive Hilbert-Schmidt operator of the Hilbert space
H with simple nonzero eigenvalues (cid:21)1 > (cid:21)2 > : : : Let D > 0 be an integer such that
(cid:21)D > 0, (cid:14)D = 1
2 ((cid:21)D (cid:0) (cid:21)D+1 ). Let B 2 H S (H) be another symmetric operator such that
kB k < (cid:14)D =2 and (A + B ) is still a positive operator. Let P D (A) (resp. P D (A + B ))
denote the orthogonal projector onto the subspace spanned by the ﬁr st D eigenvectors A
(resp. (A + B )). Then these satisfy:
kP D (A) (cid:0) P D (A + B )k (cid:20) kB k
(cid:14)D
This then gives rise to our main result on KPCA:
Theorem 4 Assume that supx2X k(x; x) (cid:20) M . Let SD ; bSD be the subspaces spanned
by the ﬁr st D eigenvectors of C , resp. Cn deﬁned earlier. Denoting (cid:21)1 > (cid:21)2 > : : : the
eigenvalues of C , if D > 0 is such that (cid:21)D > 0, put (cid:14)D = 1
2 ((cid:21)D (cid:0) (cid:21)D+1 ) and
2 ! :
(cid:14)D  1 + r (cid:24)
2M
BD =
Then provided that n (cid:21) B 2
D , the following bound holds with probability at least 1 (cid:0) e(cid:0)(cid:24) :
(cid:13)(cid:13)(cid:13)PSD (cid:0) P bSD (cid:13)(cid:13)(cid:13) (cid:20)
BDpn
(2)
:
This entails in particular
2 kgkHk o :
bSD (cid:26) ng + h; g 2 SD ; h 2 S?
D ; khkHk (cid:20) 2BD n(cid:0) 1
The important point here is that the approximation error now only depends on D through
the (inverse) gap between the D-th and (D + 1)-th eigenvalues. Note that using the results
of section 2, we would have obtained exactly the same bound for estimating the D-th
eigenvector only – or even a worse bound since e(cid:14)D = (cid:14)D ^ (cid:14)D(cid:0)1 appears in this case.
Thus, at least from the point of view of this technique (which could still yield suboptimal

(3)

bounds), there is no increase of complexity between estimating the D-th eigenvector alone
and estimating the span of the ﬁrst D eigenvectors.
Note that the inclusion (3) can be interpreted geometrically by saying that for any vector in
bSD , the tangent of the angle between this vector and its projection on SD is upper bounded
by BD =pn, which we can interpret as a stability property.
Comment about the Centered Case.
In the actual (K)PCA procedure, the data is actu-
ally ﬁrst empirically recentered, so that one has to consider the centered covariance operator
C and its empirical counterpart C n . A result similar to Theorem 4 also holds in this case
(up to some additional constant factors). Indeed, a result similar to Lemma 1 holds for the
recentered operators [2]. Combined again with Theorem 3, this allows to come to similar
conclusions for the “true”
centered KPCA.

4 Conclusion and Discussion

In this paper, ﬁnite sample size con ﬁdence bounds of the eigenspaces of Kernel-PCA (the
D-eigenspaces of the empirical covariance operator) are provided using tools of operator
perturbation theory. This provides a ﬁrst
step towards an in-depth complexity analysis of
algorithms using KPCA as pre-processing, and towards taking into account the randomness
of the obtained models (e.g. [3]). We proved a bound in which the complexity factor for
estimating the eigenspace SD by its empirical counterpart depends only on the inverse gap
between the D-th and (D + 1)-th eigenvalues. In addition to the previously cited works,
we take into account the centering of the data and obtain comparable rates.

In this work we assumed for simplicity of notation the eigenvalues to be simple. In the case
the covariance operator C has nonzero eigenvalues with multiplicities m1 ; m2 ; : : : possibly
larger than one, the analysis remains the same except for one point: we have to assume that
the dimension D of the subspaces considered is of the form m1 + (cid:1) (cid:1) (cid:1) + mr for a certain
r . This could seem restrictive in comparison with the results obtained for estimating the
sum of the ﬁrst D eigenvalues themselves [2] (which is linked to the reconstruction error
in KPCA) where no such restriction appears. However, it should be clear that we need
this restriction when considering D(cid:0)eigenspaces themselves since the target space has to
be unequivocally deﬁned, otherwise convergence cannot occur. Thus, it can happen in
this special case that the reconstruction error converges while the projection space itself
does not. Finally, a common point of the two analyses (over the spectrum and over the
eigenspaces) lies in the fact that the bounds involve an inverse gap in the eigenvalues of the
true covariance operator.

Finally, how tight are these bounds and do they at least carry some correct qualitative infor-
mation about the behavior of the eigenspaces? Asymptotic results (central limit Theorems)
in [6, 4] always provide the correct goal to shoot for since they actually give the limit distri-
butions of these quantities. They imply that there is still important ground to cover before
bridging the gap between asymptotic and non-asymptotic. This of course opens directions
for future work.
Acknowledgements: This work was supported in part by the PASCAL Network of Excel-
lence (EU # 506778).

A Appendix: proofs.

Proof of Lemma 1. This lemma is proved in [9]. We give a short proof for the sake of
n Pn
completness. kCn (cid:0) C k = k 1
i=1 CXi (cid:0) E [CX ] k with kCX k = k’(X ) (cid:10) ’(X )(cid:3) k =
k(X; X ) (cid:20) M . We can apply the bounded difference inequality to the variable kCn (cid:0) C k,

so that with probability greater than 1 (cid:0) e(cid:0)(cid:24) , kCn (cid:0) C k (cid:20) E [kCn (cid:0) C k] + 2M q (cid:24)
2n :
Moreover, by Jensen’s inequality E [kCn (cid:0) C k] (cid:20) E (cid:2)k 1
i=1 CXi (cid:0) E [CX ] k2 (cid:3) 1
n Pn
2 ; and
simple calculations leads to E (cid:2)k 1
E (cid:2)kCX (cid:0) E [CX ] k2 (cid:3) (cid:20)
i=1 CXi (cid:0) E [CX ] k2 (cid:3) = 1
n Pn
n
4M 2
n : This concludes the proof of lemma 1.
(cid:3)
Proof of Theorem 3. The variation of this proof with respect to Theorem 5.2 in [7] is (a)
to work directly in a (inﬁnite-dimensional) Hilbert space, requiring extra caution for some
details and (b) obtaining an improved bound by considering D-eigenspaces at once.
The key property of Hilbert-Schmidt operators allowing to work directly in a in ﬁnite di-
mensional setting is that H S (H) is a both right and left ideal of Lc (H; H), the Banach
space of all continuous linear operators of H endowed with the operator norm k:kop . In-
deed, 8 T 2 H S (H); 8S 2 Lc (H; H); T S and ST belong to H S (H) with
(4)
kT S k (cid:20) kT k kS kop and kST k (cid:20) kT k kS kop :
The spectrum of an Hilbert-Schmidt operator T is denoted (cid:3)(T ) and the sequence of eigen-
values in non-increasing order is denoted (cid:21)(T ) = ((cid:21)1 (T ) (cid:21) (cid:21)2 (T ) (cid:21) : : :) . In the follow-
ing, P D (T ) denotes the orthogonal projector onto the D-eigenspace of T .
The Hoffmann-Wielandt inequality in in ﬁnite dimensional setting[1] yields that:

k(cid:21)(A) (cid:0) (cid:21)(A + B )k‘2 (cid:20) kB k (cid:20)
implying in particular that

(cid:14)D
2

:

(5)

(cid:14)D
2

:

(6)

(7)

j(cid:21)i (A) (cid:0) (cid:21)i (A + B )j (cid:20)
8i > 0;
Results found in [5] p.39 yield the formula
2i(cid:25) Z(cid:13)
1
P D (A) (cid:0) P D (A + B ) = (cid:0)
(RA (z ) (cid:0) RA+B (z ))dz 2 Lc (H; H) :
where RA (z ) = (A (cid:0) z I d)(cid:0)1 is the resolvent of A, provided that (cid:13) is a simple closed
curve in C enclosing exactly the ﬁrst D eigenvalues of A and (A + B ). Moreover, the same
reference (p.60) states that for (cid:24) in the complementary of (cid:3)(A),
kRA ((cid:24) )kop = dist((cid:24) ; (cid:3)(A))(cid:0)1 :
The proof of the theorem now relies on the simple choice for the closed curve (cid:13) in (7),
drawn in the picture below and consisting of three straight lines and a semi-circle of radius
L. For all L > (cid:14)D
2 , (cid:13) intersect neither the eigenspectrum of A (by equation (6)) nor the
eigenspectrum of A + B . Moreover, the eigenvalues of A (resp. A + B ) enclosed by (cid:13) are
exactly (cid:21)1 (A); : : : ; (cid:21)D (A) (resp. (cid:21)1 (A + B ); : : : ; (cid:21)D (A + B )).
Moreover, for z 2 (cid:13) , T (z ) = RA (z ) (cid:0) RA+B (z ) = (cid:0)RA+B (z )BRA (z ) belongs to
H S (H) and depends continuously on z by (4). Consequently,
2(cid:25) Z b
1
a k(RA (cid:0) RA+B )((cid:13) (t))k j(cid:13) 0 (t)jdt :
kP D (A) (cid:0) P D (A + B )k (cid:20)
Let SN = PN
n=0 ((cid:0)1)n (RA (z )B )nRA (z ). RA+B (z ) = (I d + RA (z )B )(cid:0)1RA (z ) and,
for z 2 (cid:13) and L > (cid:14)D ,
kRA (z )B kop (cid:20) kRA (z )kop kB k (cid:20)

(cid:14)D
2 dist(z ; (cid:3)(A)) (cid:20)

(8)

1
2

;

g

L

d

D

d

D

l

D

l

2

0

l

D+1

d

D
2

d
D
2

L

L

l

1

d

D
2

k:kop(cid:0)! RA+B (z ) (uniformly for z 2 (cid:13) ). Using property (4), since B 2
imply that SN
k:k
H S (H), SN BRA (z )
(cid:0)! RA+B (z )BRA (z ) = RA+B (z ) (cid:0) RA (z ) : Finally,
RA (z ) (cid:0) RA+B (z ) = Xn(cid:21)1
((cid:0)1)n (RA (z )B )nRA (z )
where the series converges in H S (H), uniformly in z 2 (cid:13) . Using again property (4) and
(8) implies
k(RA (cid:0) RA+B )((cid:13) (t))k (cid:20) Xn(cid:21)1
op kB kn (cid:20) Xn(cid:21)1
kB kn
kRA ((cid:13) (t))kn+1
distn+1 ((cid:13) (t); (cid:3)(A))
2 (cid:20) dist((cid:13) (t);(cid:3)(A))
Finally, since for L > (cid:14)D , kB k (cid:20) (cid:14)D
,
2
(cid:25) Z b
1
kP D (A) (cid:0) P D (A + B )k (cid:20) kB k
dist2 ((cid:13) (t); (cid:3)(A)) j(cid:13) 0 (t)jdt :
a
Splitting the last integral into four parts according to the deﬁnition of the contour (cid:13) , we
obtain
Z b
2arctan( L
1
(cid:14)D
dist2 ((cid:13) (t); (cid:3)(A)) j(cid:13) 0 (t)jdt (cid:20)
(cid:14)D
a
and letting L goes to inﬁnity leads to the result.
Proof of Theorem 4. Lemma 1 and Theorem 3 yield inequality (2). Together with as-
2 . Let f 2 bSD : f = PSD (f ) + PS?
D it implies kPSD (cid:0) P bSD k (cid:20) 1
(f ) .
sumption n (cid:21) B 2
D
Lemma 5 below with F = SD and G = bSD , and the fact that the operator norm is bounded
by the Hilbert-Schmidt norm imply that
4
(f )k2
3 kPSD (cid:0) P bSD k2 kPSD (f )k2
Hk (cid:20)
kPS?
Hk :
D
Gathering the different inequalities, Theorem 4 is proved.
(cid:3)
Lemma 5 Let F and G be two vector subspaces of H such that kPF (cid:0) PGkop (cid:20) 1
2 . Then
the following bound holds:
4
8 f 2 G ; kPF ? (f )k2
op kPF (f )k2
3 kPF (cid:0) PG k2
H (cid:20)
H :

(cid:22)1 (A) (cid:0) ((cid:22)D (A) (cid:0) (cid:14)D )
L2

(cid:25)
L

+

+ 2

)

;

(cid:3)

Proof of Lemma 5. For f 2 G, we have PG (f ) = f , hence
kPF ? (f )k2 = kf (cid:0) PF (f )k2 = k(PG (cid:0) PF )(f )k2
op kf k2
(cid:20) kPF (cid:0) PGk2
op (cid:0)kPF (f )k2 + kPF ? (f )k2 (cid:1)
= kPF (cid:0) PGk2
gathering the terms containing kPF ? (f )k2 on the left-hand side and using kPF (cid:0) PG k2
op (cid:20)
1=4 leads to the conclusion.
(cid:3)

References

[1] R. Bhatia and L. Elsner. The Hoffman-Wielandt inequality in in ﬁnite dimensions.
Proc.Indian Acad.Sci(Math. Sci.) 104 (3), p. 483-494, 1994.
[2] G. Blanchard, O. Bousquet, and L. Zwald. Statistical Properties of Kernel Princi-
pal Component Analysis. Proceedings of the 17th. Conference on Learning Theory
(COLT 2004), p. 594–608. Springer, 2004.
[3] G. Blanchard, P. Massart, R. Vert, and L. Zwald. Kernel projection machine: a new
tool for pattern recognition. Proceedings of the 18th. Neural Information Processing
System (NIPS 2004), p. 1649–1656. MIT Press, 2004.
[4] J. Dauxois, A. Pousse, and Y. Romain. Asymptotic theory for the Principal Compo-
nent Analysis of a vector random function: some applications to statistical inference.
Journal of multivariate analysis 12, 136-154, 1982.
[5] T. Kato. Perturbation Theory for Linear Operators. New-York: Springer-Verlag,
1966.
[6] V. Koltchinskii. Asymptotics of spectral projections of some random matrices ap-
proximating integral operators. Progress in Probability, 43:191–227, 1998.
[7] V. Koltchinskii and E. Gin ´e. Random matrix approximation of spectra of integral
operators. Bernoulli, 6(1):113–167, 2000.
[8] B. Sch ¨olkopf, A. J. Smola, and K.-R. M ¨uller. Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998.
[9] J. Shawe-Taylor and N. Cristianini. Estimating the moments of a random vector with
applications. Proceedings of the GRETSI 2003 Conference, p. 47-52, 2003.
[10] J. Shawe-Taylor, C. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum
of the Gram matrix and the generalisation error of Kernel PCA. IEEE Transactions
on Information Theory 51 (7), p. 2510-2522, 2005.
[11] U. von Luxburg, M. Belkin, and O. Bousquet. Consistency of spectral clustering.
Technical Report 134, Max Planck Institute for Biological Cybernetics, 2004.
[12] U. von Luxburg, O. Bousquet, and M. Belkin. On the convergence of spectral clus-
tering on random samples:
the normalized case. Proceedings of the 17th Annual
Conference on Learning Theory (COLT 2004), p. 457–471. Springer, 2004.
[13] C. K. I. Williams and M. Seeger. The effect of the input density distribution on
kernel-based classiﬁers. Proceedings of the 17th International Conference on Ma-
chine Learning (ICML), p. 1159–1166. Morgan Kaufmann, 2000.


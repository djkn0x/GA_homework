Asymptotics of Gaussian Regularized
Least-Squares

Ross A. Lippert
M.I.T., Department of Mathematics
77 Massachusetts Avenue
Cambridge, MA 02139-4307
lippert@math.mit.edu

Ryan M. Rifkin
Honda Research Institute USA, Inc.
145 Tremont Street
Boston, MA 02111
rrifkin@honda-ri.com

Abstract

We consider regularized least-squares (RLS) with a Gaussian kernel. We
prove that if we let the Gaussian bandwidth σ → ∞ while letting the
regularization parameter λ → 0, the RLS solution tends to a polynomial
whose order is controlled by the rielative rates of decay of 1
σ2 and λ: if
λ = σ−(2k+1) , then, as σ → ∞, the RLS solution tends to the k th order
polynomial with minimal empirical error. We illustrate the result with an
example.

1

Introduction

Given a data set (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ), the inductive learning task is to build a
function f (x) that, given a new x point, can predict the associated y value. We study the
Regularized Least-Squares (RLS) algorithm for ﬁnding f , a common and popular algo-
nX
rithm [2, 5] that can be used for either regression or classiﬁcation:
1
(f (xi ) − yi )2 + λ||f ||2
min
K .
f ∈H
n
i=1
Here, H is a Reproducing Kernel Hilbert Space (RKHS) [1] with associated kernel function
K , ||f ||2
K is the squared norm in the RKHS, and λ is a regularization constant controlling
Pn
the tradeoff between ﬁtting the training set accurately and forcing smoothness of f .
The Representer Theorem [7] proves that the RLS solution will have the form f (x) =
i=1 ciK (xi , x), and it is easy to show [5] that we can ﬁnd the coefﬁcients
c by solving
the linear system

(K + λnI )c = y ,
(1)
where K is the n by n matrix satisfying Kij = K (xi , xj ). We focus on the Gaussian kernel
K (xi , xj ) = exp(−||xi − xj ||2 /2σ2 ).
Our work was originally motivated by the empirical observation that on a range of bench-
mark classiﬁcation tasks, we achieved surprisingly accurate classiﬁcation using a Gaussian
kernel with a very large σ and a very small λ (Figure 1; additional examples in [6]). This
prompted us to study the large-σ asymptotics of RLS. As σ → ∞, K (xi , xj ) → 1 for
arbitrary xi and xj . Consider a single test point x0 . RLS will ﬁrst ﬁnd
c using Equation 1,

Fig. 1. RLS classiﬁcation accuracy results for the UCI Galaxy dataset over a range of σ (along the
x-axis) and λ (different lines) values. The vertical labelled lines show m, the smallest entry in the
kernel matrix for a given σ . We see that when λ = 1e − 11, we can classify quite accurately when
the smallest entry of the kernel matrix is .99999.

then compute f (x0 ) = ctk where k is the kernel vector, ki = K (xi , x0 ). Combining the
training and testing steps, we see that f (x0 ) = y t (K + λnI )−1k .
Both K and k are close to 1 for large σ , i.e. Kij = 1 + ij and ki = 1 + i . If we directly
compute c = (K + λnI )−1 y , we will tend to wash out the effects of the ij term as σ
becomes large. If, instead, we compute f (x0 ) by associating to the right, ﬁrst computing
point afﬁnities (K + λnI )−1k , then the ij and j interact meaningfully; this interaction is
crucial to our analysis.
Our approach is to Taylor expand the kernel elements (and thus K and k) in 1/σ , noting
that as σ → ∞, consecutive terms in the expansion differ enormously. In computing (K +
λnI )−1k , these scalings cancel each other out, and result in ﬁnite point afﬁnities even as
σ → ∞. The asymptotic afﬁnity formula can then be “transposed ” to create an alternate
expression for f (x0 ). Our main result is that if we set σ2 = s2 and λ = s−(2k+1) , then, as
s → ∞, the RLS solution tends to the k th order polynomial with minimal empirical error.
The main theorem is proved in full. Due to space restrictions, the proofs of supporting
lemmas and corollaries are omitted; an expanded version containing all proofs is available
[4].

2 Notation and deﬁnitions
Deﬁnition 1. Let xi be a set of n + 1 points (0 ≤ i ≤ n) in a d dimensional space. The
scalar xia denotes the value of the ath vector component of the ith point.

1e−041e−011e+021e+050.40.60.81.01.21e−111e−081e−050.011010000RLSC Results for GALAXY DatasetSigmaAccuracym=1.0d−249m=0.9m=0.99999The n × d matrix, X is given by Xia = xia .
We think of X as the matrix of training data x1 , . . . , xn and x0 as an 1× d matrix consisting
of the test point.
Let 1m , 1lm denote the m dimensional vector and l × m matrix with components all 1,
similarly for 0m , 0lm . We will dispense with such subscripts when the dimensions are clear
from context.
Deﬁnition 2 (Hadamard products and powers). For two l × m matrices, N , M , N (cid:12) M
denotes the l × m matrix given by (N (cid:12) M )ij = NijMij . Analogously, we set (N (cid:12)c )ij =
N c
ij .
Y be a k × d matrix. Y I is the k dimensional vector given by (cid:0)Y I (cid:1)
i = Qd
Deﬁnition 3 (polynomials in the data). Let I ∈ Zd≥0 (non-negative multi-indices) and
a=1 Y Ia
ia . If
h : Rd → R then h(Y ) is the k dimensional vector given by (h(Y ))i = h(Yi1 , . . . , Yid ).
The d canonical vectors, ea ∈ Zd≥0 , are given by (ea )b = δab .
Any scalar function, f : R → R, applied to any matrix or vector, A, will be assumed to
denote the elementwise application of f . We will treat y → ey as a scalar function (we
have no need of matrix exponentials in this work, so the notation is unambiguous).
Pd
We can re-express the kernel matrix and kernel vector in this notation:
(cid:16)
2σ2 ||X ||2 (cid:17)
(cid:16)
2σ2 ||X ||2 (cid:17)
n−1n (X 2ea )t
a=1 2X ea (X ea )t−X 2ea 1t
1
2σ2
− 1
− 1
1
σ2 XX t
Pd
diag
e
e
e
(cid:16)
2σ2 ||X ||2 (cid:17)
0 −X 2ea 11−1n x2ea
a=1 2X ea xea
k = e
0
− 1
2σ2 ||x0 ||2
− 1
1
σ2 X xt
= diag
e
0 e
e
Let Vc = span{X I : |I | = c} and V≤c = Sc
3 Orthogonal polynomial bases
a=0 Vc which can be thought of as the set of all
(cid:18) c + d
(cid:19)
d variable polynomials of degree c, evaluated on the training data. Since the data are ﬁnite,
there exists b such that V≤c = V≤b for all c ≥ b. Generically, b is the smallest c such that
≥ n.
d
Let Q be an orthonormal matrix in Rn×n whose columns progressively span the V≤c
· · · Bc )} =
· · · Bb ) where QtQ = I and colspan{( B0
spaces, i.e. Q = ( B0 B1
V≤c . We might imagine building such a Q via the Gramm-Schmidt process on the vectors
(cid:18) |I |
(cid:19)
X 0 , X e1 , . . . , X ed , . . . X I , . . . taken in order of non-decreasing |I |.
Letting CI =
0 )(cid:12)c = X
I1 . . . Id
Q, X , and x0 are easily proved.
(XX t )(cid:12)c = X
CI X I (xI
(X xt
0 )t
|I |=c
|I |=c

be multinomial coefﬁcients, the following relations between

colspan{(XX t )(cid:12)c} = Vc

K = e
= diag

0 )(cid:12)c ∈ Vc
(X xt

(2)

(3)

(4)

(5)

1
2σ2

.

hence

CI X I (X I )t

hence

0 )(cid:12)c = 0 if i > c, B t
i (XX t )(cid:12)cBj = 0 if i > c or j > c, and
{||y − v ||} = P
i (X xt
and thus, B t
c (XX t )(cid:12)cBc is non-singular.
B t
a≤c Ba (B t
a y).
Finally, we note that argminv∈V≤c
4 Taking the σ → ∞ limit

A(s) =

We will begin with a few simple lemmas about the limiting solutions of linear systems.
At the end of this section we will arrive at the limiting form of suitably modiﬁed RLSC
equations.
Lemma 1. Let i1 < · · · < iq be positive integers. Let A(s), y(s) be a block matrix and

 b0 (s)
 ,
 A00 (s)
block vector given by
· · ·
siq A0q (s)
si1 A01 (s)
· · ·
si1 b1 (s)
siq A1q (s)
si1 A11 (s)
si1 A10 (s)
y(s) =
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
siq bq (s)
siq Aqq (s)
siq Aq1 (s)
siq Aq0 (s)
where Aij (s) and bi (s) are continuous matrix-valued and vector-valued functions of s with

−1  b0 (0)
 A00 (0)
Aii (0) non-singular for all i.
0
A10 (0) A11 (0)
b1 (0)
· · ·
· · ·
· · ·
Aq0 (0) Aq1 (0)
bq (0)

· · ·
0
· · ·
0
· · ·
· · ·
· · · Aqq (0)

A−1 (s)y(s) =

lim
s→0

We are now ready to state and prove the main result of this section, characterizing the
limiting large-σ solution of Gaussian RLS.

where

Theorem 1. Let q be an integer satisfying q < b, and let p = 2q + 1. Let λ = C σ−p for
0 )(cid:12)c .
(cid:0)K + nC σ−p I (cid:1)−1
i (XX t )(cid:12)cBj , and b(c)
some constant C . Deﬁne A(c)
ij = 1
i (X xt
i = 1
c! B t
c! B t
lim
σ→∞
 A(0)
 =
 b(0)
· · · Bq ) w
v = ( B0
· · ·
0
0
00
0
· · ·
10 A(1)
A(1)
0
b(1)
· · ·
· · ·
· · ·
· · ·
1· · ·
11
· · · A(q)
q0 A(q)
A(q)
b(q)
qq
q
q1

 w

k = v

(7)

(6)

We ﬁrst manipulate the equation (K + nλI )y = k according to the factorizations in (3)
(cid:16)
2σ2 ||X ||2 (cid:17)
2σ2 ||X ||2 (cid:17)
(cid:16)
and (5).
(cid:16)
2σ2 ||X ||2 (cid:17)
− 1
− 1
= N P N
diag
e
e
− 1
− 1
2σ2 ||x0 ||2 = N wα
1
σ2 X xt
e
0 e

K = diag

k = diag

1
σ2 XX t

e

e

diag

e

(cid:16)

(cid:16)

= limσ→∞ αN −1 = I ,

− 1
2σ2 ||x0 ||2
Noting that limσ→∞ e
we have

2σ2 ||X ||2 (cid:17)
1
v ≡ lim
σ→∞(K + nC σ−p I )−1k
σ→∞(N P N + β I )−1N wα
= lim
σ2 ||X ||2 (cid:17)(cid:17)−1
(cid:16)
σ→∞ αN −1 (P + βN −2 )−1w
= lim
σ2 XX t + nC σ−pdiag
1
1
1
σ2 X xt
= lim
0 .
e
e
e
σ→∞
(cid:17)−1
σ2 ||X ||2 (cid:17)
(cid:16)
(cid:16)
Changing bases with Q,
Q + nC σ−pQtdiag
1
1
1
σ2 X xt
σ2 XX t
Qt v = lim
Qt e
Qt e
Q
e
0 .
σ→∞
Expanding via Taylor series and writing in block form (in the b × b block structure of Q),
 + · · ·
 A(1)
1
1
 A(0)
 +
2!σ4 Qt (XX t )(cid:12)2Q + · · ·
1!σ2 Qt (XX t )(cid:12)1Q +
Q = Qt (XX t )(cid:12)0Q +
1
σ2 XX t
Qt e
· · ·
· · ·
00 A(1)
0
0
0
01
· · ·
00
· · ·
1
0
0
0
10 A(1)
A(1)
0
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
11
σ2
· · ·
· · ·
0
0
0
0
0
0
 + · · ·
 b(1)
1
1
 b(0)
 +
0 )(cid:12)2 + · · ·
0 )(cid:12)0 +
0 )(cid:12)1 +
1
σ2 X xt
0 = Qt (X xt
σ4 Qt (X xt
σ2 Qt (X xt
0
0
1
0· · ·
b(1)
1· · ·
σ2
(cid:16)
σ2 ||X ||2 (cid:17)
0
0
Q = nC σ−p I + · · · .
nC σ−pQtdiag
1
Since the A(c)
cc are non-singular, Lemma 3 applies, giving our result.

Qt e

=

=

e

ut

5 The classiﬁcation function

When performing RLS, the actual prediction of the limiting classiﬁer is given via
f∞ (x0 ) ≡ lim
σ→∞ y t (K + nC σ−p I )−1k .
Theorem 1 determines v = limσ→∞ (K + nC σ−p I )−1k ,showing that f∞ (x0 ) is a polyno-
mial in the training data X . In this section, we show that f∞ (x0 ) is, in fact, a polynomial
in the test point x0 . We continue to work with the orthonormal vectors Bi as well as the
auxilliary quantities A(c)
ij and b(c)
from Theorem 1.
i
Theorem 1 shows that v ∈ V≤q : the point afﬁnity function is a polynomial of degree q in
X
X
the training data, determined by (7).
j = (XX t )(cid:12)c
X
c!BiA(c)
ij B t
i,j≤c
j≤c
0 )(cid:12)c
c!Bc b(c)
c (X xt
i = BcB t
i≤c

c (XX t )(cid:12)c
c!BcA(c)
j = BcB t
cj B t

0 )(cid:12)c
c!Bi b(c)
i = (X xt

hence

hence

 0!A(0)
 −
 0!b(0)
 = 0
t 

 B t
 v
 B t
we can restate Equation 7 in an equivalent form:
· · ·
0
0
00
0
· · ·
0· · ·
0· · ·
1!A(1)
1!A(1)
0
1!b(1)
· · ·
· · ·
· · ·
· · ·
1· · ·
10
11
c − X
X
X
B t
B t
· · ·
q
q
q !A(q)
q !A(q)
q !A(q)
q !b(q)
qq
q
q0
q1
X
c!BcA(c)
(cid:0)(X xt
0 )(cid:12)c − (XX t )(cid:12)c v(cid:1) = 0.
c!Bc b(c)
j v = 0
cj B t
j≤c
c≤q
c≤q
c≤q
Up to this point, our results hold for arbitrary training data X . To proceed, we require a
mild condition on our training set.

BcB t
c

(10)

(8)

(9)

Deﬁnition 4. X is called generic if X I1 , . . . , X In are linearly independent for any distinct
multi-indices {Ii }.

Lemma 2. For generic X , the solution to Equation 7 (or equivalently, Equation 10) is
determined by the conditions ∀I : |I | ≤ q , (X I )t v = xI
0 , where v ∈ V≤q .
f (x0 ) = y t v = h(x0 ), where h(x) = P|I |≤q aI xI is a multivariate polynomial of degree
Theorem 2. For generic data, let v be the solution to Equation 10. For any y ∈ Rn ,
q minimizing ||y − h(X )||.
We see that as σ → ∞, the RLS solution tends to the minimum empirical error k th order
polynomial.

6 Experimental Veri ﬁcation

In this section, we present a simple experiment that illustrates our results. We consider
a ﬁth-degree polynomial function. Figure 2 plots f , along with a 150 point dataset drawn
by choosing xi uniformly in [0, 1], and choosing y = f (x) + i , where i is a Gaussian
random variable with mean 0 and standard deviation .05. Figure 2 also shows (in red) the
best polynomial approximations to the data (not to the ideal f ) of various orders. (We omit
third order because it is nearly indistinguishable from second order.)

According to Theorem 1, if we parametrize our system by a variable s, and solve a Gaussian
regularized least-squares problem with σ2 = s2 and λ = C s−(2k+1) for some integer
k , then, as s → ∞, we expect the solution to the system to tend to the k th-order data-
based polynomial approximation to f . Asymptotically, the value of the constant C does
not matter, so we (arbitrarily) set it to be 1. Figure 3 demonstrates this result.

We note that these experiments frequently require setting λ much smaller than machine-
. As a consequence, we need more precision than IEEE double-precision ﬂoating-point,
and our results cannot be obtained via many standard tools (e.g., MATLAB(TM)) We per-
formed our experiments using CLISP, an implementation of Common Lisp that includes
arithmetic operations on arbitrary-precision ﬂoating point numbers.

7 Discussion

Our result provides insight into the asymptotic behavior of RLS, and (partially) explains
Figure 1: in conjunction with additional experiments not reported here, we believe that

Fig. 2. f (x) = .5(1 − x) + 150x(x − .25)(x − .3)(x − .75)(x − .95), a random dataset drawn from
f (x) with added Gaussian noise, and data-based polynomial approximations to f .

we are recovering second-order polynomial behavior, with the drop-off in performance at
various λ’s occurring at the transition to third-order behavior, which cannot be accurately
recovered in IEEE double-precision ﬂoating-point. Although we used the speciﬁc details
of RLS in deriving our solution, we expect that in practice, a similar result would hold for
Support Vector Machines, and perhaps for Tikhonov regularization with convex loss more
generally.

An interesting implication of our theorem is that for very large σ , we can obtain various
order polynomial classiﬁcations by sweeping λ. In [6], we present an algorithm for solving
for a wide range of λ for essentially the same cost as using a single λ. This algorithm is not
currently practical for large σ , due to the need for extended-precision ﬂoating point.

Our work also has implications for approximations to the Gaussian kernel. Yang et al. use
the Fast Gauss Transform (FGT) to speed up matrix-vector multiplications when perform-
ing RLS [8]. In [6], we studied this work; we found that while Yang et al. used moderate-to-
small values of σ (and did not tune λ), the FGT sacriﬁced substantial accuracy compared
to the best achievable results on their datasets. We showed empirically that the FGT be-
comes much more accurate at larger values of σ ; however, at large-σ , it seems likely we
are merely recovering low-order polynomial behavior. We suggest that approximations to
the Gaussian kernel must be checked carefully, to show that they produce sufﬁciently good
results are moderate values of σ ; this is a topic for future work.

References

1. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society,
68:337–404, 1950.

2. Evgeniou, Pontil, and Poggio. Regularization networks and support vector machines. Advances
In Computational Mathematics, 13(1):1–50, 2000.

0.00.20.40.60.81.0−0.4−0.20.00.20.40.60.8xyllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllf0th order1st order2nd order4th order5th orderf(x), Random Sample of f(x), and Polynomial ApproximationsFig. 3. As s → ∞, σ2 = s2 and λ = s−(2k+1) , the solution to Gaussian RLS approaches the k th
order polynomial solution.

3. Keerthi and Lin. Asymptotic behaviors of support vector machines with gaussian kernel. Neural
Computation, 15(7):1667–1689, 2003.
4. Ross Lippert and Ryan Rifkin. Asymptotics of gaussian regularized least-squares. Technical
Report MIT-CSAIL-TR-2005-067, MIT Computer Science and Artiﬁcial Intelligence Laboratory,
2005.
5. Rifkin. Everything Old Is New Again: A Fresh Look at Historical Approaches to Machine Learn-
ing. PhD thesis, Massachusetts Institute of Technology, 2002.
6. Rifkin and Lippert.
Practical regularized least-squares: λ-selection and fast leave-one-out-
computation. In preparation, 2005.
7. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Conference
Series in Applied Mathematics. Society for Industrial & Applied Mathematics, 1990.
8. Yang, Duraiswami, and Davis. Efﬁcient kernel machines using the improved fast Gauss transform.
In Advances in Neural Information Processing Systems, volume 16, 2004.

0.00.20.40.60.81.0−0.4−0.20.00.20.40.60.80th order solution, and successive approximations.   Deg. 0 polynomials = 1.d+1s = 1.d+2s = 1.d+30.00.20.40.60.81.0−0.4−0.20.00.20.40.60.81st order solution, and successive approximations.  Deg. 1 polynomials = 1.d+1s = 1.d+20.00.20.40.60.81.0−0.4−0.20.00.20.40.60.84th order solution, and successive approximations.    Deg. 4 polynomials = 1.d+1s = 1.d+2s = 1.d+3s = 1.d+40.00.20.40.60.81.0−0.4−0.20.00.20.40.60.85th order solution, and successive approximations.    Deg. 5 polynomials = 1.d+1s = 1.d+3s = 1.d+5s = 1.d+6
Kernelized Infomax Clustering

Felix V. Agakov
Edinburgh University
Edinburgh EH1 2QL, U.K.
felixa@inf.ed.ac.uk

David Barber
IDIAP Research Institute
CH-1920 Martigny Switzerland
david.barber@idiap.ch

Abstract

We propose a simple information-theoretic approach to soft clus-
tering based on maximizing the mutual information I (x, y) between
the unknown cluster labels y and the training patterns x with re-
spect to parameters of speciﬁcally constrained encoding distribu-
tions. The constraints are chosen such that patterns are likely to
be clustered similarly if they lie close to speciﬁc unknown vectors
in the feature space. The method may be conveniently applied to
learning the optimal aﬃnity matrix, which corresponds to learn-
ing parameters of the kernelized encoder. The procedure does not
require computations of eigenvalues of the Gram matrices, which
makes it potentially attractive for clustering large data sets.

1 Introduction

Let x ∈ R|x| be a visible pattern, and y ∈ {y1 , . . . , y|y | } its discrete unknown cluster
label. Rather than learning a density model of the observations, our goal here will
be to learn a mapping x → y from the observations to the latent codes (cluster
labels) by optimizing a formal measure of coding eﬃciency. Good codes y should
be in some way informative about the underlying high-dimensional source vectors x,
so that the useful information contained in the sources is not lost. The fundamental
measure in this context is the mutual information
I (x, y) def= H (x) − H (x|y) ≡ H (y) − H (y |x),

(1)

which indicates the decrease in uncertainty about the pattern x due to the knowl-
edge of the underlying cluster label y (e.g. Cover and Thomas (1991)). Here
H (y) ≡ −hlog p(y)ip(y) and H (y |x) ≡ −hlog p(y |x)ip(x,y) are marginal and condi-
tional entropies respectively, and the brackets h. . .ip represent averages over p. In
our case the encoder model is deﬁned as

p(x, y) ∝

M
X
m=1

δ(x − x(m) )p(y |x),

(2)

where {x(m) |m = 1, . . . , M } is a set of training patterns.

Our goal is to maximize (1) with respect to parameters of a constrained encod-
ing distribution p(y |x). In contrast to most applications of the infomax principle

(Linsker (1988)) in stochastic channels (e.g. Brunel and Nadal (1998); Fisher and
Principe (1998); Torkkola and Campbell (2000)), optimization of the ob jective (1)
is computationally tractable since the cardinality of the code space |y | (the number
of clusters) will typically be low. Indeed, had the code space been high-dimensional,
computation of I (x, y) would have required evaluation of the generally intractable
entropy of the mixture H (y), and approximations would have needed to be consid-
ered (e.g. Barber and Agakov (2003); Agakov and Barber (2006)).

Maximization of the mutual information with respect to parameters of the encoder
model eﬀectively deﬁnes a discriminative unsupervised optimization framework,
where the model is parameterized similarly to a conditionally trained classiﬁer, but
where the cluster allocations are generally unknown. Training such models p(y |x)
by maximizing the likelihood p(x) would be meaningless, as the cluster variables
would marginalize out, which motivates also our information theoretic approach.
In this way we may extract soft cluster allocations directly from the training set,
with no additional information about class labels, relevance patterns, etc. required.
This is an important diﬀerence from other clustering techniques making a recourse
to information theory, which consider diﬀerent channels and generally require addi-
tional information about relevance or irrelevance variables (cf Tishby et al. (1999);
Chechik and Tishby (2002); Dhillon and Guan (2003)).

Our infomax approach is in contrast with probabilistic methods based on likelihood
maximization. There the task of ﬁnding an optimal cluster allocation y for an ob-
served pattern x may be viewed as an inference problem in generative models y → x,
where the probability of the data p(x) = Py p(y)p(x|y) is deﬁned as a mixture of
|y | processes. The key idea of ﬁtting such models to data is to ﬁnd a constrained
probability distribution p(x) which would be likely to generate the visible patterns
{x(1) , . . . , x(M ) } (this is commonly achieved by maximizing the marginal likelihood
for deterministic parameters of the constrained distribution). The unknown clusters
y corresponding to each pattern x may then be assigned according to the posterior
p(y |x) ∝ p(y)p(x|y). Such generative approaches are well-known but suﬀer from the
constraint that p(x|y) is a correctly normalised distribution in x. In high dimensions
|x| this restricts the class of generative distributions usually to (mixtures of ) Gaus-
sians whose mean is dependent (in a linear or non-linear way) on the latent cluster
y . Typically data will lie on low dimensional curved manifolds embedded in the high
dimensional x-space. If we are restricted to using mixtures of Gaussians to model
this curved manifold, typically a very large number of mixture components will be
required. No such restrictions apply in the infomax case so that the mappings p(y |x)
may be very complex, sub ject only to sensible clustering constraints.

2 Clustering in Nonlinear Encoder Models

Arguably, there are at least two requirements which a meaningful cluster allocation
procedure should satisfy. Firstly, clusters should be, in some sense, locally smooth.
For example, each pair of source vectors should have a high probability of being
assigned to the same cluster if the vectors satisfy speciﬁc geometric constraints.
Secondly, we may wish to avoid assigning unique cluster labels to outliers (or other
constrained regions in the data space), so that under-represented regions in the
data space are not over-represented in the code space. Note that degenerate cluster
allocations are generally suboptimal under the ob jective (1), as they would lead to
a reduction in the marginal entropy H (y). On the other hand, it is intuitive that
maximization of the mutual information I (x, y) favors hard assignments of cluster
labels to equiprobable data regions, as this would result in the growth in H (y) and
reduction in H (y |x).

2.1 Learning Optimal Parameters

Local smoothness and “softness” of the clusters may be enforced by imposing ap-
propriate constraints on p(y |x). A simple choice of the encoder is
p(yj |x(i) ) ∝ exp{−kx(i) − wj k2 /sj + bj },
where the cluster centers wj ∈ R|x| , the dispersions sj , and the biases bj are the
encoder parameters to be learned. Clearly, under the encoding distribution (3)
patterns x lying close to speciﬁc centers wj in the data space will tend to be clustered
similarly. In principle, we could consider other choices of p(y |x); however (3) will
prove to be particularly convenient for the kernelized extensions.

(3)

Learning the optimal cluster allocations corresponds to maximizing (1) with respect
to the encoder parameters (3). The gradients are given by

∂ I (x, y)
∂wj

=

1
M

M
X
m=1

p(yj |x(m) )

(x(m) − wj )
sj

α(m)
j

(4)

=

1
M

M
∂ I (x, y)
X
∂ sj
m=1
m=1 p(yj |x(m) )α(m)
Analogously, we get ∂ I (x, y)/∂ bj = PM
j
Expressions (4) and (5) have the form of the weighted EM updates for isotropic
Gaussian mixtures, with the weighting coeﬃcients α(m)
deﬁned as
j

kx(m) − wj k2
2s2
j

p(yj |x(m) )

α(m)
j

.

(5)

/M .

α(m)
j

def= αj (x(m) ) def= log

p(yj |x(m) )
p(yj )

− K L ³p(y |x(m) )khp(y |x)i ˜p(x)´ ,

(6)

where K L deﬁnes the Kullback-Leibler divergence (e.g. Cover and Thomas (1991)),
and ˜p(x) ∝ Pm δ(x − x(m) ) is the empirical distribution. Clearly, if α(m)
is kept
j
ﬁxed for all m = 1, . . . , M and j = 1, . . . , |y |, the gradients (4) are identical to
those obtained by maximizing the log-likelihood of a Gaussian mixture model (up
to irrelevant constant pre-factors). Generally, however, the coeﬃcients α(m)
j will be
functions of wl , sl , and bl for all cluster labels l = 1, . . . , |y |.

In practice, we may impose a simple construction ensuring that sj > 0, for example
by assuming that sj = exp{˜sj } where ˜sj ∈ R. For this case, we may re-express
the gradients for the variances as ∂ I (x, y)/∂ ˜sj = sj ∂ I (x, y)/∂ sj . Expressions (4)
and (5) may then be used to perform gradient ascent on I (x, y) for wj , ˜sj , and bj ,
where j = 1, . . . , |y |. After training, the optimal cluster allocations may be assigned
according to the encoding distribution p(y |x).

2.2

Infomax Clustering with Kernelized Encoder Models

We now extend (3) by considering a kernelized parameterization of a nonlinear
encoder. Let us assume that the source patterns x(i) , x(j ) have a high probability
of being assigned to the same cluster if they lie close to a speciﬁc cluster center in
some feature space. One choice of the encoder distribution for this case is
p(yj |x(i) ) ∝ exp{−kφ(x(i) ) − wj k2 /sj + bj },
where φ(x(i) ) ∈ R|φ| is the feature vector corresponding to the source pattern x(i) ,
and wj ∈ R|φ| is the (unknown) cluster center in the feature space. The feature
space may be very high- or even inﬁnite-dimensional.

(7)

Since each cluster center wi ∈ R|φ| lives in the same space as the pro jected sources
φ(x(i) ), it is representable in the basis of the pro jections as

(8)

wj =

αmj φ(x(m) ) + w⊥
j ,

M
X
m=1
where ˜w⊥
i ∈ R|φ| is orthogonal to the span of φ(x1 ), . . . , φ(xM ), and {αmj } is a set
of coeﬃcients (here j and m index |y | codes and M patterns respectively). Then
we may transform the encoder distribution (7) to
j Kaj + cj ´ /sj o
p(yj |x(m) ) ∝ exp n− ³Kmm − 2kT (x(m) )aj + aT
def= exp{−fj (x(m) )},
where k(x(m) ) corresponds to the mth column (or row) of the Gram matrix
def= {Kij } def= {φ(x(i) )T φ(x(j ) )} ∈ RM ×M , aj ∈ RM is the j th column of the
K
def= {amj } ∈ RM ×|y | , and cj = (w⊥
j )T w⊥
j − sj bj . With-
matrix of the coeﬃcients A
out loss of generality, we may assume that c = {cj } ∈ R|y | is a free unconstrained
parameter. Additionally, we will ensure positivity of the dispersions sj by consid-
ering a construction constraint sj = exp{˜sj }, where ˜sj ∈ R.

(9)

Learning Optimal Parameters

First we will assume that the Gram matrix K ∈ RM ×M is ﬁxed and known (which
eﬀectively corresponds to considering a ﬁxed aﬃnity matrix, see e.g. Dhillon et al.
(2004)). Ob jective (1) should be optimized with respect to the log-dispersions
˜sj ≡ log(sj ), biases cj , and coordinates A ∈ RM ×|y | in the space spanned by the
feature vectors {φ(x(i) )|i = 1, . . . , M }. From (9) we get

=

=

(11)

(10)

1
sj
1
2sj

hp(yj |x)fj (x)αj (x)i ˜p(x) ,

hp(yj |x) (k(x) − Kaj ) αj (x)i ˜p(x) ∈ RM ,

∂ I (x, y)
∂ aj
∂ I (x, y)
∂ ˜sj
where ˜p(x) ∝ PM
m−1 δ(x−x(m) ) is the empirical distribution. Analogously, we obtain
∂ I (x, y)/∂ cj = hαj (x)i ˜p(x) ,
(12)
where the coeﬃcients αj (x) are given by (6). For a known Gram matrix K ∈ RM ×M ,
the gradients ∂ I /∂ aj , ∂ I /∂ ˜sj , and ∂ I /∂ cj given by expressions (10) – (12) may be
used in numerical optimization for the model parameters. Note that the matrix
multiplication in (10) is performed once for each aj , so that the complexity of
computing the gradient is ∼ O(M 2 |y |) per iteration. We also note that one could
potentially optimize (1) by applying the iterative Arimoto-Blahut algorithm for
maximizing the channel capacity (see e.g. Cover and Thomas (1991)). However, for
any given constrained encoder it is generally diﬃcult to derive closed-form updates
for the parameters of p(y |x), which motivates a numerical optimization.

Learning Optimal Kernels

Since we presume that explicit computations in R|φ| are expensive, we cannot com-
pute the Gram matrix by trivially applying its deﬁnition K = {φ(xi )T φ(xj )}. In-
stead, we may interpret scalar products in feature spaces as kernel functions
φ(x(i) )T φ(x(j ) ) = KΘ (x(i) , x(j ) ; Θ), ∀x(i) , x(j ) ∈ Rx ,

(13)

where KΘ : Rx × Rx → R satisﬁes Mercer’s kernel properties (e.g. Scholkopf and
Smola (2002)). We may now apply our unsupervised framework to implicitly learn
the optimal nonlinear features by optimizing I (x, y) with respect to the parameters
Θ of the kernel function KΘ . After some algebraic manipulations, we get

M

∂ I (x, y)
∂Θ

=

M
X
m=1

K L(p(y |x(m) )kp(y))

|y |
X
k=1

∂ fk (x(m) )
∂Θ

p(yk |x(m) )

−

M
X
m=1

|y |
X
j=1

∂ fj (x(m) )
∂Θ

p(yj |x(m) ) log

p(yj |x(m) )
p(yj )

(14)

where fk (x(m) ) is given by (9). The computational complexity of computing the
updates for Θ is O(M |y |2 ), where M is the number of training patterns and |y |
is the number of clusters (which is assumed to be small). Note that in contrast
to spectral methods (see e.g. Shi and Malik (2000), Ng et al. (2001)) neither the
ob jective (1) nor its gradients require inversion of the Gram matrix K ∈ RM ×M or
computations of its eigenvalue decomposition.

In the special case of the radial basis function (RBF) kernels

Kβ (x(i) , x(j ) ) = exp{−β kx(i) − x(j ) k2 },

the gradients of the encoder potentials are simply given by

∂ fj (x(m) )
∂β

=

1
sj

³aT
j

˜Kaj − 2˜kT (x(m) )aj ´ ,

(15)

(16)

def= { ˜Kij } def= K (x(i) , x(j ) )(1 − δ(x(i) − x(j ) )), and δ is the Kronecker delta.
where ˜K
By substituting (16) into the general expression (14), we obtain the gradient of the
mutual information with respect to the RBF kernel parameters.

3 Demonstrations

We have empirically compared our kernelized information-theoretic clustering ap-
proach with Gaussian mixture, k-means, feature-space k-means, non-kernelized
information-theoretic clustering (see Section 2.1), and a multi-class spectral cluster-
ing method optimizing the normalized cuts. We illustrate the methods on datasets
that are particularly easy to visualize. Figure 1 shows a typical application of the
methods to the spiral data, where x1 (t) = t cos(t)/4, x2 (t) = t sin(t)/4 correspond
to diﬀerent coordinates of x ∈ R|x| , |x| = 2, and t ∈ [0, 3.(3)π ]. The kernel param-
eters β of the RBF-kernelized encoding distribution were initialized at β0 = 2.5
and learned according to (16). The initial settings of the coeﬃcients A ∈ RM ×|y|
in the feature space were sampled from NAij (0, 0.1). The log-variances ˜s1 , . . . , ˜s|y |
were initialized at zeros. The encoder parameters A and {˜sj |j = 1, . . . , |y |} (along
with the RBF kernel parameter β ) were optimized by applying the scaled conjugate
gradients. We found that Gaussian mixtures trained by maximizing the likelihood
usually resulted in highly stochastic cluster allocations; additionally, they led to
a large variation in cluster sizes. The Gaussian mixtures were initialized using
k-means – other choices usually led to worse performance. We also see that the
k-means eﬀectively breaks, as the similarly clustered points lie close to each other
in R2 (according to the L2 -norm), but the allocated clusters are not locally smooth
in t. On the other hand, our method with the RBF-kernelized encoders typically
led to locally smooth cluster allocations.

Gaussian mixture clustering for |y|=3
2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

0.5

1

−2

−1
2
1
0
Gaussian Mixture Clustering

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

K−means clustering for |y|=3

KMI Clustering, β=0.825 (β
 = 2.500), |y|=3
0
2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

−2

0.5

1

2
1
0
−1
Kernelized Encoders, β = 0.825

−1
0
1
K−means Clustering

2

−2

0.5

1

1.5

)
)
m
(
x
|
j
y
(
p

2

2.5

1.5

)
)
m
(
x
|
j
y
(
p

2

2.5

)
)
m
(
x
|
j
y
(
p

1.5

2

2.5

3

3.5

10

20
30
40
50
Training Patterns

60

3

3.5

10

20
30
40
50
Training Patterns

60

3

3.5

10

20
30
40
50
Training Patterns

60

Figure 1: Cluster allocations (top) and the corresponding responsibilities (bottom)
p(yj |x(m) ) for |x| = 2, |y | = 3, M = 70 (the patterns are sorted to indicate local
smoothness in the phase parameter). Left: Gaussian mixtures; midd le: K-means;
right: information-maximization for the (RBF-)kernelized encoder (the learned pa-
rameter β ≈ 0.825). Light, medium, and dark-gray squares show the cluster colors
corresponding to deterministic cluster allocations. The color intensity of each train-
ing point x(m) is the average of the pure cluster intensities, weighted by the respon-
sibilities p(yj |x(m) ). Nearly indistinguishable dark colors of the Gaussian mixture
clustering indicate soft cluster assignments.

Figure 2 shows typical results for spatially translated letters with |x| = 2, M =
150, and |y | = 2 (or |y | = 3), where we compare Gaussian mixture, feature-space
k-means, the spectral method of Ng et al. (2001), and our information-theoretic
clustering method. The initializations followed the same procedure as the previous
experiment. The results produced by our kernelized infomax method were generally
stable under diﬀerent initializations, provided that β0 was not too large or too small.
In contrast to Gaussian mixture, spectral, and feature-space k-means clustering,
the clusters produced by kernelized infomax for the cases considered are arguably
more anthropomorphically appealing. Note that feature-space k-means, as well
as the spectral method, presume that the kernel matrix K ∈ RM ×M is ﬁxed and
known (in the latter case, the Gram matrix deﬁnes the edge weights of the graph).
For illustration purposes, we show the results for the ﬁxed Gram matrices with
kernel parameters β set to the initial values β0 = 1 or the learned values β ≈
0.604 of the kernelized infomax method for |y | = 2. One may potentially improve
the performance of these methods by running the algorithms several times (with
diﬀerent kernel parameters β ), and choosing β which results in tightest clusters
(Ng et al. (2001)). We were indeed able to apply the spectral method to obtain
clusters for TA and T (for β ≈ 1.1). While being useful in some situations, the
procedure generally requires multiple runs.
In contrast, the kernelized infomax
method typically resulted in meaningful cluster allocations (TT and A) after a single
run of the algorithm (see Figure 2 (c)), with the results qualitatively consistent
under a variety of initializations.

Additionally, we note that in situations when we used simpler encoder models (see
expression (3)) or did not adapt parameters of the kernel functions, the extracted
clusters were often more intuitive than those produced by rival methods, but inferior

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

−3

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5
−3

Gaussian mixture clustering for |y|=2

1

−1

−2

0
(a)
Spectral Clustering, β ≈ 0.604, |y|=2

2

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

3

−2.5

−3

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5
−3

−2

−1

0
(d)

1

2

3

Feauture space K−means,  β=0.604

KMI Clustering, β = 0.6035 (from β
0=1), |y|=2
2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

1

−1

−2

0
(b)
KMI Clustering, β
0=1.000, |y|=3, I = 1.03

2

3

−2.5

3

2

1

−1

−2

−3

0
(c)
2.5KMI Clusters: β ≈ 0.579 (β
0 = 1), |y|=3, I = 1.10
                                                           
2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2

−1

0
(e)

1

2

3

−2.5

−3

−2

−1

1

2

3

0
(f )

Figure 2: Learning cluster allocations for |y | = 2 and |y | = 3. Where appropriate,
the stars show the cluster centers. (a) two-component Gaussian mixture trained
by the EM algorithm; (b) feature-space k-means with β = 1.0 and β ≈ 0.604 (the
only pattern clustered diﬀerently (under identical initializations) is shown by ⊚);
(c) kernelized infomax clustering for |y | = 2 (the inverse variance β of the RBF
kernel varied from β0 = 1 (at the initialization) to β ≈ 0.604 after convergence);
(d) spectral clustering for |y | = 2 and β ≈ 0.604; (e) kernelized infomax clustering
for |y | = 3 with a ﬁxed Gram matrix; (f ) kernelized infomax clustering for |y | = 3
started at β0 = 1 and reaching β ≈ 0.579 after convergence.

to the ones produced by (7) with the optimal learned β . Our results suggest that by
learning kernel parameters we may often obtain higher values of the ob jective I (x, y),
as well as more appealing cluster labeling (e.g. for the examples shown on Figure 2
(e), (f) we get I (x, y) ≈ 1.03 and I (x, y) ≈ 1.10 respectively). Undoubtedly, a careful
choice of the kernel function could potentially lead to an even better visualization
of the locally smooth, non-degenerate structure.

4 Discussion

The proposed information-theoretic clustering framework is fundamentally diﬀer-
ent from the generative latent variable clustering approaches. Instead of explicitly
parameterizing the data-generating process, we impose constraints on the encoder
distributions, transforming the clustering problem to learning optimal discrete en-
codings of the unlabeled data. Many possible parameterizations of such distribu-
tions may potentially be considered. Here we discussed one such choice, which
implicitly utilizes pro jections of the data to high-dimensional feature spaces.

Our method suggests a formal information-theoretic procedure for learning optimal
cluster allocations. One potential disadvantage of the method is a potentially large
number of local optima; however, our empirical results suggest that the method is
stable under diﬀerent initializations, provided that the initial variances are suﬃ-
ciently large. Moreover, the results suggest that in the cases considered the method

favorably compares with the common generative clustering techniques, k-means,
feature-space k-means, and the variants of the method which do not use nonlinear-
ities or do not learn parameters of kernel functions.

A number of interesting interpretations of clustering approaches in feature spaces
are possible. Recently, it has been shown (Bach and Jordan (2003); Dhillon et al.
(2004)) that spectral clustering methods optimizing normalized cuts (Shi and Malik
(2000); Ng et al. (2001)) may be viewed as a form of weighted feature-space k-means,
for a speciﬁc ﬁxed similarity matrix. We are currently relating our method to the
common spectral clustering approaches and a form of annealed weighted feature-
space k-means. We stress, however, that our information-maximizing framework
suggests a principled way of learning optimal similarity matrices by adapting param-
eters of the kernel functions. Additionally, our method does not require computa-
tions of eigenvalues of the similarity matrix, which may be particularly beneﬁcial for
large datasets. Finally, we expect that the proper information-theoretic interpreta-
tion of the encoder framework may facilitate extensions of the information-theoretic
clustering method to richer families of encoder distributions.

References

Agakov, F. V. and Barber, D. (2006). Auxiliary Variational Information Maximization
for Dimensionality Reduction. In Proceedings of the PASCAL Workshop on Subspace,
Latent Structure and Feature Selection Techniques. Springer. To appear.

Bach, F. R. and Jordan, M. I. (2003). Learning spectral clustering. In NIPS. MIT Press.

Barber, D. and Agakov, F. V. (2003). The IM Algorithm: A Variational Approach to
Information Maximization. In NIPS. MIT Press.

Brunel, N. and Nadal, J.-P. (1998). Mutual Information, Fisher Information and Popula-
tion Coding. Neural Computation, 10:1731–1757.

Chechik, G. and Tishby, N. (2002). Extracting relevant structures with side information.
In NIPS, volume 15. MIT Press.

Cover, T. M. and Thomas, J. A. (1991). Elements of Information Theory. Wiley, NY.

Information Theoretic Clustering of Sparse Co-
Dhillon, I. S. and Guan, Y. (2003).
Occurrence Data. In Proceedings of the 3rd IEEE International Conf. on Data Mining.
Dhillon, I. S., Guan, Y., and Kulis, B. (2004). Kernel k-means, Spectral Clustering and
Normalized Cuts. In KDD. ACM.

Fisher, J. W. and Principe, J. C. (1998). A methodology for information theoretic feature
extraction. In Proc. of the IEEE International Joint Conference on Neural Networks.

Linsker, R. (1988). Towards an Organizing Principle for a Layered Perceptual Network.
In Advances in Neural Information Processing Systems. American Institute of Physics.

Ng, A. Y., Jordan, M., and Weiss, Y. (2001). On spectral clustering: Analysis and an
algorithm. In NIPS, volume 14. MIT Press.

Scholkopf, B. and Smola, A. (2002). Learning with Kernels. MIT Press.

Shi, J. and Malik, J. (2000). Normalized Cuts and Image Segmentation. IEEE Transactions
on Pattern Analysis and Machine Intel ligence, 22(8):888–905.

Tishby, N., Pereira, F. C., and Bialek, W. (1999). The information bottleneck method. In
Proceedings of the 37-th Annual Al lerton Conference on Communication, Control and
Computing. Kluwer Academic Publishers.

Torkkola, K. and Campbell, W. M. (2000). Mutual Information in Learning Feature
Transformations. In ICML. Morgan Kaufmann.


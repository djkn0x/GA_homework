Fast Online Policy Gradient Learning
with SMD Gain Vector Adaptation

Nicol N. Schraudolph
Jin Yu Douglas Aberdeen
Statistical Machine Learning, National ICT Australia, Canberra
{nic.schraudolph,douglas.aberdeen}@nicta.com.au

Abstract

Reinforcement learning by direct policy gradient estimation is attractive
in theory but in practice leads to notoriously ill-behaved optimization
problems. We improve its robustness and speed of convergence with
stochastic meta-descent, a gain vector adaptation method that employs
fast Hessian-vector products. In our experiments the resulting algorithms
outperform previously employed online stochastic, ofﬂine conjugate, and
natural policy gradient methods.

1

Introduction

Policy gradient reinforcement learning (RL) methods train controllers by estimating the
gradient of a long-term reward measure with respect to the parameters of the controller [1].
The advantage of policy gradient methods, compared to value-based RL, is that we avoid
the often redundant step of accurately estimating a large number of values. Policy gradient
methods are particularly appealing when large state spaces make representing the exact
value function infeasible, or when partial observability is introduced. However, in practice
policy gradient methods have shown slow convergence [2], not least due to the stochastic
nature of the gradients being estimated.
The stochastic meta-descent (SMD) gain adaptation algorithm [3, 4] can considerably ac-
celerate the convergence of stochastic gradient descent. In contrast to other gain adaptation
methods, SMD copes well not only with stochasticity, but also with non-i.i.d. sampling of
observations, which necessarily occurs in RL. In this paper we derive SMD in the context
of policy gradient RL, and obtain over an order of magnitude improvement in convergence
rate compared to previously employed policy gradient algorithms.

2 Stochastic Meta-Descent

2.1 Gradient-based gain vector adaptation

Let R be a scalar objective function we wish to maximize with respect to its adaptive
parameter vector θ ∈ Rn , given a sequence of observations xt ∈ X at time t = 1, 2, . . .
Where R is not available or expensive to compute, we use the stochastic approximation
Rt : Rn× X → R of R instead, and maximize the expectation Et [Rt (θt , xt )]. Assuming
that Rt is twice differentiable wrt. θ , with gradient and Hessian given by
∂θ Rt (θ , xt )|θ=θt and Ht = ∂ 2
∂θ ∂θ> Rt (θ , xt )|θ=θt ,
gt = ∂

(1)

respectively, we maximize Et [Rt (θ)] by the stochastic gradient ascent
θt+1 = θt + γt · gt ,
(2)
where · denotes element-wise (Hadamard) multiplication. The gain vector γt ∈ (R+ )n
serves as a diagonal conditioner, providing each element of θ with its own positive gradient
step size. We adapt γ by a simultaneous meta-level gradient ascent in the objective Rt . A
straightforward implementation of this idea is the delta-delta algorithm [5], which would
update γ via

∂Rt+1 (θt+1 )
∂Rt+1 (θt+1 )
· ∂θt+1
= γt + µgt+1 · gt ,
= γt + µ
γt+1 = γt + µ
(3)
∂γt
∂γt
∂θt+1
where µ ∈ R is a scalar meta-step size. In a nutshell, gains are decreased where a negative
autocorrelation of the gradient indicates oscillation about a local minimum, and increased
otherwise. Unfortunately such a simplistic approach has several problems: Firstly, (3)
allows gains to become negative. This can be avoided by updating γ multiplicatively, e.g.
via the exponentiated gradient algorithm [6].
Secondly, delta-delta’s cure is worse than the disease: individual gains are meant to address
ill-conditioning, but (3) actually squares the condition number. The autocorrelation of the
gradient must therefore be normalized before it can be used. A popular (if extreme) form of
normalization is to consider only the sign of the autocorrelation. Such sign-based methods
[5, 7–9], however, do not cope well with stochastic approximation of the gradient since the
non-linear sign function does not commute with the expectation operator [10]. More recent
algorithms [3, 4, 10] therefore use multiplicative (hence linear) normalization factors to
condition the meta-level update.
Finally, (3) fails to take into account that gain changes affect not only the current, but also
future parameter updates. In recognition of this shortcoming, gt in (3) is often replaced
with a running average of past gradients. Though such ad-hoc smoothing does improve
performance, it does not properly capture long-term dependences, the average still being
one of immediate, single-step effects. By contrast, Sutton [11] modeled the long-term effect
of gains on future parameter values in a linear system by carrying the relevant partials
forward in time, and found that the resulting gain adaptation can outperform a less than
perfectly matched Kalman ﬁlter. Stochastic meta-descent (SMD) extends this approach to
arbitrary twice-differentiable nonlinear systems, takes into account the full Hessian instead
of just the diagonal, and applies a decay to the partials being carried forward.

2.2 The SMD Algorithm

ln γt+1 = ln γt + µ

SMD employs two modiﬁcations to address the problems described above: it adjusts gains
in log-space, and optimizes over an exponentially decaying trace of gradients. Thus ln γ is
tX
updated as follows:
λi ∂R(θt+1 )
tX
∂ ln γt−i
i=0
∂R(θt+1 )
=: ln γt + µ gt+1 · vt+1 ,
·
λi ∂θt+1
= ln γt + µ
(4)
∂ ln γt−i
∂θt+1
i=0
where the vector v ∈ Rn characterizes the long-term dependence of the system parameters
on their gain history over a time scale governed by the decay factor 0 ≤ λ ≤ 1. Element-
wise exponentiation of (4) yields the desired multiplicative update
2 , 1 + µ gt+1 · vt+1 ).
γt+1 = γt · exp(µ gt+1 · vt+1 ) ≈ γt · max( 1
(5)
The linearization eu ≈ max( 1
2 , 1+ u) eliminates an expensive exponentiation for each gain
update, improves its robustness by reducing the effect of outliers (|u| (cid:29) 0), and ensures

(6)

#

λi

=

λi

+

vt+1 =

∂θt
∂ ln γt−i

Noting that ∂gt
∂θt

that γ remains positive. To compute the gradient trace v efﬁciently, we expand θt+1 in
tX
tX
tX
terms of its recursive deﬁnition (2):
λi ∂ (γt · gt )
λi ∂θt+1
"
tX
∂ ln γt−i
∂ ln γt−i
i=0
i=0
i=0
≈ λvt + γt · gt + γt ·
i=0
is the Hessian Ht of Rt (θt ), we arrive at the simple iterative update
vt+1 = λvt + γt · (gt + λHtvt ) ;
v0 = 0 .
(7)
Although the Hessian of a system with n parameters has O(n2 ) entries, efﬁcient indirect
methods from algorithmic differentiation are available to compute its product with an ar-
bitrary vector in the same time as 2 –3 gradient evaluations [12, 13]. To improve stability,
SMD employs an extended Gauss-Newton approximation of Ht for which a similar (even
faster) technique is available [4]. An iteration of SMD — comprising (5), (2), and (7) —
thus requires less than 3 times the ﬂoating-point operations of simple gradient ascent. The
extra computation is typically more than compensated for by the faster convergence of
SMD. Fast convergence minimizes the number of expensive world interactions required,
which in RL is typically of greater concern than computational cost.

∂θt
∂ ln γt−i

∂ gt
∂θt

3 Policy Gradient Reinforcement Learning
A Markov decision process (MDP) consists of a ﬁnite 1 set of states s ∈ S of the world,
actions a ∈ A available to the agent in each state, and a (possibly stochastic) reward
function r(s) for each state s. In a partially observable MDP (POMDP), the controller sees
only an observation x ∈ X of the current state, sampled stochastically from an unknown
distribution P(x|s). Each action a determines a stochastic matrix P (a) = [P(s0 |s, a)] of
transition probabilities from state s to state s0 given action a. The methods discussed in this
paper do not assume explicit knowledge of P (a) or of the observation process. All policies
are stochastic, with a probability of choosing action a given state s, and parameters θ ∈ Rn
of P(a|θ , s). The evolution of the state s is Markovian, governed by an |S | × |S | transition
P(s0 |θ , s) = X
probability matrix P (θ) = [P(s0 |θ , s)] with entries given by
P(a|θ , s) P(s0 |s, a) .
a∈A

(8)

3.1 GPOMD P Monte Carlo estimates of gradient and hessian
#
" TX
GPOMD P is an inﬁnite-horizon policy gradient method [1] to compute the gradient of the
long-term average reward
1
Eθ
R(θ) := lim
r(st )
T →∞
T
t=1
with respect the policy parameters θ . The expectation Eθ is over the distribution of state
trajectories {s0 , s1 , . . . } induced by P (θ).
Theorem 1 (1) Let I be the identity matrix, and u a column vector of ones. The gradient
of the long-term average reward wrt. a policy parameter θi is
∇θi R(θ) = π(θ)>∇θi P (θ)[I − P (θ) + uπ(θ)> ]−1r ,
where π(θ) is the stationary distribution of states induced by θ .

(10)

(9)

,

1For uncountably inﬁnite state spaces, the derivation becomes more complex without substantially
altering the resulting algorithms.

β τ −t−1 r(sτ ) ,

(11)

∇θ ln P(at |θ , st )

Note that (10) requires knowledge of the underlying transition probabilities P (θ), and
the inversion of a potentially large matrix. The GPOMD P algorithm instead computes a
Monte-Carlo approximation of (10): the agent interacts with the environment, producing
an observation, action, reward sequence {x1 , a1 , r1 , x2 , . . . , xT , aT , rT }.2 Under mild
technical assumptions, including ergodicity and bounding all the terms involved, Baxter
T −1X
TX
and Bartlett [1] obtainb∇θR =
1
T
t=0
τ =t+1
where a discount factor β ∈ [0, 1) implicitly assumes that rewards are exponentially more
likely to be due to recent actions. Without it, rewards would be assigned over a potentially
inﬁnite horizon, resulting in gradient estimates with in ﬁnite variance. As
β decreases, so
does the variance, but the bias of the gradient estimate increases [1]. In practice, (11) is
implemented efﬁciently via the discounted eligibility trace
et = βet−1 + δt , where δt := ∇θ P(at |θ , st )/ P(at |θ , st ) .
(12)
Now gt = rtet is the gradient of R(θ) arising from assigning the instantaneous re-
ward to all log action gradients, where β gives exponentially more credit to recent ac-
tions. Likewise, Baxter and Bartlett [1] give the Monte Carlo estimate of the Hessian as
Ht = rt (Et + ete>
t ), using an eligibility trace matrix
P(at |θ , st )/ P(at |θ , st ) .
t , where Gt := ∇2
Et = βEt−1 + Gt − δtδ>
(13)
θ
Maintaining E would be O(n2 ), thus computationally expensive for large policy parameter
spaces. Noting that SMD only requires the product of Ht with a vector v , we instead use
t v)] , where dt = βdt−1 + Gtv − δt (δ>
Htv = rt [dt + et (e>
t v)
(14)
is an eligibility trace vector that can be maintained in O(n). We describe the efﬁcient
computation of Gtv in (14) for a speciﬁc action selection method in Section 3.3 below.

3.2 GPOMD P-Based optimization algorithms

Baxter et al. [2] proposed two optimization algorithms using GPOMD P’s policy gradient
estimates gt : O LPOMD P is a simple online stochastic gradient descent (2) with scalar gain
γt . Alternatively, CON JPOMD P performs Polak-Ribi `ere conjugation of search directions,
using a noise-tolerant line search to ﬁnd the approximately best scalar step size in a given
search direction. Since conjugate gradient methods are very sensitive to noise [14], CON J -
POMD P must average gt over many steps to obtain a reliable gradient measurement; this
makes the algorithm inherently inefﬁcient ( cf. Section 4).
O LPOMD P, on the other hand, is robust to noise but converges only very slowly. We can,
however, employ SMD’s gain vector adaptation to greatly accelerate it while retaining the
beneﬁts of high noise tolerance and online learning. Experiments (Section 4) show that the
resulting SMDPOMD P algorithm can greatly outperform O LPOMD P and CON JPOMD P.
Kakade [15] has applied natural gradient [16] to GPOMD P, premultiplying the policy gra-
dient by the inverse of the online estimate
Ft = (1 − 1
t (δtδ>
t )Ft−1 + 1
t + I )
of the Fisher information matrix for the parameter update: θt+1 = θt + γ0 · rtF −1
t et . This
approach can yield very fast convergence on small problems, but in our experience does
not scale well at all to larger, more realistic tasks; see our experiments in Section 4.

(15)

2We use rt as shorthand for r(st ), making it clear that only the reward value is known, not the
underlying state st .

(16)

3.3 Softmax action selection
For discrete action spaces, a vector of action probabilities zt := P(at |yt ) can be generated
from the output yt := f (θt , xt ) of a parameterised function f : Rn× X → R| A | (such as
eytP| A |
a neural network) via the softmax function:
zt := softmax(yt ) =
m=1 [eyt ]m .
Given action at ∼ zt , GPOMD P’s instantaneous log-action gradient wrt. y is then
˜gt := ∇y [zt ]at /[zt ]at = uat − zt ,
where ui is the unity vector in direction i. The action gradient wrt. θ is obtained by
backpropagating ˜gt through f ’s adjoint system [13], performing an efﬁcient multiplication
by the transposed Jacobian of f . The resulting gradient δt := J >
f ˜gt is then accumulated in
the eligibility trace (12). GPOMD P’s instantaneous Hessian for softmax action selection is
y [zt ]at /[zt ]at = (uat −zt )(uat −zt )> + ztz>
˜Ht := ∇2
t − diag(zt ) .
It is indeﬁnite but reasonably well-behaved: the Gerschgorin circle theorem can be em-
ployed to show that its eigenvalues must all lie in the interval [− 1
4 , 2]. Furthermore, its
expectation over possible actions is zero:
Ezt ( ˜Ht ) = [diag(zt ) − 2ztz>
t − diag(zt ) = 0 .
t ] + ztz>
t + ztz>
The extended Gauss-Newton matrix-vector product [4] employed by SMD is then given by
Gtvt := J >
˜HtJf vt ,
f
where the multiplication by the Jacobian off (resp. its transpose) is implemented efﬁciently
by propagating vt through f ’s tangent linear (resp. adjoint) system [13].

(17)

(18)

(19)

(20)

Algorithm 1 SMDPOMD P with softmax action selection
1. Given (a) an ergodic POMDP with observations xt ∈ X , actions at ∈ A,
bounded rewards rt ∈ R, and softmax action selection
(b) a differentiable parametric map f : Rn× X → R|A| (neural network)
f u) and tangent linear (v → Jf v ) maps
(c) f ’s adjoint (u → J >
(d) free parameters: µ ∈ R+ ; β , λ ∈ [0, 1]; γ0 ∈ Rn
+ ; θ1 ∈ Rn
2. Initialize in Rn : e0 = d0 = v0 = 0
3. For t = 1 to ∞: (a) interact with POMDP:
i. observe feature vector xt
ii. compute zt := softmax(f (θt , xt ))
iii. perform action at ∼ zt
iv. observe reward rt
(b) maintain eligibility traces:
f (uat − zt )
i. δt := J >
ii. pt := Jf vt
iii. qt := (uat − zt )(δ>
t pt ) − zt · pt
t vt ) + zt (z>
iv. et = βet−1 + δt
f qt − δt (δ>
v. dt = βdt−1 + J >
t vt )
(c) update SMD parameters:
i. γt = γt−1 · max( 1
2 , 1 + µ rtet · vt )
ii. θt+1 = θt + rtγt · et
iii. vt+1 = λvt + rtγt · [(1 + λe>
t vt )et + λdt ]

Fig. 1: Left: Baxter et al.’s simple 3-state POMDP. States are labelled with their observable
features and instantaneous reward r ; arrows indicate the 80% likely transition for the ﬁrst
(solid) resp. second (dashed) action. Right: our modiﬁed, more difﬁcult 3-state POMDP.

4 Experiments

4.1 Simple Three-State POMDP

Fig. 1 (left) depicts the simple 3-state POMDP used by Baxter et al. [2, Tables 1&2]. Of
the two possible transitions from each state, the preferred one occurs with 80% probability,
the other with 20%. The preferred transition is determined by the action of a simple proba-
bilistic adaptive controller that receives two state-dependent feature values as input, and is
trained to maximize the expected average reward by policy gradient methods.
Using the original code of Baxter et al. [2], we replicated their experimental results for
the O LPOMD P and CON JPOMD P algorithms on this simple POMDP. We can accurately
reproduce all essential features of their graphed results on this problem [2, Figures 7&8].
We then implemented SMDPOMD P (Algorithm 1), and ran a comparison of algorithms,
using the best free parameter settings found by Baxter et al. [2] (in particular: β = 0, γ0 =
1), and µ = λ = 1 for SMDPOMD P. We always match random seeds across algorithms.
Baxter et al. [2] collect and plot results for CON JPOMD P in terms of its T parameter, which
speciﬁes the number of Markov chain iterations per gradient evaluation. For a fair com-
parison of convergence speed we added code to record the total number of Markov chain
iterations consumed by CON JPOMD P, and plot performance for all three algorithms in
those terms, with error bars along both axes for CON JPOMD P.
The results are shown in Fig. 2 (left), averaged over 500 runs. While early on CON JPOMD P
on average reaches a given level of performance about three times faster than O LPOMD P,
it does so at the price of far higher variance. Moreover, CON JPOMD P is the only algorithm
that fails to asymptotically approach optimal performance (R = 0.8; Fig. 2 left, inset).
Once its step size adaptation gets going, SMDPOMD P converges asymptotically to the op-

Fig. 2: Left: The POMDP of Fig. 1 (left) is easy to learn. CON JPOMD P converges faster
but to asymptotically inferior solutions (see inset) than the two online algorithms. Right:
SMDPOMD P outperforms O LPOMD P and CON JPOMD P on the difﬁcult POMDP of Fig. 1
(right). Natural policy gradient has rapid early convergence but diverges asymptotically.

  6/1812/1812/18  6/185/185/18r = 0r = 1r = 0612/18126/1855/18r = -1r = 8r = 1110100100010000Total Markov Chain Iterations0.20.30.40.50.60.70.8Average Rewardsmdolconj0.780.790.81001000100001e+51e+61e+7Total Markov Chain Iterations1.21.41.61.822.22.42.6Average  Rewardsmdolconjngtimal policy about three times faster than O LPOMD P in terms of Markov chain iterations,
making the two algorithms roughly equal in terms of computational expense.
CON JPOMD P on average performs less than two iterations of conjugate gradient in each
run. While this is perfectly understandable — the controller only has two trainable pa-
rameters — it bears keeping in mind that the performance of C ON JPOMD P here is almost
entirely governed by the line search rather than the conjugation of search directions.

4.2 Modiﬁed Three-State POMDP

The three-state POMDP employed by Baxter et al. [2] has the property that greedy max-
imization of instantaneous reward leads to the optimal policy. Non-trivial temporal credit
assignment — the hallmark of reinforcement learning — is not needed. The best results
are obtained with the eligibility trace turned off (β = 0). To create a more challenging
problem, we rearranged the POMDP’s state transitions and reward structure so that the
instantaneous reward becomes deceptive (Fig. 1, right). We also multiplied one state fea-
ture by 18 to create an ill-conditioned input to the controller, while leaving the actions and
relative transition probabilities (80% resp. 20%) unchanged. In our modiﬁed POMDP, the
high-reward state can only be reached through an intermediate state with negative reward.
Fig. 2 (right) shows our experimental results for this harder POMDP, averaged over 100
runs. Free parameters were tuned to θ1 ∈ [−0.1, 0.1], β = 0.6, γ0 = 0.001; T = 105 for
CON JPOMD P; µ = 0.002, λ = 1 for SMDPOMD P. CON JPOMD P now performs the worst,
which is expected because conjugation of directions is known to collapse in the presence
of noise [14]. SMDPOMD P converges about 20 times faster than O LPOMD P because its
adjustable gains compensate for the ill-conditioned input. Kakade’s natural gradient (using
 = 0.01) performs extremely well early on, taking 2 –3 times fewer iterations than S MD -
POMD P to reach optimal performance (R = 2.6). It does, however, diverge asymptotically.

4.3 Puck World

We also implemented the Puck World benchmark of Baxter et al. [2], with the free param-
eters settings θ1 ∈ [−0.1, 0.1], β = 0.95 γ0 = 2 · 10−6 ; T = 106 for CON JPOMD P;
µ = 100, λ = 0.999 for SMDPOMD P;  = 0.01 for natural policy gradient. To improve its
stability, we modi ﬁed SMD here to track instantaneous log-action gradients δt instead of
noisy rtet estimates of ∇θR. CON JPOMD P used a quadratic weight penalty of initially 0.5,
with the adaptive reduction schedule described by Baxter et al. [2, page 369]; the online
algorithms did not require a weight penalty.
Fig. 3 shows our results averaged over 100 runs, except for natural policy gradient where
only a single typical run is shown. This is because its O(n3 ) time complexity per iteration3

3The Sherman-Morrison formula cannot be used here because of the diagonal term in (15).

Fig. 3: The action-gradient version of
SMDPOMD P yields better asymptotic
results on PuckWorld than O L -
POMD P; CON JPOMD P is inefﬁcient;
natural policy gradient even more so.

1e+61e+71e+81e-61e-7SMD Gains1e+51e+61e+71e+8Iterations-60-50-40-30-20-10Average  Rewardsmdolconjngmakes natural policy gradient intolerably slow for this task, where n = 88. Moreover, its
convergence is quite poor here in terms of the number of iterations required as well.
CON JPOMD P is again inferior to the best online algorithms by over an order of magnitude.
Early on, SMDPOMD P matches O LPOMD P, but then reaches superior solutions with small
variance. SMDPOMD P-trained controllers achieve a long-term average reward of -6.5, sig-
niﬁcantly above the optimum of -8 hypothesized by Baxter et al. [2, page 369] based on
their experiments with CON JPOMD P.

5 Conclusion

On several non-trivial RL problems we ﬁnd that our S MDPOMD P consistently outperforms
O LPOMD P, which in turn outperforms CON JPOMD P. Natural policy gradient can converge
rapidly, but is too unstable and computationally expensive for all but very small controllers.

Acknowledgements

We are indebted to John Baxter for his code and helpful comments. National ICT Australia
is funded by the Australian Government’s Backing Australia’s Ability initiative, in part
through the Australian Research Council. This work is also supported by the IST Program
of the European Community, under the Pascal Network of Excellence, IST-2002-506778.

Journal of Artiﬁcial

References
[1] J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation.
Intelligence Research, 15:319–350, 2001.
[2] J. Baxter, P. L. Bartlett, and L. Weaver. Experiments with inﬁnite-horizon, policy-gradient
estimation. Journal of Artiﬁcial Intelligence Research , 15:351–381, 2001.
[3] N. N. Schraudolph. Local gain adaptation in stochastic gradient descent. In Proc. Intl. Conf.
Artiﬁcial Neural Networks , pages 569–574, Edinburgh, Scotland, 1999. IEE, London.
[4] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation, 14(7):1723–1738, 2002.
[5] R. Jacobs. Increased rates of convergence through learning rate adaptation. Neural Networks,
1:295–307, 1988.
[6] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient updates for linear
prediction. In Proc. 27th Annual ACM Symposium on Theory of Computing, pages 209–218.
ACM Press, New York, NY, 1995.
[7] T. Tollenaere. SuperSAB: Fast adaptive back propagation with good scaling properties. Neural
Networks, 3:561–573, 1990.
[8] F. M. Silva and L. B. Almeida. Acceleration techniques for the backpropagation algorithm.
In L. B. Almeida and C. J. Wellekens, editors, Neural Networks: Proc. EURASIP Workshop,
volume 412 of Lecture Notes in Computer Science, pages 110–119. Springer Verlag, 1990.
[9] M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: The
RPROP algorithm. In Proc. Intl. Conf. Neural Networks, pages 586–591. IEEE, 1993.
[10] L. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic
optimization. In D. Saad, editor, On-Line Learning in Neural Networks, Publications of the
Newton Institute, chapter 6, pages 111–134. Cambridge University Press, 1999.
[11] R. S. Sutton. Gain adaptation beats least squares? In Proceedings of the 7th Yale Workshop on
Adaptive and Learning Systems, pages 161–166, 1992.
[12] B. A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Comput., 6(1):147–60, 1994.
[13] A. Griewank. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentia-
tion. Frontiers in Applied Mathematics. SIAM, Philadelphia, 2000.
[14] N. N. Schraudolph and T. Graepel. Combining conjugate direction methods with stochastic
approximation of gradients. In C. M. Bishop and B. J. Frey, editors, Proc. 9th Intl. Workshop
Artiﬁcial Intelligence and Statistics , pages 7–13, Key West, Florida, 2003.
[15] S. Kakade. A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors,
Advances in Neural Information Processing Systems 14, pages 1531–1538. MIT Press, 2002.
[16] S. Amari. Natural gradient works efﬁciently in learning. Neural Comput., 10(2):251–276, 1998.


Mixture Modeling by Afﬁnity Propagation

Brendan J. Frey and Delbert Dueck
University of Toronto

Software and demonstrations available at www.psi.toronto.edu

Abstract
Clustering is a fundamental problem in machine learning and has been
approached in many ways. Two general and quite different approaches
include iteratively ﬁtting a mixture model ( e.g., using EM) and linking to-
gether pairs of training cases that have high afﬁnity ( e.g., using spectral
methods). Pair-wise clustering algorithms need not compute sufﬁcient
statistics and avoid poor solutions by directly placing similar examples
in the same cluster. However, many applications require that each cluster
of data be accurately described by a prototype or model, so afﬁnity-based
clustering – and its beneﬁts – cannot be directly realized. We describe a
technique called “afﬁnity propagation ”, which combines the advantages
of both approaches. The method learns a mixture model of the data by
recursively propagating afﬁnity messages. We demonstrate afﬁnity prop-
agation on the problems of clustering image patches for image segmen-
tation and learning mixtures of gene expression models from microar-
ray data. We ﬁnd that afﬁnity propagation obtains better solutions than
mixtures of Gaussians, the K -medoids algorithm, spectral clustering and
hierarchical clustering, and is both able to ﬁnd a pre-speciﬁed number
of clusters and is able to automatically determine the number of clusters.
Interestingly, afﬁnity propagation can be viewed as belief propagation
in a graphical model that accounts for pairwise training case likelihood
functions and the identiﬁcation of cluster centers.
1 Introduction
Many machine learning tasks involve clustering data using a mixture model, so that the
data in each cluster is accurately described by a probability model from a pre-deﬁned,
possibly parameterized, set of models [1]. For example, words can be grouped according to
common usage across a reference set of documents, and segments of speech spectrograms
can be grouped according to similar speaker and phonetic unit. As researchers increasingly
confront more challenging and realistic problems, the appropriate class-conditional models
become more sophisticated and much more difﬁcult to optimize.

By marginalizing over hidden variables, we can still view many hierarchical learning prob-
lems as mixture modeling, but the class-conditional models become complicated and non-
linear. While such class-conditional models may more accurately describe the problem at
hand, the optimization of the mixture model often becomes much more difﬁcult. Exact
computation of the data likelihoods may not be feasible and exact computation of the suf-
ﬁcient statistics needed to update parameterized models may not be feasible. Further, the
complexity of the model and the approximations used for the likelihoods and the sufﬁcient
statistics often produce an optimization surface with a large number of poor local minima.

A different approach to clustering ignores the notion of a class-conditional model, and

links together pairs of data points that have high afﬁnity. The afﬁnity or similarity (a real
number in [0, 1]) between two training cases gives a direct indication of whether they should
be in the same cluster. Hierarchical clustering and its Bayesian variants [2] is a popular
afﬁnity-based clustering technique, whereby a binary tree is constructed greedily from the
leaves to the root, by recursively linking together pairs of training cases with high afﬁnity.
Another popular method uses a spectral decomposition of the normalized afﬁnity matrix
[4]. Viewing afﬁnities as transition probabilities in a random walk on data points, modes
of the afﬁnity matrix correspond to clusters of points that are isolated in the walk [3, 5].

We describe a new method that, for the ﬁrst time to our knowledge, combines the advan-
tages of model-based clustering and afﬁnity-based clustering. Unlike previous techniques
that construct and learn probability models of transitions between data points [6, 7], our
technique learns a probability model of the data itself. Like afﬁnity-based clustering,
our algorithm directly examines pairs of nearby training cases to help ascertain whether
or not they should be in the same cluster. However, like model-based clustering, our
technique uses a probability model that describes the data as a mixture of class-conditional
distributions. Our method, called “afﬁnity propagation ”, can be viewed as the sum-product
algorithm or the max-product algorithm in a graphical model describing the mixture model.
2 A greedy algorithm: K -medoids
The ﬁrst step in obtaining the beneﬁt of pair-wise training case comparisons is to replace
the parameters of the mixture model with pointers into the training data. A similar rep-
resentation is used in K -medians clustering or K -medoids clustering, where the goal is
to identify K training cases, or exemplars, as cluster centers. Exact learning is known to
be NP-hard (c.f. [8]), but a hard-decision algorithm can be used to ﬁnd approximate solu-
tions. While the algorithm makes greedy hard decisions for the cluster centers, it is a useful
intermediate step in introducing afﬁnity propagation.
For training cases x1 , . . . , xN , suppose the likelihood of training case xi given that training
case xk is its cluster center is P (xi |xi in xk ) (e.g., a Gaussian likelihood would have the
√
form e−(xi−xk )2 /2σ2
2πσ2 ). Given the training data, this likelihood depends only on
/
i and k , so we denote it by Lik . Lii is set to the Bayesian prior probability that xi is a
cluster center. Initially, K training cases are chosen as exemplars, e.g., at random. Denote
the current set of cluster center indices by K and the index of the current cluster center
for xi by si . K -medoids iterates between assigning training cases to exemplars (E step),
and choosing a training case as the new exemplar for each cluster (M step). Assuming for
simplicity that the mixing proportions are equal and denoting the responsibility likelihood
ratio by rik = P (xi |xi in xk )/P (xi |xi not in xk )1 , the updates are
For k ∈ K: rik ← Lik /(P
E step
For i = 1, . . . , N :
j :j 6=k Lij )
si ← argmaxk∈K rik
For k ∈ K: Replace k in K with argmaxj :sj =k (Q
Greedy M step
i:si=k Lij )
This algorithm nicely replaces parameter-to-training case comparisons with pair-wise
training case comparisons. However, in the greedy M step, speciﬁc training cases are
chosen as exemplars. By not searching over all possible combinations of exemplars, the
algorithm will frequently ﬁnd poor local minima. We now introduce an algorithm that
does approximately search over all possible combinations of exemplars.
1Note that using the traditional deﬁnition of responsibility, rik ← Lik /(Σj Lij ), will give the
same decisions as using the likelihood ratio.

3 Afﬁnity propagation
The responsibilities in the greedy K -medoids algorithm can be viewed as messages that are
sent from training cases to potential exemplars, providing soft evidence of the preference
for each training case to be in each exemplar. To avoid making hard decisions for the
cluster centers, we introduce messages called “availabilities ”. Availabilities are sent from
exemplars to training cases and provide soft evidence of the preference for each exemplar
to be available as a center for each training case.

Responsibilities are computed using likelihoods and availabilities, and availabilities are
computed using responsibilities, recursively. We refer to both responsibilities and avail-
abilities as afﬁnities and we refer to the message-passing scheme as afﬁnity propagation.
Here, we explain the update rules; in the next section, we show that afﬁnity propagation
can be derived as the sum-product algorithm in a graphical model describing the mixture
model. Denote the availability sent from candidate exemplar xk to training case xi by aki .
Initially, these messages are set equal, e.g., aki = 1 for all i and k . Then, the afﬁnity
propagation update rules are recursively applied:
rik ← Lik /(P
Responsibility updates
j :j 6=k aij Lij )
akk ← Q
Availability updates
Q
j :j 6=k,j 6=i (1 + rjk )−1 + 1 − Q
j :j 6=k (1 + rjk ) − 1
aki ← 1/( 1
j :j 6=k,j 6=i (1 + rjk )−1 )
rkk
The ﬁrst update rule is quite similar to the update used in EM, except the likelihoods used
to normalize the responsibilities are modulated by the availabilities of the competing ex-
emplars. In this rule, the responsibility of a training case xi as its own cluster center, rii ,
is high if no other exemplars are highly available to xi and if xi has high probability under
the Bayesian prior, Lii .
The second update rule also has an intuitive explanation. The availability of a training
case xk as its own exemplar, akk , is high if at least one other training case places high
responsibility on xk being an exemplar. The availability of xk as a exemplar for xi , aki
is high if the self-responsibility rkk is high (1/rkk − 1 approaches −1), but is decreased if
other training cases compete in using xk as an exemplar (the term 1/rkk − 1 is scaled down
if rjk is large for some other training case xj ).
to the sum of their likelihoods, i.e. P
Messages may be propagated in parallel or sequentially. In our implementation, each candi-
date exemplar absorbs and emits afﬁnities in parallel, and the centers are ordered according
i Lik . Direct implementation of the above propaga-
tion rules gives an N 2 -time algorithm, but afﬁnities need only be propagated between i and
k if Lik > 0. In practice, likelihoods below some threshold can be set to zero, leading to a
sparse graph on which afﬁnities are propagated.

Afﬁnity propagation accounts for a Bayesian prior pdf on the exemplars and is able to
automatically search over the appropriate number of exemplars. (Note that the number of
exemplars is not pre-speciﬁed in the above updates.) In applications where a particular
number of clusters is desired, the update rule for the responsibilities (in particular, the self-
responsibilities rkk , which determine the availabilities of the exemplars) can be modiﬁed,
as described in the next section. Later, we describe applications where K is pre-speciﬁed
and where K is automatically selected by afﬁnity propagation.

The afﬁnity propagation update rules can be derived as an instance of the sum-product

Figure 1: Afﬁnity propagation can be viewed as belief propagation in this factor graph.
is QN
si to denote the index of the exemplar
(“loopy BP ”) algorithm in a graphical model. Using
for xi , the product of the likelihoods of the training cases and the priors on the exemplars
(If si = i, xi is an exemplar with a priori pdf Lii .) The set of hidden
i=1 Lisi .
variables s1 , . . . , sN completely speciﬁes the mixture model, but not all con ﬁgurations of
these variables are allowed: si = k (xi in cluster xk ) implies sk = k (xk is an exemplar)
and sk = k (xk is an exemplar) implies si = k for some i 6= k (some other training case is
written QN
in cluster xk ). The global indicator function for the satisfaction of these constraints can be
0 if sk = k and si 6= k for all i 6= k
k=1 fk (s1 , . . . , sN ), where fk is the constraint for candidate cluster xk :
0 if sk 6= k and si = k for some i 6= k
1 otherwise.
NY
NY
Thus, the joint distribution of the mixture model and data factorizes as follows:
i=1
k=1
The factor graph [10] in Fig. 1 describes this factorization. Each black box corresponds to
a term in the factorization, and it is connected to the variables on which the term depends.

fk (s1 , . . . , sN ) =

fk (s1 , . . . , sN ).

P =

Lisi

While exact inference in this factor graph is NP-hard, approximate inference algorithms can
be used to infer the s variables. It is straightforward to show that the updates for afﬁnity
propagation correspond to the message updates for the sum-product algorithm or loopy
belief propagation (see [10] for a tutorial). The responsibilities correspond to messages
[K = PN
sent from the s’s to the f ’s, while the availabilities correspond to messages sent from the
f ’s to the s’s. If the goal is to ﬁnd K exemplars, an additional constraint g(s1 , . . . , sN ) =
k=1 [sk = k ]] can be included, where [ ] indicates Iverson’s notation ([true]=1
and [false] = 0). Messages can be propagated through this function in linear time, by
implementing it as a Markov chain that accumulates exemplar counts.
Max-product afﬁnity propagation. Max-product afﬁnity propagation can be derived
as an instance of the max-product algorithm, instead of the sum-product algorithm. The
update equations for the afﬁnities are modiﬁed and maximizations are used instead of
summations. An advantage of max-product afﬁnity propagation is that the algorithm is
invariant to multiplicative constants in the log-likelihoods.
4
Image segmentation
A sensible model-based approach to image segmentation is to imagine that each patch in
the image originates from one of a small number of prototype texture patches. The main
difﬁculty is that in addition to standard additive or multiplicative pixel-level noise, another
prevailing form of noise is due to transformations of the image features, and in particular
translations.

Pair-wise afﬁnity-based techniques and in particular spectral clustering has been employed
with some success [4, 9], with the main disadvantage being that without an underlying

Figure 2: Segmentation of non-aligned gray-scale characters. Patches clustered by afﬁnity
propagation and K -medoids are colored according to classiﬁcation (centers shown below
solutions). Afﬁnity propagation achieves a near-best score compared to 1000 runs of K -
medoids.
model there is no sound basis for selecting good class representatives. Having a model with
class representatives enables efﬁcient synthesis (generation) of patches, and classiﬁcation
of test patches – requiring only K comparisons (to class centers) rather than N comparisons
(to training cases).

We present results for segmenting two image types. First, as a toy example, we segment
an image containing many noisy examples of the letters ‘N’ ‘I’ ‘P’ and ‘S’ (see Fig. 2).
The original image is gray-scale with resolution 216 × 240 and intensities ranging from 0
(background color, white) to 1 (foreground color, black). Each training case xi is a 24 × 24
by P
image patch and xm
is the mth pixel in the patch. To account for translations, we include a
i
hidden 2-D translation variable T . The match between patch xi and patch xk is measured
i · f m (xk , T ), where f (xk , T ) is the patch obtained by applying a 2-D translation
m xm
Lik ∝ X
T plus cropping to patch xk . f m is the mth pixel in the translated, cropped patch. This
metric is used in the likelihood function:
i ·f m (xk ,T ))/ ¯xi ≈ eβ maxT (Σm xm
i ·f m (xk ,T ))/ ¯xi ,
p(T )eβ (Σm xm
P
T
where ¯xi = 1
m xm
is used to normalize the match by the amount of ink in xi . β
242
i
controls how strictly xi should match xk to have high likelihood. Max-product afﬁnity
propagation is independent of the choice of β , and for sum-product afﬁnity propagation we
quite arbitrarily chose β = 1. The exemplar priors Lkk were set to mediani,k 6=iLik .
We cut the image in Fig. 2 into a 9 × 10 grid of non-overlapping 24 × 24 patches, computed
the pair-wise likelihoods, and clustered them into K = 4 classes using the greedy EM
algorithm (randomly chosen initial exemplars) and afﬁnity propagation. (Max-product and
sum-product afﬁnity propagation yielded identical results.) We then took a much larger set
of overlapping patches, classiﬁed them into the 4 categories, and then colored each pixel in
the image according to the most frequent class for the pixel. The results are shown in Fig. 2.
While afﬁnity propagation is deterministic, the EM algorithm depends on initialization. So,
we ran the EM algorithm 1000 times and in Fig. 2 we plot the cumulative distribution of
the log P scores obtained by EM. The score for afﬁnity propagation is also shown, and
achieves near-best performance (98th percentile).
We next analyzed the more natural 192 × 192 image shown in Fig. 3. Since there is no
natural background color, we use mean-squared pixel differences in HSV color space to
measure similarity between the 24 × 24 patches:
Lik ∝ e−β minT Σm∈W (xm
i −f m (xk ,T ))2
,
where W is the set of indices corresponding to a 16 × 16 window centered in the patch
and f m (xk , T ) is the same as above. As before, we arbitrarily set β = 1 and Lkk to
mediani,k 6=iLik .

Figure 3: Segmentation results for several methods applied to a natural image. For methods
other than afﬁnity propagation, many parameter settings were tried and the best segmenta-
tion selected. The histograms show the percentile in score achieved by afﬁnity propagation
compared to 1000 runs of greedy EM, for different random training sets.
We cut the image in Fig. 3 into an 8 × 8 grid of non-overlapping 24 × 24 patches and
clustered them into K = 6 classes using afﬁnity propagation (both forms), greedy EM
in our model, spectral clustering (using a normalized L-matrix based on a set of 29 × 29
overlapping patches), and mixtures of Gaussians2 . For greedy EM, the afﬁnity propagation
algorithms, and mixtures of Gaussians, we then choose all possible 24 × 24 overlapping
patches and calculated the likelihoods of them given each of the 6 cluster centers, classify-
ing each patch according to its maximum likelihood.

Fig. 3 shows the segmentations for the various methods, where the central pixel of each
patch is colored according to its class. Again, afﬁnity propagation achieves a solution that
is near-best compared to one thousand runs of greedy EM.
5 Learning mixtures of gene models
Currently, an important problem in genomics research is the discovery of genes and gene
variants that are expressed as messenger RNAs (mRNAs) in normal tissues. In a recent
study [11], we used DNA-based techniques to identify 837,251 possible exons (“putative
exons”) in the mouse genome. For each putative exon, we used an Agilent microarray
probe to measure the amount of corresponding mRNA that was present in each of 12 mouse
tissues. Each 12-D vector, called an “expression pro ﬁle ”, can be viewed as a feature vector
indicating the putative exon’s function. By grouping together feature vectors for nearby
probes, we can detect genes and variations of genes. Here, we compare afﬁnity propagation
with hierarchical clustering, which was previously used to ﬁnd gene structures [12].

Fig. 4a shows a normalized subset of the data and gives three examples of groups of nearby

2For spectral clustering, we tried β = 0.5, 1 and 2, and for each of these tried clustering using 6, 8,
10, 12 and 14 eigenvectors. We then visually picked the best segmentation (β = 1, 10 eigenvectors).
The eigenvector features were clustered using EM in a mixture of Gaussians and out of 10 trials,
the solution with highest likelihood was selected. For mixtures of Gaussians applied directly to the
image patches, we picked the model with highest likelihood in 10 trials.

(a)

(b)

Figure 4: (a) A normalized subset of 837,251 tissue expression proﬁles – mRNA level
versus tissue – for putative exons from the mouse genome (most proﬁles are much noisier
than these). (b) The true exon detection rate (in known genes) versus the false discovery
rate, for afﬁnity propagation and hierarchical clustering.

(cid:17)
dydzdσ

,

q · p0 (xi ) + (1− q) max
y ,z ,σ

− 1
i −(y ·xm
2σ2 Σ12
j +z))2
m=1 (xm
p(y , z , σ) e
√
12
2πσ2
i −(y ·xm
− 1
2σ2 Σ12
j +z))2
m=1 (xm
p(y , z , σ) e
√
12

feature vectors that are similar enough to provide evidence of gene units. The actual data
is generally much noisier, and includes multiplicative noise (exon probe sensitivity can
vary by two orders of magnitude), correlated additive noise (a probe can cross-hybridize in
a tissue-independent manner to background mRNA sources), and spurious additive noise
(due to a noisy measurement procedure and biological effects such as alternative splicing).
To account for noise, false putative exons, and the distance between exons in the same
Z
Lij = λe−λ|i−j |(cid:16)
(cid:17)
gene, we used the following likelihood function:
q · p0 (xi ) + (1− q)
≈ λe−λ|i−j |(cid:16)
2πσ2
where xm
is the expression level for the mth tissue in the ith probe (in genomic order).
i
We found that in this application, the maximum is a sufﬁciently good approximation to
the integral. The distribution over the distance between probes in the same gene |i − j |
is assumed to be geometric with parameter λ. p0 (xi ) is a background distribution that
accounts for false putative exons and q is the probability of a false putative exon within a
gene. We assumed y , z and σ are independent and uniformly distributed3 . The Bayesian
prior probability that xk is an exemplar is set to θ · p0 (xk ), where θ is a control knob used
to vary the sensitivity of the system.
Because of the term λe−λ|i−j | and the additional assumption that genes on the same strand
do not overlap, it is not necessary to propagate afﬁnities between all 837, 2512 pairs of
training cases. We assume Lij = 0 for |i − j | > 100, in which case it is not necessary
to propagate afﬁnities between xi and xj . The assumption that genes do not overlap im-
plies that if si = k , then sj = k for j ∈ {min(i, k), . . . , max(i, k)}. It turns out that
this constraint causes the dependence structure in the update equations for the afﬁnities to
reduce to a chain, so afﬁnities need only be propagated forward and backward along the
genome. After afﬁnity propagation is used to automatically select the number of mixture

3Based on the experimental procedure and a set of previously-annotated genes (RefSeq), we es-
timated λ = 0.05, q = 0.7, y ∈ [.025, 40], z ∈ [−µ, µ] (where µ = maxi,m xm
i ), σ ∈ (0, µ]. We
used a mixture of Gaussians for p0 (xi ), which was learned from the entire training set.

components and identify the mixture centers and the probes that belong to them (genes),
each probe xi is labeled as an exon or a non-exon depending on which of the two terms in
the above likelihood function (q · p0 (xi ) or the large term to its right) is larger.
Fig. 4b shows the fraction of exons in known genes detected by afﬁnity propagation
versus the false detection rate. The curve is obtained by varying the sensitivity parameter,
θ . The false detection rate was estimated by randomly permuting the order of the
probes in the training set, and applying afﬁnity propagation. Even for quite low false
discovery rates, afﬁnity propagation identiﬁes over one third of the known exons. Using
a variety of metrics, including the above metric, we also used hierarchical clustering
to detect exons. The performance of hierarchical clustering using the metric with
highest sensitivity is also shown. Afﬁnity propagation has signiﬁcantly higher sensitiv-
ity, e.g., achieving a ﬁve-fold increase in true detection rate at a false detection rate of 0.4%.
6 Computational efﬁciency
The following table compares the MATLAB execution times of our implementations of
the methods we compared on the problems we studied. For methods that ﬁrst compute
a likelihood or afﬁnity matrix, we give the timing of this computation ﬁrst. Techniques
denoted by “*” were run many times to obtain the shown results, but the given time is for
a single run.

NIPS
Dog
Genes

Afﬁnity Prop
12.9 s + 2.0 s
12.0 s + 1.5 s
16 m + 43 m

K -medoids*
12.9 s + .2 s
12.0 s + 0.1 s
-

Spec Clust* MOG EM* Hierarch Clust
-
-
-
-
3.3 s
12.0 s + 29 s
-
-
16 m + 28 m

7 Summary
An advantage of afﬁnity propagation is that the update rules are deterministic, quite simple,
and can be derived as an instance of the sum-product algorithm in a factor graph. Using
challenging applications, we showed that afﬁnity propagation obtains better solutions (in
terms of percentile log-likelihood, visual quality of image segmentation and sensitivity-
to-speciﬁcity) than other techniques, including K -medoids, spectral clustering, Gaussian
mixture modeling and hierarchical clustering.

To our knowledge, afﬁnity propagation is the ﬁrst algorithm to combine advantages of
pair-wise clustering methods that make use of bottom-up evidence and model-based
methods that seek to ﬁt top-down global models to the data.
References
[1] CM Bishop. Neural Networks for Pattern Recognition. Oxford University Press, NY, 1995.
[2] KA Heller, Z Ghahramani. Bayesian hierarchical clustering. ICML, 2005.
[3] M Meila, J Shi. Learning segmentation by random walks. NIPS 14, 2001.
[4] J Shi, J Malik. Normalized cuts and image segmentation. Proc CVPR, 731-737, 1997.
[5] A Ng, M Jordan, Y Weiss. On spectral clustering: Analysis and an algorithm. NIPS 14, 2001.
[6] N Shental A Zomet T Hertz Y Weiss. Pairwise clustering and graphical models NIPS 16 2003.
[7] R Rosales, BJ Frey. Learning generative models of afﬁnity matrices. Proc UAI, 2003.
[8] M Charikar, S Guha, A Tardos, DB Shmoys. A constant-factor approximation algorithm for the
k-median problem. J Comp and Sys Sci, 65:1, 129-149, 2002.
[9] J Malik et al.. Contour and texture analysis for image segmentation. IJCV 43:1, 2001.
[10] FR Kschischang, BJ Frey, H-A Loeliger. Factor graphs and the sum-product algorithm. IEEE
Trans Info Theory 47:2, 498-519, 2001.
[11] BJ Frey, QD Morris, M Robinson, TR Hughes. Finding novel transcripts in high-resolution
genome-wide microarray data using the GenRate model. Proc RECOMB 2005, 2005.
[12] D. D. Shoemaker et al. Experimental annotation of the human genome using microarray tech-
nology. Nature 409, 922-927, 2001.


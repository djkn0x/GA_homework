Comparing the Effects of Different Weight
Distributions on Finding Sparse Representations

David Wipf and Bhaskar Rao ∗
Department of Electrical and Computer Engineering
University of California, San Diego, CA 92093
dwipf@ucsd.edu, brao@ece.ucsd.edu

Abstract

Given a redundant dictionary of basis vectors (or atoms), our goal is to
ﬁnd maximally sparse representations of signals. Previous ly, we have
argued that a sparse Bayesian learning (SBL) framework is particularly
well-suited for this task, showing that it has far fewer local minima than
other Bayesian-inspired strategies. In this paper, we provide further evi-
dence for this claim by proving a restricted equivalence condition, based
on the distribution of the nonzero generating model weights, whereby the
SBL solution will equal the maximally sparse representation. We also
prove that if these nonzero weights are drawn from an approximate Jef-
freys prior, then with probability approaching one, our equivalence con-
dition is satis ﬁed. Finally, we motivate the worst-case sce nario for SBL
and demonstrate that it is still better than the most widely used sparse rep-
resentation algorithms. These include Basis Pursuit (BP), which is based
on a convex relaxation of the ℓ0 (quasi)-norm, and Orthogonal Match-
ing Pursuit (OMP), a simple greedy strategy that iteratively selects basis
vectors most aligned with the current residual.

Introduction
1
In recent years, there has been considerable interest in ﬁnd ing sparse signal representations
from redundant dictionaries [1, 2, 3, 4, 5]. The canonical form of this problem is given by,

min
w kwk0 ,
where Φ ∈ RN ×M is a matrix whose columns represent an overcomplete or redundant
basis (i.e., rank(Φ) = N and M > N ), w ∈ RM is the vector of weights to be learned,
and t is the signal vector. The cost function being minimized represents the ℓ0 (quasi)-norm
of w (i.e., a count of the nonzero elements in w).

s.t. t = Φw ,

(1)

Unfortunately, an exhaustive search for the optimal representation requires the solution of
up to (cid:0)M
N (cid:1) linear systems of size N × N , a prohibitively expensive procedure for even
modest values of M and N . Consequently, in practical situations there is a need for ap-
proximate procedures that efﬁciently solve (1) with high pr obability. To date, the two most
widely used choices are Basis Pursuit (BP) [1] and Orthogonal Matching Pursuit (OMP)
[5]. BP is based on a convex relaxation of the ℓ0 norm, i.e., replacing kwk0 with kwk1 ,
which leads to an attractive, unimodal optimization problem that can be readily solved via
linear programming. In contrast, OMP is a greedy strategy that iteratively selects the basis

∗This work was supported by DiMI grant 22-8376, Nissan, and NSF grant DGE-0333451.

vector most aligned with the current signal residual. At each step, a new approximant is
formed by projecting t onto the range of all the selected dictionary atoms.

Previously [9], we have demonstrated an alternative algorithm for solving (1) using a sparse
Bayesian learning (SBL) framework [6] that maintains several signi ﬁcant advantages over
other, Bayesian-inspired strategies for ﬁnding sparse sol utions [7, 8]. The most basic for-
mulation begins with an assumed likelihood model of the signal t given weights w ,
p(t|w) = (2πσ2 )−N/2 exp (cid:18)−
2(cid:19) .
1
2σ2 kt − Φwk2
To provide a regularizing mechanism, SBL uses the parameterized weight prior
M
w2
2γi (cid:19) ,
(2πγi )−1/2 exp (cid:18)−
Yi=1
i
where γ = [γ1 , . . . , γM ]T is a vector of M hyperparameters controlling the prior variance
of each weight. These hyperparameters can be estimated from the data by marginalizing
over the weights and then performing ML optimization. The cost function for this task is
L(γ ) = − log Z p(t|w)p(w ; γ )dw ∝ log |Σt | + tT Σ−1
t t,
where Σt , σ2 I + ΦΓΦT and we have introduced the notation Γ , diag(γ ). This pro-
cedure, which can be implemented via the EM algorithm (or some other technique), is
referred to as evidence maximization or type-II maximum likelihood [6]. Once γ has been
estimated, a closed-form expression for the posterior weight distribution is available.

p(w ; γ ) =

(4)

(2)

(3)

Although SBL was initially developed in a regression context, it can be easily adapted to
handle (1) in the limit as σ2 → 0. To accomplish this we must reexpress the SBL iterations
to handle the low noise limit. Applying various matrix identities to the EM algorithm-based
update rules for each iteration, we arrive at the modi ﬁed upd ate [9]
γ(new) = diag (cid:18) ˆw(old) ˆwT
(old) + (cid:20)I − Γ1/2
(old)(cid:17)† Φ(cid:21) Γ(old)(cid:19)
(old) (cid:16)ΦΓ1/2
(new)(cid:17)† t,
(new) (cid:16)ΦΓ1/2
ˆw(new) = Γ1/2
where (·)† denotes the Moore-Penrose pseudo-inverse. Given that t ∈ range(Φ) and as-
suming γ is initialized with all nonzero elements, then feasibility is enforced at every itera-
tion, i.e., t = Φ ˆw . We will henceforth refer to wSBL as the solution of this algorithm when
initialized at Γ = IM and ˆw = Φ† t.1 In [9] (which extends work in [10]), we have argued
why wSBL should be considered a viable candidate for solving (1).

(5)

In comparing BP, OMP, and SBL, we would ultimately like to know in what situations a
particular algorithm is likely to ﬁnd the maximally sparse s olution. A variety of results stip-
ulate rigorous conditions whereby BP and OMP are guaranteed to solve (1) [1, 4, 5]. All
of these conditions depend explicitly on the number of nonzero elements contained in the
optimal solution. Essentially, if this number is less than some Φ-dependent constant κ, the
BP/OMP solution is proven to be equivalent to the minimum ℓ0 -norm solution. Unfortu-
nately however, κ turns out to be restrictively small and, for a ﬁxed redundanc y ratio M /N ,
grows very slowly as N becomes large [3]. But in practice, both approaches still perform
well even when these equivalence conditions have been grossly violated. To address this
issue, a much looser bound has recently been produced for BP, dependent only on M /N .
This bound holds for “most ” dictionaries in the limit as N becomes large [3], where “most ”

1Based on EM convergence properties, the algorithm will converge monotonically to a ﬁxed point.

is with respect to dictionaries composed of columns drawn uniformly from the surface of
an N -dimensional unit hypersphere. For example, with M /N = 2, it is argued that BP is
capable of resolving sparse solutions with roughly 0.3N nonzero elements with probability
approaching one as N → ∞.
Turning to SBL, we have neither a convenient convex cost function (as with BP) nor a
simple, transparent update rule (as with OMP); however, we can nonetheless come up with
an alternative type of equivalence result that is neither unequivocally stronger nor weaker
than those existing results for BP and OMP. This condition is dependent on the relative
magnitudes of the nonzero elements embedded in optimal solutions to (1). Additionally,
we can leverage these ideas to motivate which sparse solutions are the most difﬁcult to ﬁnd.
Later, we provide empirical evidence that SBL, even in this worst-case scenario, can still
outperform both BP and OMP.

2 Equivalence Conditions for SBL
In this section, we establish conditions whereby wSBL will minimize (1). To state these
results, we require some notation. First, we formally deﬁne a dictionary Φ = [φ1 , . . . , φM ]
as a set of M unit ℓ2 -norm vectors (atoms) in RN , with M > N and rank(Φ) = N . We
say that a dictionary satis ﬁes the unique representation pr operty (URP) if every subset of
N atoms forms a basis in RN . We deﬁne w(i) as the i-th largest weight magnitude and ¯w
as the kwk0 -dimensional vector containing all the nonzero weight magnitudes of w . The
set of optimal solutions to (1) is W ∗ with cardinality |W ∗ |. The diversity (or anti-sparsity)
of each w∗ ∈ W ∗ is deﬁned as D∗ , kw∗ k0 .
Result 1. For a ﬁxed dictionary Φ that satis ﬁes the URP, there exists a set of M − 1 scaling
constants νi ∈ (0, 1] (i.e., strictly greater than zero) such that, for any t = Φw ′ generated
with
w ′(i+1) ≤ νiw ′(i)
(6)
i = 1, . . . , M − 1,
SBL will produce a solution that satis ﬁes kwSBL k0 = min(N , kw ′ k0 ) and wSBL ∈ W ∗ .
Do to space limitations, the proof has been deferred to [11]. The basic idea is that, as
the magnitude differences between weights increase, at any given scale, the covariance
Σt embedded in the SBL cost function is dominated by a single dictionary atom such that
problematic local minimum are removed. The unique, global minimum in turn achieves the
stated result.2 The most interesting case occurs when kw ′ k0 < N , leading to the following:
Corollary 1. Given the additional restriction kw ′ k0 < N , then wSBL = w ′ ∈ W ∗ and
|W ∗ | = 1, i.e., SBL will ﬁnd the unique, maximally sparse representa tion of the signal t.
See [11] for the proof. These results are restrictive in the sense that the dictionary dependent
constants νi signi ﬁcantly conﬁne the class of signals
t that we may represent. Moreover,
we have not provided any convenient means of computing what the different scaling con-
stants might be. But we have nonetheless solidi ﬁed the notio n that SBL is most capable of
recovering weights of different scales (and it must still ﬁn d all D∗ nonzero weights no mat-
ter how small some of them may be). Additionally, we have speci ﬁed conditions whereby
we will ﬁnd the unique w∗ even when the diversity is as large as D∗ = N − 1. The tighter
BP/OMP bound from [1, 4, 5] scales as O (cid:0)N −1/2 (cid:1), although this latter bound is much
more general in that it is independent of the magnitudes of the nonzero weights.
In contrast, neither BP or OMP satisfy a comparable result; in both cases, simple 3D
counter examples sufﬁce to illustrate this point. 3 We begin with OMP. Assume the fol-

2Because we have effectively shown that the SBL cost function must be unimodal, etc., any proven
descent method could likely be applied in place of (5) to achieve the same result.
3While these examples might seem slightly nuanced, the situations being illustrated can occur
frequently in practice and the requisite column normalization introduces some complexity.

1
ǫ
0
0

(7)

(8)

0
0
1

1√2
0
1√2

0
1
0

0 ]T .

(9)

1√1.01
0.1√1.01
0

ǫ√2
0
1 + ǫ√2

lowing:
w∗ = 

Φ = 


t = Φw∗ = 
 ,





where Φ satis ﬁes the URP and has columns φi of unit ℓ2 norm. Given any ǫ ∈ (0, 1),
we will now show that OMP will necessarily fail to ﬁnd w∗ . Provided ǫ < 1, at the ﬁrst
iteration OMP will select φ1 , which solves maxi |tT φi |, leaving the residual vector
1 (cid:1) t = [ ǫ/√2
r1 = (cid:0)I − φ1φT
0 0 ]T .
Next, φ4 will be chosen since it has the largest value in the top position, thus solving
1 φi |. The residual is then updated to become
maxi |rT
ǫ
r2 = (cid:0)I − [ φ1 φ4 ][ φ1 φ4 ]T (cid:1) t =
101√2
[ 1 −10
From the remaining two columns, r2 is most highly correlated with φ3 . Once φ3 is se-
lected, we obtain zero residual error, yet we did not ﬁnd w∗ , which involves only φ1 and
φ2 . So for all ǫ ∈ (0, 1), the algorithm fails. As such, there can be no ﬁxed constant ν > 0
such that if w∗(2) ≡ ǫ ≤ νw∗(1) ≡ ν , we are guaranteed to obtain w∗ (unlike with SBL).
We now give an analogous example for BP, where we present a feasible solution with
smaller ℓ1 norm than the maximally sparse solution. Given
1
0.1√1.02
0.1√1.02
Φ = 

w∗ = 

t = Φw∗ = " ǫ
1 # ,
ǫ
−0.1√1.02
0.1√1.02
0




0
1√1.02
1√1.02
0
it is clear that kw∗ k1 = 1 + ǫ. However, for all ǫ ∈ (0, 0.1),
if we form a
feasible solution using only φ1 , φ3 , and φ4 , we obtain the alternate solution w =
5√1.02ǫ 5√1.02ǫ (cid:3)T with kwk1 ≈ 1 + 0.1ǫ. Since this has a smaller
(cid:2) (1 − 10ǫ)
0
ℓ1 norm for all ǫ in the speci ﬁed range, BP will necessarily fail and so again, we cannot
reproduce the result for a similar reason as before.
At this point, it remains unclear what probability distributions are likely to produce weights
that satisfy the conditions of Result 1.
It turns out that the Jeffreys prior, given by
p(x) ∝ 1/x, is appropriate for this task. This distribution has the unique property that
the probability mass assigned to any given scaling is equal. More explicitly, for any s ≥ 1,
P (cid:0)x ∈ (cid:2)si , si+1 (cid:3)(cid:1) ∝ log(s)
(11)
∀i ∈ Z.
For example, the probability that x is between 1 and 10 equals the probability that it lies
between 10 and 100 or between 0.01 and 0.1. Because this is an improper density, we
deﬁne an approximate Jeffreys prior with range parameter a ∈ (0, 1]. Speci ﬁcally, we say
that x ∼ J (a) if
−1
for x ∈ [a, 1/a].
2 log(a)x
With this deﬁnition in mind, we present the following result
.
Result 2. For a ﬁxed Φ that satis ﬁes the URP, let
t be generated by t = Φw ′ , where w ′
has magnitudes drawn iid from J (a). Then as a approaches zero, the probability that we
obtain a w ′ such that the conditions of Result 1 are satis ﬁed approaches unity.
Again, for space considerations, we refer the reader to [11]. However, on a conceptual
level this result can be understood by considering the distribution of order statistics. For

0 1
0 0
1 0

(10)

p(x) =

(12)

example, given M samples from a uniform distribution between zero and some θ , with
probability approaching one, the distance between the k-th and (k + 1)-th order statistic can
be made arbitrarily large as θ moves towards inﬁnity. Likewise, with the J (a) distribution,
the relative scaling between order statistics can be increased without bound as a decreases
towards zero, leading to the stated result.
Corollary 2. Assume that D ′ < N randomly selected elements of w ′ are set to zero.
Then as a approaches zero, the probability that we satisfy the conditions of Corollary 1
approaches unity.

In conclusion, we have shown that a simple, (approximate) noninformative Jeffreys prior
leads to sparse inverse problems that are optimally solved via SBL with high probability.
Interestingly, it is this same Jeffreys prior that forms the implicit weight prior of SBL (see
[6], Section 5.1). However, it is worth mentioning that other Jeffreys prior-based tech-
1
niques, e.g., direct minimization of p(w) = Qi
subject to t = Φw , do not provide
|wi |
any SBL-like guarantees. Although several algorithms do exist that can perform such a
minimization task (e.g., [7, 8]), they perform poorly with respect to (1) because of conver-
gence to local minimum as shown in [9, 10]. This is especially true if the weights are highly
scaled, and no nontrivial equivalence results are known to exist for these procedures.

3 Worst-Case Scenario
If the best-case scenario occurs when the nonzero weights are all of very different scales,
it seems reasonable that the most difﬁcult sparse inverse pr oblem may involve weights of
the same or even identical scale, e.g., ¯w∗1 = ¯w∗2 = . . . ¯w∗D∗ . This notion can be formalized
somewhat by considering the ¯w∗ distribution that is furthest from the Jeffreys prior. First,
we note that both the SBL cost function and update rules are independent of the overall
scaling of the generating weights, meaning α ¯w∗ is functionally equivalent to ¯w∗ provided
α is nonzero. This invariance must be taken into account in our analysis. Therefore, we
assume the weights are rescaled such that Pi ¯w∗i = 1. Given this restriction, we will ﬁnd
the distribution of weight magnitudes that is most different from the Jeffreys prior.
Using the standard procedure for changing the parameterization of a probability density,
the joint density of the constrained variables can be computed simply as
D∗
1
Xi=1
¯w∗i = 1, ¯w∗i ≥ 0, ∀i.
p( ¯w∗1 , . . . , ¯w∗D∗ ) ∝
QD∗
i=1 ¯w∗i
From this expression, it is easily shown that ¯w∗1 = ¯w∗2 = . . . = ¯w∗D∗ achieves the global
minimum. Consequently, equal weights are the absolute least likely to occur from the
Jeffreys prior. Hence, we may argue that the distribution that assigns ¯w∗i = 1/D∗ with
probability one is furthest from the constrained Jeffreys prior.

(13)

for

Nevertheless, because of the complexity of the SBL framework, it is difﬁcult to prove ax-
iomatically that ¯w∗ ∼ 1 is overall the most problematic distribution with respect to sparse
recovery. We can however provide additional motivation for why we should expect it to
be unwieldy. As proven in [9], the global minimum of the SBL cost function is guaran-
teed to produce some w∗ ∈ W ∗ . This minimum is achieved with the hyperparameters
γ ∗i = (w∗i )2 , ∀i. We can think of this solution as forming a collapsed, or degenerate co-
variance Σ∗t = ΦΓ∗ΦT that occupies a proper D∗ -dimensional subspace of N -dimensional
signal space. Moreover, this subspace must necessarily contain the signal vector t. Essen-
tially, Σ∗t proscribes inﬁnite density to t, leading to the globally minimizing solution.
Now consider an alternative covariance Σ⋄t that, although still full rank, is nonetheless ill-
conditioned ( ﬂattened), containing t within its high density region. Furthermore, assume
that Σ⋄t is not well aligned with the subspace formed by Σ∗t . The mixture of two ﬂat-
tened, yet misaligned covariances naturally leads to a more voluminous (less dense) form

as measured by the determinant |αΣ∗t + βΣ⋄t |. Thus, as we transition from Σ⋄t to Σ∗t , we
necessarily reduce the density at t, thereby increasing the cost function L(γ ). So if SBL
converges to Σ⋄t it has fallen into a local minimum.
So the question remains, what values of ¯w∗ are likely to create the most situations where
this type of local minima occurs? The issue is resolved when we again consider the D∗ -
dimensional subspace determined by Σ∗t . The volume of the covariance within this sub-
¯Φ ¯Γ∗ ¯Φ∗T (cid:12)(cid:12)
, where ¯Φ∗ and ¯Γ∗ are the basis vectors and hyperparameters
space is given by (cid:12)(cid:12)
associated with ¯w∗ . The larger this volume, the higher the probability that other basis vec-
tors will be suitably positioned so as to both (i), contain t within the high density portion
and (ii), maintain a sufﬁcient component that is misaligned with the optimal covariance.
¯Φ∗ ¯Γ∗ ¯Φ∗T (cid:12)(cid:12)
under the constraints Pi ¯w∗i = 1 and ¯γ ∗i = ( ¯w∗ )2
The maximum volume of (cid:12)(cid:12)
i
occurs with ¯γ ∗i = 1/(D∗ )2 , i.e., all the ¯w∗i are equal. Consequently, geometric considera-
tions support the notion that deviance from the Jeffreys prior leads to difﬁculty recovering
w∗ . Moreover, empirical analysis (not shown) of the relationship between volume and
local minimum avoidance provide further corroboration of this hypothesis.

4 Empirical Comparisons
The central purpose of this section is to present empirical evidence that supports our theo-
retical analysis and illustrates the improved performance afforded by SBL. As previously
mentioned, others have established deterministic equivalence conditions, dependent on D∗ ,
whereby BP and OMP are guaranteed to ﬁnd the unique w∗ . Unfortunately, the relevant
theorems are of little value in assessing practical differences between algorithms. This is
because, in the cases we have tested where BP/OMP equivalence is provably known to hold
(e.g., via results in [1, 4, 5]), SBL always converges to w∗ as well.
As such, we will focuss our attention on the insights provided by Sections 2 and 3 as well
as probabilistic comparisons with [3]. Given a ﬁxed distrib ution for the nonzero elements
of w∗ , we will assess which algorithm is best (at least empirically) for most dictionaries
relative to a uniform measure on the unit sphere as discussed.

To this effect, a number of monte-carlo simulations were conducted, each consisting of the
following: First, a random, overcomplete N × M dictionary Φ is created whose entries
are each drawn uniformly from the surface of an N -dimensional hypersphere. Next, sparse
weight vectors w∗ are randomly generated with D∗ nonzero entries. Nonzero amplitudes
¯w∗ are drawn iid from an experiment-dependent distribution. Response values are then
computed as t = Φw∗ . Each algorithm is presented with t and Φ and attempts to estimate
w∗ . In all cases, we ran 1000 independent trials and compared the number of times each
algorithm failed to recover w∗ . Under the speci ﬁed conditions for the generation of Φ
and t, all other feasible solutions w almost surely have a diversity greater than D∗ , so
our synthetically generated w∗ must be maximally sparse. Moreover, Φ will almost surely
satisfy the URP.

With regard to particulars, there are essentially four variables with which to experiment: (i)
the distribution of ¯w∗ , (ii) the diversity D∗ , (iii) N , and (iv) M . In Figure 1, we display
results from an array of testing conditions. In each row of the ﬁgure,
¯w∗i is drawn iid from
i; the ﬁrst row uses ¯w∗i = 1, the second has ¯w∗i ∼ J (a = 0.001),
a ﬁxed distribution for all
and the third uses ¯w∗i ∼ N (0, 1), i.e., a unit Gaussian. In all cases, the signs of the nonzero
weights are irrelevant due to the randomness inherent in the basis vectors.

The columns of Figure 1 are organized as follows: The ﬁrst column is based on the values
N = 50, D∗ = 16, while M is varied from N to 5N , testing the effects of an increasing
level of dictionary redundancy, M /N . The second ﬁxes N = 50 and M = 100 while D∗
is varied from 10 to 30, exploring the ability of each algorithm to resolve an increasing
number of nonzero weights. Finally, the third column ﬁxes M /N = 2 and D∗ /N ≈ 0.3

while N , M , and D∗ are increased proportionally. This demonstrates how performance
scales with larger problem sizes.

Redundancy Test
(N = 50, D* = 16)

Diversity Test
(N = 50, M = 100)

Signal Size Test
(M/N = 2,  D*/N = 0.32)

)
s
t
h
e
g
t
a
i
e
R
w
 
r
 
t
o
i
n
r
r
u
E
 
/
w
(

)
s
t
h
g
i
e
e
t
w
a
R
 
s
 
y
r
o
e
r
r
r
f
f
E
e
J
 
/
w
(

)
s
t
h
g
i
e
e
w
t
a
 
n
R
a
 
r
i
s
o
s
r
r
u
E
a
G
 
/
w
(

1

0.8

0.6

0.4

0.2

0

1

1

0.8

0.6

0.4

0.2

0

1

1

0.8

0.6

0.4

0.2

0

1

2

3

4

5

2

3

4

5

2
4
3
Redundancy Ratio (M/N)

5

1

0.8

0.6

0.4

0.2

0
10

1

0.8

0.6

0.4

0.2

0
10

1

0.8

0.6

0.4

0.2

0
10

1

0.8

0.6

0.4

0.2

0
25

1

0.8

0.6

0.4

0.2

0
25

1

0.8

0.6

0.4

0.2

0
25

50

75 100 125 150

OMP
BP
SBL

50

75 100 125 150

50
75 100 125 150
Signal Size (N)

15

20

25

30

15

20

25

30

15
25
20
Diversity (D*)

30

Figure 1: Empirical results comparing the probability that OMP, BP, and SBL fail to ﬁnd
w∗ under various testing conditions. Each data point is based on 1000 independent trials.
The distribution of the nonzero weight amplitudes is labeled on the far left for each row,
while the values for N , M , and D∗ are included on the top of each column. Independent
variables are labeled along the bottom of the ﬁgure.

The ﬁrst row of plots essentially represents the worst-case scenario for SBL per our pre-
vious analysis, and yet performance is still consistently better than both BP and OMP. In
contrast, the second row of plots approximates the best-case performance for SBL, where
we see that SBL is almost infallible. The handful of failure events that do occur are because
a is not sufﬁciently small and therefore, J (a) was not sufﬁciently close to a true Jeffreys
prior to achieve perfect equivalence (see center plot). Although OMP also does well here,
the parameter a can generally never be adjusted such that OMP always succeeds. Finally,
the last row of plots, based on Gaussian distributed weight amplitudes, reﬂects a balance
between these two extremes. Nonetheless, SBL still holds a substantial advantage.

In general, we observe that SBL is capable of handling more redundant dictionaries (col-
umn one) and resolving a larger number of nonzero weights (column two). Also, column
three illustrates that both BP and SBL are able to resolve a number of weights that grows
linearly in the signal dimension (≈ 0.3N ), consistent with the analysis in [3] (which applies
only to BP). In contrast, OMP performance begins to degrade in some cases (see the upper
right plot), a potential limitation of this approach. Of course additional study is necessary
to fully compare the relative performance of these methods on large-scale problems.

Finally, by comparing row one, two and three, we observe that the performance of BP is
roughly independent of the weight distribution, with performance slightly below the worst-

case SBL performance. Like SBL, OMP results are highly dependent on the distribution;
however, as the weight distribution approaches unity, performance is unsatisfactory.
In
summary, while the relative proﬁciency between OMP and BP is contingent on experimen-
tal particulars, SBL is uniformly superior in the cases we have tested (including examples
not shown, e.g., results with other dictionary types).
5 Conclusions
In this paper, we have related the ability to ﬁnd maximally sp arse solutions to the partic-
ular distribution of amplitudes that compose the nonzero elements. At ﬁrst glance, it may
seem reasonable that the most difﬁcult sparse inverse probl ems occur when some of the
nonzero weights are extremely small, making them difﬁcult t o estimate. Perhaps surpris-
ingly then, we have shown that the exact opposite is true with SBL: The more diverse the
weight magnitudes, the better the chances we have of learning the optimal solution. In
contrast, unit weights offer the most challenging task for SBL. Nonetheless, even in this
worst-case scenario, we have shown that SBL outperforms the current state-of-the-art; the
overall assumption here being that, if worst-case performance is superior, then it is likely
to perform better in a variety of situations.
For a ﬁxed dictionary and diversity D∗ , successful recovery of unit weights does not ab-
solutely guarantee that any alternative weighting scheme will necessarily be recovered as
well. However, a weaker result does appear to be feasible: For ﬁxed values of N , M ,
and D∗ , if the success rate recovering unity weights approaches one for most dictionar-
ies, where most is deﬁned as in Section 1, then the success rat e recovering weights of any
other distribution (assuming they are distributed independently of the dictionary) will also
approach one. While a formal proof of this conjecture is beyond the scope of this paper,
it seems to be a very reasonable result that is certainly born out by experimental evidence,
geometric considerations, and the arguments presented in Section 3. Nonetheless, this re-
mains a fruitful area for further inquiry.
References
[1] D. Donoho and M. Elad, “Optimally sparse representation in general
(nonorthogonal) dictionar-
ies via ℓ1 minimization,” Proc. Nat. Acad. Sci., vol. 100, no. 5, pp. 2197–2202, March 2003.
[2] R. Gribonval and M. Nielsen,
“Sparse representations in unions of bases,”
IEEE Transactions
on Information Theory, vol. 49, pp. 3320–3325, Dec. 2003.
[3] D. Donoho, “For most large underdetermined systems of linear eq uations the minimal ℓ1 -norm
solution is also the sparsest solution,” Stanford University Technical Report, September 2004.
[4] J.J. Fuchs,
“On sparse representations in arbitrary redundant bases,”
IEEE Transactions on
Information Theory, vol. 50, no. 6, pp. 1341–1344, June 2004.
[5] J.A. Tropp, “Greed is good: Algorithmic results for sparse appro ximation,”
on Information Theory, vol. 50, no. 10, pp. 2231–2242, October 2004.
[6] M.E. Tipping, “Sparse Bayesian learning and the relevance vector machine,” Journal of Machine
Learning Research, vol. 1, pp. 211–244, 2001.
[7] I.F. Gorodnitsky and B.D. Rao, “Sparse signal reconstruction f rom limited data using FOCUSS:
A re-weighted minimum norm algorithm,”
IEEE Transactions on Signal Processing, vol. 45, no.
3, pp. 600–616, March 1997.
[8] M.A.T. Figueiredo, “Adaptive sparseness using Jeffreys prio r,” Advances in Neural Information
Processing Systems 14, pp. 697–704, 2002.
[9] D.P. Wipf and B.D. Rao,
“ ℓ0 -norm minimization for basis selection,”
Information Processing Systems 17, pp. 1513–1520, 2005.
[10] D.P. Wipf and B.D. Rao, “Sparse Bayesian learning for basis se lection,”
Signal Processing, vol. 52, no. 8, pp. 2153–2164, 2004.
[11] D.P. Wipf, To appear in Bayesian Methods for Sparse Signal Representation, PhD Dissertation,
UC San Diego, 2006 (estimated). http://dsp.ucsd.edu/∼dwipf/

IEEE Transactions on

IEEE Transactions

Advances in Neural


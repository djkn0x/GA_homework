Fusion of Similarity Data in Clustering

Tilman Lange and Joachim M. Buhmann
(langet,jbuhmann)@inf.ethz.ch
Institute of Computational Science, Dept. of Computer Sience,
ETH Zurich, Switzerland

Abstract

Fusing multiple information sources can yield signiﬁcant beneﬁts to suc-
cessfully accomplish learning tasks. Many studies have focussed on fus-
ing information in supervised learning contexts. We present an approach
to utilize multiple information sources in the form of similarity data for
unsupervised learning. Based on similarity information, the clustering
task is phrased as a non-negative matrix factorization problem of a mix-
ture of similarity measurements. The tradeoff between the informative-
ness of data sources and the sparseness of their mixture is controlled by
an entropy-based weighting mechanism. For the purpose of model se-
lection, a stability-based approach is employed to ensure the selection
of the most self-consistent hypothesis. The experiments demonstrate the
performance of the method on toy as well as real world data sets.

1

Introduction

Clustering has found increasing attention in the past few years due to the enormous infor-
mation ﬂood in many areas of information processing and data analysis. The ability of an
algorithm to determine an interesting partition of the set of objects under consideration,
however, heavily depends on the available information. It is, therefore, reasonable to equip
an algorithm with as much information as possible and to endow it with the capability to
distinguish between relevant and irrelevant information sources. How to reasonably iden-
tify a weighting of the different information sources such that an interesting group structure
can be successfully uncovered, remains, however, a largely unresolved issue.
Different sources of information about the same objects naturally arise in many application
scenarios. In computer vision, for example, information sources can consist of plain in-
tensity measurements, edge maps, the similarity to other images or even human similarity
assessments. Similarly in bio-informatics: the similarity of proteins,e.g., can be assessed in
different ways, ranging from the comparison of gene pro ﬁles to direct comparisons at the
sequence level using alignment methods.
In this work, we use a non-negative matrix factorization approach (nmf) to pairwise clus-
tering of similarity data that is extended in a second step in order to incorporate a suitable
weighting of multiple information sources, leading to a mixture of similarities. The latter
represents the main contribution of this work. Algorithms for nmf have recently found a
lot of attention. Our proposal is inspired by the work in [11] and [5]. Only recently, [18]
have also employed a nmf to perform clustering. For the purpose of model selection, we
employ a stability-based approach that has already been successfully applied to model se-

lection problems in clustering (e.g. in [9]). Instead of following the strategy to ﬁrst embed
the similarities into a space with Euclidean geometry and then to perform clustering and,
where required, feature selection/weighting on the stacked feature vector, we advocate an
approach that is closer to the original similarity data by performing nmf.
Some work has been devoted to feature selection and weighting in clustering problems. In
[13] a variant of the k-means algorithm has been studied that employs the Fisher criterion
to assess the importance of individual features. In [14, 10], Gaussian mixture model-based
approaches to feature selection are introduced. The more general problem of learning a
suitable metric has also been investigated, e.g. in [17]. Similarity measurements represent
a particularly generic form of providing input to a clustering algorithm. Fusing such repre-
sentations has only recently been studied in the context of kernel-based supervised learning,
e.g. in [7] using semi-deﬁnite programming and in [3] using a boosting procedure. In [1],
an approach to learning the bandwidth parameter of an rbf-kernel for spectral clustering is
studied.
The paper is organized as follows: section 2 introduces the nmf-based clustering method
combined with a data-source weighting (section 3). Section 4 discusses an out-of-sample
extension allowing us to predict assignments and to employ the stability principle for model
selection. Experimental evidence in favor of our approach is given in section 5.

2 Clustering by Non-Negative Matrix Factorization
Suppose we want to group a ﬁnite set of objects On := {o1 , . . . , on }. Usually, there are
multiple ways of measuring the similarity between different objects. Such relations give
rise to similarities sij := s(oi , oj ) 1 where we assume non-negativity sij ≥ 0, symmetry
sj i = sij , and boundedness sij < ∞. For n objects, we summarize the similarity data in a
n×n matrix S = (sij ) which is re-normalized to P = S/1t
nS1n , where 1n := (1, . . . , 1)t .
The re-normalized similarities can be interpreted as the probability of the joint occurrence
of objects i, j .
We aim now at ﬁnding a non-negative matrix factorization of P ∈ [0, 1]n×n into a product
WHt of the n × k matrices W and H with non-negative entries for which additionally
nW1k = 1 and Ht1n = 1k , where k denotes the number of clusters. That is,
holds 1t
one aims at explaining the overall probability for a co-occurrence by a latent cause, the
unobserved classes. The constraints ensure, that the entries of both, W and H, can be
considered as probabilities: the entry wiν of W is the joint probability q(i, ν ) of object
P
i and class ν whereas hjk in H is the probability q(j |ν ). This model implicitly assumes
independence of object i and j conditioned on ν . Given a factorization of P in W and H,
we can use the maximum a posteriori estimate, arg maxν hiν
j wjν , to arrive at a hard
assignment of objects to classes.
pij log X
C (PkWHt ) := − X
In order to obtain a factorization, we minimize the cross-entropy
wiν hj ν
ν
i,j
is inspired by the Expectation-Maximization (EM) algorithm: Let τν ij ≥ 0 with P
which becomes minimal iff P = WHt 2 and is not convex in W and H together. Note, that
1. Then, by the convexity of − log x, we obtain − log P
ν wiν hj ν ≤ − P
the factorization is not necessarily unique. We resort to a local optimization scheme, which
ν τν ij =
ν τν ij log wiν hjν
,
τν ij
1 In the following, we represent objects by their indices.
2The Kullback-Leibler divergence is D(PkWHt ) = −H (P) + C (PkWHt ) ≥ 0 with equality
iff P = WHt .

(1)

(2)

(3)

Eααα

pij τ (t)
ν ij ,

h(t+1)
jν =

˜C (PkWHt ) := − X
which yields the relaxed objective function:
pij τν ij log wiν hjν + τν ij log τν ij ≥ C (PkWHt ).
i,j,ν
With this relaxation, we can employ an alternating minimization scheme for minimizing
the bound on C . As in EM, one iterates
1. Given W and H, minimize ˜C w.r.t. τν ij
2. Given the values τν ij , ﬁnd estimates for W and H by minimizing ˜C .
P
= X
until convergence, which produces a sequence of estimates
P
P
i pij τ (t)
iν h(t)
w(t)
jν
ν ij
w(t+1)
τ (t)
ν ij =
,
iν
a,b pab τ (t)
µ w(t)
iµ h(t)
use the convention hjν = 0 whenever P
j
jµ
ν ab
that converges to a local minimum of ˜C . This is an instance of an MM algorithm [8]. We
i,j pij τν ij = 0. The per-iteration complexity is
O(n2 ).
3 Fusing Multiple Data Sources
larity matrices P1 , . . . , PL . We introduce now weights αl , 1 ≤ l ≤ L, with P
Measuring the similarity of objects in, say, L different ways results in L normalized simi-
combination ¯P = P
l αl = 1. For
ﬁxed ααα = (αl ) ∈ [0, 1]L , the aggregated and normalized similarity becomes the convex
l αlPl . Hence, ¯pij is a mixture of individual similarities p(l)
ij , i.e. a
mixture of different explanations. Again, we seek a good factorization of ¯P by minimizing
(cid:2)C (Pl kWHt )(cid:3)
the cross-entropy, which then becomes
where Eααα [fl ] = P
min
ααα,W,H
l αl fl denotes the expectation w.r.t. the discrete distribution ααα. The
same relaxation as in the last section can be used, i.e. for all ααα, W and H, we have
Eααα [C (Pl kWHt )] ≤ Eααα [ ˜C (Pl kWHt )]. Hence, we can employ a slightly modiﬁed,
nested alternating minimization approach: Given ﬁxed ααα, obtain estimates W and H using
P
P
= X
X
the relaxation of the last section. The update equations change to
P
P
i p(l)
ij τ (t)
l αl
ν ij
w(t+1)
p(l)
ij τ (t)
h(t+1)
j ν =
.
αl
ν ij ,
iν
ij τ (t)
i,j p(l)
l αl
j
l
ν ij
c = (cl )l . Minimizing the expression in equation (4) subject to the constraints P
Given the current estimates of W and H, we could minimize the objective in equation (4)
w.r.t. ααα subject to the constraint kαααk1 = 1. To this end, set cl := C (Pl kWHt ) and let
l αl = 1
and ααα (cid:23) 0, therefore, becomes a linear program (LP) minααα ctααα such that 1t
Lααα = 1, ααα (cid:23) 0,
where (cid:23) denotes the element-wise ≥-relation. The LP solution is very sparse since the opti-
mal solutions for the linear program lie on the corners of the simplex in the positive orthant
spanned by the constraints. In particular, it lacks a means to control the sparseness of the
coefﬁcients ααα. We, therefore, use a maximum entropy approach ([6]) for sparseness control:
the entropy is upper bounded by log L and measures the sparseness of the vector ααα, since
the lower the entropy the more peaked the distribution ααα can be. Hence, by lower bounding
the entropy, we specify the maximal admissible sparseness. This approach is reasonable
as we actually want to combine multiple (not only identify one) information sources but
the best ﬁt in an unsupervised problem will be usually obtained by choosing only a single

(4)

(5)

source. Thus, we modify the objective originally given in eq. (4) to the entropy-regularized
problem Eααα [ ˜C (Pl kWHt )] − ηH (ααα), so that the mathematical program given above be-
comes
Lααα = 1, ααα (cid:23) 0,
ctααα − ηH (ααα)
min
s.t. 1t
(6)
ααα
where H denotes the (discrete) entropy and η ∈ R+ is a positive Lagrange parameter. The
optimization problem in eq. (6) has an analytical solution, namely the Gibbs distribution
αl ∝ exp(−cl /η)
(7)
For η → ∞ one obtains αl = 1/L, while for η → 0, the LP solution is recovered and the
estimates become the sparser the more the individual cl differ. Put differently, the parameter
η enables us to explore the space of different similarity combinations. The issue of selecting
a reasonable value for the parameter η will be discussed in the next section.
Iterating this nested procedure will yield a locally optimal solution to the problem of mini-
mizing the entropy-constrained objective, since (i) we obtain a local minimum of the mod-
iﬁed objective function and (ii) solving the outer optimization problem can only further
decrease the entropy-constrained objective function.

4 Generalization and Model Selection

In this section, we introduce an out-of-sample extension that allows us to classify objects,
that have not been used for learning the parameters ααα, W and H. The extension mech-
anism can be seen as in spirit of the Nystr ¨om extension (c.f. [16]). Introducing such a
generalization mechanism is worthwhile for two reasons: (i) To speed-up the computation
if the number n of objects under consideration is very large: By selecting a small subset
of m (cid:28) n objects for the initial ﬁt followed by the application of a computationally less
expensive prediction step, one can realize such a speed-up. (ii) The free parameters of the
approach, the number of clusters k as well as the sparseness control parameter η , can be
estimated using a re-sampling-based stability assessment that relies on the ability of an
algorithm to generalize to previously unseen objects.
Out-of-Sample Extension: Suppose we have to predict class memberships for r (= n −
P
m in the hold-out case) additional objects in the r × m matrix ˜Sl . Given the decomposition
between a new object o and object i as ˆpio := P
oi / P
into W and H, let zik be the “posterior” estimated for the
i-th object in the data set used for
the original ﬁt, i.e. ziν ∝ hiν
j wjν . We can express the weighted, normalized similarity
ˆzoν = X
l αl ˜s(l)
l,j αl ˜s(l)
oj . We approximate now
zoν for a new object o by
ziν ˆpio ,
i
which amounts to an interpolation of the zoν . These values can be obtained using the orig-
inally computed ziν which are weighted according to their similarity between object i and
o. In the analogy to the Nystr ¨om approximation, the (ziν ) play the role of basis elements
while the ˆpio amount to coefﬁcients in the basis approximation. The prediction procedure
requires O(mr(l + r + k)) steps.

(8)

Model Selection: The approach presented so far has two free parameters, the number
of classes k and the sparseness penalty η . In [9], a method for determining the number of
classes has been introduced, that assesses the variability of clustering solutions. Thus, we
focus on selecting η using stability. The assessment can be regarded as a generalization
of cross-validation, as it relies on the dissimilarity of solutions generated from multiple
sub-samples. In a second step, the solutions obtained from these samples are extended to
the complete data set by an appropriate predictor. Multiple classiﬁcations of the same data

(a)

(b)

(c)

Figure 1: Results on the toy data set (1(a)): The stability assessment (1(b)) suggests the
range η ∈ {101 , 102 , 5 · 102 }, which yield solutions matching the ground-truth. In 1(c), the
αl are depicted for a sub-sample and η in this range.

I{yi 6=π(y 0
i )}

set are obtained, whose similarity can be measured. For two clustering solutions Y , Y 0 ∈
{1, . . . , k}n , we deﬁne their disagreement as
nX
1
d(Y , Y 0 ) = min
π∈Sk
n
i=1
where Sk denotes the set of all permutation on sets of size k and IA is the indicator func-
tion on the expression A. The measure quantiﬁes the 0-1 loss after the labels have been
permuted, so that the two clustering solutions are in the best possible agreement. Perfect
agreement up to a permutation of the labels implies d(Y , Y 0 ) = 0. The optimal permuta-
tion can be determined in O(k3 ) by phrasing the problem as a weighted bipartite matching
problem. Following the approach in [9], we select the η , given a pre-speciﬁed range of
admissible values, such that the average disagreement observed on B sub-samples is min-
imal. In this sense, the entropy regularization mechanism guides the search for similarity
combinations leading to stable grouping solutions. Note that, multiple minima can occur
and may yield solutions emphasizing different aspects of the data.

(9)

5 Experimental Results and Discussion

The performance of our proposal is explored by analyzing toy and real world data. For
the model selection (sec. 4), we have used B = 20 sub-samples with the proposed out-of-
sample extension for prediction. For the stability assessment, different η have been chosen
by η ∈ {10−3 , 10−2 , 10−1 , .5, 1, 101 , 102 , 5 · 102 , 103 , 104 }. We compared our results with
NCut [15] and Lee and Seung’s two NMF algorithms [11] (which measure the approxi-
mation error of the factorization with (i) the KL divergence and (ii) the squared Frobenius
norm) applied to the uniform combination of similarities.

Toy Experiment: Figure 1(a) depicts a data set consisting of two nested rings, where
the clustering task consists of identifying each ring as a class. We used rbf-kernels
k(x, y) = exp(−kx − yk2 /2σ2 ) for σ varying in {10−4 , 10−3 , 10−2 , 100 , 101 } as well
as the path kernel introduced in [4]. All methods fail when used with the individual ker-
nels except for the path-kernel. The non-trivial problem is to detect the correct structure
despite the disturbing inﬂuence of 5 un-informative kernels. Data sets of size dn/5e have
been generated by sub-sampling. Figure 1(b) depicts the stability assessment, where we see
very small disagreements for η ∈ {101 , 102 , 5 · 102 }. At the minimum, the solution almost
perfectly matches the ground-truth (1 error). A plot of the resulting ααα-coefﬁcients is given
in ﬁgure 1(c). NCut as well as the other nmf-methods lead to an error rate of ≈ 0.5 when
applied to the uniformly combined similarities.

10−310−110010210300.050.10.150.20.250.30.350.40.450.5sparsity parameter havg. disagreement12345600.050.10.150.20.250.30.350.4data source indexprobability al(a)

(b)

Figure 2: Images for the segmentation experiments.

Image segmentation example:3 The next task consists of ﬁnding a reasonable segmen-
tation of the images depicted in ﬁgures 2(b) and 2(a). For both images, we measured local-
ized intensity histograms and additionally computed Gabor ﬁlter responses (e.g. [12]) on 3
scales for 4 different orientations. For each response image, the same histogramming pro-
cedure has been used. For all the histograms, we computed the pairwise Jensen-Shannon
divergence (e.g. [2]) for all pairs (i, j ) of image sites and took the element-wise expo-
nential of the negative Jensen-Shannon divergences. The resulting similarity matrices have
been used as input for the nmf-based data fusion. For the sub-sampling, m = 500 objects
have been employed. Figures 3(a) (for the shell image) and 3(b) (for the bird image) show
the stability curves for these examples which exhibit minima for non-trivial η resulting in
non-uniform ααα. Figure 3(c) depicts the resulting segmentation generated using ααα indicated
by the stability assessment, while 3(d) shows a segmentation result, where ααα is closer to the
uniform distribution but the stability score for the corresponding η is low. Again, we can see
that weighting the different similarity measurements has a beneﬁcial effect, since it leads
to improved results. The comparison with the NCut result on the uniformly weighted data
(ﬁg. 3(e)) conﬁrms that a non-trivial weighting is desirable here. Note that we have used the
full data set with NCut. For, the image in ﬁg. 2(b), we observe similar behavior: the stability
selected solution (ﬁg. 3(f)) is more meaningful than the NCut solution (ﬁg. 3(g)) obtained
on the uniformly weighted data. In this example, the intensity information dominates the
solution obtained on the uniformly combined similarities. However, the texture informa-
tion alone does not yield a sensible segmentation. Only the non-trivial combination, where
the inﬂuence of intensity information is decreased and that of the texture information is
increased, gives rise to the desired result. It is additionally noteworthy, that the prediction
mechanism employed works rather well: In both examples, it has been able to generalize
the segmentation from m = 500 to more than 3500 objects. However, artifacts resulting
from the subsampling-and-prediction procedure cannot always be avoided, as can be seen
in 3(f). They vanish, however, once the algorithm is re-applied to the full data (ﬁg. 3(h)).

Clustering of Protein Sequences: Our ﬁnal application is about the functional catego-
rization of yeast proteins. We partially adopted the data used in [7] 4 . Since several of the
3588 proteins belong to more than one category, we extracted a subset of 1579 proteins ex-
clusively belonging to one of the three categories cell cycle + DNA processing,transcription
and protein fate. This step ensures a clear ground-truth for comparison. Of the matrices used
in [7], we employed a Gauss Kernel derived from gene expression proﬁles, one derived
from Swiss-Waterman alignments, one obtained from comparisons of protein domains as
well as two diffusion kernels derived from protein-protein interaction data. Although the
data is not very discriminative for the 3-class problem, the solutions generated on the data
combined using the ααα for the most stable η lead to more than 10% improvement w.r.t. the

3Only comparisons with NCut reported. The nmf results are slightly worse than those of NCut.
4The data is available at http://noble.gs.washington.edu/proj/yeast/.

(a)

(b)

(c)

(f)

(d)

(g)

(e)

(h)

Figure 3: Stability plots and segmentation results for the images in 2(a) and 2(b) (see text).

ground-truth (the disagreement measure of section 4 is used) in comparison with the so-
lution obtained using the least stable η -parameter. The latter, however, was hardly better
than random guessing by having an overall disagreement of more than 0.60 (more pre-
cisely, 0.6392 ± 0.0455) on this data. For the most stable η , we observed a disagreement
around 0.52 depending on the sub-sample (best 0.5267 ± 0.0403). In this case, the largest
weight was assigned to the protein-protein interaction data. NCut and the two nmf meth-
ods proposed in [11] lead to rates 0.5953, 0.6080 and 0.6035, respectively, when applied
to the naive combination. Note, that the clustering results are comparable with some of
those obtained in [7], where the protein-protein interaction data has been used to construct
a (supervised) classiﬁer.

6 Conclusion

This work introduced an approach to combining similarity data originating from multiple
sources for grouping a set of objects. Adopting a pairwise clustering perspective enables
a smooth integration of multiple similarity measurements. To be able to distinguish be-
tween desired and distractive information, a weighting mechanism is introduced leading
to a potentially sparse convex combination of the measurements. Here, an entropy con-
straint is employed to control the amount of sparseness actually allowed. A stability-based
model selection mechanism is used to select this free parameter. We emphasize, that this
procedure represents a completely unsupervised model selection strategy. The experimen-
tal evaluation on toy and real world data demonstrates that our proposal yields meaningful
partitions and is able to distinguish between desired and spurious structure in data.
Future work will focus on (i) improving the optimization of the proposed model, (ii) the

10−310−1100102103−0.0500.050.10.150.20.250.3sparsity parameter havg. disagreement10−310−1100102103−0.0500.050.10.150.20.250.3sparsity parameter havg. disagreementintegration of additional constraints and (iii) the introduction of a cluster-speciﬁc weighting
mechanism. The proposed method as well as its relation to other approaches discussed in
the literature is currently under further investigation.

References

[1] F. R. Bach and M. I. Jordan. Learning spectral clustering. In NIPS, volume 16. MIT
Press, 2004.
[2] J. Burbea and C. R. Rao. On the convexity of some divergence measures based on
entropy functions. IEEE Trans. Inform. Theory, 28(3), 1982.
[3] K. Crammer, J. Keshet, and Y. Singer. Kernel design using boosting. In NIPS, vol-
ume 15. MIT Press, 2003.
[4] B. Fischer, V. Roth, and J. M. Buhmann. Clustering with the connectivity kernel. In
NIPS, volume 16. MIT Press, 2004.
[5] Thomas Hofmann. Unsupervised learning by probabilistic latent semantic analysis.
Mach. Learn., 42(1-2):177–196, 2001.
[6] E. T. Jaynes. Information theory and statistical mechanics, I and II. Physical Reviews,
106 and 108:620–630 and 171–190, 1957.
[7] G. R. G. Lanckriet, M. Deng, N. Cristianini, M. I. Jordan, and W. S. Noble. Kernel-
based data fusion and its application to protein function prediction in yeast. In Paciﬁc
Symposium on Biocomputing, pages 300 –311, 2004.
[8] Kenneth Lange. Optimization. Springer Texts in Statistics. Springer, 2004.
[9] T. Lange, M. Braun, V. Roth, and J.M. Buhmann. Stability-based model selection. In
NIPS, volume 15. MIT Press, 2003.
[10] M. H. C. Law, M. A. T. Figueiredo, and A. K. Jain. Simultaneous feature selec-
tion and clustering using mixture models. IEEE Trans. Pattern Anal. Mach. Intell.,
26(9):1154–1166, 2004.
[11] Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factor-
ization. In NIPS, volume 13, pages 556–562, 2000.
[12] B. S. Manjunath and W. Y. Ma. Texture features for browsing and retrieval of image
data. IEEE Trans. Pattern Anal. Mach. Intell., 18(8):837 –842, 1996.
[13] D. S. Modha and W. S. Spangler. Feature weighting in k-means clustering. Mach.
Learn., 52(3):217–237, 2003.
[14] V. Roth and T. Lange. Feature selection in clustering problems. In NIPS, volume 16.
MIT Press, 2004.
[15] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Trans.
Pattern Anal. Mach. Intell., 22(8):888 –905, 2000.
[16] C. K. I. Williams and M. Seeger. Using the Nystr¨ı¿ 1
2 m method to speed up kernel
machines. In NIPS, volume 13. MIT Press, 2001.
[17] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric learning with application
to clustering with side-information. In NIPS, volume 15, 2003.
[18] W. Xu, X. Liu, and Y. Gong. Document clustering based on non-negative matrix
factorization. In SIGIR ’03, pages 267–273. ACM Press, 2003.


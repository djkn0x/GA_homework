Query By Humming: Finding Songs in a Polyphonic Database

John Duchi
Computer Science Department
Stanford University
jduchi@stanford.edu

Benjamin Phipps
Computer Science Department
Stanford University
bphipps@stanford.edu

December 16, 2005

Abstract

Query by humming is the problem of retrieving mu-
sical performances from hummed or sung melodies.
This task is complicated by a wealth of factors, in-
cluding noisiness of input signals from a person hum-
ming or singing, variations in tempo between record-
ings of pieces and queries, and accompaniment noise in
the pieces we are seeking to match. Previous studies
have most often focused on the problems of retriev-
ing melodies represented symbolically (as in MIDI for-
mat), in monophonic (single voice or instrument) audio
recordings, or retrieving audio recordings from correct
MIDI or other symbolic input melodies. We take a step
toward developing a framework for query by humming
in which polyphonic audio recordings can be retrieved
by a user singing into a microphone.

1 Introduction

take the pitches and the durations that have been cal-
culated to ﬁnd the actual recording represented. There
has been research in this area before, but most has been
using music stored in MIDI or some other symbolic
formats [5] or in monophonic (single voice) recordings
[6]. In real polyphonic recordings, a number of factors
complicate queries–these include high tempo variabil-
ity, which depends on speciﬁc performances, and the
inconsistencies of the spectrum of sound due to factors
such as instrument timbre and vibrato.
To move beyond the above listed diﬃculties of mak-
ing queries over polyphonic recordings, we basae our
algorithms on a generative probabilistic model devel-
oped by Shalev-Shwartz et al. [2]. This builds on work
in dynamic Bayesian networks and HMMs [5] to cre-
ate a joint probability model over both temporal and
spectral probabilistic components of our polyphonic
recordings to give us a retrieval procedure for our sung
queries.

Suppose we hear a song on the radio but either do not
catch its title or simply cannot remember it. We ﬁnd
ourselves with songs stuck in our heads and no way to
ﬁnd the songs save visiting a music store and singing
to the music clerk, who can then (hopefully) direct us
to the pieces we want. Automating this process seems
a reasonable goal.
The ﬁrst task in such a system is to retrieve pitch
from a human humming or singing. There is a large lit-
erature on retrieving pitches from voice via a machine.
There are many algorithms to detect pitch; most rely
on a combination of diﬀerent calculations. Often, a
sliding window of 5 to 10 ms intervals is preprocessed
to gain initial estimates of pitch, then windowed auto-
correlation functions [4] or a power spectrum analysis
is done. After these steps, there is often interpolation
and postprocessing on the sound data to remove er-
rors such as octave oﬀ problems [1] to give a series of
frequencies and the times at which the frequencies are
estimated.
The second task in a query by humming system is to

2 Problem Representation

Though there are two parts to our problem–pitch ex-
traction and retrieving musical performances given a
melody–the latter necessitates the most detailed prob-
lem setting. Formally (using notation essentially iden-
tical to that of [2]), we are able to deﬁne the set of
possible pitches Γ (in Hz), in well-tempered Western
music tuning, as Γ = {440 · 2s/12 |s ∈ Z}. Thus, a
melody is a sequence of pitches p ∈ Γk , where k is the
length of the melody (in notes).
For our purposes, the real performance of a melody
is a discrete time sampled audio signal, o = o1 , . . . , oT ,
where ot is the spectrum of one of our performances
at the tth discrete sample. These performances are
those drawn from our database of pieces that we query.
Because we assume short-time invariance of our input
sounds, we set the samples to be of length .04 seconds.
To completely deﬁne a melody, we have a series of
k pitches pi and durations di , where the melody is to
play p1 for d1 seconds and so on. Performances of

1

pieces, however, rarely use the same tempo, and thus a
melody can have much more variability than the model
given. As such, we deﬁne a sequence of scaling factors
for the tempo of our queries, m ∈ (R+ )k , the set of
sequences of k positive real numbers (in our testing,
each mi is drawn from a set M of all the possible scaling
factors). Thus, the actual duration of pi is dimi , which
we must take into account when matching queries to
audio signals.
Now we have our problem deﬁned: given a melody
hp, di, we would like to ﬁnd the likelihood of some
performance, that is, we would like to maximize o in
our generative model P (o|p, d).

3 Extracting Pitch

Having deﬁned our problem, we see that the ﬁrst step
must be to extract pitches and durations from a sung
query. Saul et. al., have described an algorithm that
does not rely on power spectrum analysis or long au-
tocorrelations to ﬁnd pitches in voice [1]. Their algo-
rithm (which is called Adaptive Least Squares - ALS)
uses least squares approximations to ﬁnd the opti-
mal frequency values of a signal. A method known
as Prony’s method [3] uses only one-sample lagged
and zero-sample lagged autocorrelation as well as least
squares, which reduces errors in resolution sometimes
found by FFTs as a result of low sampling rates, and
we can extract pitches in time linear in the number of
samples we have.

3.1 Finding the Sinusoid in Voiced
Speech
Any sinusoid that we sample at discrete time points n
has the following form and identity:
(cid:20) sn−1 + sn+1
(cid:21)
sn = A sin(ωn + φ)
1
sn =
2
cos ω
(cid:19)(cid:21)2
(cid:18) xn−1 + xn+1
(cid:20)
E (α) = X
This allows, as in [1], us to deﬁne an error function
xn − α
2
n
If our signal is well described by a sinusoid, then when
α = (cos ω)−1 , the error should be small. The solution
2 P
to our least squares is given by
P
n xn (xn−1 + xn+1 )
n (xn−1 + xn+1 )2
Thus, we minimize our signal’s error function and then
check that our signal is sinusoidal rather than expo-
nential and not zero. Then our estimated frequency

α∗ =

.

.

is

ω∗ = cos−1 (1/α∗ )

3.2 Detecting Pitch in Speech
In our implementation, we followed Saul et al.’s ap-
proach of running our sung query signals through a low
pass ﬁlter to remove high frequency noise, using half-
wave rectiﬁcation to remove negative energy and con-
centrate it at the fundamental, then separating our sig-
nals into a series of eight bands using bandpass 8th or-
der Chebyshev ﬁlters. We can then use Prony’s method
for our sinusoid detection, which has proven accurate
in previous tests [1]. Saul et al. also deﬁne cost func-
tions that allow us to determine whether sounds are
voiced or not and whether the least squares method
has provided an accurate enough ﬁt to a sinusoid (see
[1] for more details).

(a) Waveform of Scale

(b) Frequencies of Sung Scale

Figure 1: Raw Data and Frequencies

Figure 2: Pitches of the Sung Scale

3.3 Transforming to Melody
Using the above method, we retrieve a frequency at
every .04 second interval, which we downsample from
22050 Hz to 918 Hz, which allows for quicker compu-
tations. Given our set of frequencies {f0 , . . . , fn } over

2

020004000600080001000012000!1!0.500.5102000400060008000100001200005010015020025005001000150020002500300082848688909294our n samples, we assign each fi to its corresponding
MIDI pitch pi ∈ [0, 127], then use mean smoothing to
achieve better pitch estimates for every pi . We group
consecutive identical pitches from the samples to give
us our melody hp, di = h(p1 , d1 ), . . . , (pk , dk )i. Lastly,
we compress this melody to be in a 12 note (one octave)
range, because it helps our computational complexity
in the alignment part of our algorithm to have fewer
possible pitches, and spectra alignments are not overly
sensitive to octave-oﬀ errors. To see examples of fre-
quencies and pitches extracted from singing, see ﬁgures
1 and 2.

4 A Generative Model
Melodies to Signals

from

As mentioned in section 2, we have a generative model
that we are trying to maximize: P (o|p, d). More con-
cretely, given a melody query hp, di, we would like to
ﬁnd the acoustic performance o that hp, di is most
likely to have generated.

4.1 Probabilistic Time Scaling

P (m)P (o|p, d, m).

As in [2], we treat the tempo sequence as independent
of the melody (which ought to hold for short pieces), to
give us problem of ﬁnding the o in our database that
P (o|p, d) = X
maximizes the following:
m
In this, having the m parameter in the conditional sim-
ply means that we are scaling the sequence of durations
d by m. m is modeled as a ﬁrst order Markov process,
kY
so
i=2

P (mi |mi−1 ).

P (m) = P (m1 )

Because the log-normal distribution has the nice trait
that it somewhat accurately reﬂects a person’s ten-
dency to speed up (rather than slow down) when doing
a musical query or performance, we say that

P (mi |mi−1 ) =

1√
2πρ

− 1
2ρ2 (log mi
mi−1
e

)2

.

We also assume a log-normal distribution of P (m1 ), so
log2 (m1 ) ∼ N (0, ρ). In these equations, the ρ parame-
ter describes how sensitive our model is to local tempo
changes–high ρ (ρ > 10) means that our model is not
very sensitive to tempo changes, low ρ (ρ < 1) give us
a model very sensitive to tempo changes.

4.2 Modeling Spectral Distribution
We let ¯oi represent a sequence of samples (that we
suppose is generated by note and duration (pi , di ) in
our query) from a piece in our database. That is,
¯oi = ot0+1 , . . . ot , where pi ends at time sample t and
t0 = t − di . We use a harmonic model of P (¯oi ) almost
identical to that in [2].
F (¯oi ) is the observed energy of some block of sam-
ples ¯oi over the entire spectra (we get this from the
Fourier transform). Also, we assume that we have a
soloist in all of our recordings, and that S (ω , ¯oi ) is the
energy of the soloist at frequency ω for our samples,
and our model assumes that S is simply bursts of en-
ergy centered at the harmonics of some pitch pi . This
is a reasonable assumption for our soloists energy, be-
cause often the harmonics of the accompaniment will
roughly follow the soloist. That is, we have a burst
at pih for h ∈ {1, 2, . . . , H }, and we set H to be 20
to keep the number of harmonics reasonable. We can
deﬁne the noise of a signal at some frequency ω to be
the energy that is not in the soloist or any of his or
her harmonics (frequencies that are multiples of ω), or
N (ω , ¯oi ) = F (¯oi ) − S (ω , ¯oi ). This gives us that
||S (ω , ¯oi )||2
log P (¯oi |pi , di ) ∝ log
||N (ω , ¯oi )||2
where || · || is the l2 -norm (see [2] for this derivation).
We assume that di is implicit in conditional probabil-
ities when given pi from here on, because they pitches
and durations in our queries come in pairs.
To actually get the energy of the soloist and the
noise, assuming the soloist is performing at a frequency
ω , we use a method called subharmonic summation
proposed by Hermes [7]. This method allows us to
determine if a pitch is predominant in a spectrum by
adding all the amplitudes of its harmonics to the funda-
HX
mental frequency. The formula we apply is as follows:
h=1
where d is a contraction rate that usually is set to make
it so that lower frequencies are more important (we
set d = 1 so we can simply remove all energy at the
frequencies we assume are the soloist’s). Thus, when
we are performing a query of a piece and we would
like ﬁnd the probability of a block of signals ¯oi given
the current pitch in our query, we simply remove all
the peak frequencies at multiples of our query’s pitch’s
frequency, then ﬁnd the remaining signals and treat
them as noise (see ﬁgure 3). This gives us P (¯oi |pi ).

dhF (hω)

S (ω) =

4.3 Matching Algorithm
With the background we have now put in place, we see
we can develop a dynamic programming algorithm as in

3

path algorithms for HMMs) that we see in ﬁgure 4, due
to [2] with some modiﬁcations.

1. Initialization
∀t, 1 ≤ t ≤ T , γ (0, t, 1) = 1

2. Inductive Building of γ
for i := 1 to k , t := 0 to T , ξ := min ξ to max ξ

γ (i, t, ξ ) =
γ (i − 1, t0 , ξ 0 )P (ξ |ξ 0 ) · P (ot0+1 , . . . , ot |pi )
max
ξ 0∈M
where t0 = t − (di + ξ ).
3. Termination
P ∗ = max
1≤t≤T ,ξ∈M

γ (k , t, ξ )

Figure 4: The alignment algorithm we use

4.4 Complexity of Matching a Query
The complexity of this algorithm, which is relatively
easy to see from the for loop nesting, is O(kT |M |2 ),
where k is the number of notes in our query, T is the
number of time samples in the polyphonic piece we
query, and M is the set of possible tempo scaling val-
ues. This holds as long as P (ot0+1 , . . . , ot |pi ) can be
computed in constant time, which we guarantee in our
implementation.
To achieve constant time probability lookups, we
pre-compute all the probability values of sample blocks
¯oi using fast Fourier transforms with 215 points for
good resolution. We compute the probability P (¯o|p)
for each pitch p that we can see in our queries for
all the possible lengths of samples in our audio signal
o. We compute probability for every block of sam-
ples ot . . . , ot0 of length .04 to 2.5 seconds, because
singers cannot change pitch in under .04s, and in most
music, especially the music we use, pitches are rarely
held for longer than two seconds. Eﬀectively, this gives
us O(T · 62) probabilities for each pitch, of which we
have 12. This pre-computation, while expensive, sig-
niﬁcantly helps running times, because we do not have
to do spectral analyses every time we wish to calculate
P (¯o|p) in our algorithm.

5 Experimental Results

We ran tests on ﬁve diﬀerent Beatles songs–Hey Jude,
Let It Be, Yesterday, It’s Only Love, and Ticket to
Ride. The system, given a melody represented by

Figure 3: Solo vs. Noise. Stars are solo frequency, the
rest is noise

[2] to retrieve our polyphonic piece given some k-length
query of pitch-duration pairs h(p1 , d1 ), . . . , (pk , dk )i.
More speciﬁcally, in our implementation, for a given
polyphonic piece, we have a spectrum sample every .04
seconds, and there are T samples for the entire length
of the piece. Recall that P (¯oi |pi ) = P (ot0+1 , . . . , ot |pi )
for appropriate t, t0 . We also have the tempo scaling
factors to account for, that is, P (mi |mi−1 ) from above.
In the algorithm, we call these tempo scaling factors ξ
(to vary tempo for our algorithm, each scaling factor
is simply a diﬀerent small multiple of .04 that is added
or subtracted from di to give us a diﬀerent duration).
There is also a chance that there are rests in the pieces
we consider, and we must take into account rests in our
queries. As such, if we have that pi = 0, we replace
P (¯oi |pi ) with the spectrum probability of a rest
P (Rest|pi = 0) =
1
(2 − P (ot0+1 , . . . , ot |pi−1 )
2
−P (ot0+1 , . . . , ot |pi+1 ))
In our model, this says that if we have a rest in our
query, then the pitches before and after pitch pi in our
query ought not be very present in the spectrum.
Putting all of this together, we deﬁne γ (i, t, ξ ) to be
the joint likelihood of ot and mi , or as the maximum
(over the set M of all the scaling factors) probability
that the ith note of our query ends at sample index t,
and its duration is scaled by ξ .
P (ot , mi |p, d)

γ (i, t, ξ ) = max
mi∈M i

While this is the joint likelihood of our polyphonic
piece’s ﬁrst t samples and its ﬁrst i scaling factors given
p and d, and ideally we would have just the likeli-
hood of the samples o = o1 , . . . , oT , we still use γ as
the retrieval score given our query. All this gives us
the alignment algorithm (reminiscent of most probable

4

0501001502002503003504004505000102030405060!(Hz)F(!)pitch-duration pairs, retrieved the song whose align-
ment score was the highest. As a ﬁrst test, the system
was given correct symbolic representations of parts of
all ﬁve songs, copied directly from scores.
In this,
the retrieval was perfect, as expected for our small
database.

5.1 Sung Queries in Key
Our system’s retrieval rates on the ﬁve songs, given
queries that were sung in the song’s keys, were again
perfect. The average ratio
Highest retrieval score for query hp, di
Second highest retrieval score for query hp, di
was .23 for queries sung in the correct key. This ac-
curacy is fairly good, though it is orders of magnitude
worse than the accuracy achieved with correct sym-
bolic queries. Thus, as long as the system did not have
to do any transposition, the querying worked for our
small database.

is bothersome and will be a sub ject of future work. We
also would like to expand the system to handle incor-
rect accidental modulations in singing, to give it a dis-
tribution over incorrect pitches. In the inductive part
of the algorithm, instead of taking P (¯o|p), we could
deﬁne a distribution over the probability that the user
meant to sing note p in his query. For example, we
might look at p − 1, p, p + 1 and take the maximum
2 P (¯o|p − 1), P (¯o|p), 1
2 P (¯o|p + 1)}, which would al-
of { 1
low the singer to miss some pitches by a semitone but
would increase the time complexity of our algorithm by
a factor of the number of pitches over which we take a
distribution to allow incorrect singing.
The system’s speed is also relatively low; to build a
large database the alignment procedure would need a
signiﬁcant speedup. It may be useful to look into learn-
ing to automatically extract themes from polyphonic
music, then performing queries over those themes. In
spite of the diﬃculties inherent in this problem, we
have demonstrated that a query by humming system
searching polyphonic audio tracks is feasible.

5.2 Transposition
To test our system’s resilience to transposition (shift-
ing an entire melody but keeping its relative pitches
constant), we had the alignment procedure attempt to
align the melody we gave it, as well as the 11 other
melodies that were transpositions (the ith transposition
simply shifts all the pitches of p up i) of the original
melody. The piece retrieved was the one which had the
maximum alignment score on any one transposition of
the melody.
As before, when given correct symbolic representa-
tions (transposed scores), the retrieval procedure was
ﬂawless, always returning correct results.
When the queries, however, were sung but not in the
key in which the Beatles sang (for example, Yesterday
sung in E instead of F, a half step down), results were
not as optimal. Hey Jude and It’s Only Love the sys-
tem still identiﬁed correctly, but the other three songs
had signiﬁcantly worse results, sometimes being given
lower alignment scores on a melody than as many as
three other songs. The reasons for this are not totally
clear, but we speculate that transpositions of a query
may put it into the key of a diﬀerent song from our
database, which would make it easier to for a query to
match the spectra of an incorrect song.

6 Conclusions and Future

We have taken a step toward building a polyphonic
music database that can be queried by singing. While
we met with success as long as queries were in the cor-
rect key, the system’s inability to handle transposition

References

[1] Saul, L., Lee, D., Isbell, C., and LeCun, E. “Real
Time Voice Processing with Audiovisual Feed-
back: Toward Autonomous Agents with Perfect
Pitch,” Advances in Neural Information Process-
ing Systems 15, pp. 1205-1212, MIT Press: Cam-
bridge, MA, 2003.

[2] Shalev-Shwartz, S., Dubnov, S., Friedman, N.,
and Singer, Y. “Robust Temporal and Spectral
Modeling for Query by Melody,” SIGIR02, pp.
331-338, ACM Press: New York, NY, 2002.

[3] Proakis, J., Rader, C., Ling, F., Nikias, C., Moo-
nen, M., and Proudler, I. Algorithms for Statis-
tical Signal Processing, Prentice Hall, 2002.

[4] Rabiner, L. “On the Use of Autocorrelation
Analysis for Pitch Determination,” IEEE Trans-
actions on Acoustics, Speech, and Signal Process-
ing, 25, pp. 22-33, 1977.

[5] Meek, C. and Birmingham, W. “Johnny Can’t
Sing: A Comprehensive Error Model for Sung
Music Queries,” in Proc. ISMIR, 2002.

[6] Durey, A. and Clements, M. “Melody Spotting
Using Hidden Markov Models,” in Proc. ISMIR,
2001.

[7] Hermes, D. “Measurement of Pitch by Subhar-
monic Summation,” Journal of Acoustical Soci-
ety of America, 83(1), pp. 257-263, 1988.

5


An Approximate Inference Approach for the
PCA Reconstruction Error

Manfred Opper
Electronics and Computer Science
University of Southampton
Southampton, SO17 1BJ
mo@ecs.soton.ac.uk

Abstract

The problem of computing a resample estimate for the reconstruction
error in PCA is reformulated as an inference problem with the help of
the replica method. Using the expectation consistent (EC) approxima-
tion, the intractable inference problem can be solved efﬁciently using
only two variational parameters. A perturbative correction to the result
is computed and an alternative simpliﬁed derivation is also presented.

1 Introduction

This paper was motivated by recent joint work with Ole Winther on approximate inference
techniques (the expectation consistent (EC) approximation [1] related to Tom Minka’s EP
[2] approach) which allows us to tackle high–dimensional sums and integrals required for
Bayesian probabilistic inference.
I was looking for a nice model on which I could test this approximation. It had to be simple
enough so that I would not be bogged down by large numerical simulations. But it had
to be nontrivial enough to be of at least modest interest to Machine Learning. With the
somewhat unorthodox application of approximate inference to resampling in PCA I hope
to be able to stress the following points:
• Approximate efﬁcient inference techniques can be useful in areas of Machine
Learning where one would not necessarily assume that they are applicable. This
can happen when the underlying probabilistic model is not immediately visible
but shows only up as a result a of mathematical transformation.
• Approximate inference methods can be highly robust allowing for analytic contin-
uations of model parameters to the complex plane or even noninteger dimensions.
• It is not always necessary to use a large number of variational parameters in order
to get reasonable accuracy.
• Inference methods could be systematically improved using perturbative correc-
tions.

The work was also stimulated by previous joint work with D ¨orthe Malzahn [3] on resam-
pling estimates for generalization errors of Gaussian process models and Supportvector–
Machines.

2 Resampling estimators for PCA

Principal Component Analysis (PCA) is a well known and widely applied tool for data
analysis. The goal is to project data vectors y from a typically high (d-) dimensional
space into an optimally chosen lower (q -) dimensional linear space with q << d, thereby
minimizing the expected projection error ε = E ||y − Pq [y]||2 , where Pq [y] denotes the
projection. E stands for an expectation over the distribution of the data. In practice where
the distribution is not available, one has to work with a data sample D0 consisting of N
vectors yk = (yk (1), yk (2), . . . , yk (d))T , k = 1, . . . , N . We arrange these vectors into a
(d × N ) data matrix Y = (y1 , y2 , . . . , yN ). Assuming centered data, the optimal subspace
is spanned by the eigenvectors ul of the d × d data covariance matrix C = 1
N YYT
corresponding to the q largest eigenvalues λk . We will assume that these correspond to all
eigenvectors λk > λ above some threshold value λ.
After computing the PCA projection, one would be interested in ﬁnding out if the computed
subspace represents the data well by estimating the average projection error on novel data
y (ie not contained in D0 ) which are drawn from the same distribution.
E = X
(cid:3)
E Tr (cid:2)yyT uluT
Fixing the projection Pq , the error can be rewritten as
l
Et = P
λl<λ
where the expectation is only over y and the training data are ﬁxed. The training error
l can be obtained without knowledge of the distribution but will usually
λl<λ λ2
only give an optimistically biased estimate for E .

(1)

2.1 A resampling estimate for the error

New artiﬁcial data samples D of arbitrary size can be created by resampling a number of
data points from D0 with or without replacement. A simple choice would be to choose
all data independently with the same probability 1/N , but other possibilities can also be
implemented within our formalism. Thus, some yi in D0 may appear multiple times in D
and others not at all. The idea of performing PCA on resampled data sets D and testing
on the remaining data D0 \D , motivates the following deﬁnition of a resample averaged
 X
(cid:1)
Tr (cid:0)yiyT
reconstruction error
1
i uluT
l
N0
yi /∈D;λl<λ
as a proxy for E . ED is the expectation over the resampling process. This is an estima-
tor of the bootstrap type [3,4]. N0 is the expected number of data in D0 which are not
contained in the random set D . The rest of the paper will discuss a method for efﬁciently
approximating (2).

Er =

ED

(2)

2.2 Basic formalism

We introduce “occupation numbers” si which count how many times yi is containd in D .
We also introduce two matrices D and C. D is a diagonal random matrix
Γ
1
Dii = Di =
µΓ
i.e. µN = ED [P
N
C(0) is proportional to the covariance matrix of the resampled data. µ is the sampling rate,
i si ] is the expexted number of data in D (counting multiplicities). The

(si + δsi ,0 )

YDYT .

C() =

(3)

role of Γ will be explained later. Using , we can generate expressions that can be used in
X
(2) to sum over the data which are not contained in the set D
1
C0 (0) =
µN
j

δsj ,0yj yT
j .

(4)

lim
η→0+

Hence, we have

k δ (λk + Γ) .
uk uT

In the following λk and uk will always denote eigenvalues and eigenvectors of the data
dependent (i.e. random) covariance matrix C(0).
The desired averages can be constructed from the d × d matrix Green’s function
G(Γ) = (C(0) + ΓI)−1 = X
uk uT
k
λk + Γ
k
√−1 and = denotes the imaginary part, we get
Using the well known representation of the Dirac δ distribution given by δ(x) =
limη→0+ = 1
= G(Γ − iη) = X
π(x−iη) where i =
1
π
Z λ
k
Er = E 0
dλ0 εr (λ0 )
r +
X
j G(−λ − iη)(cid:1)
0+
δsj ,0 Tr (cid:0)yj yT
= 1
1
lim
ED
η→0+
N0
π
j
deﬁnes the error density from all eigenvalues > 0 and E 0
r is the contribution from the
X
j ΓG(Γ)(cid:1)
eigenspace with λk = 0. The latter can also be easily expressed from G as
δsj ,0 Tr (cid:0)yj yT
j
We can also compute the resample averaged density of eigenvalues using
1
= ED [Tr G(−λ − iη)]
πµN

E 0
r = lim
Γ→0

εr (λ) =

lim
η→0+

ρ(λ) =

where

1
N0

ED

(10)

(9)

(5)

(6)

(7)

(8)

3 A Gaussian probabilistic model

The matrix Green’s function for Γ > 0 can be generated from a Gaussian partition function
Z . This is a well known construction in statistical physics, and has also been used within
the NIPS community to study the distribution of eigenvalues for an average case analysis
of PCA [5]. Its use for computing the expected reconstruction error is to my knowledge
new.
(cid:20)
(cid:21)
Z
With the (N × N ) kernel matrix K = 1
xT (cid:0)K−1 + D(cid:1) x
N YT Y we deﬁne the Gaussian partition function
(cid:21)
(cid:20)
− 1
Z
2
− 1
= |K| 1
2 Γd/2 (2π)(N −d)/2
ddz exp
2

zT (C() + ΓI) z

dx exp

Z =

(11)

(12)

.

x is an N dimensional integration variable. The equality can be easily shown by expressing
the integrals as determinants. 1 The ﬁrst representation (11) is useful for computing the
resampling average and the second one connects directly to the deﬁnition of the matrix
Green’s function G. Note, that by its dependence on the kernel matrix K, a generalization
to d = ∞ dimensional feature spaces and kernel PCA is straightforward. The partition
function can then be understood as a certain Gaussian process expectation. We will not
discuss this point further.
The free energy F = − ln Z enables us to generate the following quantities
NX
−2 ∂ ln Z
1
µN
∂ 
=0
j=1
−2 ∂ ln Z
= d
∂Γ
Γ
where we have used (4) for (13). (13) will be used for the computation of (8) and (14)
applies to the density of eigenvalues. Note that the deﬁnition of the partition function Z
requires that Γ > 0, whereas the application to the reconstruction error (7) needs negative
values Γ = −λ < 0. Hence, an analytic continuation of end results must be performed.

δsj ,0 Tr yj yT
j G(Γ)

+ Tr G(Γ)

(14)

(13)

=

4 Resampling average and replicas

(13) and (14) show that we can compute the desired resampling averages from the expected
free energy −ED [ln Z ]. This can be expressed using the “replica trick” of statistical physics
(see e.g. [6]) using
1
ln ED [Z n ] ,
ED [ln Z ] = lim
(15)
n→0
n
where one attempts an approximate computation of ED [Z n ] for integer n and uses a con-
Z
tinuation to real numbers at the end. The n times replicated and averaged partition function
(11) can be written in the form
Z (n) .= ED [Z n ] =
)#
"
(
nX
nX
.= (x1 , . . . , xn ) and
− 1
2
a=1
a=1
The unaveraged partition function Z (11) is Gaussian, but the averaged Z (n) is not and
usually intractable.

dx ψ1 (x) ψ2 (x)
"

a K−1xa
xT

where we set x

ψ2 (x) = exp

ψ1 (x) = ED

xT
a Dxa

− 1
2

#

exp

(16)

(17)

5 Approximate inference

p1 (x) =

To approximate Z (n) , we will use the EC approximation recently introduced by Opper &
Winther [1]. For this method we need two auxiliary distributions
1
1
e− 1
ψ1 (x)e−Λ1 xT x
2 Λ0 xT x ,
Z1
Z0
where Λ1 and Λ0 are “variational” parameters to be optimized. p1 tries to mimic the in-
tractable p(x) ∝ ψ1 (x) ψ2 (x), replacing the multivariate Gaussian ψ2 by a simpler, i.e.
1 If K has zero eigenvalues, a division of Z by |K| 1
2 is necessary. This additive renormalization
of the free energy − ln Z will not inﬂuence the subsequent computations.

p0 (x) =

(18)

tractable diagonal one. One may think of using a general diagonal matrix Λ1 , but we will
restrict ourselves in the present case to the simplest case of a spherical Gaussian with a
single parameter Λ1 .
Z
The strategy is to split Z (n) into a product of Z1 and a term that has to be further approxi-
mated:
Z

dx p1 (x) ψ2 (x) eΛ1 xT x
dx p0 (x) ψ2 (x) eΛ1 xT x ≡ Z (n)
EC (Λ1 , Λ0 ) .

Z (n) = Z1
≈ Z1

(19)

The approximation replaces the intractable average over p1 by a tractable one over p0 . To
optmize Λ1 and Λ0 we argue as follows: We try to make p0 as close as possible to p1 by
matching the moments hxT xi1 = hxT xi0 . The index denotes the distribution which is used
for averaging. By this step, Λ0 becomes a function of Λ1 . Second, since the true partition
function Z (n) is independent of Λ1 , we expect that a good approximation to Z (n) should
be stationary with respect to variations of Λ1 . Both conditions can be expressed by the
requirement that ln Z (n)
EC (Λ1 , Λ0 ) must be stationary with respect to variations of Λ1 and
Λ0 .
Within this EC approximation we can carry out the replica limit ED [ln Z ] ≈ ln ZEC =
(cid:20)
(cid:21)
Z
n ln Z (n)
limn→0
EC and get after some calculations
1
Z
Z
− ln ZEC = −ED
dx e− 1
2 xT (D+(Λ0−Λ)I)x
ln
− ln
dx e− 1
dx e− 1
2 xT (K−1+ΛI)x + ln
2 Λ0 xT x
where we have set Λ = Λ0 − Λ1 . Since the ﬁrst Gaussian integral factorises, we can
now perform the resampling average in (20) relatively easy for the case when all sj ’s in
(3) are independent. Assuming e.g. Poisson probabilities p(s) = e−µ µs
s! gives a good
approximation for the case of resampling µN points with replacement.
(cid:18)
(cid:19)
X
The variational equations which make (20) stationary are
1
1
1
Λ0 − Λ + Di
Λ0
N
k
where ωk are the eigenvalues of the matrix K. The variational equations have to be solved
in the region Γ = −λ < 0 where the original partition function does not exist. The resulting
parameters Λ0 and Λ will usually come out as complex numbers.

ωi
1 + ωkΛ

1
Λ0

(20)

=

(21)

−

ED

=

6 Experiments

By eliminating the parameter Λ0 from (21) it is possible to reduce the numerical com-
putations to solving a nonlinear equation for a single complex parameter Λ which can be
solved easily and fast by a Newton method. While the analytical results are based on Pois-
son statistics, the simulations of random resampling was performed by choosing a ﬁxed
number (equal to the expected number of the Poisson distribution) of data at random with
replacement.
The ﬁrst experiment was for a set of data generated at random from a spherical Gaussian.
To show that resampling maybe useful, we give on on the left hand side of Figure 1 the
reconstruction error as a function of the value of λ below which eigenvalues are dicarded.

Figure 1: Left: Errors for PCA on N = 32 spherically Gaussian data with d = 25 and µ =
3. Smooth curve: approximate resampled error estimate, upper step function: true error.
Lower step function: Training error. Right: Comparison of EC approximation (line) and
simulation (histogramme) of the resampled density of eigenvalues for N = 50 spherically
Gaussian data of dimensionality d = 25. The sampling rate was µ = 3.

The smooth function is the approximate resampling error (3× oversampled to leave not
many data out of the samples) from our method. The upper step function gives the true
reconstruction error (easy to calculate for spherical data) from (1). The lower step function
is the training error. The right panel demonstrates the accuracy of the approximation on
a similar set of data. We compare the analytically approximated density of states with the
results of a true resampling experiment, where eigenvalues for many samples are counted
into small bins. The theoretical curve follows closely the experiment.
Since the good accuracy might be attributed to the high symmetry of the toy data, we have
also performed experiments on a set of N = 100 handwritten digits with d = 784. The
results in Figure 2 are promising. Although the density of eigenvalues is more accurate
than the resampling error, the latter comes still out reasonable.

7 Corrections
I will show next that the EC approximation can be augmented by a perturbation expansion.
(cid:27)
(cid:26)Z
Z
Z
Going back to (19), we can write
Z (n)
(2π)N n e−ikT xχ(k)
dk
where χ(k) .= R dx p1 (x)eikT x is the characteristic function of the density p1 (18). ln χ(k)
1
2 ΛxT x
dx p1 (x) ψ2 (x) eΛ1 xT x =
dx ψ2 (x) e
=
Z1
is the cumulant generating function. Using the symmetries of the density p1 , we can per-
form a power series expansion of ln χ(k), which starts with a quadratic term (second cu-
mulant)

ln χ(k) = − M2
2 kT k + R(k) ,
(22)
where M2 = hxT
a xa i1 . It can be shown that if we neglect R(k) (containing the higher order
cumulants) and carry out the integral over k , we end up replacing p1 by a simpler Gaussian
p0 with matching moments M2 , i.e. the EC approximation. Higher order corrections to
the free energy −ED [ln Z ] = − ln ZEC + ∆F1 + . . . can be obtained perturbatively by
writing χ(k) = e− M2
2 kT k (1 + R(k) + . . .). This expansion is similar in spirit to Edgeworth

0123450510152025eigenvalue lFigure 2: Left: Resampling error (µ = 1) for PCA on a set of 100 handwritten digits (“5”)
with d = 784. The approximation (line) for µ = 1 is compared with simulations of the
random resampling. Right: Resampled density of eigenvalues for the same data set. Only
the nonzero eigenvalues are shown.

expansions in statistics. The present case is more complicated by the extra dimensions
introduced by the replicating of variables and the limit n → 0. After a lengthy calculation
one ﬁnds for the lowest order correction (containing the monomials in k of order 4) to the
(cid:19)2 × X
(cid:18)
(cid:16)
(cid:17)2
(cid:0)K−1 + ΛI(cid:1)−1
free energy:
∆F1 = − 1
Λ0
ii − 1
Λ0 − Λ + Di
4 ED
i
I illustrate the effect of ∆F1 on a correction to the reconstruction error in the “zero–
subspace” using (9) and (13) for the digit data as a function of µ. Resampling used the
Poisson approximation.The left panel of Figure 3 demonstrates that the true correction is
fairly small. The right panel shows that the lowest order term ∆F1 accounts for a major
part of the true correction when µ < 3. The strong underestimation for larger µ needs
further investigation.

− 1

(23)

Λ0

8 The calculation without replicas

− ln Z = − ln

Knowing with hindsight how the ﬁnal EC result (20) looks like, we can rederive it using
another method which does not rely on the “replica trick”. We ﬁrst write down an exact
expression for − ln Z before averaging. Expressing Gaussian integrals by determinants
Z
Z
yields
Z

2 xT (D+(Λ0−Λ)I)x − ln
dx e− 1
dx e− 1
2 xT (K−1+ΛI)x + (24)
1
dx e− 1
(cid:16)
(cid:17)
(cid:17) (cid:16)
2 Λ0 xT x +
(cid:0)K−1 + ΛI(cid:1)−1 − I
ln det(I + r)
2
1 −
where the matrix r has elements rij =
Λ0
. The
Λ0
Λ0−Λ+Di
ij
EC approximation is obtained by simply neglecting r. Corrections to this are found by
∞X
Tr (cid:0)rk (cid:1)
expanding
(−1)k+1
k
k=1

ln det (I + r) = Tr ln (I + r) =

+ ln

(25)

00.511.505101520eigenvalue lFigure 3: Left: Resampling error E 0
r from the λ = 0 subspace as a function of resampling
rate for the digits data. The approximation (lower line) is compared with simulations of
the random resampling (upper line). Right: The difference between approximation and
simulations (upper curve) and its estimate (lower curve) from the perturbative correction
(23).

The ﬁrst order term in the expansion (25) vanishes after averaging (see (21)) and the second
order term gives exactly the correction of the cumulant method (23).

9 Outlook

It will be interesting to extend the perturbative framework for the computation of correc-
tions to inference approximations to other, more complex models. However, our results
indicate that the use and convergence of such perturbation expansion needs to be critically
investigated and that the lowest order may not always give a clear indication of the accu-
racy of the approximation. The alternative derivation for our simple model could present
an interesting ground for testing these ideas.

Acknowledgments

I would like to thank Ole Winther for the great collaboration on the EC approximation.

References

[1] Manfred Opper and Ole Winther. Expectation consistent free energies for approximate inference.
In NIPS 17, 2005.
[2] T. P. Minka. Expectation propagation for approximate Bayesian inference. In UAI 2001, pages
362–369, 2001.
[3] D. Malzahn and M. Opper. An approximate analytical approach to resampling averages. Journal
of Machine Learning Research, pages 1151–1173, 2003.
[4] B. Efron, R. J. Tibshirani. An Introduction to the Bootstrap. Monographs on Statistics and Applied
Probability 57, Chapman & Hall, 1993.
[5] D. C. Hoyle and M. Rattray Limiting form of the sample covariance matrix eigenspectrum in
PCA and kernel PCA. In NIPS 16, 2003.
[6] A. Engel and C. Van den Broeck, Statistical Mechanics of Learning (Cambridge University Press,
2001).

00.511.522.533.54246810121416182022Resampling rate mResampled reconstruction error (l = 0)00.511.522.533.5400.20.40.6Correction to resampling errorResampling rate m
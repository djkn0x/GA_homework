Fast Krylov Methods for N-Body Learning

Nando de Freitas
Department of Computer Science
University of British Columbia
nando@cs.ubc.ca

Yang Wang
School of Computing Science
Simon Fraser University
ywang12@cs.sfu.ca

Maryam Mahdaviani
Department of Computer Science
University of British Columbia
maryam@cs.ubc.ca

Dustin Lang
Department of Computer Science
University of Toronto
dalang@cs.ubc.ca

Abstract

This paper addresses the issue of numerical computation in machine
learning domains based on similarity metrics, such as kernel methods,
spectral techniques and Gaussian processes. It presents a general solu-
tion strategy based on Krylov subspace iteration and fast N-body learn-
ing methods. The experiments show signi ﬁcant gains in computation and
storage on datasets arising in image segmentation, object detection and
dimensionality reduction. The paper also presents theoretical bounds on
the stability of these methods.

1 Introduction

Machine learning techniques based on similarity metrics have gained wide acceptance over
the last few years. Spectral clustering [1] is a typical example. Here one forms a Laplacian
matrix L = D(cid:0)1=2WD(cid:0)1=2 , where the entries of W measure the similarity between data
points xi 2 X , i = 1; : : : ; N . For example, a popular choice is to set the entries of W to

(cid:27) kxi(cid:0)xj k2
wij = e(cid:0) 1
where (cid:27) is a user-speciﬁed parameter. D is a normalizing diagonal matrix with entries
di = Pj wij . The clusters can be found by running, say, K-means on the eigenvectors of
L. K-means generates better clusters on this nonlinear embedding of the data provided one
adopts a suitable similarity metric.

The list of machine learning domains where one forms a covariance or similarity matrix
(be it W, D(cid:0)1W or D (cid:0) W) is vast and includes ranking on nonlinear manifolds [2],
semi-supervised and active learning [3], Gaussian processes [4], Laplacian eigen-maps [5],
stochastic neighbor embedding [6], multi-dimensional scaling, kernels on graphs [7] and
many other kernel methods for dimensionality reduction, feature extraction, regression and
classiﬁcation.
In these settings, one is interested in either inverting the similarity matrix
or ﬁnding some of its eigenvectors. The computational cost of both of these operations
is O(N 3 ) while the storage requirement is O(N 2 ). These costs are prohibitively large in

applications where one encounters massive quantities of data points or where one is inter-
ested in real-time solutions such as spectral image segmentation for mobile robots [8]. In
this paper, we present general numerical techniques for reducing the computational cost
to O(N log N ), or even O(N ) in speciﬁc
cases, and the storage cost to O(N ). These
reductions are achieved by combining Krylov subspace iterative solvers (such as Arnoldi,
Lanczos, GMRES and conjugate gradients) with fast kernel density estimation (KDE) tech-
niques (such as fast multipole expansions, the fast Gauss transform and dual tree recursions
[9, 10, 11]).

Speciﬁc Krylov methods have been applied to kernel problems. For example, [12] uses
Lanczos for spectral clustering and [4] uses conjugate gradients for Gaussian processes.
However, the use of fast KDE methods, in particular fast multipole methods, to further
accelerate these techniques has only appeared in the context of interpolation [13] and our
paper on semi-supervised learning [8]. Here, we go for a more general exposition and
present several new examples, such as fast nonlinear embeddings and fast Gaussian pro-
cesses. More importantly, we attack the issue of stability of these methods. Fast KDE
techniques have guaranteed error bounds. However, if these techiques are used inside it-
erative schemes based on orthogonalization of the Krylov subspace, there is a danger that
the errors might grow over iterations. In practice, good behaviour has been observed. In
Section 4, we present theoretical results that explain these observations and shed light on
the behaviour of these algorithms. Before doing so, we begin with a very brief review of
Krylov solvers and fast KDE methods.

2 Krylov subspace iteration

This section is a compressed overview of Krylov subspace iteration. The main message is
that Krylov methods are very efﬁcient algorithms for solving linear systems and eigenvalue
problems, but they require a matrix vector multiplication at each iteration.
In the next
section, we replace this expensive matrix-vector multiplication with a call to fast KDE
routines. Readers happy with this message and familiar with Krylov methods, such as
conjugate gradients and Lanczos, can skip the rest of this section.

For ease of presentation, let the similarity matrix be simply A = W 2 RN (cid:2)N , with
entries aij = a(xi ; xj ). (One can easily handle other cases, such as A = D(cid:0)1W and
A = D(cid:0)W.) Typical measures of similarity include polynomial a(x i ; xj ) = (xi xT
j + b)p ,
(cid:27) (xi(cid:0)xj )(xi(cid:0)xj )T and sigmoid a(xi ; xj ) = tanh((cid:11)xi xT
Gaussian a(xi ; xj ) = e(cid:0) 1
j (cid:0) (cid:12) )
kernels, where xi xT
j denotes a scalar inner product. Our goal is to solve linear systems
Ax = b and (possibly generalized) eigenvalue problems Ax = (cid:21)x. The former arise,
for example, in semi-supervised learning and Gaussian processes, while the latter arise in
spectral clustering and dimensionality reduction. One could attack these problems with
naive iterative methods such as the power method, Jacobi and Gauss-Seidel [14]. The
problem with these strategies is that the estimate x(t) , at iteration t, only depends on the
previous estimate x(t(cid:0)1) . Hence, these methods do typically take too many iterations to
converge. It is well accepted in the numerical computation ﬁeld that Krylov methods [14,
15], which make use of the entire history of solutions fx(1) ; : : : ; x(t(cid:0)1) g, converge at a
faster rate.

The intuition behind Krylov subspace methods is to use the history of the solutions we have
already computed. We formulate this intuition in terms of projecting an N -dimensional
problem onto a lower dimensional subspace. Given a matrix A and a vector b, the associ-
ated Krylov matrix is:

K = [b Ab A2b : : :
]:
The Krylov subspaces are the spaces spanned by the column vectors of this matrix. In

order to ﬁnd a new estimate of x(t) we could project onto the Krylov subspace. However,
K is a poorly conditioned matrix. (As in the power method, Atb is converging to the
eigenvector corresponding to the largest eigenvalue of A.) We therefore need to construct
a well-conditioned orthogonal matrix Q(t) = [q(1) (cid:1) (cid:1) (cid:1) q(t) ], with q(i) 2 RN , that spans
the Krylov space. That is, the leading t columns of K and Q span the same space. This is
easily done using the QR-decomposition of K [14], yielding the following Arnoldi relation
(augmented Schuur factorization):
AQ(t) = Q(t+1) eH(t) ;
where eH(t) is the augmented Hessenberg matrix:
0
1
h1;1 h1;2 h1;3
h2;1 h2;2 h2;3
BBBB@
CCCCA
...
...
...
0
(cid:1) (cid:1) (cid:1)
0
0
(cid:1) (cid:1) (cid:1)
0
The eigenvalues of the smaller (t + 1) (cid:2) t Hessenberg matrix approximate the eigenvalues
of A as t increases. These eigenvalues can be computed efﬁciently by applying the Arnoldi
relation recursively as shown in Figure 1. (If A is symmetric, then eH is tridiagonal and
we obtain the Lanczos algorithm.) Notice that the matrix vector multiplication v = Aq is
the expensive step in the Arnoldi algorithm. Most Krylov algorithms resemble the Arnoldi
algorithm in this. To solve systems of equations, we can minimize either the residual

h1; t
h2;t
...
ht;t
ht+1;t

(cid:1) (cid:1) (cid:1)
(cid:1) (cid:1) (cid:1)
...
ht;t(cid:0)1
0

eH(t) =

:

Initialization: b = arbitrary , q(1) = b=kbk
FOR t = 1; 2; 3; : : :
(cid:15) v = Aq(t)
(cid:15) FOR j = 1; : : : ; N
– hj;t = q(t)T v
– v = v (cid:0) hj;tq(j )
(cid:15) ht+1;t = kvk
(cid:15) q(t+1) = v=ht+1;t

Initialization: q(1) = b=kbk
FOR t = 1; 2; 3; : : :
(cid:15) Perform step t of the Arnoldi algorithm
(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13) eH(t)y (cid:0) kbki
(cid:15) Set x(t) = Q(t) y(t)

(cid:15) min

y

Figure 1: The Arnoldi (left) and GMRES (right) algorithms.

r(t) , b (cid:0) Ax(t) , leading to the GMRES and MINRES algorithms, or the A-norm, leading
to conjugate gradients (CG) [14]. GMRES, MINRES and CG apply to general, symmetric,
and spd matrices respectively. For ease of presentation, we focus on the GMRES algorithm.

At step t of GMRES, we approximate the solution by the vector in the Krylov subspace
x(t) 2 K(t) that minimizes the norm of the residual. Since x(t) is in the Krylov subspace,
it can be written as a linear combination of the columns of the Krylov matrix K (t) . Our
problem therefore reduces to ﬁnding the vector y 2 Rt that minimizes kAK(t)y (cid:0) bk.
As before, stability considerations force us to use the QR decomposition of K (t) . That is,
instead of using a linear combination of the columns of K(t) , we use a linear combination
of the columns of Q(t) . So our least squares problem becomes y (t) = miny kAQ(t)y(cid:0)bk:
Since AQ(t) = Q(t+1) eH(t) , we only need to solve a problem of dimension (t + 1) (cid:2) t:
y(t) = miny kQ(t+1) eH(t)y (cid:0) bk: Keeping in mind that the columns of the projection
matrix Q are orthonormal, we can rewrite this least squares problem as miny k eH(t)y (cid:0)
Q(t+1)T bk: We start the iterations with q(1) = b=kbk and hence Q(t+1)T b = kbki,

form of our least squares

entry. The ﬁnal
where i is the unit vector with a 1 in the ﬁrst
problem at iteration t is:
y (cid:13)(cid:13)(cid:13) eH(t)y (cid:0) kbki(cid:13)(cid:13)(cid:13) ;
y(t) = min
with solution x(t) = Q(t)y(t) . The algorithm is shown in Figure 1. The least squares
problem of size (t + 1) (cid:2) t to compute y(t) can be solved in O(t) steps using Givens
rotations [14]. Notice again that the expensive step in each iteration is the matrix-vector
product v = Aq. This is true also of CG and other Krylov methods.
One important property of the Arnoldi relation is that the residuals are orthogonal to the
space spanned by the columns of V = Q(t+1) eH(t) . That is,
VT r(t) = eH(t)T Q(t+1)T (b (cid:0) Q(t+1) eH(t)y(t) ) = eH(t)T kbki (cid:0) eH(t)T eH(t)y(t) = 0
In the following section, we introduce methods to speed up the matrix-vector product v =
Aq. These methods will incur, at most, a pre-speciﬁed (tolerance) error e (t) at iteration
t. Later, we present theoretical bounds on how these errors affect the residuals and the
orthogonality of the Krylov subspace.

3 Fast KDE

vi =

i = 1; 2; : : : ; M :

q (t)
j a(xi ; xj )

The expensive step in Krylov methods is the operation v = Aq(t) . This step requires that
we solve two O(N 2 ) kernel estimates:
NX
j=1
It is possible to reduce the storage and computational cost to O(N ) at the expense of a
small speciﬁed error tolerance (cid:15), say 10(cid:0)6 , using the fast Gauss transform (FGT) algorithm
[16, 17]. This algorithm is an instance of more general fast multipole methods for solving
N -body interactions [9]. The FGT applies when the problem is low dimensional, say
xk 2 R3 . However, to attack larger dimensions one can adopt clustering-based partitions
as in the improved fast Gauss transform (IFGT) [10].

Fast multipole methods tend to work only in low dimensions and are speciﬁc to the choice
of similarity metric. Dual tree recursions based on KD-trees and ball trees [11, 18] over-
come these difﬁculties, but on average cost O(N log N ). Due to space constraints, we can
only mention these techniques here, but refer the reader to [18] for a thorough comparison.

4 Stability results

The problem with replacing the matrix-vector multiplication at each iteration of the Krylov
methods is that we do not know how the errors accumulate over successive iterations. In
this section, we will derive bounds that describe what factors in ﬂuence
these errors. In
particular, the bounds will state what properties of the similarity metric and measurable
quantities affect the residuals and the orthogonality of the Krylov subspaces.

Several papers have addressed the issue of Krylov subspace stability [19, 20, 21]. Our
approach follows from [21]. For presentation purposes, we focus on the GMRES algorithm.

Let e(t) denote the errors introduced in the approximate matrix-vector multiplication at
each iteration of Arnoldi. For the purposes of upper-bounding, this is the tolerance of the
fast KDE methods. Then, the fast KDE methods change the Arnoldi relation to:
AQ(t) + E(t) = hAq(1) + e(1) ; : : : ; Aq(t) + e(t) i = Q(t+1) eH(t) ;

(cid:20)

where E(t) = (cid:2)e(1) ; : : : ; e(t) (cid:3). The new true residuals are therefore:
r(t) = b (cid:0) Ax(t) = b (cid:0) AQ(t) y(t) = b (cid:0) Q(t+1) eH(t)y(t) + E(t)y(t)
and er(t) = b (cid:0) Q(t+1) eH(t)y(t) are the measured residuals.
We need to ensure two bounds when using fast KDE methods in Krylov iterations. First,
the measured residuals er(t) should not deviate too far from the true residuals r(t) . Second,
deviations from orthogonality should be upper-bounded. Let us address the ﬁrst question.
The deviation in residuals is given by
ker(t) (cid:0) r(t)k = kE(t)y(t) k:
Let y(t) = [y1 ; : : : ; yt ]T . Then, this deviation satisﬁes:
yk e(k) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
ker(t) (cid:0) r(t) k = (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
tX
tX
jyk jke(k)k:
k=1
k=1
The deviation from orthogonality can be upper-bounded in a similar fashion:
kVT r(t)k=k eH(t)T Q(t+1)T (er(t) + E(t)y(t) )k = (cid:13)(cid:13)(cid:13) eH(t)T E(t)y(t) (cid:13)(cid:13)(cid:13) (cid:20) k eH(t)k
tX
k=1
(2)
The following lemma provides a relation between the yk and the measured residuals er(k(cid:0)1) .
Lemma 1. [21, Lemma 5.1] Assume that t iterations of the inexact Arnoldi method have
been carried out. Then, for any k = 1; : : : ; t,
1
ker(k(cid:0)1)k
jyk j (cid:20)
(cid:27)t ( eH(t) )
where (cid:27)t ( eH(t) ) denotes the t-th singular value of eH(t) .
The proof of the lemma follows from the QR decomposition of eH(t) , see [15, 21]. This
lemma, in conjunction with equations (1) and (2), allows us to establish the main theoretical
result of this section:
Proposition 1. Let (cid:15) > 0. If for every k (cid:20) t we have
(cid:27)t ( eH(t) )
t

jyk jke(k)k

(1)

(3)

(cid:15);

(cid:15);

1
ker(k(cid:0)1) k
1
ker(k(cid:0)1) k

(cid:27)t ( eH(t) )
tk eH(t)k

ke(k)k <
then ker(t) (cid:0) r(t) k < (cid:15). Moreover, if
ke(k)k <

then kVT r(t)k < (cid:15).

Proof: First, we have
tX
tX
(cid:27)t ( eH(t) )
1
1
ker(t) (cid:0) r(t) k (cid:20)
jyk jke(k)k <
(cid:27)t ( eH(t) )
ker(k(cid:0)1)k
t
k=1
k=1
and similarly, kVT r(t) k (cid:20) k eH(t)k Pt
k=1 jyk jke(k)k < (cid:15) (cid:3)
Proposition 1 tells us that in order to keep the residuals bounded while ensuring bounded
deviations from orthogonality at iteration k , we need to monitor the eigenvalues of eH(t) and
the measured residuals er(k(cid:0)1) . Of course, we have no access to eH(t) . However, monitoring
the residuals is of practical value. If the residuals decrease, we can increase the tolerance of
the fast KDE algorithms and viceversa. The bounds do lead to a natural way of constructing
adaptive algorithms for setting the tolerance of the fast KDE algorithms.

ker(k(cid:0)1)k = (cid:15):

(cid:15)

(a)

(b)

Time Comparison

1200

1000

800

600

400

200

)
s
d
n
o
c
e
S
(
 
e
m
i
T

NAIVE

CG

CG−DT

(d)

(c)

1000

2000

3000

4000

5000

6000

7000

Data Set Size (Number of Features)

Figure 2: Figure (a) shows a test image from the PASCAL database. Figure (b) shows the
SIFT features extracted from the image. Figure (c) shows the positive feature predictions
for the label ”car”.
Figure (d) shows the centroid of the positive features as a black dot. The
plot on the right shows the computational gains obtained by using fast Krylov methods.

5 Experimental results

computational gains may be ob-
The results of this section demonstrate that signi ﬁcant
tained by combining fast KDE methods with Krylov iterations. We present results in three
domains: spectral clustering and image segmentation [1, 12], Gaussian process regression
[4] and stochastic neighbor embedding [6].

5.1 Gaussian processes with large dimensional features

In this experiment we use Gaussian processes to predict the labels of 128-dimensional
SIFT features [22] for the purposes of object detection and localization as shown in Fig-
ure 2. There are typically thousands of features per image, so it is of paramount importance
to generate fast predictions. The hard computational task here involves inverting the co-
variance matrix of the Gaussian process. The ﬁgure
shows that it is possible to do this
efﬁciently , under the same ROC error, by combining conjugate gradients [4] with dual
trees.

5.2 Spectral clustering and image segmentation

We applied spectral clustering to color image segmentation; a generalized eigenvalue prob-
lem. The types of segmentations obtained are shown in Figure 3. There are no perceptible
differences between them. We observed that fast Krylov methods run approximately twice
as fast as the Nystrom method. One should note that the result of Nystrom depends on the
quality of sampling, while fast N-body methods enable us to work directly with the full
matrix, so the solution is less sensitive. Once again, fast KDE methods lead to signi ﬁcant
computational improvements over Krylov algorithms (Lanczos in this case).

5.3 Stochastic neighbor embedding

Our ﬁnal example is again a generalized eigenvalue problem arising in dimensionality re-
duction. We use the stochastic neighbor embedding algorithm of [6] to project two 3-D
structures to 2-D, as shown in Figure 4. Again, we observe signi ﬁcant computational im-
provements.

104

103

102

101

100

)
s
d
n
o
c
e
s
(
e
m
i
t
 
g
n
i
n
n
u
R

10−1

10−2

0

Lanczos
IFGT
Dual Tree

500

1000

1500

2000

2500
N

3000

3500

4000

4500

5000

Figure 3: (left) Segmentation results (order: original image, IFGT, dual trees and Nystrom)
and (right) computational improvements obtained in spectral clustering.

6 Conclusions

We presented a general approach for combining Krylov solvers and fast KDE methods
to accelerate machine learning techniques based on similarity metrics. We demonstrated
some of the methods on several datasets and presented results that shed light on the stability
and convergence properties of these methods. One important point to make is that these
methods work better when there is structure in the data. There is no computational gain
if there is not statistical information in the data. This is a fascinating relation between
computation and statistical information, which we believe deserves further research and
understanding. One question is how can we design pre-conditioners in order to improve the
convergence behavior of these algorithms. Another important avenue for further research
is the application of the bounds presented in this paper in the design of adaptive algorithms.

Acknowledgments

We would like to thank Arnaud Doucet, Firas Hamze, Greg Mori and Changjiang Yang.

References

[1] A Y Ng, M I Jordan, and Y Weiss. On spectral clustering: Analysis and algorithm. In Advances
in Neural Information Processing Systems, pages 849–856, 2001.
[2] D Zhou, J Weston, A Gretton, O Bousquet, and B Scholkopf. Ranking on data manifolds. In
Advances on Neural Information Processing Systems, 2004.
[3] X Zhu, J Lafferty, and Z Ghahramani. Semi-supervised learning using Gaussian ﬁelds and
harmonic functions. In International Conference on Machine Learning, pages 912–919, 2003.
[4] M N Gibbs. Bayesian Gaussian processes for regression and classiﬁcation.
In PhD Thesis,
University of Cambridge, 1997.
[5] M Belkin and P Niyogi. Laplacian eigenmaps for dimensionality reduction and data represen-
tation. Neural Computation, 15(6):1373–1396, 2003.
[6] G Hinton and S Roweis. Stochastic neighbor embedding. In Advances in Neural Information
Processing Systems, pages 833–840, 2002.
[7] A Smola and R Kondor. Kernels and regularization of graphs.
Theory, pages 144–158, 2003.

In Computational Learning

104

103

102

)
s
d
n
o
c
e
s
(
e
m
i
t
 
g
n
i
n
n
u
R

101

0

True manifold
S−curve

Sampled data

Embedding of SNE Embedding of SNE withIFGT
Swissroll

104

SNE
SNE with IFGT

1000

2000

3000

4000

5000

N

)
s
d
n
o
c
e
s
(
e
m
i
t
 
g
n
i
n
n
u
R

103

102

101

0

SNE
SNE with IFGT

1000

2000

3000

4000

5000

N

Figure 4: Examples of embedding on S-curve and Swiss-roll datasets.

Iintelligence

[8] M Mahdaviani, N de Freitas, B Fraser, and F Hamze. Fast computational methods for visually
guided robots. In IEEE International Conference on Robotics and Automation, 2004.
[9] L Greengard and V Rokhlin. A fast algorithm for particle simulations. Journal of Computa-
tional Physics, 73:325–348, 1987.
[10] C Yang, R Duraiswami, N A Gumerov, and L S Davis. Improved fast Gauss transform and
efﬁcient kernel density estimation.
In International Conference on Computer Vision, Nice,
2003.
[11] A Gray and A Moore. Rapid evaluation of multiple density models. In Artiﬁcial
and Statistics, 2003.
[12] J Shi and J Malik. Normalized cuts and image segmentation. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 731–737, 1997.
[13] R K Beatson, J B Cherrie, and C T Mouat. Fast ﬁtting of radial basis functions: Methods based
on preconditioned GMRES iteration. Advances in Computational Mathematics, 11:253 –270,
1999.
[14] J W Demmel. Applied Numerical Linear Algebra. SIAM, 1997.
[15] Y Saad. Iterative Methods for Sparse Linear Systems. The PWS Publishing Company, 1996.
[16] L Greengard and J Strain. The fast Gauss transform. SIAM Journal of Scientiﬁc Statistical
Computing, 12(1):79–94, 1991.
[17] B J C Baxter and G Roussos. A new error estimate of the fast Gauss transform. SIAM Journal
of Scienti ﬁc Computing, 24(1):257 –259, 2002.
[18] D Lang, M Klaas, and N de Freitas. Empirical testing of fast kernel density estimation algo-
rithms. Technical Report TR-2005-03, Department of Computer Science, UBC, 2005.
[19] G H Golub and Q Ye.
Inexact preconditioned conjugate gradient method with inner-outer
iteration. SIAM Journal of Scientiﬁc Computing, 21:1305–1320, 1999.
[20] G W Stewart. Backward error bounds for approximate Krylov subspaces. Linear Algebra and
Applications, 340:81–86, 2002.
[21] V Simoncini and D B Szyld. Theory of inexact Krylov subspace methods and applications to
scientiﬁc computing. SIAM Journal on Scientiﬁc Computing, 25:454 –477, 2003.
[22] D G Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.


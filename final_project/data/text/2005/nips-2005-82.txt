Robust Fisher Discriminant Analysis

Seung-Jean Kim Alessandro Magnani
Stephen P. Boyd
Information Systems Laboratory
Electrical Engineering Department, Stanford University
Stanford, CA 94305-9510
alem@stanford.edu

sjkim@stanford.edu

boyd@stanford.edu

Abstract

Fisher linear discriminant analysis (LDA) can be sensitive to the prob-
lem data. Robust Fisher LDA can systematically alleviate the sensitivity
problem by explicitly incorporating a model of data uncertainty in a clas-
si ﬁcation problem and optimizing for the worst-case scenar io under this
model. The main contribution of this paper is show that with general
convex uncertainty models on the problem data, robust Fisher LDA can
be carried out using convex optimization. For a certain type of product
form uncertainty model, robust Fisher LDA can be carried out at a cost
comparable to standard Fisher LDA. The method is demonstrated with
some numerical examples. Finally, we show how to extend these results
to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a
high dimensional feature space.

1

Introduction

Fisher linear discriminant analysis (LDA), a widely-used technique for pattern classi ﬁca-
tion, ﬁnds a linear discriminant that yields optimal discri mination between two classes
which can be identi ﬁed with two random variables, say X and Y in Rn . For a (linear)
discriminant characterized by w ∈ Rn , the degree of discrimination is measured by the
Fisher discriminant ratio

f (w, µx , µy , Σx , Σy ) =

wT (µx − µy )(µx − µy )T w
(wT (µx − µy ))2
wT (Σx + Σy )w
wT (Σx + Σy )w
where µx and Σx (µy and Σy ) denote the mean and covariance of X (Y). A discriminant
that maximizes the Fisher discriminant ratio is given by

=

,

wnom = (Σx + Σy )−1 (µx − µy ),
which gives the maximum Fisher discriminant ratio

(µx − µy )T (Σx + Σy )−1 (µx − µy ) = max
w 6=0

f (w, µx , µy , Σx , Σy ).

In applications, the problem data µx , µy , Σx , and Σy are not known but are estimated
from sample data. Fisher LDA can be sensitive to the problem data:
the discriminant
wnom computed from an estimate of the parameters µx , µy , Σx , and Σy can give very

poor discrimination for another set of problem data that is also a reasonable estimate of the
parameters. In this paper, we attempt to systematically alleviate this sensitivity problem
by explicitly incorporating a model of data uncertainty in the classi ﬁcation problem and
optimizing for the worst-case scenario under this model.
We assume that the problem data µx , µy , Σx , and Σy are uncertain, but known to belong to
a convex compact subset U of Rn × Rn × Sn
++ . Here we use Sn
++ (Sn
+ ) to denote
++ × Sn
the set of all n × n symmetric positive deﬁnite (semideﬁnite) matrices. We mak
e one
technical assumption: for each (µx , µy , Σx , Σy ) ∈ U , we have µx 6= µy . This assumption
simply means that for each possible value of the means and covariances, the classes are
distinguishable via Fisher LDA.

The worst-case analysis problem of ﬁnding the worst-case means and covariances for a
given discriminant w can be written as
minimize
subject to

f (w, µx , µy , Σx , Σy )
(µx , µy , Σx , Σy ) ∈ U ,
with variables µx , µy , Σx , and Σy . The optimal value of this problem is the worst-case
Fisher discriminant ratio (over the class U of possible means and covariances), and any op-
timal points for this problem are called worst-case means and covariances. These depend
on w .
We will show in §2 that (1) is a convex optimization problem, since the Fisher discriminant
ratio is a convex function of µx , µy , Σx , Σy for a given discriminant w . As a result, it is
computationally tractable to ﬁnd the worst-case performan ce of a discriminant w over the
set of possible means and covariances.

(1)

The robust Fisher LDA problem is to ﬁnd a discriminant that maximizes the worst-case
Fisher discriminant ratio. This can be cast as the optimization problem

maximize

min
(µx ,µy ,Σx ,Σy )∈U
subject to w 6= 0,

f (w, µx , µy , Σx , Σy )

(2)

with variable w . We denote any optimal w for this problem as w⋆ . Here we choose a linear
discriminant that maximizes the Fisher discrimination ratio, with the worst possible means
and covariances that are consistent with our data uncertainty model.

The main result of this paper is to give an effective method for solving the robust Fisher
LDA problem (2). We will show in §2 that the robust optimal Fisher discriminant w⋆ can
be found as follows. First, we solve the (convex) optimization problem

minimize max
f (w, µx , µy , Σx , Σy ) = (µx − µy )T (Σx + Σy )−1 (µx − µy )
w 6=0
subject to (µx , µy , Σx , Σy ) ∈ U ,
y ) denote any optimal point. Then the
with variables (µx , µy , Σx , Σy ). Let (µ⋆
x , Σ⋆
y , Σ⋆
x , µ⋆
discriminant
y (cid:1)−1
(µ⋆
x − µ⋆
w⋆ = (cid:0)Σ⋆
x + Σ⋆
(4)
y )
is a robust optimal Fisher discriminant, i.e., it is optimal for (2). Moreover, we will see
y are worst-case means and covariances for the robust optimal Fisher
that µ⋆
y and Σ⋆
x , µ⋆
x , Σ⋆
discriminant w⋆ . Since convex optimization problems are tractable, this means that we
have a tractable general method for computing a robust optimal Fisher discriminant.

(3)

A robust Fisher discriminant problem of modest size can be solved by standard convex
optimization methods, e.g., interior-point methods [3]. For some special forms of the un-
certainty model, the robust optimal Fisher discriminant can be solved more efﬁciently than
by a general convex optimization formulation. In §3, we consider an important special form
for U for which a more efﬁcient formulation can be given.

In comparison with the ‘nominal’ Fisher LDA, which is based on the means and covari-
ances estimated from the sample data set without considering the estimation error, the
robust Fisher LDA performs well even when the sample size used to estimate the means
and covariances is small, resulting in estimates which are not accurate. This will be demon-
strated with some numerical examples in §4.
Recently, there has been a growing interest in kernel Fisher discriminant analysis i.e., Fisher
LDA in a higher dimensional feature space, e.g., [7]. Our results can be extended to robust
kernel Fisher discriminant analysis under certain uncertainty models. This will be brieﬂy
discussed in §5.
Various types of robust classi ﬁcation problems have been co nsidered in the prior litera-
ture, e.g., [2, 5, 6]. Most of the research has focused on formulating robust classi ﬁcation
problems that can be efﬁciently solved via convex optimizat
ion. In particular, the robust
classi ﬁcation method developed in [6] is based on the criter ion

g(w, µx , µy , Σx , Σy ) =

|wT (µx − µy )|
(wT Σxw)1/2 + (wT Σy w)1/2 ,
which is similar to the Fisher discriminant ratio f . With a speci ﬁc uncertainty model on the
means and covariances, the robust classi ﬁcation problem wi
th discrimination criterion g can
be cast as a second-order cone program, a special type of convex optimization problem [5].
With general uncertainty models, however, it is not clear whether robust discriminant anal-
ysis with g can be performed via convex optimization.

2 Robust Fisher LDA

We ﬁrst consider the worst-case analysis problem (1). Here w e consider the discriminant w
as ﬁxed, and the parameters µx , µy , Σx , and Σy are variables, constrained to lie in the
convex uncertainty set U . To show that (1) is a convex optimization problem, we must
show that the Fisher discriminant ratio is a convex function of µx , µy , Σx , and Σy . To
show this, we express the Fisher discriminant ratio f as the composition

f (w, µx , µy , Σx , Σy ) = g(H (µx , µy , Σx , Σy )),

where g(u, t) = u2/t and H is the function

H (µx , µy , Σx , Σy ) = (wT (µx − µy ), wT (Σx + Σy )w).

The function H is linear (as a mapping from µx , µy , Σx , and Σy into R2 ), and the function
g is convex (provided t > 0, which holds here). Thus, the composition f is a convex
function of µx , µy , Σx , and Σy . (See [3].)
Now we turn to the main result of this paper. Consider a function of the form

(wT a)2
wT Bw
++ , evaluated at w .
+ and B ∈ Sn
which is the Rayleigh quotient for the matrix pair aaT ∈ Sn
The robust Fisher LDA problem (2) is equivalent to a problem of the form

R(w, a, B ) =

,

(5)

maximize

min
(a,B )∈V
subject to w 6= 0,

R(w, a, B )

(6)

where

a = µx − µy , B = Σx + Σy , V = {(µx − µy , Σx + Σy ) | (µx , µy , Σx , Σy ) ∈ U }. (7)

(This equivalence means that robust FLDA is a special type of robust matched ﬁltering
problem studied in the 1980s; see, e.g., [8] for more on robust matched ﬁltering.)

We will prove a ‘nonconventional’ minimax theorem for a Rayleigh quotient of the
form (5), which will establish the main result described in §1. To do this, we consider
a problem of the form

minimize
aT B−1a
subject to (a, B ) ∈ V ,
with variables a ∈ Rn , B ∈ Sn
++ , and V is a convex compact subset of Rn × Sn
++ such
that for each (a, B ) ∈ V , a is not zero. The objective of this problem is a matrix fractional
++ ; see [3, §3.1.7]. Our problem (3) is the same as (8),
function and so is convex on Rn × Sn
with (7). It follows that (3) is a convex optimization problem.

(8)

The following theorem states the minimax theorem for the function R. While R is convex in
(a, B ) for ﬁxed w , it is not concave in w for ﬁxed (a, B ), so conventional convex-concave
minimax theorems do not apply here.
Theorem 1. Let (a⋆ , B ⋆ ) be an optimal solution to the problem (8), and let w⋆ = B ⋆−1a⋆ .
Then (w⋆ , a⋆ , B ⋆ ) satis ﬁes the minimax property

R(w⋆ , a⋆ , B ⋆ ) = max
w 6=0

min
(a,B )∈V

R(w, a, B ) = min
(a,B )∈V

max
w 6=0

R(w, a, B ),

(9)

and the saddle point property

R(w, a⋆ , B ⋆ ) ≤ R(w⋆ , a⋆ , B ⋆ ) ≤ R(w⋆ , a, B ), ∀w ∈ Rn \{0}, ∀(a, B ) ∈ V .

(10)

Proof. It sufﬁces to prove (10), since the saddle point property (10 ) implies the minimax
property (9) [1, §2.6]. We start by observing that R(w, a⋆ , B ⋆ ) is maximized over nonzero
w 6= 0 by w⋆ = B ⋆−1a⋆ (by the Cauchy-Schwartz inequality). What remains is to show

min
(a,B )∈V

R(w⋆ , a, B ) = R(w⋆ , a⋆ , B ⋆ ).

(11)

Since a⋆ and B ⋆ are optimal for the convex problem (8) (by deﬁnition), they m ust satisfy
the optimality condition
D ∇a (aT B−1a)(cid:12)(cid:12)(a⋆ ,B⋆ ) , (a − a⋆ )E + D ∇B (aT B−1a)(cid:12)(cid:12)(a⋆ ,B⋆ ) , (B − B ⋆ )E
≥ 0,
∀ (a, B ) ∈ V
(see [3, §4.2.3]). Using ∇a (aT B−1a) = 2B−1a, ∇B (aT B−1a) = −B−1aaT B−1 , and
hX, Y i = Tr(X Y ) for X, Y ∈ Sn , where Tr denotes trace, we can express the optimality
condition as

2a⋆ T B ⋆−1 (a − a⋆ ) − TrB ⋆−1a⋆a⋆ T B ⋆−1 (B − B ⋆ ) ≥ 0,

∀ (a, B ) ∈ V ,

or equivalently,

2w⋆ T (a − a⋆ ) − w⋆ T (B − B ⋆ )w⋆ ≥ 0,

∀ (a, B ) ∈ V .

Now we turn to the convex optimization problem

minimize R(w⋆ , a, B )
subject to (a, B ) ∈ V ,

(12)

(13)

with variables (a, B ). We will show that (a⋆ , B ⋆ ) is optimal for this problem, which will
establish (11).

A pair (¯a, ¯B ) is optimal for (13) if and only if
* ∇a
, (a − ¯a)++* ∇B
(w⋆T a)2
(w⋆T a)2
w⋆T Bw⋆ (cid:12)(cid:12)(cid:12)(cid:12)(¯a, ¯B )
w⋆T Bw⋆ (cid:12)(cid:12)(cid:12)(cid:12)(¯a, ¯B )
Using
aT w⋆
(w⋆T a)2
(w⋆T a)2
w⋆Bw⋆ w⋆ ,
∇B
w⋆T Bw⋆ = −
w⋆T Bw⋆ = 2
∇a
the optimality condition can be written as

, (B − ¯B )+ ≥ 0,

∀ (a, B ) ∈ V .

(aT w⋆ )2
(w⋆T Bw⋆ )2 w⋆w⋆ T ,

2

= 2

w⋆ T (a − ¯a) −

w⋆ T (B − ¯B )w⋆

w⋆w⋆ T (B − ¯B )

w⋆ T (a − ¯a) − Tr

(¯aT w⋆ )2
(w⋆ T ¯Bw⋆ )2
(¯aT w⋆ )2
(w⋆ T ¯Bw⋆ )2

¯aT w⋆
w⋆ T ¯Bw⋆
¯aT w⋆
w⋆ T ¯Bw⋆
≥ 0,
∀ (a, B ) ∈ V .
Substituting ¯a = a⋆ , ¯B = B ⋆ , and noting that a⋆T w⋆ /w⋆T B ⋆w⋆ = 1, the optimality
condition reduces to
2w⋆ T (a − a⋆ ) − w⋆ T (B − B ⋆ )w⋆ ≥ 0,
which is precisely (12). Thus, we have shown that (a⋆ , B ⋆ ) is optimal for (13), which in
turn establishes (11).

∀ (a, B ) ∈ V ,

3 Robust Fisher LDA with product form uncertainty models

In this section, we focus on robust Fisher LDA with the product form uncertainty model

U = M × S ,

(14)

where M is the set of possible means and S is the set of possible covariances. For this
model, the worst-case Fisher discriminant ratio can be written as

min
(µx ,µy ,Σx ,Σy )∈U

f (a, µx , µy , Σx , Σy ) = min
(µx ,µy )∈M

(wT (µx − µy ))2
max(Σx ,Σy )∈S wT (Σx + Σy )w

.

If we can ﬁnd an analytic expression for max(Σx ,Σy )∈S wT (Σx + Σy )w (as a function of
w), we can simplify the robust Fisher LDA problem.
As a more speci ﬁc example, we consider the case in which S is given by

S = Sx × Sy ,
Sx = {Σx | Σx (cid:23) 0, kΣx − ¯Σx kF ≤ δx },
Sy = {Σy | Σy (cid:23) 0, kΣy − ¯Σy kF ≤ δy },
where δx , δy are positive constants, ¯Σx , ¯Σy ∈ Sn
++ , and kAkF denotes the Frobenius norm
of A, i.e., kAkF = (Pn
ij )1/2 . For this case, we have
i,j=1 A2
wT (Σx + Σy )w = wT ( ¯Σx + ¯Σy + (δx + δy )I )w.
max
(Σx ,Σy )∈S
++ , maxkΣ− ¯ΣkF ≤δ xT Σx = xT ( ¯Σ + δI )x
Here we have used the fact that for given ¯Σ ∈ Sn
(see, e.g., [6]). The worst-case Fisher discriminant ratio can be expressed as

(15)

(16)

min
(µx ,µy )∈M

(wT (µx − µy ))2
wT ( ¯Σx + ¯Σy + (δx + δy )I )w

.

x − µ⋆
(µ⋆
y ),

This is the same worst-case Fisher discriminant ratio obtained for a problem in which the
covariances are certain, i.e., ﬁxed to be ¯Σx + δx I and ¯Σy + δy I , and the means lie in the set
M. We conclude that a robust optimal Fisher discriminant with the uncertainty model (14)
in which S has the form (15) can be found by solving a robust Fisher LDA problem with
these ﬁxed values for the covariances. From the general solu tion method described in §1, it
is given by
w⋆ = (cid:0) ¯Σx + ¯Σy + (δx + δy )I (cid:1)−1
x and µ⋆
where µ⋆
y solve the convex optimization problem
(µx − µy )T (cid:0) ¯Σx + ¯Σy + (δx + δy )I (cid:1)−1
minimize
subject to (µx , µy ) ∈ M,
with variables µx and µy .
The problem (17) is relatively simple: it involves minimizing a convex quadratic function
over the set of possible µx and µy . For example, if M is a product of two ellipsoids, (e.g.,
µx and µy each lie in some conﬁdence ellipsoid) the problem (17) is to m inimize a convex
quadratic subject to two convex quadratic constraints. Such a problem is readily solved in
O(n3 ) ﬂops, since the dual problem has two variables, and evaluati ng the dual function
and its derivatives can be done in O(n3 ) ﬂops [3]. Thus, the effort to solve the robust is
the same order (i.e., n3 ) as solving the nominal Fisher LDA (but with a substantially larger
constant).

(µx − µy )

(17)

4 Numerical results

To demonstrate robust Fisher LDA, we use the sonar and ionosphere benchmark problems
from the UCI repository (www.ics.uci.edu/∼mlearn/MLRepository.html).
The two benchmark problems have 208 and 351 points, respectively, and the dimension
of each data point is 60 and 34, respectively. Each data set is randomly partitioned into
a training set and a test set. We use the training set to compute the optimal discriminant
and then test its performance using the test set. A larger training set typically gives better
test performance. We let α denote the size of the training set, as a fraction of the total
number of data points. For example, α = 0.3 means that 30% of the data points are used
for training, and 70% are used to test the resulting discriminant. For various values of α,
we generate 100 random partitions of the data (for each of the two benchmark problems),
and collect the results.
We use the following uncertainty models for the means µx , µy and the covariances Σx , Σy :

(µx − ¯µx )T Px (µx − ¯µx ) ≤ 1,
(µy − ¯µy )T Py (µy − ¯µy ) ≤ 1,

kΣx − ¯Σx kF ≤ ρx ,
kΣy − ¯Σy kF ≤ ρy ,

Here the vectors ¯µx , ¯µy represent the nominal means and the matrices ¯Σx , ¯Σy represent
the nominal covariances, and the matrices Px , Py and the constants ρx and ρy represent
the conﬁdence regions. The parameters are estimated throug h a resampling technique [4]
as follows. For a given training set we create 100 new sets by resampling the original
training set with a uniform distribution over all the data points. For each of these sets we
estimate its mean and covariance and then take their average values as the nominal mean
and covariance. We also evaluate the covariance Σµ of all the means obtained with the
resampling. We then take Px = Σ−1
µ /n and Py = Σ−1
µ /n. This choice corresponds
to a 50% conﬁdence ellipsoid in the case of a Gaussian distribution. The parameters ρx
and ρy are taken to be the maximum deviations between the covariances and the average
covariances in the Frobenius norm sense, over the resampling of the training set.

100

90

80

70

60

)
%
(
A
S
T

sonar

robust

nominal

ionosphere

robust

nominal

100

90

80

70

60

50
20

30

40

60

30
50
α (%)
α (%)
Figure 1: Test-set accuracy (TSA) for sonar and ionosphere benchmark versus size of the
training set. The solid line represents the robust Fisher LDA results and the dotted line the
nominal Fisher LDA results. The vertical bars represent the standard deviation.

70

80

50

60

10

20

40

50
0

Figure 1 summarizes the classi ﬁcation results. For each of o ur two problems, and for each
value of α, we show the average test set accuracy (TSA), as well as the standard deviation
(over the 100 instances of each problem with the given value of α). The plots show the
robust Fisher LDA performs substantially better than the nominal Fisher LDA for small
training sets, but this performance gap disappears as the training set becomes larger.

5 Robust kernel Fisher discriminant analysis

In this section we show how to ‘kernelize’ the robust Fisher LDA. We will consider only
a speci ﬁc class of uncertainty models; the arguments we deve lop here can be extended to
more general cases. In the kernel approach we map the problem to an higher dimensional
space Rf via a mapping φ : Rn → Rf so that the new decision boundary is more general
and possibly nonlinear. Let the data be mapped as
x → φ(x) ∼ ( ¯µφ(x) , ¯Σφ(x) ),
y → φ(y) ∼ ( ¯µφ(y) , ¯Σφ(y) ).

The uncertainty model we consider has the form

µφ(x) − µφ(y) = ¯µφ(x) − ¯µφ(y) + P uf ,
kuf k ≤ 1,
kΣφ(y) − ¯Σφ(y) kF ≤ ρy .
kΣφ(x) − ¯Σφ(x) kF ≤ ρx ,
Here the vectors ¯µφ(x) , ¯µφ(y) represent the nominal means, the matrices ¯Σφ(x) , ¯Σφ(y) rep-
resent the nominal covariances, and the (positive semideﬁn ite) matrix P and the constants
ρx and ρy represent the conﬁdence regions in the feature space. The wo rst-case Fisher
discriminant ratio in the feature space is then given by

(18)

min
kuf k≤1,kΣφ(x)− ¯Σφ(x) kF ≤ρx ,kΣφ(y)− ¯Σφ(y) kF ≤ρy

f ( ¯µφ(x) − ¯µφ(y) + P uf ))2
(wT
wT
f (Σφ(x) + Σφ(y) )wf
The robust kernel Fisher discriminant analysis problem is to ﬁnd the discriminant in the
feature space that maximizes this ratio.

.

Using the technique described in §3, we can see that the robust kernel Fisher discriminant
analysis problem can be cast as

(wT
f ( ¯µφ(x) − ¯µφ(y) + P uf ))2
maximize min
f ( ¯Σφ(x) + ¯Σφ(y) + (ρx + ρy )I )wf
wT
kuf k≤1
subject to wf 6= 0,

(19)

where the discriminant wf ∈ Rf is deﬁned in the new feature space.
To apply the kernel trick to the problem (19), the nonlinear decision boundary should be
entirely expressed in terms of inner products of the mapped data only. The following
proposition tells us a set of conditions to do so.
i=1 and {yi }Ny
Proposition 1. Given the sample points {xi }Nx
i=1 , suppose that ¯µφ(x) , ¯µφ(y) ,
¯Σφ(x) , ¯Σφ(y) , and P can be written as
¯µφ(y) = PNy
¯µφ(x) = PNx
i=1 λi+Nx φ(yi ), P = U ΥU T ,
i=1 λiφ(xi ),
¯Σφ(x) = PNx
i=1 Λi,i (φ(xi ) − ¯µφ(x) )(φ(xi ) − ¯µφ(x) )T ,
¯Σφ(y) = PNy
i=1 Λi+Nx ,i+Nx (φ(yi ) − ¯µφ(y) )(φ(yi ) − ¯µφ(y) )T ,
Nx+Ny
Nx+Ny
where λ ∈ RNx+Ny , Υ ∈ S
is a diagonal matrix, and U is a matrix
, Λ ∈ S
+
+
i=1 and {φ(yi ) − ¯µφ(y) }Ny
whose columns are the vectors {φ(xi ) − ¯µφ(x) }Nx
i=1 . Denote as Φ
i=1 , {φ(yi )}Ny
the matrix whose columns are the vectors {φ(xi )}Nx
i=1 and deﬁne
D1 = K β , D2 = K (I − λ1T
N )Υ(I − λ1T
N )K T ,
D3 = K (I − λ1T
N )Λ(I − λ1T
N )K T + (ρx + ρy )K, D4 = K,
where K is the kernel matrix Kij = (ΦT Φ)ij , 1N is a vector of ones of length Nx + Ny ,
and β ∈ RNx+Ny is such that βi = λi for i = 1, . . . , Nx and βi = −λi for i = Nx +
1, . . . , Nx + Ny . Let ν ⋆ be an optimal solution of the problem
ν T (D1 + D2 ξ )(D1 + D2 ξ )T ν
ν T D3 ν

min
ξT D4 ξ≤1
subject to
ν 6= 0.
Then, w⋆
f = Φν ⋆ is an optimal solution of the problem (19). Moreover, for every point
z ∈ Rn ,

maximize

(20)

Ny
Nx
Xi=1
Xi=1
Along the lines of the proofs of Corollary 5 in [6], we can prove this proposition.

ν ⋆
i+Nx K (z , yi ).

ν ⋆
i K (z , xi ) +

w⋆T
f φ(z ) =

(21)

References
[1] D. Bertsekas, A. Nedi ´c, and A. Ozdaglar. Convex Analysis and Optimization. Athena Scientiﬁc,
2003.
[2] C. Bhattacharyya. Second order cone programming formulations for feature selection. Journal
of Machine Learning Research, 5:1417–1433, 2004.
[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[4] B. Efron and R.J. Tibshirani. An Introduction to Bootstrap. Chapman and Hall, London UK,
1993.
[5] K. Huang, H. Yang, I. King, M. Lyu, and L. Chan. The minimum error minimax probability
machine. Journal of Machine Learning Research, 5:1253–1286, 2004.
[6] G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and M. Jordan. A robust minimax approach to
classiﬁcation.
Journal of Machine Learning Research, 3:555–582, 2002.
[7] S. Mika, G. R ¨atsch, and K. M ¨uller. A mathematical programming approach to the kernel Fisher
algorithm, 2001. In Advances in Neural Information Processing Systems, 13, pp. 591-597, MIT
Press.
[8] S. Verd ´u and H. Poor. On minimax robustness: A general approach and applications.
Transactions on Information Theory, 30(2):328–340, 1984.

IEEE


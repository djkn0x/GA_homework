Machine Learning for Controlled Slides of a RC Car
CS229 Term Pro ject

Morgan Quigley and Michael Vitus

December 16, 2005

1 Hardware

The hardware platform for this pro ject was basically
unstoppable.

1.1 Drivetrain

For the drivetrain and mechanical platform of our
car, we started with an XRay M18 kit and added
a 150-watt brushless motor,
its matching 3-phase
power converter, and a 3-cell lithium polymer bat-
tery. We found that this drivetrain produces enor-
mous amounts of power, far beyond what we truly
needed. But power is addictive and you can never
have enough of it. To allow continous drifting of the
car, we covered the tires with small pieces of 1.25”
PVC pipe. The completed drivetrain is shown in Fig-
ure 1.

Figure 1: Brushless (3-phase) drivetrain with LiPo
battery and 72 MHz radio receiver

1.2 Control

Using hardware designed by Mark Woodward, our
software was able to generate standard 72 MHz ra-
dio control signals. Mark fabricated an Atmel-based

1

Figure 2: Final assembly with vision markers in-
stalled

board which accepts R/C stick positions via RS-
232 and outputs a standard R/C PWM pulse train
which is fed into the trainer port of an R/C trans-
mitter. The transmitter’s trainer switch is then used
to choose between autonomous and manual control
of the car. A standard R/C radio receiver was used
on the car to control the steering servo and provide
throttle input to the motor’s speed control.

1.3 Localization

As shown in Figure 2, we attached squares of red and
blue fabric to the front and rear of the car so that
we would be able to derive the position and heading
of the car from an overhead camera. The overhead
camera is able to pan and tilt to see a wide area
of ﬂoor space, but we found that when the camera
was in motion, latency from its pan and tilt encoders
threw the position estimates far out of whack. To
reduce this eﬀect, we discretized the camera’s motion
into 3 settings of pan and tilt (for a total of 9 possible
camera attitudes).
In addition, we added a green dot to the middle of
the playing ﬁeld, and diﬀerenced the car’s computed
position from the computed position of the green dot.
We found that this dramatically reduced the errors
from the moving camera, as the encoding errors will

equally skew the positions of the green dot and the
car, leaving their relative positions intact. We chose
the camera pan and tilt discretizations so as to en-
sure that the green dot at the origin never left the
camera frame. Two typical camera frames are shown
in Figure 3.

(a) Camera centered

(b) Camera panned to a corner

Figure 3: Two typical camera frames. The green
dot on the ﬂoor marks the center of the coordinate
system. The car’s red and blue squares give both
position and heading. (The white tape on the ﬂoor
is not related to our pro ject.)

After some experimentation, we were able to tweak
Mark’s vision code into running at 30 frames/sec,
which is the full frame rate of the camera.

1.4 Onboard Inertial Sensing
We soon found that the yaw rate estimates from the
vision system were noisier than we wanted. This was
unsurprising, as the yaw estimate itself is computed
as the arctangent of the coordinate distances between
the front and rear markers on the car. Taking the
numerical derivative of this already-diﬀerenced yaw
estimate introduced signiﬁcant noise into the system.
To overcome this, we added a black-box IMU (an
XSens MT9) to the car. Among the many sensors in
the IMU is a z-axis MEMS rate gyro, which provided
a direct measurement of yaw rate. We attached a 900
Mhz radio transceiver to the car, and used a Rabbit
microcontroller to parse the data streaming out of

2

Interface board stacked on 900 Mhz
Figure 4:
transceiver at left, IMU at center, and Rabbit mi-
crocontroller at right

the IMU, pick out the z-gyro readings, and transmit
them 30 times per second over the 900 MHz link.
We fabricated a board which regulated the car’s LiPo
battery to 3.3V, 5V, and 6V as needed by the various
parts of the system, and performed level conversion
between RS-232 (used by the IMU) and 3.3V TTL
(used by the microcontroller). These modules were
attached to the roof of the car, as shown in Figure 4.

A 900 MHz transceiver was connected to the con-
troller PC to receive the gyro data. To calibrate
and unbias the gyroscope measurements, we drove
the car for several minutes, and performed linear re-
gression of the vision yawrate estimate against the
z-gyro yawrate measurements. A typical plot of this
data is shown in Figure 5. Running the gyroscope
readings through this linear map resulted in the de-
sired scaling of radians/sec and zeroed the sensor’s
dc bias.

Figure 5: Calibration of the yaw rate gyro measure-
ments to radians/sec. Sensor saturation can be seen
at extreme right and left, but this was outside the
normal operating range of the experiment.

2 Model

After the hardware was assembled, the next step of
the pro ject was to develop a model of the car during
controlled slides. A body frame was used in modeling
of the car to take advantage of the symmetries of
the system which is shown in Figure 6.
In normal
operation of a car, the angle between the velocity
vector and the heading of the car, β , is in the range
of ±8o , and therefore it was used to distinguish when
the car was sliding.

Numerous feature mappings were explored in de-
termining the optimal trade-oﬀ between variance
1 (x (t)) = (cid:2)u v r ur vr u2 v2 r2 u3 v3 r3 (cid:3)T
and bias of the training data. The ﬁnal feature
1 (u (t)) = (cid:2)1 δ τ δ2 τ 2 δ3 τ 3 δτ (cid:3)T .
mappings that were used in determining the model
were Ψx
and Ψu
To evaluate the validity of the model, the data that
was parsed from the driving logs was integrated for-
ward in time to estimate the position of the car. The
average of the error for each time step was plotted
versus time for two diﬀerent feature mappings in Fig-
ure 7 where Ψx
2 (x (t)) = x (t) and Ψu
2 (u (t)) = u (t).

Figure 6: Coordinate frame of the car.

The states of the car that are the most important
to model at each time step are the forward velocity
of the car, u, the cross-track velocity, v , and the rate
of rotation around the vertical axis, r .
A discrete time model instead of a continuous time
model was used to model the car’s dynamics because
the data collected of the car was limited by the frame
rate of the camera which is 30 Hz. The model of the
car’s dynamics is shown in Equation (1) in which a
higher dimensional mapping, Ψ, is used for the cur-
rent state and current input to predict the state at
the next timestep.

x (t + 1) = AΨx (x (t)) + BΨu (u (t))

(1)

where x (t) = [u v r]T , u (t) = [δ τ ]T , δ is the steering
input, and τ is the throttle input. To determine the
model parameters, A and B, least squares was used to
minimize the error between the estimate of the state
and the actual state observed, which is shown in Eqn.
min X
(2).
t

kx (t + 1) − [AΨx (x (t)) + BΨu (u (t))] k2
(2)

Figure 7: Mean errors versus time of sequence.

A =

The higher dimensional feature mapping, Ψ1 , pro-
vides slight better integration error than using just
the current state and input to predict the next state.
To obtain a feel for the coeﬃcients, for the sake of
space, the following A and B matrices are the coeﬃ-


cients for the lower dimensional feature space, Ψ2 .
−1.55
0.97
0.016
1.21 × 10−4
−2.5
0.95
 −9.61

5 × 10−4
0
0.99
58.13
−0.06 −6.36
−0.08
0.30
With a valid model of the sliding dynamics, the
next step was to learn how to control the coordinated
slides.

B =

3 Controller

We experimented initially with an LQR controller,
but decided to implement a Q-learning controller.

3

Using the model, Ψ1 , produced in the previous sec-
tion, the Q-learner “drove” the simulation thousands
of times. The desired tra jectory of the car is a coor-
dinated slide of radius 1m which is shown in Figure
8.

unstable and enter into ”donuts”. Therefore, for the
reward function, we penalize the controller for chang-
ing the control action. At each timestep, the reward
structure was as follows:
R = Ro − 2 (|δnew − δold | + |τnew − τold |)

(3)

where

Ro = +1

if 90 cm ≤ r < 110 cm ∧
s ≤ ˙r < 30 cm
s ∧
−30 cm
s ≤ ˙γ < 1 rad
−1 rad
Ro = −1 if r < 30 cm ∨ r > 190 cm
s
Ro =
0
otherwise

Whenever Ro was -1, the simulation was restarted,
just as in the pendulum problem in the problem set.
The update step for the Q-function is:
Q (s, a) = (1 − α) Q (s, a) + α (R + γ max Q (s0 , a0 ))
(4)
An example of the simulation run after the con-
troller has trained the Q-function is shown in Figure
9.

Figure 9: Simulation of the model and controller.

4 Real-World Validation

To drive the real system, we simply exported the Q-
table from MATLAB, copied it to the PC hooked up
to the vision and control systems, and executed the
policy encoded in the Q-table in realtime. Generating
the controller’s state (r, ˙r , γ , ˙γ ) in realtime from the
output of vision system and the IMU took signiﬁcant
debugging eﬀorts. However, eventually we were able

4

Figure 8: Desired tra jectory of controlled slide and
car states.

The states that were used to discretize the state
space are (r, ˙r , γ , ˙γ ). The reasoning behind the choice
of these states is that for the desired tra jectory they
should remain constant, and therefore, it is lineariz-
ing the system around an operating point.
The ranges of the discretization are:
20 cm ≤ r < 50 cm
r < 20 cm
50 cm ≤ r < 90 cm 90 cm ≤ r < 110 cm
110 cm ≤ r < 150 cm 150 cm ≤ r < 190 cm
r ≥ 190 cm

˙r < −100 cm
−30 cm
s ≤ ˙r < 30 cm
s
˙r ≥ 100 cm
s
s

s ≤ ˙r < −30 cm
−100 cm
s ≤ ˙r < 100 cm
s
30 cm
s

2 rad ≥ γ > 1.25 rad
γ > 2 rad
1.25 rad ≥ γ > 0.5 rad 0.5 rad ≥ γ > −0.25 rad
γ ≤ −0.25 rad

−2 rad
s ≤ ˙γ < −1 rad
˙γ < −2 rad
−1 rad
s ≤ ˙γ < 0 rad
s ≤ ˙γ < 1 rad
s
s
0 rad
s ≤ ˙γ < 2 rad
˙γ ≥ 2 rad
s
s
1 rad
s
s
The steering actions that the controller was able
to choose between are: steer straight, left and hard
left, and the throttle actions are: slow, medium and
fast. Since the dynamics of the car during sliding are
unstable, erratic manuevers, such as constant switch-
ing of steering angle, will cause the system to become

to compute this information, discretize it, perform
the Q-table lookup, and send out the steering and
throttle commands in realtime.
It would be an overstatement to say that we were
entirely pleased with the results. We found that the
“handoﬀ” of control from the human driver to the
computer was tricky to get right. Because our model
only learned how to drive the car when it was slid-
ing, the human driver needed to initiate a slide and
then give control to the computer. In addition, the
Q-learner only learned how to control the car within
a certain range of initial conditions of (r, ˙r , γ , ˙γ ). Al-
though we randomized these initial conditions during
training, it still took some practice and patience to
execute the control ”handoﬀ” when the car was in a
feasible state.
Camera motion was also a diﬃculty; although the
green dot at the origin of the coordinate system
helped dampen spurious measurements during cam-
era motion, there was still some residual error in the
system during camera slew, and this often caused the
state estimates to be incorrect. We also suspect that
the overall system latency, including vision process-
ing, action selection, control waveform synthesis, and
actuator response, lead to the imprecise results. How-
ever, even with all of these issues, on a good run, the
Q-learner ended up controlling the car better than we
were able to do manually. Figure 10 shows the best
orbit the computer was able to perform, along with
the ideal 1-meter orbit.

During the trial shown in Figure 10, the car’s
sideslip angle ranged from 5 and 50 degrees. The
Q-learned policy was able to control this sideslip an-
gle to maintain some semblance of an orbit. A plot
of the sideslip angle experienced during the trial is
shown in Figure 11.

Figure 11: Sideslip angle during the trial.

5 Acknowledgments

We must acknowledge the help and advice of Mark
Woodward, who designed and built the R/C trans-
mitter interface hardware and software, and con-
structed the vision system. Without his help, this
pro ject would have never happened in a single quar-
ter. We also acknowledge Pieter Abbeel and Adam
Coates in their advice for the Q-learner. In addition,
Kurt Miller helped us understand the IMU and 900
MHz radios.

Figure 10: Best performance from the controller with
the real-world system. The car entered the orbit at
upper-left, and did 1.5 circles before sliding out of
control in the center of the orbit.

5


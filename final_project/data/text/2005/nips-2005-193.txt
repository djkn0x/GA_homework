Soft Clustering on Graphs

Kai Yu1 , Shipeng Yu2 , Volker Tresp1
1Siemens AG, Corporate Technology
2 Institute for Computer Science, University of Munich
kai.yu@siemens.com, volker.tresp@siemens.com
spyu@dbs.informatik.uni-muenchen.de

Abstract

We propose a simple clustering framework on graphs encoding pairwise
data similarities. Unlike usual similarity-based methods, the approach
softly assigns data to clusters in a probabilistic way. More importantly,
a hierarchical clustering is naturally derived in this framework to grad-
ually merge lower-level clusters into higher-level ones. A random walk
analysis indicates that the algorithm exposes clustering structures in var-
ious resolutions, i.e., a higher level statistically models a longer-term dif-
fusion on graphs and thus discovers a more global clustering structure.
Finally we provide very encouraging experimental results.

1 Introduction

Clustering has been widely applied in data analysis to group similar objects. Many algo-
rithms are either similarity-based or model-based. In general, the former (e.g., normalized
cut [5]) requires no assumption on data densities but simply a similarity function, and
usually partitions data exclusively into clusters. In contrast, model-based methods apply
mixture models to ﬁt data distributions and assign data to clusters (i.e. mixture components)
probabilistically. This soft clustering is often desired, as it encodes uncertainties on data-
to-cluster assignments. However, their density assumptions can sometimes be restrictive,
e.g. clusters have to be Gaussian-like in Gaussian mixture models (GMMs).

In contrast to ﬂat clustering, hierarchical clustering makes intuitive senses by forming a
tree of clusters. Despite of its wide applications, the technique is usually achieved by
heuristics (e.g., single link) and lacks theoretical backup. Only a few principled algorithms
exist so far, where a Gaussian or a sphere-shape assumption is often made [3, 1, 2].

This paper suggests a novel graph-factorization clustering (GFC) framework that employs
data’s afﬁnities and meanwhile partitions data probabilistically. A hierarchical clustering
algorithm (HGFC) is further derived by merging lower-level clusters into higher-level ones.
Analysis based on graph random walks suggests that our clustering method models data
afﬁnities as empirical transitions generated by a mixture of latent factors. This view sig-
niﬁcantly differs from conventional model-based clustering since here the mixture model
is not directly for data objects but for their relations. Clusters with arbitrary shapes can be
modeled by our method since only pairwise similarities are considered. Interestingly, we
prove that the higher-level clusters are associated with longer-term diffusive transitions on
the graph, amounting to smoother and more global similarity functions on the data mani-

fold. Therefore, the cluster hierarchy exposes the observed afﬁnity structure gradually in
different resolutions, which is somehow similar to the wavelet method that analyzes sig-
nals in different bandwidths. To the best of our knowledge, this property has never been
considered by other agglomerative hierarchical clustering algorithms (e.g., see [3]).

The paper is organized as follows. In the following section we describe a clustering al-
gorithm based on similarity graphs. In Sec. 3 we generalize the algorithm to hierarchical
clustering, followed by a discussion from the random walk point of view in Sec. 4. Finally
we present the experimental results in Sec. 5 and conclude the paper in Sec. 6.

2 Graph-factorization clustering (GFC)

Data similarity relations can be conveniently encoded by a graph, where vertices denote
data objects and adjacency weights represent data similarities. This section introduces
graph factorization clustering, which is a probabilistic partition of graph vertices. For-
mally, let G(V, E) be a weighted undirected graph with vertices V = {vi }n
i=1 and edges
E ⊆ {(vi , vj )}. Let W = {wij } be the adjacency matrix, where wij = wj i , wij > 0
if (vi , vj ) ∈ E and wij = 0 otherwise. For instances, wij can be computed by the RBF
similarity function based on the features of objects i and j , or by a binary indicator (0 or 1)
of the k-nearest neighbor afﬁnity.

2.1 Bipartite graphs

=

(1)

, Λ = diag(λ1 , . . . , λm )

Before presenting the main idea,
it is necessary to introduce bipartite graphs. Let
K (V, U, F) be the bipartite graph (e.g., Fig. 1 –( b)), where V = {vi }n
i=1 and U =
{up}m
p=1 are the two disjoint vertex sets and F contains all the edges connecting V and
U. Let B = {bip } denote the n × m adjacency matrix with bip ≥ 0 being the weight for
(cid:16)
BΛ−1B>(cid:17)
mX
edge [vi , up ]. The bipartite graph K induces a similarity between v1 and vj [6]
bip bjp
wij =
where λp = Pn
λp
ij
p=1
i=1 bip denotes the degree of vertex up ∈ U. We can interpret Eq. (1)
by p(vi , vj ). Without loss of generality, we normalize W to ensure P
from the perspective of Markov random walks on graphs. wij is essentially a quantity
proportional to the stationary probability of direct transitions between vi and vj , denoted
ij wij = 1 and
wij = p(vi , vj ). For a bipartite graph K (V, U, F), there is no direct links between vertices
X
p(up |vi )p(vj |up ) = X
in V, and all the paths from vi to vj must go through vertices in U. This indicates
p(vi , up )p(up , vj )
p(vi , vj ) = p(vi )p(vj |vi ) = di
λp
p
p
where p(vj |vi ) is the conditional transition probability from vi to vj , and di = p(vi ) the
degree of vi . This directly leads to Eq. (1) with bip = p(vi , up ).

,

2.2 Graph factorization by bipartite graph construction
For a bipartite graph K , p(up |vi ) = bip /di tells the conditional probability of transitions
from vi to up . If the size of U is smaller than that of V, namely m < n, then p(up |vi )
indicates how likely data point i belongs to vertex p. This property suggests that one can
construct a bipartite graph K (V, U, F) to approximate a given G(V, E), and then obtain
a soft clustering structure, where U corresponds to clusters (see Fig. 1–( a) (b)).

(a)

(b)

(c)

Figure 1: (a) The original graph representing data afﬁnities; ( b) The bipartite graph repre-
senting data-to-cluster relations; (c) The induced cluster afﬁnities.

‘

,

,

(2)

s. t.

, Λ ∈ Dm×m
+

hip = 1, H ∈ Rn×m
+

Eq. (1) suggests that this approximation can be done by minimizing ‘(W, BΛ−1B> ),
given a distance ‘(·, ·) between two adjacency matrices. To make the problem easy to
solve, we remove the coupling between B and Λ via H = BΛ−1 and then have
(cid:16)
W, HΛH>(cid:17)
nX
min
H,Λ
i=1
denotes the set of m × m diagonal matrices with positive diagonal entries.
where Dm×m
+
This problem is a symmetric variant of non-negative matrix factorization [4]. In this paper
Theorem 2.1. For divergence distance ‘(X, Y) = P
we focus on the divergence distance between matrices. The following theorem suggests an
alternating optimization approach to ﬁnd a local minimum:
− xij + yij ), the cost
ij (xij log xij
X
X
yij
function in Eq. (2) is non-increasing under the update rule ( ˜· denote updated quantities)
˜hip ∝ hip
wij
˜hip = 1;
X
X
˜λp = X
(HΛH> )ij
j
i
˜λp ∝ λp
wij
normalize s.t.
hiphjp ,
wij .
(HΛH> )ij
p
ij
ij
See Appendix for all the proofs in this paper. Similar to GMM, p(up |vi ) = bip / P
The distance is invariant under the update if and only if H and Λ are at a stationary point.
q biq is
the soft probabilistic assignment of vertex vi to cluster up . The method can be seen as a
counterpart of mixture models on graphs. The time complexity is O(m2N ) with N being
the number of nonzero entries in W. This can be very efﬁcient if W is sparse (e.g., for
k-nearest neighbor graph the complexity O(m2nk) scales linearly with sample size n).

normalize s.t.

λphjp ,

(3)

(4)

3 Hierarchical graph-factorization clustering (HGFC)
As a nice property of the proposed graph factorization, a natural afﬁnity between two clus-
(cid:17)
(cid:16)
nX
ters up and uq can be computed as
bip biq
di
pq
i=1
This is similar to Eq. (1), but derived from another way of two-hop transitions U → V →
U. Note that the similarity between clusters p and q takes into account a weighted average
of contributions from all the data (see Fig. 1–( c)).

, D = diag(d1 , . . . , dn )

B>D−1B

p(up , uq ) =

(5)

=

Let G0 (V0 , E0 ) be the initial graph describing the similarities of totally m0 = n
data points, with adjacency matrix W0 . Based on G0 we can build a bipartite graph
K1 (V0 , V1 , F1 ), with m1 < m0 vertices in V1 . A hierarchical clustering method can
be motivated from the observation that the cluster similarity in Eq. (5) suggests a new adja-
cency matrix W1 for graph G1 (V1 , E1 ), where V1 is formed by clusters, and E1 contains
edges connecting these clusters. Then we can group those clusters by constructing another
bipartite graph K2 (V1 , V2 , F2 ) with m2 < m1 vertices in V2 , such that W1 is again fac-
torized as in Eq. (2), and a new graph G2 (V2 , E2 ) can be built. In principal we can repeat
this procedure until we get only one cluster. Algorithm 1 summarizes this algorithm.

Algorithm 1 Hierarchical Graph-Factorization Clustering (HGFC)
Require: given n data objects and a similarity measure
1: build the similarity graph G0 (V0 , E0 ) with adjacency matrix W0 , and let m0 = n
2: for l = 1, 2, . . . , do
choose ml < ml−1
3:
factorize Gl−1 to obtain Kl (Vl−1 , Vl , Fl ) with the adjacency matrix Bl
4:
build a graph Gl (Vl , El ) with the adjacency matrix Wl = B>
l D−1
l Bl , where Dl ’s
5:
diagonal entries are obtained by summation over Bl ’s columns
6: end for

The algorithm ends up with a hierarchical clustering structure. For level l, we can assign
data to the obtained ml clusters via a propagation from the bottom level of clusters. Based
on the chain rule of Markov random walks, the soft (i.e., probabilistic) assignment of vi ∈
(cid:16)
(cid:17)
(cid:16)
(cid:16)
p |v (l−1)(cid:17) · · · p
(cid:17)
= X
· · · X
(cid:1)
= (cid:0)D−1
p ∈ Vl is given by
V0 to cluster v (l)
p |vi
v (l)
v (l)
1
v(l−1)∈Vl−1
v(1)∈V1
where ¯Bl = B1D−1
3 B3 . . . D−1
2 B2D−1
l Bl . One can interpret this by deriving an equiv-
alent bipartite graph ¯Kl (V0 , Vl , ¯Fl ), and treating ¯Bl as the equivalent adjacency matrix
attached to the equivalent edges ¯Fl connecting data V0 and clusters Vl .

v (1) |vi

¯Bl

ip ,

(6)

p

p

4 Analysis of the proposed algorithms

4.1 Flat clustering: statistical modeling of single-hop transitions

In this section we provide some insights to the suggested clustering algorithm, mainly
from the perspective of random walks on graphs. Suppose that from a stationary stage of
random walks on G(V, E), one observes πij single-hop transitions between vi and vj in a
unitary time frame. As an intuition of graph-based view to similarities, if two data points
are similar or related, the transitions between them are likely to happen. Thus we connect
the observed similarities to the frequency of transitions via wij ∝ πij . If the observed
transitions are i.i.d. sampled from a true distribution p(vi , vj ) = (HΛH> )ij where a
L(H, Λ) = log Y
p(vi , vj )πij ∝ X
bipartite graph is behind, then the log likelihood with respect to the observed transitions is
wij log(HΛH> )ij .
ij
ij
Then we have the following conclusion
Proposition 4.1. For a weighted undirected graph G(V, E) and the log likelihood deﬁned
in Eq. (7), the following results hold: (i) Minimizing the divergence distance l(W, HΛH> )
is equivalent to maximizing the log likelihood L(H, Λ); (ii) Updates Eq. (3) and Eq. (4)
correspond to a standard EM algorithm for maximizing L(H, Λ).

(7)

Figure 2: The similarities of vertices to a ﬁxed vertex (marked in the left panel) on a 6-
nearest-neighbor graph, respectively induced by clustering level l = 2 (the middle panel)
and l = 6 (the right panel). A darker color means a higher similarity.

4.2 Hierarchical clustering: statistical modeling of multi-hop transitions

The adjacency matrix W0 of G0 (V0 , E0 ) only models one-hop transitions that follow
direct links from vertices to their neighbors. However, the random walk is a process of
diffusion on the graph. Within a relatively longer period, a walker starting from a vertex
has the chance to reach vertices faraway through multi-hop transitions. Obviously, multi-
hop transitions induce a slowly decaying similarity function on the graph. Based on the
chain rule of Markov process, the equivalent adjacency matrix for t-hop transitions is
At = W0 (D−1
0 W0 )t−1 = At−1D−1
0 W0 .
(8)
Generally speaking, a slowly decaying similarity function on the similarity graph captures
a global afﬁnity structure of data manifolds, while a rapidly decaying similarity function
only tells the local afﬁnity structure. The following proposition states that in the sug-
gested HGFC, a higher-level clustering implicitly employs a more global similarity mea-
sure caused by multi-hop Markov random walks:
Proposition 4.2. For a given hierarchical clustering structure that starts from a bottom
graph G0 (V0 , E0 ) to a higher level Gk (Vk , Ek ), the vertices Vl at level 0 < l ≤ k
induces an equivalent adjacency matrix of V0 , which is At with t = 2l−1 as deﬁned in
Eq. (8).

Therefore the presented hierarchical clustering algorithm HGFC applies different sizes of
time windows to examine random walks, and derives different scales of similarity measures
to expose the local and global clustering structures of data manifolds. Fig. 2 illustrates the
l = 2 and 6, which
employed similarities of vertices to a ﬁxed vertex in clustering levels
corresponds to time periods t = 2 and 32. It can be seen that for a short period t = 2,
the similarity is very local and helps to uncover low-level clusters, while in a longer period
t = 32 the similarity function is rather global.

5 Empirical study

We apply HGFC on USPS handwritten digits and Newsgroup text data. For USPS data we
use the images of digits 1, 2, 3 and 4, with respectively 1269, 929, 824 and 852 images per
class. Each image is represented as a 256-dimension vector. The text data contain totally
3970 documents covering 4 categories, autos, motorcycles, baseball, and hockey. Each
document is represented by an 8014-dimension TFIDF feature vector. Our method employs
a 10-nearest-neighbor graph, with the similarity measure RBF for USPS and cosine for
Newsgroup. We perform 4-level HGFC, and set the cluster number, respectively from
bottom to top, to be 100, 20, 10 and 4 for both data sets.

We compare HGFC with two popular agglomerative hierarchical clustering algorithms, sin-
gle link and complete link (e.g., [3]). Both methods merge two closest clusters at each step.

Figure 3: Visualization of HGFC for USPS data set. Left: mean images of the top 3 clus-
tering levels, along with a Hinton graph representing the soft (probabilistic) assignments of
randomly chosen 10 digits (shown on the left) to the top 3rd level clusters; Middle: a Hin-
ton graph showing the soft cluster assignments from top 3rd level to top 2nd level; Right:
a Hinton graph showing the soft assignments from top 2nd level to top 1st level.

Figure 4: Comparison of clustering methods on USPS (left) and Newsgroup (right), evalu-
ated by normalized mutual information (NMI). Higher values indicate better qualities.

Single link deﬁnes the cluster distance to be the smallest point-wise distance between two
clusters, while complete link uses the largest one. A third compared method is normalized
cut [5], which partitions data into two clusters. We apply the algorithm recursively to pro-
duce a top-down hierarchy of 2, 4, 8, 16, 32 and 64 clusters. We also compare with the
k-means algorithm, k = 4, 10, 20 and 100.
Before showing the comparison, we visualize a part of clustering results for USPS data
in Fig. 3. On top of the left ﬁgure, we show the top three levels of the hierarchy with
respectively 4, 10 and 20 clusters, where each cluster is represented by its mean image via
an average over all the images weighted by their posterior probabilities of belonging to this
cluster. Then 10 randomly sampled digits with soft cluster assignments to the top 3rd level
clusters are illustrated with a Hinton graph. The middle and right ﬁgures in Fig. 3 show
the assignments between clusters across the hierarchy. The clear diagonal block structure
in all the Hinton graphs indicates a very meaningful cluster hierarchy.

Normalized cut
1
630
744
4
817
1
6
1

3
179
4
835

635
2
2
10

1254
1
1
4

HGFC
3
886
4
8

8
33
816
2

4
9
3
838

1265
17
10
58

K-means
0
1
95
720
796
9
20
0

3
97
9
774

“1”
“2”
“3”
“4”

Table 1: Confusion matrices of clustering results, 4 clusters, USPS data. In each confusion
matrix, rows correspond true classes and columns correspond the found clusters.

autos
motor.
baseball
hockey

Normalized cut
30
98
858
16
893
79
44
33
875
893
8
11

2
5
40
85

772
42
15
7

HGFC
13
182
934
5
843
33
11
21

21
12
101
958

977
985
39
16

K-means
4
7
5
3
835
114
900
4

0
0
4
77

Table 2: Confusion matrices of clustering results, 4 clusters, Newsgroup data.
In each
confusion matrix, rows correspond true classes and columns correspond the found clusters.

We compare the clustering methods by evaluating the normalized mutual information
(NMI) in Fig. 4.
It is deﬁned to be the mutual information between clusters and true
classes, normalized by the maximum of marginal entropies. Moreover, in order to more
directly assess the clustering quality, we also illustrate the confusion matrices in Table 1
and Table 2, in the case of producing 4 clusters. We drop out the confusion matrices of
single link and complete link in the tables, for saving spaces and also due to their clearly
poor performance compared with others.

The results show that single link performs poorly, as it greedily merges nearby data and
tends to form a big cluster with some outliers. Complete link is more balanced but unsat-
isfactory either. For the Newsgroup data it even gets stuck at the 3601-th merge because
all the similarities between clusters are 0. Top-down hierarchical normalized cut obtains
reasonable results, but sometimes cannot split one big cluster (see the tables). The con-
fusion matrices indicates that k-means does well for digit images but relatively worse for
high-dimension textual data. In contrast, Fig. 4 shows that HGFC gives signiﬁcantly higher
NMI values than competitors on both tasks. It also produces confusion matrices with clear
diagonal structures (see tables 1 and 2), which indicates a very good clustering quality.

6 Conclusion and Future Work

In this paper we have proposed a probabilistic graph partition method for clustering data ob-
jects based on their pairwise similarities. A novel hierarchical clustering algorithm HGFC
has been derived, where a higher level in HGFC corresponds to a statistical model of ran-
dom walk transitions in a longer period, giving rise to a more global clustering structure.
Experiments show very encouraging results.

In this paper we have empirically speciﬁed the number of clusters in each level. In the
near future we plan to investigate effective methods to automatically determine it. Another
direction is hierarchical clustering on directed graphs, as well as its applications in web
mining.

ij wij under constraints P
Proof of Theorem 2.1. We ﬁrst notice that P
p λp = P
Appendix
fore we can normalize W by P
the solution. Under this assumption we are maximizing L(H, Λ) = P
i hip = 1. There-
an extra constraint P
ij wij and after convergence multiply all λp by this quantity to get
ij wij log(HΛH> )ij with
λp and show update Eq. (3) will not decrease L(H) ≡
p λp = 1. We ﬁrst ﬁx
L(H, Λ). We prove this by constructing an auxiliary function f (H, H∗ ) such that f (H, H∗ ) ≤
(cid:16)
(cid:17)
L(H) and f (H, H) = L(H). Then we know the update Ht+1 = arg maxH f (H, Ht ) will not
P
P
decrease L(H) since L(Ht+1 ) ≥ f (Ht+1 , Ht ) ≥ f (Ht , Ht ) = L(Ht ). De ﬁne f (H, H∗ ) =
P
P
ip λp h∗
ip λp h∗
h∗
h∗
log hipλphjp − log
. f (H, H) = L(H) can be easily veri-
jp
jp
ij wij
il λl h∗
il λl h∗
l h∗
l h∗
p
ﬁed, and f (H, H∗ ) ≤ L(H) also follows if we use concavity of log function. Then it is straightfor-
j l
j l
ward to verify Eq. (3) by setting the derivative of f with respect to hip to be zero. The normalization
is due to the constraints and can be formally derived from this procedure with a Lagrange formalism.
Similarly we can deﬁne an auxiliary function for Λ with H ﬁxed, and verify Eq. (4).

Proof of Proposition 4.1. (i) follows directly from the proof of Theorem 2.1. To prove (ii) we take
up as the missing data and follow the standard way to derive the EM algorithm. In the E-step we esti-
mate the a posteriori probability of taking up for pair (vi , vj ) using Bayes’ rule: ˆp(up |vi , vj ) ∝
ˆL(G) = P
P
p(vi |up )p(vj |up )p(up ). And then in the M-step we maximize the “complete” data likelihood
i hip = 1 and P
hip = p(vi |up ) and λp = p(up ), with constraints P
p ˆp(up |vi , vj ) log p(vi |up )p(vj |up )p(up ) with respect to model parameters
j wij ˆp(up |vi , vj ) and λp ∝ P
responding derivatives to zero we obtain hip ∝ P
ij wij
p λp = 1. By setting the cor-
ij wij ˆp(up |vi , vj ).
It is easy to check that they are equivalent to updates Eq. (3) and Eq. (4) respectively.

Proof of Proposition 4.2. We give a brief proof. Suppose that at level l the data-cluster relationship
is described by ¯Kl (V0 , Vl , ¯Fl ) (see Eq. (6)) with adjacency matrix ¯Bl , degrees D0 for V0 , and
>
−1
degrees Λl for Vl . In this case the induced adjacency matrix of V0 is ¯Wl = ¯BlΛ
¯B
l , and
l
>
−1
the adjacency matrix of Vl is Wl = ¯B
¯Bl . Let Kl (Vl , Vl+1 , Fl+1 ) be the bipartite graph
l D
0
connecting Vl and Vl+1 , with the adjacency Bl+1 and degrees Λl+1 for Vl+1 . Then the adjacency
>
−1
−1
−1
−1
l+1B>
l = ¯WlD
¯B
¯Wl ,
matrix of V0 induced by level l + 1 is ¯Wl+1 = ¯BlΛ
l Bl+1Λ
l+1Λ
0
l
>
−1
−1
−1
l B>
l+1B>
l+1 = ¯B
¯Bl and ¯Wl = BlΛ
where relations Bl+1Λ
l are applied. Given the initial
l D
0
¯Wl = At with t = 2l−1 .
condition from the bottom level ¯W1 = W0 , it is not difﬁcult to obtain

References

[1] J. Goldberger and S. Roweis. Hierarchical clustering of a mixture model. In L.K. Saul,
Y. Weiss, and L. Bottou, editors, Neural Information Processing Systems 17 (NIPS*04),
pages 505–512, 2005.
[2] K.A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In Proceedings of
the 22nd International Conference on Machine Learning, pages 297–304, 2005.
[3] S. D. Kamvar, D. Klein, and C. D. Manning.
Interpreting and extending classical
agglomerative clustering algorithms using a model-based approach. In Proceedings of
the 19th International Conference on Machine Learning, pages 283 –290, 2002.
[4] Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factor-
ization.
In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural
Information Processing Systems 13 (NIPS*00), pages 556–562, 2001.
[5] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 22(8):888 –905, 2000.
[6] D. Zhou, B. Sch ¨olkopf, and T. Hofmann. Semi-supervised learning on directed graphs.
In L.K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17 (NIPS*04), pages 1633–1640, 2005.


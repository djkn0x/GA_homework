Value Function Approximation with Diffusion
Wavelets and Laplacian Eigenfunctions

Sridhar Mahadevan
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
mahadeva@cs.umass.edu

Mauro Maggioni
Program in Applied Mathematics
Department of Mathematics
Yale University
New Haven, CT 06511
mauro.maggioni@yale.edu

Abstract

We investigate the problem of automatically constructing efﬁcient rep-
resentations or basis functions for approximating value functions based
on analyzing the structure and topology of the state space. In particu-
lar, two novel approaches to value function approximation are explored
based on automatically constructing basis functions on state spaces that
can be represented as graphs or manifolds: one approach uses the eigen-
functions of the Laplacian, in effect performing a global Fourier analysis
on the graph; the second approach is based on diffusion wavelets, which
generalize classical wavelets to graphs using multiscale dilations induced
by powers of a diffusion operator or random walk on the graph. Together,
these approaches form the foundation of a new generation of methods for
solving large Markov decision processes, in which the underlying repre-
sentation and policies are simultaneously learned.

1 Introduction

Value function approximation (VFA) is a well-studied problem: a variety of linear and
nonlinear architectures have been studied, which are not automatically derived from the
geometry of the underlying state space, but rather handcoded in an ad hoc trial-and-error
process by a human designer [1]. A new framework for VFA called proto-reinforcement
learning (PRL) was recently proposed in [7, 8, 9]. Instead of learning task-speciﬁc value
functions using a handcoded parametric architecture, agents learn proto-value functions, or
global basis functions that re ﬂect intrinsic large-scale g eometric constraints that all value
functions on a manifold [11] or graph [3] adhere to, using spectral analysis of the self-
adjoint Laplace operator. This approach also yields new control learning algorithms called
representation policy iteration (RPI) where both the underlying representations (basis func-
tions) and policies are simultaneously learned. Laplacian eigenfunctions also provide ways
of automatically decomposing state spaces since they re ﬂec t bottlenecks and other global
geometric invariants.

In this paper, we extend the earlier Laplacian approach in a new direction using the recently
proposed diffusion wavelet transform (DWT), which is a compact multi-level representa-
tion of Markov diffusion processes on manifolds and graphs [4, 2]. Diffusion wavelets

provide an interesting alternative to global Fourier eigenfunctions for value function ap-
proximation, since they encapsulate all the traditional advantages of wavelets: basis func-
tions have compact support, and the representation is inherently hierarchical since it is
based on multi-resolution modeling of processes at different spatial and temporal scales.

2 Technical Background

This paper uses the framework of spectral graph theory [3] to build basis representations
for smooth (value) functions on graphs induced by Markov decision processes. Given
any graph G, an obvious but poor choice of representation is the “table- lookup ” orthonor-
mal encoding, where φ(i) = [0 . . . i . . . 0] is the encoding of the ith node in the graph.
This representation does not re ﬂect the topology of the spec iﬁc graph under considera-
tion. Polynomials are another popular choice of orthonormal basis functions [5], where
φ(s) = [1 s . . . sk ] for some ﬁxed k . This encoding has two disadvantages: it is numeri-
cally unstable for large graphs, and is dependent on the ordering of vertices. In this paper,
we outline a new approach to the problem of building basis functions on graphs using
Laplacian eigenfunctions and diffusion wavelets.

∗

ss0 + γ V ∗ (s0 ))
P a
ss0 (Ra

A ﬁnite Markov decision process (MDP) M = (S, A, P a
ss0 ) is de ﬁned as a ﬁnite set
ss0 , Ra
ss0 specifying the distribution over
of states S , a ﬁnite set of actions A, a transition model P a
future states s0 when an action a is performed in state s, and a corresponding reward model
ss0 specifying a scalar cost or reward [10]. A state value function is a mapping S → R
Ra
or equivalently a vector in R|S | . Given a policy π : S → A mapping states to actions,
its corresponding value function V π speciﬁes the expected long-term discounted sum of
rewards received by the agent in any given state s when actions are chosen using the policy.
Any optimal policy π∗ de ﬁnes the same unique optimal value function V ∗ which satisﬁes
the nonlinear constraints
V

a X
(s) = max
s0
For any MDP, any policy induces a Markov chain that partitions the states into classes:
transient states are visited initially but not after a ﬁnite time, and recurrent states are visited
in ﬁnitely often. In ergodic MDPs, the set of transient states is empty. The construction of
basis functions below assumes that the Markov chain induced by a policy is a reversible
random walk on the state space. While some policies may not induce such Markov chains,
the set of basis functions learned from a reversible random walk can still be useful in
approximating value functions for (reversible or non-reversible) policies. In other words,
the construction of the basis functions can be considered an off-policy method:
just as
in Q-learning where the exploration policy differs from the optimal learned policy, in the
proposed approach the actual MDP dynamics may induce a different Markov chain than the
one analyzed to build representations. Reversible random walks greatly simplify spectral
analysis since such random walks are similar to a symmetric operator on the state space.

2.1 Smooth Functions on Graphs and Value Function Representation

We assume the state space can be modeled as a ﬁnite undirected weighted graph (G, E , W ),
but the approach generalizes to Riemannian manifolds. We de ﬁne x ∼ y to mean an edge
between x and y , and the degree of x to be d(x) = Px∼y w(x, y ). D will denote the
diagonal matrix de ﬁned by Dxx = d(x), and W the matrix de ﬁned by Wxy = w(x, y ) =
2 = Px∈G |f (x)|2 d(x). The gradient
w(y , x). The L2 norm of a function on G is ||f ||2
of a function is ∇f (i, j ) = w(i, j )(f (i) − f (j )) if there is an edge e connecting i to j , 0
otherwise. The smoothness of a function on a graph, can be measured by the Sobolev norm
2 = X
|f (x)|2 d(x) + X
||f ||2
2 + ||∇f ||2
H2 = ||f ||2
|f (x) − f (y )|2w(x, y ) .
(1)
x∼y
x

The ﬁrst term in this norm controls the size (in terms of L2 -norm) for the function f , and
the second term controls the size of the gradient. The smaller ||f ||H2 , the smoother is f .
We will assume that the value functions we consider have small H2 norms, except at a
few points, where the gradient may be large. Important variations exist, corresponding to
different measures on the vertices and edges of G.

Classical techniques, such as value iteration and policy iteration [10], represent value func-
tions using an orthonormal basis (e1 , . . . , e|S | ) for the space R|S | [1]. For a ﬁxed precision
, a value function V π can be approximated as

||V π − X
i∈S ()

απ
i ei || ≤ 

with αi =< V π , ei > since the ei ’s are orthonormal, and the approximation is measured
in some norm, such as L2 or H2 . The goal is to obtain representations in which the index
set S () in the summation is as small as possible, for a given approximation error . This
hope is well founded at least when V π is smooth or piecewise smooth, since in this case it
should be compressible in some well chosen basis {ei}.

3 Function Approximation using Laplacian Eigenfunctions

The combinatorial Laplacian L [3] is de ﬁned as

Lf (x) = X
y∼x

w(x, y )(f (x) − f (y )) = (D − W )f .

Often one considers the normalized Laplacian L = D− 1
2 (D−W )D− 1
2 which has spectrum
in [0, 2]. This Laplacian is related to the notion of smoothness as above, since hf , Lf i =
2 , which should be compared
Px f (x) Lf (x) = Px,y w(x, y )(f (x) − f (y ))2 = ||∇f ||2
with (1). Functions that satisfy the equation Lf = 0 are called harmonic. The Spectral
Theorem can be applied to L (or L), yielding a discrete set of eigenvalues 0 ≤ λ0 ≤ λ1 ≤
. . . λi ≤ . . . and a corresponding orthonormal basis of eigenfunctions {ξi }i≥0 , solutions to
the eigenvalue problem Lξi = λi ξi .
The eigenfunctions of the Laplacian can be viewed as an orthonormal basis of global
Fourier smooth functions that can be used for approximating any value function on a
graph. These basis functions capture large-scale features of the state space, and are par-
ticularly sensitive to “bottlenecks”, a phenomenon widely
studied in Riemannian geometry
and spectral graph theory [3]. Observe that ξi satisﬁes
2 = λi . In fact, the varia-
||∇ξi ||2
tional characterization of eigenvectors shows that ξi is the normalized function orthogonal
to ξ0 , . . . , ξi−1 with minimal ||∇ξi ||2 . Hence the projection of a function f on S onto the
top k eigenvectors of the Laplacian is the smoothest approximation to f , in the sense of the
norm in H2 . A potential drawback of Laplacian approximation is that it detects only global
smoothness, and may poorly approximate a function which is not globally smooth but only
piecewise smooth, or with different smoothness in different regions. These drawbacks are
addressed in the context of analysis with diffusion wavelets, and in fact partly motivated
their construction.

4 Function Approximation using Diffusion Wavelets

Diffusion wavelets were introduced in [4, 2], in order to perform a fast multiscale analysis
of functions on a manifold or graph, generalizing wavelet analysis and associated signal
processing techniques (such as compression or denoising) to functions on manifolds and
graphs. They allow the fast and accurate computation of high powers of a Markov chain

DiffusionWaveletTree (H0 , Φ0 , J, ):

// H0 : symmetric conjugate to random walk matrix, represented on the basis Φ0
// Φ0 : initial basis (usually Dirac’s δ -function basis), one function per column
// J : number of levels to compute
// : precision

for j from 0 to J do,
1. Compute sparse factorization Hj ∼ Qj Rj , with Qj orthogonal.
and [H 2j
Φj+1
0 ]
Φj+1
3. Compute sparse factorization I − Φj+1Φ∗
j , with Q0
j orthogonal.
j+1 = Q0
j R0
4. Ψj+1 ← Q0
j .

2. Φj+1 ← Qj = Hj R−1
j

j .
∼j Hj+1 ← Rj R∗

end

Figure 1: Pseudo-code for constructing a Diffusion Wavelet Tree

P on the manifold or graph, including direct computation of the Green’s function (or fun-
damental matrix) of the Markov chain, (I − P )−1 , which can be used to solve Bellman’s
equation. Here, “fast” means that the number of operations r
equired is O(|S |), up to loga-
rithmic factors.

Space constraints permit only a brief description of the construction of diffusion wavelet
trees. More details are provided in [4, 2]. The input to the algorithm is a “precision ”
parameter  > 0, and a weighted graph (G, E , W ). We can assume that G is connected,
otherwise we can consider each connected component separately. The construction is based
on using the natural random walk P = D−1W on a graph and its powers to “dilate ”, or
“diffuse ” functions on the graph, and then de ﬁning an associ
ated coarse-graining of the
graph. We symmetrize P by conjugation and take powers to obtain

H t = D

(2)

1
2 W D− 1
2 = (D− 1
2 P tD− 1
2 )t = (I − L)t = X
i≥0
where {λi} and {ξi} are the eigenvalues and eigenfunctions of the Laplacian as above.
Hence the eigenfunctions of H t are again ξi and the ith eigenvalue is (1 − λi )t . We assume
that H 1 is a sparse matrix, and that the spectrum of H 1 has rapid decay.

(1 − λi )t ξi (·)ξi (·)

A diffusion wavelet tree consist of orthogonal diffusion scaling functions Φj that are
smooth bump functions, with some oscillations, at scale roughly 2j (measured with respect
to geodesic distance, for small j ), and orthogonal wavelets Ψj that are smooth localized os-
cillatory functions at the same scale. The scaling functions Φj span a subspace Vj , with the
property that Vj+1 ⊆ Vj , and the span of Ψj , Wj , is the orthogonal complement of Vj into
Vj+1 . This is achieved by using the dyadic powers H 2j as “dilations”, to create smoother
and wider (always in a geodesic sense) “bump ” functions (whi ch represent densities for the
symmetrized random walk after 2j steps), and orthogonalizing and downsampling appro-
priately to transform sets of “bumps” into orthonormal scal
ing functions.
Computationally (Figure 1), we start with the basis Φ0 = I and the matrix H0 := H 1 ,
sparse by assumption, and construct an orthonormal basis of well-localized functions for
its range (the space spanned by the columns), up to precision , through a variation of
the Gram-Schmidt orthonormalization scheme, described in [4]. In matrix form, this is a
sparse factorization H0 ∼ Q0R0 , with Q0 orthonormal. Notice that H0 is |G| × |G|,
but in general Q0 is |G| × |G(1) | and R0 is |G(1) | × |G|, with |G(1) | ≤ |G|.
In fact
|G(1) | is approximately equal to the number of singular values of H0 larger than . The

columns of Q0 are an orthonormal basis of scaling functions Φ1 for the range of H0 , written
0 on the basis Φ1 :
as a linear combination of the initial basis Φ0 . We can now write H 2
H1 := [H 2 ]Φ1
0 , where we used H0 = H ∗
0 . This is a compressed
= Q∗
0H0H0Q0 = R0R∗
Φ1
representation of H 2
0 acting on the range of H0 , and it is a |G(1) | × |G(1) | matrix. We
proceed by induction: at scale j we have an orthonormal basis Φj for the rank of H 2j −1
up to precision j , represented as a linear combination of elements in Φj−1 . This basis
contains |G(j) | functions, where |G(j) | is comparable with the number of eigenvalues λj of
H0 such that λ2j −1
≥ . We have the operator H 2j
0 represented on Φj by a |G(j) | × |G(j) |
j
matrix Hj , up to precision j . We compute a sparse decomposition of Hj ∼ Qj Rj , and
and represent H 2j+1 on this basis by the
obtain the next basis Φj+1 = Qj = Hj R−1
j
]Φj+1
matrix Hj+1 := [H 2j
j Hj Hj Qj = Rj R∗
= Q∗
j .
Φj+1
j+1 ,
Wavelet bases for the spaces Wj can be built analogously by factorizing IVj − Qj+1Q∗
which is the orthogonal projection on the complement of Vj+1 into Vj . The spaces can
be further split to obtain wavelet packets [2]. A Fast Diffusion Wavelet Transform al-
lows expanding in O(n) (where n is the number of vertices) computations any function
in the wavelet, or wavelet packet, basis, and efﬁciently sea rch for the most suitable basis
set. Diffusion wavelets and wavelet packets are a very efﬁci ent tool for representation and
approximation of functions on manifolds and graphs [4, 2], generalizing to these general
spaces the nice properties of wavelets that have been so successfully applied to similar tasks
in Euclidean spaces.

Diffusion wavelets allow computing H 2k
f for any ﬁxed f , in order O(kn). This is non-
trivial because while the matrix H is sparse, large powers of it are not, and the computation
H · H . . . · (H (H f )) . . .) involves 2k matrix-vector products. As a notable consequence,
this yields a fast algorithm for computing the Green’s function, or fundamental matrix,
associated with the Markov process H , via (I −H 1 )−1f = Pk≥0 H k = Qk≥0 (I +H 2k
)f .
In a similar way one can compute (I − P )−1 . For large classes of Markov chains we can
perform this computation in time O(n), in a direct (as opposed to iterative) fashion. This is
remarkable since in general the matrix (I − H 1)−1 is full and only writing down the entries
would take time O(n2 ). It is the multiscale compression scheme that allows to efﬁc iently
represent (I − H 1 )−1 in compress form, taking advantage of the smoothness of the entries
of the matrix. This is discussed in general in [4]. We use this approach to develop a faster
policy evaluation step for solving MDPs described in [6]

5 Experiments

Figure 2 contrasts Laplacian eigenfunctions and diffusion wavelet basis functions in a three
room grid world environment. Laplacian eigenfunctions were produced by solving Lf =
λf , where L is the combinatorial Laplacian, whereas diffusion wavelet basis functions
were produced using the algorithm described in Figure 1. The input to both methods is an
undirected graph, where edges connect states reachable through a single (reversible) action.
Such graphs can be easily learned from a sample of transitions, such as that generated by
RL agents while exploring the environment in early phases of policy learning. Note how
the intrinsic multi-room environment is re ﬂected in the Lap lacian eigenfunctions. The
Laplacian eigenfunctions are globally de ﬁned over the enti re state space, whereas diffusion
wavelet basis functions are progressively more compact at higher levels, beginning at the
lowest level with the table-lookup representation, and converging at the highest level to
basis functions similar to Laplacian eigenfunctions. Figure 3 compares the approximations
produced in a two-room grid world MDP with 630 states. These experiments illustrate
the superiority of diffusion wavelets: in the ﬁrst experime nt (top row), diffusion wavelets
handily outperform Laplacian eigenfunctions because the function is highly nonlinear near

Figure 2: Examples of Laplacian eigenfunctions (left) and diffusion wavelet basis functions
(right) computed using the graph Laplacian on a complete undirected graph of a determin-
istic grid world environment with reversible actions.

the goal, but mostly linear elsewhere. The eigenfunctions contain a lot of ripples in the ﬂat
region causing a large residual error. In the second experiment (bottom row), Laplacian
eigenfunctions work signiﬁcantly better because the value
function is globally smooth.
Even here, the superiority of diffusion wavelets is clear.

20

10

10

0

0

30

20

50

40

30

20

10

0
30

300

200

100

0

−100
30

20

10

20

10

0

0

50

40

30

20

10

0
30

300

200

100

0

−100

−200
30

20

10

10

0

0

30

20

20

10

20

10

0

0

10

5

0

−5
30

60

40

20

0

−20

−40

−60
30

30

30

20

10

20

10

0

0

20

10

20

10

0

0

WP
Eig

0

50

100

150

200

WP
Eig

2

1

0

−1

−2

−3

−4

−5

−6

−7

3

2.5

2

1.5

1

30

30

0.5

0

50

100

150

200

Figure 3: Left column: value functions in a two room grid world MDP, where each room
has 21 × 15 states connected by a door in the middle of the common wall. Middle two
columns: approximations produced by 5 diffusion wavelet bases and Laplacian eigenfunc-
tions. Right column: least-squares approximation error (log scale) using up to 200 basis
functions (bottom curve: diffusion wavelets; top curve: Laplacian eigenfunctions). In the
top row, the value function corresponds to a random walk. In the bottom row, the value
function corresponds to the optimal policy.

5.1 Control Learning using Representation Policy Iteration

This section describes results of using the automatically generated basis functions inside
a control learning algorithm, in particular the Representation Policy Iteration (RPI) al-
gorithm [8]. RPI is an approximate policy iteration algorithm where the basis functions

φ(s, a) handcoded in other methods, such as LSPI [5] are learned from a random walk of
transitions by computing the graph Laplacian and then computing the eigenfunctions or the
diffusion wavelet bases as described above. One striking property of the eigenfunction and
diffusion wavelet basis functions is their ability to re ﬂec t nonlinearities arising from “bot-
tlenecks” in the state space. Figure 4 contrasts the value fu nction approximation produced
by RPI using Laplacian eigenfunctions with that produced by a polynomial approximator.
The polynomial approximator yields a value function that is “blind ” to the nonlinearities
produced by the walls in the two room grid world MDP.

Figure 4: This ﬁgures compares the value functions produced by RPI using Laplacian
eigenfunctions with that produced by LSPI using a polynomial approximator in a two
room grid world MDP with a “bottleneck ” region representing
the door connecting the two
rooms. The Laplacian basis functions on the left clearly capture the nonlinearity arising
from the bottleneck, whereas the polynomial approximator on the right smooths the value
geometry of the environment.
function across the walls as it is “blind ” to the large-scale

Table 1 compares the performance of diffusion wavelets and Laplacian eigenfunctions us-
ing RPI on the classic chain MDP from [5]. Here, an initial random walk of 5000 steps
was carried out to generate the basis functions in a 50 state chain. The chain MDP is a
sequential open (or closed) chain of varying number of states, where there are two actions
for moving left or right along the chain. In the experiments shown, a reward of 1 was pro-
vided in states 10 and 41. Given a ﬁxed k , the encoding φ(s) of a state s for Laplacian
eigenfunctions is the vector comprised of the values of the k th lowest-order eigenfunctions
on state k . For diffusion wavelets, all the basis functions at level k were evaluated at state
s to produce the encoding.

Method
RPI DF (5)
RPI DF (14)
RPI DF (19)
RPI Lap (5)
RPI Lap (15)
RPI Lap (25)

#Trials Error
4.4
2.4
4.8
6.8
0.6
8.2
3.8
4.2
7.2
3
2
9.4

Method
LSPI RBF (6)
LSPI RBF (14)
LSPI RBF (26)
LSPI Poly (5)
LSPI Poly (15)
LSPI Poly (25)

#Trials Error
3.8
20.8
2.8
4.4
2.8
6.4
4
4.2
1
34.4
36
1

Table 1: This table compares the performance of RPI using diffusion wavelets and Lapla-
cian eigenfunctions with LSPI using handcoded polynomial and radial basis functions on a
50 state chain graph MDP.

Each row re ﬂects the performance of either RPI using learned basis functions or LSPI with
a handcoded basis function (values in parentheses indicate the number of basis functions
used for each architecture). The two numbers reported are steps to convergence and the
error in the learned policy (number of incorrect actions), averaged over 5 runs. Laplacian
and diffusion wavelet basis functions provide a more stable performance at both the low
end and at the higher end, as compared to the handcoded basis functions. As the number of

basis functions are increased, RPI with Laplacian basis functions takes longer to converge,
but learns a more accurate policy. Diffusion wavelets converge slower as the number of
basis functions is increased, giving the best results overall with 19 basis functions. Unlike
Laplacian eigenfunctions, the policy error is not monotonically decreasing as the number
of bases functions is increased. This result is being investigated. LSPI with RBF is unstable
at the low end, converging to a very poor policy for 6 basis functions. LSPI with a 5 degree
polynomial approximator works reasonably well, but its performance noticeably degrades
at higher degrees, converging to a very poor policy in one step for k = 15 and k = 25.

6 Future Work

We are exploring many extensions of this framework, including extensions to factored
MDPs, approximating action value functions as well as large state spaces by exploiting
symmetries de ﬁned by a group of automorphisms of the graph. T hese enhancements will
facilitate efﬁcient construction of eigenfunctions and di ffusion wavelets. For large state
spaces, one can randomly subsample the graph, construct the eigenfunctions of the Lapla-
cian or the diffusion wavelets on the subgraph, and then interpolate these functions using
the Nystr ¨om approximation and related low-rank linear algebraic methods. In experiments
on the classic inverted pendulum control task, the Nystr ¨om approximation yielded excel-
lent results compared to radial basis functions, learning a more stable policy with a smaller
number of samples.

Acknowledgements

This research was supported in part by a grant from the National Science Foundation IIS-
0534999.

References

[1] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scienti ﬁc, Belmont,
Massachusetts, 1996.
[2] J. Bremer, R. Coifman, M. Maggioni, and A. Szlam. Diffusion wavelet packets. Technical
Report Tech. Rep. YALE/DCS/TR-1304, Yale University, 2004.
to appear in Appl. Comp.
Harm. Anal.
[3] F. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[4] R. Coifman and M Maggioni. Diffusion wavelets. Technical Report Tech. Rep. YALE/DCS/TR-
1303, Yale University, 2004. to appear in Appl. Comp. Harm. Anal.
[5] M. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Re-
search, 4:1107–1149, 2003.
[6] M. Maggioni and S. Mahadevan. Fast direct policy evaluation using multiscale Markov Diffu-
sion Processes. Technical Report Tech. Rep.TR-2005-39, University of Massachusetts, 2005.
[7] S. Mahadevan. Proto-value functions: Developmental reinforcement learning. In Proceedings
of the 22nd International Conference on Machine Learning, 2005.
[8] S. Mahadevan. Representation policy iteration. In Proceedings of the 21st International Con-
ference on Uncertainty in Arti ﬁcial Intelligence , 2005.
[9] S. Mahadevan. Samuel meets Amarel: Automating value function approximation using global
state space analysis. In National Conference on Arti ﬁcial Intelligence (AAAI) , 2005.
[10] M. L. Puterman. Markov decision processes. Wiley Interscience, New York, USA, 1994.
[11] S Rosenberg. The Laplacian on a Riemannian Manifold. Cambridge University Press, 1997.


Size Regularized Cut for Data Clustering

Yixin Chen
Department of CS
Univ. of New Orleans
yixin@cs.uno.edu

Ya Zhang
Department of EECS
Uinv. of Kansas
yazhang@ittc.ku.edu

Xiang Ji
NEC-Labs America, Inc.
xji@sv.nec-labs.com

Abstract

We present a novel spectral clustering method that enables users to incor-
porate prior knowledge of the size of clusters into the clustering process.
The cost function, which is named size regularized cut (SRcut), is deﬁned
as the sum of the inter-cluster similarity and a regularization term mea-
suring the relative size of two clusters. Finding a partition of the data set
to minimize SRcut is proved to be NP-complete. An approximation algo-
rithm is proposed to solve a relaxed version of the optimization problem
as an eigenvalue problem. Evaluations over different data sets demon-
strate that the method is not sensitive to outliers and performs better than
normalized cut.

1

Introduction

In recent years, spectral clustering based on graph partitioning theories has emerged as
one of the most effective data clustering tools. These methods model the given data set
as a weighted undirected graph. Each data instance is represented as a node. Each edge
is assigned a weight describing the similarity between the two nodes connected by the
edge. Clustering is then accomplished by ﬁnding the best cuts of the graph that optimize
certain predeﬁned cost functions. The optimization usually leads to the computation of the
top eigenvectors of certain graph afﬁnity matrices, and the clustering result can be derived
from the obtained eigen-space [12, 6]. Many cost functions, such as the ratio cut [3],
average association [15], spectral k-means [19], normalized cut [15], min-max cut [7], and
a measure using conductance and cut [9] have been proposed along with the corresponding
eigen-systems for the data clustering purpose.

The above data clustering methods, as well as most other methods in the literature, bear a
common characteristic that manages to generate results maximizing the intra-cluster sim-
ilarity, and/or minimizing the inter-cluster similarity. These approaches perform well in
some cases, but fail drastically when target data sets possess complex, extreme data distri-
butions, and when the user has special needs for the data clustering task. For example, it
has been pointed out by several researchers that normalized cut sometimes displays sensi-
tivity to outliers [7, 14]. Normalized cut tends to ﬁnd a cluster consisting of a very small
number of points if those points are far away from the center of the data set [14].

There has been an abundance of prior work on embedding user’s prior knowledge of the
data set in the clustering process. Kernighan and Lin [11] applied a local search procedure
that maintained two equally sized clusters while trying to minimize the association between

the clusters. Wagstaff et al. [16] modiﬁed k-means method to deal with a priori knowledge
about must-link and cannot link constraints. Banerjee and Ghosh [2] proposed a method to
balance the size of the clusters by considering an explicit soft constraint. Xing et al. [17]
presented a method to learn a clustering metric over user speciﬁed samples. Yu and Shi [18]
introduced a method to include must-link grouping cues in normalized cut. Other related
works include leaving  fraction of the points unclustered to avoid the effect of outliers [4]
and enforcing minimum cluster size constraint [10].

In this paper, we present a novel clustering method based on graph partitioning. The new
method enables users to incorporate prior knowledge of the expected size of clusters into
the clustering process. Speciﬁcally, the cost function of the new method is deﬁned as the
sum of the inter-cluster similarity and a regularization term that measures the relative size
of two clusters. An “optimal” partition corresponds to a tradeoff between the inter-cluster
similarity and the relative size of two clusters. We show that the size of the clusters gener-
ated by the optimal partition can be controlled by adjusting the weight on the regularization
term. We also prove that the optimization problem is NP-complete. So we present an ap-
proximation algorithm and demonstrate its performance using two document data sets.

2 Size regularized cut
We model a given data set using a weighted undirected graph G = G(V , E , W) where V ,
E , and W denote the vertex set, edge set, and graph afﬁnity matrix, respectively. Each
vertex i ∈ V represents a data point, and each edge (i, j ) ∈ E is assigned a nonnegative
weight Wij to reﬂect the similarity between the data points
i and j . A graph partitioning
method attempts to organize vertices into groups so that the intra-cluster similarity is high,
cut(V1 , V2 ) = X
and/or the inter-cluster similarity is low. A simple way to quantify the cost for partitioning
vertices into two disjoint sets V1 and V2 is the cut size
Wij ,
i∈V1 ,j∈V2
which can be viewed as the similarity or association between V1 and V2 . Finding a binary
partition of the graph that minimizes the cut size is known as the minimum cut problem.
There exist efﬁcient algorithms for solving this problem. However, the minimum cut crite-
rion favors grouping small sets of isolated nodes in the graph [15].
To capture the need for more balanced clusters, it has been proposed to include the cluster
(cid:19)
(cid:18) 1
size information as a multiplicative penalty factor in the cost function, such as average
cut [3] and normalized cut [15]. Both cost functions can be uniformly written as [5]
1
cost(V1 , V2 ) = cut(V1 , V2 )
+
(1)
|V1 |β
|V1 |β
.
Here, β = [β1 , · · · , βN ]T is a weight vector where βi is a nonnegative weight associated
with vertex i, and N is the total number of vertices in V . The penalty factor for “unbalanced
|Vj |β = X
partition” is determined by |Vj |β (j = 1, 2), which is a weighted cardinality (or weighted
size) of Vj , i.e.,
βi = P
i∈Vj
Dhillon [5] showed that if βi = 1 (for all i), the cost function (1) becomes average cut. If
j Wij , then (1) turns out to be normalized cut.
In contrast with minimum cut, average cut and normalized cut tend to generate more bal-
anced clusters. However, due to the multiplicative nature of their cost functions, average
cut and normalized cut are still sensitive to outliers. This is because the cut value for sep-
arating outliers from the rest of the data points is usually close to zero, and thus makes

βi .

(2)

the multiplicative penalty factor void. To avoid the drawback of the above multiplicative
cost functions, we introduce an additive cost function for graph bi-partitioning. The cost
function is named size regularized cut (SRcut), and is deﬁned as
SRcut(V1 , V2 ) = cut(V1 , V2 ) − α|V1 |β |V2 |β
(3)
where |Vj |β (j = 1, 2) is described in (2), β and α > 0 are given a priori. The last term in
(3), α|V1 |β |V2 |β , is the size regularization term, which can be interpreted as below.
(cid:17)2
show that the following inequality |V1 |β |V2 |β ≤ (cid:16) βT e
Since |V1 |β + |V2 |β = |V |β = βT e where e is a vector of 1’s, it is straightforward to
holds for arbitrary V1 , V2 ∈ V
2
satisfying V1 ∪ V2 = V and V1 ∩ V2 = ∅. In addition, the equality holds if and only if
|V1 |β = |V2 |β = βT e
2 .
|V1 |β |V2 |β achieves the maximum value when two clusters are of equal
Therefore,
weighted size. Consequently, minimizing SRcut is equivalent to minimizing the similar-
ity between two clusters and, at the same time, searching for a balanced partition. The
tradeoff between the inter-cluster similarity and the balance of the cut depends on the α
parameter, which needs to be determined by the prior information on the size of clusters. If
α = 0, minimum SRcut will assign all vertices to one cluster. On the other end, if α (cid:29) 0,
minimum SRcut will generate two clusters of equal size (if N is an even number). We
defer the discussion on the choice of α to Section 5.
SRassoc(V1 , V2 ) = X
In a spirit similar to that of (3), we can de ﬁne size regularized association (SRassoc) as
cut(Vi , Vi ) + 2α|V1 |β |V2 |β
i=1,2
where cut(Vi , Vi ) measures the intra-cluster similarity. An important property of SRassoc
and SRcut is that they are naturally related:
cut(V , V ) − SRassoc(V1 , V2 )
SRcut(V1 , V2 ) =
2
Hence, minimizing size regularized cut is in fact identical to maximizing size regularized
association. In other words, minimizing the size regularized inter-cluster similarity is equiv-
alent to maximizing the size regularized intra-cluster similarity. In this paper, we will use
SRcut as the clustering criterion.

.

3 Size ratio monotonicity
Let V1 and V2 be a partition of V . The size ratio r = min(|V1 |β ,|V2 |β )
max(|V1 |β ,|V2 |β ) de ﬁnes the relative
size of two clusters. It is always within the interval [0, 1], and a larger value indicates a
more balanced partition. The following theorem shows that by controlling the parameter α
in the SRcut cost function, one can control the balance of the optimal partition. In addition,
the size ratio increases monotonically as the increase of α.
Theorem 3.1 (Size Ratio Monotonicity) Let V i
1 and V i
2 be the clusters generated by the
minimum SRcut with α = αi , and the corresponding size ratio, ri , be de ﬁned as
min(|V i
1 |β , |V i
2 |β )
1 |β , |V i
max(|V i
2 |β ) .

ri =

If α1 > α2 ≥ 0, then r1 ≥ r2 .

Proof: Given vertex weight vector β , let S be the collection of all distinct values that the
size regularization term in (3) can have, i.e.,
S = {S | V1 ∪ V2 = V , V1 ∩ V2 = ∅, S = |V1 |β |V2 |β } .
Clearly, |S |, the number of elements in S , is less than or equal to 2N −1 where N is the size
(cid:19)2
(cid:18) βT e
of V . Hence we can write the elements in S in ascending order as
0 = S1 < S2 < · · · · · · < S|S | ≤
.
2
Next, we deﬁne cuti be the minimal cut satisfying |V1 |β |V2 |β = Si , i.e.,
cut(V1 , V2 ) ,
min
cuti =
|V1 |β |V2 |β = Si
V1 ∪ V2 = V
V1 ∩ V2 = ∅

then

(cuti − αSi ) .

SRcut(V1 , V2 ) = min
i=1,···,|S |

min
V1 ∪ V2 = V
V1 ∩ V2 = ∅
If V 2
1 and V 2
2 are the clusters generated by the minimum SRcut with α = α2 , then
2 |β = Sk∗ where k∗ = argmini=1,···,|S |
|V 2
(cuti − α2Si ). Therefore, for any
1 |β |V 2
1 ≤ t < k∗ ,
cutk∗ − α2Sk∗ ≤ cutt − α2St .
If α1 > α2 , we have
(α2 − α1 )Sk∗ < (α2 − α1 )St .
Adding (4) and (5) gives cutk∗ − α1Sk∗ < cutt − α1St , which implies
k∗ ≤ argmini=1,···,|S | (cuti − α1Si ) .
(6)
1 and V 1
Now, let V 1
2 be the clusters generated by the minimum SRcut with α = α1 , and
1 |β |V 1
2 |β = Sj ∗ where j ∗ = argmini=1,···,|S | (cuti − α1Si ). From (6) we have j ∗ ≥
|V 1
1 |β |V 2
2 |β . Without loss of
2 |β ≥ |V 2
k∗ , therefore Sj ∗ ≥ Sk∗ , or equivalently |V 1
1 |β |V 1
1 |β ≤ |V |β
generality, we can assume that |V 1
1 |β ≤ |V 1
2 |β and |V 2
1 |β ≤ |V 2
2 |β , therefore |V 1
2
1 |β ≤ |V |β
2 . Considering the fact that f (x) = x(|V |β − x) is strictly monotonically
and |V 2
increasing as x ≤ |V |β
1 |β . This leads to
and f (|V 1
1 |β ) ≥ f (|V 2
1 |β ), we have |V 1
1 |β ≥ |V 2
2
r1 = |V 1
≥ r2 = |V 2
1 |β
1 |β
(cid:3)
.
|V 1
2 |β
|V 2
2 |β
Unfortunately, minimizing size regularized cut for an arbitrary α is an NP-complete prob-
lem. This is proved in the following section.

(4)

(5)

4 Size regularized cut and graph bisection

The decision problem for minimum SRcut can be formulated as: whether, given an undi-
rected graph G(V , E , W) with weight vector β and regularization parameter α, a partition
exists such that SRcut is less than a given cost. This decision problem is clearly NP be-
cause we can verify in polynomial time the SRcut value for a given partition. Next we show
that graph bisection can be reduced, in polynomial time, to minimum SRcut. Since graph
bisection is a classi ﬁed NP-complete problem [1], so is minimum SRcut.
Deﬁnition 4.1 (Graph Bisection) Given an undirected graph G = G(V , E , W) with even
number of vertices where W is the adjacency matrix,
ﬁnd a pair of disjoint subsets
V1 , V2 ⊂ V of equal size and V1 ∪ V2 = V , such that the number of edges between
vertices in V1 and vertices in V2 , i.e., cut(V1 , V2 ), is minimal.

Theorem 4.2 (Reduction of Graph Bisection to SRcut) For any given undirected graph
G = G(V , E , W) where W is the adjacency matrix, ﬁnding the minimum bisection of G is
equivalent to ﬁnding a partition of G that minimizes the SRcut cost function with weights
X
β = e and the regularization parameter α > d∗ where
d∗ = max
Wij .
i=1,···,N
j=1,···,N

Proof: Without loss of generality, we assume that N is even (if not, we can always add an
isolated vertex). Let cuti be the minimal cut with the size of the smaller subset is i, i.e.,
cut(V1 , V2 ) .
min
cuti =
min(|V1 |, |V2 |) = i
V1 ∪ V2 = V
V1 ∩ V2 = ∅
2 − 1. If 0 ≤ i ≤ N
2 − 1, then
Clearly, we have d∗ ≥ cuti+1 − cuti for 0 ≤ i ≤ N
N − 2i − 1 ≥ 1. Therefore, for any α > d∗ , we have
α(N − 2i − 1) > d∗ ≥ cuti+1 − cuti .
This implies that cuti − αi(N − i) > cuti+1 − α(i + 1)(N − i − 1) , or, equivalently,
cut(V1 , V2 )−α|V1 ||V2 |
cut(V1 , V2 )−α|V1 ||V2 | >
min
min
min(|V1 |, |V2 |) = i
min(|V1 |, |V2 |) = i + 1
V1 ∪ V2 = V
V1 ∪ V2 = V
V1 ∩ V2 = ∅
V1 ∩ V2 = ∅
for 0 ≤ i ≤ N
2 − 1. Hence, for any α > d∗ , minimizing SRcut is identical to minimizing
cut(V1 , V2 ) − α|V1 ||V2 |
2 , V1 ∪ V2 = V , and V1 ∩ V2 = ∅, which is exactly
with the constraint that |V1 | = |V2 | = N
the graph bisection problem since α|V1 ||V2 | = α N 2
(cid:3)
4 is a constant.

W

5 An approximation algorithm for SRcut
Given a partition of vertex set V into two sets V1 and V2 , let x ∈ {−1, 1}N be an indicator
vector such that xi = 1 if i ∈ V1 and xi = −1 if i ∈ V2 . It is not difﬁcult to show that
ββT (e − x)
(e − x)
(e + x)T
(e + x)T
and |V1 |β |V2 |β =
cut(V1 , V2 ) =
2
2
2
2
We can therefore rewrite SRcut in (3) as a function of the indicator vector x:
(e − x)
(e + x)T
(W − αββT )
SRcut(V1 , V2 ) =
2
2
1
= − 1
xT (W − αββT )x +
eT (W − αββT )e .
4
4
Given W, α, and β , we have
argminx∈{−1,1}N SRcut(x) = argmaxx∈{−1,1}N xT (W − αββT )x
x (i.e., kyk = 1), then minimum SRcut
If we deﬁne a normalized indicator vector, y = 1√
N
can be found by solving the following discrete optimization problem
}N yT (W − αββT )y ,
y = argmaxy∈{− 1√
1√
N
N
which is NP-complete. However, if we relax all the elements in the indicator vector y from
discrete values to real values and keep the unit length constraint on y, the above optimiza-
tion problem can be easily solved. And the solution is the eigenvector corresponding to the
largest eigenvalue of W − αββT (or named the largest eigenvector).

(8)

.

(7)

.

Similar to other spectral graph partitioning techniques that use top eigenvectors to approx-
imate “optimal” partitions, the largest eigenvector of W − αββT provides a linear search
direction, along which a splitting point can be found. We use a simple approach by check-
ing each element in the largest eigenvector as a possible splitting point. The vertices, whose
continuous indicators are greater than or equal to the splitting point, are assigned to one
cluster. The remaining vertices are assigned to the other cluster. The corresponding SRcut
value is then computed. The ﬁnal partition is determined by the splitting point with the
minimum SRcut value. The relaxed optimization problem provides a lower bound on the
optimal SRcut value, SRcut∗ . Let λ1 be the largest eigenvalue of W − αββT . From (7)
and (8), it is straightforward to show that
SRcut∗ ≥ eT (W − αββT )e − N λ1
4
The SRcut value of the partition generated by the largest eigenvector provides an upper
bound for SRcut∗ .
As implied by SRcut cost function in (3), the partition of the dataset depends on the value
of α, which determines the tradeoff between inter-cluster similarity and the balance of the
partition. Moreover, Theorem 3.1 indicates that with the increase of α, the size ratio of
the clusters generated by the optimal partition increase monotonically, i.e., the partition
becomes more balanced. Even though, we do not have a counterpart of Theorem 3.1 for
the approximated partition derived above, our empirical study shows that, in general, the
size ratio of the approximated partition increases along with α. Therefore, we use the prior
information on the size of the clusters to select α. Speciﬁcally, we deﬁne expected size
ratio, R, as R = min(s1 ,s2 )
max(s1 ,s2 ) where s1 and s2 are the expected size of the two clusters
(known a priori). We then search for a value of α such that the resulting size ratio is
close to R. A simple one-dimensional search method based on bracketing and bisection
is implemented [13]. The pseudo code of the searching algorithm is given in Algorithm
1 along with the rest of the clustering procedure. The input of the algorithm is the graph
afﬁnity matrix W, the weight vector β , the expected size ratio R, and α0 > 0 (the initial
value of α). The output is a partition of V . In our experiments, α0 is chosen to be 10 eT We
.
N 2
If the expected size ratio R is unknown, one can estimate R assuming that the data are
i.i.d. samples and a sample belongs to the smaller cluster with probability p ≤ 0.5 (i.e.,
R = p
1−p ). It is not difﬁcult to prove that ˆp of n randomly selected samples from the data set
is an unbiased estimator of p. Moreover, the distribution of ˆp can be well approximated by
a normal distribution with mean p and variance p(1−p)
n when n is sufﬁciently large (say n >
30). Hence ˆp converges to p as the increase of n. This suggests a simple strategy for SRcut
with unknown R. One can manually examine n (cid:28) N randomly selected data instances
to get ˆp and the 95% conﬁdence interval
[plow , phigh ], from which one can evaluate the
invertal [Rlow , Rhigh ] for R. Algorithm 1 is then applied to a number of evenly distributed
R’s within the interval to ﬁnd the corresponding partitions. The ﬁnal partition is chosen to
be the one with the minimum cut value by assuming that a “good” partition should have a
small cut.

6 Time complexity

The time complexity of each iteration is determined by that of computing the largest eigen-
vector. Using power method or Lanczos method [8], the running time is O(M N 2 ) where
M is the number of matrix-vector computations required and N is the number of vertices.
Hence the overall time complexity is O(KM N 2 ) where K is the number of iterations in
searching α. Similar to other spectral graph clustering methods, the time complexity of
SRcut can be signiﬁcantly reduced if the afﬁnity matrix W is sparse, i.e., the graph is only

Algorithm 1: Size Regularized Cut
1 initialize αl to 2α0 and αh to α0
2
2 REPEAT
αl ← αl
2 , y ← largest eigenvector of W − αlββT
3
partition V using y and compute size ratio r
4
5 UNTIL (r < R)
6 REPEAT
αh ← 2αh, y ← largest eigenvector of W − αhββT
7
partition V using y and compute size ratio r
8
9 UNTIL (r ≥ R)
10 REPEAT
α ← αl+αh
2 , y ← largest eigenvector of W − αββT
11
partition V using y and compute size ratio r
12
IF (r < R)
13
αl ← α
14
ELSE
15
αh ← α
16
17
END IF
18 UNTIL (|r − R| < 0.01R or αh − αl < 0.01α0 )
locally connected. Although W − αββT is in general not sparse, the time complexity of
power method is still O(M N ). This is because (W − αββT )y can be evaluated as the
sum of Wy and αβ(βT y), each requiring O(N ) operations. Therefore, by enforcing the
sparsity, the overall time complexity of SRcut is O(KM N ).

7 Experiments

We test the SRcut algorithm using two data sets, Reuters-21578 document corpus and 20-
Newsgroups. Reuters-21578 data set contains 21578 documents that have been manually
assigned to 135 topics. In our experiments, we discarded documents with multiple category
labels, and removed the topic classes containing less than 5 documents. This leads to a data
set of 50 clusters with a total of 9102 documents. The 20-Newsgroups data set contains
about 20000 documents collected from 20 newsgroups, each corresponding to a distinct
topic. The number of news articles in each cluster is roughly the same. We pair each
cluster with another cluster to form a data set, so that 190 test data sets are generated. Each
document is represented by a term-frequency vector using TF-IDF weights.

We use the normalized mutual information as our evaluation metric. Normalized mutual
information is always within the interval [0, 1], with a larger value indicating a better per-
formance. A simple sampling scheme described in Section 5 is used to estimate the ex-
pected size ratio. For the Reuters-21578 data set, 50 test runs were conducted, each on a
test set created by mixing 2 topics randomly selected from the data set. The performance
score in Table 1 was obtained by averaging the scores from 50 test runs. The results for 20-
Newsgroups data set were obtained by averaging the scores from 190 test data sets. Clearly,
SRcut outperforms the normalized cut on both data sets. SRcut performs signiﬁcantly bet-
ter than normalized cut on the 20-Newsgroups data set. In comparison with Reuters-21578,
many topic classes in the 20-Newsgroups data set contain outliers. The results suggest that
SRcut is less sensitive to outliers than normalized cut.

8 Conclusions

We proposed size regularized cut, a novel method that enables users to specify prior knowl-
edge of the size of two clusters in spectral clustering. The SRcut cost function takes into

Table 1: Performance comparison for SRcut and Normalized Cut. The numbers shown are the
normalized mutual information. A larger value indicates a better performance.
20-Newsgroups
Reuters-21578
Algorithms
SRcut
0.7315
0.7330
Normalized Cut
0.7102
0.2531

account inter-cluster similarity and the relative size of two clusters. The “optimal” parti-
tion of the data set corresponds to a tradeoff between the inter-cluster similarity and the
balance of the partition. We proved that ﬁnding a partition with minimum SRcut is an NP-
complete problem. We presented an approximation algorithm to solve a relaxed version of
the optimization problem. Evaluations over different data sets indicate that the method is
not sensitive to outliers and performs better than normalized cut. The SRcut model can be
easily adapted to solve multiple-clusters problem by applying the clustering method recur-
sively/iteratively on data sets. Since graph bisection can be reduced to SRcut, the proposed
approximation algorithm provides a new spectral technique for graph bisection. Comparing
SRcut with other graph bisection algorithms is therefore an interesting future work.

References
[1] S. Arora, D. Karger, and M. Karpinski,
“Polynomial Time Approximation Schemes for Dense
Instances of NP-hard Problems,” Proc. ACM Symp. on Theory of Computing, pp. 284-293, 1995.
[2] A. Banerjee and J. Ghosh, “On Scaling up Balanced Clustering Algorithms,”
Proc. SIAM Int’l
Conf. on Data Mining, pp. 333-349, 2002.
[3] P. K. Chan, D. F. Schlag, and J. Y. Zien, “Spectral k-Way Ratio-Cut Partitioning and Clustering,”
IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems, 13:1088-1096, 1994.
[4] M. Charikar, S. Khuller, D. M. Mount, and G. Narasimhan,
“Algorithms for Facility Location
Problems with Outliers,” Proc. ACM-SIAM Symp. on Discrete Algorithms, pp. 642-651, 2001.
[5] I. S. Dhillon, “Co-clustering Documents and Words using Bipartite Spectral Graph Partitioning,”
Proc. ACM SIGKDD Conf. Knowledge Discovery and Data Mining, pp. 269-274, 2001.
[6] C. Ding,
“Data Clustering: Principal Components, Hopﬁeld and Self-Aggregation Networks,”
Proc. Int’l Joint Conf. on Artiﬁcial Intelligence , pp. 479-484, 2003.
[7] C. Ding, X. He, H. Zha, M. Gu, and H. Simon, “Spectral Min-Max Cut for Graph Partitioning
and Data Clustering,” Proc. IEEE Int’l Conf. Data Mining, pp. 107-114, 2001.
[8] G. H. Golub and C. F. Van Loan, Matrix Computations, John Hopkins Press, 1999.
[9] R. Kannan, S. Vempala, and A. Vetta, “On Clusterings - Good, Bad and Spectral,”
Symp. on Foundations of Computer Science, pp. 367-377, 2000.
[10] D. R. Karget and M. Minkoff,
“Building Steiner Trees with Incomplete Global Knowledge,”
Proc. IEEE Symp. on Foundations of Computer Science, pp. 613-623, 2000
[11] B. Kernighan and S. Lin, “An Efﬁcient Heuristic Procedure for Partitioning Graphs,”
System Technical Journal, 49:291-307, 1970.
[12] A. Y. Ng, M. I. Jordan, and Y. Weiss,
“On Spectral Clustering: Analysis and an Algorithm,”
Advances in Neural Information Processing Systems 14, pp. 849-856, 2001.
[13] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical Recipes in C,
second edition, Cambridge University Press, 1992.
[14] A. Rahimi and B. Recht, “Clustering with Normalized Cuts is Clustering with a Hyperplane,”
Statistical Learning in Computer Vision, 2004.
[15] J. Shi and J. Malik,
“Normalized Cuts and Image Segmentation,”
Analysis and Machine Intelligence, 22:888-905, 2000.
“Constrained K-means Clustering with
[16] K. Wagstaff, C. Cardie, S. Rogers, and S. Schrodl,
Background Knowledge,” Proc. Int’l Conf. on Machine Learning, pp. 577-584, 2001.
[17] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell, “Distance Metric Learning, with Applications
to Clustering with Side Information,” Advances in Neural Information Processing Systems 15,
pp. 505-512, 2003.
[18] X. Yu and J. Shi, “Segmentation Given Partial Grouping Constraints,”
Analysis and Machine Intelligence, 26:173-183, 2004.
[19] H. Zha, X. He, C. Ding, H. Simon, and M. Gu, “Spectral Relaxation for K-means Clustering,”
Advances in Neural Information Processing Systems 14, pp. 1057-1064, 2001.

IEEE Trans. on Pattern

IEEE Trans. on Pattern

Proc. IEEE

The Bell


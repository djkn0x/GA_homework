Layered Dynamic Textures

Antoni B. Chan
Nuno Vasconcelos
Department of Electrical and Computer Engineering
University of California, San Diego
abchan@ucsd.edu, nuno@ece.ucsd.edu

Abstract

A dynamic texture is a video model that treats a video as a sample from
a spatio-temporal stochastic process, speciﬁcally a linear dynamical sys-
tem. One problem associated with the dynamic texture is that it cannot
model video where there are multiple regions of distinct motion. In this
work, we introduce the layered dynamic texture model, which addresses
this problem. We also introduce a variant of the model, and present the
EM algorithm for learning each of the models. Finally, we demonstrate
the efﬁcac y of the proposed model for the tasks of segmentation and syn-
thesis of video.

1 Introduction

Traditional motion representations, based on optical ﬂo w, are inherently local and have sig-
niﬁcant difﬁculties when faced with aperture problems and noise. The classical solution to
this problem is to regularize the optical ﬂo w ﬁeld [1, 2, 3, 4], but this introduces undesirable
smoothing across motion edges or regions where the motion is, by deﬁnition, not smooth
(e.g. vegetation in outdoors scenes). More recently, there have been various attempts to
model video as a superposition of layers subject to homogeneous motion. While layered
representations exhibited signiﬁcant promise in terms of combining the advantages of reg-
ularization (use of global cues to determine local motion) with the ﬂe xibility of local repre-
sentations (little undue smoothing), this potential has so far not fully materialized. One of
the main limitations is their dependence on parametric motion models, such as afﬁne trans-
forms, which assume a piece-wise planar world that rarely holds in practice [5, 6]. In fact,
layers are usually formulated as “cardboard” models of the world that are warped by such
transformations and then stitched to form the frames in a video stream [5]. This severely
limits the types of video that can be synthesized: while layers showed most promise as
models for scenes composed of ensembles of objects subject to homogeneous motion (e.g.
leaves blowing in the wind, a ﬂock of birds, a picket fence, or highway trafﬁc), very little
progress has so far been demonstrated in actually modeling such scenes.

Recently, there has been more success in modeling complex scenes as dynamic textures or,
more precisely, samples from stochastic processes deﬁned over space and time [7, 8, 9, 10].
This work has demonstrated that modeling both the dynamics and appearance of video
as stochastic quantities leads to a much more powerful generative model for video than
that of a “cardboard”
In fact, the dynamic texture
subject to parametric motion.
ﬁgure
model has shown a surprising ability to abstract a wide variety of complex patterns of
motion and appearance into a simple spatio-temporal model. One major current limitation

of the dynamic texture framework, however, is its inability to account for visual processes
consisting of multiple, co-occurring, dynamic textures. For example, a ﬂock of birds ﬂying
in front of a water fountain, highway trafﬁc moving at different speeds, video containing
both trees in the background and people in the foreground, and so forth. In such cases,
the existing dynamic texture model is inherently incorrect, since it must represent multiple
motion ﬁelds with a single dynamic process.

In this work, we address this limitation by introducing a new generative model for video,
which we denote by the layered dynamic texture (LDT). This consists of augmenting the
dynamic texture with a discrete hidden variable, that enables the assignment of different
dynamics to different regions of the video. Conditioned on the state of this hidden variable,
the video is then modeled as a simple dynamic texture. By introducing a shared dynamic
representation for all the pixels in the same region, the new model is a layered represen-
tation. When compared with traditional layered models, it replaces the process of layer
formation based on “w arping of cardboard ﬁgures” with one based on sampling from the
generative model (for both dynamics and appearance) provided by the dynamic texture.
This enables a much richer video representation. Since each layer is a dynamic texture,
the model can also be seen as a multi-state dynamic texture, which is capable of assigning
different dynamics and appearance to different image regions.

We consider two models for the LDT, that differ in the way they enforce consistency of
layer dynamics. One model enforces stronger consistency but has no closed-form solu-
tion for parameter estimates (which require sampling), while the second enforces weaker
consistency but is simpler to learn. The models are applied to the segmentation and syn-
thesis of sequences that are challenging for traditional vision representations. It is shown
that stronger consistency leads to superior performance, demonstrating the beneﬁts of so-
phisticated layered representations. The paper is organized as follows. In Section 2, we
introduce the two layered dynamic texture models. In Section 3 we present the EM al-
gorithm for learning both models from training data. Finally, in Section 4 we present an
experimental evaluation in the context of segmentation and synthesis.

2 Layered dynamic textures

We start with a brief review of dynamic textures, and then introduce the layered dynamic
texture model.

2.1 Dynamic texture

A dynamic texture [7] is a generative model for video, based on a linear dynamical system.
The basic idea is to separate the visual component and the underlying dynamics into two
processes. While the dynamics are represented as a time-evolving state process x t 2 Rn ,
the appearance of frame yt 2 RN is a linear function of the current state vector, plus some
observation noise. Formally, the system is described by
(cid:26) xt = Axt(cid:0)1 + B vt
yt = C xt + prwt
where A 2 Rn(cid:2)n is a transition matrix, C 2 RN (cid:2)n a transformation matrix, B vt (cid:24)iid
N (0; Q;) and prwt (cid:24)iid N (0; rIN ) the state and observation noise processes parameter-
ized by B 2 Rn(cid:2)n and r 2 R, and the initial state x0 2 Rn is a constant. One interpre-
tation of the dynamic texture model is that the columns of C are the principal components
of the video frames, and the state vectors the PCA coefﬁcients
for each video frame. This
is the case when the model is learned with the method of [7].

(1)

Figure 1: The layered dynamic texture (left), and the approximate layered dynamic texture
(right). yi is an observed pixel over time, xj is a hidden state process, and Z is the collection
of layer assignment variables zi that assigns each pixels to one of the state processes.

An alternative interpretation considers a single pixel as it evolves over time. Each coor-
dinate of the state vector xt deﬁnes a one-dimensional random trajectory in time. A pixel
is then represented as a weighted sum of random trajectories, where the weighting coef-
ﬁcients
are contained in the corresponding row of C . This is analogous to the discrete
Fourier transform in signal processing, where a signal is represented as a weighted sum of
complex exponentials although, for the dynamic texture, the trajectories are not necessarily
orthogonal. This interpretation illustrates the ability of the dynamic texture to model the
same motion under different intensity levels (e.g. cars moving from the shade into sunlight)
by simply scaling the rows of C . Regardless of interpretation, the simple dynamic texture
model has only one state process, which restricts the efﬁcac y of the model to video where
the motion is homogeneous.

2.2 Layered dynamic textures

We now introduce the layered dynamic texture (LDT), which is shown in Figure 1 (left).
The model addresses the limitations of the dynamic texture by relying on a set of state pro-
cesses X = fx(j ) gK
j=1 to model different video dynamics. The layer assignment variable
zi assigns pixel yi to one of the state processes (layers), and conditioned on the layer as-
signments, the pixels in the same layer are modeled as a dynamic texture. In addition, the
collection of layer assignments Z = fzigN
i=1 is modeled as a Markov random ﬁeld (MRF)
to ensure spatial layer consistency. The linear system equations for the layered dynamic
texture are

( x(j )
t = A(j ) x(j )
t(cid:0)1 + B (j ) v (j )
j 2 f1; (cid:1) (cid:1) (cid:1) ; K g
t + pr(zi )wi;t
t
yi;t = C (zi )
x(zi )
i 2 f1; (cid:1) (cid:1) (cid:1) ; N g
i
where C (j )
2 R1(cid:2)n is the transformation from the hidden state to the observed pixel
i
domain for each pixel yi and each layer j , the noise parameters are B (j ) 2 Rn(cid:2)n and r(j ) 2
R, the iid noise processes are wi;t (cid:24)iid N (0; 1) and v (j )
t (cid:24)iid N (0; In ), and the initial
states are drawn from x(j )
1 (cid:24) N ((cid:22)(j ) ; S (j ) ). As a generative model, the layered dynamic
texture assumes that the state processes X and the layer assignments Z are independent, i.e.
layer motion is independent of layer location, and vice versa. As will be seen in Section 3,
this makes the expectation-step of the EM algorithm intractable to compute in closed-form.
To address this issue, we also consider a slightly different model.

(2)

2.3 Approximate layered dynamic texture

We now consider a different model, the approximate layered dynamic texture (ALDT),
shown in Figure 1 (right). Each pixel yi is associated with its own state process xi , and a

different dynamic texture is deﬁned for each pixel. However, dynamic textures associated
with the same layer share the same set of dynamic parameters, which are assigned by the
layer assignment variable zi . Again, the collection of layer assignments Z is modeled as an
MRF but, unlike the ﬁrst model, conditioning on the layer assignments makes all the pixels
independent. The model is described by the following linear system equations
(cid:26) xi;t = A(zi )xi;t(cid:0)1 + B (zi ) vi;t
xi;t + pr(zi )wi;t
yi;t = C (zi )
i
where the noise processes are wi;t (cid:24)iid N (0; 1) and vi;t (cid:24)iid N (0; In ), and the initial
states are given by xi;1 (cid:24) N ((cid:22)(zi ) ; S (zi ) ). This model can also be seen as a video extension
of the popular image MRF models [11], where class variables for each pixel form an MRF
grid and each class (e.g. pixels in the same segment) has some class-conditional distribution
(in our case a linear dynamical system).

i 2 f1; (cid:1) (cid:1) (cid:1) ; N g

(3)

The main difference between the two proposed models is in the enforcement of consistency
of dynamics within a layer. With the LDT, consistency of dynamics is strongly enforced by
requiring each pixel in the layer to be associated with the same state process. On the other
hand, for the ALDT, consistency within a layer is weakly enforced by allowing the pixels
to be associated with many instantiations of the state process (instantiations associated with
the same layer sharing the same dynamic parameters). This weaker dependency structure
enables a more efﬁcient
learning algorithm.

2.4 Modeling layer assignments

p(Z ) =

 i;j (zi ; zj )

The MRF which determines layer assignments has the following distribution
1
Z Yi
 i (zi ) Y(i;j )2E
where E is the set of edges in the MRF grid, Z a normalization constant (partition function),
and  i and  i;j potential functions of the form
 i (zi ) = 8><
 i;j (zi ; zj ) = (cid:26) (cid:13)1
(cid:13)2
>:
The potential function  i deﬁnes
a prior likelihood for each layer, while  i;j attributes
higher probability to conﬁgurations where neighboring pixels are in the same layer. While
the parameters for the potential functions could be learned for each model, we instead treat
them as constants that can be estimated from a database of manually segmented training
video.

; zi = 1
(cid:11)1
...
...
(cid:11)K ; zi = K

; zi = zj
; zi 6= zj

(4)

(5)

3 Parameter estimation

The parameters of the model are learned using the Expectation-Maximization (EM) algo-
rithm [12], which iterates between estimating hidden state variables X and hidden layer
assignments Z from the current parameters, and updating the parameters given the current
hidden variable estimates. One iteration of the EM algorithm contains the following two
steps

(cid:15) E-Step: Q((cid:2); ^(cid:2)) = EX;Z jY ; ^(cid:2) (log p(X; Y ; Z ; (cid:2)))
(cid:15) M-Step: ^(cid:2)(cid:3) = argmax(cid:2) Q((cid:2); ^(cid:2))

In the remainder of this section, we brieﬂy describe the EM algorithm for the two proposed
models. Due to the limited space available, we refer the reader to the companion technical
report [13] for further details.

3.1 EM for the layered dynamic texture

The E-step for the layered dynamic texture computes the conditional mean and covari-
ance of x(j )
given the observed video Y . These expectations are intractable to compute in
t
closed-form since it is not known to which state process each of the pixels y i is assigned,
and it is therefore necessary to marginalize over all con ﬁgurations of Z . This problem also
appears for the computation of the posterior layer assignment probability p(z i = j jY ). The
method of approximating these expectations which we currently adopt is to simply average
over draws from the posterior p(X; Z jY ) using a Gibbs sampler. Other approximations,
e.g. variational methods or belief propagation, could be used as well. We plan to con-
sider them in the future. Once the expectations are known, the M-step parameter updates
are analogous to those required to learn a regular linear dynamical system [15, 16], with a
minor modiﬁcation in the updates if the transformation matrices C (j )
. See [13] for details.
i

3.2 EM for the approximate layered dynamic texture

The ALDT model is similar to the mixture of dynamic textures [14], a video clustering
model that treats a collection of videos as a sample from a collection of dynamic textures.
Since, for the ALDT model, each pixel is sampled from a set of one-dimensional dynamic
textures, the EM algorithm is similar to that of the mixture of dynamic textures. There
are only two differences. First, the E-step computes the posterior assignment probability
p(zi jY ) given all the observed data, rather than conditioned on a single data point p(z i jyi ).
The posterior p(zi jY ) can be approximated by sampling from the full posterior p(Z jY )
using Markov-Chain Monte Carlo [11], or with other methods, such as loopy belief propa-
gation. Second, the transformation matrix C (j )
is different for each pixel, and the E and M
i
steps must be modiﬁed accordingly. Once again, the details are available in [13].

4 Experiments

In this section, we show the efﬁcac y of the proposed model for segmentation and synthesis
of several videos with multiple regions of distinct motion. Figure 2 shows the three video
sequences used in testing. The ﬁrst
(top) is a composite of three distinct video textures
of water, smoke, and ﬁre.
The second (middle) is of laundry spinning in a dryer. The
laundry in the bottom left of the video is spinning in place in a circular motion, and the
laundry around the outside is spinning faster. The ﬁnal video (bottom) is of a highway [17]
where the trafﬁc in each lane is traveling at a different speed. The ﬁrst,
second and fourth
lanes (from left to right) move faster than the third and ﬁfth. All three videos have multiple
regions of motion and are therefore properly modeled by the models proposed in this paper,
but not by a regular dynamic texture.

Four variations of the video models were ﬁt
to each of the three videos. The four mod-
els were the layered dynamic texture and the approximate layered dynamic texture models
(LDT and ALDT), and those two models without the MRF layer assignment (LDT-iid and
ALDT-iid). In the latter two cases, the layers assignments zi are distributed as iid multino-
mials. In all the experiments, the dimension of the state space was n = 10. The MRF grid
was based on the eight-neighbor system (with cliques of size 2), and the parameters of the
potential functions were (cid:13)1 = 0:99, (cid:13)2 = 0:01, and (cid:11)j = 1=K . The expectations required
by the EM algorithm were approximated using Gibbs sampling for the LDT and LDT-iid
models and MCMC for the ALDT model. We ﬁrst present segmentation results, to show

that the models can effectively separate layers with different dynamics, and then discuss
results relative to video synthesis from the learned models.

4.1 Segmentation

The videos were segmented by assigning each of the pixels to the most probable layer
conditioned on the observed video, i.e.

z (cid:3)
i = argmax
j

p(zi = j jY )
Another possibility would be to assign the pixels by maximizing the posterior of all the pix-
els p(Z jY ). While this maximizes the true posterior, in practice we obtained similar results
with the two methods. The former method was chosen because the individual posterior
distributions are already computed during the E-step of EM.

(6)

The columns of Figure 3 show the segmentation results obtained with for the four models:
LDT and LDT-iid in columns (a) and (b), and ALDT and ALDT-iid in columns (c) and (d).
The segmented video is also available at [18]. From the segmentations produced by the iid
models, it can be concluded that the composite and laundry videos can be reasonably well
segmented without the MRF prior. This con ﬁrms the intuition that the various video regions
contain very distinct dynamics, which can only be modeled with separate state processes.
Otherwise, the pixels should be either randomly assigned among the various layers, or uni-
formly assigned to one of them. The segmentations of the trafﬁc video using the iid models
are poor. While the dynamics are different, the differences are signi ﬁcantly more subtle,
and segmentation requires stronger enforcement of layer consistency. In general, the seg-
mentations using LDT-iid are better than to those of the ALDT-iid, due to the weaker form
of layer consistency imposed by the ALDT model. While this deﬁcienc y is offset by the in-
troduction of the MRF prior, the stronger consistency enforced by the LDT model always
results in better segmentations. This illustrates the need for the design of sophisticated
layered representations when the goal is to model video with subtle inter-layer variations.
As expected, the introduction of the MRF prior improves the segmentations produced by
both models. For example, in the composite sequence all erroneous segments in the water
region are removed, and in the trafﬁc sequence, most of the speckled segmentation also
disappears.

In terms of the overall segmentation quality, both LDT and ALDT are able to segment
the composite video perfectly. The segmentation of the laundry video by both models
is plausible, as the laundry tumbling around the edge of the dryer moves faster than that
spinning in place. The two models also produce reasonable segmentations of the trafﬁc
video, with the segments roughly corresponding to the different lanes of trafﬁc. Much of
the errors correspond to regions that either contain intermittent motion (e.g.
the region
between the lanes) or almost no motion (e.g. truck in the upper-right corner and ﬂat-bed
truck in the third lane). Some of these errors could be eliminated by ﬁltering the video
before segmentation, but we have attempted no pre or post-processing. Finally, we note
that the laundry and trafﬁc videos are not trivial to segment with standard computer vision
techniques, namely methods based on optical ﬂo w. This is particularly true in the case of
the trafﬁc video where the abundance of straight lines and ﬂat
regions makes computing
the correct optical ﬂo w difﬁcult due to the aperture problem.

4.2 Synthesis

The layered dynamic texture is a generative model, and hence a video can be synthesized
by drawing a sample from the learned model. A synthesized composite video using the
LDT, ALDT, and the normal dynamic texture can be found at [18]. When modeling a
video with multiple motions, the regular dynamic texture will average different dynamics.

Figure 2: Frames from the test video sequences: (top) composite of water, smoke, and ﬁre
video textures; (middle) spinning laundry in a dryer; and (bottom) highway trafﬁc with
lanes traveling at different speeds.

(a)

(b)

(c)

(d)

Figure 3: Segmentation results for each of the test videos using: (a) the layered dynamic
texture, and (b) the layered dynamic texture without MRF; (c) the approximate layered
dynamic texture, and (d) the approximate LDT without MRF.

This is noticeable in the synthesized video, where the ﬁre region does not ﬂick er at the same
speed as in the original video. Furthermore, the motions in different regions are coupled,
e.g. when the ﬁre begins to ﬂick er faster, the water region ceases to move smoothly. In
contrast, the video synthesized from the layered dynamic texture is more realistic, as the
ﬁre region ﬂick ers at the correct speed, and the different regions follow their own motion
patterns. The video synthesized from the ALDT appears noisy because the pixels evolve
from different instantiations of the state process. Once again this illustrates the need for
sophisticated layered models.

References

IEEE Trans. on Image

[1] B. K. P. Horn. Robot Vision. McGraw-Hill Book Company, New York, 1986.
[2] B. Horn and B. Schunk. Determining optical ﬂow. Artiﬁcial
Intelligence, vol. 17, 1981.
[3] B. Lucas and T. Kanade. An iterative image registration technique with an application to stereo
vision. Proc. DARPA Image Understanding Workshop, 1981.
[4] J. Barron, D. Fleet, and S. Beauchemin. Performance of optical ﬂow techniques. International
Journal of Computer Vision, vol. 12, 1994.
[5] J. Wang and E. Adelson. Representing moving images with layers.
Processing, vol. 3, September 1994.
[6] B. Frey and N. Jojic. Estimating mixture models of images and inferring spatial transformations
using the EM algorithm. In IEEE Conference on Computer Vision and Pattern Recognition,
1999.
[7] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic textures. International Journal of
Computer Vision, vol. 2, pp. 91-109, 2003.
[8] G. Doretto, D. Cremers, P. Favaro, and S. Soatto. Dynamic texture segmentation.
International Conference on Computer Vision, vol. 2, pp. 1236-42, 2003.
[9] P. Saisan, G. Doretto, Y. Wu, and S. Soatto. Dynamic texture recognition. In IEEE Conference
on Computer Vision and Pattern Recognition, Proceedings, vol. 2, pp. 58-63, 2001.
[10] A. B. Chan and N. Vasconcelos. Probabilistic kernels for the classiﬁcation of auto-regressive
visual processes. In IEEE Conference on Computer Vision and Pattern Recognition, vol. 1, pp.
846-51, 2005.
[11] S. Geman and D. Geman. Stochastic relaxation, Gibbs distribution, and the Bayesian restoration
of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 6(6), pp. 721-
41, 1984.
[12] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society B, vol. 39, pp. 1-38, 1977.
[13] A. B. Chan and N. Vasconcelos. The EM algorithm for layered dynamic textures. Technical
Report SVCL-TR-2005-03, June 2005. http://www.svcl.ucsd.edu/ .
[14] A. B. Chan and N. Vasconcelos. Mixtures of dynamic textures. In IEEE International Confer-
ence on Computer Vision, vol. 1, pp. 641-47, 2005.
[15] R. H. Shumway and D. S. Stoffer. An approach to time series smoothing and forecasting using
the EM algorithm. Journal of Time Series Analysis, vol. 3(4), pp. 253-64, 1982.
[16] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Compu-
tation, vol. 11, pp. 305-45, 1999.
[17] http://www.wsdot.wa.gov
[18] http://www.svcl.ucsd.edu/(cid:24)abc/nips05/

In IEEE


Efﬁcient estimation of hidden state dynamics
from spike trains

M ´arton G. Dan ´oczy
Inst. for Theoretical Biology
Humboldt University, Berlin
Invalidenstr. 43
10115 Berlin, Germany
m.danoczy@biologie.hu-berlin.de

Richard H. R. Hahnloser
Inst. for Neuroinformatics
UNIZH / ETHZ
Winterthurerstrasse 190
8057 Zurich, Switzerland
rich@ini.phys.ethz.ch

Abstract

Neurons can have rapidly changing spike train statistics dictated by the
underlying network excitability or behavioural state of an animal. To
estimate the time course of such state dynamics from single- or multi-
ple neuron recordings, we have developed an algorithm that maximizes
the likelihood of observed spike trains by optimizing the state lifetimes
and the state-conditional interspike-interval (ISI) distributions. Our non-
parametric algorithm is free of time-binning and spike-counting prob-
lems and has the computational complexity of a Mixed-state Markov
Model operating on a state sequence of length equal to the total num-
ber of recorded spikes. As an example, we ﬁt a two-state model to paired
recordings of premotor neurons in the sleeping songbird. We ﬁnd that the
two state-conditional ISI functions are highly similar to the ones mea-
sured during waking and singing, respectively.

1

Introduction

It is well known that neurons can suddenly change ﬁring statistics to reﬂect a macroscopic
change of a nervous system. Often, ﬁring changes are not accompanied by an immediate
behavioural change, as is the case, for example, in paralysed patients, during sleep [1],
during covert discriminative processing [2], and for all in-vitro studies [3]. In all of these
cases, changes in some hidden macroscopic state can only be detected by close inspection
of single or multiple spike trains. Our goal is to develop a powerful, but computationally
simple tool for point processes such as spike trains. From spike train data, we want to the
extract continuously evolving hidden variables, assuming a discrete set of possible states.

Our model for classifying spikes into discrete hidden states is based on three assumptions:

1. Hidden states form a continuous-time Markov process and thus have exponentially
distributed lifetimes

2. State switching can occur only at the time of a spike (where there is observable
evidence for a new state).

3. In each of the hidden states, spike trains are generated by mutually independent
renewal processes.

1. For a continuous-time Markov process, the probability of staying in state S = i for a
time interval T > t is given by Pi (t ) = exp(−rit ), where ri is the escape rate (or hazard rate)
of state i. The mean lifetime τi is deﬁned as the inverse of the escape rate, τi = 1/ri .

As a corollary, it follows that the probability of staying in state i for a particular duration
equals the probability of surviving for a fraction of that duration times the probability of
surviving for the remaining time, i.e., the state survival probability Pi (t ) satisﬁes the product
identity

Pi (t1 + t2 ) = Pi (t1 )Pi (t2 ).

2. According to the second assumption, state switching can occur at any spike, irrespec-
tive of which neuron ﬁred the spike. In the following, we shall refer to a spike ﬁred by any
of the neurons as an event (where state switching might occur). Note that if two (or more)
neurons happen to ﬁre a spike at exactly the same time, the respective spikes are regarded
as two (or more) distinct events. The collection of event times is denoted by te .

Combining the ﬁrst two assumptions, we formulate the hidden state sequence at the events
(i.e. observation points) as a non-homogeneous discrete Markov chain. Accordingly, the
probability of remaining in state i for the duration of the interevent-interval (IEI) ∆te =
te − te−1 is given by the state survival probability Pi (∆te ). The probability to change state is
then 1 − Pi (∆te ).

3.
In each state i, the spike trains are assumed to be generated by a renewal process that
randomly draws interspike-intervals (ISIs) t from a probability density function (pdf) hi (t ).
Because every IEI is only a fraction of an ISI, instead of working with ISI distributions, we
use an equivalent formulation based on the conditional intensity function (CIF) λi(ϕ) [4].
The CIF, also called hazard function in reliability theory, is a generalization of the Poisson
ﬁring rate. It is deﬁned as the probability density of spiking in the time interval [ϕ,ϕ + dt ],
given that no spike has occurred in the interval [0,ϕ) since the last spike. In the following,
the variable ϕ, i.e. the time that has elapsed since the last spike, shall be referred to as phase
[5]. Using the CIF, the ISI pdf can be expressed by the fundamental equation of renewal
theory,
hi (t ) = exp (cid:18)− Z t
λi(ϕ) dϕ(cid:19) λi(t ).
0
At each event e, we observe the phase trajectory of every neuron traced out since the last
event. It is clear that in multiple electrode recordings the phase trajectories between events
are not independent, since they have to start where the previous trajectory ended. Therefore,
our model violates the observation independence assumption of standard Hidden Markov
Models (HMMs). Our model is, in formal terms, a mixed-state Markov model [6], with the
architecture of a double-chain [7]. Such models are generalizations of HMMs in that the
observable outputs may not only be dependent on the current hidden state, but also on past
observations (formally, the mixed state is formed by combining the hidden and observable
states).

(1)

In our model, hidden state transition probabilities are characterized by the escape rates ri
and observable state transition probabilities by the CIFs λn
i for neuron n in hidden state i.
Our goal is to ﬁnd a set Ψ of model parameters, such that the likelihood
Pr{O | Ψ} = ∑
S∈S

Pr{S, O | Ψ}

of the observation sequence O is maximized.

As a ﬁrst step, we will derive an expression for the combined likelihood Pr{S, O | Ψ}. Then,
we will apply the expectation maximization (EM) algorithm to ﬁnd the optimal parameter
set.

2 Transition probabilities

The mixed state at event e shall be composed of the hidden state Se and the observable
outputs On
e (for neurons n ∈ {1, . . . , N }).

Hidden state transitions
In classical mixed-state Markov models, the hidden state tran-
sition probabilities are constant. In our model, however, we describe time as a continuous
quantity and observe the system whenever a spike occurs, thus in non-equidistant intervals.
Consequently, hidden state transitions depend explicitly on the elapsed time since the last
observation, i.e., on the IEIs ∆te . The transition probability ai j (∆te ) from hidden state i to
hidden state j is then given by
ai j (∆te ) = (cid:26)exp(−r j ∆te )
[1 − exp(−r j ∆te )] gi j
where gi j is the conditional probability of making a transition from state i into a new state
j , given that j 6= i. Thus, gi j has to satisfy the constraint ∑ j gi j = 1, with gii = 0.

if i = j ,
otherwise,

(2)

e = inf Φn
sup Φn
e + ∆te

Observable state transitions The observation at event e is deﬁned as Oe = {Φn
e , νe},
where νe contains the index of the neuron that has triggered event e by emitting a spike,
and Φn
e , sup Φn
e = (inf Φn
e ] is the phase interval traced out by neuron n since its last spike.
Observations form a cascade. After a spike, the phase of the respective neuron is immedi-
ately reset to zero. The interval’s bounds are thus deﬁned by
e = (cid:26)0
inf Φn
sup Φn
e−1
The observable transition probability pi (Oe ) = Pr{Oe | Oe−1 , Se = i} is the probability of
observing output Oe , given the previous output Oe−1 and the current hidden state Se . With
our independence assumption (3.), we can give its density as the product of every neuron’s
probability of having survived the respective phase interval Φn
e that it has traced out since
its last spike, multiplied by the spiking neuron’s ﬁring rate (compare equation 1):
pi (Oe ) = (cid:20)∏
exp (cid:18)− ZΦn
i (ϕ) dϕ(cid:19)(cid:21) λνe
i (sup Φνe
λn
e ) .
n
e
Note that in case of a single neuron recording, this reduces to the ISI pdf.

if νe−1 = n,
otherwise.

and

(3)

To give a closed form of the observable transition pdf, several approaches are thinkable.
Here, for the sake of ﬂexibility and computational simplicity, we approximate the CIF λn
i
for neuron n in state i by a step function, assuming that its value is constant inside small,
bins}. That is, λn
i (b), ∀ ϕ ∈ Bn(b).
i (ϕ) ≈ `n
arbitrarily spaced bins Bn(b), b ∈ {1, . . . , N n
e : the fractions f n
In order to use the discretized CIFs `n
i (b), we also discretize Φn
e (b) ∈ [0, 1]
represent how much of neuron n’s phase bin Bn(b) has been traced out since the last event.
For example, if event e − 1 happened in the middle of neuron n’s phase bin 2 and event e
e (3) = 1, and f n
e (2) = 0.5, f n
happened ten percent into its phase bin 4, then f n
e (4) = 0.1,
whereas f n
e (i) = 0 for other i, Figure 1.

Making use of these discretizations, the integral in equation 3 is approximated by a sum:
N n
exp  −
pi (Oe ) ≈ "∏
i (b) kBn(b)k!# λνe
bins
i (sup Φνe
f n
e (b) `n
∑
e ),
n
b=1
with kBn(b)k denoting the width of neuron n’s phase bin b.

(4)

Equations 2 and 4 fully describe transitions in our mixed-state Markov model. Next, we
apply the EM algorithm to ﬁnd optimal values of the escape rates ri , the conditional hidden
state transition probabilities gi j and the discretized CIFs `n
i (b), given a set of spike trains.

Neuron 1

Neuron 2

Events

1

2

3

1

2

3

4

5

1

2

0.5

1

2

f 2
e
1.0

3

0.1

4

1

2

1

te−1

te

Figure 1: Two spike trains are combined to form the event train shown in the bottom row.
The phase bins are shown below the spike trains, they are labelled with the corresponding
bin number. As an example, for the second neuron, the fractions f 2
e (b) of its phase bins
that have been traced out since event e − 1 are indicated by the horizontal arrow. They are
nonzero for b = 2, 3, and 4.

3 Parameter estimation

Our goal is to ﬁnd model parameters Ψ = {ri , gi j , `n
i (b)}, such that the likelihood Pr{O | Ψ}
of observation sequence O is maximized. According to the EM algorithm, we can ﬁnd such
values by iterating over models
Ψnew = arg max
ψ ∑
S∈S
where S is the set of all possible hidden state sequences. The product of equations 2 and 4
over all events is proportional to the combined likelihood Pr{S, O |ψ}:
Pr{S, O |ψ} ∼ ∏
aSe−1 Se (∆te ) pSe (Oe ).
e

Pr{S | O, Ψold} ln(Pr{S, O |ψ}) ,

(5)

ξj j (e) (−r∆te ) + ∑
e, i 6= j

Because of the logarithm in equation 5, the maximization over escape rates can be separated
from the maximization over conditional intensity functions. We deﬁne the abbreviations
ξi j (e) = Pr{Se−1 = i, Se = j | O, Ψold} and γi (e) = Pr{Se = i | O, Ψold} for the posterior
probabilities appearing in equation 5. In practice, both expressions are computed in the
expectation step by the classic forward-backward algorithm [8], using equations 2 and 4 as
the transition probabilities. With the abbreviations deﬁned above, equation 5 is split to
r  ∑
ξi j (e) ln[1 − exp(−r∆te )]!
rnew
j = arg max
e
` 
γi (e)
i (b)new = arg max
`n
−` ∑


e
g (cid:18)ln g ∑
ξi j (e)(cid:19) with gnew
gnew
ii = 0 and ∑
i j = arg max
e
j
In order to perform the maximization in equation 6, we compute its derivative with respect
to r and set it to zero:
ξj j (e)∆te + 

 ∑
∆te −

i 6= j

e (b) kBn(b)k + ln ` ∑
γi (e) f n
e: νe=n ∧
sup Φn
e ∈Bn(b)

∆te
j ∆te(cid:17)
1 − exp (cid:16)−rnew

0 = ∑
e

(6)

(7)

(8)

gnew
i j = 1.

ξi j (e)

This equation cannot be solved analytically, but being just a one dimensional optimiza-
tion problem, a solution can be found using numerical methods, such as the Levenberg-
Marquardt algorithm. The singularity in case of ∆te = 0, which arises when two or more
spikes occur at the same time, needs the special treatment of replacing the respective frac-
tion by its limit: 1/rnew
.
i

To obtain the reestimation formula for the discretized CIFs, equation 7’s derivative with
respect to ` is set to zero. The result can be solved directly and yields
γi (e) . ∑
e

i (b)new = ∑
`n
e: νe=n ∧
sup Φn
e ∈Bn(b)

γi (e) f n
e (b) kBn(b)k.

Finally, to obtain the reestimation formula for the conditional hidden state transition prob-
abilities gi j , we solve equation 8 using Lagrange multipliers, resulting in
ξi j (e) . ∑
e, k 6=i
4 Application to spike trains from the sleeping songbird

gnew
i 6= j = ∑
e

ξik (e).

We have applied our model to spike train data from sleeping songbirds [9]. It has been
found that during sleep, neurons in vocal premotor area RA exhibit spontaneous activity
that at times resembles premotor activity during singing [10, 9].

We train our model on the spike train of a single RA neuron in the sleeping bird with
Nbins = 100, where the ﬁrst bin extends from the sample time to 1ms and the consecutive
99 steps are logarithmically spaced up to the largest ISI. After convergence, we ﬁnd that
the ISI pdfs associated with the two hidden states qualitatively agree with the pdfs recorded
in the awake non-singing bird and the awake singing bird, respectively, Figure 2. ISI pdfs
were derived from the CIFs by using equation 1. For the state-conditional ISI histograms
we ﬁrst ran the Viterbi algorithm to ﬁnd the most likely hidden-state sequence and then
sorted spikes into two groups, for which the ISIs histograms were computed.

We ﬁnd that sleep-related activity in the RA neuron of Figure 2 is best described by random
switching between a singing-like state of lifetime τ1 = 1.18s ± 0.38s and an awake, non-
singing-like state of lifetime τ2 = 2.26s ± 0.42s. Standard deviations of lifetime estimates
were computed by dividing the spike train into 30 data windows of 10s duration each
and computing the Jackknife variance [11] on the truncated spike trains. The difference
between the singing-like state in our model and the true singing ISI pdf shown in Figure 2
is more likely due to generally reduced burst rates during sleep, rather than to a particularity
of the examined neuron.

Next we applied our model to simultaneous recordings from pairs of RA neurons. By ﬁtting
two separate models (with identical phase binning) to the two spike trains, and after running
the Viterbi algorithm to ﬁnd the most likely hidden state sequences, we ﬁnd good agreement
between the two sequences, Figure 3 (top row) and 4c. The correspondence of hidden state
sequences suggests a common network mechanism for the generation of the singing-like
states in both neurons. We thus applied a single model to both spike trains and found
again good agreement with hidden-state sequences determined for the separate models,
Figure 3 (bottom row) and 4f. The lifetime histograms for both states look approximatively
exponential, justifying our assumption for the state dynamics, Figure 4g and h.

For the model trained on neuron one we ﬁnd lifetimes τ1 = 0.63s ± 0.37s and τ2 =
1.71s ± 0.45s, and for the model trained on neuron two we ﬁnd τ1 = 0.42s ± 0.11s
and τ2 = 1.23s ± 0.17s. For the combined model, lifetimes are τ1 = 0.58s ± 0.25s and

(a)

20

15

10

5

]
%
[

.
b
o
r
p

(b)

20

15

10

5

]
%
[

.
b
o
r
p

(c)

20

15

10

5

]
%
[

.
b
o
r
p

0
100

101
102
ISI [ms]

103

0
100

101
102
ISI [ms]

103

0
100

101
102
ISI [ms]

103

Figure 2: (a): The two state-conditional ISI histograms of an RA neuron during sleep are
shown by the red and green curves, respectively. Gray patches represent Jackknife standard
deviations. (b): After waking up the bird by pinching his tail, the new ISI histogram shown
by the gray area becomes almost indistinguishable from the ISI histogram of state 1 (green
line). (c): In comparison to the average ISI histogram of many RA neurons during singing
(shown by the gray area, reproduced from [12]), the ISI histogram corresponding to state 2
(red line) is shifted to the right, but looks otherwise qualitatively similar.

τ2 = 1.13s ± 0.15s. Thus, hidden-state switching seems to occur more frequently in the
combined model. The reason for this increase might be that evidence for the song-like
state appears more frequently with two neurons, as a single neuron might not be able to
indicate song-like ﬁring statistics with high temporal ﬁdelity.

We have also analysed the correlations between state dynamics in the different models. The
hidden state function S(t ) is a binary function that equals one when in hidden state 1 and
zero when in state 2. For the case where we modelled the two spike trains separately, we
have two such hidden state functions, S1 (t ) for neuron one and S2 (t ) for neuron two. We
ﬁnd that all correlation functions CSS1 (t ), CSS2 (t ), and CS1 S2 (t ), have a peak at zero time
lag, with a high peak correlation of about 0.7, Figure 4c and f (the correlation function is
deﬁned as the cross-covariance function divided by the autocovariance functions).

We tested whether our model is a good generative model for the observed spike trains by
applying the time rescaling theorem, after which the ISIs of a good generative model with
known CIFs should reduce to a Poisson process with unit rate, which, after another trans-
formation, should lead to a uniform probability density in the interval (0, 1) [4]. Performing
this test, we found that the transformed ISI densities of the combined model are uniform,
thus validating our model (95% Kolmogorov-Smirnov test, Figure 4i).

5 Discussion

We have presented a mixed-state Markov model for point processes, assuming generation
by random switching between renewal processes. Our algorithm is suited for systems in
which neurons make discrete state transitions simultaneously. Previous attempts of ﬁtting
spike train data with Markov models exhibited weaknesses due to time binning. With large
time bins and the number of spikes per bin treated as observables [13, 14], state transitions
can only be detected when they are accompanied by ﬁring rate changes. In our case, RA
neurons have a roughly constant ﬁring rate throughout the entire recording, and so such
approaches fail.

We were able to model the hidden states in continuous time, but had to bin the ISIs in
order to deal with limited data.
In principle, the algorithm can operate on any binning
scheme for the ISIs. Our choice of logarithmic bins keeps the number of parameters small
(proportional to Nbins ), but preserves a constant temporal resolution.

The hidden-state dynamics form Poisson processes characterized by a lifetime. By esti-

]
z
H
[
R
F
I

]
z
H
[
R
F
I

        
103
        
102
        
        
101
        
100
        
        
102
        
        
101
        
100
        
0
        
103
        
102
        
        
101
        
100
        
        
102
        
        
101
        
100
        
0

5

5

10

15

20

25

30

10

15
Time [sec]

20

25

30

Figure 3: Shown are the instantaneous ﬁring rate (IFR) functions of two simultaneously
recorded RA neurons (at any time, the IFR corresponds to the inverse of the current ISI).
The green areas show the times when in the ﬁrst (awake-like) hidden state, and the red
areas when in the song-like hidden state. The top two rows show the result of computing
two independent models on the two neurons, whereas the bottom rows show the result of a
single model.

(a)

15
]
%
10
[
.
b
o
r
p

5

0
100

0
0
1
/
s
e
k
i
p
s
#

(d)

3

2

1

0
100

101
102
ISI [ms]

103

101
102
ISI [ms]

103

(b)

15
]
%
10
[
.
b
o
r
p

5

0
100

0
0
1
/
s
e
k
i
p
s
#

(e)

3

2

1

0
100

101
102
ISI [ms]

103

101
102
ISI [ms]

103

30
s
20
e
t
a
t
s
10
#

0

(g)

101
102
103
104
State duration [ms]

30
s
20
e
t
a
t
s
10
#

0

(h)

101
102
103
104
State duration [ms]

.8

.4

0

n
o
i
t
a
l
e
r
r
o
c

.8

.4

0

n
o
i
t
a
l
e
r
r
o
c

.
t
s
i
d
.02
.
f
i
n
u
o
t

0

.
f
f
i
d

0

(c)

−0.2

0

0.2

−10

−5
5 10
0
∆t [sec]

(f)

−0.2

0

0.2

−10

−5
0
5 10
∆t [sec]

(i)

1/3
2/3
Our model

1

Figure 4: (a) and (b): State-conditional ISI pdfs for each of the two neurons. (d) and (e): ISI
histograms (blue and yellow) for neurons 1 and 2, respectively, as well as state-conditional
(g) and (h): State lifetime
ISI histograms (red and green), computed as in Figure 2a.
histograms for the song-like state (red) and for the awake-like state (green). Theoretical
(exponential) histograms with escape rates r1 and r2 (ﬁne black lines) show good agreement
with the measured histograms, especially in F. (c): Correlation between state functions of
(f): Correlation between the state functions of the combined
the two separate models.
(i): Kolmogorov-
model with separate model 1 (blue) and separate model 2 (yellow).
Smirnov plot after time rescaling. After transforming the ISIs, the resulting densities for
both neurons remain within the 95% conﬁdence bounds of the uniform density (gray area).
In (a)–(c) and (f)–(h), Jackknife standard deviations are shown by the gray areas.

mating this lifetime, we hope it might be possible to form a link between the hidden states
and the underlying physical process that governs the dynamics of switching. Despite the
apparent limitation of Poisson statistics, it is a simple matter to generalize our model to
hidden state distributions with long tails (e.g., power-law lifetime distributions): By cas-
cading many hidden states into a chain (with ﬁxed CIFs), a power-law distribution can be
approximated by the combination of multiple exponentials with different lifetimes. Our
code is available at http://www.ini.unizh.ch/∼rich/software/.

Acknowledgements

We would like to thank Sam Roweis for advice on Hidden Markov models and Maria
Minkoff for help with the manuscript. R. H. is supported by the Swiss National Science
Foundation. M. D. is supported by Stiftung der Deutschen Wirtschaft.

References

[1] Z. N ´adasdy, H. Hirase, A. Czurk ´o, J. Csicsv ´ari, and G. Buzs ´aki. Replay and time compression
of recurring spike sequences in the hippocampus. J Neurosci, 19(21):9497–9507, Nov 1999.

[2] K. G. Thompson, D. P. Hanes, N. P. Bichot, and J. D. Schall. Perceptual and motor processing
stages identiﬁed in the activity of macaque frontal eye ﬁeld neurons during visual search. J
Neurophysiol, 76(6):4040–4055, Dec 1996.

[3] R. Cossart, D. Aronov, and R. Yuste. Attractor dynamics of network UP states in the neocortex.
Nature, 423(6937):283–288, May 2003.

[4] E. N. Brown, R. Barbieri, V. Ventura, R. E. Kass, and L. M. Frank. The time-rescaling theorem
and its application to neural spike train data analysis. Neur Comp, 14(2):325–346, Feb 2002.

[5] J. Deppisch, K. Pawelzik, and T. Geisel. Uncovering the synchronization dynamics from corre-
lated neuronal activity quantiﬁes assembly formation. Biol Cybern, 71(5):387–399, 1994.

[6] A. M. Fraser and A. Dimitriadis. Forecasting probability densities by using hidden Markov
models with mixed states. In Weigend and Gershenfeld, editors, Time Series Prediction: Fore-
casting the Future and Understanding the Past, pages 265–82. Addison-Wesley, 1994.

[7] A. Berchtold. The double chain Markov model. Comm Stat Theor Meths, 28:2569–2589, 1999.

[8] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recog-
nition. Proc IEEE, 77(2):257–286, Feb 1989.

[9] R. H. R. Hahnloser, A. A. Kozhevnikov, and M. S. Fee. An ultra-sparse code underlies the
generation of neural sequences in a songbird. Nature, 419(6902):65–70, Sep 2002.

[10] A. S. Dave and D. Margoliash. Song replay during sleep and computational rules for sensori-
motor vocal learning. Science, 290(5492):812–816, Oct 2000.

[11] D. J. Thomson and A. D. Chave. Jackknifed error estimates for spectra, coherences, and transfer
functions.
In Simon Haykin, editor, Advances in Spectrum Analysis and Array Processing,
volume 1, chapter 2, pages 58–113. Prentice Hall, 1991.

[12] A. Leonardo and M. S. Fee. Ensemble coding of vocal control in birdsong.
25(3):652–661, Jan 2005.

J Neurosci,

[13] G. Radons, J. D. Becker, B. D ¨ulfer, and J. Kr ¨uger. Analysis, classiﬁcation, and coding of
multielectrode spike trains with hidden Markov models. Biol Cybern, 71(4):359–373, 1994.

[14] I. Gat, N. Tishby, and M. Abeles. Hidden Markov modelling of simultaneously recorded cells
in the associative cortex of behaving monkeys. Network: Computation in Neural Systems,
8(3):297–322, 1997.


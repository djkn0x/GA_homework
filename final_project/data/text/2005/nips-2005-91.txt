A PAC-Bayes approach to the Set
Covering Machine

Fran¸cois Laviolette, Mario Marchand
IFT-GLO, Universit´e Laval
Sainte-Foy (QC) Canada, G1K-7P4
given name.surname@ift.ulaval.ca

Mohak Shah
SITE, University of Ottawa
Ottawa, Ont. Canada,K1N-6N5
mshah@site.uottawa.ca

Abstract

We design a new learning algorithm for the Set Covering Ma-
chine from a PAC-Bayes perspective and propose a PAC-Bayes
risk bound which is minimized for classiﬁers achieving a non trivial
margin-sparsity trade-oﬀ.

1 Introduction

Learning algorithms try to produce classiﬁers with small prediction error by trying
to optimize some function that can be computed from a training set of examples and
a classiﬁer. We currently do not know exactly what function should be optimized
but several forms have been proposed. At one end of the spectrum, we have the
set covering machine (SCM), proposed by Marchand and Shawe-Taylor (2002), that
tries to ﬁnd the sparsest classiﬁer making few training errors. At the other end, we
have the support vector machine (SVM), proposed by Boser et al. (1992), that tries
to ﬁnd the maximum soft-margin separating hyperplane on the training data. Since
both of these learning machines can produce classiﬁers having small prediction error,
we have recently investigated (Laviolette et al., 2005) if better classiﬁers could be
found by learning algorithms that try to optimize a non-trivial function that depends
on both the sparsity of a classiﬁer and the magnitude of its separating margin. Our
main result was a general data-compression risk bound that applies to any algorithm
producing classiﬁers represented by two complementary sources of information: a
subset of the training set, called the compression set, and a message string of
additional information.
In addition, we proposed a new algorithm for the SCM
where the information string was used to encode radius values for data-dependent
balls and, consequently, the location of the decision surface of the classiﬁer. Since
a small message string is suﬃcient when large regions of equally good radius values
exist for balls, the data compression risk bound applied to this version of the SCM
exhibits, indirectly, a non-trivial margin-sparsity trade-oﬀ. Moreover, this version
of the SCM currently suﬀers from the fact that the radius values, used in the ﬁnal
classiﬁer, depends on a a priori chosen distance scale R. In this paper, we use a new
PAC-Bayes approach, that applies to the sample-compression setting, and present a
new learning algorithm for the SCM that does not suﬀer from this scaling problem.
Moreover, we propose a risk bound that depends more explicitly on the margin
and which is also minimized by classiﬁers achieving a non-trivial margin-sparsity
trade-oﬀ.

2 Deﬁnitions
We consider binary classiﬁcation problems where the input space X consists of an
arbitrary subset of Rn and the output space Y = {0, 1}. An example z def= (x, y)
is an input-output pair where x ∈ X and y ∈ Y . In the probably approximately
correct (PAC) setting, we assume that each example z is generated independently
according to the same (but unknown) distribution D. The (true) risk R(f ) of a
classiﬁer f : X → Y is deﬁned to be the probability that f misclassiﬁes z on a
random draw according to D :
R(f ) def= Pr(x,y)∼D (f (x) (cid:54)= y) = E(x,y)∼D I (f (x) (cid:54)= y)
where I (a) = 1 if predicate a is true and 0 otherwise. Given a training set
S = (z1 , . . . , zm ) of m examples, the task of a learning algorithm is to construct
a classiﬁer with the smallest possible risk without any information about D . To
achieve this goal, the learner can compute the empirical risk RS (f ) of any given
m(cid:88)
classiﬁer f according to:
i=1

I (f (xi ) (cid:54)= yi ) def= E(x,y)∼S I (f (x) (cid:54)= y)

RS (f ) def=

1
m

,

hi,ρ (x) def=

We focus on learning algorithms that construct a conjunction (or disjunction) of
features called data-dependent bal ls from a training set. Each data-dependent bal l
is deﬁned by a center and a radius value. The center is an input example xi chosen
(cid:189)
among the training set S . For any test example x, the output of a ball h, of radius
ρ and centered on example xi , and is given by
if d(x, xi ) ≤ ρ
otherwise

yi
¯yi
where ¯yi denotes the boolean complement of yi and d(x, xi ) denotes the distance
between the two points. Note that any metric can be used for the distance here.
To specify a conjunction of bal ls we ﬁrst need to list all the examples that participate
as centers for the balls in the conjunction. For this purpose, we use a vector i def=
(i1 , . . . , i|i| ) of indices ij ∈ {1, . . . , m} such that i1 < i2 < . . . < i|i| where |i| is the
number of indices present in i (and thus the number of balls in the conjunction).
To complete the speciﬁcation of a conjunction of balls, we need a vector ρρρ =
(ρi1 , ρi2 , . . . , ρi|i| ) of radius values where ij ∈ {1, . . . , m} for j ∈ {1, . . . , |i|}.
(cid:189)
On any input example x, the output Ci,ρρρ (x) of a conjunction of balls is given by:
1 if hj,ρj (x) = 1 ∀j ∈ i
0 if ∃j ∈ i : hj,ρj (x) = 0
Finally, any algorithm that builds a conjunction can be used to build a disjunction
just by exchanging the role of the positive and negative labelled examples. Due to
lack of space, we describe here only the case of a conjunction.

Ci,ρρρ (x) def=

3 A PAC-Bayes Risk Bound

The PAC-Bayes approach, initiated by McAllester (1999a), aims at providing PAC
guarantees to “Bayesian” learning algorithms. These algorithms are speciﬁed in
terms of a prior distribution P over a space of classiﬁers that characterizes our

prior belief about good classiﬁers (before the observation of the data) and a pos-
terior distribution Q (over the same space of classiﬁers) that takes into account
the additional information provided by the training data. A remarkable result that
came out from this line of research, known as the “PAC-Bayes theorem”, provides
a tight upper bound on the risk of a stochastic classiﬁer called the Gibbs classiﬁer .
Given an input example x, the label GQ (x) assigned to x by the Gibbs classiﬁer
is deﬁned by the following process. We ﬁrst choose a classiﬁer h according to the
posterior distribution Q and then use h to assign the label h(x) to x. The PAC-
Bayes theorem was ﬁrst proposed by McAllester (1999b) and later improved by
others (see Langford (2005) for a survey). However, for all these versions of the
PAC-Bayes theorem, the prior P must be deﬁned without reference to the training
data. Consequently, these theorems cannot be applied to the sample-compression
setting where classiﬁers are partly described by a subset of the training data (as for
the case of the SCM).
In the sample compression setting, each classiﬁer is described by a subset Si of the
training data, called the compression set, and a message string σ that represents
the additional information needed to obtain a classiﬁer.
In other words, in this
setting, there exists a reconstruction function R that outputs a classiﬁer R(σ, Si )
when given an arbitrary compression set Si and a message string σ .
Given a training set S , the compression set Si ⊆ S is deﬁned by a vector of indices
i def= (i1 , . . . , i|i| ) that points to individual examples in S . For the case of a conjunc-
tion of balls, each j ∈ i will point to a training example that is used for a ball center
and the message string σ will be the vector ρρρ of radius values (deﬁned above) that
are used for the balls. Hence, given Si and ρρρ, the classiﬁer obtained from R(ρρρ, Si )
is just the conjunction Ci,ρρρ deﬁned previously.1
Recently, Laviolette and Marchand (2005) have extended the PAC-Bayes theorem
to the sample-compression setting. Their proposed risk bound depends on a data-
independent prior P and a data-dependent posterior Q that are both deﬁned on
I × M where I denotes the set of the 2m possible index vectors i and M denotes,
in our case, the set of possible radius vectors ρρρ. The posterior Q is used by a
stochastic classiﬁer, called the sample-compressed Gibbs classiﬁer GQ , deﬁned as
follows. Given a training set S and given a new (testing) input example x, a sample-
compressed Gibbs classiﬁer GQ chooses randomly (i, ρρρ) according to Q to obtain
classiﬁer R(ρρρ, Si ) which is then used to determine the class label of x.
In this paper we focus on the case where, given any training set S , the learner returns
a Gibbs classiﬁer deﬁned with a posterior distribution Q having all its weight on a
single vector i. Hence, a single compression set Si will be used for the ﬁnal classiﬁer.
However, the radius ρi for each i ∈ i will be chosen stochastically according to the
posterior Q. Hence we consider posteriors Q such that Q(i(cid:48) , ρρρ) = I (i = i(cid:48) )Qi (ρρρ)
where i is the vector of indices chosen by the learner. Hence, given a training set
S , the true risk R(GQi ) of GQi and its empirical risk RS (GQi ) are deﬁned by
R(R(ρρρ, Si ))
(R(ρρρ, Si )) ,
; RS (GQi ) def= E
R(GQi ) def= E
RSi
ρρρ∼Qi
ρρρ∼Qi
where i denotes the set of indices not present in i. Thus, i ∩ i = ∅ and i ∪ i =
(1, . . . , m).
In contrast with the posterior Q, the prior P assigns a non zero weight to several
vectors i. Let PI (i) denote the prior probability P assigned to vector i and let Pi (ρρρ)

1We assume that the examples in Si are ordered as in S so that the kth radius value
in ρρρ is assigned to the kth example in Si .

PI (i) =

+ (1 − q) ln

kl(q(cid:107)p) def= q ln q
p

denote the probability density function associated with prior P given i. The risk
bound depends on the Kullback-Leibler divergence KL(Q(cid:107)P ) between the posterior
Q and the prior P which, in our case, gives
ln Qi (ρρρ)
KL(Qi (cid:107)P ) = E
PI (i)Pi (ρρρ) .
ρρρ∼Qi
For these classes of posteriors Q and priors P , the PAC-Bayes theorem of Laviolette
and Marchand (2005) reduces to the following simpler version.
(cid:164) (cid:180)
(cid:179)
Theorem 1 (Laviolette and Marchand (2005)) Given al l our previous deﬁni-
(cid:163)
tions, for any prior P and for any δ ∈ (0, 1]
KL(Qi (cid:107)P ) + ln m+1
∀ Qi : kl(RS (GQi )(cid:107)R(GQi )) ≤
Pr
S∼Dm
δ
where

1 − q
1 − p
To obtain a bound for R(GQi ) we need to specify Qi (ρρρ), PI (i), and Pi (ρρρ).
Since all vectors i having the same size |i| are, a priori, equally “good”, we choose
1(cid:161)m|i|
(cid:162) p(|i|)
(cid:80)m
d=0 p(d) = 1. We could choose p(d) = 1/(m + 1) for d ∈
for any p(·) such that
{0, 1, . . . , m} if we have complete ignorance about the size |i| of the ﬁnal classiﬁer.
But since the risk bound will deteriorate for large |i|, it is generally preferable to
choose, for p(d), a slowly decreasing function of d.
For the speciﬁcation of Pi (ρρρ), we assume that each radius value, in some predeﬁned
interval [0, R], is equally likely to be chosen for each ρi such that i ∈ i. Here R is
some “large” distance speciﬁed a priori. For Qi (ρρρ), a margin interval [ai , bi ] ⊆ [0, R]
(cid:182)|i|
(cid:181)
(cid:89)
(cid:89)
of equally good radius values is chosen by the learner for each i ∈ i. Hence, we choose
1
1
1
bi − ai
R
R
i∈i
i∈i
Therefore, the Gibbs classiﬁer returned by the learner will draw each radius ρi
uniformly in [ai , bi ]. A deterministic classiﬁer is then speciﬁed by ﬁxing each radius
values ρi ∈ [ai , bi ]. It is tempting at this point to choose ρi = (ai + bi )/2 ∀i ∈ i (i.e.,
in the middle of each interval). However, we will see shortly that the PAC-Bayes
theorem oﬀers a better guarantee for another type of deterministic classiﬁer.
(cid:181)
(cid:181)
(cid:181)
(cid:182)
(cid:182)
(cid:182)
Consequently, with these choices for Qi (ρρρ), PI (i), and Pi (ρρρ), the KL divergence
(cid:88)
between Qi and P is given by
K L(Qi (cid:107)P ) = ln
m
|i|
i∈i
Notice that the KL divergence is small for small values of |i| (whenever p(|i|) is not
too small) and for large margin values (bi − ai ). Hence, the KL divergence term in
Theorem 1 favors both sparsity (small |i|) and large margins. Hence, in practice,
the minimum might occur for some GQi that sacriﬁces sparsity whenever larger
margins can be found.

R
bi − ai

.

Pi (ρρρ) =

=

+ ln

1
p(|i|)

; Qi (ρρρ) =

.

1
m−|i|

≥ 1 − δ ,

.

+

ln

ai ,bi (x) .
ζ i

Since the posterior Q is identiﬁed by i and by the intervals [ai , bi ] ∀i ∈ i, we will
ab where a and b are the vectors formed
now refer to the Gibbs classiﬁer GQi by Gi
by the unions of ai s and bi s respectively. To obtain a risk bound for Gi
ab , we need
to ﬁnd a closed-form expression for RS (Gi
ab ). For this task, let U [a, b] denote the
uniform distribution over [a, b] and let σ i
a,b (x) be the probability that a ball with
 1
center xi assigns to x the class label yi when its radius ρ is drawn according to
U [a, b]:
b−d(x,xi )
b−a
0
(cid:189)

if d(x, xi ) ≤ a
if a ≤ d(x, xi ) ≤ b
if d(x, xi ) ≥ bi .

a,b (x) def= Prρ∼U [a,b] (hi,ρ (x) = yi ) =
σ i

Therefore,

ab ) =
RS (Gi

yi = 1
a,b (x)
if
σ i
a,b (x) def= Prρ∼U [a,b] (hi,ρ (x) = 1) =
ζ i
1 − σ i
yi = 0 .
a,b (x)
if
ab (x) denote the probability that Ci,ρρρ (x) = 1 when each ρi ∈ ρρρ are drawn
(cid:89)
Now let Gi
according to U [ai , bi ]. We then have
ab (x) =
Gi
i∈i
Consequently, the risk R(x,y) (Gi
ab ) on a single example (x, y) is given by Gi
ab (x) if
y = 0 and by 1 − Gi
ab (x) otherwise. Therefore
ab (x)) + (1 − y)Gi
ab (x) = (1 − 2y)(Gi
ab ) = y(1 − Gi
ab (x) − y) .
R(x,y) (Gi
(cid:88)
ab ) of the Gibbs classiﬁer Gi
Hence, the empirical risk RS (Gi
ab is given by
1
ab (xj ) − yj ) .
(1 − 2yj )(Gi
m − |i|
j∈i
ab (xj ) → yj ∀j ∈ i.
From this expression we see that RS (Gi
ab ) is small when Gi
ab (xj ) ≈ 1/2 should therefore be avoided.
Training points where Gi
The PAC-Bayes theorem below provides a risk bound for the Gibbs classiﬁer Gi
ab .
ab just performs a ma jority vote under the same posterior
Since the Bayes classiﬁer B i
distribution as the one used by Gi
ab , we have that B i
ab (x) = 1 iﬀ Gi
ab (x) > 1/2.
From the above deﬁnitions, note that the decision surface of the Bayes classiﬁer,
given by Gi
ab (x) = 1/2, diﬀers from the decision surface of classiﬁer Ciρρρ when
ρi = (ai + bi )/2 ∀i ∈ i. In fact there does not exists any classiﬁer Ciρρρ that has the
same decision surface as Bayes classiﬁer B i
ab . From the relation between B i
ab and
ab ) ≤ 2R(x,y) (Gi
ab , it also follows that R(x,y) (B i
ab ) for any (x, y). Consequently,
Gi
ab ) ≤ 2R(Gi
(cid:80)m
R(B i
ab ). Hence, we have the following theorem.
Theorem 2 Given al l our previous deﬁnitions, for any δ ∈ (0, 1], for any p satis-
(cid:195)
(cid:189)
(cid:183)
(cid:181)
(cid:182)
fying
d=0 p(d) = 1, and for any ﬁxed distance value R, we have:
(cid:35)(cid:41) (cid:33)
1
(cid:181)
(cid:181)
(cid:182)
(cid:182)
ab ) ≤ sup
∀i, a, b : R(Gi
ab )(cid:107)) ≤
m
(cid:88)
 : kl(RS (Gi
+
ln
m − |i|
|i|
1
R
p(|i|)
bi − ai
i∈i
ab ) ≤ 2R(Gi
ab ) ∀i, a, b.
Furthermore: R(B i

+ ln m + 1
δ

≥ 1 − δ .

PrS∼Dm

+ ln

+

ln

Recall that the KL divergence is small for small values of |i| (whenever p(|i|) is not
too small) and for large margin values (bi − ai ). Furthermore, the Gibbs empirical
ab ) is small when the training points are located far away from the Bayes
risk RS (Gi
ab (xj ) → yj ∀j ∈ i). Consequently, the
decision surface Gi
ab (x) = 1/2 (with Gi
Gibbs classiﬁer with the smal lest guarantee of risk should perform a non trivial
margin-sparsity tradeoﬀ.

4 A Soft Greedy Learning Algorithm

Theorem 2 suggests that the learner should try to ﬁnd the Bayes classiﬁer B i
that uses a small number of balls (i.e., a small |i|), each with a large separating
ab
margin (bi − ai ), while keeping the empirical Gibbs risk RS (Gi
ab ) at a low value. To
achieve this goal, we have adapted the greedy algorithm for the set covering machine
(SCM) proposed by Marchand and Shawe-Taylor (2002). It consists of choosing the
(Boolean-valued) feature i with the largest utility Ui deﬁned as Ui = |Ni | − p |Pi | ,
where Ni is the set of negative examples covered (classiﬁed as 0) by feature i, Pi
is the set of positive examples misclassiﬁed by this feature, and p is a learning
parameter that gives a penalty p for each misclassiﬁed positive example. Once the
feature with the largest Ui is found, we remove Ni and Pi from the training set S
and then repeat (on the remaining examples) until either no more negative examples
are present or that a maximum number of features has been reached.
In our case, however, we need to keep the Gibbs risk on S low instead of the risk
of a deterministic classiﬁer. Since the Gibbs risk is a “soft measure” that uses the
piece-wise linear functions σ i
a,b instead of “hard” indicator functions, we need a
“softer” version of the utility function Ui . Indeed, a negative example that falls in
the linear region of a σ i
a,b is in fact partly covered. Following this observation, let
k be the vector of indices of the examples that we have used as ball centers so far
for the construction of the classiﬁer. Let us ﬁrst deﬁne the covering value C (Gk
(cid:88)
(cid:163)
(cid:164)
ab )
ab by the “amount” of negative examples assigned to class 0 by Gk
of Gk
ab :
C (Gk
(1 − yj )
1 − Gk
def=
ab )
ab (xj )
j∈k
We also deﬁne the positive-side error E (Gk
(cid:88)
(cid:163)
(cid:164)
ab ) of Gk
ab as the “amount” of positive
examples assigned to class 0 :
E (Gk
1 − Gk
ab (xj )
ab )
j∈k
We now want to add another ball, centered on an example with index i, to obtain
a new vector k(cid:48) containing this new index in addition to those present in k. Hence,
we now introduce the covering contribution of ball i (centered on xi ) as
(cid:88)
(cid:163)
(cid:163)
(cid:164)
(cid:164)
a(cid:48)b(cid:48) ) − C (Gk
def= C (Gk(cid:48)
C k
ab )
ab (i)
1 − ζ i
1 − ζ i
= (1 − yi )
ai ,bi (xj )
ai ,bi (xi ) Gk
ab (xi )
j∈k(cid:48)
and the positive-side error contribution of ball i as
(cid:88)
(cid:163)
(cid:164)
a(cid:48)b(cid:48) ) − E (Gk
E k
def= E (Gk(cid:48)
ab (i)
ab )
1 − ζ i
ab (xi )
ai ,bi (xi ) Gk
j∈k(cid:48)

(cid:163)
(cid:164)
1 − ζ i
ai ,bi (xj )

(1 − yj )

ab (xj ) .
Gk

def=

yj

.

.

= yi

+

yj

+

ab (xj ) ,
Gk

Typically, the covering contribution of ball i should increase its “utility” and its
positive-side error should decrease it. Hence, we deﬁne the utility U k
ab (i) of adding
bal l i to Gk
ab as

ab (i) − pE k
def= C k
ab (i)
ab (i) ,
U k
where parameter p represents the penalty of misclassifying a positive example. For
a ﬁxed value of p, the “soft greedy” algorithm simply consists of adding, to the
current Gibbs classiﬁer, a ball with maximum added utility until either the maxi-
mum number of possible features (balls) has been reached or that all the negative
examples have been (totally) covered. It is understood that, during this soft greedy
(cid:80)
algorithm, we can remove an example (xj , yj ) from S whenever it is totally covered.
This occurs whenever Gk
ab (xj ) = 0.
i∈i ln(R/(bi − ai )), present in the risk bound of Theorem 2, favors “soft
The term
balls” having large margins bi − ai . Hence, we introduce a margin parameter γ ≥ 0
that we use as follows. At each greedy step, we ﬁrst search among balls having
bi − ai = γ . Once such a ball, of center xi , having maximum utility has been found,
we try to increase further its utility be searching among all possible values of ai
and bi > ai while keeping its center xi ﬁxed2 . Both p and γ will be chosen by cross
validation on the training set.
We conclude this section with an analysis of the running time of this soft greedy
learning algorithm for ﬁxed p and γ . For each potential ball center, we ﬁrst sort the
m − 1 other examples with respect to their distances from the center in O(m log m)
time. Then, for this center xi , the set of ai values that we examine are those
speciﬁed by the distances (from xi ) of the m − 1 sorted examples3 . Since the
examples are sorted, it takes time ∈ O(km) to compute the covering contributions
and the positive-side error for al l the m − 1 values of ai . Here k is the largest
number of examples falling into the margin. We are always using small enough γ
values to have k ∈ O(log m) since, otherwise, the results are terrible. It therefore
takes time ∈ O(m log m) to compute the utility values of all the m − 1 diﬀerent
balls of a given center. This gives a time ∈ O(m2 log m) to compute the utilities
for all the possible m centers. Once a ball with a largest utility value has been
chosen, we then try to increase further its utility by searching among O(m2 ) pair
values for (ai , bi ). We then remove the examples covered by this ball and repeat
the algorithm on the remaining examples. It is well known that greedy algorithms
of this kind have the following guarantee: if there exist r balls that covers all the
m examples, the greedy algorithm will ﬁnd at most r ln(m) balls. Since we almost
always have r ∈ O(1), the running time of the whole algorithm will almost always
be ∈ O(m2 log2 (m)).

5 Empirical Results on Natural Data

We have compared the new PAC-Bayes learning algorithm (called here SCM-PB),
with the old algorithm (called here SCM). Both of these algorithms were also com-
pared with the SVM equipped with a RBF kernel of variance σ2 and a soft margin
parameter C . Each SCM algorithm used the L2 metric since this is the metric
present in the argument of the RBF kernel. However, in contrast with Laviolette
et al. (2005), each SCM was constrained to use only balls having centers of the same
class (negative for conjunctions and positive for disjunctions).

2The possible values for ai and bi are deﬁned by the location of the training points.
3Recall that for each value of ai , the value of bi is set to ai + γ at this stage.

Table 1: SVM and SCM results on UCI data sets.
SCM
SVM results
Data Set
σ2
errs
SVs
train
12
38
5
343
170
.17
169
62
58
282
2
353
22
51
.17
107
23
64
.17
150
144
1
81
39
27
53
25
235

C
1
2
100
10
1
2
1

test
340
175
300
107
147
150
200

Name
breastw
bupa
credit
glass
heart
haberman
USvotes

errs
15
66
51
29
26
39
13

b
1
5
3
5
1
1
10

SCM-PB
γ
errs
10
.08
.1
67
55
.09
19
.04
28
0
.2
38
12
.14

b
4
6
11
16
1
1
18

Each algorithm was tested the UCI data sets of Table 1. Each data set was ran-
domly split in two parts. About half of the examples was used for training and
the remaining set of examples was used for testing. The corresponding values for
these numbers of examples are given in the “train” and “test” columns of Table 1.
The learning parameters of all algorithms were determined from the training set
only. The parameters C and γ for the SVM were determined by the 5-fold cross
validation (CV) method performed on the training set. The parameters that gave
the smallest 5-fold CV error were then used to train the SVM on the whole training
set and the resulting classiﬁer was then run on the testing set. Exactly the same
method (with the same 5-fold split) was used to determine the learning parameters
of both SCM and SCM-PB.
The SVM results are reported in Table 1 where the “SVs” column refers to the
number of support vectors present in the ﬁnal classiﬁer and the “errs” column refers
to the number of classiﬁcation errors obtained on the testing set. This notation is
used also for all the SCM results reported in Table 1.
In addition to this, the
“b” and “γ ” columns refer, respectively, to the number of balls and the margin
parameter (divided by the average distance between the positive and the negative
examples). The results reported for SCM-PB refer to the Bayes classiﬁer only. The
results for the Gibbs classiﬁer are similar. We observe that, except for bupa and
heart, the generalization error of SCM-PB was always smaller than SCM. However,
the only signiﬁcant diﬀerence occurs on USvotes. We also observe that SCM-PB
generally sacriﬁces sparsity (compared to SCM) to obtain some margin γ > 0.

References

B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin
classiﬁers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning
Theory, pages 144–152. ACM Press, 1992.

John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Ma-
chine Learning Research, 6:273–306, 2005.

Fran¸cois Laviolette and Mario Marchand. PAC-Bayes risk bounds for sample-compressed
Gibbs classiﬁers. Proceedings of the 22nth International Conference on Machine Learn-
ing (ICML 2005), pages 481–488, 2005.

Fran¸cois Laviolette, Mario Marchand, and Mohak Shah. Margin-sparsity trade-oﬀ for the
set covering machine. Proceedings of the 16th European Conference on Machine Learning
(ECML 2005); Lecture Notes in Artiﬁcial Intel ligence, 3720:206–217, 2005.

Mario Marchand and John Shawe-Taylor. The set covering machine. Journal of Machine
Learning Reasearch, 3:723–746, 2002.

David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355–363, 1999a.

David A. McAllester. Pac-bayesian model averaging. In COLT, pages 164–170, 1999b.


Sequence and Tree Kernels
with Statistical Feature Mining

Jun Suzuki and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto,619-0237 Japan
{jun, isozaki}@cslab.kecl.ntt.co.jp

Abstract

This paper proposes a new approach to feature selection based on a sta-
tistical feature mining technique for sequence and tree kernels. Since
natural language data take discrete structures, convolution kernels, such
as sequence and tree kernels, are advantageous for both the concept and
accuracy of many natural language processing tasks. However, experi-
ments have shown that the best results can only be achieved when lim-
ited small sub-structures are dealt with by these kernels. This paper dis-
cusses this issue of convolution kernels and then proposes a statistical
feature selection that enable us to use larger sub-structures effectively.
The proposed method, in order to execute efﬁciently, can be embedded
into an original kernel calculation process by using sub-structure min-
ing algorithms. Experiments on real NLP tasks conﬁrm the problem in
the conventional method and compare the performance of a conventional
method to that of the proposed method.

1 Introduction

Since natural language data take the form of sequences of words and are generally analyzed
into discrete structures, such as trees (parsed trees), discrete kernels, such as sequence
kernels [7, 1] and tree kernels [2, 5], have been shown to offer excellent results in the
natural language processing (NLP) ﬁeld. Conceptually, these proposed kernels are deﬁned
as instances of convolution kernels [3, 11], which provides the concept of kernels over
discrete structures.

However, unfortunately, experiments have shown that in some cases there is a critical issue
with convolution kernels in NLP tasks [2, 1, 10]. That is, since natural language data
contain many types of symbols, NLP tasks usually deal with extremely high dimension
and sparse feature space. As a result, the convolution kernel approach can never be trained
effectively, and it behaves like a nearest neighbor rule. To avoid this issue, we generally
eliminate large sub-structures from the set of features used. However, the main reason for
using convolution kernels is that we aim to use structural features easily and efﬁciently.
If their use is limited to only very small structures, this negates the advantages of using
convolution kernels.

This paper discusses this issue of convolution kernels, in particular sequence and tree ker-

nels, and proposes a new method based on statistical signiﬁcant test. The proposed method
deals only with those features that are statistically signiﬁcant for solving the target task,
and large signiﬁcant sub-structures can be used without over-ﬁtting. Moreover, by us-
ing sub-structure mining algorithms, the proposed method can be executed efﬁciently by
embedding it in an original kernel calculation process, which is deﬁned by the dynamic-
programming (DP) based calculation.

2 Convolution Kernels for Sequences and Trees

Convolution kernels have been proposed as a concept of kernels for discrete structures,
such as sequences, trees and graphs. This framework deﬁnes the kernel function between
input objects as the convolution of “sub-kernels”, i.e. the kernels for the decompositions
(cid:80)
(parts or sub-structures) of the objects. Let X and Y be discrete objects. Conceptually,
convolution kernels K (X, Y ) enumerate all sub-structures occurring in X and Y and then
calculate their inner product, which is simply written as: K (X, Y ) = (cid:104)φ(X ), φ(Y )(cid:105) =
i φi (X ) · φi (Y ). φ represents the feature mapping from the discrete object to the feature
space; that is, φ(X ) = (φ1 (X ), . . . , φi (X ), . . .). Therefore, with sequence kernels, input
objects X and Y are sequences, and φi (X ) is a sub-sequence; with tree kernels, X and
Y are trees, and φi (X ) is a sub-tree. Up to now, many kinds of sequence and tree kernels
have been proposed for a variety of different tasks. To clarify the discussion, this paper
basically follows the framework of [1], which proposed a gapped word sequence kernel,
and [5], which introduced a labeled ordered tree kernel.

We can treat that sequence is one of the special form of trees if we say sequences are rooted
by their last symbol and each node has one child each of a previous symbol. Thus, in this
paper, the word ‘tree’ is always including sequence. Let L be a set of ﬁnite symbols. Then,
let Ln be a set of symbols whose sizes are n and P (Ln ) be a set of trees that are constructed
by Ln . The meaning of “size” in this paper is the the number of nodes in a tree. We denote
m=1Lm = Ln
1 ) whose size is n or less, where ∪n
a tree u ∈ P (Ln
1 . Let T be a tree and
sub(T ) be a function that returns a set of all possible sub-trees in T . We deﬁne a function
Cu (t) that returns a constant, λ(0 < λ ≤ 1), if the sub-tree t covers u with the same root
symbol. For example, a sub-tree ‘a-b-c-d’, where ‘a’, ‘b’, ‘c’ and ‘d’ represent symbols
and ‘-’ represents an edge between symbols, covers sub-trees ‘d’, ‘a-c-d’ and ‘b-d’. That
is, Cu (t) = λ if u matches t allowing the node skip, 0 otherwise. We also deﬁne a function
γu (t) that returns the difference of size of sub-trees t and u. For example, if t = a-b-c-d
and u = a-b, then γu (t) = |4 − 2| = 2.
(cid:88)
(cid:88)
(cid:88)
Formally, sequence and tree kernels can be deﬁned as the same form as
u∈P (Ln
t2∈sub(T 2 )
t1∈sub(T 1 )
1 )

K SK,TK (T 1 , T 2 ) =

Cu (t1 )γu (t1 )

Cu (t2 )γu (t2 ) .

(1)

Note that this formula is also including the node skip framework that is generally introduced
only in sequence kernels[7, 1]; λ is the decay factor that handles the gap present in sub-trees
u and t.

Sequence and tree kernels are deﬁned in recursive formula to calculate them efﬁciently
instead of the explicit calculation of Equation (1). Moreover, when implemented, these
kernels can calculated in O(n|T 1 ||T 2 |), where |T | represents the number of nodes in T , by
using the DP technique. Note, that if the kernel does not use size restriction, the calculation
cost becomes O(|T 1 ||T 2 |).

3 Problem of Applying Convolution Kernels to Real tasks

According to the original deﬁnition of convolution kernels, all of the sub-structures are
enumerated and calculated for the kernels. The number of sub-structures in the input ob-
ject usually becomes exponential against input object size. The number of symbols, |L|,
is generally very large number (i.e. more than 10,000) since words are treated as symbols.
Moreover, the appearance of sub-structures (sub-sequences and sub-trees) are highly corre-
lated with that of sub-structures of sub-structures themselves. As a result, the dimension of
feature space becomes extremely high, and all kernel values K (X, Y ) are very small com-
pared to the kernel value of the object itself, K (X, X ). In this situation, the convolution
kernel approach can never be trained effectively, and it will behave like a nearest neighbor
rule; we obtain a result that is very precise but with very low recall. The details of this issue
were described in [2].

To avoid this, most conventional methods use an approach that involves smoothing the
kernel values or eliminating features based on the sub-structure size. For sequence kernels,
[1] use a feature elimination method based on the size of sub-sequence n. This means that
the kernel calculation deals only with those sub-sequences whose length is n or less. As
well as the sequence kernel, [2] proposed a method that restricts the features based on sub-
tree depth for tree kernels. These methods seem to work well on the surface, however, good
results can only be achieved when n is very small, i.e. n = 2 or 3. For example, n = 3
showed the best performance for parsing in the experimental results of [2], and n = 2
showed the best for the text classiﬁcation task in [1]. The main reason for using these
kernels is that they allow us to employ structural features simply and efﬁciently. When
only small-sized sub-structures are used (i.e. n = 2 or 3), the full beneﬁts of the kernels
are missed.

Moreover, these results do not mean that no larger-sized sub-structures are useful. In some
cases we already know that certain larger sub-structures can be signi ﬁcant features for
solving the target problem. That is, signiﬁcant larger sub-structures, which the conventional
methods cannot deal with efﬁciently, should have the possibility of further improving the
performance. The aim of the work described in this paper is to be able to use any signiﬁcant
sub-structure efﬁciently, regardless of its size, to better solve NLP tasks.

4 Statistical Feature Mining Method for Sequence and Tree Kernels

This section proposes a new approach to feature selection, which is based on statistical
signiﬁcant test, in contrast to the conventional methods, which use sub-structure size.

To simplify the discussion, we restrict ourselves to dealing hereafter with the two-
class (positive and negative) supervised classiﬁcation problem.
In our approach, we
test the statistical deviation of all sub-structures in the training samples between the
appearance of positive samples and negative samples, and then, select only the sub-
structures which are larger than a certain threshold τ as features. This allows us
to select only the statistically signiﬁcant sub-structures.
In this paper, we explains
our proposed method by using the chi-squared (χ2 ) value as a statistical metric.
We note, however, we can use many
(cid:80)
types of statistical metrics in our proposed
Table 1: Contingency table and notation
method.
for the chi-squared value
¯c
row
c
(cid:80)
Ouc Ou¯c
Ou
u
¯u
O ¯u
O ¯uc O ¯u¯c
column Oc
O¯c
N

First, we brieﬂy explain how to calculate
the χ2 value by referring to Table 1. c and
¯c represent the names of classes, c for the
positive class and ¯c for the negative class.
Oij , where i ∈ {u, ¯u} and j ∈ {c, ¯c}, rep-

resents the number of samples in each case. Ou¯c , for instance, represents the number
of u that appeared in ¯c. Let N be the total number of training samples. Since N and
(cid:80)
Oc are constant for training samples, χ2 can be obtained as a function of Ou and Ouc .
The χ2 value expresses the normalized deviation of the observation from the expectation:
i∈{u, ¯u},j∈{c,¯c} (Oij − Eij )2 /Eij , where Eij = n · Oi/n · Oj /n, which
chi(Ou , Ouc ) =
represents the expectation. We simply represent chi(Ou , Ouc ) as χ2 (u).
In the kernel calculation with the statistical feature selection, if χ2 (u) < τ holds, that is, u
is not statistically signiﬁcant, then u is eliminated from the features, and the value of u is
presumed to be 0 for the kernel value. Therefore, the sequence and tree kernel with feature
selection (SK,TK+FS) can be deﬁned as follows:
(cid:88)
(cid:88)
(cid:88)
K SK,TK+FS (T 1 , T 2 ) =
u∈{u|τ ≤χ2 (u),u∈P (Ln
1 )}
t2∈sub(T 2 )
t1∈sub(T 1 )

Cu (t2 )γu (t2 ) .

Cu (t1 )γu (t1 )

(2)
The difference with their original kernels is simply the condition of the ﬁrst summation,
which is τ ≤ χ2 (u).
The basic idea of using a statistical metric to select features is quite natural, but it is not
a very attractive approach. We note, however, it is not clear how to calculate that kernels
efﬁciently with a statistical feature selection. It is computationally infeasible to calculate
χ2 (u) for all possible u with a naive exhaustive method. In our approach, we take ad-
vantage of sub-structure mining algorithms in order to calculate χ2 (u) efﬁciently and to
embed statistical feature selection to the kernel calculation. Formally, sub-structure min-
ing is to ﬁnd the complete set, but no-duplication, of all signiﬁcant (generally frequent)
sub-structures from dataset. Speciﬁcally, we apply combination of a sequential pattern
mining technique, PreﬁxSpan [9], and a statistical metric pruning (SMP) method, Apriori
SMP [8]. PreﬁxSpan can substantially reduce the search space of enumerating all signif-
icant sub-sequences. Brieﬂy saying, it ﬁnds any sub-sequences
uw whose size is n, by
searching a single symbol w in the projected database of the sub-sequence (preﬁx) u of
size n − 1. The projected database is a partial database which only contains all post ﬁxes
(pointers in the implementation) of appeared the preﬁx u in the database. It starts search-
ing from n = 1, that is, it enumerates all the signiﬁcant sub-sequences by the recursive
calculation of pattern-growth, searching in the projected database of preﬁx u and adding a
symbol w to u, and preﬁx-projection , making projected database of uw .
Before explaining the algorithm of the proposed kernels, we introduce the upper bound of
u [8]: χ2 (uv) ≤ (cid:98)χ2 (u) = max (chi(Ouc , Ouc ), chi(Ou − Ouc , 0)) . This upper bound
the χ2 value. The upper bound of the χ2 value of a sequence uv , which is the concatenation
indicates that if (cid:98)χ2 (u) < τ holds, no (super-)sequences uv , whose preﬁx is u, can be larger
of sequences u and v , can be calculated by the value of the contingency table of the preﬁx
than threshold, τ ≤ χ2 (uv). In our context, we can eliminate all (super-)sequences uv
from candidates of the feature without the explicit evaluation of uv .
Using this property in the PreﬁxSpan algorithm, we can eliminate to evaluate all the (super-
three conditions: (1) τ ≤ χ2 (uw), (2) τ > χ2 (uw), τ > (cid:98)χ2 (uw), and (3) τ > χ2 (uw),
)sequences uv by evaluating the upper bound of sequence u. After ﬁnding the number of
τ ≤ (cid:98)χ2 (uw). With condition (1), sub-sequence uw is selected as the feature. With condi-
individual symbol w appeared in projected database of u, we evaluate uw in the following
tion (2), uw is pruned, that is, all uwv are also pruned from search space. With condition
(3), uw is not a signi ﬁcant, however, uwv can be a signiﬁcant; thus uw is not selected as
features, however, mining is continue to uwv . Figure 1 shows an example of searching
and pruning the sub-sequences to select signiﬁcant features by the PreﬁxSpan with SMP
algorithm.

Figure 1: Example of searching and pruning the sub-sequences by PreﬁxSpan with SMP
algorithm

Figure 2: Example of the string encoding for trees under the postorder traversal

The famous tree mining algorithm [12] cannot be simply applied as a feature selection
method for the proposed tree kernels, because this tree mining executes preorder search of
trees while tree kernels calculate the kernel in postorder. Thus, we take advantage of the
string (sequence) encoding method for trees and treat them in sequence kernels. Figure 2
shows an example of the string encoding for trees under the postorder traversal. The brack-
ets indicate the hierarchical relation between their left and right hand side nodes. We treat
these brackets as a special symbol during the sequential pattern mining phase. Sub-trees
are evaluated as the same if and only if the string encoded sub-sequences are exactly the
same including brackets. For example, ‘d ) b ) a’ and ‘d b ) a’ are different.

We previously said that sequence can be treated as one of trees. We also encode in the case
of sequence; for example a sequence ’a b c d’ is encoded in ‘((((a) b) c) d)’. That is, we
can deﬁne sequence and tree kernels with our feature selection method in the same form.
(cid:88)
(cid:88)
Sequence and Tree Kernels with Statistical Feature Mining: Sequence and Tree kernels
with our proposed feature selection method is deﬁned in the following equations.
Hn (T 1
j ; D)
K SK,TK+FS (T 1 , T 2 ; D) =
i , T 2
1≤j≤|T 2 |
1≤i≤|T 1 |
D represents the training data, and i and j represent indices of nods in postorder of T 1
j ; D) be a function that returns the sum value of all
and T 2 , respectively. Let Hn (T 1
(cid:88)
i , T 2
j and |u| ≤ n.
i = t2
statistically signiﬁcant common sub-sequences u if t1
Ju (T 1
j ; D),
Hn (T 1
j ; D) =
i , T 2
i , T 2
u∈Γn (T 1
j ;D)
i ,T 2
j ; D) represents a set of sub-sequences, which is |u| ≤ n, that satisfy the
where Γn (T 1
i , T 2
above condition 1. Then, let Ju (T 1
j ; D), J (cid:48)
j ; D) and J (cid:48)(cid:48)
j ; D) be func-
(cid:189)
u (T 1
u (T 1
i , T 2
i , T 2
i , T 2
j ) if uw ∈ (cid:98)Γn (T 1
tions that calculate the value of the common sub-sequences between T 1
i and T 2
j recursively.
J (cid:48)
j ; D) · Iw (t1
j ; D),
Juw (T 1
u (T 1
i , T 2
i , t2
i , T 2
j ) =
i , T 2
0
otherwise,

(3)

(4)

(5)

^abcde+1-1+1-1-1-1...classtraining dataa b c d a ec a e f b c dd b c a eb a c b ba c a dd a b d e c...()2'uc()2ˆ'ucw3.21.54.80.22.51.90.90.95.21.81:22:3Projected database5:26:3bcde2.20.50.50.13.21.51.81.5c0.50.10.50.1dcd0.40.33.21.5ab2.20.51.21.2ae1.51.52.21.5d2.21.5thresholdn=1n=2n=3ce0.20.13.21.53:54:31:32:6Projected database4:56:4Sample id: pointerEx.  2:3Projected databaseselect as a featurepruning3,2,1,continue00.1=t()tc‡u2()()tctc<<uu22ˆ and ,()tc‡u2ˆabcddada(((d (b) d (d a) a) b c) a)1TString encoding underthe postordertraversal :12345678d (b) d a) bb9d a ) bsub-treebdda12367bbda457*abcddad1346789d d (d) a) b c) a)(6)

(7)

(8)

J (cid:48)
j ; D) =
u (T 1
i , T 2

(cid:98)Γn (T 1
where Iw (t1
j ) is a function that returns 1 iff t1
i = w and t2
j = w , and 0 otherwise.
i , t2
j ; D) is a set of sub-sequences, which is |u| ≤ n, that satisfy condition (3). We
i , T 2
1
introduce a special symbol Λ to represent an “empty sequence”, and deﬁne
Λw = w and
|Λw| = 1.
if u = Λ,
and u (cid:54)= Λ,
j = 0
0
(cid:189)
if
j−1 ; D) + J (cid:48)(cid:48)
j−1 , D) otherwise,
λJ (cid:48)
u (T 1
u (T 1
i , T 2
i , T 2
i = 0,
0 if
J (cid:48)(cid:48)
j ; D) =
j ; D) + Ju (T 1
j ; D) otherwise.
λJ (cid:48)(cid:48)
u (T 1
i , T 2
u (T 1
i−1 , T 2
i−1 , T 2
j ; D) = {u | u ∈ (cid:98)Γn (T 1
The following equations are introduced to select a set of signiﬁcant sub-sequences.
j ; D), τ ≤ χ2 (u), u|u| ∈ ∩|u|−1
i=1 ans(ui )}
Γn (T 1
i , T 2
i , T 2
u|u| ∈ ∩|u|−1
i=1 ans(ui ) evaluates if a sub-sequence u is complete sub-tree, where ans(ui )
(cid:189)
returns ancestor of the node ui . For example, ‘d ) b a’ is not a complete subtree, because
Ψn ((cid:98)Γ(cid:48)
(cid:98)Γn (T 1
the last node ‘a’ is not an ancestor of ‘d’ and ‘b’.
i ) ∪ {t1
j ; D), t1
i }
j ; D) =
i = t2
n (T 1
t1
i , T 2
if
j ,
i , T 2
where Ψn (F, w) = {uw | u ∈ F, τ ≤ (cid:98)χ2 (uw), |uw| ≤ n}, and F represents a set of sub-
(9)
∅
otherwise,
j ; D) and (cid:98)Γn (T 1
satisfy τ ≤ χ2 (uw) and τ ≤ (cid:98)χ2 (uw), respectively, iff t1
j ; D) have only sub-sequences u that
sequences. Note that Γn (T 1
i , T 2
i , T 2
j and |uw| ≤ n; otherwise
i = t2
they become empty sets.
j ; D) and (cid:98)Γn (T 1
(cid:189)∅
The following two equations are introduced for recursive the set operation to calculate
j ; D).
(cid:98)Γ(cid:48)
Γn (T 1
i , T 2
i , T 2
j−1 ; D) ∪ (cid:98)Γ(cid:48)(cid:48)
(cid:98)Γ(cid:48)
if j = 0,
(cid:189)∅
j ; D) =
n (T 1
i , T 2
j−1 ; D) otherwise,
(cid:98)Γ(cid:48)(cid:48)
n (T 1
n (T 1
i , T 2
i , T 2
j ; D) ∪ (cid:98)Γn (T 1
(cid:98)Γ(cid:48)(cid:48)
i = 0 ,
if
j ; D) =
n (T 1
i , T 2
j ; D) otherwise.
(11)
In the implementation, χ2 (uw) and (cid:98)χ2 (uw), where uw represents a concatenation of a
n (T 1
i−1 , T 2
i−1 , T 2
sequence u and a symbol w , can be calculated by a set of pointers of u against data and the
Γn and (cid:98)Γn . With condition (3), uw is only stored in (cid:98)Γn .
number of appearance of w in backside of the pointers. We note that the set of pointers of
uw can be simply obtained from previous search of u. With condition (1), uw is stored in
There are some technique in order to calculate kernel faster in the implementation. For
example, since χ2 (u) and ˆχ2 (u) are constant against the same data, we only have to calcu-
late them once. We store the internal search results of PreﬁxSpan with SMP algorithm in
a TRIE structure. After that, we look in that results in TRIE instead of explicitly calculate
χ2 (u) again when the kernel ﬁnds the same sub-sequence. Moreover, when the projected
database is exactly the same, these sub-sequences can be merged since the value of χ2 (uv)
and ˆχ2 (uv) for any postﬁx v are exactly the same. Moreover, we introduce a ‘transposed
index’ for fast evaluation of χ2 (u) and ˆχ2 (u). By using that, we only have to look up that
index of w to evaluate whether or not any uw are signiﬁcant features.

(10)

Equations (4) to (7) can be performed in the same as the original DP based kernel calcu-
lation. The recursive set operations of Equations (9) to (11) can be executed as well as

Table 2: Experimental Results
Subjectivity Detection
Question Classiﬁcation
4 ∞
4 ∞
1
2
1
3
2
3
.822 .839 .841 .842
-
.823 .827 .824 .822
-
.823 .824 .809 .772
-
.808 .818 .808 .797
-
-
.812 .815 .812 .812
-
.834 .857 .854 .856
.842 .850 .830 .755
-
.802 .802 .797 .783
-
.754 .792 .790 .778
-
.717
729 .715 .649
-

n
SK+FS
SK
TK+FS
TK
BOW-K

Polarity Identiﬁcation
4 ∞
2
1
3
.824 .838 .839 .839
-
.835 .835 .833 .789
-
-
.830 .832 .835 .833
.828 .827 .820 .745
-
.740 .810 .822 .795
-

Equations (5) to (7). Moreover, calculating χ2 (u) and ˆχ2 (u) with sub-structure mining al-
gorithms allow to calculate the same order of the DP based kernel calculation. As a result,
statistical feature selection can be embedded in original kernel calculation based on the DP.

Essentially, the worst case time complexity of the proposed method will become exponen-
tial, since we enumerate individual sub-structures in sub-structure mining phase. However,
actual calculation time in the most cases of our experiments is even faster than original
kernel calculation, since search space pruning efﬁciently remove vain calculation and the
implementation techniques brieﬂy explained above provide practical calculation speed.
We note that if we set τ = 0, which means all features are dealt with kernel calculation, we
can get exactly the same kernel value as the original tree kernel.

5 Experiments and Results

We evaluated the performance of the proposed method in actual NLP tasks, namely English
question classi ﬁcation (EQC), subjectivity detection (SD) and polarity identi ﬁcation (PI)
tasks. These tasks are deﬁned as a text categorization task: it maps a given sentence into
one of the pre-deﬁned classes. We used data provided by [6] for EQC, that contains about
5500 questions with 50 question types. SD data was created from Mainichi news articles,
and the size was 2095 sentences consisting of 822 subjective sentences. PI data has 5564
sentences with 2671 positive opinion. By using these data, we compared the proposed
method (SK+FS and TK+FS) with a conventional method (SK or TK), as discussed in
Section 3, and with bag-of-words (BOW) Kernel (BOW-K)[4] as baseline methods. We
used word sequences for input objects of sequence kernels and word dependency trees for
tree kernels.

Support Vector Machine (SVM) was selected as the kernel-based classiﬁer for training and
classiﬁcation with a soft margin parameter C = 1000. We used the one-vs-rest classiﬁer of
SVM as the multi-class classiﬁcation method for EQC. We evaluated the performance with
label accuracy by using ten-fold cross validation: eight for training, one for development
and remaining one for test set. The parameter λ and τ was automatically selected from
the value set of λ = {0.1, 0.3, 0.5, 0.7, 0.9} and τ = {3.84, 6.63} by the development
test. Note that these two values represent the 10% and 5% levels of signi ﬁcance in the χ2
distribution with one degree of freedom, which used the χ2 signiﬁcant test.

Tables 2 shows our experimental results. where n in each table indicates the restriction of
the sub-structure size, and n = ∞ means all possible sub-structures are used. As shown in
this table, SK or TK achieve maximum performance when n = 2 or 3. The performance
deteriorates considerably once n exceeds 4 or more. This implies that larger sub-structures
degrade classiﬁcation performance, which showed the same tendency as in the previous
studies discussed in Section 3. This is evidence of over-ﬁtting in learning. On the other
hand, SK+FS and TK+FS provided consistently better performance than the conventional
methods. Moreover, the experiments conﬁrmed one important fact: in some cases, max-
imum performance was achieved with n = ∞. This indicates that certain sub-sequences

created using very large structures can be extremely effective. If the performance is im-
proved by using a larger n, this means that signiﬁcant features do exist. Thus, we can
improve the performance of some classiﬁcation problems by dealing with larger substruc-
tures. Even if optimum performance was not achieved with n = ∞, the difference from the
performance of a smaller n is quite small compared to that of SK and TK. This indicates
that our method is very robust against sub-structure size.

6 Conclusions

This paper proposed a statistical feature selection method for sequence kernels and tree
kernels. Our approach can select signiﬁcant features automatically based on a statistical
signiﬁcance test. The proposed method can be embedded in the original DP based kernel
calculation process by using sub-structure mining algorithms.

Our experiments demonstrated that our method is superior to conventional methods. More-
over, the results indicate that complex features exist and can be effective. Our method can
employ them without over-ﬁtting problems, which yields beneﬁts in terms of concept and
performance.

References

[1] N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. Word-Sequence Kernels. Journal of
Machine Learning Research, 3:1059–1082, 2003.
[2] M. Collins and N. Duffy. Convolution kernels for natural language. In Proc. of Neural Infor-
mation Processing Systems (NIPS’2001), 2001.
[3] D. Haussler. Convolution kernels on discrete structures. In Technical Report UCS-CRL-99-10.
UC Santa Cruz, 1999.
[4] T. Joachims. Text Categorization with Support Vector Machines: Learning with Many Relevant
Features. In Proc. of European Conference on Machine Learning (ECML ’98), pages 137–142,
1998.
[5] H. Kashima and T. Koyanagi. Kernels for Semi-Structured Data. In Proc. 19th International
Conference on Machine Learning (ICML2002), pages 291–298, 2002.
[6] X. Li and D. Roth. Learning Question Classiﬁers. In Proc. of the 19th International Conference
on Computational Linguistics (COLING 2002), pages 556–562, 2002.
[7] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text Classiﬁcation
Using String Kernel. Journal of Machine Learning Research, 2:419–444, 2002.
[8] S. Morishita and J. Sese. Traversing Itemset Lattices with Statistical Metric Pruning. In Proc.
of ACM SIGACT-SIGMOD-SIGART Symp. on Database Systems (PODS’00), pages 226–236,
2000.
[9] J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. Pre ﬁxSpan: Mining Sequential Patterns Efﬁ-
ciently by Pre ﬁx-Projected Pattern Growth. In Proc. of the 17th International Conference on
Data Engineering (ICDE 2001), pages 215–224, 2001.
[10] J. Suzuki, Y. Sasaki, and E. Maeda. Kernels for Structured Natural Language Data. In Proc. of
the 17th Annual Conference on Neural Information Processing Systems (NIPS2003), 2003.
[11] C. Watkins. Dynamic alignment kernels. In Technical Report CSD-TR-98-11. Royal Holloway,
University of London Computer Science Department, 1999.
[12] M. J. Zaki. Efﬁciently Mining Frequent Trees in a Forest. In Proc. of the 8th International
Conference on Knowledge Discovery and Data Mining (KDD’02), pages 71–80, 2002.


From Weighted Classi ﬁcation to Policy Search

D. Blatt
Department of Electrical Engineering
and Computer Science
University of Michigan
Ann Arbor, MI 48109-2122
dblatt@eecs.umich.edu

A. O. Hero
Department of Electrical Engineering
and Computer Science
University of Michigan
Ann Arbor, MI 48109-2122
hero@eecs.umich.edu

Abstract

This paper proposes an algorithm to convert a T -stage stochastic decision
problem with a continuous state space to a sequence of supervised learn-
ing problems. The optimization problem associated with the trajectory
tree and random trajectory methods of Kearns, Mansour, and Ng, 2000,
is solved using the Gauss-Seidel method. The algorithm breaks a multi-
stage reinforcement learning problem into a sequence of single-stage re-
inforcement learning subproblems, each of which is solved via an exact
reduction to a weighted-classi ﬁcation problem that can be s olved using
off-the-self methods. Thus the algorithm converts a reinforcement learn-
ing problem into simpler supervised learning subproblems. It is shown
that the method converges in a ﬁnite number of steps to a solut
ion that
cannot be further improved by componentwise optimization. The impli-
cation of the proposed algorithm is that a plethora of classi ﬁcation meth-
ods can be applied to ﬁnd policies in the reinforcement learn ing problem.

1

Introduction

There has been increased interest in applying tools from supervised learning to problems
in reinforcement learning. The goal is to leverage techniques and theoretical results from
supervised learning for solving the more complex problem of reinforcement learning [3].
In [6] and [4], classi ﬁcation was incorporated into approxi mate policy iterations. In [2],
regression and classi ﬁcation are used to perform dynamic pr ogramming. Bounds on the
performance of a policy which is built from a sequence of classi ﬁers were derived in [8]
and [9].

Similar to [8], we adopt the generative model assumption of [5] and tackle the problem of
ﬁnding good policies within an inﬁnite class of policies, wh
ere performance is evaluated
in terms of empirical averages over a set of trajectory trees. In [8] the T-step reinforcement
learning problem was converted to a set of weighted classi ﬁc ation problems by trying to ﬁt
the classi ﬁers to the maximal path on the trajectory tree of t he decision process.

In this paper we take a different approach. We show that while the task of ﬁnding the global
optimum within a class of non-stationary policies may be overwhelming, the componen-
twise search leads to single step reinforcement learning problems which can be reduced
to a sequence of weighted classi ﬁcation problems. Our reduc tion is exact and is differ-

ent from the one proposed in [8]; it gives more weight to regions of the state space in
which the difference between the possible actions in terms of future reward is large, rather
than giving more weight to regions in which the maximal future reward is large. The
weighted classi ﬁcation problems can be solved by applying w eights-sensitive classi ﬁers or
by further reducing the weighted classi ﬁcation problem to a standard classi ﬁcation problem
using re-sampling methods (see [7], [1], and references therein for a description of both ap-
proaches). Based on this observation, an algorithm that converts the policy search problem
into a sequence of weighted classi ﬁcation problems is given . It is shown that the algorithm
converges in a ﬁnite number of steps to a solution, which cann ot be further improved by
changing the control of a single stage while holding the rest of the policy ﬁxed.

2 Problem Formulation

The results are presented in the context of MDPs but can be applied to POMDPs and non-
Markovian decision processes as well. Consider a T-step MDP M = {S , A, D, Ps,a },
where S is a (possibly continuous) state space, A = {0, . . . , L − 1} is a ﬁnite set of
possible actions, D is the distribution of the initial state, and Ps,a is the distribution of the
next state given that the current state is s and the action taken is a. The reward granted
when taking action a at state s and making a transition to state s′ is assumed to be a known
deterministic and bounded function of s′ denoted by r : S → [−M , M ]. No generality
is lost in specifying a known deterministic reward since it is possible to augment the state
variable by an additional random component whose distribution depends on the previous
state and action, and specify the function r to extract this random component. Denote by
S0 , S1 , . . . , ST the random state variables.
A non-stationary deterministic policy π = (π0 , π1 , . . . , πT −1 ) is a sequence of mappings
πt : S → A, which are called controls. The control πt speci ﬁes the action taken at time
t as a function of the state at time t. The expected sum of rewards of a non-stationary
deterministic policy π is given by
V (π) = Eπ ( TXt=1
r (St )) ,
where the expectation is taken with respect to the distribution over the random state vari-
ables induced by the policy π . We call V (π) the value of policy π . Non-stationary de-
terministic policies are considered since the optimal policy for a ﬁnite horizon MDP is
non-stationary and deterministic [10]. Usually the optimal policy is deﬁned as the policy
that maximizes the value conditioned on the initial state, i.e.,
Vπ (s) = Eπ ( TXt=1
R (St ) |S0 = s) ,
for any realization s of S0 [10]. The policy that maximizes the conditional value given
each realization of the initial state also maximizes the value averaged over the initial state,
and it is the unique maximizer if the distribution of the initial state D is positive over S .
Therefore, when optimizing over all possible policies, the maximization of (1) and (2) are
equivalent. When optimizing (1) over a restricted class of policies, which does not contain
the optimal policy, the distribution over the initial state speci ﬁes the importance of different
regions of the state space in terms of the approximation error. For example, assigning high
probability to a certain region of S will favor policies that well approximate the optimal
policy over that region. Alternatively, maximizing (1) when D is a point mass at state s is
equivalent to maximizing (2).

(1)

(2)

Following the generative model assumption of [5], the initial distribution D and the condi-
tional distribution Ps,a are unknown but it is possible to generate realization of the initial

state according to D and the next state according to Ps,a for arbitrary state-action pairs
(s, a). Given the generative model, n trajectory trees are constructed in the following man-
ner. The root of each tree is a realization of S0 generated according to the distribution
D . Given the realization of the initial state, realizations of the next state S1 given the L
possible actions, denoted by S1 |a, a ∈ A, are generated. Note that this notation omits
the dependence on the value of the initial state. Each of the L realizations of S1 is now
the root of the subtree. These iterations continue to generate a depth T tree. Denote by
St |i0 , i1 , . . . , it−1 the random variable generated at the node that follows the sequence of
actions i0 , i1 , . . . , it−1 . Hence, each tree is constructed using a single call to the initial state
generator and LT − 2 calls to the next state generator.

Figure 1: A binary trajectory tree.

Consider a class of policies Π, i.e., each element of Π is a sequence of T mappings from S
to A. It is possible to estimate the value of any policy in the class from the set of trajectory
trees by simply averaging the sum of rewards on each tree along the path that agrees with
the policy [5]. Denote by bV i (π) the observed value on the i’th tree along the path that
corresponds to the policy π . Then the value of the policy π is estimated by
nXi=1 bV i (π).
bVn (π) = n−1
(3)
In [5], the authors show that with high probability (over the data set) bVn (π) converges
uniformly to V (π) (1) with rates that depend on the VC-dimension of the policy class.
This result motivates the use of policies π with high bVn (π), since with high probability
these policies have high values of V (π). In this paper, we consider the problem of ﬁnding
policies that obtain high values of bVn (π).
3 A Reduction From a Single Step Reinforcement Learning Problem
to Weighted Classiﬁcation

The building block of the proposed algorithm is an exact reduction from a single step rein-
forcement learning to a weighted classi ﬁcation problem. Co nsider the single step decision
process. An initial state S0 generated according to the distribution D is followed by one
of L possible actions A ∈ {0, 1, . . . , L − 1}, which leads to a transition to state S1 whose

(4)

conditional distribution given the initial state is s and the action is a is given by Ps,a . Given
a class of policies Π, where policy in Π is a map from S to A, the goal is to ﬁnd
π∈Π bVn (π).
bπ ∈ arg max
In this single step problem the data are n realization of
the random element
{S0 , S1 |0, S1 |1, . . . , S1 |L − 1}. Denote the i’th realization by {si
1 |1, . . . , si
1 |0, si
0 , si
1 |L −
1}. In this case, bVn (π) can be written explicitly by
bVn (π) = En (L−1Xl=0
r(S1 |l)I (π(S0 ) = l)) ,
where for a function f , En {f (S0 , S1 |0, S1 |1, . . . , S1 |L − 1)} is its empirical expectation
n−1 Pn
1 |L − 1), and I (·) is the indicator function taking a value
1 |0, si
1 |1, . . . , si
i=1 f (si
0 , si
of one when its argument is true and zero otherwise.
The following proposition shows that the problem of maximizing the empirical reward (5)
is equivalent to a weighted classi ﬁcation problem.

(5)

arg max
π∈Π

Proposition 1 Given a class of policies Π and a set of n trajectory trees,
En (L−1Xl=0
r(S1 |l)I (π(S0 ) = l))
En (L−1Xl=0 (cid:20)max
r(S1 |k) − r(S1 |l)(cid:21) I (π(S0 ) = l)) .
= arg min
π∈Π
k
The proposition implies that the maximizer of the empirical reward over a class of policies
is the output of an optimal weights dependent classi ﬁer for t he data set:
1 |k), wi(cid:19)(cid:27)n
(cid:26)(cid:18)si
r(si
0 , arg max
k
i=1
where for each sample, the ﬁrst argument is the example, the s econd is the label, and
1 |L − 1)(cid:21)
wi = (cid:20)max
r(si
1 |k) − r(si
k
is the realization of the L costs of classifying example i to each of the possible labels.
Note that the realizations of the costs are always non-negative and the cost of the correct
1 |k)) is always zero. The solution to the weighted classi ﬁcation
classi ﬁcation ( arg maxk r(si
problem is a map from S to A which minimizes the empirical weighted misclassi ﬁcation
error (6). The proposition asserts that this mapping is also the control which maximizes the
empirical reward (5).

1 |k) − r(si
r(si
1 |1), . . . , max
k

1 |k) − r(si
r(si
1 |0), max
k

(6)

,

r(S1 |l)I (π(S0 ) = l) = r(S1 |j ) + (r(S1 |0) − r(S1 |j ))I (π(s) = 0) +

Proof 1 For all j ∈ {0, 1, . . . , L − 1},
L−1Xl=0
(r(S1 |1) − r(S1 |j ))I (π(s) = 1) + . . . + (r(S1 |L − 1) − r(S1 |j ))I (π(s) = L − 1).
In addition,
En (L−1Xl=0
r(S1 |l)I (π(S0 ) = l)) =

(7)

r(S1 |k) = 0)

r(S1 |k) = 1)

r(S1 |k) − r(S1 |1))I (π(S0 ) = 1) − . . . −

L−1Xl=0
L−1Xl=0
r(S1 |k) = L − 1)

r(S1 |l)I (π(S0 ) = l)) +
En (I (arg max
k
r(S1 |l)I (π(S0 ) = l)) + . . . +
En (I (arg max
k
r(S1 |l)I (π(S0 ) = l)) .
En (I (arg max
L−1Xl=0
k
Substituting (7) we obtain
En (L−1Xl=0
r(S1 |l)I (π(S0 ) = l)) =
L−1Xj=0
r(S1 |k) = j )[r(S1 |j ) −
En {I (arg max
k
(max
k
(max
k
(max
r(S1 |k) − r(S1 |L − 1))I (π(S0 ) = L − 1)]} =
k
r(S1 |k) = j )r(S1 |j )(cid:27) −
En (cid:26)I (arg max
L−1Xj=0
k
En (L−1Xl=0 (cid:20)max
R(S1 |k) − R(S1 |l)(cid:21) I (π(S0 ) = l))
k
The term in the second to last line is independent of π(s) and the result follows.
In the binary case, the optimization problem is
r(S1 |k))(cid:27) ,
En (cid:26)|r(S1 |0) − r(S1 |1)|I (π(S0 ) 6= arg max
k
i.e., the single step reinforcement learning problem reduces to the weighted classi ﬁcation
problem with samples
(cid:26)(cid:18)si
1 |1)|(cid:19)(cid:27)n
r(si
1 |k), |r(si
1 |0) − r(si
0 , arg max
k∈{0,1}
i=1
where for each sample, the ﬁrst argument is the example, the s econd is the label, and the
third is a realization of the cost incurred when misclassifying the example. Note that this
is different from the reduction in [8]. When applying the reduction in [8] to our single step
1 |k) rather than |r(si
problem the costs are taken to be maxk∈{0,1} r(si
1 |1)|. Set-
1 |0) − r(si
ting the costs to maxk∈{0,1} r(si
1 |k) instead of |r(si
1 |1)| favors classi ﬁers which
1 |0) − r(si
perform well in regions where the maximal reward is large (regardless of the difference
between the two actions) instead of regions where the difference between the rewards that
result from the two actions is large. It is easy to set an example of a simple MDP and a
restricted class of policies, which do not include the optimal policy, in which the classi ﬁer
that minimizes the weighted misclassi ﬁcation problem with costs maxk∈{0,1} r(si
1 |k) is
not equivalent to the optimal policy. When using our reduction, they are always equivalent.
On the other hand, in [8] the choice maxk∈{0,1} r(si
1 |k) led to a bound on the perfor-
mance of the policy in terms of the performance of the classi ﬁ er. We do not pursue this

r(S1 |k) − r(S1 |0))I (π(S0 ) = 0) −

arg min
π∈Π

,

type of bounds here since given the classi ﬁer, the performan ce of the resulting policy can
be directly estimated from (5). Given a sequence of classi ﬁe rs, the value of the induced
sequence of controls (or policy) can be estimated directly by (3) with generalization guar-
antees provided by the bounds in [5]. In [2], a certain single step binary reinforcement
learning problem is converted to weighted classi ﬁcation by averaging multiple realizations
of the rewards under the two possible actions for each state. As seen here, this Monte Carlo
approach is not necessary; it is sufﬁcient to sample the rewa rds once for each state.

4 Finding Good Policies for a T -Step Markov Decision Processes By
Solving a Sequence of Weighted Classiﬁcation Problems

Given the class of policies Π, the algorithm updates the controls π0 , . . . , πT −1 one at a time
in a cyclic manner while holding the rest constant. Each update is formulated as a single
step reinforcement learning problem which is then converted to a weighted classi ﬁcation
problem. In practice, if the weighted classi ﬁcation proble m is only approximately solved,
then the new control is accepted only if it leads to higher value of bV . When updating πt , the
trees are pruned from the root to stage t by keeping only the branch which agrees with the
controls π0 , π1 , . . . , πt−1 . Then a single step reinforcement learning is formulated at time
step t, where the realization of the reward which follows action a ∈ A at stage t is the im-
mediate reward obtained at the state which follows action a plus the sum of rewards which
are accumulated along the branch which agrees with the controls πt+1 , πt+2 , . . . , πT −1 .
The iterations end after the ﬁrst complete cycle with no para meter modi ﬁcations.
Note that when updating πt , each tree contributes one realization of the state at time t. A
result of the pruning process is that the ensemble of state realization are drawn from the
distribution induced by the policy up to time t − 1. In other words, the algorithm relaxes
the requirement in [2] to have access to a baseline distribution - a distribution over the
states that is induced by a good policy. Our algorithm automatically generates samples
from distributions that are induced by a sequence of monotonically improving policies.

Figure 2: Updating π1 . In the example: pruning down according to π0 (S0 ) = 0, propagat-
ing rewards up according to π2 (S2 |00) = 1, and π2 (S2 |01) = 0.

Proposition 2 The algorithm converges after a ﬁnite number of iterations t o a policy that
cannot be further improved by changing one of the controls and holding the rest ﬁxed.
Proof 2 Writing the empirical average sum of rewards bVn (π) explicitly as
bVn (π) = En  Xi0 ,...,iT −1∈AT
I (π0 (S 0 ) = i0 )I (π1 (S 1 |i0 ) = i1 ) . . .

I (πT −1 (S T −1 |i0 , i1 , . . . , iT −2 ) = iT −1 )

r(S t |i0 , i1 , . . . , it−1 )) ,
TXt=1
it can be seen that the algorithm is a Gauss-Seidel algorithm for maximizing bVn (π), where,
at each iteration, optimization of πt is carried out at one of the stages t while keeping
πt′ , t′ 6= t ﬁxed. At each iteration the previous control is a valid solut ion and hence the
objective function is non decreasing. Since bVn (π) is evaluated using a ﬁnite number of
trees, it can take only a ﬁnite set of values. Therefore, we mu st reach a cycle with no
updates after a ﬁnite number of iterations. A cycle with no im provements implies that we
cannot increase the empirical average sum of rewards by updating one of the πt ’s.

5

Initialization

There are two possible initial policies that can be extracted from the set of trajectory trees.
One possible initial policy is the myopic policy which is computed from the root of the tree
downwards. Staring from the root, π0 is found by solving the single stage reinforcement
learning resulting from taking into account only the immediate reward at the next state.
Once the weighted classi ﬁcation problem is solved the trees are pruned by following the
action which agrees with π0 . The remaining realizations of state S1 follow the distribution
induced by the myopic control of the ﬁrst stage. The process i s continued to stage T − 1.
The second possible initial policy is computed from the leaves backward to the root. Note
that the distribution of the state at a leaf that is chosen at random is the distribution of
the state when a randomized policy is used. Therefore, to ﬁnd the best control at stage
T − 1, given that the previous T − 2 controls choose random actions, we solve the weighted
classi ﬁcation problem induced by considering all the reali zation of the state ST −1 from all
the trees (these are not independent observations) or choose randomly one realization from
each tree (these are independent realizations). Given the classi ﬁer, we use the equivalent
control πT −1 to propagated the rewards up to the previous stage and solve the resulting
weighted classi ﬁcation problem. This is carried out recurs ively up to the root of the tree.

6 Extensions

The results presented in this paper generalize to the non-Markovian setting as well. In
particular, when the state space, action space, and the reward function depend on time, and
the distribution over the next state depends on all past states and actions, we will be dealing
with non-stationary deterministic policies π = (π0 , π1 , . . . , πT −1 ); πt : S0 × A0 × . . . ×
St−1 × At−1 × St → At , t = 0, 1, . . . , T − 1. POMDPs can be dealt with in terms of
the belief states as a continuous state space MDP or as a non-Markovian process in which
policies depend directly on all past observations.

While we focused on the trajectory tree method, the algorithm can be easily modi ﬁed to
solve the optimization problem associated with the random trajectory method [5] by ad-
justing the single step reinforcement learning reduction and the pruning method presented
here.

7

Illustrative Example

The following example illustrates the aspects of the problem and the components of our so-
lution. The simulated system is a two-step MDP, with continuous state space S = [0, 1] and
a binary action space A = {0, 1}. The distribution over the initial state is uniform. Given
state s and action a the next state s′ is generated by s′ = mod(s + 0.33a + 0.1randn, 1),

where mod(x, 1) is the fraction part of x, and randn is a Gaussian random variable inde-
pendent of the other variables in the problem. The reward function is r(s) = s sin(πs). We
consider a class of policies parameterized by a continuous parameter: Π = {π(·; θ)|θ =
(θ0 , θ1 ) ∈ [0, 2]2 }, where πi (s; θi ) = 1 when θi ≤ 1 and s > θi or when θi > 1 and
s < θi − 1 and zero otherwise, i = 0, 1.
In Figure 3 the objective function bVn (π(θ)), estimated from n = 20 trees, is presented as a
function of θ0 and θ1 . The path taken by the algorithm supperimposed on the contour plot
of bVn (π(θ)) is also presented. Starting from the arbitrary point 0, the algorithm performs
optimization with respect to one of the coordinates at a time and converges after 3 iterations.
2
1.8

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3
2

1.5

1

0.5

2

1.5

1

0.5

1.6

1.4

1.2

1
θ

1

0.8

0.6

0.4

0.2

1 

0 

2 

3 

0

0

1.2

1.4

1.6

1.8

0

0

2

0.8

0.6

0.4

0.2

θ
1

1
θ
θ
0
0
Figure 3: The objective function bVn (π(θ)) and the path taken by the algorithm.
References
[1] N. Abe, B. Zadrozny, and J. Langford. An iterative method for multi-class cost-sensitive learn-
ing. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pages 3–11, 2004.
[2] J. Bagnell, S. Kakade, A. Ng, and J. Schneider. Policy search by dynamic programming. In
Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003.
[3] A. G. Barto and T. G. Dietterich. Reinforcement learning and its relationship to supervised
learning. In J. Si, A. Barto, W. Powell, and D. Wunsch, editors, Handbook of learning and
approximate dynamic programming. John Wiley and Sons, Inc, 2004.
[4] A. Fern, S. Yoon, and R. Givan. Approximate policy iteration with a policy language bias. In
Advances in Neural Information Processing Systems, volume 16, 2003.
[5] M. Kearns, Y. Mansour, and A. Ng. Approximate planning in large POMDPs via reusable
trajectories. In Advances in Neural Information Processing Systems, volume 12. MIT Press,
2000.
[6] M. Lagoudakis and R. Parr. Reinforcement learning as classiﬁca tion: Leveraging modern clas-
si ﬁers. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.
[7] J. Langford and A. Beygelzimer. Sensitive error correcting output codes. In Proceedings of the
18th Annual Conference on Learning Theory, pages 158–172, 2005.
[8] J. Langford and B. Zadrozny. Reducing T-step reinforcement learning to classiﬁcation.
http://hunch.net/∼jl/projects/reductions/reductions.html, 2003.
[9] J. Langford and B. Zadrozny. Relating reinforcement learning performance to classiﬁcation per-
formance. In Proceedings of the Twenty Second International Conference on Machine Learning,
pages 473–480, 2005.
[10] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, Inc, 1994.


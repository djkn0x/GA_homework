Non-Gaussian Component Analysis: a
Semi-parametric Framework for Linear
Dimension Reduction

G. Blanchard1 , M. Sugiyama1;2 , M. Kawanabe1 , V. Spokoiny3 , K.-R. M ¨uller1;4
1 Fraunhofer FIRST.IDA, Kekul ´estr. 7, 12489 Berlin, Germany
2 Dept. of CS, Tokyo Inst. of Tech., 2-12-1, O-okayama, Meguro-ku, Tokyo, 152-8552, Japan
3Weierstrass Institute and Humboldt University, Mohrenstr. 39, 10117 Berlin, Germany
4 Dept. of CS, University of Potsdam, August-Bebel-Strasse 89, 14482 Potsdam, Germany
spokoiny@wias-berlin.de
fblanchar,sugi,nabe,klausg@first.fhg.de

Abstract

We propose a new linear method for dimension reduction to identify non-
Gaussian components in high dimensional data. Our method, NGCA
(non-Gaussian component analysis), uses a very general semi-parametric
framework. In contrast to existing projection methods we deﬁne what is
uninteresting (Gaussian): by projecting out uninterestingness, we can es-
timate the relevant non-Gaussian subspace. We show that the estimation
error of ﬁnding the non-Gaussian components tends to zero at a paramet-
ric rate. Once NGCA components are identi ﬁed and extracted, various
tasks can be applied in the data analysis process, like data visualization,
clustering, denoising or classiﬁcation. A numerical study demonstrates
the usefulness of our method.

1 Introduction

Suppose fXi gn
i=1 are i.i.d. samples in a high dimensional space Rd drawn from an un-
known distribution with density p(x) . A general multivariate distribution is typically too
complex to analyze from the data, thus dimensionality reduction is necessary to decrease
the complexity of the model (see, e.g., [4, 11, 10, 12, 1]). We will follow the rationale
that in most real-world applications the ‘signal’ or ‘information’ contained in the high-
dimensional data is essentially non-Gaussian while the ‘rest’ can be interpreted as high
dimensional Gaussian noise. Thus we implicitly ﬁx what is not interesting (Gaussian part)
and learn its orthogonal complement, i.e. what is interesting. We call this approach non-
Gaussian components analysis (NGCA).
We want to emphasize that we do not assume the Gaussian components to be of smaller
order of magnitude than the signal components. This setting therefore excludes the use
of common (nonlinear) dimensionality reduction methods such as Isomap [12], LLE [10],
that are based on the assumption that the data lies, say, on a lower dimensional manifold,
up to some small noise distortion. In the restricted setting where the number of Gaussian
components is at most one and all the non-Gaussian components are mutually independent,
Independent Component Analysis (ICA) techniques (e.g., [9]) are applicable to identify the
non-Gaussian subspace.

A framework closer in spirit to NGCA is that of projection pursuit (PP) algorithms [5, 7, 9],
where the goal is to extract non-Gaussian components in a general setting, i.e., the number
of Gaussian components can be more than one and the non-Gaussian components can be
dependent. Projection pursuit methods typically proceed by ﬁxing a single index which
measures the non-Gaussianity (or ’interestingness’) of a projection direction. This index is
then optimized to ﬁnd a good direction of projection, and the procedure is iterated to ﬁnd
further directions. Note that some projection indices are suitable for ﬁnding super-Gaussian
components (heavy-tailed distribution) while others are suited for identifying sub-Gaussian
components (light-tailed distribution) [9]. Therefore, traditional PP algorithms may not
work effectively if the data contains, say, both super- and sub-Gaussian components.

Technically, the NGCA approach to identify the non-Gaussian subspace uses a very gen-
eral semi-parametric framework based on a central property: there exists a linear mapping
h 7! ﬂ (h) 2 Rd which, to any arbitrary (smooth) nonlinear function h : Rd ! R, as-
sociates a vector ﬂ lying in the non-Gaussian subspace. Using a whole family of different
nonlinear functions h then yields a family of different vectors bﬂ (h) which all approximately
lie in, and span, the non-Gaussian subspace. We ﬁnally perform PCA on this family of vec-
tors to extract the principal directions and estimate the target space. Our main theoretical
contribution in this paper is to prove consistency of the NGCA procedure, i.e. that the
above estimation error vanishes at a rate plog(n)=n with the sample size n. In practice,
we consider functions of the particular form h! ;a (x) = fa (h! ; xi) where f is a function
class parameterized, say, by a parameter a, and k!k = 1.
Apart from the conceptual point, deﬁning uninterestingness as the point of departure in-
stead of interestingness, another way to look at our method is to say that it allows the
combination of information coming from different indices h: here the above function fa
(for ﬁx ed a) plays a role similar to that of a non-Gaussianity index in PP, but we do com-
bine a rich family of such functions (by varying a and even by considering several function
classes at the same time). The important point here is while traditional projection pursuit
does not provide a well-founded justiﬁcation for combining directions obtained from dif-
ferent indices, our framework allows to do precisely this –
thus implicitly selecting, in a
given family of indices, the ones which are the most informative for the data at hand (while
always maintaining consistency).

In the following section we will outline our main theoretical contribution, a novel semi-
parametric theory for linear dimension reduction. Section 3 discusses the algorithmic pro-
cedures and simulation results underline the usefulness of NGCA; ﬁnally a brief conclusion
is given.

2 Theoretical framework

The model. We assume the unknown probability density function p(x) of the observa-
tions in Rd is of the form

(1)
p(x) = g(T x)`¡ (x);
where T is an unknown linear mapping from Rd to another space Rm with m • d ,
g is an unknown function on Rm , and `¡ is a centered Gaussian density with unknown
covariance matrix ¡ . The above decomposition may be possible for any density p since g
can be any function. Therefore, this decomposition is not restrictive in general.

Note that the model (1) includes as particular cases both the pure parametric ( m = 0 ) and
pure non-parametric ( m = d ) models. We effectively consider an intermediate case where
d is large and m is rather small. In what follows we denote by I the m -dimensional linear
subspace in Rd generated by the dual operator T > :
I = K er(T )? = Range(T > ) :

We call I the non-Gaussian subspace. Note how this deﬁnition implements the general
point of view outlined in the introduction: by this model we deﬁne rather what is considered
uninteresting, i.e. the null space of T ; the target space is deﬁned indirectly as the orthogonal
of the uninteresting component. More precisely, using the orthogonal decomposition X =
X0 +XI , where X0 2 K er(T ) and XI 2 I , equation (1) implies that conditionally to XI ,
X0 has a Gaussian distribution. X0 is therefore ’not interesting’ and we wish to project it
out.
Our goal is therefore to estimate I by some subspace bI computed from i.i.d. samples
i=1 which follows the distribution with density p(x). In this paper we assume the
fXi gn
effective dimension m to be known or ﬁx ed a priori by the user. Note that we do not
estimate ¡ , g , and T when estimating I .
Population analysis. The main idea underlying our approach is summed up in the fol-
lowing Proposition (proof in Appendix). Whenever variable X has covariance matrix iden-
tity, this result allows, from an arbitrary smooth real function h on Rd , to ﬁnd a vector
ﬂ (h) 2 I .
Proposition 1 Let X be a random variable whose density function p(x) satis ﬁes
(1)
and suppose that h(x) is a smooth real function on Rd . Assume furthermore that
§ = E £XX > ⁄ = Id . Then under mild regularity conditions the following vector be-
longs to the target space I :
(2)
ﬂ (h) = E [rh ¡ X h(X )] :
Estimation using empirical data. Since the unknown density p(x) is used to deﬁne ﬂ
by Eq.(2), one can not directly use this formula in practice, and it must be approximated
using the empirical data. We therefore have to estimate the population expectations using
empirical ones. A bound on the corresponding approximation error is then given by the
following theorem:
Theorem 1 Let h be a smooth function. Assume that supy max (krh(y)k ; kh(y)k) < B
and that X has covariance matrix E £XX > ⁄ = Id and is such that for some ‚0 > 0:
E [exp (‚0 kX k)] • a0 < 1:
Denote eh(x) = rh(x) ¡ xh(x). Suppose X1 ; : : : ; Xn are i.i.d. copies of X and deﬁne
nXi=1 (cid:176)(cid:176)(cid:176)eh(Xi ) ¡ bﬂ (h)(cid:176)(cid:176)(cid:176)
nXi=1 eh(Xi ) ; and b(cid:190)(h) =
2
1
1
bﬂ (h) =
;
n
n
then with probability 1 ¡ 4– the following holds:
dist ‡ bﬂ (h); I · • 2r
+ C (‚0 ; a0 ; B ; d) (cid:181) log(n–¡1 ) log –¡1
¶ :
log –¡1 + log d
b(cid:190)(h)
3
n
n
4
Comments. 1. The proof of the theorem relies on standard tools using Chernoff ’s bound-
ing method and is omitted for space. In this theorem, the covariance matrix of X is assumed
to be known and equal to identity which is not a realistic assumption; in practice, we use
a standard “whitening”
procedure (see next section) using the empirical covariance matrix.
Of course there is an additional error coming from this step, since the covariance matrix
is also estimated empirically. In the extended version of the paper [3], we prove (under
somewhat stronger assumptions) a bound for the entirely empirical procedure including
whitening, resulting in an approximation error of the same order in n (up to a logarithmic
factor). This result was omitted here due to space constraints.

(3)

(4)

2. Fixing – , Theorem 1 implies that the vector bﬂ (h) obtained from any h(x) converges to
rate of order 1=pn . Further-
the unknown non-Gaussian subspace I at a “parametric”
more, the theorem gives us an estimation of the relative size of the estimation error for
different functions h through the (computable from the data) factor pb(cid:190)(h) in the main
term of the bound. This suggests using this quantity as a renormalizing factor so that the
typical approximation error is (roughly) independent of the function h used. This normal-
ization principle will be used in the main procedure.
3. Note the theorem results in an exponential deviation inequality (the dependence in the
conﬁdence level – is logarithmic). As a consequence, using the union bound over a ﬁnite
net, we can obtain as a corollary of the above theorem a uniform deviation bound of the
same form over a (discretized) set of functions (where the log-cardinality of the set appears
as an additional factor). For instance, if we consider a 1=n-discretization net of functions
with d parameters, hence of size O(nd ), then the above bounds holds uniformly when re-
placing the log –¡1 term by d log n + log –¡1 . This does not change fundamentally the
bound (up to an additional complexity factor pd log(n)), and justi ﬁes
that we consider
simultaneously such a family of functions in the main algorithm.

h(x)

h

1

h
h

2

3

h

h

4

5

x

^
b

4

^
b

2

^
b

1

I

3b^

b^

5

Figure 1: The NGCA main idea: from a varied family of real functions h, compute a family
of vectors bﬂ belonging to the target space up to small estimation error.
3 The NGCA algorithm

In the last section, we have established that given an arbitrary smooth real function h on
Rd , we are able to construct a vector bﬂ (h) which belongs to the target space I up to a small
estimation error. The main idea is now to consider a large family of such functions (hk ),
giving rise to a family of vectors bﬂk (see Fig. 1). Theorem 1 ensures that the estimation
error remains controlled uniformly, and we can also normalize the vectors such that the
estimation error is of the same order for all vectors (see Comments 2 and 3 above). Under
this condition, it can be shown that vectors with a longer norm are more informative about
the target subspace, and that vectors with too small a norm are uninformative. We therefore
throw out the smaller vectors, then estimate the target space I by applying a principal
components analysis to the remaining vector family.
In the proposed algorithm we will restrict our attention to functions of the form hf ;! (x) =
f (h! ; xi), where ! 2 Rd ; k!k = 1, and f belongs to a ﬁnite
family F of smooth real
functions of real variable. Our theoretical setting allows to ensure that the approximation
error remains small uniformly over F and ! (rigorously, ! should be restricted to a ﬁnite
"-net of the unit sphere in order to consider a ﬁnite
family of functions: in practice we
will overlook this weak restriction). However, it is not feasible in practice to sample the
whole parameter space for ! as soon as it has more than a few dimensions. To overcome
this difﬁculty , we advocate using a well-known PP algorithm, FastICA [8], as a proxy to
ﬁnd good candidates for !f for a ﬁx ed f . Note that this does not make NGCA equivalent
to FastICA: the important point is that FastICA, as a stand-alone procedure, requires to ﬁx

the “inde x function” f beforehand. The crucial novelty of our method is that we provide a
theoretical setting and a methodology which allows to combine the results of this projection
pursuit method when used over a possibly large spectrum of arbitrary index functions f .

NGCA A LGOR I THM .
Input: Data points (Xi ) 2 Rd , dimension m of target subspace.
Parameters: Number Tmax of FastICA iterations; threshold †;
family of real functions (fk ).

2

Whitening.
The data Xi is recentered by subtracting the empirical mean.
Let b§ denote the empirical covariance matrix of the data sample (Xi ) ;
put bYi = b§¡ 1
2 Xi the empirically whitened data.
Main Procedure.
Loop on k = 1; : : : ; L:
Draw !0 at random on the unit sphere of Rd .
Loop on t = 1; : : : ; Tmax : [FastICA loop]
nXi=1 ‡ bYi fk (h!t¡1 ; bYi i) ¡ f 0
k (h!t¡1 ; bYi i)!t¡1·.
1
Put bﬂt ˆ
n
Put !t ˆ bﬂt =k bﬂt k.
End Loop on t
Let Ni be the trace of the empirical covariance matrix of bﬂTmax :
nXi=1 (cid:176)(cid:176)(cid:176) bYi fk (h!Tmax¡1 ; bYi i) ¡ f 0
k (h!Tmax¡1 ; bYi i)!Tmax¡1(cid:176)(cid:176)(cid:176)
1
Ni =
n
Store v (k) ˆ bﬂTmax ⁄ pn=Ni : [Normalization]
End Loop on k
Thresholding.
From the family v (k) , throw away vectors having norm smaller than threshold †.
PCA step.
Perform PCA on the set of remaining v (k) .
Let Vm be the space spanned by the ﬁrst m principal directions.
Pull back in original space.
Output: Wm = b§¡ 1
2 Vm .
Summing up, the NGCA algorithm ﬁnally consists of the following steps (see above pseu-
docode): (1) Data whitening (see Comment 1 in the previous section), (2) Apply Fas-
tICA to each function f 2 F to ﬁnd a promising candidate value for !f , (3) Com-
pute the corresponding family of vectors ( bﬂ (hf ;!f ))f 2F (using Eq. (4)), (4) Normalize
the vectors appropriately; threshold and throw out uninformative ones, (5) Apply PCA,
(6) Pull back in original space (de-whitening).
In the implementation tested, we have
used the following forms of the functions fk : f (1)
(cid:190) (z ) = z 3 exp(¡z 2 =2(cid:190)2 ) (Gauss-Pow3),
f (2)
(z ) = tanh(bz ) (Hyperbolic Tangent), f (3)
a (z ) = fsin; cosg (az ) (Fourier). More
b
precisely, we consider discretized ranges for a 2 [0; A]; b 2 [0; B ]; (cid:190) 2 [(cid:190)min ; (cid:190)max ];
this gives rise to a ﬁnite family (fk ) (which includes simultaneously functions of the three
different above families).

¡ (cid:176)(cid:176)(cid:176) bﬂTmax (cid:176)(cid:176)(cid:176)

2

:

4 Numerical results

Parameters used. All the experiments presented where obtained with exactly the same
set of parameters: a 2 [0; 4] for the Fourier functions; b 2 [0; 5] for the Hyperbolic Tangent
functions; (cid:190) 2 2 [0:5; 5] for the Gauss-pow3 functions. Each of these ranges was divided

x 10−3

2.5

2

1.5

1

0.5

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0.03

0.025

0.02

0.015

0.01

0.005

0.03

0.025

0.02

0.015

0.01

0.005

0

0

0

0

  NGCA  

  NGCA  

  NGCA  

PP(pow3)

PP(pow3)

PP(pow3)

PP(tanh)
PP(tanh)
PP(tanh)
PP(tanh)
(A)
(B)
(C)
(D)
Figure 2: Boxplots of the error criterion E ( bI ; I ) over 100 training samples of size 1000.
x 10−3
0.12
0.03
0.014
2
0.012
0.1
0.025
1.5
0.01
)
0.08
0.02
)
3
)
)
3
3
3
w
w
w
w
o
o
o
o
p
p
p
p
(
(
 
(
(
 
P
 
 
P
P
P
P
P
P
P

PP(pow3)

  NGCA  

0.015

0.006

0.008

0.06

1

0.04

0.01

0.5

0

0

x 10−3

2

1.5

1

0.5

)
h
n
a
t
(
 
P
P

0.5

1
NGCA

1.5

2

x 10−3

0.02

0

0

0.12

0.1

0.08

0.06

0.04

0.02

)
h
n
a
t
(
 
P
P

0.02

0.04

0.06
NGCA

0.08

0.1

0.12

0.005

0

0

0.03

0.025

0.02

0.015

0.01

0.005

)
h
n
a
t
(
 
P
P

0.004

0.002

0.005

0.01

0.015
NGCA

0.02

0.025

0.03

0

0

0.002

0.004

0.006
0.008
NGCA

0.01

0.012

0.014

0.014

0.012

0.01

0.008

0.006

0.004

0.002

)
h
n
a
t
(
 
P
P

0

0

0

0

0

0

2

0

0

0.1

1.5

0.5

0.01

0.04

0.02

0.12

0.08

0.005

x 10−3

1
0.06
0.015
0.006
0.008
NGCA
NGCA
NGCA
NGCA
(A)
(B)
(C)
(D)
Figure 3: Sample-wise performance comparison plots (for error criterion E ( bI ; I )) of
NGCA versus FastICA; top: versus pow3 index; bottom: versus tanh index. Each point
represents a different sample of size 1000. In (C)-top, about 25% of the points correspond-
ing to a failure of FastICA fall outside of the range and were not represented.

0.025

0.002

0.004

0.012

0.014

0.02

0.03

0.01

into 1000 equispaced values, thus yielding a family (fk ) of size 4000 (Fourier functions
count twice because of the sine and cosine parts). Some preliminary calibration suggested
to take " = 1:5 as the threshold under which vectors are not informative. Finally we ﬁx ed
the number of FastICA iterations Tmax = 10. With this choice of parameters, with 1000
points of data the computation time is typically of the order of 10 seconds on a modern PC
under a Matlab implementation.

Tests in a controlled setting. We performed numerical experiments using various syn-
thetic data. We report exemplary results using 4 data sets. Each data set includes 1000
samples in 10 dimensions, and consists of 8-dimensional independent standard Gaussian
and 2 non-Gaussian components as follows:
(A) Simple Gaussian Mixture: 2-dimensional independent bimodal Gaussian mixtures;
(B) Dependent super-Gaussian: 2-dimensional density is proportional to exp(¡kxk);
(C) Dependent sub-Gaussian: 2-dimensional uniform on the unit circle;
(D) Dependent super- and sub-Gaussian: 1-dimensional Laplacian with density propor-
tional to exp(¡jxLap j) and 1-dimensional dependent uniform U (c; c + 1), where c = 0 for
jxLap j • log 2 and c = ¡1 otherwise.
We compare the NGCA method against stand-alone FastICA with two different index func-
tions. Figure 2 shows boxplots and Figure 3 sample-wise comparison plots, over 100 sam-
ples, of the error criterion E ( bI ; I ) = m¡1 Pm
i=1 k(Id ¡ PI )bvi k2 , where fbvi gm
i=1 is an

Figure 4: 2D projection of the “oil
(12-dimensional) data obtained by different algo-
ﬂo w”
rithms, from left two right: PCA, Isomap, FastICA (tanh index), NGCA. In each case, the
data was ﬁrst projected in 3D using the respective methods, from which a 2D projection
was chosen visually so as to yield the clearest cluster structure. Available label information
was not used to determine the projections.

orthonormal basis of bI , Id is the identity matrix, and PI denotes the orthogonal projection
on I . In datasets (A),(B),(C), NGCA appears to be on par with the best FastICA method.
As expected, the best index for FastICA is data-dependent: the ’tanh’ index is more suited
to the super-Gaussian data (B), while the ’pow3’ index works best with the sub-Gaussian
data (C) (although, in this case, FastICA with this index has a tendency to get caught in
local minima, leading to a disastrous result for about 25% of the samples. Note that NGCA
does not suffer from this problem). Finally, the advantage of the implicit index adaptation
feature of NGCA can be clearly observed in the data set (D), which includes both sub- and
super-Gaussian components. In this case, neither of the two FastICA index functions taken
alone does well, and NGCA gives signi ﬁcantly lower error than either FastICA ﬂa vor.

Example of application for realistic data: visualization and clustering We now give
an example of application of NGCA to visualization and clustering of realistic data. We
consider here “oil
ﬂo w” data, which has been obtained by numerical simulation of a com-
plex physical model. This data was already used before for testing techniques of dimension
reduction [2]. The data is 12-dimensional and our goal is to visualize the data, and possibly
exhibit a clustered structure. We compared results obtained with the NGCA methodology,
regular PCA, FastICA with tanh index and Isomap. The results are shown on Figure 4. A
3D projection of the data was ﬁrst computed using these methods, which was in turn pro-
jected in 2D to draw the ﬁgure;
this last projection was chosen manually so as to make the
cluster structure as visible as possible in each case. The NGCA result appears better with a
clearer clustered structure appearing. This structure is only partly visible in the Isomap re-
sult; the NGCA method additionally has the advantage of a clear geometrical interpretation
(linear orthogonal projection). Finally, datapoints in this dataset are distributed in 3 classes.
This information was not used in the different procedures, but we can see a posteriori that
only NGCA clearly separates the classes in distinct clusters. Clustering applications on
other benchmark datasets is presented in the extended paper [3].

5 Conclusion

We proposed a new semi-parametric framework for constructing a linear projection to sep-
arate an uninteresting, possibly of large amplitude multivariate Gaussian ‘noise’ subspace
from the ‘signal-of-interest’ subspace. We provide generic consistency results on how well
the non-Gaussian directions can be identi ﬁed (Theorem 1). Once the low-dimensional ‘sig-
nal’ part is extracted, we can use it for a variety of applications such as data visualization,
clustering, denoising or classiﬁcation.
Numerically we found comparable or superior performance to, e.g., FastICA in deﬂation
mode as a generic representative of the family of PP algorithms. Note that in general,
PP methods need to pre-specify a projection index with which they search non-Gaussian

components. By contrast, an important advantage of our method is that we are able to
simultaneously use several families of nonlinear functions; moreover, also inside a same
function family we are able to use an entire range of parameters (such as frequency for
Fourier functions). Thus, NGCA provides higher ﬂe xibility, and less restricting assump-
tions a priori on the data. In a sense, the functional indices that are the most relevant for
the data at hand are automatically selected.
Future research will adapt the theory to simultaneously estimate the dimension of the non-
Gaussian subspace. Extending the proposed framework to non-linear projection scenarios
[4, 11, 10, 12, 1, 6] and to ﬁnding the most discriminative directions using labels are exam-
ples for which the current theory could be taken as a basis.
Acknowledgements: This work was supported in part by the PASCAL Network of Excel-
lence (EU # 506778).

Proof of Proposition 1 Put ﬁ = E [X h(X )] and ˆ(x) = h(x)¡ﬁ>x. Note that rˆ = rh¡ﬁ,
hence ﬂ (h) = E [rˆ(X )]. Furthermore, it holds by change of variable that
Z ˆ(x + u)p(x)dx = Z ˆ(x)p(x ¡ u)dx:
Under mild regularity conditions on p(x) and h(x), differentiating this with respect to u gives
E [rˆ(X )] = Z rˆ(x)p(x)dx = ¡ Z ˆ(x)rp(x)dx = ¡E [ˆ(X )r log p(X )] ;
where we have used rp(x) = r log p(x) p(x). Eq.(1) now implies r log p(x) = r log g(T x) ¡
¡ ¡1x, hence
ﬂ (ˆ) = ¡E [ˆ(X )r log g(T X )] + E £ˆ(X )¡ ¡1X ⁄
E [X h(X )]i :
E hX h(X ) ¡ XX >
= ¡T >
E [ˆ(X )rg(T X )=g(T X )] + ¡ ¡1
The last term above vanishes because we assumed E £XX > ⁄ = Id : The ﬁrst term belongs to I by
deﬁnition. This concludes the proof.
⁄
References
[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data represen-
tation. Neural Computation, 15(6):1373 –1396, 2003.
[2] C.M. Bishop, M. Svensen and C.K.I. Wiliams. GTM: The generative topographic mapping.
Neural Computation, 10(1):215–234, 1998.
[3] G. Blanchard, M. Sugiyama, M. Kawanabe, V. Spokoiny, K.-R. M ¨uller.
In search of non-
Gaussian components of a high-dimensional distribution. Technical report of the Weierstrass
Institute for Applied Analysis and Stochastics, 2006.
[4] T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman & Hall, London, 2001.
[5] J.H. Friedman and J.W. Tukey. A projection pursuit algorithm for exploratory data analysis.
IEEE Transactions on Computers, 23(9):881 –890, 1975.
[6] S. Harmeling, A. Ziehe, M. Kawanabe and K.-R. M ¨uller. Kernel-based nonlinear blind source
separation. Neural Computation, 15(5):1089–1124, 2003.
[7] P.J. Huber. Projection pursuit. The Annals of Statistics, 13:435 –475, 1985.
[8] A. Hyv ¨arinen. Fast and robust ﬁx ed-point algorithms for independent component analysis. IEEE
Transactions on Neural Networks, 10(3):626 –634, 1999.
[9] A. Hyv ¨arinen, J. Karhunen and E. Oja. Independent component analysis. Wiley, 2001.
[10] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-
ence, 290(5500):2323–2326, 2000.
[11] B. Sch ¨olkopf, A.J. Smola and K.–R. M ¨uller. Nonlinear component analysis as a kernel Eigen-
value problem. Neural Computation, 10(5):1299–1319, 1998.
[12] J.B. Tenenbaum, V. de Silva and J.C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319 –2323, 2000.


Generalization Error Bounds for Aggregation by
Mirror Descent with Averaging

Anatoli Juditsky
Laboratoire de Mod ´elisation et Calcul - Universit ´e Grenoble I
B.P. 53, 38041 Grenoble, France
anatoli.iouditski@imag.fr

Alexander Nazin
Institute of Control Sciences - Russian Academy of Science
65, Profsoyuznaya str., GSP-7, Moscow, 117997, Russia
nazine@ipu.rssi.ru

Alexandre Tsybakov
Laboratoire de Probabilit ´es et Mod `eles Al ´eatoires - Universit ´e Paris VI
4, place Jussieu, 75252 Paris Cedex, France
tsybakov@ccr.jussieu.fr

Nicolas Vayatis
Laboratoire de Probabilit ´es et Mod `eles Al ´eatoires - Universit ´e Paris VI
4, place Jussieu, 75252 Paris Cedex, France
vayatis@ccr.jussieu.fr

Abstract

We consider the problem of constructing an aggregated estimator from
a ﬁnite class of base functions which approximately minimiz es a con-
vex risk functional under the ℓ1 constraint. For this purpose, we propose
a stochastic procedure, the mirror descent, which performs gradient de-
scent in the dual space. The generated estimates are additionally aver-
aged in a recursive fashion with speci ﬁc weights. Mirror des cent algo-
rithms have been developed in different contexts and they are known to
be particularly efﬁcient in high dimensional problems. Mor eover their
implementation is adapted to the online setting. The main result of the
paper is the upper bound on the convergence rate for the generalization
error.

1

Introduction

We consider the aggregation problem (cf. [16]) where we have at hand a ﬁnite class of M
predictors which are to be combined linearly under an ℓ1 constraint kθk1 = λ on the vec-
tor θ ∈ RM that determines the coefﬁcients of the linear combination.
In order to exhibit
such a combination, we focus on the strategy of penalized convex risk minimization which

is motivated by recent statistical studies of boosting and SVM algorithms [11, 14, 18].
Moreover, we take a stochastic approximation approach which is particularly relevant in
the online setting since it leads to recursive algorithms where the update uses a single data
observation per iteration step. In this paper, we consider a general setting for which we
propose a novel stochastic gradient algorithm and show tight upper bounds on its expected
accuracy. Our algorithm builds on the ideas of mirror descent methods, ﬁrst introduced by
Nemirovski and Yudin [12], which consider updates of the gradient in the dual space. The
mirror descent algorithm has been successfully applied in high dimensional problems both
in deterministic and stochastic settings [2, 7]. In the present work, we describe a partic-
ular instance of the algorithm with an entropy-like proxy function. This method presents
similarities with the exponentiated gradient descent algorithm which was derived under dif-
ferent motivations in [10]. A crucial distinction between the two is the additional averaging
step in our version which guarantees statistical performance. The idea of averaging recur-
sive procedures is well-known (see e.g. [13] and the references therein) and it has been
invoked recently by Zhang [19] for the standard stochastic gradient descent (taking place
in the initial parameter space). Also it is worth noticing that most of the existing online
methods are evaluated in terms of relative loss bounds which are related to the empirical
risk while we focus on generalization error bounds (see [4, 5, 10] for insights on connec-
tions between the two types of criteria). The rest of the paper is organized as follows.
We ﬁrst introduce the setup (Section 2), then we describe the algorithm and state the main
convergence result (Section 3). Further we provide the intuition underlying the proposed
algorithm, and compare it to other methods (Section 4). We end up with a technical section
dedicated to the proof of our main result (Section 5).

2 Setup and notations

Let Z be a random variable with values in a measurable space (Z , A). We set a parameter
λ > 0, and an integer M ≥ 2. The unknown parameter is a vector θ ∈ RM which is
compelled to stay in the decision set Θ = ΘM ,λ deﬁned by:
ΘM ,λ = (cid:26)θ = (θ(1) , . . . , θ(M ) )T ∈ RM
θ(i) = λ(cid:27) .
+ : XM
i=1
Now we introduce the loss function Q : Θ × Z → R+ such that the random function
Q(· , Z ) : Θ → R+ is convex for almost all Z and deﬁne the convex risk function A : Θ →
R+ to be minimized as follows:
(2)

A(θ) = E Q(θ , Z ) .

(1)

Assume a training sample is given in the form of a sequence (Z1 , . . . , Zt−1 ), where each
Zi has the same distribution as Z . We assume for simplicity that the training sequence is
i.i.d. though this assumption can be weakened.

We propose to minimize the convex target function A over the decision set Θ on the basis
of the stochastic sub-gradients of Q:
ui (θ) = ∇θQ(θ , Zi ) ,
i = 1, 2, . . . ,
Note that the expectations E ui (·) belong to the sub-differential of A(·).
In the sequel, we will characterize the accuracy of an estimate bθt = bθt (Z1 , . . . , Zt−1 ) ∈ Θ
of the minimizer of A by the excess risk:
E A(bθt ) − min
(4)
A(θ)
θ∈Θ
where the expectation is taken over the sample (Z1 , . . . , Zt−1 ).

(3)

θ(j ) ln θ(j ) ,

We now introduce the notation that is necessary to present the algorithm in the next section.
For a vector z = (cid:0)z (1) , . . . , z (M ) (cid:1)T
∈ RM , deﬁne the norms
def= XM
def= max
zT θ = max
j=1 |z (j ) | ,
j=1,...,M |z (j ) | .
kzk1
kzk∞
kθk1=1
The space RM equipped with the norm k · k1 is called the primal space E and the same
space equipped with the dual norm k · k∞ is called the dual space E ∗ .
Introduce a so-called entropic proxy function:
∀ θ ∈ Θ, V (θ) = λ ln (M /λ) + XM
j=1
which has its minimum at θ0 = (λ/M , . . . , λ/M )T . It is easy to check that this function is
α-strongly convex with respect to the norm k · k1 with parameter α = 1/λ , i.e.,
α
s(1 − s)kx − yk2
V (sx + (1 − s)y) ≤ sV (x) + (1 − s)V (y) −
1
2
for all x, y ∈ Θ and any s ∈ [0, 1].
Let β > 0 be a parameter. We call β -conjugate of V the following convex transform:
θ∈Θ (cid:8)−zT θ − βV (θ)(cid:9) .
∀ z ∈ RM , Wβ (z ) def= sup
As it straightforwardly follows from (5), the β -conjugate is given here by:
e−z (k) /β (cid:19) ,
Wβ (z ) = λ β ln (cid:18) 1
M XM
∀ z ∈ RM ,
k=1
which has a Lipschitz-continuous gradient w.r.t. k · k1 , namely,
λ
∀ z , ˜z ∈ RM .
k∇Wβ (z ) − ∇Wβ ( ˜z )k1 ≤
β kz − ˜zk∞ ,

(5)

(6)

(7)

(8)

Though we will focus on a particular algorithm based on the entropic proxy function, our
results apply for a generic algorithmic scheme which takes advantage of the general proper-
ties of convex transforms (see [8] for details). The key property in the proof is the inequality
(8).

3 Algorithm and main result

The mirror descent algorithm is a stochastic gradient algorithm in the dual space. At each
iteration i, a new data point (Xi , Yi ) is observed and there are two updates: one is the value
ζi as the result of the stochastic gradient descent in the dual space, the other is the update of
the parameter θi which is the ”mirror image” of
ζi . In order to tune the algorithm properly,
we need two ﬁxed positive sequences
(γi )i≥1 (stepsize) and (βi )i≥1 (temperature) such
that βi ≥ βi−1 . The mirror descent algorithm with averaging is as follows:
Algorithm.

• Fix the initial values θ0 ∈ Θ and ζ0 = 0 ∈ RM .
• For i = 1, . . . , t − 1, do

ζi = ζi−1 + γiui (θi−1 ) ,

θi = −∇Wβi (ζi ) .

(9)

(10)

(11)

• Output at iteration t the following convex combination:
γi θi−1 .Xt
ˆθt = Xt
γj .
j=1
i=1
At this point, we actually have described a class of algorithms. Given the observations of
the stochastic sub-gradient (3), particular choices of the proxy function V , of the stepsize
and temperature parameters, will determine the algorithm completely. We discuss these
choices with more details in [8]. In this paper, we focus on the entropic proxy function and
consider a nearly optimal choice for the stepsize and temperature parameters which is the
following:
βi = β0√i + 1 ,
γi ≡ 1 ,
We can now state our rate of convergence result.
Theorem. Assume that the loss function Q satis ﬁes the following boundedness condition:
E k∇θQ(θ , Z )k2
∞ ≤ L2 < ∞ .
(12)
sup
θ∈Θ

i = 1, 2, . . . ,

β0 > 0 .

Fix also β0 = L/√ln M .
Then, for any integer t ≥ 1, the excess risk of the estimate bθt described above satis ﬁes the
following bound:
A(θ) ≤ 2 Lλ ( ln M )1/2 √t + 1
E A(bθt ) − min
(13)
.
t
θ∈Θ
Example. Consider the setting of supervised learning where the data are modelled by a
pair (X, Y ) with X ∈ X being an observation vector and Y a label, either integer (clas-
si ﬁcation) or real-valued (regression). Boosting and SVM a lgorithms are related to the
minimization of a functional

R(f ) = Eϕ(Y f (X ))
where ϕ is a convex non-negative cost function (typically exponential, logit or hinge loss)
and f belongs to a given class of combined predictors. The aggregation problem consists in
et of predictors {h1 , . . . , hM }
ﬁnding the best linear combination of elements from a ﬁnite s
with hj : X → [−K, K ]. Taking compact notations, it means that we search for f of the
form f = θT H with H denoting the vector-valued function whose components are these
base predictors:

H (x) = (h1 (x), . . . , hM (x))T ,
and θ belonging in a decision set Θ = ΘM ,λ . Take for instance ϕ to be non-increasing.
It is easy to see that this problem can be interpreted in terms of our general setting with
Z = (X, Y ), Q(Z, θ) = ϕ(Y θT H (X )) and L = K ϕ′ (K λ).

4 Discussion

In this section, we provide some insights on the method and the result of the previous
section.

4.1 Heuristics

Suppose that we want to minimize a convex function θ 7→ A(θ) over a convex set Θ. If
θ0 , . . . , θt−1 are the available search points at iteration t, we can provide the afﬁne approx-
imations φi of the function A deﬁned, for θ ∈ Θ, by
φi (θ) = A(θi−1 ) + (θ − θi−1 )T ∇A(θi−1 ),

i = 1, . . . , t .

.

(14)

θt = arg min
θ∈Θ

¯φt (θ) = arg min
θ∈Θ

Here θ 7→ ∇A(θ) is a vector function belonging to the sub-gradient of A(·). Taking a
convex combination of the φi ’s, we obtain an averaged approximation of A(θ):
i=1 γi (cid:0)A(θi−1 ) + (θ − θi−1 )T ∇A(θi−1 )(cid:1)
¯φt (θ) = Pt
Pt
i=1 γi
At ﬁrst glance, it would seem reasonable to choose as the next
search point a vector θ ∈ Θ
minimizing the approximation ¯φt , i.e.,
θT   tXi=1
γi∇A(θi−1 )! .
However, this does not make any progress, because our approximation is “good” only in
the vicinity of search points θ0 , . . . , θt−1 . Therefore, it is necessary to modify the criterion,
for instance, by adding a special penalty Bt (θ , θt−1 ) to the target function in order to keep
the next search point θt in the desired region. Thus, one chooses the point:
θ∈Θ "θT   tXi=1
γi∇A(θi−1 )! + Bt (θ , θt−1 )# .
θt = arg min
Our algorithm corresponds to a speci ﬁc type of penalty Bt (θ , θt−1 ) = βtV (θ), where
V is the proxy function. Also note that in our problem the vector-function ∇A(·) is not
available. Therefore, we replace in (15) the unknown gradients ∇A(θi−1 ) by the observed
stochastic sub-gradients ui (θi−1 ). This yields a new deﬁnition of the t-th search point:
θ∈Θ "θT   tXi=1
γiui (θi−1 )! + βtV (θ)# = arg max
θ∈Θ (cid:2)−ζ T
t θ − βtV (θ)(cid:3) ,
θt = arg min
where ζt = Pt
i=1 γiui (θi−1 ). By a standard result of convex analysis (see e.g. [3]), the
solution to this problem reads as −∇Wβt (ζt ) and it is now easy to deduce the iterative
scheme (9) of the mirror descent algorithm.
4.2 Comparison with previous work

(15)

(16)

(17)

i = 1, 2, . . . ,

The versions of mirror descent method proposed in [12] are somewhat different from our
iterative scheme (9). One of them, closest to ours, is studied in detail in [3]. It is based on
the recursive relation
θi = −∇W1(cid:16) − ∇V (θi−1 ) + γiui (θi−1 )(cid:17),
where the function V is strongly convex with respect to the norm of initial space E (which
1 ) and W1 is the 1-conjugate function to V .
is not necessarily the space ℓM
If Θ = RM and V (θ) = 1
2 , the scheme of (17) coincides with the ordinary gradient
2 kθk2
method.
For the unit simplex Θ = ΘM ,1 and the entropy type proxy function V from (5) with
λ = 1, the coordinates θ(j )
of vector θi from (17) are:
i
γmum, j (θm−1 )!
0 exp  −
iXm=1
θ(j )
γmum, k (θm−1 )! .
exp  −
iXm=1
MXk=1
θ(k)
0
The algorithm is also known as the exponentiated gradient (EG) method [10]. The differ-
ences between the algorithm (17) and ours are the following:

∀j = 1, . . . , M ,

θ(j )
i =

(18)

• the initial iterative scheme of the Algorithm is different than that of (17), partic-
ularly, it includes the second tuning parameter βi ; moreover, the algorithm (18)
uses initial value θ0 in a different manner;
• our algorithm contains the additional averaging step of the updates (10).
The convergence properties of the EG method (18) have been studied in a determinis-
tic setting [6]. Namely, it has been shown that, under some assumptions, the difference
At (θt ) − minθ∈ΘM,1 At (θ), where At is the empirical risk, is bounded by a constant de-
pending on M and t. If this constant is small enough, these results show that the EG method
provides good numerical minimizers of the empirical risk At . The averaging step allows
the use of the results provided in [5] to derive generalization error bounds from relative loss
bounds. This technique leads to rates of convergence of the order p(ln M )/t as well but
with suboptimal multiplicative factor in λ.
Finally, we point out that the algorithm (17) may be deduced from the ideas mentioned in
Subsection 4.1 and which are studied in the literature on proximal methods within the ﬁeld
of convex optimization (see, e.g., [9, 1] and the references therein). Namely, under rather
general conditions, the variable θi from (17) solves the the minimization problem
θ∈Θ (cid:0)θT γiui (θi−1 ) + B (θ , θi−1 )(cid:1) ,
θi = arg min
where the penalty B (θ , θi−1 ) = V (θ) − V (θi−1 ) − (θ − θi−1 )T ∇V (θi−1 ) represents the
Bregman divergence between θ and θi−1 related to the function V .

(19)

4.3 General comments
Performance and ef ﬁciency. The rate of convergence of order √ln M /√t is typical with-
out low noise assumptions (as they are introduced in [17]). Batch procedures based on
minimization of the empirical convex risk functional present a similar rate. From the statis-
tical point of view, there is no remarkable difference between batch and our mirror-descent
procedure. On the other hand, from the computational point of view, our procedure is quite
comparable with the direct stochastic gradient descent. However, the mirror-descent algo-
rithm presents two major advantages as compared both to batch and to direct stochastic gra-
dient: (i) its behavior with respect to the cardinality of the base class is better than for direct
stochastic gradient descent (of the order of √ln M in the Theorem, instead of M or √M
for direct stochastic gradient); (ii) mirror-descent presents a higher efﬁciency especially in
high-dimensional problems as its algorithmic complexity and memory requirements are of
strictly smaller order than for corresponding batch procedures (see [7] for a comparison).
Optimality of the rate of convergence. Using the techniques of [7] and [16] it is not hard
to prove minimax lower bound on the excess risk E A(bθt ) − minθ∈ΘM,λ A(θ) having the
order (ln M )1/2/√t for M ≥ t1/2+δ with some δ > 0. This indicates that the upper bound
of the Theorem is rate optimal for such values of M .
Choice of the base class. We point out that the good behaviour of this method crucially re-
lies on the choice of the base class of functions {hj }1≤j≤M . As far as theory is concerned,
in order to provide a complete statistical analysis, one should establish approximation error
bounds on the quantity inf f ∈FM,λ A(f ) − inf f A(f ) showing that the richness of the base
class is reﬂected both by diversity (orthogonality or indep endence) of the hj ’s and by its
cardinality M . For example, one can take hj ’s as the eigenfunctions associated to some
positive deﬁnite kernel. We refer to [14], [15], for related results. The choice of λ can be
motivated by similar considerations. In fact, to minimize the approximation error it might
be useful to take λ depending on the sample size t and tending to inﬁnity with some slow
rate as in [11]. A balance between the stochastic error as given in the Theorem and the
approximation error would then determine the optimal choice of λ.

5 Proof of the Theorem

Introduce the notation ∇A(θ) = Eui (θ) and ξi (θ) = ui (θ) − ∇A(θ). Put vi = ui (θi−1 )
which gives ζi − ζi−1 = γi vi . By continuous differentiability of Wβt−1 and by (8) we have:
Wβi−1 (ζi ) = Wβi−1 (ζi−1 ) + γi vT
i ∇Wβi−1 (ζi−1 )
+γi Z 1
i (cid:2)∇Wβi−1 (τ ζi + (1 − τ )ζi−1 ) − ∇Wβi−1 (ζi−1 )(cid:3) dτ
vT
0
λγ 2
i kvi k2
∞
≤ Wβi−1 (ζi−1 ) + γi vT
.
i ∇Wβi−1 (ζi−1 ) +
2βi−1
Then, using the fact that (βi )i≥1 is a non-decreasing sequence and that, for z ﬁxed, β 7→
Wβ (z ) is a non-increasing function, we get

λγ 2
i kvi k2
∞
2βi−1

.

Wβi (ζi ) ≤ Wβi−1 (ζi ) ≤ Wβi−1 (ζi−1 ) − γi θT
i−1 vi +
Summing up over the i’s and using the representation ζt = Pt
i=1 γi vi , we get:
∀θ ∈ Θ, Xt
t θ + Xt
λγ 2
i kvi k2
∞
γi (θi−1 − θ)T vi ≤ −Wβt (ζt ) − ζ T
2βi−1
i=1
i=1
since Wβ0 (ζ0 ) = 0. From deﬁnition of Wβ , we have, ∀ ζ ∈ RM and ∀ θ ∈ Θ, −Wβt (ζ ) −
ζ T θ ≤ βtV (θ). Finally, since vi = ∇A(θi−1 ) + ξi (θi−1 ), we get
tXi=1
tXi=1
tXi=1
i kvik2
λγ 2
∞
γi (θi−1 − θ)T ξi (θi−1 ) +
γi (θi−1 − θ)T ∇A(θi−1 ) ≤ βtV (θ) −
2βi−1
As we are to take expectations, we note that, conditioning on θi−1 and using the indepen-
dence between θi−1 and (Xi , Yi ), we have: E (cid:0)(θi−1 − θ)T ξi (θi−1 )(cid:1) = 0. Now, convexity
of A and the previous display lead to:
∀ θ ∈ Θ , E A(bθt ) − A(θ) ≤ Pt
i=1 γiE [(θi−1 − θ)T ∇A(θi−1 )]
Pt
i=1 γi
tXi=1
1
E [(θi−1 − θ)T ∇A(θi−1 )]
=
t
√t + 1
(cid:18)β0V ∗ +
β0 (cid:19) ,
λL2
≤
t
where we have set V ∗ = maxθ∈Θ V (θ) and made use of the boundedness assumption
∞ ≤ L2 and of the particular choice for the stepsize and temperature parameters.
E kui (θ)k2
Noticing that V ∗ = λ ln M and optimizing this bound in β0 > 0, we obtain the result.

.

Acknowledgments

We thank Nicol `o Cesa-Bianchi for sharing with us his expertise on relative loss bounds.

References

[1] Beck, A. & Teboulle, M. (2003) Mirror descent and nonlinear projected subgradient
methods for convex optimization. Operations Research Letters, 31:167–175.

[2] Ben-Tal, A., Margalit, T. & Nemirovski, A. (2001) The Ordered Subsets Mirror De-
scent optimization method and its use for the Positron Emission Tomography recon-
struction problem. SIAM J. on Optimization, 12:79–108.
[3] Ben-Tal, A. & Nemirovski, A.S. (1999) The conjugate barrier mirror descent method
for non-smooth convex optimization. MINERVA Optimization Center Report, Tech-
nion Institute of Technology.
Available at http://iew3.technion.ac.il/Labs/Opt/opt/Pap/CP MD.pdf
[4] Cesa-Bianchi, N. & Gentile, C. (2005) Improved risk tail bounds for on-line algo-
rithms. Submitted.
[5] Cesa-Bianchi, N., Conconi, A. & Gentile, C. (2004) On the generalization ability of
on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050–
2057.
[6] Helmbold, D.P., Kivinen, J. & Warmuth, M.K. (1999) Relative loss bounds for single
neurons. IEEE Trans. on Neural Networks, 10(6):1291–1304.
[7] Juditsky, A. & Nemirovski, A. (2000) Functional aggregation for nonparametric esti-
mation. Annals of Statistics, 28(3): 681–712.
[8] Juditsky, A.B., Nazin, A.V., Tsybakov, A.B. & Vayatis N. (2005) Recursive Aggre-
gation of Estimators via the Mirror Descent Algorithm with Averaging. Technical
Report LPMA, Universit ´e Paris 6.
Available at http://www.proba.jussieu.fr/pageperso/vayatis/publication.html
[9] Kiwiel, K.C. (1997) Proximal minimization methods with generalized Bregman func-
tions. SIAM J. Control Optim., 35:1142–1168.
[10] Kivinen J. & Warmuth M.K. (1997) Additive versus exponentiated gradient updates
for linear prediction. Information and Computation, Vol.132(1): 1–64.
[11] Lugosi, G. & Vayatis, N. (2004) On the Bayes-risk consistency of regularized boost-
ing methods (with discussion). Annals of Statitics, 32(1): 30–55.
[12] Nemirovski, A.S. & Yudin, D.B. (1983) Problem Complexity and Method Efﬁciency
in Optimization. Wiley-Interscience.
[13] Polyak, B.T. & Juditsky, A.B. (1992) Acceleration of stochastic approximation by
averaging. SIAM J. Control Optim., 30:838–855.
[14] Scovel, J.C. & Steinwart, I. (2005) Fast Rates for Support Vector Machines. In Pro-
ceedings of the 18th Conference on Learning Theory (COLT 2005), Bertinoro, Italy.
[15] Tarigan, B. & van de Geer, S. (2004) Adaptivity of Support Vector Machines with ℓ1
Penalty. Preprint, University of Leiden.
[16] Tsybakov, A. (2003) Optimal Rates of Aggregation. Proceedings of COLT’03, LNCS,
Springer, Vol. 2777:303–313.
[17] Tsybakov, A. (2004) Optimal aggregation of classi ﬁers
of Statistics, 32(1):135–166.
[18] Zhang, T. (2004) Statistical behavior and consistency of classi ﬁcation methods based
on convex risk minimization (with discussion). Annals of Statistics, 32(1):56–85.
[19] Zhang, T. (2004) Solving large scale linear prediction problems using stochastic gra-
dient descent algorithms. In Proceedings of ICML’04.

in statistical learning. Annals


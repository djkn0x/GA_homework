Is Early Vision Optimized for Extracting
Higher-order Dependencies?

Yan Karklin
yan+@cs.cmu.edu

Michael S. Lewicki∗
lewicki@cnbc.cmu.edu

Computer Science Department &
Center for the Neural Basis of Cognition
Carnegie Mellon University

Abstract

Linear implementations of the efﬁcient coding hypothesis,
such as inde-
pendent component analysis (ICA) and sparse coding models, have pro-
vided functional explanations for properties of simple cells in V1 [1, 2].
These models, however, ignore the non-linear behavior of neurons and
fail to match individual and population properties of neural receptive
ﬁelds in subtle but important ways. Hierarchical models, in cluding Gaus-
sian Scale Mixtures [3, 4] and other generative statistical models [5, 6],
can capture higher-order regularities in natural images and explain non-
linear aspects of neural processing such as normalization and context ef-
fects [6, 7]. Previously, it had been assumed that the lower level represen-
tation is independent of the hierarchy, and had been ﬁxed whe n training
these models. Here we examine the optimal lower-level representations
derived in the context of a hierarchical model and ﬁnd that th e resulting
representations are strikingly different from those based on linear mod-
els. Unlike the the basis functions and ﬁlters learned by ICA or sparse
coding, these functions individually more closely resemble simple cell
receptive ﬁelds and collectively span a broad range of spati al scales. Our
work uniﬁes several related approaches and observations ab out natural
image structure and suggests that hierarchical models might yield better
representations of image structure throughout the hierarchy.

1 Introduction

Efﬁcient coding hypothesis has been proposed as a guiding co mputational principle for
the analysis of early visual system and motivates the search for good statistical models of
natural images. Early work revealed that image statistics are highly non-Gaussian [8, 9],
and models such as independent component analysis (ICA) and sparse coding have been
developed to capture these statistics to form efﬁcient repr esentations of natural images. It
has been suggested that these models explain the basic computational goal of early visual
cortex, as evidenced by the similarity between the learned parameters and the measured
receptive ﬁelds of simple cells in V1.

∗To whom correspondence should be addressed

In fact, it is not clear exactly how well these methods predict the shapes of neural recep-
tive ﬁelds. There has been no thorough characterization of I CA and sparse coding results
for different datasets, pre-processing methods, and speciﬁc learning algorithms employed,
although some of these factors clearly affect the resulting representation [10]. When ICA
or sparse coding is applied to natural images, the resulting basis functions resemble Ga-
bor functions [1, 2] — 2D sine waves modulated by Gaussian env elopes — which also
accurately model the shapes of simple cell receptive ﬁelds [ 11]. Often, these results are
visualized in a transformed space, by taking the logarithm of the pixel intensities, spher-
ing (whitening) the image space, or ﬁltering the images to ﬂa
tten their spectrum. When
analyzed in the original image space, the learned ﬁlters (the models’ analogues of ne ural
receptive ﬁelds) do not exhibit the multi-scale properties of the visual system, as they tend
to cluster at high spatial frequencies [10, 12]. Neural receptive ﬁelds, on the other hand,
span a broad range of spatial scales, and exhibit distributions of spatial phase and other
parameters unmatched by ICA and SC results [13, 14]. Therefore, as models of early visual
processing, these models fail to predict accurately either the individual or the population
properties of cortical visual neurons.

Linear efﬁcient coding methods are also limited in the type o f statistical structure they can
capture. Applied to natural images, their coefﬁcients cont ain signiﬁcant residual depen-
dencies that cannot be accounted for by the linear form of the models. Several solutions
have been proposed, including multiplicative Gaussian Scale Mixtures [4] and generative
hierarchical models [5, 6]. These models capture some of the observed dependencies; but
their analysis so far has been focused on the higher-order structure learned by the model.
Meanwhile, the lower-level representation is either chosen a priori [4] or adapted sepa-
rately, in the absence of the hierarchy [6] or with a ﬁxed hier archical structure speciﬁed in
advance [5].

Here we examine whether the optimal lower-level representation of natural images is dif-
ferent when trained in the context of such non-linear hierarchical models. We also illustrate
how the model not only describes sparse marginal densities and magnitude dependencies,
but captures a variety of joint density functions that are consistent with previous obser-
vations and theoretical conjectures. We show that learned lower-level representations are
strikingly different from those learned by the linear models: they are more multi-scale,
spanning a wide range of spatial scales and phases of the Gabor sinusoid relative to the
Gaussian envelope. Finally, we place these results in the context of whitening, gain con-
trol, and non-linear neural processing.

2 Fully adaptable scale mixture model

A simple and scalable model for natural image patches is a linear factor model, in which the
data x are assumed to be generated as a linear combination of basis functions with additive
noise

x = Au +  .

(1)

Typically, the noise is assumed to be Gaussian with variance σ2
 , thus
P (x|A, u) ∝ exp  − Xi
i ! .
|x − Au|2
The coefﬁcients u are assumed to be mutually independent, and often modeled with sparse
distributions (e.g. Laplacian) that re ﬂect the non-Gaussi an statistics of natural scenes [8, 9],
P (ui ) ∝ exp(− Xi
P (u) = Yi

1
2σ2


|ui |) .

(2)

(3)

We can then adapt the basis functions A to maximize the expected log-likelihood of the
data L = hlog P (x|A)i over the data ensemble, thereby learning a compact, efﬁcien t rep-
resentation of structure in natural images. This is the model underlying the sparse coding
algorithm [2] and closely related to independent component analysis (ICA) [1].

An alternative to ﬁxed sparse priors for u (3) is to use a Gaussian Scale Mixture (GSM)
model [3]. In these models, each observed coefﬁcient ui is modeled as a product of random
Gaussian variable yi and a multiplier λi ,
ui = pλi yi
(4)
Conditional on the value of the multiplier λi , the probability P (ui |λi ) is Gaussian with
variance λi , but the form of the marginal distribution
P (ui ) = Z N (0, λi )P (λi )dλi
depends on the probability function of λi and can assume a variety of shapes, including
sparse heavy-tailed functions that ﬁt the observed distrib utions of wavelet and ICA coef-
ﬁcients [4]. This type of model can also account for the obser ved dependencies among
coefﬁcients u, for example, by expressing them as pair-wise dependencies among the mul-
tiplier variables λ [4, 15].

(5)

A more general model, proposed in [6, 16], employs a hierarchical prior for P (u) with
adapted parameters tuned to the global patterns in higher-order dependencies. Speciﬁcally,
the logarithm of the variances of P (u) is assumed to be a linear function of the higher-order
random variables v,

log σ2
u = Bv .

(6)

Conditional on the higher-order variables, the joint distribution of coefﬁcients is fac-
torisable, as in GSM. In fact, if the conditional density P (u|v) is Gaussian, this Hi-
u and
erarchical Scale Mixture (HSM) is equivalent to a GSM model, with λ = σ2
P (u|λ) = P (u|v) = N (0, exp(Bv)), with the added advantage of a more ﬂexible rep-
resentation of higher-order statistical regularities in B. Whereas previous GSM models of
natural images focused on modeling local relationships between coefﬁcients of ﬁxed linear
transforms, this general hierarchical formulation is fully adaptable, allowing us to recover
the optimal lower-level representation A, as well as the higher-order components B.

Parameter estimation in the HSM involves adapting model parameters A and B to max-
imize data log-likelihood L = hlog P (x|A, B)i. The gradient descent algorithm for the
estimation of B has been previously described (see [6]). The optimal lower-level basis A is
computed similarly to the sparse coding algorithm — the goal
is to minimize reconstruction
error of the inferred MAP estimate ˆu. However, ˆu is estimated not with a ﬁxed sparsifying
prior, but with a concurrently adapted hierarchical prior. If we assume a Gaussian condi-
tional density P (u|v) and a standard-Normal prior P (v), the MAP estimates are computed
as

{ ˆu, ˆv} = arg min
u,v

P (u, v|x, A, B)

(7)

(8)

P (x|A, B, u, v)P (u|v)P (v)
= arg min
u,v
u,v 
2 
i + Xj   [Bv]j
2e[Bv]j ! + Xk
u2
v2
1
|x − Au|2
j
 Xi
k
= arg min
 .

2σ2
2
Marginalizing over the latent higher-order variables in the hierarchical models leads to
sparse distributions similar to the Laplacian and other density functions assumed in ICA.

(9)

+

Gaussian, qu = 2

Laplacian, qu = 1

Gen Gauss, qu = 0.7

HSM, B = [0;0]

HSM, B = [1;1]

HSM, B = [2;2]

HSM, B = [1;−1]

HSM, B = [2;−2]

HSM, B = [1;−2]

Figure 1: This model can describe a variety of joint density functions for coefﬁcients u.
Here we show example scatter plots and contour plots of some bivariate densities. Top row:
Gaussian, Laplacian, and generalized Gaussian densities of the form p(u) ∝ exp(−|u|q ).
Middle and bottom row: Hierarchical Scale Mixtures with different sets of parameters B.
For illustration, in the hierarchical models the dimensionality of v is 1, and the matrix B
is simply a column vector. These densities are computed by marginalizing over the latent
variables v, here assumed to follow a standard normal distribution. Even with this simple
hierarchy, the model can generate sparse star-shaped (bottom row) or radially symmetric
(middle row) densities, as well as more complex non-symmetric densities (bottom right). In
higher dimensions, it is possible to describe more complex joint distributions, with different
marginals along different projections.

However, although the model distribution for individual coefﬁcients is similar to the ﬁxed
sparse priors of ICA and sparse coding, the model is fundamentally non-linear and might
yield a different lower-level representation; the coefﬁci ents u are no longer mutually inde-
pendent, and the optimal set of basis functions must account for this.

Also, the shape of the joint marginal distribution in the space of all the coefﬁcients is more
complex than the i.i.d. joint density of the linear models. Bi-variate joint distributions of
GSM coefﬁcients can capture non-linear dependencies in wav elet coefﬁcients [4]. In the
fully adaptable HSM, however, the joint density can take a variety of shapes that depend on
the learned parameters B ( ﬁgure 1). Note that this model can produce sparse, star-sha ped
distributions as in the linear models, or radially symmetric distributions that cannot be
described by the linear models. Such joint density pro ﬁles h ave been observed empirically
in the responses of phase-offset wavelet coefﬁcients to nat ural images and have inspired
polar transformation and quadrature pair models [17] (as well as connections to phase-
invariant neural responses). The model described here can capture these joint densities and
others, but rather than assume this structure a priori, it learns it automatically from the data.

3 Methods

To examine how the lower-level representation is affected by the hierarchical model struc-
ture, we compared A learned by the sparse coding algorithm [2] and the HSM described
above. The models were trained on 20 × 20 image patches sampled from 40 images of out-

door scenes in the Kyoto dataset [12]. We applied a low-pass radially symmetric ﬁlter to the
full images to eliminate high corner frequencies (artifacts of the square sampling lattice),
and removed the DC component from each image patch, but did no further pre-processing.
All the results and analyses are reported in the original data space. Noise variance σ2
 was
set to 0.1, and the basis functions were initialized to small random values and adapted on
stochastically sampled batches of 300 patches. We ran the algorithm for 10,000 iterations
with a step size of 0.1 (tapered for the last 1,000 iterations, once model parameters were
relatively unchanging).

The parameters of the hierarchical model were estimated in a similar fashion. Gradient
descent on A and B was performed in parallel using MAP estimates ˆu and ˆv. The step
size for adapting B was gradually increased from .0001 to .01, because emergence of the
variance patterns requires some stabilization in the basis functions in A.

Because encoding in the sparse coding and in the hierarchical model is a non-linear process,
it is not possible to compare the inverse of A to physiological data. Instead, we estimated
the corresponding ﬁlters using reverse correlation to derive a linear approximation to a
non-linear system, which is also a common method for characterizing V1 simple cells. We
analyzed the resulting ﬁlters by ﬁtting them with 2D Gabor fu
nctions, then examining the
distribution of their frequencies, phase, and orientation parameters.

4 Results

The shapes of basis functions and ﬁlters obtained with spars e coding have been previously
analyzed and compared to neural receptive ﬁelds [10, 14]. Ho wever, some of the reported
results were in the whitened space or obtained by training on ﬁltered images. In the original
space, sparse coding basis functions have very particular shapes: except for a few large, low
frequency functions, all are localized, odd-symmetric, and span only a single period of the
sinusoid ( ﬁgure 2, top left). The estimated ﬁlters are simil
ar but smaller ( ﬁgure 2, bottom
left), with peak spatial frequencies clustered at higher frequencies ( ﬁgure 3).

In the hierarchical model, the learned representation is strikingly different ( ﬁgure 2, right
panels). Both the basis and the ﬁlters span a wider range of sp atial scales, a result previously
unobserved for models trained on non-preprocessed images, and one that is more consistent
with physiological data [13, 14]. Also, the shapes of the basis functions are different —
they more closely resemble Gabor functions, although they tend to be less smooth than the
sparse coding basis functions. Both SC- and HSM-derived ﬁlt ers are well ﬁt with Gabor
functions.

We also compared the distributions of spatial phases for ﬁlt ers obtained with sparse coding
and the hierarchical model ( ﬁgure 4). While sparse coding ﬁl
ters exhibit a strong tendency
for odd-symmetric phase pro ﬁles, the hierarchical model re sults in a much more uniform
distribution of spatial phases. Although some phase asymmetry has been observed in sim-
ple cell receptive ﬁelds, their phase properties tend to be m uch more uniform than sparse
coding ﬁlters [14].

In the hierarchical model, the higher-order representation B is also adapted to the statistical
structure of natural images. Although the choice of the prior density for v (e.g. sparse or
Gaussian) can determine the type of structure captured in B, we discovered that it does
not affect the nature of the lower-level representation. For the results reported here, we
assumed a Gaussian prior on v. Thus, as in other multi-variate Gaussian models, the precise
directions of B are not important; the learned vectors only serve to collectively describe
the volume of the space. In this case, they capture the principal components of the log-
variances. Because we were interested speciﬁcally in the lower-level
representation, we did
not analyze the matrix B in detail, though the principal components of this space seem to

SC basis funcs

HSM basis funcs

SC filters

HSM filters

Figure 2: The lower-level representations learned by sparse coding (SC) and the hierarchi-
cal scale model (HSM). Shown are subsets of the learned basis functions and the estimates
for the ﬁlters obtained with reverse correlation. These fun ctions are displayed in the origi-
nal image space.

SC ﬁlters

90°

0.5

0.25

HSM ﬁlters

90°

0.5

0.25

180°
180°
0°
0°
0
0
Figure 3: Scatter plots of peak frequencies and orientations of the Gabor functions ﬁtted
to the estimated ﬁlters. The units on the radial scale are cyc les/pixel and the solid line is
the Nyquist limit. Although both SC and HSM ﬁlters exhibit pr edominantly high spatial
frequencies, the hierarchical model yields a representation that tiles the spatial frequency
space much more evenly.

SC phase

HSM phase

SC freq

HSM freq

40

20

40

20

50

25

50

25

0

0

0
0
π/4
π/2
π/4
π/2
0
0
0.06 0.13 0.25 0.50
0.06 0.13 0.25 0.50
Figure 4: The distributions of phases and frequencies for Gabor functions ﬁtted to sparse
coding (SC) and hierarchical scale model (HSM) ﬁlters. The p hase units specify the phase
of the sinusoid in relation to the peak of the Gaussian envelope of the Gabor function; 0 is
even-symmetric, π/2 is odd-symmetric. The frequency axes are in cycles/pixel.

group co-localized lower-level basis functions and separately represent spatial contrast and
oriented image structure. As reported previously [6, 16], with a sparse prior on v, the model
learns higher-order components that individually capture complex spatial, orientation, and
scale regularities in image data.

5 Discussion

We have demonstrated that adapting a general hierarchical model yields lower-level repre-
sentations that are signiﬁcantly different than those obta ined using ﬁxed priors and linear
generative models. The resulting basis functions and ﬁlter s are multi-scale and more con-
sistent with several observed characteristics of neural receptive ﬁelds.

It is interesting that the learned representations are similar to the results obtained when ICA
or sparse coding is applied to whitened images (i.e. with a ﬂa ttened power spectrum). This
might be explained by the fact that whitening “spheres” the i nput space, normalizing the
scale of different directions in the space. The hierarchical model is performing a similar
scaling operation through the inference of higher-order variables v that scale the priors
on basis function coefﬁcients u. Thus the model can rely on a generic “white ” lower
level representation, while employing an adaptive mechanism for normalizing the space,
which accounts for non-stationary statistics on an image-by-image basis [6]. A related
phenomenon in neural processing is gain control, which might be one speciﬁc type of a
general adaptation process.

The ﬂexibility of the hierarchical model allows us to learn a lower-level representation that
is optimal in the context of the hierarchy. Thus, we expect the learned parameters to de ﬁne
a better statistical model for natural images than other approaches in which the lower-level
representation or the higher-order dependencies are ﬁxed i n advance. For example, the ﬂex-
ible marginal distributions, illustrated in ﬁgure 1, shoul d be able to capture a wider range of
statistical structure in natural images. One way to quantify the bene ﬁt of an adapted lower-
level representation is to apply the model to problems like image de-noising and ﬁlling-in
missing pixels. Related models have achieved state-of-the-art performance [15, 18], and
we are currently investigating whether the added ﬂexibilit y of the model discussed here
confers additional advantages.

Finally, although the results presented here are more consistent with the observed proper-
ties of neural receptive ﬁelds, several discrepancies rema in. For example, our results, as
well as those of other statistical models, fail to account for the prevalence of low spatial fre-
quency receptive ﬁelds observed in V1. This could be a result of the speciﬁc choice of the
distribution assumed by the model, although the described hierarchical framework makes
few assumptions about the joint distribution of basis function coefﬁcients. More likely, the
non-stationary statistics of the natural scenes play a role in determining the properties of
the learned representation. As suggested by previous results [10], different image data-sets

can lead to different parameters. This provides a strong motivation for training models
with an “over-complete ” basis, in which the number of basis f unctions is greater than the
dimensionality of the input data [19]. In this case, different subsets of the basis functions
can adapt to optimally represent different image contexts, and the population properties of
such over-complete representations could be signiﬁcantly different. It would be particu-
larly interesting to investigate representations learned in these models in the context of a
hierarchical model.

References

[1] A. J. Bell and T. J. Sejnowski. The ’independent components’ of natural scenes are edge ﬁlters.
Vision Research, 37(23):3327–3338, 1997.
[2] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive- ﬁeld properties by learning
a sparse code for natural images. Nature, 381:607–609, 1996.
[3] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal
Statistical Society B, 36(1):99–102, 1974.
[4] M. J. Wainwright, E. P. Simoncelli, and A. S. Willsky. Random cascades on wavelet trees and
their use in analyzing and modeling natural images. Applied Computational and Harmonic
Analysis, 11:89–123, 2001.
[5] A. Hyv¨arinen, P. O. Hoyer, and M. Inki. Topographic independent component analysis. Neural
Computation, 13:1527–1558, 2001.
[6] Y. Karklin and M.S. Lewicki. A hierarchical bayesian model for learning non-linear statistical
regularities in non-stationary natural signals. Neural Computation, 17:397–423, 2005.
[7] O. Schwartz and E. P. Simoncelli. Natural signal statistics and sensory gain control. Nat.
Neurosci., 4:819–825, 2001.
[8] D. Field. What is the goal of sensory coding. Neural Computation, 6:559–601, 1994.
[9] D. R. Ruderman and W. Bialek. Statistics of natural images: Scaling in the woods. Physical
Review Letters, 73(6):814–818, 1994.
[10] J. H. van Hateren and A. van der Schaaf.
Independent component ﬁlters of natural images
compared with simple cells in primary visual cortex. Proceedings of the Royal Society, London
B, 265:359–366, 1998.
[11] J. P. Jones and L. A. Palmer. An evaluation of the two-dimensional gabor ﬁlter model of simple
receptive ﬁelds in cat striate cortex.
Journal of Neurophysiology, 58(6):1233–1258, 1987.
[12] E. Doi and M. S. Lewicki. Sparse coding of natural images using an overcomplete set of limited
capacity units. In Advances in Neural Processing Information Systems 18, 2004.
[13] R. L. De Valois, D. G. Albrecht, and L. G. Thorell. Spatial frequency selectivity of cells in
macaque visual cortex. Vision Research, 22:545–559, 1982.
[14] D. L. Ringach. Spatial structure and symmetry of simple-cell receptive ﬁelds in macaque pri-
mary visual cortex. Journal of Neurophysiology, 88:455–463, 2002.
[15] J. Portilla, V. Strela, M. J. Wainwright, and E.P. Simoncelli. Image denoising using Gaussian
scale mixtures in the wavelet domain. IEEE Transactions on Image Processing, 12:1338–1351,
2003.
[16] Y. Karklin and M.S. Lewicki. Learning higher-order structures in natural images. Network:
Computation in Neural Systems, 14:483–499, 2003.
[17] C. Zetzsche and G. Krieger. Nonlinear neurons and highorder statistics: New approaches to
human vision and electronic image processing. In B. Rogowitz and T.V. Pappas, editors, Proc.
SPIE on Human Vision and Electronic Imaging IV, volume 3644, pages 2–33, 1999.
[18] M. S. Lewicki and B. A. Olshausen. A probabilistic framework for the adaptation and compar-
ison of image codes. Journal of the Optical Society of America A, 16(7):1587–1601, 1999.
[19] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy
employed by V1? Vision Research, 37(23), 1997.


Combining Monocular and Stereo Depth Cues

Fraser Cameron

December 16, 2005

Abstract

A lot of work has been done extracting depth from image sequences, and relatively less has been done
using only single images. Very little has been done merging these together. This paper describes the fusing
of depth estimation from two images, with monocular cues. The paper will provide an overview of the stereo
algorithm, and the details of fusing the stereo range data with monocular image features.

1 Introduction

Recent work has been done on depth estimation from single images1 . While this is exciting, it could beneﬁt from
the existing stereo image depth algorithms. In a real control system there would afterall be streams of incoming
images. This paper will discuss ﬁrst the stereo algorithm, then how to fuse stereo depths with the monocular
features, and ﬁnally the results obtained so far.
There has been a lot of work done calculating depth from two stereo images. Here we assume only knowledge
of the camera calibration matrix, and no relative rotation. The rest is estimated from correspondencences. We
use Feature-based depth estimation techniques due to the availability of MATLAB 2 3 code, and a perceived
speed bonus. This is more fully explained in section 2.
Recent work by Sexena et al. has focused on depth estimation from monocular image cues. They have
supervised learning to train a Markov Random Field. This work has also been applied to obstacles depth
estimation for use controlling a remote control car at speed 4 . This second application speciﬁcally requires fast
computation. The intention of this work is to begin combining monocular and stereo image cues into a fast
computation, allowing improved depth estimation for dynamic control applications. We lay the groundwork for
this in section 3.
Due to time constraints we were only able to generate and test the stereo system. These details are provided
in section 4.
Finally the paper close with some conclusions, and notes on work in progress in section 5, and acknowledge-
ments in sections 6.

2 Stereo Depth Estimation

Depth Estimation in this paper using image sequences is broken up into 3 sections: Feature Detection and
Correlation, Estimating the Fundamental Matrix, Additional Guided Matching, Depth Estimation, and Error
Estimation.

2.1 Feature Detection and Correlation

For correlation we desire a relatively small and even smattering of feature points. This is achieved by corners. In
this paper we use Harris corner detection, which uses a threshold on the top two singular values of small image
windows. The Harris algorithm uses a spatial edge suppression technique to prevent detecting multiple edges
where only one exists.

1Ashutosh Saxena, Sung H. Chung, and Andrew Y. Ng, “Learning Depth from Single Monocular Images”
2P.
D.
Kovesi,
“MATLAB
and
Octave
Functions
for
Computer
Vision
and
http://www.csse.uwa.edu.au/˜pk/research/matlabfns/
3A Zisserman, “MATLAB Functions for Multiple View Geometry” http://www.robots.ox.ac.uk/ vgg/hzbook/code/
4 Jeﬀ Michels, Ashutosh Saxena, Andrew Y. Ng, “High Speed Obstacle Avoidance using Monocular Vision and Reinforcement
Learning”

Processing”

Image

1

Kovesi provides a correlation scripts for matching through monogenic phase or direct intensity correlation.
Each searches for high window correlations in a range around each feature. Only matches where each feature
selects the other are kept. Kovesi notes a typically better performace for matching through monogenic phase,
and a potential comparability of speed. I have used monogenic phase matching here and regular matching later,
to get the beneﬁts of both.

2.2 Fundamental Matrix

From the correlated matches we can generate the fundamental Matrix, F , for the two images. F encodes the
location in pixels of the pro jection of the two cameras on the opposite image plane as well as the rotation between
the two images. The equation deﬁning F is:

¯pT
r ∗ F ∗ ¯pl = 0

(1)

Where ¯pr , and ¯pl refer to the pixel coordinates of the matches in the right and left image respectively.
Here we use the Random Sample Consunsus algorithm (RANSAC) which iteratively computes an F based
on a random sample of matches and evaluates how many matches agree. The eight point algorithm is used
to construct F. To evaluate whether or not a match ﬁts the current estimate RANSAC simply evaluates
(1)
and applies a pre-determined threshold. Ransac stores the details of the best candidate found. The algorithm
terminates when it is 99% sure that it has chosen a random set of matches containing no outliers. The probability
of an outlier being picked is set from the fraction of matches were deemed inliers for the best candidate.
Since RANSAC classiﬁes correlations into inliers and outliers, we simply discard the outlying matches.
The RANSAC distance function ignores the direction of the match, and large angular deviations from the
epipolar line for matches with very similar pixel coordinates. While these checks could be included in the distance
function, they are used afterwards, to avoid high computation costs in the iterative RANSAC algorithm.
This implementation assumes no relative rotation, since most control algorithms using image streams will
have small relative angular rotations, and will be concerned primarily with nearby ob jects. This leads to a faster
and more consistent RANSAC convergence.

2.3 Additional Guided Matches

Now that we have determined F we have a lot more information about where matching features should be. Indeed
we can reduce it to a one dimensional line search. Further, since we have only translation we can determine
whether a matching feature should be closer or farther from the epipole. We use this information to perform
another dual correlation search. We perform several rounds of matching, removing succesfully matched points
at the end of each round, to obtain as many correlations as possible.

2.4 Depth Estimation

E =

With the camera calibration matrix, M , and F found above, one can obtain the Essential Matrix, E , which
encodes information on the relative rotation, and position of the two cameras. Not knowing how far or what
direction we have moved, results in a scale factor ambiguity in E . We obtain the normalized E using:
M F M
ptr((M F M )T (M F M ))/2
Given that we have assumed pure translational motion, E takes the form:

E = 
ˆTy
− ˆTz
0
− ˆTx
ˆTz
(3)
0
− ˆTy
ˆTx
0
1 i is
It is worth noting that h ˆT1
ˆT2
Where ˆT is the normalization of the unknown translation vector.
ˆT3
ˆT3
the pro jection of the epipole on the image plane. Using M and ˆT one can estimate the depth of points by
triangulating their position using:

(2)

ˆT1 − xr ˆT3
xl − xr
Here xr and xl refer to the 1st coordinates of the pro jected matches in the right and left camera frame respectively.
This equation follows the convention that third component of the pro jected matches is 1. They are obtained

Zl =

(4)

2

from pr = M ¯pr . Since we have assumed R = I3 the only ambiguity comes from the sign of ˆT , which just causes
a sign switch in Zl . In cases where xr − xl is too small we substitute yr and yl instead.

2.5 Error Estimation

To estimate the error of this stereo algorithm, a series of photographs and laser depth scans were taken around
campus with 2 foot separations. Thus the comparison is between the depths calculated from the pictures and
the laser range depths.
There are a number of sources of error in this matching:
1. The laser depths are taken slightly misaligned from the camera. This misalignment can very easily cause
massive errors due to the features being positioned often on the edge of large stepchanges in depth. An eﬀort
has been made to select pictures with many features in consistent depth regions.
2. The relatively fewer corners oﬀered by structured rather than unstructured scenes increases the correlation
accuracy, since there are simply fewer wrong choices to confuse the matching algorithm. In extreme cases, there
were not enough matches to estimate F , leading to no depth data at all.
3. Since the pictures were taken at diﬀerent times near sundown, about 5 minutes, some scenes have dif-
ferent lighting conditions, we have removed the scenes with very signiﬁcant lighting changes, but some lighting
diﬀerences remain. Also, some ob jects (people and bikes) have moved between pictures.
4. The laser depths and stereo depths have diﬀerent ranges. This leads to large errors when stereo depths
are well beyond the maximum laser depth.
5. Metal or windows can create spurious features from reﬂected light. They can also create laser depth
readings by not reﬂectingthe laser back.

3 Fusing with Monocular Cues

P (d|X ; θ ; σ) =

1
Z

(5)

i θr )2
(d(1) − xT
2σ2
1r

Saxena, et al’s “Learning Depth from Single Monocular Images” uses a Markov Random Field to model the
depth relations among image patches. They then maximize the distance probability over several parameters
given the dataset. While, they use both Laplacian and Gaussian distributions, we will only concern ourselves
with Gaussian distributions here. Further they use multiple scale leading to several layers of distance paramters.
Only base distances are left independent, as the lower resolution copies are restricted to be averages. We extend
their gaussian model by adding a gaussian penalty for distance from the stereo depths. The model is:
exp "− Xi∈SF M
#
MXi=1
(di (1) − dm − dSF M i )2
−
2σ2
SF M

exp −
3Xs=1
MXi=1 Xj∈N2 (i)
(di (s) − dj (s))2
2σ2
2rs
di (s) refers to distance i at scale s; SF M is the set of point provided by the stereo algorithm; dSF M i is the
distance provided by the stereo algorithm for point i;dm = Pi∈SF M
di
|SF M | ;σSF M is an empirically determined
quantity expressing the error in the measurements. All other parameters are as described in Saxena, et al’s work.
We choose to restrict
If we put this into the standard Gaussian form, exp(−(d − ˜µ)T eΣ−1 (d − ˜µ)), the maximum liklihood estimate
for d would be µ. Expanding by order of d we see:
1r + Xj∈N2 (i) Xs
eΣ−1 = Σ−1
SF M + Σ−1
Σ−1
2rs∀i ∈ SF M
(6)
Where ΣSF M , Σ1r , andΣ2rs are known empirically. µ can then be found by iteratively ﬁtting it to minimize the
gap between the two term of order 1 in d. This, combined with the current monocular techniques gives us a
gaussian models for each di , and thus the maximum liklihood estimate.

(7)

4 Results

Figure 1 shows an image in various stages of processing. For unstructured environments there are an enormous
number of corners leading to many correlations. These are then whittled down by estimating F , and converted
into depths, shown below. Stereo depth algorithms will never show the sky as it has no corners.

3

(a)

(b)

(c)

Figure 1: a) Base left image. b) Potential matches from correlation. c)Right camera image with inlying matches,
crosses indicate corners on right image.

10

20

30

40

50

60

70

80

90

100

110

(a)

(b)

50

100

150

200

250

Figure 2: a) Estimated depth from features. b) Laser range scan data

Figure 2 shows the estimated depth on the left and the true depth on the right.

(a)

(b)

(c)

Figure 3: a) Base left image. b) Potential matches from correlation. c)Right camera image with inlying matches,
crosses indicate corners on right image.

Figure 3 shows a structured scene. Here we see the eﬀect of glass, causing the algorithm to give the depth of
a reﬂection in the top right. Further, we see the error inherent in extremely regular textures.
For a selected set of structured images, for which a large proportion of the features exist in surfaces as
opposed to on the edge, this stereo algorithm has a true log depth to calculated log depth correlation of 0.8606,
and a standard deviation of 0.4472. For reference, a constant guess of the mean produces a standard deviation
of 0.8773. Due to the scale ambiguity in our range estimates, they were shifted to have the same mean.

5 Conclusion

Image manipulation, and 3-D reconstruction are not trivial. Indeed, although some code existed, I needed to
code up a substantial amount of the process. Particularily, there were numerous spurious matches that needed
to be pruned.
The stereo algorithm that resulted works, but would beneﬁt from some ﬁne tuning, and speed enhancements.
It is more accurate, but less informative for structured scenes.

4

Comparing the calculated range data to laser range scans requires signiﬁcant processing to ensure that the
limited ranges, and misalignments do not destroy the results. I could easily get near zeros correlations through
the right choice of scene.
Clearly more work needs to be done. The to do list includes using the camera calibration to undistort the
image, adaptive distance limits for the guided matches, automatically selecting features insensitive to laser scan-
ner/camera misalignment for error measures, algorithm speed improvements, and of course complete integration
with monocular features.

6 Aknowledgments

I would like to thank Prof. Ng for the loan of his book ”Introductory Techniques for 3-D computer Vision” by
Trucco & Verri. Also, Ashutosh Saxena was quite generous with his time. Further, I used a large number of
online resources, mainly the sample chapter from Andrew Zisserman’s ”Multiple View Geometry in Computer
Vision” entitled “Epipolar Geometry and the Fundamental Matrix”.

5


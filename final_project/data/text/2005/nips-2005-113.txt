Context as Filtering

Daichi Mochihashi
ATR, Spoken Language Communication
Research Laboratories
Hikaridai 2-2-2, Keihanna Science City
Kyoto, Japan
daichi.mochihashi@atr.jp

Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
Takayama 8916-5, Ikoma City
Nara, Japan
matsu@is.naist.jp

Abstract

Long-distance language modeling is important not only in speech recog-
nition and machine translation, but also in high-dimensional discrete se-
quence modeling in general. However, the problem of context length has
almost been neglected so far and a na¨ıv e bag-of-words history has been
employed in natural language processing. In contrast, in this paper we
view topic shifts within a text as a latent stochastic process to give an ex-
plicit probabilistic generative model that has partial exchangeability. We
propose an online inference algorithm using particle ﬁlters
to recognize
topic shifts to employ the most appropriate length of context automati-
cally. Experiments on the BNC corpus showed consistent improvement
over previous methods involving no chronological order.

1 Introduction

Contextual effect plays an essential role in the linguistic behavior of humans. We infer
the context in which we are involved to make an adaptive linguistic response by selecting
an appropriate model from that information. In natural language processing research, such
models are called long-distance language models that incorporate distant effects of previous
words over the short-term dependencies between a few words, which are called n-gram
models. Besides apparent application in speech recognition and machine translation, we
note that many problems of discrete data processing reduce to language modeling, such as
information retrieval [1], Web navigation [2], human-machine interaction or collaborative
ﬁltering and recommendation [3].

From the viewpoint of signal processing or control theory, context modeling is clearly a
ﬁltering problem that estimates the states of a system sequentially along time to predict the
outputs according to them. However, for the problem of long-distance language modeling,
natural language processing has so far only provided simple averaging using a set of whole
words from the beginning of a text, totally dropping chronological order and implicitly
assuming that the text comes from a stationary information source [4, 5].

The inherent difﬁculties
that have prevented ﬁltering approaches to language modeling are
its discreteness and high dimensionality, which precludes Kalman ﬁlters and their exten-
sions that are all designed for vector spaces and distributions like Gaussians. As we note in
the following, ordinary discrete HMMs are not powerful enough for this purpose because
their true state is restricted to a single hidden component [6].

In contrast, this paper proposes to solve the high-dimensional discrete ﬁltering problem di-
rectly using a Particle Filter. By combining a multinomial Particle Filter recently proposed
in statistics for DNA sequence modeling [7] with Bayesian text models LDA and DM, we
introduce two models that can track multinomial stochastic processes of natural language
or similar high-dimensional discrete data domains that we often encounter.

2 Mean Shift Model of Context

2.1 HMM for Multinomial Distributions

The long-distance language models mentioned in Section 1 assume a hidden multinomial
distribution, such as a unigram distribution or a mixture distribution over the latent topics,
to predict the next word by updating its estimate according to the observations. Therefore,
to track context shifts, we need a model that describes changes of multinomial distributions.

One model for this purpose is a multinomial extension to the Mean shift model (MSM)
recently proposed in the ﬁeld of statistics [7]. This is a kind of HMM, but note that it is
different from traditional discrete HMMs. In discrete HMMs, the true state is one of M
components and we estimate it stochastically as a multinomial over the M components.
On the other hand, since the true state here is itself a multinomial over the components, we
estimate it stochastically as (possibly a mixture of) a Dirichlet distribution, a distribution
of multinomial distributions on the (M (cid:0) 1)-simplex. This HMM has some similarity to
the Factorial HMM [6] in that it has a combinatorial representational power through a
distributed state representation. However, because the true state here is a multinomial over
the latent variables, there are dependencies between the states that are assumed independent
in the FHMM. Below, we brieﬂy introduce a multinomial Mean shift model following [7]
and an associated solution using a Particle Filter.

2.2 Multinomial Mean Shift Model

The MSM is a generative model that describes the intermittent changes of hidden states
and outputs according to them. Although there is a corresponding counterpart using Normal
distribution that was ﬁrst
introduced [8, 9], here we concentrate on a multinomial extension
of MSM, following [7] for DNA sequence modeling.
In a multinomial MSM, we assume time-dependent true multinomials (cid:18)t that may change
occasionally and the following generative model for the discrete outputs yt = y1 y2 : : : yt
(yt 2 (cid:6) ; (cid:6) is a set of symbols) according to (cid:18)1(cid:18)2 : : : (cid:18)t :
8<
(cid:18)t (cid:24) Dir((cid:11)) with probability (cid:26)
with probability (1(cid:0) (cid:26)) ;
= (cid:18)t(cid:0)1
:
yt (cid:24) Mult((cid:18)t )
where Dir((cid:11)) and Mult((cid:18)) are a Dirichlet and multinomial distribution with parameters
(cid:11) and (cid:18) , respectively. Here we assume that the hyperparameter (cid:11) is known and ﬁx ed, an
assumption we will relax in Section 3.

(1)

This model ﬁrst draws a multinomial (cid:18) from Dir((cid:11)) and samples output y according to (cid:18)
for a certain interval. When a change point occurs with probability (cid:26), a new (cid:18) is sampled
again from Dir((cid:11)) and subsequent y is sampled from the new (cid:18) . This process continues
recursively throughout which neither (cid:18)t nor the change points are known to us; all we know
is the output sequence yt .
However, if we know that the change has occurred at time c, y can be predicted exactly.
Let It be a binary variable that represents whether a change occurred at time t: that is,
It = 1 means there was a change at t ((cid:18)t 6= (cid:18)t(cid:0)1 ), and It = 0 means there was no change
((cid:18)t = (cid:18)t(cid:0)1 ). When the last change occurred at time c,

1. For particles i = 1 : : : N ,
(a) Calculate f (t) and g(t) according to (6).
(b) Sample I (i)
t (cid:24) Bernoulli (f (t)=(f (t) + g(t))), and update I
(c) Update weight w (i)
t = w(i)
t(cid:0)1 (cid:1) (f (t) + g(t)).
2. Find a predictive distribution using w (1)
: : : w(N )
t
t
p(yt+1 jyt ) = PN
i=1 w(i)
t p(yt+1 jyt ; I
(i)
t ) is given by (3).
Figure 1: Algorithm of the Multinomial Particle Filter.

where p(yt+1 jyt ; I

(1)
: : : I
t
(i)
t )

and I

(N )
t

:

(i)
t(cid:0)1 to I

(i)
t

.

(4)

p(yt+1 = y j yt ; Ic = 1; Ic+1 = (cid:1) (cid:1) (cid:1) = It = 0) = Z p(y j(cid:18))p((cid:18) jyc (cid:1) (cid:1) (cid:1) yt )d(cid:18)
(cid:11)y + ny
;
=
Py ((cid:11)y +ny )
where (cid:11)y is the y ’th element of (cid:11) and ny is the number of occurrences of y in yc (cid:1) (cid:1) (cid:1) yt .
Therefore, the essence of this problem lies in how to detect a change point given the data
up to time t, a change point problem in discrete space. Actually, this problem can be solved
by an efﬁc ient Particle Filter algorithm [10] shown below.

(2)

(3)

2.3 Multinomial Particle Filter

The prediction problem above can be
solved by the efﬁcie nt Particle Filter al-
gorithm shown in Figure 1, graphically
displayed in Figure 2 (excluding prior
updates). The main intricacy involved is
as follows. Let us denote It = fI1 : : : It g.
By Bayes’ theorem,

d1

z

}|
d2
(cid:1) (cid:1) (cid:1)

dc

{ Prior

Weight

yt+1

:

Particle #1
Particle #2
Particle #3
Particle #4
:
Particle #N

t(cid:0) 1
t
Figure 2: Multinomial Particle Filter in work.

(5)

(6)

leading

p(It jIt(cid:0)1 ; yt ) / p(It ; yt jIt(cid:0)1 ; yt(cid:0)1 ) = p(yt jyt(cid:0)1 ; It(cid:0)1 ; It )p(It jIt(cid:0)1 )
= (cid:26) p(yt jyt(cid:0)1 ; It(cid:0)1 ; It = 1)p(It = 1jIt(cid:0)1 ) =: f (t)
p(yt jyt(cid:0)1 ; It(cid:0)1 ; It = 0)p(It = 0jIt(cid:0)1 ) =: g(t)
(cid:26) p(It = 1jIt(cid:0)1 ; yt ) = f (t)=(f (t) + g(t))
p(It = 0jIt(cid:0)1 ; yt ) = g(t)=(f (t) + g(t)) :
In Expression (5), the ﬁrst
term is a likelihood of observation yt when It has been ﬁx ed,
which can be obtained through (3). The second term is a prior probability of change, which
can be set tentatively by a constant (cid:26). However, when we endow (cid:26) with a prior Beta
distribution Be((cid:11); (cid:12) ), posterior estimate of (cid:26)t given the binary change point history It(cid:0)1
can be obtained using the number of 1’s in It(cid:0)1 , nt(cid:0)1 (1), following a standard Bayesian
method:
(cid:11) + nt(cid:0)1 (1)
(cid:11) + (cid:12) + t (cid:0) 1
This means that we can estimate a “rate of topic shifts” as time proceeds in a Bayesian
fashion. Throughout the following experiments, we used this online estimate of (cid:26) t .
The above algorithm runs for each observation yt (t = 1 : : : T ). If we observe a “strange ”
word that is more predictable from the prior than the contextual distribution, (6) makes
f (t) larger than g(t), which leads to a higher probability that It = 1 will be sampled in the
Bernoulli trial of Algorithm 1(b).

E [(cid:26)t jIt(cid:0)1 ] =

(8)

(7)

:

3 Mean Shift Model of Natural Language

Chen and Lai [7] recently proposed the above algorithm to analyze DNA sequences. How-
ever, when extending this approach to natural language, i.e. word sequences, we meet two
serious problems.

The ﬁrst problem is that in a natural language the number of words is extremely large.
As opposed to DNA, which has only four letters of A/T/G/C, a natural language usually
contains a minimum of some tens of thousands of words and there are strong correlations
between them. For example, if “nurse ”
follows “ho spital ”we believe that there has been
no context shift; however, if “un iversity”
follows “hospital, ”
the context probably has been
shifted to a “me dical school” subtopic, even though the two words are equally distinct from
“hosp ital.” Of course, this is due to the semantic relationship we can assume between these
words. However, the original multinomial MSM cannot capture this relationship because
it treats the words independently. To incorporate this relationship, we require an extensive
prior knowledge of words as a probabilistic model.

The second problem is that in model equation (1), the hyperparameter (cid:11) of prior Dirichlet
distribution of the latent multinomials is assumed to be known.
In the case of natural
language, this means we know beforehand what words or topics will be spoken for all the
texts. Apparently, this is not a natural assumption: we need an online estimation of (cid:11) as
well when we want to extend MSM to natural languages.

To solve these problems, we extended a multinomial MSM using two probabilistic text
models, LDA and DM. Below we introduce MSM-LDA and MSM-DM, in this order.

3.1 MSM-LDA

Latent Dirichlet Allocation (LDA) [3] is a probabilistic text model that assumes a hidden
multinomial topic distribution (cid:18) over the M topics on a document d to estimate it stochasti-
cally as a Dirichlet distribution p((cid:18) jd). Context modeling using LDA [5] regards a history
h = w1 : : : wh as a pseudo document and estimates a variational approximation q((cid:18) jh) of
a topic distribution p((cid:18) jh) through a variational Bayes EM algorithm on a document [3].
After obtaining topic distribution q((cid:18) jh), we can predict the next word as follows.
p(y jh) = Z p(y j(cid:18))q((cid:18) jh)d(cid:18) = PM
i=1 p(y j(cid:18)i )h(cid:18)i iq((cid:18) jh)
When we use this prediction with an associated VB-EM algorithm in place of the na¨ıv e
Dirichlet model (3) of MSM, we get an MSM-LDA that tracks a latent topic distribution (cid:18)
instead of a word distribution. Since each particle computes a Dirichlet posterior of topic
distribution, the ﬁnal
topic distribution of MSM-LDA is a mixture of Dirichlet distributions
for predicting the next word through (4) and (9) as shown in Figure 3(a). Note that MSM-
LDA has an implicit generative model corresponding to (1) in topic space. However, here
we use a conditional model where LDA parameters are already known in order to estimate
the context online.

(9)

In MSM-LDA, we can also update the hyperparameter (cid:11) sequentially from the history. As
seen in Figure 2, each particle has a history that has been segmented into pseudo “doc u-
ments” d1 : : : dc by the change points sampled so far. Since each pseudo “doc ument” has
a Dirichlet posterior q((cid:18) jdi ) (i = 1 : : : c), a common Dirichlet prior can be inferred by
a linear-time Newton-Raphson algorithm [3]. Note that this computation needs only be
run when a change point has been sampled. For this purpose, only the sufﬁcien t statistics
q((cid:18) jdi ) must be stored for each particle to render itself an online algorithm.
Note in passing that MSM-LDA is a model that only tracks a mixing distribution of a mix-
ture model. Therefore, in principle this model is also applicable to other mixture models,
e.g. Gaussian mixtures, where mixing distribution is not static but evolves according to (1).

Time
1

Particle#1 Particle #2

Particle #N

Time
1

Particle#1 Particle #2

Particle #N

Context

t

wn

change
points

prior

Context

Dirichlet
distribution

t

wn

Word
Simplex

change
points

prior

Mixture of
Dirichlet
distributions

Dirichlet
distribution
Unigram
distributions

w2

0.2

0.64

0.02

Weights

w1

w2

0.2

0.64

0.02

Weights

Word
Simplex
Topic
Subsimplex
w1

Expectation
of Topic Mixture

Unigram

next word

Mixture of
Dirichlet distributions

Expectation
of Unigram

next word

Mixture of
Mixture of
Dirichlet distributions

(a) MSM-LDA
(b) MSM-DM
Figure 3: MSM-LDA and MSM-DM in work.

However, in terms of multinomial estimation, this generality has a drawback because it
uses a lower-dimensional topic representation to predict the next word, which may cause
a loss of information. In contrast, MSM-DM is a model that works directly on the word
space to predict the next word with no loss of information.

3.2 MSM-DM

Dirichlet Mixtures (DM) [11] is a novel Bayesian text model that has the lowest perplexity
reported so far in context modeling. DM uses no intermediate “topic”
variables, but places
a mixture of Dirichlet distributions directly on the word simplex to model word correlations.
Speciﬁcally , DM assumes the following generative model for a document w = w1 : : : wN :1
(cid:11)

1. Draw m (cid:24) Mult((cid:21)).
2. Draw p (cid:24) Dir((cid:11)m ).
3. For n = 1 : : : N ,
a. Draw wn (cid:24) Mult(p).

m

w

N

D

m

p

wN

D

(a) Unigram Mixture (UM)
(b) Dirichlet Mixtures (DM)
Figure 4: Graphical models of UM and DM.
1 are pa-
where p is a V -dimensional unigram distribution over words, (cid:11)1 : : : (cid:11)M = (cid:11)M
rameters of Dirichlet prior distributions of p, and (cid:21) is a M -dimensional prior mixing
distribution of them. This model is considered a Bayesian extension of the Unigram
Mixture [12] and has a graphical model shown in Figure 4. Given a set of documents
1 can be iteratively estimated by a com-
D = fw1 ; w2 ; : : : ; wD g, parameters (cid:21) and (cid:11)M
bination of EM algorithm and the modi ﬁe d Newton-Raphson method shown in Figure 5,
which is a straight extension to the estimation of a Polya mixture [13]. 2
Under DM, a predictive probability p(y jh) is (omitting dependencies on (cid:21) and (cid:11)M
1 ):
(cid:18)Z p(y jp)p(pj(cid:11)m ;h)dp(cid:19) (cid:1) p(mjh)
M
M
Xm=1
Xm=1
M
(cid:11)my +ny
Xm=1
;
Py ((cid:11)my +ny )
1Step 1 of the generative model in fact can be replaced by a Dirichlet process prior. Full Bayesian
treatment of DM through Dirichlet processes is now under our development.
2DM is an extension to the model for amino acids [14] to natural language with a huge number of
parameters, which precludes the ordinary Newton-Raphson algorithm originally proposed in [14].

p(y jm; h)p(mjh) =

p(y jh) =

=

Cm

(10)

M step:

V
Yv=1

(cid:0)(Pv (cid:11)mv )
E step: p(mjwi ) / (cid:21)m
(cid:0)(Pv (cid:11)mv +Pv niv )
(cid:21)m / PD
i=1 p(mjwi ) ;
Pi p(mjwi ) niv =((cid:11)mv + niv (cid:0) 1)
(cid:11)0
mv = (cid:11)mv (cid:1)
Pi p(mjwi ) Pv niv =(Pv (cid:11)mv + Pv niv (cid:0) 1)
Figure 5: EM-Newton algorithm of Dirichlet Mixtures.

(cid:0)((cid:11)mv +niv )
(cid:0)((cid:11)mv )

(13)

(14)

(15)

where

(11)

Cm / (cid:21)m

(cid:0)(Pv (cid:11)mv )
V
(cid:0)((cid:11)mv +nv )
Yv=1
(cid:0)(Pv (cid:11)mv +h)
(cid:0)((cid:11)mv )
and nv is the number of occurrences of v in h. This prediction can also be considered an
extension to Dirichlet smoothing [15] with multiple hyperparameters (cid:11)m to weigh them
accordingly by Cm .3
When we replace a na¨ıv e Dirichlet model (3) by a DM prediction (10), we get a ﬂe xible
MSM-DM dynamic model that works on word simplex directly. Since the original multi-
nomial MSM places a Dirichlet prior in the model (1), MSM-DM is considered a natural
extension to MSM by placing a mixture of Dirichlet priors rather than a single Dirichlet
prior for multinomial unigram distribution. Because each particle calculates a mixture of
Dirichlet posteriors for the current context, the ﬁnal MSM-DM estimate is a mixture of
them, again a mixture of Dirichlet distributions as shown in Figure 3(b).

In this case, we can also update the mixture prior (cid:21) sequentially. Because each particle has
“pseu do documents” w1 : : : wc segmented by change points individually, posterior (cid:21)m can
be obtained similarly as (14),
(cid:21)m / Pc
i=1 p(mjwi )
where p(mjwi ) is obtained from (13). Also in this case, only the sufﬁcient
p(mjwi ) (i = 1 :: c) must be stored to make MSM-DM a ﬁltering algorithm.

(12)
statistics

4 Experiments

We conducted experiments using a standard British National Corpus (BNC). We randomly
selected 100 ﬁles of BNC written texts as an evaluation set, and the remaining 2,943 ﬁles
as a training set for parameter estimation of LDA and DM in advance.

4.1 Training and evaluation data

Since LDA and DM did not converge on the long texts like BNC, we divided training texts
into pseudo documents with a minimum of ten sentences for parameter estimation. Due
to the huge size of BNC, we randomly selected a maximum of 20 pseudo documents from
each of the 2,943 ﬁles
to produce a ﬁnal corpus of 56,939 pseudo documents comprising
11,032,233 words. We used a lexicon of 52,846 words with a frequency (cid:21) 5. Note that
this segmentation is optional and has an only indirect in ﬂue nce on the experiments. It only
affects the clustering of LDA and DM: in fact, we could use another corpus, e.g. newspaper
corpus, to estimate the parameters without any preprocessing.

Since the proposed method is an algorithm that simultaneously captures topic shifts and
their rate in a text to predict the next word, we need evaluation texts that have different
rates of topic shifts. For this purpose, we prepared four different text sets by sampling

3Therefore, MSM-DM is considered an ingenious dynamic Dirichlet smoothing as well as a con-
text modeling.

MSM-DM
Text
Raw 870.06 ((cid:0)6:02%)
Slow 893.06 ((cid:0)8:31%)
898.34 ((cid:0)9:10%)
Fast
VFast 960.26 ((cid:0)7:57%)

DM
925.83
974.04
988.26
1038.89

MSM-LDA
1028.04
1047.08
1044.56
1065.15

LDA
1037.42
1060.56
1061.01
1050.83

Table 2: Contextual Unigram Perplexities for Evaluation Texts.

from the long BNC texts. Speciﬁcally , we conducted sentence-based random sampling as
follows.
(1) Select a ﬁrst
sentence randomly for each text.
(2) Sample contiguous X sentences from that sentence.
(3) Skip Y sentences.
(4) Continue steps (2) and (3) until a desired length of text is obtained.
In the procedure above, X and Y are random variables that have uniform distributions
given in Table 1. We sampled 100 sentences from each of the 100 ﬁles by this procedure to
create the four evaluation text sets listed in the table.

4.2 Parameter settings

Name
Raw
Slow
Fast
VeryFast

Property
X = 100; Y = 0
1 (cid:20) X (cid:20) 10; 1 (cid:20) Y (cid:20) 3
1 (cid:20) X (cid:20) 10; 1 (cid:20) Y (cid:20) 10
X = 1; 1 (cid:20) Y (cid:20) 10

The number of latent classes in LDA
and DM are set to 200 and 50, respec-
tively.4 The number of particles is set
to N = 20, a relatively small number
because each particle executes an ex-
act Bayesian prediction once previous
change points have been sampled. Beta prior distribution of context change can be initial-
ized as a uniform distribution, ((cid:11); (cid:12) ) = (1; 1). However, based on a preliminary experiment
we set it to ((cid:11); (cid:12) ) = (1; 50): this means we initially assume a context change rate of once
every 50 words in average, which will be updated adaptively.

Table 1: Types of Evaluation Texts.

4.3 Experimental results

Table 2 shows the unigram perplexity of contextual prediction for each type of evaluation
set. Perplexity is a reciprocal of the geometric average of contextual predictions, thus
better predictions yield lower perplexity. While MSM-LDA slightly improves LDA due
to the topic space compression explained in Section 3.1, MSM-DM yields a consistently
better prediction, and its performance is more signi ﬁca nt for texts whose subtopics change
faster.
Figure 6 shows a plot of the actual improvements relative to DM, PPLMSM (cid:0) PPLDM . We
can see that prediction improves for most documents by automatically selecting appropriate
contexts. The maximum improvement was –36 5 in PPL for one of the evaluation texts.
Finally, we show in Figure 7 a sequential plot of context change probabilities p(i) (It = 1)
(i = 1::N ; t = 1::T ) calculated by each particle for the ﬁrst 1,000 words of one of the
evaluation texts.

5 Conclusion and Future Work

In this paper, we extended the multinomial Particle Filter of a small number of symbols to
natural language with an extremely large number of symbols. By combining original ﬁlter
with Bayesian text models LDA and DM, we get two models, MSM-LDA and MSM-DM,
that can incorporate semantic relationship between words and can update their hyperparam-

4We deliberately chose a smaller number of mixtures in DM because it is reported to have a better
performance in small mixtures since it is essentially a unitopic model, in contrast to LDA.

40

30

20

10

s
t
n
e
m
u
c
o
D

0
-400 -300 -200 -100
100 200 300
0
Perplexity reduction

Figure 6: Perplexity reductions of MSM
relative to DM.

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
1000

20

15
Particle

10

5

900

800

700

600

400

300

500
Time
Figure 7: Context change probabilities for
1,000 words text, sampled by the particles.

100

200

0

0

eter sequentially. According to this model, prediction is made using a mixture of different
context lengths sampled by each Monte Carlo particle.

Although the proposed method is still in its fundamental stage, we are planning to extend
it to larger units of change points beyond words, and to use a forward-backward MCMC or
Expectation Propagation to model a semantic structure of text more precisely.

References
[1] Jay M. Ponte and W. Bruce Croft. A Language Modeling Approach to Information Retrieval.
In Proc. of SIGIR ’98, pages 275–281, 1998.
[2] David Cohn and Thomas Hofmann. The Missing Link: a probabilistic model of document
content and hypertext connectivity. In NIPS 2001, 2001.
[3] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3:993 –1022, 2003.
[4] Daniel Gildea and Thomas Hofmann. Topic-based Language Models Using EM. In Proc. of
EUROSPEECH ’99, pages 2167–2170, 1999.
[5] Takuya Mishina and Mikio Yamamoto. Context adaptation using variational Bayesian learning
for ngram models based on probabilistic LSA. IEICE Trans. on Inf. and Sys., J87-D-II(7):1409–
1417, 2004.
[6] Zoubin Ghahramani and Michael I. Jordan. Factorial Hidden Markov Models. In Advances in
Neural Information Processing Systems (NIPS), volume 8, pages 472–478. MIT Press, 1995.
[7] Yuguo Chen and Tze Leung Lai. Sequential Monte Carlo Methods for Filtering and Smooth-
ing in Hidden Markov Models. Discussion Paper 03-19, Institute of Statistics and Decision
Sciences, Duke University, 2003.
[8] H. Chernoff and S. Zacks. Estimating the Current Mean of a Normal Distribution Which is
Subject to Changes in Time. Annals of Mathematical Statistics, 35:999 –1018, 1964.
[9] Yi-Chin Yao. Estimation of a noisy discrete-time step function: Bayes and empirical Bayes
approaches. Annals of Statistics, 12:1434–1 447, 1984.
[10] Arnaud Doucet, Nando de Freitas, and Neil Gordon. Sequential Monte Carlo Methods in Prac-
tice. Statistics for Engineering and Information Science. Springer-Verlag, 2001.
[11] Mikio Yamamoto and Kugatsu Sadamitsu. Dirichlet Mixtures in Text Modeling. CS Technical
Report CS-TR-05-1, University of Tsukuba, 2005. http://www.mibel.cs.tsukuba.ac.jp/˜mya ma/
pdf/dm.pdf.
[12] Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. Text Classiﬁca-
tion from Labeled and Unlabeled Documents using EM. Machine Learning, 39(2/3):103 –134,
2000.
[13] Thomas P. Minka. Estimating a Dirichlet distribution, 2000. http://research.microsoft.com/
˜minka/pape rs/dirichlet/.
[14] K. Sj ¨olander, K. Karplus, M.P. Brown, R. Hughey, R. Krogh, I.S. Mian, and D. Haussler. Dirich-
let Mixtures: A Method for Improved Detection of Weak but Signiﬁcant Protein Sequence Ho-
mology. Computing Applications in the Biosciences, 12(4):327 –245, 1996.
[15] D. J. C. MacKay and L. Peto. A Hierarchical Dirichlet Language Model. Natural Language
Engineering, 1(3):1–19, 1994.


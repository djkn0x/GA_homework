A Probabilistic Approach for Optimizing
Spectral Clustering

Rong Jin∗ , Chris Ding† , Feng Kang∗
∗Lawrence Berkeley National Laboratory, Berkeley, CA 94720
†Michigan State University, East Lansing , MI 48824

Abstract

Spectral clustering enjoys its success in both data clustering and semi-
supervised learning. But, most spectral clustering algorithms cannot
handle multi-class clustering problems directly. Additional strategies are
needed to extend spectral clustering algorithms to multi-class cluster-
ing problems. Furthermore, most spectral clustering algorithms employ
hard cluster membership, which is likely to be trapped by the local op-
timum.
In this paper, we present a new spectral clustering algorithm,
named “Soft Cut ”. It improves the normalized cut algorithm b y intro-
ducing soft membership, and can be efﬁciently computed usin g a bound
optimization algorithm. Our experiments with a variety of datasets have
shown the promising performance of the proposed clustering algorithm.

1

Introduction

Data clustering has been an active research area with a long history. Well-known cluster-
ing methods include the K-means methods (Hartigan & Wong., 1994), Gaussian Mixture
Model (Redner & Walker, 1984), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann,
1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Recently, spectral cluster-
ing methods (Shi & Malik, 2000; Ng et al., 2001; Zha et al., 2002; Ding et al., 2001; Bach
& Jordan, 2004)have attracted more and more attention given their promising performance
in data clustering and simplicity in implementation. They treat the data clustering problem
as a graph partitioning problem. In its simplest form, a minimum cut algorithm is used to
minimize the weights (or similarities) assigned to the removed edges. To avoid unbalanced
clustering results, different objectives have been proposed, including the ratio cut (Hagen
& Kahng, 1991), normalized cut (Shi & Malik, 2000) and min-max cut (Ding et al., 2001).

To reduce the computational complexity, most spectral clustering algorithms use the relax-
ation approach, which maps discrete cluster memberships into continuous real numbers.
As a result, it is difﬁcult to directly apply current spectra l clustering algorithms to multi-
class clustering problems. Various strategies (Shi & Malik, 2000; Ng et al., 2001; Yu &
Shi, 2003) have been used to extend spectral clustering algorithms to multi-class clustering
problems. One common approach is to ﬁrst construct a low-dim ension space for data repre-
sentation using the smallest eigenvectors of a graph Laplacian that is constructed based on
the pair wise similarity of data. Then, a standard clustering algorithm, such as the K-means
method, is applied to cluster data points in the low-dimension space.

One problem with the above approach is how to determine the appropriate number of eigen-
vectors. A too small number of eigenvectors will lead to an insufﬁcient representation of
data, and meanwhile a too large number of eigenvectors will bring in a signi ﬁcant amount
of noise to the data representation. Both cases will degrade the quality of clustering. Al-
though it has been shown in (Ng et al., 2001) that the number of required eigenvectors is
generally equal to the number of clusters, the analysis is valid only when data points of
different clusters are well separated. As will be shown later, when data points are not well
separated, the optimal number of eigenvectors can be different from the number of clusters.

Another problem with the existing spectral clustering algorithms is that they are based on
binary cluster membership and therefore are unable to express the uncertainty in data clus-
tering. Compared to hard cluster membership, probabilistic membership is advantageous
in that it is less likely to be trapped by local minimums. One example is the Bayesian clus-
tering method (Redner & Walker, 1984), which is usually more robust than the K-means
method because of its soft cluster memberships. It is also advantageous to use probabilistic
memberships when the cluster memberships are the intermediate results and will be used
for other processes, for example selective sampling in active learning (Jin & Si, 2004).

In this paper, we present a new spectral clustering algorithm, named “Soft Cut ”, that ex-
plicitly addresses the above two problems.
It extends the normalized cut algorithm by
introducing probabilistic membership of data points. By encoding membership of multi-
ple clusters into a set of probabilities, the proposed clustering algorithm can be applied
directly to multi-class clustering problems. Our empirical studies with a variety of datasets
have shown that the soft cut algorithm can substantially outperform the normalized cut
algorithm for multi-class clustering.

The rest paper is arranged as follows. Section 2 presents the related work. Section 3
describes the soft cut algorithm. Section 4 discusses the experimental results. Section 5
concludes this study with the future work.

2 Related Work

The key idea of spectral clustering is to convert a clustering problem into a graph partition-
ing problem.
Let n be the number of data points to be clustered. Let W = [wi,j ]n×n be the weight
matrix where each wi,j is the similarity between two data points. For the convenience of
discussion, wi,i = 0 for all data points. Then, a clustering problem can be formulated into
the minimum cut problem, i.e.,

wi,j (qi − qj )2 = qT Lq

(1)

q∗ = arg min
q∈{−1,1}n

n
Xi,j=1
where q = (q1 , q2 , ..., qn ) is a vector for binary memberships and each qi can be either −1
or 1. L is the Laplacian matrix. It is deﬁned as L = D − W, where D = [di,i ]n×n is
a diagonal matrix with each element di,i = δi,j Pn
j=1 wi,j . Directly solving the problem
in (1) requires combinatorial optimization, which is computationally expensive. Usually, a
relaxation approach (Chung, 1997) is used to replace the vector q ∈ {−1, 1}n with a vector
ˆq ∈ Rn under the constraint Pn
i = n. As a result of the relaxation, the approximate
i=1 ˆq2
solution to (1) is the second smallest eigenvector of Laplacian L.
One problem with the minimum cut approach is that it does not take into account the
size of clusters, which can lead to clusters of unbalanced sizes. To resolve this problem,
several different criteria are proposed, including the ratio cut (Hagen & Kahng, 1991),
normalized cut (Shi & Malik, 2000) and min-max cut (Ding et al., 2001). For example, in

the normalized cut algorithm, the following objective is used:

C+,− (q)
C+,− (q)
Jn (q) =
+
D− (q)
D+ (q)
where C+,− (q) = Pn
i=1 δ(qi , ±) Pn
i,j=1 wi,j δ(qi , +)δ(qj , −) and D± = Pn
j=1 wi,j . In
the above objective, the size of clusters, i.e., D± , is used as the denominators to avoid
clusters of too small size. Similar to the minimum cut approach, a relaxation approach is
used to convert the problem in (2) into a eigenvector problem. For multi-class clustering,
we can extend the objective in (2) into the following form:

(2)

(3)

Jnorm mc (q) =

Cz ,z ′ (q)
Dz (q)

K
Xz=1 Xz ′ 6=z
the number of clusters, vector q ∈ {1, 2, ..., K }n , Cz ,z ′ =
where K is
i=1 Pn
Pn
i,j=1 δ(qi , z )δ(qj , z ′ )wi,j , and Dz = Pn
j=1 δ(qi , z )wi,j . However, efﬁciently
ﬁnding the solution that minimizes (3) is rather difﬁcult.
I
n particular, a simple relax-
ation method cannot be applied directly here.
In the past, several heuristic approaches
(Shi & Malik, 2000; Ng et al., 2001; Yu & Shi, 2003) have been proposed for ﬁnding
approximate solutions to (3). One common strategy is to ﬁrst obtain the K smallest (ex-
cluding the one with zero eigenvalue) eigenvectors of Laplacian L, and project data points
onto the low-dimension space that is spanned by the K eigenvectors. Then, a standard
clustering algorithm, such as the K-means method, is applied to cluster data points in this
low-dimension space. In contrast to these approaches, the proposed spectral clustering al-
gorithm deals with the multi-class clustering problem directly. It estimates the probabilities
for each data point be in different clusters simultaneously. Through the probabilistic cluster
memberships, the proposed algorithm will be less likely to be trapped by local minimums,
and therefore will be more robust than the existing spectral clustering algorithms.

3 Spectral Clustering with Soft Membership

In this section, we describe a new spectral clustering algorithm, named “ Soft Cut ”, which
extends the normalized cut algorithm by introducing probabilistic cluster membership. In
the following, we will present a formal description of the soft cut algorithm, followed by
the procedure that efﬁciently optimizes the related optimi zation problem.

3.1 Algorithm Description
First, notice that Dz in (3) can be expanded as Dz = PK
j=1 Ci,j . Thus, the objective
function for multi-class clustering in (3) can be rewritten as:
K
K
K
Cz ,z ′ (q)
Xz=1
Xz ′ 6=z
Xz=1
= K −
Jn mc (q) =
Dz (q)
Cz,z (q)
n mc = PK
Let J ′
n mc .
Dz (q) . Thus, instead of minimizing Jn mc , we can maximize J ′
z=1
To extend the above objective function to a probabilistic framework, we introduce the prob-
abilistic cluster membership. Let qz ,i denote the probability for the i-th data point to be in
the z -th cluster. Let matrix Q = [qz ,i ]K×n include all probabilities qz ,i . Using the proba-
bilistic notations, we can rewrite Cz ,z ′ and Dz as follows:
n
Xi,j=1

qz ,i qz ′ ,j wi,j , Dz (Q) =

Cz ,z (q)
Dz (q)

n
Xi,j=1

Cz ,z ′ (Q) =

(4)

qz ,iwi,j

(5)

n mc , we have the follow-
Substituting the probabilistic expression for Cz ,z ′ and Dz into J ′
ing optimization problem for probabilistic spectral clustering:
Xz=1 Pn
K
i,j=1 qz ,i qz ,j wi,j
Pn
i,j=1 qz ,iwi,j
qz ,i = 1

s.t.∀i ∈ [1..n], z ∈ [1..K ] : qz ,i ≥ 0,

Jprob (Q) = arg max
Q∈RK×n

Q∗ = arg min
Q∈RK×n

(6)

K
Xz=1

3.2 Optimization Procedure

In this subsection, we present a bound optimization algorithm (Salakhutdinov & Roweis,
2003) for efﬁciently ﬁnding the solution to (6). It maximize
s the objective function in (6)
iteratively. In each iteration, a concave lower bound is ﬁrs t constructed for the objective
function based on the solution obtained from the previous iteration. Then, a new solution
for the current iteration is obtained by maximizing the lower bound. The same procedure
is repeated until the solution converges to a local maximum.
i,j ]K×n be the probabilities obtained in the previous iteration, and Q =
Let Q′ = [q ′
[qi,j ]K×n be the probabilities for current iteration. Deﬁne

∆(Q, Q′ ) = log

Jprob (Q)
Jprob (Q′ )

which is the logarithm of the ratio of the objective functions between two consecutive
iterations. Using the convexity of logarithm function, i.e., log(Pi pi qi ) ≥ Pi pi log(qi )
for a pdf {pi }, we have ∆(Q, Q′ ) lower bound by the following expression:
∆(Q, Q′ ) = log   K
Dz (Q) ! − log   K
Dz (Q′ ) !
Cz ,z (Q′ )
Cz ,z (Q)
Xz=1
Xz=1
K
tz (cid:18)log
Dz (Q′ ) (cid:19)
Cz ,z (Q)
Dz (Q)
Xz=1
≥
− log
Cz ,z (Q′ )

(7)

where tz is deﬁned as:

tz =

Cz,z (Q′ )
Dz (Q′ )
Cz′ ,z′ (Q′ )
Dz′ (Q′ )

PK
z ′=1
Now, the ﬁrst term within the big bracket in (7), i.e.,
as:

log

Cz ,z (Q)
Cz ,z (Q′ )

where si,j
z

is deﬁned as:

= log 
n
Xi,j=1


n
n
Xi=1
Xj=1


≥ 2

si,j
z

=

q ′
z ,i q ′
z ,j wi,j
Cz ,z (Q′ )
z 
si,j
 log(qz ,i ) −
q ′
z ,i q ′
z ,j wi,j
Cz ,z (Q′ )

log Cz,z (Q)
Cz,z (Q′ ) , can be further relaxed
z ,j 
qz ,i qz ,j

z ,i q ′
q ′
n
Xi,j=1

z ,i q ′
z log(q ′
si,j
z ,j )

(9)

(8)

(10)

Meanwhile, using the inequality log x ≤ x − 1, we have log Dz (Q)
Dz (Q′ ) upper bounded by the
following expression:

log

≤

qz ,i

Dz (Q)
Dz (Q′ )

n
n
Xj=1
Xi=1
Putting together (7), (9), and (11), we have a concave lower bound for the objective function
in (6), i.e.,

Dz (Q)
Dz (Q′ )

wi,j
Dz (Q′ )

− 1 =

(11)

− 1

log Jprob (Q) ≥

log Jprob (Q′ ) + ∆0 (Q′ ) + 2

where ∆0 (Q′ ) is deﬁned as:

K
Xz=1

n
Xi,j=1

tz si,j
z log qz ,i −

K
Xz=1

n
Xi,j=1

qz ,iwi,j
Dz (Q′ )

(12)

tz

∆0 (Q′ ) = −

z wi,j log(q ′
z ,i q ′
si,j
z ,j ) + 1

K
n
Xz=1
Xi,j=1
The optimal solution that maximizes the lower bound in (12) can be computed by setting
its derivative to zero, which leads to the following solution:
2tz Pn
j=1 si,j
z
qz ,i =
wi,j
tz Pn
Dz (Q′ ) + λi
j=1
where λi is a Lagrangian multiplier that ensure PK
z=1 qz ,i = 1. It can be acquired by
maximizing the following objective function:

z 
 log 
+ λi
K
n
n
Xz=1
Xj=1
Xj=1
si,j
l(λi ) = −λi + 2
tz
tz

Since the above objective function is concave, we can apply a standard numerical proce-
dure, such as the Newton’s method, to efﬁciently ﬁnd the valu
e for λi .

wi,j
Dz (Q′ )

(14)

(13)

4 Experiment

In this section, we focus on examining the effectiveness of the proposed soft cut algorithm
for multi-class clustering. In particular, we will address the following two research ques-
tions:

1. How effective is the proposed algorithm for data clustering? We compare the
proposed soft cut algorithm to the normalized cut algorithm with various numbers
of eigenvectors.
2. How robust is the proposed algorithm for data clustering? We evaluate the robust-
ness of clustering algorithms by examining their variance across multiple trials.

4.1 Experiment Design

Datasets In order to extensively examine the effectiveness of the proposed soft cut algo-
rithm, a variety of datasets are used in this experiment. They are:

• Text documents that are extracted from the 20 newsgroups to form two ﬁve-class
datasets, named as “M5” and “L5”. Each class contain 100 docu
ment and there
are totally 500 documents.

Table 1: Datasets Description
#Class
Description
Dataset
5
Text documents
M5
L5
5
Text documents
Pen-based handwritting
10
Pendigit
Ribosome Ribosome rDNA sequences
8

#Instance
500
500
2000
1907

#Features
1000
1000
16
27617

• Pendigit that comes from the UCI data repository. It contains 2000 examples that
belong to 10 different classes.
are
project
RDP
from
that
• Ribosomal
sequences
(http://rdp.cme.msu.edu/index.jsp).
It contains annotated rRNA sequences
of ribosome for 2000 different bacteria that belong to 10 different phylum (e.g.,
classes). Table 1 provides the detailed information regarding each dataset.

Evaluation metrics To evaluate the performance of different clustering algorithms, two
different metrics are used:

• Clustering accuracy. For the datasets that have no more than ﬁve classes, clus-
tering accuracy is used as the evaluation metric. To compute clustering accuracy,
each automatically generated cluster is ﬁrst aligned with a true class. The clas-
si ﬁcation accuracy based on the alignment is then computed,
and the clustering
accuracy is deﬁned as the maximum classi ﬁcation accuracy am ong all possible
alignments.
• Normalized mutual information. For the datasets that have more than ﬁve classes,
due to the expensive computation involved in ﬁnding the opti mal alignment, we
use the normalized mutual information (Banerjee et al., 2003) as the alternative
evaluation metric.
If Tu and Tl denote the cluster labels and true class labels
assigned to data points, the normalized mutual information “nmi ” is deﬁned as

nmi =

2I (Tu , Tl )
(H (Tu ) + H (Tl ))
where I (Tu , Tl ) stands for the mutual information between clustering labels Tu
and true class labels Tl . H (Tu ) and H (Tl ) are the entropy functions for Tu and
Tl , respectively.

Each experiment was run 10 times with different initialization of parameters. The averaged
results together with their variance are used as the ﬁnal eva luation metric.
Implementation We follow the paper (Ng et al., 2001) for implementing the normalized
cut algorithm. A cosine similarity is used to measure the afﬁ nity between any two data
points. Both the EM algorithm and the Kmeans methods are used to cluster the data points
that are projected into the low-dimension space spanned by the smallest eigenvectors of a
graph Laplacian.

4.2 Experiment (I): Effectiveness of The Soft Cut Algorithm

The clustering results of both the soft cut algorithm and the normalized cut algorithm are
summarized in Table 2. In addition to the Kmeans algorithm, we also apply the EM clus-
tering algorithm to the normalized cut algorithm. In this experiment, the number of eigen-
vectors used for the normalized cut algorithms is equal to the number of clusters.

First, comparing to both normalized cut algorithms, we see that the proposed clustering
algorithm substantially outperform the normalized cut algorithms for all datasets. Second,

Table 2: Clustering results for different clustering methods. Clustering accuracy is used
for dataset “L5” and “M5” as the evaluation metric, and norma
lized mutual information is
used for “Pendigit ” and “Ribosome” .
Normalized Cut (Kmeans) Normalized Cut (EM)
Soft Cut
89.2 ± 1.3
83.2 ± 8.8
62.4 ± 5.6
45.1 ± 4.8
64.2 ± 4.9
69.2 ± 2.7
52.8 ± 2.0
46.0 ± 6.4
56.3 ± 3.8
69.7 ± 2.9
62.2 ± 9.1
63.2 ± 3.8

M5
L5
Pendigit
Ribosome

Table 3: Clustering accuracy for normalized cut with embedding in eigenspace with K
eigenvectors. K-means is used.
M5
#Eigenvector
83.2 ± 8.8
K
77.6 ± 8.6
K + 1
79.7 ± 8.5
K + 2
80.2 ± 6.6
K + 3
74.9 ± 9.2
K + 4
70.5 ± 5.7
K + 5
75.5 ± 8.6
K + 6
75.8 ± 7.5
K + 7
73.5 ± 6.6
K + 8

Ribosome
62.2 ± 9.1
65.9 ± 5.8
63.4 ± 4.8
67.2 ± 7.6
60.7 ± 8.4
63.9 ± 8.2
63.5 ± 10.4
56.6 ± 10.7
54.3 ± 7.2

L5
64.1 ± 4.9
69.6 ± 6.7
64.1 ± 5.7
61.4 ± 5.8
59.1 ± 4.7
66.1 ± 4.7
61.9 ± 4.7
59.7 ± 5.6
61.2 ± 4.7

Pendigit
46.0 ± 6.4
43.3 ± 9.1
41.6 ± 9.3
42.9 ± 9.6
47.5 ± 3.7
39.2 ± 9.3
43.4 ± 8.3
46.8 ± 7.3
49.8 ± 8.9

comparing to the normalized cut algorithm using the Kmeans method, we see that the soft
cut algorithm has smaller variance in its clustering results. This can be explained by the
fact that the Kmeans algorithm uses binary cluster membership and therefore is likely to be
trapped by local optimums. As indicated in Table 2, if we replace the Kmeans algoirthm
with the EM algorithm in the normalized cut algorithm, the variance in clustering results is
generally reduced but at the price of degradation in the performance of clustering. Based
on the above observation, we conclude that the soft cut algorithm appears to be effective
and robust for multi-class clustering.

4.3 Experiment (II): Normalized Cut using Different Numbers of Eigenvectors

One potential reason why the normalized cut algorithm perform worse than the proposed
algorithm is that the number of clusters may not be the optimal number of eigenvectors. To
examine this issue, we test the normalized cut algorithm with different number of eigen-
vectors. The Kmeans method is used for clustering the eigenvectors. The results of the
normalized cut algorithm using different number of eigenvectors are summarized in Table
3. The best performance is highlighted by the bold fold.

First, we clearly see that the best clustering results may not necessarily happen when the
number of eigenvectors is exactly equal to the number of clusters. In fact, for three out of
four cases, the best performance is achieved when the number of eigenvectors is larger than
the number of clusters. This result indicates that the choice of numbers of eigenvectors
can have a signi ﬁcant impact on the performance of clusterin g. Second, comparing the
results in Table 3 to the results in Table 2, we see that the soft cut algorithm is still able
to outperform the normalized cut algorithm even with the optimal number of eigenvectors.
In general, since spectral clustering is originally designed for binary-class classi ﬁcation,
it requires an extra step when it is extended to multi-class clustering problems. Hence,
the resulting solutions are usually suboptimal. In contrast, the soft cut algorithm directly

targets on multi-class clustering problems, and thus is able to achieve better performance
than the normalized cut algorithm.

5 Conclusion

In this paper, we proposed a novel probabilistic algorithm for spectral clustering, called
“soft cut ” algorithm. It introduces probabilistic members hip into the normalized cut al-
gorithm and directly targets on the multi-class clustering problems. Our empirical studies
with a number of datasets have shown that the proposed algorithm outperforms the nor-
malized cut algorithm considerably. In the future, we plan to extend this work to other
applications such as image segmentation.

References

Bach, F. R., & Jordan, M. I. (2004). Learning spectral clustering. Advances in Neural
Information Processing Systems 16.
Banerjee, A., Dhillon, I., Ghosh, J., & Sra, S. (2003). Generative model-based clustering
of directional data. Proceedings of the Ninth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-2003).
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. J. Mach. Learn.
Res., 3, 993–1022.
Chung, F. (1997). Spectral graph theory. Amer. Math. Society.
Ding, C., He, X., Zha, H., Gu, M., & Simon, H. (2001). A min-max cut algorithm for graph
partitioning and data clustering. Proc. IEEE Int’l Conf. Data Mining.
Hagen, L., & Kahng, A. (1991). Fast spectral methods for ratio cut partitioning and clus-
tering. Proceedings of IEEE International Conference on Computer Aided Design (pp.
10–13).
Hartigan, J., & Wong., M. (1994). A k-means clustering algorithm. Appl. Statist., 28,
100–108.
Hofmann, T. (1999). Probabilistic latent semantic indexing. Proceedings of the 22nd
Annual ACM Conference on Research and Development in Information Retrieval (pp.
50–57). Berkeley, California.
Jin, R., & Si, L. (2004). A bayesian approach toward active learning for collaborative
ﬁltering.
Proceedings of the 20th conference on Uncertainty in arti ﬁc ial intelligence
(pp. 278–285). Banff, Canada: AUAI Press.
Ng, A., Jordan, M., & Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm.
Advances in Neural Information Processing Systems 14.
Redner, R. A., & Walker, H. F. (1984). Mixture densities, maximum likelihood and the em
algorithm. SIAM Review, 26, 195–239.
Salakhutdinov, R., & Roweis, S. T. (2003). Adaptive overrelaxed bound optimization meth-
ods. Proceedings of the Twentieth International Conference (ICML 2003) (pp. 664–671).
Shi, J., & Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 22, 888–905.
Yu, S. X., & Shi, J. (2003). Multiclass spectral clustering. Proceedings of Ninth IEEE
International Conference on Computer Vision. Nice, France.
Zha, H., He, X., Ding, C., Gu, M., & Simon, H. (2002). Spectral relaxation for k-means
clustering. Advances in Neural Information Processing Systems 14.


Statistical Convergence of Kernel CCA

Kenji Fukumizu
Institute of Statistical Mathematics
Tokyo 106-8569 Japan
fukumizu@ism.ac.jp

Francis R. Bach
Centre de Morphologie Mathematique
Ecole des Mines de Paris, France
francis.bach@mines.org

Arthur Gretton
Max Planck Institute for Biological Cybernetics
72076 T¨ubingen, Germany
arthur.gretton@tuebingen.mpg.de

Abstract

While kernel canonical correlation analysis (kernel CCA) has been
applied in many problems, the asymptotic convergence of the func-
tions estimated from a ﬁnite sample to the true functions has not
yet been established. This paper gives a rigorous proof of the statis-
tical convergence of kernel CCA and a related method (NOCCO),
which provides a theoretical justiﬁcation for these methods. The
result also gives a suﬃcient condition on the decay of the regular-
ization coeﬃcient in the methods to ensure convergence.

1 Introduction

Kernel canonical correlation analysis (kernel CCA) has been proposed as a nonlinear
extension of CCA [1, 11, 3]. Given two random variables, kernel CCA aims at
extracting the information which is shared by the two random variables, and has
been successfully applied in various practical contexts. More precisely, given two
random variables X and Y , the purpose of kernel CCA is to provide nonlinear
mappings f (X ) and g(Y ) such that their correlation is maximized.

As in many statistical methods, the desired functions are in practice estimated from
a ﬁnite sample. Thus, the convergence of the estimated functions to the population
ones with increasing sample size is very important to justify the method. Since the
goal of kernel CCA is to estimate a pair of functions, the convergence should be
evaluated in an appropriate functional norm: thus, we need tools from functional
analysis to characterize the type of convergence.

The purpose of this paper is to rigorously prove the statistical convergence of kernel
CCA, and of a related method. The latter uses a NOrmalized Cross-Covariance
Operator, and we call it NOCCO for short. Both kernel CCA and NOCCO require a
regularization coeﬃcient to enforce smoothness of the functions in the ﬁnite sample
case (thus avoiding a trivial solution), but the decay of this regularisation with
increased sample size has not yet been established. Our main theorems give a
suﬃcient condition on the decay of the regularization coeﬃcient for the ﬁnite sample

estimates to converge to the desired functions in the population limit. Another
important issue in establishing the convergence is an appropriate distance measure
for functions. For NOCCO, we obtain convergence in the norm of reproducing kernel
Hilbert spaces (RKHS) [2]. This norm is very strong: if the positive deﬁnite (p.d.)
kernels are continuous and bounded, it is stronger than the uniform norm in the
space of continuous functions, and thus the estimated functions converge uniformly
to the desired ones. For kernel CCA, we show convergence in the L2 norm, which
is a standard distance measure for functions. We also discuss the relation between
our results and two relevant studies: COCO [9] and CCA on curves [10].

2 Kernel CCA and related methods

In this section, we review kernel CCA as presented by [3], and then formulate it
with covariance operators on RKHS. In this paper, a Hilbert space always refers to
a separable Hilbert space, and an operator to a linear operator. kT k denotes the
operator norm supkϕk=1 kT ϕk, and R(T ) denotes the range of an operator T .
Throughout this paper, (HX , kX ) and (HY , kY ) are RKHS of functions on measur-
able spaces X and Y , respectively, with measurable p.d. kernels kX and kY . We
consider a random vector (X, Y ) : Ω → X × Y with distribution PX Y . The marginal
distributions of X and Y are denoted PX and PY . We always assume
(1)
EX [kX (X, X )] < ∞ and EY [kY (Y , Y )] < ∞.
Note that under this assumption it is easy to see HX and HY are continuously
included in L2 (PX ) and L2 (PY ), respectively, where L2 (µ) denotes the Hilbert
space of square integrable functions with respect to the measure µ.

2.1 CCA in reproducing kernel Hilbert spaces

Classical CCA provides the linear mappings aT X and bT Y that achieve maximum
correlation. Kernel CCA extends this by looking for functions f and g such that
f (X ) and g(Y ) have maximal correlation. More precisely, kernel CCA solves
Cov[f (X ), g(Y )]
Var[f (X )]1/2Var[g(Y )]1/2 .
max
f ∈HX ,g∈HY
In practice, we have to estimate the desired function from a ﬁnite sample. Given
an i.i.d. sample (X1 , Y1 ), . . . , (Xn , Yn ) from PX Y , an empirical solution of Eq. (2) is
dCov[f (X ), g(Y )]
max
HY (cid:1)1/2 ,
HX (cid:1)1/2 (cid:0)dVar[g(Y )] + εnkgk2
(cid:0)dVar[f (X )] + εnkf k2
f ∈HX ,g∈HY
where dCov and dVar denote the empirical covariance and variance, such as
i=1(cid:16)f (Xi ) −
j=1 f (Xj )(cid:17)(cid:16)g(Yi ) −
j=1 g(Yj )(cid:17).
n Pn
n Pn
n Pn
1
1
1
dCov[f (X ), g(Y )] =
The positive constant εn is a regularization coeﬃcient. As we shall see, the regular-
ization terms εnkf k2
HX and εn kgk2
HY make the problem well-formulated statistically,
enforce smoothness, and enable operator inversion, as in Tikhonov regularization.

(2)

(3)

2.2 Representation with cross-covariance operators

Kernel CCA and related methods can be formulated using covariance operators [4,
7, 8], which make theoretical discussions easier.
It is known that there exists a
unique cross-covariance operator ΣY X : HX → HY for (X, Y ) such that
hg , ΣY X f iHY = EX Y (cid:2)(f (X )−EX [f (X )])(g(Y )−EY [g(Y )])(cid:3)
(= Cov[f (X ), g(Y )])

holds for all f ∈ HX and g ∈ HY . The cross covariance operator represents the
covariance of f (X ) and g(Y ) as a bilinear form of f and g . In particular, if Y is
equal to X , the self-adjoint operator ΣXX is called the covariance operator.
Let (X1 , Y1 ), . . . , (Xn , Yn ) be i.i.d. random vectors on X × Y with distribution PX Y .
The empirical cross-covariance operator bΣ(n)
Y X is deﬁned by the cross-covariance
n Pn
operator with the empirical distribution 1
i=1 δXi δYi . By deﬁnition, for any f ∈
HX and g ∈ HY , the operator bΣ(n)
Y X gives the empirical covariance as follows;
hg , bΣ(n)
Y X f iHY = dCov[f (X ), g(Y )].
Let QX and QY be the orthogonal pro jections which respectively map HX onto
R(ΣXX ) and HY onto R(ΣY Y ). It is known [4] that ΣY X can be represented as
ΣY X = Σ1/2
Y Y VY X Σ1/2
XX ,
(4)
where VY X : HX → HY is a unique bounded operator such that kVY X k ≤ 1
and VY X = QY VY X QX . We often write VY X as Σ−1/2
Y Y ΣY X Σ−1/2
XX in an abuse of
XX or Σ−1/2
notation, even when Σ−1/2
are not appropriately deﬁned as operators.
Y Y
With cross-covariance operators, the kernel CCA problem can be formulated as
sub ject to (cid:26)hf , ΣXX f iHX = 1,
f ∈HX ,g∈HY hg , ΣY X f iHY
sup
hg , ΣY Y giHY = 1.
As with classical CCA, the solution of Eq. (5) is given by the eigenfunctions corre-
sponding to the largest eigenvalue of the following generalized eigenproblem:
g (cid:19) .
ΣY Y (cid:19) (cid:18)f
g (cid:19) = ρ1 (cid:18)ΣXX
O (cid:19) (cid:18)f
(cid:18) O
O
ΣX Y
O
ΣY X
Similarly, the empirical estimator in Eq. (3) is obtained by solving
sub ject to (hf , ( bΣ(n)
XX + εn I )f iHX = 1,
f ∈HX ,g∈HY hg , bΣ(n)
Y X f iHY
sup
hg , ( bΣ(n)
Y Y + εn I )giHY = 1.
Let us assume that the operator VY X is compact,1 and let φ and ψ be the unit
eigenfunctions of VY X corresponding to the largest singular value; that is,
f ∈HX ,g∈HY ,kf kHX =kgkHY =1hg , VY X f iHY .
max
hψ , VY X φiHY =
Given φ ∈ R(ΣXX ) and ψ ∈ R(ΣY Y ), the kernel CCA solution in Eq. (6) is
f = Σ−1/2
g = Σ−1/2
XX φ,
Y Y ψ .
(9)
In the empirical case, let bφn ∈ HX and bψn ∈ HY be the unit eigenfunctions corre-
sponding to the largest singular value of the ﬁnite rank operator
Y Y + εn I (cid:1)−1/2 bΣ(n)
Y X := (cid:0) bΣ(n)
Y X (cid:0) bΣ(n)
XX + εn I (cid:1)−1/2
bV (n)
.
(10)
As in Eq. (9), the empirical estimators bfn and bgn in Eq. (7) are equal to
bfn = ( bΣ(n)
bgn = ( bΣ(n)
XX + εn I )−1/2 bφn ,
Y Y + εn I )−1/2 bψn .
1A bounded operator T : H1 → H2 is called compact if any bounded sequence {un } ⊂
n′ converges in H2 . One of the useful properties
n′ } such that T u
H1 has a subsequence {u
of a compact operator is that it admits a singular value decomposition (see [5, 6])

(11)

(8)

(5)

(6)

(7)

Note that all the above empirical operators and the estimators can be expressed in
terms of Gram matrices. The solutions bfn and bgn are exactly the same as those
n Pn
given in [3], and are obtained by linear combinations of kX (·, Xi )− 1
j=1kX (·, Xj )
n Pn
j=1kY (·, Yj ). The functions bφn and bψn are obtained similarly.
and kY (·, Yi ) − 1
There exist additional, related methods to extract nonlinear dependence. The con-
strained covariance (COCO) [9] uses the unit eigenfunctions of ΣY X ;
Cov[f (X ), g(Y )].
max
hg , ΣY X f iHY =
max
f ∈HX ,g∈HY
f ∈HX ,g∈HY
kf kHX =kgkHY =1
kf kHX =kgkHY =1
The statistical convergence of COCO has been proved in [8]. Instead of normalizing
the covariance by the variances, COCO normalizes it by the RKHS norms of f and
g . Kernel CCA is a more direct nonlinear extension of CCA than COCO. COCO
tends to ﬁnd functions with large variance for f (X ) and g(Y ), which may not be the
most correlated features. On the other hand, kernel CCA may encounter situations
where it ﬁnds functions with moderately large covariance but very small variance
for f (X ) or g(Y ), since ΣXX and ΣY Y can have arbitrarily small eigenvalues.

A possible compromise is to use φ and ψ for VY X , the NOrmalized Cross-Covariance
Operator (NOCCO). While the statistical meaning of NOCCO is not as direct as
kernel CCA, it can incorporate the normalization by ΣXX and ΣY Y . We will
establish the convergence of kernel CCA and NOCCO in Section 3.

3 Main theorems: convergence of kernel CCA and NOCCO

(12)

εn = 0,

We show the convergence of NOCCO in the RKHS norm, and the kernel CCA in
L2 sense. The results may easily be extended to the convergence of the eigenspace
corresponding to the m-th largest eigenvalue.
Theorem 1. Let (εn )∞n=1 be a sequence of positive numbers such that
n1/3 εn = ∞.
lim
lim
n→∞
n→∞
Assume VY X is compact, and the eigenspaces given by Eq. (8) are one-dimensional.
Let φ, ψ , bφn , and bψn be the unit eigenfunctions of Eqs. (8) and (10). Then
|h bψn , ψiHY | → 1
|h bφn , φiHX | → 1,
in probability, as n goes to inﬁnity.
Theorem 2. Let (εn )∞n=1 be a sequence of positive numbers which satisﬁes Eq. (12).
Assume that φ and ψ are included in R(ΣXX ) and R(ΣY Y ), respectively, and that
VY X is compact. Then, for f , g , bfn , and bgn in Eqs.(9), (11), we have
(cid:13)(cid:13)( bfn − EX [ bfn (X )]) − (f − EX [f (X )])(cid:13)(cid:13)L2 (PX ) → 0,
(cid:13)(cid:13)(bgn − EY [bgn (Y )]) − (g − EY [g(Y )])(cid:13)(cid:13)L2 (PY ) → 0
in probability, as n goes to inﬁnity.
The convergence of NOCCO in the RKHS norm is a very strong result. If kX and
kY are continuous and bounded, the RKHS norm is stronger than the uniform norm
of the continuous functions. In such cases, Theorem 1 implies bφn and bψn converge
uniformly to φ and ψ , respectively. This uniform convergence is useful in practice,
because in many applications the function value at each point is important.

For any complete orthonormal systems (CONS) {φi}∞i=1 of HX and {ψi }∞i=1 of HY ,
the compactness assumption on VY X requires that the correlation of Σ−1/2
XX φi (X )
and Σ−1/2
Y Y ψi (Y ) decay to zero as i → ∞. This is not necessarily satisﬁed in general.
A trivial example is the case of variables with Y = X , in which VY X = I is not
compact. In this case, NOCCO is solved by an arbitrary function. Moreover, the
kernel CCA does not have solutions, if ΣXX has arbitrarily small eigenvalues.

Leurgans et al. ([10]) discuss CCA on curves, which are represented by stochastic
processes on an interval, and use the Sobolev space of functions with square inte-
grable second derivative. Since the Sobolev space is a RKHS, their method is an
example of kernel CCA. They also show the convergence of estimators under the
condition n1/2 εn → ∞. Although the proof can be extended to a general RKHS,
convergence is measured by the correlation,
|h bfn , ΣXX f iHX |
(h bfn , ΣXX bfn iHX )1/2 (hf , ΣXX f iHX )1/2 → 1,
In fact, using
which is weaker than the L2 convergence in Theorem 2.
it is easy to derive the above convergence from Theorem
hf , ΣXX f iHX = 1,
2. On the other hand, convergence of the correlation does not necessarily imply
h( bfn − f ), ΣXX ( bfn − f )iHX → 0. From the equality
HX − hf , ΣXX f i1/2
h( bfn − f ), ΣXX ( bfn − f )iHX = (h bfn , ΣXX bfn i1/2
)2
HX
XX bfnkHX kΣ1/2
XX f kHX )} kΣ1/2
XX bfn kHX kΣ1/2
+ 2{1 − h bfn , ΣXX f iHX /(kΣ1/2
XX f kHX ,
we require h bfn , ΣXX bfn iHX → hf , ΣXX f iHX = 1 in order to guarantee the left hand
side converges to zero. However, with the normalization h bfn , ( bΣ(n)
XX + εn I ) bfn iHX =
1, convergence of h bfn , ΣXX bfn iHX is not clear. We use the stronger assumption
n1/3 εn → ∞ to prove h( bfn − f ), ΣXX ( bfn − f )iHX → 0 in Theorem 2.
4 Outline of the proof of the main theorems
We show only the outline of the proof in this paper. See [6] for the detail.

4.1 Preliminary lemmas
We introduce some deﬁnitions for our proofs. Let H1 and H2 be Hilbert spaces.
An operator T : H1 → H2 is called Hilbert-Schmidt if P∞i=1 kT ϕi k2
H2 < ∞ for a
CONS {ϕi }∞i=1 of H1 . Obviously kT k ≤ kT kH S . For Hilbert-Schmidt operators,
the Hilbert-Schmidt norm and inner product are deﬁned as
hT1 , T2 iH S = P∞i=1 hT1ϕi , T2ϕi iH2 .
H S = P∞i=1 kT ϕik2
kT k2
H2 ,
These deﬁnitions are independent of the CONS. For more details, see [5] and [8].
For a Hilbert space F , a Borel measurable map F : Ω → F from a measurable space
F is called a random element in F . For a random element F in F with E kF k < ∞,
there exists a unique element E [F ] ∈ F , called the expectation of F , such that
hE [F ], giH = E [hF , giF ]
(∀g ∈ F )
holds. If random elements F and G in F satisfy E [kF k2 ] < ∞ and E [kGk2 ] < ∞,
then hF , GiF is integrable. Moreover, if F and G are independent, we have
E [hF , GiF ] = hE [F ], E [G]iF .

(13)

.

.

It is easy to see under the condition Eq. (1), the random element kX (·, X )kY (·, Y ) in
the direct product HX ⊗ HY is integrable, i.e. E [kkX (·, X )kY (·, Y )kHX ⊗HY ] < ∞.
Combining Lemma 1 in [8] and Eq. (13), we obtain the following lemma.
Lemma 3. The cross-covariance operator ΣY X is Hilbert-Schmidt, and
H S = (cid:13)(cid:13)EY X (cid:2)(cid:0)kX (·, X ) − EX [kX (·, X )](cid:1)(cid:0)kY (·, Y ) − EY [kY (·, Y )](cid:1)(cid:3)(cid:13)(cid:13)2
kΣY X k2
HX ⊗HY
The law of large numbers implies limn→∞ hg , bΣ(n)
Y X f iHY = hg , ΣY X f iHY for each f
and g in probability. The following lemma shows a much stronger uniform result.
Lemma 4.
Y X − ΣY X (cid:13)(cid:13)H S = Op (n−1/2 )
(cid:13)(cid:13) bΣ(n)
(n → ∞).
Proof. Write for simplicity F = kX (·, X ) − EX [kX (·, X )], G = kY (·, Y ) −
EY [kY (·, Y )], Fi = kX (·, Xi ) − EX [kX (·, X )], and Gi = kY (·, Yi ) − EY [kY (·, Y )].
Then, F , F1 , . . . , Fn are i.i.d. random elements in HX , and a similar property also
holds for G, G1 , . . . , Gn . Lemma 3 and the same argument as its proof implies
(cid:13)(cid:13) bΣ(n)
H S = (cid:13)(cid:13) 1
Y X (cid:13)(cid:13)2
j=1 Gj (cid:1)(cid:13)(cid:13)2
j=1 Fj (cid:1)(cid:0)Gi − 1
i=1 (cid:0)Fi − 1
n Pn
n Pn
n Pn
,
HX ⊗HY
i=1 (cid:0)Fi − 1
j=1 Gj (cid:1)(cid:11)HX ⊗HY
j=1 Fj (cid:1)(cid:0)Gi − 1
Y X iH S = (cid:10)E [F G], 1
n Pn
n Pn
n Pn
hΣY X , bΣ(n)
From these equations, we have
Y X − ΣY X (cid:13)(cid:13)2
H S = (cid:13)(cid:13) 1
(cid:13)(cid:13) bΣ(n)
j=1 Gj (cid:1) − E [F G](cid:13)(cid:13)2
j=1 Fj (cid:1)(cid:0)Gi − 1
i=1 (cid:0)Fi − 1
n Pn
n Pn
n Pn
HX ⊗HY
j 6=i (FiGj + Fj Gi ) − E [F G](cid:13)(cid:13)2
= (cid:13)(cid:13) 1
n (cid:0)1 − 1
n (cid:1) Pn
n2 Pn
i=1 Pn
i=1 FiGi − 1
.
HX ⊗HY
Using E [Fi ] = E [Gi ] = 0 and E [FiGj FkGℓ ] = 0 for i 6= j, {k , ℓ} 6= {i, j }, we have
E (cid:13)(cid:13) bΣ(n)
Y X − ΣY X (cid:13)(cid:13)2
n E (cid:2)kF Gk2
HX ⊗HY (cid:3) − 1
n kE [F G]k2
HX ⊗HY + O(1/n2 ).
H S = 1
The proof is completed by Chebyshev’s inequality.
The following two lemmas are essential parts of the proof of the main theorems.
Lemma 5. Let εn be a positive number such that εn → 0 (n → ∞). Then
Y X − (ΣY Y + εn I )−1/2ΣY X (ΣXX + εn I )−1/2(cid:13)(cid:13) = Op (ε−3/2
(cid:13)(cid:13) bV (n)
n−1/2 ).
n
Proof. The operator on the left hand side is equal to
(cid:8)( bΣ(n)
Y Y + εn I )−1/2 − (ΣY Y + εn I )−1/2(cid:9) bΣ(n)
Y X ( bΣ(n)
XX + εn I )−1/2
Y X − ΣY X (cid:9)( bΣ(n)
+ (ΣY Y + εn I )−1/2(cid:8) bΣ(n)
XX + εn I )−1/2
+ (ΣY Y + εn I )−1/2ΣY X (cid:8)( bΣ(n)
XX + εn I )−1/2 − (ΣXX + εn I )−1/2(cid:9).
(14)
From the equality A−1/2 − B−1/2 = A−1/2 (cid:0)B 3/2 − A3/2 (cid:1)B−3/2 + (A − B )B−3/2 , the
ﬁrst term in Eq. (14) is equal to
(cid:8)(bΣ(n)
2 (cid:0)Σ
Y Y + εn I (cid:1)− 3
Y Y − ΣY Y (cid:1)(cid:9)(cid:0) bΣ(n)
Y Y (cid:1)+ (cid:0) bΣ(n)
3
(n) 3
Y X ( bΣ(n)
2 bΣ(n)
Y Y − bΣ
Y Y + εn I )− 1
XX + εn I )− 1
2 .
2
2
Y Y + εn I )−1/2k ≤ 1/√εn , k( bΣ(n)
From k( bΣ(n)
Y X ( bΣ(n)
Y Y + εn I )−1/2 bΣ(n)
XX + εn I )−1/2 k ≤ 1
and Lemma 7, the norm of the above operator is upper-bounded by
εn (cid:8) 3√εn
Y Y k3/2(cid:9) + 1(cid:9)k bΣ(n)
max(cid:8)kΣY Y k3/2 , k bΣ(n)
1
Y Y − ΣY Y k.
A similar bound applies to the third term of Eq. (14), and the second term is
εn kΣY X − bΣ(n)
upper-bounded by 1
Y X k. Thus, Lemma 4 completes the proof.

Lemma 6. Assume VY X is compact. Then, for a sequence εn → 0,
(cid:13)(cid:13)(ΣY Y + εn I )−1/2ΣY X (ΣXX + εn I )−1/2 − VY X (cid:13)(cid:13) → 0
(n → ∞).
Proof. It suﬃces to prove k{(ΣY Y + εn I )−1/2 − Σ−1/2
Y Y }ΣY X (ΣXX + εn I )−1/2k and
Y Y ΣY X {(ΣXX + εn I )−1/2 − Σ−1/2
kΣ−1/2
XX }k converge to zero. The former is equal to
(cid:13)(cid:13)(cid:8)(ΣY Y + εn I )−1/2Σ1/2
Y Y − I (cid:9)VY X (cid:13)(cid:13).
(15)
Note that R(VY X ) ⊂ R(ΣY Y ), as remarked in Section 2.2. Let v = ΣY Y u be
an arbitrary element in R(VY X ) ∩ R(ΣY Y ). We have k{(ΣY Y + εn I )−1/2Σ1/2
Y Y −
Y Y ukHY ≤ kΣ1/2
I }vkHY = k(ΣY Y + εn I )−1/2Σ1/2
Y Y {Σ1/2
Y Y − (ΣY Y + εn I )1/2}Σ1/2
Y Y −
Y Y ukHY . Since (ΣY Y + εn I )1/2 → Σ1/2
(ΣY Y + εn I )1/2k kΣ1/2
Y Y in norm, we obtain
{(ΣY Y + εn I )−1/2Σ1/2
(n → ∞)
Y Y − I }v → 0
(16)
for all v ∈ R(VY X ) ∩ R(ΣY Y ). Because VY X is compact, Lemma 8 in the Appendix
shows Eq. (15) converges to zero. The convergence of the second norm is similar.

4.2 Proof of the main theorems

Proof of Thm. 1. This follows from Lemmas 5, 6, and Lemma 9 in Appendix.
Proof Thm. 2. We show only the convergence of bfn . W.l.o.g, we can assume bφn → φ
HX − 2hφ, Σ1/2
HX = kΣ1/2
in HX . From kΣ1/2
XX bfn iHX + kφk2
XX bfnk2
XX ( bfn − f )k2
HX , it
suﬃces to show Σ1/2
XX bfn converges to φ in probability. We have
XX bfn − φkHX ≤ kΣ1/2
XX {( bΣ(n)
kΣ1/2
XX + εn I )−1/2 − (ΣXX + εn I )−1/2} bφn kHX
XX (ΣXX + εn I )−1/2 ( bφn − φ)kHX + kΣ1/2
+ kΣ1/2
XX (ΣXX + εn I )−1/2φ − φkHX .
Using the same argument as the bound on the ﬁrst term in Eq. (14), the ﬁrst term on
the R.H.S of the above inequality is shown to converge to zero. The convergence of
the second term is obvious. Using the assumption φ ∈ R(ΣXX ), the same argument
as the proof of Eq. (16) applies to the third term, which completes the proof.

5 Concluding remarks

We have established the statistical convergence of kernel CCA and NOCCO, show-
ing that the ﬁnite sample estimators of the nonlinear mappings converge to the
desired population functions. This convergence is proved in the RKHS norm for
NOCCO, and in the L2 norm for kernel CCA. These results give a theoretical jus-
tiﬁcation for using the empirical estimates of NOCCO and kernel CCA in practice.
We have also derived a suﬃcient condition, n1/3 εn → ∞, for the decay of the
regularization coeﬃcient εn , which ensures the convergence described above. As
[10] suggests, the order of the suﬃcient condition seems to depend on the function
norm used to determine convergence. An interesting consideration is whether the
order n1/3 εn → ∞ can be improved for convergence in the L2 or RKHS norm.
Another question that remains to be addressed is when to use kernel CCA, COCO,
or NOCCO in practice. The answer probably depends on the statistical properties
of the data. It might consequently be helpful to determine the relation between the
spectral properties of the data distribution and the solutions of these methods.

Acknowledgements

This work is partially supported by KAKENHI 15700241 and Inamori Foundation.

References

[1] S. Akaho. A kernel method for canonical correlation analysis. Proc. Intern. Meeting
on Psychometric Society (IMPS2001), 2001.

[2] N. Aronsza jn. Theory of reproducing kernels. Trans. American Mathematical Society,
69(3):337–404, 1950.

[3] F. R. Bach and M. I. Jordan. Kernel independent component analysis. J. Machine
Learning Research, 3:1–48, 2002.

[4] C. R. Baker. Joint measures and cross-covariance operators. Trans. American Math-
ematical Society, 186:273–289, 1973.

[5] N. Dunford and J. T. Schwartz. Linear Operators, Part II. Interscience, 1963.

[6] K. Fukumizu, F. R. Bach, and A. Gretton. Consistency of kernel canonical correlation.
Research Memorandum 942, Institute of Statistical Mathematics, 2005.

[7] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised
learning with reproducing kernel Hilbert spaces. J. Machine Learning Research, 5:73–
99, 2004.

[8] A. Gretton, O. Bousquet, A. Smola, and B. Sch¨olkopf. Measuring statistical de-
pendence with Hilbert-Schmidt norms. Tech Report 140, Max-Planck-Institut f¨ur
biologische Kybernetik, 2005.

[9] A. Gretton, A. Smola, O. Bousquet, R. Herbrich, B. Sch¨olkopf, and N. Logothetis.
Behaviour and convergence of the constrained covariance. Tech Report 128, Max-
Planck-Institut f¨ur biologische Kybernetik, 2004.

[10] S. Leurgans, R. Moyeed, and B. Silverman. Canonical correlation analysis when the
data are curves. J. Royal Statistical Society, Series B, 55(3):725–740, 1993.

[11] T. Melzer, M. Reiter, and H. Bischof. Nonlinear feature extraction using general-
ized canonical correlation analysis. Proc. Intern. Conf. Artiﬁcial Neural Networks
(ICANN2001), 353–360, 2001.

A Lemmas used in the proofs

We list the lemmas used in Section 4. See [6] for the proofs.

Lemma 7. Suppose A and B are positive self-adjoint operators on a Hilbert space
such that 0 ≤ A ≤ λI and 0 ≤ B ≤ λI hold for a positive constant λ. Then
kA3/2 − B 3/2k ≤ 3λ3/2 kA − B k.
Lemma 8. Let H1 and H2 be Hilbert spaces, and H0 be a dense linear subspace of
H2 . Suppose An and A are bounded operators on H2 , and B is a compact operator
from H1 to H2 such that Anu → Au for al l u ∈ H0 , and supn kAn k ≤ M for some
M > 0. Then AnB converges to AB in norm.
Lemma 9. Let A be a compact positive operator on a Hilbert space H, and An (n ∈
N) be bounded positive operators on H such that An converges to A in norm. Assume
the eigenspace of A corresponding to the largest eigenvalue is one-dimensional and
spanned by a unit eigenvector φ, and the maximum of the spectrum of An is attained
by a unit eigenvector φn . Then we have |hφn , φiH | → 1 as n → ∞.


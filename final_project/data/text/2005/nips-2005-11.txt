On Local Rewards and Scaling Distributed
Reinforcement Learning

J. Andrew Bagnell
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dbagnell@ri.cmu.edu

Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu

Abstract

We consider the scaling of the number of examples necessary to achieve
good performance in distributed, cooperative, multi-agent reinforcement
learning, as a function of the the number of agents n. We prove a worst-
case lower bound showing that algorithms that rely solely on a global
reward signal to learn policies confront a fundamental limit: They re-
quire a number of real-world examples that scales roughly linearly in the
number of agents. For settings of interest with a very large number of
agents, this is impractical. We demonstrate, however, that there is a class
of algorithms that, by taking advantage of local reward signals in large
distributed Markov Decision Processes, are able to ensure good perfor-
mance with a number of samples that scales as O(log n). This makes
them applicable even in settings with a very large number of agents n.

1

Introduction

Recently there has been great interest in distributed reinforcement learning problems where
a collection of agents with independent action choices attempts to optimize a joint perfor-
mance metric. Imagine, for instance, a trafﬁc engineering a pplication where each trafﬁc
signal may independently decide when to switch colors, and performance is measured by
aggregating the throughput at all trafﬁc stops. Problems wi
th such factorizations where the
global reward decomposes in to a sum of local rewards are common and have been studied
in the RL literature. [10]
The most straightforward and common approach to solving these problems is to apply one
of the many well-studied single agent algorithms to the global reward signal. Effectively,
this treats the multi-agent problem as a single agent problem with a very large action space.
Peshkin et al. [9] establish that policy gradient learning factorizes into independent policy
gradient learning problems for each agent using the global reward signal. Chang et al. [3]
use global reward signals to estimate effective local rewards for each agent. Guestrin et
al. [5] consider coordinating agent actions using the global reward. We argue from an
information theoretic perspective that such algorithms are fundamentally limited in their
scalability. In particular, we show in Section 3 that as a function of the number of agents
n, such algorithms will need to see1 ˜Ω(n) trajectories in the worst case to achieve good
performance.
We suggest an alternate line of inquiry, pursued as well by other researchers (including

1Big- ˜Ω notation omits logarithmic terms, similar to how big-Ω notation drops constant values.

notably [10]), of developing algorithms that capitalize on the availability of local reward
signals to improve performance. Our results show that such local information can dramat-
ically reduce the number of examples necessary for learning to O(log n). One approach
that the results suggest to solving such distributed problems is to estimate model parameters
from all local information available, and then to solve the resulting model ofﬂine. Although
this clearly still carries a high computational burden, it is much preferable to requiring a
large amount of real-world experience. Further, useful approximate multiple agent Markov
Decision Process (MDP) solvers that take advantage of local reward structure have been
developed. [4]
2 Preliminaries
We consider distributed reinforcement learning problems, modeled as MDPs, in which
there are n (cooperative) agents, each of which can directly inﬂuence o nly a small number
of its neighbors. More formally, let there be n agents, each with a ﬁnite state space S of
size |S | states and a ﬁnite action space A of size |A|. The joint state space of all the agents
is therefore S n , and the joint action space An . If st ∈ S n is the joint state of the agents at
time t, we will use s(i)
to denote the state of agent i. Similarly, let a(i)
t denote the action of
t
agent i.
For each agent i ∈ {1, . . . , n}, we let neigh(i) ⊆ {1, . . . , n} denote the subset of
agents that i’s state directly inﬂuences. For notational convenience, w e assume that if
i ∈ neigh(j ), then j ∈ neigh(i), and that i ∈ neigh(i). Thus, the agents can be viewed
as living on the vertices of a graph, where agents have a direct inﬂuence on each other’s
state only if they are connected by an edge. This is similar to the graphical games formal-
ism of [7], and is also similar to the Dynamic Bayes Net (DBN)-MDP formalisms of [6]
and [2]. (Figure 1 depicts a DBN and an agent inﬂuence graph.) DBN for malisms allow
the more reﬁned notion of directionality in the inﬂuence bet ween neighbors.
More formally, each agent i is associated with a CPT (conditional probability table)
t+1 |s(neigh(i))
t ), where s(neigh(i))
Pi (s(i)
, a(i)
denotes the state of agent i’s neighbors at time
t
t
t. Given the joint action a of the agents, the joint state evolves according to
n
Yi=1
For simplicity, we have assumed that agent i’s state is directly inﬂuenced by the states of
neigh(i) but not their actions; the generalization offers no difﬁcul
ties. The initial state s1
is distributed according to some initial-state distribution D .
A policy is a map π : S n 7→ An . Writing π out explicitly as a vector-valued function, we
have π(s) = (π1 (s), . . . , πn (s)), where πi (s) : S n 7→ A is the local policy of agent i. For
some applications, we may wish to consider only policies in which agent i chooses its local
action as a function of only its local state s(i) (and possibly its neighbors); in this case, πi
can be restricted to depend only on s(i) .
Each agent has a local reward function Ri (s(i) , a(i) ), which takes values in the unit inter-
val [0, 1]. The total payoff in the MDP at each step is R(s, a) = (1/n) Pn
i=1 R(s(i) , a(i) ).
We call this R(s, a) the global reward function, since it reﬂects the total reward received
by the joint set of agents. We will consider the ﬁnite-horizo n setting, in which the MDP
terminates after T steps. Thus, the utility of a policy π in an MDP M is
t )|π# .
U (π) = UM (π) = Es1∼D [V π (s1 )] = E " 1
n
T
, a(i)
Xi=1
Xt=1
n
In the reinforcement learning setting, the dynamics (CPTs) and rewards of the problem are
unknown, and a learning algorithm has to take actions in the MDP and use the resulting
observations of state transitions and rewards to learn a good policy. Each “trial ” taken by a
reinforcement learning algorithm shall consist of a T -step sequence in the MDP.

t+1 |s(neigh(i))
p(s(i)
t

p(st+1 |st , at ) =

, a(i)
t ).

(1)

Ri (s(i)
t

Figure 1: (Left) A DBN description of a multi-agent MDP. Each row of (round) nodes in the DBN
corresponds to one agent. (Right) A graphical depiction of the inﬂuence
effects in a multi-agent
MDP. A connection between nodes in the graph implies arrows connecting the nodes in the DBN.

Our goal is to characterize the scaling of the sample complexity for various reinforcement
learning approaches (i.e., how many trials they require in order to learn a near-optimal
policy) for large numbers of agents n. Thus, in our bounds below, no serious attempt has
been made to make our bounds tight in variables other than n.
3 Global rewards hardness result
Below we show that if an RL algorithm uses only the global reward signal, then there
exists a very simple MDP—one with horizon,
T = 1, only one state/trivial dynamics, and
two actions per agent —on which the learning algorithm will re quire ˜Ω(n) trials to learn
a good policy. Thus, such algorithms do not scale well to large numbers of agents. For
example, consider learning in the trafﬁc signal problem des cribed in the introduction with
n = 100, 000 trafﬁc lights. Such an algorithm may then require on the orde r of 100, 000
days of experience (trials) to learn. In contrast, in Section 4, we show that if a reinforcement
learning algorithm is given access to the local rewards, it can be possible to learn in such
problems with an exponentially smaller O(log n) sample complexity.

Theorem 3.1: Let any 0 < ǫ < 0.05 be ﬁxed. Let any reinforcement learning algorithm L
be given that only uses the global reward signal R(s), and does not use the local rewards
Ri (s(i) ) to learn (other than through their sum). Then there exists an MDP with time
horizon T = 1, so that:

m ≥

1. The MDP is very “simple” in that it has only one state (
|S | = 1, |S n | = 1); trivial
state transition probabilities (since T = 1); two actions per agent (|A| = 2); and
deterministic binary (0/1)-valued local reward functions.
2. In order for L to output a policy ˆπ that is near-optimal satisfying2 U ( ˆπ) ≥
maxπ U (π) − ǫ,it is necessary that the number of trials m be at least
0.32n + log(1/4)
log(n + 1)
Proof. For simplicity, we ﬁrst assume that L is a deterministic learning algorithm, so that
in each of the m trials, its choice of action is some deterministic function of the outcomes
of the earlier trials. Thus, in each of the m trials, L chooses a vector of actions a ∈ AN ,
n Pn
and receives the global reward signal R(s, a) = 1
i=1 R(s(i) , a(i) ). In our MDP, each
local reward R(s(i) , a(i) ) will take values only 0 and 1. Thus, R(s, a) can take only n + 1
different values (namely, 0
n , 1
n ). Since T = 1, the algorithm receives only one such
n , . . . , n
reward value in each trial.
Let r1 , . . . , rm be the m global reward signals received by L in the m trials. Since L is
deterministic, its output policy ˆπ will be chosen as some deterministic function of these

= ˜Ω(n).

2For randomized algorithms we consider instead the expectation of U ( ˆπ) under the algorithm’s
randomization.

rewards r1 , . . . , rm . But the vector (r1 , . . . , rm ) can take on only (n + 1)m different values
(since each rt can take only n + 1 different values), and thus ˆπ itself can also take only at
most (n + 1)m different values. Let Πm denote this set of possible values for ˆπ . (|Πm | ≤
(n + 1)m ).
Call each local agent’s two actions a1 , a2 . We will generate an MDP with randomly chosen
parameters. Speci ﬁcally, each local reward Ri (s(i) , a(i) ) function is randomly chosen with
equal probability to either give reward 1 for action a1 and reward 0 for action a2 ; or vice
versa. Thus, each local agent has one “right ” action that giv es reward 1, but the algorithm
has to learn which of the two actions this is. Further, by choosing the right actions, the
optimal policy π∗ attains U (π∗ ) = 1.
n Pn
Fix any policy π . Then UM (π) = 1
i=1 R(s(i) , π(s(i) )) is the mean of n independent
Bernoulli(0.5) random variables (since the rewards are chosen randomly), and has expected
value 0.5. Thus, by the Hoeffding inequality, P (UM (π) ≥ 1− 2ǫ) ≤ exp(−2(0.5− 2ǫ)2n).
Thus, taking a union bound over all policies π ∈ ΠM , we have
P (∃π ∈ ΠM s.t. UM (π) ≥ 1 − 2ǫ) ≤ |ΠM | exp(−2(0.5 − 2ǫ)2n)
(2)
≤ (n + 1)m exp(−2(0.5 − 2ǫ)2n)
(3)
Here, the probability is over the random MDP M . But since L outputs a policy in ΠM , the
chance of L outputting a policy ˆπ with UM ( ˆπ) ≥ 1 − 2ǫ is bounded by the chance that
there exists such a policy in ΠM . Thus,
P (UM ( ˆπ) ≥ 1 − 2ǫ) ≤ (n + 1)m exp(−2(0.5 − 2ǫ)2n).
By setting the right hand side to 1/4 and solving for m, we see that so long as
2(0.5 − 2ǫ)2n + log(1/4)
0.32n + log(1/4)
log(n + 1)
log(n + 1)
we have that P (UM ( ˆπ) ≥ 1 − 2ǫ) < 1/4. (The second equality above follows by taking
ǫ < 0.05, ensuring that no policy will be within 0.1 of optimal.) Thus, under this condition,
by the standard probabilistic method argument [1], there must be at least one such MDP
under which L fails to ﬁnd an ǫ-optimal policy.
For randomized algorithms L, we can deﬁne for each string of input random numbers
to the algorithm ω a deterministic algorithm Lω . Given m samples above, the expected
performance of algorithm Lω over the distribution of MDPs
Ep(M ) [Lω ] ≤ P r(UM (Lω ) ≥ 1 − 2ǫ)1 + (1 − P r(UM (Lω ) ≥ 1 − 2ǫ))(1 − 2ǫ)
3
1
4
4

(1 − 2ǫ) < 1 − ǫ

m <

<

+

(4)

(5)

≤

,

Since

Ep(M )Ep(ω) [UM (Lω )] = Ep(ω)Ep(M ) [UM (Lω )] < Ep(ω) [1 − ǫ]
it follows again from the probabilistic method there must be at least one MDP for which
the L has expected performance less than 1 − ǫ.
(cid:3)
4 Learning with local rewards
Assuming the existence of a good exploration policy, we now show a positive result that if
our learning algorithm has access to the local rewards, then it is possible to learn a near-
optimal policy after a number of trials that grows only logarithmically in the number of
agents n.
In this section, we will assume that the neighborhood structure (encoded by
neigh(i)) is known, but that the CPT parameters of the dynamics and the reward functions
are unknown. We also assume that the size of the largest neighborhood is bounded by
maxi |neigh(i)| = B .
Deﬁnition. A policy πexplore is a (ρ, ν )-exploration policy if, given any i, any conﬁguration
of states s(neigh(i)) ∈ S |neigh(i)| , and any action a(i) ∈ A, on a trial of length T the policy
πexplore has at least a probability ν · ρB of executing action a(i) while i and its neighbors
are in state s(neigh(i)) .

Proposition 4.1: Suppose the MDP’s initial state distribution is random, so that the state
s(i)
i of each agent i is chosen independently from some distribution Di . Further, assume
that Di assigns probability at least ρ > 0 to each possible state value s ∈ S . Then
π (that on each time-step chooses each agent’s action uniformly at
the “random” policy
random over A) is a (ρ, 1
|A| )-exploration policy.

Proof. For any agent i, the initial state of s(neigh(i)) has has at least a ρB chance of being
any particular vector of values, and the random action policy has a 1/|A| chance of taking
any particular action from this state.
(cid:3)
In general, it is a fairly strong assumption to assume that we have an exploration policy.
However, this assumption serves to decouple the problem of exploration from the “sample
complexity” question of how much data we need from the MDP. Sp eci ﬁcally, it guarantees
that we visit each local conﬁguration sufﬁciently often to h
ave a reasonable amount of data
to estimate each CPT. 3
In the envisioned procedure, we will execute an exploration policy for m trials, and
then use the resulting data we collect to obtain the maximum-likelihood estimates for the
CPT entries and the rewards. We call the resulting estimates ˆp(s(i)
t+1 |s(neigh(i))
, a(i)
t ) and
t
ˆR(s(i) , a(i) ).4 The following simple lemma shows that, with a number of trials that grows
only logarithmically in n, this procedure will give us good estimates for all CPTs and local
rewards.

Lemma 4.2: Let any ǫ0 > 0, δ > 0 be ﬁxed. Suppose |neigh(i)| ≤ B for all i, and let
a (ρ, ν )-exploration policy be executed for m trials. Then in order to guarantee that, with
probability at least 1 − δ , the CPT and reward estimates are ǫ0 -accurate:
for all i, s(i)
, a(i)
t+1 , s(neigh(i))
, a(i)
t+1 |s(neigh(i))
t+1 |s(neigh(i))
| ˆp(s(i)
t ) − p(s(i)
t )| ≤ ǫ0
t
t
t
| ˆR(s(i) , a(i) )| − R(s(i) , a(i) )| ≤ ǫ0
for all i, s(i) , a(i) ,
it sufﬁces that the number of trials be

, a(i)
t
(6)

m = O((log n) · poly(

1
ǫ0

,

1
δ

, |S |, |A|, 1/(ν ρB ), B , T )).

Proof (Sketch). Given c examples to estimate a particular CPT entry (or a reward table
entry), the probability that this estimate differs from the true value by more than ǫ0 can be
controlled by the Hoeffding bound:
, a(i)
t+1 |s(neigh(i))
t ) − p(s(i)
, a(i)
t+1 |s(neigh(i))
P (| ˆp(s(i)
t )| ≥ ǫ0 ) ≤ 2 exp(−2ǫ2
0 c).
t
t
Each CPT has at most |A||S |B+1 entries and there are n such tables. There are also
n|S ||A| possible local reward values. Taking a union bound over them, setting our prob-
ability of incorrectly estimating any CPTs or rewards to δ/2, and solving for c gives
log( 4 n |A||S |B+1
c ≥ 2
). For each agent i we see each local conﬁgurations of states and
ǫ2
δ
0
actions (s(neigh(i)) , a(i) ) with probability ≥ ρB ν . For m trajectories the expected number

3Further, it is possible to show a stronger version of our result than that stated below, showing that
a random action policy can always be used as our exploration policy, to obtain a sample complexity
bound with the same logarithmic dependence on n (but signiﬁcantly worse dependencies on T and
B ). This result uses ideas from the random trajectory method of [8], with the key observation that
local conﬁgurations that are not visited reasonably frequently by the ra ndom exploration policy will
not be visited frequently by any policy, and thus inaccuracies in our estimates of their CPT entries
will not signi ﬁcantly affect the result.
4We let ˆp(s(i)
, a(i)
t ) be the uniform distribution if (s(neigh(i))
t+1 |s(neigh(i))
, a(i)
t ) was never ob-
t
t
served in the training data, and similarly let ˆR(s(i) , a(i) ) = 0 if ˆR(s(i) , a(i) ) was never observed.

of samples we see for each CPT entry is at least mρB ν . Call S (s(neigh(i)) ,a(i) )
the number of
m
samples we’ve seen of a conﬁguration (s(neigh(i)) , a(i) ) in m trajectories. Note then that:
P (S (s(neigh(i)) ,a(i) )
− E [S (s(neigh(i)) ,a(i) )
≤ c) ≤ P (S (s(neigh(i)) ,a(i) )
m
m
m
and another application of Hoeffding’s bound ensures that:

] ≤ c − mρB ν ).

P (S (s(neigh(i)) ,a(i) )
m

] ≤ c − mρB ν ) ≤ exp(

− E [S (s(neigh(i)) ,a(i) )
m

−2
mT 2 (c − mρB ν )2 ).
Applying again the union bound to ensure that the probability of failure here is ≤ δ/2 and
solving for m gives the result.
(cid:3)
Deﬁnition. Deﬁne the radius of inﬂuence r(t) after t steps to be the maximum number of
nodes that are within t steps in the neighborhood graph of any single node.
Viewed differently, r(t) upper bounds the number of nodes in the t-th timeslice of the DBN
(as in Figure 1) which are decendants of any single node in the 1-st timeslice. In a DBN
as shown in Figure 1, we have r(t) = O(t). If the neighborhood graph is a 2-d lattice in
which each node has at most 4 neighbors, then r(t) = O(t2 ). More generally, we might
expect to have r(t) = O(t2 ) for “most ” planar neigborhood graphs. Note that, even in the
worst case, by our assumption of each node having B neighbors, we still have the bound
r(t) ≤ B t , which is a bound independent of the number of agents n.

Theorem 4.3: Let any ǫ > 0, δ > 0 be ﬁxed. Suppose |neigh(i)| ≤ B for all i, and let a
(ρ, ν )-exploration policy be executed for m trials in the MDP M . Let ˆM be the maximum
likelihood MDP, estimated from data from these m trials. Let Π be a policy class, and let
U ˆM (π)
ˆπ = arg max
π∈Π
be the best policy in the class, as evaluated on ˆM . Then to ensure that, with probability
1 − δ , we have that ˆπ is near-optimal within Π, i.e., that
UM (π) − ǫ,
UM ( ˆπ) ≥ max
π∈Π

it sufﬁces that the number of trials be:
m = O((log n) · poly(1/ǫ, 1/δ, |S |, |A|, 1/(ν ρB )), B , T , r(T )).

Proof. Our approach is essentially constructive: we show that for any policy, ﬁnite-horizon
value-iteration using approximate CPTs and rewards in its backups will correctly estimate
the true value function for that policy within ǫ/2. For simplicity, we assume that the initial
state distribution is known (and thus the same in ˆM and M ); the generalization offers no
difﬁculties. By lemma (4.2) with m samples we can know both CPTs and rewards with the
probability required within any required ǫ0 .
Note also that for any MDP with the given DBN or neighborhood graph structure (including
both M and ˆM ) the value function for every policy π and at each time-step has a property
of bounded variation:

r(T )T
| ˆVt (s(1) , . . . s(n) ) − ˆVt (s(1) , . . . s(i−1) , s(i)
changed , s(i+1) , . . . , s(n) | ≤
n
This follows since a change in state can effect at most r(T ) agents’ states, so the resulting
change in utility must be bounded by r(T )T /n.
To compute a bound on the error in our estimate of overall utility we compute a bound
on the error induced by a one-step Bellman backup ||B ˆV − ˆB ˆV ||∞ . This quantity can
be bounded in turn by considering the sequence of partially correct backup operators
ˆB0 , . . . , ˆBn where ˆBi is deﬁned as the Bellman operator for policy π using the exact tran-
sitions and rewards for agents 1, 2, . . . , i, and the estimated transitions rewards/transitions

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
c
n
a
m
r
o
f
r
e
p

200 agents, 20% noise is observed rewards

local learner
global learner

2500

2000

1500

1000

500

y
r
a
s
s
e
c
e
n
 
s
e
l
p
m
a
s
 
f
o
 
r
e
b
m
u
n

local learner
global learner

0

0

500

200
1000
1500
number of agents
number of training examples
Figure 2: (Left) Scaling of performance as a function of the number of trajectories seen for a global
reward and local reward algorithms. (Right) Scaling of the number of samples necessary to achieve
near optimal reward as a function of the number of agents.

2500

2000

100

150

400

250

300

350

0

0

50

for agents i + 1, . . . , n. From this deﬁnition it is immediate that the total error is e quivalent
to the telescoping sum:
||B ˆV − ˆB ˆV ||∞ = || ˆB0 ˆV − ˆB1 ˆV + ˆB1 ˆV − ... + ˆBn−1 ˆV − ˆBn ˆV ||∞
(7)
That sum is upper-bounded by the sum of term-by-term errors Pn−1
i=0 || ˆBi ˆV − ˆBi+1 ˆV ||∞ .
We can show that each of the terms in the sum is less than ǫ0 r(T )(T + 1)/n since the
Bellman operators ˆBi ˆV − ˆBi+1 ˆV differ in the immediate reward contribution of agent
i + 1 by ≤ ǫ0 and differ in computing the expected value of the future value by
t+1 |st , π) ˆVt+1 (s)],
t+1 |st ,π) [Xsi+1
∆p(si+1
EQi+1
j=1 p(sj
j=i+2 p(sj
t+1 |st ,π) Qn
t+1 |st , π) ≤ ǫ0 the difference in the CPTs between ˆBi and ˆBi+1 . By the
with ∆p(si+1
bounded variation argument this total is then less than ǫ0 r(T )T |S |/n.
It follows then
Pi || ˆBi ˆV − ˆBi+1 ˆV ||∞ ≤ ǫ0 r(T ) (T + 1)|S |. We now appeal to ﬁnite-horizon bounds
on the error induced by Bellman backups [11] to show that the || ˆV − V ||∞ ≤ T ||B ˆV −
ˆB ˆV ||∞ ≤ T (T + 1) ǫ0 r(T )|S |. Taking the expectation of ˆV with respect to the initial
state distribution D and setting m according to Lemma (4.2) with ǫ0 =
ǫ
2|S |r(T ) T (T +1)
completes the proof.
(cid:3)
5 Demonstration
We ﬁrst present an experimental domain that hews closely to t he theory in Section (3) above
to demonstrate the importance of local rewards. In our simple problem there are n = 400
independent agents who each choose an action in {0, 1}. Each agent has a “correct ” action
that earns it reward Ri = 1 with probability 0.8, and reward 0 with probability 0.2. Equally,
if the agents chooses the wrong action, it earns reward Ri = 1 with probability 0.2.
We compare two methods on this problem. Our ﬁrst global algorithm uses only the global
rewards R and uses this to build a model of the local rewards, and ﬁnally solves the re-
sulting estimated MDP exactly. The local reward functions are learnt by a least-squares
procedure with basis functions for each agent. The second algorithm also learns a local
reward function, but does so taking advantage of the local rewards it observes as opposed
to only the global signal. Figure (2) demonstrates the advantages of learning using a global
reward signal.5 On the right in Figure (2), we compute the time required to achieve 1
4 of
optimal reward for each algorithm, as a function of the number of agents.
In our next example, we consider a simple variant of the multi-agent SY SADM IN6 prob-

5A gradient-based model-free approach using the global reward signal was also tried, but its
performance was signiﬁcantly poorer than that of the two algorithms depic ted in Figure (2, left).
6 In SY SADM IN there is a network of computers that fail randomly. A computer is more likely to
fail if a neighboring computer (arranged in a ring topology) fails. The goal is to reboot machines in
such a fashion so a maximize the number of running computers.

lem [4]. Again, we consider two algorithms: a global R E IN FORC E [9] learner, and a R E -
IN FORC E algorithm run using only local rewards, even through the local R E IN FORC E al-
gorithm run in this way is not guaranteed to converge to the globally optimal (cooperative)
solution. We note that the local algorithm learns much more quickly than using the global
reward. (Figure 3) The learning speed we observed for the global algorithm correlates well
with the observations in [5] that the number of samples needed scales roughly linearly in
the number of agents. The local algorithm continued to require essentially the same number
of examples for all sizes used (up to over 100 agents) in our experiments.

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0

Global
Local

50

100

150

200

250

300

350

400

450

500

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0

Global
Local

50

100

150

200

250

300

350

400

450

500

Figure 3: R E IN FORC E applied to the multi-agent SY SADM IN problem. Local refers to R E IN FORC E
applied using only neighborhood (local) rewards while global refers to standard R E IN FORC E (applied
to the global reward signal). (Left) shows averaged reward performance as a function of number of
iterations for 10 agents. (Right) depicts the performance for 20 agents.
References
[1] N. Alon and J. Spencer. The Probabilistic Method. Wiley, 2000.
[2] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and
computational leverage. Journal of Artiﬁcial Intelligence Research , 1999.
[3] Y. Chang, T. Ho, and L. Kaelbling. All learning is local: Multi-agent learning in global reward
games. In Advances in NIPS 14, 2004.
[4] C. Guestrin, D. Koller, and R. Parr. Multi-agent planning with factored MDPs. In NIPS-14,
2002.
[5] C. Guestrin, M. Lagoudakis, and R. Parr. Coordinated reinforcement learning. In ICML, 2002.
[6] M. Kearns and D. Koller. Efﬁcient reinforcement learning in factored mdps. In IJCAI 16, 1999.
[7] M. Kearns, M. Littman, and S. Singh. Graphical models for game theory. In UAI, 2001.
[8] M. Kearns, Y. Mansour, and A. Ng. Approximate planning in large POMDPs via reusable
trajectories. (extendedversionofpaper inNIPS12), 1999.
[9] L. Peshkin, K-E. Kim, N. Meleau, and L. Kaelbling. Learning to cooperate via policy search.
In UAI 16, 2000.
[10] J. Schneider, W. Wong, A. Moore, and M. Riedmiller. Distributed value functions. In ICML,
1999.
[11] R. Williams and L. Baird. Tight performance bounds on greedy policies based on imperfect
value functions. Technical report, Northeastern University, 1993.


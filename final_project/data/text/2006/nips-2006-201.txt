Multi-Instance Multi-Label Learning with
Application to Scene Classi ﬁcation

Zhi-Hua Zhou
Min-Ling Zhang
National Laboratory for Novel Software Technology
Nanjing University, Nanjing 210093, China
{zhouzh,zhangml}@lamda.nju.edu.cn

Abstract

In this paper, we formalize multi-instance multi-label learning, where each train-
ing example is associated with not only multiple instances but also multiple class
labels. Such a problem can occur in many real-world tasks, e.g. an image usually
contains multiple patches each of which can be described by a feature vector, and
the image can belong to multiple categories since its semantics can be recognized
in different ways. We analyze the relationship between multi-instance multi-label
learning and the learning frameworks of traditional supervised learning, multi-
instance learning and multi-label learning. Then, we propose the M IM LBOO S T
and M IM LSVM algorithms which achieve good performance in an application to
scene classi ﬁcation.

1

Introduction

In traditional supervised learning, an object is represented by an instance (or feature vector) and
associated with a class label. Formally, let X denote the instance space (or feature space) and Y
the set of class labels. Then the task is to learn a function f : X → Y from a given data set
{(x1 , y1 ), (x2 , y2 ), · · · , (xm , ym )}, where xi ∈ X is an instance and yi ∈ Y the known label of xi .
Although the above formalization is prevailing and successful, there are many real-world problems
which do not ﬁt this framework well, where a real-world objec t may be associated with a number of
instances and a number of labels simultaneously. For example, an image usually contains multiple
patches each can be represented by an instance, while in image classi ﬁcation such an image can
belong to several classes simultaneously, e.g. an image can belong to mountains as well as Africa.
Another example is text categorization, where a document usually contains multiple sections each of
which can be represented as an instance, and the document can be regarded as belonging to different
categories if it was viewed from different aspects, e.g. a document can be categorized as scienti ﬁc
novel, Jules Verne’s writing or even books on travelling. Web mining is a further example, where
each of the links can be regarded as an instance while the web page itself can be recognized as news
page, sports page, soccer page, etc.

In order to deal with such problems, in this paper we formalize multi-instance multi-label learning
(abbreviated as M IM L). In this learning framework, a training example is described by multiple
instances and associated with multiple class labels. Formally, let X denote the instance space and
Y the set of class labels. Then the task is to learn a function fM IM L : 2X → 2Y from a given data
(i)
(i)
(i)
set {(X1 , Y1 ), (X2 , Y2 ), · · · , (Xm , Ym )}, where Xi ⊆ X is a set of instances {x
ni },
1 , x
2 , · · · , x
(i)
2 , · · · , y (i)
j ∈ X (j = 1, 2, · · · , ni ), and Yi ⊆ Y is a set of labels {y (i)
1 , y (i)
}, y (i)
k ∈ Y (k =
x
li
1, 2, · · · , li ). Here ni denotes the number of instances in Xi and li the number of labels in Yi .
After analyzing the relationship between M IM L and the frameworks of traditional supervised learn-
ing, multi-instance learning and multi-label learning, we propose two M IM L algorithms, M IM L -

BOO S T and M IM LSVM . Application to scene classi ﬁcation shows that, solving so me real-world
problems in the M IM L framework can achieve better performance than solving them in existing
frameworks such as multi-instance learning and multi-label learning.

2 Multi-Instance Multi-Label Learning

We start by investigating the relationship between M IM L and the frameworks of traditional super-
vised learning, multi-instance learning and multi-label learning, and then we develop some solutions.

Multi-instance learning [4] studies the problem where a real-world object described by a number of
instances is associated with one class label. Formally, the task is to learn a function fM IL : 2X →
{−1, +1} from a given data set {(X1 , y1 ), (X2 , y2 ), · · · , (Xm , ym )}, where Xi ⊆ X is a set of
(i)
(i)
(i)
(i)
j ∈ X (j = 1, 2, · · · , ni ), yi ∈ {−1, +1} is the label of Xi .1
ni }, x
instances {x
1 , x
2 , · · · , x
Multi-instance learning techniques have been successfully applied to diverse applications including
scene classi ﬁcation [3, 7].

Multi-label learning [8] studies the problem where a real-world object described by one instance is
associated with a number of class labels. Formally, the task is to learn a function fM LL : X → 2Y
from a given data set {(x1 , Y1 ), (x2 , Y2 ), · · · , (xm , Ym )}, where xi ∈ X is an instance and Yi ⊆ Y
2 , · · · , y (i)
1 , y (i)
a set of labels {y (i)
}, y (i)
k ∈ Y (k = 1, 2, · · · , li ).2 Multi-label learning techniques
li
have also been successfully applied to scene classi ﬁcation [1].

In fact, the multi- learning frameworks result from the ambiguity in representing real-world objects.
Multi-instance learning studies the ambiguity in the input space (or instance space), where an object
has many alternative input descriptions, i.e.
instances; multi-label learning studies the ambiguity
in the output space (or label space), where an object has many alternative output descriptions, i.e.
labels; while M IM L considers the ambiguity in the input and output spaces simultaneously. We
illustrate the differences among these learning frameworks in Figure 1.

(a) Traditional supervised learning

(b) Multi-instance learning

(c) Multi-label learning

(d) Multi-instance multi-label learning

Figure 1: Four different learning frameworks

Traditional supervised learning is evidently a degenerated version of multi-instance learning as well
as a degenerated version of multi-label learning, while traditional supervised learning, multi-instance
learning and multi-label learning are all degenerated versions of M IM L . Thus, we can tackle M IM L
by identifying its equivalence in the traditional supervised learning framework, using multi-instance
learning or multi-label learning as the bridge.

1According to notions used in multi-instance learning, (Xi , yi ) is a labeled bag while Xi an unlabeled bag.
2Although most works on multi-label learning assume that an instance can be associated with multiple valid
labels, there are also works assuming that only one of the labels associated with an instance is correct [6]. We
adopt the former assumption in this paper.

Solution 1: Using multi-instance learning as the bridge: We can transform a M IM L learning task,
to learn a function fM IM L : 2X → 2Y , into a multi-instance learning task, i.e.
i.e.
to learn a
function fM IL : 2X × Y → {−1, +1}. For any y ∈ Y , fM IL (Xi , y) = +1 if y ∈ Yi and
−1 otherwise. The proper labels for a new example X ∗ can be determined according to Y ∗ =
{y | argy∈Y [fM IL (X ∗ , y) = +1]}. We can transform this multi-instance learning task further into
a traditional supervised learning task, i.e. to learn a function fS I SL : X × Y → {−1, +1}, under
(i)
a constraint specifying how to derive fM IL (Xi , y) from fS I SL (x
j , y) (j = 1, · · · , ni ). For any
(i)
j , y) = +1 if y ∈ Yi and −1 otherwise. Here the constraint can be fM IL (Xi , y) =
y ∈ Y , fS I SL (x
(i)
sign[Pni
j , y)] which has been used in transforming multi-instance learning tasks into
j=1 fS I SL (x
traditional supervised learning tasks [9].3 Note that other kinds of constraint can also be used here.
Solution 2: Using multi-label learning as the bridge: We can also transform a M IM L learning task,
i.e. to learn a function fM IM L : 2X → 2Y , into a multi-label learning task, i.e. to learn a function
fM LL : Z → 2Y . For any zi ∈ Z , fM LL (zi ) = fM IM L (Xi ) if zi = φ(Xi ), φ : 2X → Z .
The proper labels for a new example X ∗ can be determined according to Y ∗ = fM LL (φ(X ∗ )). We
can transform this multi-label learning task further into a traditional supervised learning task, i.e. to
learn a function fS I SL : Z × Y → {−1, +1}. For any y ∈ Y , fS I SL (zi , y) = +1 if y ∈ Yi and
−1 otherwise. That is, fM LL (zi ) = {y | argy∈Y [fS I SL (zi , y) = +1]}. Here the mapping φ can be
implemented with constructive clustering which has been used in transforming multi-instance bags
into traditional single-instances [11]. Note that other kinds of mapping can also be used here.

3 Algorithms

In this section, we propose two algorithms for solving M IM L problems: M IM LBOO S T works along
the ﬁrst solution described in Section 2, while M IM LSVM works along the second solution.

3.1 M IMLBOO ST

Given any set Ω, let |Ω| denote its size, i.e. the number of elements in Ω; given any predicate π , let
[[π ]] be 1 if π holds and 0 otherwise; given (Xi , Yi ), for any y ∈ Y , let Ψ(Xi , y) = +1 if y ∈ Yi
and −1 otherwise, where Ψ is a function Ψ : 2X × Y → {−1, +1}. The M IM LBOO S T algorithm is
presented in Table 1.
In the ﬁrst step, each M IM L example (Xu , Yu ) (u = 1, 2, · · · , m) is transformed into a set of |Y |
number of multi-instance bags, i.e. {[(Xu , y1 ), Ψ(Xu , y1 )], [(Xu , y2 ), Ψ(Xu , y2 )], · · · , [(Xu , y|Y | ),
Ψ(Xu , y|Y | )]}. Note that [(Xu , yv ), Ψ(Xu , yv )] (v = 1, 2, · · · , |Y |) is a labeled multi-instance
(u)
(u)
bag where (Xu , yv ) is a bag containing nu number of instances, i.e. {(x
2 , yv ), · · · ,
1 , yv ), (x
(u)
nu , yv )}, and Ψ(Xu , yv ) ∈ {+1, −1} is the label of this bag.
(x
Thus, the original M IM L data set is transformed into a multi-instance data set containing m × |Y |
number of bags, i.e. {[(X1 , y1 ), Ψ(X1 , y1 )], · · · , [(X1 , y|Y | ), Ψ(X1 , y|Y | )], [(X2 , y1 ), Ψ(X2 , y1 )],
· · · , [(Xm , y|Y | ), Ψ(Xm , y|Y | )]}. Let [(X (i) , y (i) ), Ψ(X (i) , y (i) )] denote the ith of these m × |Y |
number of bags, that is, (X (1) , y (1) ) denotes (X1 , y1 ), · · · , (X (|Y |) , y (|Y |) ) denotes (X1 , y|Y | ), · · · ,
(X (m×|Y |) , y (m×|Y |) ) denotes (Xm , y|Y | ), where (X (i) , y (i) ) contains ni number of instances, i.e.
(i)
(i)
(i)
ni , y (i) )}.
2 , y (i) ), · · · , (x
1 , y (i) ), (x
{(x
Then, from the data set a multi-instance learning function fM IL can be learned, which can accom-
plish the desired M IM L function because fM IM L (X ∗ ) = {y | argy∈Y (sign[fM IL (X ∗ , y)] = +1)}.
Here we use M IBOO S T ING [9] to implement fM IL .
For convenience, let (B , g) denote the bag [(X, y), Ψ(X, y)]. Then, here the goal is to learn a func-
tion F (B ) minimizing the bag-level exponential loss EBEG |B [exp(−gF (B ))], which ultimately

3This constraint assumes that all instances contribute equally and independently to a bag’s label, which is
different from the standard multi-instance assumption that there is one ‘key’ instance in a bag that triggers
whether the bag’s class label will be positive or negative. Nevertheless, it has been shown that this assumption
is reasonable and effective [9]. Note that the standard multi-instance assumption does not always hold, e.g. the
label Africa of an image is usually triggered by several patches jointly instead of by only one patch.

Table 1: The M IM LBOO S T algorithm

1

Transform each M IM L example (Xu , Yu ) (u = 1, 2, · · · , m) into |Y | number of multi-
instance bags {[(Xu , y1 ), Ψ(Xu , y1 )], · · · , [(Xu , y|Y | ), Ψ(Xu , y|Y | )]}. Thus, the original
data set is transformed into a multi-instance data set containing m × |Y | number of
multi-instance bags, denoted by {[(X (i) , y (i) ), Ψ(X (i) , y (i) )]} (i = 1, 2, · · · , m × |Y |).

2

Initialize weight of each bag to W (i) = 1
m×|Y | (i = 1, 2, · · · , m × |Y |).

.

ni

3b

3 Repeat for t = 1, 2, · · · , T iterations:
Set W (i)
j = W (i) /ni (i = 1, 2, · · · , m × |Y |), assign the bag’s label Ψ(X (i) , y (i) )
3a
to each of its instances (x(i)
j , y (i) ) (j = 1, 2, · · · , ni ), and build an instance-level
predictor ht [(x(i)
j , y (i) )] ∈ {−1, +1}.
For the ith bag, compute the error rate e(i) ∈ [0, 1] by counting the number of
misclassiﬁed instances within the bag, i.e. e(i) = Pni
(i)
,y(i) )] 6=Ψ(X (i) ,y(i) )]]
[[ht [(x
j
j=1
If e(i) < 0.5 for all i ∈ {1, 2, · · · , m × |Y |}, go to Step 4.
3c
3d Compute ct = arg minct Pm×|Y |
i=1 W (i) exp[(2e(i) − 1)ct ].
3e
If ct ≤ 0, go to Step 4.
Set W (i) = W (i) exp[(2e(i) − 1)ct ] (i = 1, 2, · · · , m × |Y |) and re-normalize such
3f
that 0 ≤ W (i) ≤ 1 and Pm×|Y |
i=1 W (i) = 1.
4 Return Y ∗ = {y | argy∈Y sign ³Pj Pt ctht [(x∗
j , y)]´ = +1} (x∗
j is X ∗ ’s j th instance).
2 log P r(g=1|B )
estimates the bag-level log-odds function 1
P r(g=−1|B ) . In each boosting round, the aim is to
expand F (B ) into F (B ) + cf (B ), i.e. adding a new weak classi ﬁer, so that the exponential lo ss
is minimized. Assuming all instances in a bag contribute equally and independently to the bag’s
label, f (B ) = 1
nB Pj h(bj ) can be derived, where h(bj ) ∈ {−1, +1} is the prediction of the
instance-level classi ﬁer h(·) for the j th instance in bag B , and nB is the number of instances in B .
It has been shown by [9] that the best f (B ) to be added can be achieved by seeking h(·) which
(i)
maximizes Pi Pni
j=1 [ 1
j )], given the bag-level weights W = exp(−gF (B )). By
W (i) g (i)h(b
ni
assigning each instance the label of its bag and the corresponding weight W (i) /ni , h(·) can be
learned by minimizing the weighted instance-level classi ﬁ cation error. This actually corresponds to
the Step 3a of M IM LBOO S T . When f (B ) is found, the best multiplier c > 0 can be got by directly
optimizing the exponential loss:

EBEG |B [exp(−gF (B ) + c(−gf (B )))] = Xi
= Xi
(i)
where e(i) = 1
j ) 6= g (i) )]] (computed in Step 3b). Minimization of this expectation ac-
ni Pj [[(h(b
tually corresponds to Step 3d, where numeric optimization techniques such as quasi-Newton method
can be used. Finally, the bag-level weights are updated in Step 3f according to the additive structure
of F (B ).

W (i) exp[c Ã−
g (i) Pj h(b
ni
W (i) exp[(2e(i) − 1)c]

!]

(i)
j )

3.2 M IMLSVM

Given (Xi , Yi ) and zi = φ(Xi ) where φ : 2X → Z , for any y ∈ Y , let Φ(zi , y) = +1 if y ∈ Yi
and −1 otherwise, where Φ is a function Φ : Z × Y → {−1, +1}. The M IM LSVM algorithm is
presented in Table 2.
In the ﬁrst step, the Xu of each M IM L example (Xu , Yu ) (u = 1, 2, · · · , m) is collected and put
into a data set Γ. Then, in the second step, k-medoids clustering is performed on Γ. Since each

Table 2: The M IM LSVM algorithm

1

For M IM L examples (Xu , Yu ) (u = 1, 2, · · · , m), Γ = {Xu |u = 1, 2, · · · , m}.

2 Randomly select k elements from Γ to initialize the medoids Mt (t = 1, 2, · · · , k),
repeat until all Mt do not change:
2a
Γt = {Mt } (t = 1, 2, · · · , k).
2b Repeat for each Xu ∈ (Γ − {Mt |t = 1, 2, · · · , k}):
index = arg mint∈{1,···,k} dH (Xu , Mt ), Γindex = Γindex ∪ {Xu }.
dH (A, B ) (t = 1, 2, · · · , k).
2c Mt = arg min
A∈Γt PB∈Γt
Transform (Xu , Yu ) into a multi-label example (zu , Yu ) (u = 1, 2, · · · , m), where
zu = (zu1 , zu2 , · · · , zuk ) = (dH (Xu , M1 ), dH (Xu , M2 ), · · · , dH (Xu , Mk )).

3

4

For each y ∈ Y , derive a data set Dy = {(zu , Φ (zu , y)) |u = 1, 2, · · · , m}, and then
train an SVM hy = SV M T rain(Dy ).

5 Return Y ∗ = {arg max
hy (z ∗ )} ∪ {y |hy (z ∗ ) ≥ 0, y ∈ Y }, where z ∗ = (dH (X ∗ , M1 ),
y∈Y
dH (X ∗ , M2 ), · · · , dH (X ∗ , Mk )).

data item in Γ, i.e. Xu , is an unlabeled multi-instance bag instead of a single instance, we employ
Hausdorff distance [5] to measure the distance. In detail, given two bags A = {a1 , a2 , · · · , anA }
and B = {b1 , b2 , · · · , bnB }, the Hausdorff distance between A and B is deﬁned as
kb − ak}
min
ka − bk, max
min
dH (A, B ) = max{max
a∈A
a∈A
b∈B
b∈B
where ka − bk measures the distance between the instances a and b, which takes the form of
Euclidean distance here.
After the clustering process, we divide the data set Γ into k partitions whose medoids are Mt (t =
1, 2, · · · , k), respectively. With the help of these medoids, we transform the original multi-instance
example Xu into a k-dimensional numerical vector zu , where the ith (i = 1, 2, · · · , k) component
of zu is the distance between Xu and Mi , that is, dH (Xu , Mi ). In other words, zui encodes some
structure information of the data, that is, the relationship between Xu and the ith partition of Γ.
This process reassembles the constructive clustering process used by [11] in transforming multi-
instance examples into single-instance examples except that in [11] the clustering is executed at the
instance level while here we execute it at the bag level. Thus, the original M IM L examples (Xu , Yu )
(u = 1, 2, · · · , m) have been transformed into multi-label examples (zu , Yu ) (u = 1, 2, · · · , m),
which corresponds to the Step 3 of M IM LSVM . Note that this transformation may lose information,
nevertheless the performance of M IM LSVM is still good. This suggests that M IM L is a powerful
framework which has captured more original information than other learning frameworks.
Then, from the data set a multi-label learning function fM LL can be learned, which can accom-
plish the desired M IM L function because fM IM L (X ∗ ) = fM LL (z ∗ ). Here we use M LSVM [1] to
implement fM LL .
Concretely, M LSVM decomposes the multi-label learning problem into multiple independent binary
classi ﬁcation problems (one per class), where each example associated with the label set Y is re-
garded as a positive example when building SVM for any class y ∈ Y , while regarded as a negative
example when building SVM for any class y /∈ Y , as shown in the Step 4 of M IM LSVM . In making
predictions, the T-Criterion [1] is used, which actually corresponds to the Step 5 of the M IM LSVM
algorithm. That is, the test example is labeled by all the class labels with positive SVM scores, ex-
cept that when all the SVM scores are negative, the test example is labeled by the class label which
is with the top (least negative) score.

4 Application to Scene Classiﬁcation

The data set consists of 2,000 natural scene images belonging to the classes desert, mountains, sea,
sunset, and trees, as shown in Table 3. Some images were from the COR E L image collection while
some were collected from the Internet. Over 22% images belong to multiple classes simultaneously.

Table 3: The image data set (d: desert, m: mountains, s: sea, su: sunset, t: trees)

label
d
m
s
su
t

# images
340
268
341
216
378

label
d + m
d + s
d + su
d + t
m + s

# images
19
5
21
20
38

label
m + su
m + t
s + su
s + t
su + t

# images
19
106
172
14
28

label
d + m + su
d + su + t
m + s + t
m + su + t
s + su + t

# images
1
3
6
1
4

4.1 Comparison with Multi-Label Learning Algorithms

Since the scene classi ﬁcation task has been successfully ta ckled by multi-label learning algo-
rithms [1], we compare the M IM L algorithms with established multi-label learning algorithms AD -
ABOO S T.MH [8] and M LSVM [1]. The former is the core of a successful multi-label learning system
BOO ST EX T ER [8], while the latter has achieved excellent performance in scene classi ﬁcation [1].

For M IM LBOO S T and M IM LSVM , each image is represented as a bag of nine instances generated
by the SBN method [7]. Here each instance actually corresponds to an image patch, and better
performance can be expected with better image patch generation method. For ADABOO S T.MH and
M LSVM , each image is represented as a feature vector obtained by concatenating the instances of
M IM LBOO S T or M IM LSVM . Gaussian kernel L IB SVM [2] is used to implement M LSVM , where
the cross-training strategy is used to build the classi ﬁers while the T-Criterion is used to label the
images [1]. The M IM LSVM algorithm is also realized with a Gaussian kernel, while the parameter
k is set to be 20% of the number of training images.4 Note that the instance-level predictor used in
Step 3a of M IM LBOO S T is also a Gaussian kernel L IB SVM (with default parameters).

Since ADABOO S T.MH and M LSVM make multi-label predictions, here the performance of the
compared algorithms are evaluated according to ﬁve multi-l abel evaluation metrics, as shown in
Tables 4 to 7, where ‘↓’ indicates ‘the smaller the better’ while ‘↑’ indicates ‘the bigger the better’.
Details of these evaluation metrics can be found in [8]. Tenfold cross-validation is performed and
‘mean ± std’ is presented in the tables, where the best performance achieved by each algorithm
is bolded. Note that since in each boosting round M IM LBOO S T performs more operations than
ADABOO S T.MH does, for fair comparison, the boosting rounds used by ADABOO S T.MH are set to
ten times of that used by M IM LBOO S T such that the time cost of them are comparable.

Table 4: The performance of M IM LBOO S T with different boosting rounds

boosting
rounds
5
10
15
20
25

hamm. loss ↓
.202±.011
.197±.010
.195±.009
.193±.008
.189±.009

evaluation metric
coverage ↓
one-error ↓
1.026±.093
.373±.045
.362±.040
1.013±.109
1.004±.101
.361±.034
.996±.102
.355±.037
.351±.039
.989±.103

rank . loss ↓
.208±.028
.191±.027
.186±.025
.183±.025
.181±.026

ave. prec. ↑
.764±.027
.770±.026
.772±.023
.775±.024
.777±.025

Table 5: The performance of ADABOO S T.MH with different boosting rounds

boosting
rounds
50
100
150
200
250

hamm. loss ↓
.228±.013
.234±.019
.233±.020
.232±.012
.231±.018

evaluation metric
coverage ↓
one-error ↓
.473±.031
1.299±.099
1.292±.138
.465±.042
1.279±.140
.465±.053
1.269±.107
.453±.031
.451±.046
1.258±.137

rank . loss ↓
.263±.022
.259±.030
.255±.032
.253±.022
.250±.031

ave. prec. ↑
.695±.022
.698±.033
.700±.033
.706±.020
.708±.030

4 In preliminary experiments, several percentage values have been tested ranging from 20% to 100% with an
interval of 20%. The results show that these values do not signiﬁcantly af fect the performance of M IM LSVM .

Table 6: The performance of M IM LSVM with different γ used in Gaussian kernel

Gaussian
kernel
γ = .1
γ = .2
γ = .3
γ = .4
γ = .5

hamm. loss ↓
.181±.017
.180±.017
.188±.016
.193±.014
.196±.014

evaluation metric
coverage ↓
one-error ↓
.332±.036
1.024±.089
1.022±.085
.327±.033
1.065±.094
.344±.032
1.080±.099
.358±.030
.370±.033
1.109±.101

rank . loss ↓
.187±.018
.187±.018
.196±.020
.202±.022
.209±.023

ave. prec. ↑
.780±.021
.783±.020
.772±.020
.764±.021
.757±.023

Table 7: The performance of M LSVM with different γ used in Gaussian kernel

Gaussian
kernel
γ = 1
γ = 2
γ = 3
γ = 4
γ = 5

hamm. loss ↓
.200±.014
.196±.013
.195±.015
.196±.016
.202±.015

evaluation metric
coverage ↓
one-error ↓
.379±.032
1.125±.115
1.115±.122
.368±.032
1.129±.113
.370±.034
1.151±.122
.372±.034
.388±.032
1.181±.128

rank . loss ↓
.214±.020
.211±.023
.214±.022
.220±.024
.229±.026

ave. prec. ↑
.751±.022
.756±.022
.754±.023
.751±.023
.741±.023

Comparing Tables 4 to 7 we can ﬁnd that both M IM LBOO S T and M IM LSVM are apparently better
than ADABOO S T.MH and M LSVM . Impressively, pair-wise t-tests with .05 signi ﬁcance level reveal
that the worst performance of M IM LBOO S T (with 5 boosting rounds) is even signi ﬁcantly better than
the best performance of ADABOO S T.MH (with 250 boosting rounds) on all the evaluation metrics,
and is signi ﬁcantly better than the best performance of M LSVM (with γ = 2) in terms of coverage
while comparable on the remaining metrics; the worse performance of M IM LSVM (with γ = .5)
is even comparable to the best performance of M LSVM and is signi ﬁcantly better than the best
performance of ADABOO S T.MH on all the evaluation metrics. These observations conﬁr m that for-
malizing the scene classi ﬁcation task as a M IM L problem to solve by M IM LBOO S T or M IM LSVM is
better than formalizing it as a multi-label learning problem to solve by ADABOO S T.MH or M LSVM .

4.2 Comparison with Multi-Instance Learning Algorithms

Since the scene classi ﬁcation task has been successfully ta ckled by multi-instance learning algo-
rithms [7], we compare the M IM L algorithms with established multi-instance learning algorithms
D IV ER S E D EN S I TY [7] and EM -DD [10]. The former is one of the most inﬂuential multi-instanc e
learning algorithm and has achieved excellent performance in scene classi ﬁcation [7], while the
latter has achieved excellent performance on multi-instance benchmark tests [10].

Here all the compared algorithms use the same input representation. That is, each image is repre-
sented as a bag of nine instances generated by the SBN method [7]. The parameters of D IV ER S E
D EN S I TY and EM -DD are set according to the settings that resulted in the best performance [7, 10].
The M IM LBOO S T and M IM LSVM algorithms are implemented as described in Section 4.1, with 25
boosting rounds for M IM LBOO S T while γ = .2 for M IM LSVM .
Since D IV ER S E D EN S I TY and EM -DD make single-label predictions, here the performance of the
compared algorithms are evaluated according to predictive accuracy, i.e. classi ﬁcation accuracy
on test set. Note that for M IM LBOO S T and M IM LSVM , the top ranked class is regarded as the
single-label prediction. Tenfold cross-validation is performed and ‘mean ± std’ is presented in
Table 8, where the best performance on each image class is bolded. Note that besides the predictive
accuracies on each class, the overall accuracy is also presented, which is denoted by ‘overall’.

We can ﬁnd from Table 8 that M IM LBOO S T achieves the best performance on image classes desert
and trees while M IM LSVM achieves the best performance on the remaining image classes. Overall,
M IM LSVM achieves the best performance. Pair-wise t-tests with .05 signi ﬁcance level reveal that
the overall performance of M IM LSVM is comparable to that of M IM LBOO S T , both are signi ﬁcantly
better than that of D IV ER S E D EN S I TY and EM -DD. These observations conﬁrm that formalizing the
scene classi ﬁcation task as a M IM L problem to solve by M IM LBOO S T or M IM LSVM is better than
formalizing it as a multi-instance learning problem to solve by D IV ER S E D EN S I TY or EM -DD.

Table 8: Compare predictive accuracy of M IM LBOO S T , M IM LSVM , D IV ER S E D EN S I TY and EM -DD

Image
class
desert
mountains
sea
sunset
trees
overall

Compared algorithms
M IM LBOO S T M IM LSVM D IV ER S E D EN S I TY
.869±.014
.768±.037
.868±.026
.820±.022
.791±.024
.721±.030
.730±.030
.587±.038
.729±.026
.883±.023
.841±.036
.864±.033
.801±.015
.781±.028
.798±.017
.820±.024
.811±.022
.739±.034

EM -DD
.751±.047
.717±.036
.639±.063
.815±.063
.632±.060
.711±.054

5 Conclusion

In this paper, we formalize multi-instance multi-label learning where an example is associated with
multiple instances and multiple labels simultaneously. Although there were some works investi-
gating the ambiguity of alternative input descriptions or alternative output descriptions associated
with an object, this is the ﬁrst work studying both these ambi guities simultaneously. We show that
an M IM L problem can be solved by identifying its equivalence in the traditional supervised learn-
ing framework, using multi-instance learning or multi-label learning as the bridge. The proposed
algorithms, M IM LBOO S T and M IM LSVM , have achieved good performance in the application to
scene classi ﬁcation. An interesting future issue is to deve lop M IM L versions of other popular ma-
chine learning algorithms. Moreover, it remains an open problem that whether M IM L can be tackled
directly, possibly by exploiting the connections between the instances and the labels. It is also in-
teresting to discover the relationship between the instances and labels. By unravelling the mixed
connections, maybe we can get deeper understanding of ambiguity.

Acknowledgments

This work was supported by the National Science Foundation of China (60325207, 60473046).

Pattern

References
[1] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown. Learning multi-label scene classiﬁcation.
Recognition, 37(9):1757–1771, 2004.
[2] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. Technical report, Department
of Computer Science and Information Engineering, National Taiwan University, Taipei, 2001.
[3] Y. Chen and J. Z. Wang. Image categorization by learning and reasoning with regions. Journal of Machine
Learning Research, 5:913–939, 2004.
[4] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P ´erez. Solving the multiple-instance problem with axis-
parallel rectangles. Artiﬁcial Intelligence , 89(1-2):31–71, 1997.
[5] G. A. Edgar. Measure, Topology, and Fractal Geometry. Springer, Berlin, 1990.
[6] R. Jin and Z. Ghahramani. Learning with multiple labels. In S. Becker, S. Thrun, and K. Obermayer,
editors, Advances in Neural Information Processing Systems 15, pages 897–904. MIT Press, Cambridge,
MA, 2003.
[7] O. Maron and A. L. Ratan. Multiple-instance learning for natural scene classiﬁcation. In Proceedings of
the 15th International Conference on Machine Learning, pages 341–349, Madison, MI, 1998.
[8] R. E. Schapire and Y. Singer. BoosTexter: A boosting-based system for text categorization. Machine
Learning, 39(2-3):135–168, 2000.
[9] X. Xu and E. Frank. Logistic regression and boosting for labeled bags of instances. In H. Dai, R. Srikant,
and C. Zhang, editors, Lecture Notes in Artiﬁcial Intelligence 3056 , pages 272–281. Springer, Berlin,
2004.
[10] Q. Zhang and S. A. Goldman. EM-DD: An improved multi-instance learning technique. In T. G. Diet-
terich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14,
pages 1073–1080. MIT Press, Cambridge, MA, 2002.
[11] Z.-H. Zhou and M.-L. Zhang. Solving multi-instance problems with classiﬁer ensemble based on con-
structive clustering. Knowledge and Information Systems, in press.


Large-Scale Sparsi(cid:2)ed Manifold Regularization

Ivor W. Tsang
James T. Kwok
Department of Computer Science and Engineering
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
fivor,jameskg@cse.ust.hk

Abstract

Semi-supervised learning is more powerful than supervised learning by using both
labeled and unlabeled data. In particular, the manifold regularization framework,
together with kernel methods, leads to the Laplacian SVM (LapSVM) that has
demonstrated state-of-the-art performance. However, the LapSVM solution typ-
ically involves kernel expansions of all the labeled and unlabeled examples, and
is slow on testing. Moreover, existing semi-supervised learning methods, includ-
ing the LapSVM, can only handle a small number of unlabeled examples. In this
paper, we integrate manifold regularization with the core vector machine, which
has been used for large-scale supervised and unsupervised learning. By using a
sparsi(cid:2)ed manifold regularizer and formulating as a center-constrained minimum
enclosing ball problem, the proposed method produces sparse solutions with low
time and space complexities. Experimental results show that it is much faster than
the LapSVM, and can handle a million unlabeled examples on a standard PC;
while the LapSVM can only handle several thousand patterns.

1

Introduction

In many real-world applications, collection of labeled data is both time-consuming and expensive.
On the other hand, a large amount of unlabeled data are often readily available. While traditional
supervised learning methods can only learn from the limited amount of labeled data, semi-supervised
learning [2] aims at improving the generalization performance by utilizing both the labeled and
unlabeled data. The label dependencies among patterns are captured by exploiting the intrinsic
geometric structure of the data. The underlying smoothness assumption is that two nearby patterns
in a high-density region should share similar labels [2]. When the data lie on a manifold, it is
common to approximate this manifold by a weighted graph, leading to graph-based semi-supervised
learning methods. However, many of these are designed for transductive learning, and thus cannot
be easily extended to out-of-sample patterns.
Recently, attention is drawn to the development of inductive methods, such as harmonic mixtures
[15] and Nystr ¤om-based methods [3]. In this paper, we focus on the manifold regularization frame-
work proposed in [1]. By de(cid:2)ning a data-dependent reproducing kernel Hilbert space (RKHS),
manifold regularization incorporates an additional regularizer to ensure that the learned function is
smooth on the manifold. Kernel methods, which have been highly successful in supervised learn-
ing, can then be integrated with this RKHS. The resultant Laplacian SVM (LapSVM) demonstrates
state-of-the-art semi-supervised learning performance [10]. However, a de(cid:2)ciency of the LapSVM
is that its solution, unlike that of the SVM, is not sparse and so is much slower on testing.
Moreover, while the original motivation of semi-supervised learning is to utilize the large amount
of unlabeled data available, existing algorithms are only capable of handling a small to moderate
amount of unlabeled data. Recently, attempts have been made to scale up these methods. Sindhwani
et al. [9] speeded up manifold regularization by restraining to linear models, which, however, may

not be (cid:3)exible enough for complicated target functions. Garcke and Griebel [5] proposed to use
discretization with a sparse grid. Though it scales linearly with the sample size, its time complexity
grows exponentially with data dimensionality. As reported in a recent survey [14], most semi-
supervised learning methods can only handle 100 (cid:150) 10,000 unlabeled examples. More recently,
G ¤artner et al. [6] presented a solution in the more restrictive transductive setting. The largest graph
they worked with involve 75,888 labeled and unlabeled examples. Thus, no one has ever been
experimented on massive data sets with, say, one million unlabeled examples.
On the other hand, the Core Vector Machine (CVM) is recently proposed for scaling up kernel meth-
ods in both supervised (including classi(cid:2)cation [12] and regression [13]) and unsupervised learning
(e.g., novelty detection). Its main idea is to formulate the learning problem as a minimum enclosing
ball (MEB) problem in computational geometry, and then use an (1 + (cid:15))-approximation algorithm
to obtain a close-to-optimal solution ef(cid:2)ciently. Given m samples, the CVM has an asymptotic time
complexity that is only linear in m and a space complexity that is even independent of m for a (cid:2)xed
(cid:15). Experimental results on real world data sets with millions of patterns demonstrated that the CVM
is much faster than existing SVM implementations and can handle much larger data sets.
In this paper, we extend the CVM to semi-supervised learning. To restore sparsity of the LapSVM
solution, we (cid:2)rst introduce a sparsi(cid:2)ed manifold regularizer based on the (cid:15)-insensitive loss. Then, we
incorporate manifold regularization into the CVM. It turns out that the resultant QP can be casted as
a center-constrained MEB problem introduced in [13]. The rest of this paper is organized as follows.
In Section 2, we (cid:2)rst give a brief review on manifold regularization. Section 3 then describes the
proposed algorithm for semi-supervised classi(cid:2)cation and regression. Experimental results on very
large data sets are presented in Section 4, and the last section gives some concluding remarks.

2 Manifold Regularization

min
f 2Hk

1
m

‘(xi ; yi ; f (xi )) + (cid:21)(cid:10)(kf kHk ):

i=1 with input xi 2 X and output yi 2 R. The regularized risk
Given a training set f(xi ; yi )gm
functional is the sum of the empirical risk (corresponding to a loss function ‘) and a regularizer (cid:10).
Given a kernel k and its RKHS Hk , we minimize the regularized risk over function f in Hk :
m
Xi=1
Here, k (cid:1) kHk denotes the RKHS norm and (cid:21) > 0 is a regularization parameter. By the representer
theorem, the minimizer f admits the representation f (x) = Pm
i=1 (cid:11)ik(xi ; x), where (cid:11)i 2 R.
Therefore, the problem is reduced to the optimization over the (cid:2)nite-dimensional space of (cid:11) i ’s.
In semi-supervised learning, we have both labeled examples f(xi ; yi )gm
i=1 and unlabeled examples
fxi gm+n
i=m+1 . Manifold regularization uses an additional regularizer kf k2
I to ensure that the function
f is smooth on the intrinsic structure of the input. The objective function in (1) is then modi(cid:2)ed to:
m
Xi=1
where (cid:21)I is another tradeoff parameter. It can be shown that the minimizer f is of the form f (x) =
Pm
i=1 (cid:11)ik(xi ; x) + RM (cid:11)(x0 )k(x; x0 )dPX (x0 ), where M is the support of the marginal distribution
PX of X [1].
In practice, we do not have access to PX . Now, assume that the support M of PX is a compact sub-
manifold, and take kf k2
I = RM hrf ; rf i where r is the gradient of f . It is common to approximate
this manifold by a weighted graph de(cid:2)ned on all the labeled and unlabeled data, as G = (V ; E ) with
V and E being the sets of vertices and edges respectively. Denote the weight function w and degree
I is approximated as1
d(u) = Pv(cid:24)u w(u; v). Here, v (cid:24) u means that u; v are adjacent. Then, kf k2
2
I = Xe2E (cid:12)(cid:12)(cid:12)(cid:12)pw(ue ; ve ) (cid:18) f (xue )
s(ve ) (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
f (xve )
kf k2
(cid:0)
;
s(ue )
1When the set of labeled and unlabeled data is small, a function that is smooth on this small set may not be
interesting. However, this is not an issue here as our focus is on massive data sets.

‘(xi ; yi ; f (xi )) + (cid:21)(cid:10)(kf kHk ) + (cid:21)I kf k2
I ;

1
m

(1)

(2)

(3)

where ue and ve are vertices of the edge e, and s(u) = pd(u) when the normalized graph Laplacian
is used, and s(u) = 1 with the unnormalized one. As shown in [1], the minimizer of (2) becomes
f (x) = Pm+n
i=1 (cid:11)ik(xi ; x), which depends on both labeled and unlabeled examples.
2.1 Laplacian SVM
Using the hinge loss ‘(xi ; yi ; f (xi )) = max(0; 1 (cid:0) yi f (xi )) in (2), we obtain the Laplacian
Its training involves two steps. First, solve the quadratic program (QP)
SVM (LapSVM) [1].
max (cid:12) 01 (cid:0) 1
2 (cid:12) 0Q(cid:12) : (cid:12) 0y = 0; 0 (cid:20) (cid:12) (cid:20) 1
m 1 to obtain the (cid:12)(cid:3) . Here, (cid:12) = [(cid:12)1 ; : : : ; (cid:12)m ]0 ; 1 =
[1; : : : ; 1]0 ; Qm(cid:2)m = YJK(2(cid:21)I + 2(cid:21)I LK)(cid:0)1J0Y , Ym(cid:2)m is the diagonal matrix with Yii = yi ,
K(m+n)(cid:2)(m+n) is the kernel matrix over both the labeled and unlabeled data, L(m+n)(cid:2)(m+n) is the
graph Laplacian, and Jm(cid:2)(m+n) with Jij = 1 if i = j and xi is a labeled example, and Jij = 0
otherwise. The optimal (cid:11) = [(cid:11)1 ; : : : ; (cid:11)m+n ]0 solution is then obtained by solving the linear system:
(cid:11)(cid:3) = (2(cid:21)I + 2(cid:21)I LK)(cid:0)1J0Y(cid:12)(cid:3) . Note that the matrix 2(cid:21)I + 2(cid:21)I LK is of size (m + n) (cid:2) (m + n),
and so its inversion can be very expensive when n is large. Moreover, unlike the standard SVM, the
(cid:11)(cid:3) obtained is not sparse and so evaluation of f (x) is slow.

3 Proposed Algorithm

3.1 Sparsi(cid:2)ed Manifold Regularizer

(4)

(cid:0)

(cid:0)

2

(cid:22)"

;

To restore sparsity of the LapSVM solution, we replace the square function in the manifold regular-
izer (3) by the (cid:15)-insensitive loss function2 , as
I = Xe2E (cid:12)(cid:12)(cid:12)(cid:12)pw(ue ; ve ) (cid:18) f (xue )
s(ve ) (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
f (xve )
kf k2
s(ue )
where jz j (cid:22)" = 0 if jz j (cid:20) (cid:22)"; and jz j (cid:0) (cid:22)" otherwise. Obviously, it reduces to (3) when (cid:22)" = 0. As will
be shown in Section 3.3, the (cid:11) solution obtained will be sparse. Substituting (4) into (2), we have:
f 2Hk( 1
(cid:22)")+(cid:21)(cid:10)(kf kHk ):
m
2
‘(xi ; yi ; f (xi ))+(cid:21)IXe2E (cid:12)(cid:12)(cid:12)(cid:12)pw(ue ; ve )(cid:18)f (xue )
s(ve ) (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
f (xve )
Xi=1
min
m
s(ue )
By treating the terms inside the braces as the (cid:147)loss function(cid:148), this can be regarded as regularized risk
minimization and, using the standard representer theorem, the minimizer f then admits the form
f (x) = Pm+n
i=1 (cid:11)ik(xi ; x), same as that of the original manifold regularization.
I = Pe2E jw0  e + b(cid:28)e j2
(cid:22)" , where
Moreover, putting f (x) = w 0’(x) + b into (4), we obtain kf k2
  e = pw(ue ; ve ) (cid:16) ’(xue )
s(ve ) (cid:17), and (cid:28)e = pw(ue ; ve ) (cid:16) 1
s(ve ) (cid:17). The primal of the
s(ue ) (cid:0) ’(xve )
s(ue ) (cid:0) 1
LapSVM can then be formulated as:
m
C
C (cid:18)
2 )
jE j(cid:22) Xe2E
Xi=1
e + (cid:16) (cid:3)
min kwk2 + b2 +
((cid:16) 2
(cid:24) 2
(5)
i + 2C (cid:22)" +
e
m(cid:22)
yi (w0’(xi ) + b) (cid:21) 1 (cid:0) (cid:22)" (cid:0) (cid:24)i ;
(6)
i = 1; : : : ; m;
(cid:0)(w0  e + b(cid:28)e ) (cid:20) (cid:22)" + (cid:16)e ; w0  e + b(cid:28)e (cid:20) (cid:22)" + (cid:16) (cid:3)
(7)
e ;
e 2 E :
e are slack
Here, jE j is the number of edges in the graph, (cid:24)i is the slack variable for the error, (cid:16)e ; (cid:16) (cid:3)
variables for edge e, and C; (cid:22); (cid:18) are user-de(cid:2)ned parameters. As in previous CVM formulations
[12, 13], the bias b is penalized and the two-norm errors ((cid:24) 2
ij and (cid:16) (cid:3)2
ij ) are used. Moreover, the
i ; (cid:16) 2
constraints (cid:24)i ; (cid:16)ij ; (cid:16) (cid:3)
ij ; (cid:22)" (cid:21) 0 are automatically satis(cid:2)ed. When (cid:22)" = 0, (5) reduces to the original
LapSVM (using two-norm errors). When (cid:18) is also zero, it becomes the Lagrangian SVM.
The dual can be easily obtained as the following QP:
2
C
2To avoid confusion with the (cid:15) in the (1 + (cid:15))-approximation, we add a bar to the " here.

10 00 00 ]0 (cid:0) [(cid:12) 0 (cid:13) 0 (cid:13) (cid:3) 0 ] ~K[(cid:12) 0 (cid:13) 0 (cid:13) (cid:3) 0 ]0

[(cid:12) 0 (cid:13) 0 (cid:13) (cid:3) 0 ]1 = 1; (cid:12) ; (cid:13) ; (cid:13) (cid:3) (cid:21) 0;

max [(cid:12) 0 (cid:13) 0 (cid:13) (cid:3) 0 ][

s.t.

:

(8)

is the transformed (cid:147)kernel matrix(cid:148). Here,

jE j ]0 are the dual variables, and ~K =
where (cid:12) = [(cid:12)1 ; : : : ; (cid:12)m ]0 ; (cid:13) = [(cid:13)1 ; : : : ; (cid:13)jE j ]0 ; (cid:13) (cid:3) = [(cid:13) (cid:3)
1 ; : : : ; (cid:13) (cid:3)
C I) (cid:12) yy0
(K‘ + 110 + (cid:22)m
C (cid:18) I 3
2
(cid:0)V
V
U + jE j(cid:22)
V0
(cid:0)U
C (cid:18) I
5
4
U + jE j(cid:22)
(cid:0)V0
(cid:0)U
K‘ is the kernel matrix de(cid:2)ned using kernel k on the m labeled examples, U jE j(cid:2)jE j = [  0
e f +
(cid:28)e (cid:28)f ], and Vm(cid:2)jE j = [yi’(xi )0  e + (cid:28)e ]. Note that while each entry of the matrix Q in LapSVM
(Section 2.1) requires O((m + n)2 ) kernel k(xi ; xj ) evaluations, each entry in ~K here takes only
O(1) kernel evaluations. This is particularly favorable to decomposition methods such as SMO as
most of the CPU computations are typically dominated by kernel evaluations.
Moreover, it can be shown that (cid:22) is a parameter that controls the size of (cid:22)", analogous to the
(cid:23) parameter in (cid:23) -SVR. Hence, only (cid:22), but not (cid:22)", appears in (8). Moreover, the primal vari-
ables can be easily recovered from the dual variables by the KKT conditions.
In particular,
w = C (cid:0)Pm
e )  e (cid:1) and b = C (cid:0)Pm
e )(cid:28)e (cid:1).
i=1 (cid:12)i yi’(xi ) + Pe2E ((cid:13)e (cid:0) (cid:13) (cid:3)
i=1 (cid:12)i yi + Pe2E ((cid:13)e (cid:0) (cid:13) (cid:3)
Subsequently, the decision function f (x) = w 0’(x) + b is a linear combination of k(xi ; x)’s de-
(cid:2)ned on both the labeled and unlabeled examples, as in standard manifold regularization.

3.2 Transforming to a MEB Problem

We now show that CVM can be used for solving the possibly very large QP in (8). In particular, we
will transform this QP to the dual of a center-constrained MEB problem [13], which is of the form:

max (cid:11)0 (diag(K) + (cid:1) (cid:0) (cid:17)1) (cid:0) (cid:11)0K(cid:11) : (cid:11) (cid:21) 0; (cid:11)01 = 1;
(9)
for some 0 (cid:20) (cid:1) 2 Rm and (cid:17) 2 R. From the variables in (8), de(cid:2)ne ~(cid:11) = (cid:2)(cid:12) 0 (cid:13) 0 (cid:13) (cid:3) 0 (cid:3)0 and
(cid:1) = (cid:0)diag( ~K) + (cid:17)1 + 2
C [10 00 0]0 s.t. (cid:1) (cid:21) 0 for some suf(cid:2)ciently large (cid:17) . (8) can then be written
as max ~(cid:11)0 (diag( ~K) + (cid:1) (cid:0) (cid:17)1) (cid:0) ~(cid:11)0 ~K ~(cid:11) : ~(cid:11) (cid:21) 0; ~(cid:11)01 = 1, which is of the form in (9).
The above formulation can be easily extended to the regression case, with the pattern output changed
from (cid:6)1 to yi 2 R, and the hinge loss replaced by the (cid:15)-insensitive loss. Converting the resultant
QP to the form in (9) is also straightforward.

3.3 Sparsity

In Section 3.3.1, we (cid:2)rst explain why a sparse solution can be obtained by using the KKT conditions.
Alternatively, by building on [7], we show in Section 3.3.2 that the (cid:15)-insensitive loss achieves a
similar effect as the ‘1 penalty in LASSO [11], which is known to produce sparse approximation.

3.3.1 KKT Perspective

Basically, this follows from the standard argument as for sparse solutions with the (cid:15)-insensitive loss
in SVR. From the KKT condition associated with (6): (cid:12)i (yi (w0’(xi ) + b) (cid:0) 1 + (cid:22)" + (cid:24)i ) = 0. As
for the SVM, most patterns are expected to lie outside the margin (i.e. yi (w0’(xi ) + b) > 1 (cid:0) (cid:22)")
and so most (cid:12)i ’s are zero. Similarly, manifold regularization (cid:2)nds a f that is locally smooth. Hence,
from the de(cid:2)nition of   e and (cid:28)e , many values of (w 0  e + b(cid:28)e )’s will be inside the (cid:22)"-tube. Using the
e ’s are zero. As f (x) is a linear
KKT conditions associated with (7), the corresponding (cid:13)e ’s and (cid:13) (cid:3)
e (Section 3.1), f is thus sparse.
combination of the k(xi ; x)’s weighted by (cid:12)i and (cid:13)e (cid:0) (cid:13) (cid:3)

3.3.2 LASSO Perspective

Our exposition will be along the line pioneered by Girosi [7], who established a connection
between the (cid:15)-insensitive loss in SVR and sparse approximation. Given a predictor f (x) =
Pm
i=1 (cid:11)ik(xi ; x) = K(cid:11), we consider minimizing the error between f = [f (x1 ); : : : f (xm )]0 and
y = [y1 ; : : : ; ym ]0 . While sparse approximation techniques such as basis pursuit typically use the
L2 norm for the error, Girosi argued that the norm of the RKHS Hk is a better measure of smooth-
ness. However, the RKHS norm operates on functions, while here we have vectors f and y w.r.t.
x1 ; : : : ; xm . Hence, we will use the kernel PCA map with ky (cid:0) f k2
K (cid:17) (y (cid:0) f )K(cid:0)1 (y (cid:0) f ).

First, consider the simpler case where the manifold regularizer is replaced by a simple regularizer
2 . As in LASSO, we also add a ‘1 penalty on (cid:11). The optimization problem is formulated as:
k(cid:11)k2
(cid:22)m
min ky (cid:0) f k2
(cid:11)0(cid:11) : k(cid:11)k1 = C;
K +
C

(10)

min kwk2 +

where C and (cid:22) are constants. As in [7], we decompose (cid:11) as (cid:12) (cid:0)(cid:12) (cid:3) , where (cid:12) ; (cid:12)(cid:3) (cid:21) 0 and (cid:12)i(cid:12) (cid:3)
i = 0.
Then, (10) can be rewritten as:
max [(cid:12) 0 (cid:12)(cid:3) 0 ][2y0 (cid:0) 2y0 ]0 (cid:0) [(cid:12) 0 (cid:12)(cid:3) 0 ] ~K[(cid:12) 0 (cid:12)(cid:3) 0 ]0
: (cid:12) ; (cid:12)(cid:3) (cid:21) 0; (cid:12) 01 + (cid:12)(cid:3) 0
(11)
1 = C;
where3 ~K = (cid:20) K + (cid:22)m
C I (cid:21). On the other hand, consider the following variant of SVR
(cid:0)K
C I
K + (cid:22)m
(cid:0)K
using the (cid:15)-insensitive loss:
m
C
Xi=1
m(cid:22)
It can be shown that its dual is identical to (11), with (cid:12) ; (cid:12) (cid:3) as dual variables. Moreover, the LASSO
penalty (i.e., the equality constraint in (11)) is induced from the (cid:22)" in (12). Hence, the (cid:15)-insensitive
loss in SVR achieves a similar effect as using the error ky (cid:0) f k2
K and the LASSO penalty.
We now add back the manifold regularizer. The derivation is similar, though more involved, and so
details are skipped. As above, the key steps are on replacing the ‘2 norm by the kernel PCA map,
and adding a ‘1 penalty on the variables. It can then be shown that sparsi(cid:2)ed manifold regularizer
(based on the (cid:15)-insensitive loss) can again be recovered by using the LASSO penalty.

((cid:24) 2
i + (cid:24) (cid:3)2
i ) + 2C (cid:22)" : yi (cid:0) w0’(xi ) (cid:20) (cid:22)" + (cid:24)i ; w0’(xi ) (cid:0) yi (cid:20) (cid:22)" + (cid:24) (cid:3)
i : (12)

3.4 Complexities

As the proposed algorithm is an extension of the CVM, its properties are analogous to those in [12].
For example, its approximation ratio is (1 + (cid:15))2 , and so the approximate solution obtained is very
close to the exact optimal solution. As for the computational complexities, it can be shown that the
SLapCVM only takes O(1=(cid:15)8 ) time and O(1=(cid:15)2 ) space when probabilistic speedup is used. (Here,
we ignore O(m + jE j) space required for storing the m training patterns and 2jE j edge constraints, as
these may be stored outside the core memory.) They are thus independent of the numbers of labeled
and unlabeled examples for a (cid:2)xed (cid:15). In contrary, LapSVM involves an expensive matrix inversion
for K(m+n)(cid:2)(m+n) and requires O((m + n)3 ) time and O((m + n)2 ) space.

3.5 Remarks

The reduced SVM [8] has been used to scale up the standard SVM. Hence, another natural alternative
is to extend it for the LapSVM. This (cid:147)reduced LapSVM(cid:148) solves a smaller optimization problem that
involves a random r(cid:2)(m+n) rectangular subset of the kernel matrix, where the r patterns are chosen
from both the labeled and unlabeled data. It can be easily shown that it requires O((m + n)2 r) time
and O((m + n)r) space. Experimental comparisons based on this will be made in Section 4.
Note that the CVM [12] is in many aspects similar to the column generation technique [4] commonly
used in large-scale linear or integer programs. Both start with only a small number of nonzero
variables, and the restricted master problem in column generation corresponds to the inner QP that
is solved at each CVM iteration. Moreover, both can be regarded as primal methods that maintain
primal4 feasibility and work towards dual feasibility. Also, as is typical in column generation, the
dual variable whose KKT condition is most violated is added at each iteration. The key difference 5 ,
3 For simplicity, here we have only considered the case where f does not have a bias. In the presence of a
bias, it can be easily shown that K (in the expression of ~K) has to be replaced by K + 110 .
4By convention, column generation takes the optimization problem to be solved as the primal. Hence, in
this section, we also regard the QP to be solved as CVM’s primal, and the MEB problem as its dual. Note that
each dual variable then corresponds to a training pattern.
5Another difference is that an entire column is added at each iteration of column generation. However, in
CVM, the dual variable added is just a pattern and the extra space required for the QP is much smaller. Besides,
there are other implementation tricks (such as probabilistic speedup) that further improves the speed of CVM.

however, is that CVM exploits the (cid:147)approximateness(cid:148) as in other approximation algorithms. Instead
of requiring the dual solution to be strictly feasible, CVM only requires it to be feasible within a
factor of (1 + (cid:15)). This, together with the fact that its dual is a MEB problem, allows its number of
iterations for convergence to be bounded and thus the total time and space complexities guaranteed.
On the other hand, we are not aware of any similar results for column generation.
By regarding the CVM as the approximation algorithm counterpart of column generation, this sug-
gests that the CVM can also be used in the same way as column generation in speeding up other
optimization problems. For example, the CVM can also be used for SVM training with other loss
functions (e.g. 1-norm error). However, as the dual may no longer be a MEB problem, the downside
is that its convergence bound and complexity results in Section 3.4 may no longer be available.

4 Experiments

In this section, we perform experiments on some massive data sets 6 (Table 1). The graph (for the
manifold) is constructed by using the 6 nearest neighbors of each pattern, and the weight w(ue ; ve )
jE j Pe2E kxeu (cid:0) xev k2 . For simplicity,
in (3) is de(cid:2)ned as exp((cid:0)kxue (cid:0) xve k2 =(cid:12)g ), where (cid:12)g = 1
we use the unnormalized Laplacian and so all s((cid:1))’s in (3) are 1. The value of m(cid:22) in (5) is always
(cid:2)xed at 1, and the other parameters are tuned by a small validation set. Unless otherwise speci(cid:2)ed,
m Pm
we use the Gaussian kernel exp((cid:0)kx (cid:0) zk2 =(cid:12) ), with (cid:12) = 1
i=1 kxi (cid:0) (cid:22)xk2 . For comparison,
we also run the LapSVM7 and another LapSVM implementation based on the reduced SVM [8]
(Section 3.5). All the experiments are performed on a 3.2GHz Pentium(cid:150)4 PC with 1GB RAM.
Table 1: A summary of the data sets used.
#training patns
unlabeled
labeled
500,000
1
1
500,000
144,473
1
121,604
1
408,067
5
5
481,909

#test patns
2,500
2,500
43,439
31,944
472
23,573

class
+
(cid:0)
+
(cid:0)
+
(cid:0)

data set
two-moons

extended MIT face

extended USPS

#attrib
2

676

361

4.1 Two-Moons Data Set

We (cid:2)rst perform experiments on the popular two-moons data set, and use one labeled example for
each class (Figure 1(a)). To better illustrate the scaling behavior, we vary the number of unlabeled
patterns used for training (from 1; 000 up to a maximum of 1 million). Following [1], the width of
the Gaussian kernel is set to (cid:12) = 0:25. For the reduced LapSVM implementation, we (cid:2)x r = 200.

SLapCVM
LapSVM
Reduced LapSVM

103

102

101

100

)
s
d
n
o
c
e
s
 
n
i
(
 
e
m
i
t
 
U
P
C

s
n
o
i
s
n
a
p
x
e
 
l
e
n
r
e
k
 
f
o
 
r
e
b
m
u
n

104

103

SLapCVM
core−set Size
LapSVM
Reduced LapSVM

(a) Data distribution.

102
104
105
103
106
number of unlabeled points
(b) Typical decision boundary
(d) #kernel
expan-
obtained by SLapCVM.
sions.
Figure 1: Results on the two-moons data set (some abscissas and ordinates are in log scale). The
two labeled examples are labeled in red in Figure 1(a).

10−1
104
105
103
number of unlabeled points
(c) CPU time.

106

Results are shown in Figure 1. Both the LapSVM and SLapCVM always attain 100% accuracy on
the test set, even with only two labeled examples (Figure 1(b)). However, SLapCVM is faster than
LapSVM (Figure 1(c)). Moreover, as mentioned in Section 2.1, the LapSVM solution is non-sparse

6Both the USPS and MIT face data sets are downloaded from http://www.cs.ust.hk/(cid:24)ivor/cvm.html.
7 http://manifold.cs.uchicago.edu/manifold regularization/.

and all the labeled and unlabeled examples are involved in the solution (Figure 1(d))). On the other
hand, SLapCVM uses only a small fraction of the examples. As can be seen from Figures 1(c)
and 1(d), both the time and space required by the SLapCVM are almost constant, even when the
unlabeled data set gets very large. The reduced LapSVM, though also fast, is slightly inferior to
both the SLapCVM and LapSVM. Moreover, note that both the standard and reduced LapSVMs
cannot be run on the full data set on our PC because of their large memory requirements.

4.2 Extended USPS Data Set

The second experiment is performed on the USPS data from [12]. One labeled example is randomly
sampled from each class for training. To achieve comparable accuracy, we use r = 2; 000 for the
reduced LapSVM. For comparison, we also train a standard SVM with the two labeled examples.
Results are shown in Figure 2. As can be seen, the SLapCVM is again faster (Figures 2(a)) and
produces a sparser solution than LapSVM (Figure 2(b)). For the SLapCVM, both the time required
and number of kernel expansions involved grow only sublinearly with the number of unlabeled
examples. Figure 2(c) demonstrates that semi-supervised learning (using either the LapSVMs or
SLapCVM) can have much better generalization performance than supervised learning using the
labeled examples only. Note that although the use of the 2-norm error in SLapCVM could in theory
be less robust than the use of the 1-norm error in LapSVM, the SLapCVM solution is indeed always
more accurate than that of LapSVM. On the other hand, the reduced LapSVM has comparable speed
with the SLapCVM, but its performance is inferior and cannot handle large data sets.

105

104

103

102

101

)
s
d
n
o
c
e
s
 
n
i
(
 
e
m
i
t
 
U
P
C

SLapCVM
LapSVM
Reduced LapSVM

105

104

103

s
n
o
i
s
n
a
p
x
e
 
l
e
n
r
e
k
 
f
o
 
r
e
b
m
u
n

SLapCVM
core−set Size
LapSVM
Reduced LapSVM

80

70

60

50

40

30

20

10

)
%
 
n
i
(
 
e
t
a
r
 
r
o
r
r
e

SLapCVM
LapSVM
Reduced LapSVM
SVM (#labeled = 2)

100
103

102
105
104
104
105
103
104
105
number of unlabeled points
number of unlabeled points
number of unlabeled points
(a) CPU time.
(b) #kernel expansions.
(c) Test error.
Figure 2: Results on the extended USPS data set (some abscissas and ordinates are in log scale).

106

106

106

0
103

4.3 Extended MIT Face Data Set

In this section, we perform face detection using the extended MIT face database in [12]. Five labeled
example are randomly sampled from each class and used in training. Because of the imbalanced
nature of the test set (Table 1), the classi(cid:2)cation error is inappropriate for performance evaluation
here. Instead, we will use the area under the ROC curve (AUC) and the balanced loss 1 (cid:0) (TP +
TN)=2, where TP and TN are the true positive and negative rates respectively. Here, faces are treated
as positives while non-faces as negatives. For the reduced LapSVM, we again use r = 2; 000. For
comparison, we also train two SVMs: one uses the 10 labeled examples only while the other uses
all the labeled examples (a total of 889,986) in the original training set of [12].
Figure 3 shows the results. Again, the SLapCVM is faster and produces a sparser solution than
LapSVM. Note that the SLapCVM, using only 10 labeled examples, can attain comparable AUC and
even better balanced loss than the SVM trained on the original, massive training set (Figures 3(c) and
3(d)). This clearly demonstrates the usefulness of semi-supervised learning when a large amount of
unlabeled data can be utilized. On the other hand, note that the LapSVM again cannot be run with
more than 3,000 unlabeled examples on our PC because of its high space requirement. The reduced
LapSVM performs very poorly here, possibly because this data set is highly imbalanced.

5 Conclusion

In this paper, we addressed two issues associated with the Laplacian SVM: 1) How to obtain a sparse
solution for fast testing? 2) How to handle data sets with millions of unlabeled examples? For the

105

104

103

102

101

)
s
d
n
o
c
e
s
 
n
i
(
 
e
m
i
t
 
U
P
C

SLapCVM
LapSVM
Reduced LapSVM

105

104

103

s
n
o
i
s
n
a
p
x
e
 
l
e
n
r
e
k
 
f
o
 
r
e
b
m
u
n

SLapCVM
core−set Size
LapSVM
Reduced LapSVM

50

45

40

35

30

25

20

15

)
%
 
n
i
(
 
s
s
o
l
 
d
e
c
n
a
l
a
b

SLapCVM
LapSVM
Reduced LapSVM
SVM (#labeled = 10)
CVM (w/ all training labels)

1.1

1.05

1

0.95

C
U
A

0.9

0.85

0.8

0.75

SLapCVM
LapSVM
Reduced LapSVM
SVM (#labeled = 10)
CVM (w/ all training labels)

105

105

105

100
103

102
104
104
104
103
104
number of unlabeled points
number of unlabeled points
number of unlabeled points
number of unlabeled points
(a) CPU time.
(b) #kernel expansions.
(c) Balanced loss.
(d) AUC.
Figure 3: Results on the extended MIT face data (some abscissas and ordinates are in log scale).

0.7
103

105

10
103

(cid:2)rst issue, we introduce a sparsi(cid:2)ed manifold regularizer based on the (cid:15)-insensitive loss. For the
second issue, we integrate manifold regularization with the CVM. The resultant algorithm has low
time and space complexities. Moreover, by avoiding the underlying matrix inversion in the origi-
nal LapSVM, a sparse solution can also be recovered. Experiments on a number of massive data
sets show that the SLapCVM is much faster than the LapSVM. Moreover, while the LapSVM can
only handle several thousand unlabeled examples, the SLapCVM can handle one million unlabeled
examples on the same machine. On one data set, this produces comparable or even better perfor-
mance than the (supervised) CVM trained on 900K labeled examples. This clearly demonstrates the
usefulness of semi-supervised learning when a large amount of unlabeled data can be utilized.

References
[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning
from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399(cid:150)2434, 2006.
[2] O. Chapelle, B. Sch ¤olkopf, and A. Zien. Semi-Supervised Learning. MIT Press, Cambridge, MA, USA,
2006.
[3] O. Delalleau, Y. Bengio, and N. L. Roux. Ef(cid:2)cient non-parametric function induction in semi-supervised
learning.
In Proceedings of the Tenth International Workshop on Arti(cid:2)cial Intelligence and Statistics,
Barbados, January 2005.
[4] G. Desaulniers, J. Desrosiers, and M.M. Solomon. Column Generation. Springer, 2005.
[5] J. Garcke and M. Griebel. Semi-supervised learning with sparse grids.
In Proceedings of the ICML
Workshop on Learning with Partially Classi(cid:2)ed Training Data, Bonn, Germany, August 2005.
[6] T. G ¤artner, Q.V. Le, S. Burton, A. Smola, and S.V.N. Vishwanathan. Large-scale multiclass transduction.
In Y. Weiss, B. Sch ¤olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18.
MIT Press, Cambridge, MA, 2006.
[7] F. Girosi. An equivalence between sparse approximation and support vector machines. Neural Computa-
tion, 10(6):1455(cid:150)1480, 1998.
[8] Y.-J. Lee and O.L. Mangasarian. RSVM: Reduced support vector machines. In Proceeding of the First
SIAM International Conference on Data Mining, 2001.
[9] V. Sindhwani, M. Belkin, and P. Niyogi. The geometric basis of semi-supervised learning.
supervised Learning. MIT Press, 2005.
[10] V. Sindhwani, P. Niyogi, and M. Belkin. Beyond the point cloud: from transductive to semi-supervised
learning. In Proceedings of the Twenty-Second International Conference on Machine Learning, pages
825(cid:150)832, Bonn, Germany, August 2005.
[11] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society:
Series B, 58:267(cid:150)288, 1996.
[12] I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core vector machines: Fast SVM training on very large data
sets. Journal of Machine Learning Research, 6:363(cid:150)392, 2005.
[13] I. W. Tsang, J. T. Kwok, and K. T. Lai. Core vector regression for very large regression problems. In
Proceedings of the Twenty-Second International Conference on Machine Learning, pages 913(cid:150)920, Bonn,
Germany, August 2005.
[14] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, Department of Computer
Sciences, University of Wisconsin - Madison, 2005.
[15] X. Zhu and J. Lafferty. Harmonic mixtures: Combining mixture models and graph-based methods. In Pro-
ceedings of the Twenty-Second International Conference on Machine Learning, Bonn, Germany, August
2005.

In Semi-


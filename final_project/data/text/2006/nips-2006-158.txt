Convex Repeated Games and Fenchel Duality

Shai Shalev-Shwartz1 and Yoram Singer1,2
1 School of Computer Sci. & Eng., The Hebrew University, Jerusalem 91904, Israel
2 Google Inc. 1600 Amphitheater Parkway, Mountain View, CA 94043, USA

Abstract

We describe an algorithmic framework for an abstract game which we term a con-
vex repeated game. We show that various online learning and boosting algorithms
can be all derived as special cases of our algorithmic framework. This uniﬁed view
explains the properties of existing algorithms and also enables us to derive several
new interesting algorithms. Our algorithmic framework stems from a connection
that we build between the notions of regret in game theory and weak duality in
convex optimization.

1

Introduction and Problem Setting

Several problems arising in machine learning can be modeled as a convex repeated game. Convex
repeated games are closely related to online convex programming (see [19, 9] and the discussion in
the last section). A convex repeated game is a two players game that is performed in a sequence of
consecutive rounds. On round t of the repeated game, the ﬁrst player chooses a vector wt from a
player. The goal of the ﬁrst player is to minimize its cumulative loss, P
convex set S . Next, the second player responds with a convex function gt : S → R. Finally, the
ﬁrst player suffers an instantaneous loss gt (wt ). We study the game from the viewpoint of the ﬁrst
t gt (wt ).
To motivate this rather abstract setting let us ﬁrst cast the more familiar setting of online learning as a
convex repeated game. Online learning is performed in a sequence of consecutive rounds. On round
t, the learner ﬁrst receives a question, cast as a vector xt , and is required to provide an answer for this
question. For example, xt can be an encoding of an email message and the question is whether the
email is spam or not. The prediction of the learner is performed based on an hypothesis, ht : X → Y ,
where X is the set of questions and Y is the set of possible answers. In the aforementioned example,
Y would be {+1, −1} where +1 stands for a spam email and −1 stands for a benign one. After
predicting an answer, the learner receives the correct answer for the question, denoted yt , and suffers
loss according to a loss function ‘(ht , (xt , yt )). In most cases, the hypotheses used for prediction
come from a parameterized set of hypotheses, H = {hw : w ∈ S }. For example, the set of
linear classiﬁers, which is used for answering yes/no questions, is deﬁned as H = {hw (x) =
sign(hw, xi) : w ∈ Rn }. Thus, rather than saying that on round t the learner chooses a hypothesis,
we can say that the learner chooses a vector wt and its hypothesis is hwt . Next, we note that once
the environment chooses a question-answer pair (xt , yt ), the loss function becomes a function over
the hypotheses space or equivalently over the set of parameter vectors S . We can therefore redeﬁne
the online learning process as follows. On round t, the learner chooses a vector wt ∈ S , which
deﬁnes a hypothesis hwt to be used for prediction. Then, the environment chooses a question-
answer pair (xt , yt ), which induces the following loss function over the set of parameter vectors,
gt (w) = ‘(hw , (xt , yt )). Finally, the learner suffers the loss gt (wt ) = ‘(hwt , (xt , yt )). We have
therefore described the process of online learning as a convex repeated game.
In this paper we assess the performance of the ﬁrst player using the notion of regret. Given a number
of rounds T and a ﬁxed vector u ∈ S , we deﬁne the regret of the ﬁrst player as the excess loss for

gt (u) .

1
T

1
T

,

(1)

∀u ∈ S,

gt (wt ) ≤ inf
u∈S

TX
TX
not consistently playing the vector u,
1
gt (wt ) − 1
T
T
t=1
t=1
Our main result is an algorithmic framework for the ﬁrst player which guarantees low regret with
respect to any vector u ∈ S . Speciﬁcally, we derive regret bounds that take the following form
TX
TX
gt (u) ≤ f (u) + L√
gt (wt ) − 1
1
T
T
T
t=1
t=1
where f : S → R and L ∈ R+ . Informally, the function f measures the “complexity” of vectors in
S and the scalar L is related to some generalized Lipschitz property of the functions g1 , . . . , gT . We
defer the exact requirements we impose on f and L to later sections.
Our algorithmic framework emerges from a representation of the regret bound given in Eq. (1) using
TX
TX
an optimization problem. Speciﬁcally, we rewrite Eq. (1) as follows
gt (u) + f (u) + L√
T
t=1
t=1
That is, the average loss of the ﬁrst player should be bounded above by the minimum value of an
optimization problem in which we jointly minimize the average loss of u and the “complexity” of u
as measured by the function f . Note that the optimization problem on the right-hand side of Eq. (2)
can only be solved in hindsight after observing the entire sequence of loss functions. Nevertheless,
writing the regret bound as in Eq. (2) implies that the average loss of the ﬁrst player forms a lower
bound for a minimization problem.
The notion of duality, commonly used in convex optimization theory, plays an important role in
obtaining lower bounds for the minimal value of a minimization problem (see for example [14]). By
generalizing the notion of Fenchel duality, we are able to derive a dual optimization problem, which
can be optimized incrementally, as the game progresses. In order to derive explicit quantitative regret
bounds we make an immediate use of the fact that dual objective lower bounds the primal objective.
We therefore reduce the process of playing convex repeated games to the task of incrementally
increasing the dual objective function. The amount by which the dual increases serves as a new and
natural notion of progress. By doing so we are able to tie the primal objective value, the average loss
of the ﬁrst player, and the increase in the dual.
The rest of this paper is organized as follows. In Sec. 2 we establish our notation and point to a
few mathematical tools that we use throughout the paper. Our main tool for deriving algorithms
for playing convex repeated games is a generalization of Fenchel duality, described in Sec. 3. Our
algorithmic framework is given in Sec. 4 and analyzed in Sec. 5. The generality of our framework
allows us to utilize it in different problems arising in machine learning. Speciﬁcally, in Sec. 6
we underscore the applicability of our framework for online learning and in Sec. 7 we outline and
analyze boosting algorithms based on our framework. We conclude with a discussion and point to
related work in Sec. 8. Due to the lack of space, some of the details are omitted from the paper and
can be found in [16].

.

(2)

2 Mathematical Background

We denote scalars with lower case letters (e.g. x and w), and vectors with bold face letters (e.g.
x and w). The inner product between vectors x and w is denoted by hx, wi. Sets are designated
by upper case letters (e.g. S ). The set of non-negative real numbers is denoted by R+ . For any
kxk2 = (hx, xi)1/2 is dual to itself and the ‘1 norm, kxk1 = P
k ≥ 1, the set of integers {1, . . . , k} is denoted by [k ]. A norm of a vector x is denoted by kxk.
The dual norm is deﬁned as kλk? = sup{hx, λi : kxk ≤ 1}. For example, the Euclidean norm,
i |xi |, is dual to the ‘∞ norm,
kxk∞ = maxi |xi |.
We next recall a few deﬁnitions from convex analysis. The reader familiar with convex analysis
may proceed to Lemma 1 while for a more thorough introduction see for example [1]. A set S is

(3)

convex if for any two vectors w1 , w2 in S , all the line between w1 and w2 is also within S . That
is, for any α ∈ [0, 1] we have that αw1 + (1 − α)w2 ∈ S . A set S is open if every point in S has a
neighborhood lying in S . A set S is closed if its complement is an open set. A function f : S → R
is closed and convex if for any scalar α ∈ R, the level set {w : f (w) ≤ α} is closed and convex.
The Fenchel conjugate of a function f : S → R is deﬁned as f ? (θ) = supw∈S hw, θi − f (w) . If
f is closed and convex then the Fenchel conjugate of f ? is f itself. The Fenchel-Young inequality
states that for any w and θ we have that f (w) + f ? (θ) ≥ hw, θi. A vector λ is a sub-gradient of
a function f at w if for all w0 ∈ S we have that f (w0 ) − f (w) ≥ hw0 − w, λi. The differential
set of f at w, denoted ∂ f (w), is the set of all sub-gradients of f at w. If f is differentiable at w
then ∂ f (w) consists of a single vector which amounts to the gradient of f at w and is denoted by
∇f (w). Sub-gradients play an important role in the deﬁnition of Fenchel conjugate. In particular,
the following lemma states that if λ ∈ ∂ f (w) then Fenchel-Young inequality holds with equality.
Lemma 1 Let f be a closed and convex function and let ∂ f (w0 ) be its differential set at w0 . Then,
0 ∈ ∂ f (w0 ) we have, f (w0 ) + f ? (λ
0 ) = hλ
, w0 i .
0
for all λ
A continuous function f is σ -strongly convex over a convex set S with respect to a norm k · k if S
is contained in the domain of f and for all v, u ∈ S and α ∈ [0, 1] we have
f (α v + (1 − α) u) ≤ α f (v) + (1 − α) f (u) − 1
2 σ α (1 − α) kv − uk2 .
Strongly convex functions play an important role in our analysis primarily due to the following
lemma.
Lemma 2 Let k · k be a norm over Rn and let k · k? be its dual norm. Let f be a σ -strongly
convex function on S and let f ? be its Fenchel conjugate. Then, f ? is differentiable with ∇f ? (θ) =
arg maxx∈S hθ , xi − f (x). Furthermore, for any θ , λ ∈ Rn we have
1
f ? (θ + λ) − f ? (θ) ≤ h∇f ? (θ), λi +
kλk2
? .
2 σ
Two notable examples of strongly convex functions which we use are as follows.
2 kwk2
2 is 1-strongly convex over S = Rn with respect to the ‘2
Example 1 The function f (w) = 1
Example 2 The function f (w) = Pn
2 kθk2
norm. Its conjugate function is f ? (θ) = 1
2 .
Pn
i=1 wi log(wi / 1
n ) is 1-strongly convex over the probabilistic
simplex, S = {w ∈ Rn
+ : kwk1 = 1}, with respect to the ‘1 norm. Its conjugate function is
f ? (θ) = log( 1
i=1 exp(θi )).
n
3 Generalized Fenchel Duality
(cid:17)
(cid:16)
c f (w) + PT
In this section we derive our main analysis tool. We start by considering the following optimization
problem,
t=1 gt (w)
inf
(cid:17)
(cid:16)
w∈S
c f (w0 ) + PT
where c is a non-negative scalar. An equivalent problem is
s.t. w0 ∈ S and ∀t ∈ [T ], wt = w0 .
t=1 gt (wt )
inf
w0 ,w1 ,...,wT
Introducing T vectors λ1 , . . . , λT , each λt ∈ Rn is a vector of Lagrange multipliers for the equality
t=1 gt (wt ) + PT
L(w0 , w1 , . . . , wT , λ1 , . . . , λT ) = c f (w0 ) + PT
constraint wt = w0 , we obtain the following Lagrangian
t=1 hλt , w0 − wt i .
The dual problem is the task of maximizing the following dual objective value,
(cid:16)hw0 , − 1
(cid:17) − PT
D(λ1 , . . . , λT ) =
L(w0 , w1 , . . . , wT , λ1 , . . . , λT )
PT
inf
w0∈S,w1 ,...,wT
(cid:17) − PT
= −c f ? (cid:16)− 1
= − c sup
t=1 λt i − f (w0 )
PT
t=1 sup
w0∈S
c
wt
t (λt ) ,
t=1 g?
t=1 λt
c

,

(hwt , λt i − gt (wt ))

where, following the exposition of Sec. 2, f ? , g?
T are the Fenchel conjugate functions of
1 , . . . , g?
− c f ? (cid:16)− 1
(cid:17) − PT
PT
f , g1 , . . . , gT . Therefore, the generalized Fenchel dual problem is
t (λt ) .
t=1 g?
t=1 λt
c
Note that when T = 1 and c = 1, the above duality is the so called Fenchel duality.

sup
λ1 ,...,λT

(4)

4 A Template Learning Algorithm for Convex Repeated Games

,

(5)

In this section we describe a template learning algorithm for playing convex repeated games. As
mentioned before, we study convex repeated games from the viewpoint of the ﬁrst player which we
shortly denote as P1. Recall that we would like our learning algorithm to achieve a regret bound of
!
 
mX
TX
the form given in Eq. (2). We start by rewriting Eq. (2) as follows
gt (u)
c f (u) +
t=1
t=1

gt (wt ) − c L ≤ inf
u∈S

√
where c =
T . Thus, up to the sublinear term c L, the cumulative loss of P1 lower bounds the
optimum of the minimization problem on the right-hand side of Eq. (5). In the previous section
we derived the generalized Fenchel dual of the right-hand side of Eq. (5). Our construction is
based on the weak duality theorem stating that any value of the dual problem is smaller than the
optimum value of the primal problem. The algorithmic framework we propose is therefore derived
by incrementally ascending the dual objective function. Intuitively, by ascending the dual objective
we move closer to the optimal primal value and therefore our performance becomes similar to the
performance of the best ﬁxed weight vector which minimizes the right-hand side of Eq. (5).
t = 0 for all t. We assume that inf w f (w) = 0 and
Initially, we use the elementary dual solution λ1
for all t inf w gt (w) = 0 which imply that D(λ1
T ) = 0. We assume in addition that f is
1 , . . . , λ1
(cid:17)
wt = ∇f ? (cid:16)− 1
σ -strongly convex. Therefore, based on Lemma 2, the function f ? is differentiable. At trial t, P1
PT
uses for prediction the vector
i=1 λt
i
c
After predicting wt , P1 receives the function gt and suffers the loss gt (wt ). Then, P1 updates the
dual variables as follows. Denote by ∂t the differential set of gt at wt , that is,
∂t = {λ : ∀w ∈ S, gt (w) − gt (wt ) ≥ hλ, w − wt i} .
The new dual variables (λt+1
T ) are set to be any set of vectors which satisfy the following
, . . . , λt+1
1
two conditions:
0 ∈ ∂t s.t. D(λt+1
(i). ∃λ
1
(ii). ∀i > t, λt+1
i = 0
In the next section we show that condition (i) ensures that the increase of the dual at trial t is
proportional to the loss gt (wt ). The second condition ensures that we can actually calculate the dual
at trial t without any knowledge on the yet to be seen loss functions gt+1 , . . . , gT .
We conclude this section with two update rules that trivially satisfy the above two conditions. The
(cid:26) λ
0 ∈ ∂t and set
ﬁrst update scheme simply ﬁnds λ
0
λt
i

T ) ≥ D(λt
0
1 , . . . , λt
, . . . , λt+1
t−1 , λ

T )
t+1 , . . . , λt
, λt

if i = t
if i 6= t

λt+1
i

=

.

(8)

(6)

(7)

.

.

The second update deﬁnes

(λt+1
1

T ) = argmax
, . . . , λt+1
λ1 ,...,λT

D(λ1 , . . . , λT )

s.t. ∀i 6= t, λi = λt
i

.

(10)

(9)

5 Analysis

In this section we analyze the performance of the template algorithm given in the previous section.
Our proof technique is based on monitoring the value of the dual objective function. The main result
is the following lemma which gives upper and lower bounds for the ﬁnal value of the dual objective
function.
Lemma 3 Let f be a σ -strongly convex function with respect to a norm k · k over a set S and
assume that minw∈S f (w) = 0. Let g1 , . . . , gT be a sequence of convex and closed functions such
that inf w gt (w) = 0 for all t ∈ [T ]. Suppose that a dual-incrementing algorithm which satisﬁes
the conditions of Eq. (8) is run with f as a complexity function on the sequence g1 , . . . , gT . Let
w1 , . . . , wT be the sequence of primal vectors that the algorithm generates and λT +1
, . . . , λT +1
1
0
0
T
be its ﬁnal sequence of dual variables. Then, there exists a sequence of sub-gradients λ
T ,
1 , . . . , λ
t ∈ ∂t for all t, such that
0
TX
TX
TX
where λ
gt (wt ) − 1
? ≤ D(λT +1
tk2
kλ
0
2 σ c
1
t=1
t=1
t=1

) ≤ inf
w∈S

, . . . , λT +1
T

c f (w) +

gt (w) .

Proof The second inequality follows directly from the weak duality theorem. Turning to
the left most inequality, denote ∆t = D(λt+1
T ) − D(λt
T ) and note that
, . . . , λt+1
1 , . . . , λt
) = PT
T ) = PT
1
D(λT +1
) can be rewritten as
, . . . , λT +1
1
T
D(λT +1
t=1 ∆t − D(λ1
t=1 ∆t ,
, . . . , λT +1
1 , . . . , λ1
(11)
1
T
Pt−1
where the last equality follows from the fact that f ? (0) = g?
T (0) = 0. The deﬁnition
1 (0) = . . . = g?
of the update implies that ∆t ≥ D(λt
t , 0, . . . , 0) − D(λt
0
t−1 , 0, 0, . . . , 0) for
1 , . . . , λt
1 , . . . , λt
t−1 , λ
t ∈ ∂t . Denoting θ t = − 1
some subgradient λ0
j=1 λj , we now rewrite the lower bound on ∆t as,
c
t/c) − f ? (θ t )) − g?
∆t ≥ −c (f ? (θ t − λ
0
0
t ) . Using Lemma 2 and the deﬁnition of wt we get
t (λ
that
t i − g?
∆t ≥ hwt , λ
t ) − 1
tk2
2 σ c kλ
0
0
0
t (λ
(12)
? .
t ∈ ∂t and since we assume that gt is closed and convex, we can apply Lemma 1 to get that
0
Since λ
PT
PT
t=1 ∆t ≥ PT
hwt , λ
t i − g?
0
0
t ) = gt (wt ). Plugging this equality into Eq. (12) and summing over t we obtain
t (λ
that
tk2
t=1 gt (wt ) − 1
t=1 kλ
0
? .
2 σ c
Combining the above inequality with Eq. (11) concludes our proof.

PT
The following regret bound follows as a direct corollary of Lemma 3.
PT
PT
tk2
t=1 kλ
0
Theorem 1 Under the same conditions of Lemma 3. Denote L = 1
? . Then, for all
w ∈ S we have,
T
t=1 gt (w) ≤ c f (w)
t=1 gt (wt ) − 1
T + L
1
2 σ c .
T
T
PT
PT
T , we obtain the bound,
t=1 gt (w) ≤ f (w)+L/(2 σ)
t=1 gt (wt ) − 1
√
T
T
6 Application to Online learning

In particular, if c =

√

1
T

.

In Sec. 1 we cast the task of online learning as a convex repeated game. We now demonstrate
the applicability of our algorithmic framework for the problem of instance ranking. We analyze
this setting since several prediction problems, including binary classiﬁcation, multiclass prediction,
multilabel prediction, and label ranking, can be cast as special cases of the instance ranking problem.
Recall that on each online round, the learner receives a question-answer pair. In instance ranking,
the question is encoded by a matrix Xt of dimension kt × n and the answer is a vector yt ∈ Rkt .
The semantic of yt is as follows. For any pair (i, j ), if yt,i > yt,j then we say that yt ranks

the i’th row of Xt ahead of the j ’th row of Xt . We also interpret yt,i − yt,j as the conﬁdence
in which the i’th row should be ranked ahead of the j ’th row. For example, each row of Xt en-
compasses a representation of a movie while yt,i is the movie’s rating, expressed as the number of
stars this movie has received by a movie reviewer. The predictions of the learner are determined
based on a weight vector wt ∈ Rn and are deﬁned to be ˆyt = Xt wt . Finally, let us deﬁne
two loss functions for ranking, both generalize the hinge-loss used in binary classiﬁcation prob-
lems. Denote by Et the set {(i, j ) : yt,i > yt,j }. For all (i, j ) ∈ Et we deﬁne a pair-based
hinge-loss ‘i,j (w; (Xt , yt )) = [(yt,i − yt,j ) − hw, xt,i − xt,j i]+ , where [a]+ = max{a, 0} and
xt,i , xt,j are respectively the i’th and j ’th rows of Xt . Note that ‘i,j is zero if w ranks xt,i higher
than xt,j with a sufﬁcient conﬁdence. Ideally, we would like ‘i,j (wt ; (Xt , yt )) to be zero for all
P
(i, j ) ∈ Et . If this is not the case, we are being penalized according to some combination of the
pair-based losses ‘i,j . For example, we can set ‘(w; (Xt , yt )) to be the average over the pair losses,
‘avg (w; (Xt , yt )) = 1|Et |
‘i,j (w; (Xt , yt )) . This loss was suggested by several authors
(i,j )∈Et
(see for example [18]). Another popular approach (see for example [5]) penalizes according to
the maximal loss over the individual pairs, ‘max (w; (Xt , yt )) = max(i,j )∈Et ‘i,j (w; (Xt , yt )) .
We can apply our algorithmic framework given in Sec. 4 for ranking, using for gt (w) either
‘avg (w; (Xt , yt )) or ‘max (w; (Xt , yt )). The following theorem provides us with a sufﬁcient con-
dition under which the regret bound from Thm. 1 holds for ranking as well.
Theorem 2 Let f be a σ -strongly convex function over S with respect to a norm k · k. Denote by
Lt the maximum over (i, j ) ∈ Et of kxt,i − xt,j k2∗ . Then, for both gt (w) = ‘avg (w; (Xt , yt )) and
PT
PT
PT
gt (w) = ‘max (w; (Xt , yt )), the following regret bound holds
∀u ∈ S,
t=1 gt (u) ≤ f (u)+ 1
t=1 gt (wt ) − 1
√
t=1 Lt /(2 σ)
1
T
T
T
T
7 The Boosting Game

.

In this section we describe the applicability of our algorithmic framework to the analysis of boosting
algorithms. A boosting algorithm uses a weak learning algorithm that generates weak-hypotheses
whose performances are just slightly better than random guessing to build a strong-hypothesis which
can attain an arbitrarily low error. The AdaBoost algorithm, proposed by Freund and Schapire [6],
receives as input a training set of examples {(x1 , y1 ), . . . , (xm , ym )} where for all i ∈ [m], xi is
taken from an instance domain X , and yi is a binary label, yi ∈ {+1, −1}. The boosting process
proceeds in a sequence of consecutive trials. At trial t, the booster ﬁrst deﬁnes a distribution, denoted
wt , over the set of examples. Then, the booster passes the training set along with the distribution wt
to the weak learner. The weak learner is assumed to return a hypothesis ht : X → {+1, −1} whose
def= Pm
2 . That is, there exists a constant γ > 0 such that,
average error is slightly smaller than 1
1−yi ht (xi )
≤ 1
2 − γ .
(13)
i=1 wt,i
t
2
The goal of the boosting algorithm is to invoke the weak learner several times with different distri-
butions, and to combine the hypotheses returned by the weak learner into a ﬁnal, so called strong,
hf (x) = PT
hypothesis whose error is small. The ﬁnal hypothesis combines linearly the T hypotheses returned
by the weak learner with coefﬁcients α1 , . . . , αT , and is deﬁned to be the sign of hf (x) where
t=1 αt ht (x) . The coefﬁcients α1 , . . . , αT are determined by the booster. In Ad-
m ). At iter-
aBoost, the initial distribution is set to be the uniform distribution, w1 = ( 1
m , . . . , 1
2 log((1 − t )/t ). The distribution is updated by the rule
ation t, the value of αt is set to be 1
wt+1,i = wt,i exp(−αt yi ht (xi ))/Zt , where Zt is a normalization factor. Freund and Schapire [6]
have shown that under the assumption given in Eq. (13), the error of the ﬁnal strong hypothesis is at
most exp(−2 γ 2 T ).
Several authors [15, 13, 8, 4] have proposed to view boosting as a coordinate-wise greedy optimiza-
tion process. To do so, note ﬁrst that hf errs on an example (x, y) iff y hf (x) ≤ 0. Therefore,
the exp-loss function, deﬁned as exp(−y hf (x)), is a smooth upper bound of the zero-one error,
which equals to 1 if y hf (x) ≤ 0 and to 0 otherwise. Thus, we can restate the goal of boosting as
minimizing the average exp-loss of hf over the training set with respect to the variables α1 , . . . , αT .
(cid:16)−yi
(cid:17)
PT
Pm
To simplify our derivation in the sequel, we prefer to say that boosting maximizes the negation of
the loss, that is,
− 1
t=1 αtht (xi )
i=1 exp
m

(14)

max
α1 ,...,αT

.

log

βt vt

In this view, boosting is an optimization procedure which iteratively maximizes Eq. (14) with respect
to the variables α1 , . . . , αT . This view of boosting, enables the hypotheses returned by the weak
learner to be general functions into the reals, ht : X → R (see for instance [15]).
In this paper we view boosting as a convex repeated game between a booster and a weak learner.
To motivate our construction, we would like to note that boosting algorithms deﬁne weights in
two different domains: the vectors wt ∈ Rm which assign weights to examples and the weights
{αt : t ∈ [T ]} over weak-hypotheses. In the terminology used throughout this paper, the weights
wt ∈ Rm are primal vectors while (as we show in the sequel) each weight αt of the hypothesis ht
is related to a dual vector λt . In particular, we show that Eq. (14) is exactly the Fenchel dual of a
primal problem for a convex repeated game, thus the algorithmic framework described thus far for
playing games naturally ﬁts the problem of iteratively solving Eq. (14).
To derive the primal problem whose Fenchel dual is the problem given in Eq. (14) let us ﬁrst denote
by vt the vector in Rm whose ith element is vt,i = yiht (xi ). For all t, we set gt to be the function
gt (w) = [hw, vt i]+ . Intuitively, gt penalizes vectors w which assign large weights to examples
In particular, if ht (xi ) ∈ {+1, −1} and
which are predicted accurately, that is yiht (xi ) > 0.
wt is a distribution over the m examples (as is the case in AdaBoost), gt (wt ) reduces to 1 − 2t
hypothesis ht over the examples. Consider the problem of minimizing c f (w) + PT
(see Eq. (13)). In this case, minimizing gt is equivalent to maximizing the error of the individual
t=1 gt (w) where
f (w) is the relative entropy given in Example 2 and c = 1/(2 γ ) (see Eq. (13)). To derive its
t (λt ) = 0 if there exists βt ∈ [0, 1] such that λt = βtvt and otherwise
Fenchel dual, we note that g?
t (λt ) = ∞ (see [16]).
!
!
 
 
In addition, let us deﬁne αt = 2 γ βt . Since our goal is to maximize the
g?
mX
TX
dual, we can restrict λt to take the form λt = βtvt = αt
2 γ vt , and get that
e− PT
1
= − 1
− 1
D(λ1 , . . . , λT ) = −c f ?
t=1 αt yi ht (xi )
2 γ
c
m
t=1
i=1
Minimizing the exp-loss of the strong hypothesis is therefore the dual problem of the following
primal minimization problem: ﬁnd a distribution over the examples, whose relative entropy to the
uniform distribution is as small as possible while the correlation of the distribution with each vt
is as small as possible. Since the correlation of w with vt is inversely proportional to the error of
ht with respect to w, we obtain that in the primal problem we are trying to maximize the error of
each individual hypothesis, while in the dual problem we minimize the global error of the strong
hypothesis. The intuition of ﬁnding distributions which in retrospect result in large error rates of
individual hypotheses was also alluded in [15, 8].
We can now apply our algorithmic framework from Sec. 4 to boosting. We describe the game
with the parameters αt , where αt ∈ [0, 2 γ ], and underscore that in our case, λt = αt
2 γ vt . At
the beginning of the game the booster sets all dual variables to be zero, ∀t αt = 0. At trial t of
Eq. (6), that is, wt = ∇f ? (θ t ), where θ t = − P
the boosting game, the booster ﬁrst constructs a primal weight vector wt ∈ Rm , which assigns
importance weights to the examples in the training set. The primal vector wt is constructed as in
i αivi . Then, the weak learner responds by
presenting the loss function gt (w) = [hw, vt i]+ . Finally, the booster updates the dual variables so
as to increase the dual objective function. It is possible to show that if the range of ht is {+1, −1}
then the update given in Eq. (10) is equivalent to the update αt = min{2 γ , 1
2 log((1 − t )/t )}.
We have thus obtained a variant of AdaBoost in which the weights αt are capped above by 2 γ .
A disadvantage of this variant is that we need to know the parameter γ . We would like to note in
passing that this limitation can be lifted by a different deﬁnition of the functions gt . We omit the
details due to the lack of space.
and therefore the left-hand side inequality given in Lemma 3 tells us that PT
PT
the conditions given in Lemma 3 holds
To analyze our game of boosting, we note that
t=1 gt (wt ) −
tk2∞ ≤ D(λT +1
t=1 kλ
0
) . The deﬁnition of gt and the weak learnability as-
, . . . , λT +1
1
sumption given in Eq. (13) imply that hwt , vt i ≥ 2 γ for all t. Thus, gt (wt ) = hwt , vt i ≥ 2 γ
1
T
2 c
0
t = vt . Recall that vt,i = yiht (xi ). Assuming that the range of ht
which also implies that λ
tk∞ ≤ 1. Combining all the above with the left-hand side inequality
is [+1, −1] we get that kλ
0
). Using the deﬁnition of D (see
given in Lemma 3 we get that 2 T γ − T
2 c ≤ D(λT +1
Pm
, . . . , λT +1
PT
1
T
Eq. (15)), the value c = 1/(2 γ ), and rearranging terms we recover the original bound for AdaBoost
t=1 αt ht (xi ) ≤ e−2 γ 2 T .
i=1 e−yi
1
m

.

(15)

8 Related Work and Discussion

We presented a new framework for designing and analyzing algorithms for playing convex repeated
games. Our framework was used for the analysis of known algorithms for both online learning and
boosting settings. The framework also paves the way to new algorithms. In a previous paper [17],
we suggested the use of duality for the design of online algorithms in the context of mistake bound
analysis. The contribution of this paper over [17] is three fold as we now brieﬂy discuss.
First, we generalize the applicability of the framework beyond the speciﬁc setting of online learning
with the hinge-loss to the general setting of convex repeated games. The setting of convex repeated
games was formally termed “online convex programming” by Zinkevich [19] and was ﬁrst pre-
sented by Gordon in [9]. There is voluminous amount of work on unifying approaches for deriving
online learning algorithms. We refer the reader to [11, 12, 3] for work closely related to the content
of this paper. By generalizing our previously studied algorithmic framework [17] beyond online
learning, we can automatically utilize well known online learning algorithms, such as the EG and
p-norm algorithms [12, 11], to the setting of online convex programming. We would like to note
that the algorithms presented in [19] can be derived as special cases of our algorithmic framework
2 kwk2 . Parallel and independently to this work, Gordon [10] described an-
by setting f (w) = 1
other algorithmic framework for online convex programming that is closely related to the potential
based algorithms described by Cesa-Bianchi and Lugosi [3]. Gordon also considered the problem of
deﬁning appropriate potential functions. Our work generalizes some of the theorems in [10] while
providing a somewhat simpler analysis.
Second, the usage of generalized Fenchel duality rather than the Lagrange duality given in [17] en-
ables us to analyze boosting algorithms based on the framework. Many authors derived unifying
frameworks for boosting algorithms [13, 8, 4]. Nonetheless, our general framework and the connec-
tion between game playing and Fenchel duality underscores an interesting perspective of both online
learning and boosting. We believe that this viewpoint has the potential of yielding new algorithms
in both domains.
Last, despite the generality of the framework introduced in this paper, the resulting analysis is more
distilled than the earlier analysis given in [17] for two reasons. (i) The usage of Lagrange duality
in [17] is somehow restricted while the notion of generalized Fenchel duality is more appropriate to
the general and broader problems we consider in this paper. (ii) The strongly convex property we
employ both simpliﬁes the analysis and enables more intuitive conditions in our theorems.
There are various possible extensions of the work that we did not pursue here due to the lack of space.
For instanc, our framework can naturally be used for the analysis of other settings such as repeated
games (see [7, 19]). The applicability of our framework to online learning can also be extended to
other prediction problems such as regression and sequence prediction. Last, we conjecture that our
primal-dual view of boosting will lead to new methods for regularizing boosting algorithms, thus
improving their generalization capabilities.

References
[1] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization. Springer, 2006.
[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[3] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
[4] M. Collins, R.E. Schapire, and Y. Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 2002.
[5] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive algorithms. JMLR, 7, Mar 2006.
[6] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. In EuroCOLT, 1995.
[7] Y. Freund and R.E. Schapire. Game theory, on-line prediction and boosting. In COLT, 1996.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28(2), 2000.
[9] G. Gordon. Regret bounds for prediction problems. In COLT, 1999.
[10] G. Gordon. No-regret algorithms for online convex programs. In NIPS, 2006.
[11] A. J. Grove, N. Littlestone, and D. Schuurmans. General convergence results for linear discriminant updates. Machine Learning, 43(3),
2001.
[12] J. Kivinen and M. Warmuth. Relative loss bounds for multidimensional regression problems. Journal of Machine Learning, 45(3),2001.
[13] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses. In Advances in Large Margin
Classiﬁers. MIT Press, 1999.
[14] Y. Nesterov. Primal-dual subgradient methods for convex problems. Technical report, Center for Operations Research and Econometrics
(CORE), Catholic University of Louvain (UCL), 2005.
[15] R. E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37(3):1–40, 1999.
[16] S. Shalev-Shwartz and Y. Singer. Convex repeated games and fenchel duality. Technical report, The Hebrew University, 2006.
[17] S. Shalev-Shwartz and Y. Singer. Online learning meets optimization in the dual. In COLT, 2006.
[18] J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. In ESANN, April 1999.
[19] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.


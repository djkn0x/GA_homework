Sparse Multinomial Logistic Regression via Bayesian
L1 Regularisation

Gavin C. Cawley
School of Computing Sciences
University of East Anglia
Norwich, Norfolk, NR4 7TJ, U.K.
gcc@cmp.uea.ac.uk

Nicola L. C. Talbot
School of Computing Sciences
University of East Anglia
Norwich, Norfolk, NR4 7TJ, U.K.
nlct@cmp.uea.ac.uk

Mark Girolami
Department of Computing Science
University of Glasgow
Glasgow, Scotland, G12 8QQ, U.K.
girolami@dcs.gla.ac.uk

Abstract

Multinomial
logistic regression provides the standard penalised maximum-
likelihood solution to multi-class pattern recognition problems. More recently,
the development of sparse multinomial logistic regression models has found ap-
plication in text processing and microarray classi ﬁcation , where explicit identi ﬁ-
cation of the most informative features is of value. In this paper, we propose a
sparse multinomial logistic regression method, in which the sparsity arises from
the use of a Laplace prior, but where the usual regularisatio n parameter is inte-
grated out analytically. Evaluation over a range of benchmark datasets reveals
this approach results in similar generalisation performan ce to that obtained using
cross-validation, but at greatly reduced computational expense.

1

Introduction

Multinomial logistic and probit regression are perhaps the classic statistical methods for multi-class
pattern recognition problems (for a detailed introduction , see e.g. [1, 2]). The output of a multino-
mial logistic regression model can be interpreted as an a-posteriori estimate of the probability that
a pattern belongs to each of c disjoint classes. The probabilistic nature of the multinomial logistic
regression model affords many practical advantages, such a s the ability to set rejection thresholds
[3], to accommodate unequal relative class frequencies in the training set and in operation [4], or
to apply an appropriate loss matrix in making predictions that minimise the expected risk [5]. As
a result, these models have been adopted in a diverse range of applications, including cancer clas-
si ﬁcation [6, 7], text categorisation [8], analysis of DNA b inding sites [9] and call routing. More
recently, the focus of research has been on methods for induc ing sparsity in (multinomial) logistic
or probit regression models. In some applications, the iden ti ﬁcation of salient input features is of
itself a valuable activity; for instance in cancer classi ﬁc ation from micro-array gene expression data,
the identi ﬁcation of biomarker genes, the pattern of expression of which is diagnostic of a particular
form of cancer, may provide insight into the ætiology of the c ondition. In other applications, these
methods are used to select a small number of basis functions to form a compact non-parametric clas-
si ﬁer, from a set that may contain many thousands of candidat e functions. In this case the sparsity
is desirable for the purposes of computational expediency, rather than as an aid to understanding the
data.

A variety of methods have been explored that aim to introduce sparsity in non-parametric regression
models through the incorporation of a penalty or regularisa tion term within the training criterion. In
the context of least-squares regression using Radial Basis Function (RBF) networks, Orr [10], pro-
poses the use of local regularisation, in which a weight-decay regularisation term is used with distinct
regularisation parameters for each weight. The optimisati on of the Generalised Cross-Validation
(GCV) score typically leads to the regularisation paramete rs for redundant basis functions achiev-
ing very high values, allowing them to be identi ﬁed and prune d from the network (c.f. [11, 12]).
The computational efﬁciency of this approach can be further
improved via the use of Recursive Or-
thogonal Least Squares (ROLS). The relevance vector machine (RVM) [13] implements a form of
Bayesian automatic relevance determination (ARD), using a separable Gaussian prior. In this case,
the regularisation parameter for each weight is adjusted so as to maximise the marginal likelihood,
also known as the Bayesian evidence for the model. An efﬁcient component-wise training algorit hm
is given in [14]. An alternative approach, known as the LASSO [15], seeks to minimise the negative
log-likelihood of the sample, subject to an upper bound on the sum of the absolute value of the
weights (see also [16] for a practical training procedure). This strategy is equivalent to the use of a
Laplace prior over the model parameters [17], which has been demonstrated to control over- ﬁtting
and induce sparsity in the weights of multi-layer perceptron networks [18]. The equivalence of the
Laplace prior and a separable Gaussian prior (with appropri ate choice of regularisation parameters)
has been established by Grandvalet [11, 12], unifying these strands of research.

In this paper, we demonstrate that, in the case of the Laplace prior, the regularisation parameters
can be integrated out analytically, obviating the need for a lengthy cross-validation based model
selection stage. The resulting sparse multinomial logistic regression algorithm with Bayesian regu-
larisation (SBMLR) is then fully automated and, having storage requirements that scale only linearly
with the number of model parameters, is well suited to relatively large-scale applications. The re-
mainder of this paper is set out as follows: The sparse multinomial logistic regression procedure
with Bayesian regularisation is presented in Section 2. The proposed algorithm is then evaluated
against competing approaches over a range of benchmark learning problems in Section 3. Finally,
the work is summarised in Section 5 and conclusion drawn.

2 Method

Let D = {(xn , tn )}ℓ
n=1 represent the training sample, where xn ∈ X ⊂ Rd is the vector of input
features for the ith example, and tn ∈ T = {t | t ∈ {0, 1}c , ktk1 = 1 } is the corresponding vector
of desired outputs, using the usual 1-of-c coding scheme. Multinomial logistic regression constructs
a generalised linear model [1] with a softmax inverse link function [19], allowing the outputs to be
interpreted as a-posteriori estimates of the probabilities of class membership,

d
exp{an
i }
Xj=1
i |xn ) = yn
p(tn
i =
Pc
j=1 exp{an
j }
Assuming that D represents an i.i.d. sample from a conditional multinomial distribution, then the
negative log-likelihood, used as a measure of the data-mis ﬁ
t, can be written as,

wij xn
j

an
i =

where

(1)

c
ℓ
ℓ
Xi=1
Xn=1
Xn=1
i log {yn
tn
i }
The parameters, w of the multinomial logistic regression model are given by the minimiser of a
penalised maximum-likelihood training criterion,

E n
D = −

ED =

EW =

L = ED + αEW

d
c
Xj=1
Xi=1
and α is a regularisation parameter [20] controlling the bias-va riance trade-off [21]. At a minima of
L, the partial derivatives of L with respect to the model parameters will be uniformly zero, giving
(cid:12)(cid:12)(cid:12)(cid:12)
∂wij (cid:12)(cid:12)(cid:12)(cid:12)
∂wij (cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
∂ED
∂ED

< α if |wij | = 0.

= α if |wij | > 0

|wij |

(2)

where

and

This implies that if the sensitivity of the negative log-likelihood with respect to a model parameter,
wij , falls below α, then the value of that parameter will be set exactly to zero and the corresponding
input feature can be pruned from the model.

2.1 Eliminating the Regularisation Parameters

(3)

α
2

Minimisation of (2) has a straight-forward Bayesian interpretation; the posterior distribution for w ,
the parameters of the model given by (1), can be written as
p(w |D) ∝ P (D |w)P (w).
L is then, up to an additive constant, the negative logarithm of the posterior density. The prior over
model parameters, w , is then given by a separable Laplace distribution
W
P (w) = (cid:16) α
2 (cid:17)W
Yi=1
exp {−α|wi |} ,
exp{−αEW } =
where W is the number of active (non-zero) model parameters. A good value for the regularisation
parameter α can be estimated, within a Bayesian framework, by maximising the evidence [22] or
alternatively it may be integrated out analytically [17, 23]. Here we take the latter approach, where
the prior distribution over model parameters is given by marginalising over α,
p(w) = Z p(w |α)p(α)dα.
As α is a scale parameter, an appropriate ignorance prior is give n by the improper Jeffrey’s prior,
p(α) ∝ 1/α, corresponding to a uniform prior over log α. Substituting equation (3) and noting that
α is strictly positive,
2W Z ∞
1
αW −1 exp{−αEW }dα.
p(w) =
0
Using the Gamma integral, R ∞
0 xν−1 e−µxdx = Γ(ν )
[24, equation 3.384], we obtain
µν
1
Γ(W )
=⇒ − log p(w) ∝ W log EW ,
p(w) =
EW
2W
W
giving a revised optimisation criterion for sparse logisti c regression with Bayesian regularisation,

M = ED + W log EW ,

(4)

in which the regularisation parameter has been eliminated, for further details and theoretical justi-
ﬁcation, see [17]. Note that we integrate out the regularisa tion parameter and optimise the model
parameters, which is unusual in that most Bayesian approaches, such as the relevance vector ma-
chine [13] optimise the regularisation parameters and integrate over the weights.

2.1.1 Practical Implementation

The training criterion incorporating a fully Bayesian regu larisation term can be minimised via a
simple modi ﬁcation of existing cyclic co-ordinate descent algorithms for sparse regression using a
Laplace prior (e.g. [25, 26]). Differentiating the origina l and modi ﬁed training criteria, (2) and (4)
respectively, we have that
∇L = ∇ED + α∇EW

∇M = ∇ED + ˜α∇EW

and

where

1/ ˜α =

W
Xi=1
|wi |.
From a gradient descent perspective, minimising M effectively becomes equivalent to minimising
L, assuming that the regularisation parameter, α, is continuously updated according to (5) following
every change in the vector of model parameters, w [17]. This requires only a very minor modi ﬁca-
tion of the existing training algorithm, whilst eliminating the only training parameter and hence the
need for a model selection procedure in ﬁtting the model.

1
W

(5)

2.1.2 Equivalence of Marginalisation and Optimisation under the Evidence Framework

Williams [17] notes that, at least in the case of the Laplace prior, integrating out the regularisation pa-
rameter analytically is equivalent to its optimisation under the evidence framework of MacKay [22].
The argument provided by Williams can be summarised as follows: The evidence framework sets
the value of the regularisation parameter so as to optimise the marginal likelihood,
P (D) = Z P (D |w)P (w)dw ,
also known as the evidence for the model. The Bayesian interpretation of the regularis ed objective
function gives,
1
ZW Z exp {−L} dw ,
P (D) =
where ZW is a normalising constant for the prior over the model parame ters, for the Laplace prior,
ZW = (2/α)W . In the case of multinomial logistic regression, ED represents the negative logarithm
of a normalised distribution, and so the corresponding norm alising constant for the data mis ﬁt term
is redundant. Unfortunately this integral is analytically intractable, and so we adopt the Laplace
approximation, corresponding to a Gaussian posterior dist ribution for the model parameters, centred
on their most probable value, wMP ,

L(w) = L(wMP ) +

1
(w − wMP )T A(w − wMP )
2
where A = ∇∇L is the Hessian of the regularised objective function. The regulariser corresponding
to the Laplace prior is locally a hyper-plane, and so does not contribute to the Hessian and so
A = ∇∇ED . The negative logarithm of the evidence can then be written as,
1
− log P (D) = ED + αEW +
log |A| + log ZW + constant.
2
Setting the derivative of the evidence with respect to α to zero, gives rise to a simple update rule for
the regularisation parameter,

=

1
˜α

1
W

W
Xj=1
|wj |,
which is equivalent to the update rule obtained using the integrate-out approach. Maximising the
evidence for the model also provides a convenient means for model selection. Using the Laplace
approximation, evidence for a multinomial logistic regres sion model under the proposed Bayesian
regularisation scheme is given by
2W (cid:27) +
− log {D} = ED + W log EW − log (cid:26) Γ (W )
where A = ∇∇L.
2.2 A Simple but Ef ﬁcient Training Algorithm

log |A| + constant

1
2

In this study, we adopt a simpli ﬁed version of the efﬁcient co mponent-wise training algorithm of
Shevade and Keerthi [25], adapted for multinomial, rather than binomial, logistic regression. The
principal advantage of a component-wise optimisation algorithm is that the Hessian matrix is not
required, but only the ﬁrst and second partial derivatives o f the regularised training criterion. The
ﬁrst partial derivatives of the data mis- ﬁt term are given by
,
c
Xi=1
and δij = 1 if i = j and otherwise δij = 0. Substituting, we obtain,

= yi δij − yi yj

∂E n
D
∂ yn
i

∂E n
D
∂ yn
i

∂E n
D
∂ an
j

∂ yn
i
∂ an
j

∂ yn
i
∂ an
j

= −

tn
i
yn
i

,

=

where

∂ED
∂ai

=

ℓ
Xn=1

i − tn
[yn
i ]

=⇒

∂ED
∂wij

=

ℓ
Xn=1

i ] xn
i − tn
[yn
j =

ℓ
Xn=1

i xn
yn
j −

i xn
tn
j .

ℓ
Xn=1

Similarly, the second partial derivatives are given by,

ℓ
ℓ
j (cid:3)2
Xn=1
Xn=1
i ) (cid:2)xn
i (1 − yn
yn
The Laplace regulariser is locally a hyperplane, with the magnitude of the gradient given by the
regularisation parameter, α,

∂ 2ED
∂wij

∂ yn
i
∂wij

xn
j

=

=

.

and

= 0.

∂αEW
∂wij

∂ 2αEW
= sign {wij } α
∂w2
ij
The partial derivatives of the regularisation term are not d eﬁned at the origin, and so we deﬁne the
effective gradient of the regularised loss function as follows:


Note that the value of a weight may be stable at zero if the derivative of the regularisation term
dominates the derivative of the data mis ﬁt. The parameters o f the model may then be optimised,
using Newton’s method, i.e.

∂ED
+ α if wij > 0
∂wij
∂ED
∂wij − α if wij < 0
+ α if wij = 0 and ∂ED
∂ED
+ α < 0
∂wij
∂wij
∂wij − α if wij = 0 and ∂ED
∂ED
∂wij − α > 0
0
otherwise

∂L
∂wij

=

.

ij #−1
∂wij " ∂ 2ED
∂ED
wij ← wij −
∂w2
Any step that causes a change of sign in a model parameter is truncated and that parameter set to
zero. All that remains is to decide on a heuristic used to select the parameter to be optimised in each
step. In this study, we adopt the heuristic chosen by Shevade and Keerthi, in which the parameter
having the steepest gradient is selected in each iteration. The optimisation proceeds using two nested
loops, in the inner loop, only active parameters are conside red. If no further progress can be made
by optimising active parameters, the search is extended to parameters that are currently set to zero.
An optimisation strategy based on scaled conjugate gradient descent [27] has also be found to be
effective.

3 Results

The proposed sparse multinomial logistic regression method incorporating Bayesian regularisation
using a Laplace prior (SBMLR) was evaluated over a suite of well-known benchmark datasets,
against sparse multinomial logistic regression with ﬁve-f old cross-validation based optimisation of
the regularisation parameter using a simple line search (SMLR). Table 1 shows the test error rate
and cross-entropy statistics for SMLR and SBMLR methods over these datasets. Clearly, there is
little reason to prefer either model over the other in terms o f generalisation performance, as neither
consistently dominates the other, either in terms of error r ate or cross-entropy. Table 1 also shows
that the Bayesian regularisation scheme results in models with a slightly higher degree of sparsity
(i.e. the proportion of weights pruned from the model). However, the most striking aspect of the
comparison is that the Bayesian regularisation scheme is typically around two orders of magnitude
faster than the cross-validation based approach, with SBMLR being approximately ﬁve times faster
in the worst case (COVTYPE).

3.1 The Value of Probabilistic Classi ﬁcation

Probabilistic classi ﬁers, i.e. those that providing an a-posteriori estimate of the probability of class
membership, can be used in minimum risk classi ﬁcation, usin g an appropriate loss matrix to account
for the relative costs of different types of error. Probabil istic classi ﬁers allow rejection thresholds
to be set in a straight-forward manner. This is particularly useful in a medical setting, where it may
be prudent to refer a patient for further tests if the diagnos is is uncertain. Finally, the output of

Table 1: Evaluation of linear sparse multinomial logistic regression methods over a set of nine
benchmark datasets. The best results for each statistic are shown in bold. The ﬁnal column shows
the logarithm of the ratio of the training times for the SMLR and SBMLR, such that a value of 2
would indicate that SBMLR is 100 times faster than SMLR for a given benchmark dataset.

Benchmark

Covtype
Crabs
Glass
Iris
Isolet
Satimage
Viruses
Waveform
Wine

Error Rate
Cross Entropy
Sparsity
SBMLR SMLR SBMLR SMLR SBMLR SMLR
0.4312
0.9590
0.4041
0.3069
0.9733
0.4051
0.0350
0.0891
0.2708
0.0500
0.1075
0.0635
0.4700
0.9398
0.3224
0.4400
0.9912
0.3318
0.4067
0.4067
0.0792
0.0267
0.0267
0.0867
0.9311
0.1858
0.0475
0.8598
0.2641
0.0513
0.1600
0.3708
0.3694
0.1610
0.3717
0.2747
0.8118
0.1168
0.0328
0.0328
0.7632
0.1670
0.3939
0.3124
0.1290
0.3712
0.3131
0.1302
0.0225
0.0825
0.6071
0.0281
0.0827
0.5524

log10

TSMLR
TSBMLR

0.6965
2.7949
1.9445
1.9802
1.3110
1.3083
2.1118
1.8133
2.5541

p(s)
o (ωi |xn ) =

a probabilistic classi ﬁer can be adjusted after training to compensate for a difference between the
relative class frequencies in the training set and those observed in operation. Saerens [4] provides
a simple expectation-maximisation (EM) based procedure for estimating unknown operational a-
priori probabilities from the output of a probabilistic classi ﬁer
(c.f. [28]). Let pt (Ci ) represent the
a-priori probability of class Ci in the training set and pt (Ci |xn ) represent the raw output of the
classi ﬁer for the nth pattern of the test data (representing operational conditions). The operational
a-priori probabilities, po (Ci ) can then be updated iteratively via
p(s)
o (ωi )
pt (ωi ) pt (ωi |xn )
N
Xn=1
p(s)
o (ωj )
Pc
pt (ωj ) pt (ωj |xn )
j=1
beginning with p(0)
(Ci ) = pt (Ci ). Note that the labels of the test examples are not required for
o
this procedure. The adjusted estimates of a-posteriori probability are then given by the ﬁrst part
of equation (6). The training and validation sets of the COVTYPE benchmark have been arti ﬁcially
balanced, by random sampling, so that each class is represen ted by the same number of examples.
The test set consists of the unused patterns, and so the test set a-priori probabilities are both highly
disparate and very different from the training set a-priori probabilities. Figure 1 and Table 2 sum-
marise the results obtained using the raw and corrected outp uts of a linear SBMLR model on this
dataset, clearly demonstrating a key advantage of probabil istic classi ﬁers over purely discriminative
methods, for example the support vector machine (note the same procedure could be applied to the
SMLR model with similar results).

p(s)
o (ωi |xn ),

(6)

and

p(s+1)
o

(ωi ) =

1
ℓ

Table 2:
rate and average cross-
Error
entropy score for linear SBMLR models of the
COVTYPE benchmark, using the raw and cor-
rected outputs.

Statistic
Error Rate
Cross-Entropy

Raw
40.51%
0.9590

Corrected
28.57%
0.6567

y
t
i
l
i
b
a
b
o
r
p
 
i
r
o
i
r
p
−
a

0.5

0.4

0.3

0.2

0.1

0

training set
test set
estimated

1

2

3

4
class

5

6

7

Figure 1: Training set,
test set and esti-
mated a-priori probabilities for the COVTYPE
benchmark.

4 Relationship to Existing Work

The sparsity inducing Laplace density has been utilized previously in [15, 25, 26, 29–31] and
emerges as the marginal of a scale-mixture-of-Gaussians where the corresponding prior is an Expo-
nential such that
α
Z Nw (0, τ )Eτ (γ )dτ =
exp(−α|w|)
2
where Eτ (γ ) is an Exponential distribution over τ with parameter γ and α = √γ . In [29] this
hierarchical representation of the Laplace prior is utiliz ed to develop an EM style sparse binomial
probit regression algorithm. The hyper-parameter α is selected via cross-validation but in an attempt
to circumvent this requirement a Jeffreys prior is placed on τ and is used to replace the exponential
distribution in the above integral. This yields an improper parameter free prior distribution over
w which removes the explicit requirement to perform any cross -validation. However, the method
developed in [29] is restricted to binary classi ﬁcation and has compute scaling O(d3 ) which prohibits
its use on moderately high-dimensional problems.

Likewise in [13] the RVM employs a similar scale-mixture for the prior where now the Exponential
distribution is replaced by a Gamma distribution whose marg inal yields a Student prior distribution.
No attempt is made to estimate the associated hyper-parameters and these are typically set to zero
producing, as in [29], a sparsity inducing improper prior. A s with [29] the original scaling of [13] is,
at worst, O(d3 ), though more efﬁcient methods have been developed in [14]. H owever the analysis
holds only for a binary classi ﬁer and it would be non-trivial
to extend this to the multi-class domain.

A similar multinomial logistic regression model to the one proposed in this paper is employed in
[26] where the algorithm is applied to large scale classi ﬁca tion problems and yet they, as with [25],
have to resort to cross-validation in obtaining a value for the hyper-parameters of the Laplace prior.

5 Summary

In this paper we have demonstrated that the regularisation parameter used in sparse multinomial lo-
gistic regression using a Laplace prior can be integrated ou t analytically, giving similar performance
in terms of generalisation as is obtained using extensive cross-validation based model selection, but
at a greatly reduced computational expense. It is interesting to note that the SBMLR implements a
strategy that is exactly the opposite of the relevance vector machine (RVM) [13], in that it integrates
over the hyper-parameter and optimises the weights, rather than marginalising the model parameters
and optimising the hyper-parameters. It seems reasonable to suggest that this approach is feasible
in the case of the Laplace prior as the pruning action of this prior ensures that values of all of the
weights are strongly determined by the data mis ﬁt term. A sim ilar strategy has already proved ef-
fective in cancer classi ﬁcation based on gene expression mi croarray data in a binomial setting [32],
and we plan to extend this work to multi-class cancer classi ﬁ cation in the near future.

Acknowledgements

The authors thank the anonymous reviewers for their helpful and constructive comments. MG is
supported by EPSRC grant EP/C010620/1.

References

[1] P. McCullagh and J. A. Nelder. Generalized linear models, volume 37 of Monographs on Statistics and
Applied Probability. Chapman & Hall/CRC, second edition, 1989.
[2] D. W. Hosmer and S. Lemeshow. Applied logistic regression. Wiley, second edition, 2000.
[3] C. K. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory,
16(1):41–46, January 1970.
[4] M. Saerens, P. Latinne, and C. Decaestecker. Adjusting the outputs of a classiﬁer to new a priori proba-
bilities: A simple procedure. Neural Computation, 14(1):21–41, 2001.
[5] J. O. Berger. Statistical decision theory and Bayesian analysis. Springer Series in Statistics. Springer,
second edition, 1985.

[6] J. Zhu and T. Hastie. Classi ﬁcation of gene microarrays by penalize d logistic regression. Biostatistics,
5(3):427–443, 2004.
[7] X. Zhou, X. Wang, and E. R. Dougherty. Multi-class cancer classiﬁ cation using multinomial probit
regression with Bayesian gene selection. IEE Proceedings - Systems Biology, 153(2):70–76, March 2006.
[8] T. Zhang and F. J. Oles. Text categorization based on regularised linear classiﬁcation methods.
Informa-
tion Retrieval, 4(1):5–31, April 2001.
[9] L. Narlikar and A. J. Hartemink. Sequence features of DNA binding sites reveal structural class of
associated transcription factor. Bioinformatics, 22(2):157–163, 2006.
[10] M. J. L. Orr. Regularisation in the selection of radial basis function centres. Neural Computation,
7(3):606–623, 1995.
[11] Y. Grandvalet. Least absolute shrinkage is equivalent to quadra tic penalisation.
In L. Niklasson,
M. Bod ´en, and T. Ziemske, editors, Proceedings of the International Conference on Artiﬁcial Neural
Networks, Perspectives in Neural Computing, pages 201–206, Sk ¨ovde, Sweeden, September 2–4 1998.
Springer.
[12] Y. Grandvalet and S. Canu. Outcomes of the quivalence of adaptive ridge with least absolute shrinkage.
In Advances in Neural Information Processing Systems, volume 11. MIT Press, 1999.
[13] M. E. Tipping. Sparse Bayesian learning and the Relevance Vector Machine. Journal of Machine Learn-
ing Research, 1:211–244, 2001.
[14] A. C. Faul and M. E. Tipping. Fast marginal likelihood maximisation for sparse Bayesian models. In C. M.
Bishop and B. J. Frey, editors, Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence
and Statistics, Key West, FL, USA, 3–6 January 2003.
[15] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society
- Series B, 58:267–288, 1996.
[16] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics,
32(2):407–499, 2004.
[17] P. M. Williams. Bayesian regularization and pruning using a Laplace prior. Neural Computation,
7(1):117–143, 1995.
[18] C. M. Bishop. Neural networks for pattern recognition. Oxford University Press, 1995.
[19] J. S. Bridle. Probabilistic interpretation of feedforward classiﬁcatio n network outputs, with relationships
to statistical pattern recognition. In F. Fogelman Souli ´e and J. H ´erault, editors, Neurocomputing: Algo-
rithms, architectures and applications, pages 227–236. Springer-Verlag, New York, 1990.
[20] A. N. Tikhonov and V. Y. Arsenin. Solutions of ill-posed problems. John Wiley, New York, 1977.
[21] S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilema. Neural
Computation, 4(1):1–58, 1992.
[22] D. J. C. MacKay. The evidence framework applied to classiﬁcation networks. Neural Computation,
4(5):720–736, 1992.
[23] W. L. Buntine and A. S. Weigend. Bayesian back-propagation. Complex Systems, 5:603–643, 1991.
[24] I. S. Gradshteyn and I. M. Ryzhic. Table of Integrals, Series and Products. Academic Press, ﬁfth edition,
1994.
[25] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm fo r gene selection using sparse logistic
regression. Bioinformatics, 19(17):2246–2253, 2003.
[26] D. Madigan, A. Genkin, D. D. Lewis, and D. Fradkin. Bayesian multinomial logistic regression for author
identi ﬁcation. In AIP Conference Proceedings, volume 803, pages 509–516, 2005.
[27] P. M. Williams. A Marquardt algorithm for choosing the step size in backpropagation learning with
conjugate gradients. Technical Report CSRP-229, University of Sussex, February 1991.
[28] G. J. McLachlan. Discriminant analysis and statistical pattern recognition. Wiley, 1992.
[29] M. Figueiredo. Adaptive sparseness for supervised learning. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 25(9):1150–1159, September 2003.
[30] B. Krishnapuram, L. Carin, M. A. T. Figueiredo, and A. J. Hartemink. Sprse multinomial logistic regres-
sion: Fast algorithms and generalisation bounds. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(6):957–968, June 2005.
[31] J. M. Bioucas-Dias, M. A. T. Figueiredo, and J. P. Oliveira. Adaptive total variation image deconvolution:
A majorization-minimization approach. In Proceedings of the European Signal Processing Conference
(EUSIPCO’2006), Florence, Italy, September 2006.
[32] G. C. Cawley and N. L. C. Talbot. Gene selection in cancer classiﬁca tion using sparse logistic regression
with Bayesian regularisation. Bioinformatics, 22(19):2348–2355, October 2006.


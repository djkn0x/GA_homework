A Complexity-Distortion Approach to
Joint Pattern Alignment

Andrea Vedaldi
Stefano Soatto
Department of Computer Science
University of California at Los Angeles
Los Angeles, CA 90035
{vedaldi,soatto}@cs.ucla.edu

Abstract

Image Congealing (IC) is a non-parametric method for the joint alignment of a col-
lection of images affected by systematic and unwanted deformations. The method
attempts to undo the deformations by minimizing a measure of complexity of the
image ensemble, such as the averaged per-pixel entropy. This enables alignment
without an explicit model of the aligned dataset as required by other methods (e.g.
transformed component analysis). While IC is simple and general, it may intro-
duce degenerate solutions when the transformations allow minimizing the com-
plexity of the data by collapsing them to a constant. Such solutions need to be
explicitly removed by regularization.
In this paper we propose an alternative formulation which solves this regulariza-
tion issue on a more principled ground. We make the simple observation that
alignment should simplify the data while preserving the useful information car-
ried by them. Therefore we trade off ﬁdelity and complexity of the aligned en-
semble rather than minimizing the complexity alone. This eliminates the need
for an explicit regularization of the transformations, and has a number of other
useful properties such as noise suppression. We show the modeling and computa-
tional beneﬁts of the approach to the some of the problems on which IC has been
demonstrated.

1 Introduction

Joint pattern alignment attempts to remove from an ensemble of patterns the effect of nuisance
transformations of a systematic nature. The aligned patterns have then a simpler structure and can
be processed more easily. Joint pattern alignment is not the same problem as aligning a pattern to
another; instead all the patterns are projected to a common “reference” (usually a subspace) which
is unknown and needs to be discovered in the process.
Joint pattern alignment is useful in many applications and has been addressed by several authors.
Here we only review the methods that are most related the present work.
Transform Component Analysis [7] (TCA) explicitly models the aligned ensemble as a Gaussian
linear subspace of patterns. In fact, TCA is a direct extension of Probabilistic Principal Component
Analysis (PPCA) [10]: Patterns are generated as in standard PPCA and additional hidden layers
model the nuisance deformations. Expectation-maximization is used to learn the model from data
which result in their alignment. Unfortunately the method requires the space of transformations to
be quantized and it is not clear how well the approach could scale to complex scenarios.
Image Congealing (IC) [9] takes a different perspective. The idea is that, as the nuisance deforma-
tions should increase the complexity of the data, one should be able to identify and undo them by

contrasting this effect. Thus IC transforms the data to minimize an appropriate measure of the “com-
plexity” of the ensemble. With respect to TCA, IC results in a lighter formulation which enables
addressing more complex transformations and makes fewer assumptions on the aligned ensemble.
An issue with the standard formulation of IC is that it does not require the aligned data to be a faithful
representation of the original data. Thus simplifying the data might not only remove the nuisance
factors, but also the useful information carried by the patterns. For example, if entropy is used to
measure complexity, a typical degenerate solution is obtained by mapping all the data to a constant,
which results in minimum (null) entropy. Such solutions are avoided by explicitly regularizing the
transformations, in ways that are however rather arbitrary [9].
One should instead search for an optimal compromise between complexity of the simpliﬁed data
and preservation of the useful information (Sect. 2). This approach is not only more direct, but also
conceptually more straightforward as no ad hoc regularization needs to be introduced. We illus-
trate some of its relationship with rate-distortion theory (Sect. 2.1) and information bottleneck [2]
(Sect. 2.2) and we contrast it to IC (Sect. 2.4).
In Sect. 3 we specialize our model to the problem of image alignment as done in [9]. For this case,
we show that the new model has the same computational complexity of IC (Sect. 3.1). We also show
that a Gauss-Newton based algorithm is possible, which is useful to converge quickly during the ﬁnal
stage of the optimization (Sect. 3.2; in a similar context a descent based algorithm was introduced
in [1]). In Sect. 4 we illustrate the practical behavior of the algorithm, showing how the complexity-
distortion compromise affects the ﬁnal solution. In particular, our results compare favorably with
the ones of [9], with the added simplicity and other beneﬁts, such as noise suppression.

2 Problem formulation

We formulate joint pattern alignment as the problem of ﬁnding a deformed pattern ensemble which
is simpler but faithful to the original data. This is similar to a lossy compression problem [5, 4, 3]
and is in fact equivalent to it in some cases (Sect. 2.1).
A pattern (or data) ensemble x ∈ X is a random variable with density p(x). Similarly, an aligned
ensemble or alignment y ∈ X of the ensemble x is another variable y that has conditional statistic
p(y |x). We seek for an alignment that is “simpler” than x but “faithful” to x. The complexity R
of the alignment y is measured by an operator R = H (y) such as, for example, the entropy of the
random variable y (but we will see other options). The cost of representing x by y is expressed by a
distortion function d(x, y) ∈ R+ and the faithfulness of the alignment y is quantiﬁed as the expected
distortion D = E [d(x, y)].
Consider a class W of deformations w : X → X acting on the patterns X .
In order for the
alignment y to factor out W we consider a distortion function which is invariant to the action of W ;
in particular, given a base distortion d0 (x, y), we consider the deformation invariant distortion
d(x, y) = min
w∈W d0 (x, w(y))
Thus an aligned pattern y is faithful to a deformed pattern x if it is possible to map y to x by a
nuisance deformation w .
Figuring out the best alignment y boils down in optimizing p(y |x) for complexity and distortion.
However, this require trading off complexity and distortion and there is no unique way of doing
so. The distortion-complexity function D(R) gives the best distortion D that can be achieved by
alignments of complexity R. All such distortion-optimal alignments are equally good in principle,
and it is the application that poses an upper bound on the acceptable distortion.
D(R) can be computed by optimizing the distortion D w.r.t. p(y |x) while keeping constant the
complexity R. However it is usually easier optimize the Lagrangian

min
p(y |x)
whose optimum is attained where the derivative of D(R) is equal to −λ. Then by varying λ one
spans the graph of D(R) and ﬁnds all the optimal alignments for given complexities.

D + λR

(1)

2.1 Relation to rate-distortion and entropy constrained vector quantization

If one chooses the mutual information I (x, y) as complexity measure H (y) in eq. (1), then (1)
becomes a rate-distortion problem and the function D(R) a rate-distortion function [5]. The for-
mulation is valid both for discrete and continuous spaces X , but yields to a mapping p(y |x) that
is genuinely stochastic. Therefore the alignment y of a pattern x is in general not unique. This
is because in rate-distortion y is an auxiliary variable used to derive a deterministic code for long
sequences (x1 , . . . , xn ) of data, not for data x in isolation.
In contrast, entropy constrained vector quantization [4, 3] assumes that y is ﬁnite (i.e. that it spans
a ﬁnite subset of X ) and that it is functionally determined by x (i.e. y = y(x)). Then it measures
the complexity of y as the (discrete) entropy H (y). This is analogous to a rate-distortion problem,
except that one searches for a “single letter” optimal coding y of x rather than an optimal coding
for long sequences (x1 , . . . , xn ). Unlike rate-distortion, however, the aligned ensemble y is discrete
even if the ensemble x is continuous.

2.2 Relation to information bottleneck

Information Bottleneck (IB) [2] is a special rate-distortion problem in which one compresses a
variable x while preserving the information carried by x about another variable z , representing the
task of interest. In this sense IB is similar to the idea proposed here. By designing an appropriate
distribution p(x, z ) it may also be possible to obtain an alignment effect similar to the one we seek
here. For example, if W is a group of transformations, one may deﬁne z = z (x) = {w(x) : w ∈
W }, for which z is indifferent exactly to the deformations w of x.

2.3 Alternative measures of complexity

Instead of the entropy H (y) or the mutual information I (x, y) we can use alternative measures
of complexity that yield to more convenient computations. An example is the averaged-per-pixel
entropy introduced by IC [9] and discussed in Sect. 3. Generalizing this idea, we assume that the
aligned data y depend functionally on the patterns x (i.e. y = y(x)) and we express the complexity
of y as the total entropy of lower dimensional projections φ1 (y), . . . , φM (y), φi : X → Rk of the
ensemble.
Distortion and entropies are estimated empirically and non-parametrically. Concretely, given an
ensemble x1 , . . . , xK ∈ X of patterns, we recover transformations w1 , . . . , wK ∈ W and aligned
patterns y1 , . . . , yK ∈ X that minimize
MX
KX
KX
1
K
j=1
i=1
i=1
where the densities pj (φj (y)) are estimated from the samples φj (y1 ), . . . , φj (yK ) by histogram-
ming (discrete case) or by a Parzen estimator [6] with Gaussian kernel gσ (y) of variance σ (contin-
NX
uous case1 ), i.e.
i=1

gσ (φj (y) − φj (yi )).

d(xi , wi (yi )) − λ

log pj (φj (yi )),

1
K

pj (φj (y)) =

1
N

2.4 Comparison to image congealing
In IC [9], given data x1 , . . . , xK ∈ X , one looks for transformations v : X → X , x 7→ y such that
the density p(y) estimated from samples y1 = v1 (x1 ), . . . , yK = vK (xK ) has minimum entropy. If
the transformations enable to do so, one can minimize the entropy by mapping all the patterns to a
X
constant; to avoid this one considers the regularized cost function
i

H (y) + α

R(vi )

(2)

1The Parzen estimator implies that the differential entropy of the distributions pj is always lower bounded
by the entropy of the kernel gσ . This prevents the differential entropy to have arbitrary small negative values.

where R(v) is a term penalizing unacceptable deformations. Compared to IC, in our formulation:
I The distortion term E [d(x, y)] substitutes the arbitrary regularization R(v).
I The aligned patterns y are not obtained by deforming the patterns x; instead y is obtained as a
simpliﬁcation of x within an acceptable level of distortion. This fact induces a noise-cancellation
effect (Sect. 4).
I The transformations w can be rather general, even non-invertible. IC can use complex trans-
formations too, but most likely these would need to be heavily regularized as they would tend to
annihilate the patterns.

3 Application to joint image alignment

We apply our model to the problem of removing a family of geometric distortions from images. This
is the same application for which IC [7] was proposed in the ﬁrst place.
We are given a set I1 (x), . . . , IK (x) of digital images (pattern ensemble) deﬁned on a regular lattice
x ∈ Λ ⊂ R2 and with range in [0, 1]. The images may be affected by parametric transformations
wi (·) = w(·; qi ) : R2 → R2 , so that
x ∈ Λ
Ii (x) = Ti (wx) + ni (x),
for templates (aligned ensemble2 ) Ti (y), y ∈ Λ and residuals ni (x). Here qi is the vector of param-
eters of the transformation wi (for example, wi might be a 2-D afﬁne transformation y = Lx + l
and qi the vector q = [L11 L21 L12 L22
l2 ]).
l1
The templates Ti (y), y ∈ Λ are digital images themselves. In order to deﬁne Ti (wx) when wx 6∈ Λ,
bilinear interpolation and zero-padding are used. Therefore the symbol Ti (wix) really denotes the
quantity
x ∈ Λ
T (wix) = A(x; wi )Ti ,
where A(x; wi ) is a row vector of mixing coefﬁcients determined by wi and and the interpolation
method being used and Ti is the vector obtained by stacking the pixels of the template Ti (y), y ∈ Λ.
We will also use the notation wi ◦ Ti = A(wi )Ti where the left hand side is the stacking of the
warped template T (wix), x ∈ Λ and A(wi ) is the matrix whose rows are the vectors A(x; wi ) for
The distortion is deﬁned to be the squared l2 norm of the residual d(Ii , w ◦ Ti ) = P
x ∈ Λ.
x∈Λ (Ii (x) −
Ti (wix))2 . The complexity of the aligned ensemble T (y), y ∈ Λ is computed as in Sect. 2.3 by
projecting on the image pixels and averaging their entropies (this is equivalent to assuming that the
pixels are statistically independent). For each pixel y ∈ Λ a density p(T (y) = t), t ∈ [0, 1] is esti-
mated non parametrically from the data {T1 (y), . . . , TK (y)} (we use Parzen window as explained
KX
in Sect. 2.3). The complexity of a pixel is thus
H (T (y)) = − 1
K
i=1
Finally the overall cost function is obtained by summing over all pixels and averaging over all
X
KX
X
KX
images:
(Ii (x) − Ti (wix))2 − λ
x∈Λ
y∈Λ
i=1
i=1

L(w1 , . . . , wK , T1 , . . . , TK ) =

log p(Ti (y)).

1
K

1
K

log p(Ti (y)).

(3)

3.1 Basic search

In this section we show how the optimization algorithm from [7] can be adapted to work with the
new formulation. This algorithm is a simple coordinate maximization in the dimensions of the
search space:

2With respect to Sect. 2 the patterns xi are now the images Ii and the alignment y are the templates Ti .

1: Estimate the probabilities p(T (y)), y ∈ Λ from the templates {Ti (y) : i =
1, . . . , K }
2: For each pattern i = 1, . . . , K and for each component qj i of the parameter
vector qi , try a few values of qj i . For each value re-compute the cost func-
tion (3) and keep the best.
3: Repeat, reﬁning the sampling step of the parameters.

This algorithm is appropriate if the dimensionality of the parameter vector q is reasonably small.
Here we consider afﬁne transformations for the sake of the illustration, so that q is six-dimensional.
estimating the probabilities p(Ti (y))
In (1.)
and (2.)
and the
cost
function
L(w1 , . . . , wK , T1 , . . . , TK ) requires to know Ti (y). As a ﬁrst order approximation (as the
invertible3 . Eventually all we do is substituting the regularization term P
ﬁnal result will be reﬁned by Gauss-Newton as explained in the next Section), we bypass this
◦ Ii , exploiting the fact that the afﬁne transformations wi are
problem and we simply set Ti = w−1
i
i R(vi ) of [9] with the
X
KX
X
KX
expected distortion
1
1
(Ii (x) − A(x; wi )A(w−1
◦ Ii (x)))2 =
(Ii (x) − wi ◦ (w−1
i
i
K
K
x∈Λ
x∈Λ
i=1
i=1
Note that warping and un-warping the image Ii is a lossy operation even if wi is bijective because
the transformation, applied to digital images, introduces aliasing. Thus the new algorithm is simply
avoiding those transformations wi that would introduce excessive loss of ﬁdelity.

)Ii )2

3.2 Gauss-Newton search

With respect to IC, where only the transformations w1 , . . . , wK are estimated, here we compute the
templates T1 , . . . , Tk as well. While this might be not so important when a coarse approximation to
the solution has to be found (for which the algorithm of Sect. 3.1 can be used), it must be taken into
account to get reﬁned results. This can be done (with a bit of numeric care) by Gauss-Newton (GN).
Applying Gauss-Newton requires to take derivatives with respect to the pixel values Ti (y). We
exploit the fact that the variables T (y) are continuous, as opposed to [9].
We still process a single image per time, reiterating several times across the whole ensemble
{I1 (x), . . . , IK (x)}. For a given image Ii we update the warp parameters qi and the template
Ti simultaneously. We exploit the fact that, as the number K of images is usually big, the density
p(T (y)) does not change signiﬁcantly when only one of the templates Ti is being changed. There-
fore p(T (y)) can be assumed constant in the computation of the gradient and the Hessian of the cost
2∆i (x)(A(x; wi )δy ) − X
= X
= X
function (3). The gradient is given by
˙p(Ti (y))
2∆i (x)∇Ti (wix) ∂wi
∂L
∂L
(x),
∂ q>
∂ q>
p(Ti (y))
∂Ti (y)
x∈Λ
x∈Λ
y∈Λ
i
i
where ∆i (x) = Ti (wix)− Ii (x) is the reconstruction residual, A(x; wi ) is the linear map introduced
in Sect. 3 and δy = δ(z − y) is the 2-D discrete delta function centered on y , encoded as a vector.
The approximated Hessian of the cost function (3) can be obtained as follows. First, we use the
≈ X
Gauss-Newton approximation for the derivative w.r.t. the transformation parameters qi
2 ∂w>
∂ 2L
(x)∇>Ti (wix)∇Ti (wix) ∂wi
i
∂ qi∂ q>
∂ q>
∂ qi
x∈Λ
i
i
2(A(x; wi )δy )2 − X
X
¨p(Ti (y))p(Ti (y)) − ˙p(Ti (y))2
X
p(Ti (y))2
x∈Λ
y∈Λ
X
X
2∆i (x)A(x; wi ) ˆD1 δy D2 δy
x∈Λ
x∈Λ
x∈Λ
3Our criterion avoids implicitly non-invertible afﬁne transformations as they yield highly distorted codes.

We then have
∂ 2L
∂Ti (y)2 =
∂ 2L
∂Ti (y)∂Ti (z )

2(A(x; wi )δy )∇Ti (wix)

∂ 2L
∂Ti (y)∂ q> =

˜ ∂wi
∂ q>
i

(x)

=

2(A(x; wi )δy )(A(x; wi )δz )

∂wi
∂ q>
i

+

Figure 1: Toy example. Top left. We distort the patterns by applying translations drawn uniformly
from the 8-shaped region (the center corresponds to the null translation). Top. We show the gradient
based algorithm while it gradually aligns the patterns by reducing the complexity of the alignment
y . Dark areas correspond to high values of the density of the alignment; we also superimpose the
trajectory of one of the patterns. Unfortunately the gradient based algorithm, being a local tech-
nique, gets trapped in two local modes (the modes can however be fused in a post-processing stage).
Bottom. The basic algorithm completely eliminates the effect of the nuisance transformations doing
a better job of avoiding local minima. Although for this simple problem the basic search is more
effective, on more difﬁcult scenarios the extra complexity of the Gauss-Newton search pays off (see
Sect. 4).

where D1 is the discrete linear operator used to compute the derivative of Ti (y) along its ﬁrst dimen-
sion and D2 the analogous operator for the second dimension. The second term of the last equation
gives a very small contribution and can be dropped.
δθ> (cid:18) ∂ 2L
(cid:19)
The equations are all straightforward and result in la linear system
= − ∂L
∂ θ>
∂ θ∂ θ>
where the vector θ> = (cid:2)q>T (y1 ) . . . T (yn )(cid:3) has size in the order of the number of pixels of the
template T (y), y ∈ Λ. While this system is large, it is also extremely sparse an can be solved rather
efﬁciently by standard methods [8].

4 Experiments

The ﬁrst experiment (Fig.1) is a toy problem illustrating our method. We collect K patterns xi ,
i = 1, . . . , K which are arrays of M 2D points xi = (x1i , . . . , xM i ). Such points are generated
by drawing M i.i.d. samples from a 2-D Gaussian distribution and adding a random translation
wi ∈ R2 to them. The distribution of the translations wi is generic (in the example wi is drawn
sum of the Euclidean distances Pm
uniformly from an 8-shaped region of the plane): This is not a problem as we do not need to make
any particular assumptions on w besides that it is a translation. The distortion d(xi , yi ) is simply the
j=1 kyj i + wi − xj i k2 between the patterns xi and the transformed
as p(yi ) = Q
codes wi (yi ) = (y1i + wi , . . . , ymi + wi ). The distribution p(yi ) of the codes is assumed to factorize
j=1 p(yj i ) where the p(yj i ) are identical densities estimated by Parzen window from
all the available samples {yj i , j = 1, . . . , M , i = 1, . . . , K }.
In the second experiment (Fig. 2) we align hand-written digits extracted from the NIST Special
Database 19. The results (Fig. 3) should be compared to the ones from [9]: They are of analogous
quality, but they were achieved without regularizing the class of admissible transformations. Despite
this, we did not observe any of the aligned patterns to collapse. In Fig. 4 we show the effect of
choosing different values of the parameter λ in the cost function (3). As λ is increased, the alignment
complexity is reduced and the ﬁdelity of the alignment is degraded. By an appropriate choice of λ,
the alignment can be regarded as a “restoration” or “canonization” of the pattern which abstracts
from details of the speciﬁc instance.

Figure 2: Basic vs GN image alignment algorithms. Left. We show the results of applying the basic
image alignment algorithm of Sect. 3.1. The patterns are zeroes from the NIST Special Database
19. We show in writing order: The expected value E [T (y)]; the per-pixel entropy H (T (y)) (it can
be negative as it is differential); a 3-D plot of the same function H (T (y)); the distortion-complexity
diagram as the algorithm minimizes the function D + λR (in green we show some lines of constant
cost); the probability p(T (y) = l) as l ∈ [0, 1] and y varies along the middle scan-line; and the per-
pixel distortion D(x) = E [(I (x)−T (wx))2 ]. Right. We demonstrate the GN algorithm of Sect. 3.2.
The algorithm achieves a signiﬁcantly better solution in term of the cost function (3). Moreover GN
converges in only two sweeps of the dataset, while the basic algorithm after 10 sweeps is still slowly
moving. This is due to the fact that GN selects both the best search direction and step size, resulting
in a more efﬁcient search strategy.

Figure 3: Aligned patterns. Left. A few patterns from NIST Special Database 19. Middle. Basic
algorithm: Results are very similar to [9], except that no regularization on the transformations is
used. Right. GN algorithm: Patterns achieve a better alignment due to the more efﬁcient search
strategy; they also appear to be much more “regular” due to the noise cancellation effect discussed
in Fig. 4. Bottom. More examples of patterns before and after GN alignment.
5 Conclusions

IC is a useful algorithm for joint pattern alignment, both robust and ﬂexible. In this paper we showed
that the original formulation can be improved by realizing that alignment should result in a simpliﬁed
representation of the useful information carried by the patterns rather than a simpliﬁcation of the
patterns. This results in a formulation that does not require inventing regularization terms in order
to prevent degenerate solutions. We also showed that Gauss-Newton can be successfully applied to
this problem for the case of image alignment and that this is in some regards more effective than the
original IC algorithm.

Expected value per pixel510152025051015202530Entropy per pixel5101520255101520250204002040−2.5−2−1.5−1Entropy per pixel−2.2−2−1.800.511.5x 10−4Distortion−rate diagramRateDistortionp(T(y)) along middle scanline51015202500.20.40.60.8Distortion per pixel510152025510152025Expected value per pixel510152025051015202530Entropy per pixel5101520255101520250204002040−2.5−2−1.5−1Entropy per pixel−2.2−2−1.801234x 10−4Distortion−rate diagramRateDistortionp(T(y)) along middle scanline51015202500.20.40.60.8Distortion per pixel510152025510152025(a) Distortion-Complexity

(b) Not aligned

(c) Aligned

Figure 4: Distortion-complexity balance. We illustrate the effect of varying the parameter λ in (3).
(a) Estimated distortion-complexity function D(R). The green (dashed) lines have slope equal to
λ and should be tangent to D(R) (Sect. 2). (b) We show the alignment T (wix) of eight patterns
(rows) as λ is increased (columns). In order to reduce the entropy of the alignment, the algorithm
“forgets” about speciﬁc details of each glyph. (c) The same as (b), but aligned.

Acknowledgments

We would like to acknowledge the support of AFOSR FA9550-06-1-0138 and ONR N00014-03-1-
0850.

References
[1] P. Ahammad, C. L. Harmon, A. Hammonds, S. S. Sastry, and G. M. Rubin. Joint nonparametric
alignment for analizing spatial gene expression patterns in drosophila imaginal discs. In Proc.
CVPR, 2005.
[2] K. Branson. The information bottleneck method. Lecture Slides, 2003.
[3] J. Buhmann and H. K ¨uhnel. Vector quantization with complexity costs. IEEE Trans. on Infor-
mation Theory, 39, 1993.
[4] P. A. Chou, T. Lookabaugh, and R. M. Gray. Entropy-constrained vector quantization. In 37,
editor, IEEE Trans. on Acoustics, Speech, and Signal Processing, volume 1, 1989.
[5] T. M. Cover and J. A. Thomson. Elements of Information Theory. Wiley, 2006.
[6] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. Wiley Inerscience, 2001.
[7] B. J. Frey and N. Jojic. Transformation-invariant clustering and dimensionality reduction using
EM. PAMI, 2000.
[8] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press,
1996.
[9] E. G. Learned-Miller. Data driven image models through continuous joint alignment. PAMI,
28(2), 2006.
[10] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of The
Royal Statistical Society, Series B, 61(3), 1999.

−2.3−2.2−2.1−2−1.9−1.8012345x 10−4RateDistortion
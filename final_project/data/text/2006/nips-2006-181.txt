Fast Computation of Graph Kernels

S.V. N. Vishwanathan
svn.vishwanathan@nicta.com.au
Statistical Machine Learning, National ICT Australia,
Locked Bag 8001, Canberra ACT 2601, Australia
Research School of Information Sciences & Engineering
Australian National University, Canberra ACT 0200, Australia

Karsten M. Borgwardt
borgwardt@dbs.ifi.lmu.de
Institute for Computer Science, Ludwig-Maximilians-University Munich
Oettingenstr. 67, 80538 Munich, Germany

Nicol N. Schraudolph
nic.schraudolph@nicta.com.au
Statistical Machine Learning, National ICT Australia
Locked Bag 8001, Canberra ACT 2601, Australia
Research School of Information Sciences & Engineering
Australian National University, Canberra ACT 0200, Australia

Abstract

Using extensions of linear algebra concepts to Reproducing Kernel Hilbert Spaces
(RKHS), we deﬁne a unifying framework for random walk kernels on graphs. Re-
duction to a Sylvester equation allows us to compute many of these kernels in
O(n3 ) worst-case time. This includes kernels whose previous worst-case time
complexity was O(n6 ), such as the geometric kernels of G ¨artner et al. [1] and
the marginal graph kernels of Kashima et al. [2]. Our algebra in RKHS allow us
to exploit sparsity in directed and undirected graphs more effectively than previ-
ous methods, yielding sub-cubic computational complexity when combined with
conjugate gradient solvers or ﬁxed-point iterations. Experiments on graphs from
bioinformatics and other application domains show that our algorithms are often
more than 1000 times faster than existing approaches.

1

Introduction

Machine learning in domains such as bioinformatics, drug discovery, and web data mining involves
the study of relationships between objects. Graphs are natural data structures to model such rela-
tions, with nodes representing objects and edges the relationships between them. In this context, one
often encounters the question: How similar are two graphs?
Simple ways of comparing graphs which are based on pairwise comparison of nodes or edges, are
possible in quadratic time, yet may neglect information represented by the structure of the graph.
Graph kernels, as originally proposed by G ¨artner et al. [1], Kashima et al. [2], Borgwardt et al. [3],
take the structure of the graph into account. They work by counting the number of common random
walks between two graphs. Even though the number of common random walks could potentially be

exponential, polynomial time algorithms exist for computing these kernels. Unfortunately for the
practitioner, these kernels are still prohibitively expensive since their computation scales as O(n6 ),
where n is the number of vertices in the input graphs. This severely limits their applicability to
large-scale problems, as commonly found in areas such as bioinformatics.
In this paper, we extend common concepts from linear algebra to Reproducing Kernel Hilbert Spaces
(RKHS), and use these extensions to deﬁne a unifying framework for random walk kernels. We show
that computing many random walk graph kernels including those of G ¨artner et al. [1] and Kashima
et al. [2] can be reduced to the problem of solving a large linear system, which can then be solved
efﬁciently by a variety of methods which exploit the structure of the problem.

2 Extending Linear Algebra to RKHS
Let φ : X → H denote the feature map from an input space X to the RKHS H associated with the
kernel κ(x, x0 ) = hφ(x), φ(x0 )iH . Given an n by m matrix X ∈ X n×m of elements Xij ∈ X , we
extend φ to matrix arguments by deﬁning Φ : X n×m → Hn×m via [Φ(X )]ij := φ(Xij ). We can
now borrow concepts from tensor calculus to extend certain linear algebra operations to H:
Deﬁnition 1 Let A ∈ X n×m, B ∈ X m×p, and C ∈ Rm×p. The matrix products Φ(A)Φ(B ) ∈ Rn×p
[Φ(A) C ]ik := X
[Φ(A)Φ(B )]ik := X
and Φ(A) C ∈ Hn×p are
hφ(Aij ), φ(Bjk )iH and
φ(Aij ) Cjk .
j
j
 A11B A12B . . . A1mB
 , vec(A) :=
 A∗1
 ,
Given A ∈ Rn×m and B ∈ Rp×q the Kronecker product A ⊗ B ∈ Rnp×mq and vec operator are
deﬁned as
...
...
...
...
...
A∗m
An1B An2B . . . AnmB
where A∗j denotes the j -th column of A. They are linked by the well-known property:
vec(ABC ) = (C > ⊗ A) vec(B ).
(2)
Deﬁnition 2 Let A ∈ X n×m and B ∈ X p×q . The Kronecker product Φ(A) ⊗ Φ(B ) ∈ Rnp×mq is
[Φ(A) ⊗ Φ(B )]ip+k,j q+l := hφ(Aij ), φ(Bkl )iH .
(3)
It is easily shown that the above extensions to RKHS obey an analogue of (2):
Lemma 1 If A ∈ X n×m , B ∈ Rm×p , and C ∈ X p×q , then
vec(Φ(A) B Φ(C )) = (Φ(C )> ⊗ Φ(A)) vec(B ).

A ⊗ B :=

(1)

(4)

If p = q = n = m, direct computation of the right hand side of (4) requires O(n4 ) kernel evalua-
tions. For an arbitrary kernel the left hand side also requires a similar effort. But, if the RKHS H is
isomorphic to Rr , in other words the feature map φ(·) ∈ Rr , the left hand side of (4) is easily com-
puted in O(n3 r) operations. Our efﬁcient computation schemes described in Section 4 will exploit
this observation.

3 Random Walk Kernels

Random walk kernels on graphs are based on a simple idea: Given a pair of graphs perform a random
walk on both of them and count the number of matching walks [1, 2, 3]. These kernels mainly differ
in the way the similarity between random walks is computed. For instance, G ¨artner et al. [1] count
the number of nodes in the random walk which have the same label. They also include a decay factor
to ensure convergence. Kashima et al. [2], and Borgwardt et al. [3] on the other hand, use a kernel
deﬁned on nodes and edges in order to compute similarity between random walks, and deﬁne an
initial probability distribution over nodes in order to ensure convergence. In this section we present
a unifying framework which includes the above mentioned kernels as special cases.

3.1 Notation

We use ei to denote the i-th standard basis (i.e., a vector of all zeros with the i-th entry set to one), e
to denote a vector with all entries set to one, 0 to denote the vector of all zeros, and I to denote the
identity matrix. When it is clear from context we will not mention the dimensions of these vectors
and matrices.
A graph G ∈ G consists of an ordered and ﬁnite set of n vertices V denoted by {v1 , v2 , . . . , vn }, and
a ﬁnite set of edges E ⊂ V × V . A vertex vi is said to be a neighbor of another vertex vj if they are
connected by an edge. G is said to be undirected if (vi , vj ) ∈ E ⇐⇒ (vj , vi ) ∈ E for all edges.
The unnormalized adjacency matrix of G is an n×n real matrix P with Pij = 1 if (vi , vj ) ∈ E , and
0 otherwise. If G is weighted then P can contain non-negative entries other than zeros and ones,
Let D be an n × n diagonal matrix with entries Dii = P
i.e., Pij ∈ (0, ∞) if (vi , vj ) ∈ E and zero otherwise.
j Pij . The matrix A := P D−1 is then
called the normalized adjacency matrix, or simply adjacency matrix. A walk w on G is a sequence
of indices w1 , w2 , . . . wt+1 where (vwi , vwi+1 ) ∈ E for all 1 ≤ i ≤ t. The length of a walk is equal
to the number of edges encountered during the walk (here: t). A graph is said to be connected if any
two pairs of vertices can be connected by a walk; here we always work with connected graphs. A
random walk is a walk where P(wi+1 |w1 , . . . wi ) = Awi ,wi+1 , i.e., the probability at wi of picking
wi+1 next is directly proportional to the weight of the edge (vwi , vwi+1 ). The t-th power of the
transition matrix A describes the probability of t-length walks. In other words, [At ]ij denotes the
probability of a transition from vertex vi to vertex vj via a walk of length t. We use this intuition to
deﬁne random walk kernels on graphs.
Let X be a set of labels which includes the special label . Every edge labeled graph G is associated
with a label matrix L ∈ X n×n , such that Lij =  iff (vi , vj ) /∈ E , in other words only those edges
which are present in the graph get a non- label. Let H be the RKHS endowed with the kernel
κ : X × X → R, and let φ : X → H denote the corresponding feature map which maps  to the
zero element of H. We use Φ(L) to denote the feature matrix of G. For ease of exposition we do
not consider labels on vertices here, though our results hold for that case as well. Henceforth we use
the term labeled graph to denote an edge-labeled graph.

3.2 Product Graphs
Given two graphs G(V , E ) and G0 (V 0 , E 0 ), the product graph G× (V× , E× ) is a graph with nn0
vertices, each representing a pair of vertices from G and G0 , respectively. An edge exists in E× iff
the corresponding vertices are adjacent in both G and G0 . Thus
V× = {(vi , v 0
i0 ) : vi ∈ V ∧ v 0
i0 ∈ V 0 },
(5)
j 0 )) : (vi , vj ) ∈ E ∧ (v 0
E× = {((vi ,v 0
j 0 ) ∈ E 0 }.
i0, v 0
i0 ), (vj ,v 0
(6)
If A and A0 are the adjacency matrices of G and G0 , respectively, the adjacency matrix of the product
graph G× is A× = A ⊗ A0 . An edge exists in the product graph iff an edge exits in both G and
G0 , therefore performing a simultaneous random walk on G and G0 is equivalent to performing a
random walk on the product graph [4].
Let p and p0 denote initial probability distributions over vertices of G and G0 . Then the initial
probability distribution p× of the product graph is p× := p ⊗ p0 . Likewise, if q and q 0 denote
stopping probabilities (i.e., the probability that a random walk ends at a given vertex), the stopping
probability q× of the product graph is q× := q ⊗ q 0 .
If G and G0 are edge-labeled, we can associate a weight matrix W× ∈ Rnn0×nn0
with G× , using
our Kronecker product in RKHS (Deﬁnition 2): W× = Φ(L) ⊗ Φ(L0 ). As a consequence of the
deﬁnition of Φ(L) and Φ(L0 ), the entries of W× are non-zero only if the corresponding edge exists
in the product graph. The weight matrix is closely related to the adjacency matrix: assume that
H = R endowed with the usual dot product, and φ(Lij ) = 1 if (vi , vj ) ∈ E or zero otherwise. Then
Φ(L) = A and Φ(L0 ) = A0 , and consequently W× = A× , i.e., the weight matrix is identical to the
adjacency matrix of the product graph.
To extend the above discussion, assume that H = Rd endowed with the usual dot product, and that
there are d distinct edge labels {1, 2, . . . , d}. For each edge (vi , vj ) ∈ E we have φ(Lij ) = el if

the edge (vi , vj ) is labeled l. All other entries of Φ(L) are set to 0. κ is therefore a delta kernel, i.e.,
its value between any two edges is one iff the labels on the edges match, and zero otherwise. The
weight matrix W× has a non-zero entry iff an edge exists in the product graph and the corresponding
edges in G and G0 have the same label. Let lA denote the adjacency matrix of the graph ﬁltered by
the label l, i.e., lAij = Aij if Lij = l and zero otherwise. Some simple algebra (omitted for the sake
dX
of brevity) shows that the weight matrix of the product graph can be written as
l=1

lA ⊗ lA0 .

W× =

(7)

3.3 Kernel Deﬁnition

Performing a random walk on the product graph G× is equivalent to performing a simultaneous
random walk on the graphs G and G0 [4]. Therefore, the (in + j, i0n0 + j 0 )-th entry of Ak× represents
the probability of simultaneous k length random walks on G (starting from vertex vi and ending in
vertex vj ) and G0 (starting from vertex v 0
i0 and ending in vertex v 0
j 0 ). The entries of W× represent
similarity between edges. The (in + j, i0n0 + j 0 )-th entry of W k× represents the similarity between
simultaneous k length random walks on G and G0 measured via the kernel function κ.
Given the weight matrix W× , initial and stopping probability distributions p× and q× , and an ap-
∞X
propriately chosen discrete measure µ, we can deﬁne a random walk kernel on G and G0 as
µ(k) q>
k(G, G0 ) :=
×W k×p× .
k=0
In order to show that (8) is a valid Mercer kernel we need the following technical lemma.
Lemma 2 ∀ k ∈ N0 : W k×p× = vec[Φ(L0 )k p0 (Φ(L)k p)> ].
Proof By induction over k . Base case: k = 0. Since Φ(L0 )0 = Φ(L)0 = I, using (2) we can write
W 0×p× = p× = (p ⊗ p0 ) vec(1) = vec(p0 1 p> ) = vec[Φ(L0 )0 p0 (Φ(L)0 p)> ].
Induction from k to k + 1: Using Lemma 1 we obtain
W k+1× p× = W×W k×p× = (Φ(L) ⊗ Φ(L0 )) vec[Φ(L0 )k p0 (Φ(L)k p)> ]
= vec[Φ(L0 )Φ(L0 )k p0 (Φ(L)k p)>Φ(L)> ] = vec[Φ(L0 )k+1 p0 (Φ(L)k+1 p)> ].
Lemma 3 If the measure µ(k) is such that (8) converges, then it deﬁnes a valid Mercer kernel.

(8)

Proof Using Lemmas 1 and 2 we can write
×W k×p× = (q ⊗ q 0 ) vec[Φ(L0 )k p0 (Φ(L)k p)> ] = vec[q 0>Φ(L0 )k p0 (Φ(L)k p)>q ]
q>
|
|
{z
}
{z
}
(q 0>Φ(L0 )k p0 )
= (q>Φ(L)k p)>
.
ψk (G0 )
ψk (G)>
Each individual term of (8) equals ψk (G)>ψk (G0 ) for some function ψk , and is therefore a valid
kernel. The lemma follows since a convex combination of kernels is itself a valid kernel.
3.4 Special Cases

k(G, G0 ) = X
A popular choice to ensure convergence of (8) is to assume µ(k) = λk for some λ > 0. If λ is
sufﬁciently small1 then (8) is well deﬁned, and we can write
× (I −λW× )−1 p× .
λk q>
×W k×p× = q>
k
Kashima et al. [2] use marginalization and probabilities of random walks to deﬁne kernels on graphs.
Given transition probability matrices P and P 0 associated with graphs G and G0 respectively, their
kernel can be written as (see Eq. 1.19, [2])
× (I −T× )−1 p× ,
k(G, G0 ) = q>
1The values of λ which ensure convergence depends on the spectrum of W× .

(10)

(9)

where T× := (vec(P ) vec(P 0 )> ) (cid:12) (Φ(L) ⊗ Φ(L0 )), using (cid:12) to denote element-wise (Hadamard)
i0 j 0 ) := Pij P 0
multiplication. The edge kernel ˆκ(Lij , L0
i0 j 0 κ(Lij , L0
i,j 0 ) with λ = 1 recovers (9).
nX
n0X
∞X
G ¨artner et al. [1] use the adjacency matrix of the product graph to deﬁne the so-called geometric
kernel
k(G, G0 ) =
i=1
j=1
k=0
To recover their kernel in our framework, assume an uniform distribution over the vertices of G and
G0 , i.e., set p = q = 1/n and p0 = q 0 = 1/n0 . The initial as well as ﬁnal probability distribution
over vertices of G× is given by p× = q× = e /(nn0 ). Setting Φ(L) := A, and hence Φ(L0 ) = A0
nX
∞X
∞X
n0X
and W× = A× , we can rewrite (8) to obtain
i=1
j=1
k=0
k=0
which recovers (11) to within a constant factor.

λk q>
× Ak×p× =

k(G, G0 ) =

1
n2n02

λk [Ak× ]ij ,

λk [Ak× ]ij .

(11)

4 Efﬁcient Computation

In this section we show that iterative methods, including those based on Sylvester equations, conju-
gate gradients, and ﬁxed-point iterations, can be used to greatly speed up the computation of (9).

4.1 Sylvester Equation Methods

(13)

(14)

X =

SiX Ti + X0

Consider the following equation, commonly known as the Sylvester or Lyapunov equation:
X = SX T + X0 .
(12)
Here, S, T , X0 ∈ Rn×n are given and we need for solve for X ∈ Rn×n . These equations can be
dX
readily solved in O(n3 ) time with freely available code [5], e.g. Matlab’s dlyap method. The
generalized Sylvester equation
i=1
can also be solved efﬁciently, albeit at a slightly higher computational cost of O(dn3 ).
X = X
We now show that if the weight matrix W× can be written as (7) then the problem of computing the
graph kernel (9) can be reduced to the problem of solving the following Sylvester equation:
iA0λ X iA> + X0 ,
X
i
where vec(X0 ) = p× . We begin by ﬂattening the above equation:
X
vec(iA0X iA> ) + p× .
vec(X ) = λ
i
(I −λ
iA ⊗ iA0 ) vec(X ) = p× ,
vec(X ) = (I −λW× )−1 p× .
i
use (7), and solve for vec(X ):
× (I −λW× )−1 p× .
q>
× vec(X ) = q>
Multiplying both sides by q>
× yields
The right-hand side of (18) is the graph kernel (9). Given the solution X of the Sylvester equation
(14), the graph kernel can be obtained as q>
× vec(X ) in O(n2 ) time. Since solving the generalized
Sylvester equation takes O(dn3 ) time, computing the graph kernel in this fashion is signiﬁcantly
faster than the O(n6 ) time required by the direct approach.
Where the number of labels d is large, the computational cost may be reduced further by computing
matrices S and T such that W× ≈ S ⊗ T . We then simply solve the simple Sylvester equation
(12) involving these matrices. Finding the nearest Kronecker product approximating a matrix such
as W× is a well-studied problem in numerical linear algebra and efﬁcient algorithms which exploit
sparsity of W× are readily available [6].

Using Lemma 1 we can rewrite (15) as

(15)

(18)

(16)

(17)

4.2 Conjugate Gradient Methods

Given a matrix M and a vector b, conjugate gradient (CG) methods solve the system of equations
M x = b efﬁciently [7]. While they are designed for symmetric positive semi-deﬁnite matrices,
CG solvers can also be used to solve other linear systems efﬁciently. They are particularly efﬁcient
if the matrix is rank deﬁcient, or has a small effective rank, i.e., number of distinct eigenvalues.
Furthermore, if computing matrix-vector products is cheap — because M is sparse, for instance —
the CG solver can be sped up signiﬁcantly [7]. Speciﬁcally, if computing M v for an arbitrary vector
v requires O(k) time, and the effective rank of the matrix is m, then a CG solver requires only
O(mk) time to solve M x = b.
The graph kernel (9) can be computed by a two-step procedure: First we solve the linear system
(I −λW× ) x = p× ,
(19)
for x, then we compute q>
×x. We now focus on efﬁcient ways to solve (19) with a CG solver. Recall
that if G and G0 contain n vertices each then W× is a n2 × n2 matrix. Directly computing the
matrix-vector product W× r , requires O(n4 ) time. Key to our speed-ups is the ability to exploit
Lemma 1 to compute this matrix-vector product more efﬁciently: Recall that W× = Φ(L) ⊗ Φ(L0 ).
Letting r = vec(R), we can use Lemma 1 to write
W× r = (Φ(L) ⊗ Φ(L0 )) vec(R) = vec(Φ(L0 )R Φ(L)> ).
(20)
If φ(·) ∈ Rr for some r , then the above matrix-vector product can be computed in O(n3 r) time. If
Φ(L) and Φ(L0 ) are sparse, however, then Φ(L0 )R Φ(L)> can be computed yet more efﬁciently: if
there are O(n) non- entries in Φ(L) and Φ(L0 ), then computing (20) requires only O(n2 ) time.

4.3 Fixed-Point Iterations

x = p× + λW×x.
(21)
Fixed-point methods begin by rewriting (19) as
Now, solving for x is equivalent to ﬁnding a ﬁxed point of the above iteration [7]. Letting xt denote
the value of x at iteration t, we set x0 := p× , then compute
xt+1 = p× + λW×xt
(22)
repeatedly until ||xt+1 − xt || < ε, where || · || denotes the Euclidean norm and ε some pre-deﬁned
tolerance. This is guaranteed to converge if all eigenvalues of λW× lie inside the unit disk; this can
be ensured by setting λ < 1/ξmax , where ξmax is the largest-magnitude eigenvalue of W× .
The above is closely related to the power method used to compute the largest eigenvalue of a matrix
[8]; efﬁcient preconditioners can also be used to speed up convergence [8]. Since each iteration
of (22) involves computation of the matrix-vector product W×xt , all speed-ups for computing the
matrix-vector product discussed in Section 4.2 are applicable here. In particular, we exploit the fact
that W× is a sum of Kronecker products to reduce the worst-case time complexity to O(n3 ) in our
experiments, in contrast to Kashima et al. [2] who computed the matrix-vector product explicitly.

5 Experiments

To assess the practical impact of our algorithmic improvements, we compared our techniques from
Section 4 with G ¨artner et al.’s [1] direct approach as a baseline. All code was written in MATLAB
Release 14, and experiments run on a 2.6 GHz Intel Pentium 4 PC with 2 GB of main memory
running Suse Linux. The Matlab function dlyap was used to solve the Sylvester equation.
By default, we used a value of λ = 0.001, and set the tolerance for both CG solver and ﬁxed-point
iteration to 10−6 for all our experiments. We used Lemma 1 to speed up matrix-vector multiplication
for both CG and ﬁxed-point methods (cf. Section 4.2). Since all our methods are exact and produce
the same kernel values (to numerical precision), we only report their runtimes below.
We tested the practical feasibility of the presented techniques on four real-world datasets whose
size mandates fast graph kernel computation; two datasets of molecular compounds (MUTAG and
PTC), and two datasets with hundreds of graphs describing protein tertiary structure (Protein and
Enzyme). Graph kernels provide useful measures of similarity for all these graphs; please refer to
the addendum for more details on these datasets, and applications for graph kernels on them.

Figure 1: Time (in seconds on a log-scale) to compute 100×100 kernel matrix for unlabeled (left)
resp. labelled (right) graphs from several datasets. Compare the conventional direct method (black)
to our fast Sylvester equation, conjugate gradient (CG), and ﬁxed-point iteration (FP) approaches.

5.1 Unlabeled Graphs

In a ﬁrst series of experiments, we compared graph topology only on our 4 datasets, i.e., without
considering node and edge labels. We report the time taken to compute the full graph kernel matrix
for various sizes (number of graphs) in Table 1 and show the results for computing a 100 × 100
sub-matrix in Figure 1 (left).
On unlabeled graphs, conjugate gradient and ﬁxed-point iteration — sped up via our Lemma 1 — are
consistently about two orders of magnitude faster than the conventional direct method. The Sylvester
approach is very competitive on smaller graphs (outperforming CG on MUTAG) but slows down
with increasing number of nodes per graph; this is because we were unable to incorporate Lemma 1
into Matlab’s black-box dlyap solver. Even so, the Sylvester approach still greatly outperforms
the direct method.

5.2 Labeled Graphs

In a second series of experiments, we compared graphs with node and edge labels. On our two
protein datasets we employed a linear kernel to measure similarity between edge labels representing
distances (in ˚angstr ¨oms) between secondary structure elements. On our two chemical datasets we
used a delta kernel to compare edge labels reﬂecting types of bonds in molecules. We report results
in Table 2 and Figure 1 (right).
On labeled graphs, our three methods outperform the direct approach by about a factor of 1000
when using the linear kernel. In the experiments with the delta kernel, conjugate gradient and ﬁxed-
point iteration are still at least two orders of magnitude faster. Since we did not have access to a
generalized Sylvester equation (13) solver, we had to use a Kronecker product approximation [6]
which dramatically slowed down the Sylvester equation approach.

Table 1: Time to compute kernel matrix for given number of unlabeled graphs from various datasets.
Protein
Enzyme
PTC
MUTAG
dataset
38.6
32.6
26.7
17.7
nodes/graph
edges/node
2.2
1.9
3.8
3.7
#graphs
Direct
Sylvester
Conjugate
Fixed-Point

1128
100
600
100
417
12.5y*
36d*
46.5d*
31h*
41h*
6.1d*
69’15”
36’43”
48.3”
19’30”
97’13”
55.3”
34’58”
44.6”
19’27”
5’59”
40’58”
31.1”
15’23”
13.6”
∗: Extrapolated; run did not ﬁnish in time available.

100
142’53”
73.8”
58.4”
32.4”

100
18’09”
25.9”
42.1”
12.3”

230
104’31”
2’16”
4’04”
1’09”

Table 2: Time to compute kernel matrix for given number of labeled graphs from various datasets.
kernel
linear
delta
dataset
#graphs
Direct
Sylvester
Conjugate
Fixed Point

Protein
Enzyme
1128
100
600
100
417
5.3d*
18y*
2.4d*
86d*
25d*
2.3d*
25’24”
53’55”
89.8”
46d*
4.1h
3’01”
71’28”
124.4”
53’31”
26’52”
1’47”
1.9h
50.1”
35’24”
∗: Extrapolated; run did not ﬁnish in time available.

PTC

MUTAG
230
100
7.2h
1.6d*
21d*
3.9d*
13’46”
2’35”
1’05”
6’09”

100
1.4d*
2.7d*
3’20”
1’31”

6 Outlook and Discussion

We have shown that computing random walk graph kernels is essentially equivalent to solving a large
linear system. We have extended a well-known identity for Kronecker products which allows us to
exploit the structure inherent in this problem. From this we have derived three efﬁcient techniques
to solve the linear system, employing either Sylvester equations, conjugate gradients, or ﬁxed-point
iterations. Experiments on real-world datasets have shown our methods to be scalable and fast, in
some instances outperforming the conventional approach by more than three orders of magnitude.
Even though the Sylvester equation method has a worst-case complexity of O(n3 ), the conjugate
gradient and ﬁxed-point methods tend to be faster on all our datasets. This is because computing
matrix-vector products via Lemma 1 is quite efﬁcient when the graphs are sparse, so that the feature
matrices Φ(L) and Φ(L0 ) contain only O(n) non- entries. Matlab’s black-box dlyap solver is
unable to exploit this sparsity; we are working on more capable alternatives. An efﬁcient generalized
Sylvester solver requires extensive use of tensor calculus and is part of ongoing work.
As more and more graph-structured data becomes available in areas such as biology, web data min-
ing, etc., graph classiﬁcation will gain importance over coming years. Hence there is a pressing
need to speed up the computation of similarity metrics on graphs. We have shown that sparsity, low
effective rank, and Kronecker product structure can be exploited to greatly reduce the computational
cost of graph kernels; taking advantage of other forms of structure in W× remains a challenge. Now
that the computation of random walk graph kernels is viable for practical problem sizes, it will open
the doors for their application in hitherto unexplored domains. The algorithmic challenge now is
how to integrate higher-order structures, such as spanning trees, in graph comparisons.

Acknowledgments

National ICT Australia is funded by the Australian Government’s Department of Communications, Informa-
tion Technology and the Arts and the Australian Research Council through Backing Australia’s Ability and the
ICT Center of Excellence program. This work is supported by the IST Program of the European Community,
under the Pascal Network of Excellence, IST-2002-506778, and by the German Ministry for Education, Sci-
ence, Research and Technology (BMBF) under grant no. 031U112F within the BFAM (Bioinformatics for the
Functional Analysis of Mammalian Genomes) project, part of the German Genome Analysis Network (NGFN).

References
[1] T. G ¨artner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efﬁcient alternatives.
In
B. Sch ¨olkopf and M. K. Warmuth, editors, Proc. Annual Conf. Comput. Learning Theory. Springer, 2003.
[2] H. Kashima, K. Tsuda, and A. Inokuchi. Kernels on graphs. In K. Tsuda, B. Sch ¨olkopf, and J. Vert, editors,
Kernels and Bioinformatics, Cambridge, MA, 2004. MIT Press.
[3] K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V. N. Vishwanathan, A. J. Smola, and H. P. Kriegel. Protein
function prediction via graph kernels. Bioinformatics, 21(Suppl 1):i47–i56, 2005.
[4] F. Harary. Graph Theory. Addison-Wesley, Reading, MA, 1969.
[5] J. D. Gardiner, A. L. Laub, J. J. Amato, and C. B. Moler. Solution of the Sylvester matrix equation
AX B> + CXD> = E . ACM Transactions on Mathematical Software, 18(2):223–231, 1992.
[6] C. F. V. Loan. The ubiquitous kronecker product. Journal of Computational and Applied Mathematics,
123:85 – 100, 2000.
[7] J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research, 1999.
[8] G. H. Golub and C. F. Van Loan. Matrix Computations. John Hopkins University Press, Baltimore, MD,
3rd edition, 1996.


Computation of Similarity Measures for
Sequential Data using Generalized Suf ﬁx Trees

Konrad Rieck
Fraunhofer FIRST.IDA
Kekul ´estr. 7
12489 Berlin, Germany
rieck@first.fhg.de

Pavel Laskov
Fraunhofer FIRST.IDA
Kekul ´estr. 7
12489 Berlin, Germany
laskov@first.fhg.de

S ¨oren Sonnenburg
Fraunhofer FIRST.IDA
Kekul ´estr. 7
12489 Berlin, Germany
sonne@first.fhg.de

Abstract

We propose a generic algorithm for computation of similarity measures for se-
quential data. The algorithm uses generalized sufﬁx trees f or efﬁcient calculation
of various kernel, distance and non-metric similarity functions.
Its worst-case
run-time is linear in the length of sequences and independent of the underlying
embedding language, which can cover words, k-grams or all contained subse-
quences. Experiments with network intrusion detection, DNA analysis and text
processing applications demonstrate the utility of distances and similarity coefﬁ-
cients for sequences as alternatives to classical kernel functions.

1 Introduction

The ability to operate on sequential data is a vital prerequisite for application of machine learning
techniques in many challenging domains. Examples of such applications are natural language pro-
cessing (text documents), bioinformatics (DNA and protein sequences) and computer security (byte
streams or system call traces). A key instrument for handling such data is the efﬁcient computation
of pairwise similarity between sequences. Similarity measures can be seen as an abstraction between
particular structure of data and learning theory.

One of the most successful similarity measures thoroughly studied in recent years is the kernel
function [e.g. 1–3]. Various kernels have been developed fo r sequential data, starting from the
original ideas of Watkins [4] and Haussler [5] and extending to application-speciﬁc kernels such
as the ones for text and natural language processing [e.g. 6– 8], bioinformatics [e.g. 9 –14], spam
ﬁltering [15] and computer security [e.g. 16; 17].

Although kernel-based learning has gained a major focus in machine learning research, a kernel
function is obviously only one of various possibilities for measuring similarity between objects.
The choice of a similarity measure is essentially determined by (a) understanding of a problem and
(b) properties of the learning algorithm to be applied. Some algorithms operate in vector spaces,
others in inner product, metric or even non-metric feature spaces. Investigation of techniques for
learning in spaces other than RKHS is currently one of the active research ﬁelds in machine learning
[e.g. 18–21].

The focus of this contribution lies on general similarity measures for sequential data, especially on
efﬁcient algorithms for their computation. A large number o f such similarity measures can be ex-
pressed in a generic form so that a simple linear-time algorithm can be applied for computation of a
wide class of similarity measures. This algorithm enables the investigation of alternative represen-
tations of problem domain knowledge other than kernel functions. As an example, two applications
are presented for which replacement of a kernel – or equivale ntly, the Euclidean distance – with a
different similarity measure yields a signiﬁcant improvem ent of accuracy in an unsupervised learn-
ing scenario.

The rest of the paper is organized as follows. Section 2 provides a brief review of common simi-
larity measures for sequential data and introduces a generic form in which a large variety of them
can be cast. The generalized sufﬁx tree and a corresponding a lgorithm for linear-time computation
of similarity measures are presented in Section 3. Finally, the experiments in Section 4 demon-
strate efﬁciency and utility of the proposed algorithm on re al-world applications: network intrusion
detection, DNA sequence analysis and text processing.

2 Similarity measures for sequences

2.1 Embedding of sequences

A common way to de ﬁne similarity measures for sequential dat a is via explicit embedding into a
high-dimensional feature space. A sequence x is de ﬁned as concatenation of symbols from a ﬁnite
alphabet Σ. To model the content of a sequence, we consider a language L ⊆ Σ∗ comprising subse-
quences w ∈ L. We refer to these subsequences as words, even though they may not correspond to
a natural language. Typical examples for L are a “bag of words” [e.g. 22], the set of all sequences of
ﬁxed length ( k-grams or k-mers) [e.g. 10; 23] or the set of all contained subsequences [e.g. 8; 24].
Given a language L, a sequence x can be mapped into an |L|-dimensional feature space by calcu-
lating an embedding function φw (x) for every w ∈ L appearing in x. The funcion φw is de ﬁned as
follows

φw : Σ∗ → R+ ∪ {0}, φw (x) := ψ(occ(w, x)) · Ww

(1)

where occ(w, x) is the number of occurrences of w in x, ψ a numerical transformation, e.g. a
conversion to frequencies, and W a weighting assigned to individual words, e.g. length-dependent
or position-dependent weights [cf. 3; 24]. By employing the feature space induced through L and
φ, one can adapt many vectorial similarity measures to operate on sequences.
The feature space de ﬁned via explicit embedding is sparse, s ince the number of non-zero dimensions
for each feature vector is bounded by the sequence length. Thus the essential parameter for measur-
ing complexity of computation is the sequence length, denoted hereinafter as n. Furthermore, the
length of a word |w| or in case of a set of words the maximum length is denoted by k .

2.2 Vectorial similarity measures

Several vectorial kernel and distance functions can be applied to the proposed embedding of sequen-
tial data. A list of common functions in terms of L and φ is given in Table 1.

Kernel function

k(x, y)

Linear

Polynomial

RBF

Pw∈L φw (x)φw (y)
(cid:0)Pw∈L φw (x)φw (y) + θ(cid:1)d
exp (cid:16) −d(x,y)2
(cid:17)
σ

Distance function

d(x, y)

Manhattan

Canberra

Minkowski

Hamming

Chebyshev

Pw∈L |φw (x) − φw (y)|
|φw (x)−φw (y)|
Pw∈L
φw (x)+φw (y)
kqPw∈L |φw (x) − φw (y)|k
Pw∈L sgn |φw (x) − φw (y)|
maxw∈L |φw (x) − φw (y)|

Table 1: Kernels and distances for sequential data

Similarity coef ﬁcient
Simpson
Jaccard
Braun-Blanquet
Czekanowski, Sorensen-Dice
Sokal-Sneath, Anderberg
Kulczynski (1st)
Kulczynski (2nd)
Otsuka, Ochiai

s(x, y)

a/ min(a + b, a + c)

a/(a + b + c)

a/ max(a + b, a + c)

2a/(2a + b + c)

a/(a + 2(b + c))

a/(b + c)
1
2 (a/(a + b) + a/(a + c))
a/p(a + b)(a + c)
Table 2: Similarity coefﬁcients for sequential data

Beside kernel and distance functions, a set of rather exotic similarity coefﬁcients is also suitable for
application to sequential data [25]. The coefﬁcients are co nstructed using three summation variables
a, b and c, which in the case of binary vectors correspond to the number of matching component
pairs (1-1), left mismatching pairs (0-1) and right mismatching pairs (1-0) [cf. 26; 27] Common
similarity coefﬁcients are given in Table 2. For applicatio n to non-binary data these summation
variables can be extended as proposed in [25]:

a = X
w∈L
b = X
w∈L
c = X
w∈L

min(φw (x), φw (y))

[φw (x) − min(φw (x), φw (y))]

[φw (y) − min(φw (x), φw (y))]

2.3 A generic representation

One can easily see that the presented similarity measures can be cast in a generic form that consists
of an outer function ⊕ and an inner function m:

s(x, y) = M
w∈L

m(φw (x), φw (y))

(2)

Given this de ﬁnition, the kernel and distance functions pre sented in Table 1 can be re-formulated
in terms of ⊕ and m. Adaptation of similarity coefﬁcients to the generic form ( 2) involves a re-
formulation of the summation variables a, b and c. The particular de ﬁnitions of outer and inner
functions for the presented similarity measures are given in Table 3. The polynomial and RBF ker-
nels are not shown since they can be expressed in terms of a linear kernel or a distance respectively.

Kernel function ⊕

m(x, y )

Distance function

Linear

+

x · y

Similarity coef. ⊕

m(x, y )

Variable a

Variable b

Variable c

+

min(x, y )

+ x − min(x, y )

+ y − min(x, y )

Manhattan

Canberra

Minkowskik

Hamming

Chebyshev

⊕

+

+

+

+

m(x, y )

|x − y |

|x − y |/(x + y )

|x − y |k

sgn |x − y |

max

|x − y |

Table 3: Generalized formulation of similarity measures

3 Generalized sufﬁx trees for comparison of sequences

The key to efﬁcient comparison of two sequences lies in consi dering only the minimum of words
necessary for computation of the generic form (2) of similarity measures. In the case of kernels
only the intersection of words in both sequences needs to be considered, while the union of words
is needed for calculating distances and non-metric similarity coefﬁcients. A simple and well-known
approach for such comparison is representing the words of each sequence in a sorted list. For words
of maximum length k such a list can be constructed in O(kn log n) using general sorting or O(kn)
using radix-sort. If the length of words k is unbounded, sorted lists are no longer an option as the
sorting time becomes quadratic.

Thus, special data structures are needed for efﬁcient compa rison of sequences. Two data structures
previously used for computation of kernels are tries [28; 29] and sufﬁx trees [30]. Both have been
applied for computation of a variety of kernel functions in O(kn) [3; 10] and also in O(n) run-time
using matching statistics [24]. In this contribution we will argue that a generalized sufﬁx tree is
suitable for computation of all similarity measures of the form (2) in O(n) run-time.
A generalized sufﬁx tree (GST) is a tree containing all sufﬁxes of a set of strings x1 , . . . , xl [31]. The
simplest way to construct a generalized sufﬁx tree is to exte nd each string xi with a delimiter $i and
to apply a sufﬁx tree construction algorithm [e.g. 32] to the concatenation of strings x1 $1 . . . xl $l .
In the remaining part we will restrict ourselves to the case of two strings x and y delimited by
# and $, computation of an entire similarity matrix using a single GST for a set of strings being
a straightforward extension. An example of a generalized sufﬁx tree for the strings “aab# ” and
“babab$ ” is shown in Fig. 1(a).

b

#...

$

a

$

b

ab#...

#...

ab

ab$

$

#...

$

ab$

(1,0)
(a) Generalized sufﬁx tree (GST)

b

#...

$

m(0,2)

$

(0,1)

m

(1,3)

#...

ab

x

(1,0)
ab$

(0,1)

(0,1)
(1,0)
(b) Traversal of a GST

Figure 1: Generalized sufﬁx tree for “aab# ” and “babab$ ” and

a snapshot of its traveral

Once a generalized sufﬁx tree is constructed, it remains to d etermine the number of occurences
occ(w, x) and occ(w, y) of each word w present in the sequences x and y. Unlike the case for
kernels for which only nodes corresponding to both sequences need to be considered [24], the con-
tributions must be correctly computed for all nodes in the generalized sufﬁx tree. The following
simple recursive algorithm computes a generic similarity measure between the sequence x and y in
one depth- ﬁrst traversal of the generalized sufﬁx tree (cf. Algorithm 1).

The algorithm exploits the fact that a leaf in a GST representing a sufﬁx of x contributes exactly 1 to
occ(w, x) if w is the pre ﬁx of this sufﬁx – and similarly for
y and occ(w, y). As the GST contains
all sufﬁxes of x and y, every word w in x and y is represented by at least one leaf. Whether a leaf
contributes to x or y can be determined by considering the edge at the leaf. Due to the uniqueness
of the delimiter #, no branching nodes can occur below an edge containing #, thus a leaf node
at an edge starting before the index of # must contain a sufﬁx of x; otherwise it contains a sufﬁx
of y. The contributions of all leaves are aggregated in two variables x and y during a post-order
traversal. At each node the inner function m of (2) is calculated using ψ(x) and ψ(y ) according to
the embedding φ in (1). A snapshot of the traversal procedure is illustrated in Fig. 1(b).
To account implicit nodes along the edges of the GST and to support weighted embeddings φ, the
weighting function WE IGHT introduced in [24] is employed. At a node v the function takes the
beginning (beg in[v ]) and the end (end[v ]) of the incoming edge and the depth of node (depth[v ]) as
arguments to determine how much the node and edge contribute to the similarity measure, e.g. for
k-gram models only nodes up to a path depth of k need to be considered.

Algorithm 1 Sufﬁx tree comparison
1: function COM PAR E(x, y)
S ← S U FFIXT R E E(x # y $)
2:
(x, y , s) ← MAT CH(root[S ])
3:
return s
4:
5:
6: function MAT CH(v )
if v is leaf then
7:
8:
s ← 0
if beg in[v ] ≤ index# then
9:
10:
(x, y ) ← (1, 0)
11:
j ← index# − 1
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

(x, y ) ← (0, 1)
j ← index$ − 1

else

else

⊲ Leaf of a sufﬁx of x

⊲ Leaf of a sufﬁx of y

(x, y , s) ← (0, 0, 0)
for all c in children[v ] do
( ˆx, ˆy , ˆs) ← MAT CH(c)
⊲ Traverse GST
(x, y , s) ← (x + ˆx, y + ˆy , s ⊕ ˆs)
j ← end[v ]
W ← W E IGH T(beg in[v ], j, depth[v ])
s ← s ⊕ m(ψ(x)W , ψ(y )W )
return (x, y , s)

⊲ Cf. deﬁnitions in (1) and (2)

Similarly to the extension of string kernels proposed in [33], the GST traversal can be performed on
an enhanced sufﬁx array [34] for further run-time and space r eduction.

To prove correctness of our algorithm, a different approach must be taken than the one in [24]. We
cannot claim that the computed similarity value is equivalent to the one returned by the matching
statistic algorithm, since the latter is restricted to kernel functions. Instead we show that at each
recursive call to the MATCH function correct numbers of occurences are maintained.
Theorem 1. A word w occurs occ(w, x) and occ(w, y) times in x and y if and only if MATCH( ¯w)
returns x = occ(w, x) and y = occ(w, y), where ¯w is the node at the end of a path from the root
reassembling w in the generalized sufﬁx tree of x and y .
Proof. If w occurs m times in x, there exist exactly m sufﬁxes of x with w as pre ﬁx. Since w
corresponds to a path from the root of the GST to a node ¯w all m sufﬁxes must pass
¯w. Due to
the unique delimiters # each sufﬁx of x corresponds to one leaf node in the GST whose incoming
edge contains #. Hence m equals occ(w, x) and is exactly the aggregated quantity x returned by
MATCH( ¯w). Likewise, occ(w, y) is the number of sufﬁxes beginning after # and having a pre ﬁx w,
which is computed by y .

4 Experimental Results

4.1 Run-time experiments

In order to illustrate the efﬁciency of the proposed algorit hm, we conducted run-time experiments on
three benchmark data sets for sequential data: network connection payloads from the DARPA 1999
IDS evaluation [35], news articles from the Reuters-21578 data set [36] and DNA sequences from
the human genome [14]. Table 4 gives an overview of the data sets and their speciﬁc properties.
We compared the run-time of the generalized sufﬁx tree algor ithm with a recent trie-based method
supporting computation of distances. Tries yield better or equal run-time complexity for computa-
tion of similarity measures over k-grams than algorithms using indexed arrays and hash tables. A
detailed description of the trie-based approach is given in [25]. Note that in all of the following
experiments tries were generated in a pre-processing step and the reported run-time corresponds to
the comparison procedure only.

For each of the three data sets, we implemented the following experimental protocol: the Manhattan
distances were calculated for 1000 pairs of randomly selected sequences using k-grams as an em-

Name
DNA
NIDS
TEXT

Type
Human genome sequences
TCP connection payloads
Reuters Newswire articles

Alphabet Min. length Max. length
2400
2400
4
132753
53
108
93
43
10002

Table 4: Sequential data sets

bedding language. The procedure was repeated 10 times for various values of k , and the run-time
was averaged over all runs. Fig. 2 compares the run-time of sequence comparison algorithms using
the generalized sufﬁx trees and tries. On all three data sets
the trie-based comparison has a low
run-time for small values of n but grows linearly with k . The algorithm using a generalized sufﬁx
tree is independent from complexity of the embedding language, although this comes at a price of
higher constants due to a more complex data structure. It is obvious that a generalized sufﬁx tree is
the algorithm of choice for higher values of k .

Manhattan distance runtime (NIDS dataset)

Manhattan distance runtime (TEXT dataset)

Manhattan distance runtime (DNA dataset)

)
s
(
 
.
p
m
c
 
0
0
0
1
 
r
e
p
 
e
m
i
t
n
u
r
 
n
a
e
m

10

Trie comparison
GST comparison

8

6

4

2

0

5

15

10
k−gram length
(a) NIDS data set

)
s
(
 
.
p
m
c
 
0
0
0
1
 
r
e
p
 
e
m
i
t
n
u
r
 
n
a
e
m

5

4

3

2

1

0

20

Trie comparison
GST comparison

5

15

10
k−gram length
(b) TEXT data set

)
s
(
 
.
p
m
c
 
0
0
0
1
 
r
e
p
 
e
m
i
t
n
u
r
 
n
a
e
m

12

10

8

6

4

2

0

20

Trie comparison
GST comparison

5

15

10
k−gram length
(c) DNA data set

20

Figure 2: Run-time performance for varying k-gram lengths

4.2 Applications

As a second part of our evaluation, we show that the ability of our approach to compute diverse sim-
ilarity measures pays off when it comes to real applications, especially in an unsupervised learning
scenario. The experiments were performed for (a) intrusion detection in real network trafﬁc and (b)
transcription start site (TSS) recognition in DNA sequences.

For the ﬁrst application, network data was generated by memb ers of our laboratory using virtual
network servers. Recent attacks were injected by a penetration-testing expert. The distance-based
anomaly detection method Zeta [17] was applied to 5-grams extracted from byte sequences of TCP
connections using different similarity measures: the linear kernel, the Manhattan distance and the
Kulczynski coefﬁcient. The results on network data from the HTTP protocol are shown in Fig. 3(a).
Application of the Kulczynski coefﬁcient yields the highes t detection accuracy. Over 78% of all
attacks are identiﬁed with no false-positives in an unsuper vised setup. In comparison, the linear
kernel yields roughly 30% lower detection rates.

The second application focused on TSS recognition in DNA sequences. The data set comprises ﬁxed
length DNA sequences that either cover the TSS of protein coding genes or have been extracted ran-
domly from the interior of genes [14]. We evaluated three methods on this data: an unsupervised
k-nearest neighbor (kNN) classiﬁer, a supervised and bagged kNN classiﬁer and a Support Vec-
tor Machine (SVM). Each method was trained and tested using a linear kernel and the Manhattan
distance as a similarity measure over 4-grams. Fig. 3(b) shows the performance achieved by the
unsupervised and supervised versions of the kNN classiﬁer 1 . Even though the linear kernel and
the Manhattan distance yield similar accuracy in a supervised setup, their performance differs sig-
niﬁcantly in unsupervised application. In the absence of pr ior knowledge of labels the Manhattan

1Results for the SVM are similar to the supervised kNN and have been omitted.

1

0.8

0.6

0.4

0.2

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
t

ROC for intrusion detection in HTTP

Kulczynski coefficient (unsup. knn)
Linear kernel (unsup. knn)
Manhattan distance (unsup. knn)

0

0

0.002

0.004
0.006
false positive rate
(a) Results for network application

0.008

0.01

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
t

1

0.8

0.6

0.4

0.2

0

ROC for transcription site recognition

Linear kernel (unsup. knn)
Linear kernel (sup. knn)
Manhattan distance (unsup. knn)
Manhattan distance (sup. knn)

0

0.02

0.04
0.06
false positive rate
(b) Results for DNA application

0.08

0.1

Figure 3: Comparison of similarity measures on the network and DNA data

distance expresses better discriminative properties for TSS recognition than the linear kernel. For the
supervised application the classication performance is bounded for both similarity measures, since
only some discriminative features for TSS recognition are encapsulated in n-gram models [14].

5 Conclusions

Kernel functions for sequences have recently gained strong attention in many applications of ma-
chine learning, especially in bioinformatics and natural language processing. In this contribution
we have shown that other similarity measures such as metric distances or non-metric similarity co-
efﬁcients can be computed with the same run-time complexity as kernel functions. The proposed
algorithm is based on a post-order traversal of a generalized sufﬁx tree of two or more sequences.
During the traversal, the counts of matching and mismatching words from an embedding language
are computed in time linear in sequence length – regardless o f the particular kind of chosen lan-
guage: words, k-grams or even all consecutive subsequences. By using a generic representation of
the considered similarity measures based on an outer and inner function, the same algorithm can be
applied for various kernel, distance and similarity functions on sequential data.

Our experiments demonstrate that the use of general similarity measures can bring signiﬁcant im-
provement to learning accuracy – in our case observed for uns upervised learning – and emphasize
importance of further investigation of distance- and similarity-based learning algorithms.

Acknowledgments

The authors gratefully acknowledge the funding from Bundesministerium f ¨ur Bildung und
Forschung under the project MIND (FKZ 01-SC40A) and would like to thank Klaus-Robert M ¨uller
and Mikio Braun for fruitful discussions and support.

References
[1] V.N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.
[2] B. Sch ¨olkopf and A.J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[3] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge University Press,
2004.
[4] C. Watkins. Dynamic alignment kernels. In A.J. Smola, P.L. Bartlett, B. Sch ¨olkopf, and D. Schuurmans,
editors, Advances in Large Margin Classi ﬁers , pages 39–50, Cambridge, MA, 2000. MIT Press.
[5] D. Haussler. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, UC Santa
Cruz, July 1999.
[6] T. Joachims. Text categorization with support vector machines: Learning with many relevant features.
Technical Report 23, LS VIII, University of Dortmund, 1997.

[7] E. Leopold and J. Kindermann. Text categorization with Support Vector Machines. how to represent texts
in input space? Machine Learning, 46:423–444, 2002.
[8] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text classi ﬁcation using string
kernels. Journal of Machine Learning Research, 2:419–444, 2002.
[9] A. Zien, G. R ¨atsch, S. Mika, B. Sch ¨olkopf, T. Lengauer,and K.-R. M ¨uller. Engineering Support Vector
Machine Kernels That Recognize Translation Initiation Sites. BioInformatics, 16(9):799–807, September
2000.
[10] C. Leslie, E. Eskin, and W.S. Noble. The spectrum kernel: A string kernel for SVM protein classi ﬁcation.
In Proc. Paci ﬁc Symp. Biocomputing , pages 564–575, 2002.
[11] C. Leslie, E. Eskin, A. Cohen, J. Weston, and W.S. Noble. Mismatch string kernel for discriminative
protein classi ﬁcation. Bioinformatics, 1(1):1–10, 2003.
[12] J. Rousu and J. Shawe-Taylor. Efﬁcient computation of g apped substring kernels for large alphabets.
Journal of Machine Leaning Research, 6:1323–1344, 2005.
[13] G. R ¨atsch, S. Sonnenburg, and B. Sch ¨olkopf. RASE: recognition of alternatively spliced exons in c.
elegans. Bioinformatics, 21:i369–i377, June 2005.
[14] S. Sonnenburg, A. Zien, and G. R ¨atsch. ARTS: Accurate Recognition of Transcription Starts in Human.
Bioinformatics, 22(14):e472–e480, 2006.
[15] H. Drucker, D. Wu, and V.N. Vapnik. Support vector machines for spam categorization. IEEE Transac-
tions on Neural Networks, 10(5):1048–1054, 1999.
[16] E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and S. Stolfo. Applications of Data Mining in Computer
Security, chapter A geometric framework for unsupervised anomaly detection: detecting intrusions in
unlabeled data. Kluwer, 2002.
[17] K. Rieck and P. Laskov. Detecting unknown network attacks using language models. In Proc. DIMVA,
pages 74–90, July 2006.
[18] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer. Classi ﬁcation on pairwise proximity
data. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in Neural Information Processing
Systems, volume 11, pages 438–444. MIT Press, 1999.
[19] V. Roth, J Laub, M. Kawanabe, and J.M. Buhmann. Optimal cluster preserving embedding of non-metric
proximity data. IEEE Trans. PAMI, 25:1540–1551, December 2003.
[20] J. Laub and K.-R. M ¨uller. Feature discovery in non-metric pairwise data. Journal of Machine Learning,
5(Jul):801–818, July 2004.
[21] C. Ong, X. Mary, S. Canu, and A.J. Smola. Learning with non-positive kernels. In Proc. ICML, pages
639–646, 2004.
[22] G. Salton. Mathematics and information retrieval. Journal of Documentation, 35(1):1–29, 1979.
[23] M. Damashek. Gauging similarity with n-grams: Language-independent categorization of text. Science,
267(5199):843–848, 1995.
[24] S.V.N. Vishwanathan and A.J. Smola. Kernels and Bioinformatics, chapter Fast Kernels for String and
Tree Matching, pages 113–130. MIT Press, 2004.
[25] K. Rieck, P. Laskov, and K.-R. M ¨uller. Efﬁcient algori thms for similarity measures over sequential data:
A look beyond kernels. In Proc. DAGM, pages 374–383, September 2006.
[26] R.R. Sokal and P.H. Sneath. Principles of numerical taxonomy. Freeman, San Francisco, CA, USA, 1963.
[27] M.R. Anderberg. Cluster Analysis for Applications. Academic Press, Inc., New York, NY, USA, 1973.
[28] E. Fredkin. Trie memory. Communications of ACM, 3(9):490–499, 1960.
[29] D. Knuth. The art of computer programming, volume 3. Addison-Wesley, 1973.
[30] P. Weiner. Linear pattern matching algorithms.
In Proc. 14th Annual Symposium on Switching and
Automata Theory, pages 1–11, 1973.
[31] D. Gusﬁeld. Algorithms on strings, trees, and sequences. Cambridge University Press, 1997.
[32] E. Ukkonen. Online construction of sufﬁx trees. Algorithmica, 14(3):249–260, 1995.
[33] C.H. Teo and S.V.N. Vishwanathan. Fast and space efﬁcie nt string kernels using sufﬁx arrays. In Pro-
ceedings, 23rd ICMP, pages 939–936. ACM Press, 2006.
[34] M.I. Abouelhoda, S. Kurtz, and E. Ohlebusch. Replacing sufﬁx trees with enhanced sufﬁx arrays.
of Discrete Algorithms, 2(1):53–86, 2002.
[35] R. Lippmann, J.W. Haines, D.J. Fried, J. Korba, and K. Das. The 1999 DARPA off-line intrusion detection
evaluation. Computer Networks, 34(4):579–595, 2000.
[36] D.D. Lewis. Reuters-21578 text categorization test collection. AT&T Labs Research, 1997.

Journal


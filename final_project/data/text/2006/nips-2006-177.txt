Generalized Maximum Margin Clustering and
Unsupervised Kernel Learning

Hamed Valizadegan
Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
valizade@msu.edu

Rong Jin
Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
rongjin@cse.msu.edu

Abstract

Maximum margin clustering was proposed lately and has shown promising
performance in recent studies [1, 2]. It extends the theory of support vec-
tor machine to unsupervised learning. Despite its good performance, there
are three ma jor problems with maximum margin clustering that question
its eﬃciency for real-world applications. First, it is computationally ex-
pensive and diﬃcult to scale to large-scale datasets because the number of
parameters in maximum margin clustering is quadratic in the number of
examples. Second, it requires data preprocessing to ensure that any cluster-
ing boundary will pass through the origins, which makes it unsuitable for
clustering unbalanced dataset. Third, it is sensitive to the choice of kernel
functions, and requires external procedure to determine the appropriate
values for the parameters of kernel functions. In this paper, we propose
“generalized maximum margin clustering” framework that addresses
the above three problems simultaneously. The new framework general-
izes the maximum margin clustering algorithm by allowing any clustering
boundaries including those not passing through the origins. It signiﬁcantly
improves the computational eﬃciency by reducing the number of parame-
ters. Furthermore, the new framework is able to automatically determine
the appropriate kernel matrix without any labeled data. Finally, we show a
formal connection between maximum margin clustering and spectral clus-
tering. We demonstrate the eﬃciency of the generalized maximum margin
clustering algorithm using both synthetic datasets and real datasets from
the UCI repository.

1 Introduction

Data clustering, the unsupervised classiﬁcation of samples into groups, is an important re-
search area in machine learning for several decades. A large number of algorithms have
been developed for data clustering, including the k-means algorithm [3], mixture models [4],
and spectral clustering [5, 6, 7, 8, 9]. More recently, maximum margin clustering [1, 2] was
proposed for data clustering and has shown promising performance. The key idea of maxi-
mum margin clustering is to extend the theory of support vector machine to unsupervised
learning. However, despite its success, the following three ma jor problems with maximum
margin clustering has prevented it from being applied to real-world applications:
• High computational cost. The number of parameters in maximum margin clustering
is quadratic in the number of examples. Thus, it is diﬃcult to scale to large-scale
datasets. Figure 1 shows the computational time (in seconds) of the maximum
margin clustering algorithm with respect to diﬀerent numbers of examples. We

Figure 1: The scalability of the original maximum margin clustering algorithm versus the
generalized maximum margin clustering algorithm

(a) Data distribution
(b) Clustering error versus kernel width
Figure 2: Clustering error of spectral clustering using the RBF kernel with diﬀerent kernel
width. The horizonal axis of Figure 2(b) represents the percentage of the distance range
(i.e., the diﬀerence between the maximum and the minimum distance) that is used for kernel
width.

clearly see that the computational time increases dramatically when we apply the
maximum margin clustering algorithm to even modest numbers of examples.
• Requiring clustering boundaries to pass through the origins. One important assump-
tion made by the maximum margin clustering in [1] is that the clustering boundaries
will pass through the origins. To this end, maximum margin clustering requires
centralizing data points around the origins before clustering data. It is important
to note that centralizing data points at the origins does not guarantee clustering
boundaries to go through origins, particularly when cluster sizes are unbalanced
with one cluster signiﬁcantly more popular than the other.
• Sensitive to the choice of kernel functions. Figure 2(b) shows the clustering error of
maximum margin clustering for the synthesized data of two overlapped Gaussians
clusters (Figure 2(a)) using the RBF kernel with diﬀerent kernel width. We see that
the performance of maximum margin clustering depends critically on the choice
of kernel width. The same problem is also observed in spectral clustering [10].
Although a number of studies [8, 9, 10, 6] are devote to automatically identifying
appropriate kernel matrices in clustering, they are either heuristic approaches or
require additional labeled data.

In this paper, we propose “generalized maximum margin clustering” framework that
resolves the above three problems simultaneously. In particular, the proposed framework

40608010012014016018020022002004006008001000120014001600Number of SamplesTime (seconds)Time comparisionGeneralized Maxmium Marging ClusteringMaximum Margin Clustering0.40.60.811.21.41.61.822.20.40.60.811.21.41.61.8210203040506070809010005101520253035404550Kernel Width (% of data range) in RBF functionClustering errorreformulates the problem of maximum margin clustering to include the bias term in the
classiﬁcation boundary, and therefore remove the assumption that clustering boundaries
have to pass through the origins. Furthermore, the new formulism reduces the number
of parameters to be linear in the number of examples, and therefore signiﬁcantly reduces
the computational cost. Finally, it is equipped with the capability of unsupervised kernel
learning, and therefore, is able to determine the appropriate kernel matrix and clustering
memberships simultaneously. More interestingly, we will show that spectral clustering, such
as the normalized cut algorithm, can be viewed as a special case of the generalized maximum
margin clustering.
The remainder of the paper is organized as follows: Section 2 reviews the work of maximum
margin clustering and kernel learning. Section 3 presents the framework of generalized
maximum margin clustering. Our empirical studies are presented in Section 4. Section 5
concludes this work.

2 Related Work

(cid:186) 0

e + ν − δ + λy
t − 2C δ(cid:62)e

The key idea of maximum margin clustering is to extend the theory of support vector
machine to unsupervised learning. Given the training examples D = (x1 , x2 , . . . , xn ) and
their class labels y = (y1 , y2 , . . . , yn ) ∈ {−1, +1}n , the dual problem of support vector
machine can be written as:
α(cid:62)e − 1
2 α(cid:62)diag(y)K diag(y)α
max
α∈Rn
0 ≤ α ≤ C, α(cid:62)y = 0
(1)
s. t.
where K ∈ Rn×n is the kernel matrix and diag(y) stands for the diagonal matrix that
uses the vector y as its diagonal elements. To apply the above formulism to unsupervised
learning, the maximum margin clustering approach relaxes class labels y to continuous
variables, and searches for both y and α that maximizes the classiﬁcation margin. This
leads to the following optimization problem:
(cid:182)
(cid:181)
min
t
y,λ,ν,δ
(yy(cid:62) ) ◦ K
(e + ν − δ + λy)(cid:62)
ν ≥ 0, δ ≥ 0
where ◦ stands for the element wise product between two matrices. To convert the above
problem into a convex programming problem, the authors of [1] makes two important relax-
ations. The ﬁrst one relaxes yy(cid:62) into a positive semi-deﬁnitive (PSD) matrix M (cid:186) 0 whose
diagonal elements are set to be 1. The second relaxation sets λ = 0, which is equivalent to
assuming that there is no bias term b in the expression of classiﬁcation boundaries, or in
other words, classiﬁcation boundaries have to pass through the origins of data. These two
assumption simplify the above optimization problem as follows:
(cid:182)
(cid:181)
min
t
M ,ν,δ
M ◦ K
e + ν − δ
(e + ν − δ)(cid:62) t − 2C δ(cid:62)e
ν ≥ 0, δ ≥ 0, M (cid:186) 0
(2)
Finally, a few additional constraints of M are added to the above optimization problem to
prevent skewed clustering sizes [1]. As a consequence of these two relaxations, the number
of parameters is increased from n to n2 , which will signiﬁcantly increase the computational
cost. Furthermore, by setting λ = 0, the maximum margin clustering algorithm requires
clustering boundaries to pass through the origins of data, which is unsuitable for clustering
data with unbalanced clusters.
Another important problem with the above maximum margin clustering is the diﬃculty
in determining the appropriate kernel similarity matrix K . Although many kernel based
clustering algorithms set the kernel parameters manually, there are several studies devoted
to automatic selection of kernel functions, in particular the kernel width for the RBF kernel,

s. t.

s. t.

(cid:186) 0

(cid:180)
(cid:179)
− (cid:107)xi−xj (cid:107)2
. Shi et al. [8] recommended choosing the kernel width as 10% to
i.e., σ in exp
2
2σ2
20% of the range of the distance between samples. However, in our experiment, we found
that this is not always a good choice, and in many situations it produces poor results. Ng
et al. [9] chose kernel width which provides the least distorted clusters by running the same
clustering algorithm several times for each kernel width. Although this approach seems to
generate good results, it requires running seperate experiments for each kernel width, and
therefore could be computationally intensive. Manor et al.
in [10] proposed a self-tuning
spectral clustering algorithm that computes a diﬀerent local kernel width for each data point
xi . In particular, the local kernel width for each xi is computed as the distance of xi to
its kth nearest neighbor. Although empirical study seems to show the eﬀectiveness of this
approach, it is unclear how to ﬁnd the optimal k in computing the local kernel width. As
we will see in the experiment section, the clustering accuracy depends heavily on the choice
of k .
Finally, we will brieﬂy overview the existing work on kernel learning. Most previous work
focus on supervised kernel learning. The representative approaches in this category include
the kernel alignment [11, 12], semi-deﬁnitive programming [13], and spectral graph parti-
tioning [6]. Unlike these approaches, the proposed framework is designed for unsupervised
kernel learning.

3 Generalized Maximum Margin Clustering and Unsupervised
Kernel Learning

We will ﬁrst present the proposed clustering algorithm for hard margin, followed by the
extension to soft margin and unsupervised kernel learning.

3.1 Hard Margin
In the case of hard margin, the dual problem of SVM is almost identical to the problem in
Eqn. (1) except that the parameter α does not have the upper bound C . Following [13], we
further convert the problem in (1) into its dual form:
1
(e + ν + λy)T diag(y)K −1diag(y)(e + ν + λy)
min
2
ν ,y,λ
ν ≥ 0, y ∈ {+1, −1}n
(3)
s. t.
where e is a vector with all its elements being one. Unlike the treatment in [13], which
rewrites the above problem as a semi-deﬁnitive programming problem, we introduce vari-
ables z that is deﬁned as follows:
z = diag(y)(e + ν )
Given that ν ≥ 0, the above expression for z is essentially equivalent to the constraint
i ≥ 1 for i = 1, 2, . . . , n. Then, the optimization problem in (3) is rewritten as
|zi | ≥ 1 or z 2
follows:
1
(z + λe)T K −1 (z + λe)
min
2
z,λ
i ≥ 1, i = 1, 2, . . . , n
(4)
s. t.
z 2
Note that the above problem may not have unique solutions for z and λ due to the translation
invariance of the ob jective function. More speciﬁcally, given an optimal solution z and λ,
we may be able to construct another solution z(cid:48) and λ(cid:48) such that:
z(cid:48) = z + e, λ(cid:48) = λ − .
Evidently, both solutions result in the same value for the ob jective function in (4). Fur-
thermore, with appropriately chosen , the new solution z(cid:48) and λ(cid:48) will be able to satisfy the
i ≥ 1. Thus, z(cid:48) and λ(cid:48) is another optimal solution for (3). This is in fact related
constraint z 2
to the problem in SVM where the bias term b may not be unique [14]. To remove the trans-
lation invariance from the ob jective function, we introduce an additional term Ce (z(cid:62)e)2
into the ob jective function, i.e.
min
z,λ
s. t.

1
(z + λe)T K −1 (z + λe) + Ce (z(cid:62)e)2
2
i ≥ 1, i = 1, 2, . . . , n
z 2

(5)

max
γ∈Rn

γi

(7)

where constant Ce weights the important of the punishment factor against the original
ob jective. It is set to be 10, 000 in our experiment. For the simplicity of our expression, we
further deﬁne

n+1 (cid:186) 0
γi I i

w = (z; λ) and P = (In , e).
Then, the problem in (4) becomes
wT P T K −1P w + Ce (e(cid:62)
min
0 w)2
w∈Rn+1
i ≥ 1, i = 1, 2, . . . , n
(6)
s. t.
w2
0 w)2 − n(cid:88)
where e0 is a vector with all its elements being 1 except its last element which is zero. We
then construct the Lagrangian as follows
(cid:33)
(cid:195)
n+1w − 1)
γi (w(cid:62) I i
L(w, γ ) = wT P T K −1P w + Ce (e(cid:62)
n(cid:88)
0 − n(cid:88)
i=1
P T K −1P + Cee0e(cid:62)
= w(cid:62)
w +
γi I i
γi
n+1
i=1
i=1
n+1 is an (n + 1) × (n + 1) matrix with all the elements being zero except the ith
n(cid:88)
where I i
diagonal element which is 1. Hence, the dual problem of (6) is
0 − n(cid:88)
i=1
s. t. P T K −1P + Cee0e(cid:62)
(cid:33)
(cid:195)
i=1
γi ≥ 0, i = 1, 2, . . . , n
0 − n(cid:88)
Finally, the solution w can be computed using the KKT condition, i.e.,
P T K −1P + Cee0e(cid:62)
(cid:162)
(cid:161)
0 − (cid:80)n
i=1
the
solution w is proportional
In other words,
to the
eigenvector of matrix
P T K −1P + Cee0e(cid:62)
for the zero eigenvalue. Since wi = (1 + νi )yi , i =
i=1 γi I i
1, 2, . . . , n and νi ≥ 0, the class labels {yi }n
n+1
i=1 can be inferred directly from the sign of
{wi }n
i=1 .
It is important to realize that the problem in (5) is non-convex due to the non-
Remark I
i ≥ 1. Thus, the optimal solution found by the dual problem in (7) is
convex constraint w2
not necessarily the optimal solution for the prime problem in (5). Our hope is that although
the solution found by the dual problem is not optimal for the prime problem, it is still a
good solution for the prime problem in (5). This is similar to the SDP relaxation made by
the maximum margin clustering algorithm in (2) that relaxes a non-convex programming
problem into a convex one. However, unlike the relaxation made in (2) that increases the
number of variables from n to n2 , the new formulism of maximum margin does not increase
the number of parameters (i.e., γ ), and therefore will be computational more eﬃcient. This
is shown in Figure 1, in which the computational time of generalized maximum margin
clustering is increased much slower than that of the maximum margin algorithm.
(cid:80)n
Remark II To avoid the high computational cost in estimating K −1 , we replace K −1 with
its normalized graph Laplacian L(K ) [15], which is deﬁned as L(K ) = I −D1/2KD1/2 where
D is a diagonal matrix whose diagonal elements are computed as Di,i =
j=1 Ki,j , i =
1, 2, . . . , n. This is equivalent to deﬁning a kernel matrix ˜K = L(K )† where † stands for the
operator of pseudo inverse. More interesting, we have the following theorem showing the
relationship between generalized maximum margin clustering and the normalized cut.
Theorem 1. The normalized cut algorithm is a special case of the generalized maximum
margin clustering in (7) if the fol lowing conditions hold, i.e., (1) K −1 is set to be the
normalized Laplacian ¯L(K ), (2) al l the γ s are enforced to be the same, i.e., γi = γ0 , i =
1, 2, . . . , n, and (3) Ce ≥ 1.
Proof sketch: Given the conditions 1 to 3 in the theorem, the new objective function in (7)
γ s.t. ¯L(K ) (cid:186) γ In and the solution for this problem is the largest eigenvector
becomes: max
γ≥0
of ¯L(K ).

γi I i
n+1

w = 0n+1

δ2
i

3.2 Soft Margin
n(cid:88)
We extend the formulism in (7) to the case of soft margin by considering the following
problem:
i=1

1
(e + ν − δ + λy)T diag(y)K −1diag(y)(e + ν − δ + λy) + Cδ
min
2
ν ,y,λ,δ
ν ≥ 0, δ ≥ 0, y ∈ {+1, −1}n
(8)
s. t.
where Cδ weights the importance of the clustering errors against the clustering margin.
n(cid:88)
Similar to the previous derivation, we introduce the slack variable z and simplify the above
problem as follows:
i=1

1
(z + λe)T K −1 (z + λe) + Ce (z(cid:62)e)2 + Cδ
2
(zi + δi )2 ≥ 1, δi ≥ 0, i = 1, 2, . . . , n
(9)
s. t.
n(cid:88)
By approximating (zi + δi )2 as z 2
i + δ2
i , we have the dual form of the above problem written
as:
0 − n(cid:88)
i=1
n+1 (cid:186) 0
s. t. P (cid:62)K −1P + Cee0e(cid:62)
γi I i
i=1
0 ≤ γi ≤ Cδ , i = 1, 2, . . . , n
(10)
The main diﬀerence between the above formulism and the formulism in (7) is the introduc-
tion of the upper bound Cδ for γ in the case of soft margin. In the experiment, we set the
parameter Cδ to be 100, 000, a very large value.

max
γ∈Rn

min
z,δ,λ

δ2
i

γi

βiKi

max
γ ,β

3.3 Unsupervised Kernel Learning
As already pointed out, the performance of many clustering algorithms depend on the right
choice of the kernel similarity matrix. To address this problem, we extend the formulism
(cid:80)m
in (10) by including the kernel learning mechanism. In particular, we assume that a set of
m kernel similarity matrices K1 , K2 , . . . , Km are available. Our goal is to identify the linear
n(cid:88)
i=1 βiKi , that leads to the optimal clustering
combination of kernel matrices, i.e., K =
accuracy. More speciﬁcally, we need to solve the following optimization problem:
(cid:33)−1
(cid:195)
0 − n(cid:88)
m(cid:88)
γi
i=1
P + Cee0e(cid:62)
s. t. P (cid:62)
m(cid:88)
i=1
i=1
βi = 1, βi ≥ 0, i = 1, 2, . . . , m
0 ≤ γi ≤ Cδ , i = 1, 2, . . . , n,
(cid:80)m
i=1
Unfortunately, it is diﬃcult to solve the above problem due to the complexity introduced
i=1 βiKi )−1 . Hence, we consider an alternative problem to the above one. We ﬁrst
by (
(cid:80)m
introduce a set of normalized graph Laplacian ¯L1 , ¯L2 , . . . , ¯Lm . Each Laplacian Li is con-
n(cid:88)
structed from the kernel similarity matrix Ki . We then deﬁned the inverse of the combined
matrix as K −1 =
i=1 βi ¯Li . Then, we have the following optimization problem
0 − n(cid:88)
m(cid:88)
i=1
n+1 (cid:186) 0
βiP (cid:62) ¯LiP + Cee0e(cid:62)
m(cid:88)
γi I i
i=1
i=1
0 ≤ γi ≤ Cδ , i = 1, 2, . . . , n,
i=1

βi = 1, βi ≥ 0, i = 1, 2, . . . , m

n+1 (cid:186) 0
γi I i

max
γ ,β

s. t.

γi

(11)

(12)

(a) Overlapped Gaussian
(c) Two Connected Circles
(b) Two Circles
Figure 3: Data distribution of the three synthesized datasets
By solving the above problem, we are able to resolve both γ (corresponding to clustering
memberships) and β (corresponding to kernel learning) simultaneously.

4 Experiment

We tested the generalized maximum margin clustering algorithm on both synthetic datasets
and real datasets from the UCI repository. Figure 3 gives the distribution of the synthetic
datasets. The four UCI datasets used in our study are “Vote”, “Digits”, “Ionosphere”, and
“Breast”. These four datasets comprise of 218, 180, 351, and 285 examples, respectively,
and each example in these four datasets is represented by 17, 64, 35, and 32 features. Since
the “Digits” dataset consists of multiple classes, we further decompose it into four datasets
of binary classes that include pairs of digits diﬃcult to distinguish. Both the normalized cut
algorithm [8] and the maximum margin clustering algorithm [1] are used as the baseline.
The RBF kernel is used throughout this study to construct the kernel similarity matrices.
In our ﬁrst experiment, we examine the optimal performance of each clustering algorithm
by using the optimal kernel width that is acquired through an exhaustive search. The opti-
mal clustering errors of these three algorithms are summarized in the ﬁrst three columns of
Table 1. It is clear that generalized maximum margin clustering algorithm achieve similar
or better performance than both maximum margin clustering and normlized cut for most
datasets when they are given the optimal kernel matrices. Note that the results of maxi-
mum margin clustering are reported for a subset of samples(including 80 instances) in UCI
datasets due to the out of memory problem.

Table 1: Clustering error (%) of normalized cut (NC), maximum margin clustering (MMC),
generalized maximum margin clustering (GMMC) and self-tuning spectral clustering (ST).
Unsupervised Kernel Learning
Optimal Kernel Width
Dataset
NC MMC GMMC GMMC ST (Best k)
ST(Worst k)
50
0
0
0
2
0
45
1
0
0
6.25
7
7.5
5
3.75
1.25
2.5
1.25
25
15
9.6
11.90
11
40
50
5
5.6
5.6
35
10
47
0
3
2.2
31.25
45
50
1.5
5.6
.5
1.25
34
48
9
12
16
3.75
48
25
21.25
23.5
27.3
26.5
48
41.5
37.5
37
36.1
38.75
36.5

Two Circles
Two Jointed Circles
Two Gaussian
Vote
Digits 3-8
Digits 1-7
Digits 2-7
Digits 8-9
Ionosphere
Breast

In the second experiment, we evaluate the eﬀectiveness of unsupervised kernel learning. Ten
kernel matrices are created by using the RBF kernel with the kernel width varied from 10%
to 100% of the range of distance between any two examples. We compare the proposed
unsupervised kernel learning to the self-tuning spectral clustering algorithm in [10]. One
of the problem with the self-tuning spectral clustering algorithm is that its clustering error
usually depends on the parameter k , i.e., the number of nearest neighbor used for computing
the kernel width. To provide a full picture of the self-tuning spectral clustering, we vary k
from 1 and 15 , and calculate both best and worst performance using diﬀerent k . The last
three columns of Table 1 summarizes the clustering errors of generalized maximum margin

0.40.60.811.21.41.61.822.20.40.60.811.21.41.61.82−1−0.500.51−1−0.500.51−0.6−0.4−0.200.20.40.6−0.6−0.4−0.200.20.40.6clustering and self-tuning spectral clustering with both best and worst k . First, observe
the big gap between best and worst performance of self-tuning spectral clustering with
diﬀerent choice of k , which implies that this algorithm is sensitive to parameter k . Second,
for most datasets, generalized maximum margin clustering achieves similar performance as
self-tuning spectral clustering with the best k . Furthermore, for a number of datasets, the
unsupervised kernel learning method achieves the performance close to the one using the
optimal kernel width. Both results indicate that the proposed algorithm for unsupervised
kernel learning is eﬀective in identifying appropriate kernels.

5 Conclusion

In this paper, we proposed a framework for the generalized maximum margin clustering.
Compared to the existing algorithm for maximum margin clustering, the new framework
has three advantages: 1) it reduces the number of parameters from n2 to n, and therefore
has a signiﬁcantly lower computational cost, 2) it allows for clustering boundaries that do
not pass through the origin, and 3) it can automatically identify the appropriate kernel
similarity matrix through unsupervised kernel learning. Our empirical study with three
synthetic datasets and four UCI datasets shows the promising performance of our proposed
algorithm.

References

[1] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In
Advances in Neural Information Processing Systems (NIPS) 17, 2004.
[2] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vec-
tor machines. In Proceedings of the 20th National Conference on Artiﬁcial Intel ligence
(AAAI-05)., 2005.
[3] J. Hartigan and M. Wong. A k-means clustering algorithm. Appl. Statist., 28:100–108,
1979.
[4] R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the em
algorithm. SIAM Review, 26:195–239, 1984.
[5] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. A min-max cut algorithm for graph
partitioning and data clustering. In Proc. IEEE Int’l Conf. Data Mining, 2001.
[6] F. R. Bach and M. I. Jordan. Learning spectral clustering.
In Advances in Neural
Information Processing Systems 16, 2004.
[7] R. Jin, C. Ding, and F. Kang. A probabilistic approach for optimizing spectral clus-
tering. In Advances in Neural Information Processing Systems 18, 2006.
[8] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on
Pattern Analysis and Machine Intel ligence, 22(8):888–905, 2000.
[9] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in Neural Information Processing Systems 14, 2001.
[10] L. Zelnik-Manor and P. Perona. Self-tuning spectral clustering. In Advances in Neural
Information Processing Systems 17, pages 1601–1608, 2005.
[11] N. Cristianini, J. Shawe-Taylor, A. Elisseeﬀ, and J. S. Kandola. On kernel-target
alignment. In NIPS, pages 367–373, 2001.
[12] X. Zhu, J. Kandola, Z. Ghahramani, and J. Laﬀerty. Nonparametric transforms of
graph kernels for semi-supervised learning. In Advances in Neural Information Pro-
cessing Systems 17, pages 1641–1648, 2005.
[13] G. R. G. Lanckriet, N. Cristianini, P. L. Bartlett, Laurent El Ghaoui, and Michael I.
Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine
Learning Research, 5:27–72, 2004.
[14] C. J. C. Burges and D. J. Crisp. Uniqueness theorems for kernel methods. Neurocom-
puting, 55(1-2):187–220, 2003.
[15] F.R.K. Chung. Spectral Graph Theory. Amer. Math. Society, 1997.


Online Clustering of Moving Hyperplanes

Ren ´e Vidal
Center for Imaging Science, Department of Biomedical Engineering, Johns Hopkins University
308B Clark Hall, 3400 N. Charles St., Baltimore, MD 21218, USA
rvidal@cis.jhu.edu

Abstract

We propose a recursive algorithm for clustering trajectories lying in multiple mov-
ing hyperplanes. Starting from a given or random initial condition, we use nor-
malized gradient descent to update the coefﬁcients of a time varying polynomial
whose degree is the number of hyperplanes and whose derivatives at a trajectory
give an estimate of the vector normal to the hyperplane containing that trajectory.
As time proceeds, the estimates of the hyperplane normals are shown to track
their true values in a stable fashion. The segmentation of the trajectories is then
obtained by clustering their associated normal vectors. The ﬁnal result is a simple
recursive algorithm for segmenting a variable number of moving hyperplanes. We
test our algorithm on the segmentation of dynamic scenes containing rigid mo-
tions and dynamic textures, e.g., a bird ﬂoating on water. Ou r method not only
segments the bird motion from the surrounding water motion, but also determines
patterns of motion in the scene (e.g., periodic motion) directly from the temporal
evolution of the estimated polynomial coefﬁcients. Our exp eriments also show
that our method can deal with appearing and disappearing motions in the scene.

1 Introduction

Principal Component Analysis (PCA) [1] refers to the problem of ﬁtting a linear subspace S ⊂ RD
i=1 . A natural extension
of unknown dimension d < D to N sample points X = {xi ∈ S }N
of PCA is subspace clustering, which refers to the problem of ﬁtting a union of n ≥ 1 linear
subspaces {Sj ⊂ RD }n
j=1 of unknown dimensions dj = dim(Sj ), 0 < dj < D , to N points
i=1 drawn from ∪n
j=1Sj , without knowing which points belong to which subspace.
X = {xi ∈ RD }N
This problem shows up in a variety of applications in computer vision (image compression, motion
segmentation, dynamic texture segmentation) and also in control (hybrid system identiﬁcation).

Subspace clustering has been an active topic of research over the past few years. Existing methods
randomly choose a basis for each subspace, and then iterate between data segmentation and standard
PCA. This can be done using methods such as Ksubspaces [2], an extension of Kmeans to the case
of subspaces, or Expectation Maximization for Mixtures of Probabilistic PCAs [3]. An alternative
algebraic approach, which does not require any initialization, is Generalized PCA (GPCA) [4]. In
GPCA the data points are ﬁrst projected onto a low-dimension al subspace. Then, a set of polyno-
mials is ﬁtted to the projected data points and a basis for eac h one of the projected subspaces is
obtained from the derivatives of these polynomials at the data points.

Unfortunately, all existing subspace clustering methods are batch, i.e. the subspace bases and the
segmentation of the data are obtained after all the data points have been collected. In addition,
existing methods are designed for clustering data lying in a collection of static subspaces, i.e. the
subspace bases do not change as a function of time. Therefore, when these methods are applied to
time-series data, e.g., dynamic texture segmentation, one typically applies them to a moving time
window, under the assumption that the subspaces are static within that window. A major disadvan-
tage of this approach is that it does not incorporate temporal coherence, because the segmentation

and the bases at time t + 1 are obtained independently from those at time t. Also, this approach is
computationally expensive, since a new subspace clustering problem is solved at each time instant.

In this paper, we propose a computationally simple and temporally coherent online algorithm for
clustering point trajectories lying in a variable number of moving hyperplanes. We model a union of
n moving hyperplanes in RD , Sj (t) = {x ∈ RD : b⊤
j (t)x = 0}, j = 1, . . . , n, where b(t) ∈ RD ,
as the zero set of a polynomial with time varying coefﬁcients . Starting from an initial polynomial
at time t, we compute an update of the polynomial coefﬁcients using no rmalized gradient descent.
The hyperplane normals are then estimated from the derivatives of the new polynomial at each
trajectory. The segmentation of the trajectories is obtained by clustering their associated normal
vectors. As time proceeds, new data are added, and the estimates of the polynomial coefﬁcients are
more accurate, because they are based on more observations. This not only makes the segmentation
of the data more accurate, but also allows us to handle a variable number of hyperplanes. We test our
approach on the challenging problem of segmenting dynamic textures from rigid motions in video.

2 Recursive estimation of a single hyperplane

In this section, we review the normalized gradient algorithm for estimating a single hyperplane. We
consider both static and moving hyperplanes, and analyze the stability of the algorithm in each case.
Recursive linear regression. For the sake of simplicity, let us ﬁrst revisit a simple linea r regression
problem in which we are given measurements {x(t), y (t)} related by the equation y (t) = b⊤x(t).
At time t, we seek an estimate ˆb(t) of b that minimizes f (b) = Pt
τ =1 (y (τ ) − b⊤x(τ ))2 . A simple
strategy is to recursively update ˆb(t) by following the negative of the gradient direction at time t,
v(t) = −(ˆb(t)⊤x(t) − y (t))x(t).
(1)
However, it is better to normalize this gradient in order to achieve better convergence properties. As
shown in Theorem 2.8, page 77 of [5], the following normalized gradient recursive identiﬁer
(ˆb(t)⊤x(t) − y (t))
ˆb(t + 1) = ˆb(t) − µ
1 + µkx(t)k2
where µ > 0 is a ﬁxed parameter, is such that ˆb(t) → b exponentially if the regressors {x(t)} are
persistently exciting, i.e. if there is an S ∈ N and ρ1 , ρ2 > 0 such that for all m
m+S
X
x(t)x(t)⊤ ≺ ρ2 ID ,
ρ1 ID ≺
t=m
where A ≺ B means that (B −A) is positive de ﬁnite and ID is the identity matrix in RD . Intuitively,
the condition on the left hand side of (3) means that the data has to be persistently ”rich enough ” in
time in order to uniquely estimate the vector b, while the condition on the right hand side is needed
for stability purposes, as it imposes a uniform upper bound on the covariance of the data.

x(t),

(2)

(3)

Consider now a modiﬁcation of the linear regression problem in which the parameter vector varies
with time, i.e. y (t) = b⊤ (t)x(t). As shown in [6], if the regressors {x(t)} are persistently exciting
and the sequence {b(t+ 1)− b(t)} is L2 -stable, i.e. sup
t≥1 kb(t+ 1)− b(t)k2 < ∞, then the normalized
gradient recursive identiﬁer (2) produces an estimate ˆb(t) of b(t) such that {b(t)− ˆb(t)} is L2 -stable.
Recursive hyperplane estimation. Let {x(t)} be a set of measurements lying in the moving hyper-
plane S (t) = {x ∈ RD : b⊤ (t)x = 0}. At time t, we seek an estimate ˆb(t) of b(t) that minimizes
the error f (b(t)) = Pt
τ =1 (b⊤ (τ )x(τ ))2 subject to the constraint kb(t)k = 1. Notice that the main
difference between linear regression and hyperplane estimation is that in the latter case the parame-
ter vector b(t) is constrained to lie in the unit sphere SD−1 . Therefore, instead of applying standard
gradient descent as in (2), we must follow the negative gradient direction along the geodesic curve
in SD−1 passing through ˆb(t). As shown in [7], the geodesic curve passing through b ∈ SD−1 along
the tangent vector v ∈ T SD−1 is b cos(kvk) + v
kvk sin(kvk). Therefore, the update equation for the
normalized gradient recursive identiﬁer on the sphere is
ˆb(t + 1) = ˆb(t) cos(kv(t)k) +

sin(kv(t)k),

(4)

v(t)
kv(t)k

where the negative normalized gradient is computed as

⊤
(ˆb
(t)x(t))x(t)
⊤
v(t) = −µ(cid:0)ID − ˆb(t)ˆb
(t)(cid:1)
1 + µkx(t)k2 .
Notice that the gradient on the sphere is essentially the same as the Euclidean gradient, except that it
⊤
needs to be projected onto the subspace orthogonal to ˆb(t) by the matrix ID − ˆb(t)ˆb
(t) ∈ RD×(D−1) .
Another difference between recursive linear regression and recursive hyperplane estimation is that
the persistence of excitation condition (3) needs to be modiﬁed to

(5)

m+S
X
Pb(t)x(t)x(t)⊤P ⊤
b(t) ≺ ρ2 ID−1 ,
ρ1 ID−1 ≺
t=m
where the projection matrix Pb(t) ∈ R(D−1)×D onto the orthogonal complement of b(t) accounts
for the fact that kb(t)k = 1. Under persistence of excitation condition (6), if b(t) = b the identiﬁer
(4) is such that ˆb(t) → b exponentially, while if {b(t + 1) − b(t)} is L2 -stable, so is {b(t) − ˆb(t)}.
3 Recursive segmentation of a known number of moving hyperplanes

(6)

In this section, we generalize the recursive identiﬁer (4) a nd its stability properties to the case of
N trajectories {xi (t)}N
i=1 lying in n hyperplanes {Sj (t)}n
j=1 .
In principle, we could apply the
identiﬁer (2) to each one of the hyperplanes. However, as we d o not know the segmentation of the
data, we do not know which data to use to update each one of the n identiﬁers. In the approach,
the n hyperplanes are represented with a single polynomial whose coefﬁcients do not depend on the
segmentation of the data. By updating the coefﬁcients of thi s polynomial, we can simultaneously
estimate all the hyperplanes, without ﬁrst clustering the p oint trajectories.
Representing moving hyperplanes with a time varying polynomial. Let x(t) be an arbitrary point
in one of the n hyperplanes. Then there is a vector bj (t) normal to Sj (t) such that b⊤
j (t)x(t) = 0.
Thus, the following homogeneous polynomial of degree n in D variables must vanish at x(t):
n (t)x(t)(cid:17) = 0.
2 (t)x(t)(cid:17) · · · (cid:16)b⊤
1 (t)x(t)(cid:17) (cid:16)b⊤
pn (x(t), t) = (cid:16)b⊤
This homogeneous polynomial can be written as a linear combination of all the monomials of degree
n in x, xI = xn1
1 xn2
2 · · · xnD
D with 0 ≤ nk ≤ n for k = 1, . . . , D , and n1 + n2 + · · · + nD = n, as
.
= X cn1 ,...,nD (t)xn1
1 · · · xnD
D = c(t)⊤ νn (x) = 0,
(8)
pn (x, t)
where cI (t) ∈ R represents the coefﬁcient of the monomial xI . The map νn : RD → RMn (D) is
known as the Veronese map of degree n, which is de ﬁned as [8]:
νn : [x1 , . . . , xD ]⊤ 7→ [. . . , xI , . . .]⊤ ,
(9)
where I is chosen in the degree-lexicographic order and Mn (D) = (cid:0)n+D−1
(cid:1) is the total number of
n
independent monomials. Notice that since the normal vectors {bj (t)} are time dependent, the vector
of coefﬁcients c(t) is also time dependent. Since both the normal vectors and the coefﬁcient vector
are de ﬁned up to scale, we will assume that kbj (t)k = kc(t)k = 1, without loss of generality.
Recursive identiﬁcation of the polynomial coef ﬁcients.
Thanks to the polynomial equation (8),
we now propose a new online hyperplane clustering algorithm that operates on the polynomial co-
efﬁcients c(t), rather than on the normal vectors {bj (t)}n
i=1 . The advantage of doing so is that c(t)
does not depend on which hyperplane the measurement x(t) belongs to. Our method operates as
follows. At each time t, we seek to ﬁnd an estimate ˆc(t) of c(t) that minimizes

(7)

t
N
X
X
τ =1
i=1
By using normalized gradient descent on SMn (D)−1 , we obtain the following recursive identiﬁer

(c(τ )⊤ νn (xi (τ )))2 .

f (c(t)) =

1
N

ˆc(t + 1) = ˆc(t) cos(kv(t)k) +

v (t)
kv(t)k

sin(kv(t)k),

(10)

(11)

.

(12)

where the negative normalized gradient is computed as
v(t) = −µ(cid:0)IMn (D) − ˆc(t)ˆc⊤ (t)(cid:1) PN
i=1 (ˆc⊤ (t)νn (xi (t)))νn (xi (t))/N
1 + µ PN
i=1 kνn (xi (t))k2/N
Notice that (11) reduces to (4) and (12) reduces to (5) if n = 1 and N = 1.
Recursive identiﬁcation of the hyperplane normals. Given an estimate of c(t), we may obtain an
estimate of the vector normal to the hyperplane containing a trajectory x(t) from the derivative of
the polynomial ˆpn (x, t) = ˆc⊤ (t)νn (x) at x(t) as
Dν⊤
n (x(t))ˆc(t)
kDν⊤
n (x(t))ˆc(t)k
where Dνn (x) is the Jacobian of νn at x. We choose the derivative of ˆpn to estimate the normal
vector bj (t), because if x(t) is a trajectory in the j th hyperplane, then b⊤
j (t)x(t) = 0, hence the
derivative of the true polynomial pn at the trajectory gives
n
X
k=1

(b⊤
ℓ (t)x(t))bk (t) ∼ bj (t).

∂ pn(x(t), t)
∂x(t)

Dpn (x(t), t) =

ˆb(x(t)) =

=

Y
ℓ 6=k

,

(13)

(14)

Stability of the recursive identiﬁer. Since in practice we do not know the true polynomial coefﬁ-
cients c(t), and we estimate b(t) from ˆc(t), we need to show that both ˆc(t) and ˆb(x(t)) track their
true values in a stable fashion. Theorem 1 shows that this is the case. Notice that the persistence
of excitation condition for multiple hyperplanes (15) is essentially the same as the one for a single
hyperplane (6), but properly modiﬁed to take into account th at the regressors are a set of trajectories
in the embedded space {νn(xi (t))}N
i=1 , rather than a single trajectory in the original space {x(t)}.
Theorem 1 Let Pc(t) ∈ R(Mn (D)−1)×Mn (D) be a projection matrix onto the orthogonal comple-
ment of c(t). Consider the recursive identiﬁer (11)– (13) and assume that the embedded regressors
i=1 are persistently exciting, i.e. there exist ρ1 , ρ2 > 0 and S ∈ N such that for all m
{νn (xi (t))}N
m+S
N
X
X
n (xi (t))P ⊤
Pc(t)νn (xi (t))ν⊤
(15)
c(t) ≺ ρ2 IMn (D)−1 .
ρ1 IMn (D)−1 ≺
t=m
i=1
Then the sequence c(t) − ˆc(t) is L2 -stable. Furthermore, if a trajectory x(t) belongs to the j th
hyperplane, then the corresponding ˆb(x(t)) in (13) is such that bj (t) − ˆb(x(t)) is L2 -stable. If in
addition the hyperplanes are static, then c(t) − ˆc(t) → 0 and bj (t) − ˆb(x(t)) → 0 exponentially.
Proof. [Sketch only] When the hyperplanes are static, the exponential convergence of c(t) to c
follows with minor modiﬁcations from Theorem 2.8, page 77 of
[5]. This implies that ∃κ, λ > 0
such that kˆc(t) − ck < κλ−t . Also, since the vectors b1 , . . . , bn are different, the polynomial
c⊤ νn (x) has no repeated factor. Therefore, there is a δ > 0 and a T > 0 such that for all t > T we
have kDνn(x(t))⊤ ck ≥ δ and kDνn (x(t))⊤ ˆc(t)k ≥ δ (see proof of Theorem 3 in [9] for the latter
claim). Combining this with kˆck ≤ kck + kˆc − ck and kck = 1, we obtain that when x(t) ∈ Sj ,
‚
‚
n (x(t))ˆc(t)kDν⊤
n (x(t))c − kDν⊤
n (x(t))ckDν⊤
kDν⊤
n (x(t))ˆc(t)
‚
‚
kbj − ˆb(x(t))k =
‚
‚
kDν⊤
n (x(t))ˆc(t)kkDν⊤
n (xt )ck
‚
‚
‚
‚

‚
‚
‚

‚
‚
‚

≤

n (x(t))ckDν⊤
n (x(t))c − kDν⊤
n (x(t))(ˆc(t) − c)kDν⊤
k(Dν⊤
n (x(t))(ˆc(t) − c))
δ2
n (x(t))k2 kˆc(t) − c)k
nE 2
α2
n (x(t))(ˆc(t) − c)kkDν⊤
kDν⊤
kDν⊤
n κλ−t
n (x(t))ck
≤ 2
≤ 2
= 2
δ2
δ2
δ2
showing that ˆb(x(t))→bj exponentially. In the last step we used the fact that for all x ∈ RD there
is a constant matrix of exponents Ekn ∈ RMn (D)×Mn−1 (D) such that ∂ νn (x)/∂ xk = Ekn νn−1 (x).
n
n−1pkνn (x)k ≤ αnEn , where En = max(kEkn k)
Therefore, kDνn(x)k ≤ Enkνn−1 (x)k = En
2(n−1)√ρ2 . Consider now the case in which the hyperplanes are moving. Since SD−1
and αn =
n
is compact, the sequences {bj (t + 1) − bj (t)}n
j=1 are trivially L2 -stable, hence so is the sequence
{c(t + 1) − c(t)}. The L2 -stability of {c(t) − ˆc(t)} and {bj (t) − ˆb(t)} follows.

,

Segmentation of the point trajectories. Theorem 1 provides us with a method for computing an
estimate ˆb(xi (t)) for the normal to the hyperplane passing through each one of the N trajectories
i=1 at each time instant. The next step is to cluster these normals into n groups,
{xi (t) ∈ RD }N
thereby segmenting the N trajectories. We do so by using a recursive version of the K-means
algorithm, adapted to vectors on the unit sphere. Essentially, at each t, we seek the normal vectors
ˆbj (t) ∈ SD−1 and the membership of wij (t) ∈ {0, 1} of trajectory i to hyperplane j that maximize
N
n
⊤
wij (t)(ˆb
j (t)ˆb(xi (t)))2 .
f ({wij (t)}, {ˆbj (t)}) =
X
X
(16)
i=1
j=1
The main difference with K-means is that we maximize the dot product of each data point with
the cluster center, rather than minimizing the distance. Therefore, the cluster center is given by the
principal component of each group, rather than the mean. In order to obtain temporally coherent
estimates of the normal vectors, we use the estimates at time t to initialize the iterations at time t + 1.

Algorithm 1 (Recursive hyperplane segmentation)

Initialization step
1: Randomly choose {ˆbj (1)}n
j=1 and ˆc(1), or else apply the GPCA algorithm to {xi (1)}N
i=1 .
For each t ≥ 1
1: Update the coefﬁcients of the polynomial ˆpn (x(t), t) = ˆc(t)⊤ νn (x(t)) using the recursive procedure

ˆc(t + 1) = ˆc(t) cos(kv (t)k) +

v (t)
sin(kv (t)k),
kv (t)k
⊤ (t)´ PN
i=1 (ˆc⊤ (t)νn(xi (t)))νn (xi (t))/N
v (t) = −µ`IMn (D) − ˆc(t)ˆc
1 + µ PN
i=1 kνn (xi (t))k2/N
2: Solve for the normal vectors from the derivatives of ˆpn at the given trajectories

.

ˆb(xi (t)) =

Dν ⊤
n (xi (t))ˆc(t)
n (xi (t))ˆc(t)k
kDν ⊤

i = 1, . . . , N .

3: Segment the normal vectors using the K-means algorithm on the sphere
(a) Set wij (t) = 8<
⊤
(ˆb
k (t)ˆb(xi (t)))2
1 if j = arg max
k=1,...,n
0 otherwise
:
(b) Set ˆbj (t) = P CA(ˆw1j (t)ˆb(x1 (t)) w2j (t)ˆb(x2 (t))
· · · wN j (t)ˆb(xN (t))˜),
(c) Iterate (a) and (b) until convergence of wij (t), and then set ˆbj (t + 1) = ˆbj (t).

,

i = 1, . . . , N , j = 1, . . . , n

j = 1, . . . , n

4 Recursive segmentation of a variable number of moving hyperplanes

In the previous section, we proposed a recursive algorithm for segmenting n moving hyperplanes
under the assumption that n is known and constant in time. However, in many practical situations
the number of hyperplanes may be unknown and time varying. For example, the number of moving
objects in a video sequence may change due to objects entering or leaving the camera ﬁeld of view.

In this section, we consider the problem of segmenting a variable number of moving hyperplanes.
We denote by n(t) ∈ N the number of hyperplanes at time t and assume we are given an upper
bound n ≥ n(t). We show that if we apply Algorithm 1 with the number of hyperplanes set to n,
then we can still recover the correct segmentation of the scene, even if n(t) < n. To see this, let us
have a close look at the persistence of excitation condition in equation (15) of Theorem 1. Since the
condition on the right hand side of (15) holds trivially when the regressors xi (t) are bounded, the
only important condition is the one on the left hand side. Notice that the condition on the left hand
side implies that the spatial-temporal covariance matrix of the embedded regressors must be of rank
Mn (D) − 1 in any time window of size S for some integer S . Loosely speaking, the embedded
regressors must be ”rich enough ” either in space or in time.
The case in which there is a ρ1 > 0 such that for all t
N
X
i=1

n (xi (t))P ⊤
Pc(t)νn (xi (t))ν⊤
c(t) ≻ ρ1 IMn (D)−1

n(t) = n

(17)

and

corresponds to the case of data that is rich in space. In this case, at each time instant we draw data
from all n hyperplanes and the data is rich enough to estimate all n hyperplanes at each time instant.
In fact, condition (17) is the one required by GPCA [4], which in this case can be applied at each
time t independently. Notice also that (17) is equivalent to (15) with S = 1.
The case in which n(t) = 1 and there are ρ1 > 0, S ∈ N and i ∈ {1, . . . , N } such that for all m
m+S
ρ1
X
N
t=m

νn (xi (t))ν⊤
n (xi (t)) ≻

IMn (D)−1

(18)

corresponds to the case of data that is rich in time. In this case, at each time instant we draw data
from a single hyperplane. As time proceeds, however, the data must be persistently drawn from
at least n hyperplanes in order for (18) to hold. This can be achieved either by having n different
static hyperplanes and persistently drawing data from all of them, or by having less than n moving
hyperplanes whose motion is rich enough so that (18) holds.

In summary, as long as the embedded regressors satisfy condition (15) for some upper bound n on
the number of hyperplanes, the recursive identiﬁer (11)-(1 3) will still provide L2 -stable estimates of
the parameters, even if the number of hyperplanes is unknown and variable, and n(t) < n for all t.

5 Experiments

Experiments on synthetic data. We randomly draw N = 200 3D points lying in n = 2 planes
and apply a time varying rotation to these points for t = 1, . . . , 1000 to generate N trajectories
i=1 . Since the true segmentation is known, we compute the vectors {bj (t)} normal to each
{xi (t)}N
plane, and use them to generate the vector of coefﬁcients c(t). We run our algorithm on the so-
generated data with n = 2, µ = 1 and a random initial estimate for the parameters. We compare
these estimates with the ground truth using the percentage of misclassiﬁed points. We also consider
the error of the polynomial coefﬁcients and the normal vecto rs by computing the angles between
the estimated and true values. Figure 1 shows the true and estimated parameters, as well as the
estimation errors. Observe that the algorithm takes about 100 seconds for the errors to stabilize
within 1.62◦ for the coefﬁcients, 1.62◦ for the normals, and 4% for the segmentation error.

True polynomial coefficients

Estimated polynomial coefficients

Estimation error of the polynomial (degrees)

1

0.5

0

−0.5

−1

0

1

0.5

0

−0.5

−1

0

200

400
600
Time (seconds)
True normal vector b1

800

1

0.5

0

−0.5

1000

−1

0

1

0.5

0

−0.5

200

400
600
Time (seconds)
Estimated normal vector b1

800

60

40

20

1000

0
0

200

400
600
Time (seconds)
Estimation error of b1 and b2 (degrees)

800

1000

40

30

20

10

b1
b2

50

40

30

20

10

Segmentation error (%)

200

400
600
Time (seconds)

800

1000

−1

0

200

400
600
Time (seconds)

800

1000

0
0

200

400
600
Time (seconds)

800

1000

0
0

200

400
600
Time (seconds)

800

1000

Figure 1: Segmenting 200 points lying on two moving planes in R3 using our recursive algorithm.

Segmentation of dynamic textures. We now apply our algorithm to the problem of segmenting
video sequences of dynamic textures, i.e. sequences of nonrigid scenes that exhibit some temporal
stationarity, e.g., water, smoke, or foliage. As proposed in [10], one can model the temporal evo-
lution of the image intensities as the output of a linear dynamical system. Since the trajectories of
the output of a linear dynamical system live in the so-called observability subspace, the intensity
trajectories of pixels associated with a single dynamic texture lie in a subspace. Therefore, the set
of all intensity trajectories lie in multiple subspaces, one per dynamic texture.

Given γ consecutive frames of a video sequence {I (f )}t
f =t−γ+1 , we interpret the data as a matrix
W (t) ∈ RN ×3γ , where N is the number of pixels, and 3 corresponds to the three RGB color
channels. We obtain a data point xi (t) ∈ RD from image I (t) by projecting the ith row of W (t),
i (t) onto a subspace of dimension D , i.e. xi (t) = Πwi (t), with Π ∈ RD×3γ . The projection
w⊤
matrix Π can be obtained in a variety of ways. We use the D principal components of the ﬁrst γ
frames to de ﬁne Π. More speciﬁcally, if W (γ ) = U ΣV ⊤ , with U ∈ RN ×D , Σ ∈ RD×D and V ∈
R3γ×D is a rank-D approximation of W (γ ) computed using SVD, then we choose Π = Σ−1V ⊤ .
We applied our method to a sequence (110 × 192, 130 frames) containing a bird ﬂoating on water,
while rotating around a ﬁx point. The task is to segment the bi rd’s rigid motion from the water’s
dynamic texture, while at the same time tracking the motion of the bird. We chose D = 5 principal
components of the γ = 5 ﬁrst frames of the RGB video sequence to project each frame on to a lower
dimensional space. Figure 2 shows the segmentation. Although the convergence is not guaranteed
with only 130 frames, it is clear that the polynomial coefﬁci ents already capture the periodicity of the
motion. As shown in the last row of Figure 2, some coefﬁcients of the polynomial oscillate in time.
One can notice that the orientation of the bird is related to the value of the coefﬁcient c8 . If the bird is
facing to the right showing her right side, the value of c8 achieves a local maximum. On the contrary
if the bird is oriented to the left, the value of c8 achieves a local minimum. Some irregularities seem
to appear at the local minima of this coefﬁcient:
they actual ly correspond to a rapid motion of
the bird. One can distinguish three behaviors for the polynomial coefﬁcients: oscillations, pseudo-
oscillations or quasi-linearity. For both the oscillations and the pseudo-oscillations the period is
identical to the bird’s motion period (40 frames). This example shows that the coefﬁcients of the
estimated polynomial give useful information about the scene motion.

−0.02

−0.03

−0.04

−0.02

−0.03

−0.04

−0.02

−0.03

−0.04

−0.02

−0.03

−0.04

−0.02

−0.03

−0.04

0

50
Time (seconds)

100

0

50
Time (seconds)

100

0

50
Time (seconds)

100

0

50
Time (seconds)

100

0

50
Time (seconds)

100

Figure 2: Segmenting a bird ﬂoating on water. Top: frames 17, 36, 60, 81, and 98 of the sequence.
Middle: segmentation obtained using our method. Bottom: temporal evolution of c8 during the
video sequence, with the red dot indicating the location of the corresponding frame in this evolution.

To test the performance of our method on a video sequence with a variable number of motions, we
extracted a sub-clip of the bird sequence (55 × 192, 130 frames) in which the camera moves up
at 1 pixel/frame until the bird disappears at t = 51. The camera stays stationary from t = 56 to
t = 66, and then moves down at 1 pixel/frame, the bird reappears at t = 76. We applied both
GPCA and our method initialized with GPCA to this video sequence. For GPCA we used a moving
window of γ = 5 frames. For our method we chose D = 5 principal components of the γ = 5
ﬁrst frames of the RGB video sequence to project each frame on to a ﬁxed lower dimensional space.
We set the parameter of the recursive algorithm to µ = 1. Figure 3 shows the segmentation results.
Notice that both methods give excellent results during the ﬁ rst few frames, when both the bird and
the water are present. This is expected, as our method is initialized with GPCA. Nevertheless,
notice that the performance of GPCA deteriorates dramatically when the bird disappears, because
GPCA overestimates the number of hyperplanes, whereas our method is robust to this change and
keeps segmenting the scene correctly, i.e. assigning all the pixels to the background. When the
bird reappears, our method detects the bird correctly from the ﬁrst frame whereas GPCA produces

a wrong segmentation for the ﬁrst frames after the bird reapp ears. Towards the end of the sequence,
both algorithms give a good segmentation. This demonstrates that our method has the ability to deal
with a variable number of motions, while GPCA has not. In addition the ﬁxed projection and the
recursive estimation of the polynomial coefﬁcients make ou r method much faster than GPCA.

Sequence

GPCA

Our method

Figure 3: Segmenting a video sequence with a variable number of dynamic textures. Top: frames 1,
24, 65, 77, and 101. Middle: segmentation with GPCA. Bottom: segmentation with our method.

6 Conclusions

We have proposed a simple recursive algorithm for segmenting trajectories lying in a variable num-
ber of moving hyperplanes. The algorithm updates the coefﬁc ients of a polynomial whose deriva-
tives give the normals to the moving hyperplanes as well as the segmentation of the trajectories. We
applied our method successfully to the segmentation of videos containing multiple dynamic textures.

Acknowledgments

The author acknowledges the support of grants NSF CAREER IIS-04-47739, NSF EHS-05-09101
and ONR N00014-05-10836.

References
[1] I. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986.
[2] J. Ho, M.-H. Yang, J. Lim, K.-C. Lee, and D. Kriegman. Clustering apperances of objects under varying
illumination conditions. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1,
pages 11–18, 2003.
[3] M. Tipping and C. Bishop. Mixtures of probabilistic principal component analyzers. Neural Computation,
11(2):443–482, 1999.
[4] R. Vidal, Y. Ma, and S. Sastry. Generalized Principal Component Analysis (GPCA). IEEE Trans. on
Pattern Analysis and Machine Intelligence, 27(12):1–15, 2005.
[5] B.D.O. Anderson, R.R. Bitmead, C.R. Johnson Jr., P.V. Kokotovic, R.L. Ikosut, I.M.Y. Mareels, L. Praly,
and B.D. Riedle. Stability of Adaptive Systems. MIT Press, 1986.
[6] L. Guo. Stability of recursive stochastic tracking algorithms. In IEEE Conf. on Decision & Control, pages
2062–2067, 1993.
[7] A. Edelman, T. Arias, and S. T. Smith. The geometry of algorithms with orthogonality constraints. SIAM
Journal of Matrix Analysis Applications, 20(2):303–353, 1998.
[8] J. Harris. Algebraic Geometry: A First Course. Springer-Verlag, 1992.
[9] R. Vidal and B.D.O. Anderson. Recursive identi ﬁcation o f switched ARX hybrid models: Exponential
convergence and persistence of excitation. In IEEE Conf. on Decision & Control, pages 32–37, 2004.
[10] G. Doretto, A. Chiuso, Y. Wu, and S. Soatto. Dynamic textures.
International Journal of Computer
Vision, 51(2):91–109, 2003.


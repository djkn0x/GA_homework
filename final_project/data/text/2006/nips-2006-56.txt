Data Integration for Classi ﬁcation Problems
Employing Gaussian Process Priors

Mark Girolami
Department of Computing Science
University of Glasgow
Scotland, UK
girolami@dcs.gla.ac.uk

Mingjun Zhong
IRISA, Campus de Beaulieu
F-35042 Rennes Cedex
France
zmingjun@irisa.fr

Abstract

By adopting Gaussian process priors a fully Bayesian solution to the problem
of integrating possibly heterogeneous data sets within a classi ﬁcation setting is
presented. Approximate inference schemes employing Variational & Expectation
Propagation based methods are developed and rigorously assessed. We demon-
strate our approach to integrating multiple data sets on a large scale protein fold
prediction problem where we infer the optimal combinations of covariance func-
tions and achieve state-of-the-art performance without resorting to any ad hoc
parameter tuning and classi ﬁer combination.

1

Introduction

Various emerging quantitative measurement technologies in the life sciences are producing genome,
transcriptome and proteome-wide data collections which has motivated the development of data in-
tegration methods within an inferential framework. It has been demonstrated that for certain predic-
tion tasks within computational biology synergistic improvements in performance can be obtained
via the integration of a number of (possibly heterogeneous) data sources. In [2] six different data
representations of proteins were employed for fold recognition of proteins using Support Vector
Machines (SVM). It was observed that certain data combinations provided increased accuracy over
the use of any single dataset. Likewise in [9] a comprehensive experimental study observed im-
provements in SVM based gene function prediction when data from both microarray expression and
phylogentic proﬁles were manually combined. More recently protein network inference was shown
to be improved when various genomic data sources were integrated [16] and in [1] it was shown that
superior prediction accuracy of protein-protein interactions was obtainable when a number of di-
verse data types were combined in an SVM. Whilst all of these papers exploited the kernel method
in providing a means of data fusion within SVM based classi ﬁe rs it was initially only in [5] that
a means of estimating an optimal linear combination of the kernel functions was presented using
semi-deﬁnite programming. However, the methods developed in [5] are based on binary SVM’s,
whilst arguably the majority of important classi ﬁcation pr oblems within computational biology are
inherently multiclass. It is unclear how this approach could be extended in a straightforward or
practical manner to discrimination over multiple-classes. In addition the SVM is non-probabilistic
and whilst post hoc methods for obtaining predictive probabilities are available [10] these are not
without problems such as over ﬁtting. On the other hand Gauss ian Process (GP) methods [11], [8]
for classi ﬁcation provide a very natural way to both integra te and infer optimal combinations of
multiple heterogeneous datasets via composite covariance functions within the Bayesian framework
an idea ﬁrst proposed in [8].

In this paper it is shown that GP’s can indeed be successfully employed on general classi ﬁcation
problems, without recourse to ad hoc binary classi ﬁcation combination schemes, where there are
multiple data sources which are also optimally combined employing full Bayesian inference. A

large scale example of protein fold prediction [2] is provided where state-of-the-art predictive perfor-
mance is achieved in a straightforward manner without resorting to any extensive ad hoc engineering
of the solution (see [2], [13]). As an additional important by-product of this work inference em-
ploying Variational Bayesian (VB) and Expectation Propagation (EP) based approximations for GP
classi ﬁcation over multiple classes are studied and assess ed in detail. It has been unclear whether EP
based approximations would provide similar improvements in performance in the multi-class setting
over the Laplace approximation and this work provides experimental evidence that both Variational
and EP based approximations perform as well as a Gibbs sampler consistently outperforming the
Laplace approximation. In addition we see that there is no statistically signi ﬁcant practical advan-
tage of EP based approximations over VB approximations in this particular setting.

2

Integrating Data with Gaussian Process Priors

Let us denote each of J independent (possibly heterogeneous) feature representations, Fj (X ), of
an object X by xj ∀ j = 1 · · · J . For each object there is a corresponding polychotomous re-
sponse target variable, t, so to model this response we assume an additive generalized regression
model. Each distinct data representation of X , Fj (X ) = xj , is nonlinearly transformed such that
fj (xj ) : Fj 7→ R and a linear model is employed in this new space such that the overall nonlinear
transformation is f (X ) = Pj βj fj (xj ).
2.1 Composite Covariance Functions
Rather than specifying an explicit functional form for each of the functions fj (xj ) we assume that
each nonlinear function corresponds to a Gaussian process (GP) [11] such that fj (xj ) ∼ GP (θj )
where GP (θj ) corresponds to a GP with trend and covariance functions mj (xj ) and Cj (xj , x0
j ; θj )
where θj denotes a set of hyper-parameters associated with the covariance function. Due to the
assumed independence of the feature representations the overall nonlinear function will also be a
realization of a Gaussian process deﬁned as f (X ) ∼ GP (θ1 · · · θJ , β1 · · · βJ ) where now the
j ; θj ). For N
overall trend and covariance functions follow as Pj βj mj (xj ) and Pj β 2
j Cj (xj , x0
j , denoted by
object samples, X1 · · · XN , each deﬁned by the J feature representations, x1
j · · · xN
fk = [fk (X1 ) · · · fk (XN )]T the overall GP prior is a
Xj , with associated class speci ﬁc response
multivariate Normal such that
fk | Xj=1···J , θ1k , · · · θJ k , α1k · · · αJ k ∼ Nfk (cid:16)0, Xj
αjkCjk (θjk )(cid:17)
The positive random variables β 2
jk are denoted by αjk , zero-trend GP functions have been assumed
j ; θjk ). A GP functional prior, over
and each Cjk (θjk ) is an N × N matrix with elements Cj (xm
j , xn
all possible responses (classes), is now available where possibly heterogeneous data sources are
integrated via the composite covariance function. It is then, in principle, a straightforward matter
to perform Bayesian inference with this model and no further recourse to ad hoc binary classi ﬁer
combination methods or ancillary optimizations to obtain the data combination weights is required.

(1)

2.2 Bayesian Inference

As we are concerned with classi ﬁcation problems over possib ly multiple classes we employ a multi-
nomial probit likelihood rather than a multinomial logit as it provides a means of developing a
Gibbs sampler, and subsequent computationally efﬁcient ap proximations, for the GP random vari-
ables. The Gibbs sampler is to be preferred over the Metropolis scheme as no tuning of a proposal
distribution is required. As in [3] the auxiliary variables ynk = fk (Xn ) + nk , nk ∼ N (0, 1) are
introduced and the N × 1 dimensional vector of target class values associated with each Xn is given
as t where each element tn ∈ {1, · · · , K }. The N × K matrix of GP random variables fk (Xn ) is
denoted by F. We represent the N × 1 dimensional columns of F by F·,k and the corresponding
K × 1 dimensional vectors, Fn,· , which are formed by the indexed rows of F . The N × K matrix
of auxiliary variables ynk is represented as Y , where the N × 1 dimensional columns are denoted
by Y·,k and the corresponding K × 1 dimensional vectors are obtained from the rows of Y as Yn,· .
The multinomial probit likelihood [3] is adopted which follows as

tn = j

if ynj = argmax
1≤k≤K

{ynk }

(2)

and this has the effect of dividing RK into K non-overlapping K -dimensional cones Ck = {y :
yk > yi , k 6= i} where RK = ∪k Ck and so each P (tn = i|Yn,· ) can be represented as δ(yni >
ynk ∀ k 6= i). Class speci ﬁc independent Gamma priors, with parameters ϕk , are placed on each
αjk and the individual components of θjk (denote Θk = {θjk , αjk }j=1···J ), a further Gamma prior
is placed on each element of ϕk with overall parameters a and b so this deﬁnes the full model
likelihood and associated priors.

2.3 MCMC Procedure

Samples from the full posterior P (Y , F, Θ1···K , ϕ1···K |X1···N , t, a, b) can be obtained from the
following Metropolis-within-Blocked-Gibbs Sampling scheme indexing over all n = 1 · · · N and
k = 1 · · · K .

(3)

(4)

Θ(i+1)
1

F(i+1)
·,k
|F(i+1)
·,1

n,· , tn ∼ T N (F(i)
|F(i)
Y(i+1)
n,· , I, tn )
n,·
|Y(i+1)
, Θ(i)
k , X1,··· ,N ∼ N (Σ(i)
k Y(i+1)
, Σ(i)
k )
·,k
·,k
, Y(i+1)
1 , X1,··· ,N ∼ P (Θ(i+1)
, ϕ(i)
)
·,k
k
, ak , bk ∼ P (ϕ(i+1)
|Θ(i+1)
ϕ(i+1)
)
k
k
k
where T N (Fn,· , I, tn ) denotes a conic truncation of a multivariate Gaussian with location pa-
rameters Fn,· and dispersion parameters I and the dimension indicated by the class value of tn
will be the largest. An accept-reject strategy can be employed in sampling from the conic trun-
cated Gaussian however this will very quickly become inefﬁc ient for problems with moderately
large numbers of classes and as such a further Gibbs sampling scheme may be required. Each
k (I + C(i)
k = C(i)
Σ(i)
jk ) with the elements of Cjk (θ (i)
jk Cjk (θ (i)
k = Pj=1 α(i)
k )−1 and C(i)
jk ) deﬁned
j ; θ (i)
jk ). A Metropolis sub-sampler is required to obtain samples for the conditional
as Cj (xm
j , xn
) and ﬁnally P (ϕ(i+1)
distribution over the composite covariance function parameters P (Θ(i+1)
)
k
k
is a simple product of Gamma distributions. The predictive likelihood of a test sample X∗ is
P (t∗ = k |X∗ , X1···N , t, a, b) which can be obtained by integrating over the posterior and predictive
prior such that

(5)

(6)

(7)

1
LS

Z P (t∗ = k |f∗ )p(f∗ |Ω, X∗ , X1···N )p(Ω|X1···N , t, a, b)df∗ dΩ
where Ω = Y , Θ1···K . A Monte-Carlo estimate is obtained by using samples drawn from the full
S PS
posterior 1
s=1 R P (t∗ = k |f∗ )p(f∗ |Ω(s) , X∗ , X1···N )df∗ and the integral over the predictive prior
requires further conditional samples, f (l|s)
, to be drawn from each p(f∗ |Ω(s) , X∗ , X1···N ) ﬁnally
∗
yielding a Monte Carlo approximation of P (t∗ = k |X∗ , X1···N , t, a, b)
∗,j (cid:17)
Ep(u) 
S
S
L
L
Φ (cid:16)u + f (l|s)
(cid:17) =
P (cid:16)t∗ = k |f (l|s)
∗,k − f (l|s)
Yj 6=k
Xs=1
Xs=1
Xl=1
Xl=1
∗


MCMC procedures for GP classi ﬁcation have been previously p resented in [8] and whilst this pro-
vides a practical means to perform Bayesian inference employing GP’s the computational cost in-
curred and difﬁculties associated with monitoring converg ence and running multiple-chains on rea-
sonably sized problems are well documented and have motivated the development of computation-
ally less costly approximations [15]. A recent study has shown that EP is superior to the Laplace
approximation for binary classi ﬁcation [4] and that for mul
ti-class classi ﬁcation VB methods are
superior to the Laplace approximation [3]. However the comparison between Variational and EP
based approximations for the multi-class setting have not been considered in the literature and so we
seek to address this issue in the following sections.

1
LS

(8)

2.4 Variational Approximation

From the conditional probabilities which appear in the Gibbs sampler it can be seen that a mean ﬁeld
approximation gives a simple iterative scheme which provides a computationally efﬁcient alternative
to the full sampler (including the Metropolis sub-sampler for the covariance function parameters),

details of which are given in [3]. However given the excellent performance of EP on a number of
approximate Bayesian inference problems it is incumbent on us to consider an EP solution here.
We should point out that only the top level inference on the GP variables is considered here and
the composite covariance function parameters will be obtained using another appropriate type-II
maximum likelihood optimization scheme if possible.

2.5 Expectation Propagation with Full Posterior Covariance

The required posterior can also be approximated by EP [7].
In this case the multinomial pro-
bit likelihood is approximated by a multivariate Gaussian such that p(F|t, X1···N ) ≈ Q(F) =
Qk p(F·,k |X1···N ) Qn gn (Fn,· )1 where gn (Fn,· ) = NFn,· (µn , Λn ), µn is a K × 1 vector and
Λn is a full K × K dimensional covariance matrix. Denoting the cavity density as Q\n (F) =
Qk p(F·,k |X1···N ) Qi,i 6=n gi (Fi,· ), EP proceeds by iteratively re-estimating the moments µn , Λn
by moment matching [7] giving the following
n,· } − E ˆpn {Fn,· }E ˆpn {Fn,· }T ,
n = E ˆpn {Fn,·FT
n = E ˆpn {Fn,·} and Λnew
µnew
(9)

n Q\n (Fn,· )p(tn |Fn,· ), and Zn is the required normalizing (partition) function
where ˆpn = Z −1
which is required to obtain the above mean and covariance estimates. To proceed an analytic form
for the partition function Zn is required. Indeed for binary classi ﬁcation employing a bi nomial pro-
bit likelihood an elegant EP solution follows due to the analytic form of the partition function [4].
However for the case of multiple classes with a multinomial probit likelihood the partition function
no longer has a closed analytic form and further approximations are required to make any progress.
There are two strategies which we consider, the ﬁrst retains
the full posterior coupling in the covari-
ance matrices Λn by employing Laplace Propagation (LP) [14] and the second assumes no posterior
coupling in Λn by setting this as a diagonal covariance matrix. The second form of approximation
has been adopted in [12] when developing a multi-class version of the Informative Vector Machine
(IVM) [6]. In the ﬁrst case where we employ LP an additional si gni ﬁcant O(K 3N 3 ) computational
scaling will be incurred however it can be argued that the retention of the posterior coupling is im-
portant. For the second case clearly we lose this explicit posterior coupling but, of course, do not
incur the expensive computational overhead required of LP. We observed in unreported experiments
that there is little of statistical signi ﬁcance lost, in ter ms of predictive performance, when assuming
a factorable form for each ˆpn . LP proceeds by propagating the approximate moments such that
n,· (cid:21)−1
∂ 2 log ˆpn
n ≈ (cid:20)−
log ˆpn and Λnew
∂Fn,·∂FT
The required derivatives follow straightforwardly and details are included in the accompanying ma-
terial. The approximate predictive distribution for a new data point x∗ requires a Monte Carlo
estimate employing samples drawn from a K -dimensional multivariate Gaussian for which details
are given in the supplementary material2 .

n ≈ argmax
µnew
Fn,·

(10)

2.6 Expectation Propagation with Diagonal Posterior Covariance

By assuming a factorable approximate posterior, as in the variational approximation [3], a dis-
tinct simpli ﬁcation of the problem setting follows, where n ow we assume that gn (Fn,· ) =
Qk NFn,k (µn,k , λn,k ) i.e.
is a factorable distribution. This assumption has already been made
in [12] in developing an EP based multi-class IVM. Now signi ﬁ cant computational simpli ﬁca-
nk = E ˆpnk {Fn,k } and λnew
tion follows where the required moment matching amounts to µnew
nk =
n,k } − E ˆpnk {Fn,k }2 where the density ˆpnk has a partition function which now has the
E ˆpnk {F2
analytic form
u + vqλ\n
Zn = Ep(u)p(v) 

Φ 

ni − µ\n
ni + µ\n
K
nj
Yj=1,j 6=i


q1 + λ\n


nj
1Conditioning on the covariance function parameters and associated hyper-parameters is implicit
2Supplementary material
http://www.dcs.gla.ac.uk/people/personal/girolami/
pubs_2006/NIPS2006/index.htm

(11)

where u and v are both standard Normal random variables (vqλ\n
ni = Fn,i − µ\n
ni ) with λ\n
ni and
µ\n
ni having the usual meanings (details in accompanying material). Derivatives of this partition
function follow in a straightforward way now allowing the required EP updates to proceed (details
in supplementary material). The approximate predictive distribution for a new data point X∗ in this
case takes a similar form to that for the Variational approximation [3]. So we have
!
P (t∗ = k |X∗ , X1···N , t) = Ep(u)p(v) 
Φ   u + vpλ∗
K
k − µ∗
k + µ∗
j
Yj=1,j 6=k
p1 + λ∗
j


where the predictive mean and variance follow in standard form.
j )T (Cj + Λj )−1 C∗
j )T (Cj + Λj )−1 µj and λ∗
j − (C∗
j = c∗
j = (C∗
µ∗
j

(12)

(13)

It should be noted here that the expectation over p(u) and p(v) could be computed by using either
Gaussian quadrature or a simple Monte Carlo approximation which is straightforward as sampling
from a univariate standardized Normal only is required. The VB approximation [3] however only
requires a 1-D Monte Carlo integral rather than the 2-D one required here.

3 Experiments

Before considering the main example of data integration within a large scale protein fold predic-
tion problem we attempt to assess a number of approximate inference schemes for GP multi-class
classi ﬁcation. We provide a short comparative study of the L aplace, VB, and both possible EP
approximations by employing the Gibbs sampler as the comparative gold standard. For these exper-
iments six multi-class data sets are employed 3 , i.e., Iris (N = 150, K = 3), Wine (N = 178, K =
3), Soybean (N = 47, K = 4), Teaching (N = 151, K = 3), Waveform (N = 300, K = 3) and ABE
(N = 300, K = 3, which is a subset of the Isolet dataset using the letters ‘A’, ‘B’ and ‘E’,). A single
radial basis covariance function with one length scale parameter is used in this comparative study.
Ten-fold cross validation (CV) was used to estimate the predictive log-likelihood and the percentage
predictive error. Within each of the ten folds a further 10 CV routine was employed to select the
length-scale of the covariance function. For the Gibbs sampler, after a burn-in of 2000 samples,
the following 3000 samples were used for inference, and the predictive error and likelihood were
computed from the 3000 post-burn-in samples. For each data set and each method the percentage
predictive error and the predictive log-likelihood were estimated in this manner. The summary re-
sults given as the mean and standard deviation over the ten folds are shown in Table 1. The results
which cannot be distinguished from each other, under a Wilcoxon rank sum test with a 5% signi ﬁ-
cance level, are highlighted in bold. From those results, we can see that across most data sets used,
the predictive log-likelihood obtained from the Laplace approximation is lower than those of the
three other methods. In our observations, the predictive performance of VB and the IEP approxi-
mation are consistently indistinguishable from the performance achieved from the Gibbs sampler.
From the experiments conducted there is no evidence to suggest any difference in predictive perfor-
mance between IEP & VB methods in the case of multi-way classi ﬁcation. As there is no beneﬁt
in choosing an EP based approximation over the Variational one we now select the Variational ap-
proximation in that inference over the covariance parameters follows simply by obtaining posterior
mean estimates using an importance sampler.

As a brief illustration of how the Variational approximation compares to the full Metropolis-within-
Blocked-Gibbs Sampler consider a toy dataset consisting of three classes formed by a Gaussian
surrounded by two annular rings having ten features only two of which are predictive of the class
labels [3]. We can compare the compute time taken to obtain reasonable predictions from the full
MCMC and the approximate Variational scheme [3]. Figure 1 (a) shows the samples of the co-
variance function parameters Θ drawn from the Metropolis subsampler4 and overlaid in black the
corresponding approximate posterior mean estimates obtained from the variational scheme [3]. It

3http://www.ics.uci.edu/ ˜ mlearn/MPRepository.html
4 It should be noted that multiple Metropolis sub-chains had to be run in order to obtain reasonable sampling
of the Θ ∈ R10
+

Table 1: Percentage predictive error (PE) and predictive log-likelihood (PL) for six data sets from
UCI computed using Laplace, Variational Bayes (VB), independent EP (IEP), as well as MCMC
using Gibbs sampler. Best results which are statistically indistinguishable from each other are high-
lighted in bold.

ABE
PE
4.000±3.063
2.000±2.330
3.333±3.143
5.333±5.019
Wine
PE
3.889±5.885
2.222±3.884
4.514±5.757
3.889±5.885
Teach
PE
39.24±15.74
41.12±9.92
42.41±6.22
42.54±11.32

PL
-0.290±0.123
-0.164±0.026
-0.158±0.037
-0.139±0.050

PL
-0.258±0.045
-0.182±0.057
-0.177±0.054
-0.133±0.047

PL
-0.836±0.072
-0.711±0.125
-0.730±0.113
-0.800±0.072

Laplace
VB
Gibbs
IEP

Laplace
VB
Gibbs
IEP

Laplace
VB
Gibbs
IEP

100

10−2

10−4

r
o
r
r
E
 
e
g
a
t
n
e
c
r
e
P

70

60

50

40

30

20

10

PL
-0.132±0.052
-0.087±0.056
-0.079±0.056
-0.063±0.059

PL
-0.359±0.040
-0.158±0.034
-0.158±0.039
-0.172±0.037

PL
-0.430±0.085
-0.410±0.100
-0.380±0.116
-0.383±0.107

Iris
PE
3.333±3.513
3.333±3.513
3.333±3.513
3.333±3.513
Soybean
PE
0.000±0.000
0.000±0.000
0.000±0.000
0.000±0.000
Wave
PE
17.50±9.17
18.33±9.46
15.83±8.29
17.50±10.72

−0.2

−0.4

−0.6

−0.8

−1

d
o
o
h
i
l
e
k
i
L
 
e
v
i
t
c
i
d
e
r
P

100

101

102

0
100

Time (Seconds − Log)

105

−1.2
100

Time (Seconds − Log)

105

(a)

(b)

(c)

Figure 1: (a) Progression of MCMC and Variational methods in estimating covariance function
parameters, vertical axis denotes each θd , horizontal axis is time (all log scale) (b) percentage er-
ror under the MCMC (gray) and Variational (black) schemes, (c) predictive likelihood under both
schemes.

is clear that after 100 calls to the sub-sampler the samples obtained reﬂect the relevance of the fea-
tures, however the deterministic steps taken in the variational routine achieve this in just over ten
computational steps of equal cost to the Metropolis sub-sampler. Figure 1 (b) shows the predictive
error incurred by the classi ﬁer and under the MCMC scheme 30, 000 CPU seconds are required to
achieve the same level of predictive accuracy under the variational approximation obtained in 200
seconds (a factor of 150 times faster). This is due, in part, to the additional level of sampling from
the predictive prior which is required when using MCMC to obtain predictive posteriors. Because of
these results we now adopt the variational approximation for the following large scale experiment.

4 Protein Fold Prediction with GP Based Data Fusion

To illustrate the proposed GP based method of data integration a substantial protein fold classi ﬁca-
tion problem originally studied in [2] and more recently in [13] is considered. The task is to devise a
predictor of 27 distinct SCOP classes from a set (N = 314) of low homology protein sequences. Six

y
c
a
r
u
c
c
A
 
t
n
e
c
r
e
P

60

50

40

30

20

10

0

AA HP PT PY SS VP MA MF

d
o
o
h
i
l
e
k
i
L
 
e
v
i
t
c
i
d
e
r
P

0.2

0.15

0.1

0.05

0

AA HP PT PY SS VP MA MF

(a)

(b)

t
h
g
i
e
W
 
a
h
p
l
A

2.5

2

1.5

1

0.5

0

AA

HP

PT

SS

VP

PZ

(c)

Figure 2: (a) The prediction accuracy for each individual data set and the corresponding combi-
nations, (MA) employing inferred weights and (MF) employing a ﬁxed weighting scheme (b) The
predictive likelihood achieved for each individual data set and with the integrated data (c) The pos-
terior mean values of the covariance function weights α1 · · · α6 .

different data representations (each comprised of around 20 features) are available characterizing
(1) Amino Acid composition (AA); (2) Hydrophobicity proﬁle (HP); (3) Polarity (PT); (4) Polariz-
ability (PY); (5) Secondary Structure (SS); (6) Van der Waals volume proﬁle of the protein (VP).
In [2] a number of classi ﬁer and data combination strategies were employed in devising a multiway
classi ﬁer from a series of binary SVM’s. In the original work of [2] the best predictive accuracy
obtained on an independent set (N = 385) of low sequence similarity proteins was 53%. It was
noted after extensive careful manual experimentation by the authors that a combination of Gaussian
kernels each composed of the (AA), (SS) and (HP) datasets signi ﬁcantly improved predictive accu-
racy. More recently in [13] a heavily tuned ad hoc ensemble combination of classi ﬁers raised this
performance to 62% the best reported on this problem. We employ the proposed GP based method
(Variational approximation) in devising a classi ﬁer for th is task where now we employ a composite
covariance function (shared across all 27 classes), a linear combination of RBF functions for each
data set. Figure (2) shows the predictive performance of the GP classi ﬁer in terms of percentage
prediction accuracy (a) and predictive likelihood on the independent test set (b). We note a sig-
ni ﬁcant synergistic increase in performance when all data s ets are combined and weighted (MA)
where the overall performance accuracy achieved is 62%. Although the 0-1 loss test error is the
same for an equal weighting of the data sets (MF) and that obtained using the proposed inference
procedure (MA) for (MA) there is an increase in predictive likelihood i.e. more conﬁdent correct
predictions being made. It is interesting to note that the weighting obtained (posterior mean for α)
Figure (2.c) weights the (AA) & (SS) with equal importance whilst other data sets play less of a role
in performance improvement.

5 Conclusions

In this paper we have considered the problem of integrating data sets within a classi ﬁcation setting, a
common scenario within many bioinformatics problems. We have argued that the GP prior provides
an elegant solution to this problem within the Bayesian inference framework. To obtain a computa-
tionally practical solution three approximate approaches to multi-class classi ﬁcation with GP priors,
i.e. Laplace, Variational and EP based approximations have been considered. It is found that EP and
Variational approximations approach the performance of a Gibbs sampler and indeed their predictive
performances are indistinguishable at the 5% level of signi ﬁcance. The full EP (FEP) approximation
employing LP has an excessive computational cost and there is little to recommend it in terms of
predictive performance over the independent assumption (IEP). Likewise there is little to distinguish
between IEP and VB approximations in terms of predictive performance in the multi-class classi ﬁ-
cation setting though further experiments on a larger number of data sets is desirable. We employ
VB to infer the optimal parameterized combinations of covariance functions for the protein fold
prediction problem over 27 possible folds and achieve state-of-the-art performance without recourse
to any ad hoc tinkering and tuning and the inferred combination weights are intuitive in terms of
the information content of the highest weighted data sets. This is a highly practical solution to the
problem of heterogenous data fusion in the classi ﬁcation se tting which employs Bayesian inferen-

tial semantics throughout in a consistent manner. We note that on the fold prediction problem the
best performance achieved is equaled without resorting to complex and ad hoc data and classi ﬁer
weighting and combination schemes.

5.1 Acknowledgements

MG is supported by the Engineering and Physical Sciences Research Council (UK) grant number
EP/C010620/1, MZ is supported by the National Natural Science Foundation of China grant number
60501021.

References

[1] A. Ben-Hur and W.S. Noble. Kernel methods for predicting protein-protein interactions. Bioin-
formatics, 21, Suppl. 1:38–46, 2005.
[2] Chris Ding and Inna Dubchak. Multi-class protein fold recognition using support vector ma-
chines and neural networks. Bioinformatics, 17:349–358, 2001.
[3] Mark Girolami and Simon Rogers. Variational Bayesian multinomial probit regression with
Gaussian process priors. Neural Computation, 18(8):1790–1817, 2006.
[4] M. Kuss and C.E. Rasmussen. Assessing approximate inference for binary Gaussian process
classi ﬁcation.
Journal of Machine Learning Research, 6:1679–1704, 2005.
[5] G. R. G. Lanckriet, T. De Bie, N. Cristianini, M. I. Jordan, and W. S. Noble. A statistical
framework for genomic data fusion. Bioinformatics, 20:2626–2635, 2004.
[6] Neil Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods:
The informative vector machine. In S. Thrun S. Becker and K. Obermayer, editors, Advances
in Neural Information Processing Systems 15. MIT Press.
[7] Thomas Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT,
2001.
[8] R. Neal. Regression and classi ﬁcation using Gaussian pr ocess priors.
In A.P. Dawid,
M. Bernardo, J.O. Berger, and A.F.M. Smith, editors, Bayesian Statistics 6, pages 475–501.
Oxford University Press, 1998.
[9] Paul Pavlidis, Jason Weston, Jinsong Cai, and William Stafford Noble. Learning gene func-
tional classi ﬁcations from multiple data types.
Journal of Computational Biology, 9(2):401–
411, 2002.
[10] J.C. Platt. Probabilities for support vector machines. In A. Smola, P. Bartlett, B. Schlkopf,
and D. Schuurmans, editors, Advances in Large Margin Classi ﬁers , pages 61–74. MIT Press,
1999.
[11] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine
Learning. MIT Press, 2006.
Efﬁcient n onparametric Bayesian
[12] M.W. Seeger, N.D. Lawrence, and R. Herbrich.
approximations.
modelling with
sparse Gaussian
process
Technical Report,
”http://www.kyb.tuebingen.mpg.de/bs/people/seeger/ ”
, 2006.
[13] Hong-Bin Shen and Kuo-Chen Chou. Ensemble classi ﬁer fo r protein fold pattern recognition.
Bioinformatics, Advanced Access(doi:10.1093), 2006.
[14] Alexander Smola, Vishy Vishwanathan, and Eleazar Eskin. Laplace propagation. In Sebas-
tian Thrun, Lawrence Saul, and Bernhard Sch ¨olkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA, 2004.
[15] C.K.I. Williams and D. Barber. Bayesian classi ﬁcation with Gaussian processes. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 20(12):1342–1352, 1998.
[16] Y. Yamanishi, J. P. Vert, and M. Kanehisa. Protein network inference from multiple genomic
data: a supervised approach. Bioinformatics, 20, Suppl. 1:363–370, 2004.


Geometric entropy minimization (GEM) for anomaly
detection and localization

Alfred O Hero, III
University of Michigan
Ann Arbor, MI 48109-2122
hero@umich.edu

Abstract

We introduce a novel adaptive non-parametric anomaly detection approach, called
GEM, that is based on the minimal covering properties of K-point entropic graphs
when constructed on N training samples from a nominal probability distribu-
tion. Such graphs have the property that as N → ∞ their span recovers the
entropy minimizing set that supports at least ρ = K/N (100)% of the mass of the
Lebesgue part of the distribution. When a test sample falls outside of the entropy
minimizing set an anomaly can be declared at a statistical level of signiﬁcance
α = 1 − ρ. A method for implementing this non-parametric anomaly detector is
proposed that approximates this minimum entropy set by the in ﬂuence region of a
K-point entropic graph built on the training data. By implementing an incremental
leave-one-out k-nearest neighbor graph on resampled subsets of the training data
GEM can efﬁciently detect outliers at a given level of signiﬁcance and compute
their empirical p-values. We illustrate GEM for several simulated and real data
sets in high dimensional feature spaces.

1

Introduction

Anomaly detection and localization are important but notoriously difﬁcult problems. In such prob-
lems it is crucial to identify a nominal or baseline feature distribution with respect to which statisti-
cally signiﬁcant deviations can be reliably detected. However, in most applications there is seldom
enough information to specify the nominal density accurately, especially in high dimensional fea-
ture spaces for which the baseline shifts over time. In such cases standard methods that involve
estimation of the multivariate feature density from a ﬁxed training sample are inapplicable (high
dimension) or unreliable (shifting baseline). In this paper we propose an adaptive non-parametric
method that is based on a class of entropic graphs [1] called K -point minimal spanning trees [2]
and overcomes the limitations of high dimensional feature spaces and baseline shift. This method
detects outliers by comparing them to the most concentrated subset of points in the training sample.
It follows from [2] that this most concentrated set converges to the minimum entropy set of proba-
bility ρ as N → ∞ and K/N → ρ. Thus we call this approach to anomaly detection the geometric
entropy minimization (GEM) method.

Several approaches to anomaly detection have been previously proposed. Parametric approaches
such as the generalized likelihood ratio test lead to simple and classical algorithms such as the Stu-
dent t-test for testing deviation of a Gaussian test sample from a nominal mean value and the Fisher
F-test for testing deviation of a Gaussian test sample from a nominal variance. These methods fall
under the statistical nomenclature of the classical slippage problem [3] and have been applied to
detecting abrupt changes in dynamical systems, image segmentation, and general fault detection ap-
plications [4]. The main drawback of these algorithms is that they rely on a family of parameterically
deﬁned nominal (no-fault) distributions.

An alternative to parametric methods of anomaly detection are the class of novelty detection algo-
rithms and include the GEM approach described herein. Scholkopf and Smola introduced a kernel-
based novelty detection scheme that relies on unsupervised support vector machines (SVM) [5]. The
single class minimax probability machine of Lanckriet etal [6] derives minimax linear decision re-
gions that are robust to unknown anomalous densities. More closely related to our GEM approach is
that of Scott and Nowak [7] who derive multiscale approximations of minimum-volume-sets to esti-
mate a particular level set of the unknown nominal multivariate density from training samples. For a
simple comparative study of several of these methods in the context of detecting network intrusions
the reader is referred to [8].

The GEM method introduced here has several features that are summarized below. (1) Unlike the
MPM method of Lanckriet etal [6] the GEM anomaly detector is not restricted to linear or even
convex decision regions. This translates to higher power for speciﬁed false alarm level. (2) GEMs
computational complexity scales linearly in dimension and can be applied to level set estimation
in feature spaces of unprecedented (high) dimensionality. (3) GEM has no complicated tuning pa-
rameters or function approximation classes that must be chosen by the user. (4) Like the method
of Scott and Nowak [7] GEM is completely non-parametric, learning the structure of the nominal
distribution without assumptions of linearity, smoothness or continuity of the level set boundaries.
(5) Like Scott and Nowak’s method, GEM is provably optimal - indeed uniformly most powerful
of speciﬁed level - for the case that the anomaly density is a mixture of the nominal and a uniform
density. (6) GEM easily adapts to local structure, e.g. changes in local dimensionality of the support
of the nominal density.

We introduce an incremental Leave-one-out (L1O) kNNG as a particularly versatile and fast anom-
aly detector in the GEM class. Despite the similarity in nomenclature, the L1O kNNG is different
from k nearest neighbor (kNN) anomaly detection of [9]. The kNN anomaly detector is based on
thresholding the distance from the test point to the k-th nearest neighbor. The L1O kNNG detector
computes the change in the topology of the entire kNN graph due to the addition of a test sample and
does not use a decision threshold. Furthermore, the parent GEM anomaly detection methodology
has proven theoretical properties, e.g. the (restricted) optimality property for uniform mixtures and
general consistency properties.

We introduce the statistical framework for anomaly detection in the next section. We then describe
the GEM approach in Section . Several simulations are presented n Section 4.

2 Statistical framework
The setup is the following. Assume that a training sample Xn = {X1 , . . . , Xn } of d-dimensional
vectors Xi is available. Given a new sample X the objective is to declare X to be a ”nominal”
sample consistent with Xn or an ”anomalous” sample that is signi ﬁcantly different from
Xn . This
declaration is to be constrained to give as few false positives as possible. To formulate this problem
we adopt the standard statistical framework for testing composite hypotheses. Assume that Xn is
an independent identically distributed (i.i.d.) sample from a multivariate density f0 (x) supported on
the unit d-dimensional cube [0, 1]d . Let X have density f (x). Anomaly detection can be formulated
as testing the hypotheses H0 : f = fo versus H0 : f (cid:3)= fo at a prescribed level α of signiﬁcance
P (declare H1 |H0 ) ≤ α.
(cid:1)
(cid:1)
The minimum-volume-set of level α is deﬁned as a set Ωα in IRd which minimizes the volume
(cid:1)
|Ωα | =
Ωα f0 (x)dx ≥ 1 − α. The minimum-entropy-set of level
Ωα dx subject to the constraint
(cid:1)
α is deﬁned as a set Λα in IRd which minimizes the R ´enyi entropy Hν (Λα ) = 1
1−α ln
Λα f ν (x)dx
Λα f0 (x)dx ≥ 1 − α. Here ν is any real valued parameter between
subject to the constraint
0 < ν < 1. When f is a Lebesgue density in IRd it is easy to show that these three sets are identical
almost everywhere.
The test ”decide anomaly if X (cid:3)∈ Ωα ” is equivalent to implementing the test function
(cid:2)
(cid:3)
1, x (cid:3)∈ Ωα
φ(x) =
0,
o.w.
This test has a strong optimality property: when f0 is Lebesgue continuous it is a uniformly most
powerful (UMP) level α for testing anomalies that follow a uniform mixture distribution. Specif-

.

ically, let X have density f (x) = (1 − )f0 (x) + U (x) where U (x) is the uniform density over
[0, 1]d and  ∈ [0, 1]. Consider testing the hypotheses
 = 0
:
H0
:
 > 0
H1

(1)
(2)

Proposition 1 Assume that under H0 the random vector X has a Lebesgue continuous density f0
and that Z = f0 (X ) is also a continuous random variable. Then the level-set test of level α is
uniformly most powerful for testing (2). Furthermore, its power function β = P (X (cid:3)∈ Ωα |H1 ) is
given by
β = (1 − )α + (1 − |Ωα |).
A sufﬁcient condition for the random variable Z above to be continuous is that the density f0 (x)
have no ﬂat spots over its support set {f0 (x) > 0}. The proof of this proposition is omitted.
There are two difﬁculties with implementing the level set test. First, for known f0 the level set
may be very difﬁcult if not impossible to determine in high dimensions d (cid:7) 2. Second, when only
a training sample from f0 is available and f0 is unknown the level sets have to be learned from
the training data. There are many approaches to doing this for minimum volume tests and these
are reviewed in [7]. These methods can be divided into two main approaches: density estimation
followed by plug in estimation of Ωα via variational methods; and (2) direct estimation of the level
set using function approximation and non-parametric estimation. Since both approaches involve
explicit approximation of high dimensional quantities, e.g. the multivariate density or the boundary
of the set Ωα, these methods are difﬁcult to apply in high dimensional problems, i.e. d > 2. The
GEM method we propose in the next section overcomes these difﬁculties.

3 GEM and entropic graphs

GEM is a method that directly estimates the critical region for detecting anomalies using mini-
mum coverings of subsets of points in a nominal training sample. These coverings are obtained by
constructing minimal graphs, e.g. a MST or kNNG, covering a K -point subset that is a given pro-
portion of the training sample. Points not covered by these K -point minimal graphs are identi ﬁed
as tail events and allow one to adaptively set a pvalue for the detector.
For a set of n points Xn in IRd a graph G over Xn is a pair (V , E ) where V = Xn is the set of vertices
(cid:4)
and E = {e} is the set of edges of the graph. The total power weighted length, or, more simply, the
length, of G is L(Xn ) =
e∈E |e|γ where γ > 0 is a speci ﬁed edge exponent parameter.

3.1 K-point MST
The MST with power weighting γ is deﬁned as the graph that spans Xn with minimum total length:
(cid:5)
LM ST (Xn ) = min
|e|γ .
T ∈T
e∈T
where T is the set of all trees spanning Xn .
(cid:7)
(cid:6)
subsets of K distinct points from Xn .
Deﬁnition 1 K-point MST : Let Xn,K denote one of the
n
K
Among all of the MST’s spanning these sets, the K-MST is deﬁned as the one having minimal length
minXn,K ⊂Xn LM ST (Xn,k ).
The K -MST thus speciﬁes the minimal subset of K points in addition to specifying the minimum
length. This subset of points, which we call a minimal graph covering of Xn of size K , can be viewed
as capturing the densest region of Xn . Furthermore, if Xn is a i.i.d. sample from a multivariate
density f (x) and if limK,n→∞ K/n = ρ and a greedy version of the K -MST is implemented, this
set converges a.s. to the minimum ν -entropy set containing a proportion of at least ρ = K/n of
the mass of the (Lebesgue component of) f (x), where ν = (d − γ )/d. This fact was used in [2] to
motivate the greedy K -MST as an outlier resistant estimator of entropy for ﬁnite n, K .

Deﬁne the K -point subset

n,K = argminXn,K ⊂Xn LM ST (Xn,K )
X ∗

selected by the greedy K-MST. Then we have the following As the minimum entropy set and min-
imum volume set are identical, this suggests the following minimal-volume-set anomaly detection
algorithm, which we call the ”K-MST anomaly detector.”

K-MST anomaly detection algorithm
[1]Process training sample: Given a level of signiﬁcance α and a training sample Xn =
{X1 , . . . , Xn }, construct the greedy K-MST and retain its vertex set X ∗
n,K .
[2]Process test sample: Given a test sample X run the K-MST on the merged training-test sample
Xn+1 = Xn ∪ {X } and store the minimal set of points X ∗
n+1,K .
[3]Make decision: Using the test function φ deﬁned below decide H1 if φ(X ) = 1 and decide H0
(cid:2)
if φ(X ) = 0.
1, X (cid:3)∈ X ∗
n+1,K
0,
o.w.

φ(x) =

.

When the density f0 generating the training sample is Lebesgue continuous, it follows from [2, The-
orem 2] that as K, n → ∞ the K-MST anomaly detector has false alarm probability that converges
to α = 1 − K/n and power that converges to that of the minimum-volume-set test of level α. When
the density f0 is not Lebesgue continuous some optimality properties of the K-MST anomaly detec-
tor still hold. Let this nominal density have the decomposition f0 = λ0 + δ0 , where λ0 is Lebesgue
continuous and δ0 is singular. Then, according to [2, Theorem 2], the K-MST anomaly detector will
have false alarm probability that converges to (1 − ψ)α, where ψ is the mass of the singular compo-
nent of f0 , and it is a uniformly most powerful test for anomalies in the continuous component, i.e.
for the test of H0 : λ = λ0 , δ = δ0 against H1 : λ = (1 − )λ0 + U (x), δ = δ0 .
It is well known that the K-MST construction is of exponential complexity in n [10]. In fact, even
for K = n − 1, a case one can call the leave-one-out MST, there is no simple fast algorithm
for computation. However, the leave-one-out kNNG, described below, admits a fast incremental
algorithm.

3.2 K-point kNNG
Let Xn = {X1 , . . . , Xn } be a set of n points. The k nearest neighbors (kNN) {Xi(1) , . . . Xi(k) }
of a point Xi ∈ Xn are the k closest points to Xi points in Xn − {Xi }. Here the measure of
closeness is the Euclidean distance. Let {ei(1) , . . . , ei(k) } be the set of edges between Xi and its k
nearest neighbors. The kNN graph (kNNG) over Xn is deﬁned as the union of all of the kNN edges
{ei(1) , . . . , ei(k) }n
i=1 and the total power weighted edge length of the kNN graph is
k(cid:5)
n(cid:5)
|ei(l) |γ .
LkN N (Xn ) =
(cid:7)
(cid:6)
i=1
l=1
Deﬁnition 2 K-point kNNG : Let Xn,K denote one of the
subsets of K distinct points from Xn .
n
K
Among all of the kNNG over each of these sets, the K-kNNG is deﬁned as the one having minimal
length minXn,K ⊂Xn LkN N (Xn,k ).

As the kNNG length is also a quasi additive continuous functional [11], the asymptotic KMST
theory of [2] extends to the K-point kNNG. Of course, computation of the K-point kNNG also has
exponential complexity. However, the same type of greedy approximation introduced by Ravi [10]
for the K -MST can be implemented to reduce complexity of the K-point kNNG. This approximation
to the K-point kNNG will satisfy the tightly coverable graph property of [2, Defn. 2]. We have the
following result that justiﬁes the use of such an approximation as an anomaly detector of level
α = 1 − ρ, where ρ = K/n:
Proposition 2 Let X ∗
n,K be the set of points in Xn that results from any approximation to the K-
point kNNG that satisﬁes the property [2, Defn. 2]. Then limn→∞ P0 (X ∗
n,K ⊂ Ωα ) → 1 and
limn→∞ P0 (X ∗
n,K ∩ Ωα ) → 0, where K = K (n) = ﬂoor(ρn), Ωα is a minimum-volume-set of
level α = 1 − ρ and Ωα = [0, 1]d − Ωα .

Proof: We provide a rough sketch using the terminology of [2]. Recall that a set Bm ⊂ [0, 1]d
of resolution 1/m is representable by a union of elements of the uniform partition of [0, 1]d into
hypercubes of volume 1/md . Lemma 3 of [2] asserts that there exists an M such that for m > M the
limits claimed in Proposition 2 hold with Ωα replaced by Am
α , a minimum volume set of resolution
1/m that contains Ωα . As limm→∞ Am
ρ = Ωα this establishes the proposition.
Figures 1-2 illustrate the use of the K-point kNNG as an anomaly detection algorithm.

Bivariate Gaussian mixture density

K−point kNNG, k=5, N=200,  ρ=0.9, K=180

3

2

1

0

−1

−2

−3

−4

−5

−6

−4

−2

0

2

4

3

2

1

0

−1

−2

−3

−4

−5

−6

−4

−2

0

2

4

Figure 1: Left: level sets of the nominal bivariate mixture density used to illustrate the K point kNNG
anomaly detection algorithms. Right: K-point kNNG over N=200 random training samples drawn
from the nominal bivariate mixture at left. Here k=5 and K=180, corresponding to a signiﬁcance
level of α = 0.1.

K−point kNNG, k=5, N=200,  ρ=0.9, K=180

K−point kNNG, k=5, N=200,  ρ=0.9, K=180

3

2

1

0

−1

−2

−3

−4

−5

−6

−4

−2

0

2

4

3

2

1

0

−1

−2

−3

−4

−5

−6

−4

−2

0

2

4

Figure 2: Left: The test point ’*’ is declared anomalous at level α = 0.1 as it is not captured by the
K-point kNNG (K=180) constructed over the combined test sample and the training samples drawn
from the nominal bivariate mixture shown in Fig. 1. Right: A different test point ’*’ is declared
non-anomalous as it is captured by this K-point kNNG.

3.3 Leave-one-out kNNG (L1O-kNNG)

The theoretical equivalence between the K-point kNNG and the level set anomaly detector motivates
a low complexity anomaly detection scheme, which we call the leave-one-out kNNG, discussed
in this section and adopted for the experiments below. As before, assume a single test sample
X = Xn+1 and a training sample Xn . Fix k and assume that the kNNG over the set Xn has been
computed. To determine the kNNG over the combined sample Xn+1 = Xn ∪ {Xn+1 } one can
execute the following algorithm:

L1O kNNG anomaly detection algorithm
1. For each Xi ∈ Xn+1 , i = 1, . . . , n + 1, compute the kNNG total length difference
∆iLkN N = LkN N (Xn+1 ) − LkN N (Xn+1 − {Xi }) by the following steps. For each
i:

(a) Find the k edges E k
i→∗ of all of the kNN’s of Xi .
(b) Find the edges E k∗→i of other points in Xn+1 − {Xi } that have Xi as one of their
(cid:4)
(cid:4)
|e|γ − (cid:4)
kNNs. For these points ﬁnd the edges E k+1∗
to their respective k + 1st NN point.
|e|γ +
|e|γ
(c) Compute ∆iLkN N =
e∈E k
e∈E k∗→i
e∈E k+1∗
i→∗
Xo = argmaxi=1,...,n+1∆iLkN N .
2. Deﬁne the kNNG most ”outlying point ” as
3. Declare the test sample Xn+1 an anomaly if Xn+1 = Xo .

This algorithm will detect anomalies with a false alarm level of approximately 1/(n+1). Thus larger
sizes n of the training sample will correspond to more stringent false alarm constraints. Furthermore,
the p-value of each test point Xi is easily computed by recursing over the size n of the training
∗
(cid:3)
(cid:3)
sample. In particular, let n
vary from k to n and deﬁne n
as the minimum value of n
for which
∗ + 1).
Xi is declared an anomaly. Then the p-value of Xi is approximately 1/(n
η can be deﬁned for each point Xi in the combined sample
A useful relative inﬂuence coefﬁcient
Xn+1

η(Xi ) =

∆iLkN N
maxi ∆iLkN N
The coefﬁcient η(Xn+1 ) = 1 when the test point Xn+1 is declared an anomaly.
Using matlab’s matrix sort algorithm step 1 of this algorithm can be computed an order of magnitude
faster than the K-point MST (N 2 logN vs N 3 logN ). For example, the experiments below have
shown that the above algorithm can ﬁnd and determine the p-value of 10 outliers among 1000 test
samples in a few seconds on a Dell 2GHz processor running Matlab 7.1.

.

(3)

4

Illustrative examples

Here we focus on the L1O kNNG algorithm due to its computational speed. We show a few repre-
sentative experiments for simple Gaussian and Gaussian mixture nominal densities f0 .

L1O kNN scores. rho=0.998, Mmin=500 , detection rate=0.009

)
i
∆
(
i
x
a
m
/
n
∆
 
=
 
e
r
o
c
s

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

100

200

300

400
600
500
sample number

700

800

900

1000

iteration 20, pvalue 0.001

iteration 203, pvalue 0.001

iteration 246, pvalue 0.001

−2

−4

−6

−2

−4

−6

4

2

0

−4

4

2

0

−2

4

2

0

−2

−4

−6

−5

−2

0

2

4

6

iteration 294, pvalue 0.001443

0

2

4

6

iteration 574, pvalue 0.001

0

5

−2

−4

−6

−2

−4

−6

4

2

0

−4

4

2

0

−4

4

2

0

−2

−4

−2

−2

0

2

4

6

iteration 307, pvalue 0.001

−2

0

2

4

6

iteration 712, pvalue 0.0011614

0

2

4

6

−2

−4

−6

−2

−4

−6

4

2

0

−4

4

2

0

−2

4

2

0

−2

−4

−6

−2

−2

0

2

4

6

iteration 334, pvalue 0.001

0

2

4

6

iteration 791, pvalue 0.0011682

0

2

4

6

Figure 3: Left: The plot of the anomaly curve for the L1O kNNG anomaly detector for detecting
deviations from a nominal 2D Gaussian density with mean (0,0) and correlation coefﬁcient -0.5.
The boxes on peaks of curve correspond to positions of detected anomalies and the height of the
boxes are equal to one minus the computed p-value. Anomalies were generated (on the average)
every 100 samples and drawn from a 2D Gaussian with correlation coefﬁcient 0.8. The parameter
ρ is equal to 1 − α, where α is the user deﬁned false alarm rate. Right: the resampled nominal
distribution (” •”) and anomalous points detected (”* ”) at the iterations indicated at left.

First we illustrate the L1O kNNG algorithm for detection of non-uniformly distributed anomalies
from training samples following a bivariate Gaussian nominal density. Speciﬁcally, a 2D Gaussian
density with mean (0,0) and correlation coefﬁcient -0.5 was generated to train of the L1O kNNG
detector. The test sample consisted of a mixture of this nominal and a zero mean 2D Gaussian with
 = 0.01. In Fig. 3 the results of simulation with
correlation coefﬁcient 0.8 with mixture coefﬁcient
a training sample of 2000 samples and 1000 tests samples are shown. Fig. 3 is a plot of the relative

inﬂuence curve (3) over the test samples as compared to the most outlying point in the (resampled)
training sample. When the relative inﬂuence curve is equal to 1 the corresponding test sample is the
most outlying point and is declared an anomaly. The 9 detected anomalies in Fig. 3 have p-values
less than 0.001 and therefore one would expect an average of only one false alarm at this level of
signiﬁcance. In the right panel of Fig. 3 the detected anomalies (asterisks) are shown along with the
training sample (dots) used to grow the L1O kNNG for that particular iteration - note that to protect
against bias the training sample is resampled at each iteration.

Next we compare the performance of the L1O kNNG detector to that of the UMP test for the
hypotheses (2). We again trained on a bivariate Gaussian f0 with mean zero, but this time with
identical component variances of σ = 0.1. This distribution has essential support on the unit
√
square. For this simple case the minimum volume set of level α is a disk centered at the ori-
2σ2 ln 1/α and the power of the of the UMP can be computed in closed form:
gin with radius
β = (1 − )α + (1 − 2πσ2 ln 1/α). We implemented the GEM anomaly detector with the incre-
mental leave-one-out kNNG using k = 5. The training set consisted of 1000 samples from f0 and
the test set consisted of 1000 samples from the mixture of a uniform density and f0 with parameter
 ranging from 0 to 0.2. Figure 4 shows the empirical ROC curves obtained using the GEM test vs
the theoretical curves (labeled ”clairvoyant ”) for several different values of the mixing parameter.
Note the good agreement between theoretical prediction and the GEM implementation of the UMP
using the kNNG.

0.5

0.45

0.4

0.35

0.3

β

0.25

0.2

0.15

0.1

0.05

0

 
0

ROC curves for Gaussian+uniform mixture. k=5, N=1000, Nrep=10

 

L1O −kNN
Clairvoyant
ε=0.5

ε=0.3

ε=0.1

ε=0

0.01

0.02

0.03

0.04

0.05
α

0.06

0.07

0.08

0.09

0.1

Figure 4: ROC curves for the leave-one-out kNNG anomaly detector described in Sec. 3.3. The
labeled ”clairvoyant” curve is the ROC of the UMP anomaly detector. The training sample is a zero
mean 2D spherical Gaussian distribution with standard deviation 0.1 and the test sample is a this
2D Gaussian and a 2D uniform-[0, 1]2 mixture density. The plot is for various values of the mixture
parameter .

5 Conclusions

A new and versatile anomaly detection method has been introduced that uses geometric entropy
minimization (GEM) to extract minimal set coverings that can be used to detect anomalies from a
set of training samples. This method can be implemented through the K-point minimal spanning tree
(MST) or the K-point nearest neighbor graph (kNNG). The L1O kNNG is signiﬁcantly less com-
putationally demanding than the K-point MST. We illustrated the L1O kNNG method on simulated
data containing anomalies and showed that it comes close to achieving the optimal performance of
the UMP detector for testing the nominal against a uniform mixture with unknown mixing para-
meter. As the L1O kNNG computes p-values on detected anomalies it can be easily extended to
account for false discovery rate constraints. By using a sliding window, the methodology derived in
this paper is easily extendible to on-line applications and has been applied to non-parametric intruder
detection using our Crossbow sensor network testbed (reported elsewhere).

Acknowledgments

This work was partially supported by NSF under Collaborative ITR grant CCR-0325571.

References

[1] A. Hero, B. Ma, O. Michel,
“Applications of entropic span-
and J. Gorman,
ning graphs,”
IEEE Signal Processing Magazine, vol. 19, pp. 85–95, Sept. 2002.
www.eecs.umich.edu/˜hero/imag_proc.html.
[2] A. Hero and O. Michel, “Asymptotic theory of greedy approximations to minimal k-point
random graphs,”
IEEE Trans. on Inform. Theory, vol. IT-45, no. 6, pp. 1921–1939, Sept. 1999.
[3] T. S. Ferguson, Mathematical Statistics - A Decision Theoretic Approach. Academic Press,
Orlando FL, 1967.
[4] I. V. Nikiforov and M. Basseville, Detection of abrupt changes: theory and applications.
Prentice-Hall, Englewood-Cliffs, NJ, 1993.
[5] B. Scholkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt, “Support vector method
for novelty detection,” in Advances in Neural Information Processing Systems (NIPS), vol. 13,
2000.
[6] G. R. G. Lanckriet, L. El Ghaoui, and M. I. Jordan, “Robust novelty detection with single-class
mpm,” in Advances in Neural Information Processing Systems (NIPS), vol. 15, 2002.
[7] C. Scott and R. Nowak, “Learning minimum volume sets,”
Journal of Machine Learning Re-
search, vol. 7, pp. 665–704, April 2006.
[8] A. Lazarevic, A. Ozgur, L. Ertoz, J. Srivastava, and V. Kumar, “A comparative study of anom-
aly detection schemes in network intrusion detection,” in SIAM Conference on data mining,
2003.
[9] S. Ramaswamy, R. Rastogi, and K. Shim, “Efﬁcient algorithms for mining outliers from large
data sets,” in Proceedings of the ACM SIGMOD Conference, 2000.
[10] R. Ravi, M. Marathe, D. Rosenkrantz, and S. Ravi, “Spanning trees short or small,” in Proc. 5th
Annual ACM-SIAM Symposium on Discrete Algorithms, (Arlington, VA), pp. 546–555, 1994.
[11] J. E. Yukich, Probability theory of classical Euclidean optimization, vol. 1675 of Lecture Notes
in Mathematics. Springer-Verlag, Berlin, 1998.


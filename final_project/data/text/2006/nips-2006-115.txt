learning
Attribute-efﬁcient
of decision lists and linear threshold functions
under unconcentrated distributions

Philip M. Long
Google
Mountain View, CA
plong@google.com

Rocco A. Servedio
Department of Computer Science
Columbia University
New York, NY
rocco@cs.columbia.edu

Abstract

We consider the well-studied problem of learning decision lists using few exam-
ples when many irrelevant features are present. We show that smooth boosting al-
gorithms such as MadaBoost can ef(cid:2)ciently learn decision lists of length k over n
boolean variables using poly(k ; log n) many examples provided that the marginal
distribution over the relevant variables is (cid:147)not too concentrated(cid:148) in an L2 -norm
sense. Using a recent result of H (cid:9)astad, we extend the analysis to obtain a similar
(though quantitatively weaker) result for learning arbitrary linear threshold func-
tions with k nonzero coef(cid:2)cients. Experimental results indicate that the use of
a smooth boosting algorithm, which plays a crucial role in our analysis, has an
impact on the actual performance of the algorithm.

1 Introduction

A decision list is a Boolean function de(cid:2)ned over n Boolean inputs of the following form:
if ‘1 then b1 else if ‘2 then b2 ::: else if ‘k then bk else bk+1 .
Here ‘1 ; :::; ‘k are literals de(cid:2)ned over the n Boolean variables and b1 ; : : : ; bk+1 are Boolean values.
Since the work of Rivest [24] decision lists have been widely studied in learning theory and machine
learning.
A question that has received much attention is whether it is possible to attribute-efﬁciently learn
decision lists, i.e. to learn decision lists of length k over n variables using only poly(k ; log n) many
examples. This question was (cid:2)rst asked by Blum in 1990 [3] and has since been re-posed numerous
times [4, 5, 6, 29]; as we now brie(cid:3)y describe, a range of partial results have been obtained along
different lines.
Several authors [4, 29] have noted that Littlestone’s Winnow algorithm [17] can learn decision lists
of length k using 2O(k) log n examples in time 2O(k)n log n. Valiant [29] and Nevo and El-Yaniv
[21] sharpened the analysis of Winnow in the special case where the decision list has only a bounded
number of alternations in the sequence of output bits b1 ; : : : ; bk+1 : It is well known that the (cid:147)halving
algorithm(cid:148) (see [1, 2, 19]) can learn length-k decision lists using only O(k log n) examples, but the
running time of the algorithm is nk : Klivans and Servedio [16] used polynomial threshold func-
tions together with Winnow to obtain a tradeoff between running time and the number of examples
required, by giving an algorithm that runs in time n ~O(k1=3 ) and uses 2 ~O(k1=3 ) log n examples.
In this work we take a different approach by relaxing the requirement that the algorithm work under
any distribution on examples or in the mistake-bound model. This relaxation in fact allows us to han-
dle not just decision lists, but arbitrary linear threshold functions with k nonzero coef(cid:2)cients. (Recall

that a linear threshold function f : f(cid:0)1; 1gn ! f(cid:0)1; 1gn is a function f (x) = sgn(Pn
i=1 wi xi (cid:0) (cid:18))
where wi ; (cid:18) are real numbers and the sgn function outputs the (cid:6)1 numerical sign of its argument.)
The approach and results. We will analyze a smooth boosting algorithm (see Section 2) together
with a weak learner that exhaustively considers all 2n possible literals x i ; :xi as weak hypotheses.
The algorithm, which we call Algorithm A, is described in more detail in Section 6.
The algorithm’s performance can be bounded in terms of the L2 -norm of the distribution over exam-
ples. Recall that the L2 -norm of a distribution D over a (cid:2)nite set X is kDk2 := (Px2X D(x)2 )1=2 :
The L2 norm can be used to evaluate the (cid:147)spread(cid:148) of a probability distribution: if the probability is
concentrated on a constant number of elements of the domain then the L2 norm is constant, whereas
if the probability mass is spread uniformly over a domain of size N then the L2 norm is 1=pN .
Our main results are as follows. Let D be a distribution over f(cid:0)1; 1gn: Suppose the target function
f has k relevant variables. Let D rel denote the marginal distribution over f(cid:0)1; 1gk induced by the
relevant variables to f (i.e. if the relevant variables are xi1 ; : : : ; xik , then the value that Drel puts
on an input (z1 ; : : : ; zk ) is Prx2D [xi1 : : : xik = z1 : : : zk ]: Let Uk be the uniform distribution over
f(cid:0)1; 1gk and suppose that jjDrel jj2=jjUk jj2 = (cid:28) . (Note that for any D we have (cid:28) (cid:21) 1, since Uk has
minimal L2 -norm among all distributions over f(cid:0)1; 1gk :) Then we have:
Theorem 1 Suppose the target function is an arbitrary decision list in the setting described above.
(cid:15) ; log 1
(cid:14) ) examples, Algorithm A runs in poly(n; (cid:28) ; 1
(cid:15) ; (cid:28) ; log 1
Then given poly(log n; 1
(cid:14) ) time and with
probability 1 (cid:0) (cid:14) constructs a hypothesis h that is (cid:15)-accurate with respect to D.
Theorem 2 Suppose the target function is an arbitrary linear threshold function in the setting de-
scribed above. Then given poly(k ; log n; 2 ~O(((cid:28) =(cid:15))2 ) ; log 1
(cid:14) ) examples, Algorithm A runs in poly(n;
2 ~O(((cid:28) =(cid:15))2 ) ; log 1
(cid:14) ) time and with probability 1 (cid:0) (cid:14) constructs a hypothesis h that is (cid:15)-accurate with
respect to D.
Relation to Previous Work. Jackson and Craven [14] considered a similar approach of using
Boolean literals as weak hypotheses for a boosting algorithm (in their case, AdaBoost).
Jack-
son and Craven proved that for any distribution over examples, the resulting algorithm requires
poly(K; log n) examples to learn any weight-K linear threshold function, i.e. any function of
the form sgn(Pn
i=1 wi xi (cid:0) (cid:18)) over Boolean variables where all weights wi are integers and
i=1 jwi j (cid:20) K (this clearly implies that there are at most K relevant variables). It is well known
Pn
[12, 18] that general decision lists of length k can only be expressed by linear threshold functions
of weight 2(cid:10)(k) , and thus the result of [14] does not give an attribute ef(cid:2)cient learning algorithm for
decision lists.
More recently Servedio [27] considered essentially the same algorithm we analyze in this work
by speci(cid:2)cally studying smooth boosting algorithms with the (cid:147)best-single-variable(cid:148) weak learner.
He considered a general linear threshold learning problem (with no assumption that there are few
relevant variables) and showed that if the distribution satis(cid:2)es a margin condition then the algorithm
has some level of resilience to malicious noise. The analysis of this paper is different from that of
[27]; to the best of our knowledge ours is the (cid:2)rst analysis in which the smoothness property of
boosting is exploited for attribute ef(cid:2)cient learning.

2 Boosting and Smooth Boosting
Fix a target function f : f(cid:0)1; 1gn ! f(cid:0)1; 1g and a distribution D over f(cid:0)1; 1gn. A hypothesis
function h : f(cid:0)1; 1gn ! f(cid:0)1; 1g is a (cid:13) -weak hypothesis for f with respect to D if ED [f h] (cid:21) (cid:13) :
We sometimes refer to ED [f h] as the advantage of h with respect to f :
We remind the reader that a boosting algorithm is an algorithm which operates in a sequence of
stages and at each stage t maintains a distribution Dt over f(cid:0)1; 1gn: At stage t the boosting algo-
rithm is given a weak hypothesis ht for f with respect to D; the boosting algorithm then uses this
to construct the next distribution Dt+1 over f(cid:0)1; 1gn. After T such stages the boosting algorithm
constructs a (cid:2)nal hypothesis h based on the weak hypotheses h1 ; : : : ; hT that is guaranteed to have
high accuracy with respect to the initial distribution D. See [25] for more details.

Let D1 ; D2 be two distributions. For (cid:20) (cid:21) 1 we say that D1 is (cid:20)-smooth with respect to D2 if
for all x 2 f(cid:0)1; 1gn; D1 (x)=D2 (x) (cid:20) (cid:20):
Following [15], we say that a boosting algorithm B is (cid:20)((cid:15); (cid:13) )-smooth if for any initial distribution D
and any distribution Dt that is generated starting from D when B is used to boost to (cid:15)-accuracy with
(cid:13) -weak hypotheses at each stage, Dt is (cid:20)((cid:15); (cid:13) )-smooth w.r.t. D: It is known that there are algorithms
(cid:15) ) with no dependence on (cid:13) , see e.g. [8]. For the rest of the paper B
that are (cid:20)-smooth for (cid:20) = (cid:2)( 1
will denote such a smooth boosting algorithm.
It is easy to see that every distribution D which is 1
(cid:15) -smooth w.r.t. the uniform distribution U satis(cid:2)es
kDk2=kU k2 (cid:20) p1=(cid:15): On the other hand, there are distributions D that are highly non-smooth
relative to U but which still have kDk2=kU k2 small. For instance, the distribution D over f(cid:0)1; 1gk
which puts weight
2k=2 on a single point and distributes the remaining weight uniformly on the other
1
2k (cid:0) 1 points is only 2k=2 -smooth (i.e. very non-smooth) but satis(cid:2)es kDk2=kUk k2 = (cid:2)(1): Thus
the L2 -norm condition we consider in this paper is a weaker condition than smoothness with respect
to the uniform distribution.

3 Total variation distance and L2 -norm of distributions
The total variation distance between two probability distributions D1 ; D2 over a (cid:2)nite set X is
2 Px2X jD1 (x) (cid:0) D2 (x)j : It is easy to see that the total
dT V := maxS(cid:18)X D1 (S ) (cid:0) D2 (S ) = 1
variation distance between any two distributions is at most 1, and equals 1 if and only if the supports
of the distributions are disjoint. The following is immediate:
Lemma 1 For any two distributions D1 and D2 over a ﬁnite domain X , we have dT V (D1 ; D2 ) =
1 (cid:0) Px2X minfD1 (x); D2 (x)g:
We can bound the total variation distance between a distribution D and the uniform distribution in
terms of the ratio kDk2=kU k2 of the L2 -norms as follows:
Lemma 2 For any distribution D over a ﬁnite domain X , if U is the uniform distribution over X ,
we have dT V (D; U ) (cid:20) 1 (cid:0) jjU jj2
:
2
4jjD jj2
2
Proof: Let M = jjD jj2
2 = Ex(cid:24)D [D(x)], we have Ex(cid:24)D [D(x)] = M 2 jjU jj2
. Since jjDjj2
2 = M 2
jX j :
jjU jj2
By Markov’s inequality,
2M 2
jX j

Pr
x(cid:24)D
By Lemma 1, we have
Xx:D(x)(cid:20)2M 2U (x)
1 (cid:0) dT V (D; U ) = Xx
1
D(x)
Xx:D(x)(cid:20)2M 2U (x)
(cid:21)
4M 2 ;
2M 2 (cid:21)
where the second inequality uses the fact that M (cid:21) 1 (so D(x)=2M 2 < D(x)) and the third
inequality uses (1). Using the de(cid:2)nition of M and solving for dT V (D; U ) completes the proof.

minfD(x); U (x)g (cid:21)

minfD(x); U (x)g

[D(x) (cid:21) 2M 2U (x)] = Pr
x(cid:24)D

[D(x) (cid:21)

] (cid:20) 1=2:

(1)

4 Weak hypotheses for decision lists

Let f be any decision list that depends on k variables:
if ‘1 then output b1 else (cid:1) (cid:1) (cid:1) else if ‘k then output bk else output bk+1
where each ‘i is either (cid:147)(xi = 1)(cid:148) or (cid:147)(xi = (cid:0)1).(cid:148)
The following folklore lemma can be proved by an easy induction (see e.g. [12, 26] for proofs of
essentially equivalent claims):

(2)

Lemma 3 The decision list f can be represented by a linear threshold function of the form f (x) =
sgn(c1 x1 + (cid:1) (cid:1) (cid:1) + ck xk (cid:0) (cid:18)) where each ci = (cid:6)2k(cid:0)i and (cid:18) is an even integer in the range [(cid:0)2k ; 2k ]:
It is easy to see that for any (cid:2)xed c1 ; : : : ; ck as in the lemma, as x = (x1 ; : : : ; xk ) varies over
f(cid:0)1; 1gk the linear form c1x1 + (cid:1) (cid:1) (cid:1)+ ck xk will assume each odd integer value in the range [(cid:0)2k ; 2k ]
exactly once. Now we can prove:

Lemma 4 Let f be any decision list of length k over the n Boolean variables x1 ; : : : ; xn : Let D be
any distribution over f(cid:0)1; 1gn; and let Drel denote the marginal distribution over f(cid:0)1; 1gk induced
by the k relevant variables of f : Suppose that dT V (Drel ; Uk ) (cid:20) 1 (cid:0) (cid:17) : Then there is some weak
Drel [f h] (cid:21) (cid:17)2
hypothesis h 2 fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; 1; (cid:0)1g which satis ﬁes E
16 :
Proof: We (cid:2)rst observe that by Lemma 3 and the well-known (cid:147)discriminator lemma(cid:148) of [23, 11],
under any distribution D some weak hypothesis h from fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; 1; (cid:0)1g must have
2k . This immediately establishes the lemma for all (cid:17) (cid:20) 4
2k=2 , and thus we may suppose
ED [f h] (cid:21) 1
2k=2 .
w.l.o.g. that (cid:17) > 4
We may assume w.l.o.g. that f is the decision list (2), that is, that the (cid:2)rst literal concerns x 1 , the
second concerns x2 , and so on. Let L(x) denote the linear form c1x1 + (cid:1) (cid:1) (cid:1)+ ck xk (cid:0) (cid:18) from Lemma 3,
so f (x) = sgn(L(x)): If x is drawn uniformly from f(cid:0)1; 1gk , then L(x) is distributed uniformly
over the 2k odd integers in the interval [(cid:0)2k (cid:0) (cid:18); 2k (cid:0) (cid:18)], as c1x1 is uniform over (cid:6)2k , c2x2 over
(cid:6)2k(cid:0)1 , and so on.
Let S denote the set of those x 2 f(cid:0)1; 1gk that satisfy jL(x)j (cid:20) (cid:17)
4 2k : Note that there are at most
4 2k + 1 elements in S , corresponding to L(x) = (cid:6)1; (cid:6)3; : : : ; (cid:6)(2j (cid:0) 1), where j is the greatest
(cid:17)
integer such that 2j (cid:0) 1 (cid:20) (cid:17)
2k=2 , certainly jS j (cid:20) 1 + (cid:17)
4 2k . Since (cid:17) > 4
2 2k . We thus have
4 2k (cid:20) (cid:17)
4 2k ] (cid:21) 1 (cid:0) (cid:17)=2. It follows that PrDrel [jL(x)j > (cid:17)
2 (for otherwise we would
PrUk [jL(x)j > (cid:17)
4 2k ] (cid:21) (cid:17)
have dT V (Drel ; Uk ) > 1 (cid:0) (cid:17) ), and consequently we have E
Drel [jL(x)j] (cid:21) (cid:17)2
8 2k :
Now we follow the simple argument used to prove the (cid:147)discriminator lemma(cid:148) [23, 11]. We have
(cid:17)2
2k : (3)
Drel [f (x)L(x)] = c1E[f (x)x1 ] + (cid:1) (cid:1) (cid:1) + ckE[f (x)xk ] (cid:0) (cid:18)E[f (x)] (cid:21)
Drel [jL(x)j] = E
8
Recalling that each jci j = 2k(cid:0)i , it follows that some h 2 fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; 1; (cid:0)1g must
satisfy E
8 2k )=(2k(cid:0)1 + (cid:1) (cid:1) (cid:1) + 20 + j(cid:18)j): Since j(cid:18)j (cid:20) 2k this is at least (cid:17)2
16 , and the proof
Drel [f h] (cid:21) ( (cid:17)2
is complete.

E

5 Weak hypotheses for linear threshold functions

Now we consider the more general setting of arbitrary linear threshold functions. Though there are
additional technical complications the basic idea is as in the previous section.
We will use the following fact due to H (cid:9)astad:

Fact 3 (H ˚astad) (see [28], Theorem 9) Let f : f(cid:0)1; 1gk ! f(cid:0)1; 1g be any linear threshold func-
tion that depends on all k variables x1 ; : : : ; xk : There is a representation sgn(Pk
i=1 wi xi (cid:0) (cid:18))
for f which is such that (assuming the weights w1 ; : : : ; wk are ordered by decreasing magnitude
1 = jw1 j (cid:21) jw2 j (cid:21) (cid:1) (cid:1) (cid:1) (cid:21) jwk j > 0) we have jwi j (cid:21) 1
i!(k+1) for all i = 2; : : : ; k :
The main result of this section is the following lemma. The proof uses ideas from the proof of
Theorem 2 in [28].

Lemma 5 Let f : f(cid:0)1; 1gn ! f(cid:0)1; 1g be any linear threshold function that depends on k vari-
ables. Let D be any distribution over f(cid:0)1; 1gn; and let Drel denote the marginal distribution over
f(cid:0)1; 1gk induced by the k relevant variables of f : Suppose that dT V (Drel ; Uk ) (cid:20) 1 (cid:0) (cid:17) : Then
there is some weak hypothesis h 2 fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; 1; (cid:0)1g which satis ﬁes E
Drel [f h] (cid:21)
1=(k22 ~O(1=(cid:17)2 ) ):

E

Proof sketch: We may assume that f (x) = sgn(L(x)) where L(x) = w1x1 + (cid:1) (cid:1) (cid:1) + wk xk (cid:0) (cid:18) with
w1 ; : : : ; wk as described in Fact 3.
Let ‘ := ~O(1=(cid:17) 2 ) = O((1=(cid:17) 2 )poly(log(1=(cid:17)))). (We will specify ‘ in more detail later.)
Suppose (cid:2)rst that ‘ (cid:21) k : By a well-known result of Muroga et al. [20], every linear threshold
function f that depends on k variables can be represented using integer weights each of mag-
nitude 2O(k log k) . Now the discriminator lemma [11] implies that for any distribution P , for
some h 2 fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; 1; (cid:0)1g we have EP [f h] (cid:21) 1=2O(k log k) : If ‘ (cid:21) k and ‘ =
O((1=(cid:17) 2 )poly(log(1=(cid:17)))), we have k log k = ~O(1=(cid:17) 2). Thus, in this case, EP [f h] (cid:21) 1=2 ~O(1=(cid:17)2 ) ,
so the lemma holds if ‘ (cid:21) k :
Thus we henceforth assume that ‘ < k : It remains only to show that
~O(1=(cid:17)2 ) );
Drel [jL(x)j] (cid:21) 1=(k2
once we have this, following (3) we get
~O(1=(cid:17)2 ) );
Drel [f L] = w1E[f (x)x1 ] + (cid:1) (cid:1) (cid:1) + wk E[f (x)xk ] (cid:0) (cid:18)E[f (x)] (cid:21) 1=(k2
Drel [jL(x)j] = E
E
and now since each jwi j (cid:20) 1 (and w.l.o.g. j(cid:18)j (cid:20) k ) this implies that some h satis(cid:2)es E
Drel [f h] (cid:21)
1=(k22 ~O(1=(cid:17)2 ) ) as desired.
Similar to [28] we consider two cases (which are slightly different from the cases in [28]).
j ) > (cid:17)2 =576.
Case I: For all 1 (cid:20) i (cid:20) ‘ we have w2
i =(Pk
j=i w2
Let (cid:11) := r2 (cid:16)Pk
j (cid:17) ln(8=(cid:17)): Recall the following version of Hoeffding’s bound: for any
j=‘+1 w2
0 6= w 2 Rk and any (cid:13) > 0, we have Prx2f(cid:0)1;1gk [jw (cid:1) xj (cid:21) (cid:13) kwk] (cid:20) 2e(cid:0)(cid:13) 2=2 (where we write
kwk to denote qPk
i ). This bound directly gives us that
i=1 w2
(cid:17)
[jw‘+1x‘+1 + (cid:1) (cid:1) (cid:1) + wk xk j (cid:21) (cid:11)] (cid:20) 2e(cid:0)2 ln(8=(cid:17))=2 =
Pr
4
x2Uk
Moreover, the argument in [28] that establishes equation (4) of [28] also yields
(cid:17)
[jw1x1 + (cid:1) (cid:1) (cid:1) + w‘x‘ (cid:0) (cid:18)j (cid:20) 2(cid:11)] (cid:20)
Pr
4
x2Uk
in our current setting. (The only change that needs to be made to the argument of [28] is adjusting
various constant factors in the de(cid:2)nition of ‘). Equations (5) and (6) together yield Prx2Uk [jw1x1 +
2 : Now as before, taken together with the dT V bound this yields
(cid:1) (cid:1) (cid:1) + wk xk (cid:0) (cid:18)j (cid:21) (cid:11)] (cid:21) 1 (cid:0) (cid:17)
2 and hence we have E
Drel [jL(x)j] (cid:21) (cid:17)(cid:11)=2: Since (cid:11) > w‘+1 and w‘+1 (cid:21)
PrDrel [jL(x)j (cid:21) (cid:11)] (cid:21) (cid:17)
1=((k + 1)(‘ + 1)!) by Fact 3, we have established (4) in Case I.
i ) (cid:20) (cid:17)2 =576. Let us (cid:2)x any setting z 2
Case II: For some value J (cid:20) ‘ we have w 2
J =(Pk
i=J w2
f(cid:0)1; 1gJ(cid:0)1 of the variables x1 ; : : : ; xJ(cid:0)1 : By an inequality due to Petrov [22] (see [28], Theorem
4) we have
6wJ
(cid:17)
6(cid:17)
qPk
24
4
i=J w2
i
Thus for each z 2 f(cid:0)1; 1gJ(cid:0)1 we have Prx2Uk [jL(x)j (cid:20) wJ j x1 : : : xJ(cid:0)1 = z1 : : : zJ(cid:0)1 ] (cid:20) (cid:17)
4 :
4 , which in turn gives Prx2Drel [jL(x)j >
This immediately yields Prx2Uk [jL(x)j > wJ ] (cid:21) 1 (cid:0) (cid:17)
4 and hence E
by our usual arguments. Now (4) follows using Fact 3
wJ ] (cid:21) 3(cid:17)
Drel [jL(x)j] (cid:21) 3(cid:17)wJ
4
and J (cid:20) ‘:

[jw1 z1+(cid:1) (cid:1) (cid:1)+wJ(cid:0)1 zJ(cid:0)1+wJ xJ +(cid:1) (cid:1) (cid:1)+wk xk(cid:0)(cid:18)j (cid:20) wJ ] (cid:20)

Pr
xJ ;:::;xk2Uk(cid:0)J+1

(cid:20)

:

(4)

(5)

(6)

=

:

6 Putting it all together

(cid:15) )-smooth boosting-by-(cid:2)ltering algorithm; for concreteness
Algorithm A works by running a (cid:2)( 1
we use the MadaBoost algorithm of Domingo and Watanabe [8]. At the t-th stage of boosting,

when MadaBoost simulates the distribution Dt , the weak learning algorithm works as follows:
) many examples are drawn from the simulated distribution Dt , and these examples
O( log n+log(1=(cid:14) 0 )
(cid:13) 2
are used to obtain an empirical estimate of EDt [f h] for each h 2 fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; (cid:0)1; 1g.
(Here (cid:13) is an upper bound on the advantage EDt [f h] of the weak hypotheses used at each stage;
we discuss this more below.) The weak hypothesis used at this stage is the one with the highest
observed empirical estimate. The algorithm is run for T = O( 1
(cid:15)(cid:13) 2 ) stages of boosting.
Consider any (cid:2)xed stage t of the algorithm’s execution. As shown in [8], at most O( 1
(cid:15) ) draws from
the original distribution D are required for MadaBoost to simulate a draw from the distribution D t .
(cid:15) )-smooth; the distribution Dt is
(This is a direct consequence of the fact that MadaBoost is O( 1
simulated using rejection sampling from D:) Standard tail bounds show that if the best hypothesis
h has E[f h] (cid:21) (cid:13) then with probability 1 (cid:0) (cid:14) 0 the hypothesis selected will have E[f h] (cid:21) (cid:13) =2. In
[8] it is shown that if MadaBoost always has an (cid:10)((cid:13) )-accurate weak hypothesis at each stage, then
(cid:15)(cid:13) 2 ) stages the algorithm will construct a hypothesis which has error at most
after at most T = O( 1
(cid:15). Thus it suf(cid:2)ces to take (cid:14) 0 = O((cid:14)(cid:15)2 (cid:13) ). The overall number of examples used by Algorithm A is
).
O( log n+log(1=(cid:14) 0 )
(cid:15)2 (cid:13) 4
Thus to establish Theorems 1 and 2, it remains only to show that for any initial distribution D with
kDrelk2=kUk k2 = (cid:28) , the distributions Dt that arise in the course of boosting are always such that
the best weak hypothesis h 2 fx1 ; (cid:0)x1 ; : : : ; xn ; (cid:0)xn ; (cid:0)1; 1g has suf(cid:2)ciently large advantage.
Suppose f is a target function that depends on some set of k (out of n) variables. Consider
(cid:15) -smooth boosting algorithm, where the initial distribution D satis(cid:2)es
what happens if we run a 1
kDrelk=kUk k = (cid:28) : At each stage we will have D rel
(cid:15) (cid:1) Drel (x) for all x 2 f(cid:0)1; 1gk , and
(x) (cid:20) 1
t
consequently we will have

jjDrel
t

(cid:28) 2
1
2 = Xx2f(cid:0)1;1gk Drel
(cid:15)2 Xx2f(cid:0)1;1gk Drel (x)2 (cid:20)
(cid:15)2 Xx2f(cid:0)1;1gk Uk (x)2 :
jj2
t
Thus, by Lemma 2 each distribution Dt will satisfy dT V (Drel
; Uk ) (cid:20) 1(cid:0) (cid:15)2=(4(cid:28) 2 ): Now Lemmas 4
t
and 5 imply that in both cases (decision lists and LTFs) the best weak hypothesis h does indeed have
the required advantage.

(x)2 (cid:20)

7 Experiments

The smoothness property enabled the analysis of this paper. Is smoothness really helpful for learning
decision lists with respect to diffuse distributions? Is it critical?
This section is aimed at addressing these questions experimentally. We compared the accuracy of
the classi(cid:2)ers output by a number of smooth boosters from the literature with AdaBoost (which is
known to not be a smooth booster in general, see e.g. Section 4.2 of [7]) on synthetic data in which
the examples were distributed uniformly, and the class designations were determined by applying a
randomly generated decision list. The number of relevant variables was (cid:2)xed at 10. The decision
list was determined by picking ‘1 ; :::; ‘10 and b1 ; :::; b11 from (2) independently uniformly at random
from among the possibilities.
We evaluated the following algorithms: (a) AdaBoost [9], (b) MadaBoost [8], (c) SmoothBoost [27],
and (d) a smooth booster proposed by Gavinsky [10]. Due to space constraints, we cannot describe
each of these in detail.1
Each booster was used to reweight the training data, and in each round, the literal which minimized
the weighted training error was chosen. Some of the algorithms choose the number of rounds of
1Very roughly speaking, AdaBoost reweights the data to assign more weight to examples that previously
chosen base classi(cid:2)ers have often classi(cid:2)ed incorrectly; it then outputs a weighted vote over the outputs of the
base classi(cid:2)ers, where each voting weight is determined as a function of how well its base classi(cid:2)er performed.
MadaBoost modi(cid:2)es AdaBoost to place a cap on the weight, prior to normalization. SmoothBoost [27] caps
the weight more aggressively as learning progresses, but also reweights the data and weighs the base classi(cid:2)ers
in a manner that does not depend on how well they performed. The form of the manner in which Gavinsky’s
booster updates weights is signi(cid:2)cantly different from AdaBoost, and reminiscent of [13, 15].

m
100
200
500
1000
100
200
500
1000

n
100
100
100
100
1000
1000
1000
1000

Ada Mada Gavinsky
0.088
0.077
0.086
0.050
0.045
0.052
0.024
0.018
0.022
0.024
0.014
0.016
0.123
0.119
0.116
0.083
0.072
0.079
0.045
0.039
0.045
0.033
0.026
0.035

SB(0.05)
0.071
0.067
0.056
0.063
0.093
0.071
0.050
0.048

SB(0.1)
0.067
0.047
0.031
0.036
0.101
0.064
0.040
0.038

SB(0.2)
0.077
0.047
0.025
0.028
0.117
0.072
0.040
0.032

SB(0.4)
0.089
0.051
0.031
0.033
0.128
0.081
0.044
0.036

Table 1: Average test set error rate

m
100
200
500
1000
100
200
500
1000

n
100
100
100
100
1000
1000
1000
1000

Ada Mada Gavinsky
11.7
8.8
13.6
19.8
13.1
12.5
15.2
20.7
32.2
15.3
19.2
37.2
26.8
7.7
13.3
19.8
11.5
19.4
16.2
16.7
28.1
36.7
20.1
14.7

SB(0.05)
3.9
4.1
5.0
7.1
3.7
4.4
4.9
7.2

SB(0.1)
6.0
6.9
9.1
10.7
5.3
7.4
8.6
11.0

SB(0.2)
7.5
9.4
11.5
12.1
6.1
9.5
10.9
12.1

SB(0.4)
9.1
9.9
12.2
13.0
7.4
11.7
11.5
13.3

Table 2: Average smoothness

boosting as a function of the desired accuracy; instead, we ran all algorithms for 100 rounds. All
boosters reweighted the data by normalizing some function that assigns weight to examples based
on how well previously chosen based classi(cid:2)ers are doing at classifying them correctly. The booster
proposed by Gavinsky might set all of these weights to zero: in such cases, it was terminated.
For each choice of the number of examples m and the number of features n, we repeated the fol-
lowing steps: (a) generate a random target, (b) generate m random examples, (c) split them into a
training set with 2=3 of the examples and a test set with the remaining 1=3, (d) apply all the algo-
rithms on the training set, and (e) apply all the resulting classi(cid:2)ers on the test set. We repeated the
steps enough times so that the total size of the test sets was at least 10000; that is, we repeated them
d30000=me times. The average test-set error is reported.
SmoothBoost [27] has two parameters, (cid:13) and (cid:18). In his analysis, (cid:18) = (cid:13) =(2 + (cid:13) ), so we used the same
setting. We tried his algorithm with (cid:13) set to each of 0:05, 0:1, 0:2 and 0:4.
The test set error rates are tabulated in Table 1. MadaBoost always improved on the accuracy of Ad-
aBoost. The results are consistent with the possibility that AdaBoost learns decision lists attribute-
ef(cid:2)ciently with respect to the uniform distribution; this motivates theoretical study of whether this
is true. One possible route is to prove that, for sources like this, AdaBoost is, with high probability,
a smooth boosting algorithm. The average smoothnesses are given in Table 2.
SmoothBoost [27] was seen to be fairly robust to the choice of (cid:13) ; with a good choice it sometimes
performed the best. This motivates research into adaptive boosters along the lines of SmoothBoost.

References
[1] D. Angluin. Queries and concept learning. Machine Learning, 2:319(cid:150)342, 1988.
[2] J. Barzdin and R. Freivald. On the prediction of general recursive functions. Soviet Mathematics Doklady,
13:1224(cid:150)1228, 1972.
[3] A. Blum. Learning Boolean functions in an in(cid:2)nite attribute space. In Proceedings of the Twenty-Second
Annual Symposium on Theory of Computing, pages 64(cid:150)72, 1990.
learning.
[4] A.
Blum.
On-line
algorithms
in
machine
http://www.cs.cmu.edu/ ˜avrim/Papers/pub s.html, 1996.
[5] A. Blum, L. Hellerstein, and N. Littlestone. Learning in the presence of (cid:2)nitely or in(cid:2)nitely many irrele-
vant attributes. Journal of Computer and System Sciences, 50:32(cid:150)40, 1995.

available

at

[6] A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artiﬁcial
Intelligence, 97(1-2):245(cid:150)271, 1997.
[7] N. Bshouty and D. Gavinsky. On boosting with optimal poly-bounded distributions. Journal of Machine
Learning Research, 3:483(cid:150)506, 2002.
[8] C. Domingo and O. Watanabe. Madaboost: a modi(cid:2)ed version of adaboost.
In Proceedings of the
Thirteenth Annual Conference on Computational Learning Theory, pages 180(cid:150)189, 2000.
[9] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to
boosting. Journal of Computer and System Sciences, 55(1):119(cid:150)139, 1997.
[10] Dmitry Gavinsky. Optimally-smooth adaptive boosting and application to agnostic learning. Journal of
Machine Learning Research, 4:101(cid:150)117, 2003.
[11] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits of bounded depth. Journal
of Computer and System Sciences, 46:129(cid:150)154, 1993.
[12] S. Hampson and D. Volper. Linear function neurons: structure and training. Biological Cybernetics,
53:203(cid:150)217, 1986.
[13] R. Impagliazzo. Hard-core distributions for somewhat hard problems. In Proceedings of the Thirty-Sixth
Annual Symposium on Foundations of Computer Science, pages 538(cid:150)545, 1995.
[14] J. Jackson and M. Craven. Learning sparse perceptrons. In NIPS 8, pages 654(cid:150)660, 1996.
[15] A. Klivans and R. Servedio. Boosting and hard-core sets. Machine Learning, 53(3):217(cid:150)238, 2003.
Preliminary version in Proc. FOCS’99.
[16] A. Klivans and R. Servedio. Toward attribute ef(cid:2)cient learning of decision lists and parities. In Proceed-
ings of the 17th Annual Conference on Learning Theory,, pages 224(cid:150)238, 2004.
[17] N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm.
Machine Learning, 2:285(cid:150)318, 1988.
[18] M. Minsky and S. Papert. Perceptrons: an introduction to computational geometry. MIT Press, Cam-
bridge, MA, 1968.
[19] T. Mitchell. Generalization as search. Artiﬁcial Intelligence, 18:203(cid:150)226, 1982.
[20] S. Muroga, I. Toda, and S. Takasu. Theory of majority switching elements. J. Franklin Institute, 271:376(cid:150)
418, 1961.
[21] Z. Nevo and R. El-Yaniv. On online learning of decision lists. Journal of Machine Learning Research,
3:271(cid:150)301, 2002.
[22] V. V. Petrov. Limit theorems of probability theory. Oxford Science Publications, Oxford, England, 1995.
[23] G. Pisier. Remarques sur un resultat non publi’e de B. Maurey. Sem. d’Analyse Fonctionelle, 1(12):1980(cid:150)
81, 1981.
[24] R. Rivest. Learning decision lists. Machine Learning, 2(3):229(cid:150)246, 1987.
[25] R. Schapire. Theoretical views of boosting. In Proc. 10th ALT, pages 12(cid:150)24, 1999.
[26] R. Servedio. On PAC learning using Winnow, Perceptron, and a Perceptron-like algorithm. In Proceedings
of the Twelfth Annual Conference on Computational Learning Theory, pages 296(cid:150)307, 1999.
[27] R. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning Research,
4:633(cid:150)648, 2003. Preliminary version in Proc. COLT’01.
[28] R. Servedio. Every linear threshold function has a low-weight approximator. In Proceedings of the 21st
Conference on Computational Complexity (CCC), pages 18(cid:150)30, 2006.
[29] L. Valiant. Projection learning. Machine Learning, 37(2):115(cid:150)130, 1999.


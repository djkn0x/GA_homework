Yusuf Ozuysal
Andrew Wong

Super-resolution

1

Introduction

Super-resolution deals with the construction of high-
resolution images using a set of low-resolution images
obtained from a scene with subpixel shifts. These low-
resolution images are typically obtained from a jittery
camera source, such as a camera mounted on a vibrat-
ing aircraft, or a slowly-moving subject, such as a few
frames of a standing person in front of a surveillance
camera. The low-resolution images have small trans-
lations and rotations from the high-resolution reference
image. The problem consists of constructing a model of
linear transformations for each image, and then piecing
together the images to form the high-resolution image.
Super-resolution image construction has applications in
remote sensing and medical imaging, and in scenarios
where directly capturing high-resolution images is not
feasible.

In this project, we attempt to solve the super-resolution
problem with a MAP approach, following Hardie, et. al
[1]. We construct a model of the low-resolution images
and attempt to learn the high-resolution images. Effects
of model parameters are discussed.

2 Model

The initial part of the problem consists of construct-
ing a model relating the high-resolution image to the
set of low-resolution images. From a training set of
p low-resolution images sized M1 × M2 , we represent
the k th low-resolution image as a vector yk with length
M = M1 × M2 and the high-resolution image N1 × N2
image as a vector z with length N = N1 × N2 . Low-
resolution images are modeled as a linear transforma-
tion of the high-resolution image followed by a down-
sampling step with factor L = M1/N1 = M2/N2 . An
additive Gaussian noise random variable η ∼ N (cid:0)0, σ2
η (cid:1)
accounts for camera noise effects [1] The linear trans-
formation is:

Here Wk is an M × N matrix containing the weighted
contribution of each pixel in z to the pixels of yk . Wk
is determined by the relative movement of the low-
resolution image to the reference image, speci ﬁed by a
translation parameter vector sk and rotation matrix Rk
θ ,
and a point spread function that models the diffusion of
light to pixels. To determine Wk , we calculate each
weight W k
ij as the value of a Gaussian point spread func-
tion between the reference image and the shifted low-
resolution image [2]:

W k
ij = exp(−

||vi − uj ||2
γ 2

)

Here the width γ of the point spread function determines
how much ’leakage’ there is from high-resolution pixels
to low-resolution pixels. γ is a constant that character-
izes the camera system and should be chosen to match
the type of sensor being used. vi is the pixel location
in the high resolution image and uj is the center of the
point spread function obtained by:

uj = Rθ (vj − v) + v + sk

Here v is the center of the image. Note that if there
are no rotations involved in the construction of low-
resolution images, this relationship becomes

uj = vj + sk

Given a set of yk we wish to obtain the corresponding
motion parameters s and the high resolution image z. To
simplify notation, we will construct y, a column vector
containing all k low-resolution vectors. Similarly, we
denote W as all of the Wk stacked on top of each other,
allowing us to write our model as y = Wz + η with η
now a pM -dimensional vector. We formulate the MAP
estimate of s and z.

ˆz, ˆs = arg max
z,s

Pr(y|z, s) Pr(z, s)

Assuming z and s are independent, and minimizing the
negative log-likelihood function, we obtain:

ˆz, ˆs = arg min
z,s

L(z, s)

yk = Wkz + η

= arg min
z,s

{− log(Pr(y|z, s)) − log(Pr(z)) − log(Pr(s))}

CS229 Final Project

2

Figure 1. High resolution image 256 × 256 and test low-resolution image 64 × 64 with added noise

The prior on z was chosen to be a Gaussian to model the
statistics of photons hitting a light detector [1].

Pr(z) =

1
N
2 )|Cz |1/2

(2π

exp (cid:26)−

1
2

z z(cid:27)
zTC −1

We can think of the covariance matrix Cz as describing
the similarity between pixels in the target image. We can
i=1 (cid:16)PN
j=1 di,j zj (cid:17),
λ PN
rewrite zTCz z as a product 1
where di , j controls the shape of our similarity between
pixels, and λ can control the weighting.

The prior of s is dependent on the motion characteristics
of the camera system and can be tailored to speci ﬁc ap-
plications. In the general case, we don’t know how the
system is moving, thus we will assume the prior to be a
uniform distribution.

We therefore write our log-likelihood function as:

Since we are minimizing the likelihood function with
respect to two sets of parameters, z and s, one method
would be to devise a coordinate descent-like algorithm,
optimizing cyclically between z and s[1]. We can use a
gradient descent for minimizing the gradient of the log-
likelihood with respect to z. However because the de-
pendence on s is implicit in our model, we don’t have an
expression for the derivative of the log-likelihood with
respect to the shifts. So estimating the shifts are done by
taking a block from the upsampled low resolution image
and matching this block to the high resolution image by
maximizing the 2-D correlation between the two blocks.

The algorithm begins by initializing z to Gaussian noise
with mean zero and standard deviation σ , and initial
shifts s to 0. We then calculate the W given these
shifts. Given W and the initial value of z gradient de-
scent is run to minimize the log-likelihood with respect
z. To minimize the log-likelihood we take the gradient
of L(z,s) with respect to z:

L(z, s) =

(y − Wz)T (y − Wz)

1
2σ2
η



N
Xj=1

2

di,j zj 


+

1
2λ

N
Xi=1

3

Implementation

1
σ2

∂L(z , s)
∂ zk
pM
Xm=1
N
Xi=1

1
2λ

=

N
Xr=1
wm,k (s)(

N
Xj=1
di,k (

di,j zj )

wm,r (s)zr − ym ) +

We created test low-resolution images yk from a high-
resolution image for training by Gaussian blurring a
268 × 268 high-resolution image, creating a shift (only
translations) by choosing a 256 × 256 window, and then
subsampling it by L = 4. Fig. 1 shows an example test
image.

The ﬁrst term in the gradient expression is the sum of
differences between the predicted and the actual low-
resolution image vectors. Each term in the sum is
weighted by the contribution of zk to that low-resolution
pixel,wm,k (s) . The second term is the prior gradient
which is in fact simply a linear combination of the pixels

CS229 Final Project

3

in the high resolution image and can also be computed
via a convolution operation. Using the gradient expres-
sion given above, the update rule that this routine uses
becomes;

ˆzn+1
k = ˆzn
k − α∇z L(z, s)|z=ˆzn
k ,s=ˆsn
k

where ˆzn
k and ˆsn
k are z and s estimates at the nth step
and α is the step size determined by annealing, starting
from relatively high step sizes and decreasing the step
size as the algorithm approaches convergence.

A single gradient descent run is assumed to converge
when the maximum among the absolute value of the en-
tries in the gradient is below a deﬁned tolerance value.
After the gradient descent to the current tolerance value,
the shifts are estimated by maximizing the 2-D cross-
correlation of two blocks extracted from the high reso-
lution estimate and upsampled low resolution input. The
shifts giving the maximum correlation values are stores
k .
as ˆsn
Since the accuracy of the shifts is relatively low at the
initial iterations of the main loop, this tolerance value is
decreased by half at every run of gradient descent start-
ing from a predeﬁned initial value. Thus as ˆz approaches
the original value and the accuracy of the shifts increases
the tolerance value is decreased, forcing the gradient de-
scent to achieve a more accurate z estimate.

3.1 Rotations

We tried to incorporate rotations into the same algorithm
above. Again, the gradient with respect to shift/rotation
could not be calculated directly so a search through mul-
tiple cross-correlations of rotated images was performed
to estimate s and Rθ . Also, updating z in the gradient
descent step was done by rotating taking the difference
between the low-res images and a rotated z and then ro-
tating this difference back before applying W. Unfor-
tunately, the number of cross-correlations and rotations
proved to be formidable in MATLAB and made the code
very slow.

4 Results

The algorithm was tested using synthesized low-
resolution images generated by ﬁrst convolving with a

Gaussian kernel and then downsampling with a prede-
ﬁned factor. The images generated were used as input
vectors to the algorithm and the performance of the al-
gorithm was tested for different parameter ranges. Dur-
ing these procedures convergence parameters of the al-
gorithm (initial α, the initial tolerance for gradient de-
scent convergence,)were kept constant between runs.

σ = 10 , λ = 150 , p = 32

σ = 20 , λ = 150 , p = 16

σ = 10 , λ = 250 , p = 16

Absolute Mean Error vs Number of Frames
7.5
30

7

6.5

6

5.5

0

25

20

15

10

5

0

20

40

60

Absolute Mean Error vs σ

Absolute Mean Error vs λ

9

8.5

8

7.5

7

6.5

6

5.5

0

20

40

60

500

1000

Figure 2. Example of change p, σ , and λ
and the mean pixel error between z and
original high-resolution image.

The parameters for which the performance of the algo-
rithm was tested were σ2 the variance of the additive
gaussian noise in the low-resolution images,λ the vari-
ance of the prior for z, p the number of low resolu-
tion images used. The performance criterion used for
comparison was the absolute mean error between the re-
sultant high-resolution estimate z and the original high-
resolution image. Results for selected parameter sets can
be seen in Figures 2 and 3.

According to the results seen in the ﬁgure the number
of-low resolution images used in general increases the
accuracy of the ﬁnal estimate ˆz. Moreover, note that the
changes in σ and λ effects the magnitude of the gradient
of z in the gradient descent update rule directly. Thus
the effect of changing one of these parameters in gen-
eral depends on the value of the other parameter. The
ratio between σ2 and λ signi ﬁes the importance of the
error values over the information coming from the prior
of z (which explains how values of nearby pixels be-
have with respect to each other). Thus making λ smaller
with respect to σ2 means relying mostly on the prior in-
formation and not depending on the error between the

CS229 Final Project

4

Figure 3. More estimated z’s varying p, σ , and λ.

Figure 4. Learning curves for different parameter values

Figure 5. Training on real video shot by handheld digital video camera.

estimated and original low-resolution images.

5 Conclusion

CS229 Final Project

5

We found that estimating z with a MAP framework
proved to be very successful when we generated test
images our selves. However, with real video images,
we could not successfully capture all motion parame-
ters into the model and thus found the z estimates to
be at best, smoothed interpolated versions of the low-
resolution images. If a more accurate, computationally
efﬁcient estimate of rotation could be found, we would
think the MAP estimator would be a good solution the
super-resolution problem.

6 References

[1]R.C. Hardie, K.J. Barnard, E.A. Armstrong.
Joint
MAP registration and high-resolution image estimation
using a sequence of undersampled images.
IEEE
Transactions on Image Processing, 6(12):1621-1633,
1997.

[2]M.Tipping,C.Bishop.
Advances
resolution.
tion
processing
systems
Press,Cambridge,MA,2003.

Bayesian Image Super-
informa-
in
neural
15:1279-1286.
MIT

The learning curves for some parameter combinations
can be seen in Figure 4. The spiking behaviour in the
curves is due to the tolerance criteria used. Since the
shifts are estimated again before each gradient descent
run, this changes the W matrix used during the gradi-
ent descent, making the initial delta much bigger than
the previously obtained lowest value. At each run of
gradient descent, the accuracy of the shifts increases.
When the shift values are close to convergence the
spikes disappear allowing the gradient descent to con-
tinue from the previously obtained smallest maximum
absolute value.

The algorithm was also tested on some videos by taking
64 × 64 blocks from each frame as the low resolution in-
put images. For these inputs because we dont have any
prior information on the point spread function used, ef-
fects of changing τ value (the width of the point spread
function) was observed. The results can be seen in Fig.
5. As can also be observed from the ﬁgure changing τ
didnt improve the result to a great extent although some
minor artifacts (vertical lines) seen for τ = 1 are can-
celed for τ = 2.

We speculate that the poor results from video footage
can be explained by our failure to capture rotations in
the low-resolution images. As mentioned in the previ-
ous section, attempting to incorporate rotations into our
algorithm led to inconclusive results because of the in-
creased run-time and inaccuries in estimating the rota-
tion between frames. In Fig. 6, we show the results of
attempting to estimate z from shifted and rotated test im-
ages. Inaccuracies in our rotation estimator made it very
difﬁcult to run the algorithm to convergence, causing the
resulting estimate to be much worse than our estimates
from a shifted-only training set.

Figure 6. Estimate z from rotated test im-
ages.


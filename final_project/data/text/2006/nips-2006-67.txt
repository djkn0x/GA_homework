Stratiﬁcation Learning: Detecting Mixed Density and
Dimensionality in High Dimensional Point Clouds

Gloria Haro, Gregory Randall, and Guillermo Sapiro
IMA and Electrical and Computer Engineering
University of Minnesota, Minneapolis, MN 55455
haro@ima.umn.edu,randall@fing.edu.uy,guille@umn.edu

Abstract

The study of point cloud data sampled from a stratiﬁcation, a collection of man-
ifolds with possible different dimensions, is pursued in this paper. We present a
technique for simultaneously soft clustering and estimating the mixed dimension-
ality and density of such structures. The framework is based on a maximum like-
lihood estimation of a Poisson mixture model. The presentation of the approach
is completed with artiﬁcial and real examples demonstrating the importance of
extending manifold learning to stratiﬁcation learning.

1

Introduction

Data in high dimensions is becoming ubiquitous, from image analysis and ﬁnances to computational
biology and neuroscience. This data is often given or represented as samples embedded in a high
dimensional Euclidean space, point cloud data, though it is assumed to belong to lower dimensional
manifolds. Thus, in recent years, there have been signiﬁcant efforts in the development of methods to
analyze these point clouds and their underlying manifolds. These include numerous techniques for
the estimation of the intrinsic dimension of the data and also its projection onto lower dimensional
representations. These disciplines are often called manifold learning and dimensionality reduction.
A few examples include [2, 3, 4, 9, 10, 11, 12, 16].
The vast majority of the manifold learning and dimensionality reduction techniques developed in
the literature assume, either explicitly or implicitly, that the given point cloud are samples of a
unique manifold. It is very easy to realize that a signiﬁcant part of the interesting data has mixed
dimensionality and complexity. The work here presented deals with this more general case, where
there are different dimensionalities/complexities present in the point cloud data. That is, we have
samples not of a manifold but of a stratiﬁcation. The main aim is to cluster the data according to the
complexity (dimensionality) of the underlying possible multiple manifolds. Such clustering can be
used both to better understand the varying dimensionality and complexity of the data, e.g., states in
neural recordings or different human activities for video analysis, or as a pre-processing step for the
above mentioned manifold learning and dimensionality reduction techniques.
This clustering-by-dimensionality task has been recently explored in a handful of works. Barbar ´a
and Chen, [1], proposed a hard clustering technique based on the fractal dimension (box-counting).
Starting from an initial clustering, they incrementally add points into the cluster for which the change
in the fractal dimension after adding the point is the lowest. They also ﬁnd the number of clusters
and the intrinsic dimension of the underlying manifolds. Gionis et al., [7], use local growth curves
to estimate the local correlation dimension and density for each point. The new two-dimensional
representation of the data is clustered using standard techniques. Souvenir and Pless, [14], use an
Expectation Maximization (EM) type of technique, combined with weighted geodesic multidimen-
sional scaling. The weights measure how well each point ﬁts the underlying manifold deﬁned by
the current set of points in the cluster. After clustering, each cluster dimensionality is estimated

following [10]. Huang et al., [8], cluster linear subspaces with an algebraic geometric method based
on polynomial differentiation and a Generalized PCA. They search for the best combination of lin-
ear subspaces that explains the data, and ﬁnd the number of linear subspaces and their intrinsic
dimension. The work of Mordohai and Medioni, [11], estimates the local dimension using tensor
voting.
These recent works have clearly shown the necessity to go beyond manifold learning, into “stratiﬁ-
cation learning.” In our work, we do not assume linear subspaces, and we simultaneously estimate
the soft clustering and the intrinsic dimension and density of the clusters. This collection of at-
tributes is not shared by any of the pioneering works just described. Our approach is an extension
of the Levina and Bickel’s local dimension estimator [10]. They proposed to compute the intrinsic
dimension at each point using a Maximum Likelihood (ML) estimator based on a Poisson distribu-
tion. The local estimators are then averaged, under the assumption of a single uniform manifold. We
propose to compute a ML on the whole point cloud data at the same time (and not one for each point
independently), and use a Poisson mixture model, which permits to have different classes, each one
with their own dimension and sampling density. This technique automatically gives a soft clustering
according to dimensionality and density, with an estimation of both quantities for each class. Our
approach assumes that the number of classes is given, but we are discovering the actual number
of underlying manifolds. If we search for a larger than needed number of classes, we obtain some
classes with the same dimensionality and density or some classes with very few representatives, as
shown in the examples later presented.
The remainder of this paper is organized as follows: In Section 2 we review the method proposed by
Levina and Bickel, [10], which gives a local estimation of the intrinsic dimension and has inspired
our work. In Section 3 we present our core contribution of simultaneous soft clustering and dimen-
sionality and density estimation. We present experiments with synthetic and real data in Section 4,
and ﬁnally, some conclusions are presented in Section 5.

2 Local intrinsic dimension estimation

Levina and Bickel (LB), [10], proposed a geometric and probabilistic method which estimates the
local dimension (and density) of a point cloud data1 . This is the approach we here extend, which
is based on the idea that if we sample an m-dimensional manifold with T points, the proportion
T ≈ f (xt )V (m)Rk (xt )m , where the given point
of points that fall into a ball around a point xt is k
cloud, embedded in high dimensions D , is X = {xt ∈ RD ; t = 1, . . . , T }, k is the number of points
inside the ball, f (xt ) is the local sampling density at point xt , V (m) is the volume of the unit sphere
in Rm , and Rk (xt ) is the Euclidean distance from xt to its k-th nearest neighbor (kNN). Then, they
consider the inhomogeneous process N (R, xt ), which counts the number of points falling into a
small D-dimensional sphere B (R, xt ) of radius R centered at xt . This is a binomial process, and
some assumptions need to be done to proceed. First, if T → ∞, k → ∞, and k/T → 0, then we
can approximate the binomial process by a Poisson process. Second, the density f (xt ) is constant
inside the sphere, a valid assumption for small R. With these assumptions, the rate λ of the counting
process N (R, xt ) can be written as λ(R, xt ) = f (xt )V (m)mRm−1 . The log-likelihood of the
Z R
Z R
process N (R, xt ) is then given by
0
0
where θ(xt ) := log f (xt ) is the density parameter and the ﬁrst integral is a Riemann-Stieltjes
integral [13]. The maximum likelihood estimators satisfy ∂L/∂ θ = 0 and ∂L/∂m = 0, leading to
h 1
i−1
Pk−1
a computation for the local dimension at point xt , m(xt ), depending on all the neighbors within a
distance R from xt [10]. In practice, it is more convenient to compute a ﬁxed amount k of nearest
j=1 log Rk (xt )
neighbors. Thus, the local dimension at point xt is m(xt ) =
. This
k−2
Rj (xt )
estimator is asymptotically unbiased (see [10] for more details). If the data points belong to the
same manifold, we can average over all m(xt ) in order to obtain a more robust estimator. However,
if there are two or more manifolds with different dimensions, the average does not make sense,
unless we ﬁrst cluster according to dimensionality and then we estimate the dimensionality for each

log λ(r, xt )dN (r, xt ) −

L(m(xt ), θ(xt )) =

λ(r, xt )dr,

1M. Hein pointed us out in NIPS that this dimension estimator is equivalent to the one proposed in [15].

cluster. We brieﬂy toy with this idea now, as a warm up to our simultaneous soft clustering and
estimation technique described in Section 3.

2.1 A two step clustering approach

As a ﬁrst simple approach to detect and cluster mixed dimensionality (and/or densities), we can
combine a local dimensionality estimator such as the one just described and a clustering technique.
For the second step we use the Information Bottleneck (IB) [17], which is an elegant framework to
eventually combine several local dimension estimators and other possible features such as density
[6]. The IB is a technique that allows to cluster (compress) a variable according to another related
variable. Let X be the set of variables to be clustered and S the relevance variable that gives some
information about X . An example is the information that different words provide about documents
of different topics. We call ˜X the clustered version of X . The optimal ˜X is the one that minimizes
the functional L(p( ˜xt |xt )) = I ( ˜X ; X ) − β I ( ˜X ; S ), where I (·; ·) denotes mutual information and
p(·) the probability density function. There is a trade-off, controlled by β , between compressing the
representation and preserving the meaningful information. In our context, we want to cluster the
data according to the intrinsic dimensionality (and/or density). Then, our relevant variable S will
be the set of (quantized) estimated local intrinsic dimensions. For the joint distribution p(xt , si ),
si ∈ S , we use the histogram of local dimensions inside a ball of radius R0 around xt ,2 computed
by the LB technique.
Examples of this technique will be presented in the experimental Section. Instead of a two-steps
algorithm, with local dimensionality and/or density estimation followed by clustering, we now pro-
pose a maximum likelihood technique that combines these steps.

3 Poisson mixture model

The core approach that we propose to study stratiﬁcations (mixed manifolds) is based on extending
the LB technique [10]. Instead of modelling each point and its local ball of radius R as a Poisson
process and computing the ML for each ball separately, we consider all the possible balls at the
same time in the same ML function. As the probability density function for all the point cloud
we consider a mixture of Poisson distributions with different parameters (dimension and density).
Thus, we allow the presence of different intrinsic dimensions and densities in the dataset. These are
automatically computed while being used for soft clustering.
Let us denote by J the number of different Poisson distributions considered in the mixture, each
one with a (possibly) different dimension m and density parameter θ . We consider the vector set of
parameters ψ = {ψ j = (π j , θj , mj ); j = 1, . . . , J }, where π j is the mixture coefﬁcient for class j
(the proportion of distribution j in the dataset), θj is its density parameter (f j = eθj ), and mj is its
dimension. We denote by p(·) the probability density function and by P (·) the probability.
As in the LB approach, the observable event will be yt = N (R, xt ), the number of points inside
the ball B (R, xt ) of radius R centered at point xt . The total number of observations is T 0 and
Y = {yt ; t = 1, . . . , T 0 } is the observation sequence. If we consider every possible ball in the
dataset then, T 0 coincides with the total number of points T in the point cloud. From now on, we
!
 
!
 Z R
will consider this case and T 0 ≡ T . The density function of the Poisson mixture model is given by
Z R
JX
JX
−
log λj (r) dN (r, xt )
0
0
j=1
j=1
V (mj )mj rmj −1 . Usually, problems involving a mixture of experts are solved by
where λj (r) = eθj
the Expectation Maximization (EM) algorithm [5]. In our context, there are two kinds of unknown
parameters: The membership function of an expert (class), π j , and the parameters of each expert,
mj and θj . The membership information is originally unknown, thereby making the parameter
estimation for each class difﬁcult. The EM algorithm computes its expected value (E-step) and then
this value is used for the parameter estimation procedure (M-step). These two steps are iterated.
2The value of R0 determines the amount of regularity in the classiﬁcation.

π j p(yt |θj , mj ) =

p(yt |ψ) =

π j exp

exp

λj (r)dr

,

TY
If Y contains T statistically independent variables, then the incomplete data log-likelihood is:
p(yt |ψ) = Q(ψ) + R(ψ),
L(Y |ψ) = log p(Y |ψ) = log
Q(ψ) := X
P (Z |Y , ψ) log p(Z, Y |ψ), R(ψ) := − X
t=1
P (Z |Y , ψ) log P (Z |Y , ψ),
Z
Z
where Z = {zt ∈ C ; t = 1, . . . , T } is the missing data (hidden-state information), and the set
of class labels is C = {C 1 , C 2 , . . . C J }. Here, zt = C j means that the j -th mixture generates
yt . We call Q the expectation of log p(Z, Y |ψ) with respect to Z . The EM algorithm is based on
as p(Z, Y |ψ) = QT
maximizing Q, since while improving (maximizing) the function Q at each iteration, the likelihood
function L is also improved. The probability density that appears in the function Q can be written
t=1 p(zt , yt |ψ), and the complete-data log-likelihood becomes
TX
JX
t log (cid:2)p(yt |zt = C j , ψ j )π j (cid:3) ,
log p(Z, Y |ψ) =
δ j
t=1
j=1
(cid:26)1
where a set of indicator variables δ j
t is used in order to indicate the status of the hidden variables:
if yt generated by mixture C j ,
t ≡ δ(zt , C j ) =
δ j
0
else.
Considering the expectation, with respect to Z , EZ (·) of (1) and setting ψ to a ﬁxed known value
ψn (the value at step n of the algorithm), everywhere except for the log function, we get a function
Q of ψ . We denote it by Q(ψ |ψn ), and it has the following form
t = 1, ψ j )π j i
h
TX
JX
p(yt |δ j
t=1
j=1
PJ
p(yt |δ j
n )π j
t = 1, ψ j
t |yt , ψn ] = P (δ j
t = 1|yt , ψn ) =
n (yt ) = EZ [δ j
n
hj
l=1 p(yt |δ l
n )π l
t = 1, ψ l
 Z R
 
!
!
Z R
n
is the probability that observation t belongs to mixture j . Finally, the probability density in (2) is
p(yt |δ j
−
t = 1, ψ j
n ) = exp
log λj
n (r)dN (r, xt )
0
0
n−1 . As mentioned above, the EM algorithm consists of two main
n rmj
n (r) = eθj
n )mj
n V (mj
where λj
steps. In the E-step, the function Q(ψ |ψn ) is computed, for that, we determine the best guess of
n (yt ). Once we know these probabilities, Q(ψ |ψn )
the membership function, i.e., the probabilities hj
can be considered as a function of the only unknown, ψ , and it is maximized in order to compute
the values of ψn+1 , i.e., the maximum likelihood parameters ψ at step n + 1; this is called the M-
step. The EM suffers from local maxima, hitting a local maximum can be prevented running the
algorithm several times with different initializations. Different random subset of points, from the
point cloud, may be used in each run. We have experimented with both approaches and the results
n+1 = arg maxψj Q(ψ |ψn ) + λ(PJ
are always similar if we initialize all the probabilities equally. The Algorithm PMM describes the
main components of this proposed approach. The estimators π j
n+1 , and θj
n+1 , mj
multiplier that allows to introduce the constraint PJ
n+1 are obtained by
l=1 π l − 1) in the M-step, where λ is the Lagrange
computing ψ j
2 ) = R ∞
l=1 π l = 1. This gives equations (4)-(5), where
n /2−1 e−tdt. In order to compute mj
2 )), and Γ( mj
nΓ( mj
tmj
n ) = (2πmj
V (mj
n /2 )/(mj
n
n
n+1
0
we have used the same approach as in [10], by means of a k nearest neighbor graph.

Q(ψ |ψn ) =

n (yt ) log
hj

n (r)dr
λj

,

where

(1)

(2)

(3)

,

exp

4 Experimental results

We now present a number of experimental results for the technique proposed in Section 3. We often
compare it with the two-steps algorithm described in Section 2, and denote this algorithm by LD+IB.

Algorithm PMM Poisson Mixture Model
0 } to any set of values which ensures that PJ
Require: The point cloud data, J (number of desired classes) and k (scale of observation).
Ensure: Soft clustering according to dimensionality and density.
1: Initialization of ψ0 = {π j
j=1 π j
0 , mj
0 , θj
0 = 1.
2: EM iterations on n,
For all j = 1, . . . J , compute:
• E-step: Compute hj
n (yt ) by (2).
−1
 PT
n (yt ) Pk−1
• M-step: Compute
TX
PT
j=1 log Rk (yt )
t=1 hj
Rj (yt )
 
!
n (yt ) ; mj
n+1 =
hj
n (yt )(k − 1)
TX
TX
t=1 hj
t=1
n (yt )(k − 1) − log
n (yt )Rk (yt )mj
θj
V (mj
n )
n+1 = log
hj
hj
n
t=1
t=1
Until convergence of ψn , that is, when ||ψn+1 − ψn ||2 < , for a certain small value .

π j
n+1 =

1
T

(4)

(5)

In all the experiments we use the initialization π j
0 = 0, and mj
0 = 1/J , θj
0 = j , for all j = 1, . . . , J .
The distances are normalized so that the maximum distance is 1. The embedding dimension in all
the experiments on synthetic data is 3, although the results were found to be consistent when we
increased the embedding dimension.
The ﬁrst experiment consists of a mixture of a Swiss roll manifold (700 points) and a line (700
points) embedded in a three dimensional space. The algorithm (with J = 2 and k = 10) is able
to separate both manifolds. The estimated parameters are collected in Table 1. For each table,
we display the estimated dimension m, density θ , and mixture coefﬁcient π for each one of the
classes. We also show the percentage of points of each manifold that are classiﬁed in each class
(after thresholding the soft assignment). Figure 1(a) displays both manifolds – each point is colored
according to the probability of belonging to each one of the two possible classes. Tables 1(a) and
1(c) contain the results for both PMM and LD+IB using J = 2. Table 1(b) shows the results for
the PMM algorithm with k = 10 and J = 3. Note how the parameters of the ﬁrst two classes are
quite similar to the ones obtained with J = 2, and the third class is marginal (very small π ). Figure
1(b) shows the PMM classiﬁcation when J = 3. Note that all the points of the line belong to the
class of dimension 1. The points of the Swiss roll are mainly concentrated in the other class with
dimension 2. A slight amount of Swiss roll points belong to a third class with roughly the same
dimension as the second class. Actually, these points are located in the point cloud boundaries,
where the underlying assumptions are not always valid.
If we estimate the dimension of the mixture using the LB technique with k = 10, we obtain 1.70 with
a standard deviation of 5.31. If we use the method proposed by Costa and Hero [4], the estimated
dimension is 2. In both cases, the estimated intrinsic dimension is the largest one present in the
mixture, ignoring that the data actually lives in two manifolds of different intrinsic dimension.
The same Table and Figure, second rows, show results for noisy data. We add to the point coordi-
nates Gaussian noise with σ = 0.6. The results obtained with k = 10 are displayed in Tables 1(d),
1(e) and 1(f), and in Figures 1(d), 1(e) and 1(f). Note how the classiﬁcation still separates the two
different manifolds, although the line is much more affected by the noise and it does not look like a
one dimensional manifold anymore. This is reﬂected also by the estimated dimension which now is
bigger. This phenomena is related to the scale of observation and to the level of noise. If the level
of noise is large – e.g., compared to the mean distance to the k nearest neighbors for a small k –
intuitively the estimated intrinsic dimension will be closer to the embedding dimension (this behav-
ior was experimentally veriﬁed). We can again compare the results with the ones obtained with the
LB estimator alone: Estimated dimension 2.71 and standard deviation 1.12. Using Costa and Hero
[4], the estimated dimension varies between 2 and 3 (depending on the number of bootstrap loops).
Both techniques do not consider the possibility of mixed dimensionality.
The experiment in Figure 2 illustrates how the soft clustering is done according to both dimension-
ality and density. The data consists of 2500 points on the Swiss roll, 100 on a line with high density

Estimated parameters
1.00
2.01
m
2.48
5.70
θ
0.5000
0.5000
π
% points in each class
100
Line
0
100
SR
0

Estimated parameters
2.16
1.00
2.01
1.52
2.55
5.70
0.5000
0.0208
0.4792
% points in each class
0
100
0
96.57

0
3.43

m
θ
π

Line
SR

Estimated dimension
2.00
1.67
m
% points in each class
0
100
Line
Swiss roll
3.45
96.55

(a) PMM (J = 2).

(b) PMM (J = 3).

(c) LD+IB (J = 2).

Estimated parameters
3.02
2.38
m
2.73
7.69
θ
0.4951
0.5049
π
% points in each class
1.86
98.14
Line
SR
0.86
99.14

Estimated parameters
2.26
3.01
2.40
1.72
2.88
7.70
0.4910
0.0325
0.4766
% points in each class
2.29
97.71
0.71
93.00

0
6.29

m
θ
π

Line
SR

Estimated dimension
2.30
3.09
m
% points in each class
20.29
79.71
Line
Swiss roll
24.71
75.29

(d) PMM (J = 2).

(e) PMM (J = 3).

(f) LD+IB (J = 2).

Table 1: Clustering results for the Swiss roll (SR) and a line (k = 10), without noise (ﬁrst row) and
with noise (second row).

(a) PMM (J = 2)

(b) PMM (J = 3)

(c) LD+IB (J = 2)

(d) PMM (J = 2)

(e) PMM (J = 3)

(f) LD+IB (J = 2)

Figure 1: Clustering of a line and a Swiss roll (k = 10). First row without noise, second row with
Gaussian noise (σ = 0.6). Points colored according to the probability of belonging to each class.
and 50 on another less dense line. We have set J = 4 and the algorithm gives an “empty class,”
thus discovering that three classes, with correct dimensionality and density, is enough for a good
representation. The only errors are in the borders, as expected.

m
θ
π

Line
Line (dense)
Swiss Roll

Estimated parameters
0.98
1.94
1.04
2.66
3.82
7.12
0.9330
0.0167
0.0498
% points in each class
84.31
15.69
0.0
0.0
99.00
1.00
0.0
1.08
98.92

1.93
2.57
0.0004

0.0
0.0
0.0

Figure 2: Clustering with mixed dimensions and density (k = 20, J = 4).
In order to test the algorithm with real data, we ﬁrst work with the MNIST database of handwritten
digits,3 which has a test set of 10.000 examples. Each digit is an image of 28 × 28 pixels and we
treat the data as 784-dimensional vectors.
3 http://yann.lecun.com/exdb/mnist/

We study the mixture of digits one and two and apply PMM and LD+IB with J = 2 and k = 10.
The results are shown in Figure 3. Note how the digits are well separated.4 The LB estimator alone
gives dimensions 9.13 for digits one, 13.02 for digits two, and 11.26 for the mixture of both digits.
The Costa and Hero’s method, [4], gives 8, 11 and 9 respectively. Both methods assume a single
intrinsic dimension and give an average of the dimensions of the underlying manifolds.

Estimated parameters
12.82
8.50
m
11.20
6.80
θ
0.5099
0.4901
π
% points in each class
6.52
93.48
Ones
Twos
0
100

Estimated dimension
9.17
13.74
m
% points in each class
5.29
94.71
Ones
Twos
9.08
90.02

(a) PMM

(b) LD+IB

(c) Some image examples.
Figure 3: Results for digits 1 and 2 (k = 10, J = 2).
Next, we experiment with 9-dimensional vectors formed of image patches of 3 × 3 pixels. If we
impose J = 3 and use PMM, we obtain the results in Figure 4. Notice how roughly one class
corresponds to patches in homogeneous zones (approximately constant gray value), a second class
corresponds to textured zones and a third class to patches containing edges. The estimated dimen-
sions in each region are in accordance to the estimated dimensions using Isomap or Costa and Hero’s
technique in each region after separation. This experiment is just a proof of concept, in the future
we will study how to adapt this clustering approach to image segmentation.

Figure 4: Clustering of image patches of 3 × 3 pixels with PMM, colors indicating the different
classes (complexity) (J = 3, k = 30). Left: original and segmented images of a house. Right:
original and segmented images of a portion of biological tissue. Adding spatial regularization is the
subject of current research.

Finally, as an additional proof of the validity of our approach and its potential applications, we
use the PMM framework to separate activities in video, Figure 5 (see also [14]). Each original
frame is 480 × 640, sub-sampled to 48 × 64 pixels, with 1673 frames. Four classes are present:
standing, walking, jumping, and arms waving. The whole run took 361 seconds in Matlab, while
the classiﬁcation time (PMM) can be neglected compared to the kNN component.

Samples in each cluster
C1
C2
C3
95
416
0
69
429
0
423
0
5
18
0
0

Standing
Walking
Waving
Jumping

C4
0
25
4
189

Figure 5: Classifying human activities in video (k = 10, J = 4). Four sample frames are shown
followed by the classiﬁcation results (confusion matrix). Visual analysis of the wrongly classiﬁed
frames show that these are indeed very similar to the misclassiﬁed class members. Adding features,
e.g., optical ﬂow, will improve the results.

4Since the clustering is done according to dimensionality and density, digits which share these characteris-
tics won’t be separated into different classes.

5 Conclusions
In this paper we discussed the concept of “stratiﬁcation learning,” where the point cloud data is not
assumed to belong to a single manifold, as commonly done in manifold learning and dimensionality
reduction. We extended the work in [10] in the sense that the maximum likelihood is computed
once for the whole dataset, and the probability density function is a mixture of Poisson laws, each
one modeling different intrinsic dimensions and densities. The soft clustering and the estimation
are simultaneously computed. This framework has been contrasted with a more standard two-steps
approach, a combination of the local estimator introduced in [10] with the Information Bottleneck
clustering technique [17]. In both methods we need to compute a kNN-graph which is precisely the
computationally more expensive part. The mixture of Poisson estimators is faster than the two-steps
approach one, it uses an EM algorithm, linear in the number of classes and observations, which
converges in a few iterations.
The mixture of Poisson model is not only clustering according to dimensionality, but to density as
well. The introduction of additional observations and estimates can also help to separate points that
although have the same dimensionality and density, belong to different manifolds. We would also
like to study the use of ellipsoids instead of balls in the counting process in order to better follow
the geometry of the intrinsic manifolds. Another aspect to study is the use of metrics more adapted
to the nature of the data instead of the Euclidean distance. At the theoretical level, the bias of the
PMM model needs to be studied. Results in these directions will be reported elsewhere.
Acknowledgments: This work has been supported by ONR, DARPA, NSF, NGA, and the McKnight Foun-
dation. We thank Prof. Persi Diaconis and Prof. Ren ´e Vidal for important feedback and comments. We also
thank Pablo Arias and J ´er ´emie Jakubowicz for their help. GR was on sabbatical from the Universidad de la
Republica, Uruguay, while performing this work.
References
[1] D. Barbara and P. Chen. Using the fractal dimension to cluster datasets. In Proceedings of the Sixth ACM
SIGKDD, pages 260–264, 2000.
[2] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In
Advances in NIPS 14, 2002.
[3] M. Brand. Charting a manifold. In Advances in NIPS 16, 2002.
[4] J. A. Costa and A. O. Hero. Geodesic entropic graphs for dimension and entropy estimation in manifold
learning. IEEE Trans. on Signal Processing, 52(8):2210–2221, 2004.
[5] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data. Journal of the Royal
Statistical Society Ser. B, 39:1–38, 1977.
[6] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. Multivariate information bottleneck. In Seven-
teenth Conference UAI, pages 152–161, 2001.
[7] A. Gionis, A. Hinneburg, S. Papadimitriu, and P. Tsparas. Dimension induced clustering. In Proceeding
of the Eleventh ACM SIGKDD, pages 51–60, 2005.
[8] K. Huang, Y. Ma, and R. Vidal. Minimum effective dimension for mixtures of subspaces: A robust GPCA
algorithm and its applications. In Proceedings of CVPR, pages 631–638, 2004.
[9] B. Kegl. Intrinsic dimension estimation using packing numbers. In Advances in NIPS 14, 2002.
[10] E. Levina and P.J. Bickel. Maximum likelihood estimation of intrinsic dimension. In Advances in NIPS
17, 2005.
[11] P. Mordohai and G. Medioni. Unsupervised dimensionality estimation and manifold learning in high-
dimensional spaces by tensor voting. In IJCAI, page 798, 2005.
[12] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.
[13] D. L. Snyder. Random Point Processes. Wiley, New York, 1975.
[14] R. Souvenir and R. Pless. Manifold clustering. In ICCV, pages 648–653, 2005.
[15] F. Takens. On the numerical determination of the dimension of an attractor. Lecture notes in mathematics.
Dynamical systems and bifurcations, 1125:99–106, 1985.
[16] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimen-
sionality reduction. Science, 290(5500):2319–2323, 2000.
[17] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the 37-th
Annual Allerton Conference on Communication, Control and Computing, pages 368–377, 1999.


Shifting, One-Inclusion Mistake Bounds and
Tight Multiclass Expected Risk Bounds

Benjamin I. P. Rubinstein
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720-1776, U.S.A.
benr@cs.berkeley.edu

Peter L. Bartlett
Computer Science Division and
Department of Statistics
University of California, Berkeley
bartlett@cs.berkeley.edu

J. Hyam Rubinstein
Department of Mathematics & Statistics
The University of Melbourne
Parkville, Victoria 3010, Australia
rubin@ms.unimelb.edu

Abstract

Under the prediction model of learning, a prediction strategy is presented with
an i.i.d. sample of n (cid:0) 1 points in X and corresponding labels from a concept
f 2 F , and aims to minimize the worst-case probability of erring on an n th point.
By exploiting the structure of F , Haussler et al. achieved a VC(F )=n bound
for the natural one-inclusion prediction strategy, improving on bounds implied by
PAC-type results by a O(log n) factor. The key data structure in their result is
the natural subgraph of the hypercube—the
one-inclusion graph; the key step is a
d = VC(F ) bound on one-inclusion graph density. The ﬁrst main result of this
(cid:20)d(cid:0)1 (cid:1) = ( n
paper is a density bound of n (cid:0) n(cid:0)1
(cid:20)d ) < d, which positively resolves a
conjecture of Kuzmin & Warmuth relating to their unlabeled Peeling compres-
sion scheme and also leads to an improved mistake bound for the randomized
(deterministic) one-inclusion strategy for all d (for d (cid:25) (cid:2)(n)). The proof uses
a new form of VC-invariant shifting and a group-theoretic symmetrization. Our
second main result is a k-class analogue of the d=n mistake bound, replacing the
VC-dimension by the Pollard pseudo-dimension and the one-inclusion strategy by
its natural hypergraph generalization. This bound on expected risk improves on
known PAC-based results by a factor of O(log n) and is shown to be optimal up to
a O(log k) factor. The combinatorial technique of shifting takes a central role in
understanding the one-inclusion (hyper)graph and is a running theme throughout.

1 Introduction

In [4, 3] Haussler, Littlestone and Warmuth proposed the one-inclusion prediction strategy as a nat-
ural approach to the prediction (or mistake-driven) model of learning, in which a prediction strategy
maps a training sample and test point to a test prediction with hopefully guaranteed low probabil-
ity of erring. The signiﬁcance of their contribution was two-fold. On the one hand the derived
VC(F )=n upper-bound on the worst-case expected risk of the one-inclusion strategy learning from
F (cid:18) f0; 1gX improved on the PAC-based previous-best by an order of log n. This was achieved by
taking the structure of the underlying F into account—which
had not been done in previous work —
in order to break ties between hypotheses consistent with the training set but offering contradictory
predictions on a given test point. At the same time Haussler [3] introduced the idea of shifting sub-

sets of the n-cube down around the origin —an
a
idea previously developed in Combinatorics—as
powerful tool for learning-theoretic results. In particular, shifting admitted deeply insightful proofs
of Sauer’s Lemma and a VC-dimension bound on the density of the one-inclusion graph—the
key
result needed for the one-inclusion strategy’s expected risk bound. Recently shifting has impacted
on work towards the sample compressibility conjecture of [7] e.g. in [5].

Here we continue to study the one-inclusion graph—the
natural graph structure induced by a subset
of the n-cube—and
its related prediction strategy under the lens of shifting. After the necessary
background, we develop the technique of shatter-invariant shifting in Section 3. While a subset’s
VC-dimension cannot be increased by shifting, shatter-invariant shifting guarantees a ﬁnite sequence
of shifts to a ﬁx ed-point under which the shattering of a chosen set remains invariant, thus preserv-
ing VC-dimension throughout. In Section 4 we apply a group-theoretic symmetrization to tighten
the deterministic (randomized) one-
the mistake bound—the worst-case expected risk bound —of
inclusion strategy from d=n to dDd
n < d for all n; d. The derived Dd
n e=n (Dd
n =n), where Dd
n
density bound positively resolves a conjecture of Kuzmin & Warmuth which was suggested as a
step towards a correctness proof of the Peeling compression scheme [5]. Finally we generalize the
prediction model, the one-inclusion strategy and its bounds from binary to k-class learning in Sec-
tion 5. Where (cid:9)G -dim (F ) and (cid:9)P -dim (F ) denote the Graph and Pollard dimensions of F , the
best bound on expected risk for k 2 N to-date is O((cid:11) log (cid:11)) for (cid:11) = (cid:9)G -dim (F ) =n, for consistent
learners [8, 1, 2, 4]. For large n this is O(log n(cid:9)G -dim (F ) =n); we derive an improved bound of
(cid:9)P -dim (F ) =n which we show is at most a O(log k) factor from optimal. Thus, as in the binary
case, exploiting class structure enables signi ﬁcantly better bounds on expected risk for multiclass
prediction.

As always some proofs have been omitted in the interest of ﬂo w or space. In such cases see [8].

2 Deﬁnitions & background

In this paper sets/random variables, scalars and vectors will be written in uppercase, lowercase and
(cid:20)r(cid:1) = Pr
bolded typeface as in C; x; v. We deﬁne (cid:0) n
i=0 (cid:0)n
i (cid:1), [n] = f1; : : : ; ng and Sn to be the
set of permutations on [n]. We write the density of graph G = (V ; E ) as dens (G) = jE j=jV j, the
indicator of A as 1 [A], and 9!x 2 X; P (x) to mean “there exists a unique x 2 X satisfying P .”

2.1 The prediction model of learning

We begin with the basic setup of [4]. Set X is the domain and F (cid:18) f0; 1gX is a concept class on
X . For notational convenience we write sam (x; f ) = ((x1 ; f (x1 )) ; : : : ; (xn ; f (xn ))) for x 2 X n ,
f 2 F . A prediction strategy is a mapping of the form Q : Sn>1 (X (cid:2) f0; 1g)n(cid:0)1 (cid:2) X ! f0; 1g.
De ﬁnition 2.1 The prediction model of learning concerns the following scenario. Given full knowl-
edge of strategy Q, an adversary picks a distribution P on X and concept f 2 F so as to maximize
i.i.d.
(cid:24) P . Thus the measure
the probability of fQ (sam (X1 ; : : : ; Xn(cid:0)1 ; f ) ; Xn ) 6= f (Xn )g where Xi
of performance is the worst-case expected risk
^MQ;F (n) = sup
sup
f 2F
P
A mistake bound for Q with respect to F is an upper-bound on ^MQ;F .

EX(cid:24)P n [1 [Q (sam ((X1 ; : : : ; Xn(cid:0)1 ); f ) ; Xn ) 6= f (Xn )]]

:

In contrast to Valiant’s PAC model, the prediction learning model is not interested in approximating
f given an f -labeled sample, but instead in predicting f (Xn ) with small worst-case probability of
erring. The following allows us to derive mistake-bounds by bounding a worst-case average.

Lemma 2.2 (Corollary 2.1 [4]) For any n > 1, concept class F and prediction strategy Q,
1
^MQ;F (n) (cid:20) sup
n! Xg2Sn
1 (cid:2)Q (cid:0)sam (cid:0)(cid:0)xg(1) ; : : : ; xg(n(cid:0)1) (cid:1) ; f (cid:1) ; xg(n) (cid:1) 6= f (cid:0)xg(n) (cid:1)(cid:3)
sup
x2X n
f 2F
^^MQ;F (n) :
=
A permutation mistake bound for Q with respect to F is an upper-bound on ^^MQ;F .

2.2 The capacity of function classes contained in f0; : : : ; kgX

We denote by (cid:5)x (F ) = f(f (x1 ); : : : ; f (xn )) j f 2 F g the projection of F (cid:18) Y X on x 2 X n .

De ﬁnition 2.3 The Vapnik-Chervonenkis dimension of concept class F is deﬁned as VC(F ) =
sup fn j 9x 2 X n ; (cid:5)x (F ) = f0; 1gn g. An x witnessing VC(F ) is said to be shattered by F .
n
Lemma 2.4 (Sauer’s Lemma [9]) For any n 2 N and V (cid:18) f0; 1gn , jV j (cid:20) (cid:0)
(cid:20)VC(V )(cid:1). A subset V
meeting this with equality is called maximum.
It is well-known that the VC-dimension is an inappropriate measure of capacity when jY j > 2. The
following unifying framework of class capacities for jY j < 1 is due to [1].

De ﬁnition 2.5 Let k 2 N, F (cid:18) f0; : : : ; kgX and (cid:9) be a family of mappings   : f0; : : : ; kg !
f0; 1; (cid:3)g called translations. For x 2 X n , v 2 (cid:5)x (F ) (cid:18) f0; : : : ; kgn and   2 (cid:9)n we write
 (v) = ( 1 (v1 ); : : : ;  n (vn )) and  ((cid:5)x (F )) = f (v) : v 2 (cid:5)x (F )g. x 2 X n is (cid:9)-shattered
by F if there exists a   2 (cid:9)n such that f0; 1gn (cid:18)  ((cid:5)x (F )). The (cid:9)-dimension of F is deﬁned by
(cid:9)-dim (F ) = supfn j 9x 2 X n ;   2 (cid:9)n s.t. f0; 1gn (cid:18)  ((cid:5)x (F ))g.

We next describe three important translation families used in this paper.

Example 2.6 The families (cid:9)P = f P;i
: i 2 f0; : : : ; kgg and
: i 2 [k ]g, (cid:9)G = f G;i
(cid:9)N = f N ;i;j : i; j 2 f0; : : : ; kg; i 6= j g, where  P;i (a) = 1 [a < i],  G;i (a) = 1 [a = i] and
 N ;i;j (a) equals 1; 0; (cid:3) if a = i; a = j; a =2 fi; j g respectively, deﬁne the Pollard pseudo-dimension
(cid:9)P -dim (V ), the Graph dimension (cid:9)G -dim (V ) and the Natarajan dimension (cid:9)N -dim (V ).

2.3 The one-inclusion prediction strategy

A subset of the n-cube —the
the one-inclusion graph, which under-
projection of some F —induces
lies a natural prediction strategy. The following deﬁnition generalizes this to a subset of f0; : : : ; kgn .

De ﬁnition 2.7 The one-inclusion hypergraph G (V ) = (V ; E ) of V (cid:18) f0; : : : ; kgn is the undirected
graph with vertex-set V and hyperedge-set E of maximal (with respect to inclusion) sets of pairwise
hamming-1 separated vertices.

Algorithm 1 The deterministic multiclass one-inclusion prediction strategy QG ;F
Given: F (cid:18) f0; : : : ; kgX , sam ((x1 ; : : : ; xn(cid:0)1 ); f ) 2 (X (cid:2) f0; 1g)n(cid:0)1 , xn 2 X
Returns: a prediction of f (xn )
V  (cid:0) (cid:5)x (F ) ;
G  (cid:0) G (V ) ;
(cid:0)!
G  (cid:0) orient G to minimize the maximum outdegree ;
Vspace  (cid:0) fv 2 V j v1 = f (x1 ); : : : ; vn(cid:0)1 = f (xn(cid:0)1 )g ;
if Vspace = fvg then return vn ;
else return the nth component of the head of hyperedge Vspace in

(cid:0)!
G ;

The one-inclusion graph’s prediction strategy QG ;F [4] immediately generalizes to the multiclass
prediction strategy described by Algorithm 1. For the remainder of this and Section 4 we will
restrict our discussion to the k = 1 case, on which the following main result of [4] focuses.

Theorem 2.8 (Theorem 2.3 [4]) ^MQG ;F ;F (n) (cid:20) VC(F )
n

for every concept class F and n > 1.

A lower bound in [6] showed that the one-inclusion strategy’s performance is optimal within a factor
of 1 + o(1). Replacing orientation with a distribution over each edge induces a randomized strategy
QG rand;F . The key to proving Theorem 2.8 is the following.

Lemma 2.9 (Lemma 2.4 [4]) For any n 2 N and V (cid:18) f0; 1gn , dens (G (V )) (cid:20) VC(V ).

An elegant proof of this deep result, due to Haussler [3], uses shifting. Consider any s 2 [n]; v 2 V
and let Ss (v; V ) be v shifted along s: if vs = 0, or if vs = 1 and there exists some u 2 V differing
to v only in the sth coordinate, then Ss (v; V ) = v; otherwise v shifts down —its
sth coordinate is
decreased from 1 to 0. The entire family V can be shifted to Ss (V ) = fSs (v; V ) j v 2 V g and this
shifted vertex-set induces Ss (E ) the edge-set of G (Ss (V )), where (V ; E ) = G (V ).

De ﬁnition 2.10 Let I (cid:18) [n]. We call a subset V (cid:18) f0; 1gn I -closed-below if Ss (V ) = V for all
s 2 I . If V is [n]-closed-below then we call it closed-below.

A number of properties of shifting follow relatively easily:
(1)
by the injectivity of Ss ( (cid:1) ; V )
jSs (V )j = jV j ;
(2)
as Ss (V ) shatters I (cid:18) [n] ) V shatters I
VC(Ss (V )) (cid:20) VC(V ) ;
jE j (cid:20) jV j (cid:1) VC(V ) ; as V closed-below ) maxv2V kvkl1 (cid:20) VC(V ) (3)
by cases
(4)
jSs (E )j (cid:21) jE j ;
9T 2 N; s 2 [n]T
(5)
s.t. SsT (: : : Ss1 (V )) is closed-below (a ﬁx ed-point) :
Properties (1–2) and the justiﬁcation of (3) together imply Sauer’s lemma; Properties (1–5)

lead to

jE j
jV j (cid:20) : : : (cid:20)

jSsT (:::Ss1 (E ))j
jSsT (:::Ss1 (V ))j

(cid:20) VC(SsT (: : : Ss1 (V ))) (cid:20) : : : (cid:20) VC(V )

:

3 Shatter-invariant shifting

While [3] shifts to bound density, the number of edges can increase and the VC-dimension can
decrease —both
contributing to the observed gap between graph density and capacity. The next
result demonstrates that shifting can in fact be controlled to preserve VC-dimension.

Lemma 3.1 Consider arbitrary n 2 N, I (cid:18) [n] and V (cid:18) f0; 1gn that shatters I . There exists a
ﬁnite sequence s1 ; : : : ; sT in [n] such that each Vt = Sst (: : : Ss1 (V )) shatters I and VT is closed-
below. In particular VC(VT ) = VC(VT (cid:0)1 ) = : : : = VC(V ).

Proof: (cid:5)I ((cid:1)) is invariant to shifting on I = [n]nI . So some ﬁnite number of shifts on I will produce
a I -closed-below family W that shatters I . Hence W must contain representatives for each element
of f0; 1gjI j (embedded at I ) with components equal to 0 outside I . Thus the shattering of I is
invariant to the shifting of W on I , so that a ﬁnite number of shifts on I produces an I -closed-below
W 0 that shatters I . Repeating the process a ﬁnite number of times until no non-trivial shifts are
made produces a closed-below family that shatters I . The second claim follows from (2).

4 Tightly bounding graph density by symmetrization

Kuzmin and Warmuth [5] introduced D d
n as a potential bound on the graph density of maximum
classes. We begin with properties of D d
n , a technical lemma and then proceed to the main result.

n(cid:16) n(cid:0)1
(cid:20)d(cid:0)1 (cid:17)
De ﬁnition 4.1 Deﬁne Dd
for all n 2 N and d 2 [n]. Denote by V d
n the VC-dimension
n =
( n
(cid:20)d )
d closed-below subset of f0; 1gn equal to the union of all (cid:0)n
d(cid:1) closed-below embedded d-cubes.
Lemma 4.2 Dd
n
(i) equals the graph density of V d
n for each n 2 N and d 2 [n];
(ii) is strictly upper-bounded by d, for all n;
(iii) equals d
2 for all n = d 2 N;
(iv) is strictly monotonic increasing in d (with n ﬁxed);
(v) is strictly monotonic increasing in n (with d ﬁxed); and
(vi) limits to d as n ! 1.

Proof: By counting, for each d (cid:20) n < 1, the density of G (cid:0)V d
n (cid:1) equals Dd
n :
n Pd(cid:0)1
n Pd(cid:0)1
n (cid:0) n
= Pd
i+1
i=0 (cid:0)n(cid:0)1
i=1 i(cid:0)n
i+1(cid:1)
n (cid:1)(cid:1)(cid:12)(cid:12)
(cid:12)(cid:12)E (cid:0)G (cid:0)V d
i (cid:1)
i (cid:1)
i=0
=
=
(cid:0) n
(cid:0) n
Pd
(cid:20)d(cid:1)
(cid:20)d(cid:1)
jV d
n j
i=0 (cid:0)n
i (cid:1)
B < A+C
B+D iff A
D , it is sufﬁcient
proving (i). Since for all A; B ; C; D > 0, A
B < C
n(n(cid:0)1
d(cid:0)1)
that Dd(cid:0)1
n (cid:20) d, and so
. By (i) and Lemma 2.9 Dd
n <
(n
d)

=

n(cid:0) n(cid:0)1
(cid:20)d(cid:0)1(cid:1)
(cid:0) n
(cid:20)d(cid:1)
for (iv) to prove

n (cid:1) (n (cid:0) 1)!
n!

Dd(cid:0)1
n (cid:20) d (cid:0) 1 < d =

(n(cid:0)1)!
n(cid:0)n(cid:0)1
d(cid:0)1(cid:1)
(n(cid:0)d)!(d(cid:0)1)!
(cid:0)n
n!
d(cid:1)
(n(cid:0)d)!d!
Monotonicity in d, (i) and Lemma 2.9 together prove (ii). Properties (iii,v–vi) are proven in [8].
jU j (cid:20) jV j and

Lemma 4.3 For arbitrary U; V (cid:18) f0; 1gn with dens (G (V )) (cid:21) (cid:26) > 0,
jE (G (U )) j (cid:21) jE (G (V )) j, if dens (G (U \ V )) < (cid:26) then dens (G (U [ V )) > (cid:26).

=

:

(n (cid:0) d)!
(n (cid:0) d)!

d!
(d (cid:0) 1)!

=

n

Proof: If G (U \ V ) has density less than (cid:26) then

jE (G (U [ V )) j
jU [ V j

(cid:21)

(cid:21)

>

jE (G (U )) j + jE (G (V )) j (cid:0) jE (G (U \ V )) j
jU j + jV j (cid:0) jU \ V j
2jE (G (V )) j (cid:0) jE (G (U \ V )) j
2jV j (cid:0) jU \ V j
2(cid:26)jV j (cid:0) (cid:26)jU \ V j
2jV j (cid:0) jU \ V j

= (cid:26)

y
t
i
s
n
e
d

0
1

8

6

4

2

0

d = 10

d
d
Dn

d = 2

d = 1

0

20

40

n

60

80

Figure 1: The improved graph density bound of Theorem 4.4. The density bounding D d
n
is plotted (dotted solid) alongside the previous best d (dashed), for each d 2 f1; 2; 10g.

Theorem 4.4 Every family V (cid:18) f0; 1gn with d = VC(V ) has (V ; E ) = G (V ) with graph density

jE j
jV j

(cid:20) Dd
n < d :

(6)

n is the unique closed-below VC-dimension d subset of f0; 1gn meeting (6)
For n 2 N and d 2 [n], V d
with equality. A VC-dimension d family V (cid:18) f0; 1gn meets (6) with equality only if V is maximum.

Proof: Allow a permutation g 2 Sn to act on vector v 2 f0; 1gn and family V (cid:18) f0; 1gn by
g(v) = (cid:0)vg(1) ; : : : ; vg(n) (cid:1) and g(V ) = fg(v) j v 2 V g; and deﬁne Sn (V ) = Sg2Sn
g(V ). Note

that a closed-below VC-dimension d family V (cid:18) f0; 1gn satisﬁes Sn (V ) = V iff V = V d
n , as
VC(V ) (cid:21) d implies V contains an embedded d-cube, invariance to Sn implies further that V
contains all (cid:0)n
n . Consider now any
d(cid:1) such cubes, and VC(V ) (cid:20) d implies that V (cid:18) V d
n;d 2 arg min (jU j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
dens (G (U )) ) :
V (cid:3)
U 2
arg max
fU (cid:18)f0;1gn jVC(U )(cid:20)d;U closed-belowg
For the purposes of contradiction assume that V (cid:3)
n;d ) for some g 2 Sn . Then if
n;d 6= g(V (cid:3)
dens (cid:16)G (cid:16)V (cid:3)
n;d )(cid:17)(cid:17) (cid:21) dens (cid:16)G (cid:16)V (cid:3)
n;d(cid:17)(cid:17) then V (cid:3)
n;d would not have been selected above
n;d \ g(V (cid:3)
n;d ) would have been chosen).
(i.e. a closed-below family at least as small and dense as V (cid:3)
n;d \ g(V (cid:3)
n;d(cid:17)(cid:17) by Lemma 4.3. But then again V (cid:3)
Thus dens (cid:16)G (cid:16)V (cid:3)
n;d )(cid:17)(cid:17) > dens (cid:16)G (cid:16)V (cid:3)
n;d would
n;d [ g(V (cid:3)
n;d ) would have been se-
not have been selected (i.e. a distinct family at least as dense as V (cid:3)
n;d [ g(V (cid:3)
lected instead, since every vector in this union contains no more than d 1’s). Hence V (cid:3)
n;d = Sn (V (cid:3)
n;d )
n;d(cid:17)(cid:17) = Dd0
n and by Lemma 4.2.(i) dens (cid:16)G (cid:16)V (cid:3)
n;d = V d0
n;d ) (cid:20) d. But by
n , for d0 = VC(V (cid:3)
and so V (cid:3)
n uniquely
Lemma 4.2.(iv) this implies that d = d0 and (6) is true for all closed-below families; V d
maximizes density amongst all closed-below VC-dimension d families in the n-cube.
For an arbitrary V (cid:18) f0; 1gn with d = VC(V ) consider any of its closed-below ﬁx ed-point (cf.
(5)), W (cid:18) f0; 1gn . Noting that VC(W ) (cid:20) d and dens (G (V )) (cid:20) dens (G (W )) by (2) and
(1) & (4) respectively, the bound (6) follows directly for V . Furthermore if we shift to preserve
VC-dimension then VC(W ) = d while still jV j = jW j. And since dens (G (W )) = D d
n only if
n , it follows that V maximizes density amongst all VC-dimension d families in the n-cube,
W = V d
n , only if it is maximum.
with dens (G (V )) = Dd
Theorem 4.4 improves on the VC-dimension density bound of Lemma 2.9 for low sample sizes (see
Figure 1). This new result immediately implies the following one-inclusion mistake bounds.

Theorem 4.5 Consider any n 2 N and F (cid:18) f0; 1gX with VC(F ) = d < 1. Then ^MQG ;F ;F (n) (cid:20)
n (cid:7) =n and ^MQGrand;F ;F (n) (cid:20) Dd
n =n.
(cid:6)Dd
For small d, n(cid:3) (d) = min (cid:8)n (cid:21) d j d = (cid:6)Dd
n (cid:7)(cid:9) —the
ﬁrst n for which the new and old deterministic
to remain very close to 2:96d. The randomized
one-inclusion mistake bounds coincide—appears
strategy’s mistake bound of Theorem 4.5 offers a strict improvement over that of [4].

5 Bounds for multiclass prediction

As in the k = 1 case, the key to developing the multiclass one-inclusion mistake bound is in bound-
ing hypergraph density. We proceed by shifting a graph induced by the one-inclusion hypergraph.

Theorem 5.1 For any k ; n 2 N and V (cid:18) f0; : : : ; kgn , the one-inclusion hypergraph (V ; E ) =
jE j
jV j (cid:20) (cid:9)P -dim (V ).
G (V ) satisﬁes

(cid:20)

(cid:20)

Proof: We begin by replacing the hyperedge structure E with a related edge structure E 0 . Two
vertices u; v 2 V are connected in the graph (V ; E 0 ) iff there exists an i 2 [n] such that u; v differ
only at i and no w 2 V exists such that ui < wi < vi and wj = uj = vj on [n]nfig. Trivially
jE 0 j
k jE j
jE j
jV j
jV j
jV j
Consider now shifting vertex v 2 V at shift label t 2 [k ] along shift coordinate s 2 [n] by
Ss;t (v; V ) = vs(v 0
s )
where
vs(i) = (v1 ; : : : ; vs(cid:0)1 ; i; vs+1 ; : : : ; vn )
for i 2 f0; : : : ; kg
vs(x) =2 V or x = vs (cid:9) if vs = t
s = (cid:26)min (cid:8)x 2 f0; : : : ; vs g (cid:12)(cid:12)
v 0
o.w.
vs

(7)

:

We shift V on s at t as usual; we shift V on s alone by bubbling vertices down to ﬁll gaps below:

Ss;t (V ) = fSs;t (v; V ) j v 2 V g
Ss (V ) = Ss;k (Ss;k(cid:0)1 (: : : Ss;1 (V ))) :
Let Ss (E 0 ) denote the edge-set induced by Ss (V ). Ss on a vertex-set is injective implying that

jSs (V )j = jV j :

(8)

Consider any fu; vg 2 E 0 with i 2 [n] denoting the index on which u; v differ. If i = s then
no other vertex w 2 V can come between u and v during shifting by construction of E 0 , so
fSs (u; V ); Ss (v; V )g 2 Ss (E 0 ). Now suppose that i 6= s.
If both vertices shift down by the
same number of labels then they remain connected in Ss (E 0 ). Otherwise assume WLOG that
Ss (u; V )s < Ss (v; V )s then the shifted vertices will lose their edge, however since vs did not
shift down to Ss (u; V )s there must have been some w 2 V different to u on fi; sg such that
ws < vs with Ss (w; V )s = Ss (u; V )s . Thus Ss (w; V ); Ss (u; V ) differ only on fig and a new edge
fSs (w; V ); Ss (u; V )g is in Ss (E 0 ) that was not in E 0 (otherwise u would not have shifted). Thus
jSs (E 0 )j (cid:21) jE 0 j :
Suppose that I (cid:18) [n] is (cid:9)P -shattered by Ss (V ). If s =2 I then (cid:5)I (Ss (V )) = (cid:5)I (V ) and I is
(cid:9)P -shattered by V . If s 2 I then V (cid:9)P -shatters I . Witnesses of Ss (V )’s (cid:9)P -shattering of I equal
to 1 at s, taking each value in f0; 1gjI j(cid:0)1 on I nfsg, were not shifted and so are witnesses for V ;
since these vertices were not shifted they were blocked by vertices of V of equal values on I nfsg
but equal to 0 at s, these are the remaining half of the witnesses of V ’s (cid:9)P -shattering of I . Thus

(9)

Ss (V ) (cid:9)P -shatters I (cid:18) [n] ) V (cid:9)P -shatters I :

(10)

In a ﬁnite number of shifts starting from (V ; E 0 ), a closed-below family W with induced edge-set
F will be reached. If I (cid:18) [n] is (cid:9)P -shattered by W and jI j = d = (cid:9)P -dim (W ), then since W
is closed-below the translation vector ( P;1 ; : : : ;  P;1 ) ((cid:1)) = (1 [(cid:1) < 1] ; : : : ; 1 [(cid:1) < 1]) must witness
this shattering. Hence each w 2 W has at most d non-zero components. Counting edges in F by
upper-adjoining vertices we have proved that

(V ; E 0 ) ﬁnitely shifts to closed-below graph (W; F )

s.t.

jF j (cid:20) jW j (cid:1) (cid:9)P -dim (W ) : (11)

jV j (cid:20) jE 0 j
Combining properties (7)–(11) we have that jE j
jV j (cid:20) jF j
jW j (cid:20) (cid:9)P -dim (W ) (cid:20) (cid:9)P -dim (V ).
The remaining arguments from the k = 1 case of [4, 3] now imply the multiclass mistake bound.

Theorem 5.2 Consider any k ; n 2 N and F (cid:18) f0; : : : ; kgX with (cid:9)P -dim (F ) < 1. The multi-
^MQG ;F ;F (n) (cid:20) (cid:9)P -dim (F ) =n.
class one-inclusion prediction strategy satis ﬁes

5.1 A lower bound

We now show that the preceding multiclass mistake bound is optimal to within a O(log k) factor,
noting that (cid:9)N is smaller than (cid:9)P by at most such a factor [1, Theorem 10].

De ﬁnition 5.3 We call a family F (cid:18) f0; : : : ; kgX trivial if either jF j = 1 or there exist no x1 ; x2 2
X and f1 ; f2 2 F such that f1 (x1 ) 6= f2 (x1 ) and f1 (x2 ) = f2 (x2 ).

Theorem 5.4 Consider any deterministic or randomized prediction strategy Q and any F (cid:18)
f0; : : : ; kgX that has 2 (cid:20) (cid:9)N -dim (F ) < 1 or is non-trivial with (cid:9)N -dim (F ) < 2. Then for
all n > (cid:9)N -dim (F ), ^MQ;F (n) (cid:21) maxf1; (cid:9)N -dim (F ) (cid:0) 1g=(2en).

Proof: Following [2], we use the probabilistic method to prove the existence of a target in F
for which prediction under a distribution P supported by a (cid:9)N -shattered subset is hard. Con-
sider d = (cid:9)N -dim (F ) (cid:21) 2 with n > d. Fix a Z = fz1 ; : : : ; zd g (cid:9)N -shattered by F and
then a subset FZ (cid:18) F of 2d functions that (cid:9)N -shatters Z . Deﬁne
a distribution P on X by
P (fzi g) = n(cid:0)1 for each i 2 [d (cid:0) 1], P (fzd g) = 1 (cid:0) (d (cid:0) 1)n(cid:0)1 and P (fxg) = 0 for all x 2
X nZ . Observe that PrP n (8i 2 [n (cid:0) 1]; Xn 6= Xi ) (cid:21) PrP n (Xn 6= zd ; 8i 2 [n (cid:0) 1]; Xn 6= Xi ) =

n (cid:1)n(cid:0)1
n (cid:0)1 (cid:0) 1
(cid:21) d(cid:0)1
d(cid:0)1
For any f 2 FZ and x 2 Z n with xn 6= xi for all i 2
en .
[n (cid:0) 1], exactly half of the functions in FZ consistent with sam ((x1 ; : : : ; xn(cid:0)1 ); f ) output
some i 2 f0; : : : ; kg on xn and the remaining half output some j 2 f0; : : : ; kgnfig. Thus
EUnif (FZ ) [1 [Q(sam ((x1 ; : : : ; xn(cid:0)1 ; F ) ; xn ) 6= F (xn )]] = 0:5 for such an x and so

d (cid:0) 1
^MQ;F (cid:21) ^MQ;FZ (cid:21) EUnif (FZ )(cid:2)P n [1 [Q(sam ((X1 ; : : : ; Xn(cid:0)1 ; F ) ; Xn ) 6= F (Xn )]] (cid:21)
2en
The similar case of d < 2 is omitted here and shows that there is a distribution P on X and function
f 2 F such that EP n [1 [Q(sam ((X1 ; : : : ; Xn(cid:0)1 ); f ) ; Xn ) 6= f (Xn )]] (cid:21) (2en)(cid:0)1 .

:

6 Conclusions and open problems

In this paper we have developed new shifting machinery and tightened the binary one-inclusion
mistake bound from d=n to Dd
n =n (dDd
n e=n for the deterministic strategy) representing a solid im-
provement for d (cid:25) n. We have described the multiclass generalization of the prediction learning
model and derived a mistake bound for the multiclass one-inclusion prediction strategy that improves
on previous PAC-based expected risk bounds by O(log n) and that is within O(log k) of optimal.
Here shifting with invariance to the shattering of a single set was described, however we are aware
of invariance to more complex shatterings. Another serious application of shatter-invariant shifting,
to appear in a sequel to this paper, is to the study of the cubical structure of maximum and maximal
classes with connections to the compressibility conjecture of [7]. While Theorem 4.4 resolves one
conjecture of Kuzmin & Warmuth [5], the remainder of the conjectured correctness proof for the
Peeling compression scheme is known to be false [8].
The symmetrization method of Theorem 4.4 can be extended over subgroups G (cid:26) Sn to gain tighter
n is the maximizer of density among all closed-below
density bounds. Just as the Sn -invariant V d
n , there exist G-invariant families that maximize the density over all of their sub-families.
V (cid:18) V d
In addition to Theorem 5.2 we have also proven the following special case in terms of (cid:9)G ; it is
open as to whether this generalizes to n 2 N. While a general (cid:9)G -based bound would allow direct
comparison with the PAC-based expected risk bound, it should also be noted that (cid:9)P and (cid:9)G are in
fact incomparable—neither (cid:9)G (cid:20) (cid:9)P nor (cid:9)P (cid:20) (cid:9)G singly holds for all classes [1, Theorem 1].

Lemma 6.1 ([8]) For any k 2 N and family V (cid:18) f0; : : : ; kg2 , dens (G (V )) (cid:20) (cid:9)G -dim (V ).

Acknowledgments

We gratefully acknowledge the support of the NSF under award DMS-0434383.

References

[1] Ben-David, S., Cesa-Bianchi, N., Haussler, D., Long, P. M.: Characterizations of learnability for classes of
f0; : : : ; ng-valued functions. Journal of Computer and System Sciences, 50(1) (1995) 74–86
[2] Ehrenfeucht, A., Haussler, D., Kearns, M., Valiant, L.: A general lower bound on the number of examples
needed for learning. Information and Computation, 82(3) (1989) 247–261
[3] Haussler, D.: Sphere packing numbers for subsets of the boolean n-cube with bounded Vapnik-
Chervonenkis dimension. Journal of Combinatorial Theory (A) 69(2) (1995) 217–232
[4] Haussler, D., Littlestone, N., Warmuth, M. K.: Predicting f0; 1g functions on randomly drawn points.
Information and Computation, 115(2) (1994) 284–293
[5] Kuzmin, D., Warmuth, M. K.: Unlabeled compression schemes for maximum classes. Journal of Machine
Learning Research (2006) to appear
[6] Li, Y., Long, P. M., Srinivasan, A.: The one-inclusion graph algorithm is near optimal for the prediction
model of learning. IEEE Transactions on Information Theory, 47(3) (2002) 1257–1261
[7] Littlestone, N., Warmuth, M. K.: Relating data compression and learnability. Unpublished manuscript,
http://www.cse.ucsc.edu/˜manfred/pubs/lrnk-olivier.pdf (1986)
[8] Rubinstein, B. I. P., Bartlett, P. L., Rubinstein, J. H.: Shifting: One-Inclusion Mistake Bounds and Sample
Compression. Technical report, EECS Department, UC Berkeley (2007) to appear
[9] Sauer, N.: On the density of families of sets. Journal of Combinatorial Theory (A), 13 (1972) 145–147


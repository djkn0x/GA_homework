The Robustness-Performance Tradeoff in Markov
Decision Processes

Huan Xu, Shie Mannor
Department of Electrical and Computer Engineering
McGill University
Montreal, Quebec, Canada, H3A2A7
xuhuan@cim.mcgill.ca
shie@ece.mcgill.ca

Abstract

Computation of a satisfactory control policy for a Markov decision process when
the parameters of the model are not exactly known is a problem encountered in
many practical applications. The traditional robust approach is based on a worst-
case analysis and may lead to an overly conservative policy. In this paper we con-
sider the tradeoff between nominal performance and the worst case performance
over all possible models. Based on parametric linear programming, we propose a
method that computes the whole set of Pareto efﬁcient policies in the performance-
robustness plane when only the reward parameters are subject to uncertainty. In
the more general case when the transition probabilities are also subject to error,
we show that the strategy with the “optimal ” tradeoff might b e non-Markovian
and hence is in general not tractable.

1

Introduction

In many decision problems the parameters of the problem are inherently uncertain. This uncertainty,
termed parameter uncertainty, can be the result of estimating the parameters from a ﬁnite s ample or
a speci ﬁcation of the parameters that itself includes uncer tainty. The standard approach in decision
making to circumvent the adverse effect of the parameter uncertainty is to ﬁnd a solution that per-
forms best under the worst possible parameters. This approach, termed the “robust ” approach, has
been used in both single stage ([1]) and multi-stage decision problems (e.g., [2]).

In robust optimization problems, it is usually assumed that the constraint parameters are uncertain.
By requiring the solution to be feasible to all possible parameters within the uncertainty set, Soyester
([1]) solved the column-wise independent uncertainty case, and Ben-Tal and Nemirovski ([3]) solved
the row-wise independent case.
In robust MDP problems, there may be two different types of
parameter uncertainty, namely, the reward uncertainty and the transition probability uncertainty.
Under the assumption that the uncertainty is state-wise independent (an assumption made by all
papers to date, to the best of our knowledge), the optimality principle holds and this problem can be
decomposed as a series of step by step mini-max problems solved by backward induction ([2, 4, 5]).

The above cited results focus on worst-case analysis. This implies that the vector of nominal param-
eters (the parameters used as an approximation of the true one regardless of the uncertainty) is not
treated in a special way and is just an element of the set of feasible parameters. The objective of the
worst-case analysis is to eliminate the possibility of disastrous performance. There are several dis-
advantages to this approach. First, worst-case analysis may lead to an overly conservative solution,
i.e., a solution which provides mediocre performance under all possible parameters. Second, the
desirability of the solution highly depends on the precise modeling of the uncertainty set which is
often based on some ad-hoc criterion. Third, it may happen that the nominal parameters are close to

the real parameters, so that the performance of the solution under nominal parameters may provide
important information for predicting the performance under the true parameters. Finally, there is
a certain tradeoff relationship between the worst-case performance and the nominal performance,
that is, if the decision maker insists on maximizing one criterion, the other criterion may decrease
dramatically. On the other hand, relaxing both criteria may lead to a well balanced solution with
both satisfactory nominal performance and also reasonable robustness to parameter uncertainty.

In this paper we capture the Robustness-Performance (RP) tradeoff explicitly. We use the worst-
case behavior of a solution as the function representing its robustness, and formulate the decision
problem as an optimization of both the robustness criterion and the performance under nominal
parameters simultaneously. Here, “simultaneously” is ach ieved by optimizing the weighted sum of
the performance criterion and the robustness criterion. To the best of our knowledge, this is the ﬁrst
attempt to address the overly conservativeness of worst-case analysis in robust MDP.

Instead of optimizing the weighted sum of the robustness and performance for some speci ﬁc weights,
we show how to efﬁciently ﬁnd the solutions for all possible w eights. We prove that the set of these
solutions is in fact equivalent to the set of all Pareto efﬁci ent solutions in the robustness-performance
space. Therefore, we solve the tradeoff problem without choosing a speci ﬁc tradeoff parameter, and
leave the subjective decision of determining the exact tradeoff to the decision maker. Instead of
arbitrarily claiming that a certain solution is a good tradeoff, our algorithm computes the whole
tradeoff relationship so that the decision maker can choose the most desirable solution according to
her preference, which is usually complicated and an explicit form is not available. Our approach
thus avoids the tuning of tradeoff parameters, where generally no good a-priori method exists. This
is opposed to certain relaxations of the worst-case robust optimization approach like [6] (for single
stage only) where some explicit tradeoff parameters have to be chosen. Unlike risk sensitive learning
approaches [7, 8, 9] which aim to tune a strategy online, our approach compute a robust strategy off-
line without trial and error.

The paper is organized as follows. Section 2 is devoted to the RP tradeoff for Linear Programming.
In Section 3 and Section 4 we discuss the RP tradeoff for MDP with uncertain rewards, and uncer-
tain transition probabilities, respectively. In Section 5 we present a computational example. Some
concluding remarks are offered in Section 6.

2 Parametric linear programming and RP tradeoffs in optimization

In this section, we brieﬂy recall Parametric Linear Program ming (PLP) [10, 11, 12], and show
how it can be used to ﬁnd the whole set of Pareto efﬁcient solut
ions for RP tradeoffs in Linear
Programming. This serves as the base for the discussion of RP tradeoffs in MDPs.

2.1 Parametric Linear Programming

x

(1)

For all λ ∈ [0, 1]: Minimize:
Subject to:

A Parametric Linear Programming is the following set of inﬁn itely many optimization problems:
λc(1)>
x + (1 − λ)c(2)>
Ax = b
x ≥ 0.
We call c(1)>
x the ﬁrst objective, and c(2)>
x the second objective. We assume that the Linear
Program (LP) is feasible and bounded for both objectives. Although there are uncountably many
possible λ, Problem (1) can be solved by a simplex-like algorithm. Here, “solve” means that for each
λ, we ﬁnd at least one optimal solution. An outline of the PLP al gorithm is described in Algorithm
1, which is essentially a tableau simplex algorithm while the entering variable is determined in a
speci ﬁc way. See [10] for a precise description.
Algorithm 1.
1. Find a basic feasible optimal solution for λ = 0. If multiple solutions exist,
choose one among those with minimal c(1)>
x.

2. Record current basic feasible solution. Check the reduced cost (i.e., the zero row in the
simplex table) of the ﬁrst objective, denoted as ¯c(1)
. If none of them is negative, end.
j

3. Among all columns with negative ¯c(1)
j
entering variable.

j /¯c(2)
, choose the one with largest ratio |¯c(1)
j

| as the

4. Pivot the base, go to 2.

This algorithm is based on the observation that for any λ, there exists an optimal basic feasible
solution. Hence, by ﬁnding a suitable subset of all vertices of the feasible region, we can solve
the PLP. Furthermore, we can ﬁnd this subset by sequentially pivoting among neighboring extreme
points like the simplex algorithm does. This algorithm terminates after ﬁnitely many iterations. It
is also known that the optimal value for PLP is a continuous piecewise linear function of λ. The
theoretical computational cost is exponential, although practically it works well. Such property is
shared by all simplex based algorithm. A detailed discussion on PLP can be found in [10, 11, 12].

2.2 RP tradeoffs in Linear Programming

(2)

Consider the following LP:
NOMINAL PROBLEM : Minimize: c>x
Subject to: Ax ≤ b
Here A ∈ Rn×m , x ∈ Rm , b ∈ Rn , c ∈ Rm .
Suppose that the constraint matrix A is only a guess of the unknown true parameter Ar which is
known to belonging to set A (we call A the uncertainty set). We assume that A is constraint-wise
independent and polyhedral for each of the constraints. That is, A = Qn
Ai , and for each i, there
i=1
exists a matrix T (i) and a vector v(i) such that Ai = (cid:8)a(i)> |T (i)a(i) ≤ v(i)(cid:9).
To quantify how a solution x behaves with respect to the parameter uncertainty, we deﬁne the follow-
ing criterion to be minimized as its robustness measure (more accurately, non-robustness measure).
˜A∈A (cid:13)(cid:13)(cid:13)(cid:13)h ˜Ax − bi+(cid:13)(cid:13)(cid:13)(cid:13)1
p(x) , sup
˜a(i)>x# − bi , 0) .
max ("
n
n
Xi=1
Xi=1
max (cid:2)˜a(i)>x − bi , 0(cid:3) =
= sup
sup
˜A∈A
˜a(i):T (i)˜a(i)≤v(i)
Here [·]+ stands for the positive part of a vector, ˜a(i)> is the ith row of the matrix ˜A, and bi is the
ith element of b. In words, the function p(x) is the largest possible sum of constraint violations.
Using the weighted sum of the performance and robustness objective as the minimizing objective,
we formulate the explicit tradeoff between robustness and performance as:
Minimize: λc>x + (1 − λ)p(x)
GENERAL PROBLEM : λ ∈ [0, 1]
Subject to: Ax ≤ b.
Here A ∈ Rn×m , x ∈ Rm , b ∈ Rn , c ∈ Rm .
By duality theorem, for a given x, sup˜a(i):T (i)˜a(i)≤v(i) ˜a(i)>x equals to the optimal value of the
following LP on y(i):

(3)

(4)

Minimize:
Subject to:

v(i)>y(i)
T (i)>y(i) = x
y(i) ≥ 0.
Thus, by adding slack variables, we rewrite GENERAL PROBLEM as the following PLP and solve
it using Algorithm 1:
GENERAL PROBLEM (PLP) : λ ∈ [0, 1]

Minimize: λc>x + (1 − λ)1>z
Subject to: Ax ≤ b,
T (i)>y(i) = x,
v(i)>y(i) − bi ≤ zi ,
z ≥ 0,
y(i) ≥ 0; i = 1, 2, · · · , n.

(5)

Here, 1 stands for a vector of ones of length n, zi is the ith element of z, and x, y(i), z are the
optimization variables.

3 The robustness-performance tradeoff for MDPs with uncertain rewards

A ( ﬁnite) MDP is deﬁned as a 5-tuple < T , S, As , p(·|s, a), r(s, a) > where: T is the (possibly
inﬁnite) set of decision stages; S is the state set; As is the action set of state s; p(·|s, a) is the
transition probability; and r(s, a) is the expected reward of state s with action a ∈ As . We use
r to denote the vector combining the reward for all state-action pairs and rs to denote the vector
combining all reward of state s. Thus, r(s, a) = rs (a). Both S and As are assumed ﬁnite. Both p
and r are time invariant.

In this section, we consider the case where r is not known exactly. More speci ﬁcally, we have a
nominal parameter r(s, a) which is believed to be a reasonably good guess of the true reward. The
reward r is known to belong to a bounded set R. We further assume that the uncertainty set R is
state-wise independent and a polytope for each state. That is, R = Qs∈S
Rs , and for each s ∈ S ,
there exists a matrix Cs and a vector ds such that Rs = {rs |Cs rs ≥ ds }. We assume that for
different visits of one state, the realization of the reward need not be identical and may take different
values within the uncertainty set. The set of admissible control policies for the decision maker is the
set of randomized history dependent policies, which we denote by ΠHR .
In the following three subsections we discuss different standard reward criteria: cumulative reward
with a ﬁnite horizon, discounted reward with inﬁnite horizo
n, and limiting average reward with
inﬁnite horizon under a unichain assumption.

3.1 Finite horizon case

In the ﬁnite horizon case ( T = {1, · · · , N }), we assume without loss of generality that each state
belongs to only one stage, which is equivalent to the assumption of non-stationary reward realization,
and use Si to denote the set of states at the ith stage. We also assume that the ﬁrst stage consists of
only one state s1 , and that there are no terminal rewards. We deﬁne the followi ng two functions as
the performance measure and the robustness measure of a policy π ∈ ΠHR :
N −1
Xi=1
P (π) , Eπ {
r(si , ai )},
N −1
Xi=1
Eπ {
The minimum is attainable, since R is compact and the total expected reward is a continuous func-
tion of r. We say that a strategy π is Pareto efﬁcient
if it obtains the maximum of P (π) among all
strategies that have a certain value of R(π). The following result is straightforward; the proof can
be found in the full version of the paper.
Proposition 1.
1. If π∗ is a Pareto efﬁcient strategy, then there exists a λ ∈ [0, 1] such that
π∗ ∈ arg maxπ∈ΠHR {λP (π) + (1 − λ)R(π)}.
2. If π∗ ∈ arg maxπ∈ΠHR {λP (π) + (1 − λ)R(π)} for some λ ∈ (0, 1). Then π∗ is a Pareto
efﬁcient strategy.

R(π) , min
r∈R

r(si , ai )}.

(6)

For 0 ≤ t ≤ N , s ∈ St , and λ ∈ [0, 1] deﬁne:
Pt (π , s) , Eπ (N −1
r(si , ai )|st = s)
Xi=t
Eπ (N −1
r(si , ai )|st = s)
Xi=t
Rt (π , s) , min
r∈R
cλ
t (s) , max
π∈ΠHR {λPt (π , s) + (1 − λ)Rt (π , s)} .

(7)

1 (s1 ) is the optimal RP tradeoff with weight λ. The
We set PN ≡ RN ≡ cN ≡ 0, and note that cλ
following theorem shows that the principle of optimality holds for c. The proof is omitted since it
follows similarly to standard backward induction in ﬁnite h orizon robust decision problems.
Theorem 1. For s ∈ St , t < N , let ∆s be the probability simplex on As , then
q∈∆s n min
cλ
(cid:2)λ Pa∈As r(s, a)q(a) + (1 − λ) Pa∈As r(s, a)q(a)(cid:3) +
t (s) = max
rs∈Rs
t+1 (s0 )o.
Ps0∈St+1 Pa∈As p(s0 |s, a)q(a)cλ
We now consider the maximin problem in each state and show how to ﬁnd the solutions for all λ
t (s) is piecewise linear in λ. Let St+1 = {s1 , · · · , sk }. Assume
in one pass. We also prove that cλ
t+1 (sj ) are continuous piece-wise linear functions. Thus, we can divide
for all j ∈ {1, · · · , k}, cλ
[0, 1] into ﬁnite (say n) intervals [0, λ1 ], · · · [λn−1 , 1] such that in each interval, all ct+1 functions
are linear. That is, there exist constants lj
i and mj
t+1 (sj ) = lj
i λ + mj
i such that cλ
i , for λ ∈ [λi−1 , λi ].
t (s) equals to the optimal value of the following LP on y and
By the duality theorem, we have that cλ
q.

k
Xj=1 Xa∈As

(8)

p(sj |s, a)q(a)cλ
t+1 (sj )

Maximize: (1 − λ)d>
s y + λr>
s q +
Subject to: C >
s y = q,
1>q = 1,
q, y ≥ 0.
t+1 (sj ) and rearranging, it follows
Observe that the feasible set is the same for all λ. Substituting cλ
that for λ ∈ [λi−1 , λi ] the objective function equals to
(1 − λ)n Pa∈As hPk
i i q(a) + d>
s yo
j=1 p(sj |s, a)mj
+λn Pa∈As hr(s, a) + Pk
i )i q(a)o.
j=1 p(sj |s, a)(lj
i + mj
Thus, for λ ∈ [λi−1 , λi ], from the optimal solution for λi−1 , we can solve for all λ using Algo-
rithm 1. Furthermore, we need not to re-initiate for each interval, since the optimal solution for
the end of ith interval is also the optimal solution for the begin of the next interval. It is obvious
t (s) is also continuous, piecewise linear. Thus, since cN = 0, the assumption of
that the resulting cλ
continuous and piecewise linear value functions holds by backward induction.

3.2 Discounted reward inﬁnite horizon case

In this section we address the RP tradeoff for inﬁnite horizo n MDPs with a discounted reward
criterion. For a ﬁxed λ, the problem is equivalent to a zero-sum game, with the decision maker
trying to maximize the weighted sum and Nature trying to minimize it by selecting an adversarial
reward realization. A well known result in discounted zero-sum stochastic games states that, even if
non-stationary policies are admissible, a Nash equilibrium in which both players choose a stationary
policy exists; see Proposition 7.3 in [13].
Given an initial state distribution α(s), it is also a known result [14] that there exists a one-to-
one correspondence relationship between the state-action frequencies P∞
i=1 γ i−1E(1si=s,ai=a ) for
stationary strategies and vectors belonging to the following polytope X :
x(s0 , a) − Xs∈S Xa∈As
Xa∈As0
γ p(s0 |s, a)x(s, a) = α(s0 ),
x(s, a) ≥ 0, ∀s, ∀a ∈ As .
Since it sufﬁces to consider a stationary policy for Nature,
r∈R Xs∈S Xa∈As
Maximize: inf
Subject to: x ∈ X .

[λr(s, a)x(s, a) + (1 − λ)r(s, a)x(s, a)]

the tradeoff problem becomes:

(10)

(9)

By duality of LP, Equation (10) could be rewritten as the following PLP and solved by Algorithm 1.
r(s, a)x(s, a) + (1 − λ) Xs∈S (cid:2)d>
Maximize:λ Xs∈S Xa∈As
s ys (cid:3)
x(s0 , a) − Xs∈S Xa∈As
Subject to: Xa∈As0
γ p(s0 |s, a)x(s, a) = α(s0 ),
x(s, a) ≥ 0,
∀s, ∀a,
C >
s ys = xs ∀s,
∀s.
ys ≥ 0,
3.3 Limiting average reward case (unichain)

∀s0 ,

(11)

In the unichain case, the set of limiting average state-action frequency vectors (that is, all limit points
T PT
of sequences (cid:8) 1
Eπ [1sn=s,an=a ](cid:9), for π ∈ ΠHR ) is the following polytope X :
n=1
x(s0 , a) − Xs∈S Xa∈As
Xa∈As0
p(s0 |s, a)x(s, a) = 0, ∀s0 ∈ S,
Xs∈S Xa∈As
x(s, a) = 1,
x(s, a) ≥ 0, ∀s, ∀a ∈ As .
As before, there exists an optimal maximin stationary policy. By a similar argument as for the
discounted case, the tradeoff problem can be converted to the following PLP:
Maximize:λ Xs∈S Xa∈As
r(s, a)x(s, a) + (1 − λ) Xs∈S (cid:2)d>
s ys (cid:3)
x(s0 , a) − Xs∈S Xa∈As
Subject to: Xa∈As0
p(s0 |s, a)x(s, a) = 0,
Xs∈S Xa∈As
x(s, a) = 1,
C >
s ys = xs ,
ys ≥ 0,
∀s,
∀s, ∀a.
x(s, a) ≥ 0,
4 The RP tradeoff in MDPs with uncertain transition probabilities

∀s0 ,

∀s,

(12)

(13)

In this section we provide a counterexample which demonstrates that the weighted sum criterion
in the most general case, i.e., the uncertain transition probability case, may lead to non-Markovian
optimal policies.
In the ﬁnite horizon MDP shown in the Figure 1, S = {s1, s2, s3, s4, s5, t1, t2, t3, t4}; As1 =
{a(1, 1)}; As2 = {a(2, 1)}; As3 = {a(3, 1)}; As4 = {a(4, 1)} and As5 = {a(5, 1), a(5, 2)}.
Rewards are only available at the ﬁnal stage, and are perfect
ly known. The nominal transition prob-
abilities are p (s2|s1, a(1, 1)) = 0.5, p (s4|s2, a(2, 1)) = 1, and p (t3|s5, a(5, 2)) = 1. The set of
possible realization is p (s2|s1, a(1, 1)) ∈ {0.5}, p (s4|s2, a(2, 1)) ∈ [0, 1], and p (t3|s5, a(5, 2)) ∈
[0, 1]. Observe that the worst parameter realization is p(s4|s2, a(2, 1)) = p(t3|s5, a(5, 2)) = 0.
We look for the strategy that maximizes the sum of the nominal reward and the worst-reward (i.e.,
λ = 0.5). Since multiple actions only exist in state s5, a strategy is determined by the action chosen
on s5. Let the probability of choosing action a(5, 1) and a(5, 2) be p and 1 − p, respectively.
Consider the history “ s1 → s2”. In this case, with the nominal transition probability, th is trajectory
will reach t1 with a reward of 10, regardless of the choice of p. The worst transition is that action
a(2, 1) leads to s5 and action a(5, 2) leads to t4, hence the expected reward is 5p + 4(1 − p).
Therefore the optimal p equals to 1, i.e., the optimal action is to choose a(5, 1) deterministically.

0.5 (0.5)

s2

a(2,1)

1 (0)

s4

a(4,1)

s1

a(1,1)

0 (1) 

a(5,1)

0.5 (0.5)

a(3,1)

s3

s5

a(5,2)

1 (0)

0 (1)

t1 
r=10

t2
r=5

t3
r=8

t4
r=4

Figure 1: Example of non-Markovian best strategy

Consider the history “ s1 → s3”. In this case, the nominal reward is 5p + 8(1 − p), and the worst
case reward is 5p + 4(1 − p). Thus p = 0 optimize the weighted sum, i.e., the optimal strategy is to
choose a(5, 2).
The unique optimal strategy for this example is thus non-Markovian. This non-Markovian property
implies a possibility that past actions affect the choice of future actions, and hence could render the
problem intractable. The optimal strategy is non-Markovian because we are taking expectation over
two different probability measures, hence the smoothing property of conditional expectation cannot
be used in ﬁnding the optimal strategy.

5 A computational example

We apply our algorithm to a T -stage machine maintenance problem. Let S , {1, · · · , n} denote the
state space for each stage. In state h, the decision maker can choose either to replace the machine
which will lead to state 1 deterministically, or to continue running, which with probability p will
lead to state h + 1. If the machine is in state n, then the decision maker has to replace it. The
replacing cost is perfectly known to be cr , and the nominal running cost in state h is ch . We assume
that the realization of the running cost lies in the interval [ch − δh , ch + δh ]. We set ch = √h − 1
and δh = 2h/n. The objective is to minimize the total cost, in a risk-averse attitude. Figure 2(a)
shows the tradeoff of this MDP.

For each solution found, we sample the reward 300 times according to a uniform distribution. We
normalize the cost for each simulation, i.e., we divide the cost by the smallest expected nominal
cost. Denoting the normalized cost of the ith simulation for strategy j as si (j ), we use the following
function to compare the solutions:
αs P300
i=1 |si (j )|α
300
Note that α = 1 is the mean of the simulation cost, whereas larger α puts higher penalty on deviation
representing a risk-averse decision maker. Figure 2(b) shows that, the solutions that focus on nomi-
nal parameters (i.e., λ close to 1) achieve good performance for small α, but worse performance for
large α. That is, if the decision maker is risk neutral, then the solutions based on nominal parameters
are good. However, these solutions are not robust and are not good choices for risk-averse decision
makers. Note that, in this example, the nominal cost is the expected cost for each stage, i.e., the
parameters are exactly formulated. Even in such case, we see that risk-averse decision makers can
beneﬁt from considering the RP tradeoff.

vj (α) =

.

6 Concluding remarks

In this paper we proposed a method that directly addresses the robustness versus performance trade-
off by treating the robustness as an optimization objective. Based on PLP, for MDPs where only

(a)

λ=1

29.28

29    

28.5  

28    

27.5  

27    

26.5  

26    

e
c
n
a
m
r
o
f
r
e
P
 
e
s
a
C
 
t
s
r
o
W

λ=0

25.68
16.79

17   

17.2 

17.4 
17.6 
17.8 
18   
Norminal Performance

18.2 

18.4 

18.6 

18.76

n
a
e
M
 
d
e
i
f
i
d
o
M
 
d
e
z
i
l
a
m
r
o
N

1.4

1.35

1.3

1.25

1.2

1.15

1.1

1.05

1

0.95

0.9

0

α=1
α=10
α=100
α=1000
0.2

(b)

0.4

λ

0.6

0.8

1

Figure 2: The machine maintenance problem: (a) the PR tradeoff; (b) normalized mean of the
simulation for different values of α.

rewards are uncertain, we presented an efﬁcient algorithm t hat computes the whole set of optimal
RP tradeoffs for MDPs with ﬁnite horizon, inﬁnite horizon di
scounted reward, and limiting average
reward (unichain). For MDPs with uncertain transition probabilities, we showed an example where
the solution may be non-Markovian and hence may in general be intractable.
The main advantage of the presented approach is that it addresses robustness directly. This frees
the decision maker from the need to make probabilistic assumptions on the problems parameters. It
also allows the decision maker to determine the desired robustness-performance tradeoff based on
observing the whole curve of possible tradeoffs rather than guessing a single value.

References

[1] A. L. Soyster. Convex programming with set-inclusive constraints and applications to inexact linear
programming. Oper. Res., 1973.
[2] A. Bagnell, A. Ng, and J. Schneider. Solving uncertain markov decision processes. Technical Report
CMU-RI-TR-01-25, Carnegie Mellon University, August 2001.
[3] A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear programs. Oper. Res. Lett., 25(1):1–
13, August 1999.
[4] C. C. White III and H. K. El-Deib. Markov decision process with imprecise transition probabilities. Oper.
Res., 42(4):739–748, July 1992.
[5] A. Nilim and L. El Ghaoui. Robust control of markov decision processes with uncertain transition matri-
ces. Oper. Res., 53(5):780–798, September 2005.
[6] D. Bertsimas and M. Sim. The price of robustness. Oper. Res., 52(1):35–53, January 2004.
[7] M. Heger. Consideration of risk in reinforcement learning. In Proc. 11th International Conference on
Machine Learning, pages 105–111. Morgan Kaufmann, 1994.
[8] R. Neuneier and O. Mihatsch. Risk sensitive reinforcement learning. In Advances in Neural Information
Processing Systems 11, pages 1031–1037, Cambridge, MA, USA, 1999. MIT Press.
[9] P. Geibel. Reinforcement learning with bounded risk.
In Proc. 18th International Conf. on Machine
Learning, pages 162–169. Morgan Kaufmann, San Francisco, CA, 2001.
[10] D. Bertsimas and J. N. Tsitsiklis. Introduction to Linear Optimization. Athena Scientiﬁc, 1997.
[11] M. Ehrgott. Multicriteria Optimization. Springer-Verlag Berlin Heidelberg, 2000.
[12] K. G. Murty. Linear Programming. John Wiley & Sons, 1983.
[13] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.
[14] M. L. Puterman. Markov Decision Processes. John Wiley & Sons, INC, 1994.


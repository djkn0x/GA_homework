Simplifying Mixture Models
through Function Approximation

Kai Zhang
James T. Kwok
Department of Computer Science and Engineering
The Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
{twinsen, jamesk}@cse.ust.hk

Abstract

Finite mixture model is a powerful tool in many statistical learning problems.
In this paper, we propose a general, structure-preserving approach to reduce its
model complexity, which can bring signiﬁcant computationa l bene ﬁts in many
applications. The basic idea is to group the original mixture components into
compact clusters, and then minimize an upper bound on the approximation error
between the original and simpliﬁed models. By adopting the L2 norm as the dis-
tance measure between mixture models, we can derive closed-form solutions that
are more robust and reliable than using the KL-based distance measure. Moreover,
the complexity of our algorithm is only linear in the sample size and dimensional-
ity. Experiments on density estimation and clustering-based image segmentation
demonstrate its outstanding performance in terms of both speed and accuracy.

1 Introduction

In many statistical learning problems, it is useful to obtain an estimate of the underlying probability
density given a set of observations. Such a density model can facilitate discovery of the underlying
data structure in unsupervised learning, and can also yield, asymptotically, optimal discriminant
procedures [7]. In this paper, we focus on the ﬁnite mixture model , which describes the distribution
by a mixture of simple parametric functions φ(·)’s as f (x) = Pn
j=1 αj φ(x, θj ). Here, θj is the
parameter for the j th component, and the mixing parameters αj ’s satisfy Pn
j=1 αj = 1. The most
common parametric form of φ is the Gaussian, leading to the well-known Gaussian mixtures.
The mixture model has been widely used in clustering and density estimation, where the model
parameters can be estimated by the standard Expectation-Maximization (EM) algorithm. However,
the EM can be prohibitively expensive on large problems [12]. On the other hand, note that in many
learning processes using mixture models (such as particle ﬁ ltering [6] and non-parametric belief
propagation [13]), the computational requirement is also very demanding due to the large number
of components involved in the model. In this situation, our interest is more on reducing the number
of components for prospective computational saving. Previous works typically employ spatial data
structures, such as the kd-tree [8, 9], for acceleration. Recently, [5] proposes to reduce a large
Gaussian mixture into a smaller one by minimizing a KL-based distance between the two mixtures.
This has been applied with success on hierarchical clustering of scenery images and handwritten
digits.

In this paper, we propose a new algorithm for simplifying a given ﬁnite mixture model while preserv-
ing its component structures, with application to nonparametric density estimation and clustering.
The idea is to minimize an upper bound on the approximation error between the original and sim-
pliﬁed mixture models. By adopting the L2 norm as the error criterion, we can derive closed-form
solutions that are more robust and reliable than using the KL-based distance measures. At the same

time, our algorithm can be applied to general Gaussian kernels, and the complexity is only linear in
the sample size and dimensionality.

The rest of the paper is organized as follows.
In Section 2 we describe the proposed approach
in detail, and illustrate its advantages compared with the existing ones. In Section 3, we report
experimental results on simplifying the Parzen window estimator, and color image segmentation
through the mean shift clustering procedure. Section 4 gives some concluding remarks.

2 Approximation Algorithm

Given a mixture model

n
Xj=1
we assume that the j th component φj (x) is of the form

f (x) =

αj φj (x),

φj (x) = |Hj |−1/2KHj (x − xj ) ,

(1)

(2)

with weight αj , center xj and covariance matrix Hj . Here, KH (x) = K (H−1/2x) where K (x)
is the kernel that is bounded and has compact support. Note that for radially symmetric kernels, it
sufﬁces to de ﬁne K by the pro ﬁle k such that K (x) = k(kxk2 ). With this notion, the gradient of
the kernel function, KH (x), can be conveniently written as ∂xKH (x) = k ′ (r)∂x r = 2k ′ (r)H−1x,
where r = xH−1x. Our task is to approximate f with a simpler mixture model

(x − ti ) ,

(3)

(4)

g (x) =

wi gi (x),

m
Xi=1
with m ≪ n, where each component gi also takes the form
gi (x) = | ˜Hi |−1/2K ˜Hi
with weight wi , center ti , and covariance matrix ˜Hi .
Note that direct approximation of f by g is not feasible, because they involve a large number of
components. Given a distance measure D(·, ·) between functions, the approximation error
E = D(f , g ) = D 
wi gi
n
m
Xj=1
Xi=1


is usually difﬁcult to optimize. However, the problem can be very much simpliﬁed by minimizing
an upper bound of E . Consider the L2 distance D(φ, φ′ ) = R (φ(x) − φ′ (x))2 dx, and suppose that
the mixture components {φj }n
j=1 are divided into disjoint clusters S1 , . . . , Sm . Then, it is easy to
see that the approximation error E is bounded by
E = Z 
wi gi (x)
n
m
m
Xj=1
Xi=1
Xi=1


Denote this upper bound by E = m Pm
i=1 E i , where
E i = Z 
αj φj (x)
wi gi (x) − Xj∈Si

Note that E is the sum of the “local” approximation errors
E i ’s. Hence, if we can ﬁnd a good
representative wi gi for each cluster by minimizing the local approximation error E i , the overall
approximation performance can also be guaranteed. This suggests partitioning the original mixture
components into compact clusters, wherein approximation can then be done much more easily. Our
basic algorithm proceeds as follows:

Z 
wi gi (x) − Xj∈Si

αj φj (x)


αj φj (x) −

2

dx.

2

dx.

2

dx ≤ m

αj φj ,

(5)

(6)

(Section 2.1.1) Partition the set of mixture components (φj ’s) into m clusters where m ≪ n.
1.
Let Si be the set that indexes all components belonging to the ith cluster.
2. (Section 2.1.2) For each cluster, approximate the local mixture model Pj∈Si
component wi gi , where gi is de ﬁned in (4).
3. The simpliﬁed model g is obtained by g (x) = Pm
i=1 wi gi (x).
These steps will be discussed in more detail in the following sections.

αj φj by a single

2.1 Procedure

2.1.1 Partitioning of Components

In this section, we consider how to group similar components into the same cluster, so that the
subsequent local approximation can be more accurate. A useful algorithm for this task is the classic
vector quantization (VQ) [4], where one iterates between partitioning a set of vectors and ﬁnding
the best prototype for each partition until the distortion error converges. By de ﬁning a distance
D(·, ·) between mixture components φj ’s, we can partition the mixture components in a similar
way. However, vector quantization is sensitive to the initial partitioning. So we ﬁrst introduce a
simple but highly efﬁcient partitioning method called sequential sampling (SS):
1. Randomly select a φj and add it to the set of representatives R.
2. For all the components (j = 1, 2, . . . , n), do the following
• Compute the distance D (φj , Ri ), where Ri ∈ R.
• Once if D (φj , Ri ) ≤ r, where r is a prede ﬁned threshold, assign φj to the representative
Ri , and then process the next component.
• If D (φj , Ri ) > r for all Ri ∈ R, add φj as a new representative of R.
3. Terminate when all the components have been processed.

This procedure partitions the components by choosing those φj ’s that are enough far away as rep-
resentatives, with a user-de ﬁned resolution r. So it is less sensitive to initialization. In practice, we
will ﬁrst initialize by sequential sampling, and then perfo rm the iterative VQ procedure to further
re ﬁne the partition, i.e., ﬁnd the best representative Ri for each cluster, reassign each component φj
to the closest representative Rπ(j) , and iterate until the error Pj αj D(φj , Rπ(j) ) converges.
2.1.2 Local Approximation

2

dx

= w2
i

In this part, we consider how to obtain a good representative, wi gi in (4), for each local cluster Si .
The task is to determine the unknown variables wi , ti and ˜Hi associated with gi . Using the L2 norm,
the upper bound (6) of the local approximation error can be written as
αj φj (x)
E i = Z 
wi gi (x) − Xj∈Si

CK
2CK αj k(rij )
− wi Xj∈Si
|2 ˜Hi |1/2
|Hj + ˜Hi |1/2
Here, CK = R k(x′x)dx is a kernel-dependent constant, ci = R (Pj∈Si
j (x))2 dx is a data-
αj φ2
dependent constant (irrelevant to the unknown variables), and rij = (ti −xj )′ (Hj + ˜Hi )−1 (ti −xj ).
Here we have assumed that k(a) · k(b) = k(a + b), which is valid for the Gaussian and negative
exponential kernels. Without this assumption, solutions can still be obtained but are less compact.
To minimize E i w.r.t. wi , ti and ˜Hi , one can set the corresponding partial derivatives of E i to
zero. However, this leads to a nonlinear system that is quite difﬁcult to solve. Here, we decouple
the relations among these three parameters. First, observe that E i is a quadratic function of wi .
Therefore, given ˜Hi and ti , the minimum value of E i can be easily obtained as
2
−1/2
2 
1
Xj∈Si


= | ˜Hi |

αj k(rij ) (cid:12)(cid:12)(cid:12)

Hj + ˜Hi (cid:12)(cid:12)(cid:12)

min
i

E

+ ci .

.

(7)

The remaining task is to minimize E

min
i w.r.t. ti and ˜Hi . By setting ∂ti E
αj k ′ (rij ) (Hj + ˜Hi )−1xj
|Hj + ˜Hi |1/2

,

i Xj∈Si
ti = M−1

min
i = 0, we have

(8)

αj k ′ (rij ) (Hj + ˜Hi )−1
|Hj + ˜Hi |1/2

Mi = Xj∈Si
This is an iterative contraction mapping. If ˜Hi is ﬁxed, we can obtain ti by starting with an initial
min
(0)
, and then iterate (8) until convergence. Now, to solve ˜Hi , we set ∂ ˜Hi
i = 0 and obtain
E
t
i
αj ( ˜Hi + Hj )−1
|Hj + ˜Hi |1/2 (cid:16)k(rij )Hj + 4(−k ′(rij ))(xj − ti )(xj − ti )′ ( ˜Hi + Hj )−1 ˜Hi(cid:17) ,
(9)

˜Hi = P−1
i Xj∈Si

.

where

where

Pi = Xj∈Si

( ˜Hi + Hj )−1
|Hj + ˜Hi |1/2

αj k(rij ).

In summary, we ﬁrst initialize
αj xj /(Pj∈Si
= Pj∈Si
αj ),
αj (cid:16)Hj + (t
i − xj )′(cid:17) /(Pj∈Si
(0)
(0)
= Pj∈Si
i − xj )(t
and then iterate (8) and (9) until convergence. The converged values of ti and ˜Hi are substituted
into ∂wi E i = 0 to obtain wi as

(0)
t
i
(0)
˜H
i

αj ),

wi = |2 ˜Hi |

1
2 Xj∈Si

αj k(rij )
|Hj + ˜Hi |1/2

.

(10)

2.2 Complexity

In the partitioning step, sequential sampling has a complexity of O(dmn), where n is original model
size, m is the number of clusters, and d the dimension. By using a hierarchical scheme [2], this
can be reduced to O(dn log(m)). The VQ takes O(dnm) time. In the local approximation step,
the complexity is l Pm
i=1 ni d3 = lnd3 , where l is the maximum number of iterations needed. In
practice, we can enforce a diagonal structure on the covariance matrix ˜Hi ’s while still obtaining a
closed-form solution. Hence, the complexity becomes linear in the dimension d instead of cubic.
Summing up these three terms, the overall complexity is O(dn log(m) + dnm + lnd) = O(dn(m +
l)), which is linear in both the data size and dimension (in practice m and l are quite small).

2.3 Remarks

In this section, we discuss some interesting properties of the approximation scheme proposed in
Section 2.1.2. To have better intuitions, we examine the special case of a Parzen window density
estimator [11], where all φj ’s have the same weights and bandwidths (Hj = H for j = 1, 2, . . . , n).
Equation (9) then reduces to

˜Hi = H + 4 ˜Hi ( ˜Hi + H)−1Vi ,

(11)

where

Vi = Pj∈Si
αj (−k ′ (rij ))(xj − ti )(xj − ti )′
Pj∈Si
αj k(rij )
It shows that the bandwidth ˜Hi of gi can be decomposed into two parts: the bandwidth H of the
original kernel density estimator, and the covariance Vi of the local cluster Si with an adjusting
matrix Γi = 4 ˜Hi ( ˜Hi + H)−1 . As an illustration, consider the 1-D case where H = h2 , ˜Hi = h2
i .

.

Then γi = 4h2
i = h2 + γiVi . Since Vi ≥ 0 and γi ≥ 2, we can see that h2
, and h2
i ≥ h2 + Vi .
i
h2+h2
i
Moreover, hi is closely related to the spread of the local cluster. If all the points in Si are located at
the same position (i.e., Vi = 0), then h2
i = h2 . Otherwise, the larger the spread of the local cluster,
the larger is hi . In other words, the bandwidths ˜Hi ’s are adaptive to the local data distribution.
Related works in simplifying the mixture models (such as [5]) simply choose ˜Hi = H + C ov [Si ].
In comparison, our covariance term Vi is more reliable in that it incorporates distance-based weight-
ing. Interestingly, this is somewhat similar to the bandwidth matrix used in the manifold Parzen
windows [14], which is designed for handling sparse, high-dimensional data more robustly. Note
that our choice of ˜Hi is derived rigorously by minimizing the L2 approximation error. Therefore,
this coincidence naturally indicates the robustness of the L2 -norm based distance measures. More-
over, note that the adjusting matrix Γi changes not only the scale of the bandwidth, but also its
eigen-structures in an iterative manner. This will be very bene ﬁcial in multivariate cases.
Second, in determining the center of gi , (8) can be reduced to
ti = Pj∈Si
αj k ′
H+ ˜Hi
Pj∈Si
αj k ′
H+ ˜Hi
This can be regarded a mean-shift procedure [1] in the d-dimensional space with kernel K .
It
is easy to verify that this iterative procedure is indeed locating the peak of the density function
pi (x) = |H + ˜Hi |−
1
(x − xj ). Note, on the other hand, that what we want to
2 Pj∈Si
KH+ ˜Hi
1
approximate originally is the local density fi (x) = |H|−
KH (x − xj ). In the 1-D case
2 Pj∈Si
(with H = h2 , and ˜Hi = h2
i ), the bandwidth of pi (i.e., h2 + h2
i ) is larger than that of fi (i.e., h2 ).
It appears intriguing that on ﬁtting a kernel density fi (x) estimated on the sample set {xj }j∈Si ,
one needs to locate the maximum of another density function pi (x), instead of the maximum of
fi (x) itself or simply, the mean of the sample set {xj }j∈Si as chosen in [5]. Indeed, these three
choices coincide when the distribution of Si is symmetric and uni-modal, but will differ otherwise.
Intuitively, when the data is asymmetric, the center ti should be biased towards the heavier side of
the data distribution. The maximum of fi (x) thus fails to meet this requirement. On the other hand,
the mean of Si , though biased towards the heavier side, still lacks an accurate control on the degree
of bias. In comparison, our method provides a principled way of selecting the center. Note that
pi (x) has a larger bandwidth than the original fi (x). Therefore, its maximum will move towards the
heavier side of the distribution compared with that of fi (x), with the degree of bias automatically
controlled by the mean shift iterations in (12).

(xj − ti ) xj
(xj − ti )

(12)

.

Here, we give an illustration on the performance of the three center selection schemes. Figure 1(a)
shows the histogram of a local cluster Si , whose Parzen window estimator (fi ) is asymmetric. Fig-
ure 1(b) plots the corresponding approximation error E i (6) at different bandwidths hi (the remaining
parameter, wi , is set to the optimal value by (10) ). As can be seen, the approximation error of our
method is consistently lower than those of the other two. Moreover, the resultant optimum is also
much lower.

m
a
r
g
o
t
s
i
h

35

30

25

20

15

10

5

0

1.5

2

x
(a) The histogram of a local cluster
Si and its density fi .

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

10

9

8

7

6

5

4

3

2

1

0

local maximu
local mean
our method

0.2

0.1

0.05

0.15
2
h
i
(b) Approximation error.

0.25

0.3

2.5

3

Figure 1: Approximation of an asymmetric density using different center selection schemes.

3 Experiments

In this section, we perform experiments to evaluate the performance of our mixture simpliﬁcation
scheme. We focus on the Parzen window estimator which, on given a set of samples S = {xi }n
i=1 in
2 Pn
Rd , can be written as ˆf (x) = 1
1
j=1 KH (x − xj ) . Note that the Parzen window estimator
n |H|−
is a limiting form of the mixture model, where the number of components equals the data size
and can be quite huge.
In Section 3.1, we use the proposed approach to reduce the number of
components in the kernel density estimator, and compare its performance with the algorithm in [5].
Then, in Section 3.2, we perform color image segmentation by running the mean shift clustering
algorithm on the simpliﬁed density model.

3.1 Simplifying Nonparametric Density Models

In this section, we reduce the number of kernels in the Parzen window estimator by using the pro-
posed approach and the method in [5]. Experiments are performed on a 1-D set with 1800 samples
drawn from the Gaussian mixture 8
18 N (−0.8, 0.36) + 4
18 N (−2.6, 0.09) + 6
18 N (1.7, 0.64), where
N (µ, σ2 ) denotes the normal distribution with mean µ and variance σ2 . The Gaussian kernel with
ﬁxed bandwidth h = 0.3 is used for density estimation. To make the problem more challenging,
we choose m = 5, i.e., only 5 kernels are used to approximate the density. The k-means algorithm
is used for initialization. As can be seen from Figure 2(b), the third Gaussian component has been
broken into two by the method in [5]. In comparison, our result in Figure 2(c) is more reliable.

180

160

140

120

100

80

60

40

20

0
−4

−3

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

2

3

4

0
−4

−3

1

−1

−2

0
x
(a) Histogram.

−2

−1

0

1

2

3

x
(b) Result by [5].

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

4

5

0
−4

−3

−2

−1

0

1

2

3

4

5

x
(c) Our result.

Figure 2: Approximate the Parzen window estimator by simplifying mixture models. Green: Parzen
window estimator; black: simpliﬁed mixture model; blue-da shed: components of the mixture model.

To have a quantitative evaluation, we randomly generate the 3-Gaussian data 100 times, and compare
the two algorithms (ours and [5]) using the following error criteria: 1) the L2 error (5); 2) the
standard KL distance; 3) the local KL-distance used in [5]. The local KL-distance between two
mixtures, f = Pn
j=1 αj φj and g = Pm
i=1 wi gi , is de ﬁned as
n
Xj=1
d(f , g ) =
where π(j ) is the function that maps each component φj to the closest representative component
gπ(j) such that π(j ) = arg mini=1,2,...,m K L(φj ||gi ).
Results are plotted in Figure 3, where for clarity we order the results in increasing error obtained
by [5]. We can see that under the L2 norm, the error of our algorithm is signiﬁcantly lower than
that of [5]. Quantitatively, our error is only about 36.61% of that by [5]. On using the standard
KL-distance, our error is about 87.34% of that by [5], where the improvement is less signiﬁcant.
This is because the KL-distance is sensitive to the tail of the distribution, i.e., a small difference in
the low-density regions may induce a huge KL-distance. As for the local KL-distance, our error is
about 99.35% of that by [5].

αj K L(φj ||gπ(j) ),

3.2 Image Segmentation

The Parzen window estimator can be used to reveal important clustering information, namely that its
modes (or local maxima) correspond to dominant clusters in the data. This property is utilized in the

x 10−5

method in [5]
our method

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

r
o
r
r
e
 
m
r
o
n
−
2
L

x 10−3

method in [5]
our method

7

6

5

4

3

2

1

e
c
n
a
t
s
i
d
 
L
K

method in [5]
our method

3600

3500

3400

r
o
r
r
e
 
L
K
 
l
a
c
o
l

3300

3200

3100

3000

2900

0

0

20

40
60
number of tests
(a) The L2 distance error.

80

100

0

0

60

80

100

2800

0

20

40

x
(b) Standard KL-distance.

20

40
60
number of tests
(c) Local KL-distance deﬁned by [5]

100

80

Figure 3: Quantitative comparison of the approximation errors.

mean shift clustering algorithm [1, 3], where every data point is moved along the density gradient
until it reaches the nearest local density maximum. The mean shift algorithm is robust, and can
identify arbitrarily-shaped clusters in the feature space.

Recently, mean shift is applied in color image segmentation and has proven to be quite successful [1].
The idea is to identify homogeneous image regions through clustering in a properly selected feature
space (such as color, texture, or shape). However, mean shift can be quite expensive due to the large
number of kernels involved in the density estimator. To reduce the computational requirement, we
ﬁrst reduce the density estimator ˆf (x) to a simpler model g (x) using our simpliﬁcation scheme, and
then apply the iterative mean shift procedure on the simpliﬁ ed model g (x).
Experiments are performed on a number of benchmark images1 used in [1]. We use the Gaussian
kernel with bandwidth h = 20. The partition parameter is r = 25. For comparison, we also imple-
ment the standard mean shift and its fast version using kd-trees (using the ANN library [10]). The
codes are written in C++ and run on a 2.26GHz Pentium-III machine. As the “true ” segmentation
of an image is subjective, so only a visual comparison is intended here.

Table 1: Total wall time (in seconds) on various segmentation tasks, and the number of components in g (x).
mean shift
our method
time consumption
# components
kd-tree
standard
0.18
81
11.94
1215.8
0.35
120
12.92
1679.7
1284.5
5.16
159
0.22
3.67
440
85.65
3343.0

data size
60,192 (209×288)
73,386 (243×302)
48,960 (192×255)
262,144 (512×512)

image
squirrel
hand
house
lake

Segmentation results are shown in Figures 4. The rows, from top to bottom, are: the original image,
segmentation results by standard mean shift and our approach. We can see that our results are closer
to those by the standard mean shift (applied on the original density estimator), with the number of
components (Table 1) dramatically smaller than the data size n. This demonstrates the success of
our approximation scheme in maintaining the structure of the data distribution using highly compact
models. Our algorithm is also much faster than the standard mean shift and its fast version using kd-
trees. The reason is that kd-trees only facilitates range searching but does not reduce the expensive
computations associated with the large number of kernels.

4 Conclusion

Finite mixture is a powerful model in many statistical learning problems. However, the large model
size can be a major hindrance in many applications. In this paper, we reduce the model complexity
by ﬁrst grouping the components into compact clusters, and t hen perform local function approxima-
tion that minimizes an upper bound of the approximation error. Our algorithm has low complexity,
and demonstrates more reliable performance compared with methods using KL-based distances.

1

http://www.caip.rutgers.edu/∼comanici/MSPAMI/msPamiResults.html

Figure 4: Image segmentation by standard mean shift (2nd row), and ours (bottom).

References

[1] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 24(5):603–619, 2002.
[2] T. Feder and D. Greene. Optimal algorithms for approximate clustering. In Proceedings of ACM Sympo-
sium on Theory of Computing, pages 434–444, 1988.
[3] K. Fukunaga and L. Hostetler. The estimation of the gradient of a density function, with applications in
pattern recognition. IEEE Transactions on Information Theory, 21:32–40, 1975.
[4] A. Gersho and R.M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Press, Boston,
1992.
[5] J. Goldberger and S. Roweis. Hierarchical clustering of a mixture model. In Advances in Neural Infor-
mation Processing Systems 17, pages 505–512. 2005.
[6] B. Han, D. Comaniciu, Y. Zhu, and L. Davis.
Incremental density approximation and kernel-based
Bayesian ﬁltering for object tracking. In Proceedings of the International Conference on Computer Vision
and Pattern Recognition, pages 638–644, 2004.
[7] A.J. Izenman. Recent developments in nonparametric density estimation. Journal of the American Sta-
tistical Association, 86(413):205–224, 1991.
[8] T. Kanungo, D.M. Mount, N.S. Netanyahu, C.D. Piatko, R. Silverman, and A.Y. Wu. An efﬁcient k-
means clustering algorithm: Analysis and implementation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(7):881–892, 2002.
[9] A.W. Moore. Very fast EM-based mixture model clustering using multiresolution kd-trees. In Advances
in Neural Information Processing Systems 11, pages 543–549, 1998.
[10] D.M. Mount and S. Arya. ANN: A library for approximate nearest neighbor searching. In Proceedings
of Center for Geometric Computing Second Annual Fall Workshop Computational Geometry (available
from http://www.cs.umd.edu/∼mount/ANN), 1997.
[11] E. Parzen. On estimation of a probability density function and mode. Annals of Mathematical Statistics,
33:1065–1075, 1962.
[12] K. Popat and R.W. Picard. Cluster-based probability model and its application to image and texture
processing. IEEE Transactions on Image Processing, 6(2):268–284, 1997.
[13] E.B. Sudderth, A. Torralba, W.T. Freeman, and A.S. Willsky. Describing visual scenes using transformed
Dirichlet processes. In Advances in Neural Information Processing Systems 19, 2006.
[14] P. Vincent and Y. Bengio. Manifold Parzen windows.
In Advances in Neural Information Processing
Systems 15, 2003.


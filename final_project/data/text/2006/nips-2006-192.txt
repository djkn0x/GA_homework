A Local Learning Approach for Clustering

Mingrui Wu, Bernhard Sch ¨olkopf
Max Planck Institute for Biological Cybernetics
72076 T ¨ubingen, Germany
{mingrui.wu, bernhard.schoelkopf}@tuebingen.mpg.de

Abstract

We present a local learning approach for clustering. The basic idea is that a good
clustering result should have the property that the cluster label of each data point
can be well predicted based on its neighboring data and their cluster labels, us-
ing current supervised learning methods. An optimization problem is formulated
such that its solution has the above property. Relaxation and eigen-decomposition
are applied to solve this optimization problem. We also brieﬂy investigate the pa-
rameter selection issue and provide a simple parameter selection method for the
proposed algorithm. Experimental results are provided to validate the effective-
ness of the proposed approach.

1 Introduction

In the multi-class clustering problem, we are given n data points, x1 , . . . , xn , and a positive integer
c. The goal is to partition the given data xi (1 ≤ i ≤ n) into c clusters, such that different clusters
are in some sense “distinct” from each other. Here xi ∈ X ⊆ Rd is the input data, X is the input
space.

Clustering has been widely applied for data analysis tasks. It identiﬁes groups of data, such that data
in the same group are similar to each other, while data in different groups are dissimilar. Many clus-
tering algorithms have been proposed, including the traditional k-means algorithm and the currently
very popular spectral clustering approach [3, 10].

Recently the spectral clustering approach has attracted increasing attention due to its promising
performance and easy implementation. In spectral clustering, the eigenvectors of a matrix are used
to reveal the cluster structure in the data. In this paper, we propose a clustering method that also has
this characteristic. But it is based on the local learning idea. Namely, the cluster label of each data
point should be well estimated based on its neighboring data and their cluster labels, using current
supervised learning methods. An optimization problem is formulated whose solution can satisfy this
property. Relaxation and eigen-decomposition are applied to solve this problem. As will be seen
later, the proposed algorithm is also easy to implement while it shows better performance than the
spectral clustering approach in the experiments.

The local learning idea has already been successfully applied in supervised learning problems [1].
This motivates us to incorporate it into clustering, an important unsupervised learning problem.
Adapting valuable supervised learning ideas for unsupervised learning problems can be fruitful. For
example, in [9] the idea of large margin, which has proved effective in supervised learning, is applied
to the clustering problem and good results are obtained.

The remaining of this paper is organized as follows. In section 2, we specify some notation that will
be used in later sections. The details of our local learning based clustering algorithm are presented
in section 3. Experimental results are then provided in section 4, where we also brieﬂy investigate
the parameter selection issue for the proposed algorithm. Finally we conclude the paper in the last
section.

2 Notations

In the following, “neighboring points” or “neighbors” of
xi according to some distance metric.

xi simply refers the nearest neighbors of

n
c
Cl
Ni
ni
Diag(M)

the total number of data.
the number of clusters to be obtained.
the set of points contained in the l-th cluster, 1 ≤ l ≤ c.
the set of neighboring points of xi , 1 ≤ i ≤ n, not including xi itself.
|Ni | , i.e. the number of neighboring points of xi .
the diagonal matrix with the same size and the same diagonal elements as M,
where M is an arbitrary square matrix.

3 Clustering via Local Learning

3.1 Local Learning in Supervised Learning

In supervised learning algorithms, a model is trained with all the labeled training data and is then
used to predict the labels of unseen test data. These algorithms can be called global learning algo-
rithms as the whole training dataset is used for training. In contrast, in local learning algorithms
[1], for a given test data point, a model is built only with its neighboring training data, and then the
label of the given test point is predicted by this locally learned model. It has been reported that local
learning algorithms often outperform global ones [1] as the local models are trained only with the
points that are related to the particular test data. And in [8], it is proposed that locality is a crucial
parameter which can be used for capacity control, in addition to other capacity measures such as the
VC dimension.

3.2 Representation of Clustering Results

The procedure of our clustering approach largely follows that of the clustering algorithms proposed
in [2, 10]. We also use a Partition Matrix (PM) P = [pil ] ∈ {0, 1}n×c to represent a clustering
scheme. Namely pil = 1 if xi (1 ≤ i ≤ n) is assigned to cluster Cl (1 ≤ l ≤ c), otherwise pil = 0.
So in each row of P, there is one and only one element that equals 1, all the others equal 0.
As in [2, 10], instead of computing the PM directly to cluster the given data, we compute a Scaled
Partition Matrix (SPM) F deﬁned by: F = P(P>P)− 1
1/p|Cl |. Clearly we have
2 . (The reason for this will be given later.)
As P>P is diagonal, the l-th (1 ≤ l ≤ c) column of F is just the l-th column of P multiplied by

2 P>P(P>P)− 1
F>F = (P>P)− 1
2 = I
where I is the unit matrix. Given a SPM F, we can easily restore the corresponding PM P with a
mapping P (·) deﬁned as

P = P (F) = Diag(FF> )− 1
2 F
(2)
In the following, we will also express F as: F = [f 1 , . . . , f c ] ∈ Rn×c , where f l = [f l
n ]> ∈
1 , . . . , f l
Rn , 1 ≤ l ≤ c, is the l-th column of F.

(1)

3.3 Basic Idea

The good performance of local learning methods indicates that the label of a data point can be well
estimated based on its neighbors. Based on this, in order to ﬁnd a good SPM F (or equivalently a
cX
nX
cX
(cid:13)(cid:13)f l − ol(cid:13)(cid:13)2
good clustering result), we propose to solve the following optimization problem:
i=1
l=1
l=1
F is a scaled partition matrix

i − ol
(f l
i (xi ))2 =

min
F∈Rn×c

subject to

(3)

(4)

i (·) denotes the output function of a Kernel Machine (KM), trained with some supervised
where ol
kernel learning algorithms [5], using the training data {(xj , f l
j )}xj ∈Ni , where f l
j is used as the label
n (xn )]> ∈ Rn . Details on how to compute
of xj for training this KM. In (3), ol = [ol
1 (x1 ), . . . , ol
i (·), the superscript l indicates that it is for the l-th
i (xi ) will be given later. For the function ol
ol
cluster, and the subscript i means the KM is trained with the neighbors of xi . Hence apart from xi ,
the training data {(xj , f l
j )}xj ∈Ni also inﬂuence the value of ol
j (xj ∈ Ni ) are also
i (xi ). Note that f l
variables of the problem (3)–(4).

To explain the idea behind problem (3)–(4), let us consider the following problem:
Problem 1. For a data point xi and a cluster Cl , given the values of f l
j at xj ∈ Ni , what should be
i at xi ?
the proper value of f l

In particular, we can build a KM with the
This problem can be solved by supervised learning.
j )}xj ∈Ni . As mentioned before, let ol
training data {(xj , f l
i (·) denote the output function of this
locally learned KM, then the good performance of local learning methods mentioned above implies
i (xi ) is probably a good guess of f l
i (xi ).
that ol
i should be similar as ol
i , or the proper f l
Therefore, a good SPM F should have the following property: For any xi (1 ≤ i ≤ n) and any
cluster Cl (1 ≤ l ≤ c), the value of f l
i can be well estimated based on the neighbors of xi . That is,
i should be similar to the output of the KM that is trained locally with the data {(xj , f l
j )}xj ∈Ni .
f l
This suggests that in order to ﬁnd a good SPM F, we can solve the optimization problem (3)–(4).
We can also explain our approach intuitively as follows. A good clustering method will put the
data into well separated clusters. This implies that it is easy to predict the cluster membership of a
point based on its neighbors. If, on the other hand, a cluster is split in the middle, then there will be
points at the boundary for which it is hard to predict which cluster they belong to. So minimizing
the objective function (3) favors the clustering schemes that do not split the same group of data into
different clusters.

Moreover, it is very difﬁcult to construct local clustering algorithms in the same way as for su-
pervised learning. In [1], a local learning algorithm is obtained by running a standard supervised
algorithm on a local training set. This does not transfer to clustering. Rather than simply applying
a given clustering algorithm locally and facing the difﬁculty to combine the local solution into a
global one, problem (3)–(4) seeks a global solution with the property that locally for each point,
its cluster assignment looks like the solution that we would obtain by local learning if we knew the
cluster assignment of its neighbors.

i (xi )
3.4 Computing ol

Having explained the basic idea, now we have to make the problem (3)–(4) more speciﬁc to build
a concrete clustering algorithm. So we consider, based on xi and {(xj , f l
j )}xj ∈Ni , how to com-
i (xi ) with kernel learning algorithms. It is well known that applying many kernel learning
pute ol
i (xi ) = X
algorithms on {(xj , f l
j )}xj ∈Ni will result in a KM, according to which ol
i (xi ) can be calculated as:
ij K (xi , xj )
β l
ol
xj ∈Ni
where K : X × X → R is a positive de ﬁnite kernel function [5], and β l
ij are the expansion coef-
β l
ﬁcients. In general, any kernel learning algorithms can be applied to compute the coefﬁcients
ij .
Here we choose the ones that make the problem (3)–(4) easy to solve. To this end, we adopt the
Kernel Ridge Regression (KRR) algorithm [6], with which we can obtain an analytic expression of
j )}xj ∈Ni . Thus for each xi , we need to solve the following KRR training
i (xi ) based on {(xj , f l
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Kiβ l
ol
problem:
i − f l
i )>Kiβ l
λ(β l
i +
min
(6)
i∈Rni
i
βl
(cid:3)>
i ∈ Rni denotes the vector (cid:2)f l
ij ]> for xj ∈ Ni , λ > 0 is
i ∈ Rni is the vector of the expansion coefﬁcients, i.e. β l
i = [β l
where β l
for xj ∈ Ni , and Ki ∈ Rni×ni is
the regularization parameter, f l
j
the kernel matrix over xj ∈ Ni , namely Ki = [K (xu , xv )], for xu , xv ∈ Ni .

(5)

i = (Ki + λI)−1 f l
Solving problem (6) leads to β l
i . Substituting it into (5), we have
i (Ki + λI)−1 f l
i (xi ) = k>
ol
(7)
i
where ki ∈ Rni denotes the vector [K (xi , xj )]> for xj ∈ Ni . Equation (7) can be written as a
linear equation:
i (xi ) = α>
i f l
ol
i
where αi ∈ Rni is computed as
α>
i = k>
i (Ki + λI)−1
(9)
It can be seen that αi is independent of f l
i and the cluster index l, and it is different for different xi .
Note that f l
i is a sub-vector of f l , so equation (8) can be written in a compact form as:
ol = Af l
(10)
where ol and f l are the same as in (3), while the matrix A = [aij ] ∈ Rn×n is constructed as
follows: ∀xi and xj , 1 ≤ i, j ≤ n, if xj ∈ Ni , then aij equals the corresponding element of αi
in (9), otherwise aij equals 0. Similar as αi , the matrix A is also independent of f l and the cluster
index l.
cX
cX
(cid:13)(cid:13)f l − Af l(cid:13)(cid:13)2 =
Substituting (10) into (3) results in a more speciﬁc optimization problem:
(f l )>Tf l = trace(F>TF)
l=1
l=1
F is a scaled partition matrix

min
F∈Rn×c

subject to

(11)

(8)

(12)

where

T = (I − A)> (I − A)
Thus, based on the KRR algorithm, we have transformed the objective function (3) into the quadratic
function (11).

(13)

3.5 Relaxation

Following the method in [2, 10], we relax F into the continuous domain and combine the property
(1) into the problem (11)–(12), so as to turn it into a tractable continuous optimization problem:
trace(F>TF)
min
F∈Rn×c
F>F = I
(15)
subject to
Let F? ∈ Rn×c denote the matrix whose columns consist of c eigenvectors corresponding to the c
smallest eigenvalues of the symmetric matrix T. Then it is known that the global optimum of the
above problem is not unique, but a subspace spanned by the columns of F? through orthonormal
matrices [10]:
{F?R : R ∈ Rc×c , R>R = I}
(16)
Now we can see that working on the SPM F allows us to make use of the property (1) to construct a
tractable continuous optimization problem (14)–(15), while working directly on the PM P does not
have this advantage.

(14)

3.6 Discretization: Obtaining the Final Clustering Result

According to [10], to get the ﬁnal clustering result, we need to ﬁnd a true SPM F which is close to
the subspace (16). To this end, we apply the mapping (2) on F? to obtain a matrix P? = P (F? ).
It can be easily proved that for any orthogonal matrix R ∈ Rc×c , we have P (F?R) = P?R. This
equation implies that if there exists an orthogonal matrix R such that F?R is close to a true SPM
F, then P?R should also be near to the corresponding discrete PM P. To ﬁnd such an orthogonal
matrix R and the discrete PM P, we can solve the following optimization problem [10]:
kP − P?Rk2
min
P∈Rn×c ,R∈Rc×c
P ∈ {0, 1}n×c , P1c = 1n
subject to
R>R = I

(18)
(19)

(17)

where 1c and 1n denote the c dimensional and the n dimensional vectors of all 1’s respectively.
Details on how to ﬁnd a local minimum of the above problem can be found in [10]. In [3], a method
using k-means algorithm is proposed to ﬁnd a discrete PM P based on P? . In this paper, we adopt
the approach in [10] to get the ﬁnal clustering result.

3.7 Comparison with Spectral Clustering

Our Local Learning based Clustering Algorithm (LLCA) also uses the eigenvalues of a matrix (T
in (13)) to reveal the cluster structure in the data, therefore it can be regarded as belonging to the
category of spectral clustering approaches.

The matrix whose eigenvectors are used for clustering plays the key role in spectral clustering. In
LLCA, this matrix is computed based on the local learning idea: a clustering result is obtained based
on whether the label of each point can be well estimated base on its neighbors with a well established
supervised learning algorithm. This is different from the graph partitioning based spectral clustering
method. As will be seen later, LLCA and spectral clustering have quite different performance in the
experiments.
LLCA needs one additional step: computing the matrix T in the objective function (14). The re-
maining steps, i.e. computing the eigenvectors of T and discretization (cf. section 3.6) are the same
this is very easy to implement and A can be computed with time complexity O(Pn
as in the spectral clustering approach. According to equation (13), to compute T, we need to com-
pute the matrix A in (10), which in turn requires calculating αi in (9) for each xi . We can see that
i ).
i=1 n3
In practice, just like in the spectral clustering method, the number of neighbors ni is usually set to a
ﬁxed small value k for all xi in LLCA. In this case, A can be computed efﬁciently with complexity
O(nk3 ), which scales linearly with the number of data n. So in this case the main calculation is to
obtain the eigenvectors of T. Furthermore, according to (13), the eigenvectors of T are identical to
the right singular vectors of I − A, which can be calculated efﬁciently because now I − A is sparse,
each row of which contains just k + 1 nonzero elements. Hence in this case, we do not need to
compute T explicitly.
We conclude that LLCA is easy to implement, and in practice, the main computational load is to
compute the eigenvectors of T, therefore the LLCA and the spectral clustering approach have the
same order of time complexity in most practical cases.1

4 Experimental Results

In this section, we empirically compare LLCA with the spectral clustering approach of [10] as well
as with k-means clustering. For the last discretization step of LLCA (cf. section 3.6), we use
the same code contained in the implementation of the spectral clustering algorithm, available at
http://www.cis.upenn.edu/∼jshi/software/.

4.1 Datasets

The following datasets are used in the experiments.
• USPS-3568: The examples of handwritten digits 3, 5, 6 and 8 from the USPS dataset.
• USPS-49: The examples of handwritten digits 4 and 9 from the USPS dataset.
• UMist: This dataset consists of face images of 20 different persons.
• UMist5: The data from the UMist dataset, belonging to class 4, 8, 12, 16 and 20.
1Sometimes we are also interested in a special case: ni = n − 1 for all xi , i.e. all the data are neighboring
to each other. In this case, it can be proved that T = Q>Q, where Q = (Diag(B))−1B with B = I −
K(K + λI)−1 , where K is the kernel matrix over all the data points. So in this case T can be computed with
time complexity O(n3 ). This is the same as computing the eigenvectors of the non-sparse matrix T. Hence the
order of the overall time complexity is not increased by the step of computing T, and the above statements still
hold.

• News4a: The text documents from the 20-newsgroup dataset, covering the topics in rec.∗,
which contains autos, motorcycles, baseball and hockey.
• News4b: The text documents from the 20-newsgroup dataset, covering the topics in sci.∗,
which contains crypt, electronics, med and space.

Further details of these datasets are provided in Table 1.

Table 1: Descriptions of the datasets used in the experiments. For each dataset, the number of data
n, the data dimensionality d and the number of classes c are provided.

Dataset USPS-3568 USPS-49 UMist UMist5 News4a News4b
3874
3840
140
575
1673
3082
n
5652
4989
10304
10304
256
256
d
4
2
20
5
4
4
c

In News4a and New4b, each document is represented by a feature vector, the elements of which are
related to the frequency of occurrence of different words. For these two datasets, we extract a subset
of each of them in the experiments by ignoring the words that occur in 10 or fewer documents and
then removing the documents that have 10 or fewer words. This is why the data dimensionality are
different in these two datasets, although both of them are from the 20-newsgroup dataset.

4.2 Performance Measure

In the experiments, we set the number of clusters equal to the number of classes c for all the clus-
tering algorithms. To evaluate their performance, we compare the clusters generated by these algo-
rithms with the true classes by computing the following two performance measures.

4.2.1 Normalized Mutual Information

N M I (X, Y) =

The Normalized Mutual Information (NMI) [7] is widely used for determining the quality of clusters.
pH (X)H (Y)
For two random variable X and Y , the NMI is deﬁned as [7]:
I (X, Y)
where I (X, Y) is the mutual information between X and Y , while H (X) and H (Y) are the en-
(cid:16) n·nl,h
(cid:17)
tropies of X and Y respectively. One can see that N M I (X, X) = 1, which is the maximal possible
Pc
Pc
value of NMI. Given a clustering result, the NMI in (20) is estimated as [7]:
q(cid:0)Pc
(cid:1)
(cid:1) (cid:0)Pc
h=1 nl,h log
l=1
nl ˆnh
h=1 ˆnh log ˆnh
l=1 nl log nl
n
n
where nl denotes the number of data contained in the cluster Cl (1 ≤ l ≤ c), ˆnh is the number of data
belonging to the h-th class (1 ≤ h ≤ c), and nl,h denotes the number of data that are in the intersec-
tion between the cluster Cl and the h-th class. The value calculated in (21) is used as a performance
measure for the given clustering result. The larger this value, the better the performance.

N M I =

(20)

(21)

4.2.2 Clustering Error

Another performance measure is the Clustering Error. To compute it for a clustering result, we need
to build a permutation mapping function map(·) that maps each cluster index to a true class label.
Pn
The classiﬁcation error based on map(·) can then be computed as:
i=1 δ(yi , map(ci ))
err = 1 −
n
where yi and ci are the true class label and the obtained cluster index of xi respectively, δ(x, y) is
the delta function that equals 1 if x = y and equals 0 otherwise. The clustering error is deﬁned as the
minimal classi ﬁcation error among all possible permutation mappings. This optimal matching can
be found with the Hungarian algorithm [4], which is devised for obtaining the maximal weighted
matching of a bipartite graph.

4.3 Parameter Selection

In the spectral clustering algorithm, ﬁrst a graph of n nodes is constructed, each node of which
corresponds to a data point, then the clustering problem is converted into a graph partition problem.
In the experiments, for the spectral clustering algorithm, a weighted k-nearest neighbor graph is
employed, where k is a parameter searched over the grid: k ∈ {5, 10, 20, 40, 80}. On this graph,
the edge weight between two connected data points is computed with a kernel function, for which
the following two kernel functions are tried respectively in the experiments.

K1 (xi , xj ) =

(22)

The cosine kernel:

and the Gaussian kernel:

x>
i xj
kxi k kxj k
K2 (xi , xj ) = exp(− 1
kxi − xj k2 )
(23)
γ
0 },
The parameter γ in (23) is searched in: γ ∈ {σ2
0 , 16σ2
0 , 8σ2
0 , 4σ2
0 , 2σ2
0 /2, σ2
0 /4, σ2
0 /8, σ2
0 /16, σ2
where σ0 is the mean norm of the given data xi , 1 ≤ i ≤ n.
For LLCA, the cosine function (22) and the Gaussian function (23) are also adopted respectively
as the kernel function in (5). The number of neighbors ni for all xi is set to a single value k . The
parameters k and γ are searched over the same grids as mentioned above. In LLCA, there is another
parameter λ (cf. (6)), which is selected from the grid: λ ∈ {0.1, 1, 1.5}.
Automatic parameter selection for unsupervised learning is still a difﬁcult problem. We propose a
simple parameter selection method for LLCA as follows. For a clustering result obtained with a set
of parameters, which in our case consists of k and λ when the cosine kernel (22) is used, or k , γ
and λ when the Gaussian kernel (23) is used, we compute its corresponding SPM F and then use
the objective value (11) as the evaluation criteria. Namely, the clustering result corresponding to the
smallest objective value is ﬁnally selected for LLCA.

For simplicity, on each dataset, we will just report the best result of spectral clustering. For LLCA,
both the best result (LLCA1) and the one obtained with the above parameter selection method
(LLCA2) will be provided. No parameter selection is needed for the k-means algorithm, since
the number of clusters is given.

4.4 Numerical Results

Numerical results are summarized in Table 2. The results on News4a and News4b datasets show
that different kernels may lead to dramatically different performance for both spectral clustering
and LLCA. For spectral clustering, the results on USPS-3568 are also signiﬁcantly different for
different kernels. It can also be observed that different performance measures may result in different
performance ranks of the clustering algorithms being investigated. This is reﬂected by the results
on USPS-3568 when the cosine kernel is used and the results on News4b when the Gaussian kernel
is used. Despite all these phenomena, we can still see from Table 2 that both LLCA1 and LLCA2
outperform the spectral clustering and the k-means algorithm in most cases.

We can also see that LLCA2 fails to ﬁnd good parameters on News4a and News4b when the Gaus-
sian kernel is used, while in the remaining cases, LLCA2 is either slightly worse than or identical
to LLCA1. And analogous to LLCA1, LLCA2 also improves the results of the spectral clustering
and the k-means algorithm on most datasets. This illustrates that our parameter selection method for
LLCA can work well in many cases, and clearly it still needs improvement.

Finally, it can be seen that the k-means algorithm is worse than spectral clustering, except on USPS-
3568 with respect to the clustering error criteria when the cosine kernel is used for spectral cluster-
ing. This corroborates the advantage of the popular spectral clustering approach over the traditional
k-means algorithm.

5 Conclusion

We have proposed a local learning approach for clustering, where an optimization problem is formu-
lated leading to a solution with the property that the label of each data point can be well estimated

Table 2: Clustering results. Both the normalized mutual information and the clustering error are
provided. Two kernel functions (22) and (23) are tried for both spectral clustering and LLCA. On
each dataset, the best result of the spectral clustering algorithm is reported (Spec-Clst). For LLCA,
both the best result (LLCA1) and the one obtained with the parameter selection method described
before (LLCA2) are provided. In each group, the best results are shown in boldface, the second best
is in italics. Note that the results of k-means algorithm are independent of the kernel function.

NMI,
cosine

NMI,
Gaussian

Error (%),
cosine

Error (%),
Gaussian

Spec-Clst
LLCA1
LLCA2
k-means
Spec-Clst
LLCA1
LLCA2
k-means
Spec-Clst
LLCA1
LLCA2
k-means
Spec-Clst
LLCA1
LLCA2
k-means

USPS-3568 USPS-49 UMist UMist5 News4a News4b
0.7483
0.5765
0.6468
0.8810
0.3608
0.6575
0.8720
0.6241
0.8003
1
0.7587
0.7125
0.7125
0.7587
1
0.6241
0.8720
0.7889
0.0380
0.0800
0.7193
0.6479
0.2352
0.5202
0.1861
0.4039
0.4319
0.8245
0.8773
0.8099
1
0.8377
0.5980
0.8493
0.1776
0.2642
0.8377
1
0.0296
0.0322
0.8467
0.5493
0.0380
0.0800
0.7193
0.6479
0.2352
0.5202
46.26
32.93
21.73
28.26
9.29
16.56
9.65
7.99
0
36.00
8.01
3.57
3.57
8.01
0
7.99
9.65
38.43
74.08
70.62
36.43
56.35
22.30
22.16
42.34
41.74
13.51
5.68
64.71
10.00
53.25
0
33.91
8.43
4.61
47.24
0
74.38
72.97
4.70
9.80
37.22
74.08
70.62
36.43
56.35
22.30
22.16

based on its neighbors. We have also provided a parameter selection method for the proposed clus-
tering algorithm. Experiments show encouraging results. Future work may include improving the
proposed parameter selection method and extending this work to other applications such as image
segmentation.

References

[1] L. Bottou and V. Vapnik. Local learning algorithms. Neural Computation, 4:888–900, 1992.
[2] P. K. Chan, M. D. F. Schlag, and J. Y. Zien. Spectral k-way ratio-cut partitioning and clustering.
IEEE Transactions on Computer-aided Design of Integrated Circuits and Systems, 13:1088–
1096, 1994.
[3] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing
Systems 14, Cambridge, MA, 2002. MIT Press.
[4] C. H. Papadimitriou and K. Steiglitz. Combinatorial Optimization: Algorithms and Complex-
ity. Dover, New York, 1998.
[5] B. Sch ¨olkopf and A. J. Smola. Learning with Kernels. The MIT Press, Cambridge, MA, 2002.
[6] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, Cambridge, UK, 2004.
[7] A. Strehl and J. Ghosh. Cluster ensembles – a knowledge reuse framework for combining
multiple partitions. Journal of Machine Learning Research, 3:583–617, 2002.
[8] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995.
[9] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In L. K. Saul,
Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17. MIT
Press, Cambridge, MA, 2005.
[10] S. X. Yu and J. Shi. Multiclass spectral clustering. In L. D. Raedt and S. Wrobel, editors,
International Conference on Computer Vision. ACM, 2003.


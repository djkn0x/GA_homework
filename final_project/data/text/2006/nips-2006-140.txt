Stability of K -Means Clustering

Alexander Rakhlin
Department of Computer Science
UC Berkeley
Berkeley, CA 94720
rakhlin@cs.berkeley.edu

Andrea Caponnetto
Department of Computer Science
University of Chicago
Chicago, IL 60637
and
D.I.S.I., Universit `a di Genova, Italy
caponnet@uchicago.edu

Abstract

We phrase K -means clustering as an empirical risk minimization procedure over
a class HK and explicitly calculate the covering number for this class. Next,
we show that stability of K -means clustering is characterized by the geometry of
HK with respect to the underlying distribution. We prove that in the case of a
unique global minimizer, the clustering solution is stable with respect to complete
changes of the data, while for the case of multiple minimizers, the change of
Ω(n1/2 ) samples deﬁnes the transition between stability and instab ility. While
for a ﬁnite number of minimizers this result follows from mul
tinomial distribution
estimates, the case of inﬁnite minimizers requires more reﬁ
ned tools. We conclude
by proving that stability of the functions in HK implies stability of the actual
centers of the clusters. Since stability is often used for selecting the number of
clusters in practice, we hope that our analysis serves as a starting point for ﬁnding
theoretically grounded recipes for the choice of K .

1

Introduction

Identi ﬁcation of clusters is the most basic tool for data ana lysis and unsupervised learning. While
people are extremely good at pointing out the relevant structure in the data just by looking at the
2-D plots, learning algorithms struggle to match this performance. Part of the difﬁculty comes from
the absence, in general, of an objective way to assess the clustering quality and to compare two
groupings of the data. Ben-David et al [1, 2, 3] put forward the goal of establishing a Theory of
Clustering. In particular, attempts have been made by [4, 2, 3] to study and theoretically justify the
stability-based approach of evaluating the quality of clustering solutions. Building upon these ideas,
we present a characterization of clustering stability in terms of the geometry of the function class
associated with minimizing the objective function.

To simplify the exposition, we focus on K -means clustering, although the analogous results can be
derived for K -medians and other clustering algorithms which minimize an objective function.
Let us ﬁrst motivate the notion of clustering stability. Whil e for a ﬁxed K , two clustering solutions
can be compared according to the K -means objective function (see the next section), it is not mean-
ingful to compare the value of the objective function for different K . How can one decide, then,
on the value of K ? If we assume that the observed data is distributed independently according to
some unknown distribution, the number of clusters K should correspond to the number of modes
of the associated probability density. Since density estimation is a difﬁcult task, another approach
is needed. A stability-based solution has been used for at least a few decades by practitioners. The
approach stipulates that, for each K in some range, several clustering solutions should be computed
by sub-sampling or perturbing the data. The best value of K is that for which the clustering solutions

are most “similar ”. This rule of thumb is used in practice, al
little theoretical justi ﬁcation in the literature.

though, to our knowledge, there is very

The precise details of data sub-sampling in the method described above differ from one paper to
another. For instance, Ben-Hur et al [5] randomly choose overlapping portions of the data and
evaluate the distance between the resulting clustering solutions on the common samples. Lange et al
[6], on the other hand, divide the sample into disjoint subsets. Similarly, Ben-David et al [3, 2] study
stability with respect to complete change of the data (independent draw). These different approaches
of choosing K prompted us to give a precise characterization of clustering stability with respect to
both complete and partial changes of the data.

It has been noted by [6, 4, 3] that the stability of clustering with respect to complete change of the
data is characterized by the uniqueness of the minimum of the objective function with respect to
the true distribution. Indeed, minimization of the K -means objective function can be phrased as
an empirical risk minimization procedure (see [7]). The stability follows, under some regularity
assumptions, from the convergence of empirical and expected means over a Glivenko-Cantelli class
of functions. We prove stability in the case of a unique minimizer by explicitly computing the
covering number in the next section and noting that the resulting class is VC-type.

We go further in our analysis by considering the other two interesting cases:
ﬁnite and inﬁnite
number of minimizers of the objective function. With the help of a stability result of [8, 9] for
empirical risk minimization, we are able to prove that K -means clustering is stable with respect
to changes of o(√n) samples, where n is the total number of samples. In fact, the rate of Ω(√n)
changes is a sharp transition between stability and instability in these cases.

2 Preliminaries

Let (Z , A, P ) be a probability space with an unknown probability measure P . Let k · k denote
the Euclidean norm. We assume from the outset that the data live in a Euclidean ball in Rm , i.e.
Z ⊆ B2 (0, R) ⊂ Rm for some R > 0 and Z is closed. A partition function C : Z 7→ {1, . . . , K }
assigns to each point Z its “cluster identity”. The goal of clustering is to ﬁnd a goo
d partition based
on the sample Z1 , . . . , Zn of n points, distributed independently according to P . In particular, for
K -means clustering, the quality of C on Z1 , . . . , Zn is measured by the within-point scatter1 (see
[10])

1
2n

1
n

(1)

(2)

W (C ) =

W (C ) =

K
Xi,j :C (Zi )=C (Zj )=k
Xk=1
kZi − Zj k2 .
It is easy to verify that the (scaled) within-point scatter can be rewritten as
K
Xk=1 Xi:C (Zi )=k
kZi − ck k2
where ck is the mean of the k-th cluster based on the assignment C (see Figure 1). We are interested
in the minimizers of the within-point scatter. Such assignments have to map each point to its nearest
cluster center. Since in this case the partition function C is completely determined by the K centers,
we will often abuse the notation by associating C with the set {c1 , . . . , cK }.
The K -means clustering algorithm is an alternating procedure minimizing the within-point scatter
W (C ). The centers {ck }K
k=1 are computed in the ﬁrst step, following by the assignment of each Zi
to its closest center ck ; the procedure is repeated. The algorithm can get trapped in local minima,
and various strategies, such as starting with several random assignments, are employed to overcome
the problem. In this paper, we are not concerned with the algorithmic issues of the minimization
procedure. Rather, we study stability properties of the minimizers of W (C ).
The problem of minimizing W (C ) can be phrased as empirical risk minimization [7] over the func-
tion class
HK = {hA (z ) = kz − aik2 , i = argmin
j∈{1...K } kz − aj k2 : A = {a1 , . . . , aK } ∈ Z K },
1We have scaled the within-point scatter by 1/n if compared to [10].

(3)

kck − Zik2

c1

c2

Figure 1: The clustering objective is to place the centers ck to minimize the sum of squared distances
from points to their closest centers.

hA (z ) =

where the functions are obtained by selecting all possible K centers. Functions hA (z ) in HK can
also be written as
K
Xi=1
kz − ai k2 I (z is closest to ai ),
where ties are broken, for instance, in the order of ai ’s. Hence, functions hA ∈ HK are K parabolas
glued together with centers at a1 , . . . , aK , as shown in Figure 1. With this notation, one can see that
n
1
Xi=1
n
Moreover, if C minimizes the left-hand side, hC has to minimize the right-hand side and vice versa.
Hence, we will interchangeably use C and hC as minimizers of the within-point scatter.
Several recent papers (e.g.
[11]) have addressed the question of ﬁnding the distance metric for
clustering. Fortunately, in our case there are several natural choices. One choice is to measure the
similarity between the centers {ak }K
k=1 and {bk }K
k=1 of clusterings A and B . Another choice is to
measure the Lq (P ) distance between hA and hB for some q ≥ 1. In fact, we show that these two
choices are essentially equivalent.

W (C ) = min
h∈HK

min
C

h(Zi ).

3 Covering Number for HK
The following technical Lemma shows that a covering of the ball B2 (0, R) induces a cover of HK
in the L∞ distance because small shifts of the centers imply small changes of the corresponding
functions in HK .
Lemma 3.1. For any ε > 0,

.

(cid:19)mK
N (HK , L∞ , ε) ≤ (cid:18) 16R2K + ε
ε
(cid:1)m
Proof. It is well-known that a Euclidean ball of radius R in Rm can be covered by N = (cid:0) 4R+δ
δ
balls of radius δ (see Lemma 2.5 in [12]). Let T = {t1 , . . . , tN } be the set of centers of such a
cover. Consider an arbitrary function hA ∈ HK with centers at {a1 , . . . , aK }. By the deﬁnition
of the cover, there exists ti1 ∈ T such that ka1 − ti1 k ≤ δ . Let A1 = {ti1 , a2 , . . . , aK }. Since
Z ⊆ B2 (0, R),
khA − hA1 k∞ ≤ (2R)2 − (2R − δ)2 ≤ 4Rδ.
We iterate through all the ai ’s, replacing them by the members of T . After K steps,
khA − hAK k∞ ≤ 4RK δ
and all centers of AK belong to T . Hence, each function hA ∈ H can be approximated to within
4RK δ by functions with centers in a ﬁnite set T . The upper bound on the number of functions in
(cid:1)mK functions cover HK to within 4RK δ in
HK with centers in T is N K . Hence, N K = (cid:0) 4R+δ
δ
the L∞ norm. The Lemma follows by setting ε = 4RK δ .

4 Geometry of HK and Stability
The above Lemma shows that HK is not too rich, as its covering numbers are polynomial. This is the
ﬁrst important aspect in the study of clustering stability. The second aspect is the geometry of HK
with respect to the measure P . In particular, stability of K -means clustering depends on the number
of functions h ∈ HK with the minimum expectation Eh. Note that the number of minimizers
depends only on P and K , and not on the data. Since Z is closed, the number of minimizers is at
least one. The three important cases are: unique minimum, a ﬁ nite number of minimizers (greater
than one), and an inﬁnite number of minimizers. The ﬁrst case
is the simplest one, and is a good
starting point.
Deﬁnition 4.1. For  > 0 deﬁne
Q
P = {h ∈ HK : Eh ≤ inf
h0∈HK
the set of almost-minimizers of the expected error.

Eh0 + },

Therefore, for any ε, δ > 0

lim
n→∞

Eh −

In the case of a unique minimum of Eh, one can show that the diameter of Qε
P tends to zero as
 → 0.2
Lemma 3.1 implies that the class HK is VC-type. In particular, it is uniform Donsker, as well as
uniform Glivenko-Cantelli. Hence, empirical averages of functions in HK uniformly converge to
their expectations:
h∈HK (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
h(Zi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
P   sup
> ε! = 0.
1
Xi=1
n
h∈HK (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
h(Zi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
> ε! < δ
P   sup
Xi=1
Eh −
for n > nε,δ . Denote by hA the function corresponding to a minimum of W (C ) on Z1 , . . . , Zn .
Suppose hC ∗ = argminh∈HK
Eh, i.e. C ∗ is the best clustering, which can be computed only with
the knowledge of P . Then, with probability at least 1 − δ ,
n
n
1
1
Xi=1
Xi=1
and
n
n
n
n
Xi=1
Xi=1
hA (Zi ) ≤
by the optimality of hA on the data. Combining the above,
EhA ≤ EhC ∗ + 2ε
with probability at least 1 − δ for n > nε,δ . Another way to state the result is
P−→ inf
Eh0 .
EhA
h0∈HK
Assuming the existence of a unique minimizer, i.e. diamL1 (P )Q
P → 0, we obtain
P−→ 0.
khA − hC ∗ kL1 (P )
By triangle inequality, we immediately obtain the following Proposition.

hC ∗ (Zi ) ≤ EhC ∗ + ε

for n > nε,δ . Furthermore,

EhA ≤

hA (Zi ) + ε

1
n

1
n

1
n

hC ∗ (Zi )

2This can be easily proved by contradiction. Let us assume that the diameter does not tend to zero. Then
there is a sequence of functions {h(t)} in Q(t)
P with (t) → 0 such that kh(t)− h∗ kL1 (P ) ≥ ξ for some ξ > 0.
Hence, by the compactness of HK , the sequence {h(t)} has an accumulation point h∗∗ , and by the continuity
Eh0 . Moreover, kh∗ − h∗∗ kL1 ≥ ξ , which contradicts the uniqueness of
of expectation, Eh∗∗ = inf h0∈HK
the minimizer.

Proposition 4.1. Let Z1 , . . . , Zn , Z 0
1 , . . . , Z 0
n be i.i.d. samples. Suppose the clustering A minimizes
W (C ) over the set {Z1 , . . . , Zn } while B is the minimizer over {Z 0
1 , . . . , Z 0
n }. Then
P−→ 0.
khA − hB kL1 (P )
We have shown that in the case of a unique minimizer of the objective function (with respect to the
distribution), two clusterings over independently drawn sets of points become arbitrarily close to
each other with increasing probability as the number of points increases.
If there are ﬁnite (but greater than one) number of minimizer s h ∈ HK of Eh, multinomial distri-
bution estimates tell us that we expect stability with respect to o(√n) changes of points, while no
stability is expected for Ω(√n) changes, as the next example shows.
Example 1. Consider 1-mean minimization over Z = {x1 , x2 }, x1 6= x2 , and P = 1
2 (δx1 + δx2 ). It
is clear that, given the training set Z1 , . . . , Zn , the center of the minimizer of W (C ) is either x1 or
x2 , according to the majority vote over the training set. Since the difference between the number of
points on x1 and x2 is distributed according to a binomial with zero mean and the variance scaling
as n, it is clear that by changing Ω(√n) points from Z1 , . . . , Zn , it is possible to swap the majority
vote with constant probability. Moreover, with probability approaching one, it is not possible to
achieve the swap by a change of o(√n) points. A similar result can be shown for any K -means over
a ﬁnite Z .
The above example shows that, in general, it is not possible to prove closeness of clusterings over
two sets of samples differing on Ω(√n) elements. In fact, this is a sharp threshold. Indeed, by
employing the following Theorem, proven in [8, 9], we can show that even in the case of an inﬁnite
number of minimizers, clusterings over two sets of samples differing on o(√n) elements become
arbitrarily close with increasing probability as the number of samples increases. This result cannot
be deduced from the multinomial estimates, as it relies on the control of ﬂuctuations of empirical
means over a Donsker class. Recall that a class is Donsker if it satis ﬁes a version of the central limit
theorem for function classes.
Theorem 4.1 (Corollary 11 in [9] or Corollary 2 in [8]). Assume that the class of functions F over
Z is uniformly bounded and P -Donsker, for some probability measure P over Z . Let f (S ) and f (T )
be minimizers over F of the empirical averages with respect to the sets S and T of n points i.i.d.
according to P . Then, if |S 4 T | = o(√n), it holds
P−→ 0.
kf (S ) − f (T ) kL1 (P )
We apply the above theorem to HK which is P -Donsker for any P because its covering numbers in
L∞ scale polynomially (see Lemma 3.1). The boundedness condition is implied by the assumption
that Z ⊆ B2 (0, R). We note that if the class HK were richer than P -Donsker, the stability result
would not necessarily hold.
Corollary 4.1. Suppose the clusterings A and B are minimizers of the K -means objective W (C )
over the sets S and T , respectively. Suppose that |S 4 T | = o(√n). Then
P−→ 0.
khA − hB kL1 (P )
The above Corollary holds even if the number of minimizers h ∈ HK of Eh is inﬁnite. This
concludes the analysis of stability of K -means for the three interesting cases: unique minimizer,
ﬁnite number (greater than one) of minimizers, and inﬁnite n
umber of minimizers. We remark that
the distribution P and the number K alone determine which one of the above cases is in evidence.
We have proved that stability of K -means clustering is characterized by the geometry of the class
HK with respect to P . It is evident that the choice of K maximizing stability of clustering aims to
choose K for which there is a unique minimizer. Unfortunately, for “s mall ” n, stability with respect
to a complete change of the data and stability with respect to o(√n) changes are indistinguishable,
making this rule of thumb questionable. Moreover, as noted in [3], small changes of P lead to drastic
changes in the number of minimizers.

5 Stability of the Centers

Intuitively, stability of functions hA with respect to perturbation of the data Z1 , . . . , Zn implies
stability of the centers of the clusters. This intuition is made precise in this section. Let us ﬁrst
deﬁne a notion of distance between centers of two clustering s.

Deﬁnition 5.1.
Suppose {a1 , . . . , aK } and {b1 , . . . , bK } are centers of two clusterings A and B ,
respectively. Deﬁne a distance between these clusterings a s

dmax ({a1 , . . . , aK }, {b1 , . . . , bK }) := max
1≤i≤K

min
1≤j≤K

(kai − bj k + kaj − bi k)

Lemma 5.1. Assume the density of P (with respect to the Lebesgue measure λ over Z ) is bounded
away from 0, i.e. dP > c dλ for some c > 0. Suppose

khA − hB kL1 (P ) ≤ ε.

Then

dmax ({a1 , . . . , aK }, {b1 , . . . , bK }) ≤ (cid:18) ε
cc,m (cid:19)

1
m+2

where cc,m depends only on c and m.

Proof. First, we note that

1≤j≤K kai − bj k, max
min
1≤i≤K

1≤j≤K kaj − bi k(cid:19)
dmax ({a1 , . . . , aK }, {b1 , . . . , bK }) ≤ 2 max (cid:18) max
min
1≤i≤K
Without loss of generality, assume that the maximum on the right-hand side is attained at a1 and
b1 such that b1 is the closest center to a1 out of {b1 , . . . , bK }. Suppose ka1 − b1k = d. Since
dmax ({a1 , . . . , aK }, {b1 , . . . , bK }) ≤ 2d, it is enough to show that d is small (scales as a power of
ε).
Consider B2 (a1 , d/2), a ball of radius d/2 centered at a1 . Since any point z ∈ B2 (a1 , d/2) is closer
to a1 than to b1 , we have

kz − a1k2 ≤ kz − b1 k2 .

Refer to Figure 2 for the pictorial representation of the proof.
Note that bj /∈ B2 (a1 , d/2) for any j ∈ {2 . . . K }. Also note that for any z ∈ Z ,

kz − a1k2 ≥

K
Xi=1

kz − aik2 I (ai is closest to z ) = hA (z ).

( d
2 )2

a1

B (a1 , d/2)

b1

d

Figure 2: To prove Lemma 5.1 it is enough to show that the shaded area is upperbounded by the
L1 (P ) distance between the functions hA and hB and lower-bounded by a power of d. We deduce
that d cannot be large.

Combining all the information, we obtain the following chain of inequalities
khA − hB kL1 (P ) = Z |hA (z ) − hB (z )| dP (z )
≥ ZB2 (a1 ,d/2) |hA (z ) − hB (z )| dP (z )
= ZB2 (a1 ,d/2) (cid:12)(cid:12)hA (z ) − kz − b1k2 (cid:12)(cid:12) dP (z )
= ZB2 (a1 ,d/2) (cid:0)kz − b1k2 − hA (z )(cid:1) dP (z )
K
kz − aik2 I (ai is closest to z )! dP (z )
= ZB2 (a1 ,d/2)  kz − b1k2 −
Xi=1
≥ ZB2 (a1 ,d/2) (cid:0)kz − b1k2 − kz − a1 k2 (cid:1) dP (z )
≥ ZB2 (a1 ,d/2) (cid:0)(d/2)2 − kz − a1 k2 (cid:1) dP (z )
Γ(m/2) Z d/2
2πm/2
(cid:0)(d/2)2 − r2 (cid:1) rm−1dr
≥ c ·
0
2πm/2
2
(d/2)m+2 = cc,m · dm+2 .
m(m + 2)
Γ(m/2)

= c ·

Since, by assumption,

we obtain

khA − hB kL1 (P ) ≤ ε,
1
cc,m (cid:19)
d ≤ (cid:18) ε
m+2

.

From the above lemma, we immediately obtain the following Proposition.

Proposition 5.1. Assume the density of P (with respect to the Lebesgue measure λ over Z ) is
bounded away from 0, i.e. dP > c dλ for some c > 0. Suppose the clusterings A and B are
minimizers of the K -means objective W (C ) over the sets S and T , respectively. Suppose that
|S 4 T | = o(√n). Then

dmax ({a1 , . . . , aK }, {b1 , . . . , bK }) P−→ 0.
Hence, the centers of the minimizers of the within-point scatter are stable with respect to perturba-
tions of o(√n) points. Similar results can be obtained for other procedures which optimize some
function of the data by applying Theorem 4.1.

6 Conclusions

We showed that K -means clustering can be phrased as empirical risk minimization over a class
HK . Furthermore, stability of clustering is determined by the geometry of HK with respect to P .
We proved that in the case of a unique minimizer, K -means is stable with respect to a complete
change of the data, while for multiple minimizers, we still expect stability with respect to o(√n)
changes. The rule for choosing K by maximizing stability can be viewed then as an attempt to select
K such that HK has a unique minimizer with respect to P . Although used in practice, this choice
of K is questionable, especially for small n. We hope that our analysis serves as a starting point for
ﬁnding theoretically grounded recipes for choosing the num ber of clusters.

References

[1] Shai Ben-David. A framework for statistical clustering with a constant time approximation
algorithms for k-median clustering. In COLT, pages 415–426, 2004.
[2] Ulrike von Luxburg and Shai Ben-David. Towards a statistical theory of clustering. PASCAL
Workshop on Statistics and Optimization of Clustering, 2005.
[3] Shai Ben-David, Ulrike von Luxburg, and David Pal. A sober look at clustering stability. In
COLT, 2006.
[4] A. Rakhlin. Stability of clustering methods. NIPS Workshop ”Theoretical Foundations of
Clustering”, December 2005.
[5] A. Ben-Hur, A. Elisseeff, and I. Guyon. A stability based method for discovering structure in
clustered data. In Pasi ﬁc Symposium on Biocomputing , volume 7, pages 6–17, 2002.
[6] T. Lange, M. Braun, V. Roth, and J. Buhmann. Stability-based model selection. In NIPS, 2003.
[7] Joachim M. Buhmann. Empirical risk approximation: An induction principle for unsupervised
learning. Technical Report IAI-TR-98-3, 3, 1998.
[8] A. Caponnetto and A. Rakhlin. Some properties of empirical risk minimization over Donsker
classes. AI Memo 2005-018, Massachusetts Institute of Technology, May 2005.
[9] A. Caponnetto and A. Rakhlin.
Stability properties of empirical risk minimization
over Donsker classes.
Journal of Machine Learning Research. Accepted. Available at
http://cbcl.mit.edu/people/rakhlin/erm.pdf, 2006.
[10] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning
- Data Mining, Inference, and Prediction. Springer, 2002.
[11] Marina Meil ˘a. Comparing clusterings: an axiomatic view. In ICML ’05: Proceedings of the
22nd international conference on Machine learning, pages 577–584, New York, NY, USA,
2005. ACM Press.
[12] S.A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.


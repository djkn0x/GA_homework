Isotonic Conditional Random Fields
and Local Sentiment Flow

Yi Mao
School of Elec. and Computer Engineering
Purdue University - West Lafayette, IN
ymao@ecn.purdue.edu

Guy Lebanon
Department of Statistics, and
School of Elec. and Computer Engineering
Purdue University - West Lafayette, IN
lebanon@stat.purdue.edu

Abstract

We examine the problem of predicting local sentiment (cid:3)ow in documents, and its
application to several areas of text analysis. Formally, the problem is stated as
predicting an ordinal sequence based on a sequence of word sets. In the spirit of
isotonic regression, we develop a variant of conditional random (cid:2)elds that is well-
suited to handle this problem. Using the M ¤obius transform, we express the model
as a simple convex optimization problem. Experiments demonstrate the model and
its applications to sentiment prediction, style analysis, and text summarization.

1

Introduction

The World Wide Web and other textual databases provide a convenient platform for exchanging
opinions. Many documents, such as reviews and blogs, are written with the purpose of conveying a
particular opinion or sentiment. Other documents may not be written with the purpose of conveying
an opinion, but nevertheless they contain one. Opinions, or sentiments, may be considered in several
ways, the simplest of which is varying from positive opinion, through neutral, to negative opinion.
Most of the research in information retrieval has focused on predicting the topic of a document, or
its relevance with respect to a query. Predicting the document’s sentiment would allow matching
the sentiment, as well as the topic, with the user’s interests. It would also assist in document sum-
marization and visualization. Sentiment prediction was (cid:2)rst formulated as a binary classi(cid:2)cation
problem to answer questions such as: (cid:147)What is the review’s polarity, positive or negative?(cid:148) Pang et
al. [1] demonstrated the dif(cid:2)culties in sentiment prediction using solely the empirical rules (a subset
of adjectives), which motivates the use of statistical learning techniques. The task was then re(cid:2)ned
to allow multiple sentiment levels, facilitating the use of standard text categorization techniques [2].
However, sentiment prediction is different from traditional text categorization: (1) in contrast to
the categorical nature of topics, sentiments are ordinal variables; (2) several contradicting opinions
might co-exist, which interact with each other to produce the global document sentiment; (3) context
plays a vital role in determining the sentiment. Indeed, sentiment prediction is a much harder task
than topic classi(cid:2)cation tasks such as Reuters or WebKB and current models achieve lower accuracy.
Rather than using a bag of words multiclass classi(cid:2)er, we model the sequential (cid:3)ow of sentiment
throughout the document using a sequential conditional model. Furthermore, we treat the sentiment
labels as ordinal variables by enforcing monotonicity constraints on the model’s parameters.

2 Local and Global Sentiments

Previous research on sentiment prediction has generally focused on predicting the sentiment of the
entire document. A commonly used application is the task of predicting the number of stars assigned

to a movie, based on a review text. Typically, the problem is considered as standard multiclass
classi(cid:2)cation or regression using the bag of words representation.
In addition to the sentiment of the entire document, which we call global sentiment, we de(cid:2)ne
the concept of local sentiment as the sentiment associated with a particular part of the text. It is
reasonable to assume that the global sentiment of a document is a function of the local sentiment
and that estimating the local sentiment is a key step in predicting the global sentiment. Moreover, the
concept of local sentiment is useful in a wide range of text analysis applications including document
summarization and visualization.
Formally, we view local sentiment as a function on the words in a document taking values in a (cid:2)nite
partially ordered set, or a poset, (O ; (cid:20)). To determine the local sentiment at a particular word, it
is necessary to take context into account. For example, due to context the local sentiment at each
of the following words this is a horrible product is low (in the sense of (O ; (cid:20))). Since
sentences are natural components for segmenting document semantics, we view local sentiment as
a piecewise constant function on sentences. Occasionally we encounter a sentence that violates
this rule and conveys opposing sentiments in two different parts.
In this situation we break the
sentence into two parts and consider them as two sentences. We therefore formalize the problem
as predicting a sequence of sentiments y = (y1 ; : : : ; yn ); yi 2 O based on a sequence of sentences
x = (x1 ; : : : ; xn ).
Modeling the local sentiment is challenging from several aspects. The sentence sequence x is
discrete-time and high-dimensional categorical valued, and the sentiment sequence y is discrete-
time and ordinal valued. Regression models can be applied locally but they ignore the statistical
dependencies across the time domain. Popular sequence models such as HMM or CRF, on the other
hand, typically assume that y is categorical valued. In this paper we demonstrate the prediction of
local sentiment (cid:3)ow using an ordinal version of conditional random (cid:2)elds, and explore the relation
between the local and global sentiment.

3

Isotonic Conditional Random Fields

p(cid:18) (yjx) =

=

p(cid:18) (yjx) =

(cid:18)c;k 2 R (1)

Conditional random (cid:2)elds (CRF) [3] are parametric families of conditional distributions p(cid:18) (yjx) that
correspond to undirected graphical models or Markov random (cid:2)elds
exp (cid:0) Pc2C Pk (cid:18)c;k fc;k (xjc ; yjc ) (cid:1)
= Qc2C (cid:30)c (xjc ; yjc )
p(cid:18) (y; x)
p(cid:18) (x)
Z ((cid:18) ; x)
Z ((cid:18) ; x)
where C is the set of cliques in the graph and xjc and yjc are the restriction of x and y to variables
representing nodes in c 2 C . It is assumed above that the potentials (cid:30)c are exponential functions of
features modulated by decay parameters (cid:30)c (xjc ; yjc ) = exp(Pk (cid:18)c;k fc;k (xjc ; yjc )).
CRF have been mostly applied to sequence annotation, where x is a sequence of words and y is a
sequence of labels annotating the words, for example part-of-speech tags. The standard graphical
structure in this case is a chain structure on y with noisy observations x. In other words, the cliques
are C = ffyi(cid:0)1 ; yi g; fyi ; xi g : i = 1; : : : ; ng (see Figure 1 left) leading to the model
(cid:22)k gk (yi ; xi )!
exp  Xi Xk
(cid:21)k fk (yi(cid:0)1 ; yi ) + Xi Xk
In sequence annotation a standard choice for the feature functions is fh(cid:27);(cid:28) i (yi(cid:0)1 ; yi ) = (cid:14)yi(cid:0)1 ;(cid:27) (cid:14)yi ;(cid:28)
and gh(cid:27);wi (yi ; xi ) = (cid:14)yi ;(cid:27) (cid:14)xi ;w (note that we index the feature functions using pairs rather than k
as in (2)). In our case, since xi are sentences we use instead the slightly modi(cid:2)ed feature functions
gh(cid:27);wi (yi ; xi ) = 1 if yi = (cid:27); w 2 xi and 0 otherwise. Given a set of iid training samples the param-
eters are typically estimated by maximum likelihood or MAP using standard numerical techniques
such as conjugate gradient or quasi-Newton.
Despite the great popularity of CRF in sequence labeling, they are not appropriate for ordinal data
such as sentiments. The ordinal relation is ignored in (2), and in the case of limited training data
the parameter estimates will possess high variance resulting in poor predictive power. We therefore
enforce a set of monotonicity constraints on the parameters that are consistent with the ordinal
structure and domain knowledge. The resulting model is a restricted subset of the CRF (2) and, in
accordance with isotonic regression [4], is named isotonic CRF.

1
Z (x; (cid:18))

(cid:18) = ((cid:21); (cid:22)):

(2)

Since ordinal variables express a progression of some sort, it is natural to expect some of the binary
features in (2) to correlate more strongly with some ordinal values than others.
In such cases,
we should expect the presence of such binary features to increase (or decrease) the conditional
probability in a manner consistent with the ordinal relation. Since the parameters (cid:22) h(cid:27);wi represent
the effectiveness of the appearance of w with respect to increasing the probability of (cid:27) 2 O , they
are natural candidates for monotonicity constraints. More speci(cid:2)cally, for words w 2 M1 that are
identi(cid:2)ed as strongly associated with positive sentiment, we enforce
(cid:27) (cid:20) (cid:27) 0
(3)
=)
(cid:22)h(cid:27);wi (cid:20) (cid:22)h(cid:27) 0 ;wi
8w 2 M1 :
Similarly, for words w 2 M2 identi(cid:2)ed as strongly associated with negative sentiment, we enforce
(cid:27) (cid:20) (cid:27) 0
(4)
=)
(cid:22)h(cid:27);wi (cid:21) (cid:22)h(cid:27) 0 ;wi
8w 2 M2 :

The motivation behind the above restriction is immediate for the non-conditional Markov ran-
dom (cid:2)elds p(cid:18) (x) = Z (cid:0)1 exp(P (cid:18)i fi (x)). Parameters (cid:18)i are intimately tied to model probabilities
through activation of the feature functions fi . In the case of conditional random (cid:2)elds, things get
more complicated due to the dependence of the normalization term on x. The following propositions
motivate the above parameter restriction for the case of linear structure CRF with binary features.
Proposition 1. Let p(yjx) be a linear state-emission chain CRF with binary features f h(cid:27);(cid:28) i , gh(cid:27);wi
as above, and x a sentence sequence for which v 62 xj . Then, denoting x0 = (x1 ; : : : ; xj(cid:0)1 ; xj [
fvg; xj+1 ; : : : ; xn ), we have

8y

p(yjx)
p(yjx0 )

= Ep(y0 jx) (cid:16)e

(cid:22)hy0
j

;vi(cid:0)(cid:22)hyj ;vi (cid:17) :

Proof.
p(yjx)
p(yjx0 )

where

e(Pi P(cid:27);(cid:28) (cid:21)h(cid:27);(cid:28) i fh(cid:27);(cid:28) i (yi(cid:0)1 ;yi )+Pi P(cid:27);w (cid:22)h(cid:27);wi gh(cid:27);wi (yi ;xi ))
Z (x0 )
=
Z (x)
e(Pi P(cid:27);(cid:28) (cid:21)h(cid:27);(cid:28) i fh(cid:27);(cid:28) i (yi(cid:0)1 ;yi )+Pi P(cid:27);w (cid:22)h(cid:27);wi gh(cid:27);wi (yi ;x0
i ))
i ;x0
i )+Pi P(cid:27);w (cid:22)h(cid:27);wi gh(cid:27);wi (y 0
i(cid:0)1 ;y 0
= Py0 e(Pi P(cid:27);(cid:28) (cid:21)h(cid:27);(cid:28) i fh(cid:27);(cid:28) i (y 0
i ))
i ;xi ))
Py0 e(Pi P(cid:27);(cid:28) (cid:21)h(cid:27);(cid:28) i fh(cid:27);(cid:28) i (y 0
i(cid:0)1 ;y 0
i )+Pi P(cid:27);w (cid:22)h(cid:27);wi gh(cid:27);wi (y 0
= Pr2O (cid:11)r e(cid:22)hr;vi
(cid:11)r
e(cid:22)hr;vi(cid:0)(cid:22)hyj ;vi
e(cid:0)(cid:22)hyj ;vi = Xr2O
Pr02O (cid:11)r0
Pr2O (cid:11)r
(cid:22)hy0
;vi(cid:0)(cid:22)hyj ;vi
= Xy0
p(y0 jx)e
j

=

Z (x0 )
Z (x)

e(cid:0)(cid:22)hyj ;vi

e(cid:0)(cid:22)hyj ;vi

(cid:11)r = Xy0 :y 0
j =r

exp  Xi X(cid:27);(cid:28)

i ) + Xi X(cid:27);w
i(cid:0)1 ; y 0
(cid:21)h(cid:27);(cid:28) i fh(cid:27);(cid:28) i (y 0

i ; xi )! :
(cid:22)h(cid:27);wi gh(cid:27);wi (y 0

Note that the speci(cid:2)c linear CRF structure (Figure 1, left) and binary features are essential for
the above result. Proposition 1 connects the probability ratio p(yjx)
p(yjx0 ) to the model parameters
in a relatively simple manner. Together with Proposition 2 below, it motivates the ordering of
f(cid:22)hr;vi : r 2 Og determined by the restrictions (3)-(4) in terms of the ordering of probability
ratios of transformed sequences.
Proposition 2. Let p(yjx); x; x0 be as in Proposition 1. For all label sequences s; t, we have
p(tjx)
p(sjx)
p(tjx0 )
p(sjx0 )

(cid:22)htj ;vi (cid:21) (cid:22)hsj ;vi

=)

(cid:21)

:

(5)

Proof. Since (cid:22)htj ;vi (cid:21) (cid:22)hsj ;vi we have that ez(cid:0)(cid:22)hsj ;vi (cid:0) ez(cid:0)(cid:22)htj ;vi (cid:21) 0 for all z and
(cid:22)hy0
(cid:22)hy0
;vi(cid:0)(cid:22)hsj ;vi (cid:0) e
;vi(cid:0)(cid:22)htj ;vi (cid:17) (cid:21) 0:
Ep(y0 jx) (cid:16)e
j
j
p(sjx0 ) (cid:0) p(tjx)
By Proposition 1 the above expectation is p(sjx)
p(tjx0 ) and Equation (5) follows.

gh(cid:28) ;wi (yi ; xi )

w 2 M1 [ M2

The restriction (3) may thus be interpreted as ensuring that adding a word w 2 M1 to transform x 7!
x0 will increase labeling probabilities associated with (cid:27) no less than with (cid:27) 0 if (cid:27) 0 (cid:20) (cid:27) . Similarly, the
restriction (4) may be interpreted in the opposite way. If these assumptions are correct, it is clear that
they will lead to more accurate parameter estimates and better prediction accuracy. However, even if
assumptions (3)-(4) are incorrect, enforcing them may improve prediction by trading off increased
bias with lower variance.
Conceptually, the parameter estimates for isotonic CRF may be found by maximizing the likeli-
hood or posterior subject to the monotonicity constraints (3)-(4). Since such a maximization is
relatively dif(cid:2)cult for large dimensionality, we propose a re-parameterization that leads to a much
simpler optimization problem. The re-parameterization, in the case of a fully ordered set, is rela-
tively straightforward. In the more general case of a partially ordered set we need the mechanism of
Mo ¤bius inversions on (cid:2)nite partially ordered sets.
We introduce a new set of features fg (cid:3)
h(cid:27);wi : (cid:27) 2 Og for w 2 M1 [ M2 de(cid:2)ned as
h(cid:27);wi (yi ; xi ) = X(cid:28) :(cid:28) (cid:21)(cid:27)
g(cid:3)
h(cid:27);wi : (cid:27) 2 Og. If (O ; (cid:20)) is fully ordered, (cid:22)(cid:3)
and a new set of corresponding parameters f(cid:22)(cid:3)
h(cid:27);wi =
(cid:22)h(cid:27);wi (cid:0) (cid:22)h(cid:27) 0 ;wi , where (cid:27) 0 is the largest element smaller than (cid:27) , or 0 if (cid:27) = min(O). In the more
h(cid:27);wi is the convolution of (cid:22)h(cid:27);wi with the M ¤obius function of the poset (O ; (cid:20)) (see
general case, (cid:22)(cid:3)
[5] for more details). By the M ¤obius inversion theorem [5] we have that (cid:22)(cid:3)
h(cid:27);wi satisfy
(cid:22)h(cid:27);wi = X(cid:28) :(cid:28) (cid:20)(cid:27)
and that P(cid:28) (cid:22)h(cid:28) ;wi gh(cid:28) ;wi = P(cid:28) (cid:22)(cid:3)
h(cid:28) ;wi leading to the re-parameterization of isotonic CRF
h(cid:28) ;wi g(cid:3)
exp   Xi X(cid:27);(cid:28)
1
(cid:21)h(cid:27);(cid:28) i fh(cid:27);(cid:28) i (yi(cid:0)1 ; yi ) + Xi Xw 62M1[M2 X(cid:27)
p(yjx) =
Z (x)
h(cid:27);wi (yi ; xi )!
+ Xi Xw2M1[M2 X(cid:27)
h(cid:27);wi g(cid:3)
(cid:22)(cid:3)
with (cid:22)(cid:3)
h(cid:27);wi (cid:21) 0; w 2 M1 and (cid:22)(cid:3)
h(cid:27);wi (cid:20) 0; w 2 M2 for all (cid:27) > min(O). The re-parameterized
model has the bene(cid:2)t of simple constraints and its maximum likelihood estimates can be obtained
by a trivial adaptation of conjugate gradient or quasi-Newton methods.

(cid:22)h(cid:27);wi gh(cid:27);wi (yi ; xi )

w 2 M1 [ M2

(cid:22)(cid:3)
h(cid:28) ;wi

(6)

3.1 Author Dependent Models

Thus far, we have ignored the dependency of the labeling model p(yjx) on the author, denoted here
by the variable a. We now turn to account for different sentiment-authoring styles by incorporating
this variable into the model. The word emissions yi ! xi in the CRF structure are not expected
to vary much across different authors. The sentiment transitions yi(cid:0)1 ! yi , on the other hand,
typically vary across different authors as a consequence of their individual styles. For example, the
review of an author who sticks to a list of self-ranked evaluation criteria is prone to strong sentiment
variations. In contrast, the review of an author who likes to enumerate pros before he gets to cons
(or vice versa) is likely to exhibit more local homogeneity in sentiment.
Accounting for author-speci(cid:2)c sentiment transition style leads to the graphical model in Figure 1
right. The corresponding author-dependent CRF model
1
e(Pi;a0 P(cid:27);(cid:28) ((cid:21)h(cid:27);(cid:28) i+(cid:21)h(cid:27);(cid:28) ;a0 i )fh(cid:27);(cid:28) ;a0 i (yi(cid:0)1 ;yi ;a)+Pi P(cid:27);w (cid:22)h(cid:27);wi gh(cid:27);wi (yi ;xi ))
Z (x; a)
uses features fh(cid:27);(cid:28) ;a0 i (yi(cid:0)1 ; yi ; a) = fh(cid:27);(cid:28) i (yi(cid:0)1 ; yi )(cid:14)a;a0 and transition parameters that are author-
dependent (cid:21)h(cid:27);(cid:28) ;ai as well as author-independent (cid:21)h(cid:27);(cid:28) i . Setting (cid:21)h(cid:27);(cid:28) ;ai = 0 reduces the model
to the standard CRF model. The author-independent parameters (cid:21)h(cid:27);(cid:28) i allow parameter sharing
across multiple authors in case the training data is too scarce for proper estimation of (cid:21) h(cid:27);(cid:28) ;ai . For
simplicity, the above ideas are described in the context of non-isotonic CRF. However, it is straight-
forward to combine author-speci(cid:2)c models with isotonic restrictions. Experiments demonstrating
author-speci(cid:2)c isotonic models are described in Section 4.3.

p(yjx; a) =

Yi-1

Yi

Yi+1

Yi-1

Yi

Yi+1

a

Xi-1

Xi

Xi+1

Xi-1

Xi

Xi+1

Figure 1: Graphical models corresponding to CRF (left) and author-dependent CRF (right).

3.2 Sentiment Flows as Smooth Curves

The sentence-based de(cid:2)nition of sentiment (cid:3)ow is problematic when we want to (cid:2)t a model (for
example to predict global sentiment) that uses sentiment (cid:3)ows from multiple documents. Different
documents have different number of sentences and it is not clear how to compare them or how to
build a model from a collection of discrete (cid:3)ows of different lengths. We therefore convert the
sentence-based (cid:3)ow to a smooth length-normalized (cid:3)ow that can meaningfully relate to other (cid:3)ows.
We assume from now on that the ordinal set O is realized as a subset of R and that its ordering
coincides with the standard ordering on R. In order to account for different lengths, we consider
the sentiment (cid:3)ow as a function h : [0; 1] ! O (cid:26) R that is piecewise constant on the intervals
[0; l); [l; 2l); : : : ; [(k (cid:0) 1)l; 1] where k is the number of sentences in the document and l = 1=k .
Each of the intervals represents a sentence and the function value on it is its sentiment.
To create a more robust representation we smooth out the discontinuous function by convolving it
with a smoothing kernel. The resulting sentiment (cid:3)ow is a smooth curve f : [0; 1] ! R that can
be easily related or compared to similar sentiment (cid:3)ows of other documents (see Figure 3 for an
example). We can then de(cid:2)ne natural distances between two (cid:3)ows, for example the Lp distance
dp (f1 ; f2 ) = (cid:18)Z 1
jf1 (r) (cid:0) f2 (r)jp dr(cid:19)
0
for use in a k-nearest neighbor model for relating the local sentiment (cid:3)ow to the global sentiment.

1=p

(7)

4 Experiments

To examine the ideas proposed in this paper we implemented isotonic CRF, and the normalization
and smoothing procedure, and experimented with a small dataset of 249 movie reviews, randomly
selected from the Cornell sentence polarity dataset v1.01 , all written by the same author. The code
for isotonic CRF is a modi(cid:2)ed version of the quasi-Newton implementation in the Mallet toolkit. In
order to check the accuracy and bene(cid:2)t of the local sentiment predictor, we hand-labeled the local
sentiments of each of these reviews. We assigned for each sentence one of the following values in
O (cid:26) R: 2 (highly praised), 1 (something good), 0 (objective description), (cid:0)1 (something that needs
improvement) and (cid:0)2 (strong aversion).

4.1 Sentence Level Prediction

To evaluate the prediction quality of the local sentiment we compared the performance of naive
Bayes, SVM (using the default parameters of SVMlight ), CRF and isotonic CRF. Figure 2 displays
the testing accuracy and distance of predicting the sentiment of sentences as a function of the training
data size averaged over 20 cross-validation train-test split.
The dataset presents one particular dif(cid:2)culty where more than 75% of the sentences are labeled
objective (or 0). As a result, the prediction accuracy for objective sentences is over-emphasized. To
correct for this fact, we report our test-set performance over a balanced (equal number of sentences
for different labels) sample of labeled sentences. Note that since there are 5 labels, random guessing
yields a baseline of 0.2 accuracy and guessing 0 always yields a baseline of 1.2 distance.

1Available at http://www.cs.cornell.edu/People/pabo/movie-review-data

balanced testing accuracy

isotonic CRFs
CRFs
SVM
naive Bayes

0.4

0.35

0.3

0.25

1.25

1.2

1.15

1.1

1.05

1

balanced testing distance

isotonic CRFs
CRFs
SVM
naive Bayes

0.2

25

50

75

100

125

150

175

0.95

25

50

75

100

125

150

175

Figure 2: Local sentiment prediction: balanced test results for naive Bayes, SVM, CRF and iso-CRF.

As described in Section 3, for isotonic CRF, we obtained 300 words to enforce monotonicity con-
straints. The 150 words that achieved the highest correlation with the sentiment were chosen for
positivity constraints. Similarly, the 150 words that achieved the lowest correlation were chosen for
negativity constraints. Table 1 displays the top 15 words of the two lists.

memorable
superb
great
perfection
performance
outstanding
considerable wonderfully worth
just
didnt
too
no
i
couldnt
wasnt
uninspired
lacked

enjoyable
enjoyed
beautifully
failed
satire
boring

mood
certain
delightfully
unnecessary
contrived
tended

Table 1: Lists of 15 words with the largest positive (top) and negative (bottom) correlations.

The results in Figure 2 indicate that by incorporating the sequential information, the two versions
of CRF perform consistently better than SVM and naive Bayes. The advantage of setting the mono-
tonicity constraints in CRF is elucidated by the average absolute distance performance criterion
(Figure 2, right). This criterion is based on the observation that in sentiment prediction, the cost of
misprediction is in(cid:3)uenced by the ordinal relation on the labels, rather than the 0-1 error rate.

4.2 Global Sentiment Prediction

We also evaluated the contribution of the local sentiment analysis in helping to predict the global
sentiment of documents. We compared a nearest neighbor classi(cid:2)er for the global sentiment, where
the representation varied from bag of words to smoothed length-normalized local sentiment repre-
sentation (with and without objective sentences). The smoothing kernel was a bounded Gaussian
density (truncated and renormalized) with (cid:27) 2 = 0:2. Figure 3 displays discrete and smoothed local
sentiment labels, and the smoothed sentiment (cid:3)ow predicted by isotonic CRF.
Figure 4 and Table 2 display test-set accuracy of global sentiments as a function of the train set size.
The distance in the nearest neighbor classi(cid:2)er was either L1 or L2 for the bag of words representation
or their continuous version (7) for the smoothed sentiment curve representation. The results indicate
that the classi(cid:2)cation performance of the local sentiment representation is better than the bag of
words representation. In accordance with the conclusion of [6], removing objective sentences (that
correspond to sentiment 0) increased the local sentiment analysis performance by 20.7%. We can
thus conclude that for the purpose of global sentiment prediction, local sentiment (cid:3)ow of the non-
objective sentences holds most of the relevant information. Performing local sentiment analysis on
non-objective sentences improves performance as the model estimates possess lower variance.

4.3 Measuring the rate of sentiment change

We examine the rate of sentiment change as a characterization of the author’s writing style using
the isotonic author-dependent model of Section 3.1. We assume that the CRF process is a discrete

2

(cid:20)

1

0

!1

!2

0

(cid:21)

labels
curve rep of labels
predicted curve rep

(cid:23)

(cid:22)

(cid:24)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 3: Sentiment (cid:3)ow and its smoothed curve representation. The blue circles indicate the labeled
sentiment of each sentence. The blue solid curve and red dashed curve are smoothed representations
of the labeled and predicted sentiment (cid:3)ows. Only non-objective labels are kept in generating the
two curves. The numberings correspond to sentences displayed in Section 4.4.

nearest neighbor classifier with L2
sentiment flow w/o objective
sentiment flow w/ objective
vocabulary

nearest neighbor classifier with L1
sentiment flow w/o objective
sentiment flow w/ objective
vocabulary

0.38

0.36

0.34

0.32

0.3

0.38

0.36

0.34

0.32

0.3

0.28

0.28

25

50

75

100

125

150

175

0.26

25

50

75

100

125

150

175

Figure 4: Accuracy of global sentiment prediction (4-class labeling) as a function of train set size.

sampling of a corresponding continuous time Markov jump process. A consequence of this assump-
tion is that the time T the author stays in sentiment (cid:27) before leaving is modeled by the exponential
distribution p(cid:27) (T > t) = e(cid:0)q(cid:27) (t(cid:0)1) ; t > 1. Here, we assume T > 1 and q(cid:27) is interpreted as the
rate of change of the sentiment (cid:27) 2 O : the larger the value, the more likely the author will switch to
other sentiments in the near future.
To estimate the rate of change q(cid:27) of an author we need to compute p(cid:27) (T > t) based on the marginal
probabilities p(sja) of sentiment sequences s of length l. The probability p(sja) may be approxi-
mated by
p(sja) = Xx
p(xja)p(sjx; a)
(cid:11)i (s1 jx; a) Qi+(l(cid:0)1)
j=i+1 Mj (sj(cid:0)i ; sj(cid:0)i+1 jx; a)(cid:12)i+(l(cid:0)1) (sl jx; a)
n (cid:0) l + 1  Xi
!
~p0 (xja)
(cid:25) Xx
Z (x; a)
jC j Px02C (cid:14)x;x0 for the set C of documents
where ~p0 is the empirical probability function ~p0 (xja) = 1
written by author a of length no less than l. (cid:11); M ; (cid:12) are the forward, transition and backward
probabilities analogous to the dynamic programming method in [3].
Using the model p(sja) we can compute p(cid:27) (T > t) for different authors at integer values of t
which would lead to the quantity q(cid:27) associated with each author. However, since (8) is based on an
approximation, the calculated values of p(cid:27) (T > t) will be noisy resulting in slightly different values
of q(cid:27) for different time points t and cross validation iterations. A linear regression (cid:2)t for q(cid:27) based on
the approximated values of p(cid:27) (T > t) for two authors using 10-fold cross validation is displayed in
Figure 5. The data was the 249 movie reviews from the previous experiments written by one author,
and additional 201 movie reviews from a second author. Interestingly, the author associated with the
red dashed line has a consistent lower q(cid:27) value in all those (cid:2)gures, and thus is considered as more
(cid:147)static(cid:148) and less prone to quick sentiment variations.

(8)

vocabulary
sentiment (cid:3)ow with objective sentences
sentiment (cid:3)ow without objective sentences

L1
0.3095
0.3189
0.3736

L2
0.3068
3.0%
0.3128
20.7% 0.3655

1.95%
19.1%

Table 2: Accuracy results and relative improvement when training size equals 175.

7

6

5

4

3

2

1

1.6808ﬁ 

 ‹1.143

5

4

3

2

1

1.2181ﬁ 

 ‹0.76685

10

8

6

4

2

1.8959ﬁ 

 ‹1.2231

1.8388ﬁ 

 ‹1.3504

2

3

4

0
1

5

2

3

4

0
1

5

2

3

4

5

0
1

2

3

4

5

8

7

6

5

4

3

2

1

0
1

Figure 5: Linear regression (cid:2)t for q(cid:27) , (cid:27) = 2; 1; (cid:0)1; (cid:0)2 (left to right) based on approximated values
of p(cid:27) (T > t) for two different authors. X-axis: time t; Y-axis: negative log-probability of T > t.

4.4 Text Summarization

We demonstrate the potential usage of sentiment (cid:3)ow for text summarization with a very simple
example. The text below shows the result of summarizing the movie review in Figure 3 by keeping
only sentences associated with the start, the end, the top, and the bottom of the predicted sentiment
curve. The number before each sentence relates to the circled number in Figure 3.

1 What makes this (cid:2)lm mesmerizing, is not the plot, but the virtuoso performance of Lucy Berliner (Ally Sheedy), as a wily photographer,
retired from her professional duties for the last ten years and living with a has-been German actress, Greta (Clarkson). 2 The less interesting
story line involves the ambitions of an attractive, baby-faced assistant editor at the magazine, Syd (Radha Mitchell), who lives with a boyfriend
(Mann) in an emotionally chilling relationship. 3 We just lost interest in the characters, the (cid:2)lm began to look like a commercial for a magazine
that wouldn’t stop and get to the main article. 4 Which left the (cid:2)lm only somewhat satisfying; it did create a proper atmosphere for us to view
these lost characters, and it did have something to say about how their lives are being emotionally torn apart. 5 It would have been wiser to
develop more depth for the main characters and show them to be more than the super(cid:2)cial beings they seemed to be on screen.
Alternative schemes for extracting speci(cid:2)c sentences may be used to achieve different effects, de-
pending on the needs of the user. We plan to experiment further in this area by combining local
sentiment (cid:3)ow and standard summarization techniques.

5 Discussion

In this paper, we address the prediction and application of the local sentiment (cid:3)ow concept. As
existing models are inadequate for a variety of reasons, we introduce the isotonic CRF model that is
suited to predict the local sentiment (cid:3)ow. This model achieves better performance than the standard
CRF as well as non-sequential models such as SVM. We also demonstrate the usefulness of the local
sentiment representation for global sentiment prediction, style analysis and text summarization.

References
[1] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? sentiment classi(cid:2)cation using machine learning
techniques. In Proceedings of EMNLP-02.
[2] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect
to rating scales. In Proceedings of ACL-05.
[3] J. Lafferty, F. Pereira, and A. McCallum. Conditional random (cid:2)elds: Probabilistic models for segmenting
and labeling sequence data. In International Conference on Machine Learning, 2001.
[4] R. E. Barlow, D.J. Bartholomew, J. M. Bremner, and H. D. Brunk. Statistical inference under order
restrictions; the theory and application of isotonic regression. Wiley, 1972.
[5] R. P. Stanley. Enumerative Combinatorics. Wadsworth & Brooks/Cole Mathematics Series, 1986.
[6] B. Pang and L. Lee. A sentimental education: Sentiment analysis using subjectivity summarization based
on minimum cuts. In Proceedings of ACL-04.


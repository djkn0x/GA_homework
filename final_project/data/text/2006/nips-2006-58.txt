Near-Uniform Sampling of Combinatorial Spaces
Using XOR Constraints

Carla P. Gomes
Ashish Sabharwal
Bart Selman
Department of Computer Science
Cornell University, Ithaca NY 14853-7501, USA
fgomes,sabhar,selmang@cs.cornell.edu (cid:3)

Abstract
We propose a new technique for sampling the solutions of combinatorial prob-
lems in a near-uniform manner. We focus on problems speci(cid:2)ed as a Boolean for-
mula, i.e., on SAT instances. Sampling for SAT problems has been shown to have
interesting connections with probabilistic reasoning, making practical sampling
algorithms for SAT highly desirable. The best current approaches are based on
Markov Chain Monte Carlo methods, which have some practical limitations. Our
approach exploits combinatorial properties of random parity (XOR) constraints to
prune away solutions near-uniformly. The (cid:2)nal sample is identi(cid:2)ed amongst the
remaining ones using a state-of-the-art SAT solver. The resulting sampling dis-
tribution is provably arbitrarily close to uniform. Our experiments show that our
technique achieves a signi(cid:2)cantly better sampling quality than the best alternative.

1
Introduction
We present a new method, XORSample, for uniformly sampling from the solutions of hard combi-
natorial problems. Although our method is quite general, we focus on problems expressed in the
Boolean Satis(cid:2)ability (SAT) framework. Our work is motivated by the fact that ef(cid:2)cient sampling
for SAT can open up a range of interesting applications in probabilistic reasoning [6, 7, 8, 9, 10, 11].
There has also been a growing interest in combining logical and probabilistic constraints as in the
work of Koller, Russell, Domingos, Bacchus, Halpern, Darwiche, and many others (see e.g. sta-
tistical relational learning and Markov logic networks [1]), and a recently proposed Markov logic
system for this task uses ef(cid:2)cient SAT sampling as its core reasoning mechanism [2].
Typical approaches for sampling from combinatorial spaces are based on Markov Chain Monte
Carlo (MCMC) methods, such as the Metropolis algorithm and simulated annealing [3, 4, 5]. These
methods construct a Markov chain with a prede(cid:2)ned stationary distribution. One can draw samples
from the stationary distribution by running the Markov chain for a suf(cid:2)ciently long time. Unfortu-
nately, on many combinatorial problems, the time taken by the Markov chain to reach its stationary
distribution scales exponentially with the problem size.
MCMC methods can also be used to (cid:2)nd (globally optimal) solutions to combinatorial problems. For
example, simulated annealing (SA) uses the Boltzmann distribution as the stationary distribution.
By lowering the temperature parameter to near zero, the distribution becomes highly concentrated
around the minimum energy states, which correspond to the solutions of the combinatorial problem
under consideration. SA has been successfully applied to a number of combinatorial search prob-
lems. However, many combinatorial problems, especially those with intricate constraint structure,
are beyond the reach of SA and related MCMC methods. Not only does problem structure make
reaching the stationary distribution prohibitively long, even reaching a single (optimal) solution is
often infeasible. Alternative combinatorial search techniques have been developed that are much
more effective at (cid:2)nding solutions. These methods generally exploit clever search space pruning
(cid:3)This work was supported by the Intelligent Information Systems Institute (IISI) at Cornell University
(AFOSR grant F49620-01-1-0076) and DARPA (REAL grant FA8750-04-2-0216).

techniques, which quickly focus the search on small, but promising, parts of the overall combina-
torial space. As a consequence, these techniques tend to be highly biased, and sample the set of
solutions in an extremely non-uniform way. (Many are in fact deterministic and will only return one
particular solution.)
In this paper, we introduce a general probabilistic technique for obtaining near-uniform samples
from the set of all (globally optimal) solutions of combinatorial problems. Our method can use any
state-of-the-art specialized combinatorial solver as a subroutine, without requiring any modi(cid:2)cations
to the solver. The solver can even be deterministic. Most importantly, the quality of our sampling
method is not affected by the possible bias of the underlying specialized solver (cid:151) all we need is
a solver that is good at (cid:2)nding some solution or proving that none exists. We provide theoretical
guarantees for the sampling quality of our approach. We also demonstrate the practical feasibility
of our approach by sampling near-uniformly from instances of hard combinatorial problems.
As mentioned earlier, to make our discussion more concrete, we will discuss our method in the con-
text of SAT. In the SAT problem, we have a set of logical constraints on a set of Boolean (True/False)
variables. The challenge is to (cid:2)nd a setting of the variables such that all logical constraints are sat-
is(cid:2)ed. SAT is the prototypical NP-complete problem, and quite likely the most widely studied
combinatorial problem in computer science. There have been dramatic advances in recent years in
the state-of-the-art of SAT solvers [e.g. 12, 13, 14]. Current solvers are able to solve problems with
millions of variables and constraints. Many practical combinatorial problems can be effectively
translated into SAT. As a consequence, one of the current most successful approaches to solving
hard computational problems, arising in, e.g., hardware and software veri(cid:2)cation and planning and
scheduling, is to (cid:2)rst translate the problem into SAT, and then use a state-of-the-art SAT solver to
(cid:2)nd a solution (or show that it does not exist). As stated above, these specialized solvers derive much
of their power from quickly focusing their search on a very small part of the combinatorial space.
Many SAT solvers are deterministic, but even when the solvers incorporate some randomization,
solutions will be sampled in a highly non-uniform manner.
The central idea behind our approach can be summarized as follows. Assume for simplicity that
our original SAT instance on n Boolean variables has 2s solutions or satisfying assignments. How
can we sample uniformly at random from the set of solutions? We add special randomly generated
logical constraints to our SAT problem. Each random constraint is constructed in such a way that
it rules out any given truth assignment exactly with probability 1=2 . Therefore, in expectation, after
adding s such constraints, we will have a SAT instance with exactly one solution.1 We then use a
SAT solver to (cid:2)nd the remaining satisfying assignment and output this as our (cid:2)rst sample. We can
repeat this process with a new set of s randomly generated constraints and in this way obtain another
random solution. Note that to output each sample, we can use whatever off-the-shelf SAT solver is
available, because all it needs to do is (cid:2)nd the single remaining assignment.2 The randomization in
the added constraints will guarantee that the assignment is selected uniformly at random.
How do we implement this approach? For our added constraints, we use randomly generated par-
ity or (cid:147)exclusive-or(cid:148) (XOR) constraints.
In recent work, we introduced XOR constraints for the
problem of counting the number of solutions using MBound [15]. Although the building blocks of
MBound and XORSample are the same, this work relies much more heavily on the properties of XOR
constraints, namely, pairwise and even 3-wise independence. As we will discuss below, an XOR con-
straint eliminates any given truth assignment with probability 1=2 , and therefore, in expectation, cuts
the set of satisfying assignments in half. For this expected behavior to happen often, the elimina-
tion of each assignment should ideally be fully independent of the elimination of other assignments.
Unfortunately, as far as is known, there are no compact (polynomial size) logical constraints that
can achieve such complete independence. However, XOR constraints guarantee at least pairwise
independence, i.e., if we know that an XOR constraint C eliminates assignment s1 , this provides no
information as to whether C will remove any another assignment s2 . Remarkably, as we will see,
such pairwise independence already leads to near-uniform sampling.
Our sampling approach is inspired by earlier work in computational complexity theory by Valiant
and Vazirani [16], who considered the question whether having one or more assignments affects

1 Of course, we donâ€™t know the true value of s. In practice, we use a binary style search to obtain a rough
estimate. As we will see, our algorithms work correctly even with over- and under-estimates for s.
2 The practical feasibility of our approach exploits the fact that current SAT solvers are very effective in
(cid:2)nding such truth assignments in many real-world domains.

the hardness of combinatorial problems. They showed that, in essence, the number of solutions
should not affect the hardness of the problem instances in the worst case [16]. This was received as
a negative result because it shows that (cid:2)nding a solution to a Unique SAT problem (a SAT instance
that is guaranteed to have at most one solution) is not any easier than (cid:2)nding a solution to an arbitrary
SAT instance. Our sampling strategy turns this line of research into a positive direction by showing
how a standard SAT solver, tailored to (cid:2)nding just one solution of a SAT problem, can now be used
to sample near-uniformly from the set of solutions of an arbitrary SAT problem.
In addition to introducing XORSample and deriving theoretical guarantees on the quality of the sam-
ples it generates, we also provide an empirical validation of our approach. One question that arises
is whether the state-of-the-art SAT solvers will perform well on problem instances with added XOR
(or parity) constraints. Fortunately, as our experiments show, a careful addition of such constraints
does generally not degrade the performance of the solvers. In fact, the addition of XOR constraints
can be bene(cid:2)cial since the constraints lead to additional propagation that can be exploited by the
solvers.3 Our experiments show that we can effectively sample near-uniformly from hard practical
combinatorial problems. In comparison with the best current alternative method on such instances,
our sampling quality is substantially better.
2 Preliminaries
For the rest of this paper, (cid:2)x the set of propositional variables in all formulas to be V , jV j = n. A
variable assignment s : V ! f0; 1g is a function that assigns a value in f0; 1g to each variable in V .
We may think of the value 0 as FA L S E and the value 1 as TRU E . We will often abuse notation and
write s(i) for valuations of entities i 62 V when the intended meaning is either already de(cid:2)ned or is
clear from the context. In particular, s(1) = 1 and s(0) = 0. When s(i) = 1, we say that s satis(cid:2)es
i. For x 2 V , :x denotes the corresponding negated variable; s(:x) = 1 (cid:0) s(x). Let F be a formula
over variables V . s(F ) denotes the valuation of F under s. If s satis(cid:2)es F , i.e., s(F ) = 1, then s
is a model, solution, or satisfying assignment for F . Our goal in this paper is to sample uniformly
from the set of all solutions of a given formula F .
An XOR constraint D over variables V is the logical (cid:147)xor(cid:148) or parity of a subset of V [ f1g; s satis(cid:2)es
D if it satis(cid:2)es an odd number of elements in D. The value 1 allows us to express even parity. For
instance, D = fa; b; c; 1g represents the xor constraint a (cid:8) b (cid:8) c (cid:8) 1, which is TRU E when an even
number of a; b; c are TRU E . Note that it suf(cid:2)ces to use only positive variables. E.g., :a (cid:8) b (cid:8) :c
and :a (cid:8) b are equivalent to D = fa; b; cg and D = fa; b; 1g, respectively. Our focus will be on
formulas which are a logical conjunction of a formula in Conjunctive Normal Form (CNF) and some
XOR constraints. In all our experiments, XOR constraints are translated into CNF using additional
variables so that the full formula can be fed directly to standard (CNF-based) SAT solvers.
We will need basic concepts from linear algebra. Let F2 denote the (cid:2)eld of two elements, 0 and 1,
and Fn
2 the vector space of dimension n over F. An assignment s can be thought of as an element of
Fn
2 . Similarly, an XOR constraint D can be seen as a linear constraint a1 x1 + a2 x2 + : : : + an xn + b = 1,
where ai ; b 2 f0; 1g, + denotes addition modulo 2 for F2 , ai = 1 iff D has variable i, and b = 1 iff
D has the parity constant 1. In this setting, we can talk about linear transformations of Fn
2 as well as
linear independence of s;s0 2 Fn
2 (see standard texts for details). We will use two properties: every
linear transformation maps the all-zeros vector to itself, and there exists a linear transformation that
maps any k linearly independent vectors to any other k linearly independent vectors.
Consider the set X of all XOR constraints over V . Since an XOR constraint is a subset of V [ f1g,
jX j = 2n+1 . Our method requires choosing XOR constraints from X at random. Let X(n; q) denote
the probability distribution over X de(cid:2)ned as follows: select each v 2 V independently at random
with probability q and include the constant 1 independently with probability 1=2 . This produces XORs
of average length nq. In particular, note that every two complementary XOR constraints involving
the same subset of V (e.g., c (cid:8) d and c (cid:8) d (cid:8) 1) are chosen with the same probability irrespective of
q. Such complementary XOR constraints have the simple but useful property that any assignment s
satis(cid:2)es exactly one of them. Finally, when the distribution X(n;1=2 ) is used, every XOR constraint
in X is chosen with probability 2(cid:0)(n+1) .
3 Note that there are certain classes of structured instances based on parity constraints that are designed to
be hard for SAT solvers [17]. Our augmented problem instances appear to behave quite differently from these
specially constructed instances because of the interaction between the constraints in the original instance and
the added random parity constraints.

We will be interested in the random variables which are the sum of indicator random variables:
Y = (cid:229)s Ys. Linearity of expectation says that E [Y ] = (cid:229)s E [Ys]. When various Ys are pairwise
independent, i.e., knowing Ys2 tells us nothing about Ys1 , even variance behaves linearly: Var [Y ] =
(cid:229)s Var [Ys]. We will also need conditional probabilities. Here, for a random event X , linearity of
conditional expectation says that E [Y j X ] = (cid:229)s E [Ys j X ]. Let X = Ys0 . When various Ys are 3-wise
independent, i.e., knowing Ys2 and Ys3 tells us nothing about Ys1 , even conditional variance behaves
linearly: Var (cid:2)Y j Ys0 (cid:3) = (cid:229)s Var (cid:2)Ys j Ys0 (cid:3). This will be key to the analysis of our second algorithm.
3 Sampling using XOR constraints
In this section, we describe and analyze two randomized algorithms, XORSample and XORSampleâ€™,
for sampling solutions of a given Boolean formula F near-uniformly using streamlining with random
XOR constraints. Both algorithms are parameterized by two quantities: a positive integer s and a
real number q 2 (0; 1), where s is the number of XORs added to F and X(n; q) is the distribution
from which they are drawn. These parameters determine the degree of uniformity achieved by the
algorithms, which we formalize as Theorems 1 and 2. The (cid:2)rst algorithm, XORSample, uses a SAT
solver as a subroutine on the randomly streamlined formula. It repeatedly performs the streamlining
process until the resulting formula has a unique solution. When s is chosen appropriately, it takes
XORSample a small number of iterations (on average) to successfully produce a sample. The second
algorithm, XORSampleâ€™, is non-iterative. Here s is chosen to be relatively small so that a moderate
number of solutions survive. XORSampleâ€™ then uses stronger subroutines, namely a SAT model
counter and a model selector, to output one of the surviving solutions uniformly at random.
3.1 XOR-based sampling using SAT solvers: XORSample
Let F be a formula over n variables, and q and s be the parameters of XORSample. The algorithm
works by adding to F , in each iteration, s random XOR constraints Qs drawn independently from the
distribution X(n; q). This generates a streamlined formula F q
s whose solutions (called the surviving
solutions) are a subset of the solutions of F . If there is a unique surviving solution s, XORSample
outputs s and stops. Otherwise, it discards Qs and F q
s , and iterates the process (rejection sampling).
The check for uniqueness of s is done by adding the negation of s as a constraint to F q
s and testing
whether the resulting formula is still satis(cid:2)able. See Algorithm 1 for a full description.
Params: q 2 (0; 1), a positive integer s
Input
: A CNF formula F
Output : A solution of F
begin
it erat ionSuccess f ul   FA L S E
while it erat ionSuccess f ul = FA L S E do
Qs   fs random constraints independently drawn from X(n; q)g
F q
// Add s random X O R constraints to F
s   F [ Qs
result   SATSolve(F q
// Solve using a SAT solver
s )
if result = TRU E then
s   solution returned by SATSolve (F q
s )
F 0   F q
s [ f ï¬‚sg
result 0   SATSolve(F 0)
if result 0 = FA L S E then
it erat ionSuccess f ul = TRU E
// Output s; it is the unique solution of F q
return s
s

// Remove s from the solution set

end

Algorithm 1: XORSample, sampling solutions with XORs using a SAT solver
We now analyze how uniform the samples produced by XORSample are. For the rest of this section,
(cid:2)x q = 1=2 . Let F be satis(cid:2)able and have exactly 2s(cid:3) solutions; s(cid:3) 2 [0; n]. Ideally, we would like
each solution s of F to be sampled with probability 2(cid:0)s(cid:3) . Let pone;s (s) be the probability that
XORSample outputs s in one iteration. This is typically much lower than 2(cid:0)s(cid:3) , which is accounted
for by rejection sampling. Nonetheless, we will show that when s is larger than s(cid:3) , the variation in
pone;s (s) over different s is small. Let ps (s) be the overall probability that XORSample outputs s.
This, we will show, is very close to 2(cid:0)s(cid:3) , where (cid:147)closeness(cid:148) is formalized as being within a factor
of c(a) which approaches 1 very fast. The proof closely follows the argument used by Valiant and

Vazirani [16] in their complexity theory work on unique satis(cid:2)ability. However, we give a different,
non-combinatorial argument for the pairwise independence property of XORs needed in the proof,
relying on linear algebra. This approach is insightful and will come handy in Section 3.2. We
describe the main idea below, deferring details to the full version of the paper.
Lemma 1. Let a > 0; c(a) = 1 (cid:0) 2(cid:0)a; and s = s(cid:3) + a. Then c(a)2(cid:0)s < pone;s (s) (cid:20) 2(cid:0)s .
Proof sketch. We (cid:2)rst prove the upper bound on pone;s (s). Recall that for any two complementary
XORs (e.g. c (cid:8) d and c (cid:8) d (cid:8) 1), s satis(cid:2)es exactly one XOR. Hence, the probability that s satis(cid:2)es
an XOR chosen randomly from the distribution X(n; q) is 1=2 . By independence of the s XORs in Qs
in XORSample, s survives with probability exactly 2(cid:0)s , giving the desired upper bound on pone;s (s).
For the lower bound, we resort to pairwise independence. Let s 6= s0 be two solutions of F . Let
D be an XOR chosen randomly from X(n;1=2 ). We use linear algebra arguments to show that the
probability that s(D) = 1 (i.e., s satis(cid:2)es D) is independent of the probability that s0 (D) = 1.
Recall the interpretation of variable assignments and XOR constraints in the vector space Fn
2 (cf.
Section 2). First suppose that s and s0 are linearly dependent. In Fn
2 , this can happen only if exactly
one of s and s0 is the all-zeros vector. Suppose s = (0; 0; : : : ; 0) and s0 is non-zero. Perform a linear
transformation on Fn
2 so that s0 = (1; 0; : : : ; 0). Let D be the constraint a1 x1 + a2 x2 + : : : + an xn + b =
1. Then, s0 (D) = a1 + b and s(D) = b. Since a1 is chosen uniformly from f0; 1g when D is drawn
from X(n;1=2 ), knowing a1 + b gives us no information about b, proving independence. A similar
argument works when s is non-zero and s0 = (0; 0; : : : ; 0), and also when s and s0 are linearly
independent to begin with. We skip the details.
This proves that s(D) and s0 (D) are independent when D is drawn from X(n;1=2 ). In particular,
Pr [s0 (D) = 1 j s(D) = 1] = 1=2 . This reasoning easily extends to s XORs in Qs and we have that
Pr [s0 (Qs ) = 1 j s(Qs ) = 1] = 2(cid:0)s . Now,
pone;s (s) = Pr (cid:2)s(Qs ) = 1 and for all other solutions s0 of F;s0 (Qs ) = 0(cid:3)
= Pr [s(Qs ) = 1] (cid:1) (cid:0)1 (cid:0) Pr (cid:2)for some solution s0 6= s;s0 (Qs ) = 1 j s(Qs ) = 1(cid:3)(cid:1) :
Evaluating this using the union bound and pairwise independence shows pone;s (s) > c(a) 2(cid:0)s .
Theorem 1. Let F be a formula with 2s(cid:3) solutions. Let a > 0; c(a) = 1 (cid:0) 2(cid:0)a; and s = s(cid:3) +a. For
any solution s of F , the probability ps (s) with which XORSample with parameters q = 1=2 and s
outputs s satis(cid:2)es
1
c(a) 2(cid:0)s(cid:3)
2(cid:0)s(cid:3)
f ps (s)g > c(a) max
< ps (s) <
f ps (s)g :
min
c(a)
s
s
Further, the number of iterations needed to produce one sample has a geometric distribution with
expectation between 2a and 2a=c(a).
Proof. Let (cid:136)p denote the probability that XORSample (cid:2)nds some unique solution in any single it-
eration. pone;s (s), as before, is the probability that s is the unique surviving solution. ps (s), the
overall probability of sampling s, is given by the in(cid:2)nite geometric series
ps (s) = pone;s (s) + (1 (cid:0) (cid:136)p) pone;s (s) + (1 (cid:0) (cid:136)p)2 pone;s (s) + : : :
which sums to pone;s (s)= (cid:136)p. In particular, ps (s) is proportional to pone;s (s).
Lemma 1 says that for any two solutions s1 and s2 of F , pone;s (s1 ) and pone;s (s2 ) are strictly within
a factor of c(a) of each other. By the above discussion, ps (s1 ) and ps (s2 ) must also be strictly
within a factor of c(a) of each other, already proving the min vs. max part of the result. Further,
(cid:229)s ps (s) = 1 because of rejection sampling.
For the (cid:2)rst part of the result, suppose for the sake of contradiction that ps (s0 ) (cid:20) c(a)2(cid:0)s(cid:3) for some
s0 , violating the claimed lower bound. By the above argument, ps (s) is within a factor of c(a) of
ps (s0 ) for every s, and would therefore be at most 2(cid:0)s(cid:3) . This would make (cid:229)s ps (s) strictly less
than one, a contradiction. A similar argument proves the upper bound on p s (s).
Finally, the number of iterations needed to (cid:2)nd a unique solution (thereby successfully producing a
sample) is a geometric random variable with success parameter (cid:136)p = (cid:229)s pone;s (s), and has expected
value 1= (cid:136)p. Using the bounds on pone;s (s) from Lemma 1 and the fact that the unique survival of each
of the 2s(cid:3) solutions s are disjoint events, we have (cid:136)p (cid:20) 2s(cid:3) 2(cid:0)s = 2(cid:0)a and (cid:136)p > 2s(cid:3) c(a)2(cid:0)s = c(a)2(cid:0)a.
This proves the claimed bounds on the expected number of iterations, 1= (cid:136)p.

and

end

// Compute the it h solution
// Sampled successfully!

3.2 XOR-based sampling using model counters and selectors: XORSampleâ€™
We now discuss our second parameterized algorithm, XORSampleâ€™, which also works by adding to
F s random XORs Qs chosen independently from X(n; q). However, now the resulting streamlined
formula F q
is fed to an exact model counting subroutine to compute the number of surviving solu-
s
tions, mc. If mc > 0, XORSampleâ€™ succeeds and outputs the it h surviving solution using a model
selector on F q
s , where i is chosen uniformly from f1; 2; : : : ; mcg. Note that XORSampleâ€™, in contrast
to XORSample, is non-iterative. Also, the model counting and selecting subroutines it uses are more
complex than SAT solvers; these work well in practice only because F q
s is highly streamlined.
Params: q 2 (0; 1), a positive integer s
Input
: A CNF formula F
Output : A solution of F , or Failure
begin
Qs   fs constraints randomly drawn from X(n; p)g
F q
// Add s random X O R constraints to F
s   F [ Qs
// Compute the exact model count of F q
mc   SATModelCount(F q
s )
s
if mc 6= 0 then
i   a random number chosen uniformly from f1; 2; : : : ; mcg
s   SATFindSolution(F q
s ; i)
return s
else return Failure
Algorithm 2: XORSampleâ€™, sampling with XORs using a model counter and selector
The sample-quality analysis of XORSampleâ€™ requires somewhat more complex ideas than that of
XORSample. Let F have 2s(cid:3) solutions as before. We again (cid:2)x q = 1=2 and prove that if the parameter
s is suf(cid:2)ciently smaller than s(cid:3) , the sample-quality is provably good. The proof relies on the fact that
XORs chosen randomly from X(n;1=2 ) act 3-wise independently on different solutions, i.e., knowing
the value of an XOR constraint on two variable assignments does not tell us anything about its value
on a third assignment. We state this as the following lemma, which can be proved by extending the
linear algebra arguments we used in the proof of Lemma 1 (see the full version for details).
Lemma 2 (3-wise independence). Let s1 ;s2 ; and s3 be three distinct assignments to n Boolean
variables. Let D be an XOR constraint chosen at random from X(n;1=2 ). Then for i 2 f0; 1g,
Pr [s1 (D) = i j s2 (D);s3 (D)] = Pr [s1 (D) = i].
Recall the discussion of expectation, variance, pairwise independence, and 3-wise independence in
Section 2. In particular, when a number of random variables are 3-wise independent, the conditional
variance of their sum (conditioned on one of these variables) equals the sum of their individual
conditional variances. We use this to compute bounds on the sampling probability of XORSampleâ€™.
The idea is to show that the number of solutions surviving, given that any (cid:2)xed solution s survives,
is independent of s in expectation and is highly likely to be very close to the expected value. As
a result, the probability with which s is output, which is inversely proportional to the number of
solutions surviving along with s, will be very close to the uniform probability. Here (cid:147)closeness(cid:148) is
one-sided and is measured as being within a factor of c 0 (a) which approaches 1 very quickly.
Theorem 2. Let F be a formula with 2s(cid:3) solutions. Let a > 0 and s = s(cid:3) (cid:0)a. For any solution s of
F , the probability p0s (s) with which XORSampleâ€™ with parameters q = 1=2 and s outputs s satis(cid:2)es
1 (cid:0) 2(cid:0)a=3
(1 + 2(cid:0)a)(1 + 2(cid:0)a=3 )

where c0 (a) =

s (s) > c0 (a) 2(cid:0)s(cid:3)
p0

;

:

Further, XORSampleâ€™ succeeds with probability larger than c 0 (a).
Proof sketch. See the full version for a detailed proof. We begin by setting up a framework for
analyzing the number of surviving solutions after s XORs Qs drawn from X(n;1=2 ) are added to F . Let
Ys0 be the indicator random variable which is 1 iff s0 (Qs ) = 1, i.e., s0 survives Qs . E [Ys0 ] = 2(cid:0)s and
Var [Ys0 ] (cid:20) E [Ys0 ] = 2(cid:0)s . Further, a straightforward generalization of Lemma 2 from a single XOR
constraint D to s independent XORs Qs implies that the random variables Ys0 are 3-wise independent.
The variable mc (see Algorithm 2), which is the number of surviving solutions, equals (cid:229)s0 Ys0 .
Consider the distribution of mc conditioned on the fact that s survives. Using pairwise indepen-
dence, the corresponding conditional expectation can be shows to satisfy: m = E [mc j s(Q s ) = 1] =

=

p0
s (s) = Pr [s(Qs ) = 1]

(cid:0) 1)2(cid:0)s . More interesting, using 3-wise independence, the corresponding conditional vari-
1 + (2s(cid:3)
ation can also be bounded: Var [mc j s(Qs ) = 1] < E [mc j s(Qs ) = 1].
Since s = s(cid:3) (cid:0) a, 2a < m < 1 + 2a. We show that mc conditioned on s(Qs ) = 1 indeed lies very
close to m. Let b (cid:21) 0 be a parameter whose value we will (cid:2)x later. By Chebychevâ€™s inequality,
22b Var [mc j s(Qs ) = 1]
22b
22b
m
2b j s(Qs ) = 1i (cid:20)
Pr hjmc (cid:0) mj (cid:21)
(E [mc j s(Qs ) = 1])2 <
E [mc j s(Qs ) = 1]
m
Therefore, conditioned on s(Qs ) = 1, with probability more than 1 (cid:0) 22b=m, mc lies between (1 (cid:0)
2(cid:0)b)m and (1 + 2(cid:0)b)m. Recall that p0s (s) is the probability that XORSampleâ€™ outputs s.
n
1
(cid:229)
Pr [mc = i j s(Qs ) = 1]
i
i=1
(1 + 2(cid:0)b)m (cid:21) 2(cid:0)s 1 (cid:0) 22b=m
1
(cid:21) 2(cid:0)s Pr hmc (cid:20) (1 + 2(cid:0)b)m j s(Qs ) = 1i
(1 + 2(cid:0)b)m
Simplifying this expression and optimizing it by setting b = a=3 gives the desired bound on p 0s (s).
Lastly, the success probability of XORSampleâ€™ is (cid:229)s p0s (s) > c0 (a).
Remark 1. Theorems 1 and 2 show that both XORSample and XORSampleâ€™ can be used to sample
arbitrarily close to the uniform distribution when q = 1=2 . For example, as the number of XORs
used in XORSample is increased, a increases, the deviation c(a) from the truly uniform sampling
probability p(cid:3) approaches 0 exponentially fast, and we get progressively smaller error bands around
p(cid:3) . However, for any (cid:2)xed a, these algorithms, somewhat counter-intuitively, do not always sample
truly uniformly (see the full version). As a result, we expect to see a (cid:3)uctuation around p(cid:3) , which,
as we proved above, will be exponentially small in a.

4 Empirical validation
To validate our XOR-sampling technique, we consider two kinds of formulas: a random 3-SAT in-
stance generated near the SAT phase transition [18] and a structured instance derived from a logistics
planning domain (data and code available from the authors). We used a complete model counter,
Relsat [12], to (cid:2)nd all solutions of our problem instances. Our random instance with 75 variables
has a total of 48 satisfying assignments, and our logistics formula with 352 variables has 512 sat-
isfying assignments. (We used formulas with a relatively small number of assignments in order to
evaluate the quality of our sampling. Note that we need to draw many samples for each assignment.)
We used XORSample with MiniSat [14] as the underlying SAT solver to generate samples from the
set of solutions of each formula. Each sample took a fraction of a second to generate on a 4GHz pro-
cessor. For comparison, we also ran the best alternative method for sampling from SAT problems,
SampleSAT [19, 2], allowing it roughly the same cumulative runtime as XORSample.
Figure 1 depicts our results. In the left panel, we consider the random SAT instance, generating
200,000 samples total.
In pure uniform sampling, in expectation we have 200; 000=48 (cid:25) 4; 167
samples for each solution. This level is indicated with the solid horizontal line. We see that the
samples produced by XORSample all lie in a narrow band centered around this line. Contrast this
with the results for SampleSAT: SampleSAT does sample quite uniformly from solutions that lie
near each other in Hamming distance but different solution clusters are sampled with different fre-
quencies. This SAT instance has two solution clusters: the (cid:2)rst 32 solutions are sampled around
2; 900 times each, i.e., not frequently enough, whereas the remaining 16 solutions are sampled too
frequently, around 6,700 times each. (Although SampleSAT greatly improves on other sampling
strategies for SAT, the split into disjoint sampling bands appears inherent in the approach.) The
Kullback-Leibler (KL) divergence between the XORSample data and the uniform distribution is
0.002. For SampleSAT the KL-divergence from uniform is 0.085. It is clear that the XORSample
approach leads to much more uniform sampling.
The right panel in Figure 1 gives the results for our structured logistics planning instance. (To im-
prove the readability of the (cid:2)gure, we plot the sample frequency only for every (cid:2)fth assignment.) In
this case, the difference between XORSample and SampleSAT is even more dramatic. SampleSAT
in fact only found 256 of the 512 solutions in a total of 100,000 samples. We also see that one of
these solutions is sampled nearly 60,000 times, whereas many other solutions are sampled less than

 10000

)
e
l
a
c
s
 
g
o
l
(
 
y
c
n
e
u
q
e
r
F
 
e
t
u
l
o
s
b
A

 1000

 0

XORSample
SampleSat
uniform

XORsample
SampleSat
uniform

 100000

 10000

 1000

 100

 10

 1

)
e
l
a
c
s
 
g
o
l
(
 
y
c
n
e
u
q
e
r
F
 
e
t
u
l
o
s
b
A

 10

 200
 300
 20
 30
Solution #
Solution #
Figure 1: Results of XORSample and SampleSAT on a random 3-SAT instance, the left panel, and a logistics
planning problem, the right panel. (See color (cid:2)gures in PDF.)

 100

 500

 400

 40

 50

 0

(cid:2)ve times. The KL divergence from uniform is 4.16. (Technically the KL divergence is in(cid:2)nite, but
we assigned a count of one to the non-sampled solutions.) The expected number of samples for each
assignment is 100; 000=512 (cid:25) 195. The (cid:2)gure also shows that the sample counts from XORSample
all lie around this value; their KL divergence from uniform is 0.013.
These experiments show that XORSample is a promising practical technique (with theoretical guar-
antees) for obtaining near-uniform samples from intricate combinatorial spaces.
References
[1] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62(1-2):107(cid:150)136, 2006.
[2] H. Poon and P. Domingos. Sound and ef(cid:2)cient inference with probabilistic and deterministic dependen-
cies. In 21th AAAI, pages 458(cid:150)463, Boston, MA, July 2006.
[3] N. Madras. Lectures on Monte Carlo methods. In Field Institute Monographs, vol. 16. Amer. Math. Soc.,
2002.
[4] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. Equations of state calculations by
fast computing machines. J. Chem. Phy., 21:1087(cid:150)1092, 1953.
[5] S. Kirkpatrick, D. Gelatt Jr., and M. Vecchi. Optimization by simuleated annealing. Science, 220(4598):
671(cid:150)680, 1983.
[6] D. Roth. On the hardness of approximate reasoning. J. AI, 82(1-2):273(cid:150)302, 1996.
[7] M. L. Littman, S. M. Majercik, and T. Pitassi. Stochastic Boolean satis(cid:2)ability. J. Auto. Reas., 27(3):
251(cid:150)296, 2001.
[8] J. D. Park. MAP complexity results and approximation methods. In 18th UAI, pages 388(cid:150)396, Edmonton,
Canada, August 2002.
[9] A. Darwiche. The quest for ef(cid:2)cient probabilistic inference, July 2005. Invited Talk, IJCAI-05.
[10] T. Sang, P. Beame, and H. A. Kautz. Performing Bayesian inference by weighted model counting. In 20th
AAAI, pages 475(cid:150)482, Pittsburgh, PA, July 2005.
[11] F. Bacchus, S. Dalmao, and T. Pitassi. Algorithms and complexity results for #SAT and Bayesian infer-
ence. In 44nd FOCS, pages 340(cid:150)351, Cambridge, MA, October 2003.
[12] R. J. Bayardo Jr. and R. C. Schrag. Using CSP look-back techniques to solve real-world SAT instances.
In 14th AAAI, pages 203(cid:150)208, Providence, RI, July 1997.
[13] L. Zhang, C. F. Madigan, M. H. Moskewicz, and S. Malik. Ef(cid:2)cient con(cid:3)ict driven learning in a Boolean
satis(cid:2)ability solver. In ICCAD, pages 279(cid:150)285, San Jose, CA, November 2001.
[14] N. E Â·en and N. S Â¤orensson. MiniSat: A SAT solver with con(cid:3)ict-clause minimization. In 8th SAT, St.
Andrews, U.K., June 2005. Poster.
[15] C. P. Gomes, A. Sabharwal, and B. Selman. Model counting: A new strategy for obtaining good bounds.
In 21th AAAI, pages 54(cid:150)61, Boston, MA, July 2006.
[16] L. G. Valiant and V. V. Vazirani. NP is as easy as detecting unique solutions. Theoretical Comput. Sci.,
47(3):85(cid:150)93, 1986.
[17] J. M. Crawford, M. J. Kearns, and R. E. Schapire. The minimal disagreement parity problem as a hard
satis(cid:2)ability problem. Technical report, AT&T Bell Labs., 1994.
[18] D. Achlioptas, A. Naor, and Y. Peres. Rigorous location of phase transitions in hard optimization prob-
lems. Nature, 435:759(cid:150)764, 2005.
[19] W. Wei, J. Erenrich, and B. Selman. Towards ef(cid:2)cient sampling: Exploiting random walk strategies. In
19th AAAI, pages 670(cid:150)676, San Jose, CA, July 2004.


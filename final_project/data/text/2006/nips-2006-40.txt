Differential Entropic Clustering of Multivariate
Gaussians

Inderjit Dhillon
Jason V. Davis
Dept. of Computer Science
University of Texas at Austin
Austin, TX 78712
{jdavis,inderjit}@cs.utexas.edu

Abstract

Gaussian data is pervasive and many learning algorithms (e.g., k-means) model
their inputs as a single sample drawn from a multivariate Gaussian. However, in
many real-life settings, each input object is best described by multiple samples
drawn from a multivariate Gaussian. Such data can arise, for example, in a movie
review database where each movie is rated by several users, or in time-series do-
mains such as sensor networks. Here, each input can be naturally described by
both a mean vector and covariance matrix which parameterize the Gaussian dis-
tribution. In this paper, we consider the problem of clustering such input objects,
each represented as a multivariate Gaussian. We formulate the problem using an
information theoretic approach and draw several interesting theoretical connec-
tions to Bregman divergences and also Bregman matrix divergences. We evaluate
our method across several domains, including synthetic data, sensor network data,
and a statistical debugging application.

1

Introduction

Gaussian data is pervasive in all walks of life and many learning algorithms—e.g.
k-means, principal
components analysis, linear discriminant analysis, etc—model each input object as a
single sample
drawn from a multivariate Gaussian. For example, the k-means algorithm assumes that each input
is a single sample drawn from one of k (unknown) isotropic Gaussians. The goal of k-means can be
viewed as the discovery of the mean of each Gaussian and recovery of the generating distribution of
each input object.

However, in many real-life settings, each input object is naturally represented by multiple samples
drawn from an underlying distribution. For example, a student’s scores in reading, writing, and
arithmetic can be measured at each of four quarters throughout the school year. Alternately, consider
a website where movies are rated on the basis of originality, plot, and acting. Here, several different
users may rate the same movie. Multiple samples are also ubiquitous in time-series data such as
sensor networks, where each sensor device continually monitors its environmental conditions (e.g.
humidity, temperature, or light). Clustering is an important data analysis task used in many of these
applications. For example, clustering sensor network devices has been used for optimizing routing
of the network and also for discovering trends between sensor nodes. If the k-means algorithm
is employed, then only the means of the distributions will be clustered, ignoring all second order
covariance information. Clearly, a better solution is needed.

In this paper, we consider the problem of clustering input objects, each of which can be represented
by a multivariate Gaussian distribution. The “distance” between two Gaussians can be quantiﬁed
in an information theoretic manner, in particular by their differential relative entropy. Interestingly,
the differential relative entropy between two multivariate Gaussians can be expressed as the con-
vex combination of two Bregman divergences—a Mahalanobis distance between mean vectors and

a Burg matrix divergence between the covariance matrices. We develop an EM style clustering
algorithm and show that the optimal cluster parameters can be cheaply determined via a simple,
closed-form solution. Our algorithm is a Bregman-like clustering method that clusters both means
and covariances of the distributions in a uni ﬁed framework.

We evaluate our method across several domains. First, we present results from synthetic data exper-
iments, and show that incorporating second order information can dramatically increase clustering
accuracy. Next, we apply our algorithm to a real-world sensor network dataset comprised of 52
sensor devices that measure temperature, humidity, light, and voltage. Finally, we use our algorithm
as a statistical debugging tool by clustering the behavior of functions in a program across a set of
known software bugs.

2 Preliminaries

exp

− 1
2

We ﬁrst present some essential background material. The multivariate Gaussian distribution is the
multivariate generalization of the standard univariate case. The probability density function (pdf)
of a d-dimensional multivariate Gaussian is parameterized by mean vector µ and positive deﬁnite
(cid:2)
(cid:1)
covariance matrix Σ:
p(x|µ, Σ) =
(x − µ)T Σ−1 (x − µ)
1
2 |Σ| 1
(2π) d
2
where |Σ| is the determinant of Σ.
The Bregman divergence [2] with respect to φ is deﬁned as:
Dφ (x, y) = φ(x) − φ(y) − (x − y)T ∇φ(y),
where φ is a real-valued, strictly convex function deﬁned over a convex set Q = dom(φ) ⊂ R
d such
that φ is differentiable on the relative interior of Q. For example, if φ(x) = xT x, then the resulting
Bregman divergence is the standard squared Euclidean distance. Similarly, if φ(x) = xT AT Ax,
for some arbitrary non-singular matrix A, then the resulting divergence is the Mahalanobis distance
(cid:3)
MS−1 (x, y) = (x − y)T S−1 (x − y), parameterized by the covariance matrix S , S−1 = AT A. Al-
i (xi log xi − xi ), then the resulting divergence is the (unnormalized) relative
ternately, if φ(x) =
entropy. Bregman divergences generalize many properties of squared loss and relative entropy.

,

Bregman divergences can be naturally extended to matrices, as follows:
Dφ (X , Y ) = φ(X ) − φ(Y ) − tr((∇φ(Y ))T (X − Y )),
where X and Y are matrices, φ is a real-valued, strictly convex function deﬁned over matrices,
and tr(A) denotes the trace of A. Consider the function φ(X ) = (cid:3)X (cid:3)2
F . Then the corresponding
Bregman matrix divergence is the squared Frobenius norm, (cid:3)X − Y (cid:3)2
φ(X ) = − (cid:3)
F . The Burg matrix diver-
gence is generated from a function of the eigenvalues λ1 , ..., λd of the positive deﬁnite matrix X :
i log λi = − log |X |, the Burg entropy of the eigenvalues. The resulting Burg matrix
divergence is:
B (X , Y ) = tr(X Y −1 ) − log |X Y −1 | − d.
(1)
As we shall see later, the Burg matrix divergence will arise naturally in our application. Let λ1 , ..., λd
be the eigenvalues of X and v1 , ..., vd the corresponding eigenvectors and let γ1 , ..., γd be the
(cid:4)
(cid:4)
(cid:4)
eigenvalues of Y with eigenvectors w1 , ..., wd . The Burg matrix divergence can also be written as
− d.
i wj )2 −
λi
log λi
(vT
γj
γi

B (X , Y ) =

i

j

i

From the ﬁrst term above, we see that the Burg matrix divergence is a function of the eigenvalues as
well as of the eigenvectors of X and Y .
(cid:5)
The differential entropy of a continuous random variable x with probability density function f is
deﬁned as
h(f ) = −
It can be shown [3] that an n-bit quantization of a continuous random variable with pdf f has
Shannon entropy approximately equal to h(f ) + n. The continuous analog of the discrete relative

f (x) log f (x)dx.

entropy is the differential relative entropy. Given a random variable x with pdf ’s f and g , the
differential relative entropy is deﬁned as
(cid:5)

D(f ||g) =

f (x) log f (x)
g(x) dx.

3 Clustering Multivariate Gaussians via Differential Relative Entropy

Given a set of n multivariate Gaussians parameterized by mean vectors m1 , ..., mn and covariances
S1 , ..., Sn , we seek a disjoint and exhaustive partitioning of these Gaussians into k different clusters,
π1 , ..., πk . Each cluster j can be represented by a multivariate Gaussian parameterized by mean µj
and covariance Σj . Using differential relative entropy as the distance measure between Gaussians,
the problem of clustering may be posed as the minimization (over all clusterings) of
(cid:4)
k(cid:4)
{i:πi=j}

D(p(x|mi , Si )||p(x|µj , Σj )).

(2)

j=1

3.1 Differential Relative Entropy and Multivariate Gaussians

We ﬁrst show that the differential entropy between two multivariate Gaussians can be expressed as
a convex combination of a Mahalanobis distance between means and the Burg matrix divergence
between covariance matrices.
(cid:6)
f log f − f log g = −h(f ) − (cid:6)
Consider two multivariate Gaussians, parameterized by mean vectors m and µ, and covariances S
and Σ, respectively. We ﬁrst note that the differential relative entropy can be expressed as D(f ||g) =
f log g . The ﬁrst term is just the negative differential entropy of
p(x|m, S ), which can be shown [3] to be:

+

log(2π)d |S |.

1
2

(3)

(cid:8)

We now consider the second term:
(cid:5)

h(p(x|m, S )) = d
2
(cid:7)

(cid:5)

(x − µ)T Σ−1 (x − µ) − log(2π) d
2 |Σ| 1
p(x|m, S )
p(x|m, S ) log p(x|µ, Σ) =
− 1
(cid:5)
2
2
= − 1
p(x|m, S )tr(Σ−1 (x − µ)(x − µ)T )
(cid:5)
2
p(x|m, S ) log(2π) d
−
2 |Σ| 1
2
(cid:9)
(cid:11)(cid:12) − 1
(cid:10)
log(2π)d |Σ|
= − 1
(x − µ)(x − µ)T
Σ−1E
(cid:9)
(cid:10)
2 tr
2
= − 1
((x − m) + (m − µ))((x − m) + (m − µ))T
Σ−1E
2 tr
− 1
log(2π)d |Σ|
(cid:9)
(cid:12) − 1
2
log(2π)d |Σ|
Σ−1S + Σ−1 (m − µ)(m − µ)T
= − 1
(cid:9)
(cid:12) − 1
2 tr
2
(m − µ)T Σ−1 (m − µ) − 1
= − 1
log(2π)d |Σ|.
Σ−1S
2 tr
2
2
The expectation above is taken over the distribution p(x|m, S ). The second to last line above
follows from the deﬁnition of Σ = E [(x − m)(x − m)T ] and also from the fact that E [(x −

(cid:11)(cid:12)

1
2

(4)

log(2π)d |Σ|

m)(m − µ)T ] = E [x − m](m − µ)T = 0. Thus, we have
− 1
log(2π)d |S | +
D(p(x|m, S )||p(x|µ, Σ)) = − d
1
2 tr(Σ−1S ) +
2
2
(m − µ)T Σ−1 (m − µ)
1
+
(cid:9)
(cid:12)
2
tr(SΣ−1 ) − log |SΣ−1 | − d
1
2
1
1
2 B (S , Σ) +
2 MΣ−1 (m, µ),
where B (S , Σ) is the Burg matrix divergence and MΣ−1 (m, µ) is the Mahalanobis distance, pa-
rameterized by the covariance matrix Σ.
(cid:3)
We now consider the problem of ﬁnding the optimal representative Gaussian for a set of c Gaussians
with means m1 , ..., mc and covariances S1 , ..., Sc . For non-negative weights α1 , ...αc such that
(cid:4)
i αi = 1, the optimal representative minimizes the cumulative differential relative entropy:
p(x|µ∗
αiD(p(x|mi , Si )||p(x|µ, Σ))
, Σ∗
(cid:1)
(cid:4)
i

(m − µ)T Σ−1 (m − µ)

) = arg min
p(x|µ,Σ)

(6)

(5)

=

=

+

1
2

(cid:2)
1
2 MΣ−1 (mi , µ)

1
2 B (Si , Σ) +

= arg min
p(x|µ,Σ)

αi

i

.

(7)

The second term can be viewed as minimizing the Bregman information with respect to some ﬁxed
the Mahalanobis distance parameterized by some co-
(albeit unknown) Bregman divergence (i.e.
(cid:4)
variance matrix Σ). Consequently, it has a unique minimizer [1] of the form
µ∗

=

αimi .

i
Next, we note that equation (7) is strictly convex in Σ−1 . Thus, we can derive the optimal Σ∗
setting the gradient of (7) with respect to Σ−1 to 0:
n(cid:4)
n(cid:4)
(cid:9)
(cid:12)
αiD(p(x|mi , Si )||p(x|µ, Σ)) =

Si − Σ + (mi − µ∗

)(mi − µ∗

)T

αi

∂
∂Σ−1

(8)

by

.

(9)

i=1

i=1

Setting this to zero yields

Σ∗

=

(cid:4)

(cid:9)

αi

i

Si + (mi − µ∗

)(mi − µ∗

)T

(cid:12)

.

Figure 1 illustrates optimal representatives of two 2-dimensional Gaussians with means marked by
points A and B, and covariances outlined with solid lines. The optimal Gaussian representatives are
denoted with dotted covariances; the representative on the left uses weights, (αA = 2
3 , αB = 1
3 ),
while the representative on the right uses weights (αA = 1
3 , αB = 2
3 ). As we can see from equation
(8), the optimal representative mean is the weighted average of the means of the constituent Gaus-
sians. Interestingly, the optimal covariance turns out to be the average of the constituent covariances
plus rank one updates. These rank-one changes account for the deviations from the individual means
to the representative mean.

3.2 Algorithm

Algorithm 1 presents our clustering algorithm for the case where each Gaussian has equal weight
αi = 1
n . The method works in an EM-style framework. Initially, cluster assignments are chosen
(these can be assigned randomly). The algorithm then proceeds iteratively, until convergence. First,
the mean and covariance parameters for the cluster representative distributions are optimally com-
puted given the cluster assignments. These parameters are updated as shown in (8) and (9). Next,
the cluster assignments π are updated for each input Gaussian. This is done by assigning the ith
Gaussian to the cluster j with representative Gaussian that is closest in differential relative entropy.

6

5

4

3

2

1

0

−1

−2

A

B

0

1

2

3

4

5

6

7

Figure 1: Optimal Gaussian representatives (shown with dotted lines) of two Gaussians centered at A and
B (for two different sets of weights). While the optimal mean of each representative is the average of the
individual means, the optimal covariance is the average of the individual covariances plus rank-one corrections.

Since both of these steps are locally optimal, convergence of the algorithm to a local optima can be
shown. Note that the problem is N P -hard, so convergence to a global optima cannot be guaranteed.

We next consider the running time of Algorithm 1 when the input Gaussians are d-dimensional.
Lines 6 and 9 compute the optimal means and covariances for each cluster, which requires O(nd)
and O(nd2 ) total work, respectively. Line 12 computes the differential relative entropy between each
input Gaussian and each cluster representative Gaussian. As only the arg min over all Σj is needed,
j ) − log |Σ−1
j |.
we can reduce the Burg matrix divergence computation (equation (1)) to tr(SiΣ−1
Once the inverse of each cluster covariance is computed (for a cost of O(kd3 )), the ﬁrst term can
be computed in O(d2 ) time. The second term can similarly be computed once for each cluster for
a total cost of O(kd3 ). Computing the Mahalanobis distance is an O(d2 ) operation. Thus, total
cost of line 12 is O(kd3 + nkd2 ) and the total running time of the algorithm, given τ iterations, is
O(τ kd2 (n + d)).

Algorithm 1 Differential Entropic Clustering of Multivariate Gaussians
1: {m1 , ..., mn } ← means of input Gaussians
2: {S1 , ..., Sn } ← covariance matrices of input Gaussians
3: π ← initial cluster assignments
(cid:3)
4: while not converged do
for j = 1 to k do {update cluster means}
5:
µj ← 1
i:πi=j mi
|{i:πi=j}|
6:
end for
(cid:9)
(cid:3)
7:
for j = 1 to k do {update cluster covariances}
8:
Σj ← 1
Si + (mi − µj )(mi − µj )T
|{i:πi=j}|
9:
i:πi=j
end for
10:
for i = 1 to n do {assign each Gaussian to the closest cluster representative Gaussian}
11:
πi ← argmin1≤j≤k B (Si , Σj ) + MΣj
−1 (mi , µj ) {B is the Burg matrix divergence and
12:
is the Mahalanobis distance parameterized by Σj }
MΣ−1
j
end for
13:
14: end while

(cid:12)

4 Experiments

We now present experimental results for our algorithm across three different domains: a synthetic
dataset, sensor network data, and a statistical debugging application.

4.1 Synthetic Data

Our synthetic datasets consist of a set of 200 objects, each of which consists of 30 samples drawn
from one of k randomly generated d-dimensional multivariate Gaussians. The k Gaussians are

1

0.9

0.8

0.7

0.6

0.5

n
o
i
t
a
m
r
o
f
n
I
 
l
a
u
t
u
M
 
d
e
z
i
l
a
m
r
o
N

Multivariate Gaussian Clustering
K−means

Multivariate Gaussian Clustering
K−means

1.1

1

0.9

0.8

0.7

0.6

0.5

n
o
i
t
a
m
r
o
f
n
I
 
l
a
u
t
u
M
 
d
e
z
i
l
a
m
r
o
N

0.4
2

3

4

5
7
6
Number of Clusters

8

9

10

0.4
4

5

6
8
7
Number of Dimensions

9

10

Figure 2: Clustering quality of synthetic data. Traditional k-means clustering uses only ﬁrst-order infor-
mation (i.e. the mean), whereas our Gaussian clustering algorithm also incorporates second-order covariance
information. Here, we see that our algorithm achieves higher clustering quality for datasets composed of four-
dimensional Gaussians with varied number of clusters (left), as well as for varied dimensionality of the input
Guassians with k = 5 (right).

generated by choosing a mean vector uniformly at random from the unit simplex and randomly
selecting a covariance matrix from the set of matrices with eigenvalues 1, 2, ..., d.
In Figure 2, we compare our algorithm to the k-means algorithm, which clusters each object solely
on the mean of the samples. Accuracy is quanti ﬁed in terms of normalized mutual information
(NMI) between discovered clusters and the true clusters, a standard technique for determining the
quality of clusters. Figure 2 (left) shows the clustering quality as a function of the number of clusters
when the dimensionality of the input Gaussians is ﬁxed ( d = 4). Figure 2 (right) gives clustering
quality for ﬁve clusters across a varying number of dimensions. All results represent averaged NMI
values across 50 experiments. As can be seen in Figure 2, our multivariate Gaussian clustering
algorithm yields signiﬁcantly higher NMI values than k-means for all experiments.

4.2 Sensor Networks

Sensor networks are wireless networks composed of small, low-cost sensors that monitor their sur-
rounding environment. An open question in sensor networks research is how to minimize communi-
cation costs between the sensors and the base station: wireless communication requires a relatively
large amount of power, a limited resource on current sensor devices (which are usually battery pow-
ered).

A recently proposed sensor network system, BBQ [4], reduces communication costs by modelling
sensor network data at each sensor device using a time-varying multivariate Gaussian and trans-
mitting only model parameters to the base station. We apply our multivariate Gaussian clustering
algorithm to cluster sensor devices from the Intel Lab at Berkeley [8]. Clustering has been used in
sensor network applications to determine efﬁcient routing schemes, as well as for discovering trends
between groups of sensor devices. The Intel sensor network consists of 52 working sensors, each
of which monitors ambient temperature, humidity, light levels, and voltage every thirty seconds.
Conditioned on time, the sensor readings can be ﬁt quite well by a multivariate Gaussian.

Figure 3 shows the results of our multivariate Gaussian clustering algorithm applied to this sensor
network data. For each device, we compute the sample mean and covariance from sensor readings
between noon and 2pm each day, for 36 total days. To account for varying scales of measurement,
we normalize all variables to have unit variance. The second cluster (denoted by ‘2’ in ﬁgure 3) has
the largest variance among all clusters: many of the sensors for this cluster are located in high trafﬁc
areas, including the large conference room at the top of the lab, and the smaller tables in the bottom
of the lab. Since the measurements were taken during lunchtime, we expect higher trafﬁc in these
areas. Interestingly, this cluster shows very high co-variation between humidity and voltage. Cluster
one is characterized by high temperatures, which is not surprising, as there are several windows on
the left side of the lab. This window faces west and has an unobstructed view of the ocean. Finally,
cluster three has a moderate level of total variation, with relatively low light levels. The cluster is
primarily located in the center and the right of lab, away from outside windows.

Figure 3: To reduce communication costs in sensor networks, each sensor device may be modelled by a
multivariate Gaussian. The above plot shows the results of applying our algorithm to cluster sensors into three
groups, denoted by labels ‘1’, ‘2’, and ‘3’.

4.3 Statistical Debugging

Leveraging program runtime statistics for the purpose of software debugging has received recent
research attention [12]. Here we apply our algorithm to cluster functional behavior patterns over
software bugs in the LATEX document preparation program. The data is taken from the Navel sys-
tem [7], a system that uses machine learning to provide better error messaging. The dataset contains
four software bugs, each of which is caused by an unsuccessful LATEX compilation (e.g. specifying
an incorrect number of columns in an array environment) with ambiguous or unclear error messages
provided. LATEX has notoriously cryptic error messages for document compilation failures —for ex-
ample, the message “LaTeX Error: There’s no line here to end ” can be caused by numerous problems
in the source document.

Each function in the program’s source is measured by the frequency with which it is called across
each of the four software bugs. We model this distribution as a 4-dimensional multivariate Gaussian,
one dimension for each bug. The distributions are estimated from a set of samples; each sample
corresponds to a single LATEX ﬁle drawn from a set of grant proposals and submitted computer
science research papers. For each ﬁle and for each of the four bugs, the L ATEX compiler is executed
over a slightly modi ﬁed version of the ﬁle that has been changed to exhibit the bug. During program
execution, function counts are measured and recorded. More details can be found in [7].

Clustering these function counts can yield important debugging insight to assist a software engineer
in understanding error dependent program behavior. Figure 4 shows three covariance matrices from
a sample clustering of eight clusters. To capture the dependencies between bugs, we normalize each
input Gaussian to have zero mean and unit variance. Cluster (a) represents functions that are highly
error independent—i.e. the matrix shows high levels of covariation among all pairs of error classes.
Conversely, clusters (b) and (c) show that some functions are highly error dependent. Cluster (b)
shows a high dependency between bugs 1 and 4, while cluster (c) exhibits high covariation between
bugs 1 and 3, and between bugs 2 and 4.
⎡
⎤
⎡
⎤
⎡
⎤
⎢⎣ 1.00
⎢⎣ 1.00
⎢⎣ 1.00
⎥⎦
⎥⎦
⎥⎦
0.94
0.58
0.58
0.94
0.58
0.95
0.91
0.94
0.58

0.94
0.94
0.94
1.00

0.91
0.67
0.68
1.00

0.58
0.95
0.58
1.00
1.00
0.58
0.95
0.58
(c)

0.58
0.95
0.58
1.00

0.94
0.94
0.94
1.00
1.00
0.94
0.94
0.94
(a)

0.58
0.58
0.55
1.00
1.00
0.55
0.67
0.68
(b)

Figure 4: Covariance matrices for three clusters discovered by clustering functional behavior of the LATEX doc-
ument preparation program. Cluster (a) corresponds to functions which are error-independent, while clusters
(b) and (c) represent two groups of functions that exhibit different types of error dependent behavior.

5 Related Work

In this work, we showed that the differential relative entropy between two multivariate Gaussian
distributions can be expressed as a convex combination of the Mahalanobis distance between their

mean vectors and the Burg matrix divergence between their covariances. This is in contrast to
information theoretic clustering [5], where each input is taken to be a probability distribution over
some ﬁnite set. In [5], no parametric form is assumed, and the Kullback-Liebler divergence (i.e.
discrete relative entropy) can be computed directly from the distributions. The differential entropy
between two multivariate Gaussians wass considered in [10] in the context of solving Gaussian
mixture models. Although an algebraic expression for this differential entropy was given in [10], no
connection to the Burg matrix divergence was made there.

Our algorithm is based on the standard expectation-maximization style clustering algorithm [6].
Although the closed-form updates used by our algorithm are similar to those employed by a Bregman
clustering algorithm [1], we note that the computation of the optimal covariance matrix (equation
(9)) involves the optimal mean vector.
In [9], the problem of clustering Gaussians with respect to the symmetric differential relative entropy,
D(f ||g) + D(g ||f ) is considered in the context of learning HMM parameters for speech recognition.
The resulting algorithm, however, is much more computationally expensive than ours; whereas in
our method, the optimal means and covariance parameters can be computed via a simple closed form
solution. In [9], no such solution is presented and an iterative method must instead be employed. The
problem of ﬁnding the optimal Gaussian with respect to the ﬁrst argument (note that equation (6)
is minimized with respect to the second argument) is considered in [11] for the problem of speaker
interpolation. Here, only one source is assumed, and thus clustering is not needed.

6 Conclusions

We have presented a new algorithm for the problem of clustering multivariate Gaussian distributions.
Our algorithm is derived in an information theoretic context, which leads to interesting connections
with the differential entropy between multivariate Gaussians, and Bregman divergences. Unlike
existing clustering algorithms, our algorithm optimizes both ﬁrst and second order information in
the data. We have demonstrated the use of our method on sensor network data and a statistical
debugging application.

References

In Siam

[1] A. Banerjee, S. Merugu, I. Dhillon, and S. Ghosh. Clustering with Bregman divergences.
International Conference on Data Mining, pages 234–245, 2004.
[2] L. Bregman. The relaxation method ﬁnding the common point of convex sets and its application to
the solutions of problems in convex programming. In USSR Comp. of Mathematics and Mathematical
Physics, volume 7, pages 200–217, 1967.
[3] T. M. Cover and J. A. Thomas. Elements of information theory. Wiley Series in Telecommunications,
1991.
[4] A. Deshpande, C. Guestrin, S. Madden, J. Hellerstein, and W. Hong. Model-based approximate querying
in sensor networks. In International Journal of Very Large Data Bases, 2005.
[5] I. Dhillon, S. Mallela, and R. Kumar. A divisive information-theoretic feature clustering algorithm for
text classi ﬁcation. In Journal of Machine Learning Research, volume 3, pages 1265–1287, 2003.
[6] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation . John Wiley and Sons, Inc., 2001.
[7] J. Ha, H. Ramadan, J. Davis, C. Rossbach, I. Roy, and E. Witchel. Navel: Automating software support
by classifying program behavior. Technical Report TR-06-11, University of Texas at Austin, 2006.
[8] S. Madden. Intel lab data. http://berkeley.intel-research.net/labdata, 2004.
[9] T. Myrvoll and F. Soong. On divergence based clustering of normal distributions and its application to
HMM adaptation. In Eurospeech, pages 1517–1520, 2003.
[10] Y. Singer and M. Warmuth. Batch and on-line parameter estimation of Gaussian mixtures based on the
joint entropy. In Neural Information Processing Systems, 1998.
[11] T. Yoshimura, T. Masuko, K. Tokuda, T. Kobayashi, and T. Kitamura. Speaker interpolation in HMM-
based speech synthesis. In European Conference on Speech Communication and Technology, 1997.
[12] A. Zheng, M. Jordan, B. Liblit, and A. Aiken. Statistical debugging of sampled programs. In Neural
Information Processing Systems, 2004.


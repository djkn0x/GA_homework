Doubly Stochastic Normalization for Spectral
Clustering

Ron Zass

and Amnon Shashua ∗

Abstract

In this paper we focus on the issue of normalization of the afﬁnity matrix in spec-
tral clustering. We show that the difference between N-cuts and Ratio-cuts is
in the error measure being used (relative-entropy versus L1 norm) in ﬁnding the
closest doubly-stochastic matrix to the input afﬁnity matrix. We then develop a
scheme for ﬁnding the optimal, under Frobenius norm, doubly-stochastic approxi-
mation using Von-Neumann’s successive projections lemma. The new normaliza-
tion scheme is simple and efﬁcient and provides superior clustering performance
over many of the standardized tests.

1 Introduction

The problem of partitioning data points into a number of distinct sets, known as the clustering
problem, is central in data analysis and machine learning. Typically, a graph-theoretic approach to
clustering starts with a measure of pairwise afﬁnity Kij measuring the degree of similarity between
points xi , xj , followed by a normalization step, followed by the extraction of the leading eigenvectors
which form an embedded coordinate system from which the partitioning is readily available. In this
domain there are three principle dimensions which make a successful clustering: (i) the afﬁnity
measure, (ii) the normalization of the afﬁnity matrix, and (iii) the particular clustering algorithm.
Common practice indicates that the former two are largely responsible for the performance whereas
the particulars of the clustering process itself have a relatively smaller impact on the performance.
In this paper we focus on the normalization of the afﬁnity matrix. We ﬁrst show that the existing
popular methods Ratio-cut (cf. [1]) and Normalized-cut [7] employ an implicit normalization which
corresponds to L1 and Relative Entropy based approximations of the afﬁnity matrix K to a doubly
stochastic matrix. We then introduce a Frobenius norm (L2 ) normalization algorithm based on a
simple successive projections scheme (based on Von-Neumann’s [5] successive projection lemma
for ﬁnding the closest intersection of sub-spaces) which ﬁnds the closest doubly stochastic matrix
under the least-squares error norm. We demonstrate the impact of the various normalization schemes
on a large variety of data sets and show that the new normalization algorithm often induces a signif-
icant performance boost in standardized tests. Taken together, we introduce a new tuning dimension
to clustering algorithms allowing better control of the clustering performance.

2 The Role of Doubly Stochastic Normalization

It has been shown in the past [11, 4] that K-means and spectral clustering are intimately related
where in particular [11] shows that the popular afﬁnity matrix normalization such as employed
by Normalized-cuts is related to a doubly-stochastic constraint induced by K-means. Since this
nj points in cluster ψj and P
background is a key to our work we will brieﬂy introduce the relevant arguments and derivations.
Let xi ∈ RN , i = 1, ..., n, be points arranged in k (mutually exclusive) clusters ψ1 , .., ψk with
j nj = n. Let Kij = κ(xi , xj ) be a symmetric positive-semi-deﬁnite
∗School of Engineering and Computer Science, Hebrew University of Jerusalem, Jerusalem 91904, Israel.

max
ψ1 ,...,ψk

Kr,s ,

(1)

kφ(xi ) − cj k2 ,

min
c1 ,...,ck ψ1 ,...,ψk

afﬁnity function, e.g. Kij = exp−kxi−xj k2 /σ2 . Then, the problem of ﬁnding the cluster assignments
X
kX
by maximizing:
1
nj
(r,s)∈ψj
j=1
X
kX
is equivalent to minimizing the ”kernel K-means” problem:
i∈ψj
(1/nj ) P
j=1
where φ(xi ) is a mapping associated with the kernel κ(xi , xj ) = φ(xi )>φ(xj ) and cj =
φ(xi ) are the class centers. After some algebraic manipulations it can be shown
i∈ψj
that the optimization setup of eqn. 1 is equivalent to the matrix form:
tr(G>KG) s.t G ≥ 0, GG>1 = 1, G>G = I
max
(2)
G
√
nj if i ∈ ψj and zero otherwise, and
where G is the desired assignment matrix with Gij = 1/
1 is a column vector of ones. Note that the feasible set of matrices satisfying the constraints G ≥
0, GG>1 = 1, G>G = I are of this form for some partitioning ψ1 , ..., ψk . Note also that the matrix
F = GG> must be doubly stochastic (F is non-negative, symmetric and F 1 = 1).
ble to the input matrix K (in the sense that P
Taken together, we see that the desire is to ﬁnd a doubly-stochastic matrix F as close as possi-
ij Fij Kij is maximized over all feasible F ), such
that the symmetric decomposition F = GG> satisﬁes non-negativity (G ≥ 0) and orthonormality
constraints (G>G = I ).
To see the connection with spectral clustering, and N-cuts in particular, relax the non-negativity
condition of eqn. 2 and deﬁne a two-stage approach: ﬁnd the closest doubly stochastic matrix K 0 to
K and we are left with a spectral decomposition problem:
tr(G>K 0G) s.t G>G = I
max
(3)
G
where G contains the leading k eigenvectors of K 0 . We will refer to the process of transforming K to
K 0 as a normalization step. In N-cuts, the normalization takes the form K 0 = D−1/2KD−1/2 where
D = diag(K 1) (a diagonal matrix containing the row sums of K ) [9]. In [11] it was shown that
repeating the N-cuts normalization, i.e., setting up the iterative step K (t+1) = D−1/2K (t)D−1/2
where D = diag(K (t) 1) and K (0) = K converges to a doubly-stochastic matrix (a symmetric
version of the well known ”iterative proportional ﬁtting procedure” [8]).
The conclusion of this brief background is to highlight the motivation for seeking a doubly-stochastic
approximation to the input afﬁnity matrix as part of the clustering process. The open issue is under
what error measure is the approximation to take place? It is not difﬁcult to show that repeating
the N-cuts normalization converges to the global optimum under the relative entropy measure (see
Appendix). Noting that spectral clustering optimizes the Frobenius norm it seems less natural to
have the normalization step optimize a relative entropy error measure.
We will derive in this paper the normalization under the L1 norm and under the Frobenius norm. The
purpose of the L1 norm is to show that the resulting scheme is equivalent to a ratio-cut clustering
— thereby not introducing a new clustering scheme but only contributing to the uniﬁcation and
better understanding the differences between the N-cuts and Ratio-cuts schemes. The Frobenius
norm normalization is a new formulation and is based on a simple iterative scheme. The resulting
normalization provides a new clustering performance which proves quite practical and boosts the
clustering performance in many of the standardized tests we conducted.

3 Ratio-cut and the L1 Normalization
Given that our desire is to ﬁnd a doubly stochastic approximation K 0 to the input afﬁnity matrix K ,
we begin with the L1 norm approximation:

Proposition 1 (ratio-cut) The closest doubly stochastic matrix K 0 under the L1 error norm is
K 0 = K − D + I ,
which leads to the ratio-cut clustering algorithm, i.e., the partitioning of the data set into two clusters
Proof: Let r = minF kK − F k1 s.t. F 1 = 1, F = F > , where kAk1 = P
is determined by the second smallest eigenvector of the Laplacian D − K , where D = diag(K 1).
ij abs(Aij ) is the L1
norm. Since kK − F k1 ≥ k(K − F )1k1 for any matrix F , we must have:
r ≥ k(K − F )1k1 = kD1 − 1k1 = kD − I k1 .
Let F = K − D + I , then

kK − (K − D + I )k1 = kD − I k1 .
If v is an eigenvector of the Laplacian D − K with eigenvalue λ, then v is also an eigenvector of
K 0 = K − D + I with eigenvalue 1 − λ and since (D − K )1 = 0 then the smallest eigenvector
v = 1 of the Laplacian is the largest of K 0 , and the second smallest eigenvector of the Laplacian
(the ratio-cut result) corresponds to the second largest eigenvector of K 0 .
What we have so far is that the difference between N-cuts and Ratio-cuts as two popular spectral
clustering schemes is that the former uses the relative entropy error measure in ﬁnding a doubly
stochastic approximation to K and the latter uses the L1 norm error measure (which turns out to be
the negative Laplacian with an added identity matrix).

4 Normalizing under Frobenius Norm

Given that spectral clustering optimizes the Frobenius norm, there is a strong argument in favor of
ﬁnding a Frobenius-norm optimum doubly stochastic approximation to K . The optimization setup
is that of a quadratic linear programming (QLP). However, the special circumstances of our problem
render the solution to the QLP to consist of a very simple iterative computation, as described next.
The closest doubly-stochastic matrix K 0 under Frobenius norm is the solution to the following QLP:
F = P
K 0 = argminF kK − F k2
F s.t. F ≥ 0, F 1 = 1, F = F > ,
(4)
where kAk2
ij is the Frobenius norm. We deﬁne next two sub-problems, each with a
ij A2
closed-form solution, and have our QLP solution derived by alternating successively between the
two until convergence. Consider the afﬁne sub-problem:
P1 (X ) = argminF kX − F k2
F s.t. F 1 = 1, F = F >
and the convex sub-problem:
P2 (X ) = argminF kX − F k2
F s.t. F ≥ 0
(6)
We will use the Von-Neumann [5] successive projection lemma stating that P1P2P1P2 ...P1 (K ) will
converge onto the projection of K onto the intersection of the afﬁne and conic subspaces described
above1 . Therefore, what remains to show is that the projections P1 and P2 can be solved efﬁciently
(and in closed form).
We begin with the solution for P1 . The Lagrangian corresponding to eqn. 5 takes the form:
L(F , µ1 , µ2 ) = trace(F >F − 2X >F ) − µ>
1 (F 1 − 1) − µ>
2 (F >1 − 1),
where from the condition F = F > we have that µ1 = µ2 = µ. Setting the derivative with respect
to F to zero yields:
F = X + µ1> + 1µ> .

(5)

1 actually, the Von-Neumann lemma applies only to linear subspaces. The extension to convex subspaces
involves a ”deﬂection” component described by Dykstra [3]. However, it is possible to show that for this
speciﬁc problem the deﬂection component is redundant and the Von-Neumann lemma still applies.

(a)

(b)

Figure 1: Running times of the normalization algorithms. (a) the Frobenius scheme compared to a general
Matlab QLP solver, (b) running time of the three normalization schemes.

P1 (X ) = X +

I +

X

11>X.

Isolate µ by multiplying by 1 on both sides: µ = (nI + 11> )−1 (I − X )1. Noting that (nI +
 
!
11> )−1 = (1/n)(I − (1/2n)11> ) we obtain a closed form solution:
1>X 1
I − 1
11> − 1
1
n2
n
n
n
The projection P2 (X ) can also be described in a simple closed form manner. Let I+ be the set of
indices corresponding to non-negative entries of X and I− the set of negative entries of X . The
(Xij − Fij )2 + X
F = X
criterion function kX − F k2
F becomes:
kX − F k2
(i,j )∈I+
(i,j )∈I−
Clearly, the minimum energy over F ≥ 0 is obtained when Fij = Xij for all (i, j ) ∈ I+ and zero
otherwise. Let th≥0 (X ) stand for the operator that zeroes out all negative entries of X . Then,
P2 (X ) = th≥0 (X ).
To conclude, the global optimum of eqn. 4 which returns the closest doubly stochastic matrix K 0 in
Frobenius error norm to the input afﬁnity matrix K is obtained by repeating the following steps:

(Xij − Fij )2 .

(7)

Algorithm 1 (Frobenius-optimal Doubly Stochastic Normalization) ﬁnds the closest doubly
stochastic approximation in Frobenius error norm to a given matrix K (global optimum of eqn. 4).

1. Let X (0) = K .

2. Repeat t = 0, 1, 2, ...

(a) X (t+1) = P1 (X (t) )
(b) If X (t+1) ≥ 0 then stop and set K 0 = X (t+1) , otherwise set X (t+1) = th≥0 (X (t+1) ).
This algorithm is simple and very efﬁcient. Fig. 1a shows the running time of the algorithm com-
pared to an off-the-shelf QLP Matlab solver over random matrices of increasing size — one can see
that the run-time of our algorithm is a fraction of the standard QLP solver and scales very well with
dimension. In fact the standard QLP solver can handle only small problem sizes. In Fig. 1b we plot
the running times of all three normalization schemes: the L1 norm (computing the Laplacian), the
relative-entropy (the iterative D−1/2KD−1/2 ), and the Frobenius scheme presented in this section.
The Frobenius is more efﬁcient than the relative-entropy normalization (which is the least efﬁcient
among the three).

5 Experiments
For the clustering algorithm into k ≥ 2 clusters we experimented with the spectral algorithms
described in [10] and [6]. The latter uses the N-cuts normalization D−1/2KD−1/2 followed by
K-means on the embedded coordinates (the leading k eigenvectors of the normalized afﬁnity) and

102030405001000200030004000# of data−pointssecondsProjectionMatlab QP50010001500200001234# of data−pointssecondsL1FrobeniusRelative Entropythe former uses a certain discretization scheme to turn the k leading eigenvectors into an indica-
tor matrix. Both algorithms produced similar results thus we focused on [10] while replacing the
normalization with the three schemes presented above. We refer to ”Ncuts” as the original normal-
ization D−1/2KD−1/2 , by ”RE” to the iterative application of the original normalization (which is
proven to converge to a doubly stochastic matrix [11]), by ”L1” to the L1 doubly-stochastic nor-
malization (which we have shown is equivalent to Ratio-cuts) and by ”Frobenius” to the iterative
Frobenius scheme based on Von-Neumann’s lemma described in Section 4. We also included a
”None” ﬁeld which corresponds to no normalization being applied.

Dataset

Kernel

SPECTF heart RBF
RBF
Pima
RBF
Wine
SpamBase
RBF
Poly
BUPA
WDBC
Poly

k

2
2
3
2
2
2

Size Dim.

267
768
178
4601
345
569

44
8
13
57
6
30

L1
27.5
36.2
38.8
36.1
37.4
18.8

Lowest Error Rate
RE NCuts None
Frobenius
19.2
29.5
27.5
27.5
34.9
35.4
35.2
35.2
27.0
27.5
29.2
34.3
30.3
37.7
31.8
30.4
37.4
37.4
41.7
41.7
11.1
37.4
37.4
18.8

Table 1: UCI datasets used, together with some characteristics and the best result achieved using the different
methods.

Dataset

Kernel

Poly
Leukemia
Poly
Lung
Prostate
RBF
Prostate Outcome RBF

k

2
2
2
2

Size

#PC

72
181
136
21

5
5
5
5

Lowest Error Rate
Frobenius
RE NCuts None
16.7
30.6
38.9
36.1
9.9
15.5
15.5
16.6
19.9
40.4
40.4
43.4
4.8
23.8
28.6
28.6

L1
27.8
15.5
40.4
28.6

Table 2: Cancer datasets used, together with some characteristics and the best result achieved using the differ-
ent methods.

We begin with evaluating the clustering quality obtained under the different normalization methods
taken over a number of well studied datasets from the UCI repository2 . The data-sets are listed
in Table 1 together with some of their characteristics. The best performance (lowest error rate)
kxi−xj k2
for the
is presented in Boldface. With the ﬁrst four datasets we used an RBF kernel e
σ2
afﬁnity matrix, while for the latter two a polynomial kernel (xT
i xj + 1)d was used. The kernel
parameters were calibrated independently for each method and for each dataset. In most cases the
best performance was obtained with the Frobenius norm approximation, but as a general rule the type
of normalization depends on the data. Also worth noting are instances, such as Wine and SpamBase,
when the RE or Ncuts actually worsen the performance. In that case the RE performance is worse
the Ncuts as the entire normalization direction is counter-productive. When RE outperforms None
it also outperforms Ncuts (as can be expected since Ncuts is the ﬁrst step in the iterative scheme of
RE).
With regard to tuning the afﬁnity measure, we show in Fig. 2 the clustering performance of each
dataset under each normalization scheme under varying kernel setting (σ and d values). Generally,
the performance of the Frobenius normalization behaves in a smoother manner and is more stable
under varying kernel settings than the other normalization schemes.
Our next set of experiments was over some well studied cancer data-sets3 . The data-sets are listed
in Table 2 together with some of their characteristics. The column ”#PC” refers to the number of
principal components used in a PCA pre-processing for the purpose of dimensionality reduction
prior to clustering. Note that better results can be achieved when using a more sophisticated pre-
processing, but since the focus is on the performances of the clustering algorithms and not on the
datasets, we prefer not to use the optimal pre-processing and leave the data noisy. The AML/ALL
2 http://www.ics.uci.edu/∼ mlearn/MLRepository.html
3All cancer datasets can be found at http://sdmc.i2r.a-star.edu.sg/rp/

(SPECTF)

(Pima)

(Wine)

(SpamBase)

(BUPA)

(WDBC)

Figure 2: Error rate vs. similarity measure, for the UCI datasets listed in Table 1
L1 in magenta +; Forbenius in blue o; Relative Entropy in black ×; and Normalized-Cuts in red

204060801001020304050sigma% errors1234563035404550sigma% errors2004006008002030405060sigma% errors5010015020025030030354045sigma% errors10203040506035404550degree% errors1234561020304050degree% errors(AML/ALL Leukemia)

(Lung Cancer)

(Prostate)

(Prostate Outcome)

Figure 3: Error rate vs. similarity measure, for the cancer datasets listed in Table 2.
L1 in magenta +; Forbenius in blue o; Relative Entropy in black ×; and Normalized-Cuts in red

Leukemia dataset is a challenging benchmark common in the cancer community, where the task is
to distinguish between two types of Leukemia. The original dataset consists of 7129 coordinates
probed from 6817 human genes, and we perform PCA to obtain 5 leading principal components
prior to clustering using a polynomial kernel. Lung Cancer (Brigham and Women’s Hospital, Har-
vard Medical School) dataset is another common benchmark that describes 12533 genes sampled
from 181 tissues. The task is to distinguish between malignant pleural mesothelioma (MPM) and
adenocarcinoma (ADCA) of the lung. The Prostate dataset consists of 12,600 coordinates represent-
ing different genes, where the task is to identify prostate samples as tumor or non-tumor. We use the
ﬁrst ﬁve principal components as input for clustering using an RBF kernel. The Prostate Outcome
dataset uses the same genes from another set of prostate samples, where the task is to predict the
clinical outcome (relapse or non-relapse for at least four years). Finally, Fig. 3 shows the clustering
performance of each dataset under each normalization scheme under varying kernel settings (σ and
d values).

6 Summary

Normalization of the afﬁnity matrix is a crucial element in the success of spectral clustering. The
type of normalization performed by N-cuts is a step towards a doubly-stochastic approximation of
the afﬁnity matrix under relative entropy [11]. In this paper we have extended the normalization
via doubly-stochasticity in three ways: (i) we have shown that the difference between N-Cuts and
Ratio-cuts is in the error measure used to ﬁnd the closest doubly stochastic approximation to the
input afﬁnity matrix, (ii) we have introduced a new normalization scheme based on Frobenius norm
approximation. The scheme involves a succession of simple computations, is very simple to imple-
ment and is efﬁcient computation-wise, and (iii) throughout extensive experimentation on standard
data-sets we have shown the importance of normalization to the performance of spectral clustering.

2468101020304050degree% errors24681001020304050degree% errors501001501020304050sigma% errors20040060080001020304050sigma% errorsIn the experiments we have conducted the Frobenius normalization had the upper-hand in most
cases. We have also shown that the relative-entropy normalization is not always the right approach
as in some data-sets the performance worsened after the relative-entropy but never worsened when
the Frobenius normalization was applied.

References
[1] P. K. Chan, M. D. F. Schlag, and J. Y. Zien. Spectral k-way ratio-cut partitioning and clustering.
IEEE Transactions on Computer-aided Design of Integrated Circuits and Systems, 13(9):1088–
1096, 1994.
[2] I. Csiszar. I-divergence geometry of probability distributions and minimization problems. The
Annals of Probability, 3(1):146–158, 1975.
[3] R.L. Dykstra. An algorithm for restricted least squares regression. J. of the Amer. Stat. Assoc.,
78:837–842, 1983.
[4] I.S.Dhillon, Y.Guan, and B.Kulis. Kernel k-means, spectral clustering and normalized cuts. In
International Conference on Knowledge Discovery and Data Mining(KDD), pages 551–556,
Aug. 2004.
[5] J. Von Neumann. Functional Operators Vol. II. Princeton University Press, 1950.
[6] A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In
Proceedings of the conference on Neural Information Processing Systems (NIPS), 2001.
[7] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8), 2000.
[8] R. Sinkhorn and P. Knopp. Conerning non-negative matrices and doubly stochastic matrices.
Paciﬁc J. Math., 21:343–348, 1967.
[9] Y. Weiss. Segmentation using eigenvectors: a unifying view.
Conference on Computer Vision and Pattern Recognition, 1999.
[10] S.X. Yu and J. Shi. Multiclass spectral clustering. In Proceedings of the International Confer-
ence on Computer Vision, 2003.
[11] R. Zass and A. Shashua. A unifying approach to hard and probabilistic clustering. In Proceed-
ings of the International Conference on Computer Vision, Beijing, China, Oct. 2005.

In Proceedings of the IEEE

A Normalized Cuts and Relative Entropy Normalization

The following proposition is an extension (symmetric version) of the claim about the iterative pro-
portional ﬁtting procedure converging in relative entropy error measure [2]:
Proposition 2 The closest doubly-stochastic matrix F under the relative-entropy error measure to
a given symmetric matrix K , i.e., which minimizes:
RE (F ||K ) s.t. F ≥ 0, F = F > , F 1 = 1, F >1 = 1
min
F
has the form F = DKD for some (unique) diagonal matrix D .
L() = X
µj (X
fij − 1) − X
λi (X
fij − X
kij − X
+ X
Proof: The Lagrangian of the problem is:
fij ln fij
kij
ij
i
ij
ij
i
j
j
The derivative with respect to fij is:
= ln fij + 1 − ln kij − 1 − λi − µj = 0
∂L
∂ fij

fij − 1)

from which we obtain:

fij = eλi eµj kij
Let D1 = diag(eλ1 , ..., eλn ) and D2 = diag(eµ1 , ..., eµn ), then we have:
F = D1KD2
Since F = F > and K is symmetric we must have D1 = D2 .


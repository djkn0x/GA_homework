Detecting Corporate Fraud: An Application of Machine Learning

Ophir Gottlieb, Curt Salisbury, Howard Shek, Vishal Vaidyanathan

December 15, 2006

ABSTRACT

This paper explores the application of several ma-
chine learning algorithms to published corporate
data in an eﬀort to identify patterns indicative
of securities fraud. Generally Accepted Account-
ing Principles (GAAP) represent a conglomerate
of industry reporting standards which US pub-
lic companies must abide by to aid in ensuring
the integrity of these companies. Notwithstand-
ing these principles, US public companies have
legal ﬂexibility to maneuver the way they disclose
certain items in the ﬁnancial statements making
it extremely hard to detect fraud manually. Here
we test several popular methods in machine learn-
ing (logistic regression, naive Bayes and support
vector machines) on a large set of ﬁnancial data
and evaluate their accuracy in identifying fraud.
We ﬁnd that some variants of SVM and logistic
regression are signiﬁcantly better than the cur-
rently available methods in industry. Our results
are encouraging and call for a more thorough in-
vestigation into the applicability of machine learn-
ing techniques in corporate fraud detection.

INTRODUCTION

Regulators have attempted to address corporate account-
ing fraud for decades, from the formation of the SEC
in the 1930’s through the recent Sarbanes-Oxley legisla-
tion. Generally Accepted Accounting Principles (GAAP)
represent a conglomerate of industry reporting standards
established as an attempt to minimize accounting fraud.
However, the continued incidence of fraud and ﬁnancial
misrepresentation, is evident in SEC enforcement actions,
class-action litigation, ﬁnancial restatements and, most
prominent in the recent past, criminal prosecution. Iron-
ically, the complexity within GAAP provides suﬃcient le-
gitimate room for maniuplations that make it extremely
challenging to detect corporate fraud. Human analysis of
ﬁnancial data is often intractible and an incorrect analy-
sis can result in staggering ﬁnancial losses. It is therefore
natural to explore the ability of machine learning tech-

niques for this purpose. Algorithms for automatated de-
tection of patterns of fraud are relatively recent [1] and
the models currently employed are fairly simplistic.
In
this work, we have tested variants of Logistic Regression,
Naive Bayes, and Support Vector Machines(SVM). We
conclude with a comparison of the predictive accuracy of
these methods with the best performing solution used in
industry today.

METHOD

The Data The attribute data consist of ﬁnancial met-
rics covering: disclosed ﬁnancial statements, management
discussion and analysis (MD&A), ﬁnancial ﬁling foot-
notes and most other required corporate ﬁlings (8-K, S-4,
144A, etc.). These metrics represent real instance data
(such as accounts receivable over sales), data volatility
(eg. 1 year and 2 year changes in accounts receivable over
sales), and indicators of comparison to appropriate peer
groups1 (eg. accounts receivable over sales compared to
other similar companies). There are approximately 600
attributes per company per quarter. These attributes
represent nearly all ﬁnancial and related corporate in-
formation publicly available. Attributes can vary widely
between diﬀerent types of companies and missing or in-
correctly entered data is frequent in ﬁnancial reports. To
handle this, we represented each attribute as a percentile
in the distribution for the appropriate peer group. When
attribute data is missing, the missing value is assumed
to be the 50th percentile. The response data consists of
fraudulent and non-fraudulent labelings of the public US
companies for all quarters over the same time period. A
company is deﬁned as fraudulent for a given quarter if
it was successfully litigated (or settled) by the Securities
and Exchange Commission (SEC) for material misrepre-
sentation or false forward looking statements for a period
including that quarter.
Data exist for each of approximately 9,000 publicly
traded US companies and about 350 American Deposi-
tory Receipts (ADRs) over a period of 40 quarters (1996-
2005).
In this work, we treated each company-quarter

1 generated by Reuters

1

as a separate data point which gives us about 360,000
data points. Within this set, there were approximately
2,000 instances (company-quarters) of fraud (This small
proportion highlights one of the ma jor diﬃculties in de-
tecting fraud).

of each of the methods employed. In what follows, the
vector x represents the attribute data for a single com-
pany quarter and y represents a label indicating whether
that company quarter was fraudulent.

Data Preprocessing To make computational manip-
ulations easier for this work, we reduced the dimension-
ality of the problem by pruning out the attributes which
were perceived to contribute little to fraud prediction.
The evaluation of the contribution of a given attribute
to fraud prediction was done using an odds ratio. The
odds ratio [1] (similar to the mutual information) assigns
a simple score to each datum to indicate how informative
that datum is about the likelihood of fraud. To compute
the odds ratio, both the fraud status (response data) and
related attribute status of all companies are required.
Prior to computing the odds ratio for each attribute,
the attribute value was mapped to 1 or 0 based upon a
sub jective estimate of malfeasance implied by the value
of the attribute for a given company relative to the data
of peer companies 2 . In particular, for the revenue, as-
set, high risk event and governance metrics, large values
(>80th percentile relative to peers) were considered to
implicate a greater chance for malfeasance and their at-
tribute would be cast as a 1. For the expense and liability
related metrics, small values (<20th percentile relative to
peers) were considered risky and their attribute would be
cast as a 1. To understand why those attributes cast as a
1 are more likely to be fraudulent, consider if a company
were fraudulent it would probably overstate its revenue
(to make the company’s ﬁnancial condition seem stronger
than it really is) and it would probably understate an ex-
pense item.
Of the 600 original attributes, 173 were identiﬁed as
having an odds ratio greater than 1 and were signiﬁcant
for fraud prediction. To see if we could further reduce
the dimensionality by removing linearly dependent at-
tributes, we did a principal component analysis on the
remaining attributes. Our results showed that to get 80%
of the variance, we needed to include the top 113 prin-
cipal components. Since this does not result in a large
reduction in dimensions we chose to start with all 173
components identiﬁed by the odds ratio for training the
machine learning methods discussed below.

Machine Learning Algorithms We decided to evalu-
ate some of the most popular machine learning techniques
available in the literature today. We give a brief summary

2This casting was done only for the calcuation of the odds ra-
tio. The true attributes values were used for training learning algo-
rithms.

Logistic Regression Logistic regression is a dis-
criminative learning algorithm which directly attemps to
estimate the probability of fraud given the attribute data
for a company-quarter. Our examination included several
hypothesis classes of increasing complexity within the set-
ting of logistic regression: The hypothesis used in logistic
regression is deﬁned by

h(x) = 1{g(x)>0}
1
1 + e−η(x)

g (x) =

One can choose diﬀerent forms for the function η(x)
depending on the number of parameters one wants to ﬁt

i. Linear

ii. Quadratic

η(x) = θ0 + θ1x1 + ... + θnxn

1 + ... + θ2n x2
η(x) = θ0 + θ1x1 + ... + θnxn + θn+1x2
n

iii. Cubic

η(x) = θ0 + θ1x1 + ... + θnxn + θn+1x2
1 + ... + θ2n x2
n
1 + ... + θ3n x3
+θ2n+1x3
n

Naive Bayes Naive Bayes is a generative method
that determines a model of p(x|y ) and then uses Bayes
rule to generate p(y |x) rather than ﬁtting parameters to
a model of p(y |x) directly. Consequently, this approach
can be expected to work well if the conditional distribu-
tions of the attributes are signiﬁcantly diﬀerent for diﬀer-
ent values of the label. Intuitively it is reasonable to ex-
pect that a company that has decided to act fraudulently
in a given quarter would behave in a manner similar to
other companies who have made the same decision, and
in a manner inconsistent with those companies not acting
fraudulently.
The training set for these experiments consisted of
the ﬁrst 210,000 chronological company-quarters and the
test set consisted of the remaining (≈110,000) company-
quarters. The ob jective of the chonological separation
was to mimic the natural chronological separation that
would be observed in the application of the algorithm.
In particular, one desires to predict fraud in the current

2

quarter using current and all previous quarter informa-
tion, including known fraudulent behavior in past quar-
ters.
Two variations of a Bayesian learning algorithm were
evaluated. They diﬀered only in the general model sus-
pected to describe p(x|y ). The ﬁrst model was a mul-
tivariate Bernoulli.
It was selected for its simplicity.
The percentile scores of the attributes were cast to 1 (if
perc<20 or perc>80) or 0 (otherwise). The hypothesis
was that fraudulent companies would have metrics that
deviated more from their means than their non-fraudulent
counterparts. This algorithm was naive in that it as-
sumed p(xi , xj |y ) = p(xi |y )p(xj |y ) for all i 6= j . Oth-
erwise, the algorithm would need to ﬁt an unreasonable
number (2173 ) of parameters.
The second model was a product of one-dimensional
Gaussian. It was selected to allow the algorithm to have
more ﬂexibility in the selection of a decision boundary
and to compare the performance of the simpliﬁed multi-
variate Bernoulli model to this more rich model. Since the
attribute data are percentiles, one way to represent that
data as gaussian would be to recast it back to a gaussian.
We chose a simpler method of ﬁtting a gaussian directly
to the percentile data.

Support Vector Machines Support vector ma-
chines (SVMs) are often considered to be the best oﬀ-
the-shelf tools for classiﬁcation problems. Binary classi-
ﬁcation SVMs calculate a separating hyperplane in fea-
ture space by solving a quadratic programming problem
that maximizes the distance between the separating hy-
perplane and the feature points. To handle data that is
not linearly separable, the SVM method can be kernel-
ized which allows the generation of non-linear separating
surfaces. In this work, we employed the following kernels:

i. Linear

ii. Quadratic

iii. Gaussian

K (x, y) = xT y

K (x, y) = (xT y)2

K (x, y) = exp(

|x − y|2
2σ2

)

to be fraudulent to least likely (in the case of SVMs, we
use the margins as substitutes for probability). The CAP
curve indicates the fraction of fraudulent data points as
a function of the fraction of all ranked data considered.
For example, to say that the CAP value at 20% is 50%
would mean that in the top 20% of all companies ranked
in order of likelihood of fraud, we correctly identiﬁed 50%
of all fraudulent companies. A CAP allows one to deter-
mine how many false positives result when identifying any
desired percentage of companies as high risk. Conversely,
we can identify how many false negatives result at each
percentile level of low risk companies.
The accuracy ratio (AR) is a metric for summarizing
the CAP. The CAP for random guessing has an AR of
zero and the CAP for perfect prediction has an AR of 1.
Formally the AR is deﬁned as a ratio of areas under the
CAP curves [3].
We ﬁnd that even when algorithms fail to accurately
predict probabilities or labels, they perform much bet-
ter in ranking the data by likelihood of fraud. For ex-
ample, the logistic regression algorithms did not identify
any companies as fraudulent but produces a reasonable
ordering of companies. Since a ranking is often of equal or
greater practical value in making ﬁnancial and other deci-
sions, we use the AR to assess the eﬃcacy of the diﬀerent
methods we tested.

RESULTS AND DISCUSSION

A summary of our results can be seen in table 1.
We brieﬂy discuss the results of each machine learning
method below.

Model

Variant

industry(proprietary)
logistic regression

naive Bayes

SVM

linear
quadratic
cubic
Bernoulli
Gaussian
linear
quadratic
Gaussian(σ = 0.5)

AR

0.48
0.53
0.63
0.65
0.48
0.53
0.47
0.50
0.72

Model Evaluation The models mentioned above were
evaluated using a hold out test set.
In order to as-
sign accuracy measures and to compare diﬀerent models,
we used the ﬁnancial industry standard for quantitative
model validation called the Cumulative Accuracy Proﬁle
(CAP) [2]. To obtain the CAP, the predictions of the
models are used to rank the data in order of most likely

Table 1: Performance of machine learning methods

Logistic Regression

Linear Classiﬁers Further reduction was performed
on the dimension of the attribute data by using forward

3

search feature selection with a p-value of 0.05 as the in-
clusion/exclusion rule. The resulting forward selected
model revealed 48 (of 173) metrics that met the p-value
requirement (< .05). The logistic regression algorithm
was trained on 40 × 6300 = 2.5 × 105 training examples,
each of dimension 48 and tested on 40 × 2700 = 1.1 × 105
validation examples, each of dimension 48. For the linear
hypothesis class, the AR was 0.53 (see Fig. ). Further, the
highest-risk 10% of companies were 18.8 times more likely
to be fraudulent than the lowest-risk 10%. The overall
accuracy for the logistic regression with linear classiﬁers
is fair. Clearly the learning algorithm has produced re-
sults well above a random separating hyperplane. The
variance in this initial model (and corresponding risk of
overﬁtting) is small. The in-sample and out-of-sample re-
sults are similar and the restriction to linear classiﬁers
all but guarantees we are not overﬁtting the data. The
possibility of bias does exist. The idea that a 173 dimen-
sional training set is ﬁt linearly to all of the predictors is
unlikely and requires further analysis.

ulent than the lowest risk 10%. The similarity between
the in- and out-of-sample accuracy gives little reason to
believe that substantive variance has been added to the
model. With this in mind, we looked at the cubic feature
mapping.

Cubic Classiﬁers The cubic classiﬁers demon-
strated a smaller improvement over the previous hypoth-
esis classes with an AR of 0.65. The highest risk 10% of
companies were 155 times more likely to be fraudulent
than the lowest risk 10%.
Higher order polynomial hypotheses did not seem to
achieve signiﬁcantly better results with the test set, sug-
gesting that overﬁtting occurs beyond the cubic classiﬁer.

Naive Bayes The mulitvariate-Bernoulli based
Bayesian algorithm resulted in an AR of 0.48 (see Fig
2). The gaussian based Bayesian algorithm resulted in
a better AR of 0.53. This performance is comparable
to the current industry standard. To the extent that the
gaussian based algorithm performed much better than the
mulitvariate-Bernoulli based algorithm, the casting algo-
rithm from percentiles to 0s and 1s failed to capture all
of the meaningful data. Potential improvements to the
Bayesian approach would be to recast the data from per-
centiles back to a gaussian and then estimate p(x|y ) and
exclude the naive assumption, estimating the joint prob-
ability p(x1 , x2 , ..., xn |y ). Multivariate Gaussian random
variables are quite amenable to estimation as the number
of parameters grows polynomially (rather than exponen-
tially) with the dimension of the gaussian model.

Figure 1: CAP for logistic regression using various hy-
pothesis classes.

The maximum probability of fraud placed on any one
company during the 10 year period was just over 26% -
meaning the linearly classiﬁed logistic model did not iden-
tify any companies as having a higher risk of fraud than
not. However, 26% does imply a 2600% higher proba-
bility of fraud than the average company since the prior
probability of fraud in the real world is ≈ 1%.

Quadratic Classiﬁers The quadratic classiﬁers
demonstrated a marked improvement over the linear one.
The AR is 0.63 for the test set. Further, the highest risk
10% of companies were 125 times more likely to be fraud-

4

Figure 2: CAP for Naive Bayes using Bernoulli and Gaus-
sian models.

Perhaps more surprising is that a simple model like lo-
gistic regression gave results not too far behind gaussian
SVMs. This preliminary work strongly encourages fur-

SVM We implemented a version of the SMO algo-
rithm to construct SVMs [4]. It is diﬃcult to train SVMs
on the entire data set. Consequently a subset of data
containing 2000 non fraud and 900 fraudulent company-
quarters were chosen for training purposes.
(This size
was chosen based on tradeoﬀs between training time and
improved accuracy. Given more training time, we could
potentially build better SVMs). The regularization pa-
rameter, tolerances and other training parameters were
chosen to minimize the number of support vectors ob-
tained at convergence.

Figure 4: CAP for best performing learning methods

ther investigation. Future directions include developing
more sophisticated models for the hypotheses within the
framework of the learning methods explored here and de-
signing systematic schemes for optimizing top level pa-
rameters. Further, we have only touched upon a handful
of machine learning methods in this paper and it is ex-
tremely intriguing to consider the wealth of other meth-
ods available in the literature. We conclude on the bright
note that the problem of identifying fraudulent ﬁnancial
statements shows great promise of being succesfully ad-
dressed in the near future.

ACKNOWLEDGEMENTS

We are grateful to Audit Integrity, a quantitative risk
analytics ﬁrm based in Los Angeles, CA. for providing
the ﬁnancial data set used in this work.

References

[1] Audit Integrity. Agr white paper, 2004.

[2] Sobehart J, Keenan S, and Stein R. Benchmarking
quantitative default risk models: a validation method-
ology. Moody’s Rating Methodology, 2000.

[3] Engelmann B, Hayden E, and Tasche D. Measuring
the discriminative power of rating systems. Deutsche
Bundesbank: Discussion Paper Series, 2003.

Figure 3: CAP for SVMs with diﬀerent kernels.

The AR of 0.47 obtained from the linear SVM is
quite disappointing and suggests that the data is not
linearly separable (training a simple linear SVM is typi-
cally unable to reach convergence unless tolerances are
increased to unacceptable levels). A quadratic kernel
does marginally better. The results of the gaussian kernel
SVM are dependent on the parameter σ . Larger values
of σ tend to underﬁt while smaller values tended to over-
ﬁt. The data shown here was obtained by hand tweak-
ing the regularization and σ parameters. For ‘optimal’
parameters the AR of the gaussian kernel SVM is a dra-
matic improvement over not only the other kernels, but
the other machine learning methods as well, scoring an
AR of 0.72 in the best case. This is a very signiﬁcant
improvement over the current industry standard of 0.48.
The encouraging results seen here behoove a deeper ex-
ploration of better SVM optimization algorithms, choice
of kernels and parameters.

CONCLUSION

We ﬁnd that machine learning methods are quite easily
able to outperform current industry standards in detect-
ing fraud. Not surprisingly, SVMs gave the best results.

[4] Platt J. Fast training of support vector machines
using sequential minimal optimization. Advances in
Kernel Methods - Support Vector Learning, 1998.

5


PAC-Bayes Bounds for the Risk of the Majority Vote
and the Variance of the Gibbs Classiﬁer

Alexandre Lacasse, Franc¸ ois Laviolette and Mario Marchand
D ´epartement IFT-GLO
Universit ´e Laval
Qu ´ebec, Canada
Firstname.Secondname@ift.ulaval.ca

Pascal Germain
D ´epartement IFT-GLO
Universit ´e Laval Qu ´ebec, Canada
Pascal.Germain.1@ulaval.ca

Nicolas Usunier
Laboratoire d’informatique de Paris 6
Universit ´e Pierre et Marie Curie, Paris, France
Nicolas.Usunier@lip6.fr

Abstract

We propose new PAC-Bayes bounds for the risk of the weighted majority vote that
depend on the mean and variance of the error of its associated Gibbs classiﬁer. We
show that these bounds can be smaller than the risk of the Gibbs classiﬁer and can
be arbitrarily close to zero even if the risk of the Gibbs classiﬁer is close to 1/2.
Moreover, we show that these bounds can be uniformly estimated on the training
data for all possible posteriors Q. Moreover, they can be improved by using a
large sample of unlabelled data.

1 Introduction

The PAC-Bayes approach, initiated by [1], aims at providing PAC guarantees to “Bayesian-like”
learning algorithms. Within this approach, we consider a prior1 distribution P over a space of
classiﬁers that characterizes our prior belief about good classiﬁers (before the observation of the
data) and a posterior distribution Q (over the same space of classiﬁers) that takes into account the
additional information provided by the training data. A remarkable result, known as the “PAC-
Bayes theorem”, provides a risk bound for the Q-weigthed majority-vote by bounding the risk of an
associated stochastic classiﬁer called the Gibbs classiﬁer. Bounds previously existed which showed
that you can de-randomize back to the Majority Vote classiﬁer, but these come at a cost of worse
risk. Naively, one would expect that the de-randomized classiﬁer would perform better. Indeed, it is
well-known that voting can dramatically improve performance when the “community” of classiﬁers
tend to compensate the individual errors. The actual PAC-Bayes framework is currently unable to
evaluate whether or not this compensation occurs. Consequently, this framework can not currently
help in producing highly accurate voted combinations of classiﬁers.
In this paper, we present new PAC-Bayes bounds on the risk of the Majority Vote classiﬁer based
on the estimation of the mean and variance of the errors of the associated Gibbs classiﬁer. These
bounds allow to prove that a sufﬁcient condition to provide an accurate combination is (1) that the
error of the Gibbs classiﬁer is less than half and (2) the mean pairwise covariance of the errors of
the classiﬁers appearing in the vote is small. In general, the bound allows to detect when the voted
combination provably outperforms its associated Gibbs classiﬁer.

1 Priors have been used for many years in statistics. The priors in this paper have only indirect links with the
Bayesian priors. We will nevertheless use this language, since it comes from previous work.

2 Basic Deﬁnitions
We consider binary classiﬁcation problems where the input space X consists of an arbitrary subset
of Rn and the output space Y = {−1, +1}. An example z def= (x, y) is an input-output pair where
x ∈ X and y ∈ Y . Throughout the paper, we adopt the PAC setting where each example z is drawn
according to a ﬁxed, but unknown, probability distribution D on X × Y .
We consider learning algorithms that work in a ﬁxed hypothesis space H of binary classiﬁers (de-
ﬁned without reference to the training data). The risk R(h) of any classiﬁer h : X → Y is deﬁned
(cid:179)
(cid:180)
as the probability that h misclassiﬁes an example drawn according to D:
I (h(x) (cid:54)= y),
h(x) (cid:54)= y

Pr
(x,y)∼D
where I (a) = 1 if predicate a is true and 0 otherwise.
Given a training set S , m will always represent its number of examples. Moreover, if S =
m(cid:88)
(cid:104)z1 , . . . , zm (cid:105), the empirical risk RS (h) on S , of any classiﬁer h, is deﬁned according to:
1
I (h(xi ) (cid:54)= yi ).
m
i=1

RS (h) def=

E
(x,y)∼D

R(h)

def=

=

After observing the training set S , the task of the learner is to choose a posterior distribution Q over
H such that the Q-weighted Majority Vote classiﬁer, BQ , will have the smallest possible risk. On
(cid:183)
(cid:184)
any input training example x, the output, BQ (x), of the Majority Vote classiﬁer BQ (also called the
Bayes classiﬁer) is given by:
BQ (x) def= sgn
h(x)
E
,
h∼Q
where sgn(s) = +1 if real number s > 0 and sgn(s) = −1 otherwise. The output of the determin-
istic Majority Vote classiﬁer BQ is thus closely related to the output of a stochastic classiﬁer called
the Gibbs classiﬁer. To classify an input example x, the Gibbs classiﬁer GQ chooses randomly a
(deterministic) classiﬁer h according to Q to classify x. The true risk R(GQ ) and the empirical risk
RS (GQ ) of the Gibbs classiﬁer are thus given by:
m(cid:88)
E
(x,y)∼D
i=1

RS (GQ ) def= E
h∼Q

R(GQ ) def= E
h∼Q

I (h(xi ) (cid:54)= yi ).

RS (h) = E
h∼Q

R(h) = E
h∼Q

I (h(x) (cid:54)= y)

1
m

(1)

(2)

The PAC-Bayes theorem gives a tight risk bound for the Gibbs classiﬁer GQ that depends on how
far is the chosen posterior Q from a prior P that must be chosen before observing the data. The
PAC-Bayes theorem was ﬁrst proposed by [2]. The bound presented here can be found in [3].
(cid:181)
(cid:183)
(cid:184)(cid:182)
Theorem 1 (PAC-Bayes Theorem) For any prior distribution P over H, and any δ ∈ ]0, 1], we
have
∀ Q over H : kl(RS (GQ )(cid:107)R(GQ )) ≤ 1
KL(Q(cid:107)P ) + ln m + 1
Pr
S∼Dm
δ
m
where KL(Q(cid:107)P ) is the Kullback-Leibler divergence between Q and P :
ln Q(h)
KL(Q(cid:107)P ) def= E
P (h) ,
h∼Q
and where kl(q(cid:107)p) is the Kullback-Leibler divergence between the Bernoulli distributions with prob-
p + (1 − q) ln 1−q
ability of success q and probability of success p: kl(q(cid:107)p) def= q ln q
1−p .
This theorem has recently been generalized by [4] to the sample-compression setting. In this paper,
however, we restrict ourselves to the more common case where the set H of classiﬁers is deﬁned
without reference to the training data.

≥ 1 − δ ,

A bound given for the risk of Gibbs classiﬁers can straightforwardly be turned into a bound for the
risk of Majority Vote classiﬁers. Indeed, whenever BQ misclassiﬁes x, at least half of the classiﬁers
(under measure Q), misclassiﬁes x. It follows that the error rate of GQ is at least half of the error
rate of BQ . Hence R(BQ ) ≤ 2R(GQ ). A method to decrease the R(BQ )/R(GQ ) ratio to 1 +  (for
some small positive ) has been proposed by [5] for large-margin classiﬁers. For a suitably chosen
prior and posterior, [5] have also shown that RS (GQ ) is small when the corresponding Majority Vote
classiﬁer BQ achieves a large separating margin on the training data. Consequently, the PAC-Bayes
theorem yields a tight risk bound for large margin classiﬁers.
Even if we can imagine situations where R(BQ ) > R(GQ ), they have been rarely encountered in
practice. In fact, situations where R(BQ ) is much smaller than R(GQ ) seem to occur much more
often. For example, consider the extreme case where the true label y of x is 1 iff Eh∼Qh(x) > 1/2.
In this case R(BQ ) = 0 whereas R(GQ ) can be as high as 1/2 −  for some arbitrary small . The
situations where R(BQ ) is much smaller than R(GQ ) are not captured by the PAC-Bayes theorem.
In the next section, we provide a bound on R(BQ ) that depends on R(GQ ) and other properties
that can be estimated from the training data. This bound can be arbitrary close to 0 even for a large
R(GQ ) as long as R(GQ ) < 1/2 and as long as we have a sufﬁciently large population of classiﬁers
for which their errors are sufﬁciently “uncorrelated”.

3 A Bound on R(BQ ) that Can Be Much Smaller than R(GQ )

All of our relations between R(BQ ) and R(GQ ) arise by considering the Q-weight WQ (x, y) of
classiﬁers making errors on example (x, y):

WQ (x, y) def= E
h∼Q

I (h(x) (cid:54)= y) .

Clearly, we have:

Hence, Pr
(x,y)∼D

E
(x,y)∼D

(WQ (x, y) ≥ 1/2).
(WQ (x, y) > 1/2) ≤ R(BQ ) ≤
Pr
Pr
(x,y)∼D
(x,y)∼D
(WQ (x, y) ≥ 1/2) gives a very tight upper bound on R(BQ ). Moreover,
I (h(x) (cid:54)= y) = R(GQ )
(cid:181)
(cid:182)
WQ (x, y) =
(cid:180)2
(cid:179)
E
(x,y)∼D
(cid:179)
(cid:180)
(WQ )2 −
E
WQ
(x,y)∼D
(cid:179)
(cid:180)
I (h1 (x) (cid:54)= y) E
− R2 (GQ )
I (h2 (x) (cid:54)= y)
E
h1∼Q
h2∼Q
I (h1 (x) (cid:54)= y)I (h2 (x) (cid:54)= y) − R(h1 )R(h2 )

E
(x,y)∼D

E
(x,y)∼D

E
h∼Q

=

=

E
h1∼Q

E
h2∼Q

E
(x,y)∼D

and

Var
(x,y)∼D

(WQ ) =

(3)

(4)

(5)

def=

(6)

coverr (h1 , h2 ) ,

E
E
h2∼Q
h1∼Q
where coverr (h1 , h2 ) denotes the covariance of the errors of h1 and h2 on examples drawn by D .
The next theorem is therefore a direct consequence of the one-sided Chebychev (or Cantelli-
Chebychev) inequality [6]: Pr (WQ ≥ a + E(WQ )) ≤ Var(WQ )
for any a > 0.
Var(WQ )+a2
Theorem 2 For any distribution Q over a class of classiﬁers, if R(GQ ) ≤ 1/2 then we have
(1 − 2WQ )
(WQ )
Var
Var
(x,y)∼D
(x,y)∼D
def= CQ .
(WQ ) + (1/2 − R(GQ ))2 =
(1 − 2WQ )2
(cid:80)
E
Var
(x,y)∼D
(x,y)∼D
We will always use here the ﬁrst form of CQ . However, note that 1 − 2WQ =
h∈H Q(h)yh(x)
is just the margin of the Q-convex combination realized on (x, y). Hence, the second form of CQ is
simply the variance of the margin divided by its second moment!

R(BQ ) ≤

The looser two-sided Chebychev inequality was used in [7] to bound the risk of random forests.
However, the one-sided bound CQ is much tighter. For example, the two-sided bound in [7] diverges
when R(GQ ) → 1/2, but CQ ≤ 1 whenever R(GQ ) ≤ 1/2. In fact, as explained in [8], the
one-sided Chebychev bound is the tightest possible upper bound for any random variable which is
based only on its expectation and variance.
The next result shows that, when the number of voters tends to inﬁnity (and the weight of each voter
tends to zero), the variance of WQ will tend to 0 provided that the average of the covariance of the
risks of all pairs of distinct voters is ≤ 0. In particular, the variance will always tend to 0 if the risk
of the voters are pairwise independent.
(cid:88)
(cid:88)
(cid:88)
Proposition 3 For any countable class H of classiﬁers and any distribution Q over H, we have
(WQ ) ≤ 1
Q(h1 )Q(h2 )coverr (h1 , h2 ).
Q2 (h) +
Var
(x,y)∼D
4
h2∈H:
h1∈H
h∈H
(cid:80)
h2 (cid:54)=h1
(cid:80)
The proof is straightforward and is left to the reader. The key observation that comes out of this result
h∈H Q2 (h) is usually much smaller than one. Consider, for example, the case where Q
is that
is uniform on H with |H| = n. Then q =
h∈H Q2 (h) = 1/n. Moreover, if coverr (h1 , h2 ) ≤ 0
for each pair of distinct classiﬁers in H, then Var(WQ ) ≤ 1/(4n). Hence, in these cases, we have
that CQ ∈ O(1/n) whenever 1/2−R(GQ ) is larger than some positive constant independent of n.
Thus, even when R(GQ ) is large, we see that R(BQ ) can be arbitrarily close to 0 as we increase the
number of classiﬁers having non-positive pairwise covariance of their risk.
To further motivate the use of CQ , we have investigated, on several UCI binary classiﬁcation data
sets, how R(GQ ), Var(WQ ) and CQ are respectively related to R(BQ ). The results of Figure 1 have
been obtained with the Adaboost [9] algorithm used with “decision stumps” as weak learners. Each
data set was split in two halves: one used for training and the other for testing. In the chart relating
R(GQ ) and R(BQ ), we see that we almost always have R(BQ ) < R(GQ ). There is, however, no
clear correlation between R(BQ ) and R(GQ ). We also see no clear correlation between R(BQ ) and
Var(WQ ) in the second chart. In contrast, the chart of CQ vs R(BQ ) shows a strong correlation.
Indeed, it is almost a linear relation!

Figure 1: Relation, on various data sets, between R(BQ ) and R(GQ ), Var(WQ ), and CQ .

00,10,20,30,40,500,10,20,30,40,5R(BQ) on testR(GQ) on testbreast-cancerbreast-wcredit-ghepatitisionospherekr-vs-kplabormushroomsicksonarvote00,050,10,150,20,2500,10,20,30,40,5R(BQ) on testVar(WQ) on testbreast-cancerbreast-wcredit-ghepatitisionospherekr-vs-kplabormushroomsicksonarvote00,20,40,60,8100,10,20,30,40,5R(BQ) on testCQ on testbreast-cancerbreast-wcredit-ghepatitisionospherekr-vs-kplabormushroomsicksonarvote4 New PAC-Bayes Theorems

.

and

eQ

sQ

dQ

def=

def=

E
(x,y)∼D

A uniform estimate of CQ can be obtained if we have uniform upper bounds on R(GQ ) and on
the variance of WQ . While the original PAC-Bayes theorem provides an upper bound on R(GQ )
that holds uniformly for all posteriors Q, obtaining such bounds for the variance of a random vari-
able is still an issue. To achieve this goal, we will have to generalize the PAC-Bayes theorem for
expectations over pairs of classiﬁers since E(W 2
Q ) is fundamentally such an expectation.
(cid:179)
(cid:180)
Deﬁnition 4 For any probability distribution Q over H, we deﬁne the expected joint error (eQ ), the
expected joint success (sQ ), and the expected disagreement (dQ ) as
(cid:179)
(cid:180)
I (h1 (x) (cid:54)= y)I (h2 (x) (cid:54)= y)
def=
E
E
E
h2∼Q
h1∼Q
(x,y)∼D
(cid:180)
(cid:179)
I (h1 (x) = y)I (h2 (x) = y)
E
E
h1∼Q
h2∼Q
I (h1 (x) (cid:54)= h2 (x))
E
E
E
h2∼Q
h1∼Q
(x,y)∼D
(cid:161)
(cid:162)
(cid:80)m
usual, i.e., (cid:99)eQ
The empirical estimates, over a training set S = (cid:104)z1 , . . . , zm (cid:105), of these expectations are deﬁned as
i=1 I (h1 (x) (cid:54)= y)I (h2 (x) (cid:54)= y)
def= E
, etc.
E
1
h2∼Q
h1∼Q
m
It is easy to see that
(1−WQ )2 ,
2WQ (1−WQ ) .
sQ = E
eQ = E
dQ = E
W 2
Q ,
(x,y)∼D
(x,y)∼D
(x,y)∼D
Thus, we have eQ + sQ + dQ = 1 and 2eQ + dQ = 2R(GQ ). This implies,
1
1
· (1 + eQ − sQ )
· dQ =
R(GQ ) = eQ +
2
2
· dQ )2 = eQ − 1
1
Var(WQ ) = eQ − (R(GQ ))2 = eQ − (eQ +
· (1 + eQ − sQ )2
2
4
Moreover, in that new setting, the denominator of CQ can elegantly be rewritten as
Var(WQ ) + (1/2 − R(GQ ))2 = 1/4 − dQ /2.
The next theorem can be used to bound separately either eQ , sQ or dQ .
(cid:183)
(cid:181)
Theorem 5 For any prior distribution P over H, and any δ ∈ ]0, 1], we have:
∀ Q over H : kl( (cid:99)αQ (cid:107)αQ ) ≤ 1
(m + 1)
2 ·KL(Q(cid:107)P ) + ln
Pr
S∼Dm
δ
m
where αQ can be either eQ , sQ or dQ .
In contrast with Theorem 5, the next theorem will enable us to bound directly Var(WQ ), by bound-
ing any pair of expectations among eQ , sQ and dQ .
(cid:184)(cid:182)
(cid:183)
(cid:181)
∀ Q over H : kl( (cid:99)αQ , (cid:99)βQ (cid:107)αQ , βQ ) ≤ 1
Theorem 6 For any prior distribution P over H, and any δ ∈ ]0, 1], we have:
(m + 1)(m + 2)
2 ·KL(Q(cid:107)P ) + ln
Pr
S∼Dm
2δ
m
where αQ and βQ can be any two distinct choices among eQ , sQ and dQ , and where
1 − q1 − q2
kl(q1 , q2 (cid:107)p1 , p2 ) def= q1 ln q1
+ (1 − q1 − q2 ) ln
+ q2 ln q2
1 − p1 − p2
p1
p2
is the Kullback-Leibler divergence between the distributions of two trivalent random variables Yq
and Yp with P (Yq = a) = q1 , P (Yq = b) = q2 and P (Yq = c) = 1− q1 − q2 (and similarly for Yp ).
The proof of Theorem 5 can be seen as a special case of Theorem 1. The proof of Theorem 6 essen-
tially follows the proof of Theorem 1 given in [4]; except that it is based on a trinomial distribution
instead of a binomial one2 .
2 For the proofs of these theorems, see a long version of the paper at http://www.ift.ulaval.ca/
∼laviolette/Publications/publications.html.

(cid:184)(cid:182)
≥ 1 − δ

≥ 1 − δ

(8)

(9)

(7)

(10)

5 PAC-Bayes Bounds for Var(WQ ) and R(BQ )

,

,

,

E δ
Q,S

D δ
Q,S

def=

def=

def=

def=

A δ
Q,S

(cid:105)(cid:190)

From the two theorems of the preceding section, one can easily derive several PAC-Bayes bounds
of the variance of WQ and therefore, of the majority vote. Since CQ is a quotient. Thus, an upper
bound on CQ will degrade rapidly if the bounds on the numerator and the denominator are not tight
— especially for majority votes obtained by boosting algorithms where both the numerator and the
denominator tend to be small. For this reason, we will derive more than one PAC-Bayes bound for
(cid:189)
(cid:105)(cid:190)
(cid:104)
the majority vote, and compare their accuracy. First, we need the following notations that are related
to Theorems 1, 5 and 6. Given any prior distribution P over H,
(cid:105)(cid:190)
(cid:189)
(cid:104)
(m + 1)
r : kl(RS (GQ )(cid:107)r) ≤ 1
KL(Q(cid:107)P ) + ln
R δ
e : kl((cid:99)eQ (cid:107)e) ≤ 1
Q,S
δ
m
(cid:105)(cid:190)
(cid:189)
(cid:104)
(m + 1)
2 ·KL(Q(cid:107)P ) + ln
d : kl( (cid:99)dQ (cid:107)d) ≤ 1
δ
m
(cid:189)
(cid:104)
(m + 1)
2 ·KL(Q(cid:107)P ) + ln
(e, s) : kl((cid:99)eQ , (cid:99)sQ (cid:107)e, s) ≤ 1
m
δ
(m + 1)(m + 2)
2 ·KL(Q(cid:107)P ) + ln
m
δ
v+a = 1
1+a/v , it follows from Theorem 2 that an upper bound of both Var(WQ ) and R(GQ )
Since v
will give an upper bound on CQ , and hence on R(BQ ). Hence, a ﬁrst bound can be obtained, from
Equation 9, by suitably applying Theorem 5 (with αQ = eQ ) and Theorem 1.
(cid:181)
(cid:182)
(cid:179)
(cid:180)2
PAC-Bound 1 For any prior distribution P over H, and any δ ∈ ]0, 1], we have
≥ 1 − δ ,
inf Rδ/2
∀ Q over H : Var
WQ ≤ sup E δ/2
Q,S −
∀ Q over H : R(BQ ) ≤
 ≥ 1 − δ .
(cid:180)2
(cid:179)
Pr
S∼Dm
(x,y)∼D
Q,S
(cid:180)2
(cid:179)
(cid:180)2
(cid:179)
inf Rδ/2
Q,S −
sup E δ/2
Q,S
Pr
S∼Dm
2 − sup Rδ/2
inf Rδ/2
1
Q,S
Q,S
Since Bound 1 necessitates two PAC approximations to calculate the variance, it would be better
if we could obtain directly an upper bound for Var(WQ ). The following result, which is a direct
consequence of Theorem 6 and Equation 9, shows how it can be done.
(cid:190)(cid:33)
(cid:195)
(cid:189)
PAC-Bound 2 For any prior distribution P over H, and any δ ∈ ]0, 1], we have
e − 1
≥ 1 − δ ,
· (1 + e − s)2
∀ Q over H : Var
∀ Q over H : R(BQ ) ≤
 ≥ 1 − δ .
Pr
sup
(cid:170)
(cid:169)
S∼Dm
(x,y)∼D
4
(e,s)∈A δ
Q,S
4 · (1 + e − s)2
e − 1
(cid:180)2
(cid:179)
(cid:170)
(cid:169)
sup
(e,s)∈Aδ/2
Pr
Q,S
S∼Dm
4 · (1 + e − s)2
e − 1
sup
(e,s)∈Aδ/2
Q,S
As illustrated in Figure 2, Bound 2 is generally tighter than Bound 1. This gain is principally due
to the fact that the values of e and s, that are used to bound the variance, are tied together inside the
kl() and have to tradeoff their values (e “tries to be” as large as possible and s as small as possible).
Because of this tradeoff, e is generally not an upper bound of eQ , and s not a lower bound of sQ .
In the semi-supervised framework, we can achieve better results, because the labels of the examples
do not affect the value of dQ (see Deﬁnition 4). Hence, in presence of a large amount of unlabelled
data, one can use Theorem 5 to obtain very accurate upper and lower bounds of dQ . This combined
with an upper bound of eQ , still computed via Theorem 5 but on the labelled data, gives rise to the

2 − sup Rδ/2
1
Q,S

sup E δ/2
Q,S −

WQ ≤

.

+

+

following semi-supervised upper bound3 of Var(WQ ). The bound on R(BQ ) then follows from
Theorem 2 and Equation 10.
(cid:181)
(cid:182)
(cid:179)
(cid:180)2
PAC-Bound 3 (semi-supervised bound) For any prior distribution P over H, and any δ ∈ ]0, 1]:
1
· inf D δ
sup E δ
∀ Q over H : Var
WQ ≤ sup E δ
Q,S −
≥ 1 − δ
Pr
Q,S +
Q,S (cid:48)
∀ Q over H : R(BQ ) ≤ sup E δ
 ≥ 1 − δ
(x,y)∼D
2
(cid:180)2
(cid:179)
S∼Dm
S (cid:48)∼Dm(cid:48)
unlabelled
2 · inf D δ
sup E δ
Q,S −
Q,S + 1
Q,S (cid:48)
Pr
2 · sup D δ
1/4 − 1
S∼Dm
Q,S (cid:48)
S (cid:48)∼Dm(cid:48)
unlabelled
We see, on the left part of Figure 2, that Bound 2 on Var(WQ ) is much tighter than Bound 1. We
can also see that, by using unlabeled data4 to estimate dQ , Bound 3 provides an other signiﬁcant
improvement. These numerical results were obtained by using Adaboost [9] with decision stumps
on the Mushroom UCI data set (which contains 8124 examples). This data set was randomly split
into two halves: one for training and one for testing.

Figure 2: Bounds on Var(WQ ) (left) and bounds on R(BQ ) (right).

As illustrated by Figure 2, Bound 2 and Bound 3 are (resp. for the supervised and semi-supervised
frameworks) very tight upper bounds of the variance. Unfortunately they do not lead to tight upper
bounds of R(BQ ). Indeed, one can see in Figure 2 that after T = 8, all the bounds are degrading
even if the true value of CQ (on which they are based) continues to decrease. This drawback is due
to the fact that, when the value of dQ tends to 1/2, the denominator of CQ tends to 0. Hence, if dQ
is close to 1/2, Var(WQ ) must be small as well. Thus, any slack in the bound of Var(WQ ) has a
multiplicative effect on each of the three proposed PAC-bounds of R(BQ ). Unfortunately, boosting
algorithms tend to construct majority votes with expected an disagreement dQ just slightly under
1/2. Based on the next proposition, we will show that this drawback is, in a sense, unavoidable.
(cid:113)
(cid:161)
(cid:162)
Proposition 7 (Inapproachability result) Let Q be any distribution over a class of classiﬁers, and
let B < 1 be any upper bound of CQ which holds with conﬁdence 1 − δ . If R(GQ ) < 1/2 then
(1/4 − dQ /2)
1 − B
1/2 −
is an upper bound of R(GQ ) which holds with conﬁdence 1 − δ .
3 It follows, from an easy calculation, that a lower bound of dQ , together with an upper bound of eQ , gives
rise to an upper bound of eQ − (eQ + 1
2 · dQ )2 . By Equation 9, we then obtain an upper bound of Var(WQ ).
4The UCI database (used here) does not have any unlabeled examples. To simulate the extreme case
where we have an inﬁnite amount of unlabeled data, we simply used the empirical value of dQ computed
on the testing set.

00,020,040,060,080,10,120,140,16111213141TVar(WQ) PAC-Bound 1Var(WQ) PAC-Bound 2Var(WQ) PAC-Bound 3Var(WQ) on test00,20,40,60,811,2111213141TR(BQ) PAC-Bound 1R(BQ) PAC-Bound : 2R(GQ) using Thm 1R(BQ)PAC-Bound 2R(BQ)PAC-Bound 3CQ on testR(BQ)on testFor the data set used in Figure 2, Proposition 7, together with Bound 3 on R(BQ ) (viewed as
a bound on CQ ), gives a PAC-bound on R(GQ ) which is just slightly lower (≈ 0.5%) than the
classical PAC-Bayes bound on R(GQ ) given by Theorem 1. Since any bound better than Bound 3
for CQ will continue to improve the bound on R(GQ ), it seems unlikely that such a better bound
exists. Moreover, this drawback should occur for any bound on the majority vote that only considers
Gibbs’ risk and the variance of WQ because, as already explained, CQ is the tightest possible bound
of R(BQ ) that is based only on E(WQ ) and Var(WQ ). Hence, to improve our results in the situation
where dQ is closed to 1/2, one will have to consider higher moments. However, it is not clear that
this will lead to a better bound of R(BQ ) because, even if Theorem 5 generalizes to higher moments,
its tightness is then degrading. Indeed, for the k th moment, the factor 2 that multiplies KL(Q(cid:107)P )
in Theorem 5 grows to k . However, it might be possible to overcome this degradation by using a
generalization of Theorem 6 as we have done in this paper to obtain our tightest supervised bound for
the variance (Bound 2). Indeed, if we evaluate the tightness of that bound on the variance (w.r.t. its
value on the test set), and compare it with the tightness of the bound on R(GQ ) given by Theorem 1,
we ﬁnd that both accuracies are at about 3%. This is to be contrasted with the tightness of Bound 1
and seems to indicate that we have prevented degradation even if the variance deals with both the
ﬁrst and the second moment of WQ ; whereas the Gibbs’ risk deals only with the ﬁrst moment.

6 Conclusion

We have derived a risk bound for the weighted majority vote that depends on the mean and variance
of the error of its associated Gibbs classiﬁer (Theorem 2). The proposed bound is based on the one-
sided Chebychev’s inequality, which is the tightest inequality for any real-valued random variables
given only the expectation and the variance. As shown on Figures 1, this bound seems to have a
strong predictive power on the risk of the majority vote.
We have also shown that the original PAC-Bayes Theorem, together with new ones, can be used to
obtain high conﬁdence estimates of this new risk bound that hold uniformly for all posterior distribu-
tions. Moreover, the new PAC-Bayes theorems give rise to the ﬁrst uniform bounds on the variance
of the Gibbs’s risk (more precisely, the variance of the associate random variable WQ ). Even if there
are arguments showing that bounds of higher moments of WQ should be looser, we have empirically
found that one of the proposed bounds (Bound 2) does not show any sign of degradation in compar-
ison with the classical PAC-Bayes bound on R(GQ ) (which is the ﬁrst moment). Surprisingly, there
is an improvement for Bound 3 in the semi-supervised framework. This also opens up the possibility
that the generalization of Theorem 2 to higher moment be applicable to real data. Such generaliza-
tions might overcome the main drawback of our approach, namely, the fact that the PAC-bounds,
based on Theorem 2, degrade when the expected disagreement (dQ ) is close to 1/2.
Acknowledgments: Work supported by NSERC Discovery grants 262067 and 0122405.

References
[1] David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355–363, 1999.
[2] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21, 2003.
[3] David McAllester. Simpliﬁed PAC-Bayesian margin bounds. Proceedings of the 16th Annual Conference
on Learning Theory, Lecture Notes in Artiﬁcial Intelligence, 2777:203–215, 2003.
[4] Franc¸ ois Laviolette and Mario Marchand. PAC-Bayes risk bounds for sample-compressed Gibbs classiﬁers.
Proc. of the 22nth International Conference on Machine Learning (ICML 2005), pages 481–488, 2005.
[5] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In S. Thrun S. Becker and K. Obermayer,
editors, Advances in Neural Information Processing Systems 15, pages 423–430. MIT Press, Cambridge,
MA, 2003.
[6] Luc Devroye, L ´aszl ´o Gy ¨orﬁ, and G ´abor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer
Verlag, New York, NY, 1996.
[7] Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
[8] Dimitris Bertsimas and Ioana Popescu. Optimal inequalities in probability theory: A convex optimization
approach. SIAM J. on Optimization, 15(3):780–804, 2005.
[9] Robert E. Schapire and Yoram Singer. Improved boosting using conﬁdence-rated predictions. Machine
Learning, 37(3):297–336, 1999.


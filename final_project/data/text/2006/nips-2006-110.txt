Conditional Random Sampling: A Sketch-based
Sampling Technique for Sparse Data

Ping Li
Department of Statistics
Stanford University
Stanford, CA 94305
pingli@stat.stanford.edu

Kenneth W. Church
Microsoft Research
One Microsoft Way
Redmond, WA 98052
church@microsoft.com

Trevor J. Hastie
Department. of Statistics
Stanford University
Stanford, CA 94305
hastie@stanford.edu

Abstract
We1 develop Conditional Random Sampling (CRS), a technique particularly suit-
able for sparse data. In large-scale applications, the data are often highly sparse.
CRS combines sketching and sampling in that it converts sketches of the data into
conditional random samples online in the estimation stage, with the sample size
determined retrospectively. This paper focuses on approximating pairwise l2 and
l1 distances and comparing CRS with random projections. For boolean (0/1) data,
CRS is provably better than random projections. We show using real-world data
that CRS often outperforms random projections. This technique can be applied in
learning, data mining, information retrieval, and database query optimizations.

1 Introduction

Conditional Random Sampling (CRS) is a sketch-based sampling technique that effectively exploits
data sparsity. In modern applications in learning, data mining, and information retrieval, the datasets
are often very large and also highly sparse. For example, the term-document matrix is often more
than 99% sparse [7]. Sampling large-scale sparse data is challenging. The conventional random
sampling (i.e., randomly picking a small fraction) often performs poorly when most of the samples
are zeros. Also, in heavy-tailed data, the estimation errors of random sampling could be very large.

As alternatives to random sampling, various sketching algorithms have become popular, e.g., random
projections [17] and min-wise sketches [6]. Sketching algorithms are designed for approximating
speciﬁc summary statistics. For a speciﬁc task, a sketching algorit hm often outperforms random
sampling. On the other hand, random sampling is much more ﬂex ible. For example, we can use the
same set of random samples to estimate any lp pairwise distances and multi-way associations. Con-
ditional Random Sampling (CRS) combines the advantages of both sketching and random sampling.

Many important applications concern only the pairwise distances, e.g., distance-based clustering
and classiﬁcation, multi-dimensional scaling, kernels. F or a large training set (e.g., at Web scale),
computing pairwise distances exactly is often too time-consuming or even infeasible.

Let A be a data matrix of n rows and D columns. For example, A can be the term-document matrix
with n as the total number of word types and D as the total number of documents. In modern search
engines, n ≈ 106 ∼ 107 and D ≈ 1010 ∼ 1011 . In general, n is the number of data points and D
is the number of features. Computing all pairwise associations AAT , also called the Gram matrix
in machine learning, costs O(n2D), which could be daunting for large n and D . Various sampling
methods have been proposed for approximating Gram matrix and kernels [2, 8]. For example, using
(normal) random projections [17], we approximate AAT by (AR) (AR)T , where the entries of
R ∈ RD×k are i.i.d. N (0, 1). This reduces the cost down to O(nDk +n2 k), where k ≪ min(n, D).

1The full version [13]: www.stanford.edu/∼pingli98/publications/CRS tr.pdf

Sampling techniques can be critical in databases and information retrieval. For example, the
database query optimizer seeks highly efﬁcient techniques
to estimate the intermediate join sizes
joins.
in order to choose an “optimum ” execution path for multi-way

Conditional Random Sampling (CRS) can be applied to estimating pairwise distances (in any norm)
as well as multi-way associations. CRS can also be used for estimating joint histograms (two-way
and multi-way). While this paper focuses on estimating pairwise l2 and l1 distances and inner
products, we refer readers to the technical report [13] for estimating joint histograms. Our early
work, [11, 12] concerned estimating two-way and multi-way associations in boolean (0/1) data.
We will compare CRS with normal random projections for approximating l2 distances and inner
products, and with Cauchy random projections for approximating l1 distances. In boolean data,
CRS bears some similarity to Broder’s sketches [6] with some important distinctions. [12] showed
that in boolean data, CRS improves Broder’s sketches by roughly halving the estimation variances.

2 The Procedures of CRS
Conditional Random Sampling is a two-stage procedure. In the sketching stage, we scan the data
matrix once and store a fraction of the non-zero elements in each data point, as “sketches.” In the
estimation stage, we generate conditional random samples online pairwise (for two-way) or group-
wise (for multi-way); hence we name our algorithm Conditional Random Sampling (CRS).

2.1 The Sampling/Sketching Procedure
1    2    3     4    5    6    7    8    D
1    2    3     4    5    6    7    8    D

1
2
3
4
5
n

1
2
3
4
5
n

1
2
3
4
5
n

1
2
3
4
5
n

(a) Original

(b) Permuted
(c) Postings
Figure 1: A global view of the sketching stage.

(d) Sketches

Figure 1 provides a global view of the sketching stage. The columns of a sparse data matrix (a)
are ﬁrst randomly permuted (b). Then only the non-zero entri es are considered, called postings (c).
Sketches are simply the front of postings (d). Note that in the actual implementation, we only need
to maintain a permutation mapping on the column IDs.

1   2   3   4   5   6   7   8   9   10  11  12  13  14  15
0   1   0   2   0   1   0   0   1    2    1    0    1    0    2    
1   3   0   0   1   2   0   1   0    0    3    0    0    2    1     
 

u
u

1

2

(a) Data matrix and random samples
K  :   2 (1)   4 (2)   6 (1)   9 (1)   10 (2)   
P  :   2 (1)   4 (2)   6 (1)   9 (1)   10 (2)   11 (1)   13 (1)   15 (2)
1
1
K  :   1 (1)   2 (3)   5 (1)   6 (2)     8 (1)   11 (3)   
P  :   1 (1)   2 (3)   5 (1)   6 (2)     8 (1)   11 (3)   14 (2)   15 (1)  
2
2
(c) Sketches
(b) Postings
Figure 2: (a): A data matrix with two rows and D = 15. If the column IDs are random, the ﬁrst
Ds = 10 columns constitute a random sample. ui denotes the ith row. (b): Postings consist of
tuples “ID (Value).” (c): Sketches are the ﬁrst
ki entries of postings sorted ascending by IDs. In this
example, k1 = 5, k2 = 6, Ds = min(10, 11) = 10. Excluding 11(3) in K2 , we obtain the same
samples as if we directly sampled the ﬁrst Ds = 10 columns in the data matrix.

Apparently sketches are not uniformly random samples, which may make the estimation task dif-
ﬁcult. We show, in Figure 2, that sketches are almost random s amples pairwise (or group-wise).
Figure 2(a) constructs conventional random samples from a data matrix; and we show one can gen-
erate (retrospectively) the same random samples from sketches in Figure 2(b)(c).

In Figure 2(a), when the column are randomly permuted, we can construct random samples by sim-
ply taking the ﬁrst Ds columns from the data matrix of D columns (Ds ≪ D in real applications).
For sparse data, we only store the non-zero elements in the form of tuples “ID (Value),” a structure
called postings. We denote the postings by Pi for each row ui . Figure 2(b) shows the postings for
the same data matrix in Figure 2(a). The tuples are sorted ascending by their IDs. A sketch, Ki , of
postings Pi , is the ﬁrst ki entries (i.e., the smallest ki IDs) of Pi , as shown in Figure 2(c).

The central observation is that if we exclude all elements of sketches whose IDs are larger than

Ds = min (max(ID(K1 )), max(ID(K2 ))) ,
we obtain exactly the same samples as if we directly sampled the ﬁrst Ds columns from the data
matrix in Figure 2(a). This way, we convert sketches into random samples by conditioning on Ds ,
which differs pairwise and we do not know beforehand.

(1)

2.2 The Estimation Procedure

The estimation task for CRS can be extremely simple. After we construct the conditional random
samples from sketches K1 and K2 with the effective sample size Ds , we can compute any distances
(l2 , l1 , or inner products) from the samples and multiply them by D
to estimate the original space.
Ds
(Later, we will show how to improve the estimates by taking advantage of the marginal information.)
We use ˜u1,j and ˜u2,j (j = 1 to Ds ) to denote the conditional random samples (of size Ds ) obtained
by CRS. For example, in Figure 2, we have Ds = 10, and the non-zero ˜u1,j and ˜u2,j are

˜u1,2 = 3, ˜u1,4 = 2, ˜u1,6 = 1, ˜u1,9 = 1, ˜u1,10 = 2
˜u2,1 = 1, ˜u2,2 = 3, ˜u2,5 = 1, ˜u2,6 = 2, ˜u2,8 = 1.

Denote the inner product, squared l2 distance, and l1 distance, by a, d(2) , and d(1) , respectively,

a =

D
D
D
Xi=1
Xi=1
Xi=1
Once we have the random samples, we can then use the following simple linear estimators:

|u1,i − u2,i |2 ,

|u1,i − u2,i |

u1,iu2,i ,

d(2) =

d(1) =

(2)

ˆaM F =

D
Ds

Ds
Xj=1

˜u1,j ˜u2,j ,

ˆd(2)
M F =

D
Ds

Ds
Xj=1

( ˜u1,j − ˜u2,j )2 ,

ˆd(1)
M F =

D
Ds

Ds
Xj=1

| ˜u1,j − ˜u2,j |.

(3)

2.3 The Computational Cost

Th sketching stage requires generating a random permutation mapping of length D , and linear scan
all the non-zeros. Therefore, generating sketches for A ∈ Rn×D costs O(Pn
i=1 fi ), where fi is the
number of non-zeros in the ith row, i.e., fi = |Pi |. In the estimation stage, we need to linear scan the
sketches. While the conditional sample size Ds might be large, the cost for estimating the distance
between one pair of data points would be only O(k1 + k2 ) instead of O(Ds ).

3 The Theoretical Variance Analysis of CRS

DsVar ( ˜u1,1 ˜u2,1 ) =

We give some theoretical analysis on the variances of CRS. For simplicity, we ignore the “
ﬁnite
population correction factor ”, D−Ds
D−1 , due to “sample-without-replacement.”
Ds PDs
We ﬁrst consider ˆaM F = D
j=1 ˜u1,j ˜u2,j . By assuming “sample-with-replacement,” the samples,
( ˜u1,j ˜u2,j ), j = 1 to Ds , are i.i.d, conditional on Ds . Thus,
Ds «2
Var(ˆaM F |Ds ) = „ D
D `E ( ˜u1,1 ˜u2,1 )2 − E2 ( ˜u1,1 ˜u2,1 )´ ,
D
D
1
1
a
Xi=1
Xi=1
E ( ˜u1,1 ˜u2,1 )2 =
(u1,i u2,i )2 ,
E ( ˜u1,1 ˜u2,1 ) =
(u1,iu2,i ) =
D
D
D
D ”2! =
Ds   D
D   1
D
D
(u1,i u2,i )2 − “ a
D
Xi=1
Xi=1
u2
1,iu2
2,i −
Ds
D
The unconditional variance would be simply
Ds «   D
Var(ˆaM F ) = E (Var(ˆaM F |Ds )) = E „ D
Xi=1

Var(ˆaM F |Ds ) =

u2
1,i u2
2,i −

D ! .
a2

D ! ,
a2

D
Ds

,

(4)

(5)

(6)

,

,

(7)

as Var( ˆX ) = E “Var( ˆX |Ds )” + Var “E( ˆX |Ds )” = E “Var( ˆX |Ds )”, when ˆX is conditionally unbiased.
Ds (cid:17) ≥ max (cid:16) f1
Ds (cid:17); but we know E (cid:16) D
k2 (cid:17) (similar
No closed-form expression is known for E (cid:16) D
, f2
k1
to Jensen’s inequality). Asymptotically (as k1 and k2 increase), the inequality becomes an equality
E (cid:18) D
k2 (cid:19) ,
k2 (cid:19) ≈ max (cid:18) f1
Ds (cid:19) ≈ max (cid:18) f1 + 1
f2 + 1
f2
k1
k1
where f1 and f2 are the numbers of non-zeros in u1 and u2 , respectively. See [13] for the proof.
Extensive simulations in [13] verify that the errors of (7) are usually within 5% when k1 , k2 > 20.
M F and ˆd(1)
We similarly derive the variances for ˆd(2)
M F . In a summary, we obtain (when k1 = k2 = k)
k  D
D ! ≈
Ds «   D
2,i − a2! ,
D
a2
Var (ˆaM F ) = E „ D
1
Xi=1
Xi=1
u2
1,iu2
1,i u2
u2
2,i −
[d(2) ]2
M F ” = E „ D
Ds « „d(4) −
D « ≈
1
max(f1 , f2 )
Var “ ˆd(2)
k “Dd(4) − [d(2) ]2” ,
D
[d(1) ]2
Ds « „d(2) −
D « ≈
M F ” = E „ D
1
k “Dd(2) − [d(1) ]2” .
Var “ ˆd(1)
where we denote d(4) = PD
i=1 (u1,i − u2,i )4 .
The sparsity term max(f1 ,f2 )
reduces the variances signiﬁcantly. If max(f1 ,f2 )
= 0.01, the variances
D
D
can be reduced by a factor of 100, compared to conventional random coordinate sampling.

max(f1 , f2 )
D

max(f1 , f2 )
D

(10)

(9)

(8)

4 A Brief Introduction to Random Projections
We give a brief introduction to random projections, with which we compare CRS. (Normal) Random
projections [17] are widely used in learning and data mining [2 –4].

Random projections multiply the data matrix A ∈ Rn×D with a random matrix R ∈ RD×k to
generate a compact representation B = AR ∈ Rn×k . For estimating l2 distances, R typically
consists of i.i.d. entries in N (0, 1); hence we call it normal random projections. For l1 , R consists
of i.i.d. Cauchy C (0, 1) [9]. However, the recent impossibility result [5] has ruled out estimators
that could be metrics for dimension reduction in l1 .
Denote v1 , v2 ∈ Rk the two rows in B, corresponding to the original data points u1 , u2 ∈ RD . We
also introduce the notation for the marginal l2 norms: m1 = ku1k2 , m2 = ku2k2 .

4.1 Normal Random Projections

In this case, R consists of i.i.d. N (0, 1). It is easy to show that the following linear estimators of
the inner product a and the squared l2 distance d(2) are unbiased
1
1
k
k

ˆd(2)
N RP,M F =

kv1 − v2k2 ,

ˆaN RP,M F =

vT
1 v2 ,

(11)

with variances [15, 17]

Var (ˆaN RP,M F ) =

2[d(2) ]2
1
Var (cid:16) ˆd(2)
N RP,M F (cid:17) =
k (cid:0)m1m2 + a2 (cid:1) ,
k
Assuming that the margins m1 = ku1k2 and m2 = ku2k2 are known, [15] provides a maximum
likelihood estimator, denoted by ˆaN RP,M LE , whose (asymptotic) variance is
(m1m2 − a2 )2
m1m2 + a2 + O(k−2 ).

Var (ˆaN RP,M LE ) =

(12)

(13)

1
k

.

4.2 Cauchy Random Projections for Dimension Reduction in l1
In this case, R consisting of i.i.d. entries in Cauchy C (0, 1). [9] proposed an estimator based on the
absolute sample median. Recently, [14] proposed a variety of nonlinear estimators, including, a bias-
corrected sample median estimator, a bias-corrected geometric mean estimator, and a bias-corrected

k

+

−

(14)

ˆd(1)
CRP ,M LE

[14] shows that

maximum likelihood estimator. An analog of the Johnson-Lindenstrauss (JL) lemma for dimension
reduction in l1 is also proved in [14], based on the bias-corrected geometric mean estimator.
We only list the maximum likelihood estimator derived in [14], because it is the most accurate one.
CRP,M LE ,c = (cid:18)1 −
k (cid:19) ˆd(1)
1
ˆd(1)
CRP,M LE ,
where ˆd(1)
CRP,M LE solves a nonlinear MLE equation
2 ˆd(1)
k
Xj=1
CRP ,M LE
CRP ,M LE ”2 = 0.
(v1,j − v2,j )2 + “ ˆd(1)
3[d(1) ]2
2[d(1) ]2
k2 + O (cid:18) 1
k3 (cid:19) .
Var (cid:16) ˆd(1)
CRP,M LE ,c(cid:17) =
k
4.3 General Stable Random Projections for Dimension Reduction in lp (0 < p ≤ 2)
[10] generalized the bias-corrected geometric mean estimator to general stable random projections
for dimension reduction in lp (0 < p ≤ 2), and provided the theoretical variances and exponential
tail bounds. Of course, CRS can also be applied to approximating any lp distances.
5 Improving CRS Using Marginal Information
It is often reasonable to assume that we know the marginal information such as marginal l2 norms,
numbers of non-zeros, or even marginal histograms. This often leads to (much) sharper estimates,
by maximizing the likelihood under marginal constraints. In the boolean data case, we can express
the MLE solution explicitly and derive a closed-form (asymptotic) variance. In general real-valued
data, the joint likelihood is not available; we propose an approximate MLE solution.

(16)

(15)

+

5.1 Boolean (0/1) Data
In 0/1 data, estimating the inner product becomes estimating a two-way contingency table, which
has four cells. Because of the margin constraints, there is only one degree of freedom. Therefore, it
is not hard to show that the MLE of a is the solution, denoted by ˆa0/1,M LE , to a cubic equation
s11
s00
s01
s10
a
f1 − a
f2 − a
D − f1 − f2 + a
where s11 = #{j : ˜u1,j = ˜u2,j = 1}, s10 = #{j : ˜u1,j = 1, ˜u2,j = 0}, s01 = #{j : ˜u1,j =
0, ˜u2,j = 1}, s00 = #{j : ˜u1,j = 0, ˜u2,j = 0}, j = 1, 2, ..., Ds .
The (asymptotic) variance of ˆa0/1,M LE is proved [11 –13] to be
Var(ˆa0/1,M LE ) = E (cid:18) D
Ds (cid:19)

1
1
f1−a + 1
a + 1
f2−a +

1
D−f1−f2+a

= 0,

(18)

(17)

−

+

−

.

5.2 Real-valued Data
A practical solution is to assume some parametric form of the (bivariate) data distribution based
on prior knowledge; and then solve an MLE considering various constraints. Suppose the samples
( ˜u1,j , ˜u2,j ) are i.i.d. bivariate normal with moments determined by the population moments, i.e.,
˜u2,j − ¯u2 – ∼ N „» 0
0 – , ˜Σ« ,
˜v2,j – = » ˜u1,j − ¯u1
» ˜v1,j
uT
D » ku1 k2 − D ¯u2
¨m2 – ,
Ds » ¨m1
2 – =
1
1
Ds
¨a
1u2 − D ¯u1 ¯u2
˜Σ =
1
uT
ku2 k2 − D ¯u2
¨a
1 u2 − D ¯u1 ¯u2
Ds
i=1 u1,i/D , ¯u2 = PD
where ¯u1 = PD
i=1 u2,i/D are the population means.
¨m1 =
D (cid:0)uT
Ds
1(cid:1), ¨m2 = Ds
D (cid:0)ku2k2 − D ¯u22(cid:1), ¨a = Ds
1 u2 − D ¯u1 ¯u2(cid:1). Suppose that ¯u1 ,
D (cid:0)ku1k2 − D ¯u2
¯u2 , m1 = ku1k2 and m2 = ku2k2 are known, an MLE for a = uT
1 u2 , denoted by ˆaM LE ,N , is
D
ˆ¨a + D ¯u1 ¯u2 ,
(21)
ˆaM LE ,N =
Ds

(19)

(20)

where, similar to Lemma 2 of [15], ˆ¨a is the solution to a cubic equation:
1 ˜v2 (cid:1) + ¨a (cid:0)− ¨m1 ¨m2 + ¨m1 k˜v2k2 + ¨m2k˜v1k2 (cid:1) − ¨m1 ¨m2 ˜vT
¨a3 − ¨a2 (cid:0)˜vT
1 ˜v2 = 0.
ˆaM LE ,N is fairly robust, although sometimes we observe the biases are quite noticeable. In general,
this is a good bias-variance trade-off (especially when k is not too large). Intuitively, the reason why
this (seemly crude) assumption of bivariate normality works well is because, once we have ﬁxed the
margins, we have removed to a large extent the non-normal component of the data.

(22)

6 Theoretical Comparisons of CRS With Random Projections
As re ﬂected by their variances, for general data types, whet her CRS is better than random projec-
tions depends on two competing factors: data sparsity and data heavy-tailedness. However, in the
following two important scenarios, CRS outperforms random projections.

6.1 Boolean (0/1) data
In this case, the marginal norms are the same as the numbers of non-zeros, i.e., mi = kuik2 = fi .
Var(ˆaM F )
Figure 3 plots the ratio,
Var(ˆaN RP,M F ) , verifying that CRS is (considerably) more accurate:
Var (ˆaM F )
max(f1 , f2 )a
1
max(f1 , f2 )
f1f2 + a2 ≤ 1.
Var (ˆaN RP,M F )
a + 1
1
f1f2 + a2
D−a

=

≤

Figure 4 plots Var(ˆa0/1,M LE )
Var(ˆaN RP,M LE ) . In most possible range of the data, this ratio is less than 1. When
u1 and u2 are very close (e.g., a ≈ f2 ≈ f1 ), random projections appear more accurate. However,
when this does occur, the absolute variances are so small (even zero) that their ratio does not matter.

1

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

f2/f1 = 0.2

f1 = 0.05D

f1 = 0.95D

f2/f1 = 0.5

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

f2/f1 = 0.8

1

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

f2/f1 = 1

1

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

0
0

0
0

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

a/f2
a/f2
a/f2
a/f2
Var(ˆaM F )
Var(ˆaN RP,M F ) , show that CRS has smaller variances than random
Figure 3: The variance ratios,
projections, when no marginal information is used. We let f1 ≥ f2 and f2 = αf1 with α =
0.2, 0.5, 0.8, 1.0. For each α, we plot from f1 = 0.05D to f1 = 0.95D spaced at 0.05D .

1

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

f2/f1 = 0.2

f
 = 0.05 D
1

f2/f1 = 0.5

1

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

f2/f1 = 0.8

1

0.8

0.6

0.4

0.2

o
i
t
a
r
 
e
c
n
a
i
r
a
V

f2/f1 = 1

3

2.5

2

1.5

1

0.5

o
i
t
a
r
 
e
c
n
a
i
r
a
V

0
0

f
 = 0.95 D
1
0.8

0
0

1

0.6

0.2

0.4

0.4
a/f2
a/f2
a/f2
a/f2
Figure 4: The ratios, Var(ˆa0/1,M LE )
Var(ˆaN RP,M LE ) , show that CRS usually has smaller variances than random
projections, except when f1 ≈ f2 ≈ a.

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.2

0.2

0.6

0.8

1

1

6.2 Nearly Independent Data
Suppose two data points u1 and u2 are independent (or less strictly, uncorrelated to the second
order), it is easy to show that the variance of CRS is always smaller:
m1m2 + a2
m1m2
max(f1 , f2 )
Var (ˆaM F ) ≤
≤ Var (ˆaN RP,M F ) =
k
D
k
even if we ignore the data sparsity. Therefore, CRS will be much better for estimating inner products
in nearly independent data. Once we have obtained the inner products, we can infer the l2 distances
easily by d(2) = m1 + m2 − 2a, since the margins, m1 and m2 , are easy to obtain exactly.
In high dimensions, it is often the case that most of the data points are only very weakly correlated.

(23)

,

0
0

0
0

0
0

0
0

6.3 Comparing the Computational Ef ﬁciency
As previously mentioned, the cost of constructing sketches for A ∈ Rn×D would be O(nD) (or
more precisely, O(Pn
i=1 fi )). The cost of (normal) random projections would be O(nDk), which
can be reduced to O(nDk/3) using sparse random projections [1]. Therefore, it is possible that
CRS is considerably more efﬁcient than random projections i n the sampling stage.2
In the estimation stage, CRS costs O(2k) to compute the sample distance for each pair. This cost is
only O(k) in random projections. Since k is very small, the difference should not be a concern.

7 Empirical Evaluations
We compare CRS with random projections (RP) using real data, including n = 100 randomly
sampled documents from the NSF data [7] (sparsity ≈ 1%), n = 100 documents from the NEWS-
GROUP data [4] (sparsity ≈ 1%), and one class of the COREL image data (n = 80, sparsity ≈ 5%).
We estimate all pairwise inner products, l1 and l2 distances, using both CRS and RP. For each pair,
we obtain 50 runs and average the absolute errors. We compare the median errors and the percentage
in which CRS does better than random projections.

The results are presented in Figures 5, 6, 7. In each panel, the dashed curve indicates that we sample
each data point with equal sample size (k). For CRS, we can adjust the sample size according to
the sparsity, re ﬂected by the solid curves. We adjust sample sizes only roughly. The data points are
divided into 3 groups according to sparsity. Data in different groups are assigned different sample
sizes for CRS. For random projections, we use the average sample size.

For both NSF and NEWSGROUP data, CRS overwhelmingly outperforms RP for estimating inner
products and l2 distances (both using the marginal information). CRS also outperforms RP for
approximating l1 and l2 distances (without using the margins).
For the COREL data, CRS still outperforms RP for approximating inner products and l2 distances
(using the margins). However, RP considerably outperforms CRS for approximating l1 distances
and l2 distances (without using the margins). Note that the COREL image data are not too sparse
and are considerably more heavy-tailed than the NSF and NEWSGROUP data [13].

s
r
o
r
r
e
 
n
a
i
d
e
m
 
f
o
 
o
i
t
a
R

0.06

0.05

0.04

0.03

0.02
10

1

e
g
a
t
n
e
c
r
e
P

0.9995

0.999

Inner product

0.8

0.6

0.4

L1 distance

L2 distance

1

0.8

0.6

0.12

0.1

0.08

0.06

0.04

L2 distance (Margins)

20
40
30
Sample size k

0.2
10

50

20
40
30
Sample size k

50

10

20
40
30
Sample size k

0.02
10

50

20
40
30
Sample size k

50

Inner product

1

0.98

0.96

L1 distance

L2 distance

1

0.8

0.6

0.4

L2 distance (Margins)

1

0.9998

0.9996

0.9994

0.9985
10

0.94
10

50

0.2
10

50

20
40
30
20
40
30
20
40
30
20
40
30
Sample size k
Sample size k
Sample size k
Sample size k
Figure 5: NSF data. Upper four panels: ratios (CRS over RP ( random projections)) of the median
absolute errors; values < 1 indicate that CRS does better. Bottom four panels: percentage of pairs
for which CRS has smaller errors than RP; values > 0.5 indicate that CRS does better. Dashed
curves correspond to ﬁxed sample sizes while solid curves in dicate that we (crudely) adjust sketch
sizes in CRS according to data sparsity. In this case, CRS is overwhelmingly better than RP for
approximating inner products and l2 distances (both using margins).

10

50

50

8 Conclusion
There are many applications of l1 and l2 distances on large sparse datasets. We propose a new
sketch-based method, Conditional Random Sampling (CRS), which is provably better than random
projections, at least for the important special cases of boolean data and nearly independent data. In
general non-boolean data, CRS compares favorably, both theoretically and empirically, especially
when we take advantage of the margins (which are easier to compute than distances).
2 [16] proposed very sparse random projections to reduce the cost O(nDk) down to O(n√Dk).

s
r
o
r
r
e
 
n
a
i
d
e
m
 
f
o
 
o
i
t
a
R

0.2

0.15

0.1

0.05
10

1

e
g
a
t
n
e
c
r
e
P

0.995

0.99

Inner product

0.7

0.6

0.5

0.4

L1 distance

1.2

1

0.8

L2 distance

0.2

0.15

0.1

L2 distance (Margins)

20
Sample size k

0.3
10

30

20
Sample size k

30

0.6
10

20
Sample size k

0.05
10

30

20
Sample size k

30

Inner product

1

0.95

1

0.8

0.6

0.4

L1 distance

L2 distance

1

0.995

L2 distance (Margins)

0.985
10

0.9
10

20
20
20
20
Sample size k
Sample size k
Sample size k
Sample size k
Figure 6: NEWSGROUP data. The results are quite similar to those in Figure 5 for the NSF data.
In this case, it is more obvious that adjusting sketch sizes helps CRS.

30

30

30

0.2
10

0.99
10

30

s
r
o
r
r
e
 
n
a
i
d
e
m
 
f
o
 
o
i
t
a
R

0.3

0.29

0.28

0.27

0.26
10

0.9

e
g
a
t
n
e
c
r
e
P

0.85

0.8

Inner product

2

1.9

1.8

1.7

L1 distance

20
40
30
Sample size k

50

10

20
40
30
Sample size k

50

Inner product

0.04

0.03

0.02

0.01

L1 distance

4.5

4

3.5

3
10

0.1

0.05

0

−0.05

L2 distance (Margins)

L2 distance

0.8

0.7

0.6

0.5

0.4

20
40
30
Sample size k

50

10

20
40
30
Sample size k

50

L2 distance

0.9

0.8

0.7

0.6

L2 distance (Margins)

0.75
10

20
40
30
Sample size k

50

0
10

−0.1
10

50

20
40
30
20
40
30
Sample size k
Sample size k
Figure 7: COREL image data.

0.5
10

50

20
40
30
Sample size k

50

Acknowledgment
We thank Chris Burges, David Heckerman, Chris Meek, Andrew Ng, Art Owen, Robert Tibshirani,
for various helpful conversations, comments, and discussions. We thank Ella Bingham, Inderjit
Dhillon, and Matthias Hein for the datasets.
References
[1] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System
Sciences, 66(4):671–687, 2003.
[2] D. Achlioptas, F. McSherry, and B. Sch ¨olkopf. Sampling techniques for kernel methods. In NIPS, pages 335–342, 2001.
[3] R. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161–
182, 2006.
[4] E. Bingham and H. Mannila. Random projection in dimensionality reduction: Applications to image and text data. In KDD, pages
245–250, 2001.
[5] B. Brinkman and M. Charikar. On the impossibility of dimension reduction in l1 . Journal of ACM, 52(2):766–788, 2005.
[6] A. Broder. On the resemblance and containment of documents. In the Compression and Complexity of Sequences, pages 21–29, 1997.
[7]
I. Dhillon and D. Modha. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1-2):143–175, 2001.
[8] P. Drineas and M. Mahoney. On the nystrom method for approximating a gram matrix for improved kernel-based learning. Journal of
Machine Learning Research, 6(Dec):2153–2175, 2005.
[9] P. Indyk. Stable distributions, pseudorandom generators, embeddings and data stream computation. In FOCS, pages 189–197, 2000.
[10] P. Li. Very sparse stable random projections, estimators and tail bounds for stable random projections. Technical report, http:
//arxiv.org/PS cache/cs/pdf/0611/0611114.pdf, 2006.
[11] P. Li and K. Church. Using sketches to estimate associations. In HLT/EMNLP, pages 708–715, 2005.
[12] P. Li and K. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, To Appear.
[13] P. Li, K. Church, and T. Hastie. Conditional random sampling: A sketched-based sampling technique for sparse data. Technical Report
2006-08, Department of Statistics, Stanford University), 2006.
[14] P. Li, K. Church, and T. Hastie. Nonlinear estimators and tail bounds for dimensional reduction in l1 using Cauchy random projections.
(http://arxiv.org/PS cache/cs/pdf/0610/0610155.pdf), 2006.
[15] P. Li, T. Hastie, and K. Church. Improving random projections using marginal information. In COLT, pages 635–649, 2006.
[16] P. Li, T. Hastie, and K. Church. Very sparse random projections. In KDD, pages 287–296, 2006.
[17] S. Vempala. The Random Projection Method. American Mathematical Society, Providence, RI, 2004.


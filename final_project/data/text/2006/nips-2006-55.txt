Bayesian Policy Gradient Algorithms

Yaakov Engel
Mohammad Ghavamzadeh
Department of Computing Science, University of Alberta
Edmonton, Alberta, Canada T6E 4Y8
fmgh,yakig@cs.ualberta.ca

Abstract

Policy gradient methods are reinforcement learning algorithms that adapt a param-
eterized policy by following a performance gradient estimate. Conventional pol-
icy gradient methods use Monte-Carlo techniques to estimate this gradient. Since
Monte Carlo methods tend to have high variance, a large number of samples is
required, resulting in slow convergence.
In this paper, we propose a Bayesian
framework that models the policy gradient as a Gaussian process. This reduces
the number of samples needed to obtain accurate gradient estimates. Moreover,
estimates of the natural gradient as well as a measure of the uncertainty in the
gradient estimates are provided at little extra cost.

1 Introduction

Policy Gradient (PG) methods are Reinforcement Learning (RL) algorithms that maintain a param-
eterized action-selection policy and update the policy parameters by moving them in the direction
of an estimate of the gradient of a performance measure. Early examples of PG algorithms are the
class of REINFORCE algorithms of Williams [1] which are suitable for solving problems in which
the goal is to optimize the average reward. Subsequent work (e.g., [2, 3]) extended these algorithms
to the cases of in(cid:2)nite-horizon Markov decision processes (MDPs) and partially observable MDPs
(POMDPs), and provided much needed theoretical analysis. However, both the theoretical results
and empirical evaluations have highlighted a major shortcoming of these algorithms, namely, the
high variance of the gradient estimates. This problem may be traced to the fact that in most interest-
ing cases, the time-average of the observed rewards is a high-variance (although unbiased) estimator
of the true average reward, resulting in the sample-inef(cid:2)ciency of these algorithms.
One solution proposed for this problem was to use a small (i.e., smaller than 1) discount factor in
these algorithms [2, 3], however, this creates another problem by introducing bias into the gradient
estimates. Another solution, which does not involve biasing the gradient estimate, is to subtract
a reinforcement baseline from the average reward estimate in the updates of PG algorithms (e.g.,
[4, 1]). Another approach for speeding-up policy gradient algorithms was recently proposed in [5]
and extended in [6, 7]. The idea is to replace the policy-gradient estimate with an estimate of the
so-called natural policy-gradient. This is motivated by the requirement that a change in the way the
policy is parametrized should not in(cid:3)uence the result of the policy update. In terms of the policy
update rule, the move to a natural-gradient rule amounts to linearly transforming the gradient using
the inverse Fisher information matrix of the policy.
However, both conventional and natural policy gradient methods rely on Monte-Carlo (MC) tech-
niques to estimate the gradient of the performance measure. Monte-Carlo estimation is a frequentist
procedure, and as such violates the likelihood principle [8].1 Moreover, although MC estimates are
unbiased, they tend to produce high variance estimates, or alternatively, require excessive sample
sizes (see [9] for a discussion).

1The likelihood principle states that in a parametric statistical model, all the information about a data sample
that is required for inferring the model parameters is contained in the likelihood function of that sample.

In [10] a Bayesian alternative to MC estimation is proposed. The idea is to model integrals of
the form R f (x)p(x)dx as Gaussian Processes (GPs). This is done by treating the (cid:2)rst term f in
the integrand as a random function, the randomness of which re(cid:3)ects our subjective uncertainty
concerning its true identity. This allows us to incorporate our prior knowledge on f into its prior
distribution. Observing (possibly noisy) samples of f at a set of points (x1 ; x2 ; : : : ; xM ) allows us
to employ Bayes’ rule to compute a posterior distribution of f , conditioned on these samples. This,
in turn, induces a posterior distribution over the value of the integral. In this paper, we propose a
Bayesian framework for policy gradient, by modeling the gradient as a GP. This reduces the number
of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient
and the gradient covariance are provided at little extra cost.

2 Reinforcement Learning and Policy Gradient Methods
Reinforcement Learning (RL) [11, 12] is a class of learning problems in which an agent inter-
acts with an unfamiliar, dynamic and stochastic environment, where the agent’s goal is to optimize
some measure of its long-term performance. This interaction is conventionally modeled as a MDP.
Let P (S ) be the set of probability distributions on (Borel) subsets of a set S . A MDP is a tuple
(X ; A; q ; P; P0 ) where X and A are the state and action spaces, respectively; q((cid:1)ja; x) 2 P (R) is the
probability distribution over rewards; P ((cid:1)ja; x) 2 P (X ) is the transition probability distribution; (we
assume that P and q are stationary); and P0 ((cid:1)) 2 P (X ) is the initial state distribution. We denote the
random variable distributed according to q((cid:1)ja; x) as r(x; a). In addition, we need to specify the rule
according to which the agent selects actions at each possible state. We assume that this rule does
not depend explicitly on time. A stationary policy (cid:22)((cid:1)jx) 2 P (A) is a probability distribution over
actions, conditioned on the current state. The MDP controlled by the policy (cid:22) induces a Markov
chain over state-action pairs. We generically denote by (cid:24) = (x0 ; a0 ; x1 ; a1 ; : : : ; xT (cid:0)1 ; aT (cid:0)1 ; xT ) a
path generated by this Markov chain. The probability (or density) of such a path is given by

Pr((cid:24) j(cid:22)) = P0 (x0 )

(cid:22)(at jxt )P (xt+1 jxt ; at ):

T (cid:0)1Yt=0
We denote by R((cid:24) ) = PT
t=0 (cid:13) t r(xt ; at ) the (possibly discounted, (cid:13) 2 [0; 1]) cumulative return of the
path (cid:24) . R((cid:24) ) is a random variable both because the path (cid:24) is a random variable, and because even for
a given path, each of the rewards sampled in it may be stochastic. The expected value of R((cid:24) ) for a
given (cid:24) is denoted by (cid:22)R((cid:24) ). Finally, let us de(cid:2)ne the expected return,
(cid:17)((cid:22)) = E(R((cid:24) )) = Z (cid:22)R((cid:24) ) Pr((cid:24) j(cid:22))d(cid:24) :
Gradient-based approaches to policy search in RL have recently received much attention as a means
to sidetrack problems of partial observability and of policy oscillations and even divergence encoun-
tered in value-function based methods (see [11], Sec. 6.4.2 and 6.5.3). In policy gradient (PG) meth-
ods, we de(cid:2)ne a class of smoothly parameterized stochastic policies f(cid:22)((cid:1)jx; (cid:18)); x 2 X ; (cid:18) 2 (cid:2)g, es-
timate the gradient of the expected return (2) with respect to the policy parameters (cid:18) from observed
system trajectories, and then improve the policy by adjusting the parameters in the direction of the
gradient [1, 2, 3]. The gradient of the expected return (cid:17)((cid:18)) = (cid:17)((cid:22)((cid:1)j(cid:1); (cid:18))) is given by 2
r(cid:17)((cid:18)) = Z (cid:22)R((cid:24) )
Pr((cid:24) ;(cid:18)) = r log Pr((cid:24) ; (cid:18)) is known as the score
where Pr((cid:24) ; (cid:18)) = Pr((cid:24) j(cid:22)((cid:1)j(cid:1); (cid:18))). The quantity r Pr((cid:24) ;(cid:18))
function or likelihood ratio. Since the initial state distribution P0 and the transition distribution P
are independent of the policy parameters (cid:18) , we can write the score of a path (cid:24) using Eq. 1 as

r Pr((cid:24) ; (cid:18))
Pr((cid:24) ; (cid:18))

Pr((cid:24) ; (cid:18))d(cid:24) ;

(2)

(1)

(3)

T (cid:0)1Xt=0
T (cid:0)1Xt=0
2Throughout the paper, we use the notation r to denote r(cid:18) (cid:150) the gradient w.r.t. the policy parameters.

r(cid:22)(at jxt ; (cid:18))
(cid:22)(at jxt ; (cid:18))

r Pr((cid:24) ; (cid:18))
Pr((cid:24) ; (cid:18))

r log (cid:22)(at jxt ; (cid:18)):

u((cid:24) ) =

=

=

(4)

Previous work on policy gradient methods used classical Monte-Carlo to estimate the gradient in
Eq. 3. These methods generate i.i.d. sample paths (cid:24)1 ; : : : ; (cid:24)M according to Pr((cid:24) ; (cid:18)), and estimate
the gradient r(cid:17)((cid:18)) using the MC estimator
MXi=1
cr(cid:17)M C ((cid:18)) =
3 Bayesian Quadrature

R((cid:24)i )r log Pr((cid:24)i ; (cid:18)) =

r log (cid:22)(at;i jxt;i ; (cid:18)):

Ti(cid:0)1Xt=0

R((cid:24)i )

(5)

1
M

1
M

MXi=1

Bayesian quadrature (BQ) [10] is a Bayesian method for evaluating an integral using samples of its
integrand. We consider the problem of evaluating the integral
(cid:26) = Z f (x)p(x)dx:
(6)
If p(x) is a probability density function, this becomes the problem of evaluating the expected value
of f (x). In MC estimation of such expectations, samples (x1 ; x2 ; : : : ; xM ) are drawn from p(x),
and the integral is estimated as ^(cid:26)M C = 1
i=1 f (xi ). ^(cid:26)M C is an unbiased estimate of (cid:26), with
M PM
variance that diminishes to zero as M ! 1. However, as O’Hagan points out, MC estimation is
fundamentally unsound, as it violates the likelihood principle, and moreover, does not make full use
of the data at hand [9] .
The alternative proposed in [10] is based on the following reasoning: In the Bayesian approach, f ((cid:1))
is random simply because it is numerically unknown. We are therefore uncertain about the value
of f (x) until we actually evaluate it. In fact, even then, our uncertainty is not always completely
removed, since measured samples of f (x) may be corrupted by noise. Modeling f as a Gaussian
process (GP) means that our uncertainty is completely accounted for by specifying a Normal prior
distribution over functions. This prior distribution is speci(cid:2)ed by its mean and covariance, and is
denoted by f ((cid:1)) (cid:24) N ff0 ((cid:1)); k((cid:1); (cid:1))g. This is shorthand for the statement that f is a GP with prior mean
E(f (x)) = f0 (x) and covariance Cov(f (x); f (x0 )) = k(x; x0 ), respectively. The choice of kernel
function k allows us to incorporate prior knowledge on the smoothness properties of the integrand
into the estimation procedure. When we are provided with a set of samples DM = f(xi ; yi )gM
i=1 ,
where yi is a (possibly noisy) sample of f (xi ), we apply Bayes’ rule to condition the prior on these
sampled values. If the measurement noise is normally distributed, the result is a Normal posterior
distribution of f jDM . The expressions for the posterior mean and covariance are standard:

E(f (x)jDM ) = f0 (x) + kM (x)>
CM (yM (cid:0) f 0 );
Cov(f (x); f (x0 )jDM ) = k(x; x0 ) (cid:0) kM (x)>
CM kM (x0 ):
Here and in the sequel, we make use of the de(cid:2)nitions:

(7)

f 0 = (f0 (x1 ); : : : ; f0 (xM ))> ; yM = (y1 ; : : : ; yM )> ;
; CM = (KM + (cid:6)M )(cid:0)1 ;
kM (x) = (k(x1 ; x); : : : ; k(xM ; x))> ;
[KM ]i;j = k(xi ; xj )
and [(cid:6)M ]i;j is the measurement noise covariance between the ith and j th samples. Typically, it
is assumed that the measurement noise is i.i.d., in which case (cid:6)M = (cid:27)2 I , where (cid:27)2 is the noise
variance and I is the identity matrix.
Since integration is a linear operation, the posterior distribution of the integral in Eq. 6 is also
Gaussian, and the posterior moments are given by
E((cid:26)jDM ) = Z E(f (x)jDM )p(x)dx ; Var((cid:26)jDM ) = ZZ Cov(f (x); f (x0 )jDM )p(x)p(x0 )dxdx0 : (8)
Substituting Eq. 7 into Eq. 8, we get

>
E((cid:26)jDM ) = (cid:26)0 + z
M CM (yM (cid:0) f 0 )
where we made use of the de(cid:2)nitions:
(cid:26)0 = Z f0 (x)p(x)dx ;
z0 = ZZ k(x; x0 )p(x)p(x0 )dxdx0 :
zM = Z kM (x)p(x)dx ;
Note that (cid:26)0 and z0 are the prior mean and variance of (cid:26), respectively.

Var((cid:26)jDM ) = z0 (cid:0) z

;

>
M CM zM ;

(9)

(10)

Model 1
Known part
p((cid:24) ; (cid:18)) = Pr((cid:24) ; (cid:18))
Uncertain part
f ((cid:24) ; (cid:18)) = (cid:22)R((cid:24) )r log Pr((cid:24) ; (cid:18))
Measurement
y((cid:24) ) = R((cid:24) )r log Pr((cid:24) ; (cid:18))
Prior mean of f
E(f ((cid:24) ; (cid:18))) = 0
Prior cov. of f
Cov(f ((cid:24) ; (cid:18)); f ((cid:24) 0 ; (cid:18))) = k((cid:24) ; (cid:24) 0 )I
E(r(cid:17)B ((cid:18))jDM ) =
Y M CM zM
Cov(r(cid:17)B ((cid:18))jDM ) = (z0 (cid:0) z>
M CM zM )I
k((cid:24)i ; (cid:24)j ) = (cid:0)1 + u((cid:24)i )>G(cid:0)1u((cid:24)j )(cid:1)2
Kernel function
(zM )i = 1 + u((cid:24)i )>G(cid:0)1u((cid:24)i )
zM
z0 = 1 + n
z0

Model 2
p((cid:24) ; (cid:18)) = r Pr((cid:24) ; (cid:18))
f ((cid:24) ) = (cid:22)R((cid:24) )
y((cid:24) ) = R((cid:24) )
E(f ((cid:24) )) = 0
Cov(f ((cid:24) ); f ((cid:24) 0 )) = k((cid:24) ; (cid:24) 0 )
ZM CM yM
Z 0 (cid:0) ZM CM Z >
M
k((cid:24)i ; (cid:24)j ) = u((cid:24)i )>G(cid:0)1u((cid:24)j )
ZM = U M
Z 0 = G (cid:0) U M CM U >
M

Table 1: Summary of the Bayesian policy gradient Models 1 and 2.

In order to prevent the problem from (cid:147)degenerating into in(cid:2)nite regress(cid:148), as phrased by O’Hagan
[10], we should choose the functions p, k , and f0 so as to allow us to solve the integrals in Eq. 10
analytically. For instance, O’Hagan provides the analysis required for the case where the integrands
in Eq. 10 are products of multivariate Gaussians and polynomials, referred to as Bayes-Hermite
quadrature. One of the contributions of the present paper is in providing analogous analysis for
kernel functions that are based on the Fisher kernel [13, 14]. It is important to note that in MC
estimation, samples must be drawn from the distribution p(x), whereas in the Bayesian approach,
samples may be drawn from arbitrary distributions. This affords us with (cid:3)exibility in the choice of
sample points, allowing us, for instance to actively design the samples (x1 ; x2 ; : : : ; xM ).

4 Bayesian Policy Gradient

In this section, we use Bayesian quadrature to estimate the gradient of the expected return with
respect to the policy parameters, and propose Bayesian policy gradient (BPG) algorithms. In the
frequentist approach to policy gradient our performance measure was (cid:17)((cid:18)) from Eq. 2, which is the
result of averaging the cumulative return R((cid:24) ) over all possible paths (cid:24) and all possible returns accu-
mulated in each path. In the Bayesian approach we have an additional source of randomness, which
is our subjective Bayesian uncertainty concerning the process generating the cumulative returns. Let
us denote
(cid:17)B ((cid:18)) = Z R((cid:24) ) Pr((cid:24) ; (cid:18))d(cid:24) :
(11)
(cid:17)B ((cid:18)) is a random variable both because of the noise in R((cid:24) ) and the Bayesian uncertainty. Under
the quadratic loss, our Bayesian performance measure is E((cid:17)B ((cid:18))jDM ). Since we are interested
in optimizing performance rather than evaluating it, we evaluate the posterior distribution of the
gradient of (cid:17)B ((cid:18)). For the mean we have
rE ((cid:17)B ((cid:18))jDM ) = E (r(cid:17)B ((cid:18))jDM ) = E (cid:18)Z R((cid:24) )
Pr((cid:24) ; (cid:18))d(cid:24) jDM (cid:19) :
r Pr((cid:24) ; (cid:18))
(12)
Pr((cid:24) ; (cid:18))
Consequently, in BPG we cast the problem of estimating the gradient of the expected return in
the form of Eq. 6. As described in Sec. 3, we partition the integrand into two parts, f ((cid:24) ; (cid:18)) and
p((cid:24) ; (cid:18)). We will place the GP prior over f and assume that p is known. We will then proceed by
calculating the posterior moments of the gradient r(cid:17)B ((cid:18)) conditioned on the observed data. Next,
we investigate two different ways of partitioning the integrand in Eq. 12, resulting in two distinct
Bayesian models. Table 1 summarizes the two models we use in this work. Our choice of Fisher-type
kernels was motivated by the notion that a good representation should depend on the data generating
process (see [13, 14] for a thorough discussion). Our particular choices of linear and quadratic Fisher
kernels were guided by the requirement that the posterior moments of the gradient be analytically
tractable. In Table 1 we made use of the following de(cid:2)nitions: F M = (f ((cid:24)1 ; (cid:18)); : : : ; f ((cid:24)M ; (cid:18))) (cid:24)
N (0; KM ), Y M = (y((cid:24)1 ); : : : ; y((cid:24)M )) (cid:24) N (0; KM + (cid:27)2 I ), U M = (cid:2)u((cid:24)1 ) ; u((cid:24)2 ) ; : : : ; u((cid:24)M )(cid:3),
ZM = R r Pr((cid:24) ; (cid:18))kM ((cid:24) )> d(cid:24) , and Z 0 = RR k((cid:24) ; (cid:24) 0 )r Pr((cid:24) ; (cid:18))r Pr((cid:24) 0 ; (cid:18))> d(cid:24)d(cid:24) 0 . Finally, n is the
number of policy parameters, and G = E (cid:0)u((cid:24) )u((cid:24) )> (cid:1) is the Fisher information matrix.
We can now use Models 1 and 2 to de(cid:2)ne algorithms for evaluating the gradient of the expected
return with respect to the policy parameters. The pseudo-code for these algorithms is shown in
Alg. 1. The generic algorithm (for either model) takes a set of policy parameters (cid:18) and a sample size
M as input, and returns an estimate of the posterior moments of the gradient of the expected return.

Algorithm 1 : A Bayesian Policy Gradient Evaluation Algorithm
1: BPG Eval((cid:18) ; M )
// policy parameters (cid:18) 2 Rn , sample size M > 0 //
2: Set G = G((cid:18))
; D0 = ;
3: for i = 1 to M do
4:
Sample a path (cid:24)i using the policy (cid:22)((cid:18))
5: Di = Di(cid:0)1 S f(cid:24)i g
Compute u((cid:24)i ) = PTi(cid:0)1
6:
t=0 r log (cid:22)(at jst ; (cid:18))
R((cid:24)i ) = PTi(cid:0)1
7:
t=0 r(st ; at )
8:
Update K i using K i(cid:0)1 and (cid:24)i
y((cid:24)i ) = R((cid:24)i )u((cid:24)i )
9:
(zM )i = 1 + u((cid:24)i )>G(cid:0)1u((cid:24)i )
10: end for
11: CM = (KM + (cid:27)2 I )(cid:0)1
12: Compute the posterior mean and covariance:
, Cov(r(cid:17)B ((cid:18))jDM ) = (z0 (cid:0) z>
E(r(cid:17)B ((cid:18))jDM ) = Y M CM zM
M CM zM )I
, Cov(r(cid:17)B ((cid:18))jDM ) = Z 0 (cid:0) ZM CM Z >
E(r(cid:17)B ((cid:18))jDM ) = ZM CM yM
M
, Cov (r(cid:17)B ((cid:18))jDM )
13: return E (r(cid:17)B ((cid:18))jDM )

or
y((cid:24)i ) = R((cid:24)i )
or ZM (:; i) = u((cid:24)i )

(Model 1)
(Model 2)

or

(Model 1)
(Model 1)

(Model 2)
(Model 2)

The kernel functions used in Models 1 and 2 are both based on the Fisher information matrix G((cid:18)).
Consequently, every time we update the policy parameters we need to recompute G. In Alg. 1 we
assume that G is known, however, in most practical situations this will not be the case. Let us brie(cid:3)y
outline two possible approaches for estimating the Fisher information matrix.
MC Estimation: At each step j , our BPG algorithm generates M sample paths using the current
policy parameters (cid:18) j in order to estimate the gradient r(cid:17)B ((cid:18)j ). We can use these generated sample
paths to estimate the Fisher information matrix G((cid:18) j ) by replacing the expectation in G with em-
i=1 Ti PM
i=1 PTi(cid:0)1
pirical averaging as ^GM C ((cid:18)j ) =
t=0 r log (cid:22)(at jxt ; (cid:18)j )r log (cid:22)(at jxt ; (cid:18)j )> :
1
PM
Model-Based Policy Gradient: The Fisher information matrix depends on the probability distri-
bution over paths. This distribution is a product of two factors, one corresponding to the current
policy, and the other corresponding to the MDP dynamics P0 and P (see Eq. 1). Thus, if the MDP
dynamics are known, the Fisher information matrix can be evaluated off-line. We can model the
MDP dynamics using some parameterized model, and estimate the model parameters using maxi-
mum likelihood or Bayesian methods. This would be a model-based approach to policy gradient,
which would allow us to transfer information between different policies.
Alg. 1 can be made signi(cid:2)cantly more ef(cid:2)cient, both in time and memory, by sparsifying the so-
lution. Such sparsi(cid:2)cation may be performed incrementally, and helps to numerically stabilize the
algorithm when the kernel matrix is singular, or nearly so. Here we use an on-line sparsi(cid:2)cation
method from [15] to selectively add a new observed path to a set of dictionary paths DM , which are
used as a basis for approximating the full solution. Lack of space prevents us from discussing this
method in further detail (see Chapter 2 in [15] for a thorough discussion).
The Bayesian policy gradient (BPG) algorithm is described in Alg. 2. This algorithm starts with an
initial vector of policy parameters (cid:18) 0 and updates the parameters in the direction of the posterior
mean of the gradient of the expected return, computed by Alg. 1. This is repeated N times, or
alternatively, until the gradient estimate is suf(cid:2)ciently close to zero.
Algorithm 2 : A Bayesian Policy Gradient Algorithm
1: BPG((cid:18)0 ; (cid:11); N ; M )
// initial policy parameters (cid:18) 0 , learning rates ((cid:11)j )N (cid:0)1
j=0 , number of policy updates
N > 0, BPG Eval sample size M > 0 //
2: for j = 0 to N (cid:0) 1 do
3: (cid:1)(cid:18) j = E (r(cid:17)B ((cid:18)j )jDM ) from BPG Eval((cid:18) j ; M )
4:
(regular gradient) or (cid:18) j+1 = (cid:18)j +(cid:11)j G(cid:0)1 ((cid:18)j )(cid:1)(cid:18)j
(cid:18) j+1 = (cid:18)j +(cid:11)j (cid:1)(cid:18)j
5: end for
6: return (cid:18)N

(natural gradient)

5 Experimental Results
In this section, we compare the BQ and MC gradient estimators in a continuous-action bandit prob-
lem and a continuous state and action linear quadratic regulation (LQR) problem. We also evaluate

the performance of the BPG algorithm (Alg. 2) on the LQR problem, and compare it with a standard
MC-based policy gradient (MCPG) algorithm.

5.1 A Bandit Problem
In this simple example, we compare the BQ and MC estimates of the gradient (for a (cid:2)xed set of
policy parameters) using the same samples. Our simple bandit problem has a single state and A = R.
Thus, each path (cid:24)i consists of a single action ai . The policy, and therefore also the distribution over
paths is given by a (cid:24) N ((cid:18)1 = 0; (cid:18)2
2 = 1). The score function of the path (cid:24) = a and the Fisher
information matrix are given by u((cid:24) ) = [a; a2 (cid:0) 1]> and G = diag(1; 2), respectively.
Table 2 shows the exact gradient of the expected return and its MC and BQ estimates (using 10
and 100 samples) for two versions of the simple bandit problem corresponding to two different
deterministic reward functions r(a) = a and r(a) = a2 . The average over 104 runs of the MC and
BQ estimates and their standard deviations are reported in Table 2. The true gradient is analytically
tractable and is reported as (cid:147)Exact(cid:148) in Table 2 for reference.

r(a) = a

Exact
MC (10)
BQ (10)
MC (100)
BQ (100)
0(cid:19)
(cid:0)0:0011 (cid:6) 0:977(cid:19)
0:0006 (cid:6) 0:060(cid:19)
0:0040 (cid:6) 0:317(cid:19)
0:000 (cid:6) 0:000004(cid:19)
(cid:18)1
(cid:18) 0:9950 (cid:6) 0:438
(cid:18)0:9856 (cid:6) 0:050
(cid:18)1:0004 (cid:6) 0:140
(cid:18)1:000 (cid:6) 0:000001
(cid:18)0
2(cid:19)
(cid:18)0:0136 (cid:6) 1:246
2:0336 (cid:6) 2:831(cid:19)
(cid:18)0:0010 (cid:6) 0:082
1:9250 (cid:6) 0:226(cid:19)
(cid:18)0:0051 (cid:6) 0:390
1:9869 (cid:6) 0:857(cid:19)
(cid:18)0:000 (cid:6) 0:000003
2:000 (cid:6) 0:000011(cid:19)
r(a) = a2
Table 2: The true gradient of the expected return and its MC and BQ estimates for two bandit problems.

As shown in Table 2, the BQ estimate has much lower variance than the MC estimate for both small
and large sample sizes. The BQ estimate also has a lower bias than the MC estimate for the large
sample size (M = 100), and almost the same bias for the small sample size (M = 10).

5.2 A Linear Quadratic Regulator
In this section, we consider the following linear system in which the goal is to minimize the expected
return over 20 steps. Thus, it is an episodic problem with paths of length 20.
Policy
System
Actions: at (cid:24) (cid:22)((cid:1)jxt ; (cid:18)) = N ((cid:21)xt ; (cid:27)2 )
Initial State: x0 (cid:24) N (0:3; 0:001)
Rewards: rt = x2
Parameters: (cid:18) = ((cid:21) ; (cid:27))>
t + 0:1a2
t
Transitions: xt+1 = xt + at + nx ; nx (cid:24) N (0; 0:01)
We (cid:2)rst compare the BQ and MC estimates of the gradient of the expected return for the policy
induced by the parameters (cid:21) = (cid:0)0:2 and (cid:27) = 1. We use several different sample sizes (number of
paths used for gradient estimation) M = 5j ; j = 1; : : : ; 20 for the BQ and MC estimates. For each
sample size, we compute both the MC and BQ estimates 104 times, using the same samples. The
true gradient is estimated using MC with 107 sample paths for comparison purposes.
Figure 1 shows the mean squared error (MSE) ((cid:2)rst column), and the mean absolute angular error
(second column) of the MC and BQ estimates of the gradient for several different sample sizes.
The absolute angular error is the absolute value of the angle between the true gradient and the
estimated gradient. In this (cid:2)gure, the BQ gradient estimate was calculated using Model 1 without
sparsi(cid:2)cation. With a good choice of sparsi(cid:2)cation threshold, we can attain almost identical results
much faster and more ef(cid:2)ciently with sparsi(cid:2)cation. These results are not shown here due to space
limitations. To give an intuition concerning the speed and the ef(cid:2)ciency attained by sparsi(cid:2)cation,
we should mention that the dimension of the feature space for the kernel used in Model 1 is 6
(Proposition 9.2 in [14]). Therefore, we deal with a kernel matrix of size 6 with sparsi(cid:2)cation versus
a kernel matrix of size M = 5j ; j = 1; : : : ; 20 without sparsi(cid:2)cation.
We ran another set of experiments, in which we add i.i.d. Gaussian noise to the rewards: r t = x2
t +
r = 0:1). In Model 2, we can model this by the measurement noise
t + nr ; nr (cid:24) N (0; (cid:27)2
0:1a2
r I , where T = 20 is the path length. Since each reward rt is a Gaussian
covariance matrix (cid:6) = T (cid:27) 2
t=0 rt will also be a Gaussian random
random variable with variance (cid:27) 2
r , the return R((cid:24) ) = PT (cid:0)1
variable with variance T (cid:27) 2
r . The results are presented in the third and fourth columns of Figure 1.
These experiments indicate that the BQ gradient estimate has lower variance than its MC counter-
part. In fact, whereas the performance of the MC estimate improves as 1
M , the performance of the
BQ estimate improves at a higher rate.

 

MC
BQ

106

105

104

103

r
o
r
r
E
 
d
e
r
a
u
q
S
 
n
a
e
M

102

101

 

MC
BQ

106

105

104

r
o
r
r
E
 
d
e
r
a
u
q
S
 
n
a
e
M

 

MC
BQ

)
g
e
d
(
 
r
o
r
r
E
 
r
a
l
u
g
n
A
 
e
t
u
l
o
s
b
A
 
n
a
e
M

 

MC
BQ

102

101

)
g
e
d
(
 
r
o
r
r
E
 
r
a
l
u
g
n
A
 
e
t
u
l
o
s
b
A
 
n
a
e
M

102
100
103
100
 
 
 
 
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
Number of Paths
Number of Paths
Number of Paths
Number of Paths
Figure 1: Results for the LQR problem using Model 1 (left) and Model 2 (right), without sparsi(cid:2)cation. The
Model 2 results are for a LQR problem, in which the rewards are corrupted by i.i.d. Gaussian noise. For each
algorithm, we show the MSE (left) and the mean absolute angular error (right), as functions of the number of
sample paths M . Note that the errors are plotted on a logarithmic scale. All results are averages over 10 4 runs.

Next, we use BPG to optimize the policy parameters in the LQR problem. Figure 2 shows the
performance of the BPG algorithm with the regular (BPG) and the natural (BPNG) gradient es-
timates, versus a MC-based policy gradient (MCPG) algorithm, for the sample sizes (number of
sample paths used for estimating the gradient of a policy) M = 5; 10; 20; and 40. We use Alg. 2
with the number of updates set to N = 100, and Model 1 for the BPG and BPNG methods. Since
Alg. 2 computes the Fisher information matrix for each set of policy parameters, an estimate of the
natural gradient is provided at little extra cost at each step. The returns obtained by these meth-
ods are averaged over 104 runs for sample sizes 5 and 10, and over 103 runs for sample sizes
20 and 40. The policy parameters are initialized randomly at each run.
In order to ensure that
the learned parameters do not exceed an acceptable range, the policy parameters are de(cid:2)ned as
(cid:21) = (cid:0)1:999 + 1:998=(1 + e(cid:23)1 ) and (cid:27) = 0:001 + 1=(1 + e(cid:23)2 ). The optimal solution is (cid:21)(cid:3) (cid:25) (cid:0)0:92
1 (cid:25) (cid:0)0:16 and (cid:23) (cid:3)
and (cid:27)(cid:3) = 0:001 ((cid:17)B ((cid:21)(cid:3) ; (cid:27)(cid:3) ) = 0:1003) corresponding to (cid:23) (cid:3)
2 ! 1.

101

100

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

 

MC
BPG
BPNG
Optimal

101

100

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

 

MC
BPG
BPNG
Optimal

101

100

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

 

MC
BPG
BPNG
Optimal

101

100

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

 

MC
BPG
BPNG
Optimal

 
 
 
 
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
0 20 40 60 80 100
Number of Updates (Sample Size = 5)
Number of Updates (Sample Size = 10)
Number of Updates (Sample Size = 20)
Number of Updates (Sample Size = 40)
Figure 2: A comparison of the average expected returns of BPG using regular (BPG) and natural (BPNG)
gradient estimates, with the average expected return of the MCPG algorithm for sample sizes 5; 10; 20; and 40.

Figure 2 shows that MCPG performs better than the BPG algorithm for the smallest sam-
ple size (M = 5), whereas for larger samples BPG dominates MCPG. This phenomenon is
also reported in [16]. We use two different learning rates for the two components of the
gradient. For a (cid:2)xed sample size, each method starts with an initial learning rate, and de-
creases it according to the schedule (cid:11)j = (cid:11)0 (20=(20 + j )). Table 3 summarizes the best
initial learning rates for each algorithm. The selected learning rates for BPNG are signif-
icantly larger than those for BPG and MCPG, which explains why BPNG initially learns
faster than BPG and MCPG, but contrary to our expectations, eventually performs worse.

So far we have assumed that the Fisher
M = 10 M = 20 M = 40
M = 5
0.10, 0.15
0.05, 0.10
0.05, 0.10
MCPG 0.01, 0.05
information matrix is known.
In the
0.10, 0.30
0.15, 0.20
0.07, 0.10
BPG
0.01, 0.03
next experiment, we estimate it us-
0.09, 0.30
0.45, 0.90
0.80, 0.90
BPNG 0.03, 0.50
ing both MC and maximum likelihood
(ML) methods as described in Sec. 4.
Figure 3: Initial learning rates used by the PG algorithms.
In ML estimation, we assume that the
transition probability function is P (xt+1 jxt ; at ) = N ((cid:12)1xt + (cid:12)2 at + (cid:12)3 ; (cid:12) 2
4 ), and then estimate its
parameters by observing state transitions. Figure 4 shows that when the Fisher information matrix
is estimated using MC (BPG-MC), the BPG algorithm still performs better than MCPG, and outper-
forms the BPG algorithm in which the Fisher information matrix is estimated using ML (BPG-ML).
Moreover, as we increase the sample size, its performance converges to the performance of the BPG
algorithm in which the Fisher information matrix is known (BPG).

10−0.1

10−0.2

10−0.3

10−0.4

10−0.5

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

MC
BPG
BPG−ML
BPG−MC
Optimal

 

10−0.1

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

10−0.2

10−0.3

10−0.4

10−0.5

MC
BPG
BPG−ML
BPG−MC
Optimal

 

10−0.1

n
r
u
t
e
R
 
d
e
t
c
e
p
x
E
 
e
g
a
r
e
v
A

10−0.2

10−0.3

10−0.4

10−0.5

 

MC
BPG
BPG−ML
BPG−MC
Optimal

 
 
 
0
0
100
80
60
40
20
0
100
80
60
40
20
100
80
60
40
20
Number of Updates (Sample Size = 40)
Number of Updates (Sample Size = 20)
Number of Updates (Sample Size = 10)
Figure 4: A comparison of the average return of BPG when the Fisher information matrix is known (BPG),
and when it is estimated using MC (BPG-MC) and ML (BPG-ML) methods, for sample sizes 10; 20; and 40
(from left to right). The average return of the MCPG algorithm is also provided for comparison.

6 Discussion
In this paper we proposed an alternative approach to conventional frequentist policy gradient esti-
mation procedures, which is based on the Bayesian view. Our algorithms use GPs to de(cid:2)ne a prior
distribution over the gradient of the expected return, and compute the posterior, conditioned on the
observed data. The experimental results are encouraging, but we conjecture that even higher gains
may be attained using this approach. This calls for additional theoretical and empirical work.
Although the proposed policy updating algorithm (Alg. 2) uses only the posterior mean of the gradi-
ent in its updates, we hope that more elaborate algorithms can be devised that would make judicious
use of the covariance information provided by the gradient estimation algorithm (Alg. 1). Two ob-
vious possibilities are: 1) risk-aware selection of the update step-size and direction, and 2) using
the variance in a termination condition for Alg. 1. Other interesting directions include 1) investi-
gating other possible partitions of the integrand in the expression for r(cid:17)B ((cid:18)) into a GP term f and
a known term p, 2) using other types of kernel functions, such as sequence kernels, 3) combining
our approach with MDP model estimation, to allow transfer of learning between different policies,
4) investigating methods for learning the Fisher information matrix, 5) extending the Bayesian ap-
proach to Actor-Critic type of algorithms, possibly by combining BPG with the Gaussian process
temporal difference (GPTD) algorithms of [15].
Acknowledgments We thank Rich Sutton and Dale Schuurmans for helpful discussions. M.G.
would like to thank Shie Mannor for his useful comments at the early stages of this work. M.G. is
supported by iCORE and Y.E. is partially supported by an Alberta Ingenuity fellowship.
References
[1] R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine Learning, 8:229(cid:150)256, 1992.
[2] P. Marbach. Simulated-Based Methods for Markov Decision Processes. PhD thesis, MIT, 1998.
[3] J. Baxter and P. Bartlett. In(cid:2)nite-horizon policy-gradient estimation. JAIR, 15:319(cid:150)350, 2001.
[4] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning
with function approximation. In Proceedings of NIPS 12, pages 1057(cid:150)1063, 2000.
[5] S. Kakade. A natural policy gradient. In Proceedings of NIPS 14, 2002.
[6] J. Bagnell and J. Schneider. Covariant policy search. In Proceedings of the 18th IJCAI, 2003.
[7] J. Peters, S. Vijayakumar, and S. Schaal. Reinforcement learning for humanoid robotics. In Proceedings
of the Third IEEE-RAS International Conference on Humanoid Robots, 2003.
[8] J. Berger and R. Wolpert. The Likelihood Principle. Inst. of Mathematical Statistics, Hayward, CA, 1984.
[9] A. O’Hagan. Monte Carlo is fundamentally unsound. The Statistician, 36:247(cid:150)249, 1987.
[10] A. O’Hagan. Bayes-Hermite quadrature. Journal of Statistical Planning and Inference, 29, 1991.
[11] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scienti(cid:2)c, 1996.
[12] R. Sutton and A. Barto. An Introduction to Reinforcement Learning. MIT Press, 1998.
[13] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classi(cid:2)ers. In Proceedings
of NIPS 11. MIT Press, 1998.
[14] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press, 2004.
[15] Y. Engel. Algorithms and Representations for Reinforcement Learning. PhD thesis, The Hebrew Univer-
sity of Jerusalem, Israel, 2005.
[16] C. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo. In Proceedings of NIPS 15. MIT Press, 2003.


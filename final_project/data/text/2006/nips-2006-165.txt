An Oracle Inequality for Clipped Regularized Risk
Minimizers

Ingo Steinwart, Don Hush, and Clint Scovel
Modelling, Algorithms and Informatics Group, CCS-3
Los Alamos National Laboratory
Los Alamos, NM 87545
{ingo,dhush,jcs}@lanl.gov

Abstract

We establish a general oracle inequality for clipped approximate minimizers of
regularized empirical risks and apply this inequality to support vector machine
(SVM) type algorithms. We then show that for SVMs using Gaussian RBF kernels
for classi ﬁcation this oracle inequality leads to learning rates that are faster than
the ones established in [9]. Finally, we use our oracle inequality to show that a
simple parameter selection approach based on a validation set can yield the same
fast learning rates without knowing the noise exponents which were required to
be known a-priori in [9].

1 Introduction

The theoretical understanding of support vector machines (SVMs) and related kernel-based meth-
ods has been substantially improved in recent years. For example using Talagrand’s concentration
inequality and local Rademacher averages it has recently been shown that SVMs for classiﬁcation
can learn with rates up to n−1 under somewhat realistic assumptions on the data-generating distri-
bution (see [9, 11] and the related work [2]). However, the so-called “shrinking technique” of [9, 11]
for establishing such rates, requires the free parameters to be chosen a-priori, and in addition, the
optimal values of these parameters depend on features of the data-generating distribution which are
typically unknown. Consequently, [9, 11] do not provide a practical method for learning with fast
rates. On the other hand, the oracle inequality in [2] only holds for distributions having Tsybakov
noise exponent ∞, and hence it describes a situation which is rarely met in practice.
The goal of this work is to overcome these shortcomings by establishing a general oracle inequality
(see Theorem 3.1) for regularized empirical risk minimizers. The key ingredient of this oracle
inequality is the observation that for most commonly used loss functions it is possible to “clip”
the decision function of the algorithm before beginning with the theoretical analysis. In addition,
a careful choice of the weighted empirical process Talagrand’s inequality is applied to, makes the
“shrinking technique” superﬂuous. Finally, by explicitly dealing with
-approximate minimizers of
the regularized risk our results also apply to actual SVM algorithms.

With the help of the general oracle inequality we then establish an oracle inequality for SVM type
algorithms (see Theorem 2.1) as well as a simple oracle inequality for model selection (see Theorem
4.2). For the former, we show that it leads to improved rates for e.g. binary classiﬁcation under
the assumptions considered in [9] and a-priori known noise exponents. Using the model selection
theorem we then show how our new oracle inequality for SVMs can be used to analyze a simple
parameter selection procedure based on a validation set that achieves the same learning rates without
prior knowledge on the noise exponents.

The rest of this work is organized as follows: In Section 2 we present our oracle inequality for
SVM type algorithms. We then discuss its implications and analyze the simple parameter selection

procedure when using Gaussian RBF kernels. In Section 3 we then present and prove the general
oracle inequality. The proof of Theorem 2.1 as well as the oracle inequality for model selection can
be found in Section 4.

2 Main Results
Throughout this work we assume that X is compact metric space, Y ⊂ [−1, 1] is compact, P
is a Borel probability measure on X × Y , and F is a set of functions over X such that 0 ∈ F .
Often F is a reproducing kernel Hilbert space (RKHS) H of continuous functions over X with
closed unit ball BH . It is well-known that H can then be continuously embedded into the space
of continuous functions C (X ) equipped with the usual maximum-norm k.k∞ . In order to avoid
constants we always assume that this embedding has norm 1, i.e. k.k∞ ≤ k.kH . Furthermore,
L : Y × R → [0, ∞) always denotes a continuous function which is convex in its second variable
such that L(y , 0) ≤ 1. The functions L will serve as loss functions and consequently let us recall
RL,P (f ) = E(x,y)∼P L(cid:0)y , f (x)(cid:1) .
that the associated L-risk of a measurable function f : X → R is deﬁned by
Note that the assumption L(y , 0) ≤ 1 immediately gives RL,P (0) ≤ 1. Furthermore, the minimal
L,P = inf {RL,P (f ) | f : X → R measurable}, and a function
L-risk is denoted by R∗
L,P , i.e. R∗
L,P . We always assume that such an f ∗
attaining this inﬁmum is denoted by f ∗
L,P exists.
(cid:16)
(cid:17)
The learning schemes we are mainly interested in are based on an optimization problem of the form
H + RL,P (f )
λkf k2
fP,λ := arg min
(1)
,
f ∈H
where λ > 0. Note that if we identify a training set T = ((x1 , y1 ), . . . , (xn , yn )) ∈ (X × Y )n with
its empirical measure, then fT ,λ denotes the empirical estimators of the above learning scheme.
Obviously, support vector machines (see e.g. [5]) and regularization networks (see e.g. [7]) are both
learning algorithms which fall into the above category. One way to describe the approximation error
of these learning schemes is the approximation error function
H + RL,P (fP,λ ) − R∗
a(λ) := λkfP,λk2
L,P ,
which has been discussed in some detail in [10]. Furthermore in order to deal with the complexity
of the used RKHSs let us recall that for a subset A ⊂ E of a Banach space E the covering numbers
n
o
n ≥ 1 : ∃x1 , . . . , xn ∈ E with A ⊂ n[
are deﬁned by
N (A, ε, E ) := min
(xi + εBE )
,
i=1
where BE denotes the closed unit ball of E . Given a ﬁnite sequence T = ((x1 , y1 ), . . . , (xn , yn )) ∈
(X × Y )n we write TX := (x1 , . . . , xn ). For our main results we are particularly interested in
covering numbers in the Hilbert space L2 (TX ) which consists of all equivalence classes of functions
f : X × Y → R and which is equipped with the norm
(cid:16) 1
(cid:12)(cid:12)f (xi )(cid:12)(cid:12)2(cid:17) 1
nX
kf kL2 (TX ) :=
2
n
i=1
In other words, L2 (TX ) is a L2 -space with respect to the empirical measure of (x1 , . . . , xn ).
Learning schemes of the form (1) typically produce functions fP,λ with limλ→0 kfP,λk∞ = ∞
(see e.g. [10] for a precise statement). Unfortunately, this behaviour has a serious negative impact
on the learning rates when directly employing standard tool’s such as Hoeffding’s, Bernstein’s or
Talagrand’s inequality. On the other hand, when dealing with e.g. the hinge loss it is obvious that
clipping the function fP,λ at −1 and 1 does not worsen the corresponding risks. Following this
(cid:26)L(y , 1)
simple observation we will consider loss functions L that satisfy the clipping condition
if t ≥ 1
L(y , −1)
if t ≤ −1 ,

L(y , t) ≥

λ > 0,

.

(2)

ε > 0,

(3)

for all y ∈ Y . Recall that this type of loss function was already considered in [4, 11], but the clipping
idea actually goes back to [1]. Moreover, it is elementary to check that most commonly used loss
functions including the hinge loss and the least squares loss satisfy (3). Given a function f : X → R
1
we now deﬁne its clipped version ˆf : X → [−1, 1] by
if f (x) > 1
if f (x) ∈ [−1, 1]
f (x)
−1
if f (x) < −1 .
It is clear from (3) that we always have L(y , ˆf (x)) ≤ L(y , f (x)) and consequently we obtain
RL,P ( ˆf ) ≤ RL,P (f ) for all distributions P . Finally, we also need the following Lipschitz condition
|L(y , t1 ) − L(y , t2 )|
|L|1 :=
≤ 2.
|t1 − t2 |

sup
y∈Y ,−1≤t1 ,t2≤1

ˆf (x) :=

(4)

With the help of these deﬁnitions we can now state our main result which establishes an oracle
inequality for clipped versions of fT ,λ :
Theorem 2.1 Let P be a distribution on X × Y and let L be a loss function which satisﬁes (3) and
(4). Let H be a RKHS of continuous functions on X . For a ﬁxed element f0 ∈ H we de ﬁne
(cid:12)(cid:12)L(y , f0 (x))(cid:12)(cid:12) .
:= λkf0 k2
H + RL,P (f0 ) − R∗
a(f0 )
L,P
sup
B (f0 )
:=
x∈X,y∈Y
(cid:0)L ◦ ˆf − L ◦ f ∗
(cid:1)2 ≤ v (cid:0)EP (L ◦ ˆf − L ◦ f ∗
L,P )(cid:1)ϑ
In addition, we assume that we have a variance bound of the form
EP
(6)
L,P
log N (cid:0)BH , ε, L2 (TX )(cid:1) ≤ aε−2p ,
for constants v ≥ 1, ϑ ∈ [0, 1] and all measurable f : X → R. Moreover, suppose that H satisﬁes
ε > 0,
sup
(7)
T ∈(X×Y )n
for some constants p ∈ (0, 1) and a ≥ 1. For ﬁxed λ > 0 let fT ,λ ∈ H be a function that minimizes
H + RL,T (f ) up to some  > 0. Then there exists a constant Kp,v depending only on p
f 7→ λkf k2
(cid:18) Kp,v a
(cid:19)
and v such that for all τ ≥ 1 we have with probability not less than 1 − 3e−τ that
(cid:17) 1
(cid:16) 32vτ
1
2−ϑ+p(ϑ−1)
140τ
2−ϑ +
+ 5
λpn
n
n
+ 8a(f0 ) + 4.

RL,P ( ˆfT ,λ ) − R∗
L,P ≤

14B (f0 )τ
3n

+ Kp,v a
λpn

(5)

+

(8)

The above oracle inequality has some interesting consequences as the following examples illustrate.
We begin with an example that deals with a ﬁxed kernel:

Example 2.2 (Learning rates for single kernel) Assume that in Theorem 2.1 we have a Lipschitz
continuous loss function such as the hinge loss. In addition assume that the approximation error
function satisﬁes a(λ) ≤ cλβ , λ > 0, for some constants c > 0 and β ∈ (0, 1]. Setting f0 := fP,λ
o
n
and optimizing (8) with respect to λ then shows that the corresponding SVM learns with rate n−γ ,
β (cid:0)2 − ϑ + p(ϑ − 1)(cid:1) + p
where
2β
β
.
β + 1
Recall that this learning rate has already been obtained in [11].

γ := min

,

The next example investigates SVMs that use a Gaussian RBF kernel whose width may vary with
the sample size:
Example 2.3 (Classi ﬁcation with several Gaussian kernels) Let X be the unit ball in Rd and
Y := {−1, 1}. Furthermore assume that we are interested in binary classiﬁcation using the hinge

loss and the Gaussian RKHSs Hσ that belong to the RBF kernels kσ (x1 , x2 ) := e−σ2 kx1−x2 k2 with
width σ > 0. If P has geometric noise exponent α ∈ (0, ∞) in the sense of [9] then it was shown in
aσ (f0 ) ≤ c (cid:0)σdλ + σ−αd (cid:1) ,
[9] that there exists a function f0 ∈ Hσ with kf0 k∞ ≤ 1 and
σ > 0, λ > 0,
where c > 0 is a constant independent of λ and σ . Moreover, [9, Thm. 2.1] shows that Hσ satisﬁes
(7) for all p ∈ (0, 1) with
a := cp,d,δ σ (1−p)(1+δ)d
where δ > 0 can be arbitrarily chosen and cp,d,δ is a suitable constant. Now assume that P has
Tsybakov noise exponent q ∈ [0, ∞] in the sense of [9]. It was then shown in [9] that (6) is satisﬁed
for ϑ := q
q+1 . Minimizing (8) with respect to σ and λ and choosing p and δ sufﬁciently small then
yields that the corresponding SVM can learn with rate n−γ+ε , where
α(q + 1)
α(q + 2) + q + 1 ,
and ε > 0 can be chosen arbitrarily small. Note that these rates are superior to those obtained in
[9, Theorem 2.8].

γ :=

In the above examples the optimal parameters λ and σ depend on the sample size n but not on the
training samples T . However, these optimal parameters require us to know certain characteristics
of the distribution such as the approximation exponent β or the noise exponents α and q . The
following example shows that the oracle inequality of Theorem 2.1 can be used to ﬁnd these optimal
parameters in a data-dependent fashion which does not require any a-priori knowledge:

Example 2.4 In this example we assume that our training set T consists of 2n samples. We write
T0 for the ﬁrst n samples and T1 for the last n samples. Let fT0 ,σ,λ be the SVM solution using
a Gaussian kernel with width σ . Moreover, let Σ ⊂ [1, n1/d ) and Λ ⊂ (0, 1] be ﬁnite sets with
(cid:19)
(cid:18)(cid:16) σd
cardinality mΣ and mΛ , respectively. Under the assumptions of Example 2.3 the oracle inequality
(cid:16) τ
(cid:17) q+1
(cid:17) q+1
(8) then shows that with probability not less than 1 − 3mΣmΛ e−τ we have
RL,P ( ˆfT0 ,σ,λ ) − R∗
L,P ≤ Kd,q ,α,ε
q+2 + σdλ + σ−αd
q+2−ε +
λεn
n
simultaneously for all σ ∈ Σ and λ ∈ Λ, where ε ∈ (0, 1] is arbitrarily but ﬁxed and Kd,q ,α,ε is
a suitable constant. Now using a simple model selection approach (see e.g. Theorem 4.2) for the
(cid:18) τ + log(mΣmΛ )
(cid:19) q+1
second half T1 of our training set we ﬁnd that with probability not less than 1 − e−τ we have
(cid:19)
(cid:18)(cid:16) σd
) − R∗
L,P ≤ C
q+2
(cid:17) q+1
n
q+2− + σdλ + σ−αd
+ C min
,
σ∈Σ,λ∈Λ
λεn
T1 ) ∈ Σ × Λ is a pair that
where C is a constant only depending on d, q , α, and ε, and (σ∗
T1 , λ∗
minimizes the empirical risk RL,T1 (.) over Σ × Λ.
Now assume that Σn and Λn are 1/n- and 1/n2 -nets of [1, n1/d ) and (0, 1], respectively. Obviously,
we can choose Σn and Λn such that mΣn ≤ n2 and mΛn ≤ n2 , respectively. With such parameter
sets it is then easy to check that we obtain exactly the rates we have found in Example 2.3, but
without knowing the noise exponents α and q a-priori.

RL,P ( ˆfT0 ,σ∗
T1

,λ∗
T1

3 An oracle inequality for clipped penalized ERM

Theorem 2.1 is a consequence of a far more general oracle inequality on clipped penalized empirical
risk minimizers. Since this result is of its own interest we now present it together with its proof in
detail. To this end recall that a subroot is a nondecreasing function ϕ : [0, ∞) → [0, ∞) such
√
n−1 (cid:0)σ1h(z1 ) + · · · + σnh(zn )(cid:1). Now the general oracle inequality is:
r is nonincreasing in r . Moreover, for a Rademacher sequence σ := (σ1 , . . . , σn )
that ϕ(r)/
with respect to the measure ν and a function h : Z → R we de ﬁne Rσ h : Z n → R by Rσ h :=

,

(10)

r > 0.

(9)

28τ
n

+ 8aΩ (p0 , f0 ) + 4.

Theorem 3.1 Let P 6= ∅ be a set of (hyper)-parameters, F be a set of measurable functions f :
X → R with 0 ∈ F , and Ω : P × F → [0, ∞] be a function. Let P be a distribution on X × Y and
(p0 , f0 ) ∈ P × F we de ﬁne
L be a loss function which satisﬁes (3) and (4). For a ﬁxed pair
aΩ (p0 , f0 ) := Ω(p0 , f0 ) + RL,P (f0 ) − R∗
L,P .
Moreover, let us assume that the quantity B (f0 ) de ﬁned in (5) is
ﬁnite . In addition, we assume
that we have a variance bound of the form (6) for constants v ≥ 1, ϑ ∈ [0, 1] and all measurable
L,P )(cid:12)(cid:12) ≤ ϕn (r) ,
(cid:12)(cid:12)Rσ (L ◦ ˆf − L ◦ f ∗
f : X → R. Furthermore, suppose that there exists a subroot ϕn with
ET ∼P n Eσ∼ν
sup
(p,f )∈P×F
Ω(p,f )+EP (L◦ ˆf −L◦f ∗
L,P )≤r
Finally, let (pT ,Ω , fT ,Ω ) be an -approximate minimizer of (p, f ) 7→ Ω(p, f ) + RL,T (f ). Then for
(cid:16) 32vτ
n
o
(cid:17) 1
all τ ≥ 1 and all r satisfying
r ≥ max
2−ϑ
120ϕn (r),
n
we have with probability not less than 1 − 3e−τ that
14B (f0 )τ
Ω(pT ,Ω , fT ,Ω ) + RL,P ( ˆfT ,Ω ) − R∗
L,P ≤ 5r +
3n
Proof: We write B for B (f0 ). For T ∈ (X × Y )n we now observe Ω(pT ,Ω , fT ,Ω ) + RL,T ( ˆfT ,Ω ) −
Ω(p0 , f0 ) − RL,T (f0 ) ≤  by the deﬁnition of
(pT ,Ω , fT ,Ω ), and hence we ﬁnd
Ω(pT ,Ω , fT ,Ω)+RL,P ( ˆfT ,Ω)−R∗
L,P
≤ RL,P ( ˆfT ,Ω ) − RL,T ( ˆfT ,Ω ) + RL,T (f0 ) − RL,P (f0 ) + aΩ (p0 , f0 ) + 
= RL,P ( ˆfT ,Ω)−RL,P (f ∗
L,P)−RL,T ( ˆfT ,Ω)+RL,T (f ∗
L,P)
+RL,T (f0 )−RL,T ( ˆf0 )−RL,P (f0 )+RL,P ( ˆf0 )
L,P )−RL,P ( ˆf0 )+RL,P (f ∗
+RL,T ( ˆf0 )−RL,T (f ∗
L,P )
+aΩ (p0 , f0 ) +  .
Let us ﬁrst estimate the term in line (12). To this end we write h1 := L ◦ f0 − L ◦ ˆf0 . Then our
assumption on L guarantees h1 ≥ 0, and since we also have kh1 k∞ ≤ B , we ﬁnd kh1 − EP h1 k∞ ≤
B . In addition, we obviously have EP (h1 − EP h1 )2 ≤ EP h2
1 ≤ BEP h1 . Consequently, Bernstein’s
r 2τ B EP h1
inequality [6, Thm. 8.2] shows that with probability not less than 1 − e−τ we have
2B τ
ET h1 − EP h1 <
+
3n
n
√
P n(cid:16)
(cid:17) ≥ 1− e−τ . (14)
2 ≤ EP h1 + Bτ
2τ BEP h1 · n− 1
ab ≤ a
2 + b
2n , and consequently we have
Now using
2 we ﬁnd
7B τ
T ∈ Z n : RL,T (f0 )−RL,T ( ˆf0 )−RL,P (f0 )+RL,P ( ˆf0 ) < EP h1 +
6n
Let us now estimate the term in line (13). To this end we write h2 := L ◦ ˆf0 −L ◦ f ∗
L,P . Then we have
kh2 k∞ ≤ 3 and kh2 − EP h2 k∞ ≤ 6. In addition, our variance bound gives EP (h2 − EP h2 )2 ≤
r 2τ v(EP h2 )ϑ
2 ≤ v(EP h2 )ϑ , and consequently, Bernstein’s inequality shows that with probability not less
EP h2
than 1 − e−τ we have
4τ
ET h2 − EP h2 <
+
.
2 , and b := (cid:0) 2EP h2
(cid:1)ϑ/2 we obtain
n
n
Now, for q−1 + (q 0 )−1 = 1 the elementary inequality ab ≤ aq q−1 + bq 0 (q 0 )−1 holds, and hence for
√
r 2τ v(EP h2 )ϑ
21−ϑϑϑ τ v · n− 1
2−ϑ , q 0 := 2
(cid:17) 1
(cid:17)(cid:16) 21−ϑϑϑ vτ
≤ (cid:16)
q := 2
ϑ , a :=
ϑ
1 − ϑ
2−ϑ + EP h2 .
2
n
n

(11)

(12)
(13)

√

.

Since elementary calculations show that (cid:0)2−ϑϑϑ (cid:1) 1
r 2τ v(EP h2 )ϑ
2−ϑ ≤ 1 we obtain
(cid:17) 1
(cid:17)(cid:16) 2vτ
≤ (cid:16)
1 − ϑ
2−ϑ + EP h2 .
2
n
n
(cid:17) 1
(cid:17)(cid:16) 2vτ
(cid:16)
Therefore we have with probability not less than 1 − e−τ that
4τ
RL,T ( ˆf0 ) − RL,T (f ∗
1 − ϑ
L,P ) − RL,P ( ˆf0 ) + RL,P (f ∗
2−ϑ +
L,P ) < EP h2 +
. (15)
2
n
n
Let us ﬁnally estimate the term in line (11). To this end we write hf := L ◦ ˆf − L ◦ f ∗
L,P , f ∈ F .
n
: (p, f ) ∈ P × F o
Moreover, for r > 0 we deﬁne
EP hf − hf
Gr :=
.
Ω(p, f ) + EP (hf ) + r
EP hf −hf
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) =
Ω(p,f )+EP (hf )+r ∈ Gr we have EP gp,f = 0 and
Then for gp,f :=
kEP hf − hf k∞
EP hf − hf (z )
≤ 6
kgp,f k∞ = sup
.
Ω(p, f ) + EP (hf ) + r
Ω(p, f ) + EP (hf ) + r
z∈Z
r
In addition, the inequality aϑ b2−ϑ ≤ (a + b)2 and the variance bound assumption (6) implies that
EP h2
EP h2
r2−ϑ (EP hf )ϑ ≤ v
(EP (hf ) + r)2 ≤
p,f ≤
EP g2
f
f
r2−ϑ .
EP hf − ET hf
Ω(p, f ) + EP (hf ) + r

Φ(r) := ET ∼P n

sup
(p,f )∈P×F

Now deﬁne

.

+

P n

|Rσ hf | ,

Standard symmetrization then yields
|EP hf − ET hf | ≤ 2ET ∼P n Eσ∼ν
ET ∼P n
sup
sup
(p,f )∈P×F
(p,f )∈P×F
Ω(p,f )+EP (hf )≤r
Ω(p,f )+EP (hf )≤r
and hence Lemma 3.2 proved below together with (9) shows Φ(r) ≤ 10ϕn (r)r−1 , r > 0. Therefore
r 2τ v
(cid:19)
(cid:18)
applying Talagrand’s inequality in the version of [3] to the class Gr we obtain
7τ
ET g ≤ 30ϕn (r)
≥ 1 − e−τ .
T ∈ Z n : sup
nr2−ϑ +
(cid:1)1/2 + 7τ
+ (cid:0) 2τ v
g∈Gr
r
nr
Let us deﬁne εr := 30ϕn (r)
nr . Then the above inequality gives with probability
r 2τ vrϑ
nr2−ϑ
not less than 1 − e−τ that for all (p, f ) ∈ P × F we have
r
EP hf − ET hf ≤ εr · (cid:0)Ω(p, f ) + EP hf
(cid:1) + 30ϕn (r) +
7τ
+
n
n
and consequently we have with probability not less than 1 − e−τ that
r 2τ vrϑ
RL,P ( ˆfT ,Ω ) − RL,P (f ∗
L,P ) − RL,T ( ˆfT ,Ω ) + RL,T (f ∗
≤ εr · (cid:0)Ω(pT ,Ω , fT ,Ω ) + RL,P ( ˆfT ,Ω ) − RL,P (f ∗
L,P )(cid:1) + 30ϕn (r) +
L,P )
7τ
n
n
Now observe that for the functions h1 and h2 which we de ﬁned when estimating (12) and (13) we
have
EP g + EP h = RL,P (f0 ) − R∗
(17)
L,P ,
(cid:1)
(1− εr )(cid:0)Ω(pT ,Ω , fT ,Ω ) + RL,P ( ˆfT ,Ω ) − R∗
and hence we can combine our estimates (16), (14), and (15) of the terms (11), (12), and (13) to
obtain that with probability not less than 1 − 3e−τ we have
r 2τ vrϑ
(cid:16) 2vτ
(cid:17) 1
L,P
(66+ 7B )τ
+ (1− ϑ
≤ 30ϕn (r)+
2−ϑ +
)
2
6n
n
n

+ aΩ (p0 , f0 )+RL,P (f0 )−R∗
L,P + .

(16)

,

+

.

(cid:1)1/2 ≤ 1
4 , (cid:0) 2τ v
≤ 1
In particular, for r satisfying the assumption (10) we have 30ϕn (r)
4 , and
r 32τ vrϑ
nr2−ϑ
4 , and hence we obtain with probability not less than 1 − 3e−τ that
nr ≤ 1
4 . This shows 1 − εr ≥ 1
r
(cid:17) 1
(cid:16) 2vτ
7τ
44τ
Ω(pT ,Ω , fT ,Ω ) + RL,P ( ˆfT ,Ω ) − R∗
L,P ≤ 120ϕn (r) +
+ 2(2 − ϑ)
(cid:1) + 4.
+ 4aΩ (p0 , f0 ) + 4(cid:0)RL,P (f0 )−R∗
2−ϑ +
n
n
n
14B τ
3 , and 2(2 − ϑ)(cid:0) 2vτ
(cid:1)1/2 ≤ r , 44τ
(cid:1) 1
However we also have 120ϕn (r) ≤ r , (cid:0) 32τ vrϑ
+
3n
L,P
n ≤ 5r
2−ϑ ≤
2(2 − ϑ) r
4 ≤ r , and hence we ﬁnd the assertion.
n
n
For the proof of Theorem 3.1 it remains to show the following lemma:
(cid:12)(cid:12)ET W (f ) − EP W (f )(cid:12)(cid:12)
Lemma 3.2 Let P and F be as in Theorem 3.1. Furthermore, let W : F → R and a : P × F →
[0, ∞). Deﬁne
Φ(r) := ET ∼P n
sup
a(p, f ) + r
f ∈P×F
(cid:12)(cid:12)ET W (f ) − EP W (f )(cid:12)(cid:12) ≤ Ψ(r) ,
and suppose that there exists a subroot Ψ such that
ET ∼P n
sup
(p,f )∈P×F
a(p,f )≤r
Then we have Φ(r) ≤ 5
r Ψ(r) for all r > 0.
Proof: For x > 1, r > 0, and T ∈ (X × Y )n we obtain by a standard peeling approach that
|EP W (f ) − ET W (f )|
∞X
a(p, f ) + r
|EP W (f ) − ET W (f )|
+
a(p, f ) + r
i=0
∞X
i=0

sup
(p,f )∈P×F
a(p,f )≥rxi
a(p,f )≤rxi+1
|EP W (f ) − ET W (f )|
rxi + r

|EP W (f ) − ET W (f )|
sup
(p,f )∈P×F
r
a(p,f )≤r

|EP W (f ) − ET W (f )|
a(p, f ) + r

sup
(p,f )∈P×F
a(p,f )≤r

sup
(p,f )∈P×F

r > 0.

≤

≤

+

sup
(p,f )∈P×F
a(p,f )≥rxi
∞X
a(p,f )≤rxi+1
1
xi + 1
i=0

1
r

≤ 1
r

|EP W (f ) − ET W (f )| +
sup
(p,f )∈P×F
(cid:16)
(cid:17)
∞X
a(p,f )≤r
1
Ψ(rxi+1 )
xi + 1
r
i=0
However since Ψ is a subroot we obtain that Ψ(rxi+1 ) ≤ x
by setting x := 4.

Ψ(r) +

=

.

|EP W (f ) − ET W (f )|
sup
(p,f )∈P×F
a(p,f )≤rxi+1

i+1
2 Ψ(r) so that we obtain the assertion

4 Proof of Theorem 2.1

Before we begin the proof of Theorem 2.1 let us state the following proposition which follows
directly from [8] (see also [9, Prop. 5.7]) together with simple considerations on covering numbers:
Proposition 4.1 Let F := H be a RKHS, P := {p0 } be a singleton, and Ω(p0 , f ) := λkf k2 . If (7)
1+p (cid:27)
(cid:26)
(cid:17) 1
1+p (cid:16) a
(cid:17) p
(cid:16) r
(cid:17) 1
2 (cid:16) a
(cid:17) p
2 (1−p)(cid:16) r
is satisﬁed then there exists a constant cp depending only on p such that (9) is satis ﬁed for
2 (1−p) r
1
2
ϑ
.
v
λ
n
λ
n

ϕn (r) := cp max

,

Proof of Theorem 2.1: From the covering bound assumption we observe that Proposition 4.1 im-
plies we have the bound (9) with ϕn (r) de ﬁned by the righthand side of Proposition 4.1 and therefore
n
(cid:17) 1
(cid:17) 1
1+p (cid:16) a
(cid:17) 1
2 (cid:16) a
2 (1−p)(cid:16) r
(cid:17) p
(cid:17) p
o
(cid:16) 32vτ
(cid:16) r
Theorem 3.1 implies that Condition (10) becomes
r ≥ max
2 (1−p) r
2−ϑ
1
2
1+p
ϑ
120cp v
λ
n
λ
n
n
and solving with respect to r yields the conclusion.

28τ
n

, 120cp

,

,

(18)

Finally, for the parameter selection approach in Example 2.4 we need the following oracle inequality
for model selection:
Theorem 4.2 Let P be a distribution on X × Y and let L be a loss function which satisﬁes (3),
(4), and the variance bound (6). Furthermore, let F := {f1 , . . . , fm} be a ﬁnite set of functions
mapping X into [−1, 1]. For T ∈ (X × Y )n we de ﬁne
f ∈F RL,T (f ) .
fT := arg min
Then there exists a universal constant K such that for all τ ≥ 1 we have with probability not less
(cid:16) K log m
(cid:16) 32vτ
(cid:17) 1
(cid:17) 1
than 1 − 3e−τ that
L,P ≤ 5
RL,P (fT ) − R∗
2−ϑ +
2−ϑ + 5
n
n
f ∈F (RL,P (f ) − R∗
+8 min
L,P ) .
Proof: Since all functions fi already map into [−1, 1] we do not have to consider the clipping oper-
ator. For r > 0 we now deﬁne Fr := {f ∈ F : RL,P (f ) − R∗
L,P ≤ r}. Then the cardinality of
L,P , ε, L2 (T )) ≤ m for all
Fr is smaller than or equal to m and hence we have N (L ◦ Fr − L ◦ f ∗
(cid:27)
(cid:26)pv log m rϑ/2 ,
ε > 0. Using the technique of [8] (cf. also [9, Prop. 5.7]) we hence obtain that (9) is satisﬁed for
log m√
ϕn (r) := c√
n
n
where c is a universal constant. Applying Theorem 3.1 then yields the assertion.

5K log m + 154τ
n

max

,

References

[1] P.L. Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of the weights
is more important than the size of the network. IEEE Trans. Inform. Theory, 44:525–536, 1998.
[2] G. Blanchard, O. Bousquet, and P. Massart. Statistical performance of support vector machines. Technical
Report, 2004.
[3] O. Bousquet. A Bennet concentration inequality and its application to suprema of empirical processes.
C. R. Math. Acad. Sci. Paris, 334:495–500, 2002.
[4] D.R. Chen, Q. Wu, Y.M. Ying, and D.X. Zhou. Support vector machine soft margin classiﬁers: Error
analysis. Journal of Machine Learning Research, 5:1143–1175, 2004.
[5] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University
Press, 2000.
[6] L. Devroye, L. Gy ¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, New
York, 1996.
[7] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural
Computation, 7:219–269, 1995.
[8] S. Mendelson.
Improving the sample complexity using global data.
48:1977–1991, 2002.
[9] I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Annals of
Statistics, to appear.
[10] I. Steinwart and C. Scovel. Fast rates for support vector machines. In Proceedings of the 18th Annual
Conference on Learning Theory, COLT 2005, pages 279–294. Springer, 2005.
[11] Q. Wu, Y. Ying, and D.-X. Zhou. Multi-kernel regularized classiﬁers.
J. Complexity, to appear.

IEEE Trans. Inform. Theory,


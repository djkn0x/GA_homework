Denoising and Dimension Reduction in Feature Space

Mikio L. Braun
Fraunhofer Institute1
FIRST.IDA
Kekul ´estr. 7, 12489 Berlin
mikio@ﬁrst.fhg.de

Joachim Buhmann
Inst. of Computational Science
ETH Zurich
CH-8092 Z ¨urich
jbuhmann@inf.ethz.ch

Klaus-Robert M ¨uller2,1
Technical University of Berlin2
Computer Science
Franklinstr. 28/29, 10587 Berlin
krm@cs.tu-berlin.de

Abstract

We show that the relevant information about a classiﬁcation problem in feature
space is contained up to negligible error in a ﬁnite number of leading kernel PCA
components if the kernel matches the underlying learning problem. Thus, ker-
nels not only transform data sets such that good generalization can be achieved
even by linear discriminant functions, but this transformation is also performed
in a manner which makes economic use of feature space dimensions. In the best
case, kernels provide efﬁcient implicit representations of the data to perform clas-
siﬁcation. Practically, we propose an algorithm which enables us to recover the
subspace and dimensionality relevant for good classiﬁcation. Our algorithm can
therefore be applied (1) to analyze the interplay of data set and kernel in a geo-
metric fashion, (2) to help in model selection, and to (3) de-noise in feature space
in order to yield better classiﬁcation results.

1 Introduction

Kernel machines use a kernel function as a non-linear mapping of the original data into a high-
dimensional feature space; this mapping is often referred to as empirical kernel map [6, 11, 8, 9].
By virtue of the empirical kernel map, the data is ideally transformed such that a linear discriminative
function can separate the classes with low generalization error, say via a canonical hyperplane with
large margin. The latter is used to provide an appropriate mechanism of capacity control and thus to
“protect” against the high dimensionality of the feature space.
The idea of this paper is to add another aspect, not covered by this picture. We will show theoretically
that if the learning problem matches the kernel well, the relevant information of a supervised learning
data set is always contained in a ﬁnite number of leading kernel PCA components (that is, the label
information projected to the kernel PCA directions), up to negligible error. This result is based on
recent approximation bounds dealing with the eigenvectors of the kernel matrix which show that if
a function can be reconstructed using only a few kernel PCA components asymptotically, then the
same already holds in a ﬁnite sample setting, even for small sample sizes.

Consequently, the use of a kernel function not only greatly increases the expressive power of linear
methods by non-linearly transforming the data, but it does so ensuring that the high dimensionality
of the feature space will not become overwhelming: the relevant information for classiﬁcation will
stay conﬁned within a comparably low dimensional subspace. This ﬁnding underlines the efﬁcient
use of data that is made by kernel machines using a kernel suited to the problem. While the number
of data points stays constant for a given problem, a smart choice of kernel permits to make better
use of the available data at a favorable “data point per effective dimension”-ratio, even for inﬁnite-
dimensional feature spaces. Furthermore we can use de-noising techniques in feature space, much
in the spirit of Mika et al. [8, 5] and thus regularize the learning problem in an elegant manner.

Let us consider an example. Figure 1 shows the ﬁrst six kernel PCA components for an example
data set. Above each plot, the variance of the data along this direction and the contribution of this

Figure 1: Although the data set is embedded into a high-dimensional manifold, not all directions
contain interesting information. Above the ﬁrst six kernel PCA components are plotted. Of these,
only the fourths is highly relevant for the learning problem. Note, however, that this example is
atypical in having a single relevant component.
In general, several components will have to be
combined to construct the decision boundary.

component to the class labels are plotted (normalized such that the maximal possible contribution is
one1 ). Of these six components, only the fourth contributes signiﬁcantly to the class memberships.
As we will see below, the contributions in the other directions is mostly noise. This is true espe-
cially for components with small variance. Therefore, after removing this noise, a ﬁnite number of
components sufﬁce to represent the optimal decision boundary.

The dimensionality of the data set in feature space is characteristic for the relation between a data
set and a kernel. Roughly speaking, the relevant dimensionality of the data set corresponds to the
complexity of the learning problem when viewed through the lens of the kernel function. This notion
of complexity relates the number of data points required by the learning problem and the noise, as a
small relevant dimensionality enables the de-noising of the data set to obtain an estimate of the true
class labels, making the learning process much more stable. This combination of dimension and
noise estimate allows us to distinguish among data sets showing weak performance which might
either be complex or noisy.

To summarize the main contributions of this paper: (1) We provide theoretical bounds showing that
the relevant information (deﬁned in section 2) is actually contained in the leading projected kernel
principal components under appropriate conditions. (2) We propose an algorithm which estimates
the relevant dimensionality of the data set and permits to analyze the appropriateness of a kernel for
the data set, and thus to perform model selection among different kernels. (3) We show how the
dimension estimate can be used in conjunction with kernel PCA to perform effective de-noising. We
analyze some well-known benchmark data sets and evaluate the performance as a de-noising tool
in Section 5. Note that we do not claim to obtain better performance within our framework when
compared to, for example, cross-validation techniques. Rather, we are on par. Our contribution
is to foster an understanding about a data set and to gain better insights of whether a mediocre
classiﬁcation result is due to intrinsic high dimensionality of the data or overwhelming noise level.

2 The Relevant Information and Kernel PCA Components

In this section, we will deﬁne the notion of the relevant information contained in the class labels,
and show that the location of this vector with respect to the kernel PCA components is linked to the
scalar products with the eigenvectors of the kernel matrix.

1Note, however, that these numbers do not simply add up, instead the contribution of a and b is

√
a2 + b2 .

Let us start to formalize the ideas introduced so far. As usual, we will consider a data set (X1 , Y1 ),
. . . , (Xn , Yn ) where the X lie in some space X and the Y are in Y = {±1}. We assume that the
(Xi , Yi ) are drawn i.i.d. from PX ×Y . In kernel methods, the data is non-linearly mapped into some
feature space F via the feature map Φ. Scalar products in F can be computed by the kernel k in
closed form: hΦ(x), Φ(x0 )i = k(x, x0 ). Summarizing all the pairwise scalar products results in the
(normalized) kernel matrix K with entries k(Xi , Xj )/n.
We wish to summarize the information contained in the class label vector Y = (Y1 , . . . , Yn )
about the optimal decision boundary. We deﬁne the
relevant information vector as the vector
G = (E(Y1 |X1 ), . . . , E(Yn |Xn )) containing the expected class labels for the objects in the training
set. The idea is that since E(Y |X ) = P (Y = 1|X ) − P (Y = −1|X ), the sign of G contains the
relevant information on the true class membership by telling us which class is more probable. The
observed class label vector can be written as Y = G − N with N = G − Y denoting the noise in
the class labels. We want to study the relation of G with respect to the kernel PCA components. The
following lemma relates projections of G to the eigenvectors of the kernel matrix K:
Y ∈ Rn to the leading d kernel PCA components is given by πd (Y ) = Pd
Lemma 1 The k th kernel PCA component fk evaluated on the Xi s is equal to the k th eigenvector2
of the kernel matrix K: (fk (X1 ), . . . , fk (Xn )) = uk . Consequently, the projection of a vector
k=1 uk u>
Proof The kernel PCA directions are given as (see [10]) vk = Pn
k Y .
i=1 αiΦ(Xi ), where αi =
[uk ]i /lk , [uk ]i denoting the ith component of uk , and lk , uk being the eigenvalues and eigenvec-
nX
nX
tors of the kernel matrix K. Thus, the k th PCA component for a point Xj in the training set is
fk (Xj ) = hΦ(Xj ), vk i =
hΦ(Xj ), Φ(Xi )i[uk ]i =
i=1
i=1
The sum computes the j th component of Kuk = lk uk , because uk is an eigenvector of K. Therefore
1
fk (Xj ) =
[lk uk ]j = [uk ]j .
the ﬁrst d kernel PCA components is given by Pd
lk
Since the uk are orthogonal (K is a symmetric matrix), the projection of Y to the space spanned by
(cid:4)
i=1 uiu>
i Y .
3 A Bound on the Contribution of Single Kernel PCA Components

k(Xj , Xi )[uk ]i .

1
lk

1
lk

As we have just shown, the location of G is characterized by its scalar products with the eigenvectors
of the kernel matrix. In this section, we will apply results from [1, 2] which deal with the asymptotic
convergence of spectral properties of the kernel matrix to show that the decay rate of the scalar
products are linked to the decay rate of the kernel PCA principal values.

It is clear that we cannot expect G to generally locate favorably with respect to the kernel PCA
components, but only when there is some kind of match between G and the chosen kernel. This
link will be established by asymptotic considerations. Kernel PCA is closely linked to the spectral
ψi of the integral operator Tk f = R k( · , x)f (x)PX (dx) de ﬁned on L2 (PX ), where PX is the
properties of the kernel matrix, and it is known [3, 4] that the eigenvalues and the projections to
eigenspaces converge. Their asymptotic limits are given as the eigenvalues λi and eigenfunctions
occur in the well-known Mercer’s formula: By Mercer’s theorem, k(x, x0 ) = P∞
marginal measure of PX ×Y which generates our samples. The eigenvalues and eigenfunctions also
i=1 λiψi (x)ψi (x0 ).
The asymptotic counterpart of G is given by the function g(x) = E(Y |X = x).
to saying that there exists a sequence (αi ) ∈ ‘2 such that g = P∞
We will encode ﬁtness between k and g by requiring that g lies in the image of Tk . This is equivalent
i=1 λiαiψi .3 Under this condition,
the scalar products decay as quickly as the eigenvalues, because hg , ψi i = λiαi = O(λi ). Because
of the known convergence of spectral projections, we can expect the same behavior asymptotically

2As usual, the eigenvectors are arranged in descending order by corresponding eigenvalue.
3A different condition is that g lies in the RKHS generated by k . This amounts to saying that g lies in the
image of T 1/2
. Therefore, the condition used here is slightly more restrictive.
k

from the ﬁnite sample case. However, the convergence speed is the crucial question. This question is
not trivial, because eigenvector stability is known to be linked to the gap between the corresponding
eigenvalues, which will be fairly small for small eigenvalues. In fact, for example, the results from
[14] do not scale properly with the corresponding eigenvalue, such that the bounds are too loose. A
number of recent results on the spectral properties of the kernel matrix [1, 2] speciﬁcally deal with
error bounds for small eigenvalues and their associated spectral projections. Using these results, we
obtain the following bound on u>
Theorem 1 Let g = P∞
i G.4
i=1 αiλiψi as explained above, and let G = (g(X1 ), . . . , g(Xn )). Then,
with high probability.
+ rar ΛrO(1) + Tr + p
pΛrO(n−1/2 ),
1√
i G| < 2liar ci (1 + O(rn−1/4 ))
|u>
n
ATrO(n−1/4 ) + rar
around li , ar = Pr
where r balances the different terms (1 ≤ r ≤ n), ci measures the size of the eigenvalue cluster
i=1 |αi | is a measure of the size of the ﬁrst r components, Λr is the sum of all
eigenvalues smaller than λr , A is the supremum norm of g , and Tr is the error of projecting g to the
space spanned by the ﬁrst r eigenfunctions.

The bound consists of a part which scales with li (ﬁrst term) and a part which does not (remaining
terms). Typically, the bound initially scales with li until the non-scaling part dominates the bound
for larger i. These two parts are balanced by r . However, note that all terms which do not scale with
li will typically be small: for smooth kernels, the eigenvalues quickly decay to zero as r → ∞. The
related quantities Λr , and Tr , will also decay to zero at slightly slower rates. Therefore, by adjusting
r (as n → ∞), the non-scaling part can be made arbitrarily small, leading to a small bound on |u>
i G|
for larger i.

Put differently, the bound shows that the relevant information vector G (as introduced in Section 2)
is contained in a number of leading PCA components up to a negligible error. The number of dimen-
sions depends on the asymptotic coefﬁcients αi and the decay rate of the asymptotic eigenvalues of
k . Since this rate is related to the smoothness of the kernel function, the dimension will be small for
smooth kernels whose leading eigenfunctions ψi permit good approximation of g .

4 The Relevant Dimension Estimation Algorithm

In this section, we will propose the relevant dimension estimation (RDE) algorithm which estimates
the dimensionality of the relevant information from a ﬁnite sample, allowing us to analyze the ﬁt
between a kernel function and a data set in a practical way.
Dimension Estimation We propose an approach which is motivated by the geometric ﬁndings
explained above. Since G is not known, we can only observe the contributions of the kernel PCA
components to Y , which can be written as Y = G + N (see Section 2). The contributions u>
i Y
will thus be formed as a superposition of u>
i Y = u>
i G + u>
i N . Now, by Theorem 1, we know that
G will be very close to zero for the latter coefﬁcients, while on the other hand, the noise N will be
s = u>
equally distributed over all coefﬁcients. Therefore, the kernel PCA coefﬁcients
i Y will have
the shape of an evenly distributed noise ﬂoor u>
i N from which the coefﬁcients u>
i G of the relevant
information protrude (see Figure 2(b) for an example).
We thus propose the following algorithm: Given a ﬁxed kernel k , we estimate the true dimension
1Y , . . . , u>
by ﬁtting a two component model to the coordinates of the label vector. Let s = (u>
(cid:26)N (0, σ2
nY ).
Then, assume that
1 ≤ i ≤ d
1 )
N (0, σ2
2 ) d < i ≤ n.

si ∼

4We have tried to reduce the bound to its most prominent features. For a more detailed explanation of the
quantities and the proof, see the appendix. Also, the conﬁdence δ of the “with high probability ” part is hidden
in the O( · ) notation. We have used the O( · ) notation rather deliberately to exhibit the dominant constants.

(a)

(b)

(c)

Figure 2: Further plots on the toy example from the introduction.
(a) contains the kernel PCA
component contributions (dots), and the training and test error by projecting the data set to the
given number of leading kernel PCA components. (b) shows the negative log-likelihood of the two
component model used to estimate the dimensionality of the data. (c) The resulting ﬁt when using
only the ﬁrst four components.

dX
We select the d minimizing the negative log-likelihood, which is proportional to
1 + n − d
log σ2
n
i=1

‘(d) = d
n

2 =
i , σ2
s2

1
d

log σ2
2 ,

with

1 =
σ2

nX
i=d+1

s2
i .

(1)

1
n − d

Model Selection for Kernel Choice For different kernels, we again use the likelihood and select
the kernel which leads to the best ﬁt in terms of the likelihood. If the kernel width does not match
the scale of the structure of the data set, the ﬁt of the two component model will be inferior: for very
small or very large kernels, the kernel PCA coefﬁcients of Y have no clear structure, such that the
likelihood will be small. For example, for Gaussian kernels, for very small kernel widths, noise is in-
terpreted as relevant information, such that there appears to be no noise, only very high-dimensional
data. On the other hand, for very large kernel widths, any structure will be indistinguishable from
noise such that the problem appears to be very noisy with almost no structure. In both cases, ﬁtting
the two component model will not work very well, leading to large values of ‘.
Experimental Error Estimation The estimated dimension can be used to estimate the noise
Pn
level present in the data set. The idea is to measure the error between the projected label
vector ˆG = πd (Y ), which approximates the true label information G. The resulting number
i=1 1{[ ˆG]i 6= Yi } is an estimate of the fraction of misclassiﬁed examples in the training
ˆerr = 1
n
set, and therefore an estimate for the noise level in the class labels.
A Note on Consistency Both the estimate of ˆG and the noise level are consistent if the estimated
dimension d scales sub-linearly with n. The argument can be sketched as follows: since the kernel
PCA components do not depend on Y , the noise N contained in Y is projected to a random subspace
n kπd (N )k2 ≈ d
n kN k2 ) → 0 as n → ∞, since d/n → 0 and
n ( 1
of dimension d. Therefore, 1
n kN k2 → E(N 2 ). Empirically, d was found to be rather stable, but in principle, the condition
1

on d could even be enforced by adding a small sub-linear term (for example,
estimated dimension d).

√

n, or log n, to the

5 Experiments

Toy Data Set Returning to the toy example from the introduction, let us now take a closer look at
this data set. In Figure 2(a), the spectrum for the toy data set is plotted. We can see that every kernel
PCA component contributes to the observed class label vector. However, most of these contributions
are noise, since the classes are overlapping. The RDE method estimates that only the ﬁrst four
components are relevant. This behavior of the algorithm can also be seen from the training and
independent test error measured on a second data set of size 1000 which can also be found in this
plot. In Figure 2(b), the log-likelihoods from (1) are shown, and one observes a well pronounced
minimum. Finally, in Figure 2(c), the resulting ﬁt is shown.
Benchmark data sets We performed experiments on the classiﬁcation learning sets from [7]. For
each of the data sets, we de-noise the data set using a family of rbf kernels by projecting the class
labels to the estimated number of leading kernel PCA components. The kernel width is also selected
automatically using the achieved log-likelihood as described above. The width of the rbf kernel is
selected from 20 logarithmically spaced points between 10−2 and 104 for each data set.
on cross-validation. More concretely, the matrix S = Pd
For the dimension estimation task, we compare our RDE method to a dimensionality estimate based
i=1 uiu>
i computes the projection to the
leading d kernel PCA components. Interpreting the matrix S as a linear ﬁt matrix, the leave-one-
out cross-validation error can be computed in closed form (see [12])5 , since S is diagonal with
respect to the eigenvector basis ui . Evaluating the cross-validation error for all dimensions and for
a number of kernel parameters, one can select the best dimension and kernel parameter. Since the
cross-validation can be computed efﬁciently, the computational demands of both methods are equal.
Table 3 shows the resulting dimension estimates. We see that both methods perform on par, which
shows that the strong structural prior assumption underlying RDE is justiﬁed.

For the de-noising task, we have compared a (unregularized) least-squares ﬁt in the reduced feature
space (kPCR) against kernel ridge regression (KRR) and support vector machines (SVM) on the
same data set. The resulting test errors are plotted also in Table 3. We see that a relatively simple
method on the reduced features leads to classiﬁcation which is on par with the state-of-the-art com-
petitors. Also note that the estimated error rates match the actually observed error rates quite well,
although there is a tendency to under-estimate the true error.

Finally, inspecting the estimated dimension and noise level reveals that the data sets breast-cancer,
diabetis, ﬂare-solar , german, and titanic all have only moderately large dimensionalities. This sug-
gest that these data sets are inherently noisy and better results cannot be expected, at least within
the family of rbf kernels. On the other hand, the data set image seems to be particularly noise free,
given that one can achieve a small error in spite of the large dimensionality. Finally, the splice data
set seems to be a good candidate to bene ﬁt from more data.

6 Conclusion

Both in theory and on practical data sets, we have shown that the relevant information in a supervised
learning scenario is contained in the leading projected kernel PCA components if the kernel matches
the learning problem. The theory provides a consistent estimation for the expected class labels and
the noise level. This behavior complements the common statistical learning theoretical view on ker-
nel based learning with insight on the interaction of data and kernel: A well chosen kernel (a) makes
the model estimate efﬁciently and generalize well, since only a comparatively low dimensional rep-
resentation needs to be learned for a ﬁxed given data size and (b) permits a de-noising step that
discards some void projected kernel PCA directions and thus provides a regularized model.

Practically, our RDE algorithm automatically selects the appropriate kernel model for the data and
extracts as additional side information an estimate of the effective dimension and estimated expected

5This applies only to the 2-norm. However, as the performance of 2-norm based methods like kernel ridge
regression on classiﬁcation problems show, the 2-norm is also informative on the classiﬁcation performance.

data set
banana
breast-cancer
diabetis
ﬂare-solar
german
heart
image
ringnorm
splice
thyroid
titanic
twonorm
waveform

dim dim (cv)
24
26
2
2
9
9
10
10
12
12
5
4
368
272
37
36
89
92
17
18
6
4
2
2
23
14

est. error rate
8.8 ± 1.5
25.6 ± 2.1
21.5 ± 1.3
32.9 ± 1.2
22.9 ± 1.1
15.8 ± 2.5
1.7 ± 1.0
1.9 ± 0.7
9.2 ± 1.3
2.0 ± 1.0
20.8 ± 3.8
2.3 ± 0.7
8.4 ± 1.5

kPCR
11.3 ± 0.7
27.0 ± 4.6
23.6 ± 1.8
33.3 ± 1.8
24.1 ± 2.1
16.7 ± 3.8
4.2 ± 0.9
4.4 ± 1.2
13.8 ± 0.9
5.1 ± 2.1
22.9 ± 1.6
2.4 ± 0.1
10.8 ± 0.9

KRR
10.6 ± 0.5
26.5 ± 4.7
23.2 ± 1.7
34.1 ± 1.8
23.5 ± 2.2
16.6 ± 3.5
2.8 ± 0.5
4.7 ± 0.8
11.0 ± 0.6
4.3 ± 2.3
22.5 ± 1.0
2.8 ± 0.2
9.7 ± 0.4

SVM
11.5 ± 0.7
26.0 ± 4.7
23.5 ± 1.7
32.4 ± 1.8
23.6 ± 2.1
16.0 ± 3.3
3.0 ± 0.6
1.7 ± 0.1
10.9 ± 0.6
4.8 ± 2.2
22.4 ± 1.0
3.0 ± 0.2
9.9 ± 0.4

Figure 3: Estimated dimensions and error rates for the benchmark data sets from [7]. “dim” shows
the medians of the estimated dimensionalities over the resamples. “dim (cv)” shows the same quan-
tity, but this time, the dimensions have been estimated by leave-one-out cross-validation. “est. error
rate” is the estimated error rate on the training set by comparing the de-noise class labels to the true
class labels. The last three columns show the test error rates of three algorithms: “kPCR” predicts
using a simple least-squares hyperplane on the estimated subspace in feature space, “KRR” is kernel
ridge regression with parameters estimated using leave-one-out cross-validation, and “SVM” are the
original error rates from [7].

error for the learning problem. Compared to common cross-validation techniques one could argue
that all we have achieved is to ﬁnd a similar model as usual at a comparable computing time. How-
ever, we would like to emphasize that the side information extracted by our procedure contributes
to a better understanding of the learning problem at hand: Is the classiﬁcation result limited due to
intrinsic high dimensional structure or are we facing noise and nuisance dimensions? Simulations
show the usefulness of our RDE algorithm.

An interesting future direction lies in combining these results with generalization bounds which
are also based on the notion of an effective dimension, this time, however, with respect to some
regularized hypothesis class (see, for example, [13]). Linking the effective dimension of the data set
with the dimension of a learning algorithm, one could obtain data dependent bounds in a natural way
with the potential to be tighter than bounds which are based on the abstract capacity of a hypothesis
class.

References
[1] ML Braun. Spectral Properties of the Kernel Matrix and Their Application to Kernel Methods in Ma-
chine Learning. PhD thesis, University of Bonn, 2005. Available electronically at http://hss.ulb.uni-
bonn.de/diss online/math nat fak/2005/braun mikio.
[2] ML Braun. Accurate error bounds for the eigenvalues of the kernel matrix. Journal of Machine Learning
Research, 2006. To appear.
[3] V Koltchinskii and E Gin ´e. Random matrix approximation of spectra of integral operators. Bernoulli,
6(1):113–167, 2000.
[4] VI Koltchinskii. Asymptotics of spectral projections of some random matrices approximating integral
operators. Progress in Probability, 43:191–227, 1998.
[5] S Mika, B Sch ¨olkopf, A Smola, K-R M ¨uller, M Scholz, and Gunnar R ¨athsch. Kernel PCA and de-noising
in feature space. In Advances in Neural Information Processing Systems 11. MIT Press, 1999.
[6] K-R M ¨uller, S Mika, G R ¨atsch, K Tsuda, and B Sch ¨olkopf. An introduction to kernel-based learning
algorithms. IEEE Transaction on Neural Networks, 12(2):181–201, May 2001.
[7] G R ¨atsch, T Onoda, and K-R M ¨uller. Soft margins for AdaBoost. Machine Learning, 42(3):287–320,
March 2001.
[8] B Sch ¨olkopf, S Mika, CJC Burges, P Knirsch, K-R M ¨uller, G R ¨atsch, and AJ Smola. Input space vs. fea-
ture space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000–1017, Septem-
ber 1999.

[9] B Sch ¨olkopf and AJ Smola. Learning with Kernels. MIT Press, 2001.
[10] B Sch ¨olkopf, AJ Smola, and K-R M ¨uller. Nonlinear component analysis as a kernel eigenvalue problem.
Neural Computation, 10:1299–1319, 1998.
[11] V Vapnik. Statistical Learning Theory. Wiley, 1998.
[12] G Wahba. Spline Models For Observational Data. Society for Industrial and Applied Mathematics, 1990.
[13] T Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural Computation,
17:2077–2098, 2005.
[14] L Zwald and G Blanchard. On the convergence of eigenspaces in kernel principal components analysis.
In Advances in Neural Information Processing Systems (NIPS 2005), volume 18, 2006.

A Proof of Theorem 1
k(x, x0 ) = P∞
First, let us collect the deﬁnitions concerning kernel functions. Let k be a Mercer kernel with
kernel k is approximated by the truncated kernel matrix is kr (x, x0 ) = Pr
i=1 λiψi (x)ψi (x0 ), and k(x, x) ≤ K < ∞. The kernel matrix of k for an n-sample
X1 , . . . , Xn is [K]ij = k(Xi , Xj )/n. Eigenvalues of K are li , and its eigenvectors are ui . The
i=1 λiψi (x)ψi (x0 ), and its
kernel matrix is denoted by Kr , whose eigenvalues are mi . The approximation error is measured
by Er = Kr − K. We will measure the amount of clustering ci of the eigenvalues by the number
√
of eigenvalues of Kr between li /2 and 2li . The matrix containing the sample vectors of the ﬁrst
n, 1 ≤ i ≤ n, 1 ≤ ‘ ≤ r . Since the
r eigenfunctions ψi of k is given by [Ψr ]i‘ = ψ‘ (Xi )/
let Λr = P∞
eigenfunctions are orthogonal asymptotically, we can expect that the sample vectors of the eigen-
rΨr − I. Finally,
functions converge to either 0 or 1. The error is measured by the matrix Cr = Ψ>
Next, we collect deﬁnitions concerning some function f . Let f = P∞
i=r+1 λi .
Pr
i=1 λiαiψi with (αi ) ∈
‘2 , and |f | ≤ A < ∞. The size of the contribution of the ﬁrst
(P∞
r terms is measured by ar =
i=1 |αi |. Deﬁne the error of truncating f to the ﬁrst r elements of its series expansion by Tr =
i )1/2 .
i α2
i=r+1 λ2
(cid:2)liD(r, n) + E (r, n) + T (r, n)(cid:3)
The proof of Theorem 1 is based on performing rough estimates of the bound from Theorem 4.92
in [1]. The bound is
1√
i f (X)| < min
|u>
r 1
1≤r≤n
n
T (r, n) = Tr + p
where the three terms are given by
D(r, n) = 2ar kΨ+
r kci ,
E (r, n) = 2rar kΨ+
r kkEr k,
4
,
F Tr
nδ
larger than 1− δ ([1], Lemma 3.135) that kCr k ≤ rpr(r + 1)K/λr nδ = r2O(n−1/2 ) with a rather
It holds that kΨ+
r k ≤ (1 − kΨ>
rΨr − Ik)1/2 = (1 − kCr k)−1/2 ([1], Lemma 4.44). Furthermore,
kCr k → 1 as n → ∞ for ﬁxed r . For kernel with bounded diagonal, it holds that with probability
large constant, especially, if λr is small. Consequently, kΨ+
r k ≤ (1− kCr k)−1/2 = 1+ O(rn−1/4 ).
r 2K Λr
= Λr + pΛrO(n− 1
Now, Lemma 3.135 in [1] bounds kEr k from which we can derive the asymptotic
kEr k ≤ λr + Λr +
2 ),
nδ
(cid:17)
(cid:16)
pΛrO(n− 1
Λr + pΛrO(n− 1
assuming that K will be reasonably small (for example, for rbf-kernels, K = 1). Combining this
with our rate for kΨ+
r k, we obtain
4 )) = 2rar Λr (1+O(rn− 1
(1+O(rn− 1
2 )
2 ).
4 ))+rar
E (r, n) = 2rar
Finally, we obtain
pΛrO(n− 1
2 ). + Tr + p
1√
i f (X)| = 2liar ci (1 + O(rn− 1
|u>
4 ))
n
+ 2rar Λr (1 + O(rn− 1
ATrO(n− 1
4 )) + rar
2 ).
If we assume that Λr will be rather small, we replace 1 + O(rn− 1
4 ) by O(1) for the second term and
(cid:4)
obtain the claimed rate.


Uni ﬁed Inference for Variational Bayesian Linear
Gaussian State-Space Models

David Barber
IDIAP Research Institute
rue du Simplon 4, Martigny, Switzerland
david.barber@idiap.ch

Silvia Chiappa
IDIAP Research Institute
rue du Simplon 4, Martigny, Switzerland
silvia.chiappa@idiap.ch

Abstract

Linear Gaussian State-Space Models are widely used and a Bayesian treatment
of parameters is therefore of considerable interest. The approximate Variational
Bayesian method applied to these models is an attractive approach, used success-
fully in applications ranging from acoustics to bioinformatics. The most challeng-
ing aspect of implementing the method is in performing inference on the hidden
state sequence of the model. We show how to convert the inference problem so
that standard Kalman Filtering/Smoothing recursions from the literature may be
applied. This is in contrast to previously published approaches based on Belief
Propagation. Our framework both simpli ﬁes and uni ﬁes the in
ference problem, so
that future applications may be more easily developed. We demonstrate the ele-
gance of the approach on Bayesian temporal ICA, with an application to ﬁnding
independent dynamical processes underlying noisy EEG signals.

1 Linear Gaussian State-Space Models

Linear Gaussian State-Space Models (LGSSMs)1 are fundamental in time-series analysis [1, 2, 3].
2 are generated from an underlying dynamical system on h1:T
In these models the observations v1:T
according to:

t ∼ N (0V , ΣV ),
t ∼ N (0H , ΣH ) ,
t , ηh
ht = Aht−1 + ηh
t , ηv
vt = Bht + ηv
where N (µ, Σ) denotes a Gaussian with mean µ and covariance Σ, and 0X denotes an X -
dimensional zero vector. The observation vt has dimension V and the hidden state ht has dimension
H . Probabilistically, the LGSSM is deﬁned by:

p(v1:T , h1:T |Θ) = p(v1 |h1 )p(h1 )

T
Yt=2
with p(vt |ht ) = N (Bht , ΣV ), p(ht |ht−1 ) = N (Aht−1 , ΣH ), p(h1 ) = N (µ, Σ) and where
Θ = {A, B , ΣH , ΣV , µ, Σ} denotes the model parameters. Because of the widespread use of these
models, a Bayesian treatment of parameters is of considerable interest [4, 5, 6, 7, 8].

p(vt |ht )p(ht |ht−1 ),

An exact implementation of the Bayesian LGSSM is formally intractable [8], and recently a Varia-
tional Bayesian (VB) approximation has been studied [4, 5, 6, 7, 9]. The most challenging part of
implementing the VB method is performing inference over h1:T , and previous authors have devel-
oped their own specialized routines, based on Belief Propagation, since standard LGSSM inference
routines appear, at ﬁrst sight, not to be applicable.

1Also called Kalman Filters/Smoothers, Linear Dynamical Systems.
2 v1:T denotes v1 , . . . , vT .

A key contribution of this paper is to show how the Variational Bayesian treatment of the
LGSSM can be implemented using standard LGSSM inference routines. Based on the insight we
provide, any standard inference method may be applied, including those speci ﬁcally addressed to
improve numerical stability [2, 10, 11]. In this article, we decided to describe the predictor-corrector
and Rauch-Tung-Striebel recursions [2], and also suggest a small modi ﬁcation that reduces compu-
tational cost.

The Bayesian LGSSM is particularly of interest when strong prior constraints are needed to ﬁnd
adequate solutions. One such case is in EEG signal analysis, whereby we wish to extract sources
that evolve independently through time. Since EEG is particularly noisy [12], a prior that encourages
sources to have preferential dynamics is advantageous. This application is discussed in Section 4,
and demonstrates the ease of applying our VB framework.

2 Bayesian Linear Gaussian State-Space Models

p(v1:T |Θ)p(Θ| ˆΘ) .

In the Bayesian treatment of the LGSSM, instead of considering the model parameters Θ as ﬁxed,
we deﬁne a prior distribution p(Θ| ˆΘ), where ˆΘ is a set of hyperparameters. Then:
p(v1:T | ˆΘ) = ZΘ
In a full Bayesian treatment we would deﬁne additional prior distributions over the hyperparameters
ˆΘ. Here we take instead the ML-II (‘evidence’) framework, in which the optimal set of hyperpa-
rameters is found by maximizing p(v1:T | ˆΘ) with respect to ˆΘ [6, 7, 9].
For the parameter priors, here we deﬁne Gaussians on the colu mns of A and B 3 :
H
H
Yj=1
Yj=1
which has the effect of biasing the transition and emission matrices to desired forms ˆA and ˆB . The
V are Wishart distributions [7]4 . In the
H and Σ−1
conjugate priors for general inverse covariances Σ−1
simpler case assumed here of diagonal covariances these become Gamma distributions [5, 7]. The
hyperparameters are then ˆΘ = {α, β }5 .

H (Aj − ˆAj ) ,
Σ−1

p(A|α, ΣH ) ∝

2 (Bj − ˆBj )T
βj

e−

V (Bj − ˆBj ) ,
Σ−1

(1)

2 (Aj − ˆAj )T
αj

e−

p(B |β , ΣV ) ∝

Variational Bayes

Optimizing Eq. (1) with respect to ˆΘ is difﬁcult due to the intractability of the integrals. Inst ead, in
VB, one considers the lower bound [6, 7, 9]6 :
L = log p(v1:T | ˆΘ) ≥ Hq (Θ, h1:T ) + Dlog p(Θ| ˆΘ)Eq(Θ)

+ hE (h1:T , Θ)iq(Θ,h1:T ) ≡ F ,

where

E (h1:T , Θ) ≡ log p(v1:T , h1:T |Θ).
Hd (x) signi ﬁes the entropy of the distribution d(x), and h·id(x) denotes the expectation operator.
The key approximation in VB is q(Θ, h1:T ) ≡ q(Θ)q(h1:T ), from which one may show that, for
optimality of F ,
q(Θ) ∝ p(Θ| ˆΘ)ehE (h1:T ,Θ)iq(h1:T ) .
q(h1:T ) ∝ ehE (h1:T ,Θ)iq(Θ) ,
These coupled equations need to be iterated to convergence. The updates for the parameters q(Θ)
are straightforward and are given in Appendices A and B. Once converged, the hyperparameters are
updated by maximizing F with respect to ˆΘ, which lead to simple update formulae [7].
Our main concern is with the update for q(h1:T ), for which this paper makes a departure from
treatments previously presented.

3More general Gaussian priors may be more suitable depending on the application.
4For expositional simplicity, we do not put priors on µ and Σ.
5For simplicity, we keep the parameters of the Gamma priors ﬁxed.
6Strictly we should write throughout q(·|v1:T ). We omit the dependence on v1:T for notational convenience.

3 Uniﬁed Inference on q(h1:T )

−

T

1
2

7 :
Optimally q(h1:T ) is Gaussian since, up to a constant, hE (h1:T , Θ)iq(Θ) is quadratic in h1:T
T
·(cid:173)(vt −Bht )TΣ−1
H (ht −Aht−1 )Eq(A,ΣH )¸ .
V (vt −Bht )®q(B ,ΣV ) + D(ht −Aht−1 )
Xt=1
Σ−1
(2)
In addition, optimally, q(A|ΣH ) and q(B |ΣV ) are Gaussians (see Appendix A), so we can easily
carry out the averages in Eq. (2). The further averages over q(ΣH ) and q(ΣV ) are also easy due
to conjugacy. Whilst this deﬁnes the distribution q(h1:T ), quantities such as q(ht ), required for
example for the parameter updates (see the Appendices), need to be inferred from this distribution.
Clearly, in the non-Bayesian case, the averages over the parameters are not present, and the above
simply represents the posterior distribution of an LGSSM whose visible variables have been clamped
into their evidential states. In that case, inference can be performed using any standard LGSSM
routine. Our aim, therefore, is to try to represent the averaged Eq. (2) directly as the posterior
distribution ˜q(h1:T |˜v1:T ) of an LGSSM , for some suitable parameter settings.

Mean + Fluctuation Decomposition

A useful decomposition is to write
V (vt − Bht )®q(B ,ΣV )= (vt − hB i ht )T (cid:173)Σ−1
(cid:173)(vt − Bht )TΣ−1
V ® (vt − hB i ht )
+ hT
t SB ht
| {z }
|
}
{z
f luctuation
mean
and similarly
H (ht −Aht−1 )®q(A,ΣH )= (ht − hAi ht−1 )T (cid:173)Σ−1
(cid:173)(ht −Aht−1 )TΣ−1
H ® (ht − hAi ht−1 )
+hT
t−1SAht−1
|
}
{z
|
}
{z
mean
f luctuation
V B ® − hB iT (cid:173)Σ−1
where the parameter covariances are SB ≡ (cid:173)B TΣ−1
V ® hB i = V H −1
B and SA ≡
H A®− hAiT (cid:173)Σ−1
(cid:173)ATΣ−1
H ® hAi = HH −1
A (for HA and HB deﬁned in Appendix A). The mean terms
simply represent a clamped LGSSM with averaged parameters. However, the extra contributions
from the ﬂuctuations mean that Eq.
(2) cannot be written as a c lamped LGSSM with averaged
parameters. In order to deal with these extra terms, our idea is to treat the ﬂuctuations as arising
from an augmented visible variable, for which Eq. (2) can then be considered as a clamped LGSSM.

,

,

Inference Using an Augmented LGSSM

To represent Eq. (2) as an LGSSM ˜q(h1:T |˜v1:T ), we may augment vt and B as8 :
˜B = vert(hB i , UA , UB ),
˜vt = vert(vt , 0H , 0H ),
where UA is the Cholesky decomposition of SA , so that U T
AUA = SA . Similarly, UB is the Cholesky
decomposition of SB . The equivalent LGSSM ˜q(h1:T |˜v1:T ) is then completed by specifying9
H ®−1
V ®−1
˜ΣH ≡ (cid:173)Σ−1
˜ΣV ≡ diag((cid:173)Σ−1
˜A ≡ hAi ,
˜Σ ≡ Σ.
˜µ ≡ µ,
, IH , IH ),
,
The validity of this parameter assignment can be checked by showing that, up to negligible constants,
the exponent of this augmented LGSSM has the same form as Eq. (2)10 . Now that this has been
written as an LGSSM ˜q(h1:T |˜v1:T ), standard inference routines in the literature may be applied to
compute q(ht |v1:T ) = ˜q(ht |˜v1:T ) [1, 2, 11]11 .
7For simplicity of exposition, we ignore the ﬁrst time-point here.
8The notation vert(x1 , . . . , xn ) stands for vertically concatenating the arguments x1 , . . . , xn .
9Strictly, we need a time-dependent emission ˜Bt = ˜B , for t = 1, . . . , T − 1. For time T , ˜BT has the
Cholesky factor UA replaced by 0H,H .
10There are several ways of achieving a similar augmentation. We chose this since, in the non-Bayesian limit
UA = UB = 0H,H , no numerical instabilities would be introduced.
11Note that, since the augmented LGSSM ˜q(h1:T |˜v1:T ) is designed to match the fully clamped distribution
q(h1:T |v1:T ), the ﬁltered posterior ˜q(ht |˜v1:t ) does not correspond to q(ht |v1:t ).

U T
AB

Algorithm 1 LGSSM: Forward and backward recursive updates. The smoothed posterior p(ht |v1:T )
is returned in the mean ˆhT
t .
t and covariance P T
procedure FORWARD
1a: P ← Σ
1b: P ← DΣ, where D ≡ I − ΣUAB ¡I + U T
AB ΣUAB ¢−1
2a: ˆh0
1 ← µ
2b: ˆh0
1 ← Dµ
3: K ← P B T (BP B T + ΣV )−1 , P 1
1 + K (vt − B ˆh0
1 ← ˆh0
1 ← (I − KB )P , ˆh1
1 )
for t ← 2, T do
t ← AP t−1
4: P t−1
t−1 AT + ΣH
5a: P ← P t−1
t
t UAB ¢−1
t UAB ¡I + U T
AB P t−1
, where Dt ≡ I − P t−1
5b: P ← DtP t−1
t
t ← Aˆht−1
6a: ˆht−1
t−1
6b: ˆht−1
t ← DtAˆht−1
t−1
7: K ← P B T (BP B T + ΣV )−1 , P t
t ← (I − KB )P , ˆht
t + K (vt − B ˆht−1
t ← ˆht−1
t
end for
end procedure
procedure BACKWARD
for t ← T − 1, 1 do
←−
t AT (P t
t+1 )−1
At ← P t
←−
←−
T
P T
t ← P t
t+1 − P t
At (P T
t +
t+1 )
At
←−
t+1 − Aˆht
At (ˆhT
t ← ˆht
ˆhT
t )
t +
end for
end procedure

U T
AB

)

For completeness, we decided to describe the standard predictor-corrector form of the Kalman Fil-
ter, together with the Rauch-Tung-Striebel Smoother [2]. These are given in Algorithm 1, where
˜q(ht |˜v1:T ) is computed by calling the FORWARD and BACKWARD procedures.
We present two variants of the FORWARD pass. Either we may call procedure FORWARD in
Algorithm 1 with parameters ˜A, ˜B , ˜ΣH , ˜ΣV , ˜µ, ˜Σ and the augmented visible variables ˜vt in which
we use steps 1a, 2a, 5a and 6a. This is exactly the predictor-corrector form of a Kalman Filter [2].
Otherwise, in order to reduce the computational cost, we may call procedure FORWARD with the
V ®−1
parameters ˜A, hB i , ˜ΣH , (cid:173)Σ−1
, ˜µ, ˜Σ and the original visible variable vt in which we use steps
1b (where U T
AB UAB ≡ SA + SB ), 2b, 5b and 6b. The two algorithms are mathematically equivalent.
Computing q(ht |v1:T ) = ˜q(ht |˜v1:T ) is then completed by calling the common BACKWARD pass.
The important point here is that the reader may supply any standard Kalman Filtering/Smoothing
routine, and simply call it with the appropriate parameters. In some parameter regimes, or in very
long time-series, numerical stability may be a serious concern, for which several stabilized algo-
rithms have been developed over the years, for example the square-root forms [2, 10, 11]. By
converting the problem to a standard form, we have therefore uni ﬁed and simpli ﬁed inference, so
that future applications may be more readily developed12 .

3.1 Relation to Previous Approaches

An alternative approach to the one above, and taken in [5, 7], is to write the posterior as
T
Xt=2
for suitably deﬁned quadratic forms φt (ht−1 , ht ). Here the potentials φt (ht−1 , ht ) encode the av-
eraging over the parameters A, B , ΣH , ΣV . The approach taken in [7] is to recognize this as a

φt (ht−1 , ht ) + const.

log q(h1:T ) =

12The computation of the log-likelihood bound does not require any augmentation.

pairwise Markov chain, for which the Belief Propagation recursions may be applied. The approach
in [5] is based on a Kullback-Leibler minimization of the posterior with a chain structure, which is
algorithmically equivalent to Belief Propagation. Whilst mathematically valid procedures, the re-
sulting algorithms do not correspond to any of the standard forms in the Kalman Filtering/Smoothing
literature, whose properties have been well studied [14].

4 An Application to Bayesian ICA

A particular case for which the Bayesian LGSSM is of interest is in
extracting independent source signals underlying a multivariate time-
series [5, 15]. This will demonstrate how the approach developed in
Section 3 makes VB easily to apply. The sources si are modeled as
independent in the following sense:

for i 6= j ,

i, j = 1, . . . , C .

1:T , sj
1:T )p(sj
p(si
1:T ) = p(si
1:T ),
Independence implies block diagonal transition and state noise matri-
ces A, ΣH and Σ, where each block c has dimension Hc . A one di-
mensional source sc
t for each independent dynamical subsystem is then
t = 1T
t , where 1c is a unit vector and hc
formed from sc
t is the state of
c hc
dynamical system c. Combining the sources, we can write st = P ht ,
where P = diag(1T
1 , . . . , 1T
C ), ht = vert(h1
t ). The resulting
t , . . . , hC
emission matrix is constrained to be of the form B = W P , where
W is the V × C mixing matrix. This means that the observations
are formed from linearly mixing the sources, vt = W st + ηv
t . The
graphical structure of this model is presented in Fig 1. To encourage
redundant components to be removed, we place a zero mean Gaussian
prior on W . In this case, we do not deﬁne a prior for the parameters
ΣH and ΣV which are instead considered as hyperparameters. More details of the model are given
in [15]. The constraint B = W P requires a minor modi ﬁcation from Section 3, as we discuss be low.

Figure 1: The structure of
the LGSSM for ICA.

Inference on q(h1:T )

or B occurs, namely:
A small modi ﬁcation of the mean + ﬂuctuation decomposition f
(cid:173)(vt − Bht )TΣ−1
V (vt − Bht )®q(W ) = (vt − hB i ht )TΣ−1
V (vt − hB i ht ) + hT
t P TSW P ht ,
where hB i ≡ hW i P and SW = V H −1
W . The quantities hW i and HW are obtained as in Appendix
A.1 with the replacement ht ← P ht . To represent the above as a LGSSM, we augment vt and B as
˜B = vert(hB i , UA , UW P ),
˜vt = vert(vt , 0H , 0C ),
where UW is the Cholesky decomposition of SW . The equivalent LGSSM is then completed by
specifying ˜A ≡ hAi, ˜ΣH ≡ ΣH , ˜ΣV ≡ diag(ΣV , IH , IC ), ˜µ ≡ µ, ˜Σ ≡ Σ, and inference for
q(h1:T ) performed using Algorithm 1. This demonstrates the elegance and unity of the approach in
Section 3, since no new algorithm needs to be developed to perform inference, even in this special
constrained parameter case.

4.1 Demonstration

As a simple demonstration, we used an LGSSM to generate 3 sources sc
t with random 5×5 transition
matrices Ac , µ = 0H and Σ ≡ ΣH ≡ IH . The sources were mixed into three observations
t , for W chosen with elements from a zero mean unit variance Gaussian distribution,
vt = W st + ηv
and ΣV = IV . We then trained a Bayesian LGSSM with 5 sources and 7 × 7 transition matrices Ac .
ˆAc ≡ 0Hc ,Hc for all sources. In Fig2a and Fig
To bias the model to ﬁnd the simplest sources, we used
2b we see the original sources and the noisy observations respectively. In Fig2c we see the estimated
sources from our method after convergence of the hyperparameter updates. Two of the 5 sources
have been removed, and the remaining three are a reasonable estimation of the original sources.
Another possible approach for introducing prior knowledge is to use a Maximum a Posteriori (MAP)

0

50

50

100

250

300

0

200

100

150
150
150
150
(b)
(d)
(c)
(a)
Figure 2: (a) Original sources st .
(b) Observations resulting from mixing the original sources,
t ∼ N (0, I ). (c) Recovered sources using the Bayesian LGSSM. (d) Sources
t , ηv
vt = W st + ηv
found with MAP LGSSM.

300

0

300

0

200

250

200

250

300

200

250

50

100

50

100

0  

1

2

3s

0  

1

2

3s

0

1

2

3s

0  

1

2

3s

0  

1

2

3s

(a)

(b)

(c)

(d)

(e)

Figure 3: (a) Original raw EEG recordings from 4 channels. (b-e) 16 sources st estimated by the
Bayesian LGSSM.

procedure by adding a prior term to the original log-likelihood log p(v1:T |A, W, ΣH , ΣV , µ, Σ) +
log p(A|α) + log p(W |β ). However, it is not clear how to reliably ﬁnd the hyperparame ters α and β
in this case. One solution is to estimate them by optimizing the new objective function jointly with
respect to the parameters and hyperparameters (this is the so-called joint map estimation – see for
example [16]). A typical result of using this joint MAP approach on the arti ﬁcial data is presented
in Fig 2d. The joint MAP does not estimate the hyperparameters well, and the incorrect number of
sources is identi ﬁed.

4.2 Application to EEG Analysis

In Fig 3a we plot three seconds of EEG data recorded from 4 channels (located in the right hemi-
sphere) while a person is performing imagined movement of the right hand. As is typical in EEG,
each channel shows drift terms below 1 Hz which correspond to artifacts of the instrumentation,
together with the presence of 50 Hz mains contamination and masks the rhythmical activity related
to the mental task, mainly centered at 10 and 20 Hz [17]. We would therefore like a method which
enables us to extract components in these information-rich 10 and 20 Hz frequency bands. Stan-
dard ICA methods such as FastICA do not ﬁnd satisfactory sour ces based on raw ‘noisy’ data, and
preprocessing with band-pass ﬁlters is usually required. A dditionally, in EEG research, ﬂexibility
in the number of recovered sources is important since there may be many independent oscillators
of interest underlying the observations and we would like some way to automatically determine
their effective number. To preferentially ﬁnd sources at pa rticular frequencies, we speci ﬁed a block
diagonal matrix ˆAc for each source c, where each block is a 2 × 2 rotation matrix at the desired
frequency. We deﬁned the following 16 groups of frequencies : [0.5], [0.5], [0.5], [0.5]; [10,11],
[10,11], [10,11], [10,11]; [20,21], [20,21], [20,21], [20,21]; [50], [50], [50], [50]. The temporal evo-
lution of the sources obtained after training the Bayesian LGSSM is given in Fig 3(b,c,d,e) (grouped
by frequency range). The Bayes LGSSM removed 4 unnecessary sources from the mixing matrix
W , that is one [10,11] Hz and three [20,21] Hz sources. The ﬁrst 4 sources contain dominant low
frequency drift, sources 5, 6 and 8 contain [10,11] Hz, while source 10 contains [20,21] Hz centered
activity. Of the 4 sources initialized to 50 Hz, only 2 retained 50 Hz activity, while the Ac of the

other two have changed to model other frequencies present in the EEG. This method demonstrates
the usefulness and applicability of the VB method in a real-world situation.

5 Conclusion

We considered the application of Variational Bayesian learning to Linear Gaussian State-Space Mod-
els. This is an important class of models with widespread application, and ﬁnding a simple way to
implement this approximate Bayesian procedure is of considerable interest. The most demand-
ing part of the procedure is inference of the hidden states of the model. Previously, this has been
achieved using Belief Propagation, which differs from inference in the Kalman Filtering/Smoothing
literature, for which highly efﬁcient and stabilized proce dures exist. A central contribution of this
paper is to show how inference can be written using the standard Kalman Filtering/Smoothing recur-
sions by augmenting the original model. Additionally, a minor modi ﬁcation to the standard Kalman
Filtering routine may be applied for computational efﬁcien cy. We demonstrated the elegance and
unity of our approach by showing how to easily apply a Variational Bayes analysis of temporal ICA.
Speci ﬁcally, our Bayes ICA approach successfully extracts
independent processes underlying EEG
signals, biased towards preferred frequency ranges. We hope that this simple and unifying inter-
pretation of Variational Bayesian LGSSMs may therefore facilitate the further application to related
models.

A Parameter Updates for A and B

A.1 Determining q(B |ΣV )

By examining F , the contribution of q(B |ΣV ) can be interpreted as the negative KL divergence
between q(B |ΣV ) and a Gaussian. Hence, optimally, q(B |ΣV ) is a Gaussian. The covariance
[ΣB ]ij,kl ≡ (cid:173)¡Bij − hBij i ¢¡Bkl − hBkl i ¢® (averages wrt q(B |ΣV )) is given by:
T
tEq(ht )
Xt=1 Dhj
[ΣB ]ij,kl = [H −1
t hl
B ]j l [ΣV ]ik , where [HB ]j l ≡
+ βj δj l .
t Eq(ht )
t=1 Dhj
B , where [NB ]ij ≡ PT
t + βj ˆBij .
The mean is given by hB i = NB H −1
v i

Determining q(A|ΣH )

Optimally, q(A|ΣH ) is a Gaussian with covariance

[ΣA ]ij,kl = [H −1
A ]j l [ΣH ]ik , where [HA ]j l ≡

T −1
Xt=1 Dhj
tEq(ht )
t hl
t=2 Dhj
tEq(ht−1:t )
A , where [NA ]ij ≡ PT
The mean is given by hAi = NAH −1
t−1hi

+ αj δj l .

+ αj ˆAij .

B Covariance Updates

By specifying a Wishart prior for the inverse of the covariances, conjugate update formulae are
possible. In practice, it is more common to specify diagonal inverse covariances, for which the
corresponding priors are simply Gamma distributions [7, 5]. For this simple diagonal case, the
explicit updates are given below.

Determining q(ΣV )

For the constraint Σ−1
V = diag(ρ), where each diagonal element follows a Gamma prior
Ga(b1 , b2 ) [7], q(ρ) factorizes and the optimal updates are

q(ρi ) = Ga 
b1 +
B N T
where GB ≡ NB H −1
B .

T
2

, b2 +

1
2




T
Xt=1

t )2 − [GB ]ii + Xj
(v i

ij 
βj ˆB 2



 ,

Determining q(ΣH )

Analogously, for Σ−1
H = diag(τ ) with prior Ga(a1 , a2 ) [5], the updates are
q(τi ) = Ga 

T
t )2 ® − [GA ]ii + Xj
Xt=2 (cid:173)(hi
a1 +

A N T
where GA ≡ NAH −1
A .

T − 1
2

, a2 +

1
2

ij 
αj ˆA2



 ,

Acknowledgments

This work is supported by the European DIRAC Project FP6-0027787. This paper only reﬂects the
authors’ views and funding agencies are not liable for any use that may be made of the information
contained herein.

References

[1] Y. Bar-Shalom and X.-R. Li. Estimation and Tracking: Principles, Techniques and Software. Artech
House, 1998.
[2] M. S. Grewal and A. P. Andrews. Kalman Filtering: Theory and Practice Using MATLAB. John Wiley
and Sons, Inc., 2001.
[3] R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Applications. Springer, 2000.
[4] M. J. Beal, F. Falciani, Z. Ghahramani, C. Rangel, and D. L. Wild. A Bayesian approach to reconstructing
genetic regulatory networks with hidden factors. Bioinformatics, 21:349–356, 2005.
[5] A. T. Cemgil and S. J. Godsill. Probabilistic phase vocoder and its application to interpolation of missing
values in audio signals. In 13th European Signal Processing Conference, 2005.
[6] H. Valpola and J. Karhunen. An unsupervised ensemble learning method for nonlinear dynamic state-
space models. Neural Computation, 14:2647–2692, 2002.
[7] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Gatsby Computa-
tional Neuroscience Unit, University College London, 2003.
[8] M. Davy and S. J. Godsill. Bayesian harmonic models for musical signal analysis (with discussion).
In J.O. Bernardo, J.O. Berger, A.P Dawid, and A.F.M. Smith, editors, Bayesian Statistics VII. Oxford
University Press, 2003.
[9] D. J. C. MacKay.
Ensemble learning and evidence maximisation.
www.variational-bayes.org, 1995.
[10] M. Morf and T. Kailath. Square-root algorithms for least-squares estimation.
Automatic Control, 20:487–497, 1975.
[11] P. Park and T. Kailath. New square-root smoothing algorithms. IEEE Transactions on Automatic Control,
41:727–732, 1996.
[12] E. Niedermeyer and F. Lopes Da Silva. Electroencephalography: basic principles, clinical applications
and related ﬁelds . Lippincott Williams and Wilkins, 1999.
[13] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Computation,
11:305–345, 1999.
[14] M. Verhaegen and P. Van Dooren. Numerical aspects of different Kalman ﬁlter implementations.
Transactions of Automatic Control, 31:907–917, 1986.
[15] S. Chiappa and D. Barber. Bayesian linear Gaussian state-space models for biosignal decomposition.
Signal Processing Letters, 14, 2007.
[16] S. S. Saquib, C. A. Bouman, and K. Sauer. ML parameter estimation for Markov random ﬁelds with
applicationsto Bayesian tomography. IEEE Transactions on Image Processing, 7:1029–1044, 1998.
[17] G. Pfurtscheller and F. H. Lopes da Silva. Event-related EEG/MEG synchronization and desynchroniza-
tion: basic principles. Clinical Neurophysiology, pages 1842–1857, 1999.

Unpublished manuscipt:

IEEE Transactions on

IEEE


Tighter PAC-Bayes Bounds

Amiran Ambroladze
Dep. of Mathematics
Lund University/LTH
Box 118, S-221 00 Lund, SWEDEN
amiran.ambroladze@math.lth.se

Emilio Parrado-Hern ´andez
Dep. of Signal Processing and Communications
University Carlos III of Madrid
Legan ´es, 28911, SPAIN
emipar@tsc.uc3m.es

John Shawe-Taylor
Dep. of Computer Science
University College London
Gower Street,
London WC1E 6BT, UK
jst@cs.ucl.ac.uk

Abstract

This paper proposes a PAC-Bayes bound to measure the performance of Support
Vector Machine (SVM) classiﬁers. The bound is based on learning a prior over
the distribution of classiﬁers with a part of the training samples. Experimental
work shows that this bound is tighter than the original PAC-Bayes, resulting in an
enhancement of the predictive capabilities of the PAC-Bayes bound. In addition,
it is shown that the use of this bound as a means to estimate the hyperparameters
of the classiﬁer compares favourably with cross validation in terms of accuracy of
the model, while saving a lot of computational burden.

1

Introduction

Support vector machines (SVM) implement linear classiﬁers in a high-dimensional feature space
using the kernel trick to enable a dual representation and efﬁcient computation.
The danger of overﬁtting in such high-dimensional spaces is countered by maximising the margin
of the classiﬁer on the training examples. For this reason there has been considerable interest in
bounds on the generalisation in terms of the margin.
Early bounds have relied on covering number computations [7], while later bounds have considered
Rademacher complexity. The tightest bounds for practical applications appear to be the PAC-Bayes
bound [4, 5]. In particular the form given in [3] is specially attractive for margin classiﬁers, like
SVM. The PAC-Bayesian bounds are also present in other Machine Learning models such as Gaus-
sian Processes [6].
The aim of this paper is to consider a reﬁnement of the PAC-Bayes approach and investigate whether
it can improve on the original PAC-Bayes bound and uphold its capabilities of delivering reliable
model selection.
The standard PAC-Bayes bound uses a Gaussian prior centred at the origin in weight space. The
key to the new bound is to use part of the training set to compute a more informative prior and then
compute the bound on the remainder of the examples relative to this prior. The bounds are tested
experimentally in several classiﬁcation tasks, including the model selection, on common benchmark
datasets.

The rest of the document is organised as follows. Section 2 brieﬂy reviews the PAC-Bayes bound for
SVMs obtained in [3]. The new bound obtained by means of the reﬁnement of the prior is presented
in Section 3. The experimental work, included in Section 4, compares the tightness of the new bound
with the original one and indicates about its usability in a model selection task. Finally, the main
conclusions of this work are outlined in Section 5.

2 PAC-Bayes Bound

This section is devoted to a brief review of the PAC-Bayes Bound Theorem of [3]. Let us consider a
distribution D of patterns x lying in a certain input space X , with their corresponding output labels
y , y ∈ {−1, 1}. In addition, let us also consider a distribution Q over the classiﬁers c. For every
classiﬁer c, the following two error measures are deﬁned:

Deﬁnition (True error) The true error cD of a classiﬁer c is deﬁned as the probability of misclassi-
fying a pair pattern-label (x, y ) selected at random from D
cD ≡ Pr(x,y)∼D (c(x) 6= y)
Deﬁnition (Empirical error) The empirical error ˆcS of a classiﬁer c on a sample S of size m is
mX
deﬁned as the rate of errors on a set S
1
ˆcS ≡ Pr(x,y)∼S (c(x) 6= y) =
m
i=1
where I (·) is a function equal to 1 if the argument is true and equal to 0 if the argument is false.
the true error, QD ≡
Now we can deﬁne two error measures on the distribution of classiﬁers:
Ec∼Q cD , as the probability of misclassifying an instance x chosen from D with a classiﬁer c chosen
according to Q; and the empirical error ˆQS ≡ Ec∼Q ˆcS , as the probability of classiﬁer c chosen
according to Q misclassifying an instance x chosen from a sample S .
For these two quantities we can derive the PAC-Bayes Bound on the true error of the distribution of
classiﬁers:

I (c(xi ) 6= yi )

Theorem 2.1 (PAC-Bayes Bound) For all prior distributions P (c) over the classiﬁers c, and for
(cid:19)
(cid:18)
any δ ∈ (0, 1]

∀Q(c) : KL( ˆQS ||QD ) ≤ KL(Q(c)||P (c)) + ln( m+1
≥ 1 − δ,
PrS∼Dm
δ
m
where KL is the Kullback-Leibler divergence, KL(p||q) = q ln q
p + (1 − q) ln 1−q
1−p and
KL(Q(c)||P (c)) = Ec∼Q ln Q(c)
P (c) .

)

The proof of the theorem can be found in [3].
This bound can be particularised for the case of linear classiﬁers in the following way. The m
training patterns deﬁne a linear classiﬁer that can be represented by the following equation1 :

c(x) = sign(wT φ(x))
(1)
where φ(x) is a nonlinear projection to a certain feature space where a linear classiﬁcation actually
takes place, and w is a vector from that feature space that determines the separating plane.
For any vector w we can deﬁne a stochastic classiﬁer in the following way: we choose the dis-
tribution Q = Q(w, µ) to be a spherical Gaussian with identity covariance matrix centred on the
direction given by w at a distance µ from the origin. Moreover, we can choose the prior P (c) to be
a spherical Gaussian with identity covariance matrix centred on the origin. Then, for classiﬁers of
the form in equation (1) performance can be bounded by

1We are considering here unbiased classiﬁers, i.e., with b = 0.

Corollary 2.2 (PAC-Bayes Bound for margin classiﬁers [3]) For all distributions D , for all classi-
 
!
ﬁers given by w and µ > 0, for all δ ∈ (0, 1], we have
KL( ˆQS (w, µ)||QD (w, µ)) ≤ µ2
2 + ln( m+1
δ
m

≥ 1 − δ.

Pr

)

It can be shown (see [3]) that

ˆQS (w, µ) = Em [ ˜F (µγ (x, y))]
(2)
where Em is the average over the m training examples, γ (x, y) is the normalised margin of the
training patterns
γ (x, y) = ywT φ(x)
kφ(x)kkwk
Z x
and ˜F = 1 − F , where F is the cumulative normal distribution
1√
e−x2 /2 dx.
F (x) =
2π
−∞
Note that the SVM is a thresholded linear classiﬁer expressed as (1) computed by means of the
Pr(x,y)∼D (cid:0)sign(wT φ(x)) 6= y(cid:1) ≤ 2QD (w, µ)
kernel trick [2]. The generalisation error of such a classiﬁer can be bounded by at most twice the
true (stochastic) error QD (w, µ) in Corollary 2.2, (see [4]);

(3)

(4)

for all µ.

3 Choosing a prior for the PAC-Bayes Bound

Our ﬁrst contribution is motivated by the fact that the PAC-Bayes bound allows us to choose the
prior distribution, P (c). In the standard application of the bound this is chosen to be a Gaussian
centred at the origin. We now consider learning a different prior based on training an SVM on a
subset R of the training set comprising r training patterns and labels. In the experiments this is
taken as a random subset but for simplicity of the presentation we will assume these to be the last r
examples {xk , yk }m
k=m−r+1 in the description below.
With these r examples we can determine an SVM classiﬁer, wr and form a prior P (w|wr ) consist-
ing of a Gaussian distribution with identity covariance matrix centred on wr .
The introduction of this prior P (w|wr ) in Theorem 2.1 results in the following new bound.
Corollary 3.1 (Single Prior based PAC-Bayes Bound for margin classiﬁers) Let us consider a prior
on the distribution of classiﬁers consisting in a spherical Gaussian with identity covariance centred
 
!
along the direction given by wr at a distance η from the origin. Then, for all distributions D , for all
classiﬁers wm and µ > 0, for all δ ∈ (0, 1], we have
||ηwr −µwm ||2
+ ln( m−r+1
KL( ˆQS\R (wm , µ)||QD (wm , µ)) ≤
PrS∼D
2
δ
m − r
where ˆQS\R is a stochastic measure of the error of the classiﬁer on the m − r samples not used to
learn the prior. This stochastic error is computed as indicated in equation (2) averaged over S \R.
Proof Since we separate r instances to learn the prior, the actual size of the training set to which we
apply the bound is m − r . In addition, the stochastic error must be computed only on the instances
not used to learn the prior, i.e. the subset S \R.
The KL divergence between prior and posterior is computed as follows:
KL(Q(w)||P (w)) = Ew∼Q ln Q(w)
2 (w − ηwr )T (w − ηwr )(cid:3)
(cid:2)− 1
P (w)
= Ew∼Q ln exp(− 1
2 (w−µwm )T (w−µwm ))
(cid:0)ηwT wr
(cid:1) + 1
(cid:0)µwT
mw(cid:1) − 1
exp(− 1
2 (w−ηwr )T (w−ηwr ))
2 (w − µwm )T (w − µwm ) + 1
= Ew∼Q
mwm − Ew∼Q
= Ew∼Q
2 µ2wT
2 η2wT
r wr

≥ 1 − δ

)

Taking expectations using Ew∼Qw = µwm we arrive at
1
||µwm − ηwr ||2
2

Intuitively, if the selection of the prior is appropriate, the bound can be tighter than the one given in
Corollary 2.2 when applied to the SVM weight vector on the whole training set. It is perhaps worth
stressing that the bound holds for all wm and so can be applied to the SVM trained on the whole set.
This might at ﬁrst appear as ’cheating’, but the critical point is that the bound is evaluated on the set
S \R not involved in generating the prior. The experimental work illustrates how in fact this bound
can be tighter than the standard PAC-Bayes bound.
Moreover, the selection of the prior may be further reﬁned in exchange for a very small increase in
the penalty term. This can be achieved with the application of the following result.
j=1 so that PJ
Theorem 3.2 (Bound for several priors) Let {Pj (c)}J
j=1 be a set of possible priors that can be se-
j=1 πj = 1. Then, for all priors P (c) ∈ {Pj (c)}J
lected with positive weights {πj }J
j=1 ,
 
!
for all posterior distributions Q(c), for all δ ∈ (0, 1],
∀Q(c), ∀j : KL( ˆQS ||QD ) ≤ KL(Q(c)||Pj (c)) + ln m+1
δ + ln 1
πj
m

≥ 1 − δ,

PrS∼Dm

Proof The bound in Theorem 2.1 can be particularised for a certain Pj (c) with associated weight
!
 
πj and with conﬁdence δπj
∀Q(c) : KL( ˆQS ||QD ) >

KL(Q(c)||Pj (c)) + ln( m+1
δπj
m

PrS∼Dm

< δπj ,

)

Now let us combine the bounds for all the priors {Pj (c)}J
j=1 with the union operation (we use the
!
  ∀Q(c), ∃P (c) ∈ {Pj (c)}J
fact that P (a ∪ b) ≤ P (a) + P (b)).
j=1 :
KL(Q(c)||Pj (c))+ln m+1
δ +ln 1
KL( ˆQS ||QD ) >
πj
m
Finally, let us take the negation of (5) to arrive at the ﬁnal result.

PrS∼Dm

< δ,

(5)

This result can be also particularised for the case of SVM classiﬁers. The set of priors is constructed
by allocating Gaussian distributions with identity covariance matrix along the direction given by wr
at distances {ηj }J
j=1 from the origin where {ηj }J
j=1 are real numbers. In such a case, we obtain
Corollary 3.3 (Multiple Prior PAC-Bayes Bound for linear classiﬁers) Let us consider a set
{Pj (w|wr , ηj )}J
j=1 of prior distributions of classiﬁers consisting in spherical Gaussian distribu-
tions with identity covariance matrix centred on ηj wr , where {ηj }J
j=1 are real numbers. Then, for
 
!
all distributions D , for all classiﬁers w, for all µ > 0, for all δ ∈ (0, 1], we have
||ηj wr −µw||2
+ ln( m−r+1
) + ln J
KL( ˆQS\R (w, µ)||QD (w, µ)) ≤
2
δ
m − r

≥ 1 − δ

PrS∼D

Proof The proof is straightforward, substituting πj = 1
J for all j in Theorem 3.2 and computing
the KL divergence between prior and posterior as in the proof of Corollary 3.1.
Note that the {ηj }J
j=1 must be chosen before we actually compute the posterior. However, the bound
holds for all µ. Therefore, a linear search can be implemented for the value of µ that leads to the
tightest bound. In the case of several priors, the search is repeated for every prior and the reported
value of the bound is the tightest. In Section 4 we present experimental results comparing this new
bound to the standard PAC-Bayes bound and using it to guide model selection.

4 Experiments

The tightness of the new bound is evaluated in a model selection and classiﬁcation task using some
UCI [1] datasets (see their description in terms of number of instances, input dimension and number
of positive/negative examples in Table 1).

Problem # samples
569
Wdbc
2310
Image
Waveform
5000
7400
Ringnorm

input dim.
30
18
21
20

Pos/Neg
357 / 212
1320 / 990
1647 / 3353
3664 / 3736

Table 1: Description of the datasets: for every set we give the number of patterns, number of input
variables and number of positive/negative examples.

For every dataset, we obtain 50 different training/test set partitions with 80% of the samples forming
the training set and the remaining 20% forming the test set.
With each of the partitions we learn a SVM classiﬁer with Gaussian RBF kernel preceded by a model
selection. The model selection consists in the determination of an optimal pair of hyperparameters
(C, σ). C is the SVM trade-off between the maximisation of the margin and the minimisation of
the hinge loss of the training samples, while σ is the width of the Gaussian kernel. The best pair is
sought in a 15 × 15 grid of parameters where C ∈ {0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100,
√
√
√
√
√
√
√
√
√
√
√
√
200, 500, 1000} and σ ∈ { 1
d, 2
d, 3
d, 4
d, 5
√
√
√
d, 1
d, 1
d, 1
d, 1
d, 1
d, 1
d,
d,
2
3
4
5
6
7
8
d}, where d is the input space dimension.
6
d, 8
d, 7
For completeness, this model selection is guided by the PAC-Bayes bound: we select the model
corresponding to the pair that yields a lower value of QD in the bound. Table 2 shows the value of
the PAC-Bayes Bound averaged over the 50 training/test partitions. For every partition we use the
minimum value of the bound resulting from all the pairs (C, σ) of the grid. Note that this procedure
is computationally less costly than the commonly used N -fold cross validation model selection,
since it saves the training of N classiﬁers (one for each fold) for each parameter combination.

Problem PAC-Bayes Bound Test error rate
0.334 ± 0.005
0.073 ± 0.021
Wdbc
0.254 ± 0.003
0.074 ± 0.014
Image
0.198 ± 0.002
0.089 ± 0.008
Waveform
0.026 ± 0.005
0.212 ± 0.002
Ringnorm

Table 2: Averaged PAC-Bayes Bound and Test Error Rate obtained by the model that yielded the
lowest bound in each of the 50 training/test partitions.

We repeated this experiment using the Prior PAC-Bayes Bound with different conﬁgurations for
learning the prior distribution of classiﬁers. These conﬁgurations are deﬁned by variations on the
percentage of training patterns separated to compute the prior and on the number of scalings of
the magnitude of that prior. The scalings represent different lengths η of ||wr || equally spaced
between η = 1 and η = 100. To summarize, for every training/test partition and for every pair (%
patterns, # of scalings) we look at the pair (C, σ) that outputs the smaller value of QD .
In this case, the use of the Prior PAC-Bayes Bound to perform the model selection increases the
computational burden of using the PAC-Bayes one in the training of one classiﬁer (the one used to
learn the prior), in comparison to the extra N classiﬁers needed by N -fold cross validation.
Table 3 displays both the average value and the sample standard deviation over the 50 realisations.
It seems that ten scalings of the prior are enough to obtain tighter bounds, since the use of 100 or
500 scalings does not improve the best results. With respect to the percentage of training instances
left out to learn the prior, something close to 50% of the training set works well in the considered
problems. It is worth mentioning that we treat each position in the Table as a separate experiment.

50%
0.398 ± 0.013
0.306 ± 0.018
0.315 ± 0.017
0.322 ± 0.017

50%
0.300 ± 0.008
0.184 ± 0.010
0.186 ± 0.009
0.188 ± 0.009

Winsconsin Database of Breast Cancer (PAC-Bayes Bound = 0.334±0.005)
Percentage of training set used to compute the prior
20%
30%
40%
10 %
0.351 ± 0.007
0.364 ± 0.009
0.379 ± 0.011
0.341 ± 0.006
0.323 ± 0.012
0.314 ± 0.012
0.310 ± 0.013
0.337 ± 0.010
0.319 ± 0.007
0.315 ± 0.013
0.313 ± 0.011
0.315 ± 0.010
0.320 ± 0.009
0.319 ± 0.011
0.321 ± 0.013
0.324 ± 0.007
Image Segmentation (PAC-Bayes Bound = 0.254±0.003)
Percentage of training set used to compute the prior
40%
30%
20%
10 %
0.284 ± 0.005
0.274 ± 0.003
0.262 ± 0.005
0.255 ± 0.003
0.203 ± 0.006
0.200 ± 0.005
0.188 ± 0.007
0.215 ± 0.004
0.203 ± 0.007
0.196 ± 0.005
0.187 ± 0.007
0.217 ± 0.004
0.204 ± 0.007
0.198 ± 0.005
0.189 ± 0.007
0.218 ± 0.004
Waveform (PAC-Bayes Bound = 0.198±0.002)
Percentage of training set used to compute the prior
20%
30%
40%
10 %
0.201 ± 0.003
0.207 ± 0.003
0.214 ± 0.004
0.197 ± 0.003
0.150 ± 0.005
0.153 ± 0.004
0.156 ± 0.004
0.161 ± 0.004
0.161 ± 0.004
0.155 ± 0.004
0.153 ± 0.004
0.152 ± 0.005
0.157 ± 0.004
0.155 ± 0.004
0.154 ± 0.005
0.162 ± 0.004
Ringnorm (PAC-Bayes Bound = 0.212±0.002)
Percentage of training set used to compute the prior
20%
30%
40%
0.225 ± 0.002
0.236 ± 0.002
0.249 ± 0.004
0.140 ± 0.047
0.126 ± 0.037
0.116 ± 0.030
0.117 ± 0.030
0.126 ± 0.037
0.139 ± 0.047
0.140 ± 0.047
0.127 ± 0.037
0.117 ± 0.030

10 %
0.216 ± 0.001
0.172 ± 0.068
0.173 ± 0.068
0.173 ± 0.068

Scalings

1
10
100
500

Scalings

1
10
100
500

Scalings

1
10
100
500

Scalings

1
10
100
500

50%
0.222 ± 0.005
0.151 ± 0.005
0.153 ± 0.005
0.155 ± 0.005

50%
0.265 ± 0.002
0.109 ± 0.024
0.110 ± 0.024
0.110 ± 0.024

Table 3: Averaged Prior PAC-Bayes bound for different settings of percentage of training instances
reserved to compute the prior and of number of scalings of the normalised prior.

However, one could have included the tuning of the pair (% patterns, # of scalings) in
the model selection. This would have involved a further application of the union bound with the 20
entries of the Table for each problem, at the cost of adding an extra ln(20)/m (0.0053 for Wdbc and
less for the other datasets) in the right part of Theorem 3.2. We decided to ﬁx the number of scalings
and the amount of training patterns to compute the prior since to perform all of the different options
would augment the computational burden of the model selection.
In order to evaluate the predictive capabilities of the Prior PAC-Bayes bound as a means to select
models with low test error rate, Table 4 displays the averaged test error corresponding to the mod-
els selected in the previous experiment (note that in this case the computational burden involved in
determining the model is increased by the training of the SVM that learns the prior wr ). Table 5
displays the test error rate obtained by SVMs with their hyperparameters tuned on the above men-
tioned grid by means of ten-fold cross-validation, that serves as a baseline method for comparison
purposes.
According to the values shown in the tables, the Prior PAC-Bayes bound achieves tighter predictions
of the generalization error of the randomized classiﬁer in almost all cases.
Notice how the length of the prior is not so critical in comparison with its direction. The goodness
of the latter relying on the subset of samples left out for the purpose of learning the prior classiﬁer.
Moreover it has to be remarked that this tightening of the bound does not appear to deliver any
reduction in the capabilities to select a good model (such a case would imply that we can predict
more accurately a bigger error rate, but our bound is able to predict accurately the same error rate as
the PAC-Bayes Bound).

1
10
100
500

1
10
100
500

1
10
100
500

Scalings

Winsconsin Database of Breast Cancer (PAC-Bayes Test Error = 0.073±0.021)
Percentage of training set used to compute the prior
Scalings
20%
30%
40%
10 %
0.076 ± 0.021
0.076 ± 0.021
0.076 ± 0.021
0.076 ± 0.020
0.076 ± 0.021
0.075 ± 0.021
0.074 ± 0.021
0.075 ± 0.021
0.076 ± 0.021
0.074 ± 0.020
0.074 ± 0.021
0.076 ± 0.021
0.076 ± 0.021
0.074 ± 0.020
0.073 ± 0.020
0.076 ± 0.020
Image Segmentation (PAC-Bayes Test Error = 0.074±0.014)
Percentage of training set used to compute the prior
40%
30%
20%
10 %
0.083 ± 0.019
0.078 ± 0.011
0.078 ± 0.011
0.078 ± 0.011
0.066 ± 0.011
0.063 ± 0.014
0.054 ± 0.010
0.064 ± 0.011
0.063 ± 0.011
0.061 ± 0.011
0.059 ± 0.011
0.064 ± 0.011
0.063 ± 0.011
0.061 ± 0.011
0.059 ± 0.011
0.064 ± 0.011
Waveform (PAC-Bayes Test Error = 0.089±0.008)
Percentage of training set used to compute the prior
20%
30%
40%
10 %
0.089 ± 0.008
0.090 ± 0.009
0.091 ± 0.009
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
0.089 ± 0.008
Ringnorm (PAC-Bayes Test Error = 0.026±0.005)
Percentage of training set used to compute the prior
20%
30%
40%
0.030 ± 0.007
0.038 ± 0.005
0.036 ± 0.007
0.021 ± 0.007
0.021 ± 0.007
0.025 ± 0.008
0.025 ± 0.008
0.021 ± 0.007
0.021 ± 0.007
0.021 ± 0.007
0.021 ± 0.007
0.025 ± 0.008

10 %
0.025 ± 0.004
0.020 ± 0.007
0.020 ± 0.007
0.020 ± 0.007

1
10
100
500

Scalings

Scalings

50%
0.076 ± 0.021
0.072 ± 0.021
0.072 ± 0.021
0.072 ± 0.021

50%
0.100 ± 0.019
0.056 ± 0.011
0.057 ± 0.012
0.057 ± 0.012

50%
0.091 ± 0.009
0.089 ± 0.009
0.089 ± 0.009
0.089 ± 0.009

50%
0.038 ± 0.005
0.026 ± 0.008
0.026 ± 0.008
0.025 ± 0.005

Table 4: Averaged Test Error Rate corresponding to the model determined by the bound for the
different settings of Table 3.

Problem Cross-validation error rate Test error rate
0.060 ± 0.006
0.072 ± 0.024
Wdbc
0.022 ± 0.002
0.024 ± 0.008
Image
0.079 ± 0.011
0.085 ± 0.009
Waveform
0.017 ± 0.004
0.015 ± 0.001
Ringnorm

Table 5: Averaged test error rate. For every partition we select the test error rate corresponding to
the model reporting the smaller cross-validation error.

However, the comparison with Table 5 points out that the PAC-Bayes bound is not as accurate
as Ten Fold cross-validation when it comes to selecting a model that yields a low test error rate.
Nevertheless, in two out of the four problems (waveform, and wdbc) the bound provided a model
as good as the one found by cross-validation, added to the fact that in ringnorm the error bars
overlap. We conclude the discussion by pointing that the Cross-validation error rate cannot be used
directly as a prediction on the expected test error rate in the sense of worse case performances. Of
course the values of the cross-validation error rate and the test error rate are close, but it is difﬁcult
to predict how close they are going to be.

5 Conclusions and ongoing research

In this paper we have presented a version of the PAC-Bayes bound for linear classiﬁers that intro-
duces the learning of the prior distribution over the classiﬁers. This prior distribution is a Gaussian
with identity covariance matrix. The mean weight vector is learnt in the following way: its direction
is determined from a separate subset of the training examples, while its length has to be chosen from
an a priori ﬁxed set of lengths.
The experimental work shows that this new version of the bound achieves tighter predictions of the
generalization error of the stochastic classiﬁer, compared to the original PAC-Bayes bound predic-
tions. Moreover, if the model selection is driven by the bound, the Prior PAC-Bayes does not degrade
the quality of the model selected by the original bound. Nevertheless, it has to be said that in some
of our experiments the model selected by the bounds resulted as accurate as the ones selected by
ten-fold cross-validation in terms of test error rate on a separate test. This fact is remarkable since
to include the model selection in the training of the classiﬁer roughly multiplies by ten the computa-
tional burden of the training when using ten-fold cross-validation but roughly by two when using the
prior PAC-Bayes bound. Of course the original PAC-Bayes provides with a cheaper model selection,
but its predictions about the generalization capabilities are more pessimistic.
The amount of training patterns used to learn the prior seems to be a key aspect in the goodness of
this prior and thus in the tightness of the bound. Therefore, ongoing research includes methods to
systematically determine an amount of patterns that provides with suitable priors. Another line of
research explores the use of these bounds to reinforce different properties of the design of classiﬁers,
such as sparsity. Finally, a deeper study about which dataset structure causes differences among the
performances of cross-validation and bound-driven model selections is also being carried out.

Acknowledgments

This work has been supported by the IST Programme of the European Community under the PAS-
CAL Network of Excellence IST2002-506788. E. P-H. acknowledges support from Spain CICYT
grant TEC2005-04264/TCM.

References
UCI Repository of machine learning databases.
[1] C L Blake and C J Merz.
Irvine, Dept. of
University of California,
Information and Computer Sciences,
[http://www.ics.uci.edu/∼mlearn/MLRepository.html], 1998.
[2] Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. A training algorithm for optimal
margin classiﬁers. In Computational Learing Theory, pages 144–152, 1992.
[3] J Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learn-
ing Research, 6(Mar):273–306, 2005.
[4] J Langford and J Shawe-Taylor. PAC-Bayes & Margins. In Advances in Neural Information
Processing Systems, volume 14, Cambridge MA, 2002. MIT Press.
[5] D McAllester. Pac-bayesian stochastic model selection. Machine Learning, 51(1):5–21, 2003.
[6] M Seeger. PAC-Bayesian Generalization Error Bounds for Gaussian Process Classiﬁcation.
Journal of Machine Learning Research, 3:233–269, 2002.
[7] J Shawe-Taylor, P L Bartlett, R C Williamson, and M Anthony. Structural risk minimization
over data-dependent hierarchies. IEEE Trans. Information Theory, 44(5):1926 – 1940, 1998.


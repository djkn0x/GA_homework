A PAC-Bayes Risk Bound for General Loss Functions

Pascal Germain
D ´epartement IFT-GLO
Universit ´e Laval
Qu ´ebec, Canada
Pascal.Germain.1@ulaval.ca

Alexandre Lacasse
D ´epartement IFT-GLO
Universit ´e Laval
Qu ´ebec, Canada
Alexandre.Lacasse@ift.ulaval.ca

Franc¸ ois Laviolette
D ´epartement IFT-GLO
Universit ´e Laval
Qu ´ebec, Canada
Francois.Laviolette@ift.ulaval.ca

Mario Marchand
D ´epartement IFT-GLO
Universit ´e Laval
Qu ´ebec, Canada
Mario.Marchand@ift.ulaval.ca

Abstract

We provide a PAC-Bayesian bound for the expected loss of convex combinations
of classiﬁers under a wide class of loss functions (which includes the exponential
loss and the logistic loss). Our numerical experiments with Adaboost indicate that
the proposed upper bound, computed on the training set, behaves very similarly
as the true loss estimated on the testing set.

1 Intoduction

The PAC-Bayes approach [1, 2, 3, 4, 5] has been very effective at providing tight risk bounds for
large-margin classiﬁers such as the SVM [4, 6]. Within this approach, we consider a prior distribu-
tion P over a space of classiﬁers that characterizes our prior belief about good classiﬁers (before the
observation of the data) and a posterior distribution Q (over the same space of classiﬁers) that takes
into account the additional information provided by the training data. A remarkable result that came
out from this line of research, known as the “PAC-Bayes theorem”, provides a tight upper bound
on the risk of a stochastic classiﬁer (deﬁned on the posterior Q) called the Gibbs classiﬁer. In the
context of binary classiﬁcation, the Q-weighted majority vote classiﬁer (related to this stochastic
classiﬁer) labels any input instance with the label output by the stochastic classiﬁer with probability
more than half. Since at least half of the Q measure of the classiﬁers err on an example incorrectly
classiﬁed by the majority vote, it follows that the error rate of the majority vote is at most twice the
error rate of the Gibbs classiﬁer. Therefore, given enough training data, the PAC-Bayes theorem
will give a small risk bound on the majority vote classiﬁer only when the risk of the Gibbs classiﬁer
is small. While the Gibbs classiﬁers related to the large-margin SVM classiﬁers have indeed a low
risk [6, 4], this is clearly not the case for the majority vote classiﬁers produced by bagging [7] and
boosting [8] where the risk of the associated Gibbs classiﬁer is normally close to 1/2. Consequently,
the PAC-Bayes theorem is currently not able to recognize the predictive power of the majority vote
in these circumstances.
In an attempt to progress towards a theory giving small risk bounds for low-risk majority votes
having a large risk for the associated Gibbs classiﬁer, we provide here a risk bound for convex
combinations of classiﬁers under quite arbitrary loss functions, including those normally used for
boosting (like the exponential loss) and those that can give a tighter upper bound to the zero-one
loss of weighted majority vote classiﬁers (like the sigmoid loss). Our numerical experiments with
Adaboost [8] indicate that the proposed upper bound for the exponential loss and the sigmoid loss,
computed on the training set, behaves very similarly as the true loss estimated on the testing set.

2 Basic Deﬁnitions and Motivation
We consider binary classiﬁcation problems where the input space X consists of an arbitrary subset
of Rn and the output space Y = {−1, +1}. An example is an input-output (x, y) pair where
x ∈ X and y ∈ Y . Throughout the paper, we adopt the PAC setting where each example (x, y)
is drawn according to a ﬁxed, but unknown, probability distribution D on X × Y . We consider
learning algorithms that work in a ﬁxed hypothesis space H of binary classiﬁers and produce a
convex combination fQ of binary classiﬁers taken from H. Each binary classiﬁer h ∈ H contribute
(cid:88)
to fQ with a weight Q(h) ≥ 0. For any input example x ∈ X , the real-valued output fQ (x) is given
by
(cid:80)
Q(h)h(x) ,
fQ (x) =
h∈H
where h(x) ∈ {−1, +1}, fQ (x) ∈ [−1, +1], and
h∈H Q(h) = 1. Consequently, Q(h) will be
called the posterior distribution1 .
Since fQ (x) is also the expected class label returned by a binary classiﬁer randomly chosen accord-
ing to Q, the margin yfQ (x) of fQ on example (x, y) is related to the fraction WQ (x, y) of binary
(cid:183)
(cid:184)
(cid:88)
classiﬁers that err on (x, y) under measure Q as follows. Let I (a) = 1 when predicate a is true and
I (a) = 0 otherwise. We then have:
I (h(x) (cid:54)= y) − 1
WQ (x, y) − 1
2
2
h∈H

= E
h∼Q
= − 1
2 yfQ (x) .
WQ (x, y) is the Gibbs error rate (by deﬁnition), we see that the expected margin is
Since E
(x,y)∼D
(cid:181)
(cid:182)
just one minus twice the Gibbs error rate. In contrast, the error for the Q-weighted majority vote is
given by
WQ (x, y) >

− yh(x)
2

= − 1
2

Q(h)yh(x)

= E
h∼Q

I

=
≤
≤

E
(x,y)∼D

1
1
tanh (β [2WQ (x, y) − 1]) +
lim
E
β→∞
(x,y)∼D
2
2
(∀β > 0)
tanh (β [2WQ (x, y) − 1]) + 1
E
(x,y)∼D
(∀β > 0) .
exp (β [2WQ (x, y) − 1])
E
(x,y)∼D
Hence, for large enough β , the sigmoid loss (or tanh loss) of fQ should be very close to the error
rate of the Q-weighted majority vote. Moreover, the error rate of the majority vote is always upper
bounded by twice that sigmoid loss for any β > 0. The sigmoid loss is, in turn, upper bounded by
the exponential loss (which is used, for example, in Adaboost [9]).
More generally, we will provide tight risk bounds for any loss function that can be expanded by a
∞(cid:88)
Taylor series around WQ (x, y) = 1/2. Hence we consider any loss function ζQ (x, y) that can be
written as
1
1
(cid:181)
(cid:182)k
g(k) (2WQ (x, y) − 1)k
∞(cid:88)
2
2
k=1
1
1
− yh(x)
g(k)
+
=
(2)
E
and our task is to provide tight bounds for the expected loss ζQ that depend on the empirical loss (cid:99)ζQ
,
h∼Q
2
2
k=1
m(cid:88)
measured on a training sequence S = (cid:104)(x1 , y1 ), . . . , (xm , ym )(cid:105) of m examples, where
; (cid:99)ζQ
1
m
i=1
Note that by upper bounding ζQ , we are taking into account all moments of WQ . In contrast, the
WQ (x, y).
PAC-Bayes theorem [2, 3, 4, 5] currently only upper bounds the ﬁrst moment E
(x,y)∼D
1When H is a continuous set, Q(h) denotes a density and the summations over h are replaced by integrals.

ζQ (xi , yi ) .

E
(x,y)∼D

ζQ (x, y)

ζQ (x, y)

ζQ

def=

def=

+

1
2

(1)

(3)

def=

3 A PAC-Bayes Risk Bound for Convex Combinations of Classiﬁers

I

(4)

ζQ =

1
2

1
2

1
2

R(h1−k )

def=

+

+

+

g(k) E
(x,y)∼D

The PAC-Bayes theorem [2, 3, 4, 5] is a statement about the expected zero-one loss of a Gibbs
classiﬁer. Given any distribution over a space of classiﬁers, the Gibbs classiﬁer labels any example
x ∈ X according to a classiﬁer randomly drawn from that distribution. Hence, to obtain a PAC-
(cid:182)k
(cid:181)
Bayesian bound for the expected general loss ζQ of a convex combination of classiﬁers, let us relate
ζQ to the zero-one loss of a Gibbs classiﬁer. For this task, let us ﬁrst write
· · · E
− yh(x)
(−y)k h1 (x)h2 (x) · · · hk (x) .
= E
E
E
E
E
hk∼Q
h2∼Q
h1∼Q
h∼Q
(x,y)∼D
(x,y)
Note that the product h1 (x)h2 (x) · · · hk (x) deﬁnes another binary classiﬁer that we denote as
(cid:181)
(cid:182)
h1−k (x). We now deﬁne the error rate R(h1−k ) of h1−k as
(−y)k h1−k (x) = sgn(g(k))
E
(x,y)∼D
1
1
(−y)k h1−k (x) ,
· sgn(g(k)) E
=
+
(x,y)∼D
2
2
where sgn(g) = +1 if g > 0 and −1 otherwise.
· · · E
(cid:181)
(cid:182)k
∞(cid:88)
to denote E
If we now use
, Equation 2 now becomes
E
E
h1∼Q
h2∼Q
hk∼Q
h1−k∼Qk
1
− yh(x)
∞(cid:88)
E
h∼Q
2
k=1
1
(cid:181)
(cid:182)
|g(k)| · sgn(g(k)) E
∞(cid:88)
E
(x,y)∼D
h1−k∼Qk
2
k=1
R(h1−k ) − 1
|g(k)| E
h1−k∼Qk
2
k=1
∞(cid:88)
Apart, from constant factors, Equation 5 relates ζQ the the zero-one loss of a new type of Gibbs
classiﬁer. Indeed, if we deﬁne
(cid:182)
(cid:181)
k=1
Equation 5 can be rewritten as
ζQ − 1
2

c def=
∞(cid:88)
k=1
The new type of Gibbs classiﬁer is denoted above by GQ , where Q is a distribution over the product
classiﬁers h1−k with variable length k . More precisely, given an example x to be labelled by GQ ,
we ﬁrst choose at random a number k ∈ N+ according to the discrete probability distribution given
by |g(k)|/c and then we choose h1−k randomly according to Qk to classify x with h1−k (x). The
risk R(GQ ) of this new Gibbs classiﬁer is then given by Equation 7.
We will present a tight PAC-Bayesien bound for R(GQ ) which will automatically translate into a
the the empirical loss (cid:99)ζQ (measured on the training sequence S of m examples) through the equation
bound for ζQ via Equation 7. This bound will depend on the empirical risk RS (GQ ) which relates to
(cid:182)
(cid:181)(cid:99)ζQ − 1
∞(cid:88)
1
1
1
|g(k)| E
RS (h1−k ) def= RS (GQ ) ,
=
+
(8)
h1−k∼Qk
2
2
c
c
(cid:181)
(cid:182)
m(cid:88)
k=1
(−yi )k h1−k (xi ) = sgn(g(k))
i=1

R(h1−k ) def= R(GQ ) .

|g(k)| E
h1−k∼Qk

(−y)k h1−k (x)

RS (h1−k )

|g(k)| ,

+

=

1
2

1
c

def=

1
m

(5)

(6)

(7)

I

.

.

=

=

1
c

where

.

(cid:104)
(cid:105)
ζQ − (cid:99)ζQ = c ·
Note that Equations 7 and 8 imply that
R(GQ ) − RS (GQ )
Hence, any looseness in the bound for R(GQ ) will be ampliﬁed by the scaling factor c on the bound
for ζQ . Therefore, within this approach, the bound for ζQ can be tight only for small values of c.
Note however that loss functions having a small value of c are commonly used in practice. Indeed,
learning algorithms for feed-forward neural networks, and other approaches that construct a real-
valued function fQ (x) ∈ [−1, +1] from binary classiﬁcation data, typically use a loss function of
(cid:175)(cid:175)(cid:175)(cid:175)r
(cid:175)(cid:175)(cid:175)(cid:175) E
the form |fQ (x) − y |r /2, for r ∈ {1, 2}. In these cases we have
1
1
= 2r−1 |WQ (x, y)|r ,
yh(x) − 1
|fQ (x) − y |r =
h∼Q
2
2
which gives c = 1 for r = 1, and c = 3 for r = 2.
Given a set H of classiﬁers, a prior distribution P on H, and a training sequence S of m examples,
the learner will output a posterior distribution Q on H which, in turn, gives a convex combination fQ
that suffers the expected loss ζQ . Although Equation 7 holds only for a distribution Q deﬁned by the
(cid:91)
absolute values of the Taylor coefﬁcients g(k) and the product distribution Qk , the PAC-Bayesian
theorem will hold for any prior P and posterior Q deﬁned on
H∗ def=
Hk ,
k∈N+
(cid:176)(cid:176)R(GQ )
(cid:161)
(cid:162)
and for any zero-one valued loss function (cid:96)(h(x), y)) deﬁned ∀h ∈ H∗ and ∀(x, y) ∈ X × Y
(not just the one deﬁned by Equation 4). This PAC-Bayesian theorem upper-bounds the value of
kl
RS (GQ )
, where

(9)

ln

1
c

1
c

=

+ (1 − q) ln

1 − q
kl(q(cid:107)p) def= q ln q
(cid:176)(cid:176)R(GQ )
1 − p
(cid:161)
(cid:162)
p
denotes the Kullback-Leibler divergence between the Bernoulli distributions with probability of
(cid:176)(cid:176)R(GQ )
(cid:161)
(cid:162)
success q and probability of success p. Note that an upper bound on kl
RS (GQ )
provides
both and upper and a lower bound on R(GQ ).
depends on the value of KL(Q(cid:107)P ), where
The upper bound on kl
RS (GQ )
ln Q(h)
KL(Q(cid:107)P ) def= E
h∼Q
P (h)
denotes the Kullback-Leibler divergence between distributions Q and P deﬁned on H∗ .
In our case, since we want a bound on R(GQ ) that translates into a bound for ζQ , we need a Q that
satisﬁes Equation 7. To minimize the value of KL(Q(cid:107)P ), it is desirable to choose a prior P having
properties similar to those of Q. Namely, the probabilities assigned by P to the possible values of k
will also be given by |g(k)|/c. Moreover, we will restrict ourselves to the case where the k classiﬁers
from H are chosen independently, each according to the prior P on H (however, other choices for
∞(cid:88)
P are clearly possible). In this case we have
∞(cid:88)
k=1
∞(cid:88)
k=1
1
|g(k)| · k E
=
h∼Q
c
k=1
= k · KL(Q(cid:107)P ) ,

|g(k)| · Qk (h1−k )
k(cid:88)
|g(k)| · P k (h1−k )
ln Q(hi )
. . . E
hk∼Q
P (hi )
i=1
ln Q(h)
P (h)

|g(k)| E
h1−k∼Qk

|g(k)| E
h1∼Q

KL(Q(cid:107)P ) =

(10)

where

∞(cid:88)
k=1

1
c

k def=

|g(k)| · k .

(11)

Pr
S∼Dm

RS (GQ )

≥ 1 − δ .

We then have the following theorem.
(cid:183)
(cid:181)
(cid:184)(cid:182)
Theorem 1 For any set H of binary classiﬁers, any prior distribution P on H∗ , and any δ ∈ (0, 1],
(cid:176)(cid:176)R(GQ )
(cid:162) ≤ 1
(cid:161)
we have
KL(Q(cid:107)P ) + ln m + 1
∀Q on H∗ : kl
m
δ
Proof The proof directly follows from the fact that we can apply the PAC-Bayes theorem of [4]
to priors and posteriors deﬁned on the space H∗ of binary classiﬁers with any zero-one valued loss
function.
and 8 to relate R(GQ ) and RS (GQ ) to ζQ and (cid:99)ζQ and when we use Equation 10 for KL(Q(cid:107)P ).
Note that Theorem 1 directly provides upper and lower bounds on ζQ when we use Equations 7
Theorem 2 Consider any loss function ζQ (x, y) deﬁned by Equation 1. Let ζQ and (cid:99)ζQ be, re-
Consequently, we have the following theorem.
spectively, the expected loss and its empirical estimate (on a sample of m examples) as deﬁned by
Equation 3. Let c and k be deﬁned by Equations 6 and 11 respectively. Then for any set H of binary
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176) 1
(cid:33)
(cid:195)
(cid:195)
(cid:183)(cid:99)ζQ − 1
(cid:184)
(cid:183)
(cid:184)
classiﬁers, any prior distribution P on H, and any δ ∈ (0, 1], we have
ζQ − 1
1
(cid:183)
∀Q on H : kl
2
2
2
c
k · KL(Q(cid:107)P ) + ln m + 1
δ

≥ 1 − δ .

(cid:184)(cid:33)

≤ 1
m

Pr
S∼Dm

1
c

+

1
2

+

4 Bound Behavior During Adaboost

We have decided to examine the behavior of the proposed bounds during Adaboost since
this learning algorithm generally produces a weighted majority vote having a large Gibbs risk
E (x,y)WQ (x, y) (i.e., small expected margin) and a small Var (x,y)WQ (x, y) (i.e., small variance
of the margin). Indeed, recall that one of our main motivations was to ﬁnd a tight risk bound for the
majority vote precisely under these circumstances.
We have used the “symmetric” version of Adaboost [10, 9] where, at each boosting round t, the
m(cid:88)
weak learning algorithm produces a classiﬁer ht with the smallest empirical error
Dt (i)I [ht (xi ) (cid:54)= yi ]
t =
i=1
with respect to the boosting distribution Dt (i) on the indices i ∈ {1, . . . , m} of the training exam-
ples. After each boosting round t, this distribution is updated according to
1
Dt (i) exp(−yiαtht (xi )) ,
(cid:181)
(cid:182)
Zt
where Zt is the normalization constant required for Dt+1 to be a distribution, and where
1 − t
1
2
t

Dt+1 (i) =

αt =

ln

.

Since our task is not to obtain the majority vote with the smallest possible risk but to investigate
the tightness of the proposed bounds, we have used the standard “decision stumps” for the set H

of classiﬁers that can be chosen by the weak learner. Each decision stump is a threshold classiﬁer
that depends on a single attribute: it outputs +y when the tested attribute exceeds the threshold
and predicts −y otherwise, where y ∈ {−1, +1}. For each decision stump h ∈ H, its boolean
(cid:80)n
complement is also in H. Hence, we have 2[k(i) − 1] possible decision stumps on an attribute i
having k(i) possible (discrete values). Hence, for data sets having n attributes, we have exactly
|H| = 2
i=1 2[k(i) − 1] classiﬁers. Data sets having continuous-valued attributes have been
discretized in our numerical experiments.
From Theorem 2 and Equation 10, the bound on ζQ depends on KL(Q(cid:107)P ). We have chosen a
(cid:88)
(cid:88)
uniform prior P (h) = 1/|H| ∀h ∈ H. We therefore have
Q(h) ln Q(h)
Q(h) ln Q(h) + ln |H| def= −H (Q) + ln |H| .
KL(Q(cid:107)P ) =
=
P (h)
h∈H
h∈H
At boosting round t, Adaboost changes the distribution from Dt to Dt+1 by putting more weight on
the examples that are incorrectly classiﬁed by ht . This strategy is supported by the propose bound
on ζQ since it has the effect of increasing the entropy H (Q) as a function of t. Indeed, apart from
tiny ﬂuctuations, the entropy was seen to be nondecreasing as a function of t in all of our boosting
experiments.
We have focused our attention on two different loss functions: the exponential loss and the sigmoid
loss.

exp (β [2WQ (x, y) − 1]) .

(12)

4.1 Results for the Exponential Loss
The exponential loss EQ (x, y) is the obvious choice for boosting since, the typical analysis [8, 10, 9]
shows that the empirical estimate of the exponential loss is decreasing at each boosting round 2 .
More precisely, we have chosen
EQ (x, y) def=
For this loss function, we have

1
2
c = eβ − 1
β
k =
1 − e−β .
Since c increases exponentially rapidly with β , so will the risk upper-bound for EQ . Hence, unfor-
tunately, we can obtain a tight upper-bound only for small values of β .
All the data sets used were obtained from the UCI repository. Each data set was randomly split
into two halves of the same size: one for the training set and the other for the testing set. Figure 1
illustrates the typical behavior for the exponential loss bound on the Mushroom and Sonar data sets
containing 8124 examples and 208 examples respectively.
We ﬁrst note that, although the test error of the majority vote (generally) decreases as function of
the number T of boosting rounds, the risk of the Gibbs classiﬁer, E (x,y)WQ (x, y) increases as a
function of T but its variance Var (x,y)WQ (x, y) decreases dramatically. Another striking feature
is the fact that the exponential loss bound curve, computed on the training set, is essentially parallel
to the true exponential loss curve computed on the testing set. This same parallelism was observed
for all the UCI data sets we have examined so far.3 Unfortunately, as we can see in Figure 2, the
risk bound increases rapidly as a function of β . Interestingly however, the risk bound curves remain
parallel to the true risk curves.

4.2 Results for the Sigmoid Loss
We have also investigated the sigmoid loss TQ (x, y) deﬁned by
1
1
TQ (x, y) def=
tanh (β [2WQ (x, y) − 1]) .
+
2
2
2 In fact, this is true only for the positive linear combination produced by Adaboost. The empirical expo-
nential risk of the convex combination fQ is not always decreasing as we shall see.
3These include the following data sets: Wisconsin-breast, breast cancer, German credit, ionosphere, kr-vs-
kp, USvotes, mushroom, and sonar.

(13)

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

EQ bound
EQ on test
E(WQ ) on test
MV error on test
Var(WQ ) on test

EQ bound
EQ on test
E(WQ ) on test
MV error on test
Var(WQ ) on test

0

40

80

120

160

T

0

40

80

120

160

T

Figure 1: Behavior of the exponential risk bound (EQ bound), the true exponential risk (EQ on test),
the Gibbs risk (E(WQ ) on test), its variance (Var(WQ ) on test), and the test error of the majority
vote (MV error on test) as of function of the boosting round T for the Mushroom (left) and the Sonar
(right) data sets. The risk bound and the true risk were computed for β = ln 2.

0.5

0.4

0.3

0.2

0.1

0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

β = 1
β = 2
β = 3
β = 4
MV error on test

β = 1
β = 2
β = 3
β = 4
MV error on test

1

40

80

120

160

T

1

40

80

120

160

T

Figure 2: Behavior of the true exponential risk (left) and the exponential risk bound (right) for
different values of β on the Mushroom data set.

Since the Taylor series expansion for tanh(x) about x = 0 converges only for |x| < π/2, we are
limited to β ≤ π/2. Under these circumstances, we have
c = tan(β )
1
cos(β ) sin(β ) .

k =

Similarly as in Figure 1, we see on Figure 3 that the sigmoid loss bound curve, computed on the
training set, is essentially parallel to the true sigmoid loss curve computed on the testing set. More-
over, the bound appears to be as tight as the one for the exponential risk on Figure 1.

5 Conclusion

By trying to obtain a tight PAC-Bayesian risk bound for the majority vote, we have obtained a
PAC-Bayesian risk bound for any loss function ζQ that has a convergent Taylor expansion around
WQ = 1/2 (such as the exponential loss and the sigmoid loss). Unfortunately, the proposed risk

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

TQ bound
TQ on test
E(WQ ) on test
MV error on test
Var(WQ ) on test

TQ bound
TQ on test
E(WQ ) on test
MV error on test
Var(WQ ) on test

0

40

80

120

160

T

0

40

80

120

160

T

Figure 3: Behavior of the sigmoid risk bound (TQ bound), the true sigmoid risk (TQ on test), the
Gibbs risk (E(WQ ) on test), its variance (Var(WQ ) on test), and the test error of the majority vote
(MV error on test) as of function of the boosting round T for the Mushroom (left) and the Sonar
(right) data sets. The risk bound and the true risk were computed for β = ln 2.

bound is tight only for small values of the scaling factor c involved in the relation between the
expected loss ζQ of a convex combination of binary classiﬁers and the zero-one loss of a related
Gibbs classiﬁer GQ . However, it is quite encouraging to notice in our numerical experiments with
Adaboost that the proposed loss bound (for the exponential loss and the sigmoid loss), behaves very
similarly as the true loss.

Acknowledgments

Work supported by NSERC Discovery grants 262067 and 122405.

References
[1] David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37:355–363, 1999.
[2] Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. Journal of
Machine Learning Research, 3:233–269, 2002.
[3] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21,
2003.
[4] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine
Learning Research, 6:273–306, 2005.
[5] Franc¸ ois Laviolette and Mario Marchand. PAC-Bayes risk bounds for sample-compressed
Gibbs classiﬁers. Proceedings of the 22nth International Conference on Machine Learning
(ICML 2005), pages 481–488, 2005.
[6] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In S. Thrun S. Becker and
K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 423–
430. MIT Press, Cambridge, MA, 2003.
[7] Leo Breiman. Bagging predictors. Machine Learning, 24:123–140, 1996.
[8] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences, 55:119–139, 1997.
[9] Robert E. Schapire and Yoram Singer. Improved bosting algorithms using conﬁdence-rated
predictions. Machine Learning, 37:297–336, 1999.
[10] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A
new explanation for the effectiveness of voting methods. The Annals of Statistics, 26:1651–
1686, 1998.


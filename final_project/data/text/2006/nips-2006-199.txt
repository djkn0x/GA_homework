Hyperparameter Learning for Graph Based
Semi-supervised Learning Algorithms

Xinhua Zhang∗
Statistical Machine Learning Program
National ICT Australia, Canberra, Australia
and CSL, RSISE, ANU, Canberra, Australia
xinhua.zhang@nicta.com.au

Wee Sun Lee
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
leews@comp.nus.edu.sg

Abstract

Semi-supervised learning algorithms have been successfully applied in many ap-
plications with scarce labeled data, by utilizing the unlabeled data. One important
category is graph based semi-supervised learning algorithms, for which the perfor-
mance depends considerably on the quality of the graph, or its hyperparameters. In
this paper, we deal with the less explored problem of learning the graphs. We pro-
pose a graph learning method for the harmonic energy minimization method; this
is done by minimizing the leave-one-out prediction error on labeled data points.
We use a gradient based method and designed an efﬁcient algor ithm which signif-
icantly accelerates the calculation of the gradient by applying the matrix inversion
lemma and using careful pre-computation. Experimental results show that the
graph learning method is effective in improving the performance of the classi ﬁca-
tion algorithm.

1

Introduction

Recently, graph based semi-supervised learning algorithms have been used successfully in various
machine learning problems including classi ﬁcation, regre ssion, ranking, and dimensionality reduc-
tion. These methods create graphs whose vertices correspond to the labeled and unlabeled data
while the edge weights encode the similarity between each pair of data points. Classi ﬁcation is
performed using these graphs by labeling unlabeled data in such a way that instances connected by
large weights are given similar labels. Example graph based semi-supervised algorithms include
min-cut [3], harmonic energy minimization [11], and spectral graphical transducer [8].

The performance of the classi ﬁer depends considerably on th e similarity measure of the graph, which
is normally deﬁned in two steps. Firstly, the weights are deﬁ
ned locally in a pair-wise parametric
form using functions that are essentially based on a distance metric such as radial basis functions
(RBF). It is argued in [7] that modeling error can degrade performance of semi-supervised learning.
As the distance metric is an important part of graph based semi-supervised learning, it is crucial to
use a good distance metric. In the second step, smoothing is applied globally, typically, based on
the spectral transformation of the graph Laplacian [6, 10].

There have been only a few existing approaches which address the problem of graph learning. [13]
learns a nonparametric spectral transformation of the graph Laplacian, assuming that the weight and
distance metric are given. [9] learns the spectral parameters by performing evidence maximization
using approximate inference and gradient descent. [12] uses evidence maximization and Laplace
approximation to learn simple parameters of the similarity function. Instead of learning one single
good graph, [4] proposed building robust graphs by applying random perturbation and edge removal

∗This work was done when the author was at the National University of Singapore.

from an ensemble of minimum spanning trees. [1] combined graph Laplacians to learn a graph.
Closest to our work is [11], which learns different bandwidths for different dimensions by minimiz-
ing the entropy on unlabeled data; like the maximum margin motivation in transductive SVM, the
aim here is to get conﬁdent labeling of the data by the algorit hm.

In this paper, we propose a new algorithm to learn the hyperparameters of distance metric, or more
speci ﬁcally, the bandwidth for different dimensions in the RBF form. In essence, these bandwidths
are just model parameters and normal model selection methods include k-fold cross validation or
leave-one-out (LOO) cross validation in the extreme case can be used for selecting the bandwidths.
Motivated by the same spirit, we base our learning algorithm on the aim of achieving low LOO
prediction loss on labeled data, i.e., each labeled data can be correctly classi ﬁed by the other labeled
data in a semi-supervised style with as high probability as possible. This idea is similar to [5] which
learns multiple parameters for SVM. Since most LOO style algorithms are plagued with prohibitive
computational cost, an efﬁcient algorithm is designed. Wit h a simple regularizer, the experimental
results show that learning the hyperparameters by minimizing the LOO loss is effective.

2 Graph Based Semi-supervised Learning

Suppose we have a set of labeled data points {(xi , yi )} for i ∈ L , {1, ..., l}. In this paper, we
only consider binary classi ﬁcation, i.e., yi ∈ {1 (positive), 0 (negative)}. In addition, we also have
a set of unlabeled data points {xi } for i ∈ U , {l + 1, ..., l + u}. Denote n , l + u. Suppose the
dimensionality of input feature vectors is m.

2.1 Graph Based Classi ﬁcation Algorithms

(1)

wij (fi − fj )2

One of the earliest graph based semi-supervised learning algorithms is min-cut by [3], which mini-
mizes:
E (f ) , Xi,j
where the nonnegative wij encodes the similarity between instance i and j . The label fi is ﬁxed to
yi ∈ {1, 0} if i ∈ L. The optimization variables fi (i ∈ U ) are constrained to {1, 0}. This combi-
natorial optimization problem can be efﬁciently solved by t he max- ﬂow algorithm. [11] relaxed the
constraint fi ∈ {1, 0} (i ∈ U ) to real numbers. The optimal solution of the unlabeled data’s soft
labels can be written neatly as:
fU = (DU − WU U )−1WU L fL = (I − PU U )−1PU L fL
(2)
where fL is the vector of soft labels ( ﬁxed to yi ) for L. D , diag(di ), where di , Pj wij and DU
is the submatrix of D associated with unlabeled data. P , D−1W . WU U , WU L , PU U , and PU L
are deﬁned by:
PU L PU U (cid:19) .
WU L WU U (cid:19) , P = (cid:18) PLL PLU
W = (cid:18) WLL WLU
The solution (2) has a number of interesting properties pointed out by [11]. All fi (i ∈ U ) are
automatically bounded by [0, 1], so it is also known as square interpolation. They can be interpreted
by using Markov random walk on the graph. Imagine a graph with n nodes corresponding to the n
data points. Deﬁne the probability of transferring from xi to xj as pij , which is actually row-wise
normalization of wij . The random walk starts from any unlabeled points, and stops once it hits any
labeled point (absorbing boundary). Then fi is the probability of hitting a positive labeled point. In
this sense, the labeling of each unlabeled point is largely based on its neighboring labeled points,
which helps to alleviate the problem of noisy data. (1) can also be interpreted as a quadratic energy
function and its minimizer is known to be harmonic: fi (i ∈ U ) equals the average of fj (j 6= i)
weighted by pij . So we call this algorithm Harmonic Energy Minimization (HEM). By (1), fU is
independent of wii (i = 1, ..., n), so henceforth we ﬁx wii = pii = 0.
Finally, to translate the soft labels fi to hard labels pos/neg, the simplest way is by thresholding at
0.5, which works well when the two classes are well separated. [11] proposed another approach,
called Class Mass Normalization (CMN), to make use of prior information such as class ratio in
unlabeled data, estimated by that in labeled data. Speci ﬁca lly, they normalize the soft labels to f +
i ,
fi.Pn
i , (1 − fi ).Pn
j=1 fj as the probabilistic score of being positive, and to f −
j=1 (1 − fj ) as

the score of being negative. Suppose there are r+ positive points and r− negative points in the
labeled data, then we classify xi to positive iff f +
i r+ > f −
i r− .

2.2 Basic Hyperparameter Learning Algorithms

One of the simplest parametric form of wij is RBF:
d (cid:17)
wij = exp (cid:16)− Xd
(xi,d − xj,d )2(cid:14)σ2
where xi,d is the dth component of xi , and likewise the meaning of fU,i in (4). The bandwidth σd
has considerable inﬂuence on the classi ﬁcation accuracy. H EM uses one common bandwidth for all
dimensions, which can be easily selected by cross validation. However, it will be desirable to learn
different σd for different dimensions; this allows a form of feature selection. [11] proposed learning
the hyperparameters σd by minimizing the entropy on unlabeled data points (we call it MinEnt):
H (fU ) = − Xu
i=1
The optimization is conducted by gradient descent. To prevent numerical problems, they replaced
P with ˜P = U + (1 − )P , where  ∈ [0, 1), and U is the uniform matrix with Uij = n−1 .

(fU,i log fU,i + (1 − fU,i ) log(1 − fU,i ))

(4)

(3)

3 Leave-one-out Hyperparameter Learning

In this section, we present the formulation and efﬁcient cal culation of our graph learning algorithm.

3.1 Formulation and Ef ﬁcient Calculation

P t
U L

We propose a graph learning algorithm which is similar to minimizing the leave-one-out cross vali-
dation error. Suppose we hold out a labeled example xt and predict its label by using the rest of the
labeled and unlabeled examples. Making use of the result in (2), the soft label for xt is s> f t
U (the
U ), where
ﬁrst component of f t
n )> .
s , (1, 0, ..., 0)> ∈ Ru+1 , f t
U , (f t
l+1 , ..., f t
0 , f t
U = (I − ˜P t
U U )−1 ˜P t
Here, the value of f t
U can be determined by f t
L , where
U L f t
pU t PU U (cid:19),
U U , (cid:18) ptt
ptU
L , (f1 , .., ft−1 , ft+1 , ..., fl )> , ˜pij , (1 − ε)pij + ε/n , P t
f t
pU t , (pl+1,t , ..., pn,t )> , ptU , (pt,l+1 , ..., pt,n ) ,
pt,l
· · ·
pt,t+1
pt,t−1
· · ·
pt,1

= 
pl+1,1
· · ·
pl+1,t−1
pl+1,t+1
· · ·
pl+1,l
∆


· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
pn,1
· · ·
pn,t−1
pn,t+1
· · ·
pn,l
If xt is positive, then we hope that f t
U,1 is as close to 1 as possible. Otherwise, if xt is negative, we
U,1 is as close to 0 as possible. So the cost function to be minimized can be written as:
hope that f t
Q = Xl
U,1 (cid:1) = Xl
L(cid:17)
ht (cid:16)s> (I − ˜P t
U U )−1 ˜P t
U L f t
ht (cid:0)f t
t=1
t=1
where ht (x) is the cost function for instance t. We denote ht (x) = h+ (x) for yt = 1 and ht (x) =
h− (x) for yt = 0. Possible choices of h+ (x) include 1 − x, (1 − x)a , a−x , and − log x with a > 1.
Possible choices for h− (x) include x, xa , ax−1 , and − log(1−x). Let Loo loss(xt , yt ) , ht (cid:0)f t
U,1 (cid:1).
To minimize Q, we use gradient-based optimization methods. The gradient is:
L(cid:17),
U L.∂σd · f t
U U .∂σd · f t
U U )−1 (cid:16)∂ ˜P t
U + ∂ ˜P t
U,1 (cid:1) s> (I − ˜P t
∂Q/∂σd = Pl
t (cid:0)f t
t=1 h0
U,1 )s> (I − ˜P t
using matrix property dX −1 = −X −1 (dX )X −1 . Denoting (β t )> , h0
U U )−1
t (f t
and noting ˜P = εU + (1 − ε)P , we have
∂Q/∂σd = (1 − ε) Xl
(6)
U L(cid:14)∂σd · f t
U U (cid:14)∂σd · f t
U + ∂P t
(β t )> (cid:0)∂P t
L (cid:1).
t=1
Since in both P t
U L , the ﬁrst row corresponds to xt , and the ith row (i ≥ 2) corresponds
U U and P t
to xi+l−1 , denoting P t
U U ) makes sense as each row of P t
U N corresponds to a well
U N , (P t
U L P t

(5)

.

deﬁned single data point. Let all notations about P carry over to the corresponding W . We now use
i , Pn
U N (i, k) and Pn
U N (i, k)/∂σd (i = 1, ..., u + 1) to denote the sum of these
swt
k=1 ∂wt
k=1 wt
corresponding rows. Now (6) can be rewritten in ground terms by the following “two” equations:
U • (i, j ) Xn
i )−1 (cid:16)∂wt
U N (i, k)(cid:14)∂σd(cid:17) ,
U • (i, j )(cid:14)∂σd = (swt
∂P t
U • (i, j )(cid:14)∂σd − pt
∂wt
k=1
where • can be U or L. ∂wij /∂σd = 2wij (xi,d − xj,d )2(cid:14)σ3
d by (3).
The na¨ıve way to calculate the function value Q and its gradient is presented in Algorithm 1. We
call it leave-one-out hyperparameter learning (LOOHL).

5:

4:

Algorithm 1 na¨ıve form of LOOHL
1: function value Q ← 0, gradient g ← (0, ..., 0)> ∈ Rm
2: for each t = 1, ..., l (leave-one-out loop for each labeled point) do
U U )−1 ˜P t
U ← (I − ˜P t
U L f t
L ← (f1 , .., ft−1 , ft+1 , ..., fl )> ,
f t
f t
L ,
3:
U,1 )s> (I − ˜P t
U U )−1
t (f t
(β t )> ← h0
Q ← Q + ht (cid:0)f t
U,1 (cid:1),
for each d = 1, ..., m (for all feature dimensions) do
i (cid:18) ∂wt
(cid:19)
n
∂P t
∂wt
U U (i,j )
U U (i,j )
U N (i,k)
← 1
Pk=1
swt
∂σd
∂σd
∂σd
i = Pn
where swt
k=1 wt
U N (i, k),
(cid:19)
i (cid:18) ∂wt
n
∂wt
∂P t
U N (i,k)
U L (i,j )
U L (i,j )
← 1
− pt
Pk=1
U L (i, j )
i = 1, ..., u + 1, j = 1, ..., l − 1
swt
∂σd
∂σd
∂σd
U + ∂P t
gd ← gd + (1 − )(β t )> (cid:16) ∂P t
L(cid:17)
f t
f t
7:
U L
U U
∂σd
∂σd
end for
8:
9: end for

i, j = 1, ..., u + 1

− pt
U U (i, j )

6:

The computational complexity of the na¨ıve algorithm is expensive: O(lu(mn+u2 )), just to calculate
the gradient once. Here we assume the cost of inverting a u × u matrix is O(u3 ). We reduce the two
terms in the cost by means of using matrix inversion lemma and careful pre-computation.
One part of the cost, O(lu3 ), stems from inverting I − ˜P t
U U , a (u + 1) × (u + 1) matrix, for l times in
(5). We note that for different t, I − ˜P t
U U differs only by the ﬁrst row and ﬁrst column. So there exist
two vectors α, β ∈ Ru+1 such that I − ˜P t1
U U = (I − ˜P t2
U U ) + eα> + β e> , where e = (1, 0, ..., 0)>
∈ Ru+1 . With I − ˜P t
U U expressed in this form, we are ready to apply matrix inversion lemma:
−1
= A−1 − A−1α · β>A−1(cid:14)(cid:0)1 + α>Aβ (cid:1).
(cid:0)A + αβ> (cid:1)
We only need to invert I − ˜P t
U U for t = 1 from scratch, and then apply (7) twice for each t > 2. The
new total complexity related to matrix inversion is O (cid:0)u3 + lu2 (cid:1).
The other part of the cost, O(lumn) , can be reduced by using careful pre-computation. Written in
detail, we have:
i  u+1
u+1
l−1
l
∂wt
∂wt
β t
U U (i, j )
U L (i, j )
Xi=1
Xt=1
Xj=1
Xj=1
i
swt
∂σd
∂σd
 u+1
L,j !! ,
l−1
n
n
n
∂wt
U N (i, k)
Xj=1
Xj=1
Xi=1
Xj=1
Xk=1
U U (i, j ) f t
pt
U L (i, j ) f t
pt
U,j +
−
∂σd
The crucial observation is the existence of αij , which are independent of dimension index d. There-
fore, they can be pre-computed efﬁciently. The Algorithm 2 b elow presents the efﬁcient approach
to gradient calculation.

∂wij
∂σd

∂Q
∂σd

f t
U,j +

f t
L,j

=

(7)

αij

Algorithm 2 Efﬁcient algorithm to gradient calculation
1: for i, j = 1, ..., n do
for all feature dimension d on which either xi or xj is nonzero do
2:
3:
gd = gd + αij · ∂wij /∂σd
end for
4:
5: end for

Figure 1: Examples of degenerative graphs learned by pure LOOHL.

pik f t
U,k−l+1 −pit f t
U,1 −

n
Xk=l+1
pik f i
U,k−l+1 −

for i 6 l and j 6 l,

for i > l and j > l;
pik fk
l
Xk=1:k 6=t

for i > l and j 6 l;
for i 6 l and j > l;

Letting swi , Pn
k=1 wik and δ(·) be Kroneker delta, we derive the form of αij as:
pik fk
i−l+1  f t
l
n
l
Xt=1
Xk=l+1
Xk=1:k 6=t
αij = sw−1
β t
U,j−l+1 −

i
i−l+1 
l
n
Xt=1
Xk=l+1
αij = sw−1
f t
U,1 δ(t = j ) + fj δ(t 6= j ) − pit f t
β t
pik f t
U,1 −
U,k−l+1 −
i
pik fk!
1  f i
l
Xk=1
αij = sw−1
i β i
pik f i
U,k−l+1 −
U,j−l+1 −
pik fk!
1  fj −
l
n
Xk=1
Xk=l+1
αij = sw−1
i β i
and αii are ﬁxed to 0 for all
i since we ﬁx wii = pii = 0.
All αij can be computed in O(u2 l) time and Algorithm 2 can be completed in O(n2 ˜m) time, where
˜m , 2n−1 (n − 1)−1 · X16i<j6n
In many applications such as text classi ﬁcation and image pa ttern recognition, the data is very sparse
and ˜m (cid:28) m. In sum, the computational cost has been reduced from O(lu(mn + u2 )) to O(lnu +
n2 ˜m + u3 ) . The space cost is mild at O(n2 + n ˜m).

|{ d ∈ 1...m| xi or xj is not zero on feature d}|.

4 Regularizing the Graph Learning

Similar to the MinEnt method, purely applying LOOHL can lead to degenerative graphs. In this
section, we show two such examples and then propose a simple approach which regularizes the
graph learning process.
Two degenerative graphs are shown in Figure 1. In example (a), the points with the same xv co-
ordinate are from the same classes. For each labeled point, there is another labeled point from the
opposite class which has the same xh coordinate. So the leave-one-out hyperparameter learning will
push 1/σh to zero and 1/σv to inﬁnity, i.e., all points can transfer only horizontally . Therefore the
graph will effectively split into six disconnected sub-graphs, each sharing the same xv coordinate as
showed in (a). So the desired gradual change of label from positive to negative along dimension xv
cannot appear. As a result, the point at question mark cannot hit any labeled points and cannot be
classi ﬁed. One way to prevent such degenerate graphs is to pr event 1/σv from growing too large,
e.g., with a regularizer such as Pd (1/σd )2 .
In example (b), although the negative points will encourage both horizontal and vertical walk, hor-
izontal walk will make the leave-one-out error large on positive points. So the learned 1/σv will
be far smaller than 1/σh , i.e., the result strongly encourages walking in vertical direction and ig-
noring the information from the horizontal direction. As a result, the point at the question mark
will be labeled as positive, although by nearest neighbor intuition, it should be labeled as negative.
We notice that the four negative points will be partitioned into two groups as shown in the ﬁgure.
In such a case, the regularizer Pd (1/σd )2 will not be helpful with utilizing dimensions that are
informative. A different regularizer that encourages the use of more dimensions may be better in
this case. One simple regularizer that has this property is to minimize the variance of the inverse
bandwidth Pd (1/σd − µ)2 , where µ = m−1 Pd 1/σd , assuming that the mean is non-zero. It

Table 1: Dataset properties. Sparsity is the average frequency of features to be zero in the whole
dataset. The rightmost column gives the size of the whole dataset from which the labeled data in
experiment is sampled. Some data in text dataset has unknown label, thus always used as unlabeled.

is a priori unclear which regularizer will be better empirically, but for the datasets in our experi-
ments, the minimum variance regularizer is overwhelmingly better, even when useless features are
intentionally added to the datasets.

Since the gradient based optimization can get stuck in local minima, it is advantageous to test several
different parameter initialization. With this in mind, we implement a simple approximation to the
minimum variance regularizer that tests different parameter initialization as well. We discretize µ
and minimize the leave-one-out loss plus Pd (1/σd − 1/ ˜σ)2 , where ˜σ is ﬁxed a priori to several
different possible values. We run with different ˜σ and set all initial σd to ˜σ . Then we choose the
function produced by the value of ˜σ that has the smallest regularized cost function value. This
process is similar to restarting from various values to avoid local minima, but now we are also trying
with different mean of estimated optimal bandwidth at the same time. A similar way to regularize is
by using a Gaussian prior with mean µ−1 and minimizing Q + C Pd (1/σd − 1/µ)2 with respect
to σd and µ simultaneously.
5 Experimental Results

Using HEM as a basis for classi ﬁcation, we compare the test ac curacy of three model selection
methods: LOOHL, 5-CV (tying all bandwidths and choose by 5-fold cross validation), and MinEnt,
each with both thresholding and CMN. Since the topic of this paper is how to learn the hyperpa-
rameters of a graph, we pay more attention to how the performance of a given recognized classi ﬁer
can be improved by means of learning the graph, than to the comparison between different clas-
sifers’ performance, i.e., comparing with other semi-supervised or supervised learning algorithms.
Ionosphere is from UCI repository. The other four datasets used in the experiment are from NIPS
2003 Workshop on feature selection challenge. Each of them has two versions: original version
and probe version which adds useless probing features in order to investigate the algorithm’s per-
formance in the presence of useless features, though at current stage we do not use the algorithm
as feature selector. Since the workshop did not provide the original datasets, we downloaded the
original datasets from other sites. Our original intention was to use original versions that we down-
loaded and to reproduce the probe version ourself using the pre-processing described in NIPS 2003
workshop, so that we can check the performance of the algorithms on datasets with and without
redundant features. Unfortunately, we ﬁnd that with our own effort at pre-processing, the datasets
with probes yield far different accuracies compared with the datasets with probes downloaded from
the workshop web site. Thus we are using the original version and the probe version downloaded
from difference sources, and the comparison between them should be done with care, though the
demonstration of LOOHL’s efﬁcacy is not affected.

The properties of the ﬁve datasets are summarized in Table 1. We randomly pick the labeled subset
L from all labeled data available under the constraint that both classes must be present in L. The
remaining labeled and unlabeled data are used as unlabeled data. For example, by saying |L| = 20
for text dataset, we mean randomly picking 20 points from the 600 labeled data as labeled, and label
the other 1980 points by using our algorithm. Finally we calculate the prediction accuracy on the
580 (originally) labeled points. For other datasets, say cancer, testing is on 180 points since we
know the label of all points. For each ﬁxed |L|, this random test is conducted for 10 times and the
average accuracy is reported. Then |L| is varied. We normalized all input feature vectors to have
length 1.

(a) 4 vs 9 (original)

(b) cancer (original)

(c) text (original)

(d) thrombin (original)

(e) 4 vs 9 (probe)

(f) cancer (probe)

(g) text (probe)

(h) thrombin (probe)

Figure 2: Accuracy of original and probe versions in percentage vs. number of labeled data.

The initial common bandwidth and smoothing factor  in MinEnt are selected by ﬁve fold cross
validation. For LOOHL, We ﬁx h+ (x) = (1 − x)2 and h− (x) = x2 . The ﬁnal objective function is:
C1 × Loo loss N ormal + C2 × Pd (1/σd − 1/ ˜σ)2.m,
Loo loss N ormal , (2r+ )−1 Xyi=1
Loo loss(xi , yi )+(2r− )−1 Xyi=0
Loo loss(xi , yi ), (8)
and there are r+ positive labeled examples and r− negative labeled examples. For each C1 :C2 ratio,
we run on ˜σ = 0.05, 0.1, 0.15, 0.2, 0.25, 0.3 for all datasets and select the function that corresponds
to the smallest objective function value for use in cross validation testing. The ﬁnal C1 :C2 value
was picked by ﬁve fold cross validation, with discrete level s at 10−i , where i = 1, 2, 3, 4, 5, since
strong regularizer is needed given the large number of features (variables) and much fewer labeled
points. The optimization solver we use is the Toolkit for Advanced Optimization [2].

From the results in Figure 2 and Figure 3, we can make the following observations and conclusions:

1. LOOHL generally outperforms 5-CV and MinEnt. Both LOOHL+Thrd and LOOHL+CMN
outperform 5-CV and MinEnt (regardless of Thrd or CMN) on all datasets except thrombin and
ionosphere, where either LOOHL+CMN or LOOHL+Thrd ﬁnally pe rforms best.

2. For 5-CV, CMN is almost always better than thresholding, except on the original form of cancer
and thrombin dataset, where CMN hurts 5-CV. In [11], it is claimed that although the theory of
HEM is sound, CMN is still necessary to achieve reasonable performance because the underlying
graph is often poorly estimated and may not reﬂect the classi
ﬁcation goal, i.e., one should not rely
exclusively on the graph. Now that our LOOHL is aimed at learning a good graph, the ideal case is
that the graph learned is suitable for our classi ﬁcation suc h that the improvement by CMN will not
be large. In other words, the difference between LOOHL+CMN and LOOHL+Thrd, compared with
the difference between 5-CV+CMN and 5-CV+Thrd, can be viewed as an approximate indicator of
how well the graph is learned by LOOHL.

The efﬁcacy of LOOHL can be clearly observed in datasets 4vs9 , cancer, text, ionosphere and orig-
inal version of thrombin. In these cases, we see that LOOHL+Thrd is already achieving high accu-
racy and LOOHL+CMN does not offer much improvement then or even hurts performance due to
inaccurate class ratio estimation. In fact, LOOHL+Thrd performs reliably well on all datasets. It is
thus desirable to learn the bandwidth for each dimension of the feature vector, and there is no longer
any need to post-process by using class ratio information.

3. The performance of MinEnt is generally inferior to 5-CV and LOOHL. MinEnt+Thrd has equal
chance of out-performing or losing to 5-CV+Thrd, while 5-CV+CMN is almost always better than
MinEnt+CMN. Most of the time, MinEnt+CMN performs signi ﬁca ntly better than MinEnt+Thrd,
so we can conclude that MinEnt fails to learn a good graph. This may be due to converging to a poor
local minimum, or that the idea of minimizing the entropy on unlabeled data is by itself insufﬁcient.

Figure 3: Accuracy of Ionosphere
in percentage vs. number of labeled
data.

Figure 4: Accuracy comparison of priors in percentage
between minimizing sum of square inverse bandwidth
Pd σ−2
and minimizing variance of inverse bandwidth.
d
4. For these datasets, assuming low variance of inverse bandwidth with discretization as regularizer
is more reasonable than assuming that many features are irrelevant to the classi ﬁcation. This is even
true for probe versions of the datasets. Figure 4 shows the comparison.

6 Conclusions

In this paper, we proposed learning the graph for graph based semi-supervised learning by mini-
mizing the leave-one-out prediction error, with a simple regularizer. Efﬁcient gradient calculation
algorithms are designed and the empirical result is encouraging.

Acknowledgements

This work is partially funded by the Singapore-MIT Alliance. National ICT Australia is funded
through the Australian Government’s Backing Australia’s Ability initiative, in part through the Aus-
tralian Research Council.
References
[1] Andreas Argyriou, Mark Herbster, and Massimiliano Pontil. Combining Graph Laplacians for Semi-
Supervised Learning. In NIPS 2005, Vancouver, Canada, 2005.
[2] Steven Benson, Lois McInnes, Jorge Mor ´e, and Jason Sarich. TAO User Manual ANL/MCS-TM-242,
http://www.mcs.anl.gov/tao, 2005.
[3] Avrin Blum, and Shuchi Chawla. Learning From Labeled and Unlabeled Data using Graph Mincuts. In
ICML 2001.
[4] Miguel ´A Carreira-Perpi ˇn ´an, and Richard S. Zemel. Proximity Graphs for Clustering and Manifold Learn-
ing. In NIPS 2004.
[5] Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. Choosing Multiple Param-
eters for Support Vector Machines. Machine Learning, 46, 131–159, 2002.
[6] Olivier Chapelle, Jason Weston, and Bernhard Sch ¨olkopf. Cluster Kernels for Semi-Supervised Learning.
In NIPS 2002.
[7] Fabio G. Cozman, Ira Cohen, and Marcelo C. Cirelo. Semi-Supervised Learning of Mixture Models and
Bayesian Networks. In ICML 2003.
[8] Thorsten Joachims. Transductive Learning via Spectral Graph Partitioning. In ICML 2003.
[9] Ashish Kapoor, Yuan Qi, Hyungil Ahn, and Rosalind Picard. Hyperparameter and Kernel Learning for
Graph Based Semi-Supervised Classiﬁcation. In NIPS 2005.
[10] Alexander Smola, and Risi Kondor. Kernels and Regularization on Graphs. In COLT 2003.
[11] Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-Supervised Learning Using Gaussian Fields
and Harmonic Functions. In ICML 2003.
[12] Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani. Semi-Supervised Learning: From Gaussian Fields
to Gaussian Processes. CMU Technical Report CMU-CS-03-175.
[13] Xiaojin Zhu, Jaz Kandola, Zoubin Ghahramani, and John Lafferty. Non-parametric Transforms of Graph
Kernels for Semi-Supervised Learning. In NIPS 2004.


Prediction on a Graph with a Perceptron

Mark Herbster, Massimiliano Pontil
Department of Computer Science
University College London
Gower Street, London WC1E 6BT, England, UK
{m.herbster, m.pontil}@cs.ucl.ac.uk

Abstract

We study the problem of online prediction of a noisy labeling of a graph with
the perceptron. We address both label noise and concept noise. Graph learning is
framed as an instance of prediction on a ﬁnite set. To treat label noise we show that
the hinge loss bounds derived by Gentile [1] for online perceptron learning can be
transformed to relative mistake bounds with an optimal leading constant when
applied to prediction on a ﬁnite set. These bounds depend crucially on the norm
of the learned concept. Often the norm of a concept can vary dramatically with
only small perturbations in a labeling. We analyze a simple transformation that
stabilizes the norm under perturbations. We derive an upper bound that depends
only on natural properties of the graph – the graph diameter and the cut size of a
partitioning of the graph – which are only indirectly dependent on the size of the
graph. The impossibility of such bounds for the graph geodesic nearest neighbors
algorithm will be demonstrated.

1 Introduction

We study the problem of robust online learning over a graph. Consider the following game for
predicting the labeling of a graph. Nature presents a vertex vi1 ; the learner predicts the label of the
vertex ˆy1 ∈ {−1, 1}; nature presents a label y1 ; nature presents a vertex vi2 ; the learner predicts ˆy2 ;
and so forth. The learner’s goal is minimize the total number of mistakes (|{t : ˆyt 6= yt}|). If nature
is adversarial, the learner will always mispredict; but if nature is regular or simple, there is hope
that a learner may make only a few mispredictions. Thus, a methodological goal is to give learners
whose total mispredictions can be bounded relative to the “complexity” of nature’s labeling. In this
paper, we consider the cut size as a measure of the complexity of a graph’s labeling, where the size
of the cut is the number of edges between disagreeing labels. We will give bounds which depend on
the cut size and the diameter of the graph and thus do not directly depend on the size of the graph.
The problem of learning a labeling of a graph is a natural problem in the online learning setting, as
well as a foundational technique for a variety of semi-supervised learning methods [2, 3, 4, 5, 6].
For example, in the online setting, consider a system which serves advertisements on web pages.
The web pages may be identiﬁed with the vertices of a graph and the edges as links between pages.
The online prediction problem is then that, at a given time t the system may receive a request to
serve an advertisement on a particular web page. For simplicity, we assume that there are two
alternatives to be served: either advertisement “A” or advertisement “B”. The system then interprets
the feedback as the label and then may use this information in responding to the next request to
predict an advertisement for a requested web page.

1.1 Related work
There is a well-developed literature regarding learning on the graph. The early work of Blum and
Chawla [2] presented an algorithm which explicitly ﬁnds min-cuts of the label set. Bounds have been

Figure 2: Barbell

Figure 4: Flower

Figure 5: Octopus

Figure 3: Barbell with concept noise

Input: {(vit , yt )}‘
t=1 ⊆ VM × {−1,1}.
Initialization: w1 = 0; MA = ∅.
for t = 1, . . . , ‘ do
Predict: receive vit
ˆyt = sign(e>
it wt )
Update: receive yt
if ˆyt = yt then
wt+1 = wt
else
wt+1 = wt + ytvit
MA = MA ∪ {t}
end
Figure 1: Perceptron on set VM .
proven previously with smooth loss functions [6, 7] in a batch setting. Kernels on graph labelings
were introduced in [3, 5]. This current work builds upon our work in [8]. There it was shown that,
given a ﬁxed labeling of a graph, the number of mistakes made by an algorithm similar to the kernel
perceptron [9] with a kernel that was the pseudoinverse of the graph Laplacian, could be bounded
by the quantity [8, Theorems 3.2, 4.1, and 4.2]
4ΦG (u)DG bal(u).
(1)
Here u ∈ {−1, 1}n is a binary vector deﬁning the labeling of the graph, ΦG (u) is the cut size1
and negative labels, DG is the diameter of the graph and bal(u) := (cid:0)1 − 1
n | P ui |(cid:1)−2 measures the
deﬁned as ΦG (u) := |{(i, j ) ∈ E (G) : ui 6= uj }|, that is, the number of edges between positive
label balance. This bound is interesting in that the mistakes of the algorithm could be bounded in
terms of simple properties of a labeled graph. However, there are a variety of shortcomings in this
result. First, we observe that the bound above assumed a ﬁxed labeling of the graph. In practice,
the online data sequence could contain multiple labels for a single vertex; this is the problem of
label noise. Second, for an unbalanced set of labels the bound is vacuous, for example, if u =
{1, 1, . . . , 1, −1} ∈ IRn then bal(u) = n2 . Third, consider the prototypical easy instance for the
algorithm of two dense clusters connected by a few edges, for instance, two m-cliques connected by
a single edge (a barbell graph, see Figure 2). If each clique is labeled with distinct labels then we
have that 4ΦG (u)DG bal(u) = 4×1×3×1 = 12, which is independent of m. Now suppose that, say,
the ﬁrst clique contains one vertex which is labeled as the second clique (see Figure 3). Previously
ΦG (u) = 1, but now ΦG (u) = m and the bound is vacuous. This is the problem of concept noise;
in this example, a Θ(1) perturbation of labeling increases the bound multiplicatively by Θ(m).

1.2 Overview
A ﬁrst aim of this paper is to improve upon the bounds in [8], particularly, to address the three
problems of label balance, label noise, and concept noise as discussed above. For this purpose, we
apply the well-known kernel perceptron [9] to the problem of online learning on the graph. We
discuss the background material for this problem in section 2, where we also show that the bounds
of [1] can be specialized to relative mistake bounds when applied to, for example, prediction on the
graph. A second important aim of this paper is to interpret the mistake bounds by an explanation
in terms of high level graph properties. Hence, in section 3, we reﬁne a diameter based bound
of [8, Theorem 4.2] to a sharper bound based on the “resistance distance” [10] on a weighted graph;
which we then closely match with a lower bound. In section 4, we introduce a kernel which is a
simple augmentation of the pseudoinverse of the graph Laplacian and then prove a theorem on the
performance of the perceptron with this kernel which solves the three problems above. We conclude
in section 5, with a discussion comparing the mistake bounds for prediction on the graph with the
halving algorithm [11] and the k-nearest neighbors algorithm.

2 Preliminaries
In this section, we describe our setup for Hilbert spaces on ﬁnite sets and its speciﬁcation to the
graph case. We then recall a result of Gentile [1] on prediction with the perceptron and discuss a
special case in which relative 0–1 loss (mistake) bounds are obtainable.

1Later in the paper we extend the deﬁnition of cut size to weighted graphs.

++++++------+++++-------++++++----------2.1 Hilbert spaces of functions deﬁned on a ﬁnite set
We denote matrices by capital bold letters and vectors by small bold case letters. So M denotes the
n × n matrix (Mij )n
i,j=1 and w the n-dimensional vector (wi )n
i=1 . The identity matrix is denoted
by I. We also let 0 and 1 be the n-dimensional vectors all of whose components equal to zero and
one respectively, and ei the i-th coordinate vector of IRn . Let IN be the set of natural numbers and
IN‘ := {1, . . . , ‘}. We denote a generic Hilbert space with H. We identify V := INn as the indices
of a set of n objects, e.g. the vertices of a graph. A vector w ∈ IRn can alternatively be seen as a
function f : V → IR such that f (i) = wi , i ∈ V . However, for simplicity we will use the notation
w to denote both a vector in IRn or the above function.
A symmetric positive semideﬁnite matrix M induces a semi-inner product on IRn which is deﬁned
as hu, wiM := u>Mw, where “> ” denotes transposition. The reproducing kernel [12] associated
with the above semi-inner product is K = M+ , where “+ ” denotes pseudoinverse. We also deﬁne
the coordinate spanning set

VM := {vi := M+ei : i = 1, . . . , n}
(2)
and let H(M) := span(VM ). The restriction of the semi-inner product h·, ·iM to H(M) is an inner
product on H(M). The set VM acts as “coordinates” for H(M), that is, if w ∈ H(M) we have
i Mw = hvi , wiM ,
wi = e>
i M+Mw = v>
(3)
although the vectors {v1 , . . . , vn } are not necessarily normalized and are linearly independent only
if M is positive deﬁnite. We note that equation (3) is simply the reproducing kernel property [12]
for kernel M+ .
When V indexes the vertices of an undirected graph G , a natural norm to use is that induced by
the graph Laplacian. We explain this in detail now. Let A be the n × n symmetric weight matrix
of the graph such that Aij ≥ 0, and deﬁne the edge set E (G ) := {(i, j ) : 0 < Aij , i < j }.
The distance matrix ∆ associated with G is the per-element inverse of the weight matrix, that is,
(∆ may have +∞ as a matrix element). The graph Laplacian G is the n × n matrix
di = Pn
∆ij = 1
Aij
deﬁned as G := D − A where D = diag(d1 , . . . , dn ) and di is the weighted degree of vertex i,
G := w>Gw = X
j=1 Aij . The Laplacian is positive semideﬁnite and induces the semi-norm
kwk2
Aij (wi − wj )2 .
(i,j )∈E (G)
When the graph is connected, it follows from equation (4) that the null space of G is spanned by the
constant vector 1 only. In this paper, we always assume that the graph G is connected. Where it is
not ambiguous, we use the notation G to denote both the graph G and the graph Laplacian.

(4)

2.2 Online prediction of functions on a ﬁnite set with the perceptron
Gentile [1] bounded the performance of the perceptron algorithm on nonseparable data with the
linear hinge loss. Here, we apply his result to study the problem of prediction on a ﬁnite set with the
perceptron (see Figure 1). In this case, the inputs are the coordinates in the set VM ⊂ H(M) deﬁned
above. We additionally assume that matrix M is positive deﬁnite (not just positive semideﬁnite as
in the previous subsection). This assumption, along with the fact that the inputs are coordinates,
enables us to upper bound the hinge loss and hence we may give a relative mistake bound in terms
of the complete set of base classiﬁers {−1, 1}n .
t=1 ⊆ VM × {−1, 1} is
Theorem 2.1. Let M be a symmetric positive deﬁnite matrix. If {(vit , yt )}‘
a sequence of examples, MA denotes the set of trials in which the perceptron algorithm predicted
incorrectly and X = maxt∈MA kvit kM , then the cumulative number of mistakes |MA | of the
s
algorithm is bounded by
kuk4
kuk2
MX 2
MX 4
2|MA∩Mu |kuk2
|MA | ≤ 2|MA∩Mu | +
MX 2 +
+
4
2
for all u ∈ {−1, 1}n , where Mu = {t ∈ IN‘ : uit 6= yt}. In particular, if |Mu | = 0 then
|MA | ≤ kuk2
MX 2 .

(5)

Proof. This bound follows directly from [1, Theorem 8] with p = 2, γ = 1, and w1 = 0. Since M
is assumed to be symmetric positive deﬁnite, it follows that {−1, 1}n ⊂ H(M). Thus, the hinge
loss Lu,t := max(0, 1 − yt hu, vit iM ) of any classiﬁer u ∈ {−1, 1}n with any example (vit , yt )
is either 0 or 2, since |hu, vit iM | = 1 by equation (3). This allows us to bound the hinge loss term
of [1, Theorem 8] directly with mistakes.

We emphasize that our hypothesis on M does not imply linear separability since multiple instances
of an input vector in the training sequence may have distinct target labels. Moreover, we note that,
for deterministic prediction the constant 2 in the ﬁrst term of the right hand side of equation (5) is
optimal for an online algorithm as a mistake may be forced on every trial.
3 Interpretation of the space H(G)
The bound for prediction on a ﬁnite set in equation (5) involves two quantities, namely the squared
P
norm of a classiﬁer u ∈ {−1, 1}n and the maximum of the squared norms of the coordinates
v ∈ VM . In the case of prediction on the graph, recall from equation (4) that kuk2
G := u>Gu =
(i,j )∈E (G) Aij (ui − uj )2 . Therefore, we may identify this semi-norm with the weighted cut size
1
kuk2
ΦG (u) :=
(6)
4
G
of the labeling induced when u ∈ {−1, 1}n . In particular, with boolean weighted edges (Aij ∈
{0, 1}) the cut simply counts the number of edges spanning disagreeing labels.
The norm kv − wkG is a metric distance for v, w ∈ span(VG ) however, surprisingly, the square of
Gwhen restricted to graph coordinates vp , vq ∈ VG is also a metric known as
the norm kvp − vq k2
the resistance distance [10],
rG (p, q) := (ep − eq )>G+ (ep − eq ) = kvp − vq k2
(7)
G .
It is interesting to note that the resistance distance between vertex p and vertex q is the effective
resistance between vertices p and q , where the graph is the circuit and edge (i, j ) is a resistor with
the resistance ∆ij = A−1
ij .
G = kvp − 0k2
As we shall see, our bounds in section 4 depend on kvpk2
G . Formally, this is not an
P
effective resistance between vertex p and another vertex “0”. The vector 0, informally however, is
, since 1 is in the null space of H(G). In the following, we
v
v∈VG
the center of the graph as 0 =
|VG |
further characterize kvpk2
G . First, we observe qualitatively that the more interconnected the graph
the smaller the term kvpk2
G (Corollary 3.1). Second, in Theorem 3.2 we quantitatively upper bound
kvpk2
G by the average (over q ) of the effective resistance between vertex p and each vertex q in the
Lemma 3.1. Let x ∈ H then kxk−2 = minw∈H (cid:8)kwk2 : hw, xi = 1(cid:9).
graph (including q = p), which in turn may be upper bounded by the eccentricity of p. We proceed
with the following useful lemma and theorem, as a basis for our later results.
The proof is straightforward and we do not elaborate on the details.
Theorem 3.1. If M and M0 are symmetric positive semideﬁnite matrices with span(VM ) =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nX
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
≥ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nX
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
M ≤ kwk2
span(VM0 ) and, for every w ∈ span(VM ), kwk2
M0 then
aiv0
aivi
M0 ,
i
M
i=1
i=1
where vi ∈ VM , v0
i ∈ VM0 and a ∈ IRn .
i=1aivi and x0 = Pn
Proof. Let x = Pn
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x0
i=1aiv0
i then
kxk−2
≤
M =
kxk2
kx0 k2
kx0 k2
M0
M0
M0
M
M
M
, xi
where the ﬁrst inequality follows since h
x0
x0
= 1, hence
M0 is a feasible solution to
kx0 k2
kx0 k2
M0
M
the minimization problem in the right hand side of Lemma 3.1. While the second one follows
M ≤ kwk2
immediately from the assumption that kwk2
M0 .

= kx0 k−2
M0 ,

≤

As a corollary to the above theorem we have the following when M is a graph Laplacian.
Corollary 3.1. Given connected graphs G and G0 with distance matrices ∆ and ∆0 such that
∆ij ≤ ∆0
ij then for all p, q ∈ V , we have that kvpk2
G ≤ kv0
p k2
G0 and rG (p, q) ≤ rG0 (p, q).
The ﬁrst inequality in the above corollary demonstrates that kvpk2
G is nonincreasing in a graph that
is strictly more connected. The second inequality is the well-known Rayleigh’s monotonicity law
which states that if any resistance in a circuit is decreased then the effective resistance between any
two points cannot increase.
as |P (p, q)| := P
We deﬁne the geodesic distance between vertices p, q ∈ V to be dG (p, q) := min |P (p, q)| where
the minimum is taken with respect to all paths P (p, q) from p to q , with the path length deﬁned
(i,j )∈E (P (p,q)) ∆ij . The eccentricity dG (p) of a vertex p ∈ V is the geodesic
distance on the graph between p and the furthest vertex on the graph to p, that is, dG (p) =
maxq∈V dG (p, q) ≤ DG , and DG is the (geodesic) diameter of the graph, DG := maxp∈V dG (p).
A graph G is connected when DG < ∞. A tree is an n-vertex connected graph with n − 1 edges.
The following lemma, a well known result (see e.g. [10]), establishes that the resistance distance
can be be equated with the geodesic distance when the graph is a tree.
Lemma 3.2. If the graph T is a tree with graph Laplacian T then rT (p, q) = dT (p, q).
The next theorem provides a quantitative relationship between kvpk2
G and two measures of the
connectivity of vertex p, namely its eccentricity and the mean of the effective resistances between
vertex p and each vertex on the graph.
nX
Theorem 3.2. If G is a connected graph then
G ≤ 1
kvpk2
rG (p, q) ≤ dG (p).
G (see equation (7)) and use Pn
n
q=1
Pn
Pn
Proof. Recall that rG (p, q) = kvp − vq k2
q=1 vq = 0 to obtain that
q=1 kvp − vq k2
q=1 v>
G = v>
p Gvp + 1
q Gvq which implies the left inequality in (8). Next,
1
by Corollary 3.1, if T is the Laplacian of a tree T ⊆ G then rG (p, q) ≤ rT (p, q) for p, q ∈ V .
n
n
Therefore, from Lemma 3.2 we conclude that rG (p, q) ≤ dT (p, q). Moreover, since T ⊆ G can
be any tree, we have that rG (p, q) ≤ minT dT (p, q) where the minimum is over all trees T ⊆ G.
Since the geodesic path from p to q is necessarily contained in some tree T ⊆ G it follows that
minT dT (p, q) = dG (p, q) and, so, rG (p, q) ≤ dG (p, q). Now the theorem follows by maximizing
dG (p, q) over q and the deﬁnition of dG (p).

(8)

(9)

We identify the resistance diameter of a graph G as RG := maxp,q∈V rG (p, q); thus, from the
previous theorem, we may also conclude that
kvpk2
G ≤ RG ≤ DG .
max
p∈V
We complete this section by showing that there exists a family of graphs for which the above in-
equality is nearly tight. Speciﬁcally, we consider the “ﬂower graph” (see Figure 4) obtained by
connecting the ﬁrst vertex of a chain with p − 1 vertices to the root vertex of an m-ary tree of depth
one. We index the vertices of this graph so that vertices 1 to p correspond to “stem vertices” and
G : hw, v1 i = 1(cid:9) . We note
(cid:8)kwk2
vertices p + 1 to p + m to “petals”. Clearly, this graph has diameter equal to p, hence our upper
G ≤ p. We now argue that as m grows this bound is almost
bound above establishes that kv1 k2
tight. From Lemma 3.1 we have that kv1 k−2
G = minw∈H(G)
o
n
that by symmetry, the solution ˆw = ( ˆwi : i ∈ INp+m ) to the problem above satisﬁes ˆwi = z if
i=1 (wi − wi+1 )2 : w1 = 1, Pp
m(z − wp )2 + Pp−1
i ≥ p + 1 since ˆw must take the same value on the petal vertices. Consequently, it follows that
kv1 k−2
i=1 wi + mz = 0
G = min
. We upper
p−1 for 1 ≤ i ≤ p. Thus, w1 = 1 as it is required, wp = 0
bound this minimum by choosing wi = p−i
and we compute z by the constraint set of the above minimization problem as z = − p
2m . A direct
computation gives kv1 k−2
G ≤ 1
(p−1) + p2
4m from which using a ﬁrst order Taylor expansion it follows
G ≥ (p − 1) − (p−1)2 p2
4m . Therefore, as m → ∞ the upper bound on kv1 k2
that kv1 k2
G (equation (8))
for the ﬂower graph is matched by a lower bound with a gap of 1.

we have that

4 Prediction on the graph
We deﬁne the following symmetric positive deﬁnite graph kernel,
(0 < b, 0 ≤ c),
c := G+ + b11> + cI,
Kb
(10)
c )−1 is the matrix of the associated Hilbert space H(Gb
c = (Kb
c ). In Lemma 4.1 below
where Gb
we prove the needed properties of H(Gb
c ) as a necessary step for the bound in Theorem 4.2. As
we shall see, these properties moderate the consequences of label imbalance and concept noise. To
prove Lemma 4.1, we use the following theorem which is a special case of [12, Thm I, §I.6].
Theorem 4.1. If M1 and M2 are n × n symmetric positive semideﬁnite matrices, and we set
2 )+ then kwk2
M = inf {kw1 k2
M1 + kw2 k2
M2 : wi ∈ H(Mi ), w1 + w2 = w} for
M := (M+
1 + M+
every w ∈ H(M).
Pn
Next, we deﬁne βu ∈ [0, 1] as a measure of the balance of a labeling u ∈ {−1, 1}n as βu :=
( 1
i=1 ui )2 . Note that for a perfectly balanced labeling βu = 0, while βu = 1 for a perfectly
n
unbalanced one.
Lemma 4.1. Given a vertex p with associated coordinates vp ∈ VG and v0
p ∈ VGb
c
pk2
kv0
= kvpk2
G + b + c.
Gb
c
i 6= ui }| we have that
Moreover, if u, u0 ∈ {−1, 1}n and where k := |{i : u0
4k
≤ kuk2
ku0 k2
G + βu
+
.
Gb
b
c
c
Proof. To prove equation (11) we recall equation (3) and note that kv0
pk2
Gb
p , b1+ cep i
= kvp k2
p , vp i
+ hv0
hv0
c
G + b + c.
Gb
Gb
c
c
To prove inequality (12) we proceed in two steps. First, we show that
= kuk2
kuk2
G + βu
.
Pn
Pn
Gb
b
0
Indeed, we can uniquely decompose u as the sum of a vector in H(G) and one in H( 11>
n2 b ) as
u = (u − 1 1
i=1 ui . Therefore, by Theorem 4.1 we conclude that kuk2
i=1 ui ) + 1 1
=
ku − √
b , where ku − √
G + k√
Gb
n
n
0
= kuk2
βu1k2
βu1k2
βu1k2
G = kuk2
G since 1 ∈ H⊥ (G).
G + βu
11>
Second, we show, for any symmetric positive deﬁnite matrix M, u, u0 ∈ {−1, 1}n and c > 0, that
n2 b
4k
ku0 k2
≤ kuk2
M +
(14)
,
Mc
c
i 6= ui }|. To this end, we decompose u0 as a sum of
where Mc := (M−1 + cI)−1 and k := |{i : u0
c I) as u0 = u + (u0 − u) and observe that ku0 − uk2
two elements of H(M) and H( 1
c I = 4k
c . By
1
c I = kuk2
M + ku0 − uk2
≤ kuk2
Theorem 4.1 it then follows that ku0 k2
M + 4k
c . Now inequality (12)
1
Mc
follows by combining equations (13) and (14) with M = Gb
0 .

p , vp + b1+ cep i
=hv0
Gb
c

(13)

(11)

(12)

=

We can now state our relative mistake bound for online prediction on the graph.
t=1 ⊆ VGb
Theorem 4.2. Let G be a connected graph. If {(vit , yt )}‘
× {−1, 1} is a sequence of
examples and MA denotes the set of trials in which the perceptron algorithm predicted incorrectly,
r
c
then the cumulative number of mistakes |MA | of the algorithm is bounded by
Z
Z 2
Pn
|MA | ≤ 2|MA∩Mu | +
2|MA∩Mu |Z +
+
(15)
4 ,
2
(cid:18)
(cid:19)(cid:18)
(cid:19)
i )2 , Mu = {t ∈ IN‘ : uit 6= yt},
i 6= ui }|, βu0 = ( 1
for all u,u0 ∈ {−1, 1}n , where k = |{i : u0
i=1 u0
n
and
4k
Z =
4ΦG (u0 ) + βu0
RG + b + c
+
b
c
In particular, if b = 1, c = 0, k = 0 and |Mu | = 0 then
|MA | ≤ (4ΦG (u) + βu )(RG + 1).

(16)

.

via

and kvtk2
Gb
c

c , then bounding kuk2
Proof. The proof follows by Theorem 2.1 with M = Gb
Gb
Lemma 4.1, and then using maxt∈MA kvit k2
G ≤ RG by equation (9).
c
The upper bound of the theorem is more resilient to label imbalance, concept noise, and label noise
than the bound in [8, Theorems 3.2, 4.1, and 4.2] (see equation (1)). For example, given the noisy
barbell graph in Figure 3 but with k (cid:28) n noisy vertices the bound (1) is O(kn) while the bound (15)
with b = 1, c = 1, and |Mu | = 0 is O(k). A similar argument may be given for label imbalance.
In the bound above, for easy interpretability, one may upper bound the resistance diameter RG by
the geodesic diameter DG . However, the resistance diameter makes for a sharper bound in a number
of natural situations. For example now consider (a thick barbell) two m-cliques (one labeled “+1”,
one “-1”) with ‘ edges (‘ < m) between the cliques. We observe between any two vertices there
are at least ‘ edge-disjoint paths of length no more than ﬁve, therefore the resistance diameter is
at most 5/‘ by the “resistors-in-parallel” rule while the geodesic diameter is 3. Thus, for “thick
barbells” if we use the geodesic diameter we have a mistake bound of 16‘ (substituting βu = 0,
and RG ≤ 3 into (16)) while surprisingly with the resistance diameter the bound (substituting
4n , c = 0, |Mu | = 0, βu = 0, and RG ≤ 5/‘ into (15)) is independent of ‘ and is 20.
b = 1

5 Discussion
In this paper, we have provided a bound on the performance of the perceptron on the graph in terms
of structural properties of the graph and its labeling which are only indirectly dependent on the
number of vertices in the graph, in particular, they depend on the cut size and the diameter. In the
following, we compare the perceptron with two other approaches. First, we compare the percep-
tron with the graph kernel K1
0 to the conceptually simpler k-nearest neighbors algorithm with either
the graph geodesic distance or the resistance distance. In particular, we prove the impossibility of
bounding performance of k-nearest neighbors only in terms of the diameter and the cut size. Specif-
ically, we give a parameterized family of graphs for which the number of mistakes of the perceptron
is upper bounded by a ﬁxed constant independent of the graph size while k-nearest neighbors prov-
ably incurs mistakes linearly in the graph size. Second, we compare the perceptron with the graph
kernel K1
0 with a simple application of the classical halving algorithm [11]. Here, we conclude that
the upper bound for the perceptron is better for graphs with a small diameter while the halving al-
gorithm’s upper bound is better for graphs with a large diameter. In the following, for simplicity we
limit our discussion to binary-weighted graphs, noise-free data (see equation (16)) and upper bound
the resistance diameter RG with the geodesic diameter DG (see equation (9)).

5.1 K-nearest neighbors on the graph
We consider the k-nearest neighbors algorithms on the graph with both the resistance distance (see
equation (7)) and the graph geodesic distance. The geodesic distance between two vertices is the
length of the shortest path between the two vertices (recall the discussion in section 3).
In the
following, we use the emphasis distance to refer simultaneously to both distances. Now, consider
the family of O‘,m,p of octopus graphs. An octopus graph (see Figure 5) consists of a “head” which
is an ‘-clique (C (‘) ) with vertices denoted by c1 , . . . , c‘ , and a set of m “tentacles” ({Ti }m
i=1 ), where
each tentacle is a line graph of length p. The vertices of tentacle i are denoted by {ti,0 , ti,1 , . . . , ti,p };
the ti,0 are all identiﬁed as one vertex r which acts as the root of the m tentacles. There is an
edge (the body) connecting root r to the vertex c1 on the head. Thus, this graph has diameter
D = max(p + 2, 2p) and there are ‘ + mp + 1 vertices in total; an octopus Om,p is balanced if
‘ = mp + 1. Note that the distance of every vertex in the head to every other vertex in the graph is
no more than p + 2, and every tentacle “tip” ti,p is distance 2p to other tips tj,p : j 6= i.
We now argue that k-nearest neighbors may incur mistakes linear in the number of tentacles. To this
end, choose p ≥ 3 and suppose we have the following online data sequence
{(c1 , +1), (t1,p , −1), (c2 , +1), (t2,p , −1), . . . , (cm , +1), (tm,p , −1)}.
Note that k-nearest neighbors will make a mistake on every instance (ti,p , −1) and so, even assuming
that it predicts correctly on (c1 , +1) it will always make m mistakes. We now contrast this result with
the performance of the perceptron with the graph kernel K1
0 (see equation (10)). By equation (16),
the number of mistakes will be upper bounded by 10p + 5 because there is a cut of size 1 and the

diameter is 2p. Thus, for balanced octopi Om,p with p ≥ 3, as m grows the number of mistakes of
the kernel perceptron will be bounded by a ﬁxed constant. Whereas distance k-nearest neighbors
will incur mistakes linearly in m.

5.2 Halving algorithm
We now compare the performance of our algorithm to the classical halving algorithm [11]. The
halving algorithm operates by predicting on each trial as the majority of the classiﬁers in the concept
class which have been consistent over the trial sequence. Hence, the number of mistakes of the
ﬁxed graph G . The cardinality of KkG is upper bounded by (cid:0)n(n−1)
(cid:1) since any classiﬁer (cut) in KkG
halving algorithm is upper bounded by the logarithm of the cardinality of the concept class. Let
KkG = {u ∈ {−1, 1}n : ΦG (u) = k} be the set all of all classiﬁers with a cut size equal to k on a
k
can be uniquely identiﬁed by a choice of k edges and 1 bit which determines the sign of the vertices
in the same of partition (however we overcount as not every set of edges determines a classiﬁer).
The number of mistakes of the halving algorithm is upper bounded by O(k log n
k ). For example,
on a line graph with a cut size of 1 the halving algorithm has an upper bound of dlog ne while
the upper bound for the number of mistakes of the perceptron as given in equation (16) is 5n + 5.
Although the halving algorithm has a sharper bound on such large diameter graphs as the line graph,
it unfortunately has a logarithmic dependence on n. This contrasts to the bound of the perceptron
which is essentially independent of n. Thus, the bound for the halving algorithm is roughly sharper
on graphs with a diameter ω(log n
k ), while the perceptron bound is roughly sharper on graphs with
a diameter o(log n
k ). We emphasize that this analysis of upper bounds is quite rough and sharper
bounds for both algorithms could be obtained for example, by including a term representing the
minimal possible cut, that is, the minimum number of edges necessary to disconnect a graph. For
the halving algorithm this would enable a better bound on the cardinality of KkG (see [13]). While,
for the perceptron the larger the connectivity of the graph, the weaker the diameter upper bound in
Theorem 3.2 (see for example the discussion of “thick barbells” at the end of section 4).

Acknowledgments
We wish to thank the anonymous reviewers for their useful comments. This work was supported by
EPSRC Grant GR/T18707/01 and by the IST Programme of the European Community, under the
PASCAL Network of Excellence IST-2002-506778.
References
[1] C. Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265–299, 2003.
[2] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML 2002,
pages 19–26. Morgan Kaufmann, San Francisco, CA, 2002.
[3] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In ICML 2002,
pages 315–322. Morgan Kaufmann, San Francisco, CA, 2002.
[4] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic
functions. In ICML 2003, pages 912–919, 2003.
[5] A. Smola and R.I. Kondor. Kernels and regularization on graphs. In COLT 2003, pages 144–158, 2003.
[6] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs. In
COLT 2004, pages 624 – 638, Banff, Alberta, 2004. Springer.
[7] T. Zhang and R. Ando. Analysis of spectral kernel design based semi-supervised learning. In Y. Weiss,
B. Sch ¨olkopf, and J. Platt, editors, NIPS 18, pages 1601–1608. MIT Press, Cambridge, MA, 2006.
[8] M. Herbster, M. Pontil, and L. Wainer. Online learning over graphs. In ICML 2005, pages 305–312, New
York, NY, USA, 2005. ACM Press.
[9] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine
Learning, 37(3):277–296, 1999.
[10] D. Klein and M. Randi ´c. Resistance distance. Journal of Mathematical Chemistry, 12(1):81–95, 1993.
[11] J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady,
13:1224–1228, 1972.
[12] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337–404, 1950.
[13] D. Karger and C. Stein. A new approach to the minimum cut problem. JACM, 43(4):601–640, 1996.


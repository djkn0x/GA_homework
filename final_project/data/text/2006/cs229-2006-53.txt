Final Pro ject: On The Use and Abuse of Collaborative Tagging Data

Paul Heymann∗
(Dated: December 15, 2006)

Many applications could beneﬁt from labeled metadata of the sort which collaborative tagging
systems produce. I looked at the data produced by one of these systems, the social bookmarking
system del.icio.us, to determine whether or not the tags produced by these systems are useful
externally to other applications as opposed to internally as a means of navigation. I found that due
to a variety of factors, the tagging data created by users tends to be much less well distributed and
has much more easily predictable information than systems like those of Luis von Ahn which are
speciﬁcally engineered for the purpose of leveraging users to create metadata. However, I also found
that the sub jective, intrinsic information that users create may be more valuable than ob jective,
extrinsic information that they might be forced to create because of its diﬃculty of prediction.

I.

INTRODUCTION

Collaborative tagging systems have recently emerged
as a good way to leverage large numbers of users to
help organize very large, rapidly changing corpora which
would be diﬃcult to organize automatically, ranging from
user contributed audio, photos, or video on a single web
site to the web as a whole. Often, this works because
users are working in their own self interest as they mark
an ob ject with a particular tag, and when all of these
tags are aggregated together, the system can make as-
sumptions about ob jects based on the aggregate activi-
ties of hundreds of thousands or even millions of users.
Much recent work has looked at what sort of norms arise
from collaborative tagging communities, whether a co-
herent taxonomy or folksonomy can be built from user
contributed tags, and what can be inferred about ob jects
and about tags based on a collaborative tagging dataset.
Meanwhile, there are many cases in which large la-
beled datasets organized by thousands of users would be
immensely useful. One of the most obvious is search, an
area where advances in the fundamental methods seem to
have slowed, and emphasis now lies on how to gather in-
creasing amounts of information about a given web page,
either contributed by the page creator, a trusted source,
or by users. Luis von Ahn’s work [3, 6–8] has shown that
thousands of volunteers, if enticed by an “entertaining”
game, will be willing to label data for use in image search,
vision research, or logic based on a knowledge base. A
natural question is whether the data being generated by
collaborative tagging systems designed for user informa-
tion retrieval—in addition to Luis von Ahn’s explicitly
controlled systems—can also be used to learn, reason, or
retrieve information about a domain.
In this paper, I describe my experiments investigating
the extensibility to other tasks of one of the oldest types
of collaborative tagging systems, the social bookmarking
system del.icio.us [5]. I chose del.icio.us because it is one

∗Electronic address: heymann@cs.stanford.edu; Many thanks to
Hector Garcia-Molina (my advisor) and Daniel Ramage for their
advice during this pro ject.

of the largest collaborative tagging systems and because
the ob jects that it annotates, URLs, should be of imme-
diate use to the search problem. However, in the case of
many of my experiments, I believe that my observations
are likely to be true of any tagging system, for the most
part independent of its users and the ob jects that they
annotate. Ultimately, it seems that the most valuable
information in social bookmarking systems, and perhaps
in collaborative tagging systems in general, may be the
temporal and personal qualities of tags, rather than their
organizational qualities.

II. DATA DESCRIPTION

A. Preliminaries

In the course of this paper, I will use the following con-
ventions. When a user annotates an ob ject in a collabo-
rative tagging system (in my case, a social bookmarking
system), I will call this a post. A single post will consist
of one or more triples of the form < ti , uj , ok > where
ti is a tag, uj is the user making the post, and ok is
the object being annotated with the tag ti , in my case a
URL. When there is a triple containing tag ti and ob ject
ok , I say that tag ti annotates ob ject ok . Finally, I say
an ob ject ok is a positive example of a particular tag ti
if that tag annotates the ob ject, and a negative example
otherwise.

B. Del.icio.us Dataset

consists of 470, 681 unique URLs,
My dataset
9, 544, 252 posts, and 932, 390 unique tags.
I gathered
my data from the del.icio.us site over the course of sev-
eral weeks in October 2006.
I used a crawler which
started at the tag “web” and expanded outwards, treat-
ing del.icio.us as a graph where each user, tag, or URL
has outlinks to other users, tags, or URLs. My initial
del.icio.us crawler found 1, 371, 941 distinct URLs and
22, 206, 266 posts of which I was able to download and

s
e
c
n
a
t
s
n
I
 
f
o
 
r
e
b
m
u
N

 1e+06

 100000

 10000

 1000

 100

 10

 1

 1

Unique Tags

 10

 100

 1000

 10000  100000  1e+06

Tag Rank

 100000

 10000

 1000

 100

 10

 1

 1

Tags per URL
Posts per URL

 10

 100

 1000  10000  100000  1e+06  1e+07

URLs in Order of Popularity

(a)Distribution of Tags

(b)Tags/Posts Per URL

FIG. 1: The Distribution of Tags graph shows the number of
tags by tag rank—in other words, each point on the X axis
represents one tag, and the tags are sorted by the number
of instances where someone has annotated a URL with that
particular tag. The Tags Per Url and Posts Per URL graph
shows the number of tags and posts by URL rank—in other
words, each point on the X axis represents one URL, and the
URLs are sorted by the number of instances where someone
has annotated the URL with a tag or has posted the URL.

get page text for 470, 681.1
If there is any bias in my
results due to my methodology for gathering data, it is
most likely that it is biased towards tags or users clos-
est to the web tag. There are relatively few published
statistics on the size of del.icio.us—a recent article [1]
put the number of posts at 53 million and the number
of distinct URLs at 25 million, so my dataset is proba-
bly a substantial portion of the total posts, but probably
under-represents the number of distinct URLs substan-
tially (because these are harder to reach using my crawl-
ing method).
The number of triples per tag is distributed accord-
ing to the power law distribution shown in Figure 1(a).
There are 33, 031, 412 triples in the corpus, so each URL
is annotated on average with about 70 tags. Because of
the power law distribution, more than half of the tag in-
stances in the corpus are annotations of URLs with tags
that are among the top 130 tags. Likewise, the top 1000
tags on del.icio.us comprise over 75% of the triples in
my corpus (the 1000th most common tag, debugging, oc-
curs 3, 153 times and is 177 times less common than the
2nd most common tag, software ). Appendix A contains
a list of the top 130 tags in my dataset.
One diﬀerence between collaborative tagging systems
and systems like those created by von Ahn is that the
latter meticulously control the data that users contribute.
Speciﬁcally, von Ahn’s systems both make it diﬃcult to
add poor metadata (often because the metadata must
match another, random user who the current user cannot
communicate with) and predetermine which ob jects any
given user is allowed to annotate. By contrast, users of
del.icio.us and other collaborative tagging systems can

1 The unused pages were primarily either inaccessible or were not
“classical” web pages, being either RSS feeds, binaries for down-
load, or otherwise.

2

usually add whatever annotations they desire to whatever
ob jects they desire.
This turns out to be problematic, because for an equal
number of users, systems which are explicitly designed for
gathering metadata like von Ahn’s will produce orders of
magnitude more usable metadata about the ob jects in
the system. The reason for this is that there exist power
laws over the distributions of tags to URLs and URLs
to tags which are the direct result of users being able to
annotate what they desire, rather than what is unlabeled
in the system (see Figures 1(a) and 1(b)). The result is
that if we consider our unit of work to be one post by a
user, one half of the work in the system is concentrated
on the top 2588 URLs, three quarters on the top 6354
and nine tenths on the top 9905, and 99 percent on the
top 88830 URLs. Put another way, about 90 percent of
the eﬀort by users of del.icio.us goes in to labeling less
than 2.2 percent of the data.
Arguably, most of the 90 percent of eﬀort dedicated to
the top 2.2 percent of URLs is wasted eﬀort.2 Further-
more, as a result of the neglect of the less popular URLs,
we have much less information over which to aggregate
and reason about these URLs and determine which of
the metadata generated by users is “good,” and which
is “bad.”3 However, this is not a completely hopeless
situation—it may be possible to leverage agreement with
others on the most popular URLs to determine the trust
or level of conﬁdence we have in the quality of tags that
a given user uses to annotate less popular URLs.4

III. TAGGING BIAS

I also found that the act of tagging seems to lend it-
self to certain types of behaviors that make the tags less
valuable as organizational metadata. The primary ad-
vantage of tagging is that it can be done very quickly as
a free association based on the ob ject being tagged.5 As
a result, users can be bothered to tag and do not need
any training to do so, unlike determining where in a com-
plex hierarchy an ob ject belongs, or what keywords out
of a speciﬁc predetermined vocabulary might apply to an

2 In theory, each post could have diﬀerent tags for the same URL,
but in practice this is not the case.
3 I leave these notions intentionally vague, though “bad” could
mean anything from meaningless to noisy data to spam.
4 One reason I did not pursue this task is that while this might help
with determining quality of the contributions of non-adversarial
users (e.g., non-spammers), without a social network or other
mechanism, this metric would be relatively easy to exploit to
increase one’s trust according to the system.
5 Arguably, the features that collaborative tagging systems are
non-hierarchical and that their vocabularies are determined by
the evolving needs of their community are very important as well,
but I believe that the fundamental reason that these systems
work is because the ease of tag creation overcomes whatever free
rider eﬀect there might be.

Dom. % of Tag Tag % of Dom.
87.7%
5.0%
3.2%
81.5%
82.0%
3.1%
67.9%
1.6%
1.3%
88.7%

Domain
java.sun.com
onjava.com
javaworld.com
theserverside.com
today.java.net

TABLE I: Java tag example.

ob ject. However, I found that in practice this free asso-
ciation seems to often be biased by what the user has
most recently seen, and that some of the most obvious
terms that users have been primed to use as tags may be
determinable automatically.

A. Bias Due to Location

I found two somewhat surprising things regarding the
URL of a web page and the tags that are applied to it:

1. Users will tag an ob ject with the location of that
ob ject, for instance, when users bookmark an in-
teresting photograph on the web site Flickr, they
will often tag it ﬂickr.

2. Often, a popular site will be dedicated to a very
small set of closely related topics that all ﬁt under
a single tag. For instance, for the 16th most used
tag, video, two sites entirely dedicated to video con-
tent (youtube.com and video.google.com) make up
19.3% of the URLs which are annotated with the
tag, and 60-70% of the URLs at those domains are
tagged with video.

These two observations may be speciﬁc to social book-
marking systems, but I believe that the ﬁrst might apply
to any system which has “areas” or “topics” for ob jects,
and the second might apply to any system for which there
is an easy way to determine a set of tags which automat-
ically follow from facts about the ob ject (for instance, a
user who always posts photos of cats to a collaborative
tagging system for labeling photographs).
To get an idea for how prevalent these activities are,
I calculate the percentage accuracy I would have for the
given tag on positive or negative examples of that tag if I
just classiﬁed pages as positive if they are from a domain
with greater than τ1 percent of the domain annotated
with the tag, and negative otherwise. Figure 2 shows the
accuracy on positive and negative examples respectively
for diﬀerent values of τ1 while Table II shows the average
accuracies over the top 130 tags for diﬀerent values τ1 ,
and Table I shows an example of the phenomenon for the
tag java.
Overall, of the top 130 tags which make up more than
half of the triples in my dataset, I can recover between
5 and 20 percent of the positive examples with a false
positive rate of between 3 or 4 per 1000 and 1 or 2 per

s
e
l
p
m
a
x
E
 
e
v
i
t
i
s
o
P
 
n
o
 
y
c
a
r
u
c
c
A

s
e
l
p
m
a
x
E
 
e
v
i
t
a
g
e
N
 
n
o
 
y
c
a
r
u
c
c
A

 60

 50

 40

 30

 20

 10

 0

 0

 100

 99.8

 99.6

 99.4

 99.2

 99

3

domain accuracy thresh 1/3
domain accuracy thresh 1/2
domain accuracy thresh 2/3

 20

 40

 60

 80

 100

 120

Tag Rank

(a)Positive Accuracy

domain accuracy thresh 1/3
domain accuracy thresh 1/2
domain accuracy thresh 2/3

 0

 20

 40

 60

 80

 100

 120

Tag Rank

(b)Negative Accuracy

FIG. 2: Domains by Accuracy: These graphs show for the
ﬁrst 130 tags by rank what the accuracy on positive and neg-
ative examples is for a classiﬁer which classiﬁes only based
on domain, and chooses positive if the domain contains more
than threshold τ1 percent URLs annotated with the given tag.

τ1 = 0.33
τ1 = 0.5
τ1 = 0.66

Avg Accuracy (+) Avg Accuracy (-)
99.670
19.647
99.943
7.372
4.704
99.984

TABLE II: Average accuracy predicting by domain using dif-
ferent values τ1 with positive (+) and negative (-) examples.

10000 respectively. As a result, with some very conser-
vative methods, one could probably obviate the need for
a large proportion (though not the ma jority) of triples
that users create with a relatively low false positive rate.
One interesting aspect of behavior where users add
“obvious” tags that can be predicted by location is that
it actually may help aid in recall for the user, but it only
aids the community if the system cannot automatically

4

2
Mutual Information
χ
Tag is... Count % of Top 130 Count % of Top 130
Top 1
68
52.3%
80
61.5%
75.3%
98
68.4%
89
Top 2
77.7%
101
71.5%
93
Top 3
81.5%
106
76.9%
100
Top 5
83.1%
108
82.3%
107
Top 10
Top 20
116
89.2%
113
86.9%

Extrinsic vs Intrinsic
What or Who Is It About
What It Is
Who Owns It
Reﬁning Categories
Qualities/Characteristics
Self Reference
Task Organizing

Organizational vs Social
Future Retrieval
Contribution and Sharing
Attract Attention
Play and Competition
Self Presentation
Opinion Expression

2 and Mutual Information: This shows
TABLE III: Tags vs χ
the percentage of time and the raw counts for when a given
tag in the top 130 tags is one of the top n words correlated
with the tag by diﬀerent measures.

TABLE IV: Tag Types: This table lists the two sets of general
categories of tags, Golder and Huberman’s which are based
on what the tag describes, and Marlow et al.’s which identiﬁes
the purpose of the user when applying the tag.

detect pages at a given location and ﬁlter by them, and it
does not aid people who would use the dataset for other
purposes. While other researchers have noted that some
tags (for instance, “personal”) do not really add any in-
formation, I think that the use of location-based tags
is an odd and remarkably frequent case where users are
perhaps actually trying to explicitly help organize and
annotate the corpus, but are failing to provide valuable
non-obvious information.

B. Bias Due to Page Text

I found some preliminary data to suggest that users
may be primed to some extent by the page text when
they tag. While this is not a strong enough signal to
classify just based on the tag, or a few derivatives of
the tag, I did ﬁnd some surprising things when I applied
feature selection methods to the page text of tagged web
pages.
The two feature selection heuristics I used were mutual
information and χ2 which are both commonly used in
text classiﬁcation. I found that very often, even in what
would seem to be unlikely cases, the tag is one of the
most correlated terms in page text. Table III shows the
two sets of results for mutual information and χ2 . The
high rank of the tags in the list of correlated terms for
most tags means either or both of two things:

1. Users are primed with the page text when tagging,
so they often choose tags which are in (or appear
prominently in) the page text.

2. Any properly named category should have a high
correlation with its contents, and users just happen
to be good at choosing category names.

I believe that a little of both is probably occuring: we
should expect to see the words tags represent highly cor-
related with themselves, but I also found evidence of tags
like interesting which are highly correlated with them-
selves (according to χ2 ) that one would not expect to
ﬁnd as a highly correlated term.
If users do in fact tend to choose words from the page
text when tagging, then this may be as much of an ad-
vantage to some applications as it is a disadvantage to

others. While a user who chooses a tag that occurs in
the page text gives less information to someone trying to
learn the general categories that the page pertains to, it
may emphasize aspects of the page that the user thinks
are more important.

IV.

INTRINSIC AND EXTRINSIC TAGS

Various researchers have suggested diﬀerent ways of
grouping the fundamentally diﬀerent classes of tags in
tagging systems. Golder and Huberman [2] group tags
by whether they are extrinsic or intrinsic to the user,
and hence to what extent other users will agree that a
particular ob ject should be tagged with the tag. In con-
trast, Marlow et al. [4] group tags by whether their intent
is organizational or social, and focus on the intent of the
user when creating the tag, rather than whether the tag
has meaning independent of the user. Table IV shows
the two sets of types of tags.

A. Text Classiﬁcation with Tags

In order to explore the predictability of tags, and hence
the amount of information that diﬀerent tags were adding
beyond page text, I set up a series of text classiﬁcation
experiments on a per-tag basis.
I did a set of exper-
iments using support vector machines, and speciﬁcally
Thorsten Joachim’s SVMlight package, as binary classi-
ﬁers for tags. For each tag, I created two sets of pos-
itive and negative examples, one training and one test
set. The positive and negative examples in both sets, be-
cause I had many more negative examples than positive
examples, were artiﬁcially set to be in a ratio of one pos-
itive to two negative examples. I did not modify any of
the parameters of the SVMs I trained based on the test
set results, so I did not do a second level development
set/test set split.

B. Classifying Results

My goal in performing classiﬁcation was to see which
tags provided more information, or more diﬃcult to pre-
dict information, than others. For the purposes of this
paper, I avoid touching on the complex questions of what
distribution of data should be trained and tested on, and
what sort of false positive rate would be reasonable if one
wanted to predict tags at the scale of the web. Figures 3,
4, and 5 show Best K versus Precision graphs for several
example tags.
What I found was that in terms of classiﬁcation, Golder
and Huberman’s classiﬁcation of tags as extrinsic versus
intrinsic6 seems to correlate well with the relative diﬃ-
culty of tag prediction of a given tag. For the most part,
the relative precision of my binary classiﬁers for each tag
fell into three categories (by decreasing precision):

1. Extrinsic tags describing topics or sub ject mat-
ter with very well deﬁned vocabularies, like pro-
gramming languages (php, java, seo ) and ﬁelds or
specialties (seo, fonts, typography, recipes ), as well
as tags which act in this way by virtue of other
factors, like the common domain result discussed
above (google ).

2. Extrinsic tags describing topics or sub ject matter
which are inherently vague, like media, manage-
ment, and tutorials or intrinsic tags for which the
community has some common standards, for exam-
ple wishlist and funny (the latter of which is largely
synonymous to humor ).

3. Truly intrinsic tags which only have meaning in re-
lation to the individual who applied them, like in-
teresting, inspiration and cool.

Interestingly, the community seems to play a large part in
the extent to which a particular tag will be predictable,
both in general (e.g., if the community only chooses a
small subset of web sites for pages tagged with java ) and
speciﬁcally when dealing with intrinsic tags. Further-
more, this means that intrinsic tags which have meaning
to a particular community may be predictable, but may
not transfer meanings between communities.7

V. CONCLUSION

I studied a large subset of the data in one of the most
popular collaborative tagging systems, del.icio.us, to de-

6 And to a lesser extent Marlow et al.’s designations to the extent
to which social versus organizational mirrors intrinsic versus ex-
trinsic.
7 For example, one imagines that the relatively highly predictable
wishlist tag might not cross over from the largely technology
based community of del.icio.us to the general public.

5

termine if tagging data may be useful for other appli-
cations as well as internally to improve a collaborative
tagging system. What I found was neither that the data
is or is not useful, but rather that collaborative tagging
systems have some qualities which make them useful for
some tasks and not others.

Speciﬁcally, I found that collaborative tagging systems
are very ineﬃcient for ob jectively labeling data as a func-
tion of eﬀort from the users. 90% of the eﬀort expended
on tagging web pages in del.icio.us in my dataset is ded-
icated to the roughly two percent of pages which already
have 300 or more bookmarks. Much of the labeling that
is done includes tags which are either obvious from the lo-
cation of the page (ﬂickr or google ) or arguably might be
extracted from the page text.8 Compared especially to
systems designed for the purpose of using many users to
label data, like the ESP Game, del.icio.us appears to be
producing orders of magnitude less descriptive informa-
tion per URL than a system which through constraints
on user behavior could force the users explicitly to tag
particular web pages with certain types of descriptive in-
formation.

However, on the other hand, I found that collabora-
tive tagging systems may in fact produce a large amount
of data which while not the ob jective labeling of ob jects
with descriptive labels that cannot be predicted, may be
more useful than that data would be. Speciﬁcally, in-
trinsic rather than extrinsic tags may help give opinion
information not available elsewhere on a particular ob-
ject, and data that results from the continuous, temporal
nature of tagging systems may help provide a clue as to
what is important and what topics users currently view
as important.

Ultimately, it seems that the extrinsic, descriptive in-
formation in a collaborative tagging system is useful for
internal navigation within the system because most users
will probably be interested in the most popular items,
while the intrinsic information and the information gen-
erated implicitly by users may be the most useful for
external applications.

8 My work classifying tags using SVMs has left me all the more
uncertain about what a truly relevant measure of success is when
attempting to automatically predict the tags on pages. Speciﬁ-
cally, I wonder about the following issues in order of diﬃculty:
(a) What is a reasonable distribution of positive and negative
examples for a given tag to approximate the real distribution on
the web in order to produce proper precision, recall, and F1 num-
bers? (b) When predicting tags, do we care much more about
our predictions on some pages, like the most popular or authori-
tative pages, than others? (c) If we try to predict tags on the web
in general, will our results necessarily be ﬂavored by the nature
of the community from which we gathered our training data? Is
this the case for some tags and not others?

6

APPENDIX A: TOP 130 TAGS

These 130 tags sorted in order by number of instances
(system:unﬁled occurs 682,560 times, whereas architec-
ture occurs 40,833 times and all other tags are in be-
tween) comprise more than half of the tag instances in
my del.icio.us corpus: system:unﬁled, software, reference,
design, tools, programming, web, art, music, linux, news,
howto, free, blog, web2.0, video, photography, tutorial,
fun, ajax, webdesign, google, search, windows, javascript,
games, java, development, mac, ﬂash, css, cool, inter-
net, humor, security, opensource, shopping, technology,
science, books, tips, funny, freeware, business, tech, osx,

graphics, php, politics, photo, blogs, travel, media, apple,
computer, culture, photos, tutorials, education, hardware,
audio, diy, research, history, productivity, language, web-
dev, online, social, html, toread, tool, mp3, writing, im-
ages, download, inspiration, python, rss, community, wiki,
tv, photoshop, geek, ruby, ﬁrefox, movies, fonts, utilities,
safari export, interesting, daily, resources, maps, network,
game, comics, library, article, food, database, lifehacks,
ﬂickr, email, code, hacks, gtd, xml, il lustration, learn-
ing, magazine, networking, system:imported, health, en-
tertainment, book, work, useful, links, computers, radio,
unix, ipod, reviews, dictionary, visualization, microsoft,
imported, information, architecture.

[1] M. Arrington. More stats on del.icio.us,
this time
positive. http://www.techcrunch.com/2006/08/04/more-
stats-on-delicious-this-time-positive/, Aug. 2006.
[2] S. Golder and B. A. Huberman. Usage patterns of collab-
orative tagging systems. Journal of Information Science,
32(2):198–208, April 2006.
[3] L. D. Luis von Ahn. Labeling images with a computer
game. In M. T. Elizabeth Dykstra-Erickson, editor, Pro-
ceedings of ACM CHI 2004 Conference on Human Factors
in Computing Systems, pages 319–326. ACM Press, 2004.
[4] C. Marlow, M. Naaman, D. Boyd, and M. Davis. Ht06,
tagging paper, taxonomy, ﬂickr, academic article, to read.
In HYPERTEXT ’06: Proceedings of the seventeenth con-
ference on Hypertext and hypermedia, pages 31–40, New

York, NY, USA, 2006. ACM Press.
[5] J. Schachter. del.icio.us. http://del.icio.us/, Mar. 2006.
[6] L. von Ahn. Games with a purpose. Computer, 39(6):92–
94, 2006.
[7] L. von Ahn, M. Kedia, and M. Blum. Verbosity: a game
for collecting common-sense facts. In CHI ’06: Proceedings
of the SIGCHI conference on Human Factors in computing
systems, pages 75–78, New York, NY, USA, 2006. ACM
Press.
[8] L. von Ahn, R. Liu, and M. Blum. Peekaboom: a game
for locating ob jects in images. In CHI ’06: Proceedings of
the SIGCHI conference on Human Factors in computing
systems, pages 55–64, New York, NY, USA, 2006. ACM
Press.

7

recipes svm
recipes gold standard

php svm
php gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

fonts svm
fonts gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

 0

 500

 1000

 1500

 2000

 2500

 0

 100

 200

 300

 400

 500

 600

 700

 0

 100

 200

 300

 400

 500

 600

 700

 800

Best K

(a)PHP

Best K

(b)Fonts

Best K

(c)Recipes

FIG. 3: Three easy to predict tags.

management svm
management gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

media svm
media gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

wishlist svm
wishlist gold standard

 0

 200  400  600  800  1000  1200  1400  1600  1800

 0

 500  1000  1500  2000  2500  3000  3500  4000  4500

 0

 100  200  300  400  500  600  700  800  900

Best K

(a)Management

Best K

(b)Media

Best K

(c)Wishlist

FIG. 4: Three tags of intermediate diﬃcult to predict due to vagueness and other qualities.

cool svm
cool gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

inspiration svm
inspiration gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

interesting svm
interesting gold standard

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

n
o
i
s
i
c
e
r
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

 0

 1000

 2000

 3000

 4000

 5000

 6000

 7000

 0

 500

 1000

 1500

 2000

 2500

 3000

 3500

 0

 500  1000  1500  2000  2500  3000  3500  4000

Best K

(a)Cool

Best K

(b)Inspiration

Best K

(c)Interesting

FIG. 5: Three very diﬃcult to predict intrinsic tags without very much agreed upon community meaning.


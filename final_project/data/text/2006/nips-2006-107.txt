Real-time adaptive information-theoretic
optimization of neurophysiology experiments∗

Jeremy Lewi†
School of Bioengineering
Georgia Institute of Technology
jlewi@gatech.edu

Robert Butera
School of Electrical and Computer Engineering
Georgia Institute of Technology
rbutera@ece.gatech.edu

Liam Paninski ‡
Department of Statistics
Columbia University
liam@stat.columbia.edu

Abstract

Adaptively optimizing experiments can signiﬁcantly reduce the number of trials
needed to characterize neural responses using parametric statistical models. How-
ever, the potential for these methods has been limited to date by severe compu-
tational challenges: choosing the stimulus which will provide the most informa-
tion about the (typically high-dimensional) model parameters requires evaluating a
high-dimensional integration and optimization in near-real time. Here we present
a fast algorithm for choosing the optimal (most informative) stimulus based on a
Fisher approximation of the Shannon information and specialized numerical linear
algebra techniques. This algorithm requires only low-rank matrix manipulations
and a one-dimensional linesearch to choose the stimulus and is therefore efﬁcient
even for high-dimensional stimulus and parameter spaces; for example, we re-
quire just 15 milliseconds on a desktop computer to optimize a 100-dimensional
stimulus. Our algorithm therefore makes real-time adaptive experimental design
feasible. Simulation results show that model parameters can be estimated much
more efﬁciently using these adaptive techniques than by using random (nonadap-
tive) stimuli. Finally, we generalize the algorithm to efﬁciently handle both fast
adaptation due to spike-history effects and slow, non-systematic drifts in the model
parameters.

Maximizing the efﬁciency of data collection is important in any experimental setting. In neuro-
physiology experiments, minimizing the number of trials needed to characterize a neural system
is essential for maintaining the viability of a preparation and ensuring robust results. As a result,
various approaches have been developed to optimize neurophysiology experiments online in order
to choose the “best” stimuli given prior knowledge of the system and the observed history of the
cell’s responses. The “best” stimulus can be deﬁned a number of different ways depending on the
experimental objectives. One reasonable choice, if we are interested in ﬁnding a neuron’s “preferred
stimulus,” is the stimulus which maximizes the ﬁring rate of the neuron [1, 2, 3, 4]. Alternatively,
when investigating the coding properties of sensory cells it makes sense to deﬁne the optimal stim-
ulus in terms of the mutual information between the stimulus and response [5].
Here we take a system identiﬁcation approach: we deﬁne the optimal stimulus as the one which tells
us the most about how a neural system responds to its inputs [6, 7]. We consider neural systems in
† http://www.prism.gatech.edu/∼gtg120z
‡ http://www.stat.columbia.edu/∼liam

which the probability p(rt |{~xt , ~xt−1 , ..., ~xt−tk }, {rt−1 , . . . , rt−ta }) of the neural response rt given
the current and past stimuli {~xt , ~xt−1 , ..., ~xt−tk }, and the observed recent history of the neuron’s
activity, {rt−1 , . . . , rt−ta }, can be described by a model p(rt |{~xt}, {rt−1 }, ~θ), speciﬁed by a ﬁnite
vector of parameters ~θ . Since we estimate these parameters from experimental trials, we want to
choose our stimuli so as to minimize the number of trials needed to robustly estimate ~θ .
Two inconvenient facts make it difﬁcult to realize this goal in a computationally efﬁcient manner: 1)
model complexity — we typically need a large number of parameters to accurately model a system’s
response p(rt |{~xt}, {rt−1 }, ~θ); and 2) stimulus complexity — we are typically interested in neural
responses to stimuli ~xt which are themselves very high-dimensional (e.g., spatiotemporal movies if
we are dealing with visual neurons). In particular, it is computationally challenging to 1) update our
a posteriori beliefs about the model parameters p(~θ |{rt}, {~xt}) given new stimulus-response data,
and 2) ﬁnd the optimal stimulus quickly enough to be useful in an online experimental context.
In this work we present methods for solving these problems using generalized linear models (GLM)
for the input-output relationship p(rt |{~xt}, {rt−1 }, ~θ) and certain Gaussian approximations of the
posterior distribution of the model parameters. Our emphasis is on ﬁnding solutions which scale well
in high dimensions. We solve problem (1) by using efﬁcient rank-one update methods to update the
Gaussian approximation to the posterior, and problem (2) by a reduction to a highly tractable one-
dimensional optimization problem. Simulation results show that the resulting algorithm produces
a set of stimulus-response pairs which is much more informative than the set produced by random
sampling. Moreover, the algorithm is efﬁcient enough that it could feasibly run in real-time.
Neural systems are highly adaptive and more generally nonstatic. A robust approach to opti-
mal experimental design must be able to cope with changes in ~θ . We emphasize that the model
framework analyzed here can account for three key types of changes: stimulus adaptation, spike
rate adaptation, and random non-systematic changes. Adaptation which is completely stimu-
lus dependent can be accounted for by including enough stimulus history terms in the model
p(rt |{~xt , ..., ~xt−tk }, {rt−1 , ..., rt−ta }). Spike-rate adaptation effects, and more generally spike
history-dependent effects, are accounted for explicitly in the model (1) below. Finally, we con-
sider slow, non-systematic changes which could potentially be due to changes in the health, arousal,
or attentive state of the preparation.
Methods
We model a neuron as a point process whose conditional intensity function (instantaneous ﬁring
rate) is given as the output of a generalized linear model (GLM) [8, 9]. This model class has been
discussed extensively elsewhere; brieﬂy, this class is fairly natural from a physiological point of view
[10], with close connections to biophysical models such as the integrate-and-ﬁre cell [9], and has
(cid:18) X
(cid:19)
tkX
taX
been applied in a wide variety of experimental settings [11, 12, 13, 14]. The model is summarized
as:
j=1
i
l=1
In the above summation the ﬁlter coefﬁcients ki,t−l capture the dependence of the neuron’s instan-
taneous ﬁring rate λt on the ith component of the vector stimulus at time t − l, ~xt−l ; the model
therefore allows for spatiotemporal receptive ﬁelds. For convenience, we arrange all the stimulus
coefﬁcients in a vector, ~k , which allows for a uniform treatment of the spatial and temporal compo-
nents of the receptive ﬁeld. The coefﬁcients aj model the dependence on the observed recent activity
r at time t − j (these terms may reﬂect e.g. refractory effects, burstiness, ﬁring-rate adaptation, etc.,
depending on the value of the vector ~a [9]). For convenience we denote the unknown parameter
vector as ~θ = {~k ; ~a}.
The experimental objective is the estimation of the unknown ﬁlter coefﬁcients, ~θ , given knowledge
of the stimuli, ~xt , and the resulting responses rt . We chose the nonlinear stage of the GLM, the
link function f (), to be the exponential function for simplicity. This choice ensures that the log
likelihood of the observed data is a concave function of ~θ [9].
Representing and updating the posterior. As emphasized above, our ﬁrst key task is to efﬁciently
update the posterior distribution of ~θ after t trials, p(~θt |~xt , rt ), as new stimulus-response pairs are

λt = E (rt ) = f

(1)

ki,t−lxi,t−l +

aj rt−j

(a)

(b)

(c)

Figure 1: A) Plots of the estimated receptive ﬁeld for a simulated visual neuron. The neuron’s receptive ﬁeld
~θ has the Gabor structure shown in the last panel (spike history effects were set to zero for simplicity here,
~a = 0). The estimate of ~θ is taken as the mean of the posterior, ~µt . The images compare the accuracy of
the estimates using information maximizing stimuli and random stimuli. B) Plots of the posterior entropies
for ~θ in these two cases; note that the information-maximizing stimuli constrain the posterior of ~θ much more
effectively than do random stimuli. C) A plot of the timing of the three steps performed on each iteration as a
function of the dimensionality of ~θ . The timing for each step was well-ﬁt by a polynomial of degree 2 for the
diagonalization, posterior update and total time, and degree 1 for the line search. The times are an average over
many iterations. The error-bars for the total time indicate ±1 std.

observed. (We use ~xt and rt to abbreviate the sequences {~xt , . . . , ~x0 } and {rt , . . . , r0 }.) To solve
this problem, we approximate this posterior as a Gaussian; this approximation may be justiﬁed by the
fact that the posterior is the product of two smooth, log-concave terms, the GLM likelihood function
and the prior (which we assume to be Gaussian, for simplicity). Furthermore, the main theorem of
[7] indicates that a Gaussian approximation of the posterior will be asymptotically accurate.
We use a Laplace approximation to construct the Gaussian approximation of the posterior,
p(~θt |~xt , rt ): we set ~µt to the peak of the posterior (i.e. the maximum a posteriori (MAP) esti-
mate of ~θ), and the covariance matrix Ct to the negative inverse of the Hessian of the log posterior
at ~µt . In general, computing these terms directly requires O(td2 + d3 ) time (where d = dim(~θ);
the time-complexity increases with t because to compute the posterior we must form a product of t
likelihood terms, and the d3 term is due to the inverse of the Hessian matrix), which is unfortunately
too slow when t or d becomes large.

 trial 0info. max.random trial 100 trial 500 trial 2500 trial 5000q true  −101010002000300040005000-5000500100015002000IterationEntropy  randominfo. max.02004006000.0010.010.1DimensionalityTime(Seconds)  total timediagonalizationposterior update1d line Search(2)

Therefore we further approximate p(~θt−1 |~xt−1 , rt−1 ) as Gaussian; to see how this simpliﬁes mat-
(cid:17)
(cid:16){~xt ; rt−1 }T ~θ
ters, we use Bayes to write out the posterior:
log p(~θ |rt , ~xt ) = − 1
(~θ − ~µt−1 )T C −1
t−1 (~θ − ~µt−1 ) + − exp
2
(cid:17){~xt ; rt−1 }T
(cid:16) − exp({~xt ; rt−1 }T ~θ) + rt
+ rt{~xt ; rt−1 }T ~θ + const
d log p(~θ |rt , ~xt )
= −(~θ − ~µt−1 )T C −1
t−1 +
d~θ
d2 log p(~θ |rt , ~xt )
= −C −1
t−1 − exp({~xt ; rt−1 }T ~θ){~xt ; rt−1 }{~xt ; rt−1 }T
dθidθj
Now, to update µt we only need to ﬁnd the peak of a one-dimensional function (as opposed to
a d-dimensional function); this follows by noting that that the likelihood only varies along a single
direction, {~xt ; rt−1 }, as a function of ~θ . At the peak of the posterior, µt , the ﬁrst term in the gradient
must be parallel to {~xt ; rt−1 } because the gradient is zero. Since Ct−1 is non-singular, µt − ~µt−1
must be parallel to Ct−1 {~xt ; rt−1 }. Therefore we just need to solve a one dimensional problem now
to determine how much the mean changes in the direction Ct−1 {~xt ; rt−1 }; this requires only O(d2 )
time. Moreover, from the second derivative term above it is clear that computing Ct requires just
a rank-one matrix update of Ct−1 , which can be evaluated in O(d2 ) time via the Woodbury matrix
lemma. Thus this Gaussian approximation of p(~θt−1 |~xt−1 , rt−1 ) provides a large gain in efﬁciency;
our simulations (data not shown) showed that, despite this improved efﬁciency, the loss in accuracy
due to this approximation was minimal.
Deriving the (approximately) optimal stimulus. To simplify the derivation of our maximization
strategy, we start by considering models in which the ﬁring rate does not depend on past spiking, so
~θ = {~k}. To choose the optimal stimulus for trial t + 1, we want to maximize the conditional mutual
information

I (~θ ; rt+1 |~xt+1 , ~xt , rt ) = H (~θ |~xt , rt ) − H (~θ |~xt+1 , rt+1 )
(4)
with respect to the stimulus ~xt+1 . The ﬁrst term does not depend on ~xt+1 , so maximizing the
Z
X
information requires minimizing the conditional entropy H (~θ |~xt+1 , rt+1 ) =
−p(~θ |rt+1 , ~xt+1 ) log p(~θ |rt+1 , ~xt+1 )d~θ = Ert+1 |~xt+1 log det[Ct+1 ] + const.
p(rt+1 |~xt+1 )
rt+1
(5)
We do not average the entropy of p(~θ |rt+1 , ~xt+1 ) over ~xt+1 because we are only interested
in the conditional entropy for the particular ~xt+1 which will be presented next. The equality
above is due to our Gaussian approximation of p(~θ |~xt+1 , rt+1 ). Therefore, we need to minimize
Ert+1 |~xt+1 log det[Ct+1 ] with respect to ~xt+1 . Since we set Ct+1 to be the negative inverse Hes-
Ct+1 = (cid:0)C −1
t + Jobs (rt+1 , ~xt+1 )(cid:1)−1
sian of the log-posterior, we have:
Jobs is the observed Fisher information.
Jobs (rt+1 , ~xt+1 ) = −∂ 2 log p(rt+1 |ε = ~xt
~θ)/∂ ε2 ~xt+1~xt
t+1
t+1
(cid:16)
(cid:17)
Here we use the fact that for the GLM, the likelihood depends only on the dot product, ε = ~xt
t+1
We can use the Woodbury lemma to evaluate the inverse:
I + D(rt+1 , ε)(1 − D(rt+1 , ε)~xt
t+1Ct~xt+1 )−1~xt+1~xt
Ct+1 = Ct
t+1Ct
where D(rt+1 , ε) = ∂ 2 log p(rt+1 |ε)/∂ ε2 . Using some basic matrix identities,
log det[Ct+1 ] = log det[Ct ] − log(1 − D(rt+1 , ε)~xt
t+1Ct~xt+1 )
(9)
= log det[Ct ] + D(rt+1 , ε)~xt
t+1Ct~xt+1 + o(D(rt+1 , ε)~xt
t+1Ct~xt+1 )
(10)
Ignoring the higher order terms, we need to minimize Ert+1 |~xt+1 D(rt+1 , ε)~xt
t+1Ct~xt+1 . In our case,
with f (~θt~xt+1 ) = exp(~θt~xt+1 ), we can use the moment-generating function of the multivariate

(6)

,

(3)

(7)

~θ .

(8)

(a)

(b)

(c)

Figure 2: A comparison of parameter estimates using information-maximizing versus random stimuli for a
model neuron whose conditional intensity depends on both the stimulus and the spike history. The images in
the top row of A and B show the MAP estimate of ~θ after each trial as a row in the image. Intensity indicates the
value of the coefﬁcients. The true value of ~θ is shown in the second row of images. A) The estimated stimulus
coefﬁcients, ~k . B) The estimated spike history coefﬁcients, ~a. C) The ﬁnal estimates of the parameters after
800 trials: dashed black line shows true values, dark gray is estimate using information maximizing stimuli,
and light gray is estimate using random stimuli. Using our algorithm improved the estimates of ~k and ~a.

(11)

(12)

(13)

Gaussian p(~θ |~xt , rt ) to evaluate this expectation. After some algebra, we ﬁnd that to maximize
I (~θ ; rt+1 |~xt+1 , ~xt , rt ), we need to maximize
F (~xt+1 ) = exp(~xT
t+1 ~µt ) exp(

1
t+1Ct~xt+1 )~xT
2 ~xT
t+1Ct~xt+1 .
Computing the optimal stimulus. For the GLM the most informative stimulus is undeﬁned, since
increasing the stimulus power ||~xt+1 ||2 increases the informativeness of any putatively “optimal”
stimulus. To obtain a well-posed problem, we optimize the stimulus under the usual power con-
straint ||~xt+1 ||2 ≤ e < ∞. We maximize Eqn. 11 under this constraint using Lagrange multipliers
and an eigendecomposition to reduce our original d-dimensional optimization problem to a one-
i ) X
X
F (~xt+1 ) = exp(X
dimensional problem. Expressing Eqn. 11 in terms of the eigenvectors of Ct yields:
1
ui yi )h(X
= g(X
ui yi +
ci y2
2
i
i
i
i )
ci y2
i
i
where ui and yi represent the projection of ~µt and ~xt+1 onto the ith eigenvector and ci is the cor-
by breaking the problem into an inner and outer problem by ﬁxing the value of P
responding eigenvalue. To simplify notation we also introduce the functions g() and h() which are
imizing h() subject to that constraint. A single line search over all possible values of P
monotonically strictly increasing functions implicitly deﬁned by Eqn. 12. We maximize F (~xt+1 )
i ui yi and max-
i(cid:21)
(cid:20)
i ui yi will
g(b) · h
h(X
then ﬁnd the global maximum of F (.). This approach is summarized by the equation:
i )
max
ci y2
~y :||~y ||2=e,~yt ~u=b
X
i
Since h() is increasing, to solve the inner problem we only need to solve:
i
This last expression is a quadratic function with quadratic and linear constraints and we can solve
it using the Lagrange method for constrained optimization. The result is an explicit system of

max
~y :||~y ||2=e,~yt ~u=b

max
~y :||~y ||2=e

F (~y) = max
b

ci y2
i

ci y2
i

(14)

info. max.Trial1400800i150i.i.d  -202i150info. max.i110i.i.d  i110−10−7−10−4−10−11100-202ki  110-0.0500.05iai  (a)

(b)

(c)

Figure 3: Estimating the receptive ﬁeld when ~θ is not constant. A) The posterior means ~µt and true ~θt plotted
after each trial. ~θ was 100 dimensional, with its components following a Gabor function. To simulate nonsys-
tematic changes in the response function, the center of the Gabor function was moved according to a random
walk in between trials. We modeled the changes in ~θ as a random walk with a white covariance matrix, Q, with
variance .01. In addition to the results for random and information-maximizing stimuli, we also show the ~µt
given stimuli chosen to maximize the information under the (mistaken) assumption that ~θ was constant. Each
row of the images plots ~θ using intensity to indicate the value of the different components. B) Details of the
posterior means ~µt on selected trials. C) Plots of the posterior entropies as a function of trial number; once
again, we see that information-maximizing stimuli constrain the posterior of ~θt more effectively.

equations for the optimal yi as a function of the Lagrange multiplier λ1 .
ui
yi (λ1 ) = e
||~y ||2
2(ci − λ1 )
Thus to ﬁnd the global optimum we simply vary λ1 (this is equivalent to performing a search over
b), and compute the corresponding ~y(λ1 ). For each value of λ1 we compute F (~y(λ1 )) and choose
the stimulus ~y(λ1 ) which maximizes F (). It is possible to show (details omitted) that the maximum
of F () must occur on the interval λ1 ≥ c0 , where c0 is the largest eigenvalue. This restriction on the
optimal λ1 makes the implementation of the linesearch signiﬁcantly faster and more stable.
To summarize, updating the posterior and ﬁnding the optimal stimulus requires three steps: 1) a rank-
one matrix update and one-dimensional search to compute µt and Ct ; 2) an eigendecomposition of

(15)

true qqi1100 info. max.qi1100info. max.  no diffusionqi  1100 randomqitrial11001400800−0.6−0.4−0.200.20.40.60.81−101Trial 0 qi  −101Trial 200 qi20406080100-101Trial 400 qiirandominfo. max.q true0200400600800150200250IterationEntropy  randominfo. max.Ct ; 3) a one-dimensional search over λ1 ≥ c0 to compute the optimal stimulus. The most expensive
step here is the eigendecomposition of Ct ; in principle this step is O(d3 ), while the other steps, as
discussed above, are O(d2 ). Here our Gaussian approximation of p(~θt−1 |~xt−1 , rt−1 ) is once again
quite useful: recall that in this setting Ct is just a rank-one modiﬁcation of Ct−1 , and there exist
efﬁcient algorithms for rank-one eigendecomposition updates [15]. While the worst-case running
time of this rank-one modiﬁcation of the eigendecomposition is still O(d3 ), we found the average
running time in our case to be O(d2 ) (Fig. 1(c)), due to deﬂation which reduces the cost of matrix
multiplications associated with ﬁnding the eigenvectors of repeated eigenvalues. Therefore the total
time complexity of our algorithm is empirically O(d2 ) on average.
Spike history terms. The preceding derivation ignored the spike-history components of the GLM
model; that is, we ﬁxed ~a = 0 in equation (1). Incorporating spike history terms only affects the
optimization step of our algorithm; updating the posterior of ~θ = {~k ; ~a} proceeds exactly as before.
The derivation of the optimization strategy proceeds in a similar fashion and leads to an analogous
optimization strategy, albeit with a few slight differences in detail which we omit due to space
constraints. The main difference is that instead of maximizing the quadratic expression in Eqn. 14
to ﬁnd the maximum of h(), we need to maximize a quadratic expression which includes a linear
term due to the correlation between the stimulus coefﬁcients, ~k , and the spike history coefﬁcients,~a.
The results of our simulations with spike history terms are shown in Fig. 2.
Dynamic ~θ . In addition to fast changes due to adaptation and spike-history effects, animal prepara-
tions often change slowly and nonsystematically over the course of an experiment [16]. We model
these effects by letting ~θ experience diffusion:

~θt+1 = ~θt + wt
(16)
Here wt is a normally distributed random variable with mean zero and known covariance matrix Q.
This means that p(~θt+1 |~xt , rt ) is Gaussian with mean ~µt and covariance Ct + Q. To update the
posterior and choose the optimal stimulus, we use the same procedure as described above1 .

Results

Our ﬁrst simulation considered the use of our algorithm for learning the receptive ﬁeld of a visually
sensitive neuron. We took the neuron’s receptive ﬁeld to be a Gabor function, as a proxy model of a
V1 simple cell. We generated synthetic responses by sampling Eqn. 1 with ~θ set to a 25x33 Gabor
function. We used this synthetic data to compare how well ~θ could be estimated using information
maximizing stimuli compared to using random stimuli. The stimuli were 2-d images which were
rasterized in order to express ~x as a vector. The plots of the posterior means ~µt in Fig. 1 (recall these
are equivalent to the MAP estimate of ~θ) show that the information maximizing strategy converges
an order of magnitude more rapidly to the true ~θ . These results are supported by the conclusion
of [7] that the information maximization strategy is asymptotically never worse than using random
stimuli and is in general more efﬁcient.
The running time for each step of the algorithm as a function of the dimensionality of ~θ is plotted
in Fig. 1(c). These results were obtained on a machine with a dual core Intel 2.80GHz XEON
processor running Matlab. The solid lines indicate ﬁtted polynomials of degree 1 for the 1d line
search and degree 2 for the remaining curves; the total running time for each trial scaled as O(d2 ),
as predicted. When ~θ was less than 200 dimensions, the total running time was roughly 50 ms (and
for dim(~θ) ≈ 100, the runtime was close to 15 ms), well within the range of tolerable latencies for
many experiments.
In Fig. 2 we apply our algorithm to characterize the receptive ﬁeld of a neuron whose response
depends on its past spiking. Here, the stimulus coefﬁcients ~k were chosen to follow a sine-wave;
1The one difference is that the covariance matrix of p(~θt+1 |~xt+1 , rt+1 ) is in general no longer just a rank-
one modiﬁcation of the covariance matrix of p(~θt |~xt , rt ); thus, we cannot use the rank-one update to compute
the eigendecomposition. However, it is often reasonable to take Q to be white, Q = cI ; in this case the
eigenvectors of Ct + Q are those of Ct and the eigenvalues are ci + c where ci is the ith eigenvalue of Ct ; thus
in this case, our methods may be applied without modiﬁcation.

the spike history coefﬁcients ~a were inhibitory and followed an exponential function. When choos-
ing stimuli we updated the posterior for the full ~θ = {~k ; ~a} simultaneously and maximized the
information about both the stimulus coefﬁcients and the spike history coefﬁcients. The informa-
tion maximizing strategy outperformed random sampling for estimating both the spike history and
stimulus coefﬁcients.
Our ﬁnal set of results, Fig. 3, considers a neuron whose receptive ﬁeld drifts non-systematically
with time. We take the receptive ﬁeld to be a Gabor function whose center moves according to a
random walk (we have in mind a slow random drift of eye position during a visual experiment). The
results demonstrate the feasibility of the information-maximization strategy in the presence of non-
stationary response properties ~θ , and emphasize the superiority of adaptive methods in this context.

Conclusion

We have developed an efﬁcient implementation of an algorithm for online optimization of neuro-
physiology experiments based on information-theoretic criterion. Reasonable approximations based
on a GLM framework allow the algorithm to run in near-real time even for high dimensional pa-
rameter and stimulus spaces, and in the presence of spike-rate adaptation and time-varying neural
response properties. Despite these approximations the algorithm consistently provides signiﬁcant
improvements over random sampling; indeed, the differences in efﬁciency are large enough that
the information-optimization strategy may permit robust system identiﬁcation in cases where it is
simply not otherwise feasible to estimate the neuron’s parameters using random stimuli. Thus, in
a sense, the proposed stimulus-optimization technique signiﬁcantly extends the reach and power of
classical neurophysiology methods.

Acknowledgments

JL is supported by the Computational Science Graduate Fellowship Program administered by the
DOE under contract DE-FG02-97ER25308 and by the NSF IGERT Program in Hybrid Neural Mi-
crosystems at Georgia Tech via grant number DGE-0333411. LP is supported by grant EY018003
from the NEI and by a Gatsby Foundation Pilot Grant. We thank P. Latham for helpful conversa-
tions.

References
[1] I. Nelken, et al., Hearing Research 72, 237 (1994).
[2] P. Foldiak, Neurocomputing 38–40, 1217 (2001).
[3] K. Zhang, et al., Proceedings (Computational and Systems Neuroscience Meeting, 2004).
[4] R. C. deCharms, et al., Science 280, 1439 (1998).
[5] C. Machens, et al., Neuron 47, 447 (2005).
[6] A. Watson, et al., Perception and Psychophysics 33, 113 (1983).
[7] L. Paninski, Neural Computation 17, 1480 (2005).
[8] P. McCullagh, et al., Generalized linear models (Chapman and Hall, London, 1989).
[9] L. Paninski, Network: Computation in Neural Systems 15, 243 (2004).
[10] E. Simoncelli, et al., The Cognitive Neurosciences, M. Gazzaniga, ed. (MIT Press, 2004), third
edn.
[11] P. Dayan, et al., Theoretical Neuroscience (MIT Press, 2001).
[12] E. Chichilnisky, Network: Computation in Neural Systems 12, 199 (2001).
[13] F. Theunissen, et al., Network: Computation in Neural Systems 12, 289 (2001).
[14] L. Paninski, et al., Journal of Neuroscience 24, 8551 (2004).
[15] M. Gu, et al., SIAM Journal on Matrix Analysis and Applications 15, 1266 (1994).
[16] N. A. Lesica, et al., IEEE Trans. On Neural Systems And Rehabilitation Engineering 13, 194
(2005).


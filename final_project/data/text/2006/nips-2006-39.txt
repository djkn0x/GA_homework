Kernels on Structured Objects Through Nested
Histograms

Marco Cuturi
Institute of Statistical Mathematics
Minami-azabu 4-6-7, Minato ku,
Tokyo, Japan.

Kenji Fukumizu
Institute of Statistical Mathematics
Minami-azabu 4-6-7, Minato ku,
Tokyo, Japan.

Abstract

We propose a family of kernels for structured objects which is based on the bag-of-
components paradigm. However, rather than decomposing each complex object
into the single histogram of its components, we use for each object a family of
nested histograms, where each histogram in this hierarchy describes the object
seen from an increasingly granular perspective. We use this hierarchy of his-
tograms to de ﬁne elementary kernels which can detect coarse and ﬁne similarities
between the objects. We compute through an efﬁcient averagi ng trick a mixture
of such speciﬁc kernels, to propose a ﬁnal kernel value which weights efﬁciently
local and global matches. We propose experimental results on an image retrieval
experiment which show that this mixture is an effective template procedure to be
used with kernels on histograms.

1 Introduction

Kernel methods have shown to be competitive with other techniques in classiﬁcation or regression
tasks where the input data lie in a vector space. Arguably, this success rests on two factors: ﬁrst,
the good ability of kernel algorithms, such as the support vector machine, to generalize and pro-
vide a sparse formulation for the underlying learning problem; second, the capacity of nonlinear
kernels, such as the polynomial and gaussian kernels, to quantify meaningful similarities between
vectors, notably non-linear correlations between their components. Using kernel machines with
non-vectorial data (e.g., in bioinformatics, image and text analysis or signal processing) requires
more arbitrary choices, both to represent the objects in a malleable form, and to choose suitable
kernels on these representations. The challenge of using kernel methods on real-world data has thus
recently fostered many proposals for kernels on complex objects, notably strings, trees, images or
graphs to cite a few.

In common practice, most of these objects can be regarded as structured aggregates of smaller com-
ponents, and the coarsest approach to study such aggregates is to consider them directly as bags
of components.
In the ﬁeld of kernel methods, such a represen tation has not only been widely
adopted (Haussler, 1999; Joachims, 2002; Sch ¨olkopf et al., 2004), but it has also spurred the pro-
posal of kernels better suited to the geometry of the underlying histograms (Kondor & Jebara, 2003;
Lafferty & Lebanon, 2005; Hein & Bousquet, 2005; Cuturi et al., 2005). However, one of the draw-
backs of the bag-of-components representation is that it implicitly assumes that each component
sampled in the object has been generated independently from an identical distribution. While this
viewpoint may translate into adequate properties for some learning tasks, such as translation or ro-
tation invariance when using histograms of colors to manipulate images (Chapelle et al., 1999), it
may however appear too restrictive when such a strong invariance may just be too coarse to be of
practical use.

A possible way to cope with this limitation is to expand artiﬁ cially the size of the components’
space, either by considering families of larger components to take into account more contextual in-
formation, or by considering histograms which index both components and their possible location
in the object (R ¨atsch & Sonnenburg, 2004). As one would expect, these histograms are usually
sparse and need to be regularized using ad-hoc rules and prior knowledge (Leslie et al., 2003) be-
fore being directly compared using kernels on histograms. For sequential data, other state-of-the-art
methods compute an optimal alignment between the sequences based on elementary operations such
as substitutions, deletions and insertions of components. Such alignment scores may yield positive
de ﬁnite (p.d.) kernels if particular care is taken to adapt t hem (Vert et al., 2004) and have shown very
competitive performances. However, their computational cost can be prohibitive when dealing with
large datasets, and can only be applied to sequential data. Following these contributions, we propose

t1

t2

t2.1

t2.2

t1

t2.1

t2.2

t2

Figure 1: From the bag of components representation to a set of nested bags, using a set of labels.

in this paper new families of kernels which can be easily tuned to detect both coarse and ﬁne simi-
larities between the objects, in a range spanned from kernels which only consider coarse histograms
to kernels which only detect strict local matches. To size such types of similarities between two
objects, we elaborate on the elementary bag-of-components perspective to consider instead families
of nested histograms (indexed by a set of hierarchical labels to be de ﬁned) to describe each object.
In this framework, the root label corresponds to the global representation introduced before, while
longer labels represent a speciﬁc condition under which the components have been sampled. We
then de ﬁne kernels that take into account mixtures of simila rities, spanning from detailed resolu-
tions which only compare the smallest bags to the coarsest one. This trade-off between ﬁne and
coarse perspectives sets an averaging framework to de ﬁne ke rnels, which we introduce formally in
Section 2. This theoretical framework would not be tractable without an efﬁcient factorization de-
tailed in Section 3 which yields computations which grow linearly in time and space with respect
to the number of labels to evaluate the value of the kernel. We then provide experimental results in
Section 4 on an image retrieval task which shows that the methodology improves the performance
of kernel based state-of-the art techniques in this ﬁeld wit h a low extra computational cost.

2 Kernels Deﬁned through Hierarchies of Histograms

In the kernel literature, structured objects are usually represented as histograms of components, e.g.,
images as histograms of colors and/or features, texts as bags of words and sequences as histograms
of letters or n-grams. The obvious drawback of this representation is that it usually loses all the
contextual information which may be useful to characterize each sampled component in the original
object. One may instead create families of histograms, indexed by speciﬁc sampling conditions:

• In image analysis, create color or feature histograms following a prior partition of the image
into prede ﬁned patches, as in (Grauman & Darrell, 2005). Ano ther possibility would be to
de ﬁne families of histograms, all for the same image, which w ould consider increasingly
granular discretizations of the color space.
• In sequence analysis, extract local histograms which may correspond to prede ﬁned regions
of the original sequence, as in (Matsuda et al., 2005). A different option would be to asso-
ciate to each histogram a context of arbitrary length, e.g. by considering the 26 histogram
of letters sampled just after the letters {A, B , · · · , Z }, or the 26 × 26 histograms of letters
after contexts {AA, AB , · · · , Z Z }.

• In text analysis, use histograms of words found after grammatical categories of increasing
complexity, such as verbs, nouns, articles or adverbs.
• For synchronous time series (e.g. ﬁnancial time series or ge ne expression pro ﬁles), de ﬁne
a reference series (e.g. an index or a speciﬁc gene) and decom pose each of the subsequent
series into histograms of values conditioned to the value of the reference series.
We write L for an arbitrary index set to label such speciﬁc histograms. Structured objects are thus
represented as a family µ of ML (X ) def= (M b
+ (X ))L , that is µ = {µt}t∈L where for each t ∈ L, µt
is a bounded measure of M b
+(X ). We write |µ| for Pt∈L |µt |.
2.1 Local Similarities Between Measures

To compare two objects under the light of any sampling condition t, that is comparing their respec-
tive decompositions as measures µt and µ0
t , we make use of an arbitrary p.d. kernel k on M b
+ (X )
to which we will refer as the base kernel throughout the paper. For interpretation purposes only, we
will assume in the following sections that k is an in ﬁnitely divisible kernel which can be written
as k = e− 1
λ > 0, where ψ is a negative de ﬁnite (Berg et al., 1984) kernel on M b
+ (X ), or
λ ψ ,
equivalently −ψ is a conditionally p.d. kernel. Note also that k has to be p.d. not only on probabil-
ity measures, but on any bounded measure. For two elements µ, µ0 of ML(X ) and a given element
t ∈ L, the kernel
kt (µ, µ0 ) def= k(µt , µ0
t )
quantiﬁes the similarity of µ and µ0 by measuring how similarly their components were observed
with respect to label t. For two different labels s and t of L, ks and kt can be associated through
polynomial combinations with positive coefﬁcients to resu lt in new kernels, notably their sum ks +kt
or their product ks kt . This is particularly adequate if some complementarity is assumed between s
and t, so that their combination can provide new insights for a given learning task. If on the contrary
these labels are assumed to be similar, then they can be regarded as a grouped label {s} ∪ {t} and
result in the kernel
k{s}∪{t} (µ, µ0 ) def= k(µs + µt , µ0
s + µ0
t ),
which will measure the similarity of m and m0 under both s or t labels. Let us give an intuition
for this de ﬁnition by considering two texts A, B built up with words from a dictionary D. As an
+ (D), one may consider for instance
alternative to the general histograms of words θA and θB of M b
may , the respective histograms of words that follow the words can and may in
may and θB
can , θB
can , θA
θA
texts A and B respectively. If one considers that can and may are different words, then the following
kernel quantiﬁes the similarity of A and B taking advantage of this difference:
can ) × k(θA
k{can},{may} (A, B ) = k(θA
can , θB
may , θB
may ).
If on the contrary one decides that can and may are equivalent, an adequate kernel would ﬁrst
merge the histograms, and then compare them:
k{can,may} (A, B ) = k(θA
can + θB
may , θB
can + θA
may ).
The previous formula can be naturally extended to de ﬁne kern els indexed on a set T ⊂ L of grouped
labels, through
def= Xt ∈ T
def= Xt ∈ T
kT (µ, µ0 ) def= k (µT , µ0
T ) , where µT

µt and µ0
T

µ0
t .

2.2 Resolution Speciﬁc Kernels

Having de ﬁned a family of kernels {kT , T ⊂ L} which can detect conditional similarities between
two elements of ML (X ) given a subset T of L, we de ﬁne in this section different ways to combine
them to obtain a kernel which can take into account all of their histograms. Let P be a ﬁnite partition
of L, that is a ﬁnite family P = (T1 , ..., Tn ) of sets of L, such that Ti ∩ Tj = ∅ if 1 ≤ i < j ≤ n
and Sn
i=1 Ti = L. We write P (L) for the set of all partitions of L. Consider now the kernel de ﬁned
by a partition P as
n
Yi=1

kP (µ, µ0 ) def=

kTi (µ, µ0 ).

(1)

The kernel kP quantiﬁes the similarity between two objects by detecting t heir joint similarity under
all possible labels of L, assuming a priori that certain labels can be grouped together, following the
subsets Ti enumerated in the partition P . Note that there is some arbitrary in this de ﬁnition since
a simple multiplication of base kernels kTi is used to de ﬁne kP , rather than any other polynomial
combination. We follow in that sense the convolution kernels (Haussler, 1999) approach, and indeed,
for each partition P , kP can be regarded as a convolution kernel. More precisely, the multiplicative
structure of Equation (1) quantiﬁes how similar two objects are given a partition P , in a way that
imposes for the objects to be similar according to all subsets Ti . If the base kernel k can be written
as k = e− 1
λ ψ , where ψ is a negative de ﬁnite kernel, then kP can be expressed as the exponential of
minus

ψP (µ, µ0 ) def=

n
n
Xi=1
Xi=1
a quantity which penalizes local differences between the decompositions of µ and µ0 over L, as
t ) is considered.
opposed to the coarsest approach where P = {L} and only ψ(Pt µt , Pt µ0

ψTi (µ, µ0 ) =

ψ(µTi , µ0
Ti ),

Figure 2: A useful set of labels L for images which would focus on pixel localization can be rep-
resented by a grid, such as the 8 × 8 one represented above. In this case P3 corresponds to the 43
windows presented in the left image, P2 to the 16 larger squares obtained when grouping 4 small
windows, P1 to the image divided into 4 equal parts and P0 is simply the whole image. Any partition
P of the image which complies with the hierarchy P 3
0 in the example above, can in turn be used to
represent an image as a family of sub-probability measures, which reduces in the case of two-color
images to binary histograms as illustrated in the right-most image. For two images, these respective
histograms can be directly compared through the kernel kP .

As illustrated in Figure 2, where images are summarized through histograms indexed by patches, a
partition of L re ﬂects a given belief on how patches may or may not be associa ted or split to focus
on local dissimilarities. Hence, all partitions contained in the set P (L) of all possible partitions1
are not likely to be equally meaningful given that some labels may a natural form of grouping. If
the index is built to highlight differences in locations, one would naturally favor mergers between
neighboring indexes. If one uses a Markovian analysis, that is consider histograms of components
conditioned by contexts, a natural way to group contexts would be to group them according to their
semantic or grammatical content for text analysis or according to their sufﬁx for sequence analysis.

Such meaningful partitions can be intuitively obtained when a hierarchical structure which groups
elements of L together is known a priori. A hierarchy on L, such as the triadic hierarchy shown in
Figure 3, is a family
(Pd )D
d=0 = {P0 = {L}, .., PD = {{t}, t ∈ L}}
of partitions of L. To provide a hierarchical information, the family (Pd )D
d=0 is such that any subset

present in a partition Pd is strictly included in a (unique by de ﬁnition of a partition ) subset from
the coarser partition Pd−1 . This is equivalent to stating that each subset T in a partition Pd is
divided in Pd+1 as a partition of T which is not T itself. We write s(T ) for this partition (e.g.,
in Figure 3, s(1) = {11 , · · · , 19}) and name its elements the siblings of T . Consider now the
subset PD ⊂ P (L) of all partitions of L obtained by using only sets contained in the collection
def= {P ∈ P (L) s.t. ∀ T ∈ P, T ∈ P D
def= SD
0 }. The set PD contains both the
d=0 Pd , namely PD
P D
0
coarsest and the ﬁnest resolutions, respectively P0 and PD , but also all variable resolutions for sets
enumerated in P D
0 , as can be seen for instance in the third image of Figure 2.
1P (L) is quite a big space, since if L is a ﬁnite set of cardinal r , the cardinal of the set of partitions is known
r
er ln r .
as the Bell Number of order r with Br = 1
∞
u
∼
e
r→∞

u=1

u!

1

4

7

2

5

8

P1

11

19

73

3

6

9

61

99

P2

0

P0

Figure 3: A hierarchy generated by two successive triadic partitions.

2.3 Averaging Resolution Speciﬁc Kernels

Each partition P contained in PD provides a resolution to compare two objects, which generates
a large family of kernels kP when P spans PD . Some partitions are likely to be better suited for
certain tasks, which may call for an efﬁcient estimation sch eme to select an optimal partition for
a given task. This would be similar in spirit to estimating a maximum a posteriori model for the
data and use it consequently to compare the objects. We take in this section a different direction
which has a more Bayesian ﬂavor by considering an averaging o f such kernels based on a prior
on the set of partitions. In practice, this averaging favours objects which share similarities under a
large collection of resolutions, and may also be interpreted as a Bayesian averaging of convolution
kernels (Haussler, 1999).

De ﬁnition 1 Let L be an index set endowed with a hierarchy (Pd )D
d=0 , π be a prior measure on the
corresponding set of partitions PD and k a base kernel on M b
+ (X ). The averaged kernel
+(X ) × M b
kπ on ML (X ) × ML (X ) is de ﬁned as
kπ (µ, µ0 ) = XP ∈ PD
As can be observed in Equation (2), the kernel automatically detects in the range of all partitions
the ones which provide a good match between the compared objects, to increase subsequently the
resulting similarity score. Also note that in an image-analysis context, the pyramid-matching ker-
nel proposed in (Grauman & Darrell, 2005) only considers the original partitions of the hierarchy
d=0 , while Equation (2) considers all possible partitions of PD . This can be carried out with
(Pd )D
little cost if an adequate set of priors π is selected as seen below.

π(P ) kP (µ, µ0 ).

(2)

3 Kernel Computation

d=0 and priors π for which the computation of kπ is both
We provide in this section hierarchies (Pd )D
meaningful and tractable, yielding namely a computational time to calculate kπ which is loosely
upperbounded by D × card L × c(k) where c(k) is the time required to compute the base kernel.
3.1 Partitions Generated by Branching Processes

All partitions P of PD can be generated through the following rule, starting from the initial root
partition P := P0 = {L}. For each set T of P :
1. either leave the set as it is in P with probability 1 − εT ,
2. either replace it by its siblings in s(T ) with probability εT , and reapply this rule to each
sibling unless they belong to the ﬁnest partition PD .
The resulting prior for PD depends on the overall coarseness of the considered partitions, and can be
tuned through parameters εT to favor adaptively coarse or ﬁne partitions. For a partitio n P ∈ PD ,
◦
(εT ), where the set
0 s.t. ∃V ∈ P, V ( T } gathers
P = {T ∈ P D
π(P ) = QT ∈ P (1 − εT ) QT ∈
◦
P
all coarser sets belonging to coarser resolutions than P , and can be regarded as the set of all ancestors
in P D
0 of sets enumerated in P .

3.2 Factorization of kπ

We use the branching-process prior can be used to factorize the formula in Equation (2):

Proposition 2 For two elements µ, µ0 of ML (X ), de ﬁne for T spanning recursively all sets con-
tained in PD , PD−1 , ..., P0 the quantity KT below; then kπ (µ, µ0 ) = KL .
KT = (1 − εT )kT (µ, µ0 ) + εT YU ∈ s(T )

KU .

Proof
The proof follows from a factorization which
uses the branching process prior used for the
tree generation, and can be derived from the
proof of (Catoni, 2004, Proposition 5.2). The
opposite ﬁgure underlines the importance of
incorporating to each node KT a weighted
product of the sibling kernel evaluations KU .
The update rule for the computation of kπ
takes into account the branching process prior
by weighting the kernel kT with all values kti
obtained for ﬁner resolutions
ti in s(T ).

Kt1
µt1
µ0t1

Kt2
µt2
µ0t2

Kt3
µt3
µ0t3

KT = (1 − εT )k(µT , µ0T ) + εT Q Kti
µT = P µti
µ0T = P µ0ti

If the hierarchy of L is such that the cardinality of s(T ) is ﬁxed to a constant α for any set T , typically
α = 4 for images in the case described in Figure 2, then the computation of kπ is upperbounded
by (αD+1 − 1)c(k). This complexity is also upperbounded by the total amount of components
considered in the compared objects, as in (Cuturi & Vert, 2005) for instance.

3.3 Choosing the Base Kernel

+ (X ) can be used to comply with the terms of De ﬁnition 1 and apply an average
Any kernel on M b
scheme on families of measures. We also note that an even more general formulation can be obtained
by using a different kernel kt for each label t of L, without altering the overall applicability of the
factorization above. However, we only consider in this discussion a unique choice k for all t ∈ L.
First, one can note that kernels such as the information diffusion kernel (Lafferty & Lebanon, 2005)
and variance based kernels (Kondor & Jebara, 2003; Cuturi et al., 2005) may not work in this
+(X ). The most adequate
setting since they are not p.d., nor sometimes de ﬁned, on the whole of M b
geometry of M b
+ (X ), following the denormalization scheme proposed in (Amari & Nagaoka, 2001,
p.47), may arguably be derived from the Riemannian embedding ν 7→ √ν , where the Euclidian
distance between two measures in this representation is equal to the geodesic distance between ν
+(X ) endowed with the Fisher metric, as expressed in ψH2 below. More generally,
and ν 0 in M b
one can consider the whole family of kernels for bounded measures described in (Hein & Bousquet,
2005) to choose the base kernel k , namely the family of Hilbertian metrics ψ such that k = e− 1
λ ψ .
We thus use in our experiments the Jensen divergence, the χ2 distance, the total variation, and two
variations of the Hellinger distance:
ψJD (θ, θ 0 ) = h (cid:18) θ + θ 0
(θi − θ 0
i )2
h(θ) + h(θ 0 )
2 (cid:19) −
, ψχ2 (θ, θ 0 ) = Xi
θi + θ 0
2
i
i |, ψH2 (θ, θ 0 ) = Xi
ψT V (θ, θ 0 ) = Xi
i |2 , ψH1 (θ, θ 0 ) = Xi
|pθi − pθ 0
|θi − θ 0

|pθi − pθ 0
i |.

,

4 Experiments in Image Retrieval

We present in this section experiments inspired by the image retrieval task ﬁrst considered
in (Chapelle et al., 1999) and reused in (Hein & Bousquet, 2005). Our dataset was also extracted
from the Corel Stock database and includes 12 families of labeled images, each class containing

2
0

)
1 λ
(
2
g
o
l

-6

-12

0

0.23
0.22
0.21
0.2
0.19
0.18
0.17
0.16
0.15
0.14
0.13

1/2
ε

1

Figure 4: Misclassiﬁcation rate on the corel experiment, us ing the Hellinger H1 distance between
1
histograms coupled with one-vs-all SVM classiﬁcation ( C = 100) as a function of λ and ε.
λ
is taken in {2−12 , · · · , 22} while ε spans {0, 0.1, · · · , 0.9, 1}. ε controls the granularity of the
averaging kernel, ranging from the coarsest perspective (ε = 0) when only the global histogram is
used, to the ﬁnest one ( ε = 1) when only the ﬁnest histograms are considered. Dark values represent
error rates which are greater or equal to 24%. The central values are roughly 14.5% while the best
value obtained in the columns ε = 0 and ε = 1 are 18.4% and 17.3% respectively

100 color images of 256 × 384 pixels. The families depict images of bears, African specialty an-
iors, bonsais, sunsets, clouds, apes and
imals, monkeys, cougars, ﬁreworks, mountains, ofﬁce inter
rocks and gems. The database is randomly split into balanced sets of 800 training images and 400
test images. The task consists in classifying the test images with the rule learned by training 12
one-versus-all SVM’s on the learning fold. Note that previous work conducted in (Chapelle et al.,
1999) illustrates the competitiveness of SVM’s in this context over other algorithms such as nearest
neighbors. Our results are averaged over 3 random splits, using the Spider toolbox.

We used 9 bits for the color of each pixel to reduce the size of the RGB color space to 83 = 512
from the original set of 2563 = 16, 777, 216 colors, and we de ﬁned centered grids of 4, 42 = 16
and 43 = 64 local patches. We provide results for each of the 5 considered kernels and for each
considered depth D ranging from 1 to 3. Figure 5 presents 15 = 5× 3 plots, where each plot displays
the misclassiﬁcation rate as a function of the width paramet er 1
λ and the branching process prior ε
set over all nodes of the tree. The constant C is set to 100, but other choices for C (1000 and 10)
gave comparable plots, although a bit different in shape. By considering values of ε ranging from 0
to 1, we aim at giving a sketch of the robustness of the averaging approach, since the SVM’s seem
to perform better when 0 < ε < 1 for a large span of λ values. For a better understanding of these
plots, the reader may refer to Figure 4 which focuses on ψH1 and D = 2, noting that the color scales
used for Figures 4 and 5 are the same. Finally, the Gaussian kernel was also tested but its very poor
performance (with error rate above 22% for all parameters) illustrates once more that the Gaussian
kernel is usually a poor choice to compare histograms directly.

5 Discussion

The computation of averaged kernels can be performed almost as fast as kernels which only rely
on ﬁne resolutions, which along with their robustness and im proved performance might advocate
their use, notably as an extension of kernels based on arbitrary partitions (Grauman & Darrell, 2005;
Matsuda et al., 2005). Principled ways of estimating in a semi-supervised setting both λ and ε, or
0 , might give them an additional edge. This is a topic
preferably localized priors λT and εT , T ∈ P D
of current research, and we suggest to set these parameters through cross-validation at the moment,
while H1 seems to be a reasonable choice to de ﬁne the base kernel. Our a pproach is related to
the Multiple Kernel Learning framework (Lanckriet et al., 2004), although we do not aim here at
learning linear combinations of the kernels kT , but rather start from an hierarchical belief on them
to propose an algebraic combination.
Acknowledgments: This research was supported by the Function and Induction Research Project,
Transdisciplinary Research Integration Center - Research Organization of Information and Systems.

H1

H2

TV

Xi2

JD

D = 1 

D = 2 

 D = 3

Figure 5: Error-rate results for different kernels and depths are displayed in the same way that in
Figure 4, using the same colorscale across experiments.

References

Amari, S.-I., & Nagaoka, H. (2001). Methods of information geometry. AMS vol. 191.
Berg, C., Christensen, J. P. R., & Ressel, P. (1984). Harmonic analysis on semigroups. No. 100 in
Graduate Texts in Mathematics. Springer Verlag.
Catoni, O. (2004). Statistical learning theory and stochastic optimization. No. 1851 in Lecture
Notes in Mathematics. Springer Verlag.
Chapelle, O., Haffner, P., & Vapnik, V. (1999). SVMs for histogram based image classiﬁcation.
IEEE Transactions on Neural Networks, 10, 1055.
Cuturi, M., Fukumizu, K., & Vert, J.-P. (2005). Semigroup kernels on measures. JMLR, 6, 1169 –
1198.
Cuturi, M., & Vert, J.-P. (2005). The context-tree kernel for strings. Neural Networks, 18, 1111 –
1123.
Grauman, K., & Darrell, T. (2005). The pyramid match kernel: Discriminative classiﬁcation with
sets of image features. ICCV (pp. 1458 –1465). IEEE Computer Society.
Haussler, D. (1999). Convolution kernels on discrete structures (Technical Report). UC Santa Cruz.
CRL-99-10.
Hein, M., & Bousquet, O. (2005). Hilbertian metrics and positive de ﬁnite kernels on probability
measures. Proceedings of AISTATS.
Joachims, T. (2002). Learning to classify text using support vector machines: Methods, theory, and
algorithms. Kluwer Academic Publishers.
Kondor, R., & Jebara, T. (2003). A kernel between sets of vectors. Proc. of ICML’03 (pp. 361 –368).
Lafferty, J., & Lebanon, G. (2005). Diffusion kernels on statistical manifolds. JMLR, 6, 129 –163.
Lanckriet, G., Cristianini, N., Bartlett, P., El Ghaoui, L., & Jordan, M. (2004). Learning the kernel
matrix with semide ﬁnite programming. Journal of Machine Learning Research, 5, 27 –72.
Leslie, C., Eskin, E., Weston, J., & Noble, W. S. (2003). Mismatch string kernels for svm protein
classiﬁcation. NIPS 15. MIT Press.
Matsuda, A., Vert, J.-P., Saigo, H., Ueda, N., Toh, H., & Akutsu, T. (2005). A novel representation
of protein sequences for prediction of subcellular location using support vector machines. Protein
Sci., 14, 2804 –2813.
R ¨atsch, G., & Sonnenburg, S. (2004). Accurate splice site prediction for caenorhabditis elegans,
277 –298. MIT Press series on Computational Molecular Biolo gy. MIT Press.
Sch ¨olkopf, B., Tsuda, K., & Vert, J.-P. (2004). Kernel methods in computational biology. MIT
Press.
Vert, J.-P., Saigo, H., & Akutsu, T. (2004). Local alignment kernels for protein sequences.
In
B. Sch ¨olkopf, K. Tsuda and J.-P. Vert (Eds.), Kernel methods in computational biology. MIT
Press.


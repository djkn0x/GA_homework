Learning to be Bayesian without Supervision

Martin Raphan

Courant Inst. of Mathematical Sciences
New York University
raphan@cims.nyu.edu

Eero P. Simoncelli
Center for Neural Science, and
Courant Inst. of Mathematical Sciences
New York University
eero.simoncelli@nyu.edu

Bayesian estimators are deﬁ ned in terms of the posterior distribution. Typically,
this is written as the product of the likelihood function and a prior probability
density, both of which are assumed to be known. But in many situations, the prior
density is not known, and is difﬁ cult to learn from data since one does not have
access to uncorrupted samples of the variable being estimated. We show that for a
wide variety of observation models, the Bayes least squares (BLS) estimator may
be formulated without explicit reference to the prior. Speciﬁ cally, we derive a
direct expression for the estimator, and a related expression for the mean squared
estimation error, both in terms of the density of the observed measurements. Each
of these prior-free formulations allows us to approximate the estimator given a
sufﬁ cient amount of observed data. We use the ﬁ rst form to develop practical
nonparametric approximations of BLS estimators for several different observation
processes, and the second form to develop a parametric family of estimators for
use in the additive Gaussian noise case. We examine the empirical performance
of these estimators as a function of the amount of observed data.

1

Introduction

Bayesian methods are widely used throughout engineering for estimating quantities from corrupted
measurements. Those that minimize the mean squared error (known as Bayes least squares, or BLS)
are particularly widespread. These estimators are usually derived assuming explicit knowledge of
the observation process (expressed as the conditional density of the observation given the quantity
to be estimated), and the prior density over that quantity. Despite its appeal, this approach is often
criticized for the reliance on knowledge of the prior distribution, since the true prior is usually
not known, and in many cases one does not have data drawn from this distribution with which to
approximate it.
In this case, it must be learned from the same observed measurements that are
available in the estimation problem. In general, learning the prior distribution from the observed
data presents a difﬁ cult, if not impossible task, even when the observation process is known. In the
commonly used ”empirical Bayesian” approach [1], one assumes a parametric family of densities,
whose parameters are obtained by ﬁ
tting the data. This prior is then used to derive an estimator
that may be applied to the data. If the true prior is not a member of the assumed parametric family,
however, such estimators can perform quite poorly.

An estimator may also be obtained in a supervised setting, in which one is provided with many
pairs containing a corrupted observation along with the true value of the quantity to be estimated. In
this case, selecting an estimator is a classic regression problem: ﬁ nd a function that best maps the
observations to the correct values, in a least squares sense. Given a large enough number of training
samples, this function will approach the BLS estimate, and should perform well on new samples
drawn from the same distribution as the training samples. In many real-world situations, however,
one does not have access to such training data.

In this paper, we examine the BLS estimation problem in a setting that lies between the two cases
described above. Speciﬁ cally, we assume the observation process (but not the prior) is known, and

we assume unsupervised training data, consisting only of corrupted observations (without the correct
values). We show that for many observation processes, the BLS estimator may be written directly in
terms of the observation density. We also show a dual formulation, in which the BLS estimator may
be obtained by minimizing an expression for the mean squared error that is written only in terms of
the observation density. A few special cases of the ﬁ rst formulation appear in the empirical Bayes
literature [2], and of the second formulation in another branch of the statistical literature concerned
with improvement of estimators [3, 4, 5]. Our work serves to unify these prior-free methods within
a linear algebraic framework, and to generalize them to a wider range of cases. We develop practical
nonparametric approximations of estimators for several different observation processes, demonstrat-
ing empirically that they converge to the BLS estimator as the amount of observed data increases.
We also develop a parametric family of estimators for use in the additive Gaussian case, and examine
their empirical convergence properties. We expect such BLS estimators, constructed from corrupted
observations without explicit knowledge of, assumptions about, or samples from the prior, to prove
useful in a variety real-world estimation problems faced by machine or biological systems that must
learn from examples.

2 Bayes least squares estimation

=

Suppose we make an observation, Y , that depends on a hidden variable X , where X and Y may
be scalars or vectors. Given this observation, the BLS estimate of X is simply the conditional
expectation of the posterior density, E {X |Y = y}. If the prior distribution on X is PX , and the
(cid:1)
likelihood function is PY |X then this can be written using Bayes’ rule as
(cid:1)
(cid:2)
x PX |Y (x|y) dx
E {X |Y = y} =
x PY |X (y|x) PX (x) dx
(cid:1)
where the denominator is the distribution of the observed data:
PY |X (y|x) PX (x) dx .
PY (y) =
If we know PX and PY |X , we can calculate this explicitly.
Alternatively, if we do not know PX or PY |X , but are given independent identically distributed
(i.i.d.) samples (Xn , Yn ) drawn from the joint distribution of (X, Y ), then we can solve for the
estimator f (y) = E {X |Y = y} nonparametrically, or we could choose a parametric family of
N(cid:3)
estimators {fθ }, and choose θ to minimize the empirical squared error:
|fθ (Yn ) − Xn |2 .
1
N
n=1
However, in many situations, one does not have access to PX , or to samples drawn from PX .

ˆθ = arg min
θ

PY (y) ,

(1)

(2)

N (y) =

2.1 Prior-free reformulation of the BLS estimator
(cid:1)
In many cases, the BLS estimate may be written without explicit reference to the prior distribution.
We begin by noting that in Eq. (1), the prior appears only in the numerator
PY |X (y|x) x PX (x) dx.
This equation may be viewed as a composition of linear transformations of the function PX (x)
N (y) = (A ◦ X){PX }(y),
X{f }(x) = xf (x),
(cid:1)
and the operator A computes an inner product with the likelihood function
A{f }(y) =
PY |X (y|x) f (x) dx.

where

Similarly, Eq. (2) may be viewed as the linear transformation A applied to PX (x). If the linear
transformation A is 1-1, and we restrict PY to lie in the range of A, then we can then write the
numerator as a linear transformation on PY alone, without explicit reference to PX :
N (y) = (A ◦ X ◦ A−1 ){PY }(y)
= L{PY }(y).
(3)
In the discrete case, PY (y) and N (y) are each vectors, A is a matrix containing PY |X , X is a
diagonal matrix containing values of x, and ◦ is matrix multiplication.
This allows us to write the BLS estimator as
E {X |Y = y} =

L{PY }(y)
PY (y)
Note that if we wished to calculate E {X n |Y }, then Eq. (3) would be replaced by (A ◦ Xn ◦
A−1 ){PY } = (A ◦ X ◦ A−1 )n {PY } = Ln{PY } . By linearity of the conditional expectation,
(cid:4)
(cid:5)
(cid:6)
we may extend this to any polynomial function (and thus to any function that can be approximated
M(cid:3)
with a polynomial):
k=−N
In the deﬁ nition of the operator L, A−1 effectively inverts the observation process, recovering PX
from PY . In many situations, this operation will not be well-behaved. For example, in the case
of additive Gaussian noise, A−1 is a deconvolution operation which is inherently unstable for high
frequencies. The usefulness of Eq. (4) comes from the fact that in many cases, the composite oper-
ation L may be written explicitly, even when the inverse operation is poorly deﬁ ned or unstable. In
section 3, we develop examples of operators L for a variety of observation processes.

k=−N ckLk {PY }(y)
M
PY (y)

ckX k |Y = y

(4)

=

E

.

.

2.2 Prior-free reformulation of the mean squared error

.

E

E

+ E

In some cases, developing a stable nonparametric approximation of the ratio in Eq. (4) may be difﬁ -
cult. However, the linear operator formulation of the BLS estimator also leads to a dual expression
for the mean squared error that does not depend explicitly on the prior, and this may be used to select
(cid:8)
(cid:7)
(cid:8)
(cid:7)
(cid:8)
(cid:7)
an optimal estimator from a parametric family of estimators. Speciﬁ cally, for any estimator fθ (Y )
parameterized by θ , the mean squared error may be decomposed into two orthogonal terms:
|E (X |Y ) − X |2
|fθ (Y ) − E (X |Y )|2
|fθ (Y ) − X |2
= E
(cid:8)
(cid:7)
(cid:8)
(cid:7)
(cid:7)
(cid:8)
The second term is the minimum possible MSE, obtained when using the optimal estimator. Since
it does not depend on fθ , it is irrelevant for optimizing θ . The ﬁ rst term may be expanded as
|fθ (Y )|2 − 2fθ (Y )E (X |Y )
|fθ (Y ) − E (X |Y )|2
|E (X |Y )|2
+ E
(cid:9)
(cid:10)
Again, the second expectation does not depend on fθ . Using the prior-free formulation of the previ-
ous section, the second component of the ﬁ rst expectation may be written as
L{PY }(Y )
(cid:1)
E {fθ (Y )E (X |Y )} = E
fθ (Y )
PY (Y )
L{PY }(y)
(cid:1)
fθ (y)
PY (y) PY (y)dy
(cid:1)
fθ (y) L{PY }(y)dy
L∗ {fθ }(y)PY (y)dy
=
= E {L∗{fθ }(Y )} ,

= E

=

=

.

E

(cid:7)
(cid:7)
(cid:8)
(cid:8)
is the dual operator of L (in the discrete case, L∗
where L∗
is the matrix transpose of L). Combining
all of the above, we have:
|fθ (Y )|2 − 2L∗ {fθ }(Y )
|fθ (Y ) − X |2
arg min
= arg min
θ
θ
(cid:7)
(cid:8)
N(cid:3)
where the expectation on the right is over the observation variable, Y . In practice, we can solve for
an optimal θ by minimizing the sample mean of this quantity:
|fθ (Yn )|2 − 2L∗ {fθ }(Yn )
1
ˆθ = arg min
(6)
.
N
θ
n=1
where {Yn } is a set of observed data. Again this does not require any knowledge of, or samples
drawn from, the prior PX .

(5)

E

.

3 Example estimators

where

In general, it can be difﬁ cult to obtain the operator L directly from the deﬁ nition in Eq. (3), because
inversion of the operator A could be unstable or undeﬁ ned. Instead, a solution may often be obtained
by noting that the deﬁ nition implies that
L ◦ A = A ◦ X,
or, equivalently
L{PY |X (y|x)} = xPY |X (y|x).
This is an eigenfunction equation: for each value of x, the conditional density PY |X (y|x) must be
an eigenfunction (eigenvector, for discrete variables) of eoperator L, with associated eigenvalue x.
Consider a standard example, in which the variable of interest is corrupted by independent additive
noise: Y = X + W . The conditional density is
PY |X (y|x) = PW (y − x).
We wish to ﬁ nd an operator which when applied to this conditional density (viewed as a function of
y) will give
L{PW (y − x)} = x PW (y − x)
for all x. Subtracting y PW (y − x) from both sides gives
M {PW (y − x)} = −(y − x) PW (y − x).
M {f } (y) = L{f }(y) − y f (y)
is a linear shift-invariant operator (acting in y).
(cid:11)M(ω)(cid:12)PW (ω) = − (cid:12)(yPW )(ω)
Taking Fourier transforms and using the convolution and differentiation properties gives:
(cid:12)PW (ω),
= −i∇ω
(cid:12)PW (ω)(cid:12)PW (ω)
(cid:11)M(ω) = −i
∇ω
(cid:13)(cid:12)PW (ω)
(cid:14)
= −i∇ω ln
(cid:7)
(cid:14) (cid:15)f (ω)
(cid:13)(cid:12)PW (ω)
(cid:8)
.
This gives us the linear operator
L{f }(y) = y f (y) − F −1
i∇ω ln
(y),
(11)
where F −1 denotes the inverse Fourier transform. Note that throughout this discussion X and
W played symmetric roles. Thus, in cases with known prior density and unknown additive noise
density, one can formulate the estimator entirely in terms of the prior.

so that

(10)

(7)

(8)

(9)

Our prior-free estimator methodology is quite general, and can often be applied to more complicated
observation processes. In order to give some sense of the diversity of forms that can arise, Table 1
provides additional examples. References for the speciﬁ c cases that we have found in the statistics
literature are provided in table.

Obs. process

Discrete

Gen. add.

Add.
[6]/[4]*

Gaussian

Add. Poisson

Add. Laplacian

Add. Cauchy

Add. uniform

Add.
random # of
components

Disc. exp. [2]/[5]*

Disc. inv. exp. [5]*

Cnt. exp. [2]/[3]*

Cnt. inv. exp. [3]*

Poisson [7]/[5]*

Gauss. scale mixture

Lapl. scale mixture

Obs. density: PY |X (y|x)
A
PW (y − x)
exp{ −1
2 (y−x−µ)T Λ−1 (y−x−µ)}
√
(cid:6)
|2πΛ|
δ(y − x − ks)
−λ
λk e
k!
−|(y−x)/α|
1
(cid:9)
2α e
1
(α(y−x))2+1 )
π (
α
|y − x| ≤ a
1
2a ,
|y − x| > a
W ∼ (cid:6)
0,
PW (y − x), where:
K
k=0 Wk ,
Wk i.i.d. (Pc ),
K ∼ Poiss(λ)
h(x)g(n)xn
−n
h(x)g(n)x

h(x)g(y)eT (y)x

h(x)g(y)eT (y)/x
−x
xn e
n!

− y2
e
2x

1√
2πx
− y
1
x ; x, y > 0
x e

Numerator: N (y) = L{PY }(y)
(cid:14) (cid:11)PY (ω)
(cid:13)(cid:12)PW (ω)
(cid:8)
(cid:7)
(A ◦ X ◦ A−1 )PY (y)
yPY − F −1
i∇ω ln
(y − µ)PY (y) + Λ∇yPY (y)
yPY (y) − λsPY (y − s)
W (cid:6) PY }(y)
yPY (y) + 2α2 {P
(cid:2)
(cid:6)
yPY (y) − { 1
2παy (cid:6) PY }(y)
(cid:16)
sgn(k)PY (y − ak)
yPY (y) + a
PY ( ˜y)sgn(y − ˜y)d ˜y
− 1
2

yPY (y) − λ{(yPc ) (cid:6) PY }(y)

g(y)

g(n)
g(n+1) PY (n + 1)
g(n−1) PY (n − 1)
g(n)
(cid:16)
{ PY (y)
g(y) }
g(y)
d
T (cid:1) (y)
dy
(cid:1) ( ˜y)
y−∞ T
g( ˜y) PY ( ˜y)d ˜y
(n + 1)PY (n + 1)
−EY {Y ; Y < y}
PY {Y > y}

Table 1: Prior-free estimation formulas. Functions written with hats or in terms of ω represent
multiplication in the Fourier Domain. n replaces y for discrete distributions. Bracketed numbers are
references for operators L, with * denoting references for the parametric (dual) operator, L∗
.

4 Simulations

4.1 Non-parametric examples

Since each of the prior-free estimators discussed above relies on approximating values from the
observed data, the behavior of such estimators should approach the BLS estimator as the number of
data samples grows. In Fig. 1, we examine the behavior of three non-parametric prior-free estimators
based on Eq. (4). The ﬁ rst case corresponds to data drawn independently from a binary source,
which are observed through a process in which bits are switched with probability 1
4 . The estimator
does not know the binary distribution of the source (which was a “fair coin ” for our simulation),
but does know the bit-switching probability. For this estimator we approximate PY using a simple
histogram, and then use the matrix version of the linear operator in Eq. (3). We characterize the
behavior of this estimator as a function of the number of data points, N , by running many Monte
Carlo simulations for each N and indicating the mean improvement in MSE (compared with the
ML estimator, which is the identity function), the mean improvement using the conventional BLS
estimation function, E {X |Y = y} assuming the prior density is known, and the standard deviations
of the improvements taken over our simulations.
Figure 1b shows similar results for additive Gaussian noise, with SNR replacing MSE. Signal den-
sity is a generalized Gaussian with exponent 0.5. In this case, we compute Eq. (4) using a more

0.2

0.1

0

−0.1

−0.2

t
n
e
m
e
v
o
r
p
m
i
 
E
S
M

3

2.5

2

1.5

1

0.5

t
n
e
m
e
v
o
r
p
m
i
 
R
N
S

−0.3
100

101

102
# samples
(a)

103

104

0
102

103

104

105

# samples
(b)

t
n
e
m
e
v
o
r
p
m
i
 
R
N
S

5

4

3

2

1

0

−1

−2

−3
102

105

106

103

104
# samples
(c)

Fig. 1: Empirical convergence of prior-free estimator to optimal BLS solution, as a function number
of observed samples of Y . For each number of observations, each estimator is simulated many times.
Black dashed lines show the improvement of the prior-free estimator, averaged over simulations,
relative to the ML estimator. White line shows the mean improvement using the conventional BLS
solution, E {X |Y = y}, assuming the prior density is known. Gray regions denote ± one standard
(a) Binary noise (10,000 simulations for each number of observations); (b) additive
deviation.
Gaussian noise (1,000 simulations); (c) Poisson noise (1,000 simulations).

sophisticated approximation method, as described in [8]. We ﬁ
t a local exponential model similar
to that used in [9] to the data in bins, with binwidth adaptively selected so that the product of the
number of points in the bin and the squared binwidth is constant. This binwidth selection proce-
dure, analogous to adaptive binning procedures developed in the density estimation literature [10],
provides a reasonable tradeoff between bias and variance, and converges to the correct answer for
any well-behaved density [8]. Note that in this case, convergence is substantially slower than for the
binary case, as might be expected given that we are dealing with a continuous density rather than a
single scalar probability. But the variance of the estimates is very low.
Figure 1c shows the case of estimating a randomly varying rate parameter that governs an inho-
mogeneous Poisson process. The prior on the rate (unknown to the estimator) is exponential. The
observed values Y are the (integer) values drawn from the Poisson process. In this case the his-
togram of observed data was used to obtain a naive approximation of PY (n). It should be noted
that improved performance for this estimator is expected if we were to use a more sophisticated
approximation of the ratio of densities.

4.2

Parametric examples

In this section we discuss the empirical behavior of the parametric approach applied to the additive
Gaussian case. From the derivation in section 3, and restricting to the scalar case, we have
= y − σ2 d
L∗
dy
In this particular case,, it is easier to represent the estimator as
f (y) = y + g(y).

.

Substituting into Eq. (5) gives
E {|f (Y ) − X |2 } = E {|g(Y )|2 + σ2 g
(Y )} + const,
(cid:2)
where the constant does not depend on g . Therefore, if we have a parametric family {gθ } of such g ,
N(cid:3)
and are given data {Yn} we can try and minimize
{|gθ (Yn )|2 + σ2 g
θ (Yn )}.
1
(cid:2)
N
n=1
This expression, known as Stein’s unbiased risk estimator (SURE) [4], favors estimators gθ that have
small amplitude, and highly negative derivatives at the data values. This is intuitively sensible: the
resulting estimators will “shrink” the data toward regions of high probability.

(12)

Recently, an expression similar to Eq. (12) was used as a criterion for density estimation in cases
where the normalizing constant, or partition function, is difﬁ cult to obtain [11]. The prior-free

Fig. 2: Example bump functions, used for linear parameterization of estimators in Figs. 3(a) and
3(b).

3

2.5

2

1.5

1

0.5

t
n
e
m
e
v
o
r
p
m
i
 
R
N
S

3

2.5

2

1.5

1

0.5

t
n
e
m
e
v
o
r
p
m
i
 
R
N
S

3

2.5

2

1.5

1

0.5

t
n
e
m
e
v
o
r
p
m
i
 
R
N
S

0
102

103

# samples
(a)

104

105

0
102

103

# samples
(b)

104

105

0
102

103

104

105

# samples
(c)

Fig. 3: Empirical convergence of parametric prior-free method to optimal BLS solution, as a function
number of data observations, for three different parameterized estimators. (a)3 bump; (b)15 bumps;
(c) Soft thresholding. All cases use a generalized Gaussian prior (exponent 0.5), and assume additive
Gaussian noise.

where the functions gk are of the form
gk (y) = y cos2

,

(13)

approach we are discussing provides an interpretation for this procedure: the optimal density is the
one which, when converted into an estimator using the formula in Table 1 for the additive Gaussian
(cid:3)
case, gives the best MSE. This may be extended to any of the linear operators in Table 1.
As an example, we parametrize g as a linear combination of nonlinear “bump” functions
gθ (y) =
θk gk (y)
(cid:17)
(cid:18)
k

sgn(y) log2 (|y |/σ + 1) − kπ
1
2
α
as illustrated in Fig. 2. Recently, linear parameterizations have been used in conjunction with
Eq. (12) for image denoising in the wavelet domain [12].
We can substitute Eq. (13) into Eq. (12) and pick coefﬁ cients {θk } to minimize this criteria, which
is a quadratic function of the coefﬁ cients. For our simulations, we used a generalized Gaussian
prior, with exponent 0.5. Figure 3 shows the empirical behavior of these “SURE-bump ” estimators
when using three bumps ( Fig. 3a) and ﬁ fteen bumps (Fig. 3b), illustrating the bias-variance trade-
off inherent in the ﬁ xed parameterization. Three bumps behaves fairly well, though the asymptotic
behavior for large amounts of data is biased and thus falls short of ideal. Fifteen bumps asymp-
totes correctly but has very large variance for small amounts of data (overﬁ
tting). For comparison
purposes, we have included the behavior of SURE thresholding [13], in which Eq. (4.2) is used to
choose an optimal threshold, θ , for the function
fθ (y) = sgn(y)(|y | − θ)+ .
As can be seen, SURE thresholding shows signiﬁ cant asymptotic bias although the variance behavior
is nearly ideal.

5 Discussion

We have reformulated the Bayes least squares estimation problem for a setting in which one knows
the observation process, and has access to many observations. We do not assume the prior density

is known, nor do we assume access to samples from the prior. Our formulation thus acts as a bridge
between a conventional Bayesian setting in which one derives the optimal estimator from known
prior and likelihood functions, and a data-oriented regression setting in which one learns the optimal
estimator from samples of the prior paired with corrupted observations of those samples.
In many
cases, the prior-free estimator can be written explicitly, and we have shown a number of examples
to illustrate the diversity of estimators that can arise under different observation processes. For three
simple cases, we developed implementations and demonstrated that these converge to optimal BLS
estimators as the amount of data grows. We also have derived a prior-free formulation of the MSE,
which allows selection of an estimator from a parametric family. We have shown simulations for a
linear family of estimators in the additive Gaussian case.

These simulations serve to demonstrate the potential of this approach, which holds particular ap-
peal for real-world systems (machine or biological) that must learn the priors from environmental
observations. Both methods can be enhanced by using data-adaptive parameterizations or ﬁ
tting
procedures in order to properly trade off bias and variance (see, for example [8]). It is of partic-
ular interest to develop incremental implementations, which would update the estimator based on
incoming observations. This would further enhance the applicability of this approach for systems
that must learn to do optimal estimation from corrupted observations.

Acknowledgments

This work was partially funded by the Howard Hughes Medical Institute, and by New York Univer-
sity through a McCracken Fellowship to MR.

References

[1] G. Casella, “An introduction to empirical Bayes data analysis,” Amer. Statist., vol. 39, pp. 83–
87, 1985.
[2] J. S. Maritz and T. Lwin, Empirical Bayes Methods. Chapman & Hall, 2nd ed., 1989.
[3] J. Berger, “Improving on inadmissible estimators in continuous exponential families with ap-
plications to simultaneous estimation of gamma scale parameters,” The Annals of Staistics,
vol. 8, pp. 545–571, 1980.
[4] C. M. Stein, “Estimation of the mean of a multivariate normal distribution,” Annals of Statistics,
vol. 9, pp. 1135 –1151, November 1981.
[5] J. T. Hwang, “Improving upon standard estimators in discrete exponential families with appli-
cations to poisson and negative binomial cases,” The Annals of Staistics, vol. 10, pp. 857–867,
1982.
[6] K. Miyasawa, “An empirical bayes estimator of the mean of a normal population,” Bull. Inst.
Internat. Statist., vol. 38, pp. 181 –188, 1956.
[7] H. Robbins, “An empirical bayes approach to statistics,”
Mathematcal Statistics, vol. 1, pp. 157–163, 1956.
[8] M. Raphan and E. P. Simoncelli, “Empirical Bayes least squares estimation without an explicit
prior.” NYU Courant Inst. Tech. Report, 2007.
[9] C. R. Loader,
“Local likelihood density estimation,”
pp. 1602–1618, 1996.
[10] D. W. Scott, Multivariate Density Estimation: Theory, Practice, and Visualization. John Wiley,
1992.
[11] A. Hyvarinen, “Estimation of non-normalized statistical models by score matching,”
of Machine Learning Research, vol. 6, pp. 695–709, 2005.
[12] F. Luisier, T. Blu, and M. Unser, “SURE-based wavelet thresholding integrating inter-scale
dependencies,” in Proc IEEE Int’l Conf on Image Proc , (Atlanta GA, USA), pp. 1457 –1460,
October 2006.
[13] D. Donoho and I. Johnstone, “Adapting to unknown smoothness via wavelet shrinkage,”
American Stat Assoc, vol. 90, December 1995.

Annals of Statistics, vol. 24, no. 4,

Proc. Third Berkley Symposium on

Journal

J


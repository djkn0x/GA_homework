Context Effects in Category Learning:
An Investigation of Four Probabilistic Models

Michael C. Mozer+(cid:5) , Michael Jones(cid:5)† , Michael Shettel+
+Dept. of Computer Science, †Dept. of Psychology, and (cid:5) Institute of Cognitive Science
University of Colorado, Boulder, CO 80309-0430
{mozer,mike.jones,shettel}@colorado.edu

Abstract
Categorization is a central activity of human cognition. When an individual is asked to cat-
egorize a sequence of items, context effects arise: categorization of one item inﬂuences cat-
egory decisions for subsequent items. Speci ﬁcally, when experimental subjects are shown
an exemplar of some target category, the category prototype appears to be pulled toward the
exemplar, and the prototypes of all nontarget categories appear to be pushed away. These
push and pull effects diminish with experience, and likely reﬂect long-term learning of
category boundaries. We propose and evaluate four principled probabilistic (Bayesian) ac-
counts of context effects in categorization. In all four accounts, the probability of an exem-
plar given a category is encoded as a Gaussian density in feature space, and categorization
involves computing category posteriors given an exemplar. The models differ in how the
uncertainty distribution of category prototypes is represented (localist or distributed), and
how it is updated following each experience (using a maximum likelihood gradient ascent,
or a Kalman ﬁlter update). We ﬁnd that the distributed maximum-likelihood model can ex-
plain the key experimental phenomena. Further, the model predicts other phenomena that
were conﬁrmed via reanalysis of the experimental data.

Categorization is a key cognitive activity. We continually make decisions about characteristics of
objects and individuals: Is the fruit ripe? Does your friend seem unhappy? Is your car tire ﬂat? When
an individual is asked to categorize a sequence of items, context effects arise: categorization of one
item inﬂuences category decisions for subsequent items. Intuitive naturalistic scenarios in which
context effects occur are easy to imagine. For example, if one lifts a medium-weight object after
lifting a light-weight or heavy-weight object, the medium weight feels heavier following the light
weight than following the heavy weight. Although the object-contrast effect might be due to fatigue
of sensory-motor systems, many context effects in categorization are purely cognitive and cannot
easily be attributed to neural habituation. For example, if you are reviewing a set of conference
papers, and the ﬁrst three in the set are dreadful, then even a mediocre paper seems like it might
be above threshold for acceptance. Another example of a category boundary shift due to context
is the following. Suppose you move from San Diego to Pittsburgh and notice that your neighbors
repeatedly describe muggy, somewhat overcast days as ”lovely.” Eventually, your notion of what
constitutes a lovely day accommodates to your new surroundings.

As we describe shortly, experimental studies have shown a fundamental link between context effects
in categorization and long-term learning of category boundaries. We believe that context effects can
be viewed as a re ﬂection of a trial-to-trial learning, and the cumulative effect of these trial-to-trial
modulations corresponds to what we classically consider to be category learning. Consequently, any
compelling model of category learning should also be capable of explaining context effects.

1 Experimental Studies of Context Effects in Categorization

Consider a set of stimuli that vary along a single continuous dimension. Throughout this paper,
we use as an illustration circles of varying diameters, and assume four categories of circles deﬁned
ranges of diameters; call them A, B, C, and D, in order from smallest to largest diameter.

In a classi ﬁcation paradigm, experimental subjects are given an exemplar drawn from one category
and are asked to respond with the correct category label (Zotov, Jones, & Mewhort, 2003). After
making their response, subjects receive feedback as to the correct label, which we’ll refer to as the
target. In a production paradigm, subjects are given a target category label and asked to produce
an exemplar of that category, e.g., using a computer mouse to indicate the circle diameter (Jones &
Mewhort, 2003). Once a response is made, subjects receive feedback as to the correct or true cate-
gory label for the exemplar they produced. Neither classiﬁcation nor production task has sequential
structure, because the order of trial is random in both experiments.

The production task provides direct information about the subjects’ internal representations, because
subjects are producing exemplars that they consider to be prototypes of a category, whereas the cat-
egorization task requires indirect inferences to be made about internal representations from reaction
time and accuracy data. Nonetheless, the ﬁndings in the production and classiﬁcation tasks mir-
ror one another nicely, providing converging evidence as to the nature of learning. The production
task reveals how mental representations shift as a function of trial-to-trial sequences, and these shifts
cause the sequential pattern of errors and response times typically observed in the classiﬁcation task.
We focus on the production task in this paper because it provides a richer source of data. However,
we address the categorization task with our models as well.

Figure 1 provides a schematic depiction of the key sequential effects in categorization. The hori-
zontal line represents the stimulus dimension, e.g., circle diameter. The dimension is cut into four
regions labeled with the corresponding category. The category center, which we’ll refer to as the
prototype, is indicated by a vertical dashed line. The long solid vertical line marks the current
exemplar—whether it is an exemplar presented to subjects in the classiﬁcation task or an exemplar
generated by subjects in the production task. Following an experimental trial with this exemplar, cat-
egory prototypes appear to shift: the target-category prototype moves toward the exemplar, which
we refer to as a pull effect, and all nontarget-category prototypes move away from the exemplar,
which we refer to as a push effect. Push and pull effects are assessed in the production task by ex-
amining the exemplar produced on the following trial, and in the categorization task by examining
the likelihood of an error response near category boundaries.

The set of phenomena to be explained are as follows, described in terms of the production task. All
numerical results referred to are from Jones and Mewhort (2003). This experiment consisted of 12
blocks of 40 trials, with each category label given as target 10 times within a block.
• Within-category pull: When a target category is repeated on successive trials, the exemplar
generated on the second trial moves toward the exemplar generated on the ﬁrst trial, with respect to
the true category prototype. Across the experiment, a correlation coefﬁcient of 0.524 is obtained,
and remains fairly constant over trials.
• Between-category push: When the target category changes from one trial to the next, the ex-
emplar generated on the second trial moves away from the exemplar generated on the ﬁrst trial (or
equivalently, from the prototype of the target category on the ﬁrst trial). Figure 2a summarizes the
sequential push effects from Jones and Mewhort. The diameter of the circle produced on trial t is
plotted as a function of the target category on trial t − 1, with one line for each of the four trial t tar-
gets. The mean diameter for each target category is subtracted out, so the absolute vertical offset of
each line is unimportant. The main feature of the data to note is that all four curves have a negative
slope, which has the following meaning: the smaller that target t − 1 is (i.e., the further to the left
on the x axis in Figure 1), the larger the response to target t is (further to the right in Figure 1), and
t − 1. Interestingly and importantly, the magnitude of
vice versa, reﬂecting a push away from target
the push increases with the ordinal distance between targets t − 1 and t. Figure 2a is based on data
from only eight subjects and is therefore noisy, though the effect is statistically reliable. As further
evidence, Figure 2b shows data from a categorization task (Zotov et al., 2003), where the y-axis is a
different dependent measure, but the negative slope has the same interpretation as in Figure 2a.

Figure 1: Schematic depic-
tion of sequential effects in
categorization

stimulus dimensionABCDexampleFigure 2: Push effect data
from (a) production task of
Jones and Mewhort (2003),
(b) classiﬁcation task of Zo-
tov et al. (2003), and (c)-(f)
the models proposed in this
paper. The y axis is the de-
viation of the response from
the mean, as a proportion
of the total category width.
The response to category A
is solid red, B is dashed
magenta, C is dash-dotted
blue, and D is dotted green.

• Push and pull effects are not solely a consequence of errors or experimenter feedback. In quan-
titative estimation of push and pull effects, trial t is included in the data only if the response on trial
t − 1 is correct. Thus, the effects follow trials in which no error feedback is given to the subjects,
and therefore the adjustments are not due to explicit error correction.
• Push and pull effects diminish over the course of the experiment. The magnitude of push effects
can be measured by the slope of the regression lines ﬁt to the data in Figure 2a. The slopes get shal-
lower over successive trial blocks. The magnitude of pull effects can be measured by the standard
deviation (SD) of the produced exemplars, which also decreases over successive trial blocks.
• Accuracy increases steadily over the course of the experiment, from 78% correct responses in the
ﬁrst block to 91% in the ﬁnal block. This improvement occurs despite the fact that error feedback is
relatively infrequent and becomes even less frequent as performance improves.

2 Four Models

In this paper, we explore four probabilistic (Bayesian) models to explain data described in the pre-
vious section. The key phenomenon to explain turns out to be the push effect, for which three of the
four models fail to account. Modelers typically discard the models that they reject, and present only
their pet model. In this work, we ﬁnd it useful to report on the rejected models for three reasons.
First, they help to set up and motivate the one successful model. Second, they include several ob-
vious candidates, and we therefore have the imperative to address them. Third, in order to evaluate
a model that can explain certain data, one needs to know the degree to which the the data constrain
the space of models. If many models exist that are consistent with the data, one has little reason to
prefer our pet candidate.

Underlying all of the models is a generative probabilistic framework in which a category i is rep-
resented by a prototype value, di , on the dimension that discriminates among the categories. In the
example used throughout this paper, the dimension is the diameter of a circle (hence the notation d
for the prototype). An exemplar, E , of category i is drawn from a Gaussian distribution with mean
di and variance vi , denoted E ∼ N (di , vi ). Category learning involves determining d ≡ {di }.
In this work, we assume that the {vi } are ﬁxed and given. Because d is unknown at the start of
the experiment, it is treated as the value of a random vector, D ≡ {Di }. Figure 3a shows a sim-
ple graphical model representing the generative framework, in which E is the exemplar and C the
category label.

To formalize our discussion so far, we adopt the following notation:
P (E |C = c, D = d) ∼ N (hcd, vc ),
(1)
where, for the time being, hc is a unary column vector all of whose elements are zero except for
element c which has value 1. (Subscripts may indicate either an index over elements of a vector, or
an index over vectors. Boldface is used for vectors and matrices.)

ABCD−0.1−0.08−0.06−0.04−0.0200.020.040.060.08(a) humans: productionresponse deviationprevious category labelADCBABCD(b) humans: classificationresponse biasprevious category labelABCD−0.1−0.08−0.06−0.04−0.0200.020.040.060.08(c) KFU−localprevious category labelresponse deviationABCD−0.1−0.08−0.06−0.04−0.0200.020.040.060.08(d) KFU−distribprevious category labelresponse deviationABCD−0.1−0.08−0.06−0.04−0.0200.020.040.060.08(e) MLGA−localprevious category labelresponse deviationABCD−0.1−0.08−0.06−0.04−0.0200.020.040.060.08(f) MLGA−distribprevious category labelresponse deviationFigure 3:
(a) Graphi-
cal model depicting selec-
tion of an exemplar, E , of
a category, C , based on
the prototype vector, D; (b)
Dynamic version of model
indexed by trials, t

We assume that the prototype representation, D, is multivariate Gaussian, D ∼ N (Ψ, Σ), where Ψ
and Σ encode knowledge—and uncertainty in the knowledge —of the category prototype structure.
Given this formulation, the uncertainty in D can be integrated out:
P (E |C ) ∼ N (hcΨ, hcΣhT
c + vc ).

(2)

For the categorization task, a category label can be assigned by evaluating the category posterior,
P (C |E ), via Bayes rule, Equation 1, and the category priors, P (C ).
In this framework, learning takes place via trial-to-trial adaptation of the category prototype distri-
bution, D. In Figure 3b, we add the subscript t to each random variable to denote the trial, yielding a
dynamic graphical model for the sequential updating of the prototype vector, Dt . (The reader should
be attentive to the fact that we use subscripted indices to denote both trials and category labels. We
generally use the index t to denote trial, and c or i to denote a category label.) The goal of our
modeling work is to show that the sequential updating process leads to context effects, such as the
push and pull effects discussed earlier.

We propose four alternative models to explore within this framework. The four models are obtained
via the Cartesian product of two binary choices: the learning rule and the prototype representation.

2.1 Learning rule

The ﬁrst learning rule, maximum likelihood gradient ascent (MLGA), attempts to adjust the prototype
representation so as to maximize the log posterior of the category given the exemplar. (The category,
C = c, is the true label associated with the exemplar, i.e., either the target label the subject was asked
to produce, or—if an error was made —the actual category label the subject did produce.) Gradient
ascent is performed in all parameters of Ψ and Σ:

∆ψi = ψ

log(P (c|e))

∂
∂ψi

and ∆σij = σ

log(P (c|e)),

∂
∂σij

(3)

where ψ and σ are step sizes. To ensure that Σ remains a covariance matrix, constrained gradient
i ≥ 0; (2) off-
steps are applied. The constraints are: (1) diagonal terms are nonnegative, i.e., σ2
diagonal terms are symmetric, i.e., σij = σj i ; and (3) the matrix remains positive de ﬁnite, ensured
≤ 1.
by −1 ≤ σij
σi σj
The second learning rule, a Kalman ﬁlter update (KFU), reestimates the uncertainty distribution of
the prototypes given evidence provided by the current exemplar and category label. To draw the
correspondence between our framework and a Kalman ﬁlter: the exemplar is a scalar measurement
that pops out of the ﬁlter, the category prototypes are the hidden state of the ﬁlter, the measurement
noise is vc , and the linear mapping from state to measurement is achieved by hc . Technically, the
model is a measurement-switched Kalman ﬁlter, where the switching is determined by the category
label c, i.e., the measurement function, hc , and noise, vc , are conditioned on c. The Kalman ﬁlter also
allows temporal dynamics via the update equation, dt = Adt−1 , as well as internal process noise,
whose covariance matrix is often denoted Q in standard Kalman ﬁlter notation. We investigated the
choice of A and R, but because they did not impact the qualitative outcome of the simulations, we
used A = I and R = 0. Given the correspondence we’ve established, the KFU equations—which
specify Ψt+1 and Σt+1 as a function of ct , et , Ψt , and Σt —can be found in an introductory text
(e.g., Maybeck, 1979).

DECDt-1Ct-1DtCtEtEt-1(a)(b)Figure 4:
Change to a
category prototype for each
category following a trial of
a given category.
Solid
(open) bars indicate trials in
which the exemplar is larger
(smaller) than the prototype.

2.2 Representation of the prototype

The prototype representation that we described is localist: there is a one-to-one correspondence
between the prototype for each category i and the random variable Di . To select the appropriate
prototype given a current category c, we deﬁned the unary vector hc and applied hc as a linear
transform on D. The identical operations can be performed in conjunction with a distributed repre-
sentation of the prototype. But we step back momentarily to motivate the distributed representation.

The localist representation suffers from a key weakness: it does not exploit interrelatedness con-
straints on category structure. The task given to experimental subjects speciﬁes that there are four
categories, and they have an ordering; the circle diameters associated with category A are smaller
than the diameters associated with B, etc. Consequently, dA < dB < dC < dD . One might make a
further assumption that the category prototypes are equally spaced. Exploiting these two sources of
domain knowledge leads to the distributed representation of category structure.
A simple sort of distributed representation involves deﬁning the prototype for category i not as di
but as a linear function of an underlying two-dimensional state-space representation of structure. In
this state space, d1 indicates the distance between categories and d2 an offset for all categories. This
representation of state can be achieved by applying Equation 1 and deﬁning hc = (nc , 1), where
nc is the ordinal position of the category (nA = 1, nB = 2, etc.). We augment this representation
with a bit of redundancy by incorporating not only the ordinal positions but also the reverse ordinal
positions; this addition yields a symmetry in the representation between the two ends of the ordinal
category scale. As a result of this augmentation, d becomes a three-dimensional state space, and
hc = (nc , N + 1 − nc , 1), where N is the number of categories.
To summarize, both the localist and distributed representations posit the existence of a hidden-state
space—unknown at the start of learning —that speciﬁes category prototypes. The localist model as-
sumes one dimension in the state space per prototype, whereas the distributed model assumes fewer
dimensions in the state space—three, in our proposal—than there are prototypes, and computes the
prototype location as a function of the state. Both localist and distributed representations assume a
ﬁxed, known {hc} that specify the interpretation of the state space, or, in the case of the distributed
model, the subject’s domain knowledge about category structure.

3 Simulation Methodology

We deﬁned a one-dimensional feature space in which categories A-D corresponded to the ranges
[1, 2), [2, 3), [3, 4), and [4, 5), respectively. In the human experiment, responses were considered
incorrect if they were smaller than A or larger than D; we call these two cases out-of-bounds-low
(OOBL) and out-of-bounds-high (OOBH). OOBL and OOBH were treated as two additional cate-
gories, resulting in 6 categories altogether for the simulation. Subjects and the model were never
asked to produce exemplars of OOBL or OOBH, but feedback was given if a response fell into
these categories. As in the human experiment, our simulation involved 480 trials. We performed
100 replications of each simulation with identical initial conditions but different trial sequences, and
averaged results over replications. All prototypes were initialized to have the same mean, 3.0, at the
start of the simulation. Because subjects had some initial practice on the task before the start of the
experimental trials, we provided the models with 12 initial trials of a categorization (not production)
task, two for each of the 6 categories. (For the MLGA models, it was necessary to use a large step
size on these trials to move the prototypes to roughly the correct neighborhood.)

To perform the production task, the models must generate an exemplar given a category. It seems
natural to draw an exemplar from the distribution in Equation 2 for P (E |C ). However, this distribu-

ABCD−0.200.2trial  t −1: Atrial  tprototype mvt.ABCD−0.200.2trial  t −1: Btrial  tABCD−0.200.2trial  t −1: Ctrial  tABCD−0.200.2trial  t −1: Dtrial  ttion reﬂects the full range of exemplars that lie within the category boundaries, and presumably in
the production task, subjects attempt to produce a prototypical exemplar. Consequently, we exclude
the intrinsic category variance, vc , from Equation 2 in generating exemplars, leaving variance only
via uncertainty about the prototype.

Each model involved selection of various parameters and initial conditions. We searched the pa-
rameter space by hand, attempting to ﬁnd parameters that satis ﬁed basic properties of the data: the
accuracy and response variance in the ﬁrst and second halves of the experiment. We report only pa-
rameters for the one model that was successful, the MLGA-Distrib: ψ = 0.0075, σ = 1.5 × 10−6
for off-diagonal terms and 1.5 × 10−7 for diagonal terms (the gradient for the diagonal terms was
relatively steep), Σ0 = 0.01I , and for all categories c, vc = 0.42 .

4 Results
4.1 Push effect

The phenomenon that most clearly distinguishes the models is the push effect. The push effect is
manifested in sequential-dependency functions, which plot the (relative) response on trial t as a
function of trial t − 1. As we explained using Figures 2a,b, the signature of the push effect is a
negatively sloped line for each of the different trial t target categories. The sequential-dependency
functions for the four models are presented in Figures 2c-f. KFU-Local (Figure 2c) produces a ﬂat
line, indicating no push whatsoever. The explanation for this result is straightforward: the Kalman
ﬁlter update alters only the variable that is responsible for the measurement (exemplar) obtained
on that trial. That variable is the prototype of the target class c, Dc . We thought the lack of an
interaction among the category prototypes might be overcome with KFU-Distrib, because with a
distributed prototype representation, all of the state variables jointly determine the target category
prototype. However, our intuition turned out to be incorrect. We experimented with many differ-
ent representations and parameter settings, but KFU-Distrib consistently obtained ﬂat or shallow
positive sloping lines (Figure 2d).

MLGA-Local (Figure 2e) obtains a push effect for neighboring classes, but not distant classes. For
example, examining the dashed magenta line, note that B is pushed away by A and C, but is not af-
fected by D. MLGA-Local maximizes the likelihood of the target category both by pulling the class-
conditional density of the target category toward the exemplar and by pushing the class-conditional
densities of the other categories away from the exemplar. However, if a category has little probabil-
ity mass at the location of the exemplar, the increase in likelihood that results from pushing it further
away is negligible, and consequently, so is the push effect.

MLGA-Distrib obtains a lovely result (Figure 2f)—a negatively-sloped line, diagnostic of the push
effect. The effect magnitude matches that in the human data (Figure 2a), and captures the key
property that the push effect increases with the ordinal distance of the categories. We did not build a
mechanism into MLGA-Distrib to produce the push effect; it is somewhat of an emergent property
of the model. The state representation of MLGA-Distrib has three components: d1 , the weight of
the ordinal position of a category prototype, d2 , the weight of the reverse ordinal position, and d3 ,
an offset. The last term, d3 , cannot be responsible for a push effect, because it shifts all prototypes
equally, and therefore can only produce a ﬂat sequential dependency function. Figure 4 helps provide
an intuition how d1 and d2 work together to produce the push effect. Each graph shows the average
movement of the category prototype (units on the y-axis are arbitrary) observed on trial t, for each
of the four categories, following presentation of a given category on trial t − 1. Positve values on the
y axis indicate increases in the prototype (movement to the right in Figure 1), and negative values
decreases. Each solid vertical bar represents the movement of a given category prototype following
a trial in which the exemplar is larger than its current prototype; each open vertical bar represents
movement when the exemplar is to the left of its prototype. Notice that all category prototypes
get larger or smaller on a given trial. But over the course of the experiment, the exemplar should be
larger than the prototype as often as it is smaller, and the two shifts should sum together and partially
cancel out. The result is the value indicated by the small horizontal bar along each line. The balance
between the shifts in the two directions exactly corresponds to the push effect. Thus, the model
produce a push-effect graph, but it is not truly producing a push effect as was originally conceived
by the experimentalists. We are currently considering empirical consequences of this simulation
result. Figure 5 shows a trial-by-trial trace from MLGA-Distrib.

Figure 5: Trial-by-trial trace of MLGA-Distrib. (a) exemplars generated on one run of the simulation; (b) the
mean and (c) variance of the class prototype distribution for the 6 classes on one run; (d) mean proportion cor-
rect over 100 replications of the simulation; (e) push and pull effects, as measured by changes to the prototype
means: the upper (green) curve is the pull of the target prototype mean toward the exemplar, and the lower (red)
curve is the push of the nontarget prototype means away from the exemplar, over 100 replications; (f) category
posterior of the generated exemplar over 100 replications, reﬂecting gradient ascent in the posterior.

4.2 Other phenomena accounted for

MLGA-Distrib captures the other phenomena we listed at the outset of this paper. Like all of the
other models, MLGA-Distrib readily produces a pull effect, which is shown in the movement of
category prototypes in Figure 5e. More observably, a pull effect is manifested when two successive
trials of the same category are positively correlated: when trial t − 1 is to the left of the true category
prototype, trial t is likely to be to the left as well. In the human data, the correlation coefﬁcient over
the experiment is 0.524; in the model, the coefﬁcient is 0.496. The explanation for the pull effect is
apparent: moving the category prototype to the exemplar increases the category likelihood.

Although many learning effects in humans are based on error feedback, the experimental studies
showed that push and pull effects occur even in the absence of errors, as they do in MLGA-Distrib.
The model simply assumes that the target category it used to generate an exemplar is the correct
category when no feedback to the contrary is provided. As long as the likelihood gradient is nonzero,
category prototypes will be shifted.

Pull and push effects shrink over the course of the experiment in human studies, as they do in the
simulation. Figure 5e shows a reduction in both pull and push, as measured by the shift of the pro-
totype means toward or away from the exemplar. We measured the slope of MLGA-Distrib’s push
function (Figure 2f) for trials in the ﬁrst and second half of the simulation. The slope dropped from
−0.042 to −0.025, as one would expect from Figure 5e. (These slopes are obtained by combining
responses from 100 replications of the simulation. Consequently, each point on the push function
was an average over 6000 trials, and therefore the regression slopes are highly reliable.)

A quantitative, observable measure of pull is the standard deviation (SD) of responses. As push
and pull effects diminish, SDs should decrease. In human subjects, the response SDs in the ﬁrst
and second half of the experiment are 0.43 and 0.33, respectively. In the simulation, the response
SDs are 0.51 and 0.38. Shrink reﬂects the fact that the model is approaching a local optimum in
log likelihood, causing gradients—and learning steps—to become smaller. Not all model parameter
settings lead to shrink; as in any gradient-based algorithm, step sizes that are too large do not lead to
converge. However, such parameter settings make little sense in the context of the learning objective.

4.3 Model predictions

MLGA-Distrib produces greater pull of the target category toward the exemplar than push of the
neighboring categories away from the exemplar. In the simulation, the magnitude of the target pull—
measured by the movement of the prototype mean—is 0.105, contrasted with the neighbor push,

501001502002503003504004500246example(a)501001502002503003504004500246class prototype(b)50100150200250300350400450−6−4−20log(class variance)(c)501001502002503003504004500.40.60.81P(correct)(d)50100150200250300350400450−0.200.2shift (+=toward −=away)(e)501001502002503003504004500.40.60.81posterior(f)which is 0.017. After observing this robust result in the simulation, we found pertinent experimental
data. Using the categorization paradigm, Zotov et al. (2003) found that if the exemplar on trial t is
near a category border, subjects are more likely to produce an error if the category on trial t − 1 is
repeated (i.e., a pull effect just took place) than if the previous trial is of the neighboring category
(i.e., a push effect), even when the distance between exemplars on t − 1 and t is matched. The
greater probability of error translates to a greater magnitude of pull than push.

The experimental studies noted a phenomenon termed snap back. If the same target category is
presented on successive trials, and an error is made on the ﬁrst trial, subjects perform very accurately
on the second trial, i.e., they generate an exemplar near the true category prototype. It appears as
if subjects, realizing they have been slacking, reawaken and snap the category prototype back to
where it belongs. We tested the model, but observed a sort of anti snap back. If the model made
an error on the ﬁrst trial, the mean deviation was larger—not smaller—on the second trial: 0.40
versus 0.32. Thus, MLGA-Distrib fails to explain this phenomenon. However, the phenomenon is
not inconsistent with the model. One might suppose that on an error trial, subjects become more
attentive, and increased attention might correspond to a larger learning rate on an error trial, which
should yield a more accurate response on the following trial.

McLaren et al. (1995) studied a phenomenon in humans known as peak shift, in which subjects
are trained to categorize unidimensional stimuli into one of two categories. Subjects are faster
and more accurate when presented with exemplars far from the category boundary than those near
the boundary. In fact, they respond more efﬁciently to far exemplars than they do to the category
prototype. The results are characterized in terms of the prototype of one category being pushed
away from the prototype of the other category. It seems straightforward to explain these data in
MLGA-Distrib as a type of long-term push effect.

5 Related Work and Conclusions

Stewart, Brown, and Chater (2002) proposed an account of categorization context effects in which
responses are based solely on the relative difference between the previous and present exemplars.
No representation of the category prototype is maintained. However, classiﬁcation based solely
on relative difference cannot account for a diminished bias effects as a function of experience. A
long-term stable prototype representation, of the sort incorporated into our models, seems necessary.

We considered four models in our investigation, and the fact that only one accounts for the ex-
perimental data suggests that the data are nontrivial. All four models have principled theoretical
underpinnings, and they space they deﬁne may suggest other elegant frameworks for understanding
mechanisms of category learning. The successful model, MLDA-Distrib, offers a deep insight into
understanding multiple-category domains: category structure must be considered. MLGA-Distrib
exploits knowledge available to subjects performing the task concerning the ordinal relationships
among categories. A model without this knowledge, MLGA-Local, fails to explain data. Thus,
the interrelatedness of categories appears to provide a source of constraint that individuals use in
learning about the structure of the world.

Acknowledgments

This research was supported by NSF BCS 0339103 and NSF CSE-SMA 0509521. Support for the second
author comes from an NSERC fellowship.

References
Jones, M. N., & Mewhort, D. J. K. (2003). Sequential contrast and assimilation effects in categorization of
perceptual stimuli. Poster presented at the 44th Meeting of the Psychonomic Society. Vancouver, B.C.
Maybeck, P.S. (1979). Stochastic models, estimation, and control, Volume I. Academic Press.
McLaren, I. P. L., et al. (1995). Prototype effects and peak shift in categorization. JEP:LMC, 21, 662–673.
Stewart, N. Brown, G. D. A., & Chater, N. (2002). Sequence effects in categorization of simple perceptual
stimuli. JEP:LMC, 28, 3–11.
Zotov, V., Jones, M. N., & Mewhort, D. J. K. (2003). Trial-to-trial representation shifts in categorization. Poster
presented at the 13th Meeting of the Canadian Society for Brain, Behaviour, and Cognitive Science: Hamilton,
Ontario.


Learning with Hypergraphs: Clustering,
Classiﬁcation, and Embedding

Dengyong Zhou† , Jiayuan Huang‡ , and Bernhard Sch¨olkopf§
†NEC Laboratories America, Inc.
4 Independence Way, Suite 200, Princeton, NJ 08540, USA
‡School of Computer Science, University of Waterloo
Waterloo ON, N2L3G1, Canada
§Max Planck Institute for Biological Cybernetics
Spemannstr. 38, 72076 T¨ubingen, Germany
{dengyong.zhou, jiayuan.huang, bernhard.schoelkopf}@tuebingen.mpg.de

Abstract

We usually endow the investigated ob jects with pairwise relationships,
which can be illustrated as graphs. In many real-world problems, however,
relationships among the ob jects of our interest are more complex than pair-
wise. Naively squeezing the complex relationships into pairwise ones will
inevitably lead to loss of information which can be expected valuable for
our learning tasks however. Therefore we consider using hypergraphs in-
stead to completely represent complex relationships among the ob jects of
our interest, and thus the problem of learning with hypergraphs arises. Our
main contribution in this paper is to generalize the powerful methodology
of spectral clustering which originally operates on undirected graphs to hy-
pergraphs, and further develop algorithms for hypergraph embedding and
transductive classiﬁcation on the basis of the spectral hypergraph cluster-
ing approach. Our experiments on a number of benchmarks showed the
advantages of hypergraphs over usual graphs.

1 Introduction

In machine learning problem settings, we generally assume pairwise relationships among the
ob jects of our interest. An ob ject set endowed with pairwise relationships can be naturally
illustrated as a graph, in which the vertices represent the ob jects, and any two vertices that
have some kind of relationship are joined together by an edge. The graph can be undirected
or directed. It depends on whether the pairwise relationships among ob jects are symmetric
or not. A ﬁnite set of points in Euclidean space associated with a kernel matrix is a typical
example of undirected graphs. As to directed graphs, a well-known instance is the World
Wide Web. A hyperlink can be thought of as a directed edge because given an arbitrary
hyperlink we cannot expect that there certainly exists an inverse one, that is, the hyperlink
based relationships are asymmetric [20].
However, in many real-world problems, representing a set of complex relational ob jects as
undirected or directed graphs is not complete. For illustrating this point of view, let us
consider a problem of grouping a collection of articles into diﬀerent topics. Given an article,
assume the only information that we have is who wrote this article. One may construct
an undirected graph in which two vertices are joined together by an edge if there is at
least one common author of their corresponding articles (Figure 1), and then an undirected
graph based clustering approach is applied, e.g. spectral graph techniques [7, 11, 16]. The
undirected graph may be further embellished by assigning to each edge a weight equal to the

Figure 1: Hypergraph vs. simple graph. Left: an author set E = {e1 , e2 , e3 } and an article
set V = {v1 , v2 , v3 , v4 , v5 , v6 , v7 }. The entry (vi , ej ) is set to 1 if ej is an author of article
vi , and 0 otherwise. Middle: an undirected graph in which two articles are joined together
by an edge if there is at least one author in common. This graph cannot tell us whether
the same person is the author of three or more articles or not. Right: a hypergraph which
completely illustrates the complex relationships among authors and articles.

number of authors in common. The above method may sound natural, but within its graph
representation we obviously miss the information on whether the same person joined writing
three or more articles or not. Such information loss is unexpected because the articles by
the same person likely belong to the same topic and hence the information is useful for our
grouping task.
A natural way of remedying the information loss issue occurring in the above methodology
is to represent the data as a hypergraph instead. A hypergraph is a graph in which an edge
can connect more than two vertices [2]. In other words, an edge is a subset of vertices. In
what follows, we shall uniﬁedly refer to the usual undirected or directed graphs as simple
graphs. Moreover, without special mentioning, the referred simple graphs are undirected. It
is obvious that a simple graph is a special kind of hypergraph with each edge containing two
vertices only. In the problem of clustering articles stated before, it is quite straightforward to
construct a hypergraph with the vertices representing the articles, and the edges the authors
(Figure 1). Each edge contains all articles by its corresponding author. Even more than
that, we can consider putting positive weights on the edges to encode our prior knowledge
on authors’ work if we have. For instance, for a person working on a broad range of ﬁelds,
we may assign a relatively small value to his corresponding edge.
Now we can completely represent the complex relationships among ob jects by using hy-
pergraphs. However, a new problem arises. How to partition a hypergraph? This is the
main problem that we want to solve in this paper. A powerful technique for partitioning
simple graphs is spectral clustering. Therefore, we generalize spectral clustering techniques
to hypergraphs, more speciﬁcally, the normalized cut approach of [16]. Moreover, as in the
case of simple graphs, a real-valued relaxation of the hypergraph normalized cut criterion
leads to the eigendecomposition of a positive semideﬁnite matrix, which can be regarded
as an analogue of the so-called Laplacian for simple graphs (cf. [5]), and hence we sugges-
tively call it the hypergraph Laplacian. Consequently, we develop algorithms for hypergraph
embedding and transductive inference based on the hypergraph Laplacian.
There have actually existed a large amount of literature on hypergraph partitioning, which
arises from a variety of practical problems, such as partitioning circuit netlists [11], clustering
categorial data [9], and image segmentation [1]. Unlike the present work however, they
generally transformed hypergraphs to simple ones by using the heuristics we discussed in the
beginning or other domain-speciﬁc heuristics, and then applied simple graph based spectral
clustering techniques.
[9] proposed an iterative approach which was indeed designed for
hypergraphs. Nevertheless it is not a spectral method. In addition, [6] and [17] considered
propagating label distributions on hypergraphs.
The structure of the paper is as follows. We ﬁrst introduce some basic notions on hy-
pergraphs in Section 2.
In Section 3, we generalize the simple graph normalized cut to

hypergraphs. As shown in Section 4, the hypergraph normalized cut has an elegant prob-
abilistic interpretation based on a random walk naturally associated with a hypergraph.
In Section 5, we introduce the real-valued relaxation to approximately obtain hypergraph
normalized cuts, and also the hypergraph Laplacian derived from this relaxation. In section
6, we develop a spectral hypergraph embedding technique based on the hypergraph Lapla-
cian. In Section 7, we address transductive inference on hypergraphs, this is, classifying the
vertices of a hypergraph provided that some of its vertices have been labeled. Experimental
results are shown in Section 8, and we conclude this paper in Section 9.

2 Preliminaries

Let V denote a ﬁnite set of ob jects, and let E be a family of subsets e of V such that
∪e∈E = V . Then we call G = (V , E ) a hypergraph with the vertex set V and the hyperedge set
E . A hyperedge containing just two vertices is a simple graph edge. A weighted hypergraph
is a hypergraph that has a positive number w(e) associated with each hyperedge e, called
(cid:80)
the weight of hyperedge e. Denote a weighted hypergraph by G = (V , E , w). A hyperedge
e is said to be incident with a vertex v when v ∈ e. For a vertex v ∈ V , its degree is
{e∈E |v∈e} w(e). Given an arbitrary set S, let |S | denote the cardinality
deﬁned by d(v) =
of S. For a hyperedge e ∈ E , its degree is deﬁned to be δ(e) = |e|. We say that there is
a hyperpath between vertices v1 and vk when there is an alternative sequence of distinct
vertices and hyperedges v1 , e1 , v2 , e2 , . . . , ek−1 , vk such that {vi , vi+1 } ⊆ ei for 1 ≤ i ≤ k − 1.
A hypergraph is connected if there is a path for every pair of vertices.
In what follows,
(cid:80)
(cid:80)
the hypergraphs we mention are always assumed to be connected. A hypergraph G can
be represented by a |V | × |E | matrix H with entries h(v , e) = 1 if v ∈ e and 0 otherwise,
v∈V h(v , e).
e∈E w(e)h(v , e) and δ(e) =
called the incidence matrix of G. Then d(v) =
Let Dv and De denote the diagonal matrices containing the vertex and hyperedge degrees
respectively, and let W denote the diagonal matrix containing the weights of hyperedges.
Then the adjacency matrix A of hypergraph G is deﬁned as A = HW H T − Dv , where H T
is the transpose of H.

w(e)

.

vol ∂S :=

3 Normalized hypergraph cut
For a vertex subset S ⊂ V , let S c denote the compliment of S. A cut of a hypergraph
G = (V , E , w) is a partition of V into two parts S and S c . We say that a hyperedge e is cut
if it is incident with the vertices in S and S c simultaneously.
Given a vertex subset S ⊂ V , deﬁne the hyperedge boundary ∂S of S to be a hyperedge
(cid:80)
set which consists of hyperedges which are cut, i.e. ∂S := {e ∈ E |e ∩ S (cid:54)= ∅, e ∩ S c (cid:54)= ∅},
(cid:88)
and deﬁne the volume vol S of S to be the sum of the degrees of the vertices in S , that
v∈S d(v). Moreover, deﬁne the volume of ∂S by
is,vol S :=
|e ∩ S | |e ∩ S c |
δ(e)
e∈∂S
Clearly, we have vol ∂S = vol ∂S c . The deﬁnition given by Equation (1) can be understood
as follows. Let us imagine each hyperedge e as a clique, i.e. a fully connected subgraph.
For avoiding unnecessary confusion, we call the edges in such an imaginary subgraph the
subedges. Moreover, we assign the same weight w(e)/δ(e) to all subedges. Then, when a
hyperedge e is cut, there are |e ∩ S | |e ∩ S c | subedges are cut, and hence a single sum term
in Equation (1) is the sum of the weights over the subedges which are cut. Naturally, we
try to obtain a partition in which the connection among the vertices in the same cluster
(cid:182)
(cid:181)
is dense while the connection between two clusters is sparse. Using the above introduced
deﬁnitions, we may formalize this natural partition as
1
1
(2)
+
c(S ) := vol ∂S
argmin
.
vol S c
vol S
∅(cid:54)=S⊂V
For a simple graph, |e ∩ S | = |e ∩ S c | = 1, and δ(e) = 2. Thus the right-hand side of
Equation (2) reduces to the simple graph normalized cut [16] up to a factor 1/2. In what
follows, we explain the hypergraph normalized cut in terms of random walks.

(1)

4 Random walk explanation

p(u, v) =

,

=

w(e)

(3)

(4)

=

=
(cid:182)

π(u)p(u, v) =

We written c(S ) =

1
vol V
1
vol V

We associate each hypergraph with a natural random walk which has the transition rule as
follows. Given the current position u ∈ V , ﬁrst choose a hyperedge e over all hyperedges
incident with u with the probability proportional to w(e), and then choose a vertex v ∈ e
uniformly at random. Obviously, it generalizes the natural random walk deﬁned on simple
(cid:88)
graphs. Let P denote the transition probability matrix of this hypergraph random walk.
Then each entry of P is
h(v , e)
w(e) h(u, e)
δ(e) .
d(u)
e∈E
v HW D−1
In matrix notation, P = D−1
e H T . The stationary distribution π of the random
walk is
π(v) = d(v)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
vol V
which follows from that
w(e)h(u, e)h(v , e)
d(u)
(cid:88)
(cid:88)
δ(e)
vol V
u∈V
e∈E
u∈V
e∈E
u∈V
w(e)h(v , e) = d(v)
1
(cid:181)
vol V
vol V
e∈E
e∈E

w(e)h(u, e)h(v , e)
(cid:88)
d(u)δ(e)
h(u, e) h(v , e)
δ(e)
u∈V
1
1
(cid:88)
(cid:88)
+
vol S c / vol V
vol S/ vol V
d(v)
vol S
=
π(v),
vol V
vol V
v∈V
v∈S
(cid:88)
(cid:88)
(cid:88)
(cid:88)
that is, the ratio vol S/ vol V is the probability with which the random walk occupies some
vertex in S. Moreover, from Equations (3) and (4), we have
|e ∩ S | |e ∩ S c |
vol ∂S
w(e)
w(e)
(cid:88)
(cid:88)
(cid:88)
vol V
vol V
δ(e)
vol V
v∈e∩S c
u∈e∩S
e∈∂S
e∈∂S
h(v , e)
h(u, e)
w(e) d(u)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
d(u)
vol V
δ(e)
u∈e∩S
v∈e∩S c
e∈∂S
w(e) h(u, e)
h(v , e)
d(u)
δ(e)
d(u)
vol V
e∈S
v∈S c
u∈S
v∈S c
u∈S
that is, the ratio vol ∂S/ vol V is the probability with which one sees a jump of the random
walk from S to S c under the stationary distribution. From Equations (5) and (6), we can
understand the hypergraph normalized cut criterion as follows: looking for a cut such that
the probability with which the random walk crosses diﬀerent clusters is as small as possible
while the probability with which the random walk stays in the same cluster is as large as
possible. It is worth pointing out that the random walk view is consistent with that for
the simple graph normalized cut [13]. The consistency means that our generalization of the
normalized cut approach from simple graphs to hypergraphs is reasonable.

. From Equation (4), we have

h(u, e)h(v , e)
δ(e)

vol ∂S
vol V

=

π(u)p(u, v),

.

(5)

=

=

=

=

=

5 Spectral hypergraph partitioning
(cid:33)2
(cid:195)
As in [16], the combinatorial optimization problem given by Equation (2) is NP-complete,
(cid:88)
(cid:88)
and it can be relaxed (2) into a real-valued optimization problem
− f (v)(cid:112)
f (u)(cid:112)
(cid:88)
(cid:88)
(cid:112)
d(u)
d(v)
e∈E
{u,v}⊆e
f (v)
f 2 (v) = 1,
d(v) = 0.
v∈V
v∈V

argmin
f ∈R|V |

sub ject to

w(e)
δ(e)

1
2

w(e)
δ(e)

= 2f T ∆f .

−1/2
−1/2
and ∆ = I − Θ, where I denotes the
v HW D−1
(cid:33)2
(cid:195)
We deﬁne the matrices Θ = D
e H T D
(cid:88)
(cid:88)
v
identity matrix. Then it can be veriﬁed that
− f (v)(cid:112)
f (u)(cid:112)
d(u)
d(v)
e∈E
{u,v}⊆e
√
Note that this also shows that ∆ is positive semi-deﬁnite. We can check that the smallest
eigenvalue of ∆ is 0, and its corresponding eigenvector is just
d. Therefore, from standard
results in linear algebra, we know that the solution to the optimization problem is an
eigenvector Φ of ∆ associated with its smallest nonzero eigenvalue. Hence, the vertex set
is clustered into the two parts S = {v ∈ V |Φ(v) ≥ 0} and S c = {v ∈ V |Φ(v) < 0}. For a
(cid:179)
(cid:180)
simple graph, the edge degree matrix De reduces to 2I . Thus
1
∆ = I − 1
= I − 1
(Dv + A) D−1/2
v HW H T D−1/2
2 D−1/2
2 D−1/2
2
v
v
v
which coincides with the simple graph Laplacian up to a factor of 1/2. So we suggestively
call ∆ the hypergraph Laplacian.
As in [20] where the spectral clustering methodology is generalized from undirected to
directed simple graphs, we may consider generalizing the present approach to directed hy-
pergraphs [8]. A directed hypergraph is a hypergraph in which each hyperedge e is an
ordered pair (X, Y ) where X ⊆ V is the tail of e and Y ⊆ V \ X is the head. Directed
hypergraphs have been used to model various practical problems from biochemical networks
[15] to natural language parsing [12].

I − D−1/2
v

AD−1/2
v

=

,

6 Spectral hypergraph embedding
As in the simple graph case [4, 10], it is straightforward to extend the spectral hypergraph
clustering approach to k-way partitioning. Denote a k-way partition by (V1 , · · · , Vk ), where
(cid:80)k
V1 ∪ V2 ∪ · · · ∪ Vk = V , and Vi ∩ Vj = ∅ for all 1 ≤ i, j ≤ k . We may obtain a k-way
vol ∂Vi
partition by minimizing c(V1 , · · · , Vk ) =
over all k-way partitions. Similarly,
vol Vi
i=1
the combinatorial optimization problem can be relaxed into a real-valued one, of which the
solution can be any orthogonal basis of the linear space spanned by the eigenvectors of ∆
associated with the k smallest eigenvalues.
(cid:80)k
Theorem 1. Assume a hypergraph G = (V , E , w) with |V | = n. Denote the eigenvalues of
the Laplacian ∆ of G by λ1 ≤ λ2 ≤ · · · ≤ λn . Deﬁne ck (G) = min c(V1 , · · · , Vk ), where the
i=1 λi ≤ ck (G).
minimization is over al l k-way partitions. Then
Proof. Let ri be a n-dimensional vector deﬁned by ri (v) = 1 if v ∈ Vi , and 0 otherwise.
k(cid:88)
Then
i (Dv − HW D−1
e H T )ri
rT
rT
i Dv ri
i=1
k(cid:88)
ri , and fi = si /(cid:107)si (cid:107), where (cid:107) · (cid:107) denotes the usual Euclidean norm. Thus
i ∆fi = tr F T ∆F,
f T
i=1
where F = [f1 · · · fk ]. Clearly, F T F = I . If allowing the elements of ri to take arbitrary
k(cid:88)
continuous values rather than Boolean ones only, we have
i=1
The last equality is from standard results in linear algebra. This completes the proof.

ck (G) = min c(V1 , · · · , Vk ) ≥ min
F T F =I

c(V1 , · · · , Vk ) =

c(V1 , · · · , Vk ) =

Deﬁne si = D

tr F T ∆F =

−1/2
v

λi .

The above result also shows that the real-valued optimization problem derived from the
relaxation is actually a lower bound of the original combinatorial optimization problem.
Unlike 2-way partitioning however, it is unclear how to utilize multiple eigenvectors simul-
taneously to obtain a k-way partition. Many heuristics have been proposed in the situation
of simple graphs, and they can be applied here as well. Perhaps the most popular one among
them is as follows [14]. First form a matrix X = [Φ1 · · · Φk ], where Φi ’s are the eigenvectors
of ∆ associated with the k smallest eigenvalues. And then the row vectors of X are regarded
as the representations of the graph vertices in k-dimensional Euclidian space. Those vectors
corresponding to the vertices are generally expected to be well separated, and consequently
we can obtain a good partition simply by running k-means on them once. [18] has resorted
to a semideﬁnite relaxation model for the k-way normalized cut instead of the relatively
loose spectral relaxation, and then obtained a more accurate solution. It sounds reasonable
to expect that the improved solution will lead to improved clustering. As reported in [18],
however, the expected improvement does not occur in practice.

7 Transductive inference

We have established algorithms for spectral hypergraph clustering and embedding. Now
we consider transductive inference on hypergraphs. Speciﬁcally, given a hypergraph G =
(V , E , w), the vertices in a subset S ⊂ V have labels in L = {1, −1}, our task is to predict
the labels of the remaining unlabeled vertices. Basically, we should try to assign the same
label to all vertices contained in the same hyperedge. It is actually straightforward to derive
a transductive inference approach from a clustering scheme. Let f : V (cid:55)→ R denote a
classiﬁcation function, which assigns a label sign f (v) to a vertex v ∈ V . Given an ob jective
functional Ω(·) from some clustering approach, one may choose a classiﬁcation function by
{Remp (f ) + µΩ(f )},
argmin
f ∈R|V |
where Remp (f ) denotes a chosen empirical loss, such as the least square loss or the hinge
loss, and the number µ > 0 the regularization parameter. Since in general normalized
cuts are thought to be superior to mincuts, the transductive inference approach that we
used in the later experiments is built on the above spectral hypergraph clustering method.
Consequently, as shown in [20], with the least square loss function, the classiﬁcation function
is ﬁnally given by f = (I − ξΘ)−1 y , where the elements of y denote the initial labels, and ξ
is a parameter in (0, 1). For a survey on transductive inference, we refer the readers to [21].

8 Experiments

All datasets except a particular version of the 20-newsgroup one are from the UCI Ma-
chine Learning Depository. They are usually referred to as the so-called categorical data.
Speciﬁcally, each instance in those datasets is described by one or more attributes. Each
attribute takes only a small number of values, each corresponding to a speciﬁc category.
Attribute values cannot be naturally ordered linearly as numerical values can [9]. In our
experiments, we constructed a hypergraph for each dataset, where attribute values were
regarded as hyperedges. The weights for all hyperedges were simply set to 1. How to choose
suitable weights is deﬁnitely an important problem requiring additional exploration how-
ever. We also constructed a simple graph for each dataset, and the simple graph spectral
clustering based approach [19] was then used as the baseline. Those simple graphs were
constructed in the way discussed in the beginning of Section 1, which is essentially to deﬁne
pairwise relationships among the ob jects by the adjacency matrices of hypergraphs. The
ﬁrst task we addressed is to embed the animals in the zoo dataset into Euclidean space. This
dataset contains 100 animals with 17 attributes. The attributes include hair, feathers,
eggs, milk, legs, tail, etc. The animals have been manually classiﬁed into 7 diﬀerent
categories. We embedded those animals into Euclidean space by using the eigenvectors of
the hypergraph Laplacian associated with the smallest eigenvalues (Figure 2). For the an-
imals having the same attributes, we randomly chose one as their representative to put in
the ﬁgures. It is apparent that those animals are well separated in their Euclidean repre-
sentations. Moreover, it deserves a further look that seal and dolphin are signiﬁcantly

Figure 2: Embedding the zoo dataset. Left panel: the eigenvectors with the 2nd and 3rd
smallest eigenvalues; right panel: the eigenvectors with the 3rd and 4th smallest eigenvalues.
Note that dolphin is between class 1 (denoted by ◦) containing the animals having milk
and living on land, and class 4 (denoted by (cid:166)) containing the animals living in sea.

(a) mushroom

(b) 20-newsgroup

(c) letter

(d) α (letter)

‘

Figure 3: Classiﬁcation on complex relational data. (a)-(c) Results from both the hyper-
graph based approach and the simple graph based approach. (d) The inﬂuence of the α in
letter recognition with 100 labeled instances.

mapped to the positions between class 1 consisting of the animals having milk and living
on land, and class 4 consisting of the animals living in sea. A similar observation also holds
for seasnake. The second task is classiﬁcation on the mushroom dataset that contains 8124
instances described by 22 categorical attributes, such as shape, color, etc. We remove the
11th attribute that has missing values. Each instance is labeled as edible or poisonous.
They contain 4208 and 3916 instances separately. The third task is text categorization on
a modiﬁed 20-newsgroup dataset with binary occurrence values for 100 words across 16242
articles (see http://www.cs.toronto.edu/~roweis). The articles belong to 4 diﬀerent top-
ics corresponding to the highest level of the original 20 newsgroups, with the sizes being
4605, 3519, 2657 and 5461 respectively. The ﬁnal task is to guess the letter categories with
the letter dataset, in which each instance is described by 16 primitive numerical attributes
(statistical moments and edge counts). We used a subset containing the instances of the
letters from A to E with the sizes being 789, 766, 736, 805 and 768 respectively. The exper-
imental results of the above three tasks are shown in Figures 3(a)-3(c). The regularization
parameter α is ﬁxed at 0.1. Each testing error is averaged over 20 trials. The results show
that the hypergraph based method is consistently better than the baseline. The inﬂuence
of the α used in the letter recognition task is shown in Figure 3(d). It is interesting that
the α inﬂuences the baseline much more than the hypergraph based approach.

9 Conclusion

We generalized spectral clustering techniques to hypergraphs, and developed algorithms for
hypergraph embedding and transductive inference. It is interesting to consider applying the
present methodology to a broader range of practical problems. We are particularly interested
in the following problems. One is biological network analysis [17]. Biological networks are

−0.2−0.15−0.1−0.0500.050.10.15−0.2−0.15−0.1−0.0500.050.10.150.20.25bearcarpcavyclamcrabdeerdogfishdolphindoveflamingofleafroggirlgnatgorillagullhawkhoneybeehouseflykiwiladybirdlionlobsterminknewtoctopusostrichpenguinpitviperplatypusponypussycatscorpionseahorsesealsealionseasnakeseawaspslowwormsquirrelstarfishstingrayswantoadtortoisetuataravampirewaspworm−0.2−0.15−0.1−0.0500.050.10.150.20.25−0.15−0.1−0.0500.050.10.150.20.25bearpussycatcarpcavyclamcrabdeerdogfishdolphinflamingofleafroggirlgnatgorillagullhawkhoneybeehouseflykiwiladybirdlionlobsterminknewtoctopusostrichpenguinpitviperplatypusponyscorpionseahorsesealsealionseasnakeseawaspslowwormsquirrelstingrayswantoadtortoisetuataravampirewaspwormstarfishdove204060801001201401601802000.10.150.20.250.3# labeled pointstest errorhypergraphsimple graph204060801001201401601802000.140.160.180.20.22# labeled pointstest errorhypergraphsimple graph204060801001201401601802000.120.140.160.180.20.220.24# labeled pointstest errorhypergraphsimple graph0.10.20.30.40.50.60.70.80.90.140.160.180.20.220.240.260.28different valuetest errorhypergraphsimple graphmainly modeled as simple graphs so far.
It might be more sensible to model them as
hypergraphs instead such that complex interactions will be completely taken into account.
The other is social network analysis. As recently pointed out by [3], many social transactions
are supra-dyadic; they either involve more than two actors or they involve numerous aspects
of the setting of interaction. So standard network techniques are not adequate in analyzing
these networks. Consequently, they resorted to the concept of a hypergraph, and showed
how the concept of network centrality can be adapted to hypergraphs.

References

[1] S. Agarwal, L. Zelnik-Manor J. Lim, P. Perona, D. Kriegman, and S. Belongie. Beyond pairwise
clustering. In IEEE Conf. on Computer Vision and Pattern Recognition, 2005.
[2] C. Berge. Hypergraphs. North-Holland, Amsterdam, 1989.
[3] P. Bonacich, A.C. Holdren, and M. Johnston. Hyper-edges and multi-dimensional centrality.
Social Networks, 26(3):189–203, 2004.
[4] P.K. Chan, M.D.F. Schlag, and J. Zien. Spectral k-way ratio cut partitioning and clustering.
IEEE Trans. on Computer Aided Design of Integrated Circuits and Systems, 13(9):1088–1096,
1994.
[5] F. Chung. Spectral Graph Theory. Number 92 in CBMS Regional Conference Series in Math-
ematics. American Mathematical Society, Providence, RI, 1997.
[6] A. Corduneanu and T. Jaakkola. Distributed information regularization on graphs. In Advances
in Neural Information Processing Systems 17, Cambridge, MA, 2005. MIT Press.
[7] M. Fiedler. Algebraic connectivity of graphs. Czechoslovak Mathematical Journal, 23(98):298–
305, 1973.
[8] G. Gallo, G. Longo, and S. Pallottino. Directed hypergraphs and applications. Discrete Applied
Mathematics, 42(2):177–201, 1993.
[9] D. Gibson, J. Kleinberg, and P. Raghavan. Clustering categorical data: An approach based
on dynamical systems. VLDB Journal, 8(3-4):222–236, 2000.
[10] M. Gu, H. Zha, C. Ding, X. He, and H. Simon. Spectral relaxation models and structure anal-
ysis for k-way graph clustering and bi-clustering. Technical Report CSE-01-007, Department
of Computer Science and Engineering, Pennsylvania State University, 2001.
[11] L. Hagen and A.B. Kahng. New spectral methods for ratio cut partitioning and clustering.
IEEE Trans. on Computed-Aided Desgin of Integrated Circuits and Systems, 11(9):1074–1085,
1992.
[12] D. Klein and C. Manning. Parsing and hypergraphs. In Proc. 7th Intl. Workshop on Parsing
Technologies, 2001.
[13] M. Meila and J. Shi. A random walks view of spectral segmentation.
Workshop on Artiﬁcial Intel ligence and Statistics, 2001.
[14] A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. In
Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.
[15] J.S. Oliveira, J.B. Jones-Oliveira, D.A. Dixon, C.G. Bailey, and D.W. Gull. Hyperdigraph–
Theoretic analysis of the EGFR signaling network: Initial steps leading to GTP: Ras complex
formation. Journal of Computational Biology, 11(5):812–842, 2004.
[16] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Tran. on Pattern Analysis
and Machine Intel ligence, 22(8):888–905, 2000.
[17] K. Tsuda. Propagating distributions on a hypergraph by dual information regularization. In
Proc. 22th Intl. Conf. on Machine Learning, 2005.
[18] E.P. Xing and M.I. Jordan. On semideﬁnite relaxation for normalized k-cut and connections to
spectral clustering. Technical Report CSD-03-1265, Division of Computer Science, University
of California, Berkeley, 2003.
[19] D. Zhou, O. Bousquet, T.N. Lal, J. Weston, and B. Sch¨olkopf. Learning with local and global
consistency. In Advances in Neural Information Processing Systems 16, Cambridge, MA, 2004.
MIT Press.
[20] D. Zhou, J. Huang, and B. Sch¨olkopf. Learning from labeled and unlabeled data on a directed
graph. In Proc. 22th Intl. Conf. on Machine Learning, 2005.
[21] X. Zhu. Semi-supervised learning literature survey. Technical Report Computer Sciences 1530,
University of Wisconsin - Madison, 2005.

In Proc. 8th Intl.


A Novel Gaussian Sum Smoother for Approximate
Inference in Switching Linear Dynamical Systems

David Barber and Bertrand Mesot
IDIAP Research Institute
Martigny 1920, Switzerland
david.barber/bertrand.mesot@idiap.ch

Abstract

We introduce a method for approximate smoothed inference in a class of switching
linear dynamical systems, based on a novel form of Gaussian Sum smoother. This
class includes the switching Kalman Filter and the more general case of switch
transitions dependent on the continuous latent state. The method improves on the
standard Kim smoothing approach by dispensing with one of the key approxima-
tions, thus making fuller use of the available future information. Whilst the only
central assumption required is projection to a mixture of Gaussians, we show that
an additional conditional independence assumption results in a simpler but stable
and accurate alternative. Unlike the alternative unstable Expectation Propagation
procedure, our method consists only of a single forward and backward pass and is
reminiscent of the standard smoothing ‘correction’ recursions in the simpler linear
dynamical system. The algorithm performs well on both toy experiments and in a
large scale application to noise robust speech recognition.

1 Switching Linear Dynamical System

The Linear Dynamical System (LDS) [1] is a key temporal model in which a latent linear process
generates the observed series. For complex time-series which are not well described globally by a
single LDS, we may break the time-series into segments, each modeled by a potentially different
LDS. This is the basis for the Switching LDS (SLDS) [2, 3, 4, 5] where, for each time t, a switch
variable st ∈ 1, . . . , S describes which of the LDSs is to be used. The observation (or ‘visible’)
vt ∈ RV is linearly related to the hidden state ht ∈ RH with additive noise η by
vt = B (st )ht + ηv (st )
p(vt |ht , st ) = N (B (st )ht , Σv (st ))
(1)
≡
where N (µ, Σ) denotes a Gaussian distribution with mean µ and covariance Σ. The transition
dynamics of the continuous hidden state ht is linear,
p(ht |ht−1 , st ) = N (cid:0)A(st )ht−1 , Σh (st )(cid:1)
ht = A(st )ht−1 + ηh (st ),
(2)
≡
The switch st may depend on both the previous st−1 and ht−1 . This is an augmented SLDS
(aSLDS), and de ﬁnes the model

p(v1:T , h1:T , s1:T ) =

p(vt |ht , st )p(ht |ht−1 , st )p(st |ht−1 , st−1 )

T
Yt=1
The standard SLDS[4] considers only switch transitions p(st |st−1 ). At time t = 1, p(s1 |h0 , s0 )
simply denotes the prior p(s1 ), and p(h1 |h0 , s1 ) denotes p(h1 |s1 ).
The aim of this article is to address how to perform inference in the aSLDS. In particular we desire
the ﬁltered estimate p(ht , st |v1:t ) and the smoothed estimate p(ht , st |v1:T ), for any 1 ≤ t ≤ T .
Both ﬁltered and smoothed inference in the SLDS is intractab le, scaling exponentially with time [4].

s1

h1

v1

s2

h2

v2

s3

h3

v3

s4

h4

v4

Figure 1: The independence structure of the aSLDS. Square nodes denote discrete variables, round
nodes continuous variables. In the SLDS links from h to s are not normally considered.

2 Expectation Correction

Our approach to approximate p(ht , st |v1:T ) mirrors the Rauch-Tung-Striebel ‘correction’ smoother
for the simpler LDS [1].The method consists of a single forward pass to recursively ﬁnd the ﬁltered
posterior p(ht , st |v1:t ), followed by a single backward pass to correct this into a smoothed posterior
p(ht , st |v1:T ). The forward pass we use is equivalent to standard Assumed Density Filtering (ADF)
[6]. The main contribution of this paper is a novel form of backward pass, based only on collapsing
the smoothed posterior to a mixture of Gaussians. Together with the ADF forward pass, we call the
method Expectation Correction, since it corrects the moments found from the forward pass. A more
detailed description of the method, including pseudocode, is given in [7].

2.1 Forward Pass (Filtering)

Readers familiar with ADF may wish to continue directly to Section (2.2). Our aim is to form a
recursion for p(st , ht |v1:t ), based on a Gaussian mixture approximation of p(ht |st , v1:t ). Without
loss of generality, we may decompose the ﬁltered posterior a s

p(ht , st |v1:t ) = p(ht |st , v1:t )p(st |v1:t )

(3)

The exact representation of p(ht |st , v1:t ) is a mixture with O(S t ) components. We therefore ap-
proximate this with a smaller I -component mixture

p(ht |st , v1:t ) ≈

p(ht |it , st , v1:t )p(it |st , v1:t )

I
Xit=1
where p(ht |it , st , v1:t ) is a Gaussian parameterized with mean f (it , st ) and covariance F (it , st ). To
ﬁnd a recursion for these parameters, consider
p(ht+1 |st+1 , v1:t+1 ) = Xst ,it
Evaluating p(ht+1 |st , it , st+1 , v1:t+1 )

p(ht+1 |st , it , st+1 , v1:t+1 )p(st , it |st+1 , v1:t+1 )

(4)

We
distribution
joint
the
computing
ﬁrst
by
ﬁnd
p(ht+1 |st , it , st+1 , v1:t+1 )
p(ht+1 , vt+1 |st , it , st+1 , v1:t ), which is a Gaussian with covariance and mean elements,
Σhh = A(st+1 )F (it , st )AT (st+1 ) + Σh (st+1 ), Σvv = B (st+1 )ΣhhB T (st+1 ) + Σv (st+1 )
Σvh = B (st+1 )F (it , st ),
µv = B (st+1 )A(st+1 )f (it , st ),
µh = A(st+1 )f (it , st )

(5)

and then conditioning on vt+1

1 . For the case S = 1, this forms the usual Kalman Filter recursions[1].

Evaluating p(st , it |st+1 , v1:t+1 )

The mixture weight in (4) can be found from the decomposition

p(st , it |st+1 , v1:t+1 ) ∝ p(vt+1 |it , st , st+1 , v1:t )p(st+1 |it , st , v1:t )p(it |st , v1:t )p(st |v1:t ) (6)

1 p(x|y ) is a Gaussian with mean µx + Σxy Σ−1
yy (y − µy ) and covariance Σxx − Σxy Σ−1
yy Σyx .

The ﬁrst factor in (6), p(vt+1 |it , st , st+1 , v1:t ) is a Gaussian with mean µv and covariance Σvv , as
given in (5). The last two factors p(it |st , v1:t ) and p(st |v1:t ) are given from the previous iteration.
Finally, p(st+1 |it , st , v1:t ) is found from
(7)
p(st+1 |it , st , v1:t ) = hp(st+1 |ht , st )ip(ht |it ,st ,v1:t )
In the SLDS, (7) is replaced by the Markov
where h·ip denotes expectation with respect to p.
transition p(st+1 |st ). In the aSLDS, however, (7) will generally need to be computed numerically.

Closing the recursion

p(ht+1 |st+1 , v1:t+1 ) ≈

p(ht+1 |it+1 , st+1 , v1:t+1 )p(it+1 |st+1 , v1:t+1 )

We are now in a position to calculate (4). For each setting of the variable st+1 , we have a mixture
of I × S Gaussians which we numerically collapse back to I Gaussians to form
I
Xit+1=1
Any method of choice may be supplied to collapse a mixture to a smaller mixture; our code
simply repeatedly merges low-weight components.
In this way the new mixture coefﬁcients
p(it+1 |st+1 , v1:t+1 ), it+1 ∈ 1, . . . , I are de ﬁned, completing the description of how to form a
recursion for p(ht+1 |st+1 , v1:t+1 ) in (3). A recursion for the switch variable is given by
p(st+1 |v1:t+1 ) ∝ Xst ,it
where all terms have been computed during the recursion for p(ht+1 |st+1 , v1:t+1 ).
The likelihood p(v1:T ) may be found by recursing p(v1:t+1 ) = p(vt+1 |v1:t )p(v1:t ), where
p(vt+1 |vt ) = Xit ,st ,st+1
2.2 Backward Pass (Smoothing)

p(vt+1 |it , st , st+1 , v1:t )p(st+1 |it , st , v1:t )p(it |st , v1:t )p(st |v1:t )

p(vt+1 |st+1 , it , st , v1:t )p(st+1 |it , st , v1:t )p(it |st , v1:t )p(st |v1:t )

p(ht |ht+1 , st , st+1 , v1:t )p(ht+1 |st , st+1 , v1:T )

p(st+1 |v1:T )p(ht |st , st+1 , v1:T )p(st |st+1 , v1:T )

The main contribution of this paper is to ﬁnd a suitable way to ‘correct’ the ﬁltered posterior
p(st , ht |v1:t ) obtained from the forward pass into a smoothed posterior p(st , ht |v1:T ). We derive
this for the case of a single Gaussian representation. The extension to the mixture case is straight-
forward and presented in [7]. We approximate the smoothed posterior p(ht |st , v1:T ) by a Gaussian
with mean g (st ) and covariance G(st ) and our aim is to ﬁnd a recursion for these parameters. A
useful starting point for a recursion is:
p(ht , st |v1:T ) = Xst+1
The term p(ht |st , st+1 , v1:T ) may be computed as
p(ht |st , st+1 , v1:T ) = Zht+1
The recursion therefore requires p(ht+1 |st , st+1 , v1:T ), which we can write as
(9)
p(ht+1 |st , st+1 , v1:T ) ∝ p(ht+1 |st+1 , v1:T )p(st |st+1 , ht+1 , v1:t )
The difﬁculty here is that the functional form of p(st |st+1 , ht+1 , v1:t ) is not squared exponential
in ht+1 , so that p(ht+1 |st , st+1 , v1:T ) will not be Gaussian2 . One possibility would be to approx-
imate the non-Gaussian p(ht+1 |st , st+1 , v1:T ) by a Gaussian (or mixture thereof) by minimizing
the Kullback-Leilbler divergence between the two, or performing moment matching in the case of
a single Gaussian. A simpler alternative (which forms ‘standard’ EC) is to make the assumption
p(ht+1 |st , st+1 , v1:T ) ≈ p(ht+1 |st+1 , v1:T ), where p(ht+1 |st+1 , v1:T ) is already known from the
previous backward recursion. Under this assumption, the recursion becomes
p(ht , st |v1:T ) ≈ Xst+1
p(st+1 |v1:T )p(st |st+1 , v1:T ) hp(ht |ht+1 , st , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T ) (10)
2 In the exact calculation, p(ht+1 |st , st+1 , v1:T ) is a mixture of Gaussians, see [7]. However, since in (9) the
two terms p(ht+1 |st+1 , v1:T ) will only be approximately computed during the recursion, our approximation to
p(ht+1 |st , st+1 , v1:T ) will not be a mixture of Gaussians.

(8)

Evaluating hp(ht |ht+1 , st , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T )

hp(ht |ht+1 , st , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T ) is a Gaussian in ht , whose statistics we will now com-
pute. First we ﬁnd p(ht |ht+1 , st , st+1 , v1:t ) which may be obtained from the joint distribution

(11)

p(ht , ht+1 |st , st+1 , v1:t ) = p(ht+1 |ht , st+1 )p(ht |st , v1:t )
which itself can be found from a forward dynamics from the ﬁlt ered estimate p(ht |st , v1:t ). The
statistics for the marginal p(ht |st , st+1 , v1:t ) are simply those of p(ht |st , v1:t ), since st+1 carries no
extra information about ht . The remaining statistics are the mean of ht+1 , the covariance of ht+1
and cross-variance between ht and ht+1 , which are given by
hht+1 i = A(st+1 )ft (st ), Σt+1,t+1 = A(st+1 )Ft (st )AT (st+1 )+Σh(st+1 ), Σt+1,t = A(st+1 )Ft (st )
Given the statistics of (11), we may now condition on ht+1 to ﬁnd p(ht |ht+1 , st , st+1 , v1:t ). Doing
so effectively constitutes a reversal of the dynamics,
←−
A (st , st+1 )ht+1 + ←−η (st , st+1 )
ht =
←−
←−
A (st , st+1 ) and ←−η (st , st+1 ) ∼ N (←−m (st , st+1 ),
where
Σ (st , st+1 )) are easily found using
conditioning. Averaging the above reversed dynamics over p(ht+1 |st+1 , v1:T ), we ﬁnd that
hp(ht |ht+1 , st , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T ) is a Gaussian with statistics
←−
←−
←−
A (st , st+1 )g (st+1 )+←−m (st , st+1 ), Σt,t =
A T (st , st+1 )+
µt =
A (st , st+1 )G(st+1 )
These equations directly mirror the standard RTS backward pass[1].

←−
Σ (st , st+1 )

Evaluating p(st |st+1 , v1:T )

The main departure of EC from previous methods is in treating the term

p(st |st+1 , v1:T ) = hp(st |ht+1 , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T )
The term p(st |ht+1 , st+1 , v1:t ) is given by

(12)

p(st |ht+1 , st+1 , v1:t ) =

p(ht+1 |st+1 , st , v1:t )p(st , st+1 |v1:t )
t , v1:t )p(s′
p(ht+1 |st+1 , s′
Ps′
t , st+1 |v1:t )
t
Here p(st , st+1 |v1:t ) = p(st+1 |st , v1:t )p(st |v1:t ), where p(st+1 |st , v1:t ) occurs in the forward pass,
(7). In (13), p(ht+1 |st+1 , st , v1:t ) is found by marginalizing (11).
Computing the average of (13) with respect to p(ht+1 |st+1 , v1:T ) may be achieved by any numer-
ical integration method desired. A simple approximation is to evaluate the integrand at the mean
value of the averaging distribution p(ht+1 |st+1 , v1:T ). More sophisticated methods (see [7]) such
as sampling from the Gaussian p(ht+1 |st+1 , v1:T ) have the advantage that covariance information
is used3 .

(13)

Closing the Recursion

p(st+1 |st , v1:T )p(ht |st , st+1 , v1:T )

We have now computed both the continuous and discrete factors in (8), which we wish to use to
write the smoothed estimate in the form p(ht , st |v1:T ) = p(st |v1:T )p(ht |st , v1:T ). The distribution
p(ht |st , v1:T ) is readily obtained from the joint (8) by conditioning on st to form the mixture
p(ht |st , v1:T ) = Xst+1
which may then be collapsed to a single Gaussian (the mixture case is discussed in [7]). The
smoothed posterior p(st |v1:T ) is given by
p(st |v1:T ) = Xst+1
3This is a form of exact sampling since drawing samples from a Gaussian is easy. This should not be
confused with meaning that this use of sampling renders EC a sequential Monte-Carlo scheme.

p(st+1 |v1:T ) hp(st |ht+1 , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T ) .

(14)

2.3 Relation to other methods

The EC Backward pass is closely related to Kim’s method [8]. In both EC and Kim’s method, the
approximation p(ht+1 |st , st+1 , v1:T ) ≈ p(ht+1 |st+1 , v1:T ), is used to form a numerically simple
backward pass. The other ‘approximation’ in EC is to numerically compute the average in (14).
In Kim’s method, however, an update for the discrete variables is formed by replacing the required
term in (14) by

hp(st |ht+1 , st+1 , v1:t )ip(ht+1 |st+1 ,v1:T ) ≈ p(st |st+1 , v1:t )

(15)

Since p(st |st+1 , v1:t ) ∝ p(st+1 |st )p(st |v1:t )/p(st+1 |v1:t ), this can be computed simply from the
ﬁltered results alone. The fundamental difference therefo re between EC and Kim’s method is that
the approximation, (15), is not required by EC. The EC backward pass therefore makes fuller use of
the future information, resulting in a recursion which intimately couples the continuous and discrete
variables. The resulting effect on the quality of the approximation can be profound, as we will see
in the experiments.

The Expectation Propagation (EP) algorithm makes the central assumption of collapsing the pos-
teriors to a Gaussian family [5]; the collapse is de ﬁned by a c onsistency criterion on overlapping
marginals. In our experiments, we take the approach in [9] of collapsing to a single Gaussian. En-
suring consistency requires frequent translations between moment and canonical parameterizations,
which is the origin of potentially severe numerical instability [10]. In contrast, EC works largely
with moment parameterizations of Gaussians, for which relatively few numerical difﬁculties arise.
Unlike EP, EC is not based on a consistency criterion and a subtle issue arises about possible incon-
sistencies in the Forward and Backward approximations for EC. For example, under the conditional
independence assumption in the Backward Pass, p(hT |sT −1 , sT , v1:T ) ≈ p(hT |sT , v1:T ), which
is in contradiction to (5) which states that the approximation to p(hT |sT −1 , sT , v1:T ) will depend
on sT −1 . Such potential inconsistencies arise because of the approximations made, and should not
be considered as separate approximations in themselves. Rather than using a global (consistency)
objective, EC attempts to faithfully approximate the exact Forward and Backward propagation rou-
tines. For this reason, as in the exact computation, only a single Forward and Backward pass are
required in EC.

In [11] a related dynamics reversed is proposed. However, the singularities resulting from incorrectly
treating p(vt+1:T |ht , st ) as a density are heuristically ﬁnessed.
In [12] a variational method approximates the joint distribution p(h1:T , s1:T |v1:T ) rather than the
marginal inference p(ht , st |v1:T ). This is a disadvantage when compared to other methods that
directly approximate the marginal.

Sequential Monte Carlo methods (Particle Filters)[13], are essentially mixture of delta-function ap-
proximations. Whilst potentially powerful, these typically suffer in high-dimensional hidden spaces,
unless techniques such as Rao-Blackwellization are performed. ADF is generally preferential to
Particle Filtering since in ADF the approximation is a mixture of non-trivial distributions, and is
therefore more able to represent the posterior.

3 Demonstration

Testing EC in a problem with a reasonably long temporal sequence, T , is important since numerical
instabilities may not be apparent in timeseries of just a few points. To do this, we sequentially
generate hidden and visible states from a given model, here with H = 3, S = 2, V = 1 – see
Figure(2) for full details of the experimental setup. Then, given only the parameters of the model and
the visible observations (but not any of the hidden states h1:T , s1:T ), the task is to infer p(ht |st , v1:T )
and p(st |v1:T ). Since the exact computation is exponential in T , a simple alternative is to assume
that the original sample states s1:T are the ‘correct’ inferences, and compare how our most probable
posterior smoothed estimates arg maxst p(st |v1:T ) compare with the assumed correct sample st . We
chose conditions that, from the viewpoint of classical signal processing, are difﬁcult, with changes
in the switches occurring at a much higher rate than the typical frequencies in the signal vt .
For EC we use the mean approximation for the numerical integration of (12). We included the Par-
ticle Filter merely for a point of comparison with ADF, since they are not designed to approximate

1000

800

600

400

200

0

0

PF

RBPF

EP

ADFS

KimS

ECS

ADFM

KimM

ECM

10

20

0

10

20

0

10

20

0

10

20

0

10

20

0

10

20

0

10

20

0

10

20

0

10

20

Figure 2: The number of errors in estimating p(st |v1:T ) for a binary switch (S = 2) over a time
series of length T = 100. Hence 50 errors corresponds to random guessing. Plotted are histograms
of the errors are over 1000 experiments. The x-axes are cut off at 20 errors to improve visualization
of the results. (PF) Particle Filter. (RBPF) Rao-Blackwellized PF. (EP) Expectation Propagation.
(ADFS) Assumed Density Filtering using a Single Gaussian. (KimS) Kim’s smoother using the
results from ADFS. (ECS) Expectation Correction using a Single Gaussian (I = J = 1). (ADFM)
ADF using a multiple of I = 4 Gaussians. (KimM) Kim’s smoother using the results from ADFM.
(ECM) Expectation Correction using a mixture with I = J = 4 components. S = 2, V = 1
(scalar observations), T = 100, with zero output bias. A(s) = 0.9999 ∗ orth(randn(H, H)),
B (s) = randn(V, H). H = 3, Σh (s) = IH , Σv (s) = 0.1IV , p(st+1 |st ) ∝ 1S×S + IS . At time
t = 1, the priors are p1 = uniform, with h1 drawn from N (10 ∗ randn(H, 1), IH ).

the smoothed estimate, for which 1000 particles were used, with Kitagawa resampling. For the Rao-
Blackwellized Particle Filter [13], 500 particles were used, with Kitagawa resampling. We found
that EP4 was numerically unstable and often struggled to converge. To encourage convergence, we
used the damping method in [9], performing 20 iterations with a damping factor of 0.5. Neverthe-
less, the disappointing performance of EP is most likely due to con ﬂicts resulting from numerical
instabilities introduced by the frequent conversions between moment and canonical representations.

The best ﬁltered results are given using ADF, since this is be tter able to represent the variance
in the ﬁltered posterior than the sampling methods. Unlike K im’s method, EC makes good use
of the future information to clean up the ﬁltered results con siderably. One should bear in mind
that both EC and Kim’s method use the same ADF ﬁltered results . This demonstrates that EC
may dramatically improve on Kim’s method, so that the small amount of extra work in making a
numerical approximation of p(st |st+1 , v1:T ), (12), may bring signiﬁcant bene ﬁts. We found similar
conclusions for experiments with an aSLDS[7].

4 Application to Noise Robust ASR

Here we brie ﬂy present an application of the SLDS to robust Au tomatic Speech Recognition (ASR),
for which the intractable inference is performed by EC, and serves to demonstrate how EC scales
well to a large-scale application. Fuller details are given in [14]. The standard approach to noise
robust ASR is to provide a set of noise-robust features to a standard Hidden Markov Model (HMM)
classiﬁer, which is based on modeling the acoustic feature v ector. For example, the method of Un-
supervised Spectral Subtraction (USS) [15] provides state-of-the-art performance in this respect.
Incorporating noise models directly into such feature-based HMM systems is difﬁcult, mainly be-
cause the explicit in ﬂuence of the noise on the features is po orly understood. An alternative is to
model the raw speech signal directly, such as the SAR-HMM model [16] for which, under clean con-
ditions, isolated spoken digit recognition performs well. However, the SAR-HMM performs poorly
under noisy conditions, since no explicit noise processes are taken into account by the model.

The approach we take here is to extend the SAR-HMM to include an explicit noise process, so that
the observed signal vt is modeled as a noise corrupted version of a clean hidden signal vh
t :

t + ˜ηt with ˜ηt ∼ N (0, ˜σ2 )
vt = vh

4Generalized EP [5], which groups variables together improves on the results, but is still far inferior to the
EC results presented here – Onno Zoeter personal communicat ion.

Noise Variance
0
10−7
10−6
10−5
10−4
10−3

SNR (dB)
26.5
26.3
25.1
19.7
10.6
0.7

HMM SAR-HMM AR-SLDS
100.0%
97.0%
96.8%
96.8%
79.8%
100.0%
96.4%
56.7%
90.9%
94.8%
22.2%
86.4%
59.1%
9.7%
84.0%
61.2%
9.1%
9.1%

Table 1: Comparison of the recognition accuracy of three models when the test utterances are cor-
rupted by various levels of Gaussian noise.

The dynamics of the clean signal is modeled by a switching AR process

vh
t =

t−r + ηh
cr (st )vh
t (st ),

. . .

t (st ) ∼ N (0, σ2 (st ))
ηh

R
Xr=1
where st ∈ {1, . . . , S } denotes which of a set of AR coefﬁcients cr (st ) are to be used at time t,
and ηh
t (st ) is the so-called innovation noise. When σ2 (st ) ≡ 0, this model reproduces the SAR-
HMM of [16], a specially constrained HMM. Hence inference and learning for the SAR-HMM are
tractable and straightforward. For the case σ2 (st ) > 0 the model can be recast as an SLDS. To do
this we de ﬁne ht as a vector which contains the R most recent clean hidden samples
T
ht = (cid:2) vh
t−r+1 (cid:3)
vh
t
and we set A(st ) to be an R × R matrix where the ﬁrst row contains the AR coefﬁcients −cr (st )
and the rest is a shifted down identity matrix. For example, for a third order (R = 3) AR process,
A(st ) = " −c1(st ) −c2 (st ) −c3 (st )
# .
1
0
0
0
1
0
The hidden covariance matrix Σh (s) has all elements zero, except the top-left most which is set
to the innovation variance. To extract the ﬁrst component of ht we use the (switch independent)
1 × R projection matrix B = [ 1
0 . . . 0 ]. The (switch independent) visible scalar noise
variance is given by Σv ≡ σ2
v .
A well-known issue with raw speech signal models is that the energy of a signal may vary from one
speaker to another or because of a change in recording conditions. For this reason the innovation Σh
is adjusted by maximizing the likelihood of an observed sequence with respect to the innovation
covariance, a process called GainAdaptation [16].

(16)

(17)

4.1 Training & Evaluation

Following [16], we trained a separate SAR-HMM for each of the eleven digits (0 – 9 and ‘oh’) from
the TI-DIGITS database [17]. The training set for each digit was composed of 110 single digit
utterances down-sampled to 8 kHz, each one pronounced by a male speaker. Each SAR-HMM was
composed of ten states with a left-right transition matrix. Each state was associated with a 10th-
order AR process and the model was constrained to stay an integer multiple of K = 140 time steps
(0.0175 seconds) in the same state. We refer the reader to [16] for a detailed explanation of the
training procedure used with the SAR-HMM.

An AR-SLDS was built for each of the eleven digits by copying the parameters of the correspond-
ing trained SAR-HMM, i.e., the AR coefﬁcients cr (s) are copied into the ﬁrst row of the hidden
transition matrix A(s) and the same discrete transition distribution p(st | st−1 ) is used. The models
were then evaluated on a test set composed of 112 corrupted utterances of each of the eleven digits,
each pronounced by different male speakers than those used in the training set. The recognition
accuracy obtained by the models on the corrupted test sets is presented in Table 1. As expected, the
performance of the SAR-HMM rapidly decreases with noise. The feature-based HMM with USS
has high accuracy only for high SNR levels. In contrast, the AR-SLDS achieves a recognition accu-
racy of 61.2% at a SNR close to 0 dB, while the performance of the two other methods is equivalent

to random guessing (9.1%). Whilst other inference methods may also perform well in this case, we
found that EC performs admirably, without numerical instabilities, even for time-series with several
thousand time-steps.

5 Discussion

We presented a method for approximate smoothed inference in an augmented class of switching
linear dynamical systems. Our approximation is based on the idea that due to the forgetting which
commonly occurs in Markovian models, a ﬁnite number of mixtu re components may provide a
reasonable approximation. Clearly, in systems with very long correlation times our method may
require too many mixture components to produce a satisfactory result, although we are unaware of
other techniques that would be able to cope well in that case. The main bene ﬁt of EC over Kim
smoothing is that future information is more accurately dealt with. Whilst EC is not as general as
EP, EC carefully exploits the properties of singly-connected distributions, such as the aSLDS, to
provide a numerically stable procedure. We hope that the ideas presented here may therefore help
facilitate the practical application of dynamic hybrid networks.

Acknowledgements

This work is supported by the EU Project FP6-0027787. This paper only re ﬂects the authors’ views
and funding agencies are not liable for any use that may be made of the information contained herein.

References

[1] Y. Bar-Shalom and Xiao-Rong Li. Estimation and Tracking : Principles, Techniques and Software. Artech
House, Norwood, MA, 1998.
[2] V. Pavlovic, J. M. Rehg, and J. MacCormick. Learning switching linear models of human motion. In
Advances in Neural Information Processing systems (NIPS 13), pages 981–987, 2001.
[3] A. T. Cemgil, B. Kappen, and D. Barber. A Generative Model for Music Transcription. IEEE Transactions
on Audio, Speech and Language Processing, 14(2):679 – 694, 2006.
[4] U. N. Lerner. Hybrid Bayesian Networks for Reasoning about Complex Systems. PhD thesis, Stanford
University, 2002.
[5] O. Zoeter. Monitoring non-linear and switching dynamical systems. PhD thesis, Radboud University
Nijmegen, 2005.
[6] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT Media Lab, 2001.
[7] D. Barber. Expectation Correction for Smoothed Inference in Switching Linear Dynamical Systems.
Journal of Machine Learning Research, 7:2515–2540, 2006.
[8] C-J. Kim. Dynamic linear models with Markov-switching. Journal of Econometrics, 60:1–22, 1994.
[9] T. Heskes and O. Zoeter. Expectation Propagation for approximate inference in dynamic Bayesian net-
works. In A. Darwiche and N. Friedman, editors, Uncertainty in Art. Intelligence, pages 216–223, 2002.
[10] S. Lauritzen and F. Jensen. Stable local computation with conditional Gaussian distributions. Statistics
and Computing, 11:191–203, 2001.
[11] G. Kitagawa. The Two-Filter Formula for Smoothing and an implementation of the Gaussian-sum
smoother. Annals of the Institute of Statistical Mathematics, 46(4):605–623, 1994.
[12] Z. Ghahramani and G. E. Hinton. Variational learning for switching state-space models. Neural Compu-
tation, 12(4):963–996, 1998.
[13] A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods in Practice. Springer, 2001.
[14] B. Mesot and D. Barber. Switching Linear Dynamical Systems for Noise Robust Speech Recognition.
IDIAP-RR 08, 2006.
[15] G. Lathoud, M. Magimai-Doss, B. Mesot, and H. Bourlard. Unsupervised spectral subtraction for noise-
robust ASR. In Proceedings of ASRU 2005, pages 189–194, November 2005.
[16] Y. Ephraim and W. J. J. Roberts. Revisiting autoregressive hidden Markov modeling of speech signals.
IEEE Signal Processing Letters, 12(2):166–169, February 2005.
[17] R.G. Leonard. A database for speaker independent digit recognition.
volume 3, 1984.

In Proceedings of ICASSP84,


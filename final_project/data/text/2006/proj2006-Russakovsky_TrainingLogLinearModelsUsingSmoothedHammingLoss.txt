Training Log Linear Models using Smoothed Hamming Loss

Olga Russakovsky, in collaboration with Samuel Gross, Chuong Do, Sera(cid:12)m Batzoglou

December 15, 2006

1

Introduction

In a paper that is to appear in NIPS this year [1], we proposed a new ob jective function for training Con-
ditional Random Fields (CRFs). When using the traditional log-likelihood training, the training ob jective
function is fundamentally di(cid:11)erent from the testing ob jective (the accuracy of the resulting parse). We
wanted to develop a method of training CRFs where the training and testing ob jectives would be more
similar. We proposed using a di(cid:11)erentiable approximation of the Hamming loss as the training ob jective. In
order to make the approach feasible, we derived a dynamic programming algorithm to e(cid:14)ciently calculate
the gradient of this new function. We then used BFGS to train the CRF with respect to this ob jective,
and test its performance on sequence labeling problems. Using the trained parameters, we parsed new,
unlabeled sequences by choosing, for each position independently, the label with the highest posterior proba-
bility. The results were very promising, showing that especially on di(cid:14)cult problems, where at each position
the posterior probabilities of the di(cid:11)erent labels were very similar, the maximum accuracy function signi(cid:12)-
cantly outperformed the log-likelihood ob jective. We also tested our idea on a large-scale problem of gene
prediction, and showed that this new ob jective function is much more resistant to noisy labels in the data.
For this pro ject, we wanted to see if this idea can generalize to other applications as well, such as RNA
folding or NLP part-of-speech parsing. However, both of those problems can’t be solved with a linear
sequence labeling model, and so we are using a more general tree-structured model instead.

2 Algorithmic idea

2.1 Notation

Let X L denote an input space of all possible input sequences, and let Y LxL denote the space of all possible
output labels (a label can be assigned to a span (i, j ) of the sequence).

2.2 Conditional Log-Linear Models

We de(cid:12)ne the conditional probability of a labeling y given an input sequence x as
exp (cid:16)Pn2T1;L;y
wT f (; x)(cid:17)
exp (cid:16)Pn2T1;L;y0 wT f (n; x)(cid:17)
X
02Y LxL

Pw (y j x) =

y

=

exp (cid:0)wT F1;L (x; y)(cid:1)
Z (x)

;

(1)

where we de(cid:12)ne n 2 Ta;b;y as the nodes in the parse tree corresponding to the labeling y covering the span
a, b of the sequence x, the summed feature mapping, Fa;b (x; y) = Pn2Ta;b;y
f (n; x) and where the Z (x) is
the partition function used to normalize the distribution.

1

2.3 Relationship to Stochastic Context Free Grammars

CLLMs can be used in the framework of Stochastic Context Free Grammars (SCFGs) SCFGs include pro-
duction rules r1 , r2 , ... rn and probabilities p1 , p2 , ..., pn corresponding to each rule. Consider sequences
generated according to this grammar. Then for a particular parse y corresponding to some sequence x,

P (y) = Y
i
where ni (y) is the number of times ri occurs in the parse y.
Now consider a CLLM model incorporating this SCFG. The features of each node in the parse tree thus
depend on the production rule used. Furthermore, consider setting

pni (y)
i

(2)

F1;L (x; y) = n
w = log(p)

(3)

(4)

In other words, the feature count of the ith feature in the CLLM is ni , and the weight of that feature is the
log(pi ). Then it can be shown that the two expressions for the probability of the parse (corresponding to
the CLLM and the SCFG) are equivalent.
Finally, from now on assume that the grammar we’re working with is unambiguous, so there is a unique
translation from the given labeling y to a parse tree.

2.4 Maximum accuracy

Now, instead of training this model to maximize the log likelihood of the data, as is conventional, we instead
want to train it to maximize the total accuracy of the labeling (which is the ob jective we care about in the
(cid:12)rst place). In particular, let D = fx(t) ; y(t) gm
t=1 be the set of training examples. We wish to maximize

A(w : D) =

m
X
t=1

X
n2T
1;L;y(t)

1[P (yn = y (t)
n jx(t) ) > P (yn = ajx(t) ) for all a 6= y (t)
n ]

(5)

In other words, for each node in the parse tree corresponding to an input labeling y (t) , we add 1 to the
ob jective if our CLLM correctly predicts the label for the span of that node. However, we can’t do the
maximization directly on a step function. Instead we let Q be a di(cid:11)erentiable function arbitrarily similar to
the step function I (x) = 1[x > 0], and instead maximize

~A(w : D) =

=

m
X
t=1

m
X
t=1

X
n2T
1;L;y(t)

Q[P (yn = y (t)
n jx(t) ) > P (yn = ajx(t) ) for all a 6= y (t)
n ]

(6)

X
n2T
1;L;y(t)

Q[ X
y2Y LxL :yn=y

(t)
n

exp(wT F(x(t) ; y))
Z (x)

(cid:0) max

a6=y

n X
(t)
y2Y LxL :yn=a

exp(wT F(x(t) ; y))
Z (x)

]

(7)

Now we use the chain rule to compute the gradient of A with respect to w. For the sake of space, we
omit the discussion of the dynamic programming algorithms used to e(cid:14)ciently compute the gradient of this
function; however, the equations are fairly straight-forward generalizations of the method presented in [1],
except that now they are similar to the inside/outside algorithms of Context Free Grammars instead of the
forward/backward algorithms. For the function Q, we use

Q(x; (cid:21)) =

1
1 + exp((cid:0)(cid:21)x)

(8)

As (cid:21) ! 1, Q(x; (cid:21)) ! 1fx > 0g, so ~A(w : D) ! A(w : D). However, the approximation ~A(w : D) is smooth
for any (cid:21) > 0.

2

2.5 Training

Within this framework, we’re currently using the L-BFGS algorithm to optimize the parameters of the model.
In the future, we’re considering experimenting with di(cid:11)erent algorithms; however, this algorithm worked well
for the linear sequence-labeling case, so we’re continuing to use it for now. Note that one drawback of the
proposed ob jective function is its non-convexity. In an attempt to avoid falling into a local minimum while
training, we (cid:12)rst optimize the parameters using the convex likelihood function, and then attempt to improve
on them using the maximum accuracy algorithm. This is the same approach that was used for the gene
prediction experiments.

3 RNA folding

We are planning to apply this model to various di(cid:11)erent applications. The one we’re focusing on currently
is RNA secondary structure prediction. In particular, we would like to extend the CONTRAfold[2] pro ject
to encorporate this new training algorithm.

3.1 Biology background

An RNA is a single-stranded linear molecule of nucleic acids that conveys genetic information. The nucleic
acid alphabet consists of just 4 characters: A, C, G and U. The RNA molecule can fold on itself as bonds
between these nucleotides form. The resulting secondary structure of RNA is very important for its function,
as it determines the types of interactions it can have with other molecules around it. Therefore, being able
to accurately predict RNA secondary structure is a necessary step to being able to understand the function
of the molecule. For the purposes of this pro ject, we’re interested in correctly predicting

1. the pairings within the molecule (nucleotide i pairs with and bonds to nucleotide j )

2. the unpaired nucleotides

The tradeo(cid:11) between the value of these two occurrences will be controlled by a parameter (cid:11), because
predicting a nucleotide pairing correctly is much more important to determining the overall structure of the
molecule than predicting that a certain nucleotide is unpaired; however, it should be strictly worse to predict
an incorrect pairing than to not make a prediction.

3.2 Context free grammars

The problem of RNA secondary structure prediction (cid:12)ts very nicely into the framework of context free
grammars. In particular, consider the following grammar proposed by [3] and referred to as the G6 grammar :

S ! LS jL

L ! aF ^aja
F ! aF ^ajLS

(9)

(10)
(11)

where a, ^a can correspond to any of the 4 nucleotides. This grammar is the (cid:12)rst step in testing out the
algorithm, since it is much simpler than a real grammar which would be used by an RNA folder, yet it is
unambiguous and has been shown to work quite well nevertheless [3].

3.3 Limitations

One of the limitations of this approach is that it can’t predict pseudoknots or any other structure that is
not completely nested. However, no method is currently able to predict these structures with a reasonable
time complexity, and, furthermore, these structures are fairly rare so high accuracy can be achieved even by
programs that don’t take them into account.

3

4 Results

The results are still pending, however, there are a couple of experiments that we’re in the process of running.
For these tests, we’re using RNA sequences from the Rfam database, using the same selection criteria as in
[2]. Thus only the structures that have been generated by biological experiments and not those predicted by
a computational program are going to be used.
In the immediate future, we plan on accomplishing the following tasks:

1. Comparing the performance of maximum accuracy and likelihood parsing using the simple G6 grammar

2. Encoding the more complicated grammar used in CONTRAfold, and comparing the performance
directly to the published results

3. Comparing the speed of our more generalized model with CONTRAfold

Preliminary results should be available by the end of next week.

5 Future Directions

Furthermore, some other options that we’re going to explore are

1. Evaluating the tradeo(cid:11) of (cid:21) { the potential gain in accuracy from using a more exact approximation
versus the longer training time and even the potential loss of accuracy as the function becomes harder
to optimize. One option to consider is increasing (cid:21) slowly as the training progresses.

2. Evaluating the performace of other ob jective functions besides approximate accuracy and likelihood {
e.g. max-margin methods

3. Generalizing from RNA secondary structure prediction to a more complicated model of NLP part of
speech tagging.

Overall, this is really just the beginning of this work. The goal is to conclude the pro ject and produce
publishable results within the next two quarters.

6 References

1. Gross, S.S, Russakovsky, O, Do, C.B, Batzoglou, S. Training Conditional Random Fields for Maximum
Labelwise Accuracy. To appear in Neural Information Processing Systems (NIPS) 2006.

2. Do, C.B, Woods, D.A, and Batzoglou, S. (2006). CONTRAfold: RNA Secondary Structure Prediction
without Energy-Based Models. Bioinformatics, 22(14):e90-e98.

3. Dowell, R.D. and Eddy, S. R. Evaluation of Several Lightweight Stochastic Context-Free Grammars
for RNA Secondary Structure Prediction. BMC Bioinformatics, 5:71, 2004.

4


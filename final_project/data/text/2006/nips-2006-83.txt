A Kernel Subspace Method by Stochastic Realization
for Learning Nonlinear Dynamical Systems

Yoshinobu Kawahara∗
Dept. of Aeronautics & Astronautics
The University of Tokyo

Takehisa Yairi Kazuo Machida
Research Center for Advanced Science and Technology
The University of Tokyo

Komaba 4-6-1, Meguro-ku, Tokyo, 153-8904 JAPAN
{kawahara,yairi,machida}@space.rcast.u-tokyo.ac.jp

Abstract

In this paper, we present a subspace method for learning nonlinear dynamical
systems based on stochastic realization, in which state vectors are chosen using
kernel canonical correlation analysis, and then state-space systems are identi ﬁed
through regression with the state vectors. We construct the theoretical underpin-
ning and derive a concrete algorithm for nonlinear identi ﬁc ation. The obtained
algorithm needs no iterative optimization procedure and can be implemented on
the basis of fast and reliable numerical schemes. The simulation result shows that
our algorithm can express dynamics with a high degree of accuracy.

1

Introduction

Learning dynamical systems is an important problem in several ﬁelds including engineering, phys-
ical science and social science. The objectives encompass a spectrum ranging from the control of
target systems to the analysis of dynamic characterization, and for several decades, system identi-
ﬁcation for acquiring mathematical models from obtained in put-output data has been researched in
numerous ﬁelds, such as system control.

Dynamical systems are learned by, basically, two different approaches. The ﬁrst approach is based
on the principles of minimizing suitable distance functions between data and chosen model classes.
Well-known and widely accepted examples of such functions are likelihod functions [1] and the av-
erage squared prediction-errors of observed data. For multivariate models, however, this approach
is known to have several drawbacks. First, the optimization tends to lead to an ill-conditioned esti-
mation problem because of the over-parameterization, i.e., minimum parameters (called canonical
forms) do not exist in multivariate systems. Second, the minimization, except in trivial cases, can
only be carried out numerically using iterative algorithms. This often leads to there being no guaran-
tee of reaching a global minimum and high computational costs. The second approach is a subspace
method which involves geometric operations on subspaces spanned by the column or row vectors
of certain block Hankel matrices formed by input-output data [2,3]. It is well known that subspace
methods require no a priori choice of identi ﬁable parameter izations and can be implemented by fast
and reliable numerical schemes.

The subspace method has been actively researched throughout the last few decades and several
algorithms have been proposed, which are, for representative examples, based on the orthogonal
decomposition of input-output data [2,4] and on stochastic realization using canonical correlation
analysis [5]. Recently, nonlinear extensions have begun to be discussed for learning systems that
cannot be modeled sufﬁciently with linear expressions. How ever, the nonlinear algorithms that

∗URL: www.space.rcast.u-tokyo.ac.jp/kawahara/index e.html

have been proposed to date include only those in which models with speci ﬁc nonlinearities are as-
sumed [6] or those which need complicated nonlinear regression [7,8]. In this study, we extend the
stochastic-realization-based subspace method [5] to the nonlinear regime by developing it on repro-
ducing kernel Hilbert spaces [9], and derive a nonlinear subspace identi ﬁcation algorithm which can
be executed by a procedure similar to that in the linear case.

The outline of this paper is as follows. Section 2 gives some theoretical materials for the subspace
identi ﬁcation of dynamical systems with reproducing kerne ls. In section 3, we give some approx-
imations for deriving a practical algorithm, then describe the algorithm speci ﬁcally in section 4.
Finally, an empirical result is presented in section 5, and we give conclusions in section 6.

Notation Let x, y and z be random vectors, then denote the covariance matrix of x and y by Σxy
and the conditional covariance matrix of x and y conditioned on z by Σxy |z . Let a be a vector in
a Hilbert space, and B , C Hilbert spaces. Then, denote the orthogonal projection of a onto B by
a/B and the oblique projection of a onto B along C by a/C B . Let A be an [m × n] matrix, then
L{A} := {Aα|α ∈ Rn } will be referred to as the column space and L{A0 } := {A0α|α ∈ Rm }
the row space of A. •0 denotes the transpose of a matrix •, and Id ∈ Rd×d is the identity matrix.

2 Rationales

2.1 Problem Description and Some Deﬁnitions

Consider two discrete-time wide-sense stationary vector processes {u(t), y(t), t = 0, ±1, · · · } with
dimensions nu and ny , respectively. The ﬁrst component u(t) models the input signal while the
second component y(t) models the output of the unknown stochastic system, which we want to
construct from observed input-output data, as a nonlinear state-space system:

(1)

x(t + 1) = g(x(t), u(t)) + v
y(t) = h(x(t), u(t)) + w ,
where x(t) ∈ Rn is the state vector, and v and w are the system and observation noises. Throughout
this paper, we shall assume that the joint process (u, y) is a stationary and purely nondeterministic
full rank process [3,5,10]. It is also assumed that the two processes are zero-mean and have ﬁnite
joint covariance matrices. A basic step in solving this realization problem, which is also the core
of the subspace identi ﬁcation algorithm presented later, i s the construction of a state space of the
system.
In this paper, we will derive a practical algorithm for this problem based on stochastic
realization with reproducing kernel Hilbert spaces.
We denote the joint input-output process w(t)0 = [y(t)0 , u(t)0 ] ∈ Rnw (nw = nu + ny ) and feature
maps φu : Ut → Fu ∈ Rnφu , φy : Yt → Fy ∈ Rnφy and φw : Wt → Fw ∈ Rnφw with the
Mercer kernels ku , ky and kw , where Ut , Yt and Wt are the Hilbert spaces generated by the second-
order random variables u(t), y(t) and w(t), and Fy , Fu and Fw are the respective feature spaces.
Moreover, we deﬁne the future output, input and the past inpu t-output vectors in the feature spaces
as
f φ (t) := (cid:2)φy (y(t))0 , φy (y(t + 1))0 , · · · , φy (y(t + l − 1))0 (cid:3)0
∈ Rlnφ
y ,
+ (t) := [φu (u(t))0 , φu (u(t + 1))0 , · · · , φu (u(t + l − 1))0 ]0 ∈ Rlnφ
uφ
u ,
pφ (t) := [φw (w(t − 1))0 , φw (w(t − 2))0 , · · · ]0 ∈ R∞ ,
and the Hilbert spaces generated by these random variables as:
P φ
t = span{φ(u(τ ))|τ ≥ t}, Y φ+
t = span{φ(w(τ ))|τ < t}, U φ+
t = span{φ(y(τ ))|τ ≥ t}.
(3)
U φ−
and Y φ−
are deﬁned similarly. These spaces are assumed to be closed w ith respect to the
t
t
root-mean-square norm kξk := [E {ξ 2 }]1/2 , where E {·} denotes the expectation value, and thus are
thought of as Hilbert subspaces of an ambient Hilbert space H φ := U φ ∨ Y φ containing all linear
functionals of the joint process in the feature spaces (φu (u), φy (y)).

(2)

2.2 Optimal Predictor in Kernel Feature Space

First, we require the following technical assumptions [3,5].

U φ+
t

f φ (t)

Ψuφ
+ (t))

!!

φ

ˆf

(t)

Figure 1: Optimal predictor ˆf
(t) of future output
t ∨ U +φ
in feature space based on P φ
.
t

φ

0

Πpφ (t)

P φ
t

A S SUM P T ION 1. The input u is ‘exogenous’, i.e., no feedback from the output y to the input u.
A S SUM P T ION 2. The input process is ‘sufﬁciently rich’. More precisely, at each time t, the input
t ∩ U +
(cid:0)U −
t + U +
space Ut has the direct sum decomposition Ut = U −
t = {0}(cid:1).
t
Note that assumption 2 implies that the input process is purely nondeterministic and admits a spectral
density matrix without zeros on the unit circle (i.e., coercive). This is too restrictive in many practical
situations and we can instead assume only a persistently exciting (PE) condition of sufﬁciently high
order and ﬁnite dimensionality for an underlying “true” sys
tem from the outset. Then, we can give
the following proposition which enables us to develop a subspace method in feature space, as in the
linear case.
PRO PO S IT ION 1. If assumptions 1 and 2 are satis ﬁed, then similar conditions in the feature spaces
described below are ful ﬁlled:
(1) There is no feedback from φy (y) to φu (u).
(2) U φ
t has the direct sum decomposition U φ
t = U φ−
t + U φ+
t

∩ U φ+
t = {0})

(U φ−
t

PROO F. Condition (2) is shown straightforwardly from assumption 2 and the properties of the
|U −
t ⊥ Y −
reproducing kernel Hilbert spaces. As U +
(derived from assumption 1) and
t
t
Y −/U +
t ∨ U −
t = Y −
t /U −
t are equivalent, if the orthogonal complement of Ut is denoted byU ⊥
,
t
. Now, when representing Y −φ
t = U −
we can obtain Y −
using the input space on fea-
t + U ⊥
t
t
ture space U φ
t and the orthogonal complement U ⊥φ
, we can write Y −φ
t = U −φ
t + U ⊥φ
because
t
t
U φ
t +U +φ
t = U −φ
from condition (2), U +
, and owing to the properties of the reproducing
t ⊥ U ⊥
t
t
|U −φ
t ⊥ Y −φ
kernel Hilbert spaces. Therefore, U +φ
can be shown by tracing inversely.
t
t

Using proposition 1, we now obtain the following representation result.
φ
TH EOR EM 1. Under assumptions 1 and 2, the optimal predictor ˆf
(t) of the future output vector
t ∨ U φ+
in feature space f φ (t) based on P φ
is uniquely given by the sum of the oblique projections:
t
φ
ˆf
t = Πpφ (t) + Ψuφ
t ∨ U φ+
(t) = f φ (t) /P φ
+ (t),
in which Π and Ψ satisfy the discrete Wiener-Hopf-type equations

(4)

ΠΣφp φp |φu = Σφf φp |φu , ΨΣφu φu |φp = Σφf φu |φp .

(5)

PROO F. From proposition 1, the proof can be carried out as in the linear case (cf. [3,5]).

2.3 Construction of State Vector

f , Σφp φp |φu =
Let Lf , Lp be the square root matrices of Σφf φf |φu , Σφp φp |φu , i.e., Σφf φf |φu = Lf L0
p , and assume that the SVD of the normalized conditional covariance is given by
LpL0
L−1
f Σφf φp |φu (L−1
p )0 = U SV 0 ,
where S ∈ Rlnφy ×nφp is the matrix with all entries being zero, except the leading diagonal, which
has the entries ρi satisfying ρ1 ≥ · · · ≥ ρn > 0 for n = min(lnφy , nφp ), and U , V are square
orthogonal.

(6)

We deﬁne the extended observability and controllability ma trices

O := Lf U S 1/2 , C := S 1/2V 0L0
p ,
where rank(O) = rank(R) = n. Then, from the SVD of Eq.
(6), the block Hankel matrix
Σφf φp |φu has the classical rank factorization Σφf φp |φu = OC . If a ’state vector’ is now deﬁned to
be the n-dimensional vector
x (t) = CΣ−1
φp φp |φu

p pφ (t),
pφ (t) = S 1/2V 0L−1

(7)

(8)

P φ
it is readily seen that x(t) is a basis for the stationary oblique predictor space Xt := Y φ+
t ,
/U φ+
t
t
which, on the basis of general geometric principles, can be shown to be a minimal state space for the
process φy (y), as in the linear case [3,5]. This is also assured by the fact that the oblique projection
of f φ (t) onto U φ+
along P φ
t can be expressed, using Eqs. (5), (7) and (8), as
t
P φ
t = Πpφ (t) = Σφf φp |φu Σ−1
pφ (t) = Ox(t)
φp φp |φu

f φ (t)/U φ+
t
and rank(O) = n, and the variance matrix of x(t) is nonsingular. In terms of x(t), the optimal
φ
predictor ˆf
(t) in Eq. (4) has the form

(9)

φ

ˆf

= Ox(t) + Ψuφ
+ (t).
It is seen that x(t) is a conditional minimal sufﬁcient statistic carrying exac tly all the information
contained in P φ
t that is necessary for estimating the future outputs, given the future inputs.
In analogy with the linear case [3,5], the output process in feature space φy (y(t)) now admits a
minimal stochastic realization with the state vector x(t) of the form

(10)

x(t + 1) = Aφx(t) + B φφu (u(t)) + K φe(t),
φy (y(t)) = C φx(t) + Dφφu (u(t)) + e(t),
where Aφ ∈ Rn×n , B φ ∈ Rn×nφu , C φ ∈ Rnφy ×n , Dφ ∈ Rnφy ×nφu and K φ ∈ Rn×nφy are
constant matrices and e(t) := φ(y(t)) − (φ(y(t))|P φ
t ∨ U φ
t ) is the prediction error.

(11)

2.4 Preimage

In this section, we describe the state-space model for the output y(t) while the state-space model
(11), derived in the previous section, represents the output in feature space φy (y(t)). At ﬁrst, we
deﬁne the feature maps φx : Xt 7→ Fx ∈ Rnφx , φ ¯u := Ut 7→ F ¯u ∈ Rnφ ¯u and the linear space X φ
t ,
¯U φ
t and ¯U φ
t ∩ ¯U φ
t generated by φx (x(t)), φ ¯u (u(t)). Then, the product of X φ
t satis ﬁes X φ
t = 0
because Xt ∩ U φ
t = 0 and φx , φ ¯u are bijective. Therefore, the output y(t) is represented as the
direct sum of the oblique projections as
t ∨ ¯U φ
y(t)/X φ
t = ¯C φφx (x(t)) + ¯Dφφ ¯u (u(t)).
As a result, we can obtain the following theorem.
TH EOR EM 2. Under assumptions 1 and 2, if rank Σφf φp |φu = n, then the output y can be repre-
sented in the following state-space model:
x(t + 1) = Aφx(t) + B φφu (u(t)) + ¯K φφe (¯e(t)),
y(t) = ¯C φφx (x(t)) + ¯Dφφ ¯u (u(t)) + ¯e(t),
t ∨ ¯U φ
where ¯e(t) := y(t) − y(t)/X φ
is the prediction error and ¯K φ := K φA¯e , in which A¯e is the
t
coefﬁcient matrix of the nonlinear regression from ¯e(t) to e(t) 1 .

(13)

(12)

1Let f be a map from ¯e(t) to e and minimize a regularized risk c((¯e1 , e1 , f (¯e1 )), · · · , (¯em , em , f (¯em )))+
Ω(kf kH ), where Ω : [0, ∞) → R is a strictly monotonically increasing function and c : ( ¯E × R2 )m →
R ∪ {∞} ( ¯E ∈ span{¯e}) is an arbitrary loss function; then, from the representer theorem[9], f satis ﬁes
f ∈ span{ﬃe (¯e(t))}, where ﬃe is a feature map with the associated Mercer kernel ke . Therefore, we can
represent nonlinear regression from ¯e(t) to e(t) as A¯eﬃe (¯e(t)).

3 Approximations

3.1 Realization with Finite Data

In practice, the state vector and associated state-space model should be constructed with avail-
able ﬁnite data. Let the past vector pφ (t) be truncated to ﬁnite length, i.e., pφ
T (t) := [φw (w(t −
1))0 , φw (w(t − 2))0 , · · · , φw (w(t − T ))0 ]0 ∈ RT (nφy +nφu ) , where T > 0, and deﬁne P[t−T ,t) :=
span{pφ
T (τ )|τ < t}. Then, the following theorem describes the construction of the state vec-
φ
ˆf
tor and the corresponding state-space system which form the ﬁnite-memory predictor
T (t) :=
f φ (t)/U φ+
t ∩ P φ
[t−T ,t) .
TH EOR EM 3. Under assumptions 1 and 2, if rank(Σφf φp |φu ) = n, then the process φy (y) is
expressed by the following nonstationary state-space model:
ˆxT (t + 1) = Aφ ˆxT (t) + B φφu (u(t)) + K φ (t)ˆeT (t),
φy (y(t)) = C φ ˆxT (t) + Dφφu (u(t)) + ˆeT (t).
where the state vector ˆxT (t) is a basis on the ﬁnite-memory predictor space Y φ+
t
[T ,t) ∨ U φ+
and ˆeT (t) := φy (y(t)) − (φy (y(t))|P φ
) is the prediction error.
t

P φ
[t−T ,t) ,

/U φ+
t

(14)

The proof can be carried out as in the linear case (cf. [3,5]). In other words, we can obtain the
approximated state vector ˆxT by applying the facts in Section 2 to ﬁnite data. This state ve ctor differs
from x(t) in Eq. (8); however, when T → ∞, the difference between ˆxT (t) and x(t) converges to
zero and the covariance matrix of the estimation error P φ also converges to the stabilizing solution
of the following Algebra Riccati Equation (ARE):
P φ = AφP φAφ 0
0
− (AφP φC φ 0
0
wΣφ
+Σφ
wΣφ
+Σφ
w
w

)−1 (AφP φC φ 0

)(C φP φC φ 0

e Σφ
+Σφ
e

0

0

wΣφ
+Σφ
)0 .
w
(15)

Moreover, the Kalman gain K φ converges to
K φ = (AφP φC φ 0
)(C φP φC φ 0
wΣφ
+ Σφ
e Σφ
+ Σφ
)−1 ,
(16)
w
e
e are the covariance matrices of errors in the state and observation equations,
w and Σφ
where Σφ
respectively.

0

0

3.2 Using Kernel Principal Components

Let z be a random variable, kz a Mercer kernel with a feature map φz and a feature space Fz , and
denote Φz := [φz (z 1 ), · · · , φz (zm )]0 and the associated Gram matrix Gz := Φz Φ0
ith
z . The ﬁrst
z }(i = 1, · · · , dz ) combined in a matrix Uz = [uz ,1 , · · · , uz ,dz ]
principal components uz ,i ∈ L{Φ0
z }, and can therefore also
form an orthonormal basis of a dz -dimensional subspace L{Uz } ⊆ L{Φ0
z Az , where the matrix Az ∈ Rm×dz holds the
be described as the linear combination Uz = Φ0
expansion coefﬁcients. Az is found by, for example, the eigendecomposition Gz = Γz Λz Γ− z 0
such that Az consists of the ﬁrst dz columns of Γz Λ−1/2
. Then, Φz with respect to the principal
z
z Az = Gz Az [11]. From the orthogonality of Γz (i.e.,
components is given by Cz := Φz Uz = Φz Φ0
z = Im ), we can derive the following equation:
z Γz = Γz Γ0
Γ0
z ,d )(cid:17)−1
z GzGz Az )−1 = (cid:16)(Γz Λ−1/2
z )(Γz Λ−1/2
z ,d )0 (Γz Λz Γ0
(A0
z )(Γz Λz Γ0
where Λz ,d is the matrix which consists of the ﬁrst dz columns of Λz , and ¯Az := Γz Λ1/2
z ,d satisfying
¯A0
¯Az = Idz and ¯Az A0
z = Az ¯A0
z = Im .
z Az = A0
z
This property of kernel principal components enables us to approximate matters described in the
previous sections in computable forms. First, using Eq. (17), the conditional covariance matrix
Σφf φf |φu can be expressed as
Σφf φf |φu = Σφf φf − Σφf φu Σ−1
Σφu φf
φu φu
uGuGuAu )−1 (A0
≈ A0
f Gf GuAu )(A0
f Gf Gf Af − (A0
uGuGf Af )
ˆΣf f |uAf ),
f (cid:0)Gf Gf − Gf Gu (GuGu )−1GuGf (cid:1) Af (:= A0
= A0
f

= ¯A0
z G−1
z G−1
z

¯Az ,

(17)

(18)

where ˆΣf f |u may be called the empirical conditional covariance operators, and the regularized vari-
ant can be obtained by replacing Gf Gf , GuGu with (Gf + Im )2 , (Gu + Im )2 ( > 0) (cf.[12,13]).
¯A∗ , where ˆL∗ is
∗ = ˆL−1
Σφp φp |φu and Σφf φp |φu can be approximated as well. Moreover, using L−1
∗
the square root matrix of ˆΣφ∗ φ∗ |u (∗ = p, f ) 2 , we can represent Eqs. (6) and (8) approximately as
p )0(cid:17) = ˆL−1
ˆΣf p|uAp ) (cid:16) ¯A0
p )0 = ˆU ˆS ˆV 0 , (19)
ˆΣf p|u ( ˆL−1
¯Af )(A0
p )0 ≈ ( ˆL−1
p ( ˆL−1
L−1
f Σφf φp |φu (L−1
f
f
f
p pφ (t) ≈ ˆS 1/2 ˆV 0 ( ˆL−1
¯Ap )(A0
pk(p(t))) = ˆS 1/2 ˆV 0 ˆL−1
x (t) = S 1/2V 0L−1
p k(p(t)),
p
where k(p(t)) := Φppφ (t) = [kp (p1 (t), p(t)), · · · , kp (pm (t), p(t))]0 .
In addition, we can apply this approximation with the kernel PCA to the state-space models derived
in the previous sections. First, Eq. (11) can be approximated as
x(t + 1) = Aφx(t) + B φA0
uku (u(t)) + K φe(t),
y ky (y(t)) = C φx(t) + DφA0
A0
uku (u(t)) + e(t),
where Au and Ay are the expansion coefﬁcient matrices found by the eigendec omposition of Gu
and Gy , respectively. Also, using the coefﬁcient matrices Ax , Ae and A ¯u , Eq.(13) can be written as
uku (u(t)) + ¯K φA0
x(t + 1) = Aφx(t) + B φA0
eke (¯e(t)),
y(t) = ¯C φA0
xkx (x(t)) + ¯DφA0
¯uku (u(t)) + ¯e(t).

(21)

(20)

(22)

4 Algorithm

GU :=

GW :=

Gu,i(i+N −1)

Gu,(i+1)(i+N −1)

Gu,ii

Gu,i(i+1)

· · ·

· · ·
. . .

· · ·

Gu,(i+1)i
...
Gu,(i+N −1)i

Gu,(i+1)(i+1)
...
Gu,(i+N −1)(i+1)

In this section, we give a subspace identi ﬁcation algorithm based on the discussions in the previous
sections. Denote the ﬁnite input-output data as {u(t), y(t), t = 1, 2, · · · , N + 2l − 1}, where l > 0
is an integer larger than the dimension of system n and N is the sufﬁcient
large integer, and assume
that all data is centered. First, using the Gram matrices Gu , Gy and Gw associated with the input,
the output, and the input-output, repectively, we must to calculate the Gram matrices GU , GY and
GW corresponding to the past input, the future output, and the past input-output deﬁned as


2l
2l
2l
Pi=l+1
Pi=l+1
Pi=l+1


2l
2l
2l
Pi=l+1
Pi=l+1
Pi=l+1
...
2l
2l
2l
Pi=l+1
Pi=l+1
Pi=l+1
l
l
l


Pi=1
Pi=1
Pi=1
Gw,i(i+N −1)
Gw,i(i+1)
Gw,ii


l
l
l
Pi=1
Pi=1
Pi=1
...
l
l
l
Pi=1
Pi=1
Pi=1
and GY is deﬁned analogously to GU . Now the procedure is given as follows.
Step 1 Calculate the regularized empirical covariance operators and their square root matrices as
ˆΣf f |u = (GY + IN )2 − GY GU (GU + IN )−2GU GY = ˆLf ˆL0
f ,
ˆΣpp|u = (GW + IN )2 − GW GU (GU + IN )−2GU GW = ˆLp ˆL0
p ,
ˆΣf p|u = GY GW − GY GU (GU + IN )−2GU GW .

Gw,(i+1)(i+1)
...
Gw,(i+N −1)(i+1)

Gw,(i+1)i
...
Gw,(i+N −1)i

Gw,(i+N −1)(i+N −1)

Gu,(i+N −1)(i+N −1)

· · ·

· · ·
. . .

· · ·

Gw,(i+1)(i+N −1)

,

(23)

,

(24)

(25)

2This is given by (L−1
∗ = Σ−1
∗ )0L−1
φ∗ φ∗ |φu

≈ (A0
∗

ˆΣ∗∗|uA∗ )−1 = ¯A0
∗

ˆΣ−1
∗∗|u

∗ ( ˆL−1
¯A∗ = ¯A0
∗ )0 ˆL−1
∗

¯A∗ .

(27)

ˆL−1
p GW ,

Step 2 Calculate the SVD of the normalized covariance matrix (cf. Eq. (19))
p )0 = ˆU ˆS ˆV 0 ≈ U1S1V1 ,
ˆΣf p|u ( ˆL−1
L−1
(26)
f
where S1 is obtained by neglecting the small singular values so that the dimension of the
state vector n equals the dimension of S1 .
Step 3 Estimate the state sequence as (cf. Eq. (20))
Xl := [x(l), x(l + 1), · · · , x(l + N − 1)] = S 1/2
1 V 0
1
and deﬁne the following matrices consisting of N − 1 columns:
ˆXl = ¯Xl (:, 1 : N − 1).
ˆXl+1 = ¯Xl (:, 2 : N ),
(28)
Step 4 Calculate the eigendecomposition of the Gram matrices Gu , G ¯u , Gy and Gx and the corre-
sponding expansion coefﬁcient matrices Au , A ¯u , Ay and Ax . Then, determine the system
matrices Aφ , B φ , C φ , Dφ , ¯C φ and ¯Dφ by applying regularized least square regressions to
the following equations (cf. Eqs. (21) and (22)):
ˆXk+1
ˆXk
yGy (:, 2, N ) (cid:21) = (cid:20) Aφ B φ
(cid:20)
ρe (cid:21) ,
C φ Dφ (cid:21) (cid:20)
uGu (:, 1, N − 1) (cid:21) + (cid:20) ρw
A0
A0
Yl|l = ¯C φ (A0
xGx (:, 2, N )) + ¯Dφ ( ¯A0
uGu (:, 2, N )) + ¯ρe ,
where the matrices ρw , ρe and ¯ρe are the residuals.
Step 5 Calculate the covariance matrices of the residuals
w ρw ρ0
N − 1 (cid:20) ρw ρ0
Σew Σe (cid:21) =
(cid:20) Σw Σwe
e (cid:21) ,
1
e
ρeρ0
ρeρ0
w
solve ARE (15), and, using the stabilizing solution, calculate the Kalman gain K Φ in Eq.
(16).

(29)

(30)

(31)

5 Simulation Result

(32)

In this section, we illustrate the proposed algorithm for learning nonlinear dynamical systems with
synthetic data. The data was generated by simulating the following system [8] using the 4th- and
5th-order Runge-Kutta method with a sampling time of 0.05 seconds:
˙x1 (t) = x2 (t) − 0.1 cos(x1 (t))(5x1 (t) − 4x3
1 (t) + x5
1 (t)) − 0.5 cos(x1 (t))u(t),
˙x2 (t) = −65x1 (t) + 50x3
1 (t) − 15x5
1 (t) − x2 (t) − 100u(t),
y(t) = x1 (t),
where the input was a zero-order-hold white noise signal uniformly distributed between −0.5 and
0.5. We applied our algorithm on a set of 600 data points, and then validated the obtained model
using a fresh data set of 400 points. As a kernel function, we used the RBF Gaussian kernel
k(z i , z j ) = exp(−kz i − z j k2 /2σz ). The parameters to be tuned for our method are thus the
widths of the kernels σ for u, y , w and x, the regularization degree , and the row-block number l
of the Hankel matrix. In addition, we must select the order of the system and the number of kernel
principal components npc
∗ for u, y and e. Figure 2 shows free-run simulation results of the model
acquired by our algorithm, in which the parameters were set as σu = 2.5, σy = 3.5, σw = 4.5,
x = 9 and  = 0.05, and, for comparison, by the linear subspace
y = 4, npc
σx = 1.0, npc
u = npc
identi ﬁcation [5]. The row-block number
l was set as 10 in both identi ﬁcations. The simulation
errors [2]
ny
Xc=1 s Pm
i=1 ((yi )c − (ys
i )c )2
100
Pm
j=1 ((yi )c )2
ny
where ys
i are simulated values and the used initial state is a least square estimation with the initial
few points, were improved to 40.2 for our algorithm, from 44.1 for the linear method. The accuracy
was improved by about 10 percent. The system orders are 8 for our algorithm, whle 10 for the
linear method, in this case. We can see that our method can estimate the state sequence with more
information and yield the model capturing the dynamics more precisely. However, the parameters
involved much time and effort for tuning.

(33)

 =

,

3

2

1

0

-1

-2

-3

0

Observation

Simulation

50

100

150

200

250

300

350

400

3

2

1

0

-1

-2

-3

0

Observation

Simulation

50

100

150

200

250

300

350

400

Data Point
Data Point
Figure 2: Comparison of simulated outputs. Left: Kernel subspace identi ﬁcation method (proposed
method). Right: Linear subspace identi ﬁcation method [5]. The broken lines represent the observa-
tions and the solid lines represent the simulated values.

6 Conclusion

A new subspace method for learning nonlinear dynamical systems using reproducing kernel Hilbert
spaces has been proposed. This approach is based on approximated solutions of two discrete Wiener-
Hopf equations by covariance factorization in kernel feature spaces. The algorithm needs no iterative
optimization procedures, and hence, solutions can be obtained in a fast and reliable manner. The
comparative empirical results showed the high performance of our method. However, the parameters
involved much time and effort for tuning. In future work, we will develop the idea for closed-loop
systems for the identi ﬁcation of more realistic applicatio ns. Moreover, it should be possible to
extend other established subspace identi ﬁcation methods t o nonlinear frameworks as well.

Acknowledgments
The present research was supported in part through the 21st Century COE Program, ”Mechanical
Systems Innovation,” by the Ministry of Education, Culture , Sports, Science and Technology.
References
[1] Roweis, S. & Ghahramani, Z. (1999) “A Unifying Review of Linear G aussian Models ” Neural Computation,
11 (2) : 305-345.
[2] Van Overschee, P. & De Moor, B. (1996) “Subspace Identiﬁcatio
n for Linear Systems: Theory, Implemen-
tation, Applications ” Kluwer Academic Publishers, Dordrecht, Netherlands.
[3] Katayama, T. (2005) “Subspace Methods for System Identiﬁcation
: A Realization Approach” Communica-
tions and Control Engineering, Springer Verlag, 2005.
[4] Moonen, M. & Moor, B. D. & Vandenberghe, L. & Vandewalle, J. (1989) “On- and Off-line Identiﬁcation
International Journal of Control, 49 (1) : 219-232.
of Linear State Space Models ”
[5] Katayama, T. & Picci, G. (1999) “Realization of Stochastic Systems with Exogenous Inputs and Subspace
Automatica, 35 (10) : 1635-1652.
Identiﬁcation Methods ”
[6] Goethals, I. & Pelckmans, K. & Suykens, J. A. K. & Moor, B. D. (2005) “Subspace Identiﬁcation of
IEEE Trans. on Automatic Control, 50
Hammerstein Systems Using Least Squares Support Vector Machines ”
(10) : 1509-1519.
[7] Ni, X. & Verhaegen, M. & Krijgsman, A. & Verbruggen, H. B. (1996) “A New Method for Identiﬁcation and
Control of Nonlinear Dynamic Systems ” Engineering Application of Artiﬁcial Intelligence , 9 (3) : 231-243.
[8] Verdult, V. & Suykens, J. A. K. & Boets, J. & Goethals, I. & Moor, B. D. (2004) “Least Squares Support
Vector Machines for Kernel CCA in Nonlinear State-Space Identiﬁcation”
Proceedings of the 16th Interna-
tional Symposium on Mathematical Theory of Networks and Systems, (MTNS2004).
[9] Sch ¨olkopf, B. & Smola, A. (2002) “Learning with Kernels ” MIT Press.
[10] Rozanov, N. I. (1963) “Stationary Random Processes ” Holden-Day, San Francisco, CA.
[11] Kuss, M. & Graepel, T. (2003) “The Geometry of Kernel Canon ical Correlation Analysis ” Technical
Report, Max Planck Institute for Biological Cybernetics, Tubingen, Germany (108).
[12] Bach, F. R., & Jordan, M. I. (2002) “Kernel Independent C omponent Analysis ” Journal of Machine Learn-
ing Research (JMLR), 3 : 1-48.
[13] Fukumizu, K. & Bach, F. R., & Jordan, M. I. (2004) “Dimension ality Reduction for Supervised Learning
with Reproducing Kernel Hilbert Spaces ” Journal of Machine Learning Research (JMLR), 5 : 73-99.


Parameter Expanded Variational Bayesian Methods

Yuan (Alan) Qi
MIT CSAIL
32 Vassar street
Cambridge, MA 02139
alanqi@csail.mit.edu

Tommi S. Jaakkola
MIT CSAIL
32 Vassar street
Cambridge, MA 02139
tommi@csail.mit.edu

Abstract

Bayesian inference has become increasingly important in statistical machine
learning. Exact Bayesian calculations are often not feasible in practice, however.
A number of approximate Bayesian methods have been proposed to make such
calculations practical, among them the variational Bayesian (VB) approach. The
VB approach, while useful, can nevertheless suffer from slow convergence to the
approximate solution. To address this problem, we propose Parameter-eXpanded
Variational Bayesian (PX-VB) methods to speed up VB. The new algorithm is in-
spired by parameter-expanded expectation maximization (PX-EM) and parameter-
expanded data augmentation (PX-DA). Similar to PX-EM and -DA, PX-VB ex-
pands a model with auxiliary variables to reduce the coupling between variables
in the original model. We analyze the convergence rates of VB and PX-VB and
demonstrate the superior convergence rates of PX-VB in variational probit regres-
sion and automatic relevance determination.

1 Introduction

A number of approximate Bayesian methods have been proposed to offset the high computational
cost of exact Bayesian calculations. Variational Bayes (VB) is one popular method of approxima-
tion. Given a target probability distribution, variational Bayesian methods approximate the target
distribution with a factored distribution. While factoring omits dependencies present in the target
distribution, the parameters of the factored approximation can be adjusted to improve the match.
Speciﬁcally, the approximation is optimized by minimizing the KL-divergence between the factored
distribution and the target. This minimization can be often carried out iteratively, one component
update at a time, despite the fact that the target distribution may not lend itself to exact Bayesian
calculations. Variational Bayesian approximations have been widely used in Bayesian learning (e.g.,
(Jordan et al., 1998; Beal, 2003; Bishop & Tipping, 2000)).

Variational Bayesian methods nevertheless suffer from slow convergence when the variables in the
factored approximation are actually strongly coupled in the original model. The same problem arises
in popular Gibbs sampling algorithm. The sampling process converges slowly in cases where the
variables are strongly correlated. The slow convergence can be alleviated by data augmentation (van
Dyk & Meng, 2001; Liu & Wu, 1999), where the idea is to identify an optimal reparameterization
(within a family of possible reparameterizations) so as to remove coupling. Similarly, in a deter-
ministic context, Liu et al. (1998) proposed over-parameterization of the model to speed up EM
convergence. Our work here is inspired by DA sampling and PX-EM. Our approach uses auxiliary
parameters to speed up the deterministic approximation of the target distribution.

Speciﬁcally, we propose Parameter-eXpanded Variational Bayesian (PX-VB) method. The original
model is modi ﬁed by auxiliary parameters that are optimized in conjunction with the variational
approximation. The optimization of the auxiliary parameters corresponds to a parameterized joint

optimization of the variational components; the role of the new updates is to precisely remove oth-
erwise strong functional couplings between the components thereby facilitating fast convergence.

2 An illustrative example

(4)

(2)
(3)

Consider a toy Bayesian model, which has been considered by Liu and Wu (1999) for sampling.
p(y |w, z ) = N (y | w + z , 1),
p(z ) = N (z | 0, D)
(1)
where D is a know hyperparameter and p(w) ∝ 1. The task is to compute the posterior dis-
tribution of w . Suppose we use a VB method to approximate p(w|y), p(z |y) and p(w, z |y) by
q(w), q(z ) and q(w, z ) = q(w)q(z ), respectively. The approximation is optimized by minimizing
K L(q(w)q(z )kp(y |w, z )p(z )) (the second argument need not be normalized). The general forms of
the component updates are given by
q(w) ∝ exp(hln p(y |w, z )p(z )iq(z) )
q(z ) ∝ exp(hln p(y |w, z )p(z )iq(w) )
It is easy to derive the updates in this case:
q(z ) = N (z | y − hwi
1
q(w) = N (w|y − hz i, 1)
1 + D−1 )
1 + D−1 ,
1 + D−1 = D−1 (cid:0)(1 + D−1 )−1 y + (1 + D−1 )−2 y + · · · (cid:1) = y .
Now let us analyze the convergence of the mean parameter of q(w), hwi = y − hz i. Iteratively,
hwi
hwi = D−1
1 + D−1 y +
The variational estimate hwi converges to y , which actually is the true posterior mean (For this toy
problem, p(w|y) = N (w|y , 1 + D)). Furthermore, if D is large, hwi converges slowly. Note that the
variance parameter of q(w) converges to 1 in one iteration, though underestimates the true posterior
variance 1 + D .
Intuitively, the convergence speed of hwi and q(w) suffers from strong coupling between the updates
of w and z . In other words, the update information has to go through a feedback loop w → z →
w · · · . To alleviate the coupling, we expand the original model with an additional parameter α:
p(y |w, z ) = N (y | w + z , 1)
p(z |α) = N (z | α, D)
The expanded model reduces to the original one when α equals the null value α0 = 0.
Now having computed q(z ) given α = 0, we minimize K L(q(w)q(z )kp(y |w, z )p(z |α)) over α and
obtain the minimizer α = hz i. Then, we reduce the expanded model to the original one by applying
the reduction rule
wnew = w + α = w + hz i.
z new = z − α = z − hz i,
Correspondingly, we change the measures of q(w) and q(z ):
1
q(w + hz i) → q(wnew ) = N (wnew |y , 1)
q(z − hz i) → q(z new ) = N (z new |0,
1 + D−1 )
Thus, the PX-VB method converges. Here α breaks the update loop between q(w) and q(z ) and
plays the role of a correction force; it corrects the update trajectories of q(w) and q(z ) and makes
them point directly to the convergence point.

(5)

(6)

3 The PX-VB Algorithm

In the general PX-VB formulation, we over-parameterize the model p( ˆx, D) to get pα (x, D), where
the original model is recovered for some default values of the auxiliary parameters α = α0 . The
algorithm consists of the typical VB updates relative to pα (x, D), the optimization of auxiliary
parameters α, as well as a reduction step to turn the model back to the original form where α = α0 .
This last reduction step has the effect of jointly modifying the components of the factored variational
approximation. Put another way, we push the change in pα (x, D), due to the optimization of α,
into the variational approximation instead. Changing the variational approximation in this manner
permits us to return the model into its original form and set α = α0 .
p( ˆx, D) to obtain pα (x, D). Then at the tth iteration,
Speciﬁcally, we ﬁrst expand

(7)

1. q(xs ) are updated sequentially. Note that the approximate distribution q(x) = Q
s q(xs ).
2. We minimize K L(q(x)kpα (x, D)) over the auxiliary parameters α. This optimization can
be done jointly with some components of the variational distribution, if feasible.
3. The expanded model is reduced to the original model through reparameterization. Accord-
ingly, we change q (t+1) (x) to q (t+1) ( ˆx) such that
K L(q (t+1) ( ˆx)kpα0 ( ˆx, D)) = K L(q(x)kpα(t+1) (x, D))
where q (t+1) ( ˆx) are the modiﬁed components of the variational approximation.
4. Set α = α0 .
Since each update of PX-VB decreases or maintains the KL divergence K L(q(x)kp(x, D)), which
is lower bounded, PX-VB reaches a stationary point for K L(q(x)kp(x, D)). Empirically, PX-VB
often achieves solution similar to what VB achieves, with faster convergence.
A simple strategy to implement PX-VB is to use a mapping Sα , parameterized by α, over the vari-
ables ˆx. After sequentially optimizing over the components {q(xs )}, we maximize hln pα (x)iq(x)
over α. Then, we reduce pα (x, D) to p( ˆx, D) and q(x) to q( ˆx) through the inverse mapping of Sα ,
Mα ≡ S−1
α . Since we optimize α after optimizing {q( ˆxs }, the mapping S should change at least
two components of x. Otherwise, the optimization over α will do nothing since we have already
optimized over each q( ˆxs ). If we jointly optimize α and one component q(xs ), it sufﬁces (albeit
need not be optimal) for the mapping Sα to change only q(xs ).
Algorithmically, PX-VB bears a strong similarity to PX-EM (Liu et al., 1998). They both expand the
original model and both are based on lower bounding KL-divergence. However, the key difference
is that the reduction step in PX-VB changes the lower-bounding distributions {q(xs )}, while in PX-
EM the reduction step is performed only for the parameters in p(x, D). We also note that the PX-VB
reduction step via Mα leaves the KL-divergence (lower bound on the likelihood) invariant, while in
PX-EM the likelihood of the observed data remains the same after the reduction. Because of these
differences, general EM acceleration methods (e.g., (Salakhutdinov et al., 2003)) can not be directly
applied to speed up VB convergence.

In the following sections, we present PX-VB methods for two popular Bayesian models: Probit re-
gression for data classiﬁcation and Automatic Relevance Determination (ARD) for feature selection
and sparse learner.

3.1 Bayesian Probit regression

σ(tnwTxn ),

Probit regression is a standard classi ﬁcation technique (see, e.g., (Liu et al., 1998) for the maximum
likelihood estimation). Here we demonstrate the use of variational Bayesian methods to train Probit
models.
p(t|X, w) = Y
The data likelihood for Probit regression is
n
where X = [x1 , . . . , xN ] and σ is the standard normal cumulative distribution function. We can
rewrite the likelihood in an equivalent form
p(zn |w, xn ) = N (zn |wTxn , 1)
p(tn |zn ) = sign(tn zn )
(8)
imate the posterior distribution p(w, z|X, t) by q(w, z) = q(w) Q
K L(q(w) Q
Given a Gaussian prior over the parameter, p(w) = N (w|0, v0 I), we wish to approx-
n q(zn ). Minimizing
n q(zn )kp(w, z, t|X)), we obtain the following VB updates:
q(zn ) = T N (zn |hwiTxn , 1, tn zn )
0 I)−1Xhzi, (XXT + v−1
q(w) = N (w|(XXT + v−1
0 I)−1 )
T N (zn |hwiTxn , 1, tn zn )
where
such
Gaussian
truncated
a
for
stands
T N (zn |hwiTxn , 1, tn zn ) = N (zn |hwiTxn , 1) when tn zn > 0, and it equals 0 otherwise.

(9)
(10)

that

(11)

To speed up the convergence of the above iterative updates, we apply the PX-VB method. First, we
expand the orginal model p( ˆw, ˆz, t|X) to pc (w, z, t|X) with the mapping
w = ˆwc
z = ˆzc
such that
p(w) = N (w|0, c2 v0 I)
pc (zn |w, xn ) = N (zn |wTxn , c2 )
Then, we minimize K L(cid:0)q(z)q(w)kpc (w, z, t|X)(cid:1) over c, yielding
(12)
Setting c = c0 = 1 in the expanded model, we update q(zn ) and q(w) as before, via (9) and (10).
(cid:0) X
0 hwwT i(cid:1)
1
(hz 2
n i − 2hzn ihwiTxn + xT
n hwwT ixn ) + v−1
c2 =
(13)
N + M
n
where M is the dimension of w. In the degenerate case where v0 = ∞, the denominator of the
above equation becomes N instead of N + M . Since this equation can be efﬁciently calculated, the
extra computational cost induced by the auxiliary variable is therefore small. We omit the details.
bz = z/c.
bw = w/c
The transformation back to pc0 can be made via the inverse map
Accordingly, we change q(w) to obtain a new posterior approximation qc ( bw):
qc ( bw) = N ( bw|(XXT + v−1
0 I)−1Xhzi/c, (XXT + v−1
0 I)−1 /c2 )
(15)
By changing variables w to bw through (14), the KL divergence between the approximate and exact
We do not actually need to compute qc (zn ) if this component will be optimized next.
posteriors remains the same. After obtaining new approximations qc ( bw) and q( ˆzn ), we reset c =
c0 = 1 for the next iteration.
Though similar to the PX-EM updates for the Probit regression problem (Liu et al., 1998), the PX-
VB updates are geared towards providing an approximate posterior distribution.

(14)

We use both synthetic data and a kidney biopsy data (van Dyk & Meng, 2001) as numerical examples
for probit regression. We set v0 = ∞ in the experiment. The comparison of convergence speeds for
VB and PXVB is illustrated in ﬁgure 1.

(a)

(b)

Figure 1: Comparison between VB and PX-VB for probit regression on synthetic (a) and kidney-
biospy data sets (b). PX-VB converges signiﬁcantly faster than VB. Note that the Y axis shows the
difference between two consecutive estimates of the posterior mean of the parameter w.

For the synthetic data, we randomly sample a classiﬁer and use it to deﬁne the data labels for sampled
inputs. We have 100 training and 500 test data points, each of which is 20 features. The kidney
data set has 55 data points, each of which is a 3 dimensional vector. On the synthetic data, PX-
VB converges immediately while VB updates are slow to converge. Both PX-VB and VB trained
classiﬁers achieve zero test error. On the kidney biopsy data set, PX-VB converges in 507 iterations,
while VB converges in 7518 iterations. In other words, PX-VB requires 15 times fewer iterations
than VB. In terms of CPU time, which reﬂects the extra computational cost induced by the auxiliary
variables, PX-VB is 14 times more efﬁcient. Among all these runs, PX-VB and VB achieve very
similar estimates of the model parameters and the same prediction results. In sum, with a simple
modiﬁcation of VB updates, we signiﬁcantly improve the convergence speed of variational Bayesian
estimation for probit model.

010002000300040005000−8−6−4−20Number of iterationslog(||wt+1−wt||)VBPX−VB0200040006000−8−6−4−20Number of iterationslog(||wt+1−wt||)VBPX−VB3.2 Automatic Relevance Determination

(16)

(18)

N (tn |wTφn , τ −1 )

Automatic relevance determination (ARD) is a powerful Bayesian sparse learning technique
(MacKay, 1992; Tipping, 2000; Bishop & Tipping, 2000). Here, we focus on variational ARD
proposed by Bishop and Tipping (2000) for sparse Bayesian regression and classiﬁcation.
p(t|X, w, τ ) = Y
The likelihood for ARD regression is
n
where φn is a feature vector based on xn , such as [k(x1 , xn ), . . . , [k(xN , xn )]T where k(xi , xj )
is a nonlinear basis function. For example, we can choose a radial basis function k(xi , xj ) =
In ARD, we assign a Gaussian prior on the model parameters w: p(w|α) = QM
exp(−kxi − xj k/(2λ2 ), where λ is the kernel width.
m=0 N (wm |0, α−1
Gamma(αm |a, b) = Y
p(α) = Y
m ),
where the inverse variance diag(α) follows a factorized Gamma distribution:
baαa−1
m e−bαm /Γ(a)
m
m
where a and b are hyperparameters of the model. The posterior does not have a closed form. Let
us approximate p(w, α, τ |X, t) by a factorized distribution q(w, α, τ ) = q(w)q(α)q(τ ). The
sequential VB updates on q(τ ), q(w) and q(α) are described by Bishop and Tipping (2000).
The variational RVM achieves good generalization performance as demonstrated by Bishop and
Tipping (2000). However, its training based on the VB updates can be quite slow. We apply PX-VB
First, we expand the original model p( bw, ˆα, ˆτ |X, t) via
to address this issue.
w = bw/r
(17)
pr (t|w, X, τ ) = Y
MY
while maintaining ˆα and ˆτ unchanged. Consequently, the data likelihood and the prior on w become
N (tn |rwTφn , τ −1 )
pr (w|α) =
n
m=0
Setting r = r0 = 1, we update q(τ ) and q(α) as in the regular VB. Then, we want to joint optimize
r = g + pg2 + 16M f
over q(w) and r . Instead of performing a fully joint optimization, we optimize q(w) and r separately
at the same time. This gives
where f = hτ i P
m ihαm i and g = 2hτ i P
n hwwT ixn + P
(19)
4f
m hw2
m hwT ixn tn . where hwT i and
n xT
hwwT i are the ﬁrst and second order moments of the previous q(w). Since both f and XT hwi has
been computed previously in VB updates, the added computational cost for r is negligible overall.
The separate optimization over q(w) and r often decreases the KL divergence. But it cannot guar-
antee to achieve a smaller KL divergence than what optimization only over q(w) would achieves. If
Given r and q(w), we use bw = rw to reduce the expanded model to the original one. Cor-
the regular update over q(w) achieves a smaller KL divergence, we reset r = 1.
N ( bw|rµw , r2Σw ).
respondingly, we change q(w) = N (w|µw , Σw ) via this reduction rule to obtain qr ( ˆw) =
We can also introduce another auxiliary variable s such that α = ˆα/s. Similar to the above proce-
dure, we optimize over s the expected log joint probability of the expanded model, and at the same
time update q(α). Then we change q(α) back to qs ( ˆα) using the inverse mapping ˆα = sα. Due to
the space limitation, we skip the details here.
The auxiliary variables r and s change the individual approximate posteriors q(w) and q(α) sep-
arately. We can combine these two variables into one and use it to adjust q(w) and q(α) jointly.
α = c2 bα.
w = bw/c
Speciﬁcally, we introduce the variable c:

N (wm |0, r−2α−1
m )

(a)

(b)

(c)

Figure 2: Convergence comparison between VB and PX-VB for ARD regression on synthetic data
(a,b) and gene expression data (c). The PX-VB results in (a) and (c) are based on independent aux-
iliar variables on w and α. The PX-VB result in (b) is based on the auxiliar variable that correlates
both w and α. The added computational cost for PX-VB in each iteraction is negligible overall.

Setting c = c0 = 1, we perform the regular updates over q(τ ), q(w) and q(α). Then we optimize
over c the expected log joint probablity of the expanded model. We cannot ﬁnd a closed-form solu-
tion for the maximization. But we can efﬁciently compute its gradient and Hessian. Therefore, we
perform a few steps of Newton updates to partially optimize c. Again, the additional computational
cost for calculating c is small. Then using the inverse mapping, we reduce the expanded model to
the original one and adjust both q(w) and q(α) accordingly. Empirically, this approach can achieve
faster convergence than using auxiliary variables on q(w) and q(α) separately. This is demonstrated
in ﬁgure 2(a) and (b).

We compare the convergence speed of VB and PX-VB for the ARD model on both synthetic data
and gene expression data. The synthetic data are sampled from the function sinc(x) = (sinx)/x for
x ∈ (−10, 10) with added Gaussian noise. We use RBF kernels for the feature expansion φn with
kernel width 3. VB and PX-VB provide basically identical predictions. For gene expression data,
we apply ARD to analyze the relationship between binding motifs and the expression of their target
genes. For this task, we use 3 order polynomial kernels.

The results of convergence comparison are shown in ﬁgure 2. With a little modiﬁcation of VB
updates, we increase the convergence speed signiﬁcantly. Though we demonstrate PX-VB improve-
ment only for ARD regression, the same technique can be used to speed up ARD classiﬁcation.

4 Convergence properties of VB and PX-VB

In this section, we analyze convergence of VB and PX-VB, and their convergence rates.
Deﬁne the mapping q(t+1) = M (q(t) ) as one VB update of all the approximate distributions.
Q qi (x)
Z Y
Z
Z Y
Deﬁne an objective function as the unnormalized KL divergence:
p(x)dx −
p(x)
It is easy to check that minimizing Q(q) gives the same updates as VB which minimizes KL diver-
gence.

qi (x)dx).

) + (

Q(q) =

qi (x) log

(20)

Based on Theorem 2.1 by Luo and Tseng (1992), an iterative application of this mapping to minimize
Q(q) results in at least linear convergence to an element q? in the solution set.
Deﬁne the mapping q(t+1) = Mx (q(t) ) as one PX-VB update of all the approximate distribu-
tions. The convergence of PX-VB follows from similar arguments. i.e.,β = [qTαT ]T converges to
0 ]T , where α ∈ Λ are the expanded model parameters, α0 are the null value in the original
[q?TαT
model.

4.1 Convergence rate of VB and PX-VB

The matrix rate of convergence DM (q):
q(t+1) − q? = DM (q)T (q(t) − q? )

(21)

05001000150020002500−8−6−4−20Number of iterationslog(||wt+1−wt||)VBPX−VB05001000150020002500−8−6−4−20Number of iterationslog(||wt+1−wt||)VBPX−VB05001000150020002500−8−7−6−5−4Number of iterationslog(||wt+1−wt||)VBPX−VB.

(22)

(cid:17)

where DM (q) =

(cid:16) ∂Mj (q)
∂ qi
kq(t+1)−q? k
limt→∞
=
q:
Deﬁne
kq(t)−q? k .
for
convergence
of
rate
global
the
r
Under certain regularity conditions, r = the largest eigenvalue of DM (q). The smaller r is,
the faster the algorithm converges.
Deﬁne the constraint set gs as the constraints for the sth update. Then the following theorem holds:
SY
Theorem 4.1 The matrix convergence rate for VB is:
DM (q? ) =
(cid:0)D2Q(q? )(cid:1)−1 and Bs = ∇gs (q? ).
(cid:0)D2Q(q? )(cid:1)−1Bs ]−1BT
Ps
s=1
s
Proof: Deﬁne ξ as the current approximation q. Let Gs (ξ) be qs that maximizes the objective
function Q(q) under the constraint gs (q) = gs (ξ) = [ξ\s ].
Let M0 (q) = q and

where Ps = Bs [BT
s

for all 1 ≤ s ≤ S.
Ms (q) = Gs (Ms−1 (q))
Then by construction of VB, we have q(t+s/S ) = Ms (q(t) ),
s = 1, . . . , S and DM (q? ) =
DMS (q? ). At the stationary points, q? = DMs (q? ) for all s.
We differentiate both sides of equation (23) and evaluate them at q = q? :
It follows that DM (q? ) = QS
DMs (q) = DMs−1 (q)DGS (Ms−1 (q? )) = DMs−1 (q? )DGS (q? )
s=1 DGS (q? ).
To calculate DGS (q? ), we differentiate the constraint gs (Gs (ξ)) = gs (ξ) and evaluate both sides
at ξ = q? , such that

(24)

(23)

(26)

DGs (q? )Bs = Bs .
(25)
Similarly, we differentiate the Lagrange equation DQs (G(ξ)) − ∇gs (G(ξ))λs (ξ) = 0 and evaluate
both sides at ξ = q? . This yields
DGs (q? )D2Qs (q? ) − Dλs (q? )BT
s = 0
Equation (26) holds because ∂ 2 gs
= 0.
∂ qi ∂ qj
(cid:0)D2Qs (q? )(cid:1)−1Bs ]−1BT
(cid:0)D2Qs (q? )(cid:1)−1
Combining (25) and (26) yields
DGs (q? ) = Bs [BT
.2
s
s
In the s update we ﬁx q\s , i.e., gs (q) = q\s . Therefore, Bs is an identity matrix with its sth column
Denote Cs = (cid:0)D2Qs (q? )(cid:1)−1 . Without the loss of generality, we set s = S . It is easy to obtain
removed Bs = I:,s , where I is the identity matrix and s, : means without the sth column.
S CBS = C\S,\S
BT
where \S, \S means without row S and column S .
(cid:18) Id−1 −D2Q\S ,S (D2QS,S )−1
(cid:19)
(cid:18) Id−1 C −1\S ,\S C\S ,S
Inserting (28) into (27) yields
PS = DGS (q? ) =
0
0
0
0
where Id−1 is a (d − 1) by (d − 1) identity matrix, and D2Q\S ,S = ∂ 2Q(qq (x)kp(x))
and D2QS,S =
T ∂qS
∂q\S
∂ 2Q(qq (x)kp(x))
. Notice that we use Schur complements to obtain (29). Similar to the calculation of
T ∂qS
PS via (29), we can derive Ps for s = 1, . . . , S − 1 with structures similar to PS .
∂qS

(cid:19)

(28)

(29)

(27)

=

The above results help us understand the convergence speed of VB. For example, we have
S = (cid:0) − (D2QS,S )−1D2QS,\S
0(cid:1)(q(t+(S−1)/S ) − q? ).
S · · · P T
q(t+1) − q? = P T
1 (q(t) − q? ).
− q?
For qS , q(t+1)
S
Clearly, if we view D2QS,\S as the correlation between qS and q\S , then the smaller “correlation”,
the faster the convergence. In the extreme case, if there is no correlation between qS and q\S , then
− q?
q(t+1)
S = 0 after the ﬁrst iteration. Since the global convergence rate is bounded by the
S
maximal component convergence rate and generally there are many components with convergence
rate same as the global rate. Therefore, the instant convergence of qS could help increase the global
convergence rate.

(30)

For PX-VB, we can compute the matrix rate of convergence similarly.
In the toy example in
Section 2, PX-VB introduces an auxiliary variable α which has zero correlation with w , lead-
ing an instant convergence of the algorithm. This suggests that PX-VB improves the conver-
gence by reducing the correlation among {qs }. Rigorously speaking, the reduction step in PX-
VB implictly deﬁnes a mapping between q to qα0 through the auxiliary variables α: (q, pα0 ) →
(q, pα ) → (qα , pα0 ). Denote this mapping as Mα such as qα = Mα (q). Then we have
DMx (q? ) = DG1 (q? ) · · · DGα (q? ) · · · DGS (q? )
It is known that the spectral norm has the following submultiplicative property kEF k <= kE kkF k,
where E and F are two matrices. Thus, as long as the largest eigenvalue of Mα is smaller than 1,
PX-VB converges faster than VB. The choice of α affects the convergence rate by controlling the
eigenvalue of this mapping. The smaller the largest eigenvalue of Mα , the faster PX-VB converges.
In practice, we can check this eigenvalue to make sure the constructed PX-VB algorithm enjoys a
fast convergence rate.

5 Discussion

We have provided a general approach to speeding up convergence of variational Bayesian learning.
Faster convergence is guaranteed theoretically provided that the Jacobian of the transformation from
auxiliary parameters to variational components has spectral norm bounded by one. This property
can be veriﬁed in each case separately. Our empirical results show that the performance gain due to
the auxiliary method is substantial.

Acknowledgments

T. S. Jaakkola was supported by DARPA Transfer Learning program.

References
Beal, M. (2003). Variational algorithms for approximate Bayesian inference. Doctoral dissertation, Gatsby
Computational Neuroscience Unit, University College London.
Bishop, C., & Tipping, M. E. (2000). Variational relevance vector machines. 16th UAI.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1998). An introduction to variational methods in
graphical models. Learning in Graphical Models. http://www.ai.mit.edu/˜tommi/papers.html.
Liu, C., Rubin, D. B., & Wu, Y. N. (1998). Parameter expansion to accelerate EM: the PX-EM algorithm.
Biometrika, 85, 755–770.
Liu, J. S., & Wu, Y. N. (1999). Parameter expansion for data augmentation. Journal of the American Statistical
Association, 94, 1264–1274.
Luo, Z. Q., & Tseng, P. (1992). On the convergence of the coordinate descent method for convex differentiable
minimization. Journal of Optimization Theory and Applications, 72, 7–35.
MacKay, D. J. (1992). Bayesian interpolation. Neural Computation, 4, 415–447.
Salakhutdinov, R., Roweis, S. T., & Ghahramani, Z. (2003). Optimization with EM and Expectation-Conjugate-
Gradient. Proceedings of International Conference on Machine Learning.
Tipping, M. E. (2000). The relevance vector machine. NIPS (pp. 652–658). The MIT Press.
van Dyk, D. A., & Meng, X. L. (2001). The art of data augmentation (with discussion). Journal of Computa-
tional and Graphical Statistics, 10, 1–111.


Learning to Model Spatial Dependency:
Semi-Supervised Discriminative Random Fields

Chi-Hoon Lee
Department of Computing Science
University of Alberta
chihoon@cs.ualberta.ca

Shaojun Wang (cid:3)
Department of Computer Science and Engineering
Wright State University
shaojun.wang@wright.edu

Feng Jiao
Department of Computing Science
University of Waterloo
fjiao@cs.uwaterloo.ca

Dale Schuurmans, Russell Greiner
Department of Computing Science
University of Alberta
fdale, greinerg@cs.ualberta.ca

Abstract
We present a novel, semi-supervised approach to training discriminative ran-
dom (cid:2)elds (DRFs) that ef(cid:2)ciently exploits labeled and unlabeled training data to
achieve improved accuracy in a variety of image processing tasks. We formulate
DRF training as a form of MAP estimation that combines conditional loglikeli-
hood on labeled data, given a data-dependent prior, with a conditional entropy
regularizer de(cid:2)ned on unlabeled data. Although the training objective is no longer
concave, we develop an ef(cid:2)cient local optimization procedure that produces clas-
si(cid:2)ers that are more accurate than ones based on standard supervised DRF train-
ing. We then apply our semi-supervised approach to train DRFs to segment both
synthetic and real data sets, and demonstrate signi(cid:2)cant improvements over super-
vised DRFs in each case.

1 Introduction
Random (cid:2)eld models are a popular probabilistic framework for representing complex dependencies
in natural image data. The two predominant types of random (cid:2)eld models correspond to generative
versus discriminative graphical models respectively. Classical Markov random (cid:2)elds (MRFs) [2]
follow a traditional generative approach, where one models the joint probability of the observed
image along with the hidden label (cid:2)eld over the pixels. Discriminative random (cid:2)elds (DRFs) [11,
10], on the other hand, directly model the conditional probability over the pixel label (cid:2)eld given
an observed image. In this sense, a DRF is equivalent to a conditional random (cid:2)eld [12] de(cid:2)ned
over a 2-D lattice. Following the basic tenet of Vapnik [18], it is natural to anticipate that learning
an accurate joint model should be more challenging than learning an accurate conditional model.
Indeed, recent experimental evidence shows that DRFs tend to produce more accurate image labeling
models than MRFs, in many applications like gesture recognition [15] and object detection [11, 10,
19, 17].
Although DRFs tend to produce superior pixel labellings to MRFs, partly by relaxing the assumption
of conditional independence of observed images given the labels, the approach relies more heavily
on supervised training. DRF training typically uses labeled image data where each pixel label has
been assigned. However, it is considerably more dif(cid:2)cult to obtain labeled data for image analysis
than for other classi(cid:2)cation tasks, such as document classi(cid:2)cation, since hand-labeling the individual
pixels of each image is much harder than assigning class labels to objects like text documents.
(cid:3)Work done while at University of Alberta

Recently, semi-supervised training has taken on an important new role in many application areas due
to the abundance of unlabeled data. Consequently, many researchers are now working on developing
semi-supervised learning techniques for a variety of approaches, including generative models [14],
self-learning [5], co-training [3], information-theoretic regularization [6, 8], and graph-based trans-
duction [22, 23, 24]. However, most of these techniques have been developed for univariate classi-
(cid:2)cation problems, or class label classi(cid:2)cation with a structured input [22, 23, 24]. Unfortunately,
semi-supervised learning for structured classi(cid:2)cation problems, where the prediction variables are
interdependent in complex ways, have not been as widely studied, with few exceptions [1, 9].
Current work on semi-supervised learning for structured predictors [1, 9] has focused primarily on
simple sequence prediction tasks where learning and inference can be ef(cid:2)ciently performed using
standard dynamic programming. Unfortunately, the problem we address is more challenging, since
the spatial correlations in a 2-D grid structure create numerous dependency cycles. That is, our
graphical model structure prevents exact inference from being feasible. Kumar et al [10] and Vish-
wanathan et al [19] argue that learning a model in the context of approximate inference creates a
greater risk of the over-(cid:2)tting and over estimating.
In this paper, we extend the work on semi-supervised learning for sequence predictors [1, 9], partic-
ularly the CRF based approach [9], to semi-supervised learning of DRFs. There are several advan-
tages of our approach to semi-supervised DRFs. (1) We inherit the standard advantage of discrimina-
tive conditional versus joint model training, while still being able to exploit unlabeled data. (2) The
use of unlabeled data enhances our ability to avoid parameter over-(cid:2)tting and over-estimation in grid
based random (cid:2)elds even when using a learner that uses only approximate inference methods. (3)
We are still able to model spatial correlations in a 2-D lattice, despite the fact that this introduces
dependency cycles in the model. That is, our semi-supervised training procedure can be interpreted
as a MAP estimator, where the parameter prior for the model on labeled data is governed by the
conditional entropy of the model on unlabeled data. This allows us to learn local potentials that
capture spatial correlations while often avoiding local over-estimation. We demonstrate the robust-
ness of our model by applying it to a pixel denoising problem on synthetic images, and also to a
challenging real world problem of segmenting tumor in magnetic resonance images. In each case,
we have obtained signi(cid:2)cant improvements over current baselines based on standard DRF training.
2 Semi-Supervised DRFs (SSDRFs)
We formulate a new semi-supervised DRF training principle based on the standard supervised for-
mulation of [11, 10]. Let x be an observed input image, represented by x = fx i gi2S , where S
is a set of the observed image pixels (nodes). Let y = fyigi2S be the joint set of labels over all
pixels of an image. For simplicity we assume each component y i 2 y ranges over binary classes
Y = f(cid:0)1; 1g. For example, x might be a magnetic resonance image of a brain and y is a realization
of a joint labeling over all pixels that indicates whether each pixel is normal or a tumor. In this case,
Y would be the set of pre-de(cid:2)ned pixel categories (e.g. tumor versus non-tumor). A DRF is a con-
ditional random (cid:2)eld de(cid:2)ned on the pixel labels, conditioned on the observation x. More explicitly,
the joint distribution over the labels y given the observations x is written
1
(cid:9)(cid:23) (yi ; yj ; x)(cid:17)
(cid:8)w (yi ; x) + X
exp (cid:16) X
X
Z(cid:18) (x)
i2S
i2S
j2Ni
Here Ni denotes the neighboring pixels of i. (cid:8)w (yi ; x) = log (cid:16)(cid:27)(yiwT hi (x)(cid:17) denotes the node
potential at pixel i, which quanti(cid:2)es the belief that the class label is y i for the pre-de(cid:2)ned feature
1+e(cid:0)t . (cid:9)(cid:23) (yi ; yj ; x) = yiyj vT (cid:22)ij (x) is an edge potential that captures
vextor hi (x), where (cid:27)(t) = 1
spatial correlations among neighboring pixels (here, the ones at positions i and j ), such that (cid:22) ij (x)
is the pre-de(cid:2)ned feature vector associated with observation x. Z(cid:18) (x) is the normalizing factor, also
known as a (conditional) partition function, which is
(cid:8)w (yi ; x) + X
exp (cid:16) X
X
Z(cid:18) (x) = X
j2Ni
i2S
i2S
y
Finally, (cid:18) = (w; (cid:23) ) are the model parameters. When the edge potentials are set to zero, a DRF
yields a standard logistic regression classi(cid:2)er. The potentials in a DRF can use properties of the
observed image, and thereby relax the conditional independence assumption of MRFs. Moreover,
the edge potentials in a DRF can smooth discontinuities between heterogeneous class pixels, and
also correct errors made by the node potentials.

(cid:9)(cid:23) (yi ; yj ; x)(cid:17)

p(cid:18) (yjx) =

(1)

(2)

Assume we have a set of independent labeled images, D l = (cid:16)(x
(M ) )(cid:17), and a
(M ) ; y
(1) )); (cid:1) (cid:1) (cid:1) ; (x
(1) ; y
set of independent unlabeled images, D u = (cid:16)x
(T ) (cid:17). Our goal is to build a DRF model
(M +1) ; (cid:1) (cid:1) (cid:1) ; x
from the combined set of labeled and unlabeled examples, D l [ Du .
The standard supervised DRF training procedure is based on maximizing the log of the posterior
probability of the labeled examples in D l

CL((cid:18)) =

M
X
k=1

log P (y

(k) jx

(k) ) (cid:0)

T

(cid:23)

(cid:23)

2(cid:28) 2

(3)

A Gaussian prior over the edge parameters (cid:23) is assumed and a uniform prior over parameters w.
Here p((cid:23) ) = N ((cid:23) ; 0; (cid:28) 2 I), where I is the identity matrix. The hyperparameter (cid:28) 2 adds a regular-
ization term. In effect, the Gaussian prior introduces a form of regularization to limit over-(cid:2)tting on
rare features and avoid degeneracy in the case of correlated features.
There are a few issues regarding the supervised learning criteria (3). First, the value of (cid:28) 2 is critical
to the (cid:2)nal result, and unfortunately selecting the appropriate (cid:28) 2 is a non-trivial task, which in turn
makes the learning procedures more challenging and costly [13]. Second, the Gaussian prior is
data-independent, and is not associated with either the unlabeled or labeled observations a priori.
Inspired by the work in [8] and [9], we propose a semi-supervised learning algorithm for DRFs that
makes full use of the available data by exploiting a form of entropy regularization as a prior over the
parameters on Du . Speci(cid:2)cally, for a semi-supervised DRF, we attempt to (cid:2)nd (cid:18) that maximizes the
following objective function

RL((cid:18)) =

M
X
m=1

log p(cid:18) (y

(m) jx

(m) ) + (cid:13)

T
X
m=M +1

X
y

p(cid:18) (yjx

(m) ) log p(cid:18) (yjx

(m) )

(4)

The (cid:2)rst term of (4) is the conditional likelihood over the labeled data set D l , and the second term
is a conditional entropy prior over the unlabeled data set D u , weighted by a tradeoff parameter (cid:13) .
The resulting estimate is then formulated as a MAP estimate.
The goal of the objective (4) is to minimize the uncertainty on possible con(cid:2)gurations over parame-
ters. That is, minimizing the conditional entropy over unlabeled instances provides more con(cid:2)dence
to the algorithm that the hypothetical labellings for the unlabeled data are consistent with the su-
pervised labels, as greater certainty on the estimated labellings coincides with greater conditional
likelihood on the supervised labels, and vice versa. This criterion has been shown to be effective for
univariate classi(cid:2)cation [8], and chain structured CRFs [9]; here we apply it to the 2-D lattice case.

3 Parameter Estimation

Several factors constrain the form of training algorithm: Because of overhead and the risk of diver-
gence, it was not practical to employ a Newton method. Iterative scaling was not possible because
the updates no longer have a closed form. Although the criticism of the gradient descent’s principle
is well taken, it is the most practical approach we will adopt to optimize the semi-supervised MAP
formulation (4) and allows us to improve on standard supervised DRF training.
To formulate a local optimization procedure, we need to compute the gradient of the objective (4)
with respect to the parameters. Unfortunately, because of the nonlinear mapping function (cid:27)(:), we
are not able to represent the gradient of objective function as compactly as [9], which was able to
express the gradient as a product of the covariance matrix of features and the parameter vector (cid:18).
Nevertheless, it is straightforward to show that the derivatives of objective function with respect to
the node parameters w is given by 1

1Note that the derivatives of objective function with respect to the edge parameters (cid:23) are computed analo-
gously.

@

@w

RL((cid:18)) =

M
X
m=1

X
i2Sm

0
@y(m)
i

i wT hi (x(m) )(cid:17) (cid:0) X
(cid:16)1 (cid:0) (cid:27)(y(m)
y

p(cid:18) (yjx(m) )yi (cid:16)1 (cid:0) (cid:27)(yiwT hi (x(m) )(cid:17)1
A hi (x(m) )

(5)

+(cid:13)

T
X
m=M +1

X
i2Sm

0
@X
y

p(cid:18) (yjx

T

(cid:9)(cid:23) (yi ; yj ; x)(cid:17)yi (cid:16)1 (cid:0) (cid:27)(yiw

(m) )(cid:16)(cid:8)w (yi ; x) + X
j2Ni
(cid:0)h X
p(cid:18) (yjx(m) )(cid:16)(cid:8)w (yi ; x) + X
j2Ni
y
p(cid:18) (yjx(m) )yi (cid:16)1 (cid:0) (cid:27)(yiwT hi (x(m) )(cid:17)i1
A hi (x(m) );

(cid:9)(cid:23) (yi ; yj ; x)(cid:17)i

h X
y

hi (x

(m) )(cid:17)

where the (cid:2)rst term in (5) is the gradient of the supervised component of the DRF over labeled data,
and the second term is the gradient of conditional entropy prior of the DRF over unlabeled data.
Given the lattice structure of the joint labels, it is intractable to compute the exact expectation terms
in the above derivatives. It is also intractable to compute the conditional partition function Z (cid:18) (x).
Therefore, as in standard supervised DRFs, we need to incorporate some form of approximation.
Following [2, 11, 10], we incorporate the pseudo-likelihood approximation, which assumes that the
joint conditional distribution can be approximated as a product of the local posterior probabilities
given the neighboring nodes and the observation

p(cid:18) (yjx)

p(cid:18) (yi jyNi ; x)

(cid:25) Y
i2S
1
zi (x)

p(cid:18) (yi jyNi ; x) =

exp (cid:16)(cid:8)w (yi ; x) + X
j2Ni
Using the factored approximation in (7), we can reformulate the training objective as

(cid:9)(cid:23) (yi ; yj ; x)(cid:17)

RLP L ((cid:18)) =

M
X
m=1

Sm
X
i=1

log p(cid:18) (y

(m)
i

jy

(m)
Ni

; x

(m) )

+(cid:13)

T
X
m=M +1

Sm
X
i=1

X
yi

p(cid:18) (yi jyNi ; x

(m) ) log p(cid:18) (yi jyNi x

(m) )

(6)

(7)

(8)

Here, the derivative of the second term in (8), with respect to the potential parameters w and (cid:23) , can
be reformulated as a factored conditional entropy, yielding

@

@w

RLP L ((cid:18))

=

M
X
m=1

X
i2Sm

T
X
m=M +1

0
@y(m)
i
0
@X
yi

i wT hi (x(m) )(cid:17) (cid:0) X
(cid:16)1 (cid:0) (cid:27)(y(m)
yi

p(cid:18) (yi jyNi ; x(m) )yi (cid:16)1 (cid:0) (cid:27)(yiwT hi (x(m) )(cid:17)1
A hi (x(m) )

(9)

T

hi (x

+(cid:13)

(m) )(cid:17)

X
i2Sm

p(cid:18) (yi jyNi ; x

(cid:9)(cid:23) (yi ; yj ; x)(cid:17)i

(cid:9)(cid:23) (yi ; yj ; x)(cid:17)yi (cid:16)1 (cid:0) (cid:27)(yiw

(m) )(cid:16)(cid:8)w (yi ; x) + X
j2Ni
p(cid:18) (yi jyNi x(m) )(cid:16)(cid:8)w (yi ; x) + X
(cid:0)h X
yi
j2Ni
p(cid:18) (yi jyNi ; x(m) )yi (cid:16)1 (cid:0) (cid:27)(yiwT hi (x(m) )(cid:17)i1
h X
A hi (x(m) )
yi
Note that @
@(cid:23) RLP L((cid:18)) is computed analogously. Assuming the factorization, the true conditional
entropy and feature expectations can be computed in terms of local conditional distributions. This al-
lows us ef(cid:2)ciently to approximate the global conditional entropy over unlabeled data. Note that there
may be an over-smoothing issue associated with the pseudo-likelihood approximation, as mentioned
in [10, 19]. However, due to the fast and stable performance of this approximation in the supervised
case [2, 10] we still employ it, but below show that the over-smoothing effect is mitigated by our
data-dependent prior in the MAP objective (4).

(10)

4 Inference
As a result of our formulation, the learning method is tightly coupled with the inference steps.
That is, for the unlabeled data, XU , each time we compute the local conditional covariance (9), we
perform inference steps for each node i and its neighboring nodes N i . Our inference is based on
iterative conditional modes (ICM) [2], and is given by
y(cid:3)
P (yi jyNi ; X )
i = argmax
yi2Y
where, for each position i, we assume that the labels of all of its neighbors y 0 2 Ni are (cid:2)xed. We
could alternatively compute the marginal conditional probability P (y i jX) = PySni
P (yi ; ySni jX )
for each node using the sum-product algorithm (i.e.
loopy belief propagation), which iteratively
propagates the belief of each node to its neighbors. Clearly, there are a range of approximation
methods available, each entailing different accuracy-complexity tradeoffs. However, we have found
that ICM yields good performance at our tasks below, and is probably one of the simplest possible
alternatives.
5 Experiments
Using standard supervised DRF models, Kumar and Hebert [11, 10] reported interesting experi-
mental results for joint classi(cid:2)cation tasks on a 2-D lattice, which represents an image with a DRF
model. Since labeling image data is expensive and tedious, we believe that better results could be
further obtained by formulating a MAP estimation of DRFs by also using the abundant unlabeled
image data. In this section, we present a series of experiments on synthetic and real data sets using
our novel semi-supervised DRFs(SSDRFs). In order to evaluate our model, we compare the results
with those using maximum likelihood estimation of supervised DRFs [11]. There is a major rea-
son that we consider the standard MLE DRF from [11] instead of the parameter regularized DRFs
from [10]: that is, we want to show the difference between the ML and MAP principles without
using any regularization term that can be problematic [10, 13].
(T P +F P +F N ) ; where
To quantify the performance of each model, we used the Jaccard score J =
T P
TP denotes true positives, FP false positives, and FN false negatives. Although there are many
accuracy measures available, we used this score to penalize the false negatives since many imaging
tasks are very imbalanced: that is, only a small percentage of pixels are in the (cid:147)positive(cid:148) class. The
tradeoff parameter, (cid:13) , was hand-tuned on one held out data set and then held (cid:2)xed at 0.2 for all of
the experiments.

5.1 Synthetic image sets
Our primary goal in using synthetic data sets was to demonstrate how well different models clas-
si(cid:2)ed pixels as a binary classi(cid:2)cation over a 2-D lattice in the presence of noise. We generated 18
synthetic data sets, each with its own shape. The intensities of pixels in each image were indepen-
dently corrupted by noise generated from a Gaussian N (0; 1). Figure 1 shows the results of using
supervised DRFs, as well as semi-supervised DRFs. [10, 19] reported over-smoothing effects from
the local approximation approach of PL while our experiments indicate that the over-smoothing is
caused not only by PL approximation, but also by the sensitivity of the regularization to the pa-
rameters. However, using our semi-supervised DRF as a MAP formulation, we have dramatically
improved the performance over standard supervised DRF.
Note that the (cid:2)rst row in Figure 1 shows good results from the standard DRF, while the oversmoothed
outputs are presented in the last row. Although the ML approach may learn proper parameters from
some of data sets, unfortunately its performance has not been consistent since the standard DRF’s
learning of the edge potential tends to be overestimated. For instance, the last row shows that
overestimating parameters of the DRF segment almost all pixels into a class due to the complicated
edges and structures containing non-target area within the target area, while semi-supervised DRF
performance is not degraded at all. Overall, by learning more statistics from unlabeled data, our
model dominates the standard DRF in most cases. This is because our MAP formulation avoids
the overestimate of potentials and uses the edge potential to correct the errors made by the node
potential. Figure 2(a) shows the results over 18 synthetic data sets. Each point above the diagonal
line in Figure 2(a) indicates SSDRF producing higher Jaccard scores for a data set. Note that our
model stably converged as we increased the ratio (nU=nL) of unlabeled data sets in our learning,

 J: 0.933890

 J: 0.933377

 J: 0.729527

 J: 0.957983

 J: 0.008178

 J: 0.923836

F
R
D
S
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

DRF

0.6

0.8

1

4500

4400

4300

4200

4100

4000

3900

1

2

3

4

5

6

7

8

9

10

Figure 1: Outputs from synthetic data sets.
From left to right: Testing instance, Ground
Truth, Logistic Regression (LR), DRF, and
SSDRF.

(a) Accuracy from DRF
and SSDRF for all 18 syn-
thetic data sets

(b) Log likelihood val-
ues (Y axis) for a testing
image by increasing ratio
(X axis) of unlabeled in-
stances for SSDRF
Figure 2: Accuracy and Convergency

as in Figure 2(b), where nU denotes the number of unlabeled images and nL the number of labeled
images. Similar results have also been reported in simple single variable classi(cid:2)cation task [8].

5.2 Brain Tumor Segmentation
We have applied our semi-supervised DRF model to the challenging real world problem of seg-
menting tumor in medical images. Our goal here is to classify each pixel of an magnetic resonance
(MR) image into a pre-de(cid:2)ned category: tumor and non-tumor. This is a very important, yet notori-
ously dif(cid:2)cult, task in surgical planning and radiation therapy which currently involves a signi(cid:2)cant
amount of manual work by human medical experts.
We applied three models to the classi(cid:2)cation of 9 studies from brain tumor MR images. For each
study2 , i, we divided the MR images into DL
i , where an MR image (a.k.a slice) has
i , and DS
i , DU
three modalities available (cid:151) T1, T2, and T1 contrast. Note that each modality for each slice has
66; 564 pixels.
As with much of the related work on automatic brain tumor segmentation (such as [7, 21]), our
training is based on patient-speci(cid:2)c data, where training MR images for a classi(cid:2)er are obtained
from the patient to be tested. Note that the training sets and testing sets for a classi(cid:2)er are disjoint.
i for testing sets, while SSDRF
i and DS
i as the training set and DU
Speci(cid:2)cally, LR and DRF takes DL
i for testing.
i and DS
i for training and DU
i and DU
takes DL
We segmented the (cid:147)enhancing(cid:148) tumor area, the region that appears hyper-intense after injecting
the contrast agent (we also included non-enhancing areas contained within the enhancing contour).
Table 1 and 2 present Jaccard scores of testing DU
i and DS
i for each study, pi , respectively. While
the standard supervised DRF improves over its degenerate model LR by 1%, semi-supervised DRF
signi(cid:2)cantly improves over the supervised DRF by 11%, which is signi(cid:2)cant at p < 0:00566 using
a paired example t test. Considering the fact that MR images contain much noise and the three
modalities are not consistent among slices of the same patient, our improvement is considerable.
Figure 3 shows the segmentation results by overlaying the testing slices with segmented outputs
from the three models. Each row demonstrates the segmentation for a slice, where the white blob
areas for the slice correspond to the enhancing tumor area.
6 Conclusion
We have proposed a new semi-supervised learning algorithm for DRFs, which was formulated as
MAP estimation with conditional entropy over unlabeled data as a data-dependent prior regular-
ization. Our approach is motivated by the information-theoretic argument [8, 16] that unlabeled
examples can provide the most bene(cid:2)t when classes have small overlap. We introduced a simple ap-
proximation approach for this new learning procedure that exploits the local conditional probability
to ef(cid:2)ciently compute the derivative of objective function.
2Each study involves a number (typically 21) of images of a single patient (cid:150) here parallel axial slices through
the head.

Table 1: Jaccard Scores for DU
i .
Testing from DU
i
SSDRF
DRF
LR
53.84
59.81
59.81
84.67
83.65
83.24
75.76
30.17
30.72
79.02
76.16
72.04
75.25
73.59
73.26
88.39
89.61
87.01
75.60
69.91
69.33
73.03
58.89
58.49
83.91
56.49
60.85
77.12
65.57
66.48

Studies
p1
p2
p3
p4
p5
p6
p7
p8
p9
Average

.

Table 2: Jaccard Scores for DS
i
Testing from DS
i
SSDRF
DRF
LR
68.01
68.75
68.75
70.06
69.73
69.61
71.13
21.90
23.11
68.40
63.07
56.52
51.29
52.36
51.38
85.65
86.35
85.43
70.27
68.68
66.71
73.09
45.36
44.92
38.06
20.16
21.11
66.27
54.11
55.15

Studies
p1
p2
p3
p4
p5
p6
p7
p8
p9
Average

Figure 3: From Left to Right: Human Expert, LR, DRF, and SSDRF

We have applied this new approach to the problem of image pixel classi(cid:2)cation tasks. By exploiting
the availability of auxiliary unlabeled data, we are able to improve the performance of the state of
the art supervised DRF approach. Our semi-supervised DRF approach shares all of the bene(cid:2)ts of
the standard DRF training, including the ability to exploit arbitrary potentials in the presence of
dependency cycles, while improving accuracy through the use of the unlabeled data.
The main drawback is the increased training time involved in computing the derivative of the condi-
tional entropy over unlabeled data. Nevertheless, the algorithm is ef(cid:2)cient to be trained on unlabeled
data sets, and to obtain a signi(cid:2)cant improvement in classi(cid:2)cation accuracy over standard supervised
training of DRFs as well as iid logistic regression classi(cid:2)ers. To further accelerate the performance
with respect to accuracy, we may apply loopy belief propagation [20] or graph-cuts [4] as an infer-
ence tool. Since our model is tightly coupled with inference steps during the learning, the proper
choice of an inference algorithm will most likely improve segmentation tasks.

Acknowledgments
This research is supported by the Alberta Ingenuity Centre for Machine Learning, Cross Cancer
Institute, and NSERC. We gratefully acknowledge many helpful suggestions from members of the
Brain Tumor Analysis Project, including Dr. A. Murtha and Dr. J Sander.

References
[1] Y. Altun, D. McAllester, and M. Belkin. Maximum margin semi-supervised learning for struc-
tured variables. In NIPS 18. 2006.
[2] J. Besag. On the statistical analysis of dirty pictures. Journal of Royal Statistical Society.
Series B, 48:3:259(cid:150)302, 1986.
[3] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT,
1998.

[4] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph
cuts. In ICCV (1), pages 377(cid:150)384, 1999.
[5] G. Celeux and G. Govaert. A classi(cid:2)cation EM algorithm for clustering and two stochastic
versions. Comput. Stat. Data Anal., 14(3):315(cid:150)332, 1992.
[6] A. Corduneanu and T. Jaakkola. Data dependent regularization. In O. Chapelle, B. Schoelkopf,
and A. Zien, editors, Semi-Supervised Learning. MIT Press, 2006.
[7] C. Garcia and J.A. Moreno. Kernel based method for segmentation and modeling of magnetic
resonance images. LNCS, 3315:636(cid:150)645, Oct 2004.
[8] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS 17,
2004.
[9] F. Jiao, S. Wang, C. Lee, R. Greiner, and D Schuurmans. Semi-supervised conditional random
(cid:2)elds for improved sequence segmentation and labeling. In COLING/ACL, 2006.
[10] S. Kumar and M. Hebert. Discriminative (cid:2)elds for modeling spatial dependencies in natural
images. In NIPS 16, 2003.
[11] S. Kumar and M. Hebert. Discriminative random (cid:2)elds: A discriminative framework for con-
textual interaction in classi(cid:2)cation. In CVPR, 2003.
[12] J. Lafferty, F. Pereira, and A. McCallum. Conditional random (cid:2)elds: Probabilistic models for
segmenting and labeling sequence data. In ICML, 2001.
[13] C. Lee, R. Greiner, and O. Za¤(cid:17)ane. Ef(cid:2)cient spatial classi(cid:2)cation using decoupled conditional
random (cid:2)elds. In 10th European Conference on Principles and Practice of Knowledge Dis-
covery in Databases, pages 272(cid:150)283, 2006.
[14] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classi(cid:2)cation from labeled and
unlabeled documents using EM. Machine Learning, 39(2/3):103(cid:150)134, 2000.
[15] A. Quattoni, M. Collins, and T. Darrell. Conditional random (cid:2)elds for object recognition. In
NIPS 17, 2004.
[16] S. Roberts, R. Everson, and I. Rezek. Maximum certainty data partitioning, 2000.
[17] A. Torralba, K. Murphy, and W. Freeman. Contextual models for object detection using
boosted random (cid:2)elds. In NIPS 17, 2004.
[18] V. Vapnik. Statistical Learning Theory. John-Wiley, 1998.
[19] S.V.N. Vishwanathan, N. Schraudolph, M. Schmidt, and K. Murphy. Accelerated training of
conditional random (cid:2)elds with stochastic gradient methods. In ICML, 2006.
[20] J. Yedidia, W. Freeman, and Y. Weiss. Generalized belief propagation. In NIPS 13, pages
689(cid:150)695, 2000.
[21] J. Zhang, K. Ma, M.H. Er, and V. Chong. Tumor segmentation from magnetic resonance
imaging by learning via one-class support vector machine. Intl. Workshop on Advanced Image
Technology, 2004.
[22] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, and B. Sch ¤olkopf. Learning with local and
global consistency. In NIPS 16, 2004.
[23] D. Zhou, J. Huang, and B. Sch ¤olkopf. Learning from labeled and unlabeled data on a directed
graph. In ICML, 2005.
[24] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian (cid:2)elds and
harmonic functions. In ICML, 2003.


GraphLaplacianRegularizationforLarge-ScaleSemideﬁniteProgrammingKilianQ.WeinbergerDeptofComputerandInformationScienceUofPennsylvania,Philadelphia,PA19104kilianw@seas.upenn.eduFeiShaComputerScienceDivisionUCBerkeley,CA94720feisha@cs.berkeley.eduQihuiZhuDeptofComputerandInformationScienceUofPennsylvania,Philadelphia,PA19104qihuizhu@seas.upenn.eduLawrenceK.SaulDeptofComputerScienceandEngineeringUCSanDiego,LaJolla,CA92093saul@cs.ucsd.eduAbstractInmanyareasofscienceandengineering,theproblemariseshowtodiscoverlowdimensionalrepresentationsofhighdimensionaldata.Recently,anumberofre-searchershaveconvergedoncommonsolutionstothisproblemusingmethodsfromconvexoptimization.Inparticular,manyresultshavebeenobtainedbycon-structingsemideﬁniteprograms(SDPs)withlowranksolutions.WhiletherankofmatrixvariablesinSDPscannotbedirectlyconstrained,ithasbeenobservedthatlowranksolutionsemergenaturallybycomputinghighvarianceormaximaltracesolutionsthatrespectlocaldistanceconstraints.Inthispaper,weshowhowtosolveverylargeproblemsofthistypebyamatrixfactorizationthatleadstomuchsmallerSDPsthanthosepreviouslystudied.Thematrixfactorizationisde-rivedbyexpandingthesolutionoftheoriginalproblemintermsofthebottomeigenvectorsofagraphLaplacian.ThesmallerSDPsobtainedfromthismatrixfactorizationyieldverygoodapproximationstosolutionsoftheoriginalproblem.Moreover,theseapproximationscanbefurtherreﬁnedbyconjugategradientde-scent.Weillustratetheapproachonlocalizationinlargescalesensornetworks,whereoptimizationsinvolvingtensofthousandsofnodescanbesolvedinjustafewminutes.1IntroductionInmanyareasofscienceandengineering,theproblemariseshowtodiscoverlowdimensionalrepre-sentationsofhighdimensionaldata.Typically,thishighdimensionaldataisrepresentedintheformoflargegraphsormatrices.Suchdataarisesinmanyapplications,includingmanifoldlearning[12],robotnavigation[3],proteinclustering[6],andsensorlocalization[1].Inalltheseapplications,thechallengeistocomputelowdimensionalrepresentationsthatareconsistentwithobservedmeasure-mentsoflocalproximity.Forexample,inrobotpathmapping,therobot’slocationsmustbeinferredfromthehighdimensionaldescriptionofitsstateintermsofsensorimotorinput.Inthissetting,weexpectsimilarstatedescriptionstomaptosimilarlocations.Likewise,insensornetworks,thelocationsofindividualnodesmustbeinferredfromtheestimateddistancesbetweennearbysensors.Again,thechallengeistoﬁndaplanarrepresentationofthesensorsthatpreserveslocaldistances.Ingeneral,itispossibletoformulatetheseproblemsassimpleoptimizationsoverthelowdimen-sionalrepresentations!xiofindividualinstances(e.g.,robotstates,sensornodes).Themoststraight-forwardformulations,however,leadtonon-convexoptimizationsthatareplaguedbylocalminima.Forthisreason,large-scaleproblemscannotbereliablysolvedinthismanner.Amorepromisingapproachreformulatestheseproblemsasconvexoptimizations,whoseglobalminimacanbeefﬁcientlycomputed.Convexityisobtainedbyrecastingtheproblemsasoptimiza-tionsovertheinnerproductmatricesXij=!xi·!xj.Therequiredoptimizationscanthenberelaxedasinstancesofsemideﬁniteprogramming[10],orSDPs.Twodifﬁcultiesarise,however,fromthisapproach.First,onlylowranksolutionsfortheinnerproductmatricesXyieldlowdimensionalrepresentationsforthevectors!xi.Rankconstraints,however,arenon-convex;thusSDPsandotherconvexrelaxationsarenotguaranteedtoyieldthedesiredlowdimensionalsolutions.Second,theresultingSDPsdonotscaleverywelltolargeproblems.Despitethetheoreticalguaranteesthatfol-lowfromconvexity,itremainsprohibitivelyexpensivetosolveSDPsovermatriceswith(say)tensofthousandsofrowsandsimilarlylargenumbersofconstraints.Fortheﬁrstproblemof“rankregularization”,anapparentsolutionhasemergedfromrecentworkinmanifoldlearning[12]andnonlineardimensionalityreduction[14].ThisworkhasshownthatwhiletherankofsolutionsfromSDPscannotbedirectlyconstrained,lowranksolutionsoftenemergenat-urallybycomputingmaximaltracesolutionsthatrespectlocaldistanceconstraints.MaximizingthetraceoftheinnerproductmatrixXhastheeffectofmaximizingthevarianceofthelowdimensionalrepresentation{!xi}.Thisideawasoriginallyintroducedas“semideﬁniteembedding”[12,14],thenlaterdescribedas“maximumvarianceunfolding”[9](andyetlateras“kernelregularization”[6,7]).Here,weadoptthenamemaximumvarianceunfolding(MVU)whichseemstobecurrentlyac-cepted[13,15]asbestcapturingtheunderlyingintuition.Thispaperaddressesthesecondproblemmentionedabove:howtosolveverylargeproblemsinMVU.Weshowhowtosolvesuchproblemsbyapproximatelyfactorizingthelargen×nmatrixXasX≈QYQ!whereQisapre-computedn×mrectangularmatrixwithm#n.Thefactorizationleavesonlythemuchsmallerm×mmatrixYtobeoptimizedwithrespecttolocaldistancecon-straints.Withthisfactorization,andbycollectingconstraintsusingtheSchurcomplementlemma,weshowhowtorewritetheoriginaloptimizationoverthelargematrixXasasimpleSDPinvolvingthesmallermatrixY.ThisSDPcanbesolvedveryquickly,yieldinganaccurateapproximationtothesolutionoftheoriginalproblem.Moreover,ifdesirable,thissolutioncanbefurtherreﬁned[1]by(non-convex)conjugategradientdescentinthevectors{!xi}.ThemaincontributionofthispaperisthematrixfactorizationthatmakesitpossibletosolvelargeproblemsinMVU.Wheredoesthefactorizationcomefrom?Eitherimplicitlyorexplicitly,allproblemsofthissortspecifyagraphwhosenodesrepresentthevectors{!xi}andwhoseedgesrepresentlocaldistanceconstraints.Thematrixfactorizationisobtainedbyexpandingthelowdimensionalrepresentationofthesenodes(e.g.,sensorlocations)intermsofthem#nbottom(smoothest)eigenvectorsofthegraphLaplacian.Duetothelocaldistanceconstraints,oneexpectsthelowdimensionalrepresentationofthesenodestovarysmoothlyasonetraversesedgesinthegraph.ThepresumptionofsmoothnessjustiﬁesthepartialorthogonalexpansionintermsofthebottomeigenvectorsofthegraphLaplacian[5].Similarideashavebeenwidelyappliedingraph-basedapproachestosemi-supervisedlearning[4].Matrixfactorizationsofthistypehavealsobeenpreviouslystudiedformanifoldlearning;in[11,15],though,thelocaldistanceconstraintswerenotproperlyformulatedtopermitthelarge-scaleapplicationsconsideredhere,whilein[8],theapproximationwasnotconsideredinconjunctionwithavariance-maximizingtermtofavorlowdimensionalrepresentations.Theapproachinthispaperappliesgenerallytoanysettinginwhichlowdimensionalrepresenta-tionsarederivedfromanSDPthatmaximizesvariancesubjecttolocaldistanceconstraints.Forconcreteness,weillustratetheapproachontheproblemoflocalizationinlargescalesensornet-works,asrecentlydescribedby[1].Here,weareabletosolveoptimizationsinvolvingtensofthousandsofnodesinjustafewminutes.SimilarapplicationstotheSDPsthatariseinmanifoldlearning[12],robotpathmapping[3],andproteinclustering[6,7]presentnoconceptualdifﬁculty.Thispaperisorganizedasfollows.Section2reviewstheproblemoflocalizationinlargescalesensornetworksanditsformulationby[1]asanSDPthatmaximizesvariancesubjecttolocaldistanceconstraints.Section3showshowwesolvelargeproblemsofthisform—byapproximatingtheinnerproductmatrixofsensorlocationsastheproductofsmallermatrices,bysolvingthesmallerSDPthatresultsfromthisapproximation,andbyreﬁningthesolutionfromthissmallerSDPusinglocalsearch.Section4presentsourexperimentalresultsonseveralsimulatednetworks.Finally,section5concludesbydiscussingfurtheropportunitiesforresearch.2SensorlocalizationviamaximumvarianceunfoldingFigure1:SensorsdistributedoverUScities.Dis-tancesareestimatedbetweennearbycitieswithinaﬁxedradius.Theproblemofsensorlocalizationisbestil-lustratedbyexample;seeFig.1.ImaginethatsensorsarelocatedinmajorcitiesthroughoutthecontinentalUS,andthatnearbysensorscanestimatetheirdistancestooneanother(e.g.,viaradiotransmitters).Fromonlythislocalinfor-mation,theproblemofsensorlocalizationistocomputetheindividualsensorlocationsandtoidentifythewholenetworktopology.Inpurelymathematicalterms,theproblemcanbeviewedascomputingalowrankembeddingintwoorthreedimensionalEuclideanspacesubjecttolocaldistanceconstraints.Weassumetherearensensorsdistributedintheplaneandformulatetheproblemasanoptimizationovertheirplanarcoordinates!x1,...,!xn∈%2.(Sensorlocalizationinthreedimensionalspacecanbesolvedinasimilarway.)Wedeﬁneaneighborrelationi∼jiftheithandjthsensorsaresufﬁcientlyclosetoestimatetheirpairwisedistancevialimited-rangeradiotransmission.Fromsuch(noisy)estimatesoflocalpairwisedistances{dij},theproblemofsensorlocalizationistoinfertheplanarcoordi-nates{!xi}.Workonthisproblemhastypicallyfocusedonminimizingthesum-of-squareslossfunction[1]thatpenalizeslargedeviationsfromtheestimateddistances:min!x1,...,!xn!i∼j"’!xi−!xj’2−d2ij#2(1)Insomeapplications,thelocationsofafewsensorsarealsoknowninadvance.Forsimplicity,inthisworkweconsiderthescenariowherenosuch“anchorpoints”areavailableaspriorknowledge,andthegoalissimplytopositionthesensorsuptoaglobalrotation,reﬂection,andtranslation.Thus,totheaboveoptimization,withoutlossofgeneralitywecanaddthecenteringconstraint:$$$!i!xi$$$2=0.(2)Itisstraightforwardtoextendourapproachtoincorporateanchorpoints,whichgenerallyleadstoevenbettersolutions.Inthiscase,thecenteringconstraintisnotneeded.Theoptimizationineq.(1)isnotconvex;hence,itislikelytobetrappedbylocalminima.Byrelax-ingtheconstraintthatthesensorlocations!xilieinthe%2plane,weobtainaconvexoptimizationthatismuchmoretractable[1].Thisisdonebyrewritingtheoptimizationineqs.(1–2)intermsoftheelementsoftheinnerproductmatrixXij=!xi·!xj.Inthisway,weobtain:Minimize:!i∼j"Xii−2Xij+Xjj−d2ij#2subjectto:(i)!ijXij=0and(ii)X)0.(3)Theﬁrstconstraintcentersthesensorsontheorigin,asineq.(2),whilethesecondconstraintspeciﬁesthatXispositivesemideﬁnite,whichisnecessarytointerpretitasaninnerproductmatrixinEuclideanspace.Inthiscase,thevectors{!xi}aredetermined(uptorotation)bysingularvaluedecomposition.Theconvexrelaxationoftheoptimizationineqs.(1–2)dropstheconstraintthatthatthevectors!xilieinthe%2plane.Instead,thevectorswillmoregenerallylieinasubspaceofdimensionalityequaltotherankofthesolutionX.Toobtainplanarcoordinates,onecanprojectthesevectorsintotheirtwodimensionalsubspaceofmaximumvariance,obtainedfromthetoptwoeigenvectorsofX.Unfortunately,iftherankofXishigh,thisprojectionlosesinformation.AstheerroroftheprojectiongrowswiththerankofX,wewouldliketoenforcethatXhaslowrank.However,therankofamatrixisnotaconvexfunctionofitselements;thusitcannotbedirectlyconstrainedaspartofaconvexoptimization.Mindfulofthisproblem,theapproachtosensorlocalizationin[1]borrowsanideafromrecentworkinunsupervisedlearning[12,14].Verysimply,anextratermisaddedtothelossfunctionthatfavorssolutionswithhighvariance,orequivalently,solutionswithhightrace.(Thetraceisproportionaltothevarianceassumingthatthesensorsarecenteredontheorigin,sincetr(X)=%i’!xi’2.)Theextravarianceterminthelossfunctionfavorslowranksolutions;intuitively,itisbasedontheobservationthataﬂatpieceofpaperhasgreaterdiameterthanacrumpledone.Followingthisintuition,weconsiderthefollowingoptimization:Maximize:tr(X)−ν!i∼j"Xii−2Xij+Xjj−d2ij#2subjectto:(i)!ijXij=0and(ii)X)0.(4)Theparameterν>0balancesthetrade-offbetweenmaximizingvarianceandpreservinglocaldistances.Thisgeneralframeworkfortradingoffglobalvarianceversuslocalrigidityhascometobeknownasmaximumvarianceunfolding(MVU)[9,15,13].Asdemonstratedin[1,9,6,14],thesetypesofoptimizationscanbewrittenassemideﬁniteprograms(SDPs)[10].Manygeneral-purposesolversforSDPsexistinthepublicdomain(e.g.,[2]),butevenforsystemswithsparseconstraints,theydonotscaleverywelltolargeproblems.Thus,forsmallnetworks,thisapproachtosensorlocalizationisviable,butforlargenetworks(n∼104),exactsolutionsareprohibitivelyexpensive.Thisleadsustoconsiderthemethodsinthenextsection.3Large-scalemaximumvarianceunfoldingMostSDPsolversarebasedoninterior-pointmethodswhosetime-complexityscalescubicallyinthematrixsizeandnumberofconstraints[2].TosolvelargeproblemsinMVU,evenapproximately,wemustthereforereducethemtoSDPsoversmallmatriceswithsmallnumbersofconstraints.3.1MatrixfactorizationToobtainanoptimizationinvolvingsmallermatrices,weappealtoideasinspectralgraphtheory[5].Thesensornetworkdeﬁnesaconnectedgraphwhoseedgesrepresentlocalpairwiseconnectivity.Whenevertwonodesshareanedgeinthisgraph,weexpectthelocationsofthesenodestoberelativelysimilar.Wecanviewthelocationofthesensorsasafunctionthatisdeﬁnedoverthenodesofthisgraph.Becausetheedgesrepresentlocaldistanceconstraints,weexpectthisfunctiontovarysmoothlyaswetraverseedgesinthegraph.Theideaofgraphregularizationinthiscontextisbestunderstoodbyanalogy.Ifasmoothfunctionisdeﬁnedonaboundedintervalof%1,thenfromrealanalysis,weknowthatitcanbewellapproximatedbyaloworderFourierseries.Asimilartypeofloworderapproximationexistsifasmoothfunctionisdeﬁnedoverthenodesofagraph.Thislow-orderapproximationongraphswillenableustosimplifytheSDPsforMVU,justaslow-orderFourierexpansionshavebeenusedtoregularizemanyproblemsinstatisticalestimation.FunctionapproximationsongraphsaremostnaturallyderivedfromtheeigenvectorsofthegraphLaplacian[5].Forunweightedgraphs,thegraphLaplacianLcomputesthequadraticformf!Lf=!i∼j(fi−fj)2(5)onfunctionsf∈%ndeﬁnedoverthenodesofthegraph.TheeigenvectorsofLprovideasetofbasisfunctionsoverthenodesofthegraph,orderedbysmoothness.Thus,smoothfunctionsfcanbewellapproximatedbylinearcombinationsofthebottomeigenvectorsofL.Expandingthesensorlocations!xiintermsoftheseeigenvectorsyieldsacompactfactorizationfortheinnerproductmatrixX.Supposethat!xi≈%mα=1Qiα!yα,wherethecolumnsofthen×mrectangularmatrixQstorethembottomeigenvectorsofthegraphLaplacian(excludingtheuniformeigenvectorwithzeroeigenvalue).Notethatinthisapproximation,thematrixQcanbecheaplypre-computedfromtheunweightedconnectivitygraphofthesensornetwork,whilethevectors!yαplaytheroleofunknownsthatdependinacomplicatedwayonthelocaldistanceestimatesdij.LetYdenotethem×minnerproductmatrixofthesevectors,withelementsYαβ=!yα·!yβ.Fromthelow-orderapproximationtothesensorlocations,weobtainthematrixfactorization:X≈QYQ!.(6)Eq.(6)approximatestheinnerproductmatrixXastheproductofmuchsmallermatrices.Usingthisapproximationforlocalizationinlargescalenetworks,wecansolveanoptimizationforthemuchsmallerm×mmatrixY,asopposedtotheoriginaln×nmatrixX.TheoptimizationforthematrixYisobtainedbysubstitutingeq.(6)whereverthematrixXappearsineq.(4).SomesimpliﬁcationsoccurduetothestructureofthematrixQ.BecausethecolumnsofQstoremutuallyorthogonaleigenvectors,itfollowsthattr(QYQ!)=tr(Y).BecausewedonotincludetheuniformeigenvectorinQ,itfollowsthatQYQ!automaticallysatisﬁesthecenteringconstraint,whichcanthereforebedropped.Finally,itissufﬁcienttoconstrainY)0,whichimpliesthatQYQ!)0.Withthesesimpliﬁcations,weobtainthefollowingoptimization:Maximize:tr(Y)−ν!i∼j&(QYQ!)ii−2(QYQ!)ij+(QYQ!)jj−d2ij’2subjectto:Y)0(7)Eq.(6)canalternatelybeviewedasaformofregularization,asitconstrainsneighboringsensorstohavenearbylocationsevenwhentheestimatedlocaldistancesdijsuggestotherwise(e.g.,duetonoise).Similarformsofgraphregularizationhavebeenwidelyusedinsemi-supervisedlearning[4].3.2FormulationasSDPAsnotedearlier,ourstrategyforsolvinglargeproblemsinMVUdependsoncastingtherequiredoptimizationsasSDPsoversmallmatriceswithfewconstraints.Thematrixfactorizationineq.(6)leadstoanoptimizationoverthem×mmatrixY,asopposedtothen×nmatrixX.Inthissection,weshowhowtocastthisoptimizationasacorrespondinglysmallSDP.ThisrequiresustoreformulatethequadraticoptimizationoverY)0ineq.(4)intermsofalinearobjectivefunctionwithlinearorpositivesemideﬁniteconstraints.Westartbynotingthattheobjectivefunctionineq.(7)isaquadraticfunctionoftheelementsofthematrixY.LetY∈%m2denotethevectorobtainedbyconcatenatingallthecolumnsofY.Withthisnotation,theobjectivefunction(uptoanadditiveconstant)takestheformb!Y−Y!AY,(8)whereA∈%m2×m2isthepositivesemideﬁnitematrixthatcollectsallthequadraticcoefﬁcientsintheobjectivefunctionandb∈%m2isthevectorthatcollectsallthelinearcoefﬁcients.Notethatthetracetermintheobjectivefunction,tr(Y),isabsorbedbythevectorb.Withtheabovenotation,wecanwritetheoptimizationineq.(7)asanSDPinstandardform.Asin[8],thisisdoneintwosteps.First,weintroduceadummyvariable#thatservesasalowerboundonthequadraticpieceoftheobjectivefunctionineq.(8).Next,weexpressthisboundasalinearmatrixinequalityviatheSchurcomplementlemma.Combiningthesesteps,weobtaintheSDP:Maximize:b!Y−#subjectto:(i)Y)0and(ii)(IA12Y(A12Y)!#))0.(9)InthesecondconstraintofthisSDP,wehaveusedItodenotethem2×m2identitymatrixandA12todenotethematrixsquareroot.Thus,viatheSchurlemma,thisconstraintexpressesthelowerbound#≥Y!AY,andtheSDPisseentobeequivalenttotheoptimizationineqs.(7–8).TheSDPineq.(9)representsadrasticreductionincomplexityfromtheoptimizationineq.(7).TheonlyvariablesoftheSDParethem(m+1)/2elementsofYandtheunknownscalar#.TheonlyconstraintsarethepositivesemideﬁniteconstraintonYandthelinearmatrixinequalityofsizem2×m2.NotethatthecomplexityofthisSDPdoesnotdependonthenumberofnodesoredgesinthenetwork.Asaresult,thisapproachscalesverywelltolargeproblemsinsensorlocalization.Intheaboveformulation,itisworthnotingtheimportantroleplayedbyquadraticpenalties.TheuseoftheSchurlemmaineq.(9)wasconditionedonthequadraticformoftheobjectivefunctionineq.(7).PreviousworkonMVUhasenforcedthedistanceconstraintsasstrictequalities[12],asone-sidedinequalities[9,11],andassoftconstraintswithlinearpenalties[14].ExpressedasSDPs,theseearlierformulationsofMVUinvolvedasmanyconstraintsasedgesintheunderlyinggraph,evenwiththematrixfactorizationineq.(6).Thus,thespeed-upsobtainedhereoverpreviousapproachesarenotmerelyduetographregularization,butmorepreciselytoitsuseinconjunctionwithquadraticpenalties,allofwhichcanbecollectedinasinglelinearmatrixinequalityviatheSchurlemma.3.3Gradient-basedimprovementWhilethematrixfactorizationineq.(6)leadstomuchmoretractableoptimizations,itonlyprovidesanapproximationtotheglobalminimumoftheoriginallossfunctionineq.(1).Assuggestedin[1],wecanreﬁnetheapproximationfromeq.(9)byusingitasinitialstartingpointforgradientdescentineq.(1).Ingeneral,gradientdescentonnon-convexfunctionscanconvergetoundesirablelocalminima.Inthissetting,however,thesolutionoftheSDPineq.(9)providesahighlyaccurateinitialization.Thoughnotheoreticalguaranteescanbemade,inpracticewehaveobservedthatthisinitializationoftenliesinthebasinofattractionofthetrueglobalminimum.Ourmostrobustresultswereobtainedbyatwo-stepprocess.First,startingfromthem-dimensionalsolutionofeq.(9),weusedconjugategradientmethodstomaximizetheobjectivefunctionineq.(4).ThoughthisobjectivefunctioniswrittenintermsoftheinnerproductmatrixX,thehill-climbinginthisstepwasperformedintermsofthevectors!xi∈%m.Whilenotalwaysnecessary,thisﬁrststepwasmainlyhelpfulforlocalizationinsensornetworkswithirregular(andparticularlynon-convex)boundaries.ItseemsgenerallydifﬁculttorepresentationsuchboundariesintermsofthebottomeigenvectorsofthegraphLaplacian.Next,weprojectedtheresultsofthisﬁrststepintothe%2planeanduseconjugategradientmethodstominimizethelossfunctionineq.(1).Thissecondstephelpstocorrectpatchesofthenetworkwhereeitherthegraphregularizationleadstooversmoothingand/ortherankconstraintisnotwellmodeledbyMVU.4ResultsWeevaluatedouralgorithmontwosimulatedsensornetworksofdifferentsizeandtopology.Wedidnotassumeanypriorknowledgeofsensorlocations(e.g.,fromanchorpoints).Weaddedwhitenoisetoeachlocaldistancemeasurementwithastandarddeviationof10%ofthetruelocaldistance.!0.8!0.6!0.4!0.200.20.40.60.8!0.6!0.4!0.200.20.40.6!0.8!0.6!0.4!0.200.20.40.60.8!0.6!0.4!0.200.20.40.6Figure2:Sensorlocationsinferredforn=1055largestcitiesinthecontinentalUS.Onaverage,eachsensorestimatedlocaldistancesto18neighbors,withmeasurementscorruptedby10%Gaus-siannoise;seetext.Left:sensorlocationsobtainedbysolvingtheSDPineq.(9)usingthem=10bottomeigenvectorsofthegraphLaplacian(computationtime4s).Despitetheobviousdistortion,thesolutionprovidesagoodinitialstartingpointforgradient-basedimprovement.Right:sensorlocationsafterpost-processingbyconjugategradientdescent(additionalcomputationtime3s).Figure3:Resultsonasimulatednetworkwithn=20000uniformlydistributednodesinsideacenteredunitsquare.Seetextfordetails.Theﬁrstsimulatednetwork,showninFig.1,placednodesatscaledlocationsofthen=1055largestcitiesinthecontinentalUS.Eachnodeestimatedthelocaldistancetoupto18othernodeswithinaradiusofsizer=0.09.TheSDPineq.(9)wassolvedusingthem=10bottomeigenvectorsofthegraphLaplacian.Fig.2showsthesolutionfromthisSDP(ontheleft),aswellastheﬁnalresultaftergradient-basedimprovement(ontheright),asdescribedinsection3.3.Fromtheﬁgure,itcanbeseenthatthesolutionoftheSDPrecoversthegeneraltopologyofthenetworkbuttendstoclumpnodestogether,especiallyneartheboundaries.Aftergradient-basedimprovement,however,theinferredlocationsdifferverylittlefromthetruelocations.TheconstructionandsolutionoftheSDPrequired4softotalcomputationtimeona2.4GHzPentium4desktopcomputer,whilethepost-processingbyconjugategradientdescenttookanadditional3s.05101520objectivetimeobjective valuenumber of eigenvectorscomputation time (in sec)2.01.0480240Figure4:Left:thevalueofthelossfunc-tionineq.(1)fromthesolutionoftheSDPineq.(8).Right:thecomputationtimetosolvetheSDP.Bothareplottedversusthenumberofeigenvectors,m,inthematrixfactoriza-tion.Thesecondsimulatednetwork,showninFig.3,placednodesatn=20000uniformlysampledpointsinsidetheunitsquare.Thenodeswerethencenteredontheorigin.Eachnodeestimatedthelo-caldistancetoupto20othernodeswithinaradiusofsizer=0.06.TheSDPineq.(9)wassolvedusingthem=10bottomeigenvectorsofthegraphLapla-cian.ThecomputationtimetoconstructandsolvetheSDPwas19s.Thefollow-upconjugategradi-entoptimizationrequired52sfor100linesearches.Fig.3illustratestheabsolutepositionalerrorsofthesensorlocationscomputedinthreedifferentways:thesolutionfromtheSDPineq.(8),thereﬁnedso-lutionobtainbyconjugategradientdescent,andthe“baseline”solutionobtainedbyconjugategradientdescentfromarandominitialization.Fortheseplots,thesensorswerecoloredsothatthegroundtruthpositioningrevealsthewordCONVEXinthefore-groundwitharadialcolorgradientinthebackground.Thereﬁnedsolutioninthethirdpanelisseentoyieldhighlyaccurateresults.(Note:therepresentationsinthesecondandfourthpanelswerescaledbyfactorsof0.50and1028,respectively,tohavethesamesizeastheothers.)Wealsoevaluatedtheeffectofthenumberofeigenvectors,m,usedintheSDP.(Wefocusedontheroleofm,notingthatpreviousstudies[1,7]havethoroughlyinvestigatedtheroleofparameterssuchastheweightconstantν,thesensorradiusr,andthenoiselevel.)ForthesimulatednetworkwithnodesatUScities,Fig.4plotsthevalueofthelossfunctionineq.(1)obtainedfromthesolutionofeq.(8)asafunctionofm.ItalsoplotsthecomputationtimerequiredtocreateandsolvetheSDP.Theﬁgureshowsthatmoreeigenvectorsleadtobettersolutions,butattheexpenseofincreasedcomputationtime.Inourexperience,thereisa“sweetspot”aroundm≈10thatbestmanagesthistradeoff.Here,theSDPcantypicallybesolvedinsecondswhilestillprovidingasufﬁcientlyaccurateinitializationforrapidconvergenceofsubsequentgradient-basedmethods.Finally,thoughnotreportedhereduetospaceconstraints,wealsotestedourapproachonvariousdatasetsinmanifoldlearningfrom[12].Ourapproachgenerallyreducedpreviouscomputationtimesofminutesorhourstosecondswithnonoticeablelossofaccuracy.5DiscussionInthispaper,wehaveproposedanapproachforsolvinglarge-scaleproblemsinMVU.TheapproachmakesuseofamatrixfactorizationcomputedfromthebottomeigenvectorsofthegraphLaplacian.Thefactorizationyieldsaccurateapproximatesolutionswhichcanbefurtherreﬁnedbylocalsearch.Thepoweroftheapproachwasillustratedbysimulatedresultsonsensorlocalization.Thenetworksinsection4havefarmorenodesandedgesthancouldbeanalyzedbypreviouslyformulatedSDPsforthesetypesofproblems[1,3,6,14].Beyondtheproblemofsensorlocalization,ourapproachappliesquitegenerallytoothersettingswherelowdimensionalrepresentationsareinferredfromlocaldistanceconstraints.Thuswearehopefulthattheideasinthispaperwillﬁndfurtheruseinareassuchasroboticpathmapping[3],proteinclustering[6,7],andmanifoldlearning[12].AcknowledgmentsThisworkwassupportedbyNSFAward0238323.References[1]P.Biswas,T.-C.Liang,K.-C.Toh,T.-C.Wang,andY.Ye.Semideﬁniteprogrammingapproachesforsensornetworklocalizationwithnoisydistancemeasurements.IEEETransactionsonAutomationScienceandEngineering,3(4):360–371,2006.[2]B.Borchers.CSDP,aClibraryforsemideﬁniteprogramming.OptimizationMethodsandSoftware11(1):613-623,1999.[3]M.Bowling,A.Ghodsi,andD.Wilkinson.Actionrespectingembedding.InProceedingsoftheTwentySecondInternationalConferenceonMachineLearning(ICML-05),pages65–72,Bonn,Germany,2005.[4]O.Chapelle,B.Sch¨olkopf,andA.Zien,editors.Semi-SupervisedLearning.MITPress,Cambridge,MA,2006.[5]F.R.K.Chung.SpectralGraphTheory.AmericanMathematicalSociety,1997.[6]F.Lu,S.Keles,S.Wright,andG.Wahba.Frameworkforkernelregularizationwithapplicationtoproteinclustering.ProceedingsoftheNationalAcademyofSciences,102:12332–12337,2005.[7]F.Lu,Y.Lin,andG.Wahba.Robustmanifoldunfoldingwithkernelregularization.TechnicalReport1108,DepartmentofStatistics,UniversityofWisconsin-Madison,2005.[8]F.ShaandL.K.Saul.Analysisandextensionofspectralmethodsfornonlineardimensionalityreduction.InProceedingsoftheTwentySecondInternationalConferenceonMachineLearning(ICML-05),pages785–792,Bonn,Germany,2005.[9]J.Sun,S.Boyd,L.Xiao,andP.Diaconis.ThefastestmixingMarkovprocessonagraphandaconnectiontoamaximumvarianceunfoldingproblem.SIAMReview,48(4):681–699,2006.[10]L.VandenbergheandS.P.Boyd.Semideﬁniteprogramming.SIAMReview,38(1):49–95,March1996.[11]K.Q.Weinberger,B.D.Packer,andL.K.Saul.Nonlineardimensionalityreductionbysemideﬁniteprogrammingandkernelmatrixfactorization.InZ.GhahramaniandR.Cowell,editors,ProceedingsoftheTenthInternationalWorkshoponArtiﬁcialIntelligenceandStatistics(AISTATS-05),pages381–388,Barbados,WestIndies,2005.[12]K.Q.WeinbergerandL.K.Saul.Unsupervisedlearningofimagemanifoldsbysemideﬁniteprogram-ming.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR-04),volume2,pages988–995,WashingtonD.C.,2004.ExtendedversioninInternationalJournalofCom-puterVision,70(1):77-90,2006.[13]K.Q.WeinbergerandL.K.Saul.Anintroductiontononlineardimensionalityreductionbymaximumvarianceunfolding.InProceedingsoftheTwentyFirstNationalConferenceonArtiﬁcialIntelligence(AAAI-06),Cambridge,MA,2006.[14]K.Q.Weinberger,F.Sha,andL.K.Saul.Learningakernelmatrixfornonlineardimensionalityreduction.InProceedingsoftheTwentyFirstInternationalConferenceonMachineLearning(ICML-04),pages839–846,Banff,Canada,2004.[15]L.Xiao,J.Sun,andS.Boyd.Adualityviewofspectralmethodsfordimensionalityreduction.InProceedingsoftheTwentyThirdInternationalConferenceonMachineLearning(ICML-06),pages1041–1048,Pittsburgh,PA,2006.
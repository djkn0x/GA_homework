Stock Trading with Recurrent Reinforcement 
Learning (RRL)

CS229 Application Project
Gabriel Molina, SUID 5055783

1

I.  INTRODUCTION

One relatively new approach to financial trading is to use machine learning algorithms to predict the rise and fall of
asset prices before they occur.  An optimal trader would buy an asset before the price rises, and sell the asset before 
its value declines.   

For this project, an asset trader will be implemented using recurrent reinforcement learning (RRL).  The algorithm 
and its parameters are from a paper written by Moody and Saffell1.  It is a gradient ascent algorithm which attempts
to maximize a utility function known as Sharpe’s ratio.  By choosing an optimal parameter w for the trader, we 
attempt to take advantage of asset price changes.  Test examples of the asset trader’s operation, both ‘real-world’
and contrived, are illustrated in the final section.

III.  UTILITY FUNCTION:  SHARPE’S RATIO

One commonly used metric in financial engineering is Sharpe’s ratio.  For a time series of investment returns, 
Sharpe’s ratio can be calculated as:

   

S 
T

Average
(
)
R
t
Standard
Deviation

(

R
t

)

     for interval     

1,t

T...,

tR  is the return on investment for trading period t .  Intuitively, Sharpe’s ratio rewards investment strategies 
where 
that rely on less volatile trends to make a profit.

IV. TRADER FUNCTION

The trader will attempt to maximize Sharpe’s ratio for a given price time series.  For this project, the trader function 
takes the form of a neuron:

F
t



tanh(


xw

)

     

t

where M is the number of time series inputs to the trader, the parameter
1

1


r
r
r
F
x
,...,
,1
,
p
p
vector
, and the return 
.  


t
t
Mt
t
t

t

t

w

2 M

, the input 

tr  is the difference in value of the asset between the current period  t  and the previous period.  Therefore,
Note that 
1t
tr  is the return on one share of the asset bought at time 
.  

]1,1[tF
 represents the trading position at time t .  There are three types of positions that can 
Also, the function 
be held:  long, short, or neutral.  

A long position is when
1t
period
.

0tF

.  In this case, the trader buys an asset at price 

tp and hopes that it appreciates by

0tF
.  In this case, the trader sells an asset which it does not own at price tp , with the 
A short position is when
1t
1t
expectation to produce the shares at period
.  If the price at 
is higher, then the trader is forced to buy at 
1t
1t
the higher
 price to fulfill the contract.  If the price at 
 is lower, then the trader has made a profit.

                                                
1 J Moody, M Saffell, Learning to Trade via Direct Reinforcement, IEEE Transactions on Neural Networks, Vol 12 , No 4, July 
2001.

0tF
A neutral position is when 
There will be neither gain nor loss.

.  In this case, the outcome at time 

1t

has no effect on the trader’s profits.  

2

   shares are bought (long position) or sold (short 
n
F
tF represents holdings at period  t .  That is, 
Thus,
t
t
position), whereis the maximum possible number of shares per transaction.  The return at time  t , considering the 
1tF , is:
decision

R
t





F
t


r
t



F
t



F
t


1

1


1 t
where  is the cost for a transaction at period  t .  If 
F
F
 (i.e. no change in our investment this period) then 
t
there will be no transaction penalty.  Otherwise the penalty is proportional to the difference in shares held.


1
F 
1t
r
) is the return resulting from the investment decision from the period 
.  For example, 
The first term (
t
t
1 tF
20
5.
), and each share increased 
shares, the decision was to buy half the maximum allowed (
if 
8tr
price units, this term would be 80, the total return profit (ignoring transaction penalties incurred during
period t ).

V.  GRADIENT ASCENT

Maximizing Sharpe’s ratio requires a gradient ascent.  First, we define our utility function using basic formulas 
from statistics for mean and variance:

We have

S

T



RE
[

t

RE
[
2


]

t
(

]

RE
[

t

2

])



A

AB

2

  where  

A



1
T

T


t
1

tR

     and    

B



1
T

T


t
1

2

tR

Then we can take the derivative of

TS  using the chain rule:

dS
T
dw



d
dw



T


t
1





dS
T
dA











2

A

AB
dS
dA
T
dR
dB
t







dS
T
dA




dB
dR
t





dA
dw



dS
T
dB



dB
dw

dR
T
dw



T


t
1





dS
T
dA

dA
dR
t



dS
T
dB

dB
dR
t











dR
t
dF
t

dF
t
dw



dR
t
dF
t


1

dF

t
1
dw





The necessary partial derivatives of the return function are:







dR
d
t
dF
dF
t
t



sgn(

F
t



F
t


1

)


F
t


r
t



F
t



F
t


1


1






d
dF
t






F
t



F
t


1














F
t
F
t




F
t
F
t


1


1




0
0



dR
d
t
dF
dF


1
t
1
t




r
t






F
t


r
t



F
t



F
t


1


1






r
t






F
t



F
t


1

d
dF
t


1














F
t
F
t




F
t
F
t


1


1




0
0

sgn(

F
t



F
t


1

)

Then, the partial derivatives 

dFt

dw

and 

dFt 1

dw

must be calculated:

dF
t
dw



d
dw


tanh(

T
xw

t


)



1(



tanh(

T
xw

2
))



t


T
xw

t





d
dw



1(



tanh(

T
xw

2
))



t

 
wx

t


M



2

dF

1
t
dw





3

dFt
dw
dFt
dw
is recurrent and depends on all previous values of 
Note that the derivative
.  This means that to 
dw
dFt
 from the beginning of our time series.  Because stock data 
train the parameters, we must keep a record of 
is in the range of 1000-2000 samples, this slows down the gradient ascent but does not present an insurmountable
dw
dFt
 using only the previous 
computational burden.  An alternative is to use online learning and to approximate 
dw
dFt 1
 term, effectively making the algorithm a stochastic gradient ascent as in Moody & Saffell’s paper.  
However, my chosen approach is to instead use the exact expressions as written above.

dS T
dw
 term has been calculated, the weights are updated according to the gradient ascent 
Once the 



w
w
dS
dw
eN  iterations, where
eN  is chosen to assure that 
rule
.  The process is repeated for 
i
i
T
1
Sharpe’s ratio has converged.

VI.  TRAINING

The most successful method in my exploration has been the following algorithm:

2 M
w
 using a historical window of size  T
1.  Train parameters 
1 Tt
2.  Use the optimal policy w to make ‘real time’ decisions from 
N
 predictions are complete, repeat step one.
3.  After

predict

 to 


NTt

predict

Intuitively, the stock price has underlying structure that is changing as a function of time.  Choosing T large 
assumes the stock price’s structure does not change much during T samples.  In the random process example 
N
below,  T and 
are large because the structure of the process is constant.  If long term trends do not appear to 
predict
dominate stock behavior, then it makes sense to reduce  T , since shorter windows can be a better solution than 
training on large amounts of past history.  For example, data for the years IBM 1980-2006 might not lead to a good 
strategy for use in Dec. 2006.  A more accurate policy would likely result from training with data from 2004-2006.

VII.  EXAMPLE

1

0.95

0.16
0.14
0.12
0.1
0.08
0.06

)
t
(
p
 
,
e
c
i
r
p

o
i
t
a
r
 
'
e
p
r
a
h
S

100

200

300

400

500
t

600

700

800

900

1000

10

20

30

40
training  iterat ion

50

60

70

Figure 1.  Training results for autoregressive random process.    

T

1000

, 

75eN

The first example of training a policy is executed on an autoregressive random process (randomness by injecting 
Gaussian noise into coupled equations).   In figure 1, the top graph is the generated price series.  The bottom graph 
is Sharpe’s ratio on the time series using the parameter w for each iteration of training.  So, as training progresses, 
we find better values of  w  until we have achieved an optimum Sharpe’s ratio for the given data.

Then, we use this optimal w  parameter to form a prediction for the next 

N

 data samples, shown below:

predict

4

Figure 2.  Prediction performance using optimal policy from training.  

N

predict



1000

As is apparent from the above graph, the trader is making decisions based on the w parameter.  Of course,  w  is 
suboptimal for the time series over this predicted interval, but it does better than a monkey.  After 1000 intervals 
our return would be 10%.

The next experiment, presented in the same format, is to predict real stock data with some precipitous drops
(Citigroup):

t
p
 
,
s
e
i
r
e
s
 
e
c
i
r
p

60

40

100

200

300
t

400

500

600

0.1

0.05

0

o
i
t
a
r
 
s
'
e
p
r
a
h
S

10

20

30

50
training  iteration
eN
T
Figure 3.  Training w on Citigroup stock data.   
600
, 

40

60

70

80

90

100

100

5

5

0

-5

t
r
 
,
s
n
r
u
t
e
r

-10

-15
600

1

0.5

0

-0.5

)
s
n
o
i
s
i
c
e
d
(
 
t
F

-1
600

30

20

10

)
%
(
 
s
n
i
a
g
 
t
n
e
c
r
e
p

0
600

650

700

650

700

650

700

750
t

750
t

750
t

800

850

900

800

850

900

800

850

900

tF  (middle), and percentage profit (cumulative) for Citigroup.  Note that although the general 
Figure 4.   tr  (top), 
policy is good, the precipitous drop in price (downward spike in tr ) wipes out our gains around t = 725.  

The recurrent reinforcement learner seems to work best on stocks that are constant on average, yet fluctuate up and 
down.  In such a case, there is less worry about a precipitous drop like in the above example.  With a relatively 
constant mean stock price, the reinforcement learner is free to play the ups and downs.

The recurrent reinforcement learner seems to work, although it is tricky to set up and verify.  One important trick is 
to properly scale the return series data to mean zero and variance one2, or the neuron cannot separate the resulting 
data points.

VII.  CONCLUSIONS

The primary difficulties with this approach rest in the fact that certain stock events do not exhibit structure.  As seen 
in the second example above, the reinforcement learner does not predict precipitous drops in the stock price and is 
just as vulnerable as a human.  Perhaps it would be more effective if combined with a mechanism to predict such 
precipitous drops.  Other changes to the model might be including stock volumes as features that could help in 
predicting rises and falls.

Additionally, it would be nice to augment the model to incorporate fixed transaction costs, as well as less frequent 
transactions.  For example, a model could be created that learns from long periods of data, but only periodically 
makes a decision.  This would reflect the case of a casual trader that participates in smaller volume trades with  
fixed transaction costs.  Because it is too expensive for small-time investors to trade every period with fixed
transaction costs, a model with a periodic trade strategy would more financially feasible for such users.  It would 
probably be worthwhile to try adapting this model to this sort of periodic trading and see the results.

                                                
2 Gold, Carl,  FX Trading via Recurrent Reinforcement Learning, Computational Intelligences for Financial Engineering, 
2003.  Proceedings.  2003 IEEE International Conference on.  p. 363-370.  March 2003.  Special thanks to Carl for email 
advice on algorithm implementation.


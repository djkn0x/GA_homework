Support Vector Machines on a Budget

Ofer Dekel and Yoram Singer
School of Computer Science and Engineering
The Hebrew University
Jerusalem 91904, Israel
{oferd,singer}@cs.huji.ac.il

Abstract

The standard Support Vector Machine formulation does not provide its user with
the ability to explicitly control the number of support vectors used to deﬁne the
generated classiﬁer. We present a modiﬁed version of SVM that allows the user
to set a budget parameter B and focuses on minimizing the loss attained by the B
worst-classiﬁed examples while ignoring the remaining examples. This idea can
be used to derive sparse versions of both L1-SVM and L2-SVM. Technically, we
obtain these new SVM variants by replacing the 1-norm in the standard SVM for-
mulation with various interpolation-norms. We also adapt the SMO optimization
algorithm to our setting and report on some preliminary experimental results.

1

Introduction

The L1 Support Vector Machine (L1-SVM or SVM for short) [1, 2, 3] is a powerful tech-
{(xi , yi )}m
nique for learning binary classi ﬁers from examples. Given a training set
i=1 and
(cid:1)(cid:2)
(cid:3)
the SVM solution is a hypothesis of the form h(x) =
a positive semi-deﬁnite kernel K ,
, where S is a subset of {1, . . . , m}, {αi }i∈S are real valued weights,
i∈S αi yiK (xi , x) + b
sign
and b is a bias term. The set S deﬁnes the support of the classiﬁer, namely, the set of examples that
actively participate in the classiﬁer’s de ﬁnition. The examples in this set are called
support vectors,
and we say that the SVM solution is sparse if the fraction of support vectors (|S |/m) is reasonably
small.

Our ﬁrst concern is usually with the accuracy of the classi ﬁer. However, in some applications, the
size of the support is equally important. Assuming that the kernel operator K can be evaluated in
constant time, the time-complexity of evaluating the classiﬁer on a new instance is linear in the size
of S . Therefore, a large support deﬁnes a slow classi ﬁer. Classiﬁcation speed is often important
and plays an especially critical role in real-time systems. For example, a classiﬁer that drives a
phoneme detector in a speech recognition system is evaluated hundreds of times a second. If this
classiﬁer does not manage to keep up with the rate at which the speech signal is acquired then
its classiﬁcations are useless, regardless of their accuracy. The size of the support also naturally
determines the amount of memory required to store the classi ﬁer. If a classiﬁer is intended to run
in a device with a limited memory, such as a mobile telephone, there may be a physical limit on
the amount of memory available to store support vectors. The size of S may also effect the time
required to train an SVM classiﬁer. Most modern SVM learning algorithms are active set methods,
namely, on every step of the training process, only a small set of active training examples are taken
into account. Knowing the size of S ahead of time would enable us to optimize the size of the active
set and possibly gain a signiﬁcant speed-up in the training process.

The SVM mechanism does not give us explicit control over the size of the support. The user-deﬁned
parameters of SVM have some inﬂuence on the size of S , but we often require more than this.
Speciﬁcally, we would like the ability to specify a budget parameter, B , which directly controls the
number of support vectors used to deﬁne the SVM solution. In this paper, we address this issue

and present budget-SVM, a minor modiﬁcation to the standard L1-SVM formulation that allows
the user to set a budget parameter. The budget-SVM optimization problem focuses only on the B
worst-classiﬁed examples in the training set, ignoring all other examples.
The problem of sparsity becomes even more critical when it comes to L2-SVM [3], a variant of the
SVM problem that tends to have dense solutions. L2-SVM is sometimes preferred over L1-SVM
because it exhibits good generalization properties, as well as other desirable statistical characteris-
tics [4]. We derive the budget-L2-SVM formulation by following the same technique used to derive
budget-L1-SVM.
The technique used to derive these SVM variants is as follows. We begin by generalizing the L1-
SVM formulation by replacing the 1-norm with an arbitrary norm. We obtain a general framework
for SVM-type problems, which we nickname Any-Norm-SVM. Next, we turn to the K -method of
norm interpolation to obtain the 1 − ∞ interpolation-norm and the 2 − ∞ interpolation-norm, and
use these norms in the Any-Norm-SVM framework. These norms have the property that they depend
only on the absolutely-largest elements of the vector. We rely on this property and show that our
SVM variants construct sparse solutions. For each of these norms, we present a simple modiﬁcation
of the SMO algorithm [5], which efﬁciently solves the respective optimization problem.

Related Work The problem of approximating the SVM solution using a reduced set of examples
has received much previous attention [6, 7, 8, 9]. This technique takes a two-step approach: begin
by training a standard SVM classi ﬁer, perhaps obtaining a dense solution. Then, try to ﬁnd a sparse
classiﬁer which minimizes the L2 distance to the SVM solution. A potential drawback of this ap-
proach is that once the SVM solution has been found, the distribution from which the training set was
sampled no longer plays a role in the learning process. This ignores the fact that shifting the SVM
classiﬁer by a ﬁxed amount in different directions may have dramatically different consequences on
classiﬁcation accuracy. We overcome this problem by taking the approach of [10] and reformulating
the SVM optimization problem itself in a way that promotes sparsity. Another technique used to
obtain a sparse kernel-machine takes advantage of the inherent sparsity of linear programming solu-
tions, and formalizes the kernel-machine learning problem as a linear program [11]. This approach,
often called LP-SVM or Sparse-SVM, has been shown to generally construct sparse solutions, but
still lacks the ability to introduce an explicit budget parameter. Yet another approach involves ran-
domly selecting a subset of the training set to serve as support vectors [12]. The problem of learning
a kernel-machine on a budget also appears in the online-learning mistake-bound framework, and it
is there where the term “learning on a budget ” was coined [13]. Two recent papers [14, 15] propose
online kernel-methods on a budget with an accompanying theoretical mistake-bound.

This paper is organized as follows. We present the generalized Any-Norm-SVM framework in
Sec. 2. We discuss the K -method of norm interpolation in Sec. 3 and put various interpolation
norms to use within the Any-Norm-SVM framework in Sec. 4. Then, in Sec. 5, we present some
preliminary experiments that demonstrate how the theoretical properties of our approach translate
into practice. We conclude with a discussion in Sec. 6. Due to the lack of space, some of the proofs
are omitted from this paper.

2 Any-Norm SVM
Let {(xi , yi )}m
i=1 be a training set, where every xi belongs to an instance space X and every yi ∈
{−1, +1}. Let K : X × X → R be a positive semi-deﬁnite kernel, and let H be its corresponding
Reproducing Kernel Hilbert Space (RKHS) [16], with inner product (cid:4)·, ·(cid:5)H . The L1 Support Vector
Machine is deﬁned as the solution to the following convex optimization problem:
(cid:1)
(cid:3) ≥ 1 − ξi ,
∀ 1 ≤ i ≤ m yi
2 (cid:4)f , f (cid:5)H + C (cid:6)ξ(cid:6)1
1

f (xi ) + b

(1)

min
f ∈H, b∈R, ξ≥0

s.t.

where ξ is a vector of m slack variables, and C is a positive constant that controls the tradeoff
between the complexity of the learned classiﬁer and how well it ﬁts the training data. The value
(cid:2)m
of ξi is sometimes referred to as the hinge-loss attained by the SVM classiﬁer on example i. The
1-norm, deﬁned by (cid:6)ξ(cid:6)1 =
i=1 |ξi |, is used to combine the individual hinge-loss values into a
single number.

L2-SVM is a variant of the optimization problem deﬁned above, deﬁned as follows:
(cid:1)
(cid:3) ≥ 1 − ξi .
2 (cid:4)f , f (cid:5)H + C (cid:6)ξ(cid:6)2
∀ 1 ≤ i ≤ m yi
f (xi ) + b
min
1
s.t.
f ∈H, b∈R, ξ≥0
2
(cid:2)m
This formulation differs from the L1 formulation in that the 1-norm is replaced by the squared 2-
norm, deﬁned by (cid:6)ξ(cid:6)2
2 =
i=1 ξ 2
i . In this section, we take this idea even farther, and allow the
1-norm of L1-SVM to be replaced by any norm. Formally, let (cid:6) · (cid:6) be an arbitrary norm de ﬁned on
m . Recall that a norm is a real valued operator such that for every v ∈ R
m and λ ∈ R it holds
R
that (cid:6)λv(cid:6) = |λ|(cid:6)v(cid:6) (positive homogeneity), (cid:6)v(cid:6) ≥ 0 and (cid:6)v(cid:6) = 0 if and only if v = 0 (positive
deﬁniteness), and that satisﬁes the triangle inequality. Now consider the following optimization
(cid:1)
(cid:3) ≥ 1 − ξi .
problem:
∀ 1 ≤ i ≤ m yi
2 (cid:4)f , f (cid:5)H + C (cid:6)ξ(cid:6)
min
f (xi ) + b
1
(2)
s.t.
f ∈H, b∈R, ξ≥0
L1-SVM is recovered by setting (cid:6) · (cid:6) to be the 1-norm. Setting (cid:6) · (cid:6) to be the 2-norm induces an
optimization problem which is close in nature to L2-SVM, but not identical to it since the 2-norm is
not squared. Combining the positive homogeneity property of (cid:6) · (cid:6) with the fact that it satisﬁes the
triangle inequality ensures that the objective function of Eq. (2) is convex.
(cid:2)m
An important class of norms used extensively in our derivation is the family of p-norms, deﬁned for
every p ≥ 1 by (cid:6)v(cid:6)p = (
j=1 |vj |p )1/p . A special member of this family is the ∞-norm, which is
deﬁned by (cid:6)v(cid:6)∞ = limp→∞ (cid:6)v(cid:6)p and can be shown to be equivalent to maxj |vj |. We also use the
m has a dual norm which is also deﬁned on R
m . The dual
notion of norm duality. Every norm on R
norm of (cid:6) · (cid:6) is denoted by (cid:6) · (cid:6)(cid:1) and given by
u · v
u · v .
(cid:6)u(cid:6)(cid:1) = max
max
(cid:6)v(cid:6) =
(3)
v∈Rm
v∈Rm : (cid:5)v(cid:5)=1
As its name implies, (cid:6) · (cid:6)(cid:1) also satisﬁes the requirements of a norm. For example, H ¨older’s inequal-
ity [17] states that the dual of (cid:6) · (cid:6)p is the norm (cid:6) · (cid:6)q , where q = p/(p − 1). The dual of the 1-norm
is the ∞-norm and vice versa.
Using the deﬁnition of the dual norm, we now state the dual optimization problem of Eq. (2):
m(cid:4)
m(cid:4)
m(cid:4)
m(cid:4)
αi − 1
yiαi = 0 and (cid:6)α(cid:6)(cid:1) ≤ C .
max
αiαj yi yj K (xi , xj )
s.t.
(4)
α≥0
2
i=1
j=1
i=1
i=1
As a ﬁrst sanity check, note that if (cid:6) · (cid:6) in Eq. (2) is chosen to be the 1-norm, then (cid:6) · (cid:6)(cid:1) is the ∞-
norm, and the constraint (cid:6)α(cid:6)(cid:1) ≤ C reduces to the familiar box-constraint of L1-SVM [3]. The proof
that Eq. (2) and Eq. (4) are indeed dual optimization problems relies on basic techniques in convex
(cid:2)m
analysis [18], and is omitted due to the lack of space. Moreover, it can be shown that the solution
to Eq. (2) takes the form f (·) =
i=1 αi yiK (xi , ·), and that strong duality holds regardless of the
norm used. This allows us to forget about the primal problem in Eq. (2) and to focus on solving the
dual problem in Eq. (4). As with L1-SVM, the bias term, b, cannot be directly extracted from the
solution of the dual. The standard techniques used to ﬁnd b in L1-SVM apply here as well [3].
We note that the Any-Norm-SVM formulation is not fundamentally different from the original L1-
SVM formulation. Both optimization problems have convex objective functions and linear con-
straints. More importantly, the only difference between their respective duals is in the dual-norm
constraint. Speciﬁcally, the objective function in Eq. (4) is a concave quadratic function for any
choice of (cid:6) · (cid:6). These facts enable us to efﬁciently solve the problem in Eq. (4) for any kernel K and
any norm using techniques similar to those used to solve the standard L1-SVM problem.

3

Interpolation Norms

In the previous section, we acquired the ability to replace the 1-norm in the deﬁnition of L1-SVM
with an arbitrary norm. We now use Peetre’s K -method of norm interpolation [19] to obtain norms
that promote the sparsity of the generated classiﬁer. The K -method is a technique for smoothly
interpolating between a pair of norms. Let (cid:6) · (cid:6)p1 : R
m → R+ and (cid:6) · (cid:6)p2 : R
m → R+ be two
p-norms, and let (cid:6) · (cid:6)q1 and (cid:6) · (cid:6)q2 be their respective duals. Peetre’s K -functional with respect to
p1 and p2 , and with respect to the constant t > 0, is deﬁned to be
(cid:6)
(cid:5)
(cid:6)w(cid:6)p1 + t(cid:6)z(cid:6)p2
(cid:6)v(cid:6)K (p1 ,p2 ,t) =
min
w,z : w+z=v

(5)

.

.

(6)

Peetre’s J -functional with respect to q1 , q2 , and with respect to the constant s > 0, is given by
(cid:8)
(cid:7)
(cid:6)u(cid:6)q1 , s (cid:6)u(cid:6)q2
(cid:6)u(cid:6)J (q1 ,q2 ,s) = max
The J -functional is obviously a norm: the properties of a norm all follow immediately from the
fact that (cid:6) · (cid:6)q1 and (cid:6) · (cid:6)q2 posses these properties. (cid:6) · (cid:6)K (p1 ,p2 ,t) is also a norm, and moreover,
(cid:6) · (cid:6)K (p1 ,p2 ,t) and (cid:6) · (cid:6)J (q1 ,q2 ,s) are dual to each other when t = 1/s. This fact can be proven using
elementary calculus, and this proof is omitted due to the lack of space.
We use the K -method to interpolate between the 1-norm and the ∞-norm, and to interpolate be-
tween the 2-norm and the ∞-norm. To gain some intuition on the behavior of these interpolation-
m it holds that maxi |vi |p ≤ (cid:2)m
norms, ﬁrst note that for any p ≥ 1 and any v ∈ R
i=1 |vi |p ≤
m maxi |vi |p , and therefore (cid:6)v(cid:6)∞ ≤ (cid:6)v(cid:6)p ≤ m1/p(cid:6)v(cid:6)∞ . An immediate consequence of this is
that (cid:6) · (cid:6)K (p,∞,t) ≡ (cid:6) · (cid:6)∞ when 0 < t ≤ 1 and that (cid:6) · (cid:6)K (p,∞,t) ≡ (cid:6) · (cid:6)p when m1/p ≤ t. In
other words, the interesting range of t for the 1 − ∞ interpolation-norm is [1, m], and for the 2 − ∞
√
interpolation-norm is [1,
m].
Next, we prove a theorem which states that interpolating a p-norm with the ∞-norm is approximately
equivalent to restricting that p-norm to the absolutely-largest components of the vector. Speci ﬁcally,
the 1 − ∞ interpolation norm with parameter t (with t chosen to be an integer in [1, m]) is precisely
equivalent to taking the sum of the absolute values of the t absolutely-greatest elements of the vector.
m and let π be a permutation on {1, . . . , m} such
Theorem 1. Let v be an arbitrary vector in R
that |vπ(1) | ≥ . . . ≥ |vπ(m) |. Then for any integer B in {1, . . . , m} it holds that (cid:6)v(cid:6)K (1,∞,B ) =
(cid:2)B
i=1 |vπ(i) |, and for any 1 ≤ p < ∞, if t = B 1/p then it holds that
(cid:1) (cid:2)B
(cid:3)1/p + B 1/p |vπ(B ) | .
(cid:3)1/p ≤ (cid:6)v(cid:6)K (p,∞,t) ≤ (cid:1) (cid:2)B
i=1 |vπ(i) |p
i=1 |vπ(i) |p

Proof. Beginning with the lower bound, let w and z be such that w + z = v. Then
(cid:3)1/p =
(cid:3)1/p
(cid:1) (cid:2)B
(cid:1) (cid:2)B
i=1 |wπ(i) + zπ(i) |p
i=1 |vπ(i) |p
(cid:3)1/p
(cid:3)1/p +
≤ (cid:1) (cid:2)B
(cid:1) (cid:2)B
i=1 |wπ(i) |p
i=1 |zπ(i) |p
(cid:3)1/p + (B maxi |zi |p )1/p
≤ (cid:1) (cid:2)B
i=1 |wπ(i) |p
(cid:3)1/p + t(cid:6)z(cid:6)∞ ,
≤ (cid:1) (cid:2)m
i=1 |wi |p
(cid:2)m
where the ﬁrst inequality is the triangle inequality for the p-norm. Since the above holds for any w
i=1 |wi |p )1/p + t(cid:6)z(cid:6)∞ ,
and z such that w + z = v, it also holds for the pair which minimizes (
and which deﬁnes (cid:6)v(cid:6)K (p,∞,t) . Therefore, we have that,
(cid:1) (cid:2)B
(cid:3)1/p ≤ (cid:6)v(cid:6)K (p,∞,t) .
i=1 |vπ(i) |p
(7)
let φ = |vπ(B ) |, and deﬁne for all
1 ≤ i ≤ m, ¯wi =
Turning to the upper bound,
sign(vi ) max{0, |vi | − φ} and ¯zi = sign(vi ) min{|vi |, φ}. Note that ¯w + ¯z = v, and that
B(cid:4)
|vπ(i) | = (cid:6) ¯w(cid:6)1 + B (cid:6)¯z(cid:6)∞ .
This proves that (cid:6)v(cid:6)K (1,∞,B ) ≤ (cid:2)B
i=1
i=1 |vπ(i) | and together with Eq. (7) we have proven our claim
for p = 1. Moving on to the case of an arbitrary p, we have that
(cid:6)v(cid:6)K (p,∞,t) = min
((cid:6)w(cid:6)p + t(cid:6)z(cid:6)∞ ) ≤ (cid:6) ¯w(cid:6)p + t(cid:6)¯z(cid:6)∞ .
w+z=v
Since the absolute value of each element in ¯w is at most as large as the absolute value of the
corresponding element of v, and since ¯wπ(r+1) = . . . = ¯wπ(m) = 0, we have that (cid:6) ¯w(cid:6)p ≤
(cid:2)B
(cid:6)¯z(cid:6)∞ = φ = |vπ(B ) |. This proves that (cid:6)v(cid:6)K (p,∞,t) ≤
i=1 |vπ(i) |p )1/p . By deﬁnition,
(
(cid:2)B
i=1 |vπ(i) |p )1/p + t|vπ(B ) | and together with Eq. (7) this concludes our proof for arbitrary p.
(

4 Deriving Concrete Algorithms from the General Framework
Our ﬁrst concrete algorithm is budget- L1-SVM, obtained by plugging the 1 − ∞ interpolation-norm
with parameter B into the general Any-Norm-SVM framework. Relying on Thm. 1, we know that
this norm takes into account only the B largest values in ξ . Since ξ measures how badly each
example is misclassiﬁed, the budget- L1-SVM problem essentially optimizes the soft-margin with
respect to the B worst-classiﬁed examples. We now show that this property promotes the sparsity
of the budget-L1-SVM solution.
If there are less than B examples for which yi (f (xi )+ b) < 1, then the KKT conditions of optimality
immediately imply that the number of support vectors is less than B . This holds true for every
instance of the Any-Norm-SVM framework, and is proven for L1-SVM in [3]. Therefore, we focus
on the more interesting case, where yi (f (xi ) + b) < 1 for at least B examples.
Theorem 2. Let B be an integer in {1, . . . , m}. Let (f , b, ξ , α) be an optimal primal-dual solution
of the primal problem in Eq. (2) and the dual problem in Eq. (4), where (cid:6) · (cid:6) is chosen to be the 1 −∞
interpolation-norm with parameter B . Deﬁne µi = yi (f (xi ) + b) and let π be a permutation of
{1, . . . , m} such that µπ(1) ≤ . . . ≤ µπ(m) . Assume that µπ(B ) < 1. Then, αk = 0 if µπ(B ) < µk .

Proof. We begin the proof by redeﬁning ξi = max{1 − µi , 0} for all 1 ≤ i ≤ m and noting
that (f , b, ξ , α) remains a primal-dual solution to our problem. The beneﬁt of starting with this
speciﬁc solution is that ξπ(1) ≥ . . . ≥ ξπ(m) . Let k be an index such that µπ(B ) < µk and deﬁne
2 (ξk + ξπ(B ) ). Moreover, let ξ (cid:6)
ξ (cid:6)
k = 1
be the vector obtained by replacing the k ’th coordinate in
k , or in other words, ξ (cid:6) = (ξ1 , . . . , ξ (cid:6)
ξ with ξ (cid:6)
k , . . . , ξm ). Using the assumption that µπ(B ) < 1,
we know that ξπ(B ) > 0, and since µk > µπ(B ) we get that ξk < ξπ(B ) . We can now draw two
conclusions. First, ξπ(1) ≥ . . . ≥ ξπ(B ) > ξ (cid:6)
k and therefore (cid:6)ξ (cid:6)(cid:6)K (1,∞,B ) = (cid:6)ξ(cid:6)K (1,∞,B ) . Second,
k and therefore ξ (cid:6)
(f , b, ξ (cid:6) , α) is
ξk < ξ (cid:6)
satisﬁes the constraints of Eq. (2). Overall, we obtain that
also a primal-dual solution to our problem. Moreover, we know that 1 − µk < ξ (cid:6)
k . Using the KKT
complementary slackness condition, it follows that αk , the Lagrange multiplier corresponding to
this constraint, must equal 0.

Deﬁning µi and π as above, a simple corollary of Thm. 2 is that the number of support vectors is
upper bounded by B in the case that µπ(B ) (cid:12)= µπ(B+1) .
From our discussion in Sec. 3, we know that the dual of the 1 − ∞ interpolation-norm is the func-
tion max{(cid:6)u(cid:6)∞ , (1/B )(cid:6)u(cid:6)1 }. Plugging this deﬁnition into Eq. (4) gives us the dual optimiza-
tion problem of budget-L1-SVM. The constraint (cid:6)α(cid:6)(cid:1) ≤ C simpliﬁes to αi ≤ C for all i and
(cid:2)m
i=1 αi ≤ BC . To numerically solve this optimization problem, we turn to the Sequential Mini-
mal Optimization (SMO) [5] technique. We brieﬂy describe the SMO technique, and then discuss
its adaptation to our setting. SMO is an iterative process, which on every iteration selects a pair of
dual variables, αk and αl , and optimizes the dual problem with respect to them, leaving all other
variables ﬁxed. The choice of the two variables is determined by a heuristic [5], and their optimal
values are calculated analytically. Assume that we start with a vector α which is a feasible point
(cid:2)
of the optimization problem in Eq. (4). When restricted to the two active variables, αk and αl , the
i(cid:7)=k,l αi yi = 0 simpliﬁes to αnew
k yk + αnew
l yl = αold
k yk + αold
l yl . Put another way, we
constraint
can slightly overload our notation and deﬁne the linear functions
and αl (λ) = αl − λyl ,
αk (λ) = αk + λyk
and ﬁnd the single variable λ which maximizes our constrained optimization problem. Since the
constraints in Eq. (4) deﬁne a convex and bounded feasible set, the intersection of the linear equali-
ties in Eq. (8) with this feasible set restricts λ to an interval. The objective function, as a function of
the single variable λ, takes the form O(λ) = P λ2 + Qλ + c, where c is a constant,
(cid:1)
(cid:3) − (cid:1)
(cid:3)
P = K (xk , xl ) − 1
2 K (xk , xk ) − 1
yk − f (xk )
yl − f (xl )
2 K (xl , xl ) , Q =
,
and f is the current function in the RKHS (f ≡ (cid:2)m
i=1 αi yiK (xi , ·)). Maximizing the objective
function in Eq. (4) with respect to αk and αl is equivalent to maximizing O(λ) with respect to
λ over an interval. P equals minus the Euclidean distance between the functions K (xk , ·) and

(8)

K (xl , ·) in the RKHS, and is therefore a negative number. Therefore, O(λ) is a concave function
which attains a single (unconstrained) maximum. This maximum can be found analytically by
−Q
0 = ∂O(λ)
= 2P λ + Q ⇒ λ =
2P
∂λ

(9)

.

If this unconstrained optimum falls inside the feasible interval, then it is equivalent to the constrained
optimum. Otherwise, the constrained optimum falls on one of the two end-points of the interval.
Thus, we are left with the task of ﬁnding these end-points. To do so, we consider the remaining
constraints:
(cid:4)
αk (λ) ≥ 0
αk (λ) ≤ C
αl (λ) ≤ C
αl (λ) ≥ 0

(III) αk (λ) + αl (λ) ≤ BC −

αi .

(II)

(I)

i(cid:7)=k,l

The constraints in (I) translate to
yk = −1 ⇒ λ ≤ αk
yl = −1 ⇒ λ ≥ −αl

The constraints in (II) translate to
yk = −1 ⇒ λ ≥ αk − C
yl = −1 ⇒ λ ≤ C − αl

yk = +1 ⇒ λ ≥ −αk
yl = +1 ⇒ λ ≤ αl .

yk = +1 ⇒ λ ≤ C − αk
yl = +1 ⇒ λ ≥ αl − C .

(10)

(11)

.

.

(12)

Constraint (III) translates to
(cid:1) (cid:2)m
(cid:3)
i=1 αi − BC
yk = −1 ∧ yl = +1 ⇒ λ ≥ 1
(cid:3)
(cid:1)
BC − (cid:2)m
2
yk = +1 ∧ yl = −1 ⇒ λ ≤ 1
i=1 αi
2
Finding the end-points of the interval that con ﬁnes λ amounts to ﬁnding the smallest upper bound
and the greatest lower bound in Eqs. (10,11,12). This concludes the analytic derivation of the SMO
update for budget-L1-SVM.
√
L2-SVM on a budget Next, we use the 2 − ∞ interpolation-norm with parameter t =
B in
√
the Any-Norm-SVM framework, and obtain the budget-L2-SVM problem. Thm. 1 hints that setting
B makes the 2 − ∞ interpolation-norm almost equivalent to restricting the 2-norm to the top
t =
B elements in the vector ξ . The support size of the budget-L2-SVM solution is strongly correlated
with the parameter B although the exact relation between the two is not as clear as before. Again
we begin with the dual formulation deﬁned in Eq. (4), where the constraint (cid:6)α(cid:6)(cid:1) ≤ C becomes
√
B )(cid:6)ξ(cid:6)1 } ≤ C . The intersection of this constraint with the other constraints deﬁnes
max{(cid:6)ξ(cid:6)2 , (1/
a convex and bounded feasible set, and its intersection with the linear equalities in Eq. (8) de ﬁnes
an interval. The objective function in Eq. (4) is the same as before, so the unconstrained maximum
is once again given be Eq. (9). To obtain the constrained maximum, we must ﬁnd the end-points of
the interval that conﬁnes λ. The dual-norm constraint can be written more explicitly as
(cid:4)
(cid:4)
√
l (λ) ≤ C 2 −
(I) αk (λ) + αl (λ) ≤
BC −
k (λ) + α2
(II) α2

i(cid:7)=k,l
i(cid:7)=k,l
√
Constrain (I) is similar to the constraint we had in the budget-L1-SVM case, and is given in terms
(cid:2)m
of λ by replacing B with
B in Eq. (12). Constraint (II) is new, and can be written in terms of λ as
λ2 + λβ + γ ≤ 0, where β = αk yk − αl yl and γ = 1
i − C 2 ). It can be written even more
2 (
i=1 α2
explicitly as
(cid:9)
(cid:9)

λ ≤ 1
β 2 − 4γ ) .
λ ≥ 1
2 (−β −
β 2 − 4γ )
2 (−β +
(13)
and
In addition, we still have the constraint α ≥ 0, which is common to every instance of the Any-
Norm-SVM framework. This constraint is given in terms of λ in Eq. (10). Overall, the end-points
√
of the interval we are searching for are found by taking the smallest upper bound and the greatest
lower bound in Eqs. (10,13) and Eq. (12) with B replaced by
B .

α2
i

αi

700

600

500

400

300

200

100

s

500

450

400

350

300

s

250

200

150

100

50

200

400

B

600

800

1000

200

400

600

800

1000

B

Figure 1: Average test error of budget-L1-SVM (left) and budget-L2-SVM (right) for different
values of the budget parameter B and the pruning parameter s (all but s weights in α are set to
zero). The test error in the darkest region is roughly 50%, and in the lightest region is roughly 5%.

5 Experiments

Many existing solvers for the standard L1-SVM problem deﬁne a positive threshold value close to
zero and replace every weight that falls below this threshold with zero. This heuristic signiﬁcantly
reduces the time required for the algorithm to converge. In our setting, a more natural way to speed
up the learning process is to run the iterative SMO optimization algorithm for a ﬁxed number of
iterations and then to keep only the B largest weights, setting the m − B remaining weights to
zero. This pruning heuristic enforces the budget constraint in a brute-force way, and can be equally
applied to any kernel-machine. However, the natural question is how much will the pruning heuristic
affect the classiﬁcation accuracy of the kernel-machine it is applied to. If our technique indeed lives
up to its theoretical promise, we expect the pruning heuristic to have little impact on classiﬁcation
accuracy. On the other hand, if we train an L1-SVM and it so happens that the number of large
weights exceeds B , then applying the pruning heuristic should have a dramatic negative effect on
classiﬁcation accuracy. The goal of our experiments is to demonstrate that this behavior indeed
occurs in practice.

We conducted our experiments using the MNIST dataset, which contains handwritten digits from
the 10 digit classes. We randomly generated 50 binary classiﬁcation problems by ﬁrst randomly
partitioning the 10 classes into two equally sized sets, and then randomly choosing a training set of
1000 examples and a test set of 4000 examples. The results reported below are averaged over these
50 problems. Although MNIST is generally thought to induce easy learning problems, the method
described above generates moderately difﬁcult learning tasks.
For each binary problem, we trained both the L1 and the L2 budget SVMs with B =
20, 40, . . . , 1000. Note that (cid:6)ξ(cid:6)K (1,∞,B ) grows roughly linearly with B , and that (cid:6)ξ(cid:6)K (2,∞,
√
B )
√
grows roughly like the square root of B . To compensate for this, we set C = 10/B in the L1 case
B in the L2 case. This heuristic choice of C attempts to preserve the relative weight
and C = 10/
of the regularization term with respect to the norm term in Eq. (2), across the various values of B .
In all of our experiments, we used a Gaussian kernel with σ = 1 (after scaling the data to have an
average unit norm). For each classiﬁer trained, we pruned away all but the s largest weights, with
s = 20, 40, . . . , 1000, and calculated the test error. The average test error for every choice of B
(the budget parameter in the optimization problem) and s (the number of non-zero weights kept) is
summarized in Fig. 1. In practice, s and B should be equal, however we let s take different values
in our experiment to illustrate the characteristics of our approach. Note that the test-error attained
by L1-SVM (without a budget parameter) and L2-SVM are represented by the top-right corners of
the respective plots.
As expected, classiﬁcation accuracy for any value of B deteriorates as s becomes small. However,
the accuracy attained by L1-SVM and L2-SVM can be equally attained using signiﬁcantly less
support vectors.

6 Discussion

√

r

Using the Any-Norm-SVM framework with interesting norms enabled us to introduce a budget
parameter to the SVM formulation. However, the Any-Norm framework can be used for other
tasks as well. For example, we can interpolate between L1-SVM and L2-SVM by using the 1 − 2
interpolation-norm. This gives the user the explicit ability to balance the trade-off between the pros
and cons of these two SVM variants. In [20] it is shown that there exists a constant c such that,
(cid:5) (cid:2)m
(cid:6)1/2 ≤ (cid:6)v(cid:6)K (1,2,
r) ≤ (cid:2)r
√
c(cid:6)v(cid:6)K (1,2,
j=1 |vj | +
j=r+1 v2
j
These bounds give some insight into how such an interpolation would behave. Another possible
norm that can be used in our framework is the Mahalanobis norm ((cid:6)v(cid:6) = (v(cid:9)M v)1/2 , where M
is a positive deﬁnite matrix), which would deﬁne a loss function that takes into account pair-wise
relationships between examples.
Regarding our experiments, the rule-of-thumb we used to choose the parameter C is not always
optimal. It seems preferable to tune C individually for each B using cross-validation.
We are currently exploring extensions to our SMO variant that would quickly converge to the sparse
solution without the help of the pruning heuristic. We are also considering multiplicative update
optimization algorithms as an alternative to SMO.

r) .

√

References
[1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classi ﬁers. In Proc.
of the Fifth Annual ACM Workshop on Computational Learning Theory, pages 144–152, 1992.
[2] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.
[3] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University
Press, 2000.
[4] P. Bartlett and A. Tewari. Sparseness vs estimating conditional probabilities: Some asymptotic results. In
Proc. of the Seventeenth Annual Conference on Computational Learning Theory, pages 564–578, 2004.
[5] J. C. Platt. Fast training of Support Vector Machines using sequential minimal optimization.
In
B. Sch ¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learn-
ing. MIT Press, 1998.
[6] C.J.C. Burges. Simpliﬁed support vector decision rules. In Proc. of the Thirteenth International Confer-
ence on Machine Learning, pages 71–77, 1996.
[7] E. Osuna and F. Girosi. Reducing the run-time complexity of support vector machines. In B. Sch ¨olkopf,
C. Burges, and A. Smola, editors, Advances in Kernel Methods: Support Vector Learning, pages 271–284.
MIT Press, 1999.
[8] B. Sch ¨olkopf, S. Mika, C.J.C. Burges, P. Knirsch, K-R M ¨uller, G. R ¨atsch, and A.J. Smola. Input space
versus feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000–1017,
September 1999.
[9] J-H. Chen and C-S. Chen. Reducing SVM classiﬁcation time using multiple mirror classiﬁers.
IEEE
transactions on systems, man and cybernetics – part B: Cybernetics , 34(2):1173–1183, April 2004.
[10] M. Wu, B. Sch ¨olkopf, and G. Bakir. A direct method for building sparse kernel learning algorithms.
Journal of Machine Learning Research, 7:603–624, 2006.
[11] K.P. Bennett. Combining support vector and mathematical programming methods for classiﬁcation. In
Advances in kernel methods: support vector learning, pages 307–326. MIT Press, 1999.
[12] Y. Lee and O.L. Mangasarian. RSVM: Reduced support vector machines. In Proc. of the First SIAM
International Conference on Data Mining, 2001.
[13] K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. In Advances in Neural Infor-
mation Processing Systems 16, 2003.
[14] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: A kernel-based perceptron on a ﬁxed budget.
In Advances in Neural Information Processing Systems 18, 2005.
[15] N. Cesa-Bianchi and C. Gentile. Tracking the best hyperplane with a simple budget perceptron. In Proc.
of the Nineteenth Annual Conference on Computational Learning Theory, 2006.
[16] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society,
68(3):337–404, May 1950.
[17] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.
[18] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[19] C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press, 1998.
[20] T. Holmstedt. Interpolation of quasi-normed spaces. Mathematica Scandinavica, 26:177–190, 1970.


Modeling General and Speci ﬁc Aspects of Documents
with a Probabilistic Topic Model

Chaitanya Chemudugunta, Padhraic Smyth
Department of Computer Science
University of California, Irvine
Irvine, CA 92697-3435, USA
{chandra,smyth}@ics.uci.edu

Mark Steyvers
Department of Cognitive Sciences
University of California, Irvine
Irvine, CA 92697-5100, USA
msteyver@uci.edu

Abstract

Techniques such as probabilistic topic models and latent-semantic indexing have
been shown to be broadly useful at automatically extracting the topical or seman-
tic content of documents, or more generally for dimension-reduction of sparse
count data. These types of models and algorithms can be viewed as generating an
abstraction from the words in a document to a lower-dimensional latent variable
representation that captures what the document is generally about beyond the spe-
ciﬁc words it contains. In this paper we propose a new probabi listic model that
tempers this approach by representing each document as a combination of (a) a
background distribution over common words, (b) a mixture distribution over gen-
eral topics, and (c) a distribution over words that are treated as being speciﬁc to
that document. We illustrate how this model can be used for information retrieval
by matching documents both at a general topic level and at a speciﬁc word level,
providing an advantage over techniques that only match documents at a general
level (such as topic models or latent-sematic indexing) or that only match docu-
ments at the speciﬁc word level (such as TF-IDF).

1 Introduction and Motivation

Reducing high-dimensional data vectors to robust and interpretable lower-dimensional representa-
tions has a long and successful history in data analysis, including recent innovations such as latent
semantic indexing (LSI) (Deerwester et al, 1994) and latent Dirichlet allocation (LDA) (Blei, Ng,
and Jordan, 2003). These types of techniques have found broad application in modeling of sparse
high-dimensional count data such as the “bag of words” repre
sentations for documents or transaction
data for Web and retail applications.

Approaches such as LSI and LDA have both been shown to be useful for “object matching ” in their
respective latent spaces. In information retrieval for example, both a query and a set of documents
can be represented in the LSI or topic latent spaces, and the documents can be ranked in terms of
how well they match the query based on distance or similarity in the latent space. The mapping to
latent space represents a generalization or abstraction away from the sparse set of observed words, to
a “higher-level” semantic representation in the latent spa
ce. These abstractions in principle lead to
better generalization on new data compared to inferences carried out directly in the original sparse
high-dimensional space. The capability of these models to provide improved generalization has
been demonstrated empirically in a number of studies (e.g., Deerwester et al 1994; Hofmann 1999;
Canny 2004; Buntine et al, 2005).

However, while this type of generalization is broadly useful in terms of inference and prediction,
there are situations where one can over-generalize. Consider trying to match the following query
to a historical archive of news articles: election + campaign + Camejo. The query is intended to
ﬁnd documents that are about US presidential campaigns and a lso about Peter Camejo (who ran as

vice-presidential candidate alongside independent Ralph Nader in 2004). LSI and topic models are
likely to highly rank articles that are related to presidential elections (even if they don’t necessarily
contain the words election or campaign).

However, a potential problem is that the documents that are highly ranked by LSI or topic models
need not include any mention of the name Camejo. The reason is that the combination of words
in this query is likely to activate one or more latent variables related to the concept of presidential
campaigns. However, once this generalization is made the model has “lost” the information about
the speciﬁc word Camejo and it will only show up in highly ranked documents if this word happens
to frequently occur in these topics (unlikely in this case given that this candidate received relatively
little media coverage compared to the coverage given to the candidates from the two main parties).
But from the viewpoint of the original query, our preference would be to get documents that are
about the general topic of US presidential elections with the speciﬁc constraint
that they mention
Peter Camejo.

techniques, such as the widely-used term-frequency inverse-document-
Word-based retrieval
frequency (TF-IDF) method, have the opposite problem in general. They tend to be overly speciﬁc
in terms of matching words in the query to documents.

In general of course one would like to have a balance between generality and speciﬁcity. One ad hoc
approach is to combine scores from a general method such as LSI with those from a more speciﬁc
method such as TF-IDF in some manner, and indeed this technique has been proposed in information
retrieval (Vogt and Cottrell, 1999). Similarly, in the ad hoc LDA approach (Wei and Croft, 2006), the
LDA model is linearly combined with document-speciﬁc word d istributions to capture both general
as well as speciﬁc information in documents. However, neith er method is entirely satisfactory since
it is not clear how to trade-off generality and speciﬁcity in a principled way.

The contribution of this paper is a new graphical model based on latent topics that handles the trade-
off between generality and speciﬁcity in a fully probabilis tic and automated manner. The model,
which we call the special words with background (SWB) model, is an extension of the LDA model.
The new model allows words in documents to be modeled as either originating from general topics,
or from document-speciﬁc “special” word distributions, or
from a corpus-wide background distribu-
tion. The idea is that words in a document such as election and campaign are likely to come from
a general topic on presidential elections, whereas a name such as Camejo is much more likely to
be treated as “non-topical” and speciﬁc to that document. Wo
rds in queries are automatically inter-
preted (in a probabilistic manner) as either being topical or special, in the context of each document,
allowing for a data-driven document-speciﬁc trade-off bet ween the bene ﬁts of topic-based abstrac-
tion and speciﬁc word matching. Daum ´e and Marcu (2006) inde pendently proposed a probabilistic
model using similar concepts for handling different training and test distributions in classiﬁcation
problems.

Although we have focused primarily on documents in information retrieval in the discussion above,
the model we propose can in principle be used on any large sparse matrix of count data. For example,
transaction data sets where rows are individuals and columns correspond to items purchased or Web
sites visited are ideally suited to this approach. The latent topics can capture broad patterns of
population behavior and the “special word distributions” c
an capture the idiosyncracies of speciﬁc
individuals.

Section 2 reviews the basic principles of the LDA model and introduces the new SWB model. Sec-
tion 3 illustrates how the model works in practice using examples from New York Times news
articles. In Section 4 we describe a number of experiments with 4 different document sets, includ-
ing perplexity experiments and information retrieval experiments, illustrating the trade-offs between
generalization and speciﬁcity for different models. Secti on 5 contains a brief discussion and con-
cluding comments.

2 A Topic Model for Special Words

Figure 1(a) shows the graphical model for what we will refer to as the “standard topic model”
or LDA. There are D documents and document d has Nd words. α and β are ﬁxed parameters of
symmetric Dirichlet priors for the D document-topic multinomials represented by θ and the T topic-
word multinomials represented by φ. In the generative model, for each document d, the Nd words

(b)

(a)

b

f

T

a

q

z

w

dN

D

b
1

y

b
2

W

b
0

f

T

a

q

z

w

g

l

x

dN

D

Figure 1: Graphical models for (a) the standard LDA topic model (left) and (b) the proposed special
words topic model with a background distribution (SWB) (right).

are generated by drawing a topic t from the document-topic distribution p(z |θd ) and then drawing
a word w from the topic-word distribution p(w|z = t, φt ). As shown in Grifﬁths and Steyvers
(2004) the topic assignments z for each word token in the corpus can be efﬁciently sampled vi a
Gibbs sampling (after marginalizing over θ and φ). Point estimates for the θ and φ distributions
can be computed conditioned on a particular sample, and predictive distributions can be obtained by
averaging over multiple samples.

We will refer to the proposed model as the special words topic model with background distribution
(SWB) (Figure 1(b)). SWB has a similar general structure to the LDA model (Figure 1(a)) but with
additional machinery to handle special words and background words. In particular, associated with
each word token is a latent random variable x, taking value x = 0 if the word w is generated via
the topic route, value x = 1 if the word is generated as a special word (for that document) and
value x = 2 if the word is generated from a background distribution speciﬁc for the corpus. The
variable x acts as a switch: if x = 0, the previously described standard topic mechanism is used
to generate the word, whereas if x = 1 or x = 2, words are sampled from a document-speciﬁc
multinomial Ψ or a corpus speciﬁc multinomial Ω (with symmetric Dirichlet priors parametrized by
β1 and β2 ) respectively. x is sampled from a document-speciﬁc multinomial λ, which in turn has
a symmetric Dirichlet prior, γ . One could also use a hierarchical Bayesian approach to introduce
another level of uncertainty about the Dirichlet priors (e.g., see Blei, Ng, and Jordan, 2003) —we
have not investigated this option, primarily for computational reasons. In all our experiments, we
set α = 0.1, β0 = β2 = 0.01, β1 = 0.0001 and γ = 0.3 —all weak symmetric priors.

The conditional probability of a word w given a document d can be written as:

p(w|d) = p(x = 0|d)

p(w|z = t)p(z = t|d) + p(x = 1|d)p′ (w|d) + p(x = 2|d)p′′ (w)

T
Xt=1
where p′ (w|d) is the special word distribution for document d, and p′′ (w) is the background word
distribution for the corpus. Note that when compared to the standard topic model the SWB model
can explain words in three different ways, via topics, via a special word distribution, or via a back-
ground word distribution. Given the graphical model above, it is relatively straightforward to derive
Gibbs sampling equations that allow joint sampling of the zi and xi latent variables for each word
token wi , for xi = 0:

p (xi = 0, zi = t |w, x−i , z−i , α, β0 , γ ) ∝

and for xi = 1:

Nd0,−i + γ
Nd,−i + 3γ

×

C T D
td,−i + α
Pt′ C T D
t′ d,−i + T α

×

CW T
wt,−i + β0
Pw ′ CW T
w ′ t,−i + W β0

p (xi = 1 |w, x−i , z−i , β1 , γ ) ∝

Nd1,−i + γ
Nd,−i + 3γ

×

CW D
wd,−i + β1
Pw ′ CW D
w ′ d,−i + W β1

e mail krugman  nytimes  com memo to  cri tics of the media s liberal  bias the pinkos  you really should  be  going  after  are those  business  reporters even i was startled  by 
the  tone   of  the  jan  21  issue  of  investment  news  which  describes  i tself  as   the   weekly  newspape r  for  financial   advise rs  the   headline  was  paul  o  neill  s  swee t  deal   the  
blurb  was  irs  backs  off  closing  loophole  averting  tax  liabi lity  for  execs  and  treasury  chie f  it  s  not  really  news  that  the  bush  administration  likes  tax  breaks  for 
businessmen but two weeks late r i  learned from the wall street journal that this loophole is more than a tax break for businessmen it s a gift to biznesmen and it may be  
part  of  a large r  pattern  confused in the  former soviet union the term biznesmen  pronounced beeznessmen re fe rs to the  class of sudden  new rich who eme rged  after the  
fall  of  communism  and  who  gene rally  got  rich  by  using  their  connections  to  strip  away  the  assets  of  public  ente rprises  what  we   ve   learned  from  enron  and  other  
playe rs to be named later is that ame rica has its own biznesmen and that we  need to watch out  for policies that make it easie r for them to ply their trade it  turns out that  
the  sweet   deal  investment   news  was  re fe rring  to  the  use  of  split  premium  life   insurance  pol icies  to  give   executives  largely  tax  free  compensat ion  you  don  t  want  to 
know the detai ls is  an even sweete r deal for execut ives  of  companies that  go belly up it  shields their wealth  from creditors  and even from lawsuits sure enough  reports 
the  wall  street  journal  forme r  enron  c  e  o  s  kenneth  lay  and  jeffrey  skilling  both  had  large  spl it  premium  policies  so  what  othe r  pro  biznes  policies  have  been  
promulgated lately last  year both houses of  …
 
 
 
john  w  snow  was  paid  more  than  50  mi llion  in  salary  bonus  and  stock  in  his  nearly  12  years  as  chairman  of  the  csx  corporation  the  rai lroad  company  during  that  
period  the  company  s  profits  fell  and  its  stock  rose  a  bit  more  than  half  as  much  as  that  of  the  average  big  company  mr  snow  s  compensation  amid  csx  s  uneven 
performance  has  drawn  crit icism  from  union  officials  and  some  corporate  governance  specialists  in  2000  for  example  afte r  the  stock  had  plunged  csx  decided  to  
reve rse  a  25 million  loan  to  him  the move   is  likely  to  get  more  scrutiny  afte r  yeste rday  s  announcement  that  mr  snow  has  been  chosen  by  president  bush  to  replace  
paul o ne ill as the treasury secretary like mr o ne ill mr snow is an outsider on wall street but an inside r in  corporate ame rica with long expe rience running  an industrial  
company some wall street  analysts who follow  csx  said  yeste rday that mr  snow had ably led the company through a  difficult pe riod in the  railroad industry  and would 
make  a  good treasury  secretary it  s  an excel lent  nomination said jill  evans  an  analyst  at j p morgan who has  a neutral  rating  on  csx stock i think john s  a  great  person 
for the  administration he  as the  c e o of  a railroad  has probably touched every sector  of the economy union  officials are less  complimentary  of mr snow s  performance  
at  csx  last  year  the  a  f  l  c  i  o  criticized  him  and  csx  for  the  company  s  decision  to  reverse  the  loan  allowing  him  to  return  stock  he  had  purchased  with  the  borrowed  
money  at  a  time  when  independent  directors  are  in  demand  a  corporate   gove rnance  specialist  said  recently  that  mr  snow  had  more   business  relat ionships  with 
membe rs  of  his  own  board  than  any  othe r  chie f  executive  in  addition  mr  snow  is  the   third  highest  paid  of  37  chief  executives  of  transportation  companies  said  ric 
marshall  chief  executive  of  the  corporate  library  which  provides  specialized  investment  research  into  corporate  boards  his  own  compensation  levels  have  been  pretty 
high mr marshall  said he  could afford to take a public service job a csx program in 1996 allowed mr snow and other top csx execut ives to buy… 

Figure 2: Examples of two news articles with special words (as inferred by the model) shaded in
gray. (a) upper, email article with several colloquialisms, (b) lower, article about CSX corporation.

and for xi = 2:

Nd2,−i + γ
Nd,−i + 3γ

×

p (xi = 2 |w, x−i , z−i , β2 , γ ) ∝

CW
w,−i + β2
Pw ′ CW
w ′ ,−i + W β2
where the subscript −i indicates that the count for word token i is removed, Nd is the number of
words in document d and Nd0 , Nd1 and Nd2 are the number of words in document d assigned to the
w are the
and CW
, CW D
latent topics, special words and background component, respectively, CW T
wt
wd
number of times word w is assigned to topic t, to the special-words distribution of document d, and
to the background distribution, respectively, and W is the number of unique words in the corpus.
Note that when there is not strong supporting evidence for xi = 0 (i.e., the conditional probability
of this event is low), then the probability of the word being generated by the special words route,
xi = 1, or background route, xi = 2 increases.
One iteration of the Gibbs sampler corresponds to a sampling pass through all word tokens in the
corpus. In practice we have found that around 500 iterations are often sufﬁcient for the in-sample
perplexity (or log-likelihood) and the topic distributions to stabilize.

We also pursued a variant of SWB, the special words (SW) model that excludes the background
distribution Ω and has a symmetric Beta prior, γ , on λ (which in SW is a document-speciﬁc Bernoulli
distribution). In all our SW model runs, we set γ = 0.5 resulting in a weak symmetric prior that is
equivalent to adding one pseudo-word to each document. Experimental results (not shown) indicate
that the ﬁnal word-topic assignments are not sensitive to ei ther the value of the prior or the initial
assignments to the latent variables, x and z .

3 Illustrative Examples

We illustrate the operation of the SW model with a data set consisting of 3104 articles from the
New York Times (NYT) with a total of 1,399,488 word tokens. This small set of NYT articles was
formed by selecting all NYT articles that mention the word “E nron.” The SW topic model was run
with T = 100 topics. In total, 10 Gibbs samples were collected from the model. Figure 2 shows
two short fragments of articles from this NYT dataset. The background color of words indicates the
probability of assigning words to the special words topic —d
arker colors are associated with higher
probability that over the 10 Gibbs samples a word was assigned to the special topic. The words
with gray foreground colors were treated as stopwords and were not included in the analysis. Figure
2(a) shows how intentionally misspelled words such as “bizn esmen ” and “beeznessmen ” and rare

Collection

NIPS
PATENTS
AP
FR

# of
# of
Mean
Median
Total # of
Docs Word Tokens Doc Length Doc Length Queries
N/A
1322.6
1310
1740
2,301,375
N/A
2237.2
1858
15,014,099
6711
142
242.6
235.5
2,426,181
10000
2500
6,332,681
516
2533.1
30

Table 1: General characteristics of document data sets used in experiments.

NIPS

PATENTS

set 
number 
results 
case 
problem 
function 
values 
paper 
approach 
large 

.0206
.0167
.0153
.0123
.0118
.0108
.0102
.0088
.0080
.0079

f ig 
end 
extend 
invent 
v iew 
shown 
claim 
side 
posit 
form 

.0647
.0372
.0267
.0246
.0214
.0191
.0189
.0177
.0153
.0128

AP

tagnum 
itag 
requir 
includ 
section 
determ in 
part 
inform 
addit 
applic 

.0416
.0412
.0381
.0207
.0189
.0134
.0112
.0105
.0096
.0086

FR

nation 
sai 
presid 
polici 
issu 
call 
support 
need 
govern 
ef fort 

.0147
.0129
.0118
.0108
.0096
.0094
.0085
.0079
.0070
.0068

Figure 3: Examples of background distributions (10 most likely words) learned by the SWB model
for 4 different document corpora.

words such as “pinkos” are likely to be assigned to the specia
l words topic. Figure 2(b) shows how
a last name such as “Snow ” and the corporation name “CSX ” that
are speciﬁc to the document are
likely to be assigned to the special topic. The words “Snow ” a nd “CSX ” do not occur often in other
documents but are mentioned several times in the example document. This combination of low
document-frequency and high term-frequency within the document is one factor that makes these
words more likely to be treated as “special” words.

4 Experimental Results: Perplexity and Precision

We use 4 different document sets in our experiments, as summarized in Table 1. The NIPS and
PATENTS document sets are used for perplexity experiments and the AP and FR data sets for re-
trieval experiments. The NIPS data set is available online1 and PATENTS, AP, and FR consist of
documents from the U.S. Patents collection (TREC Vol-3), Associated Press news articles from 1998
(TREC Vol-2), and articles from the Federal Register (TREC Vol-1, 2) respectively. To create the
sampled AP and FR data sets, all documents relevant to queries were included ﬁrst and the rest of
the documents were chosen randomly. In the results below all LDA/SWB/SW models were ﬁt using
T = 200 topics.
Figure 3 demonstrates the background component learned by the SWB model on the 4 different doc-
ument data sets. The background distributions learned for each set of documents are quite intuitive,
with words that are commonly used across a broad range of documents within each corpus. The ratio
of words assigned to the special words distribution and the background distribution are (respectively
for each data set), 25%:10% (NIPS), 58%:5% (PATENTS), 11%:6% (AP), 50%:11% (FR). Of note
is the fact that a much larger fraction of words are treated as special in collections containing long
documents (NIPS, PATENTS, and FR) than in short “abstract-l ike ” collections (such as AP) —this
makes sense since short documents are more likely to contain general summary information while
longer documents will have more speciﬁc details.

4.1 Perplexity Comparisons

The NIPS and PATENTS documents sets do not have queries and relevance judgments, but nonethe-
less are useful for evaluating perplexity. We compare the predictive performance of the SW and
SWB topic models with the standard topic model by computing the perplexity of unseen words in
test documents. Perplexity of a test set under a model is de ﬁn ed as follows:

1From http://www.cs.toronto.edu/ ˜roweis/data.html

1900

1800

1700

1600

1500

1400

1300

1200

1100

y
t
i
x
e
l
p
r
e
P

LDA
SW
SWB

550

500

450

400

350

300

250

200

y
t
i
x
e
l
p
r
e
P

LDA
SW
SWB

1000
10

20

30
70
60
50
40
Percentage of Words Observed

80

90

150
10

20

30
70
60
50
40
Percentage of Words Observed

80

90

Figure 4: Average perplexity of the two special words models and the standard topics model as a
function of the percentage of words observed in test documents on the NIPS data set (left) and the
PATENTS data set (right).

d=1 log p(wd |Dtrain )
Perplexity(wtest |Dtrain ) = exp(cid:18)− PDtest
(cid:19)
PDtest
d=1 Nd
where wtest is a vector of words in the test data set, wd is a vector of words in document d of the test
set, and Dtrain is the training set. For the SWB model, we approximate p(wd |Dtrain ) as follows:

p(wd |Dtrain ) ≈

1
S

p(wd |{ΘsΦs Ψs Ωs λs })

S
Xs=1
where Θs , Φs , Ψs , Ωs and λs are point estimates from s = 1:S different Gibbs sampling runs.
The probability of the words wd in a test document d, given its parameters, can be computed as
follows:
Nd
wi #
Yi=1 "λs
T
Xt=1
φs
wi t θs
td + λs
2dψs
wi d + λs
3dΩs
1d
where Nd is the number of words in test document d and wi is the ith word being predicted in the
d are point estimates from sample s.
wi and λs
wi d , Ωs
wi t , ψs
td , φs
test document. θs
When a fraction of words of a test document d is observed, a Gibbs sampler is run on the observed
words to update the document-speciﬁc parameters, θd , ψd and λd and these updated parameters are
used in the computation of perplexity. For the NIPS data set, documents from the last year of the
data set were held out to compute perplexity (Dtest = 150), and for the PATENTS data set 500
documents were randomly selected as test documents.

p(wd |{ΘsΦs Ψs Ωs λs }) =

From the perplexity ﬁgures, it can be seen that once a small fr action of the test document words
is observed (20% for NIPS and 10% for PATENTS), the SW and SWB models have signiﬁcantly
lower perplexity values than LDA indicating that the SW and SWB models are using the special
words “route ” to better learn predictive models for individ ual documents.

4.2 Information Retrieval Results

Returning to the point of capturing both speciﬁc and general aspects of documents as discussed in
the introduction of the paper, we generated 500 queries of length 3-5 using randomly selected low-
frequency words from the NIPS corpus and then ranked documents relative to these queries using
several different methods. Table 2 shows for the top k-ranked documents (k = 1, 10, 50, 100) how
many of the retrieved documents contained at least one of the words in the query. Note that we are
not assessing relevance here in a traditional information retrieval sense, but instead are assessing how

Method
TF-IDF
LSI
LDA
SW
SWB

1 Ret Doc
100.0
97.6
90.0
99.2
99.4

10 Ret Docs
100.0
82.7
80.6
97.1
96.6

50 Ret Docs
100.0
64.6
67.0
79.1
78.7

100 Ret Docs
100.0
54.3
58.7
67.3
67.2

Table 2: Percentage of retrieved documents containing at least one query word (NIPS corpus).

AP

FR

Method
TF-IDF
LSI
LDA
SW
SWB

Method
TF-IDF
LSI
LDA
SW
SWB

MAP

MAP

Title
.353
.286
.424
.466*
.460*

Title
.268
.329
.344
.371
.373

Desc
.358
.387
.394
.430*
.417

Desc
.272
.295
.271
.323*
.328*

Concepts
.498
.459
.498
.550*
.549*

Concepts
.391
.399
.396
.448*
.435

*=sig dif ference wrt LDA

Method
TF-IDF
LSI
LDA
SW
SWB

Method
TF-IDF
LSI
LDA
SW
SWB

Pr@10d
Title
Desc
.434
.406
.469
.455
.463
.478
.509*
.524*
.513*
.495

Concepts
.549
.523
.556
.599*
.603*

Pr@10d

Title
.300
.366
.428
.469
.462

Desc
.287
.327
.340
.407*
.423*

Concepts
.483
.487
.487
.550*
.523

Figure 5: Information retrieval experimental results.

often speciﬁc query words occur in retrieved documents. TF- IDF has 100% matches, as one would
expect, and the techniques that generalize (such as LSI and LDA) have far fewer exact matches.
The SWB and SW models have more speciﬁc matches than either LD A or LSI, indicating that they
have the ability to match at the level of speciﬁc words. Of cou rse this is not of much utility unless
the SWB and SW models can also perform well in terms of retrieving relevant documents (not just
documents containing the query words), which we investigate next.

For the AP and FR documents sets, 3 types of query sets were constructed from TREC Topics 1-
150, based on the T itle (short), Desc (sentence-length) and C oncepts (long list of keywords) ﬁelds.
Queries that have no relevance judgments for a collection were removed from the query set for that
collection.

The score for a document d relative to a query q for the SW and standard topic models can be com-
puted as the probability of q given d (known as the query-likelihood model in the IR community).
For the SWB topic model, we have

[p(x = 0|d)

p(w|z = t)p(z = t|d) + p(x = 1|d)p′ (w|d) + p(x = 2|d)p′′ (w)]

T
p(q |d) ≈ Yw∈q
Xt=1
We compare SW and SWB models with the standard topic model (LDA), LSI and TF-IDF. The TF-
CW D
IDF score for a word w in a document d is computed as TF-IDF(w, d) =
. For
D
× log2
wd
Nd
Dw
LSI, the TF-IDF weight matrix is reduced to a K -dimensional latent space using SVD, K = 200. A
given query is ﬁrst mapped into the LSI latent space or the TF- IDF space (known as query folding),
and documents are scored based on their cosine distances to the mapped queries.

To measure the performance of each algorithm we used 2 metrics that are widely used in IR research:
the mean average precision (MAP) and the precision for the top 10 documents retrieved (pr@10d).
The main difference between the AP and FR documents is that the latter documents are considerably
longer on average and there are fewer queries for the FR data set. Figure 5 summarizes the results,
broken down by algorithm, query type, document set, and metric. The maximum score for each
query experiment is shown in bold: in all cases (query-type/data set/metric) the SW or SWB model
produced the highest scores.

To determine statistical signiﬁcance, we performed a t-tes t at the 0.05 level between the scores of
each of the SW and SWB models, and the scores of the LDA model (as LDA has the best scores
overall among TF-IDF, LSI and LDA). Differences between SW and SWB are not signiﬁcant. In
ﬁgure 5, we use the symbol * to indicate scores where the SW and SWB models showed a statis-
tically signiﬁcant difference (always an improvement) rel ative to the LDA model. The differences
for the “non-starred ” query and metric scores of SW and SWB ar
e not statistically signiﬁcant but
nonetheless always favor SW and SWB over LDA.

5 Discussion and Conclusions

Wei and Croft (2006) have recently proposed an ad hoc LDA approach that models p(q |d) as a
weighted combination of a multinomial over the entire corpus (the background model), a multino-
mial over the document, and an LDA model. Wei and Croft showed that this combination provides
excellent retrieval performance compared to other state-of-the-art IR methods. In a number of exper-
iments (not shown) comparing the SWB and ad hoc LDA models we found that the two techniques
produced comparable precision performance, with small but systematic performance gains being
achieved by an ad hoc combination where the standard LDA model in ad hoc LDA was replaced
with the SWB model. An interesting direction for future work is to investigate fully generative
models that can achieve the performance of ad hoc approaches.

In conclusion, we have proposed a new probabilistic model that accounts for both general and spe-
ciﬁc aspects of documents or individual behavior. The model extends existing latent variable prob-
abilistic approaches such as LDA by allowing these models to take into account speciﬁc aspects of
documents (or individuals) that are exceptions to the broader structure of the data. This allows, for
example, documents to be modeled as a mixture of words generated by general topics and words
generated in a manner speciﬁc to that document. Experimenta l results on information retrieval tasks
indicate that the SWB topic model does not suffer from the weakness of techniques such as LSI
and LDA when faced with very speciﬁc query words, nor does it s uffer the limitations of TF-IDF in
terms of its ability to generalize.

Acknowledgements

We thank Tom Grifﬁths for useful initial discussions about t he special words model. This material
is based upon work supported by the National Science Foundation under grant IIS-0083489. We
acknowledge use of the computer clusters supported by NIH grant LM-07443-01 and NSF grant
EIA-0321390 to Pierre Baldi and the Institute of Genomics and Bioinformatics.

References

Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003) Latent Dirichlet allocation, Journal of Machine Learning
Research 3: 993-1022.
Buntine, W., L ¨ofstr ¨om, J., Perttu, S. and Valtonen, K. (2005) Topic-speci ﬁc scoring of documents for relevant
retrieval Workshop on Learning in Web Search: 22nd International Conference on Machine Learning,
pp. 34-41. Bonn, Germany.
Canny, J. (2004) GaP: a factor model for discrete data. Proceedings of the 27th Annual SIGIR Conference,
pp. 122-129.
Daum ´e III, H., and Marcu, D. (2006) Domain Adaptation for St atistical classi ﬁers.
Intelligence Research, 26: 101-126.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990) Indexing by latent
semantic analysis. Journal of the American Society for Information Science, 41(6): 391-407.
Grifﬁths, T. L., and Steyvers, M. (2004) Finding scienti ﬁc t
opics, Proceedings of the National Academy of
Sciences, pp. 5228-5235.
Hofmann, T. (1999) Probabilistic latent semantic indexing, Proceedings of the 22nd Annual SIGIR Confer-
ence, pp. 50-57.
Vogt, C. and Cottrell, G. (1999) Fusion via a linear combination of scores. Information Retrieval, 1(3): 151-
173.
Wei, X. and Croft, W.B. (2006) LDA-based document models for ad-hoc retrieval, Proceedings of the 29th
SIGIR Conference, pp. 178-185.

Journal of the Arti ﬁcial


Explaining Preference Learning
Alyssa Glass
CS229 Final Project
Stanford University, Stanford, CA
glass@cs.stanford.edu

Introduction
A l though   the re   is   ex is t ing   wo rk   on   lea rn ing   use r
preferences in various systems, the outputs of such systems
tend  to  be  confusing  to  users.   Studies  of  users  interacting
with  such  systems  show  that  when  the  system  incorrectly
predicts a user's preference, the user may attempt to correct
the   v iews   of   the   sys tem   (for   ins tance ,   by   random ly
providing   more   training   examples) .     In   many   cases ,
however,  users  lose  patience  with  this  approach,  largely
due to a misunderstanding of how the underlying system is
using  the  data  to make  predictions.   Ultimately,  they may
stop  trusting  the system entirely.  Even when  the system  is
correct,  users  view  such  outcomes  as  "magical"  in  some
way, but are unable  to understand why a particular answer
is  correct,  or whether  the  system  is  likely  to  be  helpful  in
the future.
In this paper, we describe the augmentation of a preference
learner,  designed  to  provide  meaningful  feedback  to  the
user  in  the  form  of  explanations.   We  begin  by  describing
the  preference  learning  problem  in  the  specific  context  of
semi-autonomous  meeting  scheduling,  and  the  preference
learning  system  currently  being  used  to  solve  the  problem
in   the   CALO   research   project .     We   describe   current
research on usability in the face of active learning systems,
including  a  study  that we  conducted  on  learning  and  trust.
We  then describe how  this preference  learning system was
modified  to  store  meta-information  about  its  learning
during  the course of  system execution, and how  this meta-
information  is  used  to  provide  explanations  of  answers
concluded  by  the  system.   Finally,  we  show  how  these
explanations can be incorporated into the larger scheduling
system  to  provide  transparency  into  the  learning  system,
enabling trust between user and system.
Motivation
The  problem  of  scheduling  meetings  among  groups  of
busy ,   over-comm i t ted   profess iona ls   is   a   we l l-known
p rob lem   fo r   anyone   who   has   wo rked   in   an   o f f ice
environment.   Often,  a  series  of  emails  or  phone  calls
among  desired  meeting  participants  are  required  before  a
jointly  acceptable  time  has  been  chosen.   Central  to  this
problem  is  the  incorporation  of  preferences  –  different
pa r t ic ipan ts   may   p re fe r   pa r t icu la r   mee t ing   leng ths ,
particular times of the day, particular days of the week, etc.

Comp l i c a t ing   th e   p rob l em   o f   in co rpo r a t ing   u s e r
preferences  is  the  problem  of  dealing  with  conflicts.   The
PTIME  personalized  scheduling  assistant  (Berry,  et  al.
2006)  is  one  system  built  to  automatically  handle  these
schedu l ing   prob lems   wh i le   tak ing   in to   accoun t   user
preferences,  as  part  of  the Cognitive Assistant  that Learns
and Organizes (CALO) project.
In PTIME, a user  initializes  the system by  indicating some
initial  preferences  for  meeting  scheduling  (for  instance,
preferring  morning  vs.  afternoon  meetings;  whether  to
over lap   mee t ings   if   necessary ;   and   whe ther   to   drop
individual  meeting  participants  in  order  to  fit  a  schedule)
as well as  information on how  these preferences should be
traded off with each other when they conflict (for instance,
whether  a  meeting  should  be  shortened  in  order  to  not
overlap another existing meeting).  To schedule a meeting,
a  user  enters  both  broad  constraints  (such  as  “any  time
before   nex t   Thursday”)   as   we l l   as   some   sugges ted
relaxations  (such  as  “Joe  is  an  optional  participant”).
PTIME  then  attempts  to  solve  the  constraint  problem
defined  by  the  meeting  specification  entered  by  the  user
and   the   preferences   of   the   user   and   o ther   mee t ing
participants,  presenting  the  user  with  multiple  possible
schedules  based  on  these  constraints,  some  of  which may
relax  the  constraints  along  varying  problem  dimensions.
These possible schedules are presented  to  the user  roughly
in   preference   order .     (In   some   cases ,   less-preferred
schedules  are  presented  ahead  of  other  schedules  in  order
to  provide  the  user  with  more  variety  of  options  and  to
allow  the  preference  learner  to  better  explore  the  space;
details  can  be  found  in  (Yorke-Smith  et  al.  2006).)   When
the  user  then  selects  one  of  the  provided  schedules,  the
system’s model of the user’s preferences is updated.
This  update  of  the  user’s  preferences  is  computed  by
PL IANT   (P re fe rence   Lea rn ing   th rough   In te rac t ive
Advisable  Nonintrusive  Training)  (Gervasio  et  al.  2005).
PLIANT  uses  support  vector  machines  (SVMs)  to  update
the preference model, using each selection of a schedule as
a new data point, as described in the next section.
Although  PTIME,  with  on-line  learning  computed  by
PLIANT, has been deployed in restricted settings and used
by  several  users  for  accomplishing  limited  tasks,  adoption
of  the  system  has  been  slow.   A  study  of  PTIME  reported
in  (Yorke-Smith et al. 2006) and a larger CALO-wide user
study   conducted   by   us ,   in   conjunction   with   Deborah
McGuinness  and Michael Wolverton,  both  confirm  that  a

significant  barrier  to  the  wide  adoption  of  a  system  like
PTIME is the problem of transparency.  Most users simply
do  not  trust  a  system  that  is  constantly  modifying  itself
through  learning  unless  they  are  able  to  ask  questions  and
be  provided with  information  about  the  inner workings  of
the  system.   PTIME  has  begun  to  address  this  problem  by
explicitly  indicating  where  constraints  are  being  violated,
but much more work  is  needed  to  provide  explanations  of
the preference  learning.   In particular, since users view  the
ranked  schedules  provided  by  PTIME  as  suggestions  for
wh i ch   s ch edu l e s   th e   u s e r   i s   exp e c t ed   to   p r e f e r ,
explanations  providing  transparency  into  why  particular
schedules  are  recommended  over  others  is  vitally  needed
for  the  usability  of  the  system.   To  solve  this  problem,
then, we attempt  to provide  transparency  into  the PLIANT
learning system.
Active Preference Learning
Active  learning  in  PLIANT  relies  on  a  starting  feature  set
of  6  criteria,  initialized  through  input  from  the  user.   The
criteria are:
1. Scheduling windows for requested meeting
2. Duration of meeting
3. Overlaps and conflicts
4. Location of meeting
5. Participants in meeting
6. Preferences of other meeting participants
The  first  time  PTIME  is  started  by  a  user,  the  system
elicits  initial  preferences  for  these  six  criteria  through  a
wizard-type  interface.   For  example,  for  “overlaps  and
conflicts,”  the  system  asks  the  user  whether  he  prefers
meetings  to  never  overlap,  or  to  allow  the  system  to
“double-book” him in two meetings at the same time when
necessary.   The  system  also  asks  the  user  to  rank  the
importance  of  these  six  criteria  relative  to  each  other;  for
example,  if  the  user’s  preference  on  duration  and  location
canno t   bo th   be   me t ,   the   sy s tem   s to re s   pa i r -w i se
information about which criteria is more important.
Given  the partial utility  functions  implied by each of  these
criteria,  PLIANT  combines  these  utility  functions  into  a
single function using a 2-order Choquet integral:
F(z1, …, zn) = Σi ai zi + Σi,j aij (zi ∧ zj)
where  each  zi  =  ui(xi),  the  utility  for  criteria  i  based  on
value  x i.   (Note  that  this  is  essentially  a  function  over
schedules,  not  individual  meetings;  for  a  given  schedule,
each  xi  is  a  measure  of  the  degree  to  which  the  full
schedule, containing multiple meetings, matches  the given
criteria.)   Thus,  the  learning  task  for  PLIANT  is  to  learn
the weights a i  and aij  associated with  each  individual  and
pair-wise matching of  the  six criteria.   Justification  for  the
use of  a 2-order Choquet  integral  is  in  (Yorke-Smith  et  al.
2006).

This  function  is  then combined with  the  initial preferences
elicited from the user by the full evaluation function:
F`(Z) = α ✕ AZ  +  (1-α) ✕ WZ
where Z  is  the  full  schedule being  evaluated, W   is  the  full
set  of  a i  and  aij  weights  learned  above,  A   is  the  initial  set
of preferences elicited  from  the user, and α  is a parameter
ind ica t ing   the   impac t   of   the   e l ic i ted   versus   learned
preferences.   The α  parameter  is  decayed  over  time,  both
to  accommodate  a  user’s  changing  preferences  and  to
acknowledge the difficulty many users have with explicitly
indicating  their  own  scheduling  preferences  without  the
use of examples.
The overall system, then, has the following work flow:  the
system  elicits  initial  preferences  from  the  user  (the  A
vector  above)  once,  during  PTIME’s  first  use.   Then,  for
each  new  meeting,  the  user  specifies  meeting  parameters
and  constraints.   The  PTIME  constraint  solver  generates
several  candidate  schedules  (Z ’s),  relaxing  constraints
when necessary, according to the stated constraints and the
user’s  existing  calendar.   The  candidate  schedules  are
presented to the user in (roughly) the calculated preference
order, and the user selects a single schedule, Z.  The user’s
preferences  (the ai  and a ij weights)  are  then updated based
on the user’s choice.
The data used  to perform  the preference update  is both  the
chosen  schedule,  and  the  other  schedules  considered  but
rejected  by  the  user.   For  example,  if  three  schedules  are
presented  in  ranked order  to  the user, and  the user chooses
the  third  schedule,  a  partial  ordering  is  imposed  on  the
schedules  indicating  that  the  third  (chosen)  schedule  is
preferred to both the first and second schedule (Gervasio et
al.  2005).   These  partial  orderings  are  then  added  into  the
optimization problem solved by the SVM, in the same way
that  search  engine  results  are  ranked  according  to  past
c l ick th rough   da ta   in   (Jo a ch im s   2002 ) .    
Joachims’
SVMlight  is used as  the basis  for PLIANT’s update of  the
preference  weights.   It  is  this  update  of  the  preference
weights, and  the resulting ranked ordering presented  to  the
user, that we seek to explain.
Usability and Active Learning
As a guide  for how best  to explain  the preferences  learned
by  PLIANT,  we  considered  the  results  of  three  user
studies.   Relevant points  from each of  the  three studies are
summarized below.
First ,   outside   of   the   work   done   for   this   project ,   we
conducted  our  own  study  of  CALO  usability  (joint  work
with  Deborah  McGuinness  and  Michael  Wolverton;  not
yet  published).   Focusing  on  PLIANT  and  PTIME,  in
particular,  we  found  a  general  lack  of  understanding
among  users  of  how  preferences  are  being  updated,  or
even that they are being updated at all.  Although there are
many  reasons  why  a  recommendation  from  PLIANT may

2

violate  the  initial  preferences  entered  by  the  user  (for
examp le ,   PL IANT   may   have   mo re   recen t   lea rned
preferences  that  violate  the  initial  preferences;  PLIANT
may  be  deliberately  violating  preferences  in  order  to
explore  the search space; or  the user himself may not have
correc t ly   ind ica ted   h is   true   preferences ,   a   common
p rob lem   w i th   exp l ic i t   p re fe rence   e l ic i ta t ion ) ,   use rs
nonetheless  expressed  surprise  and  confusion  whenever
their  true  preferences  were  violated  by  the  ranking  of
schedules presented  to  them.  Many users  commented  that
it  seemed  that  the  system  was  ignoring  their  preferences,
leading  them  to  view  the  system  as  untrustworthy.   One
user  commented,  “I  trust  [PTIME’s]  accuracy,  but  not  its
judgment.”   This  type  of  complaint  specifically  about  the
use  of  preferences was  common  among many  of  the  users
we studied.
Next,  we  considered  a  study  conducted  by  the  PLIANT
team ,   in   wh ich   users   were   in terv iewed   abou t   the ir
requirements  for  an  adaptive  scheduling  system.   One  of
the  top  requests  made  by  these  users  was  “transparency
into  assisted  scheduling  decisions  (e.g.,  explanations  of
conflicts  and  learned  preferences)”  (Yorke-Smith  et  al.
2006).  The authors continue by noting that “the preference
model  must  be  explainable  to  the  user  …  in  terms  of
f am i l i a r ,   d om a i n - r e l e v a n t   c o n c e p t s . ”     T h i s  
l a s t
requirement, in particular, is often a problem with attempts
to  understand  and  explain  statistical  learning  methods,  a
problem that we address in our formulation.
The  final  study  that  we  considered  is  by  (Stumpf  et  al.
2007).   This  study  considered  explanations  of  statistical
machine  learning  methods,  focusing  on  a  naïve  Bayes
learner  and  a  rule-learning  (classification)  system,  both  in
the context of learning the proper foldering of email.  They
found  that  rule-based explanations,  taken directly  from  the
rule-learning  system,  were  most  easily  understood  by
users,  but  were  not  trusted  to  the  same  extent.   They  also
found that similarity-based explanations, which users had a
very hard time understanding, were nonetheless considered
more  “natural”  by  the  users,  and were  easily  trusted  to  an
extent  beyond what would  be  expected,  given  the  level  of
understanding.   These  similarity-based  explanations  were
roughly of  the  form,  “This message  is  really  similar  to  the
message A  in  folder X because  they have  important words
in  common:  <common words  highlighted>.”   The  authors
also  noted  that  many  users  appreciate  a  less  formal,  non-
technical style for explanations.
We   po s i t   tha t   the   unde r s tandab i l i ty   p rob lem   w i th
similarity-based  explanations  noted  in  this  last  study  is
related  directly  to  the  findings  in  the  PTIME  study which
found  that  explanations  must  be  tied  directly  to  domain
concepts  that  are  known  to  the  user.   In  particular,  users
complained  that  the  similarities  identified  by  the  naïve
Bayes   system   seemed   arbitrary;   indeed ,   to   users   not
familiar  with  statistical  machine  learning  methods,  it
wou ld   be   d iff icu l t   to   unders tand   s im i lar i t ies   among

dictionary-style  feature vectors of  several  thousand words,
and  why  some  words  and  not  others  were  considered
important  by  the  system.   In  our  preference  learning
domain,  as  noted  above,  we  are  instead  considering  a
relatively  small  space  of  features,  each  of  which  has
previously  been  explained  to  the  user  in  simple  terms.   In
this  domain,  we  are  thus  able  to  create  similarity-based
explanations  which  may  enjoy  the  trust  noted  in  the  third
study,  while  still  staying  in  the  realm  of  familiar  domain
concepts.   These  similarity-based  explanations  can  be
created  naturally  from  the  computation  done  by  the SVM,
as described in the next section.
Prov id ing   Transparency   in to   Pre ference
Learning
Starting  with  the  PLIANT  code  base,  we  augmented  the
use  of  SVM l igh t   with  code  to  gather  additional  meta-
information  about  the  SVM  itself.   For  the  purposes  of
creating  explanations,  we  considered  the  following  SVM
meta-information:
•  The support vectors identified by the SVM
•  The support vectors “nearest” to the query point
•  The margin to the query point
•  The average margin over all data points
•   The  non-support  vectors  (i.e.,  other  data  points)
nearest to the query point
Initial preferences  elicited  from  the user,  along with
•  
the current value of α
•  The kernel transform used, if any
In   the   PLIANT   sys tem ,   the   kerne l   is   l inear ,   so   no
information  about  the  kernel  transform  was  needed.   If  a
non-linear  kernel  is  used  by  the  SVM,  the  problem  of
explaining  the  results  becomes  much  more  complicated;
we do not address the issue here.
The  next  step  was  to  represent  this  meta-information,
along with underlying information about how SVMs learn,
in   a   way   suitable   for   producing   justifications   of   the
computation  performed  in  PLIANT.   We  represented  this
information  in  Proof  Markup  Language  (PML)  (Pinheiro
da  Silva,  et  al.  2006)  because  of  its  generic  justification
representations  for  general  reasoners,  the  existing  use  of
PML  in  the  CALO  system,  and  work  we  have  done  over
the past year  in  expanding PML  to  represent  the  results of
machine learning (Glass & McGuinness 2006).
For  the  representation  in  PML, we  added  logical  rules  for
the  deduction  performed  by  an  SVM,  both  to  generate  a
conclusion  given  a  query  point  and  a  data  set,  and  to
describe  the  resulting  decision  plane  and  space  of  data
points.  First, we added the base rule “hasRank” which has
three  antecedents  (the  initial  preferences  elicited  from  the
user;  the  calculated  weights,  which  indicate  the  support
vectors  in  the  SVM;  and  the  new  query  point,  which  is  a
schedule  proposed  by  the  constraint  reasoner)  and  results
in  a  conclusion  indicating  the  ranked  preference  for  this

3

schedule,  which  is  the  result  of  the  evaluation  function
defined  above.   The  antecedent  representing  the  support
vectors contains an iw:LearnedSourceUsage, with a link to
the  data  set  used  by  the  SVM,  along  with  information
about   when   this   data   set   was   generated .     (See   PML
specification  referenced at  the end of  this document.)   The
additional  meta-information  gathered  from  the  SVM  is
represented  through  additional  rules  indicating  how  they
are  concluded.   For  example,  “hasCloseDataPoint”  takes
the   second   two   an teceden ts   men t ioned   above ,   and
concludes  a  single  data  point  near  the  query  point,  with
details  about  which  dimensions  are  closest  to  the  query.
Depending   on   context ,   then ,   the   expanded   SVM   can
c o n c l u d e   a s   m a n y   o f  
t h e s e   “hasCloseDataPoint”
conclusions as necessary;  in our  system, we  limited  this  to
5 points because of the relatively small size of the full data
se t .     Add i t iona l   ru les   fo r   each   o f   the   o the r   me ta -
information   types   mentioned   above   were   also   added .
Finally, we  added  a  rule  “hasInitialRank” which  is  similar
to  “hasRank,”  but  only  considers  the  preferences  initially
elicited  from  the user, and provides a  rank based solely on
these initial preferences, without any learning.
The  result,  then,  is  a  formal  justification  for why PLIANT
r e comm end ed   a   p a r t i cu l a r   s ch edu l e .     G iv en   th i s
justification, we  then  created  strategies  for  abstracting  the
justification  for  presentation  to  a  user.   Methods  for
abstracting  justifications  in  PML  already  exist  within  the
Inference   Web   (IW)   infras truc ture   (McGu inness   and
Pinheiro  da  Silva  2004).   We  expanded  these  methods  to
cover  justifications  of  the  form  created  by  our  augmented
SVM.
As  discussed  in  the  usability  section  above,  we  chose  to
focus  on  similarity-based  explanations,  taking  advantage
of  the  small  feature  vectors  and  the  clean  mapping  into
u s e r - u n d e r s t a n d a b l e  
t e rm s .     W e   a l s o   g e n e r a t e d
explanations  in  the  first-person,  as  if  the  system  were
informally  talking  to  the  user  about  its  observations,
because  of  the  preference  for  non-technical  explanations
noted  in  (Stumpf  et  al.  2007).   The  strategies  that  we
created take a PML justification, and consider which of the
above meta-information rules provides the strongest reason
for  presenting  a  given  schedule  to  the  user.   Note  that  this
s e l e c t i o n  
i s   a   b i t   s u b j e c t i v e ;   w e   c h o s e   f a i r l y
straightforward  heuristics  for  choosing  a  strategy, without
making  claims  that  the  best  possible  explanation  is  given
in  all  cases.   Instead,  our  goal  was  simply  to  provide  an
explanation that is reasonable enough, so that the system is
not immediately dismissed by the user.
Once a strategy is chosen, it generates a single explanation
in  English.   These  explanations  are  generated  in  a  simple
fill-in-the-blank  format;  as  such,  the  explanations  are
domain dependent, and can occasionally be a bit awkward,
but  avoid  the  issue  of  full  natural  language  generation.
Examples  of  some  of  the  explanations  generated  by  the
strategies:

4

•  

• 

•  

“This  schedule  closely  follows  the  preferences  you
told  me  when  we  started.”   (Used  when α  is  large,
or  when  the  rank  generated  by  “hasInitialRank”  is
particularly high.)
“This schedule  is similar  to other schedules  that you
have  chosen  in  the  past,  and  I  have  observed  your
preference   for   avoiding  overlaps  and  conflicts. ”
(Used  when  several  of  the  nearest  preferred  data
po in ts   are   par t icu lar ly   c lose   a long   a   par t icu lar
feature dimension.)
“This schedule seems to be strongly preferred, based
on your past selections, and I have observed that you
o f ten   have   a   p re fe rence   fo r   considering  other
participants’ preferences.”   (Used  when  the  margin
to the query point is particularly large, relative to the
average  margin.   This  strategy  also  attempts  to  call
out which feature contributes the most to the margin,
but often  leaves out  the  last clause when no  features
stand out.)
•   “Th is   schedu le   does   no t   c lose ly   ma tch   you r
preferences,  but  listing  it  here  will  help  me  to
understand  more  about  your  preferences,  so  that  I
can  do  better  in  the  future.”   (Used  when  the  ranks
computed both by the learned weights and the initial
preferences  are  poor,  but  PLIANT  is  exploring  the
space.)
Thus,  to  provide  transparency  into  PLIANT’s  preference
learning,  we  envision  a  link  into  PTIME  which  enables
users  to  ask  questions  about  why  a  particular  schedule  is
being  recommended.   The  justification  for  a  particular
schedule,  in PML,  is stored when  the ranking  is generated.
When a user then requests an explanation, this justification
is  parsed,  an  explanation  strategy  is  chosen,  and  the
resulting explanation  is presented for  the user.   In  the past,
we  have  designed  systems  which  are  intended  to  engage
the user  in a  full dialogue, enabling  the user  to ask  follow-
up  questions  and  gather  additional  information  as  desired.
Here, we instead show a system where a single explanation
is  generated  for  each  schedule,  with  no  possibility  for
follow-up  questions;  since  our  goal  is  trust  in  the  overall
preference learner, rather than trust in the recommendation
of  a  particular  schedule,  we  feel  that  the  sum  of  all
explanations  over  all  of  the  generated  schedules  better
accomplishes   our   goal ,   and   fits   more   cleanly   in   the
existing  interface  without  distracting  the  user  from  their
ma in   goa l   o f   se lec t ing   a   schedu le .     The   resu l t ing
architecture, showing the main PLIANT architecture along
with  the  explainer  component  described  here,  is  shown  in
Figure 1.
For  the  purposes  of  this  project,  the  integration  of  our
expanded  PLIANT  system  and  explanation  component
into  the  larger PTIME system was shallow, and completed
only  to  a  degree  to  validate  proof  of  concept.   Using
scheduling and preference data previously gathered during
the   CALO   Yea r   3   C r i t ica l   Lea rn ing   Pe r iod   (CLP )

Figure 1:  System architecture.  Four boxes on left are the original PLIANT system.  Component “SVM Explainer” has been
added, and additional data flow is shown.  (Original PLIANT architecture from (Gervasio et al. 2005).)
(containing  scheduling  and  preference  data  for  16  users,
CLP   data ,   and   Karen   Myers   for   related   discussions ,
support, and ideas.
gathered  over  approximately  two  weeks),  we  generated
meta-information  for  select  examples  as  described  above,
References
used  the meta-information  and  PML  rules  as  described  to
generate  justifications for a subset of  this data, and created
Berry,  P., Conley, K., Gervasio, M.,  Peintner, B., Uribe, T.,  and
explanations  from  these  justifications  using  the  strategies
Yorke-Smith, N.  Deploying a Personalized Time Management
described above.
Agent.  AAMAS-06 Industrial Track, pages 1564-1571, 2006.
CALO, 2006.  http://www.ai.sri.com/project/CALO.
Conclusion
Gervasio, M.T., Moffitt, M.D.,  Pollack, M.E.,  Taylor,  J.M.,  and
We  have  described  the  design  of  a  system  for  explaining
Uribe,  T.E.   Active   Preference   Learning   for   Personalized
Calendar  Scheduling  Assistance.   Proceedings  of  Conference
the results of an SVM active preference learner, and how it
on Intelligent User Interfaces 2005 (IUI-05), 2005.
can  be  integrated  into  an  adaptive  scheduling  assistant.
We based our design on several user studies which indicate
Glass,  A.  and  McGuinness,  D.L.   Introspective  Predicates  for
Explaining  Task  Execution  in  CALO.  Technical  Report,  KSL-
both  the  need  for  explanations  of  statistical  machine
06-04, Knowledge Systems, Artificial  Intelligence Laboratory,
learning  systems,  and  the  types  of  explanations  that  are
Stanford University, 2006.
required  by  users  to  build  trust.   Future  work  includes
Joachims,  T.   Optimizing  Search  Engines  Using  Clickthrough
system  engineering  to  provide  a  full  integration  into  the
Da ta .   Proceedings  of  the  ACM  Conference  on  Knowledge
PTIME   system;   analysis   of   whether   additional   meta-
Discovery and Data Mining (KDD), ACM, 2002.
information  or  explanation  strategies  not  studied   here
McGuinness, D.L.,  and  Pinheiro  da  Silva,  P.   2004.   Explaining
would also be appropriate for generating explanations; and
Answers   from   the   Seman t ic   Web :     The   In ference   Web
a  user  study  to  validate  the  usability  of  the  explanations
A p p r o a c h .     Journa l   of   Web   Seman t ics ,   1(4) ,   397-413 .
being  generated.   We  also  plan  to  consider  the  extensions
http://iw.stanford.edu
of  this  work  to  other  SVM  systems,  particularly  systems
Pinheiro  da  Silva,  P., McGuinness, D.L.,  and  Fikes, R.   A  Proof
that are learning over larger feature sets.
Markup  Language  for  Semantic  Web  Services.  Information
Systems, Volume 31, Issues 4-5, June-July 2006, pp 381-395.
Acknowledgements
PML specifications:
http://iw.stanford.edu/2006/06/pml-provenance.owl
We gratefully acknowledge funding support for  the CALO
http://iw.stanford.edu/2006/06/pml-justification.owl
project  from  the  Defense  Advanced  Research  Agency
Stumpf,  S.,  Rajaram,  V.,  Li,  L.,  Burnett,  M.,  Dietterich,  T.,
(DARPA)  through  contract  #55-300000680  to-2  R2.   We
Sullivan ,   E . ,   Drummond ,   R . ,   and   Herlocker,  J.   T o w a r d
thank Melinda Gervasio, Pauline Berry, Neil Yorke-Smith,
Harnessing User Feedback for Machine Learning.  Conference
and  Bart  Peintner  for  access  to  the  PLIANT  and  PTIME
on Intelligent User Interfaces 2007 (IUI-07) (to appear).
systems,  and  for  helpful  collaborations,  partnerships,  and
Yorke-Smith,  N.,  Peintner,  B.,  Gervasio,  M.,  and  Berry,  P.  M.
feedback  on  this  work.   We  thank  Deborah  McGuinness,
Balancing  the  Needs  of  Personalization  and  Reasoning  in  a
Michael  Wolverton,  and  Paulo  Pinheiro  da  Silva  for  the
User-Centric  Scheduling  Assistant.  Unpublished  manuscript
IW  and  PML  systems,  and  for  related  discussions  and
(under submission) 2006.
previous  work  that  helped  to  lay  the  foundation  for  this
effort.   We  thank  Mark  Gondek  for  access  to  the  CALO

5


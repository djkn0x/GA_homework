Using Combinatorial Optimization
within Max-Product Belief Propagation

John Duchi

Daniel Tarlow
Gal Elidan
Department of Computer Science
Stanford University
Stanford, CA 94305-9010
fjduchi,dtarlow,galel,kollerg@cs.stanford.edu

Daphne Koller

Abstract
In general, the problem of computing a maximum a posteriori (MAP) assignment
in a Markov random (cid:2)eld (MRF) is computationally intractable. However, in cer-
tain subclasses of MRF, an optimal or close-to-optimal assignment can be found
very ef(cid:2)ciently using combinatorial optimization algorithms: certain MRFs with
mutual exclusion constraints can be solved using bipartite matching, and MRFs
with regular potentials can be solved using minimum cut methods. However,
these solutions do not apply to the many MRFs that contain such tractable com-
ponents as sub-networks, but also other non-complying potentials. In this paper,
we present a new method, called COM PO SE, for exploiting combinatorial opti-
mization for sub-networks within the context of a max-product belief propagation
algorithm. COM PO SE uses combinatorial optimization for computing exact max-
marginals for an entire sub-network; these can then be used for inference in the
context of the network as a whole. We describe highly ef(cid:2)cient methods for com-
puting max-marginals for subnetworks corresponding both to bipartite matchings
and to regular networks. We present results on both synthetic and real networks
encoding correspondence problems between images, which involve both matching
constraints and pairwise geometric constraints. We compare to a range of current
methods, showing that the ability of COM PO SE to transmit information globally
across the network leads to improved convergence, decreased running time, and
higher-scoring assignments.

1 Introduction
Markov random (cid:2)elds (MRFs) [12] have been applied to a wide variety of real-world problems.
However, the probabilistic inference task in MRFs (cid:151) computing the posterior distribution of one or
more variables (cid:151) is tractable only in small tree-width networks, which are not often an appropriate
model in practice. Thus, one typically must resort to the use of approximate inference methods,
most commonly (in recent years) some variant of loopy belief propagation [11].
An alternative approach, whose popularity has grown in recent years, is based on the maximum a
posteriori (MAP) inference problem (cid:151) computing the single most likely assignment relative to the
distribution. Somewhat surprisingly, there are certain classes of networks where MAP inference can
be performed very ef(cid:2)ciently using combinatorial optimization algorithms, even though posterior
probability inference is intractable. So far, two main such classes of networks have been studied.
Regular (or associative) networks [18], where the potentials encode a preference for adjacent vari-
ables to take the same value, can be solved optimally or almost optimally using a minimum cut
algorithm. Conversely, matching networks, where the potentials encode a type of mutual exclusion
constraints between values of adjacent variables, can be solved using matching algorithms. These
types of networks have been shown to be applicable in a variety of applications, such as stereo re-
construction [13] and segmentation for regular networks, and image correspondence [15] or word
alignment for matching networks [19].

In many real-world applications, however, the problem formulation does not fall neatly into one of
these tractable subclasses. The problem may well have a large component that can be well-modeled
as regular or as a matching problem, but there may be additional constraints that take it outside this
restricted scope. For example, in a task of registering features between two images or 3D scans, we
may formulate the task as a matching problem, but may also want to encode constraints that enforce
the preservation of local or global geometry [1]. Unfortunately, once the network contains some
(cid:147)non-complying(cid:148) potentials, it is not clear if and how one can apply the combinatorial optimization
algorithm, even if only as a subroutine. In practice, in such networks, one often simply resorts to
applying standard inference methods, such as belief propagation. Unfortunately, belief propagation
may be far from an ideal procedure for these types of networks. In many cases, the MRF structures
associated with the tractable components are quite dense and contain many small loops, leading to
convergence problems and bad approximations. Indeed, recent empirical studies studies [17] show
that belief propagation methods perform considerably worse than min-cut-based methods when ap-
plied to a variety of (purely) regular MRFs. Thus, falling back on belief propagation methods for
these MRFs may result in poor performance.
The main contribution of this paper is a message-passing scheme for max-product inference that
can exploit combinatorial optimization algorithms for tractable subnetworks. The basic idea in our
algorithm, called COM PO SE (Combinatorial Optimization for Max-Product on Subnetworks), is that
the network can often be partitioned into a number of subnetworks whose union is equivalent to the
original distribution. If we can ef(cid:2)ciently solve the MAP problem for each of these subnetworks, we
would like to combine these results in order to (cid:2)nd an approximate MAP for the original problem.
The obvious dif(cid:2)culty is that a MAP solution, by itself, provides only a single assignment, and
one cannot simply combine different assignments. The key insight is that we can combine the
information from the different sub-networks by computing max-marginals for each one. A max-
marginal for an individual variable X is a vector that speci(cid:2)es, for each value x, the probability of
the MAP assignment in which X = x. If we have a black box that computes a max-marginal for
each variable X in a subnetwork, we can embed that black box as a subroutine in a max-product
belief propagation algorithm, without changing the algorithm’s basic properties.
In the remainder of this paper, we de(cid:2)ne the COM PO SE scheme, and show how combinatorial
algorithms for both regular networks and matching networks can be embedded in this framework.
In particular, we also describe ef(cid:2)cient combinatorial optimization algorithms for both types of
networks that can compute all the max-marginals in the network at a cost similar to that of (cid:2)nding
the single MAP assignment. We evaluate the applicability of COM PO SE on synthetic networks and
on an image registration task for scans of a cell obtained using an electron microscope, all of which
are matching problems with additional pairwise constraints. We compare COM PO SE to variants of
both max-product and sum-product belief propagation, as well as to straight matching. Our results
demonstrate that the ability of COM PO SE to transmit information globally across the network leads
to improved convergence, decreased running time, and higher-scoring assignments.
2 Markov Random Fields
In this paper, for simplicity of presentation, we restrict our discussion to pairwise Markov networks
(or Markov Random Fields) over discrete variables X = fX1 ; : : : ; XN g. We emphasize that our
results extend easily to the more general case of non-pairwise Markov networks. We denote an
assignment of values to X with x, and an assignment of a value to a single variable X i with xi . A
pairwise Markov network M is de(cid:2)ned as a graph G = (V ; E ) and set of potentials F that include
both node potentials (cid:30)i (xi ) and edge potentials (cid:30)ij (xi ; xj ). The network encodes a joint probability
i=1 (cid:30)i (xi ) Qi;j2U (cid:30)ij (xi ; xj ), de(cid:2)ning the
distribution via an unnormalized density P 0
F (x) = QN
F (x), where Z is the partition function given by Z = Px
distribution as PF (x) = 1
F (x).
0 P 0
Z P 0
There are different types of queries that one may want to compute on a Markov network. Most
common are (conditional) probability queries, where the task is to compute the marginal prob-
ability of one or more variables, possibly given some evidence. This type of inference is es-
sentially equivalent to computing the partition function, which sums up exponentially many as-
signments, a computation which is currently intractable except in networks of low tree width.
An alternative type of inference task is the is maximum a posteriori (MAP) problem (cid:151) (cid:2)nding
F (x). In the MAP problem, we can avoid computing the partition
arg maxx PF (x) = arg maxx P 0
function, so there are certain classes of networks to which the MAP assignment can be computed ef-
fectively, even though computing the partition problem can be shown to be intractable; we describe
two such important classes in Section 4.

In general, however, an exact solution to the MAP problem is also intractable. Max-product belief
propagation (MPBP) [20] is a commonly-used method for (cid:2)nding an approximate solution. In this
algorithm, each node Xi passes to its neighboring nodes Ni a message which is a vector de(cid:2)ning a
value for each value xi :

:

(cid:14)i!j (xj ) := max
xi

(cid:14)k!i (xi )3
2
(cid:30)i (xi )(cid:30)ij (xi ; xj ) Y
5
4
k2Ni(cid:0)fjg
as:
its own local belief
can compute
each variable
convergence,
At
b i (xi ) =
if such messages are passed from the
In a tree structured MRF,
(cid:14)k!i (xi ).
(cid:30)i (xi ) Qk2Ni
leaves towards a single root, the value of the message passed by X i towards the root encodes
a partial max-marginal: the entry for xi is the probability of the most likely assignment, to the
subnetwork emanating from Xi away from the root, where we force Xi = xi . At the root, we
obtain exact max-marginals for the entire joint distribution. However, applied to a network with
loops, MPBP often does not converge, even when combined with techniques such as smoothing and
asynchronous message passing, and the answers obtained can be quite approximate.
3 Composing Max-Product Inference on Subnetworks
We now describe the COM PO SE scheme for decomposing the network into hopefully more tractable
components, and allowing approximate max-product computation over the network as a whole to be
performed by iteratively computing max-product in one component and passing approximate max-
marginals to the other(s). As the unnormalized probability of an assignment in a Markov network
is a product of local potentials, we can partition the potentials in an MRF into an ensemble of k
subgraphs G1 ; : : : Gk over the same set of nodes V , associated edges E1 ; : : : ; Ek and sets of factors
F1 ; : : : ; Fk . We require that the product of the potentials in these subnetworks maintain the same
information as the original MRF. That is, if we originally have a factor (cid:30) i 2 F and associated factors
i (Xi ) = (cid:30)i (Xi ). One method of partitioning
i 2 Fk , we must have that Qk
l=1 (cid:30)(l)
(cid:30)(1)
i 2 F1 ; : : : ; (cid:30)(k)
that achieves this equality is simply to select, for each potential (cid:30) i , one subgraph in which it appears
unchanged, and set all of the other (cid:30)(l)
to be 1.
i
Even if MAP inference in the original network is intractable, it may be tractable in each of the
sub-networks in the ensemble. But how do we combine the results from MAP inference in an en-
semble of networks over the same set of variables? Our approach draws its motivation from the
MPBP algorithm, which computes messages that correspond to pseudo-max-marginals over single
variables (approximate max-marginals, that do not account for the loops in the network). We be-
gin by conceptually reformulating the ensemble as a set of networks over disjoint sets of variables
n g for l = 1; : : : ; k ; we enforce consistency of the joint assignment using a set of
1 ; : : : ; X (l)
fX (l)
(cid:147)communicator(cid:148) variables X1 ; : : : ; Xn , such that each X (l)
i must take the same value as Xi . We as-
sume that each subnetwork is associated with an algorithm that can (cid:147)read in(cid:148) pseudo-max-marginals
over the communicator variables, and compute pseudo-max-marginals over these variables.
More precisely, let (cid:14)(l)!i be the message sent from subnetwork l to Xi and (cid:14)i!(l) the opposite
message. Then we de(cid:2)ne the COM PO SE message passing scheme as follows:
PFl (x(l) ) Y
j 6=i

max
x(l) : X (l)
i =xi
(cid:14)i!(l) = Y
l0 6=l
That is, each subnetwork computes its local pseudo-max-marginals over each of the individual
variables, given, as input, the pseudo-max-marginals over the others. The separate pseudo-max-
marginals are integrated via the communicator variables. It is not dif(cid:2)cult to see that this message
passing scheme is equivalent to a particular scheduling algorithm for max-product belief propa-
gation over the ensemble of networks, assuming that the max-product computation in each of the
subnetworks is computed exactly using a black-box subroutine.
We note that this message passing scheme is somewhat related to the tree-reweighted max-
product (TRW) method of Wainwright et al. [8], where the network distribution is partitioned as
a weighted combination of trees, which also communicate pseudo-max-marginals with each other.

(1)

(2)

(cid:14)(l)!i (xi ) =

(cid:14)j!(l) (X (l)
j )

(cid:14)(l0 )!i :

4 Ef(cid:2)cient Computation of Max-Marginals
In this section, we describe two important classes of networks where the MAP problem can be
solved ef(cid:2)ciently using combinatorial algorithms: matching networks, which can be solved using
bipartite matching algorithms; and regular networks, which can be solved using (iterated application
of) minimum cut algorithms. We show how the same algorithms can be adapted, at minimal compu-
tational cost, for computing not only the single MAP assignment, but also the set of max-marginals.
This allows these algorithms to be used as one of our (cid:147)black boxes(cid:148) in the COM PO SE framework.
Bipartite matching. Many problems can be well-formulated as maximum-score (or minimum
weight) bipartite matching: We are given a graph G = (A; U ), whose nodes are partitioned into
disjoint sets A = A [ B . In G , each edge (a; b) has one endpoint in A and the other in B and
an associated score c(a; b). A bipartite matching is a subset of the edges W (cid:26) U such that each
node appears in at most one edge. The notion of a matching can be relaxed to include other types
of degree constraints, e.g., constraining certain nodes to appear in at most k edges. The score of the
matching is simply the sum of the scores of the edges in W .
The matching problem can also be formulated as an MRF, in several different ways. For example,
in the degree-1 case (each node in A is matched to one node in B ), we can have a variable X a for
each a 2 A whose possible values are all of the nodes in B . The edge scores in the matching graph
are then simply singleton potentials in the MRF, where (cid:30)a (Xa = b) = exp(c(a; b)). Unfortunately,
while the costs can be easily encoded in an MRF, the degree constraints on the matching induce
a set of pairwise mutual-exclusion potentials on all pairs of variables in the MRF, leading to a
fully connected network. Thus, standard methods for MRF inference cannot handle the networks
associated with matching problems.
Nevertheless, (cid:2)nding the maximum score bipartite matching (with any set of degree constraints)
can be accomplished easily using standard combinatorial optimization algorithms (e.g., [6]). How-
ever, we also need to (cid:2)nd all the max-marginals. Fortunately, we can adapt the standard algorithm
for (cid:2)nding a single best matching to also (cid:2)nd all of the max-marginals. A standard solution to
the max-matching problem reduces it to a max-weight (cid:3)ow problem, by introducing an additional
(cid:147)source(cid:148) node that connects to all the nodes in A, and an additional (cid:147)sink(cid:148) node that connects to all
the nodes in B ; the capacity of these edges is the degree constraint of the node (1 for a 1-to-1 match-
ing). We now run a standard max-weight (cid:3)ow algorithm, and de(cid:2)ne an edge to be in the matching
if it bears (cid:3)ow. Standard results show that, if the edge capacities are integers, then the (cid:3)ow too is
integral, so that it de(cid:2)nes a matching. Let w (cid:3) be the weight of the (cid:3)ow in the graph. A (cid:3)ow in the
graph de(cid:2)nes a residual graph, where there is an edge in the graph whose capacity is the amount of
(cid:3)ow it can carry relative to the current (cid:3)ow. Thus, for example, if the current solution carries a unit
of (cid:3)ow along a particular edge (a; b) in the original graph, the residual graph will have an edge with
a unit capacity going in the reverse direction, corresponding to the fact that we can now choose to
(cid:147)eliminate(cid:148) the (cid:3)ow from a to b. The scores in these inverse edges are also negative, corresponding
to the fact that score is lost when we reduce the (cid:3)ow.
Our goal now is to (cid:2)nd, for each pair (a; b), the score of the optimal matching where we force
this pair to be matched. If this pair is matched in the current solution, then the score is simply w (cid:3) .
Otherwise, we simply (cid:2)nd the highest scoring path from b to a in the residual graph. Any edges
on this new path from A to B will be included in the new matching; any edges from B to A were
included in the old matching, but are not in the new matching because of the augmenting path. This
path is the best way of changing the (cid:3)ow so as to force (cid:3)ow from a to b. Letting (cid:1) be the weight
of this augmenting path, the overall score of the new (cid:3)ow is w (cid:3) + (cid:1). It follows that the cost of this
path is necessarily negative, for otherwise it would have been optimal to apply it to the original (cid:3)ow,
improving its score. Thus, we can (cid:2)nd the highest-scoring path by simply negating all edge costs
and (cid:2)nding the shortest path in the graph.
Thus, to compute all of the max-marginals, we simply need to (cid:2)nd the shortest path from every
node a 2 A to every node b 2 B . We can (cid:2)nd this using the Floyd-Warshall all-pairs-shortest-paths
algorithm, which runs in O((nA + nB )3 ) time, for nA = jAj and nB = jB j; or we can run a single-
source shortest-path algorithm for each node in B , at a total cost of O(nB (cid:1) nAnB log(nAnB )). By
comparison, the cost of solving the initial (cid:3)ow problem is O(n3
A log(nA )).
Minimum Cuts. A very different class of networks that admits an ef(cid:2)cient solution is based
on the application of a minimum cut algorithm to a graph. At a high level, these networks encode
situations where adjacent variables like to take (cid:147)similar(cid:148) values. There are many variants of this
condition. The simplest variant is applied to pairwise MRFs over binary-valued random variables.

In this case, a potential is said to be regular if: (cid:30)ij (Xi = 1; Xj = 1) (cid:1) (cid:30)ij (Xi = 0; Xj = 0) (cid:21)
(cid:30)ij (Xi = 0; Xj = 1) (cid:1) (cid:30)ij (Xi = 1; Xj = 0). For MRFs with only regular potentials, the MAP
solution can be found as the minimum cut of a weighted graph constructed from the MRF [9]. This
construction can be extended in various ways (see [9] for a survey), including to the class of networks
with non-binary variables whose negative-log-probability is a convex function [5]. Moreover, for a
range of conditions on the potentials, an (cid:11)-expansion procedure [2], which iteratively applies a min-
cut to a series of graphs, can be used to (cid:2)nd a solution with guaranteed approximation error relative
to the optimal MAP assignment.
As above, a single joint assignment does not suf(cid:2)ce for our purposes. In recent work, Kohli
and Torr [7], studying the problem of con(cid:2)dence estimation in MAP problems, showed how all
of the max-marginals in a regular network can be computed using dynamic algorithms for (cid:3)ow
computations. Their method also applies to non-binary networks with convex potentials (as in [5]),
but not to networks for which (cid:11)-expansion is used to (cid:2)nd an approximate MAP assignment.
5 Experimental Results
We evaluate COM PO SE on the image correspondence problem, which is characteristic of match-
ing problems with geometric constraints. We compare both max-product tree-reparameterization
(TRMP) [8] and asynchronous max-product (AMP). The axes along which we compare all algo-
rithms are: the ability to achieve convergence, the time it takes to reach a solution, and the quality
(cid:151) log of the unnormalized likelihood (cid:151) of the solution found, in the Markov network that de(cid:2)nes
the problem. We use standard message damping of .3 for the max-product algorithms and a conver-
gence threshold of 10(cid:0)3 for all propagation algorithms. All tests were run on a 3.4 GHz Pentium 4
processor with 2GB of memory.
We focus our experiments on an image correspondence task, where the goal is to (cid:2)nd a 1-
to-1 mapping between landmarks in two images. Here, we have a set of template points S =
n g. We encode our MRF with a variable Xi
fx1 ; : : : ; xng and a set T of target points, fx0
1 ; : : : ; x0
for each marker xi in the source image, whose value corresponds to its aligned candidate x 0
j in the
target image. Our MRF contains singleton potentials (cid:30)i , which may encode both local appearance
information, so that a marker xi prefers to be aligned to a candidate x0
j in the target image whose
neighborhood looks similar to xi ’s, or a distance potential so that markers xi prefer to be aligned
to candidates x0
j in locations close to those in the source image. The MRF also contains pairwise
potentials f(cid:30)ij g that can encode dependencies between the landmark assignments. In particular, we
may want to encode geometric potentials, which enforce a preference for preservation of distance
l . Finally, as the goal is to
or orientation for pairs of markers xi ; xj and their assigned targets x0
k ; x0
(cid:2)nd a 1-to-1 mapping between landmarks in the source and target images, we also encode a set of
mutual exclusion potentials over pairs of variables, enforcing the constraint that no two markers are
k . Our task is to (cid:2)nd the MAP solution in this MRF.
assigned to the same candidate x0
Synthetic Networks. We (cid:2)rst experimented with synthetically generated networks that follow
the above form. To generate the networks, we (cid:2)rst create a source (cid:147)image(cid:148) that contains a set of tem-
plate points S = fx1 ; : : : ; xn g, chosen by uniformly sampling locations from a two-dimensional
plane. Next, the target set of points T = fx0
ng is generated by generating one point from
1 ; : : : ; x0
each template point xi , sampling from a Gaussian distribution with mean xi and a diagonal co-
variance matrix (cid:27) 2 I. As there was no true local information, the matching (or singleton) potentials
for both types of synthetic networks were generated uniformly at random on [0; 1). The ‘correct’
matching point, or the one the template variable generates, was given weight .7, ensuring that the
correct matching gets a non-negligible weight without making the correspondence too obvious. We
consider two different formulations for the geometric potentials. The (cid:2)rst utilizes a minimum span-
ning tree connecting the points in S , and the second simply a chain. In both cases, we generate
pairwise geometric potentials (cid:30)ij (Xi ; Xj ) that are Gaussian with mean (cid:22) = (xi (cid:0) xj ) and standard
deviation proportional to the Euclidean distance between x i and xj and variance (cid:27) 2 . Results for
the two constructions were similar, so, due to lack of space, we present results only for the line
networks.
Fig. 1(a) shows the cumulative percentage of convergent runs as a function of CPU time. COM -
PO SE converges signi(cid:2)cantly more often than either AMP or state-of-the-art TRMP. For TRMP, we
created one tree over all the geometric and singleton potentials to quickly pass information through
the entire graph; the rest of the trees chosen for TRMP were over a singleton potential, all the neigh-
boring mutual exclusion potentials, and pairwise potentials neighboring the singleton, allowing us to
maintain the mutual exclusion constraints during different reparameterization steps in TRMP. Since

(a)

(c)

(d)

(b)

Figure 1:
(a) Cumulative per-
centage of convergent runs ver-
sus CPU time on networks with
30 variables and sigma ranging
from 3 to 9.
(b) The effect of
changing the number of vari-
ables on the log score. Shown
is the difference between the log
score of each algorithm and the
score found by AMP. (c) Di-
rect comparison of COM PO SE
to TRMP on individual runs
from the same set of networks
as in (b), grouped by algorithm
convergence.
(d) Score of as-
signment based on intermediate
beliefs versus time for COM -
PO SE, TRMP, and matching on
100 variable networks. All al-
gorithms were allowed to run
for 5000 seconds.
sum-product algorithms are known in general to be less susceptible to oscillation than their max-
product counterparts, we also compared against sum-product asynchronous belief propagation. In
our experiments, however, sum-production BP did not achieve good scores even on runs in which it
did converge, perhaps because the distribution was fairly diffuse, leading to an averaging of diverse
solutions; we omit results for lack of space.
Fig. 1(b) shows the average difference in log scores between each algorithm’s result and the av-
erage log score of AMP as a function of the number of variables in the networks. COM PO SE clearly
outperforms the other algorithms, gaining a larger score margin as the size of the problem increases.
In the synthetic tests we ran for (b) and (c), COM PO SE achieved the best score in over 90% of
cases. This difference was greatest in more dif(cid:2)cult problems, where there is greater variance in the
locations of candidates in the target image leading to dif(cid:2)culty achieving a 1-to-1 correspondence.
In Fig. 1(c), we further examine scores from individual runs, comparing COM PO SE directly to
the strongest competitor, TRMP. COM PO SE consistently outperforms TRMP and never loses by
more than a small margin; COM PO SE often achieves scores on the order of 240 times better than
those achieved by TRMP. Interestingly, there appears not to be a strong correlation between relative
performance and whether or not the algorithms converged.
Fig. 1(d) examines the intermediate scores obtained by COM PO SE and TRMP on intermediate
assignments reached during the inference process, for large (100 variable) problems. Though COM -
PO SE does not reach convergence in messages, it quickly takes large steps to a very good score on
the large networks. TRMP also takes larger steps near the beginning, but it is less consistent and it
never achieves a score as high as COM PO SE. This indicates that COM PO SE scales better than TRMP
to larger problems. This behavior may also help to explain the results from (c), where we see that,
even when COM PO SE does not converge in messages, it still is able to achieve good scores. Overall,
these results indicate that we can use intermediate results for COM PO SE even before convergence.
Real Networks. We now consider real networks generated for the task of electron microscope
tomography: the three-dimensional reconstruction of cell and organelle structures based on a series
of images obtained at different tilt angles. The problem is to localize and track markers in images
across time, and it is a dif(cid:2)cult one; traditional methods like cross correlation and graph matching
often result in many errors. We can encode the problem, however, as an MRF, as described above.
In this case, the geometric constraints were more elaborate, and it was not clear how to construct
a good set of spanning trees. We therefore used a variant on AMP called residual max-product
(RMP) [3] that schedules messages in an informed way over the network; in this work and others,
we have found this variant to achieve better performance than TRMP on dif(cid:2)cult networks.
Fig. 2(a) shows a source set of markers in an electron tomography image; Fig. 2(b) shows the
correspondence our algorithm achieves, and Fig. 2(c) shows the correspondence that RMP achieves.
Note that, in Fig. 2(c), points from the source image are assigned to the same point in the target
image, whereas COM PO SE does not have the same failing. Of the twelve pairs of images we tested,

RMP failed to converge on 11/12 within 20 minutes, whereas COM PO SE failed to converge on only
two of the twelve. Because the network structure was dif(cid:2)cult for loopy approximate methods, we
ran experiments where we replaced mutual exclusion constraints with soft location constraints on
individual landmarks; while convergence improved, actual performance was inferior.
Fig. 2(d) shows the scores for the different methods we use to solve these problems. Using RMP
as the baseline score, we see the difference in scores for the different methods. It is clear that, though
RMP and TRMP run on a simpler network with soft mutual exclusion constraints are competitive
with, and even very slightly better than COM PO SE on simple problems, as problems become more
dif(cid:2)cult (more variance in target images), COM PO SE clearly dominates. We also compare COM PO SE
to simply (cid:2)nding the best matching of markers to candidates without any geometric information;
COM PO SE dominates this approach, never scoring worse than the matching.

(a)

(b)

Figure 2:
(a) Labeled
markers
in a
source
electron
microscope
image
(b) Candidates
COM PO SE assigns in the
target image (c) Candi-
dates RMP assigns in the
target
image (note the
Xs through incorrect or
duplicate
assignments)
(d) A score comparison
of COM PO SE, matching,
and RMP on the image
correspondences

(d)

(c)
6 Discussion
In this paper, we have presented COM PO SE, an algorithm that exploits the presence of tractable
substructures in MRFs within the context of max-product belief propagation. Motivated by the
existence of very ef(cid:2)cient algorithms to extract all max-marginals from combinatorial substructures,
we presented a variation of belief propagation methods that used the max-marginals to take large
steps in inference. We also demonstrated that COM PO SE signi(cid:2)cantly outperforms state-of-the-art
methods on different challenging synthetic and real problems.
We believe that one of the major reasons that belief propagation algorithms have dif(cid:2)culty with
the augmented matching problems described above is that the mutual exclusion constraints create a
phenomenon where small changes to local regions of the network can have strong effects on distant
parts of the network, and it is dif(cid:2)cult for belief propagation to adequately propagate information.
Some existing variants of belief propagation (such as TRMP) attempt to speed the exchange of
information across opposing sides of the network by means of intelligent message scheduling. Even
intelligently-scheduled message passing is limited, however, as messages are inherently local. If
there are oscillations across a wide diameter, due to global interactions in the network, they might
contribute signi(cid:2)cantly to poor performance by BP algorithms.
COM PO SE slices the network along a different axis, using subnetworks that are global in nature
but that do not have all of the information about any subset of variables.
If the component of
the network that is dif(cid:2)cult for belief propagation can be encoded in an ef(cid:2)cient special-purpose
subnetwork such as a matching, then we have a means of effectively propagating global information.
We conjecture that COM PO SE’s ability to globally pass information contributes both to its improved
convergence and to the better results it obtains even without convergence.
Some very recent work explores the case where a regular MRF contains terms that are not regu-
lar [14, 13], but this work is largely speci(cid:2)c to certain types of (cid:147)close-to-regular(cid:148) MRFs. It would
be interesting to compare COM PO SE and these methods on a range of networks containing regu-

lar subgraphs. Our work is also related to work trying to solve the quadratic assignment problem
(QAP) [10], a class of problems of which our generalized matching networks are a special case.
Standard algorithms for QAP include simulated annealing, tabu search, branch and bound, and ant
algorithms [16]; the latter have some of the (cid:3)avor of message passing, walking trails over the graph
representing a QAP and iteratively updating scores of different assignments to the QAP. To the best
of our knowledge, however, none of these previous methods attempts to use a combinatorial algo-
rithm as a component in a general message-passing algorithm, thereby exploiting the structure of
the pairwise constraints.
There are many interesting directions arising from this work. It would be interesting to perform a
theoretical analysis of the COM PO SE approach, perhaps providing conditions under which it is guar-
anteed to provide a certain level of approximation. A second major direction is the identi(cid:2)cation
of other tractable components within real-world MRFs that one can solve using combinatorial opti-
mization methods, or other ef(cid:2)cient approaches. For example, the constraint satisfaction community
has studied several special-purpose constraint types that can be solved more ef(cid:2)ciently than using
generic methods [4]; it would be interesting to explore whether these constraints arise within MRFs,
and, if so, whether the special-purpose procedures can be integrated into the COM PO SE framework.
Overall, we believe that real-world MRFs often contain large structured sub-parts that can be solved
ef(cid:2)ciently with special-purpose algorithms; the combination of special-purpose solvers within a
general inference scheme may allow us to solve problems that are intractable to any current method.
Acknowledgments
This research was supported by the Defense Advanced Research Projects Agency (DARPA) under
the Transfer Learning Program. We also thank David Karger for useful conversations and insights.
References
[1] D. Anguelov, D. Koller, P. Srinivasan, S. Thrun, H. Pang, and J. Davis. The correlated corre-
spondence algorithm for unsupervised registration of nonrigid surfaces. In NIPS, 2004.
[2] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. In
ICCV, 1999.
[3] G. Elidan, I. McGraw, and D. Koller. Residual belief propagation. In UAI, 2006.
[4] J. Hooker, G. Ottosson, E.S. Thorsteinsson, and H.J. Kim. A scheme for unifying optimization
and constraint satisfaction methods. In Knowledge Engineering Review, 2000.
[5] H. Ishikawa. Exact optimization for Markov random (cid:2)elds with convex priors. PAMI, 2003.
[6] J. Kleinberg and E. Tardos. Algorithm Design. Addison-Wesley, 2005.
[7] P. Kohli and P. Torr. Measuring uncertainty in graph cut solutions - ef(cid:2)ciently computing
min-marginal energies using dynamic graph cuts. In ECCV, 2006.
[8] V. Kolmogorov and M. Wainwright. On the optimality of tree-reweighted max-product
message-passing. In UAI ’05.
[9] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? In
ECCV, 2002.
[10] E. Lawler. The quadratic assignment problem. In Management Science, 1963.
[11] K. Murphy and Y. Weiss. Loopy belief propagation for approximate inference: An empirical
study. In UAI ’99.
[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988.
[13] A. Raj, G. Singh, and R. Zabih. MRF’s for MRI’s: Bayesian reconstruction of MR images via
graph cuts. In CVPR, 2006. To appear.
[14] C. Rother, S. Kumar, V. Kolmogorov, and A. Blake. Digital tapestry. In CVPR, 2005.
[15] J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 2000.
[16] T. St ¤utzle and M. Dorigo. ACO algorithms for the quadratic assignment problem. In New
Ideas in Optimization. 1999.
[17] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, and
C. Rother. A comparative study of energy minimization methods for Markov random (cid:2)elds.
In ECCV, 2006.
[18] B. Taskar, V. Chatalbashev, and D. Koller. Learning associative markov networks. In ICML
’04.
[19] B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction models:
a large margin approach. In ICML ’05.
[20] Y. Weiss and W. Freeman. On the optimality of solutions of the max-product belief-propagation
algorithm in arbitrary graphs. IEEE Transactions on Information Theory, 47, 2001.


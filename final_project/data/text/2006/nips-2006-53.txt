iLSTD: Eligibility Traces and Convergence Analysis

Alborz Geramifard

Michael Bowling
Richard S. Sutton
Department of Computing Science
University of Alberta
Edmonton, Alberta
{alborz,bowling,maz,sutton}@cs.ualberta.ca

Martin Zinkevich

Abstract

We present new theoretical and empirical results with the iLSTD algorithm for
policy evaluation in reinforcement learning with linear function approximation.
iLSTD is an incremental method for achieving results similar to LSTD, the data-
efﬁcient, least-squares version of temporal difference learning, without incurring
the full cost of the LSTD computation. LSTD is O(n2 ), where n is the num-
ber of parameters in the linear function approximator, while iLSTD is O(n). In
this paper, we generalize the previous iLSTD algorithm and present three new re-
sults: (1) the ﬁrst convergence proof for an iLSTD algorithm; (2) an extension to
incorporate eligibility traces without changing the asymptotic computational com-
plexity; and (3) the ﬁrst empirical results with an iLSTD algorithm for a problem
(mountain car) with feature vectors large enough (n = 10, 000) to show substan-
tial computational advantages over LSTD.

1 Introduction

A key part of many reinforcement learning algorithms is a policy evaluation process, in which the
value function of a policy is estimated online from data. In this paper, we consider the problem
of policy evaluation where the value function estimate is a linear function of state features and is
updated after each time step.
Temporal difference (TD) learning is a common approach to this problem [Sutton, 1988]. The
TD algorithm updates its value-function estimate based on the observed TD error on each time
step. The TD update takes only O(n) computation per time step, where n is the number of fea-
tures. However, because conventional TD methods do not make any later use of the time step’s
data, they may require a great deal of data to compute an accurate estimate. More recently,
LSTD [Bradtke and Barto, 1996] and its extension LSTD(λ) [Boyan, 2002] were introduced as al-
ternatives. Rather than making updates on each step to improve the estimate, these methods maintain
compact summaries of all observed state transitions and rewards and solve for the value function
which has zero expected TD error over the observed data. However, although LSTD and LSTD(λ)
make more efﬁcient use of the data, they require O(n2 ) computation per time step, which is often
impractical for the large feature sets needed in many applications. Hence, practitioners are often
faced with the dilemma of having to chose between excessive computational expense and excessive
data expense.
Recently, Geramifard and colleagues [2006] introduced an incremental least-squares TD algorithm,
iLSTD, as a compromise between the computational burden of LSTD and the relative data inefﬁ-
ciency of TD. The algorithm focuses on the common situation of large feature sets where only a
small number of features are non-zero on any given time step. iLSTD’s per-time-step computational
complexity in this case is only O(n). In empirical results on a simple problem, iLSTD exhibited a
rate of learning similar to that of LSTD.

In this paper, we substantially extend the iLSTD algorithm, generalizing it in two key ways. First,
we include the use of eligibility traces, deﬁning iLSTD(λ) consistent with the family of TD(λ) and
LSTD(λ) algorithms. We show that, under the iLSTD assumptions, the per-time-step computational
complexity of this algorithm remains linear in the number of features. Second, we generalize the
feature selection mechanism. We prove that for a general class of selection mechanisms, iLSTD(λ)
converges to the same solution as TD(λ) and LSTD(λ), for all 0 ≤ λ ≤ 1.

2 Background

Reinforcement learning is an approach to ﬁnding optimal policies in sequential decision mak-
[e.g., see Sutton and Barto, 1998]. We focus on
ing problems with an unknown environment
the class of environments known as Markov decision processes (MDPs). An MDP is a tuple,
(S , A, P a
ss0 , γ ), where S is a set of states, A is a set of actions, P a
ss0 , Ra
ss0 is the probability of
reaching state s0 after taking action a in state s, and Ra
ss0 is the reward received when that transi-
tion occurs, and γ ∈ [0, 1] is a discount rate parameter. A trajectory of experience is a sequence
s0 , a0 , r1 , s1 , a1 , r2 , s2 , . . ., where the agent in s1 takes action a1 and receives reward r2 while tran-
sitioning to s2 before taking a2 , etc.
#
" ∞X
(cid:12)(cid:12)(cid:12)(cid:12)s0 = s, π
Given a policy, one often wants to estimate the policy’s state-value function, or expected sum of
discounted future rewards:
t=1
In particular, we are interested in approximating V π using a linear function approximator. Let
φ : S → <n , be some features of the state space. Linear value functions are of the form
Vθ (s) = φ(s)T θ ,
where θ ∈ <n are the parameters of the value function. In this work we will exclusively consider
sparse feature representations: for all states s the number of non-zero features in φ(s) is no more
than k (cid:28) n. Sparse feature representations are quite common as a generic approach to handling
non-linearity [e.g., Stone et al., 2005].1

V π (s) = E

γ t−1 rt

.

.

γ i−1 rt+i

2.1 Temporal Difference Learning
 
!
TD(λ) is the traditional approach to policy evaluation [see Sutton and Barto, 1998]. It is based on
∞X
kX
t (V ), at each time step:
the computation of a λ-return, Rλ
t (V ) = (1 − λ)
λk−1
γ k V (st+k ) +
Rλ
i=1
k=1
Note that the λ-return is a weighted sum of k-step returns, each of which looks ahead k steps
summing the discounted rewards as well as the estimated value of the resulting state. The λ-return
forms the basis of the update to the value function parameters:
θ t+1 = θ t + αtφ(st ) (Rt (Vθt ) − Vθt (st )) ,
where αt is the learning rate. This “forward view” requires a complete trajectory to compute the
λ-return and update the parameters. The “backward view” is a more efﬁcient implementation that
depends only on one-step returns and an eligibility trace vector:
θ t+1 = θ t + αtut (θt )
ut (θ) = zt (rt+1 + γVθ (st+1 ) − Vθ (st ))
zt = λγ zt+1 + φ(st ),
where zt is the eligibility trace and ut (θ) is the TD update. Notice that TD(λ) requires only a
constant number of vector operations and so is O(n) per time step. In the special case where λ = 0
and the feature representation is sparse, this complexity can be reduced to O(k). In addition, TD(λ)
is guaranteed to converge [Tsitsiklis and Van Roy, 1997].
1Throughout this paper we will use non-bolded symbols to refer to scalars (e.g., γ and αt ), bold-faced
lower-case symbols to refer to vectors (e.g., θ and bt ), and bold-faced upper-case symbols for matrices (e.g.,
At ).

2.2 Least-Squares TD

zi

=

=

µt (θ) =

Least-squares TD (LSTD) was ﬁrst introduced by Bradtke and Barto [1996] and later extended with
λ-returns by Boyan [2002]. LSTD(λ) can be viewed as immediately solving for the value function
parameters which would result in the sum of TD updates over the observed trajectory being zero.
tX
tX
(cid:0)ri+1 + γVθ (si+1 ) − Vθ (si )(cid:1)
Let µt (θ) be the sum of the TD updates through time t. If we let φt = φ(st ) then,
ui (θ) =
(cid:16)
(cid:17)
tX
i=1
i=1
i+1θ − φT
ri+1 + γφT
zi
i θ
− tX
tX
i=1
zi (φi − γφi+1 )T
{z
}
|
{z
}
|
zi ri+1
i=1
i=1
At
bt
Since we want to choose parameters such that the sum of TD updates is zero, we set Equation 1 to
zero and solve for the new parameter vector,
θ t+1 = A−1
t bt .
The online version of LSTD(λ) incorporates each observed reward and state transition into the b
vector and the A matrix and then solves for a new θ . Notice that, once b and A are updated,
the experience tuple can be forgotten without losing any information. Because A only changes
by a small amount on each time step, A−1 can also be maintained incrementally. The computation
requirement is O(n2 ) per time step. Like TD(λ), LSTD(λ) is guaranteed to converge [Boyan, 2002].

θ = bt − Atθ .

(1)

2.3

iLSTD

iLSTD was recently introduced to provide a balance between LSTD’s data efﬁciency and TD’s time
efﬁciency for λ = 0 when the feature representation is sparse [Geramifard et al., 2006]. The basic
idea is to maintain the same A matrix and b vector as LSTD, but to only incrementally solve for θ .
The update to θ requires some care as the sum TD update itself would require O(n2 ). iLSTD instead
updates only single dimensions of θ , each of which requires O(n). By updating m parameters of
θ , which is a parameter that can be varied to trade off data and computational efﬁciency, iLSTD
requires O(mn + k2 ) per time step, which is linear in n. The result is that iLSTD can scale to
much larger feature spaces than LSTD, while still retaining much of its data efﬁciency. Although
the original formulation of iLSTD had no proof of convergence, it was shown in synthetic domains
to perform nearly as well as LSTD with dramatically less computation.
In the remainder of the paper, we describe a generalization, iLSTD(λ), of the original algorithm to
handle λ > 0. By also generalizing the mechanism used to select the feature parameters to update,
we additionally prove sufﬁcient conditions for convergence.

3 The New Algorithm with Eligibility Traces

The iLSTD(λ) algorithm is shown in Algorithm 1. The new algorithm is a generalization of the
original iLSTD algorithm in two key ways. First, it uses eligibility traces (z) to handle λ > 0.
Line 5 updates z, and lines 5–9 incrementally compute the same At , bt , and µt as described in
Equation 1. Second, the dimension selection mechanism has been relaxed. Any feature selection
mechanism can be employed in line 11 to select a dimension of the sum TD update vector (µ).2 Line
12 will then take a step in that dimension, and line 13 updates the µ vector accordingly. The original
iLSTD algorithm can be recovered by simply setting λ to zero and selecting features according to
the dimension of µ with maximal magnitude.
We now examine iLSTD(λ)’s computational complexity.

2The choice of this mechanism will determine the convergence properties of the algorithm, as discussed in
the next section.

Algorithm 1: iLSTD(λ)
s ← s0 , z ← 0, A ← 0, µ ← 0, t ← 0
0
Initialize θ arbitrarily
1
repeat
2
Take action according to π and observe r , s0
3
t ← t + 1
4
z ← γλz + φ(s)
5
∆b ← zr
6
∆A ← z(φ(s) − γφ(s0 ))T
7
A ← A + ∆A
8
µ ← µ + ∆b − (∆A)θ
9
for i from 1 to m do
10
j ← choose an index of µ using some feature selection mechanism
11
θj ← θj + αµj
12
µ ← µ − αµj Aej
13
end for
14
s ← s0
15
16 end repeat

Complexity

O(n)
O(n)
O(kn)
O(kn)
O(kn)

O(1)
O(n)

Theorem 1 Assume that the feature selection mechanism takes O(n) computation.
If there are
n features and, for any given state s, φ(s) has at most k non-zero elements, then the iLSTD(λ)
algorithm requires O((m + k)n) computation per time step.

Proof Outside of the inner loop,
lines 7–9 are the most computationally expensive steps of
iLSTD(λ). Since we assumed that each feature vector has at most k non-zero elements, and the
z vector can have up to n non-zero elements, the z (φ(s) − γφ(s0 ))T matrix (line 7) has at most
2kn non-zero elements. This leads to O(nk) complexity for the outside of the loop. Inside, the com-
plexity remains unchanged from iLSTD with the most expensive lines being 11 and 13. Because µ
and A do not have any speciﬁc structure, the inside loop time3 is O(n). Thus, the ﬁnal bound for
(cid:3)
the algorithm’s per-time-step computational complexity is O((m + k)n).

4 Convergence

We now consider the convergence properties of iLSTD(λ). Our analysis follows that of Bertsekas
and Tsitsiklis [1996] very closely to establish that iLSTD(λ) converges to the same solution that
TD(λ) does. However, whereas in their analysis they considered Ct and dt that had expectations
that converged quickly, we consider Ct and dt that may converge more slowly, but in value instead
of expectation.
In order to establish our result, we consider the theoretical model where for all t, yt ∈ Rn ,dt ∈ Rn ,
Rt , Ct ∈ Rn×n , βt ∈ R, and:

yt+1 = yt + βt (Rt )(Ctyt + dt ).
(2)
On every round, Ct and dt are selected ﬁrst, followed by Rt . Deﬁne Ft to be the state of the
algorithm on round t before Rt is selected. Ct and dt are sequences of random variables. In order
to prove convergence of yt , we assume that there is a C∗ , d∗ , v , µ > 0, and M such that:
A1. C∗ is negative deﬁnite,
A2. Ct converges to C∗ with probability 1,
A3. dt converges to d∗ with probability 1,
A5. limT →∞ PT
A4. E[Rt |Ft ] = I , and kRtk ≤ M ,
t=1 βt = ∞, and
A6. βt < vt−µ .
3Note that Aei selects the ith column of A and so does not require the usual quadratic time for multiplying
a vector by a square matrix.

Theorem 2 Given the above assumptions, yt converges to −(C∗ )−1d∗ with probability 1.

The proof of this theorem is included in the additional material and will be made available as a
companion technical report. Now we can map iLSTD(λ) on to this mathematical model:

1. yt = θ t ,
2. βt = tα/n,
3. Ct = −At/t,
4. dt = bt/t, and
5. Rt is a matrix, where there is an n on the diagonal in position (kt , kt ) (where kt is uniform
random over the set {1,. . . , n} and i.i.d.) and zeroes everywhere else.

The ﬁnal assumption deﬁnes the simplest possible feature selection mechanism sufﬁcient for con-
vergence, viz., uniform random selection of features.

Theorem 3 If the Markov decision process is ﬁnite, iLSTD(λ) with a uniform random feature selec-
tion mechanism converges to the same result as TD(λ).

Although this result is for uniform random selection, note that Theorem 2 outlines a broad range
of possible mechanisms sufﬁcient for convergence. However, the greedy selection of the original
iLSTD algorithm does not meet these conditions, and so has no guarantee of convergence. As
we will see in the next section, though, greedy selection performs quite well despite this lack of
asymptotic guarantee. In summary, ﬁnding a good feature selection mechanism remains an open
research question.
As a ﬁnal aside, one can go beyond iLSTD(λ) and consider the case where Rt = I, i.e., we take
a step in all directions at once on every round. This does not correspond to any feature selection
mechanism and in fact requires O(n2 ) computation. However, we can examine this algorithm’s rate
of convergence. In particular we ﬁnd it converges linearly fast to LSTD(λ).
an ζ ∈ (0, 1) such that for all yt , if yt+1 = yt + β (Ctyt + dt ), then (cid:13)(cid:13)yt+1 + (Ct )−1dt
(cid:13)(cid:13) <
ζ (cid:13)(cid:13)yt + (Ct )−1dt
(cid:13)(cid:13).
Theorem 4 If Ct is negative deﬁnite, for some β dependent upon Ct , if Rt = I, then there exists
This may explain why iLSTD(λ)’s performance, despite only updating a single dimension, ap-
proaches LSTD(λ) so quickly in the experimental results in the next section.

5 Empirical Results

We now examine the empirical performance of iLSTD(λ). We ﬁrst consider the simple problem
introduced by Boyan [2002] and on which the original iLSTD was evaluated. We then explore the
larger mountain car problem with a tile coding function approximator. In both problems, we compare
TD(λ), LSTD(λ), and two variants of iLSTD(λ). We evaluate both the random feature selection
mechanism (“iLSTD-random”), which is guaranteed to converge,4 as well as the original iLSTD
feature selection rule (“iLSTD-greedy”), which is not. In both cases, the number of dimensions
picked per iteration is m = 1. The step size (α) used for both iLSTD(λ) and TD(λ) was of the same
form as in Boyan’s experiments, with a slightly faster decay rate in order to make it consistent with
the proof ’s assumption.

N0 + 1
N0 + Episode#1.1
For the TD(λ) and iLSTD(λ) algorithms, the best α0 and N0 have been selected through experimen-
tal search of the sets of α0 ∈ {0.01, 0.1, 1} and N0 ∈ {100, 100, 106 } for each domain and λ value,
which is also consistent with Boyan’s original experiments.

αt = α0

4When selecting features randomly we exclude dimensions with zero sum TD update. To be consistent with
the assumptions of Theorem 2, we compensate by multiplying the learning rate αt by the fraction of features
that are non-zero at time t.

(a)

(b)

Figure 1: The two experimental domains: (a) Boyan’s chain example and (b) mountain car.

Figure 2: Performance of various algorithms in Boyan’s chain problem with 6 different lambda
values. Each line represents the averaged error over last 100 episodes after 100, 200, and 1000
episodes respectively. Results are also averaged over 30 trials.

5.1 Boyan Chain Problem

The ﬁrst domain we consider is the Boyan chain problem. Figure 1(a) shows the Markov chain
together with the feature vectors corresponding to each state. This is an episodic task where the
discount factor γ is one. The chain starts in state 13 and ﬁnishes in state 0. For all states s > 2, there
exists an equal probability of ending up in (s − 1) and (s − 2). The reward is -3 for all transitions
except from state 2 to 1 and state 1 to 0, where the rewards are -2 and 0, respectively.
Figure 2 shows the comparative results. The horizontal axis corresponds to different λ values,
while the vertical axis illustrates the RMS error in a log scale averaged over all states uniformly.
Note that in this domain, the optimum solution is in the space spanned by the feature vectors:
∗ = (−24, −16, −8, 0)T . Each line shows the averaged error over last 100 episodes after 100,
θ
200, and 1000 episodes over the same set of observed trajectories based on 30 trials. As expected,
LSTD(λ) requires the least amount of data, obtaining a low average error after only 100 episodes.
With only 200 episodes, though, the iLSTD(λ) methods are performing as well as LSTD(λ), and
dramatically outperforming TD(λ). Finally, notice that iLSTD-Greedy(λ) despite its lack of asymp-
totic guarantee, is actually performing slightly better than iLSTD-Random(λ) for all cases of λ.
Although λ did not play a signiﬁcant role for LSTD(λ) which matches the observation of Boyan
[Boyan, 1999], λ > 0 does show an improvement in performance for the iLSTD(λ) methods.
Table 1 shows the total averaged per-step CPU time for each method. For all methods sparse ma-
trix optimizations were utilized and LSTD used the efﬁcient incremental inverse implementation.
Although TD(λ) is the fastest method, the overall difference between the timings in this domain is
k . In the next domain, we
very small, which is due to the small number of features and a small ratio n
illustrate the effect of a larger and more interesting feature space where this ratio is larger.

-30-2-3-3-3-3-3-3-3-3-310234513 1 0 0 000.25.75 0 0 0 1 0    0 0 000.5.500.75.250010Goal00.50.70.80.9110−210−1100!RMS error of V(s) over all statesTDiLSTD−RandomiLSTD−GreedyLSTDCPU time/step (msec)
Algorithm Boyan’s chain Mountain car
0.305±7.0e-4
5.35±3.5e-3
TD(λ)
9.80±2.8e-1
0.370±7.0e-4
iLSTD(λ)
0.367±7.0e-4
LSTD(λ)
253.42

Table 1: The averaged CPU time per step of the algorithms used in Boyan’s chain and mountain car problems.

Figure 3: Performance of various methods in mountain car problem with two different lambda
values. LSTD was run only every 100 episodes. Results are averaged over 30 trials.

5.2 Mountain Car

Our second test-bed is the mountain car domain [e.g., see Sutton and Barto, 1998]. Illustrated in
Figure 1(b), the episodic task for the car is to reach the goal state. Possible actions are accelerate
forward, accelerate backward, and coast. The observation is a pair of continuous values: position
and velocity. The initial value of the state was -1 for position and 0 for velocity. Further details
about the mountain car problem are available online [RL Library, 2006]. As we are focusing on
policy evaluation, the policy was ﬁxed for the car to always accelerate in the direction of its current
velocity, although the environment is stochastic and the chosen action is replaced with a random
one with 10% probability. Tile Coding [e.g., see Sutton, 1996] was selected as our linear function
approximator. We used ten tilings (k = 10) over the combination of the two parameter sets and
hashed the tilings into 10,000 features (n = 10, 000). The rest of the settings were identical to those
in the Boyan chain domain.
Figure 3 shows the results of the different methods on this problem with two different λ values. The
horizontal axis shows the number of episodes, while the vertical axis represents our loss function in
log scale. The loss we used was ||bλ − Aλθ ||2 , where Aλ and bλ were computed for each λ from
200,000 episodes of interaction with the environment.
With λ = 0, both iLSTD(λ) methods performed considerably better than TD(λ) in terms of data ef-
ﬁciency. The iLSTD(λ) methods even reached a level competitive with LSTD(λ) after 600 episodes.
For λ = 0.9, it proved to be difﬁcult to ﬁnd stable learning rate parameters for iLSTD-Greedy(λ).
While some iterations performed competitively with LSTD(λ), others performed extremely poorly
with little show of convergence. Hence, we did not include the performance line in the ﬁgure. This
fact may suggest that the greedy feature selection mechanism does not converge, or it may simply
be more sensitive to the learning rate. Finally, notice that the plotted loss depends upon λ, and so
the two graphs cannot be directly compared.
10 = 1000), which translates into a dramatic im-
k is relatively large ( 10,000
In this environment the n
provement of iLSTD(λ) over LSTD as can be see in Table 1. Again sparse matrix optimizations
were utilized and LSTD(λ) used the efﬁcient incremental ivnerse implementation. The computa-
tional demands of LSTD(λ) can easily prohibit its application in domains with a large feature space.
When the feature representation is sparse, though, iLSTD(λ) can still achieve results competitive
with LSTD(λ) using computation more on par with the time efﬁcient TD(λ).

02004006008001000102103104105EpisodeLoss Function! = 0TDiLSTD-RandomiLSTD-GreedyLSTDSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmann02004006008001000102103104105EpisodeLoss Function! = 0TDiLSTD-RandomiLSTD-GreedyLSTDSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmannSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmannSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmann02004006008001000103104105EpisodeLoss Function! = 0.9 TDiLSTD-RandomLSTDSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmannSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmannSmallMediumLargeEasyHard10−210−1100101102103104105106107108          Boyan Chain                         Mountain CarError MeasureRandomGreedy!−GreedyBoltzmann02004006008001000102103104105EpisodeLoss Function! = 0TDiLSTD-RandomiLSTD-GreedyLSTD6 Conclusion

In this paper, we extended the previous iLSTD algorithm by incorporating eligibility traces without
increasing the asymptotic per time-step complexity. This extension resulted in improvements in
performance in both the Boyan chain and mountain car domains. We also relaxed the dimension
selection mechanism of the algorithm and presented sufﬁcient conditions on the mechanism under
which iLSTD(λ) is guaranteed to converge. Our empirical results showed that while LSTD(λ) can
be impractical in on-line learning tasks with a large number of features, iLSTD(λ) still scales well
while having similar performance to LSTD.
This work opens up a number of interesting directions for future study. Our results have focused on
two very simple feature selection mechanisms: random and greedy. Although the greedy mechanism
does not meet our sufﬁcient conditions for convergence, it actually performed slightly better on
the examined domains than the theoretically guaranteed random selection. It would be interesting
to perform a thorough exploration of possible mechanisms to ﬁnd a mechanism with both good
empirical performance while satisfying our sufﬁcient conditions for convergence. In addition, it
would be interesting to apply iLSTD(λ) in even more challenging environments where the large
number of features has completely prevented the least-squares approach, such as in simulated soccer
keepaway [Stone et al., 2005].

References
[Bertsekas and Tsitsiklis, 1996] Dmitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Pro-
gramming. Athena Scientiﬁc, 1996.
[Boyan, 1999] Justin A. Boyan. Least-squares temporal difference learning. In Proceedings of the
Sixteenth International Conference on Machine Learning, pages 49–56. Morgan Kaufmann, San
Francisco, CA, 1999.
[Boyan, 2002] Justin A. Boyan. Technical update: Least-squares temporal difference learning. Ma-
chine Learning, 49:233–246, 2002.
[Bradtke and Barto, 1996] S. Bradtke and A. Barto. Linear least-squares algorithms for temporal
difference learning. Machine Learning, 22:33–57, 1996.
[Geramifard et al., 2006] Alborz Geramifard, Michael Bowling, and Richard S. Sutton. Incremental
least-squares temporal difference learning. In Proceedings of the Twenty-First National Confer-
ence on Artiﬁcial Intelligence (AAAI), pages 356–361. AAAI Press, 2006.
[RL Library, 2006] RL Library. The University of Alberta reinforcement learning library. http:
//rlai.cs.ualberta.ca/RLR/environment.html, 2006.
[Stone et al., 2005] Peter Stone, Richard S. Sutton, and Gregory Kuhlmann. Reinforcement learn-
ing for robocup soccer keepaway. International Society for Adaptive Behavior, 13(3):165–188,
2005.
[Sutton and Barto, 1998] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.
MIT Press, 1998.
[Sutton, 1988] Richard S. Sutton. Learning to predict by the methods of temporal differences.
Machine Learning, 3:9–44, 1988.
[Sutton, 1996] Richard S. Sutton. Generalization in reinforcement learning: Successful examples
In Advances in Neural Information Processing Systems 8, pages
using sparse coarse coding.
1038–1044. The MIT Press, 1996.
[Tsitsiklis and Van Roy, 1997] John N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-
IEEE Transactions on Automatic Control,
difference learning with function approximation.
42(5):674–690, 1997.


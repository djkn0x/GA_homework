LearningfromMultipleSourcesKobyCrammer,MichaelKearns,JenniferWortmanDepartmentofComputerandInformationScienceUniversityofPennsylvaniaPhiladelphia,PA19104AbstractWeconsidertheproblemoflearningaccuratemodelsfrommultiplesourcesof“nearby”data.Givendistinctsamplesfrommultipledatasourcesandestimatesofthedissimilaritiesbetweenthesesources,weprovideageneraltheoryofwhichsamplesshouldbeusedtolearnmodelsforeachsource.Thistheoryisapplicableinabroaddecision-theoreticlearningframework,andyieldsresultsforclassiﬁ-cationandregressiongenerally,andfordensityestimationwithintheexponentialfamily.Akeycomponentofourapproachisthedevelopmentofapproximatetriangleinequalitiesforexpectedloss,whichmaybeofindependentinterest.1IntroductionWeintroduceandanalyzeatheoreticalmodelfortheproblemoflearningfrommultiplesourcesof“nearby”data.Asahypotheticalexampleofwheresuchproblemsmightarise,considerthefollow-ingscenario:Foreachwebuserinalargepopulation,wewishtolearnaclassiﬁerforwhatsitesthatuserislikelytoﬁnd“interesting.”Assumingwehaveatleastasmallamountoflabeleddataforeachuser(asmightbeobtainedeitherthroughdirectfeedback,orviaindirectmeanssuchasclick-throughsfollowingasearch),oneapproachwouldbetoapplystandardlearningalgorithmstoeachuser’sdatainisolation.However,iftherearenaturalandaccessiblemeasuresofsimilaritybetweentheinterestsofpairsofusers(asmightbeobtainedthroughtheirmutuallabelingsofcommonwebsites),anappealingalternativeistoaggregatethedataof“nearby”userswhenlearningaclassiﬁerforeachparticularuser.Thisalternativeisintuitivelysubjecttoatrade-offbetweentheincreasedsamplesizeandhowdifferenttheaggregatedusersare.Wetreatthisprobleminsomegeneralityandprovideaboundaddressingtheaforementionedtrade-off.InourmodelthereareKunknowndatasources,withsourceigeneratingadistinctsampleSiofniobservations.WeassumewearegivenonlythesamplesSi,andadisparity1matrixDwhoseentryD(i,j)boundsthedifferencebetweensourceiandsourcej.Giventheseinputs,wewishtodecidewhichsubsetofthesamplesSjwillresultinthebestmodelforeachsourcei.Ourframe-workincludessettingsinwhichthesourcesproducedataforclassiﬁcation,regression,anddensityestimation(andmoregenerallyanyadditive-losslearningproblemobeyingcertainconditions).Ourmainresultisageneraltheoremestablishingaboundontheexpectedlossincurredbyusingalldatasourceswithinagivendisparityofthetargetsource.Optimizationofthisboundthenyieldsarecommendedsubsetofthedatatobeusedinlearningamodelofeachsource.Ourboundclearlyexpressesatrade-offbetweenthreequantities:thesamplesizeused(whichincreasesasweincludedatafrommoredistantmodels),aweightedaverageofthedisparitiesofthesourceswhosedataisused,andamodelcomplexityterm.Itcanbeappliedtoanylearningsettinginwhichtheunderlyinglossfunctionobeysanapproximatetriangleinequality,andinwhichtheclassofhypothesismod-elsunderconsiderationobeysuniformconvergenceofempiricalestimatesoflosstoexpectations.1Weavoidusingthetermdistancesinceourresultsincludesettingsinwhichtheunderlyinglossmeasuresmaynotbeformaldistances.Forclassiﬁcationproblems,thestandardtriangleinequalityholds.Forregressionweprovea2-approximationtothetriangleinequality,andfordensityestimationformembersoftheexponentialfamily,weapplyBregmandivergencetechniquestoprovideapproximatetriangleinequalities.Webelievetheseapproximationsmayﬁndindependentapplicationswithinmachinelearning.Uniformconvergenceboundsforthesettingsweconsidermaybeobtainedviastandarddata-independentmodelcomplexitymeasuressuchasVCdimensionandpseudo-dimension,orviamorerecentdata-dependentapproachessuchasRademachercomplexity.Theresearchdescribedheregrewoutofanearlierpaperbythesameauthors[1]whichexaminedtheconsiderablymorelimitedproblemoflearningamodelwhenalldatasourcesarecorruptedversionsofasingle,ﬁxedsource,forinstancewheneachdatasourceprovidesnoisysamplesofaﬁxedbinaryfunction,butwithvaryinglevelsofnoise.Inthecurrentwork,eachsourcemaybeentirelyunrelatedtoallothersexceptasconstrainedbytheboundsondisparities,requiringustodevelopnewtechniques.WuandDietterichstudiedsimilarproblemsexperimentallyinthecontextofSVMs[2].Theframeworkexaminedherecanalsobeviewedasatypeoftransferlearning[3,4].InSection2weintroduceadecision-theoreticframeworkforprobabilisticlearningthatincludesclassiﬁcation,regression,densityestimationandmanyothersettingsasspecialcases,andthengiveourmultiplesourcegeneralizationofthismodel.InSection3weprovideourmainresult,whichisageneralboundontheexpectedlossincurredbyusingalldatawithinagivendisparityofatargetsource.Section4thenappliesthisboundtoavarietyofspeciﬁclearningproblems.InSection5webrieﬂyexaminedata-dependentapplicationsofourgeneraltheoryusingRademachercomplexity.2LearningmodelsBeforedetailingourmultiple-sourcelearningmodel,weﬁrstintroduceastandarddecision-theoreticlearningframeworkinwhichourgoalistoﬁndamodelminimizingageneralizednotionofempiricalloss[5].LetthehypothesisclassHbeasetofmodels(whichmightbeclassiﬁers,real-valuedfunctions,densities,etc.),andletfbethetargetmodel,whichmayormaynotlieintheclassH.Letzbea(generalized)datapointorobservation.Forinstance,in(noise-free)classiﬁcationandregression,zwillconsistofapairhx,yiwherey=f(x).Indensityestimation,zistheobservedvaluex.WeassumethatthetargetmodelfinducessomeunderlyingdistributionPfoverobservationsz.Inthecaseofclassiﬁcationorregression,PfisinducedbydrawingtheinputsxaccordingtosomeunderlyingdistributionP,andthensettingy=f(x)(possiblycorruptedbynoise).InthecaseofdensityestimationfsimplydeﬁnesadistributionPfoverobservationsx.EachsettingweconsiderhasanassociatedlossfunctionL(h,z).Forexample,inclassiﬁcationwetypicallyconsiderthe0/1loss:L(h,hx,yi)=0ifh(x)=y,and1otherwise.InregressionwemightconsiderthesquaredlossfunctionL(h,hx,yi)=(y−h(x))2.IndensityestimationwemightconsidertheloglossL(h,x)=log(1/h(x)).Ineachcase,weareinterestedintheexpectedlossofamodelg2ontargetg1,e(g1,g2)=Ez∼Pg1[L(g2,z)].Expectedlossisnotnecessarilysymmetric.Inourmultiplesourcemodel,wearepresentedwithKdistinctsamplesorpilesofdataS1,...,SK,andasymmetricK×KmatrixD.EachpileSicontainsniobservationsthataregeneratedfromaﬁxedandunknownmodelfi,andDsatisﬁese(fi,fj),e(fj,fi)≤D(i,j).2OurgoalistodecidewhichpilesSjtouseinordertolearnthebestapproximation(intermsofexpectedloss)toeachfi.Whileweareinterestedinaccomplishingthisgoalforeachfi,itsufﬁcesandisconvenienttoexaminetheproblemfromtheperspectiveofaﬁxedfi.ThuswithoutlossofgeneralityletussupposethatwearegivenpilesS1,...,SKofsizen1,...,nKfrommodelsf1,...,fKsuchthatǫ1≡D(1,1)≤ǫ2≡D(1,2)≤···≤ǫK≡D(1,K),andourgoalistolearnf1.Herewehavesimplytakentheproblemintheprecedingparagraph,focusedontheproblemforf1,andreorderedtheothermodelsaccordingtotheirproximitytof1.Tohighlightthedistinguishedroleofthetargetf1weshalldenoteitf.WedenotetheobservationsinSjbyzj1,...,zjnj.Inallcaseswewillanalyze,foranyk≤K,thehypothesisˆhkminimizingtheempiricallossˆek(h)ontheﬁrstkpilesS1,...,Sk,i.e.2WhileitmayseemrestrictivetoassumethatDisgiven,noticethatD(i,j)canbeoftenbeestimatedfromdata,forexampleinaclassiﬁcationsettinginwhichcommoninstanceslabeledbybothfiandfjareavailable.ˆhk=argminh∈Hˆek(h)=argminh∈H1n1:kkXj=1njXi=1L(h,zji)wheren1:k=n1+···+nk.Wealsodenotetheexpectederroroffunctionhwithrespecttotheﬁrstkpilesofdataasek(h)=E[ˆek(h)]=kXi=1(cid:18)nin1:k(cid:19)e(fi,h).3GeneraltheoryInthissectionweprovidetheﬁrstofourmainresults:ageneralboundontheexpectedlossofthemodelminimizingtheempiricallossonthenearestkpiles.Optimizationofthisboundleadstoarecommendednumberofpilestoincorporatewhenlearningf=f1.Thekeyingredientsneededtoapplythisboundareanapproximatetriangleinequalityandauniformconvergencebound,whichwedeﬁnebelow.Inthesubsequentsectionswedemonstratethattheseingredientscanindeedbeprovidedforavarietyofnaturallearningproblems.Deﬁnition1Forα≥1,wesaythattheα-triangleinequalityholdsforaclassofmodelsFandexpectedlossfunctioneifforallg1,g2,g3∈Fwehavee(g1,g2)≤α(e(g1,g3)+e(g3,g2)).Theparameterα≥1isaconstantthatdependsonFande.Thechoiceα=1yieldsthestandardtriangleinequality.WenotethattherestrictiontomodelsintheclassFmayinsomecasesbequiteweak—forinstance,whenFisallpossibleclassiﬁersorreal-valuedfunctionswithboundedrange—orstronger,asindensitiesfromtheexponentialfamily.Ourresultswillrequireonlythattheunknownsourcemodelsf1,...,fKlieinF,evenwhenourhypothesismodelsarechosenfromsomepossiblymuchmorerestrictedclassH⊆F.FornowwesimplyleaveFasaparameterofthedeﬁnition.Deﬁnition2AuniformconvergenceboundforahypothesisspaceHandlossfunctionLisaboundthatstatesthatforany0<δ<1,withprobabilityatleast1−δforanyh∈H|ˆe(h)−e(h)|≤β(n,δ)whereˆe(h)=1nPni=1L(h,zi)fornobservationsz1,...,zngeneratedindependentlyaccordingtodistributionsP1,...Pn,ande(h)=E[ˆe(h)]wheretheexpectationistakenoverz1,...,zn.βisafunctionofthenumberofobservationsnandtheconﬁdenceδ,anddependsonHandL.ThisdeﬁnitionsimplyassertsthatforeverymodelinH,itsempiricallossonasampleofsizenandtheexpectationofthislosswillbe“close.”Ingeneralthefunctionβwillincorporatestan-dardmeasuresofthecomplexityofH,andwillbeadecreasingfunctionofthesamplesizen,asintheclassicalO(pd/n)boundsofVCtheory.Ourboundswillbederivedfromtherichlitera-tureonuniformconvergence.Theonlytwisttooursettingisthefactthattheobservationsarenolongernecessarilyidenticallydistributed,sincetheyaregeneratedfrommultiplesources.However,generalizingthestandarduniformconvergenceresultstothissettingisstraightforward.Wearenowreadytopresentourgeneralbound.Theorem1LetebetheexpectedlossfunctionforlossL,andletFbeaclassofmodelsforwhichtheα-triangleinequalityholdswithrespecttoe.LetH⊆FbeaclassofhypothesismodelsforwhichthereisauniformconvergenceboundβforL.LetK∈N,f=f1,f2,...,fK∈F,{ǫi}Ki=1,{ni}Ki=1,andˆhkbeasdeﬁnedabove.Foranyδsuchthat0<δ<1,withprobabilityatleast1−δ,foranyk∈{1,...,K}e(f,ˆhk)≤(α+α2)kXi=1(cid:18)nin1:k(cid:19)ǫi+2αβ(n1:k,δ/2K)+α2minh∈H{e(f,h)}Beforeprovidingtheproof,letusexaminetheboundofTheorem1,whichexpressesanaturalandintuitivetrade-off.Theﬁrsttermintheboundisaweightedsumofthedisparitiesofthek≤Kmodelswhosedataisusedwithrespecttothetargetmodelf=f1.Weexpectthistermtoincreaseasweincreasektoincludemoredistantpiles.Thesecondtermisdeterminedbytheuniformconvergencebound.Weexpectthistermtodecreasewithaddedpilesduetotheincreasedsamplesize.Theﬁnaltermiswhatistypicallycalledtheapproximationerror—theresiduallossthatweincursimplybylimitingourhypothesismodeltofallintherestrictedclassH.Allthreetermsareinﬂuencedbythestrengthoftheapproximatetriangleinequalitythatwehave,asquantiﬁedbyα.TheboundsgiveninTheorem1canbeloose,butprovideanupperboundnecessaryforoptimizationandsuggestanaturalchoiceforthenumberofpilesk∗tousetoestimatethetargetf:k∗=argmink (α+α2)kXi=1(cid:18)nin1:k(cid:19)ǫi+2αβ(n1:k,δ/2K)!.Theorem1andthisoptimizationmaketheimplicitassumptionthatthebestsubsetofpilestousewillbeapreﬁxofthepiles—thatis,thatweshouldnot“skip”anearbypileinfavorofmoredistantones.Thisassumptionwillgenerallybetruefortypicaldata-independentuniformconvergencesuchasVCdimensionbounds,andtrueonaveragefordata-dependentbounds,whereweexpectuniformconvergenceboundstoimprovewithincreasedsamplesize.WenowgivetheproofofTheorem1.Proof:(Theorem1)ByDeﬁnition1,foranyh∈H,anyk∈{1,...K},andanyi∈{1,...,k},(cid:18)nin1:k(cid:19)e(f,h)≤(cid:18)nin1:k(cid:19)(αe(f,fi)+αe(fi,h))Summingoveralli∈{1,...,k},weﬁnde(f,h)≤kXi=1(cid:18)nin1:k(cid:19)(αe(f,fi)+αe(fi,h))=αkXi=1(cid:18)nin1:k(cid:19)e(f,fi)+αkXi=1(cid:18)nin1:k(cid:19)e(fi,h)≤αkXi=1(cid:18)nin1:k(cid:19)ǫi+αek(h)Intheﬁrstlineabovewehaveusedtheα-triangleinequalitytodeliberatelyintroduceaweightedsummationinvolvingthefi.Inthesecondline,wehavebrokenupthesummation.Noticethattheﬁrstsummationisaweightedaverageoftheexpectedlossofeachfi,whilethesecondsummationistheexpectedlossofhonthedata.Usingtheuniformconvergencebound,wemayassertthatwithhighprobabilityek(h)≤ˆek(h)+β(n1:k,δ/2K),andwithhighprobabilityˆek(ˆhk)=minh∈H{ˆek(h)}≤minh∈H(kXi=1(cid:18)nin1:k(cid:19)e(fi,h)+β(n1:k,δ/2K))Puttingthesepiecestogether,weﬁndthatwithhighprobabilitye(f,ˆhk)≤αkXi=1(cid:18)nin1:k(cid:19)ǫi+2αβ(n1:k,δ/2K)+αminh∈H(kXi=1(cid:18)nin1:k(cid:19)e(fi,h))≤αkXi=1(cid:18)nin1:k(cid:19)ǫi+2αβ(n1:k,δ/2K)+αminh∈H(kXi=1(cid:18)nin1:k(cid:19)αe(fi,f)+kXi=1(cid:18)nin1:k(cid:19)αe(f,h))=(α+α2)kXi=1(cid:18)nin1:k(cid:19)ǫi+2αβ(n1:k,δ/2K)+α2minh∈H{e(f,h)}00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91MAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATAMAX DATA00.20.40.60.81020406080100120140sample sizeFigure1:VisualdemonstrationofTheorem2.InthisproblemthereareK=100classiﬁers,eachdeﬁnedby2parametersrepresentedbyapointfiintheunitsquare,suchthattheexpecteddisagreementratebetweentwosuchclassiﬁersequalstheL1distancebetweentheirparameters.(Itiseasytocreatesimpleinputdistributionsandclassiﬁersthatgenerateexactlythisgeometry.)Wechosethe100parametervectorsfiuniformlyatrandomfromtheunitsquare(thecirclesintheleftpanel).Togeneratevaryingpilesizes,weletnidecreasewiththedistanceoffifromachosen“central”pointat(0.75,0.75)(marked“MAXDATA”intheleftpanel);theresultingpilesizesforeachmodelareshowninthebarplotintherightpanel,wheretheorigin(0,0)isinthenearcorner,(1,1)inthefarcorner,andthepilesizesclearlypeaknear(0.75,0.75).Giventhesefi,niandthepairwisedistances,theundirectedgraphontheleftincludesanedgebetweenfiandfjifandonlyifthedatafromfjisusedtolearnfiand/ortheconversewhenTheorem2isusedtooptimizethedistanceofthedataused.ThegraphsimultaneouslydisplaysthegeometryimplicitinTheorem2aswellasitsadaptivitytolocalcircumstances.Nearthecentralpointthegraphisquitesparseandtheedgesquiteshort,correspondingtothefactthatforsuchmodelswehaveenoughdirectdatathatitisnotadvantageoustoincludedatafromdistantmodels.Farfromthecentralpointthegraphbecomesdenseandtheedgeslong,aswearerequiredtoaggregatealargerneighborhoodtolearntheoptimalmodel.Inaddition,decisionsareaffectedlocallybyhowmanymodelsare“nearby”agivenmodel.4ApplicationstostandardlearningsettingsInthissectionwedemonstratetheapplicabilityofthegeneraltheorygivenbyTheorem1toseveralstandardlearningsettings.Webeginwiththemoststraightforwardapplication,classiﬁcation.4.1BinaryclassiﬁcationInbinaryclassiﬁcation,weassumethatourtargetmodelisaﬁxed,unknownandarbitraryfunctionffromsomeinputsetXto{0,1},andthatthereisaﬁxedandunknowndistributionPovertheX.NotethatthedistributionPoverinputdoesnotdependonthetargetfunctionf.Theobservationsareoftheformz=hx,yiwherey∈{0,1}.ThelossfunctionL(h,hx,yi)isdeﬁnedas0ify=h(x)and1otherwise,andthecorrespondingexpectedlossise(g1,g2)=Ehx,yi∼Pg1[L(g2,hx,yi)]=Prx∼P[g1(x)6=g2(x)].For0/1lossitiswell-knownandeasytoseethatthe(standard)1-triangleinequalityholds,andclassicalVCtheory[6]providesuswithuniformconvergence.TheconditionsofTheorem1arethuseasilysatisﬁed,yieldingthefollowing.Theorem2LetFbethesetofallfunctionsfromaninputsetXinto{0,1}andletdbetheVCdimensionofH⊆F.Letebetheexpected0/1loss.LetK∈N,f=f1,f2,...,fK∈F,{ǫi}Ki=1,{ni}Ki=1,andˆhkbeasdeﬁnedaboveinthemulti-sourcelearningmodel.Foranyδsuchthat0<δ<1,withprobabilityatleast1−δ,foranyk∈{1,...,K}e(f,ˆhk)≤2kXi=1(cid:18)nin1:k(cid:19)ǫi+minh∈H{e(f,h)}+2sdlog(2en1:k/d)+log(16K/δ)8n1:kInFigure1weprovideavisualdemonstrationofthebehaviorofTheorem1appliedtoasimpleclassiﬁcationproblem.4.2RegressionWenowturntoregressionwithsquaredloss.HereourtargetmodelfisanyfunctionfromaninputclassXintosomeboundedsubsetofR.(FrequentlywewillhaveX⊆Rd,butthisisnotrequired.)WeagainassumeaﬁxedbutunknowndistributionP(thatdoesnotdependonf)ontheinputs.Ourobservationsareoftheformz=hx,yi.OurlossfunctionisL(h,hx,yi)=(y−h(x))2,andtheexpectedlossisthuse(g1,g2)=Ehx,yi∼Pg1[L(g2,hx,yi)]=Ex∼P(cid:2)(g1(x)−g2(x))2(cid:3).Forregressionitisknownthatthestandard1-triangleinequalitydoesnothold.However,a2-triangleinequalitydoesholdandisstatedinthefollowinglemma.TheproofisgiveninAppendixA.3Lemma1Givenanythreefunctionsg1,g2,g3:X→R,aﬁxedandunknowndistributionPontheinputsX,andtheexpectedlosse(g1,g2)=Ex∼P(cid:2)(g1(x)−g2(x))2(cid:3),e(g1,g2)≤2(e(g1,g3)+e(g3,g1)).Theotherrequiredingredientisauniformconvergenceboundforregressionwithsquaredloss.ThereisarichliteratureonsuchboundsandtheircorrespondingcomplexitymeasuresforthemodelclassH,includingthefat-shatteringgeneralizationofVCdimension[7],ǫ-netsandentropy[6]andthecombinatorialandpseudo-dimensionapproachesbeautifullysurveyedin[5].Forconcretenesshereweadoptthelatterapproach,sinceitserveswellinthefollowingsectionondensityestimation.Whileadetailedexpositionofthepseudo-dimensiondim(H)ofaclassHofreal-valuedfunctionsexceedsbothourspacelimitationsandscope,itsufﬁcestosaythatitgeneralizestheVCdimensionforbinaryfunctionsandplaysasimilarroleinuniformconvergencebounds.Moreprecisely,inthesamewaythattheVCdimensionmeasuresthelargestsetofpointsonwhichasetofclassiﬁerscanexhibit“arbitrary”behavior(byachievingallpossiblelabelingsofthepoints),dim(H)measuresthelargestsetofpointsonwhichtheoutputvaluesinducedbyHare“full”or“space-ﬁlling.”(Technicallyweaskwhether{hh(x1),...,h(xd)i:h∈H}intersectsallorthantsofRdwithrespecttosomechosenorigin.)Ignoringconstantandlogarithmicfactors,uniformconvergenceboundscanbederivedinwhichthecomplexitypenaltyispdim(H)/n.AswiththeVCdimension,dim(H)isordinarilycloselyrelatedtothenumberoffreeparametersdeﬁningH.ThusforlinearfunctionsinRditisO(d)andforneuralnetworkswithWweightsitisO(W),andsoon.Carefulapplicationofpseudo-dimensionresultsfrom[5]alongwithLemma1andTheorem1yieldsthefollowing.AsketchoftheproofappearsinAppendixA.Theorem3LetFbethesetoffunctionsfromXinto[−B,B]andletdbethepseudo-dimensionofH⊆Fundersquaredloss.Letebetheexpectedsquaredloss.LetK∈N,f=f1,f2,...,fK∈F,{ǫi}Ki=1,{ni}Ki=1,andˆhkbeasdeﬁnedinthemulti-sourcelearningmodel.Assumethatn1≥d/16e.Foranyδsuchthat0<δ<1,withprobabilityatleast1−δ,foranyk∈{1,...,K}e(f,ˆhk)≤6kXi=1(cid:18)nin1:k(cid:19)ǫi+4minh∈H{e(f,h)}+128B2rdn1:k+sln(16K/δ)n1:k rln16e2n1:kd!4.3DensityestimationWeturntothemorecomplexapplicationtodensityestimation.Hereourmodelsarenolongerfunc-tions,butdensitiesP.ThelossfunctionforanobservationxistheloglossL(P,x)=log(1/P(x)).Theexpectedlossisthene(P1,P2)=Ex∼P1[L(P2,x)]=Ex∼P1[log(1/P2(x))].Aswearenotawareofanα-triangleinequalitythatholdssimultaneouslyforalldensityfunc-tions,weprovidegeneralmathematicaltoolstoderivespecializedα-triangleinequalitiesforspeciﬁcclassesofdistributions.Wefocusontheexponentialfamilyofdistributions,whichisquitegeneralandhasnicepropertieswhichallowustoderivethenecessarymachinerytoapplyTheorem1.Westartbydeﬁningtheexponentialfamilyandexplainingsomeofitsproperties.Weproceedbyde-rivinganα-triangleinequalityforKullback-Lieblerdivergenceinexponentialfamiliesthatimplies3Aversionofthispaperwiththeappendixincludedcanbefoundontheauthors’websites.anα-triangleinequalityforourexpectedlossfunction.Thisinequalityandauniformconvergenceboundbasedonpseudo-dimensionyieldageneralmethodforderivingerrorboundsinthemultiplesourcesettingwhichweillustrateusingtheexampleofmultinomialdistributions.Letx∈Xbearandomvariable,ineitheracontinuousspace(e.g.X⊆Rd)oradiscretespace(e.g.X⊆Zd).Wedeﬁnetheexponentialfamilyofdistributionsintermsofthefollowingcompo-nents.First,wehaveavectorfunctionofthesufﬁcientstatisticsneededtocomputethedistribution,denotedΨ:Rd→Rd′.AssociatedwithΨisavectorofexpectationparametersµ∈Rd′whichpa-rameterizesaparticulardistribution.NextwehaveaconvexvectorfunctionF:Rd′→R(deﬁnedbelow)whichisuniqueforeachfamilyofexponentialdistributions,andanormalizationfunctionP0(x).Usingthisnotationwedeﬁneaprobabilitydistribution(intheexpectationparameters)tobePF(x|µ)=e∇F(µ)·(Ψ(x)−µ)+F(µ)P0(x).(1)ForalldistributionsweconsideritwillholdthatEx∼PF(·|µ)[Ψ(x)]=µ.Usingthisfactandthelin-earityofexpectation,wecanderivetheKullback-Liebler(KL)divergencebetweentwodistributionsofthesamefamily(whichusethesamefunctionsFandΨ)andobtainKL(PF(x|µ1)kPF(x|µ2))=F(µ1)−[F(µ2)+∇F(µ2)·(µ1−µ2)].(2)WedeﬁnethequantityontherighttobetheBregmandivergencebetweenthetwo(parameter)vec-torsµ1andµ2,denotedBF(µ1kµ2).TheBregmandivergencemeasuresthedifferencebetweenFanditsﬁrst-orderTaylorexpansionaboutµ2evaluatedatµ1.Eq.(2)statesthattheKLdivergencebetweentwomembersoftheexponentialfamilyisequaltotheBregmandivergencebetweenthetwocorrespondingexpectationparameters.Wereferthereaderto[8]formoredetailsaboutBregmandivergencesandto[9]formoreinformationaboutexponentialfamilies.WewillusetheaboverelationbetweentheKLdivergenceforexponentialfamiliesandBregmandivergencestoderiveatriangleinequalityasrequiredbyourtheory.ThefollowinglemmashowsthatifwecanprovideatriangleinequalityfortheKLfunction,wecandosoforexpectedlogloss.Lemma2Letebetheexpectedlogloss,i.e.e(P1,P2)=Ex∼P1[log(1/P2(x))].ForanythreeprobabilitydistributionsP1,P2,andP3,ifKL(P1kP2)≤α(KL(P1kP3)+KL(P3kP2))forsomeα≥1thene(P1,P2)≤α(e(P1,P3)+e(P3,P2)).TheproofisgiveninAppendixB.ThenextlemmagivesanapproximatetriangleinequalityfortheKLdivergence.WeassumethatthereexistsaclosedsetP={µ}whichcontainsalltheparametervectors.Theproof(againseeAppendixB)usesTaylor’sTheoremtoderiveupperandlowerboundsontheBregmandivergenceandthenusesEq.(2)torelatetheseboundstotheKLdivergence.Lemma3LetP1,P2,andP3bedistributionsfromanexponentialfamilywithparametersµandfunctionF.ThenKL(P1kP2)≤α(KL(P1kP3)+KL(P3kP2))whereα=2supξ∈Pλ1(H(F(ξ)))/infξ∈Pλd′(H(F(ξ))).Hereλ1(·)andλd′(·)arethehighestandlowesteigenvaluesofagivenmatrix,andH(·)istheHessianmatrix.Thefollowingtheorem,whichstatesboundsformultinomialdistributionsinthemulti-sourceset-ting,isprovidedtoillustratethetypeofresultsthatcanbeobtainedusingthemachinerydescribedinthissection.MoredetailsontheapplicationtothemultinomialdistributionaregiveninAppendixB.Theorem4LetF≡HbethesetofmultinomialdistributionsoverNvalueswiththeprobabilityofeachvalueboundedfrombelowbyγforsomeγ>0,andletα=2/γ.Letdbethepseudo-dimensionofHunderlogloss,andletebetheexpectedlogloss.LetK∈N,f=f1,f2,...,fK∈F,{ǫi}Ki=1,4{n}Ki=1,andˆhkbeasdeﬁnedaboveinthemulti-sourcelearningmodel.Assumethatn1≥d/16e.Forany0<δ<1,withprobabilityatleast1−δforanyk∈{1,...,K},e(f,ˆhk)≤(α+α2)kXi=1(cid:18)nin1:k(cid:19)ǫi+αminh∈H{e(f,h)}4HerewecanactuallymaketheweakerassumptionthattheǫiboundtheKLdivergencesratherthantheexpectedlogloss,whichavoidsourneedingupperboundsontheentropyofeachsourcedistribution.+128log2(cid:16)α2(cid:17)rdn1:k+sln(16K/δ)n1:k rln16e2n1:kd!5Data-dependentboundsGiventheinterestindata-dependentconvergencemethods(suchasmaximummargin,PAC-Bayes,andothers)inrecentyears,itisnaturaltoaskhowourmulti-sourcetheorycanexploitthesemodernbounds.WeexamineonespeciﬁccaseforclassiﬁcationhereusingRademachercomplexity[10,11];analogscanbederivedinasimilarmannerforotherlearningproblems.IfHisaclassoffunctionsmappingfromasetXtoR,wedeﬁnetheempiricalRademachercom-plexityofHonaﬁxedsetofobservationsx1,...,xnasˆRn(H)=E"suph∈H(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2nnXi=1σih(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x1,...,xn#wheretheexpectationistakenoverindependentuniform{±1}-valuedrandomvariablesσ1,...,σn.TheRademachercomplexityfornobservationsisthendeﬁnedasRn(H)=EhˆRn(H)iwheretheexpectationisoverx1,...,xn.WecanapplyRademacher-basedconvergenceboundstoobtainadata-dependentmulti-sourceboundforclassiﬁcation.Aproofsketchusingtechniquesandtheoremsof[10]isinAppendixC.Theorem5LetFbethesetofallfunctionsfromaninputsetXinto{-1,1}andletˆRn1:kbetheempiricalRademachercomplexityofH⊆Fontheﬁrstkpilesofdata.Letebetheexpected0/1loss.LetK∈N,f=f1,f2,...,fK∈F,{ǫi}Ki=1,{ni}Ki=1,andˆhkbeasdeﬁnedinthemulti-sourcelearningmodel.Assumethatn1≥d/16e.Foranyδsuchthat0<δ<1,withprobabilityatleast1−δ,foranyk∈{1,...,K}e(f,ˆhk)≤2kXi=1(cid:18)nin1:k(cid:19)ǫi+minh∈H{e(f,h)}+ˆRn1:k(H)+4s2ln(4K/δ)n1:kWhiletheuseofdata-dependentcomplexitymeasurescanbeexpectedtoyieldmoreaccurateboundsandthusbetterdecisionsaboutthenumberk∗ofpilestouse,itisnotwithoutitscostsincomparisontothemorestandarddata-independentapproaches.Inparticular,inprincipletheoptimizationoftheboundofTheorem5tochoosek∗mayactuallyinvolverunningthelearningalgorithmonallpossiblepreﬁxesofthepiles,sincewecannotknowthedata-dependentcomplexitytermforeachpreﬁxwithoutdoingso.Incontrast,thedata-independentboundscanbecomputedandoptimizedfork∗withoutexaminingthedataatall,andthelearningperformedonlyonceontheﬁrstk∗piles.References[1]K.Crammer,M.Kearns,andJ.Wortman.Learningfromdataofvariablequality.InNIPS18,2006.[2]P.WuandT.Dietterich.ImprovingSVMaccuracybytrainingonauxiliarydatasources.InICML,2004.[3]J.Baxter.Learninginternalrepresentations.InCOLT,1995.[4]S.Ben-David.Exploitingtaskrelatednessformultipletasklearning.InCOLT,2003.[5]D.Haussler.DecisiontheoreticgeneralizationsofthePACmodelforneuralnetandotherlearningappli-cations.InformationandComputation,1992.[6]V.N.Vapnik.StatisticalLearningTheory.Wiley,1998.[7]M.KearnsandR.Schapire.Efﬁcientdistribution-freelearningofprobabilisticconcepts.JCSS,1994.[8]Y.CensorandS.A.Zenios.ParallelOptimization:Theory,Algorithms,andApplications.OxfordUni-versityPress,NewYork,NY,USA,1997.[9]M.J.WainwrightandM.I.Jordan.Graphicalmodels,exponentialfamilies,andvariationalinference.TechnicalReport649,DepartmentofStatistics,UniversityofCalifornia,Berkeley,2003.[10]P.L.BartlettandS.Mendelson.RademacherandGaussiancomplexities:Riskboundsandstructuralresults.JournalofMachineLearningResearch,2002.[11]V.Koltchinskii.Rademacherpenaltiesandstructuralriskminimization.IEEETrans.Info.Theory,2001.
Clustering Under Prior Knowledge with Application
to Image Segmentation

M ´ario A. T. Figueiredo
Instituto de Telecomunicac¸ ˜oes
Instituto Superior T ´ecnico
Technical University of Lisbon
Portugal

Dong Seon Cheng, Vittorio Murino
Vision, Image Processing, and Sound Laboratory
Dipartimento di Informatica
University of Verona
Italy

mario. ﬁgueiredo@lx.it.pt

cheng@sci.univr.it, vittorio.murino@univr.it

Abstract

This paper proposes a new approach to model-based clustering under prior knowl-
edge. The proposed formulation can be interpreted from two different angles: as
penalized logistic regression, where the class labels are only indirectly observed
(via the probability density of each class); as ﬁnite mixtur e learning under a group-
ing prior. To estimate the parameters of the proposed model, we derive a (gener-
alized) EM algorithm with a closed-form E-step, in contrast with other recent
approaches to semi-supervised probabilistic clustering which require Gibbs sam-
pling or suboptimal shortcuts. We show that our approach is ideally suited for
image segmentation: it avoids the combinatorial nature Markov random ﬁeld pri-
ors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based)
in a simple and computationally efﬁcient way. Finally, we ex tend our formulation
to work in unsupervised, semi-supervised, or discriminative modes.

1

Introduction

Most approaches to semi-supervised learning (SSL) see the problem from one of two (dual) per-
spectives: supervised classi ﬁcation with additional unla belled data (see [20] for a recent survey);
clustering with prior information or constraints (e.g., [4, 10, 11, 15, 17]). The second perspective,
usually termed semi-supervised clustering (SSC), is usually adopted when labels are totaly absent,
but there are (usually pair-wise) relations that one wishes to enforce or encourage.

Most SSC techniques work by incorporating the constrains (or prior) into classical algorithms such
as K-means or EM for mixtures. The semi-supervision may be hard (i.e., grouping constraints
[15, 17]), or have the form of a prior under which probabilistic clustering is performed [4, 11]. The
later is clearly the most natural formulation for cases where one wishes to encourage, not enforce,
certain relations; an obvious example is image segmentation, seen as clustering under a spatial
prior, where neighboring sites should be encouraged, but not constrained, to belong to the same
cluster/segment. However, the previous EM-type algorithms for this class of methods have a major
drawback: the presence of the prior makes the E-step non-trivial, forcing the use of expensive Gibbs
sampling [11] or suboptimal methods such as the iterated conditional modes algorithm [4].

In this paper, we introduce a new approach to mixture-based SSC, leading to a simple, fully deter-
ministic, generalized EM (GEM) algorithm. The keystone is the formulation of SSC as a penalized
logistic regression problem, where the labels are only indirectly observed. The linearity of the
resulting complete log-likelihood, w.r.t.
the missing group labels, underlies the simplicity of the
resulting GEM algorithm. When applied to image segmentation, our method allows using spatial
priors which are typical of image estimation problems (e.g., restoration/denoising), such as Gaussian

ﬁelds or wavelet-based priors. Under these priors, the M-st ep of our GEM algorithm reduces to a
simple image denoising procedure, for which there are several extremely efﬁcient algorithms.

2 Formulation

p (X |Y , φ ) =

,

(1)

p(xi |φ(k) ) =

We start from the standard formulation of ﬁnite mixture mode ls: X = {x1 , ..., xn } is an observed
data set, where each xi ∈ IRd was generated (independently) according to one of a set of K prob-
ability (density or mass) functions {p(·|φ(1) ), ..., p(·|φ(K ) )}. In image segmentation, each xi is a
pixel value (gray scale, d = 1; color, d = 3) or a vector of local (e.g., texture) features. Associated
with X , there is a hidden label set Y = {y1 , ..., yn }, where yi = [y (1)
, ..., y (K )
]T ∈ {0, 1}K , with
i
i
y (k)
i = 1 if and only if xi was generated by source k (the so-called “1-of-K” binary encoding). Thus,
KYk=1 hp(xi |φ(k) )iy
nYi=1
KYk=1 Yi: y
(k)
i
(k)
i =1
where φ = (φ(1) , ..., φ(K ) ) is the set of parameters of the generative models of classes.
In standard mixture models, all the yi are assumed to be independent and identically distrib-
uted samples following a multinomial distribution with probabilities {η (1) , ..., η (K ) }, i.e., P (Y ) =
Qi Qk (η (k) )y
(k)
. This is the part of standard mixture models that has to be modi ﬁed in order to
i
insert grouping constraints [15] or a grouping prior p(Y ) [4, 11]. However, this prior destroys the
simplicity of the standard E-step for ﬁnite mixtures, which is critically based on the independence
assumption. We follow a different route to avoid that roadblock.
Let the hidden labels Y = {y1 , ..., yn } depend on a new set of variables Z = {z1 , ..., zn }, where
, ..., z (K )
each zi = [z (1)
]T ∈ IRK, following a multinomial logistic model [5]:
i
i
KYk=1 (cid:16)P [y (k)
i = 1|zi ](cid:17)y
(k)
nYi=1
ez
i
PK
(l)
l=1 ez
i
Due to the normalization, we can set (w.l.o.g.) z (K )
i = 0, for i = 1, ..., n [5]. We’re thus left with
n (K − 1) real variables, i.e., Z = {z(1) , ..., z(K−1) }, where z(k) = [z (k)
1 , ..., z (k)
n ]T ; of course, Z
can be seen as an n × (K − 1) matrix, where z(k) is the k-th column and zi is the i-th row.
With this formulation, certain grouping preferences may be expressed by a prior p(Z ). For example,
preferred pair-wise relations can be easily embodied in a Gaussian prior
)2 =
exp −
(z(k) )T ∆ z(k)(cid:21) ,
exp (cid:20)−
K−1Yk=1
K−1Yk=1
nXj=1
nXi=1
where A is a matrix (with a null diagonal) encoding pair-wise preferences (Ai,j > 0 expresses
preference, with strength proportional to Ai,j , for having points i and j in the same cluster) and ∆
is the well-known graph-Laplacian matrix [20],
j=1 An,j o − A.
∆ = diag nPn
j=1 A1,j , ..., Pn
For image segmentation, each z(k) is an image with real-valued elements and a natural choice for
A is to have Ai,j = λ, if i and j are neighbors, and zero otherwise. Assuming periodic boundary
conditions for the neighborhood system, ∆ is a block-circulant matrix with circulant blocks [2].
However, as shown below, other more sophisticated priors (such as wavelet-based priors) can also
be used at no additional computational cost [1].

P [y (k)
i = 1|zi ] =

i − z (k)
Ai,j (z (k)
j

(k)
i
,

where

(3)

(4)

p(Z ) ∝

1
4

P (Y |Z ) =

.

(2)

1
2

3 Model Estimation

3.1 Marginal Maximum A Posteriori and the GEM Algorithm

Based on the formulation presented in the previous section, SSC is performed by estimating Z
and φ, seeing Y as missing data. The marginal maximum a posteriori estimate is obtained by

p(X |Y , φ) P (Y |Z ) p(Z ),

marginalizing out the hidden labels (over all the possible label conﬁgurations),
(cid:16) bZ , bφ(cid:17) = arg max
Z ,φ XY
Z ,φ XY
p(X , Y , Z |φ) = arg max
where we’re assuming a ﬂat prior for φ. One of the key advantages of this approach is that (5) is
a continuous (not combinatorial) optimization problem. This is in contrast Markov random ﬁeld
approaches to image segmentation, which lead to hard combinatorial problems, since they perform
optimization directly with respect to the (discrete) label variables Y . Finally, notice that once in
possession of an estimate bZ , one may compute P (Y | bZ ) which gives the probability that each data
point belongs to each class. By ﬁnding arg maxk P [y (k)
i = 1|zi ], for every i, one may obtain a hard
clustering/segmentation.
We handle (5) with a generalized EM (GEM) algorithm [13], i.e., by applying the following iterative
procedure (until some convergence criterion is satis ﬁed):

(5)

E-step: Compute the conditional expectation of the complete log-posterior, given the current esti-
mates ( bZ , bφ) and the observations X :
Q(Z , φ| bZ , bφ) = EY hlog p(Y , Z , φ|X ) (cid:12)(cid:12)(cid:12) bZ , bφ, X i .
M-step: Update the estimate: ( bZ , bφ) ← ( bZnew , bφnew ), with new values such that
Q( bZnew , bφnew | bZ , bφ) ≥ Q( bZ , bφ| bZ , bφ).
Under mild conditions, it is well known that GEM algorithms converge to a local maximum of the
marginal log-posterior [18].

(6)

(7)

3.2 E-step

The complete log-posterior is

.
=

y (k)
i

i z (k)
y (k)
i − log

log p(xi |φ(k) ) +

.
= log p(X |Y , φ) + log P (Y |Z ) + log p(Z )
log p(Y , Z , φ|X )
i # + log p(Z )
nXi=1 " KXk=1
KXk=1
nXi=1
KXk=1
(k)
ez
where .
= stands for “equal up to an additive constant ”. The key observ ation is that this function is lin-
ear w.r.t. the hidden variables y (k)
. Consequently, the E-step reduces to computing their conditional
i
expectations, which are then plugged into (8).
is binary, thus its expectation (denoted by (k)
As in standard mixtures, each missing y (k)
i
i
posterior probability of being equal to one, easily obtained via Bayes law:
(k)
) P [y (k)
p(xi | bφ
i = 1|bzi ]
by (k)
| bZ , bφ, X ] = P [y (k)
i ≡ E [y (k)
i = 1|bzi , bφ, xi ] =
PK
i
(j )
) P [y (j )
j=1 p(xi | bφ
i = 1|bzi ]
Notice that this is the same as the E-step for a standard ﬁnite mixture, where the probabilities
P [y (k)
i = 1|bzi ] (given by (2)) play the role of the probabilities of the classes/components. Finally,
the Q function is obtained by plugging the expectations by (k)
into (8).
i
3.3 M-Step
It’s clear from (8) that the maximization w.r.t. φ can be performed separately w.r.t. to each φ(k) ,
nXi=1 by (k)
(k)
bφ
new = arg max
i
φ(k)
This is the familiar weighted maximum likelihood criterion, exactly as it appears in EM for standard
mixtures. The explicit form of this update depends on the choice of p(·|φ(k) ); e.g., this step can be
easily applied to any ﬁnite mixture of exponential family de nsities [3].

log p(xi |φ(k) ).

) equals its

(8)

.

(9)

(10)

In supervised image segmentation, these parameters are known (e.g., previously estimated from
training data) and thus it’s not necessary to estimate them; the M-step reduces to the estimation of
Z . In unsupervised image segmentation, φ is unknown and (10) will have to be applied.
To update the estimate of Z , we need to maximize (or at least improve, see (7))
i # + log p(Z ).
nXi=1 " KXk=1 by (k)
KXk=1
(k)
i z (k)
L(Z | bY ) ≡
ez
i − log
Without the prior, this would be a simple logistic regression (LR) problem, with an identity design
by (k)
matrix [5]; however, instead of the usual hard labels y (k)
i ∈ {0, 1}, we have “soft ” labels
i ∈ [0, 1].
Arguably, the two standard approaches to maximum likelihood LR are the Newton-Raphson al-
gorithm (a.k.a. iteratively reweighted least squares – IRLS [7]) and the minorize-maximize (MM)
approach (formerly known as bound optimization) [5, 9]. We will show below that the MM approach
can be easily modi ﬁed to accommodate the presence of a prior.

(11)

(12)

Let’s brieﬂy review the MM approach for maximizing a twice di fferentiable concave function E (θ)
with bounded Hessian [5, 9]. Let the Hessian H(θ) of E (θ) be bounded below by −B (that is,
H(θ) (cid:23) −B, in the matrix sense, meaning that H(θ) + B is positive deﬁnite), where B is a positive
deﬁnite matrix. It’s trivial to show that E (θ) − R(θ , bθ) has a minimum at θ = bθ , where
2 (cid:16)θ − bθ − B−1g(bθ)(cid:17)T
B (cid:16)θ − bθ − B−1g(bθ)(cid:17) ,
1
R(θ , bθ) = −
with g(bθ) denoting the gradient of E (θ) at bθ . Thus, the iteration
bθnew = arg max
R(θ , bθ) = bθ + B−1g(bθ)
θ
is guaranteed to monotonically improve E (θ), i.e., E (bθnew ) ≥ E (bθ).
It was shown in [5] that the gradient and the Hessian of the logistic log-likelihood function, i.e., (11)
without the log-prior, verify (with Ia denoting an a × a identity matrix and 1a a vector of a ones)
! ⊗ In ≡ −B,
2  IK−1 −
1K−1 1T
1
K−1
and H(z) (cid:23) −
(14)
g(z) = by − η(z)
K
1 , ..., z (K−1)
n , z (2)
where z = [z (1)
1 , ..., z (1)
]T denotes the lexicographic vectorization of Z , by denotes
n
the corresponding lexicographic vectorization of bY , and η(z) = [p(1)
1 , ..., p(1)
n , p(2)
1 , ..., p(K−1)
]T
n
i = P [y (k)
with p(k)
i = 1|zi ].
Deﬁning v = bz + B−1 (by − η(bz)), the MM update equation for solving (11) is thus
z (cid:26) 1
(z − v)T B (z − v) − log p(z)(cid:27) ,
bznew (v) = arg min
2
where p(z) is equivalent to p(Z ), because z is simply the lexicographic vectorization of Z .
We now summarize our GEM algorithm:
E-step: compute by, using (9), for all i = 1, ..., n and k = 1, ..., K − 1.
(Generalized) M-step: Apply one or more iterations (15), keeping by ﬁxed, that is, loop through
the following two steps: v ← bz + B−1 (by − η(bz)) and bz ← bznew (v).
3.4 Speeding Up the Algorithm
In image segmentation, the MM update equation (15) is formally equivalent to the MAP estimation
of an image with n pixels in IRK−1 , under prior p(z), where v plays the role of observed image, and
B is the inverse covariance matrix of the noise. Due to the structure of B, even if the prior models
the several z(k) as independent, i.e., if log p(z) = log p(z(1) ) + · · · + log p(z(K−1) ), (15) can not be
decoupled into the several components {z(1) , ..., z(K−1) }. We sidestep this difﬁculty, at the cost of
using a less tight bound in (14), based the following lemma:

(13)

(15)

Lemma 1 Let ξK = 1/2, if K > 2, and ξK = 1/4, if K = 2. Then, B (cid:22) ξK In(K−1) .

Proof: Inserting K = 2 in (14) yields B = I/4, which proves the case K = 2. For K > 2, the
inequality I/2 (cid:23) B is equivalent to λmin (I/2 − B) ≥ 0, which is equivalent to λmax (B) ≤ (1/2).
Since the eigenvalues of the Kronecker product are the products of the eigenvalues of the matrices,
λmax (B) = λmax (I − (1/K ) 1 1T )/2. Since 1 1T is a rank-1 matrix with eigenvalues {0, ..., 0, K −
1}, the eigenvalues of (I − (1/K ) 1 1T ) are {1, ..., 1, 1/K }, thus λmax (I − (1/K ) 1 1T ) = 1, and
λmax (B) = 1/2.

(16)

This lemma allows replacing B with ξK In(K−1) in (15) which (assuming independent priors, as is
the case of (3)) becomes decoupled, leading to
− log p(z(k) )(cid:27) ,
z(k) (cid:26) ξK
2 (cid:13)(cid:13)(cid:13)z(k) − v(k)(cid:13)(cid:13)(cid:13)
2
new (v(k) ) = arg min
bz(k)
for
k = 1, ..., K − 1,
where v(k) = bz(k) + (1/ξK )(by(k) − η (k) (bz(k) )). Moreover, the “noise” in each of these “denoising”
problems is white and Gaussian, of variance 1/ξK .
3.5 Stationary Gaussian Field Priors
Consider a Gaussian prior of form (3), where Ai,j only depends on the relative position of i and j and
the neighborhood system deﬁned by A has periodic boundary conditions. In this case, both A and ∆
are block-circulant matrices, with circulant blocks [2], thus diagonalizable by a 2D discrete Fourier
transform (2D-DFT). Formally, ∆ = UH DU, where D is diagonal, U is the orthogonal matrix
representing the 2D-DFT, and (·)H denotes conjugate transpose. The log-prior is then expressed in
.
= 1
the DFT domain, log p(z(k) )
2 (Uz(k) )H D(Uz(k) ), and the solution of (16) is
new (v(k) ) = ξK UH [ξK In + D]−1 U v(k) ,
bz(k)
(17)
for
k = 1, ..., K − 1.
Observe that (17) corresponds to ﬁltering each image v(k) , in the DFT domain, with a ﬁxed ﬁlter
with frequency response [ξK In + D]−1 ; this inversion can be computed off-line and is trivial be-
cause ξK In + D is diagonal. Finally, it’s worth stressing that the matrix-vector products by U and
UH are not carried out explicitly but more efﬁciently via the FF T algorithm, with cost O(n log n).

3.6 Wavelet-Based Priors for Segmentation

It’s known that piece-wise smooth images have sparse wavelet-based representations (see [12]
and the many references therein); this fact underlies the state-of-the-art denoising performance of
wavelet-based methods. Piece-wise smoothness of the z(k) translates into segmentations in which
pixels in each class tend to form connected regions. Consider a wavelet expansion of each z(k)
z(k) = Wθ (k) , k = 1, ..., K − 1,
(18)
where the θ (k) are sets of coefﬁcients and W is the matrix representation of an inverse wavelet
transform; W may be orthogonal or have more columns than lines (over-complete representations)
[12]. A wavelet-based prior for z(k) is induced by placing a prior on the coefﬁcients θ (k) . A classical
choice for p(θ (k) ) is a generalized Gaussian [14]. Without going into details, under this class of
priors (and others), (16) becomes a non-linear wavelet-based denoising step, which has been widely
studied in the image processing literature. For several choices of p(θ (k) ) and W, this denoising
step has a very simple closed form, which essentially corresponds to computing a wavelet transform
of the observations, applying a coefﬁcient-wise non-linea r shrinkage/thresholding operation, and
applying the inverse transform to the processed coefﬁcient s. This is computationally very efﬁcient,
due to the existence of fast algorithms for computing direct and inverse wavelet transforms; e.g.,
O(n) for an orthogonal wavelet transform or O(n log n) for a shift-invariant redundant transform.

4 Extensions

4.1 Semi-Supervised Segmentation

For semi-supervised image segmentation, the user deﬁnes re gions in the image for which the true
label is known. Our GEM algorithm is trivially modi ﬁed to han dle this case: if at location i the label

is known to be (say) k , we freeze by (k)
i = 1, and by (j )
i = 0, for j 6= k . The E-step is only applied to
those locations for which the label is unknown. The M-step remains unchanged.
4.2 Discriminative Features
Our formulation (as most probabilistic segmentation methods) adopts a generative perspective,
where each p(·|φ(k) ) models the data generation mechanism in the corresponding class. However,
discriminative methods (such as support vector machines) are seen as the current state-of-the-art in
classi ﬁcation [7]. We will now show how a pre-trained discri minative classi ﬁer can be used in our
GEM algorithm instead of the generative likelihoods.
The E-step (see (9)) obtains the posterior probability that xi was generated by the k-th model, by
(k)
combining (via Bayes law) the corresponding likelihood p(xi | bφ
) with the local prior probability
P [y (k)
i = 1|bzi ]. Consider that, instead of likelihoods derived from generative models, we have a
discriminative classi ﬁer,
i.e., one that directly provides estimates of the posterior class probabilities
P [y (k)
i = 1|xi ]. To use these values in our segmentation algorithm, we need a way to bias these
estimates according to the local prior probabilities P [y (k)
i = 1|bzi ], which are responsible for encour-
aging spatial coherence. Let us assume that we know that the discriminative classi ﬁer was trained
using mk samples from the k-th class. It can thus be assumed that these posterior class probabilities
i = 1|xi ] ∝ mk p(xi |y (k)
verify P [y (k)
i = 1). It is then possible to “bias ” these classi ﬁers, with the
local prior probabilities P [y (k)
i = 1|bzi ], simply by computing


−1
KXj=1
i = 1|xi ] P [y (k)
P [y (k)
i = 1|xi ] P [y (j )
P [y (j )
i = 1|bzi ]
i = 1|bzi ]
mk
mj
In this section we will show experimental results of image segmentation in supervised, unsupervised,
semi-supervised, and discriminative modes. Assessing the performance of a segmentation method
is not a trivial task. Moreover, the performance of segmentation algorithms depends more critically
on the adopted features (which is not the focus of this paper) than on the spatial coherence prior. For
these reasons, we will not present any careful comparative study, but simply a set of experimental
examples testifying for the promising behavior of the proposed approach.

5 Experiments

P [y (k)
i = 1|xi ] =

.

5.1 Supervised and Unsupervised Image Segmentation

The ﬁrst experiment, reported in Fig. 1, illustrates the alg orithm on a synthetic gray scale image
with four Gaussian classes of means 1, 2, 3, and 4, and standard deviation 0.6. For this image, both
supervised and unsupervised segmentation lead to almost visually indistinguishable results, so we
only show the supervised segmentation results. In the Gaussian prior, matrix A corresponds to a
ﬁrst order neighborhood, that is, Ai,j = γ if and only if j is one of the four nearest neighbors of
i. For wavelet-based segmentation, we have used undecimated Haar wavelets and the Bayes-shrink
denoising procedure [6].

Figure 1: From left to right: observed image, maximum likelihood segmentation, GEM result with
Gaussian prior, GEM result with wavelet-based prior.

5.2 Semi-supervised Image Segmentation

We illustrate the semi-supervised mode of our approach on two real RGB images, shown in Fig. 2.
Each region is modelled by a single multivariate Gaussian density in RGB space. In the example
in the ﬁrst row, the goal is to segment the image into skin, clo th, and background regions; in the
second example, the goal is to segment the horses from the background. These examples show how
the semi-supervised mode of our algorithm is able to segment the image into regions which “look
like” the seed regions provided by the user.

Figure 2: From left to right (in each row): observed image with regions indicated by the user as
belonging to each class, segmentation result, region boundaries.

5.3 Discriminative Texture Segmentation

Finally, we illustrate the behavior of the algorithm when used with discriminative classi ﬁers by
applying it to texture segmentation. We build on the work in [8], where SVM classi ﬁers are used
for texture classi ﬁcation (see [8] for complete details abo ut the kernels and texture features used).
Fig. 3 shows two experiments; one with a two-texture 256× 512 image and the other with a 5-texture
256 × 256 image. In the two-class case, one binary SVM was trained on 1000 random patterns from
each class. For the 5-class case, 5 binary SVMs were trained in the “1-vs-all ” mode, with 500
samples from each class. In the 2-class and 5-class cases, the error rates of the SVM classi ﬁer are
12.69% and 13.92%, respectively. Our GEM algorithm achieves 0.51% and 2.22%, respectively.
These examples show that our method is able to take class predictions produced by a classi ﬁer
lacking any spatial prior and produce segmentations with a high degree of spatial coherence.

6 Conclusions

We have introduced an approach to probabilistic semi-supervised clustering which is particularly
suited for image segmentation. The formulation allows supervised, unsupervised, semi-supervised,
and discriminative modes, and can be used with classical standard image priors (such as Gaussian
ﬁelds, or wavelet-based priors). Unlike the usual Markov ra ndom ﬁeld approaches, which involve
combinatorial optimization, our segmentation algorithm consists of a simple generalized EM algo-
rithm. Several experimental examples illustrated the promising behavior of our method. Ongoing
work includes a thorough experimental comparison with state-of-the-art segmentation algorithms,
namely, spectral methods [16] and techniques based on “grap h-cuts ” [19].
Acknowledgement: This work was partially supported by the (Portuguese) Fundac¸ ˜ao para a
Ci ˆencia e Tecnologia (FCT), grant POSC/EEA-SRI/61924/2004.

Figure 3: From left to right (in each row): observed image, direct SVM segmentation, segmentation
produced by our algorithm.

References

IEEE

[1] M. Figueiredo. “Bayesian image segmentation using wavelet-based p riors ”, Proc. IEEE Conf. Computer
Vision and Pattern Recognition - CVPR’2005, San Diego, CA, 2005.
[2] N. Balram, J. Moura.
: parameter structure and estimation”,
“Noncausal Gauss-Markov random ﬁelds
IEEE Trans. Information Theory, vol. 39, pp. 1333–1355, 1993.
[3] A. Banerjee, S. Merugu, I. Dhillon, J. Ghosh. “Clustering with Breg man divergences.” Proc. SIAM Intern.
Conf. Data Mining – SDM’2004 , Lake Buena Vista, FL, 2004.
[4] S. Basu, M. Bilenko, R. Mooney. “A probabilistic framework for se mi-supervised clustering.” Proc. of the
KDD-2004, Seattle, WA, 2004.
[5] D. B ¨ohning. “Multinomial logistic regression”,
Annals Inst. Stat. Math., vol. 44, pp. 197-200, 1992.
[6] G. Chang, B. Yu, M. Vetterli. “Adaptive wavelet thresholding for ima ge denoising and compression.”
Trans. Image Proc., vol. 9, pp. 1532–1546, 2000.
[7] T. Hastie, R. Tibshirani, J. Friedman. The Elements of Statistical Learning, Springer, 2001.
[8] K. I. Kim, K. Jung, S. H. Park, H. J. Kim.
“Support vector machin es for texture classiﬁcation.”
Trans. Pattern Analysis and Machine Intelligence, vol. 24, pp. 1542–1550, 2002.
[9] D. Hunter, K. Lange. “A tutorial on MM algorithms ”,
The American Statistician, vol. 58, pp. 30–37, 2004.
[10] M. Law, A. Topchy, A. K. Jain. “Model-based clustering with prob abilistic constraints.” In Proc. of the
SIAM Conf. on Data Mining, pp. 641-645, Newport Beach, CA, 2005.
[11] Z. Lu, T. Leen. “Probabilistic penalized clustering.” In NIPS 17, MIT Press, 2005.
[12] S. Mallat. A Wavelet Tour of Signal Proc.. Academic Press, San Diego, CA, 1998.
[13] G. McLachlan, T. Krishnan. The EM Algorithm and Extensions, Wiley, N. York, 1997.
“Analysis of multiresolution image denoising scheme s using generalized - Gaussian
[14] P. Moulin, J. Liu.
and complexity priors,”
IEEE Trans. Inform. Theory, vol. 45, pp. 909–919, 1999.
[15] N. Shental, A. Bar-Hillel, T. Hertz, D. Weinshall. “Computing Gaussia n mixture models with EM using
equivalence constraints.” In NIPS 15, MIT Press, Cambridge, MA, 2003.
[16] J. Shi, J. Malik, “Normalized cuts and image segmentation.”
IEEE-TPAMI, vol. 22, pp. 888–905, 2000.
[17] K. Wagstaff, C. Cardie, S. Rogers, S. Schr ¨odl. “Constrained K-means clustering with background knowl-
edge.” In Proc. of ICML’2001, Williamstown, MA, 2001.
[18] C. Wu. “On the convergence properties of the EM algorithm,”
Ann. Statistics, vol. 11, pp. 95-103, 1983.
[19] R. Zabih, V. Kolmogorov,
“Spatially coherent clustering with graph cuts.” Proc. IEEE-CVPR, vol. II,
pp. 437–444, 2004.
[20] X. Zhu. “Semi-Supervised Learning Literature Survey”, TR-15 30, Comp. Sci. Dept., Univ. of Wisconsin,
Madison, 2006. Available at www.cs.wisc.edu/ ˜jerryzhu/pub/ssl_survey.pdf

IEEE


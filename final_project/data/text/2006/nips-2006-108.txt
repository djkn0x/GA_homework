Generalized Regularized Least-Squares Learning
with Prede(cid:2)ned Features in a Hilbert Space

Wenye Li, Kin-Hong Lee, Kwong-Sak Leung
Department of Computer Science and Engineering
The Chinese University of Hong Kong
Shatin, Hong Kong, China
fwyli, khlee, ksleungg@cse.cuhk.edu.hk

Abstract

Kernel-based regularized learning seeks a model in a hypothesis space by mini-
mizing the empirical error and the model’s complexity. Based on the representer
theorem, the solution consists of a linear combination of translates of a kernel.
This paper investigates a generalized form of representer theorem for kernel-based
learning. After mapping prede(cid:2)ned features and translates of a kernel simultane-
ously onto a hypothesis space by a speci(cid:2)c way of constructing kernels, we pro-
posed a new algorithm by utilizing a generalized regularizer which leaves part of
the space unregularized. Using a squared-loss function in calculating the empiri-
cal error, a simple convex solution is obtained which combines prede(cid:2)ned features
with translates of the kernel. Empirical evaluations have con(cid:2)rmed the effective-
ness of the algorithm for supervised learning tasks.

1 Introduction

Supervised learning, or learning from examples, refers to the task of training a system by a set of
examples which are speci(cid:2)ed by input-output pairs. The system is used to predict the output value
for any valid input object after training. Examples of such tasks include regression which produces
continuous values, and classi(cid:2)cation which predicts a class label for an input object.
Vapnik’s seminal work[1] shows that the key to effectively solving this problem is by controlling
the solution’s complexity, which leads to the techniques known as regularized kernel methods[1]
[2][3] and regularization networks[4]. The work championed by Poggio and other researchers[5][6]
implicitly treats learning as an approximation problem and gives a general scheme with ideas going
back to modern regularization theory[7][8][9]. For both frameworks, a solution is sought by simul-
taneously minimizing the empirical error and the complexity. More precisely, given a training set
D = (xi ; yi )m
i=1 , an estimator f : X ! Y , where X is a closed subset of Rd and Y (cid:26) R, is given
by

V (yi ; f (xi )) + (cid:13) kf k2
K

m
1
Xi=1
min
m
f 2HK
where V is a convex loss function, kf kK is the norm of f in a reproducing kernel Hilbert space
(RKHS) HK induced by a positive de(cid:2)nite function (a kernel) Kx (x0 ) = K (x; x0 ), and (cid:13) is a
regularization parameter that makes a trade-off between the empirical error and the complexity.
(cid:13) kf k2
K is also called a regularizer.
According to representer theorem [10][11] [12], the minimizer of (1) admits a simple solution as a
linear combination of translates of the kernel K by the training data
m
Xi=1

(1)

f (cid:3) =

ciKxi ; ci 2 R; 1 (cid:20) i (cid:20) m

for a variety of loss functions. Different loss functions lead to different learning algorithms. For
example, when used for classi(cid:2)cation, a squared-loss (y (cid:0) f (x)) 2 brings about the regularized
least-squares classi(cid:2)cation (RLSC) algorithm[13][14][15]; while a hinge loss (1 (cid:0) yf (x))+ (cid:17)
max (1 (cid:0) yf (x) ; 0) corresponds to the classical support vector machines(SVM).
Using this model, data are implicitly projected onto the hypothesis space HK via a transformation

(cid:30)K : x ! Kx

and a linear functional is sought by (cid:2)nding its representer in HK , which generally has in(cid:2)nite
dimensions. It is generally believed that learning problems associated with in(cid:2)nite dimensions are
ill-posed and need regularization. However, (cid:2)nite dimensional problems are often associated with
well-posedness and do not need regularization. Motivated by this, we uni(cid:2)ed these two views in
this paper. Using an existing trick in designing kernels, an RKHS is constructed which contains
a subspace spanned by some prede(cid:2)ned features and this subspace is left unregularized during the
learning process. Empirical results have shown the embedding of these features often has the effect
of stabilizing the algorithms’s performance for different choices of kernels and prevents the results
from deteriorating for inappropriate kernels.
The paper is organized as follows. First, a generalized regularized learning model and its associated
representer theorem are studied. Then, we introduce an existing trick with which we constructed a
hypothesis space which has a subspace of the prede(cid:2)ned features. Next, a generic learning algorithm
is proposed based on the model and especially evaluated for classi(cid:2)cation problems. Empirical
results have con(cid:2)rmed the bene(cid:2)ts brought by the algorithm.
A note on notation. Throughout the paper, vectors and matrices are represented in bold notation and
scalars in normal script, e.g. x1 ; (cid:1) (cid:1) (cid:1) ; xm 2 Rd , K 2 Rm(cid:2)m , and y1 ; (cid:1) (cid:1) (cid:1) ; ym 2 R. I and O are
used to denote an identity matrix and a zero matrix of appropriate sizes, respectively. For clarity, the
size of a matrix is sometimes added as a subscript, such as Om(cid:2)‘ .

2 Generalized regularized least-squares learning model

Suppose the space HK decomposes into the direct sum: HK = H0 (cid:8) H1 ; where H0 is spanned
by ‘ ((cid:20) m) linearly independent features: H0 = span (’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ ). We propose the generalized
regularized least-squares (G-RLS) learning model as
m
Xi=1
where P f is the orthogonal projection of f onto H0 .
Suppose f (cid:3) is the minimizer of (2). For any f 2 HK , let f = f (cid:3) + (cid:14)g where (cid:14) 2 R and g 2 HK .
@ (cid:14) j(cid:14)=0 = 0 . Then
Now take derivative w.r.t. (cid:14) and notice that @L

(yi (cid:0) f (xi ))2 + (cid:13) kf (cid:0) P f k2
K ;

min
f 2HK

L (f ) =

1
m

(2)

(cid:0)

2
m

(yi (cid:0) f (cid:3) (xi )) g (xi ) + 2(cid:13) hf (cid:3) (cid:0) P f (cid:3) ; giK = 0;

m
Xi=1
where h(cid:1); (cid:1)iK denotes the inner product in HK . This equation holds for any g 2 HK . In particular,
setting g = Kx gives
f (cid:3) (cid:0) P f (cid:3) = Pm
i=1 (yi (cid:0) f (cid:3) (xi )) Kxi
m(cid:13)
P f (cid:3) is the orthogonal projection of f (cid:3) onto H0 and hence,

(3)

(4)

:

So (4) is simpli(cid:2)ed to

P f (cid:3) =

‘
Xp=1

(cid:21)p’p ; (cid:21)p 2 R; 1 (cid:20) p (cid:20) ‘:

f (cid:3) =

‘
Xp=1

(cid:21)p’p +

ciKxi ;

m
Xi=1

(5)

(6)

where

(7)

ci =

; 1 (cid:20) i (cid:20) m:

yi (cid:0) f (cid:3) (xi )
m(cid:13)
The coef(cid:2)cients (cid:21)1 ; (cid:1) (cid:1) (cid:1) ; (cid:21)‘ ; c1 ; (cid:1) (cid:1) (cid:1) ; cm are uniquely speci(cid:2)ed by m + ‘ linear equations. The (cid:2)rst
m equations are obtained by substituting (6) into (7). The rest ‘ equations are derived from the
orthogonality constraint between P f (cid:3) and f (cid:0) P f (cid:3) , which can be written as
m
ciKxi +K
*’p ;
Xi=1
or equivalently due to the property of reproducing kernels,
m
Xi=1
The solution (6) derived from (2) satis(cid:2)es the reproduction property. Suppose (x i ; yi )m
i=1 comes
purely from a model which is perfectly linearly related to ’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ , it is desirable to get back a
solution that is independent of the other features. As an evident result of (2), the property is satis(cid:2)ed.
The parameters c1 ; (cid:1) (cid:1) (cid:1) ; cm in the resulting estimator (6) are all zero, which makes the regularizer in
(2) equal to zero.

ci’p (xi ) = 0; 1 (cid:20) p (cid:20) ‘:

= 0; 1 (cid:20) p (cid:20) ‘;

(8)

(9)

3 Kernel construction

By decomposing a hypothesis space HK and studying a generalized regularizer, we have proposed
the G-RLS model and derived a solution which consists of prede(cid:2)ned features as well as translates
of a kernel function. In this section, starting with prede(cid:2)ned features ’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ and a kernel (cid:8), we
will construct a hypothesis space which contains the features and translates of the kernel by using
an existing trick.

3.1 A kernel construction trick

Let’s consider the following reproducing kernel

K (x; x0 ) = H (x; x0 ) +

‘
Xp=1

’0
p (x) ’0
p (x0 )

(10)

where

H (x; x0 ) = (cid:8) (x; x0 ) (cid:0)

’0
q (x0 ) (cid:8) (x; xq )

(11)

’0
p (x) (cid:8) (xp ; x0 ) (cid:0)

‘
‘
Xp=1
Xq=1
q (x0 ) (cid:8) (xp ; xq ) ;
p (x) ’0
’0

+

‘
‘
Xq=1
Xp=1
(cid:8) is any strictly positive de(cid:2)nite function, and ’0
‘ de(cid:2)nes a linear transformation of
1 ; (cid:1) (cid:1) (cid:1) ; ’0
’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ w.r.t. x1 ; (cid:1) (cid:1) (cid:1) ; x‘ ,
" ’0
‘ (x) # = " ’1 (x1 )
1 (x)
(cid:1) (cid:1) (cid:1)
(cid:1) (cid:1) (cid:1)
’0
’‘ (x1 )
q (xp ) = (cid:26) 1
’0
0
This trick is studied in [16] to provide an alternative basis for radial basis functions and (cid:2)rst used in
a fast RBF interpolation algorithm[17]. A sketch of properties which are peripheral to our concerns
in this paper are given below.

(cid:1) (cid:1) (cid:1) ’1 (x‘ )
(cid:1) (cid:1) (cid:1) ’‘ (x‘ ) #
(cid:1) (cid:1) (cid:1)

" ’1 (x)
’‘ (x) #
(cid:1) (cid:1) (cid:1)

1 (cid:20) p = q (cid:20) ‘
1 (cid:20) p 6= q (cid:20) ‘

which satis(cid:2)es

(12)

(13)

(cid:0)1

:

Kxp = ’0
p ; 1 (cid:20) p (cid:20) ‘

(14)

1 (cid:20) p = q (cid:20) ‘
1 (cid:20) p 6= q (cid:20) ‘

= (cid:26) 1
(cid:10)’0
p ; ’0
q (cid:11)K
0
Hxp = H (xp ; (cid:1)) = 0; 1 (cid:20) p (cid:20) ‘
= (cid:10)H (xi ; (cid:1)) ; ’0
(cid:10)Hxi ; ’0
p (cid:11)K
p (cid:11)K
= 0; ‘ + 1 (cid:20) i (cid:20) m; 1 (cid:20) p (cid:20) ‘
(18)
(cid:10)Hxi ; Hxj (cid:11)K = H (xi ; xj ) ; ‘ + 1 (cid:20) i; j (cid:20) m
Another property is that the matrix H = (H (xi ; xj ))m
i;j=‘+1 is strictly positive de(cid:2)nite, which will
be used in the computations below.
By constructing a kernel K using this trick, prede(cid:2)ned features ’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ are explicitly mapped
‘ ) = span (’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ ). By property (15),
onto HK which has a subspace H0 = span (’0
1 ; (cid:1) (cid:1) (cid:1) ; ’0
we can see that ’0
‘ also forms an orthonormal basis of H0 .
1 ; (cid:1) (cid:1) (cid:1) ; ’0

(17)

(16)

(15)

3.2 Computation

After projecting the features ’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ onto an RKHS HK , let’s study the regularized minimiza-
tion problem in (2). As shown in (6), the minimizer has a form of a linear combination of prede(cid:2)ned
features and translates of a kernel. By the properties of K in (14)-(17), the minimizer can be rewrit-
ten as:

ciKxi

=

=

=

f (cid:3) =

(cid:21)p’p +

ci’0
i +

p!!
p (xi ) ’0
’0

‘
m
ci  Hxi +
Xp=1
Xi=‘+1
m
p (xi )! ’0
Xi=‘+1
ci’0
p +
ciHxi

‘
m
Xp=1
Xi=1
p +   ‘
‘
Xi=1
Xp=1
p’0
(cid:21)0
‘
m
Xp=1  (cid:21)0
Xi=‘+1
p + cp +
‘
m
~(cid:21)p’0
Xp=1
Xi=‘+1
~ciHxi
p +
where ~(cid:21)1 ; (cid:1) (cid:1) (cid:1) ; ~(cid:21)‘ ; ~c‘+1 ; (cid:1) (cid:1) (cid:1) ; ~cm are m parameters to be determined. Furthermore, from the orthog-
p and Hxi in (17), we have
onal property between ’0
m
Xi=‘+1
To determine the values of ~(cid:21) = (cid:16)~(cid:21)1 ; (cid:1) (cid:1) (cid:1) ; ~(cid:21)‘(cid:17)T
and ~c = (~c‘+1 ; (cid:1) (cid:1) (cid:1) ; ~cm )T , we need
m
~c (cid:19)T
~ci ~cj H (xi ; xj ) = (cid:18) ~(cid:21)
~H (cid:18) ~(cid:21)
~c (cid:19)
kf (cid:3) (cid:0) P f (cid:3)k2
Xi;j=‘+1
K =
where ~H = (cid:18) O‘(cid:2)‘
(cid:19). Substituting (21) into (2), we have
O‘(cid:2)(m(cid:0)‘)
O(m(cid:0)‘)(cid:2)‘
H
~c (cid:19)(cid:19)T (cid:18)y (cid:0) ~K (cid:18) ~(cid:21)
~c (cid:19)T
~H (cid:18) ~(cid:21)
~c (cid:19)
~c (cid:19)(cid:19) + (cid:13) (cid:18) ~(cid:21)
m (cid:18)y (cid:0) ~K (cid:18) ~(cid:21)
1
L =
where ~K = (cid:18) I‘(cid:2)‘ O‘(cid:2)(m(cid:0)‘)
. Take derivative w.r.t. (cid:18) ~(cid:21)
(cid:19) and E = (cid:0)’0
~c (cid:19)
p (xi )(cid:1)‘;m
ET
H
p=1;i=‘+1
and set the derivative to zero, and we get
~K2 (cid:18) ~(cid:21)
~c (cid:19) + (cid:13)m ~H (cid:18) ~(cid:21)
~c (cid:19) = ~Ky:

f (cid:3) (cid:0) P f (cid:3) =

(19)

(20)

~ciHxi :

(21)

(22)

(23)

I‘(cid:2)‘
(cid:0)H(cid:0)1ET

Since ~K(cid:0)1 = (cid:18)
we have

(cid:19) and ~K(cid:0)1 ~H = ~I = (cid:18) O‘(cid:2)‘
O(m(cid:0)‘)(cid:2)‘
H(cid:0)1
O(m(cid:0)‘)(cid:2)‘
~c (cid:19) = y;
(cid:16) ~K + (cid:13)m~I(cid:17) (cid:18) ~(cid:21)
y2 (cid:19) ;
~c (cid:19) = (cid:18) y1
ET H + (cid:13)mI (cid:19) (cid:18) ~(cid:21)
(cid:18) I‘(cid:2)‘ O‘(cid:2)(m(cid:0)‘)
where y1 = (y1 ; (cid:1) (cid:1) (cid:1) ; y‘)T and y2 = (y‘+1 ; (cid:1) (cid:1) (cid:1) ; ym )T . Equation (25) uniquely speci(cid:2)es ~(cid:21) by
~(cid:21) = y1 ;

I(m(cid:0)‘)(cid:2)(m(cid:0)‘) (cid:19),
O‘(cid:2)(m(cid:0)‘)
(24)

(25)

(26)

i.e.

and ~c by

(H + (cid:13)mI) ~c = y2 (cid:0) ET ~(cid:21):
(27)
H + (cid:13)mI is a strictly positive de(cid:2)nite matrix. The equation can be ef(cid:2)ciently solved either by conju-
gate gradient or by Cholesky factorization. The worst case complexity is O (cid:16)(m (cid:0) ‘)3(cid:17) (cid:25) O (cid:0)m3 (cid:1).
It is also possible to investigate iterative methods for solving linear systems coupled with recent ad-
vances in fast matrix-vector multiplication methods (e.g. fast multipole method), and the complexity
reduces to nearly O (m log m), which provides the potential to solve large scale problems.

4 A generic learning algorithm

Based on the discussions above, a generic learning algorithm (G-RLS algorithm) is summarized
below.

1. Start with data (xi ; yi )m
i=1 .
2. For ‘ ((cid:20) m) prede(cid:2)ned linearly independent features ’1 ; (cid:1) (cid:1) (cid:1) ; ’‘ of the data, de(cid:2)ne
‘ according to equation (12).
1 ; (cid:1) (cid:1) (cid:1) ; ’0
’0
3. Choose a symmetric, strictly positive de(cid:2)nite function (cid:8)x (x0 ) = (cid:8) (x; x0 ) which is con-
tinuous on X (cid:2) X . De(cid:2)ne H according to equation (11).
4. The estimator f : X ! Y is given by
‘
m
Xp=1
Xi=‘+1
where ~(cid:21)1 ; (cid:1) (cid:1) (cid:1) ; ~(cid:21)‘ ; ~c‘+1 ; (cid:1) (cid:1) (cid:1) ; ~cm are obtained by solving equations (26) and (27).

~(cid:21)p’0
p (x) +

~ciHxi (x)

f (x) =

(28)

The algorithm can be applied to a number of applications including regression and binary classi-
(cid:2)cation. As a simple example for regression, noisy points were randomly generated via a func-
tion y = j5 (cid:0) xj, and we (cid:2)tted the data by a curve. Polynomial features up to the second degree
(’1 = 1; ’2 = x; ’3 = x2 ) were used for G-RLS algorithm along with a Gaussian RBF kernel
(cid:8)x ((cid:1)) = e(cid:0) kx(cid:0)(cid:1)k2
:
(cid:27)2
We selected ridge regression with the Gaussian RBF kernel for a comparison, which can be regarded
as an implementation of standard regularized least-squares model for regression tasks. For both
algorithms, three trials were made in which the parameter (cid:27) was set to a large value, to a small
value, and by cross validation respectively. For each (cid:27) , the parameter (cid:13) was set by cross validation.
Comparing with ridge regression in (cid:2)gure 1(b), the existence of polynomial features in G-RLS has
the effect of stabilizing the results, as shown in (cid:2)gure 1(a). Varying (cid:27) , different (cid:2)tting results were
obtained by ridge regression. However, for G-RLS algorithm, the difference was not evident.
In the case of generalized regularized least-squares classi(cid:2)cation (G-RLSC), each y i of the training
set takes the values f(cid:0)1; 1g. The predicted label of any x depends on the sign of (28)
y = (cid:26) 1;
f (x) > 0
(cid:0)1
otherwise
G-RLSC uses the (cid:148)classical(cid:148) squared-loss as a classi(cid:2)cation loss criterion. The effectiveness of this
criterion has been reported by the empirical results[13][14][15].

:

5

4

3

2

1

0

−1
−5

data
s2=cv
s2=1000
s2=0.001

−4

−3

−2

−1

0

1

2

3

4

5

5

4

3

2

1

0

−1

−5

data
s2=cv
s2=1000
s2=0.001

−4

−3

−2

−1

0

1

2

3

4

5

(a) G-RLS Regression

(b) Ridge Regression

Figure 1: A Regression Example. The existence of polynomial features in G-RLS helped to improve
the stability of the algorithm.

5 Experiments

To evaluate the performance of G-RLS algorithm, empirical results are reported on text categoriza-
tion tasks using the three datasets from CMU text mining group1. The 7-sectors dataset has 4; 573
web pages belonging to seven economic sectors, with each sector containing pages varying from
300 to 1; 099. The 4-universities dataset consists of 8; 282 webpages collected mainly from four
universities, in which the pages belong to seven classes and each class has 137 to 3; 764 pages.
The 20-newsgroups dataset collects UseNet postings into twenty newsgroups and each group has
about 1; 000 messages. We experimented with its four major subsets. The (cid:2)rst subset has 5 groups
(comp.*), the second 4 groups (rec.*), the third 4 groups (sci.*) and the last 4 groups (talk.*).
For each dataset, we removed all but the 2; 000 words with highest mutual information with the
class variable by rainbow package[18]. The document was represented as bag-of-words with linear
normalization into [(cid:0)1; 1]. Probabilistic latent semantic analysis[19] (pLSA) was used to get ten
latent features ’1 ; (cid:1) (cid:1) (cid:1) ; ’10 out of the data. Experiments were carried out with different number
(100~3; 200) of data for training and the rest for testing. Each experiment consisted of ten runs
and the average accuracy is reported. In each run, the data were separated by the xval-prep utility
accompanied in C4.5 package2.
Figure 2 compares the performance of G-RLSC, RLSC and SVM. It is shown that G-RLSC reports
improved results on most of the datasets except on 4-universities. Moreover, an insightful observa-
tion may (cid:2)nd that although SVM excels on the dataset when the number of training data increases,
G-RLSC shows better performance than standard RLSC. A possible reason is that the hinge loss
used by SVM is more appropriate than the squared-loss used by RLSC and G-RLSC on this dataset;
while the embedding of pLSA features still improves the accuracy.

6 Conclusion

In this paper, we (cid:2)rst proposed a generic G-RLS learning model. Unlike the standard kernel-based
methods which only consider the translates of a kernel for model learning, the new model takes
prede(cid:2)ned features into special consideration. A generalized regularizer is studied which leaves
part of the hypothesis space unregularized. Similar ideas were explored in spline smoothing[9] in
which low degree polynomials are not regularized. Another example is semi-parametric SVM[2],
which considers the addition of some features to the kernel expansion for SVM. However, to our
knowledge, few learning algorithms and applications have been studied along this line from a uni(cid:2)ed
RKHS regularization point of view, or investigated for empirical evaluations.
The second part of our work presented a practical computation method based on the model. An
RKHS that contains the combined solutions is explicitly constructed based on a special trick in
designing kernels. (The idea of a conditionally positive de(cid:2)nite function[20] is lurking in the back-

1 http://www.cs.cmu.edu/(cid:152)TextLearning/datasets.html
2 http://www.rulequest.com/Personal/c4.5r8.tar.gz.

(cid:26)(cid:16)(cid:86)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:86)

(cid:23)(cid:16)(cid:88)(cid:81)(cid:76)(cid:89)(cid:72)(cid:85)(cid:86)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)

(cid:42)(cid:16)(cid:53)(cid:47)(cid:54)(cid:38)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:20)(cid:19)(cid:19)(cid:8)

(cid:28)(cid:19)(cid:8)

(cid:27)(cid:19)(cid:8)

(cid:26)(cid:19)(cid:8)

(cid:25)(cid:19)(cid:8)

(cid:24)(cid:19)(cid:8)

(cid:23)(cid:19)(cid:8)

(cid:22)(cid:19)(cid:8)

(cid:20)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)

(cid:23)(cid:19)(cid:19)

(cid:27)(cid:19)(cid:19)

(cid:20)(cid:25)(cid:19)(cid:19)

(cid:22)(cid:21)(cid:19)(cid:19)

(cid:20)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)

(cid:23)(cid:19)(cid:19)

(cid:27)(cid:19)(cid:19)

(cid:20)(cid:25)(cid:19)(cid:19)

(cid:22)(cid:21)(cid:19)(cid:19)

(cid:70)(cid:82)(cid:80)(cid:83)(cid:17)(cid:13)

(cid:85)(cid:72)(cid:70)(cid:17)(cid:13)

(cid:42)(cid:16)(cid:53)(cid:47)(cid:54)(cid:38)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:20)(cid:19)(cid:19)(cid:8)

(cid:28)(cid:19)(cid:8)

(cid:27)(cid:19)(cid:8)

(cid:26)(cid:19)(cid:8)

(cid:25)(cid:19)(cid:8)

(cid:24)(cid:19)(cid:8)

(cid:23)(cid:19)(cid:8)

(cid:22)(cid:19)(cid:8)

(cid:20)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)

(cid:23)(cid:19)(cid:19)

(cid:27)(cid:19)(cid:19)

(cid:20)(cid:25)(cid:19)(cid:19)

(cid:22)(cid:21)(cid:19)(cid:19)

(cid:20)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)

(cid:23)(cid:19)(cid:19)

(cid:27)(cid:19)(cid:19)

(cid:20)(cid:25)(cid:19)(cid:19)

(cid:22)(cid:21)(cid:19)(cid:19)

(cid:86)(cid:70)(cid:76)(cid:17)(cid:13)

(cid:87)(cid:68)(cid:79)(cid:78)(cid:17)(cid:13)

(cid:42)(cid:16)(cid:53)(cid:47)(cid:54)(cid:38)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:20)(cid:19)(cid:19)(cid:8)

(cid:28)(cid:19)(cid:8)

(cid:27)(cid:19)(cid:8)

(cid:26)(cid:19)(cid:8)

(cid:25)(cid:19)(cid:8)

(cid:24)(cid:19)(cid:8)

(cid:23)(cid:19)(cid:8)

(cid:22)(cid:19)(cid:8)

(cid:20)(cid:19)(cid:19)(cid:8)

(cid:28)(cid:19)(cid:8)

(cid:27)(cid:19)(cid:8)

(cid:26)(cid:19)(cid:8)

(cid:25)(cid:19)(cid:8)

(cid:24)(cid:19)(cid:8)

(cid:23)(cid:19)(cid:8)

(cid:22)(cid:19)(cid:8)

(cid:20)(cid:19)(cid:19)(cid:8)

(cid:28)(cid:19)(cid:8)

(cid:27)(cid:19)(cid:8)

(cid:26)(cid:19)(cid:8)

(cid:25)(cid:19)(cid:8)

(cid:24)(cid:19)(cid:8)

(cid:23)(cid:19)(cid:8)

(cid:22)(cid:19)(cid:8)

(cid:20)(cid:19)(cid:19)(cid:8)

(cid:28)(cid:19)(cid:8)

(cid:27)(cid:19)(cid:8)

(cid:26)(cid:19)(cid:8)

(cid:25)(cid:19)(cid:8)

(cid:24)(cid:19)(cid:8)

(cid:23)(cid:19)(cid:8)

(cid:22)(cid:19)(cid:8)

(cid:42)(cid:16)(cid:53)(cid:47)(cid:54)(cid:38)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:42)(cid:16)(cid:53)(cid:47)(cid:54)(cid:38)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:42)(cid:16)(cid:53)(cid:47)(cid:54)(cid:38)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:53)(cid:47)(cid:54)(cid:38)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:37)(cid:82)(cid:58)

(cid:54)(cid:57)(cid:48)(cid:3)(cid:83)(cid:47)(cid:54)(cid:36)

(cid:20)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)

(cid:23)(cid:19)(cid:19)

(cid:27)(cid:19)(cid:19)

(cid:20)(cid:25)(cid:19)(cid:19)

(cid:22)(cid:21)(cid:19)(cid:19)

(cid:20)(cid:19)(cid:19)

(cid:21)(cid:19)(cid:19)

(cid:23)(cid:19)(cid:19)

(cid:27)(cid:19)(cid:19)

(cid:20)(cid:25)(cid:19)(cid:19)

(cid:22)(cid:21)(cid:19)(cid:19)

Figure 2: Classi(cid:2)cation accuracies on CMU text datasets with different number of training samples.
Ten pLSA features along with a linear kernel (cid:8) were used for G-RLSC. Both bag-of-words (BoW)
and pLSA representations of documents were experimented for RLSC and SVM with a linear kernel.
The parameter (cid:13) was selected via cross validation. For multi-classi(cid:2)cation, G-RLSC and RLSC used
one-versus-all strategy. SVM used one-versus-one strategy.

ground of this trick, which goes beyond the discussion of this paper.) With the construction of the
RKHS, the computation is further optimized and the theoretical analysis of such algorithms is also
potentially facilitated.
We evaluated G-RLS learning algorithm in text categorization. The empirical results from real-world
applications have con(cid:2)rmed the effectiveness of the algorithm.

Acknowledgments

The authors thank Dr. Haixuan Yang for useful discussions. This research was partially supported
by RGC Earmarked Grant #4173/04E and #4132/05E of Hong Kong SAR and RGC Research Grant
Direct Allocation of the Chinese University of Hong Kong.

References

[1] V.N. Vapnik. Statistical Learning Theory. John Wiley and Sons, 1998.
[2] B. Sch ¤olkopf and A.J. Smola. Learning with Kernels. The MIT Press, 2002.
[3] J.S. Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.
[4] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Adv.
Comput. Math., 13:1(cid:150)50, 2000.
[5] T. Poggio and F. Girosi. Regularization algorithms for learning that are equivalent to multilayer networks.
Science, 247:978(cid:150)982, 1990.
[6] T. Poggio and S. Smale. The mathematics of learning: Dealing with data. Not. Am. Math. Soc, 50:537(cid:150)
544, 2003.
[7] A.N. Tikhonov and V.Y. Arsenin. Solutions of Ill-Posed Problems. Winston and Sons, 1977.
[8] V.A. Morozov. Methods for Solving Incorrectly Posed Problems. Springer-Verlag, 1984.
[9] G. Wahba. Spline Models for Observational Data. SIAM, 1990.
[10] G. Kimeldorf and G. Wahba. Some results on Tchebychef(cid:2)an spline functions. J. Math. Anal. Appl.,
33:82(cid:150)95, 1971.
[11] F. Girosi, M.J. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural
Comput., 7:219(cid:150)269, 1995.
[12] B. Sch ¤olkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem.
EuroCOLT’2001, 2001.
[13] R.M. Rifkin. Everything Old is New Again: A Fresh Look at Historical Approaches in Machine Learning.
PhD thesis, Massachusetts Institute of Technology, 2002.
[14] G. Fung and O.L. Mangasarian. Proximal support vector machine classi(cid:2)ers. In KDD’01, 2001.
[15] J.A.K. Suykens and J. Vandewalle. Least squares support vector machine classi(cid:2)ers. Neural Process.
Lett., 9:293(cid:150)300, 1999.
[16] W. Light and H. Wayne. Spaces of distributions, interpolation by translates of a basis function and error
estimates. J. Numer. Math., 81:415(cid:150)450, 1999.
[17] R.K. Beatson, W.A. Light, and S. Billings. Fast solution of the radial basis function interpolation equa-
tions: Domain decomposition methods. SIAM J. Sci. Comput., 22:1717(cid:150)1740, 2000.
[18] A.K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classi(cid:2)cation and clus-
tering. http://www.cs.cmu.edu/(cid:24)mccallum/bow, 1996.
[19] T. Hofmann. Probabilistic latent semantic analysis. In UAI’99, 1999.
[20] C.A. Micchelli. Interpolation of scattered data: Distances, matrices, and conditionally positive de(cid:2)nite
functions. Constr. Approx., 2:11(cid:150)22, 1986.

In COLT’2001 and


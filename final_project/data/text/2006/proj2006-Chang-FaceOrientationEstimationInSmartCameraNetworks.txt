Face Orientation Estimation in Smart Camera Networks

Chung-Ching Chang

1. Introduction

An important motivation for face orientation analysis de-
rives from the fact that most face recognition algorithms
require face images with approximately frontal view to
operate efﬁciently, such as principle component analysis
(PCA) [6], linear discriminant analysis (LDA) [4], and hid-
den markov model (HMM) techniques [2].
In a networked camera setting, the desire for a fontal
view to pursue an effective face analysis is relaxed due to
the distributed nature of the camera views. Instead of ac-
quiring frontal face image from any single camera, we pro-
pose an approach to face reconstruction in a smart camera
network by collaboratively collecting and sharing face in-
formation spatially.
The framework of spatiotemporal feature fusion for face
1. In-node
reconstruction and analysis is shown in Fig.
feature extraction in each camera node consists of low-level
vision methods to detect features for estimation of face ori-
entation or the angular velocity. These include the hair-face
ratio and optical ﬂow, which are obtained through Discrete
Fourier Transform (DFT) and Least Squares (LS), respec-
tively. Another feature extracted locally is a set of head
strips, which is used to estimate relative angular difference
to the face between cameras by a proposed matching tech-
nique. A Markov model is designed to exploit the geometic
connectivity between strips, and a Viterbi-like algorithm is
applied to select the most probable displacement between
head strips of the two cameras. Therefore, the proposed
technique does not require camera location to be known in
prior, and hence is applicable to vision networks deployed
casually without localization.
A spatiotemporal feature fusion is implemented via key-
frame detection, and a spatiotemporal reinforment learning.
The key frames are obtained when a camera node detects a
frontal face view through a hair-face analysis scheme and
this event is broadcasted to other camera nodes so that the
fusion schemes for face analysis can be adaptively adjusted
according the the relative angular estimates, in order to
maintain a high conﬁdence level. The proposed spatiotem-
poral reinforment learning cooperate the temporal correla-
tion into the state transition matrix and the spatial correla-
tion into a cost function design, and choose the trellis with
the minimum cost.

A Single 
Camera Node

Image(x,y,t)

Head strip matching 

In-node feature 
extraction

Head-strips

Other Network Nodes

Optical 
estimation

Hair-face
estimation

Face-strip 
matching

Key frame?

Y

decision

Key-frame
notification 
and receiving

Spatiotemporal
Reinforced Learning

Relative angular 
difference to the object

Face orientation 
estimates

Spatiotemporal 
data fusion

Figure 1. Framework of spatiotemporal feature fusion for face ori-
entation analysis.

2. In-node Feature Extraction

Local data processing algorithms in each camera node
consist of low-level vision methods to detect features for
estimation of face orientation, including optical ﬂow and
hair-face ratio as introduced in the following subsections.
These techniques are developed to be of low computational
complexity, allowing them to be adopted for in-node pro-
cessing implementations.

2.1. Optical Flow Estimation

This analysis project the motion of the head into several
orthogonal dimensions and estimate the projected vector by
least square estimation. In order to reduce computational
complexity, we only consider the motion vector of the edges
of a face.
In our experiments, we assume the head turns
without tilt and pan, so we decompose the head motion into
only translation and rotation in y axis to simplify the analy-
sis. The decomposition model is as follows:

vi = t + rωcos(θi )

(1)

where vi is the norm and direction of the motion vector in
the direction orthogonal to the head’s vertical axis (where
positive sign indicates the direction is to the right, and neg-
ative sign to the left) , t is the translation factor, r is the
transversal radius of the head, ω is the angular motion, and
rcos(θi ) represents the distance from the point of the mo-
tion vector to the longitudinal axis of the head in the 2D

(a)

(c)

(b)

(d)
40

30

20

10

|
i
v
|

0

-10
0

head

body

10

40
30
20
rcos((cid:84)) for red and rsin((cid:84)) for blue

50

60

Figure 2. Opical ﬂow estimate (a) image(x,y,t), (b) image(x,y,t+1),
(c) image(x,y,t) and the motion vectors, (d) Least squares esti-
mates.

image plane. By placing Eq.(1) in vector form, we have




rcos(θ1 )
1
v1
· t
ω ¸
rcos(θ2 )
1
v2




...
...
...
| {z }z
1 rcos(θn )
vn
| {z }
|
{z
}
v
A
Minimizing the mean square error of the motion vectors un-
der the model yields the least squares solution of x as:

(2)

=

zls = (AT A)−1AT v

(3)

where the ﬁrst element of zls is the translational velocity
and the second element is the angular velocity of the head.
The residue of the least squares analysis yields a measure-
ment of the conﬁdence of the estimation. Experimental re-
sults are shown in Fig. 2, where the slope indicates the an-
gular velocity, and the intersection on y axis indicates the
translational velocity.

2.2. Hair-Face Ratio Estimation

To estimate the hair-face ratio, we ﬁrst classify the head
region into face and hair regions by color. There is much
previous work on using skin color model and hair color
model for face detection [1] [3] [5]. In this paper, we sim-
ply apply similar method with value applied well in the se-
quence. Further research on exploiting model bias between
cameras and skin color model estimation based on the EM
application (paper evaluation problem) proposed by John
Platt.
Based on the hair-face classi ﬁcation, face orientation is
analyzed in the following procedures as shown in Fig. 3.
Consider the head as a ball in 3D space, and cut the surface

(a)

(b)

? ? ? ? ?

zero-padding
Figure 3. Procedure for the hair-face ratio estimation (Illustration
of how the head ellipsoid (right) is transformed into a sequence
of hair-face categorized image slices (middle), and into a ratio se-
quence with zero-padding (left)).

of the ball into N equally spaced strips along its longest axis
direction, as shown in ﬁgure 3. In each camera frame, we
can only see m of the N strips of the ellipsoid. Calculating
the ratio of the hair region to the face region in each of the
m strips and padding zero to the strips that cannot be seen in
the current frame, we form a ratio sequence of length N . By
analyzing DFT of the ratio sequence and considering only
the phase of the fundamental frequency in the frequency do-
main, we can estimate the face orientation based on the as-
sumption that the hair-face ratio is symmetric in the frontal
face and is approximately a sinusoidal curve along the sur-
face of the ellipsoid.

3. Head Strip Matching and the Relative An-
gular Difference to the Face between Cam-
eras

Instead of calculating the ratio of the hair region to the
face region in each of the m strips, we sample each of the m
strips with n samples. Prior to the sampling, a 2D median
ﬁlter is applied to reduce noise as well as introduce corre-
lation between sampling points and the nearby pixels. Geo-
metrically, if all cameras are deployed at the same horizon,
the relative angular difference to the head between two cam-
eras would cause a shift in their strips. Therefore, matching
the head strips of the two cameras and ﬁnding the displace-
ment of the strips give us the (quantized) relative angular
difference to the object between the two cameras at a given
time.
The head strip mapping is based on a Markov model
and a Viterbi-like algorithm as shown in Fig. 4. Consid-
ering two sets of head strips Y and Y ′ , corresponding to the
head images captured in two cameras, C and C ′ , let Y =
m ], where yi , y ′
[y1 y2 . . . ym ] and Y ′ = [y ′
i ∈ Rn
2 . . . y ′
1 y ′
correspond to n sample points in a single strip. Our prob-
lem now is to map the strips in Y ′ to the strips in Y with the
i are in some spatial order.
constraint that yi , y ′
We now introduce the concept of the states S. Let S =
[s1 s2 . . . sN ] denote all states for the strips of a head (360o ),
for example, s1 representing the strip that includes the nose

(a)

Si+4

Si+3

Si+2

Pxixi+2

Si+1

Pxixi+1

Pxixi

Si

(b)
(cid:652)m

Sm

Sm

Sm

Sm

Sm

(cid:652)m-1

Sm-1

Sm-1

Sm-1

Sm-1

Sm-1

Pxixi+3

Yi+3

Yi+2

Yi+1

(cid:652)3

(cid:652)2

(cid:652)1

S3

S2

S1

S3

S2

S1

S3

S2

S1

S3

S2

S1

S3

S2

S1

Input

Y’1

Y’2

Y’3

(a)

(b)

(c)

40

30

Y
20

10

10

20

Y’

30

40

Figure 4. Illustration of the Markov model and Viterbi-like algo-
rithm. (a) The Viterbi-like model generated by the head strip set
in camera C, (b) The trellis of the Viterbi-like algorithm. Sm−1
in the rightmost row is the state with the minimum cost, and the
corresponding trellis is marked in thick (red) line.

Figure 5. Example of strip matching. (a) Background-subtracted
image in camera Y ′ , (b) Background-subtracted image in Y, (c)
The trellis with minimum cost (blue) and the corresponding cost
in each step of the Viterbi-like algorithm (red).

trail. For each of the captured head images, the correspond-
ing head strips Y should map to a consecutive subset of S ,
denoted SY , which is not known in prior and is approxi-
mately of length m. In other words, Y is a representation of
the states SY . As we scan vertical sampling lines through
the head horizontally, we are actually going from state to
state, for example , from si to si+1 .
Ideally we will get
yj and yj+1 for certain i and j . However, due to the fact
that the head is not a perfect ball, we may as well get yj
and yj+k for certain j and small k ≥ 0, the latter constraint
showing that the two states should be near and cannot oc-
cur in a reverse order as we scan through the head. In other
words, the probability of P si si+1 , the probability of going
from the current state to the next state as we scan through
the head, is not necessarily 1. The probability of the tran-
sition between states forms a Markov model, as shown in
Fig.4(a). In our experiment, the choice for the probability is

Psi si+k = exp(−

(4)

)(u(k) − u(k − 4))

(k − 1)2
2σ2
where u is the unit step function and σ is the bandwidth
parameter. Further normalization is required.
As we match the set of strips Y ′ to Y , we ﬁrst as-
sume that the representation Y is ideal, corresponding to the
states SY one-by-one. Under this assumption, we transform
the Viterbi algorithm, a supervised learning algorithm, into
an unsupervised way of learning, which we call a Viterbi-
i , we can sum the cost
like algorithm. For each given input y ′
in each of the previous states and the cost-to-go(w) in each
branch, and choose the branch with the minimum cost as
the path from the previous states to the current states. The
cost of the branch w is calculated as follows:

wsi si+k = − ln(Psi si+k γ (y ′
i+k ; si si+k ))
i+k ; si si+k ) is calculated by the inverse of the
where γ (y ′
mean square error between strips y ′
i+k and yi+k .
The initial states are assumed to be equally likely, mean-
ing that the matching can start from any of the states in SY .

(5)

The ﬁrst and the last states in SY may be regarded physi-
cally as the not-in-Y (not in current face) states. Therefore,
some exceptions for the probability model are made in the
ﬁrst and the last states, where P s1 s1 is given higher proba-
bility and P si sm is 1 when i = m, and zero otherwise.
According to the Viterbi algorithm, the path with the
smallest cost is chosen. For example, as in Fig. 4, as-
sume sm−1 in the rightmost column is the state with the
minimum cost, and the corresponding previous paths are
marked with thick (red) lines, showing that the paths are
[s1 s1 s2 . . . sm−1 ]. In Fig5, an example of head strip match-
ing is shown, the tellis of the Viterbi-like algorithm is shown
in the right ﬁgure with blue dots, where red dots represent
minimum branch cost (w) in each Viterbi-like step. Notice
that the trellis, excluding those in states s1 and sm , inter-
sect the x-axis around 10, which means the displacement
between two head images is 10 strips, or 45 degrees in the
example.

4. Spatiotemporal Data Exchange

Collaboration between cameras is achieved by data ex-
change. Correlations in temporal domain is exploited since
face orientation and angular velocity, one being the deriva-
tive of the other, in consecutive frames are continuous pro-
vided that the time lapse between frames is short. Data ex-
change in spatial domain would also be helpful in sharing,
comparing, and validating data since for any time instance
the captured image in each camera should reﬂect the same
motion in 3D.

4.1. Key Frame Detection

Key frames are the frames that include feature or esti-
mates with high conﬁdence. The hair-face ratio based on
the phase of the fundamental frequency is sensitive to the
face angle. Although the true face angle is not a linear
function of the estimates, a frontal view with the hair-face
ratio approximately symmtric to the face center can be de-
tected accurately. Under the assumption that head motions

2

1

Key frame detected, 
indicating that the 
camera has the frontal 
view (0o)

3

Key frame 
notification
through the 
network

Calculate the face 
orientation by adding 0o
with the relative angular 
difference (cid:537)

4
Calculate the face 
orientation by adding 0o
with the relative angular 
difference (cid:537)+(cid:307)

(cid:537)

(cid:307)

e
c
a
f
-
r
i
a
H

)
e
e
r
g
e
d
(
 
s
e
t
a
m
i
t
s
e

a

b

t-1
*

t

ff

*

t

time

Time of a frontal 
face detected (tff)
(cid:16)
b
a
(cid:16)
(cid:16)
a b
a b

(cid:14)

t

(cid:32)

(cid:16)

1)

t

(

Figure 6. Illustration of the key frame detection procedure

are piecewise linear between samples, the time of a frontal
view, deﬁned as a key frame event, can be determined by
interpolation. Once a key frame is detected, the time of its
detection is noti ﬁed to other cameras. Since the key frame
is associated with relatively high conﬁdence, other camera s
would assume the received key frame orientation estimation
to be true and calculate the face orientation by adding that
with the relative angular difference to the object between
themselves and the camera that broadcasted the key frame.

4.2. Spatiotemporal Reinforcment Learning

In our framework,
the face orientation between key
frames are determined by the accumulation of the angular
motions. Recall that the optical ﬂow estimation is a pre-
diction over tens to hundreds of motion vectors According
to the law of large numbers, the pdf of the estimation itself
will be Gaussian distributed, no matter what the original
error distribution is. Assuming that the optical ﬂow esti-
mates in different time are independent, the variance of the
accumulated optical ﬂow estimates equals the sum of the
variance of each individual optical ﬂow estimates. There-
fore, while the orientation estimates are deterministic when
a key frame occurs, the orientation estimates between key
frames are stochastic with uncertainty increase over time.
Hence, we apply a spatiotemporal reinforcement learning
algorithm to choose the best trellis with the minimum cost
as the uncertainty increases.
In a Markov decision process(MDP), we are usually
given the states(S ), the state transition probability(Psa ),
the discount factor(γ ), and the cost function(C ), and want
to determine the optimal policy(π ) and its correponding
actions(A).
In our face orientation estimation settings, S
are the (discretized) face orientations, each column of Psa is
the probability density function of a Gaussian random vari-
able, with mean and variance determined by the optical ﬂow
estimation. The cost function is the sum of the absolute dif-
ference of the estimates between cameras. Choosing the
cost value to be a L1 norm of the difference of the estimates
can avoid the effect of an outlier. Different from the ordi-
nary MDP, the Psa here may change over time, and thus
the action also changes over time. The spatiotemporal re-
inforcement learning proposed here cooperate the temporal
information into Psa , and spatio information into C . Intu-

itively, the result is similar to that interpolated by the opti-
cal ﬂow estimates, however, the corresponbdence between
cameras are achieved through the L1 norm penalization.

5. Comparative Experiments

The setting of our experiment is as follows: Three cam-
eras are placed approximately on the same horizon. One
camera (camera 3) is placed in frontal direction to the
seat, and the other two are with about +42o (camera 2) and
−37o (camera 1) deviations from the frontal direction. The
experiment is conducted with a person sitting still on a chair
with the head turning from right(−50o ) to left(+80o ) and
then to the front(+40o ) without much translational move-
ment. The time lapse between consecutive frames in each
camera is half a second, the resolution of the cameras is
320x240 pixels2 .
The result of the estimated relative angular difference to
the face between cameras is shown in Fig. 7. The mean
and the STD of the estimates in camera 1 are −37.25o and
17.19o , and those of camera 2 are +41.25o and 6.19o . Both
of the time-averaged estimates are close to the ground truth.
Four examples of the reconstructed face based on these esti-
mates are shown in Fig. 8. The example in (c) fails to match
well in the nose trail region due to an under-estimation of
the relative angular differnce to the face between the cam-
eras. In the example in (d), one should notice that the left
and the right ears are not in the same horizon, indicating
that the in-node signal processing fails to capture the head
geometry in ﬁtting an ellipse to the head region, which in
turn deteriorates face matching performance.
The results of the collaborative face orientation estima-
tion are shown in Fig. 10 and Fig. 11, with camera loca-
tions assumed known and unknown, respectively. The dot-
ted lines in the ﬁgures show the ground truth face orien-
tation at each time instance. Although the estimation is
degraded without camera location known in prior, the es-
timates without prior knowledge of the location still predict
the face orientation in an acceptable level.

6. Conclusions

In this project, the face model reconstruction and anal-
ysis problem is approached in a networked camera setting.
Based on the distributed nature of the network, we propose
a spatiotemporal feature fusion framework to address the
problem which involves two aspects. First, a collaborative
technique for head strip matching based on a Markov model
and a Viterbi-like algorithm predicts the relative angular dif-
ferences to the face between cameras and reconstructs a face
model without having to know camera locations in prior.
Second, spatiotemporal collaboration is embodied through
identi ﬁcation and exchanging of key frames, and the de-
sign of the state transition matrix and cost function in a re-

3
m
a
C
 
d
n
a
 
a
r
e
m
a
c
 
e
h
t
 
n
e
e
w
t
e
b
 
e
c
a
f
 
e
h
t
 
o
t
 
.
f
f
i
d
 
r
a
l
u
g
n
a
 
.
t
s
E

80

60

40

20

0

−20

−40

−60

−80

Cam1
Cam2
Cam3

Hair−Face ratio Estimation

Cam1 Estimation
Cam2 Estimation
Cam3 Estimation

150

100

50

0

−50

)
e
e
r
g
e
d
(
 
n
o
i
t
a
t
n
e
i
r
O
 
d
e
t
a
m
i
t
s
E

1

2

3

4

5
frame #

6

7

8

9

Figure 7. Estimated relative angular differences to the face be-
tween the cameras.

−100

1

2

3

4

5
frame #

6

7

8

9

Figure 10. Face orientation estimation by hair-face ratio estimates.

(a)

(c)

(b)

(d)

Figure 8. Examples of the reconstructed head model. (a) and (b)
Successful examples, (c) Fair example, (d) Unsuccessful example.

Optical Flow Estimation

Cam1 Head
Cam2 Head
Cam3 Head
Cam1 Body
Cam2 Body
Cam3 Body

)
e
m
a
r
f
 
e
v
i
t
u
c
e
s
n
o
c
/
e
e
r
g
e
d
(
 
ω

60

40

20

0

−20

−40

−60

1

2

3

4

5

6

7

8

frame #

Figure 9. Face angular motion estimation by optical ﬂow esti-
mates.

inforcement learning algorithm. Comparative experiments
with and without spatiotemporal collaboration indicate that
the proposed technique can successfully predict the face ori-
entation within an acceptable level without prior camera lo-
cation information.

7. Acknowledgement

I would like to thank Prof. Hamid Aghajan for his help
I consulted him for the design of the
with this project.
framework on the data fusion between cameras.
I would

Spatiotemporal Collaborative Estimatation without Camera Location Known
150

Cam1 Estimation
Cam2 Estimation
Cam3 Estimation

)
e
e
r
g
e
d
(
n
o
i
t
a
t
n
e
i
r
O
 
d
e
t
a
m
i
t
s
E

100

50

0

−50

−100

1

2

3

4

5
frame #

6

7

8

9

Figure 11. Spatiotemporal Face orientation estimation without
knowing camera locations.

also like to thank Chen Wu for her contribution on the real-
time optical ﬂow algorithm and her idea on the framework.
Furthermore, I would like to thank Prof. Andrew Ng and
Daniel Chavez whom I consulted for project direction and
machine learning algorithms.

References

[1] Q. Chen, H. Wu, T. Fukumoto, and M. Yachida. 3d head pose
estimation without feature tracking. In IEEE Conference on
FGR, 1998. 2
[2] D. Kurata, Y. Nankaku, K. Tokuda, T. Kitamura, and
Z. Ghahramani. Face recognition based on separable lattice
hmms. In Proc. of ICASSP, 2006. 1
[3] B. Kwolek. Face tracking system based on color, stereovision
and elliptical shape features. In IEEE Conference on AVSS,
2003. 2
[4] C. Liu and H. Wechsler. Enhanced ﬁsher linear discriminant
models for face recognition.
In Proc. of ICPR, volume 2,
pages 1368–1372, 1998. 1
[5] F. Liu, Q. Liu, and H. Lu. Robust color-based tracking. In
Proc. of Third Conf. on Image and Graphics, 2004. 2
[6] M. Turk and A. Portland. Eigenfaces for recognition. J. Cog-
nition Nueralscience, 3(1):71–86, 1991. 1


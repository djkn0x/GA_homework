On Transductive Regression

Corinna Cortes
Google Research
76 Ninth Avenue
New York, NY 10011
corinna@google.com

Mehryar Mohri
Courant Institute of Mathematical Sciences
and Google Research
251 Mercer Street
New York, NY 10012
mohri@cs.nyu.edu

Abstract

In many modern large-scale learning applications, the amount of unlabeled data
far exceeds that of labeled data. A common instance of this problem is the trans-
ductive setting where the unlabeled test points are known to the learning algo-
rithm. This paper presents a study of regression problems in that setting.
It
presents explicit VC-dimension error bounds for transductive regression that hold
for all bounded loss functions and coincide with the tight classiﬁcation bounds of
Vapnik when applied to classiﬁcation. It also presents a new transductive regres-
sion algorithm inspired by our bound that admits a primal and kernelized closed-
form solution and deals efﬁciently with large amounts of unl abeled data. The
algorithm exploits the position of unlabeled points to locally estimate their labels
and then uses a global optimization to ensure robust predictions. Our study also
includes the results of experiments with several publicly available regression data
sets with up to 20,000 unlabeled examples. The comparison with other transduc-
tive regression algorithms shows that it performs well and that it can scale to large
data sets.

1 Introduction

In many modern large-scale learning applications, the amount of unlabeled data far exceeds that of
labeled data. Large amounts of digitized data are widely available but the cost of labeling is often
prohibitive since it typically requires human assistance. Semi-supervised learning or transductive
inference leverage unlabeled data to achieve better predictions and are thus particularly relevant to
modern applications. Semi-supervised learning consists of using both labeled and unlabeled data
to ﬁnd a hypothesis that accurately labels unseen examples. Transductive inference uses the same
information but only aims at predicting the labels of the known unlabeled examples.
This paper deals with regression problems in the transductive setting, which arise in a variety of
contexts. This may be to predict the real-valued labels of the nodes of a known graph in compu-
tational biology, or the scores associated to known documents in information extraction problems.
The problem of transduction inference was originally formulated and analyzed by Vapnik [1982]
who described it as a simpler task than the traditional induction treated in machine learning. A
number of recent publications have dealt with the topic of transductive inference [Vapnik, 1998,
Joachims, 1999, Bennett and Demiriz, 1998, Chapelle et al., 1999, Graepel et al., 1999, Schuurmans
and Southey, 2002, Corduneanu and Jaakkola, 2003, Zhu et al., 2004, Lanckriet et al., 2004, Der-
beko et al., 2004, Belkin et al., 2004, Zhou et al., 2005]. But, with the exception of [Chapelle et al.,
1999], [Schuurmans and Southey, 2002], and [Belkin et al., 2004], this work has primarily dealt
with classiﬁcation problems.
We present a speciﬁc study of transductive regression. We gi ve new error bounds for transductive
regression that hold for all bounded loss functions and coincide with the tight classiﬁcation bounds of
Vapnik [1998] when applied to classiﬁcation. Our results al so include explicit VC-dimension bounds
for transductive regression. This contrasts with the original regression bound given by Vapnik [1998]
which assumes a speciﬁc condition of global regularity on th e class of functions and is based on a
complicated and implicit function of the samples sizes and the con ﬁdence parameter. As stated by
Vapnik [1998], this function must be “tabulated by a compute r ”.

We also present a new algorithm for transductive regression inspired by our bound which ﬁrst ex-
ploits the position of unlabeled points to locally estimate their labels, and then uses a global opti-
mization to ensure robust predictions. We show that our algorithm admits both a primal and a ker-
nelized closed-form solution. Existing algorithms for the transductive setting require the inversion
of a matrix whose dimension is either the total number of unlabeled and labeled examples [Belkin
et al., 2004], or the total number of unlabeled examples [Chapelle et al., 1999]. This may be pro-
hibitive for many real-world applications with very large amounts of unlabeled examples. One of the
original motivations for our work was to design algorithms dealing precisely with such situations.
When the dimension of the feature space N is not too large, our algorithm provides a very efﬁcient
solution whose cost is dominated by the construction and inversion of an N × N -matrix. Similarly,
when the number of training points m is small compared to the number of unlabeled points, using
an empirical kernel map, our algorithm requires only constructing and inverting an m × m-matrix.
Our study also includes the results of our experiments with several publicly available regression data
sets with up to 20,000 unlabeled examples, limited only by the size of the data sets. We compared
our algorithm with those of Belkin et al. [2004] and Chapelle et al. [1999], which are among the
very few algorithms described in the literature dealing speciﬁcally with the problem of transductive
regression. The results show that our algorithm performs well in several data sets compared to these
algorithms and that it can scale to large data sets.
The paper is organized as follows. Section 2 describes in more detail the transductive regression
setting we are studying. New generalization error bounds for transductive regression are presented
in Section 3. Section 4 describes and analyzes both the primal and dual versions of our algorithm
and the experimental results of our study are reported in Section 5.

2 Deﬁnition of the Problem

Assume that a full sample X of m + u examples is given. The learning algorithm further receives
the labels of a random subset of X of size m which serves as a training sample:
(1)
(x1 , y1 ), . . . , (xm , ym) ∈ X × R.
The remaining u unlabeled examples, xm+1 , . . . , xm+u ∈ X , serve as test data. The learning
problem that we consider consists of predicting accurately the labels ym+1 , . . . , ym+u of the test
examples. No other test examples will ever be considered. This is a transduction regression problem
[Vapnik, 1998].1 It differs from the standard (induction) regression estimation problem by the fact
that the learning algorithm is given the unlabeled test examples beforehand. Thus, it may exploit
that information and achieve a better result than via the standard induction.
In what follows, we consider a hypothesis space H of real-valued functions for regression estima-
tion. For a hypothesis h ∈ H , we denote by R0 (h) its mean squared error on the full sample, by
bR(h) its error on the training data, and by R(h) the error of h on the test examples:
m+uXi=m+1
m+uXi=1
mXi=1
1
1
1
bR(h) =
(h(xi )−yi )2 .
(h(xi )−yi )2
(h(xi )−yi )2 R(h) =
R0 (h) =
m + u
m
u
(2)
For convenience, we will sometimes denote by yx = yi the label of a point x = xi ∈ X .
3 Transductive Regression Generalization Error

This section presents explicit generalization error bounds for transductive regression.
Vapnik [1998] introduced and analyzed the problem of transduction and presented transductive in-
ference bounds for both classiﬁcation and regression. His r egression bound assumes however a
speciﬁc regularity condition on the hypothesis functions l eading in particular to a surprising bound
where no error on the training data implies zero generalization error. The bound has the multiplica-
tive form: R(h) ≤ Ω(m, u, d, δ) bR(h), where d is the VC-dimension of the class of hypotheses used
and δ is the con ﬁdence parameter. Furthermore, for certain value s of the parameters, for example
larger ds or smaller δs, Ω becomes in ﬁnite and the bound is ineffective [Vapnik, 1998, page 349]. Ω
is also based on a complicated and implicit function of m, u, and δ , which makes its interpretation
difﬁcult. For example, it is hard to analyze the asymptotic b ehavior of the bound for large u.

1This is in fact one of the two transduction settings discussed by [Vapnik, 1998], but, under some general
conditions, the results proved with this setting carry over to the other.

,

(3)

Instead, our bounds simply hold for general bounded loss functions and, when applied to classiﬁca-
tion, coincide with the tight classiﬁcation bounds of Vapni k [1998]. Our results also include explicit
VC-dimension bounds for transductive regression. To the best of our knowledge, these are the ﬁrst
general explicit bounds for transductive regression.
Our ﬁrst bound uses the function ¯Γ de ﬁned as follows. Let Γ(ǫ, k) be de ﬁned by:
∀ǫ ≥ 0, ∀k ∈ N, uǫ ≤ k ≤ m(1 − ǫ) + u, Γ(ǫ, k) = Xr∈I (m,u,ǫ) (cid:0)k
r(cid:1)(cid:0)m+u−k
m−r (cid:1)
(cid:0)m+u
m (cid:1)
where I (m, u, k , ǫ) is the set of integers r such that: k−r
u − r
m > ǫ and max(0, k − u) ≤ r ≤
min(m, k). Γ(ǫ, k) represents the probability of observing a difference in error rate of more than
ǫ between the training and test set when the total number of errors is k (see [Cortes and Mohri,
2006]). Then ¯Γ is de ﬁned as ¯Γ(ǫ) = maxk Γ(q k
m+u ǫ, k). ¯Γ is used in the transductive classi-
ﬁcation bound of Vapnik [1998] (see [Cortes and Mohri, 2006] [Theorem 2]). [Cortes and Mohri,
2006][Corollary 2] gives an upper bound on ¯Γ.
For any subset X ′ ⊆ X , any non-negative real number t ≥ 0, and hypothesis h ∈ H , let Θ(h, t, X ′ )
denote the fraction of the points xi ∈ X ′ , i = 1, . . . , k , such that (h(xi ) − yi )2 − t > 0. Thus,
Θ(h, t, X ′ ) represents the error rate over the sample X ′ of the classiﬁer that associates to a point x
the value zero if (h(x) − yx )2 ≤ t, one otherwise.
Two classiﬁers associated in this way to Θ(h, t, X ) and Θ(h′ , t′ , X ) can be viewed as equivalent if
they label X in an identical way. Since X is ﬁnite, there is a ﬁnite number of equivalence classes of
such classiﬁers, we will denote that number by N (m + u).
Theorem 1 Let δ > 0, and let ǫ0 > 0 be the minimum value of ǫ such that N (m + u) ¯Γ(ǫ) ≤ δ , and
assume that the loss function is bounded: for all h ∈ H and x ∈ X , (h(x) − yx )2 ≤ B 2 , where
B ∈ R+ . Then, with probability at least 1 − δ , for all h ∈ H ,
+ ǫ0Bs bR(h) + (cid:18) uǫ0B
2(m + u) (cid:19)2
0B 2
uǫ2
R(h) ≤ bR(h) +
2(m + u)
For any h ∈ H , let R1 (h) be de ﬁned by:
R1 (h) = Z B 2
0 pΘ(h, t, X ) dt.
By the Cauchy-Schwarz inequality,
R1 (h) ≤  Z B 2
Θ(h, t, X ) dt!1/2  Z B 2
1dt!1/2
= B  Z B 2
Θ(h, t, X ) dt!1/2
0
0
0
Let D denote the uniform probability distribution associated to the sample X . Thus, D(x) = 1
m+u
for all x ∈ X . Let Prx∼D [Ex ] denote the probability of event Ex when x is randomly drawn
according to D . By de ﬁnition of R0 and the Lebesgue integral, for all h ∈ H ,
[(h(x) − yx )2 > t] dt = Z B 2
(h(x) − yx )2D(x) dx = Z ∞
R0 (h) = ZX
Θ(h, t, X ) dt.
Pr
x∼D
0
0
Similarly, setting Xm = {xi ∈ X : i ∈ [1, m]} and Xu = {xi ∈ X : i ∈ [m + 1, m + u]}, we have
and R(h) = Z B 2
bR(h) = Z B 2
(8)
Θ(h, t, Xu ) dt.
Θ(h, t, Xm) dt
0
0
In view of Equation 7, Inequality 6 can be rewritten as: R1 (h) ≤ BpR0 (h). By [Cortes and Mohri,
2006][Theorem 2], for all ǫ > 0 and for any t ≥ 0,
Θ(h, t, Xu ) − Θ(h, t, Xm )
> ǫ] ≤ N (m + u) ¯Γ(ǫ).
(9)
Pr[ sup
pΘ(h, t, X )
h∈H

Proof.

.

(6)

.

(4)

(5)

(7)

1
n

(11)

(12)

(10)

Fix ǫ > 0. Then, with probability at least 1 − N (m + u) ¯Γ(ǫ), for all integers n > 1 and i ≥ 0,
Θ(h, iB 2
n , Xu ) − Θ(h, iB 2
n , Xm )
≤ ǫ.
qΘ(h, iB 2
n , X )
Then, the convergence of the Riemann sums to the integral ensures that
nXi=0
nXi=0
iB 2
iB 2
1
R(h) − bR(h) = lim
, Xm )
, Xu ) −
Θ(h,
Θ(h,
n
n
n
n→∞
nXi=0 rΘ(h,
, X ) = ǫR1 (h) ≤ ǫBpR0 (h).
iB 2
1
≤ ǫ lim
n
n
n→∞
Let δ > 0 and select ǫ = ǫ0 as the minimum value of ǫ such that N (m + u) ¯Γ(ǫ) ≤ δ , then with
probability at least 1 − δ ,
R(h) − bR(h) ≤ ǫ0BpR0 (h).
(13)
Plugging in the following expression of R0 (h) with respect to R(h) and bR(h)
u
m
m + u bR(h) +
R(h),
R0 (h) =
m + u
and solving the second-degree equation in R(h) yields directly the statement of the theorem.
Theorem 1 provides a general bound on the regression error within the transduction setting. The
theorem can also be used to derive a bound in the classiﬁcatio n case by simply setting B = 1. The
resulting bound coincides with the tight classiﬁcation bou nd given by Vapnik [1998]. The bound
given by Theorem 1 depends on the function ¯Γ and is implicit. The following provides a general and
explicit error bound for transduction regression directly expressed in terms of the empirical error,
the number of equivalence N (m + u) or the VC-dimension d, and the sample sizes m and u.
Corollary 1 Let H be a set of hypotheses with VC-dimension d. Assume that the loss function is
bounded: for all h ∈ H and x ∈ X , (h(x) − yx )2 ≤ B 2 , where B ∈ R+ . Then, with probability at
least 1 − δ , for all h ∈ H ,
+ αBs bR(h) + (cid:18) uαB
2(m + u) (cid:19)2
uα2B 2
R(h) ≤ bR(h) +
2(m + u)
δ (cid:1) ≤ r 2(m+u)
with α = q 2(m+u)
mu (cid:16)d log (m+u)e
δ (cid:17).
(cid:0)log N (m + u) + log 1
d + log 1
mu
Proof. By Theorem 1, Inequality 15 holds for all α > 0 such that N (m + u) ¯Γ(α) ≤ δ . By [Cortes
and Mohri, 2006][Corollary 2], log (cid:0)N (m + u) ¯Γ(α)(cid:1) ≤ log N (m + u) − 1
mu
m+u α2 . Setting log δ
2
to match this upper bound yields the expression of α given above. Since N (m + u) is bounded by
the shattering coefﬁcient of H of order m + u, by Sauer’s lemma, log N (m + u) ≤ d log (m+u)e
.
d
This gives the upper bound on α in terms of the VC-dimension.

(14)

,

(15)

The bound is explicit and can be readily used within the Structural Risk Minimization (SRM) frame-
work, either by using the expression of α in terms of the VC-dimension, or the tighter expression
with respect to the number of equivalence classes N . In the latter case, a structure of increasing
number of equivalence classes can be constructed as in [Vapnik, 1998, page 360]. A more practical
algorithm inspired by these concepts is described in the next section.

4 Transductive Regression Algorithm

This section presents an algorithm for the transductive regression problem.
Before presenting this algorithm, let us ﬁrst emphasize tha t the algorithms introduced for transduc-
tive classiﬁcation problems, e.g., transductive SVMs [Vap nik, 1998, Joachims, 1999], cannot be
readily used for regression. These algorithms typically select the hypothesis h, out of a hypothesis
space H , that minimizes the following optimization function
uXi=1
mXi=1
L (cid:0)h(xm+i ), y∗
m+i(cid:1) ,
L (h(xi ), yi ) + C ′ 1
u

min
m+i ,i=1,...,u
y∗

Ω(h) + C

1
m

(16)

where Ω(h) is a capacity measure term, L is the loss function used, C ≥ 0 and C ′ ≥ 0 regularization
m+u for the test
parameters, and where the minimum is taken over all possible labels y∗
m+1 , . . . , y∗
points. In regression, this scheme would lead to a trivial solution not exploiting the transduction
Indeed, let h0 be the hypothesis minimizing the ﬁrst two terms, that is the s olution of
setting.
m+i = h0 (xm+i ), i = 1, . . . , u, the third term
the induction problem. For the particular choice y∗
vanishes. Thus, h0 is also minimizing the sum of all three terms.
In two-group classiﬁcation,
the trivial solution is typically not the solution of the minimization problem because in general
h0 (xm+i ) is not in {0, 1}.
The main idea behind the design of our algorithm is to exploit the additional information provided in
transduction, that is the position of the unlabeled examples. Our algorithm has two stages. The ﬁrst
stage is based on the position of unlabeled points. For each unlabeled point xi , i = m+1, . . . , m+u,
a local estimate label ¯yi is determined using the labeled points in the neighborhood of xi . In the
second stage, a global hypothesis h is found that best matches all labels, those of the training data
and the estimate labels ¯yi .
This second stage is critical and distinguishes our method from other suggested ones. While using
local information to determine labels is important (see for example the discussion of Vapnik [1998]),
it is not sufﬁcient for a robust prediction. A global estimat e of all labels is needed to make predictions
less vulnerable to noise.

4.1 Local Estimates
Let Φ be a feature mapping from X to a vector space F provided with a norm. We ﬁx a radius
r ≥ 0 and consider for all x′ ∈ Xu , the ball of radius r centered in Φ(x′ ), denoted by B (Φ(x′ ), r).
This de ﬁnes the neighborhood of the image of each unlabeled p oint. A single radius r is used for all
neighborhoods to limit the number of parameters for the algorithm. Labeled points x ∈ Xm whose
images Φ(x) fall within the neighborhood of Φ(x′ ), x′ ∈ Xu , help determine an estimate label of x′ .
With a very large radius r, the labels of all training examples contribute to the de ﬁni tion of the local
estimates. But, with smaller radii, only a limited number of computations are needed. When no such
labeled point exists in the neighborhood of x′ ∈ Xu , which depends on the radius r selected, x′ is
disregarded in both training stages of the algorithm.
There are many possible ways to de ﬁne the estimate label of x′ ∈ Xu based on the neigh-
borhood points. One simple way consists of de ﬁning it as the w eighted average of the neigh-
borhood labels yx , where the weights may be de ﬁned as the inverse of distances o f Φ(x) to
Φ(x′ ), or as similarity measures K (x, x′ ) when a positive de ﬁnite kernel K is associated to Φ.
Thus, when the set of labeled points with images in the neighborhood of Φ(x′ ) is not empty,
I = {i ∈ [1, m] : Φ(xi ) ∈ B (Φ(x′ ), r)} 6= ∅, the estimate label ¯yx′ of x′ ∈ Xu can be given by:
¯yx′ = Xi∈I
wi ¯yiPi wi
with w−1
or wi = K (x′ , xi ).
(17)
i = kΦ(x′ ) − Φ(xi )k ≤ r
The estimate labels can also be obtained as the solution of a local linear or kernel ridge regression,
which is what we used in most of our experiments.
In practice, with a relatively small radius r, the computation of an estimated label ¯yi depends only
on a limited number of labeled points and their labels, and is quite efﬁcient.

4.2 Global Optimization

The second stage of our algorithm consists of selecting a hypothesis h that ﬁts best the labels of
the training points and the estimate labels provided in the ﬁ rst stage. As suggested by Corollary 1,
hypothesis spaces with a smaller number of equivalence classes guarantee a better generalization
error. The bound also suggests reducing the empirical error. This leads us to consider the following
objective function
m+uXi=m+1
mXi=1
(h(xi ) − ¯yi )2 ,
where h is as a linear function with weight vector w ∈ F : ∀x ∈ X , h(x) = w · Φ(x), and where
C ≥ 0 and C ′ ≥ 0 are regularization parameters. The ﬁrst two terms of the obj ective function
coincide with those used in standard (kernel) ridge regression. The third term, which restricts the
estimate error, can be viewed as imposing a smaller number of equivalence classes on the hypothesis
space as suggested by the error bound of Corollary 1. The constraint explicitly exploits knowledge

(h(xi ) − yi )2 + C ′

G = ||w||2 + C

(18)

about the location of all the test points, and limits the range of the hypothesis at these locations,
thereby reducing the number of equivalence classes. Our algorithm can be viewed as a generalization
of (kernel) ridge regression to the transductive setting. In the following, we will show that this
generalized optimization problem admits a closed-form solution and a natural kernel-based solution.

4.2.1 Primal solution
Let N be the dimension of the feature space and let W ∈ RN ×1 denote the column matrix whose
components are the coordinates of w, Y ∈ Rm×1 the column matrix whose components are the
labels yi of the training examples, and Y′ ∈ Ru×1 the column-matrix whose components are the
estimated labels ¯yi of the test examples. Let X = [Φ(x1 ), . . . , Φ(xm )] ∈ RN ×m denote the matrix
whose columns are the components of the images by Φ of the training examples, and similarly
X′ = [Φ(xm+1 ), . . . , Φ(xm+u )] ∈ RN ×u the matrix corresponding to the test examples. G can
then be rewritten as:
G = kWk2 + C kX⊤W − Yk2 + C ′ kX′⊤W − Y′ k2 .
(19)
G is convex and differentiable and its gradient is given by
(20)
∇G = 2W + 2C X(X⊤W − Y) + 2C ′ X′ (X′⊤W − Y′ ).
The matrix W minimizing G is the unique solution of ∇G = 0. Since (IN + C XX⊤ + C ′ X′X′⊤ )
is invertible, it is given by the following expression
W = (IN + C XX⊤ + C ′ X′X′⊤ )−1 (C XY + C ′ X′Y′ ).
(21)
This gives a closed-form solution in the primal space based on the inversion of a matrix in RN ×N .
Let T (N ) be the time complexity of computing the inverse of a matrix in RN ×N . T (N ) = O(N 3 )
using standard methods or T (N ) = O(N 2.376 ) with the method of Coppersmith and Winograd. The
time complexity of the computation of W from X, X′ , Y, and Y′ is thus in O(T (N ) + (m + u)N 2).
When the dimension N of the feature space is small compared to the number of examples m + u,
which is typical in modern learning applications where u is large, this method remains practical and
leads to a very efﬁcient computation. The use of the so-calle d empirical kernel map [Sch ¨olkopf and
Smola, 2002] also makes this method very attractive. Given a kernel K , the empirical kernel feature
vector associated to x is the m-dimensional vector Φ(x) = [K (x, x1 ), . . . , K (x, xm )]⊤ . Thus, the
dimension of the feature space is then N = m. For relatively small m, even for very large values of
u with respect to m, the solution is efﬁciently computable and yet bene ﬁts from the use of kernels.
This computational advantage is not shared by other methods such as the manifold regularization
techniques [Belkin et al., 2004], or even by the regression technique described by [Chapelle et al.,
1999], despite it is based on a primal method (we have derived a dual version of that method as well,
see Section 5) since it requires among other things the inversion of a matrix in Ru×u .

Once W is computed, prediction can be done by computing X′⊤W in time O(uN ).

4.2.2 Dual solution

The computation can also be done in the dual space, which is useful in the case of very high-
dimensional feature spaces. Let MX ∈ RN ×(m+u) and MY ∈ R(m+u)×1 be the matrices de ﬁned
by:
MX = (cid:0)√C X √C ′ X′ (cid:1) MY = (cid:18) √C Y√C ′ Y′ (cid:19) .
(22)
Then, Equation 21 can be rewritten as: W = (IN + MXM⊤
X )−1MXMY . To determine the dual
solution, observe that
MX + γ Im+u )−1M⊤
X + γ IN )−1 = (M⊤
(23)
X (MXM⊤
M⊤
X ,
X
where Im+u denotes the identity matrix of R(m+u)×(m+u) . This can be derived without difﬁculty
from a series expansion of (MXM⊤
X + γ IN )−1 . Thus, W can also be computed via:
W = MX (Im+u + K)−1MY ,
(24)
where K is the Gram matrix K = M⊤
MX . Let K21 ∈ Ru×m and K22 ∈ Ru×u be
X
the sub-matrices of the Gram K de ﬁned by: K21 = (K (xm+i , xj )1≤i≤u,1≤j≤m ) and K22 =
(K (xm+i , xm+j )1≤i,j≤u ) and let K2 ∈ Ru×(m+u) be the matrix de ﬁned by:
K2 = (cid:0)√C K21 √C ′ K22 (cid:1) = X′⊤MX .

(25)

Dataset
Boston Housing [13]

California Housing [8]

kin-32fh [32]

Elevators [18]

No. of unlab.
points
25
500
2,500
5,000
20,000
2,500
8,000
500
2500
15,000

Relative improvement in MSE (%)
Our algorithm Chapelle et al. [1999]
Belkin et al. [2004]
2.4±5.4
4.3±11.3
20.2±14.7
3.9±12.3
2.7±3.0
8.4±6.9
0.0±0.0
0.2±0.3
25.9±8.3
17.2±8.7
0.0±0.0
0.0±0.0
22.0±11.0
9.4±3.7
18.4±5.9
14.4±10.4
9.0±6.9
9.7±5.8

2.7±3.1
0.9±0.7
2.6 ±7.7
0.0±0.0

2.2±2.6
0.5±0.5
1.5±2.7
2.2±2.9

Table 1: Transductive regression experiments. The number in brackets after the name indicates the input
dimensionality of the data set. The number of training examples was m = 481 for the Boston Housing data
set, m = 25 for the other tasks. The number of unlabeled examples was u = 25 for the Boston Housing data
set and varied from u = 500 to the maximum of 20,000 examples for the California Housing data set. For
u ≥ 10,000, the algorithms of Chapelle et al. [1999] and Belkin et al. [2004] did not terminate within the time
period of our experiments.

Then, predictions can be made using kernel functions alone since X ′⊤W can be computed by:

X′⊤W = X′⊤MX (Im+u + K)−1MY = K2 (Im+u + K)−1MY .
(26)
When the dimension of the feature space N is very large with respect to the total number of exam-
ples, this can lead to a faster computation of the solution. (Im+u + K)−1MY can be computed in
O(T (m + u) + (m + u)2 tK ) and predictions are computed in time O(u (m + u)), where tK is the
time complexity of the computation of K (x, x), x, x′ ∈ X . As already pointed in the description of
the local estimates, in practice, some unlabeled points are disregarded in the training phases because
no labeled point falls in their neighborhood. Thus, instead of u, a smaller number of unlabeled
examples u′ ≤ u determines the computational cost.
5 Experimental Results

This section reports the results of our experiments with the transductive regression algorithm just
presented with several data sets. For comparison, we also implemented the algorithm of Chapelle
et al. [1999] and that of Belkin et al. [2004], which are among the very few algorithms described
in the literature dealing speciﬁcally with the problem of tr ansductive regression. For the algorithm
of Chapelle et al. [1999], we in fact derived and implemented a dual solution not described in the
original paper. With the notation used in that paper, it can be shown that
C = I − ˆK ˆK⊤ ( ˆK ˆK⊤ + γ I)−1 .
(27)
Our comparisons were made using several publicly available regression data sets: Boston Housing,
kin-32fh a data set in the Kinematics family with high unpredictability or noise, California Housing,
and Elevators [Torgo, 2006]. For the Boston Housing data set, we used the same partitioning of the
training and test sets as in [Chapelle et al., 1999]: 481 training examples and 25 test examples. The
input variables were normalized to have mean zero and a variance one. For the kin-32fh, California
Housing, and Elevators data sets, 25 training examples were used with varying (large) amounts of
test examples: 2,500 and 8,000 for kin-32fh; from 500 up to 20,000 for California Housing; and
from 500 to 15,000 for Elevators. The experiments were repeated for 100 random partitions of
training and test sets.
The kernels used with all algorithms were Gaussian kernels. To measure the improvement produced
by the transductive inference algorithms, we used kernel ridge regression as a baseline. The optimal
values for the width of the Gaussian σ and the ridge 1
C were determined using cross-validation.
These parameters were then ﬁxed at these values. The remaini ng parameters for our algorithm, r
and C ′ , were determined using a grid search and cross-validation. The parameters of the algorithms
of Chapelle et al. [1999] and Belkin et al. [2004] were determined in the same way. Alternatively, the
parameters could be selected using the explicit VC-dimension generalization bound of Corollary 1.
For our algorithm, we found the best values of r to be typically among the 2.5% smallest distances
between training and test points. Thus, each estimate label was determined by only a small number
of labeled points.
For our algorithm, we experimented both with the dual solution using Gaussian kernels, and the
primal solution with an empirical Gaussian kernel map as described in Section 4.2.1. The results

—
—
—
—
obtained were very similar, however the primal method was dramatically faster since it required the
inversion of relatively small-dimensional matrices even for a large number of unlabeled examples.
For consistency, all the results reported for our method relate to the dual solution, except from those
with very large u, e.g. u ≤ 10,000, where the dual method was too time-consuming.
Table 1 shows the results of our experiments. For each data set and each algorithm, the relative
improvement in mean squared error (MSE) with respect to the baseline averaged over the random
partitions is indicated, followed by its standard deviation. Some improvements were small or not
statistically signiﬁcant. In general, we observed no signi ﬁcant performance improvement over the
baseline on any of these data sets using the Laplacian regularized least squares method of Belkin
et al. [2004]. We note that, while positive classiﬁcation re sults have been previously reported for this
algorithm, no transductive regression experimental result seems to have been published for it. Our
results for the method of Chapelle et al. [1999] match those reported by the authors for the Boston
Housing data set (both absolute and relative MSE).
Our algorithm achieved a signiﬁcant improvement of the MSE i n all data sets and for different
amounts of unlabeled data and was shown to be practical for large data sets of 20,000 test examples.
This matches many real-world situations where amount of unlabeled data is orders of magnitude
larger than that of labeled data.

6 Conclusion

We presented a general study of transductive regression. We gave new and general explicit error
bounds for transductive regression and described a simple and general algorithm inspired by our
bound that can scale to relatively large data sets. The results of experiments show that our algorithm
achieves a smaller error in several tasks compared to other previously published algorithms for
transductive regression.
The problem of transductive regression arises in a variety of learning contexts, in particular for
learning node labels of a very large graphs such as the web graph. This leads to computational
problems that may require approximations or new algorithms. We hope that our study will be useful
for dealing with these and other similar transduction regression problems.

References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization; a geometric framework for
learning from examples. Technical Report TR-2004-06, University of Chicago, 2004.
Kristin Bennett and Ayhan Demiriz. Semi-supervised support vector machines. NIPS 11, pages 368–374, 1998.
Olivier Chapelle, Vladimir Vapnik, and Jason Weston. Transductive Inference for Estimating Values of Func-
tions. NIPS 12, pages 421–427, 1999.
Adrian Corduneanu and Tommi Jaakkola. On information regularization.
In Christopher Meek and Uffe
Kj ærulff, editors, Proceedings of the Nineteenth Annual Conference on Uncertainty in Arti ﬁcial Intelligence ,
pages 151–158, 2003.
Corinna Cortes and Mehryar Mohri. On Transductive Regression. Technical Report TR2006-883, Courant
Institute of Mathematical Sciences, New York University, November 2006.
Philip Derbeko, Ran El-Yaniv, and Ron Meir. Explicit learning curves for transduction and application to
clustering and compression algorithms. J. Artif. Intell. Res. (JAIR), 22:117–142, 2004.
Thore Graepel, Ralf Herbrich, and Klaus Obermayer. Bayesian transduction. NIPS 12, 1999.
Thorsten Joachims. Transductive inference for text classi ﬁcation using support vector machines. In Ivan Bratko
and Saso Dzeroski, editors, Proceedings of ICML-99, 16th International Conference on Machine Learning,
pages 200–209. Morgan Kaufmann Publishers, San Francisco, US, 1999.
Gert R. G. Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I. Jordan. Learning the
kernel matrix with semideﬁnite programming.
J. Mach. Learn. Res., 5:27–72, 2004. ISSN 1533-7928.
Bernhard Sch ¨olkopf and Alex Smola. Learning with Kernels. MIT Press: Cambridge, MA, 2002.
Dale Schuurmans and Finnegan Southey. Metric-Based Methods for Adaptive Model Selection and Regular-
ization. Machine Learning, 48:51–84, 2002.
Lu´ıs Torgo. Regression datasets, 2006. http://www.liacc .up.pt/ ltorgo/Regression/DataSets.html.
Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer, Berlin, 1982.
Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.
Dengyong Zhou, Jiayuan Huang, and Bernard Scholkopf. Learning from labeled and unlabeled data on a
directed graph. In L. De Raedt and S. Wrobel, editors, Proceedings of ICML-05, pages 1041–1048, 2005.
Xiaojin Zhu, Jaz Kandola, Zoubin Ghahramani, and John Lafferty. Nonparametric transforms of graph kernels
for semi-supervised learning. NIPS 17, 2004.


MLLE: Modiﬁed Locally Linear Embedding Using
Multiple Weights

Zhenyue Zhang
Department of Mathematics
Zhejiang University, Yuquan Campus,
Hangzhou, 310027, P. R. China
zyzhang@zju.edu.cn

Jing Wang
College of Information Science and Engineering
Huaqiao University
Quanzhou, 362021, P. R. China
Dep. of Mathematics, Zhejiang University
wroaring@yahoo.com.cn

Abstract

The locally linear embedding (LLE) is improved by introducing multiple linearly
independent local weight vectors for each neighborhood. We characterize the
reconstruction weights and show the existence of the linearly independent weight
vectors at each neighborhood. The modi ﬁed locally linear embedding (MLLE)
proposed in this paper is much stable. It can retrieve the ideal embedding if MLLE
is applied on data points sampled from an isometric manifold. MLLE is also
compared with the local tangent space alignment (LTSA). Numerical examples
are given that show the improvement and efﬁciency of MLLE.

1

Introduction

The problem of nonlinear dimensionality reduction is to ﬁnd the meaningful low-dimensional struc-
ture hidden in high dimensional data. Recently, there have been advances in developing effective
and efﬁcient algorithms to perform nonlinear dimension reduction which include isometric mapping
Isomap [7], locally linear embedding (LLE) [5] and its variations, manifold charting [2], Hessian
LLE [1] and local tangent space alignment (LTSA) [9]. All these algorithms cover two common
steps: learn the local geometry around each data point and nonlinearly map the high dimensional
data points into a lower dimensional space using the learned local information [3]. The performances
of these algorithms, however, are different both in learning local information and in constructing
global embedding, though each of them solves an eigenvalue problem eventually. The effectiveness
of the local geometry retrieved determines the efﬁciency of the methods.

This paper will focus on the reconstruction weights that characterize intrinsic geometric properties
of each neighborhood in LLE [5]. LLE has many applications such as image classiﬁcation, image
recognition, spectra reconstruction and data visualization because of its simple geometric intuitions,
straightforward implementation, and global optimization [6, 11]. It is however also reported that
LLE may be not stable and may produce distorted embedding if the manifold dimension is larger
than one. One of the curses that make LLE fail is that the local geometry exploited by the reconstruc-
tion weights is not well-determined, since the constrained least squares (LS) problem involved for
determining the local weights may be ill-conditioned. A Tikhonov regularization is generally used
for the ill conditions LS problem. However, a regularized solution may be not a good approximation
to the exact solution if the regularization parameter is not suitably selected.

The purpose of this paper is to improve LLE by making use of multiple local weight vectors. We will
show the existence of linearly independent weight vectors that are approximately optimal. The local
geometric structure determined by multiple weight vectors is much stable and hence can be used to
improve the standard LLE. The modiﬁed LLE named as MLLE uses multiple weight vectors for each
point in reconstruction of lower dimensional embedding. It can stably retrieve the ideal isometric

||y0||=2.6706e−5

105

r
o
r
r
e

100

105

100

10−5

r
o
r
r
e

||y0||=8.5272e−4

103

100

10−5

r
o
r
r
e

||y0||=1.6107

10−5
10−20

10−10
10−20

100

10−10
10−10
10−10
γ
γ
γ
Figure 1: Examples of (cid:1)w(γ ) − w∗ (cid:1) (solid line) and (cid:1)w(γ ) − u(cid:1) (dotted line) for swiss-roll data.

100

100

10−10
10−20

embedding approximately for an isometric manifold. MLLE has properties similar to LTSA both
in measuring linear dependence of neighborhood and in constructing the (sparse) matrix whose
smallest eigenvectors form the wanted lower dimensional embedding. It exploits the tight relations
between LLE/MLLE and LTSA. Numerical examples given in this paper show the improvement and
efﬁciency of MLLE.

2 The Local Combination Weights
Let {x1 , . . . , xN } be a given data set of N points in Rm . LLE constructs locally linear structures
at each point xi by representing xi using its selected neighbor set Ni = {xj , j ∈ Ji}. The optimal
(cid:1)
(cid:1)
combination weights are determined by solving the constrained least squares problem
min (cid:1)xi −
wj ixj (cid:1),
wj i = 1.
(2.1)
j∈Ji
j∈Ji
Once all the reconstruction weights {wj i , j ∈ Ji }, i = 1, · · · , N , are computed, LLE maps the set
{x1 , . . . , xN } to {t1 , . . . , tN } in a lower dimensional space Rd (d < m) that preserves the local
(cid:1)
(cid:1)
combination properties totally,
wj i tj (cid:1)2 ,
(cid:1)ti −
min
j∈Ji
T =[t1 ,...,tN ]
i

s.t. T T T = I .

s.t.

(cid:2)
The low dimensional embedding T constructed by LLE tightly depends on the local weights. To
formulate the weight vector wi consisting of the local weights wj i , j ∈ Ji , let us denote matrix
error as xi − (cid:2)
Gi = [. . . , xj − xi , . . .]j∈Ji . Using the constraint
wj i = 1, we can write the combination
j∈Ji
wj ixj = Giwi and hence (2.1) reads
j∈Ji
min (cid:1)Giw(cid:1),
= 1,
s.t. wT 1ki
where 1ki denotes the ki -dimensional vector of all 1’s. Theoretically, a null vector of Gi that is not
orthogonal to 1ki can be normalized to be a weight vector as required. Otherwise, a weight vector is
given by wi = yi /1T
i Gi y = 1ki [6]. Indeed, one can
yi with yi a solution to the linear system GT
ki
formulate the solution using the singular value decomposition (SVD) of Gi .
(cid:3)
Theorem 2.1 Let G be a given matrix of k column vectors. Denote by y0 the orthogonal projection
of 1k onto the null space of G and y1 = (GT G)+1k .1 Then the vector
y0 (cid:3)= 0
w∗ = y∗
y∗ =
y0 ,
k y∗ ,
y0 = 0
1T
y1 ,
w=1 (cid:1)Gw(cid:1).
is an optimal solution to min1T
k
The problem of solving min1T w=1 (cid:1)Gw(cid:1) is not stable if GT G is singular (has zero eigenvalues) or
nearly singular (has relative small eigenvalues). To regularize the problem, it is suggested in [5] to
solve the regularized linear system replaced
(GT G + γ (cid:1)G(cid:1)2
F I )y = 1k , w = y/1T
k y
1 (·)+ denotes the Moore-Penrose generalized inverse of a matrix.

(2.2)

(2.3)

||X−Y (1)||=1.277

||X −Y (2)||=0.24936

||X −Y (3)||=0.39941

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0
0
0
0
0.5
1
0
0.5
1
0
0.5
1
Figure 2: A 2D data set (◦-points) and computed coordinates (dot points) by LLE using different
sets of optimal weight vectors (left two panels) or regularization weight vectors (right panel).

with a small positive γ . Let y(γ ) be the unique solution to the regularized linear system. One can
as γ → 0. However, the convergence behavior of
k y(γ ) converges to w∗
prove that w(γ ) = y(γ )/1T
w(γ ) is quite uncertain for small γ > 0. In fact, if y0 (cid:3)= 0 is small, then w(γ ) tends to u = y1
1T y1 at
ﬁrst and then turns to the limit value w∗ = y0
1T y0 eventually. Note that u and w∗
are orthogonal each
other. In Figure 1, we plot three examples of the error curves (cid:1)w(γ )−w∗ (cid:1) (solid line) and (cid:1)w(γ )−u(cid:1)
(dotted line) with different values of (cid:1)y0 (cid:1) for the swiss-roll data. The left two panels show the
metaphase phenomenon clearly, where (cid:1)y0 (cid:1) ≈ 0. Therefore, w∗
can not be well approximated by
w(γ ) if γ is not small enough. This partially explains the instability of LLE.
Other factor that results in the instability of LLE is that the learned linear structure by using single
weight vector at each point is brittle. LLE may give a wrong embedding even if all weight vector is
well approximated in a high accuracy. It is imaginable if Gi is rank reducible since multiple optimal
weight vectors exist in that case. Figure 2 shows a small example of N = 20 two-dimensional points
for which LLE fails even if exact optimal weight vectors are used. We plot three sets of computed 2D
embeddings T (j ) (within an optimal afﬁne transformation to the ideal X ) by LLE with k = 4 using
two sets of exact optimal weight vectors and one set of weight vectors that solve the regularized
equations, respectively. The errors (cid:1)X − Y (j ) (cid:1) = minc,L (cid:1)X − (c1T + LT (j ) )(cid:1) between the ideal
set X and the computed sets within optimal afﬁne transformation are large in the example.
The uncertainty of w(γ ) with small γ occurs because of existence of small singular values of G.
Fortunately, it also implies the existence of multiple almost optimal weight vectors simultaneously.
Indeed, if G has s ≤ k small singular values, then there are s approximately optimal weight vectors
that are linear independent on each others. The following theorem characterizes construction of the
approximately optimal weight vectors w((cid:1)) using the matrix V of left singular vectors correspond-
ing to the s smallest singular values and bounds the combination errors (cid:1)Gw((cid:1)) (cid:1) in terms of the
minimum of (cid:1)Gw(cid:1) and the largest one of the s smallest singular values.
Theorem 2.2 Let G ∈ Rm×k and σ1 (G) ≥ . . . ≥ σk (G) be the singular values of G. Denote
w((cid:1)) = (1 − α)w∗ + V H (:, (cid:4)),
(cid:4) = 1, · · · , s,
where V is the eigenvector matrix of G corresponding to the s smallest right singular values, α =
(cid:1)V T 1k (cid:1), and H is a Householder matrix that satisﬁes H V T 1k = α1s .Then
1√
s
(cid:1)Gw((cid:1)) (cid:1) ≤ (cid:1)Gw∗ (cid:1) + σk−s+1 (G).
(2.4)
The Householder matrix is symmetric and orthogonal. It is given by H = I − 2hhT with vector
h ∈ Rs deﬁned as follows. Let h0 = α1s − V T 1k . If h0 = 0, then h = 0. Otherwise, h = h0(cid:4)h0 (cid:4) .
Note that (cid:1)w∗ (cid:1) can be very large when G is approximately singular. In that case, (1 − α)w∗
domi-
nates w((cid:1)) and hence w(1) , . . . , w(s) are almost same and numerically linear dependent each others.
Equivalently, W = [w(1) , . . . , w(s) ] has large condition number cond(W ) = σmax (W )
σmin (W ) . For numer-
ical stability, we replace w∗
by a regularized weight vector w(γ ) like in LLE. This modi ﬁcation
is quite practical in application and, more importantly, it can reinforce the numerically linear inde-
pendence of {w((cid:1)) }. In our experiment, the construction of the {w((cid:1)) } is stable with respect to the
choice of γ . We show an estimation of the condition number cond(W ) for the modi ﬁed W below.

Theorem 2.3 Let W = (1 − α)w(γ )1T
s + V H . Then cond(W ) ≤ (1 +

√

k(1 − α)(cid:1)w(γ )(cid:1))2 .

3 MLLE: Modiﬁed locally linear embedding

It is justiﬁable to learn the local structure by multiple optimal weight vectors at each point, rather
than a single one. Though the exact optimal weight vector may be unique, multiple approximately
optimal weight vectors exist by Theorem 2.2. We will use these weight vectors to determine an
improved and more stable embedding. Below we show the details of the modi ﬁed locally linear
embedding using multiple local weight vectors.
Consider the neighbor set of xi with ki neighbors. Assume that the ﬁrst ri singular values of Gi are
large compared with the remaining si = ki − ri singular values. (We will discuss how to choose it
be si ≤ k linearly independent weight vectors,
, . . . , w(si )
later.) Let w(1)
i
i
i = (1 − αi )wi (γ ) + ViHi (:, (cid:4)),
(cid:4) = 1, · · · , si .
w((cid:1))
Here wi (γ ) is the regularized solution deﬁned in (2.2) with G = Gi , Vi is the matrix of Gi cor-
(cid:1)vi(cid:1) with vi = V T
responding to the si smallest right singular values, αi = 1√
i 1ki , and Hi is a
si
= αi1si .
i 1ki
Householder matrix that satisﬁes HiV T
We look for a d-dimensional embedding {t1 , . . . , tN }, that minimizes the embedding cost function
N(cid:1)
si(cid:1)
(cid:1)
j i tj (cid:1)2
(cid:1)ti −
w((cid:1))
j∈Ji
i=1
(cid:1)=1
with the constraint T T T = I . Denote by Wi = (1 − αi )wi (γ )1T
+ ViHi the local weight matrix
si
and let ˆWi ∈ RN ×si be the embedded matrix of Wi into the N -dimensional space such that
ˆW (i, :) = −1T
j /∈ Ii = Ji ∪ {i}.
ˆWi (Ji , :) = Wi ,
, ˆW (j, :) = 0,
si
(cid:1)
(cid:1)
The cost function (3.5) can be rewritten as
(cid:1)T ˆWi (cid:1)2
F = Tr(T
E (T ) =
(cid:2)
i
i
ˆW T
ˆWi
where Φ =
i . The minimizer of E (T ) is given by the matrix T = [u2 , . . . , ud+1 ]T of the
i
d eigenvectors of Φ corresponding to the 2nd to d + 1st smallest eigenvalues.

ˆW T
i T T ) = Tr(T ΦT T ),

E (T ) =

(3.5)

ˆWi

(3.6)

3.1 Determination of number si of approximation optimal weight vectors
Obviously, si should be selected such that σki−si+1 (Gi ) is relatively small.
In general, if the
data points are sampled from a d-dimensional manifold and the neighbor set is well selected, then
σd (Gi ) (cid:11) σd+1 (Gi ). So si can be any integer satisfying si ≤ ki − d, and si = ki − d is the
best choice. However because of noise and that the neighborhood is possibly not well selected,
σd+1 (Gi ) may be not relatively small. It makes sense to choose si as large as possible if the ratio
ki−si+1+···+λ(i)
λ(i)
is small, where λ(i)
j = σ2
j (Gi ) are the eigenvalues of GT
i Gi . There is a trade
ki
1 +···+λ(i)
λ(i)
ki−si
(cid:4)
(cid:5)
between the number of weight vectors and the approximation to (cid:1)Giw∗
i (cid:1). We suggest
(cid:2)ki
(cid:2)ki−(cid:1)
j=ki−(cid:1)+1 λ(i)
(cid:4) ≤ ki − d,
j
si = max
j=1 λ(i)
(cid:1)
j
for a given η < 1 that is a threshold error. Here d can be over estimated to be d(cid:5) > d.
(cid:2)ki
(cid:2)d
Obviously, si depends on the parameter η monotonically. The smaller η is, the smaller si is, and
of course, the smaller the combination errors for the weight vectors used are. We use an adaptive
j , i = 1, . . . , N , and reorder {ρi }
j=1 λ(i)
j=d+1 λ(i)
strategy to set η as follows. Let ρi =
j /
as ρπ1 ≤ . . . ≤ ρπN . Then we set η to be the middle term of {ρi }, η = ρπ(cid:1)N/2(cid:2) , where (cid:12)N/2(cid:13) is
the nearest integer of N/2 towards inﬁnity. In general, if the manifold near xi is ﬂoat or has small

(3.7)

< η

,

curvatures and the neighbors are well selected, ρi is smaller than η and si = k − d. For those
neighbor sets with large local curvatures, ρi > η and si < ki − d. So less number of weight vectors
are used in constructing the local linear structures and the combination errors decrease.

We summarize the Modiﬁed Locally linear Embedding (MLLE) algorithm as follows.

Algorithm MLLE (Modiﬁed Locally linear Embedding).
1. For each i = 1, · · · , N ,
1.1 Determine a neighbor set Ni = {xj , j ∈ Ji} of xi , i /∈ Ji .
1.2 Compute the regularized solution wi (γ ) by (2.3) with a small γ > 0.
(cid:2)ki
(cid:2)d
1.3 Compute the eigenvalues λ(i)
1 , . . . , λ(i)
ki and eigenvectors v (i)
1 , . . . , v (i)
ki of GT
i Gi . Set
j=d+1 λ(i)
j=1 λ(i)
ρi =
j /
j .
2. Sort {ρi } to be {ρπi
} in increasing order and set η = ρπ(cid:1)N/2(cid:2) .
3. For each i = 1, · · · , N ,
ki−si+1 , . . . , v (i)
3.1 Set si by (3.7) and set Vi = [v (i)
ki
3.2 Construct Φ by using Wi = wi (γ )1T
+ Vi .
si
4. Compute the d + 1 smallest eigenvectors of Φ and pick up the eigenvector matrix corre-
sponding to the 2nd to d + 1st smallest eigenvalues, and set T = [u2 , . . . , ud+1 ]T .

], αi = (cid:1)1T
ki

Vi(cid:1).

The computational cost of MLLE is almost the same as that of LLE. The additional ﬂops of MLLE
i Gi is O(k3
i ) and totally O(k3N ) with k = maxi ki .
for computing the eigendecomposition of GT
Note that the most computationally expensive steps in both LLE and MLLE are the neighborhood
selection and the computation of the d + 1 eigenvectors of the alignment matrix Φ corresponding to
small eigenvalues. They cost O(mN 2 ) and O(dN 2 ), respectively. Because k (cid:14) N , the additional
cost of MLLE is ignorable.

4 An analysis of MLLE for isometric manifolds
Consider the application of MLLE on an isometric manifold M = f (Ω) with open set Ω ⊂ Rd and
(cid:1)
(cid:1)
smooth function f . Assume that {xi } are sampled from M, xi = f (τi ), i = 1, . . . , N . We have
(cid:1)xi −
wj i τj (cid:1) + O(ε2
wj ixj (cid:1) = (cid:1)τi −
i ),
(4.8)
So we have that (cid:1)xi − (cid:2)
j∈Ji
j∈Ji
, we have (cid:1)xi − (cid:2)
due to the isometry of f . If ki > d, then the optimal reconstruction error of τi should be zero.
j ixj (cid:1) = O(ε2
w∗
i ). For the approximately optimal weight vectors
j∈Ji
(cid:1)τi − (cid:2)
j i xj (cid:1) ≈ σki−si+1 (Gi ) + O(ε2
w((cid:1))
w((cid:1))
i ). Inversely, if follows from (4.8) that
j∈Ji
i
j i τj (cid:1) ≈ σki−si+1 (Gi ) + O(ε2
i ). Therefore, denoting T ∗ = [τ1 , . . . , τN ], we have
j i τj − τi (cid:1)2 ≤ N(cid:1)
N(cid:1)
si(cid:1)
(cid:1)
w((cid:1))
j∈Ji
(cid:1)
E (T ∗ ) =
w((cid:1))
ki−si+1 (Gi ) + O(max
i ).
siσ2
ε2
i
j∈Ji
i=1
i=1
(cid:1)=1
, i.e., T ∗ = LU and U U T = I , since L = T ∗U T ∈ Rd×d , we have
For the orthogonalized U of T ∗
that σd (L) = σd (T ∗ ) and E (U ) ≤ E (T ∗ )/σ2
d (T ∗ ). Note that σ2
ki−si+1 (Gi ) is very small generally.
So E (U ) is always small and approximately achieves the minimum. Roughly speaking, MLLE can
retrieve the isometric embedding.

5 Comparison to LTSA

MLLE has similar properties similar to those of LTSA. In this section, we compare MLLE and
LTSA in the linear dependence of neighbors and alignment matrices. For simplicity, we assume that
ri = d, i.e., ki − d weight vectors are used in MLLE for each neighbor set.

5.1 Linear dependence of neighbors.
ki−d(cid:1)
(cid:1)
The total combination error
j i xj − xi (cid:1)2 = (cid:1)GiWi (cid:1)2
(cid:1)
M LLE (Ni ) =
w((cid:1))
F
(cid:2)
j∈Ji
(cid:1)=1
of xi can be a measure of the linear dependence of the neighborhood Ni . To compare it with the
measure of linear dependence deﬁned by LTSA, we denote by ¯xi = 1|Ii |
xj the mean of
j∈Ii
members in the whole neighbors of xi including xi itself, and ¯Xi = [. . . , xj − ¯xi , . . .]j∈Ii . It can
˜Wi with ˜Wi = ˆWi (Ii , :). So M LLE (Ni ) = (cid:1) ¯Xi
˜Wi (cid:1)2
be veriﬁed that GiWi = ¯Xi
F .
(cid:1)
In LTSA, the linear dependence of Ni is measured by the total errors
˜Vi (cid:1)2
j (cid:1)2 = (cid:1) ¯Xi − QiΘi(cid:1)2
LT SA (Ni ) =
(cid:1)xj − ¯xi − Qi θ(i)
F = (cid:1) ¯Xi
F ,
j∈Ii
where ˜Vi is the matrix consists of the right singular vectors of ¯Xi corresponding to ki − d smallest
singular values. The MLLE-measure M LLE and the LTSA-measure LT SA of neighborhood linear
dependence are similar,
M LLE (Ni ) = (cid:1) ¯Xi
˜Wi (cid:1)2
(cid:1) ¯Xi ˜w((cid:1))
i (cid:1) ≈ min, (cid:4) ≤ ki − d,
F ,
(cid:1) ¯XiZ (cid:1)2
˜Vi(cid:1)2
LT SA (Ni ) = (cid:1) ¯Xi
F = min
F .
Z T Z=I

5.2 Alignment matrices.
Both MLLE and LTSA minimize a trace function of an alignment matrix Φ to obtain an embedding,
N(cid:1)
minT T T =I trace(T ΦT T ). The alignment matrix can be written in the same form
Φ =
SiΦiS T
i ,
i=1
where Si is a selection matrix consisting of the columns j ∈ Ii of the large identity matrix of order
˜V T
= ˜Vi
N . In LTSA, the local matrix Φi is given by the orthogonal projection, i.e. ΦLT SA
, see
i
i
˜W T
= ˜Wi
i . It is interesting that the range space of ˜Wi span( ˜Wi ) and the
[10]. For MLLE, ΦM LLE
i
range space span( ˜Vi ) of ˜Vi are tightly close each other if the reconstruction error of xi is small. The
following theorem gives an upper bound of the closeness using the distance dist( ˜Wi , ˜Vi ) between
span( ˜Wi ) and span( ˜Vi ) that denotes the largest angle between the two subspaces.
(See [4] for
discussion about distance of subspaces.)
Theorem 5.1 Let Gi = [· · · , xj − xi , · · ·]j∈Ji . Then dist( ˜Wi , ˜Vi ) ≤ (cid:4)GiWi (cid:4)
σd ( ˜Wi )σd ( ¯Xi ) .

6 Experimental Results.

In this section, we present several numerical examples to illustrate the performance of MLLE algo-
rithm. The test data sets include simulated date sets and real world examples.

First, we compare Isomap, LLE, LTSA, and MLLE on the Swiss roll with a hole. The data points
generated from a rectangle with a missing rectangle strip punched out of the center and then the
resulting Swiss roll is not convex. We run these four algorithms with k = 10. In the top middle of
Figure 3, we plot the computed coordinates by Isomap, and there is a dilation of the missing region
and a warp on the rest of the embedding. As seen in the top right of Figure 3, there is a strong
distortion on the computed coordinates by LLE. As we have shown in the bottom of Figure 3, LTSA
and MLLE perform well.
We now compare MLLE and LTSA for a 2D manifold with 3 peaks embedded in 3D space. We
generate N = 1225 3D-points xi = [ti , si , h(ti , si )]T , where ti and si are uniformly distributed in
(cid:7)
(cid:6)
(cid:7)
(cid:6)
(cid:6)
(cid:7)
the interval [−1.5, 1.5] and h(t, s) is deﬁned by
h(t, s) = e−10
(t−0.5)2+(s−0.5)2

− e−10

− e−10

t2+(s+1)2

(1+t)2+s2

.

Original

ISOMAP

LLE

15

10

5

0

−5

− 10

− 15
30

20

10

0

−10

20

10

0

−10

0

10

−20
−60

−40

−20

0

20

40

Generating Coordinates

LTSA

20

15

10

5

0.05

0

0

0

50

100

−0.05

−0.05

0

0.05

−1

−2

−3

3

2

1

0

−2

2

1

0

−1

−2

−2

0

2

MLLE

0

2

Figure 3: Left column: Swiss-roll data and generating coordinates with a missing rectangle. Middle
column: computed results by Isomap and LTSA. Right column: results of LLE and MLLE.

0

−2

−2

0

2

0.1

0.05

0

−0.05

−0.1
−0.1

LTSA

−0.05

0

0.05

0.1

Generating parameter

MLLE

2

1

0

−1

−2

−1

0

1

2

−2

−1

0

1

2

1

0

−1
2

1.5

1

0.5

0

− 0.5

−1

− 1.5
−2

Figure 4: Left column:Plots of the 3-peak data and the generating coordinates. Right column:
Results of LTSA and MLLE.

See the left of Figure 4 for the data points and the generating parameters. It is easy to show that the
manifold parameterized by f (t, s) = [t, s, h(t, s)]T is approximately isometric since the Jacobian
Jf (t, s) is orthonormal approximately. In the right of Figure 4, we plot the computed coordinates
by LTSA and MLLE with k = 12. The deformations of the computed coordinates by LTSA near
the peaks are prominent because the curvature of the 3-peak manifold varies very much. This bias
can be reduced by the modi ﬁed curvature model of LTSA proposed in [8]. MLLE can recover the
generating parameter perfectly up to an afﬁne transformation.
Next, we consider a data set containing N = 4400 handwritten digits (’2’-’5’) with 1100 examples
of each class. The gray scale images of handwritten numerals are at 16× 16 resolution and converted
m = 256 dimensional vectors2 . The data points are mapped into a 2-dimensional space using LLE
and MLLE respectively. These experiments are shown in Figure 5. It is clear that MLLE performs
◦’, ’ (cid:16)’, ’ (cid:12)’ and ’ (cid:17)’
much better than LLE. Most of the digit classes (digits ’2’-’5’ are marked by ’
respectively) are well clustered in the resulting embedding of MLLE.

Finally, we consider application of MLLE and LLE on the real data set of 698 face images with
variations of two pose parameters (left-right and up-down) and one lighting parameter. The image
size is 64-by-64 pixel, and each image is converted to an m = 4096 dimensional vector. We apply
MLLE with k = 14 and d = 3 on the data set. The ﬁrst two coordinates of MLLE are plotted in
the middle of Figure 6. We also extract four paths along the boundaries of the set of the ﬁrst two
coordinates, and display the corresponding images along each path. These components appear to
capture well the pose and lighting variations in a continuous way.

References

[1] D. Donoho and C. Grimes. Hessian Eigenmaps: new tools for nonlinear dimensionality reduc-
tion. Proceedings of National Academy of Science, 5591-5596, 2003

2The data set can be downloaded at http://www.cs.toronto.edu/ roweis/data.html.

LLE

MLLE

5

4

3

2

1

0

−1

−2

−3

−4

−5
−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

3

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5
−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

Figure 5: Embedding results of N = 4400 handwritten digits by LLE(left) and MLLE(right).

Figure 6: Images of faces mapped into the embedding described by the ﬁrst two coordinates of
MLLE, using the parameters k = 14 and d = 3.

[2] M. Brand. Charting a manifold. Advances in Neural Information Processing Systems, 15, MIT
Press, 2003
[3] Jihun Ham, Daniel D. Lee, Sebastian Mika, Bernhard Scholkopf. A kernel view of the dimen-
sionality reduction of manifolds. International Conference On Machine Learning 21, 2004.
[4] G. H. Golub and C. F Van Loan. Matrix Computations. Johns Hopkins University Press,
Baltimore, Maryland, 3nd edition, 1996.
[5] S. Roweis and L Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-
ence, 290: 2323–2326, 2000.
[6] L. Saul and S. Roweis. Think globally, ﬁt locally: unsupervised learning of nonlinear mani-
folds. Journal of Machine Learning Research, 4:119-155, 2003.
[7] J Tenenbaum, V. De Silva and J. Langford. A global geometric framework for nonlinear
dimension reduction. Science, 290:2319–2323, 2000
[8] J. Wang, Z. Zhang and H. Zha. Adaptive Manifold Learning. Advances in Neural Information
Processing Systems 17, edited by Lawrence K. Saul and Yair Weiss and L´eon Bottou, MIT
Press, Cambridge, MA, pp.1473-1480, 2005.
[9] Z. Zhang and H. Zha. Principal Manifolds and Nonlinear Dimensionality Reduction via Tan-
gent Space Alignment. SIAM J. Scienti ﬁc Computing , 26(1):313–338, 2004.
[10] H. Zha and Z. Zhang. Spectral Analysis of Alignment in Manifold Learning. Submitted, 2006.
[11] M. Vlachos, C. Domeniconi, D. Gunopulos, G. Kollios, and N. Koudas Non-Linear Di-
mensionality Reduction Techniques for Classiﬁcation and Visualization Proc. Eighth ACM
SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, July 2002.


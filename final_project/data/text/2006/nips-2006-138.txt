Inferring Network Structure from Co-Occurrences

Michael G. Rabbat
Electrical and Computer Eng.
University of Wisconsin
Madison, WI 53706
rabbat@cae.wisc.edu

M ´ario A.T. Figueiredo
Instituto de Telecomunicac¸ ˜oes
Instituto Superior T ´ecnico
Lisboa, Portugal
mtf@lx.it.pt

Robert D. Nowak
Electrical and Computer Eng.
University of Wisconsin
Madison, WI 53706
nowak@ece.wisc.edu

Abstract

We consider the problem of inferring the structure of a network from co-
occurrence data: observations that indicate which nodes occur in a signaling path-
way but do not directly reveal node order within the pathway. This problem is
motivated by network inference problems arising in computational biology and
communication systems, in which it is difﬁcult or impossible to obtain precise
time ordering information. Without order information, every permutation of the
activated nodes leads to a different feasible solution, resulting in combinatorial
explosion of the feasible set. However, physical principles underlying most net-
worked systems suggest that not all feasible solutions are equally likely. Intu-
itively, nodes that co-occur more frequently are probably more closely connected.
Building on this intuition, we model path co-occurrences as randomly shufﬂed
samples of a random walk on the network. We derive a computationally efﬁcient
network inference algorithm and, via novel concentration inequalities for impor-
tance sampling estimators, prove that a polynomial complexity Monte Carlo ver-
sion of the algorithm converges with high probability.

1 Introduction

The study of complex networked systems is an emerging ﬁeld impacting nearly every area of engi-
neering and science, including the important domains of biology, cognitive science, sociology, and
telecommunications. Inferring the structure of signalling networks from experimental data precedes
any such analysis and is thus a basic and fundamental task. Measurements which directly reveal net-
work structure are often beyond experimental capabilities or are excessively expensive. This paper
addresses the problem of inferring the structure of a network from co-occurrence data: observations
which indicate nodes that are activated in each of a set of signaling pathways but do not directly re-
veal the order of nodes within each pathway. Co-occurrence observations arise naturally in a number
of interesting contexts, including biological and communication networks, and networks of neuronal
colonies.

Biological signal transduction networks describe fundamental cell functions and responses to envi-
ronmental stress [1]. Although it is possible to test for individual, localized interactions between
gene pairs, this approach (called genetic epistatic analysis) is expensive and time-consuming. High-
throughput measurement techniques such as microarrays have successfully been used to identify
the components of different signal transduction pathways [2]. However, microarray data only re-
ﬂects order information at a very coarse, unreliable level. Developing computational techniques for
inferring pathway orders is a largely unexplored research area [3].

A similar problem has been studied in telecommunication networks [4]. In this context, each path
corresponds to a transmission between an origin and destination. The origin and destination are ob-
served, in addition to the activated switches/routers carrying the transmission through the network.

However, due to the geographically distributed nature of the measurement infrastructure and the ra-
pidity at which transmissions are completed, it is not possible to obtain precise ordering information.

Another exciting potential application arises in neuroimaging [5, 6]. Functional magnetic resonance
imaging provides images of brain activity with high spatial resolution but has relatively poor tempo-
ral resolution. Treating distinct brain regions as nodes in a functional brain network that co-activate
when a subject performs different tasks may lead to a similar network inference problem.

Given a collection of co-occurrences, a feasible network (consistent with the observations) is easily
obtained by assigning an order to the elements of each co-occurrence, thereby specifying a path
through the hypothesized network. Since any arbitrary order of each co-occurrence leads to a feasi-
ble network, the number of feasible solutions is proportional to the number of permutations of all the
co-occurrence observations. Consequently we are faced with combinatorial explosion of the feasible
set, and without additional assumptions or side information there is no reason to prefer one particular
feasible network over the others. See the supplementary document [7] for further discussion.

Despite the apparent intractability of the problem, physical principles governing most networks
suggest that not all feasible solutions are equally plausible. Intuitively, nodes that co-occur more
frequently are more likely to be connected in the underlying network. This intuition has been used
as a stepping stone by recent approaches proposed in the context of telecommunications [4], and in
learning networks of collaborators [8]. However, because of their heuristic nature, these approaches
do not produce easily interpreted results and do not readily lend themselves to analysis or to the
incorporation of side information.

In this paper, we model co-occurrences as randomly permuted samples of a random walk on the
underlying network. The random permutation accounts for lack of observed order. We refer to this
process as the shufﬂed Markov model. In this framework, network inference amounts to maximum
likelihood estimation of the parameters governing the random walk (initial state distribution and
transition matrix). Direct maximization is intractable due to the highly non-convex log-likelihood
function and exponential feasible set arising from simultaneously considering all permutations of all
co-occurrences. Instead, we derive a computationally efﬁcient EM algorithm, treating the random
permutations as hidden variables. In this framework the likelihood factorizes with respect to each
pathway/observation, so that the computational complexity of the EM algorithm is determined by
the E-step which is only exponential in the longest path. In order to handle networks with long
paths, we propose a Monte Carlo E-step based on a simple, linear complexity importance sampling
scheme. Whereas the exact E-step has computational complexity which is exponential in path length,
we prove that a polynomial number of importance samples sufﬁces to retain desirable convergence
properties of the EM algorithm with high probability. In this sense, our Monte Carlo EM algorithm
breaks the curse of dimensionality using randomness.

It is worth noting that the approach described here differs considerably from that of learning the
structure of a directed graphical model or Bayesian network [9, 10]. The aim of graphical mod-
elling is to ﬁnd a graph corresponding to a factorization of a high-dimensional distribution which
predicts the observations well. These probabilistic models do not directly reﬂect physical structures,
and applying such an approach to co-occurrences would ignore physical constraints inherent to the
observations: co-occurring vertices must lie along a path in the network.

2 Model Formulation and EM Algorithm

2.1 The Shufﬂed Markov Model
We model a network as a directed graph G = (V , E ), where V = {1, . . . , |V |} is the vertex (node)
set and E ⊆ V 2 is the set of edges (direct connections between vertices). An observation, y ⊂ V ,
is a subset of vertices co-activated when a particular stimulus is applied to the network (e.g., col-
lection of signaling proteins activated in response to an environmental stress). Given a set of T
},
observations, Y = {y(1), . . . , y(T ) }, each corresponding to a path, where y(m) = {y (m)
1 , . . . , y (m)
Nm
we say that a graph (V , E ) is feasible w.r.t. Y if for each y(m) ∈ Y there is an ordered path
t = y (m)
) such that z (m)
, . . . , τ (m)
) and a permutation τ (m) = (τ (m)
z(m) = (z (m)
, . . . , z (m)
, and
(m)
1
1
Nm
Nm
τ
(zt−1 , zt ) ∈ E , for t = 2, ..., Nm .
t

The (unobserved) ordered paths, Z = {z(1) , ..., z(T ) }, are modelled as T independent samples of
a ﬁrst-order Markov chain with state set V . The Markov chain is parameterized by the initial state
distribution π and the (stochastic) transition matrix A. We assume that the support of the transition
matrix is determined by the adjacency structure of the graph; i.e., Ai,j > 0 ⇔ (i, j ) ∈ E . Each
observation y(m) results from shufﬂing the elements of z(m) via an unobserved permutation τ (m) ,
drawn uniformly from SNm (the set of all permutations of Nm objects); i.e., z (m)
= y (m)
, for
(m)
t
τ
t
 − log(Nm !)
 .
 X
log
t = 1, . . . , Nm . All the τ (m) are assumed mutually independent and independent of all the z(m) .
Under this model, the log-likelihood of the set of observations Y is
TX
P [y(m) |τ , A, π ]
log P [Y |A, π ] =
QN
τ ∈SNm
m=1
where P [y|τ , A, π ] = πyτ1
, and network inference consists in computing the
t=2 Ayτt−1 ,yτt
maximum likelihood (ML) estimates (AML , πML ) = arg maxA,π log P [Y |A, π ]. With the ML
estimates in hand, we may determine the most likely permutation for each y(m) and obtain a feasi-
ble reconstruction from the ordered paths. In general, log P [Y |A, π ] is a non-concave function of
(A, π), so ﬁnding (AML , πML ) is not easy. Next, we derive an EM algorithm for this purpose, by
treating the permutations as missing data.

(1)

2.2 EM Algorithm

=

log P [x(m) |r(m) , A, π ]

= (w(m)
) be a binary representation of z(m) , de ﬁned by w(m)
, ..., w(m)
Let w(m) = (w(m)
t,1 ,
t
1
Nm
t,i = 1) ⇔ (z (m)
= i); let W = {w(1) , ..., w(T ) }. Let
t,|V | ) ∈ {0, 1}|V | , with (w(m)
..., w(m)
t
X = {x(1) , . . . , x(T ) } be the binary representation for Y , de ﬁned in a similar way: x(m) =
t,|V | ) ∈ {0, 1}|V | , with (x(m)
t,i = 1) ⇔ (y (m)
, ..., x(m)
(x(m)
), where x(m)
t = (x(m)
t,1 , ..., x(m)
t = i).
1
Nm
Finally, let R = {r(1) , . . . , r(T ) } be the collection of permutation matrices corresponding to
T = {τ (1) , . . . , τ (T ) }; i.e., (r(m)
t,t0 = 1) ⇔ (τ (m)
= t0 ). With this notation in place, the com-
t
plete log-likelihood can be written as log P [X , R|A, π ] = log P [X |R, A, π ] + log P [R], where
TX
m=1

log P [X |R, A, π ] =
|V |X
NmX
TX
NmX
|V |X
NmX
TX
1,t0 x(m)
r(m)
t00 ,i x(m)
t,t0 r(m)
t−1,t00 x(m)
r(m)
t0 ,i log πi ,
t0 ,j log Ai,j +
t0=1
t0 ,t00=1
m=1
t=2
m=1
i=1
i,j=1
and P [R] is the probability of the set of permutations, which is constant and thus dropped, since the
computing Q (cid:0)A, π ; Ak , πk (cid:1)
E (cid:2)log P [X , R|A, π ] (cid:12)(cid:12)X , Ak , πk (cid:3),
permutations are independent and equiprobable.
=
by
(the E-step)
The EM algorithm proceeds
the expected value of log P [X , R|A, π ] w.r.t. the missing
R, conditioned on the observations and on the current model estimate (Ak , πk ). Examining
t0 ,t00 ≡ PNm
log P [X , R|A, π ] reveals that it is linear w.r.t. simple functions of R: (a) the ﬁrst row of each
r(m) , i.e., r(m)
1,t0 ; (b) sums of transition indicators, i.e., α(m)
t=2 r(m)
t,t0 r(m)
t−1,t00 . Consequently,
the E-step reduces to computing the conditional expectations of r(m)
t0 ,t00 , denoted ¯r(m)
1,t0 and α(m)
Q (cid:0)A, π ; Ak , πk (cid:1) .
1,t0
and ¯α(m)
t0 ,t00 , respectively, and plugging them into the complete log-likelihood (2), which yields
Since the permutations are (a priori) equiprobable, we have P [r(m) ] = (Nm !)−1 , P (cid:2)r(m)
1,t0 = 1] =
1,t0 = 1] = 1/(Nm − 1)!. Using these facts, the mutual
(Nm − 1)!/Nm ! = 1/Nm , and P [r(m) |r(m)
t0 = X
P (cid:2)x(m) (cid:12)(cid:12)r, Ak , πk (cid:3),
independence among different observations, and Bayes law, it is not hard to show that
t0PNm
γ (m)
γ (m)
t0=1 γ (m)
t0
r: r1,t0 =1

¯r(m)
1,t0 =

with

(2)

(3)

where each term P (cid:2)x(m) (cid:12)(cid:12)r, Ak , πk (cid:3) is easily computed after using r to “unshufﬂe ”
NmY
P (cid:2)x(m) (cid:12)(cid:12)r, Ak , πk (cid:3) = P (cid:2)y(m) (cid:12)(cid:12)τ , Ak , πk (cid:3) = πk
Ak
(m)
(m)
τt−1 ,y
y
y
τ1
t=2

(m)
τt

.

x(m) :

¯α(m)
t0 ,t00 =

,

The computation of ¯α(m)
t0 ,t00 is similar to that of ¯r(m)
1,t0 ; the key observations are that P [r(m)
t,t0 r(m)
t−1,t00 =
1] = (Nm − 2)!/Nm ! and P [r(m) |r(m)
t−1,t00 = 1] = 1/(Nm − 2)!, leading to
t,t0 r(m)
t0 ,t00 = X
NmX
PNm
γ (m)
P [x(m) |r, Ak , πk ]
t0 ,t00
γ (m)
rt,t0 rt−1,t00 .
with
(4)
t0 ,t00 } requires O(cid:0)Nm !(cid:1) operations. For large Nm , this is a heavy load; in
t0=1 γ (m)
t0
r
t=2
Computing {¯r(m)
1,t0 } and { ¯α(m)
Maximization of Q (cid:0)A, π ; Ak , πk (cid:1) w.r.t. A and π , under the normalization constraints, leads to the
Section 3, we describe a sampling approach for computing approximations to ¯r1,t0 and ¯αt0 ,t00 .
PT
PNm
PT
PNm
M-step:
PT
P|S |
PT
P|S |
PNm
PNm
t00 ,i x(m)
t0 ,t00 x(m)
t0 ,t00=1 ¯α(m)
t0=1 ¯r(m)
1,t0 x(m)
t0 ,j
t0 ,i
m=1
m=1
t00 ,i x(m)
1,t0 x(m)
t0 ,t00=1 ¯α(m)
t0 ,t00 x(m)
t0=1 ¯r(m)
t0 ,j
t0 ,i
j=1
i=1
m=1
m=1
(5)
Standard convergence results for the EM algorithm due to Boyles and Wu [11, 12] guarantee that the
sequence {(Ak , πk )} converges monotonically to a local maximum of the likelihood.

i =
and πk+1

i,j =
Ak+1

.

2.3 Handling Known Endpoints

In some applications, (one or both of) the endpoints of each path are known and only the internal
nodes are shufﬂed. For example, in telecommunications problems, the origin and destination of
each transmission are known, but not the network connectivity.
In estimating biological signal
transduction pathways, a physical stimulus (e.g., hypotonic shock) causes a sequence of protein
interactions, resulting in another observable physical response (e.g., a change in cell wall structure);
in this case, the stimulus and response act as ﬁxed endpoints, the goal is to infer the order of the
PT
sequence of protein interactions. Knowledge of the endpoints of each path imposes the constraints
r(m)
1,1 = 1 and r(m)
= 1. Under the ﬁrst constraint, estimates of the initial state probabilities
Nm ,Nm
m=1 x(m)
are simply given by πi = 1
1,i . Thus, EM only needs to be used to estimate A. In this
T
setup, the E-step has a similar form as (4) but with sums over r replaced by sums over permutation
matrices satisfying r1,1 = 1 and rN ,N = 1. The M-step update for Ak+1 remains unchanged.

3 Large Scale Inference via Importance Sampling

For long paths, the combinatorial nature of the exact E-step – summing over all permutations of
each sequence in (3) and (4) – may render exact computation intractable. This section presents a
Monte Carlo importance sampling (see, e.g., [13]) version of the E-step, along with ﬁnite sample
bounds guaranteeing that a polynomial complexity Monte Carlo EM algorithm retains desirable
convergence properties of the EM algorithm; i.e., monotonic convergence to a local maximum.

3.1 Monte Carlo E-Step by Importance Sampling

To lighten notation in this section we drop the superscripts from (Ak , πk ), using simply (A, π)
for the current parameter estimates. Moreover, since the statistics ¯α(m)
t0 ,t00 and ¯r(m)
1,t0 depend only
on the mth co-activation observation, y(m) , we focus on a particular length-N path observation
y = (y1 , y2 , . . . , yN ) and drop the superscript (m).
A na¨ıve Monte Carlo approximation would be based on random permutations sampled from the
uniform distribution on SN . However, the reason we resort to approximation techniques in the ﬁrst

place is that SN is large, but typically only a small fraction of its elements have non-negligible
posterior probability, P [τ |y, A, π ]. Although we would ideally sample directly from the poste-
rior, this would require determining its value for all N ! permutations.
Instead, we propose the
following sequential scheme for sampling a permutation using the current parameter estimates,
(A, π). To ensure the same element is not sampled twice we introduce a vector of binary ﬂags,
f = (f1 , f2 , . . . , f|V | ) ∈ {0, 1}|V | . Given a probability distribution p = (p1 , p2 , . . . , p|V | ) on the
vertex set, V , denote by p|f the restriction of p to those elements i ∈ V for which fi = 1; i.e.,
pi fiP|V |
(p|f )i =
for i = 1, 2, . . . , |V |.
j=1 pj fj
Our sampling scheme proceeds as follows:

(6)

,

Step 1: Initialize f so that fi = 1 if yt = i for some t = 1, . . . , N , and fi = 0 otherwise.
Sample an element v from V according to the distribution π |f on V .
Find t such that yt = v . Set τ1 = t.
Set fv = 0 to prevent yt from being sampled again (ensure τ is a permutation). Set i = 2.
Step 2: Let Av denote the v th row of the transition matrix.
Sample an element v 0 from V according to the distribution Av |f on V .
Find t such that yt = v 0 . Set τi = t. Set fv 0 = 0.
Step 3: While i < N , update v ← v 0 and i ← i + 1 and repeat Step 2; otherwise, stop.

Repeating this sampling procedure L times yields a collection of iid permutations τ 1 , τ 2 , . . . , τ L ,
where the superscript now identiﬁes the sample number; the corresponding permutation matrices
are r1 , r2 , . . . , rL . Samples generated according to the scheme described above are drawn from a
distribution R[τ |x, A, π ] on SN which is different from the posterior P [τ |x, A, π ]. Importance
PL
PL
PN
sample estimates correct for this disparity and are given by the expressions
br1,t0 =
bαt0 ,t00 =
PL
PL
‘=1 u‘ r‘
t=2 r‘
t,t0 r‘
‘=1 u‘
t−1,t00
1,t0
‘=1 u‘
‘=1 u‘
NX
NY
where the correction factor (or weight) for sample r‘ is given by
= P [τ ‘ |y, A, π ]
u‘ = P [r‘ |x, A, π ]
R[r‘ |x, A, π ]
R[τ ‘ |y, A, π ]
t0=t
t=2

Ayτ ‘
t−1

,yτ ‘
t0

and

(7)

(8)

,

=

.

A detailed derivation of the exact form of the induced distribution, R, and the correction factor, u‘ ,
based on the sequential nature of the sampling scheme, along with further discussion and comparison
with alternative sampling schemes can be found in the supplementary document [7]. In fact, terms
in the product (8) are readily available as a byproduct of Step 2 (denominator of Av |f ).

3.2 Monotonicity and Convergence

Standard EM convergence results directly apply when the exact E-step is used [11, 12]. Let
θk = (Ak , πk ). By choosing θk+1 according to (5) we have θk+1 = arg maxθ Q(θ ; θk ), and
the monotonicity property, Q(θk+1 ; θk ) ≥ Q(θk ; θk ), is satisﬁed. Together with the fact that the
marginal log-likelihood (1) is continuous in θ and bounded above, the monotonicity property guar-
antees that the exact EM iterates converge monotonically to a local maximum of log P [Y |θ ].
bθ
k+1 = arg maxθ bQ(θ ; bθ
k ), where bQ is de ﬁned analogously to Q but with ¯α(m)
When the Monte Carlo E-step is used, we no longer have monotonicity since now the M-step solves
1,t0 ; for monotonicity we need Q(bθ
k+1 ; bθ
k ) ≥ Q(bθ
k ; bθ
t0 ,t00 and br(m)
by bα(m)
t0 ,t00 and ¯r(m)
1,t0 replaced
k ). To assure the Monte Carlo
so that bQ approximates Q well enough; otherwise the MCEM may be swamped with error.
EM algorithm (MCEM) converges, the number of importance samples, L, must be chosen carefully
Recently, Caffo et al. [14] have proposed a method, based on central limit theorem-like arguments,
for automatically adapting the number of Monte Carlo samples used at each EM iteration. They

that Q(bθ
k ; bθ
k ) ≥ Q(bθ
k+1 ; bθ
guarantee what we refer to as an (, δ)-probably approximately monotonic (PAM) update, stating
k ) − , with probability at least 1 − δ .
in our problem to obtain the ﬁnite-sample PAM result below. Because bQ(bθ
k+1 ; bθ
Rather than resorting to asymptotic approximations, we take advantage of the speciﬁc form of Q
i away from zero to ensure that bQ does not
log bAk
i , in practice we bound bAk
i,j and bπk
i,j and log bπk
k ) involves terms
blow up. Speci ﬁcally, we assume a small positive constant θmin so that bAk
i,j ≥ θmin and bπk
i ≥ θmin .
(cid:19)
(cid:18)
Theorem 1 Let , δ > 0 be given. There exist ﬁnite constants bm > 0, independent of Nm , so that
if
m | log θmin |2
2N 2
2b2
mT 2N 4
log
Lm =
m
1 − (1 − δ)1/T
importance samples are used for the mth observation, then Q(bθ
k+1 ; bθ
k ) ≥ Q(bθ
2
probability greater than 1 − δ .
the importance sample estimates showing, e.g., that bα(m)
The proof involves two key steps. First, we derive ﬁnite sample concentration-style bounds for
t0 ,t00 converges to ¯α(m)
t0 ,t00 at a rate which is
exponential in the number of importance samples used. These bounds are based on rather novel
concentration inequalities for importance sampling estimators, which may be of interest in their
bAk
own right (see the supplementary document [7] for details). Then, accounting for the explicit form
i,j , bπk
of Q in our problem, the result follows from application of the union bound and the assumptions that
i ≥ θmin . In fact, by making a slightly stronger assumption it can be shown that the MCEM
update is probably monotonic (i.e., (0, δ)-PAM, not approximately monotonic) if L0
m importance
samples are used for the mth observation, where L0
m also depends polynomially on Nm and T . See
the supplementary document [7] for further discussion and for the full proof of Theorem 1.
Recall that exact E-step computation requires Nm ! operations for the mth observation (enumerating
all permutations). The bound above stipulates that the number of importance samples required for a
m log N 2
PAM update is on the order of N 4
m . Generating one importance sample using the sequential
procedure described above requires Nm operations. In contrast to the (exponential complexity) exact
EM algorithm, this clearly demonstrates that the MCEM converges with high probability while only
having polynomial computational complexity, and, in this sense, the MCEM meaningfully breaks
the curse of dimensionality by using randomness to preserve the monotonic convergence property.

k ) − , with

(9)

k ; bθ

4 Experimental Results

The performance of our algorithm for network inference from co-occurrences (NICO, pronounced
“ nee-koh”) has been evaluated on both simulated data and on a biological data set. In these ex-
periments, network structure is inferred by ﬁrst executing the EM algorithm to infer the parameters
(A, π) of a Markov chain. Then, inserting edges in the inferred graph based on the most likely order
of each path according to (A, π) ensures the resulting graph is feasible with respect to the obser-
vations. Because the EM algorithm is only guaranteed to converge to a local maximum, we rerun
the algorithm from multiple random initializations and chose the mostly likely of these solutions.
To gauge the performance of our algorithm we use the edge symmetric difference error: the total
number of false positives (edges in the inferred network which do not exist in the true network) plus
the number of false negatives (edges in the true network not appearing in the inferred network).

We simulate co-occurrence observations in the following fashion. A random graph on 50 vertices
is sampled. Disjoint sets of vertices are randomly chosen as path origins and destinations, paths
are generated between each origin-destination pair using the shortest path algorithm with either unit
weight per edge (“shortest path”) or a random weight on each edge (“random routing”), and then
co-occurrence observations are formed from each path. We keep the number of origins ﬁxed at 5
and vary the number of destinations between 5 and 40 to see how the number of observations effects
performance. NICO performance is compared against the frequency method (FM) described in [4].

Figure 1 plots the edge error for synthetic data generated using (a) shortest path routing, and (b)
random routing. Each curve is the average performance over 100 different network and path real-

(a) Shortest path routes

(b) Random routes

Figure 1: Edge symmetric differences between inferred networks and the network one would obtain
using co-occurrence measurements arranged in the correct order. Performance is averaged over
100 different network realizations. For each con ﬁguration 10 NICO and FM solutions are obtained
via different initializations. We then choose the NICO solution yielding the largest likelihood, and
compare with both the sparsest (fewest edges) and clairvoyant best (lowest error) FM solution.

izations. For each network/path realization, the EM algorithm is executed with 10 random initial-
izations. Exact E-step calculation is used for observations with Nm ≤ 12, and importance sampling
is used for longer paths. The longest observation in our data has Nm = 19. The FM uses simple
pairwise frequencies of co-occurrence to assign an order independently to each path observation. Of
the 10 NICO solutions (different random initializations), we use the one based on parameter esti-
mates yielding the highest likelihood score which also always gives the best performance. Because
it is a heuristic, the FM does not provide a similar mechanism for ranking solutions from different
initializations. We plot FM performance for two schemes; one based on choosing the sparsest FM
solution (the one with the fewest edges), and one based on clairvoyantly choosing the FM solution
with lowest error. NICO consistently outperforms even the clairvoyant best FM solution.

Our method has also been applied to infer the stress-activated protein kinease (SAPK)/Jun N -
terminal kinase (JNK) and NFκB signal transduction pathways1 (biological networks). The clus-
tering procedure described in [2] is applied to microarray data in order to identify 18 co-occurrences
arising from different environmental stresses or growth factors (path source) and terminating in the
production of SAPK/JNK or NFκB proteins. The reconstructed network (combined SAPK/JNK and
NFκB signal transduction pathways) is depicted in Figure 2. This structure agrees with the signalling
pathways identiﬁed using traditional experimental techniques which test individually for each pos-
sible edge (e.g., “MAPK” and “NF-
κB Signaling” on http://www.cellsignal.com).

5 Conclusion

This paper describes a probabilistic model and statistical inference procedure for inferring network
structure from incomplete “co-occurrence ” measurements. Co-occurrences are modelled as samples
of a ﬁrst-order Markov chain subjected to a random permutation. We describe exact and Monte Carlo
EM algorithms for calculating maximum likelihood estimates of the Markov chain parameters (ini-
tial state distribution and transition matrix), treating the random permutations as hidden variables.
Standard results for the EM algorithm guarantee convergence to a local maximum. Although our
exact EM algorithm has exponential computational complexity, we provide ﬁnite-sample bounds
guaranteeing convergence of the Monte Carlo EM variation to a local maximum with high probabil-
ity and with only polynomial complexity. Our algorithm is easily extended to compute maximum a
posteriori estimates, applying a Dirichlet prior to the initial state distribution and to each row of the
Markov transition matrix.

1NFκB proteins control genes regulating a broad range of biological processes including innate and adaptive
immunity, inﬂammation and B cell development. The NF κB pathway is a collection of paths activated by
various environmental stresses and growth factors, and terminating in the production of NFκB.

51015202530354001234567Num. DestinationsEdge Symmetric DifferenceFreq. Method (Sparsest)Freq. Method (Best)NICO (ML)51015202530354001234567Num. DestinationsEdge Symmetric DifferenceFreq. Method (Sparsest)Freq. Method (Best)NICO (ML)Figure 2: Inferred topology of the combined SAPK/JNK and NFκB signal transduction pathways.
Co-occurrences are obtained from gene expression data via the clustering algorithm described in [2],
and then network is inferred using NICO.

Acknowledgments

The authors of this paper would like to thank D. Zhu and A.O. Hero for providing the data and col-
laborating on the biological network experiment reported in Section 4. This work was supported in
part by the Portuguese Foundation for Science and Technology grant POSC/EEA-SRI/61924/2004,
the Directorate of National Intelligence, and National Science Foundation grants CCF-0353079 and
CCR-0350213.

References

[1] E. Klipp, R. Herwig, A. Kowald, C. Wierling, and H. Lehrach. Systems Biology in Practice: Concepts,
Implementation and Application. John Wiley & Sons, 2005.
[2] D. Zhu, A. O. Hero, H. Cheng, R. Khanna, and A. Swaroop. Network constrained clustering for gene
microarray data. Bioinformatics, 21(21):4014–4020, 2005.
[3] Y. Liu and H. Zhao. A computational approach for ordering signal transduction pathway components
from genomics and proteomics data. BMC Bioinformatics, 5(158), October 2004.
[4] M. G. Rabbat, J. R. Treichler, S. L. Wood, and M. G. Larimore. Understanding the topology of a telephone
network via internally-sensed network tomography. In Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing, 2005.
[5] O. Sporns and G. Tononi. Classes of network connectivity and dynamics. Complexity, 7(1):28–38, 2002.
[6] O. Sporns, D. R. Chialvo, M. Kaiser, and C. C. Hilgetag. Organization, development and function of
complex brain networks. Trends in Cognitive Science, 8(9), 2004.
[7] M.G. Rabbat, M.A.T. Figueiredo, and R.D. Nowak. Supplement to inferring network structure from
co-occurrences. Technical report, University of Wisconsin-Madison, October 2006.
[8] J. Kubica, A. Moore, D. Cohn, and J. Schneider. cGraph: A fast graph-based method for link analysis
and queries. In Proc. IJCAI Text-Mining and Link-Analysis Workshop, Acapulco, Mexico, August 2003.
[9] D. Heckerman, D. Geiger, and D. Chickering. Learning Bayesian networks: The combination of knowl-
edge and statistical data. Machine Learning, 20:197–243, 1995.
[10] N. Friedman and D. Koller. Being Bayesian about Bayesian network structure: A Bayesian approach to
structure discovery in Bayesian networks. Machine Learning, 50(1–2):95–125, 2003.
[11] R. A. Boyles. On the convergence of the EM algorithm. J. Royal Statistical Society B, 45(1):47–50, 1983.
[12] C. F. J. Wu. On the convergence properties of the EM algorithm. Ann. of Statistics, 11(1):95–103, 1983.
[13] C. Robert and G. Casella. Monte Carlo Statistical Methods. Springer Verlag, New York, 1999.
[14] B. S. Caffo, W. Jank, and G. L. Jones. Ascent-based Monte Carlo EM. J. Royal Statistical Society B,
67(2):235–252, 2005.

PI3KPLCgamma2ArtCotPKCMALT1TRAF6TAK1IKKNFkappaBC1JNKbTrCPMEKKMKKAgNFkappaBC2NFKappaBAgMHCIL1dsRNAPKRTNFGFRASHPKLTNIKUVRACCDC42RHOCS2CS1FASGCKsOSASK1
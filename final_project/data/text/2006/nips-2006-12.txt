AdaBoost is Consistent

Peter L. Bartlett
Department of Statistics and Computer Science Division
University of California, Berkeley
bartlett@stat.berkeley.edu

Mikhail Traskin
Department of Statistics
University of California, Berkeley
mtraskin@stat.berkeley.edu

Abstract

The risk, or probability of error, of the classiﬁer produced by the AdaBoost algo-
rithm is investigated. In particular, we consider the stopping strategy to be used in
AdaBoost to achieve universal consistency. We show that provided AdaBoost is
stopped after nν iterations—for sample size n and ν < 1—the sequence of risks
of the classiﬁers it produces approaches the Bayes risk if Bayes risk L∗ > 0.

1 Introduction

Boosting algorithms are an important recent development in classiﬁcation. These algorithms belong
to a group of voting methods, for example [1, 2, 3], that produce a classiﬁer as a linear combination
of base or weak classiﬁers. While empirical studies show that boosting is one of the best off the
shelf classiﬁcation algorithms (see [3]) theoretical results don’t give a complete explanation of their
effectiveness.
Breiman [4] showed that under some assumptions on the underlying distribution “population boost-
ing” converges to the Bayes risk as the number of iterations goes to inﬁnity. Since the population
version assumes inﬁnite sample size, this does not imply a similar result for AdaBoost, especially
given results of Jiang [5], that there are examples when AdaBoost has prediction error asymptoti-
cally suboptimal at t = ∞ (t is the number of iterations).
Several authors have shown that modiﬁed versions of AdaBoost are consistent. These modiﬁcations
include restricting the l1 -norm of the combined classiﬁer [6, 7] and restricting the step size of the al-
gorithm [8]. Jiang [9] analyses the unmodiﬁed boosting algorithm and proves a process consistency
property, under certain assumptions. Process consistency means that there exists a sequence (tn )
such that if AdaBoost with sample size n is stopped after tn iterations, its risk approaches the Bayes
risk. However Jiang also imposes strong conditions on the underlying distribution: the distribution
of X (the predictor) has to be absolutely continuous with respect to Lebesgue measure and the func-
2 ln P(Y =1|X )
P(Y =−1|X ) has to be continuous on X . Also Jiang’s proof is not constructive
tion FB (X ) = 1
and does not give any hint on when the algorithm should be stopped. Bickel, Ritov and Zakai in
[10] prove a consistency result for AdaBoost, under the assumption that the probability distribution
is such that the steps taken by the algorithm are not too large. We would like to obtain a simple
stopping rule that guarantees consistency and doesn’t require any modiﬁcation to the algorithm.
This paper provides a constructive answer to all of the mentioned issues:

1. We consider AdaBoost (not a modiﬁcation).
2. We provide a simple stopping rule: the number of iterations t is a ﬁxed function of the
sample size n.
3. We assume only that the class of base classiﬁers has ﬁnite VC-dimension, and that the span
of this class is sufﬁciently rich. Both assumptions are clearly necessary.

2 Setup and notation

Here we describe the AdaBoost procedure formulated as a coordinate descent algorithm and in-
troduce deﬁnitions and notation. We consider a binary classiﬁcation problem. We are given X ,
the measurable (feature) space, and Y = {−1, 1}, set of (binary) labels. We are given a sample
i=1 of i.i.d. observations distributed as the random variable (X, Y ) ∼ P , where P
Sn = {(Xi , Yi )}n
is an unknown distribution. Our goal is to construct a classiﬁer gn : X → Y based on this sample.
The quality of the classiﬁer gn is given by the misclassiﬁcation probability
L(gn ) = P(gn (X ) 6= Y |Sn ).
Of course we want this probability to be as small as possible and close to the Bayes risk
L(g) = E(min{η(X ), 1 − η(X )}),
L∗ = inf
g
where the inﬁmum is taken over all possible (measurable) classiﬁers and η(·) is a conditional prob-
ability
η(x) = P(Y = 1|X = x).
(cid:26) 1 , x > 0,
The inﬁmum above is achieved by the Bayes classiﬁer g∗ (x) = g(2η(x) − 1), where
−1 , x ≤ 0.
(cid:12)(cid:12) = 2|S |(cid:9).
max (cid:8)|S | : S ⊆ X , (cid:12)(cid:12)H|S
We are going to produce a classiﬁer as a linear combination of base classiﬁers in H = {h|h : X →
Y }. We shall assume that class H has a ﬁnite VC (Vapnik-Chervonenkis) dimension dV C (H) =
nX
Deﬁne
1
Rn (f ) =
n
i=1
Then the boosting procedure can be described as follows.
1. Set f0 ≡ 0, choose number of iterations t.
2. For k = 1, . . . , t set

R(f ) = Ee−Y f (X ) .

e−Yi f (Xi )

g(x) =

and

fk = fk−1 + αk−1hk−1 ,

where the following holds

(1)

)

f

nX
i=1

Rn (fk ) = inf
h∈H,α∈R Rn (fk−1 + αh)
We call αi the step size of the algorithm at step i.
3. Output g ◦ ft as a ﬁnal classiﬁer.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)f =
(
We shall also use the convex hull of H scaled by λ ≥ 0,
nX
Fλ =
λihi , n ∈ N ∪ {0}, λi ≥ 0,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)f =
(
i=1
as well as the set of k-combinations, k ∈ N, of functions in H
kX
F k =
λihi , λi ∈ R, hi ∈ H
f
kf k∗ = inf {X |αi |, f = X
i=1
We shall also need to deﬁne the l∗ -norm: for any f ∈ F
( l
Deﬁne the squashing function πl (·) to be
, x > l,
x , x ∈ [−l, l],
−l
, x < −l.

αihi , hi ∈ H}.

πl (x) =

λi = λ, hi ∈ H
)

.

n ˜f | ˜f = πl (f ), f ∈ F o
Then the set of truncated functions is
πl ◦ F =
The set of classiﬁers based on class F is denoted by
g ◦ F = { ˜f | ˜f = g(f ), f ∈ F }.
(cid:12)(cid:12)(cid:12)(cid:12)λ=0
Deﬁne the derivative of an arbitrary function Q(·) in the direction of h as
Q0 (f ; h) = ∂Q(f + λh)
∂λ
The second derivative Q00 (f ; h) is deﬁned similarly.

.

.

3 Consistency of boosting procedure

We shall need the following assumption.
Assumption 1 Let the distribution P and class H be such that
R(f ) = R∗ ,
λ→∞ inf
lim
f ∈Fλ
where R∗ = inf R(f ) over all measurable functions.
For many classes H, the above assumption is satisﬁed for all possible distributions P . See [6,
Lemma 1] for sufﬁcient conditions for Assumption 1. As an example of such a class, we can take
a class of indicators of all rectangles or indicators of half-spaces deﬁned by hyperplanes or binary
trees with the number of terminal nodes equal to d+ 1 (we consider trees with terminal nodes formed
by successive univariate splits), where d is the dimensionality of X (see [4]).
We begin with a simple lemma (see [1, Theorem 8] or [11, Theorem 6.1]):
Lemma 1 For any t ∈ N if dV C (H) ≥ 2 the following holds:
dP (F t ) ≤ 2(t + 1)(dV C (H) + 1) log2 [2(t + 1)/ ln 2],
where dP (F t ) is the pseudodimension of class F t .
The proof of AdaBoost consistency is based on the following result, which builds on the result by
Koltchinskii and Panchenko [12] and resembles [6, Lemma 2].

Lemma 2 For a continuous function ϕ deﬁne the Lipschitz constant
Lϕ,λ = inf {L|L > 0, |ϕ(x) − ϕ(y)| ≤ L|x − y |, −λ ≤ x, y ≤ λ}
and maximum absolute value of ϕ(·) when argument is in [−λ, λ]
ϕ(x).
Mϕ,λ = max
x∈[−λ,λ]

and

nX
Then for functions
1
q
Rϕ (f ) = Eϕ(Y f (X ))
Rϕ,n (f ) =
ϕ(Yi f (Xi )),
V = dV C (H), c = 24 R 1
n
i=1
r (V + 1)(t + 1) log2 [2(t + 1)/ ln 2]
2 d and any n, λ > 0 and t > 0,
ln 8e
0
|Rϕ (f ) − Rϕ,n (f )| ≤ cλLϕ,λ
E sup
r 2V ln(4n + 2)
f ∈πλ ◦F t
n
n

|Rϕ (f ) − Rϕ,n (f )| ≤ 4λLϕ,λ

and

E sup
f ∈Fλ

.

(2)

(3)

and

E

E sup
f ∈πλ ◦F t

(4)

(5)

+ Mϕ,λ

sup
f ∈πλ ◦F t

sup
f ∈Fλ

r ln(1/δ)
2n

.

|Rϕ (f ) − Rϕ,n (f )| ≤ 4λLϕ,λ

|Rϕ (f ) − Rϕ,n (f )| ≤ 4Lϕ,λE sup
f ∈πλ ◦F t

r (V + 1)(t + 1) log2 [2(t + 1)/ ln 2]
Also, for any δ > 0, with probability at least 1 − δ ,
r ln(1/δ)
|Rϕ (f ) − Rϕ,n (f )| ≤ cλLϕ,λ
n
+ Mϕ,λ
r 2V ln(4n + 2)
2n
n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,
Proof. Equations (3) and (5 ) constitute [6, Lemma 2]. The proof of equations (2) and (4) is similar.
nX
We begin with symmetrization to get
|Rϕ (f ) − Rϕ,n (f )| ≤ 2E sup
σi (ϕ(−Yi f (Xi )) − ϕ(0))
E sup
f ∈πλ ◦F t
f ∈πλ ◦F t
n
i=1
where σi are i.i.d. with P(σi = 1) = P(σi = −1) = 1/2. Then we use the “contraction principle”
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(see [13, Theorem 4.12, pp. 112–113]) with a function ψ(x) = (ϕ(x) − ϕ(0))/Lϕ,λ to get
nX
−σiYi f (Xi )
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
nX
n
i=1
= 4Lϕ,λE sup
σi f (Xi )
f ∈πλ ◦F t
n
i=1
Next we proceed and ﬁnd the supremum. Notice, that functions in πλ ◦ F t are bounded and clipped
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 2λE
to the absolute value equal λ, therefore we can rescale πλ ◦ F t by (2λ)−1 and get
nX
nX
E sup
σi f (Xi )
sup
σi f (Xi )
f ∈πλ ◦F t
f ∈(2λ)−1 ◦πλ ◦F t
n
n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 12√
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
i=1
i=1
Z ∞
nX
pln N (, (2λ)−1 ◦ πλ ◦ F t , L2 (Pn ))d.
Next, we are going to use Dudley’s entropy integral [14] to bound the r.h.s above
σi f (Xi )
sup
f ∈(2λ)−1 ◦πλ ◦F t
n
n
0
i=1
Since for  > 1 the covering number N is 1, then upper integration limit can be taken 1, and we can
(cid:19)dP (F )
(cid:18) 4e
use Pollard’s bound [15] for F ⊆ [0, 1]X
N (, F , L2 (P )) ≤ 2
q
where dP (F ) is a pseudodimension, and obtain for ˜c = 12 R 1
2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ˜c
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
r
ln 8e
nX
2 d
0
dP ((2λ)−1 ◦ πλ ◦ F t )
sup
E
σi f (Xi )
f ∈(2λ)−1 ◦πλ ◦F t
n
n
i=1
also notice that constant ˜c doesn’t depend on F t or λ. Next, since (2λ)−1 ◦ πλ is a non-decreasing
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ c
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
r
transform, we use inequality dP ((2λ)−1 ◦ πλ ◦ F t ) ≤ dP (F t ) (e.g. [11, Theorem 11.3])
nX
dP (F t )
σi f (Xi )
sup
E
.
f ∈(2λ)−1 ◦πλ ◦F t
n
n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ cλ
i=1
r (V + 1)(t + 1) log2 [2(t + 1)/ ln 2]
And then, since Lemma 1 gives an upper-bound on the pseudodimension of the class F t , we have
nX
σi f (Xi )
n
n
i=1

E sup
f ∈πλ ◦F t

,

,

,

,

with constant c above being independent of H, t and λ. To prove the second statement we use
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ Mϕ,λ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
McDiarmid’s bounded difference inequality [16, Theorem 9.2, p. 136], since ∀i
ϕ,n (f )|
|Rϕ (f ) − R0
|Rϕ (f ) − Rϕ,n (f )| − sup
sup
sup
f ∈πλ ◦F t
f ∈πλ ◦F t
i ,y 0
j=1 ,(x0
n
(xj ,yj )n
i )
where R0
i , y 0
ϕ,n (f ) is obtained from Rϕ,n (f ) by changing pair (xi , yi ) to (x0
i ). This completes the
(cid:5)
proof of the lemma.
Lemma 2, unlike [6, Lemma 2], allows us to choose the number of steps t, that describes the com-
plexity of the linear combination of base functions in addition to the parameter λ, which governs the
size of the deviations of the functions in F , and this is essential for the proof of the consistency. It
is easy to see that for AdaBoost (i.e. ϕ(x) = e−x ) we have to choose λ = κ ln n and t = nν with
κ > 0, ν > 0 and 2κ + ν < 1. So far we dealt with the statistical properties of the function we are
minimizing, now we turn to the algorithmic part. We need the following simple consequence of the
proof of [10, Theorem 1]
Theorem 1 Let function Q(f ) be convex in f . Let Q∗ = limλ→∞ inf f ∈Fλ Q(f ). Assume that
∀c1 , c2 , s.t. Q∗ < c1 < c2 < ∞,
0 < inf {Q00 (f ; h) : c1 < Q(f ) < c2 , h ∈ H}
≤ sup{Q00 (f ; h) : Q(f ) < c2 , h ∈ H} < ∞.
Then for any reference function ¯f and the sequence of functions fm , produced by the boosting
s
(cid:19)− 1
(cid:18)
algorithm, the following bound holds ∀m s.t. Q(fm ) > Q( ¯f ).
8B 3Q(f0 )(Q(f0 ) − Q( ¯f ))
0 + c3 (m + 1)
ln ‘2
Q(fm ) ≤ Q( ¯f ) +
2
where ‘k = (cid:13)(cid:13) ¯f − fk
(cid:13)(cid:13)∗ , c3 = 2Q(f0 )/β , β = inf {Q00 (f ; h) : Q( ¯f ) < Q(f ) < Q(f0 ), h ∈ H},
(6)
,
‘2
β 3
0
B = sup{Q00 (f ; h) : Q(f ) < Q(f0 ), h ∈ H}.
Proof. The statement of the theorem is a version of the result implicit in the proof of [10, Theorem
1]. If for some m we have Q(fm ) ≤ Q( ¯f ), then theorem is trivially true for all m0 ≥ m. Therefore,
we are going to consider only the case when Q(fm+1 ) > Q( ¯f ). By convexity of Q(·)
Let fm − ¯f = P ˜αi
|Q0 (fm ; fm − ¯f )| ≥ Q(fm ) − Q( ¯f ) = m .
(7)
˜hi , where ˜αi and ˜hi correspond to the best representation (with the smallest
(cid:12)(cid:12)(cid:12) ≤ sup
m ≤ (cid:12)(cid:12)(cid:12)X ˜αiQ0 (fm ; ˜hi )
|Q0 (fm ; h)| X | ˜αi |,
l∗ -norm). Then from (7) and linearity of the derivative we have
h∈H
(cid:13)(cid:13)fm − ¯f (cid:13)(cid:13)∗
Q0 (fm ; h) ≥
m
sup
h∈H
1
2 α2Q00 ( ˜fm ; hm ),
Q(fm + αhm ) = Q(fm ) + αQ0 (fm ; hm ) +
where ˜fm = fm + ˜αmhm , for ˜αm ∈ [0, αm ], and since by assumption ˜fm is on the path from fm to
fm+1 we have the following bounds
Q( ¯f ) < Q(fm+1 ) ≤ Q( ˜fm ) ≤ Q(fm ) ≤ Q(f0 ),
then by assumption of the theorem for β , that depends on Q( ¯f ), we have
2 α2β ) = Q(fm ) − |Q0 (fm ; hm )|2
1
Q(fm+1 ) ≥ Q(fm ) + inf
α∈R(αQ0 (fm ; hm ) +
2β
(cid:18)
On the other hand,

therefore

Next,

(8)

.

h∈H,α∈R Q(fm + αh) ≤ inf
Q(fm + αmhm ) =
inf
h∈H,α∈R
= Q(fm ) − suph∈H |Q0 (fm ; h)|2
2B

.

Q(fm ) + αQ0 (fm ; h) +

.

(9)
(cid:19)
1
2 α2B )

(10)

(11)

(12)
if |αm | <

(13)

(14)

(cid:17)1/2(cid:19)2

m

,

.

β
B

.

but by (10)

≥ m
β
‘mB 3/2

,
≥ |Q0 (fm ; hm )|2
2B
√

r
Therefore, combining (9) and (10) , we get
|Q0 (fm ; h)|
|Q0 (fm ; hm )| ≥ sup
h∈H
Another Taylor expansion, this time around fm+1 , gives us
1
mQ00 (˜˜f m ; hm ),
Q(fm ) = Q(fm+1 ) +
2 α2
where ˜˜f m is some (other) function on the path from fm to fm+1 . Therefore,
|Q0 (fm ; hm )|/B , then
|Q0 (fm ; hm )|2
Q(fm ) − Q(fm+1 ) <
2B
Q(fm ) − Q(fm+1 ) ≥ suph∈H |Q0 (fm ; h)|2
2B
therefore we conclude, by combining (11) and (8), that
√
β suph∈H |Q0 (fm ; h)|
|αm | ≥ |Q0 (fm ; hm )|
≥
B 3/2
B
mX
mX
Using (12) we have
i ≤ 2
(Q(fi ) − Q(fi+1 )) ≤ 2
(Q(f0 ) − Q( ¯f )).
α2
β
β
i=0
i=0
m−1X
(cid:13)(cid:13)fm − ¯f (cid:13)(cid:13)∗ ≤ (cid:13)(cid:13)fm−1 − ¯f (cid:13)(cid:13)∗ + |αm−1 | ≤ (cid:13)(cid:13)f0 − ¯f (cid:13)(cid:13)∗ +
 m−1X
!1/2
≤ (cid:13)(cid:13)f0 − ¯f (cid:13)(cid:13)∗ +
i=0
√
α2
,
i
i=0
mX
mX
(Q(f0 ) − Q( ¯f )) ≥ mX
therefore, combining with (14) and (13), since sequence i is decreasing,
(cid:18)
2
2
≥ β
i ≥ β
i
B 3 2
α2
m
‘2
B 3
β
i
mX
i=0
i=0
i=0
(cid:18)
(cid:17)1/2(cid:19)2
(cid:16) 2Q(f0 )
1
mX
i=0
β
1
β
2B 3 2
m
Z m+1
0 + 2Q(f0 )
‘2
i=0
β
0

≥ β
B 3 2
m

1
b

ln a + b(m + 1)
a

,

1
a + bi

dx
a + bx

(cid:16)Pi−1
1
j=0 α2
j

Recall that

√

i

√

i

‘0 +

.

i

=

|αi |

Since

‘0 +

≥
mX
i=0

≥

then

Therefore

2
β

m ≤

(Q(f0 ) − Q( ¯f )) ≥
s

8B 3Q(f0 )(Q(f0 ) − Q( ¯f ))
β 3

β 2
m ln
4B 3Q(f0 ) 2
 
ln

0 + 2Q(f0 )
‘2
β
‘2
0

0 + 2Q(f0 )
‘2
β
‘2
0

(m + 1)
.
!− 1
2

(m + 1)

,

(cid:5)
and this completes the proof.
The theorem above allows us to get an upper bound on the difference between the ϕ-risk of the
function output by AdaBoost and the ϕ-risk of the appropriate reference function.

8
(R∗ )3/2

Theorem 2 Assume R∗ > 0. Let tn = nν be the number of steps we run AdaBoost, let λn = κ ln n,
with ν > 0, κ > 0 and ν + 2κ < 1. Let ¯fn be a minimizer of the function Rn (·) within Fλn . Then
(cid:18)
(cid:19)−1/2
for n large enough with high probability the following holds
n + (4/R∗ )tn
ln λ2
Rn (ftn ) ≤ Rn ( ¯fn ) +
λ2
n
nX
nX
Proof. This theorem follows directly from Theorem 1. Because in AdaBoost
1
1
(−Yih(Xi ))2 e−Yi f (Xi ) =
R00
e−Yi f (Xi ) = R(f )
n (f ; h) =
(6) we have B = Rn (f0 ) = 1, β ≥ Rn ( ¯fn ), (cid:13)(cid:13)f0 − ¯fn
(cid:13)(cid:13)∗ ≤ λn . Since for t s.t. Rn (ft ) ≤ Rn ( ¯fn )
n
n
i=1
i=1
then all the conditions in Theorem 1 are satisﬁed (with Q(f ) replaced by Rn (f )) and in the Equation
r ln(1/δ)
r 2V ln(4n + 2)
the theorem is trivially true we only have to notice that Lemma 2 guarantees that with probability at
least 1 − δ
|R( ¯fn ) − Rn ( ¯fn )| ≤ 4λnLϕ,λn
+ Mϕ,λn
.
2n
n
Thus for n such that the r.h.s. of the above expression is less than R∗ /2 we have β ≥ Rn ( ¯fn ) ≥
(cid:5)
R∗ /2 and the result follows immediately from Equation (6) if we use the fact that Rn ( ¯f ) > 0.
Then, having all the ingredients at hand we can formulate the main result of the paper.
Theorem 3 Assume V = dV C (H) < ∞, L∗ > 0,
R(f ) = R∗ ,
λ→∞ inf
lim
f ∈Fλ
tn → ∞, and tn = O(nν ) for ν < 1. Then AdaBoost stopped at step tn returns a sequence of
classiﬁers almost surely satisfying L(g(ftn )) → L∗ .
Proof. For the exponential loss function L∗ > 0 implies R∗ > 0. Let λn = κ ln n, κ > 0,
2κ + ν < 1. Also, let ¯f be a minimizer of R and ¯fn be a minimizer of Rn within Fλn . Then we
have
R(πλn (ftn )) ≤ Rn (πλn (ftn )) + 1
by Lemma 2
≤ Rn (ftn ) + 1 + ϕ(λn )
since ϕ(πλn (x)) ≤ ϕ(x) + ϕ(λn )
≤ Rn ( ¯fn ) + 1 + ϕ(λn ) + 2
(16)
by Theorem 2
≤ R( ¯f ) + 1 + ϕ(λn ) + 2 + 3
(17)
by Lemma 2.
Inequalities (15) and (17) hold with probability at least 1 − δn , while inequality (16) is true for
r ln(1/δn )
r (V + 1)(nν + 1) log2 [2(nν + 1)/ ln 2]
sufﬁciently large n when (17) holds. The ’s above are
(cid:18)
(cid:19)−1/2
+ nκ
2n
n
(κ ln n)2 + (4/R∗ )nν
r 2V ln(4n + 2)
r ln(1/δn )
8
ln
,
(R∗ )3/2
(κ ln n)2
3 = 4nκκ ln n
+ nκ
2n
n
and ϕ(λn ) = n−κ . Therefore, by the choice of ν and κ and appropriate choice of δn , for example
δn = n−2 , we have 1 → 0, 2 → 0, 3 → 0 and ϕ(λn ) → 0. Also, R( ¯f ) → R∗ by Assumption 1.
Now we appeal to the Borel-Cantelli lemma and arrive at R(πλ (ftn )) → R∗ a.s. Eventually we can
use [17, Theorem 3] to conclude that

(15)

1 = cnκκ ln n

2 =

L(g(πλn (ftn )))a.s.→ L∗ .
But for λn > 0 we have g(πλn (ftn )) = g(ftn ), therefore
L(g(ftn ))a.s.→ L∗ .
Hence AdaBoost is consistent if stopped after nν steps.

(cid:5)

4 Discussion

We showed that AdaBoost is consistent if stopped sufﬁciently early, after tn iterations, for tn = nν
with ν < 1, given that Bayes risk L∗ > 0. It is unclear whether this number can be increased.
Results by Jiang [5] imply that for some X and function class H AdaBoost algorithm will achieve
zero training error after tn steps, where n2 /tn = o(1). We don’t know what happens in between
O(n1−ε ) and O(n2 ln n). Lessening this gap is a subject of further research.
We analyzed only AdaBoost, the boosting algorithm that uses loss function ϕ(x) = e−x . Since
the proof of Theorem 2 relies on the properties of the exponential loss, we cannot make a similar
conclusion for other versions of boosting, e.g., logit boosting with ϕ(x) = ln(1 + e−x ): in this
n (f ; h) ≥ Rn (f )/n, though the resulting
case assumption on the second derivative holds with R00
inequality is trivial, the factor 1/n precludes us from ﬁnding any useful bound. It is a subject of
future work to ﬁnd an analog of Theorem 2 that will handle logit loss.

Acknowledgments

We gratefully acknowledge the support of NSF under award DMS-0434383.

References
[1] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139,
1997.
[2] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.
[3] Leo Breiman. Arcing classiﬁers (with discussion). The Annals of Statistics, 26(3):801–849,
1998. (Was Department of Statistics, U.C. Berkeley Technical Report 460, 1996).
[4] Leo Breiman. Some inﬁnite theory for predictor ensembles. Technical Report 579, Department
of Statistics, University of California, Berkeley, 2000.
[5] Wenxin Jiang. On weak base hypotheses and their implications for boosting regression and
classiﬁcation. The Annals of Statistics, 30:51–73, 2002.
[6] G ´abor Lugosi and Nicolas Vayatis. On the Bayes-risk consistency of regularized boosting
methods. The Annals of Statistics, 32(1):30–55, 2004.
[7] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex
risk minimization. The Annals of Statistics, 32(1):56–85, 2004.
[8] Tong Zhang and Bin Yu. Boosting with early stopping: convergence and consistency. The
Annals of Statistics, 33:1538–1579, 2005.
[9] Wenxin Jiang. Process consistency for AdaBoost. The Annals of Statistics, 32(1):13–29, 2004.
[10] P. J. Bickel, Y. Ritov, and A. Zakai. Some theory for generalized boosting algorithms. Journal
of Machine Learning Research, 7:705–732, May 2006.
[11] Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cam-
bridge University Press, 1999.
[12] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the general-
ization error of combined classiﬁers. The Annals of Statistics, 30:1–50, 2002.
[13] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer-Verlag, New
York, 1991.
[14] Richard M. Dudley. Uniform central limit theorems. Cambridge University Press, Cambridge,
MA, 1999.
[15] David Pollard. Empirical Processes: Theory and Applications. IMS, 1990.
[16] Luc Devroye, L ´aszl ´o Gy ¨orﬁ, and G ´abor Lugosi. A Probabilistic Theory of Pattern Recognition.
Springer, New York, 1996.
[17] Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classiﬁcation, and risk
bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.


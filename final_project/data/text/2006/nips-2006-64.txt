Training Conditional Random Fields for Maximum
Labelwise Accuracy

Samuel S. Gross
Computer Science Department
Stanford University
Stanford, CA, USA
ssgross@cs.stanford.edu

Olga Russakovsky
Computer Science Department
Stanford University
Stanford, CA, USA
olga@cs.stanford.edu

Chuong B. Do
Computer Science Department
Stanford University
Stanford, CA, USA
chuongdo@cs.stanford.edu

Seraﬁm Batzoglou
Computer Science Department
Stanford University
Stanford, CA, USA
serafim@cs.stanford.edu

Abstract

We consider the problem of training a conditional random ﬁel d (CRF) to max-
imize per-label predictive accuracy on a training set, an approach motivated by
the principle of empirical risk minimization. We give a gradient-based procedure
for minimizing an arbitrarily accurate approximation of the empirical risk under
a Hamming loss function. In experiments with both simulated and real data, our
optimization procedure gives signi ﬁcantly better testing performance than several
current approaches for CRF training, especially in situations of high label noise.

1

Introduction

Sequence labeling, the task of assigning labels y = y1 , ..., yL to an input sequence x = x1 , ..., xL , is
a machine learning problem of great theoretical and practical interest that arises in diverse ﬁelds such
as computational biology, computer vision, and natural language processing. Conditional random
ﬁelds (CRFs) are a class of discriminative probabilistic mo dels designed speci ﬁcally for sequence
labeling tasks [1]. CRFs deﬁne the conditional distributio n Pw (y | x) as a function of features
relating labels to the input sequence.
Ideally, training a CRF involves ﬁnding a parameter set w that gives high accuracy when labeling
new sequences. In some cases, however, simply ﬁnding parame ters that give the best possible ac-
curacy on training data (known as empirical risk minimization [2]) can be difﬁcult. In particular, if
we wish to minimize Hamming loss, which measures the number of incorrect labels, gradient-based
optimization methods cannot be applied directly.1 Consequently, surrogate optimization problems,
such as maximum likelihood or maximum margin training, are solved instead.
In this paper, we describe a training procedure that addresses the problem of minimizing empirical
per-label risk for CRFs. Speci ﬁcally, our technique attemp ts to minimize a smoothed approximation
of the Hamming loss incurred by the maximum expected accuracy decoding (i.e., posterior decod-
ing) algorithm on the training set. The degree of approximation is controlled by a parameterized
function Q(·) which trades off between the accuracy of the approximation and the smoothness of
the objective. In the limit as Q(·) approaches the step function, the optimization objective converges
to the empirical risk minimization criterion for Hamming loss.

1The gradient of the optimization objective is everywhere zero (except at points where the objective is
discontinuous), because a sufﬁciently small change in parameters will n ot change the predicted labeling.

2 Preliminaries

2.1 Deﬁnitions

Let X L denote an input space of all possible input sequences, and let Y L denote an output space
of all possible output labels. Furthermore, for a pair of consecutive labels yj−1 and yj , an input
sequence x, and a label position j , let f (yj−1 , yj , x, j ) ∈ Rn be a vector-valued function; we call f
the feature mapping of the CRF.
A conditional random ﬁeld (CRF) deﬁnes the conditional prob
an input sequence x as
exp (cid:16)PL
j=1 wT f (yj−1 , yj , x, j )(cid:17)
exp (cid:0)wT F1,L (x, y)(cid:1)
Py′∈Y L exp (cid:16)PL
j , x, j )(cid:17)
Z (x)
j=1 wT f (y ′
j−1 , y ′
where we deﬁne the summed feature mapping, Fa,b (x, y) = Pb
j=a f (yj−1 , yj , x, j ), and where the
partition function Z (x) = Py′ exp (cid:0)wT F1,L (x, y′ )(cid:1) ensures that the distribution is normalized for
any set of model parameters w.2
2.2 Maximum a posteriori vs. maximum expected accuracy parsing

ability of a labeling (or parse) y given

Pw (y | x) =

(1)

=

,

Given a CRF with parameters w, the sequence labeling task is to determine values for the labels y
of a new input sequence x. One approach is to choose the most likely, or maximum a posteriori,
labeling, arg maxy Pw (y | x). This can be computed efﬁciently using the Viterbi algorith m.
An alternative approach, which seeks to maximize the per-label accuracy of the prediction rather
than the joint probability of the entire parse, chooses the most likely (i.e., highest posterior proba-
bility) value for each label separately. Note that
j = yj }
Ey′ 
L
L
Xj=1
Xj=1
1{y ′


where 1{condition} denotes the usual indicator function whose value is 1 when condition is true
and 0 otherwise, and where the expectation is taken with respect to the conditional distribution
Pw (y′ | x). From this, we see that maximum expected accuracy parsing chooses the parse with the
maximum expected number of correct labels.
In practice, maximum expected accuracy parsing often yields more accurate results than Viterbi
parsing (on a per-label basis) [3, 4, 5]. Here, we restrict our focus to maximum expected accu-
racy parsing procedures and seek training criteria which optimize the performance of a CRF-based
maximum expected accuracy parser.

Pw (yj | x) = arg max
y

arg max
y

(2)

3 Training conditional random ﬁelds

Usually, CRFs are trained in the batch setting, where a complete set D = {(x(t) , y(t) )}m
t=1 of
training examples is available up front. In this case, training amounts to numerical optimization of
a ﬁxed objective function R(w : D). A good objective function is one whose optimal value leads
to parameters that perform well, in an application-dependent sense, on previously unseen testing
examples. While this can be difﬁcult to achieve without knowi ng the contents of the testing set, one
can, under certain conditions, guarantee that the accuracy of a learned CRF on an unseen testing set
is probably not much worse than its accuracy on the training set.
In particular, when assuming independently and identically distributed (i.i.d.) training and testing
examples, there exists a probabilistic bound on the difference between empirical risk and generaliza-
tion error [2]. As long as enough training data are available (relative to model complexity), strong
training set performance will imply, with high probability, similarly strong testing set performance.
Unfortunately, minimizing empirical risk for a CRF is a very difﬁcult task. Loss functions based on
usual notions of per-label accuracy (such as Hamming loss) are typically not only nonconvex but
also not amenable to optimization by methods that make use of gradient information.

2We assume for simplicity the existence of a special initial label y0 .

In this section, we brieﬂy describe three previous approach es for CRF training which optimize surro-
gate loss functions in lieu of the empirical risk. Then, we consider a new method for gradient-based
CRF training oriented more directly toward optimizing predictive performance on the training set.
Our method minimizes an arbitrarily accurate approximation of empirical risk, where the loss func-
tion is deﬁned as the number of labels predicted incorrectly by maximum expected accuracy parsing.

3.1 Previous objective functions

3.1.1 Conditional log-likelihood

log Pw (y(t) | x(t) )

RCLL (w : D) = C ||w||2 −

Conditional log-likelihood is the most commonly used objective function for training conditional
random ﬁelds. In this criterion, the loss suffered for a trai ning example (x(t) , y(t) ) is the negative
log probability of the true parse according to the model, plus a regularization term:
m
Xt=1
The convexity and differentiability of conditional log-likelihood ensure that gradient-based opti-
mization procedures (e.g., conjugate gradient or L-BFGS [6]) will not converge to suboptimal local
minima of the objective function.
However, there is no guarantee that the parameters obtained by conditional log-likelihood training
will lead to the best per-label predictive accuracy, even on the training set. For one, maximum
likelihood training explicitly considers only the probability of exact training parses. Other parses,
even highly accurate ones, are ignored except insofar as they share common features with the exact
parse. In addition, the log-likelihood of a parse is largely determined by the sections which are most
difﬁcult to correctly label. This can be a weakness in proble ms with signi ﬁcant label noise (i.e.,
incorrectly labeled training examples).

(3)

3.1.2 Pointwise conditional log likelihood

Rpointwise (w : D) = C ||w||2 −

Kakade et al. investigated an alternative nonconvex training objective for CRFs [7, 8] which con-
siders separately the posterior label probabilities at each position of each training sequence.
In
this approach, one maximizes not the probability of an entire parse, but instead the product of the
posterior probabilities (or equivalently, sum of log posteriors) for each predicted label:
L
m
Xj=1
Xt=1
By using pointwise posterior probabilities, this objective function takes into account suboptimal
parses and focuses on ﬁnding a model whose posteriors match w ell with the training labels, even
though the model may not provide a good ﬁt for the training dat a as a whole.
Nevertheless, pointwise logloss is fundamentally quite different from Hamming loss. A training
procedure based on pointwise log likelihood, for example, would prefer to reduce the posterior
probability for a correct label from 0.6 to 0.4 in return for improving the posterior probability for
a hopelessly incorrect label from 0.0001 to 0.01. Thus, the objective retains the difﬁculties of the
regular conditional log likelihood when dealing with difﬁc ult-to-classify outlier labels.

log Pw (y (t)
j

| x(t) )

(4)

3.1.3 Maximum margin training

and Tsochantaridis et al. [10].

Rmax margin (w : D) = C ||w||2 +

The notion of Hamming distance is incorporated directly in the maximum margin training proce-
dures of Taskar et al. [9]:
m
max (cid:18)0, max
y∈Y L (cid:16)∆(y, y(t) ) − wT δF1,L (x(t) , y)(cid:17)(cid:19) ,
Xt=1
m
∆(y, y(t) ) (cid:16)1 − wT δF1,L (x(t) , y)(cid:17)(cid:19) .
max (cid:18)0, max
Xt=1
y∈Y L
Here, ∆(y, y(t) ) denotes the Hamming distance between y and y(t) , and δF1,L (x(t) , y) =
F1,L (x(t) , y(t) ) − F1,L (x(t) , y). In the former formulation, loss is incurred when the Hamming
distance between the correct parse y(t) and a candidate parse y exceeds the obtained classi ﬁcation

Rmax margin (w : D) = C ||w||2 +

(5)

(6)

margin between y(t) and y. In the latter formulation, the amount of loss for a margin violation scales
linearly with the Hamming distance betweeen y(t) and y.
Both cases lead to convex optimization problems in which the loss incurred for a particular training
example is an upper bound on the Hamming loss between the correct parse and its highest scoring
alternative. In practice, however, this upper bound can be quite loose; thus, parameters obtained via
a maximum margin framework may be poor minimizers of empirical risk.

3.2 Training for maximum labelwise accuracy

In each of the likelihood-based or margin-based objective functions introduced in the previous sub-
sections, difﬁculties arose due to the mismatch between the chosen objective function and our notion
of empirical risk as deﬁned by Hamming loss. In this section, we demonstrate how to construct a
smooth objective function for maximum expected accuracy parsing which more closely approxi-
mates our desired notion of empirical risk.

3.2.1 The labelwise accuracy objective function

R(w : D) =

Consider the following objective function,
Pw (yj | x(t) )) .
1 (y (t)
L
m
Xj=1
Xt=1
j = arg max
yj
Maximizing this objective is equivalent to minimizing empirical risk under the Hamming loss (i.e.,
the number of mispredicted labels). To obtain a smooth approximation to this objective function,
we can express the condition that the algorithm predicts the correct label for y (t)
in terms of the
j
posterior probabilities of correct and incorrect labels as
Pw (y (t)
Pw (yj | x(t) ) > 0.
| x(t) ) − max
j
(t)
yj 6=y
j
Substituting equation (8) back into equation (7) and replacing the indicator function with a generic
function Q(·), we obtain

(7)

(8)

(9)

Rlabelwise (w) =

Pw (yj | x(t) )! .
Q  Pw (y (t)
L
m
Xj=1
Xt=1
| x(t) ) − max
j
(t)
yj 6=y
j
When Q(·) is chosen to be the indicator function, Q(x) = 1{x > 0}, we recover the original
objective. By choosing a nicely behaved form for Q(·), however, we obtain a new objective that is
easier to optimize. Speci ﬁcally, we set Q(x) to be sigmoidal with parameter λ (see Figure 2a):
1
1 + exp(−λx)
As λ → ∞, Q(x; λ) → 1{x > 0}, so Rlabelwise (w : D) approaches the objective function deﬁned
in (7). However, Rlabelwise (w : D) is smooth for any ﬁnite λ > 0.
Because of this, we are free to use gradient-based optimization to maximize our new objective
function. As λ get larger, the quality of our approximation of the ideal Hamming loss objective
improves; however, the approximation itself also becomes less smooth and perhaps more difﬁcult
to optimize as a result. Thus, the value of λ controls the trade-off between the accuracy of the
approximation and the ease of optimization.3

Q(x; λ) =

(10)

.

3.2.2 The labelwise accuracy objective gradient

We now present an algorithm for efﬁciently calculating the g radient of the approximate accuracy
j denote the label other than y (t)
objective. For a ﬁxed parameter set w, let ˜y (t)
that has the maximum
j
posterior probability at position j . Also, for notational convenience, let y1:j denote the variables

3 In particular, note that that the method of using Q(x; λ) to approximate the step function is analogous to
the log-barrier method used in convex optimization for approximating inequality constraints using a smooth
function as a surrogate for the inﬁnite height barrier. As with log-barrier optimization, performing the max-
imization of Rlabelwise (w : D) using a small value of λ, and gradually increasing λ while using the previous
solution as a starting point for the new optimization, provides a viable technique for maximizing the labelwise
accuracy objective.

| x(t) ) − Pw ( ˜y (t)
j

| x(t) ) − Pw ( ˜y (t)
j

| x(t) )i .

y1 , . . . , yj . Differentiating equation (9), we compute ∇wRlabelwise (w : D) to be4
L
m
| x(t) )(cid:17) ∇w hPw (y (t)
Q′ (cid:16)Pw (y (t)
Xj=1
Xt=1
j
j
Using equation (1), the inner term, Pw (y (t)
| x(t) ) − Pw ( ˜y (t)
| x(t) ), is equal to
j
j
1
· Xy1:L (cid:16)1{yj = y (t)
j }(cid:17) · exp (cid:16)wT F1,L (x(t) , y)(cid:17) .
j } − 1{yj = ˜y (t)
Z (x(t) )
Applying the quotient rule allows us to compute the gradient of equation (12), whose complete form
we omit for lack of space. Most of the terms involved in the gradient are easy to compute using the
standard forward and backward matrices used for regular CRF inference, which we deﬁne here as
(13)
α(i, j ) = Py1:j 1{yj = i} · exp (cid:0)wT F1,j (x(t) , y)(cid:1)
(14)
β (i, j ) = Pyj :L 1{yj = i} · exp (cid:0)wT Fj+1,L (x(t) , y)(cid:1) .
The two difﬁcult terms that do not follow from the forward and backward matrices have the form,
L
k } · F1,L (x(t) , y) · exp (cid:16)wT F1,L (x(t) , y)(cid:17) ,
Xk=1 Xy1:L
Q′
(15)
k (w) · 1{yk = y⋆
j (w) = Q′ (cid:16)Pw (y (t)
| x(t) )(cid:17) and y⋆ is either y(t) or ˜y(t) . To efﬁciently
| x(t) ) − Pw ( ˜y (t)
where Q′
j
j
compute terms of this type, we deﬁne
j
k (w) · exp (cid:16)wT F1,j (x(t) , y)(cid:17)
Xk=1 Xy1:j
k ∧ yj = i} · Q′
1{yk = y⋆
α⋆ (i, j ) =
(16)
L
k (w) · exp (cid:16)wT Fj+1,L (x(t) , y)(cid:17) .
Xk=j+1 Xyj :L
k ∧ yj = i} · Q′
1{yk = y⋆
Like the forward and backward matrices, α⋆ (i, j ) and β ⋆ (i, j ) may be calculated via dynamic pro-
1 (w) and
gramming.
In particular, we have the base cases α⋆ (i, 1) = 1{i = y⋆
1 } · α(i, 1) · Q′
β ⋆ (i, L) = 0. The remaining entries are given by the following recurrences:
α⋆ (i, j ) = Xi′ (cid:16)α⋆ (i′ , j − 1) + 1{i = y⋆
j (w)(cid:17) · ewT f (i′ ,i,x(t) ,j )
j } · α(i′ , j − 1) · Q′
(18)
β ⋆ (i, j ) = Xi′ (cid:16)β ⋆ (i′ , j + 1) + 1{i′ = y⋆
j+1 (w)(cid:17) · ewT f (i,i′ ,x(t) ,j+1) . (19)
j+1} · β (i′ , j + 1) · Q′
It follows that equation (15) is equal to
L
f (i′ , i, x(t) , j ) · exp (cid:16)wT f (i′ , i, x(t) , j )(cid:17) · (A + B ),
Xj=1 Xi′ Xi
A = α⋆ (i′ , j − 1) · β (i, j ) + α(i′ , j − 1) · β ⋆ (i, j )
(21)
j } · α(i′ , j − 1) · β (i, j ) · Q′
(22)
B = 1{i = y⋆
j (w).
Thus, the algorithm above computes the gradient in O(|Y |2 · L) time and O(|Y | · L) space. Since
α⋆ (i, j ) and β ⋆ (i, j ) must be computed for both y∗ = y(t) and y∗ = ˜y(t) , the resulting total gradient
computation takes approximately three times as long and uses twice the memory of the analogous
computation for the log likelihood gradient.5

β ⋆ (i, j ) =

(11)

(12)

(17)

(20)

where

4Technically, the max function is not differentiable. One could replace the max with a softmax function, and
assuming unique probabilities for each candidate label, the gradient approaches (11) as the softmax function
approaches the max. As noted in [11], this approximation used here does not cause problems in practice.
5We note that the “trick” used in the formulation of approximate accuracy is a pplicable to a variety of other
forms and arguments for Q(·). In particular, if we change its argument to Pw (y (t)
(t) ), letting Q(x) =
| x
j
log(x) gives the pointwise logloss formulation of Kakade et al. (see section 3.1.2), while letting Q(x) = x
gives an objective function equal to expected accuracy. Computing the gradient for these objectives involves
straightforward modi ﬁcations of the recurrences presented here.

(a)

(b)

y
c
a
r
u
c
c
A
 
e
s
i
w
l
e
b
a
L

0.8

0.78

0.76

0.74

0.72

0.7

0.68

0.66

0.64

Joint Log−Likelihood
Conditional Log−Likelihood
Maximum Margin
Maximum Labelwise Accuracy

0

0.05

0.1

0.15
0.2
Noise Parameter, p

0.25

0.3

0.35

0.4

Figure 1: Panel (a) shows the state diagram for the hidden Markov model used for the simulation
experiments. The HMM consists of two states (‘C’ and ‘I’) with transition probabilities labeled
on the arrows, and emission probabilities speci ﬁed (over th e alphabet {A, C, G, T }) written inside
each state. Panel (b) shows the proportion of state labels correctly predicted by the learned mod-
els at varying levels of label noise. The error bars show 95% conﬁdence intervals on the mean
generalization performance.

4 Results

4.1 Simulation experiments

To test the performance of the approximate labelwise accuracy objective function, we ﬁrst ran simu-
lation experiments in order to assess the robustness of several different learning algorithms in prob-
lems with a high degree of label noise. In particular, we generated sequences of length 1,000,000
from a simple two-state hidden Markov model (see Figure 1a). Given a ﬁxed noise parameter
p ∈ [0, 1], we generated training sequence labels by ﬂipping each run o f consecutive ‘C’ hidden
state labels to ‘I’ with probability p. After learning parameters, we then tested each algorithm on
uncorrupted testing sequence generated by the original HMM.
Figure 1b indicates the proportion of labels correctly identi ﬁed by four different methods at varying
noise levels: a generative model trained with joint log-likelihood, a CRF trained with conditional
log-likelihood, the maximum-margin method of Taskar et al. [9] as implemented in the SVMstruct
package [10]6 , and a CRF trained with maximum labelwise accuracy. No method outperforms
maximum labelwise accuracy at any noise level. For levels of noise above 0.05, maximum labelwise
accuracy performs signi ﬁcantly better than the other metho ds.
For each method, we used the decoding algorithm (Viterbi or MEA) that led to the best performance.
The maximum margin method performed best when Viterbi decoding was used, while the other three
methods had better performance with MEA decoding. Interestingly, with no noise present, maxi-
mum margin training with Viterbi decoding peformed signi ﬁc antly better than generative training
with Viterbi decoding (0.749 vs. 0.710), but this was still much worse than generative training with
MEA decoding (0.796).

4.2 Gene prediction experiments

To test the performance of maximum labelwise accuracy training on a large-scale, real world prob-
lem, we trained a CRF to predict protein coding genes in the genome of the fruit ﬂy Drosophila
melanogaster. The CRF labeled each base pair of a DNA sequence according to its predicted func-
tional category: intergenic, protein coding, or intronic. The features used in the model were of two
types: transitions between labels and trimer composition.
The CRF was trained on approximately 28 million base pairs labeled according to annotations from
the FlyBase database [12]. The predictions were evaluated on a separate testing set of the same size.
Three separate training runs were performed, using three different objective functions: maximum

6We were unable to get SVMstruct to converge on our test problem when using the Tsochantaridis et al. max-
imum margin formulation.

(a)

s
s
o
l
 
l
e
b
a
l
−
r
e
p

3

2.5

2

1.5

1

0.5

0

−0.5
−1

(c)

 0.82

 0.8

 0.78

 0.76

 0.74

 0.72

 0.7

 0.68

y
c
a
r
u
c
c
A

pointwise logloss
zero−one loss
Q(x; 15)

(b)

y
c
a
r
u
c
c
A

−0.5

0
0.5
P(incorrect label) − P(correct label)

1

Objective
Training Accuracy
Testing Accuracy

(d)

h
t
g
n
e
L
 
/
 
d
o
o
h
i
l
e
k
i
L
 
g
o
L

y
c
a
r
u
c
c
A

-0.002

-0.0025

-0.003

-0.0035

Objective
Training Accuracy
Testing Accuracy

 0.82

 0.8

 0.78

 0.76

 0.74

 0.72

 0.7

 0.68

 0

 10

 20
 30
Iterations

 40

 0.66
 50

Objective
Training Accuracy
Testing Accuracy

-1

-1.5

-2

-2.5

 0.82

 0.8

 0.78

 0.76

 0.74

 0.72

 0.7

 0.68

 0.66

 0.82

 0.8

 0.78

 0.76

 0.74

 0.72

 0.7

 0.68

y
c
a
r
u
c
c
A
 
e
t
a
m
i
x
o
r
p
p
A

h
t
g
n
e
L
 
/
 
d
o
o
h
i
l
e
k
i
L
 
g
o
L
 
e
s
i
w
t
n
i
o
P

 0.66

 0

-0.004
 50

 0.66

 0

 10

 40

 10

 20
 30
 20
 30
Iterations
Iterations
Figure 2: Panel (a) compares three pointwise loss functions in the special case where a label has two
possible values. The green curve (f (x) = − log( 1−x
2 )) depicts pointwise logloss; the red curve rep-
resents the ideal zero-one loss; and the blue curve gives the sigmoid approximation with parameter
15. Panels (b), (c), and (d) show gene prediction learning curves using three training objective func-
tions: (b) maximum labelwise (approximate) accuracy, (c) maximum conditional log-likelihood,
and (d) maximum pointwise conditional log-likelihood, respetively. In each case, parameters were
initialized to their generative model estimates.

 40

-3
 50

likelihood, maximum pointwise likelihood, and maximum labelwise accuracy. Each run was started
from an initial guess calculated using HMM-style generative parameter estimation.7
Figures 2b, 2c, and 2d show the value of the objective function and the average label accuracy
at each iteration of the three training runs. Here, maximum accuracy training improves upon the
accuracy of the original generative parameters and outperforms the other two training objectives.
In contrast, maximum likelihood training and maximum pointwise likelihood training both give
worse performance than the simple generative parameter estimates. Evidently, for this problem the
likelihood-based functions are poor surrogate measures for per-label accuracy: Figures 2c and 2d
show declines in training and testing set accuracy, despite increases in the objective function.

5 Discussion and related work

In contrast to most previous work describing alternative objective functions for CRFs, the method
described in this paper optimizes a direct approximation of the Hamming loss. A few notable papers
have also dealt with the problem of minimizing empirical risk directly. For binary classi ﬁers, Jan-
sche showed that an algorithm designed to optimize F-measure performance of a logistic regression
model for information extraction outperforms maximum likelihood training [14]. For parsing tasks,
Och demonstrated that a statistical machine translation system choosing between a small ﬁnite col-
lection of candidate parses achieves better accuracy when it is trained to minimize error rate instead

7We did not include maximum margin methods in this comparison; existing software packages for max-
imum margin training, based on the cutting plane algorithm [10] or decomposition techniques such as
SMO [9, 13], are not easily parallelizable and scale poorly for large datasets, such as those encountered in
gene prediction.

of optimizing the more traditional maximum mutual information criterion [15]. Unlike Och’s algo-
rithm, our method does not require one to provide a small set of candidate parses, instead relying on
efﬁcient dynamic programming recurrences for all computat
ions.
After this work was submitted for consideration, a Minimum Classi ﬁcation Error (MCE) method for
training CRFs to minimize empirical risk was independently proposed by Suzuki et al. [11]. This
technique minimizes the loss incurred by maximum a posteriori, rather than maximum expected
accuracy, parsing on the training set.
In practice, Viterbi parsers often achieve worse per-label
accuracy than maximum expected accuracy parsers [3, 4, 5]; we are currently exploring whether a
similar relationship also exists between MCE methods and our proposed training objective.
The training method described in this work is theoretically attractive, as it addresses the goal of
empirical risk minimization in a very direct way. In addition to its theoretical appeal, we have shown
that it performs much better than maximum likelihood and maximum pointwise likelihood training
on a large scale, real world problem. Furthermore, our method is efﬁcient, having time complexity
approximately three times that of maximum likelihood likelihood training, and easily parallelizable,
as each training example can be considered independently when evaluating the objective function
or its gradient. The chief disadvantage of our formulation is its nonconvexity. In practice, this can
be combatted by initializing the optimization with a parameter vector obtained by a convex training
method. At present, the extent of the effectiveness of our method and the characteristics of problems
for which it performs well are not clear. Further work applying our method to a variety of sequence
labeling tasks is needed to investigate these questions.

6 Acknowledgments

SSG and CBD were supported by NDSEG fellowships. We thank Andrew Ng for useful discussions.

References

[1] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁeld s: probabilistic models for segmenting
and labeling sequence data. In ICML, 2001.
[2] V. Vapnik. Statistical Learning Theory. Wiley, 1998.
[3] C. B. Do, M. S. P. Mahabhashyam, M. Brudno, and S. Batzoglou. ProbCons: probabilistic consistency-
based multiple sequence alignment. Genome Research, 15(2):330–340, 2005.
[4] C. B. Do, D. A. Woods, and S. Batzoglou. CONTRAfold: RNA secondary structure prediction without
physics-based models. Bioinformatics, 22(14):e90–e98, 2006.
[5] P. Liang, B. Taskar, and D. Klein. Alignment by agreement. In HLT-NAACL, 2006.
[6] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.
[7] S. Kakade, Y. W. Teh, and S. Roweis. An alternate objective function for Markovian ﬁelds. In ICML,
2002.
[8] Y. Altun, M. Johnson, and T. Hofmann. Investigating loss functions and optimization methods for dis-
criminative learning of label sequences. In EMNLP, 2003.
[9] B. Taskar, C. Guestrin, and D. Koller. Max margin markov networks. In NIPS, 2003.
[10] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interde-
pendent and structured output spaces. In ICML, 2004.
[11] J. Suzuki, E. McDermott, and H. Isozaki. Training conditional random ﬁelds with multivariate evaluation
measures. In ACL, 2006.
[12] G. Grumbling, V. Strelets, and The Flybase Consortium. FlyBase: anatomical data, images and queries.
Nucleic Acids Research, 34:D484–D488, 2006.
[13] J. Platt. Using sparseness and analytic QP to speed training of support vector machines. In NIPS, 1999.
[14] M. Jansche. Maximum expected F-measure training of logistic regression models. In EMNLP, 2005.
[15] F. J. Och. Minimum error rate training in statistical machine translation. In ACL, 2003.


Bayesian Model Scoring in Markov Random Fields

Sridevi Parise
Bren School of Information and Computer Science
UC Irvine
Irvine, CA 92697-3425
sparise@ics.uci.edu

Max Welling
Bren School of Information and Computer Science
UC Irvine
Irvine, CA 92697-3425
welling@ics.uci.edu

Abstract

Scoring structures of undirected graphical models by means of evaluating the
marginal likelihood is very hard. The main reason is the presence of the parti-
tion function which is intractable to evaluate, let alone integrate over. We propose
to approximate the marginal likelihood by employing two levels of approximation:
we assume normality of the posterior (the Laplace approximation) and approxi-
mate all remaining intractable quantities using belief propagation and the linear
response approximation. This results in a fast procedure for model scoring. Em-
pirically, we ﬁnd that our procedure has about two orders of magnitude better
accuracy than standard BIC methods for small datasets, but deteriorates when the
size of the dataset grows.

1 Introduction

Bayesian approaches have become an important modeling paradigm in machine learning. They
offer a very natural setting in which to address issues such as overﬁtting which plague standard
maximum likelihood approaches. A full Bayesian approach has its computational challenges as it
often involves intractable integrals. While for Bayesian networks many of these challenges have
been met successfully[3], the situation is quite reverse for Markov random ﬁeld models. In fact, it
is very hard to ﬁnd any literature at all on model order selection in general MRF models. The main
reason for this discrepancy is the fact that MRF models have a normalization constant that depends
on the parameters but is in itself intractable to compute, let alone integrate over. In fact, the presence
of this term even prevents one to draw samples from the posterior distribution in most situations
except for some special cases1 .
In terms of approximating the posterior some new methods have become available recently. In [7]
a number of approximate MCMC samplers are proposed. Two of them were reported to be most
successful: one based on Langevin sampling with approximate gradients given by contrastive di-
vergence and one where the acceptance probability is approximated by replacing the log partition
function with the Bethe free energy. Both these methods are very general, but inefﬁcient. In [2]
MCMC methods are explored for the Potts model based on the reversible jump formalism. To com-
pute acceptance ratios for dimension-changing moves they need to estimate the partition function
1 If one can compute the normalization term exactly (e.g. graphs with small treewidth) or if one can draw
perfect samples from the MRF [8](e.g. positive interactions only) then one construct a Markov chain for the
posterior.

p(D |θn )p(θn )/Q(θn |D)

using a separate estimation procedure making it rather inefﬁcient as well. In [6] and [8] MCMC
methods are proposed that use perfect samples to circumvent the calculation of the partition func-
tion altogether. This method is elegant but limited in its application due to the need to draw perfect
samples. Moreover, two approaches that approximate the posterior by a Gaussian distribution are
proposed in [11] (based on expectation propagation) and [13] (based on the Bethe-Laplace approxi-
mation).
In this paper we focus on a different problem, namely that of approximating the marginal likelihood.
This quantity is at the heart of Bayesian analysis because it allows one to compare models of different
structure. One can use it to either optimize or average over model structures. Even if one has an
approximation to the posterior distribution it is not at all obvious how to use it to compute a good
estimate for the marginal likelihood. The most direct approach is to use samples from the posterior
N(cid:88)
and compute importance weights,
p(D) ≈ 1
N
n=1
where Q(θn |D) denotes the approximate posterior. Unfortunately, this importance sampler suffers
from very high variance when the number of parameters becomes large. It is not untypical that the
estimate is effectively based on a single example.
We propose to use the Laplace approximation, including all O(1) terms where the intractable quan-
tities of interest are approximated by either belief propagation (BP) or the linear response theorem
based on the solution of BP. We show empirically that the O(1) terms are indispensable for small
N . Their inclusion can improve accuracy to up to two orders of magnitude. At the same time we
observe that as a function of N , the O(1)-term based on the covariance between features deteriorates
and should be omitted for large N . We conjecture that this phenomenon is explained by the fact that
the calculation of the covariance between features, which is equal to the second derivative of the
log-normalization constant, becomes instable if the bias in the MAP estimate of the parameters is of
the order of the variance in the posterior. For any biased estimate of the parameters this phenomenon
is therefore bound to happen as we increase N because the variance of the posterior distribution is
expected to decrease with N .
In summary we present a very accurate estimate for the marginal likelihood where it is most needed,
i.e.
for small N . This work seems to be the ﬁrst practical method for estimating the marginal
evidence in undirected graphical models.

θn ∼ Q(θn |D)

(1)

2 The Bethe-Laplace Approximation for log p(D)
(cid:105)
(cid:104)
Without loss of generality we represent a MRF as a log-linear model,
1
p(x|λ) =
λT f (x)
Z (λ)

exp

(2)

where f (x) represent features. In the following we will assume that the random variables x are
observed. Generalizations to models with hidden variables exist in theory but we defer the empirical
evaluation of this case to future research.
(cid:90)
To score a structure we will follow the Bayesian paradigm and aim to compute the log-marginal
likelihood log p(D) where D represents a dataset of size N ,
dλ p(D|λ) p(λ)

log p(D) = log

(3)

where p(λ) is some arbitrary prior on the parameters λ.
In order to approximate this quantity we employ two approximations. Firstly, we expand the both
log-likelihood and log-prior around the MAP value λMP . For the log-likelihood this boils down to
expanding the log-partition function,
log Z (λ) ≈ log Z (λMP ) + κT δλ +

(4)

1
2 δλT C δλ

with δλ = (λ − λMP ) and
C = E[f (x)f (x)T ]p(x) − E[f (x)]p(x)E[f (x)]T
p(x) ,
and where all averages are taken over p(x|λMP ).
Similarly for the prior we ﬁnd,

κ = E[f (x)]p(x)

(5)

(6)

log p(λ) = log p(λMP ) + gT δλ +

1
2 δλT H δλ
where g is the ﬁrst derivative of log p evaluated at λMP and H is the second derivative (or Hessian).
(cid:90)
The variables δλ represent ﬂuctuations of the parameters around the MAP value λMP . The marginal
likelihood can now be approximated by integrating out the ﬂuctuations δλ, considering λMP as a
hyper-parameter,

log p(D) = log

dδλ p(D|δλ, λMP ) p(δλ|λMP )
Inserting the expansions eqns.4 and 6 into eqn.7 we arrive at the standard expression for the Laplace
approximation applied to MRFs,
(cid:88)
log p(D) ≈
λMP T f (xn ) − N log Z (λMP ) + log p(λMP ) +
n
with F the number of features.
The difference with Laplace approximations for Bayesian networks is the fact that many terms in
the expression above can not be evaluated. First of all, determining λMP requires running gradient
ascent or iterative scaling to maximize the penalized log-likelihood which requires the computation
of the average sufﬁcient statistics E[f (x)]p(x) . Secondly, the expression contains the log-partition
function Z (λMP ) and the covariance matrix C which are both intractable quantities.

2 F log(N ) − 1
2 F log(2π) − 1
1
2

(8)
log det(C − H
N

(7)

)

2.1 The BP-Linear Response Approximation

To make further progress, we introduce a second layer of approximations based on belief propa-
gation. In particular, we approximate the required marginals in the gradient for λMP with the ones
obtained with BP. For fully observed MRFs the value for λMP will be very close to the solution ob-
tained by pseudo-moment matching (PMM) [5]; the inﬂuence of the prior being the only difference
between the two. Hence, we use λPMM to initialize gradient descent. The approximation incurred by
PMM is not always small [10] in which case other approximations such as contrastive divergence
may be substituted instead. The term − log Z (λMP ) will be approximated with the Bethe free en-
ergy. This will involve running belief propagation on a model with parameters λMP and inserting the
beliefs at their ﬁxed points into the expression for the Bethe free energy [16].
To compute the covariance matrix between the features C (eqn.5), we use the linear response al-
gorithm of [15]. This approximation is based on the observation that C is the Hessian of the log-
partition function w.r.t. the parameters. This is approximated by the Hessian of the Bethe free energy
(cid:88)
w.r.t. the parameters which in turn depends to the partial derivatives of the beliefs from BP w.r.t. the
parameters.
≈ − ∂ 2 log FBethe (λ)
∂λα∂λβ
xα
where λ = λMP , pBP
α is the marginal computed using belief propagation and xα is the collection of
variables in the argument of feature fα (e.g. nodes or edges). This approximate C is also guaranteed
to be symmetric and positive semi-deﬁnite. In [15] two algorithms were discussed to compute C
in the linear response approximation, one based on a matrix inverse, the other a local propagation
algorithm. The main idea is to perform a Taylor expansion of the beliefs and messages in the
parameters δλ = λ − λMP and keep track of ﬁrst order terms in the belief propagation equations.
One can show that the ﬁrst order terms carry the information to compute the covariance matrix. We
refer to [15] for more information. In appendix A we provide explicit equations for the case of
Boltzman machines which is what is needed to reproduce the experiments in section 4.

α (xα |λ)
fα (xα ) ∂ pBP
∂λβ

Cαβ = ∂ 2 log Z (λ)
∂λα∂λβ

(9)

=

(a)

(b)

Figure 1: Comparision of various scores on synthetic data

3 Conditional Random Fields

Perhaps the most practical class of undirected graphical models are the conditional random ﬁeld
(CRF) models. Here we jointly model labels t and input variables x. The most signiﬁcant mod-
(cid:104)
(cid:105)
iﬁcation relative to MRFs is that the normalization term now depends on the input variable. The
probability of label given input is given as,
p(t|x, λ) =
λT f (t, x)

1
Z (λ, x)
To approximate the log marginal evidence we obtain an expression very similar to eqn.8 with the
N(cid:88)
following replacement,
(cid:180)
(cid:179)
(cid:180)
(cid:179)
C → 1
(cid:88)
(cid:88)
Cxn
N
n=1
λMP T f (tn , xn ) − log Z (λMP , xn )
λMP T f (xn )
n
n
Cxn = E[f (t, xn )f (t, xn )T ]p(t|xn ) − E[f (t, xn )]p(t|xn )E[f (t, xn )]T
(cid:80)
(13)
p(t|xn )
and where all averages are taken over distributions p(t|xn , λMP ) at the MAP value λMP of the condi-
n log p(tn |xn , λ).
tional log-likelihood

− N log Z (λMP ) →

where

•

•

exp

(10)

(11)

(12)

4 Experiments

In the following experiments we probe the accuracy of the Bethe-Laplace(BP-LR) approximation.
In these experiments we have focussed on comparing the value of the estimated log marginal like-
lihood with “annealed importance sampling” (AIS), which we treat as ground truth[9, 1]. We have
focussed on this performance measure because the marginal likelihood is the relevant quantity for
both Bayesian model averaging as well as model selection.
We perform experiments on synthetic data as well as a real-world dataset. For the synthetic data,
we use Boltzman machine models (binary undirected graphical models with pairwise interactions)
because we believe that the results will be representative of multi-state models and because the
implementation of the linear response approximation is straightforward in this case (see appendix
A).

−2024681012−3.5−3.45−3.4−3.35−3.3−3.25−3.2−3.15−3.1#edges (nested models)score/Ntrue model−5 nodes, 6 edges; N=50BIC−MLMAPBP−LRAISBP−LR−ExactGradLaplace−Exact−2024681012−3.11−3.1−3.09−3.08−3.07−3.06−3.05−3.04#edges (nested models)score/Ntrue model−5 nodes, 6 edges; N=10000BIC−MLMAPBP−LRAISBP−LR−ExactGradLaplace−Exact(a)

(b)

Figure 2: Mean difference in scores with AIS (synthetic data). Error-bars are too small to see.

Scores computed using the proposed method (BP-LR) were compared against MAP scores (or pe-
nalized log-likelihood) where we retain only the ﬁrst three terms in equation (8) and the commonly
used BIC-ML scores where we ignore all O(1) terms (i.e retain only terms 1, 2 and 5). BIC-ML
uses the maximum likelihood value λML instead of λMP . We also evaluate two other scores - BP-
LR-ExactGrad where we use exact gradients to compute the λMP and Laplace-Exact which is same
as BP-LR-ExactGrad but with C computed exactly as well. Note that these last two methods are
practical only for models with small tree-width. Nevertheless they are useful here to illustrate the
effect of the bias from BP.

4.1 Synthetic Data

We generated 50 different random structures on 5 nodes. For each we sample 6 different sets of
4 and biases b ∼ U [−1, 1]
parameters with weights w ∼ U {[−d, −d + ] ∪ [d, d + ]}, d > 0,  = 0.1
and varying the edge strength d in [ 0.1
4 ]. We then generated N = 10000
4 , 2.0
4 , 1.5
4 , 1.0
4 , 0.5
4 , 0.2
samples from each of these (50 × 6) models using exact sampling by exhaustive enumeration.
In the ﬁrst experiment we picked a random dataset/model with d = 0.5
4 (the true structure had 6
edges) and studied the variation of different scores with model complexity. We deﬁne an ordering
on models based on complexity by using nested model sequences. These are such that a model
appearing later in the sequence contains all edges from models appearing earlier. Figure (1) shows
the results for two such random nested sequences around the true model, for the number of datacases
N = 50 and N = 10000 respectively. The error-bars for AIS are over 10 parallel annealing runs
which we see are very small. We repeated the plots for multiple such model sequences and the
results were similar. Figure (2) shows the average absolute difference of each score with the AIS
score over 50 sequences. From these one can see that BP-LR is very accurate at low N . As known
in the literature, BIC-ML tends to over-penalize model complexity. At large N , the performance of
all methods improve but BP-LR does slightly worse than the BIC-ML.
In order to better understand the performance of various scores with N , we took the datasets at
d = 0.5
4 and computed scores at various values of N . At each value, we ﬁnd the absolute difference
in the score assigned to the true structure with the corresponding AIS score. These are then averaged
over the 50 datasets. The results are shown in ﬁgure (3). We note that all BP-LR methods are
about two orders of magnitude more accurate than methods that ignore the O(1) term based on C .
However, as we increase N , BP-LR based on λMP computed using BP signiﬁcantly deteriorates. This
does not happen with both BP-LR methods based on λMP computed using exact gradients (i.e. BP-
LR-ExactGrad and Laplace-Exact). Since the latter two methods perform identically, we conclude
that it is not the approximation of C by linear response that breaks down, but rather that the bias in
λMP is the reason that the estimate of C becomes unreliable. We conjecture that this happens when
the bias becomes of the order of the standard deviation of the posterior distribution. Since the bias is

−202468101200.050.10.150.20.250.30.35#edges (nested models)Abs. score diff. with AIStrue model:5nodes, 6 edges; N=50BIC−MLMAPBP−LRBP−LR−ExactGradLaplace−Exact−20246810120123456789x 10−3#edges (nested models)Abs. score diff. with AIStrue model:5nodes, 6 edges; N=10000BIC−MLMAPBP−LRBP−LR−ExactGradLaplace−ExactFigure 3: Variation of score accuracy with N

Figure 4: Variation of score accuracy with d

constant but the variance in the posterior decreases as O(1/N ) this phenomenon is bound to happen
for some value of N .
Finally since our BP-LR method relies on the BP approximation which is known to break down at
strong interactions, we investigated the performance of various scores with d. Again at each value
of d we compute the average absolute difference in the scores assigned to the true structure by a
method and AIS. We use N = 10000 to keep the effect of N minimal. Results are shown in ﬁgure
(4). As expected all BP based methods deteriorate with increasing d. The exact methods show that
one can improve performance by having a more accurate estimate of λMP .

4.2 Real-world Data

To see the performance of the BP-LR on real world data, we implemented a linear chain CRF on the
“newsgroup FAQ dataset”2 [4]. This dataset contains 48 ﬁles where each line can be either a header,
a question or an answer. The problem is binarized by only retaining the question/answer lines. For
each line we use 24 binary features ga (x) = 0/1, a = 1, .., 24 as provided by [4]. These are used to
i (ti , xi ) = ti ga (xi ) and f a
i (ti , ti+1 , xi ) = ti ti+1 ga (xi )
deﬁne state and transition features using: f a
where i denotes the line in a document and a indexes the 24 features.
We generated a random sequence of models by incrementally adding some state features and then
some transition features. We then score each model using MAP, BIC-MAP (which is same as BIC-
ML but with λMP ), AIS and Laplace-Exact. Note that since the graph is a chain, BP-LR is equivalent
to BP-LR-ExactGrad and Laplace-Exact. We use N = 2 ﬁles each truncated to 100 lines. The results
are shown in ﬁgure (5). Here again, the Laplace-Exact agrees very closely with AIS compared to the
other two methods. (Another less relevant observation is that the scores ﬂatten out around the point
where we stop adding the state features showing their importance compared to transition features).

5 Discussion

The main conclusion from this study is that the Bethe-Laplace approximation can give an excellent
approximation to the marginal likelihood for small datasets. We discovered an interesting phe-
nomenon, namely that as N grows the error in the O(1) term based on the covariance between
features increases. We found that this term can give an enormous boost in accuracy for small N (up
to two orders of magnitude), but its effect can be detrimental for large N . We conjecture that this
switch-over point takes place when the bias in λMP becomes of the order of the standard deviation in
the posterior (which decreases as 1/N ). At that point the second derivative of the log-likelihood in
the Taylor expansion becomes unreliable.
There are a number of ways to improve the accuracy of approximation. One approach is to use higher
order Kikuchi approximations to replace the Bethe approximation. Linear response results are also
2Downloaded from: http://www.cs.umass.edu/∼mccallum/data/faqdata/

−200002000400060008000100001200010−410−310−210−1100N (# samples)Mean absolute score diff. with AISd=0.5/4BIC−MLMAPBP−LRBP−LR−ExactGradLaplace−Exact00.511.522.510−410−310−210−1100d*4 (edge strength)Mean Absolute score diff. with AISN=10kBIC−MLMAPBP−LRBP−LR−ExactGradLaplace−ExactFigure 5: Comparision of various scores on real-world dataset

available for this case [12]. A second improvement could come from improving the estimate of
λMP using alternative learning techniques such as contrastive divergence or alternative sample-based
approaches. As discussed above, less bias in λMP will make the covariance term useful for larger N .
Finally, the case of hidden variables needs to be addressed. It is not hard to imagine how to extend the
techniques proposed in this paper to hidden variables in theory, but we haven’t run the experiments
necessary to make claims about its performance. This, we leave for future study.

A Computation of C for Boltzman Machines

wij = log

θi = log

(14)

ξij (ξij + 1 − qi − qj )
(qi − ξij )(qj − ξij )

For binary variables and pairwise interactions we deﬁne the variables as λ = {θi , wij } where θi is a
parameter multiplying the node-feature xi and wij the parameter multiplying the edge feature xixj .
Moreover, we’ll deﬁne the following independent quantities qi = p(xi = 1) and ξij = p(xi =
1, xj = 1). Note that all other quantities, e.g. p(xi = 1, xj = 0) are functions of {qi , ξij }.
In the following we will assume that {qi , ξij } are computed using belief propagation (BP). At the
(cid:33)
(cid:195)
(1 − qi )zi−1 (cid:81)
(cid:181)
(cid:182)
ﬁxed points of BP the following relations hold [14],
(cid:81)
j∈N (i) (qi − ξij )
j∈N (i) (ξij + 1 − qi − qj )
qzi−1
i
where N (i) are neighboring nodes of node i in the graph and zi = |N (i)| is the number of neighbors
(cid:34)
(cid:35)
of node i.
To compute the covariance matrix we ﬁrst compute its inverse from eqns.14 as follows, C −1 =
∂θ
∂θ
and subsequently take its inverse. The four terms in this matrix are given by,
∂q
∂ ξ
(cid:182) δik
 1 − zi
∂w
∂w
(cid:181)
(cid:88)
∂q
∂ ξ
(cid:183) −1
qi (1 − qi )
j∈N (i)
(cid:183)
1
=
ξij + 1 − qi − qj
qi − ξik
(cid:183)
1
1
ξij + 1 − qi − qj
qi − ξij
1
+
ξij

1
(cid:183) −1
ξij + 1 − qi − qj
(cid:183)
qi − ξij
1
qj − ξij
1
qj − ξij

∂ θi
∂ ξjk
∂Wij
∂ qk
∂Wij
∂ ξkl

+

−

1
ξik + 1 − qi − qk
1
ξij + 1 − qi − qj
1
+
ξij + 1 − qi − qj

(cid:184)
(cid:184)
δik

1
qi − ξij

1
qi − ξij

+

(cid:184)
+
(cid:184)

∂ θi
∂ qk

=

(15)

(16)

(17)

(18)

δij +

δik +

+
(cid:184)
−

δik δj l

=

=

+

δjk

01020304050−60−55−50−45−40−35−30−25−20−15# featuresscore/NCRF N=2, Sequence Length=100MAPBIC_MAPLaplace−ExactAISAcknowledgments

This material is based upon work supported by the National Science Foundation under Grant No.
0447903.

References
[1] M.J. Beal and Z. Ghahramani. The variational bayesian EM algorithm for incomplete data:
with application to scoring graphical model structures. In Bayesian Statistics, pages 453–464.
Oxford University Press, 2003.
[2] P. Green and S. Richardson. Hidden markov models and disease mapping. Journal of the
American Statistical Association, 97(460):1055–1070, 2002.
[3] D. Heckerman. A tutorial on learning with bayesian networks. pages 301–354, 1999.
[4] A. McCallum and D. Freitag F. Pereira. Maximum entropy Markov models for information
extraction and segmentation. In Int’l Conf. on Machine Learning, pages p.591–598, San Fran-
cisco, 2000.
[5] T.S. Jaakkola M.J. Wainwright and A.S. Willsky. Tree-reweighted belief propagation algo-
rithms and approximate ml estimation via pseudo-moment matching. In AISTATS, 2003.
[6] J. Møller, A. Pettitt, K. Berthelsen, and R. Reeves. An efﬁcient Markov chain Monte Carlo
method for distributions with intractable normalisation constants. Biometrica, 93, 2006.
to
appear.
[7] I. Murray and Z. Ghahramani. Bayesian learning in undirected graphical models: approximate
MCMC algorithms. In Proceedings of the 14th Annual Conference on Uncertainty in Artiﬁcial
Intelligence (UAI-04), San Francisco, CA, 2004.
[8] I. Murray, Z. Ghahramani, and D.J.C. MacKay. Mcmc for doubly-intractable distributions. In
Proceedings of the 14th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-06),
Pittsburgh, PA, 2006.
[9] R.M. Neal. Annealed importance sampling.
2001.
[10] S. Parise and M. Welling. Learning in markov random ﬁelds: An empirical study. In Proc. of
the Joint Statistical Meeting – JSM2005, 2005.
[11] Y. Qi, M. Szummer, and T.P. Minka. Bayesian conditional random ﬁelds. In Artiﬁcial Intelli-
gence and Statistics, 2005.
[12] K. Tanaka. Probabilistic inference by means of cluster variation method and linear response
theory. IEICE Transactions in Information and Systems, E86-D(7):1228–1242, 2003.
[13] M. Welling and S. Parise. Bayesian random ﬁelds: The Bethe-Laplace approximation. In UAI,
2006.
[14] M. Welling and Y.W. Teh. Approximate inference in boltzmann machines. Artiﬁcial Intelli-
gence, 143:19–50, 2003.
[15] M. Welling and Y.W. Teh. Linear response algorithms for approximate inference in graphical
models. Neural Computation, 16 (1):197–221, 2004.
[16] J.S. Yedidia, W. Freeman, and Y. Weiss. Constructing free energy approximations and gen-
eralized belief propagation algorithms. Technical report, MERL, 2002. Technical Report
TR-2002-35.

In Statistics and Computing, pages 125–139,


Active learning for misspeci ﬁed
generalized linear models

Francis R. Bach
Centre de Morphologie Math ´ematique
Ecole des Mines de Paris
Fontainebleau, France
francis.bach@mines.org

Abstract

Active learning refers to algorithmic frameworks aimed at selecting training data
points in order to reduce the number of required training data points and/or im-
prove the generalization performance of a learning method.
In this paper, we
present an asymptotic analysis of active learning for generalized linear models.
Our analysis holds under the common practical situation of model misspeci ﬁca-
tion, and is based on realistic assumptions regarding the nature of the sampling
distributions, which are usually neither independent nor identical. We derive un-
biased estimators of generalization performance, as well as estimators of expected
reduction in generalization error after adding a new training data point, that allow
us to optimize its sampling distribution through a convex optimization problem.
Our analysis naturally leads to an algorithm for sequential active learning which is
applicable for all tasks supported by generalized linear models (e.g., binary clas-
si ﬁcation, multi-class classi ﬁcation, regression) and ca
n be applied in non-linear
settings through the use of Mercer kernels.

1

Introduction

The goal of active learning is to select training data points so that the number of required training
data points for a given performance is smaller than the number which is required when randomly
sampling those points. Active learning has emerged as a dynamic ﬁeld of research in machine learn-
ing and statistics [1], from early works in optimal experimental design [2, 3], to recent theoretical
results [4] and applications, in text retrieval [5], image retrieval [6] or bioinformatics [7].

Despite the numerous successful applications of active learning to reduce the number of required
training data points, many authors have also reported cases where widely applied active learning
heuristic schemes such as maximum uncertainty sampling perform worse than random selection [8,
9], casting doubt into the practical applicability of active learning: why would a practitioner use an
active learning strategy that is not ensuring, unless the data satisfy possibly unrealistic and usually
non veri ﬁable assumptions, that it performs better than ran dom? The objectives of this paper are
(1) to provide a theoretical analysis of active learning with realistic assumptions and (2) to derive a
principled algorithm for active learning with guaranteed consistency.

In this paper, we consider generalized linear models [10], which provide ﬂexible and widely used
tools for many supervised learning tasks (Section 2). Our analysis is based on asymptotic arguments,
and follows previous asymptotic analysis of active learning [11, 12, 9, 13]; however, as shown in
Section 4, we do not rely on correct model speci ﬁcation and as sume that the data are not identically
distributed and may not be independent. As shown in Section 5, our theoretical results naturally
lead to convex optimization problems for selecting training data point in a sequential design. In
Section 6, we present simulations on synthetic data, illustrating our algorithms and comparing them
favorably to usual active learning schemes.

2 Generalized linear models

Given data x ∈ Rd , and targets y in a set Y , we consider the problem of modeling the conditional
probability p(y |x) through a generalized linear model (GLIM) [10]. We assume that we are given
an exponential family adapted to our prediction task, of the form p(y |η) = exp(η>T (y) − ψ(η)),
where T (y) is a k-dimensional vector of sufﬁcient statistics, η ∈ Rk is vector of natural parameters
and ψ(η) is the convex log-partition function. We then consider the generalized linear model deﬁned
as p(y |x, θ) = exp(tr(θ>xT (y)> ) − ψ(θ>x)), where θ ∈ Θ ⊂ Rd×k . The framework of GLIMs is
general enough to accomodate many supervised learning tasks [10], in particular:

• Binary classi ﬁcation:
the Bernoulli distribution leads to logistic regression, with Y =
{0, 1}, T (y) = y and ψ(η) = log(1 + eη ).
• k-class classi ﬁcation: the multinomial distribution lead s to softmax regression, with Y =
{y ∈ {0, 1}k , Pk
i=1 yi = 1}, T (y) = y and ψ(η) = log(Pk
i=1 eηi ).
the normal distribution leads to Y = R, T (y) = (y , − 1
2 y2 )> ∈ R2 , and
• Regression:
2 log 2π + η2
ψ(η1 , η2 ) = − 1
2 log η2 + 1
. When both η1 and η2 depends linearly on x, we
1
2η2
have an heteroscedastic model, while if η2 is constant for all x, we obtain homoscedastic
regression (constant noise variance).

Maximum likelihood estimation We assume that we are given independent and identically dis-
tributed (i.i.d.) data sampled from the distribution p0 (x, y) = p0 (x)p0 (y |x). The maximum likeli-
hood population estimator θ0 is deﬁned as the minimizer of the expectation under p0 of the negative
log-likelihood `(y , x, θ) = −tr(θ>xT (y)> ) + ψ(θ>x). The function `(y , x, θ) is convex in θ and
by taking derivatives and using the classical relationship between the derivative of the log-partition
and the expected sufﬁcient statistics [10], the population maximum likelihood estimate is deﬁned
by:
Ep0 (x,y)∇`(y , x, θ0 ) = Ep0 (x) (cid:8)x(Ep(y |x,θ0 )T (y) − Ep0 (y |x)T (y))>(cid:9) = 0
(1)
Given i.i.d data (xi , yi ), i = 1, . . . , n, we use the penalized maximum likelihood estimator, which
minimizes Pn
i=1 `(yi , xi , θ) + 1
2 λtrθ> θ . The minimization is performed by Newton’s method [14].
Model speci ﬁcation
A GLIM is said well-speci ﬁed is there exists a θ ∈ Rd×k such that for
all x ∈ Rd , Ep(y |x,θ)T (y) = Ep0 (y |x)T (y). A sufﬁcient condition for correct speci ﬁcation is that
there exist θ ∈ Rd×k such that for all x ∈ Rd , y ∈ Y , p(y |x, θ) = p0 (y |x). This condition is
necessary for the Bernoulli and multinomial exponential family, but not for example for the normal
distribution. In practice, the model is often misspeci ﬁed a nd it is thus of importance to consider
potential misspeci ﬁcation while deriving asymptotic expa nsions.
Kernels
The theoretical results of this paper mainly focus on generalized linear models; however,
they can be readily generalized to non-linear settings by using Mercer kernels [15], for example
leading to kernel logistic regression or kernel ridge regression. When the data are given by a kernel
matrix, we can use the incomplete Cholesky decomposition [16] to ﬁnd an approximate basis of the
feature space on which the usual linear methods can be applied. Note that our asymptotic results do
not hold when the number of parameters may grow with the data (which is the case for kernels such
as the Gaussian kernel). However, our dimensionality reduction procedure uses a non-parametric
method on the entire (usually large) training dataset and we then consider a ﬁnite dimensional prob-
lem on a much smaller sample. If the whole training dataset is large enough, then the dimension
reduction procedure may be considered deterministic and our criteria may apply.

3 Active learning set-up

We consider the following “pool-based” active learning sce nario: we have a large set of i.i.d. data
points xi ∈ Rd , i = 1, . . . , m sampled from p0 (x). The goal of active learning is to select the points
to label, i.e., the points for which the corresponding yi will be observed. We assume that given
xi , i = 1, . . . , n, the targets yi , i = 1, . . . , n are independent and sampled from the corresponding
conditional distribution p0 (yi |xi ). This active learning set-up is well studied and appears naturally
in many applications where the input distribution p0 (x) is only known through i.i.d. samples [5, 17].
For alternative scenarii, where the density p0 (x) is known, see e.g. [18, 19, 20].

More precisely, we assume that the points xi are selected sequentially, and we let denote
qi (xi |x1 , . . . , xi−1 ) the sampling distribution of xi given the previously observed points.
In
situations where the data are not sampled from the testing distribution, it has proved advanta-
geous to consider likelihood weighting techniques [13, 19], and we thus consider weights wi =
wi (xi |x1 , . . . , xi−1 ). We let ˆθn denote the weighted penalized ML estimator, deﬁned as the mi ni-
mum with respect to θ of
Pn
(2)
2 trθ> θ .
i=1 wi `(yi , xi , θ) + λ
In this paper, we work with two different assumptions regarding the sequential sampling dis-
tributions:
(1) the variables xi are independent,
i.e., qi (xi |x1 , . . . , xi−1 ) = qi (xi ), (2) the
variable xi depends on x1 , . . . , xi−1 only through the current empirical ML estimator ˆθi , i.e.,
qi (xi |x1 , . . . , xi−1 ) = q(xi | ˆθi ), where q(xi |θ) is a pre-speci ﬁed sampling distribution. The ﬁrst
assumption is not realistic, but readily leads to asymptotic expansions. The second assumption is
more realistic, as most of the heuristic schemes for sequential active learning satisfy this assumption.
It turns out that under certain assumption, the asymptotic expansions of the expected generalization
performance for both sets of assumptions are identical.

4 Asymptotic expansions

In this section, we derive the asymptotic expansions that will lead to active learning algorithms in
Section 5. Throughout this section, we assume that p0 (x) has a compact support K and has a twice
differentiable density with respect to the Lebesgue measure, and that all sampling distributions have
a compact support included in the one of p0 (x) and have twice differentiable densities.
We ﬁrst make the assumption that the variables xi are independent, i.e., we have sampling distri-
butions qi (xi ) and weights wi (xi ), both measurable, and such that wi (xi ) > 0 for all xi ∈ K . In
Section 4.4, we extend some of our results to the dependent case.

4.1 Bias and variance of ML estimator

The following proposition is a simple extension to non identically distributed observations, of clas-
sical results on maximum likelihood for misspeci ﬁed genera lized linear models [21, 13]. We let ED
and varD denote the expectation and variance with respect to the data D = {(xi , yi ), i = 1, . . . , n}.
Proposition 1 We let θn denote the minimizer of Pn
i=1 Eqi (xi )p0 (yi |xi )wi (xi )`(yi , xi , θ). If (a) the
weight functions wn and the sampling densities qn are pointwise strictly positive and such that
n (x) is bounded , then ˆθn − θn converges
wn (x)qn (x) converges in the L∞ -norm, and (b) Eqn (x)w2
to zero in probability and we have
ED ˆθn = θn + O(n−1 ) and varD ˆθn = 1
n + O(n−2 )
n InJ −1
n J −1
n Pn
n Pn
i=1 Eqi (x)wi (x)∇2 `(x, θn ) can be consistently estimated by ˆJn = 1
where Jn = 1
i=1 wihi
n Pn
and In = 1
i=1 Eqi (x)p0 (y |x)wi (x)2∇`(y , x, θn )∇`(y , x, θn )> can be consistently estimated by
n Pn
i , where gi = ∇`(yi , xi , ˆθn ) and hi = ∇2 `(xi , ˆθn ).
ˆIn = 1
i gi g>
i=1 w2
From Proposition 1, it is worth noting that in general θn will not converge to the population maxi-
mum likelihood estimate θ0 , i.e., using a different sampling distribution than p0 (x) may introduce
a non asymptotically vanishing bias in estimating θ0 . Thus, active learning requires to ensure that
(a) our estimators have a low bias and variance in estimating θn , and (b) that θn does actually con-
verge to θ0 . This double objective is taken care of by our estimates of generalization performance in
Propositions 2 and 3.
There are two situations, however, where θn is equal to θ0 . First, if the model is well speci ﬁed,
then whatever the sampling distributions are, θn is the population ML estimate (which is a simple
consequence of the fact that Ep(y |x,θ0 )T (y) = Ep0 (y |x)T (y), for all x, implies that, for all q(x),
Eq(x)p0 (y |x)∇`(y , x, θ) = Eq(x) (cid:8)x(Ep(y |x,θ0 )T (y) − Ep0 (y |x)T (y))>(cid:9) = 0).
Second, When wn (x) = p0 (x)/qn (x), then θn is also equal to θ0 , and we refer to this weighting
scheme as the unbiased reweighting scheme, which was used by [19] in the context of active learn-
ing. We refer to the weights wu
n = p0 (xn )/qn (xn ) as the importance weights. Note however, that

(3)

restricting ourselves to such unbiased estimators, as done in [19] might not be optimal because they
may lead to higher variance [13], in particular due to the potential high variance of the importance
weights (see simulations in Section 6).

4.2 Expected generalization performance

We let Lu (θ) = Ep0 (x)p0 (y |x) `(y , x, θ) denote the generalization performance1 of the parameter θ .
We now provide an unbiased estimator of the expected generalization error of ˆθn , which generalized
the Akaike information criterion [22] (for a proof, see [23]):

that

assume

Proposition 2 In
of Proposition
assumptions
the
to
addition
1, we
Eqn (x) (p0 (x)/qn (x))2 is bounded. Let
i ( ˆJn )−1 gi(cid:17) ,
n (cid:16) 1
n Pn
n Pn
i `(yi , xi , ˆθn ) + 1
bG = 1
(4)
i wi g>
i=1 wu
i=1 wu
i = p0 (xi )/qi (xi ). bG is an asymptotically unbiased estimator of ED Lu ( ˆθn ), i.e., ED bG =
where wu
ED Lu ( ˆθn ) + O(n−2 ).
The criterion bG is a sum of two terms: the second term corresponds to a variance term and will
converge to zero in probability at rate O(n−1 ); the ﬁrst term, however, which corresponds to a
selection bias induced by a speci ﬁc choice of sampling distr ibutions, will not always converge to
the minimum possible value Lu (θ0 ). Thus, in order to ensure that our active learning method are
consistent, we have to ensure that this ﬁrst term is going to i
ts minimum value. One simple way to
achieve this is to always optimize our weights so that the estimate bG is smaller than the estimate for
the unbiased reweighting scheme (see Section 5).
4.3 Expected performance gain

We now look at the following situation: we are given the ﬁrst n data points (xi , yi ) and the cur-
rent estimate ˆθn , the gradients gi = ∇`(yi , xi , ˆθn ), the Hessians hi = ∇2 `(xi , ˆθn ) and the third
derivatives Ti = ∇3 `(xi , ˆθn ), we consider the following criterion, which depends on the sampling
distributions and weights of the (n + 1)-th point:
p0 (xi ) + Pn
n3 Pn
i wn+1 (xi )2 qn+1 (xi )
i wn+1 (xi ) qn+1 (xi )
bH (qn+1 , wn+1 |α, β ) = 1
i=1 βiwu
i=1 αiwu
p0 (xi )
ˆJnA − wiwu
ˆJn ˜gi − 2˜g>
where αi = −(n + 1)n˜g>
i ˜g>
i ˜g>
i hi ˜gi + wu
i B
i
i
ˆJ u
−wi ˜g>
n ˜gi + Ti [˜gi , C ] − 2wi ˜g>
i hiA + Ti [A, ˜gi , ˜gi ]
i
1
ˆJ u
n ˜gi + A>hi ˜gi
˜g>
i
2
i wihi ˜gi , C = Pn
i gi , B = Pn
n Pn
i , ˆJ u
1
i ˜gi ˜g>
i=1 wiwu
i=1 wu
i=1 wu
n =

n gi , A = ˆJ −1
with ˜gi = ˆJ −1
n
n Pn
1
i hi .
i=1 wu
The following proposition shows that bH (qn+1 , wn+1 |α, β ) is an estimate of the expected perfor-
mance gain of choosing a point xn+1 according to distribution qn+1 and weight wn+1 (and marginal-
izing over yn+1 ) and may be used as an objective function for learning the distributions qn+1 , wn+1
(for a proof, see [23]). In Section 5, we show that if the distributions and weights are properly
parameterized, this leads to a convex optimization problem.

βi =

(5)

(6)

(7)

n (x) and Eqn (x) (p0 (x)/qn (x))2 are bounded. We let de-
Proposition 3 We assume that Eqn (x)w2
note ˆθn denote the weighted ML estimator obtained from the ﬁrst n points, and ˆθn+1 the one-step
estimator obtained from the ﬁrst n+ 1 points, i.e., ˆθn+1 is obtained by one Newton step from ˆθn [24];
then the criterion deﬁned in Eq. (5) is such that ED bH (qn+1 , wn+1 ) = ED Lu ( ˆθn ) − ED Lu ( ˆθn+1 ) +
O(n−3 ), where ED denotes the expectation with respect to the ﬁrst n+1 data points and their labels.
Moreover, for n large enough, all values of βi are positive.
1 In this paper, we use the negative log-likelihood as a measure of performance, which allows simple asymp-
totic expansions, and the focus of the paper is about the differences between testing and training sampling
distributions. The study of potentially different costs for testing and training is beyond the scope of this paper.

Note that many of the terms in Eq. (6) and Eq. (7) are dedicated to weighting schemes for the ﬁrst
n points other than the unbiased reweighting scheme. For the unbiased reweighting scheme where
i , for i = 1, . . . , n, then A = 0 and the equations may be simpli ﬁed.
wi = wu

4.4 Dependent observations

In this section, we show that under a certain form of weak dependence between the data points
xi , i = 1, . . . , n, then the results presented in Propositions 1 and 2 still hold. For simplicity and
brevity, we restrict ourselves to the unbiased reweighting scheme, i.e., wn (xn |x1 , . . . , xn−1 ) =
p0 (xn )/qn (xn |x1 , . . . , xn−1 ) for all n, and we assume that those weights are uniformly bounded
away from zero and inﬁnity. In addition, we only prove our res ult in the well-speci ﬁed case, which
leads to a simpler argument for the consistency of the estimator.

Many sequential active learning schemes select a training data point with a distribution or criterion
that depends on the estimate so far (see Section 6 for details). We thus assume that the sampling
distribution qn is of the form q(xn | ˆθn ), where q(x|θ) is a ﬁxed set of smooth parameterized densities.

Proposition 4 (for a proof, see [23]) Let
i ( ˆJn )−1 gi(cid:17) ,
n (cid:16) 1
n Pn
n Pn
i=1 wi `(yi , xi , ˆθn ) + 1
bG = 1
(8)
i=1 w2
i g>
i = p0 (xi )/q(xi | ˆθi ). bG is an asymptotically unbiased estimator of ED Lu ( ˆθn ), i.e.,
where wi = wu
ED bG = ED Lu ( ˆθn ) + O(log(n)n−2 ).
The estimator is the same as in Proposition 2. The effect of the dependence is asymptotically negli-
gible and only impacts the result with the presence of an additional log(n) term. In the algorithms
presented in Section 5, the distribution qn is obtained as the solution of a convex optimization prob-
lem, and thus the previous theorem does not readily apply. However, when n gets large, qn depends
on the previous data points only through the ﬁrst two derivat
ives of the objective function of the
convex problem, which are empirical averages of certain functions of all currently observed data
points; we are currently working out a generalization of Proposition 4 that allows the dependence
on certain empirical moments and potential misspeci ﬁcatio n.

5 Algorithms

In Section 4, we have derived a criterion bH in Eq. (5) that enables to optimize the sampling density
of the (n + 1)-th point, and an estimate bG in Eq. (4) and Eq. (8) of the generalization error. Our
algorithms are composed of the following three ingredients:
n = p0 (xn )/qn (xn ) is
1. Those criteria assume that the variance of the importance weights wu
controlled. In order to make sure that those results apply, our algorithms will ensure that
this condition is met.
2. The sampling density qn+1 will be obtained by minimizing bH (wn+1 , qn+1 |α, β ) for a cer-
tain parameterization of qn+1 and wn+1 . It turns out that those minimization problems are
convex, and can thus be efﬁciently solved, without local minima.
3. Once a new sample has been selected, and its label observed, Proposition 4 is used in a way
similar to [13], in order to search for the best mixture between the current weights (wi ) and
i ), i.e., we look at weights of the form wγ
the importance weights (wu
i )1−γ and perform
i (wu
a grid search on γ to ﬁnd γ such that bG in Eq. (4) is minimum.
The main interest of the ﬁrst and third points is that we obtai n a ﬁnal estimator of θ0 which is at
least provably consistent: indeed, although our criteria are obtained from an assumption of indepen-
dence, the generalization performance result also holds for “weakly” dependent observations and
thus ensures the consistency of our approach. Thus, as opposed to most previous active learning
heuristics, our estimator will always converge (in probability) to the ML estimator. In Section 6, we
show empirically that usual heuristic schemes do not share this property.
Convex optimization problem We assume that we have a ﬁxed set of candidate distributions
sk (x) of the form sk (x) = p0 (x)rk (x). Note that the multiplicative form of our candidate distri-

butions allows efﬁcient sampling from a pool of samples of p0 . We look at distributions qn+1 (x)
with mixture density of the form s(x|η) = Pk ηk sk (x) = p0 (x)r(x), where the weights η are
non-negative and sum to one. The criterion bH (qn+1 , wn+1 |α, β ) in Eq. (5) is thus a function
H (η |α, β ) of η . We consider two weighting schemes: (a) one with all weights equal to one (unit
weighting scheme) which leads to H0 (η |α, β ), and (b) the unbiased reweighting scheme, where
wn+1 (x) = p0 (x)/qn+1 (x), which leads to H1 (η |α, β ). We have
n3 Pk ηk (Pn
H0 (η |α, β ) = 1
i=1 (αi + βi )wu
i sk (xi )) ,
i + Pn
n3 Pn
βiwu
H1 (η |α, β ) = 1
i=1 αiwu
P k ηk sk (xi ) .
i
i=1
The function H0 (η) is linear in η , while the function H1 (η) is the sum of a constant and positive
inverse functions, and is thus convex [14].
Unless natural candidate distributions sk (x) can be deﬁned for the active learning problem, we use
the set of distributions obtained as follows: we perform K-means clustering with a large number p of
e−αk kx−µk k2 ,
clusters (e.g., 100 or 200), and then consider functions rk (x) of the form rk (x) = 1
Zk
where αk is one element of a ﬁnite given set of parameters, and µk is one of the p centroids
y1 , . . . , yp , obtained from K-means. We let ˜wi denote the number of data points assigned to the
/ Pp
centroid yi . We normalize by Zk = Pp
i=1 ˜wi e−αk kyi−µk k2
i=1 ˜wi . We thus obtained O(p) can-
didate distributions rk (x), which, if p is large enough, provides a ﬂexible yet tractable set of mixt ure
distributions.

(10)

(9)

One additional element is the constraint on the variance of the importance weights. The variance of
n+1 = Pm
r(xi ) − 1 = Pm
˜wi
˜wi
n+1 can be estimated as var wu
P k ηk rk (xi ) − 1 = V (η), which
wu
i=1
i=1
is convex in η . Thus constraining the variance of the new weights leads to a convex optimization
problem, with convex objective and convex constraints, which can be solved efﬁciently by the log-
barrier method [14], with cubic complexity in the number of candidate distributions.
Algorithms We have three versions of our algorithm, one with unit weights (referred to as “no
weight ”) which optimizes H0 (η |α, β ) at each iteration, one with the unbiased reweighting scheme,
which optimizes H1 (η |α, β ) (referred to as ”unbiased”) and one which does both and choos
es the
best one, as measured by bH (referred to as ”full ”): in the initialization phase, K-mea ns is run to
generate candidate distributions that will be used throughout the sampling of new points. Then, in
order to select the new training data point xn+1 , the scores α and β are computed from Eq. (6) and
Eq. (7), then the appropriate cost function, H0 (η |α, β ), H1 (η |α, β ) (or both) is minimized and once
η is obtained, we sample xn+1 from the corresponding distribution, and compute the weights wn+1
n+1 . As described earlier, we then ﬁnd γ such that bG((wγ
i )1−γ )i ) in Eq. (4) is minimized
and wu
i (wu
and update weights accordingly.
Regularization parameter
In the active learning set-up, the number of samples used for learning
varies a lot. It is thus not possible to use a constant regularization parameter. We thus learn it by
cross-validation every 10 new samples.

6 Simulation experiments

In this section, we present simulation experiments on synthetic examples (sampled from Gaussian
mixtures in two dimensions), for the task of binary and 3-class classi ﬁcation. We compare our al-
gorithms to the following three active learning frameworks. In the maximum uncertainty framework
(referred to as “maxunc”), the next training data point is se lected such that the entropy of p(y |x, ˆθn )
is maximal [17]. In the maximum variance reduction framework [25, 9] (referred to as “varred”), the
next point is selected so that the variance of the resulting estimator has the lowest determinant, which
is equivalent to ﬁnding x such that tr∇(x, ˆθn ) ˆJ −1
n is minimum. Note that this criterion has theo-
retical justi ﬁcation under correct model speci ﬁcation. In
the minimum prediction error framework
(referred to as “minpred”), the next point is selected so tha t it reduces the most the expected log-loss,
with the current model as an estimate of the unknown conditional probability p0 (y |x) [5, 8].
Sampling densities
In Figure 1, we look at the limit selected sampling densities, i.e., we assume
that a large number of points has been sampled, and we look at the criterion ˆH in Eq. (5). We
show the density obtained from the unbiased reweighting scheme (middle of Figure 1), as well as

5

0

−5

5

0

−5

0.8

0.6

0.4

0.2

5

0

−5

0.5

0

−0.5

−10
−5
0
5
−10
−5
0
5
−10
−5
0
5
Figure 1: Proposal distributions: (Left) density p0 (x) with the two different classes (red and blue),
(Middle) best density with unbiased reweighting, (Right) function γ (x) such that bH (qn+1 (x), 1) =
R γ (x)qn+1 (x)dx (see text for details).

0.2

e
t
a
r
 
r
o
r
r
e

0.15

0.1

random
full

0.2

0.15

0.1

e
t
a
r
 
r
o
r
r
e

random
no weight
unbiased

0.2

0.15

0.1

e
t
a
r
 
r
o
r
r
e

random
minpred
varred
maxunc

0

100

0

0

100

50
50
50
number of samples
number of samples
number of samples
Figure 2: Error rates vs. number of samples averaged over 10 replications sampled from same dis-
tribution as in Figure 1: (Left) random sampling and active learning ”full ”, with standard deviations,
(Middle) Comparison of the two schemes “unbiased” and ”no we
ight ”, (Right) Comparison with
other methods.

100

the function γ (x) (right of Figure 1) such that, for the unit weighting scheme, bH (qn+1 (x), 1) =
R γ (x)qn+1 (x)dx. In this framework, minimizing the cost without any constraint leads to a Dirac
at the maximum of γ (x), while minimizing with a constraint on the variance of the corresponding
importance weights will select point with high values of γ (x). We also show the line θ>
0 x = 0.
From Figure 1, we see that (a) the unit weighting scheme tends to be more selective (i.e., ﬁner grain)
than the unbiased scheme, and (b) that the mode of the optimal densities are close to the maximum
uncertainty hyperplane but some parts of this hyperplane are in fact leading to negative cost gains
(e.g., the part of the hyperplane crossing the central blob), hinting at the bad potential behavior of
the maximum uncertainty framework.
Comparison with other algorithms
In Figure 2 and Figure 1, we compare the performance of
our active learning algorithms. In the left of Figure 2, we see that our active learning framework does
perform better on average but also leads to smaller variance. In the middle of Figure 2, we compare
the two schemes “no weight ” and “unbiased”, showing the supe
riority of the unit weighting scheme
and the signi ﬁcance of our asymptotic results in Propositio n 2 and 3 which extend the unbiased
framework of [13].
In the right of Figure 2 and in Figure 3, we compare with the other usual
heuristic schemes: our “full ” algorithm outperforms other
schemes; moreover, in those experiments,
the other schemes do perform worse than random sampling and converge to the wrong estimator, a
bad situation that our algorithms provably avoid.

7 Conclusion

We have presented a theoretical asymptotic analysis of active learning for generalized linear models,
under realistic sampling assumptions. From this analysis, we obtain convex criteria which can be
optimized to provide algorithms for online optimization of the sampling distributions. This work
naturally leads to several extensions. First, our framework is not limited to generalized linear mod-
els, but can be readily extended to any convex differentiable M -estimators [24]. Second, it seems
advantageous to combine our active learning analysis with semi-supervised learning frameworks, in
particular ones based on data-dependent regularization [26]. Finally, we are currently investigating
applications to large scale image retrieval tasks, where unlabelled data are abundant but labelled data
are scarce.

0.5

0.4

0.3

0.2

e
t
a
r
 
r
o
r
r
e

random
full
minpred
varred
maxunc

50
number of samples
Figure 3: Error rates vs. number of samples averaged over 10 replications for 3 classes: (left) data,
(right) comparisons of methods.

100

0.1
0

References
[1] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. J. Art. Intel. Res.,
4:129–145, 1996.
[2] V. V. Fedorov. Theory of optimal experiments. Academic Press, 1972.
[3] P. Chaudhuri and P. A. Mykland. On efﬁcient designing of nonline ar experiments. Stat. Sin., 5:421–440,
1995.
[4] S. Dasgupta. Coarse sample complexity bounds for active learning. In Adv. NIPS 18, 2006.
[5] N. Roy and A. McCallum. Toward optimal active learning through sampling estimation of error reduction.
In Proc. ICML, 2001.
[6] S. Tong and E. Chang. Support vector machine active learning for image retrieval. In Proc. ACM Multi-
media, 2001.
[7] M. Warmuth, G. R ¨atsch, M. Mathieson, J. Liao, and C. Lemmen. Active learning in the drug discovery
process. In Adv. NIPS 14, 2002.
[8] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using
Gaussian ﬁelds and harmonic functions. In Proc. ICML, 2003.
[9] A I. Schein. Active Learning for Logistic Regression. Ph.D. diss., U. Penn., 2005. CIS Dpt.
[10] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman and Hall, 1989.
[11] T. Zhang and F. J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems.
In Proc. ICML, 2000.
[12] O. Chapelle. Active learning for parzen window classiﬁer. In Proc. AISTATS, 2005.
[13] H. Shimodaira.
Improving predictive inference under covariate shift by weighting the log-likelihood
function. J. Stat. Plan. Inf., 90:227–244, 2000.
[14] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2003.
[15] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press, 2004.
[16] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank k ernel representations. J. Mach. Learn.
Res., 2:243–264, 2001.
[17] S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. In
Proc. ICML, 2000.
[18] K. Fukumizu. Active learning in multilayer perceptrons. In Adv. NIPS 8, 1996.
[19] T. Kanamori and H. Shimodaira. Active learning algorithm using the maximum weighted log-likelihood
estimator. J. Stat. Plan. Inf., 116:149–162, 2003.
[20] T. Kanamori. Statistical asymptotic theory of active learning. Ann. Inst. Stat. Math., 54(3):459–475, 2002.
[21] H. White. Maximum likelihood estimation of misspeciﬁed models. Econometrica, 50(1):1–26, 1982.
[22] H. Akaike. A new look at statistical model identiﬁcation.
IEEE Trans. Aut. Cont., 19:716–722, 1974.
[23] F. R. Bach. Active learning for misspeciﬁed generalized linear mo dels. Technical Report N15/06/MM,
Ecole des Mines de Paris, 2006.
[24] A. W. Van der Vaart. Asymptotic Statistics. Cambridge Univ. Press, 1998.
[25] D. MacKay.
Information-based objective functions for active data selection. Neural Computation,
4(4):590–604, 1992.
[26] Y. Bengio and Y Grandvalet. Semi-supervised learning by entropy minimization. In Adv. NIPS 17, 2005.


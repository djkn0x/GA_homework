Particle Filtering for Nonparametric
Bayesian Matrix Factorization

Frank Wood
Department of Computer Science
Brown University
Providence, RI 02912
fwood@cs.brown.edu

Thomas L. Grif ﬁths
Department of Psychology
University of California, Berkeley
Berkeley, CA 94720
tom griffiths@berkeley.edu

Abstract

Many unsupervised learning problems can be expressed as a form of matrix fac-
torization, reconstructing an observed data matrix as the product of two matrices
of latent variables. A standard challenge in solving these problems is determining
the dimensionality of the latent matrices. Nonparametric Bayesian matrix factor-
ization is one way of dealing with this challenge, yielding a posterior distribution
over possible factorizations of unbounded dimensionality. A drawback to this ap-
proach is that posterior estimation is typically done using Gibbs sampling, which
can be slow for large problems and when conjugate priors cannot be used. As an
alternative, we present a particle ﬁlter for posterior esti mation in nonparametric
Bayesian matrix factorization models. We illustrate this approach with two matrix
factorization models and show favorable performance relative to Gibbs sampling.

1 Introduction

One of the goals of unsupervised learning is to discover the latent structure expressed in observed
data. The nature of the learning problem will vary depending on the form of the data and the kind of
latent structure it expresses, but many unsupervised learning problems can be viewed as a form of
matrix factorization – i.e. decomposing an observed data ma trix, X, into the product of two or more
matrices of latent variables.
If X is an N × D matrix, where N is the number of D-dimensional
observations, the goal is to ﬁnd a low-dimensional latent fe ature space capturing the variation in the
observations making up X. This can be done by assuming that X ≈ ZY, where Z is a N × K
matrix indicating which of (and perhaps the extent to which) K latent features are expressed in each
of the N observations and Y is a K × D matrix indicating how those K latent features are manifest
in the D dimensional observation space. Typically, K is less than D , meaning that Z and Y provide
an efﬁcient summary of the structure of X.

A standard problem for unsupervised learning algorithms based on matrix factorization is determin-
ing the dimensionality of the latent matrices, K . Nonparametric Bayesian statistics offers a way to
address this problem: instead of specifying K a priori and searching for a “best” factorization, non-
parametric Bayesian matrix factorization approaches such as those in [1] and [2] estimate a posterior
distribution over factorizations with unbounded dimensionality (i.e. letting K → ∞). This remains
computationally tractable because each model uses a prior that ensures that Z is sparse, based on
the Indian Buffet Process (IBP) [1]. The search for the dimensionality of the latent feature matrices
thus becomes a problem of posterior inference over the number of non-empty columns in Z.

Previous work on nonparametric Bayesian matrix factorization has used Gibbs sampling for poste-
rior estimation [1, 2]. Indeed, Gibbs sampling is the standard inference algorithm used in nonpara-
metric Bayesian methods, most of which are based on the Dirichlet process [3, 4]. However, recent

work has suggested that sequential Monte Carlo methods such as particle ﬁltering can provide an
efﬁcient alternative to Gibbs sampling in Dirichlet proces s mixture models [5, 6].

In this paper we develop a novel particle ﬁltering algorithm for posterior estimation in matrix fac-
torization models that use the IBP, and illustrate its applicability to two speciﬁc models – one with
a conjugate prior, and the other without a conjugate prior but tractable in other ways. Our particle
ﬁltering algorithm is by nature an “on-line ” procedure, whe
re each row of X is processed only once,
in sequence. This stands in comparison to Gibbs sampling, which must revisit each row many times
to converge to a reasonable representation of the posterior distribution. We present simulation results
showing that our particle ﬁltering algorithm can be signiﬁc
antly more efﬁcient than Gibbs sampling
for each of the two models, and discuss its applicability to the broad class of nonparametric matrix
factorization models based on the IBP.

2 Nonparametric Bayesian Matrix Factorization

Let X be an observed N × D matrix. Our goal is to ﬁnd a representation of the structure e xpressed
in this matrix in terms of the latent matrices Z (N × K ) and Y (K × D). This can be formulated
as a statistical problem if we view X as being produced by a probabilistic generative process, re-
sulting in a probability distribution P (X|Z, Y). The critical assumption necessary to make this a
matrix factorization problem is that the distribution of X is conditionally dependent on Z and Y
only through the product ZY. Although de ﬁning P (X|Z, Y) allows us to use methods such as
maximum-likelihood estimation to ﬁnd a point estimate, our goal is to instead compute a posterior
distribution over possible values of Z and Y. To do so we need to specify a prior over the latent
matrices P (Z, Y), and then we can use Bayes’ rule to ﬁnd the posterior distribu tion over Z and Y

P (Z, Y|X) ∝ P (X|Z, Y)P (Z, Y).

(1)

This constitutes Bayesian matrix factorization, but two problems remain: the choice of K , and the
computational cost of estimating the posterior distribution.

Unlike standard matrix factorization methods that require an a priori choice of K , nonparametric
Bayesian approaches allow us to estimate a posterior distribution over Z and Y where the size of
these matrices is unbounded. The models we discuss in this paper place a prior on Z that gives each
“left-ordered ” binary matrix (see [1] for details) probabi
lity

P (Z) =

exp{−αHN }

K+
αK+
Yk=1
Q2N −1
h=1 Kh !
where K+ is the number of columns of Z with non-zero entries, mk is the number of 1’s in column
k , N is the number of rows, HN = PN
i=1 1/i is the N th harmonic number, and Kh is the number
of columns in Z that when read top-to-bottom form a sequence of 1’s and 0’s corresponding to the
binary representation of the number h. This prior on Z is a distribution on sparse binary matrices
that favors those that have few columns with many ones, with the rest of the columns being all zeros.

(N − mk )!(mk − 1)!
N !

(2)

This distribution can be derived as the outcome of a sequential generative process called the Indian
buffet process (IBP) [1]. Imagine an Indian restaurant into which N customers arrive one by one and
serve themselves from the buffet. The ﬁrst customer loads he r plate from the ﬁrst Poisson (α) dishes.
The ith customer chooses dishes proportional to their popularity, choosing a dish with probability
mk /i where mk is the number of people who have choosen the k th dish previously, then chooses
Poisson(α/i) new dishes. If we record the choices of each customer on one row of a matrix whose
columns correspond to a dishes on the buffet (1 if chosen, 0 if not) then (the left-ordered form of)
that matrix constitutes a draw from the distribution in Eqn. 2. The order in which the customers enter
the restaurant has no bearing on the distribution of Z (up to permutation of the columns), making
this distribution exchangeable.

In this work we assume that Z and Y are independent, with P (Z, Y) = P (Z)P (Y). As shown in
Fig. 1, since we use the IBP prior for P (Z), Y is a matrix with an in ﬁnite number of rows and D
columns. We can take any appropriate distribution for P (Y), and the in ﬁnite number of rows will
not pose a problem because only K+ rows will interact with non-zero elements of Z. A posterior
distribution over Z and Y implicitly de ﬁnes a distribution over the effective dimens ionality of these

X

N

D

Z

1

Y

D

~

N

*

1

K
+
Figure 1: Nonparametric Bayesian matrix factorization. The data matrix X is the product of Z and
Y, which have an unbounded number of columns and rows respectively.

matrices, through K+ . This approach to nonparametric Bayesian matrix factorization has been used
for both continuous [1, 7] and binary [2] data matrices X.

Since the posterior distribution de ﬁned in Eqn. 1 is general ly intractable, Gibbs sampling has pre-
viously been employed to construct a sample-based representation of this distribution. However,
generally speaking, Gibbs sampling is slow, requiring each entry in Z and Y to be repeatedly up-
dated conditioned on all of the others. This problem is compounded in contexts where the the
number of rows of X increases as a consequence of new observations being introduced, where the
Gibbs sampler would need to be restarted after the introduction of each new observation.

3 Particle Filter Posterior Estimation

Our approach addresses the problems faced by the Gibbs sampler by exploiting the fact that the prior
on Z is recursively decomposable. To explain this we need to introduce new notation, let X(i) be
the ith row of X, and X(1:i) and Z(1:i) be all the rows of X and Z up to i respectively. Note that
because the IBP prior is recursively decomposable it is easy to sample from P (Z(1:i) |Z(1:i−1) ); to
do so simply follow the IBP in choosing dishes for the ith customer given the record of which dishes
were chosen by the ﬁrst
i − 1 customers (see Algorithm 1). Applying Bayes’ rule, we can write the
posterior on Z(1:i) and Y given X(1:i) in the following form

P (Z(1:i) , Y|X(1:i) ) ∝ P (X(i) |Z(1:i) , Y, X(1:i−1) )P (Z(1:i) , Y|X(1:i−1) ).
Here we do not index Y as it is always an in ﬁnite matrix. 1

(3)

P (Z(1:i) |Z(1:i−1) )P (Z(1:i−1) , Y|X(1:i−1) )

If we could evaluate P (Z(1:i−1) , Y|X(1:i−1) ), we could obtain weighted samples (or “particles”)
from P (Z(1:i) , Y|X(1:i) ) using importance sampling with a proposal distribution of
P (Z(1:i) , Y|X(1:i−1) ) = XZ(1:i−1)
and taking
wℓ ∝ P (X(i) |Z(1:i)
, Y(ℓ) , X(1:i−1) )
(ℓ)
as the weight associated with the ℓth particle.
However, we could also use a similar
scheme to approximate P (Z(1:i−1) , Y|X(1:i−1) ) if we could evaluate P (Z(1:i−2) , Y|X(1:i−2) ).
Following Eq. 4, we could then approximately generate a set of weighted particles from
P (Z(1:i) , Y|X(1:i−1) ) by using the IBP to sample a value from P (Z(1:i) |Z(1:i−1)
) for each parti-
(ℓ)
cle from P (Z(1:i−1) , Y|X(1:i−1) ) and carrying forward the weights associated with those particles.
This “particle ﬁltering ” procedure de ﬁnes a recursive impo
rtance sampling scheme for the full pos-
terior P (Z, Y|X), and is known as sequential importance sampling [8]. When applied in its basic
form this procedure can produce particles with extreme weights, so we resample the particles at each
iteration of the recursion from the distribution given by their normalized weights and set wℓ = 1/L
for all ℓ, which is a standard method known as sequential importance resampling [8].
The procedure de ﬁned in the previous paragraphs is a general -purpose particle ﬁlter for matrix-
factorization models based on the IBP. This procedure will work even when the prior de ﬁned on

(4)

(5)

1 In practice, we need only keep track of the rows of Y that correspond to the non-empty columns of Z, as
the posterior distribution for the remaining entries is just the prior. Thus, if new non-empty columns are added
in moving from Z(i−1) to Z(i) , we need to expand the number of rows of Y that we represent accordingly.

Algorithm 1 Sample P (Z(1:i) |Z(1:i−1) , α) using the Indian Buffet process
1: Z ← Z(1:i−1)
2: if i = 1 then
sample K new
3:
i ∼ Poisson(α)
4:
← 1
Zi,1:Knew
i
5: else
6: K+ ← number of non-zero columns in Z
for k = 1, . . . , K+ do
7:
sample zi,k according to P (zi,k = 1) ∼ Bernoulli( m−i,k
8:
i
end for
9:
sample K new
10:
i ∼ Poisson( α
i )
11:
← 1
Zi,K++1:K++Knew
i
12: end if
13: Z(1:i) ← Z

)

Y is not conjugate to the likelihood (and is much simpler than other algorithms for using the IBP
with non-conjugate priors, e.g. [9]). However, the procedure can be simpliﬁed further in special
cases. The following example applications illustrate the particle ﬁltering approach for two different
models. In the ﬁrst case, the prior over Y is conjugate to the likelihood which means that Y need
not be represented. In the other case, although the prior is not conjugate and thus Y does need to be
explicitly represented, we present a way to improve the efﬁc iency of this general particle ﬁltering
approach by taking advantage of certain analytic conditionals. The particle ﬁltering approach results
in signiﬁcant improvements in performance over Gibbs sampl ing in both models.

4 A Conjugate Model: Inﬁnite Linear-Gaussian Matrix Factor ization

In this model, explained in detail in [1], the entries of both X and Y are continuous. We report
results on the modeling of image data of the same kind as was originally used to demonstrate the
model in [1]. Here each row of X is an image, each row of Z indicates the “latent features” present
in that image, such as the objects it contains, and each column of Y indicates the pixel values
associated with a latent feature.

The likelihood for this image model is matrix Gaussian

exp{−

P (X|Z, Y, σx ) =

tr((X − ZY)T (X − ZY))}

1
1
2σ2
(2πσ2
X )N D/2
X
where σ2
X is the noise variance. The prior on the parameters of the latent features is also Gaussian
1
1
2σ2
(2πσ2
Y )KD/2
Y
with each element having variance σ2
Y . Because both the likelihood and the prior are matrix Gaus-
sian, they form a conjugate pair and Y can be integrated out to yield the collapsed likelihood,

P (Y|σY ) =

exp{−

tr(YT Y)}

P (X|Z, σx ) =

(2π)N D/2σ (N −K+ )D
X

1
σK+D
Y

|ZT
+Z+

σ2
X
σ2
Y

IK+ |D/2

exp{−

1
2σ2
X

tr(XT Σ−1X)} (6)

+Z + σ2
+ . Here Z+ =
which is matrix Gaussian with covariance Σ−1 = I − Z+ (ZT
IK+ )−1ZT
X
σ2
Y
Z1:i,1:K+ is the ﬁrst K+ columns of Z and K+ is the number of non-zero columns of Z.

4.1 Particle Filter

The use of a conjugate prior means that we do not need to represent Y explicitly in our particle ﬁlter.
In this case the particle ﬁlter recursion shown in Eqns. 3 and 4 reduces to
P (Z(1:i) |X(1:i) ) ∝ P (X(i) |Z(1:i) , X(1:i−1) ) XZ(1:i−1)
and may be implemented as shown in Algorithm 2.

P (Z(1:i) |Z(1:i−1) )P (Z(1:i−1) |X(1:i−1) )

Algorithm 2 Particle ﬁlter for In ﬁnite Linear Gaussian Model
1: initialize L particles [Z(0)
], ℓ = 1, . . . , L
ℓ
2: for i = 1, . . . , N do
for ℓ = 1, . . . , L do
3:
(1:i)
(1:i−1)
sample Z
4:
from Z
using Algorithm 1
ℓ
ℓ
calculate wℓ using Eqns. 5 and 7
5:
end for
6:
normalize particle weights
7:
8:
resample particles according to weight cumulative distribution
9: end for

 y

1,:

 y

2,:

 y

3,:

 y

4,:

 z

Y
(i,:)

noise

 x

i,:

Figure 2: Generation of X under the linear Gaussian model. The ﬁrst four images (left t o right)
correspond to the true latent features, i.e. rows of Y. The ﬁfth shows how the images get combined,
with two source images added together by multiplying by a single row of Z, zi,: = [1 0 0 1]. The
sixth is Gaussian noise. The seventh image is the resulting row of X.

Reweighting the particles requires computing P (X(i) |Z(1:i) , X(1:i−1) ), the conditional probability
of the most recent row of X given all the previous rows and Z. Since P (X(1:i) |Z(1:i) ) is matrix
Gaussian we can ﬁnd the required conditional distribution b y following the standard rules for con-
X be the covariance matrix for X(1:i) given Z(1:i) ,
ditioning in Gaussians. Letting Σ−1
∗ = Σ−1/σ2
we can partition this matrix into four parts
cT b #
∗ = " A c
Σ−1
where A is a matrix, c is a vector, and b is a scalar. Then the conditional distribution of X(i) is
X(i) |Z(1:i) , X(1:i−1) ∼ Gaussian(cT A−1X(1:i−1) , b − cT A−1c).
(7)
This requires inverting a matrix A which grows linearly with the size of the data; however, A is
highly structured and this can be exploited to reduce the cost of this inversion [10].

4.2 Experiments

We compared the particle ﬁlter in Algorithm 2 with Gibbs samp ling on an image dataset similar
to that used in [1]. Due to space limitations we refer the reader to [1] for the details of the Gibbs
sampler for this model. As illustrated in Fig. 2, our ground-truth Y consisted of four different
6 × 6 latent images. A 100 × 4 binary ground-truth matrix Z was generated with by sampling from
P (zi,k = 1) = 0.5. The observed matrix X was generated by adding Gaussian noise with σX = 0.5
to each entry of ZY.

Fig. 3 compares results from the particle ﬁlter and Gibbs sam pler for this model. The performance of
the models was measured by comparing a general error metric computed over the posterior distribu-
tions estimated by each approach. The error metric (the vertical axis in Figs. 3 and 5) was computed
by taking the expectation of the matrix ZZT over the posterior samples produced by each algorithm
and taking the summed absolute difference (i.e. L1 norm) between the upper triangular portion of
E [ZZT ] computed over the samples and the upper triangular portion of the true ZZT (including
the diagonal). See Fig. 4 for an illustration of the information conveyed by ZZT . This error metric
measures the distance of the mean of the posterior to the ground-truth. It is zero if the mean of the
distribution matches the ground truth. It grows as a function of the difference between the ground
truth and the posterior mean, accounting both for any difference in the number of latent factors that
are present in each observation and for any difference in the number of latent factors that are shared
between all pairs of observations.

The particle ﬁlter was run using many different numbers of pa rticles, P . For each value of P , the
particle ﬁlter was run 10 times. The horizontal axis locatio n of each errorbar in the plot is the mean

5000

4000

3000

2000

1000

r
o
r
r
E

 

Gibbs Sampler
Particle Filter

0

 

1

0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
5
0
Wallclock runtime in sec.
0
5
0
1
2
5
1
2
5
Figure 3: Performance results for particle ﬁlter vs. Gibbs s ampling posterior estimation for the
in ﬁnite linear Gaussian matrix factorization. Each point i s an average over 10 runs with a particular
number of particles or sweeps of the sampler P = [1, 10, 100, 500, 1000, 2500, 5000] left to right,
and error bars indicate the standard deviation of the error.

wall-clock computation time on 2 Ghz Athlon 64 processors running Matlab for the corresponding
number of particles P while the error bars indicate the standard deviation of the error. The Gibbs
sampler was run for varying numbers of sweeps, with the initial 10% of samples being discarded.
The number of Gibbs sampler sweeps was varied and the results are displayed in the same way as
described for the particle ﬁlter above. The results show tha t the particle ﬁlter attains low error in
signiﬁcantly less time than the Gibbs sampler, with the diff erence being an order or magnitude or
more in most cases. This is a result of the fact that the particle ﬁlter considers only a single row of
X on each iteration, reducing the cost of computing the likelihood.

5 A Semi-Conjugate Model: Inﬁnite Binary Matrix Factorizat ion

In this model, ﬁrst presented in the context of learning hidd en causal structure [2], the entries of
both X and Y are binary. Each row of X represents the values of a single observed variable across
D trials or cases, each row of Y gives the values of a latent variable (a “hidden cause ”) acro ss those
trials or cases, and Z is the adjacency matrix of a bipartite Bayesian network indicating which latent
variables in ﬂuence which observed variables. Learning the hidden causal structure then corresponds
to inferring Z and Y from X. The model ﬁts our schema for nonparametric Bayesian matrix factor-
ization model (and hence is amenable to the use of our particle ﬁlter) since the likelihood function
it uses depends only on the product ZY.

The likelihood function for this model assumes that each entry of X is generated independently
P (X|Z, Y) = Qi,d P (xi,d |Z, Y), with its probability given by the “noisy-OR” [11] of the cau ses
that in ﬂuence that variable (identiﬁed by the correspondin
g row of Z) and are active for that case or
trial (expressed in Y). The probability that xi,d takes the value 1 is thus
P (xi,d = 1|Z, Y) = 1 − (1 − λ)zi,: ·y:,d (1 − ǫ)
(8)
where zi,: is the ith row of Z, y:,d is the dth column of Y, and zi,: · y:,d = PK
k=1 zi,k yk,d . The
parameter ǫ sets the probability that xi,d = 1 when no relevant causes are active, and λ determines
how this probability changes as the number of relevant active hidden causes increases. To complete
the model, we assume that the entries of Y are generated independently from a Bernoulli process
with parameter p, to give P (Y) = Qk,d pyk,d (1 − p)1−yk,d , and use the IBP prior for Z.
5.1 Particle Filter

In this model the prior over Y is not conjugate to the likelihood, so we are forced to explicitly
represent Y in our particle ﬁlter state, as outlined in Eqns. 3 and 4. Howe ver, we can de ﬁne a more
efﬁcient algorithm than the basic particle ﬁlter due to the t
ractability of some integrals. This is why
we call this model a “semi-conjugate ” model.

The basic particle ﬁlter de ﬁned in Section 3 requires drawin
g the new rows of Y from the prior
when we generate new columns of Z. This can be problematic since the chance of producing an
assignment of values to Y that has high probability under the likelihood can be quite low, in effect
wasting many particles. However, if we can analytically marginalize out the new rows of Y, we
can avoid sampling those values from the prior and instead sample them from the posterior, in

Algorithm 3 Particle ﬁlter for In ﬁnite Binary Matrix Factorization
1: initialize L particles [Z(0)
, Y(0)
], ℓ = 1, . . . , L
ℓ
ℓ
2: for i = 1, . . . , N do
for ℓ = 1, . . . , L do
3:
(i)
(i−1)
sample Z
4:
from Z
using Algorithm 1
ℓ
ℓ
calculate wℓ using Eqns. 5 and 8
5:
end for
6:
normalize particle weights
7:
resample particles according to weight CDF
8:
for ℓ = 1, . . . , L do
9:
ℓ |Z(1:i)
from P (Y(i)
sample Y(i)
10:
ℓ
ℓ
end for
11:
12: end for

, Y(1:i−1)
ℓ

, X(1:i) )

Figure 4: In ﬁnite binary matrix factorization results. On t he left is ground truth, the causal graph
representation of Z and ZZT . The middle and right are particle ﬁltering results; a singl e random
particle Z and E [ZZT ] from a 500 and 10000 particle run middle and right respectively.

effect saving many of the potentially wasted particles. If we let Y(1:i) denote the rows of Y that
correspond to the ﬁrst
i columns of Z and Y(i) denote the rows (potentially more than 1) of Y that
are introduced to match the new columns appearing in Z(i) , then we can write

P (Z(1:i) , Y(1:i) |X(1:i) ) = P (Y(i) |Z(1:i) , Y(1:i−1) , X(1:i) )P (Z(1:i) , Y(1:i−1) |X(1:i) )

(9)

where
P (Z(1:i) , Y(1:i−1) |X(1:i) ) ∝ P (X(i) |Z(1:i) , Y(1:i−1) , X(1:i−1) )P (Z(1:i) , Y(1:i−1) |X(1:i−1) ).
(10)
Thus,
estimate
to
ﬁlter
particle
the
use
can
we
P (Z(1:i) , Y(1:i−1) |X(1:i) )
(vs. P (Z(1:i) , Y(1:i) |X(1:i) )) provided that we can ﬁnd a way to compute P (X(i) |Z(1:i) , Y(1:i−1) )
and sample from the distribution P (Y(i) |Z(1:i) , Y(1:i−1) , X(1:i) ) to complete our particles.
The procedure described in the previous paragraph is possible in this model because, while our prior
on Y is not conjugate to the likelihood, it is still possible to compute P (X(i) |Z(1:i) , Y(1:i−1) ). The
entries of X(i) are independent given Z(1:i) and Y(1:i) . Since the entries in each column of Y(i) will
in ﬂuence only a single entry in X(i) , this independence is maintained when we sum out Y(i) . So we
can derive an analytic solution to P (X(i) |Z(1:i) , Y(1:i−1) ) = Qd P (xi,d |Z(1:i) , Y(1:i−1) ) where
P (xi,d = 1|Z(1:i) , Y(1:i−1) ) = 1 − (1 − ǫ)(1 − λ)η (1 − λp)K new
(11)
i
with K new
being the number of new columns in Z(i) , and η = z
. For a detailed
· y
i,1:K (1:i)
1:K (1:i)
i
+ ,d
+
derivation see [2]. This gives us the likelihood we need for reweighting particles Z(1:i) and Y(1:i−1) .
The posterior distribution on Y(i) is straightforward to compute by combining the likelihood in
Eqn. 8 with the prior P (Y). The particle ﬁltering algorithm for this model is given in A lgorithm 3.

5.2 Experiments

We compared the particle ﬁlter in Algorithm 3 with Gibbs samp ling on a dataset generated from the
model described above, using the same Gibbs sampling algorithm and data generation procedure
as developed in [2]. We took K+ = 4 and N = 6, running the IBP multiple times with α = 3
until a matrix Z of correct dimensionality (6 × 4) was produced. This matrix is shown in Fig. 4 as a
bipartite graph, where the observed variables are shaded. A 4 × 250 random matrix Y was generated
with p = 0.1. The observed matrix X was then sampled from Eqn. 8 with parameters λ = .9
and ǫ = .01. Comparison of the particle ﬁlter and Gibbs sampling was don e using the procedure
outlined in Section 4.2, producing similar results: the particle ﬁlter gave a better approximation to
the posterior distribution in less time, as shown in Fig. 5.

r
o
r
r
E

50

40

30

20

10

0

 

 

Gibbs Sampler
Particle Filter

5
..
0

1

2

5

5
0
2
1
.
Wallclock runtime in sec.
0
Figure 5: Performance results for particle ﬁlter vs. Gibbs s ampling posterior estimation for the
in ﬁnite binary matrix factorization model. Each point is an average over 10 runs with a particular
number of particles or sweeps of the sampler P = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000] from left
to right, and error bars indicate the standard deviation of the error.

0
0
1

0
0
5

0
5

6 Conclusion

In this paper we have introduced particle ﬁlter posterior es timation for non-parametric Bayesian
matrix factorization models based on the Indian buffet process. This approach is applicable to any
Bayesian matrix factorization model with a sparse recursively decomposable prior. We have applied
this approach with two different models, one with a conjugate prior and one with a non-conjugate
prior, ﬁnding signiﬁcant computational savings over Gibbs
sampling for each. However, more work
needs to be done to explore the strengths and weakneses of these algorithms. In particular, simple
sequential importance resampling is known to break down when applied to datasets with many
observations, although we are optimistic that methods for addressing this problem that have been
developed for Dirichlet process mixture models (e.g., [5]) will also be applicable in this setting.
By exploring the strengths and weaknesses of different methods for approximate inference in these
models, we hope to come closer to our ultimate goal of making nonparametric Bayesian matrix
factorization into a tool that can be applied on the scale of real world problems.

Acknowledgements This work was supported by both NIH-NINDS R01 NS 50967-01 as part of the NSF/NIH
Collaborative Research in Computational Neuroscience Program and NSF grant 0631518.

References

[1] T. L. Grifﬁths and Z. Ghahramani, “Inﬁnite latent featur
e models and the Indian buffet process,” Gatsby
Computational Neuroscience Unit, Tech. Rep. 2005-001, 2005.
[2] F. Wood, T. L. Grifﬁths, and Z. Ghahramani, “A non-parame
tric Bayesian method for inferring hidden
causes,” in Proceeding of the 22nd Conference on Uncertainty in Arti ﬁci al Intelligence.
in press, 2006.
[3] T. Ferguson, “A Bayesian analysis of some nonparametric problems,” The Annals of Statistics, vol. 1, pp.
209–230, 1973.
[4] R. M. Neal, “Markov chain sampling methods for Dirichlet process mixture models,” Department of
Statistics, University of Toronto, Tech. Rep. 9815, 1998.
[5] P. Fearnhead, “Particle ﬁlters for mixture models with a n unknown number of components,” Journal of
Statistics and Computing, vol. 14, pp. 11–21, 2004.
[6] S. N. MacEachern, M. Clyde, and J. Liu, “Sequential impor tance sampling for nonparametric Bayes
models: the next generation,” The Canadian Journal of Statistics, vol. 27, pp. 251–267, 1999.
[7] T. Grifﬁths and Z. Ghahramani, “Inﬁnite latent feature m odels and the Indian buffet process,” in Advances
in Neural Information Processing Systems 18, Y. Weiss, B. Sch ¨olkopf, and J. Platt, Eds. Cambridge,
MA: MIT Press, 2006.
[8] A. Doucet, N. de Freitas, and N. Gordon, Sequential Monte Carlo Methods in Practice. Springer, 2001.
[9] D. G ¨or ¨ur, F. J ¨akel, and C. R. Rasmussen, “A choice model with inﬁnitely many latent features,” in
Pro-
ceeding of the 23rd International Conference on Machine Learning, 2006.
[10] S. Barnett, Matrix Methods for Engineers and Scientists. McGraw-Hill, 1979.
[11] J. Pearl, Probabilistic reasoning in intelligent systems. San Francisco, CA: Morgan Kaufmann, 1988.


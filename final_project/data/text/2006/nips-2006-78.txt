Kernel Maximum Entropy Data Transformation
and an Enhanced Spectral Clustering Algorithm

Robert Jenssen1 (cid:3), Torbjør n Eltoft1 , Mark Girolami2 and Deniz Erdogmus3

1 Department of Physics and Technology, University of Tromsł, Norway
2 Department of Computing Science, University of Glasgow, Scotland
3Department of Computer Science and Engineering, Oregon Health and Science University, USA

Abstract

We propose a new kernel-based data transformation technique. It is founded on
the principle of maximum entropy (MaxEnt) preservation, hence named kernel
MaxEnt. The key measure is Renyi’s entropy estimated via Parzen windowing.
We show that kernel MaxEnt is based on eigenvectors, and is in that sense similar
to kernel PCA, but may produce strikingly different transformed data sets. An
enhanced spectral clustering algorithm is proposed, by replacing kernel PCA by
kernel MaxEnt as an intermediate step. This has a major impact on performance.

1 Introduction

Data transformation is of fundamental importance in machine learning, and may greatly improve and
simplify tasks such as clustering. Some of the most well-known approaches to data transformation
are based on eigenvectors of certain matrices. Traditional techniques include principal component
analysis (PCA) and classical multidimensional scaling. These are linear methods. Recent advanced
non-linear techniques include locally-linear embedding [1] and isometric mapping [2]. Of special
interest to this paper is kernel PCA [3], a member of the kernel-based methods [4].
Recently, it has been shown that there is a close connection between the kernel methods and infor-
mation theoretic learning [5, 6, 7, 8]. We propose a new kernel-based data transformation technique
based on the idea of maximum entropy preservation. The new method, named kernel MaxEnt, is
based on Renyi’s quadratic entropy estimated via Parzen windowing. The data transformation is
obtained using eigenvectors of the data af(cid:2)nity matrix. These eigenvectors are in general not the
same as those used in kernel PCA. We show that kernel MaxEnt may produce strikingly different
transformed data sets than kernel PCA. We propose an enhanced spectral clustering algorithm, by
replacing kernel PCA by kernel MaxEnt as an intermediate step. This seemingly minor adjustment
has a huge impact on the performance of the algorithm.
This paper is organized as follows. In section 2, we brie(cid:3)y review kernel PCA. Section 3 is devoted
to the kernel MaxEnt method. Some illustrations are given in section 4. The enhanced spectral
clustering is discussed in section 5. Finally, we conclude the paper in section 6.

2 Kernel PCA

PCA is a linear data transformation technique based on the eigenvalues and eigenvectors of the
(d (cid:2) d) data correlation matrix, where d is the data dimensionality. A dimensionality reduction
from d to l < d is obtained by projecting a data point onto a subspace spanned by the eigen-
vectors (principal axes) corresponding to the l largest eigenvalues. It is well-known that this data
(cid:3)Corresponding author. Phone: (+47) 776 46493. Email: robertj@phys.uit.no.

transformation preserves the maximum amount of variance in the l-dimensional data compared
to the original d-dimensional data. Sch ¤olkopf et al. [3] proposed a non-linear extension, by per-
forming PCA implicitly in a kernel feature space which is non-linearly related to the input space
via the mapping xi ! (cid:8)(xi ), i = 1; : : : ; N . Using the kernel-trick to compute inner-products,
k(xi ; xj ) = h(cid:8)(xi ); (cid:8)(xj )i, it was shown that the eigenvalue problem in terms of the feature space
correlation matrix is reduced to an eigenvalue problem in terms of the kernel matrix Kx , where
element (i; j ) of Kx equals k(xi ; xj ), i; j = 1; : : : ; N . This matrix can be eigendecomposed as
Kx = EDET , where D is a diagonal matrix storing the eigenvalues in descending order, and E is
a matrix with the eigenvectors as columns. Let (cid:8)pca be a matrix where each column corresponds
to the PCA projection of the data points (cid:8)(xi ), i = 1; : : : ; N , onto the subspace spanned by the
1
l largest kernel space principal axes. Then, (cid:8)pca = D
l , where the (l (cid:2) l) matrix Dl stores
l ET
2
the l largest eigenvalues, and the (N (cid:2) l) matrix El stores the corresponding eigenvectors. This is
the kernel PCA transformed data set 1 . Kernel PCA thus preserves variance in terms of the kernel
induced feature space.
However, kernel PCA is not easily interpreted in terms of the input space data set. How does
variance preservation in the kernel feature space correspond to an operation on the input space data
set? To the best of our knowledge, there are no such intuitive interpretations of kernel PCA. In
the next section, we introduce kernel MaxEnt, which we show is related to kernel PCA. However,
kernel MaxEnt may be interpreted in terms of the input space, and will in general perform a different
projection in the kernel space.

3 Kernel MaxEnt
The Renyi quadratic entropy is given by H2 (x) = (cid:0) log R f 2 (x)dx [9], where f (x) is the density
associated with the random variable X. A d-dimensional data set x i ; i = 1; : : : ; N , generated from
f (x), is assumed available. A non-parametric estimator for H2 (x) is obtained by replacing the
actual pdf by its Parzen window estimator, given by [10]

d
2

1
N 2

(1)

(2)

^f (x) =

1
N

jjx (cid:0) xi jj2
2(cid:27)2

W(cid:27) (x; xi ); W(cid:27) (x; xi ) =

N
exp (cid:26)(cid:0)
(cid:27) :
1
Xi=1
(2(cid:25)(cid:27)2 )
The Parzen window need not be Gaussian, but it must be a density itself. The following derivation
assumes a Gaussian window. Non-Gaussian windows are easily incorporated. Hence, we obtain
N
N
^H2 (x) = (cid:0) log Z ^f 2 (x)dx = (cid:0) log
Xj=1 Z W(cid:27) (x; xi )W(cid:27) (x; xj )dx
Xi=1
N
N
1
Xi=1
Xj=1
Wp2(cid:27) (xi ; xj );
= (cid:0) log
N 2
where in the last step the convolution theorem for Gaussian functions has been employed.
For notational simplicity, we denote Wp2(cid:27) (xi ; xj ) as k(xi ; xj ). Note that since Wp2(cid:27) ((cid:1); (cid:1)) is a
Gaussian function, it is also a Mercer kernel, and so is k((cid:1); (cid:1)). In the following, we construct the
kernel matrix Kx , such that element (i; j ) of Kx equals k(xi ; xj ), i; j = 1; : : : ; N .
It is easily shown that the Renyi quadratic entropy may be expressed compactly in terms of the
kernel matrix as ^H2 (x) = (cid:0) log 1
N 2 1T Kx1, where 1 is a (N (cid:2) 1) ones-vector. Since the logarithm
is a monotonic function, we will in the remainder of this paper focus on the quantity V (x) =
N 2 1T Kx1. It is thus clear that all the information regarding the Renyi entropy resides in the kernel
1
matrix Kx . Hence, the kernel matrix is the input space quantity of interest in this paper.
A well-known input space data transformation principle is founded on the idea of maximum entropy
(MaxEnt), usually de(cid:2)ned in terms of minimum model complexity. In this paper, we de(cid:2)ne MaxEnt
differently, as a mapping X ! Y, such that the entropy associated with Y is maximally similar
1 In [3] the kernel feature space data was assumed centered, obtained by a centering operation of the kernel
matrix. We do not assume centered data here.

to the entropy of X. Since we are concerned with Renyi’s entropy, it is therefore clear that such
a data mapping results in a V (y) = 1
N 2 1T Ky 1, in terms of the Y data set, which should be as
close as possible to V (x) = 1
N 2 1T Kx1. This means that the kernel matrix Ky must be maximally
similar to Kx in some sense. Since our input space quantity of concern is the kernel matrix, we
are only implicitly concerned with the Y data set (we do not actually want to obtain Y, nor is the
dimensionality of interest).
The kernel matrix can be decomposed as Kx = EDET . The kernel matrix is at the same time an
inner-product matrix in the Mercer kernel induced feature space. Let (cid:8)x be a matrix such that each
column represents an approximation to the corresponding kernel feature space data point in the set
(cid:8)(x1 ); : : : ; (cid:8)(xN ). An approximation which preserves inner-products is given by (cid:8)x = D
2 ET ,
1
2 ET is the projection onto all the principal
since then Kx = (cid:8)T
x (cid:8)x = EDET . Note that (cid:8)x = D
1
axes in the Mercer kernel feature space, hence de(cid:2)ning a N -dimensional data set.
We now describe a dimensionality reduction in the Mercer kernel space, obtaining the k -dimensional
y (cid:8)y such that V (y) (cid:25) V (x). Notice that we may rewrite V (x) as
(cid:8)y from (cid:8)x , yielding Ky = (cid:8)T
follows [8]

(cid:21)i (cid:13) 2
i ;

V (x) =

(cid:21)i (1T ei )2 =

N
N
1
1
Xi=1
Xi=1
N 2
N 2
where ei is the eigenvector corresponding to the i’th column of Kx , and 1T ei = (cid:13)i . We also assume
N .
that the products (cid:21)i (cid:13) 2
i have been sorted in decreasing order, such that (cid:21)1 (cid:13) 2
1 (cid:21) : : : (cid:21) (cid:21)N (cid:13) 2
If we are to approximate V (x) using only k terms (eigenvalues/eigenvectors) of the sum Eq. (3),
we must use the k (cid:2)rst terms in order to achieve minimum approximation error. This corresponds
1
k , using the k eigenvalues
i . Let us de(cid:2)ne the data set (cid:8)y = D
to using the k largest (cid:21)i (cid:13) 2
k ET
2
and eigenvectors of Kx corresponding to the k largest products (cid:21)i (cid:13) 2
i . Hence, Ky = (cid:8)T
y (cid:8)y =
1
1
k , and
k ET
k = EkDkET
EkD
k D
2
2

(3)

(4)

V (y) =

(cid:21)i (cid:13) 2
i =

1T Ky 1;

k
1
1
Xi=1
N 2
N 2
the best approximation to the entropy estimate V (x) using k eigenvalues and eigenvectors. We
1
k as a maximum entropy data transformation in a Mercer
thus refer to the mapping (cid:8)y = D
k ET
2
kernel feature space. Note that this is not the same as the PCA dimensionality reduction in Mercer
1
kernel feature space, which is de(cid:2)ned as (cid:8)pca = D
l , using the eigenvalues and eigenvectors
l ET
2
corresponding to the l largest eigenvalues of Kx . In terms of the eigenvectors of the kernel feature
space correlation matrix, we project (cid:8)(xi ) onto a subspace spanned by different eigenvectors, which
is possibly not the most variance preserving (remember that variance in the kernel feature space data
set is given by the sum of the largest eigenvalues).
The kernel MaxEnt procedure, as described above, is summarized in Table 1. It is important to
realize that kernel MaxEnt outputs two quantities, which may be used for further data analysis. The
1
k . The input space output
kernel space output quantity is the transformed data set (cid:8)y = D
k ET
2
quantity is the kernel matrix Ky = EkDkET
k , which is an approximation to the original kernel
matrix Kx .

I nput S pace

K ernel S pace

Kx = EDET ! (cid:8)x = D

1
2 ET

o

#

Ky = EkDkET
k   (cid:8)y = D

1
k ET
2
k

Table 1. Flow of the kernel MaxEnt procedure. There are two possible outputs; the input space
kernel matrix Ky , and the kernel space data set (cid:8)y .

3.1 Interpretation in Terms of Cost Function Minimization
Kernel MaxEnt produces a new kernel matrix Ky = EkDkET
k , such that the sum of the elements of
Ky is maximally equal to the sum of the elements of Kx . Hence, kernel MaxEnt picks eigenvectors
and eigenvalues in order to minimize the cost function 1T (Kx (cid:0) Ky )1. On the other hand, it is well
known that the kernel PCA matrix Kpca = ElDlET
l , based on the l largest eigenvalues, minimizes
the Frobenius norm of (Kx (cid:0) Kpca ), that is 1T (Kx (cid:0) Kpca ):21 (where A:2 denotes elementwise
squaring of matrix A.)

3.2 Kernel MaxEnt Eigenvectors Reveal Cluster Structure

(5)

Under (cid:147)ideal(cid:148) circumstances, kernel MaxEnt and kernel PCA yield the same result, as shown in the
following. Assume that the data consists of C = 2 different maximally compact subsets, such that
k(xi ; xj ) = 1 for xi and xj in the same subset, and k(xi ; xj ) = 0 for xi and xj in different subsets
(point clusters). Assume that subset one consists of N1 data points, and subset two consists of N2
data points. Hence, N = N1 + N2 and we assume N1 (cid:21) N2 . Then
0N2(cid:2)N1 1N2(cid:2)N2 (cid:21) ; E = "
1N2 # ;
1pN1
K = (cid:20) 1N1(cid:2)N1 0N1(cid:2)N2
1N1
0N1
1pN2
0N2
where 1M (cid:2)M (0M (cid:2)M ) is the (M (cid:2)M ) all ones (zero) matrix and D = diag(N1 ; N2 ). Hence, a data
point xi in subgroup one will be represented by xi ! [1 0]T and a data point xj in subgroup two
will be represented by xj ! [0 1]T both using (cid:8)y and (cid:8)pca . (see also [11] for a related analysis).
Thus, kernel MaxEnt and kernel PCA yield the same data mapping, where each subgroup is mapped
into mutually orthogonal points in the kernel space (the clusters were points also in the input space,
but not necessarily orthogonal). Hence, in the (cid:147)ideal(cid:148) case, the clusters in the transformed data
set is spread by 90 degrees angles. Also, the eigenvectors carry all necessary information about
the cluster structure (cluster memberships can be assigned by a proper thresholding). This kind of
(cid:147)ideal(cid:148) analysis has been used as a justi(cid:2)cation for the kernel PCA mapping, where the mapping
is based on the C largest eigenvalues/eigenvectors. Such a situation will correspond to maximally
concentrated eigenvalues of the kernel matrix.
In practice, however, there will be more than C non-zero eigenvalues, not necessarily concentrated,
and corresponding eigenvectors, because there will be no such (cid:147)ideal(cid:148) situation. Shawe-Taylor and
Cristianini [4] note that kernel PCA can only detect stable patterns if the eigenvalues are concen-
trated. In practice, the (cid:2)rst C eigenvectors may not necessarily be those which carry most infor-
mation about the clustering structure of the data set. However, kernel MaxEnt will seek to pick
those eigenvectors with the blockwise structure corresponding to cluster groupings, because this
will make the sum of the elements in Ky as close as possible to the sum of the elements of Kx .
Some illustrations of this property follow in the next section.

3.3 Parzen Window Size Selection

The Renyi entropy estimate is directly connected to Parzen windowing. In theory, therefore, an
appropriate window, or kernel, size, corresponds to an appropriate density estimate. Parzen window
size selection has been thoroughly studied in statistics [12]. Many reliable data-driven methods
exist, especially for data sets of low to moderate dimensionality. Silverman’s rule [12] is one of the
1
simplest, given by ^(cid:27) = (cid:27)X h
(2d+1)N i
d+4 , where (cid:27) 2
X = d(cid:0)1 Pi (cid:6)Xii , and (cid:6)Xii are the diagonal
4
elements of the sample covariance matrix. Unless otherwise stated, the window size is determined
using this rule.

4 Illustrations

Fig. 1 (a) shows a ring-shaped data set consisting of C = 3 clusters (marked with different symbols
for clarity). The vertical lines in (b) show the 10 largest eigenvalues (normalized). The largest eigen-
value is more than twice as large as the second largest. However, the values of the remaining eigen-
values are not signi(cid:2)cantly different. The bars in (b) shows the entropy terms (cid:21) i (cid:13) 2
i (normalized)

corresponding to these largest eigenvalues. Note that the entropy terms corresponding to the (cid:2)rst,
fourth and seventh eigenvalues are signi(cid:2)cantly larger than the rest. This means that kernel MaxEnt
is based on the (cid:2)rst, fourth and seventh eigenvalue/eigenvector pair (yielding a 3-dimensional trans-
formed data set). In contrast, kernel PCA is based on the eigenvalue/eigenvector pair corresponding
to the three largest eigenvalues. In (c) the kernel MaxEnt data transformation is shown. Note that
the clusters are located along different lines radially from the origin (illustrated by the lines in the
(cid:2)gure). These lines are almost orthogonal to each other, hence approximating what would be ex-
pected in the (cid:147)ideal(cid:148) case. The kernel PCA data transformation is shown in (d). This data set is
signi(cid:2)cantly different. In fact, the mean vectors of the clusters in the kernel PCA representation are
not spread angularly. In (e), the (cid:2)rst eight eigenvectors are shown. The original data set is ordered,
such that the (cid:2)rst 63 elements correspond to the innermost ring, the next 126 elements correspond
to the ring in the middle, and the (cid:2)nal 126 elements correspond to the outermost ring. Observe how
eigenvectors one, four and seven are those which carry information about the cluster structure, with
their blockwise appearance. The kernel matrix Kx is shown in (f). Ideally, this should be a block-
wise matrix. It is not. In (g), the kernel MaxEnt approximation Ky to the original kernel matrix is
shown, obtained from eigenvectors one, four and seven. Note the blockwise appearance. In contrast,
(g) shows the corresponding Kpca . The same blockwise structure can not be observed.
Fig. 2 (a) shows a ring-shaped data set consisting of two clusters. In (b) and (c) the kernel MaxEnt
(eigenvalues/eigenvectors one and (cid:2)ve) and kernel PCA transformations are shown, respectively.
Again, kernel MaxEnt produces a data set where the clusters are located along almost orthogonal
lines, in contrast to kernel PCA. The same phenomenon is observed for the data set shown in (d),
with the kernel MaxEnt (eigenvalues/eigenvectors one and four) and kernel PCA transformations
shown in (e) and (f), respectively. In addition, (g) and (h) shows the kernel MaxEnt (eigenval-
ues/eigenvectors one, two and (cid:2)ve) and kernel PCA transformations of the 16-dimensional pen-
based handwritten digit recognition data set (three clusters, digits 0, 1 and 2), extracted from the
UCI repository. Again, similar comments can be made. These illustrations show that kernel MaxEnt
produces a different transformed data set than kernel PCA. Also, it produces a kernel matrix Ky
having a blockwise appearance. Both the transformed data (cid:8)y and the new kernel matrix can be
utilized for further data analysis. In the following, we focus on (cid:8)y .

5 An Enhanced Spectral Clustering Algorithm

^D(f1 ; f2 ) =

(6)

= cos 6 (m1 ; m2);

A recent spectral clustering algorithm [7] is based on the Cauchy-Schwarz (CS) pdf divergence
measure, which is closely connected to the Renyi entropy. Let ^f1(x) and ^f2 (x) be Parzen window
estimators of the densities corresponding to two clusters. Then, an estimator for the CS measure can
be expressed as [6]
R ^f1 (x) ^f2 (x)dx
qR ^f 2
1 (x)dx R ^f 2
2 (x)dx
where m1 and m2 are the kernel feature space mean vectors of the data points corresponding to
the two clusters. Note that ^D(f1 ; f2 ) 2 [0; 1], reaching its maximum value if m1 = m2 ( ^f1 (x) =
^f2 (x)), and its minimum value if the two vectors (densities) are orthogonal. The measure can easily
be extended to more than two pdfs. The clustering is based on computing the cosine of the angle
between a data point and the mean vector mi of each cluster !i , i = 1; : : : ; C , for then to assign
the data point to the cluster corresponding to the maximum value. This procedure minimizes the CS
measure as de(cid:2)ned above. Kernel PCA was used for representing the data in the kernel feature space.
As an illustration of the utility of kernel MaxEnt, we here replace kernel PCA by kernel MaxEnt.
This adjustment has a major impact on the performance. The algorithm thus has the following steps:
1) Use some data-driven method from statistics to determine the Parzen window size. 2) Compute
the kernel matrix Kx . 3) Obtain a C -dimensional kernel feature space representation using kernel
MaxEnt. 4) Initialize mean vectors. 5) For all data points: xt ! !i : maxi cos 6 ((cid:8)(xt ); mi ). 6)
Update mean vectors. 7) Repeat steps 5-7 until convergence. For further details (like mean vector
initialization etc.) we refer to [7].
Fig. 3 (a) shows the clustering performance in terms of the percentage of correct labeling for the
data set shown in Fig. 2 (d). There are three curves: Our spectral clustering algorithm using kernel
MaxEnt (marked by the circle-symbol), and kernel PCA (star-symbol), and in addition we compare

30

20

10

0

−10

−20

−30
−30

−20

−10

0

10

20

30

(a)

1

0.8

0.6

0.4

0.2

0

0.4

0.2

0

−0.2

−0.4
0.5

0

−0.5

0

0.2

0.6

0.4

(d)

0.2

0.1

0

−0.1

−0.2
0.2

1

2

3

4

5

6

7

8

9

10

0

−0.2

0

0.2

0.6

0.4

(c)

(b)

1

4

7

(e)

 

 

(f)

(g)

(h)

Figure 1: Examples of data transformations using kernel MaxEnt and kernel PCA.

with the state-of-the-art Ng et al. method (NG) [11] using the Laplacian matrix. The clustering is
performed over a range of kernel sizes. The vertical line indicates the (cid:147)optimal(cid:148) kernel size using
Silverman’s rule. Over the whole range, kernel MaxEnt performs equally good as NG, and better
than kernel PCA for small kernel sizes. Fig. 3 (b) shows a similar result for the data set shown in
Fig. 1 (a). Kernel MaxEnt has the best performance over the most part of the kernel range. Fig. 3 (c)
shows the mean result for the benchmark thyroid data set [13]. On this data set, kernel MaxEnt
performs considerably better than the two other methods, over a wide range of kernel sizes.
These preliminary experiments show the potential bene(cid:2)ts of kernel MaxEnt in data analysis, espe-
cially when the kernel space cost function is based on an angular measure. Using kernel MaxEnt
makes the algorithm competitive to spectral clustering using the Laplacian matrix. We note that
kernel MaxEnt in theory requires the full eigendecomposition, thus making it more computationally
complex than clustering based on only the C largest eigenvectors.

1.5

1

0.5

0

−0.5

−1

−1.5
−1.5

3

2

1

0

−1

−2

−3
−5

−1

−0.5

0

0.5

1

1.5

(a)

−0.8

−0.6

−0.4

−0.2

0

(b)

0.2

0

−0.2

−0.4

−0.6

−0.8
−1

0.6

0.5

0.4

0.3

0.2

0.1

0

−0.8

−0.6

−0.4

−0.2

0

(c)

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8
−1

1

0.5

0

−0.5

−4

−3

−2

−1

0

1

2

−0.1

0

0.2

0.4

0.6

0.8

−1

0

0.2

0.4

0.6

0.8

(d)

(e)

(f)

0.1

0

−0.1

−0.2

−0.3
0.4

0.2

0

−0.2

0

0.4

0.2

0.2

0.1

0

−0.1

−0.2
0.4

0.2

0

−0.2

0

0.4

0.2

(g)

(h)

Figure 2: Examples of data transformations using kernel MaxEnt and kernel PCA.

6 Conclusions

In this paper, we have introduced a new data transformation technique, named kernel MaxEnt, which
has a clear theoretical foundation based on the concept of maximum entropy preservation. The new
method is similar in structure to kernel PCA, but may produce totally different transformed data sets.
We have shown that kernel MaxEnt signi(cid:2)cantly enhances a recent spectral clustering algorithm.
Kernel MaxEnt also produces a new kernel matrix, which may be useful for further data analysis.
Kernel MaxEnt requires the kernel to be a valid Parzen window (i.e. a density). Kernel PCA requires
the kernel to be a Mercer kernel (positive semide(cid:2)nite), hence not necessarily a density. In that
sense, kernel PCA may use a broader class of kernels. On the other hand, kernel MaxEnt may use
Parzen windows which are not Mercer kernels (inde(cid:2)nite), such as the Epanechnikov kernel. Kernel
MaxEnt based on inde(cid:2)nite kernels will be studied in future work.
Acknowledgements
RJ is supported by NFR grant 171125/V30 and MG is supported by EPSRC grant EP/C010620/1.

References
[1] S. Roweis and L. Saul, (cid:147)Nonlinear Dimensionality Reduction by Locally Linear Embedding,(cid:148) Science,
vol. 290, pp. 2323(cid:150)2326, 2000.

 

 

%
 
e
c
n
a
m
r
o
f
r
e
P

100

80

60

40

20

0

 

%
 
e
c
n
a
m
r
o
f
r
e
P

100

80

60

40

20

0
 
1

KPCA
ME
NG

1.5

2

0.5

1
s

(a)

KPCA
ME
NG

2

3
s

4

(b)

 

100

80

60

40

20

%
 
e
c
n
a
m
r
o
f
r
e
P

0

 

0.5

1

1.5

s

(c)

KPCA
ME
NG

2

2.5

3

Figure 3: Clustering results.

[2] J. Tenenbaum, V. de Silva, and J. C. Langford, (cid:147)A Global Geometric Framework for Nonlinear Dimen-
sionality Reduction,(cid:148) Science, vol. 290, pp. 2319(cid:150)2323, 2000.
[3] B. Sch ¤olkopf, A. J. Smola, and K. R. M ¤uller, (cid:147)Nonlinear Component Analysis as a Kernel Eigenvalue
Problem,(cid:148) Neural Computation, vol. 10, pp. 1299(cid:150)1319, 1998.
[4] J. Shawe-Taylor and N. Cristianini, Kernel Methods for Pattern Analysis, Cambridge University Press,
2004.
[5] R. Jenssen, D. Erdogmus, J. C. Principe, and T. Eltoft, (cid:147)The Laplacian PDF Distance: A Cost Function
for Clustering in a Kernel Feature Space,(cid:148) in Advances in Neural Information Processing Systems 17,
MIT Press, Cambridge, 2005, pp. 625(cid:150)632.
[6] R. Jenssen, D. Erdogmus, J. C. Principe, and T. Eltoft, (cid:147)Some Equivalences between Kernel Methods and
Information Theoretic Methods,(cid:148) Journal of VLSI Signal Processing, to appear, 2006.
[7] R. Jenssen, D. Erdogmus, J. C. Principe, and T. Eltoft, (cid:147)Information Theoretic Angle-Based Spectral
Clustering: A Theoretical Analysis and an Algorithm,(cid:148) in Proceedings of International Joint Conference
on Neural Networks, Vancouver, Canada, July 16-21, 2006, pp. 4904(cid:150)4911.
[8] M. Girolami, (cid:147)Orthogonal Series Density Estimation and the Kernel Eigenvalue Problem,(cid:148) Neural Com-
putation, vol. 14, no. 3, pp. 669(cid:150)688, 2002.
[9] A. Renyi, (cid:147)On Measures of Entropy and Information,(cid:148) Selected Papers of Alfred Renyi, Akademiai Kiado,
Budapest, vol. 2, pp. 565(cid:150)580, 1976.
[10] E. Parzen, (cid:147)On the Estimation of a Probability Density Function and the Mode,(cid:148) The Annals of Mathe-
matical Statistics, vol. 32, pp. 1065(cid:150)1076, 1962.
[11] A. Y. Ng, M. Jordan, and Y. Weiss, (cid:147)On Spectral Clustering: Analysis and an Algorithm,(cid:148) in Advances
in Neural Information Processing Systems, 14, MIT Press, Cambridge, 2002, pp. 849(cid:150)856.
[12] B. W. Silverman, Density Estimation for Statistics and Data Analysis, Chapman and Hall, London, 1986.
[13] G. R ¤aetsch, T. Onoda, and K. R. M ¤uller, (cid:147)Soft Margins for Adaboost,(cid:148) Machine Learning, vol. 42, pp.
287(cid:150)320, 2001.


Implicit Surfaces with Globally Regularised and
Compactly Supported Basis Functions

Christian Walder†? , Bernhard Sch ¨olkopf† & Olivier Chapelle†
† Max Planck Institute for Biological Cybernetics, 72076 T ¨ubingen, Germany
? The University of Queensland, Brisbane, Queensland 4072, Australia
first.last@tuebingen.mpg.de

Abstract

We consider the problem of constructing a function whose zero set is to represent
a surface, given sample points with surface normal vectors. The contributions
include a novel means of regularising multi-scale compactly supported basis
functions that leads to the desirable properties previously only associated with
fully supported bases, and show equivalence to a Gaussian process with modiﬁed
covariance function. We also provide a regularisation framework for simpler
and more direct
treatment of surface normals, along with a corresponding
generalisation of the representer theorem. We demonstrate the techniques on 3D
problems of up to 14 million data points, as well as 4D time series data.

1

Introduction

The problem of reconstructing a surface from a set of points frequently arises in computer graphics.
Numerous methods of sampling physical surfaces are now available, including laser scanners,
optical triangulation systems and mechanical probing methods. Inferring a surface from millions
of points sampled with noise is a non-trivial task however, for which a variety of methods have been
proposed. The class of implicit or level set surface representations is a rather large one, however
other methods have also been suggested – for a review see [1]. The implicit surface methods closest
to the present work are those that construct the implicit using regularised function approximation [2],
such as the “Variational Implicits” of Turk and O’Brien [3], which produce excellent results, but at a
cubic computational ﬁtting cost in the number of points. The effectiveness of this type of approach
is undisputed however, and has led researchers to look for ways to overcome the computational
problems. Two main options have emerged.
The ﬁrst approach uses compactly supported kernel functions (we deﬁne and discuss kernel
functions in Section 2), leading to fast algorithms that are easy to implement [4]. Unfortunately
however these methods are suitable for benign data sets only. As noted in [5], compactly supported
basis functions “yield surfaces with many undesirable artifacts in addition to the lack of extrapolation
across holes”. A similar conclusion was reached in [6] which states that local processing methods
are “more sensitive to the quality of input data [than] approximation and interpolation techniques
based on globally-supported radial basis functions” – a conclusion corroborated by the results within
a different paper from the same group [7]. The second means of overcoming the aforementioned
computational problem does not suffer from these problems however, as demonstrated by the
FastRBFTM algorithm [5], which uses the the Fast Multipole Method (FMM) [8] to overcome the
computational problems of non-compactly supported kernels. The resulting method is non-trivial to
implement however and to date exists only in the proprietary FastRBFTMpackage.
We believe that by applying them in a different manner, compactly supported basis functions can
lead to high quality results, and the present work is an attempt to bring the reader to the same
conclusion. In Section 3 we introduce a new technique for regularising such basis functions which

Figure 1: (a) Rendered implicit
surface model
“Lucy”,
of
constructed from 14 million
(b) A
points with normals.
planar slice that cuts the nose –
the colour represents the value
of the embedding function and
the black line its zero level.
(c) A black dot at each of the
364,982 compactly supported
basis function centres which,
along with the corresponding
dilations
and magnitudes,
deﬁne the implicit.

(a)

(b)

(c)

allows high quality, highly scalable algorithms that are relatively easy to implement. We also
show that the approximation can be interpreted as a Gaussian process with modiﬁed covariance
function. Before doing so however, we present in Section 2 the other main contribution of the
present work, which is to show how surface normal vectors can be incorporated directly into
the regularised regression framework that is typically used for ﬁtting implicit surfaces, thereby
avoiding the problematic approach of constructing “off-surface” points for the regression problem.
To demonstrate the effectiveness of the method we apply it to various problems in Section 4 before
summarising in the ﬁnal Section 5.

2

Implicit Surface Fitting by Regularised Regression

Here we discuss the use of regularised regression [2] for the problem of implicit surface ﬁtting. In
Section 2.1 we motivate and introduce a clean and direct means of making use of normal vectors.
The following Section 2.2 extends on the ideas in Section 2.1 by formally generalising the important
representer theorem. The ﬁnal Section 2.3 discusses the choice of regulariser (and associated kernel
function), as well as the associated computational problems that we overcome in Section 3.

arg min
f

(1)

(2)

kf k2H + C

(f (xi ) − yi )2 ,

2.1 Regression Based Approaches and the Use of Normal Vectors
mX
Typically implicit surface has been done by solving a regularised regression problem [5, 4]
i=1
where the yi are some estimate of the signed distance function at the xi , and f is the embedding
function which takes on the value zero on the implicit surface. The norm kf kH is a regulariser
which takes on larger values for less “smooth” functions. We take H to be a reproducing kernel
Hilbert space (RKHS) with representer of evaluation (kernel function) k(·, ·), so that we have the
reproducing property, f (x) = hf , k(x, ·)iH . The solution to this problem has the form
mX
i
Note as a technical aside that the thin-plate kernel case – which we will adopt – requires a somewhat
more technical interpretatiosn, as it is only conditionally positive deﬁnite. We discuss the positive
deﬁnite case for clarity only, as it is simpler and yet sufﬁcient to demonstrate the ideas involved.
Choosing the (xi , yi ) pairs for (2) is itself a non-trivial problem, and heuristics are typically used
to prevent contradictory target values (see e.g. [5]). We now propose more direct method, novel in
the context of implicit ﬁtting, which avoids these problems. The approach is suggested by the fact
that the normal direction of the implicit surface is given by the gradient of the embedding function

f (x) =

αik (xi , x) .

(3)

(f (xi ))2 + C2

k(∇f ) (xi ) − ni k2Rd ,

– thus normal vectors can be incorporated by regression with gradient targets. The function that we
mX
mX
seek is the minimiser of:
kf k2H + C1
i=1
i=1
which uses the given surface point/normal pairs (xi , ni ) directly. By imposing stationarity and using
the reproducing property we can solve for the optimal f . A detailed derivation of this procedure is
given in [1]. Here we provide only the result, which is that we have to solve for m coefﬁcients αi as
mX
dX
mX
well as a further md coefﬁcients βlj to obtain the optimal solution
f (x) =
αik (xi , x) +
βlikl (xi , x) ,
(4)
i
i
l
where we deﬁne kl (xi , x) .= [(∇k) (xi , x)]l , the partial derivative of k in the l-th component of its
0 = (K + I /C1 )α + X
ﬁrst argument.1 The coefﬁcients α and βl of the solution are found by solving the system given by
Nm = Kmα + (Kmm + I /C2 )βk + X
(5)
Klβl
l
l 6=m
where, writing klm for the second derivatives of k(·, ·) (deﬁned similarly to the ﬁrst), we’ve deﬁned
[α]i = αi
[Nl ]i = [ni ]l ;
[K ]i,j = k(xi , xj )
[βl ]i = βli ;
[Kl ]i,j = kl (xi , xj ) ;
[Klm ]i,j = klm (xi , xj ).
In summary, minimum norm approximation in an RKHS with gradient target values is optimally
solved by a function in the span of the kernels and derivatives thereof as per Equation 4 (cf. Equation
2), and the coefﬁcients of the solution are given by Equations (5) and (6). It turns out, however, that
we can make a more general statement, which we do brieﬂy in the next sub-Section.

Klmβl , m = 1 . . . d

(6)

2.2 The Representer Theorem with Linear Operators

The representer theorem, much celebrated in the Machine Learning community, says that the
function minimising an RKHS norm along with some penalties associated with the function value
at various points (as in Equation 1 for example) is a sum of kernel functions at those points (as in
Equation 2). As we saw in the previous section however, if gradients also appear in the risk function
to be minimised, then gradients of the kernel function appear in the optimal solution. We now
make a more general statement – the case in the previous section corresponds to the following if we
choose the linear operators Li (which we deﬁne shortly) as either identities or partial derivatives.
The theorem is a generalisation of [9] (using the same proof idea) with equivalence if we choose all
Li to be identity operators. The case of general linear operators was in fact dealt with already in [2]
(which merely states the earlier result in [10]) – but only for the case of a speciﬁc loss function c.
The following theorem therefore combines the two frameworks:
Theorem 1
Denote by X a non-empty set, by k a reproducing kernel with reproducing kernel Hilbert space H,
by Ω a strictly monotonic increasing real-valued function on [0, ∞), by c : Rm → R ∪ {∞} an
arbitrary cost function, and by L1 , . . . Lm a set of linear operators H → H. Each minimiser f ∈ H
of the regularised risk functional
c((L1 f )(x1 ), . . . (Lm f )(xm )) + Ω(||f ||2H )
mX
i=1
where kx , k(·, x) and L∗
i denotes the adjoint of Li .
1Square brackets with subscripts indicate matrix elements: [a]i is the i-th element of the vector a.

admits the form

αiL∗
i kxi ,

f =

(7)

(8)

=

Proof. Decompose f into f = Pm
i kxi + f⊥ , with αi ∈ R and hf⊥ , L∗
i kxi iH = 0, for each
i=1 αiL∗
i = 1 . . . m. Due to the reproducing property we can write, for j = 1 . . . m,
mX
(Lj f )(xj ) = h(Lj f ), k(·, xj )iH
αi hLj L∗
i kxi , k(·, xj )iH + h(Lj f⊥ ), k(·, xj )iH
mX
i=1
i=1

αi hLj L∗
i kxi , k(·, xj )iH .
=
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) mX
 > Ω
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) mX
 ,
Thus, the ﬁrst term in Equation 7 is independent of f⊥ . Moreover, it is clear due to orthogonality
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
that if f⊥ 6= 0 then
H
H
i=1
i=1
so that for any ﬁxed αi ∈ R, Equation 7 is minimised when f⊥ = 0.
2.3 Thin Plate Regulariser and Associated Kernel

αiL∗
i kxi + f⊥

αiL∗
i kxi

Ω

=

· · ·

f

(9)

· · ·

f

∂
∂xim

As is well known (see e.g. [2]), the choice of regulariser (the function norm in Equation 3) leads to
a particular kernel function k(·, ·) to be used in Equation 4. For geometrical problems, an excellent
regulariser is the thin-plate energy, which for arbitrary order m and dimension d is given by [2]:
«
« „ ∂
„ ∂
Z ∞
Z ∞
kf k2H = hψf , ψf iL2
dX
dX
· · ·
· · ·
∂xi1
∂xi1
xd=−∞
x1=−∞
i1=1
im=1
where ψ is a regularisation operator taking all partial derivatives of order m, which corresponds to
(cid:26)r2m−d ln(r)
a “radial” kernel function of the form k(x, y) = t(||x − y ||), where [11]
if 2m > d and d is even,
r2m−d
otherwise.
There are a number of good reasons to use this regulariser rather than those leading to compactly
supported kernels, as we touched on in the introduction. The main problem with compactly
supported kernels is that the corresponding regularisers are somewhat poor for geometrical problems
– they always draw the function towards some nominal constant as one moves away from the data,
thereby implementing the non-intuitive behaviour of regularising the constant function and making
interpolation impossible – for further discussion see [1] as well as [5, 6, 7]. The scheme we propose
in Section 3 solves these problems, previously associated with compactly supported basis functions,
by deﬁning and computing the regulariser separately from the function basis.

dx1 . . . dxd , (10)

∂
∂xim

t(r) =

3 A Fast Scheme using Compactly Supported Basis Functions

Here we present a fast approximate scheme for solving the problem of the previous Section, in
which we restrict the class of functions to the span of a compactly supported, multi-scale basis, as
described in Section 3.1, and minimise the thin-plate regulariser within this span as per Section 3.2.

3.1 Restricting the Set of Available Functions

Computationally, using the thin-plate spline leads to the problem that the linear system we need to
solve (Equations 5 and 6), which is of size m(d + 1), is dense in the sense of having almost all
non-zero entries. Since solving such a system na¨ıvely has a cubic time complexity in m, we propose
pX
forcing f (·) to take the form:
k=1

πk fk (·),

f (·) =

(11)

(12)

φ(r) =

− n))d
+ ,

(r + ( d + 1
2

where the individual basis functions are fk (·) = φ(||vk −·||/sk ) for some function φ : R+ → R with
support [0, 1). The vk and sk are the basis function centres and dilations (or scales), respectively.
(cid:18) n
(cid:19)
4X
For φ we choose the B3 -spline function:
(−1)n
d!
d + 1
n=0
although this choice is rather inconsequential since, as we shall ensure, the regulariser is unrelated
to the function basis – any smooth compactly supported basis function could be used.
In order
to achieve the same interpolating properties as the thin-plate spline, we wish to minimise our
regularised risk function given by Equation 3 within the span of Equation 11. The key to doing
this is to note that as given before in Equation 9, the regulariser (function norm) can be written as
kf k2H = hψf , ψf iL2 . Given this fact, a straightforward calculation leads to the following system
 
!
dX
dX
for the optimal πk (in the sense of minimising Equation 3):
Kreg + C1K T
xvKxv + C2
l=1
l=1
where we have deﬁned the following matrices:
[Kreg ]k,k0 = hψfk , ψfk0 iL2 ;
[Kxvl ]i,k = [(∇fk )(xi )]l ;
[Kxv ]i,k = fk (xi );
[Nl ]i = [ni ]l .
[π ]k = πk ;
The computational advantage is that the coefﬁcients that we need are now given by a sparse p-
dimensional positive semi-deﬁnite linear system, which can be constructed efﬁciently by simple
code that takes advantage of software libraries for fast nearest neighbour type searches (see e.g.
[12]). The system can then be solved efﬁciently using conjugate gradient type methods. In [1] we
describe how we construct a basis with p (cid:28) m that results in a highly sparse linear system, but that
still contains good solutions. The critical matter of computing Kreg is dealt with next.

K T
xvlKxvl

π = C2

KxvlNl ,

(13)

3.2 Computing the Regularisation Matrix

=

πj fj (·), ψ

We now come to the crucial point of calculating Kreg , which can be thought of as the regularisation
matrix. The present Section is highly related to [13], however there numerical methods were
resorted to for the calculation of Kreg – presently we shall derive closed form solutions. Also worth
comparing to the present Section is [14], where a prior over the expansion coefﬁcients (here the
π ) is used to mimic a given regulariser within an arbitrary basis, achieving a similar result but
without the computational advantages we are aiming for. As we have already noted we can write
kf k2H = hψf , ψf iL2 [2], so that for the function given by Equation 11 we have:
‚‚‚‚‚ pX
‚‚‚‚‚2
+
*
pX
pX
πj fj (·)
πk fk (·)
ψ
pX
H
j=1
j=1
k=1
πj πk hψfj (·), ψfk (·)iL2
j,k=1
To build the sparse matrix Kreg , a fast range search library (e.g. [12]) can be used to identify the
non-zero entries – that is, all those [Kreg ]i,j for which i and j satisfy kvi − vj k ≤ si + sj . In order
to evaluate hψfj (·), ψfk (·)iL2
, it is necessary to solve the integral of Equation 10, the full derivation
of which we relegate to [1] – here we just provide the main results. It turns out that since the fi are
all dilations and translations of the same function φ(k·k), then it is sufﬁcient solve for the following
.= vi − vj :
function of si , sj and d
hψφ((·)si − d), ψφ((·)sj )iL2
,
#
"
)

which it turns out is given by

.
= π TKregπ .

(d),

(14)

=

L2

F −1
ω

(2πj kωk)2m
|s1 s2 |

Φ( ω
s1

)Φ( ω
s2

Figure 2: Various values of
the regularisation
Figure 3: Ray traced three dimensional
parameters lead to various amounts of “smoothing” –
implicits, “Happy Buddha” (543K points
here we set C1 = C2 in Equation 3 to an increasing
with normals) and the “Thai Statue” (5
value from top-left to bottom-right of the ﬁgure.
million points with normals).
where j 2 = −1, Φ = Fx [φ(x)], and by F (and F −1 ) we mean the Fourier (inverse Fourier)
transform operators in the subscripted variable. Computing Fourier transforms in d dimensions
difﬁcult in general, but for radial functions g(x) = gr (||x||) it may be made easier by the fact that
Z ∞
the Fourier transform in d dimensions (as well as its inverse) can be computed by the single integral:
(2π) d
Fx [gr (kxk)] (kωk) =
(||ω ||r)dr,
2
||ω || d−2
2
0
where Jν (r)is the ν -th order Bessel function of the ﬁrst kind.
Unfortunately the integrals required to attain Equation 14 in closed form cannot be solved for general
dimensionality d, regularisation operator ψ and basis function form φ, however we did manage to
solve them for arguably the most useful case: d = 3 with the m = 2 thin plate energy and the B3 -
spline basis function of Equation 12. The resulting expressions are rather unwieldy however, so we
give only an implementation in the C language in the Appendix of [1], where we also show that for
the cases that cannot be solved analytically the required integral can at worst always be transformed
to a two dimensional integral for which one can use numerical methods.

d
2 gr (r)J d−2
2

r

3.3

Interpretation as a Gaussian Process

Presently we use ideas from [15] to demonstrate that the approximation described in this Section 3
is equivalent to inference in an exact Gaussian Process with covariance function depending on the
choice of function basis. Placing a multivariate Gaussian prior over the coefﬁcients in (11), namely
π ∼ N (0, K −1
reg ), we see that f obeys a zero mean Gaussian process prior – writing [fx ]i = f (xi )
and denoting expectations by E [·] we have for the covariance
x ] = Kxz E [ππ T ] K T
E [fx f T
xz
= KxzK −1
reg K T
xz
yt ∼ N (cid:0)f (t), σ2 (cid:1) and the vector of observations at the xi , namely yx ∼ N (cid:0)fx , σ2 I (cid:1), which is
Now, assuming an iid Gaussian noise model with variance σ2 and deﬁning Kxt etc. similarly to
(cid:18) (cid:0)KxzK −1
reg Kzx + σ2 I (cid:1)
reg Kzt + σ2 I (cid:1) (cid:19)(cid:19)
(cid:18)
Kxz we can immediately write the joint distribution between the observation at a test point t, that is
(cid:0)KtzK −1
KxzK −1
(cid:1), and we
The posterior distribution is therefore itself Gaussian, p(yt |yx ) ∼ N (cid:0)µyt |yx , Σyt |yx
p(yx , yt ) = N
reg Kzt
0,
KtzK −1
.
reg Kzx
µt|y = (cid:0)KxzK −1
(cid:1)T (cid:0)KxzK −1
reg Kzx + σ2 I (cid:1)−1
can employ a well known expression2 for the marginals of a multivariate Gaussian followed by the
Matrix inversion lemma to derive an expression for the mean of the posterior,
(cid:0)σ2Kreg + K T
(cid:1)−1
reg Kzt
y
«««
„ A C
«
„„ a
«
„„ x
⇒ `x|y ∼ N `a + CB−1 (y − b), A − CB−1C T ´´
= Ktz
K T
xz y .
xzKxz
C T B
y
b

∼ N

2

,

Name
Bunny
Face
Armadillo
Dragon
Buddha
Asian Dragon
Thai Statue
Lucy

# Points
34834
75970
172974
437645
543197
3609455
4999996
14027872

# Bases
9283
7593
45704
65288
105993
232197
530966
364982

Basis
0.4
0.7
6.6
14.4
117.4
441.6
3742.0
1425.8

Kreg Kxv , Kzv∇ Multiply
2.4
3.7
11.7
20.3
7.0
1.9
123.4
37.0
8.5
322.8
70.9
16.3
423.7
99.4
27.4
60.9
608.3
1885.0
3121.2
1575.6
197.5
170.5
3484.1
9367.7

Solve
20.4
16.0
72.3
1381.4
2909.3
1009.5
2569.5
1340.5

Total
38.7
46.0
247.9
1805.7
3577.2
4005.2
11205.7
15788.5

Table 1: Timing results with a 2.4GHz AMD Opteron 850 processor, for various 3D data sets.
Column one is the number of points, each of which has an associated normal vector, and column
two is the number of basis vectors (the p of Section 3.1). The remaining columns are all in units of
seconds: column three is the time taken to construct the function basis, columns four and ﬁve are
the times required to construct the indicated matrices, column six is the time required to multiply
the matrices as per Equation 13, column seven is the time required to solve that same equation for
π and the ﬁnal column is the total ﬁtting time.

By comparison with (11) and (13) (but with C1 = 1/σ2 , C2 = 0 and y = 0) we can see that
reg Kzx + σ2 I (cid:1)−1 (cid:0)KxzK −1
(cid:1) (cid:0)KxzK −1
(cid:1)
reg Kzt + σ2 (cid:1) − (cid:0)KtzK −1
Σyt |yx = (cid:0)KtzK −1
the mean of the posterior distribution is identical to our approximate regularised solution based on
compactly supported basis functions. For the corresponding posterior variance we have
(cid:0)σ2Kreg + K T
(cid:1)−1
reg Kzt
reg Kzx
Kzt + σ2 .
= σ2Ktz
xzKxz
4 Experiments

We ﬁt models to 3D data sets of up to 14 million data points – timings are given in Table 1, where
we also see that good compression ratios are attained, in that relatively few basis functions represent
the shapes. Also note that the ﬁtting time scales rather well, from 38 seconds for the Stanford Bunny
(35 thousand points with normals) to 4 hours 23 minutes for the Lucy statue (14 million points with
normals ≈ 14× 106 × (1 value term + 3 gradient terms ) ≈ 56 million “regression targets”). Taking
account of the different hardware the times seem to be similar to those of the FMM approach [5].
Some rendered examples are given in Figures 1 and 3, and the well-behaved nature of the implicit
over the entire 3D volume of interest is shown for the Lucy data-set in the accompanying video.
In practice the system is extremely robust and produces excellent results without any parameter
adjustment – smaller values of C1 and C2 in Equation 3 simply lead to the smoothing effect shown
in Figure 2. The system also handles missing and noisy data gracefully, as demonstrated in [1].
Higher dimensional implicit surfaces are also possible, interesting being a 4D representation (3D
+ “time”) of a moving 3D shape – one use for this being the construction of animation sequences
from a time series of 3D point cloud data – in this case both spatial and temporal information
can help to resolve noise or missing data problems within individual scans. We demonstrate this
in the accompanying video, which shows that 4D surfaces yield superior 3D animation results
in comparison to a sequence of 3D models. Also interesting are interpolations in 4D – in the
accompanying video we effectively interpolate between two three dimensional shapes.
5 Summary

We have presented ideas both theoretically and practically useful for the computer graphics and
machine learning communities, demonstrating them within the framework of implicit surface ﬁtting.
Many authors have demonstrated fast but limited quality results that occur with compactly supported
function bases. The present work differs by precisely minimising a well justiﬁed regulariser within
the span of such a basis, achieving fast and high quality results. We also showed how normal vectors
can be incorporated directly into the usual regression based implicit surface ﬁtting framework, giving
a generalisation of the representer theorem. We demonstrated the algorithm on 3D problems of up
to 14 million data points and in the accompanying video we showed the advantage of constructing a
4D surface (3D + time) for 3D animation, rather than a sequence of 3D surfaces.

Figure 4: Reconstruction of the Stanford bunny after adding Gaussian noise with standard deviation,
from left to right, 0, 0.6, 1.5 and 3.6 percent of the radius of the smallest enclosing sphere –
the normal vectors were similarly corrupted assuming they had length equal to this radius. The
parameters C1 and C2 were chosen automatically using ﬁve-fold cross validation.

In

References
[1] C. Walder, B. Sch ¨olkopf, and O. Chapelle. Implicit surface modelling with a globally regularised basis
of compact support. Technical report, Max Planck Institute for Biological Cybernetics, Department of
Empirical Inference, Tbingen, Germany, April 2006.
[2] G. Wahba. Spline Models for Observational Data. Series in Applied Mathematics, Vol. 59, SIAM,
Philadelphia, 1990.
[3] Greg Turk and James F. O’Brien.
Shape transformation using variational implicit functions.
Proceedings of ACM SIGGRAPH 1999, pages 335–342, August 1999.
[4] Bryan S. Morse, Terry S. Yoo, David T. Chen, Penny Rheingans, and K. R. Subramanian. Interpolating
implicit surfaces from scattered surface data using compactly supported radial basis functions. In SMI
’01: Proc. Intl. Conf. on Shape Modeling & Applications, Washington, 2001. IEEE Computer Society.
[5] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans.
Reconstruction and representation of 3d objects with radial basis functions. In ACM SIGGRAPH 2001,
pages 67–76. ACM Press, 2001.
[6] Yutaka Ohtake, Alexander Belyaev, Marc Alexa, Greg Turk, and Hans-Peter Seidel. Multi-level partition
of unity implicits. ACM Transactions on Graphics, 22(3):463–470, July 2003.
[7] Y. Ohtake, A. Belyaev, and Hans-Peter Seidel. A multi-scale approach to 3d scattered data interpolation
with compactly supported basis functions. In Proc. Intl. Conf. Shape Modeling, Washington, 2003. IEEE
Computer Society.
[8] L. Greengard and V. Rokhlin. A fast algorithm for particle simulations. J. Comp. Phys., pages 280–292,
1997.
[9] Bernhard Sch ¨olkopf, Ralf Herbrich, and Alex J. Smola. A generalized representer theorem. In COLT
’01/EuroCOLT ’01: Proceedings of the 14th Annual Conference on Computational Learning Theory,
pages 416–426, London, UK, 2001. Springer-Verlag.
[10] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of Mathematical
Analysis and Applications, 33:82–95, 1971.
[11] J. Duchon. Splines minimizing rotation-invariant semi-norms in sobolev spaces. Constructive Theory of
Functions of Several Variables, pages 85–100, 1977.
[12] C. Merkwirth, U. Parlitz, and W. Lauterborn. Fast nearest neighbor searching for nonlinear signal
processing. Phys. Rev. E, 62(2):2089–2097, 2000.
[13] Christian Walder, Olivier Chapelle, and Bernhard Sch ¨olkopf. Implicit surface modelling as an eigenvalue
problem. Proceedings of the 22nd International Conference on Machine Learning, 2005.
[14] M. O. Franz and P. V. Gehler. How to choose the covariance for gaussian process regression independently
of the basis. In Proc. Gaussian Processes in Practice Workshop, 2006.
[15] J. Quionero Candela and C. E. Rasmussen. A unifying view of sparse approximate gaussian process
regression. Journal of Machine Learning Research, 6:1935–1959, 12 2005.


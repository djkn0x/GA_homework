Learning Motion Style Synthesis
from Perceptual Observations

Lorenzo Torresani
Riya, Inc.
lorenzo@riya.com

Peggy Hackney
Integrated Movement Studies
pjhackney@aol.com

Christoph Bregler
New York University
chris.bregler@nyu.edu

Abstract

This paper presents an algorithm for synthesis of human motion in speci ﬁed styles.
We use a theory of movement observation (Laban Movement Analysis) to describe
movement styles as points in a multi-dimensional perceptual space. We cast the
task of learning to synthesize desired movement styles as a regression problem:
sequences generated via space-time interpolation of motion capture data are used
to learn a nonlinear mapping between animation parameters and movement styles
in perceptual space. We demonstrate that the learned model can apply a variety of
motion styles to pre-recorded motion sequences and it can extrapolate styles not
originally included in the training data.

1

Introduction

Human motion perception can be generally thought of as the result of interaction of two factors,
traditionally termed content and style. Content generally refers to the nature of the action in the
movement (e.g. walking, reaching, etc.), while style denotes the particular way that action is per-
formed. In computer animation, the separation of the underlying content of a movement from its
stylistic characteristics is particularly important. For example, a system that can synthesize stylistic
variations of a given action would be a useful tool for animators. In this work we address such a
problem by proposing a system that applies user-speci ﬁed st yles to motion sequences. Speci ﬁcally,
given as input a target motion style and an arbitrary animation or pre-recorded motion, we want to
synthesize a novel sequence that preserves the content of the original input motion but exhibits style
similar to the user-speci ﬁed target.

Our approach is inspired by two classes of methods that have successfully emerged within the genre
of data-driven animation: sample-based concatenation methods, and techniques based on learned
parametric models. Concatenative synthesis techniques [15, 1, 11] are based on the simple idea of
generating novel movements by concatenation of motion capture snippets. Since motion is produced
by cutting and pasting pre-recorded examples, the resulting animations achieve realism similar to
that of pure motion-capture play back. Snippet concatenation can produce novel content by gener-
ating arbitrarily complex new movements. However, this approach is restricted to synthesize only
the subset of styles originally contained in the input database. Sample-based concatenation tech-
niques are unable to produce novel stylistic variations and cannot generalize style differences from
the existing examples. In recent years, several machine learning animation systems [2, 12, 9] have
been proposed that attempt to overcome some of these limitations. Unfortunately, most of these
methods learn simple parametric motion models that are unable to fully capture the subtleties and
complexities of human movement. As a consequence, animations resulting from these systems are
often plagued by low quality and scarce realism.

The technique introduced in this paper is a compromise between the pure concatenative approaches
and the methods based on learned parametric models. The aim is to maintain the animated preci-
sion of motion capture data, while introducing the ﬂexibili
ty of style changes achievable by learned

parametric models. Our system builds on the observation that stylistically novel, yet highly realistic
animations can be generated via space-time interpolation of pairs of motion sequences. We propose
to learn not a parametric function of the motion, but rather a parametric function of how the interpo-
lation or extrapolation weights applied to data snippets relate to the styles of the output sequences.
This allows us to create motions with arbitrary styles without compromising animation quality.

Several researchers have previously proposed the use of motion interpolation for synthesis of novel
movement [18, 6, 10]. These approaches are based on the na¨ıve assumption that motion interpolation
produces styles corresponding precisely to the interpolation of the styles of the original sequences.
In this paper we experimentally demonstrate that styles generated through motion interpolation are
a rather complex function of styles and contents of the original snippets. We propose to explicitly
learn the mapping between motion blending parameters and resulting animation styles. This enables
our animation system not only to generate arbitrary stylistic variations of a given action, but, more
importantly, to synthesize sequences matching user-speci ﬁed stylistic characteristics. Our approach
bears similarities with the Verbs and Adverbs work of Rose et al. [16], in which interpolation models
parameterized by style attributes are learned for several actions, such as walking or reaching. Unlike
this previously proposed algorithm, our solution can automatically identify sequences having similar
content, and therefore does not require manual categorization of motions into classes of actions. This
feature allows our algorithm to be used for style editing of sequences without content speci ﬁcation
by the user. Additionally, while the Verb and Adverb system characterizes motion styles in terms of
difﬁcult-to-measure emotional attributes, such as sad or c lueless, our approach relies on a theory of
movement observation, Laban Movement Analysis, describing styles by means of a set of rigorously
deﬁned perceptual attributes.

2 The LMA Framework

In computer animation literature motion style is a vaguely deﬁned concept. In our work, we describe
motion styles according to a movement notation system, called Laban Movement Analysis or LMA
[7]. We focus on a subset of Laban Movement Analysis: the ”LMA -Effort ” dimensions. This system
does not attempt to describe the coarse aspects of a motion, e.g. whether someone is walking, or
swinging his/her arm. Instead, it targets the subtle differences in motion style, e.g. is the movement
”bound” or ”free”? Each LMA-Effort factor varies in intensi
ty between opposing poles, and takes
values in a continuous range. The factors are brieﬂy describ ed as follows:

he movement. The two oppos-
1. The ”LMA-Effort Factor of Flow” deﬁnes the continuity of t
ing poles are ”Free” ( ﬂuid, released), and ”Bound” (control
led, contained, restrained).
2. The ”LMA-Effort Factor of Weight ” is about the relationsh ip of the movement to gravity.
The two opposing extremes are ”Light ” (gentle, delicate, ﬁn
e touch) and ”Strong” (power-
ful, forceful, ﬁrm touch).
3. The ”LMA-Effort Factor of Time” has to do with the persons i nner attitude toward the time
available, but not with how long it takes to perform the movement. The two opposing poles
are ”Sudden” (urgent, quick) and ”Sustained” (stretching t
he time, indulging).
4. The ”LMA-Effort Factor of Space” describes the directnes
s of the movement. Generally,
additional features not present in motion capture data, such as eye gaze, are necessary to
detect this factor.

We use only the LMA-Effort factors of Flow, Weight, and Time. We model styles as points in
a three-dimensional perceptual space derived by translating the LMA-Effort notations for each of
these factors into numerical values ranging in the interval [−3, 3].

3 Overview of the system

The key-idea of our work is to learn motion style synthesis from a training set of computer-generated
animations. The training animations are observed by a human expert who assigns LMA labels to
each sequence. This set of supervised data is used to learn a mapping between the space of motion
styles and the animation system parameters. We next provide a high-level description of our system,
while the following sections give speci ﬁc details of each co mponent.

3.1 Training: Learning the Style of Motion Interpolation

In order to train our system to synthesize motion styles, we employ a corpus of human motion
sequences recorded with a motion capture system. We represent the motion as a time-varying vector
of joint angles.
In the training stage each motion sequence is manually segmented by an LMA
human expert into fragments corresponding to fundamental actions or units of motions. Let Xi
denote the joint angle data of the i-th fragment in the database.

Step 1: Matching motion content. We apply a motion matching algorithm to identify fragment
pairs (Xi , Xj ) containing similar actions. Our motion matching algorithm is based on dynamic-time
warping. This allows us to compare kinematic contents while factoring out differences in timing or
acceleration, more often associated to variations in style.

Step 2: Space-time interpolation. We use these motion matches to augment the database with
new synthetically-generated styles: given matching motion fragments Xi , Xj , and an interpolation
parameter α, space-time interpolation smoothly blends the kinematics and dynamics of the two
fragments to produce a new motion Xα
i,j with novel distinct style and timing.

Step 3: Style interpolation learning. Both the synthesized animations Xα
i,j as well as the ”seed”
i,j de-
motion capture data Xi are labeled with LMA-Effort values by an LMA expert. Let ei and eα
i,j , respectively.
note the three-dimensional vectors encoding the LMA-Effort qualities of Xi and Xα
A non-linear regression model [5] is ﬁtted to the LMA labels a nd the parameters α of the space-time
interpolation algorithm. This regression deﬁnes a functio n f predicting LMA-Effort factors eα
i,j
from the style attributes and joint angle data of fragments i and j :

eα
i,j = f (Xi , Xj , ei , ej , α)

(1)

This function- ﬁtting stage allows us to learn how the knobs o f our animation system relate to the
perceptual space of movement styles.

3.2 Testing: Style Transfer

At testing stage we are given a motion sequence Y , and a user-speci ﬁed motion style ¯e. The goal
is to apply style ¯e to the input sequence Y , without modifying the content of the motion. First, we
use dynamic-time warping to segment the input sequence into snippets Yi , such that each snippet
matches the content of a set of analogous motions {Xi1 , ..., XiK } in the database. Among all
of examples in the set {Xi1 , ..., XiK }, we determine the one that
possible pairwise blends Xα
ik ,il
provides the best approximation to the target style ¯e. This objective can be formulated as

α∗ , k∗ , l∗ ← arg min
α,k,l

||¯e − f (Xik , Xil , eik , eil , α)||

(2)

The animation resulting from space-time interpolation of fragments Xik∗ and Xil∗ with parameter
α∗ will exhibit content similar to that of snippet Yi and style approximating the target ¯e. Concate-
nating these arti ﬁcially-generated snippets will produce the desired output.

4 Matching motion content

The objective of the matching algorithm is to identify pairs of sequences having similar motion
content or consisting of analogous activities. The method should ignore variations in the style with
which movements are performed. Previous work [2, 12] has shown that the differences in movement
styles can be found by examining the parameters of timing and movement acceleration. By contrast,
an action is primarily characterized by changes of body conﬁ gurations in space rather than over
time. Thus we compare the content of two motions by identifying similar spatial body poses while
allowing for potentially large differences in timing. Speci ﬁcally, we deﬁne the content similarity
between motion snippets Xi and Xj , as the minimum sum of their squared joint angle differences
SSD(Xi , Xi ) under a dynamic time warping path. Let d(p, q) = ||Xi (p) − Xj (q)||2 be our local
measure of the distance between spatial body conﬁgurations Xi at frame p and Xj at frame q . Let Ti

be the number of frames in sequence i and L the variable length of a time path w(n) = (p(n), q(n))
aligning the two snippets. We can then formally deﬁne SSD(Xi , Xi ) as:

w X
SSD(Xi , Xi ) = min
n

d(w(n))

subject to constraints:

p(1) = 1, q(1) = 1, p(L) = Ti , q(L) = Tj
if w(n) = (p, q) then w(n − 1) ∈ {(p − 1, q), (p − 1, q − 1), (p, q − 1)}

We say that two motions i and j have similar content if SSD(Xi , Xi ) is below a certain value.

(3)

(4)
(5)

5 Space-time interpolation

A time warping strategy is also employed to synthesize novel animations from the pairs of content-
matching examples found by the algorithm outlined in the previous section. Given matching snippets
Xi and Xj , the objective is to generate a stylistically novel sequence that maintains the content of
the two original motions. The idea is to induce changes in style by acting primarily on the timings
of the motions. Let w∗ = (p∗ , q∗ ) be the path minimizing Equation 3. This path deﬁnes a time
alignment between the two sequences. We can interpret frame correspondences (p∗ (n), q∗ (n)) for
n = 1, ..., L, as discrete samples from a continuous 2D curve parameterized by n. Resampling Xi
and Xj along this curve will produce synchronized versions of the two animations, but with new
k ) = k . Then Xi (p∗ (n0
are chosen such that p∗ (n0
timings. Suppose parameter values n0
1 , ...., n0
k ))
Ti
will be replayed with its original timing. However, if we use these same parameter values on se-
k )) then the resampled motion will
quence Xj (i.e. we estimate joint angles Xj at time steps q∗ (n0
can be cho-
correspond to playing sequence j with the timing of sequence i. Similarly, n1
1 , ...., n1
Tj
sen, such that q∗ (n1
k ) = k , and these parameter values can be used to synthesize motion i with the
timing of motion j . It is also possible to smoothly interpolate between these two scenarios according
to an interpolation parameter α ∈ [0, 1] to produce intermediate time warps. This will result in a
time path of length T α
ij = (1 − α)Ti + αTj . Let us indicate with nα
the path parameter val-
1 , ...., nα
T α
ij
ues obtained from this time interpolation. New stylistic versions of motions i and j can be produced
by estimating the joint angles Xi and Xj at p∗ (nα
k ) and q∗ (nα
k ), respectively. The two resulting
sequences will move in synchrony according to the new intermediate timing. From these two syn-
chronized sequences, a novel motion Xα
i,j can be generated by averaging the joint angles according
to mixing coefﬁcients
(1 − α) and α: Xα
k )). The synthe-
i,j (k) = (1 − α)Xi (p∗ (nα
k )) + αXj (q∗ (nα
i,j will display content similar to that of Xi and Xj , but it will have distinct style.
sized motion Xα
We call this procedure ”space-time interpolation”, as it mo di ﬁes the spatial body conﬁgurations and
the timings of sequences.

6 Learning style interpolation

Given a pair of content-matching snippets Xi and Xj , our goal is to determine the parameter α that
i,j exhibiting target
needs to be applied to space-time interpolation in order to produce a motion Xα
style ¯e. We propose to solve this task by learning to predict the LMA-Effort qualities of animations
synthesized by space-time interpolation. The training data for this supervised learning task consists
i,j }, and the
of our seed motion sequences {Xi } in the database, a set of interpolated motions {Xα
i,j } observed by an LMA human expert. In order to
corresponding LMA-Effort qualities {ei }, {eα
maintain a consistent data size, we stretch or shrink the time trajectories of the joint angles {Xi }
to a set length. In order to avoid over ﬁtting, we compress fur ther the motion data by projecting
it onto a low-dimensional linear subspace computed using Principal Component Analysis (PCA).
In many of the test cases, we found it was sufﬁcient to retain o nly the ﬁrst two or three principal
components in order to obtain a discriminative representation of the motion contents. Let ci denote
the vector containing the PCA coefﬁcients computed from Xi . Let zα
j , α]T . We
i,j = [cT
i , cT
j , eT
i , eT
pose the task of predicting LMA-Effort qualities as a function approximation problem: the goal is to
i,j , θ) that models the dependencies
learn the optimal parameters θ of a parameterized function f (zα

between zα
i,j and the observed LMA-Effort values eα
i,j . Parameters θ are chosen so as to minimize
the objective function:

E (θ) = U X L(f (zα
i,j ) + ||θ ||2
i,j , θ) − eα
where L is a general loss function and U is a regularization constant aimed at avoiding over ﬁtting
and improving generalization. We experimented with several function parameterizations and loss
functions applied to our problem. The simplest of the adopted approaches is linear ridge regression
[4], which corresponds to choosing the loss function L to be quadratic (i.e. L(.) = (.)2 ) and f to be
linear in input space:

(6)

f (z, θ) = zT · θ

(7)

We also applied kernel ridge regression, resulting from mapping the input vectors z into features of
a higher-dimensional space via a nonlinear function Φ: z → Φ(z). In order to avoid the explicit
computation of the vectors Φ(zj ) in the high-dimensional feature space, we apply the kernel trick
and choose mappings Φ such that the inner product Φ(zi )T · Φ(zj ) can be computed via a kernel
function k(zi , zj ) of the inputs. We compared the performance of kernel ridge regression with
that of support vector regression [5]. While kernel ridge regression requires us to store all training
examples in order to evaluate function f at a given input, support vector regression overcomes this
limitation by using an ǫ-insensitive loss function [17]. The resulting f can be evaluated using only
a subset of the training data, the set of support vectors.

7 Testing: Style Transfer

We can restate our initial objective as follows: given an input motion sequence Y in unknown style,
and a target motion style ¯e speci ﬁed by LMA-Effort values, we want to synthesize a seque nce having
style ¯e and content analogous to that of motion Y . A na¨ıve approach to this problem is to seek in
the motion database a pair of sequences having content similar to Y and whose interpolation can
approximate style ¯e. The learned function f can be used to determine the pair of motions and the
interpolation parameter α that produce the best approximation to ¯e. However, such an approach
is destined to fail as Y can be any arbitrarily long and complex sequence, possibly consisting of
several movements performed one after the other. As a consequence, we might not have in the
database examples that match sequence Y in its entirety.

7.1

Input segmentation and matching

The solution that we propose is inspired by concatenative methods. The idea is to determine the
concatenation of database motion examples [X1 , ..., XN ] that best matches the content of input
sequence Y . Our approach relies again on dynamic programming and can be interpreted as a gen-
eralization of the dynamic time warping technique presented in Section 4, for the case when a time
alignment is sought between a given sequence and a concatenation of a variable number of exam-
ples chosen from a set. Let d(p, q , i) be the sum of squared differences between the joint angles of
sequence Y at frame p and those of example Xi at frame q . The goal is to recover the time warping
path w(n) = (p(n), q(n), i(n)) that minimizes the global error

w X
min
n
subject to basic segment transition and endpoint constraints. Transitions constraints are enforced
to guarantee that time order is preserved and that no time frames are omitted. Endpoint constraints
require that the time path starts at beginning frames and ﬁni shes at ending frames of the sequences.
The above mentioned conditions can be formalized as follows:

d(w(n))

(8)

if w(n) = (p, 1, i), then w(n − 1) ∈ {(p − 1, 1, i), (p − 1, Tj , j )for j = 1, ..., J }
(9)
if w(n) = (p, q , i) and q > 1, then w(n − 1) ∈ {(p − 1, q , i), (p − 1, q − 1, i), (p, q − 1, i)} (10)
(11)
p(1) = 1, q(1) = 1, p(L) = T , q(L) = Ti(L)
where J denotes the number of fragments in the database, L the length of the time warping path, T
the number of frames of the input sequence, and Tj the length of the j -th fragment in the database.

Table 1: Mean squared error on LMA-Effort prediction for different function approximation methods
Support Vector
Linear Ridge Kernel Ridge
Linear
Function Approxim.
Regression
Regression
Regression
Interpolation
Method
0.48
0.50
1.03
0.65
Flow MSE
Weight MSE
0.97
1.04
0.39
0.48
0.61
0.60
1.01
1.01
Time MSE

The global minimum of the objective in Equation (8), subject to constraints (9),(10),(11), can be
found using a dynamic programming method originally developed by Ney [14] for the problem of
connected word recognition in speech data. Note that this approach induces a segmentation of the
input sequence Y into snippets [Y1 , ..., YN ], matching the examples in the optimal concatenation
[X1 , ..., XN ].

7.2 Piecewise Style synthesis

The ﬁnal step of our algorithm uses the concatenation of exam ples [X1 , ..., XN ] determined by the
method outlined in the previous section to synthesize a version of motion Y in style ¯e. For each
Xi in [X1 , ..., XN ], we identify the K most similar database examples according to the criterion
deﬁned in Equation 3. Let {Xi1 , ..., XiK } denote the K content-neighbors of Xi and {ei1 , ..., eiK }
their LMA-Effort values. {Xi1 , ..., XiK } deﬁnes a cluster of examples having content similar to that
of snippet Yi . The ﬁnal goal then is to replace each snippet Yi with a pairwise blend of examples
in its cluster so as to produce a motion exhibiting style ¯e. Formally, this is achieved by determining
the pair of examples (ik∗ , il∗ ) in Yi ’s cluster, and the interpolation weight α∗ that provide the best
approximation to target style ¯e, according to the learned style-prediction function f :

α∗ , k∗ , l∗ ← arg min
α,k,l

||¯e − f (zα
ik ,il )||

(12)

Minimization of this objective is achieved by ﬁrst ﬁnding th
e optimal α for each possible pair (ik , il )
of candidate motion fragments. We then select the pair (ik∗ , il∗ ) providing the minimum deviation
from the target style ¯e. In order to estimate the optimal values of α for pair (ik , il ), we evaluate
ik ,il ) for M values of α uniformly sampled in the interval [-0.25,1.25], and choose the value
f (zα
with the closest ﬁt to the target style. We found that f tends to vary smoothly as a function of α,
and thus a good estimate of the global minimum in the speci ﬁed interval can be obtained even with
a modest number M of samples. The approximation is further reﬁned using a gold en section search
[8] around the initial estimate. Note that, by allowing values α to be chosen in the range [-0.25,1.25]
rather than [0,1], we give the algorithm the ability to extrapolate from existing motion styles.
Given optimal parameters (α∗ , k∗ , l∗ ), space-time interpolation of fragments Xik∗ and Xil∗ with
parameter value α∗ produces an animation with content similar to that of Yi and style approximating
the desired target ¯e. This procedure is repeated for all snippets of Y . The ﬁnal animation is obtained
by concatenating all of the fragments generated via interpolation with optimal parameters.

8 Experiments

The system was tested using a motion database consisting of 12 sequences performed by differ-
ent professional dancers. The subjects were asked to perform a speci ﬁc movement phrase in their
own natural style. Each of the 12 sequences was segmented by an LMA expert into 5 fragments
corresponding to the main actions in the phrase. All fragments were then automatically clustered
into 5 content groups using the SSD criterion outlined in section 4. The motions were recorded
using a marker-based motion capture system. In order to derive joint angles, the 3D trajectories of
the markers were ﬁtted to a kinematic chain with 17 joints. Th e joint angles were represented with
exponential maps [13], which have the property of being locally linear and thus particularly suitable
for motion interpolation. From these 60 motion fragments, 105 novel motions were synthesized
with space-time interpolation using random values of α in the range [−0.25, 1.25]. All motions,
both those recorded and those arti ﬁcially generated, were a nnotated with LMA-Effort qualities by

3

2

1

0

-1

-2

s
e
u
l
a
v
 
t
r
o
f
f
E
-
A
M
L

FLOW

WEIGHT

TIME

3

2

1

0

-1

-2

s
e
u
l
a
v
 
t
r
o
f
f
E
-
A
M
L

FLOW

WEIGHT

TIME

3

2

1

0

-1

-2

s
e
u
l
a
v
 
t
r
o
f
f
E
-
A
M
L

-3
-0.5

0

0.5
α

1

1.5

-3
-0.5

0

0.5
α

1

1.5

-3
-0.5

0

0.5
α

FLOW

WEIGHT

TIME

1

1.5

Figure 1: Sample LMA-Effort attributes estimated by kernel ridge regression on three different pairs of mo-
tions (Xi , Xj ) and for α varying in [-0.25, 1.25]. The Flow attribute appears to be almost linearly dependent
on α. By contrast, Weight and Time exhibit non-linear relations with the interpolation parameter.

an LMA expert. From this set of motions, 85 training examples were randomly selected to train the
style regression models. The remaining 20 examples were used for testing.

Table 1 summarizes the LMA-Effort prediction performance in terms of mean squared error for the
different function approximation models discussed in the paper. Results are reported by averaging
over 500 runs of random splitting of the examples into training and testing sets. We include in our
analysis the linear style interpolation model, commonly used in previous work. This model assumes
that the style of a sequence generated via motion interpolation is equal to the interpolation of the
i,j = αei + (1 − α)ej . In all experiments involving kernel-based
styles of the two seed motions: eα
approximation methods, we used a Gaussian RBF kernel. The hyperparameters (i.e.
the kernel
and the regularization parameters) were tuned using tenfold cross-validation. Since the size of the
training data is not overly large, it was possible to run kernel ridge regression without problems
despite the absence of sparsity of this solution. The simple linear interpolation model performed
reasonably well only on the Flow dimension. Overall, non-linear regression models proved to be
much superior to the linear interpolation function, indicating that the style of sequences generates
via space-time interpolation is a complex function of the original styles and motions.

Figure 1 shows the LMA-Effort qualities predicted by kernel ridge regression while varying α for
three different sample values of the inputs (Xi , Xj , ei , ej ). Note that the shapes of the sample
curves learned by kernel ridge regression for the Flow attribute suggest an almost linear dependence
of Flow on α. By contrast, sample functions for the Weight and Time dimensions exhibit non-linear
behavior. These results are consistent with the differences in prediction performance between the
non-linear function models and the linear approximations, as outlined in Table 1.

Several additional motion examples performed by dancers not included in the training data were
used to evaluate the complete pipeline of the motion synthesis algorithm. The input sequences
were always correctly segmented by the dynamic programming algorithm into the ﬁve fragments
associated with the actions in the phrase. Kernel ridge regression was used to estimate the values
of α∗ , k∗ , l∗ as to minimize Equation 12 for different user-speci ﬁed LMA- Effort vectors ¯e. The
recovered parameter values were used to synthesize animations with the speci ﬁed desired styles.
Videos of these automatically generated motions as well as additional results can be viewed at
http://movement.nyu.edu/learning-motion-styles/ . In order to test the generalization
ability of our system, the target styles in this experiment were chosen to be considerably different
from those in the training set. All of the synthesized sequences were visually inspected by LMA
experts and, for the great majority, they were found to be consistent with the style target labels.

9 Discussions and Future Work

We have presented a novel technique that learns motion style synthesis from arti ﬁcially-generated
examples. Animations produced by our system have quality similar to pure motion capture play-
back. Furthermore, we have shown that, even with a small database, it is possible to use pair-wise
interpolation or extrapolation to generate new styles. In previous LMA-based animation systems
[3], heuristic and hand-designed rules have been adopted to implement the style changes associated
to LMA-Effort variations. To the best of our knowledge, our work represents the ﬁrst attempt at
automatically learning the mapping between LMA attributes and animation parameters. Although

our algorithm has shown to produce good results with small training data, we expect that larger
databases with a wider variety of motion contents and styles are needed in order to build an effective
animation system. Multi-way, as opposed to pair-wise, interpolation might lead to synthesis of more
varied motion styles. Our approach could be easily generalized to other languages and notations,
and to additional domains, such as facial animation. Our future work will focus on the recognition
of LMA categories in motion capture data. Research in this area might point to methods for learning
person-speci ﬁc styles and to techniques for transferring i ndividual movement signatures to arbitrary
motion sequences.

Acknowledgments

This work was carried out while LT was at Stanford University and visiting New York University.
Thanks to Alyssa Lees for her help on this project and paper. We are grateful to Edward Warburton,
Kevin Feeley, and Robb Bifano for assistance with the experimental setup and to Jared Silver for the
Maya animations. Special thanks to Jan Burkhardt, Begonia Caparros, Ed Groff, Ellen Goldman and
Pamela Schick for LMA observations and notations. This work has been supported by the National
Science Foundation.

References

[1] O. Arikan and D. A. Forsyth. Synthesizing constrained motions from examples. ACM Transactions on
Graphics, 21(3):483–490, July 2002.
[2] M. Brand and A. Hertzmann. Style machines.
In Proceedings of ACM SIGGRAPH 2000, Computer
Graphics Proceedings, Annual Conference Series, pages 183–19 2, July 2000.
[3] D. Chi, M. Costa, L. Zhao, and N. Badler. The emote model for effort and shape. In Proceedings of ACM
SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, July 2000.
[4] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines (and other kernel-based
learning methods). Cambridge University Press, 2000.
[5] H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines.
In Proc. NIPS 9, 2003.
[6] M. A. Giese and T. Poggio. Morphable models for the analysis and synthesis of complex motion patterns.
International Journal of Computer Vision, 38(1):59–73, 2000.
[7] P. Hackney. Making Connections: Total Body Integration Through Bartenieff Fundamentals. Routledge,
2000.
[8] M. T. Heath. Scientiﬁc Computing: An Introductory Survey, Second edition . McGraw Hill, 2002.
[9] E. Hsu, K. Pulli, and J. Popovic. Style translation for human motion. ACM Transactions on Graphics,
24(3):1082–1089, 2005.
[10] L. Kovar and M. Gleicher. Automated extraction and parameterization of motions in large data sets. ACM
Transactions on Graphics, 23(3):559–568, Aug. 2004.
[11] J. Lee, J. Chai, P. S. A. Reitsma, J. K. Hodgins, and N. S. Pollard. Interactive control of avatars animated
with human motion data. ACM Transactions on Graphics, 21(3):491–500, July 2002.
[12] Y. Li, T. Wang, and H.-Y. Shum. Motion texture: A two-level statistical model for character motion
synthesis. ACM Transactions on Graphics, 21(3):465–472, July 2002.
[13] R. Murray, Z. Li, and S. Sastry. A Mathematical Introduction to Robotic Manipulation. CRC Press, 1994.
[14] H. Ney. The use of a one–stage dynamic programming algorithm fo r connected word recognition. IEEE
Transactions on Acoustics, Speech, and Signal Processing, 32(3):263–271, 1984.
[15] K. Pullen and C. Bregler. Motion capture assisted animation: Texturing and synthesis. ACM Transactions
on Graphics, 21(3):501–508, July 2002.
[16] C. Rose, M. Cohen, and B. Bodenheimer. Verbs and adverbs: multidimensional motion interpolation.
IEEE Computer Graphics and Application, 18(5):32–40, 1998.
[17] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
[18] D. J. Wiley and J. K. Hahn. Interpolation synthesis of articulated ﬁgu re motion. IEEE Computer Graphics
and Application, 17(6):39–45, 1997.


CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY

OBSERVED TRAINING DATA

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

1. Motivation

Factor Analysis is typically applied to problems when we have a set of m training
examples x(i) ∈ Rn , where n ≫ m. We then model the x(i) ’s as generated by a
latent random variable z (i) ∈ Rk and noise term ǫ ∈ Rn with a diagonal covariance
Ψ, where

(1)

or equivalently

x(i) = µ + Λz (i) + ǫ
z ∼ N (0, I )

ǫ ∼ N (0, Ψ)

(2)

x ∼ N (µ, ΛΛT + Ψ)

Consider a similar situation in which, instead of the full vector x(i) , we only
observe some part of x(i) ; a vector v (i) ∈ Rn(i)
, n(i) ≤ n, such that

(3)

v (i) = J (i) T

x(i)

J (i) ∈ Rn×n(i)
is a full rank matrix, with orthonormal columns. If we let r(i) ∈
Rn(i)
consist of the indices of x(i) which are observed, i.e. present in v (i) , then

(4)

(5)

J (i) = h e
r(i)
1
We can then express the distribution of v (i) as
¡ΛΛT + Ψ¢ J (i)´

v (i) ∼ N ³J (i) T
If we deﬁne the following variables

. . . i

e
r(i)
2

µ, J (i) T

(6)

µ(i) = J (i) T
Λ(i) = J (i) T
Ψ(i) = J (i) T

µ

Λ

ΨJ (i)

then we can express the joint distribution of z (i) and v (i) as
1

2

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

(7)

+ Ψ(i) #!
µ(i) ¸ , " I
v (i) ¸ ∼ N Ã· ~0
Λ(i) T
· z
Λ(i) Λ(i)Λ(i) T
2. The E-step

To apply the E-step of the EM algorithm to our modiﬁed factor analysis problem
we need the conditional distrubition z | v (i) ; µ(i) , Λ(i) , Ψ(i) ∼ N (µz (i) |v(i) , Σz (i) |v(i) )
where

+ Ψ(i)´−1 ³v (i) − µ(i)´
µz (i) |v(i) = Λ(i) T ³Λ(i)Λ(i) T
+ Ψ(i)´−1
Σz (i) |v(i) = I − Λ(i) T ³Λ(i)Λ(i) T
Λ(i)
Therefore, Qi (z (i) ) is simply the pdf of this distribution.

3. The M-step

The M-step is not as straight forward as the E-step, and requires results found
in Appendix A. After removing terms that don’t depend on the parameters and
replacing the integral over z (i) with an expectation, the algorithm must maximize
the log likelihood of the data, which is

(8)

m
Ez (i)∼Qi hlog p ³v (i) | z (i) ; µ(i) , Λ(i) , Ψ(i)´i
Xi=1
m
Ez (i)∼Qi · −
n
1
Xi=1
log|Ψ(i) | −
log (2π)
=
2
2
Ψ(i)−1 ³v (i) − µ(i) − Λ(i) z (i)´ ¸
1
2 ³v (i) − µ(i) − Λ(i) z (i)´T
−
Here Ez (i)∼Qi indicates the expectation is with respect to z (i) drawn from the
distribution Qi . Since there is no ambiguity, we will drop the “z (i) ∼ Qi ” subscript.

3.1. M-Step for Λ. First, the log-likelihood is maximized w.r.t. Λ. We must
replace Λ(i) and µ(i) with J (i) T
Λ and J (i) T
µ respectively and then take the gradient
w.r.t. Λ. Dropping terms with no Λ dependence we get

∇Λ

=

m
Ψ(i)−1 ³v (i) − µ(i) − Λ(i) z (i)´¸
− E · 1
2 ³v (i) − µ(i) − Λ(i) z (i)´T
Xi=1
m
∇Λ E · − tr
1
ΛT J (i)Ψ(i)−1
J (i) T
z (i) T
Xi=1
Λz (i)
2

=

m
Xi=1

E h−J (i)Ψ(i)−1

J (i) T

+ z (i) T

µ´ ¸
ΛT J (i)Ψ(i)−1 ³v (i) − J (i) T
+ J (i)Ψ(i)−1 ³v (i) − J (i) T
µ´ z (i) T i
Λz (i) z (i) T

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 3

Substituting the deﬁnition of Ψ(i)−1

from 14 yields

=

m
Xi=1

E · − J (i)J (i) T

Ψ−1J (i)J (i) T

Λz (i) z (i) T

+ tr J (i)J (i) T

µ´ z (i) T ¸
Ψ−1J (i) ³v (i) − J (i) T
m
+ ∆(i)Ψ−1 ³J (i) v (i) − ∆(i)µ´ z (i) T i
E h−∆(i)Ψ−1∆(i)Λz (i) z (i) T
Xi=1
Using the fact that diagonal matrices commute, and the result from 17 we get

=

=

m
E h−Ψ−1∆(i)Λz (i) z (i) T
+ Ψ−1∆(i) ³J (i) v (i) − ∆(i)µ´ z (i) T i
Xi=1
Because ∆(i) is not necessarily full rank, we can not at this point set the equation
equal to zero and solve for Λ. However, if we set the equation equal to zero and
T , for j = 1..n, we are left with n independent equations,
multiply by ej

+ ej

then ej

T Ψ−1∆(i)Λz (i) z (i) T

m
T Ψ−1∆(i) ³J (i) v (i) − ∆(i)µ´ z (i) T i = 0
E h−ej
Xi=1
Since Ψ−1 and ∆(i) are both diagonal, with j th entries 1
and δij respectively,
ψj
T Ψ−1∆(i) = δij
T . Also, using 17 gives us
ψj
m
E ·−
T µ´ z (i) T ¸
δ ij
ψj ³ej
Xi=1
Since δij = 1 iﬀ i ∈ Sj , we can instead sum just over the set Sj and drop the
δij ’s.

T Λz (i) z (i)T +

T J (i) v (i) − ej

δ ij
ψj

ej

ej

=

+

ej

1
ψj

T Λz (i) z (i) T

T J (i) v (i) − ej

T µ´ z (i) T ¸
E ·−
1
ψj ³ej
= Xi∈Sj
T , the j th row of Λ, allowing us to independently solve for the
T Λ selects λj
ej
T J (i) v (i) = x(i)
T µ = µj , which allows us to
n rows of Λ. Additionally, ej
and ej
j
further simplify the equation as
E · 1
j − µj ´ z (i) T ¸
T z (i) z (i) T ¸ = Xi∈Sj
E · 1
ψj ³x(i)
Xi∈Sj
ψj
j − µj ´ z (i) T i
T z (i) z (i) T i = Xi∈Sj
E h³x(i)
E hλj
Xi∈Sj
E hz (i) z (i) T i = Xi∈Sj ³x(i)
j − µj ´ E hz (i) T i
T Xi∈Sj
λj
Ez (i)∼Qi hz (i) z (i) T i
j − µj ´ Ez (i)∼Qi hz (i) T i

T = 
Xi∈Sj ³x(i)
Xi∈Sj



λj

λj

−1

4

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

And after applying the rules for the expectation of z (i) T

and z (i) z (i) T

:

λj

(9)

−1
z (i) |x(i) + Σz (i) |x(i) 
T = 
z (i) |x(i) 

Xi∈Sj ³x(i)
j − µj ´ µT
Xi∈Sj
µz (i) |x(i) µT


T deals with a diﬀerent set Sj , each must be calculated seper-
Since each row λj
ately and concatenated to form the full Λ.

4. M-Step for µ and Ψ

Similarly, the log-likelihood must be maximized w.r.t. µ and Ψ. These two
derivations are very similar, and are therefore worked out in Appendix B. Only the
resulting equations are shown here:

(10)

(11)

ψj =

µj =

1
T µz (i) |x(i) ´
m(j ) Xi∈Sj ³x(i)
j − λj
T Σz (i) |v(i) λj ¶
m Xi∈Sj µ³x(i)
T µz (i) |v(i) ´2
1
j − µj − λj
5. Algorithm Results

+ λj

The algorithm was implemented in Matlab; both Pseudo-Code of this implemen-
tation and more in depth results of tests and NetFlix iterations are presented in
Appendix C and D, respectively.
Several extensive tests were conducted over a wide range of parameter values,
in order to validate the algorithm. The parameters include Σ, m/n, k , γ (fraction
of remaining training data), and Relative Noise. The bulk of our empirical testing
was done on three Windows machines: one to run full-sized Netﬂix predictions, and
two desktops for parameter variation. The approximate times for one test were 17
hours for a single NetFlix iteration, and 5-10 hours for the two desktops running
one complete multiple-parameter test. Results are shown in Appendix D.
Our Netﬂix results indicate that the algorithm does in fact converge, with re-
turned accuracies reported by NetFlix to improve from r2 =1.29 to 1.15, from
the 1st to the 4th iteration. However, the accuracy appears to be approaching an
asymptotic limit, leading us to believe that there may be several improvements
required before we could achieve better results. Speciﬁcally, next we should re-
move outlier users, and take the penalty of predicting for them inaccurately, with
a Λ matrix that does not include those users. However, presumably with the im-
provement in accuracy for the bulk of the users, our overall error decreases. We
have the cutoﬀs determined, but have not implemented this portion thus far. We
have also seen that for a given randomly generated data and user-preferences set,
speciﬁc users consistently contribute to high condition numbers K for the inverted
matrix ΛΛT + Ψ; removing these users in the manner described should improve our
predictions. This has not been tested yet.
The two desktops ran a battery of tests to ﬁnd optimal performance based on
varying parameters. First, we have conﬁrmed a few obvious properties of the algo-
rithm, in order to verify that we are running correctly: for example, the r2 value

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 5

remains relatively high until about 30 percent of the training data is retained,
whereupon most m/n ratios converge to the same ﬁnal prediction accuracy. We
tested m/n ranging from 1:1 to 200:1. The accuracy of predictions improved for the
sets with more data, as the ratio increased. After about 20:1, though improvement
is observed, it is not signiﬁcant. For the NetFlix challenge, this actual ratio of the
provided data is about 30:1, this is inside our found optimal region–at least, for the
algorithm we presented, and for the data generated. However, with approximately
only 1 percent of the training data present in NetFlix, we can safely conﬁrm that
this is a hard problem...
As the number of features, k , increases, computation becomes much more ex-
pensive, going at least as k2 . Based on observing the iteration time for a single
convergence, for a representative range of parameters, we’ve found that for the
ma jority of m/n, roughly k ≤ n/2 gives the fastest results, with not much signiﬁ-
cant improvement for very small k ; the overhead of the rest of the code might be
dominating here. For roughly k ≥ n, the computation becomes many times more
lengthy, especially for large m/n ratios (please see Fig.5).
It appears that when taking into account r2 accuracy and trying to limit com-
putation time, the optimal values (assuming parameters other than m/n ﬁxed) are
roughly n/2 ≤ k ≤ n. If this holds for the NetFlix data, then the maximum k = 40
we have used for the full data set is too small. k ’s larger than 55 crash our com-
puters, and so aside from a single iteration becoming impractically long, memory
limits prohibit testing this result on our Challenge submissions. Note that with a
larger fraction of data left out during training–more than 80 percent–and with high
m/n ratios, for small k (k ≤ 20) the iteration times are found to be roughly 2-3
times as high as for runs with k > 20. Essentially, this expresses what we’ve noted
previously: when there is lots of missing data (m ≫ n), having a small k means
that it is very diﬃcult to accurately reproduce the underlying structure and corre-
lation matrix, leading to more cycles to convergence inside each iteration. Hence,
the iteration times increase. However, once we have suﬃcient k , roughly k ≥ n/2
as shown previously, data reconstruction becomes easier, and iteration times drop
several fold despite the larger k .
With these empirical results, we have found some bounds that improve the per-
formance of this speciﬁc algorithm. The cumalitive results have not been added to
our algorithm yet, and remains for future work.

6. Conclusions

The E-M algorithm, as deﬁned by these updates, runs until convergence as indi-
cated by either very small changes in the Frobinius norms of consecutive Λ, Ψ, and
µ, or r2 prediction error. Important observations are
• Optimal performance is reached when at least 30% of the data is observed.
• Training outliers noticably eﬀect prediction accuracy.
• Higher m/n ratios improve prediction accuracy (all other parameters ﬁxed)
• Larger k improves prediction accuracy (all other parameters ﬁxed)
• Computation time becomes prohibitively long for both large m/n ratio and
large k, implying a criteria for optimizing the choice of k.
In summary, we believe the developed and tested algorithm is sound, and a useful
method for attacking the NetFlix challenge.

6

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

Appendix A. Useful Definitions and Relations

This appendix deﬁnes expressions and relations which are used in the derivation
of the E and M steps of the modiﬁed Factor Analysis algorithm.

A.1. Properties of J (i) . Because J (i) is full rank and it’s columns are basis vectors
in Rn , we can deﬁne a diagonal matrix ∆(i) ∈ Rn×n such that

(12)

J (i)J (i) T
J (i) T
J (i) = In(i)
From these two deﬁnitions we can see that

= ∆(i)

(13)

J (i) = J (i)J (i) T
= ∆(i)J (i)

J (i)

A.2. Inverse of Ψ(i) . Given the deﬁnitions of J (i) and Ψ(i) , we need to compute
Ψ(i)−1
. Since Ψ and Ψ(i) are both diagonal, we expect that

Ψ(i)−1
= J (i) T
Ψ−1J (i)
(14)
To prove that this is the case, compute Ψ(i)Ψ(i)−1
and check the result. Recall
that Ψ, Ψ−1 , and ∆(i) are diagonal, and that diagonal matrices commute.

Ψ(i)Ψ(i)−1

ΨJ (i) )(J (i) T

Ψ−1J (i) )

= (J (i) T
= J (i) T
= J (i) T
= J (i) T
= J (i) T
= I

Ψ∆(i)Ψ−1J (i)

∆(i)ΨΨ−1J (i)

∆(i)J (i)
J (i)J (i) T

J (i)

Clearly, because of the commutative property of diagonal matrices, the same
result holds for Ψ(i)−1
Ψ(i)

A.3. Relation between ∆(i) , J (i) , δij , and the set Sj . Consider what the
columns of J (i) mean. If ej is a column of J (i) then x(i)
is observed, i.e. is present
j
in v (i) . Therefore, ∆(i) has another interpretation.

(15)

where

(16)

j ℓ = (δij
∆(i)
0

if j = ℓ
if j 6= ℓ

δij = (1 if x(i)
j
0 if x(i)
j

is observed
is unobserved

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 7

From this deﬁnition one can see that

(17)

= ∆(i)

∆(i) 2
We can then deﬁne a set Sj equivalently as the set of all training example indices,
i, such that
• x(i)
is observed
j
• ej is a column of J (i)
• δij = 1
• ∆(i)
j j = 1
Finally, we can deﬁne the number of examples for which x(i)
j was observed as

(18)

m(j ) = |Sj |

8

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

Appendix B. M-Step for µ and Ψ

The derivations for µ and Ψ are presented in this section; only the resulting
equations were shown in the paper body.

B.1. M-Step for µ. The log-likelihood must be maximized w.r.t. µ. First re-
place µ(i) and Λ(i) with J (i) T
µ and J (i) T
Λ, and then take the derivative w.r.t. µ.
Dropping terms with no µ dependance we have

∂
∂µ

=

µ − J (i) T

m
E ·−
1
Λz (i)´T
Ψ(i)−1 ³v (i) − J (i) T
2 ³v (i) − J (i) T
Xi=1
m
µ + J (i)Ψ(i)−1 ³v (i) − J (i) T
E h−J (i)Ψ(i)−1
J (i) T
Λz (i)´i
Xi=1
Using the deﬁnition of Ψ(i)−1
and ∆(i) from 14 and 15 respectively we have

µ − J (i) T

Λz (i)´¸

=

=

m
E h−∆(i)Ψ−1∆(i)µ + ∆(i)Ψ−1 ³J (i) v (i) − ∆(i)Λz (i)´i
Xi=1
m
E h−Ψ−1∆(i)µ + Ψ−1∆(i) ³J (i) v (i) − Λz (i)´i
Xi=1
To maximize the w.r.t. µ we set the equation equal to zero. Once again, the
equation is simpliﬁed if we isolate the elements of µ, one at a time, by multiply-
T , and then deal with the resulting n independent equations.
ing both sides by ej
Following similar steps as in the Λ derivation we are left with

T E hz (i) i´
E [µj ] = Xi∈Sj ³x(i)
Xi∈Sj
j − λj
Since µj does not depend on the summation over Sj and is constant, we can
replace the left hand side with m(j )µj . On the right side we substitute the deﬁnition
for E £z (i) ¤. The update for µj becomes
1
m(j ) Xi∈Sj ³x(i)
j − λj
µj =
(19)

T µz (i) |x(i) ´

B.2. M-Step for Ψ. To estimate Ψ we maximize 8 with respect to Ψ. Expand
8 by replacing µ(i) , Λ(i) , and Ψ(i) with J (i) T
µ, J (i) T
Λ, and J (i) T
Ψ respectively.
Then drop terms with no Ψ dependence, take the gradient w.r.t. Ψ, and distribute
J (i) to get

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 9

−

∇Ψ

log|J (i) T

m
E·−
1
Xi=1
ΨJ (i) |
2
Λz (i)´¸
1
Λz (i)´T
Ψ−1J (i) ³v (i) − J (i) T
µ − J (i) T
2 ³v (i) − J (i) T
µ − J (i) T
J (i) T
m
E ·Ψ−1 − Ψ−1 ³J (i) v (i) − ∆(i)µ − ∆(i)Λz (i)´ ³J (i) v (i) − ∆(i)µ − ∆(i)Λz (i)´T
Xi=1
If we set this equal to 0 and both left and right multiply by Ψ (which is invertible)
we ﬁnd that the equation is maximized by

=

Ψ−1¸

Ψ =

1
m

m
E ·³J (i) v (i) − ∆(i)µ − ∆(i)Λz (i)´ ³J (i) v (i) − ∆(i)µ − ∆(i)Λz (i)´T ¸
Xi=1
At this point we use equation 13 to replace the last remaining J (i)

Ψ =

1
m

m
∆(i) T ¸
E ·∆(i) ³J (i) v (i) − µ − Λz (i)´ ³J (i) v (i) − µ − Λz (i)´T
Xi=1
However, we have restricted Ψ to be diagonal, so let us select only the diagonal
T and ej
elements of both sides of the equation by left and right multiplying by ej
respectively. Following the same steps as used in the Λ and µ derivations (recall
T J (i) v (i) = x(i)
T µ = µj ) we see that
that ej
j and ej
m
ej ¸
E ·ej
1
T ∆(i) ³J (i) v (i) − µ − Λz (i)´ ³J (i) v (i) − µ − Λz (i)´T
Xi=1
m
j − µj ´2
1
T E hz (i) z (i) T i λj
T E hz (i) i + λj
j − µj ´ λj
+ 2 ³x(i)
m Xi∈Sj ³x(i)
Plugging in for the deﬁnitions of the expectation of z (i) and z (i) z (i) T
and sim-
plifying yields

∆(i) T

ψj =

=

(20)

ψj =

m Xi∈Sj µ³x(i)
1
j − µj − λj

T µz (i) |v(i) ´2

+ λj

T Σz (i) |v(i) λj ¶

10

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

Appendix C. Implementation Details

The application which motivated development of this algorithm was the Netﬂix
challenge; a challenge to predict users’ ratings of unseen movies based on all known
rating data. The Netﬂix data consists of sparse ratings for 17,000 movies by 500,000
users. At ﬁrst we considered a training example to be the vector of all users’
ratings for a given movie, giving n = 500, 000 and m = 17, 000. Even considering
the sparsity of the data, the square matrix (Λ(i)Λ(i) T
+ Ψ(i) ) would have been, at
worst, in R250,000×250,000 . Having to repeatedly invert those matrices would have
been computationally prohibitive, severely limiting the number of test iterations
we would have been able to run.
For that reason we applied the algorithm to the “transpose” problem; i.e. as-
suming a training example is a vector of a particular user’s ratings for all movies.
In this format we have many more training examples than the dimension of each
training example, which is contrary to the typical motivation for applying factor
analysis.
However, we feel that the algorithm is still applicable because the underlying
model still holds. Speciﬁcally, we feel it is reasonable to assume that users can
be decomposed into a weighted sum of k eigen-users and that the rating for a
given movie can be generated as a linear function of a particular user’s eigen-user
components.

C.1. MATLAB. MATLAB was chosen because it is very good at evaluating an
expression of the form A−1B , which is the primary bottleneck of the algorithm. The
algorithm can be parallized across various computers, but to do so, they would need
to share intermmediate values, which can amount to approximately 1.5 gigabytes for
the Netﬂix test data. While this is feasible on today’s computers, it’s not practical
to implement on shared resources.
In order to reduce memory contraints, Ψ and J were not stored as matricies,
but rather as vectors. Since Ψ is diagonal, it can be stored as vector of its diagonal
entires. J is also stored as a vector of indicies that map the element of x to the
elements of J T x. In MATLAB, this is accomplished by executing the code x(J)
for vectors or X(J,:) for matricies.
In order to solve A−1B , we used the fact that ΛΛT + Ψ is positive deﬁnite,
and therefore a more eﬃcent algorithm can be applied to solving it. MATLAB
implements a processor optimizied BLAS software package, so it stands to be faster
than any code we write. MATLAB uses the command linsolve to calculate A−1B .
linsolve will take additional ﬂags that indicate that A is postivite deﬁnite. This
method was the fastest of several tested.
The following psuedo-code was used to update the parameters of the algorithm

Allocate memory for variables row, mat, inv_col and inv_submat

Allocate and initialize to zero variables for
NewLambda, NewPsi, NewMu and m_j

for i = {1 to m}

Load J and x from disk for user i

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 11

if dimension(x) > max_dimension
choose max_dimension random ratings from x
reform J to match x
end if

Solve the E-step for this user

[inv_col inv_submat] =
(J’(Lambda * Lambda’ + Psi)J)^-1 * [(x - J’mu) J’ Lambda]

mu_zi_xi = Lambda’J * inv_col
Sigma_zi_xi = I - J’Lambda * inv_submat

Apply this user’s contribution to the M-step

mu_term = x - J’Lambda * mu_zi_xi
mat_term = (mu_zi_xi * mu_zi_xi’) + Sigma_zi_xi

for j = {set of movies user i rated}

row_term = ( x(j) - J’mu(j) ) * mu_zi_xi

row(j) = row(j) + row_term
mat(j) = mat(j) + mat_term

NewPsi(j) = NewPsi(j) +
J’Lambda(j) * Sigma_zi_xi * Lambda(j)’J +
(mu_term - mu(j))^2
NewMu(j) = NewMu(j) + mu_term

m_j(j) = m_j(j) + 1

end for

end for

for j = {1 to n}

NewLambda(j) = row(j) * mat(j)^-1
NewMu(j) = NewMu(j) / m_j(j)

end for

The algorithm took ten hours for a single iteration on the Netﬂix data. This was
acceptable since we want to submit the prediction of each iteration to Netﬂix, and
their system only allows one predicition every twenty-four hours.

12

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

C.2. Extensions of the algorithm. It would take a quarter tetrabyte to store
the largest (Λ(i)Λ(i) T
+ Ψ(i) ) matrix in double-precision, even while taking advan-
tage of symmetry. However, for iterative linear solving algorithms, such as the
conjugate gradient method, the whole matrix would not need to be stored. The
algorithm would calculate the residual r = Ax − b and use r to update the value
of x. Ax can be calculated in MATLAB without ever storing its explicit form:
Lambda * (Lambda’ * x) + diag(Psi .* x). However, as it turns out, as x′ s
grow to the 250,000 element size, round-oﬀ error becomes a problem and the algo-
rithm fails to converge.

Appendix D. Results

The following section presents more detailed results supporting claims made in
the conclusion of the main paper.

D.1. Repeatability and Eﬀects of Noise. A data set was generated according
to the underlying model for which the algorithm was designed, with parameters
m = n = 100, k = 20. The rows of Λ, λj ∈ Rk , and the latent variables z (i) ∈ Rk
were independently drawn from a gaussian, N (0, I ). The noise terms, ǫ ∈ Rn , were
also drawn from a gaussian, N (0, σ2 I ). The algorithm was run to convergence 5
times for varying σ2 values, fraction training data observed, and the assumed value
of k. The resulting prediction errors used to generate average and sample variance
data.
Prediction accuracy drastically increases around 10% observed data, and again
around 50% observed data. Prediction with less than 10% is virtually impossible,
being not much better than randomly guessing. By comparing Figs. D.1 and D.1
we see that, as would be expected, when the noise used to generate the data is very
small, prediction is much more accurate.

101

100

)
r
o
r
r
e
(
 
2
r

10−1

10−2

10−3

0

k = 10
k = 20
k = 30

0.2

0.4
0.6
Fraction of Training Data Observed

0.8

1

Figure 1. Prediction error for moderate noise; σ2 = 1

Figs. D.1 and D.1 indicate that the algorithm is suﬃciently repeatable for more
than 10% observed data, while suggesting that ﬁnal prediction is very sensitive to
initial value of Λ, Ψ, and µ for any less.

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 13

)
r
o
r
r
e
(
 
2
r

101

100

10−1

10−2

10−3

10−4

10−5

10−6

0

k = 10
k = 20
k = 30

0.2

0.4
0.6
Fraction of Training Data Observed

0.8

1

Figure 2. Prediction error for moderate noise; σ2 = 0.001

102

100

10−2

10−4

10−6

10−8

e
c
n
a
i
r
a
V
 
e
l
p
m
a
S
 
2
r

10−10

10−12

10−14

0

k = 10
k = 20
k = 30

0.2

0.4
0.6
Fraction of Training Data Observed

0.8

1

Figure 3. Sample variance of prediction error for moderate noise;
σ2 = 1

D.2. Further Results for Optimizing Performance. The second desktop com-
puter ran further tests on locating more parameters that improve the convergence
speed and accuracy. Some results are highlighted below in Figs.5-7.

14

PAUL CSONKA, BARRETT HEYNEMAN, SALOMON TRUJILLO

e
c
n
a
i
r
a
V
 
e
l
p
m
a
S
 
2
r

105

100

10−5

10−10

10−15

10−20

0

k = 10
k = 20
k = 30

0.2

0.4
0.6
Fraction of Training Data Observed

0.8

1

Figure 4. Sample variance of prediction error for moderate noise;
σ2 = 0.001

Iteration Times; m/n ratios decrease as [200 20 10 1]:1 ; (50% missing data)
 
800

)
s
(
 
e
m
i
T
 
n
o
i
t
a
r
e
t
I
 
e
l
g
n
i
S

700

600

500

400

300

200

100

0

 
0

10

20

30
50
40
Feature Vector Count (k)

60

70

80

Figure 5. Time required for an iteration loop; i.e., time until con-
vergence for a given set of parameters. The plots are, in decreasing
order, m/n ratios of 200:1, 20:1, 10;1, and 1:1, which straddles one
of our empirically found optimum of 20:1. Please note that the
apparently short times, near small k , are due to the algorithm
terminating early from very high r2 .

CS229 PROJECT: FACTOR-ANALYSIS WITH PARTIALLY OBSERVED TRAINING DATA 15

Iteration Times; m/n ratios decrease as [200 20 10 1]:1 ; (90% missing data)
160

)
s
(
 
e
m
i
T
 
n
o
i
t
a
r
e
t
I
 
e
l
g
n
i
S

140

120

100

80

60

40

20

0

0

10

20

30
50
40
Feature Vector Count (k)

60

70

80

Figure 6. Same as Fig.5, but showing decrease in computation
time as k increases; for larger m/n values, this increase in per-
formance becomes very small past our determined threshold of
m/n ≥ 20 : 1

r2 vs. m/n Ratio vs. Data Retention
1.8

s
t
l
u
s
e
R
 
n
w
o
n
K
 
t
s
n
i
a
g
A
 
e
u
l
a
V
 
2
r

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

50

100

150

200

0.2

0.4

0.6

0.8

Data Retention Fraction

Figure 7. Prediction accuracy vs. data retention and m/n ratio.
Note the proﬁle shows increasing accuracy (decreasing r2 ) as m/n
and percent data retention both increase


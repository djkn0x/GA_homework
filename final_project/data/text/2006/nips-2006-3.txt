Learning on Graph with Laplacian Regularization

Rie Kubota Ando
IBM T.J. Watson Research Center
Hawthorne, NY 10532, U.S.A.
rie1@us.ibm.com

Tong Zhang
Yahoo! Inc.
New York City, NY 10011, U.S.A.
tzhang@yahoo-inc.com

Abstract

We consider a general form of transductive learning on graphs with Laplacian
regularization, and derive margin-based generalization bounds using appropriate
geometric properties of the graph. We use this analysis to obtain a better under-
standing of the role of normalization of the graph Laplacian matrix as well as the
effect of dimension reduction. The results suggest a limitation of the standard
degree-based normalization. We propose a remedy from our analysis and demon-
strate empirically that the remedy leads to improved classiﬁcation performance.

1 Introduction

In graph-based methods, one often constructs similarity graphs by linking similar data points that are
close in the feature space. It was proposed in [3] that one may ﬁrst project these data points into the
eigenspace corresponding to the largest eigenvalues of a normalized adjacency matrix of the graph
and then use the standard k-means method for clustering. In the ideal case, points in the same class
will be mapped into a single point in the reduced eigenspace, while points in different classes will be
mapped to different points. One may also consider similar ideas in semi-supervised learning using a
discriminative kernel method. If the underlying kernel is induced from the graph, one may formulate
semi-supervised learning directly on the graph (e.g., [1, 5, 7, 8]). In these studies, the kernel is in-
duced from the adjacency matrix W whose (i, j )-entry is the weight of edge (i, j ). W is sometimes
normalized by D−1/2WD−1/2 [2, 4, 3, 7] where D is a diagonal matrix whose (j, j )-entry is the
degree of the j -th node, but sometimes not [1, 8]. Although such normalization may signiﬁcantly
affect the performance, the issue has not been studied from the learning theory perspective. The
relationship of kernel design and graph learning was investigated in [6], which argued that quadratic
regularization-based graph learning can be regarded as kernel design. However, normalization of
W was not considered there. The goal of this paper is to provide some learning theoretical insight
into the role of normalization of the graph Laplacian matrix (D − W). We ﬁrst present a model for
transductive learning on graphs and develop a margin analysis for multi-class graph learning. Based
on this, we analyze the performance of Laplacian regularization-based graph learning in relation to
graph properties. We use this analysis to obtain a better understanding of the role of normalization
of the graph Laplacian matrix as well as dimension reduction in graph learning. The results indicate
a limitation of the commonly practiced degree-based normalization mentioned above. We propose
a learning theoretical remedy based on our analysis and use experiments to demonstrate that the
remedy leads to improved classiﬁcation performance.

2 Transductive Learning Model

We consider the following multi-category transductive learning model de ﬁned on a graph. Let V =
{v1 , . . . , vm} be a set of m nodes, and let Y be a set of K possible output values. Assume that
each node vj is associated with an output value yj ∈ Y , which we are interested in predicting. We
randomly draw a set of n indices Zn = {ji : 1 ≤ i ≤ n} from {1, . . . , m} uniformly and without

replacement. We then manually label the n nodes vji with labels yji ∈ Y , and then automatically
label the remaining m − n nodes. The goal is to estimate the labels on the remaining m − n nodes
as accurately as possible. We encode the label yj into a vector in RK , so that the problem becomes
that of generating an estimation vector fj,· = [fj,1 , . . . , fj,K ] ∈ RK , which can then be used to
recover the label yj . In multi-category classiﬁcation with K classes Y = {1, . . . , K }, we encode
each yj = k ∈ Y as ek ∈ RK , where ek is a vector of zero entries except for the k-th entry being
one. Given fj,· = [fj,1 , . . . , fj,K ] ∈ RK (which is intended to approximate eyj ), we decode the
corresponding label estimation ˆyj as: ˆyj = arg maxk {fj,k : k = 1, . . . , K }. If the true label is
yj , then the classiﬁcation error is err(fj,· , yj ) = I ( ˆyj 6= yj ), where we use I (·) to denote the set
indicator function.
In order to estimate f = [fj,k ] ∈ RmK from only a subset of labeled nodes, we consider for a
given kernel matrix K ∈ Rm , the quadratic regularization f T QK f = PK
·,kK−1f·,k , where
k=1 f T
f·,k = [f1,k , . . . , fm,k ] ∈ Rm . We assume that K is full-rank. We will consider the kernel matrix
induced by the graph Laplacian, to be introduced later in the paper. Note that the bold symbol K
denotes the kernel matrix, and regular K denotes the number of classes.
Given a vector f ∈ RmK , the accuracy of its component fj,· = [fj,1 , . . . , fj,K ] ∈ RK is measured
by a loss function φ(fj,· , yj ). Our learning method attempts to minimize the empirical risk on the
set Zn of n labeled training nodes, subject to f T QKf being small:
f ∈RmK 
φ(fj,· , yj ) + λf T QKf 
1
ˆf (Zn ) = arg min
n Xj∈Zn
 .

where λ > 0 is an appropriately chosen regularization parameter.
"
#
In this paper, we focus on a special class of loss function that is of the form φ(fj,· , yj ) =
(cid:19)
(cid:18)
X
mX
PK
k=1 φ0 (fj,k , δk,yj ), where δa,b is the delta function de ﬁned as: δa,b = 1 when a = b and δa,b = 0
otherwise. We are interested in the generalization behavior of (1) compared to a properly de ﬁned
optimal regularized risk, often referred to as “oracle ineq ualities” in the learning theory literature.
Theorem 1 Let φ(fj,· , yj ) = PK
k=1 φ0 (fj,k , δk,yj ) in (1). Assume that there exist positive constants
a, b, and c such that: (i) φ0 (x, y ) is non-negative and convex in x, (ii) φ0 (x, y ) is Lipschitz with
constant b when φ0 (x, y ) ≤ a, and (iii) c = inf {x : φ0 (x, 1) ≤ a} − sup{x : φ0 (x, 0) ≤ a}.
Then ∀p > 0, the expected generalization error of the learning method (1) over the random training
samples Zn can be bounded by:
p
1
1
1
inf
m − n
a
m
f ∈RmK
j∈ ¯Zn
j=1
j,j (cid:17)1/p
where ¯Zn = {1, . . . , m} − Zn , trp (K) = (cid:16) 1
m Pm
j=1 Kp
, and Kj,j is the (j, j )-entry of K.
Proof. The proof is similar to the proof of a related bound for binary-classiﬁcation in [6]. We shall
introduce the following notation. let in+1 6= i1 , . . . , in be an integer randomly drawn from ¯Zn , and
let Zn+1 = Zn ∪ {in+1}. Let ˆf (Zn+1 ) be the semi-supervised learning method (1) using training
data in Zn+1 : ˆf (Zn+1 ) = arg inf f ∈RmK h 1
φ(fj,· , Yj ) + λf T QK f i. Adapted from a
n Pj∈Zn+1
related lemma used in [6] for proving a similar result, we have the following inequality for each
k = 1, . . . , K :
| ˆfin+1 ,k (Zn+1 ) − ˆfin+1 ,k (Zn )| ≤ |∇1,k φ( ˆfin+1 ,· (Zn+1 ), Yin+1 )|Kin+1 ,in+1 /(2λn),
(2)
where ∇1,k φ(fi,· , y ) denotes a sub-gradient of φ(fi,· , y ) with respect to fi,k , where fi,· =
[fi,1 , . . . , fi,K ]. Next we prove
Kin+1 ,in+1 (cid:19)p (cid:21) . (3)
k=k0 ,in+1 (cid:20) 1
φ0 ( ˆfin+1 ,k (Zn+1 ), δin+1 ,k ) + (cid:18) b
err( ˆfin+1 ,· (Zn ), yin+1 ) ≤ sup
a
cλn
In fact, if ˆf (Zn ) does not make an error on the in+1 -th example, then the inequality automatically
holds. Otherwise, assume that ˆf (Zn ) makes an error on the in+1 -th example, then there exists k0 6=

φ0 (fj,· , yj ) + λf T QK f

err( ˆfj,· (Zn ), yj ) ≤

EZn

(1)

+

btrp (K)
λnc

,

(Zn ) ≤ ˆfin+1 ,k0 (Zn ). If we let d = (inf {x : φ0 (x, 1) ≤ a} + sup{x :
yin+1 such that ˆfin+1 ,yin+1
(Zn ) ≤ d or ˆfin+1 ,k0 (Zn ) ≥ d. By the de ﬁnition of c and
φ0 (x, 0) ≤ a})/2, then either ˆfin+1 ,yin+1
d, it follows that there exists k = k0 or k = in+1 such that either φ0 ( ˆfin+1 ,k (Zn+1 ), δin+1 ,k ) ≥ a or
X
X
(cid:12)(cid:12)(cid:12)
ˆfin+1 ,k (Zn+1 ) − ˆfin+1 ,k (Zn )(cid:12)(cid:12)(cid:12) ≥ c/2. Using (2), we have either φ0 ( ˆfin+1 ,k (Zn+1 ), δin+1 ,k ) ≥ a
(cid:17)p
24 1
35
a φ0 ( ˆfin+1 ,k (Zn+1 ), δin+1 ,k ) + (cid:16) bKin+1 ,in+1
or bKin+1 ,in+1 /(2λn) ≥ c/2, implying that 1
(cid:19)
(cid:18)
≥ 1 =
X
X
cλn
err( ˆfin+1 ,· (Zn ), yin+1 ). This proves (3).
24 1
35 +
We are now ready to prove Theorem 1 using (3). For every j ∈ Zn+1 , denote by Z (j)
(cid:19)
(cid:18)
n+1 the
X
X
subset of n samples in Zn+1 with the j -th data point left out. We have err( ˆfj,· (Z (j)
n ), yj ) ≤
cλn Kj,j (cid:1)p . We thus obtain for all f ∈ RmK :
a φ( ˆfj,· (Zn+1 ), yj ) + (cid:0) b
1
1
1
err( ˆfj,· (Zn ), yj ) ≤
err( ˆfj,· (Z (j )
n ), yj )
EZn
EZn+1
n + 1
m − n
j∈ ¯Zn
j∈Zn+1
b
φ( ˆfj,· (Zn+1 ), yj ) +
EZn+1
a
cλn
j∈Zn+1
j∈Zn+1
p
φ(fj,· , yj ) + λf T QK f
EZn+1
EZn+1
n
j∈Zn+1
j∈Zn+1
The formulation used here corresponds to the one-versus-all method for multi-category classiﬁ-
cation. For the SVM loss φ0 (x, y ) = max(0, 1 − (2x − 1)(2y − 1)), we may take a = 0.5,
b = 2, and c = 0.5. In the experiments reported here, we shall employ the least squares function
φ0 (x, y ) = (x − y )2 which is widely used for graph learning. With this formulation, we may choose
a = 1/16, b = 0.5, c = 0.5 in Theorem 1.

n
a(n + 1)

1
n + 1

1
n + 1

Kj,j

.

2

≤

≤

p

Kj,j

b
cλn

3 Laplacian regularization

Consider an undirected graph G = (V , E ) de ﬁned on the nodes V = {vj : j = 1, . . . , m}, with
edges E ⊂ {1, . . . , m} × {1, . . . , m}, and weights wj,j ′ ≥ 0 associated with edges (j, j ′ ) ∈ E . For
simplicity, we assume that (j, j ) /∈ E and wj,j ′ = 0 when (j, j ′ ) /∈ E . Let degj (G) = Pm
j ′=1 wj,j ′
be the degree of node j of graph G. We consider the following de ﬁnition of normalized Laplaci an.
De ﬁnition 1 Consider a graph G = (V , E ) of m nodes with weights wj,j ′ (j, j ′ = 1, . . . , m).
The unnormalized Laplacian matrix L(G) ∈ Rm×m is de ﬁned as: Lj,j ′ (G) = −wj,j ′ if j 6=
j ′ ; degj (G) otherwise. Given m scaling factors Sj (j = 1, . . . , m), let S = diag({Sj }). The
S-normalized Laplacian matrix is de ﬁned as: LS (G) = S−1/2L(G)S−1/2 . The corresponding
j′ (cid:19)2
j,j ′=1 wj,j ′ (cid:18) fj,k√Sj −
fj′ ,k√S
2 Pm
·,kLS (G)f·,k = 1
regularization is based on: f T
.
A common choice of S is S = I, corresponding to regularizing with the unnormalized Laplacian
L. The idea is natural: we assume that the predictive values fj,k and fj ′ ,k should be close when
(j, j ′ ) ∈ E with a strong link. Another common choice is to normalize by Sj = degj (G) (i.e.
S = D) so that diagonals of LS become all one [3, 4, 7, 2].
De ﬁnition 2 Given label y = {yj }j=1,...,m on V , we de ﬁne the cut for LS in De ﬁnition 1 as:
j′ (cid:19)2
2 (cid:18) 1√Sj − 1√S
j′ (cid:17) + Pj,j ′ :yj =yj′
2 (cid:16) 1
wj,j′
wj,j′
+ 1
.
cut(LS , y ) = Pj,j ′ :yj 6=yj′
Sj
S
Unlike typical graph-theoretical de ﬁnitions of graph-cut , this learning theoretical de ﬁnition of graph-
cut penalizes not only between-class edge weights but also within-class edge weights when such an
edge connects two nodes with different scaling factors. This penalization is intuitive if we look at the
regularizer in De ﬁnition (1), which encourages fj,k /pSj to be similar to fj ′ ,k /pSj ′ when wj,j ′ is
large. If j and j ′ belongs to the same class, we want fj,k to be similar to fj ′ ,k . Therefore for such

an in-class pair (j, j ′ ), we want to have Sj ≈ Sj ′ . This penalization has important consequences,
which we will investigate later in the paper. For unnormalized Laplacian (i.e. Sj = 1), the second
term on the right hand side of De ﬁnition 2 vanishes, and our le arning theoretical de ﬁnition becomes
identical to the standard graph-theoretical de ﬁnition: cut(L, y ) = Pj,j ′ :yj 6=yj′ wj,j ′ .
We consider K in (1) de ﬁned as follows: K = (αS−1 + LS (G))−1 , where α > 0 is a tuning
parameter to make K strictly positive de ﬁnite. This parameter is important.

For simplicity, we state the generalization bound based on Theorem 1 with optimal λ. Note that in
applications, λ is usually tuned through cross validation. Therefore assuming optimal λ will simplify
the bound so that we can focus on the more essential characteristics of generalization performance.

Theorem 2 Let the conditions in Theorem 1 hold with the regularization condition K = (αS−1 +
LS (G))−1 . Assume that φ0 (0, 0) = φ0 (1, 1) = 0, then ∀p > 0, there exists a sample independent
regularization parameter λ in (1) such that the expected generalization error is bounded by:
1
Cp (a, b, c)
err( ˆfj,· (Zn ), yj ) ≤
m − n Xj∈ ¯Zn
np/(p+1) (αs + cut(LS , y ))p/(p+1) trp (K)p/(p+1) ,
EZn
where Cp (a, b, c) = (b/ac)p/(p+1) (p1/(p+1) + p−p/(p+1) ) and s = Pm
j=1 S−1
j
Proof. Let fj,k = δyj ,k . It can be easily veriﬁed that Pm
j=1 φ(fj,· , yj )/m + λf T QKf = λ(αs +
cut(LS , y )). Now, we simply use this expression in Theorem 1, and then optimize over λ. 2
This theorem relates graph-cut to generalization performance. The conditions on the loss function in
Theorem 2 hold for least squares with b/ac = 16. It also applies to other standard loss functions such
as SVM. With p ﬁxed, the generalization error decreases at the rate O(n−p/(p+1) ) when n increases.
This rate of convergence is faster when p increases. However in general, trp (K) is an increasing
function of p. Therefore we have a trade-off between the two terms. The bound also suggests that if
we normalize the diagonal entries of K such that Kj,j is a constant, then trp (K) is independent of
p, and thus a larger p can be used in the bound. This motivates the idea of normalizing the diagonals
p
p
p+1 is related
of K. Our goal is to better understand how the quantity (αs + cut(LS , y ))
p+1 trp (K)
to properties of the graph, which gives better understanding of graph-based learning.

.

De ﬁnition 3 A subgraph G0 = (V0 , E0 ) of G = (V , E ) is a pure component if G0 is connected, E0
is induced by restricting E on V0 , and if labels y have identical values on V0 . A pure subgraph G′ =
∪q
ℓ=1Gℓ of G divides V into q disjoint sets V = ∪q
ℓ=1Vℓ such that each subgraph Gℓ = (Vℓ , Eℓ ) is
a pure component. Denote by λi (Gℓ ) = λi (L(Gℓ )) the i-th smallest eigenvalue of L(Gℓ ).
If we remove all edges of G that connect nodes with different labels, then the resulting subgraph is
a pure subgraph (but not the only one). For each pure component Gℓ , its ﬁrst eigenvalue λ1 (Gℓ ) is
always zero. The second eigenvalue λ2 (Gℓ ) > 0, and it measures how well-connected Gi is [2].
Theorem 3 Let the assumptions of Theorem 2 hold, and G′ = ∪q
ℓ=1Gℓ (Gℓ = (Vℓ , Eℓ )) be a pure
subgraph of G. For all p ≥ 1, there exist sample-independent λ and α, such that the generalization
err( ˆfj,· , yj )/(m − n), is bounded by
performance of (1), EZn Pj∈ ¯Zn
λ2 (Gℓ )p !1/2p
np/(p+1) 
ℓ !1/2p
+ cut(LS , y )1/2   q
s1/2   q
Cp (a, b, c)
sℓ (p)/m
sℓ (p)/m
Xℓ=1
Xℓ=1
mp

where mℓ = |Vℓ |, s = Pm
Sp
j=1 S−1
j .
, and sℓ (p) = Pj∈Vℓ
j
Proof sketch. We simply upper bound trp (K) in terms of λ2 (Gℓ ) and sℓ , where K = (αS−1 +
LS )−1 . Substitute this estimation into Theorem 2 and optimize it over α. 2
To put this into perspective, suppose that we use unnormalized Laplacian regularizer on a zero-cut
graph. Then S = I and cut(LS , y ) = 0, and by letting p = 1 and p → ∞ in Theorem 3, we have:
err( ˆfj,· , yj )
err( ˆfj,· , yj )
m − n ≤ 2r b
m
b
q
EZn Xj∈ ¯Zn
and EZn Xj∈ ¯Zn
.
ac ·
m − n ≤
ac ·
n minℓ mℓ
n

2p/(p+1)

,

That is, in the zero-cut case, the generalization performance can be bounded as O(pq/n). We can
also achieve a faster convergence rate of O(1/n), but it also depends on m/(minℓ mℓ ) ≥ q . This
implies that we will achieve better convergence at the O(1/n) level if the sizes of the components
are balanced, while the convergence may behave like O(pq/n) otherwise.
3.1 Near zero-cut optimum scaling factors

b
ac ·

,

err( ˆfj,· , yj ) ≤

The above observation motivates a scaling matrix S so that it compensates for the unbalanced pure
component sizes. From De ﬁnition 2 and Theorem 2 we know that g ood scaling factors should be
approximately constant within each class. Here we focus on the case that scaling factors are constant
within each pure component (Sj = ¯sℓ when j ∈ Vℓ ) in order to derive optimum scaling factors.
wj,j′
2 . In Theorem 3, when we
Let us de ﬁne cut(G′ , y ) = Pj,j ′ :yj 6=yj′ wj,j ′ + Pℓ 6=ℓ′ Pj∈Vℓ ,j ′ ∈Vℓ′
use cut(LS , y ) ≤ cut(G′ , y )/ minℓ ¯sℓ and let p → ∞ and assume that cut(G′ , y ) is sufﬁciently
Pq
small, the dominate term of the bound becomes maxℓ (¯sℓ /mℓ )
mℓ
, which can then be optimized
ℓ=1
¯sℓ
n
with the choice ¯sℓ = mℓ , and the resulting bound becomes:
2
n  √q + s cut(G′ , y )
u(G′ ) minℓ mℓ !
1
1
m − n Xj∈ ¯Zn
where u(G′ ) = minℓ (λ2 (Gℓ )/mℓ ). Hence, if cut(G′ , y ) is small, then we should choose ¯sℓ ∝ mℓ
for each pure component ℓ so that the generalization performance is approximately (ac)−1 b · q/n.
The analysis provided here not only formally shows the importance of normalization in the learn-
ing theoretical framework but also suggests that the good normalization factor for each node j is
approximately the size of the well-connected pure component that contains node j (assuming that
nodes belonging to different pure components are only weakly connected). The commonly practiced
degree-based normalization method Sj = degj (G) provides such good normalization factors under
a simpliﬁed “box model” used in early studies e.g. [4]. In thi
s model, each node connects to itself
and all other nodes of the same pure component with edge weight wj,j ′ = 1. The degree is thus
degj (Gℓ ) = |Vℓ | = mℓ , which gives the optimal scaling in our analysis. However, in general, the
box model may not be a good approximation for practical problems. A more realistic approxima-
tion, which we call core-satellite model, will be introduced in the experimental section. For such a
model, the degree-based normalization can fail because the degj (Gℓ ) within each pure component
Gℓ is not approximately constant (thus raising cut(LS , y )), and it may not be proportional to mℓ .
Our remedy is as follows. Let ¯K = (αI + L)−1 be the kernel matrix corresponding to the unnormal-
ized Laplacian. Let vℓ ∈ Rm be the vector whose j -th entry is 1 if j ∈ Vℓ and 0 otherwise. Then it
is easy to verify that for small α and near-zero cut(G′ , y ), we have α ¯K = Pq
ℓ=1 vℓ vT
ℓ /mℓ + O(1),
and thus ¯Kj,j ∝ m−1
for each j ∈ Vℓ . Therefore the scaling factor Sj = 1/ ¯Kj,j is nearly optimal
ℓ
for all j . We call this method of normalization (Sj = 1/ ¯Kj,j , K = (αS−1 + LS )−1 ) K-scaling in
this paper as it scales the kernel matrix K so that each Kj,j = 1. By contrast, we call the standard
degree-based normalization (Sj = degj (G), K = (αI + LS )−1 ) L-scaling as it scales diagonals
of LS to 1. Although K-scaling coincides with a common practice in standard kernel learning, it is
important to notice that showing this method behaves well in the graph learning setting is non-trivial
and novel. In fact, no one has proposed this normalization method in the graph learning setting
before this work. Without the learning theoretical results developed here, it is not obvious whether
this method should work better than the commonly practiced degree-based normalization.

4 Dimension Reduction

Normalization and dimension reduction have been commonly used in spectral clustering such as
[3, 4]. For semi-supervised learning, dimension reduction (without normalization) is known to im-
prove performance [1, 6] while normalization (without dimension reduction) has also been explored
[7]. An appropriate combination of normalization and dimension reduction can further improve per-
formance. We shall ﬁrst introduce dimension reduction with normalized Laplacian LS (G). Denote
by Pr
S (G) the projection operator onto the eigenspace of αS−1 + LS (G) corresponding to the r

(4)

smallest eigenvalues. Now, we may de ﬁne the following regul arizer on the reduced subspace:
·,kK−1
0 f·,k Pr
·,kK−1f·,k = (cid:26)f T
S (G)f·,k = f·,k ,
f T
otherwise.
+∞
Note that we will focus on bounding the generalization complexity using the reduced dimensionality
r. In such context, the choice of K0 is not important. For example, we may simply choose K0 = I.
The bene ﬁt of dimension reduction in graph learning has been investigated in [6], under the spectral
kernel design framework. Note that the normalization issue, which will change the eigenvectors and
their ordering, wasn’t investigated there. The following theorem shows that the target vectors can be
well approximated by its projection onto Pq
S (G). We skip the proof due to the space limitation.
Theorem 4 Let G′ = ∪q
ℓ=1Gℓ (Gℓ = (Vℓ , Eℓ )) be a pure subgraph of G. Consider r ≥ q :
λr+1 (LS (G)) ≥ λr+1 (LS (G′ )) ≥ minℓ λ2 (LS (Gℓ )). For each k , let ¯fj,k = δyj ,k be the target
S (G) ¯f·,k − ¯f·,kk2
2 ≤ δr (S)k ¯f·,k k2
(encoding of the true labels) for class k (j = 1, . . . , m). Then kPr
2 ,
where δr (S) = kLS (G)−LS (G′ ))k2+d(S)
− S−1/2
(S−1/2
1
)2 .
2|Vℓ | Pj,j ′ ∈Vℓ
d(S) = maxℓ
,
j
j ′
λr+1 (LS (G))
We can prove a generalization bound using Theorem 4. For simplicity, we only consider least
squares loss φ(fj,· , yj ) = PK
k=1 (fj,k − δk,yj )2 in (1) using regularization (4) and K0 = I. With
m Pm
j=1 φ( ¯fj,· , yj ) ≤ δr (S)2 + λm. It is also equivalent to take K0 = Pr
p = 1, we have 1
S (G) due
to the dimension reduction, so that we can use tr(K) = r. Now from Theorem 1 with a = 1/16,
err( ˆfj,· , yj ) ≤ 16(δr (S)2 +λm)+ r
1
b = 0.5, c = 0.5, we have EZn
λnm . By optimizing
m−n Pj∈ ¯Zn
over λ, we obtain
err( ˆfj,· , yj )
EZn Xj∈ ¯Zn
m − n ≤ 16δr (S)2 + 32pr/n.
The analysis of optimum scaling factors is analogous to Section 3.1, and the conclusions there hold.
Compared to Theorem 3, the advantage of dimension reduction in (5) is that the quantity cut(LS , y )
is replaced by kLS (G) − LS (G′ )k2 , which is typically much smaller. Instead of a rigorous analysis,
we shall just give a brief intuition. For simplicity we take S = I so that we can ignore the variations
caused by S. The 2-norm of the symmetric error matrix LS (G) − LS (G′ ) is its largest eigenvalue,
which is no more than the largest 1-norm of one of its row vectors. In contrast, cut(LS , y ) behaves
similar to the absolute sum of entries of the error matrix, which is m times more than the averaged
1-norm of its row vectors. Therefore if error is relatively uniform across rows, then cut(LS , y ) can
be at an order of m times more than kLS (G) − LS (G′ )k2 .
5 Experiments

(5)

We test the three types of the kernel matrix K (Unnormalized, normalized by K-scaling or L-
scaling) with the two regularization methods: the ﬁrst method is to u se K without dimension re-
duction, and the second method reduces the dimension of K−1 to eigenvectors corresponding to
S (G)f = f and +∞ otherwise. We
the smallest r eigenvalues and regularizes with f T K−1f if Pr
are particularly interested in how well K-scaling performs. From m data points, n training labeled
examples are randomly chosen while ensuring that at least one training example is chosen from
each class. The remaining m − n data points serve as test data. The regularization parameter λ
is chosen by cross validation on the n training labeled examples. We will show performance ei-
ther when the rest of the parameters (α and dimensionality r) are also chosen by cross validation
or when they are set to the optimum (oracle performance). The dimensionality r is chosen from
K, K + 5, K + 10, · · · , 100 where K is the number of classes unless otherwise speciﬁed. Our focu s
is on small n close to the number of classes. Throughout this section, we conduct 10 runs with
random training/test splits and report the average accuracy. We use the one-versus-all strategy with
least squares loss φk (a, b) = (a − δk,b )2 .
Controlled data experiments

The purpose of the controlled data experiments is to observe the correlation of the effectiveness of
the normalization methods with graph properties. The graphs we generate contain 2000 nodes, each
of which is assigned one of 10 classes. We show the results when dimension reduction is applied

)
%
(
 
y
c
a
r
u
c
c
A

100
80
60
40

)
%
(
 
y
c
a
r
u
c
c
A

100
90
80
70
60

graph3
graph2
graph1
Unnormalized
K-scaling
L-scaling
(a) Nearly-constant degrees. 

graph6 graph7
graph8 graph9 graph10
Unnormalized
L-scaling
K-scaling
(b) Core-satellite graphs 

Figure 1: Classi ﬁcation accuracy (%). (a) Graphs with near constant w ithin class degrees. (b) Core-satellite
graphs. n = 40, m = 2000. With dimension reduction (dim ≤ 20; chosen by cross validation).

to the three types of matrix K. The performance is averaged over 10 random splits with error bar
representing one standard deviation. Figure 1 (a) shows classiﬁcation accuracy on three graphs that
were generated so that the node degrees (of either correct edges or erroneous edges) are close to
constant within each class but vary across classes. On these graphs, both K-scaling and L-scaling
signiﬁcantly improve classiﬁcation accuracy over the unno
rmalized baseline. There is not much
difference between K-scaling’s and L-scaling’s. Observe that K-scaling and L-scaling perform
differently on the graphs used in Figure 1 (b). These ﬁve grap hs have the following properties. Each
class consists of core nodes and satellite nodes. Core nodes of the same class are tightly connected
with each other and do not have any erroneous edges. Satellite nodes are relatively weakly connected
to core nodes of the same class. They are also connected to some other classes’ satellite nodes (i.e.,
introducing errors). This core-satellite model is intended to simulate real-world data in which some
data points are close to the class boundaries (satellite nodes). For graphs generated in this manner,
degrees vary within the same class since the satellite nodes have smaller degrees than the core nodes.
Our analysis suggests that L-scaling will do poorly. Figure 1 (b) shows that on the ﬁve cor e-satellite
graphs, K-scaling indeed produces higher performance than L-scaling.
In particular, K-scaling
does well even when L-scaling rather underperforms the unnormalized baseline.
Real-world data experiments

Our real-world data experiments use an image data set (MNIST) and a text data set (RCV1). The
MNIST data set, downloadable from http://yann.lecun.com/exdb/mnist/, consists of hand-written
digit image data (representing 10 classes, from digit “0 ” to
“9 ”). For our experiments, we randomly
choose 2000 images (i.e., m = 2000). Reuters Corpus Version 1 (RCV1) consists of news articles
labeled with topics. For our experiments, we chose 10 topics (ranging from sports to labor issues;
representing 10 classes) that have relatively large populations and randomly chose 2000 articles
that are labeled with exactly one of those 10 topics. To generate graphs from the image data, as is
commonly done, we ﬁrst generate the vectors of the gray-scal e values of the pixels, and produce the
edge weight between the i-th and the j -th data points Xi and Xj by wi,j = exp(−||Xi − Xj ||2/t)
where t > 0 is a parameter (RBF kernels). To generate graphs from the text data, we ﬁrst create
the bag-of-word vectors and then set wi,j based on RBF as above. As our baseline, we test the
supervised con ﬁguration by letting W + β I be the kernel matrix and using the same least squares
loss function, where we use the oracle β which is optimal.
Figures 2 (a-1,2) shows performance in relation to the number of labeled examples (n) on the MNIST
data set. The comparison of the three bold lines (representing the methods with dimension reduc-
tion) in Figure 2 (a-1) shows that when the dimensionality and α are determined by cross validation,

(a-1) MNIST, dim and a (a-2) MNIST, optimum  
dim and a
by cross validation     
75
85
85

(b-1) RCV1      
cross validation   
75

(b-2) RCV1 
optimum

)
%
(
 
y
c
a
r
u
c
c
a

75

65

55

45

75

65

55

45

65

55

45

35

65

55

45

35

Superv ised  baseline

Unnormalized  (w / o  dim   redu. )

L-scaling  (w / o  dim   redu. )

K-scaling  (w /o dim   redu. )

Unnormalized  (w /   dim   redu. )

L-scaling  (w /   dim   redu. )

K-scaling  (w /   dim   redu. )

50
30
10
90
50
10
90
50
50
30
10
#  of  labeled  ex amples
#  of  labeled  ex amples
#  of  labeled ex amples
#  of  labeled  ex .
Figure 2: Classi ﬁcation accuracy (%) versus sample size n (m = 2000). (a-1) MNIST, dim and α determined
by cross validation. (a-2) MNIST, dim and α set to the optimum. (b-1) RCV1, dim and α determined by cross
validation. (b-2) RCV1, dim and α set to the optimum.

10

K-scaling outperforms L-scaling, and L-scaling outperforms the unnormalized Laplacian. The per-
formance differences among these three are statistically signiﬁcant ( p ≤ 0.01) based on the paired
t test. The performance of the unnormalized Laplacian (with dimension reduction) is roughly con-
sistent with the performance with similar (m, n) with heuristic dimension selection in [1]. Without
dimension reduction, L-scaling and K-scaling still improve performance over the unnormalized
Laplacian. The best performance is always obtained by K-scaling with dimension reduction.

In Figure 2 (a-1), the unnormalized Laplacian with dimension reduction underperforms the un-
normalized Laplacian without dimension reduction, indicating that dimension reduction rather de-
grades performance. By comparing Figure 2 (a-1) and (a-2), we observe that this seemingly counter-
intuitive performance trend is caused by the difﬁculty of ch oosing the right dimensionality by cross
validation. Figure 2 (a-2) shows the performance at the oracle optimal dimensionality and α. As
observed, if the optimal dimensionality is known (as in (a-2)), dimension reduction improves per-
formance either with or without normalization by K-scaling and L-scaling, and all transductive
con ﬁgurations outperform the supervised baseline. We also note that the comparison of Figure 2
(a-1) and (a-2) shows that choosing good dimensionality by cross validation is much harder than
choosing α by cross validation, especially when the number of labeled examples is small. On the
RCV1 data set, the performance trend is similar to that of MNIST. Figures 2 (b-1,2) shows the per-
formance on RCV1 using the RBF kernel (t = 0.25, 100NN). In the setting of Figure 2 (b-1) where
the dimensionality and α were determined by cross validation, K-scaling with dimension reduction
generally performs the best. By setting the dimensionality and α to the optimum, the bene ﬁt of
K-scaling with dimension reduction is even clearer (Figure 2 (b-2)). Its performance differences
from the second and third best ‘L-scaling (w/ dim redu.)’ and ‘Unnormalized (w/ dim redu.)’ are
statistically signiﬁcant ( p ≤ 0.01) in both Figure 2 (b-1) and (b-2).
In our experiments, K-scaling with dimension reduction consistently outperformed others. Without
dimension reduction, K-scaling and L-scaling are not always effective. This is consistent with our
analysis. On real data, cut is not near-zero, and the effect of normalization is unclear (Section 3.1);
however, when dimension is reduced, kLS (G) − LS (G′ )k2 (corresponding to cut) can be much
smaller (Section 4), which suggests that K-scaling should improve performance.

6 Conclusion

We derived generalization bounds for learning on graphs with Laplacian regularization, using prop-
erties of the graph. In particular, we explained the importance of Laplacian normalization and di-
mension reduction for graph learning. We argued that the standard L-scaling normalization method
has the undesirable property that the normalization factors can vary signiﬁcantly within a pure com-
ponent. An alternate normalization method, which we call K-scaling, is proposed to remedy the
problem. Experiments con ﬁrm the superiority of the this nor malization scheme.

References
[1] M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine Learning, Special
Issue on Clustering:209–239, 2004.
[2] F. R. Chung. Spectral Graph Theory. Regional Conference Series in Mathematics. American Mathematical
Society, Rhode Island, 1998.
[3] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In NIPS, pages
849–856, 2001.
[4] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell,
22:888–905, 2000.
[5] M. Szummer and T. Jaakkola. Partially labeled classi ﬁca tion with Markov random walks. In NIPS 2001,
2002.
[6] T. Zhang and R. K. Ando. Analysis of spectral kernel design based semi-supervised learning. In NIPS,
2006.
[7] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Schlkopf. Learning with local and global consistency. In
NIPS 2003, pages 321–328, 2004.
[8] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic
functions. In ICML 2003, 2003.


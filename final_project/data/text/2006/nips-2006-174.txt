Large Margin Component Analysis

Lorenzo Torresani
Riya, Inc.
lorenzo@riya.com

Kuang-chih Lee
Riya, Inc.
kclee@riya.com

Abstract

Metric learning has been shown to signi ﬁcantly improve the a ccuracy of k-nearest
neighbor (kNN) classi ﬁcation. In problems involving thous ands of features, dis-
tance learning algorithms cannot be used due to over ﬁtting a nd high computa-
tional complexity. In such cases, previous work has relied on a two-step solution:
ﬁrst apply dimensionality reduction methods to the data, an d then learn a met-
ric in the resulting low-dimensional subspace. In this paper we show that better
classi ﬁcation performance can be achieved by unifying the o bjectives of dimen-
sionality reduction and metric learning. We propose a method that solves for
the low-dimensional projection of the inputs, which minimizes a metric objective
aimed at separating points in different classes by a large margin. This projection
is deﬁned by a signi ﬁcantly smaller number of parameters tha
n metrics learned
in input space, and thus our optimization reduces the risks of over ﬁtting. Theory
and results are presented for both a linear as well as a kernelized version of the
algorithm. Overall, we achieve classi ﬁcation rates simila r, and in several cases
superior, to those of support vector machines.

1

Introduction

The technique of k-nearest neighbor (kNN) is one of the most popular classi ﬁca tion algorithms.
Several reasons account for the widespread use of this method: it is straightforward to implement,
it generally leads to good recognition performance thanks to the non-linearity of its decision bound-
aries, and its complexity is independent of the number of classes. In addition, unlike most alterna-
tives, kNN can be applied even in scenarios where not all categories are given at the time of training,
such as, for example, in face veri ﬁcation applications wher e the subjects to be recognized are not
known in advance.

The distance metric deﬁning the neighbors of a query point pl ays a fundamental role in the accuracy
of kNN classi ﬁcation. In most cases Euclidean distance is us ed as a similarity measure. This choice
is logical when it is not possible to study the statistics of the data prior to classi ﬁcation or when it is
fair to assume that all features are equally scaled and equally relevant. However, in most cases the
data is distributed in a way so that distance analysis along some speci ﬁc directions of the features
space can be more informative than along others. In such cases and when training data is available
in advance, distance metric learning [5, 10, 4, 1, 9] has been shown to yield signi ﬁcant improvement
in kNN classi ﬁcation. The key idea of these methods is to appl y transformations to the data in order
to emphasize the most discriminative directions. Euclidean distance computation in the transformed
space is then equivalent to a non-uniform metric analysis in the original input space.

In this paper we are interested in cases where the data to be used for classi ﬁcation is very high-
dimensional. An example is classi ﬁcation of imagery data, w hich often involves input spaces of
thousands of dimensions, corresponding to the number of pixels. Metric learning in such high-
dimensional spaces cannot be carried out due to over ﬁtting a nd high computational complexity. In
these scenarios, even kNN classi ﬁcation is prohibitively e xpensive in terms of storage and com-
putational costs. The traditional solution is to apply dimensionality reduction methods to the data

and then learn a suitable metric in the resulting low-dimensional subspace. For example, Princi-
pal Component Analysis (PCA) can be used to compute a linear mapping that reduces the data to
tractable dimensions. However, dimensionality reduction methods generally optimize objectives un-
related to classi ﬁcation and, as a consequence, might gener ate representations that are signi ﬁcantly
less discriminative than the original data. Thus, metric learning within the subspace might lead to
suboptimal similarity measures. In this paper we show that better performance can be achieved by
directly solving for a low-dimensional embedding that optimizes a measure of kNN classi ﬁcation
performance.

Our approach is inspired by the solution proposed by Weinberger et al. [9]. Their technique learns
a metric that attempts to shrink distances of neighboring similarly-labeled points and to separate
points in different classes by a large margin. Our contribution over previous work is twofold:

1. We describe the Large Margin Component Analysis (LMCA) algorithm, a technique that
solves directly for a low-dimensional embedding of the data such that Euclidean distance
in this space minimizes the large margin metric objective described in [9]. Our approach
solves for only D · d unknowns, where D is the dimensionality of the inputs and d is the
dimensionality of the target space. By contrast, the algorithm of Weinberger et al. [9]
learns a Mahalanobis distance of the inputs, which requires solving for a D × D matrix,
using iterative semideﬁnite programming methods. This opt
imization is unfeasible for large
values of D .
2. We propose a technique that learns Mahalanobis distance metrics in nonlinear feature
spaces. Our approach combines the goal of dimensionality reduction with a novel ”ker-
nelized” version of the metric learning objective of Weinbe rger et al. [9]. We describe an
algorithm that optimizes this combined objective directly. We demonstrate that, even when
data is low-dimensional and dimensionality reduction is not needed, this technique can be
used to learn nonlinear metrics leading to signi ﬁcant impro vement in kNN classi ﬁcation
accuracy over [9].

2 Linear Dimensionality Reduction for Large Margin kNN Classiﬁ cation

In this section we brieﬂy review the algorithm presented in [ 9] for metric learning in the context
of kNN classi ﬁcation. We then describe how this approach can be generalized to compute low
dimensional projections of the inputs via a novel direct optimization.

A fundamental characteristic of kNN is that its performance does not depend on linear separability
of classes in input space: in order to achieve accurate kNN classi ﬁcation it is sufﬁcient that the
majority of the k-nearest points of each test example have correct label. The work of Weinberger
et al. [9] exploits this property by learning a linear transformation of the input space that aims at
creating consistently labeled k-nearest neighborhoods, i.e. clusters where each training example
and its k-nearest points have same label and where points differently labeled are distanced by an
additional safety margin. Speci ﬁcally, given n input examples x1 , ..., xn in ℜD and corresponding
class labels y1 , ..., yn , the technique in [9] learns the D × D transformation matrix L that optimizes
the following objective function:
ǫ(L) = X
ηij ||L(xi − xj )||2 + c X
ij
ij l

ηij (1 − yil )h(||L(xi − xj )||2 − ||L(xi − xl )||2 + 1),

(1)
where ηij ∈ {0, 1} is a binary variable indicating whether example xj is one the k-closest points
of xi that share the same label yi , c is a positive constant, yil ∈ {0, 1} is 1 iff (yi = yl ), and
h(s) = max(s, 0) is the hinge function. The objective ǫ(L) consists of two contrasting terms. The
ﬁrst aims at pulling closer together points sharing the same
label and that were neighbors in the
original space. The second term encourages distancing each example xi from differently labeled
points by an amount equal to 1 plus the distance from xi to any of its k similarly-labeled closest
points. This term corresponds to a margin condition similar to that of SVMs and it is used to improve
generalization. The constant c controls the relative importance of these two competing terms and it
can be chosen via cross validation.
Upon optimization of ǫ(L), test example xq is classi ﬁed according to the kNN rule applied to its
projection x′
q = Lxq , using Euclidean distance as metric. Equivalently, such classi ﬁcation can be

interpreted as kNN classi ﬁcation in the original input spac e under the Mahalanobis distance metric
induced by matrix M = LT L. Although Equation 1 is non-convex in L, it can be rewritten as
a semideﬁnite program ǫ(M) in terms of the metric M [9]. Thus, optimizing the objective in M
guarantees convergence to the global minimum, regardless of initialization.

When data is very high-dimensional, minimization of ǫ(M) using semideﬁnite programming meth-
ods is impractical because of slow convergence and over ﬁtti ng problems. In such cases [9] propose
applying dimensionality reduction methods, such as PCA, followed by metric learning within the
resulting low-dimensional subspace. As outlined above, this procedure leads to suboptimal metric
learning. In this paper we propose an alternative approach that solves jointly for dimensionality
reduction and metric learning. The key idea is to choose the transformation L in Equation 1 to be a
nonsquare matrix of size d×D , with d << D . Thus L deﬁnes a mapping from the high-dimensional
input space to a low-dimensional embedding. Euclidean distance in this low-dimensional embed-
ding is equivalent to Mahalanobis distance in the original input space under the rank-deﬁcient metric
M = LT L (M has now rank at most d).
Unfortunately, optimization of ǫ(M) subject to rank-constraints on M leads to a minimization prob-
lem that is no longer convex [8] and that is awkward to solve. Here we propose an approach for
minimizing the objective that differs from the one used in [9]. The idea is to optimize Equation 1
directly with respect to the nonsquare matrix L. We argue that minimizing the objective with respect
to L rather than with respect to the rank-deﬁcient D × D matrix M, offers several advantages. First,
our optimization involves only d·D rather than D2 unknowns, which considerably reduces the risk of
over ﬁtting. Second, the optimal rectangular matrix L computed with our method automatically sat-
is ﬁes the rank constraints on M without requiring the solution of difﬁcult constrained min imization
problems. Although the objective optimized by our method is also not convex, we experimentally
demonstrate that our solution converges consistently to better metrics than those computed via the
application of PCA followed by subspace distance learning (see Section 4).

We minimize ǫ(L) using gradient-based optimizers, such as conjugate gradient methods. Differen-
tiating ǫ(L) with respect to the transformation matrix L gives the following gradient for the update
rule:

ηij (xi − xj )(xi − xj )T +

∂ ǫ(L)
∂L

= 2L X
ij
2cL X
ij l

ηij (1 − yil ) (cid:2)(xi − xj )(xi − xj )T − (xi − xl )(xi − xl )T (cid:3)
h′ (||L(xi − xj )||2 − ||L(xi − xl )||2 + 1)

(2)

We handle the non-differentiability of h(s) at s = 0, by adopting a smooth hinge function as in [8].

3 Nonlinear Feature Extraction for Large Margin kNN Classiﬁcat ion

In the previous section we have described an algorithm that jointly solves for linear dimensionality
reduction and metric learning. We now describe how to ”kerne lize” this method in order to compute
non-linear features of the inputs that optimize our distance learning objective. Our approach learns
a low-rank Mahalanobis distance metric in a high dimensional feature space F , related to the inputs
by a nonlinear map φ : ℜD → F . We restrict our analysis to nonlinear maps φ for which there exist
kernel functions k that can be used to compute the feature inner products without carrying out the
map, i.e. such that k(xi , xj ) = φT
i φj , where for brevity we denoted φi = φ(xi ).
We modify our objective ǫ(L) by substituting inputs xi with features φ(xi ) into Equation 1. L is now
a transformation from the space F into a low-dimensional space ℜd . We seek the transformation L
minimizing the modi ﬁed objective function ǫ(L).
The gradient in feature space can now be written as:
∂ ǫ(L)
= 2 X
∂L
ij
2c X
ij l

ηij (1 − yil )h′ (sij l )L (cid:2)(φi − φj )(φi − φj )T − (φi − φl )(φi − φl )T (cid:3) (3)

ηij L(φi − φj )(φi − φj )T +

where sij l = (||L(φi − φj )||2 − ||L(φi − φl )||2 + 1).

Let Φ = [φ1 , ..., φn ]T . We consider parameterizations of L of the form L = ΩΦ, where Ω is some
matrix allowing us to write L as a linear combination of the feature points. This form of nonlinear
map is analogous to that used in kernel-PCA and it allows us to parameterize the transformation L in
terms of only d · n parameters, the entries of the matrix Ω. We now introduce the following Lemma
which we will later use to derive an iterative update rule for L.

Lemma 3.1 The gradient in feature space can be computed as ∂ ǫ(L)
∂L = ΓΦ, where Γ depends on
features φi solely in terms of dot products (φT
i φj ).

ηij (ki − kj )(φi − φj )T +

ηij (1 − yil )h′ (sij l ) (cid:2)(ki − kj )(φi − φj )T − (ki − kl )(φi − φl )T (cid:3)
i Φ +
ηij hE
(ki−kj )
i

Proof Deﬁning ki = Φφi = [k(x1 , xi ), ..., k(xn , xi )]T , non-linear feature projections can be com-
puted as Lφi = ΩΦφi = Ωki . From this we derive:
∂ ǫ(L)
= 2Ω X
∂L
ij
2cΩ X
ij l
= 2Ω X
ij
2cΩ X
ij l
i = [0, ..., v, 0, ..0] is the n × n matrix having vector v in the i-th column and all 0 in the
where Ev
other columns. Setting
Γ = 2Ω X
ij
2cΩ X
ij l

ηij (1 − yil )h′ (sij l ) hE
(ki−kj )
i

ηij (1 − yil )h′ (sij l ) hE
(ki−kj )
i

(ki−kl )
i

(ki−kl )
+ E
l

ηij hE
(ki−kj )
i

(ki−kj )
− E
j

(ki−kl )
− E
i

(ki−kl )
+ E
l

(ki−kj )
− E
j

− E

(ki−kj )
− E
j

i +

(ki−kj )
− E
j

i Φ

i

(4)

proves the Lemma.

This result allows us to implicitly solve for the transformation without ever computing the features
in the high-dimensional space F : the key idea is to iteratively update Ω rather than L. For example,
using gradient descent as optimization we derive update rule:
(cid:12)(cid:12)(cid:12)(cid:12)L=Lold
∂ ǫ(L)
∂L
where λ is the learning rate. We carry out this optimization by iterating the update Ω ← (Ω − λΓ)
until convergence. For classi ﬁcation, we project points on to the learned low-dimensional space by
exploiting the kernel trick: Lφq = Ωkq .

= [Ωold − λΓold ] Φ = ΩnewΦ

Lnew = Lold − λ

(5)

4 Experimental results

We compared our methods to the metric learning algorithm of Weinberger et al.
[9], which we
will refer to as LMNN (Large Margin Nearest Neighbor). We use KLMCA (kernel-LMCA) to
denote the nonlinear version of our algorithm. In all of the experiments reported here, LMCA was
initialized using PCA, while KLMCA used the transformation computed by kernel-PCA as initial
guess. The objectives of LMCA and KLMCA were optimized using the steepest descent algorithm.
We experimented with more sophisticated minimization techniques, including the conjugate gradient
method and the Broyden-Fletcher-Goldfarb-Shanno quasi-Newton algorithm [6], but no substantial
improvement in performance or speed of convergence was achieved. The KLMCA algorithm was
implemented using a Gaussian RBF kernel. The number of nearest neighbors, the weight c in
Equation 1, and the variance of the RBF kernel, were all automatically tuned using cross-validation.

The ﬁrst part of our experimental evaluation focuses on clas si ﬁcation results on datasets with high-
dimensionality, Isolet, AT&T Faces, and StarPlus fMRI:

%
 
r
o
r
r
e
 
g
n
i
n
i
a
r
t

%
 
r
o
r
r
e
 
g
n
i
t
s
e
t

12

10

8

6

4

2

0
5

12

10

8

6

4

2

0
5

(a)

(b)

AT&T Faces

Isolet

fMRI

PCA + LMNN
LMCA + kNN
KLMCA + kNN

10
25
20
15
projection dimensions

30

AT&T Faces

PCA + LMNN
LMCA + kNN
KLMCA + kNN

10
25
20
15
projection dimensions

30

30

25

20

15

10

%
 
r
o
r
r
e
 
g
n
i
n
i
a
r
t

5

0
0

40

30

20

10

%
 
r
o
r
r
e
 
g
n
i
t
s
e
t

0
0

PCA + LMNN
LMCA + kNN
KLMCA + kNN

50
150
100
projection dimensions

200

Isolet

PCA + LMNN
LMCA + kNN
KLMCA + kNN

50
150
100
projection dimensions

200

%
 
r
o
r
r
e
 
g
n
i
n
i
a
r
t

%
 
r
o
r
r
e
 
g
n
i
t
s
e
t

15

10

5

0
0

16

14

12

10

8

6
0

PCA + LMNN
LMCA + kNN
KLMCA + kNN

10
20
projection dimensions

30

fMRI

PCA + LMNN
LMCA + kNN
KLMCA + kNN

10
20
projection dimensions

30

Figure 1: Classi ﬁcation error rates on the high-dimensional datasets Isolet, AT&T F aces and StarPlus fMRI
for different projection dimensions. (a) Training error. (b) Testing error.

• Isolet1 is a dataset of speech features from the UC Irvine repository, consisting of 6238
training examples and 1559 testing examples with 617 attributes. There are 26 classes
corresponding to the spoken letters to be recognized.
• The AT&T Faces2 database contains 10 grayscale face images of each of 40 distinct sub-
jects. The images were taken at different times, with varying illumination, facial expres-
sions and poses. As in [9], we downsampled the original 112 × 92 images to size 38 × 31,
corresponding to 1178 input dimensions.
• The StarPlus fMRI3 dataset contains fMRI sequences acquired in the context of a cognitive
experiment. In these trials the subject is shown for a few seconds either a picture or a sen-
tence describing a picture. The goal is to recognize the viewing activity of the subject from
the fMRI images. We reduce the size of the data by considering only voxels corresponding
to relevant areas of the brain cortex and by averaging the activity in each voxel over the
period of the stimulus. This yields data of size 1715 for subject ”04847,” on which our
analysis was restricted. A total number of 80 trials are available for this subject.

Except for Isolet, for which a separate testing set is speci ﬁ ed, we computed all of the experimental
results by averaging over 100 runs of random splitting of the examples into training and testing sets.
For the fMRI experiment we used at each iteration 70% of the data for training and 30% for testing.
For AT&T Faces, training sets were selected by sampling 7 images at random for each person. The
remaining 3 images of each individual were used for testing.

Unlike LMCA and KLMCA, which directly solve for low-dimensional embeddings of the input
data, LMNN cannot be run on datasets of dimensionalities such as those considered here and must
be trained on lower-dimensional representations of the inputs. As in [9], we applied the LMNN
algorithm on linear projections of the data computed using PCA. Figure 1 summarizes the training
and testing performances of kNN classi ﬁcation using the met rics learned by the three algorithms for
different subspace dimensions. LMCA and KLMCA give considerably better classi ﬁcation accu-
racy than LMNN on all datasets, with the kernelized version of our algorithm always outperforming
the linear version. The difference in accuracy between our algorithms and LMNN is particularly
dramatic when a small number of projection dimensions is used. In such cases, LMNN is unable
to ﬁnd good metrics in the low-dimensional subspace compute d by PCA. By contrast, LMCA and
KLMCA solve for the low-dimensional subspace that optimizes the classi ﬁcation-related objective

1Available at http://www.ics.uci.edu/∼mlearn/MLRepository.html
2Available at http://www.cl.cam.ac.uk/Research/DTG/attarchive/facedatabase.html
3Available at http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-81/www/

(a)

(b)

(c)

Figure 2: Image reconstruction from PCA and LMCA features. (a) Input images. (b) Reconstruc-
tions using PCA (left) and LMCA (right). (c) Absolute difference between original images and
reconstructions from features for PCA (left) and LMCA (right). Red denotes large differences, blue
indicates similar grayvalues. LMCA learns invariance to effects that are irrelevant for classi ﬁcation:
non-uniform illumination, facial expressions, and glasses (training data contains images with and
without glasses for same individuals).

of Equation 1, and therefore achieve good performance even when projecting to very low dimen-
sions. In our experiments we found that all three classi ﬁcat
ion algorithms (LMNN, LMCA+kNN,
and KLMCA+kNN) performed considerably better than kNN using the Euclidean metric in the PCA
and KPCA subspaces. For example, using d = 10 in the AT&T dataset, kNN gives a 10.9% testing
error rate when used on the PCA features, and a 9.7% testing error rate when applied to the nonlinear
features computed by KPCA.

While LMNN is applied to features in a low-dimensional space, LMCA and KLMCA learn a low-
rank metric directly from the high-dimensional inputs. Consequently the computational complexity
of our algorithms is higher than that of LMNN. However, we have found that LMCA and KLMCA
converge to a minimum quite rapidly, typically within 20 iterations, and thus the complexity of these
algorithms has not been a limiting factor even when applied to very high-dimensional datasets. As a
reference, using d = 10 and K = 3 on the AT&T dataset, LMNN learns a metric in about 5 seconds,
while LMCA and KLMCA converge to a minimum in 21 and 24 seconds, respectively.

It is instructive to look at the preimages of LMCA data embeddings. Figure 2 shows comparative re-
constructions of images obtained from PCA and LMCA features by inverting their linear mappings.
The PCA and LMCA subspaces in this experiment were computed from cropped face images of size
50 × 50 pixels, taken from a set of consumer photographs. The dataset contains 2459 face images
corresponding to 152 distinct individuals. A total of d = 125 components were used. The subjects
shown in Figure 2 were not included in the training set. For a given target dimensionality, PCA has
the property of computing the linear transformation minimizing the reconstruction error under the
L2 norm. Unsurprisingly, the PCA face reconstructions are extremely faithful reproductions of the
original images. However, PCA accurately reconstructs also visual effects, such as lighting varia-
tions and changes in facial expressions, that are unimportant for the task of face veri ﬁcation and that
might potentially hamper recognition. By contrast, LMCA seeks a subspace where neighboring ex-
amples belong to the same class and points differently labeled are separated by a large margin. As a
result, LMCA does not encode effects that are found to be insigni ﬁcant for classi ﬁcation or that vary
largely among examples of the same class. For the case of face veri ﬁcation, LMCA de-emphasizes
changes in illumination, presence or absence of glasses and smiling expressions (Figure 2).

When the input data does not require dimensionality reduction, LMNN and LMCA solve the same
optimization problem, but LMNN should be preferred over LMCA in light of its guarantees of
convergence to the global minimum of the objective. However, even in such cases, KLMCA can be
used in lieu of LMNN in order to extract nonlinear features from the inputs. We have evaluated this
use of KLMCA on the following low-dimensional datasets from the UCI repository: Bal, Wine, Iris,
and Ionosphere. All of these datasets, except Ionosphere, have been previously used in [9] to assess
the performance of LMNN. The dimensionality of the data in these sets ranges from 4 to 34. In order

BAL − training error %
14.1

WINE − training error %
30

IRIS − training error %
4.3

IONO − training error %
15.7

10

6.5

(a)

kNN w/
EUCL

LMNN

KLMCA
+ kNN

17.1

1.1

kNN w/
EUCL

LMNN

KLMCA
+ kNN

3.5

3

7.6

2.3

kNN w/
EUCL

LMNN

KLMCA
+ kNN

kNN w/
EUCL

LMNN

KLMCA
+ kNN

BAL − testing error %
14.4

WINE − testing error %
30.1

9.7

7.8

6.7

17.6

19

2.6

IRIS − testing error %
4.7

4.4

4.3

3.4

IONO − testing error %
16.5

13.7

5.8

(b)

kNN w/
EUCL

LMNN

KLMCA
+ kNN

SVM

kNN w/
EUCL

LMNN

KLMCA
+ kNN

SVM

kNN w/
EUCL

LMNN

KLMCA
+ kNN

SVM

kNN w/
EUCL

LMNN

KLMCA
+ kNN

Figure 3: kNN classi ﬁcation accuracy on low-dimensional da tasets: Bal, Wine, Iris, and Ionosphere.
(a) Training error. (b) Testing error. Algorithms are kNN using Euclidean distance, LMNN [9], kNN
in the nonlinear feature space computed by our KLMCA algorithm, and multiclass SVM.

to compare LMNN with KLMCA under identical conditions, KLMCA was restricted to compute a
number of features equal to the input dimensionality, although in our experience using additional
nonlinear features often results in better classi ﬁcation p erformance. Figure 3 summarizes the results
of this comparison. Again, we averaged the errors over 100 runs with different 70/30 splits of the
data for training and testing. On all datasets except on Wine, for which the mapping to the high-
dimensional space seems to hurt performance (note also the high error rate of SVM), KLMCA gives
better classi ﬁcation accuracy than LMNN. Note also that the error rates of KLMCA are consistently
lower than those reported in [9] for SVM under identical training and testing conditions.

5 Relationship to other methods

Our method is most similar to the work of Weinberger et al. [9]. Our approach is different in focus
as it speci ﬁcally addresses the problem of kNN classi ﬁcatio
n of very high-dimensional data. The
novelty of our method lies in an optimization that solves for data reduction and metric learning
simultaneously. Additionally, while [9] is limited to learning a global linear transformation of the
inputs, we describe a kernelized version of our method that extracts non-linear features of the inputs.
We demonstrate that this representation leads to signi ﬁcan t improvements in kNN classi ﬁcation both
on high-dimensional as well as on low-dimensional data. Our approach bears similarities with Lin-
ear Discriminant Analysis (LDA) [2], as both techniques solve for a low-rank Mahalanobis distance
metric. However, LDA relies on the assumption that the class distributions are Gaussian and have
identical covariance. These conditions are almost always violated in practice. Like our method,
the Neighborhood Component Analysis (NCA) algorithm by Goldberger et al. [4] learns a low-
dimensional embedding of the data for kNN classi ﬁcation usi ng a direct gradient-based approach.
NCA and our method differ in the deﬁnition of the objective fu nction. Moreover, unlike our method,
NCA provides purely linear embeddings of the data. A contrastive loss function analogous to the
one used in this paper is adopted in [1] for training a similarity metric. A siamese architecture con-
sisting of identical convolutional networks is used to parameterize and train the metric. In our work
the metric is parameterized by arbitrary nonlinear maps for which kernel functions exist. Recent
work by Globerson and Roweis [3] also proposes a technique for learning low-rank Mahalanobis
metrics. Their method includes an extension for computing low-dimensional non-linear features us-
ing the kernel trick. However, this approach computes dimensionality reductions through a two-step
solution which involves ﬁrst solving for a possibly full-ra nk metric and then estimating the low-rank
approximation via spectral decomposition. Besides being suboptimal, this approach is impractical
for classi ﬁcation problems with high-dimensional data, as
it requires solving for a number of un-
knowns that is quadratic in the number of input dimensions. Furthermore, the metric is trained with
the aim of collapsing all examples in the same class to a single point. This task is difﬁcult to achieve
and not strictly necessary for good kNN classi ﬁcation perfo rmance. The Support Vector Decompo-

sition Machine (SVDM) [7] is also similar in spirit to our approach. SVDM optimizes an objective
that is a combination of dimensionality reduction and classi ﬁcation. Speci ﬁcally, a linear mapping
from input to feature space and a linear classi ﬁer applied to feature space, are trained simultane-
ously. As in our work, results in their paper demonstrate that this joint optimization yields better
accuracy than that achieved by learning a low-dimensional representation and a classi ﬁer separately.
Unlike our method, which can be applied without any modi ﬁcat
ion to classi ﬁcation problems with
more than two classes, SVDM is formulated for binary classi ﬁ cation only.

6 Discussion

We have presented a novel algorithm that simultaneously optimizes the objectives of dimensionality
reduction and metric learning. Our algorithm seeks, among all possible low-dimensional projec-
tions, the one that best satis ﬁes a large margin metric objec tive. Our approach contrasts techniques
that are unable to learn metrics in high-dimensions and that must rely on dimensionality reduction
methods to be ﬁrst applied to the data. Although our optimiza tion is not convex, we have exper-
imentally demonstrated that the metrics learned by our solution are consistently superior to those
computed by globally-optimal methods forced to search in a low-dimensional subspace.

The nonlinear version of our technique requires us to compute the kernel distance of a query point to
all training examples. Future research will focus on rendering this algorithm ”sparse”. In addition,
we will investigate methods to further reduce over ﬁtting wh en learning dimensionality reduction
from very high dimensions.

Acknowledgments

We are grateful to Drago Anguelov and Burak Gokturk for discussion. We thank Aaron Hertzmann
and the anonymous reviewers for their comments.

References

[1] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to
face veriﬁcation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2005.
[2] R. A. Fisher. The use of multiple measurements in taxonomic problems. Ann. Eugenics, 7:179–188, 1936.
In Y. Weiss, B. Sch ¨olkopf, and
[3] A. Globerson and S. Roweis. Metric learning by collapsing classes.
J. Platt, editors, Advances in Neural Information Processing Systems 18. MIT Press, Cambridge, MA,
2006.
[4] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In
L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17,
2005.
[5] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation.
Pattern Analysis and Machine Intelligence (PAMI), 18:607–616, 1996.
[6] A. Mordecai. Nonlinear Programming: Analysis and Methods. Dover Publishing, 2003.
[7] F. Pereira and G. Gordon. The support vector decomposition machine. In Proceedings of the International
Conference on Machine Learning (ICML), 2006.
[8] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.
In Proceedings of the 22nd International Conference on Machine Learning (ICML), 2005.
[9] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest neighbor
classi ﬁcation. In Y. Weiss, B. Sch ¨olkopf, and J. Platt, editors, Advances in Neural Information Processing
Systems 18, 2006.
[10] E. P. Xing, A. Y. Ng, M. I. Jordan, , and S. Russell. Distance metric learning, with application to clustering
with side-information. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14, 2002.

IEEE Transactions on


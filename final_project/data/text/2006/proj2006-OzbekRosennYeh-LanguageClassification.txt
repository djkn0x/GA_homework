Language Classification in
Multilingual Documents
Gorkem Ozbek, Itamar Rosenn, Eric Yeh

Summary
We  investigate  the  use  of  machine  learning  techniques
for  language classification  in a multilingual setting. We
consider  three  contexts  in  which  this  task  may  be
performed:  identifying  the  language  of  monolingual
documents ,   identifying   the   language   of   individual
tokens,  and  finally  identifying  and  correctly  classifying
spans  of  monolingual  text  in  multilingual  documents.
Our broad goal  is  to achieve  the  third  task with optimal
eff ic iency   and   accuracy .   We   pursue   th is   goa l   by
building  and  examining  various morphologically-based
classification methods  that  attempt  to  classify, within  a
document,  the  language  identity  of  individual  words
whose  language  is  not  known,  thereby  approximating  a
potentially multilingual setting.

1 Introduction
In today’s increasingly integrated and multilingual
wo r ld ,   wh en   d ev e lop ing   n a tu r a l   l angu ag e
technology,  one  cannot  always  assume  that  the
text  one  will  encounter  will  be  solely  in  one
language .   Ma in ta in ing   th i s   a s sump t ion   and
uniformly  applying  single-language-specific  text-
processing  techniques  may  result  in  erroneous
hand l ing   o f   te rms   in   o the r   languages .   Fo r
example,  in  information  retrieval  tasks,  single-
language  approaches  can  result  in  lower precision
a n d   r e c a l l   s c o r e s :   a n   E n g l i s h  
l a n g u a g e
preprocessor may not effectively capture subtleties
relevant  to  languages  such  as  Turkish,  where
linguistic  structure  differs  drastically  even  at  the
word level.

Traditionally,  the  task  of  language  identification
has  been  applied  in  settings  where  the  entire
document  is  assumed  to  be  in  a  single  language.
However,  with  the  advent  of  the  World  Wide
Web,  instances  of  mixed-language  documents
have  become  more  prevalent.  For  example,  the
online   edition   of   the   German   magazine   D e r
Sp iege l   uses  a  sidebar  of  text  written  in  English
[see  http://www.spiegel.de/].  In  addition,  phrases
are  often  appropriated  from  one  language  into  the
context  of  another,  such  as  the  English  phrase
“sexiest  man  alive”,  which  appears  in  a  D e r

Spiegel  article  about George Clooney. The  reality
o f   mu l t i l ingua l   tex t   in t roduces   the   task   o f
identifying when  the  language of a  span of  text  in
a  document  differs  from  the  primary  language  of
that document; to our knowledge, this multilingual
identification  task  has  not  yet  been  sufficiently
explored.  At  one  extreme,  this  problem  can  be
reduced to identifying likely language of origin for
a  single  observed  token  in  a  possibly multilingual
setting; this approach motivates our present work.

2 Feature Engineering
Previous Work
A  large  body  of  previous  language  identification
wo rk   ha s   focu sed   on   s ta t i s t ica l   c la s s i f ie r s
primarily  operating  over  character-level,  non-
linguistically  motivated  features,  such  as  n-gram
cha rac te r   mode ls   [1 ,   2 ,   3 ] .   These   me thods
generally perform well only after a certain number
of  characters  has  been  seen  by  the  classifier  [3].
However,  as  mentioned  previously,  these  efforts
have  focused  on  instances  where  the  text  to  be
classified  is considered  to be of a single  language.
Furthermore,  in  a  possibly  multilingual  setting,
these methods would  also  be  unreliable  because  a
single-language  portion  of  the  text  may  be  too
shor t   to   con ta in   suff ic ien t   charac ters   for   the
character-based classification methods.

Our Approach
The   charac ter-based   approach   ach ieves   near-
perfect  classification  accuracy  within  about  100-
200  characters,  without  exploiting  any  features
that  are  idiosyncratic  to  linguistic  characteristics
of  different  languages.  In  the  hopes  of  achieving
high-accuracy  classification within  a  short  textual
span,  as  is  needed  for multi-lingual  classification,
we  would  like  to  develop  a  classification  system
based   on   linguistic   factors   that   differ   among
languages,  using  light-weight  features  that  can
accrue  significant  statistics  within  short  word
spans  of  text.  Using  morphological  features  is  an
obvious choice given these desired criteria.

Linguistic  theory  teaches us  that word  tokens of a
given  language are comprised of smaller elements
called  morphemes  [see  figure  6],  which  are  the
smallest  components  of  a  language  that  carry

semantic  value.  Thus,  a  language  model  may  be
es t ima ted   us ing   morphemes   as   lex ica l   un i ts ,
instead of the words in which they appear.

algorithm  assumes  that  morphemes  fall  into  two
ca tegor ies :   s tems   and   aff ixes ,   and   the   la t ter
category is divided into prefixes and suffixes.

We restrict our inquiry to four languages: English,
Finnish, German, and Turkish. Our approach  is  to
construct a  feature  set  for  each of  these  languages
by  obtaining  a  morpheme-based  language  model
e s t ima ted   f rom   a   un i l ingua l   co rpu s   o f   the
language. The prima facie difficulty with this goal
is   that   construction   of   a   broad ,   linguistically
informed   morpho log ica l   lex icon   for   a   g iven
language  requires  a  considerable  amount  of work
by  trained  experts. Thus,  for our purposes  and  for
the   genera l   task   of   eng ineer ing   a   successfu l
morpheme-based  language  classifier,  obtaining
such  a  lexicon  is  prohibitively  costly,  particularly
in  the  context  of  many  possible  languages  or
highly evolving languages.

An   a l te rna t ive   app roach   ha s   recen t ly   been
explored  in  the  literature:  designing  generative,
minimally  supervised  algorithms  that  attempt  to
automatically  discover  morphemes  in  a  corpus.
We   re ly   on   one   such   a lgor i thm   to   cons truc t
individual  feature  sets  –  morpheme  “language
models” – for each of our languages in an efficient
and unsupervised manner. The algorithm has been
developed  by  Mathias  Creutz,  who  demonstrates
its high accuracy for various languages [5].

Creutz’s Algorithm
T h e   a l g o r i t hm   u s e s   s e gm e n t a t i o n   a n d  
i s
formulated within a probabilistic framework. Two
features   of   the   algorithm   enable   it   to   handle
var ious   morpho log ica l ly   d ispara te   languages ,
making it especially appropriate for our purposes:
First ,   the   algorithm   treats   words   as   arbitrary
sequences of alternating stems and affixes, making
it  more  flexible  with  respect  to  languages  having
different  levels  of  inflection.  Within  our  own
framework,  Turkish  and  Finnish  are  far  more
inflective  than  German  and  English.  Second,  the
a lgo r i thm   cons ide rs   sequen t ia l   dependenc ies
between  functional  categories  of  morphemes  (an
a p p r o a c h   k n ow n   a s   mo rpho tac t ic s ),  which
increases  its  accuracy  in  an  arbitrary multilingual
setting  and  provides  us  with  a  richer  feature  set
than   a   s imp le   co l lec t ion   of   morphemes .   The

Creu tz’s   a lgor i thm   uses   an   HMM   to   mode l
morpheme   sequences ,   without   assuming   prior
k n ow l e d g e   o f  
t h e   s e gm e n t s   (m o r p h em e s )
themselves,  nor  of  their  individual  functional
categories.  The  algorithm  performs  a  baseline
segmen ta t ion ,   then   es t ima tes   probab i l i t ies   of
observing  a  particular  morph  given  its  category,
and the probability of a transition from one morph
category to another, using EM.

Our Features
The main advantages  to Creutz’s approach for our
purposes   are   tha t   i t   a l lows   us   to   ex trac t   an
estimated morphological and morphotactic feature
set  for  each  language, without  any  supervision  or
p r io r   l ingu is t ic   know ledge .   We   deve lop   an
analyzer  that  uses  Creutz’s  algorithm  to  identify
mo rpho log i c a l   un i t s   and   th e i r   app rop r i a t e
func t iona l   ca tego r ies   w i th in   each   language -
specific training document.

3 Methodology
Corpora
Each   o f   ou r   mode ls ,   d iscussed   be low ,   uses
documents made  available  for Morpho  Challenge
2007  [4].  The  site  provides  sets  of  unilingual
documen ts   in   Eng l ish ,   F inn ish ,   German ,   and
Turkish, which we use for train and test corpora in
each of our language identification procedures.

Baseline
In  order  to  establish  a  baseline  for  the  language
identification  task,  we  implement  a  Naïve  Bayes
classifier  using  n-gram  (i.e.  unigram,  bigram  and
trigram)  character  models.  In  accordance  with
current  state  of  the  art  language  classification
systems, we  choose Naïve Bayes  for our Baseline
and morphological models. We also use Laplacian
noise  modeling  throughout.  For  the  baseline,  our
n-gram  character models  are  built  from word  lists
constructed  from  English,  Finnish,  German,  and
Turkish  training  corpora.  Each  word  list  contains
unique words  (i.e. word  types)  encountered  in  the

corpus  for  one  of  the  four  languages,  along  with
the  token frequencies for  the words. The classifier
is  then  trained  with  these  character  models  for
each of the four languages.

Test  documents  in  each  language,  similar  to  the
t ra in ing   documen ts ,   a re   a lso   ob ta ined   f rom
Morpho   Cha l lenge .   Accuracy   vs .   number   of
corpus characters read is measured to establish the
success   of   the   baseline   approach   for   varying
amounts  of  data.  The  tokenized  version  of  the
corpus,  in  the  form  of  a  wordlist,  is  also  used  to
examine  baseline  performance  with  respect  to
classifying individual tokens.

Morph Classification
As a first attempt to improve upon the character n-
gram   approach   of   the   base l ine   and   ob ta in   a
classification  system  appropriate  for  multilingual
settings,  we  obtain  a  morpheme  feature  set  for
each language by applying the Creutz algorithm to
each  individual-language  training  document.  We
limit our feature set only  to a morpheme count for
each   language ,   wh ich   se rve s   a s   a   s imp le
morphological “language model” for the language.
The  morpheme  count  list  contains  a  list  of  the
unique  morphemes  found  in  the  document,  along
the   f requency   o f   each   mo rpheme .   We   then
implement  a  Naïve  Bayes  classifier  using  our
morpheme counts as features. In the testing phase,
each   language-spec if ic   tes t   documen t   is   f irs t
analyzed  using  the  Creutz  procedure  in  order  to
identify  best  guesses  of  the  correct  morpheme
segmentation  of  each  word  in  the  test  document.
We  then  apply  our  Naïve-Bayes  classifier,  along
with  the morpheme  feature  list  for  each  language,
to  the  segmented  test  document.  The  classifier
iden t if ies   the   language   of   each   word   in   the
document  according  to  its  maximum  likelihood
classification.   Note  that  the  classifier  does  not
rely on the assumption that the entire document or
even  sequences  of  words  of  the  document  are  all
in  one  language.  Therefore,  although  the  test
documents  are  all  unilingual,  the  classifier  itself
performs  exactly  as  it  would  in  a  multilingual
setting  (restricted  to  our  candidate  languages),
even  in  the  extreme  case  where  the  identity  of
each   wo rd   was   en t i re ly   independen t   o f   the
identities of other words in close proximity to it.

Morph + Morphotactics Classification
In   add i t ion   to   the   s imp le   morpheme   fea ture
c lass if ier ,   we   deve lop   a   c lass if ier   tha t   takes
advantage of the morphotactic information that the
Creutz  analyzer  provides.  For  this  classifier,  we
also  obtain  a  feature  set  of  3-gram  morpheme
category  sequences  for  each  language.  This  set  is
ob ta ined   by   exam in ing   the   language-spec if ic
training  documents  after  they  have  been  Creutz-
analyzed,  and  creating  a  count  of  all  3-gram
morpho log ica l   ca tegory   sequences   w i th in   the
document.  Thus,  for  each  of  our  four  candidate
languages,  the Creutz  analyzer  provides  us with  a
mo rpho log ica l   “ language   mode l”   ba sed   on
morpheme   frequency   count   and   morphotactic
sequence  frequency. We  then  implement  a  Naïve
Bayes  classifier  to  use  the  original  morpheme-
count  as  a  feature  set  together  with  our  new  3-
gram morphotactic feature set, for each of our four
languages.   As  before,  in  the  testing  phase,  each
language-specific  test  document  is  first  analyzed
using  the  Creutz  procedure  in  order  to  discover
morphemes and their functional categories (prefix,
suffix,  stem).  Then,  the  Naïve-Bayes  classifier,
a long   w i th   the   morpheme   and   morpho tac t ic
language   fea tu res ,   is   app l ied   to   the   tagged
document.  The  classifier  identifies  each  word
according  to  its  maximum  likelihood  language
iden t i ty ,   us ing   bo th   the   mo rpho log ica l   and
morphotactic features of the word.

Filtering
In  an  attempt  to  reduce  computation  time,  as well
as  to  improve  the  accuracy  of  our  morphological
“language models”, we also applied filtering to the
training  documents.  For  each  language-specific
training  document,  we  obtained  a  new  document
that  filtered  out words  occurring  less  than  5,  100,
and  1000  times.  We  performed  new  rounds  of
testing  classification  for  each of  these  filter  levels
to see if efficiency and performance are increased.

4 Results and Discussion
When  attempting  to  identify  the  language  of  a
single  document,  the  performance  of  the  baseline
character  n-gram  model  increases  as  the  number
of  characters  observed  increases  (see  Figure  1),
resembling  performance  curves  seen  in  previous

studies. Baseline  identification  of  single  tokens  is
far   less   successfu l ,   as   sugges ted   by   the   low
accuracies  observed  in  figure  1 when  50  or  fewer
characters have been seen.

The  results  of  the Naïve Bayes  classification  task
u s i n g   o u r   m o r p h o l o g i c a l   f e a t u r e   s e t s ,  
i n
comparison   to   the   results   of   baseline   n-gram
Naïve  Bayes  classification,  are  shown  in  Figures
2-5 .   The   f irs t   no tab le   observa t ion   is   tha t   in
u n f i l t e r e d   s e t t i n g s ,  
t h e   m o r p h em e - f e a t u r e
classifier  (Morph)  and  the morpheme-feature plus
morphotactic-feature  classifier  (Morph  +  Mts)
a ch i ev e   a t   l e a s t   a   s l igh t   imp rov em en t   in
performance  over  the  baseline.  Our  classifiers
achieve  the  most  success  in  classifying  Turkish
wo rds :   Mo rph   does   s l igh t ly   be t te r   than   the
baseline,  which  is  roughly  at  80%,  and  Morph  +
M ts   ach ieves   very   c lose   to   100%   accuracy .
Filtering   does   not   greatly   alter   these   results ,
suggesting  that  Creutz’s  method  gives  a  reliable
morphological-morphotactic model of Turkish.

With  respect  to  Finnish  and  German,  unfiltered
classification  using  Morph  and  Morph  +  Mts
ach ieve s   s l igh t ly   h ighe r   accu racy   than   the
baseline,  but  not  near  the  levels  of  success  seen
with  Turkish.  Furthermore,  adding  morphotactic
features  does  not  increase  performance  beyond
classification  using  only  morphemes.  Filtering
seems  to  reduce  accuracy  somewhat  with  both  of
these  languages,  suggesting  that  the  full  training
documents  supplied  to  Creutz’s  algorithm  yield
the most  information  in  terms  of  a morphological
language model.

The  results  for English are  the most discouraging.
As  Figure  2  shows,  neither  Morph  nor  Morph  +
Mts  classification  achieves  accuracy  scores  as
high as baseline, with the worst performance given
by  Morph  +  Mts,  with  only  40%  accuracy.  Once
filtering  is  introduced,  Morph  +  Mts  accuracy
gradually  climbs  up  toward  the  accuracy  of  the
other  two  classification  systems.  This  suggests
that the 3-gram morphotactic model is particularly
unsuitable  for  English  morphotactics,  since  at  a
high level of filtering the richness of Morphotactic
feature   set   disappears ,   and   the   system   works
similar to simple Morph classification.

5 Future Work
Our success on the Turkish test document suggests
that the true potential for better performance stems
from  constructing  an  appropriate  mo rpho tac t ic
model for a language.  If the near-perfect accuracy
of  Morph  +  Mts  in  classifying  Turkish  could  be
replicated for other languages, then morphological
and  morphotactic  feature  sets  would  be  more
accurate  than  character  n-grams  or  other  current
methods,  such  as  “most  common  substring”,  at
i d e n t i f y i n g  
t h e  
l a n g u a g e   o f   a   d o c um e n t .
Furthermore,  because  such  success  is  achieved  at
th e   wo rd   l ev e l ,   th e s e   m e thod s   wou ld   b e
appropriate  for  classifying  the  language  of  word
sequences  of  any  length  within  a  multilingual
document,  an  achievement  that  n-gram  character
classification cannot replicate. However, we are as
yet  unable  to  develop  accurate  classifiers  for  our
other  candidate  languages.  This  may  be  because
the  3-gram morphotactic model  is  appropriate  for
the  morphology  of  Turkish,  but  not  for  other
languages; a different n-gram model must be used.

Therefore,  the  first  and  crucial  avenue  for  future
work   is   to   examine   and   test   this   hypothesis .
Without  a  reliable  and  accurate  n-gram  model  of
mo rpho tac t ic s ,   ou r   p ropo sed   mo rpho log ica l
app roach   doe s   no t   have   much   to   o f fe r   in
comparison  to  the  “character  based”  state  of  the
a r t .   Howeve r ,   i f   fu tu re   wo rk   does   iden t i fy
successful  n-gram  morphotactic  models  for  other
languages ,   o ther   me thods   can   be   app l ied   to
attempt  to  increase  performance.  For  example,
rather than Naïve Bayes classification, one can use
SVM  for  the  task,  which  turned  out  to  be  too
computationally costly for the scope of our work.

The  lightweight,  informationally  dense  attributes
o f   m o r p h o l o g i c a l  
t h a t
f e a t u r e s   s u g g e s t  
morphological  approaches  to  text  classification
may   be   extremely   promising .   However ,   with
respect  to  the  language  classification  task,  the
reality of this potential is as yet uncertain.

References
.
[1] N-Gram-Based Text Categorization, William
B. Cavnar, John M. Trenkle.
http://citeseer.ist.psu.edu/68861.html.

[2] Statistical Identifaction of Language, Ted
Dunning, Computing Research Lab, New Mexico
State University.

[3] Applying Monte Carlo Techniques to
Language Identification, Arjen Poutsma,
SmartHaven, Amsterdam.

[4] Unsupervised Morpheme Analysis, Morpho
Challenge 2007, Kurimo, Creutz, Varjokallino.
http://www.cis.hut.fi/morphochallenge2007.

[5] Unsupervised Segmentation of Words Using
Prior Distributions of Morph Length and
Frequency, Matthias Creutz, Proceedings from
ACL-03, 280-287, Sapporo, Japan. July 2003.

Figures

Figure 1

English

< 5

< 1 0 0

< 1 0 0 0

Filtering
Figure 2

0.9
0.8
0.7

0.6
0.5
0.4
0.3

y
c
a
r
u
c
c
A

0.2
0.1
0
U nfiltered

Baseline
Morph
Morph + Mts

Baseline
Morph
Morph + Mts

Baseline
Morph
Morph + Mts

Baseline
Morph
Moprh + Mts

1.2

1

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
A

0
U nfiltered

0.92

0.91

0.9

0.89

0.88

0.87

0.86

y
c
a
r
u
c
c
A

0.85
U nfiltered

y
c
a
r
u
c
c
A

0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
0.64
0.62
U nfiltered

Turkish

< 5

< 1 0 0

< 1 0 0 0

Filtering
Figure 3

Finnish

< 5

< 1 0 0

< 1 0 0 0

Filtering
Figure 4

German

< 5

< 1 0 0

< 1 0 0 0

Filtering
Figure 5

Figure 6


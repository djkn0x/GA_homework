Randomized PCA Algorithms with Regret Bounds
that are Logarithmic in the Dimension

Manfred K. Warmuth
Computer Science Department
University of California - Santa Cruz
manfred@cse.ucsc.edu

Dima Kuzmin
Computer Science Department
University of California - Santa Cruz
dima@cse.ucsc.edu

Abstract

We design an on-line algorithm for Principal Component Analysis.
In each
trial the current instance is projected onto a probabilistically chosen low dimen-
sional subspace. The total expected quadratic approximation error equals the total
quadratic approximation error of the best subspace chosen in hindsight plus some
additional term that grows linearly in dimension of the subspace but logarithmi-
cally in the dimension of the instances.

1 Introduction

In Principal Component Analysis the n-dimensional data instances are projected into a k-
dimensional subspace (k < n) so that the total quadratic approximation error is minimized. After
centering the data, the problem is equivalent to ﬁnding the eigenvectors of the k largest eigenvalues
of the data covariance matrix.
We develop a probabilistic on-line version of PCA: in each trial the algorithm chooses a k-
dimensional projection matrix P t based on some internal parameter; then an instance xt is received
and the algorithm incurs loss kxt − P txtk2
2 ; ﬁnally the internal parameter is updated. The goal is to
obtain algorithms whose total loss in all trials is close to the smallest total loss of any k-dimensional
subspace P chosen in hindsight.
We ﬁrst develop our algorithms in the expert setting of on-line learning. The algorithm maintains a
loss equal to the remaining n − k components of the loss vector, i.e. P
mixture vector over the n experts. At the beginning of trial t the algorithm chooses a subset P t of k
experts based on the current mixture vector wt . It then receives a loss vector λt ∈ [0..1]n and incurs
i . Finally it
i∈{1,...,n}−P t ‘t
updates its mixture vector to wt+1 . Note that now the subset P t corresponds to the subspace onto
which we “project”, i.e. we incur no loss on the k components of P t and are charged only for the
remaining n − k components.
i ≤
The trick is to maintain a mixture vector wt as a parameter with the additional constraint that wt
n−k . We will show that these constrained mixture vectors represent an implicit mixture over subsets
1
of P
of experts of size n − k , and given wt we can efﬁciently sample from the implicit mixture and use it
to predict. This gives an on-line algorithm whose total loss is close to the smallest n − k components
t λt and this algorithm generalizes to an on-line PCA algorithm when the mixture vectors are
replaced by density matrices whose eigenvalues are bounded by 1
n−k . Now the constrained density
matrices represent implicit mixtures of the (n − k)-dimensional subspaces. The complementary
k-dimensional space is used to project the current instance.

2 Standard PCA and On-line PCA

loss(P ) =

loss(P ) = tr((I − P )

C ),

loss(P , b) =

(xt − ¯x)> (I − P )2 (xt − ¯x).

Given a sequence of data vectors x1 , . . . , xT , the goal is to ﬁnd a low-dimensional approximation
of this data that minimizes the 2-norm approximation error. Speciﬁcally, we want to ﬁnd a rank k
projection matrix P and a bias vector b ∈ Rn such that the following cost function is minimized:
TX
kxt − (P xt + b)k2
2 .
t=1
Differentiating and solving for b gives us b = (I − P ) ¯x, where ¯x is the data mean. Substituting
TX
TX
this bias b into the loss we obtain
k(I − P )(xt − ¯x)k2
2 =
t=1
t=1
Since I − P is a projection matrix, (I − P )2 = I − P , and we get:
TX
C ) = tr(C ) − tr( P|{z}
| {z }
(xi − ¯x)(xi − ¯x)> ) = tr((I − P )
rank n−k
i=1
rank k
where C is the data covariance matrix. Therefore the loss is minimized over (n − k)-dimensional
subspaces and this is equivalent to maximizing over k-dimensional subspaces.
In the on-line setting, learning proceeds in trials. (For the sake of simplicity we are not using a bias
algorithm whose total loss over a sequence of trials PT
term at this point.) At trial t, the algorithm chooses a rank k projection matrix P t . It then receives
an instance xt and incurs loss kxt − P txtk2
2 = tr((I − P t ) xt (xt )> ). Our goal is to obtain an
loss of the best rank k projection matrix P , i.e. inf P tr((I − P ) PT
t=1 tr((I − P t ) xt (xt )> ) is close to the total
t=1 xt (xt )> ). Note that the
latter loss is equal to the loss of standard PCA on data sequence x1 , . . . , xT (assuming the data is
centered).

3 Choosing a Subset of Experts
Thus a rank k projection matrix can be written as P = Pk
Recall that projection matrices are symmetric positive deﬁnite matrices with eigenvalues in {0, 1}.
i=1 pip>
i , where the pi are the k ortho-
normal vectors forming the basis of the subspace. Assume for the moment that the eigenvectors are
restricted to be standard basis vectors. Now projection matrices become diagonal matrices with en-
tries in {0, 1}, where the number of ones is the rank. Also, the trace of a product of such a diagonal
projection matrix and any symmetric matrix becomes a dot product between the diagonals of both
matrices and the whole problem reduces to working with vectors: the rank k projection matrices
reduce to vectors with k ones and n − k zeros and the diagonal of the symmetric matrix may be
seen as a loss vector λt . Our goal now is to develop on-line algorithms for ﬁnding the lowest n − k
of PT
components of the loss vectors λt so that the total loss is close the to the lowest n − k components
t=1 λt . Equivalently, we want to ﬁnd the highest k components in λt .
We begin by developing some methods for dealing with subsets of components. For convenience we
encode such subsets as probability vectors: we call r ∈ [0, 1]n an m-corner if it has m components
m and the remaining n − m components set to zero. At trial t the algorithm chooses an
set to 1
(n − k)-corner r t . It then receives a loss vector λt and incurs loss (n − k) r t · λt .
(cid:1) m-corners. Clearly any component wi of a vector w in An
of the (cid:0) n
Let An
m consist of all convex combinations of m-corners. In other words, An
m is the convex hull
dimensional vectors w for which |w | = P
m because it
m is at most 1
m ⊆ B n
m
is a convex combination of numbers in [0.. 1
m ]. Therefore An
m , where B n
m is the set of n-
i wi = 1 and 0 ≤ wi ≤ 1
m , for all i. The following
m = B n
theorem implies that An
m :
Theorem 1. Algorithm 1 produces a convex combination1 of at most n m-corners for any vector in
m .
B n

Algorithm 1 Mixture Construction
input 1 ≤ m < n and w ∈ B n
m
repeat
Let r be a corner whose m components correspond to nonzero components of w
and contain all the components of w that are equal to |w|
m
Let s be the smallest of the m chosen components in w
}|
{
z
and l be the largest value of the remaining n − m components
p
w := w −
min(m s, |w| − m l) r and output p r
until w = 0
Let eB n
Proof. Let b(w) be the number of boundary components in w , i.e. b(w) := |{i : wi is 0 or |w|
m }|.
m be all vectors w such that 0 ≤ wi ≤ |w|
m , for all i. If b(w) = n, then w is either a corner or
if w ∈ eB n
m and w is neither a corner nor 0, then the successor bw ∈ eB n
m and b( bw) > b(w). Clearly,
0. The loop stops when w = 0. If w is a corner then it takes one iteration to arrive at 0. We show
bw ≥ 0, because the amount that is subtracted in the m components of the corner is at most as large
as the corresponding components of w . We next show that bwi ≤ | bw|
m . Otherwise bwi = wi ≤ l, and l ≤ | bw|
bwi = wi − p
m . If i belongs to the corner then
p ≤ |w | − m l. This proves that bw ∈ eB n
m = | bw|
m ≤ |w|−p
m follows from the fact that
For showing that b( bw) > b(w) ﬁrst observe that all boundary components in w remain boundary
m .
components in bw : zeros stay zeros and if wi = |w|
m then i is included in the corner and bwi =
m = | bw|
|w|−p
m . However, the number of boundary components is increased at least by one because
of them becomes a boundary point in bw : if p = m s then the component corresponding to s in
the components corresponding to s and l are both non-boundary components in w and at least one
m = 0 in bw and if p = |w | − m l then the component corresponding to l in w is
w is s − p
m = | bw|
l = |w|−p
vector w ∈ eB n
m . It follows that it may take up to n iterations to arrive at a corner which has n
boundary components and one more iteration to arrive at 0. Finally note that there is no weight
m s.t. b(w) = n − 1 and therefore the size of the produced linear combination is at
The algorithm produces a linear combinations of corners, i.e. w = P
most n. More precisely, the size is at most n − b(w) if n − b(w) ≤ n − 2 and one if w is a corner.
|rj | = 1, P
j pj rj . Since pj ≥ 0 and all
j pj = 1 and we actually have a convex combination.
Fact 1. For any loss vector λ, the following corner has the smallest loss of any convex combination
m = B n
m : Greedily pick the component of minimum loss (m times).
of corners in An
(cid:0) n
(cid:1) corners of size n− k . However, the best corner is also the best convex combination of corners,
n−k where each member of this set is given by (cid:0) n
(cid:1) coefﬁcients. Luckily,
How can we use the above construction and fact? It seems too hard to maintain information about all
n−k
i.e. the best from the set An
n−k
this set of convex combinations equals B n
n−k and it takes n coefﬁcients to specify a member in that
set. Therefore we can search for the best hypothesis in the set B n
n−k and for any such hypothesis
we can always construct a convex combination (of size ≤ n) of (n − k)-corners which has the same
expected loss for each loss vector. This means that any algorithm predicting with a hypothesis vector
n−k can be converted to an algorithm that probabilistically chooses an (n − k)-corner. Finally,
in B n
the set P t of the k components missed by the chosen (n − k)-corner corresponds to the subspace
we project onto.
and (n − k) wt · λt is the expected loss in one trial. The projection bwt onto B n
Algorithm 2 spells out the details for this approach. The algorithm chooses a corner probabilistically
n−k can be achieved as
follows: ﬁnd the smallest l s.t. capping the largest l components to 1
n−k and rescaling the remaining
n−l weights to total weight 1− l
n−k makes none of the rescaled weights go above
n−k . The simplest
1

1The existence of a convex combination of at most n corners is implied by Carath ´eodory’s theorem [Roc70],
but the algorithm gives an effective construction.

algorithm starts with sorting the weights and then searches for l with a binary search. However, a
linear algorithm that recursively uses the median is given in [HW01].

Algorithm 2 Capped Weighted Majority Algorithm
Decompose wt as P
input: 1 ≤ k < n and an initial probability vector w1 ∈ B n
n−k
for t = 1 to T do
j pj rj with Algorithm 1, where m = n − k
Draw a corner r = rj with probability pj
Incur loss (n − k) r · λt = P
Let P t be the k components outside the drawn corner
Receive loss vector λt
bwt
i .
i∈{1,...,n}−P t ‘t
d(w , bwt )
i exp(−η‘t
i := wt
i ) / Z , where Z normalizes the weights to one
wt+1 := argmin
w∈Bn
n−k
end for

,

(n − k)

When k = n − 1, n − k = 1 and B n
1 is the entire probability simplex. In this case the call to
Algorithm 1 and the projection onto B n
1 are vacuous and we get the standard Randomized Weighted
Let d(u, w) denote the relative entropy between two probability vectors: d(u, w) = P
Majority algorithm [LW94]2 with loss vector λt .
i ui log ui
.
wi
Theorem 2. On an arbitrary sequence of loss vectors λ1 , . . . , λT ∈ [0, 1]n , the total expected loss
wt · λt ≤ (n − k) η PT
TX
of Algorithm 2 is bounded as follows:
t=1 u · λt + d(u, w1 ) − d(u, wT +1 )
1 − exp(−η)
t=1
for any learning rate η > 0 and comparison vector u ∈ B n
n−k .
Proof. The update for bwt in Algorithm 2 is the update of the Continuous Weighted Majority for
d(u, wt ) − d(u, bwt ) ≥ −η u · λt + wt · λt (1 − exp(−η)).
which the following basic inequality is known (essentially [LW94], Lemma 5.3):
The weight vector wt+1 is a Bregman projection of vector bwt onto the convex set B n
(1)
n−k . For such
d(u, bwt ) ≥ d(u, wt+1 ) + d(wt+1 , bwt )
projections the Generalized Pythagorean Theorem holds (see e.g [HW01] for details):
Since Bregman divergences are non-negative, we can drop the d(wt+1 , bwt ) term and get the follow-
d(u, bwt ) − d(u, wt+1 ) ≥ 0, for u ∈ B n
ing inequality:
n−k .
Adding this to the previous inequality we get:
d(u, wt ) − d(u, wt+1 ) ≥ −η u · λt + wt · λt (1 − exp(−η))
By summing over t, multiplying by n − k , and dividing by 1 − exp(−η), the bound follows.

4 On-line PCA

In this context (matrix) corners are density matrices with m eigenvalues equal to 1
m and the rest are
0. Also the set An
m consists of all convex combinations of such corners. The maximum eigenvalue
of a convex combination of symmetric matrices is at most as large as the maximum eigenvalue of
any of the matrices ([Bha97], Corollary III.2.2). Therefore each convex combination of corners is

2The original Weighted Majority algorithms were described for the absolute loss. The idea of using loss
vectors instead was introduced in [FS97].

m and An
m ⊆ Bn
m , where Bn
a density matrix whose eigenvalues are bounded by 1
m consists of all
m . Assume we have some density matrix
density matrices whose maximum eigenvalue is at most 1
ω = P
W ∈ Bn
m with eigendecomposition W diag(ω)W > . Algorithm 1 can be applied to the vector of
matrix: W = P
eigenvalues ω of this density matrix. The output convex combination of up to n diagonal corners
j pj rj can be turned into a convex combination of matrix corners that expresses the density
j pj W diag(rj )W > . It follows that An
m = Bn
m as in the diagonal case.
Theorem 3. For any symmetric matrix S , minW ∈Bn
tr(W S ) attains its minimum at the following
m
matrix corner: greedily choose orthogonal eigenvectors of S of minimum eigenvalue (m times).
Proof. Let λ↓ (W ) denote the vector of eigenvalues of W in descending order and let λ↑ (S ) be the
same vector of S but in ascending order. Since both matrices are symmetric, tr(W S ) ≥ λ↓ (W ) ·
λ↑ (S ) ([MO79], Fact H.1.h of Chapter 9). Since λ↓ (W ) ∈ B n
m , the dot product is minimized and
the inequality is tight when W is an m-corner corresponding to the m smallest eigenvalues of S .
Also the greedy algorithm ﬁnds the solution (see Fact 1 of this paper).

Algorithm 2 generalizes to the matrix setting. The Weighted Majority update is replaced by the
corresponding matrix version which employs the matrix exponential and matrix logarithm [WK06]
(The update can be seen as a special case of the Matrix Exponentiated Gradient update [TRW05]).
The following theorem shows that for the projection we can keep the eigensystem ﬁxed. Here
∆(U , W ) denotes the quantum relative entropy tr(U (log U − log W )).
Theorem 4. Projecting a density matrix onto Bn
m w.r.t. the quantum relative entropy is equivalent
to projecting the vector of eigenvalues w.r.t. the “normal” relative entropy: If W has the eigende-
composition W diag(ω)W > , then
∆(U , W ) = W u∗W > , where u∗ = argmin
argmin
u∈Bn
U ∈Bn
m
m
Proof. If λ↓ (S ) denotes the vector of eigenvalues of a symmetric matrix S arranged in de-
scending order, then tr(ST ) ≤ λ↓ (S ) · λ↓ (T ) ([MO79], Fact H.1.g of Chapter 9). This im-
plies that tr(U log W ) ≤ λ↓ (U ) · log λ↓ (W ) and ∆(U , W ) ≥ d(λ↓ (U ), λ↓ (W )). Therefore
then W diag(u∗ )W > mini-
∆(U , W ) ≥ min
d(u, ω) and if u∗ minimizes the r.h.s.
min
u∈Bn
U ∈Bn
mizes the l.h.s. because ∆(W diag(u∗ )W , W ) = d(u∗ , ω).
m
m

d(u, ω).

Algorithm 3 On-line PCA algorithm
input: 1 ≤ k < n and an initial density matrix W 1 ∈ Bn
n−k
Decompose ω as P
for t = 1 to T do
Perform eigendecomposition W t = W ωW >
j pj rj with Algorithm 1, where m = n − k
Draw a corner r = rj with probability pj
Form a matrix corner R = W diag(r)W >
Form a rank k projection matrix P t = I − (n − k)R
Receive data instance vector xt
cW
2 = tr((I − P t ) xt (xt )> )
Incur loss kxt − P txtk2
∆(W , cW
t = exp(log W t − η xt (xt )> ) / Z , where Z normalizes the trace to 1
t )
W t+1 := argmin
W ∈Bn
n−k

end for
The expected loss in trial t of this algorithm is given by (n − k)tr(W txt (xt )> )
Theorem 5. For an arbitrary sequence of data instances x1 , . . . , xT of 2-norm at most one, the
(n − k)tr(W txt (xt )> ) ≤ (n − k) η PT
TX
total expected loss of the algorithm is bounded as follows:
t=1 tr(U xt (xt )> ) + ∆(U , W 1 ) − ∆(U , W T )
1 − exp(−η)
t=1

,

for any learning rate η > 0 and comparator density matrix U ∈ Bn
n−k .3
Proof. The update for cW
t is a density matrix version of the standard Weighted Majority update
which was used for variance minimization along a single direction (i.e. k = n − 1) in [WK06]. The
∆(U , W t ) − ∆(U , cW
basic inequality (1) for that update becomes:
t ) ≥ −η tr(U xt (xt )> ) + tr(W txt (xt )> )(1 − exp(−η))
As in the proof of Theorem 2 of this paper, the Generalized Pythagorean theorem applies and drop-
∆(U , cW
ping one term we get the following inequality:
t ) − ∆(U , W t+1 ) ≥ 0, for U ∈ Bn
n−k .
Adding this to the previous inequality we get:
∆(U , W t ) − ∆(U , W t+1 ) ≥ −η tr(U xt (xt )> ) + tr(W txt (xt )> )(1 − exp(−η))
By summing over t, multiplying by n − k , and dividing by 1 − exp(−η), the bound follows.
It is easy to see that ∆(U , W 1 ) ≤ (n − k) log n
n−k . If k ≤ n/2, then this is further bounded by
k log n
k . Thus, the r.h.s. is essentially linear in k , but logarithmic in the dimension n.
By tuning η [CBFH+97, FS97], we can get regret bounds of the form:
(cid:16)r
(cid:17)
(expected total loss of alg.) - (total loss best k-space)
+ k log n
(total loss of best k-subspace) k log n
k
k
Using standard but signiﬁcantly simpliﬁed conversion techniques from [CBFH+97] based on the
leave-one-out loss we also obtain algorithms with good regret bounds in the following model: the
algorithm is given T − 1 instances drawn from a ﬁxed but unknown distribution and produces a
k-space based on those instances; it then receives a new instance from the same distribution. We can
bound the expected loss on the last instance:
(cid:16)r (expected loss of best k-subspace) k log n
(cid:17)
(expected loss of alg.) - (expected loss best k-space)
k
T

k log n
k
T

= O

= O

+

.

(3)

.

(2)

5 Lower Bound

The simplest competitor to our on-line PCA algorithm is the algorithm that does standard (uncen-
tered) PCA on all the data points seen so far. In the expert setting this algorithm corresponds to
“projecting” to the n − k experts that have minimum loss so far (where ties are broken arbitrarily).
When k = n − 1, this becomes the follow the leader algorithm. It is easy to construct an adversary
strategy for this type of deterministic algorithm (any k) that forces the on-line algorithm to incur
n times as much loss as the off-line algorithm. In contrast our algorithm is guaranteed to have ex-
pected additional loss (regret) of the order of square root of k ln n times the total loss of the best
off-line algorithm. When the instances are diagonal matrices then our algorithm specializes to the
standard expert setting and in that setting there are probabilistic lower bounds that show that our
tuned bounds (2,3) are tight [CBFH+ 97].

6 Simple Experiments

The above lower bounds do not justify our complicated algorithms for on-line PCA because natural
data might be more benign. However natural data often shifts and we constructed a simple dataset of
this type in Figure 1. The ﬁrst 333 20-dimensional points were drawn from a Gaussian distribution
with a rank 2 covariance matrix. This is repeated twice for different covariance matrices of rank
3The xt (xt )> can replaced by symmetric matrices S t whose eigenvalues have range at most one.

Figure 1: The data set used for the experiments. Dif-
ferent colors/symbols denote the data points that came
from three different Gaussians with rank 2 covariance
matrices. The data vectors are 20-dimensional but we
plot only the ﬁrst 3 dimensions.

Figure 2: The blue curve plots the total loss of
on-line algorithm up to trial t for 50 different runs
(with k = 2 and η ﬁxed to one). Note that the vari-
ance of the losses is small. The red single curve
plots the total loss of the best subspace of dimen-
sion 2 for the ﬁrst t points.

Figure 3: Behavior of the algorithm around a transition point between two distributions. Each ellipse depicts
the projection matrix with the largest coefﬁcient in the decomposition of W t . The transition sequence starts
with the algorithm focused on the projection matrix for the ﬁrst subset of data and ends with essentially the
optimal matrix for the second subset. The depicted transition takes about 60 trials.

2. We compare the total loss of our on-line algorithm with the total loss of the best subspace for
the ﬁrst t data points. During the ﬁrst 333 datapoints the latter loss is zero since the ﬁrst dataset is
2-dimensional, but after the third dataset is completed, the loss of any ﬁxed off-line comparator is
large. Figure 3 depicts how our algorithm transitions between datasets and exploits the on-lineness
of the data. Randomly permuting the dataset removes the on-lineness and results in a plot where the
total loss of the algorithm is somewhat above that of the off-line comparator (not shown).
Any simple “windowing algorithm” would also be able to detect the switches. Such algorithms are
often unwieldy and we don’t know any strong regret bounds for them. In the expert setting there is
however a long line of research on shifting (see e.g. [BW02, HW98]). An algorithm that mixes a
little bit of the uniform distribution into the current mixture vector is able to restart when the data
switches. More importantly, an algorithm that mixes in a little bit of the past average density matrix
is able to switch quickly to previously seen subspaces and to our knowledge windowing techniques
cannot exploit this type of switching. Preliminary experiments on face image data indicate that the
algorithms that accommodate switching work as expected, but more comprehensive experiments
still need to be done.

7 Conclusions

We developed a new set of techniques for low dimensional approximation with provable bounds.
Following [TRW05, WK06], we essentially lifted the algorithms and bounds developed for diagonal
case to the matrix case. Are there general reductions?
The on-line PCA problem was also addressed in [Cra06]. However, that paper does not fully capture
the PCA problem because their algorithm predicts with a full-rank matrix in each trial, whereas we
predict with a probabilistically chosen projection matrix of the desired rank k . Furthermore, that
paper proves bounds on the ﬁltering loss, which are typically easier to prove, and it is not clear how
this loss relates to the more standard regret bounds proven in this paper.
exp(− P
) = Q
For the expert setting there are alternate techniques for designing on-line algorithms that do as
well as the best subset of n − k experts: set {i1 , . . . , in−k } receives weight proportional to
j exp(−‘<t
). In this case we can get away with keeping only one weight
j ‘<t
ij
ij
per expert (the ith expert gets weight exp(−‘<t
i )) and then use dynamic programming to sum over
sets (see e.g. [TW03] for this type of methods). With some more work, dynamic programming can
also be applied for PCA. However, our new trick of using additional constraints on the eigenvalues
is an alternative that avoids dynamic programming.
Many technical problems remain. For example we would like to enhance our algorithms to learn a
bias as well and apply our low-dimensional approximation techniques to regression problems.
Acknowledgment: Thanks to Allen Van Gelder for valuable discussions re. Algorithm 1.

[HW98]

[HW01]

References
R. Bhatia. Matrix Analysis. Springer, Berlin, 1997.
[Bha97]
Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing
[BW02]
past posteriors. Journal of Machine Learning Research, 3:363–396, 2002.
[CBFH+97] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K.
Warmuth. How to use expert advice. 44(3):427–485, 1997.
In Proceedings of the 19th
Koby Crammer. Online tracking of linear subspaces.
Annual Conference on Learning Theory (COLT 06), Pittsburg, June 2006. Springer.
Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line
learning and an application to boosting. Journal of Computer and System Sciences,
55(1):119–139, August 1997.
Mark Herbster and Manfred Warmuth. Tracking the best expert. Machine Learning,
32(2):151–178, 1998. Earlier version in 12th ICML, 1995.
Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal
of Machine Learning Research, 1:281–309, 2001.
N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Inform. Com-
put., 108(2):212–261, 1994.
A. W. Marshall and I. Olkin. Inequalities: Theory of Majorization and its Applications.
Academic Press, 1979.
R. Rockafellar. Convex Analysis. Princeton University Press, 1970.
K. Tsuda, G. R ¨atsch, and M. K. Warmuth. Matrix exponentiated gradient updates for
on-line learning and Bregman projections. Journal of Machine Learning Research,
6:995–1018, June 2005.
Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates.
Journal of Machine Learning Research, 4:773–818, 2003.
Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. In Proceed-
ings of the 19th Annual Conference on Learning Theory (COLT 06), Pittsburg, June
2006. Springer.

[Roc70]
[TRW05]

[WK06]

[LW94]

[MO79]

[Cra06]

[FS97]

[TW03]


Rahul Choudhury (Stanford ID: 4781696) 
  

    CS229 Machine Learning Final Project Report 

Segmenting Descending Aorta using Machine Learning 

1.  Introduction 
Atherosclerosis is one of primary causes of adverse cardiac events today. Plaque, a mixture of calcium, cholesterol 
fibrin and other substances accumulated in the vessel lumen causes stenosis or occlusion of the vessel. Depending 
on the location, atherosclerosis may result in strokes, heart attacks, aneurysms, and peripheral artery occlusion 
diseases.  
 
The descending aorta is one of the most important vessel structures in the body. It is the largest artery in the body 
that runs down through the chest and the abdomen. The descending aorta begins below the arch of the aorta and 
ends by splitting into two great arteries (the common iliac arteries), which go to the legs. It has been observed that 
atherosclerosis of the descending aorta is a useful predictor of cardiovascular events[1]. 
 
W ith the advent of Multi Detector CT (MDCT) modality, Computer Tomography Angiography (CTA) is routinely used 
to image aortic vessels. CTA is ideal for preoperative evaluation of thoracic and abdominal aortic aneurysms, as it 
demonstrates their position, extent, and relationships to the renal and iliac arteries. CT is especially advantageous as 
it acquires information of the whole 3D volumetric data with high resolution rather than 2D projections in conventional 
angiography. 
 
Geometric processing of CT vessel data has become increasingly more important for visualization, diagnosis and 
quantification of vascular diseases. The first step towards building patient-specific vessel geometry is reliable 
segmentation. However manual segmentation of a volumetric vessel is a very tedious task. The problem is 
exacerbated by the fact that CTA produces a huge number of tomographic slices of the aortic vessel. It has been 
shown in past studies that manual segmentations vary significantly between experts as well as when an expert 
segments the same image at different times[2].  
 
The problems mentioned above underscore the need for reproducible and accurate automatic segmentation methods 
for the descending aorta in CT datasets. The aorta is complex in shape and appearance and varies significantly 
across individuals. In addition poor image contrast, noise, and missing or diffuse boundaries add to the complexity of 
the segmentation task. The goal of this project is to use machine learning algorithm(s) to build a probabilistic model, 
which can enable automatic segmentation of descending aorta in the CT volumetric datasets. 
 

2.  Methodology 
The segmentation approach is based on the observation that blood filled regions are usually more easily recognizable 
in contrast-enhanced CT images[6]. If we can isolate the blood filled regions from the non-blood filled ones, the task 
of detecting the descending aorta becomes much simpler. Based on this observation, a machine learning algorithm is 
first used to estimate the probability distribution of three main classes of volumetric data. We then find the region filled 
with blood (which includes aorta, pulmonary trunk, heart chambers, and coronary arteries) using a Bayesian 
classification. We finally apply a level-set based active contour model to extract the descending aorta and obtain a 3D 
geometric model. All these steps of the methodology are explained in detail below. 
 
The following notations have been used in describing the methodology: 
I is the input image 
• 
•  N is the number of voxels in I 
•  M is the number of classes of voxels 
=
≤≤
is a set of integer indices into I 
s
S
Ns
1|{
}
• 
s ∈
sY  is the observed intensity at the 
sY
|{
scW is a binary indicator variable that indicates the membership of a voxel s to class j. 
belongs to class j otherwise it is 0. 
sW is an M-dimensional indicator column vector whose 

thc component is the indicator variable 

is the set of intensities of I. 

scW =1 if voxel s 

ths voxel of I 

scW  

• 

• 

• 

Y

=

S

}

 

 

2.1 
Step 1: Estimating the probability densities 
We assume that there are three classes of voxels in the CT Dataset (a) Blood-filled regions (b) Myocardium (c) 
Lung. Given a set of unlabeled training CT slices, we model the volume as a mixture of three Gaussians. If we 

 

1

Rahul Choudhury (Stanford ID: 4781696) 

CS229 Machine Learning

sc

(

)

=

Wv
|

YP
(
s

words we can find out 

know the mean cµ  and standard deviation cσ  for each class where  ∈c
{blood-filled regions, myocardium, 
lung}, we can estimate the probability that the intensity of a voxel given that it belongs to a specific class. In other 
(cid:3)
(cid:6)
2
(cid:1)
(cid:4)
exp
(cid:1)
(cid:4)
πσ
σ
2
2
(cid:2)
(cid:5)
c
c
Two separate algorithms, K-Means and Expectation Maximization (EM) have been implemented to estimate the 
cµ and the standard deviation cσ for each 
densities of the Gaussians. These two methods give us the mean 
class. Step 1 of the methodology is executed offline on unlabeled training data once and the results (estimated 
densities) are stored for online usage. 
 
 

µ
c
2

)1

−

−

=

=

1

v

  

Unlabeled Training Slice 

 

 

Output of EM 

 

Figure 1: Result from EM 

s

)

v

=

=

YWP
|
(
sc

intensity. Thus 

2.2 
Step 2: Calculating the posterior probability using Bayes Rule 
We can then use Bayes’ rule to find out the probability that a voxel belongs to a specific class c given its voxel 
=
cPWv
YP
|
(
)(
)
sc
(cid:7)
cPWv
|
(
)
′
cs
′
c
uniformly distributed across all three classes. The classification of the voxels is then obtained by maximum a 
(cid:1)
=
1=sjW
posteriori (MAP) estimation. In other words 
 if 
. Here 
YWP
j
max
arg
|
(
)
sc
s
c
(cid:1)
before 
represents the result of applying an anisotropic diffusion filter on the posterior 
sc YWP
sc YWP
(
|
(
)
|
)
s
s
doing the MAP estimation. A gradient-based anisotropic diffusion filter is used to reduce noise (or unwanted 
detail) in the posterior probabilities while preserving specific image features. 

. Here we assume that the prior data 

s
YP
(
s

)(cP

is 

′
)

=

2.3 
Step 3: Extracting the aorta using level-set based active contour 
Once we have classified each voxel as belonging to one of the three classes, we turn all non-blood filled voxels 
to zero and assign a non-zero value to all the blood-filled voxels. Now our job is to extract the descending aorta 
from these blood-filled voxels and discard other regions (such as coronary arteries, pulmonary trunks, heart-
chambers etc). We employ a level set-based active contour model to detect the descending aorta. A detailed 
analysis of the level set model is available in literature[3]. Below we give a brief overview of the model. 
 
The paradigm of the level set is that it is a numerical method for tracking the evolution of contours and surfaces. 
Instead of manipulating the contour directly, the contour is embedded as the zero level set of a higher 
tXψ
dimensional function called the level-set function 
. The level-set function is then evolved under the 
(
),

2

 

 
 

 

 

Rahul Choudhury (Stanford ID: 4781696) 

CS229 Machine Learning

 

control of a differential equation. At any time, the evolving contour can be obtained by extracting the zero level-
Γ
=
=
ψ
set 
from the output. The main advantage of using level sets is that arbitrarily 
tX
tX
({
),
),
((
)
}0
complex shapes can be modeled and topological changes such as merging and splitting are handled implicitly. 
The governing level set equation is  
d

ψκγψ
+∇
∇
xZ
)(

 where A is an advection term, P is a propagation term 

αψ
−=
xA
(

βψ
−∇
xP
)(
).

dt
and Z is a spatial modifier for the mean curvature κ . The scalar coefficients  βα,
influence of each of the terms on the movement of the interface.  

and γ weight the relative 

3.  Implementation 
 

Training
CT Volume

Expectation
Maximization

Density
Functions

Input
CT
Volume

Seeds

Distance

Inflation
Strength

Length
Penalty

Anisotropic
Diffusion
Filter

Bayesian
Classifier

Binary
Threshold

Binary
Edge
Image

Fast
Marching

Input
Level Set

Geodesic
Active
Contour

Output
Level Set

Binary
Threshold

Segmented
Binary
Image

Figure 2: Implementation Pipeline 

 

 
The above diagram presents how the methodology explained in section 2 was implemented using a pipeline 
approach. First we apply Expectation Maximization algorithm to the unlabeled training CT dataset to find the 
density functions of all the three groups of voxels mentioned earlier.  These density functions are fed to the 
Bayesian classifier to classify the test CT volume. However before applying the Bayesian classifier, we first apply 
an anisotropic diffusion filter to smoothen the test CT volume while preserving the edge information. The 
smoothened image is passed as the input to a Bayesian Classifier to classify each voxel in one of three 
categories. This classified volume is then passed to a binary threshold filter, which sets all the non-blood-filled 
voxels to zero and assigns all the blood-filled voxels to the same non-zero number.  
 
At this point we are ready to apply the geodesic active contour filter. This filter expects two inputs: the first is an 
initial level set and the second input is a feature image. The initial level set is computed by a Fast Marching 
Filter. A set of user-provided seeds is passed to a Fast Marching Image Filter in order to compute a distance 
map. A constant value is subtracted from this map in order to obtain a level set in which the zero set represents 
the initial contour.  This level set as well the output of the Bayesian classifier are passed as inputs to the 
Geodesic Active Contour Level Set Image Filter. Finally, the level set generated by the Geodesic Active Contour 
Filter is passed to a Binary Threshold Image Filter in order to produce a binary mask representing the segmented 
object. For the Geodesic Active Contour Filter, several scaling parameters are used to trade off between the 
propagation (inflation), the curvature (smoothing) and the advection terms.  
 
Microsoft’s .NET technology and C++, C# language on W indows XP platform have been chosen to implement 
the methodology described above. MergeCOM3 toolkit from Merge-eMed has been used to load DICOM 
datasets[4]. ITK has been used for low-level image processing tasks[5] 

4.  Results 
As explained in the methodology section, we run two different unsupervised learning algorithms to compute 
density functions of three groups of voxels (blood-filled regions, myocardium and lung). The training data for K-

 

3

Rahul Choudhury (Stanford ID: 4781696) 

CS229 Machine Learning

means and EM is a CT volume of Heart acquired with contrast agent from a single patient. There are 20 slices in 
the volume; each slice is 512 by 512. Each pixel is represented by 16 bits. A Modality Lookup transformation was 
applied to the original voxel data to convert them to Hounsfield Unit.  
 
We found that both K-means and EM are dependent on the order in which the training image slices are 
presented, as well as on initialization points. We also found that as the bone (including rib-cages) voxels have a 
very similar intensity profile as that of blood-filled regions, both EM and K-Means usually cluster bones and 
blood-filled regions under one cluster.  
 
For this application, we ended up using the density function results from EM as the k-means algorithm implicitly 
assumes that the data points in each cluster are spherically distributed around the center. Less restrictively, the 
EM algorithm assumes that the data points in each cluster have a multidimensional Gaussian distribution with a 
covariance matrix that may or may not be fixed, or shared. We run EM using different initial values and then used 
average means and standard deviations for three classes as inputs to the Bayesian Classifier. The final mean 
and variance for all the three density functions computed by EM are mentioned below (in Hounsfield unit): 
 

 
Myocardium 
Lung 
Blood-Filled Region 

Mean 
-50.25 
-896.22 
341.72 

Variance 
14602.30 
6756.84 
24304.81 

The Bayesian Classification and geodesic active contour parts of the algorithm were executed online. We tested 
this part on a 3D CT data set of cardiac images. The images were acquired using a Siemens scanner with a slice 
spacing of 1 mm and an in-plane resolution of 0.742 mm. For the purpose of establishing ground-truth we 
manually identified the contour as well as marked all the pixels belonging to the descending aorta on each of the 
test slices. Because we applied the geodesic active contour to a binary volume, we found that the overall 
processing time for each slice is quite reasonable (~55 seconds). Below is an example of a single test slice and 
its corresponding segmentation result. 
 

Original Test Slice 

 

Segmented Slice 

 

Figure 3: Example of Segmented Slice 

Several problems were discovered during the implementation of the geodesic active contour algorithm. 
 

•  We found that the Fast Marching filter is highly susceptible to the initial seed point as well as to the 
distance parameter used to specify distance from the seed point to the input level set. In the beginning 
we supplied only a single seed point (roughly at the center slice in the descending aorta) whose 
distance from the input level set contour was set to 5.0. W ith those parameters, we found that the 
algorithm provides best sensitivity at that center slice, but the sensitivity gradually decreases as we 
move away from the center to the two extreme ends of the aorta.  To improve the overall sensitivity of 
the algorithm, we initialized the Fast Marching Level Set algorithm with multiple equidistant seed points 
in the volume. A distance of 15.0 was used for each one of the seed points. Below is a graph for the 
improved sensitivity plotted against slice position for a test volume of 20 slices. 

 

4

 

 

 

 

Rahul Choudhury (Stanford ID: 4781696) 

CS229 Machine Learning

y
t
i
v
i
t
i
s
n
e
S

120

100

80

60

40

20

0

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Slice Number

 

Figure 4: Sensitivity for each slice in the test data set 

 
 
•  We also found that the propagating surface leaks into the coronary artery, at the location where the left 
main (LM) coronary artery bifurcates from the aorta. We had to prevent this leakage by adjusting the 
weight between the curvature and the inflationary constant.  One could also cut the linking sites of the 
left main coronary artery and the aorta in a few slices of the data where the linkage exists, and evolve 
the surface solely inside the descending aorta, however this technique necessitates manual 
intervention. 

 

 

5.  Conclusions and Future Work 
We proposed a novel approach to segmenting and reconstructing the human descending aorta using machine 
learning. This method was tested on a data set of cardiac CT images. The application of our method results in a 
reconstructed geometric model of the descending aorta, which provides an improved comprehensive view of the 
vessels. This could potentially assist clinicians in achieving more accurate clinical diagnoses of atherosclerotic 
diseases in the descending aorta. Future work would involve the following items: 
•  Currently the user supplies the initial seed points for the Fast Marching Filter. We would like to 
automate the seed point selection process. 
For the Geodesic Active Contour Level Set Filter, currently the scaling parameters are found out by trial 
and error. In future we would like to learn these parameters by applying an appropriate machine-
learning algorithm. 
•  We would like to test the overall algorithm for larger test datasets 
•  We would like to measure clinically significant parameters from the descending aorta model, such as 
the centerline and diameters of vessels. 

• 

6.  References 
[1].  Albert Varga, Noemi Gruber, Tamás Forster, Györgyi Piros, Kálmán Havasi, Éva Jebelovszki, and Miklos 
Csanády. Atherosclerosis of the descending aorta predicts cardiovascular events: a transesophageal 
echocardiography study  
[2].  Warfield, S., W inalski, C., Jolesz, F., and Kikinis, R.1998. Automatic segmentation of MRI of the knee. In 
ISMRM Sixth Scientific Meeting and Exhibition, page 563. 
[3].  J.A. Sethian. Level Set Methods and Fast Marching Methods. Cambridge University Press, 1996 
[4].  http://www.merge.com 
[5].  http://www.itk.org 
[6].  Yang,Y., Tannenbaum, A., Giddens, D. 2004. Knowledge-Based 3D Segmentation & Reconstruction of 
Coronary Arteries Using CT Images. Proceedings of the 26th Annual Conference of the IEEE EMBS. 

 

 

5


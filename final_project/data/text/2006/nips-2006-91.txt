Gaussian and Wishart Hyperkernels

Risi Kondor, Tony Jebara
Computer Science Department, Columbia University
1214 Amsterdam Avenue, New York, NY 10027, U.S.A.
frisi,jebarag@cs.columbia.edu

Abstract

We propose a new method for constructing hyperkenels and de(cid:12)ne two
promising special cases that can be computed in closed form. These we call
the Gaussian and Wishart hyperkernels. The former is especially attractive
in that it has an interpretable regularization scheme reminiscent of that of
the Gaussian RBF kernel. We discuss how kernel learning can be used not
just for improving the performance of classi(cid:12)cation and regression meth-
ods, but also as a stand-alone algorithm for dimensionality reduction and
relational or metric learning.

1

Introduction

The performance of kernel methods, such as Support Vector Machines, Gaussian Processes,
etc. depends critically on the choice of kernel. Conceptually, the kernel captures our prior
knowledge of the data domain. There is a small number of popular kernels expressible in
closed form, such as the Gaussian RBF kernel k(x; x0 ) = exp((cid:0) k x (cid:0) x0 k2 =(2(cid:27)2 )), which
boasts attractive and unique properties from an abstract function approximation point of
view.
In real world problems, however, and especially when the data is heterogenous or
discrete, engineering an appropriate kernel is a ma jor part of the modelling process. It is
natural to ask whether instead it might be possible to learn the kernel itself from the data.

Recent years have seen the development of several approaches to kernel learning [5][1].
Arguably the most principled method proposed to date is the hyperkernels idea introduced
by Ong, Smola and Williamson [8][7][9]. The current paper is a continuation of this work,
introducing a new family of hyperkernels with attractive properties.

Most work on kernel learning has focused on (cid:12)nding a kernel which is subsequently to
be used in a conventional kernel machine, turning learning into an essentially two-stage
process: (cid:12)rst learn the kernel, then use it in a conventional algorithm such as an SVM to
solve a classi(cid:12)cation or regression task. Recently there has been increasing interest in using
the kernel in its own right to answer relational questions about the dataset.
Instead of
predicting individual labels, a kernel characterizes which pairs of labels are likely to be the
same, or related. Kernel learning can be used to infer the network structure underlying data.
A di(cid:11)erent application is to use the learnt kernel to produce a low dimensional embedding
via kernel PCA. In this sense, kernel learning can be also be regarded as a dimensionality
reduction or metric learning algorithm.

2 Hyperkernels

We begin with a brief review of the kernel and hyperkernel formalism. Let X be the input
space, Y the output space, and f(x1 ; y1 ) ; (x2 ; y2 ) ; : : : ; (xm ; ym )g the training data. By kernel
we mean a symmetric function k : X (cid:2) X ! R that is positive de(cid:12)nite on X . Whenever

we refer to a function being positive de(cid:12)nite, we assume that it is also symmetric. Positive
de(cid:12)niteness guarantees that k induces a Reproducing Kernel Hilbert Space (RKHS) F ,
which is a vector space of functions spanned by f kx ((cid:1)) = k(x; (cid:1)) j x 2 X g and endowed with
an inner product satisfying hkx ; kx0 i = k(x; x0 ). Kernel-based learning algorithms (cid:12)nd a
hypothesis ^f 2 F by solving some variant of the Regularized Risk Minimzation problem
f 2F " 1
F #
mXi=1
1
2 k f k2
^f = arg min
L(f (xi ); yi ) +
m
where L is a loss function of our choice. By the Representer Theorem [2], ^f is expressible
in the form ^(x) = Pm
i=1 (cid:11)i k(xi ; x) for some (cid:11)1 ; (cid:11)2 ; : : : ; (cid:11)m 2 R.
The idea expounded in [8] is to set up an analogous optimization problem for (cid:12)nding k itself
in the RKHS of a hyperkernel K : X (cid:2) X ! R, where X = X 2 . We will sometimes view K
as a function of four arguments, K ((x1 ; x0
1 ); (x2 ; x0
2 )), and sometimes as a function of two
1 ) and x2 = (x2 ; x0
pairs, K (x1 ; x2 ), with x1 = (x1 ; x0
2 ). To induce an RKHS K must be
positive de(cid:12)nite in the latter sense. Additionaly, we have to ensure that the solution of our
regularized risk minimization problem is itself a kernel. To this end, we require that the
1 ); (x2 ; x0
2 ) that we get by (cid:12)xing the (cid:12)rst two arguments of K ((x1 ; x0
1 (x2 ; x0
2 ))
functions Kx1 ;x0
be symmetric and positive de(cid:12)nite kernel in the remaining two arguments.
De(cid:12)nition 1. Let X be a nonempty set, X = X (cid:2) X and K : X (cid:2) X ! R with Kx ( (cid:1) ) =
K (x; (cid:1) ) = K ( (cid:1) ; x). Then K is cal led a hyperkernel on X if and only if
1. K is positive de(cid:12)nite on X and
2. for any x 2 X , Kx is positive de(cid:12)nite on X .
cone Kpd =
in the
lie
Denoting the RKHS of K by K, potential kernels
f k 2 K j k is pos.def. g. Unfortunately, there is no simple way of restricting kernel learn-
ing algorithms to Kpd . Instead, we will restrict ourselves to the positive quadrant K+ =
(cid:8) k 2 K j (cid:10)k ; Kx (cid:11) (cid:21) 0 8 x 2 X (cid:9), which is a subcone of Kpd .
The actual learning procedure involved in (cid:12)nding k is very similar to conventional kernel
methods, except that now regularized risk minimization is to be performed over all pairs of
data points:
K(cid:21) ;
K(cid:3) (cid:20)Q(X; Y ; k) +
1
2 k k k2
^k = arg min
where Q is a quality functional describing how well k (cid:12)ts the training data and K (cid:3) = K+ .
Several candidates for Q are described in [8].
If K(cid:3) has the property that for any S (cid:26) X the orthogonal pro jection of any k 2 K (cid:3) to the
subspace spanned by (cid:8)Kx j x 2 X (cid:9) remains in K(cid:3) , then bk is expressible as
mXi;j=1
mXi;j=1
bk(x; x0 ) =
(cid:11)ij K ((xi ; xj ); (x; x0 ))
(cid:11)ij K(xi ;xj ) (x; x0 ) =
for some real coe(cid:14)cients ((cid:11)ij )i:j . In other words, we have a hyper-representer theorem. It is
easy to see that for K(cid:3) = K+ this condition is satis(cid:12)ed provided that K ((x1 ; x0
1 ); (x2 ; x0
2 )) (cid:21) 0
1 ; x2 ; x0
for all x1 ; x0
2 2 X . Thus, in this case to solve (1) it is su(cid:14)cient to optimize the
variables ((cid:11)ij )m
i;j=1 , introducing the additional constraints (cid:11)ij (cid:21) 0 to enforce bk 2 K+ .
Finding functions that satisfy De(cid:12)nition 1 and also make sense in terms of regularization
theory or practical problem domains in not trivial. Some potential choices are presented in
[8]. In this paper we propose some new families of hyperkernels. The key tool we use is the
following simple lemma.
Lemma 1. Let fgz : X ! Rg be a family of functions indexed by z 2 Z and let h : Z(cid:2)Z ! R
be a kernel. Then
k(x; x0 ) = Z Z gz (x) h(z ; z 0 ) gz 0 (x0 ) dz dz 0
(3)

(2)

(1)

is a kernel on X . Furthermore, if h is pointwise positive (h(z ; z 0 ) (cid:21) 0) and f gz : X (cid:2) X ! R g
is a family of pointwise positive kernels, then
2 )) = Z Z gz1 (x1 ; x0
1 ) h(z1 ; z2 ) gz2 (x2 ; x0
1 ) ; (x2 ; x0
K ((x1 ; x0
2 ) dz1 dz2
1 ; x2 ; x0
2 )) (cid:21) 0 for al l x1 ; x0
1 ); (x2 ; x0
is a hyperkernel on X , and it satis(cid:12)es K ((x1 ; x0
2 2 X .

(4)

3 Convolution hyperkernels

kt (x; x0 ) =

One interpreation of a kernel k(x; x0 ) is that it quanti(cid:12)es some notion of similarity between
points x and x0 . For the Gaussian RBF kernel, and heat kernels in general, this similarity
can be regarded as induced by a di(cid:11)usion process in the ambient space [4]. Just as physical
substances di(cid:11)use in space, the similarity between x and x 0 is mediated by intermediate
points, in the sense that by virtue of x being similar to some x0 and x0 being similar to x0 ,
x and x0 themselves become similar to each other. This captures the natural transitivity of
similarity. Speci(cid:12)cally, the normalized Gaussian kernel on Rn of variance 2t = (cid:27) 2 ,
1
(4(cid:25) t)n=2 e(cid:0)k x(cid:0)x0 k2 =(4t) ;
satis(cid:12)es the well known convolution property
kt (x; x0 ) = Z kt=2 (x; x0 ) kt=2 (x0 ; x) dx0 :
Such kernels are by de(cid:12)nition homogenous and isotropic in the ambient space.
What we hope for from the hyperkernels formalism is to be able to adapt to the inhomoge-
neous and anisotropic nature of training data, while retaining the transitivity idea in some
form. Hyperkernels achieve this by weighting the integrand of (5) in relation to what is
\on the other side" of the hyperkernel. Speci(cid:12)cally, we de(cid:12)ne convolution hyperkernels by
setting

(5)

gz (x; x0 ) = r(x; z ) r(x0 ; z )
in (4) for some r : X (cid:2) X ! R. By (3), the resulting hyperkernel always satis(cid:12)es the
conditions of De(cid:12)nition 1.
De(cid:12)nition 2. Given functions r : X (cid:2)X ! R and h : X (cid:2)X ! R where h is positive de(cid:12)nite,
the convolution hyperkernel induced by r and h is
2 )) = Z Z r(x1 ; z1 ) r(x0
K ((x1 ; x0
1 ) ; (x2 ; x0
1 ; z1 ) h(z1 ; z2 ) r(x2 ; z2 ) r(x0
2 ; z2 ) dz1 dz2 :
A good way to visualize the structure of convolution
hyperkernels is to note that (6) is proportional to the
likelihood of the graphical model in the (cid:12)gure to the
right. The only requirements on the graphical model
are to have the same potential function  1 at each of
the extremities and to have a positive de(cid:12)nite potential
function  2 at the core.

(6)

3.1 The Gaussian hyperkernel

To make the foregoing more concrete we now investigate the case where r(x; x 0 ) and h(z ; z 0 )
are Gaussians. To simplify the notation we use the shorthand
1
(2(cid:25)(cid:27)2 )n=2 e(cid:0)k x(cid:0)x0 k2 =(2(cid:27)2 ) :
hx; x0 i(cid:27)2 =
The Gaussian hyperkernel on X = Rn is then de(cid:12)ned as
2 )) = ZX ZX hx1 ; z i(cid:27)2 hz ; x0
2 i(cid:27)2 dz dz 0 :
h hx2 ; z 0 i(cid:27)2 hz 0 ; x0
1 i(cid:27)2 hz ; z 0 i(cid:27)2
K ((x1 ; x0
1 ); (x2 ; x0

(7)

Fixing x and completing the square we have
(2(cid:25)(cid:27)2 )n exp(cid:18)(cid:0)
1 k2 (cid:17)(cid:19) =
2(cid:27)2 (cid:16) k z (cid:0)x1 k2 + k z (cid:0)x0
1
1
hx1 ; z i(cid:27)2 hz ; x0
1 i(cid:27)2 =
(cid:27)2 wwww z (cid:0)
2 wwww
(2(cid:25)(cid:27)2 )n exp (cid:18) (cid:0)
(cid:19) = hx1 ; x0
2
1 k2
x1 +x0
(cid:0) k x1 (cid:0)x0
1
1
1
1 i2(cid:27)2 hz ; x1 i(cid:27)2=2 ;
4(cid:27)2
where xi = (xi +x0
i )=2. By the convolution property of Gaussians it follows that
1 ); (x2 ; x0
K ((x1 ; x0
2 )) =
2 i2(cid:27)2 ZX ZX hx1 ; z i(cid:27)2 =2 hz ; z 0 i(cid:27)2
1 i2(cid:27)2 hx2 ; x0
hx1 ; x0
h hz ; x2 i(cid:27)2=2 dz dz 0 =
hx1 ; x0
1 i2(cid:27)2 hx2 ; x0
2 i2(cid:27)2 hx1 ; x2 i(cid:27)2+(cid:27)2
h
It is an important property of the Gaussian hyperkernel that it can be evaluated in closed
form. A noteworthy special case is when h(x; x0 ) = (cid:14)(x; x0 ), corresponding to (cid:27) 2
h ! 0. At
the opposite extreme, in the limit (cid:27) 2
h ! 1, the hyperkernel decouples into the product of
two RBF kernels.

(8)

:

(9)

Since the hyperkernel expansion (2) is a sum over hyperkernel evaluations with one pair of
arguments (cid:12)xed, it is worth examining what these functions look like:
(cid:19)
2 ) / exp(cid:18)(cid:0) k x1 (cid:0) x2 k2
h ) (cid:19) exp(cid:18)(cid:0) k x2 (cid:0) x0
2 k2
1 (x2 ; x0
Kx1 ;x0
2(cid:27) 0 2
2 ((cid:27)2 + (cid:27)2
with (cid:27) 0 = p2(cid:27) . This is really a conventional Gaussian kernel between x2 and x0
2 multiplied
by a spatially varying Gaussian intensity factor depending on how close the mean of x2 and
x0
2 is to the mean of the training pair. This can be regarded as a localized Gaussian, and
the full kernel (2) will be a sum of such terms with positive weights. As x2 and x0
2 move
around in X , whichever localized Gaussians are centered close to their mean will dominate
the sum. By changing the ((cid:11)ij ) weights, the kernel learning algorithm can choose k from a
highly (cid:13)exible class of potential kernels.
The close relationship of K to the ordinary Gaussian RBF kernel is further borne out
by changing coordinates to ^x = (x + x0 ) =p2 and ~x = (x (cid:0) x0 ) =p2, which factorizes the
hyperkernel in the form
h ) (cid:3)(cid:2)h ~x1 ; 0i(cid:27)2 h ~x2 ; 0i(cid:27)2 (cid:3):
K (( ^x1 ; ~x1 ); ( ^x2 ; ~x2 )) = ^K ( ^x1 ; ^x2 ) ~K ( ~x1 ; ~x2 ) = (cid:2)h ^x1 ; ^x2 i2((cid:27)2+(cid:27)2
Omitting details for brevity, the consequences of this include that K = ^K (cid:2) ~K, where ^K
is the RKHS of a Gaussian kernel over X , while ~K is the one-dimensional space gener-
ated by h ~x; 0i(cid:27)2 : each k 2 K can be written as k( ^x; ~x) = ^k( ^x) h ~x; 0i(cid:27)2 . Furthermore, the
regularization operator (cid:7) (de(cid:12)ned by hk ; k 0 iK = h(cid:7)k ; (cid:7)k 0 iL2 [10]) will be
7! h ~x; 0i(cid:27)2 Z e((cid:27)2+(cid:27)2
h ~x; 0i(cid:27)2 Z b(cid:20)(!) ei!x d!
h ) !2 =2 b(cid:20)(!) ei!x d!
where b(cid:20)(!) is the Fourier transform of bk(bx), establishing the same exponential regularization
penalty scheme in the Fourier components of ^k that is familiar from the theory of Gaussian
RBF kernels.
In summary, K behaves in ( ^x1 ; ^x2 ) like a Gaussian kernel with variance
2((cid:27)2 + (cid:27)2
h ), but in ~x it just e(cid:11)ects a one-dimensional feature mapping.

4 Anisotropic hyperkernels

With the hyperkernels so far far we can only learn kernels that are a sum of rotationally
invariant terms. Consequently, the learnt kernel will have a locally isotropic character. Yet,
rescaling of the axes and anisotropic dilations are one of the most common forms of variation
in naturally occurring data that we would hope to accomodate by learning the kernel.

4.1 The Wishart hyperkernel

(10)

(11)

(12)

where

hx; x0 i(cid:6) IW ((cid:6); C; r) =

We de(cid:12)ne the Wishart hyperkernel as
2 )) = Z(cid:6)(cid:23)0 ZX hx1 ; z i(cid:6) hz ; x0
K ((x1 ; x0
1 ); (x2 ; x0
1 i(cid:6) hx2 ; z i(cid:6) hz ; x0
2 i(cid:6) IW ((cid:6); C; r) dz d(cid:6):
1
(2(cid:25))n=2 j (cid:6) j1=2 e(cid:0)(x(cid:0)x0 )>(cid:6)(cid:0)1 (x(cid:0)x0 )=2 ;
hx; x0 i(cid:6) =
and IW ((cid:6); C; r) is the inverse Wishart distribution
j C j r=2
Zr;n j (cid:6) j(n+r+1)=2 exp (cid:0)(cid:0)tr (cid:0)(cid:6)(cid:0)1C (cid:1) =2(cid:1)
over positive de(cid:12)nite matrices (denoted (cid:6) (cid:23) 0) [6]. Here r is an integer parameter, C is an
n (cid:2) n positive de(cid:12)nite parameter matrix and Zr;n = 2 rn=2(cid:25)n(n(cid:0)1)=4 Qn
i=1 (cid:0)((r + 1(cid:0) i)=2) is
a normalizing factor. The Wishart hyperkernel can be seen as the anisotropic analog of (7)
in the limit (cid:27) 2
h ! 0, hz ; z 0 i(cid:27)2
h ! (cid:14)(z ; z 0 ). Hence, by Lemma 1, it is a valid hyperkernel. In
analogy with (8),
2 )) = Z(cid:6)(cid:23)0 hx1 ; x0
1 ); (x2 ; x0
K ((x1 ; x0
1 i2(cid:6) hx2 ; x0
2 i2(cid:6) hx1 ; x2 i(cid:6) IW ((cid:6); C; r) d(cid:6) :
By using the identity v>A v = tr(A(vv> )),
j C j r=2
(2(cid:25))n=2Zr;n j (cid:6) j(n+r+2)=2 exp (cid:0)(cid:0)tr (cid:0)(cid:6)(cid:0)1 (C +S )(cid:1) =2(cid:1) =
j C j r=2
Zr+1;n
j C + S j(r+1)=2 IW ( (cid:6) ; C +S; r + 1 ) ;
(2(cid:25))n=2Zr;n
where S = (x (cid:0) x0 )(x (cid:0) x0 )> . Cascading this through each of the terms in the integrand of
(11) and noting that the integral of a Wishart density is unity, we conclude that
j C jr=2
1 ); (x2 ; x0
K ((x1 ; x0
2 )) /
j C + Stot j(r+3)=2 ;
where Stot = S1 + S2 + S(cid:3) ; Si = 1
i )>; and S(cid:3) = (x1 (cid:0) x2 )(x1 (cid:0) x2 )> . We
2 (xi (cid:0) x0
i )(xi (cid:0) x0
2 k, and k x (cid:0) x0 k, the hyperkernel will favor
1 k, k x2 (cid:0) x0
can read o(cid:11) that for given k x1 (cid:0) x0
2 , and x (cid:0) x0 are close to parallel to each other and to the
1 , x2 (cid:0) x0
quadruples where x1 (cid:0) x0
largest eigenvector of C . It is not so easy to immediately see the dependence of K on the
1 ; x2 and x0
relative distances between x1 ; x0
2 .
To better expose the qualitative behavior of the Wishart hyperkernel, we (cid:12)x (x1 ; x0
1 ), assume
that C = cI for some c 2 R and use the identity (cid:12)(cid:12) cI + vv> (cid:12)(cid:12) = cn(cid:0)1 (cid:0)c + kvk2 (cid:1) to write
2 ) / "
(cid:0)c + 4 k x1 (cid:0) x2 k2 (cid:1)1=4 #(r+3)=2 " Qc (S1 + S(cid:3) ; S2 )
2 k2 (cid:1)1=4 #r+3
Qc (2S1 ; 2S(cid:3) )
1 (x2 ; x0
Kx1 ;x0
(cid:0)c + k x2 (cid:0) x0
where Qc (A; B ) is the a(cid:14)nity
Qc (A; B ) = j cI + 2A j1=4 (cid:1) j cI + 2B j1=4
:
j cI + A + B j1=2
This latter expression is a natural positive de(cid:12)nite similarity metric between positive de(cid:12)nite
matrices, as we can see from the fact that it is the overlap integral (Bhattacharyya kernel)
Qc (A; B ) = Z hhx; 0i(cI+2A)(cid:0)1 i1=2 hhx; 0i(cI+2B )(cid:0)1 i1=2
between two zero-centered Gaussian distributions with inverse covariances cI + 2A and cI +
2B , respectively [3].

dx

0.25

0.2

0.15

0.1

0.05

0

−0.05

−0.1

−0.15

−0.2

−0.25

0.2

0.1

0

−0.1

−0.2

−0.3

0.2

0.15

0.1

0.05

0

−0.05

−0.1

−0.15

−0.2

−0.25

−0.1

−0.05

0

0.05

−0.15

−0.1

−0.05

0

0.05

0.1

0.15

−0.2

−0.15

−0.1

−0.05

0

0.05

0.1

0.15

0.2

Figure 1: The (cid:12)rst two panes show the separation of ’3’s and ’8’s in the training and testing
sets respectively achieved by the Gaussian hyperkernel (the plots show the data plotted by
its (cid:12)rst two eigenvectors according to the learned kernel k). The right hand pane shows a
similar KernelPCA plot but based on a (cid:12)xed RBF kernel.

5 Experiments

We conducted preliminary experiments with the hyperkernels in relation learning between
pairs of datapoints. The idea here is that the learned kernel k naturally induces a distance
metric d(x; x0 ) = pk(x; x) (cid:0) 2k(x; x0 ) + k(x0 ; x0 ), and in this sense kernel learning is equiv-
alent to learning d. Given a labeled dataset, we can learn a kernel which e(cid:11)ectively remaps
the data in such a way that data points with the same label are close to each other, while
those with di(cid:11)erent labels are far apart.

(cid:24)ij

For classi(cid:12)cation problems (yi being the class label), a natural choice of quality functional
m2 Pm
similar to the hinge loss is Q(X; Y ; k) = 1
i;j=1 j 1 (cid:0) yij k(xi ; xj ) j+ , where j z j+ = z if
z (cid:21) 0 and j z j+ = 0 for z < 0, while yij = 1 if yi = yj . The corresponding optimization
problem learns k(x; x0 ) = Pm
i=1 Pm
j=1 (cid:11)ij K ((x; x0 ); (xi ; xj )) + b minimizing
(cid:11)ij (cid:11)i0 j 0 K ((xi ; xj ); (xi0 ; xj 0 )) + C Xi;j
2 Xi;j Xi0 ;j 0
1
sub ject to the classi(cid:12)cation constraints
(cid:11)i0 j 0 K ((xi0 ; xj 0 ); (xi ; xj )) + b (cid:17) (cid:21) 1 (cid:0) (cid:24)ij
yij (cid:16) Xi0 ;j 0
(cid:11)ij (cid:21) 0
(cid:24)ij (cid:21) 0
for all pairs of i; j 2 f1; 2; : : : ; mg. In testing we interpret k(x; x 0 ) > 0 to mean that x and
x0 are of the same class and k(x; x0 ) (cid:20) 0 to mean that they are of di(cid:11)erent classes.
As an illustrative example we learned a kernel (and hence, a metric) between a subset of
the NIST handwritten digits1 . The training data consisted of 20 ’3’s and 20 ’8’s randomly
rotated by (cid:6)45 degrees to make the problem slightly harder. Figure 1 shows that a kernel
learned by the above strategy with a Gaussian hyperkernel with parameters set by cross
validation is extremely good at separating the two classes in training as well as testing. In
comparison, in a similar plot for a (cid:12)xed RBF kernel the ’3’s and ’8’s are totally intermixed.
Interpreting this as an information retrieval problem, we can imagine in(cid:13)ating a ball around
each data point in the test set and asking how many other data points in this ball are of
the same class. The corresponding area under the curve (AUC) in the original space is just
0.5575, while in the hyperkernel space it is 0.7341.

1Provided at http://yann.lecun.com/exdb/mnist/ courtesy of Yann LeCun and Corinna
Cortes.

1

0.95

0.9

0.85

C
U
A

0.8

0.75

0.7

0.65

0.6
0.1

1

0.95

0.9

0.85

C
U
A

0.8

0.75

0.7

0.65

0.6
0.1

s
=0s
h

SVM
Linear HyperKernel
Conic HyperKernel

0.2

0.3

0.4

s

0.5

0.6

0.7

0.8

s
=4s
h

SVM
Linear HyperKernel
Conic HyperKernel

0.2

0.3

0.4

s

0.5

0.6

0.7

0.8

1

0.95

0.9

0.85

C
U
A

0.8

0.75

0.7

0.65

0.6
0.1

1

0.95

0.9

0.85

C
U
A

0.8

0.75

0.7

0.65

0.6
0.1

s
=1s
h

SVM
Linear HyperKernel
Conic HyperKernel

0.2

0.3

0.4

s

0.5

0.6

0.7

0.8

s
=6s
h

SVM
Linear HyperKernel
Conic HyperKernel

0.2

0.3

0.4

s

0.5

0.6

0.7

0.8

1

0.95

0.9

0.85

C
U
A

0.8

0.75

0.7

0.65

0.6
0.1

1

0.95

0.9

0.85

C
U
A

0.8

0.75

0.7

0.65

0.6
0.1

s
=2s
h

SVM
Linear HyperKernel
Conic HyperKernel

0.2

0.3

0.4

s

0.5

0.6

0.7

0.8

s
=10s
h

SVM
Linear HyperKernel
Conic HyperKernel

0.2

0.3

0.4

s

0.5

0.6

0.7

0.8

Figure 2: Test area under the curve (AUC) for Olivetti face recognition under varying (cid:27)
and (cid:27)h .

We ran a similar experiment but with multiple classes on the Olivetti faces dataset, which
consists of 92 (cid:2) 112 pixel normalized gray-scale images of 30 individuals in 10 di(cid:11)erent
poses. Here we also experimented with dropping the (cid:11)ij (cid:21) 0 constraints, which breaks the
positive de(cid:12)niteness of k , but might still give a reasonable similarity measure. The (cid:12)rst
case we call \conic hyperkernels", whereas the second are just \linear hyperkernels". Both
involve solving a quadratic program over 2m2 + 1 variables. Finally, as a baseline, we trained
an SVM over pairs of datapoints to predict yij , representing (xi ; xj ) with a concatenated
feature vector [xi ; xj ] and using a Gaussian RBF between these concatenations.

The results on the Olivetti dataset are summarized in Figure 2. We trained the system with
m = 20 faces and considered all pairs of the training data-points (i.e. 400 constraints) to (cid:12)nd
a kernel that predicted the labeling matrix. When speed becomes an issue it often su(cid:14)ces
to work with a subsample of the binary entries in the m (cid:2) m label matrix and thus avoid
having m2 constraints. Also, we only need to consider half the entries due to symmetry.
Using the learned kernel, we then test on 100 unseen faces and predict all their pairwise
kernel evaluations, in other words, 104 predicted pair-wise labelings. Test error rates are
averaged over 10 folds of the data. For both the baseline Gaussian RBF and the Gaussian
hyperkernels we varied the (cid:27) parameter from 0:1 to 0:6. For the Gaussian hyperkernel we
also varied (cid:27)h from 0 to 10(cid:27) . We used a value of C = 10 for all experiments and for all
algorithms. The value of C had very little e(cid:11)ect on the testing accuracy.

Using a conic hyperkernel combination did best in labeling new faces. The advantage over
SVMs is dramatic. The support vector machine can only achieve an AUC of less than 0:75
while the Gaussian hyperkernel methods achieve an AUC of almost 0:9 with only T = 20
training examples. While the di(cid:11)erence between the conic and linear hyperkernel methods
is harder to see, across all settings of (cid:27) and (cid:27)h , the conic combination outperformed the
linear combination over 92% of the time. The conic hyperkernel combination is also the
only method of the three that guarantees a true Mercer kernel as an output which can
then be converted into a valid metric. The average runtime for the three methods was
comparable. The SVM took 2:08s (cid:6) 0:18s, the linear hyperkernel took 2:75s (cid:6) 0:10s and
the conic hyperkernel took 7:63s (cid:6) 0:50s to train on m = 20 faces with m2 constraints. We
implemented quadratic programming using the MOSEK optimization package on a single
CPU workstation.

6 Conclusions

The main barrier to hyperkernels becoming more popular is their high computational de-
mands (out of the box algorithms run in O(m6 ) time as opposed to O(m3 ) in regular learn-
ing). In certain metric learning and on-line settings however this need not be forbidding,
and is compensated for by the elegance and generality of the framework.

The Gaussian and Wishart hyperkernels presented in this paper are in a sense canonical, with
intuitively appealing interpretations. In the case of the Gaussian hyperkernel we even have
a natural regularization scheme. Preliminary experiments show that these new hyperkernels
can capture the inherent structure of some input spaces. We hope that their introduction
will give a boost to the whole hyperkernels (cid:12)eld.

Acknowledgements

The authors wish to thank Zoubin Ghahramani, Alex Smola and Cheng Soon Ong for
discussions related to this work. This work was supported in part by National Science
Foundation grants IIS-0347499, CCR-0312690 and IIS-0093302.

References

[1] N. Cristianini, J. Shawe-Taylor, A. Elissee(cid:11), and J. Kandola. On kernel-target alignment.
In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information
Processing Systems 14, pages 367 { 373, Cambridge, MA, 2002. MIT Press.

[2] G. S. Kimeldorf and G. Wahba.
J. Math. Anal. Applic., 33:82{95, 1971.

Some results on Tchebyche(cid:14)an spline functions.

[3] R. Kondor and T. Jebara. A kernel between sets of vectors.
International Conference, ICML 2003, 2003.

In Machine Learning: Tenth

[4] R. Kondor and J. La(cid:11)erty. Di(cid:11)usion kernels on graphs and other discrete input spaces. In
Machine Learning: Proceedings of the Nineteenth International Conference (ICML ’02), 2002.

[5] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel
matrix with semi-de(cid:12)nite programming. Journal of Machine Learning Research, 5:27 { 72,
2004.

[6] T. P. Minka.
Inferring a Gaussian distribution, 2001.
http://www.stat.cmu.edu/ minka/papers/learning.html.

Tutorial paper available at

[7] C. S. Ong and A. J. Smola. Machine learning using hyperkernels.
International Conference on Machine Learning, 2003.

In Proceedings of the

[8] Cheng Soon Ong, Alexander J. Smola, and Robert C. Williamson. Hyperkernels. In S. Thrun
S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15,
pages 478{485. MIT Press, Cambridge, MA, 2003.

[9] Cheng Soon Ong, Alexander J. Smola, and Robert C. Williamson. Learning the kernel with
hyperkernels. Sumbitted to the Journal of Machine Learning Research, 2003.

[10] B. Sch(cid:127)olkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002.


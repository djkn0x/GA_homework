Teaching STAIR to Identify and Manipulate Tools

Deborah K. Meduna

December 15, 2006

Introduction

One of the current areas of research in robotics is in
the development of robots which can interact with
humans in performing a variety of tasks. In particu-
lar, there is a desire to enable robots to use ob jects as
tools to perform tasks. For example, assembly tasks
may require the robot to use a screwdriver to tighten
screws or a hammer for nailing. In this pro ject, I am
concerned with the problem of robotic tool manipula-
tion for the purpose of carrying out higher level tasks.
This involves developing a learning algorithm to de-
tect the tip of a tool (the control point), as well as
algorithms for incorporating the tool as an extension
of the robot system and controlling its subsequent
movement.
I performed this research on the STAIR robot in
the CS Department. Ashutosh Saxena and others
have recently completed work in identifying grasping
locations on generic ob jects 1 . Their algorithm en-
ables STAIR to pick up most ob jects (cups, dishes,
pens, tools, etc.) within its ﬁeld of vision. The goal
of my pro ject is to extend this work to allow STAIR
to manipulate tools once it has picked them up with
the grasping algorithm. One long-term application
being considered is to enable STAIR to construct a
bookcase from IKEA, given a set of simpliﬁed tasks.

1 STAIR Hardware

The STAIR robot consists of a robotic arm and sen-
sor network mounted to a Segue motion platform. It
is shown in Figure 1. The vision sensor used for the
pro ject is a Focus Robotics stereo camera with pixel
resolution of 768x480. I installed the camera and de-
veloped code to extract depth information from the
images which I then used to calibrate the camera im-
ages in the robot coordinate system. This was neces-

1Learning to Grasp Novel Ob jects using Vision, Ashutosh
Saxena, Justin Driemeyer, Justin Kearns, Chioma Osondu,
Andrew Y. Ng, 10th International Symposium of Experimental
Robotics, ISER 2006.

sary for translating tip estimates from pixel coordi-
nates to robot coordinates.

Figure 1: STAIR

The STAIR arm consists of 4 joints which are motor
controlled to rotate about single axes. Pre-existing
code controls movement of the arm to speciﬁed loca-
tions with respect to the arm base.

2 Tool Tip Identication

The ﬁrst step in allowing the robot to move and posi-
tion a given tool is to locate the tool tip - the control
point - with respect to the robot. Since the grasping
mechanism has already been developed on STAIR,
my speciﬁc goal was to identify the location of a tool
tip given STAIR’s image of the tool once it has been
grasped by the arm. I focused the tool tip identiﬁca-
tion process on a single tool: screwdrivers.

2.1 Logistic Regression Algorithm

To implement tool identiﬁcation, I used a weighted
logistic regression algorithm. Since the tool tip size
is relatively small in an image, the number of pos-
itive training examples is far less than the number
of negative training examples. Adding a weighting
parameter, α > 1, to positive examples serves to par-
tially oﬀset this imbalance. The resulting classiﬁer is

1

given by:

nX
i=1

ˆθ = argmin

w(i) (y (i) − hθ (x(i) ))2
(cid:26) α , y (i) = 1
1
where hθ (x(i) ) = g(θT x) =
1 + e−θT x
1
, y (i) = 0

and w(i) =

(1)

(2)

(3)

2.2.1 Selective Classiﬁed Patch Removal

The ﬁrst step in estimate consolidation is selective
removal of classiﬁed patches. Estimated patches were
removed if:

1. patches were far away from all other estimated
patches (isolated)

2. patches were far away from edges (as deﬁned
from canny edge detection)

3. patches had no neighboring patches BUT there
were other classiﬁed patch clusters in the image

These rules remove many false classiﬁers, always leav-
ing at least 2-5 classiﬁed patches in the image. Fig-
ure 3 shows an example of the removal process results.

Since individual pixels contain little information, the
image is reduced to a smaller set of 10 x 10 patches
of pixels prior to feature creation. The features used
include 9 Laws’ mask (relates orientation properties)
and 6 texture gradient (includes edge detectors) fea-
tures for a total of 15 features per image patch. These
are shown visually in Figure 2. In addition, I use a
canny edge detection feature with value 1 if an edge is
found within the patch and value 0 otherwise. This is
meant to account for the fact that the screwdriver tip
will likely be on or adjacent to an edge in the image.

Figure 2:
Image Texture Features. The ﬁrst nine
images are Laws mask ﬁlters, followed by the texture
gradient ﬁlters.

In addition to the 16 image features described above,
the algorithm appends the features for the neighbor-
ing patches to each patch feature vector in order to
gain more global information. The appended features
are associated with the patches to the top, bottom,
left and right of the current patch. Altogether, the
feature vector for a given patch contains 80 features:
x(i) ∈ Rn for i = 1..m, where n = 80. The number
of training examples is given by m = (rcN/q2) where
r and c are the number of rows and columns in the
image, N is the number of total training images, and
q = 10 is the patch size.
In general, m >> n so
the θ parameters can be uniquely determined from
gradient descent.

2.2 Consolidating Tip Estimates

The classiﬁer described above often positively clas-
siﬁes multiple patches within a new image.
It was
thus necessary to develop a method for consolidating
the estimates into a single, best estimate of the tip
location.

Figure 3: Classiﬁer Patch Removal. Red squares in-
dicate positive classiﬁed image patches

In this particular example, all patches were removed
except for the two on the tool tip because the two
patches are clustered.

In the future, the number of false classiﬁed patches
can also be reduced by limiting the image search
space. Since the tool is assumed to be grasped by
the arm, the gripper location can be used as a prior
to reduce the image space to a box around the grip-
per capable of containing all possible sizes and orien-
tations of known screwdrivers. This would alleviate
false classiﬁers far from the screwdriver location and
improve overall algorithm performance.

2.2.2 Screwdriver Edge Detection

Once the number of classiﬁed patches has been re-
duced, another algorithm uses the remaining patches
to identify the screwdriver edge in the image. Canny
edge detection is used to select edges in the image.
Then the edge is selected which is closest to the
remaining classiﬁed patches. Examples of this result
are shown in Figure 4.

The image on the left shows the resulting screwdriver
edge classiﬁcation for the images in Figure 3. Here,

2

Figure 4: Screwdriver Edge Identiﬁcation. Red lines
indicate identiﬁed edges. Green lines indicate identi-
ﬁed screwdriver edge.

the identiﬁed edge is deﬁned just along the screw-
driver end. The image on the right shows a case
where the identiﬁed screwdriver edge includes some
of the robot arm edge as well. This happens as a
result of the canny edge detector choices in breaking
up distinct edges. Adjusting parameters in the edge
detection could help reduce some of this eﬀect.

2.2.3 Finding Tip from Screwdriver Edge

Once an edge is identiﬁed, the next step is to se-
lect the screwdriver tip from the edge. Several paths
were explored for this purpose but have not yet been
successfully implemented. The two most promising
directions are:

1. Extract line segments from the identiﬁed edge
using the Hough Transform, then choose the tip
as the end point on the ’best’ line. Unforunately,
the longest line segment is often not the line
with the tip, requiring a more sophisticated al-
gorithm.

2. Estimate the curvature of the identiﬁed edge line
and choose the tip as the point corresponding
to maximum curvature. The diﬃculty with this
technique is in accurately calculating the curva-
ture.

These methods and others are still being explored.

3 Results

The classiﬁcation algorithm described in the previous
section was tested on two sets of data: one with 150
images taken in uncluttered background, the other
with 65 images taken in cluttered background. All
images were taken with the screwdriver in the robot
arm grasp. Examples of cluttered and uncluttered
images are shown in Figure 5.

Figure 5: Sample images. Left is cluttered back-
ground, right is uncluttered.

In order to characterize the performance of the clas-
siﬁer, I performed K-fold cross validation with a k of
10. The results are shown both before and after the
patch removal step in the following tables:

Table 1: K-Fold Cross Validation Results - No Patch
Removal

Uncluttered
Test
Background Train
Cluttered
Test
Background Train

FN
0.58
0.47
0.84
0.52

FP
4.1
3.74
8.66
8.13

TL
0.77
0.8
0.47
0.66

SE
0.67

0.52

Table 2: K-Fold Cross Validation Results - With
Patch Removal

Uncluttered
Test
Background Train
Cluttered
Test
Background Train

FN
0.67
0.55
1
0.63

FP
3
2.52
3.24
2.04

TL
0.69
0.72
0.43
0.58

SE
0.86

0.74

Here,
FN = Avg False Negatives/Image
FP = Avg False Positives/Image
TL = Fraction of Images which Correctly Classify
Actual Tip Location
SE = Fraction of Images which Correctly Identify
Screwdriver Edge

As expected, the average number of false positives is
reduced for both data sets when the patch removal
process is included. However, the ability of the
classiﬁer to correctly identify the trained tip location
is reduced. This worse perfomance is partially artiﬁ-
cial. The patch removal process sometimes removes
patches right at the tip because those patches are
often isolated. At the same time, however, the
patch removal process improves the accuracy of the
screwdriver edge detection signiﬁcantly for both data
sets (by about 18%). As expected, the uncluttered
background data set performed better than the

3

cluttered background data.

Finally, the similarity of the test and train results for
the uncluttered data set indicates that the algorithm
has relatively small variance. This also indicates that
in order to reduce the errors further, I will likely need
to include additional features. Additional features
could include other edge detector algorithms or other
image characteristics such as hue.

4 Tool Incorporation on STAIR

In order to use the tool tip estimate to manipulate
the tool using STAIR, the estimate is ﬁrst used to
compute the tool transformation vector. The result-
ing vector is then incorporated into the prexisting
STAIR control code to move the tool to a speciﬁed
location.

4.1 Tool Transformation Vector

Figure 6: Diagram of Screwdriver in Arm

The tool transformation vector gives the tool tip
position in the gripper coordinate frame. Once in
gripper coordinates, the tool tip position can be
determined with respect to any part of the robot
arm through pre-existing coordinate transformation
matrices. This then allows for control of the tip
position using pre-existing code.

The screwdriver and gripper coordinate frames are
illustrated, along with the robot arm, in Figure 6.
In terms of the parameters in this ﬁgure, the tool
 ls cos(θs ) + lg

transformation vector is given by:
0
ls sin(θs )
1
If TBg is the transformation matrix from the gripper
tip = TBg ~xg
frame to the arm base frame, then ~xB
tip .

~xg
tip =

(4)

The tip location and the gripper location with respect
to the base can then be related as follows:

~b
1)T = TB4
0
g = TB4 (l4g
0
~xB
tip − TB4
tip − ~xB
~b
g = TBg ~xg
~xB

(5)

(6)

In Equation 6, TB4 is the transformation matrix from
the arm base to the 4th joint (right before the grip-
per), and l4g is the length of the arm segment from
joint 4 to the gripper. The resulting expression for
the tool vector is:

(7)

tip − ~xB
Bg TBg )−1T T
~b]
~xg
g + TB4
Bg [~xB
tip = (T T
As the equation indicates, in order to ﬁnd the tool
transformation vector, all that is necessary is a set
of gripper and tool tip positions with respect to the
base. I programmed a pre-planned tra jectory to move
the grasped tool through 20 distinct positions and
orientations. At each point, the gripper position is
stored and the tool tip position with respect to the
base is estimated using the identiﬁcation algorithm
from Section 4. The estimated tool tip position in the
image plane is transferred to a position with respect
to the arm base using the camera to robot transfor-
mation matrix (see Section 1).

4.2 Commanding Tool Movement

Once the tool transformation vector has been deter-
mined, it can be incorporated into the pre-existing
STAIR control code. The current code commands
movements to STAIR’s arm joints based on mini-
mizing a cost function which exponentially penalizes
for arm joints being too close to obstacles and walls
and being far away from the goal. The goal is a 3D
position in space, relative to the robot arm base,
specifying the gripper location.

The current incorporation of the tool
into this
positioning system uses the tool vector to specify
the tool tip position as the goal and penalizes for
distance between the goal position and the tool tip
position at each iteration. This implementation
provides accurate tool tip positioning to within a
radius of 5 mm of the goal. However, it does not
yet allow for specifying tool orientation in the ﬁnal
position state. This will be a future extension.

Figure 7 shows an example of moving the tool to a
desired location. Here the tool goal and tool vector

4

were hard coded ahead of time, but the positioning
system was all automated. Once the tool tip estima-
tion algorithm is complete, the system will be able
to seamlessly transition from tool calibration to tool
positioning without external input.

Adjusting these parameters will give further insight
into the problems arising in each step of the algo-
rithm.

Finally, once the process has been acheived for the
screwdriver, further extensions of this work can be
done to include other tools such as hammers, pliers,
etc.

Acknowledgements

I would like to acknowledge and thank Ashutosh Sax-
ena and Andrew Ng for their help on all aspects
of this pro ject including primarily algorithm devel-
opment and pro ject direction.
I also thank Justin
Drieymeyer and Morgan Quigley for invaluable hard-
ware and software help on STAIR.

Figure 7: Moving tool tip from initial position to
desired location

5 Conclusions

The goal of being able to identify a grasped tool’s
orientation and move it to a desired location was
almost achevied. Many of the sub-steps proved very
successful, including the tool transformation vector
calculation and the incorporation of the tool into the
robot positioning system. The primary missing link
is the ability to predict a single tool tip location from
an identiﬁed screwdriver edge. The selective patch
removal and the screwdriver edge identiﬁcation
algorithms seem to give promising results in terms of
progressing towards a single tip estimate. The ﬁnal
step in ﬁnding the tip from the screwdriver edge
will likely be a combination of hough transform and
concavity analysis.

In addition, work can be performed on improving the
performance of the algorithm for cluttered and un-
structured environments. Much of the algorithm per-
formance improvement can be explored by analyzing
the eﬀect of the numerous parameters in the system.
These include:
1. α, the weighting parameter for the regression al-
gorithm

2. q , the patch size for the feature creation

3. the canny edge detection parameters

4. the distance parameter in the patch removal pro-
cess corresponding to the threshold at which
patches are kept

5


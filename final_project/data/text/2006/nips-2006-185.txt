Attentional Processing on a Spike-Based VLSI Neural
Network

Yingxue Wang, Rodney Douglas, and Shih-Chii Liu
Institute of Neuroinformatics
University of Zurich and ETH Zurich
Winterthurerstrasse 190
CH-8057 Zurich, Switzerland
yingxue,rjd,shih@ini.phys.ethz.ch

Abstract

The neurons of the neocortex communicate by asynchronous events called action
potentials (or ’spikes’). However, for simplicity of simulation, most models of
processing by cortical neural networks have assumed that the activations of their
neurons can be approximated by event rates rather than taking account of individ-
ual spikes. The obstacle to exploring the more detailed spike processing of these
networks has been reduced considerably in recent years by the development of
hybrid analog-digital Very-Large Scale Integrated (hVLSI) neural networks com-
posed of spiking neurons that are able to operate in real-time. In this paper we de-
scribe such a hVLSI neural network that performs an interesting task of selective
attentional processing that was previously described for a simulated ’pointer-map’
rate model by Hahnloser and colleagues. We found that most of the computational
features of their rate model can be reproduced in the spiking implementation; but,
that spike-based processing requires a modiﬁcation of the original network archi-
tecture in order to memorize a previously attended target.

1 Introduction

The network models described in the neuroscience literature have frequently used rate equations
to avoid the difﬁculties of formulating mathematical descriptions of spiking behaviors; and also to
avoid the excessive computational resources required for simulating spiking networks. Now, the
construction of multi-chip hybrid VLSI (hVLSI) systems that implement large-scale networks of
real-time spiking neurons and spike-based sensors is rapidly becoming a reality [3–5, 7], and so it
becomes possible to explore the performance of event-based systems in various processing tasks and
network behavior of populations of spiking, rather than rate neurons.
In this paper we use an hVLSI network to implement a spiking version of the ’pointer-map’ archi-
tecture previously described for rate networks by Hahnloser and colleagues [2]. In this architecture,
a small number of pointer neurons are incorporated in the feedback of a recurrently connected net-
work. The pointers steer the feedback onto the map, and so focus processing on the attended map
neurons. This is an interesting architecture because it reﬂects a general computational property of
sensorimotor and attentional/intentional processing based on pointing. Directing attention, foveat-
ing eyes, and reaching limbs all appeal to a pointer like interaction with the world, and such pointing
is known to modulate the responses of neurons in a number of cortical and subcortical areas.
The operation of the pointer-map depends on the steady focusing of feedback on the map neurons
during the period of attention.
It is easy to see how this steady control can be achieved in the
neurons have continuous, rate outputs; but it is not obvious whether this behavior can be achieved
also with intermittently spiking neural outputs. Our objective was thus to evaluate whether networks

of spiking neurons would be able to combine the beneﬁts of both event-based processing and the
attentional properties of pointer-map architecture.

2 Pointer-Map Architecture

A pointer-map network consists of two reciprocally connected populations of excitatory neurons.
Firstly, there is a large population of map neurons that for example provide a place encoding of
some variable such as the orientation of a visual bar stimulus. A second, small population of pointer
neurons exercises attentional control on the map. In addition to the reciprocal connections between
the two populations, the map neurons receive feedforward (e.g. sensory) input; and the pointer
neurons receive top-down attentional inputs that instruct the pointers to modulate the location and
intensity of the processing on the map (see Fig. 1(a)). The important functional difference between
conventional recurrent networks (equivalently, ’recurrent maps’) and the pointer-map, is that the
pointer neurons are inserted in the feedback loop, and so are able to modulate the effect of the
feedback by their top-down inputs. The usual recurrent excitatory connections between neurons
are replaced in the pointer-map by recurrent connections between the map neurons and the pointer
neurons that have sine and cosine weight proﬁles. Consequently, the activities of the pointer neurons
generate a vectorial pattern of recurrent excitation whose direction points to a particular location on
the map (Fig. 1(b)). Global inhibition provides competition between the map neurons, so that overall
the pointer-map behaves as an attentionally selective soft winner-take-all network.

Figure 1: Pointer-map architecture. (a) Network consists of two layers of excitatory neurons. The
map layer receives feedforward sensory inputs and inputs from two pointer neurons. The pointer
neurons receive top-down attentional inputs and also inputs from the map layer. The recurrent
connections between the map neurons and pointer neurons are set according to sine and cosine
proﬁles. (b) The interaction between pointer neurons and map neurons. Each circle indicates the
activity of one neuron. Clear circles indicate silent neurons and the sizes of the gray circles are
proportional to the activation of the active neurons. The vector formed by the activities of the
two pointer neurons on this angular plot points in the direction (the pointer angle γ ) of the map
neurons where the pointer-to-map input is the largest. The map-to-pointer input is proportional to
the population vector of activities of the map neurons.

3 Spiking Network Chip Architecture

We implemented the pointer-map architecture on a multi-neuron transceiver chip fabricated in a 4-
metal, 2-poly 0.35µm CMOS process. The chip (Fig. 2) has 16 VLSI integrate-and-ﬁre neurons, of
which one acts as the global inhibitory neuron. Each neuron has 8 input synapses (excitatory and
inhibitory). The circuit details of the soma and synapses are described elsewhere [4].
Input and output spikes are communicated within and between chips using the asynchronous Ad-
dress Event Representation (AER) protocol [3]. In this protocol the action potentials that travel
along point-to-point axonal connections are replaced by digital addresses on a bus that are usually

Figure 2: Architecture of multi-neuron chip. The chip has 15 integrate-and-ﬁre excitatory neurons
and one global inhibitory neuron. Each neuron has 8 input synapses. Input spikes and output spikes
are communicated using an asynchronous handshaking protocol called Address Event Representa-
tion. When an input spike is to be sent to the chip, the handshaking signals, Req and Ack, are used
to ensure that only valid addresses on a common digital bus are latched and decoded by X- and
Y-decoders. The arbiter block arbitrates between all outgoing neuron spikes; and the neuron spike
is sent off as the address of the neuron on a common digital bus through two handshaking signals
(Reqout and Ackout). The synaptic weights of 2 out of the 8 synapses can be speciﬁed uniquely
through an on-chip Digital-to-Analog converter that sets the synaptic weight of each synapse before
that particular synapse is stimulated. The synaptic weight is speciﬁed as part of the digital address
that normally codes the synaptic address.

the labels of source neurons and/or target synapses. In our chip, ﬁve bits of the AER address space
are used to encode also the synaptic weight [1]. An on-chip Digital-to-Analog Converter (DAC)
transforms the digital weights into the analog signals that set the individual efﬁcacy of the excitatory
synapses and inhibitory synapses for each neuron (Fig. 2).

(a)

(b)

Figure 3: Resulting spatial distribution of activity in map neurons in response to attentional input
to pointer neurons. The frequencies of the attentional inputs to P1, P2 are (a) [200Hz, 0Hz] (b)
[0Hz,200Hz]. The y-axis shows the ﬁring rate (Hz) of the map neurons (1–9) listed on the x-axis.
The polar plot on the side of each ﬁgure shows the pointer angle γ described by the pointer neuron
activities.

4 Experiments

Our pointer-map was composed of a total of 12 neurons: 2 served as pointer neurons; 9 as map neu-
rons; and 1 as the global inhibitory neuron. The synaptic weights of these neurons have a coefﬁcient
of variance in synaptic efﬁcacy of about 0.25 due to silicon process variations. Through the on-chip
DAC, we were able to reduce this variance for the excitatory synapses by a factor of 10. We did not
compensate for the variance in the inhibitory synapses because it was technically more challeng-
ing to do that. The synaptic weights from each pointer neuron to every map neuron j = 1, 2, ..., 9
(Fig. 4(a)) were set according to the proﬁle shown in Fig. 1(a). We compared the match between
the programmed spatial connnectivity and the desired sine/cosine proﬁle by activating only the top-
down connections from the pointer neurons to the map neurons, while the bottom-up connections
from the map neurons to the pointer neurons, and global inhibition were inactivated. In fact, because
of the lower signal resolution, chip mismatch, and system noise, the measured proﬁles were only a
qualitative match to a sine and cosine (Fig. 3), and the worst case variation from the ideal value was
up to 50% for very small weights. Nevertheless, in spite of the imperfect match, we were able to
reproduce most of the observations of [2].

(a)

(b)

Figure 4: Network architecture used on the chip for attentional modulation. (a) Originally proposed
pointer-map architecture. (b) New network architecture with no requirement for strong excitatory
recurrent connections. The global inhibition is now replaced by neuron-speciﬁc inhibition.

4.1 Attentional Input Control

We tested the attentional control of the pointer neurons for this network (Fig. 4(a)) with activated
recurrent connections and global inhibition. In addition, a common small constant input was applied
to all map neurons. The location and activity on the map layer can be steered via the inputs to the
pointer neurons, as seen in the three examples of Fig. 5. These results are similar to those observed
by Hahnloser and colleagues in their rate model.

4.2 Attentional Target Selection

One computational feature of the rate pointer-map is its multistablity: If two or more sensory stimuli
are presented to the network, strong attentional inputs to the pointer can select one of these stimuli
even if the stimulus is not the strongest one. The preferred stimulus depends on the initial activities
of the map and pointer neurons. Moreover, attentional inputs can steer the attention to a different lo-
cation on the map, even after a stronger stimulus is selected initially. We repeated these experiments
(Fig. 4 of [2]) for the spiking network. In our experiments, only two map neurons received feed-
forward sensory inputs which consist of two regular spike trains of different frequencies. As shown
in Fig. 6(a), the map neuron with the stronger feedforward input was selected. Attention could be
steered to a different part of the map array by providing the necessary attentional inputs. And, the
map neuron receiving the weaker stimulus could suppress the activity of another map neuron.
Furthermore, the original rate model can produce attentional memorization effects, that is, the loca-
tion of the map layer activity is retained even after the inputs to the pointer neurons are withdrawn.

(1)
(2)

However, we were unsuccessful in duplicating the results of these experiments (see Fig. 6(a)) be-
cause the recurrent connection strength parameter α had to be greater than 1.
To explain why this strong recurrent connection strength was necessary, we ﬁrst describe the steady
state rate activities M1 and M2 , of two arbitrary map neurons that are active:
M1 = bm1 − β (M1 + M2 ) + α (cos θ1P1 + sin θ1P2 )c+
M2 = bm2 − β (M1 + M2 ) + α (cos θ2P1 + sin θ2P2 )c+
where P1 , P2 are the steady state rate activities of the pointer neurons; m1 , m2 are the activities of
the map neurons due to the sensory inputs; β and α determine the strength of inhibitory and ex-
citatory connections respectively; α cos θi and α sin θi determine the connection strengths between
pointer neurons and map neurons i for θi ∈ [0o , 90o ].
The activities of the pointer neurons are given by
P1 = bp1 + α (cos θ1M1 + cos θ2M2 )c+
P2 = bp2 + α (sin θ1M1 + sin θ2M2 )c+
where p1 and p2 are the acitivities induced by inputs to the two pointer neurons.
Through substitution of Eqn.(3)(4) into Eqn.(1)(2) respectively, and assuming p1 , p2 = 0, it shows
p1 − cos (θ1 − θ2 )
that in order to satisfy the condition that M1 > M2 for m1 < m2 , we need
1
> 1.
There are several factors that make it difﬁcult for us to reproduce the attentional memorization
experiments. Firstly, since we are only using a small number of neurons, each input spike has to
create more than one output spike from a neuron in order to satisfy the above condition. On the
one hand, this is very hard to implement, because the neurons have a refractory period, any input
currents during this time will not inﬂuence the neuron. It means that we can not use self-excitation to
get an effective α > 1. On the other hand, even for α = 1 (one input spike causes one output spike),
it can easily lead to instability in the network because the timing of the arrival of the inhibitory and
the excitatory inputs becomes a critical factor of the system stability. Secondly, the network has to
operate in a hard winner-take-all mode because of the variance in the inhibitory synaptic efﬁcacies.
This means that the neuron is reset to its resting potential whenever it receives an inhibitory spike,
thus removing all memory.

(3)
(4)

α >

(5)

4.3 Attentional Memorization

By modifying the network architecture (see Fig. 4(b)), we were able to avoid using the strong exci-
tatory connections as required in the original network. In our modiﬁed architecture, the inhibition
is no longer global. Instead, each neuron inhibits all other neurons in the map population but itself.
The steady state rate activities M1 and M2 are now given by
M1 = bm1 − β M2 + α (cos θ1P1 + sin θ1P2 )c+
(6)
M2 = bm2 − β M1 + α (cos θ2P1 + sin θ2P2 )c+
(7)
The equations for the steady-state pointer neuron activities P1 and P2 remain as before. The new
p1 − cos (θ1 − θ2 )
condition for α is now
1 − β
which means that α can be smaller than one. The intuitive explanation for the decrease of α is
that, in the original architecture, the global inhibition inhibits all the map neurons including the
winner. Therefore, in order to memorize the attended stimulus, the excitatory connections need
to be strengthen to compensate for the self inhibition. But in the new scenario, we delete the self
inhibitions, which then releases the requirement for strong excitations.
Using this new architecture, we performed the same experiments as described in Section 4.2 and
we were now able to demonstrate attentional memorization. That is, the attended neuron with the
weaker sensory input stimulus survived even after the attentional inputs were withdrawn. The same
qualitative results were obtained even if all the remaining map neurons had a low background ﬁring
rate which mimic the effect of weak sensory inputs to different locations.

α >

(8)

(a)

(b)

(c)

Figure 5: Results of experiments showing responses of map neurons for 3 settings of input strengths
to the pointer neurons. Each map neuron has a background ﬁring rate of 30Hz measured in the
absence of activated recurrent connections and global inhibition. The attentional inputs to pointer
neurons P1 and P2 are (a) [700Hz,50Hz], (b) [700Hz,700Hz], (c) [50Hz,700Hz]. The y-axis shows
the ﬁring rate (Hz) of the map neurons (1–9) listed on the x-axis.

5 Conclusion

In this paper, we have described a hardware ’pointer-map’ neural network composed of spiking
neurons that performs an interesting task of selective attentional processing previously described in
a simulated ’pointer-map’ rate model by Hahnloser and colleagues.
Neural network behaviors in computer simulations that use rate equations would likely be observed
also in spiking networks if many input spikes can be integrated before the post-synaptic neuron’s
threshold is reached. However, extensive integration is not possible for practical electronic networks,
in which there are relatively small numbers of neurons and synapses. We were ﬁnd that most of the
computational features of their simulated rate model could be reproduced in our hardware spik-
ing implementation despite imprecisions of synaptic weights, and the inevitable fabrication related
variability in the performance of individual neurons.
One signiﬁcant difference between our spiking implementation and the rate model is the mechanism
required to memorize a previously attended target. In our spike-based implementation, it was nec-
essary to modify the original pointer-map architecture so that the inhibition no longer depends on a
single global inhibitory neuron. Instead, each excitatory neuron inhibits all other neurons in the map
population but itself.
Unfortunately, this approximate equivalence between excitatory and inhbitory neurons is inconsis-
tent with the anatomical observation that only about 15% of cortical neurons are inhibitory. How-
ever, the original architecture could probably work if we had larger populations of map neurons,
more synapses, and/or NMDA-like synapses with longer time constants. This is a scenario that we
will explore in the future along with better characterization of the switching time dynamics of the
attentional memorization experiments.

(a)

(b)

Figure 6: Results of attentional memorization experiments using the two different architectures in
Fig. 4. (a) Results from original architecture. The sensory inputs to two map neurons M3 and M7
were set to [200Hz,230Hz]. The experiment was divided into 5 phases. In phase 1, the bottom-
up connections and inhibitory connections were inactivated. In phase 2, the inhibitory connections
were activated thus map neuron M3 which received the weaker input, was suppressed. In phase 3,
the bottom-up connections were activated. Map neuron M3 was now active because of the steering
activity from the pointer neurons. In phase 4, the pointer neurons P1 and P2 were stimulated by at-
tentional inputs of frequencies [700Hz,0Hz] which ampliﬁed the activity of M3 but the map activity
returned back to the activity shown in phase 3 once the attentional inputs were withdrawn in phase
5. (b) Results from modiﬁed architecture. The sensory inputs to M3 and M7 were of frequencies
[200Hz,230Hz] for the red curve and [40Hz,50Hz] for the blue curve. The 5 phases in the exper-
iment were as described in (a). However in phase 5, we could see that map neuron M3 retained
its activity even after the attentional inputs were withdrawn (attentional inputs to P1 and P2 were
[700Hz,0Hz] for the red curve and [300Hz,0Hz] for the blue curve).

Acknowledgments

The authors would like to thank M. Oster for help with setting up the AER infrastructure, S. Zahnd
for the PCB design, and T. Delbr ¨uck for discussions on the digital-to-analog converter circuits. This
work is partially supported by ETH Research Grant TH-20/04-2, and EU grant ”DAISY” FP6-2005-
015803.

References

[1] Y. X. Wang and S. C. Liu, “Programmable synaptic weights for an aVLSI network of spiking
neurons,” in Proceedings of the 2006 IEEE International Symposium on Circuits and Systems,
pp. 4531–4534, 2006.
[2] R. Hahnloser, R. J. Douglas, M. A. Mahowald, and K. Hepp, “Feedback interactions between
neuronal pointers and maps for attentional processing,” Nature Neuroscience, vol. 2, pp. 746–
752, 1999.
[3] K. A. Boahen, “Point-to-point connectivity between neuromorphic chips using address event,”
IEEE Transactions on Circuits and Systems II, vol. 47, pp. 416-434, 2000.
[4] S.-C. Liu, and R. Douglas, “Temporal coding in a network of silicon integrate-and-ﬁre neu-
ron,” IEEE Transactions on Neural Networks: Special Issue on Temporal Coding for Neural
Information Processing, vol 15, no 5, Sep., pp. 1305-1314, 2004.

[5] S. R. Deiss, R. J. Douglas, and A. M. Whatley, “A pulse-coded communications infrastructure
for neuromorphic systems,” in Pulsed Neural Networks, W. Maass and C. M. Bishop, Eds.
Boston, MA: MIT Press, 1999, ch. 6, pp. 157–178, ISBN 0-262-13350-4.
[6] C. Itti, E. Niebur, and C. Koch, “A model of saliency-based fast visual attention for rapid scene
analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, no. 11,
pp. 1254–1259, Apr 1998.
[7] G. Indiveri, T. Horiuchi, E. Niebur, and R. Douglas, “A competitive network of spiking VLSI
neurons,” in World Congress on Neuroinformatics, F. Rattay, Ed.
Vienna, Austria: AR-
GESIM/ASIM Verlag, Sept 24–29 2001, aRGESIM Reports.
[8] M. Oster and S.-C. Liu, “Spiking inputs to a winner-take-all network,” in Advances in Neural
Information Processing Systems. Cambridge, MA: MIT Press, 2006, vol. 18.


Fundamental Limitations of Spectral Clustering

Boaz Nadler∗, Meirav Galun
Department of Applied Mathematics and Computer Science
Weizmann Institute of Science, Rehovot, Israel 76100
boaz.nadler,meirav.galun@weizmann.ac.il

Abstract

Spectral clustering methods are common graph-based approaches to clustering of
data. Spectral clustering algorithms typically start from local information encoded
in a weighted graph on the data and cluster according to the global eigenvectors of
the corresponding (normalized) similarity matrix. One contribution of this paper
is to present fundamental limitations of this general local to global approach. We
show that based only on local information, the normalized cut functional is not a
suitable measure for the quality of clustering. Further, even with a suitable simi-
larity measure, we show that the ﬁrst few eigenvectors of suc h adjacency matrices
cannot successfully cluster datasets that contain structures at different scales of
size and density. Based on these ﬁndings, a second contribut
ion of this paper is
a novel diffusion based measure to evaluate the coherence of individual clusters.
Our measure can be used in conjunction with any bottom-up graph-based cluster-
ing method, it is scale-free and can determine coherent clusters at all scales. We
present both synthetic examples and real image segmentation problems where var-
ious spectral clustering algorithms fail. In contrast, using this coherence measure
ﬁnds the expected clusters at all scales.

Keywords: Clustering, kernels, learning theory.

1

Introduction

Spectral clustering methods are common graph-based approaches to (unsupervised) clustering of
i=1 ⊂ Rp , these methods ﬁrst construct a weighted graph
data. Given a dataset of n points {xi}n
G = (V , W ), where the n points are the set of nodes V and the weighted edges Wi,j are computed
by some local symmetric and non-negative similarity measure. A common choice is a Gaussian
kernel with width σ , where k · k denotes the standard Euclidean metric in Rp
Wi,j = exp (cid:18)− kxi − xj k2
(cid:19)
2σ2
In this framework, clustering is translated into a graph partitioning problem. Two main spectral
approaches for graph partitioning have been suggested. The ﬁrst is to construct a normalized cut
(conductance) functional to measure the quality of a partition of the graph nodes V into k clusters[1,
2]. Speci ﬁcally, for a 2-cluster partition V = S ∪ (V \ S ) minimizing the following functional is
suggested in [1]
Wi,j (cid:20) 1
a(V \ S ) (cid:21)
1
φ(S ) = Xi∈S,j∈V \S
a(S )
where a(S ) = Pi∈S,j∈V Wi,j . While extensions of this functional to more than two clusters are
possible, both works suggest a recursive top-down approach where additional clusters are found by
∗Corresponding author. www.wisdom.weizmann.ac.il/∼nadler

(1)

+

(2)

minimizing the same clustering functional on each of the two subgraphs. In [3] the authors also
propose to augment this top-down approach by a bottom-up aggregation of the sub-clusters.

As shown in [1] minimization of (2) is equivalent to maxy (y
T Dy), where D is a diag-
T W y)/(y
onal n × n matrix with Di,i = Pj Wi,j , and y is a vector of length n that satis ﬁes the constraints
T D1 = 0 and y i ∈ {1, −b} with b some constant in (0, 1). Since this maximization problem is
y
NP-hard, both works relax it by allowing the vector y to take on real values. This approximation
leads to clustering according to the eigenvector with second largest eigenvalue of the normalized
graph Laplacian, W y = λDy . We note that there are also graph partitioning algorithms based on a
non-normalized functional leading to clustering according to the second eigenvector of the standard
graph Laplacian matrix D − W, also known as the Fiedler vector [4].
A second class of spectral clustering algorithms does not recursively employ a single eigenvector,
but rather proposes to map the original data into the ﬁrst k eigenvectors of the normalized adjacency
matrix (or a matrix similar to it) and then apply a standard clustering algorithm such as k-means
on these new coordinates, see for example [5]-[11] and references therein. In recent years, much
theoretical work was done to justify this approach. Belkin and Niyogi [8] showed that for data uni-
formly sampled from a manifold, these eigenvectors approximate the eigenfunctions of the Laplace
Beltrami operator, which give an optimal low dimensional embedding under a certain criterion.
Optimality of these eigenvectors, including rotations, was derived in [9] for multiclass spectral clus-
tering. Probabilistic interpretations, based on the fact that these eigenvectors correspond to a random
walk on the graph were also given by several authors [11]-[15]. Limitations of spectral clustering
in the presence of background noise and multiscale data were noted in [10, 16], with suggestions to
replace the uniform σ2 in eq. (1) with a location dependent scale σ(xi )σ(xj ).
The aim of this paper is to present fundamental limitations of spectral clustering methods, and
propose a novel diffusion based coherence measure to evaluate the internal consistency of individ-
ual clusters. First, in Section 2 we show that based on the isotropic local similarity measure (1),
the NP-hard normalized cut criterion may not be a suitable global functional for data clustering.
We construct a simple example with only two clusters, where we prove that the minimum of this
functional does not correspond to the natural expected partitioning of the data into its two clusters.
Further, in Section 3 we show that spectral clustering suffers from additional limitations, even with
a suitable similarity measure. Our theoretical analysis is based on the probabilistic interpretation
of spectral clustering as a random walk on the graph and on the intimate connection between the
corresponding eigenvalues and eigenvectors and the characteristic relaxation times and processes of
this random walk. We show that similar to Fourier analysis, spectral clustering methods are global
in nature. Therefore, even with a location dependent σ(x) as in [10], these methods typically fail to
simultaneously identify clusters at different scales. Based on this analysis, we present in Section 4
simple examples where spectral clustering fails. We conclude with Section 5, where we propose a
novel diffusion based coherence measure. This quantity measures the coherence of a set of points as
all belonging to a single cluster, by comparing the relaxation times on the set and on its suggested
partition. Its main use is as a decision tool whether to divide a set of points into two subsets or leave
it intact as a single coherent cluster. As such, it can be used in conjunction with either top-down or
bottom-up clustering approaches and may overcome some of their limitations. We show how use of
this measure correctly clusters the examples of Section 4, where spectral clustering fails.

2 Unsuitability of normalized cut functional with local information

As reported in the literature, clustering by approximate minimization of the functional (2) performs
well in many cases. However, a theoretical question still remains: Under what circumstances is
this functional indeed a good measure for the quality of clustering ? Recall that the basic goal of
clustering is to group together highly similar points while setting apart dissimilar ones. Yet this
similarity measure is typically based only on local information as in (1). Therefore, the question can
be rephrased - is local information sufﬁcient for global clu stering ?

While this local to global concept is indeed appealing, we show that it does not work in general. We
construct a simple example where local information is insufﬁcient for correct clustering according
to the functional (2). Consider data sampled from a mixture of two densities in two dimensions
1
2

[pL,ε (x1 , x2 ) + pG (x1 , x2 )]

p(x) = p(x1 , x2 ) =

(3)

Original Data

Normalized cut σ = 0.05

3

2

1

0

−1

−2

3

2

1

0

−1

−2

2

4
(a)

6

2

4
(b)

6

Figure 1: A dataset with two clusters and result of normalized cut algorithm [2]. Other spectral
clustering algorithms give similar results.

where pL,ε denotes uniform density in a rectangular region Ω = {(x1 , x2 ) | 0 < x1 < L, −ε <
x2 < 0} of length L and width ε, and pG denotes a Gaussian density centered at (µ1 , µ2 ) with
diagonal covariance matrix ρ2 I . In ﬁg. 1(a) a plot of n = 1400 points from this density is shown
with L = 8, ε = 0.05 (cid:28) L, (µ1 , µ2 ) = (2, 0.2) and ρ = 0.1. Clearly, the two clusters are the
Gaussian ball and the rectangular strip Ω.
However, as shown in ﬁg. 1(b), clustering based on the second eigenvector of the normalized graph
Laplacian with weights Wi,j given by (1) partitions the points somewhere along the long strip in-
stead of between the strip and the Gaussian ball. We now show that this result is not due to the
approximation of the NP-hard problem but rather a feature of the original functional (2). Intuitively,
the failure of the normalized cut criterion is clear. Since the overlap between the Gaussian ball and
the rectangular strip is larger than the width of the strip, a cut that separates them has a higher penalty
than a cut somewhere along the thin strip.

Wi,j =

dxdx0 =

1
2πL2

dxdy

(4)

C ut(Ω1 ) = lim
n→∞

p(x)p(y)e−kx−yk2 /2σ2

To show this mathematically, we consider the penalty of the cut due to the numerator in (2) in the
limit of a large number of points n → ∞. In this population setting, as n → ∞ each point has an
inﬁnite number of neighbors, so we can consider the limit σ → 0. Upon normalizing the similarity
measure (1) by 1/2πσ2 , the numerator is given by
1
1
2πσ2 ZΩ1 ZΩ2
|V | Xx∈Ω1 Xy∈Ω2
where Ω1 , Ω2 ⊂ R2 are the regions of the two clusters. For ε (cid:28) L, a vertical cut of the strip at
location x = x1 far away from the ball (|x1 − x0 | (cid:29) ρ) gives
0 Z 0
σ→0 Z ∞
1
1
2πσ2 e−(x−x0 )2 /2σ2
C ut(x > x1 ) ' lim
L2
−∞
A similar calculation shows that for a horizontal cut at y = 0,
e−µ2
2 /2ρ2
1
√8πρ
C ut(y > 0) '
L
Finally, note that for a vertical cut far from the rectangle boundary ∂Ω, the denominators of the
two cuts in eq. (2) have the same order of magnitude. Therefore, if L (cid:29) ρ and µ2/ρ = O(1) the
horizontal cut between the ball and the strip has larger normalized penalty than a vertical cut of the
strip. This analysis explains the numerical results in ﬁg. 1 (b). Other spectral clustering algorithms
that use two eigenvectors, including those that take a local scale into account, also fail to separate
the ball from the strip and yield similar results to ﬁg.1(b). A possible solution to this problem is to
introduce multiscale anisotropic features that capture the geometry and dimensionality of the data
in the similarity metric. In the context of image and texture segmentation, the need for multiscale
features is well known [17, 18, 19]. Our example highlights its importance in general data clustering.

(5)

(6)

3 Additional Limitations of Spectral Clustering Methods

An additional problem with recursive bi-partitioning is the need of a saliency criterion when required
to return k > 2 clusters. Consider, for example a dataset which contains k = 3 clusters. After

(7)

the ﬁrst cut, the recursive algorithm should decide which su bgraph to further partition and which
to leave intact. A common approach that avoids this decision problem is to directly ﬁnd three
clusters by using the ﬁrst three eigenvectors of W v = λDv . Speci ﬁcally, denote by {λj , v j } the
set of eigenvectors of W v = λDv with eigenvalues sorted in decreasing order, and denote by
v j (xi ) the i-th entry (corresponding to the point xi ) in the j -th eigenvector v j . Many algorithms
propose to map each point xi ∈ Rp into Ψ(xi ) = (v1 (xi ), . . . , vk (xi )) ∈ Rk , and apply simple
clustering algorithms to the points Ψ(xi ) [8, 9, 12]. Some works [6, 10] use the eigenvectors ˜v j of
D−1/2W D−1/2 instead, related to the ones above via ˜v j = D1/2
v j .
We now show that spectral clustering that uses the ﬁrst k eigenvectors for ﬁnding k clusters also
suffers from fundamental limitations. Our starting point is the observation that vj are also eigen-
vectors of the Markov matrix M = D−1W [13, 12]. Assuming the graph is connected, the largest
eigenvalue is λ1 = 1 with |λj | < 1 for j > 1. Therefore, regardless of the initial condition the
random walk converges to the unique equilibrium distribution πs , given by πs (i) = Di,i / Pj Dj,j .
Moreover, as shown in [13], the Euclidean distance between points mapped to these eigenvectors is
equal to a so called ’diffusion distance’ between points on the graph,
j (v j (x) − v j (y))2 = kp(z , t | x) − p(z , t | y )k2
Xj
λ2t
L2 (1/πs )
where p(z , t | x) is the probability distribution of a random walk at time t given that it started at
x, πs is the equilibrium distribution, and k · kL2 (w) is the weighted L2 norm with weight w(z ).
Therefore, the eigenvalues and eigenvectors {λj , v j } for j > 1, capture the characteristic relaxation
times and processes of the random walk on the graph towards equilibrium. Since most methods use
the ﬁrst few eigenvector coordinates for clustering, it is i nstructive to study the properties of these
relaxation times and of the corresponding eigenvectors.
We perform this analysis under the following statistical model: we assume that the points {xi} are
random samples from a smooth density p(x) in a smooth domain Ω ⊂ Rp . We write the density in
Boltzmann form p(x) = e−U (x)/2 and denote U (x) as the potential. As described in [13], in the
limit n → ∞, σ → 0, the random walk with transition matrix M on the graph of points sampled
from this density converges to a stochastic differential equation (SDE)
˙x(t) = −∇U (x) + √2 ˙w(t)
where w(t) is standard white noise (Brownian motion), and the right eigenvectors of the matrix M
converge to the eigenfunctions of the following Fokker-Planck operator
(9)
Lψ(x) ≡ ∆ψ − ∇ψ · ∇U = −µψ(x)
deﬁned for x ∈ Ω with reﬂecting boundary conditions on ∂Ω. This operator is non-positive and its
eigenvalues are µ1 = 0 < µ2 ≤ µ3 ≤ . . .. The eigenvalues −µj of L and the eigenvalues λj of M
are related by µj = limn→∞,σ→0 (1 − λj )/σ . Therefore the top eigenvalues of M correspond to the
smallest of L. Eq. (7) shows that these eigenfunctions and eigenvalues capture the leading charac-
teristic relaxation processes and time scales of the SDE (8). These have been studied extensively in
the literature [20], and can give insight into the success and limitations of spectral clustering [13].
For example, if Ω = Rp and the density p(x) consists of k highly separated Gaussian clusters of
roughly equal size (k clusters), then there are exactly k eigenvalues very close or equal to zero, and
their corresponding eigenfunctions are approximately piecewise constant in each of these clusters.
Therefore, in this setting spectral clustering with k eigenvectors works very well.
To understand the limitations of spectral clustering, we now explicitly analyze situations with clus-
ters at different scales of size and density. For example, consider a density with three isotropic
Gaussian clusters: one large cloud (cluster #1) and two smaller clouds (clusters 2 and 3). These cor-
respond to one wide well and two narrow wells in the potential U (x). A representative 2-D dataset
drawn from such a density is shown in ﬁg. 2 (top left).

(8)

The SDE (8) with this potential has a few characteristic time scales which determine the structure of
its leading eigenfunctions. The slowest one is the mean passage time between cluster 1 and clusters
2 or 3, approximately given by [20]

τ1,2 =

2π
p|U 00
minU 00
max |

e(U (xmax )−U (xmin ))

(10)

where xmin is the bottom of the deepest well, xmax is the saddle point of U (x), and U 00
min , U 00
max
are the second derivatives at these points. Eq. (10), also known as Arrhenius or Kramers formula
of chemical reaction theory, shows that the mean ﬁrst passag e time is exponential in the barrier
height [20]. The corresponding eigenfunction ψ2 is approximately piecewise constant inside the
large well and inside the two smaller wells with a sharp transition near the saddle point xmax . This
eigenfunction easily separates cluster 1 from clusters 2 and 3 (see top center panel in ﬁg. 2).
A second characteristic time is τ2,3 , the mean ﬁrst passage time between clusters 2 and 3, also giv en
by a formula similar to (10). If the potential barrier between these two wells is much smaller than
between wells 1 and 2, then τ2,3 (cid:28) τ1,2 . A third characteristic time is the equilibration time inside
cluster 1. To compute it we consider a diffusion process only inside cluster 1, e.g. with an isotropic
1 kx−x1 k2 /2, where x1 is the bottom of the well.
parabolic potential of the form U (x) = U (x1 )+U 00
In 1-D the eigenvalues and eigenfunctions are given by µk = (k − 1)U 00
1 , with ψk (x) a polynomial
of degree k − 1. The corresponding intra-well relaxation times are given by τ R
k = 1/µk+1 (k ≥ 1).
The key point in our analysis is that if the equilibration time inside the wide well is slower than
the mean ﬁrst passage time between the two smaller wells, τ R
1 > τ2,3 , then the third eigenfunction
of L captures the relaxation process inside the large well and is approximately constant inside the
two smaller wells. This eigenfunction cannot separate between clusters 2 and 3. Moreover, if
1 /2 is still larger than τ2,3 then even the next leading eigenfunction captures the equilibration
τ R
2 = τ R
process inside the wide well, see a plot of ψ3 , ψ4 in ﬁg. 2 (rows 1,2). Therefore, even this next
eigenfunction is not useful for separating the two small clusters. In the example of ﬁg. 2, only ψ5
separates these two clusters.

This analysis shows that when confronted with clusters of different scales, corresponding to a mul-
tiscale landscape potential, standard spectral clustering which uses the ﬁrst k eigenvectors to ﬁnd k
clusters will fail. We present explicit examples in Section 4 below. The fact that spectral clustering
with a single scale σ may fail to correctly cluster multiscale data was already noted in [10, 16]. To
overcome this failure, [10] proposed replacing the uniform σ2 in eq. (1) with σ(xi )σ(xj ) where
σ(x) is proportional to the local density at x. Our analysis can also provide a probabilistic interpre-
tation to their method. In a nutshell, the effect of this scaling is to speed up the diffusion process at
regions of low density, thus changing some of its characteristic times. If the larger cluster has low
density, as in the examples in their paper, this approach is successful as it decreases τ R
1 . However, if
the large cluster has a high density (comparable to the density of the small clusters), this approach is
not able to overcome the limitations of spectral clustering, see ﬁg. 3. Moreover, this approach may
also fail in the case of uniform density clusters deﬁned sole ly by geometry (see ﬁg. 4).

4 Examples

We illustrate the theoretical analysis of Section 3 with three examples, all in 2-D. In the ﬁrst two ex-
amples, the n points {xi} ⊂ R2 are random samples from the following mixture of three Gaussians
α1N (x1 , σ2
1 I ) + α2N (x2 , σ2
2 I ) + α3N (x3 , σ2
(11)
3 I )
with centers xi isotropic standard deviations σi and weights αi (Pi αi = 1). Speci ﬁcally, we
consider one large cluster with σ1 = 2 centered at x1 = (−6, 0), and two smaller clusters with
σ2 = σ3 = 0.5 centered at x2 = (0, 0) and x3 = (2, 0). We present the results of both the NJW
algorithm [6] and the ZP algorithm [10] for two different weight vectors.
Example I: Weights (α1 , α2 , α3 ) = (1/3, 1/3, 1/3). In the top left panel of ﬁg. 2, n = 1000
random points from this density clearly show the difference in scales between the large cluster and
the smaller ones. The ﬁrst few eigenvectors of M with a uniform σ = 1 are shown in the ﬁrst
two rows of the ﬁgure. The second eigenvector ψ2 is indeed approximately piecewise constant and
easily separates the larger cluster from the smaller ones. However, ψ3 and ψ4 are constant on the
smaller clusters, capturing the relaxation process in the larger cluster (ψ3 captures relaxation along
the y -direction, hence it is not a function of the x-coordinate). In this example, only ψ5 can separate
the two small clusters. Therefore, as predicted theoretically, the NJW algorithm [6] fails to produce
reasonable clusterings for all values of σ . In this example, the density of the large cluster is low, and
therefore as expected and shown in the last row of ﬁg. 2, the ZP algorithm clusters correctly.
Example II: Weights (α1 , α2 , α3 ) = (0.8, 0.1, 0.1). In this case the density of the large cluster is
high, and comparable to that of the small clusters. Indeed, as seen in ﬁg. 3 and predicted theoretically

Original Data

ψ
2

ψ
3

−10

−5

0

x
NJW clustering,  σ= 1

8
6
4
2
0
−10

20

10

0

−10

−5

0

−10

−10

ZP clustering, kNN = 7

−5

0

x
ZP −  ψ
2

8
6
4
2
0
−2

10

0

−10

−5

0

−10

−5

0

x
ψ
4

x
ψ
5

8
6
4
2
0

8
6
4
2
0
−2

−10

−5

0

x
ZP −  ψ
3

y

4
2
0
−2
−4

5

0

−5

5

0

−5

−10

−5

0

−10

−5
x

0

−10

−5
x

0

Figure 2: A three cluster dataset corresponding to example I (top left), clustering results of NJW
and ZP algorithms [6, 10] (center and bottom left, respectively), and various eigenvectors of M vs.
the x coordinate (blue dots in 2nd and 3rd columns). The red dotted line is the potential U (x, 0).

Original Data

ZP results kNN = 7

ZP −  ψ
2

ZP −  ψ
3

y

5

0

−5

5

0

−5

10

5

0

−10

−5

0

5

−10

−5

0

5

−10

x

0

−5
x

10

5

0

−5

−10

−5
x

0

Figure 3: Dataset corresponding to example II and result of ZP algorithm.

the ZP algorithm fails to correctly cluster this data for all values of the parameter kN N in their
algorithm. Needless to say, the NJW algorithm also fails to correctly cluster this example.
Example III: Consider data {xi } uniformly sampled from a domain Ω ⊂ R2 , which consists of
three clusters, one a large rectangular container and two smaller disks, all connected by long and
narrow tubes (see ﬁg. 4 (left)). In this example the containe r is so large that the relaxation time inside
it is slower than the characteristic time to diffuse between the small disks, hence NJW algorithm fails
to cluster correctly. Since density is uniform, the ZP algorithm fails as well, ﬁg. 4 (right).

Note that spectral clustering with the eigenvectors of the standard graph Laplacian has similar limi-
tations, since the Euclidean distance between these eigenvectors is equal to the mean commute time
on the graph [11]. Therefore, these methods may also fail when confronted with multiscale data.

5 Clustering with a Relaxation Time Coherence Measure

The analysis and examples of Sections 3 and 4 may suggest the use of more than k eigenvectors
in spectral clustering. However, clustering with k-means using 5 eigenvectors on the examples
of Section 4 produced unsatisfactory results (not shown). Moreover, since the eigenvectors of the
matrix M are orthonormal under a speci ﬁc weight function, they becom e increasingly oscillatory.
Therefore, it is quite difﬁcult to use them to detect a small c luster, much in analogy to Fourier
analysis, where it is difﬁcult to detect a localized bump in a function from its Fourier coefﬁcients.

Original Data

ZP k
 = 7
NN

10

5

0

−5

−10

10

5

0

−5

−10

−10

−5

0

5

10

−10

−5

0

5

10

Figure 4: Three clusters deﬁned solely by geometry, and resu lt of ZP clustering (Example III).

Original Image

Coherence Measure

Ncut with 4 clusters

(a)

(b)

(c)

Figure 5: Normalized cut and coherence measure segmentation on a synthetic image.

Based on our analysis, we propose a different approach to graph-based clustering. Given the impor-
tance of relaxation times on the graph as indication of clusters, we propose a novel and principled
measure for the coherence of a set of points as belonging to a single cluster. Our coherence mea-
sure can be used in conjunction with any clustering algorithm. Speci ﬁcally, let G = (V , W ) be
a weighted graph of points and let V = S ∪ (V \ S ) be a possible partition (computed by some
clustering algorithm). Our aim is to construct a meaningful measure to decide whether to accept
or reject this partition. To this end, let λ2 denote the second largest eigenvalue of the Markov
matrix M corresponding to the full graph G. We deﬁne
τV = 1/(1 − λ2 ) as the characteristic
relaxation time of this graph. Similarly, τ1 and τ2 denote the characteristic relaxation times of the
two subgraphs corresponding to the partitions S and V \ S . If V is a single coherent cluster, then
we expect τV = O(τ1 + τ2 ). If, however, V consists of two weakly connected clusters deﬁned
by S and V \ S , then τ1 and τ2 measure the characteristic relaxation times inside these two clus-
ters while τV measures the overall relaxation time. If the two sub-clusters are of comparable size,
then τV (cid:29) (τ1 + τ2 ). If however, one of them is much smaller than the other, then we expect
max(τ1 , τ2 )/ min(τ1 , τ2 ) (cid:29) 1. Thus, we deﬁne a set V as coherent if either τV < c1 (τ1 + τ2 ) or if
max(τ1 , τ2 )/ min(τ1 , τ2 ) < c2 . In this case, V is not partitioned further. Otherwise, the subgraphs
S and V \ S need to be further partitioned and similarly checked for their coherence. While a the-
oretical analysis is beyond the scope of this paper, reasonable numbers that worked in practice are
c1 = 1.8 and c2 = 10. We note that other works have also considered relaxation times for clustering
with different approaches [21, 22].

We now present use of this coherence measure with normalized cut clustering on the third example
of Section 4. The ﬁrst partition of normalized cut on this dat a with σ = 1 separates between the large
container and the two smaller disks. The relaxation times of the full graph and the two subgraphs
are (τV , τ1 , τ2 ) = (1350, 294, 360). These numbers indicate that the full dataset is not coherent, and
indeed should be partitioned. Next, we try to partition the large container. Normalized cuts partitions
the container roughly into two parts with (τV , τ1 , τ2 ) = (294, 130, 135), which according to our
coherence measure means that the big container is a single structure that should not be split. Finally,
normalized cut on the two small disks correctly separates them giving (τV , τ1 , τ2 ) = (360, 18, 28),
which indicates that indeed the two disks should be split. Further analysis of each of the single disks
by our measure shows that each is a coherent cluster. Thus, combination of our coherence measure
with normalized cut not only clusters correctly, but also automatically ﬁnds the correct number of
clusters, regardless of cluster scale. Similar results are obtained for the other examples in this paper.

Finally, our analysis also applies to image segmentation. In ﬁg. 5(a) a synthetic image is shown.
The segmentation results of normalized cuts [24] and of the coherence measure combined with
[23] appear in panels (b) and (c). Results on a real image are shown in ﬁg. 6. Each segments is

Original Image

Coherence Measure

Ncut 6 clusters

Ncut 20 clusters

Figure 6: Normalized cut and coherence measure segmentation on a real image.

represented by a different color. With a small number of clusters normalized cut cannot ﬁnd the
small coherent segments in the image, whereas with a large number of clusters, large objects are
segmented. Implementing our coherence measure with [23] ﬁn ds salient clusters at different scales.

Acknowlegments: The research of BN was supported by the Israel Science Foundation (grant 432/06), by the
Hana and Julius Rosen fund and by the William Z. and Eda Bess Novick Young Scientist fund.

References

[1] J. Shi and J. Malik. Normalized cuts and image segmentation, PAMI, Vol. 22, 2000.
[2] R. Kannan, S. Vempala, A. Vetta, On clusterings: good, bad and spectral, J. ACM, 51(3):497-515, 2004.
[3] D. Cheng, R. Kannan, S. Vempala, G. Wang, A divide and merge methodology for clustering, ACM
SIGMOD/PODS, 2005.
[4] F.R.K. Chung, Spectral Graph Theory, Regional Conference Series in Mathematics Vol. 92, 1997.
[5] Y. Weiss, Segmentation using eigenvectors: a unifying view, ICCV 1999.
[6] A.Y. Ng, M.I. Jordan, Y. Weiss, On Spectral Clustering: Analysis and an algorithm, NIPS Vol. 14, 2002.
[7] N. Cristianini, J. Shawe-Taylor, J. Kandola, Spectral kernel methods for clustering, NIPS, Vol. 14, 2002.
[8] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering,
NIPS Vol. 14, 2002.
[9] S. Yu and J. Shi. Multiclass spectral clustering. ICCV 2003.
[10] L. Zelnik-Manor, P. Perona, Self-Tuning spectral clustering, NIPS, 2004.
[11] M. Saerens, F. Fouss, L. Yen and P. Dupont, The principal component analysis of a graph and its rela-
tionships to spectral clustering. ECML 2004.
[12] M. Meila, J. Shi. A random walks view of spectral segmentation, AI and Statistics, 2001.
[13] B. Nadler, S. Lafon, R.R. Coifman, I.G. Kevrekidis, Diffusion maps spectral clustering and eigenfunc-
tions of Fokker-Planck operators, NIPS, 2005.
[14] S. Lafon, A.B. Lee, Diffusion maps and coarse graining: a uniﬁ ed framework for dimensionality reduc-
tion, graph partitioning, and data set parameterization, PAMI, 28(9):1393-1403, 2006.
[15] D. Harel and Y. Koren, On Clustering Using Random Walks, FST TCS, 2001.
[16] I. Fischer, J. Poland, Amplifying the block matrix structure for spectral clustering, Proceedings of the
14th Annual Machine Learning Conference of Belgium and the Netherlands, pp. 21-28, 2005.
[17] J. Malik, S. Belongie, T. Leung, J. Shi, Contour and texture analysis for image segmentation, Int. J.
Comp. Vis. 43(1):7-27, 2001.
[18] E. Sharon, A. Brandt, R. Basri, Segmentation and Boundary Detection Using Multiscale Intensity Mea-
surements, CVPR, 2001.
[19] M. Galun, E. Sharon, R. Basri and A. Brandt, Texture Segmentation by Multiscale Aggregation of Filter
Responses and Shape Elements, ICCV, 2003.
[20] C.W. Gardiner, Handbook of stochastic methods, third edition, Springer NY, 2004.
[21] N. Tishby, N. Slonim, Data clustering by Markovian relaxation and the information bottleneck method,
NIPS, 2000.
[22] C. Chennubhotla, A.J. Jepson, Half-lives of eigenﬂows for sp ectral clustering, NIPS, 2002.
[23] E. Sharon, A. Brandt, R. Basri, Fast multiscale image segmentation, ICCV, 2000.
[24] T. Cour, F. Benezit, J. Shi. Spectral Segmentation with Multiscale Graph Decomposition. CVPR, 2005.


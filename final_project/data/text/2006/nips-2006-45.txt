Optimal Single-Class Classi ﬁcation Strategies

Ran El-Yaniv
Department of Computer Science
Technion- Israel Institute of Technology
Technion, Israel 32000
rani@cs.technion.ac.il

Mordechai Nisenson
Department of Computer Science
Technion - Israel Institute of Technology
Technion, Israel 32000
motin@cs.technion.ac.il

Abstract

We consider single-class classiﬁcation (SCC) as a two-pers on game between the
learner and an adversary. In this game the target distribution is completely known
to the learner and the learner’s goal is to construct a classiﬁer capable of guar-
anteeing a given tolerance for the false-positive error while minimizing the false
negative error. We identify both “hard ” and “soft” optimal c
lassiﬁcation strategies
for different types of games and demonstrate that soft classiﬁcation can provide
a signiﬁcant advantage. Our optimal strategies and bounds p rovide worst-case
lower bounds for standard, ﬁnite-sample SCC and also motivate new approa ches
to solving SCC.

1 Introduction

In Single-Class Classiﬁcation (SCC)
the learner observes a training set of examples sampled from
one target class. The goal is to create a classiﬁer that can distinguish the ta rget class from other
classes, unknown to the learner during training. This problem is the essence of a great many appli-
cations such as intrusion, fault and novelty detection. SCC has been receiving much research atten-
tion in the machine learning and pattern recognition communities (for example, the survey papers
[7, 8, 4] cite, altogether, over 100 papers). The extensive body of work on SCC, which encompasses
mainly empirical studies of heuristic approaches, suffers from a lack of theoretical contributions and
few principled (empirical) comparative studies of the proposed solutions. Thus, despite the extent
of the existing literature, some of the very basic questions have remained unresolved.

Let P (x) be the underlying distribution of the target class, de ﬁned o ver some space Ω. We call P the
target distribution. Let 0 < δ < 1 be a given tolerance parameter. The learner observes a training
set sampled from P and should then construct a classiﬁer capable of distinguis hing the target class.
We view the SCC problem as a game between the learner and an adversary. The adversary selects
another distribution Q over Ω and then a new element of Ω is drawn from γP + (1 − γ )Q, where γ
is a switching parameter (unknown to the learner). The goal of the learner is to minimize the false
negative error, while guaranteeing that the false positive error will be at most δ .
The main consideration in previous SCC studies has been statistical: how can we guarantee a pre-
scribed false positive rate (δ ) given a ﬁnite sample from P ? This question led to many solutions,
almost all revolving around the idea of low-density rejection. The proposed approaches are typically
generative or discriminative. Generative solutions range from full density estimation [2], to partial
density estimation such as quantile estimation [5], level set estimation [1, 9] or local density esti-
mation [3]. In discriminative methods one attempts to generate a decision boundary appropriately
enclosing the high density regions of the training set [11].

In this paper we abstract away the statistical estimation component of the problem and model a
setting where the learner has a very large sample from the target class. In fact, we assume that the
learner knows the target distribution P precisely. While this assumption would render almost the

entire body of SCC literature super ﬂuous, it turns out that a signiﬁcant, decision-theoretic compo-
nent of the SCC problem remains – one that has so far been overl ooked. In any case, the results we
obtain here immediately apply to other SCC instances as lower bounds.

The fundamental question arising in our setting is: What are optimal strategies for the learner? In
particular, is the popular low-density rejection strategy optimal? While most or all SCC papers
adopted this strategy, nowhere in the literature could we ﬁn d a formal justiﬁcation.

The partially good news is that low-density rejection is worst-case optimal, but only if the learner is
con ﬁned to “hard ” decision strategies. In general, the wors
t-case optimal learner strategy should be
“soft”; that is, the learner should play a randomized strate gy, which could result in a very signiﬁcant
gain. We ﬁrst identify a monotonicity property of optimal SC C strategies and use it to establish
the optimality of low-density rejection in the “hard ” case. We then show an equivalence between
low-density rejection and a constrained two-class classiﬁ cation problem where the other class is the
uniform distribution over Ω. This equivalence motivates a new approach to solving SCC problems.
We next turn our attention to the power of the adversary, an issue that has been overlooked in the
literature but has crucial impact on the relevancy of SCC solutions in applications. For example,
when considering an intrusion detection application (see, e.g., [6]), it is necessary to assume that the
“attacking distribution ” has some worst-case characteris
tics and it is important to quantify precisely
what the adversary knows or can do. The simple observation in this setting is that an omniscient and
unlimited adversary, who knows all parameters of the game including the learner’s strategy, would
completely demolish the learner who uses hard strategies. By using a soft strategy, however, the
learner can achieve on average the biased coin false negative rate of 1 − δ .
We then analyze the case of an omniscient but limited adversary, who must select a sufﬁciently
distant Q satisfying DKL (Q||P ) ≥ Λ, for some known parameter Λ. One of our main contributions
is a complete analysis of this game, including identiﬁcatio n of the optimal strategy for the learner
and the adversary, as well as the best achievable false negative rate. The optimal learner strategy and
best achievable rate are obtained via a solution of a linear program speciﬁed in terms of the problem
parameters. These results are immediately applicable as lower bounds for standard ( ﬁnite-sample)
SCC problems, but may also be used to inspire new types of algorithms for standard SCC. While we
do not have a closed form expression for the best achievable false-negative rate, we provide a few
numerical examples demonstrating and comparing the optimal “hard ” and “soft” performance.

2 Problem Formulation

The single-class classiﬁcation (SCC) problem is de ﬁned as a game between the
learner and an
adversary. The learner receives a training sample of examples from a target distribution P de ﬁned
over some space Ω. On the basis of this training sample, the learner should select a rejection function
r : Ω → [0, 1], where for each ω ∈ Ω, rω = r(ω ) is the probability with which the learner will
reject ω . On the basis of any knowledge of P and/or r(·), the adversary selects selects an attacking
distribution Q, de ﬁned over Ω. Then, a new example is drawn from γP +(1−γ )Q, where 0 < γ < 1,
is a switching probability unknown to the learner. The rejection rate of the learner, using a rejection
△
function r, with respect to any distribution D (over Ω), is ρ(D) = ρ(r, D)
= ED {r(ω )}. For
notational convenience whenever we decorate r (e.g., r′ ,r∗ ), the corresponding ρ will be decorated
accordingly (e.g., ρ′ ,ρ∗ ). The two main quantities of interest here are the false positive rate (type I
error) ρ(P ), and the false negative rate (type II error) 1 − ρ(Q).
Before the start of the game, the learner receives a tolerance parameter 0 < δ < 1, giving the
maximally allowed false positive rate. A rejection function r(·) is valid if its false positive rate
ρ(P ) ≤ δ . A valid rejection function (strategy) is optimal if it guarantees the smallest false negative
rate amongst all valid strategies.

We consider a model where the learner knows the target distribution P exactly, thus focusing on the
decision-theoretic component in SCC. Clearly, our model approximates a setting where the learner
has a very large training set, but the results we obtain immediately apply, in any case, as lower
bounds to other SCC instances.

This SCC game is a two-person zero sum game where the payoff to the learner is ρ(Q). The set
△
= {r : ρ(P ) ≤ δ} of valid rejection functions is the learner’s strategy space. Let Q be the
Rδ (P )

strategy space of the adversary, consisting of all allowable distributions Q that can be selected by
the adversary. We are concerned with optimal learner strategies for game variants distinguished by
the adversary’s knowledge of the learner’s strategy, P and/or of δ and by other limitations on Q.
We distinguish a special type of this game, which we call the hard setting, where the learner must
deterministically reject or accept new events; that is, r : Ω → {0, 1}, and such rejection functions
are termed “hard.” The more general game de ﬁned above (with “
soft” functions) is called the soft
setting. As far as we know, only the hard setting has been considered in the SCC literature thus far.

In the soft setting, given any rejection function, the learner can reduce the type II error by rejecting
more (i.e., by increasing r(·)). Therefore, for an optimal r(·) we have ρ(P ) = δ (rather than
ρ(P ) ≤ δ ). It follows that the switching parameter γ is immaterial to the selection of an optimal
strategy. Speciﬁcally, the combined error of an optimal str ategy is γ ρ(P ) + (1 − γ )(1 − ρ(Q)) =
γ δ + (1 − γ )(1 − ρ(Q)), which is minimized by minimizing the type II error, 1 − ρ(Q).

△
We assume throughout this paper a ﬁnite support of size N ; that is, Ω = {1, . . . , N } and P
=
△
= {q1 , . . . , qN } are probability mass functions. Additionally, a “probabil ity
{p1 , . . . , pN } and Q
distribution ” refers to a distribution over the ﬁxed suppor
t set Ω. Note that this assumption still
leaves us with an in ﬁnite game because the learner’s pure str ategy space, Rδ (P ), is in ﬁnite. 1

3 Characterizing Monotone Rejection Functions

In this section we characterize the structure of optimal learner strategies. Intuitively, it seems plau-
sible that the learner should not assign higher rejection values to higher probability events under P .
That is, one may expect that a reasonable rejection function r(·) would be monotonically decreasing
with probability values (i.e., if pj ≤ pk then rj ≥ rk ). Such monotonicity is a key justiﬁcation for
a very large body of SCC work, which is based on low density rejection strategies. Surprisingly,
optimal monotone strategies are not always guaranteed as shown in the following example.

Example 3.1 (Non-Monotone Optimality) In the hard setting,
take N = 3, P =
(0.06, 0.09, 0.85) and δ = 0.1. The two δ -valid hard rejection functions are r′ = (1, 0, 0) and
r′′ = (0, 1, 0). Let Q = {Q = (0.01, 0.02, 0.97)}. Clearly ρ′ (Q) = 0.01 and ρ′′ (Q) = 0.02
and therefore, r′′ (·) is optimal despite breaking monotonicity. More generally, this example holds if
Q = {Q : q2 − q1 ≥ ε} for any 0 < ε ≤ 1.
In the soft setting, let N = 2, P = (0.2, 0.8), and δ = 0.1. We note that Rδ (P ) = {rε =
(0.1 + 4ε, 0.1 − ε)}, for ε ∈ [−0.025, 0.1]. We take Q = {Q = (0.1, 0.9)}. Then ρǫ (Q) = 0.1 +
0.4ε − 0.9ε = 0.1 − 0.5ε. This is clearly maximized when we minimize ε by taking ε = −0.025, and
then the optimal rejection function is (0, 0.125), which clearly breaks monotonicity. This example
also holds for Q = {Q : q2 ≥ cq1 } for any c > 4.

Fix P and δ . For any adversary strategy space, Q, let R∗
δ (P ) be the set of optimal valid rejection
= {r ∈ Rδ (P ) : minQ∈Q ρ(Q) = maxr ′∈Rδ (P ) minQ∈Q ρ′ (Q)}.2 We note that
△
functions, R∗
δ
δ is never empty in the cases we consider. A simple observation is that for any r ∈ R∗
δ there
R∗
exists r′ ∈ R∗
δ such that r′ (i) = r(i) for all i such that pi > 0 and for zero probabilities, pj = 0,
r′ (j ) = 1.
The following property ensures that R∗
δ will include a monotone (optimal) hard strategy, which
means that the search space for the learner can be conveniently con ﬁned to monotone strategies.
While the set of all distributions satisﬁes this property, l ater on we will consider limited strategic
adversary spaces where this property still holds.3

1The game is conveniently described in extensive form (i.e., game tree) where in the ﬁrst move the learner
selects a rejection function, followed by a chance move to determine the source (either P or Q) of the test
example (with probability γ ). In the case where Q is selected, the adversary chooses (randomly using Q) the
test example. In this game the choice of Q depends on knowledge of P and r(·).
2For certain strategy spaces, Q, it may be necessary to consider the inﬁmum rather than the mi nimum. In
such cases it may be necessary to replace ‘Q ∈ Q’ (in deﬁnitions, theorems, etc.) with ‘ Q ∈ cl(Q)’, where
cl(Q) is the closure of Q.
3All properties deﬁned in this paper could be made weaker for t he purposes of the proofs, but this would
needlessly complicate them. Indeed, the way they are currently deﬁned is sufﬁcient for most “reasonable”
Q.

De ﬁnition 3.2 (Property A) Let P be a distribution. A set of distributions Q has Property A w.r.t.
P if for all j, k and Q ∈ Q such that pj < pk and qj < qk , there exists Q′ ∈ Q such that q ′
k ≤ qj ,
q ′
j ≥ qk and for all i 6= j, k , we have q ′
i = qi .

Theorem 3.3 (Monotone Hard Decisions) When the learner is restricted to hard-decisions and Q
δ such that pj < pk ⇒ r(j ) ≥ r(k).4
satisﬁes Property A w.r.t. P , then ∃r ∈ R∗
Proof: Let us assume by contradiction that no such rejection function exists in R∗
δ . Let r ∈ R∗
δ .
Let j be such that pj = minω :r(ω)=0 pω . Then, there must exist k , such that pj < pk and r(k) = 1
(otherwise r is monotone). De ﬁne r∗ to be r with the values of j and k swapped; that is, r∗ (j ) =
1, r∗ (k) = 0 and for all other i, r∗ (i) = r(i). We note that ρ∗ (P ) = ρ(P ) + pj − pk < ρ(P ) ≤
k . Thus, if q∗
δ . Let Q∗ ∈ Q be such that minQ ρ∗ (Q) = ρ∗ (Q∗ ) = ρ(Q∗ ) + q∗
k ,
j ≥ q∗
j − q∗
ρ∗ (Q∗ ) ≥ ρ(Q∗ ). Otherwise, there exists Q∗ ′ as in Property A and in particular, q∗ ′
j . As a
k ≤ q∗
result, ρ∗ (Q∗ ) = ρ(Q∗ ′ ) + q∗
j − q∗ ′
k ≥ ρ(Q∗ ′ ). Therefore, there always exists Q ∈ Q such that
ρ∗ (Q∗ ) ≥ ρ(Q) (either Q = Q∗ or Q = Q∗ ′ ). Consequently, minQ ρ∗ (Q) ≥ minQ ρ(Q), and thus,
δ . As long as there are more j, k pairs which need to have their rejection levels ﬁxed, we
r∗ ∈ R∗
label r = r∗ and repeat the above procedure. Since the only changes are made to r∗ (j ) and r∗ (k),
and since j is the non-rejected event with minimal probability, the procedure will be repeated at
most N times. The ﬁnal r∗ is in R∗
δ and satisﬁes pj < pk ⇒ r(j ) ≥ r(k). Contradiction.
(cid:3)

Theorem 3.3 provides a formal justiﬁcation for the low-density rejection strategy (LDRS), popular
in the SCC literature. Speciﬁcally, assume w.l.o.g. p1 ≤ p2 ≤ · · · ≤ pN . The corresponding δ -valid
low density rejection function places rj = 1 iff Pj
i=1 pi ≤ δ .
Our discussion on soft decisions is facilitated by Property B and Theorem 3.5 that follow.
De ﬁnition 3.4 (Property B) Let P be a distribution. A set of distributions Q has Property B w.r.t.
P if for all j, k and Q ∈ Q such that 0 < pj ≤ pk and qj
< qk
, there exists Q′ ∈ Q such that
pj
pk
q′
≥ q′
j
k
pj
pk

and for all i 6= j, k , q ′
i = qi .

The rather technical proof of the following theorem is omitted for lack of space (and appears in the
adjoining, supplementary appendix).

Theorem 3.5 (Monotone Soft Decisions) If Q satisﬁes Property B w.r.t. P , then ∃r ∈ R∗
δ such
that: (i)pi = 0 ⇒ r(i) = 1; (ii) pj < pk ⇒ r(j ) ≥ r(k); and (iii) pj = pk ⇒ r(j ) = r(k).

4 Low-Density Rejection and Two-Class Classi ﬁcation

In this section we focus on the hard setting. We show that the low-density rejection strategy (LDRS
- de ﬁned in Section 3) is optimal. Moreover we show that the op timal hard performance can be ob-
tained by solving a constrained two-class classiﬁcation pr oblem where the other class is the uniform
distribution over Ω. The results here consider families Q that satisfy the following property.

De ﬁnition 4.1 (Property C) Let P be a distribution. A set of distributions Q has Property C w.r.t.
P if for all j, k and Q ∈ Q such that pj = pk there exists Q′ ∈ Q such that q ′
k = qj , q ′
j = qk and
for all i 6= j, k , q ′
i = qi .

We state without proof the following lemma (the proof can be found in the appendix).

Lemma 4.2 Let r∗ be a δ -valid low-density rejection function (LDRS). Let r be any monotone δ -
valid rejection function. Then minQ∈Q ρ∗ (Q) ≥ minQ∈Q ρ(Q) for any Q satisfying Property C.

Example 4.3 (Violation of Property C) We illustrate here that violating Property C may result in
a violation of Lemma 4.2. Let N = 5, P = (0.02, 0.03, 0.05, 0.05, 0.85), and δ = 0.1. Then the
two δ -valid LDRS rejection functions are r = (1, 1, 1, 0, 0) and r′ = (1, 1, 0, 1, 0). Let Q = {Q :
q3 − q4 > ε} for some 0 < ε < 1. Then, for any Q ∈ Q, ρ(Q) − ρ′ (Q) = q3 − q4 > ε, and
therefore, for the LDRS, r′ , there exists a monotone r such that minQ∈Q ρ′ (Q) < minQ∈Q ρ(Q).

4Here we must consider a weaker notion of monotonicity for hard strategies to be both valid and optimal.

When Q satisﬁes Property A, then by Theorem 3.3 there exists a monot one optimal rejection func-
tion. Therefore, the following corollary of Lemma 4.2 establishes the optimality of any LDRS.

Corollary 4.4 Any δ -valid LDRS is optimal if Q satisﬁes both Property A and Property C.

Thus, any LDRS strategy is indeed worst-case optimal when the learner is willing to be con ﬁned
to hard rejection functions and when the adversary’s space satisﬁes Property A and Property C. We
now show that an (optimal) LDRS solution is equivalent to an optimal solution of the following
constrained Bayesian two-class decision problem. Let the ﬁrst class c1 have distribution P (x) and
the second class, c2 , have the uniform distribution U (x) = 1/N . Let 0 < c < 1 and 0 < ǫ <
(N δc + 1 − c)/N δc. The classes have priors Pr{c1} = c and Pr{c2} = 1 − c. The loss function λij ,
giving the cost of deciding ci instead of cj (i, j = 1, 2), is λ11 = λ22 = 0, λ12 = (N c+1−c)/(1−c)
and λ21 = ǫ. The goal is to construct a classiﬁer C (x) ∈ {c1 , c2 ) that minimizes the total Bayesian
risk under the constraint that, for a given δ , Px:C (x)=c2 P (x) ≤ δ. We term this problem “the
Bayesian binary problem.”
Theorem 4.5 An optimal binary classiﬁer for the Bayesian binary problem induces an optimal
(hard) solution to the SCC problem (an LDRS) when Q satisﬁes properties A and C.

Proof Sketch: Let C ∗ (·) be an optimal classiﬁer for the Bayesian binary problem. Any classiﬁer
C (·) induces a hard rejection function r(·) by taking r(x) = 1 ⇔ C (x) = c2 . Therefore, the set of
△
feasible classiﬁers (satisfying the constraint) clearly i nduces Rδ (P ). Let Mi (C )
= {x : C (x) = i}.
Note that the constraint is equivalent to Px∈M2 (C ) P (x) ≤ δ . The Bayes risk for classifying x
△
as i is Ri (x)
= λii Pr{ci |x} + λi(3−i) Pr{c3−i |x} = λi(3−i) Pr{c3−i |x}. The total Bayes risk is
△
= Px∈M1 (C ) R1 (x) + Px∈M2 (C ) R2 (x), which is minimized at C ∗ (·). It is not difﬁcult to
R(C )
show that R1 (·) and R2 (·) are monotonically decreasing and increasing, respectively. It therefore
follows that x ∈ M1 (C ∗ ), y ∈ M2(C ∗ ) ⇒ P (x) ≥ P (y ) (otherwise, by swapping C ∗ (x) and
C ∗ (y ), the constraint can be maintained and R(C ∗ ) decreased). It is also not difﬁcult to show that
R1 (x) ≥ 1 > R2 (x) for any x. Thus, it follows that Py∈M2 (C ∗ ) P (y ) + minx∈M1 (C ∗ ) P (x) > δ
(otherwise, some x could be transferred from M1 (C ∗ ) to M2 (C ∗ ), reducing R(C ∗ )). Together,
these two properties immediately imply that C ∗ (·) induces a δ -valid LDRS.
(cid:3)
Theorem 4.5 motivates a different approach to SCC in which we sample from the uniform distribu-
tion over Ω and then attempt to approximate the optimal Bayes solution to the constrained binary
problem. It also justiﬁes certain heuristics found in the li terature [10, 11].

5 The Omniscient Adversary: Games, Strategies and Bounds

5.1 Unrestricted Adversary

In the ﬁrst game we analyze an adversary who is completely unr estricted. This means that Q is
the set of all distributions. Unsurprisingly, this game leaves little opportunity for the learner. For
△
△
= mini r(i) and Imin (r)
any rejection function r(·), de ﬁne rmin
= {i : r(i) = rmin }. For any
distribution D , ρ(D) = PN
i=1 dir(i) ≥ PN
i=1 di rmin = rmin , in particular, δ = ρ(P ) ≥ rmin
and minQ ρ(Q) ≥ rmin . By choosing Q such that qi = 1 for some i ∈ Imin (r), the adversary
can achieve ρ(Q) = rmin (the same rejection rate is achieved by taking any Q with qi = 0 for all
△
i 6∈ Imin (r)). In the soft setting, minQ ρ(Q) is maximized by the rejection function rδ (i)
= δ for
△
all pi > 0 (rδ (i)
= 1 for all pi = 0) This is equivalent to ﬂipping a
δ -biased coin for non-null
events (under P ). The best achievable Type II Error is 1 − δ . In the hard setting, clearly rmin = 0
(otherwise 1 > δ ≥ 1), and the best achievable Type II Error is precisely 1. That is, absolutely
nothing can be achieved.

This simple analysis shows the futility of the SCC game when the adversary is too powerful. In
order to consider SCC problems at all one must consider reasonable restrictions on the adversary
that lead to more useful games. One type of restriction would be to limit the adversary’s knowledge
of r(·), P and/or of δ . Another type would be to directly limit the strategic choices available to the
adversary. In the next section we focus on the latter type.

5.2 A Constrained Adversary

In seeking a quantiﬁable constraint on Q it is helpful to recall that the essence of the SCC problem is
to try to distinguish between two probability distributions (albeit one of them unknown). A natural
constraint is a lower bound on the “distance ” between these d istributions. Following similar results
in hypothesis testing, we would like to consider games in which the adversary must select Q such that
D(P ||Q) ≥ Λ, for some constant Λ > 0, where D(·||·) is the KL-divergence. Unfortunately, this
constraint is vacuous since D(P ||Q) explodes when qi ≪ pi (for any i). In this case the adversary
can optimally play the same strategy as in the unrestricted game while meeting the KL-divergence
constraint. Fortunately, by taking D(Q||P ) ≥ Λ, we can effectively constrain the adversary.
We note, as usual, that the learner can (and should) reject with probability 1 any null events under
P . Thus, an adversary would be foolish to choose a distribution Q that has any probability for
△
these events. Therefore, we henceforth assume w.l.o.g. that Ω = Ω(P )
= {ω : pω > 0}. Taking
= PN
△
△
i=1 qi log(qi /pi ), we then de ﬁne Q = QΛ
= {Q : D(Q||P ) ≥ Λ}. We note that QΛ
D(Q||P )
possesses properties A, B and C w.r.t. P ,5 and by Theorems 3.3 and 3.5 there exists a monotone
δ (in both the hard and soft settings) and by Corollary 4.4, any δ -valid LDRS is hard-optimal.
r ∈ R∗
If maxi pi ≤ 2−Λ , then any Q which is concentrated on a single event meets the constraint
D(Q||P ) ≥ Λ. Then, the adversary can play the same strategy as in the unrestricted game,
and the learner should select rδ as before. For the game to be non-trivial it is thus required that
Λ > log(1/ maxi pi ). Similarly, if the optimal r is such that there exists j ∈ Imin (r) (that is
r(j ) = rmin ) and pj ≤ 2−Λ , then a distribution Q that is completely concentrated on j has
D(Q||P ) ≥ Λ and achieves ρ(Q) = rmin as in the unrestricted game. Therefore, r = rδ , and
so maximizes rmin . We thus assume that the optimal r has no such j .
We begin our analysis of the game by identifying some useful characteristics of optimal adversary
strategies in Lemma 5.1. Then Theorem 5.2 shows that the effective support of an optimal Q has
a size of two at most. Based on these properties, we provide in Theorem 5.3 a linear program that
computes the optimal rejection function. The following lemma is stated without its (technical) proof.

Lemma 5.1 If Q minimizes ρ(Q) and meets the constraint D(Q||P ) ≥ Λ then: (i) D(Q||P ) = Λ;
(ii) pj < pk and qk > 0 ⇒ r(j ) > r(k); (iii) pj < pk and qj > 0 ⇒ qj log qj
+ qk log qk
>
pj
pk
(qj + qk ) log qj +qk
; (iv) pj < pk and qj > 0 ⇒ qj
> qk
; and (v) qj , qk > 0 ⇒ pj 6= pk .
pk
pk
pj

Theorem 5.2 Any optimal adversarial strategy Q has an effective support of size at most two.

Proof Sketch: Assume by contradiction that an optimal Q∗ has an effective support of size J ≥ 3.
W.l.o.g. we rename events such that the ﬁrst J events are the effective support of Q∗ (i.e., q∗
i > 0,
i = 1, . . . , J ). From part (i) of Lemma 5.1, Q∗ is a global minimizer of ρ(Q) subject to the
= Λ, qi > 0 (i = 1, . . . , J ) and PJ
constraints PJ
i=1 qi log qi
i=1 qi = 1. The Lagrangian of this
pi
problem is
− Λ! + λ2   J
r(i)qi + λ1   J
J
qi − 1! .
qi
Xi=1
Xi=1
Xi=1
L(Q, λ) =
qi log
pi
It is not hard to show, using parts (iv) and (v) of Lemma 5.1, that Q∗ is an extremum point of (1).
+ 1(cid:17) + λ2 = 0. Solving
= r(i) + λ1 (cid:16)log q∗
Taking the partial derivatives of (1) we have: ∂L(Q∗ ,λ)
i
∂ qi
pi
for λ1 , we get λ1 = (r(2) − r(1))/(log q∗
− log q∗
= ∂L(Q∗ ,λ)
∂L(Q∗ ,λ)
). If we assume (w.l.o.g.)
1
2
∂ q1
p1
p2
∂ q2
that p1 < p2 , then, from parts (ii) and (iv) of Lemma 5.1, r(2) < r(1) and q∗
2 /p2 . Thus
1 /p1 > q∗
λ1 < 0. Therefore, for all i, ∂ 2 L(Q,λ)
= λ1
< 0, and (1) is strictly concave. Therefore, since Q∗ is
∂ q2
qi
i
an extremum of the (strictly concave) Lagrangian function, it is the unique global maximum.

(1)

△
By part (iv) of Lemma 5.1, the smooth function fP,Λ (q1 , q2 , . . . , qJ −1 )
= D(Q||P ) − Λ has a root
at Q∗ where no partial derivative is zero. Therefore, it has an in ﬁ nite number of roots in any convex

5For any pair j, k such that pj ≤ pk , D(Q||P ) does not decrease by transferring all the probability from k
≤ (qj + qk ) log qj +qk
to j in Q: qj log qj
+ qk log qk
.
pj
pk
pj

domain where Q∗ is an internal point. Thus, there exists another distribution, ˜Q 6= Q∗ , where ˜qi > 0
for i = 1, . . . , J , which meets the equality criteria of the Lagrangian. Since Q∗ is the unique global
maximum of L(Q, λ): ρ( ˜Q) = L( ˜Q, λ) < L(Q∗ , λ) = ρ(Q∗ ). Contradiction.
(cid:3)

We now turn our attention to the learner’s selection of r(·). As already noted, it is sufﬁcient for the
learner to consider only monotone rejection functions. Since for these functions pj = pk ⇒ r(j ) =
r(k), the learner can partition Ω into K = K (P ) event subsets, which correspond, by probability,
to “level sets”, S1 , S2 , . . . , SK (all events in a level set S have probability PS ). We re-index these
subsets such that 0 < PS1 < PS2 < · · · < PSK . De ﬁne K variables r1 , r2 , . . . , rK , representing
the rejection rate assigned to each of the K level sets (∀ω ∈ Si , r(ω ) = ri ). We group our level sets
by probability: L = {S : PS < 2−Λ}, M = {S : PS = 2−Λ}, and H = {S : PS > 2−Λ}.
By Theorem 5.2, the optimal Q which the adversary selects will have an effective support of size
2 at most. If it has an effective support of size 1, then the event ω for which qω = 1 cannot be
from a level set in L or H (otherwise, part (i) of Lemma 5.1 would be violated). Therefore it must
belong to the single level set in M . Thus, if M = {Sm} (for some index m), then there are feasible
solutions Q such that qω = 1 (for ω ∈ Sm ), all of which have ρ(Q) = rm . If, on the other hand,
Q has an effective support of size 2, then it is not hard to show that one of the two events must
be from a level set Sl ∈ L, and the other, from a level set Sh ∈ H (since all other combinations
result in a violation of either part (i) or part (iii) of Lemma 5.1). Then, there is a single solution to
+ (1 − ql ) log 1−ql
ql log ql
= Λ, where ql and 1 − ql are the probabilities that Q assigns to the
PSl
PSh
events from Sl and Sh , respectively. For such a distribution, ρ(Q) = ql rl + (1 − ql )rh .
Therefore, the adversary’s choice of an optimal distribution, Q, must have one of |L||H | + |M | ≤
⌈ K 2
4 ⌉ (possibly different) rejection rates. Each of these rates, ρ1 , ρ2 , . . . , ρ|L||H |+|M | , is a linear
combination of at most two variables, ri and rj . We introduce an additional variable, z , to represent
the max-min rejection rate. We thus have:

Theorem 5.3 An optimal soft rejection function and the lower-bound on the Type II Error, 1 − z , is
obtained by solving the following linear program:6 maximizer1 ,r2 ,...,rK ,z z , subject to:

ri |Si |PSi = δ, 1 ≥ r1 ≥ r2 ≥ · · · ≥ rK ≥ 0, ρi ≥ z , i ∈ {1, 2, . . . , |L||H | + |M |}.

K
Xi=1
5.2.1 Numerical Examples

We now compare the performance of hard and soft rejection strategies for this constrained game
(D(Q||P ) ≥ Λ) for various values of Λ, and two different families of target distributions, P over
support N = 50. The families are arbitrary probability mass functions over N events and dis-
cretized Gaussians (over N bins). For each Λ we generated 50 random distributions P for each of
the families.7 For each such P we solved the optimal hard and soft strategies and computed the
corresponding worst-case optimal type II error, 1 − ρ(Q).
The results for δ = 0.05 are shown in Figure 5.2.1. Other results (not presented) for a wide variety
of the problem parameters (e.g., N , δ ) are qualitatively the same. It is evident that both the soft and
hard strategies are ineffective for small Λ. Clearly, the soft approach has signiﬁcantly lower error
than the hard approach (until Λ becomes “sufﬁciently large ”).

6Let r∗ be the solution to the linear program. Our derivation of the linear program is dependent on the
assumption that there is no event j ∈ Imin (r∗ ) such that pj ≤ 2−Λ (see discussion preceding Lemma 5.1). If
r∗ contradicts this assumption then, as discussed, the optimal strategy is rδ , which is optimal. It is not hard to
prove that in this case r∗ = rδ anyway, and thus the solution to the linear program is always optimal.
7Since maxQ D(Q||P ) = log(1/ mini pi ), it is necessary that mini pi ≤ 2−Λ when generating P (to
ensure that a Λ-distant Q exists). Distributions in the ﬁrst family of arbitrarily ra ndom distributions (a) are
generated by sampling a point (p1 ) uniformly in (0, 2−Λ ]. The other N − 1 points are drawn i.i.d. ∼ U (0, 1],
and then normalized so that their sum is 1 − p1 . The second family (b) are Gaussians centered at 0 and
discretized over N evenly spaced bins in the range [−10, 10]. A (discretized) random Gaussian N (0, σ ) is
selected by choosing σ uniformly in some range [σmin , σmax ]. σmin is set to the minimum σ ensuring that the
ﬁrst/last bin will not have “zero” probability (due to limit
ed precision). σmax was set so that the cumulative
probability in the ﬁrst/last bin will be 2−Λ , if possible (otherwise σmax is arbitrarily set to 10 ∗ σmin ).

1

0.8

0.6

0.4

0.2

r
o
r
r
E
 
I
I
 
e
p
y
T
 
e
s
a
C
 
t
s
r
o
W

 

Soft
Hard

1

0.9

0.8

0.7

0.6

0.5

0.4

r
o
r
r
E
 
I
I
 
e
p
y
T
 
e
s
a
C
 
t
s
r
o
W

 

Soft
Hard

0
 
0

2

4

6
8
Lambda (Λ)
(a) Arbitrary

10

12

 
0

2

10

12

4

6
8
Lambda (Λ)
(b) Gaussians

Figure 1: Type II Error vs. Λ, for N = 50 and δ = 0.05. 50 distributions were generated for each
value of Λ (Λ = 0.5, 0.1, · · · , 12.5). Error bars depict standard error of the mean (SEM).

6 Concluding Remarks

We have introduced a game-theoretic approach to the SCC problem. This approach lends itself well
to analysis, allowing us to prove under what conditions low-density rejection is hard-optimal and if
an optimal monotone rejection function is guaranteed to exist. Our analysis introduces soft decision
strategies, which allow for signiﬁcantly better performan ce. Observing the learner’s futility when
facing an omniscient and unlimited adversary, we considered restricted adversaries and provided
full analysis of an interesting family of constrained games. This work opens up many new avenues
for future research. We believe that our results could be useful for inspiring new algorithms for
ﬁnite-sample SCC problems. For example, the equivalence of
low-density rejection to the Bayesian
binary problem as shown in Section 3.3 obviously motivates a new approach. Clearly, the utilization
of randomized strategies should be carried over to the ﬁnite sample case as well. Our approach can
be extended and developed in several ways. A very interesting setting to consider is one in which the
adversary has partial knowledge of the problem parameters and the learner’s strategy. For example,
the adversary may only know that P is in some subspace. Additionally, it is desirable to extend
our analysis to in ﬁnite and continuous event spaces. Finall y, it would be very nice to determine an
explicit expression for the lower bound obtained by the linear program of Theorem 5.3.

References
[1] S. Ben-David and M. Lindenbaum. Learning distributions by their density-levels - a paradigm for learning
without a teacher. In EuroCOLT, pages 53–68, 1995.
[2] C.M. Bishop. Novelty detection and neural network validation. IEE Proceedings - Vision, Image, and
Signal Processing, 141(4):217–222, 1994.
[3] M.M. Breunig, H.P. Kriegel, R.T. Ng, and J. Sander. Lof: Identifying density-based local outliers. In
SIGMOD Conference, pages 93–104, 2000.
[4] V. Hodge and J. Austin. A survey of outlier detection methodologies. Arti ﬁcial Intelligence Review ,
22(2):85–126, 2004.
[5] G.R.G. Lanckriet, L. El Ghaoui, and M.I. Jordan. Robust novelty detection with single-class mpm. In
NIPS, pages 905–912, 2002.
[6] A. Lazarevic, L. Ert ¨oz, V. Kumar, A. Ozgur, and J. Srivastava. A comparative study of anomaly detection
schemes in network intrusion detection. In SDM, 2003.
[7] M. Markou and S. Singh. Novelty detection: a review – part 1: statistical approaches. Signal Processing,
83(12):2481–2497, 2003.
[8] M. Markou and S. Singh. Novelty detection: a review – part 2: neural network based approaches. Signal
Processing, 83(12):2499–2521, 2003.
[9] I. Steinwart, D. Hush, and C. Scovel. A classi ﬁcation fra mework for anomaly detection. Journal of
Machine Learning Research, 6, 2005.
[10] David M. J. Tax and Robert P. W. Duin. Uniform object generation for optimizing one-class classi ﬁers.
Journal of Machine Learning Research, 2:155–173, 2002.
[11] H. Yu. Single-class classi ﬁcation with mapping conver gence. Machine Learning, 61(1-3):49–69, 2005.


Logarithmic Online Regret Bounds for Undiscounted
Reinforcement Learning

Peter Auer
Ronald Ortner
University of Leoben, Franz-Josef-Strasse 18,
8700 Leoben, Austria
{auer,rortner}@unileoben.ac.at

Abstract

We present a learning algorithm for undiscounted reinforcement learning. Our
interest lies in bounds for the algorithm’s online performance after some ﬁnite
number of steps.
In the spirit of similar methods already successfully applied
for the exploration-exploitation tradeoff in multi-armed bandit problems, we use
upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic
online regret in the number of steps taken with respect to an optimal policy.

1 Introduction

1.1 Preliminaries

Deﬁnition 1. A Markov decision process (MDP) M on a ﬁnite set of
states S with a ﬁnite set
of actions A available in each state ∈ S consists of (i) an initial distribution µ0 over S, (ii) the
transition probabilities p(s, a, s0 ) that specify the probability of reaching state s0 when choosing
action a in state s, and (iii) the payoff distributions with mean r(s, a) and support in [0, 1] that
specify the random reward for choosing action a in state s.
A policy on an MDP M is a mapping π : S → A. We will mainly consider unichain MDPs, in
which under any policy any state can be reached (after a ﬁnite number of transitions) from any state.
ρ(M , π) := X
For a policy π let µπ be the stationary distribution induced by π on M .1 The average reward of π
then is deﬁned as
µπ (s)r(s, π(s)).
s∈S
A policy π∗ is called optimal on M , if ρ(M , π) ≤ ρ(M , π∗ ) =: ρ∗ (M ) =: ρ∗ for all policies π .
RT := PT −1
Our measure for the quality of a learning algorithm is the total regret after some ﬁnite number of
steps. When a learning algorithm A executes action at in state st at step t obtaining reward rt , then
t=0 rt − T ρ∗ denotes the total regret of A after T steps. The total regret Rε
T with respect
to an ε-optimal policy (i.e. a policy whose return differs from ρ∗ by at most ε) is de ﬁned accordingly.

(1)

1.2 Discussion

We would like to compare this approach with the various PAC-like bounds in the literature as given
for the E3 -algorithm of Kearns, Singh [1] and the R-Max algorithm of Brafman, Tennenholtz [2] (cf.
also [3]). Both take as inputs (among others) a con ﬁdence parameter δ and an accuracy parameter
Pn
1Every policy π induces a Markov chain Cπ on M . If Cπ is ergodic with transition matrix P , then there
exists a unique invariant and strictly positive distribution µπ , such that independent of µ0 one has µn =
µ0 ¯Pn → µπ , where ¯Pn = 1
j=1 P j . If Cπ is not ergodic, µπ will depend on µ0 .
n

ε. The algorithms then are shown to yield ε-optimal return after time polynomial in 1
δ , 1
ε (among
others) with probability 1 − δ . In contrast, our algorithm has no such input parameters and converges
to an optimal policy with expected logarithmic online regret in the number of steps taken.
Obviously, by using a decreasing sequence εt , online regret bounds for E3 and R-Max can be
achieved. However, it is not clear whether such a procedure can give logarithmic online regret
bounds. We rather conjecture that these bounds either will be not logarithmic in the total number of
steps (if εt decreases quickly) or that the dependency on the parameters of the MDP – in particular
on the distance between the reward of the best and a second best policy – won’t be polynomial (if εt
decreases slowly).

Moreover, although our UCRL algorithm shares the “optimism under uncertainty” maxim with R-
Max, our mechanism for the exploitation-exploration tradeoff is implicit, while E3 and R-Max have
to distinguish between “known” and “unknown ” states explicitly. Finally, in their original form both
E3 and R-Max need a policy’s ε-return mixing time Tε as input parameter. The knowledge of this
parameter then is eliminated by calculating the ε-optimal policy for Tε = 1, 2, . . ., so that sooner
or later the correct ε-return mixing time is reached. This is sufﬁcient to obtain polynomial PAC-
bounds, but seems to be intricate for practical purposes. Moreover, as noted in [2], at some time step
the assumed Tε may be exponential in the true Tε , which makes policy computation exponential in
Tε . Unlike that, we need our mixing time parameter only in the analysis. This makes our algorithm
rather simple and intuitive.

Recently, more reﬁned performance measures such as the sample complexity of exploration [3] were
introduced. Strehl and Littman [4] showed that in the discounted setting, efﬁciency in the sample
complexity implies efﬁciency in the average loss. However, average loss is deﬁned in respect to
the actually visited states, so that small average loss does not guarantee small total regret, which is
de ﬁned in respect to the states visited by an optimal policy. For this average loss polylogarithmic on-
line bounds were shown for for the MBIE algorithm [4], while more recently logarithmic bounds for
delayed Q-learning were given in [5]. However, discounted reinforcement learning is a bit simpler
than undiscounted reinforcement learning, as depending on the discount factor only a ﬁnite number
of steps is relevant. This makes discounted reinforcement learning similar to the setting with trials
of constant length from a ﬁxed initial state [6]. For this case logarithmic online regret bounds in the
number of trials have already been given in [7].

Since we measure performance during exploration, the exploration vs. exploitation dilemma be-
comes an important issue.
In the multi-armed bandit problem, similar exploration-exploitation
tradeoffs were handled with upper conﬁdence bounds for the expected immediate returns [8, 9].
This approach has been shown to allow good performance during the learning phase, while still con-
verging fast to a nearly optimal policy. Our UCRL algorithm takes into account the state structure
of the MDP, but is still based on upper conﬁdence bounds for the expected return of a policy. Up-
per conﬁdence bounds have been applied to reinforcement learning in various places and different
contexts, e.g. interval estimation [10, 11], action elimination [12], or PAC-learning [6]. Our UCRL
algorithm is similar to Strehl, Littman’s MBIE algorithm [10, 4], but our conﬁdence bounds are
different, and we are interested in the undiscounted case.

Another paper with a similar approach is Burnetas, Katehakis [13]. The basic idea of their rather
complex index policies is to choose the action with maximal return in some speciﬁed conﬁdence
region of the MDP’s probability distributions. The online-regret of their algorithm is asymptotically
logarithmic in the number of steps, which is best possible. Our UCRL algorithm is simpler and
achieves logarithmic regret not only asymptotically but uniformly over time. Moreover, unlike in
the approach of [13], knowledge about the MDP’s underlying state structure is not needed.

More recently, online reinforcement learning with changing rewards chosen by an adversary was
√
considered under the presumption that the learner has full knowledge of the transition probabilities
T ) after T steps.
[14]. The given algorithm achieves best possible regret of O(
In the subsequent Sections 2 and 3 we introduce our UCRL algorithm and show that its expected
online regret in unichain MDPs is O(log T ) after T steps. In Section 4 we consider problems that
arise when the underlying MDP is not unichain.

2 The UCRL Algorithm

To select good policies, we keep track of estimates for the average rewards and the transition prob-
abilities. For each step t let
Rt (s, a) = X
Nt (s, a) = |{0 ≤ τ < t : sτ = s, aτ = a}|,
rτ ,
0≤τ <t:
sτ =s, aτ =a
Pt (s, a, s0 ) = |{0 ≤ τ < t : sτ = s, aτ = a, sτ +1 = s0 }|,
be the number of steps when action a was chosen in state s, the sum of rewards obtained when
choosing this action, and the number of times the transition was to state s0 , respectively. From these
numbers we immediately get estimates for the average rewards and transition probabilities,
ˆrt (s, a) := Rt (s, a)
Nt (s, a) ,
ˆpt (s, a, s0 ) := Pt (s, a, s0 )
Nt (s, a) ,
provided that the number of visits in (s, a), Nt (s, a) > 0. In general, these estimates will deviate
from the respective true values. However, together with appropriate conﬁdence intervals they may
be used to deﬁne a set Mt of plausible MDPs. Our algorithm then chooses an optimal policy ˜πt for
t := ρ∗ ( ˜Mt ) among the MDPs in Mt . That is,
an MDP ˜Mt with maximal average reward ˜ρ∗
{ρ(M , π) : M ∈ Mt},
˜πt
:= arg max
and
π
{ρ(M , ˜πt )}.
˜Mt
:= arg max
M ∈Mt
More precisely, we want Mt to be a set of plausible MDPs in the sense that
P {ρ∗ > ˜ρ∗
t } < t−α
(2)
for some α > 2. Essentially, condition (2) means that it is unlikely that the true MDP M is not
in Mt . Actually, Mt is deﬁned to contain exactly those unichain MDPs M 0 whose transition
q log(2tα |S ||A|)
probabilities p0 (·, ·, ·) and rewards r 0 (·, ·) satisfy for all states s, s0 and actions a
|p0 (s, a, s0 ) − ˆpt (s, a, s0 )| ≤ q log(4tα |S |2 |A|)
r 0 (s, a) ≤ ˆrt (s, a) +
and
,
2Nt (s,a)
2Nt (s,a)
Conditions (3) and (4) describe conﬁdence bounds on the rewards and transition probabilities of the
true MDP M such that (2) is implied (cf. Section 3.1 below). The intuition behind the algorithm is
that if a non-optimal policy is followed, then this is eventually observed and something about the
MDP is learned. In the proofs we show that this learning happens sufﬁciently fast to approach an
optimal policy with only logarithmic regret.

(4)

(3)

.

As switching policies too often may be harmful, and estimates don’t change very much after few
steps, our algorithm discards the policy ˜πt only if there was considerable progress concerning the es-
timates ˆp(s, ˜πt (s), s0 ) or ˆr(s, ˜πt (s)). That is, UCRL sticks to a policy until the length of some of the
conﬁdence intervals given by conditions (3) and (4) is halved. Only then a new policy is calculated.
We will see below (cf. Section 3.3) that this condition limits the number of policy changes without
paying too much for not changing to an optimal policy earlier. Summing up, Figure 1 displays our
algorithm.
Remark 1. The optimal policy ˜π in the algorithm can be efﬁciently calculated by a modiﬁed version
of value iteration (cf. [15]).

3 Analysis for Unichain MDPs

3.1 An Upper Bound on the Optimal Reward
We show that with high probability the true MDP M is contained in the set Mt of plausible MDPs.

n
1,

q log(4tα |S |2 |A|)
2Nt (s,a)

o

Notation:
Set confp (t, s, a) := min
Initialization:
• Set t = 0.
• Set N0 (s, a) := R0 (s, a) := P0 (s, a, s0 ) = 0 for all s, a, s0 .
• Observe ﬁrst state s0 .
For rounds k = 1, 2, . . . do

and confr (t, s, a) := min

n
1,

q log(2tα |S ||A|)
2Nt (s,a)

o

.

Initialize round k :
1. Set tk := t.
2. Recalculate estimates ˆrt (s, a) and ˆpt (s, a, s0 ) according to
ˆpt (s, a, s0 ) := Pt (s,a,s0 )
ˆrt (s, a) := Rt (s,a)
Nt (s,a) , and
Nt (s,a) ,
provided that Nt (s, a) > 0. Otherwise set ˆrt (s, a) := 1 and ˆpt (s, a, s0 ) := 1|S | .
3. Calculate new policy

{ρ(M , π) : M ∈ Mt},
˜πtk := arg max
π
where Mt consists of plausible unichain MDPs M 0 with rewards
r 0 (s, a) − ˆrt (s, a) ≤ confr (t, s, a)
and transition probabilities
|p0 (s, a, s0 ) − ˆpt (s, a, s0 )| ≤ confp (t, s, a).
Execute chosen policy ˜πtk :
confr (t, S, A) > confr (tk , S, A)/2 and
4. While
confp (t, S, A) > confp (tk , S, A)/2

do
(a) Choose action at := ˜πtk (st ).
(b) Observe obtained reward rt and next state st+1 .
(c) Update:
• Set Nt+1 (st , at ) := Nt (st , at ) + 1.
• Set Rt+1 (st , at ) := Rt (st , at ) + rt .
• Set Pt+1 (st , at , st+1 ) := Pt (st , at , st+1 ) + 1.
• All other values Nt+1 (s, a), Rt+1 (s, a), and Pt+1 (s, a, s0 ) are set to
Nt (s, a), Rt (s, a), and Pt (s, a, s0 ), respectively.
(d) Set t := t + 1.

Figure 1: The UCRL algorithm.

Lemma 1. For any t, any reward r(s, a) and any transition probability p(s, a, s0 ) of the true MDP
ˆrt (s, a) < r(s, a) − q log(2tα |S ||A|)
P n
o
M we have
q log(4tα |S |2 |A|)
o
P n| ˆpt (s, a, s0 ) − p(s, a, s0 )| >
2Nt (s,a)
2Nt (s,a)

t−α
2|S ||A| ,
t−α
2|S |2 |A| .

(5)

(6)

<

<

Proof. By Chernoff-Hoeffding’s inequality.

Using the deﬁnition of Mt as given by (3) and (4) and summing over all s, a, and s0 , Lemma
1 shows that M ∈ Mt with high probability. This implies that the maximal average reward ˜ρ∗
assumed by our algorithm when calculating a new policy at step t is an upper bound on ρ∗ (M ) with
t
high probability.
P {ρ∗ > ˜ρ∗
t } < t−α .
Corollary 1. For any t:

3.2 Sufﬁcient Precision and Mixing Times

In order to upper bound the loss, we consider the precision needed to guarantee that the policy
calculated by UCRL is (ε-)optimal. This sufﬁcient precision will of course depend on ε or – in case
one wants to compete with an optimal policy – the minimal difference between ρ∗ and the average
reward of some suboptimal policy,

∆ :=

π :ρ(M ,π)<ρ∗ ρ∗ − ρ(M , π).
min
It is sufﬁcient that the difference between ρ( ˜Mt , ˜πt ) and ρ(M , ˜πt ) is small in order to guarantee
that ˜πt is an (ε-)optimal policy. For if |ρ( ˜Mt , ˜πt ) − ρ(M , ˜πt )| < ε, then by Corollary 1 with high
probability
ε > |ρ( ˜Mt , ˜πt ) − ρ(M , ˜πt )| ≥ |ρ∗ (M ) − ρ(M , ˜πt )|,
so that ˜πt is already an ε-optimal policy on M . For ε = ∆, (7) implies the optimality of ˜πt .
Thus, we consider bounds on the deviation of the transition probabilities and rewards for the assumed
MDP ˜Mt from the true values, such that (7) is implied. This is handled in the subsequent proposition,
where we use the notion of the MDP’s mixing time, which will play an essential role throughout the
analysis.
Deﬁnition 2. Given an ergodic Markov chain C , let Ts,s0 be the ﬁrst passage time for two states s,
s0 , that is, the time needed to reach s0 when starting in s. Furthermore let Ts,s the return time to
E(Ts0 ,s )
state s. Let TC := maxs,s0∈S E(Ts,s0 ), and κC := maxs∈S
maxs0 6=s
. Then the mixing time
2E(Ts,s )
of a unichain MDP M is TM := maxπ TCπ , where Cπ is the Markov chain induced by π on M .
Furthermore, we set κM := maxπ κCπ .

(7)

Our notion of mixing time is different from the notion of ε-return mixing time given in [1, 2], which
depends on an additional parameter ε. However, it serves a similar purpose.
Proposition 1. Let p(·, ·), ˜p(·, ·) and r(·), ˜r(·) be the transition probabilities and rewards of the
MDPs M and ˜M under the policy ˜π , respectively. If for all states s, s0
| ˜p(s, s0 ) − p(s, s0 )| < εp :=
|˜r(s) − r(s)| < εr := ε
2

ε
2κM |S |2 ,

and

then |ρ( ˜M , ˜π) − ρ(M , ˜π)| < ε.

The proposition is an easy consequence of the following result about the difference in the stationary
distributions of ergodic Markov chains.
Theorem 1 (Cho, Meyer[16]). Let C , ˜C be two ergodic Markov chains on the same state space
S with transition probabilities p(·, ·), ˜p(·, ·) and stationary distributions µ, ˜µ. Then the difference
X
in the distributions µ, ˜µ can be upper bounded by the difference in the transition probabilities as
follows:
|µ(s) − ˜µ(s)| ≤ κC max
|p(s, s0 ) − ˜p(s, s0 )|,
s∈S
s0∈S

max
s∈S

(8)

where κC is as given in Deﬁnition 2.
X
Proof of Proposition 1. By (8),
|µ(s) − ˜µ(s)| ≤ |S |κM max
s∈S
s∈S

X
s0∈S

| ˜p(s, s0 ) − p(s, s0 )| ≤ κM |S |2 εp .

As the rewards are ∈ [0, 1] and P
|ρ( ˜M , ˜π) − ρ(M , ˜π)| ≤ X
| ˜µ(s) − µ(s)|˜r(s) + X
s µ(s) = 1, we have by (1)
s∈S
s∈S
< κM |S |2 εp + εr = ε.

|˜r(s) − r(s)|µ(s)

Since εr > εp and the conﬁdence intervals for rewards are smaller than for transition probabilities
(cf. Lemma 1), in the following we only consider the precision needed for transition probabilities.

3.3 Bounding the Regret

As can be seen from the description of the algorithm, we split the sequence of steps into rounds,
where a new round starts whenever the algorithm recalculates its policy. The following facts follow
immediately from the form of our conﬁdence intervals and Lemma 1, respectively.
Proposition 2. For halving a conﬁdence interval of a reward or transition probability for some
(s, a) ∈ S × A, the number Nt (s, a) of visits in (s, a) has to be at least doubled.
Corollary 2. The number of rounds after T steps cannot exceed |S ||A| log2
T|S ||A| .
Proposition 3. If Nt (s, a) ≥ log(4tα |S |2 |A|)
, then the conﬁdence intervals for (s, a) are smaller than
2θ2
θ .

We need to consider three sources of regret: ﬁrst, by executing a suboptimal policy in a round of
length τ , we may lose reward up to τ within this round; second, there may be some loss when chang-
ing policies; third, we have to consider the error probabilities with which some of our conﬁdence
intervals fail.

3.3.1 Regret due to Suboptimal Rounds
Proposition 3 provides an upper bound on the number
of visits needed in each (s, a) in order to guarantee that a newly calculated policy is optimal. This
can be used to upper bound the total number of steps in suboptimal rounds.
Consider all suboptimal rounds with | ˆptk (s, a, s0 ) − p(s, a, s0 )| ≥ εp for some s0 , where a policy
˜πtk with ˜πtk (s) = a is played. Let m(s, a) be the number of these rounds and τi (s, a) (i =
2 . Thus we may separate each round i into (cid:4) τi (s,a)
(cid:5) intervals
1, . . . , m(s, a)) their respective lengths. The mean passage time between any state s00 and s is upper
bounded by TM . Then by Markov’s inequality, the probability that it takes more than 2TM steps
to reach s from s00 is smaller than 1
2TM
of length ≥ 2TM , in each of which the probability of visiting state s is at least 1
2 . Thus we may
lower bound the number of visits Ns,a (n) in (s, a) within n such intervals by an application of
o ≥ 1 − 1
Pn
− pn log T
Chernoff-Hoeffding’s inequality:
Ns,a (n) ≥ n
2
T
Since by Proposition 3, Nt (s, a) < 2 log(4T α |S |2 |A|)
(cid:22) τi (s, a)
(cid:23)
, we get
m(s,a)X
εp 2
2TM
i=1
with probability 1 − 1
(cid:17)
E(cid:16) m(s,a)X
T for a suitable constant c < 11. This gives for the expected regret in these
rounds
log(4T α |S |2 |A|)
τi (s, a)
εp2
i=1
Applying Corollary 2 and summing up over all (s, a), one sees that the expected regret due to
suboptimal rounds cannot exceed
log(4T α |S |2 |A|)
2 c |S ||A|TM
εp2

log(4T α |S |2 |A|)
εp2

|S ||A| + |S ||A|.
T

+ 2TM |S |2 |A|2 log2

+ 2 m(s, a) TM +

T .

1
T

.

(9)

< c

< 2 c · TM

3.3.2 Loss by Policy Changes For any policy ˜πt there may be some states from which the ex-
pected average reward for the next τ steps is larger than when starting in some other state. This does
not play a role if τ → ∞. However, as we are playing our policies only for a ﬁnite number of steps
before considering a change, we have to take into account that every time we switch policies, we
may need a start-up phase to get into such a favorable state. In average, this cannot take more than
TM steps, as this time is sufﬁcient to reach any “good” state from some “bad” state. This is made
more precise in the following lemma. We omit a detailed proof.
Lemma 2. For all policies π , all starting states s0 and all T ≥ 0
(cid:17) ≥ T ρ(π , M ) − TM .
E(cid:16) T −1X
r(st , π(st ))
t=0
By Corollary 2, the corresponding expected regret after T steps is ≤ |S ||A|TM log2

T|S ||A| .

3.3.3 Regret if Conﬁdence Intervals Fail
Finally, we have to take into account the error prob-
abilities, with which in each round a transition probability or a reward, respectively, is not contained
in its conﬁdence interval. According to Lemma 1, the probability that this happens at some step t
2|S ||A| + |S |
|S ||A| . Now let t1 = 1, t2 , . . . , tN ≤ T
for a given state-action pair is < t−α
2|S |2 |A| = t−α
t−α
be the steps in which a new round starts. As the regret in each round can be upper bounded by its
|S ||A| (ti+1 − ti ) ≤ N −1X
∞X
N −1X
length, one obtains for the regret caused by failure of conﬁdence intervals
t−α
t−α
t1−α
|S ||A| < c0 ,
i
i
|S ||A| cti <
t=1
i=1
i=1
using that ti+1 − ti < cti for a suitable constant c = c(|S |, |A|, TM ) and provided that α > 2.

c

3.3.4 Putting Everything Together Summing up over all the sources of regret and replacing for
εp yields the following theorem, which is a generalization of similar results that were achieved for
the multi-armed bandit problem in [8].
Theorem 2. On unichain MDPs, the expected total regret of the UCRL algorithm with respect to an
(ε-)optimal policy after T > 1 steps can be upper bounded by
T ) < const · |A|TM κ2
M |S |5
log T + 3TM |S |2 |A|2 log2
E(Rε
ε2
E(RT ) < const · |A|TM κ2
M |S |5
log T + 3TM |S |2 |A|2 log2
∆2

T
|S ||A| , and
T
|S ||A| .

4 Remarks and Open Questions on Multichain MDPs

In a multichain MDP a policy π may split up the MDP into ergodic subchains S π
i . Thus it may
happen during the learning phase that one goes wrong and ends up in a part of the MDP that gives
suboptimal return but cannot be left under no policy whatsoever. As already observed by Kearns,
Singh [1], in this case it seems fair to compete with ρ∗ (M ) := maxπ minSπ
ρ(S π
i , π).
i
Unfortunately, the original UCRL algorithm may not work very well in this setting, as it is im-
possible for the algorithm to distinguish between a very low probability for a transition and its
impossibility. Here the “optimism in the face of uncertainty” idea fails, as there is no way to falsify
the wrong belief in a possible transition.

Obviously, if we knew for each policy which subchains it induces on M (the MDP’s ergodic struc-
ture), UCRL could choose an MDP ˜Mt and a policy ˜πt that maximizes the reward among all plau-
sible MDPs with the given ergodic structure. However, only the empiric ergodic structure (based
on the observations so far) is known. As the empiric ergodic structure may not be reliable, one may
additionally explore the ergodic structures of all policies. Alas, the number of additional exploration
steps will depend on the smallest positive transition probability. If the latter is not known, it seems
that logarithmic online regret bounds can be no longer guaranteed.

However, we conjecture that for a slightly modiﬁed algorithm the logarithmic online regret bounds
still hold for communicating MDPs, in which for any two states s, s0 there is a suitable policy π such
that s is reachable from s0 under π (i.e., s, s0 are contained in the same subchain S π
i ). As Theorem
1 does not hold for communicating MDPs in general, a proof would need a different analysis.

5 Conclusion and Outlook

Beside the open problems on multichain MDPs, it is an interesting question whether our results
also hold when assuming for the mixing time not the slowest policy for reaching any state but
the fastest. Another research direction is to consider value function approximation and continuous
reinforcement learning problems.

For practical purposes, using the variance of the estimates will reduce the width of the upper conﬁ-
dence bounds and will make the exploration even more focused, improving learning speed and regret
bounds. In this setting, we have experimental results comparable to those of the MBIE algorithm
[10], which clearly outperforms other learning algorithms like R-Max or ε-greedy.

Acknowledgements.

This work was supported in part by the the Austrian Science Fund FWF (S9104-N04 SP4) and the
IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-
506778. This publication only reﬂects the authors’ views.

References

[1] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Mach.
Learn., 49:209–232, 2002.
[2] Ronen I. Brafman and Moshe Tennenholtz. R-max – a general polynomial time algorithm for near-optimal
reinforcement learning. J. Mach. Learn. Res., 3:213–231, 2002.
[3] Sham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College
London, 2003.
[4] Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval estimation.
In Proc. 22nd ICML 2005, pages 857–864, 2005.
[5] Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. Pac model-free
reinforcement learning. In Proc. 23nd ICML 2006, pages 881–888, 2006.
[6] Claude-Nicolas Fiechter. Efﬁcient reinforcement learning. In Proc. 7th COLT, pages 88–97. ACM, 1994.
[7] Peter Auer and Ronald Ortner. Online regret bounds for a new reinforcement learning algorithm. In Proc.
1st ACVW, pages 35–42. ¨OCG, 2005.
[8] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3:397–
422, 2002.
[9] Peter Auer, Nicol `o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multi-armed bandit prob-
lem. Mach. Learn., 47:235–256, 2002.
[10] Alexander L. Strehl and Michael L. Littman. An empirical evaluation of interval estimation for Markov
decision processes. In Proc. 16th ICTAI, pages 128–135. IEEE Computer Society, 2004.
[11] Leslie P. Kaelbling. Learning in Embedded Systems. MIT Press, 1993.
[12] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for rein-
forcement learning. In Proc. 20th ICML, pages 162–169. AAAI Press, 2003.
[13] Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision pro-
cesses. Math. Oper. Res., 22(1):222–255, 1997.
[14] Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour. Experts in a Markov decision process. In Proc.
17th NIPS, pages 401–408. MIT Press, 2004.
[15] Martin L. Puterman. Markov Decision Processes. Discrete Stochastic Programming. Wiley, 1994.
[16] Grace E. Cho and Carl D. Meyer. Markov chain sensitivity measured by mean ﬁrst passage times. Linear
Algebra Appl., 316:21–28, 2000.


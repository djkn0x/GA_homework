A Nonparametric Bayesian Method for Inferring
Features From Similarity Judgments

Thomas L. Grifﬁths
Daniel J. Navarro
Department of Psychology
School of Psychology
UC Berkeley
University of Adelaide
Berkeley, CA 94720, USA
Adelaide, SA 5005, Australia
daniel.navarro@adelaide.edu.au tom griffiths@berkeley.edu

Abstract

The additive clustering model is widely used to infer the features of a set of stimuli
from their similarities, on the assumption that similarity is a weighted linear func-
tion of common features. This paper develops a fully Bayesian formulation of the
additive clustering model, using methods from nonparametric Bayesian statistics
to allow the number of features to vary. We use this to explore several approaches
to parameter estimation, showing that the nonparametric Bayesian approach pro-
vides a straightforward way to obtain estimates of both the number of features
used in producing similarity judgments and their importance.

1

Introduction

One of the central problems in cognitive science is determining the mental representations that un-
derlie human inferences. A variety of solutions to this problem are based on the analysis of similarity
judgments. By deﬁning a probabilistic model that accounts for the similarity between stimuli based
on their representation, statistical methods can be used to infer underlying representations from
human similarity judgments. The particular methods used to infer representions from similarity
judgments depend on the nature of the underlying representations. For stimuli that are assumed to
be represented as points in some psychological space, multidimensional scaling algorithms [1] can
be used to translate similarity judgments into stimulus locations. For stimuli that are assumed to be
represented in terms of a set of latent features, additive clustering is the method of choice.

The original formulation of the additive clustering (ADCLUS) problem [2] is as follows. Assume
that we have data in the form of a n× n similarity matrix S = [sij ], where sij is the judged similarity
between the ith and j th of n objects. Similarities are assumed to be symmetric (with sij = sj i )
and non-negative, often constrained to lie on the interval [0, 1]. These empirical similarities are
assumed to be well-approximated by a weighted linear function of common features. Under these
assumptions, a representation that uses m features to describe n objects is given by an n × m matrix
F = [fik ], where fik = 1 if the ith object possesses the kth feature, and fik = 0 if it is not. Each
feature has an associated non-negative saliency weight w = (w1, . . . , wm ). When written in matrix
form, the ADCLUS model seeks to uncover a feature matrix F and a weight vector w such that
S ≈ FWF0 , where W = diag(w) is a diagonal matrix with nonzero elements corresponding to
the saliency weights. In most applications it is assumed that there is a ﬁxed “additive constant”, a
required feature possessed by all objects.

2 A Nonparametric Bayesian ADCLUS Model

To formalize additive clustering as a statistical model, it is standard practice to assume that error
terms are i.i.d. Gaussian [3], yielding the model:
S = FWF0 + E,

(1)

(cid:79)

(cid:68)

The Indian Buffet

w

F

The Diners

f1

f2

f3

f4

...

(cid:80)

s

n(n-1)/2

(a)

(cid:86)

f11 =1

f12 =1

f21 =1

f23 =1

f33 =1

f34 =1

(b)

Figure 1: Graphical model representation of the IBP-ADCLUS model. Panel (a) shows the hier-
archical structure of the ADCLUS model, and panel (b) illustrates the method by which a feature
matrix is generated using the Indian Buffet Process.

where E = [ij ] is an n × n matrix with entries drawn from a Gaussian (0, σ2 ) distribution. Equa-
tion 1 reveals that the additive clustering model is structurally similar to the better-known fac-
µij = Pk wk fik fj k to be the similarity predicted by a particular choice of F and w, then:
tor analysis model [4], although there are several differences: most notably the constraints that
F is binary valued, W is necessarily diagonal and S is non-negative. In any case, if we deﬁne
sij | F, w, σ ∼ Normal(µij , σ2),
where σ2 is the variance of the Gaussian error distribution. However, self-similarities sii are not
modeled in additive clustering, and are generally ﬁxed to (the same) arbitrary values for both the
model and data. It is typical to treat σ2 as a ﬁxed parameter [5], and while this could perhaps be
improved upon, we leave this open for future research.

(2)

In our approach, additive clustering is framed as a form of nonparametric Bayesian inference, in
which Equation 2 provides the likelihood function, and the model is completed by placing priors
over the weights w and the feature matrix F. We assume a ﬁxed Gamma prior over feature saliencies
though it is straightforward to extend this to other, more ﬂexible, priors. Setting a prior over binary
feature matrices F is more difﬁcult, since there is generally no good reason to assume an upper
bound on the number of features that might be relevant to a particular similarity matrix. For this
reason we use the “nonparametric ”
Indian Buffet Process (IBP) [6], which provides a proper prior
distribution over binary matrices with a ﬁxed number of rows and an unbounded number of columns.
The IBP can be understood by imagining an Indian buffet containing an inﬁnite number of dishes.
Each customer entering the restaurant samples a number of dishes from the buffet, with a preference
for those dishes that other diners have tried. For the kth dish sampled by at least one of the ﬁrst
n − 1 customers, the probability that the nth customer will also try that dish is
p(fnk = 1|Fn−1) = nk
,
n
where Fn−1 records the choices of the previous customers, and nk denotes the number of previous
customers that have sampled that dish. Being adventurous, the new customer may try some hitherto
untasted meals from the inﬁnite buffet on offer. The number of new dishes taken by customer n
follows a Poisson(α/n) distribution. The complete IBP-ADCLUS model becomes,
sij | F, w, σ ∼ Normal(µij , σ2 )
wk | λ1, λ2
∼ Gamma(λ1 , λ2)
F | α
∼ IBP(α).
The structure of this model is illustrated graphically in Figure 1(a), and an illustration of the IBP
prior is shown in Figure 1(b).

(3)

(4)

3 A Gibbs-Metropolis Sampling Scheme

As a Bayesian formulation of additive clustering, statistical inference in Equation 4 is based on the
posterior distribution over feature matrices and saliency vectors, p(F, w | S). Naturally, the ideal

approach is to calculate posterior quantities using exact methods. Unfortunately, this is generally
quite difﬁcult, so a natural alternative is to use Markov chain Monte Carlo (MCMC) methods to
repeatedly sample from the posterior distribution: estimates of posterior quantities can be made
using these samples as proxies for the full distribution. We construct a simple MCMC scheme
for the Bayesian ADCLUS model using a combination of Gibbs sampling [7] and more general
Metropolis proposals [8].

(5)

Saliency Weights . We use a Metropolis scheme to resample the saliency weights. If the current
saliency is wk , a candidate w∗
k is ﬁrst generated from a Gaussian (wk , 0.05) distribution. The value
of wk is then reassigned using the Metropolis update rule. If w−k denotes the set of all saliencies
wk ← (cid:26) w∗
except wk , this rule is
, where a = p(S | F,w−k ,w
k | λ)
∗
∗
k with probability a
wk with probability 1 − a
k )p(w
p(S | F,w−k ,wk )p(wk | λ) .
With a Gamma prior, the Metropolis sampler automatically rejects all negative valued w∗
k .
. For features currently possessed by at least one object, assignments are
“Pre-Existing” Features
updated using a standard Gibbs sampler: the value of fik is drawn from the conditional posterior
distribution over fik | S, F−ik, w. Since feature assignments are discrete, it is easy to ﬁnd this
conditional probability by noting that
p(fik |S, F−ik, w) ∝ p(S|F, w)p(fik|F−ik ),
(6)
where F−ik denotes the set of all feature assignments except fik . The ﬁrst term in this expression
is just the likelihood function for the ADCLUS model, and is simple to calculate. Moreover, since
feature assignments in the IBP are exchangeable, we can treat the kth assignment as if it were the
last. Given this, Equation 3 indicates that p(fik |F−ik) = n−ik/n, where n−ik counts the number
of stimuli (besides the ith) that currently possess the kth feature. The Gibbs sampler deletes all
single-stimulus features with probability 1, since n−ik will be zero for one of the stimuli.
. Since the IBP describes a prior over inﬁnite feature matrices, the resampling proce-
“New ” Features
dure needs to accommodate the remaining (in ﬁnite) set of features that are not currently represented
among the manifest features F. When resampling feature assignments, some ﬁnite number of those
currently-latent features will become manifest. When sampling from the conditional prior over fea-
ture assignments for the ith stimulus, we hold the feature assignments ﬁxed for all other stimuli, so
this is equivalent to sampling some number of “singleton ” features (i.e., features possessed only by
stimulus i) from the conditional prior, which is Poisson( α/n) as noted previously.

When working with this algorithm, we typically run several chains. For each chain, we initialize the
Gibbs-Metropolis sampler more or less arbitrarily. After a “burn-in ” period is allowed for the sam-
pler to converge to a sensible location (i.e., for the state to represent a sample from the posterior), we
make a “draw” by recording the state of the sampler, leaving a “lag ” of several iterations between
successive draws to reduce the autocorrelation between samples. When doing so, it is important to
ensure that the Markov chains converge on the target distribution p(F, w | S). We did so by inspect-
ing the time series plot formed by graphing the log posterior probability of successive samples. To
illustrate this, one of the chains used in our simulations (see Section 5) is displayed in Figure 2,
with nine parallel chains used for comparison: the time series plot shows no long-term trends, and
that different chains are visually indistinguishable from one another. Although elaborations and re-
ﬁnements are possible for both the sampler [9] and the convergence check [10], we have found this
approach to be reasonably effective for the moderate-sized problems considered in our applications.

4 Four Estimators for the ADCLUS Model

Since the introduction of the additive clustering model, a range of algorithms have been used to infer
features, including “subset selection” [2], expectation maximization [3], continuous approximations
[11] and stochastic hillclimbing [5] among others. A review, as well as an effective combinatorial
search algorithm, is given in [12]. Curiously, while the plethora of algorithms available for extract-
ing estimates of F and w have been discussed in the literature, the variety in the choice of estimator
has been largely overlooked, to our knowledge. One advantage of the IBP-ADCLUS approach is
that it allows us to discuss a range of different estimators that within a single framework. We will
explore estimators based on computing the posterior distribution over F and w given S. This in-
cludes estimators based on maximum a posteriori (MAP) estimation, corresponding to the value of
a variable with highest posterior probability, and taking expectations over the posterior distribution.

−170

−175

−180

−185

−190

y
t
i
l
i
b
a
b
o
r
P
 
r
o
i
r
e
t
s
o
P
−
g
o
L
 
d
e
h
t
o
o
m
S

−195

0

200

100

500
Sample Number
Figure 2: Smoothed time series showing log-posterior probabilities for successive draws from the
Gibbs-Metropolis sampler, for simulated similarity data with n = 16. The bold line shows a single
chain, while the dotted lines show the remaining nine chains.

1000

300

400

600

700

800

900

(8)

(7)

Conditional MAP Estimation. Much of the literature de ﬁnes an estimator
conditional on the as-
sumption that the number of features in the model m is ﬁxed [3][11][12]. These approaches seek
to estimate the values of F and w that jointly maximize some utility function conditional on this
known m. If we treat the posterior probability to be our measure of utility, the estimators become,
p(F, w | S, m)
ˆF1, ˆw1 = arg max
F,w
" X
#
Z p(F, w | S) dw
Estimating the dimension is harder. The natural (MAP) estimate for m is easy to state:
p(m | S) = arg max
ˆm1 = arg max
m
m
F∈Fm
where Fm denotes the set of feature matrices containing m unique features. In practice, given the
difﬁculty of working with Equation 8, it is typical to ﬁx m on the basis of intuition, or via some
heuristic method.
MAP Feature Estimation. In the previous approach, m is given primacy, since F and w cannot be
estimated until it is known. No distinction is made between F and w. In many practical situations
[13], this does not reﬂect the priorities of the researcher. Often the feature matrix F is the psycho-
logically relevant variable, with w and m being nuisance parameters. In such cases, it is natural to
(cid:20)Z p(F, w | S)dw(cid:21) .
marginalize w when estimating F, and let the estimated feature matrix itself determine m. That is,
we ﬁrst select
p(F | S) = arg max
ˆF2 = arg max
F
F
Notice that ˆF2 provides an implicit estimate of ˆm2 , which may differ from ˆm1 . The saliencies are
estimated after ˆF2 is chosen, via conditional MAP estimation:
p(w | ˆF2, S).
ˆw2 = arg max
w
This approach is typical of existing (parametric) Bayesian approaches to additive clustering [5][14],
where analytic approximations to p(F | S) are used for expediency.
Joint MAP Estimation. Both approaches discussed so far require some aspects of the model to
be estimated before others. While the rationales for this constraint differ, both approaches seem
sensible. Another approach, not as common in the literature, is to jointly estimate F and w without
conditioning on m, yielding the MAP estimators,
ˆF3 , ˆw3 = arg max
F,w

p(F, w | S).

(11)

(10)

(9)

Early papers [2] recognized that this approach can be prone to overﬁtting, and thus requires that the
prior place some emphasis on parsimony. However, many theoretically-motivated priors (includ-
ing the IBP) allow the researcher to emphasize parsimony, and some frequentist methods used in
ADCLUS-like models apply penalty functions for this reason [15].

(a)

15

10

5

d
e
r
e
v
o
c
e
R
 
s
e
r
u
t
a
e
F

(b)

n

8

16

32

So
Sn
St
So
Sn
St
So
Sn
St

ˆS1
79
78
87
89
90
96
91
91
100

ˆS2
81
81
88
88
88
95
91
91
100

ˆS3
79
78
87
89
90
96
91
91
100

ˆS4
84
84
92
90
90
97
91
91
100

0

6[8]
10[32]
8[16]
Number of Latent Features [Number of Objects]
Figure 3: Posterior distributions (a) over the number of features p(m | So ) in simulations containing
mt = 6, 8 and 10 features respectively. Variance accounted for (b) by the four similarity estimators
ˆS, where the target is either the observed training data So , a new test data set Sn, or the true similarity
matrix St.

Approximate Expectations. A fourth approach aims to summarize the posterior distribution by look-
ing at the marginal posterior probabilities associated with particular features. The probability that a
p(fk | S) = X
particular feature fk belongs in the representation is given by:
p(F | S).
F:fk∈F
Although this approach has never been applied in the ADCLUS literature, the concept is implicit in
more general discussions of mental representation [16] that ask whether or not a speciﬁc predicate
is likely to be represented. Letting ˆrk = p(fk | S) denote the posterior probability that feature fk
is manifest, we can construct a vector ˆr = [ˆrk] that contains these probabilities for all 2n possible
features. Although this vector discards the covariation between features across the posterior distribu-
tion, it is useful both theoretically (for testing hypotheses about speciﬁc features) and pragmatically,
∗ |S] = X
since the expected posterior similarities can be written as follows:
fk
where ˆwk = E [wk |fk, S] denotes the expected saliency for feature fk on those occasions when it
is represented (Equation 13 relies on the fact that features combine linearly in the ADCLUS model,
and is straightforward to derive). In practice, it is impossible to look at all 2n features, so one would
typically report only those features for which ˆrk is large. Since these tend to be the features that
∗ |S], there is a sense in which this approach approximates the
make the largest contributions to E [sij
expected posterior similarities.

fik fj k ˆrk ˆwk ,

E [sij

(12)

(13)

5 Recovering Noisy Feature Matrices

By using the IBP-ADCLUS framework, we can compare the performance of the four estimators in
a reasonable fashion. Loosely following [12], we generated noisy similarity matrices with n = 8,
16 and 32 stimuli, based on “true” feature matrices Ft in which mt = 2 log2(n), where each object
possessed each feature with probability 0.5. Saliency weights wt were generated uniformly from the
interval [1, 3], but were subsequently rescaled to ensure that the “true” similarities St had variance
1. Two sets of Gaussian noise were injected into the similarities with ﬁxed σ = 0.3, ensuring that
So and
the noise accounted for approximately 10% of the variance in the “observed” data matrix
the “new” matrix Sn. We ﬁxed α = 2 for all simulations: since the number of manifest features in
an IBP model follows a Poisson(αHn) distribution (where Hn is the nth harmonic number) [6], the
prior has a strong bias toward parsimony. The prior expected number of features is approximately
5.4, 6.8 and 8.1 (as compared to the true values of 6, 8 and 10).
We approximated the posterior distribution p(F, w | S1), by drawing samples in the following man-
ner. For a given similarity matrix, 10 Gibbs-Metropolis chains were run from different start points,
and 1000 samples were drawn from each. The chains were burnt in for 1000 iterations, and a lag
of 10 iterations was used between successive samples. Visual inspection suggested that ﬁve chains
in the n = 32 condition did not converge: log-posteriors were low, differed substantially from one

(a)

0.4

0.3

0.2

0.1

y
t
i
l
i
b
a
b
o
r
P

(b)

0.4

0.3

0.2

0.1

y
t
i
l
i
b
a
b
o
r
P

(c)

0.4

0.3

0.2

0.1

y
t
i
l
i
b
a
b
o
r
P

0

5

10
15
Number of Features

20

0

0

5
10
Number of Features

15

0

10

15
20
Number of Features

25

Figure 4: Posterior distributions over the number of features when the Bayesian ADCLUS model is
applied to (a) the numbers data, (b) the countries data and (c) the letters data.

2
0 1 2

Table 1: Two representations of the numbers data. (a) The representation reported in [3], extracted
using an EM algorithm with the number of features ﬁxed at eight. (b) The 10 most probable features
extracted using the Bayesian ADCLUS model. The ﬁrst column gives the posterior probability that
a particular feature belongs in the representation. The second column displays the average saliency
of a feature in the event that it is included.
(a)
W E IG H T
F EATU R E
0.444
4
0.345
0.331
0.291
0.255
0.216
0.214
0.172
0.148

6
9
6 7 8 9
2 3 4 5 6
3
5
1
1 2 3 4
4 5 6 7 8
additive constant

P RO B . WE IG H T
0.326
0.79
0.70
0.385
0.266
0.69
0.240
0.59
0.262
0.57
0.173
0.42
0.41
0.387
0.223
0.40
0.181
0.34
0.293
0.26
1.00
0.075

3

5
7
4 5 6 7 8
7 8 9
additive constant

0 1 2 3 4
2
4

8

4

2
0 1 2
2 3 4 5 6
6 7 8 9

F EATU R E
3
6

9

9

6

8

1

(b)

3

8

7

9

another, and had noticable positive slope. In this case, the estimators were constructed from the ﬁve
remaining chains.

Figure 3(a) shows the posterior distributions over the number of features m for each of the three
simulation conditions. There is a tendency to underestimate the number of features when provided
with small similarity matrices, with the modal number being 3, 7 and 10. However, since the pos-
terior estimate of m is below the prior estimate when n = 8, it seems this effect is data-driven, as
79% of the variance in the data matrix So can be accounted for using only three features.
Since each approach allows the construction of an estimated similarity matrix ˆS, a natural compar-
ison is to look at the proportion of variance this estimate accounts for in the observed data So , the
novel data set Sn, and the true matrix St. In view of the noise model used to construct these matri-
ces, the “ideal” answer for these three should be around 90%, 90% and 100% respectively. When
n = 32, this proﬁle is observed for all four estimators, suggesting that in this case all four estimators
have converged appropriately. For the smaller matrices, the conditional MAP and joint MAP esti-
mators ( ˆS1 and ˆS3) agree closely. The MAP feature approach ˆS3 appears to perform slightly better,
though the difference is very small. The expectation method ˆS4 provides the best estimate.

6 Modeling Empirical Similarities

We now turn to the analysis of empirical data. Since space constraints preclude detailed reporting
of all four estimators with respect to all data sets, we limit the discussion to the most novel IBP-
ADCLUS estimators, namely the direct estimates of dimensionality provided through Equation 8,
and the features extracted via “approximate expectation”.

Featural representations of numbers. A standard data set used in evaluating additive clustering
models measures the conceptual similarity of the numbers 0 through 9 [17]. This data set is often
used as a benchmark due to the complex interrelationships between the numbers. Table 1(a) shows
an eight-feature representation of these data, taken from [3] who applied a maximum likelihood
approach. This representation explains 90.9% of the variance, with features corresponding to arith-

Table 2: Featural representation of the similarity between 16 countries. The table shows the eight
highest-probability features extracted by the Bayesian ADCLUS model. Each column corresponds
to a single feature, with the associated probabilities and saliencies shown below. The average weight
associated with the additive constant is 0.035.

Italy
Germany
Spain

P RO B .
W E IG H T

1.00
0.593

F EATU R E
Vietnam Germany Zimbabwe Zimbabwe Iraq Zimbabwe Philippines
Nigeria Libya Nigeria
Russia
China
Indonesia
Nigeria
Iraq
Cuba
Japan
USA
Jamaica
Philippines China
Libya
Iraq
Indonesia
Japan
Libya
0.52
0.209

0.25
0.311

1.00
0.421

0.99
0.267

0.62
0.467

0.36
0.373

0.33
0.299

Table 3: Featural representation of the perceptual similarity between 26 capital letters. The table
shows the ten highest-probability features extracted by the Bayesian ADCLUS model. Each column
corresponds to a single feature, with the associated probabilities and saliencies shown below. The
average weight associated with the additive constant is 0.003.

F EATU R E
C
B
P
D
I
M
J
G
O
R
L
N
W
U
R
Q
T
P RO B .
1.00 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.92
W E IG H T 0.686 0.341 0.623 0.321 0.465 0.653 0.322 0.427 0.226 0.225

C
G

E
H

K
X

E
F

metic concepts and to numerical magnitude. Fixing σ = 0.05, and α = 0.5, we drew 10,000 lagged
samples to construct estimates. Although the posterior probability is spread over a large number of
feature matrices, 92.6% of sampled matrices had between 9 and 13 features. The modal number of
represented features was ˆm1 =11, with 27.2% of the posterior mass. The posterior distribution over
the number of features is shown in Figure 4(a). Since none of the existing literature has used the
“approximate expectation” approach to ﬁnd highly probable features, it is useful to note the strong
similarities between Table 1(a) and Table 1(b), which reports the ten highest-probability features
across the entire posterior distribution. Applying this approach to obtain an estimate of the posterior
predictive similarities ˆS4 revealed that this matrix accounts for 97.4% of the variance in the data.
Featural representations of countries. A second application is to human forced-choice judgments
of the similarities between 16 countries [18].
In this task, participants were shown lists of four
countries and asked to pick out the two countries most similar to each other. Applying the Bayesian
model to these data with σ = 0.1 reveals that only eight features appear in the representation more
than 25% of the time. Given this, it is not surprising that the posterior distribution over the number
of features, shown in Figure 4 (b), indicates that the modal number of features is eight. The eight
most probable features are listed in Table 2. The “approximate expectation” method explains 85.4%
of the variance, as compared to the 78.1% found by a MAP feature approach [18]. The features are
interpretable, corresponding to a range of geographical, historical, and economic regularities.

Featural representations of letters. As a third example, we analyzed a somewhat larger data set,
consisting of kindergarten children’s assessment of the perceptual similarity of the 26 capital letters
[19]. In this case, we used σ = 0.05, and the Bayesian model accounted for 89.2% of the variance
in the children’s similarity judgments. The posterior distribution over the number of represented
features is shown in Figure 4(c). Table 3 shows the ten features that appeared in more than 90% of
samples from the posterior. The model recovers an extremely intuitive set of overlapping features.
For example, it picks out the long strokes in I, L, and T, and the elliptical forms of D, O, and Q.

7 Discussion

Learning how similarity relations are represented is a difﬁcult modeling problem. Additive cluster-
ing provides a framework for learning featural representations of stimulus similarity, but remains
underused due to the difﬁculties associated with the inference. By adopting a Bayesian approach

to additive clustering, we are able to obtain a richer characterization of the structure behind human
similarity judgments. Moreover, by using nonparametric Bayesian techniques to place a prior dis-
tribution over inﬁnite binary feature matrices via the Indian Buffet Process, we can allow the data to
determine the number of features that the algorithm recovers. This is theoretically important as well
as pragmatically useful. As noted by [16], people are capable of recognizing that individual stimuli
possess an arbitrarily large number of characteristics, but in any particular context will make judg-
ments using only a ﬁnite, usually small number of properties that form part of our current mental
representation. In other words, by moving to a Bayesian nonparametric form, we are able to bring
the ADCLUS model closer to the kinds of assumptions that are made by psychological theories.

Acknowledgements. TLG was supported by NSF grant number 0631518, and DJN by ARC grants DP-
0451793 and DP-0773794. We thank Nancy Briggs, Simon Dennis and Michael Lee for helpful comments
on this work.

References

[1] W. S. Torgerson. Theory and Methods of Scaling . Wiley, New York, 1958.
[2] R. N. Shepard and P. Arabie. Additive clustering: Representation of similarities as combinations of
discrete overlapping properties. Psychological Review, 86:87–123, 1979.
[3] J. B. Tenenbaum. Learning the structure of similarity.
In D. S. Touretzky, M. C. Mozer, and M. E.
Hasselmo, editors, Advances in Neural Information Processing Systems , volume 8, pages 3–9. MIT Press,
Cambridge, MA, 1996.
[4] L. L. Thurstone. Multiple-Factor Analysis . University of Chicago Press, Chicago, 1947.
[5] M. D. Lee. Generating additive clustering models with limited stochastic complexity. Journal of Classi-
ﬁcation , 19:69–85, 2002.
[6] T. L. Grifﬁths and Z. Ghahramani. In ﬁnite latent feature models and the Indian buffet process. Technical
Report 2005-001, Gatsby Computational Neuroscience Unit, 2005.
[7] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and Machine Intelligence , 6:721–741, 1984.
[8] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equations of state
calculations by fast computing machines. Journal of Chemical Physics , 21:1087 –1092, 1953.
[9] Q.-M. Shao M.-H. Chen and J. G. Ibrahim. Monte Carlo Methods in Bayesian Computation . Springer,
New York, 2000.
[10] M. K. Cowles and B. P. Carlin. Markov chain Monte Carlo convergence diagnostics: A comparative
review. Journal of the American Statistical Association , 91:833–904, 1996.
[11] P. Arabie and J. Douglas Carroll. MAPCLUS: A mathematical programming approach to ﬁtting the
ADCLUS model. Psychometrika, 45:211–235, 1980.
[12] W. Ruml. Constructing distributed representations using additive clustering.
Information Processing Systems 14 , Cambridge, MA, 2001. MIT Press.
[13] M. D. Lee and D. J. Navarro. Extending the ALCOVE model of category learning to featural stimulus
domains. Psychonomic Bulletin and Review , 9:43–58, 2002.
[14] D. J. Navarro. Representing Stimulus Similarity . Ph.D. Thesis, University of Adelaide, 2003.
[15] L. E. Frank and W. J. Heiser. Feature selection in Feature Network Models: Finding predictive subsets of
features with the Positive Lasso. British Journal of Mathematical and Statistical Psychology , in press.
[16] D. L. Medin and A. Ortony. Psychological essentialism. In Similarity and Analogical Reasoning . Cam-
bridge University Press, New York, 1989.
[17] R. N. Shepard, D. W. Kilpatric, and J. P. Cunningham. The internal representation of numbers. Cognitive
Psychology, 7:82–138, 1975.
[18] D. J. Navarro and M. D. Lee. Commonalities and distinctions in featural stimulus representations. In
Proceedings of the 24th Annual Conference of the Cognitive Science Society , pages 685 –690, Mahwah,
NJ, 2002. Lawrence Erlbaum.
[19] E. Z. Rothkopf. A measure of stimulus similarity and errors in some paired-associate learning tasks.
Journal of Experimental Psychology , 53:94–101, 1957.

In Advances in Neural


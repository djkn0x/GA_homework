High-Dimensional Graphical Model Selection
Using `1-Regularized Logistic Regression

Martin J. Wainwright
Department of Statistics
Department of EECS
Univ. of California, Berkeley
Berkeley, CA 94720

Pradeep Ravikumar
John D. Laﬀerty
Machine Learning Dept. Computer Science Dept.
Carnegie Mellon Univ. Machine Learning Dept.
Carnegie Mellon Univ.
Pittsburgh, PA 15213
Pittsburgh, PA 15213

Abstract

We focus on the problem of estimating the graph structure associated
with a discrete Markov random ﬁeld. We describe a method based on `1 -
regularized logistic regression, in which the neighborhood of any given node
is estimated by performing logistic regression sub ject to an `1 -constraint.
Our framework applies to the high-dimensional setting, in which both the
number of nodes p and maximum neighborhood sizes d are allowed to grow
as a function of the number of observations n. Our main result is to estab-
lish suﬃcient conditions on the triple (n, p, d) for the method to succeed in
consistently estimating the neighborhood of every node in the graph simul-
taneously. Under certain mutual incoherence conditions analogous to those
imposed in previous work on linear regression, we prove that consistent
neighborhood selection can be obtained as long as the number of observa-
tions n grows more quickly than 6d6 log d + 2d5 log p, thereby establishing
that logarithmic growth in the number of samples n relative to graph size
p is suﬃcient to achieve neighborhood consistency.

Keywords: Graphical models; Markov random ﬁelds; structure learning; `1 -regularization;
model selection; convex risk minimization; high-dimensional asymptotics; concentration.

1 Introduction

Consider a p-dimensional discrete random variable X = (X1 , X2 , . . . , Xp ) where the dis-
tribution of X is governed by an unknown undirected graphical model. In this paper, we
investigate the problem of estimating the graph structure from an i.i.d. sample of n data
points {x(i) = (x(i)
1 , . . . , x(i)
p }n
i=1 . This structure learning problem plays an important role
in a broad range of applications where graphical models are used as a probabilistic repre-
sentation tool, including image processing, document analysis and medical diagnosis. Our
approach is to perform an `1 -regularized logistic regression of each variable on the remaining
variables, and to use the sparsity pattern of the regression vector to infer the underlying
neighborhood structure. The main contribution of the paper is a theoretical analysis show-
ing that, under suitable conditions, this procedure recovers the true graph structure with
probability one, in the high-dimensional setting in which both the sample size n and graph
size p = p(n) increase to inﬁnity.

The problem of structure learning for discrete graphical models—due to both its importance
and diﬃculty—has attracted considerable attention. Constraint based approaches use hy-
pothesis testing to estimate the set of conditional independencies in the data, and then
determine a graph that most closely represents those independencies [8]. An alternative ap-
proach is to view the problem as estimation of a stochastic model, combining a scoring metric
on candidate graph structures with a goodness of ﬁt measure to the data. The scoring met-

ric approach must be used together with a search procedure that generates candidate graph
structures to be scored. The combinatorial space of graph structures is super-exponential,
however, and Chickering [1] shows that this problem is in general NP-hard. The space of
candidate structures in scoring based approaches is typically restricted to directed models
(Bayesian networks) since the computation of typical score metrics involves computing the
normalization constant of the graphical model distribution, which is intractable for general
undirected models. Estimation of graph structures in undirected models has thus largely
been restricted to simple graph classes such as trees [2], polytrees [3] and hypertrees [9].

The technique of `1 regularization for estimation of sparse models or signals has a long
history in many ﬁelds; we refer to Tropp [10] for a recent survey. A surge of recent work has
shown that `1 -regularization can lead to practical algorithms with strong theoretical guaran-
tees (e.g., [4, 5, 6, 10, 11, 12]). In this paper, we adapt the technique of `1 -regularized logistic
regression to the problem of inferring graph structure. The technique is computationally
eﬃcient and thus well-suited to high dimensional problems, since it involves the solution
only of standard convex programs. Our main result establishes conditions on the sample
size n, graph size p and maximum neighborhood size d under which the true neighborhood
structure can be inferred with probability one as (n, p, d) increase. Our analysis, though
asymptotic in nature, leads to growth conditions that are suﬃciently weak so as to require
only that the number of observations n grow logarithmically in terms of the graph size.
Consequently, our results establish that graphical structure can be learned from relatively
sparse data. Our analysis and results are similar in spirit to the recent work of Meinshausen
and B¨uhlmann [5] on covariance selection in Gaussian graphical models, but focusing rather
on the case of discrete models.

The remainder of this paper is organized as follows. In Section 2, we formulate the problem
and establish notation, before moving on to a precise statement of our main result, and a
high-level proof outline in Section 3. Sections 4 and 5 detail the proof, with some technical
details deferred to the full-length version. Finally, we provide experimental results and a
concluding discussion in Section 6.

2 Problem Formulation and Notation

Let G = (V , E ) denote a graph with vertex set V of size |V | = p and edge set E . We denote
by N (s) the set of neighbors of a vertex v ∈ V ; that is N (s) = {(s, t) ∈ E }. A pairwise
graphical model with graph G is a family of probability distributions for a random variable
X = (X1 , X2 , . . . , Xp ) given by p(x) ∝ Q(s,t)∈E ψst (xs , xt ).
In this paper, we restrict
our attention to the case where each xs ∈ {0, 1} is binary, and the family of probability
distributions is given by the Ising model
p(x; θ) = exp (cid:16)Ps∈V θsxs + P(s,t)∈E θstxsxt − Ψ(θ)(cid:17) .
Given such an exponential family in a minimal representation, the log partition function
Ψ(θ) is strictly convex, which ensures that the parameter matrix θ is identiﬁable.
We address the following problem of graph learning. Given n samples x(i) ∈ {0, 1}p drawn
from an unknown distribution p(x; θ∗ ) of the form (1), let bEn be an estimated set of edges.
Our set-up includes the important situation in which the number of variables p may be large
relative to the sample size n. In particular, we allow the graph Gn = (Vn , En ) to vary with n,
so that the number of variables p = |Vn | and the sizes of the neighborhoods ds := |N (s)| may
vary with sample size. (For notational clarity we will sometimes omit subscripts indicating
a dependence on n.) The goal is to construct an estimator bEn for which P[ bEn = En ] → 1
as n → ∞. Equivalently, we consider the problem of estimating neighborhoods bNn (s) ⊂ Vn
so that P[ bNn(s) = N (s), ∀ s ∈ Vn ] −→ 1. For many problems of interest, the graphical
model provides a compact representation where the size of the neighborhoods are typically
small—say ds (cid:28) p for all s ∈ Vn . Our goal is to use `1 -regularized logistic regression to
estimate these neighborhoods; for this paper, the actual values of the parameters θij is a
secondary concern.

(1)

(2)

1
n

fs (θ; x) =

Given input data {(z (i) , y (i) )}, where z (i) is a p-dimensional covariate and y (i) ∈ {0, 1} is a
binary response, logistic regression involves minimizing the negative log likelihood
nXi=1 nlog(1 + exp(θT z (i) )) − y (i) θT z (i)o .
We focus on regularized version of this regression problem, involving an `1 constraint on (a
subset of ) the parameter vector θ. For convenience, we assume that z (i)
1 = 1 is a constant so
that θ1 is a bias term, which is not regularized; we denote by θ\s the vector of all coeﬃcients
of θ except the one in position s. For the graph learning task, we regress each variable Xs
onto the remaining variables, sharing the same data x(i) across problems. This leads to the
following collection of optimization problems (p in total, one for each graph node):
s θT z (i,s) i + λnkθ\sk1) .
θ∈Rp ( 1
nXi=1 hlog(1 + exp(θT z (i,s) )) − x(i)
bθs,λ = arg min
n
where s ∈ V , and z (i,s) ∈ {0, 1}p denotes the vector where z (i,s)
= x(i)
for t 6= s and
t
t
z (i,s)
s = 1. The parameter θs acts as a bias term, and is not regularized. Thus, the quantity
bθs,λ
can be thought of as a penalized conditional likelihood estimate of θs,t . Our estimate
t
of the neighborhood N (s) is then given by
bNn (s) = nt ∈ V , t 6= s : bθs,λ
6= 0o .
t
Our goal is to provide conditions on the graphical model—in particular, relations among the
number of nodes p, number of observations n and maximum node degree d—that ensure that
the collection of neighborhood estimates (2), one for each node s of the graph, is consistent
with high probability.

(3)

We conclude this section with some additional notation that is used throughout the sequel.
Deﬁning the probability p(z (i,s) ; θ) := [1 + exp(−θT z (i,s) )]−1 , straightforward calculations
yield the gradient and Hessian, respectively, of the negative log likelihood (2):
s z (i,s)!
p(z (i,s) ; θ) z (i,s) − θT   1
nXi=1
nXi=1
x(i)
n
nXi=1
1
∇2
p(z (i,s) ; θ) [1 − p(z (i,s) ; θ)] z (i,s) (z (i,s) )T .
θ fs (θ; x) =
n
Finally, for ease of notation, we make frequent use the shortand Qs (θ) = ∇2fs (θ; x).

∇θ fs (θ; x) =

(4b)

(4a)

1
n

3 Main Result and Outline of Analysis

In this section, we begin with a precise statement of our main result, and then provide a
high-level overview of the key steps involved in its proof.

3.1 Statement of main result

We begin by stating the assumptions that underlie our main result. A subset of the assump-
tions involve the Fisher information matrix associated with the logistic regression model,
deﬁned for each node s ∈ V as
s = E(cid:20)ps (Z ; θ∗ ) {1 − ps (Z ; θ∗ )}Z Z T (cid:21),
Q∗
Note that Q∗
s is the population average of the Hessian Qs (θ∗ ). For ease of notation we use
S to denote the neighborhood N (s), and S c to denote the complement V − N (s). Our
ﬁrst two assumptions (A1 and A2) place restrictions on the dependency and coherence
structure of this Fisher information matrix. We note that these ﬁrst two assumptions are
analogous to conditions imposed in previous work [5, 10, 11, 12] on linear regression. Our
third assumption is a growth rate condition on the triple (n, p, d).

(5)

[A1] Dependency condition: We require that the subset of the Fisher information
matrix corresponding to the relevant covariates has bounded eigenvalues: namely, there
exist constants Cmin > 0 and Cmax < +∞ such that
Cmin ≤ Λmin (Q∗
Λmax (Q∗
SS ) ≤ Cmax .
SS ),
and
These conditions ensure that the relevant covariates do not become overly dependent, and
can be guaranteed (for instance) by assuming that bθs,λ lies within a compact set.
[A2] Incoherence condition: Our next assumption captures the intuition that the large
number of irrelevant covariates (i.e., non-neighbors of node s) cannot exert an overly strong
eﬀect on the subset of relevant covariates (i.e., neighbors of node s). To formalize this
intuition, we require the existence of an  ∈ (0, 1] such that
kQ∗
S cS (Q∗
SS )−1 k∞ ≤ 1 − .
Analogous conditions are required for the success of the Lasso in the case of linear regres-
sion [5, 10, 11, 12].

(6)

(7)

[A3] Growth rates: Our second set of assumptions involve the growth rates of the number
of observations n, the graph size p, and the maximum node degree d.
In particular, we
require that:

n
d5 − 6d log(d) − 2 log(p) → +∞.
Note that this condition allows the graph size p to grow exponentially with the number of
observations (i.e., p(n) = exp(nα ) for some α ∈ (0, 1). Moreover, it is worthwhile noting that
for model selection in graphical models, one is typically interested in node degrees d that
remain bounded (e.g., d = O(1)), or grow only weakly with graph size (say d = o(log p)).

(8)

With these assumptions, we now state our main result:
Theorem 1. Given a graphical model and triple (n, p, d) such that conditions A1 through
A3
are satisﬁed, suppose that the regularization parameter λn is chosen such that (a)
n − 2 log(p) → +∞, and (b) dλn → 0. Then P[ bNn (s) = N (s), ∀ s ∈ Vn ] → 1 as
nλ2
n → +∞.
3.2 Outline of analysis

We now provide a high-level roadmap of the main steps involved in our proof of Theo-
rem 1. Our approach is based on the notion of a primal witness :
in particular, focusing
our attention on a ﬁxed node s ∈ V , we deﬁne a constructive procedure for generating a
primal vector bθ ∈ Rp as well as a corresponding subgradient bz ∈ Rn that together satisfy the
zero-subgradient optimality conditions associated with the convex program (3). We then
show that this construction succeeds with probability converging to one under the stated
conditions. A key fact is that the convergence rate is suﬃciently fast that a simple union
bound over all graph nodes shows that we achieve consistent neighborhood estimation for
all nodes simultaneously.
To provide some insight into the nature of our construction, the analysis in Section 4 shows
the neighborhood N (s) is correctly recovered if and only if the pair (bθ, bz ) satisﬁes the
following four conditions: (a) bθS c = 0; (b) |bθt | > 0 for all t ∈ S ; (c) bzS = sgn(θ∗
S ); and (d)
kbzS c k∞ < 1. The ﬁrst step in our construction is to choose the pair (bθ, bz ) such that both
conditions (a) and (c) hold. The remainder of the analysis is then devoted to establishing
that properties (b) and (d) hold with high probability.
In the ﬁrst part of our analysis, we assume that the dependence (A1) mutual incoherence
(A2) conditions hold for the sample Fisher information matrices Qs (θ∗ ) deﬁned below equa-
tion (4b). Under this assumption, we then show that the conditions on λn in the theorem
statement suﬃce to guarantee that properties (b) and (d) hold for the constructed pair
(bθ , bz ). The remainder of the analysis, provided in the full-length version of this paper, is

devoted to showing that under the speciﬁed growth conditions (A3), imposing incoherence
and dependence assumptions on the population version of the Fisher information Q∗ (θ∗ )
guarantees (with high probability) that analogous conditions hold for the sample quantities
Qs (θ∗ ). While it follows immediately from the law of large numbers that the empirical
AA (θ∗ ) converges to the population version Q∗
Fsiher information Qn
AA for any ﬁxed subset,
the delicacy is that we require controlling this convergence over subsets of increasing size.
Our analysis therefore requires the use of uniform laws of large numbers [7].

4 Primal-Dual Relations for `1 -Regularized Logistic Regression

(9)

Basic convexity theory can be used to characterize the solutions of `1 -regularized logistic
regression. We assume in this section that θ1 corresponds to the unregularized bias term,
and omit the dependence on sample size n in the notation. The ob jective is to compute
θ∈Rp (cid:8)f (θ; x) + λkθ\1 k1(cid:9)
θ∈Rp (cid:8)f (θ; x) + λ (cid:0)kθ\1k1 − b(cid:1)(cid:9) = min
θ∈Rp L(θ, λ) = min
min
The function L(θ, λ) is the Lagrangian function for the problem of minimizing f (θ; x) sub ject
to kθ\1k1 ≤ b for some b. The dual function is h(λ) = inf θ L(θ, λ).
If p ≤ n then f (θ; x) is a strictly convex function of θ. Since the `1 -norm is convex, it follows
that L(θ, λ) is convex in θ and strictly convex in θ for p ≤ n. Therefore the set of solutions
to (9) is convex. If bθ and bθ 0 are two solutions, then by convexity bθ + ρ(bθ 0 − bθ) is also a
solution for any ρ ∈ [0, 1]. Since the solutions minimize f (θ; x) sub ject to kθ\1k1 ≤ b, the
value of f (bθ + ρ(bθ 0 − bθ)) is independent of ρ, and ∇θ f (bθ ; x) is independent of the particular
solution bθ. These facts are summarized below.
Lemma 1. If p ≤ n then a unique solution to (9) exists. If p ≥ n then the set of solutions
is convex, with the value of ∇θ f (bθ; x) constant across al l solutions. In particular, if p ≥ n
and |∇θt f (bθ; x)| < λ for some solution bθ , then bθt = 0 for al l solutions.
The subgradient ∂ kθ\1k1 ⊂ Rp is the collection of all vectors z satisfying |zt | ≤ 1 and
zt = (cid:26)0
for t = 1
if θt 6= 0.
sign(θt )
Any optimum of (9) must satisfy
∂θ L(bθ , λ) = ∇θ f (bθ; x) + λz = 0
(10)
for some z ∈ ∂ kθ\1k. The analysis in the following sections shows that, with high probability,
a primal-dual pair (bθ , bz ) can be constructed so that |bzt | < 1 and therefore bθt = 0 in case
t = 0 in the true model θ∗ from which the data are generated.
θ∗
5 Constructing a Primal-Dual Pair

We now ﬁx a variable Xs for the logistic regression, denoting the set of variables in its
neighborhood by S . From the results of the previous section we observe that the `1 -
regularized regression recovers the sparsity pattern if and only if there exists a primal-dual
solution pair (bθ , bz ) satisfying the zero-subgradient condition, and the conditions (a) bθS c = 0;
(b) |bθt | > 0
for all t ∈ S and sgn(bθS ) = sgn(θ∗
S ); (c) bzS = sgn(θ∗
S ); and (d) kbzS c k∞ < 1.
Our proof proceeds by showing the existence (with high probability) of a primal-dual pair
(bθ , bz ) that satisfy these conditions. We begin by setting bθS c = 0, so that (a) holds, and
also setting bzS = sgn(bθS ), so that (c) holds. We ﬁrst establish a consistency result when
incoherence conditions are imposed on the sample Fisher information Qn . The remaining
analysis, deferred to the full-length version, establishes that the incoherence assumption
(A2) on the population version ensures that the sample version also obeys the property
with probability converging to one exponentially fast.

Theorem 2. Suppose that

(14a)

(14b)

SS )−1k∞ ≤ 1 − 
S cS (Qn
kQn
(11)
for some  ∈ (0, 1]. Assume that λn → 0 is chosen that λ2
nn − log(p) → +∞ and λnd → 0.
Then P (cid:16) bN (s) = N (s)(cid:17) = 1 − O(exp(−cnγ )) for some γ > 0.
Proof. Let us introduce the notation
1 + exp(θ∗ T z (i,s) ) !
z (i,s)  x(i)
nXi=1
exp(θ∗ T z (i,s))
1
W n
s −
:=
n
Substituting into the subgradient optimality condition (10) yields the equivalent condition
∇f (bθ; x) − ∇f (θ; x) − W n + λn bz = 0.
By a Taylor series expansion, this condition can be re-written as
∇2f (θ∗ ; x) [bθ − θ∗ ] = W n − λn bz + Rn ,
where the remainder Rn is a term of order kRnk2 = O(kbθ − θ∗ k2).
Using our shorthand Qn = ∇2
θ f (θ∗ ; x), we write the zero-subgradient condition (13) in block
form as:
S cS [bθs,λ
S − θ∗
S c − λn bzS c + Rn
Qn
S ] = W n
S c ,
SS [bθs,λ
S − θ∗
Qn
S ] = W n
S − λn bzS + Rn
S .
It can be shown that the matrix Qn
SS is invertible w.p. one, so that these conditions can be
rewritten as
SS )−1 [W n
S c − λn bzS c + Rn
S − λn bzS + Rn
Qn
S cS (Qn
S ] = W n
S c .
Re-arranging yields the condition
SS )−1 [W n
SS )−1 bzS = λn bzS c .
Qn
S cS (Qn
S − Rn
S ] − [W n
S c − Rn
S c ] + λnQn
S cS (Qn
Analysis of condition (d): We now demonstrate that kbzS c k∞ < 1. Using triangle
inequality and the sample incoherence bound (11) we have that
(2 − )
[kW nk∞ + kRnk∞ ] + (1 − )
kbzS c k∞ ≤
(17)
λn
We complete the proof that kbzS c k∞ < 1 with the following two lemmas, proved in the
full-length version.
Lemma 2. If nλ2
n − log(p) → +∞, then
P (cid:18) 2 − 
4 (cid:19) → 0

λn kW nk∞ ≥
(18)
n + log(p)(cid:1)).
at rate O(exp (cid:0)−nλ2
Lemma 3. If nλ2
n − log(p) → +∞ and dλn → 0, then we have
4 (cid:19) → 0
P (cid:18) 2 − 

λn kRnk∞ ≥
at rate O(exp (cid:0)−nλ2
n + log(p)(cid:1)).
We apply these two lemmas to the bound (17) to obtain that with probability converging
n − log(p)(cid:9)(cid:1), we have
to one at rate O(exp (cid:8)exp (cid:0)nλ2


+ (1 − ) = 1 −
kbzS c k∞ ≤
+
4
4


2

.

(15)

(16)

(12)

(13)

(19)

Analysis of condition (b): We next show that condition (b) can be satisﬁed, so that
sgn(bθS ) = sgn(θ∗
S ). Deﬁne ρn := mini∈S |θ∗
S |. From equation (14b), we have
bθs,λ
SS )−1 [WS − λn bzS + RS ] .
= θ∗
S − (Qn
(20)
S
| > 0 for all i ∈ S , and moreover that sign(bθs,λ
Therefore, in order to establish that |bθs,λ
S ) =
i
sign(θ∗
S ), it suﬃces to show that
(cid:13)(cid:13)(Qn
SS )−1 [WS − λn bzS + RS ](cid:13)(cid:13)∞ ≤
ρn
.
2
Using our eigenvalue bounds, we have
(cid:13)(cid:13)(Qn
SS )−1 [WS − λn bzS + RS ](cid:13)(cid:13)∞ ≤ k(Qn
SS )−1k∞ [kWS k∞ + λn + kRS k∞ ]
√d k(Qn
SS )−1 k2 [kWS k∞ + λn + kRS k∞ ]
≤
√d
[kWS k∞ + λn + kRS k∞ ] .
≤
Cmin
In fact, the righthand side tends to zero from our earlier results on W and R, and the
assumption that λn d → 0. Together with the exponential rates of convergence established
by the stated lemmas, this completes the proof of the result.

6 Experimental Results

We brieﬂy describe some experimental results that demonstrate the practical viability and
performance of our proposed method. We generated random Ising models (1) using the
following procedure: for a given graph size p and maximum degree d, we started with a graph
with disconnected cliques of size less than or equal to ten, and for each node, removed edges
randomly until the sparsity condition (degree less than d) was satisﬁed. For all edges (s, t)
present in the resulting random graph, we chose the edge weight θst ∼ U [−3, 3]. We drew n
i.i.d. samples from the resulting random Ising model by exact methods. We implemented the
`1 -regularized logistic regression by setting the `1 penalty as λn = O((log p)3√n), and solved
the convex program using a customized primal-dual algorithm (described in more detail in
the full-length version of this paper). We considered various sparsity regimes, including
constant (d = Ω(1)), logarithmic (d = α log(p)), or linear (d = αp).
In each case, we
evaluate a given method in terms of its average precision (one minus the fraction of falsely
included edges), and its recal l (one minus the fraction of falsely excluded edges). Figure 1
shows results for the case of constant degrees (d ≤ 4), and graph sizes p ∈ {100, 200, 400},
for the AND method (respectively the OR) method, in which an edge (s, t) is included if
and only if it is included in the local regressions at both node s and (respectively or ) node t.
Note that both the precision and recall tend to one as the number of samples n is increased.

7 Conclusion

We have shown that a technique based on `1 -regularization, in which the neighborhood of any
given node is estimated by performing logistic regression sub ject to an `1 -constraint, can be
used for consistent model selection in discrete graphical models. Our analysis applies to the
high-dimensional setting, in which both the number of nodes p and maximum neighborhood
sizes d are allowed to grow as a function of the number of observations n. Whereas the
current analysis provides suﬃcient conditions on the triple (n, p, d) that ensure consistent
neighborhood selection, it remains to establish necessary conditions as well [11]. Finally,
the ideas described here, while specialized in this paper to the binary case, should be more
broadly applicable to discrete graphical models.

Acknowledgments

Research supported in part by NSF grants IIS-0427206, CCF-0625879 and DMS-0605165.

AND Recall

 

1

0.95

0.9

0.85

0.8

0.75

0.7

n
o
i
s
i
c
e
r
P

AND Precision

 

0.7

0.6

0.5

l
l
a
c
e
R

0.4

0.3

0.2

0.1

0.65

 
0

200

400

600
1400
1200
1000
800
Number of Samples

1600

1800

2000

0

 
0

200

400

600
1400
1200
1000
800
Number of Samples

1600

1800

2000

 p = 100
 p = 200
 p = 400

 p = 100
 p = 200
 p = 400

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

n
o
i
s
i
c
e
r
P

OR Recall

 

OR Precision

 

0.7

0.6

0.5

l
l
a
c
e
R

0.4

0.3

0.2

0.1

 p = 100
 p = 200
 p = 400

 p = 100
 p = 200
 p = 400

0.5

 
0

200

400

600
1400
1200
1000
800
Number of Samples

1600

1800

2000

0

 
0

200

400

600
1400
1200
1000
800
Number of Samples

1600

1800

2000

Figure 1. Precision/recall plots using the AND method (top), and the OR method (bot-
tom). Each panel shows precision/recall versus n, for graph sizes p ∈ {100, 200, 400}.

References

[1] D. Chickering. Learning Bayesian networks is NP-complete. Proceedings of AI and
Statistics, 1995.
[2] C. Chow and C. Liu. Approximating discrete probability distributions with dependence
trees. IEEE Trans. Info. Theory, 14(3):462–467, 1968.
[3] S. Dasgupta. Learning polytrees. In Uncertainty on Artiﬁcial Intel ligence, pages 134–
14, 1999.
[4] D. Donoho and M. Elad. Maximal sparsity representation via `1 minimization. Proc.
Natl. Acad. Sci., 100:2197–2202, March 2003.
[5] N. Meinshausen and P. B¨uhlmann. High dimensional graphs and variable selection with
the lasso. Annals of Statistics, 34(3), 2006.
[6] A. Y. Ng. Feature selection, l1 vs. l2 regularization, and rotational invariance.
International Conference on Machine Learning, 2004.
[7] D. Pollard. Convergence of stochastic processes. Springer-Verlag, New York, 1984.
[8] P. Spirtes, C. Glymour, and R. Scheines. Causation, prediction and search. MIT Press,
2000.
[9] N. Srebro. Maximum likelihood bounded tree-width Markov networks. Artiﬁcial Intel-
ligence, 143(1):123–138, 2003.
[10] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals.
IEEE Trans. Info. Theory, 51(3):1030–1051, March 2006.
[11] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery
using `1 -constrained quadratic programs. In Proc. Al lerton Conference on Communi-
cation, Control and Computing, October 2006.
[12] P. Zhao and B. Yu. Model selection with the lasso. Technical report, UC Berkeley,
Department of Statistics, March 2006. Accepted to Journal of Machine Learning Re-
search.

In


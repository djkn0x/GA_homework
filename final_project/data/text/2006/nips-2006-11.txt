Sample complexity of policy search with known
dynamics

Peter L. Bartlett
Divison of Computer Science and Department of Statistics
University of California, Berkeley
Berkeley, CA 94720-1776
bartlett@cs.berkeley.edu

Ambuj Tewari
Division of Computer Science
University of California, Berkeley
Berkeley, CA 94720-1776
ambuj@cs.berkeley.edu

Abstract

We consider methods that try to ﬁnd a good policy for a Markov d ecision process
by choosing one from a given class. The policy is chosen based on its empirical
performance in simulations. We are interested in conditions on the complexity
of the policy class that ensure the success of such simulation based policy search
methods. We show that under bounds on the amount of computation involved
in computing policies, transition dynamics and rewards, uniform convergence of
empirical estimates to true value functions occurs. Previously, such results were
derived by assuming boundedness of pseudodimension and Lipschitz continuity.
These assumptions and ours are both stronger than the usual combinatorial com-
plexity measures. We show, via minimax inequalities, that this is essential: bound-
edness of pseudodimension or fat-shattering dimension alone is not sufﬁcient.

1 Introduction

A Markov Decision Process (MDP) models a situation in which an agent interacts (by performing
actions and receiving rewards) with an environment whose dynamics is Markovian, i.e. the future is
independent of the past given the current state of the environment. Except for toy problems with a
few states, computing an optimal policy for an MDP is usually out of the question. Some relaxations
need to be done if our aim is to develop tractable methods for achieving near optimal performance.
One possibility is to avoid considering all possible policies by restricting oneself to a smaller class
Π of policies. Given a simulator for the environment, we try to pick the best policy from Π. The
hope is that if the policy class is appropriately chosen, the best policy in Π would not be too much
worse than the true optimal policy.

Use of simulators introduces an additional issue: how is one to be sure that performance of policies
in the class Π on a few simulations is indicative of their true performance? This is reminiscent of
the situation in statistical learning. There the aim is to learn a concept and one restricts attention
to a hypotheses class which may or may not contain the “true ” c oncept. The sample complexity
question then is: how many labeled examples are needed in order to be con ﬁdent that error rates on
the training set are close to the true error rates of the hypotheses in our class? The answer turns out to
depend on “complexity ” of the hypothesis class as measured b y combinatorial quantities associated
with the class such as the VC dimension, the pseudodimension and the fat-shattering dimension.

Some progress [6,7] has already been made to obtain uniform bounds on the difference between
value functions and their empirical estimates, where the value function of a policy is the expected
long term reward starting from a certain state and following the policy thereafter. We continue this
line of work by further investigating what properties of the policy class determine the rate of uniform
convergence of value function estimates. The key difference between the usual statistical learning
setting and ours is that we not only have to consider the complexity of the class Π but also of the

classes derived from Π by composing the functions in Π with themselves and with the state evolution
process implied by the simulator.

Ng and Jordan [7] used a ﬁnite pseudodimension condition alo ng with Lipschitz continuity to derive
uniform bounds. The Lipschitz condition was used to control the covering numbers of the iterated
function classes. We provide a uniform convergence result (Theorem 1) under the assumption that
policies are parameterized by a ﬁnite number of parameters a nd that the computations involved
in computing the policy, the single-step simulation function and the reward function all require a
bounded number of arithmetic operations on real numbers. The number of samples required grows
linearly with the dimension of the parameter space but is independent of the dimension of the state
space. Ng and Jordan’s and our assumptions are both stronger than just assuming ﬁniteness of some
combinatorial dimension. We show that this is unavoidable by constructing two examples where the
fat-shattering dimension and the pseudodimension respectively are bounded, yet no simulation based
method succeeds in estimating the true values of policies well. This happens because iteratively
composing a function class with itself can quickly destroy ﬁ niteness of combinatorial dimensions.
Additional assumptions are therefore needed to ensure that these iterates continue to have bounded
combinatorial dimensions.

Although we restrict ourselves to MDPs for ease of exposition, the analysis in this paper carries over
easily to the case of partially obervable MDPs (POMDPs), provided the simulator also simulates the
conditional distribution of observations given state using a bounded amount of computation. The
plan of the rest of the paper is as follows. We set up notation and terminology in Section 2. In
the same section, we describe the model of computation over reals that we use. Section 3 proves
Theorem 1, which gives a sample complexity bound for achieving a desired level of performance
within the policy class. In Section 4, we give two examples of policy classes whose combinatorial
dimensions are bounded. Nevertheless, we can prove strong minimax lower bounds implying that
no method of choosing a policy based on empirical estimates can do well for these examples.

2 Preliminaries

We de ﬁne an MDP M as a tuple (S, D, A, P (·|s, a), r, γ ) where S is the state space, D the initial
state distribution, A the action space, P (s0 |s, a) gives the probability of moving to state s0 upon
taking action a in state s, r is a function mapping states to distributions over rewards (which are as-
sumed to lie in a bounded interval [0, R]), and γ ∈ (0, 1) is a factor that discounts future rewards. In
this paper, we assume that the state space S and the action space A are ﬁnite dimensional Euclidean
spaces of dimensionality dS and dA respectively.
A (randomized) policy π is a mapping from S to distributions over A. Each policy π induces a
natural Markov chain on the state space of the MDP, namely the one obtained by starting in a start
state s0 sampled from D and st+1 sampled according to P (·|st , at ) with at drawn from π(st ) for
t ≥ 0. Let rt (π) be the expected reward at time step t in this Markov chain, i.e. rt (π) = E[ρt ]
where ρt is drawn from the distribution r(st ). Note that the expectation is over the randomness in
the choice of the initial state, the state transitions, and the randomized policy and reward outcomes.
De ﬁne the value VM (π) of the policy by

∞
Xt=0
We omit the subscript M in the value function if the MDP in question is unambiguously identiﬁed.
For a class Π of policies, de ﬁne

γ t rt (π) .

VM (π) =

opt(M, Π) = sup
π∈Π
The regret of a policy π 0 relative to an MDP M and a policy class Π is de ﬁned as
RegM,Π (π 0 ) = opt(M, Π) − VM (π 0 ) .

VM (π) .

We use a degree bounded version of the Blum-Shub-Smale [3] model of computation over reals. At
each time step, we can perform one of the four arithmetic operations +, −, ×, / or can branch based
on a comparison (say <). While Blum et al. allow an arbitrary ﬁxed rational map to be computed in
one time step, we further require that the degree of any of the polynomials appearing at computation
nodes be at most 1.

De ﬁnition 1. Let k , l, m, τ be positive integers, f a function from Rk to probability distributions
over Rl and Ξ a probability distribution over Rm . The function f is (Ξ, τ )-computable if there
exists a degree bounded ﬁnite dimensional machine M over R with input space Rk+m and output
space Rl such that the following hold.

1. For every x ∈ Rk and ξ ∈ Rm , the machine halts with halting time TM (x, ξ ) ≤ τ .

2. For every x ∈ Rk , if ξ ∈ Rm is distributed according to Ξ the input-output map ΦM (x, ξ )
is distributed as f (x).

Informally, the de ﬁnition states that given access to an ora cle which generates samples from Ξ, we
can generate samples from f (x) by doing a bounded amount of computation. For precise de ﬁnit ions
of the input-output map and halting time, we refer the reader to [3, Chap. 2].

In Section 3, we assume that the policy class Π is parameterized by a ﬁnite dimensional parameter
θ ∈ Rd .
In this setting π(s; θ), P (·|s, a) and r(s) are distributions over RdA , RdS and [0, R]
respectively. The following assumption states that all these maps are computable within τ time
steps in our model of computation.
Assumption A. There exists a probability distribution Ξ over Rm and a positive integer τ such
that π(s; θ), P (·|s, a) and r(s) are (Ξ, τ )-computable. Let Mπ , MP and Mr respectively be the
machines that compute them.

that make a call to a random number
This assumption will be satisﬁed if we have three “programs”
generator for distribution Ξ, do a ﬁxed number of ﬂoating-point operations and simulate t
he policies
in our class, the state-transition dynamics and the rewards respectively. The following two examples
illustrate this for the state-transition dynamics.

• Linear Dynamical System with Additive Noise 1
Suppose P and Q are dS × dS and dS × dA matrices and the system dynamics is given by
(1)
st+1 = P st + Qat + ξt ,
where ξt are i.i.d. from some distribution Ξ. Since computing (1) takes 2(d2
S + dS dA + dS )
operations, P (·|s, a) is (Ξ, τ )-computable for τ = O(dS (dS + dA )).
• Discrete States and Actions
Suppose S = {1, 2, . . . , nS } and A = {1, 2, . . . , nA}. For some ﬁxed s, a, P (·|s, a) is
described by n numbers ~ps,a = (p1 , . . . , pnS ), Pi pi = 1. Let Pk = Pk
i=1 pi . For
ξ ∈ (0, 1], set f (ξ ) = min{k : Pk ≥ ξ}. Thus, if ξ has uniform distribution on (0, 1], then
f (ξ ) = k with probability pk . Since the Pk ’s are non-decreasing, f (ξ ) can be computed
in log nS steps using binary search. But this was for a ﬁxed s, a pair. Finding which ~ps,a
to use, further takes log(nS nA ) steps using binary search. So if Ξ denotes the uniform
distribution on (0, 1] then P (·|s, a) is (Ξ, τ )-computable for τ = O(log nS + log nA ).

For a small , let H be the  horizon time, i.e.
ignoring rewards beyond time H does not affect
the value of any policy by more than . To obtain sample rewards, given initial state s0 and policy
πθ = π(·; θ), we ﬁrst compute the trajectory s0 , . . . , sH sampled from the Markov chain induced
by πθ . This requires H “calls” each to Mπ and MP . A further H + 1 calls to Mr are then required
to generate the rewards ρ0 through ρH . These calls require a total of 3H + 1 samples from Ξ. The
empirical estimates are computed as follows. Suppose, for 1 ≤ i ≤ n, (s(i)
0 , ~ξi ) are i.i.d. samples
generated from the joint distribution D × Ξ3H+1 . De ﬁne the empirical estimate of the value of the
policy π by

n
H
1
Xi=1
Xt=0
n
We omit the subscript M in ˆV when it is clear from the context. De ﬁne an -approximate maximizer
of ˆV to be a policy π 0 such that

γ tρt (s(i)
0 , θ, ~ξi ) .

ˆV H
M (πθ ) =

ˆV H
M (π 0 ) ≥ sup
π∈Π

ˆV H
M (π) −  .

1 In this case, the realizable dynamics (mapping from state to next state for a given policy class) is not
uniformly Lipschitz if policies allow unbounded actions. So previously known bounds [7] are not applicable
even in this simple setting.

Finally, we mention the de ﬁnitions of three standard combin atorial dimensions. Let X be some
space and consider classes G and F of {−1, +1} and real valued functions on X , respectively. Fix
a ﬁnite set X = {x1 , . . . , xn } ⊆ X . We say that G shatters X if for all bit vectors ~b ∈ {0, 1}n
there exists g ∈ G such that for all i, bi = 0 ⇒ g (xi ) = −1, bi = 1 ⇒ g (xi ) = +1. We say that
F shatters X if there exists ~r ∈ Rn such that, for all bit vectors ~b ∈ {0, 1}n , there exists f ∈ F
such that for all i, bi = 0 ⇒ f (xi ) < ri , bi = 1 ⇒ f (xi ) ≥ ri . We say that F -shatters X if
these exists ~r ∈ Rn such that, for all bit vectors ~b ∈ {0, 1}n, there exists f ∈ F such that for all i,
bi = 0 ⇒ f (xi ) ≤ ri − , bi = 1 ⇒ f (xi ) ≥ ri + . We then have the following de ﬁnitions,
VCdim(G ) = max{|X | : G shatters X } ,
Pdim(F ) = max{|X | : F shatters X } ,
fatF () = max{|X | : F -shatters X } .

3 Regret Bound for Parametric Policy Classes Computable in Bounded
Time

Theorem 1. Fix an MDP M, a policy class Π = {s 7→ π(s; θ) : θ ∈ Rd}, and an  > 0. Suppose
Assumption A holds. Then
n > O (cid:18) R2H dτ
(1 − γ ) (cid:19)
R
(1 − γ )2 2 log
ensures that E (cid:2)RegM,Π (πn )(cid:3) ≤ 3 + 0 , where πn is an 0 -approximate maximizer of ˆV and H =
log1/γ (2R/((1 − γ ))) is the /2 horizon time.
Proof. The proof consists of three steps: (1) Assumption A is used to get bounds on pseudodimen-
sion; (2) The pseudodimension bound is used to prove uniform convergence of empirical estimates
to true value functions; (3) Uniform convergence and the de ﬁ nition of 0 -approximate maximizer
gives the bound on expected regret.
ST E P 1 . Given initial state s0 , parameter θ and random numbers ξ1 through ξ3H+1 , we ﬁrst compute
the trajectory as follows. Recall that ΦM refers to the input-output map of a machine M.

st = ΦMP (st−1 , ΦMπ (θ, s, ξ2t−1 ), ξ2t ), 1 ≤ t ≤ H .
The rewards are then computed by

(2)

(3)

ρt = ΦMr (st , ξ2H+t+1 ), 0 ≤ t ≤ H .
The H -step discounted reward sum is computed as
H
Xt=0
γ tρt = ρ0 + γ (ρ1 + γ (ρ2 + . . . (pH−1 + γ ρH ) . . .)) .
De ﬁne the function class R = {(s0 , ~ξ ) 7→ PH
t=0 γ tρt (s0 , θ, ~ξ ) : θ ∈ Rd }, where we have explicitly
shown the dependence of ρt on s0 , θ and ~ξ . Let us count the number of arithmetic operations needed
to compute a function in this class. Using Assumption A, we see that steps (2) and (3) require
no more than 2τ H and τ (H + 1) operations respectively. Step (4) requires H multiplications and
H additions. This gives a total of 2τ H + τ (H + 1) + 2H ≤ 6τ H operations. Goldberg and
Jerrum [4] showed that the VC dimension of a function class can be bounded in terms of an upper
bound on the number of arithmetic operations it takes to compute the functions in the class. Since
the pseudodimension of R can be written as
Pdim(R) = VCdim{(s0 , ~ξ , c) 7→ sign(f (s0 , ~ξ ) − c) : f ∈ R, c ∈ R} ,
we get the following bound by [2, Thm. 8.4],

(4)

Pdim(R) ≤ 4d(6τ H + 3) .
ST E P 2 . Let V H (π) = PH
t=0 γ trt (π). For the choice of H stated in the theorem, we have for all π ,
|V H (π) − V (π)| ≤ /2. Therefore,
P n (∃π ∈ Π : | ˆV H (π) − V (π)| > ) ≤ P n (∃π ∈ Π : | ˆV H (π) − V H (π)| > /2) .

(6)

(5)

Functions in R are positive and bounded above by R0 = R/(1 − γ ). There are well-known bounds
for deviations of empirical estimates from true expectations for bounded function classes in terms
of the pseudodimension of the class (see, for example, Theorems 3 and 5 in [5]; also see Pollard’s
book [8]). Using a weak form of these results, we get
 (cid:19)2 Pdim(R)
P n (∃π ∈ Π : | ˆV H (π) − V H (π)| > ) ≤ 8 (cid:18) 32eR0
In order to ensure that P n (∃π ∈ Π : | ˆV H (π) − V H (π)| > /2) < δ , we need
 (cid:19)2 Pdim(R)
8 (cid:18) 64eR0
Using the bound (5) on Pdim(R), we get that
P n (cid:18) sup
> (cid:19) < δ ,
π∈Π (cid:12)(cid:12)(cid:12)
ˆV H (π) − V (π)(cid:12)(cid:12)(cid:12)
256R2
δ (cid:19) + 8d(6τ H + 3) log (cid:18) 64eR
(1 − γ ) (cid:19)(cid:19) .
(1 − γ )2 2 (cid:18)log (cid:18) 8
ST E P 3 . We now show that (7) implies E RegM,Π (πn ) ≤ Rδ/(1 − γ ) + (2 + 0 ). The theorem
them immediately follows by setting δ = (1 − γ )/R.

e−2n/256R02

provided

n >

e−2n/64R02

.

< δ ,

(7)

Suppose that for all π ∈ Π, | ˆV H (π) − V (π)| ≤ . This implies that for all π ∈ Π, V (π) ≤ ˆV H (π) +
. Since πn is an 0 -approximate maximizer of ˆV , we have for all π ∈ Π, ˆV H (π) ≤ ˆV H (πn ) + 0 .
Thus, for all π ∈ Π, V (π) ≤ ˆV H (πn ) +  + 0 . Taking the supremum over π ∈ Π and using the
fact that ˆV H (πn ) ≤ V (πn ) + , we get supπ∈Π V (π) ≤ V (πn ) + 2 + 0 , which is equivalent to
RegM,Π (πn ) ≤ 2 + 0 . Thus, if (7) holds then we have
P n (cid:0)RegM,Π (πn ) > 2 + 0 (cid:1) < δ .
Denoting the event {RegM,Π (πn ) > 2 + 0} by E , we have
E RegM,Π (πn ) = E RegM,Π (πn )1E + E RegM,Π(πn )1(¬E )
≤ Rδ/(1 − γ ) + (2 + 0 ) .
where we used the fact that regret is bounded above by R/(1 − γ ).

4 Two Policy Classes Having Bounded Combinatorial Dimensions

We will describe two policy classes for which we can prove that there are strong limitations on the
performance of any method (of choosing a policy out of a policy class) that has access only to em-
pirically observed rewards. Somewhat surprisingly, one can show this for policy classes which are
“simple ” in the sense that standard combinatorial dimensio ns of these classes are bounded. This
shows that sufﬁcient conditions for the success of simulati on based policy search (such as the as-
sumptions in [7] and in our Theorem 1) have to be necessarily stronger than boundedness of standard
combinatorial dimensions.
The ﬁrst example is a policy class F1 for which fatF1 () < ∞ for all  > 0. The second example is
a class F2 for which Pdim(F2 ) = 1. Since ﬁniteness of pseudodimension is a stronger conditio n,
the second example makes our point more forcefully than the ﬁ rst one. However, the ﬁrst example
is considerably less contrived than the second one.

Example 1

Let MD = (S, D, A, P (·|s, a), r, γ ) be an MDP where S = [−1, +1], D = some distribution on
[−1, +1], A = [−2, +2],
P (s0 |s, a) = 1 if s0 = max(−1, min(s + a, 1))), 0 otherwise ,

0.2

0

−0.2

−0.4

−0.6

−0.8

−1

)
x
(
T
f

−1

−0.5

0
x

0.5

1

Figure 1: Plot of the function fT with T = {0.2, 0.3, 0.6, 0.8}. Note that, for x > 0, fT (x) is 0 iff
x ∈ T . Also, fT (x) satisﬁes the Lipschitz condition (with constant 1) everywh ere except at 0.

r = deterministic reward that maps s to s, and γ = some ﬁxed discount factor in (0, 1).
For a function f : [−1, +1] 7→ [−1, +1], let πf denote the (deterministic) policy which takes
action f (s) − s in state s. Given a class F of functions, we de ﬁne an associated policy class
ΠF = {πf : f ∈ F }.
We now describe a speciﬁc function class F1 . Fix 1 > 0. Let T be an arbitrary ﬁnite subset of
(0, 1). Let δ(x) = (1 − |x|)+ be the “triangular spike ” function. Let
fT (x) = 
−1
−1 ≤ x < 0
0
x = 0
y∈T (cid:16) 1
1 /|T | (cid:17) − 1
|T | (cid:17) 0 < x ≤ 1
|T | δ (cid:16) x−y
max

There is a spike at each point in T and the tips of the spikes just touch the X -axis (see Figure 1).
Since −1 and 0 are ﬁxed points of FT (x), it is straightforward to verify that
T (x) = 
f 2

T for all n > 2. De ﬁne F1 = {fT : T ⊂ (1 , 1), |T | < ∞}. By construction,
Also, f n
T = f 2
functions in F1 have bounded total variation and so, fatF1 () is O(1/) (see, for example, [2, Chap.
11]). Moreover, fT (x) satisﬁes the Lipschitz condition everywhere (with constan t L = 1) except at
0. This is striking in the sense that the loss of the Lipschitz property at a single point allows us to
prove the following lower bound.
Theorem 2. Let gn range over functions from S n to F1 . Let D range over probability distributions
on S . Then,

−1 ≤ x < 0
x = 0
0 < x ≤ 1

−1
0
1(x∈T ) − 1

(8)

.

.

γ 2
1 − γ

inf
gn

sup
D

− 21 .

(πgn (s1 ,...,sn ) )i ≥
E(s1 ,...,sn )∼Dn hRegMD ,ΠF1
This says that for any method that maps random initial states s1 , . . . , sn to a policy in ΠF1 , there
is an initial state distribution such that the expected regret of the selected policy is at least γ 2/(1 −
γ ) − 21 . This is in sharp contrast to Theorem 1 where we could reduce, by using sufﬁciently
many samples, the expected regret down to any positive number given the ability to maximize the
empirical estimates ˆV .
Let us see how maximization of empirical estimates behaves in this case. Since fatF1 () < ∞
for all  > 0, the law of large numbers holds uniformly [1, Thm. 2.5] over the class F1 . The
transitions, policies and rewards here are all deterministic. The reward function is just the identity.
This means that the 1-step reward function family is just F1 . So the estimates of 1-step rewards are

still uniformly concentrated around their expected values. Since the contribution of rewards from
time step 2 onwards can be no more than γ 2 + γ 3 + . . . = γ 2/(1 − γ ), we can claim that the expected
regret of the ˆV maximizer πn behaves like
γ 2
E hRegM,ΠF1
(πn )i ≤
1 − γ
where en → 0. Thus the bound in Theorem 2 above is essentially tight.
Before we prove Theorem 2, we need the following lemma whose proof is given in the appendix
accompanying the paper.
Lemma 1. Fix an interval (a, b) and let T be the set of all its ﬁnite subsets. Let gn range over
functions from (a, b)n to T . Let D range over probability distributions on (a, b). Then,
EX∼D 1(X ∈T ) − E(X1 ,...,Xn )∼Dn E(X∼D)1(X ∈gn (X1 ,...,Xn ))(cid:19) ≥ 1 .
D (cid:18) sup
sup
T ∈T
Proof of Theorem 2. We will prove the inequality when D ranges over distributions on (0, 1) which,
obviously, implies the theorem.
Since, for all f ∈ F1 and n > 2, f n = f 2 , we have
opt(MD , ΠF1 ) − E(s1 ,...,sn )∼Dn VMD (πgn (s1 ,...,sn ) )
γ 2
Es∼D (cid:20)s + γ f (s) +
f 2 (s)(cid:21)
= sup
1 − γ
f ∈F1
− E(s1 ,...,sn )∼Dn (cid:20)Es∼D [s + γ gn(s1 , . . . , sn )(s) +
γ 2
f 2 (s)(cid:21)
Es∼D (cid:20)γ f (s) +
= sup
1 − γ
f ∈F1
γ 2
− E(s1 ,...,sn )∼Dn (cid:20)Es∼D [γ gn(s1 , . . . , sn )(s) +
gn (s1 , . . . , sn )2 (s)](cid:21)
1 − γ
For all f1 , f2 , |Ef1 − Ef2 | ≤ E|f1 − f2 | ≤ 1 . Therefore, we can get rid of the ﬁrst terms in both
sub-expressions above without changing the value by more than 2γ 1 .
γ 2
Es∼D (cid:20) γ 2
gn (s1 , . . . , sn )2 (s)](cid:21)
f 2 (s)(cid:21) − E(s1 ,...,sn )∼Dn (cid:20)Es∼D [
≥ sup
1 − γ
1 − γ
f ∈F1
− 2γ 1
1 − γ   sup
Es∼D (cid:2)f 2 (s) + 1(cid:3) − E(s1 ,...,sn )∼Dn Es∼D [gn (s1 , . . . , sn )2 (s) + 1]!
γ 2
f ∈F1
− 2γ 1
T (x) + 1 restricted to x ∈ (0, 1) is the same as 1(x∈T ) . Therefore,
From (8), we know that f 2
restricting D to probability measures on (0, 1) and applying Lemma 1, we get
γ 2
D (cid:0)opt(MD , ΠF1 ) − E(s1 ,...,sn )∼Dn VMD (πgn (s1 ,...,sn ) )(cid:1) ≥
sup
inf
1 − γ
gn
To ﬁnish the proof, we note that γ < 1 and, by de ﬁnition,
(πgn (s1 ,...,sn ) ) = opt(MD , ΠF1 ) − VMD (πgn (s1 ,...,sn ) ) .
RegMD ,ΠF1

gn (s1 , . . . , sn )2 (s)](cid:21)

γ 2
1 − γ

inf
gn

=

+ en

− 2γ 1 .

Example 2

We use the MDP of the previous section with a different policy class which we now describe. For
a real number x, y ∈ (0, 1) with binary expansions (choose the terminating representation for ratio-
nals) 0.b1 b2b3 . . . and 0.c1 c2 c3 . . ., de ﬁne
mix(x, y ) = 0.b1 c1 b2 c2 . . .
even(x) = 0.b2 b4 b6 . . .

stretch(x) = 0.b10b20b3 . . .
odd(x) = 0.b1b3 b5 . . .

.

fT (x) =

Some obvious identities are mix(x, y ) = stretch(x) + stretch(y )/2, odd(mix(x, y )) = x and
even(mix(x, y )) = y . Now ﬁx 2 > 0. Since, ﬁnite subsets of
(0, 1) and irrationals in (0, 2) have
the same cardinality, there exists a bijection h which maps every ﬁnite subset T of (0, 1) to some
irrational h(T ) ∈ (0, 2 ). For a ﬁnite subset T of (0, 1), de ﬁne

x = −1
0
1(odd(−x)∈h−1 (even(−x)) −1 < x < 0
x = 0
0
0 < x < 1
− mix(x, h(T ))

x = 1
1
It is easy to check that with this de ﬁnition, f 2
T (x) = 1(x∈T ) for x ∈ (0, 1). Finally, let F2 =
{fT : T ⊂ (0, 1), |T | < ∞}. To calculate the pseudodimension of this class, note that using the
identity mix(x, y ) = stretch(x) + stretch(y )/2, every function fT in the class can be written as
fT = f0 + ˜fT where f0 is a ﬁxed function (does not depend on T ) and ˜fT is given by
˜fT (x) = 
0
−1 ≤ x ≤ 0
− stretch(h(T ))/2 0 < x < 1
0
x = 1

Let H = { ˜fT : T ⊂ (0, 1), |T | < ∞}. Since Pdim(H + f0 ) = Pdim(H) for any class H and a
ﬁxed function f0 , we have Pdim(F2 ) = Pdim(H). As each function ˜fT (x) is constant on (0, 1)
and zero elsewhere, we cannot shatter even two points using H. Thus, Pdim(H) = 1.
Theorem 3. Let gn range over functions from S n to F2 . Let D range over probability distributions
on S . Then,

.

inf
gn

γ 2
1 − γ

sup
D

E(s1 ,...,sn )∼Dn hRegMD ,ΠF1
(πgn (s1 ,...,sn ) )i ≥
Sketch. Let us only check that the properties of F1 that allowed us to proceed with the proof of
Theorem 2 are also satisﬁed by F2 . First, for all f ∈ F2 and n > 2, f n = f 2 . Second, for all
f1 , f2 ∈ F2 and x ∈ [−1, +1], |f1 (x) − f2 (x)| ≤ 2/2. This is because fT1 and fT2 can differ
only for x ∈ (0, 1). For such an x, |fT1 (x) − fT2 (x)| = | mix(x, h(T1 ) − mix(x, h(T2 ))| =
T to (0, 1) is 1(x∈T ) .
| stretch(h(T1 )) − stretch(h(T2 ))|/2 ≤ 2/2. Third, the restriction of f 2

− 2 .

Acknowledgments

We acknowledge the support of DARPA under grants HR0011-04-1-0014 and FA8750-05-2-0249.

References

[1] Alon, N., Ben-David, S., Cesa-Bianchi, N. & Haussler, D. (1997) Scale-sensitive Dimensions, Uniform
Convergence, and Learnability. Journal of the ACM 44(4):615–631.

[2] Anthony, M. & Bartlett P.L. (1999) Neural Network Learning: Theoretical Foundations. Cambridge Uni-
versity Press.

[3] Blum, L., Cucker, F., Shub, M. & Smale, S. (1998) Complexity and Real Computation. Springer-Verlag.

[4] Goldberg, P.W. & Jerrum, M.R. (1995) Bounding the Vapnik-Chervonenkis Dimension of Concept Classes
Parameterized by Real Numbers. Machine Learning 18(2-3):131–148.

[5] Haussler, D. (1992) Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learn-
ing Applications. Information and Computation 100:78–150.

[6] Jain, R. & Varaiya, P. (2006) Simulation-based Uniform Value Function Estimates of Discounted and
Average-reward MDPs. SIAM Journal on Control and Optimization, to appear.

[7] Ng A.Y. & Jordan M.I. (2000) PEGASUS: A Policy Search Method for MDPs and POMDPs. In Proceed-
ings of the 16th Annual Conference on Uncertainty in Arti ﬁci al Intelligence, pp. 405–415. Morgan Kauffman
Publishers.

[8] Pollard D. (1990) Empirical Processes: Theory and Applications. NSF-CBMS Regional Conference Series
in Probability and Statistics, Volume 2.


Linearly-solvable Markov decision problems

Emanuel Todorov

Department of Cognitive Science
University of California San Diego
todorov@cogsci.ucsd.edu

Abstract

We introduce a class of MPDs which greatly simplify Reinforcement Learning.
They have discrete state spaces and continuous control spaces. The controls have
the effect of rescaling the transition probabilities of an underlying Markov chain.
A control cost penalizing KL divergence between controlled and uncontrolled
transition probabilities makes the minimization problem convex, and allows an-
alytical computation of the optimal controls given the optimal value function. An
exponential transformation of the optimal value function makes the minimized
Bellman equation linear. Apart from their theoretical signi(cid:2)cance, the new MDPs
enable ef(cid:2)cient approximations to traditional MDPs. Shortest path problems are
approximated to arbitrary precision with largest eigenvalue problems, yielding
an O (n) algorithm. Accurate approximations to generic MDPs are obtained via
continuous embedding reminiscent of LP relaxation in integer programming. Off-
policy learning of the optimal value function is possible without need for state-
action values; the new algorithm (Z-learning) outperforms Q-learning.

This work was supported by NSF grant ECS(cid:150)0524761.

1 Introduction

In recent years many hard problems have been transformed into easier problems that can be solved
ef(cid:2)ciently via linear methods [1] or convex optimization [2]. One area where these trends have not
yet had a signi(cid:2)cant impact is Reinforcement Learning. Indeed the discrete and unstructured nature
of traditional MDPs seems incompatible with simplifying features such as linearity and convexity.
This motivates the search for more tractable problem formulations. Here we construct the (cid:2)rst MDP
family where the minimization over the control space is convex and analytically tractable, and where
the Bellman equation can be exactly transformed into a linear equation. The new formalism enables
ef(cid:2)cient numerical methods which could not previously be applied in Reinforcement Learning. It
also yields accurate approximations to traditional MDPs.
Before introducing our new family of MDPs, we recall the standard formalism. Throughout the
paper S is a (cid:2)nite set of states, U (i) is a set of admissible controls at state i 2 S , ‘ (i; u) (cid:21) 0 is a cost
for being in state i and choosing control u 2 U (i), and P (u) is a stochastic matrix whose element
pij (u) is the transition probability from state i to state j under control u. We focus on problems
where a non-empty subset A (cid:18) S of states are absorbing and incur zero cost: pij (u) = (cid:14) j
i and
‘ (i; u) = 0 whenever i 2 A. Results for other formulations will be summarized later. If A can be
reached with non-zero probability in a (cid:2)nite number of steps from any state, then the undiscounted
in(cid:2)nite-horizon optimal value function is (cid:2)nite and is the unique solution [3] to the Bellman equation
pij (u) v (j )o
u2U (i) n‘ (i; u) + Xj
(1)
v (i) = min
For generic MDPs this equation is about as far as one can get analytically.

2 A class of more tractable MDPs

In our new class of MDPs the control u 2 RjS j is a real-valued vector with dimensionality equal to
the number of discrete states. The elements uj of u have the effect of directly modifying the tran-
sition probabilities of an uncontrolled Markov chain. In particular, given an uncontrolled transition
probability matrix P with elements pij , we de(cid:2)ne the controlled transition probabilities as

pij (u) log

pij (u) = pij exp (uj )
Note that P (0) = P . In some sense this is the most general notion of "control" one can imagine (cid:150)
we are allowing the controller to rescale the underlying transition probabilities in any way it wishes.
However there are two constraints implicit in (2). First, pij = 0 implies pij (u) = 0. In this case uj
has no effect and so we set it to 0 for concreteness. Second, P (u) must have row-sums equal to 1.
Thus the admissible controls are
U (i) = nu 2 RjS j ; Xj
pij exp (uj ) = 1; pij = 0 =) uj = 0o
Real-valued controls make it possible to de(cid:2)ne a natural control cost. Since the control vector
acts directly on the transition probabilities, it makes sense to measure its magnitude in terms of
the difference between the controlled and uncontrolled transition probabilities. Differences between
probability distributions are most naturally measured using KL divergence, suggesting the following
de(cid:2)nition. Let pi (u) denote the i-th row-vector of the matrix P (u), that is, the vector of transition
probabilities from state i to all other states under control u. The control cost is de(cid:2)ned as
r (i; u) = KL (pi (u) jjpi (0)) = Xj :pij 6=0
pij (u)
pij (0)
From the properties of KL divergence it follows that r (i; u) (cid:21) 0, and r (i; u) = 0 iff u = 0.
Substituting (2) in (4) and simplifying, the control cost becomes
r (i; u) = Xj
This has an interesting interpretation. The Markov chain likes to behave according to P but can
be paid to behave according to P (u). Before each transition the controller speci(cid:2)es the price uj it
is willing to pay (or collect, if uj < 0) for every possible next state j . When the actual transition
occurs, say to state k , the controller pays the price uk it promised. Then r (i; u) is the price the
controller expects to pay before observing the transition.
Coming back to the MDP construction, we allow an arbitrary state cost q (i) (cid:21) 0 in addition to the
above control cost:
(6)
‘ (i; u) = q (i) + r (i; u)
We require q (i) = 0 for absorbing states i 2 A so that the process can continue inde(cid:2)nitely without
incurring extra costs. Substituting (5, 6) in (1), the Bellman equation for our MDP is
u2U (i) nq (i) + Xj
pij exp (uj ) (uj + v (j ))o
v (i) = min
We can now exploit the bene(cid:2)ts of this unusual construction. The minimization in (7) subject to the
constraint (3) can be performed in closed form using Lagrange multipliers, as follows. For each i
de(cid:2)ne the Lagrangian
pij exp (uj ) (cid:0) 1(cid:17)
pij exp (uj ) (uj + v (j )) + (cid:21)i (cid:16)Xj
L (u; (cid:21)i ) = Xj
The necessary condition for an extremum with respect to uj is
@L
= pij exp (uj ) (uj + v (j ) + (cid:21)i + 1)
@uj
When pij 6= 0 the only solution is

pij (u) uj

(8)

(9)

(4)

(5)

0 =

(2)

(3)

(7)

(10)

u(cid:3)j (i) = (cid:0)v (j ) (cid:0) (cid:21)i (cid:0) 1

(11)

Taking another derivative yields
@uj @uj (cid:12)(cid:12)(cid:12)(cid:12)uj =u(cid:3)j (i)
@ 2L
= pij exp (cid:0)u(cid:3)j (i)(cid:1) > 0
and therefore (10) is a minimum. The Lagrange multiplier (cid:21)i can be found by applying the constraint
(3) to the optimal control (10). The result is
(cid:21)i = log (cid:16)Xj
pij exp ((cid:0)v (j ))(cid:17) (cid:0) 1
and therefore the optimal control law is
u(cid:3)j (i) = (cid:0)v (j ) (cid:0) log (cid:16)Xk
pik exp ((cid:0)v (k))(cid:17)
Thus we have expressed the optimal control law in closed form given the optimal value function.
Note that the only in(cid:3)uence of the current state i is through the second term, which serves to nor-
malize the transition probability distribution pi (u(cid:3) ) and is identical for all next states j . Thus the
optimal controller is a high-level controller: it tells the Markov chain to go to good states without
specifying how to get there. The details of the trajectory emerge from the interaction of this con-
troller and the uncontrolled stochastic dynamics. In particular, the optimally-controlled transition
probabilities are

(13)

(12)

(15)

(14)

pij (u(cid:3) (i)) =

pij exp ((cid:0)v (j ))
Pk pik exp ((cid:0)v (k))
These probabilities are proportional to the product of two terms: the uncontrolled transition prob-
abilities pij which do not depend on the costs or values, and the (exponentiated) next-state values
v (j ) which do not depend on the current state. In the special case pij = consti the transition prob-
abilities (14) correspond to a Gibbs distribution where the optimal value function plays the role of
an energy function.
Substituting the optimal control (13) in the Bellman equation (7) and dropping the min operator,
v (i) = q (i) + Xj
pij (u(cid:3) (i)) (cid:0)u(cid:3)j (i) + v (j )(cid:1)
= q (i) + Xj
pij (u(cid:3) (i)) ((cid:0)(cid:21)i (cid:0) 1)
= q (i) (cid:0) (cid:21)i (cid:0) 1
pij exp ((cid:0)v (j ))(cid:17)
= q (i) (cid:0) log (cid:16)Xj
Rearranging terms and exponentiating both sides of (15) yields
exp ((cid:0)v (i)) = exp ((cid:0)q (i)) Xj
We now introduce the exponential transformation
z (i) = exp ((cid:0)v (i))
which makes the minimized Bellman equation linear:
z (i) = exp ((cid:0)q (i)) Xj
De(cid:2)ning the vector z with elements z (i), and the diagonal matrix G with elements exp ((cid:0)q (i))
along its main diagonal, (18) becomes
(19)
z = GP z

pij exp ((cid:0)v (j ))

pij z (j )

(16)

(17)

(18)

Thus our class of optimal control problems has been reduced to a linear eigenvalue problem.

2.1

Iterative solution and convergence analysis

From (19) it follows that z is an eigenvector of GP with eigenvalue 1. Furthermore z (i) > 0 for
all i 2 S and z (i) = 1 for i 2 A. Is there a vector z with these properties and is it unique? The
answer to both questions is af(cid:2)rmative, because the Bellman equation has a unique solution and v
is a solution to the Bellman equation iff z = exp ((cid:0)v) is an admissible solution to (19). The only
remaining question then is how to (cid:2)nd the unique solution z. The obvious iterative method is
(20)
z0 = 1
zk+1 = GP zk ;
This iteration always converges to the unique solution, for the following reasons. A stochastic matrix
P has spectral radius 1. Multiplication by G scales down some of the rows of P , therefore GP has
spectral radius at most 1. But we are guaranteed than an eigenvector z with eigenvalue 1 exists,
therefore GP has spectral radius 1 and z is a largest eigenvector. Iteration (20) is equivalent to
the power method (without the rescaling which is unnecessary here) so it converges to a largest
eigenvector. The additional constraints on z are clearly satis(cid:2)ed at all stages of the iteration. In
particular, for i 2 A the i-th row of GP has elements (cid:14) j
i , and so the i-th element of zk remains equal
to 1 for all k .
We now analyze the rate of convergence. Let m = jAj and n = jS j. The states can be permuted so
that GP is in canonical form:
GP = (cid:20) T1 T2
I (cid:21)
(21)
0
where the absorbing states are last, T1 is (n (cid:0) m) by (n (cid:0) m), and T2 is (n (cid:0) m) by m. The reason
we have the identity matrix in the lower-right corner, despite multiplication by G, is that q (i) = 0
for i 2 A. From (21) we have
(cid:21) = (cid:20) T k
= (cid:20) T k
(cid:21)
1 + (cid:1) (cid:1) (cid:1) + T1 + I(cid:1) T2
(cid:0)T k(cid:0)1
1 (cid:1) (I (cid:0) T1 )(cid:0)1 T2
(cid:0)I (cid:0) T k
(cid:0)GP (cid:1)k
1
1
0
I
0
I
A stochastic matrix P with m absorbing states has m eigenvalues 1, and all other eigenvalues are
smaller than 1 in absolute value. Since the diagonal elements of G are no greater than 1, all eigen-
values of T1 are smaller than 1 and so limk!1 T k
1 = 0. Therefore iteration (20) converges ex-
ponentially as (cid:13) k where (cid:13) < 1 is the largest eigenvalue of T1 . Faster convergence is obtained for
smaller (cid:13) . The factors that can make (cid:13) small are: (i) large state costs q (i) resulting in small terms
exp ((cid:0)q (i)) along the diagonal of G; (ii) small transition probabilities among non-absorbing states
(and large transition probabilities from non-absorbing to absorbing states). Convergence is inde-
pendent of problem size because (cid:13) has no reason to increase as the dimensionality of T1 increases.
Indeed numerical simulations on randomly generated MDPs have shown that problem size does not
systematically affect the number of iterations needed to reach a given convergence criterion. Thus
the average running time scales linearly with the number of non-zero elements in P .

(22)

2.2 Alternative problem formulations

While the focus of this paper is on in(cid:2)nite-horizon total-cost problems with absorbing states, we
have obtained similar results for all other problem formulations commonly used in Reinforcement
Learning. Here we summarize these results. In (cid:2)nite-horizon problems equation (19) becomes

(23)
z (t) = G (t) P (t) z (t + 1)
where z (t(cid:12)nal ) is initialized from a given (cid:2)nal cost function. In in(cid:2)nite-horizon average-cost-per-
stage problems equation (19) becomes

(24)
(cid:13) z = GP z
where (cid:13) is the largest eigenvalue of GP , z is a differential value function, and the average cost-per-
stage turns out to be (cid:0) log ((cid:13) ). In in(cid:2)nite-horizon discounted-cost problems equation (19) becomes
(25)
z = GP z(cid:11)
where (cid:11) < 1 is the discount factor and z(cid:11) is de(cid:2)ned element-wise. Even though the latter equation
is nonlinear, we have observed that the analog of iteration (20) still converges rapidly.

(26)

3 Shortest paths as an eigenvalue problem
Suppose the state space S of our MDP corresponds to the vertex set of a directed graph, and let D be
the graph adjacency matrix whose element dij indicates the presence (dij = 1) or absence (dij = 0)
of a directed edge from vertex i to vertex j . Let A (cid:18) S be a non-empty set of destination vertices.
Our goal is to (cid:2)nd the length s (i) of the shortest path from every i 2 S to some vertex in A. For
i 2 A we have s (i) = 0 and dij = (cid:14) j
i .
We now show how the shortest path lengths s (i) can be obtained from our MDP. De(cid:2)ne the elements
of the stochastic matrix P as
dijPk dik
pij =
corresponding to a random walk on the graph. Next choose (cid:26) > 0 and de(cid:2)ne the state costs
(27)
q(cid:26) (i) = 0 when i 2 A
q(cid:26) (i) = (cid:26) when i =2 A,
This cost model means that we pay a price (cid:26) whenever the current state is not in A. Let v(cid:26) (i) denote
the optimal value function for the MDP de(cid:2)ned by (26, 27). If the control costs were 0 then the
shortest paths would simply be s (i) = 1
(cid:26) v(cid:26) (i). Here the control costs are not 0, however they are
bounded. This can be shown using
(28)
pij (u) = pij exp (uj ) (cid:20) 1
which implies that for pij 6= 0 we have uj (cid:20) (cid:0) log (cid:0)pij (cid:1). Since r (i; u) is a convex combination of
the elements of u, the following bound holds:
r (i; u) (cid:20) maxj (uj ) (cid:20) (cid:0) log (cid:16)minj :pij 6=0 (cid:0)pij (cid:1)(cid:17)
(29)
The control costs are bounded and we are free to choose (cid:26) arbitrarily large, so we can make the state
costs dominate the optimal value function. This yields the following result:
v(cid:26) (i)
(30)
s (i) = lim
(cid:26)
(cid:26)!1
Thus we have reduced the shortest path problem to an eigenvalue problem. In spectral graph theory
many problems have previously been related to eigenvalues of the graph Laplacian [4], but the
shortest path problem was not among them until now. Currently the most widely used algorithm is
Dijkstra’s algorithm. In sparse graphs its running time is O (n log (n)). In contrast, algorithms for
(cid:2)nding largest eigenpairs have running time O (n) for sparse matrices.
Of course (30) involves a limit and so we cannot obtain the exact shortest paths by solving a single
eigenvalue problem. However we can obtain a good approximation by setting (cid:26) large enough (cid:150) but
not too large because exp ((cid:0)(cid:26)) may become numerically indistinguishable from 0. Fig 1 illustrates
the solution obtained from (30) and rounded down to the nearest integer, for (cid:26) = 1 in 1A and (cid:26) = 50
in 1B. Transitions are allowed to all neighbors. The result in 1B matches the exact shortest paths.
Although the solution for (cid:26) = 1 is numerically larger, it is basically a scaled-up version of the
correct solution. Indeed the R2 between the two solutions before rounding was 0:997.

022222232533202123254418242566162626891428281011111430301212131331310110101011119910112281011337111144612125556131366661414Fig 1AFig 1B4 Approximating discrete MDPs via continuous embedding

(32)

In the previous section we replaced the shortest path problem with a continuous MDP and obtained
an excellent approximation. Here we obtain approximations of similar quality in more general set-
tings, using an approach reminiscent of LP-relaxation in integer programming. As in LP-relaxation,
theoretical results are hard to derive but empirically the method works well.
We construct an embedding which associates the controls in the discrete MDP with speci(cid:2)c control
vectors of a continuous MDP, making sure that for these control vectors the continuous MDP has the
same costs and transition probabilities as the discrete MDP. This turns out to be possible under mild
and reasonable assumptions, as follows. Consider a discrete MDP with transition probabilities and
costs denoted ep and e‘. De(cid:2)ne the matrix B (i) of all controlled transition probabilities from state i.
This matrix has elements
(31)
baj (i) = epij (a) ;
a 2 U (i)
We need two assumptions to guarantee the existence of an exact embedding: for all i 2 S the matrix
B (i) must have full row-rank, and if any element of B (i) is 0 then the entire column must be 0. If
the latter assumption does not hold, we can replace the problematic 0 elements of B (i) with a small
(cid:15) and renormalize. Let N (i) denote the set of possible next states, i.e. states j for which epij (a) > 0
for any/all a 2 U (i). Remove the zero-columns of B (i) and restrict j 2 N (i).
The (cid:2)rst step in the construction is to compute the real-valued control vectors ua corresponding to
the discrete actions a. This is accomplished by matching the transition probabilities of the discrete
and continuous MDPs:
j (cid:1) = epij (a) ;
pij exp (cid:0)ua
8 i 2 S ; j 2 N (i) ; a 2 U (i)
These constraints are satis(cid:2)ed iff the elements of the vector ua are
j = log ( epij (a)) (cid:0) log (cid:0)pij (cid:1)
(33)
ua
The second step is to compute the uncontrolled transition probabilities pij and state costs q (i) in the
continuous MDP so as to match the costs in the discrete MDP. This yields the set of constraints
q (i) + r (i; ua ) = e‘ (i; a) ;
8 i 2 S ; a 2 U (i)
For the control vector given by (33) the KL-divergence cost is
r (i; ua ) = Xj
j = h (i; a) (cid:0) Xj epij (a) log (cid:0)pij (cid:1)
pij exp (cid:0)ua
j (cid:1) ua
where h (i; a) is the entropy of the transition probability distribution in the discrete MDP:
h (i; a) = Xj epij (a) log ( epij (a))
The constraints (34) are then equivalent to
q (i) (cid:0) Xj
baj (i) log (cid:0)pij (cid:1) = e‘ (i; a) (cid:0) h (i; a)
(37)
De(cid:2)ne the vector y (i) with elements e‘ (i; a) (cid:0) h (i; a), and the vector x (i) with elements log (cid:0)pij (cid:1).
The dimensionality of y (i) is jU (i)j while the dimensionality of x (i) is jN (i)j (cid:21) jU (i)j. The latter
inequality follows from the assumption that B (i) has full row-rank. Suppressing the dependence on
the current state i, the constraints (34) can be written in matrix notation as
(38)
q1 (cid:0) Bx = y
Since the probabilities pij must sum up to 1, the vector x must satisfy the additional constraint
Xj
(39)
exp (xj ) = 1
We are given B ; y and need to compute q ; x satisfying (38, 39). Let bx be any vector such that
(cid:0)B bx = y, for example bx = (cid:0)B yy where y denotes the Moore-Penrose pseudoinverse. Since B is
a stochastic matrix we have B1 = 1, and so
(40)
q1 (cid:0) B (bx + q1) = (cid:0)B bx = y

(34)

(35)

(36)

(41)

Therefore x = bx + q1 satis(cid:2)es (38) for all q , and we can adjust q to also satisfy (39), namely
q = (cid:0) log (cid:16)Xj
exp (bxj )(cid:17)
This completes the embedding. If the above q turns out to be negative, we can either choose another
bx by adding an element from the null-space of B , or scale all costs e‘ (i; a) by a positive constant.
Such scaling does not affect the optimal control law for the discrete MDP, but it makes the elements
of (cid:0)B yy more negative and thus q becomes more positive.
We now illustrate this construction with the example in Fig 2. The grid world has a number of
obstacles (black squares) and two absorbing states (white stars). The possible next states are the
immediate neighbors including the current state. Thus jN (i)j is at most 9. The discrete MDP has
jN (i)j (cid:0) 1 actions corresponding to stochastic transitions to each of the neighbors. For each action,
the transition probability to the "desired" state is 0:8 and the remaining 0:2 is equally distributed
among the other states. The costs e‘ (i; a) are random numbers between 1 and 10 (cid:150) which is why
the optimal value function shown in grayscale appears irregular. Fig 2A shows the optimal value
function for the discrete MDP. Fig 2B shows the optimal value function for the corresponding con-
tinuous MDP. The scatterplot in Fig 2C shows the optimal values in the discrete and continuous
MDP (each dot is a state). The values in the continuous MDP are numerically smaller (cid:150) which is
to be expected since the control space is larger. Nevertheless, the correlation between the optimal
values in the discrete and continuous MDPs is excellent. We have observed similar performance in
a number of randomly-generated problems.

5 Z-learning

So far we assumed that a model of the continuous MDP is available. We now turn to stochastic
approximations of the optimal value function which can be used when a model is not available. All
we have access to are samples (ik ; jk ; qk ) where ik is the current state, jk is the next state, qk is the
state cost incurred at ik , and k is the sample number. Equation (18) can be rewritten as
z (i) = exp ((cid:0)q (i)) Xj
pij z (j ) = exp ((cid:0)q (i)) EP [z (j )]
This suggests an obvious stochastic approximation bz to the function z , namely
(43)
bz (ik )   (1 (cid:0) (cid:11)k ) bz (ik ) + (cid:11)k exp ((cid:0)qk ) bz (jk )
where the sequence of learning rates (cid:11)k is appropriately decreased as k increases. The approxima-
tion to v (i) is simply (cid:0) log (bz (i)). We will call this algorithm Z-learning.
Let us now compare (43) to the Q-learning algorithm applicable to discrete MDPs. Here we have
samples (ik ; jk ; ‘k ; uk ). The difference is that ‘k is now a total cost rather than a state cost, and we
have a control uk generated by some control policy. The update equation for Q-learning is
u02U (jk ) (cid:16)‘k + bQ (jk ; u0 )(cid:17)
bQ (ik ; uk )   (1 (cid:0) (cid:11)k ) bQ (ik ; uk ) + (cid:11)k min

(44)

(42)

0204060010203040R2 = 0.986value in discrete MDP****Fig 2AFig 2BFig 2CTo compare the two algorithms, we (cid:2)rst constructed continuous MDPs with q (i) = 1 and transitions
to the immediate neighbors in the grid worlds shown in Fig 3. For each state we found the optimal
transition probabilities (14). We then constructed a discrete MDP which had one action (per state)
that caused the same transition probabilities, and the corresponding cost was the same as in the
continuous MDP. We then added jN (i)j (cid:0) 1 other actions by permuting the transition probabilities.
Thus the discrete and continuous MDPs were guaranteed to have identical optimal value functions.
Note that the goal here is no longer to approximate discrete with continuous MDPs, but to construct
pairs of problems with identical solutions allowing fair comparison of Z-learning and Q-learning.
We run both algorithms with the same random policy. The learning rates decayed as (cid:11)k =
c= (c + t (k)) where the constant c was optimized separately for each algorithm and t (k) is the
run to which sample k belongs. When the MDP reaches an absorbing state a new run is started from
a random initial state. The approximation error plotted in Fig 3 is de(cid:2)ned as
maxi jv (i) (cid:0) bv (i)j
maxi v (i)
and is computed at the end of each run. For small problems (Fig 3A) the two algorithms had identical
convergence, however for larger problems (Fig 3B) the new Z-learning algorithm was clearly faster.
This is not surprising: even though Z-learning is as model-free as Q-learning, it bene(cid:2)ts from the
analytical developments in this paper and in particular it does not need a maximization operator or
state-action values. The performance of Q-learning can be improved by using a non-random (say (cid:15)-
greedy) policy. If we combine Z-learning with importance sampling, the performance of Z-learning
can also be improved by using such a policy.

(45)

6 Summary

We introduced a new class of MDPs which have a number of remarkable properties, can be solved
ef(cid:2)ciently, and yield accurate approximations to traditional MDPs. In general, no single approach is
likely to be a magic wand which simpli(cid:2)es all optimal control problems. Nevertheless the results so
far are very encouraging. While the limitations remain to be clari(cid:2)ed, our approach appears to have
great potential and should be thoroughly investigated.

References
[1] B. Scholkopf and A. Smola, Learning with kernels. MIT Press (2002)
[2] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press (2004)
[3] D. Bertsekas, Dynamic programming and optimal control (2nd ed). Athena Scienti(cid:2)c (2000)
[4] F. Chung, Spectral graph theory. CMBS Regional Conference Series in Mathematics (1997)

0123x 10401*Z-learningQ-learning05101501**ZQNumber of state transitionsFig 3AFig 3B
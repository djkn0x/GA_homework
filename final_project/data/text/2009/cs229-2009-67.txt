Learning to Grasp Objects: A Novel Approach for Localizing Objects Using
Depth Based Segmentation

Deepak Rao, Arda Kara, Serena Yeung
(Under the guidance of Quoc V. Le)
Stanford University

Abstract

We consider the problem of grasping novel objects with a
robotic arm. A recent successful technique applies machine
learning to identify a point in an image corresponding to
the most likely location at which to grasp an object. An-
other approach extends their method to accomodate grasps
with multiple contact points. This paper proposes a novel
approach that tries to ﬁnd graspable points in an object af-
ter localizing it. We present a depth based segmentation
scheme for object localization and discuss how depth infor-
mation can be combined with visual imagery to improve the
performance of both segmentation and object localization.

1. Introduction
The problems of image segmentation and object local-
ization remain great challanges for computer vision, hav-
ing received continous attention since the birth of the ﬁeld.
The last few years have seen considerable progress in eigen-
vector based methods of image segmentation. These meth-
ods are too slow to be practical for many applications.
The state of the art approach for segmenting an im-
age into regions uses graph based representation of the
image[2]. Although, the method works better than its coun-
terparts, it has some limitations. Since the algorithm is
based entirely on pixel intensities it works poorly in scenes
having shadows or more than one light source.
The task of object localization involves drawing a tight
bounding box around all instances of an object class within
the image. This paper aims to highlight some of the draw-
backs of using color based segmentation for localizing an
object. Figure 1 shows an example of an object composed
of multiple colors. Performing color based segmentation
on the object results in different segments all belonging to
the same object. Taking into account these limitations and
drawbacks we propose a segmentation framework for ob-
ject localization that combines depth information with vi-
sual imagery (color/intensity). The next section gives a de-

Figure 1: Example of an object composed of muliple colors

tailed comparison between the two approaches.
The intuition behind using the depth data for segmenta-
tion came from [4] in which Quigley et al used high resolu-
tion 3D data for improving object detection. We conducted
experiments to prove that localizing objects after perform-
ing our proposed model of segmentation works better than
localization performed after segmentation using [2]. A de-
tailed explanation of our approach along with the experi-
mental results have been given in the following sections.

2. Segmentation
Our system makes use of both depth and colored image
data. To capture depth, we used an active triangulation sen-
sor [4]. An important feature of this sensor is that it gives
a very detailed depth data (also called depth map or point
cloud). Since color is an important attribute for segmenta-
tion, we added a functionality in ROS [6] that enabled us to
capture 3 channel colored images. Figure 2 gives an idea of
tha data captured by our robot.
Graph based image segmentation [2] tries to create a
graph based representataion of an image and ﬁnds the ev-
idence of a boundary between regions based on L2 -Norm
between the intensity of pixels.
||I (pi ) − I (pj )||2 > τ
(1)
where I (pi ) ∈ R3 is the intensity of the pixel pi and τ is
the threshold function.
If the L2 Norm between the intensities is greater than

Figure 2: Image and depth data captured by our robot.

the threshold function then the pixels are considered to be
in different regions.
As discussed earlier, segmentating an image truly on the
basis of color is not the correct approach. The advantage of
depth data allowed us to extend the approach in [2]. We
stick with the same scheme of representing an image as
a graph but updated the metric for ﬁnding boundaries be-
tween regions to include the depth data.
||W T ∗ (F (pi ) − F (pj ))||2 > τ
(2)
where F (pi ) ∈ R4 is the intensity of the pixel pi hav-
ing an extra dimension of depth value corresponding to that
location in 3D space, W ∈ R4 is the weight vector assign-
ing weights to different elements of F and τ is the threshold
function.
The intuition behind including weights in the equation
was to rank the elements in order of their importance. This
approach allowed us to give greater weight to the depth
value and smaller but equal weights to the intensities. An-
other approach could have been to learn the weights that
give the best segmentation but due the comparitively small
size of features, we decided to hand tune the weights to pro-
duce good results.

2.1. Comparison
Figure 3 shows the comparison between the two segmen-
tation approaches. It can be seen in Figure 3a that athough
the method proposed in [2] works well to segment the scene,
it is not robust to small color changes. This results in a large
number of artefacts on the table and the wall which ide-
ally should have been a part of the single parent segment.
The limitations of segmenting an image only on the basis of
color can be cleary seen in this example. Another observa-
tion that made us to use a combination of depth and color
for segmentation was the inability of the base algorithm to
handle shadows. All these factors contributed to our hy-
pothesis that segmentation based purely on color would not
be ideal for object localization.
In order to prove our hypothesis we decided to localize
objects using both the approaches and compare their scores.
Object localization involves drawing a tight bounding box

(a) Regular Segmentation

(b) Proposed Segmentation

Figure 3: Comparison between the two approaches.

around an object. Over the years, the complexity of the
problem has led to various approaches and different algo-
rithms to solve the same. This made it difﬁcult to choose
a standard recognized scoring system to compare our ap-
proaches. All these constraints led us to make our own
intuitive scoring scheme that was able to compare the ap-
proaches pretty well.
We took 200 images from the robot having different ob-
jects at varied positions to compare our approaches. This
was followed by manually labelling the objects using the
Image Sequence Labeler in Stair Vision Library [5]. The
human label for the object was considered to be the ground
truth upon which our scoring scheme is based. Once the la-
belling was done we ran both the segmentation algorithms
on the dataset and made bounding boxes around all the ob-
tained segments. Figure 4 gives an illustration of the same.
The green bounding box drawn around the object was the
ground truth label.
The approach while designing the scoring framework
was based on ﬁnding the individual scores for every image
and then use those scores to determine the ﬁnal score of the
algorithm.

Figure 4: Image obtained after drawing bounding boxes
around all the segments.

Figure 5: Performance of the two methods.

The scoring scheme can be broken into the following
parts:

Ai,j = F (Si,j , Gi )
where Ai,j is the area of overlap between obtained seg-
ments Si,j and ground truth Gi .

Θi =

ωi =

,

)

exp(ζ ∗ Ri,j )

Ai,j
Ri,j = min( Ai,j
Agi
Asi,j
where Asi,j is the area of segment Si,j and Agi is the
ground truth area of Gi .
NiX
Ri,j exp(ζ ∗ Ri,j )
NiX
j
j
where Ni is the number of segments in image i and ζ is
PM
some constant.
i
M
where M is the number of images in the dataset.
The scoring scheme to some extent can be consid-
ered as a weighted average of individual scores where the
weights are exponential. The intuition behind exponential
weights came from the idea that the segments similar to
the groundtruth would have a higher value of R and hence
should be given more weight.
After designing the scoring scheme we plotted a graph
showing the comparison of scores between the two ap-
proaches at various thresholds. As portrayed in Figure 5
our algorithm outperforms the base algorithm at almost ev-
ery threshold. The scores are relatively smaller at low and

S core =

Θi
ωi

(3)

high thresholds because of over and under segmenation re-
spectively. This proved our hypothesis that segmentation
based only on color is not ideal for object localization.
After observing the graph we decided to change Equa-
tion 2 to include the threshold that gives the optimal perfor-
mance.

τo = arg maxτ T Φ(S core, τ )

||W T ∗ (F (pi ) − F (pj ))||2 > τo

(4)

3. Classiﬁer Design and Feature Selection
In this section we brieﬂy touch upon the features that
were used by our classiﬁer to localize objects. Before de-
signing the classiﬁer for object localization we needed to
label segments of all the images as positive or negative.
The bounding boxes of all the segments were assigned pos-
itive or negative labels based on their overlap ratio with the
groundtruth.
In our proposed framework, we consider features from
visual and depth data. A brief description of all the features
and the intuition behind choosing them is elucidated in the
following sections

3.1. Bag of features
The features were chosen such that they give a true repre-
sentation of the bounding box. The width, height, area and
aspect ratio are a measure of the geometry of the bounding
box while distance from left and right edge gives an idea
about its position. Historically, researchers have avoided the
use of color based features due to computational and philo-
sophical reasons. The computational trade-off is obvious,
and the philosophical reason being that humans can perform
these task of localization without color information as well.
Since, the proposed segmentation framework already uses
color data we decided to use some color features as well.

(a) Area

(b) Height

(c) Width

(d) Aspect ratio

(e) Distance from the left edge

(f) Distance from the right edge

(g) Variance of L (Lightness)

(h) Variance of a

(i) Variance of b

(j) Variance of depth

Figure 6: Feature vector plots for all the features.

In order to make the classifer robust to lighting changes we
converted the color space from RGB to Lab. Instead of us-
ing raw color values or mean as features we decided to go
with variance. Variance of L, a and b within the bounding
box were chosen to be the color features. To complete the
feature vector we needed a depth feature and hence decided
to go with variance of depth within the bounding box. The
intuition behind chosing variance came from the fact that it
is a higher degree feature and probably a better indicator of
the state of the object within the bounding box.

3.2. Feature selection

After selecting the features explained in previous sec-
tion, we trained a SVM model to classify bounding boxes
as positive or negative. Surprisingly, the SVM failed to
converge. To get a better understanding of the reason we
created a feature vector plot for all the features (see Figure
6 ). There were some interesting insights after studying the
feature plots which in turn served as a motivation for our
next step. We decided to perform forward search to ﬁnd the
optimal subset of features. As expected, the best feature
subset returned by forward search was:

{width, height, area, variance
a, variance
of
b, variance

of

of L, variance
depth}
of

It can be seen from Figure 6 that the feature plots of dis-
tance from both edges and aspect ratio are randomly dis-
tributed and were the main reason for making the data in-
separable. Hence, these features were omitted from the fea-
ture vector and SVM model was trained using the remaining
features. Table 1 shows the rank of different features.

3.3. Grasping

The localization model we proposed takes a new test
scene and ﬁnds all bounding boxes that have a high prob-
ability of containing an object. This is followed by calcu-
lating centroids of the bounding boxes in 3D space by map-
ping them with the point cloud. To test our localization ap-
proach we considered the centroid to be best grasping point
and tried to grasp the object at that point.

4. Experimental Results

We consider two sets of experiments. The ﬁrst set of
experiments is performed ofﬂine to the test the accuracy of
the classiﬁer for localizing objects.
In the second set we
compare our method with [1] when grasping a novel object.
We perform all experiments with the Barrett hand having
three ﬁngers.

Rank Features
Variance of a
1
Variance of b
2
Variance of depth
3
4
Area
Variance of L
5
Width
6
7
Height

Table 1: Feature Ranks

Figure 8: Accuracy versus number of folds of the SVM
model for object localization.

4.1. Ofﬂine test
Our dataset consists of 200 images and their corre-
spondig depth data. The dataset consists of different objects
at varied positions. It is further divided into 353 positive
segments and 4216 negative segments. This test is totally
ofﬂine, i.e. without robotic execution. The goal of the test
is to determine the accuracy of the classiﬁer for object local-
ization. We tried to test the accuracy by performing k fold
cross validation on the dataset. Figure 8 shows the results
of the test. The accuracy increases as we increase the num-
ber of folds (i.e. training data) and becomes constant after
some time. This shows that the classiﬁer needs only a cer-
tain amount of data for training after which the improvemet
in accuracy becomes negligible.

4.2. Grasping novel objects
Figure 9 shows the STAIR2 robot grasping a nerf gun. In
this section we try to compare our grasping results with [1].
The comparison is not a totally fair because our grasping ap-
proach remains static and is independent of the size,shape
and orientation of the object. Still, there were some interest-
ing insights for objects that dont necessarily have an ideal
grasping point like nerf gun, football, foam and joystick.
Surprisingly, our algorithm outperformed the algorithm in
[1] for those objects that are non-uniform in their compos-

Figure 7: Results.(From left to right) original image, segmentation[2], proposed segmentation, object localization

Figure 9: An image sequence in which STAIR2 grasps a
nerf gun.

tion and don’t necessarily have an ideal grasping point. This
builds a foundation for our future work in which we will try
to ﬁnd graspable points based on different objects after lo-
calizing them.

5. Acknowledgement
The project has been made under the guidance of Quoc
V. Le. It would not have been possible to implement this
idea without Quoc’s sound mathematical ideas and direc-
tion. Siddharth Batra’s help with the Stair Vision Library
for labelling the dataset while training is also acknowl-
edged.

6. Video
We have posted a video on youtube giving a demonstra-
tion of our project.
http://www.youtube.com/watch?v=3Bb2rsC2WDA

We also have a Stair Fetches A Stapler section at the
end of the video.

References
[1] A. Saxena, J. Driemeyer and A. Y. Ng. Robotic Grasping
of Novel Objects Using Vision International Conference on
Robotics Research, 27(2):157–173, 2008.
[2] P. F. Felzenszwalb and D. P. Huttenlocher. Effecient Graph-
Based Image Segmentation International Journal of Com-
puter Vision, 59(2):167–181, 2004.
[3] Q. V. Le, D. Kamm, A. F. Kara and A. Y. Ng. Learning to
Grasp Objects with Muliple Contact Points
[4] M. Quigley, S. Batra, S. Gould, E. Klingbeil, Q. Le,
A. Wellman and A. Y. Ng High Accuracy 3D Sensing for
Mobile Manipulators: Improving Object Detection and Door
Opening ICRA,2009.
[5] S. Gould, O. Russakovsky, I. Goodfellow, P. Baumstrack,
A. Y. Ng and D. Koller The STAIR Vision Library (v2.2)
http://ai.stanford.edu/sgould/svl, 2009.
[6] The ROS (Robot Operating System) framework is an open-
source. peer-to-peer, cross-platform message passing system
being jointly developed by Stanford University and Willow
Garage. ROS is available on Sourceforge. Documentation is
avaiable at http://pr.willowgarage.com/wiki/ROS.


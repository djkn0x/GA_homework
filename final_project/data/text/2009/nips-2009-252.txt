Analysis of SVM with Indeﬁnite Kernels

Yiming Ying† , Colin Campbell† and Mark Girolami‡
†Department of Engineering Mathematics, University of Bristol,
Bristol BS8 1TR, United Kingdom
‡Department of Computer Science, University of Glasgow,
S.A.W. Building, G12 8QQ, United Kingdom

Abstract

The recent introduction of indeﬁnite SVM by Luss and d’Aspremont [15] has ef-
fectively demonstrated SVM classiﬁcation with a non-positive semi-deﬁnite ker-
nel (indeﬁnite kernel). This paper studies the properties of the objective function
introduced there. In particular, we show that the objective function is continuously
differentiable and its gradient can be explicitly computed. Indeed, we further show
that its gradient is Lipschitz continuous. The main idea behind our analysis is that
the objective function is smoothed by the penalty term, in its saddle (min-max)
representation, measuring the distance between the indeﬁnite kernel matrix and
the proxy positive semi-deﬁnite one. Our elementary result greatly facilitates the
application of gradient-based algorithms. Based on our analysis, we further de-
velop Nesterov’s smooth optimization approach [17, 18] for indeﬁnite SVM which
has an optimal convergence rate for smooth problems. Experiments on various
benchmark datasets validate our analysis and demonstrate the efﬁciency of our
proposed algorithms.

1 Introduction

Kernel methods [5, 24] such as Support Vector Machines (SVM) have recently attracted much atten-
tion due to their good generalization performance and appealing optimization approaches. The basic
idea of kernel methods is to map the data into a high dimensional (even inﬁnite-dimensional) feature
space through a kernel function. The kernel function over samples forms a similarity kernel matrix
which is usually required to be positive semi-deﬁnite (PSD). The PSD property of the similarity
matrix ensures that the SVM can be efﬁciently solved by a convex quadratic programming.
However, many potential kernel matrices could be non-positive semi-deﬁnite. Such cases are quite
common in applications such as the sigmoid kernel [14] for various values of the hyper-parameters,
hyperbolic tangent kernels [25], and the protein sequence similarity measures derived from Smith-
Waterman and BLAST score [23]. The problem of learning with a non-PSD similarity matrix (in-
deﬁnite kernel) has recently attracted considerable attention [4, 8, 9, 14, 20, 21, 26]. One widely
used method is to convert the indeﬁnite kernel matrix into a PSD one by using the spectrum trans-
formation. The denoise method neglects the negative eigenvalues [8, 21], ﬂip [8] takes the absolute
value of all eigenvalues, shift [22] shifts eigenvalues to be positive by adding a positive constant, and
the diffusion method [11] takes the exponentials of eigenvalues. One can also see [26] for a detailed
coverage. However, useful information in the data could be lost in the above spectral transformations
since they are separated from the process of training classiﬁers. In [9], the classiﬁcation problem
with indeﬁnite kernels is regarded as the minimization of the distance between convex hulls in the
pseudo-Euclidean space. In [20], general Reproducing Kernel Kreˇın spaces (RKKS) with indeﬁnite
kernels are introduced which allows a general representer theorem and regularization formulations.
Luss and d’Aspremont [15] recently proposed a regularized formulation for SVM classiﬁcation
with indeﬁnite kernel. Training a SVM with an indeﬁnite kernel was viewed as a learning the kernel

1

matrix problem [13] i.e.
learning a proxy PSD kernel matrix to approximate the indeﬁnite one.
Without realizing that the objective function is differentiable, the authors quadratically smoothed
the objective function, and then formulated two approximate algorithms including the projected
gradient method and the analytic center cutting plane method.
In this paper we follow the formulation of SVM with indeﬁnite kernels proposed in [15]. We mainly
establish the differentiability of the objective function (see its precise deﬁnition in equation (3)) and
prove that it is, indeed, differentiable with Lipschitz continuous gradient. This elementary result
suggests there is no need to smooth the objective function which greatly facilitates the application
of gradient-based algorithms. The main idea behind our analysis is from its saddle (min-max) rep-
resentation which involves a penalty term in the form of Frobenius norm of matrices, measuring
the distance between the indeﬁnite kernel matrix and the proxy PSD one. This penalty term can be
regarded as a Moreau-Yosida regularization term [12] to smooth out the objective function.
The paper is organized as follows.
In Section 2, we review the formulation of indeﬁnite SVM
classiﬁcation presented in [15]. Our main contribution is outlined in Section 3. There, we ﬁrst show
that the objective function of interest is continuously differentiable and its gradient function can
be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous. Based
on our analysis, in Section 4 we propose a simpliﬁed formulation of the projected gradient method
presented in [15] and show that it has a convergence rate of O(1/k) where k is the iteration number.
We further develop Nesterov’s smooth optimization approach [17, 18] for indeﬁnite SVM which
has an optimal convergence rate of O(1/k2 ) for smooth problems. In Section 5, our analysis and
proposed optimization approaches are validated by experiments on various benchmark data sets.

2 Indeﬁnite SVM Classiﬁcation

In this section we review the regularized formulation of indeﬁnite SVM presented in [15]. To this
end, we introduce some notation. Let Nn = {1, 2, . . . , n} for any n ∈ N and S n be the space of
all n × n symmetric matrices. If A ∈ S n is positive semi-deﬁnite, we write it as A (cid:186) 0. The
+ . For any A, B ∈ Rn×n , (cid:104)A, B (cid:105)F := Tr(A(cid:62)B ) where
cone of PSD matrices is denoted by S n
Tr(·) denotes the trace of a matrix. Finally, the Frobenius norm over the vector space S n is denoted,
for any A ∈ S n , by (cid:107)A(cid:107)F := (Tr(A(cid:62)A)) 1
2 . The standard Euclidean norm and inner product are
respectively denoted by (cid:107) · (cid:107) and (cid:104)·, ·(cid:105).
Let a set of training samples be given by inputs x = {xi ∈ Rd : i ∈ Nn } and outputs y = {yi ∈
{±1} : i ∈ Nn }. Suppose that K is a positive semi-deﬁnite kernel matrix (proxy kernel matrix)
on inputs x. Let matrix Y = diag(y), vector e be an n-dimensional vector of all ones and C be a
positive trade-off parameter. Then, the dual formulation of 1-norm soft margin SVM [5, 24] is given
by
maxα α(cid:62) e − 1
2 α(cid:62)Y K Y α
α(cid:62)y = 0, 0 ≤ α ≤ C.
s.t.
Since we assume that K is positive semi-deﬁnite, the above problem is a standard convex quadratic
program [2] and a global solution can be efﬁciently obtained by, e.g., the primal-dual interior
method. Suppose now we are only given an indeﬁnite kernel matrix K0 ∈ S n . Luss and
d’Aspremont [15] proposed the following max-min approach to simultaneously learn a proxy PSD
kernel matrix K for the indeﬁnite matrix K0 and the SVM classiﬁcation:
2 α(cid:62)Y K Y α + ρ(cid:107)K − K0 (cid:107)2
minK maxα α(cid:62) e − 1
(1)
α(cid:62)y = 0, 0 ≤ α ≤ C, K (cid:186) 0.
F
s.t.
2 α(cid:62)Y K Y α + ρ(cid:107)K − K0 (cid:107)2
Let Q1 = {α ∈ Rn : α(cid:62)y = 0, 0 ≤ α ≤ C } and L(α, K ) = α(cid:62) e − 1
F .
By the min-max theorem [2], problem (1) is equivalent to
L(α, K ).
min
max
α∈Q1
K∈S n
+
For simplicity, we refer to the following function deﬁned by
L(α, K )
f (α) = min
K∈S n
+
as the objective function. It is obviously concave since f is the minimum of a sequence of concave
functions. We also call the associated function L(α, K ) the saddle representation of the objective
function f .

(3)

(2)

2

For ﬁxed α ∈ Q1 , the optimization K (α) = arg minK(cid:186)0 L(α, K ) is equivalent to a projection to
the semi-deﬁnite cone S n
+ . Indeed, it was shown in [15] that the optimal solution is given by
K (α) = (K0 + Y αα(cid:62)Y /(4ρ))+
(4)
where, for any matrix A ∈ S n , the notation A+ denotes the positive part of A by simply setting
its negative eigenvalues to zero. The optimal solution (α∗ , K ∗ ) ∈ Q1 × S n
+ to the above min-max
problem is a saddle point of L(α, K ) (see e.g. [2]), i.e. for any α ∈ Q1 , K ∈ S n
+ there holds
L(α, K ∗ ) ≤ L(α∗ , K ∗ ) ≤ L(α∗ , K ). For a matrix A ∈ S n , denote its maximum eigenvalue by
λmax (A). The next lemma tells us that the optimal solution K ∗ belongs to a bounded domain in
S n
+ .
Lemma 1. Problem (2) is equivalent to the formulation maxα∈Q1 minK∈Q2 L(α, K ) and the ob-
jective function can be deﬁned by
(cid:111)
(cid:110)
L(α, K )
f (α) = min
K∈Q2
+ : λmax (K ) ≤ λmax (K0 ) + nC 2
K ∈ S n
4ρ
(cid:162) ≤
Proof. By the saddle theorem [2], we have L(α∗ , K ∗ ) = minK∈Q2 L(α∗ , K ). Combining this
with equation (4) yields that K ∗ = K (α∗ ) = (K0 + Y α∗ (α∗ )(cid:62)Y /(4ρ))+ . We can easily see
λmax (K ∗ ) ≤ λmax (K0 + Y α∗ (α∗ )(cid:62)Y /(4ρ) ≤ λmax (K0 ) + λmax (Y α∗ (α∗ )(cid:62)Y /(4ρ)
λmax (K0 ) + (cid:107)α∗ (cid:107)2
4ρ , where the second to last inequality uses the property of maximum eigenval-
ues (e.g. [10, Page 201]) i.e. λmax (A + B ) ≤ λmax (A) + λmax (B ) for any A, B ∈ S n . Note
that 0 ≤ α∗ ≤ C , (cid:107)α∗ (cid:107)2 ≤ nC 2 . Combining this with the above inequality yields the desired
lemma.

where Q2 :=

(5)

.

It is worthy of mentioning that it was shown in [18, Theorem 1] that a function g has a Lipschitz
continuous gradient if it enjoys a special structure: g(α) = min{(cid:104)Aα, K (cid:105) + γ d(K ) : K ∈ Q}
where Q is a closed convex subset in a certain vector space and d(·) is a strongly convex function,
and, most importantly, A is a linear operator. Since the variable α appeared in a quadratic form, i.e.
α(cid:62)Y K Y α, in the objective function deﬁned by (5), it can not be written as the above special form,
and hence the theorem there can not be applied to our case.

3 Differentiability of the Objective Function

The following lemma outlines a very useful characterization of differentiable properties of the opti-
mal value function [3, Theorem 4.1], essentially due to Danskin [7].
Lemma 2. Let X be a metric space and U be a normed space. Suppose that for all x ∈ X the
function L(α, ·) is differentiable, L(α, x) and ∂αL(α, x), the derivative of L(·, x), are continuous
on X × U and let Q be a compact subset of X . Deﬁne the optimal value function as f (α) =
inf x∈Q L(α, x). The optimal value function is directionally differentiable. Furthermore, if for α ∈
U , L(α, ·) has a unique minimizer x(α) over Q then f is differentiable at α and the gradient of f is
given by ∇f (α) = ∂αL(α, x(α)).
Applying the above lemma to the objective function f deﬁned by equation (5), we have:
Theorem 1. The objective function f deﬁned by (3) (equivalently by (5)) is differentiable and its
gradient is given by
∇f (α) = e − Y (K0 + Y αα(cid:62)Y /(4ρ))+Y α.
(6)
Proof. We apply Lemma 2 with X = S n and Q = Q2 ⊆ S n , U = Q1 and x = K . To this
end, we ﬁrst prove the uniqueness of K (α). Suppose there are two minimizers K1 , K2 for problem
+ L(α, K ). By the ﬁrst order optimality condition, for the minimizer K1 , we have that
arg minK∈S n
(cid:104)∂K L(α, K1 ), K2 − K1 (cid:105)F ≥ 0. Considering the minimizer K2 , we also have (cid:104)∂K L(α, K2 ), K1 −
K2 (cid:105)F ≥ 0. Noting that ∂K L(α, K ) = − 1
2 Y αα(cid:62)Y + 2ρ(K − K0 ) and adding the above two ﬁrst-
order optimaility inequalities together, we have −(cid:107)K2 −K1 (cid:107)2
F ≥ 0 which means that K1 = K2 , and
hence completes the proof of the uniqueness of K (α). Now the desired result follows directly from
Lemma 2 by noting that the derivative of L w.r.t. the ﬁrst argument ∂αL(α, K ) = e − Y K Y α.

3

Indeed, we can go further to establish the Lipschitz continuity of ∇f based on the strongly convex
property of L(α, ·). To this end, we ﬁrst establish a useful lemma.
Lemma 3. For any α1 , α2 ∈ Q1 ,
there holds (cid:107)(K0 + Y α1α(cid:62)
1 Y /(4ρ))+ − (K0 +
2 Y /(4ρ))+(cid:107)F ≤ ((cid:107)α1 (cid:107) + (cid:107)α2 (cid:107))(cid:107)α1 − α2 (cid:107)/(4ρ).
Y α2α(cid:62)
Proof. Let ∂K L(α, ·) denote the gradient w.r.t. K . Now, consider the minimization prob-
lem arg minK∈Q2 L(α, K ). By the ﬁrst order optimality conditions, for any K ∈ Q2 there
holds (cid:104)∂K L(α, K (α)), K − K (α)(cid:105)F ≥ 0. Applying the above inequality twice implies that
(cid:104)∂K L(α1 , K (α1 )), K (α2 ) − K (α1 )(cid:105)F ≥ 0, and (cid:104)∂K L(α2 , K (α2 )), K (α1 ) − K (α2 )(cid:105)F ≥ 0. Con-
sequently, (cid:104)∂K L(α1 , K (α1 )) − ∂K L(α2 , K (α2 )), K (α2 ) − K (α1 )(cid:105)F ≥ 0. Substituting the fact
that ∂K L(α, K ) = − 1
2 Y αα(cid:62)Y + 2ρ(K − K0 ) into the above equation, we have 4ρ(cid:107)K (α1 ) −
K (α2 )(cid:107)2
F ≤ (cid:104)Y (α2α(cid:62)
2 − α1α(cid:62)
1 )Y , K (α2 ) − K (α1 )(cid:105)F ≤ (cid:107)Y (α2α(cid:62)
2 − α1α(cid:62)
1 )Y (cid:107)F (cid:107)K (α2 ) −
K (α1 )(cid:107)F . Consequently,
2 − α1α(cid:62)
≤ (cid:107)(α2α(cid:62)
1 )(cid:107)F
2 − α1α(cid:62)
(cid:107)K (α1 ) − K (α2 )(cid:107)F ≤ (cid:107)Y (α2α(cid:62)
1 )Y (cid:107)F
(7)
4ρ
4ρ
where the last inequality follows from the fact that Y is an orthonormal matrix since yi ∈ {±1}
and Y = diag(y1 , . . . , yn ). Note that (cid:107)α2α(cid:62)
2 − α1α(cid:62)
1 (cid:107)F = (cid:107)(α2 − α1 )α(cid:62)
2 − α1 (α1 − α2 )(cid:62)(cid:107)F ≤
((cid:107)α1 (cid:107)+(cid:107)α2 (cid:107))(cid:107)α1 −α2 (cid:107). Putting this back into inequality (7) completes the proof of the lemma.

It is interesting to point out that the above lemma can be alternatively established by delicate tech-
niques in matrix analysis. To see this, recall that a spectral function G : S n → S n is deﬁned
by applying a real-valued function g to the eigenvalues of its argument i.e. for any K ∈ S n with
eigen-decomposition K = U diag(λ1 , . . . , λn )U (cid:62) , G(K ) := U diag(g(λ1 ), . . . , g(λn ))U (cid:62) . The
perturbation inequality in matrix analysis [1, Lemma VII.5.5] shows that if g is Lipschitz continu-
ous with Lipschitz constant κ then (cid:107)G(K1 ) − G(K2 )(cid:107)F ≤ κ(cid:107)K1 − K2 (cid:107)F ,
∀K1 , K2 ∈ S n .
Applying the above inequality with g(t) = max(0, t) and K1 = K0 + Y α1α(cid:62)
1 Y /(4ρ) and
K2 = K0 + Y α2α(cid:62)
2 Y /(4ρ) implies equation (7), and hence Lemma 3. However, we prefer the
original proof presented for Lemma 3 since it explains more clearly how the strong convexity of the
regularization term (cid:107)K − K0 (cid:107)2
F plays a critical role in the analysis.
From the above lemma, we can establish the Lipschitz continuity of the gradient of the objective
function.
∇f (α2 )(cid:107) ≤ (cid:163)
(cid:164)(cid:107)α1 − α2 (cid:107).
Theorem 2. The gradient of the objective function given by (6) is Lipschitz continuous with Lipschitz
i.e. for any α1 , α2 ∈ Q1 the following inequality holds (cid:107)∇f (α1 ) −
constant L = λmax (K0 ) + nC 2
ρ
λmax (K0 )) + nC 2 /ρ
can be bounded by(cid:110)
(cid:111)
Proof. For any α1 , α2 ∈ Q1 , from representation of ∇f in Theorem 1 the term (cid:107)∇f (α1 )−∇f (α2 )(cid:107)
(cid:163)
(cid:164)
(cid:111)
(cid:110)
1 Y /(4ρ))+ − (K0 + Y α2α(cid:62)
(cid:107)Y
Y α1 (cid:107)
(K0 + Y α1α(cid:62)
2 Y /(4ρ))+
(cid:107)Y (K0 + Y α2α(cid:62)
2 Y /(4ρ))+Y (α2 − α1 )(cid:107)
+
.
(cid:162)
(cid:161)
Now it remains to estimate the two terms within parentheses on the right-hand side of inequality (8).
(cid:161)
(cid:162)
Let’s begin with the ﬁrst one by applying Lemma 3.
+ − (cid:161)
(cid:162)
≤ (cid:107)(cid:161)
(cid:162)
1 Y /(4ρ))+ − (K0 + Y α2α(cid:62)
Y α1 (cid:107)
(cid:107)Y
(K0 + Y α1α(cid:62)
2 Y /(4ρ))+
1 Y /(4ρ))+ − (K0 + Y α2α(cid:62)
≤ (cid:107)Y
Y (cid:107)F (cid:107)α1 (cid:107)
(K0 + Y α1α(cid:62)
2 Y /(4ρ))+
+(cid:107)F (cid:107)α1 (cid:107)
K0 + Y α2α(cid:62)
K0 + Y α1α(cid:62)
2 Y /(4ρ)
1 Y /(4ρ)
2ρ (cid:107)α1 − α2 (cid:107).
≤ (cid:107)α1 (cid:107) ((cid:107)α1 (cid:107) + (cid:107)α2 (cid:107)) (cid:107)α1 − α2 (cid:107)/(4ρ) ≤ nC 2
where the second inequality follows from the fact that Y is an orthonormal matrix. For the
(cid:180)
(cid:179)
(cid:179)
(cid:180)
second term on the right-hand side of inequality (8), we apply the fact proved in Theorem 1
that K (α) ∈ Q2 for any α ∈ Q1 .
Indeed, (cid:107)Y (K0 + Y α2α(cid:62)
2 Y /(4ρ))+Y (α2 − α1 )(cid:107) ≤
(cid:104)
(cid:105)
(cid:107)α2 − α1 (cid:107) ≤
(cid:107)α2 − α1 (cid:107) ≤ λmax
Y (K0 + Y α2α(cid:62)
(K0 + Y α2α(cid:62)
2 Y /(4ρ))+Y
2 Y /(4ρ))+
λmax
(cid:107)α1 − α2 (cid:107). Putting this equation and (9) back into equality (8) completes the
λmax (K0 ) + nC 2
4ρ
proof of Theorem 2.

(9)

(8)

4

(cid:161)
(cid:162)
Simpliﬁed Projected Gradient Method (SPGM)
ρ . Let ε > 0, α0 ∈ Q1 be given and set k = 0.
1. Choose γ ≥ λmax (K0 ) + nC 2
2. Compute ∇f (αk ) = e − Y
K0 + Y αk α(cid:62)
k Y /(4ρ)
+ Y αk .
3. αk+1 = PQ1 (αk + ∇f (αk )/γ ) .
4. Set k ← k + 1. Go to step 2 until the stopping criterion less than ε.

Table 1: Pseudo-code of projected gradient method

4 Smooth Optimization Algorithms

This section is based on the theoretical analysis above, mainly Theorem 2. We ﬁrst outline a sim-
pliﬁed version of the projected gradient method proposed in [15] and show it has a convergence rate
of O(1/k) where k is the iteration number. We can further develop a smooth optimization approach
[17, 18] for indeﬁnite SVM (5). This scheme has an optimal convergence rate O(1/k2 ) for smooth
problems which has been applied to various problems, e.g. [6].

4.1 Simpliﬁed Projected Gradient Method

(10)

In [15], the objective function was smoothed by adding a quadratic term (see details in Section 3
there) and then they proposed a projected gradient algorithm to solve this approximation problem.
Using the explicit gradient representation in Theorem 1 we formulate its simpliﬁed version in Table
1 where the projection PQ1 : Rn → Q1 is deﬁned, for any β ∈ Rn , by
(cid:107)α − β (cid:107)2 .
PQ1 (β ) = arg min
α∈Q1
(cid:105)
(cid:104)
Indeed, from Theorem 2 we can further obtain the following result by developing the techniques in
Sections 2.1.5, 2.2.3 and 2.2.4 of [18].
and {αk : k ∈ N} be given by the simpliﬁed projected
Lemma 4. Let γ ≥
λmax (K0 ) + nC 2
ρ
gradient method in Table 1. For any α ∈ Q1 , the following inequality holds f (αk+1 ) ≥ f (α) +
γ (cid:104)αk − αk+1 , α − αk (cid:105) + γ
2 (cid:107)αk − αk+1 (cid:107)2 .
(cid:82) 1
Proof. We know from Theorem 2 that ∇f is Lipschitz continuous with Lipschitz constant L =
(cid:82) 1
ρ , then we have f (α) − f (αk ) − (cid:104)∇f (αk ), α − αk (cid:105) =
0 (cid:104)∇f (θα + (1 − θ)αk ) −
λmax (K0 ) + nC 2
2 (cid:107)α − αk (cid:107)2 . Applying this inequality with
∇f (αk ), α − αk (cid:105)dθ ≥ −L
0 (1 − θ)(cid:107)α − αk (cid:107)2dθ ≥ − γ
α = αk+1 implies that
−f (αk ) − (cid:104)∇f (αk ), αk+1 − αk (cid:105) ≥ −f (αk+1 ) − γ
2
Let φ(α) = −f (αk ) − ∇f (αk )(α − αk ) + γ
2 (cid:107)α − αk (cid:107)2 which implies that αk+1 =
arg minα∈Q1 φ(α). Then, by the ﬁrst-order optimality condition over αk+1 there holds, for any
α ∈ Q1 , (cid:104)∇φ(αk ), α − αk+1 (cid:105) ≥ 0, i.e. −(cid:104)∇f (αk ), α − αk+1 (cid:105) ≥ γ (cid:104)αk+1 − αk , αk+1 − α(cid:105). Adding
this equation and (11) together yields that −f (αk ) − (cid:104)∇f (αk ), α − αk (cid:105) ≥ −f (αk+1 ) + γ (cid:104)αk −
αk+1 , α − αk (cid:105) + γ
2 (cid:107)αk − αk+1 (cid:107)2 . Also, since −f is convex, −f (α) ≥ −f (αk ) − (cid:104)∇f (αk ), α − αk (cid:105).
(cid:105)
(cid:104)
Combining this with the above inequality ﬁnishes the proof of the lemma.
and the iteration sequence {αk : k ∈ N} be given by the
Theorem 3. Let γ ≥
λmax (K0 ) + nC 2
ρ
simpliﬁed projected gradient method in Table 1. Then, we have that
f (αk+1 ) ≥ f (αk ) + γ
(cid:107)αk+1 − αk (cid:107)2 ,
2

(cid:107)αk+1 − αk (cid:107)2 .

(11)

(12)

Moreover,

f (α) − f (αk ) ≤ γ
(cid:107)α0 − α∗ (cid:107)2
max
α∈Q1
2k
where α∗ is an optimal solution of problem maxα∈Q1 f (α).

(13)

5

(cid:161)
(cid:162)
Nesterov’s Smooth Optimization Method (SMM)
1. Let ε > 0, k = 0 and initialize α0 ∈ Q1 and let L = λmax (K0 )) + nC 2 /ρ.
(cid:179)
(cid:180)
(cid:80)k
2. Compute ∇f (αk ) = e − Y
K0 + Y αk α(cid:62)
k Y /(4ρ)
+ Y αk .
3. Compute γk = PQ1 (αk + ∇f (αk )/L) .
4. Compute βk = PQ1
i=0 (i + 1)∇f (αk )/(2L)
α0 +
k+3 βk + k+1
5. Set αk+1 = 2
k+3 γk .
6. Set k ← k + 1. Go to step 2 until the stopping criterion less than ε.

.

Table 2: Pseudo-code of ﬁrst-order Nesterov’s smooth optimization method

Proof. Applying Lemma 4 with α = αk yields inequality (12). To prove inequality (13), we ﬁrst
apply Lemma 4 with α = α∗ to get that, for any i, maxα∈Q1 f (α) − f (αi ) ≤ −γ (cid:104)αi − αi+1 , α∗ −
k (maxα∈Q1 f (α) − f (αk )) ≤ (cid:80)k−1
2 (cid:107)α∗ − αi+1 (cid:107)2 . Adding them over i from 0 and k − 1
2 (cid:107)α∗ − αi (cid:107)2 − γ
αi (cid:105) − γ
2 (cid:107)αi − αi+1 (cid:107)2 = γ
and also, noting from (12) that {maxα∈Q1 f (α) − f (αk ) : k ∈ N} is decreasing, we have that
i=0 (maxα∈Q1 f (α) − f (αi+1 )) ≤ γ
2 (cid:107)α∗ − α0 (cid:107)2 . This com-
pletes the proof of the theorem.
From the above theorem, the sequence {f (αk ) : k ∈ N} is monotonically increasing and the
iteration complexity of SPGM is O(L/ε) for ﬁnding an ε-optimal solution.

4.2 Nesterov’s Smooth Optimization Method

In [18, 17], Nesterov proposed an efﬁcient smooth optimization method for solving convex pro-
gramming problems of the form

g(x)
min
x∈U
where g is a convex function with Lipschitz continuous gradient, and U is a closed convex set in Rn .
Speciﬁcally, suppose there exists L > 0 such that (cid:107)∇g(x) − ∇g(x(cid:48) )(cid:107) ≤ L(cid:107)x − x(cid:48) (cid:107),
∀x, x(cid:48) ∈ U.
The smooth optimization approach needs to introduce a proxy-function d(x) associated with the set
U . It is assumed to be continuous and strongly convex on U with convexity parameter σ > 0.
Let x0 = arg minx∈U d(x). Without loss of generality, assume that d(x0 ) = 0. Thus, strong
(cid:112)
convexity of d means that , for any x ∈ U , d(x) ≥ 1
2 σ(cid:107)x − x0 (cid:107)2 . Then, a speciﬁc ﬁrst-order smooth
optimization scheme detailed in [18] can be then applied to the function g with convergence rate
L/ε). The ﬁrst-order method needs to deﬁne a proxy-function associated with Q1 . Here,
in O(
2 (cid:107)α − α0 (cid:107)2 with α0 ∈ Q1 . The Lipschitz constant of
we deﬁne the proxy-function by d(α) = 1
−f is established in Theorem 2 given by L = λmax (K0 ) + nC 2 /ρ. Translating the ﬁrst-order
Nesterov’s scheme [18, Section 3] to our problem (5), we can get the smooth optimization algorithm
for indeﬁnite SVM, see its pseudo-code in Table 2. One can see [17] for its variants with general
step sizes.
The effectiveness of the ﬁrst-order Nesterov’s algorithm largely depends on the Steps 2, 3 and 4 out-
(cid:80)k
lined in Table 2. By Theorem 1, the computation of ∇f (αk ) in Step 2 needs an eigen-decomposition.
Steps 3 and 4 are the projection problem (10) by replacing β respectively by αk + ∇f (αk )/L
i=0 (i + 1)∇f (αi )/(2L). The convergence of this optimal method was shown in [18]:
and α0 +
maxα∈Q1 f (α) − f (γk ) ≤ 4L(cid:107)α0−α∗ (cid:107)2
(k+1)(k+2) where α∗ is one of the optimal solutions. It is worthy of
pointing out that either {f (αk ) : k ∈ N} or {f (γk ) : k ∈ N} may not monotonically increase,
however it can be made to monotonically increase by a simple modiﬁcation of the algorithm [18].
In addition, the above estimation of the Lipschitz constant L could be loose in reality and one could
further accelerate the algorithm by using a line search scheme [16].

4.3 Related Work and Complexity Discussion

We list the theoretical time complexity of algorithms to run Indeﬁnite SVM. It is worth noting that
the number of iterations to reach a target precision of ε means that −f (αk ) − minα∈Q1 −f (α) =
maxα∈Q1 f (α) − f (αk ) ≤ ε. However, this does not mean the dual gap as used in [15] is less
than ε. In [15], the objective function is smoothed by adding a quadratic term and then they further

6

proposed a projected gradient algorithm and analytic center cutting plane method (ACCPM)1 . As
proved in Theorem 3, the number of iterations of the projected gradient method is usually O(L/ε).
In each iteration, the main complexity cost O(n3 ) is from the eigen-decomposition. Hence, the
overall complexity of SPGM is O(n3L/ε). As discussed in [15], ACCPM has an overall complexity
is O(n4 log(1/ε)2 ) for ﬁnding an ε-optimal solution. However, this method needs to use interior
methods at each iteration which would be slow for large scale datasets.
Chen and Ye [4] reformulated indeﬁnite SVM as an appealing semi-inﬁnite quadratically constrained
linear programming (SIQCLP) without applying extra smoothing techniques. There, the algorithm
iteratively solves a linear programming with a ﬁnite number of quadratic constraints. The iteration
complexity of semi-inﬁnite linear programming is usually O(1/ε3 ). In each iteration, one needs
to ﬁnd maximum violation constraints which involves eigen-decomposition of complexity O(n3 ).
Hence, the overall complexity is of O(n3 /ε3 ). The main limitation of this approach is that one needs
to save the subset of increasing quadratically constrained conditions indexed by n × n matrices and
iteratively solve a quadratically constrained linear programming (QCLP). The QCLP sub-problem
can be solved by general software packages, e.g. Mosek (http://www.mosek.com/), which is gener-
ally slow in our experience. This tends to make the algorithm inefﬁcient during the iteration process,
although pruning techniques were proposed to avoid too many quadratically constrained conditions.
Based on our theoretical results (Theorem 2), Nesterov’s smooth optimization method can be ap-
plied. The complexity of this smooth optimization method (SMM) mainly relies on the eigenvalue
(cid:112)
decomposition on Step 2 listed in Table 2 which costs O(n3 ). Step 3 and 4 are projections onto
(cid:112)
the convex region Q1 which costs O(n log n) as pointed out in [15]. The ﬁrst-order smooth op-
timization approach [17, 18] has iteration complexity O(
L/ε) for ﬁnding an ε-optimal solution.
Consequently, the overall complexity is O(n3
L/ε). Hence, from theoretical comparison the com-
plexity of smoothing optimization is better than the simpliﬁed projected gradient method (SPGM)
and SIQCLP. Compared with ACCPM, SMM has better dependence on the sample number n but
with a worse precision i.e. worse dependence on ε.

5 Experimental Validation

We run our proposed smooth optimization approach and simpliﬁed projected gradient method on
various datasets to validate our analysis. The experiments are done on several benchmark data sets
from the UCI repository [19] including Sonar, Ionosphere, Heart, Pima Indians Diabetes, Breast
Cancer, and USPS with digits 3 and 5. For USPS dataset, we randomly select 600 samples for
each digit. All the results reported are based on 10 random training/test partition with ratio 4/1.
matrices by adding a small noisy matrix i.e. K0 := K − 0.1 (cid:98)E . Here, the noisy matrix (cid:98)E =
In each data split, as in [4] we ﬁrst generate a Gaussian kernel matrix K with the hyper-parameter
determined by cross-validation on the training data using LIBSVM and then construct indeﬁnite
(E + E (cid:48) )/2 where E is randomly generated by zero mean and identity covariance matrix. For all
methods, the parameters C and ρ for Indeﬁnite SVM are tuned by cross-validation and we terminate
the algorithm if the relative change of the objective value is less than 10−6 .
In Table 3, we report the average test set accuracy (%) and CPU time (seconds) across different
algorithms: smooth optimization method (SMM), simpliﬁed projected gradient method (SPGM),
analytic center cutting plane method (ACCPM), and semi-inﬁnite quadratically constrained linear
programming (SIQCLP). For the QCLP sub-problem in the SIQCLP method, we use Mosek soft-
ware package (http://www.mosek.com/). We can see that test accuracies are statistically the same
across different algorithms, which validates our analysis on the objective function. In particular, we
observe that SMM is consistently more efﬁcient than other methods, especially for a large number
of training samples. SIQCLP needs much more time since, in each iteration, it needs to solves a
quadratically constrained linear programming. In Figure 1, we plot the objective values versus iter-
ation on Sonar and Diabetes for SMM, SPGM, and ACCPM. The SIQCLP approach is not included
here since its objective value is not based on the iteration w.r.t. the variable α which does not di-
rectly yield an increasing iteration sequence of objective values in contrast to those of the other three
algorithms. From Figure 1, we can see that SMM converges faster than SPGM which is consistent
with the complexity analysis. The convergence of ACCPM is quite similar to SMM, especially for

1MATLAB codes are available in http://www.princeton.edu/ rluss/IndeﬁniteSVM.htm

7

Data
Sonar

Size
λmin
208 −1.38

λmax
21.47

Ionosphere

Heart

Diabetes

351

270

768

-2.08

101.34

-1.98

178.03

-3.44

539.12

Breast-cancer

USPS-35

-2.87
683
1200 −3.72

290.41

112.65

SPGM ACCPM SIQCLP
SMM
76.09%
76.34% 76.34% 75.12%
244.55s
0.74s
5.12s
3.20s
93.54%
93.14% 93.43% 93.54%
5.47s
28.93s
22.73s
455.81s
79.25%
79.81% 79.44% 79.25%
689.17s
3.54s
12.05s
11.96s
69.73%
70.00% 69.86% 70.52%
39.93s
345.48s
678.85s
3134.31s
95.40%
95.93% 96.02% 96.02%
4610.82s
5.71s
50.13s
212.96s
95.54%
96.33% 96.33% 96.04%
23.22s
236.00s
3713.05s
5199.17s

Table 3: Average test set accuracy (%) and CPU time in seconds (s) of different algorithms where
λmax (λmin ) denotes the average maximum (minimum) eigenvalues of the indeﬁnite kernel matrix
over training samples.

Figure 1: Objective value versus iteration: Sonar (left) and Diabetes (right). Curves: SMM (blue),
SPGM (red) and ACCPM (black)

small-sized datasets which coincides with the complexity analysis in Section 4.3 since it generally
has a high precision. However, ACCPM needs more time in each iteration than SMM and this ob-
servation becomes more apparent for the relatively large datasets shown in the time comparison of
Table 3.

6 Conclusion

In this paper we analyzed the regularization formulation for training SVM with indeﬁnite kernels
proposed by Luss and d’Aspremont [15]. We show that the objective function of interest is continu-
ously differentiable with Lipschitz continuous gradient. Our elementary analysis greatly facilitates
the application of gradient-based methods. We formulated a simpliﬁed version of the projected gra-
dient method presented in [15] and showed that it has a convergence rate of O(1/k). We further
developed Nesterov’s smooth optimization method [17, 18] for Indeﬁnite SVM which has an opti-
mal convergence rate of O(1/k2 ) for smooth problems. Experiments on various datasets validate
our analysis and the efﬁciency of our proposed optimization approach. In future, we are planning to
further accelerate the algorithm by using a line search scheme [16]. We are also applying this method
to real biological datasets such as protein sequence analysis using sequence alignment measures.

Acknowledgements

This work is supported by EPSRC grant EP/E027296/1.

8

20406080100120−4−3.5−3−2.5−2−1.5−1−0.500.5x 104IterationObjectve value  SMMSPGMACCPM20406080100−600−500−400−300−200−1000100Iteration  SMMSPGMACCPMReferences
[1] R. Bhatia. Matrix analysis. Graduate texts in Mathematics. Springer, 1997.
[2] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, 2004.
[3] J. F. Bonnans and A. Shapiro. Optimization problems with perturbation: A guided tour. SIAM
Review, 40: 202–227, 1998.
[4] J. Chen and J. Ye. Training SVM with Indeﬁnite Kernels. ICML, 2008.
[5] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines and other
kernel-based learning methods. Cambridge University Press, 2000.
[6] A. d’Aspremont, O. Banerjee and L. El Ghaoui. First-order methods for sparse covariance
selection. SIAM Journal on Matrix Analysis and its Applications, 30: 56–66, 2007.
[7] J.M. Danskin. The theory of max-min and its applications to weapons allocation problems,
Springer-Verlag, New York, 1967.
[8] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer. Classiﬁcation on pairwise
proximity data. NIPS, 1998.
[9] B. Haasdonk. Feature space interpretation of SVMs with indeﬁnite kernels. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 27: 482–492, 2005.
[10] R. A. Horn and C. R. Johnson. Topics in Matrix Analysis. Cambridge University Press, 1991.
[11] R. I. Kondor and J. Laffferty. Diffusion kernels on graphs and other discrete input spaces.
ICML, 2002.
[12] C. Lemar ´echal and C. Sagastiz ´abal. Practical aspects of the Moreau-Yosida regularization:
theoretical preliminaries. SIAM Journal on Optimization, 7: 367–385, 1997.
[13] G. R. G. Lanckriet, N. Cristianini, N., P. L. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning
the kernel matrix with semideﬁnite programming. J. of Machine Learning Research, 5: 27–
72, 2004.
[14] H.-T. Lin and C. J. Lin. A study on sigmoid kernels for SVM and the training of non-psd
kernels by smo-type methods. Technical Report, National Taiwan University, 2003.
[15] R. Luss and A. d’Aspremont. Support vector machine classiﬁcation with indeﬁnite kernels.
NIPS, 2007.
[16] A. Nemirovski. Efﬁcient methods in convex programming. Lecture Notes, 1994.
[17] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2003.
[18] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,
103:127–152, 2005.
[19] D. Newman, S. Hettich, C. Blake, and C. Merz. UCI repository of machine learning datasets.
1998.
[20] C. S. Ong, X. Mary, S. Canu, and A. J. Smola. Learning with non-positive kernels. ICML,
2004.
[21] E. Pekalska, P. Paclik, and R. P. W. Duin. A generalized kernel approach to dissimilarity-
based classiﬁcation. J. of Machine Learning Research, 2: 175–211, 2002.
[22] V. Roth, J. Laub, M. Kawanabe, and J. M. Buhmann. Optimal cluster preserving embedding of
nonmetric proximity data. IEEE Transactions on Pattern Analysis and Machine Intelligence,
25:1540–1551, 2003.
[23] H. Saigo, J.P.Vert and N. Ueda, and T. Akutsu. Protein homology detection using string align-
ment kernels. Bioinformatics, 20: 1682–1689., 2004.
[24] B. Sch ¨olkopf, and A.J. Smola. Learning with kernels: Support vector machines, regulariza-
tion, optimization, and beyond. The MIT Press, 2001.
[25] A. J. Smola, Z. L. ´O ´vari, and R. C. Williamson. Regularization with dot-product kernels.
NIPS, 2000.
[26] G. Wu, Z. Zhang, and E. Y. Chang. An analysis of transformation on non-positive semideﬁnite
similarity matrix for kernel machines. Technical Report, UCSB, 2005.

9


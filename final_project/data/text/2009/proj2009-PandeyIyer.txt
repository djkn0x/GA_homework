Sentiment Analysis of Microblogs

Vipul Pandey and C.V.Krishnakumar Iyer

1

Abstract—In this pro ject we attempt to perform sen-
timent based classiﬁcation of Micro-blogs using Machine
Learning techniques. Sentiment Analysis of short messages
posted on Micro-blogging tools can be helpful in determin-
ing the current usability and acceptance of any target prod-
uct or service.
It can help in raising alarms in the wake
of sudden shifts in user sentiments or attitude towards the
service. We present a system that can read messages rel-
evant to a particular topic from a micro-blogging service
such as Twitter, analyze the messages for the sentiments
they carry and classify them as neutral,positive or negative.
We try out diﬀerent feature selection and classiﬁcation al-
gorithms in our search for the best combination. We also
evaluate diﬀerent strategies in order to get the best perfor-
mance over the imbalanced dataset - caused by relatively
infrequent changes in user sentiments - and to understand
the underlying nature of this problem.

I. Introduction

Sentiment or Opinion Mining has been an active area
of research in academia because of the challenges that it
poses.
It is also a vital question that is sought in the
industry as it gives an insight into the consumers’ mind, the
inﬂuences he is sub ject to, and his decision making process
- besides being an explicit feedback about the performance
of any widely used and talked about product, service, event
or a phenomenon.
In our eﬀorts, we try to detect the current attitude of the
users towards an online service, the model that can be ex-
tended to other services or products without any diﬃculty
whatsoever.
There has been a widespread interest in the area of Opin-
ion mining and Sentiment Analysis because of the chal-
lenges it oﬀers and its potential applicability. [2] gives an
excellent survey of the recent developments and the work
done in this ﬁeld.
[3] performs a work that is very simi-
lar to ours in that it seeks to identify market sentiments.
They develop hybrid methods for extracting opinions in an
automated manner from the discussions on the stock mes-
sage boards. However, they report a maximum accuracy
of 67.58% (Low Ambiguity, Test Set size = 290, Train-
ing Set Size = 913).
[4] details diﬀerent feature-selection
metrics for text classiﬁcation that we plan to apply in our
work.[5] consider a classiﬁcation problem of discriminating
in between the positive and negative sentiments, and test
their approaches using Naive Bayes Classiﬁers, Max. En-
tropy Classiﬁers and SVMs. They operate on the movie
reviews dataset, and report a maximum accuracy of 82.9%
with SVMs using the presence/absence of features. In ad-
dition, they also study the eﬀects of a few variations in the
parameters, just as we do.

II. Approach

We follow a simple approach of chaining two Classi-
ﬁers to classify messages into the three desired categories.
The ﬁrst Classiﬁer discriminates in between neutral mes-

sages and polar (negative or positive) ones. The messages
deemed polar by this Classiﬁer are then forwarded to the
second Classiﬁer in the scheme, which is trained to distin-
guish in between positive and negative sentiments. The
overall scheme is depicted in the Fig-1.

Fig. 1. Components of the System

Following operations are performed in the system to
achieve the ﬁnal results :
• Data Collection - in which data is collected from the
source.
• Preprocessing - in which collected data is preprocessed
for further action.
• Feature Selection - in which a subset of features is
selected to reduce dimensionality.
• Model Selection - in which a model is trained using
the training set.
• Classiﬁcation - in which a trained model classiﬁes the
newly arriving messages.
We developed most of the infrastructure in-house al-
though we use [1]Weka framework for standard machine
learning algorithms for prototyping and evaluation.

III. Data

We primarily focus on messages obtained from Twitter
for prototyping and benchmarking but the system can be
easily extended towards other sources as well. We use
the Twitter Search API to obtain the ’tweets’ mention-
ing the product that we focus on. We collected over 25000
such messages and manually classiﬁed them as negative,

mild negative, neutral, mild positive and positive. While
manually classifying the data we realized that the data
is imbalanced in that there are signiﬁcantly more neutral
messages than the polar ones. We also noticed that the
mild-negative and mild-positive messages, though having
some trace of polarity, either had a large overlap with neu-
tral messages or were bipolar thus adding to the ambi-
guity. Therefore in our exercise we primarily focus only
on Neutral, Positive and Negative messages by ignore the
ambiguous ones while training the classiﬁer.

IV. Preprocessing

As soon as the data is collected, it is passed through
a series of preprocessors that aid in the conversion of the
message strings into the feature vector, that is used by the
feature selection algorithm and eventually by the classiﬁer.
The preprocessing step also performs the important task
of extracting the relevant signals from the messages, while
leaving out the irrelevant ones. In our feature extraction
process we not only consider the common text based fea-
tures as used in traditional Information Retrieval tasks but
also use several domain speciﬁc features. This assumes a
greater signiﬁcance in our domain of micro-blogs since the
text, by its nature is short and hence we must depend on
many other features to identify the polarity in the message
and the type of polarity that it exhibits.
Few of the preprocessors and the preprocessing steps
that we follow are explained with examples below. This is
one of the more important steps in the entire classiﬁcation
process since the quality of the features that we extract
ultimately decides the performance of the classiﬁers. Let
us consider a running example :

@cvkkumar:The iTunes Store REEAAAALLY ROCKSS!!
:) http://bit.ly/madeup

A. Al l Caps Identiﬁcation

This forms the ﬁrst ﬁlter in our preprocessing step. Since
we operate on the micro-blogging space, we observed that
it was very common to use all capital words to represent
powerful emotions, and hence are a good indicator of the
polarity of the message.

@cvkkumar:The iTunes Store REEAAAALLY ROCKSS!!
<>HASALLCAPS<>:) http://bit.ly/madeup

B. Lower Casing

Having already exploited the fact of the al l caps, we
fold the case of all the terms in the message to have a
consistent casing. This is extremely important for us due
to the erratic casing, often found in microblogs.

@cvkkumar:the itunes store reeaaally rockss!!
<>HASALLCAPS<> :) http://bit.ly/madeup

C. URL Extraction

Most of the microblogs often point to much more elabo-
rate sources of information as a means of sharing content.

Since URLs are unique, and it would be too expensive to
crawl URLs for their content, we abstract the links to a
single keyword to avoid feature explosion.

@cvkkumar:the itunes store
<>HASALLCAPS<> :) <>URL<>

reeaaally rockss!!

D. Detection of Pointers

We deﬁne Pointer as the means of addressing a user
inside a microblog.
In Twitter, pointers take the form
of @user , where user is the username. Again, to avoid
explosion of features, we abstract it to a constant symbol.

<>PTR<>:the itunes store reeaaally rockss!!
<>HASALLCAPS<> :) <>URL<>

E. Identiﬁcation of Emoticons

Emoticons are one of the most powerful signals that we
use in our system to diﬀerentiate polar from non-polar mes-
sages and positive from negative messages. We identify a
range of emoticons and substitute the keyword SMILE if
it is positive and FROWN otherwise.

<>PTR<>:the itunes store reeaaally rockss!!
<>HASALLCAPS<> <>SMILE<> <>URL<>

F. Identiﬁcation of Punctuations

The punctuations are also a potential source of insight
into the polarity of the message. For instance, exclamation
marks are used to convey powerful emphasis which may
correlate to polar messages, as was observed. In this step
we remove the irrelevant punctuations as well so as to avoid
redundant noise in our feature set.

<>PTR<>:the itunes store reeaaally rockss <>EX<>
<>HASALLCAPS<> <>SMILE<> <>URL<>

G. Stop Word Removal

This is a standard Information Retrieval technique to
remove words that are highly common (have a high IDF
value) and hence do not add substantial value to the classi-
ﬁcation process. Common words like ”A”, ”An” and ”The”
fall in this category. In addition, the search term that is
bound to be present in every message is also added to this
list.

<>PTR<>: reeaaally rockss <>EX<>
<>HASALLCAPS<> <>SMILE<> <>URL<>

H. Compression of Words

We also realized that people tend to be very informal
while writing microblogs and most of the times elongate
the words to express strong emotions. For example SSS-
LLLOOOWWWW coveys a higher degree of power than
SLOW. However, both these modiﬁcations refer to the
same fact. We employ a heuristic in which we compress
such words to what we call as shadows. A shadow of a
word is just the compressed form of the word with at most
one character repeating consecutively twice. e.g. SSSLL-
LOOOWWWW gives out the shadows sslow, sl low, sloow,

2

sloww & slow where the last one is the most compressed or
the shortest shadow. In our experiments we also compare
the inclusion of either original, shortest or all shadows as
potential features. We later see that the shortest shadows
give the best results, an idea that is fairly intuitive.
Our initial empirical analysis of the kinds of feature vec-
tors obtained does seem to agree with the intuitive reason-
ing behind the incorporation of those features. After the
feature extraction and preprocessing, the message is con-
verted to a feature vector that can be used by the Feature
Selection algorithms and Classiﬁers.

V. Model Selection

Our Classiﬁcation mechanism consists of two Classiﬁers
as depicted in Fig-1. The ﬁrst one is the Neutral-Polar
Classiﬁer which is responsible for classifying the incoming
message as either ”Neutral” or ”Polar”. The Polarity Clas-
siﬁer’s job is to classify polar messages as either ”Positive”
or ”Negative”. We select the most popular algorithms for
feature selection and classiﬁcation and carry out our eval-
uation of each possible combination for each Classiﬁer. In
our experiments we randomly pick one Feature Selection
algorithm from the list below :
• Gain Ratio
• Information Gain
• χ2
• Symmetrical Uncertainty
and pair it up with a randomly selected classiﬁer to get a
model. A model consists of a trained classiﬁer and the fea-
ture space - selected by FeatureSet selection algorithm - to
which any new message should be translated for classiﬁca-
tion. We use the Feature Selection algorithm to select only
pre-determined number of best features and then transcribe
the training messages to the selected feature-space. This
is followed by training of the classiﬁer with the transcribed
training set using hold-out cross validation. The selected
feature subset and the trained classiﬁer instance is then
forwarded to the respective Classiﬁers .The ﬁve classiﬁers
we choose from - along with their possible conﬁguration
values - are listed below :
• SVM [ Kernels (RBF, Linear and Polynomial(2)),
C (0.5,1,1.5,2)]
• Voted Perceptron [Exponent(1-10)]
• Naive Bayes
• Bayesian Logistic Regression [Prior (Gaussian, Lapla-
cian)]
• AdaptiveBoosting[BaseClassiﬁer (Any of the above),
iterations (6,8,10)]
Thus randomly selecting a classiﬁer means selecting one
of the ﬁve algorithms above and then randomly selecting
its possible attributes. e.g. SVM (C = 0.5) with RBF
Kernel is one possible choice, AdaBoosting (6 iterations)
over Bayesian Logistic Regression with Laplacian prior is
another and SVM (C=1.0) with Polynomial Kernel is yet
another one.
In our model we treat each message as a bag-of-words
and account for only presence or absence of a word in the
message. Our initial experiments suggested that capturing

the count of times a word appeared in the message didn’t
help much. It conﬁrms the intuition that the likelihood of
a word repeating in micro-blogs is generally low since the
messages are themselves very short.

VI. Experiments and Observations

We tried a combination of diﬀerent feature selection and
classiﬁcation algorithms in our search for the best mod-
els for both the Classiﬁers. We trained over 2500 models
for Neutral-Polar (NP) Classiﬁer and an equal number of
models for Polarity-Classiﬁer. A self guided ’evaluator’
was coded which, at every iteration, randomly selects one
of all possible models and trains it over the given samples.
Once trained, the models were evaluated against the hold-
out cross validation set and the performance statistics were
captured in the database. Training time for each iteration
depended upon the nature of the feature-set selection algo-
rithm, the classiﬁer and the number of iterations wherever
boosting was applied. Training time generally varied from
anywhere in between a couple of minutes to a few hours.
While manually trying to train models for classiﬁcation
we realized that the ambiguous messages are playing a
key role in confusing the classiﬁers and making them per-
form badly. Even the human classiﬁers were were not so
sure about which category to put those ambiguous or not-
so-strong messages into. Discarding ambiguous messages
from the training set gave a signiﬁcant improvement in
classiﬁcation accuracy. Our initial manual training also
suggested that the optimal feature-set size was close to
1000 features.

A. Models for Neutral-Polar (NP) Classiﬁer

In order to generate the models for the neutral polar
classiﬁcation, there were two ma jor selections to be made.
Feature Selection Algorithm Given a set of features,
what is the best way to select the most discriminating
features amongst them?
Classiﬁer For the given data set, what is the best
model?

A.1 Feature Selection Algorithm Selection

Feature selection is the process of choosing the most dis-
criminative features so as to enable the classiﬁer to perform
better. This becomes especially relevant in the case of text
classiﬁcation where there are an extremely large number of
features. The diﬀerent feature selection algorithms that we
considered along with the average F1 scores of the mod-
els that they were associated with is summarized in table
below.

Feature Selection Algo Avg F1
0.1994
Gain Ratio
Information Gain
0.4581
0.4696
Symmetric Uncertainty
χ2
0.5031

The averages in the above table were calculated over
2500 diﬀerent models, each of which was evaluated using
the hold-out cross validation technique.

3

A.2 Classiﬁer Selection

We started with individual classiﬁers and later applied
the boosting based ensemble methods to deal with imbal-
ance in the data set as suggested by [6], [7]. Imbalance in
training set also makes the accuracy measure of a model in-
signiﬁcant hence we focus on the recall, false-positive-rate
and F1 measure of each model for comparison. After ﬁlter-
ing the ambiguous messages our training-set was reduced
to around 2500 polar messages and over 10000 neutral mes-
sages.
The observed performance of the classiﬁers without any
boosting is given in the following table.

Classiﬁer
BLR(Gauss)
BLR(Laplace)
Naive Bayes
SVM(Linear)
SVM(Poly)
SVM(RBF)
Voted Perceptron

Avg F1 Avg Acc. Max Acc.
0.432
76.02
80.09
83.00
78.39
0.454
81.55
78.66
0.4985
83.42
80.03
0.4700
0.4536
79.71
81.97
79.4
78.05
0.0864
0.4300
78.64
83.66

It was very astonishing to see SVM with the RBF kernel
perform so badly. One reason of this low accuracy could
be over-ﬁtting of the model to the training data. Hence
we remove the SVM(RBF) from further consideration here.

When boosting is applied along with oversampling of
minority samples and undersampling ma jority samples ,
we get the following results for the classiﬁers.

Classiﬁer
BLR(Gauss)
BLR(Laplace)
Naive Bayes
SVM(Linear)
SVM(Poly)
SVM(RBF)
Voted Perceptron

Avg F1 Avg Acc. Max.Acc.
85.51
80.09
0.515
87.04
79.98
0.510
84.18
75.63
0.563
85.44
80.2
0.557
0.499
79.22
83.37
-
-
-
0.508
78.70
83.62

Fig. 2. Accuracy of all models for NP Classiﬁer

Fig. 3. Legend for Fig-4,2,7,8,9

Fig. 4. All Models’ F1 Scores - NP Classiﬁer

The Fig-3 represents the legend for Figures 4,2,7,8,9.
The Figures 4 and 2 represents the F1 and accuracy scores
for all of the NP classiﬁers that we trained. It is interesting
to see how the triangles and ’+’s - representing boosted-
classiﬁers - pervade the top portion of the plots. Though
maximum accuracies are high, the F1 scores are relatively
low since either the recall is low or the false-positive rate
is considerably high.
Based on these observations, we can clearly see that
though, in both the cases (with and without boosting) the
accuracy is high whereas the F1 score is pretty low. This
happens because of the data skew that is present in our
dataset.

A.3 Actions To Counter Skew and Observations

There are a set of observations that we gathered from
these runs of the classiﬁers.
Of-the-shelf Classiﬁers : Classiﬁers with no boosting,
oversampling or undersampling gave lower recall with
relatively lower false positive rate. While in some
cases it may work but generally not being able to iden-
tify the minority class with a good success rate is not
acceptable - which is true in our case as well.
Boosting: On an average, classiﬁers with Boosting had
a bit of
improvement over the classiﬁers without
boosting. This is understandable since boosting is
supposed to enhance the accuracy by making a rela-
tively stronger classiﬁer out of a committee of weaker
classiﬁers.
Oversampling with Boosting : Oversampling is a tech-

4

nique that we attempted to use against the data skew.
It involves the duplication of the already existing mi-
nority class instances so that the sizes of the classes
become comparable. We saw some improvement with
this over the scenario where we had just boosting.
Undersampling with Boosting : Undersampling
a
is
technique in which some samples of the ma jority class
are discarded so that the sizes of the classes become
comparable. We ﬁnd that even undersampling gives
only a slight improvement over boosting.
Both Undersampling and Oversampling with Boosting :
This gives better results than any of the methods
given above, and has yet been our most successful tool
to counter the skew in the data and address the recall
issue. With this we got over 85% recall in some cases
though with the cost of a higher false-positive-rate.
SMOTE : SMOTE is proposed by Chawla et al in [9].
It tries to counter the imbalance in the skewed dataset
by synthetically generating the samples for minority
class. We brieﬂy tried SMOTE but didn’t see much
improvement.
We can see from the ROC plots in Fig-5 that for the
models trained without boosting, oversampling or under-
sampling and with 1000 features the maximum recall at-
tained is close to 65%. The points on the lower left corner
of the plot correspond to either feature selection based on
Gain Ratio and/or SVM classiﬁer with RBF kernel. Re-
peating this test for all the possible models with 500 and
1500 features did not give much improvements.

65% recall while also giving less than 20% false-positive
rate

Fig. 6. Classiﬁer Performance - Boosting, Oversampling

B. Models for Polarity Classiﬁcation

Polarity classiﬁcation was not plagued with the class im-
balance problem in our case. Although we have very small
training set - of 2500 samples with almost equal number
of positive and negative messages - to train the classiﬁers
with. The resulting models, their accuracies and the F1
Scores can be found in Fig-7 and 8

Fig. 5. Classiﬁer Performance - No Boosting, Oversampling or Un-
dersampling

We then applied the boosting techniques to get a better
recall for the minority class. We saw that oversampling
the polar messages and undersampling neutral ones im-
proved the overall recall of all the possible models but at
the same time also increased the false positive rate. As
show in Fig-6 oversampling the polar messages to become
equivalent to the under-sampled neutral message gave us
the highest recall of over 85% with the false-negative rate
touching about 30%, which we consider to be very high.
NaiveBayes and SMO gave the best recalls. VotedPercep-
tron and BayesianLogisticRegression always gave less than

Fig. 7. Accuracy-All Models-Polarity Classiﬁer

We again saw that SVM with RBF kernel is performing
poorly and gives high bias towards any one class in each
run. As can be see in Fig-10 RBFs either give a very high
FalsePositive Rate or a very poor recall.

5

Fig. 8. All F1 Scores-Polarity Classiﬁer

Fig. 9. Top F1 Scores-Polarity Classiﬁer

Classiﬁer
BLR(Gauss)
BLR(Laplace)
Naive Bayes
SVM(Linear)
SVM(Poly)
SVM(RBF)
Voted Perceptron

Avg.F1 Avg.Acc. Max.Acc.
84.496
0.879
83.52
82.94
80.098
0.857
0.882
83.91
84.80
85.736
83.29
0.880
82.946
80.80
0.8571
74.573
68.22
0.757
0.875
82.96
85.43

SVM with Polynomial Kernel gives a reasonably good
performance for Polarity classiﬁer but not as good as the
one with linear kernel. We also observed that Naive Bayes
gives only marginally better performance than SVM. Voted
Perceptron and Bayesian Logistic Regression also seem
to perform comparably. Also, the best performance is
achieved by considering either the shortest shadows of the
words or all shadows. Also, SVM and Voted Perceptron
give better performance with both InformationGain and
χ2 based feature selection as opposed to Naive Bayes that
seem to favor χ2 over Information Gain. Boosting based
ensembling methods seem to give no signiﬁcant perfor-
mance gain with any classiﬁer in this case.

6

Fig. 10. Classiﬁer Performance - SVM-RBF & LogisticRegression-
LAPLACIAN

VII. Future Work

Our future eﬀorts will be focused on ﬁnding a model
for Neutral-Polar Classiﬁer that gives a better F1 score.
As of this writing, models with high Recal l also give a
high False Positive Rate whereas the ones with a low False
Positive Rate have a lower Recal l. Boosting based ensem-
bling methods helped in uplifting the F1 score but we be-
lieve that we can get even better performance by applying
smarter heuristics and other techniques. For instance, we
believe that applying Natural Language Processing tech-
niques might help in identifying the context and nature of
the phrases thus assisting in ﬁnding similarities. Although
our initial guess is that it might still not give a very high
performance gain because microblogs are very short and
not very well structured making them further diﬃcult to
be analyzed with language parsing techniques.
We also believe that considering 2-Grams or 3-Grams
might give us slightly better performance. The intuition is
that the word limit on the microblogs increases the proba-
bility of the messages that are closer in intent to be closer
in syntax.
We brieﬂy tried SMOTE to synthetically generate the
samples for minority-class oversampling but without much
success. We believe that we can give SMOTE another try
after understanding its nuances. Coming up with our own
method of synthetically generating minority class data can
also prove to be useful.
Other areas that we can look for improvements are the
PreProcessing of messages and Feature Selection that can
help better discriminate the messages.
We also saw that SVMs gave a very good precision but
were plagued with lower recall. We can consider intro-
ducing bias in SVMs towards the minority class by using
diﬀerent error cost functions for the two classes thus penal-
izing heavily for any minority class misclassiﬁcation. Fur-
ther, using SMOTE with this technique can give better
performance as suggested in [8].
Another possible solution is to use SVM with a Kernel
that exploits the semantic relations in between the words.

VIII. Conclusion

In this work, we analyzed the problem of Sentiment Clas-
siﬁcation of Micro-blogs, that is signiﬁcantly diﬀerent from

the other usual sentiment classiﬁcation problem on struc-
tured and detailed messages. We used a two-classiﬁer ap-
proach where the ﬁrst classiﬁer was a neutral-polar classi-
ﬁer and the second was a positive-negative(polarity) clas-
siﬁer. We tried diﬀerent models, diﬀerent parameters and
diﬀerent feature selection algorithms to address the prob-
lem. The biggest unsolved challenge for the neutral polar
classiﬁer is the imbalance in the dataset that gives a high
accuracy but low recall. In an attempt to counter this ef-
fect we tried diﬀerent re-balancing, sampling and boosting
algorithms, the most successful of which is the combination
of the boosting, oversampling and under-sampling taken
together. We have been able to ﬁnd a range of models from
the ones giving a low false-positive rate but a lower recall
to the ones with higher recall but higher false-positive rate.
Some applications may beneﬁt from the ﬁrst extreme of the
models and some may from the other but the best model
is yet to be discovered that gives an optimal F1 score. In
the case of Polarity Classiﬁer may combinations seem to
perform well with Naive Bayes and SVM being marginally
better than the others. This cascade of a balanced neutral
polar classiﬁer, coupled with an eﬃcient positive negative
classiﬁer marks our approach to this problem and leaves a
wide scope for further research.

References

[1] Weka : Data Mining Software in Java. The University of Waikato
http://www.cs.waikato.ac.nz/ml/weka/
[2] Bo Pang and Lillian Lee. Opinion Mining and Sentiment Anal-
ysis
[3] S. R. Das and M. Y. Chen Yahoo!
for Amazon: Sentiment
extraction from smal l talk on the Web, Management Science,
vol. 53, pp. 13751388, 2007
[4] G. Forman An extensive empirical study of feature selection met-
rics for text classiﬁcation, Journal of Machine Learning Research,
vol.3, pp. 12891305, 2003.
[5] Bo Pang, Lillian Lee and Shivkumar Vaithyanathan Thumbs
up? Sentiment classication using machine learning techniques,
Proceedings of EMNLP,2002
[6] Mahesh V. Joshi, Ramesh C. Agarwal and Vipin Kumar Pre-
dicting Rare Classes: Can Boosting Make Any Weak Learner
Strong?
[7] Chris Seiﬀert, Taghi M. Khoshgoftaar, Jason Van Hulse and Amri
Napolitano Building Useful Models from Imbalanced Data with
Sampling and Boosting
[8] Rehan Akbani, Stephen Kwek, Nathalie Japkowicz Applying Sup-
port Vector Machines to Imbalanced Datasets
[9] Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, W.
Philip Kegelmeyer SMOTE: Synthetic Minority Over-sampling
Technique

7


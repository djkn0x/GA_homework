From PAC-Bayes Bounds to KL Regularization

Pascal Germain, Alexandre Lacasse, Franc¸ ois Laviolette, Mario Marchand, Sara Shanian
Department of Computer Science and Software Engineering
Laval University, Qu ´ebec (QC), Canada
firstname.secondname@ift.ulaval.ca

Abstract

We show that convex KL-regularized objective functions are obtained from a
PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs
classiﬁer that upper-bound the standard zero-one loss used for the weighted ma-
jority vote. By restricting ourselves to a class of posteriors, that we call quasi
uniform, we propose a simple coordinate descent learning algorithm to minimize
the proposed KL-regularized cost function. We show that standard (cid:96)p -regularized
objective functions currently used, such as ridge regression and (cid:96)p -regularized
boosting, are obtained from a relaxation of the KL divergence between the quasi
uniform posterior and the uniform prior. We present numerical experiments where
the proposed learning algorithm generally outperforms ridge regression and Ada-
Boost.

1

Introduction

What should a learning algorithm optimize on the training data in order to give classiﬁers having the
smallest possible true risk? Many different speciﬁcations of what should be optimized on the train-
ing data have been provided by using different inductive principles. But the universally accepted
guarantee on the true risk, however, always comes with a so-called risk bound that holds uniformly
over a set of classiﬁers. Since a risk bound can be computed from what a classiﬁer achieves on the
training data, it automatically suggests that learning algorithms should ﬁnd a classiﬁer that mini-
mizes a tight risk (upper) bound.
Among the data-dependent bounds that have been proposed recently, the PAC-Bayes bounds [6, 8,
4, 1, 3] seem to be especially tight. These bounds thus appear to be a good starting point for the
design of a bound-minimizing learning algorithm. In that respect, [4, 5, 3] have proposed to use
isotropic Gaussian posteriors over the space of linear classiﬁers. But a computational drawback of
this approach is the fact the Gibbs empirical risk is not a quasi-convex function of the parameters
of the posterior. Consequently, the resultant PAC-Bayes bound may have several local minima for
certain data sets—thus giving an intractable optimization problem in the general case. To avoid
such computational problems, we propose here to use convex loss functions for stochastic Gibbs
classiﬁers that upper-bound the standard zero-one loss used for the weighted majority vote. By
restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coor-
dinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show
that there are no loss of discriminative power by restricting the posterior to be quasi uniform. We
also show that standard (cid:96)p -regularized objective functions currently used, such as ridge regression
and (cid:96)p -regularized boosting, are obtained from a relaxation of the KL divergence between the quasi
uniform posterior and the uniform prior. We present numerical experiments where the proposed
learning algorithm generally outperforms ridge regression and AdaBoost [7].

1

2 Basic Deﬁnitions
We consider binary classiﬁcation problems where the input space X consists of an arbitrary subset
of Rd and the output space Y = {−1, +1}. An example is an input-output (x, y) pair where x ∈ X
and y ∈ Y . Throughout the paper, we adopt the PAC setting where each example (x, y) is drawn
according to a ﬁxed, but unknown, distribution D on X × Y .
The risk R(h) of any classiﬁer h : X → Y is deﬁned as the probability that h misclassiﬁes an
example drawn according to D . Given a training set S of m examples, the empirical risk RS (h) of
m(cid:88)
any classiﬁer h is deﬁned by the frequency of training errors of h on S . Hence
I (h(x) (cid:54)= y)
I (h(xi ) (cid:54)= yi ) ,
i=1

; RS (h) def=

E
(x,y)∼D

R(h) def=

1
m

where I (a) = 1 if predicate a is true and 0 otherwise.
After observing the training set S , the task of the learner is to choose a posterior distribution Q
over a space H of classiﬁers such that the Q-weighted majority vote classiﬁer BQ will have the
(cid:21)
(cid:20)
smallest possible risk. On any input example x, the output BQ (x) of the majority vote classiﬁer BQ
(sometimes called the Bayes classiﬁer) is given by
BQ (x) def= sgn
h(x)
E
,
h∼Q
where sgn(s) = +1 if s > 0 and sgn(s) = −1 otherwise. The output of the deterministic majority
vote classiﬁer BQ is closely related to the output of a stochastic classiﬁer called the Gibbs classiﬁer
GQ . To classify an input example x, the Gibbs classiﬁer GQ chooses randomly a (deterministic)
classiﬁer h according to Q to classify x. The true risk R(GQ ) and the empirical risk RS (GQ ) of the
Gibbs classiﬁer are thus given by
; RS (GQ ) = E
R(GQ ) = E
h∼Q
h∼Q
Any bound for R(GQ ) can straightforwardly be turned into a bound for the risk of the majority vote
R(BQ ). Indeed, whenever BQ misclassiﬁes x, at least half of the classiﬁers (under measure Q)
misclassiﬁes x. It follows that the error rate of GQ is at least half of the error rate of BQ . Hence
R(BQ ) ≤ 2R(GQ ). As shown in [5], this factor of 2 can sometimes be reduced to (1 + ).

RS (h) .

R(h)

3 PAC-Bayes Bounds and General Loss Functions

In this paper, we use the following PAC-Bayes bound which is obtained directly from Theorem 1.2.1
of [1] and Corollary 2.2 of [3] by using 1 − exp(−x) ≤ x ∀x ∈ R.
Theorem 3.1. For any distribution D , any set H of classiﬁers, any distribution P of support H,
(cid:105)(cid:21)(cid:19)
(cid:20)
(cid:18)
(cid:104)
any δ ∈ (0, 1], and any positive real number C (cid:48) , we have
1
KL(Q(cid:107)P ) + ln
C (cid:48) ·RS (GQ ) +
∀ Q on H : R(GQ ) ≤
Pr
1 − e−C (cid:48)
S∼Dm
where KL(Q(cid:107)P ) def= E
ln Q(h)
P (h) is the Kullback-Leibler divergence between Q and P .
h∼Q

≥ 1 − δ ,

1
m

1
δ

Note that the dependence on Q of the upper bound on R(GQ ) is realized via Gibbs’ empirical risk
RS (GQ ) and the PAC-Bayes regularizer KL(Q(cid:107)P ). As in boosting, we focus on the case where
the a priori deﬁned class H consists (mostly) of “weak” classiﬁers having large risk R(h) . In this
case, R(GQ ) is (almost) always large (near 1/2) for any Q even if the majority vote BQ has null
risk. In this case the disparity between R(BQ ) and R(GQ ) is enormous and the upper-bound on
R(GQ ) has very little relevance with R(BQ ). On way to obtain a more relevant bound on R(BQ )
from PAC-Bayes theory is to use a loss function ζQ (x, y) for stochastic classiﬁers which is distinct
from the loss used for the deterministic classiﬁers (the zero-one loss in our case). In order to obtain
a tractable optimization problem for a learning algorithm to solve, we propose here to use a loss
ζQ (x, y) which is convex in Q and that upper-bounds as closely as possible the zero-one loss of the
deterministic majority vote BQ .

2

ak

,

E
h∼Q

− yh(x)

ζQ (x, y) def= 1 +

Consider WQ (x, y) def= Eh∼Q I (h(x) (cid:54)= y), the Q-fraction of binary classiﬁers that err on exam-
ple (x, y). Then, R(GQ ) = E(x,y)∼D WQ (x, y). Following [2], we consider any non-negative
(cid:18)
(cid:19)k
∞(cid:88)
∞(cid:88)
convex loss ζQ (x, y) that can be expanded in a Taylor series around WQ (x, y) = 1/2:
ak (2WQ (x, y) − 1)k = 1 +
k=1
k=1
(cid:19)
(cid:18)
that upper bounds the risk of the majority vote BQ , i.e.,
ζQ (x, y) ≥ I

1
∀Q, x, y .
WQ (x, y) >
2
It has been shown [2] that ζQ (x, y) can be expressed in terms of the risk on example (x, y) of a
(cid:104)
(cid:105)
Gibbs classiﬁer described by a transformed posterior Q on N × H∞ , i.e.,
2WQ (x, y) − 1
def= (cid:80)∞
ζQ (x, y) = 1 + ca
k=1 |ak | and where
(cid:17)
(cid:16)
∞(cid:88)
1
(−y)k h1 (x) . . . hk (x) = −sgn(ak )
|ak | E
WQ (x, y) def=
h1∼Q
ca
k=1
(cid:80)m
Since WQ (x, y) is the expectation of boolean random variable, Theorem 3.1 holds if we replace
(P , Q) by (P , Q) with R(GQ ) def=
WQ (x, y) and RS (GQ ) def= 1
i=1 WQ (xi , yi ). More-
E
(x,y)∼D
m
∞(cid:88)
over, it has been shown [2] that
KL(Q(cid:107)P ) = k · KL(Q(cid:107)P ) , where k def=
k=1

. . . E
hk∼Q

|ak | · k .

where ca

1
ca

,

I

.

If we deﬁne

def=

def=

ζ (x, y) = 1 + ca [2R(GQ ) − 1]

m(cid:88)
E
ζQ
(x,y)∼D
(cid:99)ζQ
1
ζ (xi , yi ) = 1 + ca [2RS (GQ ) − 1] ,
m
i=1
Theorem 3.1 gives an upper bound on ζQ and, consequently, on the true risk R(BQ ) of the majority
vote. More precisely, we have the following theorem.
Theorem 3.2. For any D , any H, any P of support H, any δ ∈ (0, 1], any positive real number C (cid:48) ,
(cid:105)(cid:21)(cid:19)
(cid:18)
(cid:20)(cid:99)ζQ +
(cid:104)
any loss function ζQ (x, y) deﬁned above, we have
C (cid:48)
∀ Q on H : ζQ ≤ g(ca , C (cid:48) ) +
Pr
1 − e−C (cid:48)
S∼Dm
where g(ca , C (cid:48) ) def= 1 − ca + C (cid:48)
1−e−C (cid:48) · (ca − 1).
4 Bound Minimization Learning Algorithms

k · KL(Q(cid:107)P ) + ln

≥ 1 − δ ,

2ca
mC (cid:48)

1
δ

The task of the learner is to ﬁnd the posterior Q that minimizes the upper bound on ζQ for a ﬁxed
loss function given by the coefﬁcients {ak }∞
k=1 of the Taylor series expansion for ζQ (x, y). Finding
m(cid:88)
Q that minimizes the upper bound given by Theorem 3.2 is equivalent to ﬁnding Q that minimizes
i=1

ζQ (xi , yi ) + KL(Q(cid:107)P ) ,

f (Q) def= C

where C def= C (cid:48)/(2cak) .

3

y

exp

= exp

[2WQ (x, y) − 1]

To compare the proposed learning algorithms with AdaBoost, we will consider, for ζQ (x, y), the
(cid:32)
(cid:33)
(cid:18) 1
(cid:19)
(cid:88)
exponential loss given by
− 1
Q(h)h(x)
γ
γ
h∈H
For this choice of loss, we have ca = eγ−1 − 1 and k = γ−1/(1 − e−γ−1 ). Because of its simplicity,
(cid:32)
(cid:33)2
(cid:18) 1
(cid:19)2
we will also consider, for ζQ (x, y), the quadratic loss given by
(cid:88)
1
[1 − 2WQ (x, y)] − 1
Q(h)h(x) − 1
=
y
γ
γ
h∈H
has the minimum value of zero for examples having a margin y (cid:80)
For this choice of loss, we have ca = 2γ−1 + γ−2 and k = (2γ + 2)/(2γ + 1). Note that this loss
h∈H Q(h)h(x) = γ .
With these two choices of loss functions, ζQ (x, y) is convex in Q. Moreover, KL(Q(cid:107)P ) is also
convex in Q. Since a sum of convex functions is also convex, it follows that objective function f
as it is done for AdaBoost [7]. However, to ensure that Q is a distribution (i.e., that (cid:80)
is convex in Q (which has a convex domain). Consequently, f has a single local minimum which
coincides with the global minimum. We therefore propose to minimize f coordinate-wise, similarly
h∈H Q(h) =
1), each coordinate minimization will consist of a transfer of weight from one classiﬁer to another.

.

.

4.1 Quasi Uniform Posteriors
We consider learning algorithms that work in a space H of binary classiﬁers such that for
each h ∈ H, the boolean complement of h is also in H. More speciﬁcally, we have H =
{h1 , . . . , hn , hn+1 , . . . , h2n} where hi (x) = −hn+i (x) ∀x ∈ X and ∀i ∈ {1, . . . , n}. We thus
say that (hi , hn+i ) constitutes a boolean complement pair of classiﬁers.
We consider a uniform prior distribution P over H, i.e., Pi = 1
2n ∀i ∈ {1, . . . , 2n}.
The posterior distribution Q over H is constrained to be quasi uniform. By this, we mean that
n ∀i ∈ {1, . . . , n}, i.e., the total weight assigned to each boolean complement pair of
Qi + Qi+n = 1
def= Qi − Qi+n ∀i ∈ {1, . . . , n}. Then wi ∈ [−1/n, +1/n] ∀i ∈
classiﬁers is ﬁxed to 1/n. Let wi
{1, . . . , n} whereas Qi ∈ [0, 1/n] ∀i ∈ {1, . . . , 2n}.
(cid:17)
(cid:16)
(cid:17) def= sgn
(cid:16) n(cid:88)
(cid:17)
(cid:16) 2n(cid:88)
For any quasi uniform Q, the output BQ (x) of the majority vote on any example x is given by
w · h(x)
wihi (x)
= sgn
Qihi (x)
BQ (x) = sgn
i=1
i=1
Consequently, the set of majority votes BQ over quasi uniform posteriors is isomorphic to the set
Since all loss functions that we consider are functions of 2WQ (x, y) − 1 = −y (cid:80)
of linear separators with real weights. There is thus no loss of discriminative power if we restrict
ourselves to quasi uniform posteriors.
i Qihi (x), they
are thus functions of yw · h(x). Hence we will often write ζ (yw · h(x)) for ζQ (x, y).
The basic iteration for the learning algorithm consists of choosing (at random) a boolean com-
plement pair of classiﬁers, call it (h1 , hn+1 ), and then attempting to change only Q1 , Qn+1 , w1
according to:

.

; Qn+1 ← Qn+1 − δ
Q1 ← Q1 + δ
2
2
for some optimally chosen value of δ .
Let Qδ and wδ be, respectively, the new posterior and the new weight vector obtained with such a
change. The above-mentioned convex properties of objective function f imply that we only need to
look for the value of δ∗ satisfying

; w1 ← w1 + δ ,

(1)

= 0 .

(2)

df (Qδ )
dδ

4

,

(cid:35)

where

where

(cid:21)

= 0 .

m

where

(3)

(4)

(cid:19)

ln

w (i)yih1 (xi ) ,
Dql

+

Qn+1 − δ
2

(5)

(6)

(7)

2mδ
γ 2 +

2
γ 2

w (i)yih1 (xi ) +
Dql

ln

dKL(Qδ (cid:107)P )
dδ

= d
dδ
1
ln
2
d(cid:100)ζQδ
For the quadratic loss, we ﬁnd
dδ

=

m

Qn+1 − δ
2
1
2n

+ dKL(Qδ (cid:107)P )
(cid:18)
dδ

If w1 + δ∗ > 1/n, then w1 ← 1/n, Q1 ← 1/n, Qn+1 ← 0. If w1 + δ∗ < −1/n, then w1 ←
−1/n, Q1 ← 0, Qn+1 ← 1/n. Otherwise, we accept the change described by Equation 1 with
δ = δ∗ .
d(cid:100)ζQδ
For objective function f we simply have
df (Qδ )
(cid:34)(cid:18)
= Cm
(cid:19)
dδ
dδ
Q1 + δ
Q1 + δ
(cid:21)
(cid:20) Q1 + δ/2
ln
2
2
1
2n
Qn+1 − δ/2

m(cid:88)
i=1
def= yiw · h(xi ) − γ .
(cid:20) Q1 + δ/2
w (i)
Dql
m(cid:88)
Consequently, for the quadratic loss case, the optimal value δ∗ satisﬁes
2C
1
2Cmδ
γ 2 +
Qn+1 − δ/2
2
γ 2
i=1
m(cid:88)
m(cid:88)
d(cid:100)ζQδ
For the exponential loss, we ﬁnd
w (i)I (h1 (xi ) (cid:54)= yi ) − e−δ/γ
= eδ/γ
Del
(cid:18)
(cid:19)
γ
γ
dδ
i=1
i=1
− 1
yiw · h(xi )
def= exp
w (i)
Del
.
γ
m(cid:88)
Consequently, for the exponential loss case, the optimal value δ∗ satisﬁes
C eδ/γ
w (i)I (h1 (xi ) (cid:54)= yi )
(cid:20) Q1 + δ/2
Del
m(cid:88)
γ
i=1
− C e−δ/γ
1
Qn+1 − δ/2
2
γ
i=1
After changing w1 , we need to recompute1 Dw (i) ∀i ∈ {1, . . . , m}. This can be done with the
following update rules.
w (i) ← Dql
w (i) + yih1 (xi )δ
Dql
w (i) ← Del
− 1
w (i)e
γ yi h1 (xi )δ
Del
Since, initially we have
w (i) = −γ ∀i ∈ {1, . . . , m}
(13)
(quadratic loss case)
Dql
w (i) = 1 ∀i ∈ {1, . . . , m}
(14)
(exponential loss case) ,
Del
the dot product present in Equations 6 and 9 never needs to be computed. Consequently, updating
Dw takes Θ(m) time.
The computation of the summations over the m examples in Equation 7 or 10 takes Θ(m) time.
Once these summations are computed, solving Equation 7 or 10 takes Θ(1) time. Consequently,
it takes Θ(m) time to perform one basic iteration of the learning algorithm which consist of (1)
solving Equation 7 or 10 to ﬁnd δ∗ , (2) modifying w1 , Q1 , Qn+1 , and (3) updating Dw according to
Equation 11 or 12. The complete algorithm, called f minimization, is described by the pseudo code
of Algorithm 1.
1Dw (i) stands for either Dql
w (i) or Del
w (i).

(quadratic loss case)
(exponential loss case) .

w (i)I (h1 (xi ) = yi ) +
Del

w (i)I (h1 (xi ) = yi ) ,
Del

ln

= 0 .

(10)

(8)

(9)

(11)
(12)

(cid:21)

.

=

5

Algorithm 1 : f minimization
2n , wi = 0, ∀i ∈ {1, . . . , n}.
1: Initialization: Let Qi = Qn+i = 1
Initialize Dw according to Equation 13 or 14.

5:

4:

2: repeat
Choose at random h ∈ H and call it h1 (hn+1 is then the boolean complement of h1 ).
3:
Find δ∗ that solves Equation 7 or 10.
n ] then Q1 ← Q1 + δ/2; Qn+1 ← Qn+1 − δ/2; w1 ← w1 + δ .
If [ −1
n < w1 + δ∗ < 1
If [w1 + δ∗ ≥ 1
n ] then Q1 ← 1
n ; Qn+1 ← 0; w1 ← 1
n .
If [w1 + δ∗ ≤ −1
n ] then Q1 ← 0; Qn+1 ← 1
n ; w1 ← −1
n .
Update Dw according to Equation 11 or 12.
8:
9: until Convergence

6:

7:

ln

ln

ζ

− q

ln

− q

1
2n

− Qi

q ln q +

q − 1
2n
(cid:19)

The repeat-until loop in Algorithm 1 was implemented as follows. We ﬁrst mix at random the n
boolean complement pairs of classiﬁers and then go sequentially over each pair (hi , hn+i ) to update
wi and Dw . We repeat this sequence until no weight change exceeds a speciﬁed small number .
4.2 From KL(Q(cid:107)P ) to (cid:96)p Regularization
We can recover (cid:96)2 regularization if we upper-bound KL(Q(cid:107)P ) by a quadratic function. Indeed, if
(cid:18) 1
(cid:18) 1
(cid:19)
(cid:19)
(cid:18)
(cid:19)2 ∀q ∈ [0, 1/n] ,
we use
≤ 1
+ 4n
n
n
n
(cid:18) 1
(cid:19)(cid:21)
(cid:18) 1
(cid:20)
n(cid:88)
we obtain, for the uniform prior Pi = 1/(2n),
− Qi
KL(Q(cid:107)P ) = ln(2n) +
(cid:18)
(cid:19)2
Qi ln Qi +
n(cid:88)
n(cid:88)
n
n
i=1
Qi − 1
≤ 4n
w2
i .
2n
i=1
i=1
(cid:18) 1
(cid:19)
f(cid:96)2 (w) = C (cid:48)(cid:48) m(cid:88)
With this approximation, the objective function to minimize becomes
+ (cid:107)w(cid:107)2
yiw · h(xi )
2 ,
γ
i=1
subject to the (cid:96)∞ constraint |wj | ≤ 1/n ∀j ∈ {1, . . . , n}. Here (cid:107)w(cid:107)2 denotes the Euclidean norm
of w and ζ (x) = (x − 1)2 for the quadratic loss and e−x for the exponential loss.
If, instead, we minimize f(cid:96)2 for v def= w/γ and remove the (cid:96)∞ constraint, we recover exactly ridge
regression for the quadratic loss case and (cid:96)2 -regularized boosting for the exponential loss case.
(cid:1)2 ≤ 2 (cid:12)(cid:12)q − 1
(cid:12)(cid:12) ∀q ∈ [0, 1/n] since, in that case, we ﬁnd that KL(Q(cid:107)P ) ≤
ing 4n (cid:0)q − 1
We can obtain a (cid:96)1 -regularized version of Equation 17 by repeating the above steps and us-
(cid:80)n
2n
2n
i=1 |wi | def= (cid:107)w(cid:107)1 .
To sum up, the KL-regularized objective function f immediately follows from PAC-Bayes theory
and (cid:96)p regularization is obtained from a relaxation of f . Consequently, PAC-Bayes theory favors
the use of KL regularization if the goal of the learner is to produce a weighted majority vote with
good generalization.2

= n

(15)

(16)

(17)

2 Interestingly, [9] has recently proposed a KL-regularized version of LPBoost but their objective function
was not derived from a uniform risk bound.

6

5 Empirical Results

For the sake of comparison, all learning algorithms of this subsection are producing a weighted
majority vote classiﬁer on the set of basis functions {h1 , . . . , hn} known as decision stumps. Each
decision stump hi is a threshold classiﬁer that depends on a single attribute: its output is +b if
the tested attribute exceeds a threshold value t, and −b otherwise, where b ∈ {−1, +1}. For each
attribute, at most ten equally-spaced possible values for t were determined a priori. Recall that,
although Algorithm 1 needs a set H of 2n classiﬁers containing n boolean complement pairs, it
outputs a majority vote with n real-valued weights deﬁned on {h1 , . . . , hn}.
The results obtained for all tested algorithms are summarized in Table 1. We have compared Al-
gorithm 1 with quadratic loss (KL-QL) and exponential loss (KL-EL) to AdaBoost [7] (AdB) and
ridge regression (RR).
Except for MNIST, all data sets were taken from the UCI repository. Each data set was randomly
split into a training set S of |S | examples and a testing set T of |T | examples. The number a
of attributes for each data set is also speciﬁed in Table 1. For AdaBoost, the number of boosting
rounds was ﬁxed to 200. For all algorithms, RT refers to the frequency of errors, measured on the
testing set T .
In addition to this, the “C and “γ ” columns in Table 1 refer, respectively, to the C value of the
objective function f and to the γ parameter present in the loss functions. These hyperparameters
were determined from the training set only by performing the 10-fold cross validation (CV) method.
The hyperparameters that gave the smallest 10-fold CV error were then used to train the Algorithms
on the whole training set and the resulting classiﬁers were then run on the testing set.

Dataset
|T |
|S |
Name
a
9
BreastCancer 343 340
170 175
Liver
6
353 300 15
Credit-A
Glass
107 107
9
144 150
Haberman
3
150 147 13
Heart
176 175 34
Ionosphere
500 1055 16
Letter:AB
Letter:DO
500 1058 16
Letter:OQ
500 1036 16
MNIST:0vs8 500 1916 784
MNIST:1vs7 500 1922 784
MNIST:1vs8 500 1936 784
MNIST:2vs3 500 1905 784
Mushroom 4062 4062 22
3700 3700 20
Ringnorm
104 104 60
Sonar
Usvotes
235 200 16
4000 4000 21
Waveform
Wdbc
285 284 30

Table 1: Summary of results.
(4) KL-QL
(1) AdB (2) RR
(3) KL-EL
RT
γ
C
RT C RT C
RT
γ
0.047 0.02 0.4
0.050 10 0.047 0.1
0.053
0.1
0.286 0.02 0.3
0.360 0.5 0.02
0.309 5
0.320
0.157 2
0.183 0.02 0.05
0.170
0.227 0.1
0.2
0.178
0.196 0.02 0.01
0.206 5
0.187 500 0.01
0.273 100 0.253 500
0.260 0.02 0.5
0.2
0.260
0.177 0.05 0.2
0.197 1
0.252
0.211 0.2
0.1
0.131 0.05 0.120 20 0.0001 0.097 0.2 0.1
0.120
0.004 0.5 0.006 0.1 0.02
0.010
0.006 1000 0.1
0.026 0.05 0.019 500 0.01
0.036
0.020 0.02 0.05
0.038
0.045 0.5 0.043 10 0.0001 0.047 0.1 0.05
0.015 0.05 0.006 500 0.001 0.015 0.2 0.02
0.008
0.012 1
0.014 1000 0.1
0.013
0.014 500 0.02
0.024 0.2 0.016 0.2 0.001 0.031
0.025
1
0.02
0.033 0.2 0.035 500 0.0001 0.029 0.02 0.05
0.047
0.001 0.5 0.000 10 0.001 0.000 1000 0.02
0.000
0.037 0.05 0.025 500 0.01
0.039 0.05 0.05
0.043
0.115 1000 0.1
0.192 0.05 0.135 500 0.05
0.231
0.055
0.055 1000 0.05
0.060 2
0.060 0.5
0.1
0.079 0.02 0.080 0.2 0.05
0.080 0.02 0.05
0.085
0.049 0.2 0.039 500 0.02
0.049
0.046 1000 0.1

SSB

(3) < (2, 4)

(3) < (4)
(4) < (1)

(3) < (1, 2, 4)

We clearly see that the cross-validation method generally chooses very small values for γ . This, in
turn, gives a risk bound (computed from Theorem 3.2) having very large values (results not shown
here). We have also tried to choose C and γ from the risk bound values.3 This method for selecting
hyperparameters turned out to produce classiﬁers having larger testing errors (results not shown
here).
To determine whether or not a difference of empirical risk measured on the testing set T is statisti-
cally signiﬁcant, we have used the test set bound method of [4] (based on the binomial tail inversion)
3From the standard union bound argument, the bound of Theorem 3.2 holds simultaneously for k different
choices of (γ , C ) if we replace δ by δ/k .

7

with a conﬁdence level of 95%. It turns out that no algorithm has succeeded in choosing a majority
vote classiﬁer which was statistically signiﬁcantly better (SSB) than the one chosen by another al-
gorithm except for the 4 cases that are listed in the column “SSB” of Table 1. We see that on these
cases, Algorithm 1 turned out to be statistically signiﬁcantly better.

6 Conclusion
Our numerical results indicate that Algorithm 1 generally outperforms AdaBoost and ridge regres-
empirical loss (cid:99)ζQ and the KL(Q(cid:107)P ) regularizer that are present in the PAC-Bayes bound of Theo-
sion when the hyperparameters C and γ are chosen by cross-validation. This indicates that the
rem 3.2 are key ingredients for learning algorithms to focus on. The fact that cross-validation turns
PAC-Bayes theory does not yet capture quantitatively the proper tradeoff between (cid:99)ζQ and KL(Q(cid:107)P )
out to be more efﬁcient than Theorem 3.2 at selecting good values for hyperparameters indicates that
that learners should optimize on the trading data. However, we feel that it is important to pursue
this research direction since it could potentially eliminate the need to perform the time-consuming
cross-validation method for selecting hyperparameters and provide better guarantees on the gener-
alization error of classiﬁers output by learning algorithms. In short, it could perhaps yield the best
generic optimization problem for learning.

Acknowledgments

Work supported by NSERC discovery grants 122405 (M.M.) and 262067 (F.L.).

References
the thermodynamics of sta-
PAC-Bayesian surpevised classiﬁcation:
[1] Olivier Catoni.
tistical
learning.
the Institute of Mathematical Statistics,
Monograph series of
http://arxiv.org/abs/0712.0248, December 2007.
[2] Pascal Germain, Alexandre Lacasse, Franc¸ ois Laviolette, and Mario Marchand. A pac-bayes
risk bound for general loss functions. In B. Sch ¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19, pages 449–456. MIT Press, Cambridge,
MA, 2007.
[3] Pascal Germain, Alexandre Lacasse, Franc¸ ois Laviolette, and Mario Marchand. PAC-Bayesian
In L ´eon Bottou and Michael Littman, editors, Proceedings of
learning of linear classiﬁers.
the 26th International Conference on Machine Learning, pages 353–360, Montreal, June 2009.
Omnipress.
[4] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine
Learning Research, 6:273–306, 2005.
[5] John Langford and John Shawe-Taylor. PAC-Bayes & margins.
In S. Thrun S. Becker and
K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 423–430.
MIT Press, Cambridge, MA, 2003.
[6] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51:5–21,
2003.
[7] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new
explanation for the effectiveness of voting methods. The Annals of Statistics, 26:1651–1686,
1998.
[8] Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. Journal of Ma-
chine Learning Research, 3:233–269, 2002.
[9] Manfred K. Warmuth, Karen A. Glocer, and S.V.N. Vishwanathan. Entropy regularized LP-
Boost. In Proceedings of the 2008 conference on Algorithmic Learning Theory, Springer LNAI
5254,, pages 256–271, 2008.

8


Value Iteration and DDP for an Inverted Pendulum

By:
Gregory M. Horn

CS229 Final Pro ject
Professor: Andrew Ng
December 10, 2009

Abstract

My intention in this pro ject was to learn about nonlinear control from a reinforcement learning stand-

point. I got value iteration working on a simple gravity pendulum with torque controller by discretizing

the state space and quadratically interpolating in between grid points. I couldn’t apply this technique

to an inverted cart-pole because of the “curse of dimensionality” so I explored diﬀerential dynamic

programming (DDP) as an alternative.

I built a cart-pole swing up controller with receding horizon

DDP.

Discretized Value Iteration

A nonlinear dynamical system can be described as

xk+1 = f (xk , uk )
≈ Ak xk + Bk uk .

The value function is the total cost to get from a state to a ﬁnal state assuming optimal control policies

are used at each time step:

V (xk ) = C (xk , uk ) + C (xk+1 , uk+1 ) + C (xk+2 , uk+2 ) + · · ·

= C (xk , uk ) + V (xk+1 )

where C (x, u) is the cost associated with a given state and action. Assuming that the value function is

known the optimal control policy at state xk maximizes V (xk ):

uk = arg max
u
= arg max
u

V (xk )

(C (xk , uk ) + V (xk+1 )) .

A quadric cost function in u was adopted in order to derive a simple optimal policy:
(cid:18)
(cid:19)
(cid:18)
(cid:19)
C (xk ) − 1
uk = arg max
k R uk + V (xk+1 )
2 uT
u
C (xk ) − 1
0 = ∇uk
k R uk + V (xk+1 )
2 uT
0 = −R uk + (∇xV )T ∇uk (xk+1 )
R uk = (∇xV )T ∇uk (Ak xk + Bk uk )

1

R uk = (∇xV )T Bk
uk = R−1 (∇xV )T Bk .

The technique for ﬁnding the optimal policy was taken from [1].

To ﬁnd the value function the state space is discretized into states S and value iteration is applied.

The value function is quadratically interpolated in between states when needed. When the optimal value

ﬁeld is found, a real-time controller would also interpolate the value function in deriving its policy.

V (x) := 0;

for n iterations

;

//linearize dynamics

//linearize dynamics

Vold := V ;
for {xk ∈ S }
Aij := ∂ f (xk ,u)i
∂xj
Bij := ∂ f (xk ,u)i
;
∂uj
u := R−1 (∇xVold (xk ))T B ;
x(cid:48) := A xk + B u;
//propagate state with simulator
V (x(cid:48) ) := interpolate(x(cid:48) , Vold );
V (xk ) α←− C (xk , u) + V (x(cid:48) );
end

//compute policy at xj

//update value f unction

//interpolate value f unction at propagated state

end

Mass on spring

The algorithm was debugged on the simple “mass on spring” system. The dynamical equation and cost

function are:

¨x = −k x − b ˙x + u
1
1
2 xT Q x +
2 uT R u

C (x, u) =

The algorithm worked and the learned controller found the exact same path as an LQR controller designed

with MATLAB (see Figure 1).

2

Figure 1: Mass on spring value function and learned controller

Gravity pendulum

The algorithm was then applied to controlling a simple gravity pendulum with saturating torque control.

The pendulum dynamics are:

¨θ = g
l

sin(θ) − k
ml2

˙θ + u
ml2 .

The algorithm converged and the value function was learned (see Figure 2).

Figure 2: Value function for limited-torque pendulum. Streamlines show the phase portrait of the
optimally controlled system.

Diﬀerential Dynamic Programming

I was unsuccessful in applying value iteration to a cart-pole system (largely because of the curse of dimen-
sionality) so I tried diﬀerential dynamic programming (DDP). In DDP a path {(x1 , u1 ), (x2 , u2 ), ...(xN , uN )}

3

is iteratively improved upon in two steps. In the backward sweep the value function is quadratically ap-

proximated about each state in the sequence and the policy u is improved. In the forward sweep the

new policy is simulated and a more optimal path x is found. This iterates until convergence.

optimized value function)

To quadratically expand the value function, ﬁrst quadratically expand the Q function (the un-

 δx
 Qxx Qux

QT
ux Quu
δu
where subscripts denote derivatives. The Q function is maximized with respect to δu

Q(xk , uk ) = C (xk , uk ) + V k+1 (f (xk , uk ))

≈ Q0 + QT
x δx + QT
u δu +

( δx δu )T

(2)

1
2

(1)

0 = ∇δuQ(x, u) = Qu + Qux δx + Quu δu
δu = −Q−1
uu (Qu + Qux δx)

Plugging the optimal δu back into Q(x, u) yields the value function
(cid:18)
(cid:19)
2 δxT (cid:0)Qxx − QT
(cid:1) δx +
+ (cid:0)QT
1
x − QT
uxQ−1
u Q−1
uu Qux
uu Qux

Q0 − 1
u Q−1
2 QT
uu Qu

V (xk ) =

(cid:1) δx

The coeﬃcients of (2) are found by quadratically expanding (1):

C (xk , uk ) ≈ C (xk , uk ) + C T
x δx + C T
u δu +
V k+1 (f (xk , uk )) ≈ V k+1 + (V k+1
)T (Ak δxk + Bk δuk ) +

 δx
x
(cid:0)δxT δuT (cid:1) H
δu

1
2

+

 Ak δx
1
1
k Cuu δuk + δuT
k Cxx δxk +
2 δuT
2 δxT
k Cux δxk
(cid:0)(Ak δx)T (Bk δu)T (cid:1) V k+1
xx
Bk δu

1
2



where elements of H are given by

Hij = (cid:88)
l

∂V k+1
∂xl

∂ 2 f (xk )l
∂xi∂xj

.

DDP was run on the gravity pendulum with an array of starting points. From the resulting array of

optimal paths it seems that DDP is more or less ﬁnding the same controller as in value iteration (Figure

3). There are some noticeable discrepancies which I currently believe are results of buggy code.

4

Figure 3: DDP results overlaid on value iteration results. Left: DDP (red) traces out paths similar to
those found with value iteration (black). Right: value functions found with DDP (black) hug the value
function found with value iteration (colored).

Finally, DDP was run on a standard cart-pole system. Using a receding horizon a nice swing-up

controller was obtained (Figure 4).

Figure 4: Cart-pole swing up controller found with DDP. (Left: “strobelight-style” transient plot.)

References

[1] Kenji Doya. Reinforcement learning in continuous time and space. Neural Computation, 12(1):219–

245, 2000.

5


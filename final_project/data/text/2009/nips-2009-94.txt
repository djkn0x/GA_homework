Rifﬂed Independence for Ranked Data

Jonathan Huang, Carlos Guestrin
School of Computer Science,
Carnegie Mellon University
{jch1,guestrin}@cs.cmu.edu

Abstract
Representing distributions over permutations can be a daunting task due to
the fact that the number of permutations of n objects scales factorially in n.
One recent way that has been used to reduce storage complexity has been to
exploit probabilistic independence, but as we argue, full independence assump-
tions impose strong sparsity constraints on distributions and are unsuitable
for modeling rankings. We identify a novel class of independence structures,
called rifﬂed independence, which encompasses a more expressive family of
distributions while retaining many of the properties necessary for performing
efﬁcient inference and reducing sample complexity. In rifﬂed independence, one
draws two permutations independently, then performs the rifﬂe shufﬂe, common
in card games, to combine the two permutations to form a single permutation.
In ranking, rifﬂed independence corresponds to ranking disjoint sets of objects
independently, then interleaving those rankings. We provide a formal introduction
and present algorithms for using rifﬂed independence within Fourier-theoretic
frameworks which have been explored by a number of recent papers.

Introduction
1
Distributions over permutations play an important role in applications such as multi-object tracking,
visual feature matching, and ranking.
In tracking, for example, permutations represent joint
assignments of individual identities to track positions, and in ranking, permutations represent
the preference orderings of a list of items. Representing distributions over permutations is a
notoriously difﬁcult problem since there are n! permutations, and standard representations, such
as graphical models, are ineffective due to the mutual exclusivity constraints typically associated
with permutations. The quest for exploitable problem structure has led researchers to consider a
number of possibilities including distribution sparsity [17, 9], exponential family parameteriza-
tions [15, 5, 14, 16], algebraic/Fourier structure [13, 12, 6, 7], and probabilistic independence [8].
While sparse distributions have been successfully applied in certain tracking domains, we argue that
they are less suitable in ranking problems where it might be necessary to model indifference over a
number of objects. In contrast, Fourier methods handle smooth distributions well but are not easily
scalable without making aggressive independence assumptions [8].
In this paper, we argue that
while probabilistic independence might be useful in tracking, it is a poor assumption in ranking.
We propose a novel generalization of independence, called rifﬂed independence, which we argue to
be far more suitable for modeling distributions over rankings, and develop algorithms for working
with rifﬂed independence in the Fourier domain. Our major contributions are as follows.
• We introduce an intuitive generalization of independence on permutations, which we call rifﬂed
independence, and show it to be a more appropriate notion of independence for ranked data,
offering possibilities for efﬁcient inference and reduced sample complexity.
• We introduce a novel family of distributions, called biased rifﬂe shufﬂes, that are useful for rifﬂed
independence and propose an algorithm for computing its Fourier transform.
• We provide algorithms that can be used in the Fourier-theoretic framework of [13, 8, 7] for joining
rifﬂe independent factors (RifﬂeJoin), and for teasing apart the rifﬂe independent factors from a
joint (RifﬂeSplit), and provide theoretical and empirical evidence that our algorithms perform well.

1

(a)
(e)
(d)
(c)
(b)
Figure 1: Example ﬁrst-order matrices with X = {1, 2, 3}, ¯X = {4, 5, 6} independent, where black means
h(σ : σ(j ) = i) = 0. In each case, there is some 3-subset Y which X is constrained to map to with probability
one. By rearranging rows, one sees that independence imposes block-diagonal structure on the matrices.
2 Distributions on permutations and independence relations
In the context of ranking, a permutation σ = [σ1 , . . . , σn ] represents a one-to-one mapping from
n objects to n ranks, where, by σj = i (or σ(j ) = i), we mean that the j th object is assigned rank
i under σ . If we are ranking a list of fruits/vegetables enumerated as (1) Artichoke, (2) Broccoli,
(3) Cherry, and (4) Dates, then the permutation σ = [σA σB σC σD ] = [2 3 1 4] ranks Cherry ﬁrst,
Artichoke second, Broccoli third, Dates last. The set of permutations of {1, . . . , n} forms a group
with respect to function composition called the symmetric group (written Sn ). We write τ σ to
denote the permutation resulting from τ composed with σ (thus [τ σ ](j ) = τ (σ(j ))). A distribution
h(σ), deﬁned over Sn , can be viewed as a joint distribution over the n variables σ = (σ1 , . . . , σn )
(where σj ∈ {1, . . . , n}), subject to mutual exclusivity constraints ensuring that objects i and j
never map to the same rank (h(σi = σj ) = 0 whenever i (cid:54)= j ). Since there are n! permutations, it is
intractable to represent entire distributions and one can hope only to maintain compact summaries.
There have been a variety of methods proposed for summarizing distributions over permutations
ranging from older ad-hoc methods such as maintaining k-best hypotheses [17] to the more recent
Fourier-based methods which maintain a set of low-order summary statistics [18, 2, 11, 7]. The ﬁrst-
order summary, for example, stores a marginal probability of the form h(σ : σ(j ) = i) for every
pair (i, j ) and thus requires storing a matrix of only O(n2 ) numbers. For example, we might store
the probability that apples are ranked ﬁrst. More generally, one might store the sth -order marginals,
which are marginal probabilities of s-tuples. The second-order marginals, for example, take the form
h(σ : σ(k , (cid:96)) = (i, j )), and require O(n4 ) storage. Low-order marginals correspond, in a certain
sense, to the low-frequency Fourier coefﬁcients of a distribution over permutations. For example,
the ﬁrst-order matrix of h(σ) can be reconstructed exactly from O(n2 ) of the lowest frequency
Fourier coefﬁcients of h(σ), and the second-order matrix from O(n4 ) of the lowest frequency
Fourier coefﬁcients. In general, one requires O(n2s ) coefﬁcients to exactly reconstruct sth -order
marginals, which quickly becomes intractable for moderately large n. To scale to larger problems,
Huang et al. [8] demonstrated that, by exploiting probabilistic independence, one could dramatically
improve the scalability of Fourier-based methods, e.g., for tracking problems, since confusion in
data association only occurs over small independent subgroups of objects in many problems.
Probabilistic independence on permutations. Probabilistic independence assumptions on the
symmetric group can simply be stated as follows. Consider a distribution h deﬁned over Sn . Let X
be a p-subset of {1, . . . , n}, say, {1, . . . , p} and let ¯X be its complement ({p + 1, . . . , n}) with size
q = n − p. We say that σX = (σ1 , σ2 , . . . , σp ) and σ ¯X = (σp+1 , . . . , σn ) are independent if
h(σ) = f (σ1 , σ2 , . . . , σp ) · g(σp+1 , . . . , σn ).
Storing the parameters for the above distribution requires keeping O(p! + q !) probabilities instead
of the much larger O(n!) size required for general distributions. Of course, O(p! + q !) can still be
quite large. Typically, one decomposes the distribution recursively and stores factors exactly for
small enough factors, or compresses factors using Fourier coefﬁcients (but using higher frequency
terms than what would be possible without the independence assumption).
In order to exploit
probabilistic independence in the Fourier domain, Huang et al. [8] proposed algorithms for joining
factors and splitting distributions into independent components in the Fourier domain.
Restrictive ﬁrst-order conditions. Despite its utility for many tracking problems, however, we
argue that the independence assumption on permutations implies a rather restrictive constraint on
distributions, rendering independence highly unrealistic in ranking applications. In particular, using
the mutual exclusivity property, it can be shown [8] that, if σX and σ ¯X are independent, then for
some ﬁxed p-subset Y ⊂ {1, . . . , n}, σX is a permutation of elements in Y and σ ¯X is a permutation
of its complement, ¯Y , with probability 1. Continuing with our vegetable/fruit example with n = 4,

2

jiP(σ:σ(j)=i)246246jiP(σ:σ(j)=i), Y={1,2,3}246246jiP(σ:σ(j)=i), Y={2,4,5}246246jiP(σ:σ(j)=i), Y={1,2,5}246246jiP(σ:σ(j)=i), Y={4,5,6}246246if the vegetables and fruits rankings, σveg = [σA σB ] and σf ruit = [σC σD ], are known to be inde-
pendent, then for Y = {1, 2}, the vegetables are ranked ﬁrst and second with probability one, and
the fruits are ranked third and last with probability one. Huang et al. [8] refer to this as the ﬁrst-order
condition because of the block structure imposed upon ﬁrst-order marginals (see Fig. 1). In sports
tracking, the ﬁrst-order condition might say, quite reasonably, that there is potential identity confu-
sion within tracks for the red team and within tracks for the blue team but no confusion between the
two teams. In our ranking example however, the ﬁrst-order condition forces the probability of any
vegetable being in third place to be zero, even though both vegetables will, in general, have nonzero
marginal probability of being in second place, which seems quite unrealistic. In the next section, we
overcome the restrictive ﬁrst-order condition with the more ﬂexible notion of rifﬂed independence.

3 Going beyond full independence: Rifﬂed independence

The rifﬂe (or dovetail) shufﬂe [1] is perhaps the most popular method of card shufﬂing,
in
which one cuts a deck of n cards into two piles, X = {1, . . . , p} and ¯X = {p + 1, . . . , n},
of sizes p and q = n − p, respectively, and successively drops the cards, one by one,
into one deck again.
so that
the piles are interleaved (see Fig. 2)
Inspired by rifﬂe
independence, which we call rifﬂed inde-
shufﬂes, we present a novel relaxation of full
pendence.
Rankings that are rifﬂe independent are formed by independently selecting
rankings for two disjoint subsets of objects, then interleaving the rank-
ings using a rifﬂe shufﬂe to form a ranking over all objects. For example,
we might ﬁrst ‘cut the deck’ into two piles, vegetables (X ) and fruits
( ¯X ), independently decide that Broccoli is preferred over Artichoke
(σB < σA ) and that Dates is preferred over Cherry (σD < σC ), then in-
terleave the fruit and vegetable rankings to form σB < σD < σA < σC
(i.e. σ = [3 1 4 2]).
Intuitively, rifﬂed independence models complex
relationships within each of set X and ¯X while allowing correlations
Figure 2: Rifﬂe shufﬂing a
between sets to be modeled only by a constrained form of shufﬂing.
standard deck of cards.
Rifﬂe shufﬂing distributions. Mathematically, shufﬂes are modeled
as random walks on Sn . The ranking σ (cid:48) after a shufﬂe is generated from the ranking prior to that
h(cid:48) (σ (cid:48) ) = [m ∗ h] (σ (cid:48) ) = (cid:80){σ,τ : σ (cid:48)=τ σ} m(τ )h(σ). Note that we use the ∗ symbol to denote the
shufﬂe, σ , by drawing a permutation, τ from a shufﬂing distribution m(τ ), and setting σ (cid:48) = τ σ .
Given the distribution P over σ , we can ﬁnd the distribution h(cid:48) (σ (cid:48) ) after the shufﬂe via convolution:
convolution operation.
The question is, what are the shufﬂing distributions m that correspond to rifﬂe shufﬂes? To answer
this question, we use the distinguishing property of the rifﬂe shufﬂe, that, after cutting the deck into
two piles of size p and q = n − p, the relative ranking relations within each pile are preserved. Thus,
if the ith card lies above the j th in one of the piles, then after shufﬂing, the ith card remains above
the j th . In our example, relative rank preservation says that if Artichoke is preferred over Broccoli
prior to shufﬂing, it is preferred over Broccoli after shufﬂing. Any allowable rifﬂe shufﬂing distri-
bution must therefore assign zero probability to permutations which do not preserve relative ranking
relations. The set of permutations which do preserve these relations have a simple description.
Deﬁnition 1 (Rifﬂe shufﬂing distribution). Deﬁne the set of (p, q)-interleavings as:
Ωp,q ≡ {τY = [Y(1) Y(2) . . . Y(p)
¯Y(2) . . . ¯Y(q) ] : Y ⊂ {1, . . . , n}, |Y | = p} ⊂ Sn , n = p + q ,
¯Y(1)
where Y(1) represents the smallest element of Y , Y(2) the second smallest, etc. A distribution mp,q
on Sn is called a rifﬂe shufﬂing distribution if it assigns nonzero probability only to elements in Ωp,q .
The (p, q)-interleavings can be shown to preserve the relative ranking relations within each
of the subsets X = {1, . . . , p} and ¯X = {p + 1, . . . , n} upon multiplication.
In our veg-
etable/fruits example, we have n = 4, p = 2, and so the collection of subsets of size
(cid:1) = (cid:0)n
|Ωp,q | = (cid:0)n
(cid:1) = 4!/(2!2!) = 6. One possible rifﬂe shufﬂing distribution on S4
p are: { {1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4} } , and the set of (2, 2)-interleavings
is given by: Ωp,q = {[1 2 3 4], [1 3 2 4], [1 4 2 3], [2 3 1 4], [2 4 1 3], [3 4 1 2]}.
Note that
p
q
might, for example, assign uniform probability (munif
2,2 (σ) = 1/6) to each permutation in Ω2,2
and zero probability to everything else, reﬂecting indifference between vegetables and fruits. We
now formally deﬁne our generalization of independence where a distribution which fully factors
independently is allowed to undergo a single rifﬂe shufﬂe.

3

Deﬁnition 2 (Rifﬂed independence). The subsets X = {1, . . . , p} and ¯X = {p + 1, . . . , n}
are said to be rifﬂe independent if h = mp,q ∗ (f (σp ) · g(σq )), with respect to some rifﬂe
shufﬂing distribution mp,q and distributions f , g , respectively. We denote rifﬂed independence by:
h = f ⊥mp,q g , and refer to f , g as rifﬂed factors.
To draw from h, one independently draws a permutation σp , of cards {1, . . . , p}, a permutation σq ,
of cards {p + 1, . . . , n}, and a (p, q)-interleaving τY , then shufﬂes to obtain σ = τY [σp σq ]. In our
example, the rankings σp = [2 1] (Broccoli preferred to Artichoke) and σq = [4 3] (Cherry preferred
to Dates) are selected, then shufﬂed (multiplied by τ{1,3} = [1 3 2 4]) to obtain σ = [3 1 4 2].
We remark that setting mp,q to be the delta distribution on any of the (p, q)-interleavings in Ωp,q
recovers the deﬁnition of ordinary probabilistic independence, and thus rifﬂed independence is a
strict generalization thereof. Just as in the full independence regime, where the distributions f and
g are marginal distributions of rankings of X and ¯X , in the rifﬂed independence regime, they can
be thought of as marginal distributions of the relative rankings of X and ¯X .
distributions f and g , we now require O((cid:0)n
(cid:1)) storage for the nonzero terms of the rifﬂe shufﬂing
Biased rifﬂe shufﬂes. There is, in the general case, a signiﬁcant increase in storage required for
In addition to the O(p! + q !) storage required for
rifﬂed independence over full independence.
p
distribution mp,q .
Instead of representing all possible rifﬂe shufﬂing distributions, however, we
now introduce a family of useful rifﬂe shufﬂing distributions which can be described using only
a handful of parameters. The simplest rifﬂe shufﬂing distribution is the uniform rifﬂe shufﬂe,
, which assigns uniform probability to all (p, q)-interleavings and zero probability to all other
munif
p,q
p,q models potentially complex
elements in Sn . Used in the context of rifﬂed independence, munif
relations within X and ¯X , but only captures the simplest possible correlations across subsets. We
might, for example, have complex preference relations amongst vegetables and amongst fruits, but
be completely indifferent with respect to the subsets, vegetables and fruits, as a whole.
There is a simple recursive method for uni-
DRAWR I FFLEUN I F (p, q , n)
// (p + q = n)
 σ− (i)
formly drawing (p, q)-interleavings.
Starting
with prob q/n
// drop from right pile
σ− ← DRAWR I FFLEUN I F (p, q − 1, n − 1)
with a deck of n cards cut into a left pile
({1, . . . , p}) and a right pile ({p + 1, . . . , n}),
foreach i do σ(i) ←
if i < n
pick one of the piles with probability propor-
if i = n
n
tional to its size (p/n for the left pile, q/n for
otherwise
// drop from left pile
8<: σ− (i)
σ− ← DRAWR I FFLEUN I F (p − 1, q , n − 1)
the right) and drop the bottommost card, thus
mapping either card p or card n to rank n. Then
foreach i do
recurse on the n − 1 remaining undropped cards,
drawing a (p − 1, q)-interleaving if the right pile
σ(i) ←
n
was picked, or a (p, q − 1)-interleaving if the
σ− (i − 1)
left pile was picked. See Alg. 1.
return σ
8
Algorithm 1: Recurrence for drawing σ ∼
It is natural to consider generalizations where
(Base case: return σ = [1] if n = 1).
munif
one is preferentially biased towards dropping
p,q
cards from the left hand over the right hand (or vice-versa). We model this bias using a simple
one-parameter family of distributions in which cards from the left and right piles drop with
probability proportional to αp and (1 − α)q , respectively, instead of p and q . We will refer to α as
the bias parameter, and the family of distributions parameterized by α as the biased rifﬂe shufﬂes.1
In the context of rankings, biased rifﬂe shufﬂes provide a simple model for expressing groupwise
preferences (or indifference) for an entire subset X over ¯X or vice-versa. The bias parameter α can
be thought of as a knob controlling the preference for one subset over the other, and might reﬂect, for
example, a preference for fruits over vegetables, or perhaps indifference between the two subsets.
Setting α = 0 or 1 recovers the full independence assumption, preferring objects in X (vegetables)
over objects in ¯X (fruits) with probability one (or vice-versa), and setting α = .5, recovers the
uniform rifﬂe shufﬂe (see Fig. 3). Finally, there are a number of straightforward generalizations of
the biased rifﬂe shufﬂe that one can use to realize richer distributions. For example, α might depend
on the number of cards that have been dropped from each pile (allowing perhaps, for distributions
to prefer crunchy fruits over crunchy vegetables, but soft vegetables over soft fruits).

if i < p
if i = p
if i > p

1
2
3

4
5
6
7

1The recurrence in Alg. 1 has appeared in various forms in literature [1]. We are the ﬁrst to (1) use the
recurrence to Fourier transform mp,q , and to (2) consider biased versions. The biased rifﬂe shufﬂes in [4] are
not similar to our biased rifﬂe shufﬂes. See Appendix for details.

4

(a)
(e)
(d)
(c)
(b)
Figure 3: First-order matrices with a deck of 20 cards, X = {1, . . . , 10}, ¯X = {11, . . . , 20}, rifﬂe indepen-
dent and various settings of α. Note that nonzero blocks ‘bleed’ into zero regions (compare to Fig. 1). Setting
α = 0 or 1 recovers full independence, where a subset of objects is preferred over the other with probability one.
4 Between independence and conditional independence
We have presented rifﬂe independent distributions as fully independent distributions which have
been convolved by a certain class of shufﬂing distributions. In this section, we provide an alternative
view of rifﬂed independence based on conditional independence, showing that the notion of rifﬂed
independence lies somewhere between full and conditional independence.
In Section 3, we formed a ranking by ﬁrst independently drawing permutations πp and πq , of object
sets {1, . . . , p} (vegetables) and {p + 1, . . . , n} (fruits), respectively, drawing a (p, q)-interleaving
(i.e., a relative ranking permutation, τY ∈ Ωp,q ), and shufﬂing to form σ = τY [πp πq ]. Thus, an ob-
ject i ∈ {1, . . . , p} is ranked in position τY (πp (i)) after shufﬂing (and an object j ∈ {p + 1, . . . , n}
is ranked in position τY (πq (j ))). An equivalent way to form the same σ , however, is to ﬁrst draw
an interleaving τY ∈ Ωp,q , then, conditioned on the choice of Y , draw independent permutations of
the sets Y and ¯Y . In our example, we might ﬁrst draw the (2,2)-interleaving [1 3 2 4] (so that after
shufﬂing, we would obtain σV eg < σF ruit < σV eg < σF ruit ). Then we would draw a permutation
of the vegetable ranks (Y = {1, 3}), say, [3 1], and a permutation of the fruit ranks ( ¯Y = {2, 4}),
[4 2], to obtain a ﬁnal ranking over all items: σ = [3 1 4 2], or σB < σD < σA < σC .
case of conditional independence, however, has O((cid:0)n
(cid:1)(p! + q ! + 1)) parameters, while rifﬂed
It is tempting to think that rifﬂed independence is exactly the conditional independence assumption,
in which case the distribution would factor as h(σ) = h(Y ) · h(σX |Y ) · h(σ ¯X |Y ). The general
independence requires only O((cid:0)n
(cid:1) + p! + q !) parameters.
p
p
We now provide a simple correspondence between the conditional independence view of rifﬂed
independence presented in this section to the shufﬂe theoretic deﬁnition from Section 3 (Def. 2).
Deﬁne the map φ, which, given a permutation of Y (or ¯Y ), returns the permutation in σp ∈ Sp (or
Sq ) such that [σp ]i is the rank of [σX ]i relative to the set Y . For example, if the permutation of
the vegetable ranks is σX = [3 1] (with Artichoke ranked third, Broccoli ﬁrst), then φ(σX ) = [2 1]
since, relative to the set of vegetables, Artichoke is ranked second, and Broccoli ﬁrst.
Proposition 3. Consider a rifﬂe independent h = f ⊥mp,q g . For each σ ∈ Sn , h factors as h(σ) =
h(Y ) · h(σX |Y ) · h(σ ¯X |Y ), with h(Y ) = m(τY ), h(σX |Y ) = f (φ(σX )), and h(σ ¯X ) = g(φ(σ ¯X )).
Proposition 3 is useful because it shows that the probability of a single ranking can be computed
without summing over the entire symmetric group (a convolution)— a fact that might not be
obvious from Deﬁnition 2. The factorization h(σ) = m(τY )f (φ(σX ))g(φ(σ ¯X )) also suggests that
rifﬂed independence behaves essentially like full independence (without the ﬁrst-order condition),
where, in addition to the independent variables σX and σ ¯X , we also independently randomize
over the subset Y . An immediate consequence is thatjust as in the full independence regime,
conditioning operations on certain observations and MAP (maximum a posteriori) assignment
problems decompose according to rifﬂed independence structure.
Proposition 4 (Probabilistic inference decompositions). Consider rifﬂe independent prior and like-
lihood functions, hprior and hlike , on Sn which factor as: hprior = fprior ⊥mprior gprior
and hlike = flike ⊥mlike glike , respectively. The posterior distribution under Bayes rule
can be written as the rifﬂe independent distribution: hpost ∝ (fprior (cid:12) flike ) ⊥mprior (cid:12)mlike
(gprior (cid:12) glike ),where the (cid:12) symbol denotes the pointwise product operation.
A similar result allows us to also perform MAP assignments by maximizing each of the distributions
mp,q , f and g , independently and combining the results. As a corollary, it follows that conditioning
on simple pairwise ranking likelihood functions (that depend only on whether object i is preferred
to object j ) decomposes along rifﬂed independence structures.

5

jiP(σ:σ(j)=i), α=0051015205101520jiP(σ:σ(j)=i), α=1.50e−0151015205101520jiP(σ:σ(j)=i), α=5.00e−0151015205101520jiP(σ:σ(j)=i), α=8.50e−0151015205101520jiP(σ:σ(j)=i), α=0151015205101520R I FFLE JO IN ( bf , bg )
1 bh(cid:48) = JO IN( bf , bg) ;
bhi ← h(cid:91)mα
i
2
· bh(cid:48)
foreach frequency level i do
3
return bh ;
i ;
4
p,q
i
5
Algorithm 2: Pseudocode for RifﬂeJoin

R I FFLES PL I T (bh)
˜T
i ← ˆ bmunif
bh(cid:48)
· bhi ;
1
foreach frequency level i do
2
[ bf , bg ] ← S PL I T( bh(cid:48) ) ;
3
p,q
i
4
Normalize ˆf and ˆg ;
5
return ˆf , ˆg ;
6
Algorithm 3: Pseudocode for RifﬂeSplit
5 Fourier domain algorithms: RifﬂeJoin and RifﬂeSplit
In this section, we present two algorithms for working with rifﬂed independence in the Fourier theo-
retic framework of [13, 8, 7] — one algorithm for merging rifﬂed factors to form a joint distribution
(RifﬂeJoin), and one for extracting rifﬂed factors from a joint (RifﬂeSplit). We begin with a brief
introduction to Fourier theoretic inference on permutations (see [11, 7] for a detailed exposition).
Unlike its analog on the real line, the Fourier transform of a function on Sn takes the form of a
collection of Fourier coefﬁcient matrices ordered with respect to frequency. Discussing the analog
we simply index the Fourier coefﬁcient matrices of h as (cid:98)h0 , (cid:98)h1 , . . . , (cid:98)hK ordered with respect to
of frequency for functions on Sn , is beyond the scope of our paper, and, given a distribution h,
some measure of increasing complexity. We use (cid:98)h to denote the complete collection of Fourier
coefﬁcient matrices. One rough way to understand this complexity, as mentioned in Section 2,
is by the fact that the low-frequency Fourier coefﬁcient matrices of a distribution can be used to
reconstruct low-order marginals. For example, the ﬁrst-order matrix of marginals of h can always
be reconstructed from the matrices ˆh0 and ˆh1 . As on the real line, many of the familiar properties of
the Fourier transform continue to hold. The following are several basic properties used in this paper:
Proposition 5 (Properties of the Fourier transform, see [2]). Consider any f , g : Sn → R.
• (Linearity) For any α, β ∈ R, [ (cid:92)αf + β g ]i = α (cid:98)fi + β (cid:98)gi holds at all frequency levels i.
[(cid:91)f ∗ g ]i = (cid:98)fi · (cid:98)gi , for each frequency level i, where the operation · is matrix multiplication.
• (Convolution) The Fourier transform of a convolution is a product of Fourier transforms:
• (Normalization) The ﬁrst coefﬁcient matrix, ˆf0 , is a scalar and equals (cid:80)
f (σ).
σ∈Sn
A number of papers in recent years ([13, 6, 8, 7]) have considered approximating distributions
over permutations using a truncated (bandlimited) set of Fourier coefﬁcients and have proposed
inference algorithms that operate on these Fourier coefﬁcient matrices. For example, one can
perform generic marginalization, Markov chain prediction, and conditioning operations using only
Fourier coefﬁcients without ever having to perform an inverse Fourier transform. Huang et al. [8]
introduced Fourier domain algorithms, Join and Split, for combining independent factors to form
joints and for extracting the factors from a joint distribution, respectively.
In this section, we provide generalizations of the algorithms in [8] that we call RifﬂeJoin and Rifﬂe-
Split. We will assume that X = {1, . . . , p}, ¯X = {p + 1, . . . , n} and that we are given a rifﬂe inde-
pendent distribution h : Sn → R (h = f ⊥mp,q g ). We also, for the purposes of this section, assume
that the parameters for the distribution mp,q are known, though it will not matter for the RifﬂeSplit
algorithm. Although we begin each of the following discussions as if all of the Fourier coefﬁcients
are provided, we will be especially interested in algorithms that work well in cases where only a trun-
cated set of Fourier coefﬁcients are present, and where h is only approximately rifﬂe independent.
RifﬂeJoin. Given the Fourier coefﬁcients of f , g , and m, we can compute the Fourier coefﬁcients
of h using Deﬁnition 2 by applying the Join algorithm from [8] and the Convolution Theorem
(Prop. 5), which tells us that the Fourier transform of a convolution can be written as a pointwise
algorithm on (cid:98)f and (cid:98)g , and convolves the result by (cid:98)m (see Alg. 2). In general, it may be intractable
product of Fourier transforms. To compute the ˆhλ , our RifﬂeJoin algorithm simply calls the Join
shufﬂes from Section 3, one can efﬁciently compute the low-frequency terms of (cid:100)mα
to Fourier transform the rifﬂe shufﬂing distribution mp,q . However, for the class of biased rifﬂe
p,q by employing
the recurrence relation in Alg. 1. In particular, Alg. 1 expresses a biased rifﬂe shufﬂe on Sn as a
(Prop. 5), one can efﬁciently compute (cid:100)mα
linear combination of biased rifﬂe shufﬂes on Sn−1 . By invoking linearity of the Fourier transform
p,q via a dynamic programming approach. To the best of
our knowledge, we are the ﬁrst to compute the Fourier transform of rifﬂe shufﬂing distributions.

6

RifﬂeSplit. Given the Fourier coefﬁcients of the rifﬂe independent distribution h, we would like to
level i, ˆhi = [ (cid:100)mp,q ]i · [ (cid:100)f · g ]i . The ﬁrst solution to the splitting problem that might occur is to perform
tease apart the rifﬂe factors f and g . From the RifﬂeJoin algorithm, we saw that for each frequency
a deconvolution by multiplying each (cid:98)hi term by the inverse of the matrix [ (cid:100)mp,q ]i (to form [ (cid:100)mp,q ]−1
(cid:98)hi ) and call the Split algorithm from [8] on the result. Unfortunately, the matrix [ (cid:100)mp,q ]i is, in general,
·
(cid:3)T
non-invertible. Instead, our RifﬂeSplit algorithm left-multiplies each (cid:98)hi term by (cid:2) (cid:98)munif
i
, which
p,q
can be shown to be equivalent to convolving the distribution h by the ‘dual shufﬂe’, m∗ , deﬁned as
i
m∗ (σ) = munif
p,q (σ−1 ). While convolving by m∗ does not produce a distribution that factors inde-
pendently, the Split algorithm from [8] can still be shown to recover the Fourier transforms ˆf and ˆg :
Theorem 6. If h = f ⊥mp,q g , then RifﬂeSplit (Alg. 3) (with ˆh as input), returns ˆf and ˆg exactly.
, which we can again accomplish via the
As with RifﬂeJoin, it is necessary Fourier transform munif
p,q
recurrence in Alg. 1. One must also normalize the output of Split to sum to one via Prop. 5.

σq

g(σq ) = 1),

Theoretical guarantees. We now brieﬂy summarize several results which show how, (1) our
algorithms perform when called with a truncated set of Fourier coefﬁcients, and (2) when RifﬂeSplit
is called on a distribution which is only approximately rifﬂe independent.
Theorem 7. Given enough Fourier terms to reconstruct the k th -order marginals of f and g , Rif-
ﬂeJoin returns enough Fourier terms to exactly reconstruct the k th -order marginals of h. Likewise,
given enough Fourier terms to reconstruct the k th -order marginals of h, RifﬂeSplit returns enough
Fourier terms to exactly reconstruct the k th -order marginals of both f and g .
[ (cid:98)f (cid:48) , (cid:98)g (cid:48) ] = R I FFLES P L I T((cid:98)h), then (f (cid:48) , g (cid:48) ) is the minimizer of the problem:
Theorem 8. Let h be any distribution on Sn and mp,q any rifﬂe shufﬂing distribution on Sn . If
minimizef ,g DKL (h||f ⊥mp,q g), (subject to: (cid:80)
f (σp ) = 1, (cid:80)
σp
where DKL is the Kullback-Leibler divergence.
6 Experiments
In this section, we validate our algorithms and show that rifﬂed independence exists in real data.
APA dataset. The APA dataset [3] is a collection of 5738 ballots from a 1980 presidential election
of the American Psychological Association where members ordered ﬁve candidates from favorite
to least favorite. We ﬁrst perform an exhaustive search for subsets X and ¯X that are closest to rifﬂe
independent (with respect to DKL ), and ﬁnd that candidate 2 is nearly rifﬂe independent of the
remaining candidates. In Fig. 4(a) we plot the true vote distribution and the best approximation by a
distribution in which candidate 2 is rifﬂe independent of the rest. For comparison, we plot the result
of splitting off candidate 3 instead of candidate 2, which one can see to be an inferior approximation.
The APA, as described by Diaconis [3], is divided into “academicians and clinicians who are on
uneasy terms”. In 1980, candidates {1, 3} and {4, 5} fell on opposite ends of this political spectrum
with candidate 2 being somewhat independent. Diaconis conjectured that voters choose one group
over the other, and then choose within. We are now able to verify his conjecture in a rifﬂed
independence sense. After removing candidate 2 from the distribution, we perform a search within
candidates {1, 3, 4, 5} to again ﬁnd nearly rifﬂe independent subsets. We ﬁnd that X = {1, 3} and
¯X = {4, 5} are very nearly rifﬂe independent and thus are able to verify that candidate sets {2},
{1, 3}, {4, 5} are indeed grouped in a rifﬂe independent sense in the APA data. Finally since there
are two opposing groups within the APA, the rifﬂe shufﬂing distribution for sets {1, 3} and {4, 5} is
not well approximated by a biased rifﬂe shufﬂe. Instead, we ﬁt a mixture of two biased rifﬂe shufﬂes
to the data and found the bias parameters of the mixture components to be α1 ≈ .67 and α2 ≈ .17,
indicating that the two components oppose each other (since α1 and α2 lie on either side of .5).
Sushi dataset. The sushi dataset [10] consists of 5000 full rankings of ten types of sushi. Com-
pared to the APA data, it has more objects, but fewer examples. We divided the data into training
and test sets and estimated the true distribution in three ways: (1) directly from samples,(2) using
a rifﬂe independent distribution (split evenly into two groups of ﬁve) with the optimal shufﬂing
distribution m, and (3) with a biased rifﬂe shufﬂe (and optimal bias α). Fig. 4(b) plots testset
log-likelihood as a function of training set size — we see that rifﬂe independence assumptions can
help signiﬁcantly to lower the sample complexity of learning. Biased rifﬂe shufﬂes, as can be seen,

7

(a) Purple line: approximation to vote distribution when candidate 2 is rifﬂe independent;
Blue line: approximation when candidate 3 is rifﬂe independent.

(b) Average log-likelihood of held out
test examples from the Sushi dataset

(c) First-order probabilities of Uni (sea
urchin) (Sushi dataset) rankings.

(d) Estimating a rifﬂe independent distri-
bution using various sample sizes
Figure 4: Experiments

(e) Running time plot of RifﬂeJoin

are a useful learning bias with very small samples. As an illustration, see Fig. 4(c) which shows the
ﬁrst-order marginals of Uni (Sea Urchin) rankings, and the biased rifﬂe approximation.

Approximation accuracy. To understand the behavior of RifﬂeSplit in approximately rifﬂe in-
dependent situations, we draw sample sets of varying sizes from a rifﬂe independent distribution on
S8 (with bias parameter α = .25) and use RifﬂeSplit to estimate the rifﬂe factors from the empirical
distribution. In Fig. 4(d), we plot the KL-divergence between the true distribution and that obtained
by applying RifﬂeJoin to the estimated rifﬂe factors. With small sample sizes (far less than 8!), we
are able to recover accurate approximations despite the fact that the empirical distributions are not
exactly rifﬂe independent. For comparison, we ran the experiment using the Split algorithm [8]
to recover the rifﬂe factors. Somewhat surprisingly, one can show (see Appendix) that Split also
recovers the rifﬂe factors, albeit without the optimality guarantee that we have shown for Rifﬂesplit
(Theorem 8) and therefore requires far more samples to reliably approximate h.

Running times.
In general, the complexity of Split is cubic (O(d3 )) in the dimension of each
Fourier coefﬁcient matrix [8]. The complexity of RifﬂeJoin/RifﬂeSplit is O(n2d3 ), in the worst case
when p ∼ O(n). If we precompute the Fourier coefﬁcients of mp,q , (which requires O(n2d3 )) for
each coefﬁcient matrix, then the complexity of RifﬂeSplit is also O(d3 ). In Fig. 4(e), we plot running
times of RifﬂeJoin (no precomputation) as a function of n (setting p = (cid:100)n/2(cid:101)) scaling up to n = 40.

7 Future Directions and Conclusions

There are many open questions. For example, several papers note that graphical models cannot
compactly represent distributions over permutations due to mutual exclusivity. An interesting
question which our paper opens, is whether it is possible to use something similar to graphical
models by substituting conditional generalizations of rifﬂed independence for ordinary conditional
independence. Other possibilities include going beyond the algebraic approach and studying rifﬂed
independence in non-Fourier frameworks and developing statistical (rifﬂed) independence tests.
In summary, we have introduced rifﬂed independence and discussed how to exploit such structure
in a Fourier-theoretic framework. Rifﬂed independence is a new tool for analyzing ranked data and
has the potential to offer novel insights into datasets both new and old. We believe that it will lead
to the development of fast inference and low sample complexity learning algorithms.

Acknowledgements
This work is supported in part by the ONR under MURI N000140710747, and the Young Investiga-
tor Program grant N00014-08-1-0752. We thank K. El-Arini for feedback on an initial draft.

8

102030405060708090100110120APA ranking distribution  true distributionRemove candidate {3} (DKL=0.1878)Remove candidate {2} (DKL=0.0398)50  100 200 400 800 16003200−9000−8500−8000−7500Training set sizeLog−likelihood of held−out test setFull modelRiffle Independent w/optimal mBiased riffle independent w/optimal  α24681000.050.10.150.20.250.3Ranks i = 1 (favorite) through 10 (least favorite)Probability of Uni/Sea Urchin in rank i  Estimated from 1000 examplesEstimated from 100 examplesBiased riffle indep. approx.1002003004005006000510152025Sample sizesKL divergence from truth (20 trials)Split algorithmRiffleSplit algorithm10203040020406080100n, p=n/2Elapsed time in second1st order (O(n2) terms)2nd order (O(n4) terms)3rd order (O(n4) terms)References
[1] D. Bayer and P. Diaconis. Trailing the dovetail shufﬂe to its lair. The Annals of Probability, 1992.
[2] P. Diaconis. Group Representations in Probability and Statistics. IMS Lecture Notes, 1988.
[3] Persi Diaconis. A generalization of spectral analysis with application to ranked data. The Annals of
Statistics, 17(3):949–979, 1989.
[4] J. Fulman. The combinatorics of biased rifﬂe shufﬂes. Combinatorica, 18(2):173–184, 1998.
[5] D. P. Helmbold and M. K. Warmuth. Learning permutations with exponential weights. In COLT, 2007.
[6] J. Huang, C. Guestrin, and L. Guibas. Efﬁcient inference for distributions on permutations. In NIPS,
2007.
[7] J. Huang, C. Guestrin, and L. Guibas. Fourier theoretic probabilistic inference over permutations. JMLR,
10, 2009.
[8] J. Huang, C. Guestrin, X. Jiang, and L. Guibas. Exploiting probabilistic independence for permutations.
In AISTATS, 2009.
[9] S. Jagabathula and D. Shah. Inferring rankings under constrained sensing. In NIPS, 2008.
[10] Toshihiro Kamishima. Nantonac collaborative ﬁltering: recommendation based on order responses. In
KDD, pages 583–588, 2003.
[11] R. Kondor. Group Theoretical Methods in Machine Learning. PhD thesis, Columbia University, 2008.
[12] R. Kondor and K. M. Borgwardt. The skew spectrum of graphs. In ICML, pages 496–503, 2008.
[13] R. Kondor, A. Howard, and T. Jebara. Multi-object tracking with representations of the symmetric group.
In AISTATS, 2007.
[14] G. Lebanon and Y. Mao. Non-parametric modeling of partially ranked data. In NIPS, 2008.
[15] M. Meila, K. Phadnis, A. Patterson, and J. Bilmes. Consensus ranking under the exponential model.
Technical Report 515, University of Washington, Statistics Department, April 2007.
[16] J. Petterson, T. Caetano, J. McAuley, and J. Yu. Exponential family graph matching and ranking. CoRR,
abs/0904.2623, 2009.
[17] D.B. Reid. An algorithm for tracking multiple targets. IEEE Trans. on Automatic Control, 6:843–854,
1979.
[18] J. Shin, N. Lee, S. Thrun, and L. Guibas. Lazy inference on object identities in wireless sensor networks.
In IPSN, 2005.

9


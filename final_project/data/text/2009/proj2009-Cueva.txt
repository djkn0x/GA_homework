Finding a Basis for the Neural State

Chris Cueva
ccueva@stanford.edu

I.

INTRODUCTION

How is information represented in the brain? For example, consider arm movement. Neurons in dorsal premotor
cortex (PMd) are selective for reach target [1], the type of grasp [2], reach speed [3], reach curvature [4], and the
required accuracy [5]. Based on these experiments we might conclude that arm movement is represented in PMd as
a combination of these features (along with some others that we haven’t accounted for).
Example 2: Neurons in temporal cortex, especially the fusiform gyrus in humans [6] and superior temporal sulcus
portion of IT cortex in macaque monkeys [7, 8], preferentially ﬁre when the sub ject views pictures of faces . So the
features that the neurons are encoding seem to be “face” and “not-face”.
But how do we know what features the neurons are selecting for? The paradigm can be summarized as follows;
“...one day...having failed to drive a unit with any light stimulus, we waved a hand at the stimulus screen and elicited
a very vigorous response from the previously unresponsive neuron. We then spent the next 12 h testing various paper
cutouts in an attempt to ﬁnd the trigger feature for this unit” [9]. In other words, the experimenter presents many
stimuli while simultaneously monitoring neuronal activity and then concludes by identifying similarities amongst
stimuli that elicit a response [10]. This approach runs the risk of picking features that are conceptually simple to the
experimenter but that have little to do with the features encoded by neurons. In other words, the representation used
by neurons may not be simple or readily apparent.
The goal of this pro ject is to tackle the question, “How is information represented in the brain?” using machine
learning algorithms that remove experimental bias in feature selection. Let’s make some assumptions about neuron
ﬁring rates and then state the goal more clearly.

II. MODELING

Modeling Assumption 1: The neural state only depends on the instantaneous ﬁring rates, not relative spike times,
of each of the neurons. So at any point in time we can completely specify the state by the instantaneous ﬁring rates
of each of the neurons.
Modeling Assumption 2: Each neuron ﬁres with a Poisson distribution based on some underlying ﬁring rate.
The goal of this pro ject is to take spike time data from simultaneously recorded neurons and ﬁnd the underlying
rates that gave rise to these ﬁring patterns. The number of underlying ﬁring rates tell us approximately how many
features are being represented. It will be left for a future pro ject to determine what exactly the underlying ﬁring
rates represent.
If all of the neurons in the trial have the same underlying ﬁring rate we can use various smoothing techniques to
ﬁnd this rate as shown in Figure 1. The acausal methods use information from the whole spike train, across all times,
to compute the frequency at a speciﬁc time.
But what happens if the neurons do not have the same underlying ﬁring rate? The normalized variance [3], deﬁned
in Equation (1) where rtrial is the ﬁring rate on that trial and ¯r is the mean ﬁring rate across all trials, is 1 if poisson
neurons have the same underling rate and greater than 1 if the underlying rates are diﬀerent. The normalized variance
is shown in the bottom panel of Figure 2 for 10,000 neurons simulated with the same underlying rate (black line, top
panel) and 10,000 neurons each having diﬀerent noisy versions of this underlying rate (red lines, top panel). Starting
at 900 ms the noisy rates decay exponentially, with a time constant of 200 ms, to the underlying rate shown in black.
Only 10 out of the 10,000 noisy rates are shown in the top panel of Figure 2. The spike times for 10 neurons from
each group are shown in the middle panel.
× nX
trial=1
The normalized variance tells us when the neurons have diﬀerent underlying rates but does not tell us what those
rates are. Consider two neurons that have diﬀerent underlying rates that are related at all points in time as shown
by the black bar in Figure 3 [11]. The blue dots are the measured/noisy rates for the two neurons. The ﬁring rate

(rtrial (t) − ¯r(t))2
n − 1

NV(t) = c
¯r(t)

(1)

2

FIG. 1:

relationship between the two neurons is predicted by principle component analysis (PCA), red line, and factor analysis
(FA), green line. The dotted black line shows the two SD covariance ellipse.
Neuron 1 has a higher mean ﬁring rate and so has a higher variance than neuron 2 [12]. FA, which allows for
diﬀerent noise variances along diﬀerent dimensions and seeks the dimension of greatest covariance, is better able to
predict the ﬁring rate relationship than PCA which seeks the dimension of greatest variance and so is biased towards
neurons with higher ﬁring rates.

FA Notation:

A.

Initial Conditions

x ∼ N (0, I )
y|x ∼ N (µ + Λx, Ψ)

(2)
(3)

where x ∈ Rp and y ∈ Rq such that q > p.
The slope of the FA ﬁt is very sensitive to the initial values of the matrices Ψ and Λ. For example, in Figure 3 we
can match the covariance matrix ΛΛT + Ψ perfectly to the data even if we ﬁx one of the parameters at an arbitrary
value. For poisson spikes counted over a given interval the mean number of spikes equals the variance of spike counts.
Each element of the diagonal matrix Ψ is the variance of one of the neurons so we initialize these values to the
iterations if we provide a relevant scale by multiplying these random numbers by a factor of pdet(S )1/q /p where S
mean. If we initialize Λ with random normal elements EM requires ∼3 times more iterations to converge than when
initializing with an optimum value of Λ. We can reduce the number of iterations to ∼1.5 times the number of optimum
is the covariance matrix of the data. However, the real problem is that Λ and Ψ converge to a local optimum that
doesn’t accurately predict the underlying ﬁring rate. So given our value of Ψ let’s take the matrix derivative of the
log likelihood with respect to Λ, set it equal to zero, and ﬁnd an optimum initial Λ. The method for doing this is
borrowed from the ﬁrst step of J¨oreskog’s algorithm [13]. There is not suﬃcient space to write the whole procedure
(cid:19)
(cid:18)
mY
but, to start, we note that maximizing the log likelihood
(y(i) − µ)T (ΛΛT + Ψ)−1 (y(i) − µ)
i=1

1
(2π)q/2 |ΛΛT + Ψ|1/2

‘(µ, Λ, Ψ) = ln

exp

− 1
2

(4)

3

FIG. 2:

is equivalent to minimizing

FIG. 3:
‘0 = ln |ΛΛT + Ψ| + tr (cid:0)S (ΛΛT + Ψ)−1 (cid:1) − ln |S | − q
where S is the covariance matrix of the data. The log likelihood for the EM algorithm implemented with this optimum
initialization is shown in Figure 4.
Even given this optimum initialization I wasn’t sure that the local optimum that the EM algorithm converged to
was ideal for predicting the underlying rate so I also solved FA using the Newton-Raphson algorithm.
Ψ(t+1) = Ψ(t) − H −1∇Ψ ‘0

(5)

(6)

4

(7)

(8)

FIG. 4:

where H is the Hessian and
∂ ‘0
∂Ψi
∂ 2 ‘0
∂ΨiΨj

(cid:20)
(ΛΛT + Ψ)−1 (ΛΛT + Ψ − S )(ΛΛT + Ψ)−1 ∂ (ΛΛT + Ψ)
(cid:20)
(cid:21)
∂Ψi
(ΛΛT + Ψ)−1 ∂ (ΛΛT + Ψ)
(ΛΛT + Ψ)−1 ∂ (ΛΛT + Ψ)
∂Ψi
∂Ψj

(cid:21)

= tr

= tr

Both the EM and Newton-Raphson predictions for the underlying rate are plotted in Figure 3; given the same initial
conditions both converge to the same Λ and Ψ. The log likelihood for the Newton-Raphson algorithm as a function
of iteration number is shown in Figure 4. It converges faster than EM but is more computationally intensive.

III. GAUSSIAN-PROCESS FACTOR ANALYSIS [11]

If the underlying rate in Figure 4 increases/decreases continuously with time then FA has also solved the problem
of picking out a “neural tra jectory”, i.e. ﬁnding how the underlying rate changes over time.
For rates that change over time we would like to perform FA at each time point with the additional modeling
assumption that the underlying ﬁring rates vary smoothly. We will use Yu et al.’s gaussian-process factor analysis
(GPFA) algorithm [11].
Assume we have spike counts from q neurons. Divide the time interval into T nonoverlapping bins. Let yi,t be
the square root of the number of spike counts for neuron i in time bin t = 1, . . . , T . y:,t ∈ Rq×1 is related to a
low-dimensional latent neural state x:,t ∈ Rp×1 , p < q , via
y:,t |x:,t ∼ N (C x:,t + d, R)
(9)
where the model parameters are C ∈ Rq×p , d ∈ Rq×1 , and R ∈ Rq×q which is taken to be diagonal with elements
that are the independent variances of each neuron. To ensure that the neural state, x:,t varies smoothly over time let
xi,: ∼ N (0, Ki )
(cid:18)
(cid:19)
where xi,: ∈ R1×T and element (t1 , t2 ) of the covariance matrix Ki ∈ RT ×T is given by
− (t1 − t2 )2
n,i · δt1 ,t2 .
f ,i exp
Ki (t1 , t2 ) = σ2
+ σ2
2τ 2
i
f ,i ∈ R+ and σ2
n,i ∈ R+ are ﬁxed constants and δ is the Kroneker delta function. The model parameters
σ2
C, d, R, τ1 , . . . , τp are found using the expectation maximization (EM) algorithm [14]. We can then vary p to ﬁnd the
optimum dimension for the underlying neural state. The neural states from all time points are grouped into a matrix
X = [x:,1 , . . . , x:,T ] ∈ Rq×T which can be thought of as the “neural tra jectory”. For example, the neural tra jectory
from the previous FA example is shown in Figure 5 in black.

(10)

(11)

5

FIG. 5:

IV. TO DO

1. GPFA works because suﬃciently small portions of the neural tra jectory can be approximated by neurons having
the same variance and tracing out straight paths moving in one direction. One way to extend this would be to allow
diﬀerent GPFA approximations to be stitched together by letting C → Ct , d → dt , R → Rt in equation (9).
2. It can take several hours to learn the GPFA parameters. One possibility for improving speed is to implement
GPFA using the algorithm recently proposed by Zhao et al. [15].

[1] J. Messier, J. F. Kalaska (2000) Covariation of primate dorsal premotor cell activity with direction and amplitude during
a memorized-delay reaching task, Journal of Neurophysiology, 84, 152-165.
[2] M. Godschalk, R. N. Lemon, H. G. Kuypers, J. van der Steen (1985) The involvement of monkey premotor cortex neurones
in preparation of visually cued arm movements, Behav Brain Res, 18, 143-157.
[3] M. M. Churchland, B. M. Yu, S. I. Ryu, G. Santhanam, K. V. Shenoy (2006) Neural variability in premotor cortex provides
a signature of motor preparation, Journal of Neuroscience, 26 (14), 3697-3712.
[4] S. Hocherman, S. P. Wise (1991) Eﬀects of hand movement path on motor cortical activity in awake, behaving rhesus
monkeys, Exp Brain Res, 83, 285-302.
[5] J. E. Gomez, Q. Fu, D. Flament, T. J. Ebner (2000) Representation of accuracy in the dorsal premotor cortex, Eur J
Neurosci, 12, 3748-37600.
[6] J. G. Ojemann, G. A. Ojemann, E. Lettich (1992) Neuronal activity related to faces and matching in human right
nondominant temporal cortex, Brain, 115, 1-13.
[7] R. Desimone, T. D. Albright, C. G. Gross, C. Bruce (1984) Stimulus-selective properties of inferior temporal neurons in
the macaque, Journal of Neuroscience, 4, 2051-2062.
[8] D. Y. Tsao, W. A. Freiwald, R. B. H. Tootell, M. S. Livingstone (2006) A Cortical Region Consisting Entirely of Face-
Selective Cells, Science, 311, 670-674.
[9] C. G. Gross, C. E. Rocha-Miranda, D. B. Bender (1972) Visual Properties of Neurons in Inferotemporal Cortex of the
Macaque, Journal of Neurophysiology, 35, 96-111.
[10] A. H. Bell, F. Hadj-Bouziane, J. B. Frihauf, R. B. H. Tootell, L. G. Ungerleider (2009) Ob ject Representations in the
Temporal Cortex of Monkeys and Humans as Revealed by Functional Magnetic Resonance Imaging), Journal of Neuro-
physiology, 101, 688-700.
[11] B. M. Yu, J. P. Cunningham, G. Santhanam, S. I. Ryu, K. V. Shenoy, M. Sahani (2009) Gaussian-Process Factor Analysis
for Low-Dimensional Single-Trial Analysis of Neural Population Activity, Journal of Neurophysiology, 102, 614-635.
[12] P. Dayan, L. F. Abbott (2001) , Theoretical Neuroscience, cambridge, MA: MIT Press.
[13] K. G. J¨oreskog (1967) Some contributions to maximum likelihood factor analysis, Psychometrika, 32, 443-82.
[14] A. P. Dempster, N. M. Laird, D. B. Rubin (1977) Maximum Likelihood from Incomplete Data via the EM Algorithm (with
discussion), Journal of the Royal Statistical Society. Series B, 39 (1), 1-38.
[15] J. H. Zhao, P. L. H. Yu (2008) Fast ML Estimation for the Mixture of Factor Analyzers via an ECM Algorithm, IEEE
Transactions on Neural Networks, 19 (11), 1956-1961.


Relax then Compensate:
On Max-Product Belief Propagation and More

Arthur Choi
Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095
aychoi@cs.ucla.edu

Adnan Darwiche
Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095
darwiche@cs.ucla.edu

Abstract

We introduce a new perspective on approximations to the maximum a posteriori
(MAP) task in probabilistic graphical models, that is based on simplifying a given
instance, and then tightening the approximation. First, we start with a structural
relaxation of the original model. We then infer from the relaxation its deﬁcien-
cies, and compensate for them. This perspective allows us to identify two distinct
classes of approximations. First, we ﬁnd that max-product b elief propagation can
be viewed as a way to compensate for a relaxation, based on a particular idealized
case for exactness. We identify a second approach to compensation that is based
on a more reﬁned idealized case, resulting in a new approxima tion with distinct
properties. We go on to propose a new class of algorithms that, starting with a
relaxation, iteratively seeks tighter approximations.

1

Introduction

Relaxations are a popular approach for tackling intractable optimization problems. Indeed, for ﬁnd-
ing the maximum a posteriori (MAP) assignment in probabilistic graphical models, relaxations play
a key role in a variety of algorithms. For example, tree-reweighted belief propagation (TRW-BP) can
be thought of as a linear programming relaxation of an integer program for a given MAP problem
[1, 2]. Branch-and-bound search algorithms for ﬁnding opti mal MAP solutions, such as [3, 4], rely
on structural relaxations, such as mini-bucket approximations, to provide upper bounds [4, 5].

Whether a relaxation is used as an approximation on its own, or as a guide for ﬁnding optimal
solutions, a trade-off is typically made between the quality of an approximation and the complexity
of computing it. We illustrate here instead how it is possible to tighten a given relaxation itself,
without impacting its structural complexity.

More speci ﬁcally, we propose here an approach to approximat
ing a given MAP problem by perform-
ing two steps. First, we relax the structure of a given probabilistic graphical model, which results in
a simpler model whose MAP solution provides an upper bound on that of the original. Second, we
compensate for the relaxation by introducing auxiliary parameters, which we use to restore certain
properties, leading to a tighter approximation. We shall in fact propose two distinct properties on
which a compensation can be based. The ﬁrst is based on a simpl
i ﬁed case where a compensation
can be guaranteed to yield exact results. The second is based on a notion of an ideal compensation,
that seeks to correct for a relaxation more directly. As we shall see, the ﬁrst approach leads to a
new semantics for the max-product belief propagation algorithm. The second approach leads to
another approximation that further yields upper bounds on the MAP solution. We further propose
an algorithm for ﬁnding such a compensation, that starts wit h a relaxation and iteratively provides
monotonically decreasing upper bounds on the MAP solution (at least empirically).

Proofs of results are given in the auxiliary Appendix.

1

2 MAP Assignments

Let M be a factor graph over a set of variables X, inducing a distribution Pr (x) ∝ Qa ψa (xa )
where x = {X1 = x1 , . . . , Xn = xn} is an assignment of factor graph variables Xi to states xi , and
where a is an index to the factor ψa (Xa ) over the domain Xa ⊆ X. We seek the maximum a
posteriori (MAP) assignment x⋆ = argmaxx Qa ψa (xa ). We denote the log of the value of a MAP
assignment x⋆ by:

x Y
x X
⋆ = log max
ψa (xa ) = max
map
a
a
which we refer to more simply as the MAP value. Note that there may be multiple MAP assignments
x⋆ , so we may refer to just the value map
⋆ when the particular assignment is not relevant. Next,
if z is an assignment over variables Z ⊆ X, then let x ∼ z denote that x and z are compatible
assignments, i.e., they set their common variables to the same states. Consider then the MAP value
under a partial assignment z:

log ψa (xa )

x∼z X
map(z) = max
a

log ψa (xa ).

We will, in particular, be interested in the MAP value map(X = x) where we assume a single vari-
able X is set to a particular state x. We shall also refer to these MAP values more generally as
map(.), without reference to any particular assignment.

3 Relaxation

The structural relaxations that we consider here are based on the relaxation of equivalence con-
straints from a model M, where an equivalence constraint Xi ≡ Xj is a factor ψeq (Xi , Xj ) over
two variables Xi and Xj that have the same states. Further, ψeq (xi , xj ) is 1 if xi = xj and 0 oth-
erwise. We call an assignment x valid, with respect to an equivalence constraint Xi ≡ Xj , if it sets
variables Xi and Xj to the same state, and invalid otherwise. Note that when we remove an equiva-
lence constraint from a model M, the values map(x) for valid conﬁgurations x do not change, since
log 1 = 0. However, the values map(x) for invalid conﬁgurations can increase, since they are −∞
prior to the removal. In fact, they could overtake the optimal value map
⋆ . Thus, the MAP value
after relaxing an equivalence constraint in M is an upper bound on the original MAP value.
It is straightforward to augment a model M to another where equivalence constraints can be relaxed.
Consider, for example, a factor ψ1 (A, B , C ). We can replace the variable C in this factor with a
1 (A, B , C ′ ). When we now add the factor ψ2 (C, C ′ ) for the
clone variable C ′ , resulting in a factor ψ ′
equivalence constraint C ≡ C ′ , we have a new model M′ which is equivalent to the original model
M, in that an assignment x in M corresponds to an assignment x′ in M′ , where assignment x′ sets
a variable and its clone to the same state. Moreover, the value map(x) in model M is the same as
the value map
′ (x′ ) in model M′ .
We note that a number of structural relaxations can be reduced to the removal of equivalence con-
straints, including relaxations found by deleting edges [6, 7], as well as mini-bucket approximations
[5, 4]. In fact, the example above can be considered a relaxation where we delete a factor graph
edge C → ψ1 , substituting clone C ′ in place of variable C . Note that mini-bucket approximations
in particular have enabled algorithms for solving MAP problems via branch-and-bound search [3, 4].

4 Compensation

Suppose that we have a model M with MAP values map(.). Say that we remove the equivalence
constraints in M, resulting in a relaxed model with MAP values r-map(.). Our goal is to identify
a compensated model M′ with MAP values c-map(.) that is as tractable to compute as the values
r-map(.), but yielding tighter approximations of the original values map(.).
To this end, we introduce into the relaxation additional factors ψij ;i (Xi ) and ψij ;j (Xj ) for each
equivalence constraint Xi ≡ Xj that we remove. Equivalently, we can introduce the log factors
θ(Xi ) = log ψij ;i (Xi ) and θ(Xj ) = log ψij ;j (Xj ) (we omit the additional factor indices, as they

2

will be unambiguous from the context). These new factors add new parameters into the approxima-
tion, which we shall use to recover a weaker notion of equivalence into the model. More speci ﬁcally,
given a set of equivalence constraints Xi ≡ Xj to relax, we have the original MAP values map(.),
the relaxation r-map(.) and the compensation c-map(.), where:

• map(z) = maxx∼z Pa log ψa (xa ) + PXi≡Xj
• r-map(z) = maxx∼z Pa log ψa (xa )
• c-map(z) = maxx∼z Pa log ψa (xa ) + PXi≡Xj
Note that the auxiliary factors θ of the compensation do not introduce additional complexity to the
relaxation, in the sense that the treewidth of the resulting model is the same as that of the relaxation.

log ψeq (Xi = xi , Xj = xj )

θ(Xi = xi ) + θ(Xj = xj )

Consider then the case where an optimal assignment x⋆ for the relaxation happens to set variables
Xi and Xj to the same state x, for each equivalence constraint Xi ≡ Xj that we relaxed. In this
case, the optimal solution for the relaxation is also an optimal solution for the original model, i.e.,
r-map
⋆ . On the other hand, if a relaxation’s optimal assignment sets Xi and Xj to different
⋆ = map
states, then it is not a valid assignment for the original model M, as it violates the equivalence
constraint and thus has log probability −∞.
Consider, for a given equivalence constraint Xi ≡ Xj , the relaxation’s MAP values r-map(Xi = x)
and r-map(Xj = x) when we set, respectively, a single variable Xi or Xj to a state x. If for all states
x we ﬁnd that
r-map(Xi = x) 6= r-map(Xj = x), then we can infer that the MAP assignment sets
variables Xi and Xj to different states: the MAP value when we set Xi to a state x is different than
the MAP value when we set Xj to the same state. We can then ask of a compensation, for all states
x, that c-map(Xi = x) = c-map(Xj = x), enforcing a weaker notion of equivalence. In this case, if
there is a MAP assignment that sets variable Xi to a state x, then there is at least a MAP assignment
that sets variable Xj to the same state, even if there is no MAP assignment that sets both Xi and Xj
to the same state at the same time.
We now want to identify parameters θ(Xi ) and θ(Xj ) to compensate for a relaxation in this manner.
We propose two approaches: (1) based on a condition for exactness in a special case, and (2) based
on a notion of ideal compensations. To get the intuitions behind these approaches, we consider ﬁrst
the simpli ﬁed case where a single equivalence constraint is
relaxed.

4.1

Intuitions: Splitting a Model into Two

Consider the case where relaxing a single equivalence constraint Xi ≡ Xj splits a model M into two
independent sub-models, Mi and Mj , where sub-model Mi contains variable Xi and sub-model
Mj contains variable Xj . Intuitively, we would like the parameters added in one sub-model to
summarize the relevant information about the other sub-model. In this way, each sub-model could
independently identify their optimal sub-assignments. For example, we can use the parameters:
and
θ(Xi = x) = mapj (Xj = x)
θ(Xj = x) = mapi (Xi = x).
Since sub-models Mi and Mj become independent after relaxing the single equivalence constraint
Xi ≡ Xj , computing these parameters is sufﬁcient to reconstruct th e MAP solution for the original
model M. In particular, we have that θ(Xi = x) + θ(Xj = x) = map(Xi = x, Xj = x), and further
that map
⋆ = maxx [θ(Xi = x) + θ(Xj = x)].
We propose then that the parameters of a compensation, with MAP values c-map(.), should satisfy
the following condition:
c-map(Xi = x) = c-map(Xj = x) = θ(Xi = x) + θ(Xj = x) + γ
(1)
for all states x. Here γ is an arbitrary normalization constant, but the choice γ = 1
2 c-map
⋆ results in
simpler semantics. The following proposition conﬁrms that
this choice of parameters does indeed
reﬂect our earlier intuitions, showing that this choice all ows us to recover exact solutions in the
idealized case when a model is split into two.

Proposition 1 Let map(.) denote the MAP values of a model M, and let c-map(.) denote the MAP
values of a compensation that results from relaxing an equivalence constraint Xi ≡ Xj that split M
into two independent sub-models. Then the compensation has parameters satisfying Equation 1 iff
c-map(Xi = x) = c-map(Xj = x) = map(Xi = x, Xj = x) + γ .

3

Note that the choice γ = 1
2 c-map
⋆ implies that θ(Xi = x) + θ(Xj = x) = map(Xi = x, Xj = x) in
the case where relaxing an equivalent constraint splits a model into two.

In the case where relaxing an equivalence constraint does not split a model into two, a compensation
satisfying Equation 1 at least satis ﬁes a weaker notion of eq uivalence. We might expect that such a
compensation may lead to more meaningful, and hopefully more accurate, approximations than a re-
laxation. Indeed, this compensation will eventually lead to a generalized class of belief propagation
approximations. Thus, we call a compensation satisfying Equation 1 a R EC -B P approximation.

4.2

Intuitions: An Ideal Compensation

In the case where a single equivalence constraint Xi ≡ Xj is relaxed, we may imagine the possibility
of an “ideal ” compensation where, as far as computing the MAP
solution is concerned, a compen-
sated model is as good as a model where the equivalence constraint was not relaxed. Consider then
the following proposal of an ideal compensation, which has the following two properties. First, it
has valid conﬁgurations :
c-map(Xi = x) = c-map(Xj = x) = c-map(Xi = x, Xj = x)
for all states x. Second it has scaled values for valid conﬁgurations:
c-map(Xi = x, Xj = x) = κ · map(Xi = x, Xj = x).
for all states x, and for some κ > 1. If a compensation has valid conﬁgurations, then its optima l
solution sets variables Xi and Xj to the same state, and is thus a valid assignment for the orig-
inal instance (it satis ﬁes the equivalence constraint). Mo reover, if it has scaled values, then the
compensation further allows us to recover the MAP value as well. A compensation having valid
conﬁgurations and scaled values is thus ideal as it is sufﬁci
ent for us to recover the exact solution.

It may not always be possible to ﬁnd parameters that lead to an ideal compensation. However, we
propose that a compensation’s parameters should satisfy:
(2)
c-map(Xi = x) = c-map(Xj = x) = 2 · [θ(Xi = x) + θ(Xj = x)]
for all states x, where we choose κ = 2. As the following proposition tells us, if a compensation is
an ideal one, then it must at least satisfy Equation 2.

Proposition 2 Let map(.) denote the MAP values of a model M, and let c-map(.) denote the MAP
values of a compensation that results from relaxing an equivalence constraint Xi ≡ Xj in M. If
c-map(.) has valid conﬁgurations and scaled values, then c-map(.) satis ﬁes Equation 2.

We thus call a compensation satisfying Equation 2 a R EC - I compensation.

We note that other values of κ > 1 can be used, but the choice κ = 2 given above re-
sults in simpler semantics. In particular, if a compensation happens to satisfy c-map(Xi = x) =
c-map(Xj = x) = c-map(Xi = x, Xj = x) for some state x, we have that θ(Xi = x) + θ(Xj = x) =
map(Xi = x, Xj = x) (i.e., the parameters alone can recover an original MAP value).
Before we discuss the general case where we relax multiple equivalence constraints, we highlight
ﬁrst a few properties shared by both R EC -B P and R EC - I compensations, that shall follow from more
general results that we shall present. First, if the optimal assignment x⋆ for a compensation sets the
variables Xi and Xj to the same state, then: (1) the assignment x⋆ is also optimal for the original
model M; and (2) 1
⋆ . In the case where x⋆ does not set variables Xi and Xj to the
2 c-map
⋆ = map
same state, the value c-map
⋆ gives at least an upper bound that is no worse than the bound given by
the relaxation alone. In particular:

1
2
Thus, at least in the case where a single equivalence constraint is relaxed, the compensations implied
by Equations 1 and 2 do indeed tighten a relaxation (see the auxiliary Appendix for further details).

⋆ ≤ r-map
c-map

⋆ ≤
map

⋆ .

4.3 General Properties

In this section, we identify the conditions that compensations should satisfy in the more general case
where multiple equivalence constraints are relaxed, and further highlight some of their properties.

4

Suppose that k equivalence constraints Xi ≡ Xj are relaxed from a given model M. Then compen-
sations R EC -B P and R EC - I seek to recover into the relaxation two weaker notions of equivalence.

First, a R EC -B P compensation has auxiliary parameters satisfying:
c-map(Xi = x) = c-map(Xj = x) = θ(Xi = x) + θ(Xj = x) + γ
⋆ . We then approximate the exact MAP value map
1+k c-map
⋆ by the value
where γ = k
The following theorem relates R EC -B P to max-product belief propagation.

(3)
1+k c-map
⋆ .
1

Theorem 1 Let map(.) denote the MAP values of a model M, and let c-map(.) denote the MAP
values of a compensation that results from relaxing enough equivalence constraints Xi ≡ Xj in M
to render it fully disconnected. Then a compensation whose parameters satisfy Equation 3 has
values exp{c-map(Xi = x)} that correspond to the max-marginals of a ﬁxed-point of max- product
belief propagation run on M, and vice-versa.

Loopy max-product belief propagation is thus the degenerate case of a R EC -B P compensation, when
the approximation is fully disconnected (by deleting every factor graph edge, as deﬁned in Sec-
tion 3). Approximations need not be this extreme, and more structured approximations correspond
to instances in the more general class of iterative joingraph propagation approximations [8, 6].

Next, a R EC - I compensation has parameters satisfying:
c-map(Xi = x) = c-map(Xj = x) = (1 + k)[θ(Xi = x) + θ(Xj = x)]
We again approximate the exact MAP value map
⋆ with the value
1+k c-map
⋆ .
1
In both compensations, it is possible to determine if the optimal assignment x⋆ of a compensation is
an optimal assignment for the original model M: we need only check that it is a valid assignment.

(4)

Theorem 2 Let map(.) denote the MAP values of a model M, and let c-map(.) denote the MAP val-
ues of a compensation that results from relaxing k equivalence constraints Xi ≡ Xj . If the compen-
sation has parameters satisfying either Eqs. 3 or 4, and if x⋆ is an optimal assignment for the com-
pensation that is also valid, then: (1) x⋆ is optimal for the model M, and (2)
1+k c-map
⋆ .
1
⋆ = map
This result is analogous to results for max-product BP, TRW-BP, and related algorithms [9, 2, 10].

A R EC - I compensation has additional properties over a R EC -B P compensation. First, a R EC - I com-
pensation yields upper bounds on the MAP value, whereas R EC -B P does not yield a bound in general.

Theorem 3 Let map(.) denote the MAP values of a model M, and let c-map(.) denote the MAP
values of a compensation that results from relaxing k equivalence constraints Xi ≡ Xj . If the com-
⋆ .
1+k c-map
pensation has parameters satisfying Equation 4, then map
⋆ ≤ 1
We remark now that a relaxation alone has analogous properties. If an assignment x⋆ is optimal
for a relaxation with MAP values r-map(.), and it is also a valid assignment for a model M (i.e.,
it does not violate the equivalence constraints Xi ≡ Xj ), then x⋆ is also optimal for M, where
r-map(x⋆ ) = map(x⋆ ) (since they are composed of the same factor values). If an assignment x⋆ of
a relaxation is not valid for model M, then the MAP value of the relaxation is an upper bound on
the original MAP value. On the other hand, R EC - I compensations are tighter approximations than
the corresponding relaxation, at least in the case when a single equivalence constraint is relaxed:
⋆ . When we relax multiple equivalence constraints we ﬁnd, at le ast
⋆ ≤ r-map
2 c-map
⋆ ≤ 1
map
empirically, that R EC - I bounds are never worse than relaxations, although we leave this point open.

The following theorem has implications for MAP solvers that rely on relaxations for upper bounds.

Theorem 4 Let map(.) denote the MAP values of a model M, and let c-map(.) denote the MAP val-
ues of a compensation that results from relaxing k equivalence constraints Xi ≡ Xj . If the compen-
sation has parameters satisfying Eq. 4, and if z is a partial assignment that sets the same sign to vari-
ables Xi and Xj , for any equivalence constraint Xi ≡ Xj relaxed, then: map(z) ≤ 1
1+k c-map(z).
Algorithms, such as those in [3, 4], perform a depth- ﬁrst bra nch-and-bound search to ﬁnd an optimal
MAP solution. They rely on upper bounds of a MAP solution, under partial assignments, in order to
prune the search space. Thus, any method capable of providing upper bounds tighter than those of a
relaxation, can potentially have an impact in the performance of a branch-and-bound MAP solver.

5

Algorithm 1 RelaxEq-and-Compensate (R EC)
input: a model M with k equivalence constraints Xi ≡ Xj
output: a compensation M′
main:
0 ← result of relaxing all Xi ≡ Xj in M
1: M′
2: add to M′
0 the factors θ(Xi ), θ(Xj ), for each Xi ≡ Xj
2 r-map
3: initialize all parameters θ0 (Xi = x), θ0 (Xj = x), e.g., to 1
⋆
4: t ← 0
5: while parameters have not converged do
6:
t ← t + 1
for each equivalence constraint Xi ≡ Xj do
7:
t−1 , by:
update parameters θ(Xi = x)t , θ(Xj = x)t , computed using compensation M′
8:
for R EC -B P: Equations 5 & 6
9:
for R EC - I: Equations 7 & 8
10:
θt (Xi ) ← q · θt (Xi ) + (1 − q) · θt−1 (Xi ) and θt (Xj ) ← q · θt (Xj ) + (1 − q) · θt−1 (Xj )
11:
12: return M′
t

5 An Algorithm to Find Compensations

Up to this point, we have not discussed how to actually ﬁnd the auxiliary parameters θ(Xi = x) and
θ(Xj = x) of a compensation. However, Equations 3 and 4 naturally suggest iterative algorithms for
ﬁnding R EC -B P and R EC - I compensations. Consider, for the case of R EC -B P, the fact that parameters
satisfy Equation 3 iff they satisfy:
θ(Xi = x) = c-map(Xj = x) − θ(Xj = x) − γ
θ(Xj = x) = c-map(Xi = x) − θ(Xi = x) − γ
This suggests an iterative ﬁxed-point procedure for ﬁnding
the parameters of a compensation that
satisfy Equation 3. First, we start with an initial compensation with MAP values c-map0 (.), where
parameters have been initialized to some value. For an iteration t > 0, we can update our parameters
using the compensation from the previous iteration:
θt (Xi = x) = c-mapt−1 (Xj = x) − θt−1 (Xj = x) − γt−1
(5)
θt (Xj = x) = c-mapt−1 (Xi = x) − θt−1 (Xi = x) − γt−1
(6)
where γt−1 = k
t−1 . If at some point, the parameters of one iteration do not change in
1+k c-map
⋆
the next, then we can say that the iterations have converged, and that the compensation satis ﬁes
Equation 3. Similarly, for R EC - I compensations, we use the update equations:
1+k c-mapt−1 (Xj = x) − θt−1 (Xj = x)
1
θt (Xi = x) =
1+k c-mapt−1 (Xi = x) − θt−1 (Xi = x)
1
θt (Xj = x) =
to identify compensations that satisfy Equation 4.

(7)
(8)

Algorithm 1 summarizes our proposal to compensate for a relaxation, using the iterative procedures
for R EC -B P and R EC - I . We refer to this algorithm more generically as RelaxEq-and-Compensate
(R EC). Note that in Line 11, we further damp the updates by q , which is typical for such algorithms
(we use q = 1
2 r-map
2 ). Note also that in Line 3, we suggest that we initialize parameters by 1
⋆ . The
⋆ .1 That
consequence of this is that our initial compensation has the MAP value 1
1+k c-map
0 = r-map
⋆
is, the initial compensation is equivalent to the relaxation, for both R EC -B P and R EC - I . Typically,
both algorithms tend to have compensations with decreasing MAP values. R EC -B P may eventually
have MAP values that oscillate however, and may not converge. On the other hand, by Theorem 3,
we know that a R EC - I compensation must yield an upper bound on the true MAP value map
⋆ .
⋆ from the relaxation, R EC - I yields, at least empirically,
Starting with an initial upper bound r-map
monotonically decreasing upper bounds on the true MAP value from iteration to iteration. We
explore this point further in the following section.

1

c-map

0 = maxx c-map0 (x) = maxx [r-map(x) + PXi≡Xj
⋆
θ(Xi = x) + θ(Xj = x)]
⋆ + k · r-map
= maxx [r-map(x) + k · r-map
⋆ ] = r-map
⋆

6

random grid (REC-BP)

random frustrated grid (REC-I)

random frustrated grid (REC-BP)

1.0

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

0.0
0

1.0

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

0.0
0

1.0

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

0.0
0

1.0

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

0.0
0

iterations

5000

random grid (REC-I)

iterations

5000

1.0

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

0.0
0

iterations

5000

iterations

5000

random frustrated grid (REC-I)

random frustrated grid (REC-I)

1.0

0.8

0.6

0.4

0.2

r
o
r
r
e
 
n
o
i
t
a
m
i
x
o
r
p
p
a

0.0
0

iterations

5000

iterations

5000

Figure 1: The R EC algorithm in 10 × 10 grids. Left column: random grids, using R EC -B P (top) and
R EC - I (bottom). Center column: frustrated grids, using R EC - I with p = 1
2 (top), p = 1
3 (bottom).
Right column: frustrated grids, using R EC -B P (top) with a fully disconnected relaxation, and R EC - I
(bottom) with a relaxation with max cluster size 3.

6 Experiments

Our goal in this section is to highlight the degree to which different types of compensations can
tighten a relaxation, as well as to highlight the differences in the iterative algorithms to ﬁnd them.
We evaluated our compensations using randomly parametrized 10 × 10 grid networks. We judge the
quality of an approximation by the degree to which a compensation is able to improve a relaxation.
1+k c-map
⋆
⋆
1
−map
⋆ which is zero when the compensation is
In particular, we measured the error E =
r-map
⋆−map
exact, and one when the compensation is equivalent to the relaxation (remember that we initialize the
R EC algorithm, for both types of compensations, with parameters that led to an initial compensation
with an optimal MAP value
0 = r-map
⋆ ). Note also that we use no instances where the
1+k c-map
1
⋆
error E is undeﬁned, i.e.,
r-map
⋆ − c-map
⋆ = 0, where the relaxation alone was able to recover the
exact solution.
We ﬁrst consider grid networks where factors ψa (xi , xj ) were assigned to grid edges (i, j ), with
values drawn uniformly at random from 0 to 1 (we assigned no factors to nodes). We assumed ﬁrst
the coarsest possible relaxation, one that results in a fully disconnected approximation, and where
the MAP value is found by maximizing factors independently.2 We expect a relaxation’s upper
bound to be quite loose in this case.

Consider ﬁrst Figure 1 (left), where we generated ten random grid networks (we plotted only ten
for clarity) and plotted the compensation errors (y -axis) as they evolved over iterations (x-axis). At
iteration 0, the MAP value of each compensation is equivalent to that of the relaxation (by design).
We see that, once we start iterating, that both methods of compensation can tighten the approxima-
tion of our very coarse relaxation. For R EC -B P, we do so relatively quickly (in fewer iterations),
and to exact or near-exact levels (note that the 10 instances plotted behave similarly). For R EC - I ,
convergence is slower, but the compensation is still a signi ﬁcant improvement over the relaxation.
Moreover, it is apparent that further iterations would beneﬁt the compensation further.

We next generated random grid networks with frustrated interactions. In particular, each edge was
2 . An attractive
given either an attractive factor or repulsive factor, at random each with probability 1
factor ψa (Xi , Xj ) was given a value at random from 1 − p to 1 if xi = xj and a value from 0 to

2For each factor ψa and for each variable X in ψa , we replaced variable X with a unique clone ˆX and
introduced the equivalence constraint X ≡ ˆX . When we then relax all equivalence constraints, the resulting
factor graph is fully disconnected. This corresponds to deleting all factor graph edges, as described in Section 3.

7

p if xi 6= xj , which favors conﬁgurations xi = xj when p ≤ 1
2 . Similarly for repulsive factors,
which favors instead conﬁgurations where xi 6= xj . It is well known that belief propagation tends to
not converge in networks with frustrated interactions [11]. Non-convergence is the primary failure
mode for belief propagation, and in such cases, we may try to use instead R EC - I . We generated 10
random grid networks with p = 1
2 and another 10 networks with p = 1
3 . Although the frustration
in these networks is relatively mild, R EC -B P did not converge in any of these cases. On the other
hand, R EC - I compensations were relatively well behaved, and produced monotonically decreasing
upper bounds on the MAP value; see Figure 1 (center). Although the degree of compensation is not
as dramatic, we note that we are compensating for a very coarse relaxation (fully disconnected).
In Figure 1 (right), we considered frustrated grid networks where p = 1
10 , where R EC -B P converged
in only one of 10 networks generated. Moreover, we can see in that one instance, R EC -B P converges
below the true MAP value; remember that by Theorem 3, R EC - I compensations always yield upper
bounds. In the case of R EC - I , the compensations did not improve signi ﬁcantly on the full y discon-
nected relaxations (not shown). It is, however, straightforward to try less extreme relaxations. For
example, we used the mini-buckets-based approach to relaxation proposed in [4], and identi ﬁed re-
laxed models M′ with jointrees that had a maximum cluster size of 3 (c.f., [12] which re-introduced
constraints over triples). Surprisingly, this was enough for R EC - I to compensate for the relaxation
completely (to within 10−8 ) in 7 of the 10 instances plotted. R EC -B P beneﬁts from added structure
as well, converging and compensating completely (to within 10−4 ) in 9 of 10 instances (not plotted).

7 Discussion

There are two basic concepts underlying our proposed framework. The ﬁrst is to relax a problem by
dropping equivalence constraints. The second is that of compensating for a relaxation in ways that
can capture existing algorithms as special cases, and in ways that allow us to design new algorithms.
The idea of using structural relaxations for upper-bounding MAP solutions in probabilistic graphical
models goes back to mini-bucket approximations [13], which can be considered to be a particular
way of relaxing equivalence constraints from a model [4]. In this paper, we propose further a way
to compensate for these relaxations, by restoring a weaker notion of equivalence. One approach to
compensation identi ﬁed a generalized class of max-product belief propagation approximations. We
then identi ﬁed a second approach that led to another class of approximations that we have observed
to yield tighter upper bounds on MAP solutions as compared to a relaxation alone.

An orthogonal approach to upper-bounding MAP solutions is based on linear programming (LP)
relaxations, which has seen signi ﬁcant interest in recent y ears [1, 2]. This perspective is based on
formulating MAP problems as integer programs, whose solutions are upper-bounded by tractable LP
relaxations. A related approach based on Lagrangian relaxations is further capable of incorporating
structural simpli ﬁcations [14]. Indeed, there has been sig ni ﬁcant interest in identifying a precise
connection between belief propagation and LP relaxations [2, 10].

In contrast to the above approaches, compensations further guarantee, in Theorem 4, upper bounds
on MAP solutions under any partial assignment (without rerunning the algorithm). This property
has the potential to impact algorithms, such as [3, 4], that rely on such upper bounds, under partial
assignments, to perform a branch-and-bound search for optimal MAP solutions.3 Further, as we
approximate MAP by computing it exactly in a compensated model, we avoid the difﬁculties that al-
gorithms such as max-product BP and related algorithms face, which infer MAP assignments using
max-marginals (which may not have unique maximal states), which is based on local information
only [1]. The perspective that we propose further allows us to identify the intuitive differences be-
tween belief propagation and an upper-bound approximation, namely that they arise from different
notions of compensation. We hope that this perspective will enable the design of new approxima-
tions, especially in domains where speci ﬁc notions of compe nsation may suggest themselves.

Acknowledgments

This work has been partially supported by NSF grant #IIS-0916161.

3We investigated the use of R EC - I approximations in depth- ﬁrst branch-and-bound search for solving
weighted Max-SAT problems, where we were able to use a more specialized iterative algorithm [15].

8

References

[1] Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsky. MAP estimation via agreement
on trees: message-passing and linear programming. IEEE Transactions on Information Theory,
51(11):3697–3717, 2005.
[2] Amir Globerson and Tommi Jaakkola. Fixing max-product: Convergent message passing al-
gorithms for MAP LP-relaxations. In NIPS, pages 553–560, 2008.
[3] Radu Marinescu, Kalev Kask, and Rina Dechter. Systematic vs. non-systematic algorithms for
solving the MPE task. In UAI, pages 394–402, 2003.
[4] Arthur Choi, Mark Chavira, and Adnan Darwiche. Node splitting: A scheme for generating
upper bounds in Bayesian networks. In UAI, pages 57–66, 2007.
[5] Rina Dechter and Irina Rish. Mini-buckets: A general scheme for bounded inference. J. ACM,
50(2):107–153, 2003.
[6] Arthur Choi and Adnan Darwiche. An edge deletion semantics for belief propagation and its
practical impact on approximation quality. In AAAI, pages 1107–1114, 2006.
[7] Arthur Choi and Adnan Darwiche. Approximating the partition function by deleting and then
correcting for model edges. In UAI, pages 79–87, 2008.
[8] Rina Dechter, Kalev Kask, and Robert Mateescu. Iterative join-graph propagation. In UAI,
pages 128–136, 2002.
[9] Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsky. Tree consistency and bounds on
the performance of the max-product algorithm and its generalizations. Statistics and Comput-
ing, 14:143–166, 2004.
[10] Yair Weiss, Chen Yanover, and Talya Meltzer. MAP estimation, linear programming and belief
propagation with convex free energies. In UAI, 2007.
[11] Gal Elidan, Ian McGraw, and Daphne Koller. Residual belief propagation: Informed schedul-
ing for asynchronous message passing. In UAI, 2006.
[12] David Sontag, Talya Meltzer, Amir Globerson, Tommi Jaakkola, and Yair Weiss. Tightening
LP relaxations for MAP using message passing. In UAI, pages 503–510, 2008.
[13] Rina Dechter. Mini-buckets: a general scheme for approximation in automated reasoning.
In Proc. International Joint Conference on Arti ﬁcial Intelli gence (IJCAI), pages 1297–1302,
1997.
[14] Jason K. Johnson, Dmitry M. Malioutov, and Alan S. Willsky. Lagrangian relaxation for
MAP estimation in graphical models.
In Proceedings of the 45th Allerton Conference on
Communication, Control and Computing, pages 672–681, 2007.
[15] Arthur Choi, Trevor Standley, and Adnan Darwiche. Approximating weighted Max-SAT prob-
lems by compensating for relaxations. In Proceedings of the 15th International Conference on
Principles and Practice of Constraint Programming (CP), pages 211–225, 2009.

9


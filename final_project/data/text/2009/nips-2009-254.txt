A General Projection Property for Distribution Families

Yao-Liang Yu Yuxi Li Dale Schuurmans Csaba Szepesv ´ari
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8 Canada
{yaoliang,yuxi,dale,szepesva}@cs.ualberta.ca

Abstract

Surjectivity of linear projections between distribution families with ﬁxed mean
and covariance (regardless of dimension) is re-derived by a new proof. We further
extend this property to distribution families that respect additional constraints,
such as symmetry, unimodality and log-concavity. By combining our results with
classic univariate inequalities, we provide new worst-case analyses for natural
risk criteria arising in classiﬁcation, optimization, portfolio selection and Markov
decision processes.

1 Introduction

In real applications, the model of the problem at hand inevitably embodies some form of uncertainty:
the parameters of the model are usually (roughly) estimated from data, which themselves can be
uncertain due to various kinds of noises. For example, in ﬁnance, the return of a ﬁnancial product
can seldom be known exactly beforehand. Despite this uncertainty, one still usually has to take action
in the underlying application. However, due to uncertainty, any attempt to behave “optimally” in the
world must take into account plausible alternative models.
Focusing on problems where uncertain data/parameters are treated as random variables and the
model consists of a joint distribution over these variables, we initially assume prior knowledge
that the ﬁrst and second moments of the underlying distribution are known, but the distribution is
otherwise arbitrary. A parametric approach to handling uncertainty in such a setting would be to ﬁt a
speciﬁc parametric model to the known moments and then apply stochastic programming techniques
to solve for an optimal decision. For example, ﬁtting a Gaussian model to the constraints would be
a popular choice. However, such a parametric strategy can be too bold, hard to justify, and might
incur signiﬁcant loss if the ﬁtting distribution does not match the true underlying distribution very
well. A conservative, but more robust approach would be to take a decision that was “protected” in
the worst-case sense; that is, behaves optimally assuming that nature has the freedom to choose an
adverse distribution. Such a minimax formulation has been studied in several ﬁelds [1; 2; 3; 4; 5; 6]
and is also the focus of this paper. Although Bayesian optimal decision theory is a rightfully well-
established approach for decision making under uncertainty, minimax has proved to be a useful
alternative in many domains, such as ﬁnance, where it is difﬁcult to formulate appropriate priors
over models. In these ﬁelds, minimax formulation combined with stochastic programming [7] have
been extensively studied and successfully applied.
We make a contribution to minimax probability theory and apply the results to problems arising in
four different areas. Speciﬁcally, we generalize a classic result on the linear projection property of
distribution families: we show that any linear projection between distribution families with ﬁxed
mean and covariance, regardless of their dimensions, is surjective. That is, given any matrix X and
any random vector r with mean X T µ and covariance X T (cid:6)X , one can always ﬁnd another random
vector R with mean µ and covariance (cid:6) such that X T R = r (almost surely). Our proof imposes no
conditions on the deterministic matrix X , hence extends the classic projection result in [6], which
assumes X is a vector. We furthermore extend this surjective property to some restricted distribution

1

families, which allows additional prior information to be incorporated and hence less conservative
solutions to be obtained. In particular, we prove that surjectivity of linear projections remains to hold
for distribution families that are additionally symmetric, log-concave, or symmetric linear unimodal.
In each case, our proof strategy allows one to construct the worst-case distribution(s).
An immediate application of these results is to reduce the worst-case analysis of multivariate expec-
tations to the univariate (or reduced multivariate) ones, which have been long studied and produced
many fruitful results. In this direction, we conduct worst-case analyses of some common restricted
distribution families. We illustrate our results on problems that incorporate a classic worst case
value-at-risk constraint: minimax probability classiﬁcation [2]; chance constrained linear program-
ming (CCLP) [3]; portfolio selection [4]; and Markov decision processes (MDPs) with reward un-
certainty [8]. Although some of the results we obtain have been established in the respective ﬁelds
[2; 3; 4], we unify them through a much simpler proof strategy. Additionally, we provide extensions
to other constrained distribution families, which makes the minimax formulation less conservative
in each case. These results are then extended to the more recent conditional value-at-risk constraint,
and new bounds are proved, including a new bound on the survival function for symmetric unimodal
distributions.

2 A General Projection Property

First we establish a generalized linear projection property for distribution families. The key appli-
cation will be to reduce worst-case multivariate stochastic programming problems to lower dimen-
sional equivalents; see Corollary 1. Popescu [6] has proved the special case of reduction to one
dimension, however we provide a simpler proof that can be more easily extended to other distribu-
tion families1 .
Let (µ, (cid:6)) denote the family of distributions sharing common mean µ and covariance (cid:6), and let
µX = X T µ and (cid:6)X = X T (cid:6)X . Below we denote random variables by boldface letters, and use I
to denote the identity matrix. We use † to denote the pseudo-inverse.
Theorem 1 (General Projection Property (GPP)) For all µ, (cid:6) ≽ 0, and X ∈ Rm×d , the projec-
tion X T R = r from m-variate distributions R ∼ (µ, (cid:6)) to d-variate distributions r ∼ (µX , (cid:6)X ) is
surjective and many-to-one. That is, every r ∼ (µX , (cid:6)X ) can be obtained from some R ∼ (µ, (cid:6))
via X T R = r (almost surely).
Proof: The proof is constructive. Given a r ∼ (µX , (cid:6)X ), we can construct a pre-image R by letting
X r+ (Im −(cid:6)X (cid:6)
X X T )M, where M ∼ (µ, (cid:6)) is independent of r, for example, one can
†
†
R = (cid:6)X (cid:6)
choose M as a Gaussian random vector. It is easy to verify that R ∼ (µ, (cid:6)) and X T R = (cid:6)X (cid:6)
†
X r +
(Im − (cid:6)X (cid:6)
X )X T M = r. The last equality holds since (Im − (cid:6)X (cid:6)
X )r = (Im − (cid:6)X (cid:6)
†
†
†
X )X T M
(the two random vectors on both sides have the same mean and zero covariance). Note that since M
(cid:4)
can be chosen arbitrarily in (µ, (cid:6)), the projections are always many-to-one.
Although this establishes the general result, we extend it to distribution families under additional
constraints below. That is, one often has additional prior information about the underlying distri-
bution, such as symmetry, unimodality, and/or support. In such cases, if a general linear projection
property can still be shown to hold, the additional assumptions can be used to make the minimax
approach less conservative in a simple, direct manner. We thus consider a number of additionally
restricted distribution families.

Deﬁnition 1 A random vector X is called (centrally) symmetric about µ, if for all vectors x,
Pr(X ≥ µ + x) = Pr(X ≤ µ − x). A univariate random variable is called unimodal about
a if its cumulative distribution function (c.d.f.) is convex on (−∞, a] and concave on [a, ∞). A
random vector X is called log-concave if its c.d.f. is log-concave. A random m-vector X is called
linear unimodal about 0m if for all a ∈ Rm , aT X is (univariate) unimodal about 0.
Let (µ, (cid:6))S denote the family of distributions in (µ, (cid:6)) that are additionally symmetric about µ,
and similarly, let (µ, (cid:6))L denote the family of distributions that are additionally log-concave, and
1 In preparing the ﬁnal version of this paper, we noticed that a very recent work [9] proved the one dimen-
sional case by a similar technique as ours.

2

let (µ, (cid:6))SU denote the family of distributions that are additionally symmetric and linear unimodal
about µ. For each of these restricted families, we require the following properties to establish our
next main result.
Lemma 1 (a) If random vector X is symmetric about 0, then AX + µ is symmetric about µ. (b) If
X, Y are independent and both symmetric about 0, Z = X + Y is also symmetric about 0.

Although once misbelieved, it is now clear that the convolution of two (univariate) unimodal distri-
butions need not be unimodal. However, for symmetric, unimodal distributions we have
Lemma 2 ([10] Theorem 1.6) If two independent random variables x and y are both symmetric
and unimodal about 0, then z = x + y is also unimodal about 0.

There are several non-equivalent extensions of unimodality to multivariate random variables. We
consider two speciﬁc (multivariate) unimodalities in this paper: log-concave and linear unimodal.2
[
]
Lemma 3 ([10] Lemma 2.1, Theorem 2.4, Theorem 2.18)
1. Linearity: If random m-vector X is log-concave, aT X is also log-concave for all a ∈ Rm .
2. Cartesian Product: If X and Y are log-concave, then Z =
X
is also log-concave .
Y
3. Convolution: If X and Y are independent and log-concave, then Z = X +Y is also log-concave.

Given the above properties, we can now extend Theorem 1 to (µ, (cid:6))S , (µ, (cid:6))L and (µ, (cid:6))SU .
Theorem 2 (GPP for Symmetric, Log-concave, and Symmetric Linear Unimodal Distributions)
For all µ, (cid:6) ≽ 0 and X ∈ Rm×d , the projection X T R = r from m-variate R ∼ (µ, (cid:6))S to
d-variate r ∼ (µX , (cid:6)X )S is surjective and many-to-one. The same is true for (µ, (cid:6))L and
(µ, (cid:6))SU .3
Proof: The proofs follow the same basic outline as Theorem 1 except that in the ﬁrst step we
now choose N ∼ (0m , Im )S or (0m , Im )L or (0m , Im )SU . Then, respectively, symmetry of the
constructed R follows from Lemma 1; log-concavity of R follows from Lemma 3; and linear uni-
(cid:4)
modality of R follows from the deﬁnition and Lemma 2. The maps remain many-to-one.
An immediate application of the general projection property is to reduce worst-case analyses of
multivariate expectations to the univariate case. Note that in the following corollary, the optimal
distribution of R can be easily constructed from the optimal distribution of r.
Corollary 1 For any matrix X and any function g(·) (including in particular when X is a vector)
E[g(X T R)]
E[g(r)].
sup
=
sup
(1)
R∼(µ,(cid:6))
r∼(X T µ,X T (cid:6)X )
The equality continues to hold if we restrict (µ, (cid:6)) to (µ, (cid:6))S , (µ, (cid:6))L , or (µ, (cid:6))SU respectively.
Proof: It is obvious that the right hand side is an upper bound on the left hand side, since for every
R ∼ (µ, (cid:6)) there exists an r ∼ (X T µ, X T (cid:6)X ) given by r = X T R. Similarly for (µ, (cid:6))S ,
(µ, (cid:6))L , and (µ, (cid:6))SU . However, given Theorems 1 and 2, one can then establish the converse.4 (cid:4)

3 Application to Worst-case Value-at-risk

We now apply these projection properties to analyze the worst case value-at-risk (VaR) —a useful
risk criterion in many application areas. Consider the following constraint on a distribution R
Pr(−xT R ≤ α) ≥ 1 − ϵ,
(2)
2A sufﬁcient but not necessary condition for log-concavity is having log-concave densities. This can be used
to verify log-concavity of normal and uniform distributions. In the univariate case, log-concave distributions
are called strongly unimodal, which is only a proper subset of univariate unimodal distributions [10].
3 If X is a vector we can also extend this theorem to other multivariate unimodalities such as symmetric
star/block/convex unimodal.
4The closure of (µ, (cid:6)), (µ, (cid:6))S , (µ, (cid:6))L , and (µ, (cid:6))SU under linear projection is critical for Corollary 1
to hold. Corollary 1 fails for other kinds of multivariate unimodalities, such as symmetric star/block/convex
unimodal. It also fails for (µ, (cid:6))+ , a distribution family whose support is contained in the nonnegative orthant.
This is not surprising since determining whether the set (µ, (cid:6))+ is empty is already NP-hard [11].

3

(4)

(5)

(6)

(3)

for given x, α and ϵ ∈ (0, 1). In this case, the inﬁmum over α such that (2) is satisﬁed is referred
to as the ϵ-VaR of R. Within certain restricted distribution families, such as Q-radially symmetric
distributions, (2) can be (equivalently) transformed to a deterministic second order cone constraint
(depending on the range of ϵ) [3]. Unfortunately, determining whether (2) can be satisﬁed for given
x, α and ϵ ∈ (0, 1) is NP-hard in general [8]. Suppose however that one knew the distribution of
[
]
R belonged to a certain family, such as (µ, (cid:6)).5 Given such knowledge, it is natural to consider
whether (2) can be satisﬁed in a worst case sense. That is, consider
Pr(−xT R ≤ α)
≥ 1 − ϵ.

inf
R∼(µ,(cid:6))
Here the inﬁmum of α values satisfying (3) is referred to as the worst-case ϵ-VaR. If we have ad-
ditional information about the underlying distribution, such as symmetry or unimodality, the worst-
case ϵ-VaR can be reduced. Importantly, using the results of the previous section, we can easily
determine the worst-case ϵ-VaR for various distribution families. These can also be used to provide
a tractable bound on the ϵ-VaR even when the distribution is known.
√
Proposition 1 For alternative distribution families, the worst-case ϵ-VaR constraint (3) is given by:
{
√
1 − ϵ
then α ≥ −µx +
if R ∼ (µ, (cid:6))
ϵ
α ≥ −µx +
{
√
1
2ϵ σx ,
α ≥ −µx ,
α ≥ −µx + 2
if R ∼ (µ, (cid:6))SU
1
2ϵ σx ,
then
α ≥ −µx ,
3
if R ∼ N (µ, (cid:6))
then α ≥ −µx + (cid:8)−1 (1 − ϵ)σx ,
(7)
√
xT (cid:6)x and (cid:8)(·) is the c.d.f. of the standard normal distribution N (0, 1).
where µx = xT µ, σx =
It turns out some results of Proposition 1 are known. In fact, the ﬁrst bound (4) has been extensively
studied. However, given the results of the previous section, we can now provide a much simpler
proof.6 (This simplicity will also allow us to achieve some useful new bounds in Section 4 below.)
Proof: From Corollary 1 it follows that
Pr(−xT R ≤ α) =

if ϵ ∈ (0, 1
2 )
if ϵ ∈ [ 1
2 , 1)
if ϵ ∈ (0, 1
2 )
if ϵ ∈ [ 1
2 , 1)

Pr(r ≤ α) = 1 −

if R ∼ (µ, (cid:6))S

then

σx ,

(9)

(8)

Pr(r > α).

inf
R∼(µ,(cid:6))

inf
r∼(−µx ,σ2
x )

sup
r∼(−µx ,σ2
x )
Given that the problem is reduced to the univariate case, we simply exploit classical inequalities:
if x ∼ (µ, σ2 )
then Pr(x > t) ≤
σ2
σ2 + (µ − t)2 ,
then Pr(x > t) ≤ 1
if x ∼ (µ, σ2 )S
σ2
(µ − t)2 ),
min(1,
2
then Pr(x > t) ≤ 1
if x ∼ (µ, σ2 )SU
4
σ2
(µ − t)2 ),
min(1,
(11)
9
2
for t ≥ µ.7 Now to prove (4), simply plug (8) into (3) and notice that an application of (9) leads to
≥ 1 − ϵ.
1 −
α ≥ −µx
σ2
x + (−µx − α)2
x
and
σ2
(4) then follows by simple rearrangement. The same procedure can be used to prove (5), (6), (7). (cid:4)
5We will return to the question of when such moment information is also subject to uncertainty in Section 5.
6 [2] and [3] provide a proof of (4) based on the multivariate Chebyshev inequality in [12]; [4] proves (4)
from dual optimality; and the proof in [6] utilizes two point support property of the general constraint (3).
7 (9) is known as the (one-sided) Chebyshev inequality. Two-sided version of (11) is known as the Gauss
inequality. These classical bounds are tight. Proofs can be found in [13], for example.

(10)

4

Figure 1: Comparison of the coefﬁcients in front of σx for different distribution families in Proposi-
tion 1 (left) and Proposition 2 (right). Only the range ϵ ∈ (0, 1
2 ) is depicted.

ϵ s.t.

Proposition 1 clearly illustrates the beneﬁt of prior knowledge. Figure 1 compares the coefﬁcients
on σx among the different worst case VaR for different distribution families. The large gap between
coefﬁcients for general and symmetric (linear) unimodal distributions demonstrates how additional
constraints can generate much less conservative solutions while still ensuring robustness.
Beyond simplifying existing proofs, Proposition 1 can be used to extend some of the uses of the VaR
criterion in different application areas.
Minimax probability classiﬁcation [2]: Lanckriet et al. [2] ﬁrst studied the value-at-risk constraint
in binary classiﬁcation. In this scenario, one is given labeled data from two different sources and
[
[
]
]
seeks a robust separating hyperplane. From the data, the distribution families (µ1 , (cid:6)1 ) and (µ2 , (cid:6)2 )
can be estimated. Then a robust hyperplane can be recovered by minimizing the worst-case error
≥ 1 − ϵ, (12)
Pr(xT R2 ≥ α)
≥ 1 − ϵ and
Pr(xT R1 ≤ α)

inf
inf
min
x ̸=0,α,ϵ
R2∼(µ2 ,(cid:6)2 )
R1∼(µ1 ,(cid:6)1 )
where x is the normal vector of the hyperplane, α is the offset and ϵ controls the error probability.
Note that the results in [2] follow from using the bound (4). However, interesting additional facts
arise when considering alternative distribution families. For example, consider symmetric distri-
butions. In this case, suppose we knew in advance that the optimal ϵ lay in [ 1
2 , 1), meaning that
no hyperplane predicts better than random guessing. Then the constraints in (12) become linear,
covariance information becomes useless in determining the optimal hyperplane, and the optimiza-
tion concentrates solely on separating the means of two classes. Although such a result might seem
surprising, it is a direct consequence of symmetry: the worst-case distributions are forced to put
probability mass arbitrarily far away on both sides of the mean, thereby eliminating any information
brought by covariance. When the optimal ϵ lies in (0, 1
2 ), however, covariance information becomes
meaningful, since the worst-case distributions can no longer put probability mass arbitrarily far
away on both sides of the mean (owing to the existence of a hyperplane that predicts labels better
than random guessing). In this case, the optimization problems involving (µ, (cid:6))S and (µ, (cid:6))SU are
equivalent to that for (µ, (cid:6)) except that the maximum error probability ϵ becomes smaller, which
is to be expected since more information about the marginal distributions should make one more
conﬁdent to predict the labels of future data.
[3]:
Chance Constrained Linear Programming (CCLP)
Consider a linear program
minx aT x s.t. rT x ≥ 0. If the coefﬁcient r is uncertain, it is clear that solving the linear program
merely using the expected value of r could result in a solution x that was sub-optimal or even in-
[
]
feasible. Calaﬁore and El Ghaoui studied this problem in [3], and imposed the inequality constraint
with high probability, leading to the the so-called chance constrained linear program (CCLP):
Pr(−xT R ≤ 0)
≥ 1 − ϵ.

(13)

min
x

aT x s.t.

inf
R∼(µ,(cid:6))

5

(14)

In this case, α is simply 0 and ϵ is given by the user. Depending on the value of ϵ, the chance
constraint can be equivalently transformed into a second order cone constraint or a linear constraint.
The work in [3] concentrates on the general and symmetric distribution families. In the latter case,
[3] uses the ﬁrst part of inequality (5) as a sufﬁcient condition for guaranteeing robust solutions.
Note however that from Corollary 1 and Proposition 1 one can now see that (5) is also a necessary
condition. Although the symmetric linear unimodal case is not discussed in [3], from Proposition 1
again one can see that incorporating bound (6) in (13) yields a looser constraint than does (5),
hence the feasible region will be enlarged and the optimum value of the CCLP potentially reduced,
corresponding to the intuition that increased prior knowledge leads to more optimized results.
Portfolio Selection [4]: In portfolio selection, let R represent the (uncertain) returns of a suite of
ﬁnancial assets, and x the weighting one would like to put on the various assets. Here α > −xT R
[
]
represents an upper bound on the loss one might suffer with weighting x. The goal is to minimize
an upper bound on the loss that holds with high probability,8 say 1 − ϵ, speciﬁed by the user
≥ 1 − ϵ.
Pr(−xT R ≤ α)
inf
min
α s.t.
R∼(µ,(cid:6))
x,α
This criterion has been studied by El Ghaoui et al. [4] in the worst case setting. Previous work has not
addressed the case when additional symmetry or linear unimodal information is available. However,
comparing the minimal value of α in Proposition 1, we see that such additional information, such
as symmetry or unimodality, indeed decreases our potential loss, as shown clearly in Figure 1. This
makes sense, since the more one knows about uncertain returns the less risk one should have to bear.
Note also that when incorporating additional information, the optimal portfolio, represented by x, is
changed as well but remains mean-variance efﬁcient when ϵ ∈ (0, 1
2 ).
Uncertain MDPs with reward uncertainty: The standard planning problem in Markov decision
processes (MDPs) is to ﬁnd a policy such that maximizes the expected total discounted return. This
nonlinear optimization problem can be efﬁciently solved by dynamic programming, provided that
the model parameters (transition kernel and reward function) are exactly known. Unfortunately, this
is rarely the case in practice. Delage and Mannor [8] extend this problem to the uncertain case by
employing the value-at-risk type constraint (2) and assuming the unknown reward model and transi-
tion kernel are drawn from a known distribution (Gaussian and Dirichlet respectively).Unfortunately,
[8] also proves that the constraint (2) is generally NP-hard to satisfy unless one assumes some very
[
]
restricted form of distribution, such as Gaussian. Alternatively, note that one can use the worst case
value-at-risk formulation (3) to obtain a tractable approximation to (2)
Pr(−xT R ≤ α)
≥ 1 − ϵ,

inf
min
R∼(µ,(cid:6))
x,α
where R is the reward function (unknown but assumed to belong to (µ, (cid:6))) and x represents a
discounted-stationary state-action visitation distribution (which can be used to recover an optimal
behavior policy). Although this worst case formulation (15) might appear to be conservative com-
pared to working with a known distribution on R and using (2), when additional information about
the distribution is available, such as symmetry or unimodality, (15) can be brought very close to us-
ing a Gaussian distribution, as shown in Figure 1. Thus, given reasonable constraints, the minimax
approach does not have to be overly conservative, while providing robustness and tractability.

α s.t.

(15)

4 Application to Worst-case Conditional Value-at-risk
Finally, we investigate the more reﬁned conditional value-at-risk (CVaR) criterion that bounds the
conditional expectation of losses beyond the value-at-risk (VaR). This criterion has been of growing
(cid:12)(cid:12)(cid:12) Pr(−xT R ≤ α
[
]
prominence in many areas recently. Consider the following quantity deﬁned as the mean of a tail
distribution:
α s.t. Pr(−xT R ≤ α) ≥ 1−ϵ.
∗ ) ≥ 1−ϵ
−xT R
∗ = arg min
^f = E
α
(16)
∗ is the value-at-risk and ^f is the conditional value-at-risk of R. It is well-known that the
Here, α
∗ . Although it might appear that dealing with
CVaR, ^f , is always an upper bound on the VaR, α
8Note that seeking to minimize the loss surely leads to a meaningless outcome. For example, if ϵ = 0, the
optimization problem trivially says that the loss of any portfolio will be no larger than 1.

where α

6

α +

(17)

α +

α +

(18)

if R ∼ (µ, (cid:6))S then

]
[
the CVaR criterion entails greater complexity than the VaR, since VaR is directly involved in the
deﬁnition of CVaR, it turns out that CVaR can be more directly expressed as
(−xT R − α)+
1
E
^f = min
,
ϵ
α
where (x)+ = max(0, x) [14]. Unlike the VaR constraint (2), (17) is always (jointly) convex in x
and α. Thus if R were discrete, ^f could be easily computed by a linear program [14; 5]. However,
the expectation in (17) involves a high dimensional integral in general, whose analytical solution
is not always available, thus ^f is still hard to compute in practice. Although one potential remedy
might be to use Monte Carlo techniques to approximate the expectation, we instead take a robust
]
[
]
[
approach: As before, suppose one knew the distribution of R belonged to a certain family, such as
(µ, (cid:6)). Given such knowledge, it is natural to consider the worst-case CVaR
(−xT R − α)+
(−xT R − α)+
1
1
E
E
sup
f = sup
min
= min
,
R∼(µ,(cid:6))
R∼(µ,(cid:6))
ϵ
ϵ
α
α
where the interchangeability of the min and sup operators follows from the classic minimax theorem
[15]. Importantly, as in the previous section, we can determine the worst-case CVaR for various
distribution families. If one has additional information about the underlying distribution, such as
symmetry or unimodality, the worst-case CVaR can be reduced. These can be used to provide a
tractable bound on the CVaR even when the distribution is known.
√
√
Proposition 2 For alternative distribution families, the worst-case CVaR is given by:
(2ϵ − 1)
1 − ϵ
{
if R ∼ (µ, (cid:6)) then α = −µx +
f = −µx +
ϵ(1 − ϵ)
σx ,
σx ,
2
ϵ
if ϵ ∈ (0, 1
α = −µx + 1√
σx , f = −µx + 1√
2 ]
 α = −µx + 1
σx
√
1√
if ϵ ∈ [ 1
α = −µx −
σx , f = −µx +
8ϵ
2ϵ
1−ϵ√
2 , 1)
8(1−ϵ)
2ϵ
if ϵ ∈ (0, 1
ϵ σx , f = −µx + 2
√
√
√
√
3 ]
ϵ σx
3(1 − ϵ)σx if ϵ ∈ [ 1
3(1 − 2ϵ)σx , f = −µx +
α = −µx +
3
3
3 ]
3 , 2
(21)
√
α = −µx − 1
if ϵ ∈ [ 2
σx , f = −µx + 2
1−ϵ
√
3 , 1)
3ϵ σx ,
1−ϵ
3
− ((cid:8)(cid:0)1 (1(cid:0)ϵ))2
2√
if R ∼ N (µ, (cid:6)) then f = −µx + e
(22)
σx ,
√
2πϵ
xT (cid:6)x and (cid:8)(·) is the c.d.f. of a standard normal distribution N (0, 1).
The results of Proposition 2 are a novel contribution of this paper, with the exception of (22), which
[
[
]
]
is a standard result in stochastic programming [7].
Proof: We know from Corollary 1 that
(r − α)+
(−xT R − α)+
E
E
sup
sup
,
r∼(−µx ,σ2
R∼(µ,(cid:6))
x )
which reduces the problem to the univariate case. To proceed, we will need to make use of the
]
[
√
univariate results given in Proposition 3 below. Assuming Proposition 3 for now, we show how to
prove (19): In this case, substitute (23) into (18) and apply (24) from Proposition 3 below to obtain
x + (−µx − α)2
(−µx − α) +
1
√
f = min
σ2
.
2ϵ
α
This is a convex univariate optimization problem in α. Taking the derivative with respect to α and
√
setting to zero gives α = −µx + (2ϵ−1)
σx . Substituting back we obtain f = −µx +
1−ϵ
ϵ σx .
ϵ(1−ϵ)
2
(cid:4)
A similar strategy can be used to prove (20), (21), and (22).
As with Proposition 1, Proposition 2 illustrates the beneﬁt of prior knowledge. Figure 1 (right)
compares the coefﬁcients on σx among different worst-case CVaR quantities for different families.
Comparing VaR and CVaR in Figure 1 shows that unimodality has less impact on improving CVaR.
A key component of Proposition 2 is its reliance on the following important univariate results. The
following proposition gives tight bounds of the expected survival function for the various families.

if R ∼ (µ, (cid:6))SU then

where µx = xT µ, σx =

(19)

(20)

=

(23)

σx

α +

7

(24)

(25)

(26)

=

,

,

,

E

E

=

=

sup
x∼(µ,σ2 )

sup
x∼(µ,σ2 )S

sup
x∼(µ,σ2 )SU

]
[
√
[
]
Proposition 3 For alternative distribution families, the expected univariate survival functions are:
 σ−t+µ
σ2 + (µ − t)2
(x − t)+
(µ − t) +
1
E
,
]
[
2
≤ t ≤ µ + σ
if µ − σ
,
2
2
2
(x − t)+
σ2
 (
if t > µ + σ
8(t−µ) ,
2
if t < µ − σ
− σ2+8(t−µ)2
]
[
8(t−µ)
√
2
if µ − σ√
≤ t ≤ µ + σ√
3σ−t+µ)2
√
(x − t)+
3
3
3σ
4
if t > µ + σ√
σ2
9(t−µ) ,
3
if t < µ − σ√
− σ2+9(t−µ)2
9(t−µ)
3
Here (26) is a further novel contribution of this paper. Proofs of (24) and (25) can be found in [1].
Interestingly, to the best of our knowledge, the worst-case CVaR criterion has not yet been applied
to any of the four problems mentioned in the previous section9 . Given the space constraints, we can
only discuss the direct application of worst-case CVaR to the portfolio selection problem. We note
that CVaR has been recently applied to ν -SVM learning in [16].
Implications for Portfolio Selection: By comparing Propositions 1 and 2, the ﬁrst interesting con-
clusion one can reach about portfolio selection is that, without considering any additional informa-
tion, the worst-case CVaR criterion yields the same optimal portfolio weighting x as the worst-case
VaR criterion (recall that VaR minimizes α in Proposition 1 by adjusting x while CVaR minimizes
f by adjusting x in Proposition 2). However, the worst-case distributions for the two approaches are
not the same, which can be seen from the relation (16) between VaR and CVaR and observing that α
in (4) is not the same as in (19). Next, when additional symmetry information is taken into account
and ϵ ∈ (0, 1
2 ), CVaR and VaR again select the same portfolio but under different worst-case distri-
butions. When unimodality is added, the CVaR criterion ﬁnally begins to select different portfolios
than VaR.

5 Concluding Remarks

We have provided a simpler yet broader proof of the general linear projection property for distribu-
tion families with given mean and covariance. The proof strategy can be easily extended to more
restricted distribution families. A direct implication of our results is that worst-case analyses of mul-
tivariate expectations can often be reduced to those of univariate ones. By combining this trick with
classic univariate inequalities, we were able to provide worst-case analyses of two widely adopted
constraints (based on value-at-risk criteria). Our analysis recovers some existing results in a simpler
way while also provides new insights on incorporating additional information.
Above, we assumed the ﬁrst and second moments of the underlying distribution were precisely
known, which of course is questionable in practice. Fortunately, there are standard techniques for
handling such additional uncertainty. One strategy, proposed in [2], is to construct a (bounded and
convex) uncertainty set U over (µ, (cid:6)), and then applying a similar minimax formulation but with
respect to (µ, (cid:6)) ∈ U . As shown in [2], appropriately chosen uncertainty sets amount to adding
straightforward regularizations to the original problem. A second approach is simply to lower one’s
conﬁdence of the constraints and rely on the fact that the moment estimates are close to their true
values within some additional conﬁdence bound [17]. That is, instead of enforcing the constraint
(3) or (18) surely, one can instead plug-in the estimated moments and argue that constraints will be
satisﬁed within some diminished probability. For an application of this strategy in CCLP, see [3].

Acknowledgement

We gratefully acknowledge support from the Alberta Ingenuity Centre for Machine Learning, the
Alberta Ingenuity Fund, iCORE and NSERC. Csaba Szepesv `ari is on leave from MTA SZTAKI, Bp.
Hungary.
9Except the very recent work of [9] on portfolio selection.

8

References
[1] R. Jagannathan. “Minimax procedure for a class of linear programs under uncertainty”. Oper-
ations Research, vol. 25(1):pp. 173–177, 1977.
[2] Gert R.G. Lanckriet, Laurent El Ghaoui, Chiranjib Bhattacharyya and Michael I. Jordan. “A
robust minimax approach to classiﬁcation”. Journal of Machine Learning Research, vol. 03:pp.
555–582, 2002.
[3] G.C.Calaﬁore and Laurent El Ghaoui. “On distributionally robust chance-constrained linear
programs”. Journal of Optimization Theory and Applications, vol. 130(1):pp. 1–22, 2006.
[4] Laurent El Ghaoui, Maksim Oks and Francois Oustry. “Worst-case value-at-risk and robust
portfolio optimization: a conic programming approach”. Operations Research, vol. 51(4):pp.
542–556, 2003.
[5] Shu-Shang Zhu and Masao Fukushima. “Worst-case conditional value-at-risk with application
to robust portfolio management”. Operations Research, vol. 57(5):pp. 1155–1168, 2009.
[6] Ioana Popescu. “Robust mean-covariance solutions for stochastic optimization”. Operations
Research, vol. 55(1):pp. 98–112, 2007.
[7] Andr ´as Pr ´ekopa. Stochastic Programming. Springer, 1995.
[8] Erick Delage and Shie Mannor. “Percentile optimization for Markov decision processes with
parameter uncertainty”. Operations Research, to appear 2009.
[9] Li Chen, Simai He and Shuzhong Zhang. “Tight Bounds for Some Risk Measures, with Ap-
plications to Robust Portfolio Selection”. Tech. rep., Department of Systerms Engineering and
Engineering Management, The Chinese University of Hongkong, 2009.
[10] Sudhakar Dharmadhikari and Kumar Joag-Dev. Unimodality, Convexity, and Applications.
Academic Press, 1988.
[11] Dimitris Bertsimas and Ioana Popescu. “Optimal inequalities in probability theory a convex
optimization approach”. SIAM Journal on Optimization, vol. 15(3):pp. 780–804, 2005.
[12] Albert W. Marshall and Ingram Olkin. “Multivariate Chebyshev inequalities”. Annals of Math-
ematical Statistics, vol. 31(4):pp. 1001–1014, 1960.
[13] Ioana Popescu. “A semideﬁnite programming approach to optimal moment bounds for convex
classes of distributions”. Mathematics of Operations Research, vol. 30(3):pp. 632–657, 2005.
[14] R. Tyrrell Rockafellar and Stanislav Uryasev. “Optimization of conditional value-at-risk”.
Journal of Risk, vol. 2(3):pp. 493–517, 2000.
Proceedings of
[15] Ky Fan.
“Minimax Theorems”.
vol. 39(1):pp. 42–47, 1953.
[16] Akiko Takeda and Masashi Sugiyama. “ν -support vector machine as conditional value-at-risk
minimization”. In Proceedings of the 25th International Conference on Machine Learning, pp.
1056–1063. 2008.
[17] John Shawe-Taylor and Nello Cristianini. “Estimating the moments of a random vector with
applications”. In Proceedings of GRETSI 2003 Conference, pp. 47–52. 2003.

the National Academy of Sciences,

9


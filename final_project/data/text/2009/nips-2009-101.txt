Regularized Distance Metric Learning:
Theory and Algorithm

Yang Zhou1
Shijun Wang2
Rong Jin1
1Dept. of Computer Science & Engineering, Michigan State University, East Lansing, MI 48824
2Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892
rongjin@cse.msu.edu wangshi@cc.nih.gov zhouyang@msu.edu

Abstract

In this paper, we examine the generalization error of regularized distance metric
learning. We show that with appropriate constraints, the generalization error of
regularized distance metric learning could be independent from the dimensional-
ity, making it suitable for handling high dimensional data. In addition, we present
an efﬁcient online learning algorithm for regularized dist ance metric learning. Our
empirical studies with data classiﬁcation and face recogni tion show that the pro-
posed algorithm is (i) effective for distance metric learning when compared to the
state-of-the-art methods, and (ii) efﬁcient and robust for high dimensional data.

1 Introduction

Distance metric learning is a fundamental problem in machine learning and pattern recognition. It is
critical to many real-world applications, such as information retrieval, classiﬁcation, and clustering
[6, 7]. Numerous algorithms have been proposed and examined for distance metric learning. They
are usually classiﬁed into two categories: unsupervised me tric learning and supervised metric learn-
ing. Unsupervised distance metric learning, or sometimes referred to as manifold learning, aims to
learn a underlying low-dimensional manifold where the distance between most pairs of data points
are preserved. Example algorithms in this category include ISOMAP [13] and Local Linear Embed-
ding (LLE) [8]. Supervised metric learning attempts to learn distance metrics from side information
such as labeled instances and pairwise constraints. It searches for the optimal distance metric that
(a) keeps data points of the same classes close, and (b) keeps data points from different classes far
apart. Example algorithms in this category include [17, 10, 15, 5, 14, 19, 4, 12, 16]. In this work,
we focus on supervised distance metric learning.

Although a large number of studies were devoted to supervised distance metric learning (see the sur-
vey in [18] and references therein), few studies address the generalization error of distance metric
learning. In this paper, we examine the generalization error for regularized distance metric learning.
Following the idea of stability analysis [1], we show that with appropriate constraints, the general-
ization error of regularized distance metric learning is independent from the dimensionality of data,
making it suitable for handling high dimensional data. In addition, we present an online learning
algorithm for regularized distance metric learning, and show its regret bound. Note that although
online metric learning was studied in [9], our approach is advantageous in that (a) it is computation-
ally more efﬁcient in handling the constraint of SDP cone, an d (b) it has a proved regret bound while
[9] only shows a mistake bound for the datasets that can be separated by a Mahalanobis distance. To
verify the efﬁcacy and efﬁciency of the proposed algorithm f
or regularized distance metric learning,
we conduct experiments with data classiﬁcation and face rec ognition. Our empirical results show
that the proposed online algorithm is (1) effective for metric learning compared to the state-of-the-art
methods, and (2) robust and efﬁcient for high dimensional da ta.

1

2 Regularized Distance Metric Learning

min
A

where

1
2 |A|2
F +

Let D = {zi = (xi , yi ), i = 1, . . . , n} denote the labeled examples, where xk = (x1
k , . . . , xd
k ) ∈ Rd
is a vector of d dimension and yi ∈ {1, 2, . . . , m} is class label. In our study, we assume that
the norm of any example is upper bounded by R, i.e., supx |x|2 ≤ R. Let A ∈ Sd×d
be the
+
distance metric to be learned, where the distance between two data points x and x′ is calculated as
A = (x − x′ )⊤A(x − x′ ).
|x − x′ |2
Following the idea of maximum margin classiﬁers, we have the following framework for regularized
distance metric learning:
A (cid:3)(cid:1) : A (cid:23) 0, tr(A) ≤ η(d)

n(n − 1) Xi<j
g (cid:0)yi,j (cid:2)1 − |xi − xj |2
2C


• yi,j is derived from class labels yi and yj , i.e., yi,j = 1 if yi = yj and −1 otherwise.
• g (z ) is the loss function. It outputs a small value when z is a large positive value, and a large
value when z is large negative. We assume g (z ) to be convex and Lipschitz continuous with
Lipschitz constant L.
F is the regularizer that measures the complexity of the distance metric A.
• |A|2
• tr(A) ≤ η(d) is introduced to ensure a bounded domain for A. As will be revealed later,
this constraint will become active only when the constraint constant η(d) is sublinear in
d, i.e., η ∼ O(dp ) with p < 1. We will also show how this constraint could affect the
generalization error of distance metric learning.

(1)

3 Generalization Error

Let AD be the distance metric learned by the algorithm in (1) from the training examples D. Let
ID (A) denote the empirical loss , i.e.,
n(n − 1) Xi<j
g (cid:0)yi,j (cid:2)1 − |xi − xj |2
A (cid:3)(cid:1)
2
(2)
ID (A) =
For the convenience of presentation, we also write g (cid:0)yi,j (1 − |xi − xj |2
A )(cid:1) = V (A, zi , zj ) to high-
light its dependence on A and two examples zi and zj . We denote by I (A) the loss of A over the
true distribution, i.e.,
(3)
I (A) = E(zi ,zj ) [V (A, zi , zj )]
Given the empirical loss ID (A) and the loss over the true distribution I (A), we de ﬁne the estimation
error as

(4)
DD = I (AD ) − ID (AD )
In order to show the behavior of estimation error, we follow the analysis based on the stability of
the algorithm [1]. The uniform stability of an algorithm determines the stability of the algorithm
when one of the training examples is replaced with another. More speciﬁcally, an algorithm A has
uniform stability β if
(5)
∀(D, z ), ∀i, sup
u,v |V (AD , u, v) − V (ADz ,i , u, v)| ≤ β
where Dz ,i stands for the new training set that is obtained by replacing zj ∈ D with a new example
z . We further de ﬁne β = κ/n as the uniform stability β behaves like O(1/n).
The advantage of using stability analysis for the generalization error of regularized distance metric
learning. This is because the example pair (zi , zj ) used for training distance metrics are not I.I.D.
although zi is, making it difﬁcult to directly utilize the results from s tatistical learning theory.
In the analysis below, we ﬁrst show how to derive the generali zation error bound for regularized
distance metric learning given the uniform stability β (or κ). We then derive the uniform stability
constant for the regularized distance metric learning framework in (1).

2

3.1 Generalization Error Bound for Given Uniform Stability

Analysis in this section follows closely [1], and we therefore omit the detailed proofs.

Our analysis utilizes the McDiarmid inequality that is stated as follows.
Theorem 1. (McDiarmid Inequality) Given random variables {vi}l
i , and a function F : v l →
i=1 , v ′
R satisfying
i ˛˛˛
i , vi+1 , . . . , vl )˛˛˛
′
sup
F (v1 , . . . , vl ) − F (v1 , . . . , vi−1 , v
≤ ci ,
′
v1 ,...,vl ,v
the following statement holds
i !
Pr (|F (v1 , . . . , vl ) − E(F (v1 , . . . , vl ))| > ǫ) ≤ 2 exp  −
2ǫ
Pl
i=1 c2
To use the McDiarmid inequality, we ﬁrst compute E(DD ).
Lemma 1. Given a distance metric learning algorithm A has uniform stability κ/n, we have the
following inequality for E(DD )
κ
n

E(DD ) ≤ 2
where n is the number of training examples in D.
The result in the following lemma shows that the condition in McDiarmid inequality holds.
Lemma 2. Let D be a collection of n randomly selected training examples, and Di,z be the collec-
tion of examples that replaces zi in D with example z . We have |DD − DD i,z | bounded as follows
2κ + 8Lη(d) + 2g0
(7)
|DD − DD i,z | ≤
n
where g0 = supz ,z ′ |V (0, z , z ′)| measures the largest loss when distance metric A is 0.
Combining the results in Lemma 1 and 2, we can now derive the the bound for the generalization
error by using the McDiarmid inequality.
Theorem 2. Let D denote a collection of n randomly selected training examples, and AD be the
distance metric learned by the algorithm in (1) whose uniform stability is κ/n. With probability
1 − δ , we have the following bound for I (AD )
+ (2κ + 4Lη(d) + 2g0)r ln(2/δ)
2κ
n
2n
3.2 Generalization Error for Regularized Distance Metric Learning

I (AD ) − ID (AD ) ≤

(6)

(8)

First, we show that the superium of tr(AD ) is O(d1/2 ), which veriﬁes that η(d) should behave
sublinear in d. This is summarized by the following proposition.
Proposition 1. The trace constraint in (1) will be activated only when
η(d) ≤ p2dg0C

(9)

where g0 = supz ,z ′ |V (0, z , z ′)|.
Proof. It follows directly from [tr(AD )/d]2 ≤ |AD |2
z ,z ′ |V (0, z , z ′)| ≤ C g0 .
F ≤ 2C sup

To bound the uniform stability, we need the following proposition
Proposition 2. For any two distance metrics A and A′ , we have the following inequality hold for
any examples zu and zv
|V (A, zu , zv ) − V (A′ , zu , zv )| ≤ 4LR2 |A − A′ |F

(10)

3

The above proposition follows directly from the fact that (a) V (A, z , z ′) is Lipschitz continuous and
(b) |x|2 ≤ R for any example x. The following lemma bounds |AD − AD ′ |F .
Lemma 3. Let D denote a collection of n randomly selected training examples, and by z = (x, y ) a
randomly selected example. Let AD be the distance metric learned by the algorithm in (1). We have

|AD − AD i,z |F ≤

8C LR2
n

(11)

The proof of the above lemma can be found in Appendix A.

By putting the results in Lemma 3 and Proposition 2, we have the following theorem for the stability
of the Frobenius norm based regularizer.
Theorem 3. The uniform stability for the algorithm in (1) using the Frobenius norm regularizer,
denoted by β , is bounded as follows

β =

κ
n ≤

32C L2R4
n

(12)

where κ = 32C L2R4

Combing Theorem 3 and 2, we have the following theorem for the generalization error of distance
metric learning algorithm in (1) using the Frobenius norm regularizer
Theorem 4. Let D be a collection of n randomly selected examples, and AD be the distance metric
learned by the algorithm in (1) with h(A) = |A|2
F . With probability 1 − δ , we have the following
bound for the true loss function I (AD ) where AD is learned from (1) using the Frobenius norm
regularizer
+ (cid:0)32C L2R4 + 4Ls(d) + 2g0(cid:1) r ln(2/δ)
2n

32C L2R4
I (AD ) − ID (AD ) ≤
n
where s(d) = min (cid:0)√2dg0C , η(d)(cid:1).
Remark The most important feature in the estimation error is that it converges in the order of
O(s(d)/√n). By choosing η(d) to have a low dependence of d (i.e., η(d) ∼ dp with p ≪ 1), the
proposed framework for regularized distance metric learning will be robust to the high dimensional
data. In the extreme case, by setting η(d) to be a constant, the estimation error will be independent
from the dimensionality of data.

(13)

4 Algorithm

In this section, we discuss an efﬁcient algorithm for solvin g (1). We assume a hinge loss for g (z ),
i.e., g (z ) = max(0, b − z ), where b is the classiﬁcation margin. To design an online learning
algorithm for regularized distance metric learning, we follow the theory of gradient based online
learning [2] by de ﬁning potential function Φ(A) = |A|2
F /2. Algorithm 1 shows the online learning
algorithm.

The theorem below shows the regret bound for the online learning algorithm in Figure 1.
Theorem 5. Let the online learning algorithm 1 run with learning rate λ > 0 on a sequence
t ), yt , t = 1, . . . , n. Assume |x|2 ≤ R for all the training examples. Then, for all distance
(xt , x′
metric M ∈ S d×d
+ , we have
F (cid:19)
1 − 8R4λ/b (cid:18)Ln(M ) +
1
1
2λ |M |2
nXt=1

bLn ≤
max (cid:0)0, b − yt (1 − |xt − x′
M )(cid:1) , bLn =
t |2
4

At−1 )(cid:17)
max (cid:16)0, b − yt (1 − |xt − x′
t |2

where

Ł n (M ) =

nXt=1

Algorithm 1 Online Learning Algorithm for Regularized Distance Metric Learning
1: INPUT: prede ﬁned learning rate λ
2: Initialize A0 = 0
3: for t = 1, . . . , T do
Receive a pair of training examples {(x1
t ), (x2
t , y1
t , y2
4:
t )}
t , and yt = −1 otherwise.
Compute the class label yt : yt = +1 if y 1
t = y 2
5:
At−1 (cid:17) > 0 then
t ), yt is classiﬁed correctly, i.e., yt (cid:16)1 − |x1
if the training pair (x1
t |2
t − x2
t , x2
6:
At = At−1 .
7:
else
8:
t )⊤ ), where πS+ (M ) projects matrix M into the
At = πS+ (At−1 − λyt (xt − x′
t )(xt − x′
9:
SDP cone.
end if
10:
11: end for

λt = arg min
λt

The proof of this theorem can be found in Appendix B. Note that the above online learning algorithm
require computing πS+ (M ), i.e., projecting matrix M onto the SDP cone, which is expensive for
high dimensional data. To address this challenge, ﬁrst noti ce that M ′ = πS+ (M ) is equivalent to the
optimization problem M ′ = arg minM ′(cid:23)0 |M ′ − M |F . We thus approximate At = πS+ (At−1 −
t )⊤ ) with At = At−1 − λt yt (xt − x′
t )⊤ where λt is computed as
λyt (xt − x′
t )(xt − x′
t )(xt − x′
follows
(cid:8)|λt − λ| : λt ∈ [0, λ], At−1 − λt yt (xt − x′
t )⊤ (cid:23) 0(cid:9)
t )(xt − x′
The following theorem shows the solution to the above optimization problem.
Theorem 6. The optimal solution λt to the problem in (14) is expressed as
λt = (cid:26) λ
yt = −1
min (cid:0)λ, [(xt − x′
t )]−1 (cid:1)
t )⊤A−1
t−1 (xt − x′
yt = +1
Proof of this theorem can be found in the supplementary materials. Finally, the quantity (xt −
t )A−1
t ) can be computed by solving the following optimization problem
t−1 (xt − x′
x′
2u⊤(xt − x′
t ) − u⊤Au
max
u
whose optimal value can be computed efﬁciently using the con jugate gradient method [11].

(14)

Note that compared to the online metric learning algorithm in [9], the proposed online learning
algorithm for metric learning is advantageous in that (i) it is computationally more efﬁcient by
avoiding projecting a matrix into a SDP cone, and (ii) it has a provable regret bound while [9] only
presents the mistake bound for the separable datasets.

5 Experiments

We conducted an extensive study to verify both the efﬁciency and the efﬁcacy of the proposed
algorithms for metric learning. For the convenience of discussion, we refer to the propoesd online
distance metric learning algorithm as online-reg. To examine the efﬁcacy of the learned distance
metric, we employed the k Nearest Neighbor (k-NN) classiﬁer. Our hypothesis is that the better the
distance metric is, the higher the classiﬁcation accuracy o f k-NN will be. We set k = 3 for k-NN
for all the experiments according to our experience.

We compare our algorithm to the following six state-of-the-art algorithms for distance metric learn-
ing as baselines: (1) Euclidean distance metric; (2) Mahalanobis distance metric, which is com-
puted as the inverse of covariance matrix of training samples, i.e., (Pn
i=1 xixi )−1 ; (3) Xing’s algo-
rithm proposed in [17]; (4) LMNN, a distance metric learning algorithm based on the large margin
ITML, an Information-theoretic metric learning based on [4];
nearest neighbor classiﬁer [15]; (5)
and (6) Relevance Component Analysis (RCA) [10]. We set the maximum number of iterations for
Xing’s method to be 10, 000. The number of target neighbors in LMNN and parameter γ in ITML

5

Table 1: Classiﬁcation error (%) of a k-NN (k = 3) classiﬁer on the ten UCI data sets using seven
different metrics. Standard deviation is included.

Dataset
1
2
3
4
5
6
7
8
9

Eclidean
19.5 ± 2.2
39.9 ± 2.3
36.0 ± 2.0
4.0 ± 1.7
30.6 ± 1.9
25.4 ± 4.2
31.9 ± 2.8
18.9 ± 0.5
2.0 ± 0.4

Mahala
18.8 ± 2.5
6.7 ± 0.6
42.1 ± 4.0
10.4 ± 2.7
29.1 ± 2.1
18.4 ± 3.4
10.0 ± 2.8
37.3 ± 0.5
6.1 ± 0.5

Xing
29.3 ± 17.2
40.1 ± 2.6
43.5 ± 12.5
3.1 ± 2.0
30.6 ± 1.9
23.3 ± 3.4
24.6 ± 7.5
16.1 ± 0.6
12.4 ± 0.8

LMNN
13.8 ± 2.5
3.6 ± 1.1
33.1 ± 0.6
3.9 ± 1.6
29.6 ± 1.8
15.2 ± 3.1
4.5 ± 2.4
18.4 ± 0.4
1.6 ± 0.3

ITML
8.6 ± 1.7
40.0 ± 2.3
39.8 ± 3.3
3.2 ± 1.6
28.8 ± 2.1
17.1 ± 4.1
28.7 ± 3.7
23.3 ± 1.3
2.5 ± 0.4

RCA
17.4 ± 1.5
3.8 ± 0.4
41.6 ± 0.7
2.9 ± 1.5
28.6 ± 2.3
13.9 ± 2.2
1.8 ± 1.5
30.6 ± 0.7
2.8 ± 0.4

Online-reg
13.2 ± 2.2
3.7 ± 1.2
37.3 ± 4.1
3.2 ± 1.3
27.7 ± 1.3
12.9 ± 2.2
1.8 ± 1.1
19.8 ± 0.6
2.9 ± 0.4

Table 2: p-values of the Wilcoxon signed-rank test of the 7 methods on the 9 datasets.
Methods
Eclidean Mahala Xing
LMNN ITML RCA Online-reg
Euclidean
0.129
0.301
0.496
0.004
0.641
0.734
1.000
Mahala
0.004
0.004
0.570
0.008
0.301
1.000
0.734
Xing
0.027
0.074
0.359
0.027
1.000
0.301
0.641
LMNN
0.734
0.496
0.129
1.000
0.027
0.008
0.004
ITML
0.496
0.570
0.359
0.129
1.000
0.820
0.164
RCA
0.074
1.000
0.820
0.496
0.074
0.004
0.301
Online-reg
0.129
0.004
0.027
0.734
0.164
0.074
1.000

were tuned by cross validation over the range from 10−4 to 104 . All the algorithms are implemented
and run using Matlab. All the experiment are run on a AMD Processor 2.8G machine, with 8GMB
RAM and Linux operation system.

5.1 Experiment (I): Comparison to State-of-the-art Algorithms

We conducted experiments of data classiﬁcation over the fol lowing nine datasets from UCI repos-
itory: (1) balance-scale, with 3 classes, 4 features, and 625 instances; (2) breast-cancer, with 2
classes, 10 features, and 683 instance; (3) glass, with 6 classes, 9 features, and 214 instances; (4)
iris, with 3 classes, 4 features, and 150 instances; (5) pima, with 2 classes, 8 features, and 768 in-
stances; (6) segmentation, with 7 classes, 19 features, and 210 instances; (7)wine, with 3 classes,
13 features, and 178 instances; (8) waveform, with 3 classes, 21 features, and 5000 instances; (9)
optdigits, with 10 classes, 64 features, 3823 instances. For all the datasets, we randomly select 50%
samples for training, and use the remaining samples for testing. Table 1 shows the classiﬁcation
errors of all the metric learning methods over 9 datasets averaged over 10 runs, together with the
standard deviation. We observe that the proposed metric learning algorithm deliver performance that
comparable to the state-of-the-art methods. In particular, for almost all datasets, the classiﬁcation
accuracy of the proposed algorithm is close to that of LMNN, which has yielded overall the best
performance among six baseline algorithms. This is consistent with the results of the other studies,
which show LMNN is among the most effective algorithms for distance metric learning.

To further verify if the proposed method performs statistically better than the baseline methods, we
conduct statistical test by using Wilcoxon signed-rank test [3]. The Wilcoxon signed-rank test is a
non-parametric statistical hypothesis test for the comparisons of two related samples. It is known to
be safer than the Student’s t-test because it does not assume normal distributions. From table 2, we
ﬁnd that the regularized distance metric learning improves
the classiﬁcation accuracy signiﬁcantly
compared to Mahalanobis distance, Xing’s method and RCA at signiﬁcant level 0.1. It performs
slightly better than ITML and is comparable to LMNN.

6

att−face

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
l
C

Euclidean
Mahalanobis
LMNN
ITML
RCA
Online_reg

0.18

0.2

 
0.1
0.1

0.12

0.16
0.14
Image resize ratio
(a)

att−face

 

LMNN
ITML
RCA
Online_reg

7000

6000

5000

4000

3000

2000

1000

)
s
d
n
o
c
e
s
(
 
e
m
i
t
 
g
n
i
n
n
u
R

0
 
0.1

0.12

0.18

0.2

0.14
0.16
Image resize ratio
(b)

Figure 1: (a) Face recognition accuracy of kNN and (b) running time of LMNN, ITML, RCA and
sizes.
online reg algorithms on the “att-face ” dataset with varying image

5.2 Experiment (II): Results for High Dimensional Data

To evaluate the dependence of the regularized metric learning algorithms on data dimensions, we
tested it by the task of face recognition. The AT&T face database 1 is used in our study. It consists
of grey images of faces from 40 distinct subjects, with ten pictures for each subject. For every
subject, the images were taken at different times, with varied the lighting condition and different
facial expressions (open/closed-eyes, smiling/not-smiling) and facial details (glasses/no-glasses).
The original size of each image is 112 × 92 pixels, with 256 grey levels per pixel.
To examine the sensitivity to data dimensionality, we vary the data dimension (i.e., the size of
images) by compressing the original images into size different sizes with the image aspect ratio
preserved. The image compression is achieved by bicubic interpolation (the output pixel value is a
weighted average of pixels in the nearest 4-by-4 neighborhood). For each subject, we randomly spit
its face images into training set and test set with ratio 4 : 6. A distance metric is learned from the
collection of training face images, and is used by the kNN classiﬁer ( k = 3) to predict the subject ID
of the test images. We conduct each experiment 10 times, and report the classiﬁcation accuracy by
averaging over 40 subjects and 10 runs. Figure 1 (a) shows the average classiﬁcation accuracy of the
kNN classiﬁer using different distance metric learning algo rithms. The running times of different
metric learning algorithms for the same dataset is shown in Figure 1 (b). Note that we exclude
Xing’s method in comparison because its extremely long computational time. We observed that
with increasing image size (dimensions), the regularized distance metric learning algorithm yields
stable performance, indicating that the it is resilient to high dimensional data. In contrast, for almost
all the baseline methods except ITML, their performance varied signiﬁcantly as the size of the input
image changed. Although ITML yields stable performance with respect to different size of images,
its high computational cost (Figure 1), arising from solving a Bregman optimization problem in each
iteration, makes it unsuitable for high-dimensional data.

6 Conclusion

In this paper, we analyze the generalization error of regularized distance metric learning. We show
that with appropriate constraint, the regularized distance metric learning could be robust to high
dimensional data. We also present efﬁcient learning algori thms for solving the related optimiza-
tion problems. Empirical studies with face recognition and data classiﬁcation show the proposed
approach is (i) robust and efﬁcient for high dimensional dat a, and (ii) comparable to the state-of-the-
art approaches for distance learning. In the future, we plan to investigate different regularizers and
their effect for distance metric learning.

1http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

7

ACKNOWLEDGEMENTS

The work was supported in part by the National Science Foundation (IIS-0643494) and the U. S.
Army Research Laboratory and the U. S. Army Research Ofﬁce (W 911NF-09-1-0421). Any opin-
ions, ﬁndings, and conclusions or recommendations express ed in this material are those of the au-
thors and do not necessarily re ﬂect the views of NSF and ARO.

Appendix A: Proof of Lemma 3

VD (X ) =

V (X, zi , zj )

Proof. We introduce the Bregmen divergence for the proof of this lemma. Given a convex function
of matrix ϕ(X ), the Bregmen divergence between two matrices A and B is computed as follows:
dϕ (A, B ) = ϕ(B ) − ϕ(A) − tr (cid:0)∇ϕ(A)⊤ (B − A)(cid:1)
We de ﬁne convex function N (X ) and VD (X ) as follows:
n(n − 1) Xi<j
2
N (X ) = kX k2
F ,
and furthermore convex function TD (X ) = N (X ) + C VD (X ). We thus have
dN (AD , AD i,z ) + dN (AD i,z , AD ) ≤ dTD (AD , AD i,z ) + dTDi,z (AD i,z , AD )
n(n − 1) Xj 6=i
C
=
[V (AD i,z , zi , zj ) − V (AD i,z , z , zj ) + V (AD , z , zj ) − V (AD , zi , zj )]
8C LR2
≤
|AD − AD i,z |F
n
The ﬁrst inequality follows from the fact that both N (X ) and VD (X ) are convex in X . The second
step holds because matrix AD and AD i,z minimize the objective function TD (X ) and TD i,z (X ),
respectively, and therefore
(AD i,z − AD )⊤ ∇TD (AD ) ≥ 0,
(AD − AD i,z )⊤ ∇TD i,z (AD i,z ) ≥ 0
F , we therefore have
Since dN (A, B ) = kA − Bk2
8C LR2
|AD − AD i,z |2
F ≤
n
which leads to the result in the lemma.

|AD − AD i,z |F ,

Appendix B: Proof of Theorem 7

1
λ

where

DΦ∗ (M , A0 ) +

DΦ∗ (At−1 , A′
t )

t ). Following Theorem
Proof. We denote by A′
t )⊤ and At = πS+ (A′
t = At−1 − λy (xt − x′
t )(xt − x′
11.1 and Theorem 11.4 [2], we have
nXt=1
bLn − Ln(M ) ≤
1
1
2 |A|2
2 |A − B |2
F , Φ(A) = Φ∗ (A) =
DΦ∗ (A, B ) =
F
Using the relation A′
t )⊤ and A0 = 0, we have
t = At−1 − λy (xt − x′
t )(xt − x′
At−1 ) < 0i |xt − x′
I hyt (1 − |xt − x′
nXt=1
1
1
bLn − Ln (M ) ≤
2λ |M |2
t |4
t |2
F +
2λ
2 ≤ 16R4 . Since
By assuming |x|2 ≤ R for any training example, we have |xt − x′
t |4
At−1 ) < 0i |xt − x′
I hyt (1 − |xt − x′
nXt=1
nXt=1
t |2
t |4 ≤
t |2
max(0, b − yt (1 − |xt − x′
At−1 ))
we thus have the result in the theorem

16R4
b

=

16R4
b bLn

1
λ

8

References

[1] Bousquet, Olivier, and Andr ´e Elisseeff. Stability and generalization. Journal of Machine
Learning Research, 2:499 –526, March 2002.
[2] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge Uni-
versity Press, New York, NY, USA, 2006.
[3] G.W. Corder and D.I. Foreman. Nonparametric Statistics for Non-Statisticians: A Step-by-Step
Approach. New Jersey: Wiley, 2009.
[4] J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon. Information-theoretic metric learning. In
Proceedings of the 24th international conference on Machine Learning, 2007.
[5] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural
Information Processing Systems, 2005.
[6] Steven C.H. Hoi, Wei Liu, and Shih-Fu Chang. Semi-supervised distance metric learning for
collaborative image retrieval. In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2008.
[7] Steven C.H. Hoi, Wei Liu, Michael R. Lyu, and Wei-Ying Ma. Learning distance metrics with
contextual constraints for image retrieval. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2006.
[8] L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: Un supervised learning of low dimen-
sional manifolds. Journal of Machine Learning Research, 4, 2003.
[9] Shai Shalev-Shwartz, Yoram Singer, and Andrew Y. Ng. Online and batch learning of pseudo-
metrics.
In Proceedings of the twenty- ﬁrst international conference o n Machine learning,
pages 94 –101, 2004.
[10] N. Shental, T. Hertz, D. Weinshall, and M. Pavel. Adjustment learning and relevant component
analysis. In Proceedings of the Seventh European Conference on Computer Vision, volume 4,
pages 776 –792, 2002.
[11] Jonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing
pain. Technical report, Carnegie Mellon University, Pittsburgh, PA, USA, 1994.
[12] Luo Si, Rong Jin, Steven C. H. Hoi, and Michael R. Lyu. Collaborative image retrieval via
regularized metric learning. In ACM Multimedia Systems Journal (MMSJ), 2006.
[13] J.B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290, 2000.
[14] I.W. Tsang, P.M. Cheung, and J.T. Kwok. Kernel relevant component analysis for distance
metric learning. In IEEE International Joint Conference on Neural Networks (IJCNN), 2005.
[15] K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neigh-
bor classiﬁcation. In Advances in Neural Information Processing Systems, 2005.
[16] Lei Wu, Steven C.H. Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. Distance metric learning
from uncertain side information with application to automated photo tagging. In Proceedings
of ACM International Conference on Multimedia (MM), 2009.
[17] E.P. Xing, A.Y. Ng, M.I. Jordan, and S. Russell. Distance metric learning, with application
to clustering with side-information. In Advances in Neural Information Processing Systems,
2002.
[18] L. Yang and R. Jin. Distance metric learning: A comprehensive survey. Michigan State
University, Tech. Rep., 2006.
[19] L. Yang, R. Jin, R. Sukthankar, and Y. Liu. An efﬁcient al gorithm for local distance metric
learning. In the Proceedings of the Twenty-First National Conference on Artiﬁcial Intelligence
Proceedings (AAAI), 2006.

9


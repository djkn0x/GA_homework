Accelerated Gradient Methods for
Stochastic Optimization and Online Learning

Chonghai Hu♯† , James T. Kwok♯ , Weike Pan♯
♯ Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
† Department of Mathematics, Zhejiang University
Hangzhou, China
hino.hu@gmail.com, {jamesk,weikep}@cse.ust.hk

Abstract

Regularized risk minimization often involves non-smooth optimization, either be-
cause of the loss function (e.g., hinge loss) or the regularizer (e.g., ℓ1 -regularizer).
Gradient methods, though highly scalable and easy to implement, are known to
converge slowly. In this paper, we develop a novel accelerated gradient method
for stochastic optimization while still preserving their computational simplicity
and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated
GradiEnt), exhibits fast convergence rates on stochastic composite optimization
with convex or strongly convex objectives. Experimental results show that SAGE
is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD.
Moreover, SAGE can also be extended for online learning, resulting in a simple
algorithm but with the best regret bounds currently known for these problems.

1

Introduction

Risk minimization is at the heart of many machine learning algorithms. Given a class of models
parameterized by w and a loss function ℓ(·, ·), the goal is to minimize EX Y [ℓ(w; X, Y )] w.r.t. w ,
where the expectation is over the joint distribution of input X and output Y . However, since the joint
distribution is typically unknown in practice, a surrogate problem is to replace the expectation by
its empirical average on a training sample {(x1 , y1 ), . . . , (xm , ym )}. Moreover, a regularizer Ω(·)
is often added for well-posedness. This leads to the minimization of the regularized risk
m
Xi=1
where λ is a regularization parameter. In optimization terminology, the deterministic optimization
problem in (1) can be considered as a sample average approximation (SAA) of the corresponding
stochastic optimization problem:

ℓ(w; xi , yi ) + λΩ(w),

(1)

min
w

1
m

min
w
Since both ℓ(·, ·) and Ω(·) are typically convex, (1) is a convex optimization problem which can be
conveniently solved even with standard off-the-shelf optimization packages.

EX Y [ℓ(w; X, Y )] + λΩ(w).

(2)

However, with the proliferation of data-intensive applications in the text and web domains, data sets
with millions or trillions of samples are nowadays not uncommon. Hence, off-the-shelf optimization
solvers are too slow to be used. Indeed, even tailor-made softwares for speci ﬁc models, such as the
sequential minimization optimization (SMO) method for the SVM, have superlinear computational

1

complexities and thus are not feasible for large data sets. In light of this, the use of stochastic meth-
ods have recently drawn a lot of interest and many of these are highly successful. Most are based
on (variants of) the stochastic gradient descent (SGD). Examples include Pegasos [1], SGD-QN [2],
FOLOS [3], and stochastic coordinate descent (SCD) [4]. The main advantages of these methods
are that they are simple to implement, have low per-iteration complexity, and can scale up to large
data sets. Their runtime is independent of, or even decrease with, the number of training samples
[5, 6]. On the other hand, because of their simplicity, these methods have a slow convergence rate,
and thus may require a large number of iterations.

While standard gradient schemes have a slow convergence rate, they can often be “accelerated”.
This stems from the pioneering work of Nesterov in 1983 [7], which is a deterministic algorithm
for smooth optimization. Recently, it is also extended for composite optimization, where the objec-
tive has a smooth component and a non-smooth component [8, 9]. This is particularly relevant to
machine learning since the loss ℓ and regularizer Ω in (2) may be non-smooth. Examples include
loss functions such as the commonly-used hinge loss used in the SVM, and regularizers such as the
popular ℓ1 penalty in Lasso [10], and basis pursuit. These accelerated gradient methods have also
been successfully applied in the optimization problems of multiple kernel learning [11] and trace
norm minimization [12]. Very recently, Lan [13] made an initial attempt to further extend this for
stochastic composite optimization, and obtained the convergence rate of
O (cid:16)L/N 2 + (M + σ)/√N (cid:17) .
(3)
Here, N is the number of iterations performed by the algorithm, L is the Lipschitz parameter of
the gradient of the smooth term in the objective, M is the Lipschitz parameter of the nonsmooth
term, and σ is the variance of the stochastic subgradient. Moreover, note that the ﬁrst term of (3)
is related to the smooth component in the objective while the second term is related to the non-
smooth component. Complexity results [14, 13] show that (3) is the optimal convergence rate for
any iterative algorithm solving stochastic (general) convex composite optimization.

However, as pointed out in [15], a very useful property that can improve the convergence rates in ma-
chine learning optimization problems is strong convexity. For example, (2) can be strongly convex
either because of the strong convexity of ℓ (e.g., log loss, square loss) or Ω (e.g., ℓ2 regularization).
On the other hand, [13] is more interested in general convex optimization problems and so strong
convexity is not utilized. Moreover, though theoretically interesting, [13] may be of limited practi-
cal use as (1) the stepsize in its update rule depends on the often unknown σ ; and (2) the number of
iterations performed by the algorithm has to be ﬁxed in advan ce.

Inspired by the successes of Nesterov’s method, we develop in this paper a novel accelerated sub-
gradient scheme for stochastic composite optimization. It achieves the optimal convergence rate
of O (cid:16)L/N 2 + σ/√N (cid:17) for general convex objectives, and O (cid:0)(L + µ)/N 2 + σµ−1/N (cid:1) for µ-
strongly convex objectives. Moreover, its per-iteration complexity is almost as low as that for stan-
dard (sub)gradient methods. Finally, we also extend the accelerated gradient scheme to online learn-
ing. We obtain O(√N ) regret for general convex problems and O(log N ) regret for strongly convex
problems, which are the best regret bounds currently known for these problems.

2 Setting and Mathematical Background

First, we recapitulate a few notions in convex analysis.
(Lipschitz continuity) A function f (x) is L-Lipschitz if kf (x) − f (y)k ≤ Lkx − yk.
Lemma 1. [14] The gradient of a differentiable function f (x) is Lipschitz continuous with Lipschitz
parameter L if, for any x and y ,

L
2 kx − yk2 .
f (y) ≤ f (x) + h∇f (x), y − xi +
(Strong convexity) A function φ(x) is µ-strongly convex if φ(y) ≥ φ(x)+ hg(x), y −xi+ µ
2 ky −xk2
for any x, y and subgradient g(x) ∈ ∂φ(x).
Lemma 2. [14] Let φ(x) be µ-strongly convex and x∗ = arg minx φ(x). Then, for any x,
µ
2 kx − x∗ k2 .
φ(x) ≥ φ(x∗ ) +

(4)

(5)

2

We consider the following stochastic convex stochastic optimization problem, with a composite
objective function

(6)
min
x {φ(x) ≡ E[F (x, ξ )] + ψ(x)},
where ξ is a random vector, f (x) ≡ E[F (x, ξ )] is convex and differentiable, and ψ(x) is convex
but non-smooth. Clearly, this includes the optimization problem (2). Moreover, we assume that the
gradient of f (x) is L-Lipschitz and φ(x) is µ-strongly convex (with µ ≥ 0). Note that when φ(x) is
smooth (ψ(x) = 0), µ lower bounds the smallest eigenvalue of its Hessian.
Recall that in smooth optimization, the gradient update xt+1 = xt − λ∇f (xt ) on a function f (x)
can be seen as proximal regularization of the linearized f at the current iterate xt [16]. In other
words, xt+1 = arg minx (h∇f (xt ), x − xt i + 1
2λ kx − xtk2 ). With the presence of a non-smooth
component, we have the following more general notion.
(Gradient mapping) [8] In minimizing f (x) + ψ(x), where f is convex and differentiable and ψ is
convex and non-smooth,

2λ kx − xtk2 + ψ(x)(cid:19)
x (cid:18)h∇f (x), x − xt i +
1
xt+1 = arg min
is called the generalized gradient update, and δ = 1
λ (xt − xt+1 ) is the (generalized) gradient map-
ping. Note that the quadratic approximation is made to the smooth component only. It can be shown
that the gradient mapping is analogous to the gradient in smooth convex optimization [14, 8]. This
is also a common construct used in recent stochastic subgradient methods [3, 17].

(7)

3 Accelerated Gradient Method for Stochastic Learning

Let G(xt , ξt ) ≡ ∇xF (x, ξt )|x=xt be the stochastic gradient of F (x, ξt ). We assume that it is an
unbiased estimator of the gradient ∇f (x), i.e., Eξ [G(x, ξ )] = ∇f (x). Algorithm 1 shows the pro-
posed algorithm, which will be called SAGE (Stochastic Accelerated GradiEnt).
It involves the
updating of three sequences {xt}, {yt} and {zt}. Note that yt is the generalized gradient update,
and xt+1 is a convex combination of yt and zt . The algorithm also maintains two parameter se-
quences {αt } and {Lt}. We will see in Section 3.1 that different settings of these parameters lead
to different convergence rates. Note that the only expensive step of Algorithm 1 is the computation
of the generalized gradient update yt , which is analogous to the subgradient computation in other
subgradient-based methods. In general, its computational complexity depends on the structure of
ψ(x). As will be seen in Section 3.3, this can often be efﬁciently o btained in many regularized risk
minimization problems.

Algorithm 1 SAGE (Stochastic Accelerated GradiEnt).
Input: Sequences {Lt} and {αt}.
Initialize: y−1 = z−1 = 0, α0 = λ0 = 1. L0 = L + µ.
for t = 0 to N do
xt = (1 − αt )yt−1 + αt zt−1 .
yt = arg minx (cid:8)hG(xt , ξt ), x − xt i + Lt
2 kx − xtk2 + ψ(x)(cid:9).
zt = zt−1 − (Ltαt + µ)−1 [Lt (xt − yt ) + µ(zt−1 − xt )].
end for
Output yN .

3.1 Convergence Analysis

Deﬁne ∆t ≡ G(xt , ξt ) − ∇f (xt ). Because of the unbiasedness of G(xt , ξt ), Eξt [∆t ] = 0. In the
following, we will show that the value of φ(yt ) − φ(x) can be related to that of φ(yt−1 ) − φ(x) for
any x. Let δt ≡ Lt (xt − yt ) be the gradient mapping involved in updating yt . First, we introduce
the following lemma.
Lemma 3. For t ≥ 0, φ(x) is quadratically bounded from below as
µ
2 kx − xtk2 + h∆t , yt − xi +
φ(x) ≥ φ(yt ) + hδt , x − xt i +

kδtk2 .

2Lt − L
2L2
t

3

Proposition 1. Assume that for each t ≥ 0, k∆t k∗ ≤ σ and Lt > L, then
Ltα2
t + µαt
kx − ztk2
φ(yt ) − φ(x) +
2
Ltα2
2 kx − zt−1k2 +
t
≤ (1 − αt )[φ(yt−1 ) − φ(x)] +

σ2
2(Lt − L)
Proof. Deﬁne Vt (x) = hδt , x − xt i + µ
2 kx − xtk2 + Ltαt
It is easy to see that
2 kx − zt−1 k2 .
zt = arg minx∈Rd Vt (x). Moreover, notice that Vt (x) is (Ltαt + µ)-strongly convex. Hence on
applying Lemmas 2 and 3, we obtain that for any x,
Ltαt + µ
kx − ztk2
Vt (zt ) ≤ Vt (x) −
2
µ
Ltαt + µ
Ltαt
2 kx − zt−1k2 −
2 kx − xtk2 +
= hδt , x − xt i +
2
2Lt −L
Ltαt
Ltαt +µ
kδtk2 +
2 kx− zt−1 k2 −
≤ φ(x)−φ(yt )−
2L2
2
t
Then, φ(yt ) can be bounded from above, as:

kx − ztk2
kx− zt k2 + h∆t , x− yt i.

+ αt h∆t , x − zt−1 i.

(8)

(9)

(10)

Ltαt
2Lt − L
2 kzt − zt−1 k2
kδtk2 −
φ(yt ) ≤φ(x) + hδt , xt − zt i −
2L2
t
Ltαt
Ltαt + µ
2 kx − zt−1k2 −
kx − ztk2 + h∆t , x − yt i,
+
2
where the non-positive term − µ
2 kzt − xtk2 has been dropped from its right-hand-side. On the other
hand, by applying Lemma 3 with x = yt−1 , we get
2Lt − L
kδtk2 ,
φ(yt ) − φ(yt−1 ) ≤ hδt , xt − yt−1 i + h∆t , yt−1 − yt i −
2L2
t
where the non-positive term − µ
2 kyt−1 − xtk2 has also been dropped from the right-hand-side. On
multiplying (9) by αt and (10) by 1 − αt , and then adding them together, we obtain
Ltα2
2Lt − L
kδtk2 + A + B + C −
2 kzt − zt−1 k2 , (11)
t
φ(yt ) − φ(x) ≤ (1 − αt )[φ(yt−1 ) − φ(x)] −
2L2
t
where A = hδt , αt (xt − zt ) + (1 − αt )(xt − yt−1 )i, B = αt h∆t , x − yt i + (1 − αt )h∆t , yt−1 − yt i,
2 kx − zt−1 k2 − Ltα2
and C = Ltα2
t +µαt
kx − ztk2 . In the following, we consider to upper bound A
t
2
and B . First, by using the update rule of xt in Algorithm 1 and the Young’s inequality1 , we have
A = hδt , αt (xt − zt−1 ) + (1 − αt )(xt − yt−1 )i + αt hδt , zt−1 − zt i
Ltα2
2 kzt − zt−1 k2 + kδtk2
t
= αt hδt , zt−1 − zt i ≤
.
2Lt
On the other hand, B can be bounded as
B = h∆t , αtx + (1 − αt )yt−1 − xt i + h∆t , xt − yt i = αt h∆t , x − zt−1 i + h∆t , δt i
Lt
σkδtk
≤ αt h∆t , x − zt−1 i +
,
Lt
where the second equality is due to the update rule of xt , and the last step is from the Cauchy-
Schwartz inequality and the boundedness of ∆t . Hence, plugging (12) and (13) into (11),
(Lt −L)kδt k2
σkδtk
2L2
Lt
t
σ2
+ αt h∆t , x − zt−1 i + C ,
≤ (1 − αt )[φ(yt−1 ) − φ(x)] +
2(Lt − L)
where the last step is due to the fact that −ax2 + bx ≤ b2
4a with a, b > 0. On re-arranging terms, we
obtain (8).

φ(yt ) − φ(x) ≤ (1−αt )[φ(yt−1 )−φ(x)]−

+ αt h∆t , x− zt−1 i + C

(13)

(12)

+

1The Young’s inequality states that hx, yi ≤ kxk2
2a

+ akyk2
2

for any a > 0.

4

Let the optimal solution in problem (6) be x∗ . From the update rules in Algorithm 1, we observe
that the triplet (xt , yt−1 , zt−1 ) depends on the random process ξ[t−1] ≡ {ξ0 , . . . , ξt−1 } and hence is
also random. Clearly, zt−1 and x∗ are independent of ξt . Thus,
Eξ[t] h∆t , x∗ − zt−1 i = Eξ[t−1]
Eξ[t] [h∆t , x∗ − zt−1 i|ξ[t−1] ] = Eξ[t−1]
= Eξ[t−1] hx∗ − zt−1 , Eξt [∆t ]i = 0,
where the ﬁrst equality uses Ex [h(x)] = Ey Ex [h(x)|y ], and the last equality is from our assumption
that the stochastic gradient G(x, ξ ) is unbiased. Taking expectations on both sides of (8) with x =
x∗ , we obtain the following corollary, which will be useful in proving the subsequent theorems.
Corollary 1.

Eξt [h∆t , x∗ − zt−1 i]

.

,

(14)

Ltα2
t + µαt
E[kx∗ − ztk2 ]
E[φ(yt )] − φ(x∗ ) +
2
Ltα2
E[kx∗ − zt−1k2 ] +
t
≤ (1 − αt )(E[φ(yt−1 )] − φ(x∗ )) +
2

σ2
2(Lt − L)
So far, the choice of Lt and αt in Algorithm 1 has been left unspeci ﬁed. In the following, we will
show that with a good choice of Lt and αt , (the expectation of) φ(yt ) converges rapidly to φ(x∗ ).
Theorem 1. Assume that E[kx∗ − ztk2 ] ≤ D2 for some D . Set
2
3
Lt = b(t + 1)
2 + L, αt =
t + 2
where b > 0 is a constant. Then the expected error of Algorithm 1 can be bounded as
3D2L
5σ2
3b (cid:19) 1
N 2 + (cid:18)3D2 b +
E[φ(yN )] − φ(x∗ ) ≤
√N
If σ were known, we can set b to the optimal choice of √5σ
3D , and the bound in (15) becomes 3D2L
N 2 +
2√5σD√N
.
Note that so far φ(x) is only assumed to be convex. As is shown in the following theorem, the
convergence rate can be further improved by assuming strong convexity. This also requires another
setting of αt and Lt which is different from that in (14).
Theorem 2. Assume the same conditions as in Theorem 1, except that φ(x) is µ-strongly convex.
Set
for t ≥ 1; αt = sλt−1 +
λ2
λt−1
t−1
Lt = L + µλ−1
for t ≥ 1,
4 −
t−1 ,
2
where λt ≡ Πt
k=1 (1 − αt ) for t ≥ 1 and λ0 = 1. Then, the expected error of Algorithm 1 can be
bounded as
2(L + µ)D2
6σ2
E[φ(yN )] − φ(x∗ ) ≤
(17)
N 2
N µ
In comparison, FOLOS only converges as O(log(N )/N ) for strongly convex objectives.
3.2 Remarks

+

.

.

(15)

,

(16)

As in recent studies on stochastic composite optimization [13], the error bounds in (15) and (17) con-
sist of two terms: a faster term which is related to the smooth component and a slower term related to
the non-smooth component. SAGE beneﬁts from using the struc ture of the problem and accelerates
the convergence of the smooth component. On the other hand, many stochastic (sub)gradient-based
algorithms like FOLOS do not separate the smooth from the non-smooth part, but simply treat the
whole objective as non-smooth. Consequently, convergence of the smooth component is also slowed
down to O(1/√N ).
As can be seen from (15) and (17), the convergence of SAGE is essentially encumbered by the
variance of the stochastic subgradient. Recall that the variance of the average of p i.i.d. random

5

variables is equal to 1/p of the original variance. Hence, as in Pegasos [1], σ can be reduced by
estimating the subgradient from a data subset.
Unlike the AC-SA algorithm in [13], the settings of Lt and αt in (14) do not require knowledge of
σ and the number of iterations, both of which can be difﬁcult to estimate in practice. Moreover,
with the use of a sparsity-promoting ψ(x), SAGE can produce a sparse solution (as will be exper-
imentally demonstrated in Section 5) while AC-SA cannot. This is because in SAGE, the output
yt is obtained from a generalized gradient update. With a sparsity-promoting ψ(x), this reduces to
a (soft) thresholding step, and thus ensures a sparse solution. On the other hand, in each iteration
of AC-SA, its output is a convex combination of two other variables. Unfortunately, adding two
vectors is unlikely to produce a sparse vector.

3.3 Ef ﬁcient Computation of yt

The computational efﬁciency of Algorithm 1 hinges on the efﬁ
cient computation of yt . Recall that
yt is just the generalized gradient update, and so is not signi ﬁ cantly more expensive than the gradient
update in traditional algorithms. Indeed, the generalized gradient update is often a central compo-
nent in various optimization and machine learning algorithms. In particular, Duchi and Singer [3]
showed how this can be efﬁciently computed with the various s mooth and non-smooth regulariz-
2 , ℓ∞ , Berhu and matrix norms. Interested readers are referred to [3] for
ers, including the ℓ1 , ℓ2 , ℓ2
details.

4 Accelerated Gradient Method for Online Learning

In this section, we extend the proposed accelerated gradient scheme for online learning of (2). The
algorithm, shown in Algorithm 2, is similar to the stochastic version in Algorithm 1.

Algorithm 2 SAGE-based Online Learning Algorithm.
Inputs: Sequences {Lt } and {αt }, where Lt > L and 0 < αt < 1.
Initialize: z1 = y1 .
loop
xt = (1 − αt )yt−1 + αt zt−1 .
Output yt = arg minx (cid:8)h∇ft−1 (xt ), x − xt i + Lt
2 kx − xtk2 + ψ(x)(cid:9).
zt = zt−1 − αt (Lt + µαt )−1 [Lt (xt − yt ) + µ(zt−1 − xt )].
end loop

First, we introduce the following lemma, which plays a similar role as its stochastic counterpart of
Lemma 3. Moreover, let δt ≡ Lt (xt − yt ) be the gradient mapping related to the updating of yt .
Lemma 4. For t > 1, φt (x) can be quadratically bounded from below as
µ
2Lt − L
kδtk2 .
2 kx − xtk2 +
φt−1 (x) ≥ φt−1 (yt ) + hδt , x − xt i +
2L2
t
Proposition 2. For any x and t ≥ 1, assume that there exists a subgradient ˆg(x) ∈ ∂ψ(x) such that
k∇ft (x) + ˆg(x)k∗ ≤ Q. Then for Algorithm 2,
Q2
Lt
Lt + µαt
2αt kx − zt−1k2 −
kx − ztk2
φt−1 (yt−1 ) − φt−1 (x) ≤
2αt
2(1 − αt )(Lt − L)
(1 − α2
Lt
t )Lt − αt (1 − αt )L
2 kzt − ytk2 .
kyt−1 − zt−1k2 −
+
2

(18)

+

Proof Sketch. Deﬁne τt = Ltα−1
. From the update rule of zt , one can check that
t
µ
τt
2 kx − zt−1k2 .
2 kx − xtk2 +
Vt (x) ≡ hδt , x − xt i +
zt = arg min
x
Similar to the analysis in obtaining (9), we can obtain
2Lt −L
τt
τt +µ
τt
kδtk2−
2 kx−zt−1 k2−
2 kzt−zt−1 k2+
φt−1 (yt )−φt−1 (x) ≤ hδt , xt−zt i−
2L2
2
t

kx−zt k2 . (19)

6

On the other hand,
hδt , xt − zt i − kδtk2
2Lt

Lt
2 kzt − ytk2 ,

=

Lt
(kzt − xtk2 − kzt − ytk2 )
2
Lt (1 − αt )
Lt
2αt kzt − zt−1k2 +
kzt−1 − yt−1 k2 −
≤
2
on using the convexity of k · k2 . Using (20), the inequality (19) becomes
Lt (1 − αt )
Lt
2 kzt − ytk2
kzt−1 − yt−1 k2 −
φt−1 (yt ) − φt−1 (x) ≤
2
τt + µ
τt
Lt − L
t kδtk2 +
2 kx − zt−1k2 −
kx − ztk2 .
−
2L2
2
On the other hand, by the convexity of φt−1 (x) and the Young’s inequality, we have
φt−1 (yt−1 ) − φt−1 (yt ) ≤ h∇ft−1 (yt−1 ) + ˆgt−1 (yt−1 ), yt−1 − yt i
Q2
(1 − αt )(Lt − L)
kyt−1 − ytk2 .
≤
+
2(1 − αt )(Lt − L)
2
Moreover, by using the update rule of xt and the convexity of k · k2 , we have
kyt−1 − ytk2 = k(yt−1 − xt ) + (xt − yt )k2 = kαt (yt−1 − zt−1 ) + (xt − yt )k2
≤ αtkyt−1 − zt−1k2 + (1 − αt )−1 kxt − ytk2 = αtkyt−1 − zt−1 k2 + kδtk2
(1 − αt )L2
t
On using (23), it follows from (22) that

.

(20)

(21)

(22)

(23)

+

Lt −L
t kδtk2 .
2L2

kyt−1 − zt−1 k2 +

Q2
αt (1−αt )(Lt −L)
φt−1 (yt−1 ) − φt−1 (yt ) ≤
2(1−αt )(Lt −L)
2
Inequality (18) then follows immediately by adding this to (21).
Theorem 3. Assume that µ = 0, and kx∗ − ztk ≤ D for t ≥ 1. Set αt = a and Lt = aL√t − 1+ L,
where a ∈ (0, 1) is a constant. Then the regret of Algorithm 2 can be bounded as
N
a(1 − a)L (cid:21) √N .
Q2
+ (cid:20) LD2
LD2
Xt=1
[φt (yt ) − φt (x∗ )] ≤
2a
2
Theorem 4. Assume that µ > 0, and kx∗ − ztk ≤ D for t ≥ 1. Set αt = a, and Lt = aµt + L +
a−1 (µ − L)+ , where a ∈ (0, 1) is a constant. Then the regret of Algorithm 2 can be bounded as
N
Q2
[φt (yt ) − φt (x∗ )] ≤ (cid:20) (2a + a−1 )µ + L
(cid:21) D2 +
Xt=1
2a(1 − a)µ
2a
2 + L(cid:1) D2 + 2Q2
2 , the regret bound reduces to (cid:0) 3µ
In particular, with a = 1
µ log(N + 1).

log(N + 1).

+

5 Experiments

In this section, we perform experiments on the stochastic optimization of (2). Two data sets are
used2 (Table 1). The ﬁrst one is the pcmac data set, which is a subset of the 20-newsgroup data set
from [18], while the second one is the RCV1 data set, which is a ﬁltered collection of the Reuters
RCV1 from [19]. We choose the square loss for ℓ(·, ·) and the ℓ1 regularizer for Ω(·) in (2). As
discussed in Section 3.3 and [3], the generalized gradient update can be efﬁciently computed by soft
thresholding in this case. Moreover, we do not use strong convexity and so µ = 0.
We compare the proposed SAGE algorithm (with Lt and αt in (14)) with three recent algorithms: (1)
FOLOS [3]; (2) SMIDAS [4]; and (3) SCD [4]. For fair comparison, we compare their convergence
2Downloaded from http://people.cs.uchicago.edu/∼vikass/svmlin.html and http://www.cs.ucsb.edu/∼wychen/sc.html.

7

behavior w.r.t. both the number of iterations and the number of data access operations, the latter
of which has been advocated in [4] as an implementation-independent measure of time. Moreover,
the efﬁciency tricks for sparse data described in [4] are als o implemented. Following [4], we set the
regularization parameter λ in (2) to 10−6 . The η parameter in SMIDAS is searched over the range
of {10−6 , 10−5 , 10−4 , 10−3 , 10−2 , 10−1 }, and the one with the lowest ℓ1 -regularized loss is used.
As in Pegasos [1], the (sub)gradient is computed from small sample subsets. The subset size p is set
to min(0.01m, 500), where m is the data set size. This is used on all the algorithms except SCD,
since SCD is based on coordinate descent and is quite different from the other stochastic subgradient
algorithms.3 All the algorithms are trained with the same maximum amount of “time” (i.e., number
of data access operations).

Table 1: Summary of the data sets.
sparsity
#instances
#features
data set
pcmac
7,511
1,946
0.73%
RCV1
0.12%
193,844
47,236

Results are shown in Figure 1. As can be seen, SAGE requires much fewer iterations for convergence
than the others (Figures 1(a) and 1(e)). Moreover, the additional costs on maintaining xt and zt are
small, and the most expensive step in each SAGE iteration is in computing the generalized gradient
update. Hence, its per-iteration complexity is comparable with the other (sub)gradient schemes, and
its convergence in terms of the number of data access operations is still the fastest (Figures 1(b),
1(c), 1(f) and 1(g)). Moreover, the sparsity of the SAGE solution is comparable with those of the
other algorithms (Figures 1(d) and 1(h)).

1

0.8

0.6

0.4

0.2

s
s
o
l
 
d
e
z
i
r
a
l
u
g
e
r
 
1
L

0
 
0

1

0.8

0.6

0.4

0.2

s
s
o
l
 
d
e
z
i
r
a
l
u
g
e
r
 
1
L

 

SAGE
FOLOS
SMIDAS

2000
1000
3000
Number of Iterations
(a)

4000

 

100

 

8000

 

1

0.8

0.6

0.4

0.2

s
s
o
l
 
d
e
z
i
r
a
l
u
g
e
r
 
1
L

0
 
0

SAGE
FOLOS
SMIDAS
SCD

4
2
10
8
6
Number of Data Accesses
x 106
(b)

)
%
(
 
r
o
r
r
E

80

60

40

20

0
 
0

SAGE
FOLOS
SMIDAS
SCD

w
 
f
o
 
y
t
i
s
n
e
D

6000

4000

2000

SAGE
FOLOS
SMIDAS
SCD

4
2
10
8
6
Number of Data Accesses
x 106
(c)

0
 
0

4
2
8
6
Number of Data Accesses
(d)

10
x 106

 

 

100

 

SAGE
FOLOS
SMIDAS

1

0.8

0.6

0.4

0.2

s
s
o
l
 
d
e
z
i
r
a
l
u
g
e
r
 
1
L

SAGE
FOLOS
SMIDAS
SCD

)
%
(
 
r
o
r
r
E

80

60

40

20

SAGE
FOLOS
SMIDAS
SCD

x 104

4

3

2

1

w
 
f
o
 
y
t
i
s
n
e
D

 

SAGE
FOLOS
SMIDAS
SCD

0
 
0

4000

0
 
0

1
0.5
2
1.5
1
0.5
2
1.5
1
0.5
2.5
2
1.5
2000
1000
3000
Number of Data Accesses
Number of Data Accesses
Number of Iterations
Number of Data Accesses
x 108
(h)
(g)
(f)
(e)
Figure 1: Performance of the various algorithms on the pcmac (upper) and RCV1 (below) data sets.

2.5
x 108

2.5
x 108

0
 
0

0
 
0

6 Conclusion

In this paper, we developed a novel accelerated gradient method (SAGE) for stochastic con-
vex composite optimization. It enjoys the computational simplicity and scalability of traditional
(sub)gradient methods but are much faster, both theoretically and empirically. Experimental results
show that SAGE outperforms recent (sub)gradient descent methods. Moreover, SAGE can also be
extended to online learning, obtaining the best regret bounds currently known.

Acknowledgment

This research has been partially supported by the Research Grants Council of the Hong Kong Special
Administrative Region under grant 615209.

3For the same reason, an SCD iteration is also very different from an iteration in the other algorithms.
Hence, SCD is not shown in the plots on the regularized loss versus number of iterations.

8

References

In Advances in Neural

[1] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for SVM. In Proceedings of the 24th International Conference on Machine Learning, pages
807–814, Corvalis, Oregon, USA, 2007.
[2] A. Bordes, L. Bottou, and P. Gallinari. SGD-QN: Careful Quasi-Newton Stochastic Gradient
Descent. Journal of Machine Learning Research, 10:1737–1754, 2009.
[3] J. Duchi and Y. Singer. Online and batch learning using forward looking subgradients. Tech-
nical report, 2009.
[4] S. Shalev-Shwartz and A. Tewari. Stochastic methods for ℓ1 regularized loss minimization.
In Proceedings of the 26th International Conference on Machine Learning, pages 929–936,
Montreal, Quebec, Canada, 2009.
[5] L. Bottou and O. Bousquet. The tradeoffs of large scale learning.
Information Processing Systems 20. 2008.
[6] S. Shalev-Shwartz and N. Srebro. SVM optimization: Inverse dependence on training set size.
In Proceedings of the 25th International Conference on Machine Learning, pages 928–935,
Helsinki, Finland, 2008.
[7] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of con-
vergence o( 1
k2 ). Doklady AN SSSR (translated as Soviet. Math. Docl.), 269:543–547, 1983.
[8] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discus-
sion Paper 2007/76, Catholic University of Louvain, September 2007.
[9] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2:183–202, 2009.
[10] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical
Society: Series B, 58:267–288, 1996.
[11] S. Ji, L. Sun, R. Jin, and J. Ye. Multi-label multiple kernel learning. In Advances in Neural
Information Processing Systems 21. 2009.
[12] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In Proceedings
of the International Conference on Machine Learning. Montreal, Canada, 2009.
[13] G. Lan. An optimal method for stochastic composite optimization. Technical report, School
of Industrial and Systems Engineering, Georgia Institute of Technology, 2009.
[14] Y. Nesterov and I.U.E. Nesterov.
Introductory Lectures on Convex Optimization: A Basic
Course. Kluwer, 2003.
[15] S.M. Kakade and S. Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for
online optimization. In Advances in Neural Information Processing Systems 21. 2009.
[16] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167–175, 2003.
[17] S.J. Wright, R.D. Nowak, and M.A.T. Figueiredo. Sparse reconstruction by separable approx-
imation.
In Proceedings of the International Conference on Acoustics, Speech, and Signal
Processing, Las Vegas, Nevada, USA, March 2008.
[18] V. Sindhwani and S.S. Keerthi. Large scale semi-supervised linear SVMs. In Proceedings of
the SIGIR Conference on Research and Development in Information Retrieval, pages 477–484,
Seattle, WA, USA, 2006.
[19] Y. Song, W.Y. Chen, H. Bai, C.J. Lin, and E.Y. Chang. Parallel spectral clustering. In Proceed-
ings of the European Conference on Machine Learning, pages 374–389, Antwerp, Belgium,
2008.

9


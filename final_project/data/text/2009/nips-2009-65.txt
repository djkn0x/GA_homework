Sharing Features among Dynamical Systems
with Beta Processes

Emily B. Fox
Electrical Engineering & Computer Science, Massachusetts Institute of Technology
ebfox@mit.edu

Erik B. Sudderth
Computer Science, Brown University
sudderth@cs.brown.edu

Michael I. Jordan
Electrical Engineering & Computer Science and Statistics, University of California, Berkeley
jordan@cs.berkeley.edu

Alan S. Willsky
Electrical Engineering & Computer Science, Massachusetts Institute of Technology
willsky@mit.edu

Abstract
We propose a Bayesian nonparametric approach to the problem of modeling re-
lated time series. Using a beta process prior, our approach is based on the dis-
covery of a set of latent dynamical behaviors that are shared among multiple time
series. The size of the set and the sharing pattern are both inferred from data. We
develop an efﬁcient Markov chain Monte Carlo inference meth od that is based on
the Indian buffet process representation of the predictive distribution of the beta
process. In particular, our approach uses the sum-product algorithm to efﬁciently
compute Metropolis-Hastings acceptance probabilities, and explores new dynami-
cal behaviors via birth/death proposals. We validate our sampling algorithm using
several synthetic datasets, and also demonstrate promising results on unsupervised
segmentation of visual motion capture data.

1 Introduction

In many applications, one would like to discover and model dynamical behaviors which are shared
among several related time series. For example, consider video or motion capture data depicting
multiple people performing a number of related tasks. By jointly modeling such sequences, we
may more robustly estimate representative dynamic models, and also uncover interesting relation-
ships among activities. We speciﬁcally focus on time series where behaviors can be individually
modeled via temporally independent or linear dynamical systems, and where transitions between
behaviors are approximately Markovian. Examples of such Markov jump processes include the hid-
den Markov model (HMM), switching vector autoregressive (VAR) process, and switching linear
dynamical system (SLDS). These models have proven useful in such diverse ﬁelds as speech recog-
nition, econometrics, remote target tracking, and human motion capture. Our approach envisions
a large library of behaviors, and each time series or object exhibits a subset of these behaviors.
We then seek a framework for discovering the set of dynamic behaviors that each object exhibits.
We particularly aim to allow ﬂexibility in the number of tota l and sequence-speciﬁc behaviors, and
encourage objects to share similar subsets of the large set of possible behaviors.

One can represent the set of behaviors an object exhibits via an associated list of features. A stan-
dard featural representation for N objects, with a library of K features, employs an N × K binary
matrix F = {fik }. Setting fik = 1 implies that object i exhibits feature k . Our desiderata motivate
a Bayesian nonparametric approach based on the beta process [10, 22], allowing for in ﬁnitely many

1

potential features. Integrating over the latent beta process induces a predictive distribution on fea-
tures known as the Indian buffet process (IBP) [9]. Given a feature set sampled from the IBP, our
model reduces to a collection of Bayesian HMMs (or SLDS) with partially shared parameters.

Other recent approaches to Bayesian nonparametric representations of time series include the HDP-
HMM [2, 4, 5, 21] and the in ﬁnite factorial HMM [24]. These mod els are quite different from
our framework: the HDP-HMM does not select a subset of behaviors for a given time series, but
assumes that all time series share the same set of behaviors and switch among them in exactly the
same manner. The in ﬁnite factorial HMM models a single time- series with emissions dependent
on a potentially in ﬁnite dimensional feature that evolves w ith independent Markov dynamics. Our
work focuses on modeling multiple time series and on capturing dynamical modes that are shared
among the series.

Our results are obtained via an efﬁcient and exact Markov cha in Monte Carlo (MCMC) inference al-
gorithm. In particular, we exploit the ﬁnite dynamical syst em induced by a ﬁxed set of features to ef-
ﬁciently compute acceptance probabilities, and reversibl e jump birth and death proposals to explore
new features. We validate our sampling algorithm using several synthetic datasets, and also demon-
strate promising unsupervised segmentation of data from the CMU motion capture database [23].
2 Binary Features and Beta Processes
The beta process is a completely random measure [12]: draws are discrete with probability one, and
realizations on disjoint sets are independent random variables. Consider a probability space Θ, and
let B0 denote a ﬁnite base measure on Θ with total mass B0 (Θ) = α. Assuming B0 is absolutely
continuous, we de ﬁne the following L ´evy measure on the product space [0, 1] × Θ:
ν (dω , dθ) = cω−1 (1 − ω )c−1dωB0 (dθ).
(1)
Here, c > 0 is a concentration parameter; we denote such a beta process by BP(c, B0 ). A draw
B ∼ BP(c, B0 ) is then described by

(2)

B =

ωk δθk ,

∞
Xk=1
where (ω1 , θ1 ), (ω2 , θ2 ), . . . are the set of atoms in a realization of a nonhomogeneous Poisson
process with rate measure ν . If there are atoms in B0 , then these are treated separately; see [22].
The beta process is conjugate to a class of Bernoulli processes [22], denoted by BeP(B ), which
provide our sought-for featural representation. A realization Xi ∼ BeP(B ), with B an atomic
measure, is a collection of unit mass atoms on Θ located at some subset of the atoms in B . In
particular, fik ∼ Bernoulli(ωk ) is sampled independently for each atom θk in Eq. (2), and then
Xi = Pk fik δθk .
In many applications, we interpret the atom locations θk as a shared set of global features. A
Bernoulli process realization Xi then determines the subset of features allocated to object i:
B | B0 , c ∼ BP(c, B0 )
Xi | B ∼ BeP(B ),
(3)
i = 1, . . . , N .
Because beta process priors are conjugate to the Bernoulli process [22], the posterior distribution
given N samples Xi ∼ BeP(B ) is a beta process with updated parameters:
K+
B | X1 , . . . , XN , B0 , c ∼ BP c + N ,
δθk !.
Xk=1
Here, mk denotes the number of objects Xi which select the k th feature θk . For simplicity, we have
reordered the feature indices to list the K+ features used by at least one object ﬁrst.
Computationally, Bernoulli process realizations Xi are often summarized by an in ﬁnite vector of
binary indicator variables fi = [fi1 , fi2 , . . .], where fik = 1 if and only if object i exhibits fea-
ture k . As shown by Thibaux and Jordan [22], marginalizing over the beta process measure B ,
and taking c = 1, provides a predictive distribution on indicators known as the Indian buffet pro-
cess (IBP) Grifﬁths and Ghahramani [9]. The IBP is a culinary metaphor inspired by the Chinese
restaurant process, which is itself the predictive distribution on partitions induced by the Dirichlet
process [21]. The Indian buffet consists of an in ﬁnitely lon g buffet line of dishes, or features. The
ﬁrst arriving customer, or object, chooses Poisson (α) dishes. Each subsequent customer i selects
a previously tasted dish k with probability mk /i proportional to the number of previous customers
mk to sample it, and also samples Poisson(α/i) new dishes.

mk
c + N

c
c + N

B0 +

(4)

2

3 Describing Multiple Time Series with Beta Processes

(i)
t =
y

Aj,z (i)
t

(5)

(6)

(i)
t + e
˜y

(i)
t (z (i)
t ),

(i)
t−j + e
y

(i)
t (z (i)
t ) , A
z (i)
t

Assume we have a set of N objects, each of whose dynamics is described by a switching vector
autoregressive (VAR) process, with switches occurring according to a discrete-time Markov process.
Such autoregressive HMMs (AR-HMMs) provide a simpler, but often equally effective, alternative
to SLDS [17]. Let y(i)
represent the observation vector of the ith object at time t, and z (i)
the latent
t
t
dynamical mode. Assuming an order r switching VAR process, denoted by VAR(r), we have
t ∼ π (i)
z (i)
z (i)
t−1
r
Xj=1
(i)T
(i)T
t−r ]T . The
. . . Ar,k ], and ˜y(i)
where e(i)
t (k) ∼ N (0, Σk ), Ak = [A1,k
. . . y
t = [y
t−1
standard HMM with Gaussian emissions arises as a special case of this model when Ak = 0 for
all k . We refer to these VAR processes, with parameters θk = {Ak , Σk }, as behaviors, and use a
beta process prior to couple the dynamic behaviors exhibited by different objects or sequences.
As in Sec. 2, let fi be a vector of binary indicator variables, where fik denotes whether object i
exhibits behavior k for some t ∈ {1, . . . , Ti}. Given fi , we de ﬁne a feature-constrained transition
distribution π (i) = {π (i)
k }, which governs the ith object’s Markov transitions among its set of dy-
namic behaviors. In particular, motivated by the fact that a Dirichlet-distributed probability mass
function can be interpreted as a normalized collection of gamma-distributed random variables, for
each object i we de ﬁne a doubly in ﬁnite collection of random variables:
η (i)
jk | γ , κ ∼ Gamma(γ + κδ(j, k), 1),
where δ(j, k) indicates the Kronecker delta function. We denote this collection of transition vari-
ables by η (i) , and use them to de ﬁne object-speciﬁc, feature-constraine
d transition distributions:
. . . i ⊗ fi
j = hη (i)
η (i)
j2
j1
π (i)
Pk|fik =1 η (i)
jk
Here, ⊗ denotes the element-wise vector product. This construction de ﬁnes π (i)
j over the full set of
positive integers, but assigns positive mass only at indices k where fik = 1.
The preceding generative process can be equivalently represented via a sample ˜π (i)
from a ﬁnite
j
Dirichlet distribution of dimension Ki = Pk fik , containing the non-zero entries of π (i)
:
j
˜π (i)
| fi , γ , κ ∼ Dir([γ , . . . , γ , γ + κ, γ , . . . γ ]).
j
The κ hyperparameter places extra expected mass on the component of ˜π (i)
corresponding to a self-
j
transition π (i)
jj , analogously to the sticky hyperparameter of Fox et al. [4]. We refer to this model,
which is summarized in Fig. 1, as the beta process autoregressive HMM (BP-AR-HMM).
4 MCMC Methods for Posterior Inference
We have developed an MCMC method which alternates between resampling binary feature assign-
ments given observations and dynamical parameters, and dynamical parameters given observations
and features. The sampler interleaves Metropolis-Hastings (MH) and Gibbs sampling updates,
which are sometimes simpliﬁed by appropriate auxiliary var iables. We leverage the fact that ﬁxed
feature assignments instantiate a set of ﬁnite AR-HMMs, for which dynamic programming can be
used to efﬁciently compute marginal likelihoods. Our novel approach to resampling the potentially
in ﬁnite set of object-speciﬁc features employs incrementa
l “birth ” and “death ” proposals, improving
on previous exact samplers for IBP models with non-conjugate likelihoods.

.

(7)

(8)

(9)

4.1 Sampling binary feature assignments
Let F −ik denote the set of all binary feature indicators excluding fik , and K −i
+ be the number of
behaviors currently instantiated by objects other than i. For notational simplicity, we assume that

3

γ

κ

ω
k

∞

f i

(i)π

B0

θk

∞

(i)
z 1
z 1

(i)
y1

(i)
z 2

(i)
y2

(i)
z 3

(i)
y3

. . .

. . .

(i)
z T
i

(i)
yT
i

N

Figure 1: Graphical model of the BP-AR-HMM. The beta process distributed measure B | B0 ∼ BP(1, B0 )
is represented by its masses ωk and locations θk , as in Eq. (2). The features are then conditionally inde-
pendent draws fik | ωk ∼ Bernoulli(ωk ), and are used to deﬁne feature-constrained transition dist ributions
π (i)
| fi , γ , κ ∼ Dir([γ , . . . , γ , γ + κ, γ , . . . ] ⊗ fi ). The switching VAR dynamics are as in Eq. (6).
j

+ }. Given the ith object’s observation sequence y(i)
these behaviors are indexed by {1, . . . , K −i
,
1:Ti
transition variables η (i) = η (i)
, and shared dynamic parameters θ1:K−i
, feature indicators
+ ,1:K−i
1:K−i
+
+
fik for currently used features k ∈ {1, . . . , K −i
+ } have the following posterior distribution:

p(fik | F −ik, y(i)
1:Ti

, η (i), θ1:K−i
+

, α) ∝ p(fik | F −ik, α)p(y(i)
1:Ti

| fi , η(i), θ1:K−i
+

).

(10)

Here, the IBP prior implies that p(fik = 1 | F −ik, α) = m−i
k /N , where m−i
k denotes the number of
objects other than object i that exhibit behavior k . In evaluating this expression, we have exploited
the exchangeability of the IBP [9], which follows directly from the beta process construction [22].

, α)

, α)

For binary random variables, MH proposals can mix faster [6] and have greater statistical efﬁ-
ciency [14] than standard Gibbs samplers. To update fik given F −ik, we thus use the posterior
of Eq. (10) to evaluate a MH proposal which ﬂips fik to the complement ¯f of its current value f :
fik ∼ ρ( ¯f | f )δ(fik , ¯f ) + (1 − ρ( ¯f | f ))δ(fik , f )
(i)
ρ( ¯f | f ) = min ( p(fik = ¯f | F −ik, y
, η (i), θ1:K−i
, 1).
1:Ti
+
p(fik = f | F −ik, y(i)
, η (i), θ1:K−i
1:Ti
+
To compute likelihoods, we combine fi and η (i) to construct feature-constrained transition distribu-
tions π (i)
as in Eq. (8), and apply the sum-product message passing algorithm [19].
j
An alternative approach is needed to resample the Poisson(α/N ) “unique ” features associated only
with object i. Let K+ = K −i
+ + ni , where ni is the number of features unique to object i, and de ﬁne
. The posterior distribution over ni is then given by
and f+i = fi,K−i
f−i = fi,1:K−i
+ +1:K+
+
(i)
, η (i), θ1:K−i
p(ni | fi , y
, α)
1:Ti
+
N )ni e− α
( α
Z Z p(y(i)
N
1:Ti
ni !
are the parameters of unique
where H is the gamma prior on transition variables, θ+ = θK−i
+ +1:K+
features, and η+ are transition parameters η (i)
jk to or from unique features j, k ∈ {K −i
+ + 1 : K+}.
Exact evaluation of this integral is intractable due to dependencies induced by the AR-HMMs.

| f−i , f+i = 1, η(i), η+ , θ1:K−i
+

, θ+ ) dB0 (θ+ )dH (η+ ),

(11)

(12)

∝

One early approach to approximate Gibbs sampling in non-conjugate IBP models relies on a ﬁnite
truncation [7]. Meeds et al. [15] instead consider independent Metropolis proposals which replace
the existing unique features by n′
i ∼ Poisson(α/N ) new features, with corresponding parameters
θ ′
+ drawn from the prior. For high-dimensional models like that considered in this paper, however,
moves proposing large numbers of unique features have low acceptance rates. Thus, mixing rates
are greatly affected by the beta process hyperparameter α. We instead develop a “birth and death ”
reversible jump MCMC (RJMCMC) sampler [8], which proposes to either add a single new feature,

4

(16)

or eliminate one of the existing features in f+i . Some previous work has applied RJMCMC to ﬁnite
binary feature models [3, 27], but not to the IBP. Our proposal distribution factors as follows:
q(f ′
+i , θ ′
+ | f+i , θ+ , η+ ) = qf (f ′
+i | f+i )qθ (θ ′
+ | f ′
+ | f ′
+ , η ′
+i , f+i , θ+ )qη (η ′
+i , f+i , η+ ). (13)
Let ni = Pk f+ik . The feature proposal qf (· | ·) encodes the probabilities of birth and death
moves: a new feature is created with probability 0.5, and each of the ni existing features is deleted
with probability 0.5/ni . For parameters, we de ﬁne our proposal using the generative model:
+,ni+1 ) Qni
birth of feature ni + 1;
k=1 δθ+k (θ ′
+i , f+i , θ+ ) = (cid:26) b0 (θ ′
+k ),
qθ (θ ′
+ | f ′
(14)
death of feature ℓ,
Qk 6=ℓ δθ+k (θ ′
+k ),
where b0 is the density associated with α−1B0 . The distribution qη (· | ·) is de ﬁned similarly, but
using the gamma prior on transition variables of Eq. (7). The MH acceptance probability is then
ρ(f ′
+i , θ ′
+ | f+i , θ+ , η+ ) = min{r(f ′
+i , θ ′
+ , η ′
+ , η ′
(15)
+ | f+i , θ+ , η+ ), 1}.
Canceling parameter proposals with corresponding prior terms, the acceptance ratio r(· | ·) equals
(i)
i | α/N ) qf (f+i | f ′
| [f−i f ′
, θ ′
+ ) Poisson(n′
+ , η(i) , η ′
p(y
+i ], θ1:K−i
+i )
1:Ti
+
p(y(i)
, θ+ , η(i) , η+ ) Poisson(ni | α/N ) qf (f ′
| [f−i f+i ], θ1:K−i
+i | f+i )
1:Ti
+
+ik . Because our birth and death proposals do not modify the values of existing
with n′
i = Pk f ′
parameters, the Jacobian term normally arising in RJMCMC algorithms simply equals one.
4.2 Sampling dynamic parameters and transition variables
Posterior updates to transition variables η (i) and shared dynamic parameters θk are greatly simpli-
ﬁed if we instantiate the mode sequences z (i)
for each object i. We treat these mode sequences as
1:Ti
auxiliary variables: they are sampled given the current MCMC state, conditioned on when resam-
pling model parameters, and then discarded for subsequent updates of feature assignments fi .
Given feature-constrained transition distributions π(i) and dynamic parameters {θk }, along with
(i)
, we jointly sample the mode sequence z (i)
the observation sequence y
by computing backward
1:Ti
1:Ti
, π(i) , {θk }), and then recursively sampling each z (i)
, ˜y(i)
| z (i)
t ) ∝ p(y(i)
messages mt+1,t (z (i)
:
t
t
t
t+1:Ti
t )N (cid:0)y(i)
(z (i)
, π(i), {θk } ∼ π (i)
t−1 , y(i)
| z (i)
z (i)
t (cid:1)mt+1,t (z (i)
, Σz (i)
t ).
t
t
1:Ti
(i)
z
t−1
1:T , the posterior of π (i)
Because Dirichlet priors are conjugate to multinomial observations z (i)
j
jj , γ + n(i)
jj−1 , γ + κ + n(i)
j1 , . . . , γ + n(i)
| fi , z (i)
1:T , γ , κ ∼ Dir([γ + n(i)
π (i)
jj+1 , . . . ] ⊗ fi ).
j
1:T . Since the mode sequence z (i)
jk are the number of transitions from mode j to k in z (i)
Here, n(i)
1:T is
generated from feature-constrained transition distributions, n(i)
jk is zero for any k such that fik = 0.
Thus, to arrive at the posterior of Eq. (18), we only update η (i)
jk for instantiated features:
1:T , γ , κ ∼ Gamma(γ + κδ(j, k) + n(i)
η (i)
jk | z (i)
k ∈ {ℓ | fiℓ = 1}.
jk , 1),
We now turn to posterior updates for dynamic parameters. We place a conjugate matrix-normal
inverse-Wishart (MNIW) prior [26] on {Ak , Σk }, comprised of an inverse-Wishart prior IW(S0 , n0 )
on Σk and a matrix-normal prior MN (Ak ; M , Σk , K ) on Ak given Σk . We consider the follow-
ing sufﬁcient statistics based on the sets Y k = {y(i)
t = k} and ˜Y k = {˜y(i)
| z (i)
| z (i)
t = k} of
t
t
observations and lagged observations, respectively, associated with behavior k :
t ˜y(i)T
t ˜y(i)T
S (k)
˜y(i)
t + K S (k)
y(i)
˜y ˜y = X(t,i)|z
y ˜y = X(t,i)|z
t + M K
(i)
(i)
t =k
t =k
˜y ˜y S (k)T
t y(i)T
y ˜y S−(k)
y(i)
yy − S (k)
S (k)
yy = X(t,i)|z
S (k)
y | ˜y = S (k)
t + M KM T
˜y ˜y
(i)
t =k
Following Fox et al. [5], the posterior can then be shown to equal
y | ˜y + S0 , |Y k | + n0(cid:17) .
˜y ˜y (cid:17) , Σk | Y k ∼ IW (cid:16)S (k)
Ak | Σk , Y k ∼ MN (cid:16)Ak ; S (k)
, Σk , S (k)
y ˜y S−(k)
˜y ˜y
5

˜y(i)
t

; A

z (i)
t

(19)

(17)

is

(18)

,

.

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

5.5

0.5

1

1.5

2

2.5

3

3.5

4

4.5

2

4

6

8

10

12

14

16

18

20

200

400

600

800

1000

25

20

15

10

5

0

s
n
o
i
t
a
v
r
e
s
b
O

−5

0

Time
10
(b)
(a)
Figure 2: (a) Observation sequences for each of 5 switching AR(1) time series colored by true mode sequence,
and offset for clarity. (b) True feature matrix (top) of the ﬁ ve objects and estimated feature matrix (bottom)
averaged over 10,000 MCMC samples taken from 100 trials every 10th sample. White indicates active features.
The estimated feature matrices are produced from mode sequences mapped to the ground truth labels according
to the minimum Hamming distance metric, and selecting modes with more than 2% of the object’s observations.

5.5

16

12

18

20

14

5

2

4

6

8

4.3 Sampling the beta process and Dirichlet transition hyperparameters
We additionally place priors on the Dirichlet hyperparameters γ and κ, as well as the beta process
parameter α. Let F = {f i }. As derived in [9], p(F | α) can be expressed as
N
p(F | α) ∝ αK+ exp (cid:18) − α
n (cid:19),
1
Xn=1
where, as before, K+ is the number of unique features activated in F . As in [7], we place a conjugate
Gamma(aα , bα ) prior on α, which leads to the following posterior distribution:
N
n (cid:19).
p(α | F , aα , bα ) ∝ p(F | α)p(α | aα , bα ) ∝ Gamma(cid:18)aα + K+ , bα +
1
Xn=1
Transition hyperparameters are assigned similar priors γ ∼ Gamma(aγ , bγ ), κ ∼ Gamma(aκ , bκ ).
Because the generative process of Eq. (7) is non-conjugate, we rely on MH steps which iteratively
resample γ given κ, and κ given γ . Each sub-step uses a gamma proposal distribution q(· | ·) with
ﬁxed variance σ2
γ or σ2
κ , and mean equal to the current hyperparameter value. To update γ given κ,
the acceptance probability is min{r(γ ′ | γ ), 1}, where r(γ ′ | γ ) is de ﬁned to equal

(20)

(21)

=

p(γ ′ | κ, π , F )q(γ | γ ′ )
p(γ | κ, π , F )q(γ ′ | γ )

f (γ ′)Γ(ϑ)e−γ ′ bγ γ ϑ′−ϑ−aγ σ2ϑ
p(π | γ ′ , κ, F )p(γ ′ )q(γ | γ ′ )
γ
f (γ )Γ(ϑ′ )e−γ bγ γ ′ϑ−ϑ′−aγ σ2ϑ′
p(π | γ , κ, F )p(γ )q(γ ′ | γ )
γ
(j,k)=1 π (i)γ+κδ(k,j)−1
Γ(γKi+κ)Ki
−Ki Γ(γ+κ)Ki QKi
. The
Here, ϑ = γ 2/σ2
γ , ϑ′ = γ ′2/σ2
γ , and f (γ ) = Qi
Γ(γ )K2
kj
i
MH sub-step for resampling κ given γ is similar, but with an appropriately rede ﬁned f (κ).
5 Synthetic Experiments

=

.

To test the ability of BP-AR-HMM to discover shared dynamics, we generated ﬁve time series that
switched between AR(1) models

y (i)
y (i)
t (z (i)
t−1 + e(i)
t = az
t )
(i)
t
with ak ∈ {−0.8, −0.6, −0.4, −0.2, 0, 0.2, 0.4, 0.6, 0.8} and process noise covariance Σk drawn
from an IW(0.5, 3) prior. The object-speciﬁc features, shown in Fig. 2(b), wer e sampled from a
truncated IBP [9] using α = 10 and then used to generate the observation sequences of Fig. 2(a).
The resulting feature matrix estimated over 10,000 MCMC samples is shown in Fig. 2. Comparing
to the true feature matrix, we see that our model is indeed able to discover most of the underlying
latent structure of the time series despite the challenging setting de ﬁned by the close AR coefﬁcients.

(22)

One might propose, as an alternative to the BP-AR-HMM, using an architecture based on the hi-
erarchical Dirichlet process of [21]; speciﬁcally we could use the HDP-AR-HMMs of [5] tied
together with a shared set of transition and dynamic parameters. To demonstrate the difference
between these models, we generated data for three switching AR(1) processes. The ﬁrst two ob-
jects, with four times the data points of the third, switched between dynamical modes de ﬁned

6


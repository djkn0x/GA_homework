Sufﬁcient Conditions for Agnostic Active Learnable

Liwei Wang
Key Laboratory of Machine Perception, MOE,
School of Electronics Engineering and Computer Science,
Peking University,
wanglw@cis.pku.edu.cn

Abstract

We study pool-based active learning in the presence of noise, i.e. the agnostic set-
ting. Previous works have shown that the effectiveness of agnostic active learning
depends on the learning problem and the hypothesis space. Although there are
many cases on which active learning is very useful, it is also easy to construct
examples that no active learning algorithm can have advantage. In this paper, we
propose intuitively reasonable sufﬁcient conditions under which agnostic active
learning algorithm is strictly superior to passive supervised learning. We show
that under some noise condition, if the Bayesian classiﬁcation boundary and the
underlying distribution are smooth to a ﬁnite order, active learning achieves poly-
nomial improvement in the label complexity; if the boundary and the distribution
are inﬁnitely smooth, the improvement is exponential.

1 Introduction

Active learning addresses the problem that the algorithm is given a pool of unlabeled data drawn
i.i.d. from some underlying distribution. The algorithm can then pay for the label of any example
in the pool. The goal is to learn an accurate classiﬁer by requesting as few labels as possible. This
is in contrast with the standard passive supervised learning, where the labeled examples are chosen
randomly.

The simplest example that demonstrates the potential of active learning is to learn the optimal thresh-
old on an interval. If there exists a perfect threshold separating the two classes (i.e. there is no noise),
 ) labels to learn an -accurate classiﬁer, while passive learn-
then binary search only needs O(ln 1
ing requires O( 1
 ) labels. Another encouraging example is to learn a homogeneous linear separator
for data uniformly distributed on the unit sphere of Rd . In this case active learning can still give
exponential savings in the label complexity [Das05].

However, there are also very simple problems that active learning does not help at all. Suppose the
instances are uniformly distributed on [0, 1], and the positive class could be any interval on [0, 1].
Any active learning algorithms needs O( 1
 ) label requests to learn an -accurate classiﬁer [Han07].
There is no improvement over passive learning. All above are noise-free (realizable) problems. Of
more interest and more realistic is the agnostic setting, where the class labels can be noisy so that
the best classiﬁer in the hypothesis space has a non-zero error ν . For agnostic active learning, there
is no active learning algorithm that can always reduce label requests due to a lower bound Ω( ν 2
2 ) for
the label complexity [Kaa06].

It is known that whether active learning helps or not depends on the distribution of the instance-label
pairs and the hypothesis space. Thus a natural question would be that under what conditions is active
learning guaranteed to require fewer labels than passive learning.

1

In this paper we propose intuitively reasonable sufﬁcient conditions under which active learning
achieves lower label complexity than that of passive learning. Speciﬁcally, we focus on the A2 al-
gorithm [BAL06] which works in the agnostic setting. Earlier work has discovered that the label
complexity of A2 can be upper bounded by a parameter of the hypothesis space and the data dis-
tribution called disagreement coefﬁcient
[Han07]. This parameter often characterizes the intrinsic
difﬁculty of the learning problem. By an analysis of the disagreement coefﬁcient we show that, un-
der some noise condition, if the Bayesian classiﬁcation boundary and the underlying distribution are
smooth to a ﬁnite order, then A2 gives polynomial savings in the label complexity; if the boundary
and the distribution are inﬁnitely smooth, A2 gives exponential savings.

1.1 Related Works

Our work is closely related to [CN07], in which the authors proved sample complexity bounds for
problems with smooth classiﬁcation boundary under Tsybakov’s noise condition [Tsy04]. They also
assumed that the distribution of the instances is bounded from above and below. The main difference
to our work is that their analysis is for the membership-query setting [Ang88], in which the learning
algorithm can choose any point in the instance space and ask for its label; while the pool-based
model analyzed here assumes the algorithm can only request labels of the instances it observes.

Another related work is due to Friedman [Fri09]. He introduced a different notion of smoothness
and showed that this guarantees exponential improvement for active learning. But his work focused
on the realizable case and does not apply to the agnostic setting studied here.
Soon after A2 , Dasgupta, Hsu and Monteleoni [DHM07] proposed an elegant agnostic active learn-
ing algorithm. It reduces active learning to a series of supervised learning problems. If the hy-
pothesis space has a ﬁnite VC dimension, it has a better label complexity than A2 . However, this
algorithm relies on the normalized uniform convergence bound for the VC class. It is not known
whether it holds for more general hypothesis space such as the smooth boundary class analyzed in
this paper. (For recent advances on this topic, see [GKW03].) It is left as an open problem whether
our results apply to this algorithm by re ﬁned analysis of the normalized bounds.

2 Preliminaries
Let X be an instance space, D a distribution over X × {−1, 1}. Let H be the hypothesis space, a
set of classiﬁers from X to {±1}. Denote DX the marginal of D over X . In our active learning
model, the algorithm has access to a pool of unlabeled examples from DX . For any unlabeled point
(cid:80)
x, the algorithm can ask for its label y , which is generated from the conditional distribution at x.
The error of a hypothesis h according to D is erD (h) = Pr(x,y)∼D (h(x) (cid:54)= y). The empirical error
on a ﬁnite sample S is erS (h) = 1|S |
(x,y)∈S I[h(x) (cid:54)= y ], where I is the indicator function. We
use h∗ denote the best classiﬁer in H. That is, h∗ = arg minh∈H erD (h). Let ν = erD (h∗ ). Our
goal is to learn a ˆh ∈ H with error rate at most ν + , where  is a predeﬁned parameter.
A2 is the ﬁrst rigorous agnostic active learning algorithm. A description of the algorithm is given
in Fig.1. It was shown that A2 is never much worse than passive learning in terms of the label
complexity. The key observation that A2 can be superior to passive learning is that, since our goal is
to choose an ˆh such that erD (ˆh) ≤ erD (h∗ ) + , we only need to compare the errors of hypotheses.
Therefore we can just request labels of those x on which the hypotheses under consideration have
disagreement.
To do this, the algorithm keeps track of two spaces. One is the current version space Vi , consisting
of hypotheses that with statistical conﬁdence are not too bad compared to h∗ . To achieve such a
statistical guarantee, the algorithm must be provided with a uniform convergence bound over the
hypothesis space. That is, with probability at least 1 − δ over the draw of sample S according to D ,
LB (S , h, δ) ≤ erD (h) ≤ U B (S , h, δ),
hold simultaneously for all h ∈ H, where the lower bound LB (S , h, δ) and upper bound
U B (S , h, δ) can be computed from the empirical error erS (h). The other space is the region of
disagreement DI S (Vi ), which is the set of all x ∈ X for which there are hypotheses in Vi that
disagree on x. Formally, for any V ⊂ H,
DI S (V ) = {x ∈ X : ∃h, h(cid:48) ∈ V , h(x) (cid:54)= h(cid:48) (x)}.

2

Input: concept space H, accuracy parameter  ∈ (0, 1), con ﬁdence parameter δ ∈ (0, 1);
Output: classiﬁer ˆh ∈ H;
(λ depends on H and the problem, see Theorem 5) ;
 + ln 1
δ ) log2
Let ˆn = 2(2 log2
2
λ
Let δ (cid:48) = δ/ ˆn ;

V0 ← H, S0 ← ∅, i ←0, j1 ←0, k ←1 ;
while ∆(Vi )(minh∈Vi U B (Si , h, δ (cid:48) ) − minh∈Vi LB (Si , h, δ (cid:48) )) >  do
Vi+1 ← {h ∈ Vi : LB (Si , h, δ (cid:48) ) ≤ minh(cid:48)∈Vi U B (Si , h(cid:48) , δ (cid:48) )};
i ←i+1;
if ∆(Vi ) < 1
2 ∆(Vjk ) then
k ← k + 1; jk ← i;
end
i ← Rejection sample 2i−jk samples x from D satisfying x ∈ DI S (Vi );
S (cid:48)
Si ← {(x, y = label(x)) : x ∈ S (cid:48)
i };
end
Return ˆh= argminh∈Vi U B (Si , h, δ (cid:48) ).
Algorithm 1: The A2 algorithm (this is the version in [Han07])

The volume of DI S (V ) is denoted by ∆(V ) = PrX∼DX (X ∈ DI S (V )). Requesting labels of
the instances from DI S (Vi ) allows A2 require fewer labels than passive learning. Hence the key
issue is how fast ∆(Vi ) reduces. This process, and in turn the label complexity of A2 , are nicely
characterized by the disagreement coefﬁcient θ introduced in [Han07].

Deﬁnition 1 Let ρ(·, ·) be the pseudo-metric on a hypothesis space H induced by DX . That is, for
h, h(cid:48) ∈ H, ρ(h, h(cid:48) ) = PrX∼DX (h(X ) (cid:54)= h(cid:48) (X )). Let B (h, r) = {h(cid:48) ∈ H: ρ(h, h(cid:48) ) ≤ r}. The
disagreement coefﬁcient θ() is

θ() = sup
r≥

PrX∼DX (X ∈ DI S (B (h∗ , r)))
r

,

(1)

where h∗ = arg minh∈H erD (h).

Note that θ depends on H and D , and 1 ≤ θ() ≤ 1
 .

3 Main Results

As mentioned earlier, whether active learning helps or not depends on the distribution and the hy-
pothesis space. There are simple examples such as learning intervals for which active learning has
no advantage. However, these negative examples are more or less “artiﬁcial”. It is important to
understand whether problems with practical interest are actively learnable or not. In this section we
provide intuitively reasonable conditions under which the A2 algorithm is strictly superior to passive
learning. Our main results (Theorem 11 and Theorem 12) show that if the learning problem has a
smooth Bayes classiﬁcation boundary, and the distribution DX has a density bounded by a smooth
function, then under some noise condition A2 saves label requests. It is a polynomial improvement
for ﬁnite smoothness, and exponential for inﬁnite smoothness.

In Section 3.1 we formally deﬁne the smoothness and introduce the hypothesis space, which contains
smooth classiﬁers. We show a uniform convergence bound of order O(n−1/2 ) for this hypothesis
space. This bound determines U B (S , h, δ) and LB (S , h, δ) in A2 . Section 3.2 is the main technical
part, where we give upper bounds for the disagreement coefﬁcient of smooth problems. In Section
3.3 we show that under some noise condition, there is a sharper bound for the label complexity in
terms of the disagreement coefﬁcient. These lead to our main results.

3

3.1 Smoothness
(cid:80)d
Let f be a function deﬁned on Ω ⊂ Rd . For any vector k = (k1 , · · · , kd ) of d nonnegative integers,
let |k| =
i=1 ki . Deﬁne the K -norm as
|Dk f (x)| + max
(cid:107)f (cid:107)K := max
|k|≤K−1
|k|=K−1

Dk f (x) − Dk f (x(cid:48) )
(cid:107)x − x(cid:48) (cid:107)

sup
x,x(cid:48)∈Ω

sup
x∈Ω

,

(2)

where

is the differential operator.

Dk =

∂ |k|
∂ k1 x1 · · · ∂ kd xd

,

Deﬁnition 2
(Finite Smooth Functions) A function f is said to be K th order smooth with respect
to a constant C , if (cid:107)f (cid:107)K ≤ C . The set of K th order smooth functions is deﬁned as
C := {f : (cid:107)f (cid:107)K ≤ C }.
F K
(3)
Thus K th order smooth functions have uniformly bounded partial derivatives up to order K − 1, and
the K − 1th order partial derivatives are Lipschitz.

Deﬁnition 3
(Inﬁnitely Smooth Functions ) A function f is said to be inﬁnitely smooth with respect
to a constant C , if (cid:107)f (cid:107)K ≤ C for all nonnegative integers K . The set of inﬁnitely smooth functions
is denoted by F ∞
C .

With the deﬁnitions of smoothness, we introduce the hypothesis space we use in the A2 algorithm.
(Hypotheses with Smooth Boundaries) A set of hypotheses HK
C de ﬁned on [0, 1]d+1
Deﬁnition 4
is said to have K th order smooth boundaries, if for every h ∈ HK
C , the classiﬁcation boundary is
a K th order smooth function on [0, 1]d . To be precise, let x = (x1 , x2 , . . . , xd+1 ) ∈ [0, 1]d+1 . The
classiﬁcation boundary is the graph of function xd+1 = f (x1 , . . . , xd ), where f ∈ F K
C . Similarly,
C is said to have in ﬁnitely smooth boundaries, if for every h ∈ H∞
a hypothesis space H∞
C the
[0, 1]d .
classiﬁcation boundary is the graph an in ﬁnitely smooth function on

Previous results on the label complexity of A2 assumes the hypothesis space has ﬁnite VC di-
mension. The goal is to ensure a O(n−1/2 ) uniform convergence bound so that U B (S , h, δ) −
LB (S , h, δ) = O(n−1/2 ). The hypothesis space HK
C and H∞
C do not have ﬁnite VC dimensions.
Compared with the VC class, HK
C and H∞
C are exponentially larger in terms of the covering num-
C and H∞
bers [vdVW96]. But uniform convergence bound still holds for HK
C under a broad class of
distributions. The following theorem is a consequence of some known results in empirical processes.
Theorem 5 For any distribution D over [0, 1]d+1 × {−1, 1}, whose marginal distribution DX on
[0, 1]d+1 has a density upper bounded by a constant M , and any 0 < δ ≤ δ0 (δ0 is a constant), with
(cid:115)
probability at least 1 − δ over the draw of the training set S of n examples,

log 1
|erD (h) − erS (h)| ≤ λ
δ
(4)
,
n
holds simultaneously for all h ∈ HK
C provided K > d (or K = ∞). Here λ is a constant depending
only on d, K , C and M .
Proof It can be seen, from Corollary 2.7.3 in [vdVW96] that the bracketing numbers N[ ] of HK
(cid:195)
(cid:33)
(cid:181)
(cid:182)
C
log N[ ] (, HK
C , L2 (DX )) = O(( 1
 ) 2d
K ). Since K > d, then there exist constants c1 , c2 such
satisﬁes
that
− nt2
≤ c1 exp
|er(h) − erS (h)| ≥ t
sup
PD
(cid:179)
(cid:180)
h∈HK
c2
C
for all nt2 ≥ t0 , where t0 is some constant (see Theorem 5.11 and Lemma 5.10 of [vdG00]). Let
− nt2
δ = c1 exp
, the theorem follows.
c2

4

(cid:113)
(cid:113)
Now we can determine U B (S , h, δ) and LB (S , h, δ) for A2 by simply letting U B (S , h, δ) =
n and LB (S , h, δ) = erS (h) − λ
n , where S is of size n.
ln 1
ln 1
erS (h) + λ
δ
δ

3.2 Disagreement Coefﬁcient

The disagreement coefﬁcient θ plays an important role for the label complexity of active learning
algorithms. In fact previous negative examples for which active learning does not work are all the
results of large θ . For instance the interval learning problem, θ() = 1
 , which leads to the same
label complexity as passive learning. In the following two theorems we show that the disagreement
coefﬁcient θ() for smooth problems is small.
Theorem 6 Let the hypothesis space be HK
C . If the distribution DX has a density p(x1 , . . . , xd+1 )
(cid:179)(cid:161)
(cid:180)
(cid:162) d
such that there exists a K th order smooth function g(x1 , . . . , xd+1 ) and two constants 0 < α ≤
β such that αg(x1 , . . . , xd+1 ) ≤ p(x1 , . . . , xd+1 ) ≤ β g(x1 , . . . , xd+1 ) for all (x1 , . . . , xd+1 ) ∈
[0, 1]d+1 , then θ() = O
1
.
K+d

Theorem 7 Let the hypothesis space be H∞
C . If the distribution DX has a density p(x1 , . . . , xd+1 )
such that there exist an inﬁnitely smooth function g(x1 , . . . , xd ) and two constants 0 < α ≤ β such
that αg(x1 , . . . , xd ) ≤ p(x1 , . . . , xd+1 ) ≤ β g(x1 , . . . , xd ) for all (x1 , . . . , xd+1 ) ∈ [0, 1]d+1 , then
 )).
θ() = O(logd ( 1

The key points in the theorems are: the classiﬁcation boundaries are smooth; and the density is
bounded from above and below by constants times a smooth function. These two conditions include
a large class of learning problems. Note that the density itself is not necessarily smooth. We just
require the density does not change too rapidly.
The intuition behind the two theorems above is as follows. Let fh∗ (x) and fh (x) be the classiﬁcation
boundaries of h∗ and h, and suppose ρ(h, h∗ ) is small, where ρ(h, h∗ ) = Prx∼DX (h(x) (cid:54)= h∗ (x))
is the pseudo metric. If the classiﬁcation boundaries and the density are all smooth, then the two
boundaries have to be close to each other everywhere. That is, |fh (x) − ff ∗ (x)| is small uniformly
for all x. Hence only the points close to the classiﬁcation boundary of h∗ can be in DI S (B (h∗ , )),
which leads to a small disagreement coefﬁcient.
(cid:82)
The proofs of Theorem 6 and Theorem 7 rely on the following two lemmas.
[0,1]d |Φ(x)|dx ≤ r . If there exists a K th
Lemma 8 Let Φ be a function deﬁned on [0, 1]d and
order smooth function ˜Φ and 0 < α ≤ β such that α| ˜Φ(x)| ≤ |Φ(x)| ≤ β | ˜Φ(x)| for all x ∈ [0, 1]d ,
(cid:82)
then (cid:107)Φ(cid:107)∞ = O(r
K+d ), where (cid:107)Φ(cid:107)∞ = supx∈[0,1]d |Φ(x)|.
K+d ) = O(r · ( 1
K
r ) d
[0,1]d |Φ(x)|dx ≤ r . If there exists an inﬁnitely
Lemma 9 Let Φ be a function deﬁned on [0, 1]d and
smooth function ˜Φ and 0 < α ≤ β such that α| ˜Φ(x)| ≤ |Φ(x)| ≤ β | ˜Φ(x)| for all x ∈ [0, 1]d , then
(cid:107)Φ(cid:107)∞ = O(r · logd ( 1
r ))

We will brieﬂy describe the ideas of the proofs of these two lemmas in the Appendix. The formal
proofs are given in the supplementary ﬁle.
Proof of Theorem 6 First of all, since we focus on binary classiﬁcation, DI S (B (h∗ , r)) can be
written equivalently as
DI S (B (h∗ , r)) = {x ∈ X , ∃h ∈ B (h∗ , r), s.t. h(x) (cid:54)= h∗ (x)}.
Consider any h ∈ B (h∗ , r). Let fh , fh∗ ∈ F K
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) .
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
(cid:90) fh (x1 ,...,xd )
(cid:90)
C be the corresponding classiﬁcation boundaries of h
and h∗ respectively. If r is sufﬁciently small, we must have
fh∗ (x1 ,...,xd )

ρ(h, h∗ ) = Pr
X∼DX

p(x1 , . . . , xd+1 )dxd+1

(h(X ) (cid:54)= h∗ (X )) =

dx1 . . . dxd

[0,1]d

5

Denote

(cid:90) fh (x1 ,...,xd )
Φh (x1 , . . . , xd ) =
p(x1 , . . . , xd+1 )dxd+1 .
fh∗ (x1 ,...,xd )
We assert that there is a K th order smooth function ˜Φh (x1 , . . . , xd ) and two constants 0 < u ≤ v
(cid:82) fh (x1 ,...,xd )
such that u| ˜Φh | ≤ |Φh | ≤ v | ˜Φh |. To see this, remember that fh and fh∗ are K th order smooth
functions; and the density p is upper and lower bounded by constants times a K th order smooth
(cid:82) |Φh | = ρ(h, h∗ ) ≤ r . Because this holds for all h ∈ B (h∗ , r),
function g(x1 , . . . , xd+1 ); and note that ˜Φh (x1 , . . . , xd ) =
fh∗ (x1 ,...,xd ) g(x1 , . . . , xd+1 )dxd+1 is
a K th order smooth function. The latter is easy to check by taking derivatives. By Lemma 8, we have
(cid:107)Φh(cid:107)∞ = O(r · ( 1
r ) d
K+d ), since
we have suph∈B (h∗ ,r) (cid:107)Φh(cid:107)∞ = O(r · ( 1
r ) d
K+d ).
Now consider the region of disagreement of B (h∗ , r). Clearly DI S (B (h∗ , r)) = ∪h∈B (h∗ ,r) {x :
x ∈ ∪h∈B (h∗ ,r) {x : h(x) (cid:54)= h∗ (x)}(cid:162)
(cid:161)
h(x) (cid:54)= h∗ (x)}. Hence
(cid:195)
(cid:33)
(cid:182) d
(cid:181)
(cid:90)
(x ∈ DI S (B (h∗ , r))) = Pr
Pr
X∼DX
X∼DX
1
r ·
(cid:107)Φh(cid:107)∞dx1 . . . dxd = O
≤ 2
K+d
.
r
[0,1]d

sup
h∈B (h∗ ,r)

3.3 Label Complexity

The theorem follows by the deﬁnition of θ().
Theorem 7 can be proved similarly by using Lemma 9.
(cid:181)
(cid:182)
(cid:181)
(cid:181)
It was shown in [Han07] that the label complexity of A2 is
1
1
ν 2
2 + 1
ln
θ2
(5)
O
poly log
,
δ

where ν = minh∈H erD (h). When  ≥ ν , our previous results on the disagreement coefﬁcient
already imply polynomial or exponential improvements for A2 . However, when  < ν , the label
2 ), the same as passive learning whatever θ is. In fact, without any as-
complexity becomes O( 1
2 ) result is inevitable due to the Ω( ν 2
sumption on the noise, the O( 1
2 ) lower bound of agnostic
active learning [Kaa06].

(cid:182)

(cid:182)

Recently, there has been considerable interest in how noise affects the learning rate. A remarkable
notion is due to Tsybakov [Tsy04], which was ﬁrst introduced for passive learning. Let η(x) =
P (Y = 1|X = x). Tsybakov’s noise condition assumes that for some c > 0, 0 < α ≤ ∞
(|η(X ) − 1/2| ≤ t) ≤ ct−α ,
Pr
(6)
X∼DX
for all 0 < t ≤ t0 , where t0 is some constant. (6) implies a connection between the pseudo distance
ρ(h, h∗ ) and the excess risk erD (h) − erD (h∗ ):
ρ(h, h∗ ) ≤ c(cid:48) (erD (h) − erD (h∗ ))1/κ ,
(7)
α ≥ 1 is called the noise
where h∗ is the Bayes classi ﬁer, c(cid:48) is some ﬁnite constant. Here κ = 1+α
exponent. κ = 1 is the optimal case, where the problem has bounded noise; κ > 1 correspond to
unbounded noise.

Castro and Nowak [CN07] noticed that Tsybakov’s noise condition is also important in active learn-
ing. They proved label complexity bounds in terms of κ for the membership-query setting. A notable
 ) 2κ−2
fact is that ˜O(( 1
κ ) (κ > 1) is both an upper and a lower bound for membership-query in the
minimax sense. It is important to point out that the lower bound automatically applies to pool-based
model, since pool makes weaker assumptions than membership-query. Hence for large κ, active
learning has very limited improvement over passive learning whatever other factors are.

Recently, Hanneke [Han09] obtained similar label complexity for pool-based model. He showed the
labels requested by A2 is O(θ2 ln 1
 ln 1
δ ) for the bounded noise case, i.e. κ = 1. Here we slightly

6

(8)

(9)

(cid:182)

.

) ≤ c1 e−c2 T ,

generalize Hanneke’s result to unbounded noise by introducing the following noise condition. We
assume there exist c1 , c2 > 0 and T0 > 0 such that
(|η(X ) − 1/2| ≤ 1
Pr
X∼DX
(cid:181)
T
for all T ≥ T0 . It is not difﬁcult to show that (8) implies
(er(h) − er(h∗ )) ln
ρ(h, h∗ ) = O

1
(er(h) − er(h∗ ))
This condition assumes unbounded noise. Under this noise condition, A2 has a better label com-
plexity.
Theorem 10 Assume that the learning problem satisﬁes the noise condition (8) and DX has a den-
sity upper bounded by a constant M . For any hypothesis space H that has a O(n−1/2 ) uniform
convergence bound, if the Bayes classiﬁer h∗ is in H, then with probability at least 1 − δ , A2 outputs
ˆh ∈ H with erD (ˆh) ≤ erD (h∗ ) + , and the number of labels requested by the algorithm is at most
O(θ2 () · ln 1
δ · poly log( 1
 )).
Proof As the proof of [Han07], one can show that with probability 1 − δ we never remove h∗ from
Vi , and for any h, h(cid:48) ∈ Vi we must have ∆(Vi )(eri (h) − eri (h(cid:48) )) = erD (h) − erD (h(cid:48) ), where
eri (h) is the error rate of h conditioned on DI S (Vi ). These guarantees erD (ˆh) ≤ erD (h∗ ) + .
If ∆(Vi ) ≤ 2θ(), due to the O(n−1/2 ) uniform convergence bound, O(θ2 () ln 1
δ ) labels sufﬁces
to make ∆(Vi )(U B (Si , h, δ (cid:48) ) − LB (Si , h, δ (cid:48) )) ≤  for all h ∈ DI S (Vi ) and the algorithm stops.
 ) times ∆(Vi ) <
Hence we next consider ∆(Vi ) > 2θ(). Note that there are at most O(ln 1
2 ∆(Vjk ) occurs. So below we bound the number of labels needed to make ∆(Vi ) < 1
2 ∆(Vjk )
1
occurs. By the deﬁnition of θ(), if ρ(h, h∗ ) ≤ ∆(Vjk )
for all h ∈ Vi , then ∆(Vi ) < 1
2 ∆(Vjk ). Let
2θ()
γ (h) = erD (h) − erD (h∗ ). By the noise assumption (9) we have that if
∆(Vjk )
1
≤ c
2θ() ,
γ (h)
2 ∆(Vjk ). Here and below, c is appropriate constant but may be different from
then ∆(Vi ) < 1
, and in turn if γ (h) ≤ c ∆(Vjk )
line to line. Note that (10) holds if γ (h) ≤ c ∆(Vjk )
since
θ() ln 1
θ() ln θ()

)
∆(Vjk
∆(Vjk ) ≥ ∆(Vi ) > 2θ(). But to have the last inequality, the algorithm only needs to label
 ln 1
O(θ2 () ln2 1
δ ) instances from DI S (Vi ). So the total number of labels requested by A2 is
O(θ2 () ln 1
δ ln3 1
 )
Now we give our main label complexity bounds for agnostic active learning.
Theorem 11 Let the instance space be [0, 1]d+1 . Let the Hypothesis space be HK
C , where K > d.
Assume that the Bayes classiﬁer h∗ of the learning problem is in HK
C ; the noise condition (8) holds;
(cid:180)
(cid:179)(cid:161)
and DX has a density bounded by a K th order smooth function as in Theorem 6. Then the A2
(cid:161)
(cid:162) 2d
(cid:162)
algorithm outputs ˆh with error rate erD (ˆh) ≤ erD (h∗ ) +  and the number of labels requested is at
, where in ˜O we hide the poly log
most ˜O
K+d ln 1
1
1
term.

δ

Proof Note that the density DX is upper bounded by a smooth function implies that it is also upper
bounded by a constant M . Combining Theorem 5, 6 and 10 the theorem follows.

γ (h) ln

(10)

Combining Theorem 5, 7 and 10 we can show the following theorem.
Theorem 12 Let the instance space be [0, 1]d+1 . Let the Hypothesis space be H∞
C . Assume that
the Bayes classiﬁer h∗ of the learning problem is in H∞
C ; the noise condition (8) holds; and DX
(cid:161)
(cid:161)
(cid:162)
(cid:162)
has a density bounded by an inﬁnitely smooth function as in Theorem 7. Then the A2 algorithm
outputs ˆh with error rate erD (ˆh) ≤ erD (h∗ ) +  and the number of labels requested is at most
ln 1
1
.
O
poly log

δ

7

4 Conclusion

We show that if the Bayesian classiﬁcation boundary is smooth and the distribution is bounded by a
smooth function, then under some noise condition active learning achieves polynomial or exponen-
tial improvement in the label complexity than passive supervised learning according to whether the
smoothness is of ﬁnite order or inﬁnite.

Although we assume that the classiﬁcation boundary is the graph of a function, our results can be
generalized to the case that the boundaries are a ﬁnite number of functions. To be precise, consider
N functions f1 (x) ≤ · · · ≤ fN (x), for all x ∈ [0, 1]d . Let f0 (x) ≡ 0, fN +1 (x) ≡ 1. The positive (or
negative) set deﬁned by these functions is {(x, xd+1 ) : f2i (x) ≤ x ≤ f2i+1 (x), i = 0, 1, . . . , N
2 }.
Our theorems still hold in this case. In addition, by techniques in [Dud99] (page 259), our results
may generalize to problems which have intrinsic smooth boundaries (not only graphs of functions).

Appendix

f (x) =

In this appendix we describe very brie ﬂy the ideas to prove Lemma 8 and Lemma 9. The formal
proofs can be found in the supplementary ﬁle.
(cid:82) 1
Ideas to Prove Lemma 8 First consider the d = 1 case. Note that if f ∈ F K
C , then |f (K−1) (x) −
f (K−1) (x(cid:48) )| ≤ C |x − x(cid:48) | for all x, x(cid:48) ∈ [0, 1]. It is not difﬁcult to see that we only need to show for
(cid:82) |f | = r , the derivatives
0 |f (x)|dx = r , then (cid:107)f (cid:107)∞ = O(r
any f such that |f (K−1) (x)−f (K−1) (x(cid:48) )| ≤ C |x−x(cid:48) |, if
K
K+1 ).
 C
To show this, note that in order that (cid:107)f (cid:107)∞ achieves the maximum while
of f must be as large as possible. Indeed, it can be shown that (one of) the optimal f is of the form
K ! |x − ξ |K
0 ≤ x ≤ ξ ,
ξ < x ≤ 1.
(cid:82) 1
0
That is, |f (K−1) (x) − f (K−1) (x(cid:48) )| = C |x − x(cid:48) | (i.e. the K − 1 order derivatives reaches the upper
bound of the Lipschitz constant.) for all x, x(cid:48) ∈ [0, ξ ], where ξ is determined by
0 f (x)dx = r . It
is then easy to check that (cid:107)f (cid:107)∞ = O(r
K
K+1 ).
For the general d > 1 case, we relax the constraint. Note that all K − 1th order partial derivatives
 C
are Lipschitz implies that all K − 1th order directional derivatives are Lipschitz too. Under the latter
constraint, (one of) the optimal f has the form
0 ≤ (cid:107)x(cid:107) ≤ ξ ,
K ! |(cid:107)x(cid:107) − ξ |K
(cid:82)
f (x) =
ξ < (cid:107)x(cid:107).
0
[0,1]d |f (x)|dx = r . This implies (cid:107)f (cid:107)∞ = O(r
(cid:82)
where ξ is determined by
Ideas to Prove Lemma 9 Similar to the proof of Lemma 8, we only need to show that for any
[0,1]d |f (x)|dx = r , then (cid:107)f (cid:107)∞ = O(r · logd ( 1
f ∈ F ∞
r )).
C , if
Since f is inﬁnitely smooth, we can choose K large and depending on r . For the d = 1 case, let
(cid:82) 1
K + 1 = log 1
. We know that the optimal f is of the form of Eq.(11). (Actually this choice of K
r
log log 1
r
is approximately the largest K such that Eq.(11) is still the optimal form. If K is larger than this, ξ
0 |f (x)| = r , we have ξK+1 = (K+1)!
. Now, (cid:107)f (cid:107)∞ = C
will be out of [0, 1].) Since
K ! ξK . Note
C
log log 1
r . By Stirling’s formula we can show (cid:107)f (cid:107)∞ = O(r · log 1
r
log 1
that ( 1
r = log 1
r )K+1 = ( 1
r )
r ).
. By similar arguments we can show (cid:107)f (cid:107)∞ = O(r · logd 1
For the d > 1 case, let K + d = log 1
r ).
r
log log 1
r

(11)

K
K+d ).

Acknowledgement

This work was supported by NSFC(60775005).

8

References

[CN07]

[Das05]

[Dud99]
[Fri09]

[Tsy04]

In 20th Annual Conference on

[Ang88]
D. Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988.
[BAL06] M.-F. Balcan, A.Beygelzimer, and J. Langford. Agnostic active learning. In 23th International
Conference on Machine Learning, 2006.
R. Castro and R. Nowak. Minimax bounds for active learning.
Learning Theory, 2007.
S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Infor-
mation Processing Systems, 2005.
[DHM07] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Advances
in Neural Information Processing Systems, 2007.
R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.
E. Friedman. Active learning for smooth problems.
In 22th Annual Conference on Learning
Theory, 2009.
[GKW03] V.E. Gine, V.I. Koltchinskii, and J. Wellner. Ratio limit theorems for empirical processes. Stochas-
tic Inequalities and Applications, 56:249–278, 2003.
S. Hanneke. A bound on the label complexity of agnostic active learning. In 24th International
Conference on Machine Learning, 2007.
S. Hanneke. Adaptive rates of convergence in active learning.
Learning Theory, 2009.
[Kaa06] M. Kaariainen. Active learning in the non-realizable case. In 17th International Conference on
Algorithmic Learning Theory, 2006.
A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning.
32:135–166, 2004.
S. van de Geer. Applications of Empirical Process Theory. Cambridge University Press, 2000.
[vdG00]
[vdVW96] A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes with Application to
Statistics. Springer Verlag, 1996.

In 22th Annual Conference on

[Han07]

[Han09]

The Annals of Statistics,

9


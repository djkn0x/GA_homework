Robust Value Function Approximation Using
Bilinear Programming

Marek Petrik
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
petrik@cs.umass.edu

Shlomo Zilberstein
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
shlomo@cs.umass.edu

Abstract

Existing value function approximation methods have been successfully used in
many applications, but they often lack useful a priori error bounds. We propose
approximate bilinear programming, a new formulation of value function approxi-
mation that provides strong a priori guarantees. In particular, this approach prov-
ably ﬁnds an approximate value function that minimizes the Bellman residual.
Solving a bilinear program optimally is NP-hard, but this is unavoidable because
the Bellman-residual minimization itself is NP-hard. We therefore employ and
analyze a common approximate algorithm for bilinear programs. The analysis
shows that this algorithm offers a convergent generalization of approximate pol-
icy iteration. Finally, we demonstrate that the proposed approach can consistently
minimize the Bellman residual on a simple benchmark problem.

1 Motivation

Solving large Markov Decision Problems (MDPs) is a very useful, but computationally challenging
problem addressed widely in the AI literature, particularly in the area of reinforcement learning.
It is widely accepted that large MDPs can only be solved approximately. The commonly used ap-
proximation methods can be divided into three broad categories: 1) policy search, which explores
a restricted space of all policies, 2) approximate dynamic programming, which searches a restricted
space of value functions, and 3) approximate linear programming, which approximates the solu-
tion using a linear program. While all of these methods have achieved impressive results in many
domains, they have signiﬁcant limitations.
Policy search methods rely on local search in a restricted policy space. The policy may be repre-
sented, for example, as a ﬁnite-state controller [22] or as a greedy policy with respect to an approx-
imate value function [24]. Policy search methods have achieved impressive results in such domains
as Tetris [24] and helicopter control [1]. However, they are notoriously hard to analyze. We are not
aware of any theoretical guarantees regarding the quality of the solution.
Approximate dynamic programming (ADP) methods iteratively approximate the value func-
tion [4, 20, 23]. They have been extensively analyzed and are the most commonly used methods.
However, ADP methods typically do not converge and they only provide weak guarantees of approx-
imation quality. The approximation error bounds are usually expressed in terms of the worst-case
approximation of the value function over all policies [4]. In addition, most available bounds are with
respect to the L∞ norm, while the algorithms often minimize the L2 norm. While there exist some
L2 -based bounds [14], they require values that are difﬁcult to obtain.
Approximate linear programming (ALP) uses a linear program to compute the approximate value
function in a particular vector space [7]. ALP has been previously used in a wide variety of set-
tings [2, 9, 10]. Although ALP often does not perform as well as ADP, there have been some recent

1

efforts to close the gap [18]. ALP has better theoretical properties than ADP and policy search. It is
guaranteed to converge and return the closest L1 -norm approximation ˜v of the optimal value func-
tion v∗ up to a multiplicative factor. However, the L1 norm must be properly weighted to guarantee
a small policy loss, and there is no reliable method for selecting appropriate weights [7].
To summarize, the existing reinforcement learning techniques often provide good solutions, but typ-
ically require signiﬁcant domain knowledge [20]. The domain knowledge is needed partly because
useful a priori error bounds are not available, as mentioned above. Our goal is to develop a more
robust method that is guaranteed to minimize an actual bound on the policy loss.
We present a new formulation of value function approximation that provably minimizes a bound
on the policy loss. Unlike in some other algorithms, the bound in this case does not rely on values
that are hard to obtain. The new method uniﬁes policy search and value-function search methods
to minimize the L∞ norm of the Bellman residual, which bounds the policy loss. We start with a
description of the framework and notation in Section 2. Then, in Section 3, we describe the pro-
posed Approximate Bilinear Programming (ABP) formulation. A drawback of this formulation is
its computational complexity, which may be exponential. We show in Section 4 that this is unavoid-
able, because minimizing the approximation error bound is in fact NP-hard. Although our focus is
on the formulation and its properties, we also discuss some simple algorithms for solving bilinear
programs. Section 5 shows that ABP can be seen as an improvement of ALP and Approximate Pol-
icy Iteration (API). Section 6 demonstrates the applicability of ABP using a common reinforcement
learning benchmark problem. A complete discussion of sampling strategies–an essential compo-
nent for achieving robustness–is beyond the scope of this paper, but the issue is brieﬂy discussed in
Section 6. Complete proofs of the theorems can be found in [19].

2 Solving MDPs using ALP

In this section, we formally deﬁne MDPs, their ALP formulation, and the approximation errors
involved. These notions serve as a basis for developing the ABP formulation.
A Markov Decision Process is a tuple (S , A, P, r, α), where S is the ﬁnite set of states, A is the
ﬁnite set of actions. P : S × S × A (cid:55)→ [0, 1] is the transition function, where P (s(cid:48) , s, a) represents
the probability of transiting to state s(cid:48) from state s, given action a. The function r : S × A (cid:55)→ R is
the reward function, and α : S (cid:55)→ [0, 1] is the initial state distribution. The objective is to maximize
the inﬁnite-horizon discounted cumulative reward. To shorten the notation, we assume an arbitrary
ordering of the states: s1 , s2 , . . . , sn . Then, Pa and ra are used to denote the probabilistic transition
for all s ∈ S , (cid:80)
matrix and reward for action a.
The solution of an MDP is a policy π : S × A → [0, 1] from a set of possible policies Π, such that
a∈A π(s, a) = 1. We assume that the policies may be stochastic, but stationary [21].
A policy is deterministic when π(s, a) ∈ {0, 1} for all s ∈ S and a ∈ A. The transition and reward
functions for a given policy are denoted by Pπ and rπ . The value function update for a policy π is
denoted by Lπ , and the Bellman operator is denoted by L. That is:

Lv = max
Lπ v = Pπ v + rπ
Lπ v .
π∈Π
The optimal value function, denoted v∗ , satisﬁes v∗ = Lv∗ . We focus on linear value function
approximation for discounted inﬁnite-horizon problems. In linear value function approximation, the
value function is represented as a linear combination of nonlinear basis functions (vectors). For
each state s, we deﬁne a row-vector φ(s) of features. The rows of the basis matrix M correspond
to φ(s), and the approximation space is generated by the columns of the matrix. That is, the basis
− φ(s1 ) −
 v = M x.
matrix M , and the value function v are represented as:
− φ(s2 ) −
...
Deﬁnition 1. A value function, v , is representable if v ∈ M ⊆ R|S | , where M = colspan (M ),
and is transitive-feasible when v ≥ Lv . We denote the set of transitive-feasible value functions as:
K = {v ∈ R|S | v ≥ Lv}.

M =

2

(1)

Notice that the optimal value function v∗ is transitive-feasible, and M is a linear space. Also, all the
inequalities are element-wise.
Because the new formulation is related to ALP, we introduce it ﬁrst. It is well known that an inﬁ-
(cid:88)
nite horizon discounted MDP problem may be formulated in terms of solving the following linear
program:
(cid:88)
c(s)v(s)
minimize
v
s∈S
v(s) − γ
P (s(cid:48) , s, a)v(s(cid:48) ) ≥ r(s, a) ∀(s, a) ∈ (S , A)
s.t.
c represents a distribution over the states, usually a uniform one. That is, (cid:80)
s(cid:48)∈S
We use A as a shorthand notation for the constraint matrix and b for the right-hand side. The value
s∈S c(s) = 1. The
linear program in Eq. (1) is often too large to be solved precisely, so it is approximated to get an
approximate linear program by assuming that v ∈ M [8], as follows:
cT v
minimize
Av ≥ b
x
s.t.
v ∈ M
The constraint v ∈ M denotes the approximation. To actually solve this linear program, the
value function is represented as v = M x. In the remainder of the paper, we assume that 1 ∈ M to
guarantee the feasibility of the ALP, where 1 is a vector of all ones. The optimal solution of the ALP,
˜v , satisﬁes that ˜v ≥ v∗ . Then, the objective of Eq. (2) represents the minimization of (cid:107)˜v − v∗ (cid:107)1,c ,
where (cid:107) · (cid:107)1,c is a c-weighted L1 norm [7].
The ultimate goal of the optimization is not to obtain a good value function ˜v , but a good policy.
The quality of the policy, typically chosen to be greedy with respect to ˜v , depends non-trivially on
the approximate value function. The ABP formulation will minimize policy loss by minimizing
(cid:107)L˜v − ˜v(cid:107)∞ , which bounds the policy loss as follows.
Theorem 2 (e.g. [25]). Let ˜v be an arbitrary value function, and let ˆv be the value of the greedy
policy with respect to ˜v . Then:

(2)

(cid:107)v∗ − ˆv(cid:107)∞ ≤ 2
(cid:107)L˜v − ˜v(cid:107)∞ ,
1 − γ
In addition, if ˜v ≥ L˜v , the policy loss is smallest for the greedy policy.
Policies, like value functions, can be represented as vectors. Assume an arbitrary ordering of the
state-action pairs, such that o(s, a) (cid:55)→ N maps a state and an action to its position. The policies are
represented as θ ∈ R|S |×|A| , and we use the shorthand notation θ(s, a) = θ(o(s, a)).
Remark 3. The corresponding π and θ are denoted as πθ and θπ and satisfy:
πθ (s, a) = θπ (s, a).

We will also consider approximations of the policies in the policy-space, generated by columns of a
matrix N . A policy is representable when π ∈ N , where N = colspan (N ).

3 Approximate Bilinear Programs
This section shows how to formulate minv∈M (cid:107)Lv − v(cid:107)∞ as a separable bilinear program. Bilinear
programs are a generalization of linear programs with an additional bilinear term in the objective
function. A separable bilinear program consists of two linear programs with independent constraints
and are fairly easy to solve and analyze.
Deﬁnition 4 (Separable Bilinear Program). A separable bilinear program in the normal form is
deﬁned as follows:
minimize
w,x y ,z
s.t.

2 y + sT
1 x + xTC y + rT
1 w + rT
f (w, x, y , z ) = sT
2 z
A1x + B1w = b1 A2 y + B2 z = b2
w, x ≥ 0
y , z ≥ 0

(3)

3

We separate the variables using a vertical line and the constraints using different columns to em-
phasize the separable nature of the bilinear program.

In this paper, we only use separable bilinear programs and refer to them simply as bilinear programs.
An approximate bilinear program can now be formulated as follows.
θTλ + λ(cid:48)
minimize
θ λ,λ(cid:48) ,v
B θ = 1 z = Av − b
s.t.
θ ≥ 0
z ≥ 0
λ + λ(cid:48)1 ≥ z
λ ≥ 0
θ ∈ N
v ∈ M
All variables are vectors except λ(cid:48) , which is a scalar.
The symbol z is only used to simplify
the notation and does not need to represent an optimization variable. The variable v is deﬁned for
each state and represents the value function. Matrix A represents constraints that are identical to the
constraints in Eq. (2). The variables λ correspond to all state-action pairs. These variables represent
(cid:88)
the Bellman residuals that are being minimized. The variables θ are deﬁned for all state-action pairs
and represent policies in Remark 3. The matrix B represents the following constraints:
θ(s, a) = 1 ∀s ∈ S .
a∈A

(4)

As with approximate linear programs, we initially assume that all the constraints on z are used. In
realistic settings, however, the constraints would be sampled or somehow reduced. We defer the
discussion of this issue until Section 6. Note that the constraints in our formulation correspond to
elements of z and θ . Thus when constraints are omitted, also the corresponding elements of z and θ
are omitted.
To simplify the notation, the value function approximation in this problem is denoted only implicitly
by v ∈ M, and the policy approximation is denoted by θ ∈ N . In an actual implementation, the
optimization variables would be x, y using the relationships v = M x and θ = N y . We do not
assume any approximation of the policy space, unless mentioned otherwise. We also use v or θ
to refer to partial solutions of Eq.
(4) with the other variables chosen appropriately to achieve
feasibility.
The ABP formulation is closely related to approximate linear programs, and we discuss the connec-
tion in Section 5. We ﬁrst analyze the properties of the optimal solutions of the bilinear program
and then show and discuss the solution methods in Section 4. The following theorem states the main
property of the bilinear formulation.
Theorem 5. b Let ( ˜θ , ˜v , ˜λ, ˜λ(cid:48) ) be an optimal solution of Eq. (4) and assume that 1 ∈ M. Then:
v∈M (cid:107)Lv − v(cid:107)∞ ≤ 2(1 + γ ) min
v∈K∩M (cid:107)Lv − v(cid:107)∞ ≤ 2 min
˜θT ˜λ + ˜λ(cid:48) = (cid:107)L˜v − ˜v(cid:107)∞ ≤ min
v∈M (cid:107)v − v∗ (cid:107)∞ .
In addition, π ˜θ minimizes the Bellman residual with regard to ˜v , and its value function ˆv satisﬁes:
v∈M (cid:107)Lv − v(cid:107)∞ .
(cid:107)ˆv − v∗ (cid:107)∞ ≤ 2
1 − γ
min

The proof of the theorem can be found in [19]. It is important to note that, as Theorem 5 states, the
ABP approach is equivalent to a minimization over all representable value functions, not only the
transitive-feasible ones. Notice also the missing coefﬁcient 2 (2 instead of 4) in the last equation
of Theorem 5. This follows by subtracting a constant vector 1 from ˜v to balance the lower bounds
on the Bellman residual error with the upper ones. This modiﬁed approximate value function will
have 1/2 of the original Bellman residual but an identical greedy policy. Finally, note that whenever
v∗ ∈ M, both ABP and ALP will return the optimal value function.
The ABP solution minimizes the L∞ norm of the Bellman residual due to: 1) the correspondence
between θ and the policies, and 2) the dual representation with respect to variables λ and λ(cid:48) . The
theorem then follows using techniques similar to those used for approximate linear programs [7].

4

Algorithm 1: Iterative algorithm for solving Eq. (3)
(x0 , w0 ) ← random ;
(y0 , z0 ) ← arg miny ,z f (w0 , x0 , y , z ) ;
i ← 1 ;
while yi−1 (cid:54)= yi or xi−1 (cid:54)= xi do
(yi , zi ) ← arg min{y ,z A2 y+B2 z=b2 y ,z≥0} f (wi−1 , xi−1 , y , z ) ;
(xi , wi ) ← arg min{x,w A1 x+B1w=b1 x,w≥0} f (w, x, yi , zi ) ;
i ← i + 1
return f (wi , xi , yi , zi )

4 Solving Bilinear Programs

In this section we describe simple methods for solving ABPs. We ﬁrst describe optimal methods,
which have exponential complexity, and then discuss some approximation strategies.
Solving a bilinear program is an NP-complete problem [3]. The membership in NP follows from
the ﬁnite number of basic feasible solutions of the individual linear programs, each of which can be
checked in polynomial time. The NP-hardness is shown by a reduction from the SAT problem [3].
The NP-completeness of ABP compares unfavorably with the polynomial complexity of ALP. How-
ever, most other ADP algorithms are not guaranteed to converge to a solution in ﬁnite time.
The following theorem shows that the computational complexity of the ABP formulation is asymp-
totically the same as the complexity of the problem it solves.
Theorem 6. b Determining minv∈K∩M (cid:107)Lv − v(cid:107)∞ <  is NP-complete for the full constraint
representation, 0 < γ < 1, and a given  > 0. In addition, the problem remains NP-complete when
1 ∈ M, and therefore minv∈M (cid:107)Lv − v(cid:107)∞ <  is also NP-complete.

As the theorem states, the value function approximation does not become computationally simpler
even when 1 ∈ M – a universal assumption in the paper. Notice that ALP can determine whether
minv∈K∩M (cid:107)Lv − v(cid:107)∞ = 0 in polynomial time.
The proof of Theorem 6 is based on a reduction from SAT and can be found in [19]. The policy in the
reduction determines the true literal in each clause, and the approximate value function corresponds
to the truth value of the literals. The approximation basis forces literals that share the same variable
to have consistent values.
Bilinear programs are non-convex and are typically solved using global optimization techniques.
The common solution methods are based on concave cuts [11] or branch-and-bound [6]. In ABP
settings with a small number of features, the successive approximation algorithm [17] may be ap-
plied efﬁciently. We are, however, not aware of commercial solvers available for solving bilinear
programs. Bilinear programs can be formulated as concave quadratic minimization problems [11], or
mixed integer linear programs [11, 16], for which there are numerous commercial solvers available.
Because we are interested in solving very large bilinear programs, we describe simple approximate
algorithms next. Optimal scalable methods are beyond the scope of this paper.
The most common approximate method for solving bilinear programs is shown in Algorithm 1.
It is designed for the general formulation shown in Eq.
(3), where f (w, x, y , z ) represents the
objective function. The minimizations in the algorithm are linear programs which can be easily
solved.
Interestingly, as we will show in Section 5, Algorithm 1 applied to ABP generalizes a
version of API.
While Algorithm 1 is not guaranteed to ﬁnd an optimal solution, its empirical performance is often
remarkably good [13]. Its basic properties are summarized by the following proposition.

Proposition 7 (e.g. [3]). Algorithm 1 is guaranteed to converge, assuming that the linear program
solutions are in a vertex of the optimality simplex. In addition, the global optimum is a ﬁxed point
of the algorithm, and the objective value monotonically improves during execution.

5

The proof is based on the ﬁnite count of the basic feasible solutions of the individual linear programs.
Because the objective function does not increase in any iteration, the algorithm will eventually con-
verge.
In the context of MDPs, Algorithm 1 can be further reﬁned. For example, the constraint v ∈ M in
Eq. (4) serves mostly to simplify the bilinear program and a value function that violates it may still
be acceptable. The following proposition motivates the construction of a new value function from
two transitive-feasible value functions.
Proposition 8. Let ˜v1 and ˜v2 be feasible value functions in Eq.
(4). Then the value function
˜v(s) = min{˜v1 (s), ˜v2 (s)} is also feasible in Eq.
(4). Therefore ˜v ≥ v∗ and (cid:107)v∗ − ˜v(cid:107)∞ ≤
min {(cid:107)v∗ − ˜v1(cid:107)∞ , (cid:107)v∗ − ˜v2 (cid:107)∞ }.

The proof of the proposition is based on Jensen’s inequality and can be found in [19].
Proposition 8 can be used to extend Algorithm 1 when solving ABPs. One option is to take the
state-wise minimum of values from multiple random executions of Algorithm 1, which preserves
the transitive feasibility of the value function. However, the increasing number of value functions
used to obtain ˜v also increases the potential sampling error.

5 Relationship to ALP and API

In this section, we describe the important connections between ABP and the two closely related ADP
methods: ALP, and API with L∞ minimization. Both of these methods are commonly used, for
example to solve factored MDPs [10]. Our analysis sheds light on some of their observed properties
and leads to a new convergent form of API.
ABP addresses some important issues with ALP: 1) ALP provides value function bounds with re-
spect to L1 norm, which does not guarantee small policy loss, 2) ALP’s solution quality depends
signiﬁcantly on the heuristically-chosen objective function c in Eq. (2) [7], and 3) incomplete con-
straint samples in ALP easily lead to unbounded linear programs. The drawback of using ABP,
however, is the higher computational complexity.
Both the ﬁrst and the second issues in ALP can be addressed by choosing the right objective func-
tion [7]. Because this objective function depends on the optimal ALP solution, it cannot be practi-
cally computed. Instead, various heuristics are usually used. The heuristic objective functions may
lead to signiﬁcant improvements in speciﬁc domains, but they do not provide any guarantees. ABP,
on the other hand, has no such parameters that require adjustments.
The third issue arises when the constraints of an ALP need to be sampled in some large domains.
The ALP may become unbounded with incomplete samples because its objective value is deﬁned
using the L1 norm on the states, and the constraints are deﬁned using the L∞ norm of the Bellman
residual. In ABP, the Bellman residual is used in both the constraints and objective function. The
objective function of ABP is then bounded below by 0 for an arbitrarily small number of samples.
ABP can also improve on API with L∞ minimization (L∞ -API for short), which is a leading method
for solving factored MDPs [10]. Minimizing the L∞ approximation error is theoretically preferable,
since it is compatible with the existing bounds on policy loss [10]. In contrast, few practical bounds
exist for API with the L2 norm minimization [14], such as LSPI [12].
L∞ -API is shown in Algorithm 2, where f (π) is calculated using the following program:

minimize
φ,v
s.t.

φ
(I − γPπ )v + 1φ ≥ rπ
−(I − γPπ )v + 1φ ≥ −rπ
v ∈ M

(5)

Here I denotes the identity matrix. We are not aware of a convergence or a divergence proof of
L∞ -API, and this analysis is beyond the scope of this paper.

6

Algorithm 2: Approximate policy iteration, where f (π) denotes a custom value function approxi-
mation for the policy π .
π0 , k ← rand, 1 ;
πk (s) ← arg maxa∈A r(s, a) + γ (cid:80)
while πk (cid:54)= πk−1 do
˜vk ← f (πk−1 ) ;
s(cid:48)∈S P (s(cid:48) , s, a)˜vk (s) ∀s ∈ S ;
k ← k + 1

We propose Optimistic Approximate Policy Iteration (OAPI), a modiﬁcation of API. OAPI is shown
in Algorithm 2, where f (π) is calculated using the following program:
φ
minimize
φ,v
Av ≥ b
(≡ (I − γPπ )v ≥ rπ ∀π ∈ Π)
s.t.
−(I − γPπ )v + 1φ ≥ −rπ
v ∈ M
In fact, OAPI corresponds to Algorithm 1 applied to ABP because Eq. (6) corresponds to Eq. (4)
with ﬁxed θ . Then, using Proposition 7, we get the following corollary.
Corollary 9. Optimistic approximate policy iteration converges in ﬁnite time. In addition, the Bell-
man residual of the generated value functions monotonically decreases.

(6)

OAPI differs from L∞ -API in two ways: 1) OAPI constrains the Bellman residuals by 0 from below
and by φ from above, and then it minimizes φ. L∞ -API constrains the Bellman residuals by φ from
both above and below. 2) OAPI, like API, uses only the current policy for the upper bound on the
Bellman residual, but uses all the policies for the lower bound on the Bellman residual.
L∞ -API cannot return an approximate value function that has a lower Bellman residual than ABP,
given the optimality of ABP described in Theorem 5. However, even OAPI, an approximate ABP
algorithm, performs comparably to L∞ -API, as the following theorem states.
Theorem 10. b Assume that L∞ -API converges to a policy π and a value function v that both
satisfy: φ = (cid:107)v − Lπ v(cid:107)∞ = (cid:107)v − Lv(cid:107)∞ . Then ˜v = v + φ
1−γ 1 is feasible in Eq. (4), and it is a ﬁxed
point of OAPI. In addition, the greedy policies with respect to ˜v and v are identical.

The proof is based on two facts. First, ˜v is feasible with respect to the constraints in Eq. (4). The
Bellman residual changes for all the policies identically, since a constant vector is added. Second,
because Lπ is greedy with respect to ˜v , we have that ˜v ≥ Lπ ˜v ≥ L˜v . The value function ˜v is
therefore transitive-feasible. The full proof can be found in [19].
To summarize, OAPI guarantees convergence, while matching the performance of L∞ -API. The
convergence of OAPI is achieved because given a non-negative Bellman residual, the greedy policy
also minimizes the Bellman residual. Because OAPI ensures that the Bellman residual is always
non-negative, it can progressively reduce it. In comparison, the greedy policy in L∞ -API does not
minimize the Bellman residual, and therefore L∞ -API does not always reduce it. Theorem 10 also
explains why API provides better solutions than ALP, as observed in [10]. From the discussion
above, ALP can be seen as an L1 -norm approximation of a single iteration of OAPI. L∞ -API, on
the other hand, performs many such ALP-like iterations.

6 Empirical Evaluation

As we showed in Theorem 10, even OAPI, the very simple approximate algorithm for ABP, can
perform as well as existing state-of-the art methods on factored MDPs. However, a deeper under-
standing of the formulation and potential solution methods will be necessary in order to determine
the full practical impact of the proposed methods.
In this section, we validate the approach by
applying it to the mountain car problem, a simple reinforcement learning benchmark problem.
We have so far considered that all the constraints involving z are present in the ABP in Eq. (4).
Because the constraints correspond to all state-action pairs, it is often impractical to even enumerate

7

(a) L∞ error of the Bellman residual
144
100
Features
0.13 (0.1)
0.21 (0.23)
OAPI
3.6 (4.3)
13. (13.)
ALP
LSPI
9. (14.)
3.9 (7.7)
0.86 (1.18)
0.46 (0.08)
API

(b) L2 error of the Bellman residual
144
100
Features
0.1 (1.9)
0.2 (0.3)
OAPI
0.3 (0.4)
9.5 (18.)
ALP
LSPI
1.2 (1.5)
0.9 (0.1)
0.08 (0.08)
0.04 (0.01)
API

Table 1: Bellman residual of the ﬁnal value function. The values are averages over 5 executions,
with the standard deviations shown in parentheses.

them. This issue can be addressed in at least two ways. First, a small randomly-selected subset of the
constraints can be used in the ABP, a common approach in ALP [9, 5]. The ALP sampling bounds
can be easily extended to ABP. Second, the structure of the MDP can be used to reduce the number
of constraints. Such a reduction is possible, for example, in factored MDPs with L∞ -API and
ALP [10], and can be easily extended to OAPI and ABP.
In the mountain-car benchmark, an underpowered car needs to climb a hill [23]. To do so, it ﬁrst
needs to back up to an opposite hill to gain sufﬁcient momentum. The car receives a reward of 1
when it climbs the hill. In the experiments we used a discount factor γ = 0.99.
The experiments are designed to determine whether OAPI reliably minimizes the Bellman resid-
ual in comparison with API and ALP. We use a uniformly-spaced linear spline to approximate the
value function. The constraints were based on 200 uniformly sampled states with all 3 actions per
state. We evaluated the methods with the number of the approximation features 100 and 144, which
corresponds to the number of linear segments.
The results of ABP (in particular OAPI), ALP, API with L2 minimization, and LSPI are depicted
in Table 1. The results are shown for both L∞ norm and uniformly-weighted L2 norm. The run-
times of all these methods are comparable, with ALP being the fastest. Since API (LSPI) is not
guaranteed to converge, we ran it for at most 20 iterations, which was an upper bound on the number
of iterations of OAPI. The results demonstrate that ABP minimizes the L∞ Bellman residual much
more consistently than the other methods. Note, however, that all the considered algorithms would
perform signiﬁcantly better given a ﬁner approximation.

7 Conclusion and Future Work

We proposed and analyzed approximate bilinear programming, a new value-function approximation
method, which provably minimizes the L∞ Bellman residual. ABP returns the optimal approximate
value function with respect to the Bellman residual bounds, despite the formulation with regard to
transitive-feasible value functions. We also showed that there is no asymptotically simpler formula-
tion, since ﬁnding the closest value function and solving a bilinear program are both NP-complete
problems. Finally, the formulation leads to the development of OAPI, a new convergent form of API
which monotonically improves the objective value function.
While we only discussed approximate solutions of the ABP, a deeper study of bilinear solvers may
render optimal solution methods feasible. ABPs have a small number of essential variables (that
determine the value function) and a large number of constraints, which can be leveraged by the
solvers [15]. The L∞ error bound provides good theoretical guarantees, but it may be too conserva-
tive in practice. A similar formulation based on L2 norm minimization may be more practical.
We believe that the proposed formulation will help to deepen the understanding of value function
approximation and the characteristics of existing solution methods, and potentially lead to the de-
velopment of more robust and widely-applicable reinforcement learning algorithms.

Acknowledgements

This work was supported by the Air Force Ofﬁce of Scientiﬁc Research under Grant No. FA9550-
08-1-0171. We also thank the anonymous reviewers for their useful comments.

8

References
[1] Pieter Abbeel, Varun Ganapathi, and Andrew Y. Ng. Learning vehicular dynamics, with appli-
cation to modeling helicopters. In Advances in Neural Information Processing Systems, pages
1–8, 2006.
[2] Daniel Adelman. A price-directed approach to stochastic inventory/routing. Operations Re-
search, 52:499–514, 2004.
[3] Kristin P. Bennett and O. L. Mangasarian. Bilinear separation of two sets in n-space. Technical
report, Computer Science Department, University of Wisconsin, 1992.
[4] Dimitri P. Bertsekas and Sergey Ioffe. Temporal differences-based policy iteration and appli-
cations in neuro-dynamic programming. Technical Report LIDS-P-2349, LIDS, 1997.
[5] Guiuseppe Calaﬁore and M.C. Campi. Uncertain convex programs: Randomized solutions and
conﬁdence levels. Mathematical Programming, Series A, 102:25–46, 2005.
[6] Alberto Carpara and Michele Monaci. Bidimensional packing by bilinear programming. Math-
ematical Programming Series A, 118:75–108, 2009.
[7] Daniela P. de Farias. The Linear Programming Approach to Approximate Dynamic Program-
ming: Theory and Application. PhD thesis, Stanford University, 2002.
[8] Daniela P. de Farias and Ben Van Roy. The linear programming approach to approximate
dynamic programming. Operations Research, 51:850–856, 2003.
[9] Daniela Pucci de Farias and Benjamin Van Roy. On constraint sampling in the linear program-
ming approach to approximate dynamic programming. Mathematics of Operations Research,
29(3):462–478, 2004.
[10] Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efﬁcient solution
algorithms for factored MDPs. Journal of Artiﬁcial Intelligence Research, 19:399–468, 2003.
[11] Reiner Horst and Hoang Tuy. Global optimization: Deterministic approaches. Springer, 1996.
[12] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine
Learning Research, 4:1107–1149, 2003.
[13] O. L. Mangasarian. The linear complementarity problem as a separable bilinear program.
Journal of Global Optimization, 12:1–7, 1995.
[14] Remi Munos. Error bounds for approximate policy iteration. In International Conference on
Machine Learning, pages 560–567, 2003.
[15] Marek Petrik and Shlomo Zilberstein. Anytime coordination using separable bilinear pro-
grams. In Conference on Artiﬁcial Intelligence, pages 750–755, 2007.
[16] Marek Petrik and Shlomo Zilberstein. Average reward decentralized Markov decision pro-
cesses. In International Joint Conference on Artiﬁcial Intelligence, pages 1997–2002, 2007.
[17] Marek Petrik and Shlomo Zilberstein. A bilinear programming approach for multiagent plan-
ning. Journal of Artiﬁcial Intelligence Research, 35:235–274, 2009.
[18] Marek Petrik and Shlomo Zilberstein. Constraint relaxation in approximate linear programs.
In International Conference on Machine Learning, pages 809–816, 2009.
[19] Marek Petrik and Shlomo Zilberstein. Robust value function approximation using bilinear pro-
gramming. Technical Report UM-CS-2009-052, Department of Computer Science, University
of Massachusetts Amherst, 2009.
[20] Warren B. Powell. Approximate Dynamic Programming. Wiley-Interscience, 2007.
[21] Martin L. Puterman. Markov decision processes: Discrete stochastic dynamic programming.
John Wiley & Sons, Inc., 2005.
[22] Kenneth O. Stanley and Risto Miikkulainen. Competitive coevolution through evolutionary
complexiﬁcation. Journal of Artiﬁcial Intelligence Research, 21:63–100, 2004.
[23] Richard S. Sutton and Andrew Barto. Reinforcement learning. MIT Press, 1998.
[24] Istvan Szita and Andras Lorincz. Learning Tetris using the noisy cross-entropy method. Neural
Computation, 18(12):2936–2941, 2006.
[25] Ronald J. Williams and Leemon C. Baird. Tight performance bounds on greedy policies based
on imperfect value functions. In Yale Workshop on Adaptive and Learning Systems, 1994.

9


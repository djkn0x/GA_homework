Variational Gaussian-process factor analysis for
modeling spatio-temporal data

Jaakko Luttinen
Adaptive Informatics Research Center
Helsinki University of Technology, Finland
Jaakko.Luttinen@tkk.fi

Alexander Ilin
Adaptive Informatics Research Center
Helsinki University of Technology, Finland
Alexander.Ilin@tkk.fi

Abstract

We present a probabilistic factor analysis model which can be used for studying
spatio-temporal datasets. The spatial and temporal structure is modeled by using
Gaussian process priors both for the loading matrix and the factors. The posterior
distributions are approximated using the variational Bayesian framework. High
computational cost of Gaussian process modeling is reduced by using sparse ap-
proximations. The model is used to compute the reconstructions of the global
sea surface temperatures from a historical dataset. The results suggest that the
proposed model can outperform the state-of-the-art reconstruction systems.

1 Introduction

Factor analysis and principal component analysis (PCA) are widely used linear techniques for ﬁnd-
ing dominant patterns in multivariate datasets. These methods ﬁnd the most prominent correlations
in the data and therefore they facilitate studies of the observed system. The found principal pat-
terns can also give an insight into the observed data variability. In many applications, the quality of
this kind of modeling can be signiﬁcantly improved if extra k nowledge about the data structure is
used. For example, taking into account the temporal information typically leads to more accurate
modeling of time series.

In this work, we present a factor analysis model which makes use of both temporal and spatial
information for a set of collected data. The method is based on the standard factor analysis model

w:dxT
d: + noise ,

(1)

Y = WX + noise =

D
Xd=1
where Y is a matrix of spatio-temporal data in which each row contains measurements in one spatial
location and each column corresponds to one time instance. Here and in the following, we denote
by ai: and a:i the i-th row and column of a matrix A, respectively (both are column vectors). Thus,
each xd: represents the time series of one of the D factors whereas w:d is a vector of loadings which
are spatially distributed. The matrix Y can contain missing values and the samples can be unevenly
distributed in space and time.1
We assume that both the factors xd: and the corresponding loadings w:d have prominent structures.
We describe them by using Gaussian processes (GPs) which is a ﬂexible and theoretically solid
tool for smoothing and interpolating non-uniform data [8]. Using separate GP models for xd: and
w:d facilitates analysis of large spatio-temporal datasets. The application of the GP methodology
to modeling data Y directly could be unfeasible in real-world problems because the computational

1 In practical applications, it may be desirable to diminish the effect of uneven sampling over space or time
by, for example, using proper weights for different data points.

1

complexity of inference scales cubically w.r.t. the number of data points. The advantage of the
proposed approach is that we perform GP modeling only either in the spatial or temporal domain at
a time. Thus, the dimensionality can be remarkably reduced and modeling large datasets becomes
feasible. Also, good interpretability of the model makes it easy to explore the results in the spatial
and temporal domain and to set priors re ﬂecting our modeling assumptions. The proposed model is
symmetrical w.r.t. space and time.

Our model bears similarities with the latent variable models presented in [13, 16]. There, GPs were
used to describe the factors and the mixing matrix was point-estimated. Therefore the observations
Y were modeled with a GP. In contrast to that, our model is not a GP model for the observations
because the marginal distribution of Y is not Gaussian. This makes the posterior distribution of
the unknown parameters intractable. Therefore we use an approximation based on the variational
Bayesian methodology. We also show how to use sparse variational approximations to reduce the
computational load. Models which use GP priors for both W and X in (1) have recently been pro-
posed in [10, 11]. The function factorization model in [10] is learned using a Markov chain Monte
Carlo sampling procedure, which may be computationally infeasible for large-scale datasets. The
nonnegative matrix factorization model in [11] uses point-estimates for the unknown parameters,
thus ignoring posterior uncertainties. In our method, we take into account posterior uncertainties,
which helps reduce over ﬁtting and facilitates learning a mo re accurate model.

In the experimental part, we use the model to compute reconstruction of missing values in a real-
world spatio-temporal dataset. We use a historical sea surface temperature dataset which contains
monthly anomalies in the 1856-1991 period to reconstruct the global sea surface temperatures. The
same dataset was used in designing the state-of-the-art reconstruction methodology [5]. We show the
advantages of the proposed method as a Bayesian technique which can incorporate all assumptions
in one model and which uses all available data. Since reconstruction of missing values can be an
important application for the method, we give all the formulas assuming missing values in the data
matrix Y.

2 Factor analysis model with Gaussian process priors

We use the factor analysis model (1) in which Y has dimensionality M × N and the number of
factors D is much smaller than the number of spatial locations M and the number of time instances
N . The m-th row of Y corresponds to a spatial location lm (e.g., a location on a two-dimensional
map) and the n-th column corresponds to a time instance tn .
We assume that each time signal xd: contains values of a latent function χ(t) computed at time
instances tn . We use independent Gaussian process priors to describe each signal xd: :
D
Yd=1
where X: denotes a long vector formed by concatenating the columns of X, Kd is the part of the
large covariance matrix Kx which corresponds to the d-th row of X and N (a |b, C ) denotes the
Gaussian probability density function for variable a with mean b and covariance C. The ij -th
element of Kd is computed using the covariance function ψd with the kernel hyperparameters θd .
The priors for W are de ﬁned similarly assuming that each spatial pattern w:d contains measurements
of a function ω (l) at different spatial locations lm :

[Kd ]ij = ψd (ti , tj ; θd ) ,

p(X) = N (X: |0, Kx ) =

N (xd: |0, Kd ) ,

(2)

p(W) =

N (w:d |0, Kw
d ) ,

[Kw
d ]ij = ϕd (li , lj ; φd ) ,

D
Yd=1
where ϕd is a covariance function with hyperparameters φd . Any valid (positive semide ﬁnite) ker-
nels can be used to de ﬁne the covariance functions ψd and ϕd . A good list of possible covariance
functions is given in [8]. The prior model reduces to the one used in probabilistic PCA [14] when
Kd = I and a uniform prior is used for W.
The noise term in (1) is modeled with a Gaussian distribution, resulting in a likelihood function
p(Y|W, X, σ) = Ymn∈O
x:n , σ2
N (cid:0)ymn (cid:12)(cid:12)
mn (cid:1) ,
2

wT
m:

(3)

(4)

where the product is evaluated over the observed elements in Y whose indices are included in the set
O. We will refer to the model (1)–(4) as GPFA. In practice, the n oise level can be assumed spatially
(σmn = σm ) or temporally (σmn = σn ) varying. One can also use spatially and temporally varying
noise level σ2
mn if this variability can be estimated somehow.
There are two main difﬁculties which should be addressed whe n learning the model: 1) The posterior
p(W, X|Y) is intractable and 2) the computational load for dealing with GPs can be too large for
real-world datasets. We use the variational Bayesian framework to cope with the ﬁrst difﬁculty and
we also adopt the variational approach when computing sparse approximations for the GP posterior.

3 Learning algorithm

In the variational Bayesian framework, the true posterior is approximated using some restricted class
of possible distributions. An approximate distribution which factorizes as

p(W, X|Y) ≈ q(W, X) = q(W)q(X) .

is typically used for factor analysis models. The approximation q(W, X) can be found by minimiz-
ing the Kullback-Leibler divergence from the true posterior. This optimization is equivalent to the
maximization of the lower bound of the marginal log-likelihood:
log p(Y) ≥ Z q(W)q(X) log
Free-form maximization of (5) w.r.t. q(X) yields that

p(Y|W, X)p(W)p(X)
q(W)q(X)

dWdX .

(5)

q(X) ∝ p(X) exphlog p(Y|W,X)iq(W) ,

(6)

(7)

σ−2
mn hwm: iymn .

where h·i refers to the expectation over the approximate posterior distribution q . Omitting the deriva-
tions here, this boils down to the following update rule:
q(X) = N (cid:16)X: (cid:12)(cid:12)(cid:12)(cid:0)K−1
x + U(cid:1)−1 (cid:17) ,
x + U(cid:1)−1
Z: , (cid:0)K−1
where Z: is a DN × 1 vector formed by concatenation of vectors
z:n = Xm∈On
The summation in (7) is over a set On of indices m for which ymn is observed. Matrix U in (6) is a
DN × DN block-diagonal matrix with the following D × D matrices on the diagonal:
Un = Xm∈On
mn (cid:10)wm:wT
σ−2
m: (cid:11) ,
Note that the form of the approximate posterior (6) is similar to the regular GP regression: One
n .
z:n as noisy observations with the corresponding noise covariance matrices U−1
can interpret U−1
n
Then, q(X) in (6) is simply the posterior distribution of the latent functions values χd (tn ).
The optimal q(W) can be computed using formulas symmetrical to (6)–(8) in whi ch X and W are
appropriately exchanged. The variational EM algorithm for learning the model consists of alternate
updates of q(W) and q(X) until convergence. The noise level can be estimated by using a point
estimate or adding a factor factor q(σmn ) to the approximate posterior distribution. For example,
the update rules for the case of isotropic noise σ2
mn = σ2 are given in [2].

n = 1, . . . , N .

(8)

3.1 Component-wise factorization

In practice, one may need to factorize further the posterior approximation in order to reduce the
computational burden. This can be done in two ways: by neglecting the posterior correlations be-
tween different factors xd: (and between spatial patterns w:d , respectively) or by neglecting the
posterior correlations between different time instances x:n (and between spatial locations wm: , re-
spectively). We suggest to use the ﬁrst way which is computat ionally more expensive but allows to

3

Method
GP on Y
GPFA
GPFA
GPFA

Update rule

Approximation

Complexity
O(N 3M 3 )
O(D3N 3 + D3M 3 )
O(DN 3 + DM 3 )
d N + PD
O(PD
d=1 M 2
d=1 N 2
d M )
Table 1: The computational complexity of different algorithms

q(X: )
q(xd: )
q(xd: ), inducing inputs

(6)
(9)
(12)

capture stronger posterior correlations. This yields a posterior approximation q(X) = QD
d=1 q(xd: )
which can be updated as follows:
q(xd: ) = N (cid:16)xd: (cid:12)(cid:12)(cid:12)(cid:0)K−1
d + Vd (cid:1)−1 (cid:17) ,
d + Vd (cid:1)−1
cd , (cid:0)K−1
d = 1, . . . , D ,
where cd is an N × 1 vector whose n-th component is
mn hwmd i(cid:18)ymn − Xj 6=d
hwmj ihxjn i(cid:19)
[cd ]n = Xm∈On
σ−2
and Vd is an N ×N diagonal matrix whose n-th diagonal element is [Vd ]nn = Pm∈On
σ−2
mn (cid:10)w2
md (cid:11) .
The main difference to (6) is that each component is ﬁtted to t he residuals of the reconstruction based
on the rest of the components. The computational complexity is now reduced compared to (9), as
shown in Table 1.

(10)

(9)

The component-wise factorization may provide a meaningful representation of data because the
model is biased in favor of solutions with dynamically and spatially decoupled components. When
the factors are modeled using rather general covariance functions, the proposed method is somewhat
related to the blind source separation techniques using time structure (e.g., [1]). The advantage here
is that the method can handle more sophisticated temporal correlations and it is easily applicable to
incomplete data. In addition, one can use the method in semi-blind settings when prior knowledge
is used to extract components with speciﬁc types of temporal or spatial features [9]. This problem
can be addressed using the proposed technique with properly chosen covariance functions.

3.2 Variational learning of sparse GP approximations

One of the main issues with Gaussian processes is the high computational cost with respect to the
number of observations. Although the variational learning of the GPFA model works only in either
spatial or temporal domain at a time, the size of the data may still be too large in practice. A common
way to reduce the computational cost is to use sparse approximations [7]. In this work, we follow
the variational formulation of sparse approximations presented in [15].

The main idea is to introduce a set of auxiliary variables {w, x} which contain the values of the
m |m = 1, . . . , Md}, {t = τ d
latent functions ωd (l), χd (t) in some locations {l = λd
n |n = 1, . . . , Nd}
called inducing inputs. Assuming that the auxiliary variables {w, x} summarize the data well, it
holds that p(W, X|w, x, Y) ≈ p(W, X|w, x) , which suggests a convenient form of the approxi-
mate posterior:

(11)
q(W, X, w, x) = p(W|w)p(X|x)q(w)q(x) ,
where p(W|w), p(X|x) can be easily computed from the GP priors. Optimal q(w), q(x) can be
computed by maximizing the variational lower bound of the marginal log-likelihood similar to (5).

Free-form maximization w.r.t. q(x) yields the following update rule:
x (cid:1)−1
KxxUKxxK−1
x + K−1
KxxZ: , Σ (cid:1) , Σ = (cid:0)K−1
ΣK−1
q(x) = N (cid:0)x (cid:12)(cid:12)
x
x
where x is the vector of concatenated auxiliary variables for all factors, Kx is the GP prior co-
variance matrix of x and Kxx is the covariance between x and X: . This equation can be seen as a
replacement of (6). A similar formula is applicable to the update of q(w). The advantage here is that
the number of inducing inputs is smaller than then the number of data samples, that is, Md < M and
Nd < N , and therefore the required computational load can be reduced (see more details in [15]).
Eq. (12) can be quite easily adapted to the component-wise factorization of the posterior in order to
reduce the computational load of (9). See the summary for the computational complexity in Table 1.

(12)

,

4

3.3 Update of GP hyperparameters

The hyperparameters of the GP priors can be updated quite similarly to the standard GP regression
by maximizing the lower bound of the marginal log-likelihood. Omitting the derivations here, this
lower bound for the temporal covariance functions {ψd (t)}D
d=1 equals (up to a constant) to

N
trh
UnDi ,
Xn=1
log N (cid:0)U−1Z: (cid:12)(cid:12)0, U−1 + KxxK−1
Kxx (cid:1) −
x
where U and Z: have the same meaning as in (6) and D is a D × D (diagonal) matrix of variances
of x:n given the auxiliary variables x. The required gradients are shown in the appendix. The
equations without the use of auxiliary variables are similar except that KxxK−1
Kxx = Kx and
x
the second term disappears. A symmetrical equation can be derived for the hyperparameters of the
spatial functions ϕd (t). The extension of (13) to the case of component-wise factorial approximation
is straightforward. The inducing inputs can also be treated as variational parameters and they can be
changed to optimize the lower bound (13).

(13)

1
2

4 Experiments

4.1 Artiﬁcial example

We generated a dataset with M = 30 sensors (two-dimensional spatial locations) and N = 200
time instances using the generative model (1) with a moderate amount of observation noise, as-
suming σmn = σ . D = 4 temporal signals xd: were generated by taking samples from GP priors
with different covariance functions: 1) a squared exponential function to model a slowly changing
component:

(14)

r2
k(r; θ1 ) = exp (cid:18)−
1 (cid:19) ,
2θ2
2) a periodic function with decay to model a quasi-periodic component:
2 sin2 (πr/θ1 )
r2
k(r; θ1 , θ2 , θ3 ) = exp (cid:18)−
3 (cid:19) ,
2θ2
θ2
2
where r = |tj − ti |, and 3) a compactly supported piecewise polynomial function to model two fast
changing components with different timescales:
1
(1 − r)b+2 (cid:0)(b2 + 4b + 3)r2 + (3b + 6)r + 3(cid:1) ,
3
where r = min(1, |tj − ti |/θ1) and b = 3 for one-dimensional inputs with the hyperparameter
θ1 de ﬁning a threshold such that k(r) = 0 for |tj − ti | ≥ θ1 . The loadings were generated from
GPs over the two-dimensional space using the squared exponential covariance function (14) with an
additional scale parameter θ2 :

k(r; θ1 ) =

−

(15)

(16)

2 exp (cid:0)−r2/(2θ2
k(r; θ1 , θ2 ) = θ2
1 )(cid:1) .
We randomly selected 452 data points from Y as being observed, thus most of the generated data
points were marked as missing (see Fig. 1a for examples). We also removed observations from all
the sensors for a relatively long time interval. Note a resulting gap in the data marked with vertical
lines in Fig. 1a. The hyperparameters of the Gaussian processes were initialized randomly close
to the values used for data generation, assuming that a good guess about the hidden signals can be
obtained by exploratory analysis of data.

(17)

Fig. 1b shows the components recovered by GPFA using the update rule (6). Note that the algo-
rithm separated the four signals with the different variability timescales. The posterior predictive
distributions of the missing values presented in Fig. 1a show that the method was able to capture
temporal correlations on different timescales. Note also that although some of the sensors contain
very few observations, the missing values are reconstructed pretty well. This is a positive effect of
the spatially smooth priors.

5

)
t
(
9
1
y

)
t
(
1
y

)
t
(
0
2
y

)
t
(
5
y

)
t
(
1
x

)
t
(
2
x

)
t
(
3
x

)
t
(
4
x

time, t
(a)

time, t
(b)

Figure 1: Results for the artiﬁcial experiment. (a) Posteri or predictive distribution for four randomly
selected locations with the observations shown as crosses, the gap with no training observations
marked with vertical lines and some test values shown as circles. (b) The posteriors of the four
latent signals xd: . In both ﬁgures, the solid lines show the posterior mean and g ray color shows two
standard deviations.

4.2 Reconstruction of global SST using the MOHSST5 dataset

We demonstrate how the presented model can be used to reconstruct global sea surface temperatures
(SST) from historical measurements. We use the U.K. Meteorological Ofﬁce historical SST data set
(MOHSST5) [6] that contain monthly SST anomalies in the 1856-1991 period for 5◦ × 5◦ longitude-
latitude bins. The dataset contains in total approximately 1600 time instances and 1700 spatial
locations. The dataset is sparse, especially during the 19th century and the World Wars, having 55%
of the values missing, and thus, consisting of more than 106 observations in total.
We used the proposed algorithm to estimate D = 80 components, the same number was used in [5].
We withdrew 20% of the data from the training set and used this part for testing the reconstruction
accuracy. We used ﬁve time signals xd: with the squared exponential function (14) to describe cli-
mate trends. Another ﬁve temporal components were modeled w ith the quasi-periodic covariance
function (15) to capture periodic signals (e.g. related to the annual cycle). We also used ﬁve compo-
nents with the squared exponential function to model prominent interannual phenomena such as El
Ni ˜no. Finally we used the piecewise polynomial functions t o describe the rest 65 time signals xd: .
These dimensionalities were chosen ad hoc. The covariance function for each spatial pattern w:d
was the scaled squared exponential (17). The distance r between the locations li and lj was mea-
sured on the surface of the Earth using the spherical law of cosines. The use of the extra parameter
θ2 in (17) allowed automatic pruning of unnecessary factors, which happens when θ2 = 0.
We used the component-wise factorial approximation of the posterior described in Section 3.1. We
also introduced 500 inducing inputs for each spatial function ωd (l) in order to use sparse variational
approximations. Similar sparse approximations were used for the 15 temporal functions χ(t) which
modeled slow climate variability: the slowest, quasi-periodic and interannual components had 80,
300 and 300 inducing inputs, respectively. The inducing inputs were initialized by taking a random
subset from the original inputs and then kept ﬁxed throughou t learning because their optimization
would have increased the computational burden substantially. For the rest of the temporal phenom-
ena, we used the piecewise polynomial functions (16) that produce priors with a sparse covariance
matrix and therefore allow efﬁcient computations.

The dataset was preprocessed by weighting the data points by the square root of the corresponding
latitudes in order to diminish the effect of denser sampling in the polar regions, then the same noise
level was assumed for all measurements (σmn = σ). Preprocessing by weighting data points ymn
with weights sm is essentially equivalent to assuming spatially varying noise level σmn = σ/sm .
The GP hyperparameters were initialized taking into account the assumed smoothness of the spa-
tial patterns and the variability timescale of the temporal factors. The factors X were initialized

6

 

 

−0.5

0

0.5

−0.5

0

0.5

1875

1900

 

 

−0.5

0

0.5

−0.5

0

0.5

 

 

 

 

 

 

−0.5

0

0.5

1

−0.5

0

0.5

 

−1

1925

1950

1975

 

 

−0.5

0

0.5

1

−0.5

0

0.5

 

−1

 

 

1875

1900

1925

1950

1975

Figure 2: Experimental results for the MOHSST5 dataset. The spatial and temporal patterns of the
four most dominating principal components for GPFA (above) and VBPCA (below). The solid lines
and gray color in the time series show the mean and two standard deviations of the posterior distri-
bution. The uncertainties of the spatial patterns are not shown, and we saturated the visualizations
of the VBPCA spatial components to reduce the effect of the uncertain pole regions.

randomly by sampling from the prior and the weights W were initialized to zero. The variational
EM-algorithm of GPFA was run for 200 iterations. We also applied the variational Bayesian PCA
(VBPCA) [2] to the same dataset for comparison. VBPCA was initialized randomly as the initial-
ization did not have much effect on the VBPCA results. Finally, we rotated the GPFA components
such that the orthogonal basis in the factor analysis subspace was ordered according to the amount of
explained data variance (where the variance was computed by averaging over time). Thus, “GPFA
principal components” are mixtures of the original factors found by the algorithm. This was done
for comparison with the most prominent patterns found with VBPCA.

Fig. 2 shows the spatial and temporal patterns of the four most dominant principal components for
both models. The GPFA principal components and the corresponding spatial patterns are generally
smoother, especially in the data-sparse regions, for example, in the period before 1875. The ﬁrst and
the second principal components of GPFA as well as the ﬁrst an d the third components of VBPCA
are related to El Ni ˜no. We should make a note here that the rot ation within the principal subspace
may be affected by noise and therefore the components may not be directly comparable. Another
observation was that the model efﬁciently used only some of t he 15 slow components: about three
very slow and two interannual components had relatively large weights in the loading matrix W.
Therefore the selected number of slow components did not affect the results signiﬁcantly. None

7

of the periodic components had large weights, which suggests that the fourth VBPCA component
might contain artifacts.

Finally, we compared the two models by computing a weighted root mean square reconstruction
error on the test set, similarly to [4]. The prediction errors were 0.5714 for GPFA and 0.6180
for VBPCA. The improvement obtained by GPFA can be considered quite signiﬁcant taking into
account the substantial amount of noise in the data.

5 Conclusions and discussion

In this work, we proposed a factor analysis model which can be used for modeling spatio-temporal
datasets. The model is based on using GP priors for both spatial patterns and time signals corre-
sponding to the hidden factors. The method can be seen as a combination of temporal smoothing,
empirical orthogonal functions (EOF) analysis and kriging. The latter two methods are popular in
geostatistics (see, e.g., [3]). We presented a learning algorithm that can be applicable to relatively
large datasets.

The proposed model was applied to the problem of reconstruction of historical global sea surface
temperatures. The current state-of-the-art reconstruction methods [5] are based on the reduced space
(i.e. EOF) analysis with smoothness assumptions for the spatial and temporal patterns. That ap-
proach is close to probabilistic PCA [14] with ﬁtting a simpl e auto-regressive model to the posterior
means of the hidden factors. Our GPFA model is based on probabilistic formulation of essentially
the same modeling assumptions. The gained advantage is that GPFA takes into account the uncer-
tainty about the unknown parameters, it can use all available data and it can combine all modeling
assumptions in one estimation procedure. The reconstruction results obtained with GPFA are very
promising and they suggest that the proposed model might be able to improve the existing SST
reconstructions. The improvement is possible because the method is able to model temporal and
spatial phenomena on different scales by using properly selected GPs.

A The gradients for the updates of GP hyperparameters

(or inducing input) θ of any covariance

The gradient of the ﬁrst term of (13) w.r.t. a hyperparameter
function is given by
1
∂ θ i − trhUKxxA−1 ∂Kxx
1
bT ∂Kx
b + bT ∂Kxx
∂Kx
∂ θ i + −
trh(cid:0)K−1
x − A−1 (cid:1)
2
2
∂ θ
∂ θ
where A = Kx + KxxUKxx , b = A−1KxxZ: . This part is similar to the gradient reported in
[12]. Without the sparse approximation, it holds that Kx = Kx = Kxx = Kxx and the equation
simpliﬁes to the regular gradient in GP regression for proje cted observations U−1Z: with the noise
covariance U−1 . The second part of (13) results in the extra terms
U(cid:19) + tr (cid:18) ∂Kx
KxxU(cid:19) .
x (cid:19) − 2 tr (cid:18) ∂Kxx
tr (cid:18) ∂Kx
KxxUKxxK−1
∂ θ
∂ θ
∂ θ
The terms in (18) cancel out when the sparse approximation is not used. Both parts of the gra-
dient can be efﬁciently evaluated using the Cholesky decomp osition. The positivity constraints of
the hyperparameters can be taken into account by optimizing with respect to the logarithms of the
hyperparameters.

(Z: − UKxxb)

K−1
x

K−1
x

(18)

Acknowledgments

This work was supported in part by the Academy of Finland under the Centers for Excellence in Research
program and Alexander Ilin’s postdoctoral research project. We would like to thank Alexey Kaplan for fruitful
discussions and providing his expertise on the problem of sea surface temperature reconstruction.

References

[1] A. Belouchrani, K. A. Meraim, J.-F. Cardoso, and E. Moulines. A blind source separation technique based
on second order statistics. IEEE Transactions on Signal Processing, 45(2):434–444, 1997.

8

[2] C. M. Bishop. Variational principal components. In Proceedings of the 9th International Conference on
Arti ﬁcial Neural Networks (ICANN’99) , pages 509–514, 1999.
[3] N. Cressie. Statistics for Spatial Data. Wiley-Interscience, New York, 1993.
[4] A. Ilin and A. Kaplan. Bayesian PCA for reconstruction of historical sea surface temperatures. In Pro-
ceedings of the International Joint Conference on Neural Networks (IJCNN 2009), pages 1322–1327,
Atlanta, USA, June 2009.
[5] A. Kaplan, M. Cane, Y. Kushnir, A. Clement, M. Blumenthal, and B. Rajagopalan. Analysis of global sea
surface temperatures 1856–1991.
Journal of Geophysical Research, 103:18567–18589, 1998.
[6] D. E. Parker, P. D. Jones, C. K. Folland, and A. Bevan. Interdecadal changes of surface temperature since
the late nineteenth century. Journal of Geophysical Research, 99:14373–14399, 1994.
[7] J. Qui ˜nonero-Candela and C. E. Rasmussen. A unifying vi ew of sparse approximate Gaussian process
regression. Journal of Machine Learning Research, 6:1939–1959, Dec. 2005.
[8] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
[9] J. S ¨arel ¨a and H. Valpola. Denoising source separation. Journal of Machine Learning Research, 6:233–
272, 2005.
[10] M. N. Schmidt. Function factorization using warped Gaussian processes. In L. Bottou and M. Littman,
editors, Proceedings of the 26th International Conference on Machine Learning (ICML’09), pages 921–
928, Montreal, June 2009. Omnipress.
[11] M. N. Schmidt and H. Laurberg. Nonnegative matrix factorization with Gaussian process priors. Compu-
tational Intelligence and Neuroscience, 2008:1–10, 2008.
[12] M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse Gaussian pro-
cess regression. In Proceedings of the 9th International Workshop on Arti ﬁcial
Intelligence and Statistics
(AISTATS’03), pages 205–213, 2003.
[13] Y. W. Teh, M. Seeger, and M. I. Jordan. Semiparametric latent factor models. In Proceedings of the 10th
International Workshop on Arti ﬁcial Intelligence and Stat istics (AISTATS’05), pages 333–340, 2005.
[14] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal
Statistical Society Series B, 61(3):611–622, 1999.
[15] M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of
the 12th International Workshop on Arti ﬁcial Intelligence and Statistics (AISTATS’09), pages 567–574,
2009.
[16] B. M. Yu, J. P. Cunningham, G. Santhanam, S. I. Ryu, K. V. Shenoy, and M. Sahani. Gaussian-process
factor analysis for low-dimensional single-trial analysis of neural population activity.
In Advances in
Neural Information Processing Systems 21, pages 1881–1888. 2009.

9


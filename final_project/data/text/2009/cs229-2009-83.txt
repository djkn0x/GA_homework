Online Learning for URL classiﬁcation

Onkar Dalal
496 Lomita Mall, Palo Alto, CA 94306 USA

Devdatta Gangal
Yahoo!, 701 First Ave, Sunnyvale, CA 94089 USA

Abstract

This work consists of a review of online al-
gorithms for URL classiﬁcation followed by
some extensions and tweaking of these meth-
ods to make them eﬃcient in terms of com-
putational time and memory. We found out
that the trade-oﬀ in error for these exten-
sions are fairly comparable for some of these
algorithms. We also applied two more kernel
based methods namely Forgetron and Pro jec-
tron.

1. Introduction

In this pro ject, we study the online algorithms for clas-
siﬁcation of the URLs as malicious or benign. We an-
alyze and reproduce the earlier work on this and then
extend the same and analyze the change in error for
these extensions.

1.1 Outline of report

We begin with the previous work on this data in sec-
tion 2. Given the enormous size of the data and num-
ber of features, we have focused on applying feature
selection and Forgetron like techniques to these meth-
ods. In section 3, we give our methods for restricting
the size of feature set for the 4 methods. It also talks
about other kernel-based algorithms: Forgetron and
Pro jectron and our modiﬁcation to Forgetron. The
report ends with conclusion and references.

1.2 Data

The data was taken from the UC Irvine ML Reposi-
tory. It is a multivariate, time-series data consisting
of approximately 2.3 million data points with 3.2 mil-
lion attributes per point. The data is spanned over
120 days and the URLs come with a label +1 or −1
based on whether they are malicious or benign. One of

onkar@stanford.edu

devdatta@gmail.com

the important features of the data is sparseness which
compensates for its large size.

2. Previous Work

This section is reproduction of the work in [4]. We
studied the four algorithms suggested by them and re-
ran them to match their results. This was essential
to understand of the algorithms and further suggest
extensions in section 3.

2.1 Basic Algorithms

We study six algorithms for online classiﬁcations i.e.
a system in which we receive pairs (Xt , yt ) at every
point t, such that Xt is the input feature vector and
yt ∈ {−1, +1} is the actual classiﬁcation of the data
point at time t. However, we do not get the label
yt before we have to classify it using the old data
(X1 , y1 ), (X2 , y2 ), ..., (Xt−1 , yt−1 ), where each Xt is a
feature vector and yt ∈ {−1, +1} is the label. The
label prediction is given by ht (Xt ), which for linear
classiﬁers is ht (X ) = sign(wt • X ), where w is the
weight function.

2.1.1 Online Perceptron

This is a classic linear classiﬁer that updates the
weights in following manner for every misclassiﬁed
sample:

wt+1 ← wt + ytxt

(1)

This is a very simple, fast and memory eﬃcient algo-
rithm but all misclassiﬁcations are treated similarly in
this algorithm.

2.1.2 Logistic Regression with Stochastic
Gradient Descent

The Stochastic Gradient Descent (SGD) is perfectly
suited for online applications. Here SGD is applied to

= wt + γ∆txt

logistic regression to optimize the log-likelihood func-
tion with following update rule for parameters:
wt+1 ← wt + γ

∂Lt
∂w
where Lt is the log-likelihood for sigmoid distribution
2 − σ(wt • xt ) is the diﬀerence between
σ and ∆t = yt+1
the actual and the predicted likelihood that the label
is +1. The training rate γ is kept constant. This
update is similar to the Perceptron, but the training
is proportional to ∆t and the model is updated even
if the prediction is correct.

(2)

2.1.3 Passive-Aggressive (PA) Algorithm

In this algorithm, the following optimization problem
is solved for each sample to minimize the change in
model [1]:

(3)

s.t.

wt+1 ← arg minw ||wt − w||2
yi (w • xt ) ≥ 1
If the prediction is below some conﬁdence level - i.e.,
yt (wt • xt ) ≤ 1, we update the parameters as follows:
wt+1 ← wt + αt ytxt
(4)
where αt = max{ 1−yt (wt •xt )
, 0}. The use of conﬁdence
||xt ||2
measure while classiﬁcation makes this algorithm bet-
ter in practice.

2.1.4 Confidence-Weighted(CW) algorithm

As given in [3], in this algorithm, we maintain the
weights and a conﬁdence measure on them (as opposed
to conﬁdence on classiﬁcation in PA). The weights wi ’s
are modeled as Gaussian random variables with mean
µi and variance Σi . At each step we make minimal
change to the model to classify xt with probability η .
This is equivalent to minimizing the KL divergence be-
tween the Gaussians under the constraint of conﬁdence
measure:
(µt+1 , Σt+1 ) ← arg min(µ,Σ) DKL (N (µ, Σ)||N (µt , Σt ))
yi (µ • xt ) ≥ Φ−1 (η)(cid:112)xT
t Σxt
(5)
where Φ is the c.f.d of standard normal. This corre-
sponds to the following update rule:
µt+1 ← µt + αt ytΣtxt
−1/2
−1 ← Σt
−1 + αt φ u
Σt+1
t
Since every feature here is treated diﬀerently in terms
of conﬁdence, the features with less conﬁdence are up-
dated more aggressively. However, the heavy compu-
tation per update makes this slower than the rest.

diag2 (xt )

s.t.

(6)

2.2 Reworked Results

The error percentage calculated at the end of each
day is plotted for the 4 methods. We see the CW is
the best algorithm among the four. PA is marginally
better than logistic regression. Perceptron being the
most eﬃcient happens to have the maximum error
among the four. The table below gives the value of

Figure 1. Errors for the four algorithms

the error towards the end of the experiment by which
the algorithms have learned the model.

Algorithm

Error

Online Perceptron
1.95-1.99%
Logistic Regression SGD 1.73-1.77%
1.62-1.66%
Passive-Aggresive
Conﬁdence Weighted
1.03-1.07%

3. Extensions

As extensions to the previous work, we have looked
at following algorithms and variations of earlier algo-
rithms in this pro ject:
• Restrict the size of weight vector to consider only
the valuable features and see the changes in errors
for the four algorithms described in section 2
• Forgetron: A kernel based algorithm, which stores
the examples with a ﬁxed budget on number of
support vectors [2]
• Pro jectron: A kernel based algorithm, which
checks the pro jection of new data point onto sup-
port set and update only if it is suﬃciently oﬀ
[5]

3.1 Correct Feature selection (Limited W)

3.1.1 Motivation

Since each of the URL has over 3.3 million features, we
want to store and utilize only the important features.
Our motivation for limiting the memory usage (by lim-
iting size of W) comes from the applications like Web
Search where classiﬁcation needs to be done in minus-
cule times. Another application for quick, memory ef-
ﬁcient online classiﬁcation would be to decide whether
a user is a bot or not.
It is important for the busi-
ness to know which top features are shown by bots in
contrast to real people.

3.1.2 Growth in number of features

We also looked at growth of the feature set for each
of the four algorithms. The results are represented in
Fig 2. As expected we can see that Perceptron quickly
updates all the features as compared to linear growth
in LRSD and very sub-linear growth for PA and CW.

Figure 2. Growth in size of feature set with time

3.1.3 Budgeting the features

For all the four algorithms, we restrict the features se-
lected to a budget B. We implement this budgeting by
choosing only the top B features where the W vector
has changed the most. We run our budgeting after
every update. We observe that by setting the number
of features to a budget B=0.1%-0.2% of the total fea-
tures in the data, the accuracy is still comparable to
otherwise. The results of this experiment for percep-
tron, LRSD and PA are plotted in Fig 3. The results
for CW are plotted in Fig 4.

3.1.4 Observations
• Although Perceptron uses most of the features,
if we restrict the number of features, the error

Figure 3. Error dependence on budget B for Perceptron,
Logistic Regression and Passive Aggressive

change for B = 2000 or 4000 is about 1%.
• Even though PA uses less features ( 200,000) with
a budget of 2000 features, the performance goes
down for PA and LRSD by about 10%.
• Restricting the features in CW decreases the per-
formance of the algorithm by a substantial degree
(20-50%). This could be because CW not only
uses the weight of the feature, but also the conﬁ-
dence in the feature as an important measure of
how important that feature is.

actual Forgetron
Figure 5. Comparison
(dotted)
of
and simpliﬁed Forgetron (solid)
for B=200(blue),
B=400(green), B=1000(red), B=2000(black)

3.3 Pro jectron

Another kernel based algorithm we applied was Pro-
jection.
In this algorithm, whenever a data point is
misclassiﬁed, we check if it lies in span of existing sup-
port set and update the weights. If however it does
not lie in the support set, it gets added to the sup-
port. The details of this algorithm are in [5]. The
main disadvantage of this was the magnitude of cal-
culations required. This was the slowest of the six
algorithms and the errors for linear kernel were worse
than perceptron. The results are given in Fig 6.

Figure 6. Pro jectron vs Perceptron

4. Conclusion

We started with reproducing the results in [4]. As
expected Perceptron is the most eﬃcient, but low on
accuracy and CW even if slower compared to the rest,
has the best accuracy. We observed that algorithms
like Perceptron and LRSG use almost all the features

Figure 4. Error dependence on budget B for Conﬁdence
Weighted

3.2 Forgetron

Perceptron becomes very eﬀective when used along
with Kernels. But the kernel computations are costly
and memory intensive. Forgetron is a kernel based
online-learning algorithm with a budget on the ker-
nel size. Forgetron assigns heavier weight to recent
data points and lighter weight to old data. The exact
algorithm is given in [2]. However, there are few dis-
advantages to the Forgetron.
At every update, Forgetron reduces the weight of the
previous stored examples in the Kernel. Slowly for
large B , after many updates, the weights of the very
old stored example vectors become negligible. This
makes the updates slower and does not add too much
improvement in the errors. Re-weighting process is
expensive We have tried a simpliﬁed version of For-
getron which stores latest B misclassiﬁed examples
and weighs them equally. This runs 2-3 times faster
and requires less memory. And for B ≥ 1000, For-
getron stores the old samples but their weights grad-
ually become negligible. After 10-15 days, oue sim-
pliﬁed version performs better than Forgetron. The
results for these for diﬀerent values of B are given in
Fig 5.

however PA and CW add new features at a sub-linear
rate. To exploit this fact, we restricted our feature
size to a small but signiﬁcant subset (0.1%) of the fea-
tures. We observed that Perceptron works with same
accuracy for restricted weights, however, the errors in
CW increase substantially. In the second half, we ap-
plied Forgetron and Pro jectron for classiﬁcation. We
found that the errors in Forgetron and Pro jectron are
worse than Perceptron. This can be attributed to the
loss of information in both the algorithms. The algo-
rithms also turn out to be slower than the other four
because of the kernel calculations. However, the sim-
pliﬁed Forgetron works very well for moderately high
budgets (B ≥ 1000).

References

[1] K. Crammer, O. Dekel, J. Keshet, S. Shalev-
Shwartz, and Y. Singer. Online passive-aggressive
algorithms. The Journal of Machine Learning Re-
search, 7:585, 2006.

[2] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The
Forgetron: A kernel-based perceptron on a ﬁxed
budget. 2005.

[3] M. Dredze, K. Crammer,
and F. Pereira.
Conﬁdence-weighted linear classiﬁcation. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 264–271. ACM, 2008.

[4] J. Ma, L.K. Saul, S. Savage, and G.M. Voelker.
Identifying suspicious URLs: an application of
large-scale online learning.
In Proceedings of the
26th Annual International Conference on Machine
Learning. ACM New York, NY, USA, 2009.

[5] F. Orabona, J. Keshet, and B. Caputo. The Pro-
jectron: a bounded kernel-based Perceptron.
In
Proceedings of the 25th international conference on
Machine learning, pages 720–727. ACM, 2008.


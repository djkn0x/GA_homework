Semi-Supervised Learning with the Graph Laplacian:
The Limit of In ﬁnite Unlabelled Data

Boaz Nadler
Dept. of Computer Science and Applied Mathematics
Weizmann Institute of Science
Rehovot, Israel 76100
boaz.nadler@weizmann.ac.il

Nathan Srebro
Toyota Technological Institute
Chicago, IL 60637
nati@uchicago.edu

Xueyuan Zhou
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
zhouxy@cs.uchicago.edu

Abstract

We study the behavior of the popular Laplacian Regularization method for Semi-
Supervised Learning at the regime of a ﬁxed number of labeled points but a large
number of unlabeled points. We show that in Rd , d > 2, the method is actually not
well-posed, and as the number of unlabeled points increases the solution degener-
ates to a noninformative function. We also contrast the method with the Laplacian
Eigenvector method, and discuss the “smoothness ” assumpti ons associated with
this alternate method.

1

Introduction and Setup

In this paper we consider the limit behavior of two popular semi-supervised learning (SSL) methods
based on the graph Laplacian: the regularization approach [15] and the spectral approach [3]. We
consider the limit when the number of labeled points is ﬁxed a nd the number of unlabeled points
goes to inﬁnity. This is a natural limit for SSL as the basic SS L scenario is one in which unlabeled
data is virtually inﬁnite. We can also think of this limit as “ perfect ” SSL, having full knowledge
of the marginal density p(x). The premise of SSL is that the marginal density p(x) is informative
about the unknown mapping y(x) we are trying to learn, e.g. since y(x) is expected to be “smooth”
in some sense relative to p(x). Studying the inﬁnite-unlabeled-data limit, where p(x) is fully known,
allows us to formulate and understand the underlying smoothness assumptions of a particular SSL
method, and judge whether it is well-posed and sensible. Understanding the inﬁnite-unlabeled-data
limit is also a necessary ﬁrst step to studying the convergen ce of the ﬁnite-labeled-data estimator.

We consider the following setup: Let p(x) be an unknown smooth density on a compact domain Ω ⊂
Rd with a smooth boundary. Let y : Ω → Y be the unknown function we wish to estimate. In case of
regression Y = R whereas in binary classi ﬁcation Y = {−1, 1}. The standard (transductive) semi-
supervised learning problem is formulated as follows: Given l labeled points, (x1 , y1 ), . . . , (xl , yl ),
with yi = y(xi ), and u unlabeled points xl+1 , . . . , xl+u , with all points xi sampled i.i.d. from p(x),
the goal is to construct an estimate of y(xl+i ) for any unlabeled point xl+i , utilizing both the labeled
and the unlabeled points. We denote the total number of points by n = l + u. We are interested in
the regime where l is ﬁxed and u → ∞.

1

2 SSL with Graph Laplacian Regularization

We ﬁrst consider the following graph-based approach formul ated by Zhu et. al. [15]:
subject to
ˆy(x) = arg min
In (y)
y(xi ) = yi , i = 1, . . . , l
y

(1)

where

(2)

In (y) =

Wi,j (y(xi ) − y(xj ))2

1
n2 Xi,j
is a Laplacian regularization term enforcing “smoothness ” with respect to the n × n similarity matrix
W . This formulation has several natural interpretations in terms of, e.g. random walks and electrical
circuits [15]. These interpretations, however, refer to a ﬁ xed graph, over a ﬁnite set of points with
given similarities.
In contrast, our focus here is on the more typical scenario where the points xi ∈ Rd are a random
sample from a density p(x), and W is constructed based on this sample. We would like to understand
the behavior of the method in terms of the density p(x), particularly in the limit where the number
of unlabeled points grows. Under what assumptions on the target labeling y(x) and on the density
p(x) is the method (1) sensible?
The answer, of course, depends on how the matrix W is constructed. We consider the common
situation where the similarities are obtained by applying some decay ﬁlter to the distances:
(cid:17)
Wi,j = G (cid:16) kxi−xj k
σ
where G : R+ → R+ is some function with an adequately fast decay. Popular choices are the
Gaussian ﬁlter G(z ) = e−z2 /2 or the ǫ-neighborhood graph obtained by the step ﬁlter G(z ) = 1z<1 .
For simplicity, we focus here on the formulation (1) where the solution is required to satisfy the
constraints at the labeled points exactly. In practice, the hard labeling constraints are often replaced
with a softer loss-based data term, which is balanced against the smoothness term In (y), e.g. [14, 6].
Our analysis and conclusions apply to such variants as well.

(3)

Limit of the Laplacian Regularization Term

lim
n→∞

As the number of unlabeled examples grows the regularization term (2) converges to its expectation,
where the summation is replaced by integration w.r.t. the density p(x):
In (y) = I (σ) (y) = ZΩ ZΩ
G (cid:16) kx−x′ k
(cid:17) (y(x) − y(x′ ))2 p(x)p(x′ )dxdx′ .
σ
In the above limit, the bandwidth σ is held ﬁxed. Typically, one would also drive the bandwidth σ
to zero as n → ∞. There are two reasons for this choice. First, from a practical perspective, this
makes the similarity matrix W sparse so it can be stored and processed. Second, from a theoretical
perspective, this leads to a clear and well deﬁned limit of th e smoothness regularization term In (y),
at least when σ → 0 slowly enough1 , namely when σ = ω( dplog n/n). If σ → 0 as n → ∞,
and as long as nσd/ log n → ∞, then after appropriate normalization, the regularizer converges to
a density weighted gradient penalty term [7, 8]:
C σd+2 I (σ) (y) = J (y) = ZΩ
k∇y(x)k2 p(x)2dx
d
d
(5)
lim
C σd+2 In (y) = lim
n→∞
σ→0
where C = RRd kzk2G(kzk)dz , and assuming 0 < C < ∞ (which is the case for both the Gaussian
and the step ﬁlters). This energy functional J (f ) therefore encodes the notion of “smoothness ” with
respect to p(x) that is the basis of the SSL formulation (1) with the graph constructions speci ﬁed by
(3). To understand the behavior and appropriateness of (1) we must understand this functional and
the associated limit problem:
ˆy(x) = arg min
y

y(xi ) = yi , i = 1, . . . , l

subject to

J (y)

(4)

(6)

1When σ = o( dp1/n) then all non-diagonal weights Wi,j vanish (points no longer have any “close by”
neighbors). We are not aware of an analysis covering the regime where σ decays roughly as dp1/n, but would
be surprised if a qualitatively different meaningful limit is reached.

2

3 Graph Laplacian Regularization in R1

We begin by considering the solution of (6) for one dimensional data, i.e. d = 1 and x ∈ R. We ﬁrst
consider the situation where the support of p(x) is a continuous interval Ω = [a, b] ⊂ R (a and/or
b may be inﬁnite). Without loss of generality, we assume the la beled data is sorted in increasing
order a 6 x1 < x2 < · · · < xl 6 b. Applying the theory of variational calculus, the solution ˆy(x)
(xi , xi+1 ) the Euler-Lagrange equation
satis ﬁes inside each interval
dx (cid:21) = 0.
dx (cid:20)p2 (x)
dy
d
Performing two integrations and enforcing the constraints at the labeled points yields
y(x) = yi + R x
1/p2 (t)dt
xi
R xi+1
1/p2 (t)dt
xi
with y(x) = x1 for a 6 x 6 x1 and y(x) = xl for xl 6 x 6 b. If the support of p(x) is a union of
disjoint intervals, the above analysis and the form of the solution applies in each interval separately.

for xi 6 x 6 xi+1

(yi+1 − yi )

(7)

.

(8)

(9)

(10)

The solution (7) seems reasonable and desirable from the point of view of the “smoothness ” assump-
tions: when p(x) is uniform, the solution interpolates linearly between labeled data points, whereas
across low-density regions, where p(x) is close to zero, y(x) can change abruptly. Furthermore,
the regularizer J (y) can be interpreted as a Reproducing Kernel Hilbert Space (RKHS) squared
semi-norm, giving us additional insight into this choice of regularizer:
4 R b
Theorem 1. Let p(x) be a smooth density on Ω = [a, b] ⊂ R such that Ap = 1
a 1/p2 (t)dt < ∞.
Then, J (f ) can be written as a squared semi-norm J (f ) = kf k2
induced by the kernel
Kp
2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
p2 (t) dt(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Z x′
Kp (x, x′ ) = Ap − 1
1
x
with a null-space of all constant functions. That is, kf kKp is the norm of the projection of f onto
the RKHS induced by Kp .
If p(x) is supported on several disjoint intervals, Ω = ∪i [ai , bi ], then J (f ) can be written as a
squared semi-norm induced by the kernel
2 (cid:12)(cid:12)(cid:12)R x′
p2 (t) (cid:12)(cid:12)(cid:12)
Kp (x, x′ ) = ( 1
4 R bi
dt
p2 (t) − 1
dt
x
ai
0
with a null-space spanned by indicator functions 1[ai ,bi ] (x) on the connected components of Ω.
Proof. For any f (x) = Pi αiKp (x, xi ) in the RKHS induced by Kp :
dx (cid:19)2
J (f ) = Z (cid:18) df
p2 (x)dx = Xi,j
αiαj Jij
d
where Jij = Z d
Kp (x, xi )
dx
dx
When xi and xj are in different connected components of Ω, the gradients of Kp (·, xi ) and Kp (·, xj )
are never non-zero together and Jij = 0 = Kp (xi , xj ). When they are in the same connected
component [a, b], and assuming w.l.o.g. a 6 xi 6 xj 6 b:
4 "Z xi
dt#
dt + Z xj
dt + Z b
1
xj
a
xi
2 Z xj
4 Z b
1
1
1
1
dt −
dt = Kp (xi , xj ).
=
p2 (t)
p2 (t)
a
xi
Substituting Jij = Kp (xi , xj ) into (10) yields J (f ) = P αiαj Kp (xi , xj ) = kf kKp .
3

if x, x′ ∈ [ai , bi ]
if x ∈ [ai , bi ], x′ ∈ [aj , bj ], i 6= j

Kp (x, xj )p2 (x)dx

1
p2 (t)

Jij =

−1
p2 (t)

1
p2 (t)

(11)

y(x) =

βi1[ai ,bi ] (x),

Combining Theorem 1 with the Representer Theorem [13] establishes that the solution of (6) (or of
any variant where the hard constraints are replaced by a data term) is of the form:
l
Xj=1
αj Kp (x, xj ) + Xi
where i ranges over the connected components [ai , bi ] of Ω, and we have:
l
Xi,j=1
suggests understanding (6), and so also its empirical approxima-
Viewing the regularizer as kyk2
Kp
tion (1), by interpreting Kp (x, x′ ) as a density-based “similarity measure” between
x and x′ . This
similarity measure indeed seems sensible: for a uniform density it is simply linearly decreasing as a
function of the distance. When the density is non-uniform, two points are relatively similar only if
they are connected by a region in which 1/p2 (x) is low, i.e. the density is high, but are much less
“similar ”, i.e. related to each other, when connected by a lo w-density region. Furthermore, there is
no dependence between points in disjoint components separated by zero density regions.

αiαj Kp (xi , xj ).

J (y) =

(12)

4 Graph Laplacian Regularization in Higher Dimensions

The analysis of the previous section seems promising, at it shows that in one dimension, the SSL
method (1) is well posed and converges to a sensible limit. Regretfully, in higher dimensions this is
not the case anymore. In the following theorem we show that the inﬁmum of the limit problem (6) is
zero and can be obtained by a sequence of functions which are certainly not a sensible extrapolation
of the labeled points.
Theorem 2. Let p(x) be a smooth density over Rd , d > 2, bounded from above by some constant
pmax , and let (x1 , y1 ), . . . , (xl , yl ) be any (non-repeating) set of labeled examples. There exist con-
tinuous functions yǫ (x), for any ǫ > 0, all satisfying the constraints yǫ (xj ) = yj , j = 1, . . . , l, such
that J (yǫ ) ǫ→0−→ 0 but yǫ (x) ǫ→0−→ 0 for all x 6= xj , j = 1, . . . , l.

Proof. We present a detailed proof for the case of l = 2 labeled points. The generalization of the
proof to more labeled points is straightforward. Furthermore, without loss of generality, we assume
the ﬁrst labeled point is at x0 = 0 with y(x0 ) = 0 and the second labeled point is at x1 with
kx1 k = 1 and y(x1 ) = 1. In addition, we assume that the ball B1 (0) of radius one centered around
the origin is contained in Ω = {x ∈ Rd | p(x) > 0}.
We ﬁrst consider the case d > 2. Here, for any ǫ > 0, consider the function
yǫ (x) = min (cid:16) kxk
ǫ , 1(cid:17)
which indeed satis ﬁes the two constraints yǫ (xi ) = yi , i = 0, 1. Then,
p2 (x)
pmax
ǫ2 ZBǫ (0)
J (yǫ ) = ZBǫ (0)
dx = p2
maxVd ǫd−2
ǫ2 dx 6
where Vd is the volume of a unit ball in Rd . Hence, the sequence of functions yǫ (x) satisfy the
constraints, but for d > 2, inf ǫ J (yǫ ) = 0.
For d = 2, a more extreme example is necessary: consider the functions
(cid:17)(cid:14) log (cid:0) 1+ǫ
yǫ (x) = log (cid:16) kxk2+ǫ
ǫ (cid:1)
ǫ
and yǫ (x) = 1 for kxk > 1. These functions satisfy the two constraints yǫ (xi ) = yi , i = 0, 1 and:
ǫ ”i2 Z 1
ǫ ”i2 ZB1 (0)
4p2
kxk2
(kxk2+ǫ)2 p2 (x)dx 6
4
max
hlog“ 1+ǫ
hlog“ 1+ǫ
0
4πp2
ǫ ”i2 log (cid:0) 1+ǫ
ǫ (cid:1) =
max
hlog“ 1+ǫ

r2
(r2+ǫ)2 2πrdr

for kxk 6 1

J (yǫ ) =

ǫ→0−→ 0.

(13)

6

4πp2
max
log (cid:0) 1+ǫ
ǫ (cid:1)
4

The implication of Theorem 2 is that regardless of the values at the labeled points, as u → ∞, the
solution of (1) is not well posed. Asymptotically, the solution has the form of an almost every-
where constant function, with highly localized spikes near the labeled points, and so no learning
is performed. In particular, an interpretation in terms of a density-based kernel Kp , as in the one-
dimensional case, is not possible.

Our analysis also carries over to a formulation where a loss-based data term replaces the hard label
constraints, as in

ˆy = arg min
y(x)

l
Xj=1
In the limit of inﬁnite unlabeled data, functions of the form yǫ (x) above have a zero data penalty
term (since they exactly match the labels) and also drive the regularization term J (y) to zero. Hence,
it is possible to drive the entire objective functional (the data term plus the regularization term) to
zero with functions that do not generalize at all to unlabeled points.

(y(xj ) − yj )2 + γ In (y)

1
l

4.1 Numerical Example

We illustrate the phenomenon detailed by Theorem 2 with a simple example. Consider a density
p(x) in R2 , which is a mixture of two unit variance spherical Gaussians, one per class, centered at
the origin and at (4, 0). We sample a total of n = 3000 points, and label two points from each of
the two components (four total). We then construct a similarity matrix using a Gaussian ﬁlter with
σ = 0.4.
Figure 1 depicts the predictor ˆy(x) obtained from (1). In fact, two different predictors are shown,
obtained by different numerical methods for solving (1). Both methods are based on the observation
that the solution ˆy(x) of (1) satis ﬁes:
n
n
Xj=1
Xj=1
Combined with the constraints of (1), we obtain a system of linear equations that can be solved
by Gaussian elimination (here invoked through MATLAB’s backslash operator). This is the method
used in the top panels of Figure 1. Alternatively, (14) can be viewed as an update equation for ˆy(xi ),
which can be solved via the power method, or label propagation [2, 6]: start with zero labels on the
unlabeled points and iterate (14), while keeping the known labels on x1 , . . . , xl . This is the method
used in the bottom panels of Figure 1.

on all unlabeled points i = l + 1, . . . , l + u.

Wij ˆy(xj ) /

ˆy(xi ) =

Wij

(14)

As predicted, ˆy(x) is almost constant for almost all unlabeled points. Although all values are very
close to zero, thresholding at the “right ” threshold does ac tually produce sensible results in terms of
the true -1/+1 labels. However, beyond being inappropriate for regression, a very ﬂat predictor is still
problematic even from a classi ﬁcation perspective. First,
it is not possible to obtain a meaningful
conﬁdence measure for particular labels. Second, especial
ly if the size of each class is not known a-
priori, setting the threshold between the positive and negative classes is problematic. In our example,
setting the threshold to zero yields a generalization error of 45%.
The differences between the two numerical methods for solving (1) also point out to another problem
with the ill-posedness of the limit problem: the solution is numerically very un-stable.

A more quantitative evaluation, that also validates that the effect in Figure 1 is not a result of choos-
ing a “wrong” bandwidth
σ , is given in Figure 2. We again simulated data from a mixture of two
Gaussians, one Gaussian per class, this time in 20 dimensions, with one labeled point per class, and
an increasing number of unlabeled points. In Figure 2 we plot the squared error, and the classi ﬁ-
cation error of the resulting predictor ˆy(x). We plot the classi ﬁcation error both when a threshold
of zero is used (i.e. the class is determined by sign( ˆy(x))) and with the ideal threshold minimizing
the test error. For each unlabeled sample size, we choose the bandwidth σ yielding the best test
performance (this is a “cheating” approach which provides a
lower bound on the error of the best
method for selecting the bandwidth). As the number of unlabeled examples increases the squared
error approaches 1, indicating a ﬂat predictor. Using a threshold of zero leads
to an increase in the
classi ﬁcation error, possibly due to numerical instabilit y. Interestingly, although the predictors be-
come very ﬂat, the classi ﬁcation error using the ideal thres
hold actually improves slightly. Note that

5

1

0

−1
10

1

0

−1
10

0

0

10

5

0

−10

−5

POWER METHOD

10

5

0

−10

−5

10

5

0

−5

 
−10
−5

10

5

0

−5

−10
−5

6

4

2

0

0

1.5

1

0.5

0

0

8

6

4

2

0

200
400
600
OPTIMAL BANDWIDTH

800

200
600
400
OPTIMAL BANDWIDTH

800

200

400

600

800

DIRECT INVERSION

SIGN ERROR: 45%

SQUARED ERROR

OPTIMAL BANDWIDTH

 

y(x) > 0
y(x) < 0

1

0.95

0.9

0.85

0
200
400
600
800
0−1 ERROR (THRESHOLD=0)
0.32

0

5

10

SIGN ERR: 17.1

0.3

0.28

0.26

0
800
600
400
200
0−1 ERROR (IDEAL THRESHOLD)
0.19

0.18

0.17

0.16

0

200

400

600

800

0

5

10

Figure 1: Left plots: Minimizer of Eq. (1). Right plots:
the resulting classiﬁcation according to sign(y). The four
labeled points are shown by green squares. Top: mini-
mization via Gaussian elimination (MATLAB backslash).
Bottom: minimization via label propagation with 1000 it-
erations - the solution has not yet converged, despite small
residuals of the order of 2 · 10−4 .

Figure 2: Squared error (top), classiﬁcation error
with a threshold of zero (center) and minimal clas-
siﬁcation error using ideal threhold (bottom), of the
minimizer of (1) as a function of number of unla-
beled points. For each error measure and sample
size, the bandwidth minimizing the test error was
used, and is plotted.

ideal classi ﬁcation performance is achieved with a signi ﬁc
antly larger bandwidth than the bandwidth
minimizing the squared loss, i.e. when the predictor is even ﬂatter.

4.2 Probabilistic Interpretation, Exit and Hitting Times

As mentioned above, the Laplacian regularization method (1) has a probabilistic interpretation in
terms of a random walk on the weighted graph. Let x(t) denote a random walk on the graph with
transition matrix M = D−1W where D is a diagonal matrix with Dii = Pj Wij . Then, for the
binary classi ﬁcation case with yi = ±1 we have [15]:
ˆy(xi ) = 2 Prhx(t) hits a point labeled +1 before hitting a point labeled -1 (cid:12)(cid:12)(cid:12)
x(0) = xi i − 1
We present an interpretation of our analysis in terms of the limiting properties of this random walk.
Consider, for simplicity, the case where the two classes are separated by a low density region. Then,
the random walk has two intrinsic quantities of interest. The ﬁrst is the mean exit time from one
cluster to the other, and the other is the mean hitting time to the labeled points in that cluster. As the
number of unlabeled points increases and σ → 0, the random walk converges to a diffusion process
[12]. While the mean exit time then converges to a ﬁnite value c orresponding to its diffusion ana-
logue, the hitting time to a labeled point increases to inﬁni
ty (as these become absorbing boundaries
of measure zero). With more and more unlabeled data the random walk will fully mix, forgetting
where it started, before it hits any label. Thus, the probability of hitting +1 before −1 will become
uniform across the entire graph, independent of the starting location xi , yielding a ﬂat predictor.

5 Keeping σ Finite

At this point, a reader may ask whether the problems found in higher dimensions are due to taking
the limit σ → 0. One possible objection is that there is an intrinsic characteristic scale for the data
σ0 where (with high probability) all points at a distance kxi − xj k < σ0 have the same label. If this
is the case, then it may not necessarily make sense to take values of σ < σ0 in constructing W .
However, keeping σ ﬁnite while taking the number of unlabeled points to inﬁnity
does not resolve
the problem. On the contrary, even the one-dimensional case becomes ill-posed in this case. To
see this, consider a function y(x) which is zero everywhere except at the labeled points, where
y(xj ) = yj . With a ﬁnite number of labeled points of measure zero, I (σ) (y) = 0 in any dimension

6

50 points

500 points

3500 points

1

0.5

0

−0.5

y

1

0.5

0

−0.5

1

0.5

0

−0.5

2
x
Figure 3: Minimizer of (1) for a 1-d problem with a ﬁxed σ = 0.4, two labeled points and an increasing number
of unlabeled points.

−2

−2

4

6

2

4

4

6

0

2

0

−1

−2

0

6

−1

−1

and for any ﬁxed σ > 0. While this limiting function is discontinuous, it is also possible to construct
a sequence of continuous functions yǫ that all satisfy the constraints and for which I (σ) (yǫ ) ǫ→0−→ 0.
This behavior is illustrated in Figure 3. We generated data from a mixture of two 1-D Gaussians
centered at the origin and at x = 4, with one Gaussian labeled −1 and the other +1. We used
two labeled points at the centers of the Gaussians and an increasing number of randomly drawn
unlabeled points. As predicted, with a ﬁxed σ , although the solution is reasonable when the number
of unlabeled points is small, it becomes ﬂatter, with sharp s pikes on the labeled points, as u → ∞.

6 Fourier-Eigenvector Based Methods

Before we conclude, we discuss a different approach for SSL, also based on the Graph Laplacian,
suggested by Belkin and Niyogi [3]. Instead of using the Laplacian as a regularizer, constraining
candidate predictors y(x) non-parametrically to those with small In (y) values, here the predictors
are constrained to the low-dimensional space spanned by the ﬁrst few eigenvectors of the Laplacian:
The similarity matrix W is computed as before, and the Graph Laplacian matrix L = D − W is
considered (recall D is a diagonal matrix with Dii = Pj Wij ). Only predictors
ˆy(x) = Pp
(15)
j=1aj ej
spanned by the ﬁrst p eigenvectors e1 , . . . , ep of L (with smallest eigenvalues) are considered. The
coefﬁcients aj are chosen by minimizing a loss function on the labeled data, e.g. the squared loss:
(ˆa1 , . . . , ˆap ) = arg min Pl
(16)
j=1 (yj − ˆy(xj ))2 .
Unlike the Laplacian Regularization method (1), the Laplacian Eigenvector method (15) –(16) is
well posed in the limit u → ∞. This follows directly from the convergence of the eigenvectors of
the graph Laplacian to the eigenfunctions of the corresponding Laplace-Beltrami operator [10, 4].

Eigenvector based methods were shown empirically to provide competitive generalization perfor-
mance on a variety of simulated and real world problems. Belkin and Niyogi [3] motivate the
approach by arguing that ‘the eigenfunctions of the Laplace-Beltrami operator provide a natural ba-
sis for functions on the manifold and the desired classi ﬁcat
ion function can be expressed in such a
basis’. In our view, the success of the method is actually not due to data lying on a low-dimensional
manifold, but rather due to the low density separation assumption, which states that different class la-
bels form high-density clusters separated by low density regions. Indeed, under this assumption and
with sufﬁcient separation between the clusters, the eigenf unctions of the graph Laplace-Beltrami op-
erator are approximately piecewise constant in each of the clusters, as in spectral clustering [12, 11],
providing a basis for a labeling that is constant within clusters but variable across clusters. In other
settings, such as data uniformly distributed on a manifold but without any signi ﬁcant cluster struc-
ture, the success of eigenvector based methods critically depends on how well can the unknown
classi ﬁcation function be approximated by a truncated expa nsion with relatively few eigenvectors.

We illustrate this issue with the following three-dimensional example: Let p(x) denote the uniform
density in the box [0, 1] × [0, 0.8] × [0, 0.6], where the box lengths are different to prevent eigenvalue
multiplicity. Consider learning three different functions, y1 (x) = 1x1>0.5 , y2 (x) = 1x1>x2 /0.8 and
y3 (x) = 1x2 /0.8>x3 /0.6 . Even though all three functions are relatively simple, all having a linear
separating boundary between the classes on the manifold, as shown in the experiment described in
Figure 4, the Eigenvector based method (15) –(16) gives mark edly different generalization perfor-
mances on the three targets. This happens both when the number of eigenvectors p is set to p = l/5
as suggested by Belkin and Niyogi, as well as for the optimal (oracle) value of p selected on the test
set (i.e. a “cheating” choice representing an upper bound on
the generalization error of this method).

7

)
%
(
 
r
o
r
r
E
 
n
o
i
t
c
i
d
e
r
P

p = #labeled points/5
40

20

0

60
40
20
# labeled points

40

20

0

optimal p

20 labeled points

Approx. Error

50

0

0

20

10

0

0

15
10
5
# eigenvectors

15
10
5
# eigenvectors

60
40
20
# labeled points

Figure 4: Left three panels: Generalization Performance of the Eigenvector Method (15) –(16) for the three
different functions described in the text. All panels use n = 3000 points. Prediction counts the number of sign
agreements with the true labels. Rightmost panel: best ﬁt when many (all 3000) points are used, representing
the best we can hope for with a few leading eigenvectors.

The reason for this behavior is that y2 (x) and even more so y3 (x) cannot be as easily approximated
by the very few leading eigenfunctions —even though they seem “simple” and “smooth”, they are
signi ﬁcantly more complicated than y1 (x) in terms of measure of simplicity implied by the Eigen-
vector Method. Since the density is uniform, the graph Laplacian converges to the standard Lapla-
cian and its eigenfunctions have the form ψi,j,k (x) = cos(iπx1 ) cos(j πx2/0.8) cos(kπx3 /0.6),
making it hard to represent simple decision boundaries which are not axis-aligned.

7 Discussion

Our results show that a popular SSL method, the Laplacian Regularization method (1), is not well-
behaved in the limit of inﬁnite unlabeled data, despite its e mpirical success in various SSL tasks.
The empirical success might be due to two reasons.

First, it is possible that with a large enough number of labeled points relative to the number of
unlabeled points, the method is well behaved. This regime, where the number of both labeled and
unlabeled points grow while l/u is ﬁxed, has recently been analyzed by Wasserman and Laffert y
[9]. However, we do not ﬁnd this regime particularly satisfy ing as we would expect that having
more unlabeled data available should improve performance, rather than require more labeled points
or make the problem ill-posed. It also places the user in a delicate situation of choosing the “just
right ” number of unlabeled points without any theoretical g uidance.

Second, in our experiments we noticed that although the predictor ˆy(x) becomes extremely ﬂat, in
binary tasks, it is still typically possible to ﬁnd a thresho ld leading to a good classi ﬁcation perfor-
mance. We do not know of any theoretical explanation for such behavior, nor how to characterize
it. Obtaining such an explanation would be very interesting, and in a sense crucial to the theoretical
foundation of the Laplacian Regularization method. On a very practical level, such a theoretical un-
derstanding might allow us to correct the method so as to avoid the numerical instability associated
with ﬂat predictors, and perhaps also make it appropriate fo r regression.

The reason that the Laplacian regularizer (1) is ill-posed in the limit is that the ﬁrst order gradient
is not a sufﬁcient penalty in high dimensions. This fact is we ll known in spline theory, where the
Sobolev Embedding Theorem [1] indicates one must control at least d+1
2 derivatives in Rd . In the
context of Laplacian regularization, this can be done using the iterated Laplacian: replacing the
d+1
(matrix to
graph Laplacian matrix L = D − W , where D is the diagonal degree matrix, with L
2
the d+1
2 power). In the inﬁnite unlabeled data limit, this correspon ds to regularizing all order- d+1
2
(mixed) partial derivatives. In the typical case of a low-dimensional manifold in a high dimensional
ambient space, the order of iteration should correspond to the intrinsic, rather then ambient, dimen-
sionality, which poses a practical problem of estimating this usually unknown dimensionality. We
are not aware of much practical work using the iterated Laplacian, nor a good understanding of its
appropriateness for SSL.

A different approach leading to a well-posed solution is to include also an ambient regularization
term [5]. However, the properties of the solution and in particular its relation to various assumptions
y(x) relative to p(x) remain unclear.
about the “smoothness ” of

Acknowledgments The authors would like to thank the anonymous referees for valuable sugges-
tions. The research of BN was supported by the Israel Science Foundation (grant 432/06).

8

References

[1] R.A. Adams, Sobolev Spaces, Academic Press (New York), 1975.
[2] A. Azran, The rendevous algorithm: multiclass semi-supervised learning with Markov Random Walks,
ICML, 2007.
[3] M. Belkin, P. Niyogi, Using manifold structure for partially labelled classiﬁcation, NIPS, vol. 15, 2003.
[4] M. Belkin and P. Niyogi, Convergence of Laplacian Eigenmaps, NIPS 19, 2007.
[5] M. Belkin, P. Niyogi and S. Sindhwani, Manifold Regularization: A Geometric Framework for Learning
from Labeled and Unlabeled Examples, JMLR, 7:2399-2434, 2006.
[6] Y. Bengio, O. Delalleau, N. Le Roux, label propagation and quadratic criterion, in Semi-Supervised
Learning, Chapelle, Scholkopf and Zien, editors, MIT Press, 2006.
[7] O. Bosquet, O. Chapelle, M. Hein, Measure Based Regularization, NIPS, vol. 16, 2004.
[8] M. Hein, Uniform convergence of adaptive graph-based regularization, COLT, 2006.
[9] J. Lafferty, L. Wasserman, Statistical Analysis of Semi-Supervised Regression, NIPS, vol. 20, 2008.
[10] U. von Luxburg, M. Belkin and O. Bousquet, Consistency of spectral clustering, Annals of Statistics, vol.
36(2), 2008.
[11] M. Meila, J. Shi. A random walks view of spectral segmentation, AI and Statistics, 2001.
[12] B. Nadler, S. Lafon, I.G. Kevrekidis, R.R. Coifman, Diffusion maps, spectral clustering and eigenfunc-
tions of Fokker-Planck operators, NIPS, vol. 18, 2006.
[13] B. Sch ¨olkopf, A. Smola, Learning with Kernels, MIT Press, 2002.
[14] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, B. Sch ¨olkopf, Learning with local and global consistency,
NIPS, vol. 16, 2004.
[15] X. Zhu, Z. Ghahramani, J. Lafferty, Semi-Supervised Learning using Gaussian ﬁelds and harmonic func-
tions, ICML, 2003.

9


Kernel Choice and Classiﬁability for RKHS
Embeddings of Probability Distributions

Bharath K. Sriperumbudur
Department of ECE
UC San Diego, La Jolla, USA
bharathsv@ucsd.edu

Kenji Fukumizu
The Institute of Statistical Mathematics
Tokyo, Japan
fukumizu@ism.ac.jp

Arthur Gretton
Carnegie Mellon University
MPI for Biological Cybernetics
arthur.gretton@gmail.com

Gert R. G. Lanckriet
Department of ECE
UC San Diego, La Jolla, USA
gert@ece.ucsd.edu

Bernhard Sch ¨olkopf
MPI for Biological Cybernetics
T ¨ubingen, Germany
bs@tuebingen.mpg.de

Abstract

Embeddings of probability measures into reproducing kernel Hilbert spaces have
been proposed as a straightforward and practical means of representing and com-
paring probabilities. In particular, the distance between embeddings (the maxi-
mum mean discrepancy, or MMD) has several key advantages over many classical
metrics on distributions, namely easy computability, fast convergence and low bias
of ﬁnite sample estimates. An important requirement of the embedding RKHS is
that it be characteristic: in this case, the MMD between two distributions is zero
if and only if the distributions coincide. Three new results on the MMD are intro-
duced in the present study. First, it is established that MMD corresponds to the
optimal risk of a kernel classiﬁer, thus forming a natural link between the distance
between distributions and their ease of classiﬁcation. An important consequence
is that a kernel must be characteristic to guarantee classiﬁability between distri-
butions in the RKHS. Second, the class of characteristic kernels is broadened to
incorporate all strictly positive deﬁnite kernels: these include non-translation in-
variant kernels and kernels on non-compact domains. Third, a generalization of
the MMD is proposed for families of kernels, as the supremum over MMDs on
a class of kernels (for instance the Gaussian kernels with different bandwidths).
This extension is necessary to obtain a single distance measure if a large selection
or class of characteristic kernels is potentially appropriate. This generalization is
reasonable, given that it corresponds to the problem of learning the kernel by min-
imizing the risk of the corresponding kernel classiﬁer. The generalized MMD is
shown to have consistent ﬁnite sample estimates, and its performance is demon-
strated on a homogeneity testing example.

1 Introduction
Kernel methods are broadly established as a useful way of constructing nonlinear algorithms
from linear ones, by embedding points into higher dimensional reproducing kernel Hilbert spaces
(RKHSs) [9]. A generalization of this idea is to embed probability distributions into RKHSs, giving

1

(cid:82)
us a linear method for dealing with higher order statistics [6, 12, 14]. More speciﬁcally, suppose
we are given the set P of all Borel probability measures deﬁned on the topological space M , and
the RKHS (H, k) of functions on M with k as its reproducing kernel (r.k.). For P ∈ P , denote by
Pk :=
M k(., x) dP(x). If k is measurable and bounded, then we may deﬁne the embedding of P
in H as Pk ∈ H. The RKHS distance between two such mappings associated with P, Q ∈ P is
called the maximum mean discrepancy (MMD) [6, 14], and is written
γk (P, Q) = (cid:107)Pk − Qk(cid:107)H .
(1)
We say that k is characteristic [4, 14] if the mapping P (cid:55)→ Pk is injective, in which case (1) is
zero if and only if P = Q, i.e., γk is a metric on P . An immediate application of the MMD is to
problems of comparing distributions based on ﬁnite samples: examples include tests of homogeneity
[6], independence [7], and conditional independence [4]. In this application domain, the question of
whether k is characteristic is key: without this property, the algorithms can fail through inability to
distinguish between particular distributions.
Characteristic kernels are important in binary classiﬁcation: The problem of distinguishing dis-
tributions is strongly related to binary classiﬁcation: indeed, one would expect easily distinguishable
distributions to be easily classiﬁable.1 The link between these two problems is especially direct in
the case of the MMD: in Section 2, we show that γk is the negative of the optimal risk (correspond-
ing to a linear loss function) associated with the Parzen window classiﬁer [9, 11] (also called kernel
classiﬁcation rule [3, Chapter 10]), where the Parzen window turns out to be k . We also show that
γk is an upper bound on the margin of a hard-margin support vector machine (SVM). The impor-
tance of using characteristic RKHSs is further underlined by this link: if the property does not hold,
then there exist distributions that are unclassiﬁable in the RKHS H. We further strengthen this by
showing that characteristic kernels are necessary (and sufﬁcient under certain conditions) to achieve
Bayes risk in the kernel-based classiﬁcation algorithms.
Characterization of characteristic kernels: Given the centrality of the characteristic property to
both RKHS classiﬁcation and RKHS distribution testing, we should take particular care in estab-
lishing which kernels satisfy this requirement. Early results in this direction include [6], where k is
shown to be characteristic on compact M if it is universal in the sense of Steinwart [15, Deﬁnition
4]; and [4, 5], which address the case of non-compact M , and show that k is characteristic if and
only if H + R is dense in the Banach space of p-power (p ≥ 1) integrable functions. The conditions
in both these studies can be difﬁcult to check and interpret, however, and the restriction of the ﬁrst
to compact M is limiting. In the case of translation invariant kernels, [14] proved the kernel to
be characteristic if and only if the support of the Fourier transform of k is the entire Rd , which is
a much easier condition to verify. Similar sufﬁcient conditions are obtained by [5] for translation
invariant kernels on groups and semi-groups. In Section 3, we expand the class of characteristic
kernels to include kernels that may or may not be translation invariant, with the introduction of a
novel criterion: strictly positive deﬁnite kernels (see Deﬁnition 3) on M are characteristic.
Choice of characteristic kernels: In expanding the families of allowable characteristic kernels, we
have so far neglected the question of which characteristic kernel to choose. A practitioner asking by
how much two samples differ does not want to receive a blizzard of answers for every conceivable
kernel and bandwidth setting, but a single measure that satisﬁes some “reasonable” notion of dis-
tance across the family of kernels considered. Thus, in Section 4, we propose a generalization of the
MMD, yielding a new distance measure between P and Q deﬁned as
γ (P, Q) = sup{γk (P, Q) : k ∈ K} = sup{(cid:107)Pk − Qk(cid:107)H : k ∈ K},
(2)
which is the maximal RKHS distance between P and Q over a family, K of positive deﬁnite kernels.
For example, K can be the family of Gaussian kernels on Rd indexed by the bandwidth parameter.
This distance measure is very natural in the light of our results on binary classiﬁcation (in Section 2):
most directly, this corresponds to the problem of learning the kernel by minimizing the risk of the
associated Parzen-based classiﬁer. As a less direct justiﬁcation, we also increase the upper bound on
the margin allowed for a hard margin SVM between the samples. To apply the generalized MMD
in practice, we must ensure its empirical estimator is consistent. In our main result of Section 4,
we provide an empirical estimate of γ (P, Q) based on ﬁnite samples, and show that many popular
kernels like the Gaussian, Laplacian, and the entire Mat ´ern class on Rd yield consistent estimates

1There is a subtlety here, since unlike the problem of testing for differences in distributions, classiﬁcation
suffers from slow learning rates. See [3, Chapter 7] for details.

2

of γ (P, Q). The proof is based on bounding the Rademacher chaos complexity of K, which can be
understood as the U-process equivalent of Rademacher complexity [2].
Finally, in Section 5, we provide a simple experimental demonstration that the generalized MMD
can be applied in practice to the problem of homogeneity testing. Speciﬁcally, we show that when
two distributions differ on particular length scales, the kernel selected by the generalized MMD
is appropriate to this difference, and the resulting hypothesis test outperforms the heuristic kernel
choice employed in earlier studies [6]. The proofs of the results in Sections 2-4 are provided in the
supplementary material.

2 Characteristic Kernels and Binary Classiﬁcation
One of the most important applications of the maximum mean discrepancy is in nonparametric hy-
pothesis testing [6, 7, 4], where the characteristic property of k is required to distinguish between
probability measures. In the following, we show how MMD naturally appears in binary classiﬁca-
tion, with reference to the Parzen window classiﬁer and hard-margin SVM. This motivates the need
for characteristic k to guarantee that classes arising from different distributions can be classiﬁed by
kernel-based algorithms.
To this end, let us consider the binary classiﬁcation problem with X being a M -valued random
variable, Y being a {−1, +1}-valued random variable and the product space, M × {−1, +1}, being
endowed with an induced Borel probability measure µ. A discriminant function, f is a real valued
measurable function on M , whose sign is used to make a classiﬁcation decision. Given a loss
function L : {−1, +1} × R → R, the goal is to choose an f that minimizes the risk associated with
(cid:90)
(cid:90)
(cid:90)
(cid:111)
(cid:110)
L, with the optimal L-risk being deﬁned as
L1 (f ) dP + (1 − ε)
L−1 (f ) dQ
L(y , f (x)) dµ(x, y) = inf
F(cid:63) = inf
(3)
RL
ε
,
f ∈F(cid:63)
f ∈F(cid:63)
M
M
M
where F(cid:63) is the set of all measurable functions on M , L1 (α) := L(1, α), L−1 (α) := L(−1, α),
P(X ) := µ(X |Y = +1), Q(X ) := µ(X |Y = −1), ε := µ(M , Y = +1). Here, P and Q represent
the class-conditional distributions and ε is the prior distribution of class +1. Now, we present the
result that relates γk to the optimal risk associated with the Parzen window classiﬁer.
Theorem 1 (γk and Parzen classiﬁcation). Let L1 (α) = − α
1−ε . Then, γk (P, Q) =
ε and L−1 (α) = α
, where Fk = {f : (cid:107)f (cid:107)H ≤ 1} and H is an RKHS with a measurable and bounded k .
−RL
m = |{i : Yi = 1}|. If (cid:101)f ∈ Fk is an empirical minimizer of (3) (where F(cid:63) is replaced by Fk in (3)),
Fk
Suppose {(Xi , Yi )}N
i=1 , Xi ∈ M , Yi ∈ {−1, +1}, ∀ i is a training sample drawn i.i.d. from µ and
(cid:80)
(cid:80)
(cid:189)
sign( (cid:101)f (x)) =
(cid:80)
(cid:80)
then
Yi=−1 k(x, Xi )
Yi=1 k(x, Xi ) > 1
1,
1
N −m
(4)
−1,
Yi=1 k(x, Xi ) ≤ 1
m
,
Yi=−1 k(x, Xi )
1
N −m
m
which is the Parzen window classiﬁer.
Theorem 1 shows that γk is the negative of the optimal L-risk (where L is the linear loss as deﬁned
in Theorem 1) associated with the Parzen window classiﬁer. Therefore, if k is not characteristic,
which means γk (P, Q) = 0 for some P (cid:54)= Q, then RL
= 0, i.e., the risk is maximum (note that
Fk
since 0 ≤ γk (P, Q) = −RL
, the maximum risk is zero). In other words, if k is characteristic, then
Fk
the maximum risk is obtained only when P = Q. This motivates the importance of characteristic
kernels in binary classiﬁcation. In the following, we provide another result which provides a similar
motivation for the importance of characteristic kernels in binary classiﬁcation, wherein we relate γk
to the margin of a hard-margin SVM.
Theorem 2 (γk and hard-margin SVM). Suppose {(Xi , Yi )}N
i=1 , Xi ∈ M , Yi ∈ {−1, +1}, ∀ i is
a training sample drawn i.i.d. from µ. Assuming the training sample is separable, let fsvm be the
solution to the program, inf {(cid:107)f (cid:107)H : Yi f (Xi ) ≥ 1, ∀ i}, where H is an RKHS with measurable and
bounded k . If k is characteristic, then
≤ γk (Pm , Qn )
1
(cid:80)
(cid:80)
(5)
(cid:107)fsvm (cid:107)H
,
2
Yi=−1 δXi , m = |{i : Yi = 1}| and n = N − m. δx
where Pm := 1
Yi=1 δXi , Qn := 1
m
n
represents the Dirac measure at x.

3

Theorem 2 provides a bound on the margin of hard-margin SVM in terms of MMD. (5) shows that
a smaller MMD between Pm and Qn enforces a smaller margin (i.e., a less smooth classiﬁer, fsvm ,
where smoothness is measured as (cid:107)fsvm (cid:107)H ). We can observe that the bound in (5) may be loose if
the number of support vectors is small. Suppose k is not characteristic, then γk (Pm , Qn ) can be zero
for Pm (cid:54)= Qn and therefore the margin is zero, which means even unlike distributions can become
inseparable in this feature representation.
Another justiﬁcation of using characteristic kernels in kernel-based classiﬁcation algorithms can be
provided by studying the conditions on H for which the Bayes risk is realized for all µ. Steinwart
and Christmann [16, Corollary 5.37] have showed that under certain conditions on L, the Bayes risk
is achieved for all µ if and only if H is dense in Lp (M , η) for all η , where η = εP + (1 − ε)Q.
Here, Lp (M , η) represents the Banach space of p-power integrable functions, where p ∈ [1, ∞) is
dependent on the loss function, L. Denseness of H in Lp (M , η) implies H + R is dense Lp (M , η),
which therefore yields that k is characteristic [4, 5]. On the other hand, if constant functions are
included in H, then it is easy to show that the characteristic property of k is also sufﬁcient to
achieve the Bayes risk. As an example, it can be shown that characteristic kernels are necessary (and
sufﬁcient if constant functions are in H) for SVMs to achieve the Bayes risk [16, Example 5.40].
Therefore, the characteristic property of k is fundamental in kernel-based classiﬁcation algorithms.
Having showed how characteristic kernels play a role in kernel-based classiﬁcation, in the following
section, we provide a novel characterization for them.

3 Novel Characterization for Characteristic Kernels
A positive deﬁnite (pd) kernel, k is said to be characteristic to P if and only if γk (P, Q) = 0 ⇔ P =
Q, ∀ P, Q ∈ P . The following result provides a novel characterization for characteristic kernels,
which shows that strictly pd kernels are characteristic to P . An advantage with this characterization
is that it holds for any arbitrary topological space M unlike the earlier characterizations where a
(cid:82)
(cid:82)
group structure on M is assumed [14, 5]. First, we deﬁne strictly pd kernels as follows.
Deﬁnition 3 (Strictly positive deﬁnite kernels). Let M be a topological space. A measurable and
M k(x, y) dµ(x) dµ(y) > 0
bounded kernel, k is said to be strictly positive deﬁnite if and only if
M
for all ﬁnite non-zero signed Borel measures, µ deﬁned on M .
(cid:82) (cid:82)
Note that the above deﬁnition is not equivalent to the usual deﬁnition of strictly pd kernels that in-
volves ﬁnite sums [16, Deﬁnition 4.15]. The above deﬁnition is a generalization of integrally strictly
k(x, y)f (x)f (y) dx dy > 0 for all f ∈ L2 (Rd ),
positive deﬁnite functions [17, Section 6]:
which is the strictly positive deﬁniteness of the integral operator given by the kernel. Deﬁnition 3 is
stronger than the ﬁnite sum deﬁnition as [16, Theorem 4.62] shows a kernel that is strictly pd in the
ﬁnite sum sense but not in the integral sense.
Theorem 4 (Strictly pd kernels are characteristic). If k is strictly positive deﬁnite on M , then k is
characteristic to P .
The proof idea is to derive necessary and sufﬁcient conditions for a kernel not to be characteristic.
We show that choosing k to be strictly pd violates these conditions and k is therefore characteristic to
2 ), σ > 0, exp(−σ(cid:107)x−y(cid:107)1 ), σ >
P . Examples of strictly pd kernels on Rd include exp(−σ(cid:107)x−y(cid:107)2
0, (c2 + (cid:107)x − y(cid:107)2
2 )−β , β > 0, c > 0, B2l+1 -splines etc. Note that ˜k(x, y) = f (x)k(x, y)f (y) is a
strictly pd kernel if k is strictly pd, where f : M → R is a bounded continuous function. Therefore,
translation-variant strictly pd kernels can be obtained by choosing k to be a translation invariant
strictly pd kernel. A simple example of a translation-variant kernel that is a strictly pd kernel on
compact sets of Rd is ˜k(x, y) = exp(σxT y), σ > 0, where we have chosen f (.) = exp(σ(cid:107).(cid:107)2
2 /2)
and k(x, y) = exp(−σ(cid:107)x − y(cid:107)2
2 /2), σ > 0. Therefore, ˜k is characteristic on compact sets of Rd ,
which is the same result that follows from the universality of ˜k [15, Section 3, Example 1].
The following result in [10], which is based on the usual deﬁnition of strictly pd kernels, can be
(cid:80)m
(cid:80)n
obtained as a corollary to Theorem 4.
Corollary 5 ([10]). Let X = {xi }m
i=1 ⊂ M , Y = {yj }n
j=1 ⊂ M and assume that xi (cid:54)= xj , yi (cid:54)=
yj , ∀ i, j . Suppose k is strictly positive deﬁnite. Then
(cid:80)m
j=1 βj k(., yj ) for some
i=1 αik(., xi ) =
αi , βj ∈ R\{0} ⇒ X = Y .
(cid:80)n
n , ∀ j in Corollary 5. Then
m , ∀ i and βj = 1
Suppose we choose αi = 1
i=1 αik(., xi )
j=1 βj k(., yj ) represent the mean functions in H. Note that the Parzen classiﬁer in (4)
and

4

(cid:80)m
(cid:80)n
is a mean classiﬁer (that separates the mean functions) in H, i.e., sign((cid:104)k(., x), w(cid:105)H ), where
i=1 k(., xi ) − 1
w = 1
i=1 k(., yi ). Suppose k is strictly pd (more generally, suppose k is
m
n
characteristic). Then, by Corollary 5, the normal vector, w to the hyperplane in H passing through
the origin is zero, i.e., the mean functions coincide (and are therefore not classiﬁable) if and only if
X = Y .
4 Generalizing the MMD for Classes of Characteristic Kernels
The discussion so far has been related to the characteristic property of k that makes γk a metric on
P . We have seen that this characteristic property is of prime importance both in distribution testing,
and to ensure classiﬁability of dissimilar distributions in the RKHS. We have not yet addressed how
to choose among a selection/family of characteristic kernels, given a particular pair of distributions
we wish to discriminate between. We introduce one approach to this problem in the present section.
Let M = Rd and kσ (x, y) = exp(−σ(cid:107)x − y(cid:107)2
2 ), σ ∈ R+ , where σ represents the bandwidth
parameter. {kσ : σ ∈ R+} is the family of Gaussian kernels and {γkσ : σ ∈ R+ } is the family
of MMDs indexed by the kernel parameter, σ . Note that kσ is characteristic for any σ ∈ R++ and
therefore γkσ is a metric on P for any σ ∈ R++ . However, in practice, one would prefer a single
number that deﬁnes the distance between P and Q. The question therefore to be addressed is how
to choose appropriate σ . The choice of σ has important implications on the statistical aspect of γkσ .
Note that as σ → 0, kσ → 1 and as σ → ∞, kσ → 0 a.e., which means γkσ (P, Q) → 0 as σ → 0
or σ → ∞ for all P, Q ∈ P (this behavior is also exhibited by kσ (x, y) = exp(−σ(cid:107)x − y(cid:107)1 ) and
kσ (x, y) = σ2 /(σ2 + (cid:107)x − y(cid:107)2
2 ), which are also characteristic). This means choosing sufﬁciently
small or sufﬁciently large σ (depending on P and Q) makes γkσ (P, Q) arbitrarily small. Therefore, σ
has to be chosen appropriately in applications to effectively distinguish between P and Q. Presently,
the applications involving MMD set σ heuristically [6, 7].
To generalize the MMD to families of kernels, we propose the following modiﬁcation to γk , which
yields a pseudometric on P ,
γ (P, Q) = sup{γk (P, Q) : k ∈ K} = sup{(cid:107)Pk − Qk(cid:107)H : k ∈ K}.
(6)
Note that γ is the maximal RKHS distance between P and Q over a family, K of positive deﬁnite
kernels. It is easy to check that if any k ∈ K is characteristic, then γ is a metric on P . Examples for
Krbf := {(cid:82) ∞
K include: Kg := {e−σ(cid:107)x−y(cid:107)2
2 , x, y ∈ Rd : σ ∈ R+ }; Kl := {e−σ(cid:107)x−y(cid:107)1 , x, y ∈ Rd : σ ∈ R+};
Kψ := {e−σψ(x,y) , x, y ∈ M : σ ∈ R+}, where ψ : M × M → R is a negative deﬁnite kernel;
2 dµσ (λ), x, y ∈ Rd , µσ ∈ M + : σ ∈ Σ ⊂ Rd}, where M + is the set of
0 e−λ(cid:107)x−y(cid:107)2
all ﬁnite nonnegative Borel measures, µσ on R+ that are not concentrated at zero, etc.
The proposal of γ (P, Q) in (6) can be motivated by the connection that we have established in
Section 2 between γk and the Parzen window classiﬁer. Since the Parzen window classiﬁer depends
on the kernel, k , one can propose to learn the kernel like in support vector machines [8], wherein
in Theorem 1 is minimized over k ∈ K, i.e., inf k∈K RL
=
the kernel is chosen such that RL
− supk∈K γk (P, Q) = −γ (P, Q). A similar motivation for γ can be provided based on (5) as
Fk
Fk
(cid:82)
learning the kernel in a hard-margin SVM by maximizing its margin.
At this point, we brieﬂy discuss the issue of normalized vs. unnormalized kernel families, K in
(6). We say a translation-invariant kernel, k on Rd is normalized if
M ψ(y) dy = c (some positive
constant independent of the kernel parameter), where k(x, y) = ψ(x − y). K is a normalized kernel
family if every kernel in K is normalized. If K is not normalized, we say it is unnormalized. For
example, it is easy to see that Kg and Kl are unnormalized kernel families. Let us consider the
g = {(σ/π)d/2 e−σ(cid:107)x−y(cid:107)2
2 , x, y ∈ Rd : σ ∈ [σ0 , ∞)}. It can be
normalized Gaussian family, Kn
similar result also holds for the normalized inverse-quadratic kernel family, {(cid:112)
shown that for any kσ , kτ ∈ Kn
g , 0 < σ < τ < ∞, we have γkσ (P, Q) ≥ γkτ (P, Q), which
means, γ (P, Q) = γσ0 (P, Q). Therefore, the generalized MMD reduces to a single kernel MMD. A
2σ2 /π(σ2 + (cid:107)x −
2 )−1 , x, y ∈ R : σ ∈ [σ0 , ∞)}. These examples show that the generalized MMD deﬁnition
y(cid:107)2
is usually not very useful if K is a normalized kernel family.
In addition, σ0 should be chosen
beforehand, which is equivalent to heuristically setting the kernel parameter in γk . Note that σ0
cannot be zero because in the limiting case of σ → 0, the kernels approach a Dirac distribution,
which means the limiting kernel is not bounded and therefore the deﬁnition of MMD in (1) does
not hold. So, in this work, we consider unnormalized kernel families to render the deﬁnition of
generalized MMD in (6) useful.

5

8ν +

m + n

To use γ in statistical applications where P and Q are known only through i.i.d. samples {Xi }m
(cid:112)
and {Yi}n
i=1
i=1 respectively, we require its estimator γ (Pm , Qn ) to be consistent, where Pm and Qn
represent the empirical measures based on {Xi }m
i=1 and {Yj }n
j=1 . For k measurable and bounded,
[6, 12] have shown that γk (Pm , Qn ) is a
mn/(m + n)-consistent estimator of γk (P, Q). The
statistical consistency of γ (Pm , Qn ) is established in the following theorem, which uses tools from
U-process theory [2, Chapters 3,5]. We begin with the following deﬁnition.
Deﬁnition 6 (Rademacher chaos). Let G be a class of functions on M × M and {ρi}n
i=1 be
{n−1 (cid:80)n
independent Rademacher random variables, i.e., Pr(ρi = 1) = Pr(ρi = −1) = 1
2 . The
homogeneous Rademacher chaos process of order two with respect to {ρi}n
i=1 is deﬁned as
i=1 ⊂ M . The Rademacher chaos complex-
i<j ρiρj g(xi , xj ) : g ∈ G} for some {xi }n
(cid:175)(cid:175)(cid:175).
(cid:175)(cid:175)(cid:175) 1
n(cid:88)
ity over G is deﬁned as
Un (G; {xi}n
i=1 ) := Eρ sup
ρiρj g(xi , xj )
g∈G
n
i<j
We now provide the main result of the present section.
Theorem 7 (Consistency of γ (Pm , Qn )). Let every k ∈ K be measurable and bounded with ν :=
(cid:113)
(cid:114)
supk∈K,x∈M k(x, x) < ∞. Then, with probability at least 1 − δ , |γ (Pm , Qn ) − γ (P, Q)| ≤ A,
√
where
√
16Um (K; {Xi})
16Un (K; {Yi })
36ν log 4
δ )
(
√
A =
+
+
(8)
.
m
n
mn
From (8), it is clear that if Um (K; {Xi }) = OP (1) and Un (K; {Yi}) = OQ (1), then γ (Pm , Qn ) a.s.→
γ (P, Q). The following result provides a bound on Um (K; {Xi }) in terms of the entropy integral.
(cid:90) ν
Lemma 8 (Entropy bound). For any K as in Theorem 7 with 0 ∈ K, there exists a universal constant
C such that
(cid:105) 1
(cid:104)(cid:80)m
i=1 ) ≤ C
log N (K, D, ) d,
Um (K; {Xi }m
0
i<j (k1 (Xi , Xj ) − k2 (Xi , Xj ))2
2 . N (K, D, ) represents the -
where D(k1 , k2 ) = 1
m
covering number of K with respect to the metric D .
Assuming K to be a VC-subgraph class, the following result, as a corollary to Lemma 8 provides
an estimate of Um (K; {Xi}m
i=1 ). Before presenting the result, we ﬁrst provide the deﬁnition of a
VC-subgraph class.
Deﬁnition 9 (VC-subgraph class). The subgraph of a function g : M × R is the subset of M × R
given by {(x, t) : t < g(x)}. A collection G of measurable functions on a sample space is called a
VC-subgraph class, if the collection of all subgraphs of the functions in G forms a VC-class of sets
(in M × R).
The VC-index (also called the VC-dimension) of a VC-subgraph class, G is the same as the pseudo-
dimension of G. See [1, Deﬁnition 11.1] for details.
Corollary 10 (Um (K; {Xi }) for VC-subgraph, K). Suppose K is a VC-subgraph class with V (K)
being the VC-index. Assume K satisﬁes the conditions in Theorem 7 and 0 ∈ K. Then
Um (K; {Xi}) ≤ C ν log(C1V (K)(16e9 )V (K) ),
(cid:112)
for some universal constants C and C1 .
Using (10) in (8), we have |γ (Pm , Qn ) − γ (P, Q)| = OP,Q (
(m + n)/mn) and by the Borel-
Cantelli lemma, |γ (Pm , Qn ) − γ (P, Q)| a.s.→ 0. Now, the question reduces to which of the ker-
nel classes, K have V (K) < ∞. [18, Lemma 12] showed that V (Kg ) = 1 (also see [19]) and
Um (Krbf ) ≤ C2Um (Kg ), where C2 < ∞. It can be shown that V (Kψ ) = 1 and V (Kl ) = 1.
All these classes satisfy the conditions of Theorem 7 and Corollary 10 and therefore provide consis-
tent estimates of γ (P, Q) for any P, Q ∈ P . Examples of kernels on Rd that are covered by these
(cid:80)l
(cid:80)l
(cid:80)l
(cid:80)l
classes include the Gaussian, Laplacian, inverse multiquadratics, Mat ´ern class etc. Other choices
for K that are popular in machine learning are the linear combination of kernels, Klin := {kλ =
i=1 λiki | kλ is pd,
i=1 λi = 1} and Kcon := {kλ =
i=1 λiki | λi ≥ 0,
i=1 λi = 1}. [13,
Lemma 7] have shown that V (Kcon ) ≤ V (Klin ) ≤ l. Therefore, instead of using a class based on a
ﬁxed, parameterized kernel, one can also use a ﬁnite linear combination of kernels to compute γ .

(7)

(9)

(10)

6

(11)

(12)

− 2

k d(Pm − Qn ) ⊗ (Pm − Qn )

So far, we have presented the metric property and statistical consistency (of the empirical estimator)
of γ . Now, the question is how do we compute γ (Pm , Qn ) in practice. To show this, in the following,
 .
 m(cid:88)
we present two examples.
m,n(cid:88)
n(cid:88)
Example 11. Suppose K = Kg . Then, γ (Pm , Qn ) can be written as
e−σ(cid:107)Yi−Yj (cid:107)2
e−σ(cid:107)Xi−Xj (cid:107)2
e−σ(cid:107)Xi−Yj (cid:107)2
γ 2 (Pm , Qn ) = sup
σ∈R+
m2
n2
mn
i,j=1
i,j=1
i,j=1
The optimum σ∗ can be obtained by solving (11) and γ (Pm , Qn ) = (cid:107)Pmkσ∗ − Qnkσ∗ (cid:107)Hσ(cid:63) .
(cid:90) (cid:90)
Example 12. Suppose K = Kcon . Then, γ (Pm , Qn ) becomes
(cid:107)Pmk − Qnk(cid:107)2
γ 2 (Pm , Qn ) =
H = sup
sup
k∈Kcon
k∈Kcon
(cid:80)l
= sup{λT a : λT 1 = 1, λ (cid:186) 0},
(cid:80)n
(cid:80)m,n
(cid:80)m
i=1 λiki . Here λ = (λ1 , . . . , λl ) and (a)i = (cid:107)Pmki − Qnki (cid:107)2
=
where we have replaced k by
Hi
a,b=1 ki (Ya , Yb ) − 2
a,b=1 ki (Xa , Xb ) + 1
a,b=1 ki (Xa , Yb ).
It is easy to see that
1
m2
n2
mn
γ 2 (Pm , Qn ) = max1≤i≤l (a)i .
Similar examples can be provided for other K, where γ (Pm , Qn ) can be computed by solving a
semideﬁnite program (K = Klin ) or by the constrained gradient descent ( K = Kl , Krbf ).
(cid:82)
Finally, while the approach in (6) to generalizing γk is our focus in this paper, an alternative Bayesian
strategy would be to deﬁne a non-negative ﬁnite measure λ over K, and to average γk over that
measure, i.e., β (P, Q) :=
K γk (P, Q) dλ(k). This also yields a pseudometric on P . That said,
β (P, Q) ≤ λ(K)γ (P, Q), ∀ P, Q, which means if P and Q can be distinguished by β , they can be
distinguished by γ , but not vice-versa. In this sense, γ is stronger than β . One further complication
with the Bayesian approach is in deﬁning a sensible λ over K. Note that γk0 (single kernel MMD
based on k0 ) can be obtained by deﬁning λ(k) = δ(k − k0 ) in β (P, Q).
5 Experiments
In this section, we present a benchmark experiment that illustrates the generalized MMD proposed in
Section 4 is preferred above the single kernel MMD where the kernel parameter is set heuristically.
The experimental setup is as follows.
p ), a normal distribution in R with zero mean and variance, σ2
Let p = N (0, σ2
p . Let q be the perturbed
version of p, given as q(x) = p(x)(1 + sin ν x). Here p and q are the densities associated with P and
Q respectively. It is easy to see that q differs from p at increasing frequencies with increasing ν . Let
k(x, y) = exp(−(x − y)2 /σ). Now, the goal is that given random samples drawn i.i.d. from P and
Q (with ν ﬁxed), we would like to test H0 : P = Q vs. H1 : P (cid:54)= Q. The idea is that as ν increases,
it will be harder to distinguish between P and Q for a ﬁxed sample size. Therefore, using this setup
we can verify whether the adaptive bandwidth selection achieved by γ (as the test statistic) helps
to distinguish between P and Q at higher ν compared to γk with a heuristic σ . To this end, using
γ (Pm , Qn ) and γk (Pm , Qn ) (with various σ ) as test statistics Tmn , we design a test that returns H0
if Tmn ≤ cmn , and H1 otherwise. The problem therefore reduces to ﬁnding cmn . cmn is determined
as the (1 − α) quantile of the asymptotic distribution of Tmn under H0 , which therefore ﬁxes the
type-I error (the probability of rejecting H0 when it is true) to α. The consistency of this test under
γk (for any ﬁxed σ ) is proved in [6]. A similar result can be shown for γ under some conditions on
K. We skip the details here.
In our experiments, we set m = n = 1000, σ2
p = 10 and draw two sets of independent random
samples from Q. The distribution of Tmn is estimated by bootstrapping on these samples (250 boot-
strap iterations are performed) and the associated 95th quantile (we choose α = 0.05) is computed.
Since the performance of the test is judged by its type-II error (the probability of accepting H0
when H1 is true), we draw a random sample, one each from P and Q and test whether P = Q.
This process is repeated 300 times, and estimates of type-I and type-II errors are obtained for both
γ and γk . 14 different values for σ are considered on a logarithmic scale of base 2 with exponents
(−3, −2, −1, 0, 1, 3
2 , 4, 5, 6) along with the median distance between samples as one more
2 , 2, 5
2 , 3, 7
choice. 5 different choices for ν are considered: ( 1
4 , 1, 5
2 ).
2 , 3
4 , 3

+

7

(a)

(b)

(c)

(e)
(d)
Figure 1: (a) Type-I and Type-II errors (in %) for γ for varying ν . (b,c) Type-I and type-II error (in
%) for γk (with different σ ) for varying ν . The dotted line in (c) corresponds to the median heuristic,
which shows that its associated type-II error is very large at large ν . (d) Box plot of log σ grouped
by ν , where σ is selected by γ . (e) Box plot of the median distance between points (which is also a
choice for σ ), grouped by ν . Refer to Section 5 for details.

Figure 1(a) shows the estimated type-I and type-II errors using γ as the test statistic for varying
ν . Note that the type-I error is close to its design value of 5%, while the type-II error is zero for
all ν , which means γ distinguishes between P and Q for all perturbations. Figures 1(b,c) show the
estimates of type-I and type-II errors using γk as the test statistic for different σ and ν . Figure 1(d)
shows the box plot for log σ , grouped by ν , where σ is the bandwidth selected by γ . Figure 1(e)
shows the box plot of the median distance between points (which is also a choice for σ ), grouped by
ν . From Figures 1(c) and (e), it is easy to see that the median heuristic exhibits high type-II error for
ν = 3
2 , while γ exhibits zero type-II error (from Figure 1(a)). Figure 1(c) also shows that heuristic
choices of σ can result in high type-II errors. It is intuitive to note that as ν increases, (which means
the characteristic function of Q differs from that of P at higher frequencies), a smaller σ is needed
to detect these changes. The advantage of using γ is that it selects σ in a distribution-dependent
fashion and its behavior in the box plot shown in Figure 1(d) matches with the previously mentioned
intuition about the behavior of σ with respect to ν . These results demonstrate the validity of using γ
as a distance measure in applications.

6 Conclusions

In this work, we have shown how MMD appears in binary classiﬁcation, and thus that characteristic
kernels are important in kernel-based classiﬁcation algorithms. We have broadened the class of
characteristic RKHSs to include those induced by strictly positive deﬁnite kernels (with particular
application to kernels on non-compact domains, and/or kernels that are not translation invariant). We
have further provided a convergent generalization of MMD over families of kernel functions, which
becomes necessary even in considering relatively simple families of kernels (such as the Gaussian
kernels parameterized by their bandwidth). The usefulness of the generalized MMD is illustrated
experimentally with a two-sample testing problem.

Acknowledgments
The authors thank anonymous reviewers for their constructive comments and especially the re-
viewer who pointed out the connection between characteristic kernels and the achievability of Bayes
risk. B. K. S. was supported by the MPI for Biological Cybernetics, National Science Founda-
tion (grant DMS-MSPA 0625409), the Fair Isaac Corporation and the University of California MI-
CRO program. A. G. was supported by grants DARPA IPTO FA8750-09-1-0141, ONR MURI
N000140710747, and ARO MURI W911NF0810242.

8

0.50.7511.251.502456νError (in %)  Type−I errorType−II error−3−2−10123456510152025log σType−I error (in %)  ν=0.5ν=0.75ν=1.0ν=1.25ν=1.5−3−2−10123456050100log σType−II error (in %)  ν=0.5ν=0.75ν=1.0ν=1.25ν=1.50.50.7511.251.50123log σν0.50.7511.251.5891011Median as σνReferences
[1] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University
Press, UK, 1999.
[2] V. H. de la Pe ˜na and E. Gin ´e. Decoupling: From Dependence to Independence. Springer-Verlag, NY,
1999.
[3] L. Devroye, L. Gyorﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag,
New York, 1996.
[4] K. Fukumizu, A. Gretton, X. Sun, and B. Sch ¨olkopf. Kernel measures of conditional dependence. In J.C.
Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems
20, pages 489–496, Cambridge, MA, 2008. MIT Press.
[5] K. Fukumizu, B. K. Sriperumbudur, A. Gretton, and B. Sch ¨olkopf. Characteristic kernels on groups
and semigroups. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21, pages 473–480, 2009.
[6] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch ¨olkopf, and A. Smola. A kernel method for the two sample
problem. In B. Sch ¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing
Systems 19, pages 513–520. MIT Press, 2007.
[7] A. Gretton, K. Fukumizu, C.-H. Teo, L. Song, B. Sch ¨olkopf, and A. Smola. A kernel statistical test of
independence. In Advances in Neural Information Processing Systems 20, pages 585–592. MIT Press,
2008.
[8] G. R. G. Lanckriet, N. Christianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semideﬁnite programming. Journal of Machine Learning Research, 5:24–72, 2004.
[9] B. Sch ¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[10] B. Sch ¨olkopf, B. K. Sriperumbudur, A. Gretton, and K. Fukumizu. RKHS representation of measures. In
Learning Theory and Approximation Workshop, Oberwolfach, Germany, 2008.
[11] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press,
UK, 2004.
[12] A. J. Smola, A. Gretton, L. Song, and B. Sch ¨olkopf. A Hilbert space embedding for distributions. In
Proc. 18th International Conference on Algorithmic Learning Theory, pages 13–31. Springer-Verlag,
Berlin, Germany, 2007.
[13] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned kernels.
In
G. Lugosi and H. U. Simon, editors, Proc. of the 19th Annual Conference on Learning Theory, pages
169–183, 2006.
[14] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. G. Lanckriet, and B. Sch ¨olkopf. Injective Hilbert
space embeddings of probability measures. In R. Servedio and T. Zhang, editors, Proc. of the 21st Annual
Conference on Learning Theory, pages 111–122, 2008.
[15] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of
Machine Learning Research, 2:67–93, 2002.
[16] I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
[17] J. Stewart. Positive deﬁnite functions and generalizations, an historical survey. Rocky Mountain Journal
of Mathematics, 6(3):409–433, 1976.
[18] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In Proc. of the 22nd Annual
Conference on Learning Theory, 2009.
[19] Y. Ying and D. X. Zhou. Learnability of Gaussians with ﬂexible variances. Journal of Machine Learning
Research, 8:249–276, 2007.

9


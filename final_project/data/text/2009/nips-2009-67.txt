An LP View of the M-best MAP problem

Menachem Fromer Amir Globerson
School of Computer Science and Engineering
The Hebrew University of Jerusalem
{fromer,gamir}@cs.huji.ac.il

Abstract

We consider the problem of ﬁnding the M assignments with maximum
probability in a probabilistic graphical model. We show how this problem
can be formulated as a linear program (LP) on a particular polytope. We
prove that, for tree graphs (and junction trees in general), this polytope
has a particularly simple form and diﬀers from the marginal polytope in
a single inequality constraint. We use this characterization to provide an
approximation scheme for non-tree graphs, by using the set of spanning
trees over such graphs. The method we present puts the M -best inference
problem in the context of LP relaxations, which have recently received
considerable attention and have proven useful in solving diﬃcult inference
problems. We show empirically that our method often ﬁnds the provably
exact M best conﬁgurations for problems of high tree-width.

A common task in probabilistic modeling is ﬁnding the assignment with maximum proba-
bility given a model. This is often referred to as the MAP (maximum a-posteriori) problem.
Of particular interest is the case of MAP in graphical models, i.e., models where the prob-
ability factors into a product over small subsets of variables. For general models, this is an
NP-hard problem [11], and thus approximation algorithms are required. Of those, the class
of LP based relaxations has recently received considerable attention [3, 5, 18]. In fact, it has
been shown that some problems (e.g., ﬁxed backbone protein design) can be solved exactly
via sequences of increasingly tighter LP relaxations [13].

In many applications, one is interested not only in the MAP assignment but also in the
M maximum probability assignments [19]. For example, in a protein design problem, we
might be interested in the M amino acid sequences that are most stable on a given backbone
structure [2]. In cases where the MAP problem is tractable, one can devise tractable algo-
rithms for the M best problem [8, 19]. Speciﬁcally, for low tree-width graphs, this can be
done via a variant of max-product [19]. However, when ﬁnding MAPs is not tractable, it is
much less clear how to approximate the M best case. One possible approach is to use loopy
max-product to obtain approximate max-marginals and use those to approximate the M
best solutions [19]. However, this is largely a heuristic and does not provide any guarantees
in terms of optimality certiﬁcates or bounds on the optimal values.

LP approximations to MAP do enjoy such guarantees. Speciﬁcally, they provide upper
bounds on the MAP value and optimality certiﬁcates. Furthermore, they often work for
graphs with large tree-width [13]. The goal of the current work is to leverage the power
of LP relaxations to the M best case. We begin by focusing on the problem of ﬁnding
the second best solution. We show how it can be formulated as an LP over a polytope
we call the “assignment-excluding marginal polytope”. In the general case, this polytope
may require an exponential number of inequalities, but we prove that when the graph
is a tree it has a very compact representation. We proceed to use this result to obtain
approximations to the second best problem, and show how these can be tightened in various
ways. Next, we show how M best assignments can be found by relying on algorithms for

1

second best assignments, and thus our results for the second best case can be used to devise
an approximation algorithm for the M best problem.

We conclude by applying our method to several models, showing that it often ﬁnds the exact
M best assignments.

1 The M-best MAP problem and its LP formulation

Consider a function on n variables deﬁned as:
f (x1 , . . . , xn ; θ) = X
θij (xi , xj ) + X
ij∈E
i∈V
where V and E are the vertices and nodes of a graph G with n nodes. We shall be interested
in the M assignments with largest f (x; θ) value.1 Denote these by x(1) , . . . , x(M ) , so that
x(1) is the assignment that maximizes f (x; θ), x(2) is the 2nd best assignment, etc.

θi (xi )

(1)

The MAP problem (i.e., ﬁnding x(1) ) can be formulated as an LP as follows [15]. Let µ be
a vector of distributions that includes {µij (xi , xj )}ij∈E over edge variables and {µi (xi )}i∈V
over nodes. The set of µ that arise from some joint distribution is known as the marginal
polytope [15] and is denoted by M(G). Formally:
M(G) = {µ | ∃p(x) ∈ ∆ s.t. p(xi , xj ) = µij (xi , xj ) , p(xi ) = µi (xi )} .
where ∆ is the set of distributions on x. The MAP problem can then be shown to be
equivalent to the following LP:2
max
x

f (x; θ) = max
µ∈M(G)

µ · θ ,

(2)

It can be shown that this LP always has a maximizing µ that is a vertex of M(G) and
is integral. Furthermore, this µ corresponds to the MAP assignment x(1) . Although the
number of variables in this LP is only O(|E | + |V |), the diﬃculty comes from an exponential
number of linear inequalities generally required to describe the marginal polytope M(G).

We shall ﬁnd it useful to deﬁne a mapping between assignments x and integral vertices of
the polytope. Given an integral vertex v ∈ M(G), deﬁne x(v) to be the assignment that
maximizes vi (xi ). And, given an assignment z deﬁne v(z) to be the integral vertex in M(G)
corresponding to the assignment z . Thus the LP in Eq. 2 will be maximized by v(x(1) ).

One simple outer bound of the marginal polytope is the local polytope ML (G), which only
enforces pairwise constraints between variables:
µi (xi ) = 1 
ML (G) = 
µ ≥ 0 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
X


xj
The LP relaxation is then to maximize µ · θ where µ ∈ ML (G). For tree structured graphs,
ML (G) = M(G) [15] and thus the LP relaxation yields the exact MAP x(1) .

µij (xi , xj ) = µj (xj ), X
xi

µij (xi , xj ) = µi (xi ), X
xi

(3)

2 An LP Formulation for the 2nd-best MAP

Assume we found the MAP assignment x(1) and are now interested in ﬁnding x(2) . Is there
a simple LP whose solution yields x(2) ? We begin by focusing on the case where G is a tree
so that the local LP relaxation is exact. We ﬁrst treat the case of a connected tree.
To construct an LP whose solution is x(2) , a natural approach is to use the LP for x(1) (i.e.,
the LP in Eq. 2) but somehow eliminate the solution x(1) using additional constraints. This,
however, is somewhat trickier than it sounds. The key diﬃculty is that the new constraints
should not generate fractional vertices, so that the resulting LP is still exact.
We begin by deﬁning the polytope over which we need to optimize in order to obtain x(2) .

1This is equivalent to ﬁnding the maximum probability assignments for a model p(x) ∝ ef (x ;θ) .
2We use the notation µ · θ = Pij∈E Pxi ,xj
µij (xi , xj )θij (xi , xj ) + Pi Pxi
µi (xi )θi (xi)

2

Deﬁnition 1. The assignment-excluding marginal polytope is deﬁned as:
ˆM(G, z ) = {µ | ∃p(x) ∈ ∆ s.t. p(z) = 0, p(xi , xj ) = µij (xi , xj ), p(xi ) = µi (xi )} .
ˆM(G, z ) is simply the convex hul l of al l (integral) vectors v(x) for x 6= z .

(4)

The following result shows that optimizing over ˆM(G, x(1) ) will yield the second best solu-
tion x(2) , so that we refer to ˆM(G, x(1) ) as the second-best marginal polytope.
Lemma 1. The 2nd best solution is obtained via the fol lowing LP:
maxx6=x(1) f (x; θ) = maxµ∈ ˆM(G,x(1) ) µ · θ . Furthermore, the µ that maximizes the LP on
the right is integral and corresponds to the second-best MAP assignment x(2) .

The proof is similar to that of Eq. 2: instead of optimizing over x, we optimize over distribu-
tions p(x), while enforcing that p(x(1) ) = 0 so that x(1) is excluded from the maximization.

The key question which we now address is how to obtain a simple characterization of
ˆM(G, z ).
Intuitively, it would seems that ˆM(G, z ) should be “similar” to M(G), such
that it can be described as M(G) plus some constraints that “block” the assignment z .
To illustrate the diﬃculty in ﬁnding such “blocking” constraints, consider the following
constraint, originally suggested by Santos [10]: Pi µi (zi ) ≤ n − 1. This inequality is not
satisﬁed by µ = v(z ) since v(z ) attains the value n for the LHS of the above. Furthermore,
for any x 6= z and µ = v(x), the LHS would be n − 1 or less. Thus, this inequality separates
v(z ) from all other integral vertices. One might conclude that we can deﬁne ˆM(G, z ) by
adding this inequality to M(G). The diﬃculty is that the resulting polytope has fractional
vertices,3 and maximizing over it won’t generally yield an integral solution.

It turns out that there is a diﬀerent inequality that does yield an exact characterization of
ˆM(G, z ) when G is a tree. We now deﬁne this inequality and state our main theorem.
Deﬁnition 2. Consider the functional I (µ, z ) (which is linear in µ):
(1 − di )µi (zi ) + X
I (µ, z) = X
ij∈E
i
where di is the degree of node i in the tree graph G.
Theorem 1. Adding the single inequality I (µ, z ) ≤ 0 to M(G) yields ˆM(G, z ).
ˆM(G, z ) = {µ | µ ∈ M(G),
I (µ, z ) ≤ 0 }

µij (zi , zj )

(5)

(6)

The theorem is proved in the appendix. Taken together with Lemma 1, it implies that
x(2) may be obtained via an LP that is very similar to the MAP-LP, but has an additional
constraint. We note the interesting similarity between I (µ, z ) and the Bethe entropy [20].
The only diﬀerence is that in Bethe, µi , µij are replaced by H (Xi ), H (Xi , Xj ) respectively.4

The theorem also generalizes to the case where G is not a tree, but we have a junction tree
for G. In this case, the theorem still holds if we deﬁne a generalized I (µ, z) inequality as:
X
(1 − dS )µS (zS ) + X
S∈S
C∈C
where C and S are the junction tree cliques and their separators, respectively, and dS is
the number of cliques that intersect on separator S . In this case, the marginal polytope
should enforce consistency between marginals µC (zC ) and their separators µS (zS ). However,
such a characterization requires variables whose cardinality is exponential in the tree-width
and is thus tractable only for graphs of low tree-width.
In the next section, we address
approximations for general graphs.

µC (zC ) ≤ 0

(7)

A corresponding result exists for the case when G is a forest. In this case, the inequality
in Eq. 6 is modiﬁed to: I (µ, z) ≤ |P | − 1, where |P | denotes the number of connected
components of G.
Interestingly, for a graph without edges, this gives the Santos inequality.

3Consider the case of a single edge between 2 nodes where the MAP assignment is (0, 0). Adding
the inequality µ1 (0) + µ2 (0) ≤ 1 produces the fractional vertex (0.5, 0.5).
4The connection to Bethe can be more clearly understood from a duality-based proof of Theorem
1. We will cover this in an extended version of the manuscript.

3

3 2nd best LPs for general graphs - Spanning tree inequalities

When the graph G is not a tree, the marginal polytope M(G) generally requires an exponen-
tial number of inequalities. However, as mentioned above, it does have an exact description
in terms of marginals over cliques and separators of a junction tree. Given such marginals on
junction tree cliques, we also have an exact characterization of ˆM(G, z ) via the constraint
in Eq. 7. However, in general, we cannot aﬀord to be exponential in tree-width. Thus a
common strategy [15] is to replace M(G) with an outer bound that enforces consistency be-
tween marginals on overlapping sets of variables. The simplest example is ML (G) in Eq. 3.
In what follows, we describe an outer-bound approximation scheme for ˆM(G, z ). We use
ML (G) as the approximation for M(G) (more generally ML (G) can enforce consistency
between any set of small regions, e.g., triplets). When G is not a tree, the linear constraint in
Eq. 6 will no longer suﬃce to derive ˆM(G, z ). Moreover, direct application of the inequality
will incorrectly remove some integral vertices. An alternative approach is to add inequalities
that separate v (z) from the other integral vertices. This will serve to eliminate more and
more fractional vertices, and if enough constraints are added, this may result in an integral
solution. One obvious family of such constraints are those corresponding to spanning trees
in G and have the form of Eq. 5.
Deﬁnition 3. Consider any T that is a spanning tree of G. Deﬁne the functional I T (µ, z):
i )µi (zi ) + X
I T (µ, z) = X
(1 − dT
ij∈T
i

µij (zi , zj )

(8)

where dT
i

is the degree of i in T . We refer to I T (µ, z ) ≤ 0 as a spanning tree inequality.

For any sub-tree T of G, the corresponding spanning tree inequality separates the vertex
v(z ) from the other vertices. This can be shown via similar arguments as in the proof of
Theorem 1. Note, however, that the resulting polytope may still have fractional vertices.

The above argument shows that any spanning tree provides a separating inequality for
ˆM(G, z ). In principle, we would like to use as many such inequalities as possible.
Deﬁnition 4. The spanning tree assignment-excluding marginal polytope is deﬁned as:
ˆMS T
L (G, z ) = (cid:8)µ | µ ∈ ML (G), ∀ tree T ⊆ E I T (µ, z ) ≤ 0 (cid:9)
where the S T notation indicates the inclusion of al l spanning tree inequalities for G.5

(9)

Thus, we would actually like to perform the following optimization problem:

max
µ∈ ˆMST
L (G,z)
as an approximation to optimization over ˆM(G, z ); i.e., we seek the optimal µ sub ject to all
spanning tree inequalities for G with the ambition that this µ be integral and thus provide
the non-z MAP assignment, with a certiﬁcate of optimality.

µ·θ

Although the number of spanning trees is exponential in n, it turns out that al l spanning
inequalities can be used in practice. One way to achieve this is via a cutting plane algorithm
[12] that ﬁnds the most violated spanning tree inequality and adds it to the LP. To implement
this eﬃciently, we note that for a particular µ and a spanning tree T , the value of I T (µ, z )
can be decomposed into a sum over the edges in T (and a T -independent constant):
hµij (zi , zj ) − µi (zi ) − µj (zj )i + X
I T (µ, z) = X
ij∈T
i

µi (zi )

(10)

The tree maximizing the above is the maximum-weight spanning tree with edge-weights
wij = µij (zi , zj ) − µi (zi ) − µj (zj ). It can thus be found eﬃciently.

The cutting plane algorithm proceeds as follows. We start by adding an arbitrary spanning
tree. Then, as long as the optimal µ is fractional, we ﬁnd the spanning tree inequality that
µ most violates (where this is implemented via the maximum-weight spanning tree). This
constraint will necessarily remove µ from the polytope. If there are no violated inequalities

5Note that ˆM(G, z ) ⊆ ˆMS T
L (G, z ) ⊂ ML (G).

4

but µ is still fractional, then spanning tree inequalities do not suﬃce to ﬁnd an integral
solution (but see below on hypertree constraints to add in this case). In practice, we found
that only a relatively small number of inequalities are needed to successfully yield an integral
solution, or determine that all such inequalities are already satisﬁed.

An alternative approach for solving the all spanning-tree problem is to work via the dual.
The dual variables roughly correspond to points in the spanning tree polytope [16], opti-
mization over which can be done in polynomial time, e.g., via the ellipsoid algorithm. We do
not pursue this here since the cutting plane algorithm performed well in our experiments.
As mentioned earlier, we can exactly characterize ˆM(G, z ) using Eq. 7, albeit at a cost
exponential in the tree-width of the graph. A practical compromise would be to use in-
equalities over clique trees of G, where the cliques are relatively small, e.g., triplets. The
corresponding constraint (Eq. 7 with the small cliques and their separators) will necessarily
separate v(z ) from the other integral vertices. Finding the maximally violated such inequal-
ity is an NP-hard problem, equivalent to a prize collecting Steiner tree problem, but recent
work has found that such problems are often exactly solvable in practice [7]. It thus might
be practical to include all such trees as constraints using a cutting plane algorithm.

4 From 2nd-best to M-best

Thus far, we only dealt with the 2nd best case. As we show now, it turns out that the
2nd -best formalism can be used to devise an algorithm for M best. We begin by describing
an algorithm for the exact M best and then show how it can be used to approximate those
via the approximations for 2nd best described above. Fig. 1 describes our scheme, which
we call Partitioning for Enumerating Solutions (or PES) for solving the M best problem.
The scheme is general and only assumes that MAP-“like” problems can be solved. It is
inspired by several pre-existing M best solution schemes [4, 6, 8, 19] but diﬀers from them
in highlighting the role of ﬁnding a second best solution within a given subspace.

for m ← 1 to M do
if m = 1 then
Run MAP solver to obtain the best assignment: x(1) ≡ arg max f (x; θ)
CONSTRAINTS1 ← ∅

else

k ←−

arg max
k′ ∈{1,...,m−1}

f (y(k′ ) ; θ) // sub-space containing mth best assignment

x(m) ← y(k) // mth best assignment

// A variable choice that distinguishes x(m) from x(k) :
) : x(m)
(v, a) ← any member of the set {(i, x(m)
6= x(k)
i }
i
i

CONSTRAINTSm ← CONSTRAINTSk ∪ {xv = a} // Eliminate x(k) (as MAP) from subspace m

CONSTRAINTSk ← CONSTRAINTSk ∪ {xv 6= a} // Eliminate x(m) (as 2nd -best) from subspace k

y(k) ← CalcNextBestSolution(CONSTRAINTSk

, x(k) )

end

y(m) ← CalcNextBestSolution(CONSTRAINTSm

, x(m) )

end
return {x(m) }M
m=1

/* Find next best solution in sub-space defined by CONSTRAINTS */

, x(∗) )
Function CalcNextBestSolution(CONSTRAINTS
// x(∗) is the MAP in the sub-space defined by CONSTRAINTS:
Run MAP solver to obtain the second-best solution: y ≡
arg max
x 6=x(∗) ,CONSTRAINTS

end

f (x; θ), and return y .

Figure 1: Pseudocode for the PES algorithm.

The modus operandi of the PES algorithm is to eﬃciently partition the search space while
systematically excluding all previously determined assignments. Signiﬁcantly, any MAP

5

Attractive Grids
Run-times
Ranks
1

0.5

0

S

N

B

S

N

B

50

0

Mixed Grids
Run-times
Ranks
1

0.5

0

S+R N+R B+R

S+R N+R B+R

50

0

Hard Protein SCP
Run-times
Ranks
1

0.5

0

S+R B

B+R

S+R B

B+R

50

0

Figure 2: Number of best ranks and normalized run-times for the attractive and mixed grids, and
the more diﬃcult protein SCP problems. S, N, and B denote the STRIPES, Nilsson, and BMMF
algorithms. Algorithms marked with +R denote that regions of variables were added for those runs.

solver can be plugged into it, on the condition that it is capable of solving the arg max in
the CalcNextBestSolution subroutine. The correctness of PES can be shown by observing
that at the M th stage, all previous best solutions are excluded from the optimization and
no other assignment is excluded. Of note, this simple partitioning scheme is possible due
to the observation that the ﬁrst-best and second-best MAP assignments must diﬀer in the
assignment of at least one variable in the graph.

The main computational step of the PES algorithm is to maximize f (x; θ) sub ject to
x 6= x(∗) and x ∈ CONSTRAINTS (see the CalcNextBestSolution subroutine). The
CONSTRAINTS set merely enforces that some of the coordinates of x are either equal
to or diﬀerent from speciﬁed values.6 Within the LP, these can be enforced by setting
µi (xi = a) = 1 or µi (xi = a) = 0.
It can be shown that if one optimizes µ · θ with
these constraints and µ ∈ ˆM(G, x(∗) ), the solution is integral. Thus, the only element
requiring approximation in the general case is the description of ˆM(G, x(∗) ). We choose as
this approximation the polytope ˆMS T
L (G, x(∗) ) in Eq. 9. We call the resulting approxima-
tion algorithm Spanning TRee Inequalities and Partitioning for Enumerating Solutions, or
STRIPES. In the next section, we evaluate this scheme experimentally.

5 Experiments

We compared the performance of STRIPES to the BMMF algorithm [19] and the
Lawler/Nilsson algorithm [6, 8]. Nilsson’s algorithm is equivalent to PES where the 2nd
best assignment is obtained from maximizations within O(n) partitions, so that its run-
time is O(n) times the cost of ﬁnding a single MAP. Here we approximated each MAP
with its LP relaxation (as in STRIPES), so that both STRIPES and Nilsson come with
certiﬁcates of optimality when their LP solutions are integral. BMMF relies on loopy BP to
approximate the M best solutions.7 We used M = 50 in all experiments. To compare the
algorithms, we pooled all their solutions, noting the 50 top probabilities, and then counted
the fraction of these that any particular algorithm found (its solution rank). For run-time
comparisons, we normalized the times by the longest-running algorithm for each example.

We begin by considering pairwise MRFs on binary grid graphs of size 10 × 10. In the ﬁrst
experiment, we used an Ising model with attractive (submodular) potentials, a setting in
which the pairwise LP relaxation is exact [14]. For each grid edge ij , we randomly chose
Jij ∈ [0, 0.5], and local potentials were randomized in the range ±0.5. The results for 25
graphs are shown in Fig. 2. Both the STRIPES and Nilsson algorithms obtained the 50
optimal solutions (as learned from their optimality certiﬁcates), while BMMF clearly fared
less well for some of the graphs. While the STRIPES algorithm took < 0.5 to 2 minutes
to run, the Nilsson algorithm took around 13 minutes. On the other hand, BMMF was
quicker, taking around 10 seconds per run, while failing to ﬁnd a signiﬁcant portion of the
top solutions. Overall, the STRIPES algorithm was required to employ up to 19 spanning
tree inequalities per calculation of second-best solution.

6This is very diﬀerent from the second best constraint, since setting x1 = 1 blocks al l assignments
with this value, as opposed to setting x = 1 which blocks only the assignment with all ones.
7For BMMF, we used the C implementation at http://www.cs.huji.ac.il/~ talyam/
inference.html. The LPs for STRIPES and Nilsson were solved using CPLEX.

6

Next, we studied Ising models with mixed interaction potentials (with Jij and the local po-
tentials randomly chosen in [−0.5, 0.5]). For almost all of the 25 models, all three algorithms
were not able to successfully ﬁnd the top solutions. Thus, we added regions of triplets (two
for every grid face) to tighten the LP relaxation (for STRIPES and Nilsson) and to perform
GBP instead of BP (for BMMF). This resulted in STRIPES and Nilsson always provably
ﬁnding the optimal solutions, and BMMF mostly ﬁnding these solutions (Fig. 2). For these
more diﬃcult grids, however, STRIPES was the fastest of the algorithms, taking 0.5 - 5
minutes. On the other hand, the Nilsson and BMMF algorithms took 18 minutes and 2.5 -
7 minutes, respectively. STRIPES added up to 23 spanning tree inequalities per iteration.

The protein side-chain prediction (SCP) problem is to to predict the placement of amino
acid side-chains given a protein backbone [2, 18]. Minimization of a protein energy function
corresponds to ﬁnding a MAP assignment for a pairwise MRF [19]. We employed the
dataset of [18] (up to 45 states per variable, mean approximate tree-width 50), running all
algorithms to calculate the optimal side-chain conﬁgurations. For 315 of 370 problems in
the dataset, the ﬁrst MAP solution was obtained directly as a result of the LP relaxation
having an integral solution (“easy” problems). STRIPES provably found the subsequent
top 50 solutions within 4.5 hours for all but one of these cases (up to 8 spanning trees per
calculation), and BMMF found the same 50 solutions for each case within 0.5 hours; note
that only STRIPES provides a certiﬁcate of optimality for these solutions. On the other
hand, only for 146 of the 315 problems was the Nilsson method able to complete within
ﬁve days; thus, we do not compare its performance here. For the remaining 55 (“hard”)
problems (Fig. 2), we added problem-speciﬁc triplet regions using the MPLP algorithm
[13]. We then ran the STRIPES algorithm to ﬁnd the optimal solutions. Surprisingly, it
was able to exactly ﬁnd the 50 top solutions for all cases, using up to 4 standard spanning
tree inequalities per second-best calculation. The STRIPES run-times for these problems
ranged from 6 minutes to 23 hours. On the other hand, whether running BMMF without
these regions (BP) or with the regions (GBP), it did not perform as well as STRIPES
in terms of the number of high-ranking solutions or its speed. To summarize, STRIPES
provably found the top 50 solutions for 369 of the 370 protein SCP problems.

6 Conclusion

In this work, we present a novel combinatorial ob ject ˆM(G, z ) and show its utility in
obtaining the M best MAP assignments. We provide a simple characterization of it for
tree structured graphs, and show how it can be used for approximations in non-tree graphs.
As with the marginal polytope, many interesting questions arise about the properties of
ˆM(G, z ). For example, in which non-tree cases can we provide a compact characterization
(e.g., as for the cut-polytope for planar graphs [1]). Another compelling question is in which
problems the spanning tree inequalities are provably optimal.

An interesting generalization of our method is to predict diverse solutions satisfying some
local measure of “distance” from each other, e.g., as in [2].

Here we studied the polytope that results from excluding one assignment. An intriguing
question is to characterize the polytope that excludes M assignments. We have found that
it does not simply correspond to adding M constraints I (µ, z i ) ≤ 0 for i = 1, . . . , M , so its
geometry is apparently more complicated than that of ˆM(G, z ).

Here we used LP solvers to solve for µ. Such generic solvers could be slow for large-scale
problems. However, in recent years, specialized algorithms have been suggested for solving
MAP-LP relaxations [3, 5, 9, 17]. These use the special form of the constraints to obtain
local-updates and more scalable algorithms. We intend to apply these schemes to our
method. Finally, our empirical results show that our method indeed leverages the power of
LP relaxations and yields exact M best optimal solutions for problems with large tree-width.

Acknowledgements

We thank Nati Linial for his helpful discussions and Chen Yanover and Talya Meltzer for their
insight and help in running BMMF. We also thank the anonymous reviewers for their useful advice.

7

A Proof of Theorem 1

Recall that for any µ ∈ M(G), there exists a probability density p(x) s.t. µ = Px p(x)v (x).
Denote pµ (z ) as the minimal value of p(z ) among all p(x) that give µ. We prove that
pµ (z ) = max(0, I (µ, z )), from which the theorem follows (since pµ (z ) = 0 iﬀ µ ∈ ˆM(G, z )).

The proof is by induction on n. For n = 1, the node has degree 0, so I (µ, z) = µ1 (z1 ).
Clearly, pµ (z ) = µ1 (z1 ), so pµ (z ) = I (µ, z ).
For n > 1, there must exist a leaf in G
(assume that its index is n and its neighbor’s is n − 1). Denote ˆG as the tree obtained
by removing node n and its edge with n − 1. For any assignment x, denote ˆx as the
corresponding sub-assignment for the ﬁrst n − 1 variables. Also, any µ can be derived by
adding appropriate coordinates to a unique ˆµ ∈ M( ˆG). For an integral vertex µ = v(x),
denote its pro jected ˆµ as ˆv( ˆx). Denote by ˆI ( ˆµ, ˆz) the functional in Eq. 5 applied to ˆG. For
any µ and its pro jected ˆµ, it can be seen that:
I (µ, z ) = ˆI ( ˆµ, ˆz) − α
(11)
where we deﬁne α = Pxn 6=zn
µn−1,n (zn−1 , xn ) (so 0 ≤ α ≤ 1). The inductive assumption
gives a ˆp( ˆx) that has marginals ˆµ and also ˆp(ˆz ) = max(0, I ( ˆµ, ˆz )). We next use ˆp( ˆx) to
construct a p(x) that has marginals µ and the desired minimal pµ (z). Consider three cases:
I. I (µ, z ) ≤ 0 and ˆI ( ˆµ, ˆz) ≤ 0. From the inductive assumption, ˆp ˆµ (ˆz ) = 0, so we deﬁne:
µn−1,n (xn−1 , xn )
µn−1 (xn−1 )
which indeed marginalizes to µ, and p(z ) = 0 so that pµ (z ) = 0 as required. If µn−1 (xn−1 ) =
0, then ˆp( ˆx) is necessarily 0, in which case we deﬁne p(x) = 0. Note that this construction
is identical to that used in proving that ML (G) = M(G) for a tree graph G.
II. I (µ, z ) > 0. Based on Eq. 11 and α ≥ 0, we have ˆI ( ˆµ, ˆz) > 0. Applying the inductive
assumption to ˆµ, we obtain ˆI ( ˆµ, ˆz) = ˆp ˆµ (ˆz ) > 0. Now, deﬁne p(x) so that p(z ) = I (µ, z):

p(x) = ˆp( ˆx)

(12)

xl , l ≤ n − 2

δ(xn−1 = zn−1 )

δ(xn = zn )

p(x)

no constraint

∃ l xl 6= zl

∀ l xl = zl

0

1

1

no constraint

As in Eq. 12

0

1

0

1

0

ˆp( ˆx)

µn−1,n (zn−1 , xn )

I (µ, z )

Simple algebra shows that p(x) is non-negative and has µ as marginals. We now show that
p(z ) is minimal. Based on the inductive assumption and Eq. 11, it can easily be shown
that I (v (z ), z) = 1, I (v (x), z ) ≤ 0 for x 6= z . For any p(x) s.t. µ = Px p(x)v(x), from
linearity, I (µ, z ) = p(z) + Px6=z p(x)I (v (x), z ) ≤ p(z) (since I (v (x), z) ≤ 0 for x 6= z ).
Since the p(z) we deﬁne achieves this lower bound, it is clearly minimal.

III. I (µ, z) ≤ 0 but ˆI ( ˆµ, ˆz) > 0. Applying the inductive assumption to ˆµ, we see that
ˆp ˆµ (ˆz ) = ˆI ( ˆµ, ˆz ) > 0; Eq. 11 implies α − ˆI ( ˆµ, ˆz) ≥ 0. Deﬁne β = µn−1 (zn−1 ) − ˆp ˆµ (ˆz ), which
is non-negative since µn−1 (zn−1 ) = ˆµn−1 (ˆzn−1 ) and ˆp marginalizes to ˆµ. Deﬁne p(x) as:

xl , l ≤ n − 2

δ(xn−1 = zn−1 )

δ(xn = zn )

p(x)

no constraint

∃ l xl 6= zl

∀ l xl = zl

0

1

1

no constraint

0

1

0

1

As in Eq. 12
α− ˆI ( ˆµ , ˆz )
ˆp( ˆx) µn−1,n (zn−1 ,xn )
β
α
ˆp( ˆx) µn−1,n (zn−1 ,zn )
β
ˆI ( ˆµ, ˆz ) µn−1,n (zn−1 ,xn )
α
0

which indeed marginalizes to µ, and p(z) = 0 so that pµ (z) = 0, as required.

8

References

[1] F. Barahona. On cuts and matchings in planar graphs. Math. Program., 60(1):53–68, 1993.

[2] M. Fromer and C. Yanover. Accurate prediction for atomic-level protein design and its ap-
plication in diversifying the near-optimal sequence space. Proteins: Structure, Function, and
Bioinformatics, 75:682–705, 2009.

[3] A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms
for MAP LP-relaxations. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 21. MIT Press, Cambridge, MA, 2007.

[4] E. Kloppmann, G. M. Ullmann, and T. Becker. An extended dead-end elimination algorithm
to determine gap-free lists of low energy states. Journal of Comp. Chem., 28:2325–2335, 2007.

[5] N. Komodakis and N. Paragios. Beyond loose LP-relaxations: Optimizing MRFs by repairing
cycles. In D. Forsyth, P. Torr, and A. Zisserman, editors, ECCV, pages 806–820, Heidelberg,
Germany, 2008. Springer.

[6] E. L. Lawler. A procedure for computing the K best solutions to discrete optimization problems
and its application to the shortest path problem. Management Science, 18(7):401–405, 1972.

[7] I. Ljubic, R. Weiskircher, U. Pferschy, G. W. Klau, P. Mutzel, and M. Fischetti. An algorithmic
framework for the exact solution of the prize-collecting steiner tree problem. Mathematical
Programming, 105:427–449, Feb 2006.

[8] D. Nilsson. An eﬃcient algorithm for ﬁnding the M most probable conﬁgurations in proba-
bilistic expert systems. Statistics and Computing, 8:159–173, Jun 1998.

[9] P. Ravikumar, A. Agarwal, and M. Wainwright. Message-passing for graph-structured linear
programs: proximal pro jections, convergence and rounding schemes.
In Proc. of the 25th
international conference on Machine learning, pages 800–807, New York, NY, USA, 2008.
ACM.

[10] E. Santos. On the generation of alternative explanations with implications for belief revision.
In Proc. of the 7th Annual Conference on Uncertainty in Artiﬁcial Intel ligence, 1991.

[11] Y. Shimony. Finding the MAPs for belief networks is NP-hard. Aritiﬁcal Intel ligence,
68(2):399–410, 1994.

[12] D. Sontag and T. Jaakkola. New outer bounds on the marginal polytope. In J. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages
1393–1400. MIT Press, Cambridge, MA, 2007.

[13] D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations
for MAP using message passing. In Proc. of the 24th Annual Conference on Uncertainty in
Artiﬁcial Intel ligence, pages 503–510, 2008.

[14] B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and
bregman pro jections. J. Mach. Learn. Res., 7:1627–1653, 2006.

[15] M. Wainwright and M. Jordan. Graphical models, exponential families, and variational infer-
ence. Found. Trends Mach. Learn., 1(1-2):1–305, 2008.

[16] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log
partition function. IEEE Transactions on Information Theory, 51(7):2313–2335, 2005.

[17] T. Werner. A linear programming approach to max-sum problem: A review. IEEE Trans.
Pattern Anal. Mach. Intel l., 29(7):1165–1179, 2007.

[18] C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation
– an empirical study. Journal of Machine Learning Research, 7:1887–1907, 2006.

[19] C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief prop-
agation. In Advances in Neural Information Processing Systems 16. MIT Press, Cambridge,
MA, 2004.

[20] J. Yedidia, W. W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and
generalized belief propagation algorithms. IEEE Trans. on Information Theory, 51(7):2282–
2312, 2005.

9


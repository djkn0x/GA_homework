Face Tracking in Video

Hamidreza Khazaei and Pegah Tootoonchi Afshar
Stanford University
350 Serra Mall
Stanford, CA 94305, USA

I . INTRODUC T ION

has detected.

Object tracking is a hot area of research, and has
many practical applications. These applications include
navigation, security, robotics, vehicular communications,
and etc. It has also found applications in biology, where
people are studying cells and mitosis. One of the biggest
applications of object tracking is being able to track a
face.

In a face tracking application, Boosting and cascading
detectors have gained great popularity due to the their
efﬁciency in selecting features for faces. However these
detectors suffer from high miss-classiﬁcation rates. In
addition they depend on the orientation of the face. They
require that the face be in a full frontal position. If there
is any deviations from this position (the face is tilted by
45 degrees, or the face turns to a proﬁle position) the
AdaBoost fails to detect a face.

We will implement a face tracking algorithm, and
compare it to other well known tracking algorithm pre-
sented in the literature. Speciﬁcally we will be enhancing
the AdaBoost algorithm, and comparing the resulting
system to one that uses AdaBoost only. There are three
components to our algorithm: image representation, ap-
pearance model, and motion model. Image representation
is done using Haar like features. These features are
calculated efﬁciently using integral image representa-
tion, which allows the computation of these features
to be done in constant time. Once the features have
been attained the face detection is performed using two
classiﬁers, the AdaBoost algorithm creates a classiﬁer by
combining a set of weak classiﬁers. The motion model
tracks the aspect ratio, and the center of the face that it

The paper is organized as follows. Section II will
provide the necessary background and present a detailed
description of our model, and will provide the necessary
background information. Section IV will present
the
results. Section V will serve as a conclusion, and will
discuss possible future works.

I I . BACKGROUND AND SY ST EM MOD EL

A. Haar features

The features used to represent the pictures are the haar
features that were originally proposed by Papageorgiou
et al.
[1]. There are three types of rectangular features
that can be computed. For example a two-rectangular
feature can be computed by the difference between the
sum of two rectangular blocks. Viola-Jones proposed
a fast method for computing these features making
the features an attractive option for face detection [2].
Their method, which is called integral image and can
be calculated with one pass over the picture, uses an
intermediate array to store a running sum of pixel above
(cid:88)
and to the left of the point x,y:
i(cid:48) (x(cid:48) , y (cid:48) ),
x(cid:48)≤x,y (cid:48)≤y

ii(x, y) =

(1)

where ii(x, y) is the integral image, and i(cid:48) (x, y) is
the original image. Now using this representation for
the image any rectangular sum, and thus haar features,
can be computed efﬁciently. For example the rectangular
region D in ﬁgure 1 can be computed by ii(4) − ii(3) −
ii(2) + ii(1).

is constructed by combining well-performed weak clas-
siﬁers. It is shown that the training error of the strong
classiﬁer approaches zero exponentially in the number
of rounds [3]. The AdaBoost algorithm is summarized
below.

• Given training examples (xi ,yi ) where yi = 0, 1 for
negetive and positive examples respectively.
• Initialize weights w1,i = 1
for yi = 0, 1
2m , 1
2l
respectively, where m and l are the number of
negative and positive examples respectively.
• for t = 1,...,T
– Normalize the weights wt,i = wt,i(cid:80)
j wt,j
(cid:80)
– Train each classiﬁer hj corresponding to fea-
ture fj . The weighted error with respect to wt
i wi |hj (xi ) − yi |
is j =
– Choose the classiﬁer ht with the lowest error
t .
– Update the weights, wt+1,i = wt,iβ 1−ei
, where
t
ei = 0 if example is classiﬁed correctly and 1,
1
otherwise; And β = t
1−t
(cid:80)
(cid:80)
• the ﬁnal strong classiﬁer is
t αtht (x) ≥ 1
t αt
2
0 otherwise
C. Background-Subtraction

h(x) =

Given a frame sequence from a ﬁxed camera,
detecting all
the foreground objects is based on the
difference between the current frame and the stored
(and frequently updated ) image of the background:

if

|F rameij
Backgroundij | ≥ T hreshouldij
F rameij is foreground
else
F rameij is background

The threshold was chosen proportional to the variance
of pixel values of the current frame (for a more complex
model each pixel could have a different
threshold
based on its own gradual changes). After detecting the
foreground objects, the background image is updated as
follows:

Fig. 1. Demonstration of the way integral image is used to compute
the sum of pixels in a rectangle.

B. AdaBoost

The number of Haar-like features in each window is
much more than the number of pixels, and it is not possi-
ble to efﬁciently classify the image using all the features.
However, an effective classiﬁers can be created by using
only a small subset of these features. The algorithm
used in this work is the AdaBoost [3] algorithm which
extracts the essential features to create a well developed
classiﬁer. The AdaBoost algorithm creates a classiﬁer by
combining a set of ”weak classiﬁers”, which are trained
on only one feature.

We can explain the algorithm in two steps. First, each
weak classiﬁer optimizes its performance, i.e. minimizes
the classiﬁcation error over the training set. In this
application, a weak classiﬁer has a feature (fi ), threshold
(θi ) and a parity (pi ). It tries to ﬁnd the optimal threshold
that correctly classiﬁes the maximum possible number of
examples. The parity is used to determine the direction
x if pj fj ≤ pj θj
of inequality sign.
0
otherwise
After the ﬁrst round of learning, a weight is assigned
to each training example (in the ﬁrst round they were all
equally weighted). In the next step, a strong classiﬁer

hj (x) =

Fig. 2. Overview of the face tracking system.

if F rameij is detected as background
Backgroundij = α F rameij + 1-α Backgroundij
else
Backgroundij = Backgroundij

Where α is the updating rate which was chosen
to be a small value. Furthermore in order to eliminate
the noise from the subtracted foreground image we
used the common low pass ﬁlters in image processing
called dilation and erosion ﬁlters. Dilation, in general,
increases the size of objects and erosion causes objects
to shrink in size. The amount and the way that they
affect the objects depend on their structuring elements.
To explain them simply, dilation take each binary object
pixel (value 1) and set all background pixels (value 0)
that are connected to it (determined by the structuring
element) to value 1; but erosion takes each binary object
pixel (value 1) which is connected to background pixels

and set its value to 0. [5]

D. Overall Model

In order to display the results the face tracking system
draws a rectangle around the region that it has recognized
to be a face. The system will maintain a set of parame-
ters, which are the center, width, height and the aspect
ratio of the rectangle from the previous frame and the
current frame.

The system Initializes when the AdaBoost classiﬁer
detects the ﬁrst instance of a face, and initialize all
the parameters of the system accordingly. In the next
frame the AdaBoost classiﬁer will try to draw a rectangle
around the position of the face. If this classiﬁer could
not ﬁnd a face or if the rectangle’s parameters are
signiﬁcantly different from the previous rectangle, then
the background-subtraction algorithm that was describe
in section II-C will process the image, and remove the

background. The system will then draw a number of
random rectangles, Nrectang les , near the vicinity of the
previous center. Within each rectangle the sum of the
pixels is calculated, and the rectangle with the maxi-
mum sum is selected. The parameters are then updated
using this rectangle, so they can be used in the next
frame. An overview of the entire system can be seen in
ﬁgure˜refﬁg:SystemModel.

I I I . EX P ER IM EN T SE TU P

A 480 x 720 video, with 1803 frames was used
to test the system. As described in section II-D when
the parameters of the current rectangle are signiﬁcantly
different from the parameters of the previous rectan-
gle, the background subtraction algorithm is used. Two
criterions were used to determine whether the previous
rectangle and the current rectangle are different. First,
the difference between the centers of the rectangles were
calculated, and if the difference was greater than ten
percent of the video width (720) in the x-direction, and
ten percent of the video height (480) in the y direction,
then the two rectangles were classiﬁed as being signiﬁ-
cantly different. Secondly, if the aspect ratio of the two
rectangles differ by more than ten percent, then the two
rectangles were classiﬁed as being signiﬁcantly different.
The threshold γ that was described in section II-C was
set to be sixty percent of the variance of the picture.
This threshold was selected to take into consideration
the change in lighting of each frame. And ﬁnally, the
Nrectang les parameter described in section II-D was set
to ten.

TABLE I
TAB LE COM PAR ING TH E ADABOO ST A LGOR I THM , AND TH E
A LGOR I THM DE SCR IB E S BY TH I S PA P ER

AdaBoost only
Algorithm
Proposed Algorithm

Number of frames
Percent
missed (total of 1803 frames). missed
726
40.3%

193

10.7%

detection and a false positive detection. In other words
we consider a missed frame as either a false positive or
no detection.

V. CONC LU S ION S

The face tracking algorithm proposed by this paper
does very well, and it was shown that it can enhance
performance signiﬁcantly. The only problem with this
algorithm is that it is slow, which means face tracking
cannot be done in real-time. Currently, the face tracking
system is implemented in Mat-lab, and reading in a
frame and processing it
takes a long time. In order
to increase the speed of the system the algorithm can
be implemented in C++, where reading and writing
ﬁles can be done with ease. Another modiﬁcation that
can be made, is to reduce the number of times the
AdaBoost classiﬁer is used to detect the face. Instead
of having the AdaBoost classiﬁer look at each frame,
the algorithm could be modiﬁed so that
it calls the
AdaBoost classiﬁer every ten or twenty frames. These
implementation changes could be laid out in the future
to increase the speed of the face tracking algorithm.

IV. R E SU LT S

R E F ER ENC E S

The face tracking system was tested using the ex-
perimental setup described above, and was compared
to a face tracking system that only uses an Ada-Boost
classiﬁer to track faces. The results are summarized in
table I. As it can be seen our face tracking system out
performed the Ada-Boost face tracking system. We have
managed to reduce the errors from 726 frames to 193
frames which is a signiﬁcant number. It must be noted
that the results do not make a distinction between no

[1] C. Papageorgiou, M. Oren, and T. Poggio, “A general frame-
work for object detection”, International Conference on Com-
puter Vision, 1998.
[2] P. Viola and M. Jones, “Robust Real-time Object Detection”,
Second International Workshop on Statistical and Computa-
tional Theories of Vision - Modeling, Learning, Computing,
and Sampling, 2001.
[3] Yoav Freund and Robert E. Schapire. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
In Computational Learning Theory: Eurocolt 95, pages 2337.
Springer-Verlag, 1995

[4] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, ”Detecting
moving objects, ghosts and shadows in video streams”, IEEE
Trans. on Patt. Anal. and Machine Intell., vol. 25, no. 10, Oct.
2003, pp. 1337-1342.
[5] ”Image Processing Fundamentals”, Delft University of Technol-
ogy.


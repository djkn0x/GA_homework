Learning Non-Linear Combinations of Kernels

Corinna Cortes
Google Research
76 Ninth Ave
New York, NY 10011
corinna@google.com

Mehryar Mohri
Courant Institute and Google
251 Mercer Street
New York, NY 10012
mohri@cims.nyu.edu

Afshin Rostamizadeh
Courant Institute and Google
251 Mercer Street
New York, NY 10012
rostami@cs.nyu.edu

Abstract

This paper studies the general problem of learning kernels based on a polynomial
combination of base kernels. We analyze this problem in the case of regression
and the kernel ridge regression algorithm. We examine the corresponding learning
kernel optimization problem, show how that minimax problem can be reduced to a
simpler minimization problem, and prove that the global solution of this problem
always lies on the boundary. We give a projection-based gradient descent algo-
rithm for solving the optimization problem, shown empirically to converge in few
iterations. Finally, we report the results of extensive experiments with this algo-
rithm using several publicly available datasets demonstrating the effectiveness of
our technique.

1 Introduction

Learning algorithms based on kernels have been used with much success in a variety of tasks [17,19].
Classiﬁcation algorithms such as support vector machines ( SVMs) [6, 10], regression algorithms,
e.g., kernel ridge regression and support vector regression (SVR) [16, 22], and general dimensional-
ity reduction algorithms such as kernel PCA (KPCA) [18] all bene ﬁt from kernel methods. Positive
de ﬁnite symmetric (PDS) kernel functions implicitly speci fy an inner product in a high-dimension
Hilbert space where large-margin solutions are sought. So long as the kernel function used is PDS,
convergence of the training algorithm is guaranteed.

However, in the typical use of these kernel method algorithms, the choice of the PDS kernel, which
is crucial to improved performance, is left to the user. A less demanding alternative is to require
the user to instead specify a family of kernels and to use the training data to select the most suitable
kernel out of that family. This is commonly referred to as the problem of learning kernels.

There is a large recent body of literature addressing various aspects of this problem, including de-
riving efﬁcient solutions to the optimization problems it g enerates and providing a better theoretical
analysis of the problem both in classiﬁcation and regressio n [1, 8, 9, 11, 13, 15, 21]. With the excep-
tion of a few publications considering in ﬁnite-dimensiona l kernel families such as hyperkernels [14]
or general convex classes of kernels [2], the great majority of analyses and algorithmic results focus
on learning ﬁnite linear combinations of base kernels as originally considered by [12]. However,
despite the substantial progress made in the theoretical understanding and the design of efﬁcient
algorithms for the problem of learning such linear combinations of kernels, no method seems to re-
liably give improvements over baseline methods. For example, the learned linear combination does
not consistently outperform either the uniform combination of base kernels or simply the best single
base kernel (see, for example, UCI dataset experiments in [9, 12], see also NIPS 2008 workshop).
This suggests exploring other non-linear families of kernels to obtain consistent and signiﬁcant
performance improvements.

Non-linear combinations of kernels have been recently considered by [23]. However, here too,
experimental results have not demonstrated a consistent performance improvement for the general

1

learning task. Another method, hierarchical multiple learning [3], considers learning a linear combi-
nation of an exponential number of linear kernels, which can be efﬁciently represented as a product
of sums. Thus, this method can also be classiﬁed as learning a non-linear combination of kernels.
However, in [3] the base kernels are restricted to concatenation kernels, where the base kernels
apply to disjoint subspaces. For this approach the authors provide an effective and efﬁcient algo-
rithm and some performance improvement is actually observed for regression problems in very high
dimensions.

This paper studies the general problem of learning kernels based on a polynomial combination of
base kernels. We analyze that problem in the case of regression using the kernel ridge regression
(KRR) algorithm. We show how to simplify its optimization problem from a minimax problem
to a simpler minimization problem and prove that the global solution of the optimization problem
always lies on the boundary. We give a projection-based gradient descent algorithm for solving this
minimization problem that is shown empirically to converge in few iterations. Furthermore, we give
a necessary and sufﬁcient condition for this algorithm to re ach a global optimum. Finally, we report
the results of extensive experiments with this algorithm using several publicly available datasets
demonstrating the effectiveness of our technique.

In Section 2, we introduce the non-linear family of kernels
The paper is structured as follows.
considered. Section 3 discusses the learning problem, formulates the optimization problem, and
presents our solution. In Section 4, we study the performance of our algorithm for learning non-
linear combinations of kernels in regression (NKRR) on several publicly available datasets.

2 Kernel Family

(1)

µk1 ···kp ≥ 0.

µk1 ···kp K k1
1 · · · K kp
p ,

This section introduces and discusses the family of kernels we consider for our learning kernel
problem. Let K1 , . . . , Kp be a ﬁnite set of kernels that we combine to de ﬁne more complex
kernels.
We refer to these kernels as base kernels. In much of the previous work on learning kernels, the
family of kernels considered is that of linear or convex combinations of some base kernels. Here,
we consider polynomial combinations of higher degree d ≥ 1 of the base kernels with non-negative
coefﬁcients of the form:
X
Kµ =
0≤k1+···+kp≤d, ki≥0, i∈[0,p]
Any kernel function Kµ of this form is PDS since products and sums of PDS kernels are PDS [4].
1 · · ·K kp
Note that Kµ is in fact a linear combination of the PDS kernels K k1
p . However, the number
of coefﬁcients µk1 ···kp is in O(pd ), which may be too large for a reliable estimation from a sample
of size m. Instead, we can assume that for some subset I of all p-tuples (k1 , . . . , kp ), µk1 ···kp can
1 · · · µkp
be written as a product of non-negative coefﬁcients µ1 , . . . , µp : µk1 ···kp = µk1
p . Then, the
general form of the polynomial combinations we consider becomes
p + X(k1 ,...,kp )∈J
K = X(k1 ,...,kp )∈I
µk1 ···kp K k1
p K k1
µk1
1 · · · K kp
1 · · · K kp
1 · · · µkp
p ,
where J denotes the complement of the subset I . The total number of free parameters is then
reduced to p + |J |. The choice of the set I and its size depends on the sample size m and possible
prior knowledge about relevant kernel combinations. The second sum of equation (2) de ﬁning our
general family of kernels represents a linear combination of PDS kernels. In the following, we
focus on kernels that have the form of the ﬁrst sum and that are thus non-linear in the parameters
µ1 , . . . , µp . More speciﬁcally, we consider kernels Kµ de ﬁned by
Kµ = Xk1+···+kp=d
µk1
p K k1
1 · · · µkp
1 · · · K kp
p ,
where µ = (µ1 , . . . , µp )⊤ ∈ Rp . For the ease of presentation, our analysis is given for the case d = 2,
where the quadratic kernel can be given the following simpler expression:
pXk,l=1
But, the extension to higher-degree polynomials is straightforward and our experiments include
results for degrees d up to 4.

Kµ =

µk µl KkKl .

(2)

(3)

(4)

2

3 Algorithm for Learning Non-Linear Kernel Combinations

3.1 Optimization Problem

We consider a standard regression problem where the learner receives a training sample of size
m, S = ((x1 , y1 ), . . . , (xm , ym )) ∈ (X × Y )m , where X is the input space and Y ∈ R the label
space. The family of hypotheses Hµ out of which the learner selects a hypothesis is the reproducing
kernel Hilbert space (RKHS) associated to a PDS kernel function Kµ : X × X → R as de ﬁned in
the previous section. Unlike standard kernel-based regression algorithms however, here, both the
parameter vector µ de ﬁning the kernel Kµ and the hypothesis are learned using the training sample
S .
The learning kernel algorithm we consider is derived from kernel ridge regression (KRR). Let y =
[y1 , . . . , ym ]⊤ ∈ Rm denote the vector of training labels and let Kµ denote the Gram matrix of the
kernel Kµ for the sample S : [Kµ ]i,j = Kµ (xi , xj ), for all i, j ∈ [1, m]. The standard KRR dual
optimization algorithm for a ﬁxed kernel matrix Kµ is given in terms of the Lagrange multipliers
α ∈ Rm by [16]:

max
α∈Rm
The related problem of learning the kernel Kµ concomitantly can be formulated as the following
min-max optimization problem [9]:

⊤ (Kµ + λI)α + 2α
⊤y
−α

(5)

⊤ (Kµ + λI)α + 2α
⊤y,
−α

min
max
α∈Rm
µ∈M
where M is a positive, bounded, and convex set. The positivity of µ ensures that Kµ is positive
semi-de ﬁnite (PSD) and its boundedness forms a regularizat ion controlling the norm of µ.1 Two
natural choices for the set M are the norm-1 and norm-2 bounded sets,
(7)
M1 = {µ | µ (cid:23) 0 ∧ kµ − µ0k1 ≤ Λ}
(8)
M2 = {µ | µ (cid:23) 0 ∧ kµ − µ0 k2 ≤ Λ}.
These de ﬁnitions include an offset parameter µ0 for the weights µ. Some natural choices for µ0
are: µ0 = 0, or µ0/kµ0k = 1. Note that here, since the objective function is not linear in µ, the
norm-1-type regularization may not lead to a sparse solution.

(6)

3.2 Algorithm Formulation

For learning linear combinations of kernels, a typical technique consists of applying the minimax
theorem to permute the min and max operators, which can lead to optimization problems com-
putationally more efﬁcient to solve [8, 12]. However, in the non-linear case we are studying, this
technique is unfortunately not applicable.

Instead, our method for learning non-linear kernels and solving the min-max problem in equation (6)
consists of ﬁrst directly solving the inner maximization pr oblem. In the case of KRR for any ﬁxed
µ the optimum is given by

α = (Kµ + λI)−1y.
(9)
Plugging the optimal expression of α in the min-max optimization yields the following equivalent
minimization in terms of µ only:

F (µ) = y⊤ (Kµ + λI)−1y.

min
µ∈M
We refer to this optimization as the NKRR problem. Although the original min-max problem has
been reduced to a simpler minimization problem, the function F is not convex in general as illus-
trated by Figure 1. For small values of µ, concave regions are observed. Thus, standard interior-
point or gradient methods are not guaranteed to be successful at ﬁnding a global optimum.

(10)

In the following, we give an analysis which shows that under certain conditions it is however possible
to guarantee the convergence of a gradient-descent type algorithm to a global minimum.

Algorithm 1 illustrates a general gradient descent algorithm for the norm-2 bounded setting which
projects µ back to the feasible set M2 after each gradient step (projecting to M1 is very similar).
1To clarify the difference between similar acronyms, a PDS function corresponds to a PSD matrix [4].

3

)
2
µ
,
1
µ
(
F

210

205

200

195
1

0.5

µ
1

0

1

0.5

µ
2

21

)
2
µ
,
1
µ
(
F

20.5

0

20
1

0.5

0

1

µ
1

0.5

µ
2

2.09

)
2
µ
,
1
µ
(
F

2.08

2.07

2.06
1

0

0

0.5

0

1

µ
1

0.5
µ
2

Figure 1: Example plots for F de ﬁned over two linear base kernels generated from the ﬁrst t wo
features of the sonar dataset. From left to right λ = 1, 10, 100. For larger values of λ it is clear that
there are in fact concave regions of the function near 0.

Algorithm 1 Projection-based Gradient Descent Algorithm
Input: µinit ∈ M2 , η ∈ [0, 1], ǫ > 0, Kk , k ∈ [1, p]
′ ← µinit
µ
repeat
′
µ ← µ
′ ← −η∇F (µ) + µ
µ
k ← max(0, µ′
∀k , µ′
k )
normalize µ
′ , s.t. kµ
′ − µ0k = Λ
until kµ
′ − µk < ǫ

In Algorithm 1 we have ﬁxed the step size η , however this can be adjusted at each iteration via
a line-search. Furthermore, as shown later, the thresholding step that forces µ
′ to be positive is
unnecessary since ∇F is never positive.
Note that Algorithm 1 is simpler than the wrapper method proposed by [20]. Because of the closed
form expression (10), we do not alternate between solving for the dual variables and performing a
gradient step in the kernel parameters. We only need to optimize with respect to the kernel parame-
ters.

3.3 Algorithm Properties

We ﬁrst explicitly calculate the gradient of the objective f unction for the optimization problem (10).
In what follows, ◦ denotes the Hadamard (pointwise) product between matrices.
Proposition 1. For any k ∈ [1, p], the partial derivative of F : µ → y⊤ (Kµ + λI)−1y with respect
to µi is given by

∂F
∂µk

⊤Uk α,
= −2α

(11)

yy⊤M−1⊤ , we can write:

r=1 (µrKr ) ◦ Kk (cid:1).
where Uk = (cid:0) Pp
Proof. In view of the identity ∇M Tr(y⊤M−1y) = −M−1⊤
(cid:21)
= Tr (cid:20) ∂y⊤ (Kµ + λI)−1y
∂ (Kµ + λI)
∂F
∂µk
∂µk
∂ (Kµ + λI)
= − Tr (cid:20)(Kµ + λI)−1yy⊤ (Kµ + λI)−1 ∂ (Kµ + λI)
(cid:21)
∂µk
(µrKr ) ◦ Kk (cid:17)#
= − Tr "(Kµ + λI)−1yy⊤ (Kµ + λI)−1(cid:16)2
pXr=1
(µrKr ) ◦ Kk(cid:17)(Kµ + λI)−1y = −2α
= − 2y⊤ (Kµ + λI)−1(cid:16) pXr=1
⊤Ukα.
4

≤ 0 for all i ∈ [1, p] and ∇F ≤ 0.
Matrix Uk just de ﬁned in proposition 1 is always PSD, thus
∂F
∂µk
As already mentioned, this fact obliterates the thresholding step in Algorithm 1. We now provide
guarantees for convergence to a global optimum. We shall assume that λ is strictly positive: λ > 0.
Proposition 2. Any stationary point µ
⋆ of the function F : µ → y⊤ (Kµ + λI)−1y necessarily
maximizes F :

F (µ

⋆ ) = max
µ

F (µ) =

kyk2
λ

.

⋆ ,

(12)

(13)

⋆ ) = 0. Thus,

Proof. In view of the expression of the gradient given by Proposition 1, at any point µ
pXi=1
⋆ is a stationary point, ∇F (µ
By de ﬁnition, if µ
⋆ ) = 0, which implies µ
⋆ α = 0, that is
⋆ α = 0, which implies Kµ
⊤Kµ
α

µ⋆
⊤Kµ
⋆ α.
kUkα = α

⋆⊤∇F (µ
µ

⋆⊤∇F (µ

⋆ ) = α
⊤

Kµ

⋆ (Kµ

⋆ + λI)−1y = 0 ⇔ (Kµ
⇔ y − λ(Kµ

⋆ + λI)−1y = 0
⋆ + λI − λI)(Kµ
⋆ + λI)−1 y = 0
y
λ
⋆ + λI)−1y = y⊤y
λ , which is clearly a

⋆ + λI)−1y =

(14)
(15)

(16)

.

⋆ ) = y⊤ (Kµ

⇔ (Kµ

Thus, for any such stationary point µ
maximum.

⋆ , F (µ

We next show that there cannot be an interior stationary point, and thus any local minimum strictly
within the feasible set, unless the function is constant.
Proposition 3. If any point µ
⋆ > 0 is a stationary point of F : µ → y⊤ (Kµ + λI)−1y, then the
function is necessarily constant.

⋆ > 0 is a stationary point, then, by Proposition 2, F (µ
Proof. Assume that µ
⋆ ) = y⊤ (Kµ
⋆ +
λI)−1y = y⊤ y
⋆ +λI)−1 with eigenvalue λ−1 . Equiv-
λ , which implies that y is an eigenvector of (Kµ
⋆ ).
⋆ + λI with eigenvalue λ, which is equivalent to y ∈ null(Kµ
alently, y is an eigenvector of Kµ
Thus,
pXk,l=1
mXr,s=1
yr ysKk (xr , xs )Kl (xr , xs )
{z
}
|
(∗)
Since the product of PDS functions is also PDS, (*) must be non-negative. Furthermore, since by
assumption µi > 0 for all i ∈ [1, p], it must be the case that the term (*) is equal to zero. Thus,
equation 17 is equal to zero for all µ and the function F is equal to the constant kyk2/λ.

y⊤Kµ
⋆ y =

µk µl

(17)

= 0.

The previous propositions are sufﬁcient to show that the gra dient descent algorithm will not become
stuck at a local minimum while searching the interior of a convex set M and, furthermore, they
indicate that the optimum is found at the boundary.

The following proposition gives a necessary and sufﬁcient c ondition for the convexity of F on a
convex region C . If the boundary region de ﬁned by kµ − µ0k = Λ is contained in this convex
region, then Algorithm 1 is guaranteed to converge to a global optimum. Let u ∈ Rp represent an
arbitrary direction of µ in C . We simplify the analysis of convexity in the following derivation by
separating the terms that depend on Kµ and those depending on Ku , which arise when showing
the positive semi-de ﬁniteness of the Hessian, i.e. u⊤∇2F u (cid:23) 0. We denote by ⊗ the Kronecker
product of two matrices.
Proposition 4. The function F : µ → y⊤ (Kµ + λI)−1y is convex over the convex set C iff the
following condition holds for all µ ∈ C and all u:
hM, N − e1iF ≥ 0,
5

(18)

Data
Parkinsons
Iono
Sonar
Breast

m
194
351
208
683

p
21
34
60
9

lin. base
.70 ± .03
.82 ± .03
.90 ± .02
.70 ± .02

lin. ℓ1
.70 ± .04
.81 ± .04
.92 ± .03
.71 ± .02

lin. ℓ2
.70 ± .03
.81 ± .03
.90 ± .04
.70 ± .02

quad. base
.65 ± .03
.62 ± .05
.84 ± .03
.70 ± .02

quad. ℓ1
.66 ± .03
.62 ± .05
.80 ± .04
.70 ± .01

quad. ℓ2
.64 ± .03
.60 ± .05
.80 ± .04
.70 ± .01

(19)

(20)

(21)

[Kµ ]ik [Ku ]ik [V]kl [Kµ ]ik [Kµ ]lj

⊤ (Kµ ◦ Ku )V(Kµ ◦ Ku )α =
α

Table 1: The square-root of the mean squared error is reported for each method and several datasets.
⊤ )⊤ (cid:1) ◦ (Ku ⊗ Ku ), N = 4(cid:0)1 ⊗ vec(V)⊤ (cid:1) ◦ (Kµ ⊗ Kµ ), and e1 is the
where M = (cid:0)1 ⊗ vec(αα
matrix with zero-one entries constructed to select the terms [M]ijkl where i = k and j = l , i.e. it is
non-zero only in the (i, j )th coordinate of the (i, j )th m × m block.
Proof. For any u ∈ Rp the expression of the Hessian of F at the point µ ∈ C can be derived from
that of its gradient and shown to be
u⊤ (∇2F )u = 4α
⊤ (Kµ ◦ Ku )V(Kµ ◦ Ku )α − α
⊤ (Ku ◦ Ku )α.
Expanding each term, we obtain:
mXk,l=1
mXi,j=1
αiαj
mXi,j,k,l=1
(αiαj [Ku ]ik [Ku ]lj )([V]kl [Kµ ]ik [Kµ ]lj )
⊤ (Ku ◦ Ku )α = Pm
i,j=1 αiαj [Ku ]ij [Ku ]ij . Let 1 ∈ Rm2 de ﬁne the column vector of all
and α
ones and let vec(A) denote the vectorization of a matrix A by stacking its columns. Let the matrices
M and N be de ﬁned as in the statement of the proposition. Then,
[M]ijkl = (αiαj [Ku ]ik [Ku ]lj )
and [N]ijkl = [V]kl [Kµ ]ik [Kµ ]lj . Then, in view of the de ﬁnition of e1, the terms of equation (19)
can be represented with the Frobenius inner product,
u⊤ (∇2F )u = hM, NiF − hM, e1iF = hM, N − e1iF .
For any µ ∈ Rp , let Kµ = Pi µiKi and let V = (Kµ + λI)−1 . We now show that the condition
of Proposition 4 is satisﬁed for convex regions for which Λ, and therefore µ, is sufﬁciently large, in
the case where Ku and Kµ are diagonal. In that case, M, N and V are diagonal as well and the
condition of Proposition 4 can be rewritten as follows:
Xi,j
Using the fact that V is diagonal, this inequality we can be further simpliﬁed
mXi=1
i (4[Kµ ]2
ii α2
[Ku ]2
iiVii − 1) ≥ 0.
A sufﬁcient condition for this inequality to hold is that eac h term (4[Kµ ]2
iiVii − 1) be non-negative,
µV − I (cid:23) 0, that is Kµ (cid:23) q λ
or equivalently that 4K2
3 I. Therefore, it sufﬁces to select µ such that
k=1 µk [Kk ]ii ≥ pλ/3.
mini Pp
4 Empirical Results

[Ku ]ii [Ku ]jj αiαj (4[Kµ ]ii [Kµ ]jj Vij − 1i=j ) ≥ 0.

=

(22)

(23)

To test the advantage of learning non-linear kernel combinations, we carried out a number of ex-
periments on publicly available datasets. The datasets are chosen to demonstrate the effectiveness
of the algorithm under a number of conditions. For general performance improvement, we chose a
number of UCI datasets frequently used in kernel learning experiments, e.g., [7, 12, 15]. For learning
with thousands of kernels, we chose the sentiment analysis dataset of Blitzer et. al [5]. Finally, for
learning with higher-order polynomials, we selected datasets with large number of examples such as
kin-8nm from the Delve repository. The experiments were run on a 2.33 GHz Intel Xeon Processor
with 2GB of RAM.

6

E
S
M
R

1.7

1.65

1.6

1.55

1.5

1.45

1.4
 
0

Kitchen

L2 reg.

Baseline

L1 reg.

 

1000

2000
# bigrams

3000

4000

E
S
M
R

1.7

1.65

1.6

1.55

1.5

1.45

1.4
 
0

Electronics

L2 reg.

Baseline

L1 reg.

 

1000

2000
3000
# bigrams

4000

5000

Figure 2: The performance of baseline and learned quadratic kernels (plus or minus one standard
deviation) versus the number of bigrams (and kernels) used.

4.1 UCI Datasets

We ﬁrst analyzed the performance of the kernels learned as qu adratic combinations. For each
dataset, features were scaled to lie in the interval [0, 1]. Then, both labels and features were centered.
In the case of classiﬁcation dataset, the labels were set to ±1 and the RMSE was reported. We as-
sociated a base kernel to each feature, which computes the product of this feature between different
examples. We compared both linear and quadratic combinations, each with a baseline (uniform),
norm-1-regularized and norm-2-regularized weighting using µ0 = 1 corresponding to the weights of
the baseline kernel. The parameters λ and Λ were selected via 10-fold cross validation and the error
reported was based on 30 random 50/50 splits of the entire dataset into training and test sets. For the
gradient descent algorithm, we started with η = 1 and reduced it by a factor of 0.8 if the step was
found to be too large, i.e., the difference kµ
′ − µk increased. Convergence was typically obtained
in less than 25 steps, each requiring a fraction of a second (∼ 0.05 seconds).
The results, which are presented in Table 1, are in line with previous ones reported for learning
kernels on these datasets [7, 8, 12, 15]. They indicate that learning quadratic combination kernels can
sometimes offer improvements and that it clearly does not degrade with respect to the performance
of the baseline kernel. The learned quadratic combination performs well, particularly on tasks where
the number of features was large compared to the number of points. This suggests that the learned
kernel is better regularized than the plain quadratic kernel and can be advantageous is scenarios
where over- ﬁtting is an issue.

4.2 Text Based Dataset

We next analyzed a text-based task where features are frequent word n-grams. Each base kernel
computes the product between the counts of a particular n-gram for the given pair of points. Such
kernels have a direct connection to count-based rational kernels, as described in [8]. We used the
sentiment analysis dataset of Blitzer et. al [5]. This dataset contains text-based user reviews found
for products on amazon.com. Each text review is associated with a 0-5 star rating. The product re-
views fall into two categories: electronics and kitchen-wares, each with 2,000 data-points. The data
was not centered in this case since we wished to preserve the sparsity, which offers the advantage of
signiﬁcantly more efﬁcient computations. A constant featu
re was included to act as an offset.

For each domain, the parameters λ and Λ were chosen via 10-fold cross validation on 1,000 points.
Once these parameters were ﬁxed, the performance of each alg orithm was evaluated using 20 ran-
dom 50/50 splits of the entire 2,000 points into training and test sets. We used the performance of
the uniformly weighted quadratic combination kernel as a baseline, and showed the improvement
when learning the kernel with norm-1 or norm-2 regularization using µ0 = 1 corresponding to the
weights of the baseline kernel. As shown by Figure 2, the learned kernels signiﬁcantly improved
over the baseline quadratic kernel in both the kitchen and electronics categories. For this case too,
the number of features was large in comparison with the number of points. Using 900 training points
and about 3,600 bigrams, and thus kernels, each iteration of the algorithm took approximately 25

7

KRR, with (dashed) and without (solid) learning

0.25

0.20
E
S
M
0.15

0.10

0

1st degree
2nd degree
3rd degree
4th degree

20
80
60
40
Training data subsampling factor

100

Figure 3: Performance on the kin-8nm dataset. For all polynomials, we compared un-weighted,
standard KRR (solid lines) with norm-2 regularized kernel learning (dashed lines). For 4th degree
polynomials we observed a clear performance improvement, especially for medium amount of train-
ing data (subsampling factor of 10-50). Standard deviations were typically in the order 0.005, so the
results were statistically signiﬁcant.

seconds to compute with our Matlab implementation. When using norm-2 regularization, the algo-
rithm generally converges in under 30 iterations, while the norm-1 regularization requires an even
fewer number of iterations, typically less than 5.

4.3 Higher-order Polynomials

We ﬁnally investigated the performance of higher-order non -linear combinations. For these exper-
iments, we used the kin-8nm dataset from the Delve repository. This dataset has 20,000 examples
with 8 input features. Here too, we used polynomial kernels over the features, but this time we
experimented with polynomials with degrees as high as 4. Again, we made the assumption that all
coefﬁcients of µ are in the form of products of µi s (see Section 2), thus only 8 kernel parameters
needed to be estimated.

We split the data into 10,000 examples for training and 10,000 examples for testing, and, to inves-
tigate the effect of the sample size on learning kernels, subsampled the training data so that only a
fraction from 1 to 100 was used. The parameters λ and Λ were determined by 10-fold cross vali-
dation on the training data, and results are reported on the test data, see Figure 3. We used norm-2
regularization with µ0 = 1 and compare our results with those of uniformly weighted KRR.
For lower degree polynomials, the performance was essentially the same, but for 4th degree poly-
nomials we observed a signiﬁcant performance improvement o f learning kernels over the uniformly
weighted KRR, especially for a medium amount of training data (subsampling factor of 10-50). For
the sake of readability, the standard deviations are not indicated in the plot. They were typically in
the order of 0.005, so the results were statistically signiﬁ cant. This result corroborates the ﬁnding
on the UCI dataset, that learning kernels is better regularized than plain unweighted KRR and can
be advantageous is scenarios where over ﬁtting is an issue.

5 Conclusion

We presented an analysis of the problem of learning polynomial combinations of kernels in regres-
sion. This extends learning kernel ideas and helps explore kernel combinations leading to better
performance. We proved that the global solution of the optimization problem always lies on the
boundary and gave a simple projection-based gradient descent algorithm shown empirically to con-
verge in few iterations. We also gave a necessary and sufﬁcie nt condition for that algorithm to
converge to a global optimum. Finally, we reported the results of several experiments on publicly
available datasets demonstrating the bene ﬁts of learning p olynomial combinations of kernels. We are
well aware that this constitutes only a preliminary study and that a better analysis of the optimization
problem and solution should be further investigated. We hope that the performance improvements
reported will further motivate such analyses.

8

References

[1] A. Argyriou, R. Hauser, C. Micchelli, and M. Pontil. A DC-programming algorithm for kernel
selection. In International Conference on Machine Learning, 2006.
[2] A. Argyriou, C. Micchelli, and M. Pontil. Learning convex combinations of continuously
parameterized basic kernels. In Conference on Learning Theory, 2005.
[3] F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Advances
in Neural Information Processing Systems, 2008.
[4] C. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups. Springer-
Verlag: Berlin-New York, 1984.
[5] J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Do-
main Adaptation for Sentiment Classiﬁcation. In Association for Computational Linguistics,
2007.
[6] B. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classiﬁers. In
Conference on Learning Theory, 1992.
[7] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for
support vector machines. Machine Learning, 46(1-3), 2002.
[8] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning sequence kernels. In Machine Learning
for Signal Processing, 2008.
[9] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In Uncer-
tainty in Artiﬁcial Intelligence , 2009.
[10] C. Cortes and V. Vapnik. Support-Vector Networks. Machine Learning, 20(3), 1995.
[11] T. Jebara. Multi-task feature and kernel selection for SVMs. In International Conference on
Machine Learning, 2004.
[12] G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. Jordan. Learning the kernel
matrix with semide ﬁnite programming. Journal of Machine Learning Research, 5, 2004.
[13] C. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine
Learning Research, 6, 2005.
[14] C. S. Ong, A. Smola, and R. Williamson. Learning the kernel with hyperkernels. Journal of
Machine Learning Research, 6, 2005.
[15] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. Simplemkl. Journal of Machine
Learning Research, 9, 2008.
[16] C. Saunders, A. Gammerman, and V. Vovk. Ridge Regression Learning Algorithm in Dual
Variables. In International Conference on Machine Learning, 1998.
[17] B. Sch ¨olkopf and A. Smola. Learning with Kernels. MIT Press: Cambridge, MA, 2002.
[18] B. Scholkopf, A. Smola, and K. Muller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural computation, 10(5), 1998.
[19] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, 2004.
[20] S. Sonnenburg, G. R ¨atsch, C. Sch ¨afer, and B. Sch ¨olkopf. Large scale multiple kernel learning.
Journal of Machine Learning Research, 7, 2006.
[21] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned ker-
nels. In Conference on Learning Theory, 2006.
[22] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.
[23] M. Varma and B. R. Babu. More generality in efﬁcient mult iple kernel learning. In Interna-
tional Conference on Machine Learning, 2009.

9


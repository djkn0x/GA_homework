Finding the Augmented Neural Pathways to Math Processing:
fMRI Pattern Classiﬁcation of Turner Syndrome Brains

Gary Tang
{garytang} @ {stanford} · {edu}

Abstract

The use of statistical and machine learning methods
for multi-voxel pattern analysis (MVPA) has grown
rapid ly in the past two decades. Today, sophisticated
pattern classiﬁcation techniques applied to functional
magnetic resonance images (fMRI) have al lowed re-
searchers to reliably discern diﬀerent mental states
with high accuracy. In this work, we perform MVPA
using support vector machines (SVM) to study the
neural activity associated to math processing in a clin-
ical population with cognitive deﬁciencies in math.
Namely, we studied the eﬀect of training to reha-
bilitate Turner Syndrome patients and its ability to
augment the neural pathways associated of math pro-
cessing. Applying a modiﬁed form of the canonical
recursive feature elimination (RFE) algorithm to spa-
tial and spatial-temporal formulations of the data, we
successful ly classiﬁed the two groups with great pre-
dictive accuracy. Unfortunately, the resulting clas-
siﬁers were unable to demonstrate that post-training
images were more similar than pre-training images to
a healthy individual with any statistical signiﬁcance.

Introduction

Suﬀerers of mental or cognitive disorders have tra-
ditionally been diagnosed using symptom-based cri-
teria. Even common disorders, such as depression,
are not diagnosed in a rigorous scientiﬁc manner,
but rather through a checklist of known related-
symptoms deﬁned in a medical manual. A rigorous
and precise measure of cognitive function and activity
has long been the holy grail for psychiatrists and neu-
roscientist. Today, many believe neuro-imaging, such
as magnetic resonance imaging (MRI) and computed
tomography (CT), to be the prophetic technology.
In particular, functional magnetic resonance imaging
(fMRI) has emerged as one of the most popular tools
to map of neural activity to mental states. These
images track the hemodynamic response (HR) of the
brain as a correlate to neural activity and its associ-

ated mental state. For this pro ject, we are concerned
with the set of neural sequences that suggest a clin-
ical brain is performing a math-related task. While
it’s generally understood how healthy brains perform
math, it remains unclear how brains suﬀering from
cognitive deﬁciencies process math. One such pop-
ulation are individuals with Turner Syndrome. This
condition, aﬀecting 1 in 2500 girls, leads to diﬀerent
types of cognitive impairment, including poorer than
average math skills.

Problem Statement

Members of the Stanford School of Medicine con-
ducted a study to rehabilitate the mathematical abil-
ities of individuals with Turner Syndrome through
a series of training exercises that taught students
to mimic the way a healthy individual processes
math. Testing before and after the program suggests
a statistically signiﬁcant improvement in math per-
formance, but a univariate analysis of the fMRI data
found no identiﬁable diﬀerence between pre-training
and post-training brain activity. The overall goal of
this pro ject is to determine which,
if any, regions
of the brain can be used to discriminate between
pre-training and post-training images, and by corol-
lary, identify the possibly new, math-related, neu-
ronal pathways that were generated as a result of the
training.

Ob jective

We separate and deﬁne the ob jective into two compo-
nents. Firstly, we wish ﬁnd a reliable measure, in this
case a classiﬁer, with which we can claim that a dif-
ference exists between pre-training and post-training
images. We must take steps to ensure that the clas-
siﬁer is well-generalized such that it can predict on
persons from whom we have no prior data, what this
paper refers to as disjoint data, which is acknowl-
edged to be a much harder task1 than simply predict-
ing on data left out of the training set. The second

1

part is simply to apply that measure, either directly
or indirectly, to determine if the post-training images
look more similar to that of healthy individuals. The
classiﬁer chosen and the data ob jects that we are clas-
sifying is problem can be formulated in diﬀerent ways
which will be presented in later sections.

Materials

Data

The fMRI data comes from eleven Turner Syndrome
patients and ﬁve control patients who were imaged
while performing a block test. During this test, im-
ages were sampled every two seconds over 14 blocks,
which span 420s in total. Each block is either a 30s
math task or a 30s control task, where a control block
follows a math block. Each image is composed of
31608, 4mm voxels. Each Turner Syndrome patient
performed two block tests, corresponding to a pre-
training date and a post-training data. The two block
tests were separated by approximately six months.

Computational Tools

The images were processed by the MATLAB toolbox
SPM5 and a custom extension authored by Profes-
sor Fumiko Hoeﬀt. The optimizer CVX6 was used
to solve the SVM optimization problem. It primarily
employs a interior point method whose complexity is
either constant or weakly proportional to the num-
ber of features. This feature is critical to our prob-
lem since the data from each image is converted to
a single feature space with dimensions on the order
O (104 ). All additional programming development
was performed inside the MATLAB environment

Approach

For this pro ject we employ the support vector ma-
chine (SVM) learning algorithm for its robustness and
proven record of eﬀectiveness. More speciﬁcally, we
employ a linear SVM since neither the author nor
the existing literature? has found a signiﬁcant perfor-
mance gain in the use of nonlinear SVM. And while
we mainly use the algorithm in its classical form,
we explore diﬀerent feature deﬁnitions, feature se-
lection techniques, and pre-processing strategies that
are best suited to handle our particular application.

Challenges and Strategies to Analyzing
fMRI

Overﬁtting

If the ultimate goal for neuroscientists/psychiatrists
to use classiﬁers for diagnosis, they need to have the
ability to generalize well. But for scientists who study
fMRI, the problem is further compounded by the fact
that the training size m is much smaller than the fea-
ture space dimension n. An SVM applied under these
conditions will tend to overﬁt the training data, re-
sulting in poor generalization. And while soft-max
regularization helps alleviate this issue, many believe
it is essential to explicitly remove features, pruning
noisy and uninformative features.1, 2 Therefore, in
this pro ject we apply a modiﬁed form of the canoni-
cal recursive feature elimination (RFE) that seeks to
further improve the generalization. Details can be
found in later in the report.

Data Quality I

While much of the fMRI data is noisy and uninforma-
tive, this is consequence after the data is in a usable
form (e.g. greyscale numbers from an image). That
is to say, some factors aﬀecting the quality of the
data are outside the control of the statistician or the
analyst. The ability of capturing a proper fMRI is a
tenuous one. Patients often move, the fMRI machines
experience drift, etc. In order to obtain a useful fMRI
volume, it must ﬁrst be motion corrected, repaired,
transformed onto a template, and smoothed. Not
only will this aﬀect the signal that is ultimately sent
to the SVM, but often times the images are simply
not salvageable and their inclusion into a training set
will adversely aﬀect the performance of the classiﬁer.
One example is when the ”pre-pre-processing” (be-
fore our pre-processing, see Data Quality II) results
in a lot of redundant data. In general, the SVM is
agnostic to redundant data so long as they are consis-
tent (i.e. the labeling of redundant data is the same).
But manual inspection indicated that the redundant
data is not consistent. Evidenced by poorer predic-
tion rates in a preliminary study, these datasets were
left out when producing the ﬁnal results. Because we
posit that each individual is statistically representa-
tive of the population we seek to classify, we did not
preclude the use of an individual’s data obtained if
data obtained at a diﬀerent time is usable (e.g.
if
sub ject A had a poor test 1 but a usable test 2, that
data is included in our pooled set of data).

2

Data Quality II

Once useful a useful training set is selected, the data
often remains inundated with noise and other addi-
tive signals (e.g. drift). We address this with several
strategies ﬁltering/detrending, masking and the use
of principal components.

De-trending

Because of the data acquired by the fMRI machine
experiences a signiﬁcant amount of drift, the drift is
regressed out by a quadratic regression model. This
could also be performed using a high pass ﬁlter, but
using this particular form of de-trending reduces the
possibility of removing any informative modes.

Low Pass Filtering

It is well known within the neuroscience community
that fMRI signal is very noisy.1 The author’s own
preliminary study indicatesthat classiﬁcation is much
more diﬃcult without the use of a low pass ﬁlter.We
used a discrete cosine transform (DCT) to decom-
pose the modes and perform the ﬁltering. Because
the signal of interest has a theoretical frequency of
about 0.03hz, and motivated by a survey of the spec-
tral distribution of our data, we use a 0.1hz cut oﬀ
threshold. It is important to note that the survey is
best performed on voxels that lie inside the region of
interest (ROI). Otherwise, a random survey of unin-
formative nodes can erroneously infer that there is no
underlying signal or similarly, there is no discernable
diﬀerence between noise and signal.

Science-Driven Feature Reduction: Masking

The use of a mask immediately eliminates features
that are deﬁnitively known to lack information (e.g.
grey matter). A common practice, however, is also to
mask the regions of the brain that are hypothesized
a priori to carry no informative information. In this
study, we try both approaches: a minimalist mask,
and a mask that includes only the ROI. The results,
as expected, are strongly dependent on the choice of
mask since most, and also in our case, fMRI analy-
ses are performed with very few samples. As most
learning and statistical methods rely on the assump-
tion that the data is representative of the population,
when applied to small samples algorithms often ﬁnd
a solution that is strongly dependent on the speciﬁc
training data and unrepresentative of reality, namely,
science.

Principal Component Analysis (PCA)
The principal components highlight those features
whose variance is high, likely indicating a signal or
an informative voxel. This has the eﬀect of further
de-noising the data matrix before going it into the
SVM algorithm. If ﬁltering is applied, one should be
careful to examine the eigenvalues or squared singu-
lar values on a scree plot to ensure that the proper
number of principal components are taken, or rather,
that enough are taken.
In this pro ject, we essen-
tially take all principal components, only removing
those that are likely the result of numerical round oﬀ
(e.g. λi > 10−6 ).
It also has the eﬀect of remov-
ing redundant data (by virtue of the fact that the
maximum number of principal components does not
exceed the rank of the matrix in our case) which is
good from both a classiﬁcation point of view (con-
ﬂicting redundant data) and from a computational
point of view since we now are required to process less
data while extracting an equivalent amount of infor-
mation. Applying this within RFE has the eﬀect of
subset-matrix-speciﬁc de-noising. Furthermore, be-
cause RFE is such a computationally intensive pro-
cedure, it is even more important to have an eﬃcient
algorithm.
It is important to remark that PCA is
not a feature reduction method; the complete set of
original features is retained. PCA is applied at each
step within the RFE procedure (i.e. a new PCA is
done on each resulting subset)

Data Deﬁnition
The problem of classifying post-training and pre-
training math processing can be formulated in dif-
ferent ways. This study choose two particular for-
mulations:
instantaneous volume classiﬁcation and
time series classiﬁcation. Traditional classiﬁcation of
fMRI uses single volumes (images) as samples and
has strongly demonstrated to be a reliable way to
classify diﬀering cognitive states.4 Alternatively, we
can classify pre-training and post-training images via
an ensemble of volumes (15 math volumes + 15 con-
trol volumes), i.e. a time series. Since each image
represents a snapshot of the given activity (i.e. math
task or control task) at some point within the 30s in-
terval. One could imagine that the neural activity at
the end of the interval looks very diﬀerent from at the
beginning of the interval. Furthermore, we know that
the brain signal within a block follows the hemody-
namic response, which is a lagging indicator. There-
fore, we expect a single ”representative image” of the
math task or the control task to have a large variance.
Alternatively, the collection images, strewn into one
30x31608 feature vector, representing ”duty cycle” is

3

likely to have a lower variance and more amenable to
classiﬁcation. Results for both formulations are pre-
sented in this paper. This technique has also been
used to classify the time dependent changes in fMRI
images in order to capture temporal features.3 Al-
though we built the implementation to conduct such
a study, it was not explored further at time of writing.

Classiﬁer Formulation

The most intuitive form to achieve our ob jective is
to use the pre-training images representing the math
task and comparing them to the post-training images
representing the math task. The resulting discrimi-
nating weight vector (i.e. the separating hyperplane
parameter w) would then be used to classify the con-
trol images representing math. That is, if the training
did indeed augment the neural patterns to be more
similar to a healthy individual, we expect the weight
vector to classify the control images with the label
corresponding to the post-training images. Alterna-
tively, we can classify through a more indirect means.
We can classify the control and math images in the
pre-training set and obtain a classiﬁer wmc
pre and sim-
ilarly for the post training set wmc
post and using those
two weight vectors, determine which weight vector
can classify the control data (math + task) the best.
Results for both formulations can be found later in
this paper.

Speciﬁcation of Our Classiﬁcation Pro-
cedure
We follow the soft-max form of the linear SVM.?
Speciﬁcally, we chose a soft-max regularization pa-
rameter C = 1000. Although a nonlinear ker-
nel was tried in the study, it did not demonstrate
provable performance improvements over the linear
form, which is consistent with existing literature in
fMRI classiﬁcation. Furthermore, the discriminating
weight vector (i.e. the support hyperplane) resulting
from a nonlinear kernel is much hard to interpret as
it relates to functional localization. The data is sepa-
rated into three sets for each time the classiﬁcation is
run. Firstly, we create one set B that includes all the
data belong to one (or two) individual(s); this will
be our disjoint set and will remain constant through
the RFE procedure. We then take all remaining data
and call this set A. For each classiﬁcation, set A
is randomly broken into a testing set Atesting and a
training set Atraining in a 30/70 split. Cross valida-
tion (k-fold, loocv, etc) will be limited to the training
set only and prediction rates are calculated for each
of the three sets.

Generalization Weighted Recursive
Feature Elimination
As discussed earlier, the ability to maximize general-
ization at all steps will be critical to infer on disjoint
data. Therefore, we begin with the classical RFE
algorithm,2 whose procedure can be found in detail
in the literature, and add modiﬁcations to improve
the generalization. Firstly, the in our application of
the RFE procedure, we remove a percentage of the
worst ranking features as opposed to a single feature.
Naturally this is for computational purposes since we
are working with a very large features space. We
begin our modiﬁcation with where we perform our
ranking.
In the author’s experience, the RFE pro-
cess has a very large variance with respect to the re-
duced subsets that it creates. Therefore, within the
RFE algorithm, we perform a k-fold cross validation.
Furthermore, we seek to enhance the generalization
of our classiﬁer by weighting the weight vectors vi
(corresponding to the ith step in the cross valida-
tion procedure) in the cross validation process by the
ability of vi to predict on the set Atesting . A simi-
lar process can be applied to the disjoint set B but
experience shows that this has no eﬀect on the result-
ing weight vector because the disjoint set B is almost
always predicted poorly to the same degree. One pos-
sible intepretation is for that particular iteration, you
want the separating hyperplane to ﬁt your test data
well. Another, possibly better interpretation, is that
you are picking separating hyperplanes that gener-
alizes to all the data very well, that is, both train-
ing and testing. Since training will always be low,
we expect that the generalization can be improved
if we weight against he performance of the test set
i.e. when we weight it against hte performance on
the entire set. This idea isnt limited to fMRI but
rather this is helpful whenever you have many more
features than samples and overﬁtting is expected to
be a problem. Since we are essentially applying a
mean over the cross validation set, it is important to
have this weight factor since some particular data sets
will be conditioned such that the optimizer is unable
to converge.of interest (ROI) mask.

Algorithm
Algorithm Generalization Weighted RFE (X, S, R, k , P CA)
Given a dataset X , a starting feature set S , a
removed feature set R, the number of folds in
cross validation k , and the option to perform
PCA within iterations
1. Begin with set S = [allf eatures] and R = []
2. Do PCA if desired
3. Randomly generate Atraining , Atesting

4

4.

5.

6.
7.

for i=1:k Perform k-fold or loocv cross valida-
tion Perform SVM Obtain weight vector wi
Now weight obtain a weight-averaged weight
vector ¯w = Pk
i (1−i )
where i is the testing error
Pk
i i
associated to weight vector wi
Perform ranking on ¯w
Update S , and R

Results

Figure 1: Prediction Rates For Single Volume and
Duty Cycle Formulations.

Prediction Rates
Overall, we can conﬁdently classify pre-training and
post training data with a great deal of accuracy, with
prediction rates averaging 87%. We can also make
other observations from the ﬁgures. The top ﬁgure
shows that the disjoint set B does not change as we
proceed in the RFE process, which leads us to believe
that generalizing outside an individual’s data remains
very diﬃcult with the number of samples we have
for the study. The bottom ﬁgure shows the eﬀect of
ﬁltering on the prediction rate and demonstrates that
ﬁltering is a key part to getting quality data.

Comparing Against Healthy Brains
Prediction Control Data
Data Set
0.48
Pre, MathvCT, single
0.45
Post,MathvCT, single
Pre,ROI,MathvCT, single
0.523
0.4913
Post,ROI,MathvCT, single
0.5
Pre,ROI,Filter,MathvCT,single
Post,ROI,Filter,MathvCT,single
0.503
0.5238 (CT=Post)
DutyCycle
0.5714 (CT=Post)
DutyCycle, ROI
Pre Math v Post Math
0.487 (CT=Post)
From the table above, we can see that we have
a very diﬃcult time classifying the control data, as
we’d expect from our experience trying to classify
the disjoint set. Despite our attempts to maximize
the generalization, we were unable to discern from
the data whether or not the Turner Syndrome brains
were more similar to the healthy brains after training.

Conclusion

The goal for our pro ject was to determine if we could
discern pre-training and post-training fMRI volumes
and if we could, whether or not we could claim that
the post-training volumes look more similar to the
pre-training volumes. Although we can ﬁrmly state
that there is indeed a diﬀerence between pre-training
and post-training data, our attempts to generalize
those results to data that were disjoint from our
training were ultimately unsuccessful. Analysis of
fMRI data is strongly hindered by the lack of data
and until more data can be acquired, the holy grail
of diagnosing mental states with fMRI classiﬁers re-
mains just that.

References
et. al. ”Beyond Mind Reading: multi-
[1] Norman, A. Kenneth,
voxel pattern analysis of
fMRI data.” TRENDS in Cognitive Science.
doi:10.1016/j.tics.2006.07.005.

[2] Guyon, I., Vapnik, V. et al. ”Gene Selection for Cancer Classification us-
ing Support Vector Machines.” Machine Learning, Kluwer Academic Pub-
lishers, 2002, p389-442.

[3] Mourao-Miranda, Janaina, Friston, Karl J., and Michael Brammer. ”Dy-
namic Discrimination Analysis: A spatial-temporal SVM.” NeuroImage
doi:10.1016/j.neuroimage.2007.02.020.

[4] Cox, David and Robert Savoy. ”Functional magnetic resonance imaging
(fMRI) ”brain reading”: detecting and classifying distributed patterns
of fMRI activity in human visual cortex.” NeuroImage. doi:10.1016/S1053-
8119(03)00049-1.

[5] Wellcome
for
Center
Trust
http://www.fil.ion.ucl.ac.uk/spm/software/spm5/,
Parametric Mapping. UCL Institute of Neurology. Dec. 2005.

Neuroimaging.
Statistical

[6] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex

programming (web page and software). http://stanford.edu/ boyd/cvx,

June 2009.

5


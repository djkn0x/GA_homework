Group Orthogonal Matching Pursuit for
Variable Selection and Prediction

Aur ´elie C. Lozano, Grzegorz ´Swirszcz, Naoki Abe
IBM Watson Research Center,
1101 Kitchawan Road,
Yorktown Heights NY 10598,USA
{aclozano,swirszcz,nabe}@us.ibm.com

Abstract

We consider the problem of variable group selection for least squares regression,
namely, that of selecting groups of variables for best regression performance,
leveraging and adhering to a natural grouping structure within the explanatory
variables. We show that this problem can be efﬁciently addressed by using a cer-
tain greedy style algorithm. More precisely, we propose the Group Orthogonal
Matching Pursuit algorithm (Group-OMP), which extends the standard OMP pro-
cedure (also referred to as “forward greedy feature selection algorithm” for least
squares regression) to perform stage-wise group variable selection. We prove that
under certain conditions Group-OMP can identify the correct (groups of) vari-
ables. We also provide an upperbound on the l∞ norm of the difference between
the estimated regression coefﬁcients and the true coefﬁcients. Experimental re-
sults on simulated and real world datasets indicate that Group-OMP compares
favorably to Group Lasso, OMP and Lasso, both in terms of variable selection
and prediction accuracy.

1 Introduction

We address the problem of variable selection for regression, where a natural grouping structure
exists within the explanatory variables, and the goal is to select the correct group of variables, rather
than the individual variables. This problem arises in many situations (e.g. in multifactor ANOVA,
generalized additive models, time series data analysis, where lagged variables belonging to the same
time series may form a natural group, gene expression analysis from microarrays data, where genes
belonging to the same functional cluster may be considered as a group). In these settings, selecting
the right groups of variables is often more relevant to the subsequent use of estimated models, which
may involve interpreting the models and making decisions based on them.
Recently, several methods have been proposed to address this variable group selection problem, in
(cid:179)
(cid:180)
(cid:107)y − (cid:80)J
(cid:80)J
the context of linear regression [12, 15]. These methods are based on extending the Lasso formula-
tion [8] by modifying the l1 penalty to account for the group structure. Speciﬁcally, Yuan & Lin [12]
j=1 XGj βGj (cid:107)2 + λ
j=1 (cid:107)βGj (cid:107)2
proposed the Group Lasso, which solves arg minβ
1
,
(cid:80)J
2
where XG1 , . . . , XGJ are the natural groupings within the variables of X and βGj are the coefﬁ-
cient vectors for variables in groups Gj . Zhao et al [15] considered a more general penalty class, the
j=1 (cid:107)βj (cid:107)l0
Composite Absolute Penalties family T (β ) =
, of which the Group Lasso penalty is a
lj
special instance. This development opens up a new direction of research, namely that of extending
the existing regression methods with variable selection to the variable group selection problem and
investigating to what extent they carry over to the new scenario.
The present paper establishes that indeed one recent advance in variable selection methods for re-
gression, “forward greedy feature selection algorithm”, also known as the Orthogonal Matching

1

Pursuit (OMP) algorithm in the signal processing community [5], can be generalized to the current
setting of group variable selection. Speciﬁcally we propose the “Group Orthogonal Matching Pur-
suit” algorithm (Group-OMP), which extends the OMP algorithm to leverage variable groupings,
and prove that, under certain conditions, Group-OMP can identify the correct (groups of) variables
when the sample size tends to inﬁnity. We also provide an upperbound on the l∞ norm of the dif-
ference between the estimated regression coefﬁcients and the true coefﬁcients. Hence our results
generalize those of Zhang [13], which established consistency of the standard OMP algorithm. A
key technical contribution of this paper is to provide a condition for Group-OMP to be consistent,
which generalizes the “Exact Recovery Condition” of [9](Theorem 3.1) stated for OMP under the
noiseless case. This result should also be of interest to the signal processing community in the
context of block-sparse approximation of signals. We also conduct empirical evaluation to com-
pare the performance of Group-OMP with existing methods, on simulated and real world datasets.
Our results indicate that Group-OMP favorably compares to the Group Lasso, OMP and Lasso al-
gorithms, both in terms of the accuracy of prediction and that of variable selection. Related work
include [10, 3] using OMP for simultaneous sparse approximation, [11] showing that standard MP
selects features from correct groups, and [4] that consider a more general setting than ours.
The rest of the paper is organized as follows. Section 2 describes the proposed Group-OMP pro-
cedure. The consistency results are then stated in Section 3. The empirical evaluation results are
presented in Section 4. We conclude the paper with some discussions in Section 5.

2 Group Orthogonal Matching Pursuit
Consider the general regression problem y = X ¯β + ν, where y ∈ Rn is the response vector, X =
[f1 , . . . , fd ] ∈ Rn×d is the matrix of feature (or variable) vectors fj ∈ Rn , ¯β ∈ Rd is the coefﬁcient
vector and ν ∈ Rn is the noise vector. We assume that the noise components νi , i = 1, . . . , n,
are independent Gaussian variables with mean 0 and variance σ2 . For any G ⊂ {1, . . . , d} let XG
denote the restriction of X to the set of variables, {fj , j ∈ G}, where the colums fj are arranged in
ascending order. Similarly for any vector β ∈ Rd of regression coefﬁcients, denote βG its restriction
to G, with reordering in ascending order. Suppose that a natural grouping structure exists within the
(cid:80)J
variables of X consisting of J groups XG1 , . . . , XGJ , where Gi ⊂ {1, . . . , d}, Gi ∩ Gj = ∅ for
i (cid:54)= j and XGi ∈ Rn×dj . Then, the above regression problem can be decomposed with respect to
¯βGj + ν, where ¯βGj ∈ Rdj . Furthermore, to simplify the exposition,
the groups, i.e. y =
j=1 XGj
assume that each XGj is orthonormalized, i.e. X ∗
XGj = Idj .
Gj
Given β ∈ Rd let supp(β ) = {j : βj (cid:54)= 0}. For any such G and v ∈ Rn , denote by ˆβX (G, v) the co-
efﬁcients resulting from applying ordinary least squares (OLS) with non-zero coefﬁcients restricted
2 subject to supp(β ) ⊂ G. Given the above setup, the
to G, i.e., ˆβX (G, v)=arg minβ∈Rd (cid:107)X β − v(cid:107)2
Group-OMP procedure we propose is described in Figure 1, which extends the OMP procedure to
deal with group selection. Note that this procedure picks the best group in each iteration, with re-
spect to reduction of the residual error, and it then re-estimates the coefﬁcients, β (k) , as in OMP. We
recall that this re-estimation step is what distinguishes OMP, and our group version, from standard
boosting-like procedures.

• Input: The data matrix X = [f1 , . . . , fd ] ∈ Rn×d , with group structure G1 , . . . , GJ , such that
XGj = Idj . The response y ∈ Rn . Precision  > 0 for the stopping criterion.
X ∗
Gj
• Output: The selected groups G (k) , the regression coefﬁcients β (k) .
• Initialization: G (0) = ∅, β (0) = 0.
For k = 1, 2, . . .
(X β (k−1) − y)(cid:107)2 .
Let j (k) = arg maxj (cid:107)X ∗
Gj
If ((cid:107)X ∗
(X β (k−1) − y)(cid:107)2 ≤ ) break
G
j (k)
Set G (k) = G (k−1) ∪ Gj (k) . Let β (k) = ˆβX (G (k) , y).

(∗)

End

Figure 1: Method Group-OMP

2

3 Consistency Results

3.1 Notation
(cid:83)
Let Ggood denote the set of all the groups included in the true model. We refer to the groups in
(cid:83)
Ggood as good groups. Similarly we call Gbad the set of all the groups which are not included. We
let ggood and gbad denote the set of “good incides” and “bad indices”, i.e. ggood =
Gi
Gi∈Ggood
and gbad =
Gi . When they are used to restrict index sets for matrix columns or vectors,
Gi∈Gbad
they are assumed to be in canonical (ascending) order, as we did for G. Furthermore, the elements
of Ggood are groups of indices, and |Ggood | is the number of groups in Ggood , while ggood is deﬁned
in terms of individual indices, i.e. ggood is the set of indices corresponding to the groups in Ggood .
The same holds for Gbad and gbad . In this notation supp( ¯β ) ⊂ ggood .
(cid:169)(cid:107)X β (cid:107)2
(cid:170)
We denote by ρX (Ggood ) the smallest eigenvalue of X ∗
ggood Xggood , i.e.
2 /(cid:107)β (cid:107)2
ρX (Ggood ) = inf β
2 : supp(β ) ⊂ ggood
.
Here and throughout the paper we let A∗ denote the conjugation of the matrix A (which, for a
real matrix A, coincides with its transpose) and A+ denote the Moore–Penrose pseudoinverse of
If rows of A are linearly independent A+ = A∗ (AA∗ )−1 and when
the matrix A (c.f. [6, 7]).
(cid:114) (cid:80)
(cid:114) (cid:80)
columns of A are linearly independent A+ = (A∗A)−1A∗ . Generally for u = {u1 , . . . , u|ggood |},
(cid:80)
(cid:80)
v = {v1 , . . . , v|gbad | } we deﬁne
j , and (cid:107)v(cid:107)bad
(cid:107)u(cid:107)good
(2,1) =
(2,1) =
u2
Gi∈Gbad
Gi∈Ggood
j∈Gi
j∈Gi
and then for any matrix A ∈ R|ggood |×|gbad | , let (cid:107)A(cid:107)good/bad
= sup
(2,1)
(cid:107)v(cid:107)bad
(2,1)=1
ggood Xgbad (cid:107)good/bad
Then we deﬁne µX (Ggood ) = (cid:107)X +
(2,1)

v2
j
(cid:107)Av(cid:107)good
(2,1) .

.

3.2 The Noiseless Case
We ﬁrst focus on the noiseless case (i.e. ν ≡ 0). For all k , let rk = X β (k) − y . In the noiseless case,
we have r0 = −y ∈ S pan(Ggood ). So if Group-OMP has not made a mistake up to round k , we
also have rk ∈ S pan(Ggood ). The following theorem and its corollary provide a condition which
guarantees that Group-OMP does not make a mistake at the next iteration, given that it has not made
any mistakes up to that point. By induction on k , it implies that Group-OMP never makes a mistake.
Theorem 1. Reorder the groups in such a way that Ggood = G1 , . . . , Gm and Gbad =
Gm+1 , . . . , GJ . Let r ∈ S pan(Xggood ). Then the following holds
Gm+2 r(cid:107)2 , . . . , (cid:107)X ∗
Gm+1 r(cid:107)2 , (cid:107)X ∗
(cid:107)((cid:107)X ∗
GJ r(cid:107)2 )(cid:107)∞
≤µX (Ggood ).
G2 r(cid:107)2 , . . . , (cid:107)X ∗
(cid:107)((cid:107)X ∗
G1 r(cid:107)2 , (cid:107)X ∗
r(cid:107)2 )(cid:107)∞
Gm
(cid:161)
(cid:162)T
Proof of Theorem 1. Reorder the groups in such way that Ggood = {G1 , . . . , Gm } and Gbad =
{Gm+1 , . . . , GJ }. Let Φ∗ : Rn → Rd1 ⊕ Rd2 ⊕ . . . ⊕ Rdm be deﬁned as
(cid:180)T
(cid:179)
G2 x)T , . . . , (X ∗
Φ∗ (x) =
(X ∗
G1 x)T , (X ∗
x)T
Gm
and analogously let Ψ∗ : Rn → Rdm+1 ⊕ Rdm+2 ⊕ . . . ⊕ RdJ be deﬁned as
Gm+2 x)T , . . . , (X ∗
Ψ∗ (x) =
(X ∗
Gm+1 x)T , (X ∗
x)T
.
GJ
We shall denote V Φ = Rd1 ⊕ Rd2 ⊕ . . . ⊕ Rdm with a norm (cid:107).(cid:107)Φ
(2,∞) deﬁned as:
(2,∞) = (cid:107)((cid:107)v1 (cid:107)2 , (cid:107)v2 (cid:107)2 , . . . , (cid:107)vm (cid:107)2 )(cid:107)∞ for vi ∈ Rdi , i = 1, . . . , m.
(cid:107)(v1 , v2 , . . . , vm )(cid:107)Φ
Analogously V Ψ = Rdm+1 ⊕ Rdm+2 ⊕ . . . ⊕ RdJ with a norm (cid:107).(cid:107)Ψ
(2,∞) deﬁned as:
(2,∞)=(cid:107)((cid:107)v1 (cid:107)2 , (cid:107)v2 (cid:107)2 , . . . , (cid:107)vJ −m(cid:107)2 )(cid:107)∞ for vj ∈ Rdm+j , j = 1, . . . , J − m.
(cid:107)(v1 , v2 , . . . , vJ −m )(cid:107)Ψ
It is easy to verify that (cid:107).(cid:107)Φ
(2,∞) , (cid:107).(cid:107)Ψ
(2,∞) are norms indeed.Now the condition expressed by Eq. (1)
can be rephrased as
(cid:107)Ψ∗ (r)(cid:107)Ψ
(2,∞)
(cid:107)Φ∗ (r)(cid:107)Φ
(2,∞)

≤ µX (Ggood )

(1)

(2)

3

(cid:83)m
Lemma 1. The map Φ∗ restricted to S pan
i=1 XGi is a linear isomorphism onto its image.
(cid:83)m
Proof of Lemma 1. By deﬁnition if Φ∗ (x) = (0)VΦ then x must be orthogonal to each of the sub-
spaces spanned by XGi , i = 1, . . . , m. Thus ker Φ∗ ∩ S pan
i=1 XGi = 0
Let (Φ∗ )+ denote the inverse mapping whose existence was proved in Lemma 1. The choice
of symbol is not coincident, the matrix of this mapping is indeed a pseudoinverse of the matrix
(cid:107)Ψ∗ ((Φ∗ )+Φ∗ (r))(cid:107)Ψ
(cid:107)Ψ∗ (r)(cid:107)Ψ
(XG1 |XG2 | . . . |XGm )T .We have
≤ (cid:107)Ψ∗ ◦ (Φ∗ )+(cid:107)(2,∞) ,
(2,∞)
(2,∞)
=
(cid:107)Φ∗ (r)(cid:107)Φ
(cid:107)Φ∗ (r)(cid:107)Φ
(2,∞)
(2,∞)
where the last term is the norm of the operator Ψ∗ ◦ (Φ∗ )+ : V Φ → V Ψ . We are going to need the
following
Lemma 2. A dual space of V Φ is (V Φ )∗ = Rd1 ⊕ Rd2 ⊕ . . . ⊕ Rdm with a norm (cid:107).(cid:107)Φ
(2,1) deﬁned
(2,1) = (cid:107)((cid:107)v1 (cid:107)2 , (cid:107)v2 (cid:107)2 , . . . , (cid:107)vm(cid:107)2 )(cid:107)1 .
as: (cid:107)(v1 , v2 , . . . , vm )(cid:107)Φ
A dual space of V Ψ is (V Ψ )∗ = Rdm+1 ⊕ Rdm+2 ⊕ . . . ⊕ RnJ with a norm (cid:107).(cid:107)Ψ
(2,1) deﬁned as:
(2,1) = (cid:107)((cid:107)v1 (cid:107)2 , (cid:107)v2 (cid:107)2 , . . . , (cid:107)vJ −m(cid:107)2 )(cid:107)1 .
(cid:107)(v1 , v2 , . . . , vJ −m )(cid:107)Ψ
J(cid:80)
J(cid:80)
Proof of Lemma 2. We prove for V Ψ , the proof for V Φ is identical.
Rdm+1 ⊕ Rdm+2 ⊕ . . . ⊕ RdJ .
∈
Let v∗ = (v∗
1 , v∗
2 , . . . , v∗
J −m )
i , vi (cid:105)| =
|v∗ (v)| = sup
i , vi (cid:105)| =
|(cid:104)v∗
(cid:107)v∗(cid:107) = sup
|(cid:104)v∗
sup
vi ∈Rni
vi ∈Rni
v∈V Ψ
i=m+1
i=m+1
(cid:107)v(cid:107)2,∞=1
(cid:107)vi (cid:107)2=1
(cid:107)v(cid:107)2,∞=1
i , vi (cid:105)| = (cid:107)v∗
|(cid:104)v∗
i (cid:107)2 (as (cid:96)∗
2 = (cid:96)2 ) and Schwartz inequality.
The last equality follows from sup
vi ∈Rni
(cid:107)vi (cid:107)2=1

J(cid:80)
i=m+1

We have
(cid:107)v∗
i (cid:107)2 .

A fundamental fact from Functional Analysis states that a (Hermitian) conjugation is an isometric
isomorphism. Thus
(cid:107)Ψ∗ ◦ (Φ∗ )+ (cid:107)(2,∞) = (cid:107)(Φ)+ ◦ Ψ(cid:107)(2,1) .
(3)
We used here (A∗ )∗ = A and (A∗ )+ = (A+ )∗ . The right hand side of (3) is equal to
(cid:107)X +
ggood Xgbad (cid:107)good/bad
in matrix notation. Thus the inequality (1) holds. This concludes the proof
(2,1)
of Theorem 1.
Corollary 1. Under the conditions of Theorem 1, if µX (Ggood ) < 1 then the following holds
Gm+2 r(cid:107)2 , . . . , (cid:107)X ∗
Gm+1 r(cid:107)2 , (cid:107)X ∗
(cid:107)((cid:107)X ∗
GJ r(cid:107)2 )(cid:107)∞
G2 r(cid:107)2 , . . . , (cid:107)X ∗
r(cid:107)2 )(cid:107)∞
(cid:107)((cid:107)X ∗
G1 r(cid:107)2 , (cid:107)X ∗
Gm
Intuitively, the condition µX (Ggood ) < 1 guarantees that no bad group “mimics” any good group too
well. Note that Theorem 1 and Corollary 1 are the counterpart to Theorem 3.3 in [9] which states the
Exact Recovery condition for the standard OMP algorithm, namely that (cid:107)X +
ggood Xgbad (cid:107)(1,1) < 1,
where ggood is not deﬁned in terms of groups, but rather in terms of the variables present in the true
model (since the notion of groups does not pertain to OMP in its original form).

< 1.

(4)

3.3 The Noisy Case

The following theorem extends the results of Theorem 1 to deal with the non-zero Gaussian noise
ν. It shows that under certain conditions the Group-OMP algorithm does not select bad groups. A
sketch of the proof is provided at the end of this section.
Theorem 2. Assume that µX (Ggood ) < 1 and 1 ≥ ρX (Ggood ) > 0. For any η ∈ (0, 1/2), with
(cid:112)
probability at least 1 − 2η , if the stopping criterion of the Group-OMP algorithm is such that
1
1 − µX (Ggood ) σ
then when the algorithm stops all of the following hold:
(C1)G (k−1) ⊂ Ggood

2d ln(2d/η),

 >

4

√

√

(cid:113)
|Ggood \G (k−1) |
(C2)(cid:107)β (k−1) − ˆβX (Ggood , y)(cid:107)2 ≤ 
(cid:175)(cid:175)(cid:169)
(cid:170)(cid:175)(cid:175) .
ρX (Ggood )
2 ln(2|ggood |/η)
(C3)(cid:107) ˆβX (Ggood , y) − ¯β (cid:107)∞ ≤ σ
ρX (Ggood )
(C4)|Ggood \ G (k−1) | ≤ 2
Gj ∈ Ggood : (cid:107) ¯βGj (cid:107)2 <
8ρX (Ggood )−1
We thus obtain the following theorem which states the main consistency result for Group-OMP.
(cid:112)
Theorem 3. Assume that µX (Ggood ) < 1 and 1 ≥ ρX (Ggood ) > 0. For any η ∈ (0, 1/2), with
(cid:112)
probability at least 1 − 2η , if the stopping criterion of the Group-OMP algorithm is such that
2d ln(2d/η) and minGj ∈Ggood (cid:107) ¯βGj (cid:107)2 ≥ √
8ρX (Ggood )−1 then when the
1
 >
1−µX (Ggood ) σ
algorithm stops G (k−1) = Ggood and (cid:107)β (k−1) − ¯β (cid:107)∞ ≤ σ
(2 ln(2|Ggood |/η))/ρX (Ggood ).
Except for the condition on µX (Ggood ) (and the deﬁnition of µX (Ggood ) itself), the conditions in
Theorem 2 and Theorem 3 are similar to those required for the standard OMP algorithm [13], the
main advantage being that for Group-OMP it is the l2 norm of the coefﬁcient groups for the true
model that need to be lower-bounded, rather than the amplitude of the individual coefﬁcients.1
Proof Sketch of Theorem 2. To prove the theorem a series of lemmas are needed, whose proofs are
omitted due to space constraint, as they can be derived using arguments similar to Zhang [13] for
the standard OMP case. The following lemma gives a lower bound on the correlation between the
good groups and the residuals from the OLS prediction where the coefﬁcients have been restricted
to a set of good groups.
Lemma 3. Let G ⊂ Ggood , i.e., G is a set of good groups. Let β = ˆβX (G , y), β (cid:48) = ˆβX (Ggood , y),
√
ρX (Ggood )
√|Ggood \G | (cid:107)f − f (cid:48)(cid:107)2 .
(y − f )(cid:107)2 ≥
f = X β and f (cid:48) = X β (cid:48) . Then maxGj ∈Ggood (cid:107)X ∗
Gj
The following lemma relates the parameter ˆβX (Ggood ), which is estimated by OLS given that the
(cid:113)
set of good groups has been correctly identiﬁed, to the true parameter ¯β .
Lemma 4. For all η ∈ (0, 1), with probability at least 1 − η , we have
2 ln(2|ggood |/η)
(cid:107) ˆβX (Ggood , y) − ˆβX (Ggood , Ey)(cid:107)∞ ≤ σ
ρX (Ggood )

.

The following lemma provides an upper bound on the correlation of the bad features to the residuals
(cid:180)
(cid:179)
from the prediction by OLS given that the set of good groups has been correctly identiﬁed.
(cid:112)
Lemma 5. Let β (cid:48) = ˆβX (Ggood , y) and f (cid:48) = X β (cid:48) . We have
(f (cid:48) − y)(cid:107)2 ≤ σ
maxGj (cid:54)∈Ggood (cid:107)X ∗
2d ln(2d/η)
P
Gj

≥ 1 − η .

We are now ready to prove Theorem 2. We ﬁrst prove that for each iteration k before the Group-
OMP algorithm stops, G (k−1) ⊂ Ggood by induction on k . Now, suppose that the claim holds after
k − 1 iterations, where k ≥ 1. So at the beginning of the k th iteration, we have G (k−1) ⊂ Ggood . We
have
(cid:107)X ∗
Gj (X β (k−1) − y)(cid:107)2
max
Gj (cid:54)∈Ggood
Gj (X β (cid:48) − y)(cid:107)2
Gj X (β (k−1) − β (cid:48) )(cid:107)2 + max
≤
(cid:107)X ∗
(cid:107)X ∗
max
Gj (cid:54)∈Ggood
Gj (cid:54)∈Ggood
Gj X (β (k−1) − β (cid:48) )(cid:107)2 + max
(cid:107)X ∗
(cid:107)X ∗
Gj (X β (cid:48) − y)(cid:107)2
≤ µX (Ggood ) max
Gj ∈Ggood
Gj (cid:54)∈Ggood
Gj (X β (cid:48) − y)(cid:107)2
Gj (X β (k−1) − y)(cid:107)2 + max
(cid:107)X ∗
(cid:107)X ∗
= µX (Ggood ) max
Gj ∈Ggood
Gj (cid:54)∈Ggood
1The sample size n is explicitly part of the conditions in [13] while it
is implicit here due to
(cid:169) 1
(cid:170)
the different ways of normalizing the matrix X. One recovers the same dependency on n by con-
√
√
√
X (cid:48) (Ggood ) =
sidering X (cid:48) =
nX, β (cid:48)(k) = β (k) /
n, ¯β (cid:48) = ¯β /
n, deﬁning (as in [13]) ρ(cid:48)
2 /(cid:107)β (cid:107)2
n (cid:107)X (cid:48)β (cid:107)2
X (cid:48)(Ggood ) = ρX(Ggood ) and ˆβX (cid:48)(Ggood , y) =
2 : supp(β ) ⊂ ggood
, and noting that ρ(cid:48)
√
inf β
n. If X had i.i.d. entries, with mean 0, variance 1/n and ﬁnite 4th moment, ρX (Ggood ) con-
ˆβX(Ggood , y)/
verges a.s. to (1 − √
g)2 as n → ∞ and |ggood |/n → g ≤ 1 [2]. Hence the rates in C2-C4 are unaffected by
ρX (Ggood ).

(5)

(6)

5

Here Eq. 5 follows by applying Theorem 1, and Eq. 6 is due to the fact that for all Gj ∈ Ggood
(X β (cid:48) − y) = 0(dj ) holds.
X ∗
(cid:112)
Gj
Lemma 5 together with the condition on  of Theorem 2 implies that with probability at least 1 − η ,
2d ln(2d/η) < (1 − µX (Ggood )).
(cid:107)X ∗
Gj (X β (cid:48) − y)(cid:107)2 ≤ σ
max
(7)
Gj (cid:54)∈Ggood
Lemma 3 together with the deﬁnition of ρX (Ggood ) implies
(cid:112)
ρX (Ggood )
(cid:107)X ∗
Gj (y − X β (k−1) )(cid:107)2 ≥
|Ggood \ G (k−1) | (cid:107)β (k−1) − β (cid:48)(cid:107)2

max
Gj ∈Ggood

(8)

(9)

We then have to deal with the following cases.
√
|Ggood \G (k−1) |
Case 1: (cid:107)β (k−1) − β (cid:48) (cid:107)2 > 
. It follows that
ρX (Ggood )
Gj (y − X β (k−1) )(cid:107)2 >  > max
(cid:107)X ∗
Gj (cid:54)∈G (cid:107)X ∗
Gj (X β (cid:48) − y)(cid:107)2 /(1 − µX (Ggood )),
max
Gj ∈Ggood
where
Then Eq.
inequality
that
implies
6
7.
from Eq.
follows
last
the
(X β (k−1) − y)(cid:107)2 < maxGj ∈Ggood (cid:107)X ∗
maxGj (cid:54)∈Ggood (cid:107)X ∗
(X β (k−1) − y)(cid:107)2 . So a good
Gj
Gj
group is selected, i.e., Gi(k) ∈ Ggood and Eq. 9 implies that the algorithm does not stop.
√
|Ggood \G (k−1) |
Case 2: (cid:107)β (k−1) − β (cid:48) (cid:107)2 ≤ 
. We then have three possibilities.
ρX (Ggood )
Case 2.1: Gi(k) ∈ Ggood and the procedure does not stop.
Case 2.2: Gi(k) ∈ Ggood and the procedure stops.
(cid:54)∈ Ggood in which case we have maxGj ∈Ggood (cid:107)X ∗
(X β (k−1) − y)(cid:107)2 ≤
Case 2.3: Gi(k)
Gj
(X β (k−1) − y)(cid:107)2 ≤ µX (Ggood ) maxGj ∈Ggood (cid:107)X ∗
maxGj (cid:54)∈Ggood (cid:107)X ∗
(X β (k−1) − y)(cid:107)2 +
Gj
Gj
(X β (cid:48) − y)(cid:107)2 ≤ µX (Ggood ) maxGj (cid:54)∈Ggood (cid:107)X ∗
maxGj (cid:54)∈Ggood (cid:107)X ∗
(X β (k−1) − y)(cid:107)2 +
Gj
Gj
maxGj (cid:54)∈Ggood (cid:107)X ∗
(X β (cid:48) − y)(cid:107)2 , where the second inequality follows from Eq. 6 and
Gj
the last
follows from applying the ﬁrst
inequality once again. We thus obtain that
(X β (cid:48) − y)(cid:107)2 < ,
1−µX (Ggood ) maxGj (cid:54)∈Ggood (cid:107)X ∗
maxGj (cid:54)∈Ggood (cid:107)X ∗
(X β (k−1) − y)(cid:107)2 ≤
1
Gj
Gj
where the last inequality follows by Eq. 7. Hence the algorithm stops.

The above cases imply that if the algorithm does not stop we have Gi(k) ∈ Ggood , and hence G (k) ⊆
√
|Ggood \G (k−1) |
Ggood and if the algorithm stops we have (cid:107)β (k−1) − β (cid:48)(cid:107)2 ≤ 
. Thus by induction,
ρX (Ggood )
if the Group-OMP algorithm stops at iteration k , we have that G (k−1) ⊆ Ggood and (cid:107)β (k−1) −
√
(cid:112)
(cid:112)
|Ggood \G (k−1) |
β (cid:48)(cid:107)2 ≤ 
. So (C1) and (C2) are satisﬁed. Lemma 4 implies that (C3) holds, and
ρX (Ggood )
together with the theorem’s condition on  also implies that with probability at least 1 − η , we have
(cid:107) ˆβX (Ggood , y) − ˆβX (Ggood , Ey)(cid:107)∞ ≤ σ
(2 ln(2|Ggood |/η))/ρX (Ggood ) < /
ρX (Ggood ). This
allows us to show that (C4) holds, using similar arguments as in [13], which we omit due to space
constraints. This leads to Theorem 2.

4 Experiments

4.1 Simulation Results

We empirically evaluate the performance of the proposed Group-OMP method, against comparison
methods OMP, Group Lasso, Lasso and OLS (Ordinary Least Square). Comparison with OMP will
test the effect of “grouping” OMP, while Group Lasso is included as a representative existing method
of group variable selection. We compare the performance of these methods in terms of the accu-
racy of variable selection, variable group selection and prediction. As measure of variable (group)
selection accuracy we use the F1 measure, which is deﬁned as F1 = 2P R
P +R , where P denotes the
precision and R denotes the recall. For computing variable group F1 for a variable selection method,

6

we consider a group to be selected if any of the variables in the group is selected.2 As measure of
(cid:161)(cid:107)Y − X β (cid:107)2 + λ(cid:107)β (cid:107)1
(cid:162)
prediction accuracy, we use the model error, deﬁned as Model error = ( ˆβ − ¯β )∗E (X ∗X )( ˆβ − ¯β ),
where ¯β are the true model coefﬁcients and ˆβ the estimated coefﬁcients. Recall that Lasso solves
arg minβ
. So the tuning parameter for Lasso and Group Lasso is the
penalty parameter λ. For Group-OMP and OMP rather than parameterizing the models according to
precision , we do so using the iteration number (i.e. a stopping point). We consider two estimates:
the “oracle estimate” and the “holdout validated estimate”. For the oracle estimate, the tuning pa-
rameter is chosen so as to minimize the model error. Note that such estimate can only be computed
in simulations and not in practical situations, but it is useful for evaluating the relative performance
of comparison methods, independently of the appropriateness of the complexity parameter. The
holdout-validated estimate is a practical version of the oracle estimate, obtained by selecting the
tuning parameter by minimizing the average squared error on a validation set. We now describe the
experimental setup.
Experiment 1: We use an additive model with categorical variables taken from [12](model I).
Consider variables Z1 , . . . , Z15 , where Zi ∼ N (0, 1)(i = 1, . . . , 15) and cov(Zi , Zj ) = 0.5|i−j | .
Let W1 , . . . , W15 be such that Wi = 0 if Zi < Φ−1 (1/3), Wi = 1 if Zi > Φ−1 (2/3) and Wi = 2
if Φ−1 (1/3) ≤ Zi ≤ Φ−1 (2/3), where Φ−1 is the quantile function for the normal distribution.
The responses in the data are generated using the true model:
Y = 1.8I (W1 = 1) − 1.2I (W1 = 0) + I (W3 = 1) + 0.5I (W3 = 0) + I (W5 = 1) + I (W5 = 0) + ν,
where I denote the indicator function and ν ∼ N (0, σ = 1.476). Then let (X2(i−1)+1 , X2i ) =
(I (Wi = 1), I (Wi = 0)), which are the variables that the estimation methods use as the explanatory
variables, with the following variable groups: Gi = {2i − 1, 2i}(i = 1, . . . , 15). We ran 100 runs,
each with 50 observations for training and 25 for validation.
Experiment 2: We use an additive model with continuous variables taken from [12](model III),
where the groups correspond to the expansion of each variable into a third-order polynomial.
.
Consider variables Z1 , . . . , Z17 , with Zi i.i.d. ∼ N (0, 1) (i = 1, . . . , 17). Let W1 , . . . , W16 be
√
(cid:162)
(cid:161)
6 − W 2
6 + 2
3 + W3 + 1
3 + W 2
deﬁned as Wi = (Zi + Z17 )/
2. The true model is Y = W 3
3 W6 + ν,
3 W 3
where ν ∼ N (0, σ = 2). Then let the explanatory variables be (X3(i−1)+1 , X3(i−1)+2 , X3i ) =
with the variable groups Gi = {3(i − 1) + 1, 3(i − 1) + 2, 3i}(i = 1, . . . , 16). We
i , W 2
W 3
i , Wi
ran 100 runs, each with 100 observations for training and 50 for validation.
Experiment 3: We use an additive model with continuous variables similar to that of [16]. Consider
three independent hidden variables Z1 , . . . , Z3 such that Zi ∼ N (0, σ = 1). Consider 40 predictors
(cid:80)15
(cid:80)10
(cid:80)5
deﬁned as: Xi = Z(cid:98)(i−1)/3(cid:99)+1 + νi for i = 1, . . . , 15 and Xi ∼ N (0, 1) for i = 16, . . . , 40, where
νi i.i.d. ∼ N (0, σ = 0.11/2 ). The true model is
i=11 Xi + ν, where ν ∼ N (0, σ = 15)
i=6 Xi + 2
i=1 Xi + 4
Y = 3
and the groups are Gk = {5(k − 1) + 1, . . . , 5k}, for k = (1, . . . , 3), and Gk = k + 12, for k > 3.
We ran 100 runs, each with 500 observations for training and 50 for validation.
Experiment 4: We use an additive model with continuous variables taken from [15]. Consider ﬁve
hidden variables Z1 , . . . , Z5 such that Zi i.i.d. ∼ N (0, σ = 1). Consider 10 measurements of each
of these hidden variables such that Xi = (0.05)Z(cid:98)(i−1)/10(cid:99)+1 + (1 − 0.052 )1/2 νi , i=1,. . . ,50, where
 7
νi ∼ N (0, 1) and cov(νi , νj ) = 0.5|i−j | . The true model is Y = X ¯β + ν , where ν ∼ N (0, σ =
19.22), and
for i = 1, . . . , 10
for i = 11, . . . , 20
2
for i = 21, . . . , 30
1
0
for i = 31, . . . , 50
The groups are Gk = {10(k − 1) + 1, . . . , 10k}, for k = (1, . . . , 5). We ran 100 runs, each with
300 observations for training and 50 for validation.
The results of the four experiments are presented in Table 1. We note that F1 (Var) and F1 (Group)
are identical for the grouped methods for Experiments 1, 2 and 4, since in these the groups have
equal size. Overall, Group-OMP performs consistently better than all the comparison methods, with
respect to all measures considered . In particular, Group-OMP does better than OMP not only for

¯βi =

2Other ways of translating variable selection to variable group selection are possible, but the F1 measure is
relatively robust with respect to this choice.

7

F1 (Var)
OLS
Lasso (Oracle)
Lasso (Holdout)
OMP (Oracle)
OMP (Holdout)
Group Lasso (Oracle)
Group Lasso (Holdout)
Group-OMP (Oracle)
Group-OMP (Holdout)
F1 (Group)
OLS
Lasso (Oracle)
Lasso (Holdout)
OMP (Oracle)
OMP (Holdout)
Group Lasso (Oracle)
Group Lasso (Holdout)
Group-OMP (Oracle)
Group-OMP (Holdout)
ME
OLS
Lasso (Oracle)
Lasso (Holdout)
OMP (Oracle)
OMP (Holdout)
Group Lasso (Oracle)
Group Lasso (Holdout)
Group-OMP (Oracle)
Group-OMP (Holdout)

Exp 1
0.333 ± 0
0.483 ± 0.010
0.389 ± 0.012
0.531 ± 0.019
0.422 ± 0.014
0.545 ± 0.010
0.624 ± 0.017
0.730 ± 0.017
0.615 ± 0.020
Exp 1
0.333 ± 0
0.458 ± 0.012
0.511 ± 0.010
0.687 ± 0.018
0.621 ± 0.020
0.545 ± 0.010
0.624 ± 0.017
0.730 ± 0.017
0.615 ± 0.020
Exp 1
3.184 ± 0.129
1.203 ± 0.078
2.536 ± 0.097
0.711 ± 0.020
0.945 ± 0.031
0.457 ± 0.021
1.279 ± 0.017
0.601 ± 0.0273
0.965 ± 0.050

Exp 2
0.222 ± 0
0.541 ± 0.010
0.528 ± 0.015
0.787 ± 0.009
0.728 ± 0.013
0.449 ± 0.011
0.459 ± 0.016
0.998 ± 0.002
0.921 ± 0.012
Exp 2
0.222 ± 0
0.346 ± 0.008
0.340 ± 0.014
0.808 ± 0.020
0.721 ± 0.025
0.449 ± 0.011
0.459 ± 0.016
0.998 ± 0.002
0.921 ± 0.012
Exp 2
7.063 ± 0.251
1.099 ± 0.067
1.309 ± 0.080
1.052 ± 0.061
1.394 ± 0.102
0.867 ± 0.052
1.047 ± 0.075
0.379 ± 0.035
0.605 ± 0.089

Exp 3
0.545 ± 0
0.771 ± 0.007
0.758 ± 0.015
0.532 ± 0.004
0.477 ± 0.006
0.693 ± 0.005
0.706 ± 0.013
0.999 ± 0.001
0.918 ± 0.011
Exp 3
0.194 ± 0
0.494 ± 0.011
0.547 ± 0.029
0.224 ± 0.004
0.421 ± 0.026
0.317 ± 0.006
0.364 ± 0.018
0.998 ± 0.001
0.782 ± 0.025
Exp 3
19.592 ± 0.451
9.228 ± 0.285
12.987 ± 0.670
19.006 ± 0.443
28.246 ± 1.942
11.538 ± 0.370
14.979 ± 0.538
6.727 ± 0.252
12.553 ± 1.469

Exp 4
0.750 ± 0
0.817 ± 0.004
0.810 ± 0.005
0.781 ± 0.005
0.741 ± 0.006
0.755 ± 0.002
0.794 ± 0.008
0.998 ± 0.002
0.890 ± 0.011
Exp 4
0.750 ± 0
0.751 ± 0.001
0.776 ± 0.006
0.842 ± 0.010
0.827 ± 0.010
0.755 ± 0.002
0.794 ± 0.008
0.998 ± 0.002
0.890 ± 0.011
Exp 4
46.845 ± 0.985
30.343 ± 0.796
38.089 ± 1.353
38.497 ± 0.926
48.564 ± 1.957
31.053 ± 0.831
37.359 ± 1.260
27.765 ± 0.703
35.989 ± 1.127

Table 1: Average F1 score at the variable level and group level, and model error for the models
output by Ordinary Least Squares, Lasso, OMP, Group Lasso, and Group-OMP.

Boston Housing
Prediction Error
Number of Original Variables

OLS
29.30 ± 3.25
13 ± 0

Lasso
17.82 ± 0.48
12.82 ± 0.05

OMP
19.10 ± 0.78
11.51 ± 0.20

Group Lasso
18.45 ± 0.59
12.50 ± 0.13

Group-OMP
17.60 ± 0.51
9.09 ± 0.31

Table 2: Average test set prediction error, average number of original variables, for the models
output by OLS, Lasso, OMP, Group Lasso, and Group-OMP on the“ Boston Housing” dataset.

variable group selection, but also for variable selection and predictive accuracy. Against Group-
Lasso, Group-OMP does better in all four experiments with respect to variable (group) selection
when using Oracle, while it does worse in one case when using holdout validation. Group-OMP also
does better than Group-Lasso with respect to the model error in three out of the four experiments.

4.2 Experiment on a real dataset

We use the “Boston Housing” dataset (UCI Machine Learning Repository). The continuous vari-
ables appear to have non-linear effects on the target value, so for each such variable, say Xi , we
i , and consider them as a variable
i and X 3
consider its third-order polynomial expansion, i.e., Xi , X 2
group. We ran 100 runs, where for each run we select at random half of the instances as training
examples, one quarter as validation set, and the remaining quarter as test examples. The penalty
parameter was chosen with holdout validation for all methods. The average test set prediction error,
the average number of selected original variables (i.e. groups) are reported in Table 2. These results
conﬁrm that Group-OMP has the highest prediction accuracy among the comparison methods, and
also leads to the sparsest model.

5 Concluding Remarks

In addition to its merits in terms of consistency and accuracy, Group-OMP is particulary attractive
due to its computational efﬁciency (the entire path is computed in J rounds, where J is the num-
ber of groups). Interesting directions for future research include comparing the conditions for the
consistency of Group-OMP to those for Group Lasso and the bounds on their respective accuracy in
estimating the regression coefﬁcients, evaluating modiﬁed versions of Group-OMP where the group
selection step (∗) in Figure 1 includes a penalty to account for the group size, and considering a
forward/backward extension that allows correcting for mistakes (similarly to [14]).

8

References
[1] BACH , F.R . , Consistency of the Group Lasso and Multiple Kernel Learning, J. Mach. Learn.
Res., 9, 1179-1225, 2008.
[2] BA I D . , Y IN Y.Q ., Limit of the smallest eigenvalue of a large dimensional sample covariance
matrix, Ann. Probab. 21, 1275-1294, 1993.
[3] CH EN J . , HUO X . , Sparse representations for multiple measurement vectors (MMV) in an
overcomplete dictionary, in Proc. of the 2005 IEEE Int. Conf. on Acoustics, Speech, and Signal
Proc., 2005.
[4] HUANG J . , ZHANG T. , M ETAXA S D . , Learning with Structured Sparsity, in ICML’09, 2009.
[5] MA LLAT S . , ZHANG Z ., Matching pursuits with time-frequency dictionaries, IEEE Transac-
tions on Signal Processing, 41, 3397-3415, 1993.
[6] MOORE , E .H , On the reciprocal of the general algebraic matrix, Bulletin of the American
Mathematical Society 26, 394-395, 1920.
[7] P ENRO SE , R . , A generalized inverse for matrices, Proceedings of the Cambridge Philosophical
Society 51, 406-413, 1955.
[8] T IB SH IRAN I , R . , Regression shrinkage and selection via the lasso, J. Royal. Statist. Soc B.,
58(1), 267-288, 1996.
[9] TRO P P J .A . , Greed is good: Algorithmic results for sparse approximation, IEEE Trans. Info.
Theory, 50(10), 2231-2242, 2004.
[10] TRO P P J .A . , G ILB ERT A .C . , S TRAU S S M . J . , Algorithms for simultaneous sparse approxi-
mation, Part I: greedy pursuit, Signal Proc. 86 (3), 572-588, 2006.
[11] P EOT TA L . , VAND ERGHEYN S T P., Matching Pursuit with Block Incoherent Dictionaries, Sig-
nal Proc. 55 (9), 2007.
[12] YUAN , M . , L IN , Y. , Model selection and estimation in regression with grouped variables, J.
R. Statist. Soc. B, 68, 4967, 2006.
[13] ZHANG , T., On the consistency of feature selection using greedy least squares regression, J.
Machine Learning Research, 2008.
[14] ZHANG , T., Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear
Models, in NIPS08, 2008.
[15] ZHAO , P, ROCHA , G . AND YU , B . , Grouped and hierarchical model selection through com-
posite absolute penalties, Manuscript, 2006.
[16] ZOU , H . , HA S T I E T., Regularization and variable selection via the Elastic Net., J. R. Statist.
Soc. B, 67(2) 301-320, 2005.

9


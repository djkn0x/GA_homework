Whose Vote Should Count More:
Optimal Integration of Labels from Labelers of
Unknown Expertise

Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan
Machine Perception Laboratory
University of California, San Diego
La Jolla, CA, USA
{ jake, paul, ting, jbergsma, movellan }@mplab.ucsd.edu

Abstract

Modern machine learning-based approaches to computer vision require very large
databases of hand labeled images. Some contemporary vision systems already
require on the order of millions of images for training (e.g., Omron face detector
[9]). New Internet-based services allow for a large number of labelers to collab-
orate around the world at very low cost. However, using these services brings
interesting theoretical and practical challenges: (1) The labelers may have wide
ranging levels of expertise which are unknown a priori, and in some cases may
be adversarial; (2) images may vary in their level of difﬁculty; and (3) multiple
labels for the same image must be combined to provide an estimate of the actual
label of the image. Probabilistic approaches provide a principled way to approach
these problems. In this paper we present a probabilistic model and use it to si-
multaneously infer the label of each image, the expertise of each labeler, and the
difﬁculty of each image. On both simulated and real data, we demonstrate that
the model outperforms the commonly used “Majority Vote” heuristic for inferring
image labels, and is robust to both noisy and adversarial labelers.

1

Introduction

In recent years machine learning-based approaches to computer vision have helped to greatly ac-
celerate progress in the ﬁeld. However, it is now becoming clear that many practical applications
require very large databases of hand labeled images. The labeling of very large datasets is becoming
a bottleneck for progress. One approach to address this incoming problem is to make use of the vast
human resources on the Internet. Indeed, projects like the ESP game [17], the Listen game[16], Soy-
lent Grid [15], and reCAPTCHA [18] have revealed the possibility of harnessing human resources to
solve difﬁcult machine learning problems. While these approaches use clever schemes to obtain data
from humans for free, a more direct approach is to hire labelers online. Recent Web tools such as
Amazon’s Mechanical Turk [1] provide ideal solutions for high-speed, low cost labeling of massive
databases.
Due to the distributed and anonymous nature of these tools, interesting theoretical and practical
challenges arise. For example, principled methods are needed to combine the labels from multiple
experts and to estimate the certainty of the current labels. Which image should be labeled (or
relabeled) next must also be decided – it may be prudent, for example, to collect many labels for
each image in order to increase one’s conﬁdence in that image’s label. However, if an image is easy
and the labelers of that image are reliable, a few labels may be sufﬁcient and valuable resources may
be used to label other images. In practice, combining the labels of multiple coders is a challenging
process due to the fact that: (1) The labelers may have wide ranging levels of expertise which are

unknown a priori, and in some cases may be adversarial; (2) images may also vary in their level of
difﬁculty, in a manner that may also be unknown a priori.
Probabilistic methods provide a principled way to approach this problem using standard inference
tools. We explore one such approach by formulating a probabilistic model of the labeling process,
which we call GLAD (Generative model of Labels, Abilities, and Difﬁculties), and using inference
methods to simultaneously infer the expertise of each labeler, the difﬁculty of each image, and the
most probable label for each image. On both simulated and real-life data, we demonstrate that the
model outperforms the commonly used “Majority Vote” heuristic for inferring image labels, and is
robust to both adversarial and noisy labelers.

2 Modeling the Labeling Process

Consider a database of n images, each of which belongs to one of two possible categories of interest
(e.g., face/non-face; male/female; smile/non-smile; etc.). We wish to determine the class label Zj
(0 or 1) of each image j by querying from m labelers. The observed labels depend on several causal
factors: (1) the difﬁculty of the image; (2) the expertise of the labeler; and (3) the true label. We
model the difﬁculty of image j using the parameter 1/βj ∈ [0, ∞) where βj is constrained to be
positive. Here 1/βj = ∞ means the image is very ambiguous and hence even the most proﬁcient
labeler has a 50% chance of labeling it correctly. 1/βj = 0 means the image is so easy that even the
most obtuse labeler will always label it correctly.
The expertise of each labeler i is modeled by the parameter αi ∈ (−∞, +∞). Here an α = +∞
means the labeler always labels images correctly; −∞ means the labeler always labels the images
incorrectly, i.e., he/she can distinguish between the two classes perfectly but always inverts the label,
either maliciously or because of a consistent misunderstanding. In this case (αi < 0), the labeler
is said to be adversarial. Finally, αi = 0 means that the labeler cannot discriminate between the
two classes – his/her labels carry no information about the true image label Zj . Note that we do not
require the labelers to be human – labelers can also be, for instance, automatic classiﬁers. Hence,
the proposed approach will provide a principled way of combining labels from any combination of
human and previously existing machine-based classiﬁers.
The labels given by labeler i to image j (which we call the given labels) are denoted as Lij and,
under the model, are generated as follows:
p(Lij = Zj |αi , βj ) =

1
1 + e−αi βj
Thus, under the model, the log odds for the obtained labels being correct are a bilinear function
function of the difﬁculty of the label and the expertise of the labeler, i.e.,
p(Lij = Zj )
1 − p(Lij = Zj )
More skilled labelers (higher αi ) have a higher probability of labeling correctly. As the difﬁculty
1/βj of an image increases, the probability of the label being correct moves toward 0.5. Similarly,
as the labeler’s expertise decreases (lower αi ), the chance of correctness likewise drops to 0.5.
Adversarial labelers are simply labelers with negative α.
Figure 1 shows the causal structure of the model. True image labels Zj , labeler accuracy values αi ,
and image difﬁculty values βj are sampled from a known prior distribution. These determine the
observed labels according to Equation 1. Given a set of observed labels l = {lij }, the task is to infer
simultaneously the most likely values of Z = {Zj } (the true image labels) as well as the labeler
accuracies α = {αi } and the image difﬁculty parameters β = {βj }. In the next section we derive
the Maximum Likelihood algorithm for inferring these values.

= αiβj

log

(1)

(2)

3
Inference
The observed labels are samples from the {Lij } random variables. The unobserved variables are
the true image labels Zj , the different labeler accuracies αi , and the image difﬁculty parameters
1/βj . Our goal is to efﬁciently search for the most probable values of the unobservable variables

2

Figure 1: Graphical model of image difﬁculties, true image labels, observed labels, and labeler
accuracies. Only the shaded variables are observed.

Z, α and β given the observed data. Here we can use Expectation-Maximization approach (EM)
to obtain maximum likelihood estimates of the parameters of interest (the full derivation is in the
Supplementary Materials):
E step: Let the set of all given labels for an image j be denoted as lj = {lij 0 | j 0 = j }. Note
that not every labeler must label every single image. In this case, the index variable i in lij 0 refers
only to those labelers who labeled image j . We need to compute the posterior probabilities of all
zj ∈ {0, 1} given the α, β values from the last M step and the observed labels:
p(zj |l, α, β) = p(zj |lj , α, βj )
∝ p(zj ) Y
∝ p(zj |α, βj )p(lj |zj , α, βj )
p(lij |zj , αi , βj )
i
where we noted that p(zj |α, βj ) = p(zj ) using the conditional independence assumptions from the
graphical model.
M step: We maximize the standard auxiliary function Q, which is deﬁned as the expectation of the
joint log-likelihood of the observed and hidden variables (l, Z) given the parameters (α, β ), w.r.t. the
posterior probabilities of the Z values computed during the last E step:
!
ln Y
 
Q(α, β) = E [ln p(l, z|α, β)]
p(zj ) Y
p(lij |zj , αi , βj )
= E
E [ln p(zj )] + X
= X
j
i
since lij are cond. indep. given z, α, β
E [ln p(lij |zj , αi , βj )]
j
ij
where the expectation is taken over z given the old parameter values αold , βold as estimated during
the last E-step. Using gradient ascent, we ﬁnd values of α and β that locally maximize Q.

3.1 Priors on α, β

The Q function can be modiﬁed straightforwardly to handle a prior over each αi and βj by adding a
log-prior term for each of these variables. These priors may be useful, for example, if we know that
most labelers are not adversarial. In this case, the prior for α can be made very low for α < 0.
The prior probabilities are also useful when the ground-truth Z value of particular images is (some-
how) known for certain. By “clamping” the Z values (using the prior) for the images on which the

3

Observed labelsLabeler accuraciesTrue labelsZ1Z2Z3Zn...α2α3αm...L11L21L22...Image difﬁcultiesβ1β2β3βn...α1...L32L12...true label is known for sure, the model may be able to better estimate the other parameters. The Z
values for such images can be clamped by setting the prior probability p(zj ) (used in the E-Step) for
these images to be very high towards one particular class. In our implementation we used Gaussian
priors (µ = 1, σ = 1) for α. For β , we need a prior that does not generate negative values. To do so
and imposed a Gaussian prior (µ = 1, σ = 1) on β 0 .
.= eβ 0
we re-parameterized β

3.2 Computational Complexity

The computational complexity of the E-Step is linear in the number of images and the total number
of labels. For the M-Step, the values of Q and ∇Q must be computed repeatedly until convergence.1
Computing each function is linear in the number of images, number of labelers, and total number of
image labels.
Empirically when using the approach on a database of 1 million images that we recently collected
and labeled we found that the EM procedure converged in about 10 minutes using a single core of
a Xeon 2.8 GHz processor. The algorithm is parallelizable and hence this running time could be
reduced substantially using multiple cores. Real time inference may also be possible if we maintain
parameters close to the solution that are updated as new labels become available. This would allow
using the algorithm in an active manner to choose in real-time which images should be labeled next
so as to minimize the uncertainty about the image labels.

4 Simulations

Here we explore the performance of the model using a set of image labels generated by the model
itself. Since, in this case we know the parameters Z, α, and β that generated the observed labels,
we can compare them with corresponding parameters estimated using the EM procedure.
In particular, we simulated between 4 and 20 labelers, each labeling 2000 images, whose true labels
Z were either 0 or 1 with equal probability. The accuracy αi of each labeler was drawn from a normal
distribution with mean 1 and variance 1. The inverse-difﬁculty for each image βj was generated
by exponentiating a draw from a normal distribution with mean 1 and variance 1. Given these
labeler abilities and image difﬁculties, the observed labels lij were sampled according to Equation
1 using Z. Finally, the EM inference procedure described above was executed to estimate α, β , Z.
This procedure was repeated 40 times to smooth out variability between trials. On each trial we
computed the correlation between the parameter estimates ˆα, ˆβ and the true parameter values α, β .
The results (averaged over all 40 experimental runs) are shown in Figure 2. As expected, as the
number of labelers grows, the parameter estimates converge to the true values.
We also computed the proportion of label estimates ˆZ that matched the true image labels Z. We
compared the maximum likelihood estimates of the GLAD model to estimates obtained by taking
the majority vote as the predicted label. The predictions of the proposed GLAD model were ob-
tained by thresholding at 0.5 the posterior probability of the label of each image being of class 1
given the accuracy and difﬁculty parameters returned by EM (see Section 3). Results are shown
in Figure 2. GLAD makes fewer errors than the majority vote heuristic. The difference between
the two approaches is particularly pronounced when the number of labelers per image is small. On
many images, GLAD correctly infers the true image label Z even when that Z value was the mi-
nority opinion. In essence, GLAD is exploiting the fact that some labelers are experts (which it
infers automatically), and hence their votes should count more on these images than the votes of less
skilled labelers.
Modeling Image Difﬁculty : To explore the importance of estimating image difﬁculty we performed
a simple simulation: Image labels (0 or 1) were assigned randomly (with equal probability) to 1000
images. Half of the images were “hard”, and half were “easy.” Fifty simulated labelers labeled all
1000 images. The proportion of “good” to “bad” labelers is 25:1. The probability of correctness for
each image difﬁculty and labeler quality combination was given by the table below:

1The libgsl conjugate gradient descent optimizer we used requires both Q and ∇Q.

4

Figure 2: Left: The accuracies of the GLAD model versus simple voting for inferring the underlying
class labels on simulation data. Right: The ability of GLAD to recover the true alpha and beta
parameters on simulation data.

Image Type
Labeler type Hard Easy
1
0.95
Good
0.54
1
Bad

We measured performance in terms of proportion of correctly estimated labels. We compared three
approaches: (1) our proposed method, GLAD; (2) the method proposed in [5], which models labeler
ability but not image difﬁculty; and (3) Majority Vote. The simulations were repeated 20 times
and average performance calculated for the three methods. The results shown below indicated that
modeling image difﬁculty can result in signiﬁcant performance improvements.

Method
GLAD
Majority Vote
Dawid & Skene [5]

Error
4.5%
11.2%
8.4%

4.1 Stability of EM under Various Starting Points

Empirically we found that the EM procedure was fairly insensitive to varying the starting point of the
parameter values. In a simulation study of 2000 images and 20 labelers, we randomly selected each
αi ∼ U [0, 4] and log(βj ) ∼ U [0, 3], and EM was run until convergence. Over the 50 simulation
runs, the average percent-correct of the inferred labels was 85.74%, and the standard deviation of
the percent-correct over all the trials was only 0.024%.

5 Empirical Study I: Greebles

As a ﬁrst test-bed for GLAD using real data obtained from the Mechanical Turk, we posted pictures
of 100 “Greebles” [6], which are synthetically generated images that were originally created to study
human perceptual expertise. Greebles somewhat resemble human faces and have a “gender”: Males
have horn-like organs that point up, whereas for females the horns point down. See Figure 3 (left)
for examples. Each of the 100 Greeble images was labeled by 10 different human coders on the Turk
for gender (male/female). Four greebles of each gender (separate from the 100 labeled images) were
given as examples of each class. Shown at a resolution of 48x48 pixels, the task required careful
inspection of the images in order to label them correctly. The ground-truth gender values were all
known with certainty (since they are rendered objects) and thus provided a means of measuring the
accuracy of inferred image labels.

5

51015200.750.80.850.90.951Effect of Number of Labelers on AccuracyNumber of LabelersProportion of Labels Correct  GLADMajority vote510152000.20.40.60.81Effect of Number of Labelers on Parameter EstimatesNumber of LabelersCorrelation  Beta: Spearman Corr.Alpha: Pearson Corr.Figure 3: Left: Examples of Greebles. The top two are “male” and the bottom two are “female.”
Right: Accuracy of the inferred labels, as a function of the number of labels M obtained for each
image, of the Greeble images using either GLAD or Majority Vote. Results were averaged over 100
experimental runs.

We studied the effect of varying the number of labels M obtained from different labelers for each
image, on the accuracy of the inferred Z. Hence, from the 10 labels total we obtained per Greeble
image, we randomly sampled 2 ≤ M ≤ 8 labels over all labelers during each experimental trial. On
each trial we compared the accuracy of labels Z as estimated by GLAD (using a threshold of 0.5
on p(Z )) to labels as estimated by the Majority Vote heuristic. For each value of M we averaged
performance for each method over 100 trials.
Results are shown in Figure 3 (right). For all values of M we tested, the labels as inferred by GLAD
are signiﬁcantly higher than for Majority Vote (p < 0.01). This means that, in order to achieve the
same level of accuracy, fewer labels are needed. Moreover, the variance in accuracy was less for
GLAD than for Majority Vote for all M that were tested, suggesting that the quality of GLAD’s
outputs is more stable than of the heuristic method. Finally, notice how, for the even values of M ,
the Majority Vote accuracy decreases. This may stem from the lack of optimal decision rule under
Majority Vote when an equal number of labelers say an image is Male as who say it is Female.
GLAD, since it makes its decisions by also taking ability and difﬁculty into account, does not suffer
from this problem.

6 Empirical Study II: Duchenne Smiles

As a second experiment, we used the Mechanical Turk to label face images containing smiles as
either Duchenne or Non-Duchenne. A Duchenne smile (“enjoyment” smile) is distinguished from a
Non-Duchenne (“social” smile) through the activation of the Orbicularis Oculi muscle around the
eyes, which the former exhibits and the latter does not (see Figure 4 for examples). Distinguishing
the two kinds of smiles has applications in various domains including psychology experiments,
human-computer interaction, and marketing research. Reliable coding of Duchenne smiles is a
difﬁcult task even for certiﬁed experts in the Facial Action Coding System.
We obtained Duchenne/Non-Duchenne labels for 160 images from 20 different Mechanical Turk
labelers; in total, there were 3572 labels. (Hence, labelers labeled each image a variable number of
times.) For ground truth, these images were also labeled by two certiﬁed experts in the Facial Action
Coding System. According to the expert labels, 58 out of 160 images contained Duchenne smiles.
Using the labels obtained from the Mechanical Turk, we inferred the image labels using either
GLAD or the Majority Vote heuristic, and then compared them to ground truth.

6

23456780.850.90.951Number of labels per imageAccuracy (% correct)Inferred Label Accuracy of Greeble Images  GLADMajority VoteDuchenne Smiles

Non-Duchenne Smiles

Figure 4: Examples of Duchenne (left) and Non-Duchenne (right) smiles. The distinction lies in
the activation of Orbicularis Oculi muscle around the eyes, and is difﬁcult to discriminate even for
experts.

Figure 5: Accuracy (percent correct) of inferred Duchenne/Non-Duchenne labels using either
GLAD or Majority Vote under (left) noisy labelers or (right) adversarial labelers. As the number of
noise/adversarial labels increases, the performance of labels inferred using Majority Vote decreases.
GLAD, in contrast, is robust to these conditions.

Results: Using just the raw labels obtained from the Mechanical Turk, the labels inferred using
GLAD matched the ground-truth labels on 78.12% of the images, whereas labels inferred using
Majority Vote were only 71.88% accurate. Hence, GLAD resulted in about a 6% performance gain.

Simulated Noisy and Adversarial Labelers: We also simulated noisy and adversarial labeler con-
ditions. It is to be expected, for example, that in some cases labelers may just try to complete the task
in a minimum amount of time disregarding accuracy. In other cases labelers may misunderstand the
instructions, or may be adversarial, thus producing labels that tend to be opposite to the true labels.
Robustness to such noisy and adversarial labelers is important, especially as the popularity of Web-
based labeling tools increases, and the quality of labelers becomes more diverse. To investigate the
robustness of the proposed approaches we generated data from virtual “labelers” whose labels were
completely uninformative, i.e., uniformly random. We also added artiﬁcial “adversarial” labelers
whose labels tended to be the opposite of the true label for each image.
The number of noisy labels was varied from 0 to 5000 (in increments of 500), and the number of
adversarial labels was varied from 0 to 750 (in increments of 250). For each setting, label inference
accuracy was computed for both GLAD and the Majority Vote method. As shown in Figure 5, the
accuracy of GLAD-based label inference is much less affected from labeling noise than is Majority
Vote. When adversarial labels are introduced, GLAD automatically inferred that some labelers were
purposely giving the opposite label and automatically ﬂipped their labels. The Majority Vote heuris-
tic, in contrast, has no mechanism to recover from this condition, and the accuracy falls steeply.

7

0100020003000400050000.660.680.70.720.740.760.780.8Num of Noisy LabelsAccuracyAccuracy under Noise  GLADMajority Vote02004006008000.40.50.60.70.80.91Num of Adversarial LabelsAccuracyAccuracy under Adversarialness  GLADMajority Vote7 Related Work

To our knowledge GLAD is the ﬁrst model in the literature to simultaneously estimate the true label,
item difﬁculty, and coder expertise in an unsupervised and efﬁcient manner.
Our work is related to the literature on standardized tests, particularly the Item Response Theory
(IRT) community (e.g., Rasch [10], Birnbaum [3]). The GLAD model we propose in this paper can
be seen as an unsupervised version of previous IRT models for the case in which the correct answers
(i.e., labels) are unknown.
Snow, et al [14] used a probabilistic model similar to Naive Bayes to show that by averaging mul-
tiple naive labelers (<= 10) one can obtain labels as accurate as a few expert labelers. Two key
differences between their model and GLAD are that: (1) they assume a signiﬁcant proportion of
images have been pre-labeled with ground truth values, and (2) all the images have equal difﬁculty.
As we show in this paper, modeling image difﬁculty may be very important in some cases. Sheng,
et al [12] examine how to identify which images of an image dataset to label again in order to reduce
uncertainty in the posterior probabilities of latent class labels.
Dawid and Skene [5] developed a method to handle polytomous latent class variables. In their case
the notion of “ability” is handled using full confusion matrices for each labeler. Smyth, et al [13]
used a similar approach to combine labels from multiple experts for items with homogeneous levels
of difﬁculty. Batchelder and Romney [2] infer test answers and test-takers’ abilities simultaneously,
but do not estimate item difﬁculties and do not admit adversarial labelers.
Other approaches employ a Bayesian model of the labeling process that considers both variability
in labeler accuracies as well as item difﬁculty (e.g. [8, 7, 11]). However, inference in these models
is based on MCMC which is likely to suffer from high computational expense, and the need to wait
(arbitrarily long) for parameters to “burn in” during sampling.

8 Summary and Further Research

An important bottleneck facing the machine learning community is the need for very large datasets
with hand-labeled data. Datasets whose scale was unthinkable a few years ago are becoming com-
monplace today. The Internet makes it possible for people around the world to cooperate on the
labeling of these datasets. However, this makes it unrealistic for individual researchers to obtain the
ground truth of each label with absolute certainty. Algorithms are needed to automatically estimate
the reliability of ad-hoc anonymous labelers, the difﬁculty of the different items in the dataset, and
the probability of the true labels given the currently available data.
We proposed one such system, GLAD, based on standard probabilistic inference on a model of the
labeling process. The approach can handle the millions of parameters (one difﬁculty parameter per
image, and one expertise parameter per labeler) needed to process large datasets, at little compu-
tational cost. The model can be used seamlessly to combine labels from both human labelers and
automatic classiﬁers. Experiments show that GLAD can recover the true data labels more accurately
than the Majority Vote heuristic, and that it is highly robust to both noisy and adversarial labelers.
Active Sampling: One advantage of probabilistic models is that they lend themselves to implement-
ing active methods (e.g., Infomax [4]) for selecting which images should be re-labeled next. We are
currently pursuing the development of control policies for optimally choosing whether to obtain
more labels for a particular item – so that the inferred Z label for that item becomes more certain
– versus obtaining more labels from a particular labeler – so that his/her accuracy α may be better
estimated, and all the images that he/she labeled can have their posterior probability estimates of Z
improved.
A software implementation of GLAD is available at http://mplab.ucsd.edu/∼jake.

References
[1] Amazon. Mechanical turk. http://www.mturk.com.
[2] W. H. Batchelder and A. K. Romney. Test theory without an answer key. Psychometrika, 53(1):71–92,
1988.

8

In Proceedings of the

[3] A. Birnbaum. Some latent trait models and their use in inferring an examinee’s ability. Statistical theories
of mental test scores, 1968.
[4] N. Butko and J. Movellan.
I-POMDP: An infomax model of eye movement.
International Conference on Development and Learning, 2008.
[5] A. Dawid and A. Skene. Maximum likelihood estimation of observer error-rates using the em algorithm.
Applied Statistics, 28(1):20–28, 1979.
[6] I. Gauthier and M. Tarr. Becoming a “greeble” expert: Exploring mechanisms for face recognition. Vision
Research, 37(12), 1997.
[7] V. Johnson. On bayesian analysis of multi-rater ordinal data: An application to automated essay grading.
Journal of the American Statistical Association, 91:42–51, 1996.
[8] G. Karabatsos and W. H. Batchelder. Markov chain estimation for test theory without an answer key.
Psychometrika, 68(3):373–389, 2003.
[9] Omron. OKAO vision brochure, July 2008.
[10] G. Rasch. Probabilistic Models for Some Intelligence and Attainment Tests. Denmark, 1960.
[11] S. Rogers, M. Girolami, and T. Polajnar. Semi-parametric analysis of multi-rater data. Statistics and
Computing, 2009.
[12] V. Sheng, F. Provost, and P. Ipeirotis. Get another label? improving data quality and data mining using
multiple noisy labelers. In Knowledge Discovery and Data Mining, 2008.
[13] P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective labelling of
venus images. In Advances of Neural Information Processing Systems, 1994.
[14] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast - but is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods on
Natural Language Processing, 2008.
[15] S. Steinbach, V. Rabaud, and S. Belongie. Soylent grid: it’s made of people! In International Conference
on Computer Vision, 2007.
[16] D. Turnbull, R. Liu, L. Barrington, and G. Lanckriet. A Game-based Approach for Collecting Semantic
Annotations of Music. In 8th International Conference on Music Information Retrieval (ISMIR), 2007.
[17] L. von Ahn and L. Dabbish. Labeling Images with A Computer Game. In Proceedings of the SIGCHI
conference on Human factors in computing systems, pages 319–326. ACM Press New York, NY, USA,
2004.
[18] L. von Ahn, B. Maurer, C. McMillen, D. Abraham, and M. Blum. reCAPTCHA: Human-Based Character
Recognition via Web Security Measures. Science, 321(5895):1465, 2008.

9


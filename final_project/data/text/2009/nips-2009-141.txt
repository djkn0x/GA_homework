Compressed Least-Squares Regression

Odalric-Ambrym Maillard and R ´emi Munos
SequeL Project, INRIA Lille - Nord Europe, France
{odalric.maillard, remi.munos}@inria.fr

Abstract

We consider the problem of learning, from K data, a regression function in a lin-
ear space of high dimension N using projections onto a random subspace of lower
dimension M . From any algorithm minimizing the (possibly penalized) empiri-
cal risk, we provide bounds on the excess risk of the estimate computed in the
projected subspace (compressed domain) in terms of the excess risk of the esti-
mate built in the high-dimensional space (initial domain). We show that solving
the problem in the compressed domain instead of the initial domain reduces the
estimation error at the price of an increased (but controlled) approximation error.
We apply the analysis to Least-Squares (LS) regression and discuss the excess
√
risk and numerical complexity of the resulting “Compressed Least Squares Re-
√
gression” (CLSR) in terms of N , K , and M . When we choose M = O(
K ), we
K ).
show that CLSR has an estimation error of order O(log K/

1 Problem setting
We consider a regression problem where we observe data DK = ({xk , yk }k≤K ) (where xk ∈ X and
yk ∈ R) are assumed to be independently and identically distributed (i.i.d.) from some distribution
P , where xk ∼ PX and yk = f ∗ (xk ) + ηk (xk ), where f ∗ is the (unknown) target function, and ηk
a centered independent noise of variance σ2 (xk ). For a given class of functions F , and f ∈ F , we
K(cid:88)
deﬁne the empirical (quadratic) error
k=1

[yk − f (xk )]2 ,

LK (f ) def=

1
K

and the generalization (quadratic) error
L(f ) def= E(X,Y )∼P [(Y − f (X ))2 ].
Our goal is to return a regression function (cid:98)f ∈ F with lowest possible generalization error L( (cid:98)f ).
Notations: In the sequel we will make use of the following notations about norms: for h : X (cid:55)→ R,
(cid:1)1/2 .
of h w.r.t. the empirical measure PK , and for u ∈ Rn , ||u|| denotes by default (cid:0) (cid:80)n
we write ||h||P for the L2 norm of h with respect to (w.r.t.) the measure P , ||h||PK for the L2 norm
i=1 u2
f ∗ /∈ F . For any regression function (cid:98)f , we deﬁne the excess risk
i
The measurable function minimizing the generalization error is f ∗ , but it may be the case that
L( (cid:98)f ) − L(f ∗ ) = || (cid:98)f − f ∗ ||2
which decomposes as the sum of the estimation error L( (cid:98)f ) − inf f ∈F L(f ) and the approximation
P ,
error inf f ∈F L(f ) − L(f ∗ ) = inf f ∈F ||f − f ∗ ||2
P which measures the distance between f ∗ and the
function space F .

1

,

(1)

def= (cid:80)N
In this paper we consider a class of linear functions FN deﬁned as the span of a set of N functions
{ϕn}1≤n≤N called features. Thus: FN
def= {fα
n=1 αnϕn , α ∈ RN }.
When the number of data K is larger than the number of features N , the ordinary Least-Squares
Regression (LSR) provides the LS solution f bα which is the minimizer of the empirical risk LK (f )
in FN . Note that here LK (fα ) rewrites 1
K ||Φα − Y ||K where Φ is the K × N matrix with elements
(ϕn (xk ))1≤n≤N ,1≤k≤K and Y the K -vector with components (yk )1≤k≤K .
Usual results provide bound on the estimation error as a function of the capacity of the function
space and the number of data. In the case of linear approximation, the capacity measures (such as
covering numbers [23] or the pseudo-dimension [16]) depend on the number of features (for example
the pseudo-dimension is at most N + 1). For example, let f bα be a LS estimate (minimizer of LK
in FN ), then (a more precise statement will be stated later in Subsection 3) the expected estimation
L(f )(cid:3) ≤ cσ2 N log K
E(cid:2)L(f bα ) − inf
error is bounded as:
f ∈FN
K
where c is a universal constant, σ def= supx∈X σ(x), and the expectation is taken with respect to P .
[13] (e.g. when (cid:83)
Now, the excess risk is the sum of this estimation error and the approximation error inf f ∈FN ||f −
f ∗ ||P of the class FN . Since the later usually decreases when the number of features N increases
N FN is dense in L2 (P )), we see the usual tradeoff between small estimation error
(low N ) and small approximation error (large N ).
In this paper we are interested in the setting when N is large so that the approximation error is small.
Whenever N is larger than K we face the overﬁtting problem since there are more parameters than
actual data (more variables than constraints), which is illustrated in the bound (1) which provides
no information about the generalization ability of any LS estimate.
In addition, there are many
minimizers (in fact a vector space of same dimension as the null space of ΦT Φ) of the empirical
risk. To overcome the problem, several approaches have been proposed in the literature:
ror with minimal (l1 or l2 )-norm: (cid:98)α = arg minΦα=Y ||α||1 or 2 , (or a robust solution
• LS solution with minimal norm: The solution is the minimizer of the empirical er-
arg min||Φα−Y ||2≤ε ||α||1 ). The choice of (cid:96)2 -norm yields the ordinary LS solution. The
choice of (cid:96)1 -norm has been used for generating sparse solutions (e.g. the Basis Pursuit
[10]), and assuming that the target function admits a sparse decomposition, the ﬁeld of
Compressed Sensing [9, 21] provides sufﬁcient conditions for recovering the exact so-
lution. However, such conditions (e.g. that Φ possesses a Restricted Isometric Property
(RIP)) does not hold in general in this regression setting. On another aspect, solving these
problems (both for l1 or l2 -norm) when N is large is numerically expensive.
• Regularization. The solution is the minimizer of the empirical error plus a penalty term,
(cid:98)f = arg min
for example
LK (f ) + λ||f ||p
for p = 1 or 2.
p ,
f ∈FN
(cid:96)1 (LASSO [19]). A close alternative is the Dantzig selector [8, 5] which solves: (cid:98)α =
where λ is a parameter and usual choices for the norm are (cid:96)2 (ridge-regression [20]) and
arg min||α||1≤λ ||ΦT (Y − Φα)||∞ . The numerical complexity and generalization bounds
of those methods depend on the sparsity of the target function decomposition in FN .
Now if we possess a sequence of function classes (FN )N ≥1 with increasing capacity, we may per-
term that depends on the size of the model: (cid:98)fN = arg minf ∈FN ,N ≥1 LK (f ) + pen(N , K ), where
form structural risk minimization [22] by solving in each model the empirical risk penalized by a
the penalty term measures the capacity of the function space.
In this paper we follow another approach where instead of searching in the large space FN (where
N > K ) for a solution that minimizes the empirical error plus a penalty term, we simply search
for the empirical error minimizer in a (randomly generated) lower dimensional subspace GM ⊂ FN
(where M < K ).

Our contribution: We consider a set of M random linear combinations of the initial N features
and perform our favorite LS regression algorithm (possibly regularized) using those “compressed

2

features”. This is equivalent to projecting the K points {ϕ(xk ) ∈ RN , k = 1..K } from the initial
domain (of size N ) onto a random subspace of dimension M , and then performing the regres-
sion in the “compressed domain” (i.e. span of the compressed features). This is made possible
because random projections approximately preserve inner products between vectors (by a variant of
the Johnson-Lindenstrauss Lemma stated in Proposition 1.
Our main result is a bound on the excess risk of a linear estimator built in the compressed domain
in terms of the excess risk of the linear estimator built in the initial domain (Section 2). We further
detail the case of ordinary Least-Squares Regression (Section 3) and discuss, in terms of M , N , K ,
the different tradeoffs concerning the excess risk (reduced estimation error in the compressed do-
main versus increased approximation error introduced by the random projection) and the numerical
complexity (reduced complexity of solving the LSR in the compressed domain versus the additional
load of performing the projection).
√
K ) projections we deﬁne a Compressed
As a consequence, we show that by choosing M = O(
√
Least-Squares Regression which uses O(N K 3/2 ) elementary operations to compute a regression
function with estimation error (relatively to the initial function space FN ) of order log K/
K up to
a multiplicative factor which depends on the best approximation of f ∗ in FN . This is competitive
with the best methods, up to our knowledge.

Related works: Using dimension reduction and random projections in various learning areas has
received considerable interest over the past few years. In [7], the authors use a SVM algorithm in a
compressed space for the purpose of classiﬁcation and show that their resulting algorithm has good
generalization properties. In [25], the authors consider a notion of compressed linear regression.
For data Y = X β + ε, where β is the target and ε a standard noise, they use compression of the
set of data, thus considering AY = AX β + Aε, where A has a Restricted Isometric Property.
They provide an analysis of the LASSO estimator built from these compressed data, and discuss a
property called sparsistency, i.e. the number of random projections needed to recover β (with high
probability) when it is sparse. These works differ from our approach in the fact that we do not
consider a compressed (input and/or output) data space but a compressed feature space instead.
In [11], the authors discuss how compressed measurements may be useful to solve many detection,
classiﬁcation and estimation problems without having to reconstruct the signal ever. Interestingly,
they make no assumption about the signal being sparse, like in our work. In [6, 17], the authors
show how to map a kernel k(x, y) = ϕ(x) · ϕ(y) into a low-dimensional space, while still approx-
imately preserving the inner products. Thus they build a low-dimensional feature space speciﬁc for
(translation invariant) kernels.

2 Linear regression in the compressed domain
def= {fα = (cid:80)N
We remind that the initial set of features is {ϕn : X (cid:55)→ R, 1 ≤ n ≤ N } and the initial domain
n=1 αnϕn , α ∈ RN } is the span of those features. We write ϕ(x) the N -vector of
FN
components (ϕn (x))n≤N . Let us now deﬁne the random projection. Let A be a M × N matrix of
i.i.d. elements drawn for some distribution ρ. Examples of distributions are:
• Gaussian random variables N (0, 1/M ),
√
• Distribution taking values ±(cid:112)3/M with probability 1/6 and 0 with probability 2/3.
• ± Bernoulli distributions, i.e. which takes values ±1/
M with equal probability 1/2,
The following result (proof in the supplementary material) states the property that inner-product are
approximately preserved through random projections (this is a simple consequence of the Johnson-
Lindenstrauss Lemma):
Proposition 1 Let (uk )1≤k≤K and v be vectors of RN . Let A be a M × N matrix of i.i.d. el-
ements drawn from one of the previously deﬁned distributions. For any ε > 0, δ > 0, for
δ , we have, with probability at least 1 − δ , for all k ≤ K ,
M ≥ 1
log 4K
4 − ε3
ε2
6
|Auk · Av − uk · v | ≤ ε||uk || ||v ||.

3

(cid:80)N
We now introduce the set of M compressed features (ψm )1≤m≤M such that ψm (x) def=
def= {gβ = (cid:80)M
n=1 Am,nϕn (x). We also write ψ(x) the M -vector of components (ψm (x))m≤M . Thus
ψ(x) = Aϕ(x). We deﬁne the compressed domain GM
m=1 βmψm , β ∈ RM } the
span of the compressed features (vector space of dimension at most M ). Note that each ψm ∈ FN ,
thus GM is a subspace of FN .

2.1 Approximation error
We now compare the approximation error assessed in the compressed domain GM versus in the
initial space FN . This applies to the linear algorithms mentioned in the introduction such as ordinary
LS regression (analyzed in details in Section 3), but also its penalized versions, e.g. LASSO and
ridge regression. Deﬁne α+ = arg minα∈RN L(fα ) − L(f ∗ ) the parameter of the best regression
function in FN .
Theorem 1 For any δ > 0, any M ≥ 15 log(8K/δ), let A be a random M × N matrix deﬁned
like in Proposition 1, and GM be the compressed domain resulting from this choice of A. Then with
(cid:114) log 4/δ
(cid:17)
||α+ ||2(cid:16)
probability at least 1 − δ ,
E(cid:2)||ϕ(X )||2 (cid:3)+2 sup
P ≤ 8 log(8K/δ)
||g−f ∗ ||2
||f −f ∗ ||2
||ϕ(x)||2
inf
+ inf
P .
g∈GM
f ∈FN
2K
x∈X
M
(2)
This theorem shows the tradeoff in terms of estimation and approximation errors for an estimator (cid:98)g
obtained in the compressed domain compared to an estimator (cid:98)f obtained in the initial domain:
• Bounds on the estimation error of (cid:98)g in GM are usually smaller than that of (cid:98)f in FN when
M < N (since the capacity of FN is larger than that of GM ).
• Theorem 1 says that the approximation error assessed in GM increases by at most
M )||α+ ||2E||ϕ(X )||2 compared to that in FN .
O( log(K/δ)
Proof: Let us write f + def= fα+ = arg minf ∈FN ||f − f ∗ ||P and g+ def= gAα+ . The approximation
error assessed in the compressed domain GM is bounded as
||g − f ∗ ||2
P ≤ ||g+ − f ∗ ||2
P = ||g+ − f + ||2
P + ||f + − f ∗ ||2
inf
(3)
P ,
g∈GM
since f + is the orthogonal projection of f ∗ on FN and g+ belongs to FN . We now bound ||g+ −
P using concentration inequalities. Deﬁne Z (x) def= Aα+ · Aϕ(x) − α+ · ϕ(x). Deﬁne ε2 def=
f + ||2
M log(8K/δ). For M ≥ 15 log(8K/δ) we have ε < 3/4 thus M ≥ log(8K/δ)
ε2 /4−ε3 /6 . Proposition 1
8
applies and says that on an event E of probability at least 1 − δ/2, we have for all k ≤ K ,
||ϕ(x)|| def= C
|Z (xk )| ≤ ε||α+ || ||ϕ(xk )|| ≤ ε||α+ || sup
x∈X
(cid:114) log(2/δ (cid:48) )
On the event E , we have with probability at least 1 − δ (cid:48) ,
K(cid:88)
P = EX∼PX |Z (X )|2 ≤ 1
||g+ − f + ||2
(cid:114) log(2/δ (cid:48) )
≤ ε2 ||α+ ||2(cid:16) 1
K(cid:88)
2K
K
k=1
||ϕ(xk )||2 + sup
||ϕ(x)||2
(cid:114) log(2/δ (cid:48) )
≤ ε2 ||α+ ||2(cid:16)
(cid:17)
E(cid:2)||ϕ(X )||2 (cid:3) + 2 sup
2K
x∈X
K
k=1
2K
x∈X
where we applied two times Chernoff-Hoeffding’s inequality. Combining with (3), unconditioning,
and setting δ (cid:48) = δ/2 then with probability at least (1 − δ/2)(1 − δ (cid:48) ) ≥ 1 − δ we have (2).
(cid:3)

|Z (xk )|2 + C 2

||ϕ(x)||2

(4)

(cid:17)

.

4

2.2 Computational issues
We now discuss the relative computational costs of a given algorithm applied either in the initial or
operations) of an algorithm A to compute the regression function (cid:98)f when provided with the data
in the compressed domain. Let us write Cx(DK , FN , P ) the complexity (e.g. number of elementary
DK and function space FN .
We plot in the table below, both for the initial and the compressed versions of the algorithm A, the
estimator, (iii) the cost for making one prediction (i.e. computing (cid:98)f (x) for any x):
order of complexity for (i) the cost for building the feature matrix, (ii) the cost for computing the
Compressed domain
Initial domain
Construction of the feature matrix
N K
N KM
Computing the regression function Cx(DK , FN , P )
Cx(DK , GM , P )
Making one prediction
N M
N

Note that the values mentioned for the compressed domain are upper-bounds on the real complexity
and do not take into account the possible sparsity of the projection matrix A (which would speed up
matrix computations, see e.g. [2, 1]).

3 Compressed Least-Squares Regression

We now analyze the speciﬁc case of Least-Squares Regression.

3.1 Excess risk of ordinary Least Squares regression

In order to bound the estimation error, we follow the approach of [13] which truncates (up to the
level ±L where L is a bound, assumed to be known, on ||f ∗ ||∞ ) the prediction of the LS regression
function. The ordinary LS regression provides the regression function f bα where
(cid:98)α =
||α||.
argmin
α∈argminα(cid:48) ∈RN ||Y −Φα(cid:48) ||
Note that ΦΦT (cid:98)α = ΦT Y , hence (cid:98)α = Φ†Y ∈ RN where Φ† is the Penrose pseudo-inverse of Φ1 .
Then the truncated predictor is: (cid:98)fL (x) def= TL [f bα (x)], where
(cid:26) u
if |u| ≤ L,
TL (u) def=
L sign(u) otherwise.
Truncation after the computation of the parameter (cid:98)α ∈ RN , which is the solution of an unconstrained
optimization problem, is easier than solving an optimization problem under the constraint that ||α||
bounds. Indeed, the excess risk of (cid:98)fL is bounded as
is small (which is the approach followed in [23]) and allows for consistency results and prediction
E(|| (cid:98)f − f ∗ ||2
P ) ≤ c(cid:48) max{σ2 , L2} 1 + log K
||f − f ∗ ||2
N + 8 inf
(5)
f ∈FN
P
K
where a bound on c(cid:48) is 9216 (see [13]). We have a simpler bound when we consider the expectation
EY (|| (cid:98)f − f ∗ ||2
EY conditionally on the input data:
f ∈F ||f − f ∗ ||2
+ inf
PK
PK
Remark: Note that because we use the quadratic loss function, by following the analysis in [3],
or by deriving tight bounds on the Rademacher complexity [14] and following Theorem 5.2 of
Koltchinskii’s Saint Flour course, it is actually possible to state assumptions under which we can
remove the log K term in (5). We will not further detail such bounds since our motivation here is
not to provide the tightest possible bounds, but rather to show how the excess risk bound for LS
regression in the initial domain extends to the compressed domain.
1 In the full rank case, Φ† = (ΦT Φ)−1ΦT when K ≥ N and Φ† = ΦT (ΦΦT )−1 when K ≤ N

) ≤ σ2 N
K

(6)

5

CLSR is deﬁned as the ordinary LSR in the compressed domain. Let (cid:98)β = Ψ†Y ∈ RM , where Ψ
3.2 Compressed Least-Squares Regression (CLSR)
(cid:98)gL (x) def= TL [g bβ (x)]. From Theorem 1, (5) and (6), we deduce the following excess risk bounds for
is the K × M matrix with elements (ψm (xk ))1≤m≤M ,1≤k≤K . The CLSR estimate is deﬁned as
(cid:113) K log(8K/δ)
the CLSR estimate:
√
||α+ ||
E||ϕ(X )||2
c(cid:48) (1+log K ) . Then whenever M ≥
Corollary 1 For any δ > 0, set M = 8
max(σ,L)
15 log(8K/δ), with probability at least 1 − δ , the expected excess risk of the CLSR estimate is
(cid:114) (1 + log K ) log(8K/δ)
c(cid:48) max{σ, L}||α+ ||(cid:112)E||ϕ(X )||2
bounded as
E(||(cid:98)gL − f ∗ ||2
√
(cid:114) log 4/δ
P ) ≤ 16
(cid:17)
×(cid:16)
K
supx ||ϕ(x)||2
||f − f ∗ ||2
+ 8 inf
1 +
(7)
E||ϕ(X )||2
(cid:112)8K log(8K/δ). Assume N > K and that the features (ϕk )1≤k≤K
P .
f ∈FN
2K
√
||α+ ||
E||ϕ(X )||2
Now set M =
are linearly independent. Then whenever M ≥ 15 log(8K/δ), with probability at least 1 − δ , the
σ
(cid:114) 2 log(8K/δ)
(cid:114) log 4/δ
(cid:16)
(cid:17)
) ≤ 4σ ||α+ ||(cid:112)E||ϕ(X )||2
expected excess risk of the CLSR estimate conditionally on the input samples is upper bounded as
EY (||(cid:98)gL − f ∗ ||2
supx ||ϕ(x)||2
E||ϕ(X )||2
2K
PK
K
(cid:98)gL is bounded as
Proof: Whenever M ≥ 15 log(8K/δ) we deduce from Theorem 1 and (5) that the excess risk of
E(||(cid:98)gL − f ∗ ||2
(cid:114) log 4/δ
P ) ≤ c(cid:48) max{σ2 , L2 } 1 + log K
(cid:104) 8 log(8K/δ)
||α+ ||2(cid:16)
K
E||ϕ(X )||2 + 2 sup
||ϕ(x)||2
+8
+ inf
f ∈FN
2K
M
x
EY (||(cid:98)gL − f ∗ ||2
By optimizing on M , we deduce (7). Similarly, using (6) we deduce the following bound on
(cid:114) log 4/δ
(cid:17)
log(8K/δ)||α+ ||2(cid:16)
):
PK
8
||ϕ(x)||2
E||ϕ(X )||2 + 2 sup
σ2 M
+
2K
K
M
x
By optimizing on M and noticing that inf f ∈FN ||f − f ∗ ||2
= 0 whenever N > K and the features
PK
(cid:3)
(ϕk )1≤k≤K are linearly independent, we deduce the second result.
Remark 1 Note that the second term in the parenthesis of (7) is negligible whenever K (cid:29) log 1/δ .
(cid:17)
(cid:16)||α+ ||(cid:112)E||ϕ(X )||2 log K/δ√
Thus we have the expected excess risk
E(||(cid:98)gL − f ∗ ||2
+ inf
P ) = O
f ∈FN
K
The choice of M in the previous corollary depends on ||α+ || and E||ϕ(X )|| which are a priori
If we set M independently of ||α+ ||, then an addi-
unknown (since f ∗ and PX are unknown).
tional multiplicative factor of ||α+ || appears in the bound, and if we replace E||ϕ(X )|| by its bound
supx ||ϕ(x)|| (which is known) then this latter factor will appear instead of the former in the bound.

||f − f ∗ ||2
PK

||f − f ∗ ||2
P

||f − f ∗ ||2
P

.

(cid:105)

.

.

(8)

+ inf
f ∈FN

.

1 +

M

(cid:17)

Complexity of CLSR: The complexity of LSR for computing the regression function in the com-
pressed domain only depends on M and K , and is (see e.g. [4]) Cx(DK , GM , P ) = O(M K 2 ) which
√
is of order O(K 5/2 ) when we choose the optimized number of projections M = O(
K ). However
the leading term when using CLSR is the cost for building the Ψ matrix: O(N K 3/2 ).

6

4.1 The factor ||α+ ||(cid:112)E||ϕ(X )||2
4 Discussion
generalization error or not is ||α+ ||(cid:112)E||ϕ(X )||2 . This factor indicates that a good set of features
In light of Corollary 1, the important factor which will determine whether the CLSR provides low
(for CLSR) should be such that the norm of those features as well as the norm of the parameter
α+ of the projection of f ∗ onto the span of those features should be small. A natural question is
whether this product can be made small for appropriate choices of features. We now provide two
speciﬁc cases for which this is actually the case: (1) when the features are rescaled orthonormal
basis functions, and (2) when the features are speciﬁc wavelet functions. In both cases, we relate
the bound to an assumption of regularity on the function f ∗ , and show that the dependency w.r.t. N
decreases when the regularity increases, and may even vanish.

Rescaled Orthonormal Features: Consider a set of orthonormal functions (ηi )i≥1 w.r.t a measure
µ, i.e. (cid:104)ηi , ηj (cid:105)µ = δi,j . In addition we assume that the law of the input data is dominated by µ,
i.e. PX ≤ C µ where C is a constant. For instance, this is the case when the set X is compact, µ is
the uniform measure and PX has bounded density.
any f ∈ FN decomposes as f = (cid:80)N
i=1 (cid:104)f , ηi (cid:105) ηi = (cid:80)N
def= ci ηi , where ci > 0, for i ∈ {1, . . . , N }. Then
We deﬁne the set of N features as: ϕi
(cid:82)
||α||2 = (cid:80)N
)2 and E||ϕ||2 = (cid:80)N
i (x)dPX (x) ≤ C (cid:80)N
def= (cid:104)f , ηi (cid:105). Thus
ϕi , where bi
bi
)2 (cid:80)N
||α+ ||2E||ϕ||2 ≤ C (cid:80)N
i=1
ci
i=1 ( bi
we have:
i . Thus
i=1 c2
X η2
i=1 c2
i
ci
i=1 ( bi
i .
i=1 c2
ci
Now, linear approximation theory (Jackson-type theorems) tells us that assuming a function f ∗ ∈
L2 (µ) is smooth, it may be decomposed onto the span of the N ﬁrst (ηi )i∈{1,...,N } functions with
decreasing coefﬁcients |bi | ≤ i−λ for some λ ≥ 0 that depends on the smoothness of f ∗ . For
example the class of functions with bounded total variation may be decomposed with Fourier basis
(in dimension 1) with coefﬁcients |bi | ≤ ||f ||V /(2π i). Thus here λ = 1. Other classes (such as
By choosing ci = i−λ/2 , we have ||α+ ||(cid:112)E||ϕ||2 ≤ √
C (cid:80)N
Sobolev spaces) lead to larger values of λ related to the order of differentiability.
i=1 i−λ . Thus if λ > 1, then this term
is bounded by a constant that does not depend on N . If λ = 1 then it is bounded by O(log N ), and
However any orthonormal basis, even rescaled, would not necessarily yield a small ||α+ ||(cid:112)E||ϕ||2
if 0 < λ < 1, then it is bounded by O(N 1−λ ).
term (this is all the more true when the dimension of X is large). The desired property that the
coefﬁcients (α+ )i of the decomposition of f ∗ rapidly decrease to 0 indicates that hierarchical bases,
such as wavelets, that would decompose the function at different scales, may be interesting.
h,l ) (indexed by n ≥ 1 or
Wavelets: Consider an inﬁnite family of wavelets in [0, 1]: (ϕ0
n ) = (ϕ0
equivalently by the scale h ≥ 0 and translation 0 ≤ l ≤ 2h − 1) where ϕ0
h,l (x) = 2h/2ϕ0 (2hx − l)
and ϕ0 is the mother wavelet. Then consider N = 2H features (ϕh,l )1≤h≤H deﬁned as the rescaled
(cid:80)
def= ch2−h/2ϕ0
h,l , where ch > 0 are some coefﬁcients. Assume the mother wavelet
wavelets ϕh,l
is C p (for p ≥ 1), has at least p vanishing moments, and that for all h ≥ 0, supx
l ϕ0 (2hx −
supx∈X ||ϕ(x)||2 (thus on (cid:112)E||ϕ(X )||2 ) by a constant independent of N :
l)2 ≤ 1. Then the following result (proof in the supplementary material) provides a bound on
Proposition 2 Assume that f ∗ is (L, γ )-Lipschitz (i.e. for all v ∈ X there exists a polynomial pv of
(cid:82) 1
degree (cid:98)γ (cid:99) such that for all u ∈ X , |f (u) − pv (u)| ≤ L|u − v |γ ) with 1/2 < γ ≤ p. Then setting
ch = 2h(1−2γ )/4 , we have ||α+ || supx ||ϕ(x)|| ≤ L
0 |ϕ0 |, which is independent of N .
2γ
1−21/2−γ
Notice that the Haar walevets has p = 1 vanishing moment but is not C 1 , thus the Proposition does
not apply directly. However direct computations show that if f ∗ is L-Lipschitz (i.e. γ = 1) then
h,l ≤ L2−3h/2−2 , and thus ||α+ || supx ||ϕ(x)|| ≤
4(1−2−1/2 ) with ch = 2−h/4 .
L
α0

7

In the case when the factor ||α+ ||(cid:112)E||ϕ(X )||2 does not depend on N (such as in the previous
4.2 Comparison with other methods
√
√
example), the bound (8) on the excess risk of CLSR states that the estimation error (assessed in
terms of FN ) of CLSR is O(log K/
K ). It is clear that whenever N >
K (which is the case of
interest here), this is better than the ordinary LSR in the initial domain, whose estimation error is
O(N log K/K ).
It is difﬁcult to compare this result with LASSO (or the Dantzig selector that has similar properties
[5]) for which an important aspect is to design sparse regression functions or to recover a solution
assumed to be sparse. From [12, 15, 24] one deduces that under some assumptions, the estimation
√
error of LASSO is of order S log N
K where S is the sparsity (number of non-zero coefﬁcients) of the
best regressor f + in FN . If S <
K then LASSO is more interesting than CLSR in terms of excess
risk. Otherwise CLSR may be an interesting alternative although this method does not make any
assumption about the sparsity of f + and its goal is not to recover a possible sparse f + but only to
the regression function (cid:98)gL lies in a space GM of small dimension M (cid:28) N and can thus be expressed
make good predictions. However, in some sense our method ﬁnds a sparse solution in the fact that
using only M coefﬁcients.
Now in terms of numerical complexity, CLSR requires O(N K 3/2 ) operations to build the matrix
and compute the regression function, whereas according to [18], the (heuristical) complexity of the
LASSO algorithm is O(N K 2 ) in the best cases (assuming that the number of steps required for
convergence is O(K ), which is not proved theoretically). Thus CLSR seems to be a good and
simple competitor to LASSO.

5 Conclusion

We considered the case when the number of features N is larger than the number of data K . The
result stated in Theorem 1 enables to analyze the excess risk of any linear regression algorithm (LS
or its penalized versions) performed in the compressed domain GM versus in the initial space FN .
regression, when the term ||α+ ||(cid:112)E||ϕ(X )||2 has a mild dependency on N , then by choosing a
In the compressed domain the estimation error is reduced but an additional (controlled) approxima-
tion error (when compared to the best regressor in FN ) comes into the picture. In the case of LS
√
√
random subspace of dimension M = O(
K ), CLSR has an estimation error (assessed in terms of
FN ) bounded by O(log K/
K ) and has numerical complexity O(N K 3/2 ).
In short, CLSR provides an alternative to usual penalization techniques where one ﬁrst selects a ran-
for which the term ||α+ ||(cid:112)E||ϕ(X )||2 is small.
dom subspace of lower dimension and then performs an empirical risk minimizer in this subspace.
Further work needs to be done to provide additional settings (when the space X is of dimension > 1)
Acknowledgements: The authors wish to thank Laurent Jacques for numerous comments and
Alessandro Lazaric and Mohammad Ghavamzadeh for exciting discussions. This work has been
supported by French National Research Agency (ANR) through COSINUS program (project
EXPLO-RA, ANR-08-COSI-004).

References
[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with bi-
nary coins. Journal of Computer and System Sciences, 66(4):671–687, June 2003.
[2] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast Johnson-
Lindenstrauss transform. In STOC ’06: Proceedings of the thirty-eighth annual ACM sym-
posium on Theory of computing, pages 557–563, New York, NY, USA, 2006. ACM.
[3] Jean-Yves Audibert and Olivier Catoni. Risk bounds in linear regression through pac-bayesian
truncation. Technical Report HAL : hal-00360268, 2009.
[4] David Bau III and Lloyd N. Trefethen. Numerical linear algebra. Philadelphia: Society for
Industrial and Applied Mathematics, 1997.

8

[5] Peter J. Bickel, Ya’acov Ritov, and Alexandre B. Tsybakov. Simultaneous analysis of Lasso
and Dantzig selector. To appear in Annals of Statistics, 2008.
[6] Avrim Blum. Random projection, margins, kernels, and feature-selection. Subspace, Latent
Structure and Feature Selection, pages 52–68, 2006.
[7] Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal
sparse dimensionality reduction and learning in the measurement domain. Technical Report,
2009.
[8] Emmanuel Candes and Terence Tao. The Dantzig selector: Statistical estimation when p is
much larger than n. Annals of Statistics, 35:2313, 2007.
[9] Emmanuel J. Candes and Justin K. Romberg. Signal recovery from random projections. vol-
ume 5674, pages 76–86. SPIE, 2005.
[10] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM
Journal on Scientiﬁc Computing, 20:33–61, 1998.
[11] Mark A. Davenport, Michael B. Wakin, and Richard G. Baraniuk. Detection and estimation
with compressive measurements. Technical Report TREE 0610, Department of Electrical and
Computer Engineering, Rice University, 2006.
[12] E. Greenshtein and Y. Ritov. Persistency in high dimensional linear predictor-selection and the
virtue of over-parametrization. Bernoulli, 10:971–988, 2004.
[13] L. Gy ¨orﬁ, M. Kohler, A. Krzy ˙zak, and H. Walk. A distribution-free theory of nonparametric
regression. Springer-Verlag, 2002.
[14] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear predic-
tion: Risk bounds, margin bounds, and regularization. In Daphne Koller, Dale Schuurmans,
Yoshua Bengio, and Leon Bottou, editors, Neural Information Processing Systems, pages 793–
800. MIT Press, 2008.
[15] Yuval Nardi and Alessandro Rinaldo. On the asymptotic properties of the group Lasso estima-
tor for linear models. Electron. J. Statist., 2:605–633, 2008.
[16] D. Pollard. Convergence of Stochastic Processes. Springer Verlag, New York, 1984.
[17] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Neural
Information Processing Systems, 2007.
[18] Saharon Rosset and Ji Zhu. Piecewise linear regularized solution paths. Annals of Statistics,
35:1012, 2007.
[19] Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal
Statistical Society, Series B, 58:267–288, 1994.
[20] A. N. Tikhonov. Solution of incorrectly formulated problems and the regularization method.
Soviet Math Dokl 4, pages 1035–1038, 1963.
[21] Yaakov Tsaig and David L. Donoho. Compressed sensing.
52:1289–1306, 2006.
[22] Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA, 1995.
[23] Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal
of Machine Learning Research, 2:527–550, 2002.
[24] Tong Zhang. Some sharp performance bounds for least squares regression with L1 regulariza-
tion. To appear in Annals of Statistics, 2009.
[25] Shuheng Zhou, John D. Lafferty, and Larry A. Wasserman. Compressed regression. In John C.
Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, Neural Information Process-
ing Systems. MIT Press, 2007.

IEEE Trans. Inform. Theory,

9


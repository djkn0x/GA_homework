Robust Nonparametric Regression with Metric-Space
valued Output

Matthias Hein
Department of Computer Science, Saarland University
Campus E1 1, 66123 Saarbr ¨ucken, Germany
hein@cs.uni-sb.de

Abstract

Motivated by recent developments in manifold-valued regression we propose a
family of nonparametric kernel-smoothing estimators with metric-space valued
output including several robust versions. Depending on the choice of the output
space and the metric the estimator reduces to partially well-known procedures for
multi-class classiﬁcation, multivariate regression in Euclidean space, regression
with manifold-valued output and even some cases of structured output learning.
In this paper we focus on the case of regression with manifold-valued input and
output. We show pointwise and Bayes consistency for all estimators in the family
for the case of manifold-valued output and illustrate the robustness properties of
the estimators with experiments.

1

Introduction

In recent years there has been an increasing interest in learning with output which differs from
the case of standard classiﬁcation and regression. The need for such approaches arises in several
applications which possess more structure than the standard scenarios can model.
In structured
output learning, see [1, 2, 3] and references therein, one generalizes multiclass classiﬁcation to
more general discrete output spaces, in particular incooperating structure of the joint input and
output space. These methods have been successfully applied in areas like computational biology,
natural language processing and information retrieval. On the other hand there has been a recent
series of work which generalizes regression with multivariate output to the case where the output
space is a Riemannian manifold, see [4, 5, 6, 7], with applications in signal processing, computer
vision, computer graphics and robotics. One can also see this branch as structured output learning
if one thinks of a Riemannian manifold as isometrically embedded in a Euclidean space. Then the
restriction that the output has to lie on the manifold can be interpreted as constrained regression in
Euclidean space, where the constraints couple several output features together.
In this paper we propose a family of kernel estimators for regression with metric-space valued input
and output motivated by estimators proposed in [6, 8] for manifold-valued regression. We discuss
loss functions and the corresponding Bayesian decision theory for this general regression problem.
Moreover, we show that this family of estimators has several well known estimators as special
cases for certain choices of the output space and its metric. However, our main emphasis lies on
the problem of regression with manifold-valued input and output which includes the multivariate
Euclidean case. In particular, we show for all our proposed estimators their pointwise and Bayes
consistency, that is in the limit as the sample size goes to inﬁnity the estimated mapping converges
to the Bayes optimal mapping. This includes estimators implementing several robust loss functions
like the L1 -loss, Huber loss or the ε-insensitive loss. This generality is possible since our proof
considers directly the functional which is minimized instead of its minimizer as it is usually done in
consistency proofs of the Nadaraya-Watson estimator. Finally, we conclude with a toy experiment
illustrating the robustness properties and difference of the estimators.

1

2 Bayesian decision theory and loss functions for metric-space valued output
We consider the structured output learning problem where the task is to learn a mapping φ : M → N
between two metric spaces M and N , where dM denotes the metric of M and dN the metric of N .
We assume that both metric spaces M and N are separable1 . In general, we are in a statistical
setting where the given input/output pairs (Xi , Yi ) are i.i.d. samples from a probability measure P
on M × N .
In order to prove later on consistency of our metric-space valued estimator we ﬁrst have to deﬁne the
Bayes optimal mapping φ∗ : M → N in the case where M and N are general metric spaces which
depends on the employed loss function. In multivariate regression the most common loss function
is, L(y , f (x)) = (cid:107)y − f (x)(cid:107)2
2 . However, it is well known that this loss is sensitive to outliers. In
univariate regression one therefore uses the L1 -loss or other robust loss functions like the Huber or ε-
insensitive loss. For the L1 -loss the Bayes optimal function f ∗ is given as f ∗ (x) = Med[Y |X = x],
where Med denotes the median of P(Y |X = x) which is a robust location measure. Several general-
izations of the median for multivariate output have been proposed, see e.g. [9]. In this paper we refer
to the minimizer of the loss function L(y , f (x)) = (cid:107)y − f (x)(cid:107)Rn resp. L(y , f (x)) = dN (y , f (x))
as the (generalized) median, since this seems to be the only generalization of the univariate me-
L(y , φ(x)) = Γ(cid:0)dN (y , φ(x))(cid:1),
dian which has a straightforward extension to metric spaces. In analogy to Euclidean case, we will
therefore use loss functions penalizing the distance between predicted output and desired output:
y ∈ N , x ∈ M ,
where Γ : R+ → R+ . We will later on restrict Γ to a certain family of functions. The associated
Γ : M → N
risk (or expected loss) is: RΓ (φ) = E[L(Y , φ(X ))] and its Bayes optimal mapping φ∗
E[Γ(cid:0)dN (Y , φ(X ))(cid:1)]
can then be determined by
EX [EY |X [Γ(cid:0)dN (Y , φ(X ))(cid:1) | X ].
φ∗
arg min
RΓ (φ) =
Γ :=
arg min
φ:M→N , φ measurable
φ:M→N , φ measurable
arg min
=
(1)
φ:M→N , φ measurable
In the second step we used a result of [10] which states that a joint probability measure on the product
φ : M → N so that E[Γ(cid:0)dN (Y , φ(X ))(cid:1)] < ∞. This holds always once N has bounded diameter.
of two separable metric spaces can always be factorized into a conditional probability measure and
the marginal. In order that the risk is well-deﬁned, we assume that there exists a measurable mapping
Γ (x, φ(x)) = EY |X [Γ(cid:0)dN (Y , φ(X ))(cid:1) | X = x],
Apart from the global risk RΓ (φ) we analyze for each x ∈ M the pointwise risk R(cid:48)
Γ (x, φ(x)),
R(cid:48)
which measures the loss suffered by predicting φ(x) for the input x ∈ M . The total loss RΓ (φ) of
the mapping φ is then RΓ (φ) = E[R(cid:48)
(cid:90)
Γ (X, φ(X ))]. As in standard regression the factorization allows
Γ(cid:0)dN (y , p)(cid:1) dµx (y),
E[Γ(cid:0)dN (Y , p)(cid:1) | X = x] = arg min
to ﬁnd the Bayes optimal mapping φ∗ pointwise,
R(cid:48)
φ∗
Γ (x, p) = arg min
Γ (x) = arg min
p∈N
p∈N
p∈N
N
where dµx is the conditional probability of Y conditioned on X = x. Later on we prove consistency
for a set of kernel estimators each using a different loss function Γ from the following class of
functions.
Deﬁnition 1 A convex function Γ : R+ → R+ is said to be (α, s)-bounded if
• Γ : R+ → R+ is continuously differentiable, monotonically increasing and Γ(0) = 0,
• Γ(2x) ≤ α Γ(x) for x ≥ s and Γ(s) > 0 and Γ(cid:48) (s) > 0.
Several functions Γ corresponding to standard loss functions in regression are (α, s)-bounded:
• Lp -type loss: Γ(x) = xγ for γ ≥ 1 is (2γ , 1)-bounded,
• Huber-loss: Γ(x) = 2x2
for x ≤ ε
2 and Γ(x) = 2x − ε
2 is (3, ε
2 )-bounded.
2 for x > ε
ε
1A metric space is separable if it contains a countable dense subset.

2

• ε-insensitive loss: Γ(x) = 0 for x ≤ ε and Γ(x) = x − ε if x > ε is (3, 2ε)-bounded.
Γ (x, ·) cannot be guaranteed
While uniqueness of the minimizer of the pointwise loss functional R(cid:48)
Γ (x, ·) has
anymore in the case of metric space valued output, the following lemma shows that R(cid:48)
reasonable properties (all longer proofs can be found in Section 7 or in the supplementary material).
It generalizes a result provided in [11] for Γ(x) = x2 to all (α, s)-bounded losses.
Lemma 1 Let N be a complete and separable metric space such that d(x, y) < ∞ for all x, y ∈ N
Γ (x, q) < ∞ for some
and every closed and bounded set is compact. If Γ is (α, s)-bounded and R(cid:48)
q ∈ N , then
• R(cid:48)
Γ (x, p) < ∞ for all p ∈ N ,
• R(cid:48)
Γ (x, ·) is continuous on N ,
• The set of minimizers Q∗ = arg min
q∈N

R(cid:48)
Γ (x, q) exists and is compact.
(cid:90)
It is interesting to have a look at one special loss, the case Γ(x) = x2 . The minimizer of the
pointwise risk,
N (y , p) dµx (y),
F (p) = arg min
d2
p∈N
N
is called the Frech ´et mean2 or Karcher mean in the case where N is a manifold. It is the generaliza-
tion of a mean in Euclidean space to a general metric space. Unfortunately, it needs to be no longer
unique as in the Euclidean case. A simple example is the sphere as the output space together with
a uniform probability measure on it. In this case every point p on the sphere attains the same value
F (p) and thus the global minimum is non-unique. We refer to [12, 13, 11] for more information
under which conditions one can prove uniqueness of the global minimizer if N is a Riemannian
manifold. The generalization of the median to Riemannian manifolds, that is Γ(x) = x, is discussed
in [9, 4, 8]. For a discussion of the computation of the median in general metric spaces see [14].

3 A family of kernel estimators with metric-space valued input and output

(2)

In the following we provide the deﬁnition of the kernel estimator with metric-space valued out-
put motivated by the two estimators proposed in [6, 8] for manifold-valued output. We use in the
following the notation kh (x) = 1
hm k(x/h).
i=1 be the sample with Xi ∈ M and Yi ∈ N . The metric-space-valued
Deﬁnition 2 Let (Xi , Yi )l
l(cid:88)
Γ(cid:0)dN (q , Yi )(cid:1) kh
(cid:0)dM (x, Xi )(cid:1),
kernel estimator φl : M → N from metric space M to metric space N is deﬁned for all x ∈ M as
1
φl (x) = arg min
q∈N
l
i=1
where Γ : R+ → R+ is (α, s)-bounded and k : R+ → R+ .
(cid:0)dM (x, Xi )(cid:1) is to measure the similarity between x and Xi in M which should decrease as the
If the data contains a large fraction of outliers one should use a robust loss function Γ, see Sec-
tion 6. Usually the kernel function should be monotonically decreasing since the interpretation of
kh
distance increases. The computational complexity to determine φl (x) is quite high as for each test
point one has to solve an optimization problem but comparable to structured output learning (see
discussion below) where one maximizes for each test point the score function over the output space.
For manifold-valued output we will describe in the next section a simple gradient-descent type opti-
mization scheme in order to determine φl (x).
It is interesting to see that several well-known nonparametric estimators for classiﬁcation and re-
gression can be seen as special cases of this estimator (or a slightly more general form) for different
choices of the output space, its metric and the loss function. In particular, the approach shows a cer-
tain analogy of a generalization of regression into a continuous space (manifold-valued regression)
and regression into a discrete space (structured output learning).
2 In some cases the set of all local minimizers is denoted as the Frech ´et mean set and the Frech ´et mean is
called unique if there exists only one global minimizer.

3

φl (x) = arg max
q∈N

Multiclass classiﬁcation: Let N = {1, . . . , K } where K denotes the number of classes K . If
there is no special class-structure, then we use the discrete metric on N , dN (q , q (cid:48) ) = 1 if q (cid:54)= q (cid:48) and
0 else leads for any Γ to the standard multiclass classiﬁcation scheme using a majority vote. Cost-
sensitive multiclass classiﬁcation can be done by using dN (q , q (cid:48) ) to model the cost of misclassifying
class q by class q (cid:48) . Since general costs can generally not be modeled by a metric, it should be noted
that the estimator can be modiﬁed using a similarity function, s : N × N → R,
l(cid:88)
s(cid:0)q , Yi
(cid:1) kh
(cid:0)dM (x, Xi )(cid:1),
1
l
i=1
The consistency result below can be generalized to this case given that N has ﬁnite cardinality.
l(cid:88)
Multivariate regression: Let N = Rn and M be a metric space. Then for Γ(x) = x2 , one gets
(cid:0)dM (x, Xi )(cid:1),
1
(cid:107)q − Yi (cid:107)2 kh
(cid:0)dM (x,Xi )(cid:1)Yi
φl (x) = arg min
q∈N
l
(cid:80)l
(cid:0)dM (x,Xi )(cid:1) . This is the well-known Nadaraya-Watson
i=1
(cid:80)l
i=1 kh
which has the solution, φl (x) =
1
i=1 kh
l
estimator, see [15, 16], on a metric space. In [17] a related estimator is discussed when M is a closed
Riemannian manifold and [18] discusses the Nadaraya-Watson estimator when M is a metric space.

(3)

1
l

Manifold-valued regression:
In [6] the estimator φl (x) has been proposed for the case where N is
a Riemannian manifold and Γ(x) = x2 , in particular with the emphasis on N being the manifold of
shapes. The discussion of a robust median-type estimator, that is Γ(x) = x, has been done recently
in [8]. While it has been shown in [7] that an approach using a global smoothness regularizer
outperforms the estimator φl (x), it is a well working baseline with a simple implementation, see
Section 4.
lated using kernels k(cid:0)(x1 , q1 ), (x2 , q2 )(cid:1) on the product M × N of input and output space, which are
Structured output: Structured output learning, see [1, 2, 3] and references therein, can be formu-
supposed to measure jointly the similarity and thus can capture non-trivial dependencies between
input and output. Using such kernels [1, 2, 3] learn a score function s : M × N → R, with
Ψ(x) = arg max
s(x, q).
q∈N
being the ﬁnal prediction for x ∈ M . The similarity to our estimator φl (x) in (2) becomes more
l(cid:88)
αi k(cid:0)(x, q), (Xi , Yi )(cid:1),
obvious when we use that in the framework of [1] the learned score function can be written as
1
Ψl (x) = arg max
q∈N
l
i=1
where α ∈ Rl is the learned coefﬁcient vector. Apart from the coefﬁcient vector α this has almost
the form of the previously discussed estimator in Equation (3), using a joint similarity function on
input and output space. Clearly, a structured output method where the coefﬁcients α have been
learning. Moreover, if the joint kernel factorizes, k(cid:0)(x1 , q1 ), (x2 , q2 )(cid:1) = kM (x1 , x2 ) kN (q1 , q2 ) on
optimized, should perform better than αi = const. In cases where training time is prohibitive the
estimator without α is an alternative, at least it provides a useful baseline for structured output
l(cid:88)
M and N , and kN (q , q) = const., then one can rewrite the problem in (4) as,
1
l
i=1
where dN is the induced (semi)-metric3 of kN . Apart from the learned coefﬁcients this is basically
equivalent to φl (x) in (2) for Γ(x) = x2 .
In the following we restrict ourselves to the case where M and N are Riemannian manifolds. In this
case the optimization to obtain φl (x) can still be done very efﬁciently as the next section shows.
N (p, q) = kN (p, p) + kN (q , q) − 2kN (p, q).
3The kernel kN induces a (semi)-metric dN on N via: d2

Ψl (x) = arg min
q∈N

αi kM (x, Xi )d2
N (q , Yi ),

(4)

4

4
Implementation of the kernel estimator for manifold-valued output
For ﬁxed x ∈ M , the functional F (q) for q ∈ N which is optimized in the kernel estimator φl (x)
l(cid:88)
wi Γ(cid:0)dN (q , Yi )(cid:1).
can be rewritten with wi = kh (dM (x, Xi )) as,
The covariant gradient of F (q) is given as, ∇F (cid:12)(cid:12)q = (cid:80)l
i=1 wiΓ(cid:48) (cid:0)dN (p, Yi )(cid:1) vi , where vi ∈ TqN is
F (q) =
i=1
a tangent vector at q with (cid:107)vi (cid:107)Tq N = 1 given by the tangent vector at q of the minimizing4 geodesic
from Yi to q (pointing “away” from Yi ). Denoting by expq : TqN → N the exponential map at q ,
the simple gradient descent based optimization scheme can be written as
• choose a random point q0 from N ,
• while stopping criteria not fulﬁlled,
(cid:0) − α∇F |qk
(cid:1)
1. compute gradient ∇F at qk
2. one has: qk+1 = expqk
3. determine stepsize α by Armijo rule [19].
As stopping criterion we use either the norm of the gradient or a threshold on the change of F . For
the experiments in Section 6 we get convergence in 5 to 40 steps.

5 Consistency of the kernel estimator for manifold-valued input and output

In this section we show the pointwise and Bayes consistency of the kernel estimator φl in the case
where M and N are Riemannian manifolds. This case already subsumes several of the interesting
applications discussed in [6, 8]. The proof of consistency of the general metric-space valued kernel
estimator (for a restricted class of metric spaces including all Riemannian manifolds) requires high
technical overload which is interesting in itself but which would make the paper hard accessible.
The consistency of φl will be proven under the following assumptions:

Assumptions (A1):
1. The loss Γ : R+ → R+ is (α, s)-bounded.
i=1 is an i.i.d. sample of P on M × N ,
2. (Xi , Yi )l
3. M and N are compact m-and n-dimensional manifolds,
4. The data-generating measure P on M × N is absolutely continuous with respect to the
natural volume element,
5. The marginal density on M fulﬁlls: p(x) ≥ pmin , ∀ x ∈ M ,
7. The kernel fulﬁlls: a 1s≤r1 ≤ k(s) ≤ b e−γ s2 and (cid:82)
6. The density p(·, y) is continuous on M for all y ∈ N ,
Rm (cid:107)x(cid:107) k((cid:107)x(cid:107)) dx < ∞,
√
Note, that existence of a density is not necessary for consistency. However, in order to keep the
det g dx denotes the
proofs simple, we restrict ourselves to this setting. In the following dV =
natural volume element of a Riemannian manifold with metric g , vol(S ) and diam(N ) are the
volume and diameter of the set S . For the proof of our main theorem we need the following two
propositions. The ﬁrst one summarizes two results from [20].
S1 rm ≤ vol (cid:0)B (x, r)(cid:1) ≤ S2 rm .
Proposition 1 Let M be a compact m-dimensional Riemannian manifold. Then, there exists r0 > 0
and S1 , S2 > 0 such that for all x ∈ M the volume of the balls B (x, r) with radius r ≤ r0 satisﬁes,
(cid:17)m
(cid:16) 2
Moreover, the cardinality K of a δ -covering of M is upper bounded as, K ≤ vol(N )
S1
δ
4The set of points where there the minimizing geodesic is not unique, the so called cut locus, has measure
zero and therefore plays no role in the optimization.

.

5

Moreover, we need a result about convolutions on manifolds.
(cid:90)
Proposition 2 Let the assumptions A1 hold, then if f is continuous we get for any x ∈ M \∂M ,
(cid:82)
lim
h→0
M
(cid:90)
M kh (dM (x, z )) dV (z ) > 0.
where Cx = limh→0
If moreover f is Lipschitz continuous with
Lipschitz constant L, then there exists a h0 > 0 such that for all h < h0 (x),
M

kh (dM (x, z ))f (z ) dV (z ) = Cx f (x) + O(h).

kh (dM (x, z ))f (z ) dV (z ) = Cx f (x),

The following main theorem proves the almost sure pointwise convergence of the manifold-valued
kernel estimator for all (α, s)-bounded loss functions Γ.

almost surely.

,

Theorem 1 Suppose the assumptions in A1 hold. Let φl (x) be the estimate of the kernel estimator
for sample size l. If h → 0 and lhm/ log l → ∞, then for any x ∈ M \∂M ,
l→∞ |R(cid:48)
Γ (x, φl (x)) − arg min
Γ (x, q)| = 0,
R(cid:48)
lim
almost surely.
q∈N
Γ (x, q)| = O(h) + O(cid:0)(cid:112)log l/(l hm )(cid:1),
If additionally p(·, y) is Lipschitz-continuous for any y ∈ N , then
Γ (x, φl (x)) − arg min
l→∞ |R(cid:48)
R(cid:48)
The optimal rate is given by h = O(cid:0)(log l/l) 1
2+m (cid:1) so that
lim
q∈N
(cid:16)(cid:0) log l/l(cid:1) 1
2+m (cid:17)
Γ (x, φl (x)) − arg min
R(cid:48)
l→∞ R(cid:48)
lim
Γ (x, q) = O
q∈N
Note, that the condition l hm / log l → ∞ for convergence is the same as for the Nadaraya-Watson
estimator on a m-dimensional Euclidean space. This had to be expected as this condition still holds
if one considers multivariate output, see [15, 16]. Thus, doing regression with manifold-valued
output is not more “difﬁcult” than standard regression with multivariate output.
Next, we show Bayes consistency of the manifold-valued kernel estimator.
Theorem 2 Let the assumptions A1 hold. If h → 0 and lhm / log l → ∞, then
l→∞ RΓ (φl ) − RΓ (φ∗ ) = 0,
lim
almost surely.
Γ (X, φ∗ (X ))|]. Moreover,
Γ (X, φl (X )) − R(cid:48)
Proof: We have, RΓ (φl ) − RΓ (φ∗ ) ≤ E[|R(cid:48)
Γ (x, φ∗ (x)) almost surely.
Γ (x, φl (x)) = R(cid:48)
liml→∞ R(cid:48)
Since
we have almost everywhere,
Γ (X, φ∗ (X ))] < ∞, an extension of the dominated convergence
Γ (X, φ(X ))] < ∞ and E[R(cid:48)
E[R(cid:48)
(cid:3)
theorem proven by Glick, see [21], provides the result.

almost surely.

6 Experiments
k(cid:0)|x − y |/h(cid:1) = 1 − |x − y |/h. The parameter h was found by 5-fold cross validation from the set
We illustrate the differences of median and mean type estimator on a synthetic dataset with the task
of estimating a curve on the sphere, that is M = [0, 1] and N = S 1 . The kernel used had the form,
[5, 10, 20, 40] ∗ 10−3 . The results are summarized for different levels of outliers and different levels
of van-Mises noise (note that the parameter k is inverse to the variance of the distribution) in Table
1. As expected the the L1 -loss and the Huber loss as robust loss functions outperform the L2 -loss
in the presence of outliers, whereas the L2 -loss outperforms the robust versions when no outliers
are present. Note, that the Huber loss as a hybrid version between L1 - and L2 -loss is even slightly
better than the L1 -loss in the presence of outliers as well as in the outlier free case. Thus for a given
dataset it makes sense not only to do cross-validation of the parameter h of the kernel function but
also over different loss functions in order to adapt to possible outliers in the data.

6

Figure 1: Regression problem on the sphere with 1000 training points (black points). The blue
points are the ground truth disturbed by van Mises noise with parameter k = 100 and 20% (outliers)
with k = 3. The estimated curves are shown in green. Left: Result of L1 -loss, mean error (ME)
0.256, mean squared error (MSE) 0.165. Middle: Result of L2 -loss: ME = 0.265, MSE = 0.169.
Right: Result of Huber loss with ε = 0.1: ME = 0.255, MSE = 0.165. In particular, the curves
found using L1 and Huber loss are very close to the ground truth.

Table 1: Mean squared error (unit 10−1 ) for regression on the sphere - for different noise levels k ,
number of labeled points, without and with outliers. Results are averaged over 10 runs.
20% outliers
no outliers
Number of samples
1000
500
100
1000
500
100
1.521 ± 0.015
0.260 ± 0.027
0.219 ± 0.003
2.1 ± 0.2 1.57 ± 0.05
0.63 ± 0.11
L1 -Loss
k = 100
1.400 ± 0.008
0.030 ± 0.001
0.043 ± 0.005
2.1 ± 0.5
1.45 ± 0.03
k = 1000 0.43 ± 0.12
Γ(x) = x
1.549 ± 0.021
k = 100 0.43 ± 0.10 0.230 ± 0.007 0.208 ± 0.001 2.0 ± 0.2 1.59 ± 0.02
L2 -Loss
k = 1000 0.28 ± 0.16 0.032 ± 0.003 0.025 ± 0.001 2.0 ± 0.4 1.51 ± 0.03
1.447 ± 0.015
Γ(x) = x2
2.1 ± 0.2 1.57 ± 0.05 1.520 ± 0.021
0.61 ± 0.11
0.257 ± 0.026
0.218 ± 0.003
Huber-Loss
k = 100
with ε = 0.1 k = 1000 0.42 ± 0.12
0.040 ± 0.005
0.028 ± 0.001
2.1 ± 0.5 1.44 ± 0.02 1.397 ± 0.008

7 Proofs
Lemma 2 Let φ : R+ → R be convex, differentiable and monotonically increasing. Then
min{φ(cid:48) (x), φ(cid:48) (y)}|y − x| ≤ |φ(y) − φ(x)| ≤ max{φ(cid:48) (x), φ(cid:48) (y)}|y − x|.
(cid:80)l
Proof of Theorem 1 We deﬁne R(cid:48)
1
i=1 Γ(dN (q ,Yi )) kh (dM (x,Xi ))
. Note that φl (x) =
Γ,l (x, q) =
l
E[kh (dM (x,X ))]
R(cid:48)
arg min
Γ,l (x, q) as we have only divided by a constant factor. We use the standard technique for
q∈N
the pointwise estimate,
Γ (x, φl (x)) − R(cid:48)
Γ (x, q) ≤ R(cid:48)
Γ,l (x, φl (x)) − min
Γ (x, φl (x)) − min
R(cid:48)
Γ,l (x, φl (x)) + R(cid:48)
R(cid:48)
q∈N
q∈N
≤ 2 sup
|R(cid:48)
Γ,l (x, q) − R(cid:48)
Γ (x, q)|.
q∈N
(cid:12)(cid:12) 1
E[kh (dM (x,X ))] − 1(cid:12)(cid:12) < 1
(cid:80)l
to bound the supremum, we will work on the event E , where we assume,
In order
2 , which holds with probability 1 − 2 e−C l hm for some constant C .
(cid:16) 2
(cid:17)n
i=1 kh (dM (x,Xi ))
l
Moreover, we assume to have a δ -covering of N with centers Nδ = {qα }K
α=1 where using Lemma
1 we have K ≤ vol(N )
. Thus for each q ∈ N there exists qα ∈ Nδ such that dN (q , qα ) ≤ δ .
S1
δ
Γ (x, q) = E[Γ(dN (q ,Y ))kh (dM (x,X ))]
Introducing RE
and using the decomposition,
E[kh (dM (x,X ))]
Γ,l (x, q) − R(cid:48)
Γ,l (x, q) − R(cid:48)
Γ,l (x, qα ) − RE
R(cid:48)
Γ (x, q) =R(cid:48)
Γ,l (x, qα ) + R(cid:48)
Γ (x, qα )
Γ (x, q) − R(cid:48)
Γ (x, qα ) − RE
Γ (x, q) + RE
+ RE
Γ (x, q),
(cid:0)Γ(cid:0)dN (q , Yi )(cid:1) − Γ(cid:0)dN (qα , Yi )(cid:1)(cid:1)kh (dM (x, Xi ))
(cid:80)l
(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)R(cid:48)
Γ,l (x, qα )(cid:12)(cid:12) =
we have to control four terms,
(cid:80)l
Γ,l (x, q)−R(cid:48)
i=1
l
≤ 3 Γ(cid:48) (cid:0) diam(N )(cid:1) δ.
≤ 2 dN (q , qα ) Γ(cid:48) (cid:0) diam(N )(cid:1) 1
E[kh (dM (x, X ))]
i=1 kh (dM (x, Xi ))
l
E[kh (dM (x, X ))]

R(cid:48)
Γ (x, q)

7

,

lim
h→0

gh =

=

max
1≤α≤K

kh (dM (x, z ))p(z )dV (z ) = Cxp(x),

(cid:16) 2
(cid:16)
(cid:17) ≤ 2
(cid:17)n
where we have used Lemma 2 and the fact that E holds. Then, there exists a constant C such that
vol(N )
|R(cid:48)
Γ,l (x, qα ) − RE
Γ (x, qα )| > ε
e−C l hm ε2
(cid:80)l
P
,
S1
δ
i=1 Wi − E[Wi ] where Wi =
which can be shown using Bernstein’s inequality for 1
l
together with a union bound over the elements in the covering Nδ using
Γ(dN (qα ,Yi ))kh (dM (x,Xi ))
E[kh (dM (x,X ))]
, Var Wi ≤ Γ(diam(N ))2E[k2
h (dM (x, X ))]
Γ(diam(N ))2
Γ(diam(N ))
≤ b
|Wi | ≤ b
(E[kh (dM (x, X ))])2
hmS1 rm
hmS1 rm
1 pmin
1 pmin
a
a
where we used Proposition 1 to lower bound vol(B (x, h r1 )) for small enough h. Third, we get for
the third term using again Lemma 2,
Γ (x, q)| ≤ 2Γ(cid:48) (diam(N ))dN (q , qα ) ≤ 2Γ(cid:48) (diam(N ))δ.
Γ (x, qα ) − RE
|RE
Γ (x, q) − R(cid:48)
Γ (x, q), Under the continuity assump-
Last, we have to bound the approximation error RE
(cid:90)
(cid:90)
tion on the joint density p(x, y) we can use Proposition 2. For every x ∈ M \∂M we get,
kh (dM (x, z ))p(z , y)dV (z ) = Cx p(x, y),
lim
h→0
(cid:90)
(cid:90)
M
M
where Cx > 0. Thus with
kh (dM (x, z ))p(z , y)dV (z ),
fh =
M
M
(cid:12)(cid:12)(cid:12) ≤ lim
(cid:12)(cid:12)(cid:12) fh
we get for every x ∈ M \∂M ,
|gh − g |
|fh − f |
− f
lim
+ lim
= 0,
f
h→0
h→0
h→0
g gh
gh
g
gh
where we have used gh ≥ aS1 r1pmin > 0 and g = Cxp(x) > 0. Moreover, using results from
the proof of Proposition 2 one can show fh < C for some constant C . Thus fh/gh < C for some
(cid:90)
constant and fh/gh → f /g as h → 0. Using the dominated convergence theorem we thus get
Γ(cid:0)dN (q , y)(cid:1) p(x, y)
E[Γ(dN (q , Y ))kh (dM (x, X ))]
p(x) dy = R(cid:48)
Γ (x, q).
E[kh (dM (x, X ))]
N
Γ (x, q) =
For the case where the joint density is Lipschitz continuous one gets using Proposition 2, RE
R(cid:48)
Γ (x, q) + O(h).
In total, there exist constants A, B , C, D1 , D2 , such that for sufﬁciently small h one has with prob-
ability 1 − AeB n log( 1
δ )−C lhm ε2 ,
Γ (x, q)| ≤ 2D1 δ + ε.
Γ,l (x, q) − RE
|R(cid:48)
sup
q∈N
log l → ∞ together with
With δ = l−s for some s > 0 one gets convergence if
lhm
For the case where p(·, y) is Lipschitz continuous for all
Γ (x, q) = R(cid:48)
Γ (x, q).
limh→0 RE
y ∈ N we have RE
Γ (x, q) = R(cid:48)
Γ (x, q) + O(h) and can choose s large enough so that the bound
log l → ∞ the
from the approximation error dominates the one of the covering. Under the condition lhm
probabilistic bound is summable in l which yields almost sure convergence by the Borel-Cantelli-
Lemma. The optimal rate in the Lipschitz continuous case is then determined by ﬁxing h such that
(cid:3)
both terms of the bound are of the same order.

kh (dM (x, z ))p(z )dV (z ),

lim
h→0

Γ (x, q) = lim
RE
h→0

Acknowledgments

We thank Florian Steinke for helpful discussions about relations between generalized kernel esti-
mators and structured output learning. This work has been partially supported by the Cluster of
Excellence MMCI at Saarland University.

8

References
[1] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured
and interdependent output variables. JMLR, 6:1453–1484, 2005.
[2] J. Weston, G. BakIr, O. Bousquet, B. Sch ¨olkopf, T. Mann, and W. S. Noble. Joint kernel maps.
In Predicting Structured Data, pages 67–84. MIT Press, 2007.
[3] E. Ricci, T. De Bie, and N. Cristianini. Magic moments for structured output prediction. JMLR,
9:2803–2846, 2008.
[4] K.V. Mardia and P.E. Jupp. Directional statistics. Wiley New York, 2000.
[5] Inam Ur Rahman, Iddo Drori, Victoria C. Stodden, David L. Donoho, and Peter Schroder.
Multiscale representations for manifold-valued data. Multiscale Modeling and Simulation,
4(4):1201–1232, 2005.
[6] B. C. Davis, P. T. Fletcher, E. Bullitt, and S. Joshi. Population shape regression from random
design data. Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–7, 2007.
[7] F. Steinke and M. Hein. Non-parametric regression between Riemannian manifolds. In Ad-
vances in Neural Information Processing Systems (NIPS) 21, pages 1561 – 1568, 2009.
[8] P. T. Fletcher, S. Venkatasubramanian, and S. Joshi. The geometric median on Riemannian
manifolds with application to robust atlas estimation. NeuroImage, 45:143 – 152, 2009.
[9] C. G. Small. A survey of multidimensional medians. International Statistical Review, 58:263–
277, 1990.
[10] D. Blackwell and M. Maitra. Factorization of probability measures and absolutely measurable
sets. Proc. Amer. Math. Soc., 92(2):251–254, 1984.
[11] R. Bhattacharya and V. Patrangenaru. Large sample theory of intrinsic and extrinsic sample
means on manifolds I. Ann. Stat., 31(1):1–29, 2003.
[12] H. Karcher. Riemannian center of mass and molliﬁer smoothing. Communications on Pure
and Applied Mathematics, 30:509–541, 1977.
[13] W. Kendall. Probability, convexity, and harmonic maps with small image. I. Uniqueness and
ﬁne existence. Proc. London Math. Soc., 61(2):371–406, 1990.
[14] P. Indyk. Sublinear time algorithms for metric space problems. In Proceedings of the 31st
Symposium on Theory of computing (STOC), pages 428 – 434, 1999.
[15] L. Gy ¨orﬁ, M. Kohler, A. Krzy ˙zak, and H. Walk. A Distribution-Free Theory of Nonparametric
Regression. Springer, New York, 2004.
[16] W. Greblicki and M. Pawlak. Nonparametric System Identiﬁcation. Cambridge University
Press, Cambrige, 2008.
[17] B. Pelletier. Nonparametric regression estimation on closed Riemannian manifolds. J. of
Nonparametric Stat., 18:57–67, 2006.
[18] S. Dabo-Niang and N. Rhomari. Estimation non parametrique de la regression avec variable
explicative dans un espace metrique. C. R. Math. Acad. Sci. Paris, 1:75–80, 2003.
[19] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, Mass., 1999.
[20] M. Hein. Uniform convergence of adaptive graph-based regularization.
In G. Lugosi and
H. Simon, editors, Proc. of the 19th Conf. on Learning Theory (COLT), pages 50–64, Berlin,
2006. Springer.
[21] N. Glick. Consistency conditions for probability estimators and integrals of density estimators.
Utilitas Math., 6:61–74, 1974.

9


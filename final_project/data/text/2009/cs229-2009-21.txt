Bubble Clustering: Set Covering via Union
of Ellipses

Matt Kraning, Arezou Keshavarz, Lei Zhao
CS229, Stanford University
Email: {mkraning,arezou,leiz}@stanford.edu

Abstract

We develop an algorithm called bubble clustering which attempts to cover a given set of points
with the union of k ellipses of minimum total volume. This algorithm operates by splitting and
merging ellipses according to an annealing schedule and does not assume any prior distribution on
the data. We compare our algorithm with k-means and the EM algorithm for mixture of Gaussians.
Numerical results suggest that our algorithm achieves superior performance to k-means for prior-
less data and comparable performance to algorithms that have access to prior information for
statistically generated data.

I . PRO JECT GOAL
We want to cover a set of N points X = {x1 , x2 , . . . , xN }, xi ∈ Rd , by the union of k
ellipses (E1 , E2 , . . . , Ek ) such that the total volume of all ellipses is minimized. Unlike mixture of
Gaussians, we do not assume a prior distribution on data, nor do we even assume the data has a
prior distribution. Consequently, the optimization problem we are attempting to solve is

minimize
subject to

Pk
i=1 log det A−1
i
kAIj xj + bIj k2 ≤ 1
Ij ∈ {1, . . . , k},
j = 1 . . . N
Ai ∈ S n
i = 1 . . . k ,
++ ,
which is an NP-Hard problem due to the presence of the constraint Ij ∈ {1, . . . , k}; this constraint
ta point xj to a speciﬁc ellipse EIj .
speciﬁcs that we ﬁnd the optimal assignment of every given da

I I . ALGOR ITHM
Because solving our problem exactly is NP-Hard, we instead focus on developing an approximate
algorithm to solve this problem. Rather than use EM, whose performance can depend heavily on
initial starting conditions that are usually randomly generated, we sought an algorithm that is
both deterministic and as free of tunable parameters as possible. This was done to maximize the
algorithm’s universality: it should not depend on random numbers, nor should it have to be highly
‘tuned’ to work well on speciﬁc datasets.
Bubble clustering works by iteratively splitting minimum volume ellipses that were ﬁt to parts
of the data on earlier iterations. The intuition behind bubble clustering is that good clustering
performance can be obtained by looking at a global summary of part of the dataset (the points
within a given ellipse) and then taking a lower volume covering of that same set by splitting
the ellipse containing that set into two smaller ellipses. After splitting down to k ellipses, bubble
clustering then performs a variant of simulated annealing to escape from weak local minima. The
annealing schedule controls if ellipses are split or merged during each iteration and guarantees that
the number of ellipses converges to the pre-speciﬁed value k . This is described by the pseudocode

below and a more detailed description of how splitting and merging of ellipses is performed is
given in the following sections.
Given data X, number of ﬁnal clusters k , and an annealing schedule, anneal, ﬁt a single ellipse
to the data. Then,
for t = 1 to k do
split ellipse
end for
for i = 1 to length(anneal) do
if anneal(i) == split then
split ellipse
else if anneal(i) == merge then
if one ellipse contained inside another then
eliminate smaller ellipse
else
merge two ellipses
end if
end if
end for

A. Splitting
The density of the ith ellipse, ρi , is the ratio of the volume of the ellipse to the number of data
points inside the ellipse, i.e.

ρi =

vol(Ei )
|{i : 1 ≤ i ≤ κ, xi ∈ Ei}|
In the splitting phase of a given iteration with κ ellipses, bubble clustering greedily chooses the
ellipse with lowest density, EI , where I = argmin
ρi , and splits it into two ellipses EI1 , EI2 . It splits
i∈{1,...,κ}
EI along the hyperplane perpendicular to the direction which gives maximum data variance for data
points within EI . This direction is the eigenvector corrosponding to the maximum eigenvalue for the
empirical covariance matrix of the data points inside EI . After splitting, it ﬁts new minimum-volume
ellipses around both sets of points by solving the convex optimization problem

.

log det A−1
minimize
Ij
subject to kAIj xi + bIj k2 ≤ xi ,

xi ∈ EIj

for j = 1, 2.

Fig. 1.
Splitting along the eigenvector of maximum eigenvalue for the empirical covariance matrix of the data point
inside the chosen ellipse.

B. Merging Ellipses
is completely contained in another
In the merging step, we ﬁrst check if there is any ellipse that
2 ≤ 1} and Ej = {x|kAj x − bj k2
ellipse. Denote the ellipses by Ei = {x|kAi x − bik2
2 ≤ 1}. We want
to check if Ei ⊆ Ej , or equivalently, whether kAix − bik2
2 ≤ 1.
2 ≤ 1 ⇒ kAj x − bj k2
The S-Procedure speciﬁes the necessary and sufﬁcient condi
tions for this to occur: (If ∃τ ≥ 0
such that Q − τ P ≥ 0) ⇐⇒ (xT P x ≥ 0 ⇒ xT Qx ≥ 0) [1]. Applying the S-procedure to our
problem, it is sufﬁcient to check the feasibility of the line ar matrix inequalities (LMIs)
(Dd)T −dT Dd + 1 # − τ " −I 0
1 # ≥ 0,
" −D
Dd
0
i Ai )− T
i Ai )− 1
1
i bi − A−1
2 (A−1
j Aj (AT
j bj ) and D = (AT
2 AT
2 , for all pairs of ellipses

where d = (AT
i Ai )
Ei , Ej , i 6= j .
If we cannot ﬁnd an ellipse contained inside another ellipse , we merge two ellipses based on their
relative orientations and overall closeness. To measure relative orientation, we use the difference
between the Mahalanobis distances of each ellipse to the center of the other ellipse, where the
Mahalanobis distance to a point x from the ellipse Ei with center ci and inverse covariance matrix
= (x−ci )T Ai (x−ci). If these distances are close, then the ellipses are aligned.
Ai is de ﬁned as kxk2
Ei
Consequently, we merge the two ellipses, Ei and Ej , that minimize |kcj kEi − kci kEj | + λkci − cj k2 ,
where λ is a tradeoff parameter, and ci and cj are the centers of Ei and Ej , respectivly. Then, we
merge ellipses Ei and Ej by ﬁtting a minimum volume ellipse around the union of points contained
in either ellipse (see Figure 2).

τ ≥ 0,

Fig. 2. Merging ellipses: two black ellipses are merged into the blue ellipse.

I I I . NUMER ICAL RE SULT S

A. Synthetic data, no prior
We initialized k-means randomly 50 times, and took the result with minimum total ellipsoid
volume, which was 1.28. The average volume over all iterations was 1.59. Our Bubble clustering
algorithm attained a total volume of 1.24. The clusters generated by the K-Means algorithm have
some overlapping clusters, whereas the clusters generated by the bubble clustering algorithm are
separable (Figure 3).

B. Synthetic data, mixture of Gaussian prior
In this experiment, we assume a Gaussian mixture model with 9 classes and a uniform distribution
over those classes.

p(x) =

1
9

9
1
Xi=1
(2π)d/2pdet |Si |
where the dimension d is set to be 2. The parameters Si , µi , i = 1, .., 9 are randomly generated and
ﬁxed for all the data. We use this Gaussian mixture model to ge nerate 300 points, shown in Figure
4.

(x − µi)T S−1
i

(x − µi )},

exp{−

1
2

2.5

2

1.5

1

0.5

0

−0.5

−1
−1

Bubble Clustering, total ellipse volume = 1.24

0

1

2

3

4

5

6

2.5

2

1.5

1

0.5

0

−0.5

−1
−1

K−Means Clustering, total ellipse volume = 1.28

0

1

2

3

4

5

6

Fig. 3. Comparison between bubble clustering (left) and k-means (right)

True distributionKt = 9

25

20

15

10

5

0

−5

−10

−15

−20

−25
−15

−10

−5

0

5

10

15

20

Fig. 4. True distribution of the data, generated by a mixture of Gaussians.

We run Bubble Clustering on this data set and compare it with the result obtained by the EM
algorithm for mixture of Gaussian. As Figure 5 indicates, even without a prior assumption, the
bubble clustering obtains comparable results.

Mixture of Gaussian via EM algorithmK = 9

25

20

15

10

5

0

−5

−10

−15

−20

−25
−15

−10

−5

0

5

10

15

20

30

20

10

0

−10

−20

−30
−15

−10

−5

0

5

10

15

20

25

Fig. 5. Comparison between bubble clustering algorithm (left) and the EM algorithm for mixture of Gaussian (right)

C. Real Data: Blogger Population
We also applied the bubble clustering algorithm to a real dataset. We obtained an XML feed from
[2], which provides a live feed of blogger entries. We extracted the bloggers residing in California,

and ran the bubble clustering algorithm to ﬁnd clusters of bl ogger population (Figure 6).

40

39

38

37

36

35

34

33

Eureka

Sacramento

Berkeley

San Francisco

Stanford

Fresno

Los Angeles

−125

−124

−123

−122

−121

−120

−119

San Diego
−118

−117

−116

Fig. 6. Blogger population clusters in California

The clusters correspond pretty well to metropolitan areas such as Los Angeles and San Francisco,
with smaller ellipses being generally indicative of high population densities over their area.

IV. CONCLU S ION S AND FUTURE WORK
Bubble Clustering is a novel universal clustering method that does not have to rely on the kindness
of random number generators or the smoke and mirrors of tunable parameters for good performance.
It is effective over off the shelf methods for both clustering and ﬁnding small set coverings and
can also show data cluster correlations when a prior is either unknown or non-existent.
The main areas for improvement involve using lookahead in both splitting and merging ellipses.
In picking which ellipse to split, our ultimate aim is to have the sum of the volumes of both
new ellipses be signiﬁcantly smaller than the volume of the o riginal ellipse before splitting. It is
computationally intensive to evaluate the volume reduction gained by splitting each ellipse and
only then pick the best ellipse to split; so we instead rely on the heuristic of density as a proxy
metric for splitting efﬁcacy. Similarly, in the case of merg ing, if we check all possible pairs of
ellipses to merge and only then choose to merge the two with least volume increase, the algorithm
will be drastically slowed down. Finding ways to effectively prune these search spaces (as we did
by utilizing S-Procedure to check if one ellipse is inside another) would allow lookahead to be
implemented for both splitting and merging, which would likely lead to better performance.

V. ACKNOWLEDGEMENT S
We would like to thank Eric Chu and Brendan O’Donoghue for their discussions and help on
this problem with us.

[1] Boyd, S. P. (2009). Linear Matrix Inequalities and the S-Procedure, EE363 Notes, Winter 2009.
[2] We feel ﬁne, ”http://wefeel ﬁne.org/ ”.

RE FERENCE S


Boosting with Spatial Regularization

Zhen James Xiang1 Yongxin Taylor Xi1 Uri Hasson2

Peter J. Ramadge1

1: Department of Electrical Engineering, Princeton University, Princeton NJ, USA
2: Department of Psychology, and Neuroscience Institute, Princeton University, Princeton NJ, USA
{zxiang, yxi, hasson, ramadge} @ princeton.edu

Abstract

By adding a spatial regularization kernel to a standard loss function formulation
of the boosting problem, we develop a framework for spatially informed boosting.
From this regularized loss framework we derive an efﬁcient boosting algorithm
that uses additional weights/priors on the base classiﬁers. We prove that the pro-
posed algorithm exhibits a “grouping effect”, which encourages the selection of
all spatially local, discriminative base classiﬁers. The algorithm’s primary advan-
tage is in applications where the trained classiﬁer is used to identify the spatial
pattern of discriminative information, e.g. the voxel selection problem in fMRI.
We demonstrate the algorithm’s performance on various data sets.

1

Introduction

When applying off-the-shelf machine learning algorithms to data with spatial dimensions (images,
geo-spatial data, fMRI, etc) a central question arises: how to incorporate prior information on the
spatial characteristics of the data? For example, if we feed a boosting or SVM algorithm with
individual image voxels as features, the voxel spatial information is ignored. Indeed, if we randomly
shufﬂed the voxels, the algorithm would not notice any difference. Yet in many cases the spatial
arrangement of the voxels together with prior information about expected spatial characteristics of
the data may be very helpful. We are particularly interested in the situation when the trained classiﬁer
is used to identify relevant spatial regions. To make this more concrete, consider the problem of
training a classiﬁer to distinguish two different brain states based on fMRI responses. Successful
classiﬁcation suggests that the voxels used are important in discriminating between the two classes.
Hence we could use a successful classiﬁer to learn a set of discriminative voxels. We expect that
these voxels will be spatially compact and clustered. How can this prior knowledge be incorporated
into the training of the classiﬁer? In summary, our primary objective is improving the ability of
the trained classiﬁer to usefully identify the spatial pattern of discriminative information. However,
incorporating spatial information into boosting may also improve classiﬁcation accuracy.
Our key contribution is the development of a framework for spatially regularized boosting. We
do this by adding a spatial regularization kernel to the standard loss minimization formulation of
boosting. We then design an associated boosting algorithm by using coordinate descent on the
regularized loss. We show that the algorithm minimizes the regularized loss function and has a
natural interpretation of boosting with additional adaptive priors/weights on both spatial locations
and training examples. We also show that it exhibits a natural grouping effect on nearby spatial
locations with similar discriminative power.
We believe our contributions are fundamental and relevant to a variety of applications where base
classiﬁers are attributed with a known auxiliary variable and prior information is known about this
auxiliary variable. However, since our study is motivated by the particular problem of voxel selection
in fMRI analysis, we brieﬂy review the state of the art in this domain so as to put our contribution
into a concrete context.

1

Brieﬂy, the fMRI voxel selection problem is to use the fMRI signal to identify a subset of voxels
that are key in discriminating between two stimuli. One expects such voxels to be spatially compact
and clustered. Traditionally this is done by thresholding a statistical univariate test score on each
voxel [1]. Spatial smoothing prior to this analysis is commonly employed to integrate activity from
neighboring voxels. An extreme case is hypothesis testings on clusters of voxels rather than on
voxels themselves [2]. The problem with these methods is that they greatly sacriﬁce the spatial
resolution of the results and averaging could hide ﬁne patterns in data. An alternative is to spatially
average the univariate test scores, e.g.
thresholding in some transformed domain (e.g. wavelet
domain) [3, 4]. However, this also compromises the spatial accuracy of the result because one
selects discriminating wavelet components, not voxels. A more promising spatially aware approach
selects voxels with tree-based spatial regularization of a univariate statistic [5, 6]. This can achieve
both spatial precision and smoothness but uses a complex regularization method. Our proposed
method also selects single voxels with the help of spatial regularization but operates in a multivariate
classiﬁer framework using a simpler form of regularization.
Recent research has suggested that multivariate analysis has potential advantages over univariate
tests [7, 8], e.g. it brings in machine learning algorithms (such as boosting, SVM, etc.) and there-
fore might capture more intricate activation patterns involving multiple voxels. To ensure spatial
clustering of selected voxels, one can run a searchlight (a spherical mask) [9] to pre-select clustered
informative features. In each searchlight location, a multivariate analysis is performed to see whether
the masked area contains informative data. One can then train a classiﬁer on the pre-selected voxels.
A variant of this two-stage framework is to train classiﬁers on a few predeﬁned masks, and then
aggregate these classiﬁers by boosting [10, 11]. This is faster but assumes detailed prior knowledge
to select the predeﬁned masks. Unlike two-stage approaches, [12] directly uses AdaBoost to train
classiﬁers with “rich features” (features involving the values of several adjacent voxels) to capture
spatial structure in the data. Although exhibiting superior performance, this method selects “rich
features” rather than individual discriminating voxels. Moreover, there is no control on the spatial
smoothness of the results. Our method is similar to [12] in that we combine the feature selection
and classiﬁcation into one boosting process. But our algorithm operates on single voxels and uses
simple spatial regularization to incorporate spatial information.
The remainder of the paper is organized as follows. After introducing notation in §2, we formu-
late our spatial regularization approach in §3 and derive an associated spatially regularized boosting
algorithm in §4. We prove an interesting property of the algorithm in §5 that guarantees the simulta-
neous selection of equivalent locations that are spatially close. In §6, we test the algorithm on face
gender detection, OCR image classiﬁcation, and fMRI experiments.

2 Boosting Preliminaries
In a supervised learning setting, we are given m training instances X = {xi ∈ Rn , i = 1, . . . , m}
a composite binary classiﬁer of the form hα (xi ) = sgn((cid:80)p
and corresponding binary labels Y = {yi = ±1, i = 1, . . . , m}. Using the training instances X , we
select a pool of base classiﬁers H = {hj : Rn → {−1, +1}, j = 1, . . . , p}. Our objective is to train
j=1 αj hj (xi )). We can further assume
that hj ∈ H ⇒ −hj ∈ H, thus all values in α can be assumed to be nonnegative. Boosting is a
technique for constructing from X , Y and H the weight α of a composite classiﬁer to best predict
m(cid:88)
the labels. This can be done by seeking α to minimize a loss function of the form:
L(X , Y , α) =
i=1
Various boosting algorithms can be derived as iterative greedy coordinate descent procedures to
minimize (1) [13]. In particular, AdaBoost [14] is of this form with l(yi , hα (xi )) = e−yi hα (xi ) .
The result of a conventional boosting algorithm is determined by the m × p matrix M = [yihj (xi )]
[15]. Under a component permutation ˆxi = P xi , the base classiﬁers become ˆhj = hj · P −1 ; so
ˆhj ( ˆxi )] = [yihj (xi )] = M . Hence training on {P xi , yi } or {xi , yi } yields the same α,
ˆM = [yi
i.e., the arrangement of the components can be arbitrary as long as it is consistent.
The weights α of a composite classiﬁer not only indicate how to construct the classiﬁer, but also
the relative reliance of the classiﬁer on each of the n instance components. To see this, assume each

l(yi , hα (xi )).

(1)

2

hj depends on only a single component of x ∈ Rn , i.e., for some standard basis vector ek , and
function gj : R → {−1, +1}, hj (x) = gj (eT
k x) (the base classiﬁers are decision stumps). To make
the association between base classiﬁers and components explicit, let s be the function s(j ) = k if
k x) and Q = [qkj ] be the n × p matrix with qkj = 1[s(j )=k] . Then the vector β = Qα
hj (x) = gj (eT
indicates the relative importance the classiﬁer assigns to each instance component. Although we
used decision stumps above for simplicity, more complex base classiﬁers such as decision trees could
be used with proper modiﬁcation of mapping from α to β . We call β the component importance
map. Suppose the instance components reﬂect spatial structure in the data, e.g. the components are
samples along an interval or pixels in an image. Then the component importance map is indicating
the spatial distribution of weights that the classiﬁer employs. Presumably a good classiﬁer distributes
the weights in accordance with the discriminative power of the components; in which case, the
map is indicating how discriminative information is spatially distributed. It is in this aspect of the
classiﬁer that we are particularly interested. Now as shown above, conventional boosting ignores
spatial information. Our objective, pursued in the next sections, is to incorporated prior information
on spatial structure, e.g. a prior on the component importance map, into the boosting problem.

3 Adding Spatial Regularization

αj hj (xi )) + λβT K β

To incorporate spatial information we add spatial regularization of the form βT K β to the loss (1)
where the kernel K ∈ Rn×n
++ is positive deﬁnite. For concreteness, we employ the exponential loss
p(cid:88)
m(cid:88)
l(yi , hα (xi )) = e−yi hα (xi ) . Thus the regularized loss is:
exp(−yi
Lexp
reg (X , Y , α) =
m(cid:88)
p(cid:88)
j=1
i=1
j=1
i=1
The term βT K β imposes a spatial smoothness constraint on β . To see this, consider the eigen-
decomposition K = U ΣU T , where the columns {uj } of U are the orthonormal eigenvectors, σj
is the eigenvalue of uj and Σ = diag(σ1 , σ2 , . . . , σn ). Then the regularizing term can be rewrit-
ten as λ(cid:107)Σ 1
2 U T β(cid:107)2
2 where U T β is the “spectrum” of β under the orthogonal transformation U T .
Rather than standard Tikhonov regularization with (cid:107)β(cid:107)2
2 = (cid:107)U T β(cid:107)2
2 , we penalize the variation in
direction uj proportional to the eigenvalue σj . By doing so we are encouraging β to be close to the
eigenvectors uj with small eigenvalues. This encodes our prior spatial knowledge.

αj hj (xi )) + λαT QT KQα.

(2)

(3)

=

exp(−yi

Figure 1: Each graph is the eigenimage of size d × d corresponding to an eigenvector of K = µI − G.

As an example, consider the kernel K = µI − G, where G is a Gaussian kernel matrix:
2 (cid:107)vi−vj (cid:107)2
Gij = e− 1
2 /r2
(4)
,
with vj the spatial location of component j , (cid:107)vi − vj (cid:107)2 the Euclidean distance (other distances
can also be used) between components i and j , and r the radius parameter of the Gaussian kernel.
For the 2D case, i = (i1 , i2 ) ranges over (1, 1), (1, 2), . . . , (d, d). j = (j1 , j2 ) ranges over the same
coordinates. So G is a size d2 ×d2 matrix. We plot the 6 eigenimages of K with smallest eigenvalues
in Figure 1. The regularization imposes a spatial smoothness constraint by encouraging β to give
more weight to the eigenimages with smaller eigenvalues, e.g. the patterns shown in Figure 1.

4 A Spatially Regularized Boosting Algorithm

We now derive a spatially regularized boosting algorithm (abbreviated as SRB) using coordinate
descent on (3). In particular, in each iteration we choose a coordinate of α with the largest negative

3

αj hj (xi )) − 2eT
j (cid:48) λQT KQα.

gradient and increase the weight of that coordinate by step size ε. This results in an algorithm similar
to AdaBoost, but with additional consideration of spatial location.
p(cid:88)
m(cid:88)
To begin, we take the partial derivative of (3) w.r.t. αj (cid:48) :
Lexp
reg (X , Y , α) =
yihj (cid:48) (xi ) exp(−yi
− ∂
∂αj (cid:48)
j=1
i=1
Here ej (cid:48) is the j (cid:48) -th standard basis vector, so eT
j (cid:48) λQT KQα is the j (cid:48) -th element of λQT KQα. By
(cid:80)p
j (cid:48) QT )λKQα is the s(j (cid:48) )-th element of λKQα. Therefore if we deﬁne γ to
the deﬁnition of Q, (eT
be γ = −2λK β , and wi = exp(−yi
j=1 αj hj (xi )) (1 ≤ i ≤ m) to be the unnormalized weight
m(cid:88)
on training instance xi , then the partial derivative in (4) can be written as:
Lexp
reg (X , Y , α) =
− ∂
The term (cid:80)m
∂αj (cid:48)
i=1
i=1 yihj (cid:48) (xi )wi is the weighted performance of base classiﬁer hj (cid:48) on the training ex-
amples. Normally, we choose hj (cid:48) to maximize this term. This corresponds to choosing the best
base classiﬁer under the current weight distribution. However, here we have an additional term: the
performance of base classiﬁer hj (cid:48) is enhanced by a weight γs(j (cid:48) ) on its corresponding component
s(j (cid:48) ). We call γ the spatial compensation weight. To proceed, we choose a base classiﬁer hj (cid:48) to
maximize the sum of these two terms and then increase the weight of that base classiﬁer by a step
size ε. This gives Algorithm 1 shown in Figure 2. The key differences from AdaBoost are: (a) the
new algorithm maintains a new set of “spatial compensation weights” γ ; (b) the weights on training
examples wi are not normalized at the end of each iteration.

yihj (cid:48) (xi )wi + γs(j (cid:48) )

Algorithm 1 The SRB algorithm
1: wi ← 1, 1 ≤ i ≤ m
2: α ← 0
3: for t = 1 to T do
β ← Qα
4:
γ ← −2λK β
5:
(cid:8)Ω(hj , w) + γs(j )
(cid:9)
ﬁnd the “best” base classiﬁer in the fol-
6:
lowing sense:
j (cid:48) ← arg maxj
(cid:26) wi eε
choose a step size ε, αj (cid:48) ← αj (cid:48) + ε
7:
adjust weights:
8:
if yihj (cid:48) (xi ) = −1
wi ←
wi e−ε
if yihj (cid:48) (xi ) = 1
10: Output result: hα (x) = (cid:80)p
for 1 ≤ i ≤ m
9: end for
j=1 αj hj (x)

m(cid:88)
In both algorithms, Ω(hj , w) is deﬁned to be:
i=1
which is a performance measure of classiﬁer hj un-
der weight distribution w on training examples.

yihj (xi )wi ,

Ω(hj , w) =

7:
8:

Algorithm 2 SRB algorithm with backward steps
1: wi ← 1, 1 ≤ i ≤ m
2: α ← 0
3: for t = 1 to T do
β ← Qα
4:
γ ← −2λK β
5:
(cid:8)Ω(hj , w) + γs(j )
(cid:9)
ﬁnd the “best” base classiﬁer in the follow-
6:
ing sense:
j (cid:48) ← arg maxj
(cid:26) wi eε1
choose a step size ε1 , αj (cid:48) ← αj (cid:48) + ε1
adjust weights:
if yihj (cid:48) (xi ) = −1
wi ←
wi e−ε1
if yihj (cid:48) (xi ) = 1
(cid:8)Ω(hj , w) + γs(j )
(cid:9)
ﬁnd the “worst” active classiﬁer in the fol-
lowing sense:
j (cid:48)(cid:48) ← arg minj :αj >0
(cid:26) wi e−ε2 /2
αj (cid:48)(cid:48) ← αj (cid:48)(cid:48) − ε2
10:
2
adjust weights again:
11:
if yihj (cid:48)(cid:48) (xi ) = −1
wi ←
if yihj (cid:48)(cid:48) (xi ) = 1
wi eε2 /2
13: Output result: hα (x) = (cid:80)p
for 1 ≤ i ≤ m
12: end for
j=1 αj hj (x)

9:

Figure 2: The SRB (spatially regularized boosting algorithms).

To elucidate the effect of the compensation weights, consider the kernel K = µI −G, with G deﬁned
in (4). In this case, γ = 2λ( ¯β − µβ) where ¯β = Gβ is the Gaussian smoothing of β . Therefore,

4

.

(5)

yihj (xi )wi + γs(j )

j (cid:48)(cid:48) = arg min
1≤j≤p,αj >0

a component receives a high compensation weight γk = 2λ( ¯βk − µβk ) if some neighboring spatial
locations have already been selected (i.e., made “active”) by the composite classiﬁer. On the other
hand, the weight of a component is reduced (proportional to the magnitude of parameter µ) if it
is already “active”, i.e., βk > 0. So the algorithm encourages the selection of base classiﬁers
associated with “inactive” locations that are close to “active” locations.
(cid:40) m(cid:88)
(cid:41)
We can enhance the algorithm by including a backward step each iteration: αj (cid:48)(cid:48) ← αj (cid:48)(cid:48) − ε(cid:48) , where
i=1
This helps remove prematurely selected base classiﬁers [16, 17]. This is Algorithm 2 in Figure 2.
Spatial regularization brings no signiﬁcant computational overhead: Compared to AdaBoost, SRB
has additional steps 4,5, which can be computed in time O(n) every iteration. Adaptive weight γ
incurs no additional complexity for step 6 in our current implementation.
We now brieﬂy discuss the choice of step size ε in Algorithm 1 (ε1 and ε2 in Algorithm 2 can be
chosen similarly). ε could be a ﬁxed (small) step size at each iteration. This is not greedy but may
necessitate a large number of iterations. Alternatively, one can be greedy and select ε to minimize
the value of the loss function (3) after the change αj (cid:48) ← αj (cid:48) + ε:
where W− = (cid:80)
i:yi hj (cid:48) (xi )=−1 exp(−yihα (xi )), W+ = (cid:80)
W− eε + W+ e−ε + λ(β + εek(cid:48) )T K (β + εek(cid:48) ),
(6)
i:yi hj (cid:48) (xi )=1 exp(−yihα (xi )) and k (cid:48) =
s(j (cid:48) ). Setting the derivative of (6) to 0 yields:
W− eε − W+ e−ε − γk(cid:48) + 2λεKk(cid:48) k(cid:48) = 0.
(7)
Using e±ε ≈ 1 ± ε gives the solution ˆε = W+−W−+γk(cid:48)
W++W−+2λKk(cid:48) k(cid:48) , which can be used as a step size.
(cid:26)
(cid:27)
However, for the following slightly more conservative step size we can prove algorithm convergence:
W+ − W− + γk(cid:48)
(W + − W − )
W+ + 1.36W−
W+ + W− + 2λKk(cid:48) k(cid:48)
Theorem 1. The step size (8) ensures convergence of Algorithm 1.
Proof. (6) is convex, so its minimum point ε∗ is the unique solution of (7): f1 (ε∗ ) + f2 (ε∗ ) = 0
where f1 (ε) = W− eε − W+ e−ε and f2 (ε) = 2λKk(cid:48) k(cid:48) ε − γk(cid:48) . We have the inequality chain:
f1 ( ˜ε) + f2 ( ˜ε) ≤ g1 ( ˜ε) + f2 ( ˜ε) ≤ g1 ( ˆε) + f2 ( ˆε) = 0 = f1 (ε∗ ) + f2 (ε∗ ),
(9)
where g1 (ε) = W− (1 + ε) − W+ (1 − ε). So ˜ε is on the descending slope of (6), which is a sufﬁcient
condition for ˜ε to reduce the objective (6). Since the objective (3) is nonnegative and each iteration
of the algorithm reduces (3), the algorithm converges. The second inequality in (9) uses monoticity
while the ﬁrst inequality in (9) uses the following lemma proved in the supplementary material:
Lemma: If 0 < ε ≤ min{3 (W +−W − )
W++1.36W− , 1}, then f1 (ε) − g1 (ε) ≤ 0.

˜ε = min

3

,

, 1

.

(8)

5 The Grouping Effect: Asymptotic Analysis

Recall our objective of using the component importance map of the trained classiﬁer to ascertain
the spatial distribution of informative components in the data. Ideally, we would like β to faithfully
represent this information. In general, however, a boosting algorithm will select a sufﬁcient but
incomplete collection of base classiﬁers (and hence components) to accomplish the classiﬁcation.
For example, after selecting one base classiﬁer hj , AdaBoost will adjust the weights of training
2 (totally uninformative), thus preventing
examples to make the weighted training error of hj exactly 1
the selection of any classiﬁers similar to hj in the next iteration. In fact, for AdaBoost we can prove
that in the optimal solution α∗ , we can transfer coefﬁcient weights between any two equivalent base
classiﬁers without impacting optimality. So minimizing the loss function (1) does not require any
particular distribution among the β coefﬁcients of identical components. This is the content of the
following proposition.

5

Proposition 1. Assume hj1 and hj2 , j1 < j2 , are base classiﬁers with s(j1 ) (cid:54)= s(j2 ), and hj1 (xi ) =
j2 }],
hj2 (xi ) for all xi ∈ X . If α∗ minimizes the loss function (1), then for any η in [0, min{α∗
j1 , α∗
α† also minimizes loss function (1) where α† = α∗ − ηej1 + ηej2 where ej denotes the j -th standard
basis vector in Rp .
Proof. hj1 (xi ) = hj2 (xi ) implies that hα∗ (xi ) = hα† (xi ) for all xi ∈ X .
What is desirable is a “grouping effect”, in which components with similar behavior under H receive
similar β weights. We will prove that asymptotically, SRB exhibits a “grouping effect”. In particular,
for kernel K = µI − G, G deﬁned in (4), we will look at the minimizer β
∗ = Qα∗ of the loss
function (2), and in the spirit of [18], establish a bound on the difference |β ∗
i1 −β ∗
i2 | of the coefﬁcients
on two similar components.
∗ = Qα∗ , γ ∗ = −2λK β
∗ , and the corresponding training
To proceed, let α∗ minimize (3) with: β
instance weight w∗ . Let Hk denote the subset of base classiﬁers acting on component k , i.e., Hk =
(cid:80)m
{hj ∈ H : s(j ) = k}. The following lemma is proved in the supplementary material:
Lemma: For any k , 1 ≤ k ≤ n, −γ ∗
k ≥ maxhj ∈Hk
i with equality if β ∗
i=1 yihj (xi )w∗
k > 0.
Assuming K = µI − G, G deﬁned in (4), we have the following result:
∗ be the smoothed version of vector β
∗ = Gβ
∗ . Then for any k1 and k2 :
Theorem 2. Let ¯β
k2 | ≤ 1
1
|β ∗
k1 − β ∗
| ¯β ∗
k1 − ¯β ∗
k2 | +
(cid:80)m
(cid:80)m
d(k1 , k2 ),
µ
λµ
where d(k1 , k2 ) = | maxhj ∈Hk1
i − maxhj ∈Hk2
i |.
i=1 yihj (xi )w∗
i=1 yihj (xi )w∗
(cid:12)(cid:12)(2λ ¯β ∗
k2 )(cid:12)(cid:12) = |γ ∗
Proof. We prove the following three cases separately:
In this case, using the lemma on γ ∗
k1 and β ∗
(1). β ∗
k1 and γ ∗
k2 are both positive.
k2 yields:
k2 | = d(vk1 , vk2 ). We can then use the tri-
k1 − 2λµβ ∗
k1 ) − (2λ ¯β ∗
k2 − 2λµβ ∗
k1 − γ ∗
(cid:80)m
(cid:80)m
angle inequality on the LHS to obtain the result.
k1 = 0. Then −γ ∗
k1 ≥
(2). One of β ∗
k1 and β ∗
k2 is zero the other is positive. WLOG assume β ∗
i and −γ ∗
i=1 yihj (xi )w∗
i=1 yihj (xi )w∗
m(cid:88)
m(cid:88)
maxhj ∈Hk1
k2 = maxhj ∈Hk2
i . This gives:
i ≤ d(vk1 , vk2 ).
i − max
k2 ≤ max
k1 − γ ∗
yihj (xi )w∗
yihj (xi )w∗
γ ∗
hj ∈Hk1
hj ∈Hk2
i=1
i=1
k1 − 2λµ0) −
Substituting the deﬁnition of γ : γ = 2λGβ − 2λµβ = 2λ ¯β − 2λµβ , yields (2λ ¯β ∗
k2 − 2λµβ ∗
k2 ) ≤ d(vk1 , vk2 ). Therefore 2λµβ ∗
k2 ≤ (2λ ¯β ∗
k2 − 2λ ¯β ∗
(2λ ¯β ∗
k1 ) + d(vk1 , vk2 ). Using the
triangle inequality on the right hand side of the previous expression yields the result.
k1 = β ∗
(3) β ∗
k2 = 0. In this case, the inequality is obvious.

(10)

The theorem upper bounds the difference in the importance coefﬁcient of two components by the
sum of two terms: the ﬁrst, | ¯β ∗
k1 − ¯β ∗
k2 |, takes into account the importance weight of nearby locations.
This term is small when the two locations are spatially close, or when they are in two neighborhoods
that contain a similar amount of important voxels. The second term reﬂects the dissimilarity between
two voxels. This term measures the difference in the weighted performances of a location’s best base
classiﬁer. Clearly, d(k1 , k2 ) = 0 when components k1 and k2 are identical under H over the training
instances. More generally, we can sort all the training examples by the activation level on a single
component. If sorting on locations k1 and k2 yields the same results, then d(k1 , k2 ) = 0.

6 Experiments

The ﬁrst experiment is gender classiﬁcation using features located on 58 annotated landmark points
in the IMM face data set [19] (Figure 3(a)). For each point we extract the ﬁrst 3 principal components
of a 15×15 window as features. We randomly choose 7 males and 7 females to do leave-one-out 7-
fold cross-validation for 100 trials. AdaBoost yields an average classiﬁcation accuracy of τ = 78.8%

6

and demonstrates the grouping effect. (All experiments in this section use µ = maxj ((cid:80)
with a standard deviation of σ = 19.9%. SRB (λ = 0.1, r = 10 pixel-length) achieves τ = 80.5% and
σ = 18.7%. The component importance map β of SRB reveals both eyes as discriminating areas
i Gij ). By
(10), a larger µ will make this grouping effect more dominant). The β for AdaBoost is less smooth
and less interpretable with the most important component on the left chin (Figure 3(b,c)).

(a)

(b)

(c)

(d)

(a)

(b)

(c)

(e)

(f)

(g)

(h)

Figure 3: Experiment 1. (a): an example showing annotated
points; (b-c): the average component importance map β (in-
dicated by sizes of the circles) after running (b) AdaBoost
and (c) SRB for 50 iterations.

Figure 4: Experiment 2.
(a-d): example im-
ages; (e): example training image with noise;
(f): ground truth of discriminative pixels; (g-h):
pixels selected by (g) AdaBoost and (h) SRB.

The second experiment is a binary image classiﬁcation task. Each image contains the handwritten
digits 1,1,0,3 and a random digit, all in ﬁxed locations. Digits 0 and 1 are swapped between the
classes (Figure 4(a-d)). The handwritten digit images are from the OCR digits data set [20]. To
obtain the training/testing instances we add noise to the images (Figure 4(e)). We test the ability
of several algorithms to: (a) ﬁnd the discriminating pixels, and (b) if a classiﬁcation algorithm, ac-
curately classify the classes. The quality of pixel selection is measured by a precision-recall curve,
with ground truth pixels (Figure 4(f)) selected by a t-test on the two classes of noiseless images. This
curve is plotted for the following methods: (1) SRB (λ = 0.5, r = 1√
pixel-length) (2) AdaBoost;
2
(3) thresholding the univariate t-test score; (4) thresholding the ﬁrst one or two principle compo-
nent(s); (5) thresholding the pixel coefﬁcients in an LDA model with diagonal covariance (Gaussian
naive bayes classiﬁer); (6) level-set method [6] on a Z-statistics map. We plot the precision-recall
curve by varying the number of iterations (for (1),(2)) or the value of the threshold (for (3)-(6)). We
also tried all methods with Gaussian spatial pre-smoothing as a preprocessing step. The classiﬁca-
tion accuracies are measured for methods (1), (2) and (5) on separate test data.
The results, averaged over 100 noise realizations, are plotted in Figure 5. SRB showed no loss of
classiﬁcation accuracy nor convergence speed (usually within 100 iterations), and achieved the best
pixel selection among all methods. It is better than Gaussian naive Bayes and PCA methods, even
when the noise matches the i.i.d. Gaussian assumption of these methods (Figure 5(a,d)). In all cases,
local spatial averaging deteriorates the classiﬁcation performance of boosting.
In the third experiment, subjects watch a movie during the fMRI scan. The classiﬁcation task is
to discriminate two types of scenes (faces and objects) based on the fMRI responses. Each fMRI
responses is a single TR scan of the brain volume. We divide the data (14 subjects, 26 face and
18 object fMRI responses) into 10 cross validation groups and average the classiﬁcation accuracies.
SRB (λ = 0.1, r = 5 voxel-length) trained for 100 iterations yields accuracy τ = 73.3% with
σ = 9.3% across 14 subjects. AdaBoost yields τ = 75.5% with σ = 4.9%. To make sure this
is signiﬁcant, we repeated the training with shufﬂed labels. After shufﬂing, τ = 49.7%, with
σ = 4.6%, which is effectively chance. We note that spatially regularized boosting yields a more
clustered and interpretable selection of voxels. The result for one subject (Figure 6) shows that
standard boosting (AdaBoost) selects voxels scattered in the brain, while SRB selects clustered
voxels and nicely highlights the relevant FFA area [21] and posterior central sulcus [22, 23].

7 Conclusions

The proposed SRB algorithm is applicable to a variety of situations in which one needs to boost
the performance of base classiﬁers with spatial structure. The mechanism of the algorithm has a

7

(a)

(b)

(c)

(d)

(e)

(f)

Figure 5: Experiment 2. (a-c): test classiﬁcation accuracy: (a) i.i.d. Gaussian noise, (b) poisson noise, (c)
spatially correlated Gaussian noise. (b,c) share the legend of (a). (d-f): pixel selection performances: (d) i.i.d.
Gaussian noise, (e) poisson noise, (f) spatial correlated Gaussian noise. (e,f) share the legend of (d).

(a)

(b)

(c)

Figure 6: Experiment 3: an example: sets of voxels selected by (a) univariate t-test (b) AdaBoost and (c) SRB

natural interpretation: in each iteration, the algorithm selects a base classiﬁer with the best perfor-
mance evaluated under two sets of weights: weights on training examples (as in AdaBoost) and
weights on locations. The additional set of location weights encourages or discourages the selection
of certain base classiﬁers based on the spatial location of base classiﬁers that have already been se-
lected. Computationally, SRB is as effective as AdaBoost. We demonstrated the effectiveness of the
algorithm both by providing a theoretical analysis of the “grouping effect” and by experiments on
three data sets. The grouping effect is clearly demonstrated in the face gender detection experiment.
In the OCR classiﬁcation experiment, the algorithm shows superior performance in pixel selection
accuracy without loss of classiﬁcation accuracy. The algorithm matches the performance of the
state-of-the-art set estimation methods [6] that use a more complex spatial regularization and cycle
spinning technique. In the fMRI experiment, the algorithm yields a clustered selection of voxels in
positions relevant to the task. An alternative approach, being explored, is to combine searchlight [9]
with a strong learning algorithm (e.g. SVM) to integrate spatial locality and accurate classiﬁcation.

8 Acknowledgments

The authors thank Princeton University’s J. Insley Blair Pyne Fund for seed research funding.

8

0501001502000.860.880.90.920.940.960.981iterations of boostingclassification accuracy on test images  Spatial Regularized BoostingSpatial Regularized Boosting with smoothingAdaBoostAdaBoost with smoothingGaussian naive BayesGaussian naive Bayes with smoothing0501001502000.70.750.80.850.90.95iterations of boostingclassification accuracy on test images0501001502000.80.850.90.951iterations of boostingclassification accuracy on test images00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecision  Spatial Regularized BoostingSpatial Regularized Boosting with smoothingAdaBoostAdaBoost with smoothingUnivariate testUnivariate test with smoothingPCA, first PCPCA, first two PCsPCA with smoothing, first PCGaussian naive BayesGaussian naive Bayes with smoothinglevel−setlevel−set with smoothing00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecision00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionReferences
[1] K.J. Friston, J. Ashburner, J. Heather, et al. Statistical parametric mapping. Neuroscience Databases: A
Practical Guide, page 237, 2003.
[2] R. Heller, D. Stanley, D. Yekutieli, N. Rubin, and Y. Benjamini. Cluster-based analysis of FMRI data.
NeuroImage, 33(2):599–608, 2006.
[3] D. Van De Ville, T. Blu, and M. Unser. Integrated wavelet processing and spatial statistical testing of
fMRI data. NeuroImage, 23(4):1472–1485, 2004.
[4] D. Van De Ville, M.L. Seghier, F. Lazeyras, T. Blu, and M. Unser. WSPM: Wavelet-based statistical
parametric mapping. NeuroImage, 37(4):1205–1217, 2007.
[5] Z. Harmany, R. Willett, A. Singh, and R. Nowak. Controlling the error in fmri: Hypothesis testing or set
estimation? In Biomedical Imaging, 5th IEEE International Symposium on, pages 552–555, 2008.
IEEE Transactions on Image
[6] R.M. Willett and R.D. Nowak. Minimax optimal level-set estimation.
Processing, 16(12):2965–2979, 2007.
[7] J.V. Haxby, M.I. Gobbini, M.L. Furey, A. Ishai, J.L. Schouten, and P. Pietrini. Distributed and overlapping
representations of faces and objects in ventral temporal cortex. Science, 293(5539):2425–2430, 2001.
[8] K.A. Norman, S.M. Polyn, G.J. Detre, and J.V. Haxby. Beyond mind-reading: multi-voxel pattern analysis
of fMRI data. Trends in Cognitive Sciences, 10(9):424–430, 2006.
[9] N. Kriegeskorte, R. Goebel, and P. Bandettini. Information-based functional brain mapping. Proceedings
of the National Academy of Sciences, 103(10):3863–3868, 2006.
[10] V. Koltchinskii, M. Martınez-Ramon, and S. Posse. Optimal aggregation of classiﬁers and boosting maps
in functional magnetic resonance imaging. Advances in Neural Information Processing Systems, 17:705–
712, 2005.
[11] M. Mart´ınez-Ram ´on, V. Koltchinskii, G.L. Heileman, and S. Posse.
fMRI pattern classiﬁcation using
neuroanatomically constrained boosting. NeuroImage, 31(3):1129–1141, 2006.
[12] Melissa K. Carroll, Kenneth A. Norman, James V. Haxby, and Robert E. Schapire. Exploiting spatial
information to improve fmri pattern classiﬁcation. In 12th Annual Meeting of the Organization for Human
Brain Mapping, Florence, Italy, 2006.
[13] J.H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics,
29(5):1189–1232, 2001.
[14] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application
to boosting. In European Conference on Computational Learning Theory, pages 23–37, 1995.
[15] C. Rudin, I. Daubechies, and R.E. Schapire. The dynamics of adaboost: Cyclic behavior and convergence
of margins. Journal of Machine Learning Research, 5(2):1557, 2005.
[16] Z.J. Xiang and P.J. Ramadge. Sparse boosting. In IEEE International Conference on Acoustics, Speech
and Signal Processing, 2009.
[17] T. Zhang. Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models. In
Proc. Neural Information Processing Systems, 2008.
[18] H. Zou and T. Hastie. Regression shrinkage and selection via the elastic net, with applications to microar-
rays. JR Statist. Soc. B, 2004.
[19] M.M. Nordstrøm, M. Larsen, J. Sierakowski, and M.B. Stegmann. The IMM face database-an annotated
dataset of 240 face images. Technical report, DTU Informatics, Building 321, 2004.
[20] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[21] N. Kanwisher, J. McDermott, and M.M. Chun. The fusiform face area: a module in human extrastriate
cortex specialized for face perception. Journal of Neuroscience, 17(11):4302–4311, 1997.
[22] U. Hasson, M. Harel, I. Levy, and R. Malach. Large-scale mirror-symmetry organization of human
occipito-temporal object areas. Neuron, 37(6):1027–1041, 2003.
[23] U. Hasson, Y. Nir, I. Levy, G. Fuhrmann, and R. Malach. Intersubject synchronization of cortical activity
during natural vision. Science, 303(5664):1634–1640, 2004.

9


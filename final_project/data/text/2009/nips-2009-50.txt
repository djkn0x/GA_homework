White Functionals for Anomaly Detection in
Dynamical Systems

Marco Cuturi
ORFE - Princeton University
mcuturi@princeton.edu

Jean-Philippe Vert
Mines ParisTech, Institut Curie, INSERM U900
Jean-Philippe.Vert@mines.org

Alexandre d’Aspremont
ORFE - Princeton University
aspremon@princeton.edu

Abstract

We propose new methodologies to detect anomalies in discrete-time processes
taking values in a probability space. These methods are based on the inference
of functionals whose evaluations on successive states visited by the process are
stationary and have low autocorrelations. Deviations from this behavior are used
to ﬂag anomalies. The candidate functionals are estimated i n a subspace of a
reproducing kernel Hilbert space associated with the original probability space
considered. We provide experimental results on simulated datasets which show
that these techniques compare favorably with other algorithms.

1 Introduction

Detecting abnormal points in small and simple datasets can often be performed by visual inspec-
tion, using notably dimensionality reduction techniques. However, non-parametric techniques are
often the only credible alternative to address these problems on the many high-dimensional, richly
structured data sets available today.

When carried out on independent and identically distributed (i.i.d) observations, anomaly detection
is usually referred to as outlier detection and is in many ways equivalent to density estimation.
Several density estimators have been used in this context and we refer the reader to the exhaustive
review in [1]. Among such techniques, methods which estimate non-parametric alarm functions in
reproducing kernel Hilbert spaces (rkHs) are particularly relevant to our work. They form alarm
functions of the type f ( · ) = Pi∈I ci k(xi , · ), where k is a positive de ﬁnite kernel and (ci )i∈I
is a family of coefﬁcients paired with a family (xi )i∈I of previously observed data points. A new
observation x is ﬂagged as anomalous whenever f (x) goes outside predetermined boundaries which
are also provided by the algorithm. Two well known kernel methods have been used so far for
this purpose, namely kernel principal component analysis (kPCA) [2] and one-class support vector
machines (ocSVM) [3]. The ocSVM is a popular density estimation tool and it is thus not surprising
that it has already found successful applications to detect anomalies in i.i.d data [4]. kPCA can also
be used to detect outliers as described in [5], where an outlier is de ﬁned as any point far enough
from the boundaries of an ellipsoid in the rkHs containing most of the observed points.

These outlier detection methods can also be applied to dynamical systems. We now monitor discrete
time stochastic processes Z = (Zt )t∈N taking values in a space Z and, based on previous obser-
vations zt−1 , · · · , z0 , we seek to detect whether a new observation zt abnormally deviates from the
usual dynamics of the system. As explained in [1], this problem can be reduced to density estimation
when either Zt or a suitable representation of Zt that includes a ﬁnite number of lags is Markovian,
i.e. when the conditional probability of Zt given its past depends only on the values taken by Zt−1 .

1

In practice, anomaly detection then involves a two step procedure. It ﬁrst produces an estimator
ˆZt of the conditional expectation of Zt given Zt−1 to extract an empirical estimator for the residues
ˆεt = Zt − ˆZt . Under an i.i.d assumption, abnormal residues can then be used to ﬂag anomalies. This
approach and advanced extensions can be used both for multivariate data [6, 7] and linear processes
in functional spaces [8] using spaces of H ¨olderian functions.

The main contribution of our paper is to propose an estimation approach of alarm functionals that
can be used on arbitrary Hilbert spaces and which bypasses the estimation of residues ˆεt ∈ Z by fo-
cusing directly on suitable properties for alarm functionals. Our approach is based on the following
intuition. Detecting anomalies in a sequence generated by white noise is a task which is arguably
easier than detecting anomalies in arbitrary time-series. In this sense, we look for functionals α such
that α(Zt ) exhibits a stationary behavior with low autocorrelations, ideally white noise, which can
be used in turn to ﬂag an anomaly whenever α(Zt ) departs from normality. We call functionals α
that strike a good balance between exhibiting a low autocovariance of order 1 and a high variance on
successive values Zt a white functional of the process Z . Our de ﬁnition can be naturally generalized
to higher autocovariance orders as the reader will naturally see in the remaining of the paper.

Our perspective is directly related to the concept of cointegration (see [9] for a comprehensive re-
view) for multivariate time series, extensively used by econometricians to study equilibria between
various economic and ﬁnancial indicators. For a multivaria te stochastic process X = (Xt )t∈Z tak-
ing values in Rd , X is said to be cointegrated if there exists a vector a of Rd such that (aT Xt )t∈Z is
stationary. Economists typically interpret the weights of a as describing a stable linear relationship
between various (non-stationary) macroeconomic or ﬁnanci al indicators. In this work we discard the
immediate interpretability of the weights associated with linear functionals aT Xt to focus instead
on functionals α in a rkHs H such that α(Zt ) is stationary, and use this property to detect anomalies.
The rest of this paper is organized as follows. In Section 2, we study different criterions to measure
the autocorrelation of a process, directly inspired by min/max autocorrelation factors [10] and the
seminal work of Box-Tiao [11] on cointegration. We study the asymptotic properties of ﬁnite sample
estimators of these criterions in Section 3 and discuss the practical estimation of white functionals
in Section 4. We discuss relationships with existing methods in Section 5 and provide experimental
results to illustrate the effectiveness of these approaches in Section 6.

2 Criterions to deﬁne white functionals

Consider a process Z = (Zt )t∈ Z taking values in a probability space Z . Z will be mainly considered
in this work under the light of its mapping onto a rkHs H associated with a bounded and continuous
kernel k on Z × Z . Z is assumed to be second-order stationary, that is the densities p(Zt = z ) and
joint densities p(Zt = z , Zt+k = z ′ ) for k ∈ N are independent of t. Following [12, 13] we write

φt = ϕ(Zt ) − Ep [ϕ(Zt )],
for the centered projection of Z in H, where ϕ : z ∈ Z → k(z , ·) ∈ H is the feature map associated
with k . For two elements α and β of H we write α ⊗ β for their tensor product, namely the linear
map of H onto itself such that α ⊗ β : x → hα, xiH β . Using the notations of [14] we write

D = Ep [φt ⊗ φt+1 ],
C = Ep [φt ⊗ φt ],
respectively for the covariance and autocovariance of order 1 of φt . Both C and D are linear op-
erators of H by weak stationarity [14, De ﬁnition 2.4] of
(φt )t∈Z , which can be deduced from the
second-order stationarity of Z . The following de ﬁnitions introduce two criterions which q uantify
how related two successive evaluations of α(Zt ) are.
De ﬁnition 1 (Autocorrelation Factor [10]). Given an element α of H such that hα, C αiH > 0,
γ (α) is the absolute autocorrelation of α(Z ) of order 1,

γ (α) = | corr(α(Zt ), α(Zt+1 )| =

|hα, DαiH |
hα, C αiH

.

(1)

The condition hα, C αiH > 0 requires that var α(φt ) is not zero, which excludes constant or van-
ishing functions on the support of the density of φt . Note also that de ﬁning γ requires no other
assumption than second-order stationarity of Z .

2

If we assume further that φ is an autoregressive Hilbertian process of order 1 [14], ARH(1) for short,
there exists a compact operator ρ : H → H and a H strong white noise1 (εt )t∈Z such that

φt+1 = ρ φt + εt .
In their seminal work, Box and Tiao [11] quantify the predictability of the linear functionals of
a vector autoregressive process in terms of variance ratios. The following de ﬁnition is a direct
adaptation of that principle to autoregressive processes in Hilbert spaces. From [14, Theorem 3.2]
we have that C = ρ C ρ∗ + Cε where for any linear operator A of H, A∗ is its adjoint.
De ﬁnition 2 (Predictability in the Box-Tiao sense [11]). Given an element α of H such that
hα, C αiH > 0, the predictability λ(α) is the quotient

λ(α) =

varhα, ρ φt iH
varhα, φt iH

=

hα, ρ C ρ∗αiH
hα, C αiH

=

hα, DC −1D∗αiH
hα, C αiH

.

(2)

The right hand-side of Equation (2) follows from the fact that ρ C = D and ρ∗ = C −1D∗ [14],
the latter equality being always valid irrelevant of the existence of C −1 on the whole of H as noted
in [15]. Combining these two equalities gives ρ C ρ∗ = DC −1D∗ .
Both γ and λ are convenient ways to quantify for a given function f of H the independence of f (Zt )
with its immediate past. We provide in this paragraph a common representation for λ and γ . For any
linear operator A of H and any non-zero element x of H write R(A, x) for the Rayleigh quotient

.

R(A, x) =

hx, AxiH
hx, xiH
We use the notations in [12] and introduce the normalized cross-covariance (or rather auto-
2 DC − 1
covariance in the context of this paper) operator V = C − 1
2 . Note that for any skew-
symmetric operator A, that is A = −A∗ , we have that hx, AxiH = hA∗x, xiH = −hAx, xiH = 0
and thus R(A, x) = R( A+A∗
, x). Both λ and γ applied on a function α ∈ H can thus be written as
2
R (cid:18) V + V ∗
2 α(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
γ (α) = (cid:12)(cid:12)(cid:12)(cid:12)
1
2
As detailed in Section 4, our goal is to estimate functions in H from data such that they have either
low γ or λ values. Minimizing λ is equivalent to solving a generalized eigenvalue problem through
the Courant-Fisher-Weyl theorem. Minimizing γ is a more challenging problem since the operator
V + V ∗ is not necessarily positive de ﬁnite. The S-lemma from contr ol theory [16, Appendix B.2]
can be used to cast the problem of estimating functions with low γ as a semi-de ﬁnite program. In
practice the eigen-decomposition of V + V ∗ provides good approximate answers.
The formulation of γ and λ as Rayleigh quotients is also useful to obtain the asymptotic convergence
of their empirical counterparts (Section 3) and to draw comparisons with kernel-CCA (Section 5).

, λ(α) = R(V V ∗ , C

1
2 α).

, C

3 Asymptotics and matrix expressions for empirical estimators of γ and λ

3.1 Asymptotic convergence of the normalized cross-covariance operator V

The covariance operator C and cross-covariance operator D can be estimated through a ﬁnite sample
of points z0 , · · · , zn translated into a sample of centered points φ1 , · · · , φn in H, where φi =
n+1 Pn
ϕ(zi ) − 1
j=0 ϕ(zj ). We write

n−1
n
Xi=1
Xi=1
for the estimates of C and D respectively which converge in Hilbert-Schmidt norm [14]. Estimators
for γ or λ require approximating C − 1
2 , which is a typical challenge encountered when studying

φi ⊗ φi , Dn =

1
n − 1

1
n − 1

φi ⊗ φi+1 ,

Cn =

1 namely a sequence (εt )t∈Z of H random variables such that (i) 0 < E kεt k2 = σ 2 , E εt = 0 and the
covariance Cεt is constant, equal to Cε ; (ii) (εt ) is a sequence of i.i.d H-random variables

3

ARH(1) processes and more generally stationary linear processes in Hilbert spaces [14, Section
8]. This issue is addressed in this section through a Tikhonov-regularization, that is considering a
sequence of positive numbers ǫn we write
Vn = (Cn + ǫn I )− 1
2 Dn (Cn + ǫn I )− 1
2 ,
for the empirical estimate of V regularized by ǫn . We have already assumed that k is bounded and
continuous. The convergence of Vn to V in norm is ensured under the additional conditions below
1
(log n/n)
Theorem 3. Assume that V is a compact operator,
3
ǫn
writing k · kS for the Hilbert-Schmidt operator norm,

ǫn = 0 and lim
lim
n→∞
n→∞
kVn − V kS = 0.
lim
n→∞

= 0. Then

Proof. The structure of the proof is identical to that of of [12, Theorem 1] except that the i.i.d
assumption does not hold here. In [12], the norm kVn − V kS is upper-bounded by the two terms
kVn − (C + ǫn I )− 1
2 D(C + ǫn I )− 1
2 kS + k(C + ǫnI )− 1
2 D(C + ǫn I )− 1
2 − V kS . The second term
converges under the assumption that ǫn → 0 [12, Lemma 7] while the ﬁrst term decreases at a rate
that is proportional to the rates of kCn − C kS and kDn − DkS . With the assumptions above [14,
1
1
Corollary4.1,Theorem 4.8] gives us that kCn −C kS = O(( log n
2 ) and kDn −DkS = O(( log n
2 ).
n )
n )
We use this result to substitute the latter rate to the faster rate obtained for i.i.d observations in [12,
Lemma 5] and conclude the proof.

3.2 Empirical estimators and matrix expressions

,

, (Cn + ǫn I )

Given α ∈ H, consider the following estimators of γ (α) and λ(α) de ﬁned in Equations (1) and (2),
= (cid:12)(cid:12)hα, 1
n)αiH (cid:12)(cid:12)
2 (Dn + D∗
R (cid:18) Vn + V ∗
γn (α) = (cid:12)(cid:12)(cid:12)(cid:12)
2 α(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
1
n
2
hα, (Cn + ǫn I )αiH
hα, Dn (Cn + ǫn I )−1D∗
nαiH
1
λn (α) = R(Vn V ∗
,
n , (Cn + ǫn I )
2 α) =
hα, (Cn + ǫn I )αiH
1
which converge to the adequate values through the convergence of (Cn + ǫn I )
n and
2 , Vn + V ∗
n . The n observations φ1 , . . . , φn which de ﬁne the empirical estimators above also span a
Vn V ∗
subspace Hn in H which can be used to estimate white functionals. Given α ∈ Hn we use any
arbitrary decomposition α = Pn
aiφi . We write K for the original n + 1 × n + 1 Gram matrix
i=1
[k(zi , zj )]i,j and ¯K for its centered counterpart ¯K = (In − 1
1n,n)K (In − 1
1n,n ) = [hφi , φj iH ]i,j .
n
n
Because of the centering span{φ0 , . . . , φn} is actually equal to span{φ1 , . . . , φn } and we will only
¯K .
use the n × n matrix K obtained by removing the ﬁrst row and column of
For a n × n matrix M , we write M−i for the n × n − 1 matrix obtained by removing the ith column
of M . With these notations, λn and γn take the following form when evaluated on α ∈ Hn ,
γn (α) = γn   n
aiφi! =
−n + K−1KT
|aT (K−1KT
−n )a|
1
Xi=1
aT (K2 + nǫnK)a
2
λn (α) = λn   n
aiφi! =
−n (K2 + nǫnK)−1K−nKT
aT K−1KT
−1
Xi=1
aT (K2 + nǫnK)a
If ǫn follows the assumptions of Theorem 3, both γn and λn converge to γ and λ pointwise in Hn .

,

a

.

4 Selecting white functionals in practice

Both γ (α) and λ(α) are proxies to quantify the independence of successive observations α(Zt ).
Namely, functions with low γ and λ are likely to have low autocorrelations and be stationary when
evaluated on the process Z , and the same can be said of functions with low γn and λn asymptotically.
However, when H is of high or in ﬁnite dimension, the direct minimization of γn and λn is likely
to result in degenerate functions2 which may have extremely low autocovariance on Z but very low
variance as well. We select white functionals having this trade off in mind, such that both hα, C, αiH
is not negligible and γ or λ are low at the same time.
2Since the rank of operator Vn is actually n − 1, we are even guaranteed to ﬁnd in Hn a minimizer for γn
and another for λn with respectively zero predictability and zero absolute autocorrelation.

4

4.1 Enforcing a lower bound on hα, C αiH

We consider the following strategy: following the approach outlined in [14, Section 8] to estimate
autocorrelation operators, and more generally in [17] in the context of kernel methods, we restrict
Hn to the directions spanned by the p ﬁrst eigenfunctions of the operator Cn . Namely, suppose Cn
can be decomposed as Cn = Pn
i=1 giei ⊗ ei where ei is an orthonormal basis of eigenvectors with
eigenvalues in decreasing order g1 ≥ g2 ≥ · · · ≥ gn ≥ 0. For 1 ≤ p ≤ n We write Hp for the
span{e1 , . . . , ep} of the p ﬁrst eigenfunctions. Any function α in Hp is such that hα, Cn αiH ≥ gp
and thus allows us to keep the empirical variance of α(Zt ) above a certain threshold. Let Ep be the
n × p coordinate matrix of eigenvectors3 e1 , . . . , ep expressed in the family of n vectors φ1 , . . . , φn
and G the p × p diagonal matrix of terms (g1 , . . . , gp ). We consider now a function β = Pp
bi ei
i
in Hp , and note that
|bT ET
p (K−1KT
−n + K−1KT
−n)Epb|
1
γn (β ) =
,
bT (G + nǫnI)b
2
−n (K2 + nǫnK)−1K−nKT
bT ET
K−1KT
p
−1
bT (G + nǫnI)b
We de ﬁne two different functions of Hp , βmac and βBT , as the the functionals in Hp whose coefﬁ-
cients correspond to the eigenvector with minimal (absolute) eigenvalue of the two Rayleigh quo-
tients of Equations (3) and (4) respectively. We call these functionals the minimum autocorrelation
(MAC) and Box-Tiao (BT) functionals of Z . Below is a short recapitulation of all the computational
steps we have described so far.

λn (β ) =

(3)

Epb

.

(4)

• Input: n + 1 observations z0 , · · · , zn ∈ Z of a time-series Z , a p.d. kernel k on Z × Z
and a parameter p (we propose an experimental methodology to set p in Section 6.3)
• Output: a real-valued function f (·) = Pn
i=0 ci k(zi , ·) that is a white functional of Z .
• Algorithm:
– Compute the (n + 1) × (n + 1) kernel matrix K , center it and drop the ﬁrst row and
column to obtain K.
– Store K’s p ﬁrst eigenvectors and eigenvalues in matrices U and diag(v1 , · · · , vp ).
– Compute Ep = U diag(v1 , · · · , vp )−1/2 and G = 1
n diag(v1 , · · · , vp ).
– Compute the matrix numerator N and denominator D of either Equation (3) or Equa-
tion (4) and recover the eigenvector b with minimal absolute eigenvalue of the gener-
alized eigenvalue problem (N, D)
n Pn
n Pn
– Set a = Epb ∈ Rn . Set c0 = − 1
aj and ci = ai − 1
1
1
5 Relation to other methods and discussion

aj

The methods presented in this work offer numerous parallels with other kernel methods such as
kernel-PCA or kernel-CCA which, similarly to the BT and MAC functionals, provide a canonical
decomposition of Hn into n ranked eigenfunctions.
When Z is ﬁnite dimensional, the authors of [18] perform PCA on a tim e-series sample z0 , . . . , zn
and consider its eigenvector with smallest eigenvalue to detect cointegrated relationships in the pro-
cess Zt . Their assumption is that a linear mapping αT Zt that has small variance on the whole
sample can be interpreted as an integrated relationship. Although the criterion considered by PCA,
namely variance, disregards the temporal structure of the observations and only focuses on the val-
ues spanned by the process, this technique is useful to get rid of all non-stationary components of
Zt . On the other hand, kernel-PCA [2], a non-parametric extension of PCA, can be naturally applied
for anomaly detection in an i.i.d. setting [5]. It is thus natural to use kernel-PCA, namely an eigen-
function with low variance, and hope that it will have low autocorrelation to de ﬁne white functionals
of a process. Our experiments show that this is indeed the case and in agreement with [5] seem to

3Recall that if (ui , vi ) are eigenvalue and eigenvector pairs of K, the matrix E of coordinates of eigenfunc-
tions ei expressed in the n points φ1 , . . . , φn can be written as U diag(v−1/2
) and the eigenvalues gi are equal
i
to vi
n if taken in the same order[2].

5

indicate that the eigenfunctions which lie at the very low end of the spectrum, usually discarded as
noise and less studied in the literature, can prove useful for anomaly detection tasks.

kernel-CCA and variations such as NOCCO [12] are also directly related to the BT functional.
Indeed, the operator V V ∗ used in this work to de ﬁne λ is used in the context of kernel-CCA to
extract one of the two functions which maximally correlate two samples, the other function being
obtained from V ∗V . Notable differences between our approach and kernel-CCA are: 1.
in the
context of this paper, V is an autocorrelation operator while the authors of [12] consider normalized
covariances between two different samples; 2. kernel-CCA assumes that samples are independently
and identically drawn, which is de ﬁnitely not the case for th e BT functional; 3. while kernel-CCA
maximizes the Rayleigh quotient of V V ∗ , we look for eigenfunctions which lie at the lower end of
the spectrum of the same operator. A possible extension of our work is to look for two functionals f
and g which, rather than maximize the correlation of two distinct samples as is the case in CCA, are
estimated to minimize the correlation between g (zt ) and f (zt+1 ). This direction has been explored
in [19] to shed a new light on the Box-Tiao approach in the ﬁnit e dimensional case.

6 Experimental results using a population dynamics model

6.1 Generating sample paths polluted by anomalies

S = 4,

,

(5)

Xt+1 = Xt +

r ◦ Xt ◦ (1S − AXt ) .

We consider in this experimental section a simulated dynamical system perturbed by arbitrary
anomalies. To this effect, we use the Lotka-Volterra equations to generate time-series quantify-
ing the populations of different species competing for common resources. For S species, the model
tracks the population level Xt,i at time t of each species i, which is a number bounded between 0
and 1. Values of 0 and 1 account respectively for the extinction and the saturation levels of each
species. Writing ◦ for the coordinate-wise kronecker product of vectors and matrices and h > 0 for
a discretization step, the population vector Xt ∈ [0, 1]S follows the discrete-time dynamic equation
1
h
We consider the following coefﬁcients introduced in [20] wh ich are known to yield chaotic behavior,
0
1.09 1.52
1
1


, A = 
r = 
1.36
.44
1
0
0.72




.47
1
2.33
0
1.53
.51
1.21
.35
1
1.27
which can be turned into a stochastic system by adding an i.i.d. standard Gaussian noise εt ,
1
r ◦ Zt ◦ (14 − AZt ) + σε εt .
Zt+1 = Zt +
h
Whenever the equations generate coordinates below 0 or above 1, the violating coordinates are set
to 0 + u or 1 − u respectively, where u is uniform over [0, 0.01].
We consider trajectories of length 800 of the Lotka-Volterra system described in Equation (5). For
each experiment we draw a starting point Z0 randomly with uniform distribution on [0, 1]4 , discard
the 10 ﬁrst iterations and generate 400 iterations followin g Equation (5). Following this we select
randomly (uniformly over the remaining 400 steps) 40 time stamps t1 , · · · , t40 where we introduce
a random perturbation at tk such that Ztk , rather than following the dynamic of Equation (5) is
randomly perturbed by a noise δt chosen uniformly over {−1, 1}4 with a magnitude σδ , that is
Ztk = Ztk−1 + σδ δtk −1 .
For all other timestamps tk < t < tk+1 , the system follows the usual dynamic of Equation (5).
Anomalies violate the usual dynamics in two different ways: ﬁrst, they ignore the usual dynamical
equations and the current location of the process to create instead purely random increments; second,
depending on the magnitude of σδ relative to σǫ , such anomalies may induce unusual jumps.

6.2 Estimation of white functionals and other alarm functions

We compare in this experiment ﬁve techniques to detect the an omalies described above: the Box-
Tiao functional and a variant described in the paragraph below, the minimal autocorrelation func-
tional, a one-class SVM and the low-variance functional de ﬁ ned by the (p + 1)th eigenfunction of

6

1

0.5

0

4

2

0

−2

0

−0.1

−0.2

Lotka Volterra System

20
60
40
Box−Tiao − AUC: 0.828

80
weights

100

4

2

0

0.2
0
−0.2
−2
−0.4

50
150
100
ocSVM − AUC: 0.444

200
weights

0.1

50

100

150

200

0

120
160
140
kMAC − AUC: 0.797

200

180
weights

50
100
150
kPCA − AUC: 0.628

0.4
0.2
0
−0.2
−0.4

200
weights

0.4
0.2
0
−0.2

50

100

150

200

6
4
2
0
−2

0.05

Figure 1: The ﬁgure on the top plots a sample path of length 200 of a 4-dimensional Lotka-Volterra
dynamic system with perturbations drawn with σε = .01 and σδ = 0.02. The data is split between
80 regular observations and 120 observations polluted by 10 anomalies. All four functionals have
been estimated using ρ = 1, and we highlight by a red dot the values they take when an anomaly
is actually observed. The respective weights associated to each of the 80 training observations are
displayed on the right of each methodology.

the empirical covariance Cn , given by kernel-PCA. All techniques are parameterized by a kernel k .
Writing ∆zi = zi − zi−1 , we use the following mixture of kernels k :

k(zi , zj ) = ρ e−100k∆zi−∆zj k2

+ (1 − ρ)e−10kzi−zj k2

,

(6)

with ρ ∈ [0, 1]. The ﬁrst term in k discriminates observations according to their location in [0, 1]4 .
When ρ = 0.5, k accounts for both the state of the system and its most recent increments, while
only increments are considered for ρ = 1. Anomalies can be detected with both criterions, since
they can be tracked down when the process visits unusual regions or undergoes brusque and atypical
changes. The kernel widths have been set arbitrarily.

We discuss in this paragraph a variant of the BT functional. While the MAC functional is de ﬁned
and estimated in order to behave as closely as possible to random i.i.d noise, the BT functional βBT
is tuned to be stationary as discussed in [11]. In order to obtain a white functional from βBT it is
possible to model the time series βBT (zt ) as an unidimensional autoregressive model, that is estimate
(on the training sample again) coefﬁcients r1 , r2 , . . . , rq such that

βBT (zt ) =

riβBT (zt−i ) + ˆεBT
t

q
Xi=1
Both the order q and the autoregressive coefﬁcients can be estimated on the t raining sample with
standard AR packages, using for instance Schwartz’s criterion to select q . Note that although φ(Zt )
is assumed to be ARH(1), this does not necessarily translate into the fact that the real-valued process
βBT (Zt ) = hβBT , φt iH is AR(1) as pointed out in [14, Theorem 3.4]. In practice however we use
t = βBT (zt ) − Pp
the residuals ˆεBT
i=1 riβBT (zt−i ) to de ﬁne the Box-Tiao residuals functional which
we write ˜βBT .

.

7

 ρ = 0

BT Res
BT
MAC
ocSVM
kpca

1

0.9

0.8

0.7

0.6

0.5

C
U
A

0.01 0.02 0.03 0.04 0.05

1

0.9

0.8

0.7

0.6

0.5
 

 ρ = 0.5

 

 ρ = 1

1

0.9

0.8

0.7

0.6

0.5

0.01 0.02 0.03 0.04 0.05
Noise Amplitude σδ

0.01 0.02 0.03 0.04 0.05

Figure 2: The three successive plot stand for three different values of ρ = 0, 0.5, 1. The detection
rate naturally increases with the size of the anomaly, to the extent that the task becomes only a gap
detection problem when σδ becomes closer to 0.05. Functionals βBT , ˜βBT and βmac have a similar
performance and outperform other techniques when the task is most difﬁcult and σδ is small.

6.3 Parameter selection methodology and numerical results

The BT functional βBT and its residuals ˜βBT , the MAC function βmac , the one-class SVM ˆfocSVM
and the p + 1th eigenfunction ep+1 are estimated on a set of 400 observations. We set p through the
rule that the p ﬁrst directions must carry at least 98% of the total variance of Cn , that is p is the ﬁrst
i=1 gi > 0.98 ·Pn
integer such that Pp
i=1 gi . We ﬁx the ν paramater of the ocSVM to 0.1. The BT and
MAC functionals additionally require the use of a regularization term ǫn which we select by ﬁnding
the best ridge regressor of φt+1 given φt through a 4-fold cross validation procedure on the training
set. For βBT , ˜βBT , βmac and the kPCA functional ep+1 we use their respective empirical mean µ
and variance σ on the training set to rescale and whiten their output on the test set, namely consider
values (f (z ) − µ)/σ . Although more elaborate anomaly detection schemes on such unidimensional
time-series might be considered, for the sake of simplicity we treat directly these raw outputs as
alarm scores.

Having on the one hand the correct labels for anomalies and the scores for all detectors, we vary
the threshold at which an alarm is raised to produce ROC curves. We use the area under the curve
of each method on each sample path as a performance measure for that path. Figure 1 provides
a summary of the performance of each method on a unique sample path of 200 observations and
10 anomalies. Perturbation parameters are set such that σε = 0.01 and σδ varies between 0.005
and 0.055. For each couple (σε , σδ ) we generate 500 draws and compute the mean AUC of each
technique on such draws. We report in Figure 2 these averaged performances for three different
choices of the kernel, namely three different values for ρ as de ﬁned in Equation (6).

6.4 Discussion

In the experimental setting, anomalies can be characterized as unusual increments between two
successive states of an otherwise smooth dynamical system. Anomalies are unusual due to their
size, controlled by σδ , and their directions, sampled in {−1, 1}4. When the step σδ is relatively
small, it is difﬁcult to ﬂag correctly an anomaly without tak
ing into account the system’s dynamic
as illustrated by the relatively poor performance of the ocSVM and the kPCA compared to the
BT, BTres and MAC functions. On the contrary, when σδ is big, anomalies can be more simply
discriminated as big gaps. The methods we propose do not perform as well as the ocSVM in such a
setting. We can hypothesize two reasons for this: ﬁrst, whit e functionals may be less useful in such
a regime that puts little emphasis on dynamics than a simple ocSVM with adequate kernel. Second,
in this study the BT and MAC functions ﬂag anomalies whenever an evaluation goes outside of a
certain bounding tube. More advanced detectors of a deviation or change from normality, such as
CUSUM [21], might be studied in future work.

8

References

[1] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Computing Surveys, 2009.
[2] B. Sch ¨olkopf, A. Smola, and K. M ¨uller. Nonlinear component analysis as a kernel eigenvalue problem.
Neural Comput., 10(5):1299–1319, 1998.
[3] B. Sch ¨olkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of
a high-dimensional distribution. Neural Comput., 13:2001, 1999.
[4] A.B. Gardner, A.M. Krieger, G. Vachtsevanos, and B. Litt. One-class novelty detection for seizure analysis
from intracranial EEG. J. Mach. Learn. Res., 7:1025–1044, 2006.
[5] H. Hoffmann. Kernel PCA for novelty detection. Pattern Recognit., 40(3):863–874, 2007.
[6] A. J. Fox. Outliers in time series. J. R. Stat. Soc. Ser. B, 34(3):350–363, 1972.
[7] R.S. Tsay, D. Pena, and A.E. Pankratz. Outliers in multivariate time series. Biometrika, 87(4):789–804,
2000.
[8] A. Laukaitis and A. Ra ˇckauskas. Testing changes in Hilb ert space autoregressive models. Lithuanian
Mathematical Journal, 42(4):343–354, 2002.
[9] G. S. Maddala and I. M. Kim. Unit roots, cointegration, and structural change. Cambridge Univ. Pr.,
1998.
[10] P. Switzer and A.A. Green. Min/max autocorrelation factors for multivariate spatial imagery. Computer
science and statistics, 16:13–16, 1985.
[11] G. Box and G. C. Tiao. A canonical analysis of multiple time series. Biometrika, 64(2):355–365, 1977.
[12] K. Fukumizu, F. R. Bach, and A. Gretton. Statistical consistency of kernel canonical correlation analysis.
J. Mach. Learn. Res., 8:361–383, 2007.
[13] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.
Kluwer Academic Publishers, 2003.
[14] D. Bosq. Linear Processes in Function Spaces: Theory and Applications. Springer, 2000.
[15] A. Mas. Asymptotic normality for the empirical estimator of the autocorrelation operator of an ARH (1)
process. Compt. Rendus Acad. Sci. Math., 329(10):899–902, 1999.
[16] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[17] L. Zwald and G. Blanchard. Finite dimensional projection for classi ﬁcation and statistical learning.
Trans. Inform. Theory, 54:4169, 2008.
[18] J. H. Stock and M. W. Watson. Testing for common trends. J. Am. Stat. Assoc., pages 1097–1107, 1988.
[19] P. Bossaerts. Common nonstationary components of asset prices. J. Econ. Dynam. Contr., 12(2-3):347–
364, 1988.
[20] J. A. Vano, J. C. Wildenberg, M. B. Anderson, J. K. Noel, and J. C. Sprott. Chaos in low-dimensional
lotka-volterra models of competition. Nonlinearity, 19(10):2391–2404, 2006.
[21] M. Basseville and I.V Nikiforov. Detection of abrupt changes: theory and applications. Prentice-Hall,
1993.

IEEE

9


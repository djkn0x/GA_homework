Adaptive Execution with Online Price Impact Learning
Beomsoo Park
Electrical Engineering, Stanford University
beomsoo@stanford.edu
December 11, 2009

1.
Introduction
Buying or selling a large block of security is often followed by unfavorable movement of price which is called
price impact. One reason for the impact is that the block execution causes abrupt imbalance between supply
and demand and the other is that it might convey to other investors information about fundamental value of
the security that will be reﬂected on their future investment decisions. Thus, when submitting large orders,
it is important to take into account the price impact in order to minimize the amount of value lost by it.
We propose an eﬃcient multi-period execution algorithm to minimize transaction cost incurred by the
price impact when buying or selling a block order. We assume that we have incomplete knowledge about
the price impact which is indeed the case in practice. A good execution algorithm strikes a balance between
“exploration” and “exploitation.” That is, on one hand, it needs to learn unknown characteristics of the
price impact for better future trading decisions in exchange of losing optimality at present. On the other
hand, it should make the best of current knowledge about the price impact for making an optimal decision
that is not necessarily eﬀective for identifying remaining uncertainty about the impact.
More precisely, this problem can be formulated as a dynamic program that has random variables rep-
resenting uncertainty on the price impact. But it is quite challenging to solve the corresponding Bellman
equation due mainly to need for incorporating probability distributions on the random variables into a state
space. Also, diﬃculty comes from the fact that both state and action spaces are continuous and that in most
practical cases a trading horizon is ﬁnite, say a few days or a week. Therefore, we seek to propose a rea-
sonably good, simple heuristic strategy that captures a good balance between exploration and exploitation
and compare its performance with that of a naive baseline strategy to be deﬁned later and an upper bound
derived through information relaxation. To this end, we propose a linearized least squares with regularization
that is a modiﬁed version of least squares with regularization dealing eﬃciently with nonlinear relationship
between observations and model parameters.

2. Problem Formulation
Consinder a trader who wants to liquidate a large long position or to recover a large short position of a
stock over a ﬁnite time horizon T . Let xt denote the size of his ex-trade position at period t with an initial
position x0 such that a positive value implies a long position. He requires that his ﬁnal position xT be zero.
"
#
TX
(cid:9)(cid:12)(cid:12) x0
(cid:8)
He is assumed to be risk averse such that he seeks to maximize the objective function of the form
t=1
where ∆pt ≡ pt − pt−1 is deﬁned as the increment of a per-share transaction price, which will be deﬁned
later. The ﬁrst term in the sum represents the change in book value that can be viewed as a per-stage

∆ptxt−1 − ρσx2
t−1

E

1

0 ≤ αi ≤ 1, γi > 0 ∀i

pt = ˜pt + 1> yt , yt = Ayt−1 + γut ∈ Rn , A =

revenue. The second term reﬂects a holding cost, with ρσ expressing the extent to which the trader would
execute sooner rather than later. Note that the risk aversion coeﬃcient is proportional to the volatility of
the stock price σ .
We model a natural price movement, deﬁned as evolution over time of the price without the trader’s
transactions, as a Gaussian random walk, i.e. ˜pt = ˜pt−1 + t , t ∼ N (0, σ2
 ). In practice, due to presence
of the price impact is the trader faced with a less favorable per-share transaction price at which he sells or
buys a share. In order to capture this unfavorable movement of the price, we propose the following price
 α1
 ,
 γ1
 , γ =
impact model with n “time constants”.
· · ·
0
...
...
...
. . .
· · · αn
0
γn
where ut represents the size of market order submitted by the trader at period t and the pair (αi , γi )
characterizes one market impact component. Note that the per-share transaction price pt is a function of
ut . For αi = 0, trading ut aﬀects the transaction price only at stage t and we call this impact an immediate
impact. For 0 < αi < 1, the excitation triggered by ut dies away as time goes by and we call this impact a
temporary impact. For αi = 1, the eﬀect of ut on the transaction price persists and we call this impact a
permanent impact. In this paper, we consider the case in which the coeﬃcients αi ’s and γi ’s are unknown to
the trader. Instead, we assume that he has (usually inaccurate) estimates for the parameters through data
analysis on historic transaction records.
"
#
(cid:9) (cid:12)(cid:12)(cid:12)x0
TX
(cid:8)
To sum up, the trader solves the following ﬁnite horizon control problem:
∆ptxt−1 − ρσx2
E
t−1
t=1
˜pt = ˜pt−1 + t , t ∼ N (0, σ2
pt = ˜pt + 1> yt ,
 ),
yt = Ayt−1 + γut , y0 = ~0 ∈ Rn ,
xt = xt−1 + ut , xT = 0.
Pt
where the policy π = (π1 , π2 , . . .) is a collection of functions πt each of which maps history up to time t to a
j=1 αt−j
t =
i uj and z i0 = 0 for all i. Then, it is easy
trading decision ut . For notational simplicity, deﬁne z i
t−1 + ut and y i
t = γi z i
t for all i.
to see that z i
t satisﬁes the recursion z i
t = αi z i

max
π
subject to

3. Analysis
3.1. Bellman Equation with Augmented State
Let Ht be history up to period t. From Bayesian perspective, together with some prior distributions on α
(cid:12)(cid:12)(cid:12)Ht
h
i
and γ , Bellman equation for this problem can be written as
t−1 + Vt+1 (xt , Ht+1 )
∆ptxt−1 − ρσx2
Vt (xt−1 , Ht ) = max
ut
But it is quite challenging to get a closed-form solution for Vt . Therefore, we aim to ﬁnd an approximate
solution via policy parameterization. One might want to think of value function approximation as an
alternative solving methodology.
In this particular problem, we prefer the former to the latter by the
following reasons: ﬁrst, a class of well-structured policies are readily available from analysis of clairvoyant
case which will be done in the subsequent section. Moreover, it is impossible to simulate price impact without
actually trading and to get samples.1 Finally, we need to learn the model parameters in an online fashion
over a relatively short time horizon and thus we should take advantage of special structural properties of
this execution model that the clairvoyant policy can capture eﬀectively.
1 Some people call this Heisenberg Uncertainty Principle of Price Impact

E

.

2

3.2. Policy Parameterization
In order to derive an upper bound for the trader’s proﬁt and a class of policies parameterized by α and γ ,
consider the case in which both α and γ are known. We seek to ﬁnd reward-to-go functions Vt (xt−1 , zt−1 ; α, γ )
(cid:8)
(cid:9)
that satisfy the following Bellman equation.
γ> (1ut − (I − A)zt−1 )xt−1 − ρσx2
t−1 + Vt+1 (xt−1 + ut , Azt−1 + 1ut ; α, γ )
Vt (xt−1 , zt−1 ; α, γ ) = max
ut∈R
with the terminal condition VT (xT −1 , zT −1 ; α, γ ) = −(γ>1 + ρσ )x2
T −1 − γ> (I − A)zT −1xT −1 . It is natural to
conjecture that Vt (xt−1 , zt−1 ; α, γ ) is quadratic in the two arguments xt−1 and zt−1 , that is, Vt (xt−1 , zt−1 ; α, γ ) =
t−1Dt zt−1 . In order to determine the coeﬃcients Bt ∈ R, Ct ∈ Rn , and Dt ∈ Rn×n ,
t xt−1 zt−1 + z>
t−1 + C >
Btx2
n
we need to solve the following equation for them.
o
γ> (1ut − (I − A)zt−1 )xt−1 − ρσx2
t−1 + C >
t zt−1xt−1 + z>
Btx2
t−1Dt zt−1 = max
t−1
ut
+ Bt+1 (xt−1 + ut )2 + C >
t+1 (Azt−1 + 1ut )(xt−1 + ut ) + (Azt−1 + 1ut )>Dt+1 (Azt−1 + 1ut )
The second-order optimality condition is Bt+1 + C >
t+11 + 1>Dt+11 < 0. Comparing the coeﬃcients on both
sides, we obtain the following set of three recursive equations:
Bt = Bt+1 − ρσ − (2Bt+1 + C >
t+11 + γ>1)2
t+11 + 1>Dt+11) ,
4(Bt+1 + C >
t+1 + 2(1>Dt+1 ))A
t+11 + γ>1)(C >
t+1A − γ> (I − A) − (2Bt+1 + C >
(cid:18)
(cid:19)
t = C >
C >
2(Bt+1 + C >
t+11 + 1>Dt+11)
t+1 + 2(1>Dt+1 ))
Dt+1 − (Ct+1 + 2(Dt+11))(C >
Dt = A>
4(Bt+1 + C >
t+11 + 1>Dt+11)
with terminal condition BT = −(γ>1 + ρσ ), C >
T = −γ> (I − A) and DT = ~0. Note that each Bt , Ct and
Dt are functions of α and γ . The upper bound for the proﬁt is given by V1 (x0 , z0 ; α, γ ).

A.

,

3.3. Parameter Estimation
Two primary diﬃculties are present for estimating the price impact parameters α and γ . One is the absence
of oﬄine training examples and the other is highly nonlinear, nonconvex relationship between observations
and the model parameters. Especially, due to the second reason we cannot directly apply ordinary least
squares into the estimation problem of interest. In light of this, we propose linearized least squares with
regularization to address the technical diﬃculties. Let us highlight two important features of this algorithm.
One is that this algorithm converts the nonlinear observation process into a linear one using iterates in the
previous stage so that it preserves eﬃciency of ordinary least squares. The other is that it deals eﬀectively
with the constraints for α and γ , i.e. 0 ≤ α ≤ 1 and γ > 0, using regularization while the solution of
ordinary least squares usually violates the range constraints. Algorithm 1 gives the details of the linearized
least squares with regularization.
Let us ﬁnish this section with comparison of Algorithm 1 with two possible alternatives: extended Kalman
ﬁltering and nonlinear least squares. The extended Kalman ﬁltering suﬀers from the “out-of-range” problem
related to the range constraints for α and γ and requires a strong probabilistic assumption that the prior
distributions on α and γ be Gaussian. Meanwhile, nonlinear least squares can avoid these problems but it
turns out that it converges much more slowly than Algorithm 1.

4. Numerical Experimentation
For numerical experimentation, we use the following setting for the model parameters.

3

T

• T = 100 for 1 trading day (6.5 hours), the lentgh of one trading interval ≈ 4 mins
√
• Daily volatility = $0.25 (two ticks), i.e. σ = $0.25/
• Risk aversion coeﬃcient: ρ = 5 × 10−5
• Initial position x0 = 100, 000 shares
• True parameter values: α∗ = (0.259, 0.509, 0.713, 0.874), γ ∗ = (3, 4, 5, 5) × 10−5
• Initial estimates: α(0) = (0.406, 0.763, 0.874, 0.973), γ (0) = (6, 7, 8, 10) × 10−5
The choice of (α∗ , γ ∗ ) and (α(0) , γ (0) ) represents the case where today’s market is more liquid than we
expected yesterday. For the purpose of comparison, we deﬁne a naive baseline policy as the one using the
same initial estimates for α and γ over the entire time horizon, i.e. no adaptation for α and γ . It is based
on the hypothetical notion that daily change of α and γ is so small that it can be ignored.
We evaluate performances of our adaptive execution policy(AE) and the baseline policy(BL) by two
performance measures: percentage performance loss relative to the clairvoyant case(CL) and percentage
performance gain relative to the baseline policy. We carry out 40 simulation runs for averaging. The
following summerizes the results.
• Average performance percentage loss of BL relative to CL: −11.4%
• Average performance percentage loss of AE relative to CL: −0.62%
• Average performance percentage gain of AE over BL: 9.68%
• Percentage sample deviation from the CL value function = −0.24%

Figure 1: Evolution over time of estimates for α (above) and γ (below)

Figure 2: Evolution over time of size of order: (i) clairvoyant case(left), (ii) adaptive execution(middle), (iii)
baseline policy (right)

4

05010000.51estimate of a10501000.40.60.8estimate of a20501000.51estimate of a30501000.80.91estimate of a405010000.51x 10−4estimate of g105010000.51x 10−4estimate of g205010000.51x 10−4estimate of g30501000.511.5x 10−4estimate of g4050100−8000−6000−4000−20000CL Trades over timet050100−8000−6000−4000−20000AE Trades over timetsize of sell order050100−8000−6000−4000−20000BL Trades over timetsize of sell order5. Conclusion
We quantify contribution of the adaptive execution policy adopting online learning of the unknown price
impact coeﬃcients relative to the naive baseline policy with no adaptation and we show that the performance
gain from adaptation is signiﬁcant. This execution model ﬁts for intraday execution in the situation where
price impact remains unchanged within a single trading day but changes daily. One might want to argue the
presence of intraday change of price impact but it is quite challenging to track the time-varying aspect because
of lack of data set for training. Hopefully, the price impact model adopted here can capture practical intraday
price impact pattern reasonably well on average and our adaptive execution policy can make a contribution
to reduce transaction costs incurred by price impact. Future work includes (i) theoretical justiﬁcation for
choice of regularization coeﬃcient and (ii) exploration for a better estimate over an uncertainty ellipsoid
centered at the linearized least squares estimate that has been studied in multi-armed bandit literature.

References
1. D. Bertsimas and A. W. Lo, 1998. Optimal control of execution costs. Journal of Financial Markets
2. A. Obizhaeva and J. Wang, 2005. Optimal trading strategy and supply/demand dynamics. Revised
and resubmitted. Journal of Financial Markets
3. C. C. Moallemi, B. Park, and B. Van Roy, 2009. Strategic execution in the presence of an uninformed
arbitrageur. submitted for publication.
4. D. Bertsekas, 2007. Dynamic programming and optimal control.
5. R. Sutton and A. Barto, 1998. Reinforcement learning: an introduction.

Algorithm 1 Linearized Least Squares with Regularization
Initialize x0 , y0 , z0 , α(0) , γ (0)
// Wait until coeﬃcient matrices for OLS are ful l-rank
for t = 1 : n − 1 do
Choose greedy ut w.r.t. Vt (xt−1 , zt−1 ; α(t−1) , γ (t−1) )
xt := xt−1 + ut ; zt := A(t−1) zt−1 + 1ut ; α(t) := α(t−1) ; γ (t) := γ (t−1)
end for
// Once the matrices are ful l-rank, perform ordinary least squares with regularization on the linearized model
for t = n : T do
Initialize α(t) := α(t−1) , γ (t) := γ (t−1)
 y>
, hα
 ∆p1 + 1> (y0 − γu1 )

while α(t) and γ (t) do not converge yet do
Compute y1 , . . . , yt−1 using α(t) , γ (t) .
1
...
...
t :=
t :=
Gα
∆pt + 1> (yt−1 − γut )
y>
, hγ
 u1 1> − z>
" ∆p1
#
t ||α − α(t−1) ||2
α(t) := argminα ||Gα
t α − hα
t ||2
t
2 + λα
2
Compute z1 , . . . , zt−1 using α(t) .
0 (I − A)
...
...
t :=
t :=
Gγ
ut 1> − z>
t−1 (I − A)
∆pt
γ (t) := argminγ ||Gγ
t ||γ − γ (t−1) ||2
t ||2
t γ − hγ
2 + λγ
2
end while
Choose greedy ut+1 w.r.t. Vt (xt−1 , zt−1 ; α(t) , γ (t) ); xt+1 := xt + ut+1 ; zt+1 := A(t) zt + 1ut+1
end for

5


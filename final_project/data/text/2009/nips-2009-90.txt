Multi-Label Prediction via Compressed Sensing

Daniel Hsu
UC San Diego
djhsu@cs.ucsd.edu

Sham M. Kakade
TTI-Chicago
sham@tti-c.org

John Langford
Yahoo! Research
jl@hunch.net

Tong Zhang
Rutgers University
tongz@rci.rutgers.edu

Abstract

We consider multi-label prediction problems with large output spaces under the
assumption of output sparsity – that the target (label) vectors have small support.
We develop a general theory for a variant of the popular error correcting output
code scheme, using ideas from compressed sensing for exploiting this sparsity.
The method can be regarded as a simple reduction from multi-label regression
problems to binary regression problems. We show that the number of subprob-
lems need only be logarithmic in the total number of possible labels, making this
approach radically more efﬁcient than others. We also state and prove robustness
guarantees for this method in the form of regret transform bounds (in general),
and also provide a more detailed analysis for the linear prediction setting.

1

Introduction

Suppose we have a large database of images, and we want to learn to predict who or what is in any
given one. A standard approach to this task is to collect a sample of these images x along with
corresponding labels y = (y1 , . . . , yd ) ∈ {0, 1}d , where yi = 1 if and only if person or object i
is depicted in image x, and then feed the labeled sample to a multi-label learning algorithm. Here,
d is the total number of entities depicted in the entire database. When d is very large (e.g. 103 ,
104 ), the simple one-against-all approach of learning a single predictor for each entity can become
prohibitively expensive, both at training and testing time.

Our motivation for the present work comes from the observation that although the output (label)
space may be very high dimensional, the actual labels are often sparse. That is, in each image, only
a small number of entities may be present and there may only be a small amount of ambiguity in
who or what they are. In this work, we consider how this sparsity in the output space, or output
sparsity, eases the burden of large-scale multi-label learning.
Exploiting output sparsity. A subtle but critical point that distinguishes output sparsity from more
common notions of sparsity (say, in feature or weight vectors) is that we are interested in the sparsity
of E[y |x] rather than y . In general, E[y |x] may be sparse while the actual outcome y may not (e.g. if
there is much unbiased noise); and, vice versa, y may be sparse with probability one but E[y |x] may
have large support (e.g. if there is little distinction between several labels).

Conventional linear algebra suggests that we must predict d parameters in order to ﬁnd the value of
the d-dimensional vector E[y |x] for each x. A crucial observation – central to the area of compressed
sensing [1] – is that methods exist to recover E[y |x] from just O(k log d) measurements when E[y |x]
is k-sparse. This is the basis of our approach.

1

Our contributions. We show how to apply algorithms for compressed sensing to the output coding
approach [2]. At a high level, the output coding approach creates a collection of subproblems of
the form “Is the label in this subset or its complement?”, sol ves these problems, and then uses their
solution to predict the ﬁnal label.

The role of compressed sensing in our application is distinct from its more conventional uses in data
compression. Although we do employ a sensing matrix to compress training data, we ultimately
are not interested in recovering data explicitly compressed this way. Rather, we learn to predict
compressed label vectors, and then use sparse reconstruction algorithms to recover uncompressed
labels from these predictions. Thus we are interested in reconstruction accuracy of predictions,
averaged over the data distribution.

The main contributions of this work are:

1. A formal application of compressed sensing to prediction problems with output sparsity.

2. An efﬁcient output coding method, in which the number of re quired predictions is only
logarithmic in the number of labels d, making it applicable to very large-scale problems.

3. Robustness guarantees, in the form of regret transform bounds (in general) and a further
detailed analysis for the linear prediction setting.

Prior work. The ubiquity of multi-label prediction problems in domains ranging from multiple ob-
ject recognition in computer vision to automatic keyword tagging for content databases has spurred
the development of numerous general methods for the task. Perhaps the most straightforward ap-
proach is the well-known one-against-all reduction [3], but this can be too expensive when the num-
ber of possible labels is large (especially if applied to the power set of the label space [4]). When
structure can be imposed on the label space (e.g. class hierarchy), efﬁcient learning and prediction
methods are often possible [5, 6, 7, 8, 9]. Here, we focus on a different type of structure, namely
output sparsity, which is not addressed in previous work. Moreover, our method is general enough to
take advantage of structured notions of sparsity (e.g. group sparsity) when available [10]. Recently,
heuristics have been proposed for discovering structure in large output spaces that empirically offer
some degree of efﬁciency [11].

As previously mentioned, our work is most closely related to the class of output coding method
for multi-class prediction, which was ﬁrst introduced and s hown to be useful experimentally in [2].
Relative to this work, we expand the scope of the approach to multi-label prediction and provide
bounds on regret and error which guide the design of codes. The loss based decoding approach [12]
suggests decoding so as to minimize loss. However, it does not provide signi ﬁcant guidance in the
choice of encoding method, or the feedback between encoding and decoding which we analyze here.

The output coding approach is inconsistent when classi ﬁers are used and the underlying problems
being encoded are noisy. This is proved and analyzed in [13], where it is also shown that using a
Hadamard code creates a robust consistent predictor when reduced to binary regression. Compared
to this method, our approach achieves the same robustness guarantees up to a constant factor, but
requires training and evaluating exponentially (in d) fewer predictors.
Our algorithms rely on several methods from compressed sensing, which we detail where used.

2 Preliminaries

Let X be an arbitrary input space and Y ⊂ Rd be a d-dimensional output (label) space. We assume
the data source is deﬁned by a ﬁxed but unknown distribution o
ver X × Y . Our goal is to learn a
predictor F : X → Y with low expected ℓ2
2 -error Ex kF (x) − E[y |x]k2
2 (the sum of mean-squared-
errors over all labels) using a set of n training data {(xi , yi )}n
i=1 .
We focus on the regime in which the output space is very high-dimensional (d very large), but for
any given x ∈ X , the expected value E[y |x] of the corresponding label y ∈ Y has only a few
non-zero entries. A vector is k-sparse if it has at most k non-zero entries.

2

3 Learning and Prediction

3.1 Learning to Predict Compressed Labels

Let A : Rd → Rm be a linear compression function, where m ≤ d (but hopefully m ≪ d). We use
A to compress (i.e. reduce the dimension of) the labels Y , and learn a predictor H : X → A(Y ) of
these compressed labels. Since A is linear, we simply represent A ∈ Rm×d as a matrix.
Speci ﬁcally, given a sample {(xi , yi )}n
i=1 , we form a compressed sample {(xi , Ayi )}n
i=1 and then
2 .
2 -error Ex kH (x) − E[Ay |x]k2
learn a predictor H of E[Ay |x] with the objective of minimizing the ℓ2
3.2 Predicting Sparse Labels

To obtain a predictor F of E[y |x], we compose the predictor H of E[Ay |x] (learned using the com-
pressed sample) with a reconstruction algorithm R : Rm → Rd . The algorithm R maps predictions
of compressed labels h ∈ Rm to predictions of labels y ∈ Y in the original output space. These
algorithms typically aim to ﬁnd a sparse vector y such that Ay closely approximates h.
Recent developments in the area of compressed sensing have produced a spate of reconstruction
algorithms with strong performance guarantees when the compression function A satis ﬁes certain
properties. We abstract out the relevant aspects of these guarantees in the following deﬁnition.
Deﬁnition. An algorithm R is a valid reconstruction algorithm for a family of compression functions
(Ak ⊂ Sm≥1
Rm×d : k ∈ N) and sparsity error sperr : N × Rd → R, if there exists a function
f : N → N and constants C1 , C2 ∈ R such that: on input k ∈ N, A ∈ Ak with m rows, and
h ∈ Rm , the algorithm R(k , A, h) returns an f (k)-sparse vector by satisfying
kby − yk2
2 ≤ C1 · kh − Ayk2
2 + C2 · sperr(k , y)
for all y ∈ Rd . The function f is the output sparsity of R and the constants C1 and C2 are the regret
factors.
Informally, if the predicted compressed label H (x) is close to E[Ay |x] = AE[y |x], then the sparse
vector by returned by the reconstruction algorithm should be close to E[y |x]; this latter distance
2 should degrade gracefully in terms of the accuracy of H (x) and the sparsity of E[y |x].
kby − E[y |x]k2
Moreover, the algorithm should be agnostic about the sparsity of E[y |x] (and thus the sparsity error
sperr(k , E[y |x])), as well as the “measurement noise” (the prediction error
kH (x) − E[Ay |x]k2 ).
This is a subtle condition and precludes certain reconstruction algorithm (e.g. Basis Pursuit [14])
that require the user to supply a bound on the measurement noise. However, the condition is needed
in our application, as such bounds on the prediction error (for each x) are not generally known
beforehand.

We make a few additional remarks on the deﬁnition.

1. The minimum number of rows of matrices A ∈ Ak may in general depend on k (as well as
the ambient dimension d). In the next section, we show how to construct such A with close
to the optimal number of rows.
2. The sparsity error sperr(k , y) should measure how poorly y ∈ Rd is approximated by a
k-sparse vector.
3. A reasonable output sparsity f (k) for sparsity level k should not be much more than k ,
e.g. f (k) = O(k).

Concrete examples of valid reconstruction algorithms (along with the associated Ak , sperr, etc.) are
given in the next section.

4 Algorithms

Our prescribed recipe is summarized in Algorithms 1 and 2. We give some examples of compression
functions and reconstruction algorithms in the following subsections.

3

Algorithm 1 Training algorithm
parameters sparsity level k , compression
function A ∈ Ak with m rows, regression
learning algorithm L
input training data S ⊂ X × Rd
for i = 1, . . . , m do
hi ← L({(x, (Ay)i ) : (x, y) ∈ S })
end for
output regressors H = [h1 , . . . , hm ]

Algorithm 2 Prediction algorithm
parameters sparsity level k , compression
function A ∈ Ak with m rows, valid re-
construction algorithm R for Ak
input regressors H = [h1 , . . . , hm ],
test
point x ∈ X
output by = ~R(k , A, [h1 (x), . . . , hm (x)])
Figure 1: Training and prediction algorithms.

4.1 Compression Functions

Several valid reconstruction algorithms are known for compression matrices that satisfy a restricted
isometry property.
Deﬁnition. A matrix A ∈ Rm×d satis ﬁes the (k , δ)-restricted isometry property ((k , δ)-RIP), δ ∈
2 for all k-sparse x ∈ Rd .
(0, 1), if (1 − δ)kxk2
2 ≤ (1 + δ)kxk2
2 ≤ kAxk2
While some explicit constructions of (k , δ)-RIP matrices are known (e.g. [15]), the best guarantees
are obtained when the matrix is chosen randomly from an appropriate distribution, such as one of
the following [16, 17].

• All entries i.i.d. Gaussian N (0, 1/m), with m = O(k log(d/k)).
• All entries i.i.d. Bernoulli B (1/2) over {±1/√m}, with m = O(k log(d/k)).
• m randomly chosen rows of the d × d Hadamard matrix over {±1/√m}, with m =
O(k log5 d).

The hidden constants in the big-O notation depend inversely on δ and the probability of failure.
A striking feature of these constructions is the very mild dependence of m on the ambient dimension
d. This translates to a signi ﬁcant savings in the number of lea rning problems one has to solve after
employing our reduction.
Some reconstruction algorithms require a stronger guarantee of bounded coherence µ(A) ≤
O(1/k), where µ(A) deﬁned as
1≤i<j≤d |(A⊤A)i,j |/q|(A⊤A)i,i ||(A⊤A)j,j |
µ(A) = max
It is easy to check that the Gaussian, Bernoulli, and Hadamard-based random matrices given
above have coherence bounded by O(p(log d)/m) with high probability. Thus, one can take
m = O(k2 log d) to guarantee 1/k coherence. This is a factor k worse than what was needed
for (k , δ)-RIP, but the dependence on d is still small.
4.2 Reconstruction Algorithms

In this section, we give some examples of valid reconstruction algorithms. Each of these algorithm
is valid with respect to the sparsity error given by

1
k ky − y(1:k) k2
sperr(k , y) = ky − y(1:k)k2
2 +
1
where y(1:k) is the best k-sparse approximation of y (i.e. the vector with just the k largest (in mag-
nitude) coefﬁcients of y ).
The following theorem relates reconstruction quality to approximate sparse regression, giving a
sufﬁcient condition for any algorithm to be valid for RIP mat rices.

4

Algorithm 3 Prediction algorithm with R = OMP
parameters sparsity level k , compression function A = [a1 | . . . |ad ] ∈ Ak with m rows,
input regressors H = [h1 , . . . , hm ], test point x ∈ X
(predict compressed label vector)
h ← [h1 (x), . . . , hm (x)]⊤
by ← ~0, J ← ∅, r ← h
for i = 1, . . . , 2k do
(column of A most correlated with residual r)
j∗ ← arg maxj |r⊤aj |/kaj k2
(add j∗ to set of selected columns)
J ← J ∪ {j∗ }
byJ ← (AJ )†h, byJ c ← ~0
(least-squares restricted to columns in J )
(update residual)
r ← h − Aby
end for
output by
Figure 2: Prediction algorithm specialized with Orthogonal Matching Pursuit.
Theorem 1. Let Ak = {(k + f (k), δ)-RIP matrices} for some function f : N → N, and let A ∈ Ak
have m rows. If for any h ∈ Rm , a reconstruction algorithm R returns an f (k)-sparse solution
by = R(k , A, h) satisfying
kAby − hk2
C kAy(1:k) − hk2
2 ≤ inf
2 ,
y∈Rd
then it is a valid reconstruction algorithm for Ak and sperr given above, with output sparsity f and
regret factors C1 = 2(1 + √C )2 /(1 − δ) and C2 = 4(1 + (1 + √C )/(1 − δ))2 .
Proofs are deferred to Appendix B.
Iterative and greedy algorithms. Orthogonal Matching Pursuit (OMP) [18], FoBa [19], and
CoSaMP [20] are examples of iterative or greedy reconstruction algorithms. OMP is a greedy
forward selection method that repeatedly selects a new column of A to use in ﬁtting h (see Al-
gorithm 3). FoBa is similar, except it also incorporates backward steps to un-select columns that are
later discovered to be unnecessary. CoSaMP is also similar to OMP, but instead selects larger sets
of columns in each iteration.

FoBa and CoSaMP are valid reconstruction algorithms for RIP matrices ((8k , 0.1)-RIP and
(4k , 0.1)-RIP, respectively) and have linear output sparsity (8k and 2k). These guarantees are ap-
parent from the cited references. For OMP, we give the following guarantee.
Theorem 2. If µ(A) ≤ 0.1/k , then after f (k) = 2k steps of OMP, the algorithm returns by satisfying
kAby − hk2
2 ≤ 23kAy(1:k) − hk2
2 ∀y ∈ Rd .
This theorem, combined with Theorem 1, implies that OMP is valid for matrices A with µ(A) ≤
0.1/k and has output sparsity f (k) = 2k .
ℓ1 algorithms. Basis Pursuit (BP) [14] and its variants are based on ﬁnding t he minimum ℓ1 -norm
solution to a linear system. While the basic form of BP is ill-suited for our application (it requires
the user to supply the amount of measurement error kAy − hk2 ), its more advanced path-following
or multi-stage variants may be valid [21].

5 Analysis

5.1 General Robustness Guarantees

We now state our main regret transform bound, which follows immediately from the deﬁnition of a
valid reconstruction algorithm and linearity of expectation.
Theorem 3 (Regret Transform). Let R be a valid reconstruction algorithm for {Ak : k ∈ N} and
sperr : N × Rd → R. Then there exists some constants C1 and C2 such that the following holds.

5

Pick any k ∈ N, A ∈ Ak with m rows, and H : X → Rm . Let F : X → Rd be the composition of
R(k , A, ·) and H , i.e. F (x) = R(k , A, H (x)). Then
ExkF (x) − E[y |x]k2
2 ≤ C1 · ExkH (x) − E[Ay |x]k2
2 + C2 · sperr(k , E[y |x]).
The simplicity of this theorem is a consequence of the careful composition of the learned predictors
with the reconstruction algorithm meeting the formal speci ﬁcations described above.

.

In order compare this regret bound with the bounds afforded by Sensitive Error Correcting Output
Codes (SECOC) [13], we need to relate ExkH (x) − E[Ay |x]k2
2 to the average scaled mean-squared-
error over all induced regression problems; the error is scaled by the maximum difference Li =
maxy∈Y (Ay)i − miny (Ay)i between induced labels:
Ex (cid:18) H (x)i − E[(Ay)i |x]
(cid:19)2
mXi=1
1
¯r =
m
Li
In k-sparse multi-label problems, we have Y = {y ∈ {0, 1}d : kyk0 ≤ k}. In these terms, SECOC
can be tuned to yield ExkF (x) − E[y |x]k2
2 ≤ 4k2 · ¯r for general k .
For now, ignore the sparsity error. For simplicity, let A ∈ Rm×d with entries chosen i.i.d. from the
Bernoulli B (1/2) distribution over {±1/√m}, where m = O(k log d). Then for any k-sparse y ,
we have kAyk∞ ≤ k/√m, and thus Li ≤ 2k/√m for each i. This gives the bound
C1 · Ex kH (x) − E[Ay |x]k2
2 ≤ 4C1 · k2 · ¯r,
which is within a constant factor of the guarantee afforded by SECOC. Note that our reduction
induces exponentially (in d) fewer subproblems than SECOC.
Now we consider the sparsity error.
In the extreme case m = d, E[y |x] is allowed to be fully
dense (k = d) and sperr(k , E[y |x]) = 0. When m = O(k log d) < d, we potentially incur an
extra penalty in sperr(k , E[y |x]), which relates how far E[y |x] is from being k-sparse. For example,
suppose E[y |x] has small ℓp norm for 0 ≤ p < 2. Then even if E[y |x] has full support, the penalty
will decrease polynomially in k ≈ m/ log d.
5.2 Linear Prediction

A danger of using generic reductions is that one might create a problem instance that is even harder
to solve than the original problem. This is an oft cited issue with using output codes for multi-
class problems. In the case of linear prediction, however, the danger is mitigated, as we now show.
Suppose, for instance, there is a perfect linear predictor of E[y |x], i.e. E[y |x] = B⊤x for some
B ∈ Rp×d (here X = Rp ). Then it is easy to see that H = BA⊤ is a perfect linear predictor of
E[Ay |x]:
H ⊤x = AB⊤x = AE[y |x] = E[Ay |x].
The following theorem generalizes this observation to imperfect linear predictors for certain well-
behaved A.
Theorem 4. Suppose X ⊂ Rp . Let B ∈ Rp×d be a linear function with
Ex (cid:13)(cid:13)B⊤x − E[y |x](cid:13)(cid:13)2
2 = ǫ.
Let A ∈ Rm×d have entries drawn i.i.d. from N (0, 1/m), and let H = BA⊤ . Then with high
probability (over the choice of A),
2 ≤ (cid:0)1 + O(1/√m)(cid:1) ǫ.
ExkH ⊤x − AE[y |x]k2
Remark 5. Similar guarantees can be proven for the Bernoulli-based matrices. Note that d does not
appear in the bound, which is in contrast to the expected spectral norm of A: roughly 1+O(pd/m).
Theorem 4 implies that the errors of any linear predictor are not magni ﬁed much by the compres-
sion function. So a good linear predictor for the original problem implies an almost-as-good linear
predictor for the induced problem. Using this theorem together with known results about linear
prediction [22], it is straightforward to derive sample complexity bounds for achieving a given error
relative to that of the best linear predictor in some class. The bound will depend polynomially in k
but only logarithmically in d. This is cosmetically similar to learning bounds for feature-efﬁcient
algorithms (e.g. [23, 22]) which are concerned with sparsity in the weight vector, rather than in the
output.

6

6 Experimental Validation

We conducted an empirical assessment of our proposed reduction on two labeled data sets with large
label spaces. These experiments demonstrate the feasibility of our method – a sanity check that the
reduction does in fact preserve learnability – and compare d ifferent compression and reconstruction
options.

6.1 Data

Image data.1 The ﬁrst data set was collected by the ESP Game [24], an online game in which
players ultimately provide word tags for a diverse set of web images.

The set contains nearly 68000 images, with about 22000 unique labels. We retained just the 1000
most frequent labels: the least frequent of these occurs 39 times in the data, and the most frequent
occurs about 12000 times. Each image contains about four labels on average. We used half of the
data for training and half for testing.

We represented each image as a bag-of-features vector in a manner similar to [25]. Speci ﬁcally, we
identi ﬁed 1024 representative SURF features points [26] from 10 × 10 gray-scale patches chosen
randomly from the training images; this partitions the space of image patches (represented with
SURF features) into Voronoi cells. We then built a histogram for each image, counting the number
of patches that fall in each cell.
Text data.2 The second data set was collected by Tsoumakas et al. [11] from del.icio.us, a
social bookmarking service in which users assign descriptive textual tags to web pages.

The set contains about 16000 labeled web page and 983 unique labels. The least frequent label
occurs 21 times and the most frequent occurs almost 6500 times. Each web page is assigned 19
labels on average. Again, we used half the data for training and half for testing.

Each web page is represented as a boolean bag-of-words vector, with the vocabulary chosen using a
combination of frequency thresholding and χ2 feature ranking. See [11] for details.
Each binary label vector (in both data sets) indicates the labels of the corresponding data point.

6.2 Output Sparsity

We ﬁrst performed a bit of exploratory data analysis to get a s ense of how sparse the target in our
data is. We computed the least-squares linear regressor bB ∈ Rp×d on the training data (without any
output coding) and predicted the label probabilities bp(x) = bB⊤x on the test data (clipping values
to the range [0, 1]). Using bp(x) as a surrogate for the actual target E[y |x], we examined the relative
2 error of bp and its best k-sparse approximation ǫ(k , bp(x)) = Pd
2 , where
ℓ2
i=k+1 bp(i) (x)2 /k bp(x)k2
bp(1) (x) ≥ . . . ≥ bp(d) (x).
Examining Ex ǫ(k , bp(x)) as a function of k , we saw that in both the image and text data, the fall-
off with k is eventually super-polynomial, but we are interested in the behavior for small k where it
appears polynomial k−r for some r . Around k = 10, we estimated an exponent of 0.50 for the image
data and 0.55 for the text data. This is somewhat below the standard of what is considered sparse
(e.g. vectors with small ℓ1 -norm show k−1 decay). Thus, we expect the reconstruction algorithms
will have to contend with the sparsity error of the target.

6.3 Procedure

We used least-squares linear regression as our base learning algorithm, with no regularization on the
image data and with ℓ2 -regularization with the text data (λ = 0.01) for numerical stability. We did
not attempt any parameter tuning.

1http://hunch.net/∼learning/ESP-ImageSet.tar.gz
2http://mlkd.csd.auth.gr/multilabel.html

7

The compression functions we used were generated by selecting m random rows of the 1024 × 1024
Hadamard matrix, for m ∈ {100, 200, 300, 400}. We also experimented with Gaussian matrices;
these yielded similar but uniformly worse results.

We tested the greedy and iterative reconstruction algorithms described earlier (OMP, FoBa, and
CoSaMP) as well as a path-following version of Lasso based on LARS [21]. Each algorithm was
used to recover a k-sparse label vector byk from the predicted compressed label H (x), for k =
2 distance kbyk − yk2
1, . . . , 10. We measured the ℓ2
2 of the prediction to the true test label y . In
addition, we measured the precision of the predicted support at various values of k using the 10-
sparse label prediction. That is, we ordered the coefﬁcient s of each 10-sparse label prediction by10
by magnitude, and measured the precision of predicting the ﬁ rst k coordinates | supp(by10
(1:k) ) ∩
supp(y)|/k . Actually, for k ≥ 6, we used by2k instead of by10 .
We used correlation decoding (CD) as a baseline method, as it is a standard decoding method for
ECOC approaches. CD predicts using the top k coordinates in A⊤H (x), ordered by magnitude. For
mean-squared-error comparisons, we used the least-squares approximation of H (x) using these k
columns of A. Note that CD is not a valid reconstruction algorithm when m < d.

6.4 Results

As expected, the performance of the reduction, using any reconstruction algorithm, improves as the
number of induced subproblems m is increased (see ﬁgures in Appendix A) When m is small and
A 6∈ AK , the reconstruction algorithm cannot reliably choose k ≥ K coordinates, so its perfor-
mance may degrade after this point by over- ﬁtting. But when t he compression function A is in AK
for a sufﬁciently large K , then the squared-error decreases as the output sparsity k increases up to
K . Note the fact that precision-at-k decreases as k increases is expected, as fewer data will have at
least k correct labels.
All of the reconstruction algorithms at least match or out-performed the baseline on the mean-
squared-error criterion, except when m = 100. When A has few rows, (1) A ∈ AK only for very
small K , and (2) many of its columns will have signi ﬁcant correlatio n. In this case, when choosing
k > K columns, it is better to choose correlated columns to avoid over- ﬁtting. Both OMP and
FoBa explicitly avoid this and thus do not fare well; but CoSaMP, Lasso, and CD do allow selecting
correlated columns and thus perform better in this regime.

The results for precision-at-k are similar to that of mean-squared-error, except that choosing corre-
lated columns does not necessarily help in the small m regime. This is because the extra correlated
columns need not correspond to accurate label coordinates.

In summary, the experiments demonstrate the feasibility and robustness of our reduction method for
two natural multi-label prediction tasks. They show that predictions of relatively few compressed
labels are sufﬁcient to recover an accurate sparse label vec tor, and as our theory suggests, the ro-
bustness of the reconstruction algorithms is a key factor in their success.

Acknowledgments

We thank Andy Cotter for help processing the image features for the ESP Game data. This work
was completed while the ﬁrst author was an intern at TTI-C in 2 008.

References
[1] David Donoho. Compressed sensing. IEEE Trans. Info. Theory, 52(4):1289–1306, 2006.
[2] T. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes.
Journal of Arti ﬁcial Intelligence Research , 2:263–286, 1995.
[3] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation.
5:101–141, 2004.
[4] M. Boutell, J. Luo, X. Shen, and C. Brown. Learning multi-label scene classiﬁcation. Pattern Recognition,
37(9):1757–1771, 2004.
[5] A. Clare and R.D. King. Knowledge discovery in multi-label phenotype data. In European Conference
on Principles of Data Mining and Knowledge Discovery, 2001.

Journal of Machine Learning Research,

8

[6] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003.

[7] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni.
Incremental algorithms for hierarchical classiﬁcation.
Journal of Machine Learning Research, 7:31–54, 2006.

[8] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interde-
pendent and structured output spaces. In ICML, 2004.

[9] J. Rousu, C. Saunders, S. Szedmak, and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel
classiﬁcation models.
Journal of Machine Learning Research, 7:1601–1626, 2006.

[10] J. Huang, T. Zhang, and D. Metaxax. Learning with structured sparsity. In ICML, 2009.

[11] G. Tsoumakas, I. Katakis, and I. Vlahavas. Effective and efﬁ cient multilabel classiﬁcation in domains
with large number of labels. In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data,
2008.

[12] Erin Allwein, Robert Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying approach
for margin classiﬁers.
Journal of Machine Learning Research, 1:113–141, 2000.

[13] J. Langford and A. Beygelzimer. Sensitive error correcting output codes. In Proc. Conference on Learning
Theory, 2005.

[14] Emmanuel Cand `es, Justin Romberg, and Terrence Tao. Stable signal recovery from incomplete and
inaccurate measurements. Comm. Pure Appl. Math., 59:1207–122, 2006.

[15] R. DeVore. Deterministic constructions of compressed sensing matrices. J. of Complexity, 23:918–925,
2007.

[16] Shahar Mendelson, Alain Pajor, and Nicole Tomczak-Jaegermann. Uniform uncertainty principle for
Bernoulli and subgaussian ensembles. Constructive Approximation, 28(3):277–289, 2008.

[17] M. Rudelson and R. Vershynin. Sparse reconstruction by convex relaxation: Fourier and Gaussian mea-
surements. In Proc. Conference on Information Sciences and Systems, 2006.

[18] S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal
Processing, 41(12):3397–3415, 1993.

[19] Tong Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In
Proc. Neural Information Processing Systems, 2008.

[20] D. Needell and J.A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples.
Applied and Computational Harmonic Analysis, 2007.

[21] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. Annals of
Statistics, 32(2):407–499, 2004.

[22] Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk
bounds, margin bounds, and regularization. In Proc. Neural Information Processing Systems, 2008.

[23] Andrew Ng. Feature selection, l1 vs. l2 regularization, and rotational invariance. In ICML, 2004.

[24] Luis von Ahn and Laura Dabbish. Labeling images with a computer game. In Proc. ACM Conference on
Human Factors in Computing Systems, 2004.

[25] Marcin Marszałek, Cordelia Schmid, Hedi Harzallah, and Joost v an de Weijer. Learning object repre-
sentations for visual object class recognition. In Visual Recognition Challange Workshop, in conjunction
with ICCV, 2007.

[26] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. SURF: Speeded up robust features.
Computer Vision and Image Understanding, 110(3):346–359, 2008.

[27] David Donoho, Michael Elad, and Vladimir Temlyakov. Stable recovery of sparse overcomplete repre-
sentations in the presence of noise. IEEE Trans. Info. Theory, 52(1):6–18, 2006.

[28] Sanjoy Dasgupta. Learning Probability Distributions. PhD thesis, University of California, 2000.

9


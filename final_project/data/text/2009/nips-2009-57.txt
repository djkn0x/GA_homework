A Bayesian Model for Simultaneous Image
Clustering, Annotation and Object Segmentation

Lan Du, Lu Ren, 1David B. Dunson and Lawrence Carin
Department of Electrical and Computer Engineering
1Statistics Department
Duke University
Durham, NC 27708-0291, USA
{ld53, lr, lcarin}@ee.duke.edu, dunson@stats.duke.edu

Abstract

A non-parametric Bayesian model is proposed for processing multiple images.
The analysis employs image features and, when present, the words associated
with accompanying annotations. The model clusters the images into classes, and
each image is segmented into a set of objects, also allowing the opportunity to
assign a word to each object (localized labeling). Each object is assumed to be
represented as a heterogeneous mix of components, with this realized via mixture
models linking image features to object types. The number of image classes, num-
ber of object types, and the characteristics of the object-feature mixture models
are inferred nonparametrically. To constitute spatially contiguous objects, a new
logistic stick-breaking process is developed. Inference is performed efﬁciently
via variational Bayesian analysis, with example results presented on two image
databases.

1 Introduction
There has recently been much interest in developing statistical models for analyzing and organiz-
ing images, based on image features and, when available, auxiliary information, such as words
(e.g ., annotations). Three important aspects of this problem are:
(i) sorting multiple images
into scene-level classes, (ii) image annotation, and (iii) segmenting and labeling localized objects
within images. Probabilistic topic models, originally developed for text analysis [8, 12], have been
adapted and extended successfully for many image-understanding problems [3, 6, 9–11, 16, 23, 24].
Moreover, recent work has also used the Dirichlet process (DP) [5] or similar non-parametric pri-
ors to enhance the topic-model structure [2, 20, 26]. Using such statistical models, researchers
[2, 3, 6, 10, 16, 20, 23, 24, 26] have addressed two or all three of the objectives simultaneously
within a single setting. Such uniﬁed formalisms have realiz ed marked improvements in overall al-
gorithm performance. A relatively complete summary of the literature may be found in [16, 23],
where the advantages of the approaches in [16, 23] are described relative to previous related ap-
proaches [3, 6, 10, 11, 18, 24, 27]. The work in [16, 23] is based on the correspondence LDA
(Corr-LDA) model [6]. The approach in [23] integrates the Corr-LDA model and the supervised
LDA (sLDA) model [7] into a single framework. Although good classiﬁcation performance was
achieved using this approach, the model is employed in a supervised manner, utilizing scene-labeled
images for scene classiﬁcation. A class label variable is in troduced in [16] to cluster all images in
an unsupervised manner, and a switching variable to address noisy annotations. Nevertheless, to
improve performance, in [16] some images are required for supervised learning, based on the seg-
mented and labeled objects obtained via the method proposed in [10], with these used to initialize
the algorithm.

The research reported here seeks to build upon and extend recent research on uniﬁed image-analysis
models. Speciﬁcally, motivated by [16, 23], we develop a nov el non-parametric Bayesian model

1

that simultaneously addresses all three objectives discussed above. The four main contributions of
this paper are:

• Each object in an image is represented as a mixture of image-feature model parameters, account-
ing for the heterogeneous character of individual objects. This framework captures the idea that a
particular object may be composed as an aggregation of distinct parts. By contrast, each object is
only associated with one image-feature component/atom in the Corr-LDA-like models [6, 16, 23].
• Multiple images are processed jointly; all, none or a subset of the images may be annotated. The
model infers the linkage between image-feature parameters and object types, with this linkage used
to yield localized labeling of objects within all images. The unsupervised framework is executed
without the need for a human to constitute training data.
• A novel logistic stick-breaking process (LSBP) is proposed, imposing the belief that proximate
portions of an image are more likely to reside within the same segment (object). This spatially con-
strained prior yields contiguous objects with sharp boundaries, and via the aforementioned mixture
models the segmented objects may be composed of heterogeneous building blocks.
• The proposed model is nonparametric, based on use of stick-breaking constructions [13], which
can be easily implemented by fast variational Bayesian (VB) inference [14]. The number of image
classes, number of object types, number of image-feature mixture components per object, and the
linkage between words and image model parameters are inferred nonparametrically.
2 The Hierarchical Generative Model
2.1 Bag of image features
We jointly process data from M images, and each image is assumed to come from an associated
class type (e.g ., city scene, beach scene, ofﬁce scene, etc.). The class type associated with image m
is denoted by zm ∈ {1, . . . , I }, and it is drawn from the mixture model
I
X
i=1
where StickI (αu ) is a stick-breaking process [13] that is truncated to I sticks, with hyper-parameter
αu > 0. The symbol δi represents a unit measure at the integer i, and the parameter ui denotes the
probability that image type i will be observed across the M images.
The observed data are image feature vectors, each tied to a local region in the image (for example,
associated with an over-segmented portion of the image). The Lm observed image feature vectors
associated with image m are {xml}Lm
l=1 , and the lth feature vector is assumed drawn xml ∼ F (θml ).
The expression F (·) represents the feature model, and θml represents the model parameters.
Each image is assumed to be composed of a set of latent objects. An indicator variable ζml de ﬁnes
which object type the lth feature vector from image m is associated with, and it is drawn
K
X
k=1
where index k corresponds to the k th type of object that may reside within an image. The vector
wi de ﬁnes the probability that each of the K object types will occur, conditioned on the image type
i ∈ {1, . . . , I }; the k th component of wzm , wzm k , denotes the probability of observing object type
k in image m, when image m was drawn from class zm ∈ {1, . . . , I }.

wzm k δk , w i ∼ StickK (αw )

uiδi , u ∼ StickI (αu )

ζml ∼

zm ∼

(1)

(2)

The image class zm and corresponding objects {ζml }Lm
l=1 associated with image m are latent vari-
ables. The generative process for the observed data, {xml}Lm
l=1 , is manifested via mixture models
with respect to model parameter θ . Speciﬁcally, a separate such mixture model is manifested f or
each of the K object types, motivated by the idea that each object will in general be composed of a
different set of image-feature building blocks. The mixture model for object type k ∈ {1, . . . , K }
is represented as
J
X
j=1
where H is a base measure, usually selected to be conjugate to F (·).
2.2 Bag of clustered image features
While the model described above is straightforward to understand, it has been found to be ineffec-
tive. This is because each of the ζml is drawn i.i.d. from PK
k=1 wzm k δk , and therefore there is

, hk ∼ StickJ (αh ) , θ∗
j ∼ H

hkj δθ∗
j

Gk =

(3)

2

nothing in the model that encourages the image features, xml and xml′ , which are associated with
the same image-feature atom θ∗
j , to be assigned to the same object k .
To address this limitation, we add a clustering step within each of the images; this is similar to
the structure of the hierarchical Dirichlet process (HDP) [21]. Speciﬁcally, consider the following
augmented model:

(4)

uiδi

wzm k δk , zm ∼

vmt δζmt , ζmt ∼

xml ∼ F (θml ) , θml ∼ Gcml , cml ∼

T
K
I
X
X
X
t=1
i=1
k=1
where vm ∼ StickT (αv ), and Gk is as de ﬁned in (3). We make truncation level T < K , to
encourage a relatively small number of objects in a given image.
2.3 Linking words with images
In the above discussion it was assumed that the only observed data are the image feature vectors
{xml }Lm
l=1 . However, there are situations for which annotations (words) may be available for at
least a subset of the M images. In this setting we assume that we have a K -dimensional dictionary
of words associated with objects in images, and a word is assigned to each of the objects k ∈
{1, . . . , K }. Of the collection of M images, some may be annotated and some not, and all will
be processed simultaneously by the joint model; in so doing, annotations will be inferred for the
originally non-annotated images.

For an image for which no annotation is given, the image is assumed generated via (4). When
an annotation is available, the words associated with image m are represented as a vector ym =
[ym1 , · · · , ymK ]T , where ymk denotes the number of times word k is present in the annotation to
image m (typically ymk will either be one or zero), and ym is assumed drawn from a multinomial
distribution associated with a parameter ϕm : ym ∼ Mult(ϕm ). If image m is in class zm , then we
simply set

ym ∼ Mult(wzm ) , wi ∼ StickK (αw )
Namely, ϕm = wzm , recalling that wi de ﬁnes the probability of observing each object type
for image class i. When a dictionary of K words is available, we generally use wi ∼
Dir(αw /K, . . . , αw /K ), consistent with LDA [8].
3 Encouraging Spatially Contiguous Objects
3.1 Logistic stick-breaking process (LSBP)
In (5), note that once the image class zm is drawn for image m, the order/location of the xml within
the image may be interchanged, and nothing in the generative process will change. This is because
the indicator variable cml , which de ﬁnes the object class associated with feature vect or l in image m,
is drawn i.i.d. cml ∼ PT
t=1 vmt δζmt . It is therefore desirable to impose that if two feature vectors
are proximate within the image, they are likely to be associated with the same object.
With each feature vector xml there is an associated spatial location, which we denote sml (this is a
two-dimensional vector). We wish to draw
T
K
X
X
t=1
k=1
where the cluster probabilities vmt (sml ) are now a function of position sml (the ζmt ∈ {1, . . . , K }
correspond to object types). The challenge, therefore, becomes development of a means of construct-
ing vmt (s) to encourage nearby feature vectors to come from the same object type. Toward this goal,
let σ [gmt (s)] represent a logistic link function, which is a function of s. For t = 1, . . . , T − 1 we
impose
t−1
Y
{1 − σ [gmτ (s)]}
τ =1
tl K(s, sml ) + W (m)
l=1 W (m)
where vmT (s) = 1 − PT −1
gmt(s) = PLm
t=1 vmt (s). We de ﬁne
t0
where K(s, sml ) is a kernel, and here we utilize the radial basis function kernel K(s, sml ) =
exp[−ks − sml k2/φmt ]. The parameter kernel width φmt plays an important role in dictating the
size of segments associated with stick t, and therefore these parameters should be learned by the
data in the analysis. In practice we de ﬁne a library of discre te kernel widths φ∗ = {φ∗
d=1 , and
d }D
infer each φmt , placing a uniform prior on the elements of φ∗ .

vmt (s) = σ [gmt (s)]

vmt (sml )δζmt

wzm k δk

cml ∼

,

ζmt ∼

(5)

(6)

(7)

3


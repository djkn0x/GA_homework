Sensitivity analysis in HMMs
with application to likelihood maximization

Pierre-Arnaud Coquelin,
Vekia, Lille, France
pacoquelin@vekia.fr

Romain Deguest⁄
Columbia University, New York City, NY 10027
rd2304@columbia.edu

Rémi Munos
INRIA Lille - Nord Europe, Sequel Project, France
remi.munos@inria.fr

Abstract

This paper considers a sensitivity analysis in Hidden Markov Models with con-
tinuous state and observation spaces. We propose an Inﬁnitesimal Perturbation
Analysis (IPA) on the ﬁltering distribution with respect to some parameters of the
model. We describe a methodology for using any algorithm that estimates the ﬁl-
tering density, such as Sequential Monte Carlo methods, to design an algorithm
that estimates its gradient. The resulting IPA estimator is proven to be asymptoti-
cally unbiased, consistent and has computational complexity linear in the number
of particles.
We consider an application of this analysis to the problem of identifying unknown
parameters of the model given a sequence of observations. We derive an IPA
estimator for the gradient of the log-likelihood, which may be used in a gradient
method for the purpose of likelihood maximization. We illustrate the method with
several numerical experiments.

1 Introduction

We consider a parameterized hidden Markov model (HMM) deﬁned on continuous state and ob-
servation spaces. The HMM is deﬁned by a state process (Xt )t‚0 ∈ X and an observation process
(Yt )t‚1 ∈ Y that are parameterized by a continuous parameter θ = (θ1 , . . . , θd ) ∈ Θ, where Θ is a
compact subset of Rd .
The state process is a Markov chain taking its values in a (measurable) state space X , with initial
probability measure µ ∈ M(X ) (i.e. X0 ∼ µ) and Markov transition kernel K (θ , xt , dxt+1 ). We
assume that we can sample this Markov chain using a transition function F and independent random
numbers, i.e. for all t ≥ 0,

i.i.d.∼ ν,
Xt+1 = F (θ, Xt , Ut ), with Ut
(1)
where F : Θ × X × U → X and (U, σ(U ), ν ) is a probability space. In many practical situations
U = [0, 1]p , ν is uniform, thus Ut is a p-uple of uniform random numbers. For simplicity, we
adopt the notations F (θ , x¡1 , u) , Fµ (θ, u), where Fµ is the ﬁrst transition function (i.e. X0 =
Fµ (θ , U¡1 ) with U¡1 ∼ ν ).
The observation process (Yt )t‚1 lies in a (measurable) space Y and is linked with the state process
by the conditional probability measure P(Yt ∈ dyt |Xt = xt ) = g(θ, xt , yt ) dyt , where g : Θ ×
∗
also afﬁliated with CMAP, Ecole Polytechnique, France

1

X × Y → [0, 1] is the marginal density function of Yt given Xt . We assume that observations are
conditionally independent given the state.
Since the transition and observation processes are parameterized by the parameter θ , the state Xt
and the observation Yt processes depend explicitly on θ . For notation simplicity we will omit to
write the dependence of θ (in K , F , g , Xt , Yt , ...) when there is no possible ambiguity.
One of the main interest in HMMs is to recover the state at time n given a sequence of past obser-
vations (y1 , . . . , yn ) (written y1:n ). The ﬁltering distribution (or belief state)
πn (dxn ) , P(Xn ∈ dxn |Y1:n = y1:n )
is the distribution of Xn conditioned on the information y1:n . We deﬁne analogously the predictive
distribution
πn+1jn (dxn+1 ) , P(Xn+1 ∈ dxn+1 |Y1:n = y1:n ).
∫
Our contribution is an Inﬁnitesimal Perturbation Analysis (IPA) that estimates the gradient ∇πn
(where ∇ refers to the derivative with respect to the parameter θ) of the ﬁltering distribution πn .
More precisely, we estimate ∇πn (f ) (where π(f ) ,
X f (x)π(dx)) for any integrable function f
under the ﬁltering distribution πn .
We also consider as application, the problem of parameter identiﬁcation in HMMs which consists
⁄ of the model that has served to generate the sequence
in estimating the (unknown) parameter θ
of observations. In a Maximum Likelihood (ML) approach, one searches for the parameter θ that
maximizes the likelihood (or its logarithm) given the sequence of observations. The log-likelihood
of parameter θ is deﬁned by ln (θ) , log pθ (y1:n ) where pθ (y1:n ) dy1:n , P(Y1:n (θ) ∈ dy1:n ).
The Maximum Likelihood (ML) estimator ˆθn , arg maxθ2Θ ln (θ) is asymptotically consistent
⁄ when n → ∞ under some
(in the sense that ˆθn converges almost surely to the true parameter θ
identiﬁably conditions and mild assumptions on the model, see Theorem 2 of [DM01]). Thus, using
the ML approach, the parameter identiﬁcation problem reduces to an optimization problem.
Our second contribution is a sensitivity analysis of the predictive distribution ∇πt+1jt , for t <
n, which enables to estimate the gradient ∇ln (θ) of the log-likelihood function, which may be
used in a (stochastic) gradient method for the purpose of optimizing the likelihood. The approach
is numerically illustrated on two parameter identiﬁcation problems (autoregressive model and a
stochastic volatility model) and compared to other approaches (EM algorithm, the Kalman ﬁlter,
and the Likelihood ratio approach) when these latter apply.

2 Links with other works

First, let us mention that we are interested in the continuous state case since numerous applications
in signal processing, ﬁnance, robotics, or telecommunications naturally ﬁt in this framework. In the
general setting there exists no closed-form expression of the ﬁltering distribution (unlike in ﬁnite
spaces where the Viterbi algorithm may apply or in linear-Gaussian models where the Kalman ﬁlter
can be used). Thus, in this paper, we will make use of the so-called Sequential Monte Carlo
methods (SMC) (also known as Particle Filters) which are numerical tools that can be applied to
a large class of models, see e.g. [DFG01]. For illustration, a challenging example in ﬁnance is
the problem of parameter estimation in the stochastic volatility model, which is a non-linear non-
Gaussian continuous space HMM parameterized by three continuous parameters (see e.g. [ME07])
which will be described in the experimental section.
A usual approach for parameter estimation consists in performing a maximum likelihood estimation
(MLE), i.e. search for the most likely value of the parameter, given the observed data. For ﬁnite state
space problems, the Expectation Maximization (EM) algorithm is a popular method for solving the
MLE problem. However, in continuous space problems, see [CM05], the EM algorithm is difﬁcult to
use mainly because the Expectation part relies on the estimation of the posterior path measure which
is intractable in many situations. The Maximization part may also be very complicated and time-
consuming when the model does not belong to a linear or exponential family. An alternative method
consists in using brute force optimization methods based on the evaluation of the likelihood such
as grid-based or simulated annealing methods. These approaches, which can be seen as black-box
optimization are not very efﬁcient in high dimensional parameter spaces.

2

Another approach is to treat the parameter as part of the state variable and then compute the optimal
ﬁlter (see [DFG01] and [Sto02]). In this case, the Bayesian posterior distribution of the parameter
is a marginal of the optimal ﬁlter. It is well known that those methods are stable only under certain
conditions, see [Pap07], and do not perform well in practice for a large number of time steps.
A last solution consists in using an optimization procedure based on the evaluation of the gradient
of the log-likelihood function with respect to the parameter. These approaches have been studied in
the ﬁeld of continuous space HMMs e.g. in [DT03, FLM03, PDS05, Poy06]. The idea was to use a
likelihood ratio approach (also called score method) to evaluate the gradient of the likelihood. This
approach suffers from high variance of the estimator, in particular for problems with small noise in
the dynamic. To tackle this issue, [PDS05] proposed to use a marginal particle ﬁlter instead of a
simple path-based particle ﬁlter as Monte Carlo approximation method. This approach is efﬁcient
in terms of variance reduction but its computational complexity becomes quadratic in the number of
particles instead of being linear, like in path-based particle methods.
The IPA approach proposed in this paper is an alternative gradient-based maximum likelihood ap-
proach. Compared with works on gradient approaches previously cited, the IPA provides usually a
lower variance estimators than the likelihood ratio methods, and its numerical complexity is linear
in the number of particles.
Other works related to ours are the so-called tangent ﬁlter approach described in [CGN01] for dy-
namics coming from a discretization of a diffusion process, and the Finite-Difference (FD) approach
described in a different setting (i.e. policy gradient in Partially Observable Markov Decision Pro-
cesses) in [CDM08]. A similar FD estimator could be designed in our setting too but the resulting
FD estimator would be biased (like usual FD schemes) whereas the IPA estimator is not.

.

(2)

=

∏
∏
E[f (Xn )
n
t=0 Gt (Xt )]
E[
n
t=0 Gt (Xt )]

3 Sequential Monte Carlo methods (SMC)
∫
∏
Given a measurable test function f : X → R, we have:
∫ ∏
n
t=0 K (xt¡1 , dxt )Gt (xt )
πn (f ) , E[f (Xn )|Y1:n = y1:n ] =
f (xn )
n
t=0 K (xt¡1 , dxt )Gt (xt )
where we used the simpliﬁed notation: Gt (xt ) , g(xt , yt ) and G0 (x0 ) , 1.
In general, it is impossible to write πn (f ) analytically except for speciﬁc cases (such as lin-
ear/Gaussian with Kalman ﬁltering). In this paper, we consider a numerical approximation of πn (f )
based on a SMC method. But it should be mentioned that other methods (such as Extended Kalman
ﬁlter, quantization methods, Markov Chain Monte Carlo methods) may be used as well to build the
∑
IPA estimator that we propose in the next section.
The basic SMC method, called Bootstrap Filter, see [DFG01] for details, approximates πn (f ) by an
n (f ) , 1
N
i=1 f (xi
n ) made of N particles x1:N
empirical distribution πN
n .
N
iid∼ ν and set exi
Algorithm 1 Generic Sequential Monte Carlo
for t = 1 to n do
t¡1 ), ∀i ∈ {1, . . . , N }. Then deﬁne the
Sampling: Sample ui
t = F (xi
t¡1 , ui
t¡1
t = Gt (exi
t = exki
PN
j=1 Gt (exj
t )
importance sampling weights wi
,
t )
t , ∀i ∈ {1, . . . , N }, where k1:N are indices selected from the weights
∑
Resampling: Set xi
.
w1:N
t
end for
N
i=1 f (xi
n )
n (f ) = 1
RETURN: πN
The sampling (or transition) step generates a successor particle population ex1:N
N
according to the
t
the set ex1:N
state dynamics from the previous population x1:N
are eval-
t¡1 . The importance sampling weights w1:N
t
uated, and the resampling (or selection) step resamples (with replacement) N particles x1:N
from
t
according to the weights w1:N
. Resampling is used to avoid the problem of degeneracy
t
t
of the algorithm, i.e. that most of the weights decreases to zero. It consists in selecting new parti-

3

∑
∑
tφ(exi
t ) = E[ 1
N
N
i=1 φ(xi
t )]).
cle positions such as to preserve a consistency property (i.e.
i=1 wi
N
The simplest version introduced in [GSS93] chooses the selection indices k1:N
by an independent
sampling from the set {1, . . . , N } according to a multinomial distribution with parameters w1:N
t
,
t , for all 1 ≤ i ≤ N . The idea is to replicate the particles in proportion to their
t
i.e. P(k i
t = j ) = wj
weights. Many variants have been proposed in the literature, among which the stratiﬁed resampling
method [Kit96] which is optimal in terms of variance minimization.
n (f ) to πn (f ) (e.g. Law of Large Numbers or Central Limit Theorems) are
Convergence issues of πN
n (f ) is
discussed in [Del04] or [DM08]. For our purpose we note that under mild conditions on f , πN
an asymptotically unbiased (see [DMDP07] for the asymptotic expression of the bias) and consistent
estimator of πn (f ).

4 Inﬁnitesimal Perturbation Analysis in HMMs
4.1 Sensitivity analysis of the ﬁltering distribution
∏
∏
∏
[
]
∏
∏
∏
The following decomposition of the gradient of the ﬁltering distribution πn applied to a function f :
∇E[f (Xn )
∇E[
E[f (Xn )
n
n
n
∇[πn (f )] = ∇
−πn (f )
t=0 Gt (Xt )]
t=0 Gt (Xt )]
t=0 Gt (Xt )]
=
E[
E[
E[
∏
n
n
n
t=0 Gt (Xt )]
t=0 Gt (Xt )]
t=0 Gt (Xt )]
(3)
shows that the problem of ﬁnding an estimator of ∇πn (f ) is reduced to the problem of ﬁnding an
estimator of ∇E[f (Xn )
n
t=0 Gt (Xt )]. There are two dominant inﬁnitesimal methods for estimat-
ing the gradient of an expectation in a Markov chain: the Inﬁnitesimal Perturbation Analysis (IPA)
method and the Score Function (SF) method (also called likelihood ratio method), see for instance
[Gla91] and [Pﬂ96] for a detailed presentation of both methods. SF has been used in [DT03, FLM03]
to estimate ∇πn . Although IPA is known for having a lower variance than SF in general, as far as
we know, it has never been used in this context. This is therefore the object of this Section.
Under appropriate smoothness assumptions (see Proposition 1 below), the gradient of an expectation
over a random variable X is equal to an expectation involving the pair of random variables (X, ∇X )
∏
∇E[f (X )] = E[∇[f (X )]] = E[f
0 (X )∇X ],
[
]
[
[
]]
(where 0 refers to the derivative with respect to the state variable). Applying this property to estimate
n∏
n∏
∇E [f (Xn )
n
t=0 Gt (Xt )], we deduce
[(
]
)
∇E
∇
= E
n∑
n∏
f (Xn )
Gt (Xt )
f (Xn )
Gt (Xt )
t=0
t=0
∇[Gt (Xt )]
[(
]
)
∇[f (Xn )] + f (Xn )
n∑
n∏
Gt (Xt )
Gt (Xt )
t=0
t=0
t (Xt )∇Xt + ∇Gt (Xt )
0
0 (Xn )∇Xn + f (Xn )
G
Gt (Xt )
.
Gt (Xt )
t=0
t=0
 Xt+1 = F (Xt , Ut ), where Ut ∼ ν
{
Now we deﬁne an augmented Markov chain (Xt , Zt , Rt )t‚0 by the following recursive relations
(where Zt , ∇Xt )
X0 = Fµ (U¡1 ), U¡1 ∼ ν
Zt+1 = ∇F (Xt , Ut ) + F
0 (Xt , Ut )Zt ,
Z0 = ∇Fµ (U¡1 ),
t+1 (Xt+1 )Zt+1+rGt+1 (Xt+1 )
0
Rt+1 = Rt + G
R0 = 0,
∏
∏
Gt+1 (Xt+1 )
By introducing this augmented Markov Chain in Equation (4) and using Equation (3) we can rewrite
∇πn (f ) as:
∏
∏
∏
0 (Xn )Zn + f (Xn )Rn )
E[Rn
n
n
∇πn (f ) =
t=0 Gt (Xt )]
t=0 Gt (Xt )]
∏
E[
E[
n
n
t=0 Gt (Xt )]
t=0 Gt (Xt )]
0 (Xn )Zn + Rn (f (Xn ) − πn (f )))
E[
n
t=0 Gt (Xt )]
We now state some sufﬁcient conditions under which the previous derivations are sound.

− πn (f )
n
t=0 Gt (Xt )]

∀t ≥ 0,

E[(f

E[(f

= E

f

.

(5)

(4)

,

= E

=

4

Proposition 1. Equation (5) is valid on Θ whenever the following conditions are satisﬁed:
• for all θ ∈ Θ, the path θ 7→ (X0 , X1 , · · · , Xn )(θ) is almost surely (a.s.) differentiable,
• for all θ ∈ Θ, f is a.s. continuously differentiable at Xn (θ), and for all 1 ≤ t ≤ n, Gt is
a.s. continuously differentiable at (θ, Xt (θ)),
• θ 7→ f (Xn (θ)) and for all 1 ≤ t ≤ n, θ 7→ Gt (θ, Xt (θ)) are a.s. continuous and piecewise
0 (Xn ) Zn + Rn (f (Xn ) − πn (f ))| ∏
differentiable throughout Θ,
• Let D be the random subset of Θ at which f (Xn (θ)) or one Gt (θ, Xt (θ)) fails to be differ-
|f
t=0 Gt (Xt )] < ∞,
entiable. We require that E[ sup
n
θ /2D
The proof of this Proposition is a direct application of Theorem 1.2 from [Gla91]. We notice that
requiring the a.s. differentiability of the path θ 7→ (X0 , X1 , · · · , Xn )(θ) is equivalent to requiring
that for all θ ∈ Θ, the transition function F is a.s. continuously differentiable with respect to θ .
From Equation (5), we can derive the IPA estimator of ∇πn (f ) by using a SMC algorithm:
)]
[
N∑
N∑
(
0 (xi
n )
n + f (xi
n )z i
f
,
j=1
i=1

, 1
N

− 1
N

I N
n

(6)

ri
n

rj
n

where (xi
n ) are particles derived by using a SMC algorithm on the augmented Markov chain
n , r i
n , z i
(Xt , Zt , Rt ) described in Algorithm 2.
Algorithm 2 IPA estimation of ∇πn
for t = 1 to n do
For all i ∈ {1, . . . , N } do
iid∼ ν and set ˜xi
t¡1 ),
t = F (xi
Sample ui
t¡1 , ui
t¡1
t = ∇F (xi
0 (xi
t¡1 )z i
t¡1 ) + F
Set ˜z i
t¡1 ,
t¡1 , ui
t¡1 , ui
Pj Gt ( ˜xj
t +rGt ( ˜xi
0
t ) ˜z i
t ( ˜xi
, and compute the weights wi,t = Gt ( ˜xi
t¡1 + G
t )
t )
t = ri
Set ˜ri
[
(
)]
∑
∑
Gt ( ˜xi
t )
t )
t ), where k1:N are the indices selected from w1:N
t , ˜rki
Set (xi
t ) = ( ˜xki
t , ˜z ki
t , z i
t , r i
t
end for
− 1
0 (xi
N
N
n + f (xi
n )z i
n = 1
n )
RETURN: I N
j=1 rj
n
i=1
N
N

ri
n

f

,

Proposition 2. Under the assumptions of Proposition 1, the estimator I N
n deﬁned by (6) has a bias
¡1 ) and is consistent with ∇πn (f ), i.e. E[I N
n ] = ∇πn (f ) + O(N
¡1 ), and limN!1 I N
O(N
n =
∇πn (f ) almost surely. In addition, its (asymptotic) variance is O(N
¡1 ).

∑
Proof. We use the general SMC convergence properties for Feynman-Kac (FK) models (see [Del04]
or [DM08]) which, applied to a FK ﬂow with Markov chain X0:n , (random) potential func-
E[H (X0:n ) Qn
N
tions G(X0:n ), and test function H (X0:n ), states that the SMC estimate:
i=1 H (xi
0:n ) is
1
E[Qn
N
t=0 G(Xt )]
. Moreover, an asymptotic expression of the bias, given
consistent with
t=0 G(Xt )]
¡1 ). Applying those results to the test function
in [DMDP07], shows that it is of order O(N
0 (Xn )Zn + Rn (f (Xn ) − πn (f )), using the representation (5) of the gradient, we deduce that
H , f
the SMC estimator (6) is asymptotically unbiased and consistent with ∇πn (f ). Now the asymptotic
¡1 ) since the Central Limit Theorem (see e.g. [Del04, DM08]) applies to the IPA
variance is O(N
estimator (6) of (5).

Remark 1. Notice that the computation of the gradient estimator requires O(nN md) (where m is
the dimension of X ) elementary operations, which is linear in the number of particles N and linear
in the number of parameters d, and has memory requirement O(N md).

5

4.2 Gradient of the log-likelihood

∇ln (θ) =

In the Maximum Likelihood approach for the problem of parameter identiﬁcation, one may follow
n¡1∑
a stochastic gradient method for maximizing the log-likelihood ln (θ) where the gradient
∇πt+1jt (Gt+1 )
πt+1jt (Gt+1 )
t=0
is obtained by estimating each term ∇πt+1jt (Gt+1 ) of the sum using a similar decomposition as in
]
[
∏
(5) and (4) for the predictive distribution applied to Gt+1 :
∏
E[Gt+1 (Xt+1 )
∏
t
∇πt+1jt (Gt+1 ) = ∇
k=0 Gk (Xk )]
E[
∏
t
k=0 Gk (Xk )]
∇E[Gt+1 (Xt+1 )
t
k=0 Gk (Xk )]
E[
t
k=0 Gk (Xk )]
[(
Gk (Xk )] = E

∏
∏
∇E[
t
k=0 Gk (Xk )]
E[
t
k=0 Gk (Xk )]

with
∇E[Gt+1 (Xt+1 )

− πt+1jt (Gt+1 )

=
t∏
k=0

]
Gk (Xk )

.

+Gt+1 (Xt+1 )

∇Gt+1 (Xt+1 ) + G
t+1 (Xt+1 )∇Xt+1
0
t∑
k (Xk )∇Xk + ∇Gk (Xk )
0
G
Gk (Xk )
k=0
)
∑
j rj
t¡1 )

) t∏
k=0

,

J N
n

− 1
N

(
∑
We deduce the IPA estimator of ∇ln (θ)
, n∑
∑
∇Gt ( ˜xi
0
N
t )(ri
t + Gt ( ˜xi
t ) ˜z i
t ( ˜xi
t ) + G
t¡1
i=1
N
i=1 Gt ( ˜xi
t )
t=1
where (xi
n , ˜ri
n , ˜z i
n ) (and ( ˜xi
n )) are particles derived by using a SMC algorithm on the aug-
n , ri
n , z i
mented Markov chain (Xt , Zt , Rt ) described in the previous subsection. Using similar arguments
as those detailed in proofs of Propositions 1 and 2, we have that this estimator is asymptotically
unbiased and consistent with ∇ln (θ).
∑
∑
The resulting gradient algorithm is described in Algorithm 3. The steps γk are chosen appropriately
k < ∞), see e.g. [KY97]
k‚1 γk = ∞ and
so that local convergence occurs (e.g. such that
k‚1 γ 2
for a detailed analysis of Stochastic Approximation algorithms.
Algorithm 3 Likelihood Maximization by gradient ascent using the IPA estimator of ∇ln (θ)
for k = 1, 2, . . . , Number of gradient steps do
0 = 0
Initialize J N
for t = 1 to n do
For all i ∈ {1, . . . , N } do
iid∼ ν and set ˜xi
t¡1 ),
t = F (xi
Sample ui
t¡1 , ui
t¡1
t = ∇F (xi
0 (xi
t¡1 + PN
N Pj rj
Set ˜z i
t¡1 ) + F
t¡1 )z i
t¡1 ,
t¡1 , ui
t¡1 , ui
PN
i=1 (rGt ( ˜xi
t¡1¡ 1
0
t¡1 ))
t +Gt ( ˜xi
t ) ˜z i
t ( ˜xi
t )(ri
t )+G
t = J N
Set J N
,
i=1 Gt ( ˜xi
t )
Pj Gt ( ˜xj
t +rGt ( ˜xi
0
t = Gt ( ˜xi
t ) ˜z i
t ( ˜xi
t )
t¡1 + G
t )
t = ri
Set ˜ri
and compute the weights wi
Gt ( ˜xi
t )
t )
t ), where k1:N are indices selected from w1:N
Set (xi
t ) = ( ˜xki
t , ˜z ki
t , ˜rki
.
t , z i
t , r i
t
end for
Perform a gradient ascent step: θk = θk¡1 + γk J N
n (θk¡1 )
end for

6

Figure 1: Box-and-whiskers plots of the three parameters (φ, σ, β ) estimates for the AR1 model
with θ? = (0.8, 1.0, 1.0). We compare three methods: (1) Kalman, (2) EM and (3) IPA. Here we
used n = 500 observations and N = 102 particles.

5 Numerical experiments

We consider two typical problems and report our results focussing on the variance of the estimator:
Autoregressive model AR1 is a simple linear-Gaussian HMMs thus may be solved by other meth-
ods (such as Kalman ﬁltering and EM algorithms) which enables to compare the performances of
several algorithms for parameter identiﬁcation. The dynamics are
and for t ≥ 1, Xt = φXt¡1 + σUt ,
X0 ∼ N (0, σ2 ),
Yt = Xt + βVt ,
(7)
i.i.d.∼ N (0, 1) are independent sequences of random variables, and
i.i.d.∼ N (0, 1) and Vt
where Ut
θ = (φ, σ, β ) is a three-dimensional parameter in (R+ )3 .
Stochastic volatility model is very popular in the ﬁeld of quantitative ﬁnance [ME07] to evaluate
derivative securities, such as options. This is a non-linear non-Gaussian model, so the Kalman
method cannot be used anymore. The dynamics are
and for t ≥ 1, Xt = φXt¡1 + σUt ,
X0 ∼ N (0, σ2 ),
Yt = β exp (Xt/2) Vt ,
i.i.d.∼ N (0, 1) and the parameter θ = (φ, σ, β ) ∈ (R+ )3 .

i.i.d.∼ N (0, 1) and Vt
where again Ut
5.1 Parameter identiﬁcation
Figure 1 shows the results of our IPA gradient estimator for the AR1 parameter identiﬁcation prob-
lem and compares those with two other methods: Kalman ﬁlter (K) and EM (which apply since the
⁄ = (0.8, 1.0, 1.0). Notice the apparent
model is linear-Gaussian). The unknown parameter used is θ
⁄ (even for Kalman which provides here the exact
bias of the three methods in the estimation of θ
ﬁltering distribution) since the number of observations n = 500 is ﬁnite. For IPA, we used N = 102
particles and 150 gradient iterations. Algorithm 3 was run 50 times with random starting points uni-
formly drawn between [θ , ¯θ ], where θ = (0.5, 0.5, 0.5) and ¯θ = (1.0, 1.5, 1.5) in order to illustrate
that the method is not sensitive to the starting point.
We observe that in terms of estimation accuracy, IPA is very competitive to the other methods,
Kalman and EM, which are designed for speciﬁc models (here linear-Gaussian). The IPA method
applies to general models, for example, to the stochastic volatility model. Figure 2 shows the sets of
estimates of θ? = (0.8, 1.0, 1.0) using IPA with n = 103 observations and N = 102 particles (no
comparison is made here since Kalman does not apply and EM becomes more complicated).
5.2 Variance study for Score and IPA algorithms
IPA and Score methods provide gradient estimators for general models. We compare the variance
of the corresponding estimators of the gradient ∇ln for the AR1 since for this model we know its
exact value (using Kalman).

(8)

7

1230.810.820.830.840.850.860.870.880.89φMethod1230.750.80.850.90.951σMethod1231.061.081.11.121.141.161.181.21.221.24βMethodFigure 2: Box-and-whiskers plots of the three parameters (φ, σ, β ) estimates for the IPA method
applied to the stochastic volatility model with θ? = (0.8, 1.0, 1.0). We used n = 103 observations
and N = 102 particles.

Figure 3 shows the variance of the IPA and Score estimators of the partial derivative ∂σ ln (we
focused our study on σ since the problem of volatility estimation is challenging, and also because
the value of σ inﬂuences the respective performances of the two algorithms, which is not the case
for the other parameters φ, β ). We used n = N = 103 . The IPA estimator performs better than the
Score estimator for small values of σ . On the other hand, in case of huge variance in the state model,
it is better to use the Score estimator.

Figure 3: Variance of the log-likelihood derivative ∂σ ln computed with both the IPA and Score meth-
⁄ = (φ? , σ? , β ? ) = (0.8, 1.0, 1.0) and the estimations are computed at
ods. The true parameter is θ
θ = (0.7, σ, 0.9).

Let us mention that the variance of the IPA (as well as Score) estimator increases when the number
of observations n increases. However, under weak conditions on the HMM [LM00], the ﬁltering
distribution and its gradient forget exponentially fast the initial distribution. This property has al-
ready been used for EM estimators in [CM05] to show that ﬁxed-lag smoothing drastically reduces
the variance without signiﬁcantly raising the bias. Similar smoothing (either ﬁxed-lag or discounted)
would provide efﬁcient variance reduction techniques for the IPA estimator as well.

6 Conclusions
We proposed a sensitivity analysis in HMMs based on an Inﬁnitesimal Perturbation Analysis and
provided a computationally efﬁcient gradient estimator that provides an interesting alternative to
the usual Score method. We showed how this analysis may be used for estimating the gradient of
the log-likelihood in a gradient-based likelihood maximization approach for the purpose of param-
eter identiﬁcation. Finally let us mention that estimators of higher-order derivatives (e.g. Hessian)
could be derived as well along this IPA approach, which would enable to use more sophisticated
optimization techniques (e.g. Newton method).

8

1230.80.850.90.9511.051.11.151.21.25ValuesParameter number0.20.40.60.811.21.4102030405060708090100110σvnN  Score methodIPA method[DT03]

[CM05]

[Del04]

[DM01]

[DM08]

[FLM03]

[CGN01]

References
[CDM08] P.A. Coquelin, R. Deguest, and R. Munos. Particle ﬁlter-based policy gradient in
POMDPs. In Neural Information Processing Systems, 2008.
F. Cérou, F. Le Gland, and N. J. Newton. Stochastic particle methods for linear tangent
ﬁltering equations. In J.-L. Menaldi, E. Rofman, and A. Sulem, editors, Optimal Con-
trol and PDE’s - Innovations and Applications, in honor of Alain Bensoussan’s 60th
anniversary, pages 231–240. IOS Press, 2001.
O. Cappé and E. Moulines. On the use of particle ﬁltering for maximum likelihood
parameter estimation. European Signal Processing Conference, 2005.
P. Del Moral. Feynman-Kac Formulae, Genealogical and Interacting Particle Systems
with Applications. Springer, 2004.
[DFG01] A. Doucet, N. De Freitas, and N. Gordon. Sequential Monte Carlo Methods in Practice.
Springer, 2001.
R. Douc and C. Matias. Asymptotics of the maximum likelihood estimator for general
hidden markov models. Bernouilli, 7:381–420, 2001.
R. Douc and E. Moulines. Limit theorems for weighted samples with applications to
sequential monte carlo methods. Annals of Statistics, 36:5:2344–2376, 2008.
[DMDP07] P. Del Moral, A. Doucet, and GW Peters. Sharp Propagation of Chaos Estimates for
Feynman–Kac Particle Models. SIAM Theory of Probability and its Applications, 51
(3):459–485, 2007.
A. Doucet and V.B. Tadic. Parameter estimation in general state-space models using
particle methods. Ann. Inst. Stat. Math, 2003.
J. Fichoud, F. LeGland, and L. Mevel. Particle-based methods for parameter estimation
and tracking : numerical experiments. Technical Report 1604, IRISA, 2003.
P. Glasserman. Gradient estimation via perturbation analysis. Kluwer, 1991.
N. Gordon, D. Salmond, and A. F. M. Smith. Novel approach to nonlinear and non-
gaussian bayesian state estimation. In Proceedings IEE-F, volume 140, pages 107–113,
1993.
G. Kitagawa. Monte-Carlo ﬁlter and smoother for non-Gaussian nonlinear state space
models. J. Comput. Graph. Stat., 5:1–25, 1996.
H. J. Kushner and G. Yin. Stochastic Approximation Algorithms and Applications.
Springer-Verlag, Berlin and New York, 1997.
F. LeGland and L. Mevel. Exponential forgetting and geometric ergodicity in hidden
markov models. mathematic and control sugnal and systems, 13:63–93, 2000.
R. Mamon and R.J. Elliott. Hidden markov models in ﬁnance. International Series in
Operations Research and Management Science, 104, 2007.
A. Papavasiliou. A uniformly convergent adaptive particle ﬁlter. Journal of Applied
Probability, 42 (4):1053–1068, 2007.
G. Poyadjis, A. Doucet, and S.S. Singh. Particle methods for optimal ﬁlter derivative:
Application to parameter estimation. In IEEE ICASSP, 2005.
G. Pﬂug. Optimization of Stochastic Models: The Interface Between Simulation and
Optimization. Kluwer Academic Publishers, 1996.
G. Poyiadjis. Particle Method for Parameter Estimation in General State Space Models.
PhD thesis, University of Cambridge, 2006.
G. Storvik. Particle ﬁlters for state-space models with the presence of unknown static
parameters. IEEE Transactions on Signal Processing, 50:281–289, 2002.

[Gla91]
[GSS93]

[PDS05]

[KY97]

[LM00]

[ME07]

[Pap07]

[Kit96]

[Pﬂ96]

[Poy06]

[Sto02]

9


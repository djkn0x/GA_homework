Variants of Pegasos 
SooWoong Ryu            Youngsoo Choi 
bshboy@stanford.edu            yc344@stanford.edu 
 
December 11, 2009 
 

1 Introduction 
 
Developing  a  new  SVM  algorithm  is  ongoing  research  topic.  Among  many  exiting  SVM 
algorithms,  we  will  focus  on  Pegasos.  Pegasos  uses  alternating  iteration  schemes,  which 
are  by  alternating  stochastic  gradient  descent  steps  and  projection  step.  The  Pegasos  can 
solve  a  text  classification  problem  from  Reuters  Corpus  Volume  1  with  800,000  training 
examples  in  5  seconds.  We  will  examine  the  algorithms  robustness  by  applying  the 
algorithm  to  rather  simple  text  classification  problem  (namely,  the  one  given  in  the  cs229 
homework  2). We  also  present  three  possible  variants  of  Pegasos;  namely,  undo  method, 
two-out method and probability method. We will examine con and pro of each method. 
 
2 Algorithm description 
 
2.1 Pegasos 
The  Pegasos  is  a  subgradient  based  algorithm  that  minimizes  the  following  unconstrained 
strictly convex objective function. 

where 

ω
f(
) = 

2

ω

λ

2

1
+ ∑
m
∈
S
x y
)
( ,

l

ω
( ; ( ,
x y

))

 

                              (1) 

l

ω
(
; ( ,
x y

=
)) max{0,1

−

y

ω
, }
x

   

 

              (2) 

A
t

ω
f(
;

where a set

                                                           

However, it does not use eq(1) as an objective function at each iteration. Instead, it uses 
λ
1
+ ∑
k
∈
x y A
( ,
)
t
S⊆ whose size  is k and S, a training set. The Pegasos consists of  two major 
tA
steps  in  updating tω :    1.  substeepest  gradient  update 
tω+
1
2
Specifically,  in  each  iteration,  the  Pegasos  looks  for  a  steepest  gradient  search  direction 
1
tλη =
tω+
tA   and sets the  learning rate to be
given 
. Then,  it projects the updated 
  into the 
t

  and  2.  projection  step. 

                                      (3) 

ω
( ; ( ,
x y

) = 

ω

))

l

2

2

1
2

}

≤

B

ball, 

ω ω=
{ :

.  The  reason  for  projection  is  that  we  already  know  the  optimal 

1
λ
(
)
sqrt
solution,ω∗ , resides in B. 
 
2.2 A Variant of Pegasos 
A  key  point  in  a  variant  of  pegasos  is  how  to  deal  with  outliers  in  training  set,  S.  There  are 
outliers  in  S.  These  outliers  result  in  increase  in  objective  function  value  in  some  iteration 
tA   i.i.d.  from  S.  To  prevent  this,  three  methods  are  used;  1.  Undo 
since  pegasos  chooses 
method 2. two-out method and 3. probability method. A modification from pegasos  is applied 
1tω+ is  computed.  In  the  undo method,  with  computed
1tω+ ,  the 
at  the  end  of  iteration  when 
+ >
ω
ω
f ω+
(
(
f
f
)
)
)
(
then  we  do  not 
,  is  also  computed.  If 
new  objective  function  value, 
1
t
t
1
t
+ ≤
ω
1tω+ .  Instead  we  let 
1tω+   to  be  equal  to tω .  If
ω
)
(
(
t
1
t

,  then  we  keep 

use  computed

)

f

f

(

1tω+   as  updated.  In  the  two-out  method,  the  training  set,  S,  is  updated  at  each  iteration.  If
+ >
ω
ω
1tω+   and  also  append  a  flag  to  each  data,
(
f
)
(
)
f
,  then  we  do  not  use  compute 
t
t
1
x y   in 
,
)
1tA +   and count the number of flags in each data in
1tA + . If the number of flags in a 
i
i
n x y = ),  the data  is taken away  from 
1tD + to be a set 
1tS + . Let 
data becomes two  (i.e 
(
2
)
,
f
i
i
of  those  data  that  are  taken  away  from 1tS + . Then, we  obtain
S D
S
+
+
1
t
t
2
+ ≤
1tω+ as 
ω
ω
(
2tS + is  used  as  a  training  set.  Again,  if
(
f
)
of  t  +  2, 
1
t
t
updated  as  in  undo  method.  The  probability  method  is  similar  to  the  two-out  method.  The 
x y ,  the 
difference  is  in  how  to  update  S.  In  the  probability  method,  for  each  data, (
,
)
i
i
tA   at  t  iteration  (let’s  call  that  probability 
p x y )  is  set  to 
)
,
(
i
i
1  in  the  beginning.  As  the  iteration  proceeds,  these  probabilities  are  updated  by  examining 
+ >
ω
ω
x y
p x y for  each  (
(
f
)
(
)
f
(
,
)
,
)
,  then  we  decrease 
the  objective  function  value.  If
i
i
i
i
t
t
1
in tA . The variant algorithm is shown below. 
 

probability  of  being  sampled  to 

,  then  we  keep 

. At  the  iteration 

=

t
f

+
1

)

INPUT: 

S

,

T kλ
,
,

 

INITIALIZE: Choose 

1w   s.t. 

1w

2

≤

1
λ

 

FOR 

t

=

1, 2, 3, ...,

T

 

S⊆ , where 

tA

k= . 

{( ,
x y

)

∈

,
:
A y w x
t
t

<

1}

1
tλη=  

, 

η
+ ∑
t
k

yx

 

+
∈
)
( ,
x y A
t

Choose 

tA
+ =
A
t

Set 

Set 

IF 

f

w
t

= −
ηλ
(1
)
t

w
t

+

)

1
2
+ >
ω
ω
(
)
(
f
t
1
t
ω ω+ =
1t
t
=
1, 2, 3, ...,
i

FOR 

Set 

 

 

 

Size A
)t
(
+  
) 1

=

)

(
n x y
,
(
n x y
,
i
i
f
f
i
n x y =  
(
2
)
,
i
i
f

IF 

i

Set 

D
t

+
1

=

D
t

+
1

∪

{(

,
x y
i
i

)}

S

t

+

2

=

S D
+
1
t
t

+
1

 

, 

ELSE 

OUTPUT: 

1tω+  

Set 

ω
=
1 min{1,
+
t

1 /

w
t

+

wλ
}
t
1
2

 

+

1
2

 
3 Result 
 
First  of  all,  we  downloaded  the  Pegasos  source  code  from  http://www.cs.huji.ac.il/ 
shais/code/index.html.  There  are  three  sample  sets  of  training  data  and  one  test  data  set: 
One training set of size 2000, two other sets of musch smaller size and a test set of size 600. 
One interesting characteristic of this data set is that l2 norm of every data is one. This puts all 
the data onto the surface of 2 spheres. We needed  to  look for more general data sets which 
do  not  exhibit  such  a  characteristic.  Thus,  we  use  the  training  set  and  test  sets  given  for 
cs229 homework #2. The size of training set is 2144 and test set has size of 800. 
One of the  interesting characteristics of the Pegasos  is that we have a control on  the size of
S=
tA , which is k. If k = 1, then Pegasos is very similar to stochastic gradient method. If  k
, 

then it results in a modified gradient-descent algorithm. 
 
3.1 undo method 
What the undo method actually dose in subgradient based algorithm is to prevent the ascent 
direction.  Please  see  fig.(1)  and  (2).  In  the  beginning  of  the  iterations,  the  undo  method 
seems  to  do  much  better  than  the  pegasos.  However,  as  the  iteration  reaches  the  point 
close  to  the  convergence  region,  the  pegasos  does  better  than  undo method.  This  result  is 
analogs  to  the  comparison  between  steepest  descent  algorithm  and  conjugate  gradient 
method.  Although  the  steepest  descent  may  exhibit  better  performance  in  the  beginning  of 
iterations,  the one that wins  the performance  is conjugate gradient method. The  lesson here 
is  that  allowing  ascent  direction  sometimes  can  be  poison  at  that  moment,  but  can  be 
medicine. As described in algorithm description section, we combined the undo, two-out, and 
probability method and the result is that the undo method is dominant in choosing the search 
direction.  In  fig.(3)  and  (4),  all  the  three  methods  (i.e.  undo  method,  undo-probability 
combined  method,  and  undo-probability-two  out  combined  method  exhibit  almost  identical 
performance). 
 
3.2 two-out method 
Since we’ve found that the undo method should not be used, we decided not to use it in both 
+ >
ω
ω
(
(
f
f
)
)
two-out method  and  probability method. Thus,  in  two-out method,  although 
 
1
t
t
in some  t, we keep  the wt+1 as updated. Only  thing we change  is  to update S every  iteration 
by  eliminating  data  whose  flag  is  equal  to  two.  The  result  is  almost  identical  to  the  original 
pegasos  method.  (i.e.  see  the  fig.(5)  and  (6).  We  strongly  suspect  that  the  reason  for  the 
almost  identical  result  is  that  the  training  set  we  have  (i.e.  the  size  of  about  2000)  is  too 
small  to take an advantage of two-out method. To  take an advantage of two-out method, we 
need enough iteration to get rid of outliers. The average number of data taken away from the 
training set  until  convergence  is  around  80. Among  80  data,  there may  be good  data  in  it.  It 
is  because  taking  away  the  data  whose  number  of  flag  is  two  is  too  cruel.  Thus,  we  took 
away  the data whose number of flag  is four. Then, however,  the number of data taken away 
from  the  training  set  is  very  small  (i.e.  zero,  one  or  two  until  the  convergence).  The  reason 
that  we  have  hope  in  this  method  is  that  even  with  our  training  set,  the  two-out  method 
exhibits slightly better performance than the pegasos right before the convergence occur (i.e. 
see  fig.(7)).  However,  this  is  simply  our  hope  and  it  need  to  be  proven  with  bigger  training 
set in the future. 
 
3.3 probability method 
As  in  case  of  the  two-out  method,  the  probability  method  exhibits  almost  identical  result  to 
pegasos (i.e. see fig.(8) and (9)). This also leaves us a hope that the probability method may 
3 works better than pegasos when the training set size is much bigger. 

Figure1. Comparison between pegasos and undo method 

Figure2. Comparison between pegasos and undo method 

Figure4. undo method is dominant in choosing search direction 

Figure3. undo method is dominant in choosing search direction 
  

Figure5. comparison between pegasos and two-out method 

Figure6. comparison between pegasos and two-out method 

Figure7. zoomed in comparison between pegasos and two-out method 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

Figure8. comparison between pegasos and probability method 

Figure9. comparison between pegasos and probability method 

 
4 Conclusion 
 
Developing  a  new  algorithm  is  a  very  hard  job.  After  reading many  papers  and  getting  very 
deep  insight  about  the  algorithm,  then  finally  someone  can  develop what  is  so  called  ’noble 
algorithm.’  Throughout  the  project,  we  have  tried  many  other  algorithms  which  have  not 
been presented  in this report. However the results were all pessimistic. Initially, for example, 
we  tried  to  extract  only  support  vectors  from  training  set  to  increase  the  convergence  rate. 
However,  due  to  the  presence  of  outliers,  when  we  extract  the  support  vectors  (SVs), 
outliers were also extracted with SVs. This caused even worse test error than pegasos gave. 
In  this  report, we  presented  three  different methods  as  a  variant  of  Pegasos:  undo method, 
two-out  method  and  probability  method.  The  undo  method  acted  like  steepest  descent 
method.  It  may  converge  to  a  good  solution,  but  in  other  times,  it  would  not  converge  to  a 
good  solution.  Both  two-out method  and  probability method  give  a  way  of  finding  outliers  in 
training  set. Taking  away  data  that gets  two  flags  during  the  simulations, we may  have  high 
risk  of  giving  away  a  good  data.  This  result  can  be  fixed  either  by  increasing  two  to  four  or 
five. However  to accomplish  this, we need a bigger  training set. The probability method also 
shows the potential to be used in bigger training set. 
 
 
5 Reference 
 

1.  Shalev-Shwartz, S., Singer, Y. and Srebro, N. (2007). Pegasos: Primal Estimated 
sub-GrAdient Solver  for SVM. Proceedings of  the 24th  International Conference 
on Machine Learning. 
2.  Joachims,  T.  (1999).  Making  Large-Scale  SVM  Learning  Practical.  MIT  Press   
Cambridge, MA, USA 
3.  Joachims,  T.  (2006).  Training  Linear  SVMs  in  Linear  Time.  Proceedings  of  the 
12th  ACM  SIGKDD  international  conference  on Knowledge  discovery  and  data 
mining 

 


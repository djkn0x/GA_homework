Discrete MDL Predicts in Total Variation

Marcus Hutter
RSISE @ ANU and SML @ NICTA
Canberra, ACT, 0200, Australia
marcus@hutter1.net
www.hutter1.net

Abstract

The Minimum Description Length (MDL) principle selects the model that has the
shortest code for data plus model. We show that for a countable class of models,
MDL predictions are close to the true distribution in a strong sense. The result
is completely general. No independence, ergodicity, stationarity, identiﬁability,
or other assumption on the model class need to be made. More formally, we show
that for any countable class of models, the distributions selected by MDL (or MAP)
asymptotically predict (merge with) the true measure in the class in total variation
distance. Implications for non-i.i.d. domains like time-series forecasting, discrim-
inative learning, and reinforcement learning are discussed.

1

Introduction

The minimum description length (MDL) principle recommends to use, among competing models,
the one that allows to compress the data+model most [Gr ¨u07]. The better the compression, the
more regularity has been detected, hence the better will predictions be. The MDL principle can be
regarded as a formalization of Ockham’s razor, which says to select the simplest model consistent
with the data.
Multistep lookahead sequential prediction. We consider sequential prediction problems, i.e. hav-
ing observed sequence x ≡ (x1 ,x2 ,...,x(cid:96) ) ≡ x1:(cid:96) , predict z ≡ (x(cid:96)+1 ,...,x(cid:96)+h ) ≡ x(cid:96)+1:(cid:96)+h , then observe
x(cid:96)+1 ∈ X for (cid:96) ≡ (cid:96)(x) = 0,1,2,.... Classical prediction is concerned with h = 1, multi-step looka-
head with 1 < h < ∞, and total prediction with h = ∞. In this paper we consider the last, hardest
case. An infamous problem in this category is the Black raven paradox [Mah04, Hut07]: Having
observed (cid:96) black ravens, what is the likelihood that all ravens are black. A more computer science
problem is (inﬁnite horizon) reinforcement learning, where predicting the inﬁnite future is necessary
for evaluating a policy. See Section 6 for these and other applications.
Discrete MDL. Let M = {Q1 ,Q2 ,...} be a countable class of models=theories=hypotheses=
probabilities over sequences X ∞ , sorted w.r.t. to their complexity=codelength K (Qi ) = 2log2 i (say),
containing the unknown true sampling distribution P . Our main result will be for arbitrary measur-
able spaces X , but to keep things simple in the introduction, let us illustrate MDL for ﬁnite X .
In this case, we deﬁne Qi (x) as the Qi -probability of data sequence x ∈ X (cid:96) . It is possible to code x
in logP (x)−1 bits, e.g. by using Huffman coding. Since x is sampled from P , this code is optimal
(shortest among all preﬁx codes). Since we do not know P , we could select the Q ∈M that leads to
the shortest code on the observed data x. In order to be able to reconstruct x from the code we need
to know which Q has been chosen, so we also need to code Q, which takes K (Q) bits. Hence x can
be coded in minQ∈M{−logQ(x)+K (Q)} bits. MDL selects as model the minimizer
Q∈M{− log Q(x) + K (Q)}
MDLx := arg min
Main result. Given x, the true predictive probability of some “future” event A is P [A|x], e.g. A
could be x(cid:96)+1:(cid:96)+h or any other measurable set of sequences (see Section 3 for proper deﬁnitions).

1

We consider the sequence of predictive measures MDLx [·|x] for (cid:96) = 0,1,2,... selected by MDL. Our
main result is that

MDLx [·|x] converges to P [·|x] in total variation distance for (cid:96) → ∞ with P -probability 1

(see Theorem 1). The analogous result for Bayesian prediction is well-known, and an immediate
corollary of Blackwell&Dubin’s celebrated merging-of-opinions theorem [BD62]. Our primary con-
tribution is to prove the analogous result for MDL. A priori it is not obvious that it holds at all, and
indeed the proof turns out to be much more complex.
Motivation. The results above hold for completely arbitrary countable model classes M. No inde-
pendence, ergodicity, stationarity, identiﬁability, or other assumption need to be made.
The bulk of previous results for MDL are for continuous model classes [Gr ¨u07]. Much has been
shown for classes of independent identically distributed (i.i.d.) random variables [BC91, Gr ¨u07].
Many results naturally generalize to stationary-ergodic sequences like (k th-order) Markov. For in-
stance, asymptotic consistency has been shown in [Bar85]. There are many applications violating
these assumptions, some of them are presented below and in Section 6. For MDL to work, P needs
to be in M or at least close to some Q ∈M, and there are interesting environments that are not even
close to being stationary-ergodic or i.i.d.
Non-i.i.d. data is pervasive [AHRU09]; it includes all time-series prediction problems like weather
forecasting and stock market prediction [CBL06]. Indeed, these are also perfect examples of non-
ergodic processes. Too much green house gases, a massive volcanic eruption, an asteroid impact,
or another world war could change the climate/economy irreversibly. Life is also not ergodic; one
inattentive second in a car can have irreversible consequences. Also stationarity is easily violated
in multi-agent scenarios: An environment which itself contains a learning agent is non-stationary
(during the relevant learning phase). Extensive games and multi-agent reinforcement learning are
classical examples [WR04].
Often it is assumed that the true distribution can be uniquely identiﬁed asymptotically. For non-
ergodic environments, asymptotic distinguishability can depend on the realized observations, which
prevent a prior reduction or partitioning of M. Even if principally possible, it can be practically
burdensome to do so, e.g. in the presence of approximate symmetries. Indeed this problem is the
primary reason for considering predictive MDL. MDL might never identify the true distribution, but
our main result shows that the sequentially selected models become predictively indistinguishable.
For arbitrary countable model classes, the following results are known: The MDL one-step lookahead
predictor (i.e. h = 1) of three variants of MDL converges to the true predictive distribution. The
proof technique used in [PH05] is inherently limited to ﬁnite h. Another general consistency result
is presented in [Gr ¨u07, Thm.5.1]. Consistency is shown (only) in probability and the predictive
implications of the result are unclear. A stronger almost sure result is alluded to, but the given
reference to [BC91] contains only results for i.i.d. sequences which do not generalize to arbitrary
classes. So existing results for discrete MDL are far less satisfactory than the elegant Bayesian
is useful. A semi-parametric problem class (cid:83)∞
merging-of-opinions result.
The countability of M is the severest restriction of our result. Nevertheless the countable case
or other estimate of Md [Gr ¨u07]. Alternatively, (cid:83)
d=1Md with Md = {Qθ,d : θ ∈ IRd} (say) can be
reduced to a countable class M = {Pd} for which our result holds, where Pd is a Bayes or NML
dMd could be reduced to a countable class by
considering only computable parameters θ . Essentially all interesting model classes contain such
a countable topologically dense subset. Under certain circumstances MDL still works for the non-
computable parameters [Gr ¨u07]. Alternatively one may simply reject non-computable parameters
on philosophical grounds [Hut05]. Finally, the techniques for the countable case might aid proving
general results for continuous M, possibly along the lines of [Rya09].
Contents. The paper is organized as follows: In Section 2 we provide some insights how MDL
works in restricted settings, what breaks down for general countable M, and how to circumvent the
problems. The formal development starts with Section 3, which introduces notation and our main
result. The proof for ﬁnite M is presented in Section 4 and for denumerable M in Section 5. In
Section 6 we show how the result can be applied to sequence prediction, classiﬁcation and regression,
discriminative learning, and reinforcement learning. Section 7 discusses some MDL variations.

2

2 Facts, Insights, Problems

Before starting with the formal development, we describe how MDL works in some restricted set-
tings, what breaks down for general countable M, and how to circumvent the problems. For deter-
ministic environments, MDL reduces to learning by elimination, and results can easily be understood.
Consistency of MDL for i.i.d. (and stationary-ergodic) sources is also intelligible. For general M,
MDL may no longer converge to the true model. We have to give up the idea of model identiﬁcation,
and concentrate on predictive performance.
Deterministic MDL = elimination learning. For a countable class M = {Q1 ,Q2 ,...} of de-
terministic theories=models=hypotheses=sequences, sorted w.r.t. to their complexity=codelength
K (Qi ) = 2log2 i (say) it is easy to see why MDL works: Each Q is a model for one inﬁnite se-
1:∞ , i.e. Q(xQ ) = 1. Given the true observations x ≡ xP
quence xQ
1:(cid:96) so far, MDL selects the simplest
1:(cid:96) and for h = 1 predicts xQ
Q consistent with xP
(cid:96)+1 . This (and potentially other) Q becomes (forever)
inconsistent if and only if the prediction was wrong. Assume the true model is P = Qm . Since
elimination occurs in order of increasing index i, and Qm never makes any error, MDL makes at
most m − 1 prediction errors. Indeed, what we have described is just classical Gold style learning
by elimination. For 1 < h < ∞, the prediction xQ
(cid:96)+1:(cid:96)+h may be wrong only on xQ
(cid:96)+h , which causes
h wrong predictions before the error is revealed. (Note that at time (cid:96) only xP
(cid:96) is revealed.) Hence
the total number of errors is bounded by h · (m− 1). The bound is for instance attained on the class
consisting of Qi = 1ih0∞ , and the true sequence switches from 1 to 0 after having observed m · h
ones. For h = ∞, a wrong prediction gets eventually revealed. Hence each wrong Qi (i < m) gets
eventually eliminated, i.e. P gets eventually selected. So for h = ∞ we can (still/only) show that the
number of errors is ﬁnite. No bound on the number of errors in terms of m only is possible. For
instance, for M = {Q1 = 1∞ ,Q2 = P = 1n0∞ }, it takes n time steps to reveal that prediction 1∞ is
wrong, and n can be chosen arbitrarily large.
Comparison of deterministic↔probabilistic and MDL↔Bayes. The ﬂavor of results carries over
to some extent to the probabilistic case. On a very abstract level even the line of reasoning carries
over, although this is deeply buried in the sophisticated mathematical analysis of the latter. So the
special deterministic case illustrates the more complex probabilistic case. The differences are as
follows: In the probabilistic case, the true P can in general not be identiﬁed anymore. Further, while
the Bayesian bound trivially follows from the 1/2-century old classical merging of opinions result
[BD62], the corresponding MDL bound we prove in this paper is more difﬁcult to obtain.
(cid:80)(cid:96)
(cid:80)
Consistency of MDL for stationary-ergodic sources. For an i.i.d. class M, the law of large num-
t=1Zt → KL(P ||Q) :=
zero, which is the case if and only if P = Q, or logP (x1:(cid:96) )− logQ(x1:(cid:96) ) ≡ (cid:80)(cid:96)
bers applied to the random variables Zt := log[P (xt )/Q(xt )] implies 1
(cid:96)
P (x1 )log[P (x1 )/Q(x1 )] with P -probability 1. Either the Kullback-Leibler (KL) divergence is
x1
t=1Z(cid:96) ∼ KL(P ||Q)(cid:96) →
∞, i.e. asymptotically MDL does not select Q. For countable M, a reﬁnement of this argument
shows that MDL eventually selects P [BC91]. This reasoning can be extended to stationary-ergodic
M, but essentially not beyond. To see where the limitation comes from, we present some troubling
examples.
Trouble makers. For instance, let P be a Bernoulli(θ0 ) process, but let the Q-probability that
xt = 1 be θt , i.e. time-dependent (still assuming independence). For a suitably converging but “os-
cillating” (i.e. inﬁnitely often larger and smaller than its limit) sequence θt → θ0 one can show that
log[P (x1:t )/Q(x1:t )] converges to but oscillates around K (Q) − K (P ) w.p.1, i.e. there are non-
stationary distributions for which MDL does not converge (not even to a wrong distribution).
One idea to solve this problem is to partition M, where two distributions are in the same partition
if and only if they are asymptotically indistinguishable (like P and Q above), and then ask MDL
to only identify a partition. This approach cannot succeed generally, whatever particular criterion is
used, for the following reason: Let P (x1 ) > 0 ∀x1 . For x1 = 1, let P and Q be asymptotically indis-
tinguishable, e.g. P = Q on the remainder of the sequence. For x1 = 0, let P and Q be asymptotically
distinguishable distributions, e.g. different Bernoullis. This shows that for non-ergodic sources like
this one, asymptotic distinguishability depends on the drawn sequence. The ﬁrst observation can lead
to totally different futures.
Predictive MDL avoids trouble. The Bayesian posterior does not need to converge to a single (true
or other) distribution, in order for prediction to work. We can do something similar for MDL. At

3

each time we still select a single distribution, but give up the idea of identifying a single distribu-
tion asymptotically. We just measure predictive success, and accept inﬁnite oscillations. That’s the
approach taken in this paper.

3 Notation and Main Result

The formal development starts with this section. We need probability measures and ﬁlters for inﬁnite
sequences, conditional probabilities and densities, the total variation distance, and the concept of
merging (of opinions), in order to formally state our main result.
Measures on sequences. Let (Ω := X ∞ ,F ,P ) be the space of inﬁnite sequences with natural ﬁl-
tration and product σ -ﬁeld F and probability measure P . Let ω ∈ Ω be an inﬁnite sequence sam-
pled from the true measure P . Except when mentioned otherwise, all probability statements and
expectations refer to P , e.g. almost surely (a.s.) and with probability 1 (w.p.1) are short for with
P -probability 1 (w.P .p.1). Let x = x1:(cid:96) = ω1:(cid:96) be the ﬁrst (cid:96) symbols of ω .
For countable X , the probability that an inﬁnite sequence starts with x is P (x) := P [{x}×X ∞ ]. The
conditional distribution of an event A given x is P [A|x] := P [A ∩ ({x}×X ∞ )]/P (x), which exists
w.p.1. For other probability measures Q on Ω, we deﬁne Q(x) and Q[A|x] analogously. General X
are considered at the end of this section.
Convergence in total variation. P is said to be absolutely continuous relative to Q, written
P (cid:28) Q :⇔ [Q[A] = 0 implies P [A] = 0 for all A ∈ F ]
P and Q are said to be mutually singular, written P ⊥Q, iff there exists an A ∈ F for which P [A] = 1
(cid:12)(cid:12)Q[A|x] − P [A|x](cid:12)(cid:12)
and Q[A] = 0. The total variation distance (tvd) between Q and P given x is deﬁned as
d(P , Q|x) := sup
(1)
A∈F
Q is said to predict P in tvd (or merge with P ) if d(P ,Q|x) → 0 for (cid:96)(x) → ∞ with P -probability
1. Note that this in particular implies, but is stronger than one-step predictive on- and off-sequence
convergence Q(x(cid:96)+1 = a(cid:96)+1 |x1:(cid:96) ) − P (x(cid:96)+1 = a(cid:96)+1 |x1:(cid:96) ) → 0 for any a, not necessarily equal ω
[KL94]. The famous Blackwell and Dubins convergence result [BD62] states that if P is absolutely
continuous relative to Q, then (and only then [KL94]) Q merges with P :
(cid:96)(x) → ∞
If P (cid:28) Q then d(P , Q|x) → 0 w.p.1 for
(cid:80)
Q∈MQ[A]wQ with wQ > 0 ∀Q and (cid:80)
Bayesian prediction. This result can immediately be utilized for Bayesian prediction. Let M :=
{Q1 ,Q2 ,Q3 ,...} be a countable (ﬁnite or inﬁnite) class of probability measures, and Bayes[A] :=
Q∈MwQ = 1. If the model assumption P ∈ M holds, then
obviously P (cid:28) Bayes, hence Bayes merges with P , i.e. d(P ,Bayes|x) → 0 w.p.1 for all P ∈ M.
Unlike many other Bayesian convergence and consistency theorems, no (independence, ergodicity,
stationarity, identiﬁability, or other) assumption on the model class M need to be made. The analo-
gous result for MDL is as follows:
Theorem 1 (MDL predictions) Let M be a countable class of probability measures on X ∞ con-
Q∈M{− log Q(x) + K (Q)} with (cid:88)
taining the unknown true sampling distribution P . No (independence, ergodicity, stationarity, iden-
tiﬁability, or other) assumptions need to be made on M. Let
MDLx := arg min
Q∈M
be the measure selected by MDL at time (cid:96) given x ∈ X (cid:96) . Then the predictive distributions MDLx [·|x]
(cid:12)(cid:12)MDLx [A|x] − P [A|x](cid:12)(cid:12) → 0
converge to P [·|x] in the sense that
d(P , MDLx |x) ≡ sup
A∈F
case (cid:80)
K (Q) is usually interpreted and deﬁned as the length of some preﬁx code for Q,
in which
Q is chosen as complexity, by Bayes rule Pr(Q|x) =
Q2−K (Q) ≤ 1.
If K (Q) := log2w−1
Q(x)wQ/Bayes(x), the maximum a posteriori estimate MAPx := argmaxQ∈M{Pr(Q|x)} ≡MDLx .
Hence the theorem also applies to MAP. The proof of the theorem is surprisingly subtle and complex
compared to the analogous Bayesian case. One reason is that MDLx (x) is not a measure on X ∞ .

(cid:96)(x) → ∞ w.p.1

2−K (Q) < ∞

for

4

Arbitrary X . For arbitrary measurable spaces X , deﬁnitions are more subtle, essentially because
point probabilities Q(x) have to be replaced by probability densities relative to some base measure
M , usually Lebesgue for X = IRd , counting measure for countable X , and e.g. M [·] = Bayes[·] for
general X . We have taken care of that all results and proofs are valid unchanged for general X ,
with Q(·) deﬁned as a version of the Radon-Nikodym derivative relative to M . We spare the reader
the details, since they are completely standard and do not add any value to this paper, and space is
limited. The formal deﬁnitions of Q(x) and Q[A|x] can be found e.g. in [Doo53, BD62]. Note that
MDLx is independent of the particular choice of M .

4 Proof for Finite Model Class
We ﬁrst prove Theorem 1 for ﬁnite model classes M. For this we need the following Deﬁnition and
Lemma:
Deﬁnition 2 (Relations between Q and P ) For any probability measures Q and P , let
• Qr+Qs =Q be the Lebesgue decomposition of Q relative to P into an absolutely continuous
derivative, i.e. Qr [A] = (cid:82)
non-negative measure Qr (cid:28) P and a singular non-negative measure Qs⊥P .
• g(ω) := dQr /dP = lim(cid:96)→∞ [Q(x1:(cid:96) )/P (x1:(cid:96) )] be (a version of) the Radon-Nikodym
A g dP .
• Ω◦ := {ω : Q(x1:(cid:96) )/P (x1:(cid:96) ) → 0} ≡ {ω : g(ω) = 0}.
• (cid:126)Ω := {ω : d(P ,Q|x) → 0 for (cid:96)(x) → ∞}.
It is well-known that the Lebesgue decomposition exists and is unique. The representation of
the Radon-Nikodym derivative as a limit of local densities can e.g. be found in [Doo53, VII§8]:
Z r/s
(ω) := Qr/s (x1:(cid:96) )/P (x1:(cid:96) ) for (cid:96) = 1,2,3,... constitute two martingale sequences, which con-
verge w.p.1. Qr (cid:28) P implies that the limit Z r∞ is the Radon-Nikodym derivative dQr /dP . (Indeed,
(cid:96)
Doob’s martingale convergence theorem can be used to prove the Radon-Nikodym theorem.) Qs⊥P
implies Z r∞ = 0 w.p.1. So g is uniquely deﬁned and ﬁnite w.p.1.
Lemma 3 (Generalized merging of opinions) For any Q and P , the following holds:
(i) P (cid:28) Q if and only if P [Ω◦ ] = 0
(ii) P [Ω◦ ] = 0 implies P [(cid:126)Ω] = 1
(iii) P [Ω◦ ∪ (cid:126)Ω] = 1
(i) says that Q(x)/P (x) converges almost surely to a strictly positive value if and only if P is
absolutely continuous relative to Q, (ii) says that an almost sure positive limit of Q(x)/P (x) implies
that Q merges with P . (iii) says that even if P (cid:54)(cid:28) Q, we still have d(P ,Q|x) → 0 on almost every
sequence that has a positive limit of Q(x)/P (x).
(i⇐) Assume P [Ω◦ ]= 0: P [A]> 0 implies Q[A]≥Qr [A]= (cid:82)
Proof. Recall Deﬁnition 2.
(i⇒) Assume P (cid:28) Q: Choose a B for which P [B ] = 1 and Qs [B ] = 0. Now Qr [Ω◦ ] = (cid:82)
A g dP > 0, since g > 0 a.s. by assumption
P [Ω◦ ] = 0. Therefore P (cid:28) Q.
Ω◦ g dP = 0
implies 0 ≤ Q[B ∩ Ω◦ ] ≤ Qs [B ] + Qr [Ω◦ ] = 0 + 0. By P (cid:28) Q this implies P [B ∩ Ω◦ ] = 0, hence
P [Ω◦ ] = 0.
(ii) That P (cid:28) Q implies P [(cid:126)Ω] = 1 is Blackwell-Dubins’ celebrated result. The result now follows
from (i).
(iii) generalizes [BD62]. For P [Ω◦ ] = 0 it reduces to (ii). The case P [Ω◦ ] = 1 is trivial. Therefore
Assume Q[A] = 0. Using (cid:82)
Ω◦ g dP = 0, we get 0 = Qr [A] = (cid:82)
A g dP = (cid:82)
we can assume 0 < P [Ω◦ ] < 1. Consider measure P (cid:48) [A] := P [A|B ] conditioned on B := Ω \Ω◦ .
A\Ω◦ g dP . Since g > 0 outside
Ω◦ , this implies P [A \Ω◦ ] = 0. So P (cid:48) [A] = P [A ∩B ]/P [B ] = P [A \Ω◦ ]/P [B ] = 0. Hence P (cid:48) (cid:28) Q.
Now (ii) implies d(P (cid:48) ,Q|x) → 0 with P (cid:48) probability 1. Since P (cid:48) (cid:28) P we also get d(P (cid:48) ,P |x) → 0
w.P (cid:48) .p.1.
Together this implies 0 ≤ d(P ,Q|x) ≤ d(P (cid:48) ,P |x)+d(P (cid:48) ,Q|x)→ 0 w.P (cid:48) .p.1, i.e. P (cid:48) [(cid:126)Ω] = 1. The claim
now follows from

[(i)+[BD62]]
[generalizes (ii)]

5

P [Ω◦ ∪ (cid:126)Ω] = P (cid:48) [Ω◦ ∪ (cid:126)Ω]P [Ω \ Ω◦ ] + P [Ω◦ ∪ (cid:126)Ω|Ω◦ ]P [Ω◦ ]
= 1 · P [Ω \ Ω◦ ] + 1 · P [Ω◦ ] = P [Ω] = 1

The intuition behind the proof of Theorem 1 is as follows. MDL will asymptotically not select Q
for which Q(x)/P (x) → 0. Hence for those Q potentially selected by MDL, we have ω (cid:54)∈ Ω◦ , hence
ω ∈ (cid:126)Ω, for which d(P ,Q|x) → 0 (a.s.). The technical difﬁculties are for ﬁnite M that the eligible Q
depend on the sequence ω , and for inﬁnite M to deal with non-uniformly converging d, i.e. to infer
d(P ,MDLx |x) → 0.
Proof of Theorem 1 for ﬁnite M. Recall Deﬁnition 2, and let gQ ,Ω◦
Q ,(cid:126)ΩQ refer to some Q ∈ M ≡
{Q1 ,...,Qm}. The set of sequences ω for which some gQ for some Q ∈ M is undeﬁned has P -
measure zero, and hence can be ignored. Fix some sequence ω ∈ Ω for which gQ (ω) is deﬁned for
all Q ∈M, and let Mω := {Q ∈M : gQ (ω) = 0}.
Q∈M LQ (x), where LQ (x) := − log Q(x) + K (Q).
MDLx := arg min

+ K (Q) − K (P ) (cid:96)→∞−→ − log gQ (ω) + K (Q) − K (P )

Consider the difference
LQ (x) − LP (x) = − log
Q(x)
P (x)
For Q ∈Mω , the r.h.s. is +∞, hence
∀Q ∈Mω ∃(cid:96)Q∀(cid:96) > (cid:96)Q : LQ (x) > LP (x)
Since M is ﬁnite, this implies
∀(cid:96) > (cid:96)0 ∀Q ∈Mω : LQ (x) > LP (x), where
(cid:96)0 := max{(cid:96)Q : Q ∈ Mω } < ∞
focus on Q ∈Mω :=M \Mω . Let Ω1 := (cid:84)
Therefore, since P ∈ M, we have MDLx (cid:54)∈ Mω ∀(cid:96) > (cid:96)0 , so we can safely ignore all Q ∈ Mω and
Q ∪ (cid:126)ΩQ ). Since P [Ω1 ] = 1 by Lemma 3(iii), we
(Ω◦
Q∈Mω
can also assume ω ∈ Ω1 .
Q ∈ Mω ⇒ gQ (ω) > 0 ⇒ ω (cid:54)∈ Ω◦
Q ⇒ ω ∈ (cid:126)ΩQ ⇒ d(P , Q|x) → 0
d(P , MDLx |x) ≤ sup
d(P , Q|x) → 0
This implies
Q∈Mω
where the inequality holds for (cid:96) > (cid:96)0 and the limit holds, since M is ﬁnite. Since the set of ω
excluded in our considerations has measure zero, d(P ,MDLx |x) → 0 w.p.1, which proves the
theorem for ﬁnite M.

5 Proof for Countable Model Class
The proof in the previous Section crucially exploited ﬁniteness of M. We want to prove that the
probability that MDL asymptotically selects “complex” Q is small. The following Lemma estab-
lishes that the probability that MDL selects a speciﬁc complex Q inﬁnitely often is small.

Proof.

Lemma 4 (MDL avoids complex probability measures Q) For any Q and P we have
P [Q(x)/P (x) ≥ c inﬁnitly often] ≤ 1/c.
P [∀(cid:96)0∃(cid:96) > (cid:96)0 :
Q(x)
P (x)
Q(x)
P (x)

(a)
= P [ lim
(cid:96)→∞
Q(x)
P (x)

≥ c] ≤

Q(x)
P (x)
(d)≤ 1
c

]

Q(x)
P (x)

]

(e)≡ 1
c

(b)≤ 1
c

E[lim
(cid:96)

≥ c]

(c)
=

]

1
c

E[lim
(cid:96)

E[

lim
(cid:96)

(a) is true by deﬁnition of the limit superior lim, (b) is Markov’s inequality, (c) exploits the fact that
the limit of Q(x)/P (x) exists w.p.1, (d) uses Fatou’s lemma, and (e) is obvious.

For sufﬁciently complex Q, Lemma 4 implies that LQ (x) > LP (x) for most x. Since convergence is
non-uniform in Q, we cannot apply the Lemma to all (inﬁnitely many) complex Q directly, but need
to lump them into one ¯Q.

6

P (x) 2K (P )−K (Qi ) ≥ 1]
Qi (x)

Proof of Theorem 1 for countable M. Let the Q ∈ M = {Q1 ,Q2 ,...} be ordered somehow,
(cid:102)M := {Qm+1 ,Qm+2 ,...} be the set of “complex” Q. We show that the probability that MDL selects
e.g. in increasing order of complexity K (Q), and P = Qn . Choose some (large) m ≥ n and let
P [MDLx ∈ (cid:102)M inﬁnitely often] ≡ P [∀(cid:96)0∃(cid:96) > (cid:96)0 : MDLx ∈ (cid:102)M]
inﬁnitely often complex Q is small:
≤ P [∀(cid:96)0∃(cid:96) > (cid:96)0 ∧ Q ∈ (cid:102)M : LQ (x) ≤ LP (x)] = P [∀(cid:96)0∃(cid:96) > (cid:96)0 : sup
i>m
P (x) δ 2K (P ) ≥ 1]
(b)≤ δ 2K (P )
(c)≤ ε
(a)≤ P [∀(cid:96)0∃(cid:96) > (cid:96)0 :
¯Q(x)
The ﬁrst three relations follow immediately from the deﬁnition of the various quantities. Bound (a)
∞(cid:88)
is the crucial “lumping” step. First we bound
¯Q(x)
2−K (Qi ) ≤
2−K (Qi ) = δ
Qi (x)
(cid:88)
(cid:88)
P (x)
P (x)
i=m+1
2−K (Qi ) < ∞,
Qi (x)2−K (Qi ) ,
1
¯Q(x) :=
δ :=
δ
While MDL· [·] is not a (single) measure on Ω and hence difﬁcult to deal with, ¯Q is a proper prob-
i>m
i>m
ability measure on Ω. In a sense, this step reduces MDL to Bayes. Now we apply Lemma 4 in (b)
to the (single) measure ¯Q. The bound (c) holds for sufﬁciently large m = mε (P ), since δ → 0 for
m → ∞. This shows that for the sequence of MDL estimates
{MDLx1:(cid:96) : (cid:96) > (cid:96)0} ⊆ {Q1 , ..., Qm} with probability at least 1 − ε
Hence the already proven Theorem 1 for ﬁnite M implies that d(P ,MDLx |x) → 0 with probability
at least 1− ε. Since convergence holds for every ε > 0, it holds w.p.1.

sup
i>m

Qi (x)
P (x)

,

6

Implications

Due to its generality, Theorem 1 can be applied to many problem classes. We illustrate some imme-
diate implications of Theorem 1 for time-series forecasting, classiﬁcation, regression, discriminative
learning, and reinforcement learning.
Time-series forecasting. Classical online sequence prediction is concerned with predicting x(cid:96)+1
from (non-i.i.d.) sequence x1:(cid:96) for (cid:96) = 1,2,3,.... Forecasting farther into the future is possible by
predicting x(cid:96)+1:(cid:96)+h for some h > 0. Hence Theorem 1 implies good asymptotic (multi-step) predic-
tions. Ofﬂine learning is concerned with training a predictor on x1:(cid:96) for ﬁxed (cid:96) in-house, and then
selling and using the predictor on x(cid:96)+1:∞ without further learning. Theorem 1 shows that for enough
training data, predictions “post-learning” will be good.
Classiﬁcation and Regression. In classiﬁcation (discrete X ) and regression (continuous X ), a sam-
ple is a set of pairs D = {(y1 ,x1 ),...,(y(cid:96) ,x(cid:96) )}, and a functional relationship ˙x = f ( ˙y)+noise, i.e. a
conditional probability P ( ˙x| ˙y) shall be learned. For reasons apparent below, we have swapped the
usual role of ˙x and ˙y . The dots indicate ˙x ∈ X and ˙y ∈ Y ), while x = x1:(cid:96) ∈ X (cid:96) and y = y1:(cid:96) ∈ Y (cid:96) .
If we assume that also ˙y follows some distribution, and start with a countable model class M of
joint distributions Q( ˙x, ˙y) which contains the true joint distribution P ( ˙x, ˙y), our main result implies
that MDLD [( ˙x, ˙y)|D ] converges to the true distribution P ( ˙x, ˙y). Indeed since/if samples are assumed
i.i.d., we don’t need to invoke our general result.
Discriminative learning. Instead of learning a generative [Jeb03] joint distribution P ( ˙x, ˙y), which
requires model assumptions on the input ˙y , we can discriminatively [LSS07] learn P (·| ˙y) directly
without any assumption on y (not even i.i.d). We can simply treat y1:∞ as an oracle to all Q, deﬁne
M(cid:48) = {Q(cid:48)} with Q(cid:48) (x) := Q(x|y1:∞ ), and apply our main result to M(cid:48) , leading to MDL(cid:48) x [A|x] →
P (cid:48) [A|x], i.e. MDLx|y1:∞ [A|x,y1:∞ ] → P [A|x,y1:∞ ]. If y1 ,y2 ,... are conditionally independent, or
more generally for any causal process, we have Q(x|y) = Q(x|y1:∞ ). Since the x given y are not
identically distributed, classical MDL consistency results for i.i.d. or stationary-ergodic sources do
not apply. The following corollary formalizes our ﬁndings:
Corollary 5 (Discriminative MDL) Let M (cid:51) P be a class of discriminative causal distributions
Q[·|y1:∞ ], i.e. Q(x|y1:∞ ) = Q(x|y), where x = x1:(cid:96) and y = y1:(cid:96) . Regression and classiﬁcation are

7

(cid:12)(cid:12)MDLx|y [A|x,y ] −
P [A|x,y ](cid:12)(cid:12) → 0 for (cid:96)(x) → ∞, P [·|y1:∞ ] almost surely, for every sequence y1:∞ .
typical examples. Further assume M is countable. Let MDLx|y := argminQ∈M{−logQ(x|y) +
K (Q)} be the discriminative MDL measure (at time (cid:96) given x,y ). Then supA
For ﬁnite Y and conditionally independent x, the intuitive reason how this can work is as follows:
If ˙y appears in y1:∞ only ﬁnitely often, it plays asymptotically no role; if it appears inﬁnitely often,
then P (·| ˙y) can be learned. For inﬁnite Y and deterministic M, the result is also intelligible: Every
˙y might appear only once, but probing enough function values xt = f (yt ) allows to identify the
function.
Reinforcement learning (RL). In the agent framework [RN03], an agent interacts with an envi-
ronment in cycles. At time t, an agent chooses an action yt based on past experience x<t ≡
(cid:81)(cid:96)
(x1 ,...,xt−1 ) and past actions y<t with probability π(yt |x<t y<t ) (say). This leads to a new
perception xt with probability µ(xt |x<t y1:t ) (say). Then cycle t + 1 starts. Let P (xy) =
t=1µ(xt |x<t y1:t )π(yt |x<t y<t ) be the joint interaction probability. We make no (Markov, sta-
tionarity, ergodicity) assumption on µ and π . They may be POMDPs or beyond.
{ν1 ,ν2 ,...} (cid:51) µ, let M = {Qi } with Qi (x|y) = (cid:81)(cid:96)
Corollary 6 (Single-agent MDL) For a ﬁxed policy=agent π , and a class of environments
t=1 νi (xt |x<t y1:t ). Then d(P [·|y ],MDLx|y ) → 0
with joint P -probability 1.
The corollary follows immediately from the previous corollary and the facts that the Qi are causal
and that with P [·|y1:∞ ]-probability 1 ∀y1:∞ implies w.P .p.1 jointly in x and y .
In reinforcement learning [SB98], the perception xt := (ot ,rt ) consists of some regular observation
ot and a reward rt ∈ [0,1]. Goal is to ﬁnd a policy which maximizes accrued reward in the long run.
The previous corollary implies

Corollary 7 (Fixed-policy MDL value function convergence) Let VP [xy ]
:= EP [·|xy ] [r(cid:96)+1 +
γ r(cid:96)+2 + γ 2 r(cid:96)+3 + ...] be the future γ -discounted P -expected reward sum (true value of π ), and simi-
larly VQi [xy ] for Qi . Then the MDL value converges to the true value, i.e. VMDLx|y [xy ]−VP [xy ]→ 0,
w.P .p.1. for any policy π .
Proof. The corollary follows from the general inequality |EP [f ] − EQ [f ]| ≤ sup|f | · supA |P [A] −
Q[A]| by inserting f := r(cid:96)+1 + γ r(cid:96)+2 + γ 2 r(cid:96)+3 + ... and P = P [·|xy ] and Q = MDLx|y [·|xy ], and
using 0 ≤ f ≤ 1/(1− γ ) and Corollary 6.

Since the value function probes the inﬁnite future, we really made use of our convergence result in
total variation. Corollary 7 shows that MDL approximates the true value asymptotically arbitrarily
well. The result is weaker than it may appear. Following the policy that maximizes the estimated
(MDL) value is often not a good idea, since the policy does not explore properly [Hut05]. Neverthe-
less, it is a reassuring non-trivial result.

7 Variations

MDL is more a general principle for model selection than a uniquely deﬁned procedure. For instance,
there are crude and reﬁned MDL [Gr ¨u07], the related MML principle [Wal05], a static, a dynamic,
(cid:81)(cid:96)
and a hybrid way of using MDL for prediction [PH05], and other variations. For our setup, we could
have deﬁned multi-step lookahead prediction as a product of single-step predictions: MDLI(x1:(cid:96) ) :=
t=1MDLx<t (xt |x<t ) and MDLI(z |x) = MDLI(xz )/MDLI(x), which is a more incremental MDL
version. Both, MDLx and MDLI are ‘static’ in the sense of [PH05], and each allows for a dynamic
and a hybrid version. Due to its incremental nature, MDLI likely has better predictive properties than
MDLx , and conveniently deﬁnes a single measure over X ∞ , but inconveniently is (cid:54)∈M. One reason
for using MDL is that it can be computationally simpler than Bayes. E.g. if M is a class of MDPs,
then MDLx is still an MDP and hence tractable, but MDLI like Bayes are a nightmare to deal with.
Acknowledgements. My thanks go to Peter Sunehag for useful discussions.

8

References
[AHRU09] M.-R. Amini, A. Habrard, L. Ralaivola, and N. Usunier, editors. Learning from non-IID data:
Theory, Algorithms and Practice (LNIDD’09), Bled, Slovenia, 2009.
[Bar85] A. R. Barron. Logically Smooth Density Estimation. PhD thesis, Stanford University, 1985.
IEEE Transactions on
[BC91] A. R. Barron and T. M. Cover. Minimum complexity density estimation.
Information Theory, 37:1034–1054, 1991.
[BD62] D. Blackwell and L. Dubins. Merging of opinions with increasing information. Annals of Mathemat-
ical Statistics, 33:882–887, 1962.
[CBL06] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,
2006.
[Doo53] J. L. Doob. Stochastic Processes. Wiley, New York, 1953.
[Gr ¨u07] P. D. Gr ¨unwald. The Minimum Description Length Principle. The MIT Press, Cambridge, 2007.
[Hut03] M. Hutter. Convergence and loss bounds for Bayesian sequence prediction. IEEE Transactions on
Information Theory, 49(8):2061–2067, 2003.
[Hut05] M. Hutter. Universal Artiﬁcial Intelligence: Sequential Decisions based on Algorithmic Probability.
Springer, Berlin, 2005. 300 pages, http://www.hutter1.net/ai/uaibook.htm.
[Hut07] M. Hutter. On universal prediction and Bayesian conﬁrmation. Theoretical Computer Science,
384(1):33–48, 2007.
[Jeb03] T. Jebara. Machine Learning: Discriminative and Generative. Springer, 2003.
[KL94] E. Kalai and E. Lehrer. Weak and strong merging of opinions. Journal of Mathematical Economics,
23:73–86, 1994.
[LSS07] P. Long, R. Servedio, and H. U. Simon. Discriminative learning can succeed where generative learning
fails. Information Processing Letters, 103(4):131–135, 2007.
[Mah04] P. Maher. Probability captures the logic of scientiﬁc conﬁrmation. In C. Hitchcock, editor, Contem-
porary Debates in Philosophy of Science, chapter 3, pages 69–93. Blackwell Publishing, 2004.
J. Poland and M. Hutter. Asymptotics of discrete MDL for online prediction. IEEE Transactions on
Information Theory, 51(11):3780–3795, 2005.
[RN03] S. J. Russell and P. Norvig. Artiﬁcial Intelligence. A Modern Approach. Prentice-Hall, Englewood
Cliffs, NJ, 2nd edition, 2003.
[Rya09] D. Ryabko. Characterizing predictable classes of processes. In Proc. 25th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI’09), Montreal, 2009.
[SB98] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA,
1998.
[Wal05] C. S. Wallace. Statistical and Inductive Inference by Minimum Message Length. Springer, Berlin,
2005.
[WR04] M. Weinberg and J. S. Rosenschein. Best-response multiagent learning in non-stationary environ-
ments. In Proc. 3rd International Joint Conf. on Autonomous Agents & Multi Agent Systems (AA-
MAS’04), pages 506–513, 2004.

[PH05]

9


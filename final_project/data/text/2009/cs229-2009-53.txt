CS229: Grant Prediction with Network Features

Stefan Krawczyk
Computer Science Depar tment, Stanford University
stefank@cs.stanford.edu

ABSTRACT
This pro ject investigates supervised learning to predict grant
proposal approval or rejection, using the Mimir pro ject’s
data on external grants applied for by Stanford faculty. Base-
line features relating to the grant itself and faculty features
are used, in particular the novel use of a faculty member’s
network to extract features is investigated and its eﬀects on
grant prediction.

1.
INTRODUCTION
Grants take time and eﬀort to put together and there is
often anxiety related to their outcome. Wouldn’t it be great
for faculty members to assess their grant proposal before
sending it out?

Lots of research lately has been done to try to study and
analyze networks were people are represented as nodes and
edges represent some type of relationship. In social networks
this is a friend edge, in citation networks this is a cite from
one paper to an older paper, or in collaboration networks the
edge represents a link between co-authors. The properties
of such networks then have been studied.

Burt[2] showed that people who sit in structural holes, or
places of the network where they connect distant compo-
nents, look to gain quicker promotions, better salaries, bet-
ter ideas than their peers in the electronics company he ana-
lyzed. Burt[1] also produces networks constraints which help
to detect these structural holes. This shows that there are
certain characteristics of a network that could be amenable
to a machine learning algorithm.

Having access to a Stanford faculty data set, ripe for feature
extraction, containing information relating faculty to each
other through relationship links, in addition to supplemental
information about age, tenure, etc, a network of faculty as
vertices and edges as relationships can be produced.

The saying “success breeds success ”can also be hypothesized

to mean that the relationships that one has with other peo-
ple helps in determining your own success. Can we given
the diﬀerent network relationship information, help predict
grant approval and rejection of Stanford faculty?

This paper is structured in the following way: section 2 gives
an overview of the data, section 3 details the approach, sec-
tion 4 talks about the feature sets, section ?? showcases the
results and section 6 wraps up with the conclusion and fu-
ture work.

2. DATASET
The data comprises of a collection of SQL databases, that
was put together for the Mimir pro ject wanting to study the
ﬂow of knowledge/ideas in an academic network. It contains
information from 1995 to 2007 on:

• external grants applied for by faculty: the applicants;
the amount proposed and awarded; sponsoring orga-
nization; school and department the grant is under;
grant approval or rejection; and whether there are con-
tinuing grants.
• dissertation committees faculty sit on and thesis de-
tails of the student
• co-taught courses (from 1999)
• co-authored publications (not complete)
• supplemental information on the faculty like tenure
status, age, gender, tenure status, position, number
of appointments, courtesy appointments.

This data is both quite interesting and unique, as well as
plentiful. The grant data in particular has over 22 thousand
grant proposals, with a roughly 50/50 split in approvals and
rejections.

In addition to grant prediction, there are many other op-
tions with which one could pursue both supervised or unsu-
pervised learning. For instance the data is quite amenable
to clustering; one could try to ﬁgure out which department
courtesy appointments really belong to based on the data
and see how that changes with time. There is also lots of
prediction based tasks possible for example predicting de-
parture of faculty, or number of students graduating in a
given year, etc.

3. APPROACH
The data resided in SQL tables which after some manipula-
tion was then output to CSV ﬁles. Feature extraction was
then performed on the data to produce the three diﬀerent
feature sets.

Once the feature sets were produced they were fed into three
diﬀerent machine learning algorithms: logistic regression, a
linear classiﬁer, and a support vector machine (SVM). Com-
binations of the feature sets were explored to discern the
impact of the diﬀerent feature sets using two approaches:

4.1 Grant Signature Features
These features were extracted entirely from the grant pro-
posal itself. These were to be the simple baseline features
that would be added to by the other feature sets.

It was comprised of: a bernoulli bag of word model on the
pro ject title, a bernoulli bag of word model on the spon-
soring organization, the amount proposed, the school id the
grant is from turned into a categorical feature, the depar-
ment id the grant is from turned into a categorical feature,
and the year.

1. randomly intermingling the years, not using any cu-
mulative features, thus training and testing on grants
from the entire time span. This was the quickest test
but not a realistic test.

2. training on all previous years and testing on the next
year - thus testing on each year from 1995 to 2007.
This would be the realistic test as people would want
to know how the current year’s grant proposals are
predicted.

Logistic regression was ﬁrst used to test out the diﬀerent
feature sets before running the other classiﬁers, as the turn
around time was much quicker.

The linear classiﬁer was chosen because it was a readily avail-
able package and because the data was rather high dimen-
sional, it was hypothesized that this classiﬁer should be able
to ﬁnd some decent hyperplane. Initially gradient descent
was used, but it kept getting stuck and was changed in favour
to use the quasi-newton method.

Lastly the SVM with a linear kernel was picked because
training it took the longest amount of time, and it was hoped
that it should be as good if not better than the linear clas-
siﬁer.

3.1 Software Packages
There were two software packages that were used, the Java
Universal Network/Graph[3] (JUNG) and the JavaNLP li-
brary.

JUNG was used for creating the network graphs so that
feature extraction on the graphs could be performed.
It
was chosen because it was in Java and would interface well
with the JavaNLP library and that it had a few network
measures/metrics that would be of use in feature extraction.

The JavaNLP library was utilized for all of the machine
learning algorithms.

4. FEATURES
The bulk of this pro ject’s work was in feature creation and
extraction. In addition to what is described below, the log
was taken of all continuous valued features, while the square
was take only of all continuous valued features that would
not cause an possible underﬂow or overﬂow to add to the
feature mix. The following describes the three feature sets
produced.

4.2 Non-network Faculty Features
This feature set was based on anything else that could be
extracted from the SQL tables that was not a network fea-
ture, that could be related to the faculty on the grant ap-
plication. It was decomposed into three subsets: professor,
department and school sets. When appropriate, cumulative
features that spanned the history until the current year were
also produced.

The professor set dealt with features directly attributable to
one particular facutly member and was the largest set out
of the three.

This set comprised of: gender, tenure status, primary de-
partment id (categorical feature), school id (categorical fea-
ture), primary job class code (categorical feature), number of
appointments, H-index (was only available for a select num-
ber of faculty), number of courtesy appointments, number of
dissertation students, number of publications (not complete
for any faculty), ethnicity as a categorical feature, age, hire
year, time at Stanford, total number of grants awarded last
year, total number of grants rejected last year, total grants
applied for last year, cumulative number of grants applied
for, previous year’s grant success rate, overall grant suc-
cess up to last year, total amount proposed, total amount
awarded, average amount proposed, ratio of total amount
awarded over total amount proposed, average amount pro-
posed for awarded grants, average amount proposed for re-
jected grants, and ratio of average proposed amount awarded
over average proposed amount rejected.

The school and department sets dealt with aggregate fea-
tures that covered the school and department respectively.
The idea would be that would help cover and school or de-
partment wide attributes related to grants. These sets had
features comprised of the aggregate grant related features
from the professor set.

4.3 Network Faculty Features
This feature set was largest and took the most time to pro-
duce out of all the sets. It was produced by running a lot
of network related metrics on the produced network graphs
for each faculty member in the graph.

Eight graphs were produced per year: dissertation commit-
tees; (not complete) co-authorship; co-taught classes; new
grants awarded in that year; grants continuing; grants con-
tinuing and awarded; grants rejected that year; and a com-
bined edge set. The edges also contained weights represent-
ing the number of interactions that the faculty had together,

Figure 1: An example of a network graph created.

for feature extraction weighted as well as unweighted ver-
sions were produced where possible. An example graph can
be seen in ﬁgure ??

The following was extracted from each graph for each fac-
ulty member: ﬁve structural hole signature measures[1]: ag-
gregate constraint, constraint, eﬀective size, eﬃciency, hi-
erarchy; barycenter value1 ;random walk betweeness value2 ;
betweeness centrality3 ; closenes centrality4 ; eigenvector cen-
trality5 ; clustering coeﬃcient6 ; number of total edges; radius
one and two features: number of tenured and untenured
links, number of awarded and continuing grants, number
of rejected grants, number of grant and publication edges
that overlapped, number of publications, neighbour edge in-
cidence; and cumulative weighted, unweigted and average
weight edge incidence.

5. RESULTS
For all the results we had a ma jority baseline of roughly 50%
for each test.

5.1 Randomly Intermingling the years
5.1.1 Logistic Regression
In ﬁgure 2 we see the results from running logistic regression.
The best testing accuracy came from the combined grant
and network feature sets, so a positive result for the use of
network features. From the graph we can also see that grant
features by themselves do not give anything above baseline,
while adding the other feature sets allows everything to be
just above it.

The convergence parameters where ﬁddled with extensively
for logistic regression, but did not yield any improvement
over these results.

1 sum of distances to each vertex
2measures the expected number of times a node is traversed
by a random walk averaged over all pairs of nodes
3how many shortest paths go through me
4based on average distance to each vertex
5 the fraction of time that a random walk will spend at that
vertex over an inﬁnite time horizon
6how dense is my network around me

Figure 2: Results of testing on randomly intermin-
gled years for logistic regression.

Figure 3: Results of testing on randomly intermin-
gled years for the linear classiﬁer.

5.1.2 Linear Classiﬁer
Counter-intuitively to what the results were in ﬁgure 2,
the grant features by themselves produce the best result.
Adding in the other features actually hurts the performance,
but adding in the network features hurts the least, even
though they overﬁt terribly.

Interestingly the overﬁtting with only the addition of the
network features, but not with the addition of the non-
network faculty suggests that the non-network features add
a lot more nosie to the data, while the network features allow
a very clear separation. Thus just adding the non-network
features to the grant features yields the worst results.

The convergence parameters on the quasi-newton method
that was being used in the linear classiﬁer was played with
extensively to try reduce the overﬁtting when using the grant
and network features. Overﬁtting was reduced slightly down
to 92%, but this did not change the testing accuracy much
at all.

5.1.3
SVM
The SVM was a disappointment. It somehow kept breaking
when the non-network faculty feature set was used, nor did
it perform anywhere near as well as the linear classiﬁer as
it should. The regularization parameter was played with to
no avail and thus only the random intermingling years test
was completed which is shown in ﬁgure 4. This suggests a
broken library7 as the kernel used was a linear one.

7 or user error

Figure 4: Results of testing on randomly intermin-
gled years for the support vector machine.

Figure 6: Testing on all years, training on the pre-
vious years for the linear classiﬁer

Overall the top features from any of the sets turned out to
be the bag of words on the title and sponsoring organization.

Table 1 shows the comparison between logsitic regression
and linear classiﬁer for the grant, and grant + network fea-
ture sets. Notice that logisistic regression was not able to
discern betters weights for the features as the linear clas-
siﬁer. They are largely similar, but with the addition of
the network features, the linear classiﬁer is then able to ﬁnd
better weights for these largely similar features.

Overall the addition of the network features changes the
top features to be more title oriented. No network features
appear in the top twenty weighted features in either logistic
regression or the linear classiﬁer when using the grant +
network feature sets.

Looking at table 2 the top features when using the non-
network features sees the top being largely related to de-
partments and the average minimum average amount award
for a faculty member. The title or sponsoring organization
features from the grants are not to be found in the top twenty
weights.

The combined feature sets actually sees most of the top fea-
tures come from the features in the network feature set. In-
terestingly one of its top features was on the publication co-
author graph, their betweeness centrality score, which was
how often this person was on a path to all other people in
the network.

6. CONCLUSION & FUTURE WORK
The bag of word model on the grant title and sponsoring or-
ganization does surprisingly well overall. But by just looking
at the logisitic regression results one could hypothesize that
the network features can help, but then looking at the lin-
ear classiﬁer the network features actually hurt performance.
This is probably due the overﬁtting that is happening, not
allowing the model to generalize itself as well as just using
the baseline features. More data or removing features would
be the next approach to see whether the overﬁtting can be
reduced.

From the results of the year to year tests, the non-network
features showed that they were not infact a hinderance as
all the classiﬁer approached the same performance.

Figure 5: Testing on all years, training on the pre-
vious years for logistic regression

5.2 Training on previous years, testing on the
next
5.2.1 Logistic Regression
Naturally this second set of tests should be expected to have
a poor testing accuracy at the beginning as the training set
size is rather small. This is clearly exhibited in ﬁgure 2, as
all the testing accuracy lines start below the baseline and
only start to creep over towards the end third of the years.
The spikes in training accuracy seems to be related to the
grant feature set. This unfortunately was not investigated
further due to time constraints.

5.2.2 Linear Classiﬁer
In ﬁgure 6 we see that in general all the features sets are
above the baseline after the ﬁrst half of the training data.
Again the best performance came from just using the grant
feature set, which infact was above baseline for all the years
tested. It also exhibited a text book curve in terms of in-
creased training data reducing the overﬁtting and increasing
testing accuracy.

It is cool to see that towards the end all the classiﬁers had
increasing accuracy, and were roughly achieving the same
result, being a nice amount above baseline. This also in-
dicates that the non-network features just required more
training data to clear away any noise that was present in
the intermingled year testing.

5.3 Top Features

Rank
1.
2.
3.
4.
Rank
1.
2.
3.
4.

Table 1: Top features given by the classiﬁers with corresponding weights
LC Grants
LC Grant + Network features
GSF.sponsor.org:Merck 1.332017948078512
GSF.sponsor.org:Aﬀairs 0.42320376196406856
GSF.pro ject.title:Agreement 1.246513339384414
GSF.sponsor.org:American 0.4193289572745211
GSF.pro ject.title:Proteases 1.1682290874186396
GSF.sponsor.org:Veterans 0.4148158949296257
GSF.pro ject.title:Material 1.10557329991572
GSF.sponsor.org:Defense 0.3324431609698573
LR Grants
LR Grant + Network
GSF.pro ject.title:Agreement: 2.1819922788681505
GSF.proposed.total: 3.4019115047822216E-6
GSF.sponsor.org :Merck: 2.1438229249679956
GSF.sponsor.org:Institutes: 2.084470711054364E-11
GSF.pro ject.title:Material: 1.6967485373680784
GSF.sponsor.org:Health: 1.5289357472387933E-11
GSF.pro ject.title:Material: 1.4695567254611942E-11
GSF.pro ject.title:NSF: 1.6583620244115467

Rank
1.
2.
3.
4.

Table 2: Top features given by the classiﬁers with corresponding weights
LC Grant + Non-network
LC Combined
GDPT.ratio.aA.aR 0.00818726612441789
LG.GDPT.totalAppliedInYear 0.005985296645086139
NET.grantCA.BarycenterScorerW 0.0028962810755648995
SQ.GDPT.totalAppliedInYear 0.004965024091320833
SQ.NET.pubCoAuthor.BCentralityW 0.001914286988455054
GPF.totalAwarded 8.772522148671472E-4
GPF.MIN.avg.awarded 8.53257547026869E-4
LG.NET.grantCA.BarycenterScorer 0.002842374890139031

The change in top features across the sets was surprising,
especially the ones picked by the combined feature set, sug-
gesting that network features do infact hold some promise
for use in machine learning. However it’s clear that the bag
of word model on the grant title and sponsoring organiza-
tion proved to be the best features, when the classiﬁer was
able to weight them appropriately, in this case.

6.1 Future Work
There is lots that could be done to build on top of this work.
For brevity the top three on my agenda are:

1. investigating the SVM issues, as this classiﬁer should
give just as good performance as the linear classiﬁer.

2. looking at the network feature set and investigating
the overﬁtting by removing features as it seems that
classiﬁer accuracy could be easily improved if overﬁt-
ting was addressed.

3. approach the grant approval prediciton by clustering
the data, and investigating whether prediction could
be improved by using this clustering approach.

7. REFERENCES
[1] R. Burt. Structural holes: The social structure of
competition. Belknap Pr, 1995.
[2] R. Burt. Structural Holes and Good Ideas 1. American
journal of sociology, 110(2):349–399, 2004.
[3] J. OˇSMadadhain, D. Fisher, S. White, and Y. Boey.
The jung (java universal network/graph) framework.
University of California, Irvine, California, 2003.


CS 229 Final Pro ject: Single Image Depth Estimation From Predicted
Semantic Labels

Beyang Liu beyangl@cs.stanford.edu
Stephen Gould sgould@stanford.edu
Prof. Daphne Koller koller@cs.stanford.edu

December 10, 2009

1

Introduction

Recovering the 3D structure of a scene from a single im-
age is a fundamental problem in computer vision that has
applications in robotics, surveillance, and general scene
understanding. However, estimating structure from raw
image features is notoriously diﬃcult since local appear-
ance is insuﬃcient to resolve depth ambiguities (e.g., sky
and water regions in an image can have similar appearance
but dramatically diﬀerent geometric placement within a
scene).
Intuitively, semantic understanding of a scene
plays an important role in our own perception of scale
and 3D structure.
Our goal is to estimate the depth of each pixel in an
image. We employ a two phase approach:
In the ﬁrst
phase, we use a learned multi-class image labeling MRF
to estimate the semantic class for each pixel in the image.
We currently label pixels as one of: sky, tree, road, grass,
water, building, mountain, and foreground object.
In the second phase, we use the predicted semantic class
labels to inform our depth reconstruction model. Here, we
ﬁrst learn a separate depth estimator for each semantic
class. We incorporate these predictions in a Markov ran-
dom ﬁeld (MRF) that includes semantic-aware reconstruc-
tion priors such as smoothness and orientation. Motivated
by the work of Saxena et. al.,
[6], we explore both pixel-
based and superpixel-based variants of our model.

2 Depth Estimation Model

As mentioned above, our algorithm works in two phases.
Out of concern for length, we forgo a detailed discussion of
the semantic labeling phase here. Brieﬂy, our model can
use any multi-class image labeling method that produces
pixel-level semantic annotations [7, 3, 2]. The particular
algorithm we employ deﬁnes an MRF over the semantic
class labels (sky, road, water, grass, tree, building, moun-
tain and foreground object ) of each pixel in the image. The
MRF includes singleton potential terms, which are simply
the conﬁdence of a multi-class boosted decision tree clas-

siﬁer in the predicted semantic label of a pixel. This clas-
siﬁer is trained on a standard set of 17 ﬁlter response fea-
tures [7] computed in a small neighborhood around each
pixel. The MRF also includes pairwise potential terms,
which deﬁne a contrast-dependent smoothing prior be-
tween adjacent pixels, encouraging them to take the same
label. The weighting between the singleton terms and the
pairwise terms is determined via cross-validation on the
training set. Given the semantic labeling of the image, we
also predict the location of the horizon. We now begin
our discussion of the second phase of our algorithm with
an overview of the geometry of image formation.

2.1 Image Formation and Scene Geome-
try
Consider an ideal camera model (i.e., with no lens distor-
tion). Then, a pixel p with coordinates (up , vp ) (in the
camera plane) is the image of a point in 3D space that
lies on the ray extending from the camera origin through
(up , vp ) in the camera plane. The ray rp in the world co-
up

ordinate system is given by
rp ∝ R−1K−1
vp
fu

1
u0
0
K =
v0
1

0
1
0
0
sin θ
cos θ
0
0 − sin θ
cosθ
Here, R deﬁnes the rotation matrix from camera coordi-
nate system to world coordinate system and K is the cam-
era matrix [5]. 1 fu and fv are the (u- and v -scaled) focal

0
fv
0

R =

(1)

(2)

(3)

1 In our model, we assume that there is no translation between the
world coordinate system and the camera coordinate system, i.e., that
the images were taken from approximately the same height above the
ground.

1

Our feature vector includes the same 17 raw pixel ﬁl-
ter features from the semantic labeler phase and also the
log of these features. These describe the local appear-
ance of the pixel. We also include the (u, v) coordinates of
the pixel and an a priori estimated log-depth determined
by pixel coordinates (u, v) and semantic label Lp . The
prior log-depth is learned, for each semantic class, by av-
eraging the log-depth at a particular (u, v)-pixel location
over the set of training images (see Figure 2). Since not
all semantic class labels appear in all pixel locations, we
smooth the priors with a global log-depth prior (the aver-
age of the log-depths over all the classes at the particular
location). We encode additional geometric constraints as
features by examining the three key pixels discussed in
Section 2.1. For each of these pixels (bottommost and
topmost pixel with class Lp and topmost ground pixel),
we use the pixel’s prior log-depth to calculate a depth
estimate for p (assuming that most ob jects are roughly
vertical) and include this estimate as a feature. We also
include the (horizon-adjusted) vertical coordinate of these
pixels as features. Note that our verticality assumption is
not a hard constraint, but rather a soft one that can be
overridden by the strength of other features. By including
these as features, we allow our model to learn the strength
of these constraints. Finally, we add the square of each
feature, allowing us to learn quadratic depth correlations.
We note this set of features is signiﬁcantly simpler than
those used in previous works such as Saxena et. al.,
[6].
For numerical stability, we also normalize each feature to
zero mean and unit variance.
We learn a diﬀerent local depth predictor (i.e., a dif-
ferent linear regression) for each semantic class. Moti-
vated by the desire to more accurately model the depth
of nearby ob jects and the fact that relative depth is more
appropriate for scene understanding, we learn a model to
predict log-depth rather than depth itself. We thus esti-
mate pointwise log-depth as a linear function of the pixel
features (given the pixel’s semantic class),

log ˆdp = θT
Lp

fp

(5)

where ˆdp is the pointwise estimated depth for pixel p, Lp
is its predicted semantic class label, fp ∈ Rn is the pixel
feature vector, and {θl }l∈L are the learned parameters of
the model.

2.3 MRF Models for Depth Reconstruc-
tion

The pointwise depth estimation provided by Eq. (5) is
somewhat noisy and can be improved by including pri-
ors that constrain the structure of the scene. We de-
velop two diﬀerent MRF models—one pixel-based and one
superpixel-based—for the inclusion of such priors.

(a) Image view

(b) Side view

Illustration of semantically derived geometric
Figure 1:
constraints. See text for details.

lengths of the camera, and the principal point (u0 , v0 ) is
the center pixel in the image. As in Saxena et. al.,
[6]
we assume a reasonable value for the focal length (in our
experiments we set fu = fv = 348 for a 240 × 320 im-
age). The form of our rotation matrix assumes the cam-
era’s horizontal x axis is parallel to the ground and we
estimate the yz -rotation of the camera plane from the pre-
dicted location of the horizon (assumed to be at depth ∞):
(vhz −v0 )). In the sequel, we will assume that
θ = tan−1 ( 1
fv
rp has been normalized (i.e., (cid:107)rp(cid:107)2 = 1).
We now describe constraints about the geometry of a
scene. Consider, for example, the simple scene in Figure 1,
and assume that we would like to estimate the depth of
some pixel p on a vertical ob ject A attached to the ground.
We deﬁne three key points that are strong indicators of
the depth of p. First, let g be the topmost ground pixel
in the same column below p. The depth of g is a lower
bound on the depth of p. Second, let b be the bottommost
visible pixel b on the ob ject A. By extending the camera
ray through b to the ground, we can calculate an upper
bound on the depth of p. Third, the topmost point t on
the ob ject may also be useful since a non-sky pixel high in
the image (e.g., an overhanging tree) tends to be close to
the camera.
Simple geometric reasoning allows us to encode the ﬁrst
(cid:32)
(cid:33)
(cid:33)(cid:32)
(cid:32)
(cid:33)
two constraints as

dg

rT
g e3
rT
p e3

≤ dp ≤ dg

rT
g e2
rT
b e2

rT
b e3
rT
p e3

(4)

where dp and dg are the distances to the points p and
g , respectively, and ei is the i-th standard basis vector.
t e3 ≈
The third constraint can similarly be encoded as dt rT
p e3 . We incorporate these constraints both implicitly
dp rT
as features and explicitly in our pixel MRF model.

2.2 Features and Pointwise Depth Esti-
mation

The basis of our model is linear regression toward the log-
depth of each pixel in the image. The output from the
linear regression becomes the singleton terms of our MRFs.

(a) sky

(b) tree

(c) road

(d) grass

(e) water

(f ) building

(g) mountain

(h) fg. ob j.

2.3.1 Pixel-based Markov Random Field

(6)

(7)

Figure 2: Smoothed per-pixel log-depth prior for each semantic class with horizon rotated to center of image. Colors
indicate distance (red is further away and blue is closer). The classes “water” and “mountain” had very few samples
and so are close to the global log-depth prior (not shown). See text for details.
γpq = exp (cid:0)−c−1(cid:107)xp − xq (cid:107)2 (cid:1) measures the contrast be-
tween two adjacent pixels, where xp and xq are the CIELab
color vectors for pixels p and q , respectively, and c is the
mean square-diﬀerence over all adjacent pixels in the im-
age.
(Incidentally, this is the same contrast term used
by the MRF in the semantic labeling phase.) We choose
the prior strength by cross-validation on a set of training
images.
The soft geometry constraints ψpg , ψpt and ψpb model
our prior belief that certain semantic classes are vertically
oriented (e.g., buildings, trees and foreground ob jects).
Here, we impose the soft constraint that a pixel within
such a region should be the same depth as other pixels in
the region (i.e., via the constraint on the topmost and bot-
tommost pixels in the region), and be between the nearest
and farthest ground plane points g and g (cid:48) deﬁned in Sec-
tion 2.1. The constraints are encoded using the Huber
penalty, (e.g., h(dp − dg ; β ) for the nearest ground pixel
constraint). Each term is weighted by a semantic-speciﬁc
prior strength {λg
l }l∈L .
l , λt
l , λb

Our pixel-based MRF includes a prior for smoothness.
Here, we add a potential over three consecutive pixels (in
the same row or column) that prefers co-linearity. We also
encode semantically-derived depth constraints which pe-
nalize vertical surfaces from deviating from geometrically
plausible depths (as described in Section 2.1). Formally,
+ (cid:88)
E (D | I , L) = (cid:88)
we deﬁne the energy function over pixel depths D as
(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:124)
(cid:124)
ψpqr (dp , dq , dr )
ψp (dp )
ψpb (dp , db ) + (cid:88)
ψpg (dp , dg ) + (cid:88)
+ (cid:88)
pqr
p
smoothness
data term
(cid:125)
(cid:123)(cid:122)
(cid:124)
ψpt (dp , dt )
p
p
p
geometry (see §2.1)
where the data term, ψp , attempts to match the depth
for each pixel dp to the pointwise estimate ˆdp , and ψpqr
represents the co-linearity prior. The terms ψpg , ψpb and
ψpt represent the geometry constraints described in Sec-
tion 2.1 above. Recall that the pixel indices g , b and t are
determined from p and the semantic labels.
The data term in our model is given by
ψp (dp ) = h(dp − ˆdp ; β )
where h(x; β ) is the Huber penalty, which takes the value
x2 for −β ≤ x ≤ β and β (2|x| − β ) otherwise. We
choose the Huber penalty because it is more robust to out-
liers than the more commonly used (cid:96)2 -penalty and, unlike
the robust (cid:96)1 -penalty, is continuously diﬀerentiable (which
simpliﬁes inference). In our model, we set β = 10−3 .
Our smoothness prior encodes a preference for co-
linearity of adjacent pixels within uniform regions. As-
suming pixels p, q , and r are three consecutive pixels (in
any row or column), we have

2.3.2 Superpixel-based Markov Random Field

In the superpixel model, we segment the image into a set of
non-overlapping regions (or superpixels) using a bottom-
up over-segmentation algorithm.
In our experiments we
used mean-shift [1], but could equally have used a graph-
based approach or normalized cuts. Each superpixel Si is
assumed to be planar, a constraint that we strictly enforce.
Instead of deﬁning an MRF over pixel depths, we deﬁne an
MRF over superpixel plane parameters, {αi }, where any
point x ∈ R3 on the plane satisﬁes αT
i x = 1. The depth
of pixel p corresponds to the intersection of the ray rp and
i rp )−1 .
the plane, and is given by (αT
We deﬁne an energy function that includes terms that
penalize the distance between the superpixel planes and
the pointwise depth estimates ˆdp (Eq. (5)) and terms
that enforce soft connectivity, co-planarity, and orienta-
tion constraints over the planes. All of these are condi-
tioned on the semantic class of the superpixel (which we
deﬁne as the ma jority semantic class over the superpixel’s

ψpqr (dp , dq , dr ) =
λsmooth · √
γpq γqr · h(2dq − dp − dr ; β )
where the smoothness penalty is weighted by a contrast-
dependent term and the prior strength λsmooth . Here,

(8)

constituent pixels). Formally, we have
+ (cid:88)
E (α | I , L, S) =(cid:88)
+ (cid:88)
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ψi (αi )
ψp (αi∼p )
ψij (αi , αj )
p
i
ij
orientation prior
data term
connectivity and
co-planarity prior

(9)

h

(10)

1
ˆdp

ψp (αi ) =

Here αi∼p indicates the αi associated with the superpixel
containing pixel p, i.e., αi : p ∈ Si .
Region Data Term. The data term penalizes the
plane parameters from deviating away from the pointwise
(cid:17)
(cid:16) ˆdp · αT
depth estimates. It takes the form
i rp − 1; β
where h(x; β ) is the Huber penalty as deﬁned in Sec-
tion 2.3.1 above. We weight each pixel term by the in-
verse pointwise depth estimate to give higher preference
to nearby regions.
Orientation Prior. The orientation prior enables us
to encode a preference for orientation of diﬀerent seman-
tic surfaces, e.g., ground plane surfaces (“road”, “grass”,
etc., ) should be horizontal while buildings should be ver-
tical. We encode this preference as
ψi (αi ) = Ni · λl · (cid:107)Pl (αi − ¯αl ) (cid:107)2

(11)

where Pl pro jects onto the planar directions that we would
like to constrain and ¯αl is the prior estimate for the ori-
entation of a surface with semantic class label Li = l. We
weight this term by the number of pixels (Ni ) in the su-
perpixel and a semantic-class-speciﬁc prior strength (λl ).
The latter captures our conﬁdence in a semantic class’s
orientation prior (for example, we are very conﬁdent that
ground is horizontal, but we are less certain a priori about
the orientation of tree regions).
Connectivity and Co-planarity Prior. The con-
nectivity and co-planarity term captures the relationship
between two adjacent superpixels. For example, we would
not expect adjacent “sky” and “building” superpixels to
be connected, whereas we would expect “road” and “build-
ing” to be connected. Deﬁning Bij to be the set of pixels
along the boundary between superpixels i and j , we have
· (cid:88)
ψij (αi , αj ) = Ni + Nj
2|Bij | λconn
lk
p∈Bij
+ Ni + Nj
λco-plnr
2
lk

i rp − αT
(cid:107)αT
j rp(cid:107)2

· (cid:107)αi − αj (cid:107)2

(12)

2.4 Inference and Learning

Both of our MRF formulations (Eq. (6) and Eq. (9)) de-
ﬁne convex ob jectives which we solve using the L-BFGS
algorithm [4] to obtain a depth prediction for every pixel
in the image—for the superpixel-based model we compute
where αi are the inferred plane
pixel depths as dp = 1
αT
i rp
parameters for the superpixel containing pixel p. In our
experiments, inference takes about 2 minutes per image
for the pixel-based MRF and under 30 seconds for the
superpixel-based model (on a 240 × 320 image).
The various prior strengths (λsmooth , etc., ) are learned
by cross-validation on the training data set. To make this
process computationally tractable, we add terms in an in-
cremental fashion, freezing each weight before adding the
next term. This coordinate-wise optimization seemed to
yield good parameters.

3 Experimental Results and Dis-
cussion

We run experiments on the publicly available dataset from
Saxena et. al.,
[6]. The dataset consists of 534 images
with corresponding depth maps and is divided into 400
training and 134 testing images. We hand-annotated the
400 training images with semantic class labels. The 400
training images were then used for learning the parameters
of the semantic and depth models. All images were resized
to 240 × 320 before running our algorithm.2
We report results on the 134 test images. Since the
maximum range of the sensor used to collect ground truth
measurements was 81m, we truncate our predictions to the
range [0, 81]. Table 3 shows our results compared against
previous published results. We compare both the average
log-error and average relative error, deﬁned as | log10 gp −
log10 dp | and |gp−dp |
, respectively, where gp is the ground
gp
truth depth for pixel p. We also compare our results to
our own baseline implementation which does not use any
semantic information.
Both our pixel-based and superpixel-based models
achieve state-of-the-art performance for the log10 metric
and comparable performance to state-of-the-art for the rel-
ative error metric. Importantly, they achieve good results
on both metrics unlike the previous results which perform
well at either one or the other. This can be clearly seen in
Figure 4 where we have plotted the performance metrics
on the same graph.
Having semantic labels allows us to break down our re-
sults by class. Our best performing results are the ground
plane classes (especially road), which are easily identiﬁed

where we weight each term by the average number of pixels
in the associated superpixels and a semantic-class-speciﬁc
prior strength.

2Note that, although the horizon in the dataset tends to be ﬁxed
at the center of the image, we still adjust our camera rays to the
predicted horizon location.

Method
SCN †
HEH †
Pointwise MRF †
PP-MRF †
Pixel MRF Baseline
Pixel MRF Model (§2.3.1)
Superpixel MRF Baseline
Superpixel MRF Model (§2.3.2)
† Results reported in Saxena et. al.,

log10
0.198
0.320
0.149
0.187
0.206
0.149
0.209
0.148
[6].

Rel.
0.530
1.423
0.458
0.370
0.464
0.375
0.471
0.379

Figure 3: Quantitative results comparing variants of
our “semantic-aware” approach with strong baselines and
other state-of-the-art methods. Baseline models do not
use semantic class information.

by our semantic model and tightly constrained geomet-
rically. We achieve poor performance on the foreground
class which we attribute to the lack of foreground ob jects
in the training set (less than 1% of the pixels).
Unexpectedly, we also perform poorly on sky pixels
which are easy to predict and should always be positioned
at the maximum depth. This is due, in part, to errors
in the groundtruth measurements (caused by sensor mis-
alignment) and the occasional misclassiﬁcation of the re-
ﬂective surfaces of buildings as sky by our semantic model.
Note that the nature of the relative error metric is to mag-
nify these mistakes since the ground truth measurement in
these cases is always closer than the maximum depth.
Finally, we show some qualitative results in Figure 5.
The results show that we correctly model co-planarity of
the ground plane and building surfaces. Notice our accu-
rate prediction of the sky (which is sometimes penalized
by misalignment in the groundtruth, e.g., third example).
Our algorithm also makes mistakes, such as positioning
the building too close in the second example and missing
the ledge in the foreground (a mistake that many humans
would also make).
Overall, our model attains state-of-the-art results,
though it utilizes relatively simple image features, because
it incorporates semantic reasoning about the scene.

Figure 4: Plot of log10 error metric versus relative error
metric comparing algorithms from Table 3 Bottom-left in-
dicates better performance.

Figure 5: (Above) Some qualitative depth reconstructions
from our model showing (from left to right) the image,
semantic overlay, ground truth depth measurements, and
our predicted depths. Red signiﬁes a distance of 80m,
black 0m. (Below) Example 3D reconstructions.

References

[1] D. Comaniciu and P. Meer. Mean shift: A robust approach
toward feature space analysis. PAMI, 2002.
[2] S. Gould, R. Fulton, and D. Koller. Decompsing a scene into
geometric and semantically consistent regions.
In ICCV,
2009.
[3] X. He, R. Zemel, and M. Carreira-Perpinan. Multiscale
CRFs for image labeling. In CVPR, 2004.
[4] D. Liu and J. Nocedal. On the limited memory method for
large scale optimization. In Mathematical Programming B,
1989.
[5] Y. Ma, S. Soatto, J. Kosecka, and S. S. Sastry. An Invitation
to 3-D Vision. Springer, 2005.
[6] A. Saxena, M. Sun, and A. Y. Ng. Make3D: Learning 3-D
scene structure from a single still image. In PAMI, 2008.
[7] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Texton-
Boost: Joint appearance, shape and context modeling for
multi-class ob ject recognition and segmentation. In ECCV,
2006.

0.150.160.170.180.190.20.210.350.40.450.50.55log10 errorrelative errorPointwise MRF [19]SCN [8]Region BaselinePixel BaselinePP−MRF [19]Our Superpixel MRFOur Pixel MRF
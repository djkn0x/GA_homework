Thresholding Procedures for High Dimensional
Variable Selection and Statistical Estimation

Shuheng Zhou
Seminar f ¨ur Statistik
ETH Z ¨urich
CH-8092, Switzerland

Abstract

Given n noisy samples with p dimensions, where n ≪ p, we show that the multi-
step thresholding procedure can accurately estimate a spar se vector β ∈ Rp in a
linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov
09). Thus our conditions for model selection consistency are considerably weaker
than what has been achieved in previous works. More importan tly, this method al-
lows very signi ﬁcant values of s, which is the number of non-zero elements in the
true parameter. For example, it works for cases where the ord inary Lasso would
have failed. Finally, we show that if X obeys a uniform uncertainty principle and
if the true parameter is sufﬁciently sparse, the Gauss-Dant zig selector (Cand `es-
Tao 07) achieves the ℓ2 loss within a logarithmic factor of the ideal mean square
error one would achieve with an oracle which would supply per fect information
about which coordinates are non-zero and which are above the noise level, while
selecting a sufﬁciently sparse model.

1

Introduction

In a typical high dimensional setting, the number of variables p is much larger than the number of
observations n. This challenging setting appears in linear regression, signal recovery, covariance
selection in graphical modeling, and sparse approximations. In this paper, we consider recovering
β ∈ Rp in the following linear model:

(1.1)
Y = X β + ǫ,
where X is an n × p design matrix, Y is a vector of noisy observations and ǫ is the noise term. We
assume throughout this paper that p ≥ n (i.e. high-dimensional), ǫ ∼ N (0, σ2 In ), and the columns
of X are normalized to have ℓ2 norm √n. Given such a linear model, two key tasks are to identify
the relevant set of variables and to estimate β with bounded ℓ2 loss.
In particular, recovery of the sparsity pattern S = supp (β ) := {j : βj 6= 0}, also known as variable
(model) selection, refers to the task of correctly identify ing the support set (or a subset of “signi ﬁ-
cant ” coefﬁcients in
β ) based on the noisy observations. Even in the noiseless case, recovering β (or
its support) from (X, Y ) seems impossible when n ≪ p. However, a line of recent research shows
that it becomes possible when β is also sparse: when it has a relatively small number of nonzero
coefﬁcients and when the design matrix X is also sufﬁciently nice, which we elaborate below. One
important stream of research, which we also adopt here, requ ires computational feasibility for the
estimation methods, among which the Lasso and the Dantzig selector are both well studied and
shown with provable nice statistical properties; see for example [11, 9, 19, 21, 5, 18, 12, 2]. For a
chosen penalization parameter λn ≥ 0, regularized estimation with the ℓ1 -norm penalty, also known

1

(1.2)

as the Lasso [16] or Basis Pursuit [6] refers to the following convex optimization problem
1
bβ = arg min
2n kY − X β k2
2 + λnkβ k1 ,
β
where the scaling factor 1/(2n) is chosen by convenience; The Dantzig selector [5] is deﬁned as,
subject to (cid:13)(cid:13)(cid:13)(cid:13)
X T (Y − X bβ )(cid:13)(cid:13)(cid:13)(cid:13)∞ ≤ λn .
bβ∈Rp (cid:13)(cid:13)(cid:13) bβ(cid:13)(cid:13)(cid:13)1
1
(1.3)
(DS ) arg min
n
Our goal in this work is to recover S as accurately as possible: we wish to obtain bβ such that
| supp ( bβ ) \ S | (and sometimes |S△ supp ( bβ )| also) is small, with high probability, while at the same
time k bβ − β k2
2 is bounded within logarithmic factor of the ideal mean square error one would achieve
with an oracle which would supply perfect information about which coordinates are non-zero and
which are above the noise level (hence achieving the oracle inequality as studied in [7, 5]); We deem
the bound on ℓ2 -loss as a natural criteria for evaluating a sparse model whe n it is not exactly S . Let
s = |S |. Given T ⊆ {1, . . . , p}, let us deﬁne XT as the n × |T | submatrix obtained by extracting
columns of X indexed by T ; similarly, let βT ∈ R|T | , be a subvector of β ∈ Rp conﬁned to T .
Formally, we study a Multi-step Procedure: First we obtain an initial estimator βinit using the Lasso
as in (1.2) or the Dantzig selector as in (1.3), with λn = Θ(σp2 log p/n).
1. We then threshold the estimator βinit with t0 , with the general goal such that, we get a
set I1 with cardinality at most 2s; in general, we also have |I1 ∪ S | ≤ 2s, where I1 =
{j ∈ {1, . . . , p} : βj,init ≥ t0} for some t0 to be speci ﬁed. Set I = I1 .
2. We then feed (Y , XI ) to either the Lasso estimator as in (1.2) or the ordinary leas t squares
(OLS) estimator to obtain bβ , where we set bβI = (X T
I Y and bβI c = 0.
I XI )−1X T
3. We then possibly threshold bβI1 with t1 = 4λnp|I1 | (to be speci ﬁed), to obtain I2 , repeat
step 2 with I = I2 to obtain bβI and set all other coordinates to zero; return bβ .
Our algorithm is constructive in that it does not rely on the unknown parameters s, βmin :=
minj∈S |βj | or those that characterize the incoherence conditions on X ; instead, our choice of λn
and thresholding parameters only depends on σ, n, and p. In our experiments, we apply only the
ﬁrst two steps, which we refer to as a two-step procedure; In particular, the Gauss-Dantzig selector
is a two-step procedure with the Dantzig selector as βinit [5]. In theory, we apply the third step only
when βmin is sufﬁciently large and when we wish to get a “sparser” model
I .

More deﬁnitions. For a matrix A, let Λmin (A) and Λmax (A) denote the smallest and the largest
eigenvalues respectively. We refer to a vector υ ∈ Rp with at most s non-zero entries, where s ≤ p,
as a s-sparse vector. Throughout this paper, we assume that n ≥ 2s and
△
υ 6=0;2s−sparse kX υk2
2 /(n kυk2
min
Λmin (2s)
=
2 ) > 0.
It is clear that n ≥ 2s is necessary, as any submatrix with more than n columns must be singular. In
△
= maxυ 6=0;s−sparse kX υk2
2 /(n kυk2
general, we also assume Λmax (s)
2 ) < ∞. As deﬁned in [4],
the s-restricted isometry constant δs of X is the smallest quantity such that
2 /n ≤ (1 + δs ) kυk2
2 ≤ kXT υk2
(1 − δs ) kυk2
2 ,
It is clear that δs is
for all T ⊆ {1, . . . , p} with |T | ≤ s and coefﬁcients sequences
(υj )j∈T .
non-decreasing in s and 1 − δs ≤ Λmin (s) ≤ Λmax (s) ≤ 1 + δs . Hence δ2s < 1 implies (1.4).
Occasionally, we use βT ∈ R|T | , where T ⊆ {1, . . . , p}, to also represent its 0-extended version
β ′ ∈ Rp such that β ′
T c = 0 and β ′
T = βT ; for example in (1.5) below.
Oracle inequalities. The following idea has been explained in [5]; we hence describe it here only
brieﬂy. Note that due to different normalization of columns of X , our expressions are slightly

(1.4)

2

2

2

(1.6)

(1.7)

(1.8)

(1.5)

different from those in [5]. Consider the least square estimator bβI = (X T
I Y , where
I XI )−1X T
|I | ≤ s and consider the ideal least-squares estimator β ⋄
E (cid:13)(cid:13)(cid:13)β − bβI (cid:13)(cid:13)(cid:13)
2
β ⋄ =
arg min
,
2
I⊆{1,...,p}, |I |≤s
which minimizes the expected mean squared error. It follows from [5] that for Λmax (s) < ∞,
pXi=1
E kβ − β ⋄ k2
min(β 2
i , σ2 /n).
2 ≥ min (1, 1/Λmax (s))
Now we check if for Λmax (s) < ∞, it holds with high probability that
(cid:13)(cid:13)(cid:13) bβ − β(cid:13)(cid:13)(cid:13)
pXi=1
2
i , σ2 /n), so that
min(β 2
= O(log p)
(cid:13)(cid:13)(cid:13) bβ − β(cid:13)(cid:13)(cid:13)
= O(log p) max(1, Λmax (s))E kβ ⋄ − β k2
2 in view of (1.6).
2
These bounds are meaningful since
pXi=1
2 + |I |σ2
I⊆{1,...,p} kβ − βI k2
min(β 2
i , σ2 /n) = min
n
represents the ideal squared bias and variance. We elaborat e on conditions on the design, under
which we accomplish these goals using the multi-step procedures in the rest of this section. We now
deﬁne a constant λσ,a,p for each a > 0, by which we bound the maximum correlation between the
noise and covariates of X , which we only apply to X with column ℓ2 norm bounded by √n; Let
n (cid:13)(cid:13)(cid:13)(cid:13)∞ ≤ λσ,a,p(cid:27), where λσ,a,p = σ√1 + ar 2 log p
Ta := (cid:26)ǫ : (cid:13)(cid:13)(cid:13)(cid:13)
X T ǫ
, hence
n
P (Ta ) ≥ 1 − (pπ log ppa )−1 , for a ≥ 0; see [5].
(1.10)
Variable selection. Our ﬁrst result in Theorem 1.1 shows that consistent variabl e selection is pos-
sible under the Restricted Eigenvalue conditions, as formalized in [2]. Similar conditions have been
used by [10] and [17].
Assumption 1.1 (Restricted Eigenvalue assumption RE (s, k0 , X ) [2]) For some integer 1 ≤
s ≤ p and a positive number k0 , the following holds:
kX υk2
1
△
√n kυJ0 k2
= min
min
K (s, k0 , X )
υ 6=0,
J0⊆{1,...,p},
‚‚‚1
|J0 |≤s
≤k0kυJ0 k1
If RE (s, k0 , X ) is satis ﬁed with k0 ≥ 1, then the square submatrices of size ≤ 2s of X T X are nec-
essarily positive deﬁnite (see [2]) and hence (1.4) must hol d. We do not impose any extra constraint
on s besides what is allowed in order for (1.11) to hold. Note that when s > n/2, it is impossible
for the restricted eigenvalue assumption to hold as XI for any I such that |I | = 2s becomes singular
in this case. Hence our algorithm is especially relevant if one would like to estimate a parameter β
such that s is very close to n; See Section 4 for such examples. Let βmin := minj∈S |βj |.
Theorem 1.1 (Variable selection under Assumption 1.1) Suppose that RE (s, k0 , X ) condition
holds, where k0 = 1 for the DS and = 3 for the Lasso. Suppose λn ≥ Bλσ,a,p for λσ,a,p as in (1.9),
1
BΛmin (2s) . Let s ≥ K 4 (s, k0 , X ) and
where B ≥ 1 for the DS and ≥ 2 for the Lasso. Let B2 =
βmin ≥ 4√2 max(K (s, k0 , X ), 1)λn√s + max (cid:16)4K 2 (s, k0 , X ), √2B2(cid:17) λn√s.
Then with probability at least P (Ta ), the multi-step procedure returns bβ such that
B 2
2
S ⊆ I := supp ( bβ ), where |I \ S | <
and
16
λ2
2 log p(1 + a)sσ2 (1 + B 2
σ,a,p |I |
2 /16)
k bβ − β k2
min (|I |) ≤
2 ≤
,
Λ2
nΛ2
min (2s)
which satis ﬁes (1.7) and (1.8) given that βmin ≥ σ/√n and Pp
i=1 min(β 2
i , σ2/n) = sσ2/n.
3

‚‚‚υJ c
0

(1.11)

(1.9)

> 0.

Our analysis builds upon the rate of convergence bounds for βinit derived in [2]. The ﬁrst implica-
tion of this work and also one of the motivations for analyzing the thresholding methods is: under
Assumption 1.1, one can obtain consistent variable selection for very signi ﬁcant values of s, if only
a few extra variables are allowed to be included in the estima tor bβ . In our simulations, we recover
the exact support set S with very high probability using a two-step procedure. Note that we did not
optimize the lower bound on s as we focus on cases when the support of S is large.
Thresholding that achieves the oracle inequalities. The natural question upon obtaining Theo-
rem 1.1 is: is there a good thresholding rule that enables us to obtain a sufﬁciently sparse estimator
bβ when some components of βS (and hence βmin ) are well below σ/√n, which also satis ﬁes the
oracle inequality as in (1.7)? Before we answer this question, we deﬁne s0 as the smallest integer
such that
pXi=1
i , λ2σ2 ) ≤ s0λ2σ2 , where λ = p2 log p/n,
min(β 2
(1.12)
and the (s, s′ )-restricted orthogonality constant [4] θs,s′ as the smallest quantity such that
| h XT c, XT ′ c′ i /n| ≤ θs,s′ kck2 kc′k2
(1.13)
holds for all disjoint sets T , T ′ ⊆ {1, . . . , p} of cardinality |T | ≤ s and |T ′ | < s′ , where s + s′ ≤
p. Note that θ is non-decreasing in s, s′ and small values of θs,s′ indicates that disjoint subsets
covariates in XT and XT ′ span nearly orthogonal subspaces.
Theorem 1.2 says that under a uniform uncertainty principle (UUP), thresholding of an initial
Dantzig selector βinit , at the level of Θ(σp2 log p/n) indeed identi ﬁes a sparse model
I of car-
2 -loss for its corresponding least-squares estimator is ind eed
dinality at most 2s0 such that the ℓ2
bounded within O(log p) of the ideal mean square error as in (1.5), when β is as sparse as required
by the Dantzig selector to achieve such an oracle inequality [5]. This is accomplished without any
knowledge of the signi ﬁcant coordinates of β and not being able to observe parameter values.
Assumption 1.2 (A Uniform Uncertainly Principle) [5] For some integer 1 ≤ s < n/3, assume
δ2s + θs,2s < 1, which implies that λmin (2s) > θs,2s given that 1 − δ2s ≤ Λmin (2s).
Theorem 1.2 Choose τ , a > 0 and set λn = λp,τ σ , where λp,τ := (√1 + a + τ −1 )p2 log p/n,
in (1.3). Suppose β is s-sparse with δ2s + θs,2s < 1 − τ . Let threshold t0 be chosen from the
range (C1λp,τ σ, C4λp,τ σ ] for some constants C1 , C4 to be deﬁned. Then with probability at least
1−(√π log ppa )−1 , the Gauss-Dantzig selector bβ selects a model I := supp ( bβ ) such that |I | ≤ 2s0 ,
3 log p  σ2/n +
i , σ2 /n)! ,
pXi=1
|I \ S | ≤ s0 ≤ s, and k bβ − β k2
2 ≤ 2C 2
min(β 2
(1.14)
where C3 depends on a, τ , δ2s , θs,2s and C4 ; see (3.3).
Our analysis builds upon [5]. Note that allowing t0 to be chosen from a range (as wide as one
would like, with the cost of increasing the constant C3 in (1.14)), saves us from having to estimate
C1 , which indeed depends on δ2s and θs,2s . Assumption 1.2 implies that Assumption 1.1 holds for
k0 = 1 with K (s, k0 , X ) = pΛmin (2s)/(Λmin (2s) − θs,2s ) ≤ pΛmin (2s)/(1 − δ2s − θs,2s )
(see [2]); It is an open question if we can derive the same result under Assumption 1.1.
Previous work. Finally, we brieﬂy review related work in multi-step proced ures and the role of
sparsity for high-dimensional statistical inference. Before this work, hard thresholding idea has
been shown in [5] (via Gauss-Dantzig selector) as a method to correct the bias of the initial Dantzig
selector. The empirical success of the Gauss-Dantzig selector in terms of improving the statistical
accuracy is strongly evident in their experimental results . Our theoretical analysis on the oracle
inequalities, which hold for the Gauss-Dantzig selector under a uniform uncertainty principle, is
exactly inspired by their theoretical analysis of the initial Dantzig selector under the same conditions.
For the Lasso, [12] has also shown in theoretical analysis that thresholding is effective in obtaining

4

a two-step estimator bβ that is consistent in its support with β ; however, the choice of threshold level
depends on the unknown value βmin (which needs to be sufﬁciently large) and s, and their theory
does not directly yield (or imply) an algorithm for ﬁnding su ch parameters. Further, as pointed out
by [2], a weakening of their condition is still sufﬁcient for Assumption 1.1 to hold.

The sparse recovery problem under arbitrary noise is also we ll studied, see [3, 15, 14]. Although
as argued in [3, 14], the best accuracy under arbitrary noise has essentially been achieved in both
work, their bounds are worse than that in [5] (hence the prese nt paper) under the stochastic noise as
discussed in the present paper; see more discussions in [5]. Moreover, greedy algorithms in [15, 14]
require s to be part of their input, while the iterative algorithms in the present paper do not have such
requirement, and hence adapt to the unknown level of sparsity s well. A more general framework
on multi-step variable selection was studied by [20]. They control the probability of false positives
at the price of false negatives, similar to what we aim for in the present paper. Unfortunately, their
analysis is constrained to the case when s is a constant. Finally, under a restricted eigenvalue con-
dition slightly stronger than Assumption 1.1, [22] require s s = O(pn/ log p) in order to achieve
variable selection consistency using the adaptive Lasso [23] as the second step procedure.
Organization of the paper. We prove Theorem 1.1 essentially in Section 2. A thresholding frame-
work for the general setting is described in Section 3, which also sketches the proof of Theorem 1.2.
Section 4 brieﬂy discusses the relationship between linear
sparsity and random design matrices.
Section 5 includes simulation results showing that our two-step procedure is consistent with our
theoretical analysis on variable selection.

2 Thresholding procedure when βmin is large
We use a penalization parameter λn = Bλσ,a,p and assume βmin > C λn√s for some constants
B , C throughout this section; we ﬁrst specify the thresholding p arameters in this case. We then show
in Theorem 2.1 that our algorithm works under any conditions so long as the rate of convergence
of the initial estimator obeys the bounds in (2.2). Theorem 1.1 is a corollary of Theorem 2.1 under
Assumption 1.1, given the rate of convergence bounds for βinit following derivations in [2].

The Iterative Procedure. We obtain an initial estimator βinit using the Lasso or the Dantzig selector.
Let bS0 = {j : βj,init > 4λn }, and bβ (0) := βinit ; Iterate through the following steps twice, for i =
0, 1: (a) Set ti = 4λnq| bSi |; (b) Threshold bβ (i) with ti to obtain I := bSi+1 , where
j ≥ 4λnq| bSi |(cid:27) and compute bβ (i+1)
bSi+1 = (cid:26)j ∈ bSi : bβ (i)
I XI )−1X T
= (X T
(2.1)
I Y .
I
= bβ (2)
and bβj = 0, ∀j ∈ bS c
Return the ﬁnal set of variables in bS2 and output bβ such that bβ bS2
2 .
bS2
Theorem 2.1 Let λn ≥ Bλσ,a,p , where B ≥ 1 is a constant suitably chosen such that the initial
estimator βinit satis ﬁes on Ta , for υinit = βinit − β and some constants B0 , B1 ,
kυinit,S k2 ≤ B0λn√s and kυinit,S c k1 ≤ B1λn s;
Suppose βmin ≥ (cid:16)max (cid:16)pB1 , 2(cid:17) 2√2 + max (cid:16)B0 , √2B2(cid:17)(cid:17) λn√s,
(2.3)
1 /16, it holds on Ta that | bSi | ≤ 2s, ∀i = 1, 2, and
where B2 = 1/(BΛmin (2s)). Then for s ≥ B 2
k bβ (i) − β k2 ≤ λσ,a,pq| bSi |/Λmin (| bSi |) ≤ λnB2√2s, ∀i = 1, 2,
(2.4)
where bβ (i) are the OLS estimators based on I = bSi ; Finally, the Iterative Procedure includes the
correct set of variables in bS2 such that S ⊆ bS2 ⊆ bS1 and
(cid:12)(cid:12)(cid:12) bS2 \ S (cid:12)(cid:12)(cid:12) := (cid:12)(cid:12)(cid:12)supp ( bβ ) \ S (cid:12)(cid:12)(cid:12) ≤
B 2
1
2
(2.5)
min (| bS1 |) ≤
.
16
16B 2Λ2

(2.2)

5

Remark 2.2 Without the knowledge of σ , one could use bσ ≥ σ in λn ; this will put a stronger
requirement on βmin , but all conclusions of Theorem 2.1 hold. We also note that in order to obtain
bS1 such that | bS1 | ≤ 2s and bS1 ⊇ S , we only need to threshold βinit at t0 = B1λn (see Section 3 and
Lemma 3.2 for an example); instead of having to estimate B1 , we use t0 = Θ(λn√s) to threshold.
3 A thresholding framework for the general setting

In this section, we wish to derive a meaningful criteria for consistency in variable selection, when
βmin is well below the noise level. Suppose that we are given an initial estimator βinit that achieves
the rate of convergence bound as in (1.14), which adapts nearly ideally to the uncertainty in the
support set S and the “signi ﬁcant ” set. We show that although we cannot gua
rantee the presence
of variables indexed by {j : |βj | < σp2 log p/n} to be included in the ﬁnal set I (cf. (3.7)) due
to their lack of strength, we wish to include the signi ﬁcant v ariables from S in I such that the OLS
estimator based on I achieves this almost ideal rate of convergence as βinit does, even though some
variables from S are missing in I . Here we pay a price for the missing variables in order to obta in a
sparse model I . Toward this goal, we analyze the following algorithm under Assumption 1.2.
The General Two-step Procedure: Assume δ2s + θs,2s < 1 − τ , where τ > 0;
1. First we obtain an initial estimator βmin using the Dantzig selector with λp,τ := (√1 + a +
τ −1 )p2 log p/n, where τ , a ≥ 0; we then threshold βinit with t0 , chosen from the range
(C1λp,τ σ, C4λp,τ σ ], to obtain a set I of cardinality at most 2s, (we prove a stronger result
in Lemma 3.2), where
(3.1)
for C1 as deﬁned in (3.3) ;
I := {j ∈ {1, . . . , p} : βj,init > t0} ,
2. In the second step, given a set I of cardinality at most 2s, we run the OLS regression to
I Y and set bβj = 0, ∀j 6∈ I .
obtain obtained via (3.1), bβI = (X T
I XI )−1X T
Theorem 2 in [5] has shown that the Dantzig selector achieves nearly the ideal level of MSE.
Proposition 3.1 [5] Let Y = X β + ǫ, for ǫ being i.i.d. N (0, σ2 ) and kXj k2
2 = n. Choose τ , a > 0
and set λn = λp,τ σ := (√1 + a + τ −1 )σp2 log p/n in (1.3). Then if β is s-sparse with δ2s +
θs,2s < 1 − τ , the Dantzig selector obeys with probability at least 1 − (√π log ppa )−1 , (cid:13)(cid:13)(cid:13) bβ − β(cid:13)(cid:13)(cid:13)
2
2 ≤
2 (√1 + a + τ −1 )2 log p (cid:0)σ2/n + Pp
i=1 min (cid:0)β 2
i , σ2 /n(cid:1)(cid:1) .
2C 2
From this point on we let δ := δ2s and θ := θs,2s ; Analysis in [5] (Theorem 2) and the current paper
yields the following constants, where C3 has not been optimized,
1 + δ
C0
C2 = 2C ′
where C ′
0 +
0 =
1 − δ − θ
1 − δ − θ
1−δ−θ (cid:17) + (1 + 1/√2) (1+δ)2
where C0 = 2√2 (cid:16)1 + 1−δ2
1−δ−θ ; We now deﬁne
3 = 3(√1 + a + τ −1 )2 ((C ′
1 + δ
4(1 + a)
C1 = C ′
and C 2
0 + C4 )2 + 1) +
0 +
Λ2
1 − δ − θ
min (2s0 )
We ﬁrst set up the notation following that in [5]. We order the βj ’s in decreasing order of magnitude
(3.4)
|β1 | ≥ |β2 |... ≥ |βp |.
Recall that s0 is the smallest integer such that Pp
i , λ2σ2 ) ≤ s0λ2σ2 , where λ =
i=1 min(β 2
p2 log p/n. Thus by deﬁnition of s0 , as essentially shown in [5], that 0 ≤ s0 ≤ s and
i , λ2σ2 ) ≤ 2 log p   σ2
n (cid:19)! (3.5)
min (cid:18)β 2
pXi=1
pXi=1
σ2
s0λ2σ2 ≤ λ2σ2 +
min(β 2
+
i ,
n
s0+1Xj=1
min(β 2
j , λ2σ2 ) ≥ (s0 + 1) min(β 2
s0+1 , λ2σ2 ) for s < p,
(3.6)

θ(1 + δ)
(1 − δ − θ)2 ,

and s0λ2σ2 ≥

.

(3.3)

+

(3.2)

6

(3.7)

(3.8)

s0+1 , λ2σ2 ) < λ2σ2 and hence by (3.4),
which implies that min(β 2
|βj | < λσ for all j > s0 .
We now show in Lemma 3.2 that thresholding at the level of C λσ at step 1 selects a set I of at most
2s0 variables, among which at most s0 are from S c .
Lemma 3.2 Choose τ > 0 such that δ2s + θs,2s < 1 − τ . Let βinit be the ℓ1 -minimizer subject to
the constraints, for λ := p2 log p/n and λp,τ := (√1 + a + t−1 )p2 log p/n,
(cid:13)(cid:13)(cid:13)(cid:13)
X T (Y − X βinit )(cid:13)(cid:13)(cid:13)(cid:13)∞ ≤ λp,τ σ.
1
n
Given some constant C4 ≥ C1 , for C1 as in (3.3), choose a thresholding parameter t0 so that
C4λp,τ σ ≥ t0 > C1λp,τ σ ; Set I = {j : |βj,init | > t0 }.
0 as in (3.2),
Then with probability at least P (Ta ), as detailed in Proposition 3.1, we have for C ′
(3.9)
|I | ≤ 2s0 , and |I ∪ S | ≤ s + s0 , and
kβD k2 ≤ q(C ′
0 + C4 )2 + 1λp,τ σ√s0 , where D := {1, . . . , p} \ I .
Next we show that even if we miss some columns of X in S , we can still hope to get the convergence
rate as required in Theorem 1.2 so long as kβD k2 is bounded and I is sufﬁciently sparse, for example,
as bounded in Lemma 3.2. We ﬁrst show in Lemma 3.3 a general res ult on rate of convergence of
the OLS estimator based on a chosen model I , where a subset of relevant variables are missing.
Lemma 3.3 (OLS estimator with missing variables) Let D := {1, . . . , p} \ I and SR = D ∩ S
such that I ∩ SR = ∅. Suppose |I ∪ SR | ≤ 2s. Then we have on Ta , for the least squares estimator
based on I , bβI = (X T
I XI )−1X T
I Y , it holds that
(cid:13)(cid:13)(cid:13) bβI − β(cid:13)(cid:13)(cid:13)
2 ≤ (cid:16)(cid:16)θ|I |,|SR | kβD k2 + λσ,a,pp|I |(cid:17) /Λmin (|I |)(cid:17)2
2
Now Theorem 1.2 is an immediate corollary of Lemma 3.2 and 3.3 in view of (3.5), given that
|SR | < s, and |I | ≤ 2s0 and |I ∪ SR | ≤ |I ∪ S | ≤ s + s0 ≤ 2s as in Lemma 3.2 (3.9). Hence it is
clear by (3.10) that we cannot cut too many “signi ﬁcant ” vari
ables; in particular, for those that are
larger λσ√s0 , we can cut at most a constant number of them.

+ kβD k2
2 .

(3.10)

4 Linear sparsity and random matrices

A special case of design matrices that satisfy the Restricted Eigenvalue assumptions are the random
design matrices. This is shown in a large body of work, for example [3, 4, 5, 1, 13], which shows
that the uniform uncertainty principle (UUP) holds for “gen eric” or random design matrices for very
signi ﬁcant values of s. For example, it is well known that for a random matrix with i. i.d. Gaussian
variables (that is, Gaussian Ensemble, subject to normalizations of columns), and the Bernoulli and
Subgaussian Ensembles [1, 13], the UUP holds for s = O(n/ log(p/n)); hence the thresholding
procedure can recover a sparse model using nearly a constant number of measurements per non-
zero component despite the stochastic noise, when n is a nonnegligible fraction of p. See [5] for
other examples of random designs. In our simulations as shown in Section 5, exact recovery rate of
the sparsity pattern is very high for a few types of random mat rices using a two-step procedure, once
the number of samples passes a certain threshold. For example, for an i.i.d. Gaussian Ensemble, the
threshold for exact recovery is n = Θ(s log(p/n)), where Θ hides a very small constant, when βmin
is sufﬁciently large; this shows a strong contrast with the o rdinary Lasso, for which the probability of
success in terms of exact recovery of the sparsity pattern te nds to zero when n < 2s log(p − s) [19].
In an ongoing work, the author is exploring thresholding algorithms for a broader class of random
designs that satisfy the Restricted Eigenvalue assumptions.

7

(a) p = 256

(b) p = 512

s = 8 Two−step
s = 8 Lasso
s = 64 Two−step
s = 64 Lasso

20

50

100

200
n

500

1000

(c) p = 1024

s=18
s=36
s=64
s=103
s=128
s=192
s=256

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

s
s
e
c
c
u
s
 
f
o
 
.
b
o
r
P

n

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
0
8

0
0
7

0
0
6

0
0
5

0
0
4

0
0
3

0
0
2

s=9
s=18
s=32
s=57
s=64
s=96
s=128

0

100

200

300

400

500

n

(d) p = 1024 Sample size vs. Sparsity

Prob. of succ.

90%
80%

200

400

600

800

n

50

100

150

s

200

250

Figure 1: (a) Compare the probability of success under s = 8 and 64 for p = 256. The two-step
procedure requires much fewer samples than the ordinary Las so. (b) (c) show the probability of
success of the two-step procedure under different levels of sparsity when n increases for p = 512
and 1024 respectively; (d) The number of samples n increases almost linearly with s for p = 1024.

5

Illustrative experiments

In our implementation, we choose to use the Lasso as the initial estimator. We show in Figure 1
that the two-step procedure indeed recovers a sparse model u sing a small number of samples per
non-zero component in β when X is a Gaussian Ensemble. Similar behavior was also observed
for the Bernoulli Ensemble in our simulations. We run under t hree cases of p = 256, 512, 1024;
for each p, we increase the sparsity s by roughly equal steps from s = 0.2p/log 0.2p to p/4. For
each tuple (p, s, n), we ﬁrst generate a random Gaussian Ensemble of size n × p as X , where
Xij ∼ N (0, 1), which is then normalized to have column ℓ2 -norm √n. For a given (p, s, n) and
X , we repeat the following experiment 100 times: 1) Generate a vector β of length p: within β
randomly choose s non-zero positions; for each position, we assign a value of 0.9 or −0.9 randomly.
2) Generate a vector ǫ of length p according to N (0, Ip ), where Ip is the identity matrix. 3) Compute
Y = X β + ǫ. Y and X are then fed to the two-step procedure to obtain bβ . 4) We then compare
bβ with β ; if all components match in signs, we count this experiment as a success. At the end of
the 100 experiments, we compute the percentage of successful runs as the probability of success.
We compare with the ordinary Lasso, for which we search over t he full path of LARS [8] and
always choose the bβ that best matches β in terms of support. Inside the two-step procedure, we
always ﬁx λn ≈ 0.69p2 log p/n and threshold βinit at t0 = ftq log p
√
bs, where bs = | bS0 | for
n
bS0 = {j : βj,init ≥ 0.5λn }, and ft is a constant chosen from the range of [1/6, 1/3].
Acknowledgments. This research was supported by the Swiss National Science Foundation
(SNF) Grant 20PA21-120050/1. The author thanks Larry Wasse rman, Sara van de Geer and Pe-
ter B ¨uhlmann for helpful discussions, comments and their kind support throughout this work.

8

References

[1] R. G. Baraniuk, M. Davenport, R. A. DeVore, and M. B. Wakin. A simple proof of the restricted isometry
property for random matrices. Constructive Approximation, 28(3):253–263, 2008.
[2] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The
Annals of Statistics, 37(4):1705–1732, 2009.
[3] E. Cand `es, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements.
Communications in Pure and Applied Mathematics, 59(8):1207–1223, August 2006.
[4] E. Cand `es and T. Tao. Decoding by Linear Programming. IEEE Trans. Info. Theory, 51:4203–4215, 2005.
[5] E. Cand `es and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals of
Statistics, 35(6):2313–2351, 2007.
[6] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on
Scientiﬁc and Statistical Computing , 20:33–61, 1998.
[7] D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81:425–455,
1994.
[8] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–
499, 2004.
[9] E. Greenshtein and Y. Ritov. Persistency in high dimensional linear predictor-selection and the virtue of
over-parametrization. Bernoulli, 10:971–988, 2004.
[10] V. Koltchinskii. Dantzig selector and sparsity oracle inequalities. Bernoulli, 15(3):799–828, 2009.
[11] N. Meinshausen and P. B ¨uhlmann. High dimensional graphs and variable selection with the Lasso. Annals
of Statistics, 34(3):1436–1462, 2006.
[12] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.
Annals of Statistics, 37(1):246–270, 2009.
[13] S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann. Uniform uncertainty principle for bernoulli and
subgaussian ensembles. Constructive Approximation, 28(3):277–289, 2008.
[14] D. Needell and J. A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples.
Applied and Computational Harmonic Analysis, 26(3):301–321, 2008.
[15] D. Needell and R. Vershynin. Signal recovery from incomplete and inaccurate measurements via regu-
larized orthogonal matching pursuit.
IEEE Journal of Selected Topics in Signal Processing, to appear,
2009.
[16] R. Tibshirani. Regression shrinkage and selection via the Lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267–288,
1996.
[17] S. A. van de Geer. The deterministic Lasso. The JSM Proceedings, American Statistical Association, 2007.
[18] S. A. van de Geer. High-dimensional generalized linear models and the Lasso. The Annals of Statistics,
36:614–645, 2008.
[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1 -constrained
quadratic programming. IEEE Trans. Inform. Theory, 2008.
to appear, also posted as Technical Report
709, 2006, Department of Statistics, UC Berkeley.
[20] L. Wasserman and K. Roeder. High dimensional variable selection. The Annals of Statistics, 37(5A):2178–
2201, 2009.
[21] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research,
7:2541–2567, 2006.
[22] S. Zhou, S. van de Geer, and P. B ¨uhlmann. Adaptive Lasso for high dimensional regression and gauss ian
graphical modeling, 2009. arXiv:0903.2515.
[23] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical Association,
101:1418–1429, 2006.

9


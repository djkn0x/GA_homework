Canonical Time Warping
for Alignment of Human Behavior

Feng Zhou
Robotics Institute
Carnegie Mellon University
www.f-zhou.com

Fernando de la Torre
Robotics Institute
Carnegie Mellon University
ftorre@cs.cmu.edu

Abstract

Alignment of time series is an important problem to solve in many scientiﬁc dis-
ciplines. In particular, temporal alignment of two or more subjects performing
similar activities is a challenging problem due to the large temporal scale differ-
ence between human actions as well as the inter/intra subject variability. In this
paper we present canonical time warping (CTW), an extension of canonical cor-
relation analysis (CCA) for spatio-temporal alignment of human motion between
two subjects. CTW extends previous work on CCA in two ways: (i) it combines
CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing
local spatial deformations. We show CTW’s effectiveness in three experiments:
alignment of synthetic data, alignment of motion capture data of two subjects per-
forming similar actions, and alignment of similar facial expressions made by two
people. Our results demonstrate that CTW provides both visually and qualitatively
better alignment than state-of-the-art techniques based on DTW.

1
Introduction
Temporal alignment of time series has been an active research topic in many scientiﬁc disciplines
such as bioinformatics, text analysis, computer graphics, and computer vision. In particular, tem-
poral alignment of human behavior is a fundamental step in many applications such as recognition
[1], temporal segmentation [2] and synthesis of human motion [3]. For instance consider Fig. 1a
which shows one subject walking with varying speed and different styles and Fig. 1b which shows
two subjects reading the same text.
Previous work on alignment of human motion has been addressed mostly in the context of recog-
nizing human activities and synthesizing realistic motion. Typically, some models such as hidden
Markov models [4, 5, 6], weighted principal component analysis [7], independent component anal-
ysis [8, 9] or multi-linear models [10] are learned from training data and in the testing phase the
time series is aligned w.r.t.
the learned dynamic model. In the context of computer vision a key
aspect for successful recognition of activities is building view-invariant representations. Junejo et
al. [1] proposed a view-invariant descriptor for actions making use of the afﬁnity matrix between
time instances. Caspi and Irani [11] temporally aligned videos from two closely attached cameras.
Rao et al. [12, 13] aligned trajectories of two moving points using constraints from the fundamental
matrix. In the literature of computer graphics, Hsu et al. [3] proposed the iterative motion warping,
a method that ﬁnds a spatio-temporal warping between two instances of motion captured data. In the
context of data mining there have been several extensions of DTW [14] to align time series. Keogh
and Pazzani [15] used derivatives of the original signal to improve alignment with DTW. Listgarten
et al. [16] proposed continuous proﬁle models, a probabilistic method for simultaneously aligning
and normalizing sets of time series.
A relatively unexplored problem in behavioral analysis is the alignment between the motion of the
body of face in two or more subjects (e.g., Fig. 1). Major challenges to solve human motion align-

1

Figure 1: Temporal alignment of human behavior. (a) One person walking in normal pose, slow
speed, another viewpoint and exaggerated steps (clockwise). (b) Two people reading the same text.

ment problems are: (i) allowing alignment between different sets of multidimensional features (e.g.,
audio/video), (ii) introducing a feature selection or feature weighting mechanism to compensate for
subject variability or irrelevant features and (iii) execution rate [17]. To solve these problems, this
paper proposes canonical time warping (CTW) for accurate spatio-temporal alignment between two
behavioral time series. We pose the problem as ﬁnding the temporal alignment that maximizes the
spatial correlation between two behavioral samples coming from two subjects. To accommodate for
subject variability and take into account the difference in the dimensionally of the signals, CTW uses
CCA as a measure of spatial alignment. To allow temporal changes CTW incorporates DTW. CTW
extends DTW by adding a feature weighting mechanism that is able to align signals of different
dimensionality. CTW also extends CCA by incorporating time warping and allowing local spatial
transformations.
The remainder of the paper is organized as follows. Section 2 reviews related work on dynamic time
warping and canonical correlation analysis. Section 3 describes the new CTW algorithm. Section 4
extends CTW to take into account local transformations. Section 5 provides experimental results.

2 Previous work
This section describes previous work on canonical correlation analysis and dynamic time warping.

2.1 Canonical correlation analysis
Canonical correlation analysis (CCA) [18] is a technique to extract common features from a pair of
multivariate data. CCA identiﬁes relationships between two sets of variables by ﬁnding the linear
combinations of the variables in the ﬁrst set1 (X ∈ Rdx×n ) that are most correlated with the linear
combinations of the variables in the second set (Y ∈ Rdy ×n ). Assuming zero-mean data, CCA ﬁnds
a combination of the original variables that minimizes:
Jcca (Vx , Vy ) = (cid:107)VT
y Y(cid:107)2
x X − VT
x XXT Vx = VT
y YYT Vy = Ib ,
s.t. VT
(1)
F
where Vx ∈ Rdx×b is the projection matrix for X (similarly for Vy ). The pair of canonical variates
x X, vT
(vT
y Y) is uncorrelated with other canonical variates of lower order. Each successive canon-
ical variate pair achieves the maximum correlation orthogonal to the preceding pairs. Eq. 1 has a
closed form solution in terms of a generalized eigenvalue problem. See [19] for a uniﬁcation of
several component analysis methods and a review of numerical techniques to efﬁciently solve the
generalized eigenvalue problems.
In computer vision, CCA has been used for matching sets of images in problems such as activity
recognition from video [20] and activity correlation from cameras [21]. Recently, Fisher et al. [22]
1Bold capital letters denote a matrix X, bold lower-case letters a column vector x. xi represents the ith
column of the matrix X. xij denotes the scalar in the ith row and j th column of the matrix X. All non-bold
letters represent scalars. 1m×n , 0m×n ∈ Rm×n are matrices of ones and zeros. In ∈ Rn×n is an identity
√
xT x denotes the Euclidean distance. (cid:107)X(cid:107)2
matrix. (cid:107)x(cid:107) =
F = Tr(XT X) designates the Frobenious norm.
X ◦ Y and X ⊗ Y are the Hadamard and Kronecker product of matrices. Vec(X) denotes the vectorization of
matrix X. {i : j } lists the integers, {i, i + 1, · · · , j − 1, j }.

2

(a)(b)Figure 2: Dynamic time warping. (a) 1-D time series (nx = 7 and ny = 9). (b) DTW alignment.
(d) Policy function at each node, where ↑, (cid:45), ← denote the policy,
(c) Binary distance matrix.
π(pt ) = [1, 0]T , [1, 1]T , [0, 1]T , respectively. The optimal alignment path is denoted in bold.

proposed an extension of CCA with parameterized warping functions to align protein expressions.
The learned warping function is a linear combination of hyperbolic tangent functions with non-
negative coefﬁcients, ensuring monotonicity. Unlike our method, the warping function is unable to
deal with feature weighting.

− ypy
t

(cid:107)2 ,

(2)

(cid:107)xpx
t

Jdtw (P) =

2.2 Dynamic time warping
Given two time series, X = [x1 , x2 , · · · , xnx ] ∈ Rd×nx and Y = [y1 , y2 , · · · , yny ] ∈ Rd×ny ,
dynamic time warping [14] is a technique to optimally align the samples of X and Y such that the
m(cid:88)
following sum-of-squares cost is minimized:
t=1
where m is the number of indexes (or steps) needed to align both signals. The correspondence
matrix P can be parameterized by a pair of path vectors, P = [px , py ]T ∈ R2×m , in which px ∈
{1 : nx }m×1 and py ∈ {1 : ny }m×1 denote the composition of alignment in frames. For instance,
t , py
t ]T = [i, j ]T
the ith frame in X and the j th frame in Y are aligned iff there exists pt = [px
for some t. P has to satisfy three additional constraints: boundary condition (p1 ≡ [1, 1]T and
pm ≡ [nx , ny ]T ), continuity (0 ≤ pt − pt−1 ≤ 1) and monotonicity (t1 ≥ t2 ⇒ pt1 − pt2 ≥ 0).
gramming [23] offers an efﬁcient (O(cid:0)nxny
(cid:1)) approach to minimize Jdtw using Bellman’s equation:
Although the number of possible ways to align X and Y is exponential in nx and ny , dynamic pro-
(cid:107)2 + L∗ (pt+1 ),
− ypy
(cid:107)xpx
L∗ (pt ) = min
(3)
t
t
π(pt )
where the cost-to-go value function, L∗ (pt ), represents the remaining cost starting at tth step to
be incurred following the optimum policy π∗ . The policy function, π : {1 : nx } × {1 : ny } →
{[1, 0]T , [0, 1]T , [1, 1]T }, deﬁnes the deterministic transition between consecutive steps, pt+1 =
pt + π(pt ). Once the policy queue is known, the alignment steps can be recursively constructed
from the starting point, p1 = [1, 1]T . Fig. 2 shows an example of DTW to align two 1-D time series.

3 Canonical time warping (CTW)
This section describes the energy function and optimization strategies for CTW.

3.1 Energy function for CTW
In order to have a compact and compressible energy function for CTW, it is important to notice that
ny(cid:88)
nx(cid:88)
Eq. 2 can be rewritten as:
y (cid:107)2
x − YWT
j (cid:107)xi − yj (cid:107)2 = (cid:107)XWT
T wy
wx
F ,
i
j=1
i=1
where Wx ∈ {0, 1}m×nx , Wy ∈ {0, 1}m×ny are binary selection matrices that need to be inferred
to align X and Y . In Eq. 4 the matrices Wx and Wy encode the alignment path. For instance,

Jdtw (Wx , Wy ) =

(4)

3

123456723451234567892345(a)(b)(c)(d)01110110110111111011011111111101001111101001101110110110111111012345678912345671234567891234567th frame in Y . For
th frame in X and py
= wy
= 1 assigns correspondence between the px
wx
tpx
tpy
t
t
t
t
convenience, we denote, Dx = WT
y Wy and W = WT
x Wx , Dy = WT
x Wy . Observe that Eq.
4 is very similar to the CCA’s objective (Eq. 1). CCA applies a linear transformation to the rows
(features), while DTW applies binary transformations to the columns (time).
In order to accommodate for differences in style and subject variability, add a feature selection mech-
anism, and reduce the dimensionality of the signals, CTW adds a linear transformation (VT
x , VT
y )
(as CCA) to the least-squares form of DTW (Eq. 4). Moreover, this transformation allows aligning
temporal signals with different dimensionality (e.g., video and motion capture). CTW combines
DTW and CCA by minimizing:
y (cid:107)2
x − VT
Jctw (Wx , Wy , Vx , Vy ) = (cid:107)VT
y YWT
x XWT
(5)
F ,
where Vx ∈ Rdx×b , Vy ∈ Rdy ×b , b ≤ min(dx , dy ) parameterize the spatial warping by pro-
jecting the sequences into the same coordinate system. Wx and Wy warp the signal in time to
achieve optimum temporal alignment. Similar to CCA, to make CTW invariant to translation, rota-
x 1m = 0dx , YWT
y 1m = 0dy , (ii)
tion and scaling, we impose the following constraints: (i) XWT
x XDxXT Vx = VT
y YDyYT Vy = Ib and (iii) VT
x XWYT Vy to be a diagonal matrix. Eq. 5
VT
is the main contribution of this paper. CTW is a direct and clean extension of CCA and DTW to
align two signals X and Y in space and time. It extends previous work on CCA by adding temporal
alignment and on DTW by allowing a feature selection and dimensionality reduction mechanism for
aligning signals of different dimensions.

3.2 Optimization for CTW

Algorithm 1: Canonical Time Warping
input : X, Y
output: Vx , Vy , Wx , Wy
begin
Initialize Vx = Idx , Vy = Idy
repeat
x X, VT
Use dynamic programming to compute, Wx , Wy , for aligning the sequences, VT
y Y
(cid:20) XDxXT
(cid:20)
(cid:21)
(cid:21)
y ], be the leading b generalized eigenvectors of:
Set columns of, VT = [VT
x , VT
0
XWYT
0
0
YDyYT

0
YWT XT

V =

VΛ

until Jctw converges
end

Optimizing Jctw is a non-convex optimization problem with respect to the alignment matrices
(Wx , Wy ) and projection matrices (Vx , Vy ). We alternate between solving for Wx , Wy using
DTW, and optimally computing the spatial projections using CCA. These steps monotonically de-
crease Jctw and since the function is bounded below it will converge to a critical point.
Alg. 1 illustrates the optimization process (e.g., Fig. 3e). The algorithm starts by initializing Vx
and Vy with identity matrices. Alternatively, PCA can be applied independently to each set, and
used as initial estimation of Vx and Vy if dx (cid:54)= dy . In the case of high-dimensional data, the
generalized eigenvalue problem is solved by regularizing the covariance matrices adding a scaled
identity matrix. The dimension b is selected to preserve 90% of the total correlation. We consider
the algorithm to converge when the difference between two consecutive values of Jctw is small.

4 Local canonical time warping (LCTW)
In the previous section we have illustrated how CTW can align in space and time two time series of
different dimensionality. However, there are many situations (e.g., aligning long sequences) where
a global transformation of the whole time series is not accurate. For these cases, local models
have been shown to provide better performance [3, 24, 25]. This section extends CTW by allowing
multiple local spatial deformations.

4

rx
icx

(6)

(cid:107)2

=

wx
i

4.1 Energy function for LCTW
Let us assume that the spatial transformation for each frame in X and Y can be model as a
T ]T ∈ Rkx dx×b , Vy =
T , · · · , Vx
linear combination of kx or ky bases. Let be Vx = [Vx
1
kx
T ]T ∈ Rky dy ×b and b ≤ min(kxdx , ky dy ). CTW allows for a more ﬂexible spatial
T , · · · , Vy
[Vy
1
ky
warping by minimizing:
j (cid:107)(cid:16) kx(cid:88)
T (cid:17)
xi − (cid:16) ky(cid:88)
T (cid:17)
ky(cid:88)
kx(cid:88)
nx(cid:88)
ny(cid:88)
Jlctw (Wx , Wy , Vx , Vy , Rx , Ry )
(cid:105)
(cid:104)
(cid:104)
(cid:105)
T wy
Vx
cx
cx=1
cy =1
cx=1
cy =1
j=1
i=1
=(cid:107)VT
F + (cid:107)FyRy (cid:107)2
F + (cid:107)FxRx (cid:107)2
y (cid:107)2
x − VT
x ⊗ 1dx )
y ⊗ 1dy )
(1ky ⊗ Y) ◦ (RT
(1kx ⊗ X) ◦ (RT
WT
WT
F ,
x
y
where Rx ∈ Rnx×kx , Ry ∈ Rny ×ky are the weighting matrices. rx
icx denotes the coefﬁcient (or
x basis for the ith frame of X (similarly for ry
). We further constrain the weights
weight) of the cth
j cy
to be positive (i.e., Rx , Ry ≥ 0) and the sum of weights to be one (i.e., Rx1kx = 1nx , Ry 1ky =
1ny ) for each frame. The last two regularization terms, Fx ∈ Rnx×nx , Fy ∈ Rny ×ny , are 1st
∈ Rny ×1 , encouraging smooth solutions over time.
∈ Rnx×1 , ry
order differential operators of rx
cy
cx
Observe that Jctw is a special case of Jlctw when kx = ky = 1.

(cid:107)Fx rx
cx

(cid:107)Fy ry
cy

yj (cid:107)2 +

(cid:107)2 +

ry
j cy

Vy
cy

4.2 Optimization for LCTW

Algorithm 2: Local Canonical Time Warping
input : X, Y
output: Wx , Wy , Vx , Vy , Rx , Ry
begin
Initialize,

= 1 for (cid:98) (cx − 1)nx
kx

rx
icx

Vx = 1kx ⊗ Idx , Vy = 1ky ⊗ Idy
= 1 for (cid:98) (cy − 1)ny
(cid:99) < i ≤ (cid:98) cxnx
(cid:99),
ry
j cy
ky
kx

(cid:99) < j ≤ (cid:98) cy ny
ky

(cid:99)

repeat
Denote,

x ⊗ 1dx ), Zy = (1ky ⊗ Y) ◦ (RT
Zx = (1kx ⊗ X) ◦ (RT
y ⊗ 1dy )
x (Ikx ⊗ X), Qy = VT
y (Iky ⊗ Y)
Qx = VT
Use dynamic programming to compute, Wx , Wy , between the sequences, VT
x Zx , VT
y Zy
(cid:21)
(cid:20) ZxDxZT
(cid:21)
(cid:20)
Set columns of, VT = [VT
y ], be the leading b generalized eigenvectors,
x , VT
0
ZxWZT
0
x
y
ZyDy ZT
0
ZyWT ZT
0
y
x
(cid:20) 1kx×kx ⊗ Dx ◦ QT
Set, r = Vec([Rx , Ry ]), be the solution of the quadratic programming problem,
x Qx + Ikx ⊗ FT
−1kx×ky ⊗ W ◦ QT
(cid:20) 1T
(cid:21)
x Fx
x Qy
rT
−1ky ×kx ⊗ WT ◦ QT
y Qy + Iky ⊗ FT
1ky ×ky ⊗ Dy ◦ QT
y Qx
y Fy
⊗ Inx
0
r ≥ 0nx kx+ny ky
r = 1nx+ny
⊗ Iny
kx
0
until Jlctw converges
end

min
r

V =

1T
ky

VΛ

s.t.

(cid:21)

r

As in the case of CTW, we use an alternating scheme for optimizing Jlctw , which is summarized in
Alg. 2. In the initialization, we assume that each time series is divided into kx or ky equal parts,
being the identity matrix the starting value for Vx
, Vy
cy and block structure matrices for Rx , Ry .
cx

5

The main difference between the alternating scheme of Alg. 1 and Alg. 2 is that the alternation
step is no longer unique. For instance, when ﬁxing Vx , Vy , one can optimize either Wx , Wy
or Rx , Ry . Consider a simple example of warping sin(t1 ) towards sin(t2 ), one could shift the
ﬁrst sequence along time axis by δt = t2 − t1 or do the linear transformation, at1 sin(t1 ) + bt1 ,
where at1 = cos(t2 − t1 ) and bt1 = cos(t1 ) sin(t2 − t1 ).
In order to better control the trade-
off between time warping and spatial transformation, we propose a stochastic selection process.
Let us denote pw|v the conditional probability of optimizing W when ﬁxing V. Given the prior
(cid:35)
(cid:34)
(cid:35)
(cid:34) pw −pv
probabilities [pw , pv , pr ], we can derive the conditional probabilities using Bayes’ theorem and the
fact that, [pr|w , pr|v , pv |r ] = 1 − [pv |w , pw|v , pw|r ].
[pv |w , pw|v , pw|r ]T = A−1b , where A =
0
0
0
. Fig. 3f (right-lower corner) shows the optimization
pw−pv + pr
pr
pw
0 −pv
pr
strategy, pw = .5, pv = .3, pr = .2, where the time warping process is more often optimized.

and b =

5 Experiments
This section demonstrates the beneﬁts of CTW and LCTW against state-of-the-art DTW approaches
to align synthetic data, motion capture data of two subjects performing similar actions, and similar
facial expressions made by two people.

5.1 Synthetic data
In the ﬁrst experiment we synthetically generated two spatio-temporal signals (3-D in space and 1-D
in time) to evaluate the performance of CTW and LCTW. The ﬁrst two spatial dimensions and the
y , where Z ∈ R2×m is
x and Y = UT
time dimension are generated as follows: X = UT
y ZMT
x ZMT
a curve in two dimensions (Fig. 3a). Ux , Uy ∈ R2×2 are randomly generated afﬁne transformation
matrices for the spatial warping and Mx ∈ Rnx×m , My ∈ Rny ×m , m ≥ max(nx , ny ) are randomly
generated matrices for time warping2 . The third spatial dimension is generated by adding a (1 × nx )
or (1 × ny ) extra row to X and Y respectively, with zero-mean Gaussian noise (see Fig. 3a-b).
We compared the performance of CTW and LCTW against three other methods: (i) dynamic time
warping (DTW) [14], (ii) derivative dynamic time warping (DDTW) [15] and (iii) iterative time
warping (IMW) [3]. Recall that in the case of synthetic data we know the ground truth alignment
matrix Wtruth = MxMT
y . The error between the ground truth and a given alignment Walg is
computed by the area enclosed between both paths (see Fig. 3g).
Fig. 3c-f show the spatial warping estimated by each algorithm. DDTW (Fig. 3c) cannot deal with
this example because the feature derivatives do not capture well the structure of the sequence. IMW
(Fig. 3d) warps one sequence towards the other by translating and re-scaling each frame in each
dimension. Fig. 3h shows the testing error (space and time) for 100 new generated time series. As it
can be observed CTW and LCTW obtain the best performance. IMW has more parameters (O(dn))
than CTW (O(db)) and LCTW (O(kdb + kn)), and hence IMW is more prone to overﬁtting. IMW
tries to ﬁt the noisy dimension (3rd spatial component) biasing alignment in time (Fig. 3g), whereas
CTW and LCTW have a feature selection mechanism which effectively cancels the third dimension.
x = [.002, .001, −.067]T , vT
y =
Observe that the null space for the projection matrices in CTW is vT
[−.002, −.001, −.071]T .

5.2 Motion capture data
In the second experiment we apply CTW and LCTW to align human motion with similar behavior.
The motion capture data is taken from the CMU-Multimodal Activity Database [26]. We selected
a pair of sub-sequences from subject 1 and subject 3 cooking brownies. Typically, each sequence
contains 500-1000 frames. For each instance we computed the quaternions for the 20 joints resulting
in a 60 dimensional feature vector that describes the body conﬁguration. CTW and LCTW are
initialized as described in previous sections and optimized until convergence. The parameters of
LCTW are manually set to kx = 3, ky = 3 and pw = .5, pv = .3, pr = .2.

2The generation of time transformation matrix Mx (similar for My ) is initialized by setting Mx = Inx .
Then, randomly pick and replicate m − nx columns of Mx . We normalize each row Mx1m = 1nx to make
the new frame to be an interpolation of zi .

6

Figure 3: Example with synthetic data. Time series are generated by (a) spatio-temporal transfor-
mation of 2-D latent sequence (b) adding Gaussian noise in the 3rd dimension. The result of space
warping is computed by (c) derivative dynamic time warping (DDTW), (d) iterative time warping
(IMW), (e) canonical time warping (CTW) and (f) local canonical time warping (LCTW). The en-
ergy function and order of optimizing the parameters for CTW and LCTW are shown in the top right
and lower right corner of the graphs. (g) Comparison of the alignment results for several methods.
(h) Mean and variance of the alignment error.

Figure 4: Example of motion capture data alignment. (a) PCA. (b) CTW. (c) LCTW. (d) Alignment
path. (e) Motion capture data. 1st row subject one, rest of the rows aligned subject two.

Fig. 4 shows the alignment results for the action of opening a cabinet. The projection on the principal
components for both sequences can be seen in Fig. 4a. CTW and LCTW project the sequences in
a low dimensional space that maximizes the correlation (Fig. 4b-c). Fig. 4d shows the alignment
path. In this case, we do not have ground truth data, and we evaluated the results visually. The ﬁrst
row of Fig. 4e shows few instances of the ﬁrst subject, and the last three rows the alignment of the
third subject for DTW, CTW and LCTW. Observe that CTW and LCTW achieve better temporal
alignment.

7

(a)(b)(c)(d)(e)(f)−100102030−10010203040−202−202−2024−4−202451015202530352530354045−20220406080102030405060708090  TruthDTWDDTWIMWCTWLCTW−0.100.10.2−0.15−0.1−0.0500.050.10.15510150.20.45101520WV−0.100.10.2−0.15−0.1−0.0500.050.10.15510150245101520WVR(g)(h)−10010203040−10010203040DTWDDTWIMWCTWLCTW00.020.040.060.080.10.120.140.16(a)(b)(c)(d)−0.500.5−0.500.500.050.10.15−0.0500.05−0.15−0.1−0.050−0.0500.05200400600800100200300400500600700  DTWCTWLCTWDTWCTWLCTW(e)5.3 Facial expression data
In this experiment we tested the ability of CTW and LCTW to align facial expressions. We took
29 subjects from the RU-FACS database [27] which consists of interviews with men and women
of varying ethnicity. The action units (AUs) in this database have been manually coded, and we
selected AU12 (smiling) to run our experiments. Each event of AU12 is coded with an onset (start),
peak and offset (end). We used person-speciﬁc AAM [28] to track 66 landmark points on the face.
For the alignment of AU12 we only used 18 landmarks corresponding to the outline of the mouth,
so for each frame we have a vector (R36×1 ) with (x, y) coordinates.
We took subject 14 and 30 and ran CTW and LCTW on the segments where the AU12 was coded.
The parameters of LCTW are manually set to kx = 3, ky = 3 and pw = .5, pv = .3, pr = .2. Fig. 5
shows the results of the alignment. Fig. 5b-c shows that the low dimensional projection obtained
with CTW and LCTW has better alignment than DTW in Fig. 5a. Fig. 5d shows the position of
the peak frame as the intersection of the two dotted lines. As we can observe from Fig. 5d, the
alignment paths found by CTW and LCTW are closer to the manually labeled peak than the ones
found by DTW. This shows that CTW and LCTW provide better alignment because the manually
labeled peaks in both sequences should be aligned. Fig. 5e shows several frames illustrating the
alignment.

Figure 5: Example of facial expression alignment. (a) PCA. (b) CTW. (c) LCTW. (d) Alignment
path. (e) Frames from an AU12 event. The AU peaks are indicated by arrows.

6 Conclusions

In this paper we proposed CTW and LCTW for spatio-temporal alignment of time series. CTW
integrates the beneﬁts of DTW and CCA into a clean and simple formulation. CTW extends DTW by
adding a feature selection mechanism and enables alignment of signals with different dimensionality.
CTW extends CCA by adding temporal alignment and allowing temporal local projections. We
illustrated the beneﬁts of CTW for alignment of motion capture data and facial expressions.

7 Acknowledgements

This material is based upon work partially supported by the National Science Foundation under
Grant No. EEC-0540865.

8

(a)(e)DTWCTWLCTW−150−100−50050100−50050(b)−0.2−0.100.10.2−0.1−0.0500.050.10.150.20.25(c)−0.2−0.100.1−0.2−0.100.10.2(d)50100150102030405060708090  DTWCTWLCTWReferences
[1] I. N. Junejo, E. Dexter, I. Laptev, and P. P ´erez. Cross-view action recognition from temporal self-
similarities. In ECCV, pages 293–306, 2008.
[2] F. Zhou, F. de la Torre, and J. K. Hodgins. Aligned cluster analysis for temporal segmentation of human
motion. In FGR, pages 1–7, 2008.
[3] E. Hsu, K. Pulli, and J. Popovic. Style translation for human motion. In SIGGRAPH, 2005.
[4] M. Brand, N. Oliver, and A. Pentland. Coupled hidden Markov models for complex action recognition.
In CVPR, pages 994–999, 1997.
[5] M. Brand and A. Hertzmann. Style machines. In SIGGRAPH, pages 183–192, 2000.
[6] G. W. Taylor, G. E. Hinton, and S. T. Roweis. Modeling human motion using binary latent variables. In
NIPS, volume 19, page 1345, 2007.
[7] A. Heloir, N. Courty, S. Gibet, and F. Multon. Temporal alignment of communicative gesture sequences.
J. Visual. Comp. Animat., 17(3-4):347–357, 2006.
[8] A. Shapiro, Y. Cao, and P. Faloutsos. Style components. In Graphics Interface, pages 33–39, 2006.
[9] G. Liu, Z. Pan, and Z. Lin. Style subspaces for character animation. J. Visual. Comp. Animat., 19(3-
4):199–209, 2008.
[10] A. M. Elgammal and C.-S. Lee. Separating style and content on a nonlinear manifold. In CVPR, 2004.
[11] Y. Caspi and M. Irani. Aligning non-overlapping sequences. Int. J. Comput. Vis., 48(1):39–51, 2002.
[12] C. Rao, A. Gritai, M. Shah, and T. Fathima Syeda-Mahmood. View-invariant alignment and matching of
video sequences. In ICCV, pages 939–945, 2003.
[13] A. Gritai, Y. Sheikh, C. Rao, and M. Shah. Matching trajectories of anatomical landmarks under view-
point, anthropometric and temporal transforms. Int. J. Comput. Vis., 2009.
[14] L. Rabiner and B.-H. Juang. Fundamentals of speech recognition. Prentice Hall, 1993.
[15] E. J. Keogh and M. J. Pazzani. Derivative dynamic time warping. In SIAM ICDM, 2001.
[16] J. Listgarten, R. M. Neal, S. T. Roweis, and A. Emili. Multiple alignment of continuous time series. In
NIPS, pages 817–824, 2005.
[17] Y. Sheikh, M. Sheikh, and M. Shah. Exploring the space of a human action. In ICCV, 2005.
[18] T. W. Anderson. An introduction to multivariate statistical analysis. Wiley-Interscience, 2003.
[19] F. de la Torre. A uniﬁcation of component analysis methods. Handbook of Pattern Recognition and
Computer Vision, 2009.
[20] T. K. Kim and R. Cipolla. Canonical correlation analysis of video volume tensors for action categorization
and detection. IEEE Trans. Pattern Anal. Mach. Intell., 31:1415–1428, 2009.
[21] C. C. Loy, T. Xiang, and S. Gong. Multi-camera activity correlation analysis. In CVPR, 2009.
[22] B. Fischer, V. Roth, and J. Buhmann. Time-series alignment by non-negative multiple generalized canon-
ical correlation analysis. BMC bioinformatics, 8(10), 2007.
[23] D. P. Bertsekas. Dynamic programming and optimal control. 1995.
[24] Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. University of
Toronto Tec. Rep., 1997.
[25] J. J. Verbeek, S. T. Roweis, and N. A. Vlassis. Non-linear CCA and PCA by alignment of local models.
In NIPS, 2003.
[26] F. de la Torre, J. K. Hodgins, J. Montano, S. Valcarcel, A. Bargteil, X. Martin, J. Macey, A. Collado,
and P. Beltran. Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) Database.
Carnegie Mellon University Tec. Rep., 2009.
[27] M. S. Bartlett, G. C. Littlewort, M. G. Frank, C. Lainscsek, I. Fasel, and J. R. Movellan. Automatic
recognition of facial actions in spontaneous expressions. J. Multimed., 1(6):22–35, 2006.
[28] I. Matthews and S. Baker. Active appearance models revisited. Int. J. Comput. Vis., 60(2):135–164, 2004.

9


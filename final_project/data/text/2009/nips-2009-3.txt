Learning from Multiple Partially Observed Views –
an Application to Multilingual Text Categorization

Massih R. Amini
Interactive Language Technologies Group
National Research Council Canada
Massih-Reza.Amini@cnrc-nrc.gc.ca

Nicolas Usunier
Laboratoire d’Informatique de Paris 6
Universit ´e Pierre et Marie Curie, France
Nicolas.Usunier@lip6.fr

Cyril Goutte
Interactive Language Technologies Group
National Research Council Canada
Cyril.Goutte@cnrc-nrc.gc.ca

Abstract

We address the problem of learning classi ﬁers when observat
ions have multiple
views, some of which may not be observed for all examples. We assume the
existence of view generating functions which may complete the missing views
in an approximate way. This situation corresponds for examp le to learning text
classi ﬁers from multilingual collections where documents are not available in all
languages. In that case, Machine Translation (MT) systems may be used to trans-
late each document in the missing languages. We derive a generalization error
bound for classi ﬁers learned on examples with multiple arti
ﬁcially created views.
Our result uncovers a trade-off between the size of the train ing set, the number
of views, and the quality of the view generating functions. As a consequence,
we identify situations where it is more interesting to use mu ltiple views for learn-
ing instead of classical single view learning. An extension of this framework is
a natural way to leverage unlabeled multi-view data in semi-supervised learning.
Experimental results on a subset of the Reuters RCV1/RCV2 collections support
our ﬁndings by showing that additional views obtained from M T may signi ﬁcantly
improve the classi ﬁcation performance in the cases identi ﬁ
ed by our trade-off.

1

Introduction

We study the learning ability of classi ﬁers trained on examp les generated from different sources,
but where some observations are partially missing. This pro blem occurs for example in non-parallel
multilingual document collections, where documents may be available in different languages, but
each document in a given language may not be translated in all (or any) of the other languages.

Our framework assumes the existence of view generating func tions which may approximate miss-
ing examples using the observed ones. In the case of multilingual corpora these view generating
functions may be Machine Translation systems which for each document in one language produce
its translations in all other languages. Compared to other multi-source learning techniques [6],
we address a different problem here by transforming our init ial problem of learning from partially
observed examples obtained from multiple sources into the c lassical multi-view learning. The con-
tributions of this paper are twofold. We ﬁrst introduce a sup ervised learning framework in which
we deﬁne different multi-view learning tasks. Our main resu lt is a generalization error bound for
classi ﬁers trained over multi-view observations. From thi s result we induce a trade-off between the
number of training examples, the number of views and the abil ity of view generating functions to

produce accurate additional views. This trade-off helps us identify situations in which arti ﬁcially
generated views may lead to substantial performance gains. We then show how the agreement of
classi ﬁers over their class predictions on unlabeled train ing data may lead to a much tighter trade-off.
Experiments are carried out on a large part of the Reuters RCV 1/RCV2 collections, freely available
from Reuters, using 5 well-represented languages for text c lassi ﬁcation. Our results show that our
approach yields improved classi ﬁcation performance in bot h the supervised and semi-supervised
settings.

In the following two sections, we ﬁrst deﬁne our framework, t
hen the learning tasks we address.
Section 4 describes our trade-off bound in the Empirical Risk Minimization (ERM) setting, and
shows how and when the additional, arti ﬁcially generated vi ews may yield a better generalization
performance in a supervised setting. Section 5 shows how to exploit these results when additional
unlabeled training data are available, in order to obtain a more accurate trade-off. Finally, section 6
describes experimental results that support this approach .

2 Framework and De ﬁnitions

In this section, we introduce basic deﬁnitions and the learn ing objectives that we address in our
setting of arti ﬁcially generated representations.

2.1 Observed and Generated Views

def= (x1 , ..., xV ), where different views xv provide a rep-
A multi-view observation is a sequence x
resentation of the same object in different sets Xv . A typical example is given in [3] where each
Web-page is represented either by its textual content ( ﬁrst view) or by the anchor texts which point
to it (second view). In the setting of multilingual classi ﬁc ation, each view is the textual representa-
tion of a document written in a given language (e.g. English, German, French).

We consider binary classi ﬁcation problems where, given a mu lti-view observation, some of the
views are not observed (we obviously require that at least one view is observed). This hap-
pens, for instance, when documents may be available in different languages, yet a given document
may only be available in a single language. Formally, our observations x belong to the input set
X def= (X1 ∪ {⊥}) × ... × (XV ∪ {⊥}), where xv =⊥ means that the v -th view is not observed.
(x, y), with y ∈ Y def= {0, 1}, drawn
In binary classi ﬁcation, we assume that examples are pairs
according to a ﬁxed, but unknown distribution D over X × Y , such that P(x,y)∼D (∀v : xv =⊥) = 0
(at least one view is available). In multilingual text classi ﬁcation, a parallel corpus is a dataset where
all views are always observed (i.e. P(x,y)∼D (∃v : xv =⊥) = 0), while a comparable corpus is a
dataset where only one view is available for each example (i.e. P(x,y)∼D (|{v : xv 6=⊥}| 6= 1) = 0).
For a given observation x, the views v such that xv 6=⊥ will be called the observed views. The
originality of our setting is that we assume that we have view generating functions Ψv→v ′ : Xv →
Xv ′ which take as input a given view xv and output an element of Xv ′ , that we assume is close
to what xv ′ would be if it was observed. In our multilingual text classi ﬁ cation example, the view
generating functions are Machine Translation systems. These generating functions can then be used
to create surrogate observations, such that all views are available. For a given partially observed x,
the completed observation x is obtained as:
∀v , xv = (cid:26) xv
Ψv ′→v (xv ′
6=⊥
In this paper, we focus on the case where only one view is observed for each example. This setting
corresponds to the problem of learning from comparable corpora, which will be the focus of our
experiments. Our study extends to the situation where two or more views may be observed in a
straightforward manner. Our setting differs from previous multi-view learning studies [5] mainly on
the straightforward generalization to more than two views a nd the use of view generating functions
to induce the missing views from the observed ones.

if xv 6=⊥
otherwise, where v ′ is such that xv ′

)

(1)

2.2 Learning objective

The learning task we address is to ﬁnd, in some predeﬁned clas
si ﬁer set C , the stochastic classi ﬁer c
that minimizes the classi ﬁcation error on multi-view examp les (with, potentially, unobserved views)
drawn according to some distribution D as described above. Following the standard multi-view
framework, in which all views are observed [3, 13], we assume that we are given V deterministic
v=1 , each working on one speci ﬁc view 1 . That is, for each view v , Hv is a set
classi ﬁer sets
(Hv )V
of functions hv : Xv → {0, 1}. The ﬁnal set of classi ﬁers
C contains stochastic classi ﬁers, whose
output only depends on the outputs of the view-speci ﬁc class i ﬁers. That is, associated to a set of
v=1 × X → [0, 1] such that:
classi ﬁers C , there is a function ΦC : (Hv )V
C = {x 7→ ΦC (h1 , ..., hV , x) |∀v , hv ∈ Hv }
For simplicity, in the rest of the paper, when the context is c lear, the function x 7→ ΦC (h1 , ..., hV , x)
will be denoted by ch1 ,...,hV . The overall objective of learning is therefore to ﬁnd c ∈ C with low
generalization error, deﬁned as:
(2)

ǫ(c) = E
(x,y)∼D
where e is a pointwise error, for instance the 0/1 loss: e(c, (x, y)) = c(x)(1 − y) + (1 − c(x))y .
In the following sections, we address this learning task in our framework in terms of supervised and
semi-supervised learning.

e (c, (x, y))

3 Supervised Learning Tasks

We ﬁrst focus on the supervised learning case. We assume that we have a training set S of m
examples drawn i.i.d. according to a distribution D , as presented in the previous section. Depending
on how the generated views are used at both training and test s tages, we consider the following
learning scenarios:

e(h, (xv , y))

- Baseline: This setting corresponds to the case where each view-speci ﬁ c classi ﬁer is trained using
the corresponding observed view on the training set, and pre diction for a test example is
done using the view-speci ﬁc classi ﬁer corresponding to the
observed view:
h∈Hv X(x,y)∈S :xv 6=⊥
∀v , hv ∈ arg min
h1 ,...,hV (x) = hv (xv ), where v is the observed view for x. Notice
In this case we pose ∀x, cb
that this is the most basic way of learning a text classi ﬁer fr om a comparable corpus.
- Generated Views as Additional Training Data: The most natural way to use the generated
views for learning is to use them as additional training material for the view-speci ﬁc clas-
si ﬁers:
h∈Hv X(x,y)∈S
∀v , hv ∈ arg min
with x deﬁned by eq. (1). Prediction is still done using the view-sp eci ﬁc classi ﬁers cor-
responding to the observed view, i.e. ∀x, cb
h1 ,...,hV (x) = hv (xv ). Although the test set
distribution is a subdomain of the training set distribution [2], this mismatch is (hopefully)
compensated by the addition of new examples.
- Multi-view Gibbs Classi ﬁer:
In order to avoid the potential bias introduced by the use of gener-
ated views only during training, we consider them also during testing. This becomes a stan-
dard multi-view setting, where generated views are used exactly as if they were observed.
The view-speci ﬁc classi ﬁers are trained exactly as above (e
q. 4), but the prediction is car-
ried out with respect to the probability distribution of cla sses, by estimating the probability
of class membership in class 1 from the mean prediction of each view-speci ﬁc classi ﬁer:
V
Xv=1
1We assume deterministic view-speciﬁc classiﬁers for simplicity and with no los

∀x, cmg
h1 ,...,hV (x) =

e(h, (xv , y))

1
V

hv (xv )

(3)

(4)

(5)

s of generality.

- Multi-view Majority Voting: With view generating functions involved in training and test, a nat-
ural way to obtain a (generally) deterministic classi ﬁer wi
th improved performance is to
take the majority vote associated with the Gibbs classi ﬁer. The view-speci ﬁc classi ﬁers are
again trained as in eq. 4, but the ﬁnal prediction is done usin g a majority vote:
if PV
h1 ,...,hV (x) = ( 1
v=1 hv (xv ) = V
2
2
∀x, cmv
I (cid:16)PV
2 (cid:17)
otherwise
v=1 hv (xv ) > V
Where I (.) is the indicator function. The classi ﬁer outputs either the majority voted class,
or either one of the classes with probability 1/2 in case of a tie.

(6)

4 The trade-offs with the ERM principle

We now analyze how the generated views can improve generalization performance. Essentially,
the trade-off is that generated views offer additional training material, therefore potentially helping
learning, but can also be of lower quality, which may degrade learning.

The following theorem sheds light on this trade-off by providing bounds on the baseline vs. multi-
view strategies. Note that such trade-offs have already been studied in the literature, although in
different settings (see e.g. [2, 4]). Our ﬁrst result is the f ollowing theorem. The notion of func-
tion class capacity used here is the empirical Rademacher complexity [1]. Proof is given in the
supplementary material.

Theorem 1 Let D be a distribution over X × Y , satisfying P(x,y)∼D (|{v : xv 6=⊥}| 6= 1) = 0.
Let S = ((xi , yi ))m
i=1 be a dataset of m examples drawn i.i.d. according to D . Let e be the 0/1
v=1 be the view-speci ﬁc deterministic classi ﬁer sets. For each
view v , denote
loss, and let (Hv )V
def= {(xv , y) 7→ e(h, (xv , y))|h ∈ Hv }, and denote , for any sequence S v ∈ (Xv × Y )mv of
e ◦ Hv
size mv , ˆRmv (e ◦ Hv , S v ) the empirical Rademacher complexity of e ◦ Hv on S v . Then, we have:
Baseline setting: for all 1 > δ > 0, with probability at least 1 − δ over S :
V
ˆRmv (e ◦ Hv , S v ) + 6r ln(2/δ)
mv
)i + 2
v ∈Hv hǫ(cb
Xv=1
ǫ(cb
h1 ,...,hV ) ≤ inf
1 ,...,h′
h′
m
2m
h′
V
where, for all v , S v def= {(xv
i 6=⊥}, mv = |S v | and hv ∈ Hv is the
i , yi )|i = 1..m and xv
classi ﬁer minimizing the empirical risk on S v .
Multi-view Gibbs classi ﬁcation setting:
for all 1 > δ > 0, with probability at least 1 − δ over S :
V
ˆRm (e ◦ Hv , S v ) + 6r ln(2/δ)
2
)i +
v ∈Hv hǫ(cb
Xv=1
ǫ(cmg
h1 ,...,hV ) ≤ inf
1 ,...,h′
h′
V
2m
h′
V
where, for all v , S v def= {(xv
i , yi )|i = 1..m}, hv ∈ Hv is the classi ﬁer minimizing the
empirical risk on S v , and
v ∈Hv hǫ(cmg
)i − inf
v ∈Hv hǫ(cb
)i
η = inf
h′
1 ,...,h′
h′
1 ,...,h′
h′
h′
V
V
This theorem gives us a rule for whether it is preferable to le arn only with the observed views
(the baseline setting) or preferable to use the view-genera ting functions in the multi-view Gibbs
ˆRmv (e ◦ Hv , S v ) < 2
ˆRm (e ◦
mv
classi ﬁcation setting: we should use the former when 2 Pv
V Pv
m
Hv , S v ) + η , and the latter otherwise.
Let us ﬁrst explain the role of η (Eq. 7). The difference between the two settings is in the train
and test distributions for the view-speci ﬁc classi ﬁers.
η compares the best achievable error for each
v ∈Hv hǫ(cb
)i is the best achievable error in the baseline setting (i.e.
of the distribution. inf h′
1 ,...,h′
h′
V
without generated views), with the automatically generated views, the best achievable error becomes
v ∈Hv hǫ(cmg
)i.
inf h′
h′
1 ,...,h′
V

+ η

(7)

In a favorable

Therefore η measures the loss incurred by using the view generating func tions.
situation, the quality of the generating functions will be sufﬁcient to make η small.
The terms depending on the complexity of the class of functions may be better explained using
orders of magnitude. Typically, the Rademacher complexity for a sample of size n is usually of
order O( 1√n ) [1].
Assuming, for simplicity, that all empirical Rademacher complexities in Theorem 1 are approxi-
mately equal to d/√n, where n is the size of the sample on which they are computed, and assuming
that mv = m/V for all v . The trade-off becomes:
Choose the Multi-view Gibbs classi ﬁcation setting when: d (cid:16)q V
m − 1√m (cid:17) > η
This means that we expect important performance gains when the number of examples is small, the
generated views of sufﬁciently high quality for the given cl assi ﬁcation task, and/or there are many
views available. Note that our theoretical framework does not take the quality of the MT system in a
standard way: in our setup, a good translation system is (roughly) one which generates bag-of-words
representations that allow to correctly discriminate betw een classes.

Majority voting One advantage of the multi-view setting at prediction time is that we can use a
majority voting scheme, as described in Section 2. In such a case, we expect that ǫ(cmv
) ≤
1 ,...,h′
h′
V
ǫ(cmg
rrors. It can not be guaranteed
) if the view-speci ﬁc classi ﬁers are not correlated in their e
1 ,...,h′
h′
V
) ≤ 2ǫ(cmg
in general, though, since, in general, we can not prove any be tter than ǫ(cmv
)
1 ,...,h′
h′
1 ,...,h′
h′
V
V
(see e.g. [9]).

5 Agreement-Based Semi-Supervised Learning

One advantage of the multi-view settings described in the previous section is that unlabeled training
examples may naturally be taken into account in a semi –super vised learning scheme, using existing
approaches for multi-view learning (e.g. [3]).

In this section, we describe how, under the framework of [11] , the supervised learning trade-off
presented above can be improved using extra unlabeled examp les. This framework is based on
the notion of disagreement between the various view-speci ﬁc classi ﬁers, deﬁned as the
expected
variance of their outputs:

def=

(8)

V (h1 , ..., hV )

hv (xv )!2
(x,y)∼D 
hv (xv )2 −   1
1
V Xv
V Xv
E


The overall idea is that a set of good view-speci ﬁc classi ﬁer
s should agree on their predictions,
making the expected variance small. This notion of disagreement has two key advantages. First, it
does not depend on the true class labels, making its estimation easy over a large, unlabeled training
set. The second advantage is that if, during training, it turns out that the view-speci ﬁc classi ﬁers
have a disagreement of at most µ on the unlabeled set, the set of possible view-speci ﬁc class i ﬁers
that needs be considered in the supervised learning stage is reduced to:
H∗v (µ) def= {h′v ∈ Hv |∀v ′ 6= v , ∃h′v ′ ∈ Hv ′ , V(h′1 , ..., h′V ) ≤ µ }
Thus, the more the various view-speci ﬁc classi ﬁers tend to a
gree, the smaller the possible set of
functions will be. This suggests a simple way to do semi-supervised learning: the unlabeled data
can be used to choose, among the classi ﬁers minimizing the em pirical risk on the labeled training
set, those with best generalization performance (by choosing the classi ﬁers with highest agreement
on the unlabeled set). This is particularly interesting when the number of labeled examples is small,
as the train error is usually close to 0.
Theorem 3 of [11] provides a theoretical value B (ǫ, δ) for the minimum number of unlabeled ex-
amples required to estimate Eq. 8 with precision ǫ and probability 1 − δ (this bound depends on
{Hv }v=1..V ). The following result gives a tighter bound of the generalization error of the multi-view
Gibbs classi ﬁer when unlabeled data are available. The proo f is similar to Theorem 4 in [11].

Proposition 2 Let 0 ≤ µ ≤ 1 and 0 < δ < 1. Under the conditions and notations of Theorem
1, assume furthermore that we have access to u ≥ B (µ/2, δ/2) unlabeled examples drawn i.i.d.
according to the marginal distribution of D on X .
risk minimizers hv
empirical
the
if
Then, with probability at
least 1 − δ ,
∈
arg minh∈Hv P(xv ,y)∈S v e(h, (xv , y)) have a disagreement
less than µ/2 on the unlabeled
set, we have:
V
ˆRm (e ◦ H∗v (µ), S v ) + 6r ln(4/δ)
2
)i +
v ∈Hv hǫ(cb
Xv=1
ǫ(cmg
h1 ,...,hV ) ≤ inf
1 ,...,h′
h′
V
2m
h′
V
We can now rewrite the trade-off between the baseline setting and the multi-view Gibbs classi ﬁer,
taking semi-supervised learning into account. Using orders of magnitude, and assuming that for
each view, ˆRm (e ◦ H∗v (µ), S v ) is O(du /√m), with the proportional factor du ≪ d, the trade-off
becomes:
Choose the mutli-view Gibbs classi ﬁcation setting when: dpV /m − du /√m > η .
Thus, the improvement is even more important than in the supervised setting. Also note that the
more views we have, the greater the reduction in classi ﬁer se t complexity should be.
Notice that this semi-supervised learning principle enforces agreement between the view speci ﬁc
classi ﬁers. In the extreme case where they almost always giv e the same output, majority voting is
then nearly equivalent to the Gibbs classi ﬁer (when all vote rs agree, any vote is equal to the majority
vote). We therefore expect the majority vote and the Gibbs classi ﬁer to yield similar performance in
the semi-supervised setting.

+ η

6 Experimental Results

In our experiments, we address the problem of learning docum ent classi ﬁers from a comparable
corpus. We build the comparable corpus by sampling parts of the Reuters RCV1 and RCV2 collec-
tions [12, 14]. We used newswire articles written in 5 languages, English, French, German,
Italian and Spanish. We focused on 6 relatively populous classes: C15, CCAT, E21, ECAT,
GCAT, M11.

For each language and each class, we sampled up to 5000 documents from the RCV1 (for English)
or RCV2 (for other languages). Documents belonging to more than one of our 6 classes were as-
signed the label of their smallest class. This resulted in 12-30K documents per language, and 11-34K
documents per class (see Table 1). In addition, we reserved a test split containing 20% of the doc-
uments (respecting class and language proportions) for tes ting. For each document, we indexed
the text appearing in the title (headline tag), and the body (body tags) of each article. As prepro-
cessing, we lowercased, mapped digits to a single digit token, and removed non alphanumeric
tokens. We also ﬁltered out function words using a stop-list
, as well as tokens occurring in less than
5 documents.
Documents were then represented as a bag of words, using a TFIDF-based weighting scheme. The
ﬁnal vocabulary size for each language is given in table 1. Th e arti ﬁcial views were produced using

Table 1: Distribution of documents over languages and classes in the comparable corpus.
Size (all lang.)
Class
(%)
# tokens
(%)
# docs
Language
C15
English
16.84
18, 816
21, 531
16.78
18, 758
CCAT
French
24, 893
19.17
21, 426
23.45
26, 648
German
E21
34, 279
29, 953
26.80
13, 701
12.26
Italian
ECAT
21.51
24, 039
15, 506
17.18
19, 198
Spanish
GCAT
12, 342
11.46
11, 547
17.16
19, 178
Total
M11
111, 740
19, 421
17.39

PORTAGE, a statistical machine translation system developed at NRC [15]. Each document from
the comparable corpus was thus translated to the other 4 languages.2
For each class, we set up a binary classi ﬁcation task by using all documents from that class as
positive examples, and all others as negative. We ﬁrst prese nt experimental results obtained in
supervised learning, using various amounts of labeled examples. We rely on linear SVM models as
base classi ﬁers, using the SVM-Perf package [8]. For comparisons, we employed the four learning
strategies described in section 3: 1− the single-view baseline svb (Eq. 3), 2− generated views as
additional training data gvb (Eq. 4), 3− multi-view Gibbs mvg (Eq. 5), and 4− multi-view majority
voting mvm (Eq. 6). Recall that the second setting, gvb , is the most straightforward way to train and
test classi ﬁers when additional examples are available (or generated) from different sources. It can
thus be seen as a baseline approach, as opposed to the last two strategies (mvg and mvm ), where
view-speci ﬁc classi ﬁers are both trained and tested over bo
th original and translated documents.
Note also that in our case (V = 5 views), additional training examples obtained from machine
translation represent 4 times as many labeled examples as the original texts used to train the baseline
svb . All test results were averaged over 10 randomly sampled training sets.

Table 2: Test classi ﬁcation accuracy and F1 in the supervised setting, for both baselines (svb , gvb ),
Gibbs (mvg ) and majority voting (mvw ) strategies, averaged over 10 random sets of 10 labeled
examples per view. ↓ indicates statistically signi ﬁcantly worse performance t hat the best result,
according to a Wilcoxon rank sum test (p < 0.01) [10].
E21
CCAT
C15
Strategy
Acc.
Acc.
Acc.
F1
F1
F1
.388↓ .639↓ .403↓ .557↓ .294↓
.559↓
.474↓ .691↓ .464↓ .665↓ .351↓
.705
.494↓ .681↓ .445↓ .665↓ .375↓
.693↓
.716
.521
.708
.478
.693
.405

GCAT
Acc.
F1
.800↓ .501↓
.835↓ .595↓
.834↓ .594↓
.860
.642

svb
gvb
mvg
mvm

ECAT
Acc.
F1
.579↓ .374↓
.623↓ .424↓
.620↓ .420↓
.636
.441

M11
Acc.
F1
.651↓ .483↓
.786↓ .589↓
.787↓ .600↓
.820
.644

Results obtained in a supervised setting with only 10 labeled documents per language for training are
summarized in table 2. All learning strategies using the generated views during training outperform
the single-view baseline. This shows that, although imperfect, arti ﬁcial views do bring additional
information that compensates the lack of labeled data. Although the multi-view Gibbs classi ﬁer
predicts based on a translation rather than the original in 80% of cases, it produces almost identical
performance to the gvb run (which only predicts using the original text). These results indicate that
the translation produced by our MT system is of sufﬁcient qua lity for indexing and classi ﬁcation
purposes. Multi-view majority voting reaches the best performance, yielding a 6 − 17% improve-
ment in accuracy over the baseline. A similar increase in per formance is observed using F1 , which
suggests that the multi-view SVM appropriately handles unbalanced classes.

Figure 1 shows the learning curves obtained on 3 classes, C15, ECAT and M11. These ﬁgures show
that when there are enough labeled examples (around 500 for these 3 classes), the arti ﬁcial views do
not provide any additional useful information over the orig inal-language examples. These empirical
results illustrate the trade-off discussed at the previous section. When there are sufﬁcient original
labeled examples, additional generated views do not provide more useful information for learning
than what view-speci ﬁc classi ﬁers have available already.

We now investigate the use of unlabeled training examples for learning the view-speci ﬁc classi ﬁers.
Our overall aim is to illustrate our ﬁndings from section 5. R ecall that in the case where view-speci ﬁc
classi ﬁers are in agreement over the class labels of a large n umber of unlabeled examples, the multi-
view Gibbs and majority vote strategies should have the same performance. In order to enforce
agreement between classi ﬁers on the unlabeled set, we use a v ariant of the iterative co-training
algorithm [3]. Given the view-speci ﬁc classi ﬁers trained o
n an initial set of labeled examples, we
iteratively assign pseudo-labels to the unlabeled examples for which all classi ﬁer predictions agree.
We then train new view-speci ﬁc classi ﬁers on the joint set of
the original labeled examples, and those
unanimously (pseudo-)labeled ones. Key differences between this algorithm and co-training are the
number of views used for learning (5 instead of 2), and the use of unanimous and simultaneous
labeling.

2The dataset is available from http://multilingreuters.iit.nrc.ca/ReutersMultiLingualMultiView.htm

C15

ECAT

M11

1
F

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

 
 
 

10

20

50
200
100
Labeled training size

mvm
mvg
svb

500

1
F

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

 
 
 

10

20

50
200
100
Labeled training size

mvm
mvg
svb

500

1
F

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

 0.45

 0.4

 0.35

 
 
 

10

20

50
200
100
Labeled training size

mvm
mvg
svb

500

Figure 1: F1 vs. size of the labeled training set for classes C15, ECAT and M11.

We call this iterative process self-learning multiple-view algorithm, as it also bears a similarity with
the self-training paradigm [16]. Prediction from the multi -view SVM models obtained from this
m ).
g ) or majority voting (mvs
self-learning multiple-view algorithm is done either using Gibbs (mvs
These results are shown in table 3. For comparison we also tra ined a TSVM model [7] on each view
separately, a semi-supervised equivalent to the single-view baseline strategy. Note that the TSVM
model mostly out-performs the supervised baseline svb , although the F1 suffers on some classes.
This suggests that the TSVM has trouble handling unbalanced classes in this setting.

Table 3: Test classi ﬁcation accuracy and F1 in the semi-supervised setting, for single-view TSVM
m ), averaged over 10
g ) or majority voting (mvs
and multi-view self-learning using either Gibbs (mvs
random sets using 10 labeled examples per view to start. For comparison we provid e the single-view
baseline and multi-view majority voting performance for supervised learning.

Strategy

svb
mvm
TSVM
mvs
g
mvs
m

C15
Acc.
.559↓
.716↓
.721↓
.772
.773

F1
.388↓
.521↓
.482↓
.586
.589

CCAT
Acc.
F1
.403↓
.639↓
.478↓
.708↓
.721↓
.405↓
.538
.762
.766
.545

E21
Acc.
F1
.294↓
.557↓
.405↓
.693↓
.746↓
.269↓
.470
.765
.767
.473

ECAT
Acc.
F1
.579↓ .374↓
.636↓ .441↓
.665↓ .263↓
.691 .504
.701
.508

GCAT
Acc.
F1
.800↓ .501↓
.860↓ .642↓
.876↓ .606↓
.903 .729
.905
.734

M11
Acc.
F1
.651↓ .483↓
.820↓ .644↓
.834↓ .706↓
.900 .764
.901
.766

The multi-view self-learning algorithm achieves the best classi ﬁcation performance in both accuracy
and F1 , and signi ﬁcantly outperforms both the TSVM and the supervised multi-view strategy in all
classes. As expected, the performance of both mvs
g and mvs
m strategies are similar.

7 Conclusion

The contributions of this paper are twofold. First, we propo sed a bound on the risk of the Gibbs
classi ﬁer trained over arti ﬁcially completed multi-view o
bservations, which directly corresponds to
our target application of learning text classi ﬁers from a co mparable corpus. We showed that our
bound may lead to a trade-off between the size of the training set, the number of views, and the
quality of the view generating functions. Our result identi ﬁes in which case it is advantageous to
learn with additional arti ﬁcial views, as opposed to sticki ng with the baseline setting in which a clas-
si ﬁer is trained over single view observations. This result
leads to our second contribution, which is
a natural way of using unlabeled data in semi-supervised multi-view learning. We showed that in the
case where view-speci ﬁc classi ﬁers agree over the class lab
els of additional unlabeled training data,
the previous trade-off becomes even much tighter. Empirica l results on a comparable multilingual
corpus support our ﬁndings by showing that additional views obtained using a Machine Translation
system may signi ﬁcantly increase classi ﬁcation performan
ce in the most interesting situation, when
there are few labeled data available for training.

Acknowlegdements This work was supported in part by the IST Program of the Europ ean Com-
munity, under the PASCAL2 Network of Excellence, IST-2002-506778.

References

[1] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: risk bounds and
structural results. Journal of Machine Learning Research, 3:463–482, 2003.
[2] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman. Learning bounds for domain
adaptation. In NIPS, 2007.
[3] A. Blum and T. M. Mitchell. Combining labeled and unlabeled sata with co-training. In COLT,
pages 92–100, 1998.
[4] K. Crammer, M. Kearns, and J. Wortman. Learning from multiple sources. Journal of Machine
Learning Research, 9:1757–1774, 2008.
[5] J. D. R. Farquhar, D. Hardoon, H. Meng, J. Shawe-Taylor, and S. Szedmak. Two view learning:
Svm-2k, theory and practice. In Advances in Neural Information Processing Systems 18, pages
355–362. 2006.
[6] D. R. Hardoon, G. Leen, S. Kaski, and J. S.-T. (eds). Nips workshop on learning from multiple
sources. 2008.
[7] T. Joachims. Transductive inference for text classi ﬁca tion using support vector machines. In
ICML, pages 200–209, 1999.
[8] T. Joachims. Training linear svms in linear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), pages 217–226, 2006.
[9] J. Langford and J. Shawe-taylor. Pac-bayes & margins. In NIPS 15, pages 439–446, 2002.
[10] E. Lehmann. Nonparametric Statistical Methods Based on Ranks. McGraw-Hill, New York,
1975.
[11] B. Leskes. The value of agreement, a new boosting algorithm. In COLT, pages 95–110, 2005.
[12] D. D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmark collection for text catego-
rization research. Journal of Machine Learning Research, 5:361–397, 2004.
[13] I. Muslea. Active learning with multiple views. PhD thesis, USC, 2002.
[14] Reuters. Corpus, volume 2, multilingual corpus, 1996-08-20 to 1997-08-19. 2005.
[15] N. Uefﬁng, M. Simard, S. Larkin, and J. H. Johnson. NRC’s PORTAGE system for WMT. In
In ACL-2007 Second Workshop on SMT, pages 185–188, 2007.
[16] X. Zhu. Semi-supervised learning literature survey. Technical report, Univ. Wisconsis, 2007.


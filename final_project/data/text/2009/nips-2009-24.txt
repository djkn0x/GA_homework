Learning to Explore and Exploit in POMDPs

Chenghui Cai, Xuejun Liao, and Lawrence Carin
Department of Electrical and Computer Engineering
Duke University
Durham, NC 27708-0291, USA

Abstract

A fundamental objective in reinforcement learning is the maintenance of a proper
balance between exploration and exploitation. This problem becomes more chal-
lenging when the agent can only partially observe the states of its environment.
In this paper we propose a dual-policy method for jointly learning the agent be-
havior and the balance between exploration exploitation, in partially observable
environments. The method subsumes traditional exploration, in which the agent
takes actions to gather information about the environment, and active learning, in
which the agent queries an oracle for optimal actions (with an associated cost for
employing the oracle). The form of the employed exploration is dictated by the
speciﬁc problem. Theoretical guarantees are provided conc erning the optimality
of the balancing of exploration and exploitation. The effectiveness of the method
is demonstrated by experimental results on benchmark problems.

1 Introduction

A fundamental challenge facing reinforcement learning (RL) algorithms is to maintain a proper
balance between exploration and exploitation. The policy designed based on previous experiences
is by construction constrained, and may not be optimal as a result of inexperience. Therefore, it
is desirable to take actions with the goal of enhancing experience. Although these actions may not
necessarily yield optimal near-term reward toward the ultimate goal, they could, over a long horizon,
yield improved long-term reward. The fundamental challenge is to achieve an optimal balance
between exploration and exploitation; the former is performed with the goal of enhancing experience
and preventing premature convergence to suboptimal behavior, and the latter is performed with the
goal of employing available experience to de ﬁne perceived o ptimal actions.

For a Markov decision process (MDP), the problem of balancing exploration and exploitation has
been addressed successfully by the E 3 [4, 5] and R-max [2] algorithms. Many important applica-
tions, however, have environments whose states are not completely observed, leading to partially
observable MDPs (POMDPs). Reinforcement learning in POMDPs is challenging, particularly in
the context of balancing exploration and exploitation. Recent work targeted on solving the explo-
ration vs. exploitation problem is based on an augmented POMDP, with a product state space over
the environment states and the unknown POMDP parameters [9]. This, however, entails solving a
complicated planning problem, which has a state space that grows exponentially with the number
of unknown parameters, making the problem quickly intractable in practice. To mitigate this com-
plexity, active learning methods have been proposed for POMDPs, which borrow similar ideas from
supervised learning, and apply them to selectively query an oracle (domain expert) for the optimal
action [3]. Active learning has found success in many collaborative human-machine tasks where
expert advice is available.

In this paper we propose a dual-policy approach to balance exploration and exploitation in POMDPs,
by simultaneously learning two policies with partially shared internal structure. The ﬁrst policy,
termed the primary policy, de ﬁnes actions based on previous experience; the second po licy, termed

1

the auxiliary policy, is a meta-level policy maintaining a proper balance between exploration and
exploitation. We employ the regionalized policy representation (RPR) [6] to parameterize both
policies, and perform Bayesian learning to update the policy posteriors. The approach applies in
either of two cases: (i) the agent explores by randomly taking the actions that have been insufﬁciently
tried before (traditional exploration), or (ii) the agent explores by querying an oracle for the optimal
action (active learning). In the latter case, the agent is assessed a query cost from the oracle, in
addition to the reward received from the environment. Either (i) or (ii) is employed as an exploration
vehicle, depending upon the application.

The dual-policy approach possesses interesting convergence properties, similar to those of E3 [5]
and Rmax [2]. However, our approach assumes the environment is a POMDP while E3 and Rmax
both assume an MDP environment. Another distinction is that our approach learns the agent policy
directly from episodes, without estimating the POMDP model. This is in contrast to E3 and Rmax
(both learn MDP models) and the active-learning method in [3] (which learns POMDP models).

2 Regionalized Policy Representation

We ﬁrst provide a brief review of the regionalized policy rep resentation, which is used to parame-
terize the primary policy and the auxiliary policy as discussed above. The material in this section is
taken from [6], with the proofs omitted here.

De ﬁnition 2.1 A regionalized policy representation is a tuple (A, O, Z , W, µ, π). The A and O are
respectively a ﬁnite set of actions and observations. The Z is a ﬁnite set of belief regions. The W is
the belief-region transition function with W (z , a, o′ , z ′) denoting the probability of transiting from
z to z ′ when taking action a in z results in observing o′ . The µ is the initial distribution of belief
regions with µ(z ) denoting the probability of initially being in z . The π are the region-dependent
stochastic policies with π(z , a) denoting the probability of taking action a in z .

2.1 Learning Criterion

We denote A = {1, 2, . . . , |A|}, where |A| is the cardinality of A. Similarly, O = {1, 2, . . . , |O|}
and Z = {1, 2, . . . , |Z |}. We abbreviate (a0 , a1 , . . . , aT ) as a0:T and similarly, (o1 , o2 , . . . , aT )
as o1:T and (z0 , z1 , . . . , zT ) as z0:T , where the subscripts indexes discrete time steps. The history
ht = {a0:t−1 , o1:t } is de ﬁned as a sequence of actions performed and observation s received up to
t. Let Θ = {π , µ, W } denote the RPR parameters. Given ht , the RPR yields a joint probability
distribution of z0:t and a0:t as follows
p(a0:t , z0:t |o1:t , Θ) = µ(z0 )π(z0 , a0 )Qt
(1)
τ =1W (zτ −1 , aτ −1 , oτ , zτ )π(zτ , aτ )
By marginalizing z0:t out in (1), we obtain p(a0:t |o1:t , Θ). Furthermore, the history-dependent
distribution of action choices is obtained as follows:
p(aτ |hτ , Θ) = p(a0:τ |o1:τ , Θ)[p(a0:τ −1 |o1:τ −1 , Θ)]−1
which gives a stochastic policy for choosing the action aτ . The action choice depends solely on the
historical actions and observations, with the unobservable belief regions marginalized out.
Bayesian learning of the RPR is based on the experiences collected from the agent-environment
interaction. Assuming the interaction is episodic, i.e., it breaks into subsequences called episodes
[10], we represent the experiences by a set of episodes.
	
	
De ﬁnition 2.2 An episode is a sequence of agent-environment
interactions terminated in
An episode is denoted by
an absorbing state that
transits to itself with zero reward.
), where the subscripts are discrete times, k indexes the episodes, and o,
(ak
1 · · · ok
1 rk
1 ak
0 ok
0 rk
ak
rk
Tk
Tk
Tk
a, and r are respectively observations, actions, and immediate rewards.
De ﬁnition 2.3
(The RPR Optimality Criterion) Let D(K ) = {(ak
)}K
rk
ak
1 · · · ok
1 rk
1 ak
0 ok
0 rk
k=1
Tk
Tk
Tk
be a set of episodes obtained by an agent interacting with the environment by following policy
Π to select actions, where Π is an arbitrary stochastic policy with action-selecting distributions
pΠ (at |ht ) > 0, ∀ action at , ∀ history ht . The RPR optimality criterion is de ﬁned as
k=1 PTk
K PK
τ |hk
τ =0 p(ak
t
def .
τ ,Θ)
bV (D(K ) ; Θ)
= 1
t=0 γ t rk
t
t
τ =0 pΠ (ak
τ |hk
τ )
2

(2)

where hk
t is the history of actions and observations up to time t in the k-th episode,
1 · · · ok
1 ak
0 ok
t = ak
0 < γ < 1 is the discount, and Θ denotes the RPR parameters.
Throughout the paper, we call bV (D(K ) ; Θ) the empirical value function of Θ. It is proven in [6]
that limK→∞ bV (D(K ) ; Θ) is the expected sum of discounted rewards by following the RPR policy
parameterized by Θ for an in ﬁnite number of steps. Therefore, the RPR resulting from maximization
of bV (D(K ) ; Θ) approaches the optimal as K is large (assuming |Z | is appropriate). In the Bayesian
setting discussed below, we use a noninformative prior for Θ, leading to a posterior of Θ peaked at
the optimal RPR, therefore the agent is guaranteed to sample the optimal or a near-optimal policy
with overwhelming probability.

2.2 Bayesian Learning

Let G0 (Θ) represent the prior distribution of the RPR parameters. We de ﬁne the posterior of Θ as
def .
= bV (D(K ) ; Θ)G0 (Θ)[ bV (D(K ) )]−1
p(Θ|D(K ) , G0 )
(3)
where bV (D(K ) ) = R bV (D(K ) ; Θ)G0 (Θ)dΘ is the marginal empirical value. Note that bV (D(K ) ; Θ)
is an empirical value function, thus (3) is a non-standard use of Bayes rule. However, (3) indeed
gives a distribution whose shape incorporates both the prior and the empirical information.
Since each term in bV (D(K ) ; Θ) is a product of multinomial distributions, it is natural to choose the
prior as a product of Dirichlet distributions,
(4)
G0 (Θ) = p(µ|υ)p(π |ρ)p(W |ω )
i=1Dir(cid:0)π(i, 1), · · · , π(i, |A|)(cid:12)(cid:12)(cid:12)ρi(cid:1),
where p(µ|υ) = Dir(cid:0)µ(1), · · · , µ(|Z |)(cid:12)(cid:12)υ(cid:1), p(π |ρ) = Q|Z |
i=1Dir(cid:0)W (i, a, o, 1), · · · , W (i, a, o, |Z |)(cid:12)(cid:12)ωi,a,o(cid:1); ρi = {ρi,m }|A|
p(W |ω ) = Q|A|
a=1 Q|O |
o=1 Q|Z |
m=1 ,
	
b
υ = {υi}|Z |
i=1 , and ωi,a,o = {ωi,a,o,j }|Z |
j=1 are hyper-parameters. With the prior thus chosen, the
posterior in (3) is a large mixture of Dirichlet products, and therefore posterior analysis by Gibbs
sampling is inefﬁcient. To overcome this, we employ the vari ational Bayesian technique [1] to obtain
a variational posterior by maximizing a lower bound to ln R bV (D(K ) ; Θ)G0 (Θ)dΘ,
t }, g (Θ)) = lnZ bV (D(K ); Θ)G0 (Θ)dΘ − KL({qk
LB({qk
t (z k
0:t )g (Θ)}||{ν k
t p(z k
0:t , Θ|ak
0:t , ok
1:t )})
0:t ) ≥ 1, g (Θ) ≥ 1, R g (Θ)dΘ = 1,
t }, g (Θ) are variational distributions satisfying qk
where {qk
t (z k
t=1 P|Z |
K PK
k=1 PTk
γ t rk
t p(ak
0:t |ok
1:t )
and 1
0:t ) = 1; ν k
and KL(qkp) denotes
qk
t (z k
t =
0 ,··· ,zk
zk
t
t =1
V (D (K ) )
τ =0 pΠ (ak
τ |hk
τ )
the Kullback-Leibler (KL) distance between probability measure q and p.
The factorized form {qt (z0:t )g (Θ)} represents an approximation of the weighted joint posterior of
Θ and z ’s when the lower bound reaches the maximum, and the corresponding g (Θ) is called the
variational approximate posterior of Θ. The lower bound maximization is accomplished by solving



bυi ) , and fW (i, a, o, j ) = eψ(bωi,a,o,j )−ψ(
bωi,a,o,j ) ,
eψ( bρi,m )−ψ(
bρi,m ) , eµ(i) = eψ(bυi )−ψ(
{qt (z0:t )} and g (Θ) alternately, keeping one ﬁxed while solving for the other. T he solutions are
summarized in Theorem 2.4; the proof is in [6].
Theorem 2.4 Given the initialization bρ = ρ, bυ = υ , bω = ω , iterative application of the following
updates produces a sequence of monotonically increasing lower bounds LB({qk
t }, g (Θ)), which
converges to a maxima. The update of {qk
t } is
1:t , eΘ)
0:t , ok
0:t |ak
t p(z k
0:t ) = σk
z (z k
qk
where eΘ = {eπ , eµ, fW } is a set of under-normalized probability mass functions, with eπ (i, m) =
|A|
|A|
|Z |
j=1
m=1
i=1
and ψ is the digamma function. The g (Θ) has the same form as the prior G0 in (4), except that the
hyper-parameter are updated as
k=1 PTk
bυi = υi + PK
t φk
t=0σk
t,0 (i)
3

t=0 Pt
k=1 PTk
bρi,a = ρi,a + PK
t,τ (i)δ(ak
t φk
τ =0σk
τ , a)
k=1PTk
t=0Pt
bωi,a,o,j = ωi,a,o,j +PK
τ −1 , a)δ(ok
t,τ −1 (i, j )δ(ak
τ =1σk
t ξ k
τ , o)
1:t , eΘ), φk
1:t , eΘ), and
where ξ k
t,τ (i, j ) = p(z k
τ = i, z k
τ +1 = j |ak
0:t , ok
t,τ (i) = p(z k
τ = i|ak
0:t , ok
t = (cid:2)γ t rk
1:t , eΘ)(cid:3)(cid:2)Qt
τ ) bV (D(K ) | eΘ)(cid:3)−1
t p(ak
σk
0:t |ok
τ =0 pΠ (ak
τ |hk
3 Dual-RPR: Joint Policy for the Agent Behavior and the Trade-Off
Between Exploration and Exploitation

(5)

Assume that the agent uses the RPR described in Section 2 to govern its behavior in the unknown
POMDP environment (the primary policy). Bayesian learning employs the empirical value function
bV (D(K ) ; Θ) in (2) in place of a likelihood function, to obtain the posterior of the RPR parameters
Θ. The episodes D(K ) may be obtained from the environment by following an arbitrary stochastic
policy Π with pΠ (a|h) > 0, ∀ a, ∀ h. Although any such Π guarantees optimality of the resulting
RPR, the choice of Π affects the convergence speed. A good choice of Π avoids episodes that do
not bring new information to improve the RPR, and thus the agent does not have to see all possible
episodes before the RPR becomes optimal.

In batch learning, all episodes are collected before the learning begins, and thus Π is pre-chosen
and does not change during the learning [6]. In online learning, however, the episodes are collected
during the learning, and the RPR is updated upon completion of each episode. Therefore there is
a chance to exploit the RPR to avoid repeated learning in the same part of the environment. The
agent should recognize belief regions it is familiar with, and exploit the existing RPR policy there;
in belief regions inferred as new, the agent should explore. This balance between exploration and
exploitation is performed with the goal of accumulating a large long-run reward.

We consider online learning of the RPR (as the primary policy) and choose Π as a mixture of two
policies: one is the current RPR Θ (exploitation) and the other is an exploration policy Πe . This gives
the action-choosing probability pΠ (a|h) = p(y = 0|h)p(a|h, Θ, y = 0)+ p(y = 1|h)p(a|h, Πe , y =
1), where y = 0 (y = 1) indicates exploitation (exploration). The problem of choosing good Π then
reduces to a proper balance between exploitation and exploration: the agent should exploit Θ when
doing so is highly rewarding, while following Πe to enhance experience and improve Θ.
An auxiliary RPR is employed to represent the policy for balancing exploration and exploitation,
i.e., the history-dependent distribution p(y |h). The auxiliary RPR shares the parameters {µ, W }
with the primary RPR, but with π = {π(z , a) : a ∈ A, z ∈ Z } replaced by λ = {λ(z , y ) : y =
0 or 1, z ∈ Z }, where λ(z , y ) is the probability of choosing exploitation (y = 0) or exploration
(y = 1) in belief region z . Let λ have the prior
i=1Beta(cid:16)λ(i, 0), λ(i, 1)(cid:12)(cid:12)(cid:12)u0 , u1(cid:17).
p(λ|u) = Q|Z |
(6)
In order to encourage exploration when the agent has little experience, we choose u0 = 1 and u1 > 1
so that, at the beginning of learning, the auxiliary RPR always suggests exploration. As the agent
accumulates episodes of experience, it comes to know a certain part of the environment in which the
episodes have been collected. This knowledge is re ﬂected in the auxiliary RPR, which, along with
the primary RPR, is updated upon completion of each new episode.

Since the environment is a POMDP, the agent’s knowledge should be represented in the space of
belief states. However, the agent cannot directly access the belief states, because computation of
belief states requires knowing the true POMDP model, which is not available. Fortunately, the RPR
formulation provides a compact representation of H = {h}, the space of histories, where each
history h corresponds to a belief state in the POMDP. Within the RPR formulation, H is represented
internally as the set of distributions over belief regions z ∈ Z , which allows the agent to access
H based on a subset of samples from H. Let Hknown be the part of H that has become known to
the agent, i.e., the primary RPR is optimal in Hknown and thus the agent should begin to exploit
upon entering Hknown . As will be clear below, Hknown can be identiﬁed by Hknown = {h : p(y =
0|h, Θ, λ) ≈ 1}, if the posterior of λ is updated by
t=0 Pt
k=1 PTk
bui,0 = u0 + PK
t φk
τ =0σk
t,τ (i),
bui,1 = max (cid:0)η , u1 − PK
t,τ (i)(cid:1),
k=1 PTk
t=0 Pt
τ =0yk
t γ t c φk
4

(8)

(7)

t is the same in (5) except that rk
where η is a small positive number, and σk
t is replaced by mk
t , the
meta-reward received at t in episode k . We have mk
t = rmeta if the goal is reached at time t in
episode k , and mk
t = 0 otherwise, where rmeta > 0 is a constant. When Πe is provided by an oracle
(active learning), a query cost c > 0 is taken into account in (8), by subtracting c from u1 . Thus, the
probability of exploration is reduced each time the agent makes a query to the oracle (i.e., yk
t = 1).
After a certain number of queries, bui,1 becomes the small positive number η (it never becomes zero
due to the max operator), at which point the agent stops querying in belief region z = i.
In (7) and (8), exploitation always receives a “credit”, whi
le exploration never receives credit (ex-
ploration is actually discredited when Πe is an oracle). This update makes sure that the chance
of exploitation monotonically increases as the episodes accumulate. Exploration receives no credit
because it has been pre-assigned a credit (u1 ) in the prior, and the chance of exploration should
monotonically decrease with the accumulation of episodes. The parameter u1 represents the agent’s
prior for the amount of needed exploration. When c > 0, u1 is discredited by the cost and the agent
needs a larger u1 (than when c = 0) to obtain the same amount of exploration. The fact that the
amount of exploration monotonically increases with u1 implies that, one can always ﬁnd a large
enough u1 to ensure that the primary RPR is optimal in Hknown = {h : p(y = 0|h, Θ, λ) ≈ 1}.
However, an unnecessarily large u1 makes the agent over-explore and leads to slow convergence.
denote the minimum u1 that ensures optimality in Hknown . We assume umin
Let umin
exists in the
1
1
is examined in the experiments.
analysis below. The possible range of umin
1

4 Optimality and Convergence Analysis

(9)

Let M be the true POMDP model. We ﬁrst introduce an equivalent expr ession for the empirical
value function in (2),
T PT
; Θ) = PE (K )
bV (E (K )
t=0γ t rt p(a0:t , o1:t , rt |y0:t = 0, Θ, M ),
T
where the ﬁrst summation is over all elements in E (K )
T ⊆ ET , and ET = {(a0:T , o1:T , r0:T ) : at ∈
A, ot ∈ O, t = 0, 1, · · · , T } is the complete set of episodes of length T in the POMDP, with no
repeated elements. The condition y0:t = 0, which is an an abbreviation for yτ = 0 ∀ τ = 0, 1, · · · , t,
indicates that the agent always follows the RPR (Θ) here. Note bV (E (K )
; Θ) is the empirical value
T
function of Θ de ﬁned on E (K )
, as is bV (D(K ) ; Θ) on D(K ) . When T = ∞ 1 , the two are identical
T
up to a difference in acquiring the episodes: E (K )
is a simple enumeration of distinct episodes
T
while D(K ) may contain identical episodes. The multiplicity of an episode in D(K ) results from the
sampling process (by following a policy to interact with the environment). Note that the empirical
value function de ﬁned using E (K )
is interesting only for theoretical analysis, because the evaluation
T
requires knowing the true POMDP model, not available in practice. We de ﬁne the optimistic value
function
1Xy0 ,··· ,yt=0(cid:0)rt + (Rmax− rt)∨t
TXt=0
T ; Θ,λ, Πe ) =X
τ =0 yτ(cid:1)p(a0:t , o1:t , rt , y0:t |Θ,λ,M ,Πe ) (10)
bVf (E (K )
γ t
E (K)
T
where ∨t
τ =0yτ indicates that the agent receives rt if and only if yτ = 0 at all time steps τ =
1, 2, · · · , t; otherwise, it receives Rmax at t, which is an upper bound of the rewards in the environ-
ment. Similarly we can de ﬁne bV (D(K ) ; Θ, λ, Πe ), the equivalent expression for bVf (E (K )
; Θ, λ, Πe ).
T
The following lemma is proven in the Appendix.
Lemma 4.1 Let bV (E (K )
; Θ), bVf (E (K )
; Θ, λ, Πe ), and Rmax be de ﬁned as above.
Let
T
T
Pexlpore (E (K )
, Θ, λ, Πe ) be the probability of executing the exploration policy Πe at least once in
T
some episode in E (K )
, under the auxiliary RPR (Θ, λ) and the exploration policy Πe . Then
T
1 − γ
| bV (E (K )
; Θ) − bVf (E (K )
T
T
Rmax
1An episode almost always terminates in ﬁnite time steps in pr actice and the agent stays in the absorbing
state with zero reward for the remaining inﬁnite steps after an episode is terminated [10]. The inﬁnite horizon
is only to ensure theoretically all episodes have the same horizon length.

Pexlpore (E (K )
T

, Θ, λ, Πe ) ≥

; Θ, λ, Πe )|.

5

Proposition 4.2 Let Θ be the optimal RPR on E (K )
∞ and Θ∗ be the optimal RPR in the complete
POMDP environment. Let the auxiliary RPR hyper-parameters (λ) be updated according to (7) and
. Let Πe be the exploration policy and ǫ ≥ 0. Then either (a) bV (E∞ ; Θ) ≥
(8), with u1 ≥ umin
1
bV (E∞ ; Θ∗ ) − ǫ, or (b) the probability that the auxiliary RPR suggests executing Πe in some episode
unseen in E (K )
∞ is at least ǫ(1−γ )
.
Rmax
Proof:
It is sufﬁcient to show that if (a) does not hold, then (b) must hold. Let us assume
bV (E∞ ; Θ) < bV (E∞ ; Θ∗) − ǫ. Because Θ is optimal in E (K )
∞ ; Θ) ≥ bV (E (K )
∞ , bV (E (K )
∞ ; Θ∗ ), which
implies bV (E (\K )
∞ ; Θ) < bV (E (\K )
∞ ; Θ∗ ) − ǫ. where E (\K )
= E∞ \ E (K )
∞ . We show below that
∞
∞ ; Θ, λ, Πe ) ≥ bV (E (\K )
bVf (E (\K )
∞ ; Θ∗) which, together with Lemma 4.1, implies
∞ ; Θ)i
Rmax h bVf (E (\K )
1 − γ
∞ ; Θ, λ, Πe ) − bV (E (\K )
Pexlpore (E (\K )
∞ , Θ, λ, Πe ) ≥
Rmax h bV (E (\K )
∞ ; Θ)i ≥
ǫ(1 − γ )
1 − γ
∞ ; Θ∗) − bV (E (\K )
≥
Rmax
We now show bVf (E (\K )
∞ ; Θ, λ, Πe ) ≥ bV (E (\K )
∞ ; Θ∗). By construction, bVf (E (\K )
∞ ; Θ, λ, Πe ) is an
optimistic value function, in which the agent receives Rmax at any time t unless if yτ = 0 at τ =
0, 1, · · · , t. However, yτ = 0 at τ = 0, 1, · · · , t implies that {hτ : τ = 0, 1, · · · , t} ⊂ Hknown , By
the premise, λ is updated according to (7) and (8) and u1 ≥ umin
, therefore Θ is optimal in Hknown
1
(see the discussions following (7) and (8)), which implies Θ is optimal in {hτ : τ = 0, 1, · · · , t}.
Thus, the inequality holds.
Q.E.D.

Proposition 4.2 shows that whenever the primary RPR achieves less accumulative reward than
the optimal RPR by ǫ,
the auxiliary RPR suggests exploration with a probability exceeding
max . Conversely, whenever the auxiliary RPR suggests exploration with a probability
ǫ(1 − γ )R−1
smaller than ǫ(1 − γ )R−1
max , the primary RPR achieves ǫ-near optimality. This ensures that the agent
is either receiving sufﬁcient rewards or it is performing su fﬁcient exploration.

5 Experimental Results

Our experiments are based on Shuttle, a benchmark POMDP problem [7], with the following setup.
The primary policy is a RPR with |Z | = 10 and a prior in (4), with all hyper-parameters initially
set to one (which makes the initial prior non-informative). The auxiliary policy is a RPR sharing
{µ, W } with the primary RPR and having a prior for λ as in (6). The prior of λ is initially biased
towards exploration by using u0 = 1 and u1 > 1. We consider various values of u1 to examine
the different effects. The agent performs online learning: upon termination of each new episode,
the primary and auxiliary RPR posteriors are updated by using the previous posteriors as the current
priors. The primary RPR update follows Theorem 2.4 with K = 1 while the auxiliary RPR update
follows (7) and (8) for λ (it shares the same update with the primary RPR for µ and W ). We perform
100 independent Monte Carlo runs. In each run, the agent starts learning from a random position
in the environment and stops learning when Ktotal episodes are completed. We compare various
methods that the agent uses to balance exploration and exploitation: (i) following the auxiliary RPR,
with various values of u1 , to adaptively switch between exploration and exploitation; (ii) randomly
switching between exploration and exploitation with a ﬁxed exploration rate Pexplore (various values
of Pexplore are examined). When performing exploitation, the agent follows the current primary RPR
(using the Θ that maximizes the posterior); when performing exploration, it follows an exploration
policy Πe . We consider two types of Πe : (i) taking random actions and (ii) following the policy
obtained by solving the true POMDP using PBVI [8] with 2000 belief samples. In either case,
rmeta = 1 and η = 0.001. In case (ii), the PBVI policy is the oracle and incurs a query cost c.
We report: (i) the sum of discounted rewards accrued within each episode during learning; these
rewards result from both exploitation and exploration. (ii) the quality of the primary RPR upon
termination of each learning episode, represented by the sum of discounted rewards averaged over
251 episodes of following the primary RPR (using the standard testing procedure for Shuttle: each
episode is terminated when either the goal is reached or a maximum of 251 steps is taken); these
rewards result from exploitation alone. (iii) the exploration rate Pexplore in each learning episode,
which is the number of time steps at which exploration is performed divided by the total time steps in

6


Asymptotic Analysis of MAP Estimation via the
Replica Method and Compressed Sensing∗

Sundeep Rangan
Qualcomm Technologies
Bedminster, NJ
srangan@qualcomm.com

Alyson K. Fletcher
University of California, Berkeley
Berkeley, CA
alyson@eecs.berkeley.edu

Vivek K Goyal
Mass. Inst. of Tech.
Cambridge, MA
vgoyal@mit.edu

Abstract

The replica method is a non-rigorous but widely-accepted technique from statis-
tical physics used in the asymptotic analysis of large, random, nonlinear prob-
lems. This paper applies the replica method to non-Gaussian maximum a pos-
teriori (MAP) estimation. It is shown that with random linear measurements and
Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional
vector “decouples” as n scalar MAP estimators. The result is a counterpart to Guo
and Verd ´u’s replica analysis of minimum mean-squared erro r estimation.
The replica MAP analysis can be readily applied to many estimators used in
compressed sensing, including basis pursuit, lasso, linear estimation with thresh-
olding, and zero norm-regularized estimation.
In the case of lasso estimation
the scalar estimator reduces to a soft-thresholding operator, and for zero norm-
regularized estimation it reduces to a hard-threshold. Among other bene ﬁts, the
replica method provides a computationally-tractable method for exactly comput-
ing various performance metrics including mean-squared error and sparsity pat-
tern recovery probability.

1 Introduction

Estimating a vector x ∈ Rn from measurements of the form
(1)
y = Φx + w,
where Φ ∈ Rm×n represents a known measurement matrix and w ∈ Rm represents measurement
errors or noise, is a generic problem that arises in a range of circumstances. One of the most basic
estimators for x is the maximum a posteriori (MAP) estimate
ˆxmap (y) = arg max
px|y (x|y),
x∈Rn
which is de ﬁned assuming some prior on x. For most priors, the MAP estimate is nonlinear and its
behavior is not easily characterizable. Even if the priors for x and w are separable, the analysis of
the MAP estimate may be difﬁcult since the matrix Φ couples the n unknown components of x with
the m measurements in the vector y.
The primary contribution of this paper—an abridged version
of [1] —is to show that with certain
large random Φ and Gaussian w, there is an asymptotic decoupling of (1) into n scalar MAP estima-
tion problems. Each equivalent scalar problem has an appropriate scalar prior and Gaussian noise
with an effective noise level. The analysis yields the asymptotic joint distribution of each component
xj of x and its corresponding estimate ˆxj in the MAP estimate vector ˆxmap (y). From the joint
distribution, various further computations can be made, such as the mean-squared error (MSE) of
the MAP estimate or the error probability of a hypothesis test computed from the MAP estimate.

(2)

∗This work was supported in part by a University of California President’s Postdoctoral Fellowship and
National Science Foundation CAREER Award 0643836.

1

Replica Method. Our analysis is based on a powerful but non-rigorous technique from statistical
physics known as the replica method. The replica method was originally developed by Edwards and
Anderson [2] to study the statistical mechanics of spin glasses. Although not fully rigorous from the
perspective of probability theory, the technique was able to provide explicit solutions for a range of
complex problems where many other methods had previously failed [3].

The replica method was ﬁrst applied to the study of nonlinear MAP estimation problems by
Tanaka [4] and M ¨uller [5]. These papers studied the behavior of the MAP estimator of a vector
x with i.i.d. binary components observed through linear measurements of the form (1) with a large
random Φ and Gaussian w. The results were then generalized in a remarkable paper by Guo and
Verd ´u [6] to vectors x with arbitrary distributions. Guo and Verd ´u’s result was a lso able to incor-
porate a large class of minimum postulated MSE estimators, where the estimator may assume a
prior that is different from the actual prior. The main result in this paper is the corresponding MAP
statement to Guo and Verd ´u’s result. In fact, our result is d erived from Guo and Verd ´u’s by taking
appropriate limits with large deviations arguments.

The non-rigorous aspect of the replica method involves a set of assumptions that include a self-
averaging property, the validity of a “replica trick,” and t he ability to exchange certain limits. Some
progress has been made in formally proving these assumptions; a survey of this work can be found
in [7]. Also, some of the predictions of the replica method have been validated rigorously by other
means [8]. To emphasize our dependence on these unproven assumptions, we will refer to Guo and
Verd ´u’s result as the Replica MMSE Claim. Our main result, w hich depends on Guo and Verd ´u’s
analysis, will be called the Replica MAP Claim.

Applications to Compressed Sensing. As an application of our main result, we will develop a few
analyses of estimation problems that arise in compressed sensing [9 –11]. In compressed sensing,
one estimates a sparse vector x from random linear measurements. Generically, optimal estimation
of x with a sparse prior is NP-hard [12]. Thus, most attention has focused on greedy heuristics such
as matching pursuit and convex relaxations such as basis pursuit [13] or lasso [14]. While successful
in practice, these algorithms are difﬁcult to analyze preci sely.

Recent compressed sensing research has provided scaling laws on numbers of measurements that
guarantee good performance of these methods [15 –17]. Howev er, these scaling laws are in general
conservative. There are, of course, notable exceptions including [18] and [19] which provide match-
ing necessary and sufﬁcient conditions for recovery of stri ctly sparse vectors with basis pursuit and
lasso. However, even these results only consider exact recovery and are limited to measurements
that are noise-free or measurements with a signal-to-noise ratio (SNR) that scales to in ﬁnity.

Many common sparse estimators can be seen as MAP estimators with certain postulated priors.
Most importantly, lasso and basis pursuit are MAP estimators assuming a Laplacian prior. Other
commonly-used sparse estimation algorithms, including linear estimation with and without thresh-
olding and zero norm-regularized estimators, can also be seen as MAP-based estimators. For these
algorithms, the replica method provides—under the assumpt
ion of the replica hypotheses—not just
bounds, but the exact asymptotic behavior. This in turns permits exact expressions for various per-
formance metrics such as MSE or fraction of support recovery. The expressions apply for arbitrary
ratios k/n, n/m and SNR.

2 Estimation Problem and Assumptions

Consider the estimation of a random vector x ∈ Rn from linear measurements of the form
y = Φx + w = AS1/2x + w,
(3)
where y ∈ Rm is a vector of observations, Φ = AS1/2 , A ∈ Rm×n is a measurement matrix, S is
a diagonal matrix of positive scale factors,

(4)
S = diag (s1 , . . . , sn ) , sj > 0,
and w ∈ Rm is zero-mean, white Gaussian noise. We consider a sequence of such problems indexed
by n, with n → ∞. For each n, the problem is to determine an estimate bx of x from the observations
y knowing the measurement matrix A and scale factor matrix S.
2

The components xj of x are modeled as zero mean and i.i.d. with some prior probability distribution
p0 (xj ). The per-component variance of the Gaussian noise is E|wj |2 = σ2
0 . We use the subscript
“0 ” on the prior and noise level to differentiate these quant
ities from certain “postulated ” values to
be de ﬁned later.
In (3), we have factored Φ = AS1/2 so that even with the i.i.d. assumption on xj s above and an
i.i.d. assumption on entries of A, the model can capture variations in powers of the components of
x that are known a priori at the estimator. Variations in the power of x that are not known to the
estimator should be captured in the distribution of x.

We summarize the situation and make additional assumptions to specify the problem precisely as
follows:

(a) The number of measurements m = m(n) is a deterministic quantity that varies with n and
satisﬁes

lim
n/m(n) = β
n→∞
for some β ≥ 0. (The dependence of m on n is usually omitted for brevity.)
(b) The components xj of x are i.i.d. with probability distribution p0 (xj ).
(c) The noise w is Gaussian with w ∼ N (0, σ2
0 Im ).
(d) The components of the matrix A are i.i.d. zero mean with variance 1/m.
(e) The scale factors sj are i.i.d. and satisfy sj > 0 almost surely.
(f) The scale factor matrix S, measurement matrix A, vector x and noise w are independent.

3 Review of the Replica MMSE Claim

We begin by reviewing the Replica MMSE Claim of Guo and Verd ´u [6]. Suppose one is given a
post that may be different from
“postulated ” prior distribution
ppost and a postulated noise level σ2
the true values p0 and σ2
0 . We de ﬁne the minimum postulated MSE (MPMSE) estimate of x as
post (cid:1) = Z xpx|y (x | y ; ppost , σ2
ˆxmpmse (y) = E (cid:0)x | y ; ppost , σ2
post ) dx,
where px|y (x | y ; q , σ2 ) is the conditional distribution of x given y under the x distribution and
noise variance speciﬁed as parameters after the semicolon:
px|y (x | y ; q , σ2 ) = C −1 exp (cid:18)−
2σ2 ky − AS1/2xk2(cid:19) q(x),
nYj=1
1
where C is a normalization constant.
The Replica MMSE Claim describes the asymptotic behavior of the postulated MMSE estimator via
an equivalent scalar estimator. Let q(x) be a probability distribution de ﬁned on some set X ⊆ R.
Given µ > 0, let px|z (x | z ; q , µ) be the conditional distribution
px|z (x | z ; q , µ) = (cid:20)Zx∈X
φ(z − x ; µ)q(x) dx(cid:21)−1
where φ(·) is the Gaussian distribution
1
√2πµ
φ(v ; µ) =
The distribution px|z (x|z ; q , µ) is the conditional distribution of the scalar random variable x ∼
q(x) from an observation of the form
z = x + √µv ,
(8)
where v ∼ N (0, 1). Using this distribution, we can de ﬁne the scalar condition al MMSE estimate,
scalar (z ; q , µ) = Zx∈X
ˆxmmse
(9)
xpx|z (x|z ; µ) dx.
3

φ(z − x ; µ)q(x)

e−|v|2/(2µ) .

(7)

q(x) =

q(xj ),

(5)

(6)

(10)

Also, given two distributions, p0 (x) and p1 (x), and two noise levels, µ0 > 0 and µ1 > 0, de ﬁne
mse(p1 , p0 , µ1 , µ0 , z ) = Zx∈X |x − ˆxmmse
scalar (z ; p1 , µ1 )|2px|z (x | z ; p0 , µ0 ) dx,
which is the mean-squared error in estimating the scalar x from the variable z in (8) when x has a
true distribution x ∼ p0 (x) and the noise level is µ = µ0 , but the estimator assumes a distribution
x ∼ p1 (x) and noise level µ = µ1 .
Replica MMSE Claim [6]. Consider the estimation problem in Section 2. Let ˆxmpmse (y) be the
MPMSE estimator based on a postulated prior ppost and postulated noise level σ2
post . For each
n, let j = j (n) be some deterministic component index with j (n) ∈ {1, . . . , n}. Then there exist
effective noise levels σ2
eﬀ and σ2
p−eﬀ such that:
(a) As n → ∞, the random vectors (xj , sj , ˆxmpmse
) converge in distribution to the random
j
vector (x, s, ˆx) where x, s, and v are independent with x ∼ p0 (x), s ∼ pS (s), v ∼ N (0, 1),
and
scalar (z ; ppost , µp ), z = x + √µv .
ˆx = ˆxmmse
(11)
where µ = σ2
eﬀ /s and µp = σ2
p−eﬀ /s.
(b) The effective noise levels satisfy the equations

(12a)
(12b)

eﬀ = σ2
σ2
0 + βE [s mse(ppost , p0 , µp , µ, z )]
σ2
p−eﬀ = σ2
post + βE [s mse(ppost , ppost , µp , µp , z )] ,
where the expectations are taken over s ∼ pS (s) and z generated by (11).
The Replica MMSE Claim asserts that the asymptotic behavior of the joint estimation of the n-
dimensional vector x can be described by n equivalent scalar estimators. In the scalar estimation
problem, a component x ∼ p0 (x) is corrupted by additive Gaussian noise yielding a noisy mea-
eﬀ /s, which is the effective noise divided by the
surement z . The additive noise variance is µ = σ2
scale factor s. The estimate of that component is then described by the (generally nonlinear) scalar
estimator ˆx(z ; ppost , µp ).
p−eﬀ are described by the solutions to ﬁxed-point equations
eﬀ and σ2
The effective noise levels σ2
(12). Note that σ2
eﬀ and σ2
p−eﬀ appear implicitly on the left- and right-hand sides of these equations
via the terms µ and µp . When there are multiple solutions to these equations, the true solution is the
minimizer of a certain Gibbs’ function [6].

4 Replica MAP Claim

We now turn to MAP estimation. Let X ⊆ R be some (measurable) set and consider an estimator of
the form
nXj=1
1
2γ ky − AS1/2xk2
ˆxmap (y) = arg min
2 +
x∈X n
where γ > 0 is an algorithm parameter and f : X → R is some scalar-valued, non-negative cost
function. We will assume that the objective function in (13) has a unique essential minimizer for
almost all y.

f (xj ),

(13)

The estimator (13) can be interpreted as a MAP estimator. Speciﬁcally, for any u > 0, it can be
veriﬁed that ˆxmap (y) is the MAP estimate

ˆxmap (y) = arg max
x∈X n

px|y (x | y ; pu , σ2
u ),

u are the prior and noise level
where pu (x) and σ2
pu (x) = (cid:20)Zx∈X n
exp(−uf (x))dx(cid:21)−1
4

exp(−uf (x)), σ2
u = γ /u,

(14)

where f (x) = Pj f (xj ). To analyze this MAP estimator, we consider a sequence of MMSE
estimators
u (cid:1) .
bxu (y) = E (cid:0)x | y ; pu , σ2
(15)
The proof of the Replica MAP Claim below (see [1]) uses a standard large deviations argument to
show that
u→∞ bxu (y) = ˆxmap (y)
lim
for all y. Under the assumption that the behaviors of the MMSE estimators are described by the
Replica MMSE Claim, we can then extrapolate the behavior of the MAP estimator.
To state the claim, de ﬁne the scalar MAP estimator

1
ˆxmap
2λ |z − x|2 + f (x).
scalar (z ; λ) = arg min
x∈X
where, again, we assume that (16) has a unique essential minimizer for almost all λ and almost all
z . We also assume that the limit

F (x, z , λ), F (x, z , λ) =

(16)

σ2 (z , λ) = lim
x→ ˆx

|x − ˆx|2
2(F (x, z , λ) − F ( ˆx, z , λ))
exists where ˆx = ˆxmap
scalar (z ; λ). We make the following additional assumptions:
Assumption 1 Consider the MAP estimator (13) applied to the estimation problem in Section 2.
Assume:

(17)

,

(a) For all u > 0 sufﬁciently large, assume the postulated prior pu and noise level σ2
u satisfy
the Replica MMSE Claim. Also, assume that for the corresponding effective noise levels,
σ2
eﬀ (u) and σ2
p−eﬀ (u), the following limits exists:
σ2
σ2
eﬀ (u), γp = lim
eﬀ ,map = lim
u→∞
u→∞

uσ2
p−eﬀ (u).

lim
u→∞

ˆxu
j (n),

(b) Suppose for each n, ˆxu
j (n) is the MMSE estimate of the component xj for some index
j ∈ {1, . . . , n} based on the postulated prior pu and noise level σ2
u . Then, assume that the
following limits can be interchanged:
ˆxu
j (n) = lim
n→∞

lim
lim
u→∞
n→∞
where the limits are in distribution.
(c) Assume that f (x) is non-negative and satisﬁes f (x)/ log |x| → ∞ as |x| → ∞.
Item (a) is stated to reiterate that we are assuming the Replica MMSE Claim is valid. See [1, Sect.
IV] for additional discussion of technical assumptions.
Replica MAP Claim [1]. Consider the estimation problem in Section 2. Let ˆxmap (y) be the MAP
estimator (13) de ﬁned for some f (x) and γ > 0 satisfying Assumption 1. For each n, let j = j (n)
be some deterministic component index with j (n) ∈ {1, . . . , n}. Then:
(a) As n → ∞, the random vectors (xj , sj , ˆxmap
) converge in distribution to the random
j
vector (x, s, ˆx) where x, s, and v are independent with x ∼ p0 (x), s ∼ pS (s), v ∼ N (0, 1),
and
scalar (z , λp ), z = x + √µv ,
ˆx = ˆxmap
(18)
where µ = σ2
eﬀ ,map/s and λp = γp /s.
(b) The limiting effective noise levels σ2
eﬀ ,map and γp satisfy the equations
0 + βE (cid:2)s|x − ˆx|2 (cid:3)
σ2
eﬀ ,map = σ2
(19a)
γp = γ + βE (cid:2)sσ2 (z , λp )(cid:3) ,
(19b)
where the expectations are taken over x ∼ p0 (x), s ∼ pS (s), and v ∼ N (0, 1), with ˆx and
z de ﬁned in (18).

5

Analogously to the Replica MMSE Claim, the Replica MAP Claim asserts that asymptotic behavior
of the MAP estimate of any single component of x is described by a simple equivalent scalar esti-
mator. In the equivalent scalar model, the component of the true vector x is corrupted by Gaussian
noise and the estimate of that component is given by a scalar MAP estimate of the component from
the noise-corrupted version.

5 Analysis of Compressed Sensing

Our results thus far hold for any separable distribution for x and under mild conditions on the cost
function f . The role of f is to determine the estimator. In this section, we ﬁrst consi der choices of
f that yield MAP estimators relevant to compressed sensing. We then additionally impose a sparse
prior for x for numerical evaluations of asymptotic performance.

Lasso Estimation. We ﬁrst consider the lasso or basis pursuit estimate [13, 14] given by

1
2γ ky − AS1/2xk2
2 + kxk1 ,
where γ > 0 is an algorithm parameter. This estimator is identical to the MAP estimator (13) with
the cost function

ˆxlasso (y) = arg min
x∈Rn

(20)

f (x) = |x|.
With this cost function, the scalar MAP estimator in (16) is given by

(z ),

(21)

(22)

(23)

(24)

T soft
λ

where T soft
λ

ˆxmap
scalar (z ; λ) = T soft
λ
(z ) is the soft thresholding operator
(z ) = ( z − λ,
if z > λ;
if |z | ≤ λ;
0,
if z < −λ.
z + λ,
eﬀ ,map and γp such that
The Replica MAP Claim now states that there exists effective noise levels σ2
for any component index j , the random vector (xj , sj , ˆxj ) converges in distribution to the vector
(x, s, ˆx) where x ∼ p0 (x), s ∼ pS (s), and ˆx is given by
z = x + √µv ,
ˆx = T soft
(z ),
λp
eﬀ ,map/s. Hence, the asymptotic behavior of lasso
where v ∼ N (0, 1), λp = γp/s, and µ = σ2
has a remarkably simple description: the asymptotic distribution of the lasso estimate ˆxj of the
component xj is identical to xj being corrupted by Gaussian noise and then soft-thresholded to
yield the estimate ˆxj .
To calculate the effective noise levels, one can perform a simple calculation to show that σ2 (z , λ) in
(17) is given by
σ2 (z , λ) = (cid:26) λ,
if |z | > λ;
if |z | ≤ λ.
0,
E (cid:2)sσ2 (z , λp )(cid:3) = E [sλp Pr(|z | > λp )] = γp Pr(|z | > γp/s)
(25)
where we have use the fact that λp = γp/s. Substituting (21) and (25) into (19), we obtain the
ﬁxed-point equations
(z )|2 i
0 + βE hs|x − T soft
σ2
eﬀ ,map = σ2
λp
(26b)
γp = γ + βγp Pr(|z | > γp /s),
where the expectations are taken with respect to x ∼ p0 (x), s ∼ pS (s), and z in (23). Again, while
these ﬁxed-point equations do not have a closed-form soluti on, they can be relatively easily solved
numerically given distributions of x and s.

Hence,

(26a)

6

Zero Norm-Regularized Estimation. Lasso can be regarded as a convex relaxation of zero norm-
regularized estimation

1
ˆxzero (y) = arg min
2γ ky − AS1/2xk2
2 + kxk0 ,
x∈Rn
where kxk0 is the number of nonzero components of x. For certain strictly sparse priors, zero
norm-regularized estimation may provide better performance than lasso. While computing the zero
norm-regularized estimate is generally very difﬁcult, we c an use the replica analysis to provide a
simple characterization of its performance. This analysis can provide a bound on the performance
achievable by practical algorithms.

(27)

The zero norm-regularized estimator is identical to the MAP estimator (13) with the cost function
f (x) = (cid:26) 0,
if x = 0;
if x 6= 0.
1,
Technically, this cost function does not satisfy the conditions of the Replica MAP Claim. To avoid
this problem, we can consider an approximation of (28),
fδ,M (x) = (cid:26) 0,
if |x| < δ ;
if |x| ∈ [δ, M ],
1,
which is de ﬁned on the set X = {x : |x| ≤ M }. We can then take the limits δ → 0 and M → ∞.
To simplify the presentation, we will just apply the Replica MAP Claim with f (x) in (28) and omit
the details in taking the appropriate limits.

(28)

With f (x) given by (28), the scalar MAP estimator in (16) is given by
t = √2λ,

ˆxmap
scalar (z ; λ) = T hard
t

(z ),

(29)

(30)

(31)

(32)

T hard
t

where T hard
t

is the hard thresholding operator,
(z ) = (cid:26) z ,
if |z | > t;
if |z | ≤ t.
0,
Now, similar to the case of lasso estimation, the Replica MAP Claim states there exists effective
eﬀ ,map and γp such that for any component index j , the random vector (xj , sj , ˆxj )
noise levels σ2
converges in distribution to the vector (x, s, ˆx) where x ∼ p0 (x), s ∼ pS (s), and ˆx is given by
z = x + √µv ,
ˆx = T hard
(z ),
t
where v ∼ N (0, 1), λp = γp /s, µ = σ2
eﬀ ,map /s, and
t = p2λp = q2γp/s.
Thus, the zero norm-regularized estimation of a vector x is equivalent to n scalar components cor-
rupted by some effective noise level σ2
eﬀ ,map and hard-thresholded based on a effective noise level
γp .
The ﬁxed-point equations for the effective noise levels σ2
eﬀ ,map and γp can be computed similarly to
the case of lasso. Speciﬁcally, one can verify that (24) and ( 25) are both satisﬁed for the hard thresh-
olding operator as well. Substituting (25) and (29) into (19), we obtain the ﬁxed-point equations
(z )|2 (cid:3) ,
0 + βE (cid:2)s|x − T hard
σ2
eﬀ ,map = σ2
(33a)
t
(33b)
γp = γ + βγp Pr(|z | > t),
where the expectations are taken with respect to x ∼ p0 (x), s ∼ pS (s), z in (31), and t given by
(32). These ﬁxed-point equations can be solved numerically .

7

0

−2

−4

−6

−8

−10

−12

−14

−16

)
B
d
(
 
r
o
r
r
e
 
d
e
r
a
u
q
s
 
n
a
i
d
e
M

 

Linear   
(replica)
Linear
(sim.)
Lasso    
(replica)
Lasso 
(sim.)
Zero    
norm−reg
Optimal
MMSE   

−18
 
0.5

1
2.5
2
1.5
Measurement ratio β = n/m

3

Figure 1: MSE performance prediction with the Replica MAP Claim. Plotted is the median nor-
malized SE for various sparse recovery algorithms: linear MMSE estimation, lasso, zero norm-
regularized estimation, and optimal MMSE estimation. Solid lines show the asymptotic predicted
MSE from the Replica MAP Claim. For the linear and lasso estimators, the circles and triangles
show the actual median SE over 1000 Monte Carlo simulations.

Numerical Simulation. To validate the predictive power of the Replica MAP Claim for ﬁnite
dimensions, we performed numerical simulations where the components of x are a zero-mean
Bernoulli–Gaussian process. Speciﬁcally,
xj ∼ (cid:26) N (0, 1), with prob. 0.1;
0, with prob. 0.9.
We took the vector x to have n = 100 i.i.d. components, and we used ten values of m to vary β =
n/m from 0.5 to 3. For each problem size, we simulated the lasso and linear MMSE estimators over
1000 independent instances with noise levels chosen such that the SNR with perfect side information
is 10 dB. Each set of trials is represented by its median squared error in Fig. 1.

The simulated performance is matched very closely by the asymptotic values predicted by the replica
analysis. (Analysis of the linear MMSE estimator using the Replica MAP Claim is detailed in [1];
the Replica MMSE Claim is also applicable to this estimator.) In addition, the replica analysis can be
applied to zero norm-regularized and optimal MMSE estimators that are computationally infeasible
for large problems. These results are also shown in Fig. 1, illustrating the potential of the replica
method to quantify the precise performance losses of practical algorithms.

Additional numerical simulations in [1] illustrate convergence to the replica MAP limit, applicability
to discrete distributions for x, effects of power variations in the components, and accurate prediction
of the probability of sparsity pattern recovery.

6 Conclusions

We have shown that the behavior of vector MAP estimators with large random measurement matri-
ces and Gaussian noise asymptotically matches that of a set of decoupled scalar estimation problems.
We believe that this equivalence to a simple scalar model will open up numerous doors for analysis,
particularly in problems of interest in compressed sensing. One can use the model to dramatically
improve upon existing performance analyses for sparsity pattern recovery and MSE. Also, the tech-
nique is sufﬁciently general to study effects of dynamic ran ge.

8

References

[1] S. Rangan, A. K. Fletcher, and V. K. Goyal. Asymptotic analysis of MAP estimation via
the replica method and applications to compressed sensing. arXiv:0906.3234v1 [cs.IT]., June
2009.
[2] S. F. Edwards and P. W. Anderson. Theory of spin glasses. J. Phys. F: Metal Physics, 5:965 –
974, 1975.
[3] H. Nishimori. Statistical physics of spin glasses and information processing: An introduction.
International Series of Monographs on Physics. Oxford Univ. Press, Oxford, UK, 2001.
[4] T. Tanaka. A statistical-mechanics approach to large-system analysis of CDMA multiuser
detectors. IEEE Trans. Inform. Theory, 48(11):2888 –2910, November 2002.
[5] R. R. M ¨uller. Channel capacity and minimum probability of error in large dual antenna array
systems with binary modulation. IEEE Trans. Signal Process., 51(11):2821 –2828, November
2003.
[6] D. Guo and S. Verd ´u. Randomly spread CDMA: Asymptotics v ia statistical physics. IEEE
Trans. Inform. Theory, 51(6):1983 –2010, June 2005.
[7] M. Talagrand. Spin Glasses: A Challenge for Mathematicians. Springer, New York, 2003.
[8] A. Montanari and D. Tse. Analysis of belief propagation for non-linear problems: The example
of CDMA (or: How to prove Tanaka’s formula). arXiv:cs/0602028v1 [cs.IT]., February 2006.
[9] E. J. Cand `es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489 –
509, February 2006.
[10] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289 –1306, April
2006.
[11] E. J. Cand `es and T. Tao. Near-optimal signal recovery f rom random projections: Universal
encoding strategies? IEEE Trans. Inform. Theory, 52(12):5406 –5425, December 2006.
[12] B. K. Natarajan. Sparse approximate solutions to linear systems.
SIAM J. Computing,
24(2):227 –234, April 1995.
[13] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM
J. Sci. Comp., 20(1):33 –61, 1999.
[14] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc., Ser. B,
58(1):267 –288, 1996.
[15] D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recovery of sparse overcomplete repre-
sentations in the presence of noise. IEEE Trans. Inform. Theory, 52(1):6 –18, January 2006.
[16] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform.
Theory, 50(10):2231 –2242, October 2004.
[17] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise.
IEEE Trans. Inform. Theory, 52(3):1030 –1051, March 2006.
[18] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
ℓ1 -constrained quadratic programming (lasso). IEEE Trans. Inform. Theory, 55(5):2183 –2202,
May 2009.
[19] D. L. Donoho and J. Tanner. Counting faces of randomly-projected polytopes when the pro-
jection radically lowers dimension. J. Amer. Math. Soc., 22(1):1 –53, January 2009.

9


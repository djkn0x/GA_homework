Predictions and Rankings in College Football

Dennis Qin

10th December 2009

1

Introduction

College football is one of the most heavily attended sports leagues in world, with over 48
million in attendance over the course of the season. In the past, the national champion
was decided solely by polls, but after several seasons of multiple undefeated teams, the
NCAA instituted the Bowl Championship Series, where the top two teams face oﬀ in a
national championship game. The top teams are decided by a combination of human
polls and computer rankings, making it a unique situation where computer algorithms
can directly aﬀect the outcomes of a season.

Given the physical demands of the sport, teams can only play one game per week, so
throughout the entire season, teams play at most 14 games, including championship
games, and with over a hundred teams contending for the national title, it is not uncom-
mon for there to be controversy over who deserves the title.

In the past, computer rankings in the BCS took margin of victory into consideration, but
the BCS has gradually removed such rankings from the system, arguing that the ob jective
of the game is defeat the opponent and not to score as many points as possible against
weaker opposition, which is considered bad sportsmanship. It is thus of great interest to
ﬁnd methods of consistently ranking teams based on the results of relatively few matches.

2 Data

The data was taken from James Howell’s College Football Scores website [1], which has
the scores for every game in which at least one of the teams was in the Football Bowl
Subdivision, broken down by season. Since the BCS does not allow computer ranking
systems to consider margin of victory, the main approach used here will only consider the
wins and losses and not the scores.

Each season is approximately 800 games between 120 diﬀerent teams, so each team can
be identiﬁed by 7 bits and the outcome of a game can be represented by 1 bit, so conserva-
tively, this is at most 1500 bytes of data, which makes it a challenge to extract meaningful
results.

1

3 Bradley-Terry Model

3.1 Description

The Bradley-Terry model [2] assumes that each team has a strength parameter πi and
πi
the probability of that team i winning a game against team j is
with independent
πi+πj
outcomes. With a vector of strength parameters π , we can calculate the likelihood of the
outcome of any game and also calculate the likelihood function given a set of outcomes as
the product of all the individual outcomes. The estimates for the strength parameters is
then the maximum likelihood estimator based on the outcomes during the season. These
estimates can then be used to rank the teams as well as a probabilistic prediction for any
matchup.

3.2

Implementation

This was implemented in MATLAB, using constrained numerical optimzation of the log-
likelihood function. The constraints were set such that the strength of each team was
bounded between 0.01 and 1000, since MLE would give estimates of 0 or inﬁnity if left
unconstrained.

3.3 Results

The MLE yields reasonable rankings when compared to the consensus rankings [3], which
are compiled by Kenneth Massey from the results of over a hundred ranking systems,
including both mathematical systems and human polls. The idea is that if ranking systems
have errors that are not strongly correlated, then the consensus result would have a much
lower error than the individual results, and the best rankings are the ones that are most
highly correlated with those results. In 2007, the Bradley-Terry model had 9 of the top
10 teams in common with the consensus rankings, and in 2008, the Bradley-Terry model
had all of the top 10 teams the consensus.

At ﬁrst, there were quite a few teams from lower divisions which topped the rankings with
1-0 records, so the rankings were recompiled after removing teams that had played less
than 4 games to ﬁlter out teams that were not in the FBS. This ﬁlter worked eﬀectively
to provide solid rankings, but it is more of a workaround. Another solution was to run the
algorithm on results from all divisions to acquire a more complete picture, but the lack
of connectivity between teams caused any improvements in rankings to be overshadowed
by the introduction of more weak undefeated teams.

4 Maximum a Posteriori Estimation

4.1 Motivation

Although the results using the MLE were reasonably consistent, there are two ma jor
ﬂaws with the method. The most obvious ﬂaw is that under MLE, undefeated teams

2

Table 1: Comparison of Bradley-Terry Estimation and Consensus Top 10
2008 Consensus
2008 Bradley-Terry
2007 Consensus
2007 Bradley-Terry
Florida
Utah*
Kansas*
LSU
Texas
Florida*
Missouri
LSU*
Missouri
West Virginia
Texas*
USC
Oklahoma
Oklahoma
Kansas
Georgia
Utah
USC
Ohio State
USC
Alabama
Alabama
Georgia
USC
Virginia Tech
Ohio State
Texas Tech
Penn State
TCU
TCU
Oklahoma
West Virginia
Texas Tech
Boise State
Virginia Tech
Oklahoma
Tennessee
Florida
Penn State
Boise State
* Indicates that the strength parameter was at the upper bound of the constraint

will be estimated at inﬁnity if unconstrained and winless teams will be estimated at zero.
This implies that teams that go undefeated against weak opponents will always be ranked
higher than teams that have lost while facing strong opponents, as discussed in detail in
the previous section. It also implies that there is no way to diﬀerentiate between teams
which are undefeated. The second ﬂaw is that any scalar multiple of a vector will have
the same likelihood, so the results aren’t unique. With the MAP estimation, the results
will be regularized by the prior distribution, eliminating the problems above.

The prior distribution chosen for the model is the lognormal distribution. This bounds
the estimates to positive values and assigns low probabilities to both very low and very
high estimates. The lognormal distribution is deﬁned by parameters µ and σ, and changes
in µ only aﬀect the estimates by a constant factor and thus have no eﬀect on the esti-
mated rankings or probability estimates. The value of 1.5 for σ was chosen after some
experimentation, but the choice does not have a large impact on the results.

4.2 Results

The rankings using MAP estimation are much more robust than before, and there is no
need for the ﬁlter used in the previous section. The teams that have only played a single
game are estimated to be close to the median strength, a much more reasonable estimate
than the extremes. This also removed the need for constraints during optimization. The
results from using a lognormal prior are shown in Table 2.

There are other distributions to be considered, so the model was rerun with logistic,
normal and exponential priors. The results are fairly similar, as none of the distributions
showed a signiﬁcant diﬀerence in rankings for the vast ma jority of teams. The lognormal
prior, however, has the least inconsistencies where teams are far displaced from their rank
in the polls, making it the most accurate result.

After comparing the diﬀerent implementations against the consensus results, there appear
to be two ma jor factors causing the discrepancies. First, the models, be it MLE or MAP,
overemphasize wins and losses, making it very diﬃcult for teams with more losses to be

3

Table 2: Comparison of MAP Estimation and Consensus Top 10
2008 MAP 2008 Consensus
2007 Consensus
2007 MAP
Florida
Utah
LSU
LSU
Texas
Florida
Kansas
Missouri
West Virginia
Missouri
Texas
USC
Oklahoma
Oklahoma
Kansas
Georgia
USC
USC
West Virginia
Utah
Alabama
Alabama
Ohio State
Georgia
Ohio State
Hawaii
Texas Tech
Penn State
TCU
Oklahoma
USC
TCU
Texas Tech
Boise State
Virginia Tech
Virginia Tech
Oklahoma
Florida
Penn State
Boise State

Figure 1: Estimates for MLE

Figure 2: Estimates for MAP

higher in the rankings despite facing tougher opposition. Also, the models do not consider
the date on which games were played. In sequential update systems and human polls, a
greater weight is put on more recent results. While it can be argued that such a weighting
only increases the variance of the results, it also reﬂects the reality that team strengths
change over time as they practice and make adjustments throughout the season or as
injuries occur.

From the histograms of parameter estimates in Figures 1 and 2, we can see that there is a
lot of separation for the top teams, but the rest of the teams all have very low estimated
strengths. It is also signiﬁcant that the distribution of MAP estimates does not appear
to be lognormal, the distribution of the prior. This indicates that perhaps an entirely
diﬀerent probabilistic model would provide more accurate results.

5 Conclusion

The ﬁnal model provides a fairly accurate method of ranking college football teams with
very little data. The use of MAP estimates signiﬁcantly increases the robustness of
the model over MLE, especially in situations where teams are not well connected or

4

there are not many results, such as early in the season. This model can adapted as a
ranking system in many situations with paired comparisons, particularly when sequential
updating systems do not learn fast enough. There are, however, consistent discrepancies
between the model and the consensus rankings where future work can be done. Possible
modiﬁcations include weighting by time, using a prior based on expert knowledge, and a
diﬀerent probabilistic framework.

6 References

[1] Howell, James. ”James Howell’s College Football Scores.” College Football History.
Web. 9 Dec 2009. <http://homepages.cae.wisc.edu/˜dwilson/rsfc/history/howell/>.

[2] Bradley, R.A., and M.E. Terry. ”Rank analysis of incomplete block designs, I. the
method of paired comparisons.” Biometrika. 39 (1952): 324-345. Print.

[3] Massey, Kenneth. ”College Football Ranking Comparison.” Massey Ratings. 05 12
2009. Web. 9 Dec 2009. <http://www.masseyratings.com/cf/compare.htm>.

5


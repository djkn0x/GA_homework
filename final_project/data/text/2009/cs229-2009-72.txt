Machine Learning in Statistical Arbitrage

Xing Fu, Avinash Patra

December 11, 2009

Abstract

We apply machine learning methods to obtain an
index arbitrage strategy. In particular, we em-
ploy linear regression and support vector regres-
sion (SVR) onto the prices of an exchange-traded
fund and a stream of stocks. By using principal
component analysis (PCA) in reducing the di-
mension of feature space, we observe the beneﬁt
and note the issues in application of SVR. To
generate trading signals, we model the residuals
from the previous regression as a mean reverting
process. At the end, we show how our trading
strategies beat the market.

1

Introduction

In the ﬁeld of investment, statistical arbitrage
refers to attempting to proﬁt from pricing ineﬃ-
ciencies identiﬁed through mathematical models.
The basic assumption is that prices will move to-
wards a historical average. The most commonly
used and simplest case of statistical arbitrage is
pairs trading. If stocks P and Q are in the same
industry or have similar characteristics, we ex-
pect the returns of the two stocks to track each
other. Accordingly, if Pt and Qt denote the cor-
responding price time series, then we can model
the system as

+ dXt ,

= αdt + β

dQt
dPt
Qt
Pt
where Xt is a mean reverting Ornstein-Uhlenbeck
stochastic process.
In many cases of interest, the drift α is small
compared to the ﬂuctuations of Xt and can

therefore be neglected. The model suggests a
contrarian investment strategy in which we go
long one dollar of stock P and short beta dollars
of stock Q if Xt is small and, conversely, go short
P and long Q if Xt is large.

Opportunities for this kind of pairs trading de-
pend upon the existence of similar pairs of assets,
and thus are naturally limited. Here, we use a
natural extension of pairs trading called index
arbitrage, as described in [1]. The trading system
we develop tries to exploit discrepancies between
a target asset, the iShares FTSE/MACQ traded
in the London Stock Exchange, and a synthetic
artiﬁcial asset represented by a data stream ob-
tained as a linear combination of a possibly large
set of explanatory streams assumed to be corre-
lated with the target stream. In our case, we use
the stock prices of the 100 constituents of FTSE
100 Index to reproduce the target asset.

We start with linear regression on the 100 con-
stituents and take the window as 101 days from
April to September 2009. After extracting sig-
niﬁcant factors by Principal Component Analy-
sis, we calibrate the model cutting the window
size down to 60 days. Closed prices of each as-
set are used. The emphasis here is to decom-
pose the stock data into systematic and idiosyn-
cratic components and statistically model the
idiosyncratic part. We apply both Linear Re-
gression and Supported Vector Regression, and
collect the residual that remains after the de-
composition is done.
Finally, we study the
mean-reversion using auto-regression model in
the residuals to generate trading signals for our
target asset.

1

2 Linear Regression

We consider the data of 101 days from Apr.28 to
Sep.18, 2009. The reason we choose this time pe-
riod is that given 100 feature variables, we need
at least 101 observations to train the 101 param-
eters in the linear regression model. To avoid
overﬁtting parameters, we only use 101 training
examples.
Denote Pt to be target asset and Qit to be the
ith stock constituent at time t. The linear model
100(cid:88)
can be written as
i=1
Implementing the ordinary least squares algo-
rithm in MATLAB, we get parameters θ and
training errors, i.e. residuals.

Pt = θ0 +

θiQit .

we apply Principal Component Analysis to re-
duce the dimension of the model.

Figure 2: Generalization error on 30 testing days

3 Principal
Component
Analysis (PCA)

Now we apply PCA to analyze the 100 stocks.
The estimation window for the correlation ma-
trix is 101 days. The eigenvalues at the top of
the spectrum which are isolated from the bulk
spectrum are obviously signiﬁcant. The problem
becomes evident by looking at eigenvalues of the
correlation matrix in Figures 3. Apparently, the

Figure 1: Residuals of linear regression over 100
constituents

From Figure 1, we see that the empirical er-
ror is acceptable. However, when we test this
linear model using 30 examples not in the train-
ing set, i.e. prices of each asset from Sep. 21 to
Oct. 30, 2009, the generalization error is far from
satisfying, as shown in Figure 2. This implies
that we are facing an overﬁtting problem, even
though we’ve used the smallest possible training
set. There are so many parameters in the linear
model, which gets us into this problem. Hence,

Figure 3: eigenvalue of the correlation matrix

eigenvalues within the ﬁrst twenty reveal almost

2

all information of the matrix. Now we apply val-
idation rules to ﬁnd how many principal compo-
nents to use that can give us the least general-
ization error. Given that the dimension of the
model is reduced, we reset our window size to 60
days to avoid overﬁtting problem. After running
multivariate linear regression within the ﬁrst 20
components using 60 days training set, we ﬁnd
that the ﬁrst 12 components give the smallest
generalization error on the 30 testing days. The
out-of-sample residual is shown in Figure 4.

Figure 5: Empirical error on the ﬁrst 12 compo-
nents over 60 training days

4 Support Vector Regres-
sion (SVR)

We apply SVR on the 12 feature attributes ob-
tained through PCA. We use a Gaussian ker-
nel and empirically decide on the kernel band-
width, cost and epsilon (slack variable) parame-
ters. (We used SVM and Kernel Methods MAT-
LAB toolbox, available online for implementa-
tion). We do not notice any improvement in this
approach,as seen in the plots of training error
and test error, Figure 6 and Figure 7. A main
issue here is to be able to determine appropriate
SVR parameters.

Figure 4: Generalization error on the ﬁrst 12
components over 30 out-of-sample days

We take a quick look back at the empirical er-
ror of these 12 components over 60 training days.
From Figure 5, we see that the residuals are not
as satisfying as in Figure 1, in terms of magni-
tude, but residuals here successfully reveal the
trend of residuals using 100 constituents. Thus,
by applying PCA to reduce the dimension of the
model, we avoid overﬁtting parameters. We will
use the in-sample residuals from regression on
principal components to generate trading signals
in a following section.

5 Mean Reverting Process

We want to model the price of the target as-
set such that it accounts for a drift which mea-
sures systematic deviations from the sector and
a price ﬂuctuation that is mean-reverting to the
overall industry level. We construct a trading
strategy when signiﬁcant ﬂuctuations from equi-
librium are observed.
We introduce a parametric mean reverting
model for the asset, the Ornstein-Uhlembeck

3

Figure 6: Empirical error on the ﬁrst 12 compo-
nents over 60 training days using SVR

Figure 7: Generalization error on the ﬁrst 12
components over 30 testing days using SVR

process,
dX (t) = κ(m − X (t))dt + σdW (t), κ > 0.

The term dX (t) is assumed to be the increment
of a stationary stochastic process which mod-
els price ﬂuctuations corresponding to idiosyn-
cratic ﬂuctuations in the prices which are not
reﬂected in the industry sector, i.e. the residuals
from linear regression on principal components
in the previous section. Note that the increment
dX (t) has unconditional mean zero and condi-
tional mean equal to
E {dX (t)|X (s), s ≤ t} = κ(m − X (t))dt.

The conditional mean, i.e.
the forecast of ex-
pected daily returns, is positive or negative ac-
cording to the sign of m − X (t).
This process is stationary and can be esti-
mated by an auto-regression with lag 1. We
use the residuals on a window of length 60 days,
assuming the parameters are constant over the
window. In fact, the model is shown as
t = 1, 2, · · · , 59,

Xt+1 = a + bXt + t+1 ,

where we have

a = m(1 − e−κ∆t )

b = e−κ∆t
V ar() = σ2 1 − e−κ∆t
2κ

.

Hence, we get
κ = − log(b) × 101 = 178.2568
(cid:114) V ar() · 2κ
= −0.0609
a
m =
1 − b
(cid:114) V ar()
1 − b2 = 135.9869
1 − b2 = 7.2021.

σeq =

σ =

6 Signal Generation

We deﬁne a scalar varaible called the s-score
s = X (t) − m
σeq

.

The s-score measures the distance of the cointe-
grated residual from equilibrium per unit stan-
dard deviation, i.e. how far away a given stock is
from the theoretical equilibrium value associated
with our model. According to empirical studies
in this ﬁeld, our basic trading signal based on
mean-reversion is

4

1. Buy iShare FTSE if s < −1.25
2. Sell iShare FTSE if s > 1.25

3. Close short position in iShare FTSE if s <
0.75

4. Close long position in iShare FTSE if s >
−0.5

The rationale for opening trades only when the
s-score s is far from equilibrium is to trade only
when we think that we detected an anomalous es-
cursion of the co-integration residual. We then
need to consider when we close trades. Clos-
ing trades when the s-score is near zero makes
sense, since we expect most stocks to be near
equilibrium most of the time. Thus, our trad-
ing rule detects stocks with large excursions and
trades assuming these excursions will revert to
the mean.
Figure 8 shows the signals we generate over 60
days.

60 days data) that using our strategy based on
the trading signals clearly makes a trading proﬁt
as on average we purchase iShare FTSE when
it is ”low” and sell it when it is ”high”. On
the other hand, in the future, idiosyncratic fac-
tors behaving erratically (such as occurrence of
some speciﬁc unforeseen event) might lead to the
index systematically under-performing or doing
much better than the ”signiﬁcant factors” found
from PCA and this could seriously undermine
the eﬀectiveness of our approach. To achieve a
systematic method, online learning would be a
good way to try out, in order to update our fea-
ture set with the latest information.

References

[1] G. Montana, K. Triantafyllopoulos and T.
Tsagaris, Flexible least squares for tempo-
ral data mining and statistical arbitrage, Ex-
pert Systems with Applications, Vol. 36, Is-
sue 2, Part 2, pp. 2819-2830, 2009.

[2] A. Marco and Jeong-Hyun Lee , Statistical
Arbitrage in the U.S. Equities Market, So-
cial Science Research Network, 2008.

[3] T. Fletcher, Support Vector Macines Ex-
plained, 2009

Figure 8: Trading signals over 60 days

7 Conclusion

We note that PCA helped in getting rid of over-
ﬁtting problem with 100 feature attributes, while
conducting linear regression. However, we see
that to eﬀectively apply Support Vector Regres-
sion, a technique to learn SVR-parameters might
have to be developed. We see on testing (over

5


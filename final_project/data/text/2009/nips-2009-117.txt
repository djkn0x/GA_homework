Sparsistent Learning of Varying-coefﬁcient Models
with Structural Changes

Mladen Kolar, Le Song and Eric P. Xing ∗
School of Computer Science, Carnegie Mellon University
{mkolar,lesong,epxing}@cs.cmu.edu

Abstract

To estimate the changing structure of a varying-coefﬁcient
varying-structure
(VCVS) model remains an important and open problem in dynamic system mod-
elling, which includes learning trajectories of stock prices, or uncovering the
topology of an evolving gene network. In this paper, we investigate sparsistent
learning of a sub-family of this model — piecewise constant V CVS models. We
analyze two main issues in this problem: inferring time points where structural
changes occur and estimating model structure (i.e., model selection) on each of
the constant segments. We propose a two-stage adaptive procedure, which ﬁrst
identi ﬁes jump points of structural changes and then identi
ﬁes relevant covariates
to a response on each of the segments. We provide an asymptotic analysis of
the procedure, showing that with the increasing sample size, number of structural
changes, and number of variables, the true model can be consistently selected. We
demonstrate the performance of the method on synthetic data and apply it to the
brain computer interface dataset. We also consider how this applies to structure
estimation of time-varying probabilistic graphical models.

1
Introduction
Consider the following regression model:
Yi = X′
(1)
iβ (ti ) + ǫi ,
i = 1, . . . , n,
where the design variables Xi ∈ Rp are i.i.d. zero mean random variables sampled at some con-
ditions indexed by i = 1, . . . , n, such as the prices of a set of stocks at time i, or the signals from
some sensors deployed at location i; the noise ǫ1 , . . . , ǫn are i.i.d. Gaussian variables with variance
σ2 independent of the design variables; and β (ti ) = (β1 (ti ), . . . , βp (ti ))′ : [0, 1] 7→ Rp is a vector
of unknown coefﬁcient functions. Since the coefﬁcient vect
or is a function of the conditions rather
than a constant, such a model is called a varying-coefﬁcient model
[12]. Varying-coefﬁcient models
are a non-parametric extension to the linear regression models, which unlike other non-parametric
models, assume that there is a linear relationship (generalizable to log-linear relationship) between
the feature variables and the output variable, albeit a changing one. The model given in Eq. (1) has
the ﬂexibility of a non-parametric model and the interpreta bility of an ordinary linear regression.

Varying-coefﬁcient models were popularized in the work of [ 9] and [16]. Since then, they have been
applied to a variety of domains, including multidimensional regression, longitudinal and functional
data analysis, and modeling problems in econometrics and ﬁn ance, to model and predict time- or
space- varying response to multidimensional inputs (see e.g. [12] for an overview.) One can easily
imagine a more general form of such a model applicable to these domains, where both the coefﬁcient
value and the model structure change with values of other variables. We refer to this class of models
as varying-coefﬁcient varying-structure (VCVS) models. T he more challenging problem of structure
recovery (or model selection) under VCVS has started to catch attention very recently [1, 24].
∗LS is supported by a Ray and Stephenie Lane Research Fellowship. EPX is supported by grant ONR
N000140910758, NSF DBI-0640543, NSF DBI-0546594, NSF IIS-0713379 and an Alfred P. Sloan Research
Fellowship. We also thank Za¨ıd Harchaoui for useful discussions.

1

)
t
(
1
β

)
t
(
2
β

0.5
0
−0.5
0.5
0
−0.5

…

)
t
(
1
β

)
t
(
2
β

0.5
0
−0.5
0.5
0
−0.5

β1

β2

)
t
(
1
β

)
t
(
2
β

0.5
0
−0.5
0.5
0
−0.5

…

…

Y

.

.

.

)
t
(
p
β

0.5
0
−0.5

0

)
t
(
p
β

0.5
0
−0.5

0

)
t
(
p
β

0.5
0
−0.5

0

1

βp

0.8

0.2

0.8

0.2

0.4
0.6
0.4
0.6
0.4
0.6
Time t (i/n)
Time t (i/n)
Time t (i/n)
(d)
(c)
(b)
(a)
Figure 1: (a) Illustration of an VCVS as varying functions of time. The interval [0, 1] is partitioned into
{0, 0.25, 0.4, 0.7, 1}, which deﬁnes blocks on which the coefﬁcient functions are constant. A t different blocks
only covariates with non-zero coefﬁcient affect the response, e.g. on the interval B2 = (0.25, 0.4) covariates
X2 and Xp do not affect response. (b) Schematic representation of the covariates affecting the response during
the second block in panel (a), which is reminiscent of neighborhood selection in graph structure learning. (c)
and (d) Application of VCVS for graph structure estimation (see Section 7) of non-piecewise constant evolving
graphs. Coefﬁcients deﬁning neighborhoods of different nodes ca
n change on different partitions.

0.2

0.8

1

1

In this paper, we analyze VCVS as functions of time, and the main goal is to estimate the dynamic
structure and jump points of the unknown vector function β (t). To be more speci ﬁc, we consider the
case where the function β (t) is time-varying, but piecewise constant (see Fig. 1), i.e., there exists
a partition T = {T1 = 0 < T2 < . . . < TB = 1}, 1 < B ≤ n, of the time interval (scaled to)
[0, 1], such that β (t) = γj , t ∈ [Tj−1 , Tj ) for some constant vectors γj ∈ Rp , j = 1, . . . , B . We
refer to points T1 , . . . , TB as jump points. Furthermore, we assume that at each time point ti only
a few covariates affect the response, i.e., the vector β (ti ) is sparse. A good estimation procedure
would be able to identify the correct partition of the interval [0, 1] so that within each segment the
coefﬁcient function is constant. In addition, the procedur e can identify active coefﬁcients and their
values within each segment, i.e., the time-varying structure of the model. This estimation problem
is particularly important in applications where one needs to uncover dynamic relational information
or model structures from time series data. For example, one may want to infer at chosen time points
the (changing) set of stocks that are predictive of a particular stock one has been holding from a
time series of all stock prices; or to understand the evolving circuitry of gene regulation at different
growth stages of an organism that determines the activity of a target gene based on other regulative
genes, based on time series of microarray data. Another important problem is to identify structural
changes in ﬁelds such as signal processing, EEG segmentatio n and analysis of seismic signals. In
all these problems, the goal is not to estimate the optimum value of β (t) for predicting Y , but to
consistently uncover the zero and non-zero patterns in β (t) at time points of interest that reveal the
changing structure of the model. In this paper, we provide a new algorithm to achieve this goal, and
a theoretical analysis that proves the asymptotic consistency of our algorithm.

Our problem is remotely related to, but very different from, earlier works on linear regression models
with structural changes [4], and the problem of change-point detection (e.g. [19]), which can also
be analyzed in the framework of varying-coefﬁcient models. A number of existing methods are
available to identify only one structural change in the data; in order to identify multiple changes
these methods can be applied sequentially on smaller intervals that are assumed to harbor only one
change [14]. Another common approach is to assume that there are K changes and use Dynamic
Programming to estimate them [4]. In this paper, we propose and analyze a penalized least squares
approach, which automatically adapts to the unknown number of structural changes present in the
data and performs the variable selection on each of the constant regions.

2 Preliminaries

For a varying-coefﬁcient regression model described in Eq.
(1) with structural changes, a reason-
able estimator of the time-varying structure can be obtained by minimizing the so-called TESLA
(temporally smoothed L1 -regularized regression) loss proposed in [1]: (for simplicity we suppress
the sample-size notation n in the regularization constants λn = {λn
2 }, but it should be clear that
1 , λn
their values depend on n)

ˆβ (t1 ; λ), . . . , ˆβ (tn ; λ) = arg min
β

p
n
n
Xi=1
Xi=1
Xk=1
where ||·||1 denotes the ℓ1 norm, and ||·||TV denotes a total variation norm:
||βk ||TV =
Pn
i=2 |βk (ti ) − βk (ti−1 )|. From the analysis of [20], it is known that each component function

iβ (ti ))2 + 2λ1
(Yi − X′

||β (ti )||1 + 2λ2

||βk ||TV ,

(2)

2

βk can be chosen as a piecewise constant and right continuous function, i.e., βk is a spline function,
with potential jump points at observation times ti , i = 1, . . . , n. In this particular case, the total
variation penalty deﬁned above allows us to conceptualize βk as a vector in Rn , whose components
βk,i ≡ βk (ti ) correspond to function values at ti , i = 1, . . . , n, but not as a function [0, 1] 7→ R. We
continue to use the vector representation through the rest of the paper as it will simplify the notation.

The estimation problem deﬁned in Eq. (2) has a few appealing p roperties. The objective function on
the right-hand-side is convex and there exists a solution ˆβ , which can be found efﬁciently using a
standard convex optimization package. Furthermore, the penalty terms in Eq. (2) are constructed in
a way to perform model selection. Observe that ℓ1 penalty encourages sparsity of the signal at each
time point and enables a selection over the relevant coefﬁci ents; whereas the total variation penalty
is used to partition the interval [0, 1] so that ˆβk is constant within each segment. However, there are
also some drawbacks of the procedure, as shown in Lemma 1 below.

Let’s start with some notational clari ﬁcations. Let X denote the design matrix, input observation
Xi at time i corresponds to the i-th row in X. For simplicity, we assume throughout the paper
that X are normalized to have unit length columns, i.e., each dimension has unit Euclidean norm.
Let Bj , j = 1, . . . , B , denote the set of time points that fall into the interval [Tj−1 , Tj ); when the
meaning is clear from the context, we also use Bj as a shorthand of this interval. For example, XBj
and YBj represent the submatrix of X and subvector of Y , respectively, that include elements only
corresponding to time points within interval Bj . For a given solution ˆβ to Eq. (2), there exists a
block partition ˆT = { ˆT1 , . . . , ˆT ˆB } of [0, 1] (possibly a trivial one) and unique vectors ˆγj ∈ Rp , j =
1, . . . , ˆB , such that ˆβk,i = ˆγj,k for ti ∈ ˆBj . The set of relevant covariates during inverval Bj , i.e., the
support of vector γj , is denoted as SBj = {k | γj,k 6= 0}. Likewise we deﬁne ˆS ˆBj
over ˆγj .
By construction, no consecutive vectors ˆγj and ˆγj+1 are identical. Note that both the number of
partitions ˆB = | ˆT |, and the elements in the partition ˆT , are random quantities. The following
lemma characterizes the vectors ˆγj using the subgradient equation of Eq. (2).
Lemma 1 Let ˆγj and ˆBj , j = 1, . . . , ˆB be vectors and segments obtained from a minimizer of
Eq. (2). Then each ˆγj can be found as a solution to the subgradient equation:
j + λ2 ˆs(TV)
+ λ1 | ˆBj |ˆs(1)
ˆγj − X′
X′
ˆBj
ˆBj
j
ˆs(1)
j ∈ ∂ ||ˆγj ||1 = sign(γj ),
by convention sign(0) ∈ [−1, 1], and ˆs(TV)
∈ Rp such that
j
= (cid:26) 1 if ˆγ ˆB ,k − ˆγ ˆB−1,k > 0
if ˆγ2,k − ˆγ1,k > 0
1,k = (cid:26) −1
ˆs(TV)
ˆs(TV)
,
−1 if ˆγ ˆB ,k − ˆγ ˆB−1,k < 0
if ˆγ2,k − ˆγ1,k < 0
1
ˆB ,k
and, for 1 < j < ˆB ,
if ˆγj+1,k − ˆγj,k > 0, ˆγj,k − ˆγj−1,k < 0
j,k = ( 2
ˆs(TV)
if ˆγj+1,k − ˆγj,k < 0, ˆγj,k − ˆγj−1,k > 0
−2
if (ˆγj,k − ˆγj−1,k )(ˆγj+1,k − ˆγj,k ) = 1.
0
Lemma 1 does not provide a practical way to estimate ˆβTV , but it does characterize a solution.
From Eq. (3) we can see that the coefﬁcients in each of the esti mated blocks are biased by two terms
coming from the ℓ1 and ||·||TV penalties. The larger the estimated segments, the smaller the relative
inﬂuence of the bias from the total variation, while the magn itude of the bias introduced by the ℓ1
penalty is uniform across different segments. The additional bias coming from the total variation
penalty was also noted in the problem of signal denoising [23]. In the next section, we introduce a
two step procedure which alleviate this effect.

where

X ˆBj

= 0,

Y ˆBj

(3)

(4)

(5)

(6)

3 A two-step procedure for estimating time-varying structures

In this section, we propose a new algorithm for estimating the time-varying structure of the varying-
coefﬁcient model in Eq. (1), which does not suffer from the bi as introduced by minimizing the
objective in Eq. (2). The algorithm is a two-step procedure summarized as follows:

3

(7)

(8)

(Yi − X′
iγ )2 + 2λ1 ||γ ||1 .

1. Estimate the block partition ˆT , on which the coefﬁcient vector is constant within each
block. This can be obtained by minimizing the following objective:
p
n
iβ (ti ))2 + 2λ2
Xk=1
Xi=1
(Yi − X′
||βk ||TV ,
which we refer to as a temporal difference (TD) regression for reasons that will be clear
shortly. We will employ a TD-transformation to Eq. (7) and turn it into an ℓ1 -regularized
regression problem, and solve it using the randomized Lasso. Details of the algorithm and
how to extract ˆT from the TD-estimate will be given shortly.
2. For each block of the partition, ˆBj , 1 ≤ j ≤ ˆB , estimate ˆγj by minimizing the Lasso
objective within the block:
γ∈Rp Xti∈ ˆBj
ˆγj = argmin
We name this procedure TDB-Lasso (or TDBL), after the two steps (TD randomized Lasso, and
Lasso within Blocks) given above. The advantage of the TDB-Lasso compared to a minimizer of
Eq. (2) comes from decoupling the interactions between the ℓ1 and TV penalties (note that the two
procedures result in different estimates). Now we discuss step 1 in detail; step 2 is straightforward
using a standard Lasso toolbox.
To obtain a consistent estimate of ˆT from the TD-regression in Eq. (7), we can transform Eq. (7)
into an equivalent ℓ1 penalized regression problem, which allows us to cast the ˆT estimation
problem as a feature selection problem. Let β †
k,i denote the temporal difference between the re-
gression coefﬁcients corresponding to the same covariate k at successive time points ti−1 and
ti : β †
k,i ≡ βk (ti ) − βk (ti−1 ), k = 1, . . . , p, i = 1, . . . , n with βk (t0 ) = 0, by conven-
It can be shown that the model in Eq. (1) can be expressed as Y † = X†β † + ǫ† , where
tion.
Y † ∈ Rn is a transformed vector of the TDs of responses, i.e., each element Y †
i ≡ Yi − Yi−1 ;
X† = (X†
p ) ∈ Rn×np is the transformed design matrix with lower triangular matrices
1 , . . . , X†
X†
k ∈ Rn×n corresponding to TD features computed from the covariates; ǫ† ∈ Rn is the trans-
formed TD-error vector; and β † ∈ Rnp is a vector obtained by stacking TD-coefﬁcient vectors β †
k .
(See Appendix for more details of the transformation.) Note that the elements of the vector ǫ† are
not i.i.d. any more. Using the transformation above, the estimation problem deﬁned on objective
Eq. (7) can be expressed in the following matrix form:
2
ˆβ † = argmin
β∈Rnp (cid:12)(cid:12)(cid:12)(cid:12)Y † − X†β † (cid:12)(cid:12)(cid:12)(cid:12)
2 + 2λ2 (cid:12)(cid:12)(cid:12)(cid:12)β † (cid:12)(cid:12)(cid:12)(cid:12)1 .
This transformation was proposed in [8] in the context of one-dimensional signal denoising, how-
ever, we are interested in the estimation of jump points in the context of time-varying coefﬁcient
model.

(9)

The estimator deﬁned in Eq. (9) is not robust with respect to s mall perturbations of data, i.e., small
changes of variables Xi or Yi would result in a different ˆT . To deal with the problem of robustness,
we employed the stability selection procedure of [22] (see also the bootstrap Lasso [2], however,
we have decided to use the stability selection because of the weaker assumptions). The stability
selection approach to estimating the jump-points is comprised of two main components: i) simulat-
ing multiple datasets using bootstrap, and ii) using the randomized Lasso outlined in Algorithm 1
(see also Appendix) to solve (9). While the bootstrap step improves the robustness of the estimator,
the randomized Lasso weakens the conditions under which the estimator ˆβ † selects exactly the true
features.
Let { ˆβ †
b , ˆJ †
b=1 represent the set of estimates and their supports (i.e., index of non-zero elements)
b }M
obtained by minimizing (9) for each of the M bootstrapped datasets. We obtain a stable estimate of
the support by selecting variables that appear in multiple supports
ˆJ τ = {k | PM
b=1 1I{k ∈ ˆJ †
b }
≥ τ },
M
which is then used to obtain the block partition estimate ˆT . The parameter τ is a tuning parameter
that controls the number of falsely identi ﬁed jump points.
4

(10)

Algorithm 1 Randomized Lasso
Input: Dataset {Xi , Yi }n
i=1 Xi ∈ Rp , penalty parameter λ, weakness parameter α ∈ (0, 1]
Output: Estimate ˆβ ∈ Rp , support ˆS
1: Choose randomly p weights {Wk }p
k=1 from interval [α, 1]
2: ˆβ = argminβ∈Rp Pn
|βk |
i=1 (Yi − Xiβ )2 + 2λ Pp
k=1
Wk
3: ˆS = {k | ˆβk 6= 0}

4 Theoretical analysis
We provide a theoretical analysis of TDB-Lasso, and show that under certain conditions both the
jump points and structure of VCVS can be consistently estimated. Proofs are deferred to Appendix.

4.1 Estimating jump points

,

We ﬁrst address the issue of estimating jump points by analyz ing the transformed TD-regression
problem Eq. (9) and its feature selection properties. The feature selection using ℓ1 penalization has
been analyzed intensively over the past few years and we can adapt some of the existing results to
the problem at hand. To prove that all the jump points are included in ˆJ τ , we ﬁrst state a sparse
eigenvalue condition on the design (e.g. [6]). The minimal and maximal sparse eigenvalue, for
matrix X ∈ Rn×p , are deﬁned as
||Xa||2
||Xa||2
inf
ϕmin (k , X) :=
sup
k ≤ p.
||a||2
||a||2
a∈Rp ,||a||0≤k
a∈Rp ,||a||0≤k
Note that in Eq. (11) eigenvalues are computed over submatrices of size k (i.e., due to the constraint
on a by the ||·||0 norm). We can now express the sparse eigenvalues condition on the design.
A1: Let J † be the true support of β † and J = |J † |. There exist some C > 1 and κ ≥ 10 such that
< √C /κ.
ϕmax (C J 2 , X† )
(12)
ϕ3/2
min (C J 2 , X† )
This condition guarantees a correlation structure between TD-transformed covariates that allows for
detection of the jump points. Comparing to the irrepresentible condition [30, 21, 27], necessary for
the ordinary Lasso to perform feature selection, condition A1 is much weaker [22] and is sufﬁcient
for the randomized Lasso to select the relevant feature with high probability (see also [26]).

ϕmax (k , X) :=

,

(11)

Theorem 1 Let A1 be satis ﬁed; and let the weakness α be given as α2 = νϕmin (C J 2 , X† )/(C J 2 ),
for any ν ∈ (7/κ, 1/√2). If the minimum size of the jump is bounded away from zero as
k∈J † |β †
k | ≥ 0.3(C J )3/2λmin ,
min
where λmin = 2σ † (√C J + 1)q log np
≥ V ar(Y †
and σ †2
i ), for np > 10 and J ≥ 7, there exists
n
some δ = δJ ∈ (0, 1) such that for all τ ≥ 1 − δ , the collection of the estimated jump points ˆJ τ
satis ﬁes,
P( ˆJ τ = J † ) ≥ 1 − 5/np.
(14)
Remark: Note that Theorem 1 gives conditions under which we can recover every jump point in
every covariates. In particular, there are no assumptions on the number of covariates that change
values at a jump point. Assuming that multiple covariates change their values at a jump point, we
could further relax the condition on the minimal size of a jump given in Eq. (13). It was also pointed
to us that the framework of [18] may be a more natural way to estimate jump points.

(13)

4.2

Identifying correct covariates

Now we address the issue of selecting the relevant features for every estimated segment. Under
the conditions of Theorem 1, correct jump points will be detected with probability arbitrarily close
to 1. That means under the assumption A1, we can run the regular Lasso on each of the estimated
segments to select the relevant features therein. We will assume that the mutual coherence condition
Xi , with σ j
[10] holds for each segment Bj . Let Σj = 1
kl = (Σj )k,l .
X′
|Bj | Pi∈Bj
i
5

A2: We assume there is a constant 0 < d ≤ 1 such that
k∈SBj ,l 6=k (|σ j
)! = 1.
P   max
d
kl | ≤
(cid:12)(cid:12)SBj (cid:12)(cid:12)
The assumption A2 is a mild version of the mutual coherence condition used in [7], which is neces-
sary for identi ﬁcation of the relevant covariates in each se gment. Let ˆγj , k = 1, . . . , ˆBn denote the
Lasso estimates for each segment obtained by minimizing (8).

(15)

Theorem 2 Let A2 be satis ﬁed. Also, assume that the conditions of Theor em 1 are satis ﬁed. Let
K = max1≤j≤B ||γj ||0 be the upper bound on the number of features in segments and let L be
an upper bound on elements of X. Let ρ = min1≤j≤B |Bj | denote the number of samples in the
smallest segment. Then for a sequence δ = δn → 0,
λ1 ≥ 4Lσs ln 2K p
ln 4K p
δ
δ
ρ
ρ

k∈SBj |γj,k | ≥ 2λ1 ,
min

min
1≤j≤B

∨ 8L

and

we have

lim
n→∞

P( ˆB = B ) = 1,

lim
n→∞
P(||ˆγj − γj ||1 = 0) = 1,
max
1≤j≤B
P( ˆSBj = SBj ) = 1.
min
1≤j≤B

lim
n→∞

(16)

(17)

(18)

Theorem 2 states that asymptotically, the two stage procedure estimates the correct model, i.e., it
selects the correct jump points and for each segment between two jump points it is able to select the
correct covariates. Furthermore, we can conclude that the procedure is consistent.

5 Practical considerations
As in standard Lasso, the regularization parameters in TDB-Lasso need to be tuned appropriately
to attain correct structural recovery. The TD regression procedure requires three parameters: the
penalty parameter λ2 , cut-off parameter τ , and weakness parameter α. From our empirical experi-
ence, the recovered set of jump points ˆT vary very little with respect to these parameters in a wide
range. The result of Theorem 1 is valid as long as λ2 is larger than λmin given in the statement
of the theorem. Theorem 1 in [22] gives a way to select the cutoff τ while controlling the number
of falsely included jump points. Note that this relieves users from carefully choosing the range of
parameter λ2 , which is challenging. The weakness parameter can be chosen in quite a large interval
(see Appendix on the randomized Lasso) and we report our results using the values α = 0.6.
In the second step of the algorithm, the ordinary Lasso minimizes Eq. (8) on each estimated segment
to select relevant variables, which requires a choice of the penalty parameter λ1 . We do so by
minimizing the BIC criterion [25].

In practice, one cannot verify assumptions A1 and A2 on real datasets. In cases where the assump-
tions are violated, the resulting set of estimated jump points is larger than the true set T , e.g.
the
points close to the true jump points get included into the resulting estimate ˆT . We propose to use
an ad hoc heuristic to reﬁne the initially selected set of jump points . A commonly used procedure
for estimation of linear regression models with structural changes [3] is a dynamic programming
method that considers a possible structural change at every location ti , i = 1, . . . , n, with a compu-
tational complexity of O(n2 ) (see also [15]). We modify this method to consider jump points only
in the estimated set ˆT and thus considerably reducing the computational complexity to O(| ˆT |2 ),
since | ˆT | ≪ n. The algorithm effectively chooses a subset ˜T ⊆ ˆT of size ˆB that minimizes the BIC
objective.

6 Experiments on Synthetic Data
We compared the TDB-Lasso on synthetic data with commonly used methods for estimating VCVS
models. The synthetic data was generated as follows. We varied the sample size from n = 100

6

1.5

1

0.5

0

 

/l
Kernel + l
1
2

Kernel + l
1

l
 + TV
1

TDB−Lasso

MREE

 

1

0.5

Corr. zeros

Precision

Recall

1

0.5

1

0.5

F
1

1

0.5

0

0

0

0

200
400
200
400
200
400
200
400
200
400
Sample size
Sample size
Sample size
Sample size
Sample size
Figure 2: Comparison results of different estimation procedures on a synthetic dataset.

p = 20. The block partition
to 500 time points, and ﬁxed the number of covariates is ﬁxed to
was generated randomly and consists of ten blocks with minimum length set to 10 time points. In
each of the block, only 5 covariates out of 20 affected the response. Their values were uniformly at
random drawn from [−1, −0.1] ∪ [0.1, 1]. With this conﬁguration, a dataset was created by randomly
drawing Xi ∼ N (0, Ip ), ǫi ∼ N (0, 1.52 ) and computing Yi = Xiβ (ti ) + ǫi for i = 1, . . . , n. For
each sample size, we independently generated 100 datasets and report results averaged over them.

(19)

β 2
i′ ,j ,

min
β∈Rp×n

A simple local regression method [13], which is commonly used for estimation in varying coefﬁcient
models, was used as the simplest baseline for comparing the relative performance of estimation. Our
ﬁrst competitor is an extension of the baseline, which uses t he following estimator [28]:
λjvuut
p
n
n
n
Xi=1
Xi′=1
Xj=1
Xi′=1
(Yi − X′
iβi′ )2Kh (ti′ − ti ) +
where Kh (·) = 1
h K (·/h) is the kernel function. We will call this method “Kernel ℓ1 /ℓ2 ”. Another
competitor uses the ℓ1 penalized local regression independently at each time point, which leads to
the following estimator of β (t),
p
n
Xi=1
Xj=1
(Yi − X′
iβ )2Kh (ti − t) +
λj |βj |.
We call this method “Kernel ℓ1 ”. The difference between the two methods is that “Kernel
ℓ1 /ℓ2 ”
biases certain covariates toward zero at every time point, based on global information; whereas
“Kernel ℓ1 ” biases covariates toward zero only based on local informat ion. The ﬁnal competitor is
chosen to be the minimizer of Eq. (2) [1], which we call “ ℓ1 + TV ”. The bandwidth parameter for
“Kernel ℓ1 ” and “Kernel
ℓ1/ℓ2 ” is chosen using a generalized cross validation of a non-pen alized
estimator. The penalty parameters λj are chosen according to the BIC criterion [28]. For the “ ℓ1 +
TV ” method, we optimize the BIC criterion over a two-dimension al grid of values for λ1 and λ2 .
j=1 | ˆβi,j −β∗
i=1 Pp
Pn
i,j |
, where ˜β is the baseline
We report the relative estimation error, REE = 100 ×
j=1 | ˜βi,j −β∗
i=1 Pp
Pn
i,j |
local linear estimator, as a measure of estimation accuracy. To asses the performance of the model
selection, we report precision, recall and their harmonic mean F1 measure when estimating the
relevant covariates at each time point and the percentage of correctly identi ﬁed irrelevant covariates.

min
β∈Rp

(20)

From the experimental results, summarized in Fig. 2, we can see that the TDB-Lasso succeeds in
recovering the true model as the sample size increases. It also estimates the coefﬁcient values with
better accuracy than the other methods. It worth noting that the “Kernel + ℓ1 ” performs better than
the “Kernel + ℓ1/ℓ2 ” approach, which is due to the violation of the assumptions m ade in [28]. The
“ ℓ1 + TV ” performs better than the local linear regression approach es, however, the method gets
very slow for the larger values of the sample size and it requires selecting two tuning parameters,
which makes it quite difﬁcult to use. We conjecture that the “
ℓ1 + TV ” and TDB-Lasso have similar
asymptotic properties with respect to model selection, however, from our numerical experiments we
can see that for ﬁnite sample data, the TDB-Lasso performs be tter.

7 Application to Time-varying Graph Structure Estimation
An interesting application of the TDB-Lasso is in structural estimation of time-varying undirected
graphical models [1, 17]. A graph structure estimation can be posed as a neighborhood selection

7

problem, in which neighbors of each node are estimated independently. Neighborhood selection
in the time-varying Gaussian graphical models (GGM) is equivalent to model selection in VCVS,
where value of one node is regressed to the rest of nodes. The regression problem for each node
can be solved using the TDB-Lasso. Graphs estimated in this way will have neighborhoods of each
node that are constant on a partition, but the graph as a whole changes more ﬂexibly (Fig. 1b-d).

t=1.00s

t=2.00s

t=3.00s

a
a
 
t
c
e
j
b
u
S

The graph structure estimation using the TDB-
Lasso is demonstrated on a real dataset of elec-
troencephalogram (EEG) measurements. We use
the brain computer interface (BCI) dataset IVa
from [11] in which the EEG data is collected from
5 subjects, who were given visual cues based on
which they were required to imagine right hand
or right foot for 3.5s. The measurement was per-
formed when the visual cues were presented on the
screen (280 times), intermitted by periods of random length in which the subject could relax. We
use the down-sampled data at 100Hz. Fig. 3 gives a visualization of the brain interactions over the
time of the experiment for the subject ’aa’ while presented with visual cues for the class 1 (right
hand). Estimated graphs of interactions between different parts of the brain for other subjects and
classes are given in Appendix due to the space limit.

Figure 3: Brain interactions for the subject ’aa’
when presented with visual cues of the class 1

We also want to study whether the estimated time-varying network are discriminative features for
classifying the type of imaginations in the EEG signal. For this purpose, we perform unsupervised
clustering of EEG signals using the time-varying networks and study whether the grouping corre-
spond to the true grouping according to imagination label. We estimate a time-varying GGM using
the TDB-Lasso for each visual cue and cluster the graphs using the spectral K-means clustering [29]
(using a linear kernel on the coefﬁcients to measure similar ity). Each cluster is labeled according to
the majority of points it contains. Finally, each cue if classi ﬁed based on labels of the time-points
that it contains. Table 1 summarizes the classi ﬁcation accu racy for each subject based on K = 4
clusters (K was chosen as a cutoff point, when there was little decrease in K-means objective). We
compare this approach to a case when GGMs with a static structure are estimated [5]. Note that
the supervised classi ﬁers with special EEG features are abl e to achieve much higher classi ﬁcation
accuracy, however, our approach does not use any labeled data and can be seen as an exploratory
step. We also used TDB-Lasso for estimating the time-varying gene networks from microarray data
time series data, but due to space limit, results will be reported later in a biological paper.

Table 1: Classiﬁcation accuracies based on learned brain interactions.
Subject
aa
al
av
aw
ay
0.83
0.67
0.59
0.80
0.69
TDB-Lasso
Static
0.58
0.63
0.54
0.57
0.61

8 Discussion

We have developed the TDB-Lasso procedure, a novel approach for model selection and variable es-
timation in the varying-coefﬁcient varying-structure mod els with piecewise constant functions. The
VCVS models form a ﬂexible nonparametric class of models tha t retain interpretability of parametric
models. Due to their ﬂexibility, important classical probl ems, such as linear regression with struc-
tural changes and change point detection, and some more recent problems, like structure estimation
of varying graphical models, can be modeled within this class of models. The TDB-Lasso compares
favorably to other commonly used [28] or latest [1] techniques for estimation in this class of models,
which was demonstrated on the synthetic data. The model selection properties of the TDB-Lasso,
demonstrated on the synthetic data, are also supported by the theoretical analysis. Furthermore, we
demonstrate a way of applying the TDB-Lasso for graph estimation on a real dataset.

Application of the TDB-Lasso procedure goes beyond the linear varying coefﬁcient regression mod-
els. A direct extension is to generalized varying-coefﬁcie nt models g(m(Xi , ti )) = X′
iβ (ti ), i =
1, . . . , n, where g(·) is a given link function and m(Xi , ti ) = E[Y |X = Xi , t = ti ] is the con-
ditional mean. Estimation in generalized varying-coefﬁci ent models proceeds by changing the
squared loss in Eq. (7) and Eq. (8) to a different appropriate loss function. The generalized varying-
coefﬁcient models can be used to estimate the time-varying s tructure of discrete Markov Random
Fields, again by performing the neighborhood selection.

8

References
[1] Amr Ahmed and Eric P. Xing. Tesla: Recovering time-varying networks of dependencies in social and
biological studies. Proceeding of the National Academy of Science, 2009.
[2] Francis R. Bach. Bolasso: model consistent lasso estimation through the bootstrap. In William W. Cohen,
Andrew McCallum, and Sam T. Roweis, editors, ICML, volume 307 of ACM International Conference
Proceeding Series, pages 33–40. ACM, 2008.
[3] J Bai and P Perron. Computation and analysis of multiple structural change models. Journal of Applied
Econometrics, (18):1–22, 2003.
[4] Jushan Bai and Pierre Perron. Estimating and testing linear models with multiple structural changes.
Econometrica, 66(1):47–78, January 1998.
[5] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation. J. Mach. Learn. Res., 9:485–516, 2008.
[6] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of lasso and dantzig selector. Ann. of Stat.
[7] Florentina Bunea. Honest variable selection in linear and logistic regression models via ℓ1 and ℓ1 + ℓ2
penalization. Electronic Journal of Statistics, 2:1153, 2008.
[8] Scott S. Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis pursuit.
SIAM Journal on Scientiﬁc Computing , 20(1):33–61, 1999.
[9] William S. Cleveland, Eric Grosse, and William M. Shyu. Local regression models. In John M. Chambers
and Trevor J. Hastie, editors, Statistical Models in S, pages 309–376, 1991.
[10] David L. Donoho, Michael Elad, and Vladimir N. Temlyakov. Stable recovery of sparse overcomplete
representations in the presence of noise. IEEE Trans. Inform. Theory, 52:6–18, 2006.
[11] G. Dornhege, B. Blankertz, G. Curio, and K. M ¨uller. Boosting bit rates in non-invasive EEG single-trial
classiﬁcations by feature combination and multi-class paradigms.
IEEE Trans. Biomed. Eng., 51:993–
1002, 2004.
[12] Jianqing Fan and Qiwei Yao. Nonlinear Time Series: Nonparametric and Parametric Methods. (Springer
Series in Statistics). Springer, August 2005.
[13] Jianqing Fan and Wenyang Zhang. Statistical estimation in varying-coefﬁcient models.
Statistics, 27:1491–1518, 2000.
[14] Za¨ıd Harchaoui, Francis Bach, and ´Eric Moulines. Kernel change-point analysis. In D. Koller, D. Schu-
urmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21. 2009.
[15] Za¨ıd Harchaoui and C ´eline Levy-Leduc. Catching change-points with lasso.
In J.C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 617–
624. MIT Press, Cambridge, MA, 2008.
[16] Trevor Hastie and Robert Tibshirani. Varying-coefﬁcient mode ls. Journal of the Royal Statistical Society.
Series B (Methodological), 55(4):757–796, 1993.
[17] Mladen Kolar, Le Song, and Eric Xing. Estimating time-varying networks. In arXiv:0812.5087, 2008.
[18] Marc Lavielle and Eric Moulines. Least-squares estimation of an unknown number of shifts in a time
series. Journal of Time Series Analysis, 21(1):33–59, 2000.
[19] E. Lebarbier. Detecting multiple change-points in the mean of gaussian process by model selection. Signal
Process., 85(4):717–736, 2005.
[20] E. Mammen and S. van de Geer. Locally adaptive regression splines. Ann. of Stat., 25(1):387–413, 1997.
[21] N. Meinshausen and P. B ¨uhlmann. High-dimensional graphs and variable selection with the lasso. Annals
of Statistics, 34:1436, 2006.
[22] Nicolai Meinshausen and Peter B ¨uhlmann. Stability selection. Preprint, 2008.
[23] Alessandro Rinaldo. Properties and reﬁnements of the fused lass o. Preprint, 2008.
[24] Le Song, Mladen Kolar, and Eric P. Xing. Keller: Estimating time-evolving interactions between genes.
In Proceedings of the 16th International Conference on Intelligent Systems for Molecular Biology, 2009.
[25] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness
via the fused lasso. Journal Of The Royal Statistical Society Series B, 67(1):91–108, 2005.
[26] S. A. van de Geer and P. Buhlmann. On the conditions used to prove oracle results for the lasso, 2009.
[27] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity. Preprint, 2006.
[28] H. Wang and Y. Xia. Shrinkage estimation of the varying coefﬁcient model. Manuscript, 2008.
[29] H Zha, C Ding, M Gu, X He, and H Simon. Spectral relaxation for k-means clustering. pages 1057–1064.
MIT Press, 2001.
[30] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res., 7:2541–2563, 2006.

The Annals of

9


Spatial Normalized Gamma Processes

Vinayak Rao
Gatsby Computational Neuroscience Unit
University College London
vrao@gatsby.ucl.ac.uk

Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
ywteh@gatsby.ucl.ac.uk

Abstract

Dependent Dirichlet processes (DPs) are dependent sets of random measures, each
being marginally DP distributed. They are used in Bayesian nonparametric models
when the usual exchangeability assumption does not hold. We propose a simple
and general framework to construct dependent DPs by marginalizing and nor-
malizing a single gamma process over an extended space. The result is a set of
DPs, each associated with a point in a space such that neighbouring DPs are more
dependent. We describe Markov chain Monte Carlo inference involving Gibbs
sampling and three different Metropolis-Hastings proposals to speed up conver-
gence. We report an empirical study of convergence on a synthetic dataset and
demonstrate an application of the model to topic modeling through time.

1

Introduction

Bayesian nonparametrics have recently garnered much attention in the machine learning and statis-
tics communities, due to their elegant treatment of inﬁnite dimensional objects like functions and
densities, as well as their ability to sidestep the need for model selection. The Dirichlet process (DP)
[1] is a cornerstone of Bayesian nonparametrics, and forms a basic building block for a wide variety
of extensions and generalizations, including the inﬁnite hidden Markov model [2], the hierarchical
DP [3], the inﬁnite relational model [4], adaptor grammars [5], to name just a few.
By itself, the DP is a model that assumes that data are inﬁnitely exchangeable, i.e. the ordering of
data items does not matter. This assumption is false in many situations and there has been a concerted
effort to extend the DP to more structured data. Much of this effort has focussed on deﬁning priors on
collections of dependent random probability measures. [6] expounded on the notion of dependent
DPs, that is, a dependent set of random measures that are all marginally DPs. The property of
being marginally DP here is both due to a desire to construct mathematically elegant solutions, and
also due to the fact that the DP and its implications as a statistical model, e.g. on the behaviour
of induced clusterings of data or asymptotic consistency, are well-understood. In this paper, we
propose a simple and general framework for the construction of dependent DPs on arbitrary spaces.
The idea is based on the fact that just as Dirichlet distributions can be generated by drawing a set
of independent gamma variables and normalizing, the DP can be constructed by drawing a sample
from a gamma process (ΓP) and normalizing (i.e. it is an example of a normalized random measure
[7, 8]). A ΓP is an example of a completely random measure [9]: it has the property that the random
masses it assigns to disjoint subsets are independent. Furthermore, the restriction of a ΓP to a subset
is itself a ΓP. This implies the following easy construction of a set of dependent DPs: deﬁne a ΓP
over an extended space, associate each DP with a different region of the space, and deﬁne each DP
by normalizing the restriction of the ΓP on the associated region. This produces a set of dependent
DPs, with the amount of overlap among the regions controlling the amount of dependence. We call
this model a spatial normalized gamma process (SNΓP). More generally, our construction can be
extended to normalizing restrictions of any completely random measure, and we call the resulting
dependent random measures spatial normalized random measures (SNRMs).

1

In Section 2 we brieﬂy describe the ΓP. Then we describe our construction of the SNΓP in Section 3.
We describe inference procedures based on Gibbs and Metropolis-Hastings sampling in Section 4
and report experimental results in Section 5. We conclude by discussing limitations and possible
extensions of the model as well as related work in Section 6.

2 Gamma Processes

(1)

We brieﬂy describe the gamma process (ΓP) here. A good high-level introduction can be found in
[10]. Let (Θ, Ω) be a measure space on which we would like to deﬁne a ΓP. Like the DP, realizations
of the ΓP are atomic measures with random weighted point masses. We can visualize the point
masses θ ∈ Θ and their corresponding weights w > 0 as points in a product space Θ ⊗ [0, ∞).
Consider a Poisson process over this product space with mean measure
µ(dθdw) = α(dθ)w−1 e−w dw.
i=1 since (cid:82)
Here α is a measure on the space (Θ, Ω) and is called the base measure of the ΓP. A sample from
this Poisson process will yield an inﬁnite set of atoms {θi , wi }∞
Θ⊗[0,∞) µ(dθdw) = ∞.
∞(cid:88)
A sample from the ΓP is then deﬁned as
wi δθi ∼ ΓP(α).
G =
It can be shown that the total mass G(S ) = (cid:80)∞
i=1
i=1 wi1(θi ∈ S ) of any measurable subset S ⊂ Θ is
simply gamma distributed with shape parameter α(S ), thus the natural name gamma process. Divid-
ing G by G(Θ), we get a normalized random measure—a random probability measure. Speciﬁcally,
we get a sample from the Dirichlet process DP(α):
D = G/G(Θ) ∼ DP(α).
Here we used an atypical parameterization of the DP in terms of the base measure α. The usual
(equivalent) parameters of the DP are: strength parameter α(Θ) and base distribution α/α(Θ).
Further, the DP is independent of the normalization: D⊥⊥G(Θ).
The gamma process is an example of a completely random measure [9]. This means that for mutually
disjoint measurable subsets S1 , . . . , Sn ∈ Ω the random numbers {G(S1 ), . . . , G(Sn )} are mutually
independent. Two straightforward consequences will be of importance in the rest of this paper.
G(cid:48)(cid:48) (dθ1 ) = (cid:82)
G(dθ1dθ2 ) onto Θ1 is also a ΓP with base measure α(cid:48)(cid:48) (dθ1 ) = (cid:82)
Firstly, if S ∈ Ω then the restriction G(cid:48) (dθ) = G(dθ ∩ S ) onto S is a ΓP with base measure
α(cid:48) (dθ) = α(dθ ∩ S ). Secondly, if Θ = Θ1 ⊗ Θ2 is a two dimensional space, then the projection
α(dθ1dθ2 ).
Θ2
Θ2
3 Spatial Normalized Gamma Processes

(2)

(3)

In this section we describe our proposal for constructing dependent DPs. Let (Θ, Ω) be a probability
space and T an index space. We wish to construct a set of dependent random measures over (Θ, Ω),
one Dt for each t ∈ T such that each Dt is marginally DP. Our approach is to deﬁne a gamma
process G over an extended space and let each Dt be a normalized restriction/projection of G.
Because restrictions and projections of gamma processes are also gamma processes, each Dt will
be DP distributed.
To this end, let Y be an auxiliary space and for each t ∈ T, let Yt ⊂ Y be a measurable set. For any
(cid:90)
measure µ over Θ ⊗ Y deﬁne the restricted projection µt by
µ(dθdy) = µ(dθ ⊗ Yt ).
µt (dθ) =
(4)
Yt
Note that µt is a measure over Θ for each t ∈ T. Now let α be a base measure over the product
space Θ ⊗ Y and consider a gamma process
G ∼ ΓP(α)

(5)

2

over Θ ⊗ Y. Since restrictions and projections of ΓPs are ΓPs as well, Gt will be a ΓP over Θ with
(cid:90)
base measure αt :
Yt

G(dθdy) ∼ ΓP(αt )

Gt (dθ) =

(6)

Now normalizing,

Dt = Gt/Gt (Θ) ∼ DP(αt )
(7)
We call the resulting set of dependent DPs {Dt}t∈T spatial normalized gamma processes (SNΓPs).
If the index space is continuous, {Dt}t∈T can equivalently be thought of as a measure-valued
stochastic process. The amount of dependence between Ds and Dt for s, t ∈ T is related to the
amount of overlap between Ys and Yt . Generally, the subsets Yt are deﬁned so that the closer s and
t are in T, the more overlap Ys and Yt have and as a result Ds and Dt are more dependent.

3.1 Examples
We give two examples of SNΓPs, both with index set T = R interpreted as the time line. Generaliza-
tions to higher dimensional Euclidean spaces Rn are straightforward. Let H be a base distribution
over Θ and γ > 0 be a concentration parameter.
The ﬁrst example uses Y = R as well, with the subsets being Yt = [t − L, t + L] for some ﬁxed
window length L > 0. The base measure is α(dθdy) = γH (dθ)dy/2L. In this case the measure-
(cid:90) t+L
valued stochastic process {Dt}t∈R is stationary. The base measure αt works out to be:
αt (dθ) =
γH (dθ)dy/2L = γH (dθ),
(8)
t−L
so that each Dt ∼ DP(γH ) with concentration parameter γ and base distribution H . We can
interpret this SNΓP as follows. Each atom in the overall ΓP G has a time-stamp y and a time-span
of [y − L, y + L], so that it will only appear in the DPs Dt within the window t ∈ [y − L, y + L]. As
a result, two DPs Ds and Dt will share more atoms the closer s and t are to each other, and no atoms
if |s − t| > 2L. Further, the dependence between Ds and Dt depends on |s − t| only, decreasing
with increasing |s − t| and independent if |s − t| > 2L.
The second example generalizes the ﬁrst one by allowing different atoms to have different window
lengths. Each atom now has a time-stamp y and a window length l, so that it appears in DPs in the
window [y − l, y + l]. Our auxiliary space is thus Y = R ⊗ [0, ∞), with Yt = {(y , l) : |y − t| ≤ l}
(see Figure 1). Let β (dl) be a distribution over window lengths in [0, ∞). We use the base measure
(cid:90)
(cid:90) ∞
(cid:90) t+l
α(dθdydl) = γH (dθ)dyβ (dl)/2l. The restricted projection is then
|y−t|≤l
t−l
0
so that each Dt is again simply DP(γH ). Now Ds and Dt will always be dependent with the amount
of dependence decreasing as |s − t| increases.

γH (dθ)dyβ (dl)/2l = γH (dθ)

dy/2l = γH (dθ)

αt (dθ) =

β (dl)

(9)

3.2

Interpretation as Mixtures of DPs

Even though the SNΓP as described above deﬁnes an uncountably inﬁnite number of DPs, in practice
we will only have observations at a ﬁnite number of times, say t1 , . . . , tm . We deﬁne R as the
smallest collection of disjoint regions of Y such that each Ytj is a union of subsets in R. Thus
R = {∩m
j=1Sj : Sj = Ytj or Sj = Y\Ytj , with at least one Sj = Ytj and ∩m
j=1 Sj (cid:54)= ∅}. For
1 ≤ j ≤ m let Rj be the collection of regions in R contained in Ytj , so that ∪R∈Rj = Ytj . For
each R ∈ R deﬁne
GR (dθ) = G(dθ ⊗ R)
(10)
We see that each GR is a ΓP with base measure αR (dθ) = α(dθ ⊗ R). Normalizing, DR =
Dtj (dθ) = (cid:80)
GR /GR (Θ) ∼ DP(αR ), with DR⊥⊥DR(cid:48) for distinct R, R(cid:48) ∈ R. Now
P
GR (Θ)
GR(cid:48) (Θ) DR (dθ)
R∈Rj
R(cid:48) ∈Rj

(11)

3

Figure 1: The extended space Y ⊗ L over which the overall ΓP is deﬁned in the second example. Not
shown is the Θ-space over which the DPs are deﬁned. Also not shown is the fourth dimension W
needed to deﬁne the Poisson process used to construct the ΓP. t1 , t2 , t3 ∈ Y are three times at which
observations are present. The subset Ytj corresponding to each tj is the triangular area touching tj .
The regions in R are the six areas formed by various intersections of the triangular areas.

gR ∼ Gamma(αR (Θ))
gRP
πjR =
R(cid:48) ∈Rj

so each Dtj is a mixture where each component DR is drawn independently from a DP. Further, the
mixing proportions are Dirichlet distributed and independent from the components by virtue of each
GR (Θ) being gamma distributed and independent from DR . Thus we have the following equivalent
construction for a SNΓP:
Dtj = (cid:88)
DR ∼ DP(αR )
πjRDR
R∈Rj
Note that the DPs in this construction are all deﬁned only over Θ, and references to the auxiliary
space Y and the base measure α are only used to deﬁne the individual base measures αR and the
shape parameters of the gR ’s. Figure 1 shows the regions for the second example corresponding to
observations at three times.
The mixture of DPs construction is related to the hierarchical Dirichlet process deﬁned in [11] (not
the one deﬁned by Teh et al [3]). The difference is that the parameters of the prior over the mixing
proportions exactly matches the concentration parameters of the individual DPs. A consequence of
this is that each mixture Dtj is now conveniently also a DP.

gR

for R ∈ R
for R ∈ Rj

(12)

4

Inference in the SNΓP

The mixture of DPs interpretation of the SNΓP makes sampling from the model, and consequently
inference via Markov chain Monte Carlo sampling, easy. In what follows, we describe both Gibbs
sampling and Metropolis-Hastings based updates for a hierarchical model in which the dependent
DPs act as prior distributions over a collection of inﬁnite mixture models. Formally, our observations
now lie in a measurable space (X, Σ) equipped with a set of probability measures Fθ parametrized
by θ ∈ Θ. Observation i at time tj is denoted xj i , lies in region rj i and is drawn from mixture
component parametrized by θj i . Thus to augment (12), we have
xj i ∼ Fθj i
rj i ∼ Mult({πjR : R ∈ Rj })
θj i ∼ Drj i
(13)
where rj i = R with probability πjR for each R ∈ Rj . In words, we ﬁrst pick a region rj i from the
set Rj , then a mixture component θj i , followed by drawing xj i from the mixture distribution.

4.1 Gibb Sampling

We derive a Gibbs sampler for the SNΓP where the region DPs DR are integrated out and replaced
by Chinese restaurants. Let cj i denote the index of the cluster in Drj i to which observation xj i is
assigned. We also assume that the base distribution H is conjugate to the mixture distributions Fθ
so that the cluster parameters are integrated out as well. The Gibbs sampler iteratively resamples the

4

t1LSCALE = Lt2t3Ygr

(14)

¬j i
Rc (xj i )

latent variables left: rj i ’s, cj i ’s and gR ’s. In the following, let mjRc be the number of observations
¬j i
Rc (xj i ) be the density of
from time tj assigned to cluster c in the DP DR in region R, and let f
observation xj i conditioned on the other variables currently assigned to cluster c in DR , with its
cluster parameters integrated out. We denote marginal counts with dots, for example m·Rc is the
number of observations (over all times) assigned to cluster c in region R. Superscripts ¬j i means
observation xj i is excluded.
(cid:18)
(cid:19)(cid:18)
(cid:19)
rj i and cj i are resampled together; their conditional joint probability given the other variables is:
gRP
¬j i
p(rj i = R, cj i = c|others) ∝
m
·Rc
f
¬j i
r∈Rj
m
·R· +αR (Θ)
(cid:19)
(cid:19)(cid:18)
(cid:18)
The probability of xj i joining a new cluster in region R is
gRP
p(rj i = R, cj i = cnew |others) ∝
αR (Θ)
fRcnew (xj i )
(15)
¬j i
gr
r∈Rj
·R· +αR (Θ)
m
where R ∈ Rj and c denotes the index of an existing cluster in region R. The updates of the gR ’s
(cid:19)−mj ··
(cid:18) (cid:80)
(cid:19) (cid:81)
(cid:18) (cid:81)
are more complicated as they are coupled and not of standard form:
R∈R gαR (Θ)+m·R·−1
p({gR }R∈R |others) =
e−gR
(16)
gR
R∈Rj
R
j
To sample the gR ’s we introduce auxiliary variables {Aj } to simplify the rightmost term above. In
(cid:19)−mj ··
(cid:18) (cid:80)
(cid:90) ∞
particular, using the Gamma identity
− P
Amj ··−1
R∈Rj
Γ(mj ·· )
=
gR
e
R∈Rj
j
0
q({gR }R∈R , {Aj }) ∝ (cid:89)
e−gR (cid:89)
we have that (16) is the marginal of {gR }R∈R of the distribution:
Amj ··−1
gαR (Θ)+m·R·−1
j
R
R∈R
j
gR |others ∼Gamma(αR (Θ) + m·R· , 1 + (cid:80)
Now we can Gibbs sample the gR ’s and Aj ’s:
Aj |others ∼Gamma(mj ·· , (cid:80)
gR )
R∈Rj
Here JR is the collection of indices j such that R ∈ Rj .

gRAj dAj
− P

(19)
(20)

j∈JR

Aj )

(17)

e

R∈Rj

gRAj

(18)

4.2 Metropolis-Hastings Proposals

To improve convergence and mixing of the Markov chain, we introduce three Metropolis-Hastings
(MH) proposals in addition to the Gibbs sampling updates described above. These propose non-
incremental changes in the assignment of observations to clusters and regions, allowing the Markov
chain to traverse to different modes that are hard to reach using Gibbs sampling.
The ﬁrst proposal (Algorithm 1) proceeds like the split-merge proposal of [12]. It either splits an
existing cluster in a region into two new clusters in the same region, or merges two existing clusters
in a region into a single cluster. To improve the acceptance probability, we use 5 rounds of restricted
Gibbs sampling [12].
The second proposal (Algorithm 2) seeks to move a picked cluster from one region to another.
The new region is chosen from a region neighbouring the current one (for example in Figure 1
the neigbors are the four regions diagonally neighbouring the current one). To improve acceptance
probability we also resample the gR ’s associated with the current and proposed regions. The move
can be invalid if the cluster contains an observation from a time point not associated with the new
region; in this case the move is simply rejected.
The third proposal (Algorithm 3) we considered seeks to combine into one step what would take
two steps under the previous two proposals: splitting a cluster and moving it to a new region (or the
reverse: moving a cluster into a new region and merging it with a cluster therein).

5

Algorithm 1 Split and Merge in the Same Region (MH1)
1: Let S0 be the current state of the Markov chain.
2: Pick a region R with probability proportional to m·R· and two distinct observations in R
3: Construct a launch state S (cid:48) by creating two new clusters, each containing one of the two obser-
vations, and running restricted Gibbs sampling
4: if the two observations belong to the same cluster in S0 then
Propose split: run one last round of restricted Gibbs sampling to reach the proposed state S1
5:
6: else
(cid:18)
(cid:19)
Propose merge: the proposed state S1 is the (unique) state merging the two clusters
7:
8: end if
1, p(S1 )q(S (cid:48)→S0 )
9: Accept proposed state S1 according to acceptance probability min
where
p(S0 )q(S (cid:48)→S1 )
p(S ) is the posterior probability of state S and q(S (cid:48) → S ) is the probability of proposing state
S from the launch state S (cid:48) .

Algorithm 2 Move (MH2)
1: Pick a cluster c in region R0 with probability proportional to m·R0 c
2: Pick a region R1 neighbouring R0 and propose moving c to R1
3: Propose new weights gR0 , gR1 by sampling both from (19)
4: Accept or reject the move

Algorithm 3 Split/merged Move (MH3)
1: Pick a region R0 , a cluster c contained in R, and a neighbouring region R1 with probability
proportional to the number of observations in c that cannot be assigned to a cluster in R1
2: if c contains observations than can be moved to R1 then
Propose assigning these observations to a new cluster in R1
3:
4: else
Pick a cluster from those in R1 and propose merging it into c
5:
6: end if
7: Propose new weights gR0 , gR1 by sampling from (19)
8: Accept or reject the proposal

5 Experiments

Synthetic data In the ﬁrst of our experiments, we artiﬁcially generated 60 data points at each of
5 times by sampling from a mixture of 10 Gaussians. Each component was assigned a timespan,
ranging from a single time to the entire range of ﬁve times. We modelled this data as a collection of
ﬁve DP mixture of Gaussians, with a SNΓP prior over the ﬁve dependent DPs. We used the set-up as
described in the second example. To encourage clusters to be shared across times (i.e. to avoid sim-
ilar clusters with non-overlapping timespans), we chose the distribution over window lengths β (w)
to give larger probabilities to larger timespans. Even in this simple model, Gibbs sampling alone
usually did not converge to a good optimum; remaining stuck around local maxima. Figure 2 shows
the evolution of the log-likelihood for 5 different samplers: plain Gibbs sampling, Gibbs sampling
augmented with each of MH proposals 1, 2 and 3, and ﬁnally a sampler that interleaved all three
MH samplers with Gibbs sampling. Not surprisingly, the complete sampler converged fastest, with
Gibbs sampling with MH-proposal 2 (Gibbs+MH2) performing nearly as well. Gibbs+MH1 seemed
converge no faster than just Gibbs sampling, with Gibbs+MH3 giving performance somewhere in
between. The fact that Gibbs+MH2 performs so well can be explained by the easy clustering struc-
ture of the problem, so that exploring region assignments of clusters rather than cluster assignments
of observations was the challenge faced by the sampler (note its high acceptance rate in Figure 4).
To demonstrate how the additional MH proposals help mixing, we examined how the cluster as-
signment of observations varied over iterations. At each iteration, we construct a 600 by 600 binary
matrix, with element (i, j ) being 1 if observations i and j are assigned to the same cluster.
In
Figure 3, we plot the average L1 difference between matrices at different iteration lags. Some-
what counterintuitively, Gibbs+MH1 does much better than Gibbs sampling with all MH proposals.

6

Figure 4: Acceptance rates of the MH proposals
for Gibbs+MH1+MH2+MH3 after burn-in (per-
centages).
Proposal
MH-Proposal 1
MH-Proposal 2
MH-Proposal 3

Synthetic
0.51
11.7
0.22

NIPS
0.6621
0.6548
0.0249

Figure 2: log-likelihoods (the coloured lines are
ordered at iteration 80 like the legend).

Figure 3: Dissimilarity in clustering structure vs
lag (the coloured lines are ordered like the leg-
end).

Figure 5: Evolution of the timespan of a cluster.
From top to bottom: Gibbs+MH1+MH2+MH3,
Gibbs+MH2
and
Gibbs+MH1
(pink),
Gibbs+MH3 (black) and Gibbs (magenta).

This is because the latter is simultaneously exploring the region assignment of clusters as well. In
Gibbs+MH1, clusters split and merge frequently since they stay in the same regions, causing the
cluster matrix to vary rapidly. In Gibbs+MH1+MH2+MH3, after a split the new clusters often move
into separate regions; so it takes longer before they can merge again. Nonetheless, this demonstrates
the importance of split/merge proposals like MH1 and MH3; [12] studied this in greater detail. We
next examined how well the proposals explore the region assignment of clusters. In particular, at
each step of the Markov chain, we pick the cluster with mean closest to the mean of one of the true
Gaussian mixture components, and tracked how its timespan evolved. Figure 5 shows that without
MH proposal 2, the clusters remain essentially frozen in their initial regions.

NIPS dataset For our next experiment we modelled the proceedings of the ﬁrst 13 years of NIPS.
The number of word tokens was about 2 million spread over 1740 documents, with about 13000
unique words. We used a model that involves both the SNΓP (to capture changes in topic distri-
butions across the years) and the hierarchical Dirichlet process (HDP) [3] (to capture differences
among documents). Each document is modeled using a different DP, with the DPs in year i sharing
the same base distribution Di . On top of this, we place a SNΓP (with structure given by the second
example in Section 3.1) prior on {Di }13
i=1 . Consequently, each topic is associated with a distribution
over words, and has a particular timespan. Each document in year i is a mixture over the topics
whose timespan include year i. Our model allows statistical strength to be shared in a more reﬁned
manner than the HDP. Instead of all DPs having the same base distribution, we have 13 dependent
base distributions drawn from the SNΓP. The concentration parameters of our DPs were chosen to
encourage shared topics, their magnitude chosen to produce about a 100 topics over the whole cor-
pus on average. Figure 6 shows some of the topics identiﬁed by the model and their timespans. For
inference, we used Gibbs sampling, interleaved with all three MH proposals to update the SNΓP. the
Markov chain was initialized randomly except that all clusters were assigned to the top-most region
(spanning the 13 years). We calculated per-word perplexity [3] on test documents (about half of all
documents, withheld during training). We obtained an average perplexity of 3023.4, as opposed to
about 3046.5 for the HDP.

7

0100200300400500600700800−1000−950−900−850−800−750  Gibbs+MH1MH2+MH3  Gibbs+MH2Gibbs+MH3Gibbs+MH1Gibbs123456789105001000150020002500300035004000  Gibbs+MH1Gibbs+MH1MH2+MH3  Gibbs+MH3Gibbs+MH2Gibbs050010001500200025003000350040004500500056789050010001500200025003000350040004500500068100500100015002000250030003500400045005000678Topic A

Topic B

Topic C

Topic D

Topic E

Topic F

Topic G

Topic H

Topic I

function, model, data, error, learning, probability, distribution

model, visual, ﬁgure, image, motion, object, ﬁeld

network, memory, neural, state, input, matrix, hopﬁeld

rules, rule, language, tree, representations, stress, grammar

classiﬁer, genetic, memory, classiﬁcation, tree, algorithm, data

map, brain, ﬁsh, electric, retinal, eye, tectal

recurrent, time, context, sequence, gamma, tdnn, sequences

chain, protein, region, mouse, human, markov, sequence

routing, load, projection, forecasting, shortest, demand, packet

Figure 6: Inferred topics with their timespans (the horizontal lines). In parentheses are the number
of words assigned to each topic. On the right are the top ten most probable words in the topics.

Computationally, the 3 MH steps are much cheaper than a round of Gibbs sampling. When trying to
split a large cluster (or merge 2 large clusters), MH proposal 1 can still be fairly expensive because
of the rounds of restricted Gibbs sampling. MH proposal 3 does not face this problem. However
we ﬁnd that after the burn-in period it tends to have low acceptance rate. We believe we need to
redesign MH proposal 3 to produce more intelligent splits to increase the acceptance rate. Finally,
MH-proposal 2 is the cheapest, both in terms of computation and book-keeping, and has reasonably
high acceptance rate. We ran MH-proposal 2 a hundred times between successive Gibbs sampling
updates. The acceptance rates of the MH proposals (given in Figure 4) are slightly lower than those
reported by [12], where a plain DP mixture model was applied to a simple synthetic data set, and
where split/merge acceptance rates were on the order of 1 to 5 percent.

6 Discussion

We described a conceptually simple and elegant framework for the construction of dependent DPs
based on normalized gamma processes. The resulting collection of random probability measures has
a number of useful properties: the marginal distributions are DPs and the weights of shared atoms
can vary across DPs. We developed auxiliary variable Gibbs and Metropolis-Hastings samplers for
the model and applied it to time-varying topic modelling where each topic has its own time-span.
Since [6] there has been strong interest in building dependent sets of random measures. Interestingly,
the property of each random measure being marginally DP, as originally proposed by [6], is often not
met in the literature, where dependent stochastic processes are deﬁned through shared and random
parameters [3, 14, 15, 11]. Useful dependent DPs had not been found [16] until recently, when
a ﬂurry of models were proposed [17, 18, 19, 20, 21, 22, 23]. However most of these proposals
have been deﬁned only for the real line (interpreted as the time line) and not for arbitrary spaces.
[24, 25, 26, 13] proposed a variety of spatial DPs where the atoms and weights of the DPs are
dependent through Gaussian processes. A model similar to ours was proposed recently in [23],
using the same basic idea of introducing dependencies between DPs through spatially overlapping
regions. This model differs from ours in the content of these shared regions (breaks of a stick in that
case vs a (restricted) Gamma process in ours) and the construction of the DPs (they use the stick
breaking construction of the DP, we normalize the restricted Gamma process). Consequently, the
nature of the dependencies between the DPs differ; for instance, their model cannot be interpreted
as a mixture of DPs like ours.
There are a number of interesting future directions. First, we can allow, at additional complexity, the
locations of atoms to vary using the spatial DP approach [13]. Second, more work need still be done
to improve inference in the model, e.g. using a more intelligent MH proposal 3. Third, although
we have only described spatial normalized gamma processes, it should be straightforward to extend
the approach to spatial normalized random measures [7, 8]. Finally, further investigations into the
properties of the SNΓP and its generalizations, including the nature of the dependency between DPs
and asymptotic behavior, are necessary for a complete understanding of these processes.

8

11323456789101112(60385 words)(173268 words)(98342 words)(20290 words)(7021 words)(3223 words)(2074 words)(5334 words)(780 words)topic Btopic Ctopic Dtopic Etopic Ftopic Gtopic Itopic Hscaletopic AyearReferences
[1] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1(2):209–230,
1973.
[2] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The inﬁnite hidden Markov model. In Advances in
Neural Information Processing Systems, volume 14, 2002.
[3] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the
American Statistical Association, 101(476):1566–1581, 2006.
[4] C. Kemp, J. B. Tenenbaum, T. L. Grifﬁths, T. Yamada, and N. Ueda. Learning systems of concepts with
an inﬁnite relational model. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 21,
2006.
[5] M. Johnson, T. L. Grifﬁths, and S. Goldwater. Adaptor grammars: A framework for specifying compo-
sitional nonparametric Bayesian models. In Advances in Neural Information Processing Systems, vol-
ume 19, 2007.
[6] S. MacEachern. Dependent nonparametric processes. In Proceedings of the Section on Bayesian Statisti-
cal Science. American Statistical Association, 1999.
[7] L. E. Nieto-Barajas, I. Pruenster, and S. G. Walker. Normalized random measures driven by increasing
additive processes. Annals of Statistics, 32(6):2343–2360, 2004.
[8] L. F. James, A. Lijoi, and I. Pruenster. Bayesian inference via classes of normalized random measures.
ICER Working Papers - Applied Mathematics Series 5-2005, ICER - International Centre for Economic
Research, April 2005.
[9] J. F. C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics, 21(1):59–78, 1967.
[10] J. F. C. Kingman. Poisson Processes. Oxford University Press, 1993.
[11] P. M ¨uller, F. A. Quintana, and G. Rosner. A method for combining inference across related nonparametric
Bayesian models. Journal of the Royal Statistical Society, 66:735–749, 2004.
[12] S. Jain and R. M. Neal. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process
mixture model. Technical report, Department of Statistics, University of Toronto, 2004.
[13] J. A. Duan, M. Guindani, and A. E. Gelfand. Generalized spatial Dirichlet process models. Biometrika,
94(4):809–825, 2007.
[14] A. Rodr´ıguez, D. B. Dunson, and A. E. Gelfand. The nested Dirichlet process. Technical Report 2006-19,
Institute of Statistics and Decision Sciences, Duke University, 2006.
[15] D. B. Dunson, Y. Xue, and L. Carin. The matrix stick-breaking process: Flexible Bayes meta anal-
ysis. Technical Report 07-03, Institute of Statistics and Decision Sciences, Duke University, 2007.
http://ftp.isds.duke.edu/WorkingPapers/07-03.html.
[16] N. Srebro and S. Roweis. Time-varying topic models using dependent Dirichlet processes. Technical
Report UTML-TR-2005-003, Department of Computer Science, University of Toronto, 2005.
[17] J. E. Grifﬁn and M. F. J. Steel. Order-based dependent Dirichlet processes. Journal of the American
Statistical Association, Theory and Methods, 101:179–194, 2006.
[18] J. E. Grifﬁn. The Ornstein-Uhlenbeck Dirichlet process and other time-varying processes for Bayesian
nonparametric inference. Technical report, Department of Statistics, University of Warwick, 2007.
[19] F. Caron, M. Davy, and A. Doucet. Generalized Polya urn for time-varying Dirichlet process mixtures. In
Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, volume 23, 2007.
[20] A. Ahmed and E. P. Xing. Dynamic non-parametric mixture models and the recurrent Chinese restaurant
process. In Proceedings of The Eighth SIAM International Conference on Data Mining, 2008.
[21] J. E. Grifﬁn and M. F. J. Steel. Bayesian nonparametric modelling with the Dirichlet process regression
smoother. Technical report, University of Kent and University of Warwick, 2008.
[22] J. E. Grifﬁn and M. F. J. Steel. Generalized spatial Dirichlet process models. Technical report, University
of Kent and University of Warwick, 2009.
[23] Y. Chung and D. B. Dunson. The local Dirichlet process. Annals of the Institute of Mathematical Statistics,
2009. to appear.
[24] S.N. MacEachern, A. Kottas, and A.E. Gelfand. Spatial nonparametric Bayesian models. In Proceedings
of the 2001 Joint Statistical Meetings, 2001.
[25] C. E. Rasmussen and Z. Ghahramani.
Inﬁnite mixtures of Gaussian process experts.
Neural Information Processing Systems, volume 14, 2002.
[26] A. E. Gelfand, A. Kottas, and S. N. MacEachern. Bayesian nonparametric spatial modeling with Dirichlet
process mixing. Journal of the American Statistical Association, 100(471):1021–1035, 2005.

In Advances in

9


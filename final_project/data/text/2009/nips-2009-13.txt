Manifold Regularization for SIR with Rate Root-n
Convergence

Wei Bian
School of Computer Engineering
Nanyang Technological University
Singapore, 639798
weibian@pmail.ntu.edu.sg

Dacheng Tao
School of Computer Engineering
Nanyang Technological University
Singapore, 639798
dctao@ntu.edu.sg

Abstract

In this paper, we study the manifold regularization for the Sliced Inverse Regres-
sion (SIR). The manifold regularization improves the standard SIR in two aspects:
1) it encodes the local geometry for SIR and 2) it enables SIR to deal with trans-
ductive and semi-supervised learning problems. We prove that the proposed graph
Laplacian based regularization is convergent at rate root-n. The projection direc-
tions of the regularized SIR are optimized by using a conjugate gradient method
on the Grassmann manifold. Experimental results support our theory.

1 Introduction

Sliced inverse regression (SIR) [7] was proposed for sufﬁcient dimension reduction. In a regression
setting, with the predictors X and the response Y, the sufﬁcient dimension reduction (SDR) sub-
space B is deﬁned by the conditional independency Y⊥ X| BTX. Under the assumption that the
distribution of X is elliptic symmetric [7], it has been proved that the SDR subsapce B is related
to the inverse regression curve E(X|Y). It can be estimated at least partially by a generalized eigen-
decomposition between the covariance matrix of the predictors Cov(X) and the covariance matrix of
the inverse regression curve Cov(E(X|Y)). When Y is a continuous random variable, it is discretized
by slicing its range into several slices so as to estimate E(X|Y) empirically. This procedure reﬂects
the name of SIR.
For practical applications, the elliptic symmetric assumption on P (X) in SIR cannot be fully satis-
ﬁed, because many real datasets are embedded on manifolds [1]. Therefore, SIR cannot select an
efﬁcient subspace for predicting the response Y because the local geometry of the predictors X is
ignored. Additionally, SIR only utilizes labeled (given response) data (predictors). Thus, it is valu-
able to extend SIR to deal with transductive and semi-supervised learning problems by considering
unlabelled samples.
We solve the above two problems of SIR by using the manifold regularization [2], which has been
developed to incorporate the local geometry in learning classiﬁcation or regression functions. In
this paper, we utilize it to preserve the local geometry of predictors in learning the SDR subspace
B . In addition, it helps SIR to solve transductive/semi-supervised learning problems because the
regularization encodes the marginal distribution of the unlabelled predictors.
Different regularizations for SIR have been well studied, e.g., the non-singular regularization [14],
the ridge regularization [9], and the sparse regularization [8]. However, all existing regularizations
do not encode the local geometry of the predictors. Although the localized sliced inverse regression
[12] considers the local geometry, it is heuristic and does not follow up the regularization framework.
The rest of the paper is organized as following. Section 2 presents the manifold regularization for
SIR. Section 3 proves the convergence of the new manifold regularization. We discuss the optimiza-

1

tion algorithm of the regularized SIR by using the conjugate gradient method on the Grassmann
manifold in Section 4. Section 5 presents the experimental results on synthetic and real datasets.
Section 6 concludes this paper.

2 Manifold Regularization for SIR

In the rest of the paper, we use terminologies in regression and deem classiﬁcation as regression
with the category response. Upper case letters X ∈ Rp and Y ∈ R are respectively the predictors
and the response, and lower case letters x and y are corresponding realizations. Given a sample
set, containing nl labeled samples {xi , yi }nl
i=1 and nu unlabeled samples {xi }n=nl+nu
, we seek an
i=nl+1
optimal k-dimensional subspace spanned by B = [β1 , ..., βk ] such that the response Y is predictable
with the projected predictors B T X. We also use matrix X = [x1 , x2 , ..., xn ] to denote all predictors
in the sample set.

2.1 Sliced Inverse Regression

(cid:161)
(cid:162)
Suppose the response Y is predictable with a sufﬁcient k-dimensional projection of the original
predictors X. We can consider the following regression model [7].
1 X, β T
Y = f
2 X, ..., β T
k X, ε
(1)
β T
where β ’s are linear independent projection vectors and ε is the independent noise. Given a set
of samples {xi , yi }nl
i=1 , SIR estimates the projection subspace B = [β1 , ..., βk ] via following steps:
(cid:161) ¯Xh − ¯X
(cid:162) (cid:161) ¯Xh − ¯X
(cid:162)T ;
(cid:80)
discretize Y by slicing its range into H slices; calculate the sample frequency fh of Y falling into the
h-th slice and the sample estimation of the conditional mean ¯Xh = E (X|Y = h); estimate the mean
¯X and covariance matrix Σ of predictors X; calculate the matrix Γ =
h fh
(cid:180)
(cid:179)(cid:161)
(cid:162)−1
and B is ﬁnally obtained by using the generalized eigen-decomposition Σβ = λΓβ . It can be proved
that the generalized eigen-decomposition is equivalent to the following optimization,
max
B
We refer to (2) as the objective function of SIR and thus we can impose with the manifold regular-
ization on (2).
Remark 2.1 Another way to get the objective (2) is based on the least square formulation for SIR
H(cid:88)
(cid:162)
(cid:162)T Σ−1 (cid:161) ¯Xh − ¯X − ΣBCh
(cid:161) ¯Xh − ¯X − ΣBCh
proposed in [3],
h=1
where C = [C1 , C2 , ..., Ch ] are auxiliary variables. Eliminate Ch by setting the partial derivative
∂L/∂Ch = 0, and then (2) can be obtained directly. Additionally, (2) shows that SIR could have
a similar objective as linear discriminant analysis, although they are obtained from different under-
standings of discriminative dimension reduction.

L (B , C ) =

B T ΣB

min
B

(3)

fh

trace

B T ΓB

.

(2)

2.2 Manifold Regularization for SIR

Each dimension reduction projection β can be deemed as a linear function or a mapping g(x) =
β T x. We expect to preserve the local geometry of the distribution of the predictors X while doing
mapping g(x). Suppose the predictors X are embedded on a manifold M , this can be achieved by
penalizing the gradient ∇M g along the manifold M . Because we are dealing with random variables
(cid:90)
with the distribution P (X), the following formulation can be applied,
(cid:107)∇M g(cid:107)2 dP (X).
X∈M

R =

(4)

The above formulation is different from the original manifold regularization [2]on the point that
the function g(x) is a dimension reduction mapping here while it is a classiﬁcation or regression

2

function in [2]. Usually, both the manifold and the marginal distribution of X are unknown. It has
been well studied in manifold learning, however, that the regularization (4) can be approximated by
using the associated graph Laplacian of labeled and unlabeled {xi }n=nl+nu
(cid:162)
(cid:161)−d2
.
i=1
Construct an adjacent graph for {xi }n=n1+nu
, where the pairwise edge weight (W )ij =
(cid:80)
i=1
φ ((cid:107)xi − xj (cid:107)) is deﬁned by the kernel function φ (·), e.g., the heat kernel φ (d) = exp
,
and then the associated graph Laplacian is L = D − W , where D is a diagonal matrix given
by Dii =
j Wij . Thus, the regularization in (4) can be approximated by R = gT Lg, where
g = [β T x1 , ..., β T xn ]. Furthermore, because there are k independent projections B = [β1 , ..., βk ] ,
k(cid:88)
(cid:162)
(cid:161)
we take the summation of k regularizations
GT LG
i=1

i Lgi = trace
gT

R =

(5)

where G = [g1 , ..., gk ].
In manifold learning, it is suggested to use the normalized graph Laplacian D−1/2LD−1/2 to re-
place L, or to use an equivalent constraint GT DG = I , to get a better performance [1], and the
(cid:180)
(cid:179)(cid:161)
(cid:162)−1
solution obtained by the normalized graph Laplacian is consistent with weaker conditions than
In the proposed regularized SIR, we normalize the regularization
the unnormalized one [13].
(5) as R = trace
, which is equivalent to the constraint GT DG = I .
GT DG
GT LG
(cid:179)(cid:161)
(cid:180)
(cid:162)−1
This normalization makes R invariant to scalar and rotation transformations of the projections
B = [β1 , ..., βk ], which is preferred for dimension reduction problems. By adding the regularization
(cid:180)
(cid:180)
(cid:179)(cid:161)
(cid:179)(cid:161)
to SIR (2), and substituting G = X T B , we get the regularized
R = trace
GT DG
GT LG
(cid:162)−1
(cid:162)−1
SIR
− ηtrace
B T ΣB
B T ΓB
max
S I Rr (B ) = trace
(6)
B T SB
B T QB
B
where Q = 1/n (n − 1) X LX T , S = 1/n (n − 1) XDX T , and η is the positive weighting factor.

3 Convergence of the Regularization

Different from the existing regularizations [8,9,14] for SIR, which are constructed as deterministic
terms, the manifold regularization in (6) is a random term that involves two data dependent variables
(matrices) Q and S . Therefore, it is necessary to discuss the convergence property of the proposed
manifold regularization.
It has been well proved that both Σ and Γ converge at rate root-n [7,11,15]. Therefore, the con-
vergence rate of the objective (6) depends on whether the regularization term converges at rate
root-n. Below, we prove that both the sample based estimations Q = 1/n (n − 1) X LX T and
S = 1/n (n − 1) XDX T converge to deterministic matrices at rate root-n. Note that the conver-
gence of a special case where the graph Laplacian is built by the kernel function φ (d) = 1 (d < ε)
was proved in [6]. Our proof scheme, however, is quite other than that used in [6]. Additionally, we
target a general choice of kernel φ (·) and also prove the root-n convergence rate which has not been
obtained before.
Although samples {xi }n=nl+nu
are independent, the dependency of L and D on samples makes
i=1
Q and S cannot be expanded as a summation of independent items. Therefore, it is difﬁcult to
apply the law of large numbers and the central limit theorem to prove the convergence and obtain
the corresponding convergence rate. However, we can prove them by constructing the converged
limitation and show that the variance of the sample based estimation with respect to the constructed
limitation decades at rate root-n. Throughout the results obtained in this Section, we assume the
(cid:176)(cid:176)(cid:176)(cid:180)
(cid:179)(cid:176)(cid:176)(cid:176)(cid:161)
(cid:162) (cid:161)
(cid:162)T
following conditions hold.
Conditions 3.1 For kernel function φ (d) , it satisﬁes φ (0) = 1 and |φ (d)| (cid:54) 1. For the distribution
< ∞,
of predictors P (X), the fourth order moment exists, i.e.,E
vec(xxT )
vec(xxT )
where vec() vectorizes a matrix into a column vector.

3

(7)

(8)

T2 = 1
n(n−1)

T1 = 1
n(n−1)

j = 1
Wij xixT
n

j = T1 − T2 .
Wij xixT
(cid:33)
φ ((cid:107)xi − xj (cid:107))
xixT
i

n(cid:88)
n(cid:88)
We start by splitting Q into two parts T1 and T2 ,
1
1
1
i −
(Dii − Wii ) xixT
n (n − 1) X LX T =
Q =
n (n − 1)
n (n − 1)
i (cid:54)=j
i=1

(cid:195)
(cid:33)
(cid:195)
n(cid:80)
n(cid:80)
n(cid:80)
n(cid:80)
Substituting the function φ (·) into (7), we have
(cid:195)
φ ((cid:107)xi − xj (cid:107)) − φ (0)
n(cid:80)
n(cid:80)
n(cid:80)
i = 1
xixT
n
j (cid:54)=i
i=1
j=1
i=1
φ ((cid:107)xi − xj (cid:107)) xT
j
j (cid:54)=i
i (cid:54)=j
i
(cid:161)
(cid:162)
Under the condition 3.1, the next two lemmas show the convergence of T1 and T2 , respectively.
Lemma 3.1 Let the conditional expectation ϕ (x) = E (φ ((cid:107)z − x(cid:107)) |x ), wherez and x are indepen-
(cid:180)
(cid:179)
dent and both are sampled from P (X). The E
ϕ (x) xxT
exists, and T1 in (8) converges almost
ϕ (x) xxT (cid:162)
(cid:161)
surely at rate n−1/2 , i.e.,
n−1/2
+ O
(cid:180)
(cid:179)
Lemma 3.2 Let the conditional expectation η (x) = E (φ ((cid:107)z − x(cid:107)) z |x ), where z and x are inde-
xη (x)T
pendent and both are sampled from P (X). The E
exists, and T2 in (8) converges almost
(cid:179)
(cid:180)
(cid:179)
(cid:180)
surely at rate n−1/2 , i.e.,

(cid:33)
1
n−1
.

a.s= E

1
n−1

(9)

T1

xi

.

a.s= E

T2

xη (x)T

+ O

n−1/2

.

(10)

The proofs of above two lemmas are given in Section 6. Based on Lemmas 1 and 2, we have the
(cid:179)
(cid:180)
(cid:161)
(cid:162) − E
following two theorems for the convergence of Q and S .
(cid:161)
(cid:162)
Theorem 3.1 Given the Conditions 3.1, the sample based estimation Q converges almost surely to
at rate n−1/2 , i.e., Q
a.s= E (Q) +
xη (x)T
a deterministic matrix E (Q) = E
ϕ (x) xxT
n−1/2
.
O
Proof. Because Q = T1 − T2 , the theorem is an immediate result from Lemmas 3.1 and 3.2.
(cid:161)
(cid:162)
(cid:161)
(cid:162)
(cid:161)
(cid:162)
Theorem 3.2 Given the Conditions 3.1, the sample based estimation S converges almost surely to a
n(cid:80)
(cid:80)
n(cid:80)
at rate n−1/2 , i.e., S
n−1/2
a.s= E
ϕ (x) xxT
ϕ (x) xxT
+ O
.
deterministic matrix E
(cid:33)
(cid:33)
(cid:195)
(cid:195) (cid:80)
φ ((cid:107)xi − xj (cid:107)),
n(cid:80)
n(cid:80)
n(cid:80)
Dii =
j Wij =
i=1
j=1
n(cid:80)
n(cid:80)
φ ((cid:107)xi − xj (cid:107)) + φ (0)
φ ((cid:107)xi − xj (cid:107))
i = T1 +
i =
xixT
xixT
j (cid:54)=i
j=1
i=1
i=1
(cid:161)
(cid:162)
n(cid:80)
is an unbiased estimation of Cov(X ), we have
i . Because
1
1
xixT
xixT
(n−1)
n(n−1)
i
(cid:161)
(cid:161)
(cid:162) a.s.= E
(cid:161)
(cid:162)
(cid:161)
(cid:162)
(cid:162)
i=1
i=1
n−1
Therefore, according to Lemma 3.1, we have S = T1 +
1
xixT
n(n−1)
i
i=1
. Note that here E (S ) (cid:54)= E
n−1
n−1/2
ϕ (x) xxT
+ O
O
can be asymptotically achieved when n → ∞.

, but equality

so S =

ϕ (x) xxT

DiixixT
i

1
n(n−1)

1
n(n−1)

1
n(n−1)

a.s.= O

Proof.

=

.

4 Optimization on the Grassmann Manifold

The optimization of the regularized SIR (6) is much more difﬁcult than that of the standard SIR (2),
which can be solved by the generalized eigen-decomposition. In this section, we present a conjugate

4

+ ηtrace

gradient method on the Grassmann manifold to solve (6), based on the fact it is invariant to scalar and
rotation transformations of the projection B . By exploiting the geometry of the Grassmann manifold,
the conjugate gradient algorithm converges faster than the gradient scheme in the Euclidean space.
Given a constrained optimization problem min F (A) subject to A ∈ Rp×k and AT A = I , if the
problem further satisﬁes F (A) = F (AO) for an arbitrary orthonormal matrix O , then it is called
an optimization problem deﬁned on the Grassmann manifold Gpk . By the following theorem, we
can transform (6) into its equivalent form (11) which is deﬁned on the Grassmann manifold.
Theorem 4.1 Suppose that Σ is nonsingular and given the eigen-decomposition Σ−1/2SΣ−1/2 =
(cid:182)
(cid:181)(cid:179)
(cid:180)−1
(cid:180)
(cid:179)
U ˜ΛU T , problem (6) is equivalent to
F (A) = −trace
AT ˜QA
AT ˜ΛA
AT ˜ΓA
min
(11)
AT A=I
(cid:180)
(cid:179)(cid:161)
where ˜Γ = U T Σ−1/2ΓΣ−1/2U and ˜Q = U T Σ−1/2QΣ−1/2U . Given the optimal solution A of
(cid:162)−1
(11), the optimal solution of (6) is given by B = Σ−1/2U A .
(cid:181)(cid:179)
(cid:182)
(cid:180)−1
−
Proof. Substituting B = Σ−1/2U A into (6), we have S I Rr (A) = trace
AT ˜ΓA
AT A
. Given a nonsingular Σ, B = Σ−1/2U A is an invertible variable
AT ˜QA
AT ˜ΛA
ηtrace
transform. Thus, we know that if A maximizes S I Rr (A) then B maximizes S I Rr (B ). Because
S I Rr (A) is invariant to scalar and rotation transformations, a constraint AT A = I can be added to
(6). We then get (11). This completes the proof.
To implement the conjugate gradient method on the Grassmann manifold, the gradient of F (A)
in (11) is required. According to [4], the gradient GA of F (A) on the manifold is deﬁned by
GA = ΠAFA where FA is the gradient of F (A) in the Euclidian space and ΠA = I − AAT is the
(cid:182)
(cid:181)
(cid:180)−1
(cid:179)
(cid:180)−1
(cid:179)
I − AAT (cid:162) ˜ΓA − η
(cid:161)
projection onto the tangent space at A of the manifold. In case of F (A) in (11), it is given by,
I − ˜ΛA
AT ˜ΛA
AT ˜ΛA
Next, we present the conjugate gradient method on the Grassmann manifold [4] to solve (11). The
algorithm is given by the following three steps:
• 1-D searching along the geodesic: given the current position Ak , the gradient Gk and the
Ak V cos (Σt) V T + U sin (Σt) V T (cid:162)
(cid:161)
searching direction Hk , the 1-D searching along the geodesic is given by
F (A (t)) s.t. A (t) = F
min
t
where U ΣV T is the compact SVD of Hk . Record the minimum solution tk = tmin , and Ak+1 =
A (tk ) as the starting position for next searching.
• Transporting gradient and search direction: parallel transport Gk and Hk from Ak to Ak+1 by
using

GA =

AT

ˆQA

.

(12)

(13)

(14)

τ Gk = Gk − (Ak V sin Σtk + U (I − cos Σtk )) U T Gk
τ Hk = (−Ak V sin Σtk + U cos Σtk ) ΣV T
(15)
(cid:180)
(cid:179)
• Calculating the conjugate direction: given the gradient Gk+1 at Ak+1 , the conjugate searching
(cid:161)
(cid:162)
direction is
Hk+1 = −Gk+1 + trace
(Gk+1 − τ Gk )T Gk+1
(16)
GT
/trace
k Gk
τ Hk .
0 A0 = I ) and let H0 = −G0 , and then repeat the
Initialize A0 by a random guess (subject to AT
above three steps iteratively to minimize F (A) until convergence, i.e., |F (Ak+1 ) − F (Ak )| < ε0 .
Note that, the same as the conjugate gradient method in the Euclidian space, the searching direction
Hk has to be resetting as Hk = −Gk with a period of p (n − p), i.e., the dimension of the searching
space.

5

5 Experiments

In this section, we evaluate the proposed regularized SIR on two real datasets. We show the results
of the standard SIR and the localized SIR on the same experiments for reference.

5.1 USPS Test

The USPS dataset contains 9,298 handwriting characters of digits 0 to 9. The entire USPS database
is divided into two parts, a training set is with 7,291 samples and a test set is with 2,007 samples
[5].
In our experiment, dimension reduction is ﬁrst implemented and then the nearest neighbor
rule is used for classiﬁcation. By using the 1/3 of the data in training set as labeled data and the
rest 2/3 as unlabeled data, we conduct supervised and semisupervised dimension reduction by the
following ﬁve methods: supervised training of standard SIR, the manifold regularized SIR, and the
localized SIR, and semi-supervised training of the manifold regularized SIR and the localized SIR.
Performances are evaluated on the independent testing set. Table 1 summarizes the experimental
results.
It shows that both the regularized SIR and the localized SIR [12] can achieve superior
performance to the standard SIR, and the manifold regularized SIR performs better than the localized
SIR in both the supervised and the semi-supervised training. Experimental results reﬂect that the
manifold regularized SIR is effective on exploiting the local geometry of a dataset.

Table 1: Experimental results on the USPS dataset: SIR; the manifold regularized SIR (RSIR);
the localized SIR (LSIR); semi-supervised training of the manifold regularized SIR (sRSIR); semi-
supervised training of the localized SIR (sLSIR).

Dimensionality
SIR
RSIR
sRSIR
LSIR
sLSIR

7
0.8635
0.8575
0.8685
0.8301
0.8526

9
0.8794
0.8809
0.8864
0.8421
0.8675

11
—
0.8859
0.8934
0.8535
0.8795

13
—
0.8889
0.8909
0.8724
0.8826

15
—
0.9028
0.9053
0.8789
0.8914

17
—
0.9108
0.9128
0.8949
0.8954

19
—
0.9148
0.9208
0.8989
0.9038

21
—
0.9193
0.9193
0.9003
0.9063

5.2 Transductive Visualization

In Coil-20 database [10], each object has 72 images taken from different view angles. All images
are cropped into 128×128 pixel arrays with 256 gray levels. We then reduce the size to 32×32, and
used the ﬁrst 10 objects for 2-D visualization, with randomly labeled 6 out of 72 images. Figure
1 shows the visualization results obtained by SIR, the proposed regularized SIR and the localized
SIR [12]. The ﬁgure shows that by exploiting the unlabeled data via the manifold regularization
for dimension reduction, the performance for data visualization can be signiﬁcantly improved. The
localized SIR performs better than SIR, but not as good as the regularized SIR.

Figure 1: Visualization of the ﬁrst 10 objects in Coil-20 database: from left to right, by the standard
SIR, the manifold regularized SIR, and the localized SIR.

6

-4000-3000-2000-1000010002000300040000100020003000400050006000-2000-1500-1000-500050010001500-1500-1000-50005001000-1500-1000-5000500100015002000-1500-1000-50005001000E (T1 ) = E

.

(17)

(18)

=

1
n

1
n

E

E

E

=

=

=

6 Proofs of Lemmas
(cid:161)
(cid:162)
(cid:161)
(cid:162)
(cid:161)
(cid:162)
(cid:161)
(cid:162)
Proof of Lemma 3.1 Because the kernel function φ (·) is bounded by |φ (d)| (cid:54) 1, we have
(cid:180)
(cid:179)
|ϕ (x)| = |E (φ ((cid:107)z − x(cid:107)) |x )| (cid:54) 1, which implies that E
(cid:161)
(cid:162)
ϕ (x) xxT
exists. Then, to prove
n−1/2
a.s= E
ϕ (x) xxT
, it is sufﬁcient to show that E (T1 ) = E
+ O
ϕ (x) xxT
and
T1
− (vec (E (T1 ))) (vec (E (T1 )))T = O
n−1
 1

xixT
 1
(vec (T1 )) (vec (T1 ))T
Cov (vec (T1 )) = E
.
First, because xi and xj are independent when i (cid:54)= j , it follows,
n(cid:88)
n(cid:88)
 1
xixT

φ ((cid:107)xi − xj (cid:107))
n − 1
n(cid:88)
n(cid:88)
i
n
j (cid:54)=i,j=1
i=1
E (φ ((cid:107)xi − xj (cid:107)) |xi )
n(cid:88)
n − 1
(cid:161)
ϕ (x) xxT (cid:162)
(cid:161)
(cid:162)
i
j (cid:54)=i,j=1
i=1
(cid:180)
(cid:179)
= E
i ϕ (xi )
=
xixT
(cid:162)
(cid:161)
i=1
(vec (T1 )) (vec (T1 ))T
is a summation of two terms, of which one is
Next, we show E
(cid:179)
(cid:180)
n−1
(vec(E (T1 ))) (vec(E (T1 )))T and the other is O
.
vec

T
 n(cid:88)
 vec
 n(cid:88)
(vec (T1 )) (vec (T1 ))T
1
φ ((cid:107)xi − xj (cid:107)) xixT
n2 (n − 1)2 E
(cid:179)
(cid:180)
n(cid:88)
n(cid:88)
(cid:162)(cid:162)T
(cid:162) (cid:161)
(cid:161)
i
i (cid:54)=j
i (cid:54)=j
φ ((cid:107)xi(cid:48) − xj (cid:48) (cid:107)) xi(cid:48) xT
φ ((cid:107)xi − xj (cid:107)) xixT
(cid:88)
(cid:88)
E
i(cid:48)
i
i(cid:48) (cid:54)=j (cid:48)
i (cid:54)=j
E (Φi,j,i(cid:48) ,j (cid:48) ),
(cid:162)(cid:162)T .
(cid:161)
i,j,i(cid:48) ,j (cid:48) distinct
else
φ ((cid:107)xi(cid:48) − xj (cid:48) (cid:107)) xi(cid:48) xT
φ ((cid:107)xi − xj (cid:107)) xixT
i(cid:48)
i
(cid:179)(cid:161)
(cid:162)(cid:162) (cid:161)
(cid:161)
(cid:161)
When i, j, i(cid:48) , j (cid:48) are distinct, xi ,xj ,xi(cid:48) , and xj (cid:48) are independent, we have
(cid:161)
(cid:161)
(cid:161)
(cid:161)
(cid:161)
(cid:162)(cid:162)(cid:162) (cid:161)
(cid:162)(cid:162)(cid:162)T
φ ((cid:107)xi(cid:48) − xj (cid:48) (cid:107)) xi(cid:48) xT
φ ((cid:107)xi − xj (cid:107)) xixT
E (Φi,j,i(cid:48) ,j (cid:48) ) = E
vec
vec
i(cid:48)
i
(cid:179)
(cid:180)
=
ϕ (x) xxT
ϕ (x) xxT
E
vec
E
vec
= (vec (E (T1 ))) (vec (E (T1 )))T .
(cid:80)
vec (T1 ) (vec (T1 ))T
Therefore, the ﬁrst term in E
is
E (Φi,j,i(cid:48) ,j (cid:48) ) = n(n−1)(n−2)(n−3)
n2 (n−1)2
i,j,i(cid:48) ,j (cid:48)
distinct
(cid:179)
(cid:180)
= (vec (E (T1 ))) (vec (E (T1 )))T + O
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) (cid:54)
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
(vec (T1 )) (vec (T1 ))T
, E (Φi,j,i(cid:48) ,j (cid:48) ) is bounded by a constant (matrix)
For the second term in E
(cid:88)
(cid:88)
n−1 (cid:162)
(cid:161)
M under the Conditions 3.1, and thus we have
E (Φi,j,i(cid:48) ,j (cid:48) )
else
else

(vec (E (T1 ))) (vec (E (T1 )))T
(cid:162)
(cid:161)

M = n(n − 1) (4n − 6)
n2 (n − 1)2 M = O

φ((cid:107)xi − xj (cid:107))xixT
(cid:161)
i

1
n2 (n − 1)2
1
n2 (n − 1)2

E (Φi,j,i(cid:48) ,j (cid:48) ) +
(cid:162) (cid:161)

1
n2 (n − 1)2
(cid:161)

1
n2 (n − 1)2

1
n2 (n − 1)2

where Φi,j,i(cid:48) ,j (cid:48) = vec

vec

vec

(19)

(20)

(cid:180)

(cid:162)(cid:162)T

n−1

.

. (21)

1
n2 (n−1)2

vec

7

E (T2 ) = E

(cid:179)

(22)

(23)

=

1
n

E

(cid:179)
Next, we split E

φ ((cid:107)xi − xj (cid:107)) xT
(cid:161)
j

n−1 (cid:162)
(cid:161)
Combining the above two results, we have
(cid:179)
(cid:180)
Cov (vec (T1 )) = E (vec (T1 )) (vec (T1 ))T − (vec (E (T1 ))) (vec (E (T1 )))T = O
(cid:179)
(cid:180)
(cid:161)
(cid:162)
Proof of Lemma 3.2 Similar to the proof of Lemma 3.1, E
xη (x)T
exists. Then, it is sufﬁcient
 1

 1
n−1
n(cid:88)
n(cid:88)
xη (x)T
and Cov (vec (T2 )) = O
to show that E (T2 ) = E
. First, we have
(cid:162)
 1
xi
n − 1
xi
n(cid:88)
n(cid:88)
n
j (cid:54)=i,j=1
i=1
φ ((cid:107)xi − xj (cid:107)) xT
j |xi
n(cid:88)
n − 1
j (cid:54)=i,j=1
i=1
(cid:180)
1
E (xi η (xi )) = E (xη (x)) .
=
n
(cid:180)
i=1
(vec (T2 )) (vec (T2 ))T
into two terms
vec
 vec
 n(cid:88)
 n(cid:88)
(vec (T2 )) (vec (T2 ))T
1
φ ((cid:107)xi − xj (cid:107)) xixT
 n(cid:88)
n2 (n − 1)2 E
(cid:179)(cid:161)
j
n(cid:88)
(cid:162)(cid:162) (cid:161)
(cid:161)
i (cid:54)=j
i (cid:54)=j
φ ((cid:107)xi − xj (cid:107)) xixT
(cid:88)
(cid:88)
vec
j
i(cid:48) (cid:54)=j (cid:48)
i (cid:54)=j
1
E (Ψi,j,i(cid:48) ,j (cid:48) ) +
(cid:161)
(cid:162) (cid:161)
(cid:161)
n2 (n − 1)2
i,j,i(cid:48) ,j (cid:48) distinct
else
φ ((cid:107)xi(cid:48) − xj (cid:48) (cid:107)) xi(cid:48) xT
φ ((cid:107)xi − xj (cid:107)) xixT
where Ψi,j,i(cid:48) ,j (cid:48) = vec
vec
(cid:88)
j (cid:48)
j
Following the same method used in the proof of Lemma 3.1, we have
1
E (Ψi,j,i(cid:48) ,j (cid:48) ) = (vec (E (T2 ))) (vec (E (T2 )))T + O
n2 (n − 1)2
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) (cid:54) O
(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)
(cid:88)
n−1 (cid:162)
(cid:161)
else
(cid:162)
(cid:161)
1
E (Ψi,j,i(cid:48) ,j (cid:48) )
n2 (n − 1)2
else
n−1
Therefore, we have Cov (vec (T2 )) = O

φ ((cid:107)xi(cid:48) − xj (cid:48) (cid:107)) xi(cid:48) xT
j (cid:48)
E (Ψi,j,i(cid:48) ,j (cid:48) )
(cid:162)(cid:162)T .


T
(cid:180)
(cid:162)(cid:162)T

φ ((cid:107)xi − xj (cid:107)) xixT
j
(cid:161)

1
n2 (n − 1)2
1
n2 (n − 1)2

(cid:161)

n−1 (cid:162)

E

=

=

=

(25)

(26)

.

.

E

vec

(24)

E

and

7 Conclusion

We have studied the manifold regularization for Sliced Inverse Regression (SIR). The regularized
SIR extended the original SIR in many ways, i.e., it utilizes the local geometry that is ignored
originally and enables SIR to deal with the tranductive/semisupervised learning problems. We also
discussed the statistical properties of the proposed regularization, that under mild conditions, the
manifold regularization converges at rate root-n. To solve the regularized SIR problem, we present
a conjugate gradient method conducted on the Grassmann manifold. Experiments on real datasets
validate the effectiveness of the regularized SIR.

Acknowledgments

This project was supported by the Nanyang Technological University Nanyang SUG Grant (under
project number M58020010).

8

References
[1] Belkin, M. & Niyogi, P. (2003) Laplacian eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computation, 15(6): 1373-1396.
[2] Belkin, M., Niyogi, P. & Sindhwani, V. (2006) Manifold regularization: Ageometric framework
for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 1:
1-48.
[3] Cook, R.D.(2004) Testing predictor contributions in sufﬁcient dimension reduction. Annals of
Statistics, 32: 1061-1092.
[4] Edelman, A., Arias, T.A., & Smith, S.T. (1998) The geometry of algorithms with orthogonality
constraints. SIAM J. Matrix Anal. Appl., 20(2):303-353.
[5] Hastie, T., Buja, A., & Tibshirani, R. (1995) Penalized discriminant analysis. Annals of Statistics,
2: 73-102.
[6] He, X., Deng, C., & Min, W. (2005) Statistical and computational analysis of locality preserving
projection. In 22th International Conference on Machine Learning (ICML).
[7] Li, K. (1991) Sliced inverse regression for dimension reduction (with discussion). J. Amer.
Statist. Assoc., 86:316-342.
[8] Li, L. (2007). Sparse sufﬁcient dimension reduction. Biometrika 94(3): 603-613.
[9] Li, L., & YIN, X. (2008). Sliced inverse regression with regularizations. Biometrics 64: 124-131.
[10] Nene, S.A., Nayar, S.K., & Murase, H. (1996) Columbia object image library: COIL-20. Tech-
nical Report No. CUCS-006-96, Dept. of Computer Science, Columbia University.
[11] Saracco, J. (1997). An asymptotic theory for sliced inverse regression. Comm. Statist. Theory
Methods 26: 2141-2171.
[12] Wu, Q., Mukherjee, S., & Liang, F. (2008) Localized sliced inverse regression. Advances in
neural information processing systems 20, Cambridge, MA: MIT Press.
[13] von Luxburg, U., Bousquet, O., & Belkin, M. (2005) Limits of spectral clustering. In L. K.
Saul, Y. Weiss and L. Bottou (Eds.), Advances in neural information processing systems 17,
Cambridge, MA: MIT Press.
[14] Zhong, W., Zeng, P., Ma, P., Liu, J. S., & Zhu, Y. (2005) RSIR: Regularized sliced inverse
regression for motif discovery. Bioinformatics 21: 4169-4175.
[15] Zhu, L.X., & NG, K.W. (1995) Asymptotics of sliced inverse regression. Statistica Sinica 5:
727-736.

9


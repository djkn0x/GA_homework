Classiﬁcation of Amazon Reviews

Mayank Agarwal, Maneesh Bhand

December 11, 2009

1

Introduction

Product reviews are a valuable source of information when it comes to making online purchases.
However, some reviews are more helpful than others. The goal of this pro ject is to predict the
usefulness of a product review. Our corpus is a set of online reviews taken from amazon.com
(http://www.amazon.com). Given the title of the review, the author’s name and location, the
author’s rating of the product on a scale of 1 - 5, and the text of the review, we want to predict
the percentage of users who ﬁnd the review helpful.

2 Problem Statement
This problem is a supervised learning classiﬁcation problem. We are given an input vector x ∈ Rn
which can consist of various things, such as the summary text, review text, etc, and our output
vector is the percentage of people who ﬁnd the review helpful, y ∈ [0, 1]. Thus, the goal is to learn
the mapping h : Rn → [0, 1]
There are two ways of approaching this problem: one way is to treat it as a classiﬁcation prob-
lem by discretizing the percentage we wish to predict into bins. This allows us to use classiﬁcation
algorithms like Naive Bayes and SVM. The other strategy is to treat the percentage as a contin-
uous value and use regression algorithms to predict its value. We explored both strategies and
implemented various algorithms in order to ﬁnd a good model for the data set.

3 Methods and Results

3.1 Multinomial model

The ﬁrst model we implemented was the multinomial event model, as described in the lecture notes
(cid:110)
(cid:111)
[3]. If we divide the interval [0, 1] in K bins, then the rth interval corresponds to
y | r − 1
Ir =
≤ y ≤ r
K
K
The parameters of our model are φr (y) = prob(y ∈ Ir ) and φi|y∈Ir = prob(xj = i|y ∈ Ir ). The
goal is to choose these parameters so as to maximize the log-likelihood estimates of the data.
(cid:80)m
We applied Laplace smoothing to the parameters so that the ﬁnal expression is
i=1 1(y (i) ∈ Ir )
m

φr (y) =

(1)

1

φk|y∈Ir =

(2)

(cid:80)m
i=1

(cid:80)ni
(cid:80)m
j=1 1{x(i)
j = k ∧ y (i) ∈ Ir } + α
i=1 1{y (i) ∈ Ir }ni + α|V |
Here α is the Laplace coeﬃcient which we choose as 1 for our ﬁrst run, |V | denotes the length
of our dictionary which was constructed by accruding all the distinct words in the training reviews,
m denotes the total number of reviews, and ni denotes the length of the text of the review. We
divided the corpus into roughly 70% training data and 30% test data. We removed all the reviews
where the number of people who voted whether they found the review helpful was less than 10.
The results are tabulated below:

Number of Bins Train Error % Test Error %
2
16.55
28.89
48.76
28.125
3
4
31.00
57.83

Next, we removed all the low frequency words and extreme high frequency words such as ‘of ’
and ‘the’, since we believed these words to be “content-free” and not indicative of the quality of
the review. Originally, we had 90,000 words in our dictionary, which was truncated to 15,000.
Unfortunately, however, as the results below depict, the error actually worsened.

Number of Bins Train Error % Test Error %
33.98
26.55
2
52.51
39.73
3
4
44.76
63.09

3.2 Stemming and Removal of Stop Words

We realized that removing high frequency words is not necessarily the right thing to do because
it removes words such as ‘good’ and ‘worst’ which are very relevant to our classiﬁcation. So we
got a list of stop words from the course taught by Prof. Chris Manning and removed only those
words. This list of stop words had around 550 words and consisted of content free words such as
‘and’, ‘of ’ etc. Next, we applied a standard stemming algorithm, due to Porter[1], to our dataset.
The length of the dictionary after the above two steps reduced to about 66000 words. We achieved
signiﬁcant improvement in the results. The table below summarizes the result achieved:

Number of Bins Train Error % Test Error %
21.89
19.92
2
3
31.30
38.18
52.44
38.43
4

3.3 Changing the smoothing parameter

We tried changing the smoothing parameter in the laplace smoothing equation (2) above. However,
it did not make a noticable diﬀerence to the output error. By decreasing the Laplace coeﬃcient,
the training error increased while the testing error decreased.

2

3.4 Bigrams and Trigrams

Next, we included all the bigrams and trigrams of the words in our dataset to our model. The size
of our dictionary increased to about 4 mil lion words. We then applied all the previous methods
on this extended model and Multinomial Naive Bayes stood out. The results below indicate that
the train error was close to 0% which means that the model was able to learn the training set
with remarkable accuracy. The impact on testing error was signiﬁcant but not as dramatic as the
training error.

Number of Bins Train Error % Test Error %
2
0.42
20.34
36.90
1.22
3
4
2.24
48.83

The very low training error indicates that some overﬁtting may have occurred. A sample of
n -grams the model found most indicative of a helpful or unhelpful review is below. Looking at the
n -grams which were most indicative of helpfulness, some phrases, like ”arrived in” and ”shipment”,
make sense as useful features (as some customers complain about shipping problems they expe-
rienced instead of reviewing the product). Other phrases, like ”man of babylon” and ”benedict
xvi”, seem to be speciﬁc to certain products or reviews and may not generalize well. Nevertheless,
perhaps because the corpus is so large, all these review-speciﬁc features, when aggregated, produce
a model that does generalize to other, similar review data.

Indicative of Helpfulness
benedict xvi
peter schiﬀ
liesels
a lifestyle
hard things
days of
perfume
rate of

Indicative of Unhelpfulness
man of babylon
money of this
her kids
condition
with oprah
zombies
ready the
in great condition

3.5 Looking at Other Features

There is other information contained in the reviews besides the words in the text; the length of
the review and usage of capitalization/punctuation can also be used as features in predicting the
usefulness of a review. Longer reviews might be more likely to be helpful, for example, while
reviews written entirely in upper case might not be as helpful.
We used a set of features that included the word count, average word length, count of words
beginning with a capital letter, count of words entirely in uppercase, and counts of various punc-
tuation marks.
In order to use these features in the context of a multivariate Bernoulli model, the feature
vectors, which contained continuous values, were discretized by assignment into bins. Then, each
feature in the vector was treated as a “word” in the review, and the modeled probability of a review
was adjusted accordingly. The results are below:

3

Number of Bins Train Error % Test Error %
35.90
16.51
2
3
18.84
37.18
56.99
22.79
4

Although some of the features selected, such as review length, did provide information on the
distributions of reviews, the performance of the classiﬁer did not improve when given these extra
features; training error decreased, while testing error increased, indicating that the inclusion of
extra features led to overﬁtting.
One possible explanation for the lack of improvement is the discretization procedure; if values
are assigned to a small number of bins, a lot of detail is lost, whereas if values are assigned to a large
number of bins, the bins are sparsely populated and not representative of the true distribution.
Another explanation is that simply treating the features as additional words is not an eﬀective
way of integrating them into a Naive Bayes model. A diﬀerent approach to combining feature
probabilities with word probabilities is needed.

3.6 Logistic Regression

In order to explore predicting the percentage as a continuous value, we implemented a logistic
regression model based on just the non-word features. The predicted values were placed into bins
and evaluated, so as to make the results comparable to those of the discrete classiﬁers:

Number of Bins Train Error % Test Error %
45.35
34.69
2
3
48.90
56.78
66.27
59.70
4

From the results, it is clear that the non-word features are not nearly as eﬀective as n -grams
of the review text in predicting usefulness. This is a somewhat surprising result, as one might
expect features such as capitalization and punctuation to impact the legibility of a review and,
consequently, its perceived usefulness.

3.7 Locally-weighted Linear Regression

We also attempted locally-weighted linear regression [3]. However, in a large corpus such as this one,
the non-parametric nature of the algorithm makes computation prohibitively expensive. Therefore,
we used k -means clustering on the training data in order to produce a smaller, representative set
of examples, and then performed locally-weighted linear regression on these clusters. In order to
obtain a reasonable measure of the distance between two feature vectors, we mapped each feature
to a normal distribution and measured the distance between feature z-values.
However, even at low bandwidths, predicted values almost always fell into the 60-70% range,
which is close to the average helpfulness of a review. This suggests that the non-word features
alone were not suﬃcient for describing the helpfulness of a review, or that the clustering process
removed too much information from the training data.

3.8 Other methods

We tried various other classiﬁcation methods on our dataset as well. We used the WEKA software
package to try SVM, Max-Ent, and the K-Star clustering algorithm. We also implemented the

4

Complementary Naive Bayes model, as described in [2]. However, none of these classiﬁers produced
signiﬁcant improvements in the classiﬁer accuracy. The multinomial Naive Bayes model, using n -
grams of the review text, remains, by far, the most simple and accurate model for our problem.

4 Conclusions

Various algorithms as described above were tried on the dataset. It is surprising that the multi-
nomial model performed the best among the lot of models that we tried. The performance of the
Naive Bayes model improved by applying Porter’s stemming algorithm, removing stop words, and
including bigrams and trigrams of the review text. We looked at some non-word features, and found
that, while a few features, such as review length, did correlate with review quality, most non-word
features were not eﬀective in predicting review usefulness. Another observation is that the classiﬁer
is relatively good at identifying the very helpful and very unhelpful reviews; the confusion matrix
indicates that the ma jority of the error is in the middle - with the classiﬁer sometimes mistakenly
predicting a neighboring bin.
The problem of predicting review usefulness is somewhat diﬀerent from more standard text
classiﬁcation problems like spam ﬁltering. Because the helpfulness of a review is a very sub jective
quantity, conventional models such as the ones described in this paper were unable to capture all
of the variance in the data. For example, in the case of two bins, it is very diﬃcult to get the
classiﬁer accuracy more than 80%,without overﬁtting the model.
Overall, the model is reasonably able to predict the usefulness of a review. This work is
useful because it can be directly applied in a variety of ways; for example, online markets like
www.amazon.com can use it to automatically serve better reviews to its customers, or to ﬂag
potentially unhelpful reviews for moderation.

5 Acknowledgements

This pro ject is the idea of Prof. Chris Potts; we are grateful to him for providing us with the
corpus and some code to process it.

6 References

[1] C. J. van Rijsbergen, S.E. Robertson and M.F. Porter, 1980. New models in probabilistic
information retrieval. London: British Library
[2] Tackling the Poor Assumptions of Naive Bayes Text Classiﬁers, Jason Rennie et al, Proceedings
of the Twentieth International Conference on Machine Learning (ICML-2003), Washington DC,
2003
[3] Andrew Ng. CS 229 Lecture Notes: Supervised Learning, Discriminative Algorithms

5


Unsupervised Clustering with Axis-Aligned Rectangular Regions

Sung Hee Park∗
Stanford University

Jae-Young Kim†
Stanford University

(a) Input data

(b) Clustering with three rectangular regions

Figure 1: An example of input and output of our clustering method

Abstract

This project presents a new method for binary clustering that clas-
siﬁes input data into two regions separated by axis-aligned rectan-
gular boundaries. Given the number of rectangular regions to use,
this algorithm automatically ﬁnds the best boundaries that are de-
termined concurrently. The formulation of the optimization prob-
lem involves minimizing the sum of minimum functions. To solve
this problem, we introduce underestimate of the minimum function
with piecewise linear and convex envelope, which results in MILP
(Mixed Integer and Linear Programming). We show several results
of our algorithm and compare the effects of each term in our objec-
tive function. Finally, we demonstrate that our method can be used
in image capturing application to determine the optimal scheme that
minimize the total readout time of pixel data.

Keywords:
machine learning, unsupervised clustering, axis-
aligned rectangular regions, mixed integer and linear programming

1 Introduction

The main goal of clustering problem is to ﬁgure out an efﬁcient al-
gorithm to classify random data into several groups so that it can be
easily handled in more complex processing stage afterwards. Each
clustering algorithm is designed to handle data in a systematic way,
based on the assumptions it makes on its input data. In this project,
we would like to propose an unsupervised clustering method that
classiﬁes binary labeled input data into two separate regions. The
main difference from other existing clustering methods is that we
choose multiple rectangular regions to separate data. Each rectan-
gular region is axis-aligned which means that its sides are aligned
to the axes of orthogonal coordinate system and it is not allowed to
rotate to arbitrary angles.

To make the clustering algorithm applicable to real world applica-
tions, we propose four desirable characteristics that our clustering
algorithm should have.

Unsupervised clustering.
It is desired that clustering algorithm
should work by itself. Users can not always guide clustering if the

∗ e-mail: shpark7@stanford.edu
† e-mail: jykim1@stanford.edu

size of input data is large or clustering should be done many times
iteratively.

Optimality. Rather than using heuristic methods, solving the opti-
mization problem guarantees to have the best results. Also, it can
be served as a standard reference case to evaluate other variant clus-
tering methods.

Robustness to outliers. The real data tend to be noisy and include
many outliers. The algorithm should be robust enough to handle
outliers. Also, it is very common to get non-separable input data
and error terms should be handled properly.

Adjustable objective function. The method should be easily mod-
iﬁed so that it can ﬁt into various applications with different con-
straints. In addition, having explicit trade-off parameters will help
users to tweak the algorithm to work well on the problem.

In the following sections, we will explain details about our algo-
rithm and demonstrate it satis ﬁes the properties stated above. Fi-
nally, we will talk about the application for image capturing that is
utilized by our clustering method.

2 Clustering as an Optimization Problem

Input data is given with binary labels on the 2-dimensional plane.
We want to formulate an optimization problem to classify binary la-
beled data with axis-aligned rectangles efﬁciently. Figure 1 shows
one example when three rectangles are used to classify the given
input data. In general, input data may not be separable with rect-
angular classiﬁcation as in the Figure 1. Therefore, our goal is to
maximize the number of interesting points classiﬁed in the rectan-
gles, while keep the number of unwanted points as small as possible
in them. In the following sections, we will deﬁne several penalty
functions for misclassiﬁcation and formulate an optimization prob-
lem for the case with one rectangular boundary. Then we will ex-
tend our framework to handle multiple rectangles at the same time.

2.1 Classiﬁcation Error for One Rectangular Region

We would like to deﬁne two types of classiﬁcation error. Type A er-
ror quantiﬁes the amount of violation when unwanted points are lo-
cated inside the rectangle. Similarly, type B error is incurred when

volved in our objective, which are concave. This term prevents us to
directly apply convex optimization techniques. Thus we use some
trick to formulate the problem as a MILP which will be described
in Section 2.3.

2.2 Classiﬁcation Error for Multiple Rectangular Re-
gions

Let’s deﬁne a variable L as the number of rectangles. Then every
terms we deﬁned at the previous section can be used with index l to
represent they are the terms regarding lth rectangular region. Thus
we can write y l
ik and z l
jk as

kAi − γ l
ik = max{ω l
y l
k , 0}
z l
kBi + γ l
jk = max{−ω l
k , 0}
y l
i = min
k

yik

z l
j =

4
Xk=1

zjk

If we consider all L rectangles, then we have non-zero type A error
when Ai point is enclosed by any rectangular boundary. So, we
i as shown in Figure
can represent type A error as yi = maxl y l
3. Likewise, type B points are located outside of all rectangles to
have non-zero misclassiﬁcation. Therefore, we can formulate it as
j . Figure 3 shows type A and B errors when we
zj = minl z l
use two rectangles. As a result, for the case with L rectangular
boundaries, our optimization problem becomes:

minγ ,y,z

s.t.

where

max
l

min
k

{y l
ik } +

ma
Xi=1
kAi − γ l
ik ≥ ω l
y l
k
z l
jk ≥ −ω l
kBj + γ l
k
y l
ik ≥ 0 i = 1, . . . , ma , ∀l, ∀k
z l
jk ≥ 0 j = 1, . . . , mb , ∀l, ∀k
K = 4, ω l = (cid:18) 0
−1

0 −1
0
1

mb
4
jk o
l n
Xj=1
Xk=1
z l
min
i = 1, . . . , ma , ∀l, ∀k

j = 1, . . . , mb , ∀l, ∀k

0 (cid:19)
1

2.3 MILP Formulation using Integer Variables

As noted at the beginning of this section, our objective function
is not a convex function. To reformulate the problem as a convex
form, we introduce new variables. Let’s consider type B error term
ﬁrst.

tl
j = 1

v l
j =z l
j tl
j where tl
j is 0-1 integer
L
Xl=1
j are integer variables that can have values 0 and 1 only. Also,
tl
since the sum of L variables is one, only one tl
j will have the value
of 1 while other L − 1 terms are zero. Here, we want to make
tl′
j = 1 where l′ = argminl z l
j so that we have

L
Xl=1

v l
j = min
l

l n
z l
j = min

4
Xk=1

jk o.
z l

Figure 2: Classi ﬁcation error for one rectangular region.

Figure 3: Classi ﬁcation error for multiple rectangular regions.

our target points are not included in the rectangle. In Figure 2, yi
is type A error of ith data point with label A and zj represents type
B error of j th point with label B. If we deﬁne a rectangle as a hy-
perplane set {ωk x = γk , k = 1, . . . , 4}, type A and B error can be
formulated as follows:

yik = max{ωkAi − γk , 0}
zjk = max{−ωkBj + γk , 0}
i = 1, . . . , ma , j = 1, . . . , mb , k = 1, . . . , K
0 (cid:19)
For K = 4, ω = (cid:18) 0
1
−1
where Ai is ith input data with label A, Bj is j th input data with
label B, ma is the number of input data with label A and mb is the
number of input data with label B.

0 −1
0
1

yik is an error incurred by ith data from kth side of the rectangle
and zjk is an error by j th data from kth side of the rectangle. When
a type A point is in the rectangle, then all the yik are positive. Thus,
we deﬁne type A error of
ith points as yi = mink yik . On the con-
trary, type B error happens when type B points have at least one
positive value of zik . Type B error can be set as zj = P4
k=1 zjk .
Then our goal is to minimize the sum of two error terms over all
input points. However, we have the sum of minimum functions in-

To enforce the relation, let’s introduce underestimate of minimum
function by piecewise linear and convex envelope as done in [Ryoo
2006; Ryoo and Sahinidis 2001].

v l
j ≥ max{z l
j + M tl
j − M , 0} j = 1, . . . , mb , ∀l
where M is an arbitrary positive number that is greater than the
maximum value of z l
j . We can see that v l
j will have non-zero value
only when tl
j to
j = 1 and the optimization process will minimize v l
have the minimum of z l
j for each j . As a result, the terms relevant
jk in the objective function can be formulated into:
to z l

mb
mb
L
4
jk o =
l n
Xj=1
Xj
Xk=1
Xl
z l
min
Now objective function becomes linear. We can apply the same
ik . Let’s deﬁne the following
linearization trick to type A error y l
variable ui .

v l
j

ul
i = min
k

{y l
ik }

ul
ui = max
i = max
min
l
l
k
Then, we can redeﬁne ui using integer variables and the underesti-
mate as follows:

{y l
ik }

ik where sl
ul
ik sl
ik = y l
ik is 0-1 integer
ik + M sl
ul
ik ≥ max{y l
ik − M , 0}
4
Xk=1
sl
ik = 1 i = 1, . . . , ma , ∀l

i = 1, . . . , ma , ∀l

ui ≥ ul
i =

ul
ik

Xk
Consequently, our objective function for y l
ik becomes:
ma
ma
Xi=1
Xi=1
Combining all together, our ﬁnal convex optimization problem us-
ing integer variables becomes:

{y l
ik } =

max
l

min
k

ui

minu,v,y,z ,t,s C1

s.t.

v l
j

ui + C2

mb
ma
L
Xi=1
Xj=1
Xl=1
j + M tl
v l
j ≥ z l
j − M j = 1, . . . , mb , ∀l
ik + M sl
ul
ik ≥ y l
ik − M i = 1, . . . , ma , ∀l, ∀k
4
Xk=1

i = 1, . . . , ma , ∀l

ui ≥

ul
ik

ik = 1 i = 1, . . . , ma , sl
sl
ik ∈ {0, 1}, ∀l

i = 1, . . . , ma , ∀l, ∀k

j = 1, . . . , mb , ∀l, ∀k

j = 1 j = 1, . . . , mb , tl
tl
j ∈ {0, 1}

L
Xl=1
4
Xk=1
y l
ik ≥ ω l
kAi − γ l
k
z l
jk ≥ −ω l
kBj + γ l
k
y l
ik ≥ 0 i = 1, . . . , ma , ∀l, ∀k
z l
jk ≥ 0 j = 1, . . . , mb , ∀l, ∀k
v l
j ≥ 0 j = 1, . . . , mb , ∀l
ul
ik ≥ 0 i = 1, . . . , ma , ∀l, ∀k

where C1 and C2 are weighting parameters.
If we increase C2
weight by ﬁxing C1 , then we consider type B errors are more im-
portant and optimization will try more to reduce type B classiﬁca-
tion error. On the contrary, if C1 is increasing while C2 is ﬁxed,
then optimization will focus more on trying to exclude Bj points.

2.4 Applying Additional Geometry Constraints

Since our problem formulation involves only linear constraints, it is
straightforward to add additional geometry constraints to determine
the shape of rectangular regions. First, we can add quadratic terms
in the objective function to control the dimensions of the rectangles.

min
u,v,y,z ,t,s

ui + C2

C1

ma
Xi=1
1 + γ l
hl = γ l
2
3 + γ l
wl = γ l
4

mb
Xj=1

L
Xl=1

v l
j + C3

L
Xl=1

{(wl )2 + (hl )2 }

hl and wl correspond to the height and width of lth rectangle. Thus,
the third term will make tighter rectangular bounds and user can
control C3 to change the strength of the effect. This term also has
the effect of minimizing the area of the rectangles because it is min-
imizing the upper bound of the area. This relation come from the
following inequality.

l + w2
hlwl ≤ 0.5(h2
l )

Moreover, we can specify the aspect ratio of the rectangle or ex-
actly ﬁx the dimension of the rectangles by adding the following
constraints.

wl = αhl
wl = α1 , hl = α2

3 Clustering Results

3.1 Clustering Results

We applied our algorithm to a test data set with 64 type A points
and 40 type B points. We used C1=1, C2 = 10, C3 = 0.1 as weight-
ing terms. AMPL/CPLEX is used to solve MILP. Figure 4 shows
the visualization of the results we obtained. When no additional
constraint is applied, the optimizer ﬁnds arbitrary axis-aligned rect-
angles to minimize objective function. This case gives the tightest
rectangles that ﬁt input data as shown in Figure 4(a). If we give
ﬁxed aspect ratio constraints, 4:3 in this example, all boundaries
has the same shape but different scales. For L=2 case in Figure
4(b), we can see that rectangular regions are less tight than the case
for arbitrary rectangles, but it still tries to ﬁnd the best ones with
same aspect ratio. Figure 4(c) is obtained by locating ﬁxed size
rectangles to reduce error terms. Since the area of each rectangle is
the same, the third term in objective function are not taken into ac-
count. The solver places ﬁxed-size rectangles in the best locations.
The result looks quite compelling, which conﬁrms the correctness
of our problem formulation.

3.2 The Effects of the Weighting Terms

Here, we brieﬂy show how the results will change depending on
the weighting terms in the objective function. First, we compare
the effect of ﬁrst two terms of the objective. We set C1=1 and use
C2=0.1, 1, 10. The red rectangle in Figure 5(a) corresponds to
C2=10 case which has the largest area among three. It tries to min-
imize misclassiﬁcation of type B data and the rectangular region

(a) Clustering with arbitrary size rectangles

(b) Clustering with rectangles with 4:3 aspect ratio

(c) Clustering with ﬁxed size rectangles

Figure 4: Clustering results with different constraints for L=1, 2,
3. (C1=1, C2=10, C3=0.1)

(a) Weights on type A and B mis-
classiﬁcation

(b) Weights on the dimensions of a
rectangle

Figure 5: The effects of changing weight terms in the objective
function.

includes all type B points. On the other hands, the dark color rect-
angle, when C2=0.1, makes correct decision for all type A points,
forcing them to be located outside the rectangle. Figure 5(b) shows
the effect of the third term in the objective. Let C1=1, C2=10 and
C3=0.1, 1, 10, 100. As C3 gets bigger, the resulting rectangular re-
gion becomes smaller. Bigger C3 is represented as darker color in
Figure 5(b). This experiment shows that each term in the objective
function plays an important role in deciding the best classiﬁcation
result and users can tweak the weighting parameters to obtain the
boundaries they want.

4 Image Capturing Application

In this section, we will talk about a novel way of capturing image
data using our algorithm. The readout speed of imaging system is
often bandwidth-limited and it becomes a bottleneck for increasing
capturing frame rates. For the limited amount of pixel budget you
have at a certain amount of time, it will be better if we can only cap-
ture image data that we are more interested in, rather than capturing
every pixel from whole scene. Thus, we can adaptively design an
efﬁcient image readout scheme by determining which information
is more interesting and how to capture that data efﬁciently. Since

(a) Original

(b) L=1

(c) L=2

(d) L=3

Figure 6: Results of clustering applied on real image data

most image sensors are restricted to readout only rectangular win-
dows from sensor plane, determining how to capture data becomes
a problem of deciding rectangular regions that efﬁciently incorpo-
rate important pixel data. Thus, once you determined which regions
are more important, the next step can be solved by applying the al-
gorithm discussed in this project.

For instance, let’s say you cycle through multiple exposure levels by
changing parameter settings every frame to capture high dynamic
range video. If over-exposed or under-exposed regions are clumped
in small regions, you will want to capture only badly exposed re-
gions again since most parts of image are already well-captured by
mid-exposure frames and you don’t want to waste your pixel bud-
get to capture them again. In this case, badly exposed pixels are
points you want to put inside rectangular regions in next capture,
while the rest of the area is supposed to be at outside of rectangles.
This case is shown in Figure 6. Here, we want to set the regions
so that we can capture most of over-exposed area while minimiz-
ing total readout time. We deﬁned interesting pixels as the ones
with value higher than the threshold, which means that they are sat-
urated. A mask is generated to use it as an input data set for the
algorithm. Figure 6(b) through 6(d) show the results applied on
86 x 128 image for the cases of one, two and three rectangular re-
gions. The total readout time for each case will be determined by
actual image sensor characteristics. The total readout time is the
sum of pixel readout time and overhead time. Pixel readout time is
proportional to the sum of area of all rectangular regions and over-
head time grows as the number of rectangles increases. Thus, if the
sensor has very small overhead time, then having more number of
rectangular regions will result in less total readout time. However,
if overhead time is big relative to pixel readout time, using one or
two rectangles will be optimal.

5 Conclusion

In this project, we have proposed an unsupervised clustering algo-
rithm that classiﬁes binary labeled data into two regions separated
by multiple axis-aligned rectangles. By formulating the problem as
a mixed integer and linear programming, we are able to determine
the boundaries of multiple rectangles at the same time. It gives sat-
isfactory results and easily extended to problems with additional
constraints such as enforcing rectangles to have speciﬁc aspect ra-
tio or ﬁxed sizes. We also demonstrated that this method can be
applied to real image data to come up with an optimal image cap-
turing scheme.

We believe that it will be very useful to extend this method to be
applied on larger data sets. Rather than using all input data samples,
ﬁnding a better representation of data by pre-clustering can help
reducing the amount of computation required. Using hierarchical
approach with some heuristics may give reasonable results in much
less time. Also, our framework will serve as a good reference in
evaluating various heuristic approaches.

References

RYOO , H . S . , AND SAH IN ID I S , N . V. 2001. Analysis of bounds for
multilinear functions. J. of Global Optimization 19, 4, 403–424.

RYOO , H . S . 2006. Pattern classiﬁcation by concurrently deter-
mined piecewise linear and convex discriminant functions. Com-
put. Ind. Eng. 51, 1, 79–89.


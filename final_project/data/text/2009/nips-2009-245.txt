Dirichlet-Bernoulli Alignment: A Generative Model
for Multi-Class Multi-Label Multi-Instance Corpora

Shuang-Hong Yang
College of Computing
Georgia Tech
shy@gatech.edu

Hongyuan Zha
College of Computing
Georgia Tech
zha@cc.gatech.edu

Bao-Gang Hu
NLPR & LIAMA
Chinese Academy of Sciences
hubg@nlpr.ia.ac.cn

Abstract

We propose Dirichlet-Bernoulli Alignment (DBA), a generative model for cor-
pora in which each pattern (e.g., a document) contains a set of instances (e.g.,
paragraphs in the document) and belongs to multiple classes. By casting prede-
ﬁned classes as latent Dirichlet variables (i.e., instance level labels), and modeling
the multi-label of each pattern as Bernoulli variables conditioned on the weighted
empirical average of topic assignments, DBA automatically aligns the latent top-
ics discovered from data to human-de ﬁned classes. DBA is use ful for both pattern
classiﬁcation and instance disambiguation, which are test ed on text classiﬁcation
and named entity disambiguation in web search queries respectively.

1 Introduction

We consider multi-class, multi-label and multi-instance classiﬁcation (M 3C), a task of learning de-
cision rules from corpora in which each pattern consists of multiple instances1 and is associated
with multiple classes. M3C ﬁnds its application in many ﬁelds: For example, in web page
classiﬁ-
cation, a web page (pattern) typically comprises of different entities (instances) (e.g., texts, pictures
and videos) and is usually associated with several different topics (e.g., ﬁnance, sports and poli-
tics). In such tasks, a pattern usually consists of a set of instances, and the possible instances may
be too diverse in nature (e.g., of different structures or types, described by different features) to be
represented in a universal space. What makes the problem more complicated and challenging is
that the pattern is usually ambiguous, i.e., it can belong to several different classes simultaneously.
Traditional classiﬁcation algorithms are typically incap able of handling such complications.

Even for corpora consisting of relatively homogenous data, treating the tasks as M3C might still
be advantageous since it enables us to explore the inner structures and the ambiguity of the data
simultaneously. For example, in text classiﬁcation, a docu ment usually comprises several separate
semantic parts (e.g., paragraphs), and several different topics are evolving along these parts. Since
the class-labels are often only locally tied to the document (e.g., paragraphs are often far more topic-
focused than the whole document), base the classiﬁcation on the whole document would incur too
much noise and in turn harm the performance. In addition, treating the task as M3C also offers a
natural way to track the topic evolution along paragraphs, a task that is otherwise difﬁcult to handle.

M3C also arises naturally when the acquisition of labeled data is expensive. For example, in scene
classiﬁcation, a picture usually contains several objects
(e.g., cat, desk, man) belonging to several
different classes (e.g., animal, furniture, human). Ideal annotation requires a skilled expert to specify
both the exact location and class label of each object in the image, which, though not completely
impossible, involves too much human efforts especially for large image repositories. The annotation
burden would be greatly relieved if each image is labeled as a whole (e.g., a caption indicating what
is in the image), which, however, requires the learning system to be capable to handle M3C tasks.

1A “pattern” or “example” is a typical sample in a data collect

ion and an“instance” is a part of a “pattern”.

1

Recently, the Latent Dirichlet Allocation (LDA, [4]) model has been established for automatic ex-
traction of topical structures from large repository of documents. LDA is a highly-modularized
probabilistic model with various variations and extensions (e.g., [2, 3]). By modeling a document
as a mixture over topics, LDA allows each document to be associated with multiple topics with
different proportions, and thus provides a promising way to capture the heterogeneity/ambiguity in
the data. However, the topics discovered by LDA are implicit (i.e., each topic is expressed as a dis-
tribution over words, comprehensible interpretation of which requires human expertise), and cannot
be easily aligned to the topics of human interests. In addition, the standard LDA does not model the
multi-instance structure of a pattern. Hence, LDA and its like cannot be directly applied to M3C.

In this paper, by taking advantage of the LDA building blocks, we present a new probabilistic gener-
ative model for multi-class, multi-label and multi-instance corpora, referred to as Dirichlet-Bernoulli
Alignment (DBA). DBA assumes a tree-structure about the data, i.e., each multi-labeled pattern is a
bag of single-labeled instances. In DBA, each pattern is modeled as a mixture over the set of pre-
de ﬁned classes, an instance is then generated independentl y conditioned on a sampled class-label,
and the label of a pattern is generated from a Bernoulli distribution conditioned on all the sampled
labels used for generating its instances. DBA is essentially a topic model similar to LDA except that
(1) an instance rather than a single feature is generated conditioned on each sampled topic; and (2)
instead of using implicit topics for dimensionality reduction as in LDA, DBA casts each class as an
explicit topic to gain discriminative power from the data. Through likelihood maximization, DBA
automatically aligns the topics discovered from the data to the prede ﬁned classes of our interests.
DBA can be naturally tailored to M3C tasks for both pattern classiﬁcation and instance disambi gua-
tion. In this paper, we apply the DBA model to text classiﬁcat ion tasks and an interesting real-world
problem, i.e., named entity disambiguation for web search queries. The experiments con ﬁrm the
usefulness of the proposed DBA model.

The rest parts of this paper is organized as follows. Section 2 brie ﬂy reviews some related topics
and Section 3 presents the formal description of the corpora used in M3C and the basic assumptions
of our model. Section 4 introduces the detailed DBA model. In Section 5, we establish algorithms
for inference and parameter estimation for DBA. And in Section 6, we apply the DBA model to text
classiﬁcation and query disambiguation tasks. Finally, Se ction 7 presents concluding remarks.

2 Related Works

Traditional classiﬁcation largely focuses on a single-lab el single-instance framework (i.e., i.i.d pat-
terns, associated with exclusive/disjoint classes). However, the real-world is more like a web of
(sub-)patterns connected with a web of classes that they belong to. Clearly, M3C re ﬂects more of
the reality. Recently, two partial solutions, i.e., multi-instance classiﬁcation (MIC) [7, 11, 1] and
multi-label classiﬁcation (MLC) [10, 8, 5] were investigat ed. MIC assumes that each pattern con-
sists of multiple instances but belongs to a single class, whereas MLC studies single-instance pattern
associated with multiple classes. Although both MLC and MIC have drawn increasing attentions in
the literature, neither of them can handle the cases where multi-instance and multi-label are simulta-
neously present. Perhaps the ﬁrst work investigating M 3C is [13], in which the authors proposed an
indirect solution, i.e., to convert an M3C task into several MIC or MLC sub-tasks each of which is
then divided into single-label and single-instance classiﬁcation problems and solved by discrimina-
tive algorithms such as AdaBoost or SVM. A practical challenge of this approach is its complexity,
i.e, the number of sub-tasks can be huge, making the training data extremely sparse for each sub-
classiﬁer and the computation cost unacceptably high in bot h training and testing. Recently, Cour et
al proposed a discriminative framework [6] based on convex surrogate loss minimization for clas-
sifying ambiguously labeled images; and Xu et al established a hybrid generative/discriminative
approach (i.e., a heuristically regularized LDA classiﬁer ) [12] to mining named entity from web
search click-through data. In this paper, we present a generative approach for M3C.

Our proposed DBA model can be viewed as a supervised version of topic models. A widely used
topic model for categorical data is the LDA model [4]. By modeling a pattern as a random mixture
over latent topics and a topic as a Multinomial distribution over features in a dictionary, LDA is
effective in discovering implicit topics from a corpus. The supervised LDA (sLDA) model [2], by
linking the empirical topics to the label of each pattern, is able to learn classiﬁers using Generalized
Linear Models. However, both LDA and sLDA are in essence dimensionality reduction techniques,
and cannot be employed directly for the M3C tasks.

2

pattern

X

a

z

B

f

y

x

c

...

c

x

c

x

class

L
M

instance

θ
(b)
(a)
Figure 1: (a): Tree structure of a multi-class multi-label multi-instance corpus. (b):A graphic repre-
sentation of the DBA model with multinomial bag-of-feature instance model.
Intuitively, we can think of a pattern as a document, an instance as a paragraph, and a feature as a
word. In M3C, we are interested in inferring class labels for both the document and its paragraphs.

3 Problem Formalization

N

Formally, let X ⊂ RD denote the instance space (e.g., a vector space), Y = {1, 2, . . . , C } (C > 2)
the set of class labels, and F = {f1 , f2 , . . . , fD } the dictionary of features. A multi-class, multi-
label multi-instance corpus D consists of a set of input patterns {Xn}n=1,2,...,N along with the
corresponding labels {Yn}n=1,2,...,N , where each pattern Xn = {xmn}m=1,2,...,Mn contains a set
of instances xmn ∈ X , and Yn ⊂ Y consists of a set of class labels. The goal of M3C is to ﬁnd a
decision rule Y = ϕ(X ) : 2X → 2Y , where 2A denotes the power set of a set A. For simplicity, we
make the following assumptions.

Assumption 1 [Exchangeability]: A corpus is a bag of patterns, and each pattern is a bag of instances.

Assumption 2 [Distinguishablity]: Each pattern can belong to several classes, but each instance
belongs to a single class.

These assumptions are equivalent to assuming a tree structure for the corpus (Figure 1(a)).

4 Dirichlet-Bernoulli Alignment

In this section, we present Dirichlet-Bernoulli Alignment (DBA), a probabilistic generative model
for the multi-class, multi-label and multi-instance corpus described in Section 3. In DBA, each
pattern X in a corpus D is assumed to be generated by the following process:

1. Sample θ∼Dir(a).
2. For each of the M instances in X :
⊲ Choose a class z ∼Mult(θ );
⊲ Generate an instance x ∼ p(x|z, B );
3. Generate the label y∼ p(y|z1:M ,λ).

a =
We assume the total number of prede ﬁned classes, C ,
In DBA,
is known and ﬁxed.
[a1 , . . . , aC ]⊤ with ac > 0, c = 1, . . . , C , is a C -vector prior parameter for a Dirichlet distribu-
tion Dir(a), which is de ﬁned in the ( C -1)-simplex: θc > 0, PC
c=1 θc = 1. z is a class indicator, i.e.,
a binary C -vector with the 1-of-C code: zc = 1 if the c-th class is chosen, and ∀i 6= c, zi = 0.
y = [y1 , . . . , yC ]⊤ is also a binary C -vector with yc = 1 if the pattern X belongs to the c-th class
and yc = 0 otherwise.
In this paper, we assume the label of a pattern is generated by a cost-sensitive voting process accord-
ing to the labels of the instances in it, which is intuitively reasonable. As a result, yc (c = 1, . . . , C ) is
generated from a Bernoulli distribution, i.e., p(yc |πc ) = (πc )yc (1−πc )(1−yc ) , where π is a probabil-
ity vector based on a weighted empirical average of the Dirichlet realization λ⊤¯z, ¯z = [ ¯z1 , . . . , ¯zC ]⊤
M PM
is the average of z1 , . . . , zM : ¯zc = 1
m=1 zmc . For example, π can be a Dirichlet distribution
π∼Dir(λ1 ¯z1 , . . . , λC ¯zC ). In this paper, we use a logistic model:
p(yc = 1|¯z, λ) =

(1)

.

exp(λc ¯zc )
1 + exp(λc ¯zc )

3

In practice, the set of possible instances can be quite diverse, such as pictures, texts, music and
videos on a web page. Without loss of generality, we follow the convention of topic models to
assume that each instance x is a bag of discrete features {f1 , f2 , . . . , fL } and use a multinomial
distribution2:

p(x|z, B ) = p({f1 , . . . , fL}|z, B ) ∝ bx1
c1 bx2
c2 . . . bxD
cD |zc=1 ,

p(X, y, Z, θ |a, B , λ) = p(θ |a)

where L is the total number of feature occurrences in x (e.g., the length of a paragraph), B =
[b1 , . . . , bD ] is a C × D-matrix with the (c, d)-th entry bcd = p(fd = 1|zc = 1) and xd is the
frequency of fd in x. The joint probability is then given by:
p(fml |B , zm )! p(y|¯z, λ).
Ym=1  p(zm |θ)
L
M
Yl=1
The graphical model for DBA is depicted in Figure 1(b). We can see that DBA has a diagram very
similar to that of sLDA (Figure 1 in [2]). The key differences are: (1) Instead of using implicit
topics for dimensionality reduction as in sLDA, DBA casts the prede ﬁned classes as explicit topics
to discover the discriminative properties from the data; (2) A bag-of-feature instance rather than a
single feature is generated conditioned on each sampled topic (class); (3) DBA models a multi-class,
multi-label multi-instance corpus and can be applied directly to M3C, i.e., the classiﬁcation of each
pattern as well as the instances within it.

(2)

5 Parameter Estimation and Inference

Both parameter estimation and inferential tasks in DBA involve intractable computation of marginal
probabilities. We use variational methods to approximate those distributions.

5.1 Variational Approximations

We use the following fully-factorized variational distribution to approximate the posterior distribu-
tion of the latent variables:

q(Z, θ |γ , Φ) = q(θ |γ )

Yc=1  θγc−1
M
M
C
Γ(PC
c=1 γc )
Ym=1
Ym=1
c
QC
c=1 Γ(γc )
where γ and Φ=[φ1 ,. . . ,φM ] are variational parameters for a pattern X . We have:

q(zm |φm ) =

mc ! ,
φzmc

log P (X, y|a, B , λ) = log Zθ XZ
=L(γ , Φ) + K L(q(Z, θ |γ , Φ)||p(Z, θ |a, B , λ)) ≈ max
γ ,Φ

p(X, y, Z, θ |a, B , λ)dθ

L(γ , Φ),

(3)

(4)

where K L(q(x)||p(x)) = Rx q(x) log q(x)
p(x) dx is the Kullback-Leibler (KL) divergence between two
distributions p and q , and L(·) is the variational lower bound for the log-likelihood:
L(γ , Φ) = log Zθ XZ
M
M
Xm=1
Xm=1
Eq [log p(zm |θ )] +
+
2This is only a simple special case instance model for DBA. It is quite straightforward to substitute other
instance models such as Gaussian, Poisson and other more complicated models like Gaussian mixtures.

Eq [log p(xm |B , zm )] + Eq [log p(y|¯z, λ)] + Hq .

p(X, y, Z, θ |a, B , λ)
q(Z, θ |γ , Φ)

dθ = Eq [log p(θ |a)]

q(Z, θ |γ , Φ) log

(5)

4

The ﬁrst two terms and the ﬁfth term (the entropy of the variat
ional distribution) in the right-hand
side of Eq.(5) are identical to the corresponding terms in sLDA [2]. The third term, i.e., the varia-
tional expectation of the log likelihood for instance observations is:

D
C
M
M
Xd=1
Xc=1
Xm=1
Xm=1
The forth term in the righthand side of Eq.(5) corresponds to the expected log likelihood of observing
the labels given the topic assignments:

Eq [log p(xm |B , zm )] =

φmcxmd log bcd .

(6)

Eq [log p(y|¯z, λ)] =

C
C
M
Xc=1
Xc=1
Xm=1
We bound the second term above by using the lower bound for logistic function [9]:

Eq [log(exp

λc ¯zc
2

)λcφmc −

+ exp

(yc −

1
M

1
2

−λc ¯zc
2

)].

(7)

− log(exp

λc ¯zc
2

+ exp

−λc ¯zc
2

) > − log(1 + exp(−ξc )) −

≈ − log(1 + exp(−ξc )) −

ξc
2
ξc
2

c − ξ 2
c ¯z 2
+ ςc (λ2
c )

+ 2ςc (λc ¯zcξc − ξ 2
c ),

(8)

tanh( ξc
where ξ=[ξ1 , . . . , ξC ]⊤ are variational parameters, ςc = 1
2 ), and the second order residue
4ξc
term is omitted since the lower bound is exact when ξc = −λc ¯zc .
Obtaining an approximate posterior distribution for the latent variables is then reduced to optimizing
the objective max L(q) or min K L(q ||p) with respect to the variational parameters. By using La-
grange multipliers, we can easily derive the optimal condition which can be achieved by iteratively
updating the variational parameters according to the following formulas:

(9)

ξc
2

φmc ∝

λc
2M

ξc = −λc

[2yc − 1 + tanh(

)](cid:19),
φmc ,

D
(bcd )xmd exp (cid:18)Ψ(γc ) +
Yd=1
M
M
Xm=1
Xm=1
φmc ,
γc = ac +
where Ψ(·) is the digamma function. Note that instead of only one feature contributing to φmc as in
LDA, all the features appearing in an instance are now responsible for contributing. This property
tends to make DBA more robust to data sparsity. Also, DBA makes use of the supervision infor-
mation with a term PC
c=1 λc ¯zc (2yc − 1) in the variational likelihood bound L. As L is optimized,
this term is equivalent to maximizing the likelihood of sampling the classes to which the pattern be-
longs: {max λc PM
m=1 zmc , if yc = 1} and simultaneously minimizing the likelihood of sampling
the classes to which the pattern does not belong: {min λc PM
m=1 zmc , if yc = 0}. Here λc (-λc )
acts like a utility (cost) of assigning X to the c-th class. As a result, it tends to align the Dirichlet
topics discovered from the data to the class labels (Bernoulli observations) y. This is why we coin
the name Dirichlet-Bernoulli Alignment.

1
M

5.2 Parameter Estimation

The maximum likelihood parameter estimation of DBA relies on the variational approximation pro-
cedure. Given a corpus D = {(Xn , yn )}n=1,...,N , the MLE can be formulated as:

a∗ , B ∗ , λ∗ = arg max log P (D|a, B , λ) = arg max
a,B ,λ

N
Xn=1

max
γ n ,Φn

L(γ n , Φn |a, B , λ).

(10)

5

Data Set
Text
Query

#Train
1200
300

Table 1: Characteristic of the data sets.
#Test D
|Y |avg #(|Y | > 1) Mavg Mmin Mmax
C
36
1
8.2
721 (38.4%)
1.4
10
500
679
100
2000
101
1.4
99 (24.8%)
65
3
731

DBA

MNB

MIMLSVM

MIMLBoost

 

100

90

80

70

 

overall
wheat
trade
ship
interest money
grain
earn
crude
corn
acq
Figure 2: Accuracies(%) of DBA, MNB, MIMLSVM, and MIMLBoost for text classiﬁcation.

The two-layer optimization in Eq.(10) involves two groups of parameters corresponding to the DBA
model and its variational approximation, respectively. Optimizing alternatively between these two
groups leads to a Variational Expectation Maximization (VEM) algorithm similar to the one used in
LDA, where the E-step corresponds to the variational approximation for each pattern in the corpus.
And the M-step in turn maximizes the objective in Eq.(6) w.r.t. the model parameters. These two
steps are repeated alternatively until convergence.

5.3 Inference
DBA involves three types of inferential tasks. The ﬁrst task is to infer the latent variables for a
given pattern, which is straightforward after the variational approximation. The second task, pat-
tern classiﬁcation, addresses prediction of labels for a ne w pattern X : p(yc = 1|X ; a, B , λ) ≈
exp(λc ¯φc )/(1 + exp(λc ¯φc )), where ¯φc = 1
M PM
2M [2yc − 1 + tanh( ξc
m=1 φmc and the term λc
2 )]
is removed when updating φ in Eq.(9). The third task, instance disambiguation, ﬁnds la bels
for each instances within a pattern: p(zm |X, y) = Rθ p(zm , θ |X, y)dθ ≈ q(zm |φm ), that is,
p(zmc = 1|X, y) = φmc .
6 Experiments

In this section, we conduct extensive experiments to test the DBA model as it is applied to pattern
classiﬁcation and instance disambiguation respectively. We ﬁrst apply DBA to text classiﬁcation and
compare its performance with state-of-the-art M3C algorithms. Then the instance disambiguation
performance of DBA is tested on a novel real-world task, i.e., named entity disambiguation for web
search queries. Table 1 shows the information of the data sets used in our experiments.

6.1 Text Classiﬁcation
This experiment is conducted on the ModApte split of the Reuters-21578 text collection, which
contains 10788 documents belonging to the most popular 10 classes. We use the top 500 words with
the highest document frequency as features, and represent each document as a pattern with each of
its paragraphs being an instance in order to exploit the semantic structure of documents explicitly.
After eliminating the documents that have empty label set or less than 20 features, we obtain a subset
of 1879 documents, among which 721 documents (about 38.4%) have multiple labels. The average
number of labels per document is 1.4±0.6 and the average number of instances (paragraphs) per
pattern (document) is 8.2±4.8. The data set is further randomly partitioned into a subset of 1200
documents for training and the rest for testing.

For comparison, we also test two state-of-the-art M3C algorithms, the MIMLSVM and MIMLBoost
[13], and use the Multinomial Na¨ıve Bayes (MNB) classiﬁer t
rained on the vector space model of the
whole documents as the baseline. For a fair comparison, linear kernel is used in both MIMLSVM
and MIMLBoost and all the hyper-parameters are tuned by 5-fold cross validation prior to training.
We use the Hamming-Accuracy [13] to evaluate the results, for DBA and MNB, the label is esti-
mated by: y = δ(p(y = 1|X ) > t), where the cut-off probability threshold is also selected based
on 5-fold cross validation. Each experiment is repeated for 5 random runs and the average results
are reported by a bar chart as depicted in Figure 2. We can see that: (1) for most classes, the three

6

Table 2: Accuracy@N (N = 1, 2, 3) and micro-averaged and macro-averaged F-measures of DBA, MNB and
SVM based disambiguation methods.

Method
MNB-TF
MNB-TF-IDF
SVM-TF
SVM-TF-IDF
DBA

A@1
0.4154
0.4177
0.4927
0.4912
0.5415

Gain
30.4%
29.6%
9.9%
10.2%
-

A@2
0.4913
0.4918
NA
NA
0.6175

A@3
0.5168
0.5176
NA
NA
0.6482

Gain
25.4%
25.2%

-

Fmicro
0.4154
0.4177
0.4927
0.4912
0.5415

Gain
30.4%
29.6%
9.9%
10.2%
-

Fmacro
0.3144
0.2988
0.3720
0.3670
0.4622

Gain
25.7%
25.6%

-

 

Gain
47.0%
54.7%
24.2%
25.0%

 

n
o
i
s
i
c
e
r
p

1

0.8

0.6

0.4

0.2

0

 
0

DBA
MNBTF
MNBTFIDF
SVMTF
SVMTFIDF

20

40

60

80

100

l
l
a
c
e
r

1

0.8

0.6

0.4

0.2

0

 
0

DBA
MNBTF
MNBTFIDF
SVMTF
SVMTFIDF

20

40

60

80

100

class
class
Figure 3: Precision and Recall scores for each of 101 classes by using DBA, MNB and SVM based methods.

M3C algorithms outperform the MNB baseline; (2) the performance of DBA is at least comparable
with MIMLBoost and MIMLSVM. For most classes and overall, DBA performs the best, whereas
for some classes, MIMLBoost and MIMLSVM perform even slightly worse than MNB. A possi-
ble reason might be: if the documents are very short, splitting them might introduce severe data
sparseness and in turn harms the performance. We also observe that DBA is much more efﬁcient
than MIMLBoost and MIMLSVM. For training, DBA takes 42 mins on average, in contrast to 557
minutes (MIMLSVM) and 806 minutes (MIMLBoost).

6.2 Named Entity Disambiguation
Query ambiguity is a fundamental obstacle for search engine to capture users’ search intentions. In
this section, we employ DBA to disambiguate the named entities in web search queries. This is a
very challenging problem because queries are usually very short (2 to 3 words on average), noisy
(e.g., misspellings, abbreviations, less grammatical structure) and topic-distracted. A single named-
entity query Q can be viewed as a combination of a single named entity e and a set of context words
w (the remaining text in Q). By differentiating the possible meanings of the named entity in a query
and identifying the most possible one, entity disambiguation can help search engines to capture the
precise information need of the user and in turn improve search by responding with the truly most
relevant documents. For example, when a user inputs “ When are the casting calls for Harry Potter
in USA? ”, the system should be able to identify that the ambiguous na med entity “ Harry Potter ”
(i.e., it can be a movie, a book or a game) really refers to a movie in this speciﬁc query.

We treat the ambiguity of e as a hidden class z over e and make use of the query log as a data
source for mining the relationship among e, w and z . In particular, the query log can be viewed
as a multi-class, multi-label and multi-instance corpus {(Xn , Yn )}n=1,2,...,N , in which each pat-
tern X corresponds to a named-entity e and is characterized by a set of instances {xm}m=1,2,...,M
corresponding to all the contexts {wm}m=1,2,...,M that co-occur with e in queries, and the label Y
contains all the ambiguities of e.
Our data was based on a snapshot of answers.yahoo.com crawled in early 2008, containing
216563 queries from 101 classes. We manually collect 400 named entities and label them according
to the labels of their co-occurring queries in Yahoo! CQA. A randomly chosen subset of 300 entities
are used as training data and the other 100 are used for testing. We compare our DBA based method
with baselines including Multinomial Na¨ıve Bayes classiﬁ er using TF (MNB-TF) or TF-IDF (MNB-
TFIDF) as word attributes, and SVM classiﬁer using TF ( SVM-TF) or TFIDF (SVM-TF-IDF). For
SVM, a similar scheme as MIMLSVM is used for learning M3C classiﬁers.

Table 2 demonstrates the Accuracy@N (N = 1, 2, 3) as well as micro-averaged and macro-average
F-measure scores of each disambiguation approach3. All the results are obtained through 5-fold
cross-validation. From the table, we observe that DBA achieves signiﬁcantly better performance
than all the other methods. In particular, for Accuracy@1 scores, DBA can achieve a gain of about

3Since SVM only outputs hard class assignments, there is no Accuracy@2,3 for SVM based methods.

7

30% relative to two MNB methods, and about 10% relative to two SVM methods; for macro-average
F-measures, DBA can achieve a gain of about 50% over MNB methods, and about 25% over SVM
methods. As a reference, in Figure 3, we also illustrate the sorted precision and recall scores for each
of the 101 classes. We can see that, DBA slightly outperforms the baselines in terms of precision,
and signiﬁcantly performs better in terms of the recall scor es. In particular, for recall, DBA can
achieve a gain of more than 50% relative to MNB and SVM baselines.

7 Concluding Remarks
Multi-class, multi-label and multi-instance classiﬁcati on (M3C) is encountered in many applications.
Even for task that is not explicitly an M3C problem, it might still be advantageous to treat it as
M3C so as to better explore its inner structures and effectively handle the ambiguities. M3C also
naturally arises from the difﬁculty of acquiring ﬁnely-lab
eled data. In this paper, we have proposed a
probabilistic generative model for M3C corpora. The proposed DBA model is useful for both pattern
classiﬁcation and instance disambiguation, as has been tes ted respectively in text classiﬁcation and
named-entity disambiguation tasks.

An interesting observation in practice is that, although there might be a large number of
classes/topics, usually a pattern is only associated with a very limited number of them.
In our
experiment, we found that substantial improvement could be achieved by simply enforcing label
sparsity, e.g., by using LASSO style regularization. In future, we will investigate such “Label Parsi-
moniousness” in a principled way. Another meaningful inves tigation would be to explicitly capture
or explore the class correlations by using, for example, the Logistic Normal distribution [3] rather
than Dirichlet.
Acknowledgments
Hongyuan Zha is supported by NSF #DMS-0736328 and grant from Microsoft. Bao-Gang Hu is
supported by NSFC #60275025 and the MOST of China grant #2007DFC10740.

References
[1] Andrews S. and Hofmann T. (2003) Multiple Instance Learning via Disjunctive Programming
Boosting, In Advances in Neural Information Processing Systems 17 (NIPS’03), MIT Press.
[2] Blei D. and McAuliffe J. (2007) Supervised topic models. In Advances in Neural Information
Processing Systems 21 (NIPS’07 ), MIT Press.
[3] Blei D. and Lafferty J. (2007) A correlated topic model of Science. Annals of Applied Statistics.
Vol. 1, No. 1, pp. 17 –35, 2007.
[4] Blei D., Ng A. and Jordan M. (2003) Latent Dirichlet Allocation. Journal of Machine Learning
Research, Vol. 3, pp.993 –1022, Jan. 2003, MIT Press.
[5] Boutell M. R., Luo J., Shen X. and Brown C. M. (2004) Learning Multi-Label Scene Classiﬁ-
cation. Pattern Recognition, 37(9), pp.1757 –1771, 2004.
[6] Cour T., Sapp B., Jordan C. and Taskar B. (2009) Learning from Ambiguously Labeled Images,
In the 23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR’09).
[7] Dietterich T. G., Lathrop R. H., Lozano-Perez T. (1997) Solving the Multiple-Instance Problem
with Axis-Parallel Rectangles. Artiﬁcial Intelligence Journal , Vol. 89, pp.31 –71, Jan.1997.
[8] Ghamrawi N. and McCallum A. (2005) Collective Multi-Label Classiﬁcation, In ACM Interna-
tional Conference On Information And Knowledge Management (CIKM’05), pp.195 –200.
[9] Jaakkola, T. and Jordan M. I. (2000). Bayesian parameter estimation via variational methods.
Statistics and Computing, Vol 10, Issue 1, pp. 25 –37.
[10] Ueda N. and Saito K. (2002) Parametric Mixture Models For Multi-Labeled Text. In Advances
in Neural Information Processing Systems 15 (NIPS’02).
[11] Viola P., Platt J. and Zhang C. (2006). Multiple Instance Boosting For Object Detection. In
Advances in Neural Information Processing Systems 20 (NIPS’06), pp.1419 –1426, MIT Press.
[12] Xu G., Yang S.-H. and Li H. (2009) Named Entity Mining from Click-Through Data Using
Weakly Supervised LDA, In ACM Knowledge Discovery and Data Mining (KDD’09).
[13] Zhou Z.-H. and Zhang M.-L. (2006) Multi-Instance Multi-Label Learning with Application to
Scene Classiﬁcation, In Advances in Neural Information Processing Systems 20 (NIPS’06).

8


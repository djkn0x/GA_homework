Efﬁcient Match Kernels between Sets of Features
for Visual Recognition

Liefeng Bo
Toyota Technological Institute at Chicago
blf0218@tti-c.org

Cristian Sminchisescu
University of Bonn
sminchisescu.ins.uni-bonn.de

Abstract

In visual recognition, the images are frequently modeled as unordered collections
of local features (bags). We show that bag-of-words representations commonly
used in conjunction with linear classiﬁers can be viewed as special match kernels,
which count 1 if two local features fall into the same regions partitioned by vi-
sual words and 0 otherwise. Despite its simplicity, this quantization is too coarse,
motivating research into the design of match kernels that more accurately mea-
sure the similarity between local features. However, it is impractical to use such
kernels for large datasets due to their signiﬁcant computational cost. To address
this problem, we propose efﬁcient match kernels (EMK) that map local features
to a low dimensional feature space and average the resulting vectors to form a set-
level feature. The local feature maps are learned so their inner products preserve,
to the best possible, the values of the speciﬁed kernel function. Classiﬁers based
on EMK are linear both in the number of images and in the number of local fea-
tures. We demonstrate that EMK are extremely efﬁcient and achieve the current
state of the art in three difﬁcult computer vision datasets: Scene-15, Caltech-101
and Caltech-256.

1 Introduction

Models based on local features have achieved state-of-the art results in many visual object recogni-
tion tasks. For example, an image can be described by a set of local features extracted from patches
around salient interest points or regular grids, or a shape can be described by a set of local features
deﬁned at edge points. This raises the question on how should one measure the similarity between
two images represented as sets of local features. The problem is non-trivial because the cardinality
of the set varies with each image and the elements are unordered.
Bag of words (BOW) [27] is probably one of the most popular image representations, due to both
its conceptual simplicity and its computational efﬁciency. BOW represents each local feature with
the closest visual word and counts the occurrence frequencies in the image. The resulting histogram
is used as an image descriptor for object recognition, often in conjunction with linear classiﬁers.
The length of the histogram is given by the number of visual words, being the same for all images.
Various methods for creating vocabularies exist [10], the most common being k-means clustering of
all (or a subsample of) the local features to obtain visual words.
An even better approach to recognition is to deﬁne kernels over sets of local features. One way is to
exploit closure rules. The sum match kernel of Haussler [7] is obtained by adding local kernels over
all combinations of local features from two different sets. In [17], the authors modify the sum kernel
by introducing an integer exponent on local kernels. Neighborhood kernels [20] integrate the spatial
location of local features into a sum match kernel. Pyramid match kernels [5, 14, 13] map local
features to multi-resolution histograms and compute a weighted histogram intersection. Algebraic
set kernels [26] exploit tensor products to aggregate local kernels, whereas principal angle kernels

1

[29] measure similarities based on angles between linear subspaces spanned by local features in the
two sets. Other approaches estimate a probability distribution on sets of local features, then derive
their similarity using distribution-based comparison measures [12, 18, 2]. All of the above methods
need to explicitly evaluate the full kernel matrix, hence they require space and time complexity that
is quadratic in the number of images. This is impractical for large datasets (see §4).
In this paper we present efﬁcient match kernels (EMK) that combine the strengths of both bag of
words and set kernels. We map local features to a low dimensional feature space and construct
set-level features by averaging the resulting feature vectors. This feature extraction procedure is not
signiﬁcantly different than BOW. Hence EMK can be used in conjunction with linear classiﬁers and
do not require the explicit computation of a full kernel matrix—this leads to both space and time
complexity that is linear in the number of images. Experiments on three image categorization tasks
show that EMK are effective computational tools.

2 Bag of Words and Match Kernels

with

δ(x, y) =

δ(x, y)

(1)

1
|X||Y |

µ(x)(cid:62)µ(y) =

In supervised image classiﬁcation, we are given a training set of images and their corresponding
labels. The goal is to learn a classiﬁer to label unseen images. We adopt a bag of features method,
which represents an image as a set of local features. Let X = {x1 , . . . , xp} be a set of local features
in an image and V = {v1 , . . . , vD } the dictionary, a set of visual words.
In BOW, each local
(cid:80)
feature is quantized into a D dimensional binary indicator vector µ(x) = [µ1 (x), . . . , µD (x)](cid:62) .
µi (x) is 1 if x ∈ R(vi ) and 0 otherwise, where R(vi ) = {x : (cid:107)x − vi (cid:107) ≤ (cid:107)x − v(cid:107), ∀v ∈ V}.
The feature vectors for one image form a normalized histogram µ(X) = 1|X|
x∈X µ(x), where
| · | is the cardinality of a set. BOW features can be used in conjunction with either a linear or a
kernel classiﬁer, albeit the latter often leads to expensive training and testing (see §4). When a linear
(cid:88)
(cid:88)
(cid:88)
(cid:88)
classiﬁer is used, the resulting kernel function is:
1
KB (X, Y) = µ(X)(cid:62)µ(Y) =
|X||Y |
(cid:189)
y∈Y
x∈X
y∈Y
x∈X

1, x, y ⊂ R(vi ), ∃i ∈ {1, . . . , D}
0, otherwise
δ(x, y) is obviously a positive deﬁnite kernel, measuring the similarity between two local features
x and y: δ(x, y) = 1 if x and y belong the same region R(vi ), and 0 otherwise. However, this type
of quantization can be too coarse when measuring the similarity of two local features (see also ﬁg.
1 in [21]), risking a signiﬁcant decrease in classiﬁcation performance. Better would be to replace
(cid:88)
(cid:88)
δ(x, y) with a continuous kernel function that more accurately measures the similarity between x
and y:
1
|X||Y |
y∈Y
x∈X
In fact, this is related to the normalized sum match kernel [7, 17]. Based on closure properties,
Ks (X, Y) is a positive deﬁnite kernel, as long as the components k(x, y) are positive deﬁnite. For
convenience, we refer to k(x, y) as the local kernel. A negative impact of kernelization is the high
computational cost required to compute the summation match function, which takes O(|X||Y |) for
a single kernel value rather than O(1), the cost of evaluating a single kernel function deﬁned on
vectors. When used in conjunction with kernel machines, it takes O(n2 ) and O(n2m2d) to store
and compute the entire kernel matrix, respectively, where n is the number of images in the training
set, and m is the average cardinality of all sets. For image classiﬁcation, m can be in the thousands
(cid:80)n
of units, so the computational cost rapidly becomes quartic as n approaches (or increases beyond)
m. In addition to expensive training, the match kernel function has also a fairly high testing cost:
i=1 αiKs (Xi , X) takes O(nm2d). This
for a test input, evaluating the discriminant f (X) =
is, again, unacceptably slow for large n. For sparse kernel machines, such as SVMs, the cost can
decrease to some extent, as some of the αi are zero. However, this does not change the order of
complexity, as the level of sparsity usually grows linearly in n.
Deﬁnition 1. The kernel function k(x, y) = φ(x)(cid:62)φ(y) is called ﬁnite dimensional if the feature
map φ(·) is ﬁnite dimensional.

(2)

(3)

KS (X, Y) =

k(x, y)

2

Sum [7]
Train O(n2m2 d)
Test
O(nm2 d)

Bhattacharyya [12]
O(n2m3 d)
O(nm3 d)

PMK [6]
EMK-CKSVD
O(n2m log(T )d) O(nmDd + nD2 )
O(mDd + D2 )
O(nm log(T )d)

EMK-Fourier
O(nmDd)
O(mDd)

Table 1: Computational complexity for ﬁve types of ‘set kernels’. ’Test’ means the computational
cost per image.’Sum’ is the sum match kernel used in [7]. ’Bhattacharyya’ is the Bhattacharyya
kernel in [12]. PMK is the pyramid match kernel of [6], with T in PMK giving the value of the
maximal feature range. d is the dimensionality of local features. D in EMK is the dimensionality
of feature maps and does not change with the training set size. Our experiments suggest that a
value of D in the order of thousands of units is sufﬁcient for good accuracy. Thus, O(nmDd) will
dominate the computational cost for training, and O(mDd) the one for testing, since m is usually
in the thousands, and d in the hundreds of units. EMK uses linear classiﬁers and does not require
the evaluation of the kernel matrix. The other four methods are used in conjunction with kernel
classiﬁers, hence they all need to evaluate the entire kernel matrix. In the case of nearest neighbor
classiﬁers, there is no training cost, but testing costs remain unchanged.

(4)

δ(x, y) is a special type of ﬁnite dimensional kernel. With the ﬁnite dimensional kernel, the match
kernel can be simpliﬁed as:
(cid:80)
KS (X, Y) = φ(X)(cid:62)φ(Y)
where φ(X) = 1|X|
x∈X φ(x) is the feature map on the set of vectors. Since φ(X) is ﬁnite
and can be computed explicitly, we can extract feature vectors on the set X, then apply a linear
classiﬁer on the resulting represenation. We call (4) an efﬁcient match kernel (EMK). The feature
extraction in EMK is not signiﬁcantly different from the bag of words method. The training and
testing costs are O(nmDd) and O(mDd) respectively, where D is the dimensionality of the feature
map φ(x). If the feature map φ(x) is low dimensional, the computational cost of EMK can be
much lower than the one required to evaluate the match kernel by computing the kernel functions
k(x, y). For example, the cost is 1
n lower when D has the same order as m (this is the case in
our experiments). Notice that we only need the feature vectors φ(X) in EMK, hence it is not
necessary to compute the entire kernel matrix. Since recent developments have shown that linear
SVMs can be trained in linear complexity [25], there is no substantial cost added in the training
(cid:80)L−1
(cid:80)2l
phase. The complexity of EMK and of several other well-known set kernels is reviewed in table 1.
If necessary, location information can be incorporated into EMK, using a spatial pyramid [14, 13]:
t=1 2−lKS (X(l,t) , Y(l,t) ) = φS (X)(cid:62)φS (Y), where L is the number of
KP (X, Y) =
l=0
pyramid levels, 2l is the number of spatial cells in the l-th pyramid level, X(l,s) are local features
falling within the spatial cell (l, s), and φP (X) = [φ(X(1,1) )(cid:62) , . . . , φ(X(l,s) )(cid:62) ](cid:62) .
While there can be many choices for the local feature maps φ(x)—and the positive deﬁniteness of
k(x, y) = φ(x)(cid:62)φ(x) can be always guaranteed—, most do not necessarily lead to a meaningful
similarity measure. In the paper, we give two principled methods to create meaningful local feature
maps φ(x), by arranging for their inner products to approximate a given kernel function.

3 Efﬁcient Match Kernels
In this section we present two kernel approximations, based on low-dimensional projections (§3.1),
and based on random Fourier set features (§3.2).

3.1 Learning Low Dimensional Set Features

Our approach is to project the high dimensional feature vectors ψ(x) induced by the kernel
k(x, y) = ψ(x)(cid:62)ψ(y) to a low dimensional space spanned by D basis vectors, then construct a
local kernel from inner products, based on low-dimensional representations. Given {ψ(zi )}D
i=1 , a
set of basis vectors zi , we can approximate the feature vector ψ(x):
(cid:107)ψ(x) − Hvx(cid:107)2

(5)

vx = argmin
vx

3

Figure 1: Low-dimensional approximations for a Gaussian kernel. Left: approximated Gaussian
kernel with 20 learned feature maps. Center: the training objective (12) as a function of stochastic
gradient descent iterations. Right: approximated Gaussian kernel based on 200 random Fourier
features. The feature maps are learned from 200 samples, uniformly drawn from [-10,10].

(6)

where H = [ψ(z1 ), . . . , ψ(zD )] and vx are low-dimensional (projection) coefﬁcients. This is a
convex quadratic program with analytic solution:
vx = (H(cid:62)H)−1 (H(cid:62)ψ(x))
The local kernel derived from the projected vectors is:
kl (x, y) = [Hvx ](cid:62) [Hvy ] = kZ (x)(cid:62)K−1
ZZ kZ (y)
(7)
where kZ is a D × 1 vector with {kZ }i = k(x, zi ) and KZZ is a D × D matrix with {KZZ }ij =
ZZ (notice that K−1
k(zi , zj ). For G(cid:62)G = K−1
ZZ is positive deﬁnite), the local feature maps are:
(cid:163)(cid:80)
(cid:164)
φ(x) = GkZ (x)
The resulting full feature map is: φ(X) = 1|X| G
x∈X kZ (x)
, with computational complexity
O(mDd + D2 ) for a set of local features. A related method is the kernel codebook [28], where
a set-level feature is also extracted based on a local kernel, but with different feature map φ(·).
An essential difference is that inner products of our set-level features φ(X) formally approximate
the sum-match kernel, whereas the ones induced by the kernel codebook do not. Therefore EMK
only requires a linear classiﬁer wherea a kernel codebook would require a non-linear classiﬁer for
comparable performance. As explained, this can be prohibitively expensive to both train and test, in
large datasets. Our experiments, shown in table 3, further suggest that EMK outperforms the kernel
codebook, even in the non-linear case.
How can we learn the basis vectors? One way is kernel principal component analysis (KPCA) [24]
on a randomly selected pool of F local features, with the basis set to the topmost D eigenvectors.
(cid:80)F
This faces two difﬁculties, however: (i) KPCA scales cubically in the number of selected local fea-
tures, F ; (ii) O(F md) work is required to extract the set-level feature vector for one image, because
i=1 αiψ(xi ). For
the eigenvectors are linear combinations of the selected local feature vectors,
large F , as typically required for good accuracy, this approach is too expensive. Although the ﬁrst
the pre-image problem (z, β ) = argminz,β (cid:107) (cid:80)F
difﬁculty can be palliated by iterative KPCA [11], the second computational challenge remains. An-
other option would be to approximate each eigenvector with a single feature vector ψ(z) by solving
i=1 αiψ(xi ) − βψ(z)(cid:107)2 , after KPCA. However,
the two step approach is sub-optimal. Intuitively, it should be better to ﬁnd the single vector ap-
proximations within an uniﬁed objective function. This motivates our constrained singular value
F(cid:88)
decomposition in kernel feature space (CKSVD):
i=1
where F is the number of the randomly selected local features, Z = [z1 , . . . , zD ] and V =
[v1 , . . . , vF ]. If the pre-image constraints H = [ψ(z1 ), . . . , ψ(zD )] are dropped, it is easy to show
that KPCA can be recovered. The partial derivatives of R with respect to vi are:
∂R(V, Z)
= 2H(cid:62)Hvi − 2H(cid:62)ψ(xi )
∂vi

(cid:107)ψ(xi ) − Hvi (cid:107)2

(10)

argmin
V,Z

R(V, Z) =

1
F

(8)

(9)

4

−10−50510−0.200.20.40.60.811.2  ApproximationExact0100200300400500−190−185−180−175−170−165−10−50510−0.200.20.40.60.811.2  ApproximationExactExpanding equalities like ∂R(V,Z)
= 0 produces a linear system with respect to vi for a ﬁxed Z. In
∂vi
this case, we can obtain the optimal, analytical solution: vi = (H(cid:62)H)−1 (H(cid:62)ψ(xi )). Substituting
the solution in eq. (9), we can eliminate the variable V. To learn the basis vectors, instead of directly
F(cid:88)
optimizing R(V, Z), we can solve the equivalent optimization problem:
R∗ (Z) = − 1
kZ (xi )(cid:62)K−1
argmin
ZZ kZ (xi )
F
Z
i=1
Optimizing R∗ (Z) is tractable because its parameter space is much smaller than R(V, Z). The prob-
lem (11) can be solved using any gradient descent algorithm. For efﬁciency, we use the stochastic
(on-line) gradient descent (SGD) method. SGD applies to problems where the full gradient decom-
poses as a sum of individual gradients of the training samples. The standard (batch) gradient descent
method updates the parameter vector using the full gradient whereas SGD approximates it using the
gradient at a single training sample. For large datasets, SGD is usually much faster than batch gra-
(cid:163)−kZ (xt )(cid:62)K−1
(cid:164)
dient descent. At the t-th iteration, in SCG, we randomly pick a sample xt from the training set and
update the parameter vector based on:
ZZ kZ (xt )
Z(t + 1) = Z(t) − η
∂Z
t
where η is the learning rate. In our implementation, we use D samples (rather than just one) to
compute the gradient. This produces more accurate results and matches the cost of inverting KZZ ,
which is O(D3 ) per iteration.

(11)

∂

(12)

3.2 Random Fourier Set Features

(cid:82)
Another tractable approach to large-scale learning is to approximate the kernel using random fea-
ture maps [22, 23]. For a given function µ(x; θ) and the probability distribution p(θ), one can
deﬁne the local kernel as: kf (x, y) =
p(θ)µ(x; θ)µ(y, θ)dθ . We consider feature maps of
the form µ(x; θ) = cos(ω(cid:62)x + b) with θ = (ω , b), which project local features to a ran-
(cid:113)
domly chosen line, then pass the resulting scalar through a sinusoid. For example, to approxi-
mate the Gaussian kernel kf (x, y) = exp(−γ (cid:107)x − y(cid:107)2 ), the random feature maps are: φ(x) =
(cid:80)
D x + bD )](cid:62) , where bi are drawn from the uniform distribution
D [cos(ω(cid:62)
1 x + b1 ),. . . , cos(ω(cid:62)
2
[−π , π ] and ω are drawn from a Gaussian with 0 mean and covariance 2γ I. Our proposed set-
level feature map is (c.f . §2): φ(X) = 1|X|
x∈X φ(x). Although any shift invariant kernel can
be represented using random Fourier features, currently these are limited to Gaussian kernels or
to kernels with analytical inverse Fourier transforms. In particular, ω needs to be sampled from
the inverse Fourier transform of the corresponding shift invariant kernel. The constraint of a shift-
invariant kernel excludes a number of practically interesting similarities. For example, the χ2 kernel
[8] and the histogram intersection kernel [5] are designed to compare histograms, hence they can
be used as local kernels, if the features are histograms. However, no random Fourier features can
approximate them. Such problems do not occur for the learned low dimensional features—a method-
ology applicable to any Mercer kernel. Moreover, in experiments, we show that kernels based on
low-dimensional approximations (§3.1) can produce superior results when the dimensionality of the
feature maps is small. As seen in ﬁg. 2, for applicable kernels, the random Fourier set features also
produce very competitive results in the higher-dimensional regime.

4 Experiments

We illustrate our methodology in three publicly available computer vision datasets: Scene-15,
Caltech-101 and Caltech-256. For comparisons, we consider four algorithms: BOW-Linear, BOW-
Gaussian, EMK-CKSVD and EMK-Fourier. BOW-Linear and BOW-Gaussian use a linear classiﬁer
and a Gaussian kernel classiﬁer on BOW features, respectively. EMK-CKSVD and EMK-Fourier
use linear classiﬁers. For the former, we learn low dimensional feature maps (§3.1), whereas for the
latter we obtain them using random sampling (§3.2).
All images are transformed into grayscale form. The local features are SIFT descriptors [16] ex-
tracted from 16×16 image patches. Instead of detecting the interest points, we compute SIFT de-
scriptors over dense regular grids with spacing of 8 pixels. For EMK, our local kernel is a Gaussian

5

exp(−γ (cid:107)x − y(cid:107)2 ). We use the same ﬁxed γ = 1 for our SIFT descriptor in all datasets: Scene-15,
Caltech-101 and Caltech-256, although a more careful selection is likely to further improve perfor-
mance. We run k-means clustering to identify the visual words and stochastic gradient descent to
learn the local feature maps, using a 100,000 random set of SIFT descriptors.
Our classiﬁer is a support vector machine (SVM), which is extended to multi-class decisions by
combining one-versus-all votes. We work with LIBLINEAR [3] for BOW-Linear, EMK-Fourier
and EMK-CKSVD, and LIBSVM for BOW-Gaussian (the former need a linear classiﬁer whereas
the latter uses a nonlinear classiﬁer). The regularization and the kernel parameters (if available) in
SVM are tuned by ten-fold cross validation on the training set. The dimensionality of the feature
maps and the vocabulary size are both set to 1000 for fair comparisons, unless otherwise speciﬁed.
We have also experimented with larger vocabulary sizes in BOW, but no substantial improvement
was found (ﬁg. 2). We measure performance based on classiﬁcation accuracy, averaged over ﬁve
random training/testing splits. All experiments are run on a cluster built of compute nodes with 1.0
GHz processors and 8GB memory.
Scene-15: Scene-15 consists of 4485 images labeled into 15 categories. Each category contains 200
to 400 images whose average size is 300×250 pixels. In our ﬁrst experiment, we train models on a
randomly selected set of 1500 images (100 images per category) and test on the remaining images.
We vary the dimensionality of the feature maps (EMK) and the vocabulary size (BOW) from 250
to 2000 with step length 250. For this dataset, we only consider the ﬂat BOW and EMK (only
pyramid level 0) in all experiments. The classiﬁcation accuracy of BOW-Linear, BOW-Gaussian,
EMK-Fourier and EMK-CKSVD is plotted in ﬁg. 2 (left). Our second experiment is similar with
the ﬁrst one, but the dimensionality of the feature maps and the vocabulary size vary from 50 to 200
with step length 25. In our third experiment, we ﬁx the dimensionality of the feature maps to 1000,
and vary the training set size from 300 to 2400 with step length 300. We show the classiﬁcation
accuracy of the four models as a function of the training set size in ﬁg. 2 (right).
We notice that EMK is consistently 5-8% better than BOW in all cases. BOW-Gaussian is about 2
% better than BOW-Linear on average, whereas EMK-CKSVD give are very similar performance
to EMK-Fourier in most cases. We observe that EMK-CKSVD signiﬁcantly outperforms EMK-
Fourier for low-dimensional feature maps, indicating that learned features preserve the values of the
Gaussian kernel better than the random Fourier maps in this regime, see also ﬁg. 1 (center).
For comparisons, we attempted to run the sum match kernel, on the full Scene-15 dataset. However,
we weren’t able to ﬁnish in one week. Therefore, we considered a smaller dataset, by training and
testing with only 40 images from each category. The sum match kernel obtains 71.8% accuracy
and slightly better than EMK-Fourier 71.0% and EMK-CKSVD 71.4% on the same dataset. The
sum match kernel takes about 10 hours for training and 10 hours for testing, respectively whereas
EMK-Fourier and EMK-CKSVD need less than 1 hour, most spent computing SIFT descriptors.
In addition, we use 10,000 randomly selected SIFT descriptors to learn KPCA-based local feature
maps, which takes about 12 hours for the training and testing sets on the full Scene-15 dataset,
respectively. We obtain slightly lower accuracy than EMK-Fourier and EMK-CKSVD. One reason
can be the small sample size, but it is currently prohibitive, computationally, to use larger ones.
Caltech-101: Caltech-101 [15] contains 9144 images from 101 object categories and a background
category. Each category has 31 to 800 images with signiﬁcant color, pose and lighting variations.
Caltech-101 is one of the most frequently used benchmarks for image classiﬁcation, and results ob-
tained by different algorithms are available from the published papers, allowing direct comparisons.
Following the common experimental setting, we train models on 15/30 image per category and test
on the remaining images. We consider three pyramid levels: L = 0, L = 1, amd L = 2 (for the
latter two, spatial information is used). We have also tried increasing the number of levels in the
pyramid, but did not obtain a signiﬁcant improvement.
We report the accuracy of BOW-Linear, BOW-Gaussian, EMK-Fourier and EMK-CKSVD in ta-
ble 2. EMK-Fourier and EMK-CKSVD perform substantially better than BOW-Linear and BOW-
Gaussian for all pyramid levels. The performance gap increases as more pyramid levels are added.
EMK-CKSVD is very close to EMK-Fourier and BOW-Gaussian does not improve over BOW-
Linear much. In table 3, we compare EMK to other algorithms. As we have seen, EMK is compara-
ble to the best-scoring classiﬁers to date. The best result on Caltech101 was obtained by combining
multiple descriptor types [1]. Our main goal in this paper is to analyze the strengths of EMK relative

6

Figure 2: Classiﬁcation accuracy on Scene-15. Left: Accuracy in the high-dimensional regime, and
(center) in the low-dimensional regime. Right: Accuracy as a function of the training set size. The
training set size is 1500 in the left plot; the dimensionality of feature maps and the vocabulary size
are both set to 1000 in the right plot (for fair comparisons).

Algorithms
BOW-Linear
BOW-Gaussian
EMK-Fourier
EMK-CKSVD

Pyramid levels(15 training)
L=0
L=1
L=2
45.0±0.5
41.6± 0.7
37.3± 0.9
46.5±0.6
43.7± 0.7
38.7± 0.8
46.3± 0.7
53.0± 0.6
60.2±0.8
46.6±0.9
53.4±0.8
60.5±0.9

Pyramid levels (30 training)
L=0
L=1
L=2
56.2±0.7
53.0± 0.9
46.2±0.8
58.1±0.6
54.7± 0.8
47.5±0.7
54.0± 0.7
64.1± 0.8
70.1±0.8
54.5±0.8
63.7±0.9
70.3±0.8

Table 2: Classiﬁcation accuracy comparisons for three pyramid levels. The results are averaged
over ﬁve random training/testing splits. The dimensionality of the feature maps and the vocabulary
size are both set to 1000. We have also experimented with large vocabularies, but did not observe
noticeable improvement—the performance tends to saturate beyond 1000 dimensions.

to BOW. Only SIFT descriptors are used in BOW and EMK for all compared algorithms, listed in
table 3. To improve performance, EMK can be conveniently extended to multiple feature types.
Caltech-256: Caltech-256 consists of 30,607 images from 256 object categories and background,
where each category contains at least 80 images. Caltech-256 is challenging due to the large number
of classes and the diverse lighting conditions, poses, backgrounds, images size, etc. We follow the
standard setup and increase the training set from 15 to 60 images per category with step length 15.
In table 4, we show the classiﬁcation accuracy obtained from BOW-Linear, BOW-Gaussian, EMK-
Fourier and EMK-CKSVD. As in the other datasets, we notice that EMK-Fourier and EMK-CKSVD
consistently outperform the BOW-Linear and the BOW-Gaussian.
To compare the four algorithms computationally, we select images from each category proportion-
ally to the total number of images of that category, as the training set. We consider six different
training set sizes: (cid:98)0.3 × 30607(cid:99), . . . , (cid:98)0.8 × 30607(cid:99). The results are shown in ﬁg. 3. To acceler-
ate BOW-Gaussian, we precompute the entire kernel matrix. As expected, BOW-Gaussian is much
slower than the other three algorithms as the training set size increases, for both training and testing.

Algorithms
PMK [5, 6]
HMAX [19]
ML+PMK [9]
KC [28]
SPM [14]
SVM-KNN [31]

15 training
50.0±0.9
51.0
52.2
N/A
56.4
59.1±0.5

30 training
58.2
56.0
62.1
64.0
64.4±0.5
66.2±0.8

Algorithms
kCNN [30]
LDF [4]
ML+CORR [9]
NBNN [1]
EMK-Fourier
EMK-CKSVD

15 training
59.2
60.3
61.0
65.0±1.1
60.2±0.8
60.5±0.9

30 training
67.4
N/A
69.6
73.0
70.1±0.8
70.3±0.8

Table 3: Accuracy comparisons on Caltech-101. EMK is compared with ten recently published
methods. N/A indicates that results are not available. Notice that EMK is used in conjunction with a
linear classiﬁer (linear SVM here) whereas all other methods (except HMAX [19]) require nonlinear
classiﬁers.

7

5001000150020000.60.650.70.750.8DimentionalityAccuracyScene−15  BOW−LinearBOW−GaussianEMK−FourierEMK−CKSVD501001502000.60.650.70.75DimentionalityAccuracyScene−15  BOW−LinearBOW−GaussianEMK−FourierEMK−CKSVD501001500.50.550.60.650.70.750.8Training Set SizeAccuracyScene−15  BOW−LinearBOW−GaussianEMK−FourierEMK−CKSVDAlgorithms BOW-Linear BOW-Gaussian EMK-Fourier EMK-CKSVD
23.2±0.6
22.6±0.7
19.1±0.8
17.4±0.7
15 training
30.5±0.4
30.1±0.5
24.4±0.6
22.7±0.4
30 training
26.9±0.3
28.3±0.5
34.1±0.5
34.4±0.4
45 training
29.3±0.6
30.9±0.4
37.4±0.6
37.6±0.5
60 training

Table 4: Accuracy on Caltech-256. The results are averaged over ﬁve random training/testing splits.
The dimensionality of the feature maps and the vocabulary size are both set to 1000 (for fair com-
parisons). We use 2 pyramid levels.

Figure 3: Computational costs on Caltech-256. Left: training time as a function of the training set
size. Right: testing time as a function of the training set size. Testing time is in seconds per 100
samples. Flat BOW and EMK are used (no pyramid, L = 0). Notice that PMK has a similar training
and testing cost with BOW-Gaussian.

Nonlinear SVMs takes O(n2 ∼ n3 ) even when a highly optimized software package like LIBSVM
is used. For large n, the SVM training dominates the training cost. The testing time of BOW-
Gaussian is linear in the training set size, but constant for the other three algorithms. Although we
only experiment with a Gaussian kernel, a similar complexity would be typical for other nonlinear
kernels, as used in [6, 9, 14, 31, 4].

5 Conclusion

We have presented efﬁcient match kernels for visual recognition, based on a novel insight that
popular bag-of-words representations used in conjunction with linear models can be viewed as a
special type of match kernel which counts 1 if two local features fall into the same regions parti-
tioned by visual words and 0 otherwise. We illustrate the quantization limitations of such models
and propose more sophisticated kernel approximations that preserve the computational efﬁciency of
bag-of-words while being just as (or more) accurate than the existing, computationally demanding,
non-linear kernels. The models we propose are built around Efﬁcient Match Kernels (EMK), which
map local features to a low dimensional feature space, average the resulting feature vectors to form
a set-level feature, then apply a linear classiﬁer. In experiments, we show that EMK are efﬁcient and
achieve state of the art classiﬁcation results in three difﬁcult computer vision datasets: Scene-15,
Caltech-101 and Caltech-256.

Acknowledgements: This research was supported, in part, by awards from NSF (IIS-0535140) and
the European Commission (MCEXT-025481). Liefeng Bo thanks Jian Peng for helpful discussions.

References
[1] O. Boiman, E. Shechtman, and M. Irani. In defense of nearest-neighbor based image classiﬁ-
cation. In CVPR, 2008.
[2] M. Cuturi and J. Vert. Semigroup kernels on ﬁnite sets. In NIPS, 2004.
[3] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. Liblinear: A library for large linear classi-
ﬁcation. JMLR, 9:1871–1874, 2008.

8

11.522.5x 104051015x 104Trainning set sizeTraining time (seconds)Caltech−256  BOW−LinearBOW−GaussianEMK−FourierEMK−CKSVD11.522.5x 10420406080100120Trainning set sizeTesting time (seconds)Caltech−256  BOW−LinearBOW−GaussianEMK−FourierEMK−CKSVDImage retrieval and classiﬁcation using local distance

[4] A. Frome, Y. Singer, and J. Malik.
functions. In NIPS, 2006.
[5] K. Grauman and T. Darrell. The pyramid match kernel: discriminative classiﬁcation with sets
of image features. In ICCV, 2005.
[6] K. Grauman and T. Darrell. The pyramid match kernel: Efﬁcient learning with sets of features.
JMLR, 8:725–760, 2007.
[7] D. Haussler. Convolution kernels on discrete structures. Technical report, 1999.
[8] Zhang J., Marszalek M., Lazebnik S., and Schmid C. Local features and kernels for classiﬁca-
tion of texture and object categories: A comprehensive study. IJCV, 73(2):213–238, 2007.
[9] P. Jain, B. Kulis, and K. Grauman. Fast image search for learned metrics. In CVPR, 2008.
[10] F. Jurie and B. Triggs. Creating efﬁcient codebooks for visual recognition. In ICCV, 2005.
[11] K. Kim, M. Franz, and B. Sch ¨olkopf. Iterative kernel principal component analysis for image
modeling. PAMI, 27(9):1351–1366, 2005.
[12] R. Kondor and T. Jebara. A kernel between sets of vectors. In ICML, 2003.
[13] A. Kumar and C. Sminchisescu. Support kernel machines for object recognition. In ICCV,
2007.
[14] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for
recognizing natural scene categories. In CVPR, 2006.
[15] F. Li, R. Fergus, and P. Perona. One-shot learning of object categories. PAMI, 28(4):594–611,
2006.
[16] D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004.
[17] S. Lyu. Mercer kernels for object recognition with local features. In CVPR, 2005.
[18] P. Moreno, P. Ho, and N. Vasconcelos. A kullback-leibler divergence based kernel for svm
classiﬁcation in multimedia applications. In NIPS, 2003.
[19] J. Mutch and D. Lowe. Multiclass object recognition with sparse, localized features. In CVPR,
2006.
[20] M. Parsana, S. Bhattacharya, C. Bhattacharyya, and K. Ramakrishnan. Kernels on attributed
pointsets with applications. In NIPS, 2007.
[21] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Lost in quantization: Improving
particular object retrieval in large scale image databases. In CVPR, 2008.
[22] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
[23] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In NIPS, 2008.
[24] B. Sch ¨olkopf, A. Smola, and K. M ¨uller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Computation, 10:1299–1319, 1998.
[25] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for svm. In ICML, pages 807–814. ACM, 2007.
[26] A. Shashua and T. Hazan. Algebraic set kernels with application to inference over local image
representations. In NIPS, 2004.
[27] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in
videos. In ICCV, 2003.
[28] J. van Gemert, J. Geusebroek, C. Veenman, and A. Smeulders. Kernel codebooks for scene
categorization. In ECCV, 2008.
[29] L. Wolf and A. Shashua. Learning over sets using kernel principal angles. JMLR, 4:913–931,
2003.
[30] K. Yu, W. Xu, and Y. Gong. Deep learning with kernel regularization for visual recognition.
In NIPS, 2008.
[31] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Discriminative nearest neighbor classi-
ﬁcation for visual category recognition. In CVPR, 2006.

9


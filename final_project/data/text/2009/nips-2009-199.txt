Time-Varying Dynamic Bayesian Networks

Le Song, Mladen Kolar and Eric P. Xing
School of Computer Science, Carnegie Mellon University
{lesong, mkolar, epxing}@cs.cmu.edu

Abstract

Directed graphical models such as Bayesian networks are a favored formalism
for modeling the dependency structures in complex multivariate systems such as
those encountered in biology and neural science. When a system is undergo-
ing dynamic transformation, temporally rewiring networks are needed for cap-
turing the dynamic causal inﬂuences between covariates. In this paper, we pro-
pose time-varying dynamic Bayesian networks (TV-DBN) for modeling the struc-
turally varying directed dependency structures underlying non-stationary biologi-
cal/neural time series. This is a challenging problem due the non-stationarity and
sample scarcity of time series data. We present a kernel reweighted (cid:96)1 -regularized
auto-regressive procedure for this problem which enjoys nice properties such as
computational efﬁciency and provable asymptotic consistency. To our knowledge,
this is the ﬁrst practical and statistically sound method for structure learning of TV-
DBNs. We applied TV-DBNs to time series measurements during yeast cell cycle
and brain response to visual stimuli. In both cases, TV-DBNs reveal interesting
dynamics underlying the respective biological systems.

Introduction
1
Analysis of biological networks has led to numerous advances in understanding the organizational
principles and functional properties of various biological systems, such as gene regulatory sys-
tems [1] and central nervous systems [2]. However, most such results are based on static networks,
that is, networks with invariant topology over a given set of biological entities. A major challenge in
systems biology is to understand and model, quantitatively, the dynamic topological and functional
properties of biological networks. We refer to these time or condition speciﬁc biological circuitries
as time-varying networks or structural non-stationary networks, which are ubiquitous in biological
systems. For example (i) over the course of a cell cycle, there may exist multiple biological “themes”
that determine functions of each gene and their regulatory relations, and these “themes” are dynamic
and stochastic. As a result, the molecular networks at each time point are context-dependent and
can undergo systematic rewiring rather than being invariant over time [3]. (ii) The emergence of
a uniﬁed cognitive moment relies on the coordination of scattered mosaics of functionally special-
ized brain regions. Neural assemblies, distributed local networks of neurons transiently linked by
dynamic connections, enable the emergence of coherent behaviour and cognition [4].
A key technical hurdle preventing us from an in-depth investigation of the mechanisms that drive
temporal biological processes is the unavailability of serial snapshots of time-varying networks un-
derlying biological processes. Current technology does not allow for experimentally determining a
series of time speciﬁc networks for a realistic dynamic biological system. Usually, only time series
measurements of the activities of the nodes can be made, such as microarray, EEG or fMRI. Our
goal is to recover the latent time-varying networks underlying biological processes, with temporal
resolution up to every single time point based on time series measurements of the nodal states. Re-
cently, there has been a surge of interests along this direction [5, 6, 7, 8, 9, 10]. However, most
existing approaches are computationally expensive, making large scale genome-wide reverse engi-
neering nearly infeasible. Furthermore, these methods also lack formal statistical characterization of

1

the estimation procedure. For instance, non-stationary dynamic Bayesian networks are introduced
in [9], where the structures are learned via MCMC sampling; such approach is not likely to scale
up to more than 1000 nodes and without a regularization term it is also prone to overﬁtting when
the dimension of the data is high but the number of observations is small. More recent efforts have
focused on efﬁcient kernel reweighted or total-variation penalized sparse structure recovery meth-
ods for undirected time-varying networks [10, 11, 12], which possess both attractive computational
schemes and rigorous statistical consistency results. However, what has not been addressed so far is
how to recover directed time-varying networks. Our current paper advances in this direction.
More speciﬁcally, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling
the directed time-evolving network structures underlying non-stationary biological time series. To
make this problem statistically tractable, we rely on the assumption that the underlying network
structures are sparse and vary smoothly across time. We propose a kernel reweighted (cid:96)1 -regularized
auto-regressive approach for learning this sequence of networks. Our approach has the following at-
tractive properties: (i) The aggregation of observations from adjacent time points by kernel reweight-
ing greatly alleviates the statistical problem of sample scarcity when the networks can change at
each time point whereas only one or a few time series replicates are available. (ii) The problem
of structural estimation for a TV-DBN decomposes into a collection of simpler and atomic struc-
tural learning problems. We can choose from a battery of highly scalable (cid:96)1 -regularized least-square
solvers for learning each structure. (iii) We can formally characterize the conditions under which our
estimation procedure is structurally consistent: as time series are sampled in increasing resolution,
our algorithm can recover the true structure of the underlying TV-DBN with high probability.
It is worth emphasizing that our approach is very different from earlier approaches, such as the
structure learning algorithms for dynamic Bayesian networks [13], which learn time-homogeneous
dynamic systems with ﬁxed node dependencies, or approaches which start from an a priori static
network and then trace time-dependent activities [3]. The Achilles’ heel of this latter approach
is that edges that are transient over a short period of time may be missed by the summary static
network in the ﬁrst place. Furthermore, our approach is also different from change point based al-
gorithms [14, 8] which ﬁrst segment time series and then ﬁt an invariant structure to each segment.
These approaches can only recover piece-wise stationary models rather than constantly varying net-
works. In our experiments, we demonstrate the advantange of TV-DBNs using synthetic networks.
We also apply TV-DBNs to real world datasets: a gene expression dataset measured during yeast
cell cycle; and an EEG dataset recorded during a motor imagination task. In both cases, TV-DBNs
reveal interesting time-varying causal structures of the underlying biological systems.

2 Preliminary
We concern ourselves with stochastic processes in time or space domains, such as the dynamic con-
trol of gene expression during cell cycle, or the sequential activation of brain areas during cognitive
decision making, of which the state of a variable at one time point is determined by the states of a
set of variables at previous time points. Models describing a stochastic temporal processes can be
naturally represented as dynamic Bayesian networks (DBN) [15]. Taking the transcriptional regula-
p )(cid:62) ∈ Rp be a vector representing the
tion of gene expression as an example, let X t := (X t
1 , . . . , X t
expression levels of p genes at time t, a stochastic dynamic process can be modeled by a “ﬁrst-order
Markovian transition model” p(X t |X t−1 ), which deﬁnes the probabilistic distribution of gene ex-
pressions at time t given those at time t − 1. Under this assumption, the likelihood of the observed
p(X t |X t−1 ) = p(X 1 ) (cid:89)T
p(X 1 , . . . , X T ) = p(X 1 ) (cid:89)T
(cid:89)p
expression levels of these genes over a time series of T steps can be expressed as:
i |X t−1
),
p(X t
(1)
πi
i=1
t=2
t=2
individual genes, i.e., (cid:81)
where we assume that the topology of the networks is speciﬁed by a set of regulatory relations
:= {X t−1
i }, and hence the transition model p(X t |X t−1 ) factors over
: X t−1
X t−1
regulates X t
πi
j
j
i |X t−1
i |X t−1
i p(X t
). Each p(X t
) can be viewed as a regulatory gate func-
πi
πi
tion that takes multiple covariates (regulators) and produce a single response.
A simple form of the transition model p(X t |X t−1 ) in a DBN is a linear dynamics model:
 ∼ N (0, σ2I ),
X t = A · X t−1 + ,
(2)
where A ∈ Rp×p is a matrix of coefﬁcients relating the expressions at time t − 1 to those of the
next time point, and  is a vector of isotropic zero mean Gaussian noise with variance σ2 . In this

2

i |X t−1
) can be expressed as a
case, the gate function that deﬁnes the conditional distribution p(X t
πi
i |X t−1
) = N (X t
i ; Ai·X t−1 , σ2 ), where Ai· denotes the ith row of
univariate Gaussian, i.e., p(X t
πi
the matrix A. This model is also known as an auto-regressive model.
The major reason for favoring DBNs over standard Bayesian networks (BN) or undirected graph-
ical models is its enhanced semantic interpretability. An edge in a BN does not necessarily imply
causality due to the Markov equivalence of different edge conﬁgurations in the network [16]. In
DBNs (of the type deﬁned above), all directed edges only point from time t − 1 to t, which bear a
natural causal implication and are more likely to suggest regulatory relations. The auto-regressive
model in (2) also offers an elegant formal framework for consistent estimation of the structures of
DBNs; we can read off the edges between variables in X t−1 and X t by simply identifying the
nonzero entries in the transition matrix A. For example, the non-zero entries of Ai· represent the
set of regulator Xπi that directly lead to a response on Xi .
Contrary to the name of dynamic Bayesian networks may suggest, DBNs are time-invariant models
and the underlying network structures do not change over time. That is, the dependencies between
variables in X t−1 and X t are ﬁxed, and both p(X t |X t−1 ) and A are invariant over time. The term
“dynamic” only means that the DBN can model dynamical systems. In the sequel, we will present a
new formalism where the structures of DBNs are time-varying rather than invariant.

3 A New Formalism: Time-Varying Dynamic Bayesian Networks
We will focus on recovering the directed time-varying network structure (or the locations of non-
zero entries in A) rather than the exact edge values. This is related to the structure estimation
problems studied in [11, 12], but in our case for auto-regressive models (and hence directed net-
works). Structure estimation results in parse models for easy interpretation, but it is statistically
more challenging than the value estimation problem. This is also different from estimating a non-
stationary model in the conventional sense, where one interests in recovering the exact values of the
(cid:32)0
(cid:33)
(cid:32)0
(cid:33)
(cid:32)0 1
(cid:33)
varying coefﬁcients [17, 18]. To make this distinction clear, we use the following 3 examples:
0.1
0
0
1.1
0
3
0
1
0 0
0 0
0
0
0
0
0

0.1
0
0

1
0
0.1

,

B3 =

,

B2 =

B1 =

.

(3)

Matrices B1 and B2 encode the same graph structure, since the locations of their non-zero entries
are exactly same. Although B1 is closer to B3 than B2 in terms of matrix values (eg. measured in
Frobenius norm), they encodes very different graph strucutres.
Formally, let graph G t = (V , E t ) represents the conditional independence relations between the
components of random vectors X t−1 and X t . The vertex set V is a common set of variables
underlying X 1:T , i.e., each node in V corresponds to a sequence of variables X 1:T
. The edge set
E t ⊆ V × V contains directed edges from components of X t−1 to those of X t ; an edge (i, j ) (cid:54)∈ E t
i
i is conditionally independent of X t−1
if and only if X t
given the rest of the variables in the model.
j
Due to the time-varying nature of the networks, the transition model pt (X t |X t−1 ) in (1) becomes
time dependent. In the case of the auto-regressive DBN in (2), its time-varying extension becomes:
X t = At · X t−1 + ,
 ∼ N (0, σ2I ),
be recovered via E t = (cid:8)(i, j ) ∈ V × V | i (cid:54)= j, At
ij (cid:54)= 0(cid:9).
and our goal is to estimate the non-zero entries in the sequence of time dependent transition matrices
{At} (t = 1 . . . T ). The directed edges E t := E t (At ) in network G t associated with each At can
4 Estimating Time-Varying DBN

(4)

Note that if we follow the naive assumption that each temporal snapshot is a completely different
network, the task of jointly estimating {At} by maximizing the log-likelihood would be statistically
impossible because the estimator would suffer from extremely high variance due to sample scarcity.
Therefore, we make a statistically tractable yet realistic assumption that the underlying network
structures are sparse and vary smoothly across time; and hence temporally adjacent networks are
likely to share common edges than temporally distal networks.

3

1
T

,

(5)

wt∗

i − At∗
i· xt−1 )2 + λ
(t)(xt

Overall, we have designed a procedure that decomposes the problem of estimating the time-varying
networks along two orthogonal axes. The ﬁrst axis is along the time, where we estimate the network
for each time point separately by reweighting the observations accordingly; and the second axis is
along the set of genes, where we estimate the neighborhood for each gene separately and then join
these neighborhoods to form the overall network. One beneﬁt of such decomposition is that the
estimation problem is reduced to a set of atomic optimizations, one for each node i (i = 1 . . . |V |) at
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)At∗
(cid:88)T
each time point t∗ (t∗ = 1 . . . T ):
ˆAt∗
i· = argmin
i·
At∗
i· ∈R1×n
t=1
where λ is a parameter for the (cid:96)1 -regularization term, which controls the number of non-zero en-
tries in the estimated ˆAt∗
i· , and hence the sparsity of the networks; wt∗ (t) is the weighting of an
observation from time t when we estimate the network at time t∗ . More speciﬁcally, it is deﬁned as
PT
wt∗ (t) = Kh (t−t∗ )
t=1 Kh (t−t∗ ) , where Kh (·) = K ( ·
h ) is a symmetric nonnegative kernel function and h is
the kernel bandwidth. We use a Gaussian RBF kernel, Kh (t) = exp(− t2
h ), in our later experiments.
Note that multiple measurements at the same time point are considered as i.i.d. observations and can
be trivially handled by assigning them the same weights.
The objective deﬁned in (5) is essentially a weighted regression problem. The square loss function
is due to the fact that we are ﬁtting a linear model with uncorrelated Gaussian noise. Two other
key components of the objective are: (i) a kernel reweighting scheme for aggregating observations
across time; and (ii) an (cid:96)1 -regularization for sparse structure estimation. The ﬁrst component origi-
nates from our assumption that the structural changes of the network vary smoothly across time. This
assumption allows us to borrow information across time by reweighting the observations from dif-
ferent time points and then treating them as if they were i.i.d. observations. Intuitively, the weighting
should place more emphasis on observations at or near time point t∗ with weights becoming smaller
as observations move further away from time point t∗ . The second component is to promote sparse
structure and avoid model overﬁtting. This is also consistent with the biological observation that net-
works underlying biological processes are parsimonious in structure. For example, a transcription
factor only controls a small fraction of target genes at particular time point or under a speciﬁc con-
dition [19]. It is well-known that (cid:96)1 -regularized least square linear regression, has a parsimonious
property, and exhibits model-selection consistency (i.e., recovers the set of true non-zero regression
coefﬁcients asymptotically) in noisy settings even when p (cid:29) T [20].
of higher order D : X t = (cid:80)D
Note that our procedure can also be easily extended to learn the structure of auto-regressive models
d=1 At (d) · X t−d + ,
 ∼ N (0, σ2I ). The change we need to
i − (cid:80)D
make is to incorporate the higher order auto-regressive coefﬁcients in the square loss function, i.e.,
i· (d)xt−d )2 , and penalize the (cid:96)1 -norms of these At∗
d=1 At∗
i· (d) correspondingly.
(xt
5 Optimization

Estimating time-varying networks using the decomposition scheme above requires solving a collec-
tion of optimization problems in (5). In a genome-wide reverse engineering task, there can be tens
of thousands of genes and hundreds of time points, so one can easily have a million optimization
problems. Therefore, it is essential to use an efﬁcient algorithm for solving the atomic optimization
wt∗ (t) into the square loss function by scaling the covariates and response variables by (cid:112)wt∗ (t),
problem in (5), which can be trivially parallelized for each genes at each time point.
i and ˜xt−1 ← (cid:112)wt∗ (t)xt−1 . After this transformation, the optimization
i ← (cid:112)wt∗ (t)xt
Instead of solving the form of the optimization problem in (5), we will push the weighting
i.e. ˜xt
problem becomes a standard (cid:96)1 -regularized least-square problem which can be solved via a bat-
(cid:80)T
tery of highly scalable and specialized solvers, such as the shooting algorithm [21]. The shooting
algorithm is a simple, straightforward and fast algorithm that iteratively solves a system of nonlin-
i· ˜xt−1 − ˜xt
i ) ˜xt−1
t=1 (At∗
j =
ear equations related to the optimality condition of problem (5): 2
T
−λ sign(At∗
ij ) (∀j = 1 . . . p). At each iteration of the shooting algorithm, one entries of Ai· is up-
dated by holding all other entries ﬁxed. Overall, our procedure for estimating time-varying networks
is summarized in Algorithm 1, which uses the shooting algorithm as the key building block (step

4

Algorithm 1: Procedure for Estimating Time-Varying DBN
Input: Time series {x1 , . . . , xT }, regularization parameter λ and kernel parameter h.
Output: Time-varying networks {A1 , . . . , AT }.
begin
Introduce variable A0 and randomly initialize it
for i = 1 . . . p do
for t∗ = 1 . . . T do
i , ˜xt−1 ← (cid:112)wt∗ (t)xt−1 (t = 1 . . . T )
i ← (cid:112)wt∗ (t)xt
i· ← At∗−1
Initialize: At∗
i·
Scale time series: ˜xt
(cid:80)T
t=1 ((cid:80)
(cid:80)T
while At∗
i· not converges do
for j = 1 . . . p do
Compute: Sj ← 2
k − ˜xt
t=1 ˜xt−1
i ) ˜xt−1
ik ˜xt−1
k (cid:54)=j At∗
, bj = 2
j
j
T
T
ij ← (sign(Sj − λ)λ − Sj )/bj , if |Sj | > λ, otherwise 0
Update: At∗

1
2
3
4
5
6
7
8

9

10

˜xt−1
j

11

end

7-10). In step 5, we uses a warm start for each atomic optimization problem: since the networks
vary smoothly across time, we can use At∗−1
as a good initialization for At∗
i· for further speedup.
i·

6 Statistical Properties

In this section, we study the statistical consistency of the estimation procedure in Section 4. Our
analysis is different from the consistency results presented by [11] on recovering time-varying undi-
rected graphical models. Their analysis deals with Frobenius norm consistency which is a weaker
result than the structural consistency we pursue here. Our structural consistency result for TV-DBNs
estimation procedure follows the proof strategy of [20]; however, the analysis is complicated by two
major factors. First, times series observations are very often non-i.i.d.— current observations may
depend on past history. Second, we are modeling non-stationary processes, where we need to deal
with the additional bias term that arises due to locally stationary approximation to non-stationarity.
In the following, we state our assumptions and theorem, but leave the detailed proof of this theorem
for a full version of the paper (a sketch of the proof can be found in the appendix).

Theorem 1 Assume that the conditions below hold:

1. Elements of At are smooth functions with bounded second derivatives, i.e. there exists a
| ∂ 2
ij | < L and
ij | < L.
constant L > 0 s.t. | ∂
∂ t2 At
∂ t At
2. The minimum absolute value of non-zero elements of At is bounded away from zero at
observation points, and this bound tends to zero as we observe more and more samples,
|At
ij | > 0.
i.e., amin := mint∈{1/T ,2/T ,...,1} mini∈[p],j∈S t
i
3. Let Σt = E[X t (X t )T ] = [σij (t)]p
i,j=1 and let S t
i denote the set of non-zero elements of
ij (cid:54)= 0}. Assume that there exist a
i = {j ∈ [p]
: At
the i-th row of the matrix At , i.e. S t
s , ∀i ∈ [p], t ∈ [0, 1], where s is an upper
constant d ∈ (0, 1] s.t. maxj∈S t
i ,k (cid:54)=j |σjk (t)| ≤ d
bound on the number of non-zero elements, i.e. s = maxt∈[0,1] maxi∈[p] |S t
i |.
4. The kernel K (·) : R (cid:55)→ R is a symmetric function and has bounded support on [0, 1]. There
exists a constant MK s.t. maxx∈R |K (x)| ≤ MK and maxx∈R K (x)2 ≤ MK .
Let the regularization parameter scale as λ = O((cid:112)(log p)/T h), the minimum absolute non-zero
entry amin of At∗ be sufﬁciently large (amin ≥ 2λ). If h = O(T 1/3 ) and s log p
T h = o(1) then

P[supp( ˆAt∗

) = supp(At∗

)] → 1, T → ∞,

∀t∗ ∈ [0, 1].

(6)

5

7 Experiments

To the best of our knowledge, this is the ﬁrst practical method for structure learning of non-stationary
DBNs. Thus we mainly compare with static DBN structure learning methods. The goal is to demon-
strate the advantage of TV-DBNs for modeling time-varying structures of non-stationary processes
which are ignored by traditional approaches. We conducted 3 experiments using synthetic data,
gene expression data and EEG signals. In these experiments, TV-DBNs either better recover the
underlying networks, or provide better explanatory power for the underlying biological processes.
Synthetic Data In this experiment, we generate synthetic time series using a ﬁrst order auto-
regressive models with smoothly varying model structures. More speciﬁcally, we ﬁrst generate
8 different anchor transition matrices At1 . . . At8 , each of which corresponds to an Erd ¨os-R ´enyi
random graph of node size p = 50 and average indegree of 2 (we have also experimented with
p = 75 and 100 which provides similar results). We then evenly space these 8 anchor matrices,
and interpolate a suitable number of intermediate matrices to match the number of observations
T . Due to the interpolation, the average indegree of each node is around 4. With the sequence of
{At}(t = 1 . . . T ), we simulate the time series according to equation (4) with noise variance σ2 = 1.
We then study the behavior of TV-DBNs and static DBNs [22] in recovering the underlying varying
networks as we increase the number of observations T . We also compare with a piecewise constant
DBN that estimate a static network for each segment obtained from change point detection [14].
For the TV-DBN, we choose the bandwidth parameter h of the Gaussian kernel according to
the spacing between two adjacent anchor matrices (T /7) such that exp(− T 2
49h ) = exp(−1).
For all methods, we choose the regularization parameter such that the resulting networks has
an average indegree of 4. We evaluate the performance using an F1 score, which is the har-
monic mean of precision and recall scores in retrieving the true time-varying network edges.

Figure 1: F1 score of estimating time-
varying networks for different methods.

We can see that estimating a static DBN or a piecewise
constant DBN does not provide a good estimation of the
network structures (Figure 1). In contrast, the TV-DBN
leads to a signiﬁcantly higher F1 score, and its perfor-
mance also beneﬁt quickly from increasing the number
of observations. Note that these results are not surpris-
ing since time-varying networs simply ﬁt better with the
data generating process. As time-varying networks occur
often in biological systems, we expect TV-DBNs will be
useful for studying biological systems.
Yeast Gene Regulatory Networks. In this experiment, we will reverse engineer the time varying
gene regulatory networks from time series of gene expression measured across two yeast cell cycles.
A yeast cell cycle is divided into four stages: S phase for DNA synthesis, M phase for mitosis,
and G1 and G2 phase separating S and M phases. We use two time series (alpha30 and alpha38)
from [23] which are technical replicates of each other with a sampling interval of 5 minutes and
a total of 25 time points across two yeast cell cycles. We consider a set of 3626 genes which
are common to both arrays. We choose the bandwidth parameter h such that the weighting decay
to exp(−1) for half of a cell cycle, i.e. exp(−62/h) = exp(−1). We choose the regularization
parameter such that the sparsity of the networks are around 0.01.
During the cell cycle of yeasts, there exist multiple underlying “themes” that determine the func-
tionalities of each gene and their relationships to each other, and such themes are dynamical and
stochastic. As a result, the gene regulatory networks at each time point are context-dependent and
can undergo systematic rewiring, rather than being invariant over time. A summary of the estimated
time-varying networks are visualized in Figure 2. We group genes according to 50 ontology groups.
We can see that the most active groups of genes are related to background processes such as cy-
toskeleton organization, enzyme regulator activity, ribosome activity. We can also spot transient
interactions, for instance, between genes related to site of polarized growth and nucleolus (time
point 18), and between genes related to ribosome and cellular homeostasis (time point 24). Note
that, although gene expressions are measured across two cell cycles, the values do not necessarily
exhibit periodic behavior. In fact, only a small fraction of yeast genes (less than 20%) has been
reported to exhibit cycling behavior [23].

6

(b) t2

(c) t4

(d) t6

(e) t8

(f) t10

(g) t12

(h) t14

(i) t16

(j) t18

(k) t20

(l) t22

(m) t24

(a) t1
Figure 2: Interactions between gene ontological groups. The weight of an edge between two ontological
groups is the total number of connection between genes in the two groups. We thresholded the edge weight
such that only the dominant interactions are displayed.
Table 1: The number of enriched unique gene
sets discovered by the static and time-varying
Next we study genes sets that are related to speciﬁc
networks respectively. Here we are interested
stage of cell cycle where we expect to see periodic be-
in recall score: the time-varying networks better
havior.
In particular, we obtain gene sets known to
models the biological system.
be related to G1, S and S/G2 stage respectively.1 We
DBN TV-DBN
use interactivity, which is the total number of edges a
23
7
TF
group of genes is connected to, to describe the activity
7
26
Knockout
of each group of genes. Since the regulatory networks
77
13
Ontology
are directed, we can examine both indegree and out-
degree separately for each gene sets. In Figure 3(a)(b)(c), the interactivities of these genes indeed
exhibit periodic behavior which corresponds well with their supposed functions in cell cycles.
We also plot the histogram of indegree and outdegree (averaged across time) for the time-varying
networks in Figure 3(d). We ﬁnd that the outdegrees approximately follow a scale free distribution
with largest outdegree reaching 90. This corresponds well with the biological observation that there
are a few genes (regulators) that regulate a lot of other genes. The indegree distribution is very dif-
ferent from that of the outdegree, and it exhibits a clear peak between 5 and 6. This also corresponds
well with biological observations that most genes are controlled only by a few regulators.
To further assess the modeling power of the time-varying networks and its advantage over static
network, we perform gene set enrichment studies. More speciﬁcally, we use three types of infor-
mation to deﬁne the gene sets: transcription factor binding targets (TF), gene knockout signatures
(Knockout), and gene ontology (Ontology) groups [24]. We partition the genes in the time varying
networks at each time point into 50 groups using spectral clustering, and then test whether these
groups are enriched with genes from any predeﬁned gene sets. We use a max-statistic and a 99%
conﬁdence level for the test [25]. Table 1 indicates that time-varying networks are able to discover
more functional groups as deﬁned by the genes sets than static networks as commonly used in bio-
logical literature. In the appendix, we also visualize the time spans of these active functional groups.
It can be seen that many of them are dynamic and transient, and not captured by a static network.
Brain Response to Visual Stimuli. In this experiment, we will explore the interactions between
brain regions in response to visual stimuli using TV-DBNs. We use the EEG dataset from [26]
where ﬁve healthy subjects (labeled ‘aa’, ‘al’, ‘av’, ‘aw’ and ‘ay’ respectively) were required to
imagine body part movement based on visual cues in order to generate EEG changes. We focus our

1We obtain gene sets from http://genome-www.stanford.edu/cellcycle/data/rawdata/KnowGenes.doc.

7

(d)
(c)
(b)
(a)
Figure 3: (a) Genes speciﬁc to G1 phase are being regulated periodically; we can see that the average in-
degree of these genes increases during G1 stage and starts to decline right after the G1 phase. (b) S phase
speciﬁc genes periodically regulate other genes; we can see that the average outdegree of these genes peaks at
the end of S phase and starts to decline right after S phase. (c) The interactivity of S/G2 speciﬁc genes also
show nice correspondence with their functional roles; we can see that the average outdegree increases till G2
phase and then starts to decline. (d) Indegree and outdegree distribution averaged over 24 time points.
t = 1.0s
t = 1.5s
t = 2.0s
t = 2.5s
SB

al

av
Figure 4: Temporal progression of brain interactions for subject ‘al’ and BCI “illiterate” ‘av’. The plot for the
other 3 subjects can be found in the appendix. The dots correspond to EEG electrode positions in 10-5 system.

analysis on trials related to right hand imagination, and signals in the window [1.0, 2.5] second after
the visual cue is presented. We bandpass ﬁlter data at 8–12 Hz to obtain EEG alpha activity. We
further normalize each EEG channel to zero mean and unit variance, and estimate the time-varying
networks for all 5 subject using exactly the same regularization parameter and kernel bandwidth
(h s.t. exp(−(0.5)2/h) = exp(−1)). We tried a range of different regularization parameters, but
obtained qualitatively similar results to Figure 4.
What is particularly interesting in this dataset is that subject ‘av’ is called BCI “illiterate”; he/she
is unable to generate clear EEG changes during motor imagination. The estimated time-varying
networks reveal that the brain interactions of subject ‘av’ is particularly weak and the brain con-
nectivity actually decreases as the experiment proceeds. In contrast, all other four subjects show an
increased brain interaction as they engage in active imagination. Particularly, these increased inter-
actions occur between visual and motor cortex. This dynamic coherence between visual and motor
cortex corresponds nicely to the fact that subjects are consciously transforming visual stimuli into
motor imaginations which involves the motor cortex. It seems that subject ‘av’ fails to perform such
integration due to the disruption of brain interactions.

8 Conclusion
In this paper, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling the
varying network structures underlying non-stationary biological time series. We have designed a
simple and scalable kernel reweighted structural learning algorithm to make the learning possible.
Given the rapid advances in data collection technologies for biological systems, we expect that com-
plex, high-dimensional, and feature rich data from complex dynamic biological processes, such as
cancer progression, immune responses, and developmental processes, will continue to grow. Thus,
we believe our new method is a timely contribution that can narrow the gap between imminent
methodological needs and the available data and offer deeper understanding of the mechanisms and
processes underlying biological networks.
Acknowledgments LS is supported by a Ray and Stephenie Lane Research Fellowship. EPX is supported
by grant ONR N000140910758, NSF DBI-0640543, NSF DBI-0546594, NSF IIS-0713379 and an Alfred P.
Sloan Research Fellowship. We also thank Grace Tzu-Wei Huang for helpful discussions.

8

References
[1] A. L. Barabasi and Z. N. Oltvai. Network biology: Understanding the cell’s functional organization.
Nature Reviews Genetics, 5(2):101–113, 2004.
[2] Francisco Varela, Jean-Philippe Lachaux, Eugenio Rodriguez, and Jacques Martinerie. The brainweb:
Phase synchronization and large-scale integration. Nature Reviews Neuroscience, 2:229–239, 2001.
[3] N. Luscombe, M. Babu, H. Yu, M. Snyder, S. Teichmann, and M. Gerstein. Genomic analysis of regula-
tory network dynamics reveals large topological changes. Nature, 431:308–312, 2004.
[4] Eugenio Rodriguez, Nathalie George, Jean-Philippe Lachaux, Jacques Martinerie, Bernard Renault, and
Francisco J. Varela1. Perception’s shadow: long-distance synchronization of human brain activity. Nature,
397(6718):430–433, 1999.
[5] M. Talih and N. Hengartner. Structural learning with time-varying components: Tracking the cross-
section of ﬁnancial time series. J. Royal Stat. Soc. B, 67(3):321C341, 2005.
[6] S. Hanneke and E. P. Xing. Discrete temporal models of social networks. In Workshop on Statistical
Network Analysis, ICML06, 2006.
[7] F. Guo, S. Hanneke, W. Fu, and E. P. Xing. Recovering temporally rewiring networks: A model-based
approach. In International Conference in Machine Learning, 2007.
[8] X. Xuan and K. Murphy. Modeling changing dependency structure in multivariate time series. In Inter-
national Conference in Machine Learning, 2007.
[9] J. Robinson and A. Hartemink. Non-stationary dynamic bayesian networks. In Neural Information Pro-
cessing Systems, 2008.
[10] Amr Ahmed and Eric P. Xing. Tesla: Recovering time-varying networks of dependencies in social and
biological studies. Proceeding of the National Academy of Sciences, in press, 2009.
[11] S. Zhou, J. Lafferty, and L. Wasserman. Time varying undirected graphs. In Computational Learning
Theory, 2008.
[12] L. Song, M. Kolar, and E. Xing. Keller: Estimating time-evolving interactions between genes. In Bioin-
formatics (ISMB), 2009.
[13] N. Friedman, M. Linial, I. Nachman, and D. Peter. Using bayesian networks to analyze expression data.
Journal of Computational Biology, 7:601–620, 2000.
[14] N. Dobingeon, J. Tourneret, and M. Davy. Joint segmentation of piecewise constant autoregressive pro-
cesses by using a hierarchical model and a bayesian sampling approach. IEEE Transactions on Signal
Processing, 55(4):1251–1263, 2007.
[15] K. Kanazawa, D. Koller, and S. Russell. Stochastic simulation algorithms for dynamic probabilistic
networks. Uncertainty in AI, 1995.
[16] L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning probabilistic models with link uncertainty.
Journal of Machine Learning Research, 2002.
[17] R. Dahlhaus. Fitting time series models to nonstationary processes. Ann. Statist, (25):1–37, 1997.
[18] C. Andrieu, M. Davy, and A. Doucet. Efﬁcient particle ﬁltering for jump markov systems: Application to
time-varying autoregressions. IEEE Transactions on Signal Processing, 51(7):1762–1770, 2003.
[19] E. H. Davidson. Genomic Regulatory Systems. Academic Press, 2001.
[20] Florentina Bunea. Honest variable selection in linear and logistic regression models via (cid:96)1 and (cid:96)1 + (cid:96)2
penalization. Electronic Journal of Statistics, 2:1153, 2008.
[21] W. Fu. Penalized regressions: the bridge versus the lasso. Journal of Computational and Graphical
Statistics, 7(3):397–416, 1998.
[22] M. Schmidt, A. Niculescu-Mizil, and K Murphy.
regularization paths. In AAAI, 2007.
[23] Tata Pramila, Wei Wu, Shawna Miles, William Noble, and Linda Breeden. The forkhead transcription fac-
tor hcm1 regulates chromosome segregation genes and ﬁlls the s-phase gap in the transcriptional circuitry
of the cell cycle. Gene and Development, 20:2266–2278, 2006.
[24] Jun Zhu, Bin Zhang, Erin Smith, Becky Drees, Rachel Brem, Leonid Kruglyak, Roger Bumgarner, and
Eric E Schadt. Integrating large-scale functional genomic data to dissect the complexity of yeast regula-
tory networks. Nature Genetics, 40:854–861, 2008.
[25] T. Nichols and A. Holmes. Nonparametric permutation tests for functional neuroimaging: a primer with
examples. Human Brain Mapping, 15:1–25, 2001.
[26] G. Dornhege, B. Blankertz, G. Curio, and K.R. M ¨uller. Boosting bit rates in non-invasive eeg single-trial
classiﬁcations by feature combination and multi-class paradigms. IEEE Trans. Biomed. Eng., 51:993–
1002, 2004.

Learning graphical model structure using l1-

9


Lower bounds on minimax rates for nonparametric
regression with additive sparsity and smoothness

Garvesh Raskutti1 , Martin J. Wainwright1,2 , Bin Yu1,2
1UC Berkeley Department of Statistics
2UC Berkeley Department of Electrical Engineering and Computer Science

Abstract

We study minimax rates for estimating high-dimensional nonparametric regression mod-
els with sparse additive structure and smoothness constraints. More precisely, our goal
is to estimate a function f ∗ : Rp → R that has an additive decomposition of the form
f ∗ (X1 , . . . , Xp ) = Pj∈S h∗j (Xj ), where each component function h∗j lies in some class
H of “smooth” functions, and S ⊂ {1, . . . , p} is an unknown subset with cardinality s = |S |.
Given n i.i.d. observations of f ∗ (X ) corrupted with additive white Gaussian noise where the
covariate vectors (X1 , X2 , X3 , ..., Xp ) are drawn with i.i.d. components from some distribu-
tion P, we determine lower bounds on the minimax rate for estimating the regression function
with respect to squared-L2 (P) error. Our main result is a lower bound on the minimax rate
that scales as max (cid:0) s log(p/s)
n (H)(cid:1). The ﬁrst term reﬂects the sample size required for
, s ǫ2
n
performing subset selection, and is independent of the function class H. The second term
n (H) is an s-dimensional estimation term corresponding to the sample size required for
s ǫ2
estimating a sum of s univariate functions, each chosen from the function class H. It depends
linearly on the sparsity index s but is independent of the global dimension p. As a special case,
if H corresponds to functions that are m-times differentiable (an mth -order Sobolev space),
n (H) ≍ s n−2m/(2m+1) . Either of
then the s-dimensional estimation term takes the form sǫ2
the two terms may be dominant in different regimes, depending on the relation between the
sparsity and smoothness of the additive decomposition.

1 Introduction

Many problems in modern science and engineering involve high-dimensional data, by which we mean that the
ambient dimension p in which the data lies is of the same order or larger than the sample size n. A simple
example is parametric linear regression under high-dimensional scaling, in which the goal is to estimate a
regression vector β ∗ ∈ Rp based on n samples.
In the absence of additional structure, it is impossible to
obtain consistent estimators unless the ratio p/n converges to zero which precludes the regime p ≫ n. In
many applications, it is natural to impose sparsity conditions, such as requiring that β ∗ have at most s non-zero
parameters for some s ≪ p. The method of ℓ1 -regularized least squares, also known as the Lasso algorithm [14],
has been shown to have a number of attractive theoretical properties for such high-dimensional sparse models
(e.g., [1, 19, 10]).

Of course, the assumption of a parametric linear model may be too restrictive for some applications. Accord-
ingly, a natural extension is the non-parametric regression model y = f ∗ (x1 , . . . , xp ) + w , where w ∼ N (0, σ2 )
is additive observation noise. Unfortunately, this general non-parametric model is known to suffer severely from
the “curse of dimensionality”, in that for most natural func tion classes, the sample size n required to achieve
a given estimation accuracy grows exponentially in the dimension. This challenge motivates the use of addi-
tive non-parametric models (see the book [6] and references therein), in which the function f ∗ is decomposed
additively as a sum f ∗ (x1 , x2 , ..., xp ) = Pp
j=1 h∗j (xj ) of univariate functions h∗j . A natural sub-class of these

1

(1)

models are the sparse additive models, studied by Ravikumar et. al [12], in which
f ∗ (x1 , x2 , ..., xp ) = Xj∈S
h∗j (xj ),
where S ⊂ {1, 2, . . . , p} is some unknown subset of cardinality |S | = s.
A line of past work has proposed and analyzed computationally efﬁcient algorithms for estimating regression
functions of this form. Just as ℓ1 -based relaxations such as the Lasso have desirable properties for sparse
parametric models, similar ℓ1 -based approaches have proven to be successful. Ravikumar et al. [12] propose a
back- ﬁtting algorithm to recover the component functions hj and prove consistency in both subset recovery and
consistency in empirical L2 (Pn ) norm. Meier et al. [9] propose a method that involves a sparsity-smoothness
penalty term, and also demonstrate consistency in L2 (P) norm. In the special case that H is a reproducing
kernel Hilbert space (RKHS), Koltchinskii and Yuan [7] analyze a least-squares estimator based on imposing
an ℓ1 − ℓH
-penalty. The analysis in these paper demonstrates that under certain conditions on the covariates,
such regularized procedures can yield estimators that are consistent in the L2 (P)-norm even when n ≪ p.
Of complementary interest to the rates achievable by practical methods are the fundamental limits of the esti-
mating sparse additive models, meaning lower bounds that apply to any algorithm. Although such lower bounds
are well-known under classical scaling (where p remains ﬁxed independent of n), to the best of our knowledge,
lower bounds for minimax rates on sparse additive models have not been determined. In this paper, our main
n (H)(cid:1).
result is to establish a lower bound on the minimax rate in L2 (P) norm that scales as max (cid:0) s log(p/s)
, sǫ2
n
The ﬁrst term s log(p/s)
is a subset selection term, independent of the univariate function space H in which
n
n (H) in an
the additive components lie, that reﬂects the difﬁculty of ﬁ
nding the subset S . The second term sǫ2
s-dimensional estimation term, which depends on the low dimension s but not the ambient dimension p, and
reﬂects the difﬁculty of estimating the sum of
s univariate functions, each drawn from function class H. Either
the subset selection or s-dimensional estimation term dominates, depending on the relative sizes of n, p, and
s as well as H. Importantly, our analysis applies both in the low-dimensional setting (n ≫ p) and the high-
dimensional setting (p ≫ n) provided that n, p and s are going to ∞. Our analysis is based on information-
theoretic techniques centered around the use of metric entropy, mutual information and Fano’s inequality in
order to obtain lower bounds. Such techniques are standard in the analysis of non-parametric procedures under
classical scaling [5, 2, 17], and have also been used more recently to develop lower bounds for high-dimensional
inference problems [16, 11].

The remainder of the paper is organized as follows. In the next section, the results are stated including appropri-
ate preliminary concepts, notation and assumptions. In Section 3, we state the main results, and provide some
comparisons to the rates achieved by existing algorithms. In Section 4, we provide an overview of the proof.
We discuss and summarize the main consequences in Section 5.

2 Background and problem formulation

In this paper, we consider a non-parametric regression model with random design, meaning that we make n
observations of the form

y (i) = f ∗ (X (i) ) + w(i) ,
(2)
for i = 1, 2, . . . , n.
Here the random vectors X (i) ∈ Rp are the covariates, and have elements X (i)
drawn i.i.d. from some un-
j
derlying distribution P. We assume that the noise variables w(i) ∼ N (0, σ2 ) are drawn independently, and
independent of all X (i) ’s. Given a base class H of univariate functions with norm k · kH
, consider the class of
functions f : Rp → R that have an additive decomposition:
pXj=1
F : = (cid:8)f : Rp → R | f (x1 , x2 , ..., xp ) =
khj kH ≤ 1 ∀j = 1, . . . , p(cid:9).
and
hj (xj ),
s(cid:1) s-dimensional
Given some integer s ∈ {1, . . . , p}, we deﬁne the function class F0 (s), which is a union of (cid:0)p
subspaces of F , given by
pXj=1
F0 (s) : = (cid:8)f ∈ F |
I(hj 6= 0) ≤ s(cid:9).
(3)
The minimax rate of estimation over F0 (s) is deﬁned by the quantity min bf maxf ∗∈F0 (s) Ek bf − f ∗ k2
L2 (P) , where
the expectation is taken over the noise w , and randomness in the sampling, and bf ranges over all (measurable)
2

functions of the observations {(y (i) , X (i) )}n
i=1 . The goal of this paper is to determine lower bounds on this
minimax rate.

2.1 Inner products and norms

Given a univariate function hj ∈ H, we deﬁne the usual L2 (P) inner product
hhj , h′j iL2 (P) : = ZR
hj (x)h′j (x) dP(x).
(With a slight abuse of notation, we use P to refer to the measure over Rp as well as the induced marginal
measure in each direction deﬁned over R). Without loss of generality (re-centering the functions as needed), we
may assume
E[hj (X )] = ZR
for all hj ∈ H. As a consequence, we have E[f (X1 , . . . , Xp )] = 0 for all functions f ∈ F0 (s). Given our
assumption that the covariate vector X = (X1 , . . . , Xp ) has independent components, the L2 (P) inner product
on F has the additive decomposition hf , f ′ iL2 (P) = Pp
j=1 hhj , h′j iL2 (P) . (Note that if independence were not
assumed the L2 (P) inner product over F would involve cross-terms.)
2.2 Kullback-Leibler divergence

hj (x) dP(x) = 0,

Since we are using information theoretic techniques, we will be using the Kullback-Leibler (KL) divergence as a
measure of “distance” between distributions. For a given pa ir of functions f and ef , consider the n-dimensional
vectors f (X ) = (cid:0)f (X (1) ), f (X (2) ), . . . , f (X (n) )(cid:1)T and ef (X ) = (cid:0) ef (X (1) ), ef (X (2) ), . . . , ef (X (n) )(cid:1)T . Since
Y |f (X ) ∼ N (f (X ), σ2 In×n ) and Y | ef (X ) ∼ N ( ef (X ), σ2 In×n ),
1
2σ2 kf (X ) − ef (X )k2
D(Y |f (X ) k Y | ef (X )) =
(4)
2 .
We also use the notation D(f k ef ) to mean the average K-L divergence between the distributions of Y induced
by the functions f and ef respectively. Therefore we have the relation
D(f k ef ) = EX (cid:2)D(Y |f (X ) k Y | ef (X ))(cid:3)
n
2σ2 kf − ef k2
=
L2 (P) .
This relation between average K-L divergence and squared L2 (P) distance plays an important role in our proof.

(5)

2.3 Metric entropy for function classes

In this section, we deﬁne the notion of metric entropy, which provides a way in which to measure the relative
sizes of different function classes with respect to some metric ρ. More speci ﬁcally, central to our results is the
metric entropy of F0 (s) with respect to the L2 (P) norm.
Deﬁnition 1 (Covering and packing numbers). Consider a metric space consisting of a set S and a metric
ρ : S × S → R+ .
(a) An ǫ-covering of S in the metric ρ is a collection {f 1 , . . . , f N } ⊂ S such that for all f ∈ S , there
exists some i ∈ {1, . . . , N } with ρ(f , f i ) ≤ ǫ. The ǫ-covering number Nρ (ǫ) is the cardinality of the
smallest ǫ-covering.
(b) An ǫ-packing of S in the metric ρ is a collection {f 1 , . . . , f M } ⊂ S such that ρ(f i , f j ) ≥ ǫ for all
i 6= j . The ǫ-packing number Mρ (ǫ) is the cardinality of the largest ǫ-packing.
The covering and packing entropy (denoted by log Nρ (ǫ) and log Mρ (ǫ) respectively) are simply the logarithms
of the covering and packing numbers, respectively.
It can be shown that for any convex set, the quantities
log Nρ (ǫ) and log Mρ (ǫ) are of the same order (within constant factors independent of ǫ).

3

In this paper, we are interested in packing (and covering) subsets of the function class F0 (s) in the L2 (P) metric,
and so drop the subscript ρ from here onwards. En route to characterizing the metric entropy of F0 (s), we need
to understand the metric entropy of the unit balls of our univariate function class H —namely, the sets
H (1) : = {h ∈ H | khkH ≤ 1}.
B
The metric entropy (both covering and packing entropy) for many classes of functions are known. We provide
some concrete examples here:

(i) Consider the class H = {hβ : R → R | hβ (x) = βx} of all univariate linear functions with the norm
khβ kH = |β |. Then it is known [15] that the metric entropy of B
H (1) scales as log M (ǫ; H) ∼ log(1/ǫ).
(ii) Consider the class H = {h :
|h(x) − h(y)| ≤ |x − y |} of all 1-Lipschitz func-
[0, 1] → [0, 1]
|
tions on [0, 1] with the norm khkH = supx∈[0,1] |h(x)|. In this case, it is known [15] that the metric entropy
scales as log M H (ǫ; H) ∼ 1/ǫ. Compared to the previous example of linear models, note that the metric
entropy grows much faster as ǫ → 0, indicating that the class of Lipschitz functions is much richer.
(iii) Consider the class of Sobolev spaces W m for m ≥ 1, consisting of all functions that have m derivatives,
and the mth derivative is bounded in L2 (P) norm. In this case, it is known that log M (ǫ; H) ∼ ǫ− 1
m (e.g., [3]).
Clearly, increasing the smoothness constraint m leads to smaller classes. Such Sobolev spaces are a particular
class of functions whose packing/covering entropy grows at a rate polynomial in 1
ǫ .
In our analysis, we require that the metric entropy of B
H (1) satisfy the following technical condition:
Assumption 1. Using log M (ǫ; H) to denote the packing entropy of the unit ball B
H (1) in the L2 (P)-norm,
assume that there exists some α ∈ (0, 1) such that
log M (αǫ; H)
lim
log M (ǫ; H)
ǫ→0
The condition is required to ensure that log M (cǫ)/ log M (ǫ) can be made arbitrarily small or large uniformly
over small ǫ by changing c, so that a bound due to Yang and Barron [17] can be applied. It is satis ﬁed for most
non-parametric classes, including (for instance) the Lipschitz and Sobolev classes deﬁned in Examples (ii) and
(iii) above. It may fail to hold for certain parametric classes, such as the set of linear functions considered
in Example (i); however, we can use an alternative technique to derive bounds for the parametric case (see
Corollary 2).

> 1.

3 Main result and some consequences

In this section, we state our main result and then develop some of its consequences. We begin with a theorem
that covers the function class F0 (s) in which the univariate function classes H have metric entropy that satis ﬁes
Assumption 1. We state a corollary for the special cases of univariate classes H with metric entropy growing
polynomial in (1/ǫ), and also a corollary for the special case of sparse linear regression.
Consider the observation model (2) where the covariate vectors have i.i.d. elements Xj ∼ P, and the regression
function f ∗ ∈ F0 (s). Suppose that the univariate function class H that underlies F0 (s) satis ﬁes Assumption 1.
Under these conditions, we have the following result:
Theorem 1. Given n i.i.d. samples from the sparse additive model (2), the minimax risk in squared-L2 (P)
norm is lower bounded as
L2 (P) ≥ max (cid:20) σ2 s log(p/s)
n (H)(cid:21),
s
Ek bf − f ∗ k2
ǫ2
max
min
32n
16
bf
f ∗∈F0 (s)
where, for a ﬁxed constant c, the quantity ǫn (H) = ǫn > 0 is largest positive number satisfying the inequality
nǫ2
2σ2 ≤ log M (cid:0)c ǫn (cid:1).
n
(7)
For the case where H has an entropy that is growing to ∞ at a polynomial rate as ǫ → 0—say
log M (ǫ; H) =
Θ(ǫ−1/m ) for some m > 1
2 , we can compute the rate for the s-dimensional estimation term explicitly.

(6)

,

4

Corollary 1. For the sparse additive model (2) with univariate function space H such that such that
log M (ǫ; H) = Θ(ǫ−1/m ), we have
2m+1 (cid:21),
L2 (P) ≥ max (cid:20) σ2 s log(p/s)
, C s (cid:0) σ2
n (cid:1) 2m
Ek bf − f ∗ k2
max
min
32n
bf
f ∗∈F0 (s)

(8)

for some C > 0.

3.1 Some consequences

In this section, we discuss some consequences of our results.

Effect of smoothness: Focusing on Corollary 1, for spaces with m bounded derivatives (i.e., functions in the
Sobolev space W m ), the minimax rate is n− 2m
2m+1 (for details, see e.g. Stone [13]). Clearly, faster rates are
obtained for larger smoothness indices m, and as m → ∞, the rate approaches the parametric rate of n−1 .
Since we are estimating over an s-dimensional space (under the assumption of independence), we are effectively
estimating s univariate functions, each lying within the function space H. Therefore the uni-dimensional rate is
multiplied by s.
Smoothness versus sparsity: It is worth noting that depending on the relative scalings of s, n and p and the metric
entropy of H, it is possible for either the subset selection term or s-dimensional estimation term to dominate
the lower bound. In general, if log(p/s)
n (H)), the s-dimensional estimation term dominates, and vice
= o(ǫ2
n
versa (at the boundary, either term determines the minimax rate). In the case of a univariate function class H
with polynomial entropy as in Corollary 1, it can be seen that for n = o((log(p/s))2m+1 ), the s-dimensional
estimation term dominates while for n = Ω((log(p/s))2m+1 ), the subset selection term dominates.
Rates for linear models: Using an alternative proof technique (not the one used in this paper), it is possible [11]
to derive the exact minimax rate for estimation in the sparse linear regression model, in which we observe
y (i) = Xj∈S
βj X (i)
j + w(i) ,
Note that this is a special case of the general model (2) in which H corresponds to the class of univariate linear
functions (see Example (i)).
n (cid:1).
Corollary 2. For sparse linear regression model (9), the the minimax rate scales as max (cid:0) s log(p/s)
, s
n
In this case, we see clearly the subset selection term dominates for p → ∞, meaning the subset selection
problem is always “harder ” (in a statistical sense) than the
s-dimensional estimation problem. As shown
by Bickel et al. [1], the rate achieved by ℓ1 -regularized methods is s log p
under suitable conditions on the
n
covariates X .

for i = 1, 2, ..., n.

(9)

Upper bounds: To show that the lower bounds are tight, upper bounds that are matching need to be derived.
Upper bounds (matching up to constant factors) can be derived via a classical information-theoretic approach
(e.g., [5, 2]), which involves constructing an estimator based on a covering set and bounding the covering
entropy of F0 (s). While this estimation approach does not lead to an implementable algorithm, it is a simple
theoretical device to demonstrate that lower bounds are tight. We turn our focus on implementable algorithms
in the next point.

Comparison to existing bounds: We now provide a brief comparison of the minimax lower bounds with upper
bounds on rates achieved by existing implementable algorithms provided by past work [12, 7, 9]. Ravikumar
et al. [12] propose a back- ﬁtting algorithm to minimize the l east-squares objective with a sparsity constraint on
the the function f . The rates derived in Koltchinskii and Yuan [7] do not match the lower bounds derived in
Theorem 1. Further, it is difﬁcult to directly compare the ra tes in Ravikumar et al. [12] and Meier et al. [9] with
our minimax lower bounds since their analysis does not explicitly track the sparsity index s. We are currently
in the process of conducting a thorough comparison with the above-mentioned ℓ1 -based methods.

4 Proof outline

In this section, we provide an outline of the proof of Theorem 1; due to space constraints, we defer some of
the technical details to the full-length version. The proof is based on a combination of information-theoretic

5

techniques and the concepts of packing and covering entropy, as deﬁned previously in Section 2.3. First, we
provide a high-level overview of the proof. The basic idea is to carefully choose two subsets T1 and T2 of the
function class F0 (s) and lower bound the minimax rates over these two subsets. In Section 4.1, application of
the generalized Fano method—a technique based on Fano’s ineq uality—to the set
T1 deﬁned in equation (10)
yields a lower bound on the subset selection term. In Section 4.2, we apply an alternative method for obtaining
lower bounds over a second set T2 deﬁned in equation (11) that captures the difﬁculty of estim ating the sum
of s univariate functions.. The second technique also exploits Fano’s inequality but uses a more reﬁned upper
bound on the mutual information developed by Yang and Barron [17].
Before procedding, we ﬁrst note that for any T ⊂ F0 (s), we have
Ek bf − f ∗ k2
Ek bf − f ∗ k2
L2 (P) ≥ min
min
max
L2 (P) .
max
f ∗∈T
bf
bf
f ∗∈F0 (s)
Moreover, for any subsets T1 , T2 ⊂ F0 (s), we have
L2 (P) ≥ max(cid:0) min
L2 (P) (cid:1),
Ek bf − f ∗ k2
Ek bf − f ∗ k2
Ek bf − f ∗ k2
max
min
max
max
L2 (P) , min
f ∗∈T1
f ∗∈T2
bf
bf
bf
f ∗∈F0 (s)
since the bound holds for each of the two terms. We apply this lower bound using the subsets T1 and T2 deﬁned
in equations (10) and (11).

4.1 Bounding the complexity of subset selection

max
j

For part of the proof, we use the generalized Fano’s method [4], which we state below without proof. Given
some parameter space, we let d be a metric on it.
Lemma 1. (Generalized Fano Method) For a given integer r ≥ 2, consider a collection Mr = {P1 , . . . , Pr }
of r probability distributions such that
for all i 6= j ,
d(θ(Pi ), θ(Pj )) ≥ αr
and the pairwise KL divergence satis ﬁes
for all i, j = 1, . . . , r .
D(Pi k Pj ) ≤ βr
Then the minimax risk over the family is lower bounded as
2 (cid:18)1 −
log r (cid:19).
βr + log 2
αr
Ej d(θ(Pj ), bθ) ≥
The proof of Lemma 1 involves applying Fano’s inequality over the discrete set of parameters θ ∈ Θ indexed
by the set of distributions Mr . Now we construct the set T1 which creates the set of probability distributions
Mr .
4 q log (p/s)
Let g be an arbitrary function in H such that kgkL2 (P) = σ
. The set T1 is deﬁned as
n
T1 : =(cid:26)f : f (X1 , X2 , ..., Xp ) =
cj g(Xj ), cj ∈ {−1, 0, 1} | kck0 = s(cid:27).
pXj=1
T1 may be viewed as a hypercube of F0 (s) and will lead to the lower bound for the ’subset selection’ term. This
hypercube construction is often used to prove lower bounds (see Yu [18]). Next, we require a further reduction
of the set T1 to a set A (deﬁned in Lemma 2) to ensure that elements of A are well-separated in L2 (P) norm.
The construction of A is as follows:
Lemma 2. There exists a subset A ⊂ T1 such that:
(i) log |A| ≥ 1
2 s log(p/s),
L2 (P) ≥ σ2 s log(p/s)
(ii) kf − f ′k2
∀ f , f ′ ∈ A, and
16n
(iii) D(f k f ′ ) ≤ 1
8 s log(p/s) ∀f , f ′ ∈ A.
The proof involves using a combinatoric argument to construct the set A. For an argument on how the set is
constructed, see K ¨uhn [8]. For s log p
s ≥ 8 log 2, applying the Generalized Fano Method (Lemma 1) together
with Lemma 2 yields the bound
σ2 s log(p/s)
Ek bf − f ∗ k2
Ek bf − f ∗ k2
L2 (P) ≥ min
L2 (P) ≥
max
min
max
32n
f ∗∈A
bf
bf
f ∗∈F0 (s)
This completes the proof for the subset selection term ( s log(p/s)
) in Theorem 1.
n

(10)

.

6

4.2 Bounding the complexity of s-dimensional estimation

and

(11)

(12)

min
bf

max
f ∗∈FS

Next we derive a bound for the s-dimensional estimation term by determining a lower bound over T2 . Let S be
an arbitrary subset of s integers in {1, 2, .., p}, and deﬁne the set FS as
T2 : = FS : = (cid:8)f ∈ F : f (X ) = Xj∈S
hj (Xj )(cid:9).
Clearly FS ⊂ F0 (s) meaning that
Ek bf − f ∗ k2
Ek bf − f ∗ k2
L2 (P) ≥ min
L2 (P) .
min
max
max
f ∗∈FS
bf
bf
f ∗∈F0 (s)
We use a technique used in Yang and Barron [17] to lower bound the minimax rate over FS . The idea is to
construct a maximal δn -packing set for FS and a minimal ǫn -covering set for FS , and then to apply Fano’s
inequality to a carefully chosen mixture distribution involving the covering and packing sets (see the full-length
version for details). Following these steps yields the following result:
Lemma 3.
4 (cid:18)1 −
(cid:19).
log N (ǫn ; FS ) + nǫ2
n /2σ2 + log 2
δ2
Ek bf − f ∗ k2
n
L2 (P) ≥
log M (δn ; FS )
Now we have a bound with expressions involving the covering and packing entropies of the s-dimensional space
FS . The following Lemma allows bounds on log M (ǫ; FS ) and log N (ǫ; FS ) in terms of the unidimensional
packing and covering entropies respectively:
Lemma 4. Let H be function space with a packing entropy log M (ǫ; H) that satis ﬁes Assumption 1. Then we
have the bounds
log M (ǫ; FS ) ≥ s log M (ǫ/√s; H),
log N (ǫ; FS ) ≤ s log N (ǫ/√s; H).
The proof involves constructing ǫ√s - packing set and covering sets in each of the s dimensions and displaying
that these are ǫ-packing and coverings sets in FS (respectively). Combining Lemmas 3 and 4 leads to the
inequality
s log N (ǫn /√s; H) + nǫ2
4 (cid:18)1 −
(cid:19).
n /2σ2 + log 2
δ2
Ek bf − f ∗ k2
n
s log M (δn /√s; H)
L2 (P) ≥
max
min
f ∗∈FS
bf
Now we choose ǫn and δn to meet the following constraints:
ǫn√s
n
2σ2 ǫ2
n ≤ s log N (
δn√s
ǫn√s
; H).
; H) ≤ log M (
Combining Assumption 1 with the well-known relations log M (2ǫ; H) ≤ log N (2ǫ; H) ≤ log M (ǫ; H), we
conclude that in order to satisfy inequalities (13a) and (13b), it is sufﬁcient to choose ǫn = cδn for a constant c,
2σ2 . Furthermore, if we deﬁne δn /√s = eδn , then this inequality can
and then require that s log M ( cδn√s ; H) ≥ nδ2
n
2
be re-expressed as log M (c eδn ) ≥ nfδn
2σ2 . For n
n ≥ log 2, using inequalities (13a) and (13b) together with
2σ2 ǫ2
equation (12) yields the desired rate
2
s eδn
min
16
bf

max
f ∗∈FS

4 log N (

; H),

and

(13a)

(13b)

Ek bf − f ∗ k2
L2 (P) ≥

,

thereby completing the proof.

5 Discussion

In this paper, we have derived lower bounds for the minimax risk in squared L2 (P) error for estimating sparse
additive models based on the sum of univariate functions from a function class H. The rates show that the
estimation problem effectively decomposes into a subset selection problem and an s-dimensional estimation

7

problem, and the “harder ” of the two problems (in a statistical sense ) determines the rate of convergence.
More concretely, we demonstrated that the subset selection term scales as s log(p/s)
, depending linearly on
n
the number of components s and only logarithmically in the ambient dimension p. This subset selection term is
independent of the univariate function space H. On the other hand, the s-dimensional estimation term depends
on the “richness ” of the univariate function class, measure d by its metric entropy; it scales linearly with s and is
independent of p. Ongoing work suggests that our lower bounds are tight in many cases, meaning that the rates
derived in Theorem 1 are minimax optimal for many function classes.

There are a number of ways in which the work can be extended. One implicit and strong assumption in our
analysis was that the covariates Xj , j = 1, 2, ..., p are independent. It would be interesting to investigate the case
when the random variables are endowed with some correlation structure. One would expect the rates to change,
particularly if many of the variables are collinear. It would also be interesting to develop a more complete
understanding of whether computationally efﬁcient algori
thms [7, 12, 9] based on regularization achieve the
lower bounds on the minimax rate derived in this paper.

References

[1] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of the Lasso and Dantzig selector. Annals of
Statistics, 2009. To appear.
[2] L. Birg ´e. Approximation dans les espaces metriques et theorie de l’estimation. Z. Wahrsch. verw. Gebiete,
65:181–327, 1983.
[3] M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the classes W α
p .
Math. USSR-Sbornik, 2(3):295–317, 1967.
[4] T. S. Han and S. Verdu. Generalizing the Fano inequality. IEEE Transactions on Information Theory,
40:1247–1251, 1994.
[5] R. Z. Has’minskii. A lower bound on the risks of nonparametric estimates of densities in the uniform
metric. Theory Prob. Appl., 23:794–798, 1978.
[6] T. Hastie and R. Tibshirani. Generalized Additive Models. Chapman and Hall Ltd, Boca Raton, 1999.
[7] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines. In Proceedings of
COLT, 2008.
[8] T. K ¨uhn. A lower estimate for entropy numbers. Journal of Approximation Theory, 110:120–124, 2001.
[9] L. Meier, S. van de Geer, and P. Buhlmann. High-dimensional additive modeling. Annals of Statistics, To
appear.
[10] N. Meinshausen and B.Yu. Lasso-type recovery of sparse representations for high-dimensional data. An-
nals of Statistics, 37(1):246–270, 2009.
[11] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear regres-
sion over ℓq -balls. Technical Report arXiv:0910.2042, UC Berkeley, Department of Statistics, 2009.
[12] P. Ravikumar, H. Liu, J. Lafferty, and L. Wasserman. Sparse additive models. Journal of the Royal
Statistical Society, To appear.
[13] C. J. Stone. Optimal global rates of convergence for nonparametric regression. Annals of Statistics,
10:1040–1053, 1982.
[14] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,
Series B, 58(1):267–288, 1996.
[15] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.
[16] M. J. Wainwright. Information-theoretic bounds for sparsity recovery in the high-dimensional and noisy
setting. IEEE Trans. Info. Theory, December 2009. Presented at International Symposium on Information
Theory, June 2007.
[17] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of
Statistics, 27(5):1564–1599, 1999.
[18] B. Yu. Assouad, Fano and Le Cam. Research Papers in Probability and Statistics: Festschrift in Honor of
Lucien Le Cam, pages 423–435, 1996.
[19] C. H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regres-
sion. Annals of Statistics, 36:1567–1594, 2006.

8


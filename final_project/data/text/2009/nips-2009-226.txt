Decoupling Sparsity and Smoothness in the
Discrete Hierarchical Dirichlet Process

Chong Wang
Computer Science Department
Princeton University
chongw@cs.princeton.edu

David M. Blei
Computer Science Department
Princeton University
blei@cs.princeton.edu

Abstract

We present a nonparametric hierarchical Bayesian model of document collections
that decouples sparsity and smoothness in the component distributions (i.e., the
“topics”). In the sparse topic model (sparseTM), each topic is represented by a
bank of selector variables that determine which terms appear in the topic. Thus
each topic is associated with a subset of the vocabulary, and topic smoothness is
modeled on this subset. We develop an efﬁcient Gibbs sampler for the sparseTM
that includes a general-purpose method for sampling from a Dirichlet mixture
with a combinatorial number of components. We demonstrate the sparseTM on
four real-world datasets. Compared to traditional approaches, the empirical results
will show that sparseTMs give better predictive performance with simpler inferred
models.

1

Introduction

The hierarchical Dirichlet process (HDP) [1] has emerged as a powerful model for the unsupervised
analysis of text. The HDP models documents as distributions over a collection of latent components,
which are often called “topics” [2, 3]. Each word is assigned to a topic, and is drawn from a distribution
over terms associated with that topic. The per-document distributions over topics represent systematic
regularities of word use among the documents; the per-topic distributions over terms encode the
randomness inherent in observations from the topics. The number of topics is unbounded.
Given a corpus of documents, analysis proceeds by approximating the posterior of the topics and topic
proportions. This posterior bundles the two types of regularity. It is a probabilistic decomposition
of the corpus into its systematic components, i.e., the distributions over topics associated with
each document, and a representation of our uncertainty surrounding observations from each of
those components, i.e., the topic distributions themselves. With this perspective, it is important to
investigate how prior assumptions behind the HDP affect our inferences of these regularities.
In the HDP for document modeling, the topics are typically assumed drawn from an exchangeable
Dirichlet, a Dirichlet for which the components of the vector parameter are equal to the same scalar
parameter. As this scalar parameter approaches zero, it affects the Dirichlet in two ways. First,
the resulting draws of random distributions will place their mass on only a few terms. That is, the
resulting topics will be sparse. Second, given observations from such a Dirichlet, a small scalar
parameter encodes increased conﬁdence in the estimate from the observed counts. As the parameter
approaches zero, the expectation of each per-term probability becomes closer to its empirical estimate.
Thus, the expected distribution over terms becomes less smooth. The single scalar Dirichlet parameter
affects both the sparsity of the topics and smoothness of the word probabilities within them.
When employing the exchangeable Dirichlet in an HDP, these distinct properties of the prior have
consequences for both the global and local regularities captured by the model. Globally, posterior
inference will prefer more topics because more sparse topics are needed to account for the observed

1

words of the collection. Locally, the per-topic distribution over terms will be less smooth—the
posterior distribution has more conﬁdence in its assessment of the per-topic word probabilities—and
this results in less smooth document-speciﬁc predictive distributions.
The goal of this work is to decouple sparsity and smoothness in the HDP. With the sparse topic model
(sparseTM), we can ﬁt sparse topics with more smoothing. Rather than placing a prior for the entire
vocabulary, we introduce a Bernoulli variable for each term and each topic to determine whether
or not the term appears in the topic. Conditioned on these variables, each topic is represented by a
multinomial distribution over its subset of the vocabulary, a sparse representation.
This prior smoothes only the relevant terms and thus the smoothness and sparsity are controlled
through different hyper-parameters. As we will demonstrate, sparseTMs give better predictive
performance with simpler models than traditional approaches.

2 Sparse Topic Models

Sparse topic models (sparseTMs) aim to separately control the number of terms in a topic, i.e.,
sparsity, and the probabilities of those words, i.e., smoothness. Recall that a topic is a pattern of word
use, represented as a distribution over the ﬁxed vocabulary of the collection. In order to decouple
smoothness and sparsity, we deﬁne a topic on a random subset of the vocabulary (giving sparsity),
and then model uncertainty of the probabilities on that subset (giving smoothness). For each topic, we
introduce a Bernoulli variable for each term in the vocabulary that decides whether the term appears
in the topic. Similar ideas of using Bernoulli variables to represent “on” and “off ” have been seen
in several other models, such as the noisy-OR model [4] and aspect Bernoulli model [5]. We can
view this approach as a particular “spike and slab” prior [6] over Dirichlet distributions. The “spike”
chooses the terms for the topic; the “slab” only smoothes those terms selected by the spike.
Assume the size of the vocabulary is V . A Dirichlet distribution over the topic is deﬁned on a
V − 1-simplex, i.e.,

β ∼ Dirichlet(γ1),
(1)
where 1 is a V -length vector of 1s. In an sparseTM, the idea of imposing sparsity is to use Bernoulli
variables to restrict the size of the simplex over which the Dirichlet distribution is deﬁned. Let b
be a V -length binary vector composed of V Bernoulli variables. Thus b speciﬁes a smaller simplex
through the “on”s of its elements. The Dirichlet distribution over the restricted simplex is
β ∼ Dirichlet(γ b),
(2)
which is a degenerate Dirichlet distribution over the sub-simplex speciﬁed by b. In [7], Friedman and
Singer use this type of distributions for language modeling.
Now we introduce the generative process of the sparseTM. The sparseTM is built on the hierarchical
Dirichlet process for text, which we shorthand HDP-LDA. 1 In the Bayesian nonparametric setting
the number of topics is not speciﬁed in advance or found by model comparison. Rather, it is inferred
through posterior inference. The sparseTM assumes the following generative process:
1. For each topic k ∈ {1, 2, . . .}, draw term selection proportion πk ∼ Beta(r, s).
(b) Let bV +1 = 1[(cid:80)V
(a) For each term v , 1 ≤ v ≤ V , draw term selector bkv ∼ Bernoulli(πk ).
v=1 bkv = 0] and bk = [bkv ]V +1
v=1 .
Draw topic distribution βk ∼ Dirichlet(γ bk ).
2. Draw stick lengths α ∼ GEM(λ), which are the global topic proportions.
3. For document d:
(a) Draw per-document topic proportions θd ∼ DP(τ , α).
(b) For the ith word:
i. Draw topic assignment zdi ∼ Mult(θd ).
ii. Draw word wdi ∼ Mult(βzdi )
Figure 1 illustrates the sparseTM as a graphical model.

1This acronym comes from the fact that the HDP for text is akin to a nonparametric Bayesian version of
latent Dirichlet allocation (LDA).

2

Figure 1: A graphical model representation for sparseTMs.

(4)

The distinguishing feature of the sparseTM is step 1, which generates the latent topics in such a
way that decouples sparsity and smoothness. For each topic k there is a corresponding Beta random
variable πk and a set of Bernoulli variables bkv s, one for each term in the vocabulary. Deﬁne the
(cid:44) 1 − (cid:80)V
sparsity of the topic as
(3)
sparsityk
v=1 bkv /V .
This is the proportion of zeros in its bank of Bernoulli random variables. Conditioned on the Bernoulli
parameter πk , the expectation of the sparsity is
E [sparsityk |πk ] = 1 − πk .
The conditional distribution of the topic βk given the vocabulary subset bk is Dirichlet(γ bk ). Thus,
topic k is represented by those terms with non-zero bkv s, and the smoothing is only enforced over
these terms through hyperparameter γ . Sparsity, which is determined by the pattern of ones in bk , is
One nuance is that we introduce bV +1 = 1[(cid:80)V
controlled by the Bernoulli parameter. Smoothing and sparsity are decoupled.
v=1 bkv = 0]. The reason is that when bk,1:V = 0,
Dirichlet(γ bk,1:V ) is not well deﬁned. The term bV +1 extends the vocabulary to V + 1 terms, where
the V + 1th term never appears in the documents. Thus, Dirichlet(γ bk,1:V +1 ) is always well deﬁned.
(cid:90)
We next compute the marginal distribution of βk , after integrating out Bernoullis bk and their
parameter πk :
(cid:90)
p(βk |γ , r, s) =
dπk p(βk |γ , πk )p(πk |r, s)
= (cid:88)
dπk p(bk |πk )p(πk |r, s).
p(βk |γ , bk )
bk
We see that p(βk |γ , r, s) and p(βk |γ , πk ) are mixtures of Dirichlet distributions, where the mixture
components are deﬁned over simplices of different dimensions. In total, there are 2V components;
each conﬁguration of Bernoulli variables bk speciﬁes one particular component. In posterior inference
we will need to sample from this distribution. Sampling from such a mixture is difﬁcult in general,
due to the combinatorial sum. In the supplement, we present an efﬁcient procedure to overcome this
issue. This is the central computational challenge for the sparseTM.
Step 2 and 3 mimic the generative process of HDP-LDA [1]. The stick lengths α come from a
Grifﬁths, Engen, and McCloskey (GEM) distribution [8], which is drawn using the stick-breaking
construction [9],
(cid:81)k−1
ηk ∼ Beta(1, λ),
Note that (cid:80)
j=1 (1 − ηj ), k ∈ {1, 2, . . . }.
αk = ηk
k αk = 1 almost surely. The stick lengths are used as a base measure in the Dirichlet
process prior on the per-document topic proportions, θd ∼ DP(τ , α). Finally, the generative process
for the topic assignments z and observed words w is straightforward.

3 Approximate posterior inference using collapsed Gibbs sampling

Since the posterior inference is intractable in sparseTMs, we turn to a collapsed Gibbs sampling
algorithm for posterior inference. In order to do so, we integrate out topic proportions θ , topic distri-
butions β and term selectors b analytically. The latent variables needed by the sampling algorithm

3

are stick lengths α, Bernoulli parameter π and topic assignment z . We ﬁx the hyperparameter s
equal to 1.
To sample α and topic assignments z , we use the direct-assignment method, which is based on an
analogy to the Chinese restaurant franchise (CRF) [1]. To apply direct assignment sampling, an
auxiliary table count random variable m is introduced. In the CRF setting, we use the following
notation. The number of customers in restaurant d (document) eating dish k (topic) is denoted ndk ,
and nd· denotes the number of customers in restaurant d. The number of tables in restaurant d serving
dish k is denoted mdk , md· denotes the number of tables in restaurant d, m·k denotes the number
of tables serving dish k , and m·· denotes the total number of tables occupied. (Marginal counts are
represented with dots.) Let K be the current number of topics. The function n(v)
k denotes the number
of times that term v has been assigned to topic k , while n(·)
k denotes the number of times that all the
terms have been assigned to topic k . Index u is used to indicate the new topic in the sampling process.
Note that direct assignment sampling of α and z is conditioned on π .
The crux for sampling stick lengths α and topic assignments z (conditioned on π ) is to compute the
conditional density of wdi under the topic component k given all data items except wdi as,
(wdi = v |πk ) (cid:44) p(wdi = v |{wd(cid:48) i(cid:48) , zd(cid:48) i(cid:48) : zd(cid:48) i(cid:48) = k , d(cid:48) i(cid:48) (cid:54)= di}, πk ).
−wdi
(5)
f
k
The derivation of equations for computing this conditional density is detailed in the supplement.2 We
summarize our ﬁndings as follows. Let V (cid:44) {1, . . . , V } be the set of vocabulary terms, Bk (cid:44) {v :
k,−di > 0, v ∈ V } be the set of terms that have word assignments in topic k after excluding wdi
n(v)
(cid:26) (n(v)
and |Bk | be its cardinality. Let’s assume that Bk is not an empty set.3 We have the following,
if v ∈ Bk
k,−di + γ )E [gBk (X )|πk ]
(wdi = v |πk ) ∝
−wdi
γπk E [g ¯Bk ( ¯X )|πk ]
k
otherwise.

(6)

f

,

where

,

Γ((|Bk | + x)γ )
gBk (x) =
Γ(n(·)
k,−di + 1 + (|Bk | + x)γ )
X | πk ∼ Binomial(V − |Bk |, πk ),
¯X | πk ∼ Binomial(V − | ¯Bk |, πk ),
and where ¯Bk = Bk ∪ {v}. Further note Γ(·) is the Gamma function and n(v)
k,−di describes the
corresponding count excluding word wdi . In the supplement, we also show that E [gBk (X )|πk ] >
πk E [g ¯Bk ( ¯X )|πk ]. The central difference between the algorithms for HDP-LDA and the sparseTM is
conditional probability in Equation 6 which depends on the selector variables and selector proportions.
We now describe how we sample stick lengths α and topic assignments z . This is similar to the
sampling procedure for HDP-LDA [1].

(7)

Sampling stick lengths α. Although α is an inﬁnite-length vector, the number of topics K is
ﬁnite at every point in the sampling process. Sampling α can be replaced by sampling α (cid:44)
[α1 , . . . , αK , αu ] [1]. That is,

α | m ∼ Dirichlet(m·1 , . . . , m·K , λ).
(cid:26) (ndk,−di + τ αk )f
Sampling topic assignments z . This is similar to the sampling approach for HDP-LDA [1] as well.
Using the conditional density f deﬁned Equation 5 and 6, we have
(wdi |πk )
−wdi
p(zdi = k |z−di , m, α, πk ) ∝
if k previously used,
(9)
(wdi |πu )
τ αu f −wdi
k
k = u.
u
If a new topic knew is sampled, then sample κ ∼ Beta(1, λ), and let αknew = καu and αunew =
(1 − κ )αu .
2Note we integrate out βk and bk . Another sampling strategy is to sample b (by integrating out π ) and the
Gibbs sampler is much easier to derive. However, conditioned on b, sampling z will be constrained to a smaller
set of topics (speciﬁed by the values of b), which slows down convergence of the sampler.
3 In the supplement, we show that if Bk is an empty set, the result is trivial.

(8)

4

= p(bk |πk )p(πk |r)

Sampling Bernoulli parameter π . To sample πk , we use bk as an auxiliary variable. Note that bk
was integrated out earlier. Recall Bk is the set of terms that have word assignments in topic k . (This
time, we don’t need to exclude certain words since we are sampling π .) Let Ak = {v : bkv = 1, v ∈
V } be the set of the indices of bk that are “on”, the joint conditional distribution of πk and bk is
(cid:90)
p(πk , bk |rest) ∝ p(bk |πk )p(πk |r)p({wdi : zdi = k}|bk , {zdi : zdi = k})
1Bk⊂Ak Γ(|Ak |γ ) (cid:81)
= p(bk |πk )p(πk |r)
dβk p({wdi : zdi = k}|βk , {zdi : zdi = k})p(βk |bk )
Γ(n(v)
k + γ )
v∈Ak
1Bk⊂Ak Γ(|Ak |γ ) (cid:81)
Γ|Ak | (γ )Γ(n(·)
k + |Ak |γ )
Γ(n(v)
k + γ )
v∈Bk
= p(bk |πk )p(πk |r)
∝ (cid:89)
Γ|Bk | (γ )Γ(n(·)
k + |Ak |γ )
1Bk⊂Ak Γ(|Ak |γ )
p(bkv |πk )p(πk |r)
where 1Bk⊂Ak is an indicator function and |Ak | = (cid:80)
,
Γ(n(·)
k + |Ak |γ )
v
v bkv . This follows because if Ak is not a
super set of Bk , there must be a term, say v in Bk but not in Ak , causing βkv = 0, a.s., and then
p({wdi : (d, i) ∈ Zk }|βk , {zdi : (d, i) ∈ Zk }) = 0 a.s.. Using this joint conditional distribution4 ,
we iteratively sample bk conditioned on πk and πk conditioned on bk to ultimately obtain a sample
from πk .

(10)

Others. Sampling the table counts m is exactly the same as for the HDP [1], so we omit the details
here. In addition, we can sample the hyper-parameters λ, τ and γ . For the concentration parameters
λ and τ in both HDP-LDA and sparseTMs, we use previously developed approaches for Gamma
priors [1, 10]. For the Dirichlet hyper-parameter γ , we use Metropolis-Hastings.
Finally, with any single sample we can estimate topic distributions β from the value topic assignments
z and term selector b by
k + (cid:80)
ˆβk,v = n(v)
k + bk,v γ
n(·)
v bkv γ
where we can smooth only those terms that are chosen to be in the topics. Note that we can obtain the
samples of b when sampling the Bernoulli parameter π .

(11)

,

4 Experiments

In this section, we studied the performance of the sparseTM on four datasets and demonstrated how
sparseTM decouples the smoothness and sparsity in the HDP.5 We placed Gamma(1, 1) priors over
the hyper-parameters λ and τ . The sparsity proportion prior was a uniform Beta, i.e., r = s = 1.
For hyper-parameter γ , we use Metropolis-Hastings sampling method using symmetric Gaussian
proposal with variance 1.0. A disadvantage of sparseTM is that its running speed is about 4-5 times
slower than the HDP-LDA.

4.1 Datasets

The four datasets we use in the experiments are:

1. The arXiv data set contains 2500 (randomly sampled) online research abstracts
(http://arxiv.org). It has 2873 unique terms, around 128K observed words and an aver-
age of 36 unique terms per document.
algorithm might be achieved by modeling the joint conditional distribution of πk and P
p(πk , P
v bkv |rest), since sampling πk only depends on P
4 In our experiments, we used the algorithm described in the main text to sample π . We note that an improved
v bkv instead, i.e.,
v bkv .
5Other experiments, which we don’t report here, also showed that the ﬁnite version of sparseTM outperforms
LDA with the same number of topics.

5

2. The Nematode Biology data set contains 2500 (randomly sampled) research abstracts
(http://elegans.swmed.edu/wli/cgcbib). It has 2944 unique terms, around 179K observed
words and an average of 52 unique terms per document.
3. The NIPS data set contains
the NIPS articles published between 1988-1999
(http://www.cs.utoronto.ca/∼sroweis/nips). It has 5005 unique terms and around 403K
observed words. We randomly sample 20% of the words for each paper and this leads to an
average of 150 unique terms per document.
4. The Conf.
abstracts set data contains abstracts (including papers and posters)
from six international conferences: CIKM, ICML, KDD, NIPS, SIGIR and WWW
(http://www.cs.princeton.edu/∼chongw/data/6conf.tgz). It has 3733 unique terms, around
173K observed words and an average of 46 unique terms per document. The data are from
2005-2008.

For all data, stop words and words occurring fewer than 10 times were removed.

4.2 Performance evaluation and model examinations

We studied the predictive performance of the sparseTM compared to HDP-LDA. On the training
documents our Gibbs sampler uses the ﬁrst 2000 steps as burn-in, and we record the following 100
samples as samples from the posterior. Conditioned on these samples, we run the Gibbs sampler for
test documents to estimate the predictive quantities of interest. We use 5-fold cross validation.
We study two predictive quantities. First, we examine overall predictive power with the predictive
perplexity of the test set given the training set. (This is a metric from the natural language literature.)
(cid:40)
(cid:41)
(cid:80)
The predictive perplexity is
(cid:80)
log p(wd |Dtrain )
d∈Dtest
−
Nd
d∈Dtest

perplexitypw = exp

.

Lower perplexity is better.
Second, we compute model complexity. Nonparametric Bayesian methods are often used to sidestep
model selection and integrate over all instances (and all complexities) of a model at hand (e.g., the
number of clusters). The model, though hidden and random, still lurks in the background. Here
we study its posterior distribution with the desideratum that between two equally good predictive
distributions, a simpler model—or a posterior peaked at a simpler model—is preferred.
To capture model complexity we ﬁrst deﬁne the complexity of topic. Recall that each Gibbs sample
contains a topic assignment z for every observed word in the corpus (see Equation 9). The topic
complexity is the number of unique terms that have at least one word assigned to the topic. This can
d 1 [((cid:80)
complexityk = (cid:80)
be expressed as a sum of indicators,
n 1[zd,n = k ]) > 0] ,
where recall that zd,n is the topic assignment for the nth word in document d. Note a topic with
no words assigned to it has complexity zero. For a particular Gibbs sample, the model complexity
is the sum of the topic complexities and the number of topics. Loosely, this is the number of free
complexity = #topics + (cid:80)
parameters in the “model” that the nonparametric Bayesian method has selected, which is
k complexityk .
We performed posterior inference with the sparseTM and HDP-LDA, computing predictive perplexity
and average model complexity with 5-fold cross validation. Figure 2 illustrates the results.

(12)

Perplexity versus Complexity. Figure 2 (ﬁrst row) shows the model complexity versus predictive
perplexity for each fold: Red circles represent sparseTM, blue squares represent HDP-LDA, and the
dashed line connecting a red circle and blue square indicates the that the two are from the same fold.
These results shows that the sparseTM achieves better perplexity than HDP-LDA, and at simpler
models. (To see this, notice that all the connecting lines going from HDP-LDA to sparseTM point
down and to the left.)

6

Figure 2: Experimental results for sparseTM (shortened as STM in this ﬁgure) and HDP-LDA on four
datasets. First row. The scatter plots of model complexity versus predictive perplexity for 5-fold
cross validation: Red circles represent the results from sparseTM, blue squares represent the results
from HDP-LDA and the dashed lines connect results from the same fold. Second row. Box plots of
the hyperparameter γ values. Third row. Box plots of the number of topics. Fourth row. Box plots
of the number of terms per topic.

Hyperparameter γ , number of topics and number of terms per topic. Figure 2 (from the second
to fourth rows) shows the Dirichlet parameter γ and posterior number of topics for HDP-LDA and
sparseTM. HDP-LDA tends to have a very small γ in order to attain a reasonable number of topics,
but this leads to less smooth distributions. In contrast, sparseTM allows a larger γ and selects more
smoothing, even with a smaller number of topics. The numbers of terms per topic for two models
don’t have a consistent trend, but they don’t differ too much either.

Example topics. For the NIPS data set, we provide some example topics (with top 15 terms)
discovered by HDP-LDA and sparseTM in Table 1. Accidentally, we found that HDP-LDA seems to
produce more noisy topics, such as, those shown in Table 2.

7

llllllllll1800020000220001150120012501300arXivcomplexityperplexitystmhdp−ldalllllllllllllll16000170001800019000700750800850Nematode Biologycomplexitystmhdp−ldalllllllllllllll34000400004600013501450NIPScomplexitystmhdp−ldalllllllllllllll19000210002300092096010001040Conf. abstractscomplexitystmhdp−ldallllllHDP−LDASTM0.010.030.05ggHDP−LDASTM0.020.040.060.08llHDP−LDASTM0.000.040.080.12lHDP−LDASTM0.010.03HDP−LDASTM8009001100#topicslHDP−LDASTM500600700HDP−LDASTM80012001600lHDP−LDASTM80010001200lHDP−LDASTM182022242628#terms per topiclHDP−LDASTM24262830HDP−LDASTM303540lHDP−LDASTM182022HDP-LDA
sparseTM
sparseTM HDP-LDA
variational
belief
svm
support
networks
networks
vector
vector
support
svm
jordan
inference
parameters
lower
kernel
machines
inference
bound
kernel
machines
bound
variational
svms
margin
belief
jordan
decision
training
http
vapnik
distributions
graphical
approximation
exact
solution
digit
ﬁeld
lower
machine
examples
probabilistic methods
diagonal
space
regression
sv
quadratic
approximate
ﬁeld
conditional
sparse
note
variables
distribution
kernels
optimization
intractable
misclassiﬁcation models
svms

Example “noise topics”
resulting
epsilon
mation
stream
direct
inferred
development
transfer
depicted
behaviour
global
motor
submitted
corner
carried
inter
applications
applicable
replicated
mixture
refers
served
speciﬁcation
searching
operates
modest
tension
vertical
class
matter

Table 1: Similar topics discovered.

Table 2: “Noise” topics in HDP-LDA.

5 Discussion

These results illuminate the issue with a single parameter controlling both sparsity and smoothing. In
the Gibbs sampler, if the HDP-LDA posterior requires more topics to explain the data, it will reduce
the value of γ to accommodate for the increased (necessary) sparseness. This smaller γ , however,
leads to less smooth topics that are less robust to “noise”, i.e., infrequent words that might populate
a topic. The process is circular: To explain the noisy words, the Gibbs sampler might invoke new
topics still, thereby further reducing the hyperparameter. As a result of this interplay, HDP-LDA
settles on more topics and a smaller γ . Ultimately, the ﬁt to held out data suffers.
For the sparseTM, however, more topics can be used to explain the data by using the sparsity control
gained from the “spike” component of the prior. The hyperparameter γ is controlled separately. Thus
the smoothing effect is retained, and held out performance is better.

Acknowledgements. We thank anonymous reviewers for insightful suggestions. David M. Blei is
supported by ONR 175-6343, NSF CAREER 0745520, and grants from Google and Microsoft.

References
[1] Teh, Y. W., M. I. Jordan, M. J. Beal, et al. Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566–1581, 2006.
[2] Blei, D., A. Ng, M. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, 2003.
[3] Griffths, T., M. Steyvers. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning.
2006.
[4] Saund, E. A multiple cause mixture model for unsupervised learning. Neural Comput., 7(1):51–71, 1995.
[5] Kab ´an, A., E. Bingham, T. Hirsim ¨aki. Learning to read between the lines: The aspect Bernoulli model. In
SDM. 2004.
[6] Ishwaran, H., J. S. Rao. Spike and slab variable selection: Frequentist and Bayesian strategies. The Annals
of Statistics, 33(2):730–773, 2005.
[7] Friedman, N., Y. Singer. Efﬁcient Bayesian parameter estimation in large discrete domains. In NIPS. 1999.
[8] Pitman, J. Poisson–Dirichlet and GEM invariant distributions for split-and-merge transformations of an
interval partition. Comb. Probab. Comput., 11(5):501–514, 2002.
[9] Sethuraman, J. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650, 1994.
[10] Escobar, M. D., M. West. Bayesian density estimation and inference using mixtures. Journal of the
American Statistical Association, 90:577–588, 1995.

8


Bayes-Adaptive POMDPs

St ´ephane Ross
McGill University
Montr ´eal, Qc, Canada
sross12@cs.mcgill.ca

Brahim Chaib-draa
Laval University
Qu ´ebec, Qc, Canada
chaib@ift.ulaval.ca

Joelle Pineau
McGill University
Montr ´eal, Qc, Canada
jpineau@cs.mcgill.ca

Abstract

Bayesian Reinforcement Learning has generated substantial interest recently, as it
provides an elegant solution to the exploration-exploitation trade-off in reinforce-
ment learning. However most investigations of Bayesian reinforcement learning
to date focus on the standard Markov Decision Processes (MDPs). Our goal is
to extend these ideas to the more general Partially Observable MDP (POMDP)
framework, where the state is a hidden variable. To address this problem, we in-
troduce a new mathematical model, the Bayes-Adaptive POMDP. This new model
allows us to (1) improve knowledge of the POMDP domain through interaction
with the environment, and (2) plan optimal sequences of actions which can trade-
off between improving the model, identifying the state, and gathering reward. We
show how the model can be ﬁnitely approximated while preserv ing the value func-
tion. We describe approximations for belief tracking and planning in this model.
Empirical results on two domains show that the model estimate and agent’s return
improve over time, as the agent learns better model estimates.

1

Introduction

In many real world systems, uncertainty can arise in both the prediction of the system’s behavior, and
the observability of the system’s state. Partially Observable Markov Decision Processes (POMDPs)
take both kinds of uncertainty into account and provide a powerful model for sequential decision
making under these conditions. However most solving methods for POMDPs assume that the model
is known a priori, which is rarely the case in practice. For instance in robotics, the POMDP must
reﬂect exactly the uncertainty on the robot’s sensors and ac tuators. These parameters are rarely
known exactly and therefore must often be approximated by a human designer, such that even if
this approximate POMDP could be solved exactly, the resulting policy may not be optimal. Thus we
seek a decision-theoretic planner which can take into account the uncertainty over model parameters
during the planning process, as well as being able to learn from experience the values of these
unknown parameters.

Bayesian Reinforcement Learning has investigated this problem in the context of fully observable
MDPs [1, 2, 3]. An extension to POMDP has recently been proposed [4], yet this method relies on
heuristics to select actions that will improve the model, thus forgoing any theoretical guarantee on
the quality of the approximation, and on an oracle that can be queried to provide the current state.

In this paper, we draw inspiration from the Bayes-Adaptive MDP framework [2], which is formu-
lated to provide an optimal solution to the exploration-exploitation trade-off. To extend these ideas
to POMDPs, we face two challenges: (1) how to update Dirichlet parameters when the state is a
hidden variable? (2) how to approximate the inﬁnite dimensi onal belief space to perform belief
monitoring and compute the optimal policy. This paper tackles both problem jointly. The ﬁrst prob-
lem is solved by including the Dirichlet parameters in the state space and maintaining belief states
over these parameters. We address the second by bounding the space of Dirichlet parameters to a
ﬁnite subspace necessary for ǫ-optimal solutions.

We provide theoretical results for bounding the state space while preserving the value function and
we use these results to derive approximate solving and belief monitoring algorithms. We compare
several belief approximations in two problem domains. Empirical results show that the agent is able
to learn good POMDP models and improve its return as it learns better model estimate.

2 POMDP

It has transition
S , actions A and observations Z .
A POMDP is deﬁned by ﬁnite sets of states
}s,s′∈S,a∈A where T sas′
probabilities {T sas′
= Pr(st+1 = s′ |st = s, at = a) and observation
probabilities {Osaz }s∈S,a∈A,z∈Z where Osaz = Pr(zt = z |st = s, at−1 = a). The reward function
R : S × A → R speci ﬁes the immediate reward obtained by the agent. In a POM DP, the state is
never observed. Instead the agent perceives an observation z ∈ Z at each time step, which (along
with the action sequence) allows it to maintain a belief state b ∈ ∆S . The belief state speci ﬁes
the probability of being in each state given the history of action and observation experienced so far,
starting from an initial belief b0 . It can be updated at each time step using Baye’s rule: bt+1 (s′ ) =
Os′ at zt+1 Ps∈S T sat s′
bt (s)
.
Ps′′ ∈s Os′′ at zt+1 Ps∈S T sat s′′ bt (s)
A policy π : ∆S → A indicates how the agent should select actions as a func-
Solving a POMDP involves ﬁnding t he optimal policy π∗
tion of
the current belief.
The return ob-
that maximizes the expected discounted return over the inﬁn ite horizon.
tained by following π∗ from a belief b is deﬁned by Bellman’s equation:
V ∗ (b) =
maxa∈A (cid:2)Ps∈S b(s)R(s, a) + γ Pz∈Z Pr(z |b, a)V ∗ (τ (b, a, z ))(cid:3), where τ (b, a, z ) is the new be-
lief after performing action a and observation z and γ ∈ [0, 1) is the discount factor.
Exact solving algorithms [5] are usually intractable, except on small domains with only a few states,
actions and observations. Various approximate algorithms, both ofﬂine [6, 7, 8] and online [9],
have been proposed to tackle increasingly large domains. However, all these methods requires full
knowledge of the POMDP model, which is a strong assumption in practice. Some approaches do
not require knowledge of the model, as in [10], but these approaches generally require a lot of data
and do not address the exploration-exploitation tradeoff.

3 Bayes-Adaptive POMDP

In this section, we introduce the Bayes-Adaptive POMDP (BAPOMDP) model, an optimal decision-
theoretic algorithm for learning and planning in POMDPs under parameter uncertainty. Throughout
we assume that the state, action, and observation spaces are ﬁnite and known, but that the transition
and observation probabilities are unknown or partially known. We also assume that the reward
function is known as it is generally speci ﬁed by the user for t he speci ﬁc task he wants to accomplish,
but the model can easily be generalised to learn the reward function as well.
To model the uncertainty on the transition T sas′ and observation Osaz parameters, we use Dirichlet
distributions, which are probability distributions over the parameters of multinomial distributions.
Given φi , the number of times event ei has occurred over n trials, the probabilities pi of each event
follow a Dirichlet distribution, i.e. (p1 , . . . , pk ) ∼ Dir(φ1 , . . . , φk ). This distribution represents
the probability that a discrete random variable behaves according to some probability distribution
(p1 , . . . , pk ), given that the counts (φ1 , . . . , φk ) have been observed over n trials (n = Pk
i=1 φi ). Its
i=1 pφi−1
B (φ) Qk
probability density function is deﬁned by: f (p, φ) = 1
, where B is the multinomial
i
beta function. The expected value of pi is E(pi ) = φi
.
Pk
j=1 φj

3.1 The BAPOMDP Model

The BAPOMDP is constructed from the model of the POMDP with unknown parameters. Let
(S, A, Z, T , O, R, γ ) be that model. The uncertainty on the distributions T sa· and Os′ a· can be
ss′ ∀s′ represents the number of times the transition (s, a, s′ ) oc-
represented by experience counts: φa
s′ z ∀z is the number of times observation z was made in state s′ after doing action
curred, similarly ψa
a. Let φ be the vector of all transition counts and ψ be the vector of all observation counts. Given

.

(1)

(2)

φa
ss′
Ps′′ ∈S φa
ss′′

, and

the count vectors φ and ψ , the expected transition probability for T sas′ is: T sas′
φ =
ψa
similarly for Os′ az : Os′ az
s′ z
ψ =
Pz′ ∈Z ψa
s′ z′
The objective of the BAPOMDP is to learn an optimal policy, such that actions are chosen to
maximize reward taking into account both state and parameter uncertainty. To model this, we
follow the Bayes-Adaptive MDP framework, and include the φ and ψ vectors in the state of
the BAPOMDP. Thus, the state space S ′ of the BAPOMDP is deﬁned as S ′ = S × T × O ,
where T = {φ ∈ N|S |2 |A| |∀(s, a), Ps′∈S φa
ss′ > 0} represents the space in which φ lies and
O = {ψ ∈ N|S ||A||Z | |∀(s, a), Pz∈Z ψa
sz > 0} represents the space in which ψ lies. The action and
observation sets of the BAPOMDP are the same as in the original POMDP. Transition and obser-
vation functions of the BAPOMDP must capture how the state and count vectors φ, ψ evolve after
every time step. Consider an agent in a given state s with count vectors φ and ψ , which performs
action a, causing it to move to state s′ and observe z . Then the vector φ′ after the transition is deﬁned
as φ′ = φ + δa
ss′ , where δa
ss′ is a vector full of zeroes, with a 1 for the count φa
ss′ , and the vector
ψ ′ after the observation is deﬁned as ψ ′ = ψ + δa
s′ z , where δa
s′ z is a vector full of zeroes, with a 1
for the count ψa
s′ z . Note that the probabilities of such transitions and observations occurring must
be deﬁned by considering all models and their probabilities
as speci ﬁed by the current Dirichlet
distributions, which turn out to be their expectations. Hence, we deﬁne T ′ and O ′ to be:
T ′ ((s, φ, ψ), a, (s′ , φ′ , ψ ′ )) = (cid:26) T sas′
φ Os′ az
ψ ,
0,
O ′ ((s, φ, ψ), a, (s′ , φ′ , ψ ′ ), z ) = (cid:26) 1,
0,
Note here that the observation probabilities are folded into the transition function, and that the ob-
servation function becomes deterministic. This happens because a state transition in the BAPOMDP
automatically speci ﬁes which observation is acquired afte r transition, via the way the counts are
incremented. Since the counts do not affect the reward, the reward function of the BAPOMDP is de-
ﬁned as R′ ((s, φ, ψ), a) = R(s, a); the discount factor of the BAPOMDP remains the same. Using
he tuple (S ′ , A, Z, T ′ , O ′ , R′ , γ ).
these deﬁnitions, the BAPOMDP has a known model speci ﬁed by t
The belief state of the BAPOMDP represents a distribution over both states and count values. The
model is learned by simply maintaining this belief state, as the distribution will concentrate over
most likely models, given the prior and experience so far.
If b0 is the initial belief state of the
unknown POMDP, and the count vectors φ0 ∈ T and ψ0 ∈ O represent the prior knowledge on this
POMDP, then the initial belief of the BAPOMDP is: b′
0 (s, φ0 , ψ0 ) = {b0 (s), if (φ, ψ) = (φ0 , ψ0 );
0, otherwise}. After actions are taken, the uncertainty on the POMDP model is represented by
mixtures of Dirichlet distributions (i.e. mixtures of count vectors).

if φ′ = φ + δa
ss′ and ψ ′ = ψ + δa
s′ z
otherwise.

if φ′ = φ + δa
ss′ and ψ ′ = ψ + δa
s′ z
otherwise.

Note that the BAPOMDP is in fact a POMDP with a countably inﬁni
te state space. Hence the belief
update function and optimal value function are still deﬁned as in Section 2. However these functions
now require summations over S ′ = S × T × O . Maintaining the belief state is practical only if the
number of states with non-zero probabilities is ﬁnite. We pr ove this in the following theorem:
Theorem 3.1. Let (S ′ , A, Z, T ′ , O ′ , R′ , γ ) be a BAPOMDP constructed from the POMDP
= {σ ∈ S ′ |b′
(S, A, Z, T , O, R, γ ). If S is ﬁnite, then at any time t, the set S ′
t (σ) > 0} has
b′
t
size |S ′
| ≤ |S |t+1 .
b′
t

Proof. Proof available in [11]. Proceeds by induction from b′
0 .

The proof of this theorem suggests that it is sufﬁcient to ite rate over S and S ′
in order to compute
b′
t−1
the belief state b′
t when an action and observation are taken in the environment. Hence, Algorithm
3.1 can be used to update the belief state.

3.2 Exact Solution for BAPOMDP in Finite Horizons

The value function of a BAPOMDP for ﬁnite horizons can be repr esented by a ﬁnite set Γ of func-
tions α : S ′ → R, as in standard POMDP. For example, an exact solution can be computed using

function τ (b, a, z )
Initialize b′ as a 0 vector.
for all (s, φ, ψ , s′ ) ∈ S ′
b × S do
φ Os′ az
s′ z ) + b(s, φ, ψ)T sas′
ss′ , ψ + δa
s′ z ) ← b′ (s′ , φ + δa
ss′ , ψ + δa
b′ (s′ , φ + δa
ψ
end for
return normalized b′

Algorithm 3.1: Exact Belief Update in BAPOMDP.

dynamic programming (see [5] for more details):

Γa
1
Γa,z
t
Γa
t
Γt

= {αa |αa (s, φ, ψ) = R(s, a)},
φ Os′ az
(s, φ, ψ) = γ Ps′∈S T sas′
= {αa,z
|αa,z
i (s′ , φ + δa
ψ α′
s′ z ), α′
ss′ , ψ + δa
i ∈ Γt−1},
i
i
a,z|Z |
t ⊕ Γa,z2
1 ⊕ Γa,z1
(where ⊕ is the cross sum operator),
= Γa
t ⊕ · · · ⊕ Γ
,
t
= Sa∈A Γa
t .
(3)
αa,z
fact
Note
that
from the
obtained
is
of
deﬁnition
here
that
the
(s, φ, ψ)
i
T ′ ((s, φ, ψ), a, (s′ , φ′ , ψ ′ ))O ′ ((s, φ, ψ), a, (s′ , φ′ , ψ ′ ), z ) = 0 except when φ′ = φ + δa
ss′ and
ψ ′ = ψ + δa
α(σ)b(σ). In
s′ z . The optimal policy is extracted as usual: πΓ (b) = argmaxα∈Γ Pσ∈S ′
b
practice, it will be impossible to compute αa,z
(s, φ, ψ) for all (s, φ, ψ) ∈ S ′ . In order to compute
i
these more efﬁciently, we show in the next section that the in ﬁnite state space can be reduced to a
ﬁnite state space, while still preserving the value functio n to arbitrary precision for any horizon t.

4 Approximating the BAPOMDP: Theory and Algorithms

Solving a BAPOMDP exactly for all belief states is impossible in practice due to the dimensionnality
of the state space (in particular to the fact that the count vectors can grow unbounded). We now show
how we can reduce this inﬁnite state space to a ﬁnite state spa
ce. This allows us to compute an ǫ-
optimal value function over the resulting ﬁnite-dimension nal belief space using standard POMDP
techniques. Various methods for belief tracking in the inﬁn ite model are also presented.

4.1 Approximate Finite Model

We ﬁrst present an upper bound on the value difference betwee n two states that differ only by
their model estimate φ and ψ . This bound uses the following deﬁnitions: given φ, φ′ ∈ T , and
S (φ, φ′ ) = Ps′∈S (cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
Z (ψ , ψ ′ ) = Pz∈Z (cid:12)(cid:12)(cid:12)
ψ ′ (cid:12)(cid:12)(cid:12)
T sas′
φ − T sas′
ψ , ψ ′ ∈ O , deﬁne Dsa
and Dsa
,
Osaz
ψ − Osaz
φ′
sz .
ss′ and N sa
and N sa
ψ = Pz∈Z ψa
φ = Ps′∈S φa
Theorem 4.1. Given any φ, φ′ ∈ T , ψ , ψ ′ ∈ O , and γ ∈ (0, 1), then for all t:
s,s′∈S,a∈A hDsa
S (φ, φ′ ) + Ds′ a
|αt (s, φ, ψ) − αt (s, φ′ , ψ ′ )| ≤ 2γ ||R||∞
Z (ψ , ψ ′ )
sup
sup
(1−γ )2
αt∈Γt ,s∈S
ln(γ−e ) (cid:18) Ps′′ ∈S |φa
ψ′ +1) (cid:19)(cid:21)
s′ z |
Pz∈Z |ψa
ss′′ |
s′ z −ψ ′a
ss′′ −φ′a
+ 4
φ′ +1) +
(N s′ a
ψ +1)(N s′ a
(N sa
φ +1)(N sa
Proof. Proof available in [11] ﬁnds a bound on a 1-step backup and sol ves the recurrence.

We now use this bound on the α-vector values to approximate the space of Dirichlet parameters
ven any ǫ > 0, deﬁne ǫ′ = ǫ(1−γ )2
within a ﬁnite subspace. We use the following deﬁnitions: gi
,
8γ ||R||∞
S = max (cid:16) |S |(1+ǫ′ )
Z = max (cid:16) |Z |(1+ǫ′ )
ǫ′′ = ǫ(1−γ )2 ln(γ−e )
ǫ′′ − 1(cid:17).
ǫ′′ − 1(cid:17) and N ǫ
, 1
, 1
, N ǫ
ǫ′
ǫ′
32γ ||R||∞
Theorem 4.2. Given any ǫ > 0 and (s, φ, ψ) ∈ S ′ such that ∃a ∈ A, s′ ∈ S , N s′ a
S or
φ > N ǫ
N s′ a
Z , then ∃(s, φ′ , ψ ′ ) ∈ S ′ such that ∀a ∈ A, s′ ∈ S , N s′ a
S and N s′ a
Z where
ψ > N ǫ
φ′ ≤ N ǫ
ψ ′ ≤ N ǫ
|αt (s, φ, ψ) − αt (s, φ′ , ψ ′ )| < ǫ holds for all t and αt ∈ Γt .

Proof. Proof available in [11].

Theorem 4.2 suggests that if we want a precision of ǫ on the value function, we just need to restrict
the space of Dirichlet parameters to count vectors φ ∈ ˜Tǫ = {φ ∈ N|S |2 |A| |∀a ∈ A, s ∈ S, 0 <
Z }. Since ˜Tǫ and ˜Oǫ are
S } and ψ ∈ ˜Oǫ = {ψ ∈ N|S ||A||Z | |∀a ∈ A, s ∈ S, 0 < N sa
ψ ≤ N ǫ
φ ≤ N ǫ
N sa
( ˜Sǫ , A, Z, ˜Tǫ , ˜Oǫ , ˜Rǫ , γ ) where
ﬁnite, we can deﬁne a ﬁnite approximate BAPOMDP as the tuple
˜Sǫ = S × ˜Tǫ × ˜Oǫ is the ﬁnite state space. To deﬁne the transition and observa
tion functions over
that ﬁnite state space, we need to make sure that when the coun t vectors are incremented, they stay
n operator Pǫ : S ′ → ˜Sǫ that simply
within the ﬁnite space. To achieve, this we deﬁne a projectio
projects every state in S ′ to their closest state in ˜Sǫ .
Deﬁnition 4.1. Let d : S ′ × S ′ → R be deﬁned such that:
s,s′∈S,a∈A hDsa
S (φ, φ′ ) + Ds′ a
2γ ||R||∞

Z (ψ , ψ ′ )
sup
(1−γ )2
ψ′ +1) (cid:19)(cid:21) ,
ln(γ−e ) (cid:18) Ps′′ ∈S |φa
ss′′ −φ′a
s′ z −ψ ′a
Pz∈Z |ψa
ss′′ |
s′ z |
+ 4
φ′ +1) +
ψ +1)(N as′
(N as′
(N as
φ +1)(N as

(1−γ )2 (cid:16)1 + 4
ln(γ−e ) (cid:17) + 2||R||∞
8γ ||R||∞
(1−γ ) ,
Deﬁnition 4.2. Let Pǫ : S ′ → ˜Sǫ be deﬁned as Pǫ (s) = arg min
s′∈ ˜Sǫ

d(s, φ, ψ , s′ , φ′ , ψ ′ ) =

otherwise.

if s = s′

d(s, s′ )

if (s′ , φ′ , ψ ′ ) = Pǫ (s′ , φ + δa
ss′ , ψ + δa
s′ z )
otherwise.

The function d uses the bound deﬁned in Theorem 4.1 as a distance between sta tes that only differs
by their φ and ψ vectors, and uses an upper bound on that value when the states differ. Thus
Pǫ always maps states (s, φ, ψ) ∈ S ′ to some state (s, φ′ , ψ ′ ) ∈ ˜Sǫ . Note that if σ ∈ ˜Sǫ , then
Pǫ (σ) = σ . Using Pǫ , the transition and observation function are deﬁned as foll ows:
φ Os′ az
˜Tǫ ((s, φ, ψ), a, (s′ , φ′ , ψ ′ )) = (cid:26) T sas′
ψ ,
0,
if (s′ , φ′ , ψ ′ ) = Pǫ (s′ , φ + δa
˜Oǫ ((s, φ, ψ), a, (s′ , φ′ , ψ ′ ), z ) = (cid:26) 1,
ss′ , ψ + δa
s′ z )
otherwise.
0,
These deﬁnitions are the same as the one in the inﬁnite BAPOMD P, except that now we add an extra
projection to make sure that the incremented count vectors stays in ˜Sǫ . Finally, the reward function
˜Rǫ : ˜Sǫ × A → R is deﬁned as
˜Rǫ ((s, φ, ψ), a) = R(s, a).
Theorem 4.3 bounds the value difference between α-vectors computed with this ﬁnite model and
the α-vector computed with the original model.
Theorem 4.3. Given any ǫ > 0, (s, φ, ψ) ∈ S ′ and αt ∈ Γt computed from the inﬁnite BAPOMDP.
Let ˜αt be the α-vector representing the same conditionnal plan as αt but computed with the ﬁnite
BAPOMDP ( ˜Sǫ , A, Z, ˜Tǫ , ˜Oǫ , ˜Rǫ , γ ), then | ˜αt (Pǫ (s, φ, ψ)) − αt (s, φ, ψ)| < ǫ
1−γ .

(4)

(5)

Proof. Proof available in [11]. Solves a recurrence over the 1-step approximation in Thm. 4.2.

Because the state space is now ﬁnite, solution methods from t he literature on ﬁnite POMDPs could
theoretically be applied. This includes en particular the equations for τ (b, a, z ) and V ∗ (b) that were
presented in Section 2. In practice however, even though the state space is ﬁnite, it will generally
be very large for small ǫ, such that it may still be intractable, even for small domains. We therefore
favor a faster online solution approach, as described below.

4.2 Approximate Belief Monitoring

As shown in Theorem 3.1, the number of states with non-zero probability grows exponentially in
the planning horizon, thus exact belief monitoring can quickly become intractable. We now discuss
different particle-based approximations that allow polynomial-time belief tracking.
Monte Carlo sampling: Monte Carlo sampling algorithms have been widely used for sequential
state estimation [12]. Given a prior belief b, followed by action a and observation z , the new belief
b′ is obtained by ﬁrst sampling K states from the distribution b, then for each sampled s a new state
s′ is sampled from T (s, a, ·). Finally, the probability O(s′ , a, z ) is added to b′ (s′ ) and the belief b′
is re-normalized. This will capture at most K states with non-zero probabilities. In the context of

BAPOMDPs, we use a slight variation of this method, where (s, φ, ψ) are ﬁrst sampled from b, and
then a next state s′ ∈ S is sampled from the normalized distribution T sa·
φ O ·az
ψ . The probability 1/K
is added directly to b′ (s′ , φ + δa
s′ z ).
ss′ , ψ + δa
Most Probable: Alternately, we can do the exact belief update at a given time step, but then only
keep the K most probable states in the new belief b′ and renormalize b′ .
Weighted Distance Minimization: The two previous methods only try to approximate the distribu-
tion τ (b, a, z ). However, in practice, we only care most about the agent’s expected reward. Hence,
instead of keeping the K most likely states, we can keep K states which best approximate the be-
lief ’s value. As in the Most Probable method, we do an exact belief update, however in this case
we ﬁt the posterior distribution using a greedy K -means procedure, where distance is deﬁned as in
Deﬁnition 4.1, weighted by the probability of the state to re move. See [11] for algorithmic details.

4.3 Online planning

While the ﬁnite model presented in Section 4.1 can be used to ﬁn
d provably near-optimal policies
ofﬂine, this will likely be intractable in practice due to th e very large state space required to ensure
good precision. Instead, we turn to online lookahead search algorithms, which have been proposed
for solving standard POMDPs [9]. Our approach simply performs dynamic programming over all the
beliefs reachable within some ﬁxed ﬁnite planning horizon f
rom the current belief. The action with
highest return over that ﬁnite horizon is executed and then p lanning is conducted again on the next
belief. To further limit the complexity of the online planning algorithm, we used the approximate
belief monitoring methods detailed above. Its overall complexity is in O((|A||Z |)D Cb ) where D is
the planning horizon and Cb is the complexity of updating the belief.

5 Empirical Results

We begin by evaluating the different belief approximations introduced above. To do so, we use a
simple online d-step lookahead search, and compare the overall expected return and model accuracy
in two different problems: the well-known Tiger [5] and a new domain called Follow. Given T sas′
and Os′ az the exact probabilities of the (unknown) POMDP, the model accuracy is measured in
terms of the weighted sum of L1-distance, denoted W L1, between the exact model and the probable
models in a belief state b:
W L1(b) = P(s,φ,ψ)∈S ′
b(s, φ, ψ)L1(φ, ψ)
b
L1(φ, ψ) = Pa∈A Ps′∈S hPs∈S |T sas′
φ − T sas′
5.1 Tiger

ψ − Os′ az |i
| + Pz∈Z |Os′ az

(6)

In the Tiger problem [5], we consider the case where the transition and reward parameters are known,
but the observation probabilities are not. Hence, there are four unknown parameters: OLl , OLr ,
ORl , ORr (OLr stands for Pr(z = hear right|s = tiger Lef t, a = Listen)). We deﬁne the
observation count vector ψ = (ψLl , ψLr , ψRl , ψRr ). We consider a prior of ψ0 = (5, 3, 3, 5), which
speci ﬁes an expected sensor accuracy of 62.5% (instead of the correct 85%) in both states. Each
simulation consists of 100 episodes. Episodes terminate when the agent opens a door, at which
point the POMDP state (i.e. tiger’s position) is reset, but the distribution over count vector is carried
over to the next episode.

Figures 1 and 2 show how the average return and model accuracy evolve over the 100 episodes
(results are averaged over 1000 simulations), using an online 3-step lookahead search with varying
belief approximations and parameters. Returns obtained by planning directly with the prior and ex-
act model (without learning) are shown for comparison. Model accuracy is measured on the initial
belief of each episode. Figure 3 compares the average planning time per action taken by each ap-
proach. We observe from these ﬁgures that the results for the Most Probable and Weighted Distance
approximations are very similar and perform well even with few particles (lines are overlapping in
many places, making Weighted Distance results hard to see). On the other hand, the performance
of Monte Carlo is signi ﬁcantly affected by the number of part
icles and had to use much more par-

Exact model

2

1

0

n
r
u
t
e
R

−1

−2

−3

−4

Prior model

Most Probable (2)
Monte Carlo (64)
Weighted Distance (2)

0

20

40
60
Episode

80

100

Most Probable (2)
Monte Carlo (64)
Weighted Distance (2)

1

0.8

0.6

0.4

0.2

1
L
W

0
0

20

40
60
Episode

80

100

)
s
m
(
 
n
o
i
t
c
A
/
e
m
i
T
 
g
n
i
n
n
a
l
P

20

15

10

5

0

MP (2)

MC (64)

WD (2)

Figure 1: Return with different
belief approximations.

Figure 2: Model accuracy with
different belief approximations.

Figure 3: Planning Time with
different belief approximations.

ticles (64) to obtain an improvement over the prior. This may be due to the sampling error that is
introduced when using fewer samples.

5.2 Follow

We propose a new POMDP domain, called Follow, inspired by an interactive human-robot task. It
is often the case that such domains are particularly subject to parameter uncertainty (due to the dif-
ﬁculty in modelling human behavior), thus this environment motivates the utility of Bayes-Adaptive
POMDP in a very practical way. The goal of the Follow task is for a robot to continuously follow one
of two individuals in a 2D open area. The two subjects have different motion behavior, requiring the
robot to use a different policy for each. At every episode, the target person is selected randomly with
P r = 0.5 (and the other is not present). The person’s identity is not observable (except through their
motion). The state space has two features: a binary variable indicating which person is being fol-
lowed, and a position variable indicating the person’s position relative to the robot (5 × 5 square grid
with the robot always at the center). Initially, the robot and person are at the same position. Both the
robot and the person can perform ﬁve motion actions {N oAction, N orth, E ast, S outh, W est}.
The person follows a ﬁxed stochastic policy (stationary ove r space and time), but the parameters of
this behavior are unknown. The robot perceives observations indicating the person’s position rela-
tive to the robot: {S ame, N orth, E ast, S outh, W est, U nseen}. The robot perceives the correct
observation P r = 0.8 and U nseen with P r = 0.2. The reward R = +1 if the robot and person
are at the same position (central grid cell), R = 0 if the person is one cell away from the robot, and
R = −1 if the person is two cells away. The task terminates if the person reaches a distance of 3
cells away from the robot, also causing a reward of -20. We use a discount factor of 0.9.

When formulating the BAPOMDP, the robot’s motion model (deterministic),
the observation
probabilities and the rewards are assumed to be known. We maintain a separate count vec-
tor for each person, representing the number of times they move in each direction, i.e. φ1 =
W ), φ2 = (φ2
W ). We assume a prior φ1
(φ1
N A , φ1
N , φ1
E , φ1
S , φ1
N A , φ2
N , φ2
E , φ2
S , φ2
0 = (2, 3, 1, 2, 2)
0 = (2, 1, 3, 2, 2) for person 2, while in reality person 1 moves with probabilities
for person 1 and φ2
P r = (0.3, 0.4, 0.2, 0.05, 0.05) and person 2 with P r = (0.1, 0.05, 0.8, 0.03, 0.02). We run 200
simulations, each consisting of 100 episodes (of at most 10 time steps). The count vectors’ distri-
butions are reset after every simulation, and the target person is reset after every episode. We use a
2-step lookahead search for planning in the BAPOMDP.

Figures 4 and 5 show how the average return and model accuracy evolve over the 100 episodes (aver-
aged over the 200 simulations) with different belief approximations. Figure 6 compares the planning
time taken by each approach. We observe from these ﬁgures tha t the results for the Weighted Dis-
tance approximations are much better both in terms of return and model accuracy, even with fewer
particles (16). Monte Carlo fails at providing any improvement over the prior model, which indi-
cates it would require much more particles. Running Weighted Distance with 16 particles require
less time than both Monte Carlo and Most Probable with 64 particles, showing that it can be more
time efﬁcient for the performance it provides in complex env ironment.

2

0

−2

−4

−6

n
r
u
t
e
R

Exact model

Most Probable (64)
Monte Carlo (64)
Weighted Distance (16)

Prior model

2

1.5

1
L
W

1

0.5

Most Probable (64)
Monte Carlo (64)
Weighted Distance (16)

−8

0

20

40
60
Episode

80

100

0
0

20

40
60
Episode

80

100

)
s
m
(
 
n
o
i
t
c
A
/
e
m
i
T
 
g
n
i
n
n
a
l
P

200

150

100

50

0

MP (64)

MC (64)

WD (16)

Figure 4: Return with different
belief approximations.

Figure 5: Model accuracy with
different belief approximations.

Figure 6: Planning Time with
different belief approximations.

6 Conclusion

The objective of this paper was to propose a preliminary decision-theoretic framework for learning
and acting in POMDPs under parameter uncertainty. This raises a number of interesting challenges,
including (1) deﬁning the appropriate model for POMDP param eter uncertainty, (2) approximating
this model while maintaining performance guarantees, (3) performing tractable belief updating, and
(4) planning action sequences which optimally trade-off exploration and exploitation.

We proposed a new model, the Bayes-Adaptive POMDP, and showed that it can be approximated
to ǫ-precision by a ﬁnite POMDP. We provided practical approach es for belief tracking and online
planning in this model, and validated these using two experimental domains. Results in the Follow
problem, showed that our approach is able to learn the motion patterns of two (simulated) individu-
als. This suggests interesting applications in human-robot interaction, where it is often essential that
we be able to reason and plan under parameter uncertainty.

Acknowledgments

This research was supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Fonds Qu ´eb ´ecois de la Recherche sur la Nature et les Technologies (FQRNT).

References
[1] R. Dearden, N. Friedman, and N. Andre. Model based bayesian exploration. In UAI, 1999.
[2] M. Duff. Optimal Learning: Computational Procedure for Bayes-Adaptive Markov Decision Processes.
PhD thesis, University of Massachusetts, Amherst, USA, 2002.
[3] P. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic solution to discrete bayesian reinforcement
learning. In Proc. ICML, 2006.
[4] R. Jaulmes, J. Pineau, and D. Precup. Active learning in partially observable markov decision processes.
In ECML, 2005.
[5] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic
domains. Arti ﬁcial Intelligence , 101:99–134, 1998.
[6] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: an anytime algorithm for POMDPs. In
IJCAI, pages 1025–1032, Acapulco, Mexico, 2003.
[7] M. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for POMDPs. JAIR, 24:195–
220, 2005.
[8] T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In UAI, Banff, Canada, 2004.
[9] S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for complex multiagent environ-
ments. In AAMAS, 2005.
[10] Jonathan Baxter and Peter L. Bartlett. Inﬁnite-horizon policy-grad ient estimation. Journal of Artiﬁcial
Intelligence Research (JAIR), 15:319–350, 2001.
[11] St ´ephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps. Technical Report SOCS-
TR-2007.6, McGill University, 2007.
[12] A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods In Practice. Springer, 2001.


Bayesian Inference for Spiking Neuron Models
with a Sparsity Prior

Sebastian Gerwinn

Jakob H Macke

Matthias Seeger

Matthias Bethge
Max Planck Institute for Biological Cybernetics
Spemannstrasse 41
72076 Tuebingen, Germany
{firstname.surname}@tuebingen.mpg.de

Abstract

Generalized linear models are the most commonly used tools to describe the stim-
ulus selectivity of sensory neurons. Here we present a Bayesian treatment of such
models. Using the expectation propagation algorithm, we are able to approximate
the full posterior distribution over all weights. In addition, we use a Laplacian
prior to favor sparse solutions. Therefore, stimulus features that do not critically
inﬂuence neural activity will be assigned zero weights and thus be effectively
excluded by the model. This feature selection mechanism facilitates both the in-
terpretation of the neuron model as well as its predictive abilities. The posterior
distribution can be used to obtain conﬁdence intervals which makes it possible
to assess the statistical signiﬁcance of the solution. In neural data analysis, the
available amount of experimental measurements is often limited whereas the pa-
rameter space is large. In such a situation, both regularization by a sparsity prior
and uncertainty estimates for the model parameters are essential. We apply our
method to multi-electrode recordings of retinal ganglion cells and use our uncer-
tainty estimate to test the statistical signiﬁcance of functional couplings between
neurons. Furthermore we used the sparsity of the Laplace prior to select those
ﬁlters from a spike-triggered covariance analysis that are most informative about
the neural response.

1

Introduction

A central goal of systems neuroscience is to identify the functional relationship between environ-
mental stimuli and a neural response. Given an arbitrary stimulus we would like to predict the neural
response as well as possible. In order to achieve this goal with limited amount of data, it is essential
to combine the information in the data with prior knowledge about neural function. To this end,
generalized linear models (GLMs) have proven to be particularly useful as they allow for ﬂexible
model architectures while still being tractable for estimation.
The GLM neuron model consists of a linear ﬁlter, a static nonlinear transfer function and a Poisson
spike generating mechanism. To determine the neural response to a given stimulus, the stimulus
is ﬁrst convolved with the linear ﬁlter (i.e. the receptive ﬁeld of the neuron). Subsequently, the
ﬁlter output is converted into an instantaneous ﬁring rate via a static nonlinear transfer function,
and ﬁnally spikes are generated from an inhomogeneous Poisson-process according to this ﬁring
rate. Note, however, that the GLM neuron model is not limited to describe neurons with Poisson
ﬁring statistics. Rather, it is possible to incorporate inﬂuences of its own spiking history on the
neural response. That is, the ﬁring rate is then determined by a combination of both the external

1

stimulus and the spiking-history of the neuron. Thus, the model can account for typical effects
such as refractory periods, bursting behavior or spike-frequency adaptation. Last but not least, the
GLM neuron model can also be applied for populations of coupled neurons by making each neuron
dependent not only on its own spiking activity but also on the spiking history of all the other neurons.
In previous work (Pillow et al., 2005; Chornoboy et al., 1988; Okatan et al., 2005) it has been
shown how point-estimates of the GLM-parameters can be obtained using maximum-likelihood (or
maximum a posteriori (MAP)) techniques. Here, we extend this approach one step further by using
Bayesian inference methods in order to obtain an approximation to the full posterior distribution,
rather than point estimates. In particular, the posterior determines conﬁdence intervals for every
linear weight, which facilitates the interpretation of the model and its parameters. For example, if
a weight describes the strength of coupling between two neurons, then we can use these conﬁdence
intervals to test whether this weight is signiﬁcantly different from zero. In this way, we can readily
distinguish statistical signiﬁcant interactions between neurons from spurious couplings.
Another application of the Bayesian GLM neuron model arises in the context of spike-triggered
covariance analysis. Spike-triggered covariance basically employs a quadratic expansion of the
external stimulus parameter space and is often used in order to determine the most informative
subspace. By combining spike-triggered covariance analysis with the Bayesian GLM framework,
we will present a new method for selecting the ﬁlters of this subspace.
Feature selection in the GLM neuron model can be done by the assumption of a Laplace prior over
the linear weights, which naturally leads to sparse posterior solutions. Consequently, all weights
are equally strongly pushed to zero. This contrasts the Gaussian prior which pushes weights to zero
proportional to their absolute value. In this sense, the Laplace prior can also be seen as an efﬁcient
regularizer, which is well suited for the situation when a large range of alternative explanations for
the neural response shall be compared on the basis of limited data. As we do not perform gradient
descent on the posterior, differentiability of the posterior is not required.
The paper is organized as follows: In section 2, we describe the model, and the “expectation prop-
agation” algorithm (Minka, 2001; Opper & Winther, 2000) used to ﬁnd the approximate posterior
distribution. In section 3, we estimate the receptive ﬁelds, spike-history effects and functional cou-
plings of a small population of retinal ganglion cells. We demonstrate that for small training sets,
the Laplace-prior leads to superior performance compared to a Gaussian-prior, which does not lead
to sparse solutions. We use the conﬁdence intervals to test whether the functional couplings between
the neurons are signiﬁcant.
In section 4, we use the GLM neuron model to describe a complex cell response recorded in macaque
primary visual cortex: After computing the spike-triggered covariance (STC) we determine the
relevant stimulus subspace via feature selection in our model. In contrast to the usual approach, the
selection of the subspace in our case becomes directly linked to an explicit neuron model which also
takes into account the spike-history dependence of the spike generation.

2 Generalized Linear Models and Expectation Propagation

2.1 Generalized Linear Models
Let Xt ∈ Rd , t ∈ [0, T ] denote a time-varying stimulus and Di = {ti,j } the spike-times of i =
1, . . . , n neurons. Here Xt consists of the sensory input at time t and can include preceeding input
frames as well. We assume that the stimulus can only change at distinct time points, but can be
evaluated at continous time t. We would like to incorporate spike-history effects, couplings between
neurons and dependence on nonlinear features of the stimulus. Therefore, we describe the effective
ψ(t) = ψst (Xt ) M
input to a neuron via the following feature-map:
ψsp ({ti,j ∈ Di : ti,j < t}),
i
where ψsp represents the spike time history and ψst the possibly nonlinear feature map for the
stimulus. That is, the complete feature vector ψ contains possibly nonlinear features of the stimulus
and the spike history of every neuron. Any feature which is causal in the sense that it does not
depend on future events can be used. We model the spike history dependence by a set of small time

2

,

(1)

t∈Di

.

ui,j = wT
i ψj ,

l ) (ti,j )
1[t−τl ,t−τ 0

(ψsp,i ({ti,j ∈ Di : ti,j < t}))l = X
windows [t − τl , t − τ 0
l ) in which occuring spikes are counted.
j :ti,j <t
where 1[a,b) (t) denotes the indicator function which is one if t ∈ [a, b) and zero otherwise. In other
words, for each neuron there is a set of windows l = 1, . . . , L with time-lags τl and width τl − τ 0
l
describing its spiking history. More precisely, the rate can only change if the stimulus changes
or a spike leaves or enters one of these windows. Thus, we obtain a sequence of changepoints
0 = ˜t0 < ˜t1 < · · · < ˜tj < · · · < T , where each feature ψ i (t) is constant in [˜tj−1 , ˜tj ), attaining the
value ψi,j . In the GLM neuron model setting the instantanious ﬁring rate of neuron i is obtained by
a linear ﬁlter of the feature map:
p(spike|Xt , {ti,j ∈ D : ti,j < t}) = λ(wT
i ψ(t)),
the likelihood P (D|{w}) = Qn
where λ is the nonlinear transfer function. Following general point process theory (Snyder & Miller,
1991) and using the fact that the features stay constant between two changepoints we can write down
Li (wi ) ∝ Y
i=1 Li (wi ), where each Li (wi ) has the form
φi,j (ui,j ),
P
j
δ(t−˜tj ) exp(−λi (ui,j )(˜tj − ˜tj−1 ))
φi,j (ui,j ) = λi (ui,j )
The function δ(.) in the second equation is deﬁned to be one if and only if its argument equals zero.
The sum therfore is 1 iff a spike of neuron i occurs at changepoint ˜tj . Note that the changepoints
˜tj depend on the spikes and therefore, the process is not Poissonian, as it might be suggested by the
functional form of the likelihood.
As it has been shown in (Paninski, 2004), the likelihood is log-concave in wi if λi (·) is both convex
and log-concave. We are using the transfer function λi (u) = eu which, in particular, gives rise to a
log-linear point process model. Alternatively, one could also use λi (u) = eu1u<0 + (1 + u)1u≥0 ,
which grows only linearly (cf. Harris et al. (2003); Pillow et al. (2005)).
While we require all rates λi (t) to be piecewise constant, it should be noted that we do not restrict
ourselves to a uniform quantization of the time axis. In this way, we achieve an efﬁcient architecture
for which the density of change points automatically adapts to the speed with which the input signal
is changing.
The choice of the prior distribution can play a central role when coping with limited amount of data.
We use a Laplace prior distribution over the weights in order to favor sparse solutions over those
P (wi ) ∝ Y
which explain the data equally well but require more weights different from zero (c.f. Tibshirani
(1996)):
e−ρk |wk,i | .
(2)
k
2 exp(−ρk |ui,k |) with ψk = (1l=k )l and ui,k =
Thus, prior factors have the form φi,k (ui,k ) = ρk
wT
of the stimulus-dependent
i ψk as above. In our applications, we allowed the prior variance 2
P (w|D) ∝ Y
ρ2
k
features to be different from the variance of the spike-history features. The posterior takes the form:
i,j
where each φi,j individually instantiates a Generalized Linear Model (either corresponding to a
likelihood factor or to a prior factor). As the posterior factorizes over neurons, we can perform
our analysis for each neuron seperately. Therefore, for simplicity we drop the subscript i in the
following.
Our model does not assume or require any speciﬁc stimulus distribution.
In particular, it is not
limited to white noise stimuli or elliptically contoured distributions but it can be used without mod-
iﬁcation for other stimulus distributions such as natural image sequences. Finally, this framework
allows exact sampling of spike trains due to the piecewise constant rate.

φi,j (ui,j ),

3

2.2 Expectation Propagation

As exact Bayesian inference is intractable in our model, we seek to ﬁnd a good approximation to the
full posterior. In our case all likelihood and prior factors are log-concave. Therefore, the posterior is
unimodal and a Gaussian approximation is well suited. A frequently used technique for this purpose
is the Laplace-approximation which computes a quadratic approximation to the log-posterior based
on the Hessian around the maximum. For the Laplacian prior, however, this approach falls short
since the distribution is not differentiable at zero. Instead, we employ the Expectation Propagation
(EP) algorithm (Minka, 2001; Opper & Winther, 2000). In this approximation technique, each factor
(also called site) φj of the posterior is replaced by an unnormalised Gaussian:
N U (uj |bj , πj ) = exp(− 1
πj ≥ 0
j + bj uj ) =: ˆφ(uj ),
2 πj u2
Leibler divergence between the full posterior P (w|D) and the approximation, Q(w) ≈ Q
where the bj , πj are called the site parameters. The approximation aims at minimizing the Kullback-
ˆφ(uj ).
The log-concavity of the model implies that all πj ≥ 0, which supports the numerical stability of
j
the EP algorithm. Some of the πj may even be 0, as long as Q(w) is a (normalizable) Gaussian. An
EP update at j consists of computing the Gaussian cavity distribution Q\j ∝ Q ˆφ−1
and the non-
j
Gaussian tilted distribution ˆP ∝ Q\j φj , then updating bj , πj such that the new Q0 has the same
mean and covariance as ˆP (moment matching). This is iterated in random order over the sites until
convergence.
We omit the detailed update schemes here and refer to (Seeger et al., 2007; Seeger, 2005). Conver-
gence guarantees for EP applied to non-Gaussian log-concave models have not been shown so far.
Nevertheless it is reported that at least in the log-concave case EP behaves stable (e.g., Rasmussen
& Williams (2006)), and we observe quick convergence in our case ( ≤ 20 iterations over all sites
are required). The model still contains hyperparameters, namely the prior variances 2
. In each ex-
ρ2
k
periment, these were determined via a standard crossvalidation procedure (80% training data, 10%
validation, 10% test).

3 Modeling retinal ganglion cells: Which cells are functionally coupled?

n
o

e
r
o
c
s

t
e
s
-
t
s
e
t

We applied the GLM neuron model to multi-electrode recordings of three rabbit retinal ganglion
cells. The stimulus consisted of 32767 frames each of which showing a random 16×16 checkerboard
pattern with a refresh rate of 50 Hz (data provided by G. Zeck, see (Zeck et al., 2005)).
First,
in order to investigate the role of the
Laplace prior, we trained a single cell GLM neu-
ron model on datasets of different sizes with ei-
ther a Laplace prior or a Gaussian prior. The
models, which have the same number of param-
eters, were compared by evaluating their nega-
tive log-likelihood on an independent test set. As
can be seen on the right the choice of prior be-
comes less important for large training sets as the
weights are sufﬁciently constrained by the data.
For each training set size a separate crossvalida-
tion was carried out. Errorbars were obtained by
training data-set size (% of complete dataset)
drawing 100 samples from the posterior.
Fig. 1 shows the spatiotemporal receptive ﬁeld of each neuron, as well as the ﬁlters describing
the inﬂuence of spiking history and input from other cells. For conciseness, we only plot the ﬁlters
for 80 and 120 ms time lags, but the ﬁtted model included 60 and 140 ms time lags as well. The
strongly positive weights on the diagonal of ﬁgure 1(c) for the spiking history can be interpreted
as “self-excitation”. In this way, it is possible to model the bursting behavior exhibited by the cells
in our recordings (see also Fig. 2). The strongly negative weights at small time lags represent re-
fractory periods. The red lines correspond to 3 standard deviations of the posterior. The ﬁrst neuron
seems to elicit ”bursts” at lower frequencies. Note the different scaling of the y-axis for diagonal and
off-diagonal terms. By analyzing the coupling terms, we can see that there is signiﬁcant interaction

d
o
o
h
i
l
e
k
i
l

g
o
l

.
g
e
n

4

between cells 2 and 3, but not between any other pair of cells. As our prior assumption is that the
couplings are 0, this interaction-term is not merely a consequence of our choice of prior. As a result
of our crossvalidation it turns out that the prior variance for spike history weights should be set to
very large values (ρ= 0.1, variance = 2 1
ρ2 ) meaning that these are well determinated by the data. In
contrast, prior variances for the stimulus weights should be more strongly biased towards zero (ρ =
150).

(a) GLM

(b) STA

(c)

Figure 1: (a): Stimulus dependence inferred by the GLM for the three neurons (columns) at different
time lags (rows). 2 of 4 time lags are plotted (60, 140 ms not shown). (b): Spike-triggered average
for the same neurons and time lags as in (a). (c): Causal dependencies between the three neurons.
Each plot shows the value of the linear weight as a function of increasing time lag τl (in ms). Shown
are posterior mean and three std. dev. (indicated in red). Different scaling of the y-axis is used for
diagonal and off-diagonal plots.

Neuron 1
Neuron 2
Neuron 3
Mean

STA
0.2199
0.1746
0.1828
0.1924

GLM GLM with couplings
0.3576
0.2442
0.2348
0.3320
0.4202
0.3319
0.2703
0.3699

Table 1: Predictions performance of different models. Entries correspond to the correlation coef-
ﬁcient between the predicted rate of each model and spikes on a test set. Both rate and spikes are
binned in 5 ms bins. The ﬁrst GLM models neither connections nor self-feedback.

Because of the regularization by the prior the spatio-temporal receptive ﬁelds are much smoother
than the spike-triggered average ones, see Fig. 1(a). The receptive ﬁelds of the STA seems to be

5

Figure 2: Predicted rate for the GLM neuron model with and without any spike history and the
predicted rate for the STA for the same neurons as in the other plots. For the STA the linear response
is rectiﬁed. Rate for the GLM with spike dependence is obtained by averaging over 1000 sampled
spike-trains. Rates are rescaled to have the same standard deviation.

more smeared out which might be due to the fact that it cannot model bursting behavior. The more
conservative estimate of the sparse neuron model should increase the prediction performance. To
verify this, we calculated the linear response from the spike-triggered average and the rate of our
GLM neuron model. In order to have the same number of parameters we neglected all connections.
As a model free performance measure we used the correlation coefﬁcient between the spike trains
and the rates (each are binned in 5 ms bins). For the GLM with couplings, rates were estimated
by sampling 1000 spike trains with the posterior mean as linear weights. As our model explicitly
includes the nonlinearity during ﬁtting, the rate is more sharply peaked around the spikes, see Fig. 2.
The prediction performance can be increased even further by modeling couplings between neurons
as summarized in Tab. 1.

4 Modeling complex cells: How many ﬁlters do we need?

Complex cells in primary visual cortex exhibit strongly nonlinear response properties which cannot
be well described by a single linear ﬁlter, but rather requires a set of ﬁlters. A common approach
for ﬁnding these ﬁlters is based on the covariance of the spike-triggered ensemble: Eigenvectors of
eigenvalues that are much bigger (or smaller) than the eigenvalues of the whole stimulus ensemble
indicate directions in stimulus space to which the cell is sensitive to. Usually, a statistical hypothesis
test on the eigenvalue-spectrum is used to decide how many of the eigenvectors ei are needed to
model the cells (Simoncelli et al., 2004; Touryan et al., 2002; Rust et al., 2005; Steveninck & Bialek,
1988). Here, we take a different approach: We use the conﬁdence intervals of our GLM neuron
model to determine the relevant dimensions within the subspace revealed by STC. We ﬁrst apply
STC to ﬁnd the space spanned by a set of eigenvectors that is substantially larger than the expected
dimensionality of the relevant subspace. Next, we ﬁt a nonlinear function ni to the ﬁlter-outputs
fi (Xt ) = hXt , ei i. Finally, we linearly combine the ni (t), resulting in a model of the same form as
equation (1) with (ψst )i (Xt ) = ni (fi (Xt ))

6

(a)

(b)

Figure 3: (a): 24 out of 40 Filters estimated by STC. The ﬁlters are ordered according to their log-
ratio of their eigenvalue to the corresponding eigenvalue of the complete stimulus ensemble (from
left to right). Highlighted ﬁlter are those with signiﬁcant non-zero weights, red indicating excitatory
and blue inhibitory ﬁlters. (b) Upper: Posterior mean +/- 3 std. dev. Filter indices are ordered in the
same way as in (a). Lower: Predicted rate on a test set for STC and for the GLM neuron model with
spike history dependence on a test set.

As the model is linear in the weights wi , we can use the GLM neuron model to ﬁt these weights
and obtain conﬁdence intervals. If a ﬁlter fi (t) is not needed for explaining the cells response, its
corresponding weight wi will automatically be set to zero by the model due to the sparsity prior.
This provides an alternative, model-based method of determining the number of ﬁlters required to
model the cell. The signiﬁcance of each ﬁlter is not determined by a separate hypothesis test on
the spectrum of the spike-triggered covariance, but rather by assessing its inﬂuence on the neural
activity within the full model.
As in the previous application, we can model the spike history effects with an additional feature
vector ψsp to take into account temporal dynamics of single neurons or couplings.
Before applying our method to real data, we tested it on data generated from an artiﬁcial complex cell
similar to the one in (Rust et al., 2005). On this simulated data we were able to recover the original
ﬁlters. We then ﬁtted this GLM neuron model to data recorded from a complex cell in primary visual
cortex of an anesthetized macaque monkey (same data as in (Rust et al., 2005)). We ﬁrst extracted
40 ﬁlters which eigenvalues were most different to their corresponding eigenvalues of the complete
stimulus ensemble. Any nonlinear regression procedure could be used to ﬁt a nonlinearity to each
ﬁlter output. We used a simple quadratic regression technique. Having ﬁxed the ﬁrst nonlinearity
we approximated the posterior as above. The resulting conﬁdence intervals for the linear weights
are plotted in Fig. 3(b). The ﬁlters with signiﬁcant non-zero weights are highlighted in Fig. 3(a).
Red indicates exitatory and blue inhibitory effects on the ﬁring rate. Using 3 std. dev. conﬁdence
intervals 9 excitatory and 8 inhibitory ﬁlters turned out to be signiﬁcant in our model. The number
of ﬁlters is similar to that reportet in Rust et al., who regarded 7 excitatory and 7 inhibitory ﬁlters as
signiﬁcant (Rust et al., 2005). The rank order of the linear weights is closely related but not identical
to the order of eigenvalues, as can be seen in Fig. 3(b), top.

5 Summary and Conclusions

We have shown how approximate Bayesian inference within the framework of generalized linear
models can be used to address the problem of identifying relevant features of neural data. More
precisely, the use of a sparsity prior favors sparse posterior solutions: non-zero weights are assigned
only to those features which which are critical for explaining the data. Furthermore, the explicit

7

uncertainty information obtained from the posterior distribution enables us to identify ranges of sta-
tistical signiﬁcance and therefore facilitates the interpretation of the solution. We used this technique
to determine couplings between neurons in a multi-cell recording and demonstrated an increase in
prediction performance due to regularization by the sparsity prior. Also, in the context of spike-
triggered covariance analysis, we used our method to determine the relevant stimulus subspace
within the space spanned by the eigenvectors. Our subspace selection method is directly linked
to an explicit neuron model which also takes into account the spike-history dependence of the spike
generation.

Acknowledgements

We would like to thank G ¨unther Zeck and Nicole Rust for generously providing their data and for
useful discussions.

References
Chornoboy, E., Schramm, L., & Karr, A. (1988). Maximum likelihood identiﬁcation of neural point
process systems. Biological Cybernetics, 59, 265-275.
Harris, K., Csicsvari, J., Hirase, H., Dragoi, G., & Buzsaki, G. (2003). Organization of cell assem-
blies in the hippocampus. Nature, 424(6948), 552–6.
Minka, T. (2001). Expectation propagation for approximate Bayesian inference. Uncertainty in
Artiﬁcial Intelligence, 17, 362–369.
Okatan, M., Wilson, M. A., & Brown, E. N. (2005). Analyzing functional connectivity using a
network likelihood model of ensemble neural spiking activity. Neural Computation, 17, 1927-
1961.
Opper, M., & Winther, O. (2000). Gaussian Processes for Classiﬁcation: Mean-Field Algorithms.
Neural Computation, 12(11), 2655-2684.
Paninski, L. (2004). Maximum likelihood estimation of cascade point-process neural encoding
models. Network, 15(4), 243–262.
Pillow, J. W., Paninski, L., Uzzell, V. J., Simoncelli, E. P., & Chichilnisky, E. J. (2005). Prediction
and decoding of retinal ganglion cell responses with a probabilistic spiking model. J Neurosci,
25(47), 11003–11013.
Rasmussen, C., & Williams, C. (2006). Gaussian processes for machine learning. Springer.
Rust, N., Schwartz, O., Movshon, J., & Simoncelli, E. (2005). Spatiotemporal Elements of Macaque
V1 Receptive Fields. Neuron, 46(6), 945–956.
Seeger, M. (2005). Expectation propagation for exponential families (Tech. Rep.). University of
California at Berkeley. (See www.kyb.tuebingen.mpg.de/bs/people/seeger.)
Seeger, M., Steinke, F., & Tsuda, K. (2007). Bayesian inference and optimal design in the sparse
linear model. AI and Statistics.
Simoncelli, E., Paninski, L., Pillow, J., & Schwartz, O. (2004). Characterization of neural responses
with stochastic stimuli. In M. Gazzaniga (Ed.), (Vol. 3, pp. 327–338). MIT Press.
Snyder, D., & Miller, M. (1991). Random point processes in time and space. Springer Texts in
Electrical Engineering.
Steveninck, R., & Bialek, W. (1988). Real-Time Performance of a Movement-Sensitive Neuron in
the Blowﬂy Visual System: Coding and Information Transfer in Short Spike Sequences. Proceed-
ings of the Royal Society of London. Series B, Biological Sciences, 234(1277), 379–414.
Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal
Statistical Society. Series B (Methodological), 58(1), 267–288.
Touryan, J., Lau, B., & Dan, Y. (2002). Isolation of Relevant Visual Features from Random Stimuli
for Cortical Complex Cells. Journal of Neuroscience, 22(24), 10811.
Zeck, G. M., Xiao, Q., & Masland, R. H. (2005). The spatial ﬁltering properties of local edge
detectors and brisk-sustained retinal ganglion cells. Eur J Neurosci, 22(8), 2016-26.

8


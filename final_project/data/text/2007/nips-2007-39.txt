How SVMs can estimate quantiles and the median

Ingo Steinwart
Information Sciences Group CCS-3
Los Alamos National Laboratory
Los Alamos, NM 87545, USA
ingo@lanl.gov

Andreas Christmann
Department of Mathematics
Vrije Universiteit Brussel
B-1050 Brussels, Belgium
andreas.christmann@vub.ac.be

Abstract

We investigate quantile regression based on the pinball loss and the ǫ-insensitive
loss. For the pinball loss a condition on the data-generating distribution P is
given that ensures that the conditional quantiles are approximated with respect to
k · k1 . This result is then used to derive an oracle inequality for an SVM based
on the pinball loss. Moreover, we show that SVMs based on the ǫ-insensitive loss
estimate the conditional median only under certain conditions on P .

1

Introduction

Let P be a distribution on X × Y , where X is an arbitrary set and Y ⊂ R is closed. The goal of
quantile regression is to estimate the conditional quantile, i.e., the set valued function
F ∗
τ ,P (x) := (cid:8)t ∈ R : P(cid:0)(−∞, t] | x(cid:1) ≥ τ and P(cid:0)[t, ∞) | x(cid:1) ≥ 1 − τ (cid:9) ,
x ∈ X,
where τ ∈ (0, 1) is a ﬁxed constant and P( · | x), x ∈ X , is the (regular) conditional probability. For
conceptual simplicity (though mathematically this is not necessary) we assume throughout this paper
that F ∗
τ ,P (x) consists of singletons, i.e., there exists a function f ∗
τ ,P : X → R, called the conditional
τ -quantile function, such that F ∗
τ ,P (x)}, x ∈ X . Let us now consider the so-called
τ ,P (x) = {f ∗
τ -pinball loss Lτ : R × R → [0, ∞) deﬁned by Lτ (y , t) := ψτ (y − t), where ψτ (r) = (τ − 1)r , if
r < 0, and ψτ (r) = τ r , if r ≥ 0. Moreover, given a (measurable) function f : X → R we deﬁne the
Lτ -risk of f by RLτ ,P (f ) := E(x,y)∼PLτ (y , f (x)). Now recall that f ∗
τ ,P is up to zero sets the only
function that minimizes the Lτ -risk, i.e. RLτ ,P (f ∗
Lτ ,P , where the inﬁ-
τ ,P ) = inf RLτ ,P (f ) =: R∗
mum is taken over all f : X → R. Based on this observation several estimators minimizing a (mod-
i ﬁed) empirical Lτ -risk were proposed (see [5] for a survey on both parametric and non-parametric
methods) for situations where P is unknown, but i.i.d. samples D := ((x1 , y1 ), . . . , (xn , yn )) drawn
from P are given. In particular, [6, 4, 10] proposed an SVM that ﬁnds a solution fD,λ ∈ H of
n
1
Xi=1
λkf k2
arg min
H +
Lτ (yi , f (xi )) ,
n
f ∈H
where λ > 0 is a regularization parameter and H is a reproducing kernel Hilbert space (RKHS) over
X . Note that this optimization problem can be solved by considering the dual problem [4, 10], but
since this technique is nowadays standard in machine learning we omit the details. Moreover, [10]
contains an exhaustive empirical study as well some theoretical considerations.

(1)

Empirical methods estimating quantiles with the help of the pinball loss typically obtain functions
fD for which RLτ ,P (fD ) is close to R∗
Lτ ,P with high probability. However, in general this only
implies that fD is close to f ∗
τ ,P in a very weak sense (see [7, Remark 3.18]), and hence there is so
far only little justi ﬁcation for using fD as an estimate of the quantile function. Our goal is to address
this issue by showing that under certain realistic assumptions on P we have an inequality of the form
τ ,P kL1 (PX ) ≤ cP qRLτ ,P (f ) − R∗
kf − f ∗
Lτ ,P .

(2)

We then use this inequality to establish an oracle inequality for SVMs deﬁned by (1). In addition,
we illustrate how this oracle inequality can be used to obtain learning rates and to justify a data-
dependent method for ﬁnding the hyper-parameter λ and H . Finally, we generalize the methods for
establishing (2) to investigate the role of ǫ in the ǫ-insensitive loss used in standard SVM regression.

2 Main results

In the following X is an arbitrary, non-empty set equipped with a σ -algebra, and Y ⊂ R is a closed
non-empty set. Given a distribution P on X × Y we further assume throughout this paper that the
σ -algebra on X is complete with respect to the marginal distribution PX of P, i.e., every subset of a
PX -zero set is contained in the σ -algebra. Since the latter can always be ensured by increasing the
original σ -algebra in a suitable manner we note that this is not a restriction at all.

(3)

Deﬁnition 2.1 A distribution Q on R is said to have a τ -quantile of type α > 0 if there exists a
τ -quantile t∗ ∈ R and a constant cQ > 0 such that for all s ∈ [0, α] we have
Q(cid:0)(t∗ , t∗ + s)(cid:1) ≥ cQ s
Q(cid:0)(t∗ − s, t∗ )(cid:1) ≥ cQ s .
and
It is not difﬁcult to see that a distribution Q having a τ -quantile of some type α has a unique τ -
quantile t∗ . Moreover, if Q has a Lebesgue density hQ then Q has a τ -quantile of type α if hQ is
bounded away from zero on [t∗ − α, t∗ + α] since we can use cQ := inf {hQ (t) : t ∈ [t∗ − α, t∗ + α]}
in (3). This assumption is general enough to cover many distributions used in parametric statistics
such as Gaussian, Student’s t, and logistic distributions (with Y = R), Gamma and log-normal
distributions (with Y = [0, ∞)), and uniform and Beta distributions (with Y = [0, 1]).
The following deﬁnition describes distributions on X × Y whose conditional distributions P( · |x),
x ∈ X , have the same τ -quantile type α.
Deﬁnition 2.2 Let p ∈ (0, ∞], τ ∈ (0, 1), and α > 0. A distribution P on X × Y is said to have a
τ -quantile of p-average type α, if Qx := P( · |x) has PX -almost surely a τ -quantile type α and b :
X → (0, ∞) deﬁned by b(x) := cP( · |x) , where cP( · |x) is the constant in (3), satis ﬁes b−1 ∈ Lp (PX ).
Let us now give some examples for distributions having τ -quantiles of p-average type α.
Example 2.3 Let P be a distribution on X × R with marginal distribution PX and regular condi-
tional probability Qx (cid:0)(−∞, y ](cid:1) := 1/(1+ e−z ), y ∈ R, where z := (cid:0)y −m(x)(cid:1)/σ(x), m : X → R
describes a location shift, and σ : X → [β , 1/β ] describes a scale modi ﬁcation for some constant
β ∈ (0, 1]. Let us further assume that the functions m and σ are measurable. Thus Qx is a logistic
distribution having the positive and bounded Lebesgue density hQx (y) = e−z /(1 + e−z )2 , y ∈ R.
τ ,Qx = m(x) + σ(x) log( τ
The τ -quantile function is t∗ (x) := f ∗
1−τ ), x ∈ X , and we can choose
b(x) = inf {hQx (t) : t ∈ [t∗ (x) − α, t∗ (x) + α]}. Note that hQx (m(x) + y) = hQx (m(x) − y) for
all y ∈ R, and hQx (y) is strictly decreasing for y ∈ [m(x), ∞). Some calculations show
1
u2 (x)
u1 (x)
4 (cid:17),
(1+u2 (x))2 o ∈ (cid:16)cα,β ,
b(x) = min(cid:8)hQx (t∗ (x)−α), hQx (t∗ (x)+α)(cid:9) = minn
(1+u1 (x))2 ,
where u1 (x) := 1−τ
τ e−α/σ(x) , u2 (x) := 1−τ
τ eα/σ(x) and cα,β > 0 can be chosen independent of x,
because σ(x) ∈ [β , 1/β ]. Hence b−1 ∈ L∞ (PX ) and P has a τ -quantile of ∞-average type α.
Example 2.4 Let ˜P be a distribution on X × Y with marginal distribution ˜PX and regular con-
ditional probability ˜Qx := ˜P(· | x) on Y . Furthermore, assume that ˜Qx is ˜PX -almost surely
of τ -quantile type α. Let us now consider the family of distributions P with marginal distribu-
tion ˜PX and regular conditional distributions Qx := ˜P(cid:0)(· − m(x))/σ(x)(cid:12)(cid:12) x(cid:1), x ∈ X , where
m : X → R and σ : X → (β , 1/β ) are as in the previous example. Then Qx has a τ -quantile
τ ,Qx = m(x) + σ(x)f ∗
f ∗
of type αβ , because we obtain for s ∈ [0, αβ ] the inequality
τ , ˜Qx
τ ,Qx + s)(cid:1) = ˜Qx (cid:0)(f ∗
, f ∗
τ ,Qx , f ∗
Qx (cid:0)(f ∗
+ s/σ(x))(cid:1) ≥ b(x)s/σ(x) ≥ b(x)β s .
τ , ˜Qx
τ , ˜Qx
Consequently, P has a τ -quantile of p-average type αβ if and only if ˜P does have a τ -quantile of
p-average type α.

The following theorem shows that for distributions having a quantile of p-average type the condi-
tional quantile can be estimated by functions that approximately minimize the pinball risk.

Theorem 2.5 Let p ∈ (0, ∞], τ ∈ (0, 1), α > 0 be real numbers, and q := p
p+1 . Moreover, let
P be a distribution on X × Y that has a τ -quantile of p-average type α. Then for all f : X → R
Lτ ,P ≤ 2− p+2
2p
satisfying RLτ ,P (f ) − R∗
p+1 we have
p+1 α
√2 kb−1 k1/2
Lp (PX ) qRLτ ,P (f ) − R∗
kf − f ∗
τ ,PkLq (PX ) ≤
Lτ ,P .
Our next goal is to establish an oracle inequality for SVMs deﬁned by (1). To this end let us assume
Y = [−1, 1]. Then we have Lτ (y , ¯t) ≤ Lτ (y , t) for all y ∈ Y , t ∈ R, where ¯t denotes t clipped
to the interval [−1, 1], i.e., ¯t := max{−1, min{1, t}}. Since this yields RLτ ,P ( ¯f ) ≤ RLτ ,P (f ) for
all functions f : X → R we will focus on clipped functions ¯f in the following. To describe the
approximation error of SVMs we need the approximation error function A(λ) := inf f ∈H λkf k2
H +
Lτ ,P , λ > 0. Recall that [8] showed limλ→0 A(λ) = 0 if the RKHS H is dense in
RLτ ,P (f ) − R∗
L1 (PX ). We also need the covering numbers which for ε > 0 are deﬁned by
N (cid:0)BH , ε, L2 (µ)(cid:1) := min(cid:8)n ≥ 1 : ∃ x1 , . . . , xn ∈ L2 (µ) with BH ⊂ ∪n
i=1 (xi + εBL2 (µ) )(cid:9), (4)
where µ is a distribution on X , and BH and BL2 (µ) denote the closed unit balls ofH and the Hilbert
space L2 (µ), respectively. Given a ﬁnite sequence D = ((x1 , y1 ), . . . , (xn , yn )) ∈ (X × Y )n
we write DX := (x1 , . . . , xn ), and N (BH , ε, L2 (DX )) := N (BH , ε, L2 (µ)) if µ is the empirical
measure deﬁned by DX . Finally, we write Lτ ◦ f for the function (x, y) 7→ Lτ (y , f (x)). With these
preparations we can now recall the following oracle inequality shown in more generality in [9].

+

1
2−ϑ

.

1
2−ϑ+̺(ϑ−1)

K̺,v a
λ̺n

Theorem 2.6 Let P be a distribution on X × [−1, 1] for which there exist constants v ≥ 1, ϑ ∈ [0, 1]
with
τ ,P )(cid:1)ϑ
τ ,P (cid:1)2
≤ v (cid:0)EP (Lτ ◦ ¯f − Lτ ◦ f ∗
EP (cid:0)Lτ ◦ ¯f − Lτ ◦ f ∗
(5)
for all f : X → R. Moreover, let H be a RKHS over X for which there exist ̺ ∈ (0, 1) and a ≥ 1
with
log N (cid:0)BH , ε, L2 (DX )(cid:1) ≤ aε−2̺ ,
(6)
sup
ε > 0 .
D∈(X×Y )n
Then there exists a constant K̺,v depending only on ̺ and v such that for all ς ≥ 1, n ≥ 1, and
λ > 0 we have with probability not less than 1 − 3e−ς that
Lτ ,P ≤ 8A(λ) + 30r A(λ)
λ̺n (cid:19)
+ (cid:18) K̺,v a
ς
+ 5(cid:16) 32v ς
n (cid:17)
RLτ ,P ( ¯fD,λ ) − R∗
λ
n
Moreover, [9] showed that oracle inequalities of the above type can be used to establish learning
rates and to investigate data-dependent parameter selection strategies. For example if we assume that
there exist constants c > 0 and β ∈ (0, 1] such that A(λ) ≤ cλβ for all λ > 0 then RLτ ,P ( ¯fT ,λn )
β
β (2−ϑ+̺(ϑ−1))+̺ , 2β
converges to R∗
Lτ ,P with rate n−γ where γ := min {
β+1 } and λn = n−γ /β .
Moreover, [9] shows that this rate can also be achieved by selecting λ in a data-dependent way with
the help of a validation set. Let us now consider how these learning rates in terms of risks translate
into rates for k ¯fT ,λ − f ∗
τ ,P kLq (PX ) . To this end we assume that P has a τ -quantile of p-average type
α for τ ∈ (0, 1). Using the Lipschitz continuity of Lτ and Theorem 2.5 we then obtain
Lτ ,P (cid:1)q/2
τ ,P (cid:1)2
τ ,P |2 ≤ k ¯f −f ∗
≤ EP | ¯f −f ∗
EP | ¯f −f ∗
τ ,P |q ≤ c(cid:0)RLτ ,P ( ¯f )−R∗
EP (cid:0)Lτ ◦ ¯f −Lτ ◦f ∗
τ ,Pk2−q
∞
2p
Lτ ,P ≤ 2− p+2
for all f satisfying RLτ ,P ( ¯f )−R∗
p+1 , i.e. we have a variance bound (5) for ϑ := q/2
p+1 α
and clipped functions with small excess risk. Arguing carefully to handle the restriction on ¯f we
then see that k ¯fT ,λ − f ∗
τ ,P kLq (PX ) can converge as fast as n−γ , where
γ := minn
β+1o .
β (4−q+̺(q−2))+2̺, β
β
To illustrate the latter let us assume that H is a Sobolev space W m (X ) of order m ∈ N over X ,
where X is the unit ball in Rd . Recall from [3] that H satis ﬁes (6) for ̺ := d/(2m) if m > d/2 and

in this case H also consists of continuous functions. Furthermore, assume that we are in the ideal
τ ,P ∈ W m (X ) which implies β = 1. Then the learning rate for k ¯fT ,λ − f ∗
situation f ∗
τ ,PkLq (PX ) be-
comes n−1/(4−q(1−̺)) , which for ∞-average type distributions reduces to n−2m/(6m+d) ≈ n−1/3 .
Let us ﬁnally investigate whether the ǫ-insensitive loss deﬁned by L(y , t) := max{0, |y − t| − ǫ}
for y , t ∈ R and ﬁxed ǫ > 0, can be used to estimate the median, i.e. the (1/2)-quantile.
Theorem 2.7 Let L be the ǫ-insensitive loss for some ǫ > 0 and P be a distribution on X × R which
has a unique median f ∗
1/2,P . Furthermore, assume that all conditional distributions P(·|x), x ∈ X ,
are atom-free, i.e. P({y}|x) = 0 for all y ∈ R, and symmetric, i.e. P(h(x) + A|x) = P(h(x) − A|x)
for all measurable A ⊂ R and a suitable function h : X → R. If for the conditional distributions
have a positive mass concentrated around f ∗
1/2,P ± ǫ then f ∗
1/2,P is the only minimizer of RL,P .
Note that using [7] one can show that for distributions speci ﬁed in the above theorem the
1/2,P whenever the SVM is RL,P -consistent,
SVM using the ǫ-insensitive loss approximates f ∗
i.e. RL,P (fT ,λ ) → R∗
L,P in probability, see [2]. More advanced results in the sense of Theorem
2.5 seem also possible, but are out of the scope of this paper.

3 Proofs

(7)

Let us ﬁrst recall some notions from [7] who investigated sur rogate losses in general and the question
how approximate risk minimizers approximate exact risk minimizers in particular. To this end let
L : X × Y × R → [0, ∞) be a measurable function which we call a loss in the following. For a
distribution P and an f : X → R the L-risk is then deﬁned by RL,P (f ) := E(x,y)∼PL(x, y , f (x)),
and, as usual, the Bayes L-risk, is denoted by R∗
L,P := inf RL,P (f ), where the inﬁmum is taken over
all (measurable) f : X → R. In addition, given a distribution Q on Y the inner L-risks were deﬁned
by CL,Q,x (t) := RY L(x, y , t) dQ(y), x ∈ X , t ∈ R, and the minimal inner L-risks were denoted by
L,Q,x := inf CL,Q,x (t), x ∈ X , where the inﬁmum is taken over all
t ∈ R. Moreover, following
C ∗
[7] we usually omit the indexes x or Q if L is independent of x or y , respectively. Obviously, we
have
RL,P (f ) = ZX CL,P( · |x),x (cid:0)f (x)(cid:1) dPX (x) ,
L,P( · |x),x is measurable if the σ -algebra on X is
and [7, Theorem 3.2] further shows that x 7→ C ∗
complete. In this case it was also shown that the intuitive formula R∗
L,P = RX C ∗
L,P( · |x),x dPX (x)
holds, i.e. the Bayes L-risk is obtained by minimizing the inner risks and subsequently integrating
with respect to the marginal distribution PX . Based on this observation the basic idea in [7] is to
consider both steps separately. In particular, it turned out that the sets of ε-approximate minimizers
L,Q,x + ε(cid:9), ε ∈ [0, ∞], and the set of exact minimizers
ML,Q,x (ε) := (cid:8)t ∈ R : CL,Q,x (t) < C ∗
ML,Q,x (0+ ) := Tε>0 ML,Q,x (ε) play a crucial role. As in [7] we again omit the subscripts x and
Q in these deﬁnitions if L happens to be independent of x or y , respectively.
Now assume we have two losses Ltar : X × Y × R → [0, ∞] and Lsur : X × Y × R → [0, ∞], and
that our goal is to estimate the excess Ltar -risk by the excess Lsur -risk. This issue was investigated
in [7], where the main device was the so-called calibration function δmax ( · , Q, x) deﬁned by
δmax (ε, Q, x) := (inf t∈R\MLtar ,Q,x (ε) CLsur ,Q,x (t) − C ∗
if C ∗
Lsur ,Q,x < ∞ ,
Lsur ,Q,x
if C ∗
∞
Lsur ,Q,x = ∞ ,
for all ε ∈ [0, ∞]. In the following we sometimes write δmax,Ltar ,Lsur (ε, Q, x) := δmax (ε, Q, x)
whenever we need to explicitly mention the target and surrogate losses. In addition, we follow our
convention which omits x or Q whenever this is possible. Now recall that [7, Lemma 2.9] showed
δmax (cid:0)CLtar ,Q,x (t) − C ∗
Ltar ,Q,x , Q, x(cid:1) ≤ CLsur ,Q,x (t) − C ∗
(8)
t ∈ R
Lsur ,Q,x ,
if both C ∗
Ltar ,Q,x < ∞ and C ∗
Lsur ,Q,x < ∞. Before we use (8) to establish an inequality between
the excess risks of Ltar and Lsur , we ﬁnally recall that the Fenchel-Legendre bi-conjugate g∗∗ :
I → [0, ∞] of a function g : I → [0, ∞] deﬁned on an interval
I is the largest convex function
h : I → [0, ∞] satisfying h ≤ g . In addition, we write g∗∗ (∞) := limt→∞ g∗∗ (t) if I = [0, ∞).
With these preparations we can now establish the following generalization of [7, Theorem 2.18].

(10)

Theorem 3.1 Let P be a distribution on X × Y with R∗
Ltar ,P < ∞ and R∗
Lsur ,P < ∞ and assume
that there exist p ∈ (0, ∞] and functions b : X → [0, ∞] and δ : [0, ∞) → [0, ∞) such that
(9)
δmax (ε, P( · |x), x) ≥ b(x) δ(ε) ,
ε ≥ 0, x ∈ X,
p+1 , ¯δ := δ q : [0, ∞) → [0, ∞), and all f : X → R we have
and b−1 ∈ Lp (PX ). Then for q := p
Lsur ,P (cid:1)q
¯δ∗∗ (cid:0)RLtar ,P (f ) − R∗
Ltar ,P (cid:1) ≤ kb−1 kq
Lp (PX ) (cid:0)RLsur ,P (f ) − R∗
.
Proof: Let us ﬁrst consider the case RLtar ,P (f ) < ∞. Since ¯δ∗∗ is convex and satis ﬁes ¯δ∗∗ (ε) ≤
¯δ(ε) for all ε ∈ [0, ∞) we see by Jensen’s inequality that
Ltar ,P (cid:1) ≤ ZX
¯δ(cid:0)CLtar ,P( · |x),x (t) − C ∗
¯δ∗∗ (cid:0)RLtar ,P (f ) − R∗
Ltar ,P( · |x),x (cid:1) dPX (x)
Moreover, using (8) and (9) we obtain
b(x) δ(cid:0)CLtar ,P( · |x),x (t) − C ∗
Ltar ,P( · |x),x (cid:1) ≤ CLsur ,P( · |x),x (t) − C ∗
Lsur ,P( · |x),x
for PX -almost all x ∈ X and all t ∈ R. By (10), the deﬁnition of ¯δ , and H ¨older’s inequality in the
form of k · kq ≤ k · kp · k · k1 , we thus ﬁnd that ¯δ∗∗ (cid:0)RLtar ,P (f ) − R∗
Ltar ,P (cid:1) is less than or equal to
dPX (x)(cid:17)q/q
(cid:16)ZX (cid:0)b(x)(cid:1)−q (cid:0)CLsur ,P( · |x),x (cid:0)f (x)(cid:1) − C ∗
Lsur ,P( · |x),x (cid:1)q
b−pdPX (cid:17)q/p(cid:16)ZX (cid:0)CLsur ,P( · |x),x (cid:0)f (x)(cid:1) − C ∗
Lsur ,P( · |x),x (cid:1) dPX (x)(cid:17)q
≤ (cid:16)ZX
Ltar ,P (cid:1)q
≤ kb−1 kq
Lp (PX ) (cid:0)RLsur ,P (f ) − R∗
.
Let us ﬁnally deal with the case RLtar ,P (f ) = ∞. If ¯δ∗∗ (∞) = 0 there is nothing to prove and
hence we assume ¯δ∗∗ (∞) > 0. Following the proof of [7, Theorem 2.13] we then see that there
exist constants c1 , c2 ∈ (0, ∞) satisfying t ≤ c1 δ∗∗ (t) + c2 for all t ∈ [0, ∞]. From this we obtain
Ltar ,P ≤ c1 ZX
¯δ∗∗ (cid:0)CLtar ,P( · |x),x (t) − C ∗
∞ = RLtar ,P (f ) − R∗
Ltar ,P( · |x),x (cid:1) dPX (x) + c2
Lsur ,P( · |x),x(cid:17)q
≤ c1 ZX (cid:0)b(x)(cid:1)−q (cid:16)CLsur ,P( · |x),x (cid:0)f (x)(cid:1) − C ∗
dPX (x) + c2 ,
where the last step is analogous to our considerations for RLtar ,P (f ) < ∞. By b−1 ∈ Lp (PX ) and
H ¨older’s inequality we then conclude RLsur ,P (f ) − R∗
Lsur ,P = ∞.
Our next goal is to determine the inner risks and their minimizers for the pinball loss. To this end
recall (see, e.g., [1, Theorem 23.8]) that given a distribution Q on R and a non-negative function
g : X → [0, ∞) we have
g dQ = Z ∞
ZR
0
Proposition 3.2 Let τ ∈ (0, 1) and Q be a distribution on R with C ∗
Lτ ,Q < ∞ and t∗ be a τ -quantile
of Q. Then there exist q+ , q− ∈ [0, ∞) with q+ + q− = Q({t∗ }), and for all t ≥ 0 we have
CLτ ,Q (t∗ + t) − CLτ ,Q (t∗ ) = tq+ + Z t
Q(cid:0)(t∗ , t∗ + s)(cid:1) ds ,
and
0
CLτ ,Q (t∗ − t) − CLτ ,Q (t∗ ) = tq− + Z t
Q(cid:0)(t∗ − s, t∗ )(cid:1) ds .
0
Proof: Let us consider the distribution Q(t∗ ) deﬁned by Q(t∗ ) (A) := Q(t∗ + A) for all measurable
A ⊂ R. Then it is not hard to see that 0 is a τ -quantile of Q(t∗ ) . Moreover, we obviously have
CLτ ,Q (t∗ + t) = CLτ ,Q(t∗ ) (t) and hence we may assume without loss of generality that t∗ = 0. Then
our assumptions together with Q((−∞, 0]) + Q([0, ∞)) = 1 + Q({0}) yield τ ≤ Q((−∞, 0]) ≤
τ + Q({0}), i.e., there exists a q+ satisfying 0 ≤ q+ ≤ Q({0}) and
(14)
Q((−∞, 0]) = τ + q+ .

Q(g ≥ s) ds .

(11)

(12)

(13)

Q((0, s))ds ,

Let us now compute the inner risks of Lτ . To this end we ﬁrst assume t ≥ 0. Then we have
Zy<t
(y − t) dQ(y) = Zy<0
y dQ(y) − tQ((−∞, t)) + Z0≤y<t
y dQ(y)
and Ry≥t (y − t) dQ(y) = Ry≥0 y dQ(y) − tQ([t, ∞)) − R0≤y<t y dQ(y) and hence we obtain
CLτ ,Q (t) = (τ − 1) Zy<t
(y − t) dQ(y) + τ Zy≥t
(y − t) dQ(y)
= CLτ ,Q (0) − τ t + tQ((−∞, 0)) + tQ([0, t)) − Z0≤y<t
Moreover, using (11) we ﬁnd
Q([s, t)) ds = tQ({0}) + Z t
Q([0, t))ds − Z t
y dQ(y) = Z t
tQ([0, t)) − Z0≤y<t
0
0
0
and since (14) implies Q((−∞, 0)) + Q({0}) = Q((−∞, 0]) = τ + q+ we thus obtain (12).
Now (13) can be derived from (12) by considering the pinball loss with parameter 1 − τ and the
distribution ¯Q deﬁned by ¯Q(A) := Q(−A), A ⊂ R measurable. This further yields a q− satisfying
0 ≤ q− ≤ Q({0}) and Q([0, ∞) = 1 − τ + q− . By (14) we then ﬁnd q+ + q− = Q({0}).
For the proof of Theorem 2.5 we recall a few more concepts from [7]. To this end let us now assume
that our loss is independent of x, i.e. we consider a measurable function L : Y × R → [0, ∞]. We
write
Qmin (L) := (cid:8)Q ∈ Qmin (L) : ∃ t∗
L,Q ∈ R such that ML,Q (0+ ) = {t∗
L,Q }(cid:9) ,
i.e. Qmin (L) contains the distributions on Y whose inner L-risks have exactly one exact minimizer.
L,Q < ∞ for all Q ∈ Qmin (L). Follow-
Furthermore, note that this deﬁnition immediately yields C ∗
ing [7] we now deﬁne the self-calibration loss of L by
˘L(Q, t) := |t − t∗
(15)
L,Q | ,
Q ∈ Qmin (L), t ∈ R .
This loss is a so-called template loss in the sense of [7], i.e., for a given distribution P on X × Y ,
where X has a complete σ -algebra and P( · |x) ∈ Qmin (L) for PX -almost all x ∈ X , the P-instance
˘LP (x, t) := |t − t∗
L,P( · |x) | is measurable and hence a loss. [7] extended the deﬁnition of
inner risks
to the self-calibration loss by setting C ˘L,Q (t) := ˘L(Q, t), and based on this the minimal inner risks
and their (approximate) minimizers were deﬁned in the obvio us way. Moreover, the self-calibration
function was deﬁned by δmax, ˘L,L (ε, Q) = inf t∈R; |t−t∗
L,Q . As shown in [7] this
L,Q |≥ε CL,Q (t) − C ∗
self-calibration function has two important properties: ﬁ rst it satis ﬁes
δmax, ˘L,L (cid:0)|t − t∗
L,Q |, Q(cid:1) ≤ CL,Q (t) − C ∗
(16)
t ∈ R,
L,Q ,
i.e. it measures how well approximate L-risk minimizers t approximate the true minimizer t∗
L,Q , and
second it equals the calibration function of the P-instance ˘LP , i.e.
δmax, ˘LP ,L (ε, P( · |x), x) = δmax, ˘L,L (ε, P( · |x)) ,
ε ∈ [0, ∞], x ∈ X.
In other words, the self-calibration function can be utilized in Theorem 3.1.
Proof of Theorem 2.5: Let Q be a distribution on R with C ∗
L,Q < ∞ and t∗ be the only τ -quantile
of Q. Then the formulas of Proposition 3.2 show
Q(cid:0)(t∗ , t∗ + s)(cid:1) ds, εq− + Z ε
δmax, ˘L,L (ε, Q) = minnεq+ + Z ε
Q(cid:0)(t∗ − s, t∗ )(cid:1) dso ,
ε ≥ 0,
0
0
where q+ and q− are the real numbers deﬁned in Proposition 3.2. Let us additi onally assume that
the τ -quantile t∗ is of type α. For the Huber type function δ(ε) := ε2 /2 if ε ∈ [0, α], and δ(ε) :=
αε − α2/2 if ε > α, a simple calculation then yields δmax, ˘L,L (ε, Q) ≥ cQ δ(ε), where cQ is the
constant satisfying (3). Let us further deﬁne ¯δ : [0, ∞) → [0, ∞) by ¯δ(ε) := δ q (ε1/q ), ε ≥ 0. In
view of Theorem 3.1 we then need to ﬁnd a convex function ˆδ : [0, ∞) → [0, ∞) such that ˆδ ≤ ¯δ .
p ε2 if ε ∈ (cid:2)0, spap (cid:3) and ˆδ(ε) := ap (cid:0)ε − sp+2
To this end we deﬁne ˆδ(ε) := sp
ap (cid:1) if ε > spap ,
p
where ap := αq and sp := 2−q/p . Then ˆδ : [0, ∞) → [0, ∞) is continuously differentiable and its
derivative is increasing, and thus ˆδ is convex. Moreover, we have ˆδ ′ ≤ ¯δ ′ and hence ˆδ ≤ ¯δ which in
turn implies ˆδ ≤ ¯δ∗∗ . Now we ﬁnd the assertion by (16), (17), and Theorem 3.1.

y dQ(y) .

(17)

The proof of Theorem 2.7 follows immediately from the following lemma.

if t > ǫ.

Q([s, b])ds .

(18)

if t ∈ [0, ǫ],

Lemma 3.3 Let Q be a symmetric, atom-free distribution on R with median t∗ = 0. Then for ǫ > 0
L,Q = 2R ∞
and L being the ǫ-insensitive loss we have CL,Q (0) = C ∗
ǫ Q[s, ∞)ds and if CL,Q (0) < ∞
we further have
Q[s, ǫ] ds + Z ǫ+t
CL,Q (t) − CL,Q (0) = Z ǫ
Q[ǫ, s] ds,
ǫ
ǫ−t
t−ǫ
ǫ+t
t−ǫ
Z0
Z2ǫ
Z0
Q[0, s] ds ≥ 0,
Q[s, ∞) ds + 2
Q[s, ∞) ds −
CL,Q (t) − CL,Q (ǫ) =
In particular, if Q[ǫ − δ, ǫ + δ ] = 0 for some δ > 0 then CL,Q (δ) = C ∗
L,Q .
Proof: Because L(y , t) = L(−y , −t) for all y , t ∈ R we only have to consider t ≥ 0. For later use
we note that for 0 ≤ a ≤ b ≤ ∞ Equation (11) yields
y dQ(y) = aQ([a, b]) + Z b
Z b
a
a
Moreover, the deﬁnition of L implies
t − y − ǫ dQ(y) + Z ∞
CL,Q (t) = Z t−ǫ
y − ǫ − t dQ(y) .
t+ǫ
−∞
−∞ y dQ(y) = R ∞
Using the symmetry of Q yields − R t−ǫ
ǫ−t y dQ(y) and hence we obtain
y dQ(y) + 2 Z ∞
Q[t + ǫ, ∞)ds + Z t+ǫ
Q(−∞, t − ǫ]ds − Z t+ǫ
CL,Q (t) = Z t−ǫ
y dQ(y) . (19)
t+ǫ
ǫ−t
0
0
Let us ﬁrst consider the case t ≥ ǫ. Then the symmetry of Q yields R t+ǫ
ǫ−t y dQ(y) = R t+ǫ
t−ǫ y dQ(y),
and hence (18) implies
Q[t− ǫ, t+ ǫ] ds + Z t+ǫ
Q[ǫ − t, ∞)ds + Z t−ǫ
CL,Q (t) = Z t−ǫ
Q[s, t+ ǫ] ds
t−ǫ
0
0
Q[s, ∞) ds + Z t+ǫ
+2Z ∞
Q[t+ ǫ, ∞) ds.
0
t+ǫ
Q[s, t + ǫ) ds − Z t−ǫ
Q[s, t + ǫ) ds = Z t+ǫ
Z t+ǫ
0
0
t−ǫ
we further obtain
t−ǫ
∞
∞
t+ǫ
t+ǫ
Z0
Z0
Zt+ǫ
Z0
Zt−ǫ
Q[s, ∞) ds −
Q[s, ∞) ds =
Q[t + ǫ, ∞) ds +
From this and R t−ǫ
0 Q[t − ǫ, t + ǫ] ds − R t−ǫ
0 Q[s, t + ǫ] ds = −R t−ǫ
0 Q[s, t − ǫ] ds follows
Q[s, ∞) ds+Z ∞
Q[ǫ − t, ∞) ds+Z ∞
Q[s, t − ǫ] ds+Z t−ǫ
CL,Q (t) = −Z t−ǫ
Q[s, ∞) ds .
0
t+ǫ
0
0
The symmetry of Q implies R t−ǫ
0 Q[ǫ − t, t − ǫ] ds = 2R t−ǫ
0 Q[0, t − ǫ] ds, and we get
Q[0, s) ds + Z t−ǫ
Q[ǫ − t, ∞) ds = 2Z t−ǫ
Q[s, t − ǫ] ds + Z t−ǫ
−Z t−ǫ
Q[s, ∞) ds .
0
0
0
0
This and
Q[s, ∞) ds + Z t+ǫ
Q[s, ∞) ds = 2Z ∞
Q[s, ∞) ds + Z ∞
Z ∞
0
t+ǫ
0
t+ǫ

Q[s, ∞) ds

Q[s, t + ǫ) ds +

Q[s, t + ǫ) ds .

Using

Q[s, t + ǫ) ds

Q[s, ∞) ds .

Q[s, ∞) ds

By

yields
Q[s, ∞) ds + Z t+ǫ
Q[s, ∞) ds + 2Z ∞
Q[0, s) ds + Z t−ǫ
CL,Q (t) = 2Z t−ǫ
0
t+ǫ
0
0
Q[s, ∞) ds + Z t+ǫ
Q[s, ∞) ds = 2Z t−ǫ
Q[s, ∞) ds + Z t+ǫ
Z t−ǫ
t−ǫ
0
0
0
we obtain
Q[s, ∞) ds + Z t+ǫ
Q[0, ∞) ds + 2Z ∞
CL,Q (t) = 2Z t−ǫ
Q[s, ∞) ds
t−ǫ
t+ǫ
0
if t ≥ ǫ. Let us now consider the case t ∈ [0, ǫ]. Analogously we obtain from (19) that
Q[s, t + ǫ] ds + 2Z ∞
Q[ǫ − t, t + ǫ] ds + Z ǫ+t
CL,Q (t) = Z ǫ−t
Q[s, ∞) ds
ǫ+t
ǫ−t
0
Q[ǫ − t, ∞) ds − Z ǫ+t
Q[ǫ + t, ∞) ds − Z ǫ−t
+2Z ǫ+t
0
0
0
Combining this with
Q[ǫ − t, ∞) ds = −Z ǫ−t
Q[ǫ − t, t + ǫ] ds − Z ǫ−t
Z ǫ−t
Q[ǫ + t, ∞) ds
0
0
0
0 Q[ǫ + t, ∞) ds = R ǫ+t
0 Q[ǫ + t, ∞) ds − R ǫ−t
and R ǫ+t
ǫ−t Q[ǫ + t, ∞) ds we get
Q[s, t + ǫ] ds + 2Z ∞
Q[ǫ + t, ∞) ds + Z ǫ+t
CL,Q (t) = Z ǫ+t
Q[s, ∞) ds
ǫ+t
ǫ−t
ǫ−t
Q[s, ∞) ds + Z ∞
Q[s, ∞) ds = Z ∞
Q[s, ∞) ds + 2Z ∞
= Z ǫ+t
Q[s, ∞) ds.
ǫ+t
ǫ−t
ǫ+t
ǫ−t
Hence CL,Q (0) = 2R ∞
ǫ Q[s, ∞) ds. The expressions for CL,Q (t)−CL,Q (0), t ∈ (0, ǫ], and CL,Q (t)−
CL,Q (ǫ), t > ǫ, given in Lemma 3.3 follow by using the same arguments. Hence one exact minimizer
of CL,Q (·) is the median t∗ = 0. The last assertion is a direct consequence of the formula for
CL,Q (t) − CL,Q (0) in the case t ∈ (0, ǫ].
References

Q[ǫ + t, ∞) ds .

[1] H. Bauer. Measure and Integration Theory. De Gruyter, Berlin, 2001.
[2] A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression.
Bernoulli, 15:799–819, 2007.
[3] D.E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential Operators.
Cambridge University Press, 1996.
[4] C. Hwang and J. Shim. A simple quantile regression via support vector machine. In Advances
in Natural Computation: First International Conference (ICNC), pages 512 –520. Springer,
2005.
[5] R. Koenker. Quantile Regression. Cambridge University Press, 2005.
[6] B. Sch ¨olkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New support vector algorithms.
Neural Computation, 12:1207–1245, 2000.
[7] I. Steinwart. How to compare different loss functions. Constr. Approx., 26:225–287, 2007.
[8] I. Steinwart, D. Hush, and C. Scovel. Function classes that approximate the Bayes risk. In
Proceedings of the 19th Annual Conference on Learning Theory, COLT 2006, pages 79–93.
Springer, 2006.
[9] I. Steinwart, D. Hush, and C. Scovel. An oracle inequality for clipped regularized risk mini-
mizers. In Advances in Neural Information Processing Systems 19, pages 1321–1328, 2007.
[10] I. Takeuchi, Q.V. Le, T.D. Sears, and A.J. Smola. Nonparametric quantile estimation. J. Mach.
Learn. Res., 7:1231–1264, 2006.


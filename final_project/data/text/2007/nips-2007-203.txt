Inﬁnite State Bayesian Networks

Max Welling∗, Ian Porteous, Evgeniy Bart†
Donald Bren School of Information and Computer Sciences
University of California Irvine
Irvine, CA 92697-3425 USA
{welling,iporteou}@ics.uci.edu, bart@caltech.edu

Abstract

A general modeling framework is proposed that uni ﬁes nonparametric-Bayesian
models, topic-models and Bayesian networks. This class of inﬁnite state Bayes
nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet
processes’ (HDPs) where the domain of the variables can be structured (e.g. words
in documents or features in images). We show that collapsed Gibbs sampling can
be done efﬁciently in these models by leveraging the structure of the Bayes net
and using the forward- ﬁltering-backward-sampling algorithm for junction trees.
Existing models, such as nested-DP, Pachinko allocation, mixed membership sto-
chastic block models as well as a number of new models are described as ISBNs.
Two experiments have been performed to illustrate these ideas.

1 Introduction

Bayesian networks remain the cornerstone of modern AI. They have been applied to a wide range
of problems both in academia as well as in industry. A recent development in this area is a class
of Bayes nets known as topic models (e.g. LDA [1]) which are well suited for structured data such
as text or images. A recent statistical sophistication of topic models is a nonparametric extension
known as HDP [2], which adaptively infers the number of topics based on the available data.

This paper has the goal of bridging the gap between these three developments. We propose a general
modeling paradigm, the “inﬁnite state Bayes net” (ISBN), that incorporates these three aspects.
We consider models where the variables may have the nested structure of documents and images,
may have in ﬁnite discrete state spaces, and where the random variables are related through the
intuitive causal dependencies of a Bayes net. ISBN’s can be viewed as collections of HDP “modules”
connected together to form a network. Inference in these networks is achieved through a two-stage
Gibbs sampler, which combines the “forward- ﬁltering-backward-sampling algorithm” [3] extended
to junction trees and the direct assignment sampler for HDPs [2].

2 Bayes Net Structure for ISBN
Consider observed random variables xA (cid:44) {xa }, a = 1..A. These variables can take values
in an arbitrary domain. In the following we will assume that xa is sampled from a (conditional)
distribution in the exponential family. We will also introduce hidden (unobserved, latent) variables
{zb }, b = 1..B which will always take discrete values. The indices a, b thus index the nodes of the
Bayesian network.
We will introduce a separate index, e.g. na , to label observations. In the simplest setting we assume
IID data n = i, i.e. N independent identically distributed observations for each variable. We will
∗On sabbatical at Radboud University Nijmegen, Netherlands, Dept. of Biophysics.
† Joint appointment at California Institute of Technology, USA, Dept. of Electrical Engineering.

1

(a)

(b)

(c)

Figure 1: Graphical representation of (a) Unstructured inﬁnite state Bayesian network, (b) HDP, (c) H2DP.

however also be interested in more structured data, such as words in documents, where the index
n can be decomposed into e.g. n = (j, ij ). In this notation we think of j as labelling a document
and ij as labelling a word in document j . To simplify notation we will often write n = (j i). It is
straightforward to generalize to deeper nestings of indices, e.g. n = (k , jk , ijk ) = (kj i) where k
can index e.g. books, j chapters and i words. We interpret this as the observed structure in the data,
as opposed to the latent structure which we seek to infer. The unobserved structure is labelled with
za
the discrete “assignment variables”
n which assign the object indexed by n to latent groups (a.k.a.
topics, factors, components).
The assignment variables z together with the observed variables x are organized into a Bayes net,
where dependencies are encoded by the usual “conditional probability tables” (CPT), which we
(cid:80)
denote with φa
zb |℘b for latent variables1 . Here, ℘a denotes the
xa |℘a for observed variables and π b
joint state of all the parent variables of xa or zb . When a vertical bar is present we normalize over
xa |℘a = 1, ∀a, ℘a . Note that CPTs are considered random
xa φa
the variables to the left of it, e.g.
variables and may themselves be indexed by (a subset of) n, e.g. φxa |℘a j .
We assume that each π b is sampled from a Dirichlet prior: e.g. πzb |℘b ∼ D[αbτ zb ] inde-
pendently and identically for all values of ℘b . The distribution τ itself is Dirichlet distributed,
τ za ∼ D [γ a /K a ], where K a is the number of states for variable za . We can put gamma priors on
αa , γ a and consider them as random variables as well, but to keep things simple we will consider
them ﬁxed variables here. We refer to [4] for algorithms to learn them from data and to [5] and [2]
for ways to infer them through sampling. In section 5 we further discuss these hierarchical priors.

In drawing BNs we will not include the plates to avoid cluttering the ﬁgures. However, it is always
possible to infer the number of times variables in the BN are replicated by looking at its indices. For
instance, the variable node labelled with πz1 |z2 j in Fig.3a stands for K (2) × J IID copies of π1
sampled from τ 1 .

3 Networks of HDPs

In Fig.1b we have drawn the ﬁnite version of the HDP. Here φ is a distribution over words, one for
each topic value z, and is often referred to a “topic distribution”. Topic values are generated from
a document speciﬁc distribution π which in turn are generated from a “mother distribution” over
topics τ . As was shown in [2] one can take the inﬁnite limit K → ∞ in this model and arrive at
the HDP. We will return to this inﬁnite limit when we describe Gibbs sampling. In the following we
will use the same graphical model for ﬁnite and inﬁnite versions of ISBNs.

1We will often avoid writing the super-indices a, b when it is clear from the context, e.g. φa
xa |℘a = φxa |℘a .

2

1iz1ix112|xzz(cid:73)1(cid:78)2iz3iz3z(cid:83)3(cid:68)2z(cid:87)24|zz(cid:83)2(cid:68)2(cid:74)1z(cid:87)134|zzz(cid:83)1(cid:68)1(cid:74)4iz2ix4z(cid:83)4(cid:68)22|xz(cid:73)2(cid:78)jizjix|xz(cid:73)|zj(cid:83)z(cid:87)(cid:78)(cid:68)(cid:74)kjizkjix|xz(cid:73)|zkj(cid:83)z(cid:87)(cid:78)(cid:68)|zk(cid:85)(cid:69)(cid:74)(a)

(b)

(c)

Figure 2: Graphical representation for (a) BiHDP, (b) Mixed membership stochastic block model and (c) the
“multimedia” model.

One of the key features of the HDP is that topics are shared across all documents indexed by j . The
reason for this is the distribution τ : new states are “invented” at this level and become available
to all other documents. In other words, there is a single state space for all copies of π . One can
interpret j is an instantiation of a dummy, fully observed random variable ι. We could add this node
to the BN as a parent of z (since π depends on it) and reinterpret the statement of sharing topics
as a fully connected transition matrix between states of ι to states of z. This idea can be extended
to a combination of fully observed parent variables and multiple unobserved parent variables, e.g.
ι → z2 , z3 , ι. Moreover, the child variables do not have to be observed either, so we can also replace
x → z. In this fashion we can connect together multiple vertical stacks τ → φ → z where each
such module is part of a “virtual-HDP” where the joint child states act as virtual data and the joint
parent states act as virtual document labels. Examples are given in Figs. 1a (inﬁnite extension of a
Bayes net with IID data items) and 3a (inﬁnite extension of Pachinko Allocation).

4 Inference

To simplify the presentation we will now restrict attention to a Bayesian network where all CPTs are
shared across all data-items (see Fig.1a). In this case data is unstructured, assumed IID and indexed
by a ﬂat index n = i. Instead of going through the detailed derivation, which is an extension of the
derivation in [2] for HDP, we will describe the sampling process in the following.

There is a considerable body of empirical evidence which conﬁrms that marginalizing out the vari-
ables π , φ will result in improved inference (e.g. [6, 7]). In this collapsed space, we sample two sets
of variables alternatingly, {z} on the hand and {τ } on the other. First, we focus on the latter given
z and notice that all τ are conditionally independent given z, x.
(cid:80)
Sampling τ |(z, x): Given x, z we can compute count matrices2 Nzb |℘b and Nxa |℘a as
i = k ∧ ℘b
i I[z b
i = l] and similarly for Nxa |℘a . Given these counts, for each value
Nzb=k|℘b=l =
(cid:80)
of k , l, we now create the following vector: vkl = ατ k /(ατ k + nk|l − 1) with nk|l = [1, 2, .., Nk|l ].
We then draw a number Nk|l Bernoulli random variables with probability of success given by the
k|l and compute their sum across t: Sk|l =
elements of v, which we call3 st
t st
k|l . This pro-
(cid:80)
cedure is equivalent to running a Chinese restaurant process (CRP) with Nk|l customers and only
keeping track of how many tables become occupied. We will denote it with Sk|l ∼ A[Nk|l , ατ k ]
after Antoniak [8]. Next we compute Sk =
l Sk|l and sample τ from a Dirichlet distribution,
τ ∼ D [γ , S1 , .., Sk ]. Note that τ is a distribution over K a + 1 states, where we now denote with
K a the number of occupied states. If the state corresponding to γ is picked, a new state is created
and we increment K a ← K a + 1. If on the other hand a state becomes empty, we remove it from
2Note that these can be also used to compute Rao-Blackwellised estimates of π a φ, i.e. E[πzb |℘b ] =
(αb τ b
z + Nzb |℘b )/(αb + N℘b ) and similarly for φ.
3These variables are so called auxiliary variables to facilitate sampling τ .

3

1jizjix12|xzz(cid:73)1z(cid:87)(cid:78)2jiz2z(cid:87)1(cid:74)1|zi(cid:83)2|zj(cid:83)1(cid:68)2(cid:74)2(cid:68)jiz(cid:111)jix|,’xzz(cid:73)z(cid:87)(cid:78)(cid:74)|zj(cid:83)(cid:68)1jwz1jwx11|xz(cid:73)1|zj(cid:83)1z(cid:87)1(cid:78)1(cid:68)1(cid:74)2jfz2jfx2z(cid:87)2(cid:78)2(cid:68)2(cid:74)0jz0z(cid:83)0(cid:68)2|zj(cid:83)22|xz(cid:73)(a)

(b)

Figure 3: Graphical representation for (a) Pachinko Allocation and (b) Nested DP.

the list and we decrement K a ← K a − 1. This will allow assignment variables to add or remove
states adaptively4 .
Sampling z|(τ , x): The conditional probability of all {zi , xi } variables jointly (for ﬁxed i) is
(cid:89)
(cid:89)
given by,
a
b

+ N ¬i
αbτ zb
i |℘b
P (xi , zi |z¬i , x¬i , τ , α) =
i |xa¬i , ℘a¬i )
zb
F (xa
i
i
αb + N ¬i
℘b
i
where N ¬i
is the number data-cases assigned to group zb
(cid:82)
i for variable b and its parents assigned
(cid:81)
i |℘b
zb
i
to group ℘b
i , where we exclude data-case i from this count. Also,
(cid:82)
(cid:81)
i(cid:48) |φk ) P (φk )
i |φk )
i(cid:48) =k P (xa
dφk P (xa
i(cid:48) \i:℘a
i |xa¬i , ℘a¬i = k) =
F (xa
i(cid:48) |φk ) P (φk )
i(cid:48) =k P (xa
dφk
i(cid:48) \i:℘a
Importantly, equation 1 follows the structure of the original Bayes net, where each term has the
i |℘a
form of a conditional distribution P (za
i ) and is based on sufﬁcient statistics collected over all
the other data-cases. Hence, we can use the structure of the Bayes net to sample the assignment
variables jointly across the BN (for data-case i). The general technique that allows one to exploit
network structure is ‘forward-ﬁltering-backward-sampling’ (FFBS) [3]. Assume for instance that
the network is a tree. In that case we ﬁrst propagate information from the leaves to the root, comput-
ing the probabilities P (zb |{xb↓ }) as we go where ‘↓’ means that we compute a marginal conditioned
on ‘downstream’ evidence. When we reach the root we draw a sample from P (zroot |{xb }). Finally,
we work our way back to the leaves, conditioning on drawn samples (which summarize upstream
information) and using the marginal probabilities P (zb |{xb↓ }) cashed during the ﬁltering phase to
represent downstream evidence. For networks with higher treewidth we can extend this technique
to junction trees. Alternatively, one can use cut-set sampling techniques [9].

(1)

(2)

5 ISBN for Structured Data

In section 2 we introduced an index n to label the known structure of the data. The simplest nontrivial
example is given by the HDP, where n = (j i) indexing e.g. documents and words. In this case the
CPT πz |j is not shared across all data, but is speciﬁc to a document. Next consider Fig.1c where
n = (kj i) is labelling for instance words (i) in chapters (j ) in books (k). The ﬁrst level CPT πz |kj is
speciﬁc to chapters (and hence books) and is sampled from a Dirichlet distribution with mean given

4We adopted the name ‘inﬁnite state Bayesian network’ because the (K a + 1)th state actually represents an
inﬁnite pool of indistinguishable states.

4

1jizjix1|xz(cid:73)1z(cid:87)(cid:78)2jiz2z(cid:87)3jiz3z(cid:87)12|zzj(cid:83)23|zzj(cid:83)3|zj(cid:83)1(cid:68)3(cid:74)2(cid:68)3(cid:68)2(cid:74)1(cid:74)1nznx1|xz(cid:73)(cid:78)2nz3nz12|zz(cid:83)23|zz(cid:83)3z(cid:83)1(cid:68)2(cid:68)3(cid:68)tu|k

by a second level CPT ρz |k speciﬁc to books, which in turn is sampled from a Dirichlet distribution
with mean τ z , which ﬁnally is sampled from a Dirichlet prior with parameters γ . Sampling occurs
again in two phases: sampling ρ, τ |x, z and z|ρ, τ , x while marginalizing out π , φ.
To sample from ρ, τ we compute counts Nu|m,jk which is the number of times words were assigned
in chapter j and book k to the joint state z = u, ℘ = m. We then work our way up the stack,
sampling new count arrays S, R as we go, and then down again sampling the CPTs (τ , ρ) using
these count arrays5 . Note that this is just one step of Gibbs sampling from P (τ , ρ|z, x) and does
(cid:88)
(cid:88)
not (unlike the other phase for z) generate an equilibrium sample from this conditional distribution.
↑: su|jkm ∼ A[Nu|jkm , αρu|k ] → Su|k =
su|jkm → ru|k ∼ A[Su|k , β τ u ] → Ru =
j,m
k
↓: τ u ∼ D [(γ , Ru )] → ρu|k ∼ D [β τ u + Su|k ]
(3)
A similar procedure is deﬁned for the priors of φ and extensions to deeper stacks are straightforward.
If all z variables carry the same index n, sampling zn given the hierarchical priors is very similar
to the FFBS procedure described in the previous section, except that the count arrays may carry
¬ijk
a subset of the indices from n, e.g. N
z |℘jk . Since these counts are speciﬁc to a chapter they are
typically smaller resulting in a higher variance for the samples z. If two neighboring z variables
carry different subsets of n labels, e.g. node z0
j in Fig.2c, the conditional distributions are harder to
compute. The general rule is to identify and remove all z(cid:48) variables that are impacted by changing
j f , ∀f } in Fig.2c if we resample z0
the value for z under consideration, e.g. {z1
jw , ∀w ∪ z2
j . To
compute the conditional probability we set z = k and add the impacted variables z(cid:48) back into the
system, one-by-one in an arbitrary order and assigning them to their old values.
It is also instructive to place DP priors (instead of HDP priors) of the form D [αb /K b ] directly on π
(skipping τ ). In taking the inﬁnite limit the conditional distribution for existing states zb becomes
directly proportional to Nzb |℘b (the αbτ zb term is missing). This has the effect that a new state
z b = k that was discovered for some parent state ℘b = l will not be available to other parent states,
simply because Nk|l(cid:48) = 0, l(cid:48) (cid:54)= l. The result is that the state space forks into a tree structure as we
move down the Bayes net. When the network structure is a linear chain, this model is equivalent
to the ‘nested-DP’ introduced in [10] as a prior on tree-structures. The corresponding Bayes net
is depicted in Fig.3b. A chain of length 1 is of course just a Dirichlet process mixture model. A
DP prior is certainly appropriate for nodes zb with CPTs that do not depend on other parents or
additional labels, e.g. nodes z3 and z4 in Fig.1a. Interestingly, an HDP would also be possible
and would result in a different model. We will however follow the convention that we will use the
minimum depth necessary for modelling the structure of the data.

6 Examples
Example: HDP Perhaps the simplest example is an HDP itself, see Fig.1b. It consists of a single
topic node and a single observation node. If we make φ depend on the item index i, i.e. φx|z ,i , we
obtain the inﬁnite version of the ’user rating proﬁle’ (URP) model [11]. If we make φ depend on j
instead and add a prior: ψx|z → φx|z ,j , we obtain an “HDP with random effects” [12] which has
the beneﬁt that shared topics across documents can vary slightly relative to each other.
Example: Inﬁnite State Chains
The ‘Pachinko allocation model’ (PAM) [13] consists of a linear
chain of assignment variables with document speciﬁc transition probabilities, see Fig.3a. It was
proposed to model correlations between topics. The inﬁnite version of this is clearly an example
of an ISBN. An equivalent Chinese restaurant process formulation was published in [14]. A slight
variation on this architecture was described in [15] (POM). Here, images are modeled as mixtures
over parts and parts were modeled as mixtures over visual words. Finally, a visual word is a dis-
tribution over features. POM is only subtly different from PAM (see Fig.3a) in that parts are not
image-speciﬁc distributions over words, and so the distribution πz1 |z2 does not depend on j .
Example: BiHDP This model, depicted in Fig.2a has a data variable xj i and two parent topic
variables z1
j i and z2
j i . One can think of j as the customer index and i as the product index (and no IID
repeated index). The value of x is the rating of that customer for that product. The hidden variables

5Teh’s code npbayes-r21, (available from his web-site) does in fact implement this sampling process.

5

z1
j i and z2
j i represent product groups and customer groups. Every data entry is assigned to both a
customer group and a product group which together determine the factor from which we sample
the rating. Note that the difference between the assignment variables is that their corresponding
CPTs πz1 ,j and πz2 ,i depend on j and i respectively. Extensions are easily conceived. For instance,
instead of two modalities, we can model multiple modalities (e.g. customers, products, year). Also,
single topics can be generalized to hierarchies of topics, so every branch becomes a PAM. Note
that for unobserved xj i values (not all products have been rated by all customers) the corresponding
za
j i , zb
j i are “dangling” and can be integrated out. The result is that we should skip that variable in
the Gibbs sampler.
Example: The Mixed-Membership Stochastic Block Model[16] This model is depicted in
Fig.2b. The main difference with HDP is that (like BiHDP) π depends on two parent states zi→j
and zj→i by which we mean that item i has chosen topic zi→j to interact with item j and vice
versa. However, (unlike BiHDP) those topic states share a common distribution π . Indices only
run over distinct pairs i > j . These features make the model suitable for modeling social interac-
tion networks or protein-protein interaction networks. The hidden variables jointly label the type of
interaction that was used to generate ‘matrix-element’ xij .
Example: The Multimedia Model
In the above examples we had a single observed variable in
the graphical model (repeated over ij ). The model depicted in Fig.2c has two observed variables
and an assignment variable that is not repeated over items. We can think of the middle node z0
j as
the class label for a web-page j . The left branch can then model words on the web-page while the
right branch can model visual features on the web-page. Since no sharing is required for z0
j we used
a Dirichlet prior. The other variables have the usual HDP priors.

7 Experiments

To illustrate the ideas we implemented two models: BiHDP of Fig.2a and the “probabilistic object
model” (POM), explained in the previous section.
Market Basket Data
In this experiment we investigate the performance of BiHDP on a synthetic
market basket dataset. We used the IBM Almaden association and sequential patterns generator to
create this dataset [17]. This is a standard synthetic transaction dataset generator often used by the
association research community. The generated data consists of purchases from simulated groups
of customers who have similar buying habits. Similarity of buying habits refers to the fact that
customers within a group buy similar groups of items. For example, items like strawberries and
cream are likely to be in the same item group and thus are likely to be purchased together in the
same market basket. The following parameters were used to generate data for our experiments: 1M
transactions, 10K customers, 1K different items, 4 items per transaction on average, 4 item groups
per customer group on average, 50 market basket patterns, 50 customer patterns. Default values
were used for the remaining parameters.

The two assignment variables correspond to customers and items respectively. For a given pair of
customer and item groups, a binomial distribution was used to model the probability of a customer
group making a purchase from that item group. A collapsed Gibbs sampler was used to ﬁt the model.
After 1000 epochs the system converged to 278 customer groups and 39 item factors. Fig.4 shows
the results. As can be seen, most item groups correspond directly to the hidden ground truth data.
The conclusion is that the model can learn successfully the hidden structure in the data.
Learning Visual Vocabularies LDA has also gained popularity to model images as collections of
features. The visual vocabulary is usually determined in a preprocessing step where k-means is run
to cluster features collected from the training set. In [15] a different approach was proposed in which
the visual word vocabulary was learned jointly with ﬁtting the parameters of the model. This can
have the beneﬁt that the vocabulary is better adapted to suit the needs of the model. Our extension of
their PLSA-based model is the inﬁnite state model given by Fig.3a with 2 hidden variables (instead
of 3) and πz1 |z2 independent of j . x is modeled as a Gaussian-distributed random variable over
feature values, z1 represents the word identity and z2 is the topic index.
We used the Harris interest-point detector and 21×21 patches centered on each interest point as input
to the algorithm. We normalized the patches to have zero mean. Next we reduced the dimensionality
of detections from 441 to 100 using PCA. The procedure described above generates a set of between

6

Learned: 223, 619, 271, 448, 39, 390
Learned: 364, 250, 718, 952, 326, 802
Learned: 159, 563, 780, 995, 103, 216, 598, 72
Learned: 227, 130, 862, 991, 904, 213
Learned: 953, 175, 956, 385, 269, 14, 64
Learned: 49, 657, 906, 604, 229
Learned: 295, 129, 662, 922, 705, 210
Learned: 886, 460, 471, 933, 544
Learned: 489, 818, 927, 378, 64, 710
Learned: 776, 224, 139, 379

True: 223, 271, 448, 39, 427, 677
True: 364, 718, 952, 326, 542, 98
True: 159, 563, 780, 995, 103, 216, 542, 72
True: 227, 130, 862, 991, 904, 213
True: 953, 175, 956, 385, 269, 14, 956
True: 49, 657, 906, 604, 229
True: 295, 129, 662, 922, 705, 68
True: 886, 460, 471, 933, 917
True: 489, 818, 927, 378, 64, 247
True: 776, 224, 139, 379

Figure 4: The 10 most popular item groups learned by the BiHDP model (left) compared to ground truth
item groups for market basket data (right). Learned items are ordered by decreasing popularity. Ground truth
items have no associated weight; therefore, they were ordered to facilitate comparison with the left row. Non-
matching items are shown in boldface.

Figure 5: Precision Recall curves for Caltech-4 dataset (left) and turntable dataset (right). Solid curve repre-
sents POM and dashed curve represents LDA.

50 and 400, 100-dimensional detections per image. Experiments were performed using the Caltech-
4 and ‘turntable’ datasets. For Caltech-4 we used 130 randomly sampled images from each of the 4
categories for training. LDA was ﬁt using 500 visual words and 50 parts (which we found to give
the best results). The turntable database contains images of 15 toy objects. The objects were placed
on a turntable and photographed every 5 degrees. We have used angles 15, 20, 25, 35, 40, and 45
for training, and angles 10, 30, and 50 for testing. LDA used 15 topics and 200 visual words (which
again was optimal). LDA was then ﬁtted to both datasets using Gibbs sampling. We initialized POM
with the output of LDA to make sure the comparison involved similar modes of the distribution.

The precision-recall curves for this dataset are shown in Fig.5. Images were labelled by choosing
the majority class across the 11 most similar retrieved images. Similarity was measured as the
probability of the query image given the part probabilities of the retrieved image.

These experiments show that ISBNs can be successfully implemented. We are not interested in
claiming superiority of ISBNs, but rather hope to convey that ISBNs are a convenient tool to design
models and to facilitate the search for the number of latent states.

8 Discussion

We have presented a uniﬁed framework to organize the fast growing class of ‘topic models’. By
merging ideas from Bayes nets, nonparametric Bayesian statistics and topic models we have arrived
at a convenient framework to 1) extend existing models to inﬁnite state spaces, 2) reason about
and design new models and 3) derive efﬁcient inference algorithms that exploit the structure of the
underlying Bayes net.

Not every topic model naturally ﬁts the suit of an ISBN. For instance, the inﬁnite HMM [18] is like a
POM model with emission states, but with a single transition probability shared across time. When
marginalizing out π this has the effect of coupling all z variables. An efﬁcient sampler for this
model was introduced in [19]. Also, in [10, 20] models were studied where a word can be emitted at

7

00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81any node corresponding to a topic variable z. We would need an extra switching variable to ﬁt this
into the ISBN framework.

We are currently working towards a graphical interface where one can design ISBN models by
attaching together HkDP modules and where the system will automatically perform the inference
necessary for the task at hand.

Acknowledgements

This material is based upon work supported by the National Science Foundation under Grant No.
0447903 and No. 0535278 and by ONR under Grant No. 00014-06-1-0734.

References

[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.
[2] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. To appear in
Journal of the American Statistical Association, 2006.
[3] S. L. Scott. Bayesian methods for hidden Markov models, recursive computing in the 21st century.
volume 97, pages 337–351, 2002.
[4] T. Minka. Estimating a dirichlet distribution. Technical report, 2000.
[5] M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the
American Statistical Association, 90:577–588, 1995.
[6] T.L. Grifﬁths and M. Steyvers. A probabilistic approach to semantic representation. In Proceedings of
the 24th Annual Conference of the Cognitive Science Society, 2002.
[7] Y.W. Teh, D. Newman, and M. Welling. A collapsed variational bayesian inference algorithm for latent
dirichlet allocation. In NIPS, volume 19, 2006.
[8] C.E. Antoniak. Mixtures of Dirichlet processes with applications to bayesian nonparametric problems.
The Annals of Statistics, 2:1152–1174, 1974.
[9] B. Bidyuk and R. Dechter. Cycle-cutset sampling for Bayesian networks. In Sixteenth Canadian Conf.
on AI, 2003.
[10] David Blei, Thomas L. Grifﬁths, Michael I. Jordan, and Joshua B. Tenenbaum. Hierarchical topic models
and the nested chinese restaurant process. In Neural Information Processing Systems 16, 2004.
[11] B. Marlin. Modeling user rating proﬁles for collaborative ﬁltering. In
Advances in Neural Information
Processing Systems 16. 2004.
[12] S. Kim and P. Smyth. Hierarchical dirichlet processes with random effects. In NIPS, volume 19, 2006.
[13] W. Li and A. McCallum. Pachinko allocation: Dag-structured mixture models of topic correlations. In
Proceedings of the 23rd international conference on Machine learning, pages 577–584, 2006.
[14] W. Li, A. McCallum, and D. Blei. Nonparametric bayes pachinko allocation. In UAI, 2007.
[15] D. Larlus and F. Jurie. Latent mixture vocabularies for object categorization. In British Machine Vision
Conference, 2006.
[16] E. Airoldi, D. Blei, E. Xing, and S. Fienberg. A latent mixed membership model for relational data. In
LinkKDD ’05: Proceedings of the 3rd international workshop on Link discovery, pages 82–89, 2005.
[17] R. Agrawal, T. Imielinski, and A. Swami. Mining associations between sets of items in massive databases.
In Proc. of the ACM-SIGMOD 1993 Intl Conf on Management of Data, 1993.
[18] M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. The inﬁnite hidden markov model.
577–584, 2001.
[19] Y. W. Teh, D. G ¨or ¨ur, and Z. Ghahramani. Stick-breaking construction for the Indian buffet process. In
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics , volume 11, 2007.
[20] W. Li D. Mimno and A. McCallum. Mixtures of hierarchical topics with pachinko allocation. In Pro-
ceedings of the 21st International Conference on Machine Learning, 2007.

In NIPS, pages

8


Discriminative Keyword Selection Using Support
Vector Machines

W. M. Campbell, F. S. Richardson
MIT Lincoln Laboratory
Lexington, MA 02420
wcampbell,frichard@ll.mit.edu

Abstract

Many tasks in speech processing involve classiﬁcation of lo ng term characteristics
of a speech segment such as language, speaker, dialect, or topic. A natural tech-
nique for determining these characteristics is to ﬁrst conv ert the input speech into
a sequence of tokens such as words, phones, etc. From these tokens, we can then
look for distinctive sequences, keywords, that characterize the speech. In many
applications, a set of distinctive keywords may not be known a priori. In this
case, an automatic method of building up keywords from short context units such
as phones is desirable. We propose a method for the construction of keywords
based upon Support Vector Machines. We cast the problem of keyword selection
as a feature selection problem for n-grams of phones. We propose an alternat-
ing ﬁlter-wrapper method that builds successively longer k eywords. Application
of this method to language recognition and topic recognition tasks shows that the
technique produces interesting and signiﬁcant qualitativ e and quantitative results.

1 Introduction

A common problem in speech processing is to identify properties of a speech segment such as
the language, speaker, topic, or dialect. A typical solution to this problem is to apply a detection
paradigm. A set of classiﬁers is applied to a speech segment t o produce a decision. For instance, for
language recognition, we might construct detectors for English, French, and Spanish. The maximum
scoring detector on a speech segment would be the predicted language.

Two basic categories of systems have been applied to the detection problem. A ﬁrst approach uses
short-term spectral characteristics of the speech and models these with Gaussian mixture models
(GMMs) or support vector machines (SVMs) directly producing a decision. Although quite accurate,
this type of system produces only a classiﬁcation decision w ith no qualitative interpretation. A
second approach uses high level features of the speech such as phones and words to detect the
properties. An advantage of this approach is that, in some instances, we can explain why we made a
decision. For example, a particular phone or word sequence might indicate the topic. We adopt this
latter approach for our paper.

SVMs have become a common method of extracting high-level properties of sequences of speech
tokens [1, 2, 3, 4]. Sequence kernels are constructed by viewing a speech segment as a document of
tokens. The SVM feature space in this case is a scaling of co-occurrence probabilities of tokens in
an utterance. This technique is analogous to methods for applying SVMs to text classiﬁcation [5].

SVMs have been applied at many linguistic levels of tokens as detectors. Our focus in this paper
is at the acoustic phone level. Our goal is to automatically derive long sequences of phones which

∗This work was sponsored by the Department of Homeland Security under Air Force Contract FA8721-
05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not
necessarily endorsed by the United States Government.

1

we call keywords which are characteristic of a given class. Prior work, for example, in language
recognition [6], has shown that certain words are a signiﬁca nt predictor of a language. For instance,
the presence of the phrase “you know ” in a conversational spe
ech segment is a strong indicator of
English. A difﬁculty in using words as the indicator of the la nguage is that we may not have available
a speech-to-text (STT) system in all languages of interest. In this case, we’d like to automatically
construct keywords that are indicative of the language. Note that a similar problem can occur in
other property extraction problems. For instance, in topic recognition, proper names not in our STT
system dictionary may be a strong indicator of topic.

Our basic approach is to view keyword construction as a feature selection problem. Keywords are
composed of sequences of phones of length n, i.e. n-grams. We would like to ﬁnd the set of
n-grams that best discriminates between classes. Unfortunately, this problem is difﬁcult to solve
directly, since the number of unique n-grams grows exponentially with increasing n. To alleviate
this difﬁcultly, we propose a method that starts with lower o rder n-grams and successively builds
higher order n-grams.
The outline of the paper is as follows. In Section 2.1, we review the basic architecture that we use
for phone recognition and how it is applied to the problem. In Section 2.2, we review the application
of SVMs to determining properties. Section 3.1 describes a feature selection method for SVMs.
Section 3.2 presents our method for constructing long context units of phones to automatically cre-
ate keywords. We use a novel feature selection approach that attempts to ﬁnd longer strings that
discriminate well between classes. Finally, in Section 4, we show the application of our method
to language and topic recognition problems. We show qualitatively that the method produces in-
teresting keywords. Quantitatively, we show that the method produces keywords which are good
discriminators between classes.

2 Phonotactic Classi ﬁcation

2.1 Phone Recognition

The high-level token extraction component of our system is a phone recognizer based upon the Brno
University (BUT) design [7]. The basic architecture of this system is a monophone HMM system
with a null grammar. Monophones are modeled by three states. This system uses two powerful
components to achieve high accuracy. First, split temporal context (STC) features provide contextual
cues for modeling monophones. Second, the BUT recognizer extensively uses discriminatively
trained feedforward artiﬁcial neural networks (ANNs) to mo del HMM state posterior probabilities.

We developed a phone recognizer for English units using the BUT architecture and automatically
generated STT transcripts on the Switchboard 2 Cell corpora [8]. Training data consisted of approx-
imately 10 hours of speech. ANN training was accomplished using the ICSI Quicknet package [9].
The resulting system has 49 monophones including silence.

The BUT recognizer is used along with the HTK HMM toolkit [10] to produce lattices. Lattices
encode multiple hypotheses with acoustic likelihoods. From a lattice, a 1-best (Viterbi) output can
be produced. Alternatively, we use the lattice to produce expected counts of tokens and n-grams of
tokens.

Expected counts of n-grams can be easily understood as an extension of standard counts. Suppose
we have a hypothesized string of tokens, W = w1 , · · · , wn . Then bigrams are created by group-
ing two tokens at a time to form, W2 = w1 _w2 , w2 _w3 , · · · , wn−1 _wn . Higher order n-grams
are formed from longer juxtapositions of tokens. The count function for a given bigram, di , is
count(di |W2 ) is the number of occurrences of di in the sequence W2 . To extend counts to a lattice,
L, we ﬁnd the expected count over all all possible hypotheses i n the lattice,
count(di |L) = EW [count(di |W )] = XW ∈L
The expected counts can be computed efﬁciently by a forward- backward algorithm; more details
can be found in Section 3.3 and [11].

p(W |L) count(di |W ).

(1)

2

A useful application of expected counts is to ﬁnd the probabi lity of an n-gram in a lattice. For a
lattice, L, the joint probability of an n-gram, di , is
count(di |L)
p(di |L) =
Pj count(dj |L)
where the sum in (2) is performed over all unique n-grams in the utterance.

(2)

2.2 Discriminative Language Modeling: SVMs

We focus on token-based language recognition with SVMs using the approach from [1, 4]. Similar
to [1], a lattice of tokens, L, is modeled using a bag-of-n-grams approach. Joint probabilities of
the unique n-grams, dj , on a per conversation basis are calculated, p(dj |L), see (2). Then, the
probabilities are mapped to a sparse vector with entries
Dj p(dj |W ).
The selection of the weighting, Dj , in (3) is critical for good performance. A typical choice is of the
form
Dj = min (cid:18)Cj , gj (cid:18)
p(dj |all) (cid:19)(cid:19)
1
where gj (·) is a function which squashes the dynamic range, and Cj is a constant. The probabil-
ity p(dj |all) in (4) is calculated from the observed probability across all classes. The squashing
function should monotonically map the interval [1, ∞) to itself to suppress large inverse probabili-
ties. Typical choices for gj are gj (x) = √x and gj (x) = log(x) + 1. In both cases, the squashing
function gj normalizes out the typicality of a feature across all classes. The constant Cj limits the
effect of any one feature on the kernel inner product. If we set Cj = 1, then this makes Dj = 1 for
all j . For the experiments in this paper, we use gj (x) = √x, which is suited to high frequency token
streams.

(3)

(4)

The general weighting of probabilities is then combined to form a kernel between two lattices,
see [1] for more details. For two lattices, L1 and L2 , the kernel is
K (L1 , L2 ) = Xj
D2
j p(dj |L1 )p(dj |L2 ).
Intuitively, the kernel in (5) says that if the same n-grams are present in two sequences and the
normalized frequencies are similar there will be a high degree of similarity (a large inner product).
If n-grams are not present, then this will reduce similarity since one of the probabilities in (5) will be
zero. The normalization Dj insures that n-grams with large probabilities do not dominate the kernel
function. The kernel can alternatively be viewed as a linearization of the log-likelihood ratio [1].

(5)

Incorporating the kernel (5) into an SVM system is straightforward. SVM training and scoring
require only a method of kernel evaluation between two objects that produces positive de ﬁnite kernel
matrices (the Mercer condition). We use the package SVMTorch [12]. Training is performed with a
one-versus-all strategy. For each target class, we group all remaining class data and then train with
these two classes.

3 Discriminative Keyword Selection

3.1 SVM Feature Selection

A ﬁrst step towards an algorithm for automatic keyword gener ation using phones is to examine
feature selection methods. Ideally, we would like to select over all possible n-grams, where n is
varying, the most discriminative sequences for determining a property of a speech segment. The
number of features in this case is prohibitive, since it grows exponentially with n. Therefore, we
have to consider alternate methods.

As a ﬁrst step, we examine feature selection for ﬁxed
n and look for keywords with n or less phones.
Suppose that we have a set of candidate keywords. Since we are already using an SVM, a natural
algorithm for discriminative feature selection in this case is to use a wrapper method [13].

3

Suppose that the optimized SVM solution is

f (X ) = Xi

αiK (X, Xi ) + c

(6)

and

w = Xi
where b(Xi ) is the vector of weighted n-gram probabilities in (3). We note that the kernel presented
in (5) is linear. Also, the n-gram probabilities have been normalized in (3) by their probability across
the entire data set. Intuitively, because of this normalization and since f (X ) = wtb(X ) + c, large
magnitude entries in w correspond to signiﬁcant features.

αib(Xi )

(7)

A con ﬁrmation of this intuitive idea is the algorithm of Guyo n, et. al. [14]. Guyon proposes an
iterative wrapper method for feature selection for SVMs which has these basic steps:

• For a set of features, S , ﬁnd the SVM solution with model w.
• Rank the features by their corresponding model entries w2
i . Here, wi is the ith entry of w
in (7).
• Eliminate low ranking features using a threshold.
The algorithm may be iterated multiple times.

Guyon’s algorithm for feature selection can be used for picking signiﬁcant n-grams as keywords.
We can create a kernel which is the sum of kernels as in (5) up to the desired n. We then train an
SVM and rank n-grams according to the magnitude of the entries in the SVM model vector, w.
As an example, we have looked at this feature selection method for a language recognition task
with trigrams (to be described in Section 4). Figure 1 provides a motivation for the applicability of
Guyon’s feature selection method. The ﬁgure shows two funct ions. First, the cumulative density
function (CDF) of the SVM model values, |wi |, is shown. The CDF has an S-curve shape; i.e., only
a small set of models weights has large magnitudes. The second curve shows the equal error rate
(EER) of the task as a function of applying one iteration of the Guyon algorithm and retraining the
SVM. EER is de ﬁned as the value where the miss and false alarm r ates are equal. All features with
|wi | below the value on the x-axis are discarded in the ﬁrst iterat ion. From the ﬁgure, we see that
only a small fraction (< 5%) of the features are needed to obtain good error rates. This interesting
result provides motivation that a small subset of keywords are signiﬁcant to the task.

1

0.75

|
i
w
|
 
F
D
C

0.5

0.25

CDF

EER

0.2

0.15

0.1

0.05

e
t
a
R
 
r
o
r
r
E
 
l
a
u
q
E

0
10−4
10−4

10−3
10−3

10−2
10−2
Threshold

10−1
10−1

1000
100

Figure 1: Feature selection for a trigram language recognition task using Guyon’s method

4

3.2 Keywords via an alternating wrapper/ﬁlter method

The algorithm in Section 3.1 gives a method for n-gram selection for ﬁxed n. Now, suppose we
want to ﬁnd keywords for arbitrary n. One possible hypothesis for keyword selection is that since
higher order n-grams are discriminative, lower order n-grams in the keywords will also be discrim-
inative. Therefore, it makes sense to ﬁnding distinguishin g lower order n-grams and then construct
longer units from these. On the basis of this idea, we propose the following algorithm for keyword
construction:
Keyword Building Algorithm

• Start with an initial value of n = ns . Initialize the set, S ′
n , to all possible n-grams of phones
including lower order grams. By default, let S1 be the set of all phones.
• (Wrapper Step) General n. Apply the feature selection algorithm in Section 3.1 to produce
a subset of distinguishing n-grams, Sn ⊂ S ′
n .
• (Filter Step) Construct a new set of (n + 1)-grams by juxtaposing elements from Sn with
phones. Nominally, we take this step to be juxtaposition on the right and left, S ′
n+1 =
{dp, qd|d ∈ Sn , p ∈ S1 , q ∈ S1}.
• Iterate to the wrapper step.
• Output: Sn at some stopping n.
A few items should be noted about the proposed keyword building algorithm. First, we call the sec-
ond feature selection process a ﬁlter step, since induction has not been applied to the (n + 1)-gram
features. Second, note that the purpose of the ﬁlter step is t o provide a candidate set of possible
(n + 1)-grams which can then be more systematically reduced. Third, several potential algorithms
exist for the ﬁlter step. In our experiments and in the algori thm description, we nominally append
one phone to the beginning and end of an n-gram. Another possibility is to try to combine over-
lapping n-grams. For instance, suppose the keyword is some_people which has phone transcript
s_ah_m_p_iy_p_l. Then, if we are looking at 4-grams, we might see as top features s_ah_m_p and
p_iy_p_l and combine these to produce a new keyword.

3.3 Keyword Implementation

The expected n-gram counts were computed from lattices using the forward-backward algo-
rithm. Equation (8) gives the posterior probability of a connected sequence of arcs in the lattice
where src_nd(a) and dst_nd(a) are the source and destination node of arc a, ℓ(a)is the likelihood
associated with arc a, α(n) and β (n) are the forward and backward probabilities of reaching node n
from the beginning or end of the lattice L respectively, and ℓ(L) is the total likelihood of the lattice
(the α(·) of the ﬁnal node or β (·) of the initial node of the lattice).
α(src_nd(aj ))ℓ(aj ) . . . ℓ(aj+n )β (dst_nd(aj+n ))
p(aj , ..., aj+n ) =
ℓ(L)
Now if we de ﬁne the posterior probability of a node p(n) as p(n) = (α(n)β (n))/ℓ(L). Then
equation (8) becomes:

(8)

p(aj , ..., aj+n ) =

p(aj ) . . . p(aj+n )
p(src_nd(aj+1 )) . . . p(src_nd(aj+n ))
Equation (9) is attractive because it provides a way of computing the path posteriors locally using
only the individual arc and node posteriors along the path. We use this computation along with a
trie structure [15] to compute the posteriors of our keywords.

.

(9)

4 Experiments

4.1 Language Recognition Experimental Setup

The phone recognizer described in Section 2.1 was used to generate lattices across a train and an
evaluation data set. The training data set consists of more than 360 hours of telephone speech

5

spanning 13 different languages and coming from a variety of different sources including Callhome,
Callfriend and Fisher. The evaluation data set is the NIST 2005 Language Recognition Evaluation
data consisting of roughly 20,000 utterances (with duration of 30, 10 or 3 seconds depending on the
task) coming from three collection sources including Callfriend, Mixer and OHSU. We evaluated
our system for the 30 and 10 second task under the the NIST 2005 closed condition which limits
the evaluation data to 7 languages (English, Hindi, Japanese, Korean, Mandarin, Spanish and Tamil)
coming only from the OHSU data source.

The training and evaluation data was segmented using an automatic speech activity detector and
segments smaller than 0.5 seconds were thrown out. We also sub-segmented long audio ﬁles in the
training data to keep the duration of each utterance to around 5 minutes (a shorter duration would
have created too many training instances). Lattice arcs with posterior probabilities lower than 10−6
were removed and lattice expected counts smaller than 10−3 were ignored. The top and bottom
600 ranking keywords for each language were selected after each training iteration. The support
vector machine was trained using a kernel formulation which requires pre-computing all of the
kernel distances between the data points and using an alternate kernel which simply indexes into
the resulting distance matrix (this approach becomes difﬁc ult when the number of data points is too
large).

4.2 Language Recognition Results (Qualitative and Quantitative)

To get a sense of how well our keyword building algorithm was working, we looked at the top
ranking keywords from the English model only (since our phone recognizer is trained using the
English phone set). Table 1 summarizes a few of the more compelling phone 5-grams, and a possible
keyword that corresponds to each one. Not suprisingly, we noticed that in the list of top-ranking
n-grams there were many variations or partial n-gram matches to the same keyword, as well as
n-grams that didn’t correspond to any apparent keyword.
The equal error rates for our system on the NIST 2005 language recognition evaluation are summa-
rized in Table 2. The 4-gram system gave a relative improvement of 12% on the 10 second task and
9% on the 30 second task, but despite the compelling keywords produced by the 5-gram system, the
performance actually degraded signiﬁcantly compared to th e 3-gram and 4-gram systems.

Table 1: Top ranking keywords for 5-gram SVM for English language recognition model
keyword
Rank
phones
1
you know
<s> yeah
3
<s> ???
4
6
people
7
really
you know (var)
8
? like
17
23
like (var)
right </s>
27
have an
29
37
<s> well

SIL_Y_UW_N_OW
!NULL_SIL_Y_EH_AX
!NULL_SIL_IY_M_TH
P_IY_P_AX_L
R_IY_L_IY_SIL
Y_UW_N_OW_OW
T_L_AY_K_SIL
L_AY_K_K_SIL
R_AY_T_SIL_!NULL
HH_AE_V_AX_N
!NULL_SIL_W_EH_L

Table 2: %EER for 10 and 30 second NIST language recognition tasks
N
1
2
3
4
5
13.6
10.0
11.3
16.5
25.3
10sec
30sec
18.3
07.4
04.3
03.9
05.6

6

4.3 Topic Recognition Experimental Setup

Topic recognition was performed using a subset of the phase I Fisher corpus (English) from LDC.
This corpus consists of 5, 851 telephone conversations. Participants were given instructions to dis-
cuss a topic for 10 minutes from 40 different possible topics. Topics included “Education ”, “ Hob-
bies,”
t discussion on the topics. An example
“Foreign Relations”, etc. Prompts were used to elici
prompt is:

Movies: Do each of you enjoy going to the movies in a theater, or would you
rather rent a movie and stay home? What was the last movie that you saw? Was it
good or bad and why?

For our experiments, we used 2750 conversation sides for training. We also constructed development
and test sets of 1372 conversation sides each. The training set was used to ﬁnd key words and models
for topic detection.

4.4 Topic Recognition Results

We ﬁrst looked at top ranking keywords for several topics; so me results are shown in Table 3. We
can see that many keywords show a strong correspondence with the topic. Also, there are partial
keywords which correspond to what appears to be longer keywords, e.g. “eh_t_s_ih_k ” corresponds
to get sick.

As in the language recognition task, we used EER as the performance measure. Results in Table 4
show the performance for several n-gram orders. Performance improves going from 3-grams to 4-
grams. But, as with the language recognition task, we see a degradation in performance for 5-grams.

5 Conclusions and future work

We presented a method for automatic construction of keywords given a discriminative speech classi-
ﬁcation task. Our method was based upon successively buildi ng longer span keywords from shorter
span keywords using phones as a fundamental unit. The problem was cast as a feature selection
problem, and an alternating ﬁlter and wrapper algorithm was proposed. Results showed that reason-
able keywords and improved performance could be achieved using this methodology.

Table 3: Top keyword for 5-gram SVM in Topic Recognition
Topic
Phones
Professional Sports on TV
Hypothetical: Time Travel
Afﬁrmative Action
US Public Schools
Movies
Hobbies
September 11
Issues in the Middle East
Illness
Hypothetical: One Million Dollars to leave the US

Keyword
sport
S_P_AO_R_T
go back
G_OW_B_AE_K
AX_V_AE_K_CH [afﬁrmat] ive act[ion]
schools
S_K_UW_L_Z
DVD
IY_V_IY_D_IY
hobbies
HH_OH_B_IY_Z
happen
HH_AE_P_AX_N
Israel
IH_Z_R_IY_L
[g]et sick
EH_T_S_IH_K
you may
Y_UW_M_AY_Y

Table 4: Performance of Topic Detection for Different n-gram orders
5
4
n-gram order
3
10.22
EER (%)
8.95
9.40

7

Numerous possibilities exist for future work on this task. First, extension and experimentation on
other tasks such as dialect and speaker recognition would be interesting. The method has the poten-
tial for discovery of new interesting characteristics. Second, comparison of this method with other
feature selection methods may be appropriate [16]. A third area for extension is various technical
improvements. For instance, we might want to consider more general keyword models where skips
are allowed (or more general ﬁnite state transducers [17]). Also, alternate methods for the ﬁlter for
constructing higher order n-grams is a good area for exploration.

References

[1] W. M. Campbell, J. P. Campbell, D. A. Reynolds, D. A. Jones, and T. R. Leek,
“Phonetic
speaker recognition with support vector machines,” in Advances in Neural Information Pro-
cessing Systems 16, Sebastian Thrun, Lawrence Saul, and Bernhard Sch ölkopf, E ds. MIT
Press, Cambridge, MA, 2003.
[2] W. M. Campbell, T. Gleason, J. Navratil, D. Reynolds, W. Shen, E. Singer, and P. Torres-
Carrasquillo, “Advanced language recognition using cepst ra and phonotactics: MITLL system
performance on the NIST 2005 language recognition evaluation,”
in Proc. IEEE Odyssey,
2006.
[3] Bin Ma and Haizhou Li,
“A phonotactic-semantic paradigm for automatic spoken document
classiﬁcation,” in
The 28th Annual International ACM SIGIR Conference, Brazil, 2005.
[4] Lu-Feng Zhai, Man hung Siu, Xi Yang, and Herbert Gish, “Di scriminatively trained language
models using support vector machines for language identiﬁc ation,”
in Proc. IEEE Odyssey:
The Speaker and Language Recognition Workshop, 2006.
[5] T. Joachims, Learning to Classify Text Using Support Vector Machines, Kluwer Academic
Publishers, 2002.
[6] W. M. Campbell, F. Richardson, and D. A. Reynolds, “Langu age recognition with word lattices
and support vector machines,” in Proceedings of ICASSP, 2007, pp. IV–989 – IV–992.
[7] Petr Schwarz, Matejka Pavel, and Jan Cernocky, “Hierarc hical structures of neural networks
for phoneme recognition,” in Proceedings of ICASSP, 2006, pp. 325 –328.
[8] Linguistic Data Consortium, “Switchboard-2 corpora,”
http://www.ldc.upenn.edu.
[9] “ICSI QuickNet,” http://www.icsi.berkeley.edu/Spee
ch/qn.html.
[10] S. Young, Gunnar Evermann, Thomas Hain, D. Kershaw, Gareth Moore, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland, The HTK book, Entropic, Ltd., Cambridge, UK, 2002.
[11] L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition, Prentice-Hall, 1993.
[12] Ronan Collobert and Samy Bengio,
“SVMTorch: Support ve ctor machines for large-scale
regression problems,” Journal of Machine Learning Research, vol. 1, pp. 143 –160, 2001.
[13] Avrim L. Blum and Pat Langley,
“Selection of relevant fe atures and examples in machine
learning,” Artiﬁcial Intelligence , vol. 97, no. 1-2, pp. 245 –271, Dec. 1997.
[14] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “Gene se lection for cancer classiﬁcation using
support vector machines,” Machine Learning, vol. 46, no. 1-3, pp. 389 –422, 2002.
[15] Konrad Rieck and Pavel Laskov, “Language models for det ection of unknown attacks in net-
work trafﬁc,”
Journal of Computer Virology, vol. 2, no. 4, pp. 243 –256, 2007.
[16] Takaaki Hori, I. Lee Hetherington, Timothy J. Hazen, and James R. Glass, “Open-vocabulary
spoken utterance retrieval using confusion neworks,” in Proceedings of ICASSP, 2007.
[17] C. Cortes, P. Haffner, and M. Mohri, “Rational kernels, ” in Advances in Neural Information
Processing Systems 15, S. Thrun S. Becker and K. Obermayer, Eds., Cambridge, MA, 2003,
pp. 601 –608, MIT Press.

8


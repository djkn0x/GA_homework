Stability Bounds for Non-i.i.d. Processes

Mehryar Mohri
Courant Institute of Mathematical Sciences
and Google Research
251 Mercer Street
New York, NY 10012
mohri@cims.nyu.edu

Afshin Rostamizadeh
Department of Computer Science
Courant Institute of Mathematical Sciences
251 Mercer Street
New York, NY 10012
rostami@cs.nyu.edu

Abstract

The notion of algorithmic stability has been used effectively in the past to derive
tight generalization bounds. A key advantage of these bounds is that they are de-
signed for speciﬁc learning algorithms, exploiting their p articular properties. But,
as in much of learning theory, existing stability analyses and bounds apply only
in the scenario where the samples are independently and identically distributed
(i.i.d.). In many machine learning applications, however, this assumption does
not hold. The observations received by the learning algorithm often have some
inherent temporal dependence, which is clear in system diagnosis or time series
prediction problems. This paper studies the scenario where the observations are
drawn from a stationary mixing sequence, which implies a dependence between
observations that weaken over time. It proves novel stability-based generalization
bounds that hold even with this more general setting. These bounds strictly gen-
eralize the bounds given in the i.i.d. case. It also illustrates their application in the
case of several general classes of learning algorithms, including Support Vector
Regression and Kernel Ridge Regression.

1 Introduction

The notion of algorithmic stability has been used effectively in the past to derive tight generalization
bounds [2 –4, 6]. A learning algorithm is stable when the hypo theses it outputs differ in a limited way
when small changes are made to the training set. A key advantage of stability bounds is that they are
tailored to speciﬁc learning algorithms, exploiting their particular properties. They do not depend
on complexity measures such as the VC-dimension, covering numbers, or Rademacher complexity,
which characterize a class of hypotheses, independently of any algorithm.

But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario
where the samples are independently and identically distributed (i.i.d.). Note that the i.i.d. assump-
tion is typically not tested or derived from a data analysis. In many machine learning applications
this assumption does not hold. The observations received by the learning algorithm often have some
inherent temporal dependence, which is clear in system diagnosis or time series prediction prob-
lems. A typical example of time series data is stock pricing, where clearly prices of different stocks
on the same day or of the same stock on different days may be dependent.

This paper studies the scenario where the observations are drawn from a stationary mixing sequence,
a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between
observations that weakens over time [8, 10, 16, 17]. Our proofs are also based on the independent
block technique commonly used in such contexts [17] and a generalized version of McDiarmid’s
inequality [7]. We prove novel stability-based generalization bounds that hold even with this more
general setting. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all
stable learning algorithms thereby extending the usefulness of stability-bounds to non-i.i.d. scenar-

1

ios. It also illustrates their application to general classes of learning algorithms, including Support
Vector Regression (SVR) [15] and Kernel Ridge Regression [13].

Algorithms such as support vector regression (SVR) [14, 15] have been used in the context of time
series prediction in which the i.i.d. assumption does not hold, some with good experimental re-
sults [9, 12]. To our knowledge, the use of these algorithms in non-i.i.d. scenarios has not been
supported by any theoretical analysis. The stability bounds we give for SVR and many other kernel
regularization-based algorithms can thus be viewed as the ﬁ rst theoretical basis for their use in such
scenarios.

In Section 2, we will introduce the de ﬁnitions for the non-i.
i.d. problems we are considering and
discuss the learning scenarios. Section 3 gives our main generalization bounds based on stabil-
ity, including the full proof and analysis. In Section 4, we apply these bounds to general kernel
regularization-based algorithms, including Support Vector Regression and Kernel Ridge Regression.

2 Preliminaries

servations in mixing theory [5] and
We ﬁrst introduce some standard de ﬁnitions for dependent ob
then brie ﬂy discuss the learning scenarios in the non-i.i.d . case.

2.1 Non-i.i.d. De ﬁnitions
De ﬁnition 1. A sequence of random variables Z = {Zt}∞
t=−∞ is said to be stationary if for any t
and non-negative integers m and k , the random vectors (Zt , . . . , Zt+m ) and (Zt+k , . . . , Zt+m+k )
have the same distribution.

Thus, the index t or time, does not affect the distribution of a variable Zt in a stationary sequence.
This does not imply independence however. In particular, for i < j < k , Pr[Zj | Zi ] may not
equal Pr[Zk | Zi ]. The following is a standard de ﬁnition giving a measure of th e dependence of the
random variables Zt within a stationary sequence. There are several equivalent de ﬁnitions of this
quantity, we are adopting here that of [17].

(1)

β (k) = sup
n

De ﬁnition 2. Let Z = {Zt}∞
t=−∞ be a stationary sequence of random variables. For any i, j ∈
Z ∪ {−∞, +∞}, let σj
i denote the σ-algebra generated by the random variables Zk , i ≤ k ≤ j .
Then, for any positive integer k , the β -mixing and ϕ-mixing coefﬁcients of the stochastic process Z
are de ﬁned as
(cid:12)(cid:12)(cid:12)Pr[A | B ] − Pr[A](cid:12)(cid:12)(cid:12).
n+k (cid:12)(cid:12)(cid:12)Pr[A | B ] − Pr[A](cid:12)(cid:12)(cid:12)i ϕ(k) = sup
−∞h sup
E
B∈σn
n
A∈σ∞
A∈σ∞
n+k
B∈σn
−∞
Z is said to be β -mixing (ϕ-mixing) if β (k) → 0 (resp. ϕ(k) → 0) as k → ∞. It is said to be
algebraically β -mixing (algebraically ϕ-mixing) if there exist real numbers β0 > 0 (resp. ϕ0 > 0)
and r > 0 such that β (k) ≤ β0/kr (resp. ϕ(k) ≤ ϕ0/kr ) for all k , exponentially mixing if there
exist real numbers β0 (resp. ϕ0 > 0) and β1 (resp. ϕ1 > 0) such that β (k) ≤ β0 exp(−β1kr ) (resp.
ϕ(k) ≤ ϕ0 exp(−ϕ1kr )) for all k .
Both β (k) and ϕ(k) measure the dependence of the events on those that occurred more than k
units of time in the past. β -mixing is a weaker assumption than φ-mixing. We will be using a
concentration inequality that leads to simple bounds but that applies to φ-mixing processes only.
However, the main proofs presented in this paper are given in the more general case of β -mixing
sequences. This is a standard assumption adopted in previous studies of learning in the presence
of dependent observations [8, 10, 16, 17]. As pointed out in [16], β -mixing seems to be “just the
right” assumption for carrying over several PAC-learning r esults to the case of weakly-dependent
sample points. Several results have also been obtained in the more general context of α-mixing but
they seem to require the stronger condition of exponential mixing [11]. Mixing assumptions can be
checked in some cases such as with Gaussian or Markov processes [10]. The mixing parameters can
also be estimated in such cases.

2

Most previous studies use a technique originally introduced by [1] based on independent blocks of
equal size [8, 10, 17]. This technique is particularly relevant when dealing with stationary β -mixing.
We will need a related but somewhat different technique since the blocks we consider may not have
the same size. The following lemma is a special case of Corollary 2.7 from [17].
Lemma 1 (Yu [17], Corollary 2.7). Let µ ≥ 1 and suppose that h is measurable function, with
absolute value bounded by M , on a product probability space (cid:16)Qµ
ri (cid:17) where ri ≤
j=1 Ωj , Qµ
i=1 σsi
si ≤ ri+1 for all i. Let Q be a probability measure on the product space with marginal measures Qi
rj (cid:17), i = 1, . . . , µ−1.
), and let Qi+1 be the marginal measure of Q on (cid:16)Qi+1
j=1 Ωj , Qi+1
j=1 σsj
on (Ωi , σsi
ri
Let β (Q) = sup1≤i≤µ−1 β (ki ), where ki = ri+1 − si , and P = Qµ
i=1 Qi . Then,
(2)
| E
[h] − E
[h]| ≤ (µ − 1)M β (Q).
Q
P
The lemma gives a measure of the difference between the distribution of µ blocks where the blocks
are independent in one case and dependent in the other case. The distribution within each block
is assumed to be the same in both cases. For a monotonically decreasing function β , we have
β (Q) = β (k∗ ), where k∗ = mini (ki ) is the smallest gap between blocks.

2.2 Learning Scenarios

We consider the familiar supervised learning setting where the learning algorithm receives a sample
of m labeled points S = (z1 , . . . , zm) = ((x1 , y1 ), . . . , (xm , ym)) ∈ (X × Y )m , where X is the
input space and Y the set of labels (Y = R in the regression case), both assumed to be measurable.
For a ﬁxed learning algorithm, we denote by hS the hypothesis it returns when trained on the sample
S . The error of a hypothesis on a pair z ∈ X ×Y is measured in terms of a cost function c : Y ×Y →
R+ . Thus, c(h(x), y ) measures the error of a hypothesis h on a pair (x, y ), c(h(x), y ) = (h(x) − y )2
in the standard regression cases. We will use the shorthand c(h, z ) := c(h(x), y ) for a hypothesis h
and z = (x, y ) ∈ X × Y and will assume that c is upper bounded by a constant M > 0. We denote
by bR(h) the empirical error of a hypothesis h for a training sample S = (z1 , . . . , zm ):
mXi=1
1
bR(h) =
c(h, zi ).
m
In the standard machine learning scenario, the sample pairs z1 , . . . , zm are assumed to be i.i.d., a
restrictive assumption that does not always hold in practice. We will consider here the more general
case of dependent samples drawn from a stationary mixing sequence Z over X × Y . As in the i.i.d.
case, the objective of the learning algorithm is to select a hypothesis with small error over future
samples. But, here, we must distinguish two versions of this problem.

(3)

In the most general version, future samples depend on the training sample S and thus the general-
ization error or true error of the hypothesis hS trained on S must be measured by its expected error
conditioned on the sample S :

[c(hS , z ) | S ].
R(hS ) = E
z
This is the most realistic setting in this context, which matches time series prediction problems.
A somewhat less realistic version is one where the samples are dependent, but the test points are
assumed to be independent of the training sample S . The generalization error of the hypothesis hS
trained on S is then:

(4)

[c(hS , z ) | S ] = E
R(hS ) = E
[c(hS , z )].
z
z
This setting seems less natural since if samples are dependent, then future test points must also
depend on the training points, even if that dependence is relatively weak due to the time interval
after which test points are drawn. Nevertheless, it is this somewhat less realistic setting that has
been studied by all previous machine learning studies that we are aware of [8, 10, 16, 17], even when
examining speciﬁcally a time series prediction problem [10 ]. Thus, the bounds derived in these
studies cannot be applied to the more general setting.

(5)

We will consider instead the most general setting with the de ﬁnition of the generalization error based
on Eq. 4. Clearly, our analysis applies to the less general setting just discussed as well.

3

3 Non-i.i.d. Stability Bounds

This section gives generalization bounds for ˆβ -stable algorithms over a mixing stationary distribu-
tion.1 The ﬁrst two sections present our main proofs which hold for β -mixing stationary distri-
butions. In the third section, we will be using a concentration inequality that applies to φ-mixing
processes only.

The condition of ˆβ -stability is an algorithm-dependent property ﬁrst introd uced in [4] and [6]. It has
been later used successfully by [2, 3] to show algorithm-speciﬁc stability bounds for i.i.d. samples.
Roughly speaking, a learning algorithm is said to be stable if small changes to the training set do
not produce large deviations in its output. The following gives the precise technical de ﬁnition.
De ﬁnition 3. A learning algorithm is said to be (uniformly) ˆβ -stable if the hypotheses it returns for
any two training samples S and S ′ that differ by a single point satisfy
|c(hS , z ) − c(hS ′ , z )| ≤ ˆβ .
∀z ∈ X × Y ,
Many generalization error bounds rely on McDiarmid’s inequality. But this inequality requires the
random variables to be i.i.d. and thus is not directly applicable in our scenario. Instead, we will
use a theorem that extends McDiarmid’s inequality to general mixing distributions (Theorem 1,
Section 3.3).
To obtain a stability-based generalization bound, we will apply this theorem to Φ(S ) = R(hS ) −
bR(hS ). To do so, we need to show, as with the standard McDiarmid’s inequality, that Φ is a Lipschitz
function and, to make it useful, bound E[Φ]. The next two sections describe how we achieve both of
these in this non-i.i.d. scenario.
3.1 Lipschitz Condition

(6)

As discussed in Section 2.2, in the most general scenario, test points depend on the training sample.
We ﬁrst present a lemma that relates the expected value of the generalization error in that scenario
and the same expectation in the scenario where the test point is independent of the training sample.
We denote by R(hS ) = Ez [c(hS , z )|S ] the expectation in the dependent case and by eR(hSb ) =
Eez [c(hSb , ez )] that expectation when the test points are assumed independent of the training, with
Sb denoting a sequence similar to S but with the last b points removed. Figure 1(a) illustrates that
sequence. The block Sb is assumed to have exactly the same distribution as the corresponding block
of the same size in S .
Lemma 2. Assume that the learning algorithm is ˆβ -stable and that the cost function c is bounded
by M . Then, for any sample S of size m drawn from a β -mixing stationary distribution and for any
b ∈ {0, . . . , m}, the following holds:
[ eR(hSb )]| ≤ b ˆβ + β (b)M .
[R(hS )] − E
| E
S
S
Proof. The ˆβ -stability of the learning algorithm implies that
[c(hSb , z )] + b ˆβ .

(7)

(8)

[R(hS )] = E
E
S
S,z

[c(hS , z )] ≤ E
S,z

The application of Lemma 1 yields
[c(hSb , ez )] + b ˆβ + β (b)M = eES [R(hSb )] + b ˆβ + β (b)M .
[R(hS )] ≤ E
E
S
S,ez
The other side of the inequality of the lemma can be shown following the same steps.
We can now prove a Lipschitz bound for the function Φ.

(9)

1The standard variable used for the stability coefﬁcient is β . To avoid the confusion with the β -mixing
coefﬁcient, we will use ˆβ instead.

4

Sb

(a)

z

b

b

Si

zi

b
(b)

Si,b

zi

b

b
(c)

S i
i,b

z

z

z

b

b

b

b
(d)

Figure 1: Illustration of the sequences derived from S that are considered in the proofs.

Lemma 3. Let S = (z1 , z2 , . . . , zm) and S i = (z ′
m) be two sequences drawn from a
2 , . . . , z ′
1 , z ′
β -mixing stationary process that differ only in point i ∈ [1, m], and let hS and hS i be the hypotheses
returned by a ˆβ -stable algorithm when trained on each of these samples. Then, for any i ∈ [1, m],
the following inequality holds:

M
m

.

(10)

|Φ(S ) − Φ(S i )| ≤ (b + 1)2 ˆβ + 2β (b)M +
Proof. To prove this inequality, we ﬁrst bound the difference of the empirical errors as in [3], then
the difference of the true errors. Bounding the difference of costs on agreeing points with ˆβ and the
one that disagrees with M yields
mXj=1
1
| bR(hS ) − bR(hS i )| =
m
m Xj 6=i
1
=
Now, applying Lemma 2 to both generalization error terms and using ˆβ -stability result in
)| + 2b ˆβ + 2β (b)
|R(hS ) − R(hS i )| ≤ | eR(hSb ) − eR(hS i
(12)
b
, ez )] + 2b ˆβ + 2β (b)M ≤ ˆβ + 2b ˆβ + 2β (b)M .
[c(hSb , ez ) − c(hS i
= E
b
ez
The lemma’s statement is obtained by combining inequalities 11 and 12.

1
i )| ≤ ˆβ +
m |c(hS , zi ) − c(hS i , z ′

|c(hS , zj ) − c(hS i , z ′
j )| +

|c(hS , zj ) − c(hS i , z ′
j )|

M
m

(11)

.

3.2 Bound on E[Φ]

As mentioned earlier, to make the bound useful, we also need to bound ES [Φ(S )]. This is done by
analyzing independent blocks using Lemma 1.
Lemma 4. Let hS be the hypothesis returned by a ˆβ -stable algorithm trained on a sample S drawn
from a stationary β -mixing distribution. Then, for all b ∈ [1, m], the following inequality holds:
[|Φ(S )|] ≤ (6b + 1) ˆβ + 3β (b)M .
(13)
E
S
Proof. We ﬁrst analyze the term ES [ bR(hS )]. Let Si be the sequence S with the b points before and
after point zi removed. Figure 1(b) illustrates this de ﬁnition. Si is thus made of three blocks. Let eSi
denote a similar set of three blocks each with the same distribution as the corresponding block in Si ,
but such that the three blocks are independent. In particular, the middle block reduced to one point
ezi is independent of the two others. By the ˆβ -stability of the algorithm,
S " 1
Si " 1
c(hSi , zi )# + 2b ˆβ .
c(hS , zi )# ≤ E
mXi=1
mXi=1
[ bR(hS )] = E
E
m
m
S
Applying Lemma 1 to the ﬁrst term of the right-hand side yield s
eSi " 1
, ezi )# + 2b ˆβ + 2β (b)M .
mXi=1
[ bR(hS )] ≤ E
E
m
S

c(h eSi

(15)

(14)

5

Combining the independent block sequences associated to bR(hS ) and R(hS ) will help us prove the
lemma in a way similar to the i.i.d. case treated in [3]. Let Sb be de ﬁned as in the proof of Lemma 2.
To deal with independent block sequences de ﬁned with respec t to the same hypothesis, we will
consider the sequence Si,b = Si ∩ Sb , which is illustrated by Figure 1(c). This can result in as many
as four blocks. As before, we will consider a sequence eSi,b with a similar set of blocks each with
the same distribution as the corresponding blocks in Si,b , but such that the blocks are independent.
Since three blocks of at most b points are removed from each hypothesis, by the ˆβ -stability of the
learning algorithm, the following holds:
c(hS , zi ) − c(hS , z )#
S,z " 1
mXi=1
[ bR(hS ) − R(hS )] = E
[Φ(S )] = E
E
m
S
S
Si,b ,z " 1
c(hSi,b , zi ) − c(hSi,b , z )# + 6b ˆβ .
mXi=1
E
≤
m
Now, the application of Lemma 1 to the difference of two cost functions also bounded by M as in
the right-hand side leads to
eSi,b ,ez " 1
, ez )# + 6b ˆβ + 3β (b)M .
mXi=1
(18)
[Φ(S )] ≤ E
E
c(h eSi,b
, ezi ) − c(h eSi,b
m
S
Since ez and ezi are independent and the distribution is stationary, they have the same distribution and
we can replace ezi with ez in the empirical cost and write
, ez )# + 6b ˆβ + 3β (b)M ≤ ˆβ + 6b ˆβ + 3β (b)M , (19)
eSi,b ,ez " 1
mXi=1
c(h eS i
, ez ) − c(h eSi,b
E
[Φ(S )] ≤ E
m
S
i,b
i,b is the sequence derived from eSi,b by replacing ezi with ez . The last inequality holds by
where eS i
ˆβ -stability of the learning algorithm. The other side of the inequality in the statement of the lemma
can be shown following the same steps.
3.3 Main Results

(16)

(17)

This section presents several theorems that constitute the main results of this paper. We will use the
following theorem which extends McDiarmid’s inequality to ϕ-mixing distributions.
Theorem 1 (Kontorovich and Ramanan [7], Thm. 1.1). Let Φ : Z m → R be a function de ﬁned over
a countable space Z . If Φ is l-Lipschitz with respect to the Hamming metric for some l > 0, then
the following holds for all ǫ > 0:
[|Φ(Z ) − E[Φ(Z )]| > ǫ] ≤ 2 exp (cid:18)
∞ (cid:19) ,
−ǫ2
Pr
2ml2 ||∆m ||2
Z
mXk=1
ϕ(k).
where ||∆m ||∞ ≤ 1 + 2
Theorem 2 (General Non-i.i.d. Stability Bound). Let hS denote the hypothesis returned by a ˆβ -
stable algorithm trained on a sample S drawn from a ϕ-mixing stationary distribution and let c be
a measurable non-negative cost function upper bounded by M > 0, then for any b ∈ [0, m] and any
ǫ > 0, the following generalization bound holds
2m((b + 1)2 ˆβ + 2M ϕ(b) + M/m)2 ! .
S h˛˛˛R(hS ) − bR(hS )˛˛˛ > ǫ + (6b + 1) ˆβ + 6M ϕ(b)i ≤ 2 exp  
−ǫ2 (1 + 2 Pm
i=1 ϕ(i))−2
Pr
Proof. The theorem follows directly the application of Lemma 3 and Lemma 4 to Theorem 1.
The theorem gives a general stability bound for ϕ-mixing stationary sequences.
If we further
assume that the sequence is algebraically ϕ-mixing, that is for all k , ϕ(k) = ϕ0 k−r for some r > 1,
then we can solve for the value of b to optimize the bound.

(20)

6

Theorem 3 (Non-i.i.d. Stability Bound for Algebraically Mixing Sequences). Let hS denote the
hypothesis returned by a ˆβ -stable algorithm trained on a sample S drawn from an algebraically
ϕ-mixing stationary distribution, ϕ(k) = ϕ0 k−r with r > 1 and let c be a measurable non-negative
cost function upper bounded by M > 0, then for any ǫ > 0, the following generalization bound
holds
2m(2 ˆβ + (r + 1)2M ϕ(b) + M/m)2 ! ,
S h˛˛˛R(hS ) − bR(hS )˛˛˛ > ǫ + ˆβ + (r + 1)6M ϕ(b)i ≤ 2 exp  
−ǫ2 (4 + 2/(r − 1))−2
Pr
where ϕ(b) = ϕ0 (cid:16) ˆβ
rϕ0M (cid:17)r/(r+1)
.
Proof. For an algebraically mixing sequence, the value of b minimizing the bound of Theorem 2
ˆβ b = rM ϕ(b), which gives b = (cid:16) ˆβ
and ϕ(b) = ϕ0 (cid:16) ˆβ
rϕ0M (cid:17)−1/(r+1)
rϕ0M (cid:17)r/(r+1)
satisﬁes
following term can be bounded as
1 − r (cid:19) .
i−r di(cid:19) = 1 + 2 (cid:18)1 +
i−r ≤ 1 + 2 (cid:18)1 + Z m
mXi=1
mXi=1
m1−r − 1
ϕ(i) = 1 + 2
1 + 2
1
For r > 1, the exponent of m is negative, and so we can bound this last term by 3 + 2/(r − 1).
Plugging in this value and the minimizing value of b in the bound of Theorem 2 yields the statement
of the theorem.

. The

(21)

In the case of a zero mixing coefﬁcient ( ϕ = 0 and b = 0), the bounds of Theorem 2 and Theorem 3
coincide with the i.i.d. stability bound of [3]. In order for the right-hand side of these bounds to
converge, we must have ˆβ = o(1/√m) and ϕ(b) = o(1/√m). For several general classes of
algorithms, ˆβ ≤ O(1/m) [3]. In the case of algebraically mixing sequences with r > 1 assumed in
Theorem 3, ˆβ ≤ O(1/m) implies ϕ(b) = ϕ0 ( ˆβ /(rϕ0M ))(r/(r+1)) < O(1/√m). The next section
illustrates the application of Theorem 3 to several general classes of algorithms.

4 Application

(22)

We now present the application of our stability bounds to several algorithms in the case of an al-
gebraically mixing sequence. Our bound applies to all algorithms based on the minimization of a
regularized objective function based on the norm k · kK in a reproducing kernel Hilbert space, where
K is a positive de ﬁnite symmetric kernel:
mXi=1
1
c(h, zi ) + λkhk2
argmin
K ,
m
h∈H
under some general conditions, since these algorithms are stable with ˆβ ≤ O(1/m) [3]. Two speciﬁc
instances of these algorithms are SVR, for which the cost function is based on the ǫ-insensitive cost:
c(h, z ) = |h(x) − y |ǫ = (cid:26)0
if |h(x) − y | ≤ ǫ,
otherwise,
|h(x) − y | − ǫ
and Kernel Ridge Regression [13], for which c(h, z ) = (h(z ) − y )2 .
Corollary 1. Assume a bounded output Y = [0, B ], for some B > 0, and assume that K (x, x) ≤ κ
for all x for some κ > 0. Let hS denote the hypothesis returned by the algorithm when trained on
a sample S drawn from an algebraically ϕ-mixing stationary distribution. Then, with probability at
least 1 − δ , the following generalization bounds hold for
a. Support vector regression (SVR):
λ ! r 2 ln(1/δ)
+ 5   3κ2
+ κr B
R(hS ) ≤ bR(hS ) +
λ
m
b. Kernel Ridge Regression (KRR):
λ ! r 2 ln(1/δ)
+ 5   12κ2B 2
+ κr B
26κ2B 2
R(hS ) ≤ bR(hS ) +
λm
λ
m

13κ2
2λm

(23)

(24)

(25)

;

.

7

Proof. It has been shown in [3] that for SVR ˆβ ≤ κ2/(2λm) and that M < κpB/λ and for KRR,
ˆβ ≤ 2κ2B 2/(λm) and M < κpB/λ. Plugging in these values in the bound of Theorem 3 and
using the lower bound on r, r > 1, yield the statement of the corollary.
These bounds give, to the best of our knowledge, the ﬁrst stab ility-based generalization bounds for
SVR and KRR in a non-i.i.d. scenario. Similar bounds can be obtained for other families of algo-
rithms such as maximum entropy discrimination, which can be shown to have comparable stability
properties [3]. Our bounds have the same convergence behavior as those derived by [3] in the i.i.d.
case. In fact, they differ only by some constants. As in the i.i.d. case, they are non-trivial when the
condition λ ≫ 1/√m on the regularization parameter holds for all large values of m. It would be
interesting to give a quantitative comparison of our bounds and the generalization bounds of [10]
based on covering numbers for mixing stationary distributions, in the scenario where test points
are independent of the training sample. In general, because the bounds of [10] are not algorithm-
dependent, one can expect tighter bounds using stability, provided that a tight bound is given on
the stability coefﬁcient. The comparison also depends on ho w fast the covering number grows with
the sample size and trade-off parameters such as λ. For a ﬁxed λ, the asymptotic behavior of our
stability bounds for SVR and KRR is tight.

5 Conclusion

Our stability bounds for mixing stationary sequences apply to large classes of algorithms, including
SVR and KRR, extending to weakly dependent observations existing bounds in the i.i.d. case. Since
they are algorithm-speciﬁc, these bounds can often be tight er than other generalization bounds.
Weaker notions of stability might help further improve or re ﬁne them.

References
[1] S. N. Bernstein. Sur l’extension du th ´eor `eme limite du calcul des probabilit ´es aux sommes de quantit ´es
d ´ependantes. Math. Ann., 97:1–59, 1927.
[2] O. Bousquet and A. Elisseeff. Algorithmic stability and generalization performance. In NIPS 2000, 2001.
[3] O. Bousquet and A. Elisseeff. Stability and generalization. JMLR, 2:499–526, 2002.
[4] L. Devroye and T. Wagner. Distribution-free performance bounds for potential function rules. In Infor-
mation Theory, IEEE Transactions on, volume 25, pages 601–604, 1979.
[5] P. Doukhan. Mixing: Properties and Examples. Springer-Verlag, 1994.
[6] M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation.
In Computational Learing Theory, pages 152–162, 1997.
[7] L. Kontorovich and K. Ramanan. Concentration inequalities for dependent random variables via the
martingale method, 2006.
[8] A. Lozano, S. Kulkarni, and R. Schapire. Convergence and consistency of regularized boosting algorithms
with stationary β -mixing observations. In NIPS, 2006.
[9] D. Mattera and S. Haykin. Support vector machines for dynamic reconstruction of a chaotic system. In
Advances in kernel methods: support vector learning, pages 211–241. MIT Press, Cambridge, MA, 1999.
[10] R. Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning,
39(1):5–34, 2000.
[11] D. Modha and E. Masry. On the consistency in nonparametric estimation under mixing assumptions.
IEEE Transactions of Information Theory, 44:117–133, 1998.
[12] K.-R. M ¨uller, A. Smola, G. R ¨atsch, B. Sch ¨olkopf, J. K., and V. Vapnik. Predicting time series with support
vector machines. In Proceedings of ICANN’97, LNCS, pages 999–1004. Springer, 1997.
[13] C. Saunders, A. Gammerman, and V. Vovk. Ridge Regression Learning Algorithm in Dual Variables. In
Proceedings of the ICML ’98, pages 515–521. Morgan Kaufmann Publishers Inc., 1998.
[14] B. Sch ¨olkopf and A. Smola. Learning with Kernels. MIT Press: Cambridge, MA, 2002.
[15] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.
[16] M. Vidyasagar. Learning and Generalization: With Applications to Neural Networks. Springer, 2003.
[17] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of
Probability, 22(1):94–116, Jan. 1994.

8


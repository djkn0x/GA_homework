Self-Calibration of a Pair of Webcams for Stereo

Vision

Rebecca Illowsky and Landry Huet
Stanford University
CS 221 and CS 229 nal pro ject

December 14, 2007

1 Motivations

The idea of the pro ject was originally given by Steve Gould, a PhD student at
the AI laboratory. Researchers often waste a lot of time calibrating the cameras
they are using as sensors for their robots. There are two kinds of parameters
for cameras. The rst ones are internal to the cameras. They account for
the properties of their lenses and for their defects. Using these parameters
helps getting rid of both linear and quadratic deformations of the pictures and
therefore enhance the pictures for use with computer vision algorithms. They
are called the intrinsic parameters because they depend only on the cameras
themselves and not on their environment. They only need to be computed
once. The second kind of parameters is the class of extrinsic parameters. They
relate the cameras to the outside world. Usually the only interesting extrinsic
parameters are the positions and orientations of the cameras. In the case of a
pair of cameras, the extrinsic parameters are essential for stereoscopic vision.
However, these parameters are sensitive to the outside world. If an engineer is
tuning a robot, it means that he will have to start the calibration of extrinsic
parameters all over again every time the cameras of his robot are moved.
A common technique for camera calibration requires taking multiple pictures
of a checkerboard under various angles, and then feeding them to the matlab
camera calibration toolbox. Although the process is eective, it is very tedious
and takes at least half an hour for a trained operator. This is not what you want
to focus on when you are working on a robot, but it might use a lot of your time
and energy nonetheless if you have to remove the video sensors of the robot
and put them back often. Our aim was therefore to design a technique that
would make the robot recalibrate the extrinsic parameters of its video sensors
by itsself, just by looking at the outside world.

2 Theoretical background

2.1 Scene reconstruction using two cameras
Let (C1 , C2 ) be a pair of cameras. Let us suppose that we know the translation
âˆ’â†’
(cid:104) X
(cid:105)
t and the rotation matrix R that link the image plane of camera C1
vector
to camera C2 , as well as the matrices A1 and A2 accounting for the intrinsic
âˆ’â†’
M =
parameters of each camera. Let
be the position vector of a physical
Y
(cid:105)
(cid:104) u2
(cid:105)
(cid:104) u1
Z
point M in space, relative to the frame of the rst camera. Let P1 and P2
(cid:101)P1 =
and (cid:101)P2 =
be the pro jections of M on the frames of cameras C1 and C2 respectively, and
be their homogeneous coordinates in their respective
v2
v1
frames. M lies on the lines linking the focal centers of the cameras C1 and C2
1
1
(cid:40)âˆ’â†’
1 (cid:101)P1
to P1 and P2 respectively, so there must exist two real numbers s1 and s2 such
that:
2 (cid:101)P2 âˆ’ âˆ’â†’
s1Aâˆ’1
M =
âˆ’â†’
M = s2R(cid:62)Aâˆ’1
t
âˆ’â†’
Except in degenerate cases there is always a unique solution to this system
M
of equations. This is not the case when doing numerical computation but
can be well approximated by plugging the result of the minimization problem

2

(cid:13)(cid:13)(cid:13)s1Aâˆ’1
(cid:13)(cid:13)(cid:13)2
1 (cid:101)P1 âˆ’ s2R(cid:62)Aâˆ’1
2 (cid:101)P2 +
âˆ’â†’
t to reconstruct the position of points in space.

âˆ’â†’
t

back into the system. Hence the need

min(s1 ,s2 )
for R and

2.2 General scheme to compute the extrinsic parameters

The rst step of our method is to nd corresponding points in various pairs of
images. It turns out that for our purpose it is not necessary that all the points
belong to the same pair of images. From these correspondences we can infer the
epipolar constraint in the form of a matrix. The epipolar constraint is the fact
that all the points of the image plane of C2 that could possibly be related to a
given point P1 of the image plane of C1 as coming from the same physical point
âˆ’â†’
in space lay on a straight line. The knowledge of this constraint then gives us
t between the origin of the image plane of camera C1
the translation vector
and that of C2 , as well as the rotation R of space that transforms the rst image
plane into the second image plane.
We started from pairs of corresponding points. We did not want to study
this part because there are already plenty of librairies that can do the job, and
because it should really be up to the user of our method to choose which one
he prefers. Let us now take a closer look at each step of our algorithm.

2.3 Getting the essential matrix

Using the same notations as in subsection (2.1), let us consider a point M that
pro jects onto P1 and P2 . Let us moreover consider F1 and F2 the respective
focal centers of cameras C1 and C2 .

P1 , P2 and M are coplanar. With respect to an arbitrary origin this con-
(cid:16)âˆ’â†’
(cid:17)
straint yields the equation :
t Ã— (RP2 )
P (cid:62)
= 0
This is bilinear in P1 and P2 and can be summed up by the matrix E = (cid:98)T R
1
where (cid:98)T is the matrix of the map âˆ’â†’u (cid:55)â†’ âˆ’â†’
t Ã— âˆ’â†’u . E is called the essential matrix
considering (cid:101)P1 = A1P1 and (cid:101)P2 = A2P2 , as dened in subsection (2.1). Writing
of the pair of views, and it sums up all the geometry of the system. In practice
the 3D coordinates of P1 and P2 are not directly known. We can x this by

3

MF1F2P1P2the same constraint now gives us(cid:101)P (cid:62)
1 (cid:98)T RAâˆ’1
2 (cid:101)P2 = 0
1 Aâˆ’(cid:62)
1 (cid:98)T RAâˆ’1
(1)
Since (cid:101)P1 and (cid:101)P2 can be directly measured, we can use (1) to compute F and
F = Aâˆ’(cid:62)
is called the fundamental matrix of the uncalibrated system.
2
then infer E , given a sucient number of reliable corresponding points between
our two frames. F and E are determined up to a scale factor, so since we want
to keep a track of the sense of displacement, if we know a non-null essential
matrix E , we will also be interested by âˆ’E .
Given E = (cid:98)T R we are looking for (cid:98)T and R. According to ([1]), if E = U Î£V (cid:62) is
2.4 From the essential matrix to the extrinsic parameters
(cid:17)
(cid:16) 0 1 0
values for (cid:98)T and R : (cid:16) (cid:98)T1 , R1
âˆ’1 0 0
the SVD of E and Rz =
then there are exactly two possible pairs of
(cid:17)
= (cid:0)U Rz Î£U (cid:62) , U R(cid:62)
z V (cid:62) (cid:1)
0 0 1
(cid:17)
(cid:16) (cid:98)T2 , R2
= (cid:0)U R(cid:62)
z Î£U (cid:62) , U Rz V (cid:62) (cid:1)
Since we get two essential matrices already, we now have four possible solutions,
used for the calibration. Then we can simply discard the pairs ( (cid:98)T , R) that re-
three of which are physically incorrect. It is possible to nd out by reconstruct-
ing the spatial positions linked to the pairs of corresponding points we already
construct one or more points behind the cameras (negative z -value). Note that
since the essential matrix is dened up to a scale factor, so is the translation
vector we nd. Theory tells us this is the best we can do without a measurement
of a recognized ob ject in the actual scene.

(2)

3 From theory to practice

3.1

Implementation

We programmed a C++ implementation of the method.

Point correspondences We used the Scale-Invariant Feature Transform (SIFT)
algorithm from the STAIR research group, which seems to give fairly acurate
results.

Essential matrix The Open Computer Vision library has a built-in function
that can compute the fundamental matrix of a pair of pictures given pairs of cor-
respondences. Our program gives it all the correspondences the rst step could
nd. The essential matrix is then obtained with two matrix multiplications.

Retrieving the extrinsic parameters We use the SVD decomposition func-
tion from the Open Computer Vision library. It is impossible to select the cor-
rect parameters just by eliminating the parameters that produce bad reconstruc-
tions, because incorrect correspondences may give inconsistent reconstructions

4

even with the right parameters. We therefore keep the solution that produces
the least bad results. We assume that this will be correct if we have many more
good correspondences than bad correspondences.

3.2 Results

In order to assess our method we conducted a series of 21 experiments1 . All
the experiments were run on the same setup of our pair of cameras. There were
four runs of experiments, three with an incresing threshold for SIFT (therefore
getting more and more correspondences of decreasing quality), the last one with
another technique (see [3]) to replace SIFT. For each run we used more and more
pictures and therefore got more and more points.

Fundamental matrix Since our method is supposed unbiaised, the mean of
the fundamental matrices we get can be considered the closest guess we have
about the true fondamental matrix of our system. Let us plot the distance to
this best guess against the SIFT threshold (1 stands for [3]):

The leftmost points refer to the experiments with the best correspondences
but the least points, whereas the rightmost points correspond to the experiments
with the most correspondences, of lesser quality. We can infer from this that
quantity and quality are about equally acceptable to nd a good fundamental
matrix.
The nal extrinsic parameters only depend on the fundamental matrix. It is
likely that better fundamental matrices will give better results in the end, so let
us choose a particularly successful experiment as far as the fundamental matrix
is concerned, and assess the quality of the other experiments by comparing
their parameters to that of our selection. Let us plot the distance between the
translation vectors2 and our supposedly good translation vector, against the
quality of the fundamental matrix (distance to the average):

1You can email rebeccai@stanford.edu for full results.
2The translation vectors are normalized so this quantity is to be considered with care,
all the more as the vectors we found are scattered. A similar problem would arise with the
rotations.

5

As we can see, even if the supposed quality of the fundamental matrix is
good, the nal result is not necessarily acurate. However, as soon as the fun-
damental matrix goes bad, the result for the translation vector is always very
bad. The bad conditionning of the matrices of intrinsic parameters is probably
accountable for the extreme sensitivity of the end result to inaccuracy of the
fundamental matrix.

Conclusion

The results of our work are promising but our implementation should be tuned
up before being used in real applications. Even though it is doubtful that self
calibration will be as acurate as the matlab calibration toolbox, our technique
can be very helpful when time is a bigger issue than precision, as is sometimes
the case in experimental robotics.
Finally our work could be improved in several ways:
â€¢ By improving accuracy, for instance with a better algorithm to nd cor-
respondences or to approximate the fundamental matrix of the system.
â€¢ The program could be made more iterative, so that the user could give the
algorithm correspondences until a certain degree of precision is reached.

Bibliography and acknowledgements

References

[1] Yi Ma, Stefano Soatto, Jana KoÂ²eckÃ¡, S. Shankar Sastry: An Invitation to
3-D Vision

[2] Quang-Tuan Luong, Olivier Faugeras: Self-calibration of a stereo rig from
unknown camera motions and point correspondences

[3] MetaMorph V 6.2r6 Universal Image Corp. Automated ob ject tracking with
"Track Ob jects" App

Best thanks to the STAIR research group and to Steve Gould in
particular.

6


TERM PROJECT REPORT (CS229: Machine Learning of Fall 2007)

1

Seeing Invisible Properties of Subsurface Oil and
Gas Reservoir through Extensive Uses of Machine
Learning Algorithms

Kwangwon Park

Abstract—Current geostatistical simulation methods al-
low generating multiple realizations that honor all available
data, such as hard and secondary data under certain ge-
ological scenarios (e.g. 3D training image-based models,
multi-Gaussian law, Boolean models). However, it is diﬃ-
cult to simulate large models that honor highly nonlinear
response functions (e.g. remote sensing data, geophysical
data or ﬂow in porous media data). The large CPU cost to
evaluate the response function imposes limitations on the
size of the model. This is particularly the case when one
desires to generate a sizeable set of realizations matching
the same data. The ob jective of this study is to generate
multiple realizations all of which honor all available data
(hard and secondary data and especially the non-linear re-
sponse function) under certain geological scenarios. First,
we generate a large ensemble of possible realizations de-
scribing the spatial uncertainty for given hard and sec-
ondary data. Any fast geostatistical simulation methods
can be used for generating these prior models. Secondly,
using multidimensional scaling, we map these models into
a low-dimensional metric space by deﬁning a distance be-
tween these prior models. We propose to make this distance
a function of the response function. Next, kernel principal
component analysis is applied to the coordinates of realiza-
tions mapped in this metric space to create a kernel, or fea-
ture, space with linear/Gaussian type variability between
the input realizations.
In this space, we can apply opti-
mization algorithms, such as gradient-based algorithms or
ensemble Kalman ﬁltering or gradual deformation method
to generate multiple realizations matching the same data.
A back-transformation, ﬁrst from the kernel space, then
to metric space and ﬁnally to the actual space of realiza-
tions allows then the generation of multiple geostatistical
models that match all data, hard, secondary and non-linear
response. We apply the proposed framework to generate a
realistic model which honors geologic information and dy-
namic (time-varying) response data. A ﬂow simulator is
used as the non-linear response function and may require
several hours of CPU-time per simulation. We show how
this technique applies to non-Gaussian (e.g. multiple-point
or Boolean) geostatistical models. We also demonstrate the
importance of using a distance function tailored to the par-
ticular response function used in creating a low-dimensional
parameterization of the ensemble of geostatistical model in
feature space.

Index Terms—CS229, Machine Learning, Optimization,
Reservoir

I. Introduction
IN order to estimate oil and gas reserves and maximize
their production,
it is essential to ﬁnd out unknown
reservoir properties, such as geologic subsurface structures.
Reservoir engineers often use a reservoir simulator that
provides future prediction about oil and gas ﬂow perfor-
mance, which is a kind of PDE (partial diﬀerential equa-

Kwangwon Park is with the Department of Energy Resources
Engineering, Stanford University, Stanford, CA 94305. Phone:
+1 650 725–1324, e-mail: wonii@stanford.edu

tion) solvers with various reservoir properties as input pa-
rameters. However, we can seldom have enough informa-
tion about these input parameters due to the limitation
to access very deep subsurface formation. Those parame-
ters which can not be measured directly are often obtained
by solving optimization problem because we can measure
output data only, such as production rates or pressure vari-
ation at production wells.
However, optimization problems for ascertaining reser-
voir parameters are containing a lot of tough limitations.
First, we usually have to determine 106 to 108 number of
input parameters, which means input space is extremely
high dimensional. Second, single evaluation of ob jective
function (diﬀerence between observed and simulated, in
other words misﬁt function) usually takes several hours to
several days. Moreover there are lots of local minima in
input space due to highly nonlinear and extremely large
system of partial diﬀerential equations in reservoir simula-
tion. Third, our optimization is limited by very few infor-
mation about reservoir, as we have very few wells, which
are the only way for us to measure the reservoir input and
output data. Fourth, since all those parameters are related
with geologic subsurface structures, the input parameters
should be geologically realistic.
In the process of this tough optimization problem, var-
ious machine learning algorithms can be introduced ex-
tensively. The problem, theory, methodology, and simple
results will be followed.

II. Problem Statements
THE ultimate ob jective is to solve an inverse problem,
which can be represented by Equation 1.
x = f −1
(1)
α (y)

where, x is the unknown input parameters (reservoir prop-
erties) and y is the output (solution variables). α repre-
sents the known parameters. f means a PDE as Equa-
tion 2.

(2)

(αy) − ∇ · (αx∇y) + α = 0

∂
∂ t
where, t represents time. Equation 2 can be solved by
various numerical methods with the initial and boundary
conditions that are controlled by α. Equation 1 and 2 have
several interesting aspects:
Nonlinearity: Since α is a function of y , Equation 2 is
a highly nonlinear equation, which also increases the
simulation. Besides, although we know all the y ’s in

2

TERM PROJECT REPORT (CS229: Machine Learning of Fall 2007)

the domain (actually rarely possible in ﬁeld), it is very
diﬃcult to solve the inverse problem (Equation 1).
Large number of parameters: Usually we have to deter-
mine several unknown properties at each node, which
means the dimension of x is 106 to 108 . In other words,
we have to solve a highly nonlinear inverse partial dif-
ference equation in extremely high dimension.
It is
needless to say that a lot of local minima exist in the
space.
Large number of nodes: In order to get the desired and
meaningful resolution from Equation 2, we usually
have to make 105 to 107 nodes at each of which the
output y has to be calculated in 3-dimensional space.
Large number of nodes result in dramatic increase in
forward simulation time (hours to days per single sim-
ulation).
Limited output data: Actually, we can obtain very lim-
ited information of y , as we have to install a kind
of measurement device in the deep subsurface, which
costs a lot. In a reservoir, the measurement device is
equivalent to wells (injectors or producers) and drilling
one well costs usually millions to tens of millions of
dollars. In many cases, we know a few (tens to thou-
sands) measurements of y among 106 to 108 values.
Lots of constraints: In this optimization problem, we
have a lot of constraints on x.
In a reservoir, x is
reservoir properties, such as permeability or poros-
ity, which are highly dependent upon subsurface rock
properties. Therefore, the main constraint is that x
should be geologically realistic.
With all these challenges, it is often impossible to solve
this optimization problem using conventional optimiza-
tion techniques themselves. Therefore, somewhat diﬀerent
methods will be applied and in each step the appropriate
machine learning algorithms are going to be applied eﬀec-
tively.

III. Distances
THE distance is a measure of dissimilarity between two
realizations. Simply, we can evaluate the dissimilar-
ity between realizations xa and xb (discretized into Ngb
gridblocks) through the Minkowski model, such as Euclid-
ian space or City-Block (Manhattan) space. Although the
Minkowski model is easy to calculate, it may not be well-
correlated with dynamic data because the forward simu-
lated dynamic data may change dramatically by perturb-
ing a small portion of the realization. Figure 1 depicts
the correlation between Euclidian or Manhattan distance
and the dissimilarity between dynamic data (the diﬀerence
between watercut curves). It turns out that both Euclid-
ian and Manhattan distances are not correlated with the
dynamic data.
In order to optimize an inverse solution eﬃciently in the
distance space, it is necessary that the dynamic data are
spatially correlated in the space. For this, various distances
may be utilized. If we need Euclidian distance (actually
we sometimes have to deal with Euclidian distance for sat-
isfying the metric axioms or the positive deﬁniteness of the

(a) Euclidian distance

(b) Manhattan distance

Fig. 1. The distance and dissimilarity of dynamic data (watercut).
On the y-axis is the diﬀerence in watercut between any two realiza-
tions. On the x-axis the distance between any two realizations.

distance matrix, for example when employing the RBF ker-
nels), then any similarity distance can be easily converted
to Euclidian distance by means of multidimensional scaling
(MDS) or principal coordinate analysis.

IV. Parameterization
PRIOR to the parameterization of the geological model
space, we start from an ensemble of realizations, xj ,
(j = 1, ..., NR , if we generate NR realizations). x could
represent a facies porosity or permeability model or any
combination of those. The initial ensemble can be gen-
erated by various geostatistical algorithms honoring the
geologic information and conditioning to the static data
(hard and soft data). For simplicity, deﬁne the matrix for
the ensemble X as

[X]:,j = xj

,

(3)

where, [X]i,j means (i, j ) element of matrix X and (:, j ) is
j -th column of matrix X. The covariance of the ensembles
NR(cid:88)
is calculated numerically by Equation 4.
1
1
NR
NR
j=1

j =
xj xT

C =

XXT

(4)

When we perform an eigenvalue decomposition on the
covariance (Equation 5), then a new realization can be
obtained by Equation 6 (Karhunen-Loeve expansion).

Cv = λv
xnew = EΛ1/2ξnew

(5)
(6)

where, v and λ is the eigenvector and eigenvalue, respec-
tively. E is a matrix each column of which is the eigenvec-
tor of the covariance. Λ is a diagonal matrix each diag-
onal element of which is the eigenvalue of the covariance.
ξnew is the parameter vector for the realization xnew . The
parameter ξ is Gaussian random vector and the size is de-
termined by how many eigenvalues are chosen to retain.
We do not have to use all the nonzero NR eigenvalues;
typically a few large eigenvalues are retained. By Equa-
tion 6, we can generate many models based on the same
covariance.
In order to consider higher-order moments or spatial cor-
relation beyond the point-by-point covariance, the feature
expansions of the realizations can be introduced. Let φ be

PARK: SPECIFICATION FOR COMMON IEEE STYLES

3

=

[ai ]j

the feature map from realizations space R to feature space
F (Equations 7 and 8).

(7)

φ : R → F
x (cid:55)→ φ := φ(x)
where φ is the feature expansion of realization. With the
feature expansions of the ensemble φ(X) (deﬁned by Equa-
tion 9), the new feature expansion can be generated in the
same manner above (Equation 11). The covariance of the
feature expansions (φ(xj )) of the ensemble is calculated by
Equation 10.

(8)

[φ(X)]:,j = φ(xj )

(9)

φ(xj )φ(xj )T =

1
NR

φ(X)φ(X)T

(10)

NR(cid:88)
j=1

C =

1
NR

φ(xnew ) = EΛ1/2ξnew

(11)

However, since the feature expansion is often very high-
dimensional and sometimes inﬁnite-dimensional, the eigen-
value decomposition of the covariance matrix is almost im-
possible. The kernel trick makes it possible to obtain the
exactly equivalent solution to the eigenvalue decomposi-
tion of the covariance. If we deﬁne a kernel function as a
dot product of two feature expansions (Equation 12), the
kernel function can be evaluated without representing the
high-dimensional feature expansions explicitly. Then, the
kernel matrix (Equation 13) can be calculated eﬃciently.

k(xi , xj ) : = < φ(xi ), φ(xj ) >
K : = φ(X)T φ(X)

(12)
(13)

where, [K]i,j is k(xi , xj ) and < > means the dot product.
The main idea of the kernel trick is to assume that the
new feature expansion is a linear combination of the feature
expansions of the ensemble and represent all the elements
in the equations as dot products of two feature expansions.
Actually, Equation 11 means that a new feature expan-
sion is a linear combination of the eigenvectors. Since the
eigenvectors lie in the span of the feature expansions of the
ensemble (Equation 14), it is true that the new feature ex-
pansion is a linear combination of the feature expansions
of the ensemble. It turns out that the coeﬃcient ai is the
i-th eigenvector of the kernel matrix and NRλi is the i-th
eigenvalue of the kernel matrix (Equation 15).
NR(cid:88)
j=1
Ka = NRλa

[ai ]j φ(xj )

vi =

(14)

(15)

Therefore, we can acquire the new feature expansion
without the costly eigenvalue decomposition of the NR ×

φ(xj ) =

[b]j φ(xj ) = φ(X)b(16)

[ξ ]iλ1/2
NR(cid:88)
i
j=1

xnew = arg min
xnew
= arg min
xnew

NR(cid:88)
[ai ]j φ(xj )
j=1

NR covariance matrix (Equation 16).
NR(cid:88)
NR(cid:88)
(cid:40) NR(cid:88)
(cid:41)
[ξ ]iλ1/2
i vi =
φ(z) = EΛ1/2ξ =
NR(cid:88)
i=1
i=1
[ξ ]iλ1/2
i
NR(cid:80)
i=1
j=1
[ξ ]iλ1/2
[ai ]j
where, [b]j =
i
i=1
Once the new feature expansion is acquired, the new re-
alization can be calculated from the new feature expansion
(xnew = φ−1 (ξnew )). Since φ−1 cannot often be calculated
explicitly, we have to calculate the new model such that
(cid:8)φ(xnew )T φ(xnew ) − 2φ(xnew )T φ(X)b + bT Kb(cid:9) .
(cid:107)φ(xnew ) − φ(X)b(cid:107)
(17)
This is another optimization problem which is called the
pre-image problem. This optimization problem can be
solved by the ﬁxed point iteration method ((Sch¨olkopf and
(cid:8)φ(xnew )T φ(xnew ) − 2φ(xnew )T φ(X)b + bT Kb(cid:9) = 0
Smola, 2002)). We ﬁnd xnew such that
∇xnew
(18)
by iterations (Equation 19).
NR(cid:80)
[b]j k (cid:48) (xj , xnew )xj
NR(cid:80)
j=1
[b]j k (cid:48) (xj , xnew )
j=1
where k (cid:48) means the diﬀerential of k . Since we have the
kernel functions not the explicit feature expansion, these
iterations can be done eﬃciently. In conclusion, the new
realization can be obtained by a nonlinear combination of
the ensemble members. Note that the nonlinear weight
sum to unity.
We can use various types of kernels, but the kernel ma-
trix should be positive deﬁnite (Mercer theorem). Some
(cid:16)− (cid:107)x−z(cid:107)2
(cid:17)
widely used kernels are:
• Polynomial: k(x, z) = (< x, z > +c)d
• Gaussian: k(x, z) = exp
2σ2
• Sigmoid: k(x, z) = tanh(κ < x, z > +ϑ)
Figure 2 shows the correlation between the polynomial
kernel and the dissimilarity between dynamic data (the
diﬀerence of watercut curves) of any two realizations. It
turns out that polynomial kernels are not correlated with
the dynamic data in this case. Since a kernel is the measure
of similarity, it is desirable for the kernel to be negatively
correlated with the dissimilarity between dynamic data.
Likewise the Gaussian kernel, the kernel that is based on
the Euclidian distance is called RBF kernel. Even though
we know the Euclidian distance only, the RBF kernel func-
tion can be evaluated. Also, although the dissimilarity dis-
tance is not a Euclidian distance, we can map the ensemble

xnew =

(19)

4

into the metric space by multidimensional scaling. Once
the Euclidian distance in the metric space is well corre-
lated with the dissimilarity distance, we can evaluate the
kernel function by replacing the distance to the Euclidian
distance in the metric space. Figure 3 depicts the correla-
tion between the Gaussian kernel and the dissimilarity be-
tween dynamic data of any two realizations. The Hausdorﬀ
distance (Suzuki and Caers, 2006) and connectivity-based
distance (Park and Caers 2007) after MDS (ten eigenval-
ues are retained) are used for the Euclidian distance in
Gaussian kernel. The connectivity-based distance shows,
to some extent, negative correlation with the dissimilarity
of dynamic data.

(b) k(x, z) = (< x, z > +1)3
(a) k(x, z) =< x, z >
Fig. 2. The polynomial kernels and dissimilarity of dynamic data
(watercut). On the y-axis is the diﬀerence in watercut between any
two realizations. On the x-axis the kernel between any two realiza-
tions.

(a) Hausdorﬀ distance

(b) Connectivity distance

Fig. 3. The RBF kernels and dissimilarity of dynamic data (water-
cut). On the y-axis is the diﬀerence in watercut between any two
realizations. On the x-axis the kernel between any two realizations.

V. Further parameterization of ξ
THE parameterized feature expansions of realizations
(ξ ’s) are Gaussian random vectors, so the optimiza-
tion can be accomplished by the sequential calibration with
gradual deformation.
Hu (2000) developed the GDM for performing history
matching on stochastic reservoir models. It consists in it-
eratively updating a combination of independent realiza-
tions of a random function until both static and dynamic
data are matched.
Consider a Gaussian random vector ξi with zero mean
and unit variance. The GDM consists in writing a new
random vector, ξnew as a linear combination of N inde-
N(cid:88)
pendent random vectors (Equation 20).
i=1

ξnew =

(20)

ρiξi

TERM PROJECT REPORT (CS229: Machine Learning of Fall 2007)
N(cid:80)
with
i = 1 (Hu and Blanc, 1998).
ρ2
i=1
Considering for instance the gradual deformation of 2
random vectors ξ1 and ξ2 , we have a single gradual defor-
2(cid:88)
mation parameter as Equation 21.
i=1
For the calibration of a stochastic model, the follow-
ing iterative optimization procedure is often used (Equa-
tion 22).

ρiξi = ξ1 cos(θ) + ξ2 sin(θ)

ξnew (θ) =

(21)

(22)
ξn (θ) = ξn−1 cos θ + ζn sin θ
where ξn−1 is the optimized parameterization vector at it-
eration n − 1, and ζn are a series of independent Gaussian
random vectors. Then by minimizing the ob jective func-
tion with respect to parameter θ , we get a new parameteri-
zation vector ξn (θopt ) that improves (or at least maintains)
the calibration to the nonlinear data (Hu, 2000).

VI. The proposed workflow
BASED on the theories that are stated above, the pro-
posed procedure for conditioning ensemble to dynamic
data under realistic geologic scenarios is as follows (Fig-
ure 4):
1. Generate the initial ensemble (realization space)
First we generate an initial ensemble. The initial
ensemble should include models that are honoring ge-
ologic information and are conditioned to all available
static data, that is, hard and soft data. To do this, we
can choose a proper geostatistical algorithms, such as
SGSIM, SISIM, DSSIM, and so on as variogram-based
methods and SNESIM and FILTERSIM as multiple-
point (MP) simulation methods. Generating the en-
semble, we may have to consider the uncertainty in
the static data. For example, if our geologic informa-
tion is uncertain, we can use multiple training images
for MP simulations.
2. Calculate the dissimilarity distances (distance space
to metric space)
From the initial ensemble, we calculate the dis-
similarity distances and construct a distance matrix.
At this step, it is important for the distances to be
correlated with the dynamic data that we want to
condition. If needed, we can apply multidimensional
scaling to lower the dimension and get Euclidian dis-
tances, which make it possible to use RBF kernels.
3. Calculate the kernel matrix (to feature space)
Based on the Euclidian distances, we calculate
the kernel matrix. RBF kernel matrix would be eas-
ily calculated but a proper kernel should be chosen
cautiously.
4. Parameterize the initial ensemble (to parameteriza-
tion space)
After obtaining the eigenvalues and eigenvectors
of the kernel matrix, each member of the initial en-
semble is parameterized to relatively short Gaussian
random variables.

PARK: SPECIFICATION FOR COMMON IEEE STYLES

5

5. Optimize the parameters (in parameterization space)
The optimization process would be done on the
parameterization of the initial ensemble. Since the pa-
rameters are low-dimensional Gaussian random vari-
ables, we may apply various optimization methods,
such as gradient-based methods using the sensitivity
coeﬃcients, probability perturbation method, gradual
deformation method, ensemble Kalman ﬁlter, and so
on. Since we already have an ensemble, EnKF would
be applied eﬀectively and provide multiple models
which show the same dynamic data response. Ad-
ditionally, the optimization might be accelerated by
an eﬃcient selection method through kernel PCA.
6. Solve the pre-image problems (to realization space)
Now, the optimized parameters are converted into
model state vectors. Using a proper minimization al-
gorithm, such as a ﬁxed-point iteration, we solve the
pre-image problems for all the optimized parameters.
7. Analyze multiple models
Finally, we obtain multiple models which satisfy
all available data and geologic scenarios. We can use
these multiple models in a variety of purposes. Since
we generate an initial ensemble reﬂecting the uncer-
tainty after conditioning to static data acquired so far,
these ﬁnal multiple models indicate the uncertainty (a
posteriori ) after conditioning to static and dynamic
data. The multiple models may suggest which type
of data should be acquired additionally, or a value of
information question can be posed.

Fig. 4. The proposed workﬂow.

VII. Evaluations
WE applied the proposed work ﬂow to a simple case.
Within 300 iterations, we could ﬁnd a realization
which are conditioned to the dynamic data (Figure 6). The
ﬁnal realization indicates similar channel locations and di-
rections (Figure 5).

(a) reference
Fig. 5. The reference and ﬁnal realizations.

(b) ﬁnal

(a) Decrease of the
mismatch of the dynamic
data with iterations.

(b) Dynamic data of the
initial (green), ﬁnal (red),
and reference (blue)
realizations.
Fig. 6. Decrease of the mismatch of the dynamic data with iterations
and the dynamic data matching.

VIII. Conclusion
The ob jective of this research is to generate multiple
models which are satisfying all available static and dy-
namic data. For the ob jective, multiple optimization meth-
ods will be combined and applied in kernel feature space
based on a dissimilarity distance. The proposed method
will be veriﬁed in both theoretical and experimental ways.
This research has potential for applications in various areas
of reservoir modeling and real-time production optimiza-
tion.

Acknowledgements
The author would like to thank prof. Andrew Ng and
ﬁve TA’s for great lectures and helps.

References

[1] Deutsch, C.V. and Journel, A.G.: Geostatistical Software Li-
brary and User’s Guide, Oxford University Press, NY (1998).
[2] Hu, L.Y.: Gradual Deformaion and Iterative Calibration
of Gaussian-Related Stochastic Models, Mathematical Geology
(2000) 32, 1.
[3] Park, K. and Caers, J.: History Matching in Low-Dimensional
Connectivity Vector Space, proceedings of EAGE Petroleum
Geostatistics 2007 Conference, Cascais, Portugal.
[4] Sch¨olkopf, B. and Smola, A.J.: Learning with Kernels: Support
Vector Machines, Regularization, Optimization, and Beyond,
The MIT Press, Cambridge, MA (2002).
[5] Suzuki, S. and Caers, J.: History Matching with an Uncertain
Geological Scenario, paper SPE102154 presented at the 2006
SPE ATCE, TX.


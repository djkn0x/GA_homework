Classifying Press Releases and Company Relationships
Based on Stock Performance∗

Mike Mintz
Stanford University
mintz@stanford.edu

Ruka Sakurai
Stanford University
ruka.sakurai@gmail.com

Nick Briggs
Stanford University
njbriggs@stanford.edu

Abstract

We classify press releases as “good” or ‘bad’
news for 3 companies based on whether the
stock increases n minutes after publication.
We tried different classiﬁers (Multinomial
Naive Bayes, Regularized SVM, and Near-
est Neighbors) and various feature representa-
tions (such as the TF-IDF of the words in the
document). We do a few percent better than
majority baseline with our best setup: near-
est neighbor classiﬁer with a cosine similar-
ity metric, binary word-in-doc features, and
n = 15 minutes. Stemming words to base
forms helped signiﬁcantly. Using the cluster-
ing to predict the stock price of related com-
panies did not work. Overall a lack of sufﬁ-
cient press release data was the limiting factor
of our research. Various suggestions for im-
provement are discussed in the conclusion.

1 Introduction
Press releases are usually the ﬁrst time news about
companies is made available to the public. We there-
fore hypothesized that the contents of the press re-
leases are a majority indicator of the short term value
of a company’s stock. A machine learning approach
would be able to analyze these press releases and
make predictions about the stock price much faster
than a human analyst could. Such a tool could aid
a trader in making quicker decisions based on press
release information, and also help classify press re-
leases as “good” or “bad” news for a particular com-
pany.
∗Thank you Dan Ramage for the great advice for text clas-
siﬁcation!

We compiled a large corpus of press releases for
publicly traded companies, as well as a corpus of
stock price changes for these companies, with high
time precision. We created a classiﬁer for these ar-
ticles, and trained it using the short-term percent
change in stock price for the company. Then, given a
press release when it is announced, our classiﬁer at-
tempts to predict whether the stock price of its com-
pany will increase or decrease in the short term.

1.1 Prior Work
Previous work in this area was performed by Mitter-
mayer [4], who designed a system to analyze press
releases in real time and make stock transactions de-
cisions based on them. He used an SVM and re-
ported that the SVM had trouble marking press re-
leases as “good news” or “bad news”.

2 Data Collection

2.1 Stock Data
Through the Graduate School of Business Library,
we collected stock data from the New York Stock
Exchange Trade and Quote Database (NYSE TAQ)
provided by University of Chicago’s Center for Re-
search in Securities Prices (CRSP). We focused on
intraday data about all companies in the NYSE. For
the intraday data we retrieved the price, volume
and time (to the second) of all trades that occurred.
Typically there are multiple transactions that occur
within a minute. This provides us with stock val-
ues that are highly precise with respect to time. This
data is also used for clustering companies with sim-
ilar market ﬂuctuations.

A month’s worth of data for all companies in the
NYSE comprises more than 10 gigabytes of infor-
mation. Therefore the challenge is to store this data
in an efﬁcient way without losing the precision that
is needed in our analysis. Since press release times
are recorded with precision to the nearest minute, we
store the stock value at each minute. The stock value
at a speciﬁc minute is calculated by the weighted
average of the trades that occurred in that minute,
where the weights are the volumes of the transac-
tions. The value of the stock at times without data
from the NYSE TAQ are computed by taking the
value from the nearest minute that has price infor-
mation.

2.2 Article Data
We retrieved press releases and news articles from
the Factiva system through the Graduate School of
Business Library. We focused on press releases from
2006 and 2007, since there was a lot less data avail-
able for other years. To simplify the problem, we
limited ourselves to classifying press releases from
three large companies: Boeing, McDonald’s, and
Verizon.1
The press releases were available as XML ﬁles,
and contained information about the title, date, para-
graph structure, and other metadata that Factiva used
for indexing. We simply stored the date and cal-
culated a set of all words contained in the article.
All letters were converted to lowercase, punctuation
was removed, stop words were dropped, and we did
some generalization by replacing speciﬁc numbers
with generic “number” tokens.
Articles are kept only if they have date and time
information fully set. Some articles only have a pub-
lished date, which makes it impossible to associate
them with stock price changes during the day. At
our milestone, we had a lot of noisy articles in our
database that were not actually press releases, since
Factiva’s “press release” classiﬁcation was not very
accurate. By identifying the most common distrib-
utors of true press releases in our corpus, we were
able to remove this noise.
We also test the publication date against the stock

1Since we train a separate classiﬁer on each company, it
would not improve our performance to gather data from more
companies, but doing more than one allows us to do better error
analysis.

data to make sure there were trades going on around
that time. Articles that do not have any trades be-
tween its publish time and 15 minutes later are dis-
carded. This brings the number of articles down
from 3583 articles with full time information to
2690.
Over our entire corpus of articles, our vocabulary
size is about 27,000 (after lower-casing words and
removing stop words and numbers). We incorpo-
rated a word stemmer [5] into our project to convert
every word to its base form. For example, it con-
verts both “running” and “run” to “run”, and reduces
our vocabulary size to about 19,000 (30% fewer fea-
tures). As described in our results, this helps our
accuracy signiﬁcantly.
We wanted to identify bigrams (and possibly
higher-order phrases), since phrases like “high
proﬁt” are only recorded as “high” and “proﬁt”, each
of which on its on is not particularly correlated with
good or bad news. We tried adding all seen bi-
grams as features, but because of the large number
of unique bigrams used in our entire corpus, we had
a data explosion and could not store the feature vec-
tors for even one company in memory.

3 Classiﬁcation

3.1
Implementation
A press release was categorized as “good news” if
it preceded a rise in stock price over the next n
minutes, and “bad news” otherwise. We associated
each press release with stock trade data in the ap-
propriate window. We trained on 80% of our data
for each company (selected randomly using a con-
sistent random seed) and tested on the remaining
20%. We implemented and trained three classiﬁ-
cation algorithms: Multinomial Naive Bayes (NB),
Support Vector Machine (SVM), and Nearest Neigh-
bor (NN).
Our implementation of NB was based on [1].
Since we only had 2 categories, we did not imple-
ment Complement Naive Bayes as described in the
paper.
Instead, we implemented category weight
normalization, document length normalization, text
frequency adjustment (using the power law distribu-
tion log(1 + fi ), where fi is the number of occur-
rences of a term in the document), and inverse doc-
ument frequency.

To implement the regularized SVM, we adopted
the LIBSVM library [2].
NN was suggested by [3]. At ﬁrst, we tried to cal-
culate distances by using the Euclidean norm. Later
u·v
testing showed that max cosine similarity
||u||2 ||v ||2
gave the best results.
In addition to these three classiﬁers, we also im-
plemented a voting classiﬁer that trained these 3
classiﬁers, and used a majority vote to make a pre-
diction (weighted by the conﬁdence of each classi-
ﬁer that supported probabilistic predictions).

3.2 Features
We started out by using tokens from press releases
as-is. One of the ﬁrst things we added to increase
accuracy was a stemmer [5], reducing the feature
set size by removing different forms of the same
word. As we experimented, we began to take into
account document length, term frequency in a docu-
ment, and the inverse document frequency of terms
(it is assumed that especially important individual
terms appear in few documents, hence inverse doc-
ument frequency). For our ﬁnal round of tests, we
had four conﬁgurations for NN and SVM: existence
of a term in a document, the count of a term in a
document, the count of a term divided by the docu-
ment’s length (normalized word count), and TF-IDF
(normalized word count times a term that penalizes
words that appear in many documents). For NB, the
features mentioned in [1] were always used.

4 Classiﬁcation Results

A comparison of the classiﬁcation accuracy of var-
ious algorithms and feature types are shown in Fig-
ure 1. The vertical axis shows how much more accu-
rate the results were compared to a majority baseline
classiﬁer. The majority baseline classiﬁer classiﬁes
all examples as the most frequent class in the train-
ing set. In this case since the stock market increased
on average in our corpus, the majority baseline clas-
siﬁed press releases as positive. The majority base-
line classiﬁed with a 51-53% accuracy.
Among the various algorithms, NN performed
the best, followed by SVM. Computing the feature
values as a binary Word-In-Doc out performed the
other methods. Normalized word count and TF-
IDF performed below majority baseline.
It is es-

Figure 1: The performance of classiﬁcation with var-
ious algorithms and feature types. Algorithms:(SVM-
Support Vector Machine, NB- Multinomial Naive Bayes,
NN- Nearest Neighbors All-Combination of three algo-
rithms) Feature Types:(WID- Word In Document, WC-
Word Count, NWC- Normalized Word Count, TFIDF-
Term Frequency Inverse Document Frequency)

pecially surprising that the normalized word count
performed signiﬁcantly worse than the unnormal-
ized word count.
It’s possible that our classiﬁers
were taking advantage of the document length being
an important feature, and by normalizing the word
counts, we removed this information from our fea-
tures.
The classiﬁers worked best for classifying the
press releases of McDonald’s. The nearest neigh-
bor classiﬁer with features represented as a binary
word in document classiﬁed McDonald’s press re-
leases 11% better than the majority baseline classi-
ﬁer.
Press releases are written by each company itself,
so it is reasonable that our algorithms perform dif-
ferently for different companies. The press releases
on some companies may have a very neutral tone
at all times, using very similar vocabulary. On the
other hand the press releases of other companies
may vary its vocabulary signiﬁcantly between pub-
lications. The positive correlated features of Mc-
Donald’s (according to Naive Bayes weights) were
mostly related to its service such as ‘variety’, ‘food-
service’, and ‘customers.’ On the other hand, the
negative features of McDonald’s seem to be related

to ﬁnance such as ‘share’, ‘outlook’, and ‘report.’
This might suggest that press releases announcing
news related to its services correlates with an in-
crease in stock price, whereas press releases an-
nouncing ﬁnancial information correlates with a de-
crease in stock price.

Positive
variety
over
visit
llc
ingredients
foodservice
inc
restaurants
customers
through

Negative
now
common
shares
full
outlook
you
open
related
report
stock

Figure 2: Most important features for McDonald’s

Figure 3: The effect of stemming on accuracy.

Figure 3 shows how stemming improved our clas-
siﬁcation accuracy. For each classiﬁcation method,
the results when stemming is used outperform the
results when stemming is not used in all but one
case. Stemming reduces the feature size. The im-
provement in performance may be due to reduced
overﬁtting by decreasing the feature size.
The algorithm depends on the time it takes for the
stock market to respond to a press release. Tests
were performed with various assumptions about the
stock market response time. Some of the results
of these tests are shown in Figure 4. The graph
shows the performance of the nearest neighbor clas-
siﬁer (using word-in-doc) as a function of various
response times. The best performance was observed

Figure 4: The effect of trade timing on accuracy. Trade
timing is how long the algorithms waits after the press
release publication time to compute the change in stock
price.

when the stock market response time was assumed
to be 15 minutes. Without more data and test re-
sults, the difference in accuracy may not be signif-
icant enough to make a conﬁdent conclusion about
the response time of the stock market to a press re-
lease.

5 Clustering

We implemented a clustering algorithm to ﬁnd
stocks that perform similarly. We obtained one
month of stock trades for every company available
in TAQ. We discretized the average trade price by
the hour, and for every hour from the beginning
to the end of the month, we calculated the percent
change in price for every stock from the previous
hour. Thus, for every stock, we had feature vectors
with about 700 features representing the direction of
stock movement.
At ﬁrst we clustered the stocks using K-Means,
but no matter how high k was, there were always
some very large clusters. We simpliﬁed the al-
gorithm by just ﬁnding the k closest stocks for
each stock (using the same Euclidean distance met-
ric). As validation for the success of using percent
changes every hour, we noticed that the closest com-
pany to Boeing was Rockwell Collins, an indepen-
dent branch of a company that Boeing bought sev-

eral years ago. Also, we found that for oil compa-
nies like Exxon and BP, other oil companies were
in its cluster, which makes sense since their stock
prices are all dependent on a single variable for the
price of oil. However, most of highly related com-
panies were big investment companies that we had
never heard of, which are probably correlated with
the companies because they invest in them.
Speciﬁcally, we looked at the 2-3 closest stocks
to McDonald’s, Verizon, and Boeing. For each re-
lated company, we trained a new classiﬁer for its
stock price, based on the press releases of the orig-
inal company (e.g., McDonald’s). However, on our
best classiﬁer setup, the accuracy of the related com-
panies was in general signiﬁcantly worse than the
accuracy of the original companies, and in 4 of the 5
related companies, was worse than majority base-
line. This suggests that short term stock changes
are not correlated very well with related companies,
which is an unfortunate result, but it also tells us that
the features we got from the press releases are actu-
ally meaningful to the company they were trained
for – at least, meaningful enough that the classiﬁer
performs worse on data from other companies.

ative results are due to most of the press releases ac-
tually being uncorrelated with changes in the stock
market. Changes in the stock market only happen
when investors get new information that affects their
judgment about the proﬁtability of the company, and
many articles might not actually provide information
to this effect. Upon further analysis of our press re-
leases, only 13 of the 2690 press releases that hap-
pened during trading hours saw a 1% or higher stock
price increase. Lowering our standards to change in
either direction by at least 0.1%, we found that only
half of the articles have this. We tried considering
only examples where the stock price rose above a
threshold percent positive, and the rest negative, but
this only lowered our accuracy because of very few
positive examples. Since we depleted our source of
press releases for 2006 and 2007, it may not be pos-
sible to get more data. But what might help is having
our system analyze more volatile stocks, since more
exciting news tends to be announced which can sur-
prise investors. We wanted to ﬁnd NASDAQ data
but the best database we found at the library was the
NYSE TAQ.

As possibilities for further research, we could de-
crease the number of features and add more va-
riety to the type of features. We could decrease
our features by ignoring words that appear with ap-
proximately equal distribution in positive and neg-
ative examples, and more advanced word cluster-
ing (in addition to stemming, we could use Word-
Net to collapse synonymous words to the same fea-
ture). We could increase the variety of the types fea-
tures by adding other metadata about press releases
and stocks, such as the change in stock price be-
fore the press release was published and the num-
ber of words in the press release, as well as bigrams
(without those that appear rarely or in equal distri-
bution among positive and negative examples). Fi-
nally, we could try reducing noise by removing the
overall change in the stock market from the change
in price, so that external effects like interest rate cuts
have less effect on our data. We could use a cluster-
ing algorithm to divide the companies by industry
so that we could train companies differently based
on their industry.

Figure 5: The performance of related companies vs. orig-
inal companies with NN WID. Parenthetical companies
are the original companies they are clustered with.

6 Conclusion

Although we had positive results after ﬁne-tuning
our classiﬁer setup, we believe that a lot of our neg-

7 References
1. Jason D. M. Rennie, Lawrence Shih, Jaime Teevan, David R.
Karger, “Tackling the Poor Assumptions of Naive Bayes Text
Classiﬁers” in Proceedings of the Twentieth International Con-
ference on Machine Learning (ICML-2003), Washington, D.C.,
2003. Available HTTP:
http://groups.csail.mit.edu/haystack/papers/rennie.icml03.pdf

2. Chih-Chung Chang, Chih-Jen Lin, LIBSVM - A Library for Sup-
port Vector Machines, Available HTTP:
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/

3. Christopher D. Manning, Prabhakar Raghavan and Hinrich
Introduction to Information Retrieval, Cambridge
Schtze,
University Press. 2008. Available HTTP:
http://www-csli.stanford.edu/∼hinrich/information-retrieval-
book.html

4. M.A. Mittermayer, “Forecasting Intraday Stock Price Trends
with Text Mining Techniques” in Proceedings of the Hawai’i
International Conference on System Sciences, January 5-8,
2004, Big Island, Hawaii. Available HTTP:
http://www.ie.iwi.unibe.ch/staff/mittermayer/resource/Hawaii.pdf

5. Martin Porter, Snowball, Available HTTP:
http://snowball.tartarus.org/


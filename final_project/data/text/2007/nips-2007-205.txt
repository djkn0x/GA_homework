Exponential Family
Predictive Representations of State

David Wingate
Computer Science and Engineering
University of Michigan
wingated@umich.edu

Satinder Singh
Computer Science and Engineering
University of Michigan
baveja@umich.edu

Abstract

In order to represent state in controlled, partially observable, stochastic dynamical
systems, some sort of sufﬁcient statistic for history is nec essary. Predictive repre-
sentations of state (PSRs) capture state as statistics of the future. We introduce a
new model of such systems called the “Exponential family PSR ,” which deﬁnes
as state the time-varying parameters of an exponential family distribution which
models n sequential observations in the future. This choice of state representation
explicitly connects PSRs to state-of-the-art probabilistic modeling, which allows
us to take advantage of current efforts in high-dimensional density estimation, and
in particular, graphical models and maximum entropy models. We present a pa-
rameter learning algorithm based on maximum likelihood, and we show how a
variety of current approximate inference methods apply. We evaluate the qual-
ity of our model with reinforcement learning by directly evaluating the control
performance of the model.

1

Introduction

One of the basic problems in modeling controlled, partially observable, stochastic dynamical sys-
tems is representing and tracking state. In a reinforcement learning context, the state of the system
is important because it can be used to make predictions about the future, or to control the system
optimally. Often, state is viewed as an unobservable, latent variable, but models with predictive rep-
resentations of state [4] propose an alternative: PSRs represent state as statistics about the future.

The original PSR models used the probability of speci ﬁc, det ailed futures called tests as the statistics
of interest. Recent work has introduced the more general notion of using parameters that model the
distribution of length n futures as the statistics of interest [8]. To clarify this, consider an agent
interacting with the system. It observes a series of observations o1 ...ot , which we call a history
ht (where subscripts denote time). Given any history, there is some distribution over the next n
observations: p(Ot+1 ...Ot+n |ht ) ≡ p(F n |ht ) (where Ot+i is the random variable representing
an observation i steps in the future, and F n is a mnemonic for future). We emphasize that this
distribution directly models observable quantities in the system.

Instead of capturing state with tests, the more general idea is to capture state by directly modeling
the distribution p(F n |ht ). Our central assumption is that the parameters describing p(F n |ht ) are
sufﬁcient for history, and therefore constitute state (as t he agent interacts with the system, p(F n |ht )
changes because ht changes; therefore the parameters and hence state change). As an example of
this, the Predictive Linear-Gaussian (PLG) model [8] assumes that p(F n |ht ) is jointly Gaussian;
state therefore becomes its mean and covariance. Nothing is lost by deﬁning state in terms of ob-
servable quantities: Rudary et al [8] proved that the PLG is formally equivalent to the latent-variable
approach in linear dynamical systems. In fact, because the parameters are grounded, statistically
consistent parameter estimators are available for PLGs.

1

Thus, as part of capturing state in a dynamical system in our method, p(F n |ht ) must be estimated.
This is a density estimation problem.
In systems with rich observations (say, camera images),
p(F n |ht ) may have high dimensionality. As in all high-dimensional density estimation problems,
structure must be exploited. It is therefore natural to connect to the large body of recent research
dealing with high-dimensional density estimation, and in particular, graphical models.
In this paper, we introduce the Exponential Family PSR (EFPSR) which assumes that p(F n |ht ) is
a standard exponential family distribution. By selecting the sufﬁcient statistics of the distribution
carefully, we can impose graphical structure on p(F n |ht ), and therefore make explicit connections to
graphical models, maximum entropy modeling, and Boltzmann machines. The EFPSR inherits both
the advantages and disadvantages of graphical exponential family models: inference and parameter
learning in the model is generally hard, but all existing research on exponential family distributions
is applicable (in particular, work on approximate inference).
Selecting the form of p(F n |ht ) and estimating its parameters to capture state is only half of the prob-
lem. We must also model the dynamical component, which describes the way that the parameters
vary over time (that is, how the parameters of p(F n |ht ) and p(F n |ht+1 ) are related). We describe a
method called “extend-and-condition,” which generalizes many state update mechanisms in PSRs.

Importantly, the EFPSR has no hidden variables, but can still capture state, which sets it apart from
other graphical models of sequential data. It is not directly comparable to latent-variable models
such as HMMs, CRFs [3], or Maximum-entropy Markov Models (MEMMs) [5], for example. In
particular, EM-based procedures used in the latent-variable models for parameter learning are un-
necessary, and indeed, impossible. This is a consequence of the fact that the model is fully observed:
all statistics of interest are directly related to observable quantities.

We refer the reader to [11] for an extended version of this paper.

2 The Exponential Family PSR

We now present the Exponential Family PSR (EFPSR) model. The next sections discuss the speci ﬁcs
of the central parts of the model: the state representation, and how we maintain that state.

2.1 Standard Exponential Family Distributions

We ﬁrst discuss exponential family distributions, which we use because of their close connections
to maximum entropy modeling and graphical models. We refer the reader to Jaynes [2] for detailed
justi ﬁcation, but brieﬂy, he states that the maximum entrop
y distribution “agrees with everything
that is known, but carefully avoids assuming anything that is not known,” which “is the fundamental
property which justi ﬁes its use for inference.” The standar
d exponential family distribution is the
form of the maximum entropy distribution under certain constraints.

For a random variable X , a standard exponential family distribution has the form p(X = x; s) =
exp{sT φ(x) − Z (s)}, where s is the canonical (or natural) vector of parameters and φ(x) is a vector
of features of variable x. The vector φ(x) also forms the sufﬁcient statistics of the distribution.
The term Z (s) is known as the log-partition function, and is a normalizing constant which ensures
that p(X ; s) deﬁnes a valid distribution: Z (s) = log R exp{sT φ(x)}dx. By carefully selecting the
features φ(x), graphical structure may be imposed on the distribution.
2.2 State Representation and Dynamics

State. The EFPSR deﬁnes state as the parameters of an exponential f amily distribution modeling
p(F n |ht ). To emphasize that these parameters represent state, we will refer to them as st :
p(F n = f n |ht ; st ) = exp (cid:8)s⊤
t φ(f n ) − log Z (st )(cid:9) ,
with both { φ(f n ), st } ∈ Rl×1 . We emphasize that st changes with history, but φ(f n ) does not.
Maintaining State. In addition to selecting the form of p(F n |ht ), there is a dynamical component:
given the parameters of p(F n |ht ), how can we incorporate a new observation to ﬁnd the paramete rs
of p(F n |ht , ot+1 )? Our strategy is to extend and condition, as we now explain.

(1)

2

Extend. We assume that we have the parameters of p(F n |ht ), denoted st . We extend the distribu-
tion of F n |ht to include Ot+n+1 , which forms a new variable F n+1 |ht , and we assume it has the
distribution p(F n , Ot+n+1 |ht ) = p(F n+1 |ht ). This is a temporary distribution with (n + 1)d ran-
dom variables. In order to add the new variable Ot+n+1 , we must add new features which describe
Ot+n+1 and its relationship to F n . We capture this with a new feature vector φ+ (f n+1 ) ∈ Rk×1 ,
t ∈ Rk×1 to be the parameters associated with this feature vector. We thus
and deﬁne the vector s+
have the following form for the extended distribution:
t ) = exp (cid:8)s+⊤
p(F n+1 = f n+1 |ht ; s+
t φ+ (f n+ ) − log Z (s+
t )(cid:9) .
ent state vector to the parameters
To deﬁne the dynamics, we deﬁne a function which maps the curr
t = extend(st ; θ), where θ is a
of the extended distribution. We call this the extension function: s+
vector of parameters controlling the extension function (and hence, the overall dynamics).

The extension function helps govern the kinds of dynamics that the model can capture. For example,
in the PLG family of work, a linear extension allows the model to capture linear dynamics [8], while
a non-linear extension allows the model to capture non-linear dynamics [11].
Condition. Once we have extended the distribution to model the n + 1’st observation in the future,
we then condition on the actual observation ot+1 , which results in the parameters of a distribution
t , ot+1 ), which are precisely
over observations from t + 1 through t + n + 1: st+1 = condition(s+
the statistics representing p(F n |ht+1 ), which is our state at time t + 1.
By extending and conditioning, we can maintain state for arbitrarily long periods. Furthermore, for
many choices of features and extension function, the overall extend-and-condition operation does
not involve any inference, mean that tracking state is computationally efﬁcient.

There is only one restriction on the extension function: we must ensure that after extending and con-
ditioning the distribution, the resulting distribution can be expressed as: p(F n = f n |ht+1 ; st+1 ) =
t+1φ(f n ) − log Z (st+1 )}. This looks like exactly like Eq. 1, which is the point: the fea-
exp{s⊤
ture vector φ did not change between timesteps, which means the form of the distribution does not
change. For example, if p(F n |ht ) is a Gaussian, then p(F n |ht+1 ) will also be a Gaussian.

2.3 Representational Capacity

The EFPSR model is quite general. It has been shown that a number of popular models can be uni ﬁed
under the umbrella of the general EFPSR: for example, every PSR can be represented as an EFPSR
(implying that every POMDP, MDP, and k-th order Markov model can also be represented as an
EFPSR); and every linear dynamical system (Kalman ﬁlter) an d some nonlinear dynamical systems
can also be represented by an EFPSR. These different models are obtained with different choices of
the features φ and the extension function, and are possible because many popular distributions (such
as multinomials and Gaussians) are exponential family distributions [11].

3 The Linear-Linear EFPSR

We now choose speci ﬁc features and extension function to gen erate an example model designed to
be analytically tractable. We select a linear extension function, and we carefully choose features
so that conditioning is always a linear operation. We restrict the model to domains in which the
observations are vectors of binary random variables. The result is named the Linear-Linear EFPSR.
Features. Recall that the features φ() and φ+ () do not depend on time. This is equivalent to saying
that the form of the distribution does not vary over time. If the features impose graphical structure
on the distribution, it is also equivalent to saying that the form of the graph does not change over
time. Because of this, we will now discuss how we can use a graph whose form is independent of
time to help deﬁne structure on our distributions.
We construct the feature vectors φ() and φ+ () as follows. Let each Ot ∈ {0, 1}d ; therefore, each
F n |ht ∈ {0, 1}nd . Let (F n )i be the i’th random variable in F n |ht . We assume that we have an
undirected graph G which we will use to create the features in the vector φ(), and that we have
another graph G+ which we will use to deﬁne the features in the vector φ+ (). Deﬁne G = (V , E )
i ), and (i, j ) ∈ E are the
where V = {1, ..., nd} are the nodes in the graph (one for each F n |ht

3

G

G+

G

s
e
r
u
t
a
e
f
 
n
o
i
t
a
v
r
e
s
b
O

t+1

t+2

t+n

t+1

t+2

t+n

t+n+1

t+1

t+2

t+n

t+n+1

Distribution of next n observations
p(F n |ht )

Extended distribution
p(F n , Ot+n+1 |ht )

Conditioned distribution
p(F n |ht , ot+1 )

Figure 1: An illustration of extending and conditioning the distribution.

edges. Similarly, we deﬁne G+ = (V +, E+) where V + = {1, ..., (n + 1)d} are the nodes in the
graph (one for each (F n+1 |ht )i ), and (i, j ) ∈ E+ are the edges. Neither graph depends on time.
To use the graph to deﬁne our distribution, we will let entrie s in φ be conjunctions of atomic obser-
vation variables (like the standard Ising model): for i ∈ V , there will be some feature k in the vector
t . We also create one feature for each edge: if (i, j ) ∈ E , then there will be
such that φ(ft )k = f i
t f j
t . Similarly, we use G+ to deﬁne φ+ ().
some feature k in the vector such that φ(ft )k = f i
As discussed previously, neither G nor G+ (equivalently, φ and φ+ ) can be arbitrary. We must ensure
that after conditioning G+ , we recover G. To accomplish this, we ensure that both temporally shifted
copies and conditioned versions of each feature exist in the graphs (seen pictorially in Fig. 1).

Because all features are either atomic variables or conjunctions of variables, conditioning the dis-
tribution can be done with an operation which is linear in the state (this is true even if the random
variables are discrete or real-valued). We therefore deﬁne the linear conditioning operator G(ot+1 )
to be a matrix which transforms s+
into st+1 : st+1 = G(ot+1 )s+
t . See [11] for details.
t
Linear extension. In general, the function extend can take any form. We choose a linear extension:
s+
t = Ast + B
where A ∈ Rk×l and B ∈ Rk×1 are our model parameters. The combination of a linear extension
and a linear conditioning operator can be rolled together into a single operation. Without loss of
generality, we can permute the indices in our state vector such that st+1 = G(ot+1 ) (Ast + B ).
Note that although this is linear in the state, it is nonlinear in the observation.

4 Model Learning

We have deﬁned our concept of state, as well as our method for t racking that state. We now address
the question of learning the model from data. There are two things which can be learned in our
model: the structure of the graph, and the parameters governing the state update. We brieﬂy address
each in the next two subsections. We assume we are given a sequence of T observations, [o1 · · · oT ],
which we stack to create a sequence of samples from the F n |ht ’s: ft |ht = [ot+1 · · · ot+n |ht ].

4.1 Structure Learning

To learn the graph structure, we make the approximation of ignoring the dynamical component of
the model. That is, we treat each ft as an observation, and try to estimate the density of the re-
sulting unordered set, ignoring the t subscripts (we appeal to density estimation because many good
algorithms have been developed for structure induction). We therefore ignore temporal relationships
across samples, but we preserve temporal relationships within samples. For example, if observation
a is always followed by observation b, this fact will be captured within the ft ’s.
The problem therefore becomes one of inducing graphical structure for a non-sequential data set,
which is a problem that has already received considerable attention. In all of our experiments, we
used the method of Della Pietra et. al [7]. Their method iteratively evaluates a set of candidate
features and adds the one with highest expected gain in log-likelihood. To enforce the temporal

4

invariance property, whenever we add a feature, we also add all of the temporally shifted copies of
that feature, as well as the conditioned versions of that feature.

4.2 Maximum Likelihood Parameter Estimation

With the structure of the graph in place, we are left to learn the parameters A and B of the state ex-
tension. It is now useful that our state is deﬁned in terms of o bservable quantities, for two reasons:
ﬁrst, because everything in our model is observed, EM-style procedures for estimating the parame-
ters of our model are not needed, simply because there are no unobserved variables over which to
take expectations. Second, when trying to learn a sequence of states (st ’s) given a long trajectory
of futures (ft ’s), each ft is a sample of information directly from the distribution we’re trying to
model. Given a parameter estimate, an initial state s0 , and a sequence of observations, the sequence
of st ’s is completely determined. This will be a key element to our proposed maximum-likelihood
learning algorithm.
Although the sequence of state vectors st are the parameters deﬁning the distributions p(F n |ht ),
they are not the model parameters – that is, we cannot freely select them.
Instead, the model pa-
rameters are the parameters θ which govern the extension function. This is a signi ﬁcant di fference
from standard maximum entropy models, and stems from the fact that our overall problem is that of
modeling a dynamical system, rather than just density estimation.
The likelihood of the training data is p(o1 , o2 ...oT ) = QT
t=1 p(ot |ht ). We will ﬁnd it more conve-
nient to measure the likelihood of the corresponding ft ’s: p(o1 , o2 ...oT ) ≈ n QT
t=1 p(ft |ht ) (the
likelihoods are not the same because the likelihood of the ft ’s counts a single observation n times;
the approximate equality is because the ﬁrst n and last n are counted fewer than n times).
The expected log-likelihood of the training ft ’s under the model deﬁned in Eq. 1 is
T   T
t φ(ft ) − log Z (st )!
1
Xt=1
−s⊤
Our goal is to maximize this quantity. Any optimization method can be used to maximize the log-
likelihood. Two popular choices are gradient ascent and quasi-Newton methods, such as (L-)BFGS.
We use both, for different problems (as discussed later). However, both methods require the gradient
of the likelihood with respect to the parameters, which we will now compute.

LL =

(2)

=

(4)

(3)

Using the chain rule of derivatives, we can compute the derivative with respect to the parameters A:
T
⊤ ∂ st
∂LL
∂LL
Xt=1
∂A
∂ st
∂A
First, we compute the derivative of the log-likelihood with respect to each state:
∂LL
∂
∂ st (cid:2)−s⊤
t φ(ft ) − log Z (st )(cid:3) = Est [φ(F n |ht )] − φ(ft ) ≡ δt
∂ st
where Est [φ(F n |ht )] ∈ Rl×1 is the vector of expected sufﬁcient statistics at time
t. Computing
this is a standard inference problem in exponential family models, as discussed in Section 5. This
gradient tells us that we wish to adjust each state to make the expected features of the next n ob-
servations closer to the observed features however, we cannot adjust st directly; instead, we must
adjust it implicitly by adjusting the transition parameters A and B .
We now compute the gradients of the state with respect to each parameter:
t−1 ⊗ I (cid:19) .
G(ot+1 ) (Ast−1 + B ) = G(ot+1 ) (cid:18)A
∂ st
∂
∂ st−1
+ s⊤
∂A
∂A
∂A
where ⊗ is the Kronecker product, and I is an identity matrix the same size as A. The gradients of
the state with respect to B are given by
+ I (cid:19)
G(ot+1 ) (Ast−1 + B ) = G(ot+1 ) (cid:18)A
∂
∂ st−1
∂ st
∂B
∂B
∂B
These gradients are temporally recursive – they implicitly depend on gradients from all previous
timesteps. It might seem prohibitive to compute them: must an algorithm examine all past t1 · · · tt−1
data points to compute the gradient at time t? Fortunately, the answer is no: the necessary statistics
can be computed in a recursive fashion as the algorithm walks through the data.

=

=

=

5

d
o
o
h
i
l
e
k
i
l
−
g
o
L

−1.4

−1.6

−1.8

−2

 

Training LL
Testing LL
True LL
Naive LL

−2.07

0

10

20

 
0
20
10
Iterations of optimization
(a)

0

−2.08 

0

10

20

10

20

p

A

1−p

1−q

q

B

(b)

Figure 2: Results on two-state POMDPs. The right shows the generic model used. By varying the
transition and observation probabilities, three different POMDPs were generated. The left shows
learning performance on the three models. Likelihoods for naive predictions are shown as a dotted
line near the bottom; likelihoods for optimal predictions are shown as a dash-dot line near the top.

Problem
Paint
Network
Tiger

# of
states
16
7
2

# of
obs.
2
2
2

# of
actions
4
4
3

Naive
LL
6.24
6.24
6.24

True
LL
4.66
4.49
5.23

Training set
%
LL
99.7
4.67
99.5
4.50
92.4
5.24

Test set
%
LL
99.9
4.66
98.0
4.52
86.0
5.25

Figure 3: Results on standard POMDPs. See text for explanation.

5

Inference

In order to compute the gradients needed for model learning, the expected sufﬁcient statistics
E[φ(F n |ht )] at each timestep must be computed (see Eq. 4):
E [φ(F n |ht )] = Z φ(ft )p(F n |ht )dft = ∇Z (s).
This quantity, also known as the mean parameters, is of central interest in standard exponential fam-
ilies, and has several interesting properties. For example, each possible set of canonical parameters
s induces one set of mean parameters; assuming that the features are linearly independent, each set
of valid mean parameters is uniquely determined by one set of canonical parameters [9].

Computing these marginals is an inference problem. This is repeated T times (the number of sam-
ples) in order to get one gradient, which is then used in an outer optimization loop; because inference
must be repeatedly performed in our model, computational efﬁciency is a more stringent require-
ment than accuracy.
In terms of inference, our model inherits all of the properties of graphical
models, for better and for worse. Exact inference in our model is generally intractable, except in
the case of fully factorized or tree-structured graphs. However, many approximate algorithms ex-
ist: there are variational methods such as naive mean- ﬁeld,
tree-reweighted belief propagation, and
log-determinant relaxations [10]; other methods include Bethe-Kikuchi approximations, expectation
propagation, (loopy) belief propagation, MCMC methods, and contrastive divergence [1].

6 Experiments and Results

Two sets of experiments were conducted to evaluate the quality of our model and learning algorithm.
The ﬁrst set tested whether the model could capture exact sta te, given the correct features and exact
inference. We evaluated the learned model using exact inference to compute the exact likelihood of
the data, and compared to the true likelihood. The second set tested larger models, for which exact
inference is not possible. For the second set, bounds can be provided for the likelihoods, but may be
so loose as to be uninformative. How can we assess the quality of the ﬁnal model? One objective
gauge is control performance: if the model has a reward signal, reinforcement learning can be used
to determine an optimal policy. Evaluating the reward achieved becomes an objective measure of
model quality, even though approximate likelihood is the learning signal.

6

d
r
a
w
e
R
 
e
g
a
r
e
v
A

0.2

0.15

0.1

0.05

0

 
1

 

6

 

 
EFPSR/VMF
EFPSR/LBP
EFPSR/LDR
POMDP
Reactive
Random

d
r
a
w
e
R
 
e
g
a
r
e
v
A

0.15

0.1

0.05

0

 
1

 

6

2
5
4
3
Steps of optimization

2
5
4
3
Steps of optimization

Figure 4: Results on Cheesemaze (left) and Maze 4x3 (right) for different inference methods.

First set. We tested on three two-state problems, as well as three small, standard POMDPs. For
each problem, training and test sets were generated (using a uniformly random policy for controlled
systems). We used 10,000 samples, set n = 3 and used structure learning as explained in Section 4.1.
We used exact inference to compute the E[φ(F n |ht )] term needed for the gradients. We optimized
the likelihood using BFGS. For each dataset, we computed the log-likelihood of the data under the
true model, as well as the log-likelihood of a “naive” model, which assigns uniform probability
to every possible observation. We then learned the best model possible, and compared the ﬁnal
log-likelihood under the learned and true models.

Figure 2 (a) shows results for three two-state POMDPs with binary observations. The left panel of
Fig. 2 (a) shows results for a two-state MDP. The likelihood of the learned model closely approaches
the likelihood of the true model (although it does not quite reach it; this is because the model
has trouble modeling deterministic observations, because the weights in the exponential need to be
inﬁnitely large [or small] to generate a probability of one [ or zero]). The middle panel shows results
for a moderately noisy POMDP; again, the learned model is almost perfect. The third panel shows
results for a very noisy POMDP, in which the naive and true LLs are very close; this indicates that
prediction is difﬁcult, even with a perfect model.
Figure 3 shows results for three standard POMDPs, named Paint, Network and Tiger1 . The ta-
ble conveys similar information to the graphs: naive and true log-likelihoods, as well as the log-
likelihood of the learned models (on both training and test sets). To help interpret the results, we
also report a percentage (highlighted in bold), which indicates the amount of the likelihood gap (be-
tween the naive and true models) that was captured by the learned model. Higher is better; again we
see that the learned models are quite accurate, and generalize well.
Second set. We also tested on a two more complicated POMDPs called Cheesemaze and Maze
4x31 . For both problems, exact inference is intractable, and so we used approximate inference. We
experimented with loopy belief propagation (LBP) [12], naive mean ﬁeld (or variational mean ﬁeld,
VMF), and log-determinant relaxations (LDR) [10]. Since the VMF and LDR bounds on the log-
likelihood were so loose (and LBP provides no bound), it was impossible to assess our model by an
appeal to likelihood. Instead, we opted to evaluate the models based on control performance.

We used the Natural Actor Critic (or NAC) algorithm [6] to test our model (see [11] for further
experiments). The NAC algorithm requires two things: a stochastic, parameterized policy which
operates as a function of state, and the gradients of the log probability of that policy. We used a
softmax function of a linear projection of the state: the probability of taking action ai from state st
t θi(cid:9) / P|A|
t θj (cid:9). The parameters
given the policy parameters θ is: p(ai ; st , θ) = exp (cid:8)s⊤
j=1 exp (cid:8)s⊤
θ are to be determined. For comparison, we also ran the NAC planner with the POMDP belief
state: we used the same stochastic policy and the same gradients, but we used the belief state of the
true POMDP in place of the EFPSR’s state (st ). We also tested NAC with the ﬁrst-order Markov
assumption (or reactive policy) and a totally random policy.
Results. Figure 4 shows the results for Cheesemaze. The left panel shows the best control perfor-
mance obtained (average reward per timestep) as a function of steps of optimization. The “POMDP”
line shows the best reward obtained using the true belief state as computed under the true model,
the “Random” line shows the reward obtained with a random pol
icy, and the “Reactive” line shows
the best reward obtained by using the observation as input to the NAC algorithm. The lines “VMF,”
“LBP,” and “LDR” correspond to the different inference meth
ods.

1From Tony Cassandra’s POMDP repository at http://www.cs.brown.edu/research/ai/pomdp/index.html

7

The EFPSR models all start out with performance equivalent to the random policy (average reward of
0.01), and quickly hop to of 0.176. This is close to the average reward of using the true POMDP state
at 0.187. The EFPSR policy closes about 94% of the gap between a random policy and the policy
obtained with the true model. Surprisingly, only a few iterations of optimization were necessary to
generate a usable state representation. Similar results hold for the Maze 4x3 domain, although the
improvement over the ﬁrst order Markov model is not as strong : the EFPSR closes about 77.8% of
the gap between a random policy and the optimal policy. We conclude that the EFPSR has learned
a model which successfully incorporates information from history into the state representation, and
that it is this information which the NAC algorithm uses to obtain better-than-reactive performance.
This implies that the model and learning algorithm are useful even with approximate inference
methods, and even in cases where we cannot compare to the exact likelihood.

7 Conclusions

We have presented the Exponential Family PSR, a new model of controlled, stochastic dynamical
systems which provably uni ﬁes other models with predictive ly deﬁned state. We have also discussed
a speci ﬁc member of the EFPSR family, the Linear-Linear EFPS R, and a maximum likelihood learn-
ing algorithm. We were able to learn almost perfect models of several small POMDP systems, both
from a likelihood perspective and from a control perspective. The biggest drawback is computa-
tional: the repeated inference calls make the learning process very slow. Improving the learning
algorithm is an important direction for future research. While slow, the learning algorithm generates
models which can be accurate in terms of likelihood and useful in terms of control performance.

Acknowledgments

David Wingate was supported under a National Science Foundation Graduate Research Fellowship.
Satinder Singh was supported by NSF grant IIS-0413004. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of the authors and do not necessarily reﬂect
the views of the NSF.

References
[1] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,
14(8):1771–1800, 2002.
[2] E. T. Jaynes. Notes on present status and future prospects. In W. Grandy and L. Schick, editors, Maximum
Entropy and Bayesian Methods, pages 1–13, 1991.
[3] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁeld s: Probabilistic models for segmenting
and labeling sequence data. In International Conference on Machine Learning (ICML), 2001.
[4] M. L. Littman, R. S. Sutton, and S. Singh. Predictive representations of state. In Neural Information
Processing Systems (NIPS), pages 1555–1561, 2002.
[5] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction
and segmentation. In International Conference on Machine Learning (ICML), pages 591–598, 2000.
[6] J. Peters, S. Vijayakumar, and S. Schaal. Natural actor-critic.
In European Conference on Machine
Learning (ECML), pages 280–291, 2005.
[7] S. D. Pietra, V. D. Pietra, and J. Lafferty.
Inducing features of random ﬁelds.
Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997.
[8] M. Rudary, S. Singh, and D. Wingate. Predictive linear-Gaussian models of stochastic dynamical systems.
In Uncertainty in Arti ﬁcial Intelligence (UAI) , pages 501–508, 2005.
[9] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.
Technical Report 649, UC Berkeley, 2003.
[10] M. J. Wainwright and M. I. Jordan. Log-determinant relaxation for approximate inference in discrete
Markov random ﬁelds.
IEEE Transactions on Signal Processing, 54(6):2099–2109, 2006.
[11] D. Wingate. Exponential Family Predictive Representations of State. PhD thesis, University of Michigan,
2008.
[12] J. S. Yedida, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations.
Technical Report TR-2001-22, Mitsubishi Electric Research Laboratories, 2001.

IEEE Transactions on

8


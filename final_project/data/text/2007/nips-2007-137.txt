Estimating divergence functionals and the likelihood
ratio by penalized convex risk minimization

XuanLong Nguyen
SAMSI & Duke University

Martin J. Wainwright
UC Berkeley

Michael I. Jordan
UC Berkeley

Abstract

We develop and analyze an algorithm for nonparametric estimation of divergence
functionals and the density ratio of two probability distributions. Our method is
based on a variational characterization of f -divergences, which turns the estima-
tion into a penalized convex risk minimization problem. We present a derivation
of our kernel-based estimation algorithm and an analysis of convergence rates for
the estimator. Our simulation results demonstrate the convergence behavior of the
method, which compares favorably with existing methods in the literature.

1

Introduction

An important class of “distances ” between multivariate pro bability distributions P and Q are the Ali-
Silvey or f -divergences [1, 6]. These divergences, to be deﬁned formal
ly in the sequel, are all of the
form Dφ (P, Q) = R φ(dQ/dP)dP, where φ is a convex function of the likelihood ratio. This family,
including the Kullback-Leibler (KL) divergence and the variational distance as special cases, plays
an important role in various learning problems, including classi ﬁcation, dimensionality reduction,
feature selection and independent component analysis. For all of these problems, if f -divergences
are to be used as criteria of merit, one has to be able to estimate them efﬁciently from data.

With this motivation, the focus of paper is the problem of estimating an f -divergence based on i.i.d.
samples from each of the distributions P and Q. Our starting point is a variational characterization
of f -divergences, which allows our problem to be tackled via an M -estimation procedure. Speci ﬁ-
cally, the likelihood ratio function dP/dQ and the divergence functional Dφ (P, Q) can be estimated
by solving a convex minimization problem over a function class. In this paper, we estimate the like-
lihood ratio and the KL divergence by optimizing a penalized convex risk. In particular, we restrict
the estimate to a bounded subset of a reproducing kernel Hilbert Space (RKHS) [17]. The RKHS
is sufﬁciently rich for many applications, and also allows f or computationally efﬁcient optimization
procedures. The resulting estimator is nonparametric, in that it entails no strong assumptions on the
form of P and Q, except that the likelihood ratio function is assumed to belong to the RKHS.

The bulk of this paper is devoted to the derivation of the algorithm, and a theoretical analysis of the
performance of our estimator. The key to our analysis is a basic inequality relating a performance
metric (the Hellinger distance) of our estimator to the suprema of two empirical processes (with
respect to P and Q) deﬁned on a function class of density ratios. Convergence r ates are then obtained
using techniques for analyzing nonparametric M -estimators from empirical process theory [20].
Related work. The variational representation of divergences has been derived independently and
exploited by several authors [5, 11, 14]. Broniatowski and Keziou [5] studied testing and estimation
problems based on dual representations of f -divergences, but working in a parametric setting as op-
posed to the nonparametric framework considered here. Nguyen et al. [14] established a one-to-one
correspondence between the family of f -divergences and the family of surrogate loss functions [2],
through which the (optimum) “surrogate risk” is equal to the
negative of an associated f -divergence.
Another link is to the problem of estimating integral functionals of a single density, with the Shan-
non entropy being a well-known example, which has been studied extensively dating back to early

1

work [9, 13] as well as the more recent work [3, 4, 12]. See also [7, 10, 8] for the problem of
(Shannon) entropy functional estimation. In another branch of related work, Wang et al. [22] pro-
posed an algorithm for estimating the KL divergence for continuous distributions, which exploits
histogram-based estimation of the likelihood ratio by building data-dependent partitions of equiv-
alent (empirical) Q-measure. The estimator was empirically shown to outperform direct plug-in
methods, but no theoretical results on its convergence rate were provided.

This paper is organized as follows. Sec. 2 provides a background of f -divergences. In Sec. 3, we
describe an estimation procedure based on penalized risk minimization and accompanying conver-
gence rates analysis results. In Sec. 4, we derive and implement efﬁcient algorithms for solving
these problems using RKHS. Sec. 5 outlines the proof of the analysis. In Sec. 6, we illustrate the
behavior of our estimator and compare it to other methods via simulations.

2 Background

We begin by deﬁning
f -divergences, and then provide a variational representation of the f -
divergence, which we later exploit to develop an M -estimator.
Consider two distributions P and Q, both assumed to be absolutely continuous with respect to
Lebesgue measure µ, with positive densities p0 and q0 , respectively, on some compact domain
X ⊂ Rd . The class of Ali-Silvey or f -divergences [6, 1] are “distances ” of the form:
Dφ (P, Q) = Z p0φ(q0 /p0 ) dµ,
(1)
where φ : R → ¯R is a convex function. Different choices of φ result in many divergences that play
important roles in information theory and statistics, including the variational distance, Hellinger
distance, KL divergence and so on (see, e.g., [19]). As an important example, the Kullback-Leibler
(KL) divergence between P and Q is given by DK (P, Q) = R p0 log(p0 /q0 ) dµ, corresponding to
the choice φ(t) = − log(t) for t > 0 and +∞ otherwise.
Variational representation: Since φ is a convex function, by Legendre-Fenchel convex duality [16]
we can write φ(u) = supv∈R (uv − φ∗ (v)), where φ∗ is the convex conjugate of φ. As a result,
f µZ f dQ − Z φ∗ (f ) dP¶ ,
Dφ (P, Q) = Z p0 sup
(f q0 /p0 − φ∗ (f )) dµ = sup
f
where the supremum is taken over all measurable functions f : X → R, and R f dP denotes the
expectation of f under distribution P. Denoting by ∂φ the subdifferential [16] of the convex function
φ, it can be shown that the supremum will be achieved for functions f such that q0 /p0 ∈ ∂φ∗ (f ),
where q0 , p0 and f are evaluated at any x ∈ X . By convex duality [16], this is true if f ∈ ∂φ(q0 /p0 )
for any x ∈ X . Thus, we have proved [15, 11]:
Lemma 1. Letting F be any function class in X → R, there holds:
f ∈F Z f dQ − φ∗ (f ) dP,
Dφ (P, Q) ≥ sup
with equality if F ∩ ∂φ(q0 /p0 ) 6= ∅.
To illustrate this result in the special case of the KL divergence, here the function φ has the form
φ(u) = − log(u) for u > 0 and +∞ for u ≤ 0. The convex dual of φ is φ∗ (v) = supu (uv−φ(u)) =
−1 − log(−v) if u < 0 and +∞ otherwise. By Lemma 1,
g>0 Z log g dP − Z gdQ + 1.
f <0 Z f dQ − Z (−1 − log(−f )) dP = sup
DK (P, Q) = sup
In addition, the supremum is attained at g = p0/q0 .

(2)

(3)

3 Penalized M-estimation of KL divergence and the density ratio

Let X1 , . . . , Xn be a collection of n i.i.d. samples from the distribution Q, and let Y1 , . . . , Yn be
n i.i.d. samples drawn from the distribution P. Our goal is to develop an estimator of the KL
divergence and the density ratio g0 = p0 /q0 based on the samples {Xi }n
i=1 and {Yi }n
i=1 .

2

(4)

The variational representation in Lemma 1 motivates the following estimator of the KL divergence.
First, let G be a function class of X → R+ . We then compute
g∈G Z log g dPn − Z gdQn + 1,
ˆDK = sup
where R dPn and R dQn denote the expectation under empirical measures Pn and Qn , respectively.
If the supremum is attained at ˆgn , then ˆgn serves as an estimator of the density ratio g0 = p0 /q0 .
In practice, the “true” size of G is not known. Accordingly, our approach in this paper is an alter-
native approach based on controlling the size of G by using penalties. More precisely, let I (g) be a
non-negative measure of complexity for g such that I (g0 ) < ∞. We decompose the function class
G as follows:
(5)
G = ∪1≤M ≤∞GM ,
where GM := {g | I (g) ≤ M } is a ball determined by I (·).
The estimation procedure involves solving the following program:
λn
ˆgn = argming∈G Z gdQn − Z log g dPn +
2
where λn > 0 is a regularization parameter. The minimizing argument ˆgn is plugged into (4) to
obtain an estimate of the KL divergence DK .
For the KL divergence, the difference | ˆDK − DK (P, Q)| is a natural performance measure. For
estimating the density ratio, various metrics are possible. Viewing g0 = p0/q0 as a density function
with respect to Q measure, one useful metric is the (generalized) Hellinger distance:
1
2 Z (g1/2
0 − g1/2 )2 dQ.
For the analysis, several assumptions are in order. First, assume that g0 (not all of G ) is bounded
from above and below:

h2
Q (g0 , g) :=

I 2 (g),

(6)

(7)

0 < η0 ≤ g0 ≤ η1 for some constants η0 , η1 .
Next, the uniform norm of GM is Lipchitz with respect to the penalty measure I (g), i.e.:
g∈GM |g |∞ ≤ cM for any M ≥ 1.
sup
Finally, on the bracket entropy of G [21]: For some 0 < γ < 2,
δ (GM , L2 (Q)) = O(M /δ)γ for any δ > 0.
HB
The following is our main theoretical result, whose proof is given in Section 5:
Theorem 2. (a) Under assumptions (8), (9) and (10), and letting λn → 0 so that:
λ−1
n = OP (n2/(2+γ ) )(1 + I (g0 )),

then under P:

hQ (g0 , ˆgn ) = OP (λ1/2
n )(1 + I (g0 )),
I (ˆgn ) = OP (1 + I (g0 )).
(b) If, in addition to (8), (9) and (10), there holds inf g∈G g(x) ≥ η0 for any x ∈ X , then
| ˆDK − DK (P, Q)| = OP (λ1/2
n )(1 + I (g0 )).
4 Algorithm: Optimization and dual formulation

(8)

(9)

(10)

(11)

G is an RKHS. Our algorithm involves solving program (6), for some choice of function class G .
In our implementation, relevant function classes are taken to be a reproducing kernel Hilbert space
induced by a Gaussian kernel. The RKHS’s are chosen because they are sufﬁciently rich [17], and
as in many learning tasks they are quite amenable to efﬁcient optimization procedures [18].

3

Let K : X × X → R be a Mercer kernel function [17]. Thus, K is associated with a feature
map Φ : X → H, where H is a Hilbert space with inner product h., .i and for all x, x0 ∈ X ,
K (x, x0 ) = hΦ(x), Φ(x0 )i. As a reproducing kernel Hilbert space, any function g ∈ H can be
expressed as an inner product g(x) = hw, Φ(x)i, where kgkH = kwkH . A kernel used in our
simulation is the Gaussian kernel:
K (x, y) := e−kx−yk2 /σ ,
where k.k is the Euclidean metric in Rd , and σ > 0 is a parameter for the function class.
Let G := H, and let the complexity measure be I (g) = kgkH . Thus, Eq. (6) becomes:
n
n
1
λn
1
Xi=1
Xj=1
2 kwk2
H ,
n
n
where {xi } and {yj } are realizations of empirical data drawn from Q and P, respectively. The log
function is extended take value −∞ for negative arguments.
Lemma 3. minw J has the following dual form:

loghw, Φ(yj )i +

hw, Φ(xi )i −

J := min
w

min
w

(12)

−min
α>0

n
Xj=1

1
n −

1
n

−

log nαj +

1
2λn Xi,j

αiαj K (yi , yj )+

1
2λnn2 Xi,j

K (xi , xj )−

1
λnn Xi,j

αj K (xi , yj ).

min
w

n loghw, Φ(yj )i, and Ω(w) = λn
Proof. Let ψi (w) := 1
n hw, Φ(xi )i, ϕj (w) := − 1
H . We have
2 kwk2
(h0, wi − J (w)) = −J ∗ (0)
J = − max
w
n
n
n
n
Xj=1
Xi=1
Xj=1
Xi=1
j (vj ) + Ω∗ (−
ϕ∗
ψ∗
i (ui ) +
where the last line is due to the inf-convolution theorem [16]. Simple calculations yield:

= − min
ui ,vj

ui −

vj ),

1
1
ϕ∗
log nαj if v = −αj Φ(yj ) and + ∞ otherwise
j (v) = −
n −
n
1
ψ∗
Φ(xi ) and + ∞ otherwise
i (u) = 0 if u =
n
1
Ω∗ (v) =
2λn kvk2
H .
2λn k Pn
n Pn
So, minw J = − minαi Pn
j=1 αj Φ(yj ) − 1
n log nαj ) + 1
n − 1
j=1 (− 1
H , which
i=1 Φ(xi )k2
implies the lemma immediately.
If ˆα is solution of the dual formulation, it is not difﬁcult to sho w that the optimal ˆw is attained at
(Pn
n Pn
ˆw = 1
j=1 ˆαj Φ(yj ) − 1
i=1 Φ(xi )).
λn
For an RKHS based on a Gaussian kernel, the entropy condition (10) holds for any γ > 0 [23].
Furthermore, (9) trivially holds via the Cauchy-Schwarz inequality:
|g(x)| = |hw, Φ(x)i| ≤
kwkH kΦ(x)kH ≤ I (g)pK (x, x) ≤ I (g). Thus, by Theorem 2(a), k ˆwkH = kˆgn kH = OP (kg0 kH ),
so the penalty term λnk ˆwk2 vanishes at the same rate as λn . We have arrived at the following esti-
mator for the KL divergence:

ˆDK = 1 +

n
Xj=1

1
n −

1
n

(−

log n ˆαj ) =

n
Xj=1

1
n

−

log n ˆαj .

log G is an RKHS. Alternatively, we could set
log G to be the RKHS,
letting g(x) =
exphw, Φ(x)i, and letting I (g) = k log gkH = kwkH . Theorem 2 is not applicable in this case,
because condition (9) no longer holds, but this choice nonetheless seems reasonable and worth in-
vestigating, because in effect we have a far richer function class which might improve the bias of
our estimator when the true density ratio is not very smooth.

4

A derivation similar to the previous case yields the following convex program:

1
n

1
n

min
w

n
Xj=1

J := min
w

λn
2 kwk2
H

ehw, Φ(xi )i −

n
Xi=1
hw, Φ(yj )i +
n
n
n
Xi=1
Xj=1
Xi=1
Letting ˆα be the solution of the above convex program, the KL divergence can be estimated by:
n
Xi=1

αi log(nαi ) − αi +

= − min
α>0

ˆαi log ˆαi + ˆαi log

αiΦ(xi ) −

Φ(yj )k2
H .

ˆDK = 1 +

1
2λn k

1
n

n
e

.

5 Proof of Theorem 2

I 2 (g0 ).

h2
Q (g0 , ˆgn ) +

ˆgn + g0
2g0

We now sketch out the proof of the main theorem. The key to our analysis is the following lemma:
Lemma 4. If ˆgn is an estimate of g using (6), then:
1
λn
λn
I 2 (ˆgn ) ≤ − Z (ˆgn − g0 )d(Qn − Q) + Z 2 log
d(Pn − P) +
4
2
2
2 log x ≤ √x − 1. Thus,
Proof. Deﬁne dl (g0 , g) = R (g − g0 )dQ − log g
dP. Note that for x > 0, 1
g0
dP ≤ 2 R (g1/2 g−1/2
R log g
− 1) dP. As a result, for any g , dl is related to hQ as follows:
0
g0
dl (g0 , g) ≥ Z (g − g0 ) dQ − 2 Z (g1/2 g−1/2
− 1) dP
0
0 − g0 ) dQ = Z (g1/2 − g1/2
= Z (g − g0 ) dQ − 2 Z (g1/2 g1/2
0
By the deﬁnition (6) of our estimator, we have:
λn
λn
Z ˆgndQn − Z log ˆgndPn +
I 2 (ˆgn ) ≤ Z g0dQn − Z log g0dPn +
2
2
Both sides (modulo the regularization term I 2 ) are convex functionals of g . By Jensen’s inequality,
if F is a convex function, then F ((u + v)/2) − F (v) ≤ (F (u) − F (v))/2. We obtain:
λn
λn
ˆgn + g0
Z ˆgn + g0
dQn − Z log
I 2 (ˆgn ) ≤ Z g0dQn − Z log g0dPn +
dPn +
2
2
4
4
d(Qn − Q) − R log ˆgn+g0
Rearranging, R ˆgn−g0
d(Pn − P) + λn
4 I 2 (ˆgn ) ≤
2
2g0
dP − Z ˆgn − g0
ˆgn + g0
g0 + ˆgn
λn
Z log
I 2 (g0 ) = −dl (g0 ,
dQ +
4
2g0
2
2
g0 + ˆgn
λn
1
λn
≤ −2h2
I 2 (g0 ) ≤ −
Q (g0 ,
) +
4
8
4
2
where the last inequality is a standard result for the (generalized) Hellinger distance (cf. [20]).

)2dQ = 2h2
Q (g0 , g).

h2
Q (g0 , ˆgn ) +

I 2 (g0 ).

I 2 (g0 ).

I 2 (g0 ),

I 2 (g0 )

λn
4

) +

Let us now proceed to part (a) of the theorem. Deﬁne fg := log g+g0
, and let FM := {fg |g ∈ GM }.
2g0
Since fg is a Lipschitz function of g , conditions (8) and (10) imply that
HB
δ (FM , L2 (P)) = O(M /δ)γ .
(13)
Apply Lemma 5.14 of [20] using distance metric d2 (g0 , g) = kg − g0 kL2 (Q) , the following is true
under Q (and so true under P as well, since dP/dQ is bounded from above),
| R (g − g0 )d(Qn − Q)|
n−1/2d2 (g0 , g)1−γ /2 (1 + I (g) + I (g0 ))γ /2 ∨ n− 2
2+γ (1 + I (g) + I (g0 ))
5

= OP (1).

sup
g∈G

(14)

sup
g∈G

In the same vein, we obtain that under P measure:
| R fg d(Pn − P)|
n−1/2d2 (g0 , g)1−γ /2 (1 + I (g) + I (g0 ))γ /2 ∨ n− 2
2+γ (1 + I (g) + I (g0 ))
By condition (9), we have: d2 (g0 , g) = kg − g0kL2 (Q) ≤ 2c1/2 (1 + I (g) + I (g0 ))1/2hQ (g0 , g).
Combining Lemma 4 and Eqs. (15), (14), we obtain the following:

= OP (1).

(15)

1
4

(16)

λn
I 2 (ˆgn ) ≤ λn I (g0 )2 /2+
h2
Q (g0 , ˆgn ) +
2
2+γ (1 + I (g) + I (g0 ))¶.
OPµn−1/2hQ (g0 , g)1−γ /2 (1 + I (g) + I (g0 ))1/2+γ /4 ∨ n− 2
From this point, the proof involves simple algebraic manipulation of (16). To simplify notation, let
ˆh = hQ (g0 , ˆgn ), ˆI = I (ˆgn ), and I0 = I (g0 ). There are four possibilities:
Case a. ˆh ≥ n−1/(2+γ ) (1 + ˆI + I0 )1/2 and ˆI ≥ 1 + I0 . From (16), either
ˆh2 /4 + λn ˆI 2/2 ≤ OP (n−1/2 )ˆh1−γ /2 ˆI 1/2+γ /4 or ˆh2 /4 + λn ˆI 2/2 ≤ λn I 2
0 /2,
which implies, respectively, either
ˆh ≤ λ−1/2
ˆI ≤ λ−1
n OP (n−2/(2+γ ) ) or
n OP (n−2/(2+γ ) ),
ˆh ≤ OP (λ1/2
ˆI ≤ OP (I0 ).
n I0 ),
Both scenarios conclude the proof if we set λ−1
n = OP (n2/(γ+2) (1 + I0 )).
Case b. ˆh ≥ n−1/(2+γ ) (1 + ˆI + I0 )1/2 and ˆI < 1 + I0 . From (16), either
ˆh2/4 + λn ˆI 2/2 ≤ OP (n−1/2 )ˆh1−γ /2 (1 + I0 )1/2+γ /4 or ˆh2/4 + λn ˆI 2 /2 ≤ λn I 2
0 /2,
which implies, respectively, either
ˆh ≤ (1 + I0 )1/2OP (n−1/(γ+2) ),
ˆI ≤ 1 + I0 or
ˆh ≤ OP (λ1/2
ˆI ≤ OP (I0 ).
n I0 ),
Both scenarios conclude the proof if we set λ−1
n = OP (n2/(γ+2) (1 + I0 )).
Case c. ˆh ≤ n−1/(2+γ ) (1 + ˆI + I0 )1/2 and ˆI ≥ 1 + I0 . From (16)
ˆh2/4 + λn ˆI 2/2 ≤ OP (n−2/(2+γ ) ) ˆI ,
n OP (n−2/(2+γ ) ). This means that ˆh ≤
which implies that ˆh ≤ OP (n−1/(2+γ ) ) ˆI 1/2 and ˆI ≤ λ−1
OP (λ1/2
ˆI ≤ OP (1 + I0 ) if we set λ−1
n = OP (n2/(2+γ ) )(1 + I0 ).
n )(1 + I0 ),
Case d. ˆh ≤ n−1/(2+γ ) (1 + ˆI + I0 )1/2 and ˆI ≤ 1 + I0 . Part (a) of the theorem is immediate.
Finally, part (b) is a simple consequence of part (a) using the same argument as in Thm. 9 of [15].

6 Simulation results

In this section, we describe the results of various simulations that demonstrate the practical viability
of our estimators, as well as their convergence behavior. We experimented with our estimators
using various choices of P and Q, including Gaussian, beta, mixture of Gaussians, and multivariate
Gaussian distributions. Here we report results in terms of KL estimation error. For each of the eight
estimation problems described here, we experiment with increasing sample sizes (the sample size,
n, ranges from 100 to 104 or more). Error bars are obtained by replicating each set-up 250 times.
For all simulations, we report our estimator’s performance using the simple ﬁxed rate λn ∼ 1/n,
noting that this may be a suboptimal rate. We set the kernel width to be relatively small (σ = .1) for
one-dimension data, and larger for higher dimensions. We use M1 to denote the method in which
G is the RKHS, and M2 for the method in which log G is the RKHS. Our methods are compared to

6

0.4

0.3

0.2

0.1

0

−0.1

2.5

2

1.5

1

0.5

0

1.5

1

0.5

0

2

1.5

1

0.5

0

Estimate of KL(Beta(1,2),Unif[0,1])

0.1931
M1, σ = .1, λ = 1/n
M2, σ = .1, λ = .1/n
WKV, s = n1/2
WKV, s = n1/3

100

200

500

1000 2000

5000 10000 20000

50000

(0,1),N
Estimate of KL(N
(4,2))
t
t

1.9492
M1, σ = .1, λ = 1/n
M2, σ = .1, λ = .1/n
WKV, s = n1/3
WKV, s = n1/2
WKV, s = n2/3

100

200

500

1000

2000

5000

10000

),Unif[−3,3] 2)
(0,I
Estimate of KL(N
2
t

0.777712
M1, σ = .5, λ = .1/n
M2, σ = .5, λ = .1/n
WKV, n1/3
WKV, n1/2

100

200

500

1000

2000

5000

10000

),Unif[−3,3] 3)
(0,I
Estimate of KL(N
3
t

1.16657
M1 σ = 1, λ = .1/n1/2
M2, σ = 1, λ = .1/n
M2, σ = 1, λ = .1/n2/3
WKV, n1/3
WKV, n1/2
2000

5000

10000

100

200

500

1000

(0,1)+ 1/2 N
Estimate of KL(1/2 N
(1,1),Unif[−5,5])
t
t

0.414624
M1, σ = .1, λ = 1/n
M2, σ = 1, λ = .1/n
WKV, s = n1/3
WKV, s = n1/2
WKV, s = n2/3

100

200

500

1000

2000

5000

10000

(4,2),N
Estimate of KL(N
(0,1))
t
t

4.72006
M1, σ = 1, λ = .1/n
M2, σ = 1, λ = .1/n
WKV, s = n1/4
WKV, s = n1/3
WKV, s = n1/2
2000
5000

10000

100

200

500

1000

(1,I
),N
(0,I
Estimate of KL(N
))
2
t
2
t

0.959316
M1, σ = .5, λ = .1/n
M2, σ = .5, λ = .1/n
WKV, n1/3
WKV, n1/2

100

200

500

1000

2000

5000

10000

(1,I
),N
(0,I
Estimate of KL(N
))
3
t
3
t

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

6

5

4

3

2

1

0

−1

−2

1.5

1

0.5

0

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.43897
M1, σ = 1, λ = .1/n
M2, σ = 1, λ = .1/n
WKV, n1/2
WKV, n1/3
2000
5000

10000

−0.2

100

200

500

1000

Figure 1. Results of estimating KL divergences for various choices of probability distributions. In all
plots, the X-axis is the number of data points plotted on a log scale, and the Y-axis is the estimated
value. The error bar is obtained by replicating the experiment 250 times. Nt (a, Ik ) denotes a truncated
normal distribution of k dimensions with mean (a, . . . , a) and identity covariance matrix.

7

algorithm A in Wang et al [22], which was shown empirically to be one of the best methods in the
literature. Their method, denoted by WKV, is based on data-dependent partitioning of the covariate
space. Naturally, the performance of WKV is critically dependent on the amount s of data allocated
to each partition; here we report results with s ∼ nγ , where γ = 1/3, 1/2, 2/3.
The ﬁrst four plots present results with univariate distrib utions. In the ﬁrst two, our estimators M 1
and M 2 appear to have faster convergence rate than WKV. The WKV estimator performs very well
in the third example, but rather badly in the fourth example. The next four plots present results with
two and three dimensional data. Again, M1 has the best convergence rates in all examples. The
M2 estimator does not converge in the last example, suggesting that the underlying function class
exhibits very strong bias. The WKV methods have weak convergence rates despite different choices
of the partition sizes. It is worth noting that as one increases the number of dimensions, histogram
based methods such as WKV become increasingly difﬁcult to imp lement, whereas increasing di-
mension has only a mild effect on our method.

References

[1] S. M. Ali and S. D. Silvey. A general class of coefﬁcients of diver gence of one distribution from another.
J. Royal Stat. Soc. Series B, 28:131–142, 1966.
[2] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁ cation, and risk bounds. Journal of the
American Statistical Association, 101:138–156, 2006.
[3] P. Bickel and Y. Ritov. Estimating integrated squared density derivatives: Sharp best order of convergence
estimates. Sankhy¯a Ser. A, 50:381–393, 1988.
[4] L. Birg ´e and P. Massart. Estimation of integral functionals of a density. Ann. Statist., 23(1):11–29, 1995.
[5] M. Broniatowski and A. Keziou. Parametric estimation and tests through divergences. Technical report,
LSTA, Universit ´e Pierre et Marie Curie, 2004.
[6] I. Csisz ´ar. Information-type measures of difference of probability distributions and indirect observation.
Studia Sci. Math. Hungar, 2:299–318, 1967.
[7] L. Gyor ﬁ and E.C. van der Meulen. Density-free convergence p roperties of various estimators of entropy.
Computational Statistics and Data Analysis, 5:425–436, 1987.
[8] P. Hall and S. Morton. On estimation of entropy. Ann. Inst. Statist. Math., 45(1):69–88, 1993.
[9] I. A. Ibragimov and R. Z. Khasminskii. On the nonparametric estimation of functionals. In Symposium
in Asymptotic Statistics, pages 41–52, 1978.
[10] H. Joe. Estimation of entropy and other functionals of a multivariate density. Ann. Inst. Statist. Math.,
41:683–697, 1989.
[11] A. Keziou. Dual representation of φ-divergences and applications. C. R. Acad. Sci. Paris, Ser. I 336,
pages 857–862, 2003.
[12] B. Laurent. Efﬁcient estimation of integral functionals of a density. Ann. Statist., 24(2):659–681, 1996.
[13] B. Ya. Levit. Asymptotically efﬁcient estimation of nonlinear functiona ls. Problems Inform. Transmis-
sion, 14:204–209, 1978.
[14] X. Nguyen, M. J. Wainwright, and M. I. Jordan. On divergences, surrogate losses and decentralized
detection. Technical Report 695, Dept of Statistics, UC Berkeley, October 2005.
[15] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Nonparametric estimation of the likelihood ratio and
divergence functionals. In International Symposium on Information Theory (ISIT), 2007.
[16] G. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970.
[17] S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman, Harlow, UK, 1988.
[18] B. Sch ¨olkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[19] F. Topsoe. Some inequalities for information divergence and related measures of discrimination. IEEE
Transactions on Information Theory, 46:1602–1609, 2000.
[20] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.
[21] A. W. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag, New
York, NY, 1996.
[22] Q. Wang, S. R. Kulkarni, and S. Verd ´u. Divergence estimation of continuous distributions based on
data-dependent partitions. IEEE Transactions on Information Theory, 51(9):3064–3074, 2005.
[23] D. X. Zhou. The covering number in learning theory. Journal of Complexity, 18:739–767, 2002.

8


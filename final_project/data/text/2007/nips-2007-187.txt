Bayesian Agglomerative Clustering with Coalescents

Yee Whye Teh
Gatsby Unit
University College London
ywteh@gatsby.ucl.ac.uk

Hal Daum ´e III
School of Computing
University of Utah
me@hal3.name

Daniel Roy
CSAIL
MIT
droy@mit.edu

Abstract

We introduce a new Bayesian model for hierarchical clustering based on a prior
over trees called Kingman’s coalescent. We develop novel greedy and sequential
Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We
show experimentally the superiority of our algorithms over the state-of-the-art,
and demonstrate our approach in document clustering and phylolinguistics.

1

Introduction

Hierarchically structured data abound across a wide variety of domains. It is thus not surprising that
hierarchical clustering is a traditional mainstay of machine learning [1]. The dominant approach to
hierarchical clustering is agglomerative: start with one cluster per datum, and greedily merge pairs
until a single cluster remains. Such algorithms are efﬁcient and easy to implement. Their primary
limitations—a lack of predictive semantics and a coherent mechanism to deal with missing data—
can be addressed by probabilistic models that handle partially observed data, quantify goodness-of-
ﬁt, predict on new data, and integrate within more complex models, all in a principled fashion.
Currently there are two main approaches to probabilistic models for hierarchical clustering. The
ﬁrst takes a direct Bayesian approach by deﬁning a prior over trees followed by a distribution over
data points conditioned on a tree [2, 3, 4, 5]. MCMC sampling is then used to obtain trees from
their posterior distribution given observations. This approach has the advantages and disadvantages
of most Bayesian models: averaging over sampled trees can improve predictive capabilities, give
conﬁdence estimates for conclusions drawn from the hierarchy, and share statistical strength across
the model; but it is also computationally demanding and complex to implement. As a result such
models have not found widespread use. [2] has the additional advantage that the distribution induced
on the data points is exchangeable, so the model can be coherently extended to new data. The
second approach uses a ﬂat mixture model as the underlying probabilistic model and structures the
posterior hierarchically [6, 7]. This approach uses an agglomerative procedure to ﬁnd the tree giving
the best posterior approximation, mirroring traditional agglomerative clustering techniques closely
and giving efﬁcient and easy to implement algorithms. However because the underlying model has
no hierarchical structure, there is no sharing of information across the tree.
We propose a novel class of Bayesian hierarchical clustering models and associated inference algo-
rithms combining the advantages of both probabilistic approaches above. 1) We deﬁne a prior and
compute the posterior over trees, thus reaping the beneﬁts of a fully Bayesian approach; 2) the dis-
tribution over data is hierarchically structured allowing for sharing of statistical strength; 3) we have
efﬁcient and easy to implement inference algorithms that construct trees agglomeratively; and 4) the
induced distribution over data points is exchangeable. Our model is based on an exchangeable distri-
bution over trees called Kingman’s coalescent [8, 9]. Kingman’s coalescent is a standard model from
population genetics for the genealogy of a set of individuals. It is obtained by tracing the genealogy
backwards in time, noting when lineages coalesce together. We review Kingman’s coalescent in
Section 2. Our own contribution is in using it as a prior over trees in a hierarchical clustering model
(Section 3) and in developing novel inference procedures for this model (Section 4).

Figure 1: (a) Variables describing the n-coalescent. (b) Sample path from a Brownian diffusion
coalescent process in 1D, circles are coalescent points. (c) Sample observed points from same in
2D, notice the hierarchically clustered nature of the points.

2 Kingman’s coalescent

Kingman’s coalescent is a standard model in population genetics describing the common genealogy
(ancestral tree) of a set of individuals [8, 9]. In its full form it is a distribution over the genealogy of
a countably inﬁnite set of individuals. Like other nonparametric models (e.g. Gaussian and Dirich-
let processes), Kingman’s coalescent is most easily described and understood in terms of its ﬁnite
dimensional marginal distributions over the genealogies of n individuals, called n-coalescents. We
obtain Kingman’s coalescent as n → ∞.
Consider the genealogy of n individuals alive at the present time t = 0. We can trace their ancestry
backwards in time to the distant past t = −∞. Assume each individual has one parent (in genetics,
haploid organisms), and therefore genealogies of [n] = {1, . . . , n} form a directed forest. In general,
at time t ≤ 0, there are m (1 ≤ m ≤ n) ancestors alive. Identify these ancestors with their correspond-
ing sets ρ1 , . . . , ρm of descendants (we will make this identiﬁcation throughout the paper). Note that
π(t) = {ρ1 , . . . , ρm} form a partition of [n], and interpret t (cid:55)→ π(t) as a function from (−∞, 0] to the
set of partitions of [n]. This function is piecewise constant, left-continuous, monotonic (s ≤ t implies
that π(t) is a reﬁnement of π(s)), and π(0) = {{1}, . . . , {n}} (see Figure 1a). Further, π completely
and succinctly characterizes the genealogy; we shall henceforth refer to π as the genealogy of [n].
Kingman’s n-coalescent is simply a distribution over genealogies of [n], or equivalently, over the
space of partition-valued functions like π . More speciﬁcally, the n-coalescent is a continuous-time,
partition-valued, Markov process, which starts at {{1}, . . . , {n}} at present time t = 0, and evolves
backwards in time, merging (coalescing) lineages until only one is left. To describe the Markov
process in its entirety, it is sufﬁcient to describe the jump process (i.e. the embedded, discrete-time,
Markov chain over partitions) and the distribution over coalescent times. Both are straightforward
and their simplicity is part of the appeal of Kingman’s coalescent. Let ρli , ρri be the ith pair of
lineages to coalesce, tn−1 < · · · < t1 < t0 = 0 be the coalescent times and δi = ti−1 − ti > 0 be the
rate (cid:0)m
(cid:1) = m(m−1)
. Therefore δi ∼ Exp (cid:0)(cid:0)n−i+1
(cid:1)(cid:1) independently, the pair ρli , ρri is chosen from
duration between adjacent events (see Figure 1a). Under the n-coalescent, every pair of lineages
merges independently with exponential rate 1. Thus the ﬁrst pair amongst m lineages merge with
2
2
2
among those right after time ti , and with probability one a random draw from the n-coalescent is a
{{1}, . . . , {n}}
binary tree with a single root at t = −∞ and the n individuals at time t = 0. The genealogy is:
if t = 0;
πti−1 − ρli − ρri + (ρli ∪ ρri )
π(t) =
if t = ti ;
πti
if ti+1 < t < ti .
(cid:1)
(cid:1)δi
exp (cid:0)−(cid:0)n−i+1
(cid:1) = (cid:81)n−1
(cid:1) /(cid:0)n−i+1
(cid:1) exp (cid:0)−(cid:0)n−i+1
(cid:0)n−i+1
(cid:1)δi
p(π) = (cid:81)n−1
Combining the probabilities of the durations and choices of lineages, the probability of π is simply:
(2)
i=1
i=1
2
2
2
2
The n-coalescent has some interesting statistical properties [8, 9]. The marginal distribution over
tree topologies is uniform and independent of the coalescent times. Secondly, it is inﬁnitely ex-
changeable: given a genealogy drawn from an n-coalescent, the genealogy of any m contemporary
individuals alive at time t ≤ 0 embedded within the genealogy is a draw from the m-coalescent.
Thus, taking n → ∞, there is a distribution over genealogies of a countably inﬁnite population
for which the marginal distribution of the genealogy of any n individuals gives the n-coalescent.
Kingman called this the coalescent.

(1)

!!"#!!"$!!"%!!!&"’!&"#!&"$!&"%&!(!%")!%!!")!!!&")&&")!!")t1t2t3−∞t0=0δ1δ2δ3x1x2x3x4y{1,2}y{3,4}y{1,2,3,4}z{{1,2,3,4}}{{1,2},{3,4}}{{1},{2},{3},{4}}{{1},{2},{3,4}}π(t)=!!!"!#!$%$!#!$&’!$!%&’%%&’$$&’##&’(a)(b)(c)t3 Hierarchical clustering with coalescents

We take a Bayesian approach to hierarchical clustering, placing a coalescent prior on the latent
tree and modeling the observed data with a tree structured Markov process evolving forward in
time. We will alter our terminology from genealogy to tree, from n individuals at present time to n
observed data points, and from individuals on the genealogy to latent variables on the tree-structured
distribution. Let x = {x1 , . . . , xn} be n observed data points at the leaves of a tree π drawn from
the n-coalescent. π has n − 1 coalescent points, the ith occuring when ρli and ρri merge at time ti
to form ρi = ρli ∪ ρri . Let tli and tri be the times at which ρli and ρri are themselves formed.
We use a continuous-time Markov process to deﬁne the distribution over the n data points x given
the tree π . The Markov process starts in the distant past, evolves forward in time, splits at each
coalescent point, and evolves independently down both branches until we reach time 0, when n data
points are observations of the process at the n leaves of the tree. The joint distribution described by
this process respects the conditional independences implied by the structure of the directed tree π .
Let yρi be a latent variable that takes on the value of the Markov process at ρi just before it splitsLet
y{i} = xi at leaf i. See Figure 1a.
To complete the description of the likelihood model, let q(z ) be the initial distribution of the Markov
process at time t = −∞, and kst (x, y) be the transition probability from state x at time s to state y
) (cid:81)n−1
at time t. This Markov process need be neither stationary nor ergodic. Marginalizing over paths of
the Markov process, the joint probability over the latent variables and the observations is:
p(x, y, z |π) = q(z )k−∞ tn−1
(z , yρn−1
(yρi
)kti tri
(yρi
)
kti tli
, yρli
, yρri
(3)
i=1
Notice that the marginal distributions for each observation p(xi |π) are identical and given by the
Markov process at time 0. However the observations are not independent as they share the same
sample path down the Markov process until it splits. In fact the amount of dependence between two
observations is a function of the time at which the observations coalesce. A more recent coalescent
time implies larger dependence. The overall distribution induced on the observations p(x) inherits
the inﬁnite exchangeability of the n-coalescent. We consider in Section 4.3 a brownian diffusion
(Figures 1(b,c)) and a simple independent sites mutation process on multinomial vectors.

4 Agglomerative sequential Monte Carlo and greedy inference

We develop two classes of efﬁcient and easily implementable inference algorithms for our hierar-
chical clustering model based on sequential Monte Carlo (SMC) and greedy schemes respectively.
In both classes, the latent variables are integrated out, and the trees are constructed in a bottom-up
fashion. The full tree π can be expressed as a series of n − 1 coalescent events, ordered backwards
in time. The ith coalescent event involves the merging of the two subtrees with leaves ρli and ρri
and occurs at a time δi before the previous coalescent event. Let θi = {δj , ρlj , ρrj for j ≤ i} denote
the ﬁrst i coalescent events. θn−1 is equivalent to π and we shall use them interchangeably.
We assume that the form of the Markov process is such that the latent variables {yρi }n−1
i=1 and z can
(y) be
be efﬁciently integrated out using an upward pass of belief propagation on the tree. Let Mρi
the message passed from yρi to its parent; M{i} (y) = δxi
(y) is point mass at xi for leaf i. Mρi
(y)
(cid:82) kti tbi
(x, θi ) (cid:81)
is proportional to the likelihood of the observations at the leaves below coalescent event i, given that
= y . Belief propagation computes the messages recursively up the tree; for i = 1, . . . , n − 1:
yρi
(y) = Z −1
(y , yb )Mρbi
(yb ) dyb
Mρi
(4)
ρi
b=l,r
(x, θi ) = (cid:82)(cid:82) q(z )k−∞ti
(x, θi ) is a normalization constant.
where Zρi
The choice of Z does not affect the computed
Z−∞ (x, θn−1 ) = (cid:82)(cid:82) q(z )k−∞ tn−1
probability of x, but does impact the accuracy and efﬁciency of our inference algorithms. We found
(y) dy dz worked well. At the root, we have:
(z , y)Mρi
that Zρi
(z , y)Mρn−1
(y) dy dz
p(x|π) = Z−∞ (x, θn−1 ) (cid:81)n−1
The marginal probability p(x|π) is now given by the product of normalization constants:
(x, θi )
Zρi
(6)
exp (cid:0)−(cid:0)n−i+1
(cid:1)δi
(cid:1) Zρi
p(x, π) = Z−∞ (x, θn−1 ) (cid:81)n−1
i=1
Multiplying in the prior (2) over π , we get the joint probability for the tree π and observations x:
(x, θi )
(7)
i=1
2

(5)

Our inference algorithms are based upon (7). The sequential Monte Carlo (SMC) algorithms approx-
imate the posterior over the tree θn−1 using a weighted sum of samples, while the greedy algorithms
(cid:1)δi
based upon the ith term in (7), interpreted as the product of a local prior exp (cid:0)−(cid:0)n−i+1
(cid:1) and a
construct θn−1 by maximizing local terms in (7). Both proceeds by iterating over i = 1, . . . , n − 1,
choosing a duration δi and a pair of subtrees ρli , ρri to coalesce at each iteration. This choice is
2
(x, θi ) for choosing δi , ρli and ρri given θi−1 .
local likelihood Zρi

4.1 Sequential Monte Carlo algorithms

)

(8)

SMC algorithms approximate the posterior by iteratively constructing a weighted sum of point
masses. At iteration i − 1, particle s consists of θs
= {δ s
rj for j < i}, and has weight
, ρs
, ρs
i−1
j
lj
i−1 . At iteration i, s is extended by sampling δ s
ws
i , ρs
li and ρs
ri from a proposal distribution
(cid:1) Zρi
(cid:1)δ s
exp (cid:0)−(cid:0)n−i+1
ri |θs
fi (δ s
), and the weight is updated by:
, ρs
, ρs
i−1
i
li
ri |θs
)/fi (δ s
(x, θs
= ws
, ρs
, ρs
ws
is approximated by: p(π , x) ≈ (cid:80)
i−1
i−1
i
i
i
li
i
2
After n − 1 iterations, we obtain a set of trees θs
n−1 and weights ws
n−1 . The joint distribution
(π), while the posterior is approximated with the
ws
δθs
n−1
n−1
s
weights normalized. An important aspect of SMC is resampling, which places more particles in
high probability regions and prunes particles stuck in low probability regions. We resample as in
(cid:1) and ρs
i is drawn from an exponential with rate (cid:0)n−i+1
Algorithm 5.1 of [10] when the effective sample size ratio as estimated in [11] falls below one half.
SMC-PriorPrior. The simplest proposal distribution is to sample δ s
i , ρs
li and ρs
ri from the local
prior. δ s
, ρs
ri are drawn uniformly from
li
2
(x, θs
). This approach is
all available pairs. The weight updates (8) reduce to multiplying by Zρi
i
computationally very efﬁcient, but performs badly with many objects due to the uniform draws over
pairs. SMC-PriorPost. The second approach addresses the suboptimal choice of pairs to coalesce.
(cid:80)
i from its local prior, then draw ρs
li , ρs
We ﬁrst draw δ s
ri from the local posterior:
ri |δ s
) ∝ Zρi
= ws
); ws
(x, θs
fi (ρs
(x, θs
, ρs
, ρs
, δs
, θs
, ρs
Zρi
i−1
i−1
i−1
i−1
ρ(cid:48)
l ,ρ(cid:48)
i
i
ri
i
li
li
r
This approach is more computationally demanding since we need to evaluate the local likelihood of
every pair. It also performs signiﬁcantly better than SMC-PriorPrior. We have found that it works
reasonably well for small data sets but fails in larger ones for which the local posterior for δi is highly
(cid:1)δ s
(cid:1) Zρi
) ∝ exp (cid:0)−(cid:0)n−i+1
li and ρs
peaked. SMC-PostPost. The third approach is to draw all of δ s
i , ρs
ri from their posterior:
(cid:82) exp (cid:0)−(cid:0)n−i+1
(cid:1)δ (cid:48) (cid:1) Zρi
(cid:80)
ri |θs
fi (δ s
)
(x, θs
, ρs
, ρs
, δs
, ρs
i−1
i−1
i
i
i
ri
li
2
= ws
(x, θs
ws
i−1
i−1
ρ(cid:48)
l ,ρ(cid:48)
i
2
r
This approach requires the fewest particles, but is the most computationally expensive due to the
integral for each pair. Fortunately, for the case of Brownian diffusion process described below, these
integrals are tractable and related to generalized inverse Gaussian distributions.

, δ (cid:48) , ρ(cid:48)
l

) (9)

) dδ (cid:48)

, δs
i

, ρ(cid:48)
l

, ρ(cid:48)
r

, ρ(cid:48)
r

(10)

, ρs
li

4.2 Greedy algorithms

SMC algorithms are attractive because they can produce an arbitrarily accurate approximation to the
full posterior as the number of samples grow. However in many applications a single good tree is
often sufﬁcient. We describe a few greedy algorithms to construct a good tree.
Greedy-MaxProb: the obvious greedy algorithm is to pick δi , ρli and ρri maximizing the ith term
in (7). We do so by computing the optimal δi for each pair of ρli , ρri , and then picking the pair
pair at each iteration, since the prior rate (cid:0)n−i+1
(cid:1) on the duration varies with the iteration i. The total
maximizing the ith term at its optimal δi . Greedy-MinDuration: pick the pair to coalesce whose
optimal duration is minimum. Both algorithms require recomputing the optimal duration for each
ρli and ρri we determine the optimal δi , replacing the (cid:0)n−i+1
(cid:1) prior rate with 1. We coalesce the
2
computational cost is thus O(n3 ). We can avoid this by using the alternative view of the n-coalesent
as a Markov process where each pair of lineages coalesces at rate 1. Greedy-Rate1: for each pair
2
pair with most recent time (as in Greedy-MinDuration). This reduces the complexity to O(n2 ). We
found that all three performed similarly, and use Greedy-Rate1 in our experiments as it is faster.

4.3 Examples

Brownian diffusion. Consider the case of continuous data evolving via Brownian diffusion. The
transition kernel kst (y , ·) is a Gaussian centred at y with variance (t − s)Λ, where Λ is a symmetric
(y) as a Gaussian with mean (cid:98)yρi and covariance Λvρi . The local
positive deﬁnite covariance matrix. Because the joint distribution (3) over x, y and z is Gaussian,
2 exp(cid:0)− 1
(cid:1);
we can express each message Mρi
(x, θi ) = |2π (cid:98)Λi |− 1
(cid:98)Λi = Λ(vρli
2 ||(cid:98)yρli − (cid:98)yρri ||2bΛi
likelihood is:
+ tli + tri − 2ti )
+ vρri
Zρi
(11)
(cid:1)−1(cid:16)(cid:113)4(cid:0)n−i+1
(cid:17) − 1
(cid:1) ||(cid:98)yρli − (cid:98)yρri ||2
(cid:0)n−i+1
where (cid:107)x(cid:107)Ψ = x(cid:62)Ψ−1x is the Mahanalobis norm. The optimal duration δi can also be solved for,
+D2 − D
+ tli + tri − 2ti−1 ) (12)
(vρli
+ vρri
δi = 1
2
2
Λ
4
2
= (cid:0)
+ tri − ti )−1 (cid:1)−1 ; (cid:98)yρi
= (cid:0)(vρli
(cid:1)vρi
where D is the dimensionality. The message at the newly coalesced point has parameters:
byρli
byρri
+ tli − ti )−1 + (vρri
+
vρli +tli−ti
vρri +tri−ti
Multinomial vectors. Consider a Markov process acting on multinomial vectors with each entry
taking one of K values and evolving independently. Entry d evolves at rate λd and has equilibrium
111K − Ik ) where 111K is a vector of
distribution vector qd . The transition rate matrix is Qd = λd (q(cid:62)
h
K ones and IK is identity matrix of size K , while the transition probability matrix for entry d in
a time interval of length t is eQd t = e−λd t IK + (1 − e−λd t )q(cid:62)
111K . Representing the message for
](cid:62) , normalized so that qd · M d
d
= [M d1
= 1,
, . . . , M dK
entry d from ρi to its parent as a vector M d
(x, θi ) = 1 − eλh (2ti−tli−tri ) (cid:0)1 − (cid:80)K
(cid:1)
ρi
ρi
ρi
ρi
the local likelihood terms and messages are computed as,
qdkM dk
M dk
(14)
ρli
ρri
k=1
= (1 − eλd (ti−tli ) (1 − M d
))(1 − eλd (ti−tri ) (1 − M d
))/Z d
(x, θi )
M d
(15)
ρi
ρli
ρi
ρri
Unfortunately the optimal δi cannot be solved analytically and we use Newton steps to compute it.

(13)

vρi

Z d
ρi

4.4 Hyperparameter estimation

We perform hyperparameter estimation by iterating between estimating a tree, and estimating the
In the Brownian case, we place an inverse Wishart prior on Λ and the MAP
hyperparameters.
posterior ˆΛ is available in a standard closed form. In the multinomial case, the updates are not
available analytically and are solved iteratively. Further information on hyperparameter estimation,
as well predictive densities and more experiments are available in a longer technical report.

5 Experiments

Synthetic Data Sets. In Figure 2 we compare the various SMC algorithms and Greedy-Rate1 on a
range of synthetic data sets drawn from the Brownian diffusion coalescent process itself (Λ = ID )
to investigate the effects of various parameters on the efﬁcacy of the algorithms1 . Generally SMC-
PostPost performed best, followed by SMC-PriorPost, SMC-PriorPrior and Greedy-Rate1. With
increasing D the amount of data given to the algorithms increases and all algorithms do better,
especially Greedy-Rate1. This is because the posterior becomes concentrated and the Greedy-Rate1
approximation corresponds well with the posterior. As n increases, the amount of data increases
as well and all algorithms perform better. However, the posterior space also increases and SMC-
PriorPrior which simply samples from the prior over genealogies does not improve as much. We
see this effect as well when S is small. As S increases all SMC algorithms improve. Finally, the
algorithms were surprisingly robust when there is mismatch between the generated data sets’ λ and
the λ used by the model. We expected all models to perform worse with SMC-PostPost best able to
maintain its performance (though this is possibly due to our experimental setup).
MNIST and SPAMBASE. We compare the performance of Greedy-Rate1 to two other hierarchical
clustering algorithms: average-linkage and Bayesian hierarchical clustering (BHC) [6]. In MNIST,

1Each panel was generated from independent runs. Data set variance affected all algorithms, varying overall
performance across panels. However, trends in each panel are still valid, as they are based on the same data.

Figure 2: Predictive performance of algorithms as we vary (a) the numbers of dimensions D , (b)
observations n, (c) the mutation rate λ (Λ = λID ), and (d) number of samples S . In each panel
other parameters are ﬁxed to their middle values (we used S = 50) in other panels, and we report
log predictive probabilities on one unobserved entry, averaged over 100 runs.

MNIST
Coalescent
Avg-link
BHC
.363± .004 .392± .006 .412± .006
.581± .005 .579± .005 .610± .005
.755± .005 .763± .005 .773± .005

SPAMBASE
Coalescent
Avg-link
BHC
.616± .007 .711± .010 .689± .008
.661± .012
.549± .015
.607± .011
.846± .010
.832± .010
.861± .008

Purity
Subtree
LOO-acc

Table 1: Comparative results. Numbers are averages and standard errors over 50 and 20 repeats.

we use 20 exemplars from each of 10 digits from the MNIST data set, reduced via PCA to 20
dimensions, repeating the experiment 50 times. In SPAMBASE, we use 100 examples of 57 binary
attributes from each of 2 classes, repeating 20 times. We present purity scores [6], subtree scores
(#{interior nodes with all leaves of same class}/(n − #classes)) and leave-one-out accuracies (all
scores between 0 and 1, higher better). The results are in Table 1; except for purity on SPAMBASE,
ours gives the best performance. Experiments not presented here show that all greedy algorithms
perform about the same and that performance improves with hyperparameter updates.
Phylolinguistics. We apply Greedy-Rate1 to a phylolinguistic problem: language evolution. Un-
like previous research [12] which studies only phonological data, we use a full typological database
of 139 binary features over 2150 languages: the World Atlas of Language Structures (WALS) [13].
The data is sparse: about 84% of the entries are unknown. We use the same version of the database
as extracted by [14]. Based on the Indo-European subset of this data for which at most 30 features
are unknown (48 languages total), we recover the coalescent tree shown in Figure 3(a). Each lan-
guage is shown with its genus, allowing us to observe that it teases apart Germanic and Romance
languages, but makes a few errors with respect to Iranian and Greek.
Next we compare predictive abilities to other algo-
rithms. We take a subset of WALS and tested on
5% of withheld entries, restoring these with var-
ious techniques: Greedy-Rate1; nearest neighbors
(use value from nearest observed neighbor); average-
linkage (nearest neighbor in the tree); and probabilistic
PCA (latent dimensions in 5, 10, 20, 40, chosen opti-
mistically). We use ﬁve subsets of the WALS database,
obtained by sorting both the languages and features of
the database according to sparsity and using a varying
percentage (10% − 50%) of the densest portion. The
results are in Figure 3(b). Our approach performed
reasonably well.
Finally, we compare the trees generated by Greedy-Rate1 with trees generated by either average-
linkage or BHC, using the same evaluation criteria as for MNIST and SPAMBASE, using language
genus as classes. The results are in Table 5, where we can see that the coalescent signiﬁcantly
outperforms the other methods.

Indo-European Data
Avg-link BHC Coalescent
0.813
0.491
0.510
0.414
0.414
0.690
0.538
0.769
0.590
Whole World Data
Avg-link BHC Coalescent
0.269
0.160
0.162
0.227
0.099
0.177
0.369
0.248
0.080

Table 2: Comparative performance of var-
ious algorithms on phylolinguistics data.

Purity
Subtree
LOO-acc

Purity
Subtree
LOO-acc

468−1.6−1.4−1.2−1−0.8−0.6(a)averagelogpredictiveD:dimensions468−1.6−1.4−1.2−1−0.8−0.6(b)n:observations0.512−1.6−1.4−1.2−1−0.8−0.6(c)λ:mutationrate10305070−1.6−1.4−1.2−1−0.8−0.6(d)S:particles  SMC−PostPostSMC−PriorPostSMC−PriorPriorGreedy−Rate1(b) Data restoration on WALS. Y-axis is accuracy;
X-axis is percentage of data set used in experiments.
At 10%, there are N = 215 languages, H = 14
features and p = 94% observed data; at 20%, N =
430, H = 28 and p = 80%; at 30%: N = 645,
H = 42 and p = 66%; at 40%: N = 860, H =
56 and p = 53%; at 50%: N = 1075, H = 70
and p = 43%. Results are averaged over ﬁve folds
with a different 5% hidden each time. (We also tried
a “mode” prediction, but its performance is in the
60% range in all cases, and is not depicted.)

(a) Coalescent for a subset of Indo-European lan-
guages from WALS.

Figure 3: Results of the phylolinguistics experiments.

LLR (t) Top Words
Top Authors (# papers)
32.7 (-2.71) bifurcation attractors hopﬁeld network saddle Mjolsness (9) Saad (9) Ruppin (8) Coolen (7)
0.106 (-3.77) voltage model cells neurons neuron
Koch (30) Sejnowski (22) Bower (11) Dayan (10)
83.8 (-2.02) chip circuit voltage vlsi transistor
Koch (12) Alspector (6) Lazzaro (6) Murray (6)
140.0 (-2.43) spike ocular cells ﬁring stimulus
Sejnowski (22) Koch (18) Bower (11) Dayan (10)
2.48 (-3.66) data model learning algorithm training
Jordan (17) Hinton (16) Williams (14) Tresp (13)
31.3 (-2.76) infomax image ica images kurtosis
Hinton (12) Sejnowski (10) Amari (7) Zemel (7)
31.6 (-2.83) data training regression learning model
Jordan (16) Tresp (13) Smola (11) Moody (10)
39.5 (-2.46) critic policy reinforcement agent controller
Singh (15) Barto (10) Sutton (8) Sanger (7)
23.0 (-3.03) network training units hidden input
Mozer (14) Lippmann (11) Giles (10) Bengio (9)

Table 3: Nine clusters discovered in NIPS abstracts data.

NIPS. We applied Greedy-Rate1 to all NIPS abstracts through NIPS12 (1740, total). The data was
preprocessed so that only words occuring in at least 100 abstracts were retained. The word counts
were then converted to binary. We performed one iteration of hyperparameter re-estimation. In
the supplemental material, we depict the top levels of the coalescent tree. Here, we use the tree to
generate a ﬂat clustering. To do so, we use the log likelihood ratio at each branch in the coalescent
to determine if a split should occur. If the log likelihood ratio is greater than zero, we break the
branch; otherwise, we recurse down. On the NIPS abstracts, this leads to nine clusters, depicted
in Table 3. Note that clusters two and three are quite similar—had we used a slighly higher log
likelihood ratio, they would have been merged (the LLR for cluster 2 was only 0.105). Note that
the clustering is able to tease apart Bayesian learning (cluster 5) and non-bayesian learning (cluster
7)—both of which have Mike Jordan as their top author!

6 Discussion

We described a new model for Bayesian agglomerative clustering. We used Kingman’s coalescent
as our prior over trees, and derived efﬁcient and easily implementable greedy and SMC inference
algorithms for the model. We showed empirically that our model gives better performance than other

00.10.2[Armenian]  Armenian (Eastern)[Armenian]  Armenian (Western)[Indic]  Bengali[Indic]  Marathi[Indic]  Maithili[Iranian]  Ossetic[Indic]  Nepali[Indic]  Sinhala[Indic]  Kashmiri[Indic]  Hindi[Indic]  Panjabi[Iranian]  Pashto[Slavic]  Czech[Baltic]  Latvian[Baltic]  Lithuanian[Slavic]  Russian[Slavic]  Ukrainian[Slavic]  Serbian−Croatian[Slavic]  Slovene[Slavic]  Polish[Albanian]  Albanian[Romance]  Catalan[Romance]  Italian[Romance]  Portuguese[Romance]  Romanian[Slavic]  Bulgarian[Greek]  Greek (Modern)[Romance]  Spanish[Germanic]  Danish[Germanic]  Norwegian[Germanic]  Swedish[Germanic]  Icelandic[Germanic]  English[Germanic]  Dutch[Germanic]  German[Romance]  French[Iranian]  Kurdish (Central)[Iranian]  Persian[Iranian]  Tajik[Celtic]  Breton[Celtic]  Cornish[Celtic]  Welsh[Celtic]  Gaelic (Scots)[Celtic]  Irish0.10.20.30.40.5727476788082  CoalescentNeighborAgglomerativePPCAagglomerative clustering algorithms, and gives good results on applications to document modeling
and phylolinguistics.
Our model is most similar in spirit to the Dirichlet diffusion tree of [2]. Both use inﬁnitely ex-
changeable priors over trees. While [2] uses a fragmentation process for trees, our prior uses the
reverse—a coalescent process instead. This allows us to develop simpler inference algorithms than
those in [2] (we have not compared our model against the Dirichlet diffusion tree due to the com-
plexity of implementing it). It will be interesting to consider the possibility of developing similar
agglomerative style algorithms for [2]. [3] also describes a hierarchical clustering model involving
a prior over trees, but his prior is not inﬁnitely exchangeable. [5] uses tree-consistent partitions to
model relational data; it would be interesting to apply our approach to their setting. Another related
work is the Bayesian hierarchical clustering of [6], which uses an agglomerative procedure returning
a tree structured approximate posterior for a Dirichlet process mixture model. As opposed to our
work [6] uses a ﬂat mixture model and does not have a notion of distributions over trees.
There are a number of unresolved issues with our work. Firstly, our algorithms take O(n3 ) compu-
tation time, except for Greedy-Rate1 which takes O(n2 ) time. Among the greedy algorithms we see
that there are no discernible differences in quality of approximation thus we recommend Greedy-
Rate1. It would be interesting to develop SMC algorithms with O(n2 ) runtime, and compare these
against Greedy-Rate1 on real world problems. Secondly, there are unanswered statistical questions.
For example, since our prior is inﬁnitely exchangeable, by de Finetti’s theorem there is an underly-
ing random distribution for which our observations are i.i.d. draws. What is this underlying random
distribution, and how do samples from this distribution look like? We know the answer for at least a
simple case: if the Markov process is a mutation process with mutation rate α/2 and new states are
drawn i.i.d. from a base distribution H , then the induced distribution is a Dirichlet process DP(α, H )
[8]. Another issue is that of consistency—does the posterior over random distributions converge to
the true distribution as the number of observations grows? Finally, it would be interesting to gen-
eralize our approach to varying mutation rates, and to non-binary trees by using generalizations to
Kingman’s coalescent called Λ-coalescents [15].

References
[1] R. O. Duda and P. E. Hart. Pattern Classiﬁcation And Scene Analysis. Wiley and Sons, New York, 1973.
[2] R. M. Neal. Deﬁning priors for distributions using Dirichlet diffusion trees. Technical Report 0104,
Department of Statistics, University of Toronto, 2001.
[3] C. K. I. Williams. A MCMC approach to hierarchical mixture modelling. In Advances in Neural Infor-
mation Processing Systems, volume 12, 2000.
[4] C. Kemp, T. L. Grifﬁths, S. Stromsten, and J. B. Tenenbaum. Semi-supervised learning with trees. In
Advances in Neural Information Processing Systems, volume 16, 2004.
[5] D. M. Roy, C. Kemp, V. Mansinghka, and J. B. Tenenbaum. Learning annotated hierarchies from rela-
tional data. In Advances in Neural Information Processing Systems, volume 19, 2007.
[6] K. A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In Proceedings of the International
Conference on Machine Learning, volume 22, 2005.
[7] N. Friedman. Pcluster: Probabilistic agglomerative clustering of gene expression proﬁles. Technical
Report Technical Report 2003-80, Hebrew University, 2003.
[8] J. F. C. Kingman. On the genealogy of large populations. Journal of Applied Probability, 19:27–43, 1982.
Essays in Statistical Science.
[9] J. F. C. Kingman. The coalescent. Stochastic Processes and their Applications, 13:235–248, 1982.
[10] P. Fearnhead. Sequential Monte Carlo Method in Filter Theory. PhD thesis, Merton College, University
of Oxford, 1998.
[11] R. M. Neal. Annealed importance sampling. Technical Report 9805, Department of Statistics, University
of Toronto, 1998.
[12] A. McMahon and R. McMahon. Language Classiﬁcation by Numbers. Oxford University Press, 2005.
[13] M. Haspelmath, M. Dryer, D. Gil, and B. Comrie, editors. The World Atlas of Language Structures.
Oxford University Press, 2005.
[14] H. Daum ´e III and L. Campbell. A Bayesian model for discovering typological implications. In Proceed-
ings of the Annual Meeting of the Association for Computational Linguistics, 2007.
[15] J. Pitman. Coalescents with multiple collisions. Annals of Probability, 27:1870–1902, 1999.


Support Vector Machine Classi ﬁcation
with Inde ﬁnite Kernels

Ronny Luss
ORFE, Princeton University
Princeton, NJ 08544
rluss@princeton.edu

Alexandre d’Aspremont
ORFE, Princeton University
Princeton, NJ 08544
aspremon@princeton.edu

Abstract

In this paper, we propose a method for support vector machine classiﬁcation using
inde ﬁnite kernels. Instead of directly minimizing or stabi lizing a nonconvex loss
function, our method simultaneously ﬁnds the support vecto rs and a proxy kernel
matrix used in computing the loss. This can be interpreted as a robust classiﬁcation
problem where the inde ﬁnite kernel matrix is treated as a noi sy observation of the
true positive semide ﬁnite kernel. Our formulation keeps th e problem convex and
relatively large problems can be solved efﬁciently using th e analytic center cutting
plane method. We compare the performance of our technique with other methods
on several data sets.

1 Introduction

Here, we present an algorithm for support vector machine (SVM) classiﬁcation using inde ﬁnite ker-
nels. Our interest in inde ﬁnite kernels is motivated by seve ral observations. First, certain similarity
measures take advantage of application-speciﬁc structure
in the data and often display excellent
empirical classiﬁcation performance. Unlike popular kern els used in support vector machine clas-
siﬁcation, these similarity matrices are often inde ﬁnite a
nd so do not necessarily correspond to a
reproducing kernel Hilbert space (see [1] for a discussion).

An application of classiﬁcation with inde ﬁnite kernels to i mage classiﬁcation using Earth Mover’s
Distance was discussed in [2]. Similarity measures for protein sequences such as the Smith-
Waterman and BLAST scores are inde ﬁnite yet have provided hi nts for constructing useful positive
semide ﬁnite kernels such as those decribed in [3] or have bee n transformed into positive semide ﬁnite
kernels (see [4] for example). Here instead, our objective is to directly use these inde ﬁnite similarity
measures for classiﬁcation.

Our work also closely follows recent results on kernel learning (see [5] or [6]), where the kernel
matrix is learned as a linear combination of given kernels, and the resulting kernel is explicitly
constrained to be positive semide ﬁnite (the authors of [7] h ave adapted the SMO algorithm to solve
the case where the kernel is written as a positively weighted combination of other kernels). In our
case however, we never explicitly optimize the kernel matrix because this part of the problem can be
solved explicitly, which means that the complexity of our method is substantially lower than that of
classical kernel learning methods and closer in spirit to the algorithm used in [8], who formulate the
multiple kernel learning problem of [7] as a semi-in ﬁnite li near program and solve it with a column
generation technique similar to the analytic center cutting plane method we use here.

Finally, it is sometimes impossible to prove that some kernels satisfy Mercer’s condition or the
numerical complexity of evaluating the exact positive semide ﬁnite kernel is too high and a proxy
(and not necessarily positive semide ﬁnite) kernel has to be used instead (see [9] for example). In
both cases, our method allows us to bypass these limitations.

1

1.1 Current results

Several methods have been proposed for dealing with inde ﬁni te kernels in SVMs. A ﬁrst direction
embeds data in a pseudo-Euclidean (pE) space: [10] for example, formulates the classiﬁcation prob-
lem with an inde ﬁnite kernel as that of minimizing the distan ce between convex hulls formed from
the two categories of data embedded in the pE space. The nonseparable case is handled in the same
manner using reduced convex hulls (see [11] for a discussion of SVM geometric interpretations).

Another direction applies direct spectral transformations to inde ﬁnite kernels: ﬂipping the nega-
tive eigenvalues or shifting the kernel’s eigenvalues and reconstructing the kernel with the original
eigenvectors in order to produce a positive semide ﬁnite ker nel (see [12] and [2]).

Yet another option is to reformulate either the maximum margin problem or its dual in order to
use the inde ﬁnite kernel in a convex optimization problem (s ee [13]). An equivalent formulation
of SVM with the same objective but where the kernel appears in the constraints can be modiﬁed
to a convex problem by eliminating the kernel from the objective. Directly solving the nonconvex
problem sometimes gives good results as well (see [14] and [10]).

1.2 Contribution

Here, instead of directly transforming the inde ﬁnite kerne l, we simultaneously learn the support vec-
tor weights and a proxy positive semide ﬁnite kernel matrix, while penalizing the distance between
this proxy kernel and the original, inde ﬁnite one. Our main r esult is that the kernel learning part of
that problem can be solved explicitly, meaning that the classiﬁcation problem with inde ﬁnite kernels
can simply be formulated as a perturbation of the positive semide ﬁnite case.

Our formulation can also be interpreted as a worst-case robust classiﬁcation problem with uncer-
tainty on the kernel matrix. In that sense, inde ﬁnite simila rity matrices are seen as noisy observa-
tions of an unknown positive semide ﬁnite kernel. From a comp lexity standpoint, while the original
SVM classiﬁcation problem with inde ﬁnite kernel is nonconv
ex, the robustiﬁcation we detail here is
a convex problem, and hence can be solved efﬁciently with gua ranteed complexity bounds.

The paper is organized as follows. In Section 2 we formulate our main classiﬁcation problem and
detail its interpretation as a robust SVM. In Section 3 we describe an algorithm for solving this
problem. Finally, in Section 4, we test the numerical performance of these methods on various
applications.

2 SVM with indeﬁnite kernels

Here, we introduce our robustiﬁcation of the SVM classiﬁcat

ion problem with inde ﬁnite kernels.

2.1 Robust classiﬁcation

Let K ∈ Sn be a given kernel matrix and y ∈ Rn be the vector of labels, with Y = diag(y ) the
matrix with diagonal y , where Sn is the set of symmetric matrices of size n and Rn is the set of
n-vectors of real numbers. We can write the dual of the SVM classiﬁcation problem with hinge loss
and quadratic penalty as:

maximize αT e − Tr(K (Y α)(Y α)T )/2
subject to αT y = 0
0 ≤ α ≤ C
in the variable α ∈ Rn and where e is an n-vector of ones. When K is positive semide ﬁnite, this
problem is a convex quadratic program. Suppose now that we are given an inde ﬁnite kernel matrix
K0 ∈ Sn . We formulate a robust version of problem (1) by restricting K to be a positive semide ﬁnite
kernel matrix in some given neighborhood of the original (inde ﬁnite) kernel matrix K0 :
1
min
2
{K(cid:23)0, kK−K0 k2
F ≤β}
in the variables K ∈ Sn and α ∈ Rn , where the parameter β > 0 controls the distance between
the original matrix K0 and the proxy kernel K . This can be interpreted as a worst-case robust

max
{αT y=0, 0≤α≤C }

(1)

αT e −

Tr(K (Y α)(Y α)T )

(2)

2

classiﬁcation problem with bounded uncertainty on the kern el matrix K . The above problem is
infeasible for some values of β so we replace here the hard constraint on K by a penalty on the
distance between the proxy positive semide ﬁnite kernel and the given inde ﬁnite matrix. The problem
we solve is now:

(3)

αT e −

Tr(K (Y α)(Y α)T ) + ρkK − K0k2
F

1
max
min
2
{K(cid:23)0}
{αT y=0,0≤α≤C }
in the variables K ∈ Sn and α ∈ Rn , where the parameter ρ > 0 controls the magnitude of the
penalty on the distance between K and K0 . The inner minimization problem is a convex conic
program on K . Also, as the pointwise minimum of a family of concave quadratic functions of α, the
solution to the inner problem is a concave function of α, and hence the outer optimization problem
is also convex (see [15] for further details). Thus, (3) is a concave maximization problem subject to
linear constraints and is therefore a convex problem in α.
Our key result here is that the inner kernel learning optimization problem can be solved in closed
form. For a ﬁxed α, the inner minimization problem is equivalent to the following problem:
minimize
4ρ (Y α)(Y α)T )k2
kK − (K0 + 1
F
subject to K (cid:23) 0

in the variable K ∈ Sn . This is the projection of the K0 + (1/4ρ)(Y α)(Y α)T on the cone of
positive semide ﬁnite matrices. The optimal solution to thi s problem is then given by:
K ∗ = (K0 + (1/4ρ)(Y α)(Y α)T )+
(4)
where X+ is the positive part of the matrix X , i.e. X+ = Pi max(0, λi )xixT
i where λi and xi are
the ith eigenvalue and eigenvector of the matrix X . Plugging this solution into (3), we get:
1
Tr(K ∗ (Y α)(Y α)T ) + ρkK ∗ − K0k2
αT e −
max
F
2
{αT y=0, 0≤α≤C }
in the variable α ∈ Rn , where (Y α)(Y α)T is the rank one matrix with coefﬁcients yiαiαj yj ,
i, j = 1, . . . , n. We can rewrite this as an eigenvalue optimization problem by using the eigenvalue
representation of X+ . Letting the eigenvalue decomposition of K0 + (1/4ρ)(Y α)(Y α)T be V DV T ,
we get K ∗ = V D+V T and, with vi the ith column of V , we can write:
Tr(K ∗ (Y α)(Y α)T ) = (Y α)T V D+V T (Y α)
n
(Y α)(Y α)T (cid:19)(cid:19) (αT Y vi )2
max (cid:18)0, λi (cid:18)K0 +
Xi=1
where λi (X ) is the ith eigenvalue of the quantity X . Using the same technique, we can also rewrite
the term kK ∗ − K0 |2
F using this eigenvalue decomposition. Our original optimization problem (3)
ﬁnally becomes:

1
4ρ

=

maximize αT e − 1
2 Pi max(0, λi (K0 + (Y α)(Y α)T /4ρ))(αT Y vi )2
+ρ Pi (max(0, λi (K0 + (Y α)(Y α)T /4ρ)))2
−2ρ Pi Tr((vi vT
i )K0 )max(0, λi (K0 + (Y α)(Y α)T /4ρ)) + ρ Tr(K0K0 )
subject to αT y = 0, 0 ≤ α ≤ C
in the variable α ∈ Rn .

(5)

2.2 Dual problem

Because problem (3) is convex with at least one compact feasible set, we can formulate the dual
problem to (5) by simply switching the max and the min. The inner maximization is a quadratic
program in α, and hence has a quadratic program as its dual. We then get the dual by plugging this
inner dual quadratic program into the outer minimization, to get the following problem:

minimize Tr(K −1(Y (e − λ + µ + yν ))(Y (e − λ + µ + yν ))T )/2 + C µT e + ρkK − K0k2
F
subject to K (cid:23) 0, λ, µ ≥ 0

(6)

3

in the variables K ∈ Sn , λ, µ ∈ Rn and ν ∈ R. This dual problem is a quadratic program in the
variables λ and µ which correspond to the primal constraints 0 ≤ α ≤ C and ν which is the dual
variable for the constraint αT y = 0. As we have seen earlier, any feasible solution to the primal
problem produces a corresponding kernel in (4), and plugging this kernel into the dual problem in
(6) allows us to calculate a dual feasible point by solving a quadratic program which gives a dual
objective value, i.e. an upper bound on the optimum of (5). This bound can then be used to compute
a duality gap and track convergence.

2.3 Interpretation

We noted that our problem can be viewed as a worst-case robust classiﬁcation problem with uncer-
tainty on the kernel matrix. Our explicit solution of the optimal worst-case kernel given in (4) is the
projection of a penalized rank-one update to the inde ﬁnite k ernel on the cone of positive semide ﬁnite
matrices. As ρ tends to in ﬁnity, the rank-one update has less effect and in t he limit, the optimal ker-
nel is the kernel given by zeroing out the negative eigenvalues of the inde ﬁnite kernel. This means
that if the inde ﬁnite kernel contains a very small amount of n oise, the best positive semide ﬁnite
kernel to use with SVM in our framework is the positive part of the inde ﬁnite kernel.

This limit as ρ tends to in ﬁnity also motivates a heuristic for the transfor mation of the kernel on
the testing set. Since the negative eigenvalues of the training kernel are thresholded to zero in the
limit, the same transformation should occur for the test kernel. Hence, we update the entries of the
full kernel corresponding to training instances by the rank-one update resulting from the optimal
solution to (3) and threshold the negative eigenvalues of the full kernel matrix to zero. We then use
the test kernel values from the resulting positive semide ﬁn ite matrix.

3 Algorithms

We now detail two algorithms that can be used to solve Problem (5). The optimization problem is
the maximization of a nondifferentiable concave function subject to convex constraints. An optimal
point always exists since the feasibility set is bounded and nonempty. For numerical stability, in both
algorithms, we quadratically smooth our objective to calculate a gradient instead. We ﬁrst describe
a simple projected gradient method which has numerically cheap iterations but has no convergence
bound. We then show how to apply the much more efﬁcient analyt ic center cutting plane method
whose iterations are slightly more complex but which converges linearly.

Smoothing Our objective contains terms of the form max{0, f (x)} for some function f (x), which
are not differentiable (described in the section below). These functions are easily smoothed out by
a regularization technique (see [16] for example). We replace them by a continuously differentiable
2 -approximation as follows:
ǫ

ϕǫ (f (x)) = max
(uf (x) −
0≤u≤1

ǫ
2

u2 ).

and the gradient is given by ∇ϕǫ (f (x)) = u∗(x)∇f (x) where u∗(x) = argmax ϕǫ (f (x)).

Gradient Calculating the gradient of our objective requires a full eigenvalue decomposition to
compute the gradient of each eigenvalue. Given a matrix X (α), the derivative of the ith eigenvalue
with respect to α is given by:

∂X (α)
∂ λi (X (α))
∂α
∂α
where vi is the ith eigenvector of X (α). We can then combine this expression with the smooth
approximation above to get the gradient.

= vT
i

vi

(7)

We note that eigenvalues of symmetric matrices are not differentiable when some of them have mul-
tiplicities greater than one (see [17] for a discussion). In practice however, most tested kernels were
of full rank with distinct eigenvalues so we ignore this issue here. One may also consider projected
subgradient methods, which are much slower, or use subgradients for analytic center cutting plane
methods (which does not affect complexity).

4

3.1 Projected gradient method

The projected gradient method takes a steepest descent, then projects the new point back onto the
feasible region (see [18] for example). In order to use these methods the objective function must be
differentiable and the method is only efﬁcient if the projec tion step is numerically cheap. We choose
an initial point α0 ∈ Rn and the algorithm proceeds as follows:

Projected gradient method

1. Compute αi+1 = αi + t∇f (αi ).
2. Set αi+1 = pA (αi+1 ).
3. If gap ≤ ǫ stop, otherwise go back to step 1.

The complexity of each iteration breaks down as follows.

Step 1. This requires an eigenvalue decomposition and costs O(n3 ). We note that a line search would
be costly because it would require multiple eigenvalue decompositions to recalculate the objective
multiple times.

Step 2. This is a projection onto the region A = {αT y = 0, 0 ≤ α ≤ C } and can be solved
explicitly by sorting the vector of entries, with cost O(n log n).
Stopping Criterion. We can compute a duality gap using the results of §2.2: let Ki = (K0 +
(Y αi )(Y αi )T /4ρ)+ (the kernel at iteration i), then solving problem (1) which is just an SVM with
a convex kernel Ki produces an upper bound on (5), and hence a bound on the suboptimality of the
current solution.

Complexity. The number of iterations required by this method to reach a target precision of ǫ is
typically in O(1/ǫ2 ).

3.2 Analytic center cutting plane method

The analytic center cutting plane method (ACCPM) reduces the feasible region on each iteration
using a new cut of the feasible region computed by evaluating a subgradient of the objective function
at the analytic center of the current set, until the volume of the reduced region converges to the target
precision. This method does not require differentiability. We set A0 = {αT y = 0, 0 ≤ α ≤ C }
which we can write as {A0 ≤ b0} to be our ﬁrst localization set for the optimal solution. The
method then works as follows (see [18] for a more complete reference on cutting plane methods):

Analytic center cutting plane method

1. Compute αi as the analytic center of Ai by solving:

m
Xi=1
where aT
i represents the ith row of coefﬁcients from the left-hand side of {A0 ≤ b0}.
2. Compute ∇f (x) at the center αi+1 and update the (polyhedral) localization set:

αi+1 = argmin
y∈Rn

−

log(bi − aT
i y )

Ai+1 = Ai ∪ {∇f (αi+1 )(α − αi+1 ) ≥ 0}

3. If gap ≤ ǫ stop, otherwise go back to step 1.

The complexity of each iteration breaks down as follows.

Step 1. This step computes the analytic center of a polyhedron and can be solved in O(n3 ) operations
using interior point methods for example.

5

Step 2. This simply updates the polyhedral description.

Stopping Criterion. An upper bound is computed by maximizing a ﬁrst order Taylor a pproximation
of f (α) at αi over all points in an ellipsoid that covers Ai , which can be done explicitly.
Complexity. ACCPM is provably convergent in O(n log(1/ǫ)2 ) iterations when using cut elimina-
tion which keeps the complexity of the localization set bounded. Other schemes are available with
slightly different complexities: an O(n2 /ǫ2) is achieved in [19] using (cheaper) approximate centers
for example.

4 Experiments

In this section we compare the generalization performance of our technique to other methods of
applying SVM classiﬁcation given an inde ﬁnite similarity m easure. We also test SVM classiﬁcation
performance on positive semide ﬁnite kernels using the LIBS VM library. We ﬁnish with experiments
showing convergence of our algorithms. Our algorithms were implemented in Matlab.

4.1 Generalization

We compare our method for SVM classiﬁcation with inde ﬁnite k
ernels to several of the kernel pre-
processing techniques discussed earlier. The ﬁrst three te chniques perform spectral transformations
on the inde ﬁnite kernel. The ﬁrst, denoted
denoise, thresholds the negative eigenvalues to zero. The
second transformation, called ﬂip , takes the absolute value of all eigenvalues. The last transforma-
tion, shift, adds a constant to each eigenvalue making them all positive. See [12] for further details.
We ﬁnally also compare with using SVM on the original inde ﬁni
te kernel (SVM converges but the
solution is only a stationary point and is not guaranteed to be optimal).

We experiment on data from the USPS handwritten digits database (described in [20]) using the
inde ﬁnite Simpson score (SS) to compare two digits and on two data sets from the UCI repository
(see [21]) using the inde ﬁnite Epanechnikov (EP) kernel. Th e data is randomly divided into training
and testing data. We apply 5-fold cross validation and use an accuracy measure (described below)
to determine the optimal parameters C and ρ. We then train a model with the full training set and
optimal parameters and test on the independent test set.

Table 1: Statistics for various data sets.

Data Set
USPS-3-5-SS
USPS-4-6-SS
Diabetes-EP
Liver-EP

# Train
767
829
614
276

# Test
773
857
154
69

λmin

λmax

-34.76
-37.30
-0.27
-1.38e-15

453.58
413.17
18.17
3.74

Table 1 provides statistics including the minimum and maximum eigenvalues of the training kernels.
The main observation is that the USPS data uses highly inde ﬁn ite kernels while the UCI data use
kernels that are nearly positive semide ﬁnite. Table 2 displ ays performance by comparing accuracy
and recall. Accuracy is de ﬁned as the percentage of total ins tances predicted correctly. Recall is the
percentage of true positives that were correctly predicted positive.

Our method is referred to as Inde ﬁnite SVM. We see that our met hod performs favorably among
the USPS data. Both measures of performance are quite high for all methods. Our method does
not perform as well on the UCI data sets but is still favorable on one of the measures in each
experiment. Notice though that recall is not good in the liver data set overall which could be the
result of over ﬁtting one of the classiﬁcation categories. T he liver data set uses a kernel that is almost
positive semide ﬁnite - this is an example where the input is a lmost a true kernel and Inde ﬁnite
SVM ﬁnds one slightly better. We postulate that our method wi ll perform better in situations where
the similarity measure is highly inde ﬁnite as in the USPS dat aset, while measures that are almost
positive semide ﬁnite maybe be seen as having a small amount o f noise.

6

Table 2: Performance Measures for various data sets.

Data Set

USPS-3-5-SS

Measure
Accuracy
Recall
USPS-4-6-SS Accuracy
Recall
Accuracy
Recall
Accuracy
Recall

Diabetes-EP

Liver-EP

Denoise
95.47
94.50
97.78
98.42
75.32
90.00
63.77
22.58

Flip
95.73
95.45
97.90
98.65
74.68
90.00
63.77
22.58

Shift
90.43
92.11
94.28
93.68
68.83
92.00
57.97
25.81

SVM Indeﬁnite SVM
96.25
74.90
96.65
72.73
97.90
90.08
98.87
88.49
75.32
68.83
95.00
90.00
65.22
63.77
22.58
22.58

4.2 Algorithm Convergence

We ran our two algorithms on data sets created by randomly perturbing the four USPS data sets used
above. The average results with one standard deviation above and below the mean are displayed in
Figure 1 with the duality gap in log scale (note that the codes were not stopped here and that the
target gap improvement is usually much smaller than 10−8 ). As expected, ACCPM converges much
faster (in fact linearly) to a higher precision while each iteration requires solving a linear program
of size n. The gradient projection method converges faster in the beginning but stalls at a higher
precision, however each iteration only requires sorting the current point.

104

103

102

101

100

10−1

10−2

10−3

p
a
G
y
t
i
l
a
u
D

101

100

10−1

10−2

p
a
G
y
t
i
l
a
u
D

10−4

0

50

100
Iteration

150

200

10−3

0

200

400
600
Iteration

800

1000

Figure 1: Convergence plots for ACCPM (left) & projected gradient method (right) on randomly perturbed
USPS data sets (average gap versus iteration number, dashed lines at plus and minus one standard deviation).

5 Conclusion

We have proposed a technique for incorporating inde ﬁnite ke rnels into the SVM framework with-
out any explicit transformations. We have shown that if we view the inde ﬁnite kernel as a noisy
instance of a true kernel, we can learn an explicit solution for the optimal kernel with a tractable
convex optimization problem. We give two convergent algorithms for solving this problem on rel-
atively large data sets. Our initial experiments show that our method can at least fare comparably
with other methods handling inde ﬁnite kernels in the SVM fra mework but provides a much clearer
interpretation for these heuristics.

7

References

[1] C. S. Ong, X. Mary, S. Canu, and A. J. Smola. Learning with non-positive kernels. Proceedings of the
21st International Conference on Machine Learning, 2004.
[2] A. Zamolotskikh and P. Cunningham. An assessment of alternative strategies for constructing emd-based
kernel functions for use in an svm for image classi ﬁcation. Technical Report UCD-CSI-2007-3, 2004.
[3] H. Saigo, J. P. Vert, N. Ueda, and T. Akutsu. Protein homology detection using string alignment kernels.
Bioinformatics, 20(11):1682–1689, 2004.
[4] G. R. G. Lanckriet, N. Cristianini, M. I. Jordan, and W. S. Noble. Kernel-based integration of genomic
data using semideﬁnite programming. 2003. citeseer.ist.p su.edu/648978.html.
[5] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semideﬁnite programming.
Journal of Machine Learning Research, 5:27–72, 2004.
[6] C. S. Ong, A. J. Smola, and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine
Learning Research, 6:1043–1071, 2005.
[7] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo
algorithm. Proceedings of the 21st International Conference on Machine Learning, 2004.
[8] S. Sonnenberg, G. R ¨atsch, C. Sch¨afer, and B. Sch ¨olkopf. Large scale multiple kernel learning. Journal of
Machine Learning Research, 7:1531–1565, 2006.
[9] Marco Cuturi. Permanents, transport polytopes and positive deﬁnite kernels on histograms. Proceedings
of the Twentieth International Joint Conference on Arti ﬁci al Intelligence, 2007.
[10] B. Haasdonk. Feature space interpretation of svms with indeﬁnite kernels.
IEEE Transactions on Pattern
Analysis and Machine Intelligence, 27(4), 2005.
[11] K. P. Bennet and E. J. Bredensteiner. Duality and geometry in svm classi ﬁers. Proceedings of the 17th
International conference on Machine Learning, pages 57–64, 2000.
[12] G. Wu, E. Y. Chang, and Z. Zhang. An analysis of transformation on non-positive semideﬁnite similarity
matrix for kernel machines. Proceedings of the 22nd International Conference on Machine Learning,
2005.
[13] H.-T. Lin and C.-J. Lin. A study on sigmoid kernel for svm and the training of non-psd kernels by
smo-type methods. 2003.
[14] A. Wo ´znica, A. Kalousis, and M. Hilario. Distances and (indeﬁnite) kernels for set of objects. Proceedings
of the 6th International Conference on Data Mining, pages 1151–1156, 2006.
[15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[16] C. Gigola and S. Gomez. A regularization method for solving the ﬁnite convex min-max problem. SIAM
Journal on Numerical Analysis, 27(6):1621–1634, 1990.
[17] M. Overton. Large-scale optimization of eigenvalues. SIAM Journal on Optimization, 2(1):88–120, 1992.
[18] D. Bertsekas. Nonlinear Programming, 2nd Edition. Athena Scienti ﬁc, 1999.
[19] J.-L. Gofﬁn and J.-P. Vial. Convex nondifferentiable o ptimization: A survey focused on the analytic center
cutting plane method. Optimization Methods and Software, 17(5):805–867, 2002.
[20] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 16(5), 1994.
UCI Machine Learning Repository.
[21] A. Asuncion
and D.J. Newman.
sity
of California,
Irvine,
School
of
Information
and Computer
Sciences,
http://www.ics.uci.edu/∼mlearn/MLRepository.html.

Univer-
2007.

8


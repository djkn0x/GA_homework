Boosting Algorithms for Maximizing the Soft Margin

Manfred K. Warmuth∗
Dept. of Engineering
University of California
Santa Cruz, CA, U.S.A.

Karen Glocer
Dept. of Engineering
University of California
Santa Cruz, CA, U.S.A.

Gunnar R ¨atsch
Friedrich Miescher Laboratory
Max Planck Society
T ¨ubingen, Germany

Abstract

We present a novel boosting algorithm, called SoftBoost, designed for sets of bi-
nary labeled examples that are not necessarily separable by convex combinations
of base hypotheses. Our algorithm achieves robustness by capping the distribu-
tions on the examples. Our update of the distribution is motivated by minimizing
a relative entropy subject to the capping constraints and constraints on the edges
of the obtained base hypotheses. The capping constraints imply a soft margin in
the dual optimization problem. Our algorithm produces a convex combination of
hypotheses whose soft margin is within δ of its maximum. We employ relative en-
tropy projection methods to prove an O( ln N
δ2 ) iteration bound for our algorithm,
where N is number of examples.
We compare our algorithm with other approaches including LPBoost, Brown-
Boost, and SmoothBoost. We show that there exist cases where the number of iter-
ations required by LPBoost grows linearly in N instead of the logarithmic growth
for SoftBoost. In simulation studies we show that our algorithm converges about
as fast as LPBoost, faster than BrownBoost, and much faster than SmoothBoost.
In a benchmark comparison we illustrate the competitiveness of our approach.

1 Introduction

Boosting methods have been used with great success in many applications like OCR, text classiﬁ-
cation, natural language processing, drug discovery, and computational biology [13]. For AdaBoost
[7] it was frequently observed that the generalization error of the combined hypotheses kept de-
creasing after the training error had already reached zero [19]. This sparked a series of theoretical
studies trying to understand the underlying principles that govern the behavior of ensemble methods
[19, 1]. It became apparent that some of the power of ensemble methods lies in the fact that they
tend to increase the margin of the training examples. This was consistent with the observation that
AdaBoost works well on low-noise problems, such as digit recognition tasks, but not as well on tasks
with high noise. On such tasks, better generalizaton can be achieved by not enforcing a large margin
on all training points. This experimental observation was supported by the study of [19], where the
generalization error of ensemble methods was bounded by the sum of two terms: the fraction of
training points which have a margin smaller than some value ρ plus a complexity term that depends
on the base hypothesis class and ρ. While this worst-case bound can only capture part of what is
going on in practice, it nevertheless suggests that in some cases it pays to allow some points to have
small margin or be misclassiﬁed if this leads to a larger over all margin on the remaining points.

To cope with this problem, it was necessary to construct variants of AdaBoost which trade off the
fraction of examples with margin at least ρ with the size of the margin ρ. This was typically done
by preventing the distribution maintained by the algorithm from concentrating too much on the
most difﬁcult examples. This idea is implemented in many alg orithms including AdaBoost with
soft margins [15], MadaBoost [5], ν -Arc [16, 14], SmoothBoost [21], LPBoost [4], and several
others (see references in [13]). For some of these algorithms, signiﬁcant improvements were shown
compared to the original AdaBoost algorithm on high noise data.

∗Supported by NSF grant CCR 9821087.

1

In parallel, there has been a signiﬁcant interest in how the l inear combination of hypotheses gener-
ated by AdaBoost is related to the maximum margin solution [1, 19, 4, 18, 17]. It was shown that
AdaBoost generates a combined hypothesis with a large margin, but not necessarily the maximum
hard margin [15, 18]. This observation motivated the development of many Boosting algorithms
that aim to maximize the margin [1, 8, 4, 17, 22, 18]. AdaBoost∗ [17] and TotalBoost [22] provable
converge to the maximum hard margin within precision δ in 2 ln(N/δ2 ) iterations. The other algo-
rithms have worse or no known convergence rates. However, such margin-maximizing algorithms
are of limited interest for a practitioner working with noisy real-world data sets, as over ﬁtting is
even more problematic for such algorithms than for the original AdaBoost algorithm [1, 8].

In this work we combine these two lines of research into a single algorithm, called SoftBoost, that
for the ﬁrst time implements the soft margin idea in a practic al boosting algorithm. SoftBoost
ﬁnds in O(ln(N )/δ2 ) iterations a linear combination of base hypotheses whose soft margin is at
least the optimum soft margin minus δ . BrownBoost [6] does not always optimize the soft margin.
SmoothBoost and MadaBoost can be related to maximizing the soft margin, but while they have
known iterations bounds in terms of other criteria, it is unknown how quickly they converge to
the maximum soft margin. From a theoretical point of view the optimization problems underlying
SoftBoost as well as LPBoost are appealing, since they directly maximize the margin of a (typically
large) subset of the training data [16]. This quantity plays a crucial role in the generalization error
bounds [19].

Our new algorithm is most similar to LPBoost because its goal is also to optimize the soft margin.
The most important difference is that we use slightly relaxed constraints and a relative entropy to
the uniform distribution as the objective function. This leads to a distribution on the examples that
is closer to the uniform distribution. An important result of our work is to show that this strategy
may help to increase the convergence speed: We will give examples where LPBoost converges much
more slowly than our algorithm —linear versus logarithmic g
rowth in N .
The paper is organized as follows: in Section 2 we introduce the notation and the basic optimization
problem. In Section 3 we discuss LPBoost and give a separable setting where N/2 iterations are
needed by LPBoost to achieve a hard margin within precision .99. In Section 4 we present our new
SoftBoost algorithm and prove its iteration bound. We provide an experimental comparison of the
algorithms on real and synthetic data in Section 5, and conclude with a discussion in Section 6.

2 Preliminaries

In the boosting setting, we are given a set of N labeled training examples (xn , yn ), n = 1 . . . N ,
where the instances xn are in some domain X and the labels yn ∈ ±1. Boosting algorithms maintain
a distribution d on the N examples, i.e. d lies in the N dimensional probability simplex P N . Intu-
itively, the hard to classify examples receive more weight. In each iteration, the algorithm gives the
current distribution to an oracle (a.k.a. base learning algorithm), which returns a new base hypothe-
sis h : X → [−1, 1]N with a certain guarantee of performance. This guarantee will be discussed at
the end of this section.

One measure of the performance of a base hypothesis h with respect to distribution d is its edge,
γh = PN
n=1 dn ynh(xn ). When the range of h is ±1 instead of the interval [-1,1], then the edge is
ǫh of hypothesis h: i.e. ǫh(d) = 1
2 − 1
just an afﬁne transformation of the weighted error
2 γh . A
hypothesis that predicts perfectly has edge γ = 1, a hypothesis that always predicts incorrectly has
edge γ = −1, and a random hypothesis has edge γ ≈ 0. The higher the edge, the more useful is the
hypothesis for classifying the training examples. The edge of a set of hypotheses is de ﬁned as the
maximum edge of the set.

After a hypothesis is received, the algorithm must update its distribution d on the examples. Boost-
ing algorithms (for the separable case) commonly update their distribution by placing a constraint
on the edge of most recent hypothesis. Such algorithms are called corrective [17]. In totally cor-
rective updates, one constrains the distribution to have small edge with respect to all of the previous
hypotheses [11, 22]. The update developed in this paper is an adaptation of the totally corrective
update of [22] that handles the inseparable case. The ﬁnal ou tput of the boosting algorithm is always
a convex combination of base hypotheses fw (xn ) = PT
t=1 wt ht (xn ), where ht is the hypothesis
added at iteration t and wt is its coefﬁcient. The margin of a labeled example (xn , yn) is de ﬁned as

2

ρn = ynfw (xn ). The (hard) margin of a set of examples is taken to be the minimum margin of the
set.

(1)

s.t. Xt
m=1

It is convenient to de ﬁne an N -dimensional vector um that combines the base hypothesis hm with
the labels yn of the N examples: um
n := ynhm (xn ). With this notation, the edge of the t-th
hypothesis becomes d · ut and the margin of the n-th example w.r.t. a convex combination w of the
t − 1 hypotheses is Pt−1
ﬁrst
n wm .
m=1 um
For a given set of hypotheses {h1 , . . . , ht}, the following linear programming problem (1) optimizes
the minimum soft margin. The term “soft” here refers to a relaxation of the margin con straint. We
now allow examples to lie below the margin but penalize them linearly via slack variables ψn . The
dual problem (2) minimizes the maximum edge when the distribution is capped with 1/ν , where
ν ∈ {1, . . . , N }:
ν XN
w,ρ,ψ (cid:0)ρ −
ψn (cid:1)
1
γ ∗
t (ν ) = min
ρ∗
t (ν ) = max
d,γ
n=1
s.t. d · um ≤ γ , for 1 ≤ m ≤ t,
um
n wm ≥ ρ − ψn , for 1 ≤ n ≤ N ,
1
w ∈ P t , ψ ≥ 0.
ν
By duality, ρ∗
t (ν ). Note that the relationship between capping and the hinge loss has
t (ν ) = γ ∗
long been exploited by the SVM community [3, 20] and has also been used before for Boosting in
[16, 14]. In particular, it is known that ρ in (1) is chosen such that N − ν examples have margin at
least ρ. This corresponds to ν active constraints in (2). The case ν = 1 is degenerate: there are no
capping constraints in (2) and this is equivalent to the hard margin case.1
Assumption on the weak learner We assume that for any distribution d ≤ 1
1 on the examples,
ν
the oracle returns a hypothesis h with edge at least g , for some ﬁxed g . This means that for the
corresponding u vector, d · u ≥ g . For binary valued features, this is equivalent to the assumption
that the base learner always returns a hypothesis with error at most 1
2 − 1
2 g .
t (ν ) of the minimization problem (2) and
Adding a new constraint can only increase the value γ ∗
t (ν ) is non-decreasing in t. It is natural to de ﬁne γ ∗ (ν ) as the value of (2) w.r.t. the entire
therefore γ ∗
hypothesis set from which the oracle can choose. Clearly γ ∗
t (ν ) approaches γ ∗(ν ) from below.
Also, the guarantee g of the oracle can be at most γ ∗ (ν ) because for the optimal distribution d∗ that
realizes γ ∗ (ν ), all hypotheses have edge at most γ ∗(ν ). For computational reasons, g might however
be lower than γ ∗(ν ) and in that case the optimum soft margin we can achieve is g .

d ∈ P N , d ≤

1.

γ

(2)

3 LPBoost

In iteration t, the LPBoost algorithm [4] sends its current distribution dt−1 to the oracle and receives
a hypothesis ht that satisﬁes dt−1 · ut ≥ g . It then updates its distribution to dt by solving the linear
programming problem (1) based on the t hypotheses received so far.
The goal of the boosting algorithms is to produce a convex combination of T hypotheses such that
γT (ν ) ≥ g − δ . The simplest way to achieve this is to break when this condition is satisﬁed.
Although the guarantee g is typically not known, it is upper bounded by bγt = min1≤m≤t dt−1 · ut
and therefore LPBoost uses the more stringent stopping criterion γt (ν ) ≥ bγt − δ .
To our knowledge, there is no known iteration bound for LPBoost even though it provably converges
to the δ -optimal solution of the optimization problem after it has seen all hypotheses [4, 10]. Empir-
ically, the convergence speed depends on the linear programming optimizer, e.g. simplex or interior
point solver [22]. For the ﬁrst time, we are able to establish a lower bound showing that, independent
of the optimizer, LPBoost can require Ω(N ) iterations:

Theorem 1 There exists a case where LPBoost requires N/2 iterations to achieve a hard margin
that is within δ = .99 of the optimum hard margin.
Proof. Assume we are in the hard margin case (ν = 1). The counterexample has N examples and
2 iterations, the optimal value γ ∗
2 + 1 base hypothesis. After N
t (1) for the chosen hypotheses will
N

1Please note that [20] have previously used the parameter ν with a slightly different meaning, namely ν /N
in our notation. We use an unnormalized version of ν denoting a number of examples instead of a fraction.

3

Algorithm 1 LPBoost with accuracy param. δ and capping parameter ν
1. Input: S = h(x1 , y1 ), . . . , (xN , yN )i, accuracy δ , capping parameter ν ∈ [1, N ].
2. Initialize: d0 to the uniform distribution and bγ0 to 1.
3. Do for t = 1, . . .
(a) Send dt−1 to oracle and obtain hypothesis ht .
Set ut
n = ht (xn )yn and bγt = min{bγt−1 , dt−1 · ut}.
(Assume dt−1 · ut ≥ g , where edge guarantee g is unknown.)
(b) Update the distribution to any dt that solves the LP problem

s.t. d · um ≤ γ , for 1 ≤ m ≤ t; d ∈ P N , d ≤
(dt , γ ∗
t ) = argmin
γ
d,γ
t ≥ bγt − δ then set T = t and break.2
(c) If γ ∗
4. Output: fw (x) = PT
m=1 wm hm (x), where the coefﬁcients wm maximize the soft
margin over the hypothesis set {h1 , . . . , hT } using the LP problem (1).
2When g is known, then one can break already when γ ∗
t (ν ) ≥ g − δ .

1.

1
ν

still be close to −1, whereas after the last hypothesis is added, this value is at least ǫ/2. Here ǫ is a
precision parameter that is an arbitrary small number.
Figure 1 shows
the case
1
2
3
4
5
n \ t
where N = 8 and T = 5,
1
+1
−1 + 5ǫ
−1 + 7ǫ
−1 + 9ǫ
−1 + ǫ
2
but it is trivial to generalize
+1
−1 + 5ǫ
−1 + 7ǫ
−1 + 9ǫ
−1 + ǫ
3
+1
−1 + 5ǫ
−1 + 7ǫ
−1 + 9ǫ
−1 + ǫ
this example to any even N .
4
+1
−1 + 5ǫ
−1 + 7ǫ
−1 + 9ǫ
−1 + ǫ
There are 8 examples/rows
5
−1 + 2ǫ
+1
−1 + 7ǫ
−1 + 9ǫ
+1 − ǫ
and the ﬁve columns are the
6
−1 + 3ǫ
−1 + 4ǫ
+1
−1 + 9ǫ
+1 − ǫ
ut ’s of the ﬁve available base
7
−1 + 3ǫ
−1 + 5ǫ
−1 + 6ǫ
+1
+1 − ǫ
hypotheses. The examples
8
−1 + 3ǫ
−1 + 5ǫ
−1 + 7ǫ
−1 + 8ǫ
+1 − ǫ
are separable because if we
t (1) −1 + 2ǫ
γ ∗
−1 + 4ǫ
−1 + 6ǫ
−1 + 8ǫ
≥ ǫ/2
put half of the weight on the
Figure 1: The ut vectors that are hard for LPBoost (for ν = 1).
ﬁrst and last hypothesis, then
the margins of all examples are at least ǫ/2.
We assume that in each iteration the oracle will return the remaining hypothesis with maximum
edge. This will result in LPBoost choosing the hypotheses in order, and there will never be any ties.
The initial distribution d0 is uniform. At the end of iteration t (1 ≤ t ≤ N/2), the distribution dt
will focus all its weight on example N/2 + t, and the optimum mixture of the columns will put all
of its weight on the tth hypothesis that was just received. In other words the value will be the bolded
entries in Figure 1: −1 + 2ǫt at the end of iteration t = 1, . . . , N/2. After N/2 iterations the value
t (1) of the underlying LP problem will still be close to −1, because ǫ can be made arbitrary small.
γ ∗
We reasoned already that the value for all N/2 + 1 hypotheses will be positive. So if ǫ is small
enough, then after N/2 iterations LPBoost is still at least .99 away from the optimal solution.
(cid:3)
Although the example set used in the above proof is linearly separable, we can modify it explicitly
to argue that capping the distribution on examples will not help in the sense that “soft” LPBoost
with ν > 1 can still have linear iteration bounds. To negate the effect of capping, simply pad out
the problem by duplicating all of the rows ν times. There will now be ˜N = N ν examples, and after
2 = ˜N
2ν iterations, the value of the game is still close to −1. This is not a claim that capping has no
N
value. It remains an important technique for making an algorithm more robust to noise. However, it
is not sufﬁcient to improve the iteration bound of LPBoost fr om linear growth in N to logarithmic.
Another attempt might be to modify LPBoost so that at each iteration a base hypothesis is chosen
that increases the value of the optimization problem the most. Unfortunately we found similar Ω(N )
counter examples to this heuristic (not shown). It is also easy to see that the algorithms related to
the below SoftBoost algorithm choose the last hypothesis after ﬁrst and ﬁnish in just two iterations.

4

Algorithm 2 SoftBoost with accuracy param. δ and capping parameter ν
1. Input: S = h(x1 , y1 ), . . . , (xN , yN )i, desired accuracy δ , and capping parameter
ν ∈ [1, N ].
2. Initialize: d0 to the uniform distribution and bγ0 to 1.
3. Do for t = 1, . . .
(a) Send dt−1 to the oracle and obtain hypothesis ht .
Set ut
n = ht (xn )yn and bγt = min{bγt−1 , dt−1 · ut}.
(Assume dt−1 · ut ≥ g , where edge guarantee g is unknown.)
(b) Update3

∆(d, d0 ),

dt = argmin
d

s.t. d·um ≤ bγt−δ, for 1 ≤ m ≤ t, X
n
(c) If above infeasible or dt contains a zero then T = t and break.
4. Output: fw (x) = PT
wmhm (x), where the coefﬁcients wm maximize the soft
m=1
margin over the hypothesis set {h1 , . . . , ht} using the LP problem (1).
3 When g is known, replace the upper bound bγt − δ by g − δ .

dn = 1, d ≤

1.

1
ν

4 SoftBoost

In this section, we present the SoftBoost algorithm, which adds capping to the TotalBoost algorithm
of [22]. SoftBoost takes as input a sequence of examples S = h(x1 , y1 ), . . . , (xN , yN )i, an accuracy
parameter δ , and a capping parameter ν . The algorithm has an oracle available with unknown
guarantee g . Its initial distribution d0 is uniform. In each iteration t, the algorithm prompts the oracle
for a new base hypothesis, incorporates it into the constraint set, and updates its distribution dt−1 to
dt by minimizing the relative entropy ∆(d, d0 ) := Pn dn ln dn
subject to linear constraints:
d0
n
dt+1 = argmind ∆(d, d0 )
s.t. d · um ≤ bγt − δ, for 1 ≤ m ≤ t (where bγt = min1≤m≤t dm−1 · um ),
Pn dn = 1, d ≤ 1
1.
ν
It is easy to solve this optimization problem with vanilla sequential quadratic programming methods
(see [22] for details). Observe that removing the relative entropy term from the objective, results
in a feasibility problem for linear programming where the edges are upper bounded by bγt − δ . If
we remove the relative entropy and minimize the upper bound on the edges, then we arrive at the
optimization problem of LPBoost, and logarithmic growth in the number of examples is no longer
possible. The relative entropy in the objective assures that the probabilities of the examples are
always proportional to their exponentiated negative soft margins (not shown). That is, more weight
is put on the examples with low soft margin, which are the examples that are hard to classify.

4.1 Iteration bounds for SoftBoost

Our iteration bound for SoftBoost is very similar to the bound proven for TotalBoost [22], differing
only in the additional details related to capping.

Theorem 2 SoftBoost terminates after at most ⌈ 2
δ2 ln(N/ν )⌉ iterations with a convex combination
that is at most δ below the optimum value g .
Proof. We begin by observing that if the optimization problem at iteration t is infeasible, then
t (ν ) > bγt − δ ≥ g − δ . Also if dt contains a zero, then since the objective function ∆(d, d0 ) is
γ ∗
strictly convex in d and minimized at the interior point d0 , there is no optimal solution in the interior
t (ν ) = bγt − δ ≥ g − δ .
of the simplex. Hence, γ ∗
Let Ct be the convex subset of probability vectors d ∈ P N satisfying d ≤ 1
1 and maxt
d · ut ≤
m=1
ν
bγt − δ . Notice that C0 is the N dimensional probability simplex where the components are capped
to 1
ν . The distribution dt−1 at iteration t − 1 is the projection of d0 onto the closed convex set Ct−1 .
Because adding a new hypothesis in iteration t results in an additional constraint and bγt ≤ bγt−1 ,
5

we have Ct ⊆ Ct−1 . If t ≤ T − 1, then our termination condition assures that at iteration t − 1
the set Ct−1 has a feasible solution in the interior of the simplex. Also, d0 lies in the interior and
dt ∈ Ct ⊆ Ct−1 . These preconditions assure that at iteration t − 1, the projection dt−1 of d0 onto
Ct−1 , exists and the Generalized Pythagorean Theorem for Bregman divergences [2, 9] is applicable:

∆(dt , d0 ) − ∆(dt−1 , d0 ) ≥ ∆(dt , dt−1 ).
By Pinsker’s inequality, ∆(dt , dt−1 ) ≥ (||dt−dt−1 ||1 )2
, and by H ¨older’s inequality,||dt−1 − dt ||1 ≥
2
||dt−1 − dt ||1 ||ut ||∞ ≥ dt−1 · ut − dt · ut . Also dt−1 · ut ≥ bγt by the de ﬁnition of bγt , and the
constraints on the optimization problem assure that dt · ut ≤ bγt − δ and thus dt−1 · ut − dt · ut ≥
bγt − (bγt − δ) = δ . We conclude that ∆(dt , dt−1 ) ≥ δ2
2 at iterations 1 through T − 1. By summing (3)
over the ﬁrst T − 1 iterations, we obtain

(3)

∆(dT , d0 ) − ∆(d0 , d0 ) ≥ (T − 1)

δ2
2
Since the left side is at most ln(N/ν ), the bound of the theorem follows.
When ν = 1, then capping is vacuous and the algorithm and its iteration bound coincides with the
bound for TotalBoost. Note that the upper bound ln(N/ν ) on the relative entropy decreases with ν .
When ν = N , then the distribution stays at d0 and the iteration bound is zero.

.

(cid:3)

5 Experiments

In a ﬁrst study, we use experiments on synthetic data to illus trate the general behavior of the con-
sidered algorithms.2 We generated a synthetic data set by starting with a random matrix of 2000
rows and 100 columns, where each entry was chosen uniformly in [0, 1]. For the ﬁrst 1000 rows, we
added 1/2 to the ﬁrst 10 columns and rescaled such that the entries in th ose columns were again in
[0, 1]. The rows of this matrix are our examples and the columns and their negation are the base hy-
potheses, giving us a total of 200 of them. The ﬁrst 1000 examp les were labeled +1 and the rest −1.
This results in a well separable dataset. To illustrate how the algorithms deal with the inseparable
case, we ﬂipped the sign of a random 10% of the data set. We then chose a random 500 examples as
our training set and the rest as our test set. In every boosting iteration we chose the base hypothesis
which has the largest edge with respect to the current distribution on the examples.

We have trained LPBoost and SoftBoost for different values of ν and recorded the generalization
error (cf. Figure 2; δ = 10−3 ). We should expect that for small ν (e.g. ν /N < 10%) the data is
not easily separable, even when allowing ν wrong predictions. Hence the algorithm may mistakenly
If ν is large enough, most incorrectly
concentrate on the random directions for discrimination.
labeled examples are likely to be identiﬁed as margin errors
(ψi > 0) and the performance should
stabilize. In Figure 2 we observe this expected behavior and also that for large ν the classiﬁcation
performance decays again. The generalization performances of LPBoost and SoftBoost are very
similar, which is expected as they both attempt to maximize the soft-margin.

Using the same data set, we analysed the convergence speed of several algorithms: LPBoost, Soft-
Boost, BrownBoost, and SmoothBoost. We chose δ = 10−2 and ν = 200.3 For every iteration
we record all margins and compute the soft margin objective (1) for optimally chosen ρ and ψ ’s.
Figure 3 plots this value against the number of iterations for the four algorithms. SmoothBoost
takes dramatically longer to converge to the maximum soft margin than the other other three algo-
rithms. In our experiments it nearly converges to the maximum soft margin objective, even though
no theoretical evidence is known for this observed convergence. Among the three remaining algo-
rithms, LPBoost and SoftBoost converge in roughly the same number of iterations, but SoftBoost
has a slower start. BrownBoost terminates in fewer iterations than the other algorithms but does not
maximize the soft margin.4 This is not surprising as there is no theoretical reason to expect such a
result.

2Our code is available at https://sourceforge.net/projects/nboost
3Smaller choices of ν lead to an even slower convergence of SmoothBoost.
4SmoothBoost has two parameters: a guarantee g on the edge of the base learner and the target margin
θ . We chose g = γ ∗ (ν ) (computed with LPBoost) and θ = g/2
2+g/2 as proposed in [21]. Brownboost’s one
parameter, c = 0.35, was chosen via cross-validation.

6

← LPBoost

0.18

0.16

0.14

0.12

0.1

0.08

r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c

SoftBoost →

0.05

0.04

0.03

0.02

0.01

0

−0.01

−0.02

e
v
i
t
c
e
j
b
o
 
n
i
g
r
a
m
 
t
f
o
s

LPBoost →

← BrownBoost

← SoftBoost

SmoothBoost →

0.1

0.2

0.3

0.5

0.8

0.06
0

103

101

0.6

0.7

100

0.4
ν/N
Figure 2: Generalization performance of SoftBoost
(solid) and LPBoost (dotted) on a synthetic data set
with 10% label-noise for different values of ν .

102
number of iterations
Figure 3: Soft margin objective vs. the number of
iterations for LPBoost, SoftBoost, BrownBoost and
SmoothBoost.
Finally, we present a small comparison on ten benchmark data sets derived from the UCI benchmark
repository as previously used in [15]. We analyze the performance of AdaBoost, LPBoost, Soft-
Boost, BrownBoost [6] and AdaBoostReg [15] using RBF networks as base learning algorithm.5
The data comes in 100 prede ﬁned splits into training and test
sets. For each of the splits we use
5-fold cross-validation to select the optimal regularization parameter for each of the algorithms.
This leads to 100 estimates of the generalization error for each method and data set. The means
and standard deviations are given in Table 1.6 As before, the generalization performances of Soft-
Boost and LPBoost are very similar. However, the soft margin algorithms outperform AdaBoost on
most data sets. The genaralization error of BrownBoost lies between that of AdaBoost and Soft-
Boost. AdaBoostReg performs as well as SoftBoost, but there are no iteration bounds known for this
algorithm.

Even though SoftBoost and LPBoost often have similar generalization error on natural datasets, the
number of iterations needed by both algorithms can be radically different (see Theorem 1). Also, in
[22] there are some artiﬁcial data sets where TotalBoost (i. e. SoftBoost with ν = 1) outperformed
LPBoost i.t.o. generalization error.

AdaBoost reg
BrownBoost
SoftBoost
LPBoost
AdaBoost
11.3 ± 0.6
12.9 ± 0.7
11.1 ± 0.5
11.1 ± 0.6
13.3 ± 0.7
Banana
27.3 ± 4.3
30.2 ± 3.9
28.0 ± 4.5
27.8 ± 4.3
32.1 ± 3.8
B.Cancer
24.5 ± 1.7
27.2 ± 1.6
24.4 ± 1.7
24.4 ± 1.7
27.9 ± 1.5
Diabetes
German
25.0 ± 2.2
24.8 ± 1.9
24.7 ± 2.1
24.6 ± 2.1
26.9 ± 1.9
17.6 ± 3.0
20.0 ± 2.8
18.2 ± 2.7
18.4 ± 3.0
20.1 ± 2.7
Heart
1.7 ± 0.2
1.9 ± 0.2
1.8 ± 0.2
1.9 ± 0.2
1.9 ± 0.3∗
Ringnorm
34.4 ± 1.7
36.1 ± 1.4
35.5 ± 1.4
35.7 ± 1.6
36.1 ± 1.5
F.Solar
4.9 ± 2.0
4.6 ± 2.1
4.9 ± 1.9
4.9 ± 1.9
4.4 ± 1.9∗
Thyroid
22.7 ± 1.0
22.8 ± 0.8
23.0 ± 0.8
22.8 ± 1.0
22.8 ± 1.0
Titanic
Waveform 10.5 ± 0.4
10.4 ± 0.7
10.4 ± 0.4
9.8 ± 0.5
10.1 ± 0.5
Table 1: Generalization error estimates and standard deviations for ten UCI benchmark data sets. SoftBoost
and LPBoost outperform AdaBoost and BrownBoost on most data sets.
6 Conclusion

We prove by counterexample that LPBoost cannot have an O(ln N ) iteration bound. This counterex-
ample may seem similar to the proof that the Simplex algorithm for LP can take exponentially more
steps than interior point methods. However this similarity is only super ﬁcial. First, our iteration
bound does not depend on the LP solver used within LPBoost. This is because in the construction,
the interim solutions are always unique and thus all LP solvers will produce the same solution. Sec-
ond, the iteration bound essentially says that column generation methods (of which LPBoost is a
canonical example) should not solve the current subproblem at iteration t optimally. Instead a good
algorithm should loosen the constraints and spread the weight via a regularization such as the rela-
tive entropy. These two tricks used by the SoftBoost algorithm make it possible to obtain iteration

5The data is from http://theoval.cmp.uea.ac.uk/∼gcc/matlab/index.shtml. The RBF
networks were obtained from the authors of [15], including the hyper-parameter settings for each data set.
6Note that [15] contains a similar benchmark comparison. It is based on a different model selection setup
leading to underestimates of the generalization error. Presumably due to slight differences in the RBF hyper-
parameters settings, our results for AdaBoost often deviate by 1-2%.

7

bounds that grow logarithmic in N . The iteration bound for our algorithm is a straightforward ex-
tension of a bound given in [22] that is based on Bregman projection methods. By using a different
divergence in SoftBoost, such as the sum of binary relative entropies, the algorithm morphs into a
“soft” version of LogitBoost (see discussion in [22]) which
has essentially the same iteration bound
as SoftBoost. We think that the use of Bregman projections illustrates the generality of the meth-
ods. Although the proofs seem trivial in hindsight, simple logarithmic iteration bounds for boosting
algorithms that maximize the soft margin have eluded many researchers (including the authors) for
a long time. Note that duality methods typically can be used in place of Bregman projections. For
example in [12], a number of iteration bounds for boosting algorithms are proven with both methods.

On a more technical level, we show that LPBoost may require N/2 examples to get .99 close to the
maximum hard margin. We believe that similar methods can be used to show that Ω(N/δ) examples
may be needed to get δ close. However the real challenge is to prove that LPBoost may require
Ω(N/δ2 ) examples to get δ close.
References

In Proc. COLT ’00, pages

[1] L. Breiman. Prediction games and arcing algorithms. Neural Computation, 11(7):1493–1518, 1999. Also
Technical Report 504, Statistics Department, University of California Berkeley.
[2] Y. Censor and S. A. Zenios. Parallel Optimization. Oxford, New York, 1997.
[3] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.
[4] A. Demiriz, K.P. Bennett, and J. Shawe-Taylor. Linear programming boosting via column generation.
Machine Learning, 46(1-3):225–254, 2002.
[5] C. Domingo and O. Watanabe. Madaboost: A modi ﬁcation of A daboost.
180–189, 2000.
[6] Y. Freund. An adaptive version of the boost by majority algorithm. Mach. Learn., 43(3):293–318, 2001.
[7] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application
to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.
[8] A.J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In
Proceedings of the Fifteenth National Conference on Arti ﬁc al Intelligence, 1998.
[9] Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal of Machine Learning
Research, 1:281–309, 2001.
[10] R. Hettich and K.O. Kortanek. Semi-inﬁnite programmin g: Theory, methods and applications. SIAM
Review, 3:380–429, September 1993.
[11] J. Kivinen and M. K. Warmuth. Boosting as entropy projection.
In Proc. 12th Annu. Conference on
Comput. Learning Theory, pages 134–144. ACM Press, New York, NY, 1999.
[12] J. Liao. Totally Corrective Boosting Algorithms that Maximize the Margin. PhD thesis, University of
California at Santa Cruz, December 2006.
[13] R. Meir and G. R ¨atsch. An introduction to boosting and leveraging.
In S. Mendelson and A. Smola,
editors, Proc. 1st Machine Learning Summer School, Canberra, LNCS, pages 119–184. Springer, 2003.
[14] G. R ¨atsch. Robust Boosting via Convex Optimization: Theory and Applications. PhD thesis, University
of Potsdam, Germany, December 2001.
[15] G. R ¨atsch, T. Onoda, and K.-R. M ¨uller. Soft margins for AdaBoost. Machine Learning, 42(3):287–320,
2001.
[16] G. R ¨atsch, B. Sch ¨olkopf, A.J. Smola, S. Mika, T. Onoda, and K.-R. M ¨uller. Robust ensemble learn-
ing. In A.J. Smola, P.L. Bartlett, B. Sch ¨olkopf, and D. Schuurmans, editors, Advances in Large Margin
Classi ﬁers , pages 207–219. MIT Press, Cambridge, MA, 2000.
[17] G. R ¨atsch and M. K. Warmuth. Efﬁcient margin maximizin g with boosting. Journal of Machine Learning
Research, 6:2131–2152, December 2005.
[18] C. Rudin, I. Daubechies, and R.E. Schapire. The dynamics of adaboost: Cyclic behavior and convergence
of margins. Journal of Machine Learning Research, 5:1557–1595, 2004.
[19] R.E. Schapire, Y. Freund, P.L. Bartlett, and W.S. Lee. Boosting the margin: A new explanation for the
effectiveness of voting methods. The Annals of Statistics, 26(5):1651–1686, 1998.
[20] B. Sch ¨olkopf, A.J. Smola, R.C. Williamson, and P.L. Bartlett. New support vector algorithms. Neural
Comput., 12(5):1207–1245, 2000.
[21] Rocco A. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning
Research, 4:633–648, 2003.
[22] M.K. Warmuth, J. Liao, and G. R ¨atsch. Totally corrective boosting algorithms that maximize the margin.
In Proc. ICML ’06, pages 1001–1008. ACM Press, 2006.

8


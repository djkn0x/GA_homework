A Game-Theoretic Approach to Apprenticeship
Learning

Umar Syed
Computer Science Department
Princeton University
35 Olden St
Princeton, NJ 08540-5233
usyed@cs.princeton.edu

Robert E. Schapire
Computer Science Department
Princeton University
35 Olden St
Princeton, NJ 08540-5233
schapire@cs.princeton.edu

Abstract

We study the problem of an apprentice learning to behave in an environment with
an unknown reward function by observing the behavior of an expert. We follow
on the work of Abbeel and Ng [1] who considered a framework in which the true
reward function is assumed to be a linear combination of a set of known and ob-
servable features. We give a new algorithm that, like theirs, is guaranteed to learn
a policy that is nearly as good as the expert’s, given enough examples. However,
unlike their algorithm, we show that ours may produce a policy that is substantially
better than the expert’s. Moreover, our algorithm is computationally faster, is eas-
ier to implement, and can be applied even in the absence of an expert. The method
is based on a game-theoretic view of the problem, which leads naturally to a direct
application of the multiplicative-weights algorithm of Freund and Schapire [2] for
playing repeated matrix games. In addition to our formal presentation and analysis
of the new algorithm, we sketch how the method can be applied when the transi-
tion function itself is unknown, and we provide an experimental demonstration of
the algorithm on a toy video-game environment.

1

Introduction

When an agent is faced with the task of learning how to behave in a stochastic environment, a com-
mon approach is to model the situation using a Markov Decision Process. An MDP consists of
states, actions, rewards and a transition function. Once an MDP has been provided, the usual objec-
tive is to ﬁnd a policy (i.e. a mapping from states to actions)
that maximizes expected cumulative
reward collected by the agent.

Building the MDP model is usually the most difﬁcult part of th is process. One reason is that it is
often hard to correctly describe the environment’s true reward function, and yet the behavior of the
agent is quite sensitive to this description. In practice, reward functions are frequently tweaked and
tuned to elicit what is thought to be the desired behavior. Instead of maximizing reward, another
approach often taken is to observe and follow the behavior of an expert in the same environment.
Learning how to behave by observing an expert has been called apprenticeship learning, with the
agent in the role of the apprentice.

Abbeel and Ng [1] proposed a novel and appealing framework for apprenticeship learning. In this
framework, the reward function, while unknown to the apprentice, is assumed to be equal to a linear
combination of a set of known features. They argued that while it may be difﬁcult to correctly
describe the reward function, it is usually much easier to specify the features on which the reward
function depends.

1

With this setting in mind, Abbeel and Ng [1] described an efﬁc ient algorithm that, given enough
examples of the expert’s behavior, produces a policy that does at least as well as the expert with
respect to the unknown reward function. The number of examples their algorithm requires from the
expert depends only moderately on the number of features.

While impressive, a drawback of their results is that the performance of the apprentice is both upper-
and lower-bounded by the performance of the expert. Essentially, their algorithm is an efﬁcient
method for mimicking the expert’s behavior. If the behavior of the expert is far from optimal, the
same will hold for the apprentice.

In this paper, we take a somewhat different approach to apprenticeship learning that addresses this
issue, while also signi ﬁcantly improving on other aspects o f Abbeel and Ng’s [1] results. We pose
the problem as learning to play a two-player zero-sum game in which the apprentice chooses a
policy, and the environment chooses a reward function. The goal of the apprentice is to maximize
performance relative to the expert, even though the reward function may be adversarially selected by
the environment with respect to this goal. A key property of our algorithm is that it is able to leverage
prior beliefs about the relationship between the features and the reward function. Speci ﬁcally, if it is
known whether a feature is “good” (related to reward) or “bad
” (inversely related to reward), then the
apprentice can use that knowledge to improve its performance. As a result, our algorithm produces
policies that can be signi ﬁcantly better than the expert’s p olicy with respect to the unknown reward
function, while at the same time are guaranteed to be no worse.

Our approach is based on a multiplicative weights algorithm for solving two-player zero-sum games
due to Freund and Schapire [2]. Their algorithm is especially well-suited to solving zero-sum games
in which the “game matrix” is extremely large. It turns out th at our apprenticeship learning setting
can be viewed as a game with this property.

Our results represent a strict improvement over those of Abbeel and Ng [1] in that our algorithm
is considerably simpler, provides the same lower bound on the apprentice’s performance relative to
the expert, and removes the upper bound on the apprentice’s performance. Moreover, our algorithm
requires less computational expense – speci ﬁcally, we are a
ble to achieve their performance guar-
antee after only O(ln k) iterations, instead of the O(k ln k), where k is the number of features on
which the reward function depends. Additionally, our algorithm can be applied to a setting in which
no examples are available from the expert. In that case, our algorithm produces a policy that is op-
timal in a certain conservative sense. We are also able to extend our algorithm to a situation where
the MDP’s transition function θ is unknown. We conducted experiments from a small car driving
simulation that illustrate some of our theoretical ﬁndings .

Ratliff et al [3] formulated a related problem to apprenticeship learning, in which the goal is to ﬁnd
a reward function whose optimal policy is similar to the expert’s policy. Quite different from our
work, mimicking the expert was an explicit goal of their approach.

2 Preliminaries

Our problem setup largely parallels that outlined in Abbeel and Ng [1]. We are given an inﬁnite-
horizon Markov Decision Process in which the reward function has been replaced by a set of
features. Speci ﬁcally, we are given an MDP \R M = (S , A, γ , D, θ , φφφ), consisting of ﬁnite
state and action sets S and A, discount factor γ , initial state distribution D , transition function
θ(s, a, s0 ) , Pr(st+1 = s0 | st = s, at = a), and a set of k features deﬁned by the function
φφφ : S → Rk .
The true reward function R∗ is unknown. For ease of exposition, we assume that R∗ (s) = w∗ ·φφφ(s),
for some w∗ ∈ Rk , although we also show how our analysis extends to the case when this does not
hold.

For any policy π in M , the value of π (with respect to the initial state distribution) is deﬁned b y
V (π) , E " ∞Xt=0
γ tR∗ (st ) (cid:12)(cid:12)(cid:12) π , θ , D# .
2

where the initial state s0 is chosen according to D , and the remaining states are chosen according to
π and θ . We also deﬁne a k-length feature expectations vector,
µ(π) , E " ∞Xt=0
γ tφφφ(st ) (cid:12)(cid:12)(cid:12) π , θ , D# .
From its deﬁnition, it should be clear that “feature expecta
tions ” is a (somewhat misleading) abbre-
viation for “expected, cumulative, discounted feature val ues.” Importantly, since R∗ (s) = w∗ ·φφφ(s),
we have V (π) = w∗ · µ(π), by linearity of expectation.
We say that a feature expectations vector ˆµ is an -good estimate of µ(π) if k ˆµ − µ(π)k∞ ≤ .
Likewise, we say that a policy ˆπ is -optimal for M if |V ( ˆπ) − V (π∗ )| ≤ , where π∗ is an optimal
policy for M , i.e. π∗ = arg maxπ V (π).1
We also assume that there is a policy πE , called the expert’s policy, which we are able to observe
executing in M . Following Abbeel and Ng [1], our goal is to ﬁnd a policy π such that V (π) ≥
V (πE ) − , even though the true reward function R∗ is unknown. We also have the additional goal
of ﬁnding a policy when no observations from the expert’s policy are available. In that case, we ﬁnd
a policy that is optimal in a certain conservative sense.

Like Abbeel and Ng [1], the policy we ﬁnd will not necessarily be stationary, but will instead be
a mixed policy. A mixed policy ψ is a distribution over Π, the set of all deterministic stationary
policies in M . Because Π is ﬁnite (though extremely large), we can ﬁx a numbering of th
e policies
in Π, which we denote π1 , . . . , π |Π| . This allows us to treat ψ as a vector, where ψ(i) is the
probability assigned to π i . A mixed policy ψ is executed by randomly selecting the policy π i ∈ Π
at time 0 with probability ψ(i), and exclusively following π i thereafter. It should be noted that the
deﬁnitions of value and feature expectations apply to mixed policies as well: V (ψ) = Ei∼ψ [V (π i )]
and µ(ψ) = Ei∼ψ [µ(π i )]. Also note that mixed policies do not have any advantage over stationary
policies in terms of value: if π∗ is an optimal stationary policy for M , and ψ∗ is an optimal mixed
policy, then V (ψ∗ ) = V (π∗ ).
The observations from the expert’s policy πE are in the form of m independent trajectories in M ,
each for simplicity of the same length H . A trajectory is just the sequence of states visited by the
H (cid:1) for the ith trajectory. Let µE = µ(πE ) be the expert’s feature expectations.
expert: (cid:0)si
0 , si
1 , . . . , si
We compute an estimate ˆµE of µE by averaging the observed feature values from the trajectories:
HXt=0
mXi=0
1
γ tφφφ(si
ˆµE =
t ).
m
3 Review of the Projection Algorithm

We compare our approach to the “projection algorithm” of Abb eel and Ng [1], which ﬁnds a policy
that is at least as good as the expert’s policy with respect to the unknown reward function.2
Abbeel and Ng [1] assume that φφφ(s) ∈ [0, 1]k , and that R∗ (s) = w∗ ·φφφ(s) for some w∗ ∈ Bk , where
Bk = {w : kwk1 ≤ 1}. Given m independent trajectories from the expert’s policy, the projection
algorithm runs for T iterations. It returns a mixed policy ψ such that kµ(ψ) − µE k2 ≤  as long as
T and m are sufﬁciently large. In other words, their algorithm seek s to “match” the expert’s feature
expectations. The value of ψ will necessarily be close to that of the expert’s policy, since

|V (ψ) − V (πE )| = |w∗ · µ(ψ) − w∗ · µE |
≤ kw∗ k2 kµ(ψ) − µE k2
≤ 

(1)

where in Eq. (1) we used the Cauchy-Schwartz inequality and kw∗ k2 ≤ kw∗ k1 ≤ 1.

1Note that this is weaker than the standard deﬁnition of optimality, as the policy on ly needs to be optimal
with respect to the initial state distribution, and not necessarily at every state simultaneously.
2Abbeel and Ng [1] actually presented two algorithms for this task. Both had the same theoretical guaran-
tees, but the projection algorithm is simpler and was empirically shown to be slightly faster.

3

The following theorem is the main result in Abbeel and Ng [1]. However, some aspects of their
analysis are not covered by this theorem, such as the complexity of each iteration of the projec-
tion algorithm, and the sensitivity of the algorithm to various approximations. These are discussed
immediately below.
Theorem 1 (Abbeel and Ng [1]). Given an MDP\R, and m independent trajectories from an ex-
pert’s policy πE . Suppose we execute the projection algorithm for T iterations. Let ψ be the mixed
policy returned by the algorithm. Then in order for

|V (ψ) − V (πE )| ≤ 
to hold with probability at least 1 − δ , it sufﬁces that
(1 − γ ) (cid:19)
T ≥ O (cid:18)
k
2k
2k
((1 − γ ))2 ln
.
m ≥
δ
We omit the details of the algorithm due to space constraints, but note that each iteration involves
only two steps that are computationally expensive:

k
((1 − γ ))2 ln

and

(2)

1. Find an optimal policy with respect to a given reward function.
2. Compute the feature expectations of a given policy.

The algorithm we present in Section 5 performs these same expensive tasks in each iteration, but
requires far fewer iterations — just O(ln k) rather than O(k ln k), a tremendous savings when the
number of features k is large. Also, the projection algorithm has a post-processing step that requires
invoking a quadratic program (QP) solver. Comparatively, the post-processing step for our algorithm
is trivial.

Abbeel and Ng [1] provide several reﬁnements of the analysis
in Theorem 1. In particular, suppose
that each sample trajectory has length H ≥ (1/(1 − γ )) ln(1/(H (1 − γ ))), and that an P -optimal
policy is found in each iteration of the projection algorithm (see Step 1 above). Also let R =
minw∈Bk maxs |R∗ (s) − w · φφφ(s)| be the “representation error ” of the features. Abbeel and Ng
[1] comment at various points in their paper that H , P , and O(R ) should be added to the error
bound of Theorem 1. In Section 5 we provide a uni ﬁed analysis o f these error terms in the context
of our algorithm, and also incorporate an F term that accounts for computing an F -good feature
expectations estimate in Step 2 above. We prove that our algorithm is sensitive to these error terms
in a similar way as the projection algorithm.

4 Apprenticeship Learning via Game Playing

Notice the two-sided bound in Theorem 1: the theorem guarantees that the apprentice will do almost
as well as the expert, but also almost as badly. This is because the value of a policy is a linear
combination of its feature expectations, and the goal of the projection algorithm is to match the
expert’s feature expectations.

We will take a different approach. We assume that φφφ(s) ∈ [−1, 1]k , and that R∗ (s) = w∗ · φφφ(s) for
some w∗ ∈ Sk , where Sk = {w ∈ Rk : kwk1 = 1 and w (cid:23) 0}.3 The impact of this minor change
in the domains of w and φφφ is discussed further in Section 5.2. Let Ψ be the set of all mixed policies
in M . Now consider the optimization
v∗ = max
min
ψ∈Ψ
w∈Sk
Our goal will be to ﬁnd (actually, to approximate) the mixed p olicy ψ∗ that achieves v∗ . Since
V (ψ) = w∗ · µ(ψ) for all ψ , we have that ψ∗ is the policy in Ψ that maximizes V (ψ) − V (πE )
with respect to the worst-case possibility for w∗ . Since w∗ is unknown, maximizing for the worst-
case is appropriate.

[w · µ(ψ) − w · µE ] .

(3)

3We use (cid:23) to denote componentwise inequality. Likewise, we use (cid:31) to denote strict inequality in every
component.

4

We begin by noting that, because w and ψ are both distributions, Eq. (3) is in the form of a two-
person zero-sum game. Indeed, this is the motivation for redeﬁning the domain of w as we did.
The quantity v∗ is typically called the game value. In this game, the “min player ” speci ﬁes a reward
function by choosing w, and the “max player ” chooses a mixed policy ψ . The goal of the min player
is to cause the max player’s policy to perform as poorly as possible relative to the expert, and the
max player’s goal is just the opposite. A game is deﬁned by its associated game matrix. In our case,
the game matrix is the k × |Π| matrix
G(i, j ) = µj (i) − µE (i)
(4)
where µ(i) is the ith component of µ and we have let µj = µ(π j ) be the vector of feature expecta-
tions for the j th deterministic policy π j . Now Eq. (3) can be rewritten in the form
v∗ = max
ψ∈Ψ

wT Gψ .

min
w∈Sk

(5)

In Eq. (3) and (5), the max player plays ﬁrst, suggesting that
the min player has an advantage.
However, the well-known minmax theorem of von Neumann says that we can swap the min and max
operators in Eq. (5) without affecting the game value. In other words,

v∗ = max
ψ∈Ψ

min
w∈Sk

wT Gψ = min
w∈Sk

max
ψ∈Ψ

wT Gψ .

(6)

Finding ψ∗ will not be useful unless we can establish that v∗ ≥ 0, i.e. that ψ∗ will do at least as well
as the expert’s policy with respect to the worst-case possibility for w∗ . This fact is not immediately
clear, since we are restricting ourselves to mixtures of deterministic policies, while we do not assume
that the expert’s policy is deterministic. However, note that in the rightmost expression in Eq. (6),
the maximization over Ψ is done after w — and hence the reward function — has been ﬁxed. So
the maximum is achieved by the best policy in Ψ with respect to this ﬁxed reward function. Note
that if this is also an optimal policy, then v∗ will be nonnegative. It is well-known that in any MDP
there always exists a deterministic optimal policy. Hence v∗ ≥ 0.
In fact, we may have v∗ > 0. Suppose it happens that µ(ψ∗ ) (cid:31) µ(πE ). Then ψ∗ will dominate πE ,
i.e. ψ∗ will have higher value than πE regardless of the actual value of w∗ , because we assumed
that w∗ (cid:23) 0. Essentially, by assuming that each component of the true weight vector is nonnegative,
we are assuming that we have correctly speci ﬁed the “sign” of
each feature. This means that, other
things being equal, a larger value for each feature implies a larger reward.
So when v∗ > 0, the mixed policy ψ∗ to some extent ignores the expert, and instead exploits prior
knowledge about the true reward function encoded by the features. We present experimental results
that explore this aspect of our approach in Section 7.

5 The Multiplicative Weights for Apprenticeship Learning (MWAL)
Algorithm

In the previous section, we motivated the goal of ﬁnding the m ixed policy ψ∗ that achieves the
maximum in Eq. (3) (or equivalently, in Eq. (5)). In this section we present an efﬁcient algorithm
for solving this optimization problem.

Recall the game formulated in the previous section. In the terminology of game theory, w and ψ are
called strategies for the min and max player respectively , and ψ∗ is called an optimal strategy for
the max player. Also, a strategy ˙w is called pure if ˙w(i) = 1 for some i.
Typically, one ﬁnds an optimal strategy for a two-player zer o-sum game by solving a linear program.
However, the complexity of that approach scales with the size of the game matrix. In our case, the
game matrix G is huge, since it has as many columns as the number of deterministic policies in the
MDP\R.
Freund and Schapire [2] described a multiplicative weights algorithm for ﬁnding approximately
optimal strategies in games with large or even unknown game matrices. To apply their algorithm to
a game matrix G, it sufﬁces to be able to efﬁciently perform the following tw o steps:

1. Given a min player strategy w, ﬁnd arg maxψ∈Ψ wT Gψ .

5

2. Given a max player strategy ψ , compute ˙wT Gψ for each pure strategy ˙w.

Observe that these two steps are equivalent to the two steps of the projection algorithm from Section
3. Step 1 amounts to ﬁnding the optimal policy in a standard MD P with a known reward function.
There are a huge array of techniques available for this, such as value iteration and policy iteration.
Step 2 is the same as computing the feature expectations of a given policy. These can be computed
exactly by solving k systems of linear equations, or they can be approximated using iterative tech-
niques. Importantly, the complexity of both steps scales with the size of the MDP\R, and not with
the size of the game matrix G.

Our Multiplicative Weights for Apprenticeship Learning (MWAL) algorithm is described below.
Lines 7 and 8 of the algorithm correspond to Steps 1 and 2 directly above. The algorithm is es-
sentially the MW algorithm of Freund and Schapire [2], applied to a game matrix very similar to
G.4 We have also slightly extended their results to allow the MWAL algorithm, in lines 7 and 8, to
estimate the optimal policy and its feature expectations, rather than requiring that they be computed
exactly.

Algorithm 1 The MWAL algorithm
1: Given: An MDP\R M and an estimate of the expert’s feature expectations ˆµE .
T (cid:19)−1
2: Let β = (cid:18)1 + q 2 ln k
.
3: Deﬁne eG(i, µ) , ((1 − γ )(µ(i) − ˆµE (i)) + 2)/4, where µ ∈ Rk .
4: Initialize W(1) (i) = 1 for i = 1, . . . , k .
5: for t = 1, . . . , T do
Set w(t) (i) = W(t) (i)
W(t) (i) for i = 1, . . . , k .
6:
Pi
Compute an P -optimal policy ˆπ (t) for M with respect to reward function R(s) = w(t) ·φφφ(s).
7:
Compute an F -good estimate ˆµ(t) of µ(t) = µ( ˆπ (t) ).
8:
9: W(t+1) (i) = W(t) (i) · exp(ln(β ) · eG(i, ˆµ(t) )) for i = 1, . . . , k .
10: end for
11: Post-processing: Return the mixed policy ψ that assigns probability 1
T to ˆπ (t) , for all t ∈
{1, . . . , T }.

Theorem 2 below provides a performance guarantee for the mixed policy ψ returned by the MWAL
algorithm, relative to the performance of the expert and the game value v∗ . Its correctness is largely
based on the main result in Freund and Schapire [2]. A proof is available in the supplement [4].
Theorem 2. Given an MDP\R M , and m independent trajectories from an expert’s policy πE .
Suppose we execute the MWAL algorithm for T iterations. Let ψ be the mixed policy returned
by the algorithm. Let F and P be the approximation errors from lines 7 and 8 of the al-
gorithm. Let H ≥ (1/(1 − γ )) ln(1/(H (1 − γ ))) be the length of each sample trajectory.
Let R = minw∈Sk maxs |R∗ (s) − w · φφφ(s)| be the representation error of the features. Let
v∗ = maxψ∈Ψ minw∈Sk [w · µ(ψ) − w · µE ] be the game value. Then in order for
V (ψ) ≥ V (πE ) + v∗ − 
to hold with probability at least 1 − δ , it sufﬁces that
9 ln k
2(0 (1 − γ ))2
2
(0 (1 − γ ))2 ln

m ≥

T ≥

2k
δ

(7)

(8)

(9)

where

0 ≤

 − (2F + P + 2H + 2R /(1 − γ ))
3
4Note that eG in Algorithm 1, in contrast to G in Eq. (4), depends on ˆµE instead of µE . This is because
µE is unknown, and must be estimated. The other differences between eG and G are of no real consequence,
and are further explained in the supplement [4].

(11)

.

(10)

6

Note the differences between Theorem 1 and Theorem 2. Because v∗ ≥ 0, the guarantee of the
MWAL algorithm in (7) is at least as strong as the guarantee of the projection algorithm in (2), and
has the further beneﬁt of being one-sided. Additionally, th e iteration complexity of the MWAL algo-
rithm is much lower. This not only implies a faster run time, but also implies that the mixed policy
output by the MWAL algorithm consists of fewer stationary policies. And if a purely stationary
policy is desired, it is not hard to show that the guarantee in (7) must hold for at least one of the
stationary polices in the mixed policy (this is also true of the projection algorithm [1]).

The sample complexity in the Theorem 2 is also lower, but we believe that this portion of our anal-
ysis applies to the projection algorithm as well [Abbeel, personal communication], so the MWAL
algorithm does not represent an improvement in this respect.

5.1 When no expert is available

Our game-playing approach can be very naturally and easily extended to the case where we do not
have data from an expert. Instead of ﬁnding a policy that maxi mizes Eq. (3), we ﬁnd a policy ψ∗
that maximizes

[w · µ(ψ)] .

min
max
ψ∈Ψ
w∈Sk
Here ψ∗ is the best policy for the worst-case possibility for w∗ . The MWAL algorithm can be
trivially adapted to ﬁnd this policy just by setting µE = 0 (compare (12) to (3)).
The following corollary follows straightforwardly from the proof of Theorem 2.
Corollary 1. Given an MDP\R M . Suppose we execute the ‘no expert’ version of the MWAL
algorithm for T iterations. Let ψ be the mixed policy returned by the algorithm. Let F , P , R be
deﬁned as in Theorem 2. Let v∗ = maxψ∈Ψ minw∈Sk [w · µ(ψ)]. Then
V (ψ) ≥ v∗ − 

(12)

(13)

if

where

T ≥

9 ln k
2(0 (1 − γ ))2

0 ≤

 − (2F + P + 2R /(1 − γ ))
3

.

(14)

(15)

5.2 Representation error

Although the MWAL algorithm makes different assumptions about the domains of w and φφφ than the
projection algorithm, these differences are of no real consequence. The same class of reward func-
tions can be expressed under either set of assumptions by roughly doubling the number of features.
Concretely, consider a feature function φφφ that satis ﬁes the assumptions of the projection algorithm.
Then for each s, if φφφ(s) = (f1 , . . . , fk ), deﬁne φφφ0 (s) = (f1 , . . . , fk , −f1 , . . . , −fk , 0). Observe that
φφφ0 satis ﬁes the assumptions of the MWAL algorithm, and that minw∈Bk maxs |R∗ (s) − w · φφφ(s)| ≥
minw∈S2k+1 maxs |R∗ (s) − w · φφφ0 (s)|. So by only doubling the number of features, we can en-
sure that the representation error R does not increase. Notably, employing this reduction forces the
game value v∗ to be zero, ensuring that the MWAL algorithm, like the projection algorithm, will
mimic the expert. This obsevation provides us with some useful guidance for selecting features for
the MWAL algorithm: both the original and negated version of a feature should be used if we are
uncertain how that feature is correlated with reward.

6 When the transition function is unknown

In the previous sections, as well as in Abbeel and Ng [1], it was assumed that the transition function
θ(s, a, ·) was known. In this section we sketch how to remove this assumption. Our approach to
applying the MWAL algorithm to this setting can be informally described as follows: Let M =
(S , A, θ , γ , φ) be the true MDP\R for which we are missing θ . Consider the MLE estimate bθ of θ
that is formed from the expert’s sample trajectories. Let Z ⊆ S × A be the set of state-action pairs
observing enough trajectories, bθ will
that are visited “most frequently” by the expert. Then after
7

be an accurate estimate of θ on Z . We form a pessimistic estimate cMZ of M by using bθ to model
the transitions in Z , and route all other transitions to a special “dead state.” F ollowing Kearns and
Singh [5], who used a very similar idea in their analyis of the E 3 algorithm, we call cMZ the induced
MDP\R on Z .
By a straightforward application of several technical lemmas due to Kearns and Singh [5] and
Abbeel and Ng [6], it is possible to show that if the number of expert trajectories m is at least
O( |S |3 |A|
ln |S |3 |A|
δ + |S ||A| ln 2|S ||A|
), and we let Z be the set of state-action pairs visited by the
83
δ
expert at least O( |S |2
42 ln |S |3 |A|
) times, then using cMZ in place of M in the MWAL algorithm will

add only O() to the error bound in Theorem 2. More details are available in the supplement [4],
including a precise procedure for constructing cMZ .
7 Experiments
For ease of comparison, we tested the MWAL algorithm and the projection algorithm in a car driving
simulator that resembled the experimental setup from Abbeel and Ng [1]. Videos of the experiments
discussed below are available in the supplement [4].

In our simulator, the apprentice must navigate a car through randomly-generated trafﬁc on a three-
lane highway. We deﬁne three features for this environment:
a collision feature (0 if contact with
another car, and 1/2 otherwise), an off-road feature (0 if on the grass, and 1/2 otherwise), and a
speed feature (1/2, 3/4 and 1 for each of the three possible speeds, with higher values corresponding
to higher speeds). Note that the features encode that, other things being equal, speed is good, and
collisions and off-roads are bad.

Speed
Collisions (per sec)
Off-roads (per sec)

Fast Exper t
Fast
1.1
0

Proj
Fast
1.1
0

MWAL
Fast
0.5
0

Bad Exper t
Slow
2.23
8.0

Proj.
Slow
2.23
8.0

MWAL
Medium
0
0

No Exper t
-
-
-

MWAL
Medium
0
0

The table above displays the results of using the MWAL and projection algorithms to learn a driving
policy by observing two kinds of experts: a “fast ” expert (dr
ives at the fastest speed; indifferent to
collisions), and a “bad” expert (drives at the slowest speed ; tries to hit cars and go off-road). In both
cases, the MWAL algorithm leverages information encoded in the features to produce a policy that
is signi ﬁcantly better than the expert’s policy.

(see Section 5.1). In that case, it
We also applied the MWAL algorithm to the “no expert ” setting
produced a policy that drives as fast as possible without risking any collisions or off-roads. Given
our features, this is indeed the best policy for the worst-case choice of reward function.

Acknowledgments

We thank Pieter Abbeel for his helpful explanatory comments regarding his proofs. We also thank
the anonymous reviewers for their suggestions for additional experiments and other improvements.
This work was supported by the NSF under grant IIS-0325500.

References
[1] P. Abbeel, A. Ng (2004). Apprenticeship Learning via Inverse Reinforcement Learning. ICML 21.
[2] Y. Freund, R. E. Schapire (1999). Adaptive Game Playing Using Multiplicative Weights. Games and
Economic Behavior 29, 79–103.
[3] N. Ratliff, J. Bagnell, M. Zinkevich (2006). Maximum Margin Planning. ICML 23.
[4] U. Syed, R. E. Schapire (2007). “A Game-Theoretic Approach to Apprenticeship Learning — Supple-
ment ”. http://www.cs.princeton.edu/ ˜usyed/nips2007/ .
[5] M. Kearns, S. Singh (2002). Near-Optimal Reinforcement Learning in Polynomial Time. Machine Learn-
ing 49, 209–232.
[6] P. Abbeel, A. Ng (2005). Exploration and Apprenticeship Learning in Reinforcement Learning. ICML 22.
(Long version; available at http://www.cs.stanford.edu/ ˜pabbeel/ )

8


One-Pass Boosting

Zafer Barutcuoglu
zbarutcu@cs.princeton.edu

Philip M. Long
plong@google.com

Rocco A. Servedio
rocco@cs.columbia.edu

Abstract

This paper studies boosting algorithms that make a single pass over a set of base
classi(cid:2)ers.
We (cid:2)rst analyze a one-pass algorithm in the setting of boosting with diverse base
classi(cid:2)ers. Our guarantee is the same as the best proved for any boosting algo-
rithm, but our one-pass algorithm is much faster than previous approaches.
We next exhibit a random source of examples for which a (cid:147)picky(cid:148) variant of Ad-
aBoost that skips poor base classi(cid:2)ers can outperform the standard AdaBoost al-
gorithm, which uses every base classi(cid:2)er, by an exponential factor.
Experiments with Reuters and synthetic data show that one-pass boosting can sub-
stantially improve on the accuracy of Naive Bayes, and that picky boosting can
sometimes lead to a further improvement in accuracy.

1 Introduction

Boosting algorithms use simple (cid:147)base classi(cid:2)ers(cid:148) to build more complex, but more accurate, aggre-
gate classi(cid:2)ers. The aggregate classi(cid:2)er typically makes its class predictions using a weighted vote
over the predictions made by the base classi(cid:2)ers, which are usually chosen one at a time in rounds.
When boosting is applied in practice, the base classi(cid:2)er at each round is usually optimized: typically,
each example is assigned a weight that depends on how well it is handled by the previously chosen
base classi(cid:2)ers, and the new base classi(cid:2)er is chosen to minimize the weighted training error. But
sometimes this is not feasible; there may be a huge number of base classi(cid:2)ers with insuf(cid:2)cient
apparent structure among them to avoid simply trying all of them out to (cid:2)nd out which is best. For
example, there may be a base classi(cid:2)er for each word or k -mer. (Note that, due to named entities, the
number of (cid:147)words(cid:148) in some analyses can far exceed the number of words in any natural language.)
In such situations, optimizing at each round may be prohibitively expensive.
The analysis of AdaBoost, however, suggests that there could be hope in such cases. Recall
that if AdaBoost is run with a sequence of base classi(cid:2)ers b1 ; : : : ; bn that achieve weighted er-
ror 1
2 (cid:0) (cid:13)n , then the training error of AdaBoost’s (cid:2)nal output hypothesis is at most
2 (cid:0) (cid:13)1 ; : : : ; 1
t ): One could imagine applying AdaBoost without performing optimization: (a)
exp((cid:0)2 Pn
t=1 (cid:13) 2
(cid:2)xing an order b1 ; :::; bn of the base classi(cid:2)ers without looking at the data, (b) committing to use
base classi(cid:2)er bt in round t, and (c) setting the weight with which bt votes as a function of its
weighted training error using AdaBoost. (In a one-pass scenario, it seems sensible to use AdaBoost
since, as indicated by the above bound, it can capitalize on the advantage over random guessing
of every hypothesis.) The resulting algorithm uses essentially the same computational resources as
Naive Bayes [2, 7], but bene(cid:2)ts from taking some account of the dependence among base classi-
(cid:2)ers. Thus motivated, in this paper we study the performance of different boosting algorithms in a
one-pass setting.
Contributions. We begin by providing theoretical support for one-pass boosting using the (cid:147)diverse
base classi(cid:2)ers(cid:148) framework previously studied in [1, 6]. In this scenario there are n base classi(cid:2)ers.
For an unknown subset G of k of the base classi(cid:2)ers, the events that the classi(cid:2)ers in G are correct
on a random item are mutually independent. This formalizes the notion that these k base classi(cid:2)ers

2 (cid:0) (cid:13) under the initial
are not redundant. Each of these k classi(cid:2)ers is assumed to have error 1
distribution, and no assumption is made about the other n (cid:0) k base classi(cid:2)ers. In [1] it is shown
that if Boost-by-Majority is applied with a weak learner that does optimization (i.e. always uses the
(cid:147)best(cid:148) of the n candidate base classi(cid:2)ers at each of (cid:2)(k) stages of boosting), the error rate of the
combined hypothesis with respect to the underlying distribution is (roughly) at most exp((cid:0)(cid:10)((cid:13) 2k)):
In Section 2 we show that a one-pass variant of Boost-by-Majority achieves a similar bound with a
single pass through the n base classi(cid:2)ers, reducing the computation time required by an (cid:10)(k) factor.
We next show in Section 3 that when running AdaBoost using one pass, it can sometimes be advan-
tageous to abstain from using base classi(cid:2)ers that are too weak. Intuitively, this is because using
many weak base classi(cid:2)ers early on can cause the boosting algorithm to reweight the data in a way
that obscures the value of a strong base classi(cid:2)er that comes later. (Note that the quadratic depen-
dence on (cid:13)t in the exponent of the exp((cid:0)2 Pn
t ) means that one good base classi(cid:2)er is more
t=1 (cid:13) 2
valuable than many poor ones.) In a bit more detail, suppose that base classi(cid:2)ers are considered
in the order b1 ; : : : ; bn , where each of b1 ; : : : ; bn(cid:0)1 has a (cid:147)small(cid:148) advantage over random guessing
under the initial distribution D and bn has a (cid:147)large(cid:148) advantage under D: Using b1 ; : : : ; bn(cid:0)1 for the
(cid:2)rst n (cid:0) 1 stages of AdaBoost can cause the distributions D2 ; D3 ; : : : to change from the initial D1
in such a way that when bn is (cid:2)nally considered, its advantage under Dn is markedly smaller than
its advantage under D0 , causing AdaBoost to assign bn a small voting weight. In contrast, a (cid:147)picky(cid:148)
version of AdaBoost would pass up the opportunity to use b1 ; : : : ; bn(cid:0)1 (since their advantages are
too small) and thus be able to reap the full bene(cid:2)t of using bn under distribution D0 (since when bn
is (cid:2)nally considered the distribution D is still D0 , since no earlier base classi(cid:2)ers have been used).
Finally, Section 4 gives experimental results on Reuters and synthetic data. These show that one-pass
boosting can lead to substantial improvement in accuracy over Naive Bayes while using a similar
amount of computation, and that picky one-pass boosting can sometimes further improve accuracy.

2 Faster learning with diverse base classiﬁers

We consider the framework of boosting in the presence of diverse base classi(cid:2)ers studied in [1].

De ﬁnition 1 (Diverse (cid:13) -good) Let D be a distribution over X (cid:2) f(cid:0)1; 1g: We say that a set G of
classiﬁer s is diverse and (cid:13) -good with respect to D if (i) each classi ﬁer in G has advantage at least
(cid:13) (i.e., error at most 1
2 (cid:0) (cid:13) ) with respect to D, and (ii) the events that the classi ﬁer s in G are correct
are mutually independent under D:

We will analyze the Picky-One-Pass Boost-by-Majority (POPBM) algorithm, which we de(cid:2)ne as
follows. It uses three parameters, (cid:11), T and (cid:15).

1. Choose a random ordering b1 ; :::; bn of the base classi(cid:2)ers in H , and set i1 = 1.
2. For as many rounds t as it (cid:20) minfT ; ng:
(a) De(cid:2)ne Dt as follows: for each example (x; y),
i. Let rt (x; y) be the the number of previously chosen base classi(cid:2)ers h1 ; : : : ; ht(cid:0)1
that are correct on (x; y);
2 (cid:0) (cid:11))d T
2 + (cid:11))b T
2 c(cid:0)rt (x;y)( 1
2 e(cid:0)t(cid:0)1+rt (x;y) , let
ii. Let wt (x; y) = (cid:0)
T (cid:0)t(cid:0)1
2 c(cid:0)rt (x;y)(cid:1)( 1
b T
.
Zt = E(x;y)(cid:24)D (wt (x; y)), and let Dt (x; y) = wt (x;y)D(x;y)
Zt
(b) Compare Zt to (cid:15)=T , and
i. If Zt (cid:21) (cid:15)=T , then try bit ; bit+1 ; ::: until you encounter a hypothesis bj with advan-
tage at least (cid:11) with respect to Dt (and if you run out of base classi(cid:2)ers before this
happens, then go to step 3). Set ht to be bj (i.e. return bj to the boosting algorithm)
and set it+1 to j + 1 (i.e. the index of the next base classi(cid:2)er in the list).
ii. If Zt < (cid:15)=T , then set ht to be the constant-1 hypothesis (i.e. return this constant
hypothesis to the boosting algorithm) and set it+1 = it .
3. If t < T + 1 (i.e. the algorithm ran out of base classi(cid:2)ers before selecting T of them), abort.
Otherwise, output the (cid:2)nal classi(cid:2)er f (x) = M aj (h1 (x); : : : ; hT (x)).

The idea behind step 2.b.ii is that if Zt is small, then Lemma 4 will show that it doesn’t much matter
how good this weak hypothesis is, so we simply use a constant hypothesis.
To simplify the exposition, we have assumed that POPBM can exactly determine quantities such
as Zt and the accuracies of the weak hypotheses. This would provably be the case if D were
concentrated on a moderate number of examples, e.g. uniform over a training set. With slight
complications, a similar analysis can be performed when these quantities must be estimated.
The following lemma from [1] shows that if the (cid:2)ltered distribution is not too different from the
original distribution, then there is a good weak hypothesis relative to the original distribution.

Lemma 2 ([1]) Suppose a set G of classi ﬁer s of size k is diverse and (cid:13) -good with respect to D: For
3 e(cid:13) 2 k=2D(x; y) for all (x; y) 2 X (cid:2) f(cid:0)1; 1g,
any probability distribution Q such that Q(x; y) (cid:20) (cid:13)
there is a g 2 G such that
(1)
2 + (cid:13)
Pr(x;y)(cid:24)Q(g(x) = y) (cid:21) 1
4 :
The following simple extension of Lemma 2 shows that, given a stronger constraint on the (cid:2)ltered
distribution, there are many good weak hypotheses available.

Lemma 3 Suppose a set G of classiﬁer s of size k is diverse and (cid:13) -good with respect to D: Fix any
‘ < k : For any probability distribution Q such that
(cid:13)
e(cid:13) 2 ‘=2D(x; y)
3
for all (x; y) 2 X (cid:2) f(cid:0)1; 1g, there are at least k (cid:0) ‘ + 1 members g of G such that (1) holds.
Proof: Fix any distribution Q satisfying (2). Let g1 ; :::; g‘ be an arbitrary collection of ‘ elements
of G. Since the fg1 ; :::; g‘g and Q satisfy the requirements of Lemma 2 with k set to ‘, one of
g1 ; : : : ; g‘ must satisfy (1); so any set of ‘ elements drawn from G contains an element that satis(cid:2)es
(1). This yields the lemma.

Q(x; y) (cid:20)

(2)

We will use another lemma, implicit in Freund’s analysis [3], formulated as stated here in [1]. It
formalizes two ideas: (a) if the weak learners perform well, then so will the strong learner; and (b)
the performance of the weak learner is not important in rounds for which Z t is small.

Lemma 4 Suppose that Boost-by-Majority is run with parameters (cid:11) and T , and generates clas-
2 + (cid:13)1 ; : : : ; DT (hT (x) = y) = 1
siﬁer s h1 ; :::; hT for which D1 (h1 (x) = y) = 1
2 + (cid:13)T : Then,
for a random element of D, a majority vote over h1 ; :::; hT is incorrect with probability at most
e(cid:0)2(cid:11)2 T + PT
t=1 ((cid:11) (cid:0) (cid:13)t )Zt :
Now we give our analysis.

Theorem 5 Suppose the set H of base classi ﬁer s used by POPBM contains a subset G of k elements
that is diverse and (cid:13) -good with respect to the initial distribution D, where (cid:13) is a constant (say 1=4).
Then there is a setting of the parameters of POPBM so that, with probability 1 (cid:0) 2(cid:0)(cid:10)(k) , it outputs
a classiﬁer with accuracy exp((cid:0)(cid:10)((cid:13) 2k)) with respect to the original distribution D.

Proof: We prove that (cid:11) = (cid:13) =4, T = k=64, and (cid:15) = 3k
8(cid:13) e(cid:0)(cid:13) 2k=16 is a setting of parameters as
required. We will establish the following claim:

Claim 6 For the above parameter settings we have Pr[POPBM aborts in Step 3] = 2(cid:0)(cid:10)(k) .

Suppose for now that the claim holds, so that with high probability POPBM outputs a classi(cid:2)er.
In case it does, let f be this output. Then since POPBM runs for a full T rounds, we may apply
Lemma 4 which bounds the error rate of the Boost-by-Majority (cid:2)nal classi(cid:2)er. The lemma gives us
that D(f (x) 6= y) is at most
T
((cid:11) (cid:0) (cid:13)t )Zt = e(cid:0)(cid:13) 2T =8 + Pt:Zt< (cid:15)
((cid:11) (cid:0) (cid:13)t )Zt + Pt:Zt(cid:21) (cid:15)
Pt=1
T
T
(cid:20) e(cid:0)(cid:10)((cid:13) 2k) + T ((cid:15)=T ) + 0 = e(cid:0)(cid:10)((cid:13) 2 k) :

(Theorem 5)

e(cid:0)2(cid:11)2 T +

((cid:11) (cid:0) (cid:13)t )Zt

The (cid:2)nal inequality holds since (cid:11) (cid:0) (cid:13)t (cid:20) 0 if Zt (cid:21) (cid:15)=T and (cid:11) (cid:0) (cid:13)t (cid:20) 1 if Zt < (cid:15)=T :
Proof of Claim 6: In order for POPBM to abort, it must be the case that as the k base classi(cid:2)ers in
G are encountered in sequence as the algorithm proceeds through h1 ; : : : ; hn , more than 63k=64 of
them are skipped in Step 2.b.i. We show this occurs with probability at most 2(cid:0)(cid:10)(k) :
For each j 2 f1; :::; kg, let Xj be an indicator variable for the event that the j th member of G
in the ordering b1 ; : : : ; bn is encountered during the boosting process and skipped, and for each
‘ 2 f1; :::; kg, let S‘ = minf(P‘
j=1 Xj ) (cid:0) (3=4)‘; k=8g: We claim that S1 ; :::; Sk=8 is a super-
martingale, i.e. that E[S‘+1 jS1 ; : : : ; S‘ ] (cid:20) S‘ for all ‘ < k=8. If S‘ = k=8 or if the boosting
process has terminated by the ‘th member of G, this is obvious. Suppose that S ‘ < k=8 and that the
algorithm has not terminated yet. Let t be the round of boosting in which the ‘th member of G is en-
countered. The value wt (x; y) can be interpreted as a probability, and so we have that wt (x; y) (cid:20) 1.
Consequently, we have that
D(x; y)
T
(cid:13)
(cid:13)
e(cid:13) 2k=16 < D(x; y) (cid:1)
e(cid:13) 2 k=8 :
Dt (x; y) (cid:20)
(cid:20) D(x; y) (cid:1)
= D(x; y) (cid:1)
3
24
(cid:15)
Zt
Now Lemma 3 implies that at least half of the classi(cid:2)ers in G have advantage at least (cid:11) w.r.t. D t .
Since ‘ < k=4, it follows that at least k=4 of the remaining (at most k ) classi(cid:2)ers in G that have not
yet been seen have advantage at least (cid:11) w.r.t. Dt . Since the base classi(cid:2)ers were ordered randomly,
any order over the remaining hypotheses is equally likely, and so also is any order over the remaining
hypotheses from G: Thus, the probability that the next member of G to be encountered has advantage
at least (cid:11) is at least 1=4, so the probability that it is skipped is at most 3=4. This completes the proof
that S1 ; :::; Sk=8 is a supermartingale.
Since jS‘ (cid:0) S‘(cid:0)1 j (cid:20) 1, Azuma’s inequality for supermartingales implies that Pr(Sk=8 > k=64) (cid:20)
e(cid:0)(cid:10)(k) : This means that the probability that at least k=64 good elements were not skipped is at least
1 (cid:0) e(cid:0)O(k) , which completes the proof.

3 For one-pass boosting, PickyAdaBoost can outperform AdaBoost

f (x) =

AdaBoost is the most popular boosting algorithm. It is most often applied in conjunction with a
weak learner that performs optimization, but it can be used with any weak learner. The analysis
of AdaBoost might lead to the hope that it can pro(cid:2)tably be applied for one-pass boosting. In this
section, we compare AdaBoost and its picky variant on an arti(cid:2)cial source especially designed to
illustrate why the picky variant may be needed.
AdaBoost. We brie(cid:3)y recall some basic facts about AdaBoost (see Figure 1). If we run AdaBoost
for T stages with weak hypotheses h1 ; : : : ; hT , it constructs a (cid:2)nal hypothesis
T
H (x) = sgn(f (x)) where
Pt=1
(cid:11)tht (x)
: Here (cid:15)t = Pr(x;y)(cid:24)Dt [ht (x) 6= y ] where Dt is the t-th distribution constructed
with (cid:11)t = 1
2 ln 1(cid:0)(cid:15)t
(cid:15)t
by the algorithm (the (cid:2)rst distribution D1 is just D, the initial distribution). We write (cid:13)t to denote
2 (cid:0) (cid:15)t , the advantage of the t-th weak hypothesis under distribution D t . Freund and Schapire [5]
1
proved that if AdaBoost is run with an initial distribution D over a set of labeled examples, then the
error rate of the (cid:2)nal combined classi(cid:2)er H is at most exp((cid:0)2 PT
t ) under D:
i=1 (cid:13) 2
T
t (cid:19) :
Pr(x;y)(cid:24)D [H (x) 6= y ] (cid:20) exp (cid:18)(cid:0)2
(cid:13) 2
Pi=1
(We note that AdaBoost is usually described in the case in which D is uniform over a training set, but
the algorithm and most of its analyses, including (4), go through in the greater generality presented
here. The fact that the de(cid:2)nition of (cid:11)t depends indirectly on an expectation evaluated according to
D makes the case in which D is uniform over a sample most directly relevant to practice. However,
it is easiest to describe our construction using this more general formulation of AdaBoost.)
PickyAdaBoost. Now we de(cid:2)ne a (cid:147)picky(cid:148) version of AdaBoost, which we call PickyAdaBoost.
The PickyAdaBoost algorithm is initialized with a parameter (cid:13) > 0. Given a value (cid:13) , the Pick-
yAdaBoost algorithm works like AdaBoost but with the following difference. Suppose that Pick-
yAdaBoost is performing round t of boosting, the current distribution is some D 0 , and the current

(3)

(4)

Given a source D of random examples.
(cid:15) Initialize D1 = D.
(cid:15) For each round t from 1 to T :
– Present Dt to a weak learner, and receive base classi(cid:2)er ht ;
;
– Calculate error (cid:15)t = Pr(x;y)(cid:24)Dt [ht (x) 6= y ] and set (cid:11)t = 1
2 ln 1(cid:0)(cid:15)t
(cid:15)t
De(cid:2)ne Dt+1
– Update
setting D 0
by
distribution:
the
=
t+1 (x; y)
exp((cid:0)(cid:11)tyht (x))Dt (x; y) and normalizing D 0
t+1 to get the probability distribu-
tion Dt+1 = D 0
t+1=Zt+1 ;
(cid:15) Return the (cid:2)nal classi(cid:2)cation rule H (x) = sgn (Pt (cid:11)tht (x)) :
Figure 1: Pseudo-code for AdaBoost (from [4]).

base classi(cid:2)er ht being considered has advantage (cid:13) under D 0 , where j(cid:13) j < (cid:13) . If this is the case
then PickyAdaBoost abstains in that round and does not include h t into the combined hypothesis it
is constructing. (Note that consequently the distribution for the next round of boosting will also be
D 0 .) On the other hand, if the current base classi(cid:2)er has advantage (cid:13) where j(cid:13) j (cid:21) (cid:13) , then PickyAd-
aBoost proceeds to use the weak hypothesis just like AdaBoost, i.e. it adds (cid:11) tht to the function f
described in (3) and adjusts D 0 to obtain the next distribution.
Note that we only require the magnitude of the advantage to be at least (cid:13) . Whether a given base
classi(cid:2)er is used, or its negation is used, the effect that it has on the output of AdaBoost is the same
1(cid:0)(cid:15) ). Consequently, the appropriate notion of a (cid:147)picky(cid:148) version of
(brie(cid:3)y, because ln 1(cid:0)(cid:15)
(cid:15) = (cid:0) ln (cid:15)
AdaBoost is to require the magnitude of the advantage to be large.

3.1 The construction

We consider a sequence of n + 1 base classi(cid:2)ers b1 ; : : : ; bn ; bn+1 . For simplicity we suppose that
the domain X is f(cid:0)1; 1gn+1 and that the value of the i-th base classi(cid:2)er on an instance x 2 f0; 1gn
is simply bi (x) = xi :
Now we de(cid:2)ne the distribution D over X (cid:2) f(cid:0)1; 1g. A draw of (x; y) is obtained from D as follows:
the bit y is chosen uniformly from f+1; (cid:0)1g. Each bit x1 ; : : : ; xn is chosen independently to equal
y with probability 1
2 + (cid:13) , and the bit xn+1 is chosen to equal y if there exists an i, 1 (cid:20) i (cid:20) n, for
which xi = y ; if xi = (cid:0)y for all 1 (cid:20) i (cid:20) n then xn+1 is set to (cid:0)y :

3.2 Base classiﬁers

in order b1 ; : : : ; bn ; bn+1
Throughout Section 3.2 we will only consider parameter settings of (cid:13) ; (cid:13) ; n for which (cid:13) < (cid:13) (cid:20)
2 (cid:0) (cid:13) )n : Note that the inequality (cid:13) < 1
2 (cid:0) (cid:13) )n is equivalent to ( 1
2 (cid:0) (cid:13) , which
2 (cid:0) ( 1
2 (cid:0) ( 1
2 (cid:0) (cid:13) )n < 1
1
holds for all n (cid:21) 2:
PickyAdaBoost. In the case where (cid:13) < (cid:13) (cid:20) 1
2 (cid:0) (cid:13) )n , it is easy to analyze the error rate of
2 (cid:0) ( 1
PickyAdaBoost((cid:13)) after one pass through the base classi(cid:2)ers in the order b1 ; : : : ; bn ; bn+1 . Since
each of b1 ; : : : ; bn has advantage exactly (cid:13) under D and bn+1 has advantage 1
2 (cid:0) (cid:13) )n under D,
2 (cid:0) ( 1
PickyAdaBoost((cid:13)) will abstain in rounds 1; : : : ; n and so its (cid:2)nal hypothesis is sgn(bn+1 ((cid:1))), which
is the same as bn+1 : It is clear that bn+1 is wrong only if each xi 6= y for i = 1; : : : ; n, which occurs
with probability ( 1
2 (cid:0) (cid:13) )n . We thus have:

Lemma 7 For (cid:13) < (cid:13) (cid:20) 1
2 (cid:0) ( 1
2 (cid:0) (cid:13) )n , PickyAdaBoost((cid:13)) constructs a ﬁnal hypothesis which has
error rate precisely ( 1
2 (cid:0) (cid:13) )n under D:
AdaBoost. Now let us analyze the error rate of AdaBoost after one pass through the base classi(cid:2)ers
in the order b1 ; : : : ; bn+1 : We write Dt to denote the distribution that AdaBoost uses at the t-th stage
of boosting (so D = D1 ). Recall that (cid:13)t is the advantage of bt under distribution Dt .
The following claim is an easy consequence of the fact that given the value of y , the values of the
base classi(cid:2)ers b1 ; : : : ; bn are all mutually independent:

Claim 8 For each 1 (cid:20) t (cid:20) n we have that (cid:13)t = (cid:13) :

It follows that the coef(cid:2)cients (cid:11)1 ; : : : ; (cid:11)n of b1 ; : : : ; bn are all equal to 1
2 ln 1=2+(cid:13)
2 ln 1+2(cid:13)
1=2(cid:0)(cid:13) = 1
1(cid:0)2(cid:13) :
The next claim can be straightforwardly proved by induction on t:

Claim 9 Let Dr denote the distribution constructed by AdaBoost after processing the base classi-
ﬁer s b1 ; : : : ; br(cid:0)1 in that order. A draw of (x; y) from Dr is distributed as follows:
(cid:15) The bit y is uniform random from f(cid:0)1; +1g;
(cid:15) Each bit x1 ; : : : ; xr(cid:0)1 independently equals y with probability 1
2 , and each bit xr ; : : : ; xn
independently equals y with probability 1
2 + (cid:13) ;
(cid:15) The bit xn+1 is set as described in Section 3.1, i.e. xn+1 = (cid:0)y if and only if x1 = (cid:1) (cid:1) (cid:1) =
xn = (cid:0)y :

Claim 9 immediately gives (cid:15)n+1 = Pr(x;y)(cid:24)Dn+1 [bn+1 (x) 6= y ] = 1=2n. It follows that (cid:11)n+1 =
2 ln(2n (cid:0) 1): Thus an explicit expression for the (cid:2)nal hypothesis of AdaBoost after
2 ln 1(cid:0)(cid:15)n+1
= 1
1
(cid:15)n+1
one pass over the n + 1 classi(cid:2)ers b1 ; : : : ; bn+1 is H (x) = sgn(f (x)), where
1(cid:0)2(cid:13) (cid:17)(cid:17) (x1 + (cid:1) (cid:1) (cid:1) + xn ) + 1
2 (cid:16)ln (cid:16) 1+2(cid:13)
f (x) = 1
2 (ln(2n (cid:0) 1))xn+1 :
Using the fact that H (x) 6= y if and only if yf (x) < 0, it is easy to establish the following:

Claim 10 The classiﬁer H (x) makes a mistake on (x; y) if and only if more than A of the variables
2 + ln(2n(cid:0)1)
x1 ; : : : ; xn disagree with y ; where A = n
:
2 ln 1+2(cid:13)
1(cid:0)2(cid:13)

For (x; y) drawn from source D, we have that each of x1 ; : : : ; xn independently agrees with y with
2 + (cid:13) . Thus we have established the following:
probability 1
Lemma 11 Let B (n; p) denote a binomial random variable with parameters n; p (i.e. a draw from
B (n; p) is obtained by summing n i.i.d. 0=1 random variables each of which has expectation p).
Then the AdaBoost ﬁnal hypothesis error rate is Pr[B (n; 1
2 (cid:0) (cid:13) ) > A], which equals
n
i (cid:19)(1=2 (cid:0) (cid:13) )i (1=2 + (cid:13) )n(cid:0)i :
Pi=bAc+1 (cid:18)n
In terms of Lemma 11, Lemma 7 states that the PickyAdaBoost((cid:13)) (cid:2)nal hypothesis has error
2 (cid:0) (cid:13) ) (cid:21) n]: We thus have that
if A < n (cid:0) 1 then AdaBoost’s (cid:2)nal hypothesis has
Pr[B (n; 1
greater error than PickyAdaBoost.
We now give a few concrete settings for (cid:13) , n with which PickyAdaBoost beats AdaBoost. First
we observe that even in some simple cases the AdaBoost error rate (5) can be larger than the Pick-
yAdaBoost error rate by a fairly large additive constant. Taking n = 3 and (cid:13) = 0:38, we (cid:2)nd that
the error rate of PickyAdaBoost((cid:13)) is ( 1
2 (cid:0) 0:38)3 = 0:001728; whereas the AdaBoost error rate is
2 + 0:38) = 0:03974.
( 1
2 (cid:0) 0:38)3 + 3( 1
2 (cid:0) 0:38)2 (cid:1) ( 1
Next we observe that there can be a large multiplicative factor difference between the AdaBoost and
PickyAdaBoost error rates. We have that Pr[B (n; 1=2 (cid:0) (cid:13) ) > A] equals Pn(cid:0)bAc(cid:0)1
(cid:0)n
i (cid:1)(1=2 (cid:0)
i=0
(cid:13) )n(cid:0)i (1=2 + (cid:13) )i : This can be lower bounded by
n(cid:0)bAc(cid:0)1
Pi=0 (cid:18)n
i (cid:19);
this bound is rough but good enough for our purposes. Viewing n as an asymptotic parameter and (cid:13)
as a (cid:2)xed constant, we have
(cid:11)n
Pi=0 (cid:18)n
i (cid:19)

Pr[B (n; 1=2 (cid:0) (cid:13) ) > A] (cid:21) (1=2 (cid:0) (cid:13) )n

(6) (cid:21) (1=2 (cid:0) (cid:13) )n

(5)

(6)

(7)

i (cid:1) = 2n(cid:1)(H ((cid:11))(cid:6)o(1)) , which holds for
(cid:0) o(1): Using the bound P(cid:11)n
where (cid:11) = 1
i=0 (cid:0)n
2 (cid:0) ln 2
2 ln 1+2(cid:13)
1(cid:0)2(cid:13)
2 , we see that any setting of (cid:13) such that (cid:11) is bounded above zero by a constant gives an
0 < (cid:11) < 1
exponential gap between the error rate of PickyAdaBoost (which is (1=2 (cid:0) (cid:13) )n) and the lower bound
on AdaBoost’s error provided by (7). As it happens any (cid:13) (cid:21) 0:17 yields (cid:11) > 0:01. We thus have

Claim 12 For any ﬁxed (cid:13) 2 (0:17; 0:5) and any (cid:13) < (cid:13) , the ﬁnal error rate of AdaBoost on the
source D is 2(cid:10)(n) times that of PickyAdaBoost((cid:13)).

3.3 Base classiﬁers

in an arbitrary ordering

The above results show that PickyAdaBoost can outperform AdaBoost if the base classi(cid:2)ers are
considered in the particular order b1 ; : : : ; bn+1 : A more involved analysis (omitted because of space
constraints) establishes a similar difference when the base classi(cid:2)ers are chosen in a random order:

Proposition 13 Suppose that 0:3 < (cid:13) < (cid:13) < 0:5 and 0 < c < 1 are ﬁxed constants independent of
4
ln
def
(1(cid:0)2(cid:13) )2
: Suppose the base classi ﬁer s are listed in an
n that satisfy Z ((cid:13) ) < c, where Z ((cid:13) )
=
ln 1+2(cid:13)
(1(cid:0)2(cid:13) )3
order such that bn+1 occurs at position c (cid:1) n: Then the error rate of AdaBoost at least 2n(1(cid:0)c) (cid:0) 1 =
2(cid:10)(n) times greater than the error of PickyAdaBoost((cid:13)).

For the case of randomly ordered base classi(cid:2)ers, we may view c as a real value that is uniformly
distributed in [0; 1]; and for any (cid:2)xed constant 0:3 < (cid:13) < 0:5 there is a constant probability (at least
1 (cid:0) Z ((cid:13) )) that AdaBoost has error rate 2(cid:10)(n) times larger than PickyAdaBoost((cid:13)). This probability
can be fairly large, e.g. for (cid:13) = 0:45 it is greater than 1=5:

4 Experiments

We used Reuters data and synthetic data to examine the behavior of three algorithms: (i) Naive
Bayes; (ii) one-pass Adaboost; and (iii) PickyAdaBoost.
The Reuters data was downloaded from www.daviddlewis.com . We used the ModApte splits
into training and test sets. We only used the text of each article, and the text was converted into
lower case before analysis. We compared the boosting algorithms with multinomial Naive Bayes
[7]. We used boosting with con(cid:2)dence-rated base classi(cid:2)ers [8], with a base classi(cid:2)er for each stem
of length at most 5; analogously to the multinomial Naive Bayes, the con(cid:2)dence of a base classi(cid:2)er
was taken to be the number of times its stem appeared in the text. (Schapire and Singer [8, Section
3.2] suggested, when the con(cid:2)dence of base classi(cid:2)ers cannot be bounded a priori, to choose each
voting weight (cid:11)t in order to maximize the reduction in potential. We did this, using Newton’s
method to do this optimization.) We averaged over 10 random permutations of the features. The
results are compiled in Table 1. The one-pass boosting algorithms usually improve on the accuracy
of Naive Bayes, while retaining similar simplicity and computational ef(cid:2)ciency. PickyAdaBoost
appears to usually improve somewhat on AdaBoost. Using a t-test at level 0.01, the W-L-T for
PickyAdaBoost(0:1) against multinomial Naive Bayes is 5-1-4.
We also experimented with synthetic data generated according to a distribution D de(cid:2)ned as follows:
to draw (x; y), begin by picking y 2 f(cid:0)1; +1g uniformly at random. For each of the k features
x1 ; : : : ; xk in the diverse (cid:13) -good set G, set xi equal to y with probability 1=2 + (cid:13) (independently
for each i). The remaining n (cid:0) k variables are in(cid:3)uenced by a hidden variable z which is set
independently to be equal to y with probability 4=5. The features xk+1 ; : : : ; xn are each set to
be independently equal to z with probability p. So each such xj (j (cid:21) k + 1) agrees with y with
probability (4=5) (cid:1) p + (1=5) (cid:1) (1 (cid:0) p).
There were 10000 training examples and 10000 test examples. We tried n = 1000 and n = 10000.
Results when n = 10000 are summarized in Table 2. The boosting algorithms predictably perform
better than Naive Bayes, because Naive Bayes assigns too much weight to the correlated features.
The picky boosting algorithm further ameliorates the effect of this correlation. Results for n = 1000
are omitted due to space constraints: these are qualitatively similar, with all algorithms performing
better, and the differences between algorithms shrinking somewhat.

Data

NB

OPAB

earn
acq
money-fx
crude
grain
trade
interest
wheat
ship
corn

0.042
0.036
0.043
0.026
0.038
0.068
0.026
0.022
0.013
0.027

0.023
0.094
0.042
0.031
0.021
0.028
0.032
0.014
0.018
0.014

Error rates
PickyAdaBoost
0.1
0.01
0.001
0.027
0.018
0.020
0.153
0.071
0.065
0.041
0.041
0.048
0.040
0.026
0.027
0.018
0.019
0.023
0.029
0.026
0.028
0.029
0.032
0.035
0.017
0.013
0.013
0.016
0.017
0.018
0.014
0.014
0.013

NB

19288
19288
19288
19288
19288
19288
19288
19288
19288
19288

Feature counts
OPAB
PickyAdaBoost
0.1
0.01
0.001
52
542
2871
41
508
3041
2288
576
62
58
697
2865
64
650
2622
61
641
2579
2002
501
58
61
632
2294
67
804
2557
2343
640
67

19288
19288
19288
19288
19288
19288
19288
19288
19288
19288

Table 1: Experimental results. On the left are error rates on the 3299 test examples for Reuters
data sets. On the right are counts of the number of features used in the models. NB is the multino-
mial Naive Bayes, and OPAB is one-pass AdaBoost. Results are shown for three PickyAdaBoost
thresholds: 0.001, 0.01 and 0.1.

k

p

(cid:13)

NB OPAB

20
20
20
50
50
50
100
100
100

0.85
0.9
0.95
0.7
0.75
0.8
0.63
0.68
0.73

0.24
0.24
0.24
0.15
0.15
0.15
0.11
0.11
0.11

0.2
0.2
0.21
0.2
0.2
0.21
0.2
0.2
0.2

0.11
0.09
0.06
0.13
0.12
0.11
0.14
0.13
0.1

PickyAdaBoost
0.16
0.1
0.07
0.03
0.04
0.04
0.03
0.03
0.03
0.02
0.02
0.02
0.09
0.04
0.06
0.03
0.04
0.05
0.03
0.04
0.03
0.07
0.05
0.05
0.06
0.05
0.04

Table 2: Test-set error rate for synthetic data. Each value is an average over 100 independent runs
(random permutations of features). Where a result is omitted, the corresponding picky algorithm did
not pick any base classi(cid:2)ers.

References
[1] S. Dasgupta and P. M. Long. Boosting with diverse base classi(cid:2)ers. COLT, 2003.
[2] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley, 1973.
[3] Y. Freund. Boosting a weak learning algorithm by majority. Inf. and Comput., 121(2):256(cid:150)285,
1995.
[4] Y. Freund and R. Schapire. Experiments with a new boosting algorithm. In ICML, pages 148(cid:150)
156, 1996.
[5] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. JCSS, 55(1):119(cid:150)139, 1997.
[6] N. Littlestone. Redundant noisy attributes, attribute errors, and linear-threshold learning using
Winnow. In COLT, pages 147(cid:150)156, 1991.
[7] A. Mccallum and K. Nigam. A comparison of event models for naive bayes text classi(cid:2)cation.
In AAAI-98 Workshop on Learning for Text Categorization, 1998.
[8] R. Schapire and Y. Singer. Improved boosting algorithms using con(cid:2)dence-rated predictions.
Machine Learning, 37:297(cid:150)336, 1999.


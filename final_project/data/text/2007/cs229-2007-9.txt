Multiple Ob ject Detection with Optimized Spatial
Weighting

Jay Ni, Kiat Chuan Tan, Takashi Yonebayashi

Abstract

This paper il lustrates a method to enhance spa-
tial weighting in object recognition as presented by
Marszalek and Schmid [2]. The original algorithm
uses spatial relations and boundaries to weight rele-
vant areas of an image, and is robust for single object
detection. We have derived a method that is both less
expensive to run, and can detect multiple object clus-
ters in an image in a single pass. The two methods de-
scribed in this paper wil l be cal led Nearest Neighbor
Weighting (NNW) and Correlation Tree Elim-
ination (CTE). Our tests have shown CTE to be
eﬀective, but NNW to be somewhat ineﬀective.

1 Introduction

The Bag-of-Features representation is a popular im-
age classiﬁcation technique that uses local descrip-
tors to create a visual vocabulary to represent image
data. We achieve this representation by ﬁrst clus-
tering a random set of local image descriptors, and
then discretizing the descriptors of a particular im-
age to the closest respective words in our visual vo-
cabulary. While this technique performs very well
under simple circumstances, it is still vulnerable to
intra-class variation, background clutter, and pose
changes. The resulting descriptors obtained from un-
wanted noise makes Bag-of-Features less eﬀective by
increasing false positives. Zhang et al.
[4] suggested
that using context information within an image could
signiﬁcantly improve the test accuracy under such
conditions.

Marszalek and Schmid [2] extended this idea and
proposed a method for ob ject recognition that ex-
ploits the spatial relations of an ob ject to decrease
background clutter in training images. Using local-
ized features, they were able to generate segmenta-
tion masks for training images that gave the target

ob jects a stronger weight than the background (See
Figure 1). The algorithm, known as spatial weight-
ing, led to a 1.5% improvement over the original ERR
(Error Equal Rates) of 94.5% demonstrated by Zhang
et al on the PASCAL training data. [4]. This improve-
ment was mostly due to a higher success rates on the
more diﬃcult PASCAL training sets containing high
background clutter. There was no signiﬁcant improve-
ment over images which did lacked background noise,
as there was little or no background to remove. Never-
theless, their results showed promising improvements
over training images that few classiﬁers can utilize.

Figure 1: Test images of Graz02 data set (left), generated
masks (middle), multiplication of two (on right). Source:
Marszalek and Schmid [2]

Marszalek and Schmid [2] further proposed an ex-
tention their spatial weighting algorithm by using the
segmentation masks to localize the target ob ject of an
image. We can achieve this by selecting the point in
the segmentation mask with the highest probability,
and applying thresholding to determine the bound-
aries of the image. However, the segmentation masks
generated by the original spatial weighting algorithm
are only eﬀective for images where only one ob ject
is present. Furthermore, the original algorithm is ex-
tremely runtime intensive during testing, as it involves
convolving the image several times for each descrip-

1

tor in our test image. We have developed two multi-
class variations (NNW and CTE) of the segmentation
mask that will allow a single mask to detect multiple
ob jects in one image. These methods are not com-
putationally intensive and have shown signiﬁcant im-
provements when compared to no weighting during
our initial tests.

We will ﬁrst describe our implementation for fea-
ture extraction based on the work of Marszalek and
Schmid [2] in section 2.
In section 3, we present a
summary of the original spatial weighting algorithm
and our two algorithms, NNW and CTE. Section 4
describes our ﬁnal results, and proposed extensions
to our work.

tracted by the HS detector. Using the HS points as
the centers of 8 x 8 pixel windows, we ﬁrst weighted
the magnitudes of the pixels in each window using a
Gaussian window of σ = 4.0. For each pixel, we then
calculated its discretized relative orientation and its
weighted contribution to its 4 x 4 grid (See Figure 2).
The result was a 32-dimensional feature vector, which
we normalized to unity to negate the eﬀect of illumi-
nation. The ﬁnal vector was clipped to a threshold to
ﬁlter out extremities, and then renormalized for train-
ing. Since our method diﬀers from the conventional
SIFT algorithm in keypoint extraction, we did not use
an external library to compute the descriptors, as it
was more practical to implement by hand.

2 Feature selection and Bag-of-
Features representation

Our feature selection is primarily based on the work
of Marszalek and Schmid [2] as an extension of Zhang
et al.
[4]. Our method ﬁrst extracts several local
image descriptors (32-dimensional HS-SIFT and LS-
SIFT) from each class in our training set, which it
then clusters to create the bag-of-features over sev-
eral classes of ob jects. This diﬀers from the original
method, which clusters the centroids over all descrip-
tors randomly sampled from the entire data set, re-
gardless of class. For a given test image, classiﬁcation
is based on the conﬁdence of which class a particular
descriptor belongs to. Several descriptors with high
conﬁdence intervals are then chained together to cre-
ate the boundary of the ob ject.

2.1 HS-SIFT and LS-SIFT

For scale-invariant feature detection, we implemented
both the Harris-Laplace (HS) and Laplacian (LS) de-
tectors for extracting corner and edge regions of our
image. Our detectors, based on Lowe’s implementa-
tion [1], sampled descriptors over 3 octaves of the im-
age, using diﬀerence-of-gaussians (DoG) to calculate
relevant keypoints. For all training and test images,
we limited our number of descriptors to 400 HS-SIFT
and 600 LS-SIFT to speed up both training and clas-
siﬁcation.

We then used Lowe’s SIFT descriptor [1] to com-
pute the gradient orientations of the local regions ex-

Figure 2: The calculation of a keypoint descriptor,
weighted by a Gaussian window (in blue) and accumu-
lated over 4x4 subregions. These generate a 4x8 = 32-
dimensional feature space. Source: Lowe [1]

2.2 Class Based K-Means Clustering

During training, we sampled approximately 5000 de-
scriptors from each of our ﬁve classes of images (mug,
stapler, scissors, clock, and keyboard). Using K-
Means clustering with k = 200, we clustered each set
of 5000 descriptors, obtaining 5 centroid sets of 200
elements each (1000 centroids total). We can then
assign each raw descriptor a conﬁdence interval and
a label by the closest (in 32-dimensional space) Eu-
clidean distance centroid from all 1000 centroids.

During testing, after extracting our descriptors
and running our ﬁlter (NNW/CTE), we determined
each descriptor’s closest centroid in our class based k-
means centroids. The conﬁdence level of the descrip-
tor matching the centroid is given by the diﬀerence
in the angles of their orientations in 32-dimensional
A · B
.) We then ﬁlter the descriptors
space (
2 + ||B ||2
||A||2
2
to only those that have a conﬁdence level greater than

2

0.9, and for each descriptor, we search for other de-
scriptors within δ pixel distance to the current de-
scriptor (where δ increases with conﬁdence), and re-
peat this process until we have a chain of descriptors.
When the chain exceeds a certain threshold, the pixel
location bounds of the chain determine the location
of an ob ject. For our purposes, we chose a threshold
of 32 descriptors. Figure 3 illustrates the technique of
chaining descriptors together.

Figure 3: Chaining descriptors together to detect the lo-
cation of an ob ject.

Figure 4: Pseudocode for Marszalek and Schmid’s algo-
rithm. Source: Marszalek and Schmid [2]

3 Spatial Weighting

Spatial weighting is a technique that reduces the in-
ﬂuence of background clutter and thus highlights de-
scriptors that are unique to the image. For example,
it is possible for a training image for a bike to have
both a bike and a dog in the background, whereas
we are only truly interested in the bike for training
purposes. Using the visual vocabulary from HS-SIFT
and LS-SIFT, we can hypothesize about the location
of an ob ject given its spatial relations to words in the
visual vocabulary.

These hypothesis allow us to weight relevant ar-
eas of the image, and the least weighted portions of
the image correspond to background clutter and con-
tribute signiﬁcantly less to the overall image. Marsza-
lek and Schmid’s method ﬁrst calculates the N = 100
closest descriptors to each descriptor in the test im-
age, and generates a segmentation mask by repeatedly
convolving the mask with Gaussians based on the de-
scriptor locations. (See Figure 4).

We have developed two alternatives to spatial
weighting that are less computationally expensive and
support multiple-ob ject detection. Both methods are
eﬀective at ﬁltering out background descriptors, al-
though both methods make the general assumption
that background descriptors are generally weak and

do not have a strong correlation with other descrip-
tors in the image. Our tests indicate that CTE is
more eﬀective than NNW.

3.1 Nearest Neighbor Weighting

NNW is heavily based on the original method of spa-
tial weighting. However, instead of obtaining infor-
mation from the training information ground truth,
it na¨ıvely assumes that the general shape of an ob ject
can be approximated by a sum of Gaussians. We re-
peat the process of weighting the entire image over the
descriptors which fall into the top N = 25 centroids.

Figure 5: Result of using NNW on the ﬁrst few frames of
the CS221 test video.

Although NNW was eﬀective at eliminating inter-
est points beyond the weighted boundaries, the issue
was that it could not eliminate enough points within
the interest boundary that were not necessarily spe-

3

ciﬁc to the ob ject. For example, the orientation of
the hands, or the shape of the numbers on a clock
are not indicative of whether the ob ject is a clock it-
self. Furthermore, NNW was poor at eliminating in-
terest points on more elongated ob jects (such as key-
boards). We also noticed that although NNW elim-
inated background noise, the algorithm is not guar-
anteed to converge and can often alternate between
diﬀerent weighting conﬁgurations. In general, NNW
did not improve performance overall and we will not
discuss it further.

3.2 Correlation Tree Elimination

CTE is our original algorithm that uses the same idea
of spatial weighting, but at a very localized level. The
idea behind CTE is to construct several correlation
trees out of the descriptors in our set, and classify
only based on descriptors that are in a correlation tree
that exceeds some size threshold T. To construct each
correlation tree, we ﬁrst compute the closest N = 10
centroids to each descriptor in our test image. For
each descriptor, we then check every other descriptor
to see if it matches the top N centroids, and add re-
cursively add it to the tree, performing a depth-ﬁrst
search. We do not insert elements into the tree if they
were previously encountered. The result is a partition
of all descriptors into several trees of varying lengths.
All descriptors that belong in a certain tree of length
less than T = 32 are eliminated.

Figure 6: Pseudocode for CTE algorithm

4

CTE yielded very positive results for eliminating
background noise. Figure 7 shows illustrates the re-
sult of running CTE on CS221 test video. It success-
fully eliminates almost poor keypoints caused by the
granular texture of the wall and board.

Figure 7: CTE example of keypoint elimination on the
CS221 vision pro ject. Before (left) and after (right).

4 Results and Extensions

We ran our test algorithm on both the easy and mod-
erate CS221 testing videos. The easy video has almost
no background clutter, and the moderate video has a
fair amount of background noise. We used the scoring
algorithm as speciﬁced in CS221, which computed the
area overlap between pairs of correct ob jects penalized
by the number of true negatives and false positives.
The results are shown by the following graphs:

Improved results using the CTE algorithm were
mostly from ﬁltering out false positives, as expected.
In both cases, the CTE yields signiﬁcantly higher
scores than running on our entire set of HS-SIFT and
LS-SIFT descriptors.
It should be noted, however,
that our algorithm for detection was not particularly
accurate for all ob jects, and our current results are
not representative of an improvement over the orig-
inal method my Marszalek and Schmid. However,
CTE was able to serve our purposes by eliminating
background keypoints quickly while still having the
ability to identify multiple ob jects in one frame. The
reason for this is because CTE keeps all relevant key-
points that belong to some contour of some ob ject. To
show that CTE does not in fact decrease the rate of
general ob ject detection, we ran both test videos with
all labels untagged. The CTE scored an average of
0.8696 on the moderate video, compared with 0.8697
without. On the easy video, CTE scored 0.8809 and
0.8916 without. The insigniﬁcant diﬀerence between
score shows that our improved results from CTE is
not due to a lack ob jects being detected.

sults in the CS221 test videos. CTE is a viable alter-
native to spatial weighting, especially in cases where
speed is needed, such as classifying a streaming video.

References

[1] D. Lowe, “Distinctive Image Features from Scale-
Invariant Keypoints,” International Journal of
Computer Vision, (submitted), 2003

[2] M. Marszalek, C. Schmid, “Spatial Weighting
for Bag-of-Features,” IEEE Computer Society
Conference on & Computer Vision and Pattern
Recognition, V2, pp. 2118-2125, 2006

[3] S. Winder, M. Brown, “Learning Local Image
Descriptors,” IEEE Computer Society Confer-
ence on & Computer Vision and Pattern Recog-
nition, pp. 1-8, 2007

[4] J. Zhang, M. Marszalek, S. Lazebnik, C.
Schmid, “Local features and kernels for classi-
ﬁcation of texture and ob ject categories: An in-
depth study.” Technical Report RR-5737, INRIA
Rhone-Alpes, 2005

Figure 8: Score per frame for the easy and moderate
CS221 test videos. Results shown are run with and with-
out CTE.

4.1 Future extensions and Summary

There are several potential extensions to our CTE al-
gorithm. One potential extension is the addition of
CTE to the training process. When training images
contain a signiﬁcant amount of background clutter, it
is possible to approximate centroids using descriptors
of previously trained images, and use those centroids
to eliminate the clutter in future images. We believe
that some of the inaccuracy from testing is due to
inaccurate descriptors during training, such as logos
on mugs, numbers on clocks, and other various noise
that may appear in the background. Another pos-
sible extension is to use the partitions generated by
CTE as features for bag-of-features. Partitions using
CTE generate edge curvature like features that may
be eﬀective as an addition to our HS and LS-SIFT
descriptors.

In this paper, we have shown a new and eﬃcient
algorithm for ﬁltering out unwanted background key-
points of an image. We demonstrated that CTE is
eﬀective and signiﬁcantly improved our initial test re-

5


Patent Cases Docket Classi(cid:12)cation

Ioannis Antonellis (cid:3)

Panagiotis Papadimitriou y

Abstract

We contribute to the Intellectual Property Litigation
Clearinghouse (IPLC) pro ject by providing an extensive
experimental evaluation of two classi(cid:12)cation techniques,
namely classi(cid:12)cation trees and support vector machines.
We focus on dockets belonging to the \Claim Construc-
tion order" class and we build classi(cid:12)ers that achieve
up to 87% precision with 88% recall. This provides
a 79% improvement on the best rules-based domain-
speci(cid:12)c classi(cid:12)er that IPLC possess.

1 Problem Description

Our work was a contribution to the Intellectual Prop-
erty Litigation Clearinghouse (IPLC) [4].
IPLC aims
to be a comprehensive online information source on IP
lawsuits and will host general statistical information, as
well as text-searchable dockets, complaints, select mo-
tions, judicial opinions, and related data.
The dockets that are going to be searchable com-
prise of 2-3 lines of human-written text and are ac-
companied by some pdf documents with 20-30 pages
of images and text. These dockets are indexed by the
Patent Case they refer to and can be classi(cid:12)ed to ap-
proximately 40 di(cid:11)erent classes such as Order, Motion,
Patent, Judgement, etc..
In Fig. 1 we provide an ex-
ample of the dockets that belong to a speci(cid:12)c patent
case. The information that the docket text conveys
is unstructured, since there are no rules for writing a
docket. People seem to follow general conventions for
the docket text of particular classes, but this is not true
for all the cases. For example, most of the docket texts
of class "Answer" start with the word ANSWER in up-
percase letters. This convention results in an easy rule
for the identi(cid:12)cation of dockets from a particular class.
However, there are classes where simple rules cannot be
derived; an example of such a class ("Marksman") is
shown in the same (cid:12)gure.
A human can classify any docket to a class, since
he has access to the pdf documents that are attached to
it. However, since the pdf documents are scanned, we

(cid:3)Computer Science Dept, Stanford University.
antonell@cs.stanford.edu
yElectrical Engineering Dept, Stanford University. Email:
ppapadim@stanford.edu

Email:

Figure 1: Dockets of a Patent Case

need to apply some OCR method to get their content
in machine-readable format and provide it as input to a
learning algorithm. Given the cost of such a procedure
we cannot consider it as feasible and any classi(cid:12)cation
algorithm should constrain its input to the rest of the
features that are available. The goal of our pro ject
was in the (cid:12)rst place to investigate whether there is
enough information in the rest of the features for the
classi(cid:12)cation of a docket and then build an optimal
classi(cid:12)er if that was possible.

2 Dataset

2.1 Description One of the non-trivial classes for
classi(cid:12)cation is the "Claim Construction Order" (CCO)
class. Our Dataset consists of 3674 dockets that were
classi(cid:12)ed by an editorial team. 899 of them are "Claim
Construction Order" (CCO) dockets and the rest 2775
are dockets of other classes. The dockets of the Dataset
refer to 1369 di(cid:11)erent cases and in general case they are
not related to each other, e.g. the non-CCO dockets are
not responses to CCO dockets.

2.2 Preparation The dockets preprocessing steps
performed are the following: i) Lexical analysis; ii) stop-
word elimination, that is the removal of very frequent
words such as articles, prepositions, conjunctions, et.
that carry little information about the contents of the
processed dockets; iii) stemming, that is the replace-
ment of all variants of a word with a single common

2

Table 1: Local and Global term weighting schemes
Formula

t
b
l
a
n

Symbol Name
Local term-weighting (lij )
Term frequency
Binary
Logarithmic
Alternate log
Augmented normalized term frequency
Global term-weighting (gi )
None
Entropy
Inverse document frequency (IDF)
GfIdf

x
e

fij
b(fij )
log2 (1 + fij )
b(fij )(1 + log2 fij )
(b(fij ) + (fij = maxk fkj ))=2

1
1 + (Pj (pij log2 (pij ))= log2 n)
log2 (n= Pj b(fij ))
(Pj fij )=(Pj b(fij ))
1=qPj f 2
ij
log2 ((n (cid:0) Pj b(fij ))= Pj b(fij ))

f

g

n

p

Normal

probabilistic Inverse

Figure 2: Data Preparation

stem; iv) index-term selection, that is the selection of
a subset of words encountered in the dockets to form
the docket index;v) index construction. These steps are
illustrated in Fig. 2. The output of this process is a
term document matrix A. Each element (cid:11)ij of the term
document matrix A measures the importance of term i
in docket j and in the entire collection. There has been
proposed various term weighting schemes using alter-
native functions for the local and global weighting for
a term. Table 1 tabulates the various local and global
weighting schemes we considered in our experiments.
Another usual practise is the use of normalization for
each docket vector. This normalization factor is used for
the obliteration of bias towards longer documents. For
the implementation of the preprocessing step we used
the matlab toolbox Term Matrix Generator (TMG).
The dimension of our feature vector is n = 4947.
After the extraction of the feature vector for each

docket we divided our dataset into a training and a
test set. The training set included 70% of the positive
training examples and 70% of the negative examples of
the original dataset, and the test set included the rest
of the examples. We performed the partition of the
dataset using reservoir sampling to guarantee the size
of the resulting subsets.

3 Baseline Approach

The algorithm that is currently used for the classi(cid:12)ca-
tion of the dockets is rule-based. These rules derive
from human heuristics and take advantage of domain-
speci(cid:12)c knowledge. To evaluate the performance of this
algorithm, as well as the classi(cid:12)cation methods we pro-
pose, we use standard performance measures such as
precision, recall and F-1 measure. We present the cor-
responding formulas for the classi(cid:12)cation of CCO doc-
uments in the following equations:
classi(cid:12)ed in CCO [ dockets in CCO
classi(cid:12)ed in CCO
classi(cid:12)ed in CCO [ dockets in CCO
dockets in CCO
precision (cid:1) recall
precision + recall

Precision =

Recall =

F = 2

The performance measures for the used classi(cid:12)ca-
tion method and the class CCO are shown in Eq. 3.1.
We observe that precision is high and recall is small;
this is so that the classi(cid:12)cation method can return a few
false negative dockets. An editorial team reviewed the
positive dockets returned by the algorithm and (cid:12)ltered
out the true CCO dockets. We use the performance
measures of this classi(cid:12)cation method as a baseline for
comparison with our proposed methods.

3

(4.2)

pk0 =

(4.3)

pk1 =

1
mk X
xi2Rk
1
mk X
xi2Rk

I fyi = 0g

I fyi = 1g

We start the training of our algorithm by classifying
all the training examples of region R1 to the ma jority
value of the response variable y . Since most of our
training examples do not belong to class "CCO", the
most naive classi(cid:12)er would predict that a given docket
does not belong to this class (since it tries to minimize
the classi(cid:12)cation error). We de(cid:12)ne the impurity Q(1) of
area R1 using the ’Gini’ index that we show in Eq. 4.4
for area Rk .

(4.4)

Q(k) = pk0 (1 (cid:0) pk0 ) + pk1 (1 (cid:0) pk1 )

The ’Gini’ index is an indicator of how "pure" an area is
in terms of the di(cid:11)erent values that the target variable
takes in that area. We get Q(k) = 0 if all training
examples of one area belong to the same class.
The next step of the algorithm depends on the value
of the complexity parameter cp and the gain Q(1) (cid:0)
(Q(2) + Q(3)) we obtain if we partition our training set
into two distinct subsets that span regions R2 and R3 of
our feature space, so that R2 \R3 = ; and R2 [R3 = R1 .
First we explain how we determine the optimal partition
of region R1 and then we give more details in the role
of cp. We partition the region R1 into regions R2 and
R3 in a way such that we can determine whether an
element xi 2 R1 belongs either to R2 or R3 with a
binary decision based on the value of only one speci(cid:12)c
dimension of its feature vector. The binary decision is
whether xi ’s value for the speci(cid:12)c dimension is le or >
than one splitting value. We select the dimension and
the splitting value so that the gain Q(1) (cid:0) (Q(2) + Q(3))
we have in the purity of the new regions R2 and R3 is
maximized. In our case, the splitting of a region based
on a value of a speci(cid:12)c dimension of the feature vector
yields partition of the dockets into two subsets based
on the frequency of one speci(cid:12)c word. For example, if
word "constru" (derived from construct, construction,
constructed, etc.) is the word that divides the dockets
into two as pure as possible subsets, we will divide the
dockets in these two subsets. In the best case, where
"constru" appears in all dockets of our class and no
document that does not belong to our class has the word
"contru" in it, we will divide the dockets into two pure
subsets and we get the maximum possible gain.
It is easy to see that without penalizing a region
splitting we would stop splitting only in cases where
we have pure areas, since it is easy to prove that we can

Precisionbase = 0:98

Recallbase = 0:32
Fbase = 0:49

(3.1)

4 Classi(cid:12)cation Trees

Tree-based methods for classi(cid:12)cation and regression
partition the feature space into a set of rectangles and
then (cid:12)t a simple model in each one. The most common
such model is the constant function. For our problem,
we used CART [1], a popular method usually used in
regression and classi(cid:12)cation problems.
The conceptual simplicity of the CART yields its
limitations. For example, the partition of the feature-
space into rectangles fails to capture non-rectangular
distributions of the data. Trees can approximate other
distributions only with a big number of small rectan-
gles that increase the complexity and, consequently, the
variance of the model. There are several proposed meth-
ods that handle CART’s limitations such as bagging [2]
and boosting [3], but they are out of the scope of this
paper.
Despite their limitations,
in our work we used
only naive tree-based methods, because of the great
interpretability of the classi(cid:12)cation algorithm decisions
that they o(cid:11)er. As we discuss below, such trees
reduce the classi(cid:12)cation problem into binary decisions
on values of feature vector’s dimensions. Our goal
using classi(cid:12)cation trees was to gain an insight of
the signi(cid:12)cant dimensions of the feature vectors that
actually have an impact on classi(cid:12)cation. Since CART
is conceptually simple, we could convey this insight to
people with little or no knowledge of machine learning.

4.1 Description In our case we deal with a binary
classi(cid:12)cation problem. The target variable has value 1
if a training example belongs to the class "Claim Con-
struction Order" and value 0 if the example does not
belong to this class. Our feature vector xi shows the
frequency of the words of our dictionary in docket i (cor-
responding to the weighting scheme txx as described in
Section 2.2).
Let R1 be the feature space of our training exam-
ples. In our case R1 = Rn and it contains all m1 = m
training examples. Let p10 be the ratio of examples in
R1 that do not belong to "Claim Construction Order"
class. Similarly, we de(cid:12)ne the ration p11 for examples
that do belong to the class. In general, for region Rk
we de(cid:12)ne:

4

always split an impure region into two, having a positive
gain in the purity of the resulting regions. However,
we limit the number of possible distinct regions by
adding to the impurity of area Rk the term cp. Hence,
in our case it would be worthwhile to split region R1
into regions R2 and R3 only if the gain Q(1) + cp (cid:0)
(Q(2) + cp + Q(3) + cp) = Q(1) (cid:0) (Q(2) + Q(3)) (cid:0) cp
was positive.
After splitting a region into two, the next step of
the algorithm requires to (cid:12)nd (cid:12)rst which region would
yield the maximum gain and then (cid:12)nd the optimal
split for this region.
In this optimization problem we
should take into consideration that a region splitting
that seems redundant may give the opportunity for
subregions’ splitting with great gain. To deal with
the recursive nature of the optimization problem we
construct the classi(cid:12)cation tree in a bottom-up fashion
rather than a top-down that we described so far. We
build initially a very large tree and then we prune it in
such a way so that we maximize the purity of the nodes
taking into consideration the complexity parameter that
is associated with each terminal node.
A slight variation of the algorithm above uses a
weighted version of the ’Gini’ index of Eq. 4.4. This
variation is used to penalize more the impurity of
regions with respect to one class than the other. We
show the updated formula in Eq. 4.5.

(4.5)

Q(k) = L0 pk0 (1 (cid:0) pk0 ) + L1 pk1 (1 (cid:0) pk1 )

The new formula can result in di(cid:11)erent decisions for
node splitting, since if L0 > L1 , for example, we obtain
a greater gain if we split areas where the ma jority of
the examples belong to class ’1’ and, hence, we have
misclassi(cid:12)ed examples of class ’0’. [[probably give more
detail]]
The algorithms (cid:12)nally returns a decision tree where
each branch illustrates a decision on speci(cid:12)c dimension
of the features vectors. As a result, the dimensions of
the feature vectors that do not appear in any branch
are completely ignored during the classi(cid:12)cation of a
new feature vector.
In our context, that means that
docket words that do not appear on the tree do not play
any role in the classi(cid:12)cation of new docket that does
not belong to our training set. Hence, the words that
actually appear in the tree branches can be considered
as the signi(cid:12)cant ones in our classi(cid:12)cation problem.

4.2 Tuning Classi(cid:12)cation Trees We (cid:12)tted di(cid:11)er-
ent classi(cid:12)cation trees to the data of our training set and
test their performance measures in the classi(cid:12)cation of
the test set. To construct di(cid:11)erent trees we varied the
parameters L0 and cp.
In particular, we considered the value of L1 = 1 as

0.9

0.85

0.8

0.75

0.7

 

precision
recall
F

0.65
 
0.5

1

1.5

2

2.5

3

3.5

4

L
0

Figure 3: Performance Measures of Classi(cid:12)cation Tree
for Varying L0

(cid:12)xed and we varied the parameter L0 in the interval
[0.2,4]. For each value of L0 we varied the complexity
parameter cp to all possible values that are greater than
0.0001. These values are (cid:12)nite if we consider only one
value in every interval that results in a di(cid:11)erent pruning
of the large tree we initially construct. Finally, for each
L0 we kept only the tree (an the corresponding value of
cp) that had the smallest misclassi(cid:12)cation error on the
test set.

4.3 Results We show the performance measures for
the optimal tree we obtained for each value of L0 in
Fig. 3. The x-axis of the plot show the values of L0 an
the three curves correspond to the precision, recall and
F performance measures. We see that the value of L0
that maximizes the F measure is 0.6. We see that as L0
increases we see an increase in the precision, because
our classi(cid:12)cation tree classi(cid:12)es only the pure nodes as
CCO and they tend represent approximately 85% of the
total number of CCO dockets. The change of the weight
L0 cannot change the classi(cid:12)cation decision for these
regions, sine there are no misclassi(cid:12)ed dockets that are
not CCO. On the other hand, recall decreases because
we misclassify more and more positive examples as
negatives in non-pure nodes, since this is not penalized
because of the increase of L0 with respect to L1 .
In Eq. 5.7 we provide the performance measures
for the optimal tree. We see that with the appropriate
tuning of this simple classi(cid:12)er we get an improvement
of +68.5% with respect to our base case.

Precisiontree = 0:79

Recalltree = 0:86
Ftree = 0:8

(4.6)

Improvement = +68:5%

Precision/Recall/F−1 values for SVM with different Cost Factors (linear kernel)

5

|

motion< 0.5

motion>=0.5

0

hear>=0.5

hear< 0.5

opinion< 0.5

opinion>=0.5

judg< 0.5

judg>=0.5

0

1

0

brief>=0.5

brief< 0.5

0

1

Figure 5: Pruned Optimal Tree

We show a pruned version of the optimal tree in Fig. 5.
The words that were actually used in the construction
of the optimal tree are: brief, constru, coordin, deni,
disput, (cid:12)le, hear, hrg,
judg, motion, manag, notic,
opinion, parti, patent, plainti(cid:11), schedul, set, sta(cid:11),
stipul, strike and summari. We see that out of the
approximately 5000 dimensions of our feature vectors,
classi(cid:12)cation trees achieve an 68.5% by utilizing only 22
of them.

0.5
1
1.5
2
2.5
3
3.5
Cost Factor by which training errors on positive examples outweight errors on negative  examples

4

Precision/Recall/F −1 values for SVM with different Cost Factors (linear kernel)

recall txx
precision txx
F−1 txx

0.5
3.5
3
2.5
2
1.5
1
Cost Factor by which training errors on positive examples outweight errors on negative  examples

4

Precision/Recall/F−1 values for SVM with different Cost Factors (linear kernel)

F−1 tpx
precision tpx
recall tpx

F−1 tix
precision tix
recall tix

0.5
3.5
3
2.5
2
1.5
1
Cost Factor by which training errors on positive examples outweight errors on negative  examples

4

5 Support Vector Machines (SVM)

s
e
u
l
a
v
 
1
−
F
/
l
l
a
c
e
r
/
n
o
i
s
i
c
e
r
p

s
e
u
l
a
v
 
1
F
/
l
l
a
c
e
r
/
n
o
i
s
i
c
e
r
p

s
e
u
l
a
v
 
1
F
/
l
l
a
c
e
r
/
n
o
i
s
i
c
e
r
p

s
e
u
l
a
v
 
1
F
/
l
l
a
c
e
r
/
n
o
i
s
i
c
e
r
p

s
e
u
l
a
v
 
1
F
/
l
l
a
c
e
r
/
n
o
i
s
i
c
e
r
p

100

90

80

70

60

50

40

30

20

10

0

0

100

90

80

70

60

50

40

30

20

10

0

0

100

90

80

70

60

50

40

30

20

10

0

0

100

90

80

70

60

50

40

30

20

10

0

0

100

90

80

70

60

50

40

30

20

10

0

0

Precision/Recall/F−1 values for SVM with different Cost Factors (linear kernel)

0.5
3.5
3
2.5
2
1.5
1
Cost Factor by which training errors on positive examples outweight errors on negative  examples

4

Precision/Recall/F−1 values for SVM with different Cost Factors (linear kernel)

F−1 lxx
precision lxx
recall lxx

0.5
3.5
3
2.5
2
1.5
1
Cost Factor by which training errors on positive examples outweight errors on negative  examples

4

F−1 lpx
precision lpx
recall lpx

5.1 Description Support vector machines (SVMs)
are a set of related supervised learning methods used
for classi(cid:12)cation and regression. They belong to a fam-
ily of generalized linear classi(cid:12)ers. A special property
of SVMs is that they simultaneously minimize the em-
pirical classi(cid:12)cation error and maximize the geometric
margin; hence they are also known as maximum margin
classi(cid:12)ers. Support vector machines map input vectors
to a higher dimensional space where a maximal sepa-
rating hyperplane is constructed. Two parallel hyper-
planes are constructed on each side of the hyperplane
that separates the data. The separating hyperplane is
the hyperplane that maximizes the distance between the
two parallel hyperplanes. An assumption is made that
the larger the margin or distance between these parallel
hyperplanes the better the generalisation error of the
classi(cid:12)er will be.

5.2 Tuning SVMs We performed a set of experi-
ments using support vector machines, and speci(cid:12)cally
Thorsten Joachims SVMlight package, as binary classi-
(cid:12)ers for dockets. We used a linear kernel function and
we varied the cost-factor; the factor by which training

Figure 4: Comparing Precision/Recall for SVM classi-
(cid:12)ers with di(cid:11)erent cost factor and for (cid:12)ve di(cid:11)erent in-
dexing schemes ((i) term frequency (txx) (ii) term fre-
quency/probabilistic inverse (tpx) (iii) term frequency /
inverse document frequency (tix) (iv) logarithmix (lxx)
(v) logarithmix/probabilistic inverse)

−
−
−
−
[3] J. Friedman. Greedy function approximation: a gradi-
ent boosting machine, 1999.
[4] Mark A. Lemley and J. H. Walker. Intellectual Prop-
erty Litigation Clearinghouse: Data Overview. SSRN
eLibrary, 2007.

6

errors on positive examples outweight errors on nega-
tive examples. We report results for cost factors in the
interval [0:1; 4] with a step of 0:1. We did not modify
any of the parameters of the SVMs we trained based
on the test set results, so we did not do a second level
development set/test set split.
Also, for each docket we used all combinations of
local and global term weighting schemes from Table 1
to derive the vector space representation of a docket.

5.3 Results Figure 4 illustrates the precision, recall
and F-1 measure for (cid:12)ve di(cid:11)erent combinations of term
weighting schemes and variations of the cost factor used
in SVMs. Overall we noticed that simple term frequency
can yield the maximum F-1 value.
Speci(cid:12)cally the
performance results in that case are:

Precisiontree = 0:8696

Recalltree = 0:88
Ftree = 0:87
Improvement = +79%

(5.7)

This could be justi(cid:12)ed from the relatively small size
of the content of each docket. Notice that overall SVMs
improve the rules-based baseline by a factor of 79%.

6 Conclusions

In this paper, we focused on building classi(cid:12)ers for
the dockets belonging in a speci(cid:12)c class. This was
done, since IPLC provided us training data for only this
class. We provided experimental evidence that we can
e(cid:14)ciently classify dockets of non-trivial classes using
only the docket text. Our optimal classi(cid:12)er yielded
an improvement in the current classi(cid:12)cation method
+79%. However, we think that we can build upon
our classi(cid:12)cation techniques and further improve the
classi(cid:12)cation accuracy by taking advantage of docket
feature that we were not provided with. These features
include the names of the attached documents to each
docket and the sequence of docket classes that are
formed in a patent case.
Future work includes the investigation of classi(cid:12)ca-
tion methods that take into consideration the aforemen-
tioned additional features.

References

[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone.
Classi(cid:12)cation and Regression Trees. Wadsworth and
Brooks, Monterey, CA, 1984. new edition [?]?
[2] Leo Breiman. Bagging predictors. Machine Learning,
24(2):123{140, 1996.


McRank: Learning to Rank Using Multiple
Classi ﬁcation and Gradient Boosting
Qiang Wu
Christopher J.C. Burges
Ping Li ∗
Microsoft Research
Microsoft Research
Dept. of Statistical Science
Microsoft Corporation
Microsoft Corporation
Cornell University
qiangwu@microsoft.com
cburges@microsoft.com
pingli@cornell.edu

Abstract
We cast the ranking problem as (1) multiple classiﬁcation ( “ Mc ”) (2) multiple or-
dinal classiﬁcation, which lead to computationally tracta ble learning algorithms
for relevance ranking in Web search. We consider the DCG criterion (discounted
cumulative gain), a standard quality measure in information retrieval. Our ap-
proach is motivated by the fact that perfect classiﬁcations
result in perfect DCG
scores and the DCG errors are bounded by classiﬁcation error s. We propose us-
ing the Expected Relevance to convert class probabilities into ranking scores. The
class probabilities are learned using a gradient boosting tree algorithm. Evalua-
tions on large-scale datasets show that our approach can improve LambdaRank [5]
and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An
efﬁcient implementation of the boosting tree algorithm is a lso presented.

1 Introduction
The general ranking problem has widespread applications including commercial search engines and
recommender systems. We develop McRank, a computationally tractable learning algorithm for the
general ranking problem; and we present our approach in the context of ranking in Web search.

For a given user input query, a commercial search engine returns many pages of URLs, in an order
determined by the underlying proprietary ranking algorithm. The quality of the returned results are
largely evaluated on the URLs displayed in the very ﬁrst page . The type of ranking problem in this
study is sometimes referred to as dynamic ranking (or simply, just ranking), because the URLs are
dynamically ranked (in real-time) according to the speciﬁc user input query. This is different from
the query-independent static ranking based on, for example, “page rank ” [3] or “authorities and
hubs” [12], which may, at least conceptually, serve as an imp ortant “feature ” for dynamic ranking
or to guide the generation of a list of URLs fed to the dynamic ranker.

There are two main categories of ranking algorithms. A popular scheme is based on learning
pairwise preferences, including RankNet [4], LambdaRank [5], RankSVM [11], RankBoost [7],
GBRank [14], and FRank [13]. Both LambdaRank and RankNet used neural nets.1 RankNet used
a cross-entropy type of loss function and LambdaRank used a gradient based on NDCG smoothed
by the RankNet loss. Another scheme is based on regression [6]. [6] considered the DCG measure
(discounted cumulative gain) [10] and showed that the DCG errors are bounded by regression errors.

In this study, we also consider the DCG measure. From the de ﬁn ition of DCG, it appears more direct
to cast the ranking problem as multiple classiﬁcation ( “Mc ”
) as opposed to regression. In order to
convert classiﬁcation results into ranking scores, we prop ose a simple and stable mechanism by
using the Expected Relevance. Our evaluations on large-scale datasets demonstrate the superiority
of the classiﬁcation-based ranker ( McRank) over both the regression-based and pair-based schemes.
2 Discounted Cumulative Gain (DCG)
For an input query, the ranker returns n ordered URLs. Suppose the URLs fed to the ranker are orig-
inally ordered {1, 2, 3, ..., n}. The ranker will output a permutation mapping π : {1, 2, 3, ..., n} →
{1, 2, 3, ..., n}. We denote the inverse mapping by σi = σ(i) = π−1 (i).
The DCG score is computed from the relevance levels of the n URLs as
n
n
Xi=1
Xi=1
∗Much of the work was conducted while Ping Li was an intern at Microsoft in 2006.
1 In fact LambdaRank supports any preference function, although the reported results in [5] are for pairwise.

c[i] (2yσi − 1) =

DCG =

c[πi ] (2yi − 1) ,

(1)

c[i] =

,

if i > L,

and c[i] = 0,

where [i] is the rank order, and yi ∈ {0, 1, 2, 3, 4} is the relevance level of the ith URL in the
original (pre-ranked) order. yi = 4 corresponds to a “perfect” relevance and
yi = 0 corresponds to
a “poor ” relevance. For generating training datasets, huma n judges have manually labeled a large
number of queries and URLs. In this study, we assume these labels are “gold-standard.”
In the de ﬁnition of DCG, c[i] , which is a non-increasing function of i, is typically set as
1
if i ≤ L,
log(1 + i)
where L is the “truncation level” and is typically set to be L = 10, to re ﬂect the fact that the search
quality of commercial search engines is mainly determined by the URLs displayed in the ﬁrst page.
Suppose a dataset contains NQ queries. It is a common practice to normalize the DCG score for
each query and report the normalized DCG ( “NDCG ”) score aver
aged over all queries. In other
words, the NDCG for the j th query (NDCGj ) and the ﬁnal NDCG of the dataset (NDCG F ) are
NQ
DCGj
1
Xj=1
DCGj,g
NQ
where DCGj,g is the maximum possible (or “gold standard ”) DCG score of the
3 Learning to Rank Using Classi ﬁcation
The de ﬁnition of DCG suggests that we can cast the ranking pro blem naturally as multiple classi-
ﬁcation (i.e., K = 5 classes), because obviously perfect classiﬁcations will l ead to perfect DCG
scores. While the DCG criterion is non-convex and non-smooth, classiﬁcation is very well-studied.

j th query.

NDCGF =

NDCGj =

NDCGj ,

(2)

,

(3)

We should mention that one does not really need perfect classiﬁcations in order to produce perfect
DCG scores. For example, suppose within a query, the URLs are all labeled level 1 or higher. If
, URLs labeled level 4 are classiﬁed as
an algorithm always classiﬁes the URLs one level lower (i.e.
level 3, and so on), we still have the perfect DCG score but the classiﬁcation “error ” is
100%. This
phenomenon to an extent, may provide some “safety cushion ” f or casting ranking as classiﬁcation.

[6] cast ranking as regression and showed that the DCG errors are bounded by regression errors. It
appears to us that the regression-based approach is less direct and possibly also less accurate than our
classiﬁcation-based proposal. For example, it is well-kno wn that, although one can use regression
for classiﬁcation, it is often better to use logistic regres sion especially for multiple classiﬁcation [8].
3.1 Bounding DCG Errors by Classiﬁcation Errors
Following [6, Theorem 2], we show that the DCG errors can be bounded by classiﬁcation errors.
For a permutation mapping π , the error is DCGg - DCGπ . One simple way to obtain the perfect
DCGg is to rank the URLs directly according to the gold-standard relevance levels. That is, all
URLs with relevance level k + 1 are ranked higher than those with relevance level ≤ k ; and the
URLs with the same relevance levels are arbitrarily ranked without affecting DCGg . We denote the
corresponding permutation mapping also by g .

Proof:

Lemma 1 Given n URLs, originally ordered as {1, 2, 3, ..., n}. Suppose a classiﬁer assigns a rele-
vance level ˆyi ∈ {0, 1, 2, 3, 4} to the ith URL, for all n URLs. A permutation mapping π ranks the
URLs according to ˆyi , i.e., π(i) < π(j ) if ˆyi > ˆyj , and, URL i and URL j are arbitrarily ranked if
ˆyi = ˆyj . The corresponding DCG error is bounded by the square root of the classiﬁcation error,
[i] !1/2   n
1yi 6= ˆyi !1/2
DCGg − DCGπ ≤15√2   n
n
c2/n
Xi=1
Yi=1
Xi=1
c2
[i] − n
n
n
n
c[πi ] (cid:16)2 ˆyi − 1(cid:17) +
c[πi ] (cid:16)2yi − 2 ˆyi (cid:17)
Xi=1
Xi=1
Xi=1
c[πi ] (2yi − 1) =
DCGπ =
n
n
c[gi ] (cid:16)2 ˆyi − 1(cid:17) +
c[πi ] (cid:16)2yi − 2 ˆyi (cid:17)
Xi=1
Xi=1
n
n
c[gi ] (cid:16)2yi − 2 ˆyi (cid:17) +
Xi=1
Xi=1
c[gi ] (2yi − 1) −
n
Xi=1 (cid:0)c[πi ] − c[gi ] (cid:1) (cid:16)2yi − 2 ˆyi (cid:17) .
=DCGg +

c[πi ] (cid:16)2yi − 2 ˆyi (cid:17)

n
Xi=1

(4)

≥

=

.

Note that Pn
i=1 c[πi ] (cid:0)2 ˆyi − 1(cid:1) ≥ Pn
i=1 c[gi ] (cid:0)2 ˆyi − 1(cid:1). Therefore,
n
Xi=1 (cid:0)c[gi ] − c[πi ] (cid:1) (cid:16)2yi − 2 ˆyi (cid:17)
DCGg − DCGπ ≤
Xi=1 (cid:16)2yi − 2 ˆyi (cid:17)2!1/2
Xi=1 (cid:0)c[gi ] − c[πi ] (cid:1)2!1/2   n
[i] !1/2
1yi 6= ˆyi !1/2
≤   n
n
15   n
n
≤  2
c2/n
Xi=1
Xi=1
Yi=1
c2
[i] − 2n
[gi ] = Qn
[πi ] = Qn
Note that Pn
[πi ] = Pn
[gi ] = Pn
[i] , Qn
i=1 c2
i=1 c2
i=1 c2
i=1 c2
i=1 c2
[i] , and 24 − 20 = 15.
i=1 c2
Thus, we can minimize the classiﬁcation error Pn
i=1 1yi 6= ˆyi as a surrogate for minimizing the DCG
error. Of course, since the classiﬁcation error itself is no n-convex and non-smooth, we actually
should use other (well-known) surrogate loss functions such as (7).
3.2 Input Data for Classiﬁcation
A training dataset contains NQ queries. The j th query corresponds to nj URLs; each URL is
manually labeled by one of the K = 5 relevance levels. Engineers have developed methodologies
to construct “features” by combining the query and URLs, but
the details are usually “trade secret.”

One important aspect in designing features, at least for the convenience of using traditional machine
learning algorithms, is that these features should be comparable across queries. For example, one
(artiﬁcial) feature could be the number of times the query ap pears in the Web page, which is com-
parable across queries. Both pair-based rankers and regression-based rankers implicitly made this
assumption, as they tried to learn a single rank function for all queries using the same set of features.

Thus, after we have generated feature vectors by combining the queries and URLs, we can create a
“training data matrix ” of size N × P , where N = PNQ
j=1 nj is the total number of “data points” (i.e.,
Query+URL) and P is the total number of features. This way, we can use the traditional machine
learning notation {yi , xi }N
i=1 to denote the training dataset. Here xi ∈ RP is the ith feature vector
in P dimensions; and yi ∈ {0, 1, 2, 3, 4 = K − 1} is the class (relevance) label of the ith data point.

3.3 From Classiﬁcation to Ranking
Although perfect classiﬁcations lead to perfect DCG scores , in reality, we will need a mechanism to
convert (imperfect) classiﬁcation results into ranking sc ores.

One possibility is already mentioned in Lemma 1. That is, we classify each data point into one of
the K = 5 classes and rank the data points according to the class labels (data points with the same
labels are arbitrarily ranked). This suggestion, however, will lead to highly unstable ranking results.

Our proposed solution is very simple. We ﬁrst learn the class probabilities by some soft classiﬁcation
algorithm and then score each data point (query+URL) according to the Expected Relevance.
Recall we assume a training dataset {yi , xi }N
i=1 , where the class label yi ∈ {0, 1, 2, 3, 4 = K − 1}.
We learn the class probabilities pi,k = Pr(yi = k), denoted by ˆpi,k , and de ﬁne a scoring function:

(5)

Si =

ˆpi,k T (k),

K−1
Xk=0
where T (k) is some monotone (increasing) function of the relevance level k . Once we have com-
puted the scores Si for all data points, we can then sort the data points within each query by the
descending order of Si . This approach is apparently sensible and highly stable. In fact, we exper-
imented with both T (k) = k and T (k) = 2k ; the performance difference in terms of the NDCG
scores was negligible, although T (k) = k appeared to be a slightly better choice (see Figure 3(c) in
Appendix II). In this paper, the reported experimental results were based on T (k) = k .
When T (k) = k , the scoring function Si is the Expected Relevance. Note that any monotone
transformation on Si (e.g., 2Si − 1) will not change the ranking results. Consequently, the ranking
results are not affected by any afﬁne transformation on T (k), aT (k) + b, (a > 0), because
pi,k (a × T (k) + b) = a ×  K−1
pi,k T (k)! + b,
Xk=0

K−1
Xk=0

K−1
Xk=0

since

pi,k = 1.

(6)

3.4 The Boosting Tree Algorithm for Learning Class Probabilities
For multiple classiﬁcation, we consider the following comm on (e.g., [8, 9]) surrogate loss function

N
K−1
Xi=1
Xk=0
Algorithm 1 implements a boosting tree algorithm for learning class probabilities pi,k ; and we use
basically the same implementation later for regression as well as multiple ordinal classiﬁcation.

− log(pi,k )1yi=k .

(7)

Algorithm 1 The boosting tree algorithm for multiple classiﬁcation, ta ken from [9, Algorithm 6],
although the presentation is slightly different.
0: ˜yi,k = 1, if yi = k , and ˜yi,k = 0 otherwise.
1: Fi,k = 0, k = 0 to K − 1, i = 1 to N
2: For m = 1 to M Do
For k = 0 to K − 1 Do
3:
pi,k = exp(Fi,k )/ PK−1
4:
s=0 exp(Fi,s )
{Rj,k,m}J
j=1 = J -terminal node regression tree for { ˜yi,k − pi,k , xi }N
5:
i=1
K P xi∈Rj,k,m
˜yi,k−pi,k
βj,k,m = K−1
P xi∈Rj,k,m
(1−pi,k )pi,k
Fi,k = Fi,k + ν PJ
j=1 βj,k,m1xi∈Rj,k,m

End

6:

7:
8:
9: End

There are three main parameters. M is the total number of boosting iterations, J is the tree size
(number of terminal nodes), and ν is the shrinkage coefﬁcient. As commented in [9] and veriﬁed
in
our experiments, the performance of the algorithm is not sensitive to these parameters.

In Algorithm 1, Line 5 contains most of the implementation work, i.e., building the regression trees
with J terminal nodes. Appendix I describes an efﬁcient implement ation for building the trees.

4 Multiple Ordinal Classi ﬁcation to Further Improve Rankin g

There is the possibility to (slightly) further improve our classiﬁcation-based ranking scheme by
taking into account the natural orders among the class labels, i.e., the multiple ordinal classiﬁcation.

A common approach for multiple ordinal classiﬁcation is to l earn the cumulative probabilities
Pr (yi ≤ k) instead of the class probabilities Pr (yi = k) = pi,k . We suggest a simple method
similar to the so-called cumulative logits approach known in statistics [1, Section 7.2.1].
We ﬁrst partition the training data points into two groups: {yi ≥ 4} and {yi ≤ 3}. Now we have
a binary classiﬁcation problem and hence we can use exactly t he same boosting tree algorithm for
multiple classiﬁcation. Thus we can learn Pr (yi ≤ 3) easily. We can similarly partition the data and
learn Pr (yi ≤ 2), Pr (yi ≤ 1), and Pr (yi ≤ 0), separately. We then infer the class probabilities

pi,k = Pr (yi = k) = Pr (yi ≤ k) − Pr (yi ≤ k − 1) ,
and again we use the Expected Relevance to compute the ranking scores and sort the URLs.

(8)

We call both rankers based on multiple classiﬁcation and mul tiple ordinal classiﬁcation as McRank.

5 Regression-based Ranking Using Boosting Tree Algorithm

With slight modiﬁcations, the boosting tree algorithm can b e used for regressions. Recall the input
data are {yi , xi }N
i=1 , where yi ∈ {0, 1, 2, 3, 4}. [6] suggested regressing the feature vectors xi on
the response values 2yi − 1.
Algorithm 2 implements the least-square boosting tree algorithm. The pseudo code is similar to [9,
Algorithm 3] by replacing the (l1 ) least absolute deviation (LAD) loss with the (l2 ) least square loss.
In fact, we also implemented the LAD boosting tree algorithm but we found the performance was
considerably worse than the least-square tree boost.

Algorithm 2 The boosting tree algorithm for regressions. After we have learned the values for Si ,
we use them directly as the ranking scores to order the data points within each query.
0: ˜yi = 2yi − 1
N PN
1: Si = 1
s=1 ˜ys , i = 1 to N
2: For m = 1 to M Do
{Rj,m}J
j=1 = J -terminal node regression tree for { ˜yi − Si , xi }N
5:
i=1
βj,m = meanxi∈Rj,m ˜yi − Si
6:
Si = Si + ν PJ
7:
j=1 βj,m1xi∈Rj,m
9: End
6 Experimental Results
We present the evaluations of 4 ranking algorithms (LambdaRank with two-layer nets, regression,
multiple classiﬁcation, and multiple ordinal classiﬁcati
on) on 4 datasets, including one artiﬁcial
dataset and three Web search datasets, denoted by Web-1, Web-2, and Web-3. The artiﬁcial dataset
and Web-1 are the same datasets used in [5]. Web-2 is the main dataset used in [13].

For the artiﬁcial data and Web-1, [5] reported that LambdaRank improved RankNet by about 1.0 (%)
NDCG. For Web-2, [13] reported that FRank slightly improved RankNet (by about 0.5 (%) NDCG)
and considerably improved RankSVM and RankBoost; but [13] did not compare with LambdaRank.
Our experiment showed that LambdaRank improved FRank by about 0.9 (%) NDCG on Web-2.

6.1 The Datasets
The artiﬁcial dataset [5] was meant to remove any variance ca used by the quality of features and/or
relevance labels. The data were generated from random cubic polynomials, with 50 features, 50
URLs per query, and 10,000/5,000/10,000 queries for train/validation/test.

The Web search dataset Web-1 [5] has 367 features and 10,000/5,000/10,000 queries for
train/validation/test, with in total 652,500 URLs.

Web-2 [13] has 619 features and 12,000/3,800/3,800 queries for train/validation/test, with in total
1,741,930 URLs. Note that this dataset is only partially labeled with 20 unlabeled URLs per query.
These unlabeled URLs were assigned the level 0 [13].

Web-3 has 450 features and 26,000 queries, with in total 474,590 URLs. We conducted ﬁve-fold
cross-validations and report the average NDCG scores.

6.2 The Parameters: M , J , ν
There are three main parameters in the boosting tree algorithm. M is the total number of iterations,
J is the number of terminal nodes in each tree, and ν is the shrinkage factor. Our experiments verify
that these parameters are not sensitive as long as they are within some “reasonable ” ranges [9]. Since
these experiments are time-consuming, we did not tune these parameters (M , J , ν ) exhaustively;
but the experiments appear to be convincing enough to establish the superiority of McRank.

[9] suggested setting ν ≤ 0.1, to avoid over- ﬁtting. We ﬁx
ν = 0.05 for the artiﬁcial dataset
and Web-1, and ﬁx ν = 0.02 for Web-2 and Web-3. The number of terminal nodes, J , should be
reasonably big (but not too big) when the dataset is large with a large number of features, because
the tree has to be deep enough to consider higher-order interactions [9]. We let J = 10 for the
artiﬁcial dataset and Web-1, J = 40 for Web-2, and J = 20 for Web-3.
With these values of J and ν , we did not observe obvious over- ﬁtting even for a very large number
of boosting iterations M . We will report the results with M = 1000 for the artiﬁcial data and Web-1,
M = 2000 for Web-2, and M = 1500 for Web-3.

6.3 The Test NDCG Results at Truncation Level L = 10
Table 1 lists the NDCG results (both the mean and standard deviation, in percentages (%)) for all 4
datasets and all 4 ranking algorithms, evaluated at the truncation level L = 10.
The NDCG scores indicate that that McRank (ordinal classiﬁcation and classiﬁcation) considerably
improves the regression-based ranker and LambdaRank. If we conduct a one-sided t-test, the im-

Table 1: The test NDCG scores produced by 4 rankers on 4 datasets. The average NDCG scores are presented
in percentages (%) with the standard deviations in the parentheses. Note that for the arti ﬁcial data and Web-1,
the LambdaRank results were taken directly from [5]. We also report the (one-sided) p-values to measure the
statistical signi ﬁcance of the improvement of McRank over regression and LambdaRank. For the arti ﬁcial data,
Web-1, and Web-3, we use the ordinal classi ﬁcation results t o compute the p-values. However, for Web-2,
because our implementation for testing ordinal classi ﬁcat
ion required too much memory for M = 2000, we
did not obtain the ﬁnal test NDCG scores; the partial results indicated that ordinal classi ﬁcation did not improve
classi ﬁcation for this dataset. Therefore, we compute the p-values using classi ﬁcation results for Web-2.

Datasets

Ordinal Classi ﬁcation Classi ﬁcation Regression

, p-value

LambdaRank, p-value

Arti ﬁcial [5]
Web-1 [5]
Web-2 [13]
Web-3

85.0 (9.5)
72.4 (24.1)

72.5 (26.5)

83.7 (9.9)
72.2 (24.1)
75.8 (23.8)
72.4 (27.3)

82.9 (10.2), 0
71.7 (24.4), 0.021
74.7 (24.4), 0.023
72.0 (27.6), 0.017

74.9, (12. 6), 0
71.2 (24.5), 0.0002
74.3 (24.3), 0.00 3
71.3 (28.8), 3.8 × 10−7

provements are signiﬁcant at about 98% level. However, multiple ordinal classiﬁcation did not sho w
signiﬁcant improvement over multiple classiﬁcation, exce
pt for the artiﬁcial dataset.

For the artiﬁcial data, all other 3 rankers exhibit very larg e improvements over LambaRank. This is
probably due to the fact that the artiﬁcial data are generate d noise-free and hence the ﬂexible (with
high capacity) rankers using boosting tree algorithms tend to ﬁt the data very well.

6.4 The NDCG Results at Various Truncation Levels (L = 1 to 10)
For the artiﬁcial dataset and Web-1, [5] also reported the ND CG scores at various truncation levels,
L = 1 to 10. To make the comparisons more convincing, we also report similar results for the arti-
ﬁcial dataset and Web-1, in Figure 1. For a better clarity, we plot the standard deviations separately
from the averages. Figure 1 veriﬁes that the improvements sh own in Table 1 are not only true for
L = 10 but also (essentially) true for smaller truncation levels.

73
72
71
70
69
68
67
66
65
64
63
 
1

Web−1

Ordinal
Classification
Regression
LambdaRank
7
4
9
8
6
5
Truncation level

2

3

Classification
Regression
LambdaRank

 

76

75

74

73

72

71

70

69

)
%
(
 
G
C
D
N

 

 

Web−3

72

70

68

66

64

62

60

)
%
(
 
G
C
D
N

Web−2

Ordinal
Classification
Regression
LambdaRank

68
 
1

2

3

4
7
6
5
Truncation level

8

9

10

58
 
1

2

3

4
7
6
5
Truncation level

8

9

10

Artificial

 

85

80

75

70

)
%
(
 
G
C
D
N

Ordinal
Classification
Regression
LambdaRank
8
10
9

65
 
1

2

3

4
7
6
5
Truncation level

 

Ordinal
Classification
Regression
LambdaRank

35

30

25

20

15

10

)
%
(
 
d
t
s
 
G
C
D
N

)
%
(
 
G
C
D
N

)
%
(
 
d
t
s
 
G
C
D
N

42
40
38
36
34
32
30
28
26
24
 
1

10

 

Ordinal
Classification
Regression
LambdaRank

)
%
(
 
d
t
s
 
G
C
D
N

46
44
42
40
38
36
34
32
30
28
26
24
22
 
1

 

Classification
Regression
LambdaRank

)
%
(
 
d
t
s
 
G
C
D
N

46
44
42
40
38
36
34
32
30
28
26
 
1

 

Ordinal
Classification
Regression
LambdaRank

5
 
1

8

9

3

2

4
7
6
5
4
7
6
5
4
7
6
5
4
7
6
5
Truncation level
Truncation level
Truncation level
Truncation level
Figure 1: The NDCG scores at truncation levels L = 1 to 10, for four datasets. Upper Panels: the
average NDCG scores. Bottom Panels: the corresponding standard deviations.

10

10

9

10

8

9

2

3

2

3

8

9

10

8

2

3

7 Conclusion
The ranking problem has become an important topic in machine learning, partly due to its
widespread applications in many decision-making processes especially in commercial search en-
gines.
In one aspect, the ranking problem is difﬁcult becaus e the measures of rank quality are
usually based on sorting, which is not directly optimizable (at least not efﬁciently). On the other
hand, one can cast ranking into various classical learning tasks such as regression and classiﬁcation.

The proposed classiﬁcation-based ranking scheme is motiva ted by the fact that perfect classiﬁcations
lead to perfect DCG scores and the DCG errors are bounded by the classiﬁcation errors. It appears

—
natural that the classiﬁcation-based ranker is more direct and should work better than the regression-
based ranker suggested in [6]. To convert classiﬁcation res ults into ranking, we propose a simple and
stable mechanism by using the Expected Relevance, computed from the learned class probabilities.

To learn the class probabilities, we implement a boosting tree algorithm for multiple classiﬁca-
tion and we use the same implementation for multiple ordinal classiﬁcation and regression. Since
commercial proprietary datasets are usually very large, an adaptive quantization-based approach ef-
ﬁciently implements the boosting tree algorithm, which avo ids sorting and has lower memory cost.

Our experimental results have demonstrated that McRank (including multiple classiﬁcation and mul-
tiple ordinal classiﬁcation) outperforms both the regress ion-based ranker and the pair-based Lamb-
daRank. However, except for the artiﬁcial dataset, we did not obser ve signiﬁcant improvement of
ordinal classiﬁcation over classiﬁcation.

In a summary, we regard McRank algorithm (retrospectively) simple, robust, and capable of produc-
ing quality ranking results.

An Ef ﬁcient Implementation for Building Boosting Trees

Appendix I
We use the standard regression tree algorithm [2], which recursively splits the training data points
into two groups on the current “best” feature that will reduc
e the mean square errors (MSE) the most.
Efﬁcient (in both time and memory) implementation needs som e care. The standard practice [9] is to
pre-sort all the features; then after every split, carefully keep track of the indexes of the data points
and the sorted orders in all other features for the next split.

We suggest a simpler and more efﬁcient approach, by taking ad vantage of some properties of the
boosting tree algorithm. While the boosting tree algorithm is well-known to be robust and also
accurate, an individual tree has limited predictive power and usually can be built quite crudely.

When splitting on one feature, Figure 2(a) says that sometimes the split point can be chosen within a
certain range without affecting the accuracy (i.e., the reduced MSE due to the split). In Figure 2(b),
we bin (quantize) the data points into two (0/1) levels on the horizontal (i.e., feature) axis. Suppose
we choose the quantization as shown in the Figure 2(b), then the accuracy will not be affected either.

y

y

y

Bin length

s
L

s

sR

x

Bin 0

s

Bin 1

x

x

4 5 6 7 8 9
(c)
(b)
(a)
Figure 2: To split on one feature (x), we seek a split point s on x such that after the splitting, the
mean square error (MSE, in the y axis) of the data points at the left plus the MSE at the right is
reduced the most. Panel (a) suggests that in some cases we can choose s in a range (within sL and
sR ) without affecting the reduced MSE. Panel (b) suggests that, if we bin the data on the x axis to
be binary, the reduced MSE will not be affected either, if the data are binned in the way as in (b).
Panel (c) pictures an adaptive binning scheme to make the accuracy loss (if any) as little as possible.

10 11 12

0 1

2 3

Of course, we would not know ahead of time how to bin the data to avoid losing accuracy. Therefore,
we suggest an adaptive quantization scheme, pictured in Figure 2(c), to make the accuracy loss (if
any) as little as possible. In the pre-processing stage, for each feature, the training data points are
sorted according to the feature value; and we bin the feature values in the sorted order. We start with
a very small initial bin length, e.g., 10−8 . As shown in Figure 2(c), we only bin the data where there
are indeed data, because the boosting tree algorithm will not consider the area where there are no
data anyway. We set an allowed maximum number of bins, denoted by B . If the bin length is so
small that we need more than B bins, we simply increment the bin length and re-do the quantization.
After the quantization, we replace the original feature value by the bin labels (0, 1, 2, ...). Note that
since we start with a small bin length, the ordinal categorical features are naturally taken care of.

This simple binning scheme is very effective particularly for the boosting tree algorithm:

• It simpliﬁes the implementation. After the quantization, t here is no need for sorting (and
keeping track of the indexes after splitting) because we conduct “bucket sort” implicitly.
• It speeds up the computations for the tree-building step, the bottleneck of the algorithm.
• It reduces the memory cost for training. For example, if we set the maximum allowed
number of bins to be B = 28 , we only need one byte per data entry.
• It does not really result in loss of accuracy. We experimented with both B = 28 = 256 and
B = 216 = 65536; and we did not observe real differences in the NDCG scores, although
reported experimental results were all based on B = 216 . See Appendix II, Figure 3(a)(b).
Appendix II
Figure 3 (a)(b) present the experiment with our adaptive quantization scheme on Web-1 dataset. We
binned the data with the maximum bin number B = 23 , 24 , 25 , 26 , 27 , 28 , and 216 . In (a) and (b),
the horizontal axis is the “exponent” of B . Panel (a) plots the relative number of total bins in Web-1
as a function of the exponent, normalized by the total number of bins at B = 216 . Panel (b) plots
the “NDCG loss” due to the quantization, relative to the NDCG scores at B = 216 . When B = 28 ,
the total number of bins is only about 6% of that when B = 216 ; however, both quantization levels
achieved the same test NDCG scores. Besides the bene ﬁt of com putational efﬁciency, quantization
can also be considered as a way of “regularization ” to slow do wn the training, as re ﬂected in (b).

Some More Experiments on Web-1

100

10−1

10−2

s
n
i
b
 
l
a
t
o
t
 
f
o
 
e
g
a
t
n
e
c
r
e
P

1

0

)
%
(
 
s
s
o
L
 
G
C
D
N

−1

−2

−3

 

)
%
(
 
G
C
D
N

Train
Validation
Test

Relevance
Gain

 

Train

Validation

Test

77
76
75
74
73
72
71
70
69
 
1

10−3

−4

16

 
400
600
2 3 4 5 6 7 8
2 3 4 5 6 7 8
Iteration
Max bin number (Exponent)
Max bin number (Exponent)
(c)
(b)
(a)
Figure 3: Web-1. (a)(b): Experiment with our adaptive quantization scheme. (c): Experiment with
two different scoring functions.

200

16

800

1000

Figure 3 (c) compares two scoring functions to convert learned class probabilities into rank-
ing scores, including the Expected Relevance Si = PK−1
k=0 ˆpi,k k and the Expected Gain Si =
PK−1
k=0 ˆpi,k (cid:0)2k − 1(cid:1). Panel (c) suggests that using the Expected Relevance is consistently better
than using the Expected Gain but the differences are small, especially for the test NDCG scores.
References
[1] A. Agresti. Categorical Data Analysis. John Wiley & Sons, Inc., Hoboken, NJ, second edition, 2002.
[2] L. Brieman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees . 1983.
[3] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In WWW, pages 107–117, 1998.
[4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In
ICML, pages 89–96, 2005.
[5] C. Burges, R. Ragno, and Q. Le. Learning to rank with nonsmooth cost functions. In NIPS, pages 193–200, 2007.
[6] D. Cossock and T. Zhang. Subset ranking using regression. In COLT, pages 605–619, 2006.
[7] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efﬁcien t boosting algorithm for combining preferences. Journal of Machine Learning
Research, 4:933–969, 2003.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. The Annals of Statistics, 28(2):337–
407, 2000.
[9] J. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):1189–1232, 2001.
[10] K. J ¨arvelin and J. Kek ¨al¨ainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR, pages 41–48, 2000.
[11] T. Joachims. Optimizing search engines using clickthrough data. In KDD, pages 133–142, 2002.
[12] J. Kleinberg. Authoritative sources in a hyperlinked environment. In SODA, pages 668–677, 1998.
[13] M. Tsai, T. Liu, T. Qin, H. Chen, and W. Ma. Frank: a ranking method with ﬁdelity loss. In SIGIR, pages 383–390, 2007.
[14] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression framework for learning ranking functions using relative relevance judgments. In
SIGIR, pages 287-294, 2007.


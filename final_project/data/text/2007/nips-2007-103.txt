The Epoch-Greedy Algorithm for Contextual
Multi-armed Bandits

John Langford
Yahoo! Research
jl@yahoo-inc.com

Tong Zhang
Department of Statistics
Rutgers University
tongz@rci.rutgers.edu

Abstract

We present Epoch-Greedy, an algorithm for contextual multi-armed bandits (also
known as bandits with side information). Epoch-Greedy has the following prop-
erties:
1. No knowledge of a time horizon T is necessary.
2. The regret incurred by Epoch-Greedy is controlled by a sample complexity
bound for a hypothesis class.
3. The regret scales as O(T 2/3S 1/3 ) or better (sometimes, much better). Here S
is the complexity term in a sample complexity bound for standard supervised
learning.

1

Introduction

The standard k-armed bandits problem has been well-studied in the literature (Lai & Robbins, 1985;
Auer et al., 2002; Even-dar et al., 2006, for example).
It can be regarded as a repeated game
between two players, with every stage consisting of the following: The world chooses k rewards
r1 , ..., rk ∈ [0, 1]; the player chooses an arm i ∈ {1, k} without knowledge of the world’s chosen
rewards, and then observes the reward ri . The contextual bandits setting considered in this paper
is the same except for a modiﬁcation of the ﬁrst step, in which the player also observes context
information x which can be used to determine which arm to pull.
The contextual bandits problem has many applications and is often more suitable than the standard
bandits problem, because settings with no context information are rare in practice. The setting
considered in this paper is directly motivated by the problem of matching ads to web-page contents
on the internet. In this problem, a number of ads (arms) are available to be placed on a number of
web-pages (context information). Each page visit can be regarded as a random draw of the context
information (one may also include the visitor’s online proﬁle as context information if available)
from an underlying distribution that is not controlled by the player. A certain amount of revenue is
generated when the visitor clicks on an ad. The goal is to put the most relevant ad on each page to
maximize the expected revenue. Although one may potentially put multiple ads on each web-page,
we focus on the problem that only one ad is placed on each page (which is like pulling an arm given
context information). The more precise deﬁnition is given in Section 2.
Prior Work. The problem of bandits with context has been analyzed previously (Pandey et al., 2007;
Wang et al., 2005), typically under additional assumptions such as a correct prior or knowledge of
the relationship between the arms. This problem is also known as associative reinforcement learning
(Strehl et al., 2006, for example) or bandits with side information. A few results under as weak or
weaker assumptions are directly comparable.

1. The Exp4 algorithm (Auer et al., 1995) notably makes no assumptions about the world.
Epoch-Greedy has a worse regret bound in T (O(T 2/3 ) rather than O(T 1/2 )) and is only

1

analyzed under an IID assumption. An important advantage of Epoch-Greedy is a much
better dependence on the size of the set of predictors. In the situation where the number
of predictors is inﬁnite but with ﬁnite VC-Dimension d, Exp4 has a vacuous regret bound
while Epoch-Greedy has a regret bound no worse than O(T 2/3 (ln m)1/3 ). Sometimes we
can achieve much better dependence on T , depending on the structure of the hypothe-
sis space. For example, we will show that it is possible to achieve O(ln T ) regret bound
using Epoch-Greedy, while this is not possible with Exp4 or any simple modiﬁcation of
it. Another substantial advantage is reduced computational complexity. The ERM step in
Epoch-Greedy can be replaced with any standard learning algorithm that achieves approxi-
mate loss minimization, making guarantees that degrade gracefully with the approximation
factor. Exp4 on the other hand requires computation proportional to the explicit count of
hypotheses in a hypothesis space.
2. The random trajectories method (Kearns et al., 2000) for learning policies in reinforcement
learning with hard horizon T = 1 is essentially the same setting. In this paper, bounds are
stated for a batch oriented setting where examples are formed and then used for choosing
a hypothesis. Epoch-Greedy takes advantage of this idea, but it also has analysis which
states that it trades off the number of exploration and exploitation steps so as to maximize
the sum of rewards incurred during both exploration and exploitation.

What we do. We present and analyze the Epoch-Greedy algorithm for multiarmed bandits with
context. This has all the nice properties stated in the abstract, resulting in a practical algorithm for
solving this problem.
The paper is broken up into the following sections.

1. In Section 2 we present basic deﬁnitions and background.
2. Section 3 presents the Epoch-Greedy algorithm along with a regret bound analysis which
holds without knowledge of T .
3. Section 4 analyzes the instantiation of the Epoch-Greedy algorithm in several settings.

2 Contextual bandits

We ﬁrst formally deﬁne contextual bandit problems and algorithms to solve them.

Deﬁnition 2.1 (Contextual bandit problem) In a contextual bandits problem, there is a distribu-
tion P over (x, r1 , ..., rk ), where x is context, a ∈ {1, . . . , k} is one of the k arms to be pulled,
and ra ∈ [0, 1] is the reward for arm a. The problem is a repeated game: on each round, a sample
(x, r1 , ..., rk ) is drawn from P , the context x is announced, and then for precisely one arm a chosen
by the player, its reward ra is revealed.
Deﬁnition 2.2 (Contextual bandit algorithm) A contextual bandits algorithm B determines an
arm a ∈ {1, . . . , k} to pull at each time step t, based on the previous observation sequence
Our goal is to maximize the expected total reward PT
(x1 , a1 , ra,1 ), . . . , (xt−1 , at−1 , ra,t−1 ), and the current context xt .
t=1 E(xt ,~rt )∼P [ra,t ]. Note that we use the
notation ra,t = rat to improve readability. Similar to supervised learning, we assume that we are
given a set H consisting of hypotheses h : X → {1, . . . , k}. Each hypothesis maps side information
x to an arm a. A natural goal is to choose arms to compete with the best hypothesis in H. We
introduce the following deﬁnition.
(cid:2)rh(x)
(cid:3) .
Deﬁnition 2.3 (Regret) The expected reward of a hypothesis h is
R(h) = E(x,~r)∼D
Consider any contextual bandits algorithm B . Let Z T = {(x1 , ~r1 ), . . . , (xT , ~rT )}, and the expected
regret of B with respect to a hypothesis h be:
TX
∆R(B , h, T ) = T R(h) − EZ T ∼P T
t=1

rB(x),t .

2

The expected regret of B up to time T with respect to hypothesis space H is deﬁned as
∆R(B , H, T ) = sup
∆R(B , h, T ).
h∈H

The main challenge of the contextual bandits problem is that when we pull an arm, rewards of
other arms are not observed. Therefore it is necessary to try all arms (explore) in order to form an
accurate estimation. In this context, methods we investigate in the paper make explicit distinctions
between exploration and exploitation steps. In an exploration step, the goal is to form unbiased
samples by randomly pulling all arms to improve the accuracy of learning. Because it does not
focus on the best arm, this step leads to large immediate regret but can potentially reduce regret
for the future exploitation steps. In an exploitation step, the learning algorithm suggests the best
hypothesis learned from the samples formed in the exploration steps, and the arm given by the
hypothesis is pulled: the goal is to maximize immediate reward (or minimize immediate regret).
Since the samples in the exploitation steps are biased (toward the arm suggested by the learning
algorithm using previous exploration samples), we do not use them to learn the hypothesis for the
future steps. That is, in methods we consider, exploitation does not help us to improve learning
accuracy for the future.
More speciﬁcally, in an exploration step, in order to form unbiased samples, we pull an arm a ∈
{1, . . . , k} uniformly at random. Therefore the expected regret comparing to the best hypothesis
in H can be as large as O(1). In an exploitation step, the expected regret can be much smaller.
Therefore a central theme we examine in this paper is to balance the trade-off between exploration
and exploitation, so as to achieve a small overall expected regret up to some time horizon T .
Note that if we decide to pull a speciﬁc arm a with side information x, we do not observe rewards
ra0 for a0 6= a. In order to apply standard sample complexity analysis, we ﬁrst show that exploration
samples, where a is picked uniformly at random, can create a standard learning problem without
missing observations. This is simply achieved by setting fully observed rewards r 0 such that
r 0
a0 (ra ) = kI (a0 = a)ra ,
(1)
where I (·) is the indicator function. The basic idea behind this transformation from partially ob-
served to fully observed data dates back to the analysis of “Sample Selection Bias” (Heckman,
1979). The above rule is easily generalized to other distribution over actions p(a) by replacing k
with 1/p(a).
The following lemma shows that this method of ﬁlling missing reward components is unbiased.
h
i
Lemma 2.1 For all arms a0 : E~r∼P |x [ra0 ] = E~r∼P |x,a∼U (1,...,k) [r 0
a0 (ra )]. Therefore for any hy-
r 0
pothesis h(x), we have R(h) = E(x,~r)∼P,a∼U (1,...,k)
h(x) (ra )
.
kX
kX
a=1
a=1

Proof We have:
E~r∼P |x,a∼U (1,...,k) [r 0
a0 (ra )] =E~r∼P |x

k−1 [r 0
a0 (ra )]

=E~r∼P |x

k−1 [kra I (a0 = a)] = E~r∼P |x [ra0 ] .

samples as P
Lemma 2.1 implies that we can estimate reward R(h) of any hypothesis h(x) using expectation
with respect to exploration samples (x, a, ra ). The right hand side can then be replaced by empirical
t I (h(xt ) = at )ra,t for hypotheses in a hypothesis space H. The quality of this
estimation can be obtained with uniform convergence learning bounds.

3 Exploration with the Epoch-Greedy algorithm

The problem of treating contextual bandits as standard bandits is that the information in x is lost.
That is, the optimal arm to pull should be a function of the context x, but this is not captured by the

3

standard bandits setting. An alternative approach is to regard each hypothesis h as a separate artiﬁ-
cial “arm”, and then apply a standard bandits algorithm to these artiﬁcial arms. Using this approach,
let m be the number of hypotheses, we can get a bound of O(m). However, this solution ignores
the fact that many hypotheses can share the same arm so that choosing an arm yields information
for many hypotheses. For this reason, with a simple algorithm, we can get a bound that depends on
m logarithmically, instead of O(m) as would be the case for the standard bandits solution discussed
above.
As discussed earlier, the key issue in the algorithm is to determine when to explore and when to
exploit, so as to achieve appropriate balance. If we are given the time horizon T in advance, and
would like to optimize performance with the given T , then it is always advantageous to perform a
ﬁrst phase of exploration steps, followed by a second phase of exploitation steps (until time step T ).
The reason that there is no advantage to take any exploitation step before the last exploration step is:
by switching the two steps, we can more accurately pick the optimal hypothesis in the exploitation
step due to more samples from exploration. With ﬁxed T , assume that we have taken n steps of
exploration, and obtain an average regret bound of n for each exploitation step at the point, then
we can bound the regret of the exploration phase as n, and the exploitation phase as n (T − n). The
total regret is n + (T − n)n . Using this bound, we shall switch from exploration to exploitation at
the point n that minimizes the sum.
Without knowing T in advance, but with the same generalization bound, we can run explo-
total average regret is no more than PL
ration/exploitation in epochs, where at the beginning of each epoch ‘, we perform one step of explo-
ration, followed by d1/n e steps of exploitation. We then start the next epoch. After epoch L, the
n=1 (1 + n d1/n e) ≤ 3L. Moreover, the epoch L∗ contain-
ing T is no more than the optimal regret bound minn [n + (T − n)n ] (with known T and optimal
stopping point). Therefore the performance of our method (which does not need to know T ) is no
worse than three time the optimal bound with known T and optimal stopping point. This motivates
a modiﬁed algorithm in Figure 1. The idea described above is related to forcing in (Lai & Yakowitz,
1995).
Let L∗ = min{L : PL
Proposition 3.1 Consider a sequence of nonnegative and monotone non-increasing numbers {n }.
‘=1 (1 + d1/‘ e) ≥ T }, then
[n + (T − n)n ].
L∗ ≤ min
n∈[0,T ]
need consider the case n∗ ≤ L∗ − 1. By assumption, PL∗−1
Proof Let n∗ = arg minn∈[0,T ] [n + (T − n)n ]. The bound is trivial if n∗ ≥ L∗ . We only
PL∗−1
‘=1 1/‘ ≥ PL∗−1
‘=1 (1 + 1/‘ ) ≤ T − 1. Since
‘=n∗ 1/‘ ≥ (L∗ − n∗ )1/n∗ , we have L∗ − 1 + (L∗ − n∗ )1/n∗ ≤ T − 1.
Rearranging, we have L∗ ≤ n∗ + (T − L∗ )n∗ .

1 ) is a sample-dependent (integer valued) exploitation step count. Proposition 3.1
In Figure 1, s(Z n
1 )e, where n (Z n
1 ) = d1/n (Z n
1 ) is a sample dependent average gen-
suggests that choosing s(Z n
eralization bound, yields performance comparable to the optimal bound with known time horizon
T .
Deﬁnition 3.1 (Epoch-Greedy Exploitation Cost) Consider a hypothesis space H consisting of
hypotheses that take values in {1, 2, . . . , k}. Let Zt = (xt , at , ra,t ) for i = 1, . . . , n be indepen-
dent random samples, where ai is uniform randomly distributed in {1, . . . , k}, and ra,t ∈ [0, 1] is
1 = {Z1 , . . . , Zn }, and the empirical reward maximization
nX
the observed (random) reward. Let Z n
estimator
ˆh(Z n
1 ) = arg max
ra,t I (h(xt ) = at ).
h∈H
t=1
Given any ﬁxed n, δ ∈ [0, 1], and observation Z n
(cid:19)
(cid:18)
1 , we denote by s(Z n
1 ) a data-dependent exploitation
step count. Then the per-epoch exploitation cost is deﬁned as:
R(h) − R(ˆh(Z n
µn (H, s) = EZ n
1 ))
1

s(Z n
1 ).

sup
h∈H

4

Epoch-Greedy (s(W‘ )) /*parameter s(W‘ ): exploitation steps*/
initialize: exploration samples W0 = {} and t1 = 1
iterate ‘ = 1, 2, . . .
t = t‘ , and observe xt /*do one-step exploration*/
select an arm at ∈ {1, . . . , k} uniformly at random
receive reward ra,t ∈ [0, 1]
W‘ = W‘−1 ∪ {(xt , at , ra,t )}
maxh∈H P
ﬁnd best hypothesis ˆh‘ ∈ H by solving
ra I (h(x) = a)
(x,a,ra )∈W‘
t‘+1 = t‘ + s(W‘ ) + 1
for t = t‘ + 1, · · · , t‘+1 − 1 /*do s(W‘ )-steps exploitation*/
select arm at = ˆh‘ (xt )
receive reward ra,t ∈ [0, 1]
end for
end iterate

Figure 1: Exploration by -greedy in epochs
Theorem 3.1 For all T , n‘ , L such that: T ≤ L + PL
‘=1 n‘ , the expected regret of Epoch-Greedy
LX
LX
in Figure 1 is bounded by
‘=1
‘=1

∆R(Epoch-Greedy, H, T ) ≤ L +

µ‘ (H, s) + T

P [s(Z ‘
1 ) < n‘ ].

This theorem statement is very general, because we want to allow sample dependent bounds to be
used. When sample-independent bounds are used the following simple corollary holds:
arg minL{L : L + PL
1 ) = s‘ ≤ b1/µ‘ (H, 1)c (‘ = 1, . . .), and let LT =
Corollary 3.1 Assume we choose s(Z ‘
‘=1 s‘ ≥ T }. Then the expected regret of Epoch-Greedy in Figure 1 is
bounded by
∆R(Epoch-Greedy, H, T ) ≤ 2LT .
(of the main theorem) Let B be the Epoch-Greedy algorithm. One of the following events
Proof
will occur:
• A: s(Z ‘
1 ) < n‘ for some ‘ = 1, . . . , L.
1 ) ≥ n‘ for all ‘ = 1, . . . , L.
• B: s(Z ‘
If event A occurs, then since each reward is in [0,1], up to time T , regret cannot be larger than T .
Thus the total expected contribution of A to the regret ∆R(B , H, T ) is at most
LX
‘=1
If event B occurs, then t‘+1 − t‘ ≥ n‘ + 1 for ‘ = 1, . . . , L, and thus tL+1 > T . Therefore the
expected contribution of B to the regret ∆R(B , H, T ) is at most the sum of expected regret in the
ﬁrst L epochs.
By deﬁnition and construction, after the ﬁrst step of epoch ‘, W‘ consists of ‘ random observations
Zj = (xj , aj , ra,j ) where aj is drawn uniformly at random from {1, . . . , k}, and j = 1, . . . , ‘.
This is independent of the number of exploitation steps before epoch ‘. Therefore we can treat
W‘ as ‘ independent samples. This means that the expected regret associated with exploitation
steps in epoch ‘ is µ‘ (H, s). Since the exploration step in each epoch contributes at most 1 to the

P [s(Z ‘
1 ) < n‘ ].

T P (A) ≤ T

(2)

5

total expected regret for epochs ‘ = 1, . . . , L is at most L + PL
expected regret, the total expected regret for each epoch ‘ is at most 1 + µ‘ (H, s). Therefore the
‘=1 µ‘ (H, s). Combined with (2),
we obtain the desired bound.

In the theorem, we bound the expected regret of each exploration step by one. Clearly this assumes
the worst case scenario and can often be improved. Some consequences of the theorem with speciﬁc
function classes are given in Section 4.

4 Examples

Theorem 3.1 is quite general. In this section, we present a few simple examples to illustrate the
potential applications.

Exi

4.1 Finite hypothesis space worst case bound
Consider the ﬁnite hypothesis space situation, with m = |H| < ∞. We apply Theorem 3.1 with a
worst-case deviation bound.
Let x1 , . . . , xn ∈ [0, k ] be iid random variables, such that Exi ≤ 1, then Bernstein inequality
vuutln(1/η)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ c0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) nX
implies that there exists a constant c0 > 0 such that ∀η ∈ (0, 1), with probability 1 − η :
xi − nX
nX
pnk ln(1/η) + c0k ln(1/η).
i + c0k ln(1/η) ≤ c0
Ex2
i=1
i=1
i=1
µn (H, 1) ≤ c−1pk ln m/n.
It follows that there exists a universal constant c > 0 such that
1 ) = bcp‘/(k ln m)c,
Therefore in Figure 1, if we choose
s(Z ‘
then µ‘ (H, s) ≤ 1: this is consistent with the choice recommended in Proposition 3.1.
n‘ = bcp‘/(k ln m)c.
In order to obtain a performance bound of this scheme using Theorem 3.1, we can simply take
satisﬁes the condition T ≤ PL
1 ) < n‘ ) = 0. Moreover, with this choice, for any T , we can pick an L that
This implies that P (s(Z ‘
‘=1 n‘ . It implies that there exists a universal constant c0 > 0 such
that for any given T , we can take
L = bc0T 2/3 (k ln m)1/3 c
1 ) = bcp‘/(k ln m)c in Figure 1, then
in Theorem 3.1.
In summary, if we choose s(Z ‘
∆(Epoch-Greedy, H, T ) ≤ 2L ≤ 2c0T 2/3 (k ln m)1/3 .

Reducing the problem to standard bandits, as discussed at the beginning of Section 3, leads to
a bound of O(m ln T ) (Lai & Robbins, 1985; Auer et al., 2002). Therefore when m is large,
the Epoch-Greedy algorithm in Figure 1 can perform signiﬁcantly better. In this particular situa-
√
tion, Epoch-Greedy does not do as well as Exp4 in (Auer et al., 1995), which implies a regret of
O(
kT ln m). However, the advantage of Epoch-Greedy is that any learning bound can be applied.
For many hypothesis classes, the ln m factor can be improved for Epoch-Greedy. In fact, a similar
result can be obtained for classes with inﬁnitely many hypotheses but ﬁnite VC dimensions. More-
over, as we will see next, under additional assumptions, it is possible to obtain much better bounds
in terms of T for Epoch-Greedy, such as O(k ln m + k ln T ). This extends the classical O(ln T )
bound for standard bandits, and is not possible to achieve using Exp4 or simple variations of it.

6

4.2 Finite hypothesis space with unknown expected reward gap

ra,t I (h(xt ) = at ).

This example illustrates the importance of allowing sample-dependent s(Z ‘
1 ). We still assume a
ﬁnite hypothesis space, with m = |H| < ∞. However, we would like to improve the performance
bound by imposing additional assumptions. In particular we note that the standard bandits problem
has regret of the form O(ln T ) while in the worst case, our method for the contextual bandits problem
has regret O(T 2/3 ). A natural question is then: what are the assumptions we can impose so that the
Epoch-Greedy algorithm can have a regret of the form O(ln T ).
The main technical reason that the standard bandits problem has regret O(ln T ) is that the expected
reward of the best bandit and that of the second best bandit has a gap:
the constant hidden in
the O(ln T ) bound depends on this gap, and the bound becomes trivial (inﬁnity) when the gap
approaches zero. In this example we show that a similar assumption for contextual bandits problems
leads to a similar regret bound of O(ln T ) for the Epoch-Greedy algorithm.
Let H = {h1 , . . . , hm}, and assume without loss of generality that R(h1 ) ≥ R(h2 ) ≥ · · · ≥
R(hm ). Suppose that we know that R(h1 ) ≥ R(h2 ) + ∆ for some ∆ > 0, but the value of ∆ is not
known in advance.
1 . Let the empirical reward of h ∈ H
nX
Although ∆ is not known, it can be estimated from the data Z n
be
ˆR(h|Z n
1 ) = k
n
t=1
1 , and ˆh2 be the hypothesis with second
Let ˆh1 be the hypothesis with highest empirical reward on Z n
highest empirical reward. We deﬁne the empirical gap as
1 ) − ˆR(ˆh2 |Z n
1 ) = ˆR(ˆh1 |Z n
ˆ∆(Z n
1 ).
Let h1 be the hypothesis with the highest true expected reward, then we suffer a regret when ˆh1 6=
h1 . Again, the standard large deviation bound implies that there exists a universal constant c > 0
such that for all j ≥ 1:
1 ) ≥ (j − 1)∆, ˆh1 6= h1 ) ≤me−ck−1 n(1+j 2 )∆2
P ( ˆ∆(Z n
1 ) ≤ 0.5∆) ≤me−ck−1 n∆2
P ( ˆ∆(Z n
.
1 ) = bm−1 e(2k)−1 cn ˆ∆(Z n
1 )2 c. With this choice, there exists a constant c0 > 0
Now we can set s(Z n
d∆−1 eX
such that
d∆−1 eX
j=1
d∆−1 eX
j=1
d∆−1 eX
j=1
≤
e−ck−1 n(0.5j 2+1)∆2
≤c0pk/n∆−1 e−ck−1 n∆2
j=1
.
There exists a constant c00 > 0 such that for any L:
∞X
LX
pk/‘∆−1 e−ck−1 ‘∆2
µ‘ (H, s) ≤L + c0
‘=1
‘=1
≤L + c00k∆−2 .

1 ) ≤ j∆}P ( ˆ∆(Z n
sup{s(Z n
1 ) ∈ [(j − 1)∆, j∆], ˆh1 6= h1 )
1 ) : ˆ∆(Z n

1 ) ∈ [(j − 1)∆, j∆], ˆh1 6= h1 )
P ( ˆ∆(Z n

e(2k)−1 cnj 2∆2−ck−1 n(1+j 2 )∆2

µn (H, s) ≤

≤

≤

m−1 e(2k)−1 cnj 2∆2

7

(cid:24) 8k(ln m + ln(T + 1))
(cid:25)
Now, consider any time horizon T . If we set n‘ = 0 when ‘ < L, nL = T , and
c∆2

L =

,

then

1 ) ≤ nL ) ≤ P ( ˆ∆(Z L
1 ) ≤ 0.5∆) ≤ me−ck−1L∆2 ≤ 1/T .
P (s(Z L
(cid:25)
(cid:24) 8k(ln m + ln(T + 1))
1 ) = bm−1 e(2k)−1 cn ˆ∆(Z n
1 )2 c in Figure 1, then
That is, if we choose s(Z n
∆R(Epoch-Greedy, H, T ) ≤ 2L + 1 + c00k∆−2 ≤ 2
c∆2

+ 1 + c00k∆−2 .

The regret for this choice is O(ln T ), which is better than O(T 2/3 ) of Section 4.1. However, the
constant depends on the gap ∆ which can be small. It is possible to combine the two strategies (that
1 ) choice of Section 4.1 when ˆ∆(Z n
is, use the s(Z n
1 ) is small) and obtain bounds that not only work
well when the gap ∆ is large, but also not much worse than the bound of Section 4.1 when ∆ is small.
As a special case, we can apply the method in this section to solve the standard bandits problem.
The O(k ln T ) bound of the Epoch-Greedy method matches those more specialized algorithms for
the standard bandits problem, although our algorithm has a larger constant.

5 Conclusion

We consider a generalization of the multi-armed bandits problem, where observable context can
be used to determine which arm to pull and investigate the sample complexity of the explo-
ration/exploitation trade-off for the Epoch-Greedy algorithm.
The Epoch-Greedy algorithm analysis leaves one important open problem behind. Epoch-Greedy is
much better at dealing with large hypothesis spaces or hypothesis spaces with special structures due
to its ability to employ any data-dependent sample complexity bound. However, for ﬁnite hypothesis
space, in the worst case scenario, Exp4 has better dependency on T . In such situations, it’s possible
that a better designed algorithm can achieve both strengths.

References
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite time analysis of the multi-armed bandit
problem. Machine Learning, 47, 235–256.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995). Gambling in a rigged casino: The
adversarial multi-armed bandit problem. FOCS.
Even-dar, E., Mannor, S., & Mansour, Y. (2006). Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. JMLR, 7, 1079–1105.
Heckman, J. (1979). Sample selection bias as a speciﬁcation error. Econometrica, 47, 153–161.
Kearns, M., Mansour, Y., & Ng, A. Y. (2000). Approximate planning in large pomdps via reusable
trajectories. NIPS.
Lai, T., & Robbins, H. (1985). Asymptotically efﬁcient adaptive allocation rules. Advances in
Applied Mathematics, 6, 4–22.
Lai, T., & Yakowitz, S. (1995). Machine learning and nonparametric bandit theory. IEEE TAC, 40,
1199–1209.
Pandey, S., Agarwal, D., Chakrabarti, D., & Josifovski, V. (2007). Bandits for taxonomies: a model-
based approach. SIAM Data Mining Conference.
Strehl, A. L., Mesterharm, C., Littman, M. L., & Hirsh, H. (2006). Experience-efﬁcient learning in
associative bandit problems. ICML.
Wang, C.-C., Kulkarni, S. R., & Poor, H. V. (2005). Bandit problems with side observations. IEEE
Transactions on Automatic Control, 50, 338–355.

8


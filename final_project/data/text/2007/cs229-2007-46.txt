Sparse Coding of Point Cloud Data
CS229 Project Report
Alex Teichman
alex.teichman@cs.stanford.edu

1

Abstract—Point clouds, made available through laser
range ﬁnders, stereo cameras, or time of ﬂight cameras,
are frequently used in robot navigation systems. However,
no unsupervised machine perception algorithm exists to
provide understanding of the data; e.g. that a particular
blob of points looks roughly like, say, a car. In this paper,
we take steps towards such an algorithm based on sparse
coding. The work here generalizes to any binary data.
An algorithm for learning the bases and the activations
for point cloud data is derived and demonstrated. Given
precomputed basis vectors and an input vector, calculating
the activations is very fast and could be used in real-time
applications.

I . BACKGROUND
Sparse coding is known to be used in the abstract
representation of input in biological sensory systems [2].
An efﬁcient method of determining the bases and acti-
vations of a sparse coding representation of image data
has been speciﬁed already [3], but the initial assumptions
that are made need to be changed for use in the point
cloud regime.
There are two main reasons for doing this work.
First, the bases and activations may be useful as features
in other machine learning algorithms. Second, stacking
layers of these algorithms in a hierarchy may result
in even more abstract and useful features that would
make the job of recognizing frequently-seen objects in
the environment easier. This may also provide some
insight into the way the brain generates abstract features,
though this is a secondary goal to developing useful
unsupervised machine learning algorithms.

I I . O PT IM I ZAT ION PROBL EM D ER IVAT ION
We use the following conventions:

x(i) ∈ {−1, 1}k , an input vector, i ∈ {1...m}.
b(l) ∈ Rk , a basis vector, l ∈ {1...n}.
s(i) ∈ Rn , an activations vector, i ∈ {1...m}.

Because we working with 3D point cloud data, the
the receptive ﬁelds in this case are cubes. The geometry
of the data is ignored, however, and the receptive ﬁeld

Fig. 1. An example of Ashutosh Saxena’s point cloud data of natural
scenes [1].

cubes are vectorized to create the inputs x(i) . The
same reasoning applies for the bases. Matrices X , B ,
and S have columns of inputs, activations, and bases
respectively. B can also be seen as a matrix with rows
of bT
l vectors.

 |
|
· · ·


b(1)
b(2)
|
|
1 —
— bT
2 —
— bT
...
— bT
k —
We make the following assumptions to reﬂect the bi-
nary nature of the data and the sparsity of the activations.

|
b(n)
|

B =

=

m(cid:89)
P (x(i) |s(i) , B ) =
j |s(i) , bj )
P (x(i)
i=1
j |s(i) , bj ) = σ(x(i)
P (x(i)
j s(i) )
j bT
1
=
1 + exp(−x(i)
j s(i) )
j bT
P (s(i) ) ∝ exp(−β ||s(i) ||1 )

Starting with the usual MAP estimate used in sparse
coding and applying the above assumptions, we have the
optimization problem
X
β ||s(i) ||1 − X
X
i
j
i
||b(l) ||2
2 ≤ 1, ∀l.

j |s(i) , bj )
log P (x(i)

minS,B

s.t.

(1)

The norm constraint on the bases is necessary because
there always exists a linear transformation of b(l) and s(i)
which will not change the reconstruction term but will
make the sparsity term go to zero.
This optimization problem will be solved by alternat-
ing minimization. First, consider the case of holding the
bases ﬁxed and ﬁnding the activations. (1) can then be
written in the following form.
||s(i) ||1 − X
X
X
0@β ||s(i) ||1 − X
i
i
j
j

j |bj , s(i) )
1A (2)
log P (x(i)
j |bj , s(i) )
log P (x(i)

minS
P
i

min
s(i)

β

Each of the i = 1..m minimization problems in (2)
can be solved efﬁciently using the L1 regularized logistic
regression algorithm described in [4].
Now consider the case of holding the activations ﬁxed
and ﬁnding the bases. We can then write (1) in the
following form and use projected gradient descent to
solve it.
minB − (cid:88)
(cid:88)
1
log
1 + exp(−x(i)
j s(i) )
j bT
j
i
2 ≤ 1, ∀l
||b(l) ||2
s.t.
Gradient descent is run on the objective function only;
at every iteration, B is projected back to the feasible set.
aj is a column vector used to select a column from B T
(i.e. one of the bj ’s). This gives us the update rule

2

∇B (obj ) = − m(cid:88)
k(cid:88)
B := B − α∇B (obj)
(cid:17) .
(cid:16)
x(i)
j aj s(i)T
x(i)
1 + exp
j s(i)T B T aj
i=1
j=1
Several other methods were tested for calculating
subject to (cid:80)
the bases. L1 logistic regression with the constraint on
the bj ’s and gradient descent on the objective function
l ||b(l) ||2
2 ≤ 1 both failed, likely because of
the inadequacy of the constraints.

I I I . R E SU LT S
The alternating minimization described in the previous
section was run on ten thousand 5x5x5 cubic samples
(discretized at three points per meter) from Ashutosh
Saxena’s laser scans of natural scenes [1]. Convergence
was declared when the change in all of the bases or all of
the minimizations (deﬁned by euclidean distance of the
vectors) dropped below 10−4 for at least ten iterations
or when 100 iterations completed. The number of bases
was chosen to be thirty.
All code was written in Matlab; the Matlab version
of the L1 regularized logistic regression solver made
available along with [4] was used for the calculation of
the activations.
The resulting bases seem to be a mix of planes and
gradients at different orientations and small sets of points
with no apparent structure. An example of typical bases
that result are shown in Fig. 2. The average time to
calculate the activations given the bases and an input
vector was .0034 seconds; the average time to compute
the bases with projected gradient descent given the
activations and all the inputs was 15.9 seconds. To give
a rough idea of total computation time, the test which
produced the example in Fig. 2 took about an hour and
a half to converge on a Linux box with an Intel Core 2
Duo T7500, 2.2GHz processor and 2GB RAM.

Fig. 2.
Five bases are shown here; since they are a 5x5x5 cube of
real numbers, 5 slices of each base are shown across the rows.

IV. D I SCU S S ION
The calculation of the bases is relatively slow, but
the calculation of the activations is very fast. With the
current code, ﬁnding the activations for, say, 100 inputs
that comprise the “eye” of a robot would correspond
to a processing rate of about 3Hz. This is approaching
the rate required for useful real time operation. Further,
this code was all in Matlab, and it is likely that faster
implementations could be produced. This is encouraging.

A. Shortcomings
Beacuse the activations are positive or negative num-
bers, there is not a natural interpretation for stacking the
algorithm, i.e. making the outputs of one the inputs of
the next. This is a somewhat serious concern.
Further, it was inteded that the basis vector activations
correspond roughly to which features are present
in
the data; however,
the case when negative
this isn’t
activations are possible. It makes sense to allow positive
and negative values for elements of basis vectors in
the same way that neurons in the early visual system
respond to input with light present
in one area and
absent
in another. However, reconstructing the input
by subtracting features doesn’t have a neural correlate
that I am aware of - but I am not an expert in this.
More importantly, it seems that positive activations (i.e.
presences of basis vectors in the input data) could be
more useful in classiﬁcation tasks. That intuition may be
completely wrong, but it is something to explore further.
For example, look at the activations for the input in
Fig. 3. The base in the ﬁrst row is used to subtract from
all layers except the middle. The next two bases are
gradients and are used to add to the region with the
points present. The input, however, has nothing to do
with planes or gradients.
These two shortcomings could be addressed by chang-
ing the assumptions so that s(i) ∈ Rn
+ . The bi-
nary input to the next layer could then be generated
from P (s(i) |x(i) , B ). Another approach might be to try
s(i) ∈ {0, 1}n with a larger set of basis vectors to make
up for the lack of granularity in the activations.

B. Future Work
Despite the discussion in Section IV-A, the ﬁrst thing
to do is see if the resulting sparse representation of the
data can be used for anything useful. Two immediate
opportunities present
themselves: using the bases as
features in detecting the presence or absence of cars
in Velodyne data from Junior and using the bases as
features in ﬁnding a grasping point for objects with
STAIR. Suggestions for other applications are welcome.

Fig. 3. An example of an undesirable reconstruction resulting from
negative activations. For the input, white indicates a point that is
present. The numbers underneath the base labels indicate the value
of the activations.

Some tweaking of the current algorithm also needs
to occur. It is possible that the less-desireable bases -
those with just a small set of points with no apparent
structure - can be removed by training on a larger and
more varied dataset or changing the sparsity parameter
β . Also, cross validation over the number of bases would
be interesting to see; only the pre-set choice of 30 bases
has been tested so far.
Finally, it would be interesting to test this algorithm
on binary data of a different sensor modality to see if
the resulting bases are useful.

V. CONC LU S ION S
In this paper, we derive an efﬁcient sparse coding
algorithm for binary inputs. It is possible that the re-
sulting bases could be used as better features for other
machine learning algorithms. It
is also possible that
stacking modiﬁed versions of this algorithm would result
in hierarchies of more abstract features that would be
even more useful.
Building on the very efﬁcient implementation of L1
regularized logistic regression in [4], we show that this
method has the potential
to be useful
in real
time
applications.

V I . ACKNOW L EDG EM EN T S
Many people were helpful
in the creation of this
work. Thanks to Honglak Lee, Rajat Raina, Suin Lee,
Ashutosh Saxena, Varun Ganapathi, Dan Ramage, Quoc
Le, Paul Baumstark, and Catie Chang for making code
available, making data available, and/or discussions on
the derivation.

3

R E FERENC E S
[1] A. Saxena, A. Ng, and S. Chung, “Learning depth from single
monocular images,” NIPS, vol. 18, 2005. [Online]. Available:
http://ai.stanford.edu/ asaxena/learningdepth/
[2] B. A. Olshausen and D.
coding of
“Sparse
J. Field,
sensory inputs,” Current Opinion in Neurobiology, vol. 14,
no. 4, pp. 481–487, August 2004.
[Online]. Available:
http://dx.doi.org/10.1016/j.conb.2004.07.007
[3] H. Lee, A. Battle, R. Raina, and A. Y. Ng, “Efﬁcient sparse coding
algorithms,” in Advances in Neural Information Processing Sys-
tems 19, B. Sch ¨olkopf, J. Platt, and T. Hoffman, Eds. Cambridge,
MA: MIT Press, 2007, pp. 801–808.
and A. Y. Ng,
P. Abbeel,
Lee, H.
[4] S.-I.
Lee,
in AAAI.
regression,”
regularized
“Efﬁcient
l1
logistic
AAAI
Press,
2006.
[Online]. Available:
http://dblp.uni-
trier.de/db/conf/aaai/aaai2006.html#LeeLAN06

4


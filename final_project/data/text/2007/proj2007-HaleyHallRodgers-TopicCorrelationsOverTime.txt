Topic Correlations over Time

David Haley(dhaley@), David Hall (dlwh@), and Mike Rodgers (mikepr@cs)

Abstract

Topic models have proved useful for analyzing
large clusters of documents. Most models devel-
oped, however, have paid little attention to the
analysis of the latent topics themselves, particu-
larly with regards to change in their correlation
over time. We present a novel, probabilistically
well-founded extension to Latent Dirichlet Allo-
cation (LDA) which can explicitly model topic
drift over time. Using this extension, we analyze
the correlations of topics over time in a corpus
of ACL papers.

1 Introduction

A well-documented “Statistical Revolution” in Natu-
ral Language Processing (NLP) took place in the early
1990’s (Gaz96), leading to a very rapid increase in the
use of statistical methods to build natural language sys-
tems. Obvious examples include machine translation
and parsing, which have become largely synonymous
with their statistical variants. However, no work has
been done to analyze this shift, despite the recent pro-
liferation of topic models designed for large document
corpora.
In this paper, we develop a model to examine this
feature of topic shift over time. We then use the model
to analyze the texts of about 12,500 papers from the As-
sociation of Computational Linguists, available via the
ACL Anthology (Bir). This corpus covers nearly all pa-
pers from 1965 until 2006. We ﬁrst automatically extract
a number of topics. We then infer correlation among the
most salient of these topics. Finally, we present the re-
sults of our analysis, concluding with promising direc-
tions for future work.

2 Previous Work

A signiﬁcant amount of attention has been given to topic
modeling, and most recent work in this area has followed

D = the number of documents in the corpus
K = the number of topics in the corpus
Ni = the number of words in document i
θi ∼ Dir(α), i ∈ [1, D ]
φk ∼ Dir(η), k ∈ [1, D ]
zi,n ∼ Mult(θi ), i ∈ [1, D ], n ∈ [1, Ni ]
wi,n ∼ Mult(φzi,n )

Figure 1: Latent Dirichlet Allocation

from LDA(BNJ03), which merits a brief review here.
(Figure 1.) LDA is a mixture model of latent topics z
in documents d, which have words w . Each document is
characterized by a multinomial distribution over topics
θd , and each topic is characterized by a multinomial dis-
tribution over words φz . Finally, both the θd ’s and φz ’s
have (usually symmetric) Dirichlet priors with hyperpa-
rameters α and η respectively. These hyperparameters
can be ﬁxed or sampled from a Gamma distribution.
Thus far, this model has no explicit representation
of time: all documents are exchangeable. Several mod-
els have been proposed to remove this assumption. Most
similar to LDA is the Topics over Time model (ToT)
(WM06). ToT assigns a time stamp t in the range [0,1]
to each document, and repeatedly samples the timestamp
according to a topic-dependent Beta distribution ψz . To
conserve space, we forgo reproducing the entire speciﬁ-
cation, simply adding the extra parameter here:
ti,n ∼ Beta(ψzi,n ), i ∈ [1, D ], n ∈ [1, Ni ]
Representation of time as a continuous variable has sev-
eral useful properties. First, it is a rather minimal addi-
tion to the model, making it easy to implement. More-
over, it avoids the problem of Markovization: there is
no need to divide time into a speciﬁc number of epochs.
This negates the need to determine the proper dividing
line for documents and the need to decide whether or
not to cluster natural epochs. However, it makes ﬁnding

T = # epochs
Dt = # documents in the corpus for the t’th epoch
K = # topics in the corpus
Nt,i = # words in document i for epoch t
αt ∼ Norm(αt−1 , σ1 I )
φt ∼ Norm(φt−1 , σ2 I )
θt,i ∼ Norm(αt , σ3 I ), i ∈ [1, Dt ]
zt,i,n ∼ Mult(π(θt,i )), i ∈ [1, Dt ], n ∈ [1, Nt,i ]
wt,i,n ∼ Mult(π(φt,zi,n ))

T = # epochs
Dt = # documents in epoch t
K = # topics in the corpus
Nt,i = # words in document i in epoch t
αt,i ∼ Gamma(1, ˆα), t ∈ [1, T ], i ∈ [1, K ]
θt,i ∼ Dir(αt ), i ∈ [1, Dt ]
φk ∼ Dir(η), k ∈ [1, Dt ]
zt,i,n ∼ Mult(θi ), i ∈ [1, Dt ], n ∈ [1, Ni ]
wt,i,n ∼ Mult(φzi,n )

Figure 2: Dynamic Topic Model

Figure 3: Unmarkovized Time Model

the change in relationship between any two topics over
time much harder.
On the other extreme, the Dynamic Topic Model
(DTM) represents documents as points on the topic-
distribution simplex about a centroid (BL06). The DTM
uses a normal distribution and a normalization function
to constrain the points to be on the simplex. More-
over, each epoch’s centroid is sampled from the previous
year’s centroid, enabling “drift” of topics from epoch to
epoch. Similarly, the vocabulary distributions for each of
the topics are chosen as a centroid. The model is speci-
ﬁed fully in Figure 2. This model requires a substantial
change to the inference procedures, and it requires renor-
malizing topic and vocabulary distributions by a trans-
form function π .
Finally, we mention brieﬂy the Correlated Topic
Model (BLar), which is more or less identical to the
DTM, except that there are no times, only correlations
between topics. In principle these models could be com-
bined to give the desired results, but with great math-
matical overhead. In this paper we propose models that
accomplish the same result but require slightly less com-
plicated machinations.

3 Temporal Model Speciﬁcation

a normal distribution, we opt instead to concentrate on
the hyperparameter αt , which we will take as represen-
tative of the mean “topic strength” (expectation of θt )
for a given epoch t. (Strictly, αt is a multiple of the ex-
pectation of the Dirichlet, but normalizing it solves the
problem.) First, we remove the symmetry of the Dirich-
let distribution for θ , allowing the components of the α’s
to vary. Finally, we add a prior on these α’s, which we
call ˆα. The speciﬁcation for this non-Markovized model
is in Figure 3. ˆα is the scale parameter of the αs’ prior.
This ensures conjugacy, but still maintains the same ex-
pectation for the αs.1 This parameter can be sampled
from a Gamma distribution, though in our experiments
we found that 0.1 produces good results.

4 Topic Correlations

We considered a number of techniques for determining
correlations, but in this section we develop an extension
to the UTM (or, simply, LDA) that has support for mod-
eling topic correlations as part of the inference. How-
ever, going forward, we will use “correlation” in the in-
formal sense. In particular, we will model “correlation”
as “confusability:” with what probability can one topic
be “confused” for another topic?

4.1 The Correlated Dirichlet Distribution

In this section we develop an unmarkovized (but still
discretized) dynamic model with minimal disruption to
LDA. Like the DTM, we split time into discrete epochs
(which, for now, are exchangeable), but instead of using

In this section, we brieﬂy discuss a new distribution,
which we term the Correlated Dirichlet Distribution
1We omit the details of deriving the Gibbs sampler in this paper
due to space constraints.

T = # epochs
Dt = # documents in epoch t
K = # topics in the corpus
Nt,i = # words in document i in epoch t
αt,i ∼ Gamma(1, ˆα), t ∈ [1, T ], i ∈ [1, K ]
t,i ∼ Dir(βi ), t ∈ [1, T ], i ∈ [1, K ]
BT
θt,i ∼ CDir(αt , Bt ), i ∈ [1, Dt ]
φk ∼ Dir(η), k ∈ [1, Dt ]
zt,i,n ∼ Mult(θi ), i ∈ [1, Dt ], n ∈ [1, Ni ]
wt,i,n ∼ Mult(φzi,n )

be a probability distribution. We also introduce a strong
diagonal bias in the β hyperparameters to help ensure
that topics do not get “washed out” in the mixing. That
is, bii (cid:29) bij for bii ! = bj j For inference, however,
this model becomes slightly more difﬁcult. We cannot
use Gibbs sampling to sample the matrix B, though with
ﬁxed B it is straightforward to use Gibbs sampling for α.
To update B, we use the Metropolis-Hastings algorithm
(Has70) to update the columns of B individually. Details
of the proposal distributions we used are discussed in the
appendix.

5 Results

Figure 4: Correlated Unmarkovized Time Model

(CDD). To begin, we note that the Dirichlet has neutral-
ity; that is, if θ |α ∼ Dir(α), then θi ⊥ θj
. Unfor-
1−θi
tunately, this property indicates that the Dirichlet has no
natural means to specify the strength of correlation (for-
mally or informally) between any of the components of
its samples. Therefore, we propose the CDD, which ex-
plicitly adds a mixture component to the Dirichlet Distri-
bution. Intuitively, we can think of samples from a CDD
as samples from a standard Dirichlet Distribution that
have been distorted as the result of one step on a random
walk speciﬁed by some stochastic matrix. Formally, we
say that if B is a stochastic matrix, and
θ∗ |α ∼ Dir(α)
θ = θ∗B
then θ ∼ CDir(α, B). We should note that the CDD
bears some resemblance to the Dependent Dirichlet Dis-
tribution in (LLWC06). However, the requires that the
matrix B take a speciﬁc form relating to the Markov tran-
sition probabilities deﬁned by their problem. We spec-
ify the matrix as a parameter to the Distribution. In this
sense, the CDD is a generalization of the DDD.

4.2 The Correlated Unmarkovized Time Model

Integrating the CDD into our model is fairly straight-
forward. The Correlated Unmarkovized Time Model
(CUTM) is speciﬁed in Figure 4. In particular, we in-
troduce the CDD and a diagonally-biased prior on the
rows of B. Thus, we require that each of the rows of B

We implemented both the UTM and CUTM and ran the
model on 11,000 papers of the ACL Anthology over a
wide range of topics until convergence. For K=100, the
UTM ran in about 3/4 the time of the CUTM. In the in-
terest of space, we report only topics from the CUTM.
Figure 5 lists the top 20 words chosen by Mutual Infor-
mation for the top 3 topics (by word count) as well as
Topic 3, which we use to illustrate correlations. We cap-
tion the topics with titles based on our interpretation of
the topics’ distribution over the vocabulary.
Figure 6 is a smoothed plot of the α’s for those top-
ics over time. As you can see, the graph is incredibly
jagged, leading us to conclude that treating epochs as
exchangeable should be reconsidered. However, there
is a reasonable correspondence between peaks on the
graph and the topics themselves. Topic 73 peaks in the
early 1980’s, and the topics themselves are constrained
to dates in the 1970’s, and words we associate with pa-
pers from that era. Topic 3 has peaks exclusively in
the 1980’s, which corresponds to the words listed. We
see 1987 as a prominent year.
(We also mention that
1982, 1984, and 1985 also are in the top 40 words for
Topic 3.) Topics 18 and 73 have peaks later on, and
have words like “probability”, “evaluation”, and “per-
formance”: words we associate with the Statistical Rev-
olution.
Figure 7 is a smoothed plot of the directional con-
fusability of Topic 3 with Topic 73 and of Topic 73 with
itself. Again, we note that there is a considerable amount
of variance in both graphs, indicating that the B matrices
should also be markovized. However, we see that Topic
3 (“the 1980’s”) and Topic 73 (“1970’s”) are most corre-

Topic 3 “1980’s parsing”
university
grammar
class
trees
semantic
language
process
constraints
parser
event
np
syntactic
set
elements
1987
wordnet
number
utterance
book
Topic 73 “1970s”
computer
object
science
speech
1975
program
network
discourse
report
structure
objects
federal
act
1973
1976
information
research
semantic
location

Topic 18 “Statistics”
features
model
document
task
data
lexical
language
precision
ranking
verb
score
relation
probability
research
semantic
evaluation
labeled
goal
dialogue
Topic 74 “Stat. Parsing”
discourse
target
lexicon
information
verb
constraints
machine
language
state
similar
rule
entity
clause
feature
head
performance
deﬁnite
module
function

Figure 5: Topic 20 words for 4 topics, chosen by Mutual
Information

Figure 6: αt,i over time for 4 topics

lated in the 1990’s. Again, this matches our expectation:
the older two topics become, the more confusable they
are. These topics tend to arise in historical papers, or
they are used in papers that cite older work.

6 Conclusion and Future Directions

We have developed an unmarkovized model for directly
learning topic prominence and correlation over time us-
ing an additional level of hyperparameters and a stochas-
tic matrix, respectively. We then examined the results
of ﬁtting that model, and observed that the topics and
their strengths were in fact meaningfully related. We
also looked at confusability of two topics and found a
similar relation there.
However, much more work is needed. The assump-
tion of the exchangeability of epochs is clearly incorrect:
the jaggedness seen in the graphs is too pronounced. On
the other hand, that we saw localized peaks and not sev-
eral disjoint eras indicates that the topics being extracted
were in fact located in certain eras, so this model is still
valuable. Regardless, for practical purposes, it is proba-
bly best to condition epochs on their ancestors.
More work should also be done to compare this
model with LDA and the DTM in terms of log likeli-
hood. Moreover, as with all topic models, a better metric
of performance is necessary: log likelihood is useful for
training, but still we could use a better method of evalu-
ating these systems.

Gerald Gazdar. Paradigm merger in natural language pro-
cessing, pages 88–109. Cambridge University Press,
1996.

W.K. Hastings. Monte carlo sampling methods using markov
chains and their applications, 1970.

Xuejun Liao, Qiuhua Liu, Chunping Wang,
and
Lawrence Carin. Neighborhood-based classiﬁcation.
http://www.ee.duke.edu/∼lcarin/DDD talk yale 3.pdf,
2006.

Xuerui Wang and Andrew McCallum. Topics over time: A
non-markov continuous-time model of topical trends.
2006.

A Derivation of Proposal Distributions for
Metropolis Hastings Sampling
A.1 Resampling θ∗
A highly effective proposal distribution for θ∗ can be
(θ∗Bk )zk Y
p(~z |θ∗ )p(θ∗ |α) ∝ Y
computed fairly easily. We proceed by relaxing the pos-
terior. Let ~z be the topic counts for a document.
(θ∗
= Y
(X
k )αk
k
k
i bki )zk (θ∗
k )αk−1
θ∗
≈ Y
i
k
(θ∗
k bkk )zk (θ∗
k )αk−1
∝ Y
k
(θ∗
k )zk+αk−1
k
= Dir(α + ~z )
The step in which we dropped the summation takes ad-
vantage of the assumption that bkk (cid:29) bkj , for k 6= j ,
which we enforce on our priors. This result matches our
intuition: draws from a Dirichlet distribution should be
roughly the same as draws from a Correlated Dirichlet
distribution with a strong diagonal bias.

A.2 Resampling BT
k

Resampling the rows of B is a bit different. While a
similar calculation could be used, in practice we ﬁnd that
is it is often better to choose a weak diagonally-biased
prior and weight the current sample more highly when
conditioning the Dirichlet. That is,
k ∼ Dir(βk + C BT
BT0
k )
where C is large.

Figure 7: βt,ik (Confusability) over time for 2 topics

Finally, there are still other ways to look at cor-
relations. One could argue that correlations should be
inferred post hoc, and thus an approach like Indepen-
dent Component Analysis should be useful. Some initial
analyses (which do not ﬁt here), seem to ﬁnd reasonable
post hoc topics.

Acknowledgements

We would like to thank Dan Ramage, Chris Manning and
Dan Jurafsky for their help in thinking through the issues
here. We would also like to thank Hal Daum ´e for the
Hierarchical Bayesian Compiler (Dau), which we used
to implement early prototypes of the UTM.

References

Steven Bird. Association of computational linguists anthol-
ogy. http://www.aclweb.org/anthology-index/.

David Blei and John D. Lafferty. Dynamic topic models.
ICML, 2006.

David Blei and John D. Lafferty. A correlated topic model of
science. Annals of Applied Science, To Appear.

David Blei, Andrew Ng, and Michael Jordan. Latent dirich-
let allocation. Journal of Machine Learning Research,
3:993–1022, 2003.

Hal Daum ´e.
HBC: Hierarchical Bayes Compiler.
http://hal3.name/HBC. 2007.


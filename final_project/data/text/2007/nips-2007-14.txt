Incremental Natural Actor-Critic Algorithms

Shalabh Bhatnagar
Department of Computer Science & Automation, Indian Institute of Science, Bangalore, India

Richard S. Sutton, Mohammad Ghavamzadeh, Mark Lee
Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada

Abstract
We present four new reinforcement learning algorithms based on actor-critic and
natural-gradient ideas, and provide their convergence proofs. Actor-critic rein-
forcement learning methods are online approximations to policy iteration in which
the value-function parameters are estimated using temporal difference learning
and the policy parameters are updated by stochastic gradient descent. Methods
based on policy gradients in this way are of special interest because of their com-
patibility with function approximation methods, which are needed to handle large
or in(cid:2)nite state spaces. The use of temporal difference learning in this way is of
interest because in many applications it dramatically reduces the variance of the
gradient estimates. The use of the natural gradient is of interest because it can
produce better conditioned parameterizations and has been shown to further re-
duce variance in some cases. Our results extend prior two-timescale convergence
results for actor-critic methods by Konda and Tsitsiklis by using temporal differ-
ence learning in the actor and by incorporating natural gradients, and they extend
prior empirical studies of natural actor-critic methods by Peters, Vijayakumar and
Schaal by providing the (cid:2)rst convergence proofs and the (cid:2)rst fully incremental
algorithms.

1 Introduction
Actor-critic (AC) algorithms are based on the simultaneous online estimation of the parameters of
two structures, called the actor and the critic. The actor corresponds to a conventional action-
selection policy, mapping states to actions in a probabilistic manner. The critic corresponds to a
conventional value function, mapping states to expected cumulative future reward. Thus, the critic
addresses a problem of prediction, whereas the actor is concerned with control. These problems are
separable, but are solved simultaneously to (cid:2)nd an optimal policy, as in policy iteration. A variety
of methods can be used to solve the prediction problem, but the ones that have proved most effective
in large applications are those based on some form of temporal difference (TD) learning (Sutton,
1988) in which estimates are updated on the basis of other estimates. Such bootstrapping methods
can be viewed as a way of accelerating learning by trading bias for variance.
Actor-critic methods were among the earliest to be investigated in reinforcement learning (Barto
et al., 1983; Sutton, 1984). They were largely supplanted in the 1990’s by methods that estimate
action-value functions and use them directly to select actions without an explicit policy structure.
This approach was appealing because of its simplicity, but when combined with function approxima-
tion was found to have theoretical dif(cid:2)culties including in some cases a failure to converge. These
problems led to renewed interest in methods with an explicit representation of the policy, which
came to be known as policy gradient methods (Marbach, 1998; Sutton et al., 2000; Konda & Tsit-
siklis, 2000; Baxter & Bartlett, 2001). Policy gradient methods without bootstrapping can be easily
proved convergent, but converge slowly because of the high variance of their gradient estimates.
Combining them with bootstrapping is a promising avenue toward a more effective method.
Another approach to speeding up policy gradient algorithms was proposed by Kakade (2002) and
then re(cid:2)ned and extended by Bagnell and Schneider (2003) and by Peters et al. (2003). The idea

1

was to replace the policy gradient with the so-called natural policy gradient. This was motivated by
the intuition that a change in the policy parameterization should not in(cid:3)uence the result of the policy
update. In terms of the policy update rule, the move to the natural gradient amounts to linearly
transforming the gradient using the inverse Fisher information matrix of the policy.
In this paper, we introduce four new AC algorithms, three of which incorporate natural gradients. All
the algorithms are for the average reward setting and use function approximation in the state-value
function. For all four methods we prove convergence of the parameters of the policy and state-value
function to a local maximum of a performance function that corresponds to the average reward plus
a measure of the TD error inherent in the function approximation. Due to space limitations, we
do not present the convergence analysis of our algorithms here; it can be found, along with some
empirical results using our algorithms, in the extended version of this paper (Bhatnagar et al., 2007).
Our results extend prior AC methods, especially those of Konda and Tsitsiklis (2000) and of Peters
et al. (2005). We discuss these relationships in detail in Section 6. Our analysis does not cover the
use of eligibility traces but we believe the extension to that case would be straightforward.

2 The Policy Gradient Framework
We consider the standard reinforcement learning framework (e.g., see Sutton & Barto, 1998), in
which a learning agent interacts with a stochastic environment and this interaction is modeled as a
discrete-time Markov decision process. The state, action, and reward at each time t 2 f0; 1; 2; : : :g
are denoted st 2 S , at 2 A, and rt 2 R respectively. We assume the reward is random, real-
valued, and uniformly bounded. The environment’s dynamics are characterized by state-transition
probabilities p(s0 js; a) = Pr(st+1 = s0 jst = s; at = a), and single-stage expected rewards
r(s; a) = E[rt+1 jst = s; at = a], 8s; s0 2 S ; 8a 2 A. The agent selects an action at each time t
using a randomized stationary policy (cid:25)(ajs) = Pr(at = ajst = s). We assume

(cid:25)(ajs)r(s; a);

J ((cid:25)) = lim
T !1

(B1) The Markov chain induced by any policy is irreducible and aperiodic.
The long-term average reward per step under policy (cid:25) is de(cid:2)ned as
E "T (cid:0)1Xt=0
rt+1 j(cid:25)# = Xs2S
d(cid:25) (s) Xa2A
1
T
where d(cid:25) (s) is the stationary distribution of state s under policy (cid:25) . The limit here is well-
de(cid:2)ned under (B1). Our aim is to (cid:2)nd a policy (cid:25) (cid:3) that maximizes the average reward, i.e.,
(cid:25)(cid:3) = arg max(cid:25) J ((cid:25)). In the average reward formulation, a policy (cid:25) is assessed according to the
expected differential reward associated with states s or state(cid:150)action pairs (s; a). For all states s 2 S
and actions a 2 A, the differential action-value function and the differential state-value function
under policy (cid:25) are de(cid:2)ned as1
1Xt=0
V (cid:25) (s) = Xa2A
In policy gradient methods, we de(cid:2)ne
class of parameterized stochastic policies
a
f(cid:25)((cid:1)js; (cid:18)); s 2 S ; (cid:18) 2 (cid:2)g, estimate the gradient of the average reward with respect to the
policy parameters (cid:18) from the observed states, actions, and rewards, and then improve the policy
by adjusting its parameters in the direction of the gradient. Since in this setting a policy (cid:25) is
represented by its parameters (cid:18) , policy dependent functions such as J ((cid:25)), d(cid:25) ((cid:1)), V (cid:25) ((cid:1)), and Q(cid:25) ((cid:1); (cid:1))
can be written as J ((cid:18)), d((cid:1); (cid:18)), V ((cid:1); (cid:18)), and Q((cid:1); (cid:1); (cid:18)), respectively. We assume

E[rt+1 (cid:0) J ((cid:25))js0 = s; a0 = a; (cid:25) ]

Q(cid:25) (s; a) =

;

(cid:25)(ajs)Q(cid:25) (s; a):

(1)

pair (s; a), policy (cid:25)(ajs; (cid:18)) is continuously differentiable in the

(B2) For any state –action
parameters (cid:18) .
Previous works (Marbach, 1998; Sutton et al., 2000; Baxter & Bartlett, 2001) have shown that the
gradient of the average reward for parameterized policies that satisfy (B1) and (B2) is given by 2
rJ ((cid:25)) = Xs2S
d(cid:25) (s) Xa2A
r(cid:25)(ajs)Q(cid:25) (s; a):
1 From now on in the paper, we use the terms state-value function and action-value function instead of
differential state-value function and differential action-value function.
2Throughout the paper, we use notation r to denote r(cid:18) (cid:150) the gradient w.r.t. the policy parameters.

(2)

2

d(cid:25) (s)b(s)r(1) = 0;

Observe that if b(s) is any given function of s (also called a baseline), then
(cid:25)(ajs)! = Xs2S
d(cid:25) (s)b(s)r  Xa2A
r(cid:25)(ajs)b(s) = Xs2S
d(cid:25) (s) Xa2A
Xs2S
and thus, for any baseline b(s), the gradient of the average reward can be written as
d(cid:25) (s) Xa2A
rJ ((cid:25)) = Xs2S
The baseline can be chosen such in a way that the variance of the gradient estimates is minimized
(Greensmith et al., 2004).
The natural gradient, denoted ~rJ ((cid:25)), can be calculated by linearly transforming the regular gra-
dient, using the inverse Fisher information matrix of the policy: ~rJ ((cid:25)) = G(cid:0)1 ((cid:18))rJ ((cid:25)). The
Fisher information matrix G((cid:18)) is positive de(cid:2)nite and symmetric, and is given by
G((cid:18)) = Es(cid:24)d(cid:25) ;a(cid:24)(cid:25) [r log (cid:25)(ajs)r log (cid:25)(ajs)> ]:

r(cid:25)(ajs)(Q(cid:25) (s; a) (cid:6) b(s)):

(3)

(4)

3 Policy Gradient with Function Approximation
Now consider the case in which the action-value function for a (cid:2)xed policy (cid:25) , Q(cid:25) , is approximated
by a learned function approximator. If the approximation is suf(cid:2)ciently good, we might hope to
use it in place of Q(cid:25) in Eqs. 2 and 3, and still point roughly in the direction of the true gradient.
Sutton et al. (2000) showed that if the approximation ^Q(cid:25)
w with parameters w is compatible, i.e.,
w (s; a) = r log (cid:25)(ajs), and minimizes the mean squared error
rw ^Q(cid:25)
E (cid:25) (w) = Xs2S
d(cid:25) (s) Xa2A
(cid:25)(ajs)[Q(cid:25) (s; a) (cid:0) ^Q(cid:25)
w (s; a)]2
w(cid:3) in Eqs. 2 and 3. Thus, we work with
for parameter value w(cid:3) , then we can replace Q(cid:25) with ^Q(cid:25)
a linear approximation ^Q(cid:25)
w (s; a) = w> (s; a), in which the  (s; a)’s are compatible features
de(cid:2)ned according to  (s; a) = r log (cid:25)(ajs). Note that compatible features are well de(cid:2)ned under
(B2). The Fisher information matrix of Eq. 4 can be written using the compatible features as
G((cid:18)) = Es(cid:24)d(cid:25) ;a(cid:24)(cid:25) [ (s; a)  (s; a)> ]:
Suppose E (cid:25) (w) denotes the mean squared error
d(cid:25) (s) Xa2A
E (cid:25) (w) = Xs2S
of our compatible linear parameterized approximation w> (s; a) and an arbitrary baseline b(s).
Let w(cid:3) = arg minw E (cid:25) (w) denote the optimal parameter. Lemma 1 shows that the value of w (cid:3)
does not depend on the given baseline b(s); as a result the mean squared error problems of Eqs. 5
and 7 have the same solutions. Lemma 2 shows that if the parameter is set to be equal to w (cid:3) , then
the resulting mean squared error E (cid:25) (w(cid:3) ) (now treated as a function of the baseline b(s)) is further
minimized when b(s) = V (cid:25) (s). In other words, the variance in the action-value-function estimator
is minimized if the baseline is chosen to be the state-value function itself.3
Lemma 1 The optimum weight parameter w (cid:3) for any given (cid:18) (policy (cid:25) ) satis ﬁes 4
w(cid:3) = G(cid:0)1 ((cid:18))Es(cid:24)d(cid:25) ;a(cid:24)(cid:25) [Q(cid:25) (s; a)  (s; a)]:
Proof Note that
rw E (cid:25) (w) = (cid:0)2 Xs2S
d(cid:25) (s) Xa2A
Equating the above to zero, one obtains
Xs2S
d(cid:25) (s) Xa2A
(cid:25)(ajs)Q(cid:25) (s; a) (s; a)(cid:0)Xs2S
d(cid:25) (s) Xa2A
(cid:25)(ajs) (s; a) (s; a)>w(cid:3) = Xs2S
d(cid:25) (s) Xa2A
3 It is important to note that Lemma 2 is not about the minimum variance baseline for gradient estimation.
It is about the minimum variance baseline of the action-value-function estimator.
4This lemma is similar to Kakade’s (2002) Theorem 1.

(cid:25)(ajs)[Q(cid:25) (s; a) (cid:0) w> (s; a) (cid:0) b(s)]  (s; a):

(cid:25)(ajs)[Q(cid:25) (s; a) (cid:0) w> (s; a) (cid:0) b(s)]2

(8)

(6)

(5)

(7)

(cid:25)(ajs)b(s) (s; a):

3

The last term on the right-hand side equals zero because Pa2A (cid:25)(ajs) (s; a) = Pa2A r(cid:25)(ajs) = 0
w E (cid:25) (w) evaluated at w(cid:3) can be seen to be 2G((cid:18)).
for any state s. Now, from Eq. 8, the Hessian r2
The claim follows because G((cid:18)) is positive de(cid:2)nite for any (cid:18) .
(cid:3)
Next, given the optimum weight parameter w (cid:3) , we obtain the minimum variance baseline in
the action-value-function estimator corresponding to policy (cid:25) . Thus we consider now E (cid:25) (w(cid:3) ) as a
function of the baseline b, and obtain b(cid:3) = arg minb E (cid:25) (w(cid:3) ).

Lemma 2 For any given policy (cid:25) , the minimum variance baseline b(cid:3) (s) in the action-value-
function estimator corresponds to the state-value function V (cid:25) (s).
let E (cid:25) ;s (w(cid:3) ) = Pa2A (cid:25)(ajs)[Q(cid:25) (s; a) (cid:0) w(cid:3)> (s; a) (cid:0) b(s)]2 .
Proof For any s 2 S ,
Then E (cid:25) (w(cid:3) ) = Ps2S d(cid:25) (s)E (cid:25) ;s (w(cid:3) ). Note that by (B1), the Markov chain corresponding to any
policy (cid:25) is positive recurrent because the number of states is (cid:2)nite. Hence, d(cid:25) (s) > 0 for all s 2 S .
Thus, one needs to (cid:2)nd the baseline b(s) that minimizes E (cid:25) ;s (w(cid:3) ) for each s 2 S . For any s 2 S ,
= (cid:0)2 Xa2A
@ E (cid:25) ;s (w(cid:3) )
(cid:25)(ajs)[Q(cid:25) (s; a) (cid:0) w(cid:3)> (s; a) (cid:0) b(s)]:
@ b(s)
Equating the above to zero, we obtain
(cid:25)(ajs)Q(cid:25) (s; a) (cid:0) Xa2A
b(cid:3) (s) = Xa2A
(cid:25)(ajs)w(cid:3)> (s; a):
The rightmost term equals zero because Pa2A (cid:25)(ajs) (s; a) = 0. Hence b(cid:3) (s) = Pa2A (cid:25)(ajs)
Q(cid:25) (s; a) = V (cid:25) (s). The second derivative of E (cid:25) ;s (w(cid:3) ) w.r.t. b(s) equals 2. The claim follows. (cid:3)
From Lemmas 1 and 2, w(cid:3)> (s; a) is a least-squared optimal parametric representation for
the advantage function A(cid:25) (s; a) = Q(cid:25) (s; a) (cid:0) V (cid:25) (s) as well as for the action-value function
Q(cid:25) (s; a). However, because Ea(cid:24)(cid:25) [w> (s; a)] = Pa2A (cid:25)(ajs)w> (s; a) = 0; 8s 2 S , it is
better to think of w> (s; a) as an approximation of the advantage function rather than of the
action-value function.
The TD error (cid:14)t is a random quantity that is de(cid:2)ned according to (cid:14)t = rt+1(cid:0) ^Jt+1+ ^V (st+1 )(cid:0) ^V (st ),
where ^V and ^J are consistent estimates of the state-value function and the average reward, respec-
tively. Thus, these estimates satisfy E[ ^V (st )jst ; (cid:25) ] = V (cid:25) (st ) and E[ ^Jt+1 jst ; (cid:25) ] = J ((cid:25)), for any
t (cid:21) 0. The next lemma shows that (cid:14)t is a consistent estimate of the advantage function A(cid:25) .

Lemma 3 Under given policy (cid:25) , we have E[(cid:14)t jst ; at ; (cid:25) ] = A(cid:25) (st ; at ).
Proof Note that
E[(cid:14)t jst ; at ; (cid:25) ] = E[rt+1(cid:0) ^Jt+1+ ^V (st+1 )(cid:0) ^V (st )jst ; at ; (cid:25) ] = r(st ; at )(cid:0)J ((cid:25))+E[ ^V (st+1 )jst ; at ; (cid:25) ](cid:0)V (cid:25) (st ):
Now
E[ ^V (st+1 )jst ; at ; (cid:25) ] = E[E[ ^V (st+1 )jst+1 ; (cid:25) ]jst ; at ; (cid:25) ] = E[V (cid:25) (st+1 )jst ; at ] = Xst+1 2S
p(st+1 jst ; at )V (cid:25) (st+1 ):
Also r(st ; at ) (cid:0) J ((cid:25)) + Pst+12S p(st+1 jst ; at )V (cid:25) (st+1 ) = Q(cid:25) (st ; at ). The claim follows.
(cid:3)
By setting the baseline b(s) equal
to the value function V (cid:25) (s), Eq. 3 can be written as
rJ ((cid:25)) = Ps2S d(cid:25) (s) Pa2A (cid:25)(ajs)  (s; a)A(cid:25) (s; a). From Lemma 3, (cid:14)t is a consistent estimate
of the advantage function A(cid:25) (s; a). Thus, drJ ((cid:25)) = (cid:14)t (st ; at ) is a consistent estimate of rJ ((cid:25)).
However, calculating (cid:14)t requires having estimates, ^J , ^V , of the average reward and the value func-
tion. While an average reward estimate is simple enough to obtain given the single-stage reward
function, the same is not necessarily true for the value function. We use function approximation for
the value function as well. Suppose f (s) is a feature vector for state s. One may then approximate
V (cid:25) (s) with v>f (s), where v is a parameter vector that can be tuned (for a (cid:2)xed policy (cid:25) ) using a
TD algorithm. In our algorithms, we use (cid:14)t = rt+1 (cid:0) ^Jt+1 + v>
t f (st ) as an estimate
t f (st+1 ) (cid:0) v>
for the TD error, where v t corresponds to the value function parameter at time t.

4

Let (cid:22)V (cid:25) (s) = Pa2A (cid:25)(ajs)[r(s; a) (cid:0) J ((cid:25)) + Ps02S p(s0 js; a)v(cid:25)>f (s0 )], where v(cid:25)>f (s0 ) is an
estimate of the value function V (cid:25) (s0 ) that is obtained upon convergence viz., limt!1 v t = v(cid:25) with
probability one. Also, let (cid:14) (cid:25)
t = rt+1 (cid:0) ^Jt+1 + v(cid:25)>f (st+1 ) (cid:0) v(cid:25)>f (st ), where (cid:14)(cid:25)
t corresponds to
a stationary estimate of the TD error with function approximation under policy (cid:25) .
t  (st ; at )j(cid:18) ] = rJ ((cid:25)) + Ps2S d(cid:25) (s)[r (cid:22)V (cid:25) (s) (cid:0) rv(cid:25)>f (s)].
Lemma 4 E[(cid:14)(cid:25)
Proof of this lemma can be found in the extended version of this paper (Bhatnagar et al., 2007).
Note that E[(cid:14)t (st ; at )j(cid:18) ] = rJ ((cid:25)), provided (cid:14)t is de(cid:2)ned as (cid:14)t = rt+1 (cid:0) ^Jt+1 + ^V (st+1 ) (cid:0) ^V (st )
(as was considered in Lemma 3). For the case with function approximation that we study, from
Lemma 4, the quantity Ps2S d(cid:25) (s)[r (cid:22)V (cid:25) (s) (cid:0) rv(cid:25)>f (s)] may be viewed as the error or bias in
the estimate of the gradient of average reward that results from the use of function approximation.
4 Actor-Critic Algorithms
We present four new AC algorithms in this section. These algorithms are in the general form shown
in Table 1. They update the policy parameters along the direction of the average-reward gradient.
While estimates of the regular gradient are used for this purpose in Algorithm 1, natural gradient
estimates are used in Algorithms 2(cid:150)4. While critic updates in our algorithms can be easily extended
to the case of TD((cid:21)), (cid:21) > 0, we restrict our attention to the case when (cid:21) = 0. In addition to
assumptions (B1) and (B2), we make the following assumption:

(B3) The step-size schedules for the critic f(cid:11)t g and the actor f(cid:12)t g satisfy
t ; Xt
(cid:12)t = 1 ; Xt
(cid:11)t = Xt
Xt
(cid:11)2
As a consequence of Eq. 9, (cid:12)t ! 0 faster than (cid:11)t . Hence the critic has uniformly higher increments
than the actor beyond some t0 , and thus it converges faster than the actor.

(cid:12) 2
t < 1 ;

= 0:

lim
t!1

(cid:12)t
(cid:11)t

(9)

Table 1: A Template for Incremental AC Algorithms.

^Jt+1 = (1 (cid:0) (cid:24)t ) ^Jt + (cid:24)t rt+1
(cid:14)t = rt+1 (cid:0) ^Jt+1 + v>
t f (st+1 ) (cid:0) v>
t f (st )
algorithm speci(cid:2)c (see the text)
algorithm speci(cid:2)c (see the text)

1:

2:

3:
4:

5:
6:
7:
8:
9 :
10:

Input:
(cid:15) Randomized parameterized policy (cid:25)((cid:1)j(cid:1); (cid:18)),
(cid:15) Value function feature vector f (s).
Initialization:
(cid:15) Policy parameters (cid:18) = (cid:18) 0 ,
(cid:15) Value function weight vector v = v 0 ,
(cid:15) Step sizes (cid:11) = (cid:11)0 ; (cid:12) = (cid:12)0 ; (cid:24) = c(cid:11)0 ,
(cid:15) Initial state s0 .
for t = 0; 1; 2; : : : do
Execution:
(cid:15) Draw action at (cid:24) (cid:25)(at jst ; (cid:18) t ),
(cid:15) Observe next state st+1 (cid:24) p(st+1 jst ; at ),
(cid:15) Observe reward rt+1 .
Average Reward Update:
TD error:
Critic Update:
Actor Update:
endfor
return Policy and value-function parameters (cid:18) ; v

We now present the critic and the actor updates of our four AC algorithms.

Algorithm 1 (Regular-Gradient AC):

Critic Update:
Actor Update:

v t+1 = v t + (cid:11)t (cid:14)tf (st );
(cid:18) t+1 = (cid:18) t + (cid:12)t (cid:14)t (st ; at ):

5

This is the only AC algorithm presented in the paper that is based on the regular gradient estimate.
This algorithm stores two parameter vectors (cid:18) and v . Its per time-step computational cost is linear
in the number of policy and value-function parameters.
The next algorithm is based on the natural-gradient estimate ~rJ ((cid:18) t ) = G(cid:0)1 ((cid:18) t )(cid:14)t (st ; at ) in
place of the regular-gradient estimate in Algorithm 1. We derive a procedure for recursively esti-
mating G(cid:0)1 ((cid:18)) and show in Lemma 5 that our estimate G(cid:0)1
converges to G(cid:0)1 ((cid:18)) as t ! 1 with
t
probability one. This is required for proving convergence of this algorithm. The Fisher information
t+1 Pt
matrix can be estimated in an online manner as Gt+1 = 1
i=0  (si ; ai ) > (si ; ai ). One may
t+1  (st ; at ) > (st ; at ), or more generally
obtain recursively Gt+1 = (1 (cid:0) 1
t+1 )Gt + 1
Gt+1 = (1 (cid:0) (cid:16)t )Gt + (cid:16)t (st ; at ) > (st ; at ):
Using the Sherman-Morrison matrix inversion lemma, one obtains
1 (cid:0) (cid:16)t (cid:20)G(cid:0)1
t  (st ; at ) (cid:21)
G(cid:0)1
t  (st ; at )(G(cid:0)1
t  (st ; at ))>
1
t (cid:0) (cid:16)t
1 (cid:0) (cid:16)t + (cid:16)t (st ; at )>G(cid:0)1
For our Alg. 2 and 4, we require the following additional assumption for the convergence analysis:

G(cid:0)1
t+1 =

(10)

(11)

(B4) The iterates Gt and G(cid:0)1
t

satisfy supt;(cid:18) ;s;a k Gt k and supt;(cid:18) ;s;a k G(cid:0)1
t

k< 1.

Lemma 5 For any given parameter (cid:18) ; G(cid:0)1
t
probability one.

in Eq. 11 satis ﬁes G(cid:0)1
t ! G(cid:0)1 ((cid:18)) as t ! 1 with

It is easy to see from Eq. 10 that Gt ! G((cid:18)) as t ! 1 with probability one, for
Proof
any given (cid:18) held (cid:2)xed. For a (cid:2)xed (cid:18) ,

t (cid:0) I ) k=k G(cid:0)1 ((cid:18))(G((cid:18)) (cid:0) Gt )G(cid:0)1
t (cid:0)G(cid:0)1 ((cid:18)) k=k G(cid:0)1 ((cid:18))(G((cid:18))G(cid:0)1
k G(cid:0)1
t
as
k G(cid:0)1
k G(cid:0)1 ((cid:18)) k sup
sup
k (cid:1) k G((cid:18)) (cid:0) Gt k! 0
t ! 1
t
(cid:18)
t;(cid:18) ;s;a
by assumption (B4). The claim follows.

k(cid:20)

(cid:3)

Our second algorithm stores a matrix G(cid:0)1 and two parameter vectors (cid:18) and v .
Its per time-
step computational cost is linear in the number of value-function parameters and quadratic in the
number of policy parameters.

Algorithm 2 (Natural-Gradient AC with Fisher Information Matrix):

Critic Update:
v t+1 = v t + (cid:11)t (cid:14)tf (st );
(cid:18) t+1 = (cid:18) t + (cid:12)tG(cid:0)1
Actor Update:
t+1 (cid:14)t (st ; at );
with the estimate of the inverse Fisher information matrix updated according to Eq. 11. We let
0 = kI , where k is a positive constant. Thus G(cid:0)1
and G0 are positive de(cid:2)nite and symmetric
G(cid:0)1
0
matrices. From Eq. 10, Gt ; t > 0 can be seen to be positive de(cid:2)nite and symmetric because these
are convex combinations of positive de(cid:2)nite and symmetric matrices. Hence, G(cid:0)1
; t > 0, are
t
positive de(cid:2)nite and symmetric as well.
As mentioned in Section 3, it is better to think of the compatible approximation w> (s; a)
as an approximation of the advantage function rather than of the action-value function.
In our
next algorithm we tune the parameters w in such a way as to minimize an estimate of the
least-squared error E (cid:25) (w) = Es(cid:24)d(cid:25) ;a(cid:24)(cid:25) [(w> (s; a) (cid:0) A(cid:25) (s; a))2 ]. The gradient of E (cid:25) (w)
is thus rw E (cid:25) (w) = 2Es(cid:24)d(cid:25) ;a(cid:24)(cid:25) [(w> (s; a) (cid:0) A(cid:25) (s; a))  (s; a)], which can be estimated as
\rw E (cid:25) (w) = 2[  (st ; at ) (st ; at )>w (cid:0) (cid:14)t (st ; at )]. Hence, we update advantage parameters w
along with value-function parameters v in the critic update of this algorithm. As with Peters et al.
(2005), we use the natural gradient estimate ~rJ ((cid:18) t ) = wt+1 in the actor update of Alg. 3. This
algorithm stores three parameter vectors, v , w , and (cid:18) . Its per time-step computational cost is linear
in the number of value-function parameters and quadratic in the number of policy parameters.

6

Algorithm 3 (Natural-Gradient AC with Advantage Parameters):
Critic Update:

v t+1 = v t + (cid:11)t (cid:14)tf (st );
wt+1 = [I (cid:0) (cid:11)t (st ; at ) (st ; at )> ]wt + (cid:11)t (cid:14)t (st ; at );
Actor Update:
(cid:18) t+1 = (cid:18) t + (cid:12)twt+1 :
Although an estimate of G(cid:0)1 ((cid:18)) is not explicitly computed and used in Algorithm 3, the con-
vergence analysis of this algorithm shows that the overall scheme still moves in the direction of
the natural gradient of average reward. In Algorithm 4, however, we explicitly estimate G(cid:0)1 ((cid:18))
(as in Algorithm 2), and use it in the critic update for w . The overall scheme is again seen
to follow the direction of the natural gradient of average reward. Here, we let ~rw E (cid:25) (w) =
[ (st ; at ) (st ; at )>w (cid:0) (cid:14)t (st ; at )] be the estimate of the natural gradient of the least-
2G(cid:0)1
t
squared error E (cid:25) (w). This also simpli(cid:2)es the critic update for w . Algorithm 4 stores a matrix G(cid:0)1
and three parameter vectors, v , w , and (cid:18) . Its per time-step computational cost is linear in the num-
ber of value-function parameters and quadratic in the number of policy parameters.
Algorithm 4 (Natural-Gradient AC with Advantage Parameters and Fisher Information Matrix):
Critic Update:

v t+1 = v t + (cid:11)t (cid:14)tf (st );
wt+1 = (1 (cid:0) (cid:11)t )wt + (cid:11)tG(cid:0)1
t+1 (cid:14)t (st ; at );
Actor Update:
(cid:18) t+1 = (cid:18) t + (cid:12)twt+1 ;
where the estimate of the inverse Fisher information matrix is updated according to Eq. 11.
5 Convergence of Our Actor-Critic Algorithms
Since our algorithms are gradient-based, one cannot expect to prove convergence to a globally
optimal policy. The best that one could hope for is convergence to a local maximum of J ((cid:18)).
However, because the critic will generally converge to an approximation of the desired projection of
the value function (de(cid:2)ned by the value function features f ) in these algorithms, the corresponding
convergence results are necessarily weaker, as indicated by the following theorem.

Theorem 1 For the parameter iterations in Algorithms 1-4,5 we have ( ^Jt ; v t ; (cid:18) t ) !
f(J ((cid:18)(cid:3) ); v(cid:18)(cid:3)
; (cid:18)(cid:3) )j(cid:18)(cid:3) 2 Z g as t ! 1 with probability one, where the set Z corresponds to
the set of local maxima of a performance function whose gradient is E[(cid:14) (cid:25)
t  (st ; at )j(cid:18) ] (cf. Lemma 4).

For the proof of this theorem, please refer to Section 6 (Convergence Analysis) of the ex-
tended version of this paper (Bhatnagar et al., 2007). This theorem indicates that the policy
and state-value-function parameters converge to a local maximum of a performance function
that corresponds to the average reward plus a measure of the TD error inherent in the function
approximation.
6 Relation to Previous Algorithms
Actor-Critic Algorithm of Konda and Tsitsiklis (2000): Unlike our Alg. 2(cid:150)4, their algorithm
does not use estimates of the natural gradient in its actor’s update. Their algorithm is similar to
our Alg. 1, but with some key differences. 1) Konda’s algorithm uses the Markov process of state(cid:150)
action pairs, and thus its critic update is based on an action-value function. Alg. 1 uses the state
process, and therefore its critic update is based on a state-value function. 2) Whereas Alg. 1 uses
a TD error in both critic and actor recursions, Konda’s algorithm uses a TD error only in its critic
update. The actor recursion in Konda’s algorithm uses an action-value estimate instead. Because
the TD error is a consistent estimate of the advantage function (Lemma 3), the actor recursion in
Alg. 1 uses estimates of advantages instead of action-values, which may result in lower variances.
3) The convergence analysis of Konda’s algorithm is based on the martingale approach and aims at
bounding error terms and directly showing convergence; convergence to a local optimum is shown
when a TD(1) critic is used. For the case where (cid:21) < 1, they show that given an (cid:15) > 0, there exists
(cid:21) close enough to one such that when a TD((cid:21)) critic is used, one gets lim inf t jrJ ((cid:18) t )j < (cid:15) with
5The proof of this theorem requires another assumption viz., (A3) in the extended version of this paper
(Bhatnagar et al., 2007), in addition to (B1)-(B3) (resp. (B1)-(B4)) for Algorithm 1 and 3 (resp. for Algorithm 2
and 4). This was not included in this paper due to space limitations.

7

probability one. Unlike Konda and Tsitsiklis, we primarily use the ordinary differential equation
(ODE) based approach for our convergence analysis. Though we use martingale arguments in our
analysis, these are restricted to showing that the noise terms asymptotically diminish; the resulting
scheme can be viewed as an Euler-discretization of the associated ODE.
Natural Actor-Critic Algorithm of Peters et al. (2005): Our Algorithms 2(cid:150)4 extend their algo-
rithm by being fully incremental and in that we provide convergence proofs. Peters’s algorithm uses
a least-squares TD method in its critic’s update, whereas all our algorithms are fully incremental.
It is not clear how to satisfactorily incorporate least-squares TD methods in a context in which the
policy is changing, and our proof techniques do not immediately extend to this case.
7 Conclusions and Future Work
We have introduced and analyzed four AC algorithms utilizing both linear function approximation
and bootstrapping, a combination which seems essential to large-scale applications of reinforcement
learning. All of the algorithms are based on existing ideas such as TD-learning, natural policy gradi-
ents, and two-timescale stochastic approximation, but combined in new ways. The main contribution
of this paper is proving convergence of the algorithms to a local maximum in the space of policy
and value-function parameters. Our Alg. 2(cid:150)4 are explorations of the use of natural gradients within
an AC architecture. The way we use natural gradients is distinctive in that it is totally incremental:
the policy is changed on every time step, yet the gradient computation is never reset as it is in the
algorithm of Peters et al. (2005). Alg. 3 is perhaps the most interesting of the three natural-gradient
algorithms. It never explicitly stores an estimate of the inverse Fisher information matrix and, as
a result, it requires less computation. In empirical experiments using our algorithms (not reported
here) we observed that it is easier to (cid:2)nd good parameter settings for Alg. 3 than it is for the other
natural-gradient algorithms and, perhaps because of this, it converged more rapidly than the others
and than Konda’s algorithm. All our algorithms performed better than Konda’s algorithm.
There are a number of ways in which our results are limited and suggest future work. 1) It is
important to characterize the quality of the converged solutions, either by bounding the performance
loss due to bootstrapping and approximation error, or through a thorough empirical study. 2) The
algorithms can be extended to incorporate eligibility traces and least-squares methods. As discussed
earlier, the former seems straightforward whereas the latter requires more fundamental extensions.
3) Application of the algorithms to real-world problems is needed to assess their ultimate utility.
References
Bagnell, J., & Schneider, J. (2003). Covariant policy search. Proceedings of the Eighteenth International Joint
Conference on Artiﬁcial Intelligence.
Barto, A. G., Sutton, R. S., & Anderson, C. (1983). Neuron-like elements that can solve dif(cid:2)cult learning
control problems. IEEE Transaction on Systems, Man and Cybernetics, 13, 835(cid:150)846.
Baxter, J., & Bartlett, P. (2001). In(cid:2)nite-horizon policy-gradient estimation. JAIR, 15, 319(cid:150)350.
Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., & Lee, M. (2007). Natural actor-critic algorithms. Submitted
to Automatica.
Greensmith, E., Bartlett, P., & Baxter, J. (2004). Variance reduction techniques for gradient estimates in rein-
forcement learning. Journal of Machine Learning Research, 5, 1471(cid:150)1530.
Kakade, S. (2002). A natural policy gradient. Proceedings of NIPS 14.
Konda, V., & Tsitsiklis, J. (2000). Actor-critic algorithms. Proceedings of NIPS 12 (pp. 1008(cid:150)1014).
Marbach, P. (1998). Simulated-based methods for Markov decision processes. Doctoral dissertation, MIT.
Peters, J., Vijayakumar, S., & Schaal, S. (2003). Reinforcement learning for humanoid robotics. Proceedings
of the Third IEEE-RAS International Conference on Humanoid Robots.
Peters, J., Vijayakumar, S., & Schaal, S. (2005). Natural actor-critic. Proceedings of the Sixteenth European
Conference on Machine Learning (pp. 280(cid:150)291).
Sutton, R. S. (1984). Temporal credit assignment in reinforcement learning. Doctoral dissertation, UMass
Amherst.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3, 9(cid:150)44.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An introduction. MIT Press.
Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy gradient methods for reinforcement
learning with function approximation. Proceedings of NIPS 12 (pp. 1057(cid:150)1063).

8


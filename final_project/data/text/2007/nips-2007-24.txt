FilterBoost: Regression and Classiﬁcation on Large
Datasets

Joseph K. Bradley
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213
jkbradle@cs.cmu.edu

Robert E. Schapire
Department of Computer Science
Princeton University
Princeton, NJ 08540
schapire@cs.princeton.edu

Abstract

We study boosting in the ﬁltering setting, where the booster draws examples from
an oracle instead of using a ﬁxed training set and so may train efﬁciently on very
large datasets. Our algorithm, which is based on a logistic regression technique
proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve
bounds equivalent to or better than previous work. Moreover, we give the ﬁrst
proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the
ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical proper-
ties for both classiﬁcation and conditional probability estimation, and we validate
these results through extensive experiments. Empirically, our algorithm proves
more robust to noise and overﬁtting than batch boosters in conditional probability
estimation and proves competitive in classiﬁcation.

1

Introduction

Boosting provides a ready method for improving existing learning algorithms for classiﬁcation.
Taking a weaker learner as input, boosters use the weak learner to generate weak hypotheses which
are combined into a classiﬁcation rule more accurate than the weak hypotheses themselves. Boosters
such as AdaBoost [1] have shown considerable success in practice.
Most boosters are designed for the batch setting where the learner trains on a ﬁxed example set.
This setting is reasonable for many applications, yet it requires collecting all examples before train-
ing. Moreover, most batch boosters maintain distributions over the entire training set, making them
computationally costly for very large datasets. To make boosting feasible on larger datasets, learners
can be designed for the ﬁltering setting. The batch setting provides the learner with a ﬁxed training
set, but the ﬁltering setting provides an oracle which can produce an unlimited number of labeled
examples, one at a time. This idealized model may describe learning problems with on-line example
sources, including very large datasets which must be loaded piecemeal into memory. By using new
training examples each round, ﬁltering boosters avoid maintaining a distribution over a training set
and so may use large datasets much more efﬁciently than batch boosters.
The ﬁrst polynomial-time booster, by Schapire, was designed for ﬁltering [2]. Later ﬁltering boosters
included two more efﬁcient ones proposed by Freund, but both are non-adaptive, requiring a priori
bounds on weak hypothesis error rates and combining weak hypotheses via unweighted majority
votes [3,4]. Domingo & Watanabe’s MadaBoost is competitive with AdaBoost empirically but the-
oretically requires weak hypotheses’ error rates to be monotonically increasing, an assumption we
found to be violated often in practice [5]. Bshouty & Gavinsky proposed another, but, like Freund’s,
their algorithm requires an a priori bound on weak hypothesis error rates [6]. Gavinsky’s AdaFlatﬁlt
algorithm and Hatano’s GiniBoost do not have these limitations, but the former has worse bounds
than other adaptive algorithms while the latter explicitly requires ﬁnite weak hypothesis spaces [7,8].

1

This paper presents FilterBoost, an adaptive boosting-by-ﬁltering algorithm. We show it is appli-
cable to both conditional probability estimation, where the learner predicts the probability of each
label given an example, and classiﬁcation. In Section 2, we describe the algorithm, after which
we interpret it as a stepwise method for ﬁtting an additive logistic regression model for conditional
probabilities. We then bound the number of rounds and examples required to achieve any target
error in (0, 1). These bounds match or improve upon those for previous ﬁltering boosters but require
fewer assumptions. We also show that FilterBoost can use the conﬁdence-rated predictions from
weak hypotheses described by Schapire & Singer [9].
In Section 3, we give results from extensive experiments. For conditional probability estimation, we
show that FilterBoost often outperforms batch boosters, which prove less robust to overﬁtting. For
classiﬁcation, we show that ﬁltering boosters’ efﬁciency on large datasets allows them to achieve
higher accuracies faster than batch boosters in many cases.
FilterBoost is based on a modiﬁcation of AdaBoost by Collins, Schapire & Singer designed to min-
imize logistic loss [10]. Their batch algorithm has yet to be shown to achieve arbitrarily low test
error, but we use techniques similar to those of MadaBoost to adapt the algorithm to the ﬁltering set-
ting and prove generalization bounds. The result is an adaptive algorithm with realistic assumptions
and strong theoretical properties. Its robustness and efﬁciency on large datasets make it competitive
with existing methods for both conditional probability estimation and classiﬁcation.

2 The FilterBoost Algorithm

Let X be the set of examples and Y a discrete set of labels. For simplicity, assume X is count-
able, and consider only binary labels Y = {−1, +1}. We assume there exists an unknown target
distribution D over labeled examples (x, y) ∈ X × Y from which training and test examples are
generated. The goal in classiﬁcation is to choose a hypothesis h : X → Y which minimizes the
classiﬁcation error PrD [h(x) 6= y ], where the subscript indicates that the probability is with respect
to (x, y) sampled randomly from D .
In the batch setting, a booster is given a ﬁxed training set S and a weak learner which, given any
distribution Dt over training examples S , is guaranteed to return a weak hypothesis ht : X → R
such that the error t ≡ PrDt [sign(ht (x)) 6= y ] < 1/2. For T rounds t, the booster builds a
hypothesis H which is a linear combination of the weak hypotheses (e.g. H (x) = P
distribution Dt over S , runs the weak learner on S and Dt , and receives ht . The booster usually then
estimates t using S and weights ht with αt = αt (t ). After T rounds, the booster outputs a ﬁnal
t αtht (x)).
The sign of H (x) indicates the predicted label ˆy for x.
Two key elements of boosting are constructing Dt over S and weighting weak hypotheses. Dt is
built such that misclassiﬁed examples receive higher weights than in Dt−1 , eventually forcing the
weak learner to classify previously poorly classiﬁed examples correctly. Weak hypotheses ht are
generally weighted such that hypotheses with lower errors receive higher weights.

2.1 Boosting-by-Filtering

We describe a general framework for boosting-by-ﬁltering which includes most existing algorithms
as well as our algorithm Filterboost. The ﬁltering setting assumes the learner has access to an
example oracle, allowing it to use entirely new examples sampled i.i.d. from D on each round.
However, while maintaining the distribution Dt is straightforward in the batch setting, there is no
ﬁxed set S on which to deﬁne Dt in ﬁltering. Instead, the booster simulates examples drawn from
Dt by drawing examples from D via the oracle and reweighting them according to Dt . Filtering
boosters generally accept each example (x, y) from the oracle for training on round t with probability
proportional to the example’s weight Dt (x, y). The mechanism which accepts examples from the
oracle with some probability is called the ﬁlter.
Thus, on each round, a boosting-by-ﬁltering algorithm draws a set of examples from Dt via the
ﬁlter, trains the weak learner on this set, and receives a weak hypothesis ht . Though a batch booster
would estimate t using the ﬁxed set S , ﬁltering boosters may use new examples from the ﬁlter.
Like batch boosters, ﬁltering boosters may weight ht using αt = αt (t ), and they output a linear
combination of h1 , . . . , hT as a ﬁnal hypothesis.

2

The ﬁltering setting allows the learner
to estimate the error of Ht to arbitrary
precision by sampling from D via the
oracle, so FilterBoost does this to de-
cide when to stop boosting.

2.2 FilterBoost

Deﬁne Ft (x) ≡ Pt−1
t0=1 αt0 ht0 (x)
Algorithm F ilterB oost accepts Oracle(), ε, δ , τ :
For t = 1, 2, 3, . . .
δt ←− δ
3t(t+1)
Call F ilter(t, δt , ε) to get
(cid:17)
(cid:16) 1/2+ˆγ 0
mt examples to train WL; get ht
t ←− getE dge(t, τ , δt , ε)
ˆγ 0
(cid:16)
(cid:17)
αt ←− 1
2 ln
1/2−ˆγ 0
t
t
Deﬁne Ht (x) = sign
Ft+1 (x)
(Algorithm exits from F ilter() function.)
Function F ilter(t, δt , ε) returns (x, y)
Deﬁne r = # calls to Filter so far on round t
t ←− δt
δ 0
r(r+1)
); i = i + 1):
For (i = 0; i < 2
ε ln( 1
δ 0
t
(x, y) ←− Oracle()
qt (x, y) ←−
1
1+eyFt (x)
Return (x, y) with probability qt (x, y)
End algorithm; return Ht−1
Function getE dge(t, τ , δt , ε) returns ˆγ 0
t
Let m ←− 0, n ←− 0, u ←− 0, α ←− ∞
While (|u| < α(1 + 1/τ )):
(x, y) ←− F ilter(t, δt , ε)
n ←− n + 1
m ←− m + I (ht (x) = y)
α ←− p(1/2n) ln(n(n + 1)/δt )
u ←− m/n − 1/2
Return u/(1 + τ )

FilterBoost, given in Figure 1, is mod-
eled after the aforementioned algo-
rithm by Collins et al. [10] and Mada-
Boost [5]. Given an example oracle,
weak learner, target error ε ∈ (0, 1),
and conﬁdence parameter δ ∈ (0, 1)
upper-bounding the probability of fail-
ure, it iterates until the current com-
bined hypothesis Ht has error ≤ ε.
On round t, FilterBoost draws mt ex-
amples from the ﬁlter to train the weak
learner and get ht . The number mt
must be large enough to ensure ht has
error t < 1/2 with high probabil-
ity. The edge of ht is γt = 1/2 − t ,
and this edge is estimated by the func-
tion getE dge(), discussed below, and
Ht = sign(Pt
is used to set ht ’s weight αt . The cur-
rent combined hypothesis is deﬁned as
t0=1 αt0 ht0 ).
The F ilter() function generates (x, y)
from Dt by repeatedly drawing (x, y)
from the oracle, calculating the weight
qt (x, y) ∝ Dt (x, y), and accepting
(x, y) with probability qt (x, y).
Function getE dge() uses a modiﬁca-
tion of the Nonmonotonic Adaptive
Sampling method of Watanabe [11]
and Domingo, Galvad `a & Watanabe
[12]. Their algorithm draws an adap-
tively chosen number of examples from
the ﬁlter and returns an estimate ˆγt of the edge of ht within relative error τ of the true edge γt with
high probability. The getE dge() function revises this estimate as ˆγ 0
t = ˆγt/(1 + τ ).

Figure 1: The algorithm FilterBoost.

log

2.3 Analysis: Conditional Probability Estimation
We begin our analysis of FilterBoost by interpreting it as an additive model for logistic regression,
= X
for this interpretation will later aid in the analysis for classiﬁcation. Such models take the form
Pr[y = 1|x]
1
Pr[y = 1|x] =
ft (x) = F (x),
which implies
Pr[y = −1|x]
1 + e−F (x)
t
(cid:20)
(cid:21)
where, for FilterBoost, ft (x) = αtht (x). Dropping subscripts, we can write the expected negative
1 + e−y(F (x)+αh(x))(cid:17)i
h
(cid:16)
log likelihood of example (x, y) after round t as
1
− ln
1 + e−y(F (x)+αh(x))
Taking a similar approach to the analysis of AdaBoost in [13], we show in the following theorem
that FilterBoost performs an approximate stepwise minimization of this negative log likelihood. The
proof is in the Appendix.

π(Ft + αtht ) = π(F + αh) = E

= E

ln

.

3

Theorem 1 Deﬁne the expected negative log likelihood π(F + αh) as above. Given F , FilterBoost
chooses h to minimize a second-order Taylor expansion of π around h = 0. Given this h, it then
chooses α to minimize an upper bound of π .

The batch booster given by Collins et al. [10] which FilterBoost is based upon is guaranteed to
converge to the minimum of this objective when working over a ﬁnite sample. Note that Filter-
Boost uses weak learners which are simple classiﬁers to perform regression. AdaBoost too may
be interpreted as an additive logistic regression model of the form Pr[y = 1|x] =
1+e−2F (x) with
1
E[exp(−yF (x))] as the optimization objective [13].

2.4 Analysis: Classiﬁcation

In this section, we interpret FilterBoost as a traditional boosting algorithm for classiﬁcation and
prove bounds on its generalization error. We ﬁrst give a theorem relating errt , the error rate of Ht
over the target distribution D , to pt , the probability with which the ﬁlter accepts a random example
generated by the oracle on round t.
Theorem 2 Let errt = PrD [Ht (x) 6= y ], and let pt = ED [qt (x, y)]. Then errt ≤ 2pt .
Proof:

errt = PrD [Ht (x) 6= y ] = PrD [yFt−1 (x) ≤ 0]
= PrD [qt (x, y) ≥ 1/2] ≤ 2 · ED [qt (x, y)]
= 2pt (using Markov’s inequality above)

(cid:4)

t, we can write πt = − P
We next use the expected negative log likelihood π from Section 2.3 as an auxiliary function to aid
in bounding the required number of boosting rounds. Viewing π as a function of the boosting round
(x,y) D(x, y) ln(1 − qt (x, y)). Our goal is then to minimize πt , and the
following lemma captures the learner’s progress in terms of the decrease in πt on each round. This
lemma assumes edge estimates returned by getE dge() are exact, i.e. ˆγ 0
t = γt , which leads to a
simpler bound on T in Theorem 3. We then consider the error in edge estimates and give a revised
bound in Lemma 2 and Theorem 5. The proofs of Lemmas 1 and 2 are in the Appendix.
− P
6= 0 and γt
Lemma 1 Assume for all t that γt
is estimated exactly.
(cid:19)
(cid:18)
q
(x,y) D(x, y) ln(1 − qt (x, y)). Then
1/4 − γ 2
πt − πt+1 ≥ pt
t

Let πt =

1 − 2

.

T >

Combining Theorem 2, which bounds the error of the current combined hypothesis in terms of pt ,
with Lemma 1 gives the following upper bound on the required rounds T .
Theorem 3 Let γ = mint |γt |, and let ε be the target error. Given Lemma 1’s assumptions, if
(cid:16)
(cid:17)
1 − 2p1/4 − γ 2
FilterBoost runs
2 ln(2)
ε
rounds, then errt < ε for some t, 1 ≤ t ≤ T . In particular, this is true for T > ln(2)
2εγ 2 .
Proof: For all (x, y), since F1 (x, y) = 0, then q1 (x, y) = 1/2 and π1 = ln(2). Now, suppose
(cid:16)
(cid:17)
1 − 2p1/4 − γ 2
errt ≥ ε, ∀t ∈ {1, ..., T }. Then, from Theorem 2, pt ≥ ε/2, so Lemma 1 gives
πt − πt+1 ≥ 1
Unraveling this recursion as PT
2 ε
t=1 (πt − πt+1 ) = π1 − πT +1 ≤ π1 gives
(cid:17) .
(cid:16)
1 − 2p1/4 − γ 2
2 ln(2)
T ≤
4

ε

So, errt ≥ ε, ∀t ∈ {1, ..., T } is contradicted if T exceeds the theorem’s lower bound. The simpliﬁed
bound follows from the ﬁrst bound via the inequality 1 − √
1 − x ≤ x for x ∈ [0, 1].
(cid:4)
Theorem 3 shows FilterBoost can reduce generalization error to any ε ∈ (0, 1), but we have thus far
overlooked the probabilities of failure introduced by three steps: training the weak learner, deciding
when to stop boosting, and estimating edges. We bound the probability of each of these steps failing
on round t with a conﬁdence parameter δt =
3t(t+1) so that a simple union bound ensures the
δ
probability of some step failing to be at most FilterBoost’s conﬁdence parameter δ . Finally, we
revise Lemma 1 and Theorem 3 to account for error in estimating edges.
The number mt of examples the weak learner trains on must be large enough to ensure weak hy-
pothesis ht has a non-zero edge and should be set according to the choice of weak learner.
To decide when to stop boosting (i.e. when errt ≤ ε), we can use Theorem 2, which upper-bounds
the error of the current combined hypothesis Ht in terms of the probability pt that F ilter() accepts
a random example from the oracle. If the ﬁlter rejects enough examples in a single call, we know pt
is small, so Ht is accurate enough. Theorem 4 formalizes this intuition; the proof is in the Appendix.
Theorem 4 In a single call to F ilter(t), if n examples have been rejected, where n ≥ 2
ε ln(1/δ 0
t ),
then errt ≤ ε with probability at least 1 − δ 0
t .
Theorem 4 provides a stopping condition which is checked on each call to F ilter(). Each check
may fail with probability at most δ 0
r(r+1) on the r th call to F ilter() so that a union bound
t = δt
ensures FilterBoost stops prematurely on round t with probability at most δt . Theorem 4 uses a
similar argument to that used for MadaBoost, giving similar stopping criteria for both algorithms.
We estimate weak hypotheses’ edges γt using the Nonmonotonic Adaptive Sampling (NAS) algo-
rithm [11,12] used by MadaBoost. To compute an estimate ˆγt of the true edge γt within relative error
τ ∈ (0, 1) with probability ≥ 1 − δt , the NAS algorithm uses at most 2(1+2τ )2
ln(
) ﬁltered
1
(τ γt )2
τ γt δt
examples. With this guarantee on edge estimates, we can rewrite Lemma 1 as follows:
πt = − P
Lemma 2 Assume for all t that γt 6= 0 and γt is estimated to within τ ∈ (0, 1) relative error. Let
s
(cid:19)2!
 
(cid:18) 1 − τ
(x,y) D(x, y) ln(1 − qt (x, y)). Then
1 − 2
πt − πt+1 ≥ pt
1 + τ

1/4 − γ 2
t

.

T >

Using Lemma 2, the following theorem modiﬁes Theorem 3 to account for error in edge estimates.
Theorem 5 Let γ = mint |γt |. Let ε be the target error. Given Lemma 2’s assumptions, if Filter-
q
(cid:16)
(cid:17)
Boost runs
2 ln(2)
1/4 − γ 2 ( 1−τ
1 − 2
1+τ )2
ε
rounds, then errt < ε for some t, 1 ≤ t ≤ T .
The bounds from Theorems 3 and 5 show FilterBoost requires at most O(ε−1γ−2 ) boosting rounds.
MadaBoost [5], which we test in our experiments, resembles FilterBoost but uses truncated ex-
ponential weights qt (x, y) = min{1, exp(yFt−1 (x))} instead of the logistic weights qt (x, y) =
(1 + exp(yFt (x)))−1 used by FilterBoost. The algorithms’ analyses differ, with MadaBoost requir-
ing the edges γt to be monotonically decreasing, but both lead to similar bounds on the number of
rounds T proportional to ε−1 . The non-adaptive ﬁltering boosters of Freund [3,4] and of Bshouty
& Gavinsky [6] and the batch booster AdaBoost [1] have smaller bounds on T , proportional to
log(ε−1 ). However, we can use boosting tandems, a technique used by Freund [4] and Gavinsky
[7], to create a ﬁltering booster with T bounded by O(log(ε−1 )γ−2 ). Following Gavinsky, we can
use FilterBoost to boost the accuracy of the weak learner to some constant and, in turn, treat Fil-
terBoost as a weak learner and use an algorithm from Freund to achieve any target error. As with
AdaFlatﬁlt , boosting tandems turn FilterBoost into an adaptive booster with a bound on T pro-
portional to log(ε−1 ). (Without boosting tandems, AdaFlatﬁlt requires T ∝ ε−2 rounds.) Note,
however, that boosting tandems result in more complicated ﬁnal hypotheses.

5

An alternate bound for FilterBoost may be derived using techniques from Shalev-Shwartz & Singer
[14]. They use the framework of convex repeated games to deﬁne a general method for bounding the
performance of online and boosting algorithms. For FilterBoost, their techniques, combined with
Theorem 2, give a bound similar to that in Theorem 3 but proportional to ε−2 instead of ε−1 .
Schapire & Singer [9] show AdaBoost beneﬁts from conﬁdence-rated predictions, where weak hy-
potheses return predictions whose absolute values indicate conﬁdence. These values are chosen
to greedily minimize AdaBoost’s exponential loss function over training data, and this aggressive
P
weighting can result in faster learning. FilterBoost may use conﬁdence-rated predictions in an iden-
tical manner. In the proof of Lemma 1, the decrease in the negative log likelihood πt of the data
(relative to Ht and the target distribution D) is lower-bounded by pt − pt
(x,y) Dt (x, y)e−αt yht (x) .
Since pt is ﬁxed, maximizing this bound is equivalent to minimizing the exponential loss over Dt .

3 Experiments

Vanilla FilterBoost accepts examples (x, y) from the oracle with probability qt (x, y), but it may
instead accept all examples and weight each with qt (x, y). Weighting instead of ﬁltering examples
increases accuracy but also increases the size of the training set passed to the weak learner. For
efﬁciency, we choose to ﬁlter when training the weak learner but weight when estimating edges
γt . We also modify FilterBoost’s getE dge() function for efﬁciency. The Nonmonotonic Adaptive
Sampling (NAS) algorithm used to estimate edges γt uses many examples, but using several orders
of magnitude fewer sacriﬁces little accuracy. The same is true for MadaBoost. In all tests, we use
Cn log(t + 1) examples to estimate γt , where Cn = 300 and the log factor scales the number as
the NAS algorithm would. For simplicity, we train weak learners with Cn log(t + 1) examples as
well. These modiﬁcations mean τ (error in edge estimates) and δ (conﬁdence) have no effect on our
tests. To simulate an oracle, we randomly permute the data and use examples in the new order. In
practice, ﬁltering boosters can achieve higher accuracy by cycling through training sets again instead
of stopping once examples are depleted, and we use this “recycling” in our tests.
We test FilterBoost with and without conﬁdence-rated predictions (labeled “(C-R)” in our results).
We compare FilterBoost against MadaBoost [5], which does not require an a priori bound on weak
hypotheses’ edges and has similar bounds without the complication of boosting tandems. We imple-
ment MadaBoost with the same modiﬁcations as FilterBoost. We test FilterBoost against two batch
boosters: the well-studied and historically successful AdaBoost [1] and the algorithm from Collins
et al. [10] which is essentially a batch version of FilterBoost (labeled “AdaBoost-LOG”). We test
both with and without conﬁdence-rated predictions as well as with and without resampling (labeled
“(resamp)”). In resampling, the booster trains weak learners on small sets of examples sampled from
the distribution Dt over the training set S rather than on the entire set S , and this technique often
increases efﬁciency with little effect on accuracy. Our batch boosters use sets of size Cm log(t + 1)
for training, like the ﬁltering boosters, but use all of S to estimate edges γt since this can be done
efﬁciently. We test the batch boosters using conﬁdence-rated predictions and resampling in order to
compare FilterBoost with batch algorithms optimized for the efﬁciency which boosting-by-ﬁltering
claims as its goal.
We test each booster using decision stumps and decision trees as weak learners to discern the effects
of simple and complicated weak hypotheses. The decision stumps minimize training error, and the
decision trees greedily maximize information gain and are pruned using 1/3 of the data. Both weak
learners minimize exponential loss when outputing conﬁdence-rated predictions.
We use four datasets, described in the Appendix. Brieﬂy, we use two synthetic sets: Majority
(majority vote) and Twonorm [15], and two real sets from the UCI Machine Learning Repository
[16]: Adult (census data; from Ron Kohavi) and Covertype (forestry data with 7 classes merged to
2; Copyr. Jock A. Blackard & Colorado State U.). We average over 10 runs, using new examples
for synthetic data (with 50,000 test examples except where stated) and cross validation for real data.
Figure 2 compares the boosters’ runtimes. As expected, ﬁltering boosters run slower per round than
batch boosters on small datasets but much faster on large ones. Interestingly, ﬁltering boosters take
longer on very small datasets in some cases (not shown), for the probability the ﬁlter accepts an
example quickly shrinks when the booster has seen that example many times.

6

Figure 2: Running times: Ada/Filter/MadaBoost. Majority; WL = stumps.

3.1 Results: Conditional Probability Estimation

In Section 2.3, we discussed the in-
terpretation of FilterBoost and Ada-
Boost as stepwise algorithms for con-
ditional probability estimation. We
test both algorithms and the variants
discussed above on all four datasets.
We do not test MadaBoost, as it is
not clear how to use it to estimate
conditional probabilities. As Figure
3 shows, both FilterBoost variants
are competitive with batch algorithms
when boosting decision stumps. With
decision trees, all algorithms except
for FilterBoost overﬁt badly, includ-
ing FilterBoost(C-R). In each plot,
we compare FilterBoost with the best
of AdaBoost and AdaBoost-LOG:
AdaBoost was best with decision
stumps and AdaBoost-LOG with de-
cisions trees. For comparison, batch
logistic regression via gradient de-
scent achieves RMSE 0.3489 and log
(base e) loss .4259; FilterBoost, inter-
pretable as a stepwise method for logistic regression, seems to be approaching these asymptotically.
On Adult and Twonorm, FilterBoost generally outperforms the batch boosters, which tend to overﬁt
when boosting decision trees, though AdaBoost slightly outperforms FilterBoost on smaller datasets
when boosting decision stumps.
The Covertype dataset is an exception to our results and highlights a danger in ﬁltering and in
resampling for batch learning: the complicated structure of some datasets seems to require decision
trees to train on the entire dataset. With decision stumps, the ﬁltering boosters are competitive,
yet only the non-resampling batch boosters achieve high accuracies with decision trees. The ﬁrst
decision tree trained on the entire training set achieves about 94% accuracy, which is unachievable
by any of the ﬁltering or resampling batch boosters when using Cm = 300 as the base number of
examples for training the weak learner. To compete with non-resampling batch boosters, the other
boosters must use Cm on the order of 105 , by which point they become very inefﬁcient.

Figure 3: Log (base e) loss & root mean squared error
(RMSE). Majority; 10,000 train exs.
Left two: WL = stumps (FilterBoost vs. AdaBoost);
Right two: WL = trees (FilterBoost vs. AdaBoost-LOG).

3.2 Results: Classiﬁcation

Vanilla FilterBoost and MadaBoost perform similarly in classiﬁcation (Figure 4). Conﬁdence-rated
predictions allow FilterBoost to outperform MadaBoost when using decision stumps but sometimes
cause FilterBoost to perform poorly with decision trees. Figure 5 compares FilterBoost with the
best batch booster for each weak learner. With decision stumps, all boosters achieve higher accu-
racies with the larger dataset, on which ﬁltering algorithms are much more efﬁcient. Majority is
represented well as a linear combination of decision stumps, so the boosters all learn more slowly

7

Figure 4: FilterBoost vs. MadaBoost.

Figure 5: FilterBoost vs. AdaBoost & AdaBoost-LOG. Majority.

when using the overly complicated decision trees. However, this problem generally affects ﬁltering
boosters less than most batch variants, especially on larger datasets. Adult and Twonorm gave simi-
lar results. As in Section 3.1, ﬁltering and resampling batch boosters perform poorly on Covertype.
Thus, while FilterBoost is competitive in classiﬁcation, its best performance is in regression.

References

[1] Freund, Y., & Schapire, R. E. (1997) A decision-theoretic generalization of on-line learning and an applica-
tion to boosting. Journal of Computer and System Sciences, 55, 119-139.
[2] Schapire., R. E. (1990) The strength of weak learnability. Machine Learning, 5(2), pp. 197-227.
[3] Freund, Y. (1995) Boosting a weak learning algorithm by majority. Information and Computation, 121, pp.
256-285.
[4] Freund, Y. (1992) An improved boosting algorithm and its implications on learning complexity. 5th Annual
Conference on Computational Learning Theory, pp. 391-398.
[5] Domingo, C., & Watanabe, O. (2000) MadaBoost: a modiﬁcation of AdaBoost. 13th Annual Conference
on Computational Learning Theory, pp. 180-189.
[6] Bshouty, N. H., & Gavinsky, D. (2002) On boosting with polynomially bounded distributions. Journal of
Machine Learning Research, 3, pp. 483-506.
[7] Gavinsky, D. (2003) Optimally-smooth adaptive boosting and application to agnostic learning. Journal of
Machine Learning Research, 4, pp. 101-117.
[8] Hatano, K. (2006) Smooth boosting using an information-based criterion. 17th International Conference
on Algorithmic Learning Theory, pp. 304-319.
[9] Schapire, R. E., & Singer, Y. (1999) Improved boosting algorithms using conﬁdence-rated predictions. Ma-
chine Learning, 37, 297-336.
[10] Collins, M., Schapire, R. E., & Singer, Y. (2002) Logistic regression, AdaBoost and Bregman distances.
Machine Learning, 48, pp. 253-285.
[11] Watanabe, O. (2000) Simple sampling techniques for discovery science. IEICE Trans. Information and
Systems, E83-D(1), 19-26.
[12] Domingo, C., Galvad `a, R., & Watanabe, O. (2002) Adaptive sampling methods for scaling up knowledge
discovery algorithms. Data Mining and Knowledge Discovery, 6, pp. 131-152.
[13] Friedman, J., Hastie, T., & Tibshirani, R. (2000) Additive logistic regression: a statistical view of boosting.
The Annals of Statistics, 28, 337-407.
[14] Shalev-Shwartz, S., & Singer, Y. (2006) Convex repeated games and Fenchel duality. Advances in Neural
Information Processing Systems 20.
[15] Breiman, L. (1998) Arcing classiﬁers. The Annals of Statistics, 26, pp. 801-849.
[16] Newman, D. J., Hettich, S., Blake, C. L., & Merz, C. J. (1998) UCI Repository of machine learning
databases [http://www.ics.uci.edu/∼mlearn/MLRepository.html]. Irvine, CA: U. of California, Dept. of Infor-
mation & Computer Science.

8


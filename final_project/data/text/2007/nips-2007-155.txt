Theoretical Analysis of Heuristic Search Methods for
Online POMDPs

St ´ephane Ross
McGill University
Montr ´eal, Qc, Canada
sross12@cs.mcgill.ca

Joelle Pineau
McGill University
Montr ´eal, Qc, Canada
jpineau@cs.mcgill.ca

Brahim Chaib-draa
Laval University
Qu ´ebec, Qc, Canada
chaib@ift.ulaval.ca

Abstract

Planning in partially observable environments remains a challenging problem, de-
spite signi ﬁcant recent advances in ofﬂine approximation t
echniques. A few on-
line methods have also been proposed recently, and proven to be remarkably scal-
able, but without the theoretical guarantees of their ofﬂin e counterparts. Thus it
seems natural to try to unify ofﬂine and online techniques, p reserving the theo-
retical properties of the former, and exploiting the scalability of the latter. In this
paper, we provide theoretical guarantees on an anytime algorithm for POMDPs
which aims to reduce the error made by approximate ofﬂine val ue iteration algo-
rithms through the use of an efﬁcient online searching proce dure. The algorithm
uses search heuristics based on an error analysis of lookahead search, to guide the
online search towards reachable beliefs with the most potential to reduce error. We
provide a general theorem showing that these search heuristics are admissible, and
lead to complete and ǫ-optimal algorithms. This is, to the best of our knowledge,
the strongest theoretical result available for online POMDP solution methods. We
also provide empirical evidence showing that our approach is also practical, and
can ﬁnd (provably) near-optimal solutions in reasonable ti me.

1

Introduction

Partially Observable Markov Decision Processes (POMDPs) provide a powerful model for sequen-
tial decision making under state uncertainty. However exact solutions are intractable in most do-
mains featuring more than a few dozen actions and observations. Signi ﬁcant efforts have been
devoted to developing approximate ofﬂine algorithms for la rger POMDPs [1, 2, 3, 4]. Most of these
methods compute a policy over the entire belief space. This is both an advantage and a liability.
On the one hand, it allows good generalization to unseen beliefs, and this has been key to solving
relatively large domains. Yet it makes these methods impractical for problems where the state space
is too large to enumerate. A number of compression techniques have been proposed, which han-
dle large state spaces by projecting into a sub-dimensional representation [5, 6]. Alternately online
methods are also available [7, 8, 9, 10, 11]. These achieve scalability by planning only at execution
time, thus allowing the agent to only consider belief states that can be reached over some (small)
ﬁnite planning horizon. However despite good empirical per formance, both classes of approaches
lack theoretical guarantees on the approximation. So it would seem we are constrained to either
solving small to mid-size problems (near-)optimally, or solving large problems possibly badly.

This paper suggests otherwise, arguing that by combining ofﬂine and online techniques, we can
preserve the theoretical properties of the former, while exploiting the scalability of the latter. In
previous work [11], we introduced an anytime algorithm for POMDPs which aims to reduce the
error made by approximate ofﬂine value iteration algorithm s through the use of an efﬁcient online
searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead
search, to guide the online search towards reachable beliefs with the most potential to reduce error. In

this paper, we derive formally the heuristics from our error minimization point of view and provide
theoretical results showing that these search heuristics are admissible, and lead to complete and ǫ-
optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available
for online POMDP solution methods. Furthermore the approach works well with factored state
representations, thus further enhancing scalability, as suggested by earlier work [2]. We also provide
empirical evidence showing that our approach is computationally practical, and can ﬁnd (provably)
near-optimal solutions within a smaller overall time than previous online methods.

2 Background: POMDP

A POMDP is deﬁned by a tuple
(S, A, Ω, T , R, O, γ ) where S is the state space, A is the action
set, Ω is the observation set, T : S × A × S → [0, 1] is the state-to-state transition function,
R : S × A → R is the reward function, O : Ω × A × S → [0, 1] is the observation function,
and γ is the discount factor. In a POMDP, the agent often does not know the current state with full
certainty, since observations provide only a partial indicator of state. To deal with this uncertainty,
the agent maintains a belief state b(s), which expresses the probability that the agent is in each state
at a given time step. After each step, the belief state b is updated using Bayes rule. We denote the
belief update function b′ = τ (b, a, o), deﬁned as b′ (s′ ) = ηO(o, a, s′ ) Ps∈S T (s, a, s′ )b(s), where
η is a normalization constant ensuring Ps∈S b′ (s) = 1.
Solving a POMDP consists in ﬁnding an optimal policy, π∗ : ∆S → A, which speci ﬁes the best
action a to do in every belief state b, that maximizes the expected return (i.e., expected sum of
discounted rewards over the planning horizon) of the agent. We can ﬁnd the optimal policy by
computing the optimal value of a belief state over the planning horizon. For the inﬁnite horizon, the
optimal value function is deﬁned as V ∗ (b) = maxa∈A [R(b, a) + γ Po∈Ω P (o|b, a)V ∗ (τ (b, a, o))],
where R(b, a) represents the expected immediate reward of doing action a in belief state b and
P (o|b, a) is the probability of observing o after doing action a in belief state b. This probability can
be computed according to P (o|b, a) = Ps′∈S O(o, a, s′ ) Ps∈S T (s, a, s′ )b(s). We also denote the
value Q∗ (b, a) of a particular action a in belief state b, as the return we will obtain if we perform a in
b and then follow the optimal policy Q∗ (b, a) = R(b, a) + γ Po∈Ω P (o|b, a)V ∗ (τ (b, a, o)). Using
this, we can deﬁne the optimal policy π∗ (b) = argmaxa∈A Q∗ (b, a).
While any POMDP problem has inﬁnitely many belief states, it h as been shown that the optimal
value function of a ﬁnite-horizon POMDP is piecewise linear and convex. Thus we can deﬁne the
optimal value function and policy of a ﬁnite-horizon POMDP u sing a ﬁnite set of
|S |-dimensional
hyper plans, called α-vectors, over the belief state space. As a result, exact ofﬂ ine value iteration
algorithms are able to compute V ∗ in a ﬁnite amount of time, but the complexity can be very high.
Most approximate ofﬂine value iteration algorithms achieve computational tractability by selecting
a small subset of belief states, and keeping only those α-vectors which are maximal at the selected
belief states [1, 3, 4]. The precision of these algorithms depend on the number of belief points and
their location in the space of beliefs.

3 Online Search in POMDPs

Contrary to ofﬂine approaches, which compute a complete pol
icy determining an action for every
belief state, an online algorithm takes as input the current belief state and returns the single action
which is the best for this particular belief state. The advantage of such an approach is that it only
needs to consider belief states that are reachable from the current belief state. This naturally provides
a small set of beliefs, which could be exploited as in ofﬂine m ethods. But in addition, since online
planning is done at every step (and thus generalization between beliefs is not required), it is sufﬁcient
to calculate only the maximal value for the current belief state, not the full optimal α-vector. A
lookahead search algorithm can compute this value in two simple steps.

First we build a tree of reachable belief states from the current belief state. The current belief is the
top node in the tree. Subsequent belief states (as calculated by the τ (b, a, o) function) are represented
using OR-nodes (at which we must choose an action) and actions are included in between each layer
of belief nodes using AND-nodes (at which we must consider all possible observations). Note that
in general the belief MDP could have a graph structure with cycles. Our algorithm simply handle

if b is a leaf in T ,
otherwise;

such structure by unrolling the graph into a tree. Hence, if we reach a belief that is already elsewhere
in the tree, it will be duplicated.1
Second, we estimate the value of the current belief state by propagating value estimates up from the
fringe nodes, to their ancestors, all the way to the root. An approximate value function is generally
used at the fringe of the tree to approximate the inﬁnite-hor izon value. We are particularly interested
in the case where a lower bound and an upper bound on the value of the fringe belief states is
available, as this allows us to get a bound on the error at any speci ﬁc node. The lower and upper
bounds can be propagated to parent nodes according to:
UT (b) = (cid:26) U (b)
maxa∈A UT (b, a)
UT (b, a) = RB (b, a) + γ Xo∈Ω
LT (b) = (cid:26) L(b)
maxa∈A LT (b, a)
LT (b, a) = RB (b, a) + γ Xo∈Ω
where UT (b) and LT (b) represent the upper and lower bounds on V ∗ (b) associated to belief state
b in the tree T , UT (b, a) and LT (b, a) represent corresponding bounds on Q∗ (b, a), and L(b) and
U (b) are the bounds on fringe nodes, typically computed ofﬂine.
Performing a complete k-step lookahead search multiplies the error bound on the approximate value
function used at the fringe by γ k ([13]), and thus ensures better value estimates. However, it has
complexity exponential in k , and may explore belief states that have very small probabilities of oc-
curring (and an equally small impact on the value function) as well as exploring suboptimal actions
(which have no impact on the value function). We would evidently prefer to have a more efﬁcient
online algorithm, which can guarantee equivalent or better error bounds. In particular, we believe
that the best way to achieve this is to have a search algorithm which uses estimates of error reduction
as a criteria to guide the search over the reachable beliefs.

if b is a leaf in T ,
otherwise;

P (o|b, a)UT (τ (b, a, o));

P (o|b, a)LT (τ (b, a, o));

(1)

(2)

(3)

(4)

4 Anytime Error Minimization Search

In this section, we review the Anytime Error Minimization Search (AEMS) algorithm we had ﬁrst
introduced in [11] and present a novel mathematical derivation of the heuristics that we had sug-
gested. We also provide new theoretical results describing sufﬁcient conditions under which the
heuristics are guaranteed to yield ǫ-optimal solutions.
Our approach uses a best- ﬁrst search of the belief reachabil
ity tree, where error minimization (at the
root node) is used as the search criteria to select which fringe nodes to expand next. Thus we need a
way to express the error on the current belief (i.e. root node) as a function of the error at the fringe
nodes. This is provided in Theorem 1. Let us denote (i) F (T ), the set of fringe nodes of a tree T ; (ii)
eT (b) = V ∗ (b) − LT (b), the error function for node b in the tree T ; (iii) e(b) = V ∗ (b) − L(b), the
error at a fringe node b ∈ F (T ); (iv) hb0 ,b
T , the unique action/observation sequence that leads from
the root b0 to belief b in tree T ; (v) d(h), the depth of an action/observation sequence h (number of
actions); and (vi) P (h|b0 , π∗ ) = Qd(h)
o |bhi−1
a ), the probability of executing
a )π∗ (bhi−1 , hi
, hi
i=1 P (hi
0
the action/observation sequence h if we follow the optimal policy π∗ from the root node b0 (where
o refers to the ith action and observation in the sequence h, and bhi is the belief obtained
a and hi
hi
after taking the i ﬁrst actions and observations from belief
b. π∗ (b, a) is the probability that the
optimal policy chooses action a in belief b).
By abuse of notation, we will use b to represent both a belief node in the tree and its associated
belief2 .
1We are considering using a technique proposed in the LAO* algorithm [12] to handle cycle, but we have
not investigated this fully, especially in terms of how it affects the heuristic value presented below.
2 e.g. Pb∈F (T ) should be interpreted as a sum over all fringe nodes in the tree, while e(b) to be the error
associated to the belief in fringe node b.

Theorem 1. In any tree T , eT (b0 ) ≤ Pb∈F (T ) γ d(h
b0 ,b
)P (hb0 ,b
T |b0 , π∗ )e(b).
T
Proof. Consider an arbitrary parent node b in tree T and let’s denote ˆaT
b = argmaxa∈A LT (b, a). We
b = π∗ (b), then eT (b) = γ Po∈Ω P (o|b, π∗ (b))e(τ (b, π∗ (b), o)).
have eT (b) = V ∗ (b) − LT (b).
If ˆaT
6= π∗ (b), then we know that LT (b, π∗ (b)) ≤ LT (b, ˆaT
On the other hand, when ˆaT
b ) and therefore
eT (b) ≤ γ Po∈Ω P (o|b, π∗ (b))e(τ (b, π∗ (b), o)). Consequently, we have the following:
b
eT (b) ≤ ( e(b)
if b ∈ F (T )
γ Po∈Ω
P (o|b, π∗ (b))eT (τ (b, π∗ (b), o))
otherwise
Then eT (b0 ) ≤ Pb∈F (T ) γ d(h
b0 ,b
)P (hb0 ,b
T
T
4.1 Search Heuristics

|b0 , π∗ )e(b) can be easily shown by induction.

From Theorem 1, we see that the contribution of each fringe node to the error in b0 is simply
b0 ,b
)P (hb0 ,b
the term γ d(h
T |b0 , π∗ )e(b). Consequently, if we want to minimize eT (b0 ) as quickly as
T
possible, we should expand fringe nodes reached by the optimal policy π∗ that maximize the term
b0 ,b
)P (hb0 ,b
T |b0 , π∗ )e(b) as they offer the greatest potential to reduce eT (b0 ). This suggests us
γ d(h
T
a sound heuristic to explore the tree in a best- ﬁrst-search w ay. Unfortunately we do not know V ∗
nor π∗ , which are required to compute the terms e(b) and P (hb0 ,b
T |b0 , π∗ ); nevertheless, we can
approximate them. First, the term e(b) can be estimated by the difference between the lower and
upper bound. We deﬁne ˆe(b) = U (b) − L(b) as an estimate of the error introduced by our bounds at
fringe node b. Clearly, ˆe(b) ≥ e(b) since U (b) ≥ V ∗ (b).

To approximate P (hb0 ,b
T |b0 , π∗ ), we can view the term π∗ (b, a) as the probability that action a
is optimal in belief b. Thus, we consider an approximate policy ˆπT that represents the proba-
bility that action a is optimal in belief state b given the bounds LT (b, a) and UT (b, a) that we
have on Q∗ (b, a) in tree T . More precisely, to compute ˆπT (b, a), we consider Q∗ (b, a) as a
random variable and make some assumptions about its underlying probability distribution. Once
cumulative distribution functions F b,a
T , s.t. F b,a
T (x) = P (Q∗ (b, a) ≤ x), and their associated
density functions f b,a
are determined for each (b, a) in tree T , we can compute the probability
T
ˆπT (b, a) = P (Q∗ (b, a′ ) ≤ Q∗ (b, a)∀a′ 6= a) = R ∞
T (x) Qa′ 6=a F b,a′
−∞ f b,a
(x)dx. Computing this
T
integral may not be computationally efﬁcient depending on h ow we deﬁne the functions f b,a
T . We
consider two approximations.
One possible approximation is to simply compute the probability that the Q-value of a given action
is higher than its parent belief state value (instead of all actions’ Q-value). In this case, we get
ˆπT (b, a) = R ∞
−∞ f b,a
T (x)dx, where F b
T is the cumulative distribution function for V ∗ (b),
T (x)F b
given bounds LT (b) and UT (b) in tree T . Hence by considering both Q∗ (b, a) and V ∗ (b) as random
variables with uniform distributions between their respective lower and upper bounds, we get:
ˆπT (b, a) = ( η (UT (b,a)−LT (b))2
if UT (b, a) > LT (b),
(5)
UT (b,a)−LT (b,a)
otherwise.
0
where η is a normalization constant such that Pa∈A ˆπT (b, a) = 1. Notice that if the density function
is 0 outside the interval between the lower and upper bound, then ˆπT (b, a) = 0 for dominated
actions, thus they are implicitly pruned from the search tree by this method.
A second practical approximation is:
ˆπT (b, a) = (cid:26) 1 if a = argmaxa′∈A UT (b, a′ ),
0 otherwise.
which simply selects the action that maximizes the upper bound. This restricts exploration of the
search tree to those fringe nodes that are reached by sequence of actions that maximize the upper
bound of their parent belief state, as done in the AO∗ algorithm [14]. The nice property of this
approximation is that these fringe nodes are the only nodes that can potentially reduce the upper
bound in b0 .

(6)

Using either of these two approximations for ˆπT , we can estimate the error contribution ˆeT (b0 , b) of
b0 ,b
)P (hb0 ,b
a fringe node b on the value of root belief b0 in tree T , as: ˆeT (b0 , b) = γ d(h
T |b0 , ˆπT )ˆe(b).
T
Using this as a heuristic, the next fringe node eb(T ) to expand in tree T is deﬁned as eb(T ) =
b0 ,b
T |b0 , ˆπT )ˆe(b). We use AEMS13 to denote the heuristic that uses ˆπT
)P (hb0 ,b
argmaxb∈F (T ) γ d(h
T
as deﬁned in Equation 5, and AEMS24 to denote the heuristic that uses ˆπT as deﬁned in Equation 6.
4.2 Algorithm

Algorithm 1 presents the anytime error minimization search. Since the objective is to provide a
near-optimal action within a ﬁnite allowed online planning time, the algorithm accepts two input
parameters: t, the online search time allowed per action, and ǫ, the desired precision on the value
function.

Algorithm 1 AEMS: Anytime Error Minimization Search
Function S EARCH(t, ǫ)
Static : T : an AND-OR tree representing the current search tree.
t0 ← T IM E()
while T IM E() − t0 ≤ t and not SO LV ED(ROOT(T ), ǫ) do
b∗ ← eb(T )
EX PAND(b∗ )
U PDAT EANC E S TOR S(b∗ )
end while
return argmaxa∈A LT (ROOT(T ), a)

The EX PAND function expands the tree one level under the node b∗ by adding the next action and
belief nodes to the tree T and computing their lower and upper bounds according to Equations 1-
4. After a node is expanded, the U PDAT EANC E S TOR S function simply recomputes the bounds of
its ancestors according to Equations determining b′ (s′ ), V ∗ (b), P (o|b, a) and Q∗ (b, a), as outlined
in Section 2. It also recomputes the probabilities ˆπT (b, a) and the best actions for each ancestor
node. To ﬁnd quickly the node that maximizes the heuristic in the whole tree, each node in the tree
contains a reference to the best node to expand in their subtree. These references are updated by
the U PDAT EANC E S TOR S function without adding more complexity, such that when this function
terminates, we always know immediatly which node to expand next, as its reference is stored in the
root node. The search terminates whenever there is no more time available, or we have found an ǫ-
optimal solution (veri ﬁed by the S O LV ED function). After an action is executed in the environment,
the tree T is updated such that our new current belief state becomes the root of T ; all nodes under
this new root can be reused at the next time step.

4.3 Completeness and Optimality

We now provide some sufﬁcient conditions under which our heu ristic search is guaranteed to con-
verge to an ǫ-optimal policy after a ﬁnite number of expansions. We show t hat the heuristics pro-
posed in Section 4.1 satisfy those conditions, and therefore are admissible. Before we present the
main theorems, we provide some useful preliminary lemmas.
Lemma 1. In any tree T , the approximate error contribution ˆeT (b0 , bd ) of a belief node bd at depth
d is bounded by ˆeT (b0 , bd ) ≤ γ d supb ˆe(b).

Proof. P (hb0 ,b
|b0 , ˆπT ) ≤ 1 and ˆe(b) ≤ supb′ ˆe(b′ ) for all b. Thus ˆeT (b0 , bd ) ≤ γ d supb ˆe(b).
T
For the following lemma and theorem, we will denote P (ho |b0 , ha ) = Qd(h)
o |bhi−1
a ) the
, hi
i=1 P (hi
0
probability of observing the sequence of observations ho in some action/observation sequence h,
given that the sequence of actions ha in h is performed from current belief b0 , and bF (T ) ⊆ F (T )
the set of all fringe nodes in T such that P (hb0 ,b
T |b0 , ˆπT ) > 0, for ˆπT deﬁned as in Equation 6 (i.e.
3This heuristic is slightly different from the AEMS1 heuristic we had introduced in [11].
4This is the same as the AEMS2 heuristic we had introduced in [11].

the set of fringe nodes reached by a sequence of actions in which each action maximizes UT (b, a)
in its respective belief state.)
Lemma 2. For any tree T , ǫ > 0, and D such that γD supb ˆe(b) ≤ ǫ, if for all b ∈ bF (T ), either
d(hb0 ,b
T ) ≥ D or there exists an ancestor b′ of b such that ˆeT (b′ ) ≤ ǫ, then ˆeT (b0 ) ≤ ǫ.
Proof. Let’s denote ˆaT
b = argmaxa∈A UT (b, a). Notice that for any tree T , and parent belief b ∈ T , ˆeT (b) =
b ) = γ Po∈Ω P (o|b, ˆaT
b )ˆeT (τ (b, ˆaT
b )−LT (b, ˆaT
UT (b)−LT (b) ≤ UT (b, ˆaT
b , o)). Consequently, the following
recurrence is an upper bound on ˆeT (b):
ˆeT (b) ≤ 8><>:
if b ∈ F (T )
ˆe(b)
if ˆeT (b) ≤ ǫ
ǫ
γ Po∈Ω
b )ˆeT (τ (b, ˆaT
P (o|b, ˆaT
otherwise
b , o))
By unfolding the recurrence for b0 , we get ˆeT (b0 ) ≤ Pb∈A(T ) γ d(h
b0 ,b
T ,o |b0 , hb0 ,b
)P (hb0 ,b
T ,a )ˆe(b) +
T
ǫ Pb∈B(T ) γ d(h
b0 ,b
T ,a ), where B (T ) is the set of parent nodes b′ having a descendant in bF (T )
)P (hb0 ,b
T ,o |b0 , hb0 ,b
T
such that ˆeT (b′ ) ≤ ǫ and A(T ) is the set of fringe nodes b′′ in bF (T ) not having an ancestor in B (T ). Hence
if for all b ∈ bF (T ), d(hb0 ,b
) ≥ D or there exists an ancestor b′ of b such that ˆeT (b′ ) ≤ ǫ, then this means
T
) ≥ D , and therefore, ˆeT (b0 ) ≤ γD supb ˆe(b) Pb′∈A(T ) P (hb0 ,b′
T ,o |b0 , hb0 ,b′
that for all b in A(T ), d(hb0 ,b
T ,a ) +
T
T ,a ) ≤ ǫ Pb′∈A(T )∪B(T ) P (hb0 ,b′
ǫ Pb′∈B(T ) P (hb0 ,b′
T ,o |b0 , hb0 ,b′
T ,o |b0 , hb0 ,b′
T ,a ) = ǫ.
Theorem 2. For any tree T and ǫ > 0, if ˆπT is deﬁned such that
b ) > 0 for
inf b,T |ˆeT (b)>ǫ ˆπT (b, ˆaT
b = argmaxa∈A UT (b, a), then Algorithm 1 using eb(T ) is complete and ǫ-optimal.
ˆaT
Proof. If γ = 0, then the proof is immediate. Consider now the case where γ ∈ (0, 1). Clearly, since U
is bounded above and L is bounded below, then ˆe is bounded above. Now using γ ∈ (0, 1), we can ﬁnd a
positive integer D such that γD supb ˆe(b) ≤ ǫ. Let’s denote AT
b the set of ancestor belief states of b in the
tree T , and given a ﬁnite set A of belief nodes, let’s deﬁne ˆemin
(A) = minb∈A ˆeT (b). Now let’s deﬁne Tb =
T
{T |T f inite, b ∈ bF (T ), ˆemin
b ) > ǫ} and B = {b|ˆe(b) inf T ∈Tb P (hb0 ,b
|b0 , ˆπT ) > 0, d(hb0 ,b
(AT
) ≤ D}.
T
T
T
Clearly, by the assumption that inf b,T |ˆeT (b)>ǫ ˆπT (b, ˆaT
b ) > 0, then B contains all belief states b within depth
T ,a ) > 0 and there exists a ﬁnite tree T where b ∈ bF (T ) and all ancestors
D such that ˆe(b) > 0, P (hb0 ,b
T ,o |b0 , hb0 ,b
b′ of b have ˆeT (b′ ) > ǫ. Furthermore, B is ﬁnite since there are only ﬁnitely many belief states within depth
b0 ,b
) ˆe(b) inf T ∈Tb P (hb0 ,b
D . Hence there exist a Emin = minb∈B γ d(h
|b0 , ˆπT ). Clearly, Emin > 0 and we
T
T
know that for any tree T , all beliefs b in B ∩ bF (T ) have an approximate error contribution ˆeT (b0 , b) ≥ Emin .
Since Emin > 0 and γ ∈ (0, 1), there exist a positive integer D ′ such that γD′
supb ˆe(b) < Emin . Hence
by Lemma 1, this means that Algorithm 1 cannot expand any node at depth D ′ or beyond before expanding
a tree T where B ∩ bF (T ) = ∅. Because there are only ﬁnitely many nodes within depth D ′ , then it is clear
that Algorithm 1 will reach such tree T after a ﬁnite number of expansions. Furthermore, for this tree T , since
B ∩ bF (T ) = ∅, we have that for all beliefs b ∈ bF (T ), either d(hb0 ,b
) ≥ D or ˆemin
(AT
b ) ≤ ǫ. Hence by
T
T
Lemma 2, this implies that ˆeT (b0 ) ≤ ǫ, and consequently Algorithm 1 will terminate after a ﬁnite number of
expansions (SO LV ED(b0 , ǫ) will evaluate to true) with an ǫ-optimal solution (since eT (b0 ) ≤ ˆeT (b0 )).
From this last theorem, we notice that we can potentially develop many different admissible
the main sufﬁcient condition be ing that ˆπT (b, a) > 0 for a =
heuristics for Algorithm 1;
argmaxa′∈A UT (b, a′ ). It also follows from this theorem that the two heuristics described above,
AEMS1 and AEMS2, are admissible. The following corollaries prove this:
Corollary 1. Algorithm 1, using eb(T ), with ˆπT as deﬁned in Equation 6 is complete and ǫ-optimal.
b ) = 1 for all b, T .
Proof. Immediate by Theorem 2 and the fact that ˆπT (b, ˆaT
Corollary 2. Algorithm 1, using eb(T ), with ˆπT as deﬁned in Equation 5 is complete and ǫ-optimal.
Proof. We ﬁrst notice that
(UT (b, a) − LT (b))2 /(UT (b, a) − LT (b, a)) ≤ ˆeT (b, a), since LT (b) ≥
LT (b, a) for all a. Furthermore, ˆeT (b, a) ≤ supb′ ˆe(b′ ). Therefore the normalization constant
b ) = UT (b), and there-
b = argmaxa∈A UT (b, a), we have UT (b, ˆaT
η ≥ (|A| supb ˆe(b))−1 . For ˆaT
fore UT (b, ˆaT
b ) − LT (b) = ˆeT (b). Hence this means that ˆπT (b, ˆaT
b ) = η(ˆeT (b))2 /ˆeT (b, ˆaT
b ) ≥

(|A|(supb′ ˆe(b′ ))2 )−1 (ˆeT (b))2 for all T , b. Hence, for any ǫ > 0, inf b,T |ˆeT (b)>ǫ ˆπT (b, ˆaT
b ) ≥
(|A|(supb ˆe(b))2 )−1 ǫ2 > 0. Hence, corrolary follows from Theorem 2.

5 Experiments

In this section we present a brief experimental evaluation of Algorithm 1, showing that in addition to
its useful theoretical properties, the empirical performance matches, and in some cases exceeds, that
of other online approaches. The algorithm is evaluated in three large POMDP environments: Tag
[1], RockSample [3] and FieldVisionRockSample (FVRS) [11]; all are implemented using a factored
In each environments we compute the Blind policy5 to get a lower bound
state representation.
and the FIB algorithm [15] to get an upper bound. We then compare performance of Algorithm 1
with both heuristics (AEMS1 and AEMS2) to the performance achieved by other online approaches
(Satia [7], BI-POMDP [8], RTBSS [10]). For all approaches we impose a real-time constraint of
1 sec/action, and measure the following metrics: average return, average error bound reduction6
(EBR), average lower bound improvement7 (LBI), number of belief nodes explored at each time
step, percentage of belief nodes reused in the next time step, and the average online time per action
(< 1s means the algorithm found an ǫ-optimal action)8 . Satia, BI-POMDP, AEMS1 and AEMS2
were all implemented using the same algorithm since they differ only in their choice of search
heuristic used to guide the search. RTBSS served as a base line for a complete k-step lookahead
search using branch & bound pruning. All results were obtained on a Xeon 2.4 Ghz with 4Gb of
RAM; but the processes were limited to use a max of 1Gb of RAM.

Table 1 shows the average value (over 1000+ runs) of the different statistics. As we can see from
these results, AEMS2 provides the best average return, average error bound reduction and average
lower bound improvement in all considered environments. The higher error bound reduction and
lower bound improvement obtained by AEMS2 indicates that it can guarantee performance closer
to the optimal. We can also observe that AEMS2 has the best average reuse percentage, which
indicates that AEMS2 is able to guide the search toward the most probable nodes and allows it to
generally maintain a higher number of belief nodes in the tree. Notice that AEMS1 did not perform
very well, except in FVRS[5,7]. This could be explained by the fact that our assumption that the
values of the actions are uniformly distributed between the lower and upper bounds is not valid in
the considered environments.

Finally, we also examined how fast the lower and upper bounds converge if we let the algorithm run
up to 1000 seconds on the initial belief state. This gives an indication of which heuristic would be
the best if we extended online planning time past 1sec. Results for RockSample[7,8] are presented
in Figure 2, showing that the bounds converge much more quickly for the AEMS2 heuristic.

6 Conclusion

In this paper we examined theoretical properties of online heuristic search algorithms for POMDPs.
To this end, we described a general online search framework, and examined two admissible heuris-
tics to guide the search. The ﬁrst assumes that Q∗ (b, a) is distributed uniformly at random be-
tween the bounds (Heuristic AEMS1), the second favors an optimistic point of view, and assume
the Q∗ (b, a) is equal to the upper bound (Heuristic AEMS2). We provided a general theorem that
shows that AEMS1 and AEMS2 are admissible and lead to complete and ǫ-optimal algorithms. Our
experimental work supports the theoretical analysis, showing that AEMS2 is able to outperform on-
line approaches. Yet it is equally interesting to note that AEMS1 did not perform nearly as well.
This highlights the fact that not all admissible heuristics are equally useful. Thus it will be interest-
ing in the future to develop further guidelines and theoretical results describing which subclasses of
heuristics are most appropriate.

5The policy obtained by taking the combination of the |A| α-vectors that each represents the value of a
policy performing the same action in every belief state.
6The error bound reduction is deﬁned as 1 − UT (b0 )−LT (b0 )
, when the search process terminates on b0
U (b0 )−L(b0 )
7The lower bound improvement is deﬁned as LT (b0 ) − L(b0 ), when the search process terminates on b0
8For RTBSS, the maximum search depth under the 1sec time constraint is show in parenthesis.

Figure 1: Comparison of different online search al-
gorithm in different environments.
Belief Reuse Time
Heuristic /
(ms)
(%)
Nodes
Algorithm
LBI
EBR (%)
Return
± 0.01
-
± 0.01
± 0.1
±0.1
±1
Tag (|S | = 870, |A| = 5, |Ω| = 30)
0
45067
3.03
22.3
-10.30
RTBSS(5)
10.0
36908
2.47
22.9
-8.35
Satia & Lave
25.1
43693
3.92
49.0
-6.73
AEMS1
54.6
79508
7.81
76.2
-6.22
BI-POMDP
AEMS2
54.8
-6.19
76.3
7.81
80250
RockSample[7,8] (|S | = 12545, |A| = 13, |Ω| = 2)
Satia & Lave
7.35
3.6
0
509
8.9
5.3
579
0.90
9.5
10.30
AEMS1
0
439
1.00
9.7
10.30
RTBSS(2)
29.9
2152
4.33
33.3
18.43
BI-POMDP
AEMS2
20.75
52.4
5.30
3145
36.4
FVRS[5,7] (|S | = 3201, |A| = 5, |Ω| = 128)
516
2.07
7.7
20.57
22.75
11.1
2.08
4457
3683
2.05
11.1
22.79
3856
2.24
12.4
23.31
23.39
13.3
2.35
4070

RTBSS(1)
BI-POMDP
Satia & Lave
AEMS1
AEMS2

254
923
947
942
944

580
856
814
622
623

900
916
896
953
859

0
0.4
0.4
1.4
1.6

)
0
b
(
V

30

25

20

15

10

5
10−2

AEMS2
AEMS1
BI−POMDP
Satia

10−1

100

Time (s)

101

102

103

Figure 2: Evolution of the upper / lower bounds on
the initial belief state in RockSample[7,8].

Acknowledgments

This research was supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Fonds Qu ´eb ´ecois de la Recherche sur la Nature et les Technologies (FQRNT).

References

[1] J. Pineau. Tractable planning under uncertainty: exploiting structure. PhD thesis, Carnegie Mellon
University, Pittsburgh, PA, 2004.
[2] P. Poupart. Exploiting structure to efﬁciently solve large scale partially observable Mark ov decision
processes. PhD thesis, University of Toronto, 2005.
[3] T. Smith and R. Simmons. Point-based POMDP algorithms: improved analysis and implementation. In
UAI, 2005.
[4] M. T. J. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for POMDPs. JAIR,
24:195–220, 2005.
[5] N. Roy and G. Gordon. Exponential family PCA for belief compression in POMDPs. In NIPS, 2003.
[6] P. Poupart and C. Boutilier. Value-directed compression of POMDPs. In NIPS, 2003.
[7] J. K. Satia and R. E. Lave. Markovian decision processes with probabilistic observation of states. Man-
agement Science, 20(1):1–13, 1973.
[8] R. Washington. BI-POMDP: bounded, incremental partially observable Markov model planning. In 4th
Eur. Conf. on Planning, pages 440–451, 1997.
[9] D. McAllester and S. Singh. Approximate Planning for Factored POMDPs using Belief State Simpliﬁca-
tion. In UAI, 1999.
[10] S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for complex multiagent environ-
ments. In AAMAS, 2005.
[11] S. Ross and B. Chaib-draa. AEMS: an anytime online search algorithm for approximate policy reﬁnement
in large POMDPs. In IJCAI, 2007.
[12] E. A. Hansen and S. Zilberstein. LAO * : A heuristic search algorithm that ﬁnds solutions with loops.
Artiﬁcial Intelligence , 129(1-2):35–62, 2001.
[13] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &
Sons, Inc., New York, NY, USA, 1994.
[14] N.J. Nilsson. Principles of Artiﬁcial Intelligence . Tioga Publishing, 1980.
[15] M. Hauskrecht. Value-function approximations for POMDPs. JAIR, 13:33–94, 2000.


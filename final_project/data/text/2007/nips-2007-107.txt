Simulated Annealing: Rigorous ﬁnite-time guarantees
for optimization on continuous domains

Andrea Lecchini-Visintini
Department of Engineering
University of Leicester, UK
alv1@leicester.ac.uk

John Lygeros
Automatic Control Laboratory
ETH Zurich, Switzerland.
lygeros@control.ee.ethz.ch

Jan Maciejowski
Department of Engineering
University of Cambridge, UK
jmm@eng.cam.ac.uk

Abstract

Simulated annealing is a popular method for approaching the solution of a global
optimization problem. Existing results on its performance apply to discrete com-
binatorial optimization where the optimization variables can assume only a ﬁnite
set of possible values. We introduce a new general formulation of simulated an-
nealing which allows one to guarantee ﬁnite-time performan ce in the optimiza-
tion of functions of continuous variables. The results hold universally for any
optimization problem on a bounded domain and establish a connection between
simulated annealing and up-to-date theory of convergence of Markov chain Monte
Carlo methods on continuous domains. This work is inspired by the concept of
ﬁnite-time learning with known accuracy and conﬁdence deve
loped in statistical
learning theory.

Optimization is the general problem of ﬁnding a value of a vec tor of variables θ that maximizes
(or minimizes) some scalar criterion U (θ). The set of all possible values of the vector θ is called
the optimization domain. The elements of θ can be discrete or continuous variables. In the ﬁrst case
the optimization domain is usually ﬁnite, such as in the well -known traveling salesman problem; in
the second case the optimization domain is a continuous set. An important example of a continuous
optimization domain is the set of 3-D conﬁgurations of a sequ ence of amino-acids in the problem of
ﬁnding the minimum energy folding of the corresponding prot ein [1].
In principle, any optimization problem on a ﬁnite domain can be solved by an exhaustive search.
However, this is often beyond computational capacity: the optimization domain of the traveling
salesman problem with 100 cities contains more than 10155 possible tours. An efﬁcient algorithm
to solve the traveling salesman and many similar problems has not yet been found and such prob-
lems remain reliably solvable only in principle [2]. Statistical mechanics has inspired widely used
methods for ﬁnding good approximate solutions in hard discr ete optimization problems which defy
efﬁcient exact solutions [3, 4, 5, 6]. Here a key idea has been that of simulated annealing [3]: a
random search based on the Metropolis-Hastings algorithm, such that the distribution of the ele-
ments of the domain visited during the search converges to an equilibrium distribution concentrated
around the global optimizers. Convergence and ﬁnite-time p erformance of simulated annealing on
ﬁnite domains has been evaluated in many works, e.g. [7, 8, 9, 10].
On continuous domains, most popular optimization methods perform a local gradient-based
search and in general converge to local optimizers; with the notable exception of convex criteria
where convergence to the unique global optimizer occurs [11]. Simulated annealing performs a
global search and can be easily implemented on continuous domains. Hence it can be considered
a powerful complement to local methods. In this paper, we introduce for the ﬁrst time rigorous
guarantees on the ﬁnite-time performance of simulated anne aling on continuous domains. We will

show that it is possible to derive simulated annealing algorithms which, with an arbitrarily high level
of conﬁdence, ﬁnd an approximate solution to the problem of o
ptimizing a function of continuous
variables, within a speci ﬁed tolerance to the global optima l solution after a known ﬁnite number of
steps. Rigorous guarantees on the ﬁnite-time performance o f simulated annealing in the optimiza-
tion of functions of continuous variables have never been obtained before; the only results available
state that simulated annealing converges to a global optimizer as the number of steps grows to inﬁn-
ity, e.g. [12, 13, 14, 15].
The background of our work is twofold. On the one hand, our notion of approximate solution to
a global optimization problem is inspired by the concept of ﬁ nite-time learning with known accuracy
and conﬁdence developed in statistical learning theory [16 , 17]. We actually maintain an important
aspect of statistical learning theory which is that we do not introduce any particular assumption on
the optimization criterion, i.e. our results hold regardless of what U is. On the other hand, we ground
our results on the theory of convergence, with quantitative bounds on the distance to the target dis-
tribution, of the Metropolis-Hastings algorithm and Markov Chain Monte Carlo (MCMC) methods,
which has been one of the main achievements of recent research in statistics [18, 19, 20, 21].
In this paper, we will not develop any ready-to-use optimization algorithm. We will instead in-
troduce a general formulation of the simulated annealing method which allows one to derive new
simulated annealing algorithms with rigorous ﬁnite-time g uarantees on the basis of existing theory.
The Metropolis-Hastings algorithm and the general family of MCMC methods have many degrees
of freedom. The choice and comparison of speci ﬁc algorithms goes beyond the scope of the paper.
The paper is organized in the following sections.
In Simulated annealing we introduce the
method and ﬁx the notation. In Convergence we recall the reasons why ﬁnite-time guarantees for
simulated annealing on continuous domains have not been obtained before. In Finite-time guaran-
tees we present the main result of the paper. In Conclusions we state our ﬁndings and conclude the
paper.

1 Simulated annealing

The original formulation of simulated annealing was inspired by the analogy between the stochastic
evolution of the thermodynamic state of an annealing material towards the conﬁgurations of minimal
energy and the search for the global minimum of an optimization criterion [3]. In the procedure, the
optimization criterion plays the role of the energy and the state of the annealed material is simulated
by the evolution of the state of an inhomogeneous Markov chain. The state of the chain evolves
according to the Metropolis-Hastings algorithm in order to simulate the Boltzmann distribution of
thermodynamic equilibrium. The Boltzmann distribution is simulated for a decreasing sequence of
temperatures ( “cooling”). The target distribution of the c ooling procedure is the limiting Boltzmann
distribution, for the temperature that tends to zero, which takes non-zero values only on the set of
global minimizers [7].
The original formulation of the method was for a ﬁnite domain . However, simulated anneal-
ing can be generalized straightforwardly to a continuous domain because the Metropolis-Hastings
algorithm can be used with almost no differences on discrete and continuous domains The main
difference is that on a continuous domain the equilibrium distributions are speci ﬁed by probability
densities. On a continuous domain, Markov transition kernels in which the distribution of the el-
ements visited by the chain converges to an equilibrium distribution with the desired density can
be constructed using the Metropolis-Hastings algorithm and the general family of MCMC methods
[22].
We point out that Boltzmann distributions are not the only distributions which can be adopted as
equilibrium distributions in simulated annealing [7]. In this paper it is convenient for us to adopt a
different type of equilibrium distribution in place of Boltzmann distributions.

1.1 Our setting

The optimization criterion is U : Θ → [0, 1], with Θ ⊂ RN . The assumption that U takes values in
the interval [0, 1] is a technical one. It does not imply any serious loss of generality. In general, any
bounded optimization criterion can be scaled to take values in [0, 1]. We assume that the optimiza-
tion task is to ﬁnd a global maximizer; this can be done withou t loss of generality. We also assume
that Θ is a bounded set.
We consider equilibrium distributions deﬁned by probabili

ty density functions proportional to

[U (θ) + δ ]J where J and δ are two strictly positive parameters. We use π (J ) to denote an equi-
librium distribution, i.e. π (J ) (dθ) ∝ [U (θ) + δ ]J πLeb (dθ) where πLeb is the standard Lebesgue
measure. Here, J −1 plays the role of the temperature: if the function U (θ) plus δ is taken to a
positive power J then as J increases (i.e. as J −1 decreases) [U (θ) + δ ]J becomes increasingly
peaked around the global maximizers. The parameter δ is an offset which guarantees that the
equilibrium densities are always strictly positive, even if U takes zero values on some elements
of the domain. The offset δ is chosen by the user and we show later that our results allow one to
make an optimal selection of δ . The zero-temperature distribution is the limiting distribution, for
J → ∞, which takes non-zero values only on the set of global maximizers. It is denoted by π (∞) .
In the generic formulation of the method, the Markov transition kernel of the k-th step of the
inhomogeneous chain has equilibrium distribution π (Jk ) where {Jk }k=1,2,... is the “cooling sched-
ule”. The cooling schedule is a non-decreasing sequence of p ositive numbers according to which
the equilibrium distribution become increasingly sharpened during the evolution of the chain. We
use θk to denote the state of the chain and Pθk to denote its probability distribution. The distribution
Pθk obviously depends on the initial condition θ0 . However, in this work, we don’t need to make
this dependence explicit in the notation.
Remark 1: If, given an element θ in Θ, the value U (θ) can be computed directly, we say that U
is a deterministic criterion, e.g. the energy landscape in protein structure prediction [1]. In problems
involving random variables, the value U (θ) may be the expected value U (θ) = R g(x, θ)px (x; θ)dx
of some function g which depends on both the optimization variable θ , and on some random vari-
able x which has probability density px (x; θ) (which may itself depend on θ). In such problems it
is usually not possible to compute U (θ) directly, either because evaluation of the integral requires
too much computation, or because no analytical expression for px (x; θ) is available. Typically one
must perform stochastic simulations in order to obtain samples of x for a given θ , hence obtain
sample values of g(x, θ), and thus construct a Monte Carlo estimate of U (θ). The Bayesian design
of clinical trials is an important application area where such expected-value criteria arise [23]. The
authors of this paper investigate the optimization of expected-value criteria motivated by problems
of aircraft routing [24]. In the particular case that px (x; θ) does not depend on θ , the optimization
task is often called “empirical risk minimization”, and is s
tudied extensively in statistical learning
theory [16, 17]. The results of this paper apply in the same way to the optimization of both deter-
ministic and expected-value criteria. The MCMC method developed by M ¨uller [25, 26] allows one
to construct simulated annealing algorithms for the optimization of expected-value criteria. M ¨uller
[25, 26] employs the same equilibrium distributions as those described in our setting; in his context
J is restricted to integer values.

2 Convergence

The rationale of simulated annealing is as follows: if the temperature is kept constant, say Jk = J ,
then the distribution of the state of the chain Pθk tends to the equilibrium distribution π (J ) ; if J → ∞
then the equilibrium distribution π (J ) tends to the zero-temperature distribution π (∞) ; as a result, if
the cooling schedule Jk tends to inﬁnity, one obtains that Pθk “follows ”
π (Jk ) and that π (Jk ) tends
to π (∞) and eventually that the distribution of the state of the chain Pθk tends to π (∞) . The theory
shows that, under conditions on the cooling schedule and the Markov transition kernels, the distri-
bution of the state of the chain Pθk actually converges to the target zero-temperature distribution
π (∞) as k → ∞ [12, 13, 14, 15]. Convergence to the zero-temperature distribution implies that
asymptotically the state of the chain eventually coincides with a global optimizer with probability
one.
step results on simulated an-
The difﬁculty which must be overcome in order to obtain ﬁnite
nealing algorithms on a continuous domain is that usually, in an optimization problem deﬁned over
continuous variables, the set of global optimizers has zero Lebesgue measure (e.g. a set of isolated
points). If the set of global optimizers has zero measure then the set of global optimizers has null
probability according to the equilibrium distributions π (J ) for any ﬁnite J and, as a consequence,
according to the distributions Pθk for any ﬁnite k . Put another way, the probability that the state of
the chain visits the set of global optimizers is constantly zero after any ﬁnite number of steps. Hence
the conﬁdence of the fact that the solution provided by the al gorithm in ﬁnite time coincides with a
global optimizer is also constantly zero. Notice that this is not the case for a ﬁnite domain, where
the set of global optimizers is of non-null measure with respect to the reference counting measure

[7, 8, 9, 10].
It is instructive to look at the issue also in terms of the rate of convergence to the target zero-
temperature distribution. On a discrete domain, the distribution of the state of the chain at each
step and the zero-temperature distribution are both standard discrete distributions. It is then possible
to deﬁne a distance between them and study the rate of converg ence of this distance to zero. This
analysis allows one to obtain results on the ﬁnite-time beha vior of simulated annealing [7, 8]. On a
continuous domain and for a set of global optimizers of measure zero, the target zero-temperature
distribution π (∞) ends up being a mixture of probability masses on the set of global optimizers. In
this situation, although the distribution of the state of the chain Pθk still converges asymptotically
to π (∞) , it is not possible to introduce a sensible distance between the two distributions and a rate
of convergence to the target distribution cannot even be deﬁ ned (weak convergence), see [12, The-
orem 3.3]. This is the reason that until now there have been no guarantees on the performance of
simulated annealing on a continuous domain after a ﬁnite num ber of computations: by adopting the
zero-temperature distribution π (∞) as the target distribution it is only possible to prove asymptotic
convergence in inﬁnite time to a global optimizer.
Remark 2: The standard distance between two distributions, say µ1 and µ2 , on a continuous sup-
port is the total variation norm kµ1 − µ2 kT V = supA |µ1 (A) − µ2 (A)|, see e.g. [21]. In simulated
annealing on a continuous domain the distribution of the state of the chain Pθk is absolutely con-
tinuous with respect to the Lebesgue measure (i.e. πLeb (A) = 0 ⇒ Pθk (A) = 0), by construction
for any ﬁnite k . Hence if the set of global optimizers has zero Lebesgue measure then it has zero
measure also according to Pθk . The set of global optimizers has however measure 1 according to
π (∞) . The distance kPθk − π (∞) kT V is then constantly 1 for any ﬁnite k .
It is also worth mentioning that if the set of global optimizers has zero measure then asymp-
totic convergence to the zero-temperature distribution π (∞) can be proven only under the additional
assumptions of continuity and differentiability of U [12, 13, 14, 15].

3 Finite-time guarantees

In general, optimization algorithms for problems deﬁned on continuous variables can only ﬁnd ap-
proximate solutions in ﬁnite time [27]. Given an element θ of a continuous domain how can we
assess how good it is as an approximate solution to an optimization problem? Here we introduce
the concept of approximate global optimizer to answer this question. The deﬁnition is given for
a maximization problem in a continuous but bounded domain. We use two parameters: the value
imprecision ǫ (greater than or equal to 0) and the residual domain α (between 0 and 1) which to-
gether determine the level of approximation. We say that θ is an approximate global optimizer of U
with value imprecision ǫ and residual domain α if the function U takes values strictly greater than
U (θ) + ǫ only on a subset of values of θ no larger than an α portion of the optimization domain. The
formal deﬁnition is as follows.

Deﬁnition 1 Let U : Θ → R be an optimization criterion where Θ ⊂ RN is bounded. Let πLeb
denote the standard Lebesgue measure. Let ǫ ≥ 0 and α ∈ [0, 1] be given numbers. Then θ is an
approximate global optimizer of U with value imprecision ǫ and residual domain α if πLeb {θ ′ ∈ Θ :
U (θ ′ ) > U (θ) + ǫ} ≤ α πLeb (Θ) .

In other words, the value U (θ) is within ǫ of a value which is greater than the values that U takes
on at least a 1 − α portion of the domain. The smaller ǫ and α are, the better is the approximation
of a true global optimizer. If both α and ǫ are equal to zero then U (θ) coincides with the essential
supremum of U .
Our deﬁnition of approximate global optimizer carries an im portant property, which holds re-
gardless of what the criterion U is: if ǫ and α have non-zero values then the set of approximate
global optimizers always has non-zero Lebesgue measure. It follows that the probability that the
chain visits the set of approximate global optimizers can be non-zero. Hence, it is sensible to study
the conﬁdence of the fact that the solution found by simulate d annealing in ﬁnite time is an approx-
imate global optimizer.
Remark 3: The intuition that our notion of approximate global optimizer can be used to obtain
formal guarantees on the ﬁnite-time performance of optimiz ation methods based on a stochastic
search of the domain is already apparent in the work of Vidyasagar [17, 28]. Vidyasagar [17, 28]
introduces a similar deﬁnition and obtains rigorous ﬁnite-
time guarantees in the optimization of ex-

pected value criteria based on uniform independent sampling of the domain. Notably, the number
of independent samples required to guarantee some desired accuracy and conﬁdence turns out to be
polynomial in the values of the desired imprecision, residual domain and conﬁdence. Although the
method of Vidyasagar is not highly sophisticated, it has had considerable success in solving difﬁcult
control system design applications [28, 29]. Its appeal stems from its rigorous ﬁnite-time guarantees
which exist without the need for any particular assumption on the optimization criterion.
Here we show that ﬁnite-time guarantees for simulated annea ling can be obtained by selecting a
distribution π (J ) with a ﬁnite J as the target distribution in place of the zero-temperature distribution
π (∞) . The fundamental result is the following theorem which allows one to select in a rigorous way
δ and J in the target distribution π (J ) . It is important to stress that the result holds universally for
any optimization criterion U on a bounded domain. The only minor requirement is that U takes
values in [0, 1].

Theorem 1 Let U : Θ → [0, 1] be an optimization criterion where Θ ⊂ RN is bounded. Let
J ≥ 1 and δ > 0 be given numbers. Let θ be a multivariate random variable with distribution
π (J ) (dθ) ∝ [U (θ) + δ ]J πLeb (dθ). Let α ∈ (0, 1] and ǫ ∈ [0, 1] be given numbers and deﬁne
1
ǫ + 1 + δ (cid:21) J (cid:20) 1
− 1(cid:21) 1 + δ
1 + (cid:20) 1 + δ
1 + δ
ǫ + δ
α
δ
Then the statement “ θ is an approximate global optimizer of U with value imprecision ǫ and residual
domain α ” holds with probability at least σ .
Proof. See Appendix A.

σ =

.

(1)

The importance of the choice of a target distribution π (J ) with a ﬁnite J is that π (J ) is absolutely
continuous with respect to the Lebesgue measure. Hence, the distance kPθk − π (J ) kTV between the
distribution of the state of the chain Pθk and the target distribution π (J ) is a meaningful quantity.
Convergence of the Metropolis-Hastings algorithm and MCMC methods in total variation norm
is a well studied problem. The theory provides simple conditions under which one derives upper
bounds on the distance to the target distribution which are known at each step of the chain and
decrease monotonically to zero as the number of steps of the chain grows. The theory has been
developed mainly for homogeneous chains [18, 19, 20, 21].
In the case of simulated annealing, the factor that enables us to employ these results is the abso-
lute continuity of the target distribution π (J ) with respect to the Lebesgue measure. However, simu-
lated annealing involves the simulation of inhomogeneous chains. In this respect, another important
fact is that the choice of a target distribution π (J ) with a ﬁnite J implies that the inhomogeneous
Markov chain can in fact be formed by a ﬁnite sequence of homog eneous chains (i.e. the cooling
schedule {Jk }k=1,2,... can be chosen to be a sequence that takes only a ﬁnite set of val ues). In turn,
this allows one to apply the theory of homogeneous MCMC methods to study the convergence of
Pθk to π (J ) in total variation norm.
On a bounded domain, simple conditions on the ‘proposal distribution’ in the iteration of the sim-
ulated annealing algorithm allows one to obtain upper bounds on kPθk − π (J )kTV that decrease geo-
metrically to zero as k → ∞, without the need for any additional assumption on U [18, 19, 20, 21].
It is then appropriate to introduce the following ﬁnite-tim e result.

Theorem 2 Let the notation and assumptions of Theorem 1 hold. Let θk , with distribution Pθk , be
the state of the inhomogeneous chain of a simulated annealing algorithm with target distribution
π (J ) . Then the statement “ θk is an approximate global optimizer of U with value imprecision ǫ and
residual domain α ” holds with probability at least σ − kPθk − π (J )kTV .

The proof of the theorem follows directly from the deﬁnition of the total variation norm.
It follows that if simulated annealing is implemented with an algorithm which converges in total
variation distance to a target distribution π (J ) with a ﬁnite J , then one can state with conﬁdence
arbitrarily close to 1 that the solution found by the algorithm after the known appropriate ﬁnite
number of steps is an approximate global optimizer with the desired approximation level. For given
non-zero values of ǫ, α the value of σ given by (1) can be made arbitrarily close to 1 by choice of
J ; while the distance kPθk − π (J ) kTV can be made arbitrarily small by taking the known sufﬁcient
number of steps.

It can be shown that there exists the possibility of making an optimal choice of δ and J in the
target distribution π (J ) . In fact, for given ǫ and α and a given value of J there exists an optimal
choice of δ which maximizes the value of σ given by (1). Hence, it is possible to obtain a desired σ
with the smallest possible J . The advantage of choosing the smallest J , consistent with the required
approximation and conﬁdence, is that it will decrease the nu mber of steps required to achieve the
desired reduction of kPθk − π (J )kTV .

4 Conclusions

We have introduced a new formulation of simulated annealing which admits rigorous ﬁnite-time
guarantees in the optimization of functions of continuous variables. First, we have introduced the
notion of approximate global optimizer. Then, we have shown that simulated annealing is guaranteed
to ﬁnd approximate global optimizers, with the desired conﬁ
dence and the desired level of accuracy,
in a known ﬁnite number of steps, if a proper choice of the targ et distribution is made and conditions
for convergence in total variation norm are met. The results hold for any optimization criterion on a
bounded domain with the only minor requirement that it takes values between 0 and 1.
In this framework, simulated annealing algorithms with rigorous ﬁnite-time guarantees can be
derived by studying the choice of the proposal distribution and of the cooling schedule, in the generic
iteration of simulated annealing, in order to ensure convergence to the target distribution in total
variation norm. To do this, existing theory of convergence of the Metropolis-Hastings algorithm and
MCMC methods on continuous domains can be used [18, 19, 20, 21].
Vidyasagar [17, 28] has introduced a similar deﬁnition of ap proximate global optimizer and has
shown that approximate optimizers with desired accuracy and conﬁdence can be obtained with a
number of uniform independent samples of the domain which is polynomial in the accuracy and
conﬁdence parameters.
In general, algorithms developed wi
th the MCMC methodology can be
expected to be equally or more efﬁcient than uniform indepen dent sampling.

Acknowledgments
Work supported by EPSRC, Grant EP/C014006/1, and by the European Commission under projects
HYGEIA FP6-NEST-4995 and iFly FP6-TREN-037180. We thank S. Brooks, M. Vidyasagar and
D. M. Wolpert for discussions and useful comments on the paper.

A Proof of Theorem 1

Let ¯α ∈ (0, 1] and ρ ∈ (0, 1] be given numbers. Let Uδ (θ) := U (θ) + δ . Let πδ be a normalized
measure such that πδ (dθ) ∝ Uδ (θ)πLeb (dθ). In the ﬁrst part of the proof we ﬁnd a lower bound on
the probability that θ belongs to the set {θ ∈ Θ : πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α} .
Let y ¯α := inf {y : πδ {θ ∈ Θ : Uδ (θ) ≤ y} ≥ 1 − ¯α}. To start with we show that the set
{θ ∈ Θ : πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α} coincides with {θ ∈ Θ : Uδ (θ) ≥ ρ y ¯α}. Notice
that the quantity πδ {θ ∈ Θ : Uδ (θ) ≤ y} is a right-continuous non-decreasing function of y because
it has the form of a distribution function (see e.g. [30, p.162] and [17, Lemma 11.1]). Therefore we
have πδ {θ ∈ Θ : Uδ (θ) ≤ y ¯α} ≥ 1 − ¯α and
y ≥ ρ y ¯α ⇒ πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) ≤ y} ≥ 1 − ¯α ⇒ πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > y} ≤ ¯α .
Moreover,
y < ρ y ¯α ⇒ πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) ≤ y} < 1 − ¯α ⇒ πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > y} > ¯α
and taking the contrapositive one obtains
πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > y} ≤ ¯α ⇒ y ≥ ρ y ¯α .
Therefore {θ ∈ Θ : Uδ (θ) ≥ ρ y ¯α} ≡ {θ ∈ Θ : πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α}.
We now derive a lower bound on π (J ) {θ ∈ Θ : Uδ (θ) ≥ ρ y ¯α}. Let us introduce the notation
A ¯α := {θ ∈ Θ : Uδ (θ) < y ¯α}, ¯A ¯α := {θ ∈ Θ : Uδ (θ) ≥ y ¯α}, B ¯α,ρ := {θ ∈ Θ : Uδ (θ) < ρ y ¯α }
and ¯B ¯α,ρ := {θ ∈ Θ : Uδ (θ) ≥ ρ y ¯α }. Notice that B ¯α,ρ ⊆ A ¯α and ¯A ¯α ⊆ ¯B ¯α,ρ . The quantity
πδ {θ ∈ Θ : Uδ (θ) < y} as a function of y is the left-continuous version of πδ {θ ∈ Θ : Uδ (θ) ≤
y}[30, p.162]. Hence, the deﬁnition of y ¯α implies πδ (A ¯α ) ≤ 1 − ¯α and πδ ( ¯A ¯α ) ≥ ¯α. Notice that
δπLeb (A ¯α )
(cid:2)RΘ Uδ (θ)πLeb (dθ)(cid:3)

πδ (A ¯α ) ≤ 1 − ¯α ⇒

≤ 1 − ¯α ,

≥ ¯α .

πδ ( ¯A ¯α ) ≥ ¯α ⇒

Hence, πLeb ( ¯A ¯α ) > 0 and

(1 + δ)πLeb ( ¯A ¯α )
(cid:2)RΘ Uδ (θ)πLeb (dθ)(cid:3)
1 − ¯α
πLeb (A ¯α )
1 + δ
≤
.
πLeb ( ¯A ¯α )
¯α
δ
Notice that πLeb ( ¯A ¯α ) > 0 implies πLeb ( ¯B ¯α,ρ ) > 0. We obtain
1
Uδ (θ)J πLeb (dθ)
Uδ (θ)J πLeb (dθ)

π (J ) {θ ∈ Θ : Uδ (θ) ≥ ρ y ¯α } =

≥

≥

≥

1 +

1
Uδ (θ)J πLeb (dθ)
1 + RB ¯α,ρ
R ¯A ¯α
Uδ (θ)J πLeb (dθ)
.
1 + δ
δ

1 + RB ¯α,ρ
R ¯B ¯α,ρ
1
1
ρ J yJ
1 + ρ J πLeb (A ¯α )
πLeb (B ¯α,ρ )
¯α
πLeb ( ¯A ¯α )
πLeb ( ¯A ¯α )
yJ
¯α
Since {θ ∈ Θ : Uδ (θ) ≥ ρ y ¯α} ≡ {θ ∈ Θ : πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α} the ﬁrst part of
the proof is complete.
In the second part of the proof we show that the set {θ ∈ Θ : πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) >
Uδ (θ)} ≤ ¯α} is contained in the set of approximate global optimizers of U with value imprecision
˜ǫ := (ρ−1 − 1)(1 + δ) and residual domain ˜α := 1+δ
˜ǫ+δ ¯α. Hence, we show that {θ ∈ Θ : πδ {θ ′ ∈
Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α} ⊆ {θ ∈ Θ : πLeb {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} ≤ ˜α πLeb (Θ)}. We
have

1
1 + ρ J 1 − ¯α
¯α

≥

U (θ ′ ) > U (θ) + ˜ǫ ⇔ ρ Uδ (θ ′ ) > ρ [Uδ (θ) + ˜ǫ] ⇒ ρ Uδ (θ ′ ) > Uδ (θ)
which is proven by noticing that ρ [Uδ (θ) + ˜ǫ] ≥ Uδ (θ) ⇔ 1 − ρ ≥ U (θ)(1 − ρ)
and U (θ) ∈ [0, 1]. Hence {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ⊇ {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} .
Therefore πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α ⇒ πδ {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} ≤ ¯α . Let
Qθ,˜ǫ := {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} and notice that
ZQθ,˜ǫ
ZΘ

πδ {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} =

U (θ ′ )πLeb (dθ ′ ) + δπLeb (Qθ,˜ǫ )

U (θ ′ )πLeb (dθ ′ ) + δπLeb (Θ)

.

We obtain

πδ {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} ≤ ¯α ⇒ ˜ǫ πLeb (Qθ,˜ǫ ) + δπLeb (Qθ,˜ǫ ) ≤ ¯α(1 + δ)πLeb (Θ)
⇒ πLeb {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} ≤ ˜α πLeb (Θ) .

Hence we can conclude that

πδ {θ ′ ∈ Θ : ρ Uδ (θ ′ ) > Uδ (θ)} ≤ ¯α ⇒ πLeb {θ ′ ∈ Θ : U (θ ′ ) > U (θ) + ˜ǫ} ≤ ˜α πLeb (Θ)
and the second part of the proof is complete.
We have shown that given ¯α ∈ (0, 1], ρ ∈ (0, 1], ˜ǫ := (ρ−1 − 1)(1 + δ), ˜α := 1+δ
˜ǫ+δ ¯α and

=

σ :=

1
1 + ρ J 1 − ¯α
¯α

1
˜ǫ + 1 + δ (cid:21) J (cid:20) 1
− 1(cid:21) 1 + δ
1 + (cid:20) 1 + δ
1 + δ
˜α
˜ǫ + δ
δ
the statement “ θ is an approximate global optimizer of U with value imprecision ˜ǫ and residual
domain ˜α ” holds with probability at least σ . Notice that ˜ǫ ∈ [0, 1] and ˜α ∈ (0, 1] are linked through
2+δ , 1] and ¯α ∈ (0, ˜ǫ+δ
a bijective relation to ρ ∈ [ 1+δ
1+δ ]. The statement of the theorem is eventually
obtained by expressing σ as a function of desired ˜ǫ = ǫ and ˜α = α.
(cid:3)

1 + δ
δ

,

References

[1] D. J. Wales. Energy Landscapes. Cambridge University Press, Cambridge, UK, 2003.
[2] D. Achlioptas, A. Naor, and Y. Peres. Rigorous location of phase transitions in hard optimization prob-
lems. Nature, 435:759–764, 2005.

Science,

[3] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by Simulated Annealing.
220(4598):671–680, 1983.
[4] E. Bonomi and J. Lutton. The N -city travelling salesman problem: statistical mechanics and the Metropo-
lis algorithm. SIAM Rev., 26(4):551–568, 1984.
[5] Y. Fu and P. W. Anderson. Application of statistical mechanics to NP-complete problems in combinatorial
optimization. J. Phys. A: Math. Gen., 19(9):1605–1620, 1986.
[6] M. M ´ezard, G. Parisi, and R. Zecchina. Analytic and Algorithmic Solution of Random Satis ﬁability
Problems. Science, 297:812–815, 2002.
[7] P. M. J. van Laarhoven and E. H. L. Aarts. Simulated Annealing: Theory and Applications. D. Reidel
Publishing Company, Dordrecht, Holland, 1987.
[8] D. Mitra, F. Romeo, and A. Sangiovanni-Vincentelli. Convergence and ﬁnite-time behavior of simulated
annealing. Adv. Appl. Prob., 18:747–771, 1986.
[9] B. Hajek. Cooling schedules for optimal annealing. Math. Oper. Res., 13:311–329, 1988.
[10] J. Hannig, E. K. P. Chong, and S. R. Kulkarni. Relative Frequencies of Generalized Simulated Annealing.
Math. Oper. Res., 31(1):199–216, 2006.
[11] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK, 2004.
[12] H. Haario and E. Saksman. Simulated annealing process in general state space. Adv. Appl. Prob., 23:866–
893, 1991.
[13] S. B. Gelfand and S. K. Mitter. Simulated Annealing Type Algorithms for Multivariate Optimization.
Algorithmica, 6:419–436, 1991.
[14] C. Tsallis and D. A. Stariolo. Generalized simulated annealing. Physica A, 233:395–406, 1996.
[15] M. Locatelli. Simulated Annealing Algorithms for Continuous Global Optimization: Convergence Con-
ditions. J. Optimiz. Theory App., 104(1):121–133, 2000.
[16] V. N. Vapnik. The Nature of Statistical Learning Theory. Cambridge University Press, Springer, New
York, US, 1995.
[17] M. Vidyasagar. Learning and Generalization: With Application to Neural Networks. Springer-Verlag,
London, second edition, 2003.
[18] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Springer-Verlag, London, 1993.
[19] J. S. Rosenthal. Minorization Conditions and Convergence Rates for Markov Chain Monte Carlo. J. Am.
Stat. Assoc., 90(430):558–566, 1995.
[20] K. L. Mengersen and R. L. Tweedie. Rates of convergence of the Hastings and Metropolis algorithm.
Ann. Stat., 24(1):101–121, 1996.
[21] G. O. Roberts and J. S. Rosenthal. General state space Markov chains and MCMC algorithms. Prob.
Surv., 1:20–71, 2004.
[22] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer-Verlag, New York, second edition,
2004.
[23] D.J. Spiegelhalter, K.R. Abrams, and J.P. Myles. Bayesian approaches to clinical trials and health-care
evaluation. John Wiley & Sons, Chichester, UK, 2004.
[24] A. Lecchini-Visintini, W. Glover, J. Lygeros, and J. M. Maciejowski. Monte Carlo Optimization for
Conﬂict Resolution in Air Trafﬁc Control.
IEEE Trans. Intell. Transp. Syst., 7(4):470–482, 2006.
[25] P. M ¨uller. Simulation based optimal design. In J. O. Berger, J. M. Bernardo, A. P. Dawid, and A. F. M.
Smith, editors, Bayesian Statistics 6: proceedings of the Sixth Valencia International Meeting, pages
459–474. Oxford: Clarendon Press, 1999.
[26] P. M ¨uller, B. Sans ´o, and M. De Iorio. Optimal Bayesian design by Inhomogeneous Markov Chain Simu-
lation. J. Am. Stat. Assoc., 99(467):788–798, 2004.
[27] L. Blum, C. Cucker, M. Shub, and S. Smale. Complexity and Real Computation. Springer-Verlag, New
York, 1998.
[28] M. Vidyasagar. Randomized algorithms for robust controller synthesis using statistical learning theory.
Automatica, 37(10):1515–1528, 2001.
[29] R. Tempo, G. Calaﬁore, and F. Dabbene. Randomized Algorithms for Analysis and Control of Uncertain
Systems. Springer-Verlag, London, 2005.
[30] B.V. Gnedenko. Theory of Probability. Chelsea, New York, fourth edition, 1968.


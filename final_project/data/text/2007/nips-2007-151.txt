SpAM: Sparse Additive Models

Pradeep Ravikumar† Han Liu†

John Lafferty∗ † Larry Wasserman‡

† Machine Learning Department
‡ Department of Statistics
∗Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

We present a new class of models for high-dimensional nonparametric regression
and classi ﬁcation called sparse additive models (SpAM). Ou r methods combine
ideas from sparse linear modeling and additive nonparametric regression. We de-
rive a method for ﬁtting the models that is effective even whe n the number of
covariates is larger than the sample size. A statistical analysis of the properties of
SpAM is given together with empirical results on synthetic and real data, show-
ing that SpAM can be effective in ﬁtting sparse nonparametri c models in high
dimensional data.

1

Introduction

Substantial progress has been made recently on the problem of ﬁtting high dimensional linear re-
gression models of the form Yi = X T
i β + i , for i = 1, . . . , n . Here Yi is a real-valued response, X i
is a p-dimensional predictor and i is a mean zero error term. Finding an estimate of β when p > n
that is both statistically well-behaved and computationally efﬁcient has proved challenging; how-
ever, the lasso estimator (Tibshirani (1996)) has been remarkably successful. The lasso estimator bβ
minimizes the `1 -penalized sums of squares
pXj =1
Xi
(Yi − X T
(1)
i β ) + λ
|β j |
with the `1 penalty kβ k1 encouraging sparse solutions, where many components bβ j are zero. The
good empirical success of this estimator has been recently backed up by results conﬁrming that it has
strong theoretical properties; see (Greenshtein and Ritov, 2004; Zhao and Yu, 2007; Meinshausen
and Yu, 2006; Wainwright, 2006).
The nonparametric regression model Yi = m ( X i )+ i , where m is a general smooth function, relaxes
the strong assumptions made by a linear model, but is much more challenging in high dimensions.
Hastie and Tibshirani (1999) introduced the class of additive models of the form
pXj =1
which is less general, but can be more interpretable and easier to ﬁt; in particular, an additive model
can be estimated using a coordinate descent Gauss-Seidel procedure called backﬁtting. An extension
of the additive model is the functional ANOVA model
m j ,k ( X i j , X i k ) + Xj <k<`
m j ( X i j ) + Xj <k
Yi = X1≤ j ≤ p
1

m j ,k ,` ( X i j , X i k , X i ` ) + · · · + i

(2)

Yi =

m j ( X i j ) + i

(3)

‡
†
which allows interactions among the variables. Unfortunately, additive models only have good
statistical and computational behavior when the number of variables p is not large relative to the
sample size n .

In this paper we introduce sparse additive models (SpAM) that extend the advantages of sparse linear
models to the additive, nonparametric setting. The underlying model is the same as in (2), but con-
straints are placed on the component functions {m j }1≤ j ≤ p to simultaneously encourage smoothness
of each component and sparsity across components; the penalty is similar to that used by the COSSO
of Lin and Zhang (2006). The SpAM estimation procedure we introduce allows the use of arbitrary
nonparametric smoothing techniques, and in the case where the underlying component functions are
linear, it reduces to the lasso. It naturally extends to classi ﬁcation problems using generalized addi-
tive models. The main results of the paper are (i) the formulation of a convex optimization problem
for estimating a sparse additive model, (ii) an efﬁcient bac kﬁtting algorithm for constructing the
estimator, (iii) simulations showing the estimator has excellent behavior on some simulated and real
data, even when p is large, and (iv) a statistical analysis of the theoretical properties of the estimator
that support its good empirical performance.

2 The SpAM Optimization Problem

In this section we describe the key idea underlying SpAM. We ﬁ rst present a population version
of the procedure that intuitively suggests how sparsity is achieved. We then present an equivalent
convex optimization problem. In the following section we derive a backﬁtting procedure for solving
this optimization problem in the ﬁnite sample setting.
To motivate our approach, we ﬁrst consider a formulation tha t scales each component function g j
by a scalar β j , and then imposes an `1 constraint on β = (β1 , . . . , β p )T . For j ∈ {1, . . . , p}, let H j
denote the Hilbert space of measurable functions f j (x j ) of the single scalar variable x j , such that
E( f j ( X j )) = 0 and E( f j ( X j )2 ) < ∞, furnished with the inner product
D f j , f 0j E = E (cid:16) f j ( X j ) f 0j ( X j )(cid:17) .
(4)
Let Hadd = H1 + H2 + . . . , H p denote the Hilbert space of functions of (x1 , . . . , x p ) that have
f (x ) = P j f j (x j ). The standard additive model optimization problem, in the
an additive form:
population setting, is
E (cid:16)Y − P p
j =1 f j ( X j )(cid:17)2
min
(5)
f j ∈H j , 1≤ j ≤ p
and m (x ) = E(Y | X = x ) is the unknown regression function. Now consider the following modi ﬁ-
cation of this problem that imposes additional constraints:
j =1 β j g j ( X j )(cid:17)2
E (cid:16)Y − P p
pXj =1
|β j | ≤ L
E (cid:16)g 2
j (cid:17) = 1, j = 1, . . . , p
E (cid:0)g j (cid:1) = 0, j = 1, . . . , p
noting that g j is a function while β is a vector. Intuitively, the constraint that β lies in the `1 -ball
{β : kβ k1 ≤ L } encourages sparsity of the estimated β , just as for the parametric lasso. When β is
sparse, the estimated additive function f (x ) = P p
j =1 f j (x j ) = P p
j =1 β j g j (x j ) will also be sparse,
meaning that many of the component functions f j (·) = β j g j (·) are identically zero. The constraints
(6c) and (6c) are imposed for identi ﬁability; without (6c),
for example, one could always satisfy (6a)
by rescaling.
While this optimization problem makes plain the role `1 regularization of β to achieve sparsity, it has
the unfortunate drawback of not being convex. More speci ﬁca lly, while the optimization problem is
convex in β and {g j } separately, it is not convex in β and {g j } jointly.

min
β ∈R p ,g j ∈H j
subject to

(6b)

( P )

(6a)

(6c)

(6d)

2

(Q )

(7a)

(7b)

subject to

min
f j ∈H j

However, consider the following related optimization problem:
E (cid:16)Y − P p
j =1 f j ( X j )(cid:17)2
pXj =1 qE( f 2
j ( X j )) ≤ L
(7c)
E( f j ) = 0, j = 1, . . . , p .
This problem is convex in { f j }. Moreover, the solutions to problems ( P ) and (Q ) are equivalent:
(cid:16)nβ ∗j o , ng∗j o(cid:17) optimizes ( P ) implies n f ∗j = β ∗j g∗j o optimizes (Q );
n f ∗j o optimizes (Q ) implies (cid:16)nβ ∗j = (k f j k2 )T o , ng∗j = f ∗j /k f ∗j k2o(cid:17) optimizes ( P ).
While optimization problem (Q ) has the important virtue of being convex, the way it encourages
sparsity is not intuitive; the following observation provides some insight. Consider the set C ⊂ R4
deﬁned by C = (cid:26)( f 11 , f 12 , f 21 , f 22 )T ∈ R4 : q f 2
22 ≤ L (cid:27). Then the projec-
12 + q f 2
11 + f 2
21 + f 2
tion π12C onto the ﬁrst two components is an `2 ball. However, the projection π13C onto the ﬁrst
and third components is an `1 ball. In this way, it can be seen that the constraint P j (cid:13)(cid:13) f j (cid:13)(cid:13)2 ≤ L
acts as an `1 constraint across components to encourage sparsity, while it acts as an `2 constraint
within components to encourage smoothness, as in a ridge regression penalty. It is thus crucial that
the norm (cid:13)(cid:13) f j (cid:13)(cid:13)2 appears in the constraint, and not its square (cid:13)(cid:13) f j (cid:13)(cid:13)2
2 . For the purposes of sparsity,
this constraint could be replaced by P j (cid:13)(cid:13) f j (cid:13)(cid:13)q ≤ L for any q ≥ 1.
In case each f j is linear,
( f j (x1 j ), . . . , f (xn j )) = β j (x1 j , . . . , xn j ), the optimization problem reduces to the lasso.
The use of scaling coefﬁcients together with a nonnegative g arrote penalty, similar to our problem
( P ), is considered by Yuan (2007). However, the component functions g j are ﬁxed, so that the
procedure is not asymptotically consistent. The form of the optimization problem (Q ) is similar
to that of the COSSO for smoothing spline ANOVA models (Lin and Zhang, 2006); however, our
method differs signi ﬁcantly from the COSSO, as discussed be low.
In particular, our method is
scalable and easy to implement even when p is much larger than n .

3 A Back ﬁtting Algorithm for SpAM

We now derive a coordinate descent algorithm for ﬁtting a spa rse additive model. We assume that
we observe Y = m ( X ) +  , where  is mean zero Gaussian noise. We write the Lagrangian for the
optimization problem (Q ) as
pXj =1 qE( f 2
j =1 f j ( X j )(cid:17)2
E (cid:16)Y − P p
j ( X j )) + Xj
1
(8)
L( f , λ , µ) =
µ j E( f j ).
+ λ
2
Let R j = Y − Pk 6= j f k ( X k ) be the j th residual. The stationary condition for minimizing L as a
function of f j , holding the other components f k ﬁxed for k 6= j , is expressed in terms of the Frechet
derivative δL as
δL( f , λ , µ; δ f j ) = E (cid:2)( f j − R j + λv j )δ f j (cid:3) = 0
(9)
for any δ f j ∈ H j satisfying E(δ f j ) = 0, where v j ∈ ∂qE( f 2
j ) is an element of the subgradient,
satisfying qEv 2
j ≤ 1 and v j = f j .qE( f 2
j ) if E( f 2
j ) 6= 0. Therefore, conditioning on X j , the
stationary condition (9) implies
(10)
f j + λv j = E( R j | X j ).
Letting P j = E[ R j | X j ] denote the projection of the residual onto H j , the solution satis ﬁes


λ
if E( P 2
 f j = P j
1 +
qE( f 2
j ) > λ
j )
3

(11)

Input: Data ( X i , Yi ), regularization parameter λ.
Initialize f j = f (0)
, for j = 1, . . . , p .
j
Iterate until convergence:
For each j = 1, . . . , p :
Compute the residual: R j = Y − Pk 6= j f k ( X k );
Estimate the projection P j = E[ R j | X j ] by smoothing: bP j = S j R j ;
Estimate the norm s j = qE[ P j ]2 using, for example, (15) or (35);
bs j (cid:21)+ bP j ;
Soft-threshold: f j = (cid:20)1 −
λ
Center: f j ← f j − mean( f j ).
Output: Component functions f j and estimator bm ( X i ) = P j f j ( X i j ).
Figure 1: TH E S PAM BACK FIT T ING A LGOR I THM
and f j = 0 otherwise. Condition (11), in turn, implies


j ) = qE( P 2
j ) = qE( P 2
j ) or qE( f 2
 qE( f 2
λ
1 +
qE( f 2
j ) − λ .
j )
Thus, we arrive at the following multiplicative soft-thresholding update for f j :

f j = 
λ
1 −
+
qE( P 2
j )
where [·]+ denotes the positive part. In the ﬁnite sample case, as in sta ndard backﬁtting (Hastie and
Tibshirani, 1999), we estimate the projection E[ R j | X j ] by a smooth of the residuals:
bP j = S j R j
(14)
where S j is a linear smoother, such as a local linear or kernel smoother. Let bs j be an estimate of
qE[ P 2
j ]. A simple but biased estimate is
√n k bP j k2 = qmean( bP 2
1
(15)
bs j =
j ).
More accurate estimators are possible; an example is given in the appendix. We have thus derived
the SpAM backﬁtting algorithm given in Figure 1.
While the motivating optimization problem (Q ) is similar to that considered in the COSSO (Lin
and Zhang, 2006) for smoothing splines, the SpAM backﬁtting algorithm decouples smoothing and
sparsity, through a combination of soft-thresholding and smoothing. In particular, SpAM backﬁtting
can be carried out with any nonparametric smoother; it is not restricted to splines. Moreover, by
iteratively estimating over the components and using soft thresholding, our procedure is simple to
implement and scales to high dimensions.

(12)

P j

(13)

3.1 SpAM for Nonparametric Logistic Regression

The SpAM backﬁtting procedure can be extended to nonparamet ric logistic regression for classi ﬁ-
cation. The additive logistic model is
j =1 f j ( X j )(cid:17)
exp (cid:16)P p
1 + exp (cid:16)P p
j =1 f j ( X j )(cid:17)

P(Y = 1 | X ) ≡ p( X ; f ) =

(16)

4

(18)

(17)

( Z , X )

where Y ∈ {0, 1}, and the population log-likelihood is `( f ) = E (cid:2)Y f ( X ) − log (1 + exp f ( X ))(cid:3).
Recall that in the local scoring algorithm for generalized additive models (Hastie and Tibshirani,
1999) in the logistic case, one runs the backﬁtting procedur e within Newton’s method. Here one
iteratively computes the transformed response for the current estimate f 0
Yi − p( X i ; f 0 )
Z i = f 0 ( X i ) +
p( X i ; f 0 )(1 − p( X i ; f 0 ))
and weights w( X i ) = p( X i ; f 0 )(1 − p( X i ; f 0 ), and carries out a weighted backﬁtting of
with weights w . The weighted smooth is given by
S j (w R j )
bP j =
S j w
To incorporate the sparsity penalty, we ﬁrst note that the La grangian is given by
pXj =1 qE( f 2
j ( X j )) + Xj
L( f , λ , µ) = E (cid:2)log (1 + exp f ( X )) − Y f ( X )(cid:3) + λ
(19)
µ j E( f j )
and the stationary condition for component function f j is E (cid:0) p − Y | X j (cid:1) + λv j = 0 where v j is an
element of the subgradient ∂qE( f 2
j ). As in the unregularized case, this condition is nonlinear in f ,
and so we linearize the gradient of the log-likelihood around f 0 . This yields the linearized condition
E (cid:2)w( X )( f ( X ) − Z ) | X j (cid:3) + λv j = 0. When E( f 2
j ) 6= 0, this implies the condition

qE( f j )2 
E (cid:0)w | X j (cid:1) +
λ
 f j ( X j ) = E(w R j | X j ).
In the ﬁnite sample case, in terms of the smoothing matrix S j , this becomes
S j (w R j )
f j =
S j w + λ .qE( f 2
.
j )
If kS j (w R j )k2 < λ, then f j = 0. Otherwise, this implicit, nonlinear equation for f j cannot be
solved explicitly, so we propose to iterate until convergence:
S j (w R j )
f j ←
S j w + λ√n (cid:14)k f j k2
When λ = 0, this yields the standard local scoring update (18). An example of logistic SpAM is
given in Section 5.

(20)

(21)

(22)

.

.

4 Properties of SpAM

4.1 SpAM is Persistent

The notion of risk consistency, or persistence, was studied by Juditsky and Nemirovski (2000) and
Greenshtein and Ritov (2004) in the context of linear models. Let ( X , Y ) denote a new pair (inde-
pendent of the observed data) and deﬁne the predictive risk w hen predicting Y with f ( X ) by
R ( f ) = E(Y − f ( X ))2 .
(23)
Since we consider predictors of the form f (x ) = P j β j g j (x j ) we also write the risk as R (β , g )
where β = (β1 , . . . , β p ) and g = (g1 , . . . , g p ). Following Greenshtein and Ritov (2004), we say
that an estimator bm n is persistent relative to a class of functions Mn if
P
R (bm n ) − R (m ∗n )
(24)
→ 0
where m ∗n = argmin f ∈Mn R ( f ) is the predictive oracle. Greenshtein and Ritov (2004) showed
that the lasso is persistent for the class of linear models Mn = { f (x ) = x T β : kβ k1 ≤ L n } if
L n = o((n/ log n )1/4 ). We show a similar result for SpAM.
Theorem 4.1. Suppose that pn ≤ en ξ for some ξ < 1. Then SpAM is persistent relative to the
j =1 β j g j (x j ) : kβ k1 ≤ L n o if L n = o (cid:0)n (1−ξ )/4 (cid:1).
classof additivemodelsMn = n f (x ) = P p
5

min
β

+ λn

j  T
β T
j  j β j

4.2 SpAM is Sparsistent
In the case of linear regression, with m j ( X j ) = β T
j X j , Wainwright (2006) shows that under certain
conditions on n , p , s = |supp(β )|, and the design matrix X , the lasso recovers the sparsity pattern
asymptotically; that is, the lasso estimator bβn is sparsistent: P (cid:0)supp(β ) = supp(bβn )(cid:1) → 1. We
show a similar result for SpAM with the sparse backﬁtting pro cedure.
For the purpose of analysis, we use orthogonal function regression as the smoothing procedure. For
each j = 1, . . . , p let ψ j be an orthogonal basis for H j . We truncate the basis to ﬁnite dimension
dn , and let dn → ∞ such that dn / n → 0. Let  j denote the n × d matrix  j (i , k ) = ψ j k ( X i j ).
If A ⊂ {1, . . . , p}, we denote by  A the n × d | A| matrix where for each i ∈ A, i appears as a
submatrix in the natural way. The SpAM optimization problem can then be written as
pXj =1 r 1
2n (cid:16)Y − P p
j =1  j β j (cid:17)2
1
n
where each β j is a d -dimensional vector. Let S denote the true set of variables { j : m j
6= 0}, with
s = | S |, and let S c denote its complement. Let bSn = { j : bβ j
6= 0} denote the estimated set of
variables from the minimizer bβn of (25).
Theorem 4.2. Suppose that  satisﬁes the conditions
S  S (cid:19) ≤ Cmax < ∞ and min (cid:18) 1
max (cid:18) 1
S  S (cid:19) ≥ Cmin > 0
 T
 T
n
n
2 ≤ s Cmin
S  S (cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) 1
S c  S (cid:17) (cid:16) 1
2
1 − δ
n  T
n  T
, for some 0 < δ ≤ 1
√s
Cmax
Let the regularizationparameter λn → 0 be chosen to satisfy
λn ps dn → 0,
dn (log dn + log( p − s ))
s
dn λn → 0,
and
nλ2
n
ThenSpAM is sparsistent: P (cid:0)bSn = S (cid:1) −→ 1.
5 Experiments
In this section we present experimental results for SpAM applied to both synthetic and real data,
including regression and classi ﬁcation examples that illu strate the behavior of the algorithm in vari-
ous conditions. We ﬁrst use simulated data to investigate th e performance of the SpAM backﬁtting
algorithm, where the true sparsity pattern is known. We then apply SpAM to some real data. If not
explicitly stated otherwise, the data are always rescaled to lie in a d -dimensional cube [0, 1]d , and
a kernel smoother with Gaussian kernel is used. To tune the penalization parameter λ, we use a C p
statistic, which is deﬁned as
pXj =1
nXi =1 (cid:16)Yi − P p
j =1 bf j ( X j )(cid:17)2
2bσ 2
1
C p ( bf ) =
trace(S j ) 1[ bf j 6= 0]
+
n
n
where S j is the smoothing matrix for the j -th dimension and bσ 2 is the estimated variance.
5.1 Simulations
We ﬁrst apply SpAM to an example from (Härdle et al., 2004). A d ataset with sample size n = 150
is generated from the following 200-dimensional additive model:
(30)
Yi = f 1 (x i 1 ) + f 2 (x i 2 ) + f 3 (x i 3 ) + f 4 (x i 4 ) + i
f 2 (x ) = x 2 − 1
f 3 (x ) = x − 1
f 4 (x ) = e−x + e−1 − 1
(31)
f 1 (x ) = −2 sin(2x ),
3 ,
2 ,
and f j (x ) = 0 for j ≥ 5 with noise i ∼ N (0, 1). These data therefore have 196 irrelevant
dimensions. The results of applying SpAM with the plug-in bandwidths are summarized in Figure 2.

→ 0.

(25)

(26)

(27)

(28)

(29)

6

4
1

2
1

0
1

3

4

2

4
9

9

4
9
1

p
8
C

6

4

2

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

l1=97.05

l1=88.36

l1=90.65

l1=79.26

6

4

2

2
m

2
−

4
−

6

4

2

3
m

2
−

4
−

6

4

2
4
m

2
−

0
.
1

y
8
r
e
.
0
v
o
c
e
6
r
.
 
0
t
c
e
r
r
o
4
c
.
0
 
f
o
 
.
b
2
o
.
r
0
p

0
.
0

6

4

2

5
m

2
−

4
−

p=128

p=256

0 10 20 30 40 50 60 70 80 90
sample size
zero

zero

110 130 150

6

4

2

6
m

2
−

4
−

6
.
0

5
s
.
m
0
r
o
4
N
.
0
 
t
n
e
3
n
.
0
o
p
m
2
.
o
0
C

1
.
0

0
.
0

4

2

1
m

2
−

4
−

4
−

6
6
6
−
−
−
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
x1
x2
x3
x4
x5
x6
Figure 2: (Simulated data) Upper left: The empirical `2 norm of the estimated components as plotted
against the tuning parameter λ; the value on the x -axis is proportional to P j k bf j k2 . Upper center:
The C p scores against the tuning parameter λ; the dashed vertical line corresponds to the value of
λ which has the smallest C p score. Upper right: The proportion of 200 trials where the correct
relevant variables are selected, as a function of sample size n . Lower (from left to right): Estimated
(solid lines) versus true additive component functions (dashed lines) for the ﬁrst 6 dimensions; the
remaining components are zero.

5.2 Boston Housing

The Boston housing data was collected to study house values in the suburbs of Boston; there are
altogether 506 observations with 10 covariates. The dataset has been studied by many other authors
(Härdle et al., 2004; Lin and Zhang, 2006), with various tran sformations proposed for different
covariates. To explore the sparsistency properties of our method, we add 20 irrelevant variables. Ten
of them are randomly drawn from Uniform(0, 1), the remaining ten are a random permutation of the
original ten covariates, so that they have the same empirical densities.

The full model (containing all 10 chosen covariates) for the Boston Housing data is:
medv = α + f 1 (crim) + f 2 (indus) + f 3 (nox) + f 4 (rm) + f 5 (age)
+ f 6 (dis) + f 7 (tax) + f 8 (ptratio) + f 9 (b) + f 10 (lstat)
The result of applying SpAM to this 30 dimensional dataset is shown in Figure 3. SpAM identi ﬁes 6
nonzero components. It correctly zeros out both types of irrelevant variables. From the full solution
path, the important variables are seen to be rm, lstat, ptratio, and crim. The importance
of variables nox and b are borderline. These results are basically consistent with those obtained
by other authors (Härdle et al., 2004). However, using C p as the selection criterion, the variables
indux, age, dist, and tax are estimated to be irrelevant, a result not seen in other studies.

(32)

5.3 SpAM for Spam

Here we consider an email spam classi ﬁcation problem, using the logistic SpAM backﬁtting algo-
rithm from Section 3.1. This dataset has been studied by Hastie et al. (2001), using a set of 3,065
emails as a training set, and conducting hypothesis tests to choose signi ﬁcant variables; there are a
total of 4,601 observations with p = 57 attributes, all numeric. The attributes measure the percent-
age of speci ﬁc words or characters in the email, the average a nd maximum run lengths of upper case
letters, and the total number of such letters. To demonstrate how SpAM performs well with sparse
data, we only sample n = 300 emails as the training set, with the remaining 4301 data points used
as the test set. We also use the test data as the hold-out set to tune the penalization parameter λ. The
results of a typical run of logistic SpAM are summarized in Figure 4, using plug-in bandwidths.

7

l1=177.14

l1=1173.64

0
2

0
1
1
m

0
1
−

0
2

0
1
4
m

0
1
−

0.0 0.2 0.4 0.6 0.8 1.0
x1
l1=478.29

0.0 0.2 0.4 0.6 0.8 1.0
x4
l1=1221.11

0
2

0
1
8
m

0
2

0
1
0
1
m

0
1
−

3
s
m
r
o
N
 
t
n
2
e
n
o
p
m
o
1
C

0

4

0
1

8

3

6

5

7

7
1

0
8

0
7

0
6

p
C
0
5

0
4

0
3

0
2

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0
1
−

Figure 3: (Boston housing) Left: The empirical `2 norm of the estimated components versus the
regularization parameter λ. Center: The C p scores against λ; the dashed vertical line corresponds to
best C p score. Right: Additive ﬁts for four relevant variables.

0.0 0.2 0.4 0.6 0.8 1.0
x8

0.0 0.2 0.4 0.6 0.8 1.0
x10

λ(×10−3 )
5 . 5
5 . 0
4 . 5
4 . 0
3 . 5
3 . 0
2 . 5
2 . 0

ERROR
0 . 2 0 0 9
0 . 1 7 2 5
0 . 1 3 5 4
0 . 1 0 8 3 (√)
0 . 1 1 1 7
0 . 1 1 7 4
0 . 1 2 5 1
0 . 1 2 5 9

# Z ERO S
5 5
5 1
4 6
2 0
0
0
0
0

S E L EC T ED VAR IAB L E S
{ 8 ,54 }
{ 8 , 9 , 27 , 53 , 54 , 57 }
{7 , 8 , 9 , 17 , 18 , 27 , 53 , 54 , 57 , 58 }
{4 , 6 –10 , 14 –22 , 26 , 27 , 38 , 53 –58 }
A L L
A L L
A L L
A L L

0
2
.
0

r
o
8
r
1
r
e
.
0
 
n
o
i
t
6
c
1
i
d
.
0
e
r
p
 
l
4
a
1
c
.
0
i
r
i
p
m
2
E
1
.
0

3.0
4.5
4.0
3.5
penalization parameter
Figure 4: (Email spam) Classi ﬁcation accuracies and variab le selection for logistic SpAM.

2.0

2.5

5.0

5.5

6 Acknowlegments

This research was supported in part by NSF grant CCF-0625879 and a Siebel Scholarship to PR.

References

GR E EN SH T E IN , E . and R I TOV, Y. (2004). Persistency in high dimensional linear predictor-selection and the
virtue of over-parametrization. Journal of Bernoulli 10 971–988.
H ÄRD L E , W. , M Ü L L ER , M . , S P ER L ICH , S . and W ERWAT Z , A . (2004). Nonparametric and Semiparametric
Models. Springer-Verlag Inc.
HA S T I E , T. and T IB SH IRAN I , R . (1999). Generalized additive models. Chapman & Hall Ltd.
HA S T I E , T. , T IB SH IRAN I , R . and FR I EDMAN , J . H . (2001). The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer-Verlag.
JUD I T SKY, A . and N EM IROV SK I , A . (2000). Functional aggregation for nonparametric regression. Ann.
Statist. 28 681–712.
L IN , Y. and ZHANG , H . H . (2006). Component selection and smoothing in multivariate nonparametric regres-
sion. Ann. Statist. 34 2272–2297.
M E IN SHAU S EN , N . and YU , B . (2006). Lasso-type recovery of sparse representations for high-dimensional
data. Tech. Rep. 720, Department of Statistics, UC Berkeley.
T IB SH IRAN I , R . (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, Methodological 58 267–288.
WA INWR IGH T, M . (2006). Sharp thresholds for high-dimensional and noisy recovery of sparsity. Tech. Rep.
709, Department of Statistics, UC Berkeley.
YUAN , M . (2007). Nonnegative garrote component selection in functional ANOVA models. In Proceedings of
AI and Statistics, AISTATS.
ZHAO , P. and YU , B . (2007). On model selection consistency of lasso. J. of Mach. Learn. Res. 7 2541–2567.

8


Temporal Difference Updating
without a Learning Rate

Marcus Hutter
RSISE@ANU and SML@NICTA
Canberra, ACT, 0200, Australia
marcus@hutter1.net
www.hutter1.net

Shane Legg
IDSIA, Galleria 2, Manno-Lugano CH-6928, Switzerland
shane@vetta.org
www.vetta.org/shane

Abstract

We derive an equation for temporal difference learning from statistical principles.
Speci ﬁcally, we start with the variational principle and th en bootstrap to produce
an updating rule for discounted state value estimates. The resulting equation is
similar to the standard equation for temporal difference learning with eligibil-
ity traces, so called TD(λ), however it lacks the parameter α that speci ﬁes the
learning rate. In the place of this free parameter there is now an equation for the
learning rate that is speci ﬁc to each state transition. We ex perimentally test this
new learning rule against TD(λ) and ﬁnd that it offers superior performance in
various settings. Finally, we make some preliminary investigations into how to
extend our new temporal difference algorithm to reinforcement learning. To do
this we combine our update equation with both Watkins’ Q(λ) and Sarsa(λ) and
ﬁnd that it again offers superior performance without a lear ning rate parameter.

1

Introduction

In the ﬁeld of reinforcement learning, perhaps the most popu lar way to estimate the future discounted
reward of states is the method of temporal difference learning. It is unclear who exactly introduced
this ﬁrst, however the ﬁrst explicit version of temporal dif
ference as a learning rule appears to be
Witten [9]. The idea is as follows: The expected future discounted reward of a state s is,
V s := E (cid:8)rk + γ rk+1 + γ 2 rk+2 + · · · |sk = s(cid:9) ,
where the rewards rk , rk+1 , . . . are geometrically discounted into the future by γ < 1. From this
deﬁnition it follows that,
V s = E (cid:8)rk + γV sk+1 |sk = s(cid:9) .
s of V s for each state s. The only information we
Our task, at time t, is to compute an estimate V t
have to base this estimate on is the current history of state transitions, s1 , s2 , . . . , st , and the current
history of observed rewards, r1 , r2 , . . . , rt . Equation (1) suggests that at time t + 1 the value of
st then perhaps
s should be: If it is higher than V t
rt + γVst+1 provides us with information on what V t
this estimate should be increased, and vice versa. This intuition gives us the following estimation
heuristic for state st ,
st (cid:17) ,
st + α (cid:16)rt + γV t
V t+1
st+1 − V t
:= V t
st
where α is a parameter that controls the rate of learning. This type of temporal difference learning
is known as TD(0).

(1)

1

One shortcoming of this method is that at each time step the value of only the last state st is updated.
States before the last state are also affected by changes in the last state’s value and thus these could
be updated too. This is what happens with so called temporal difference learning with eligibility
traces, where a history, or trace, is kept of which states have been recently visited. Under this
method, when we update the value of a state we also go back through the trace updating the earlier
states as well. Formally, for any state s its eligibility trace is computed by,
if s 6= st ,
s := (cid:26) γλE t−1
E t
s
if s = st ,
γλE t−1
s + 1
where λ is used to control the rate at which the eligibility trace is discounted. The temporal differ-
ence update is then, for all states s,
st (cid:17) .
s (cid:16)r + γV t
V t+1
st+1 − V t
s + αE t
:= V t
s
This more powerful version of temporal different learning is known as TD(λ) [7].
The main idea of this paper is to derive a temporal difference rule from statistical principles and
compare it to the standard heuristic described above. Super ﬁcially, our work has some similarities
to LSTD(λ) ([2] and references therein). However LSTD is concerned with ﬁnding a least-squares
linear function approximation, it has not yet been developed for general γ and λ, and has update time
quadratic in the number of features/states. On the other hand, our algorithm “exactly” coincides
with TD/Q/Sarsa(λ) for ﬁnite state spaces, but with a novel learning rate deriv ed from statistical
principles. We therefore focus our comparison on TD/Q/Sarsa. For a recent survey of methods to
set the learning rate see [1].

(2)

In Section 2 we derive a least squares estimate for the value function. By expressing the estimate as
an incremental update rule we obtain a new form of TD(λ), which we call HL(λ). In Section 3 we
compare HL(λ) to TD(λ) on a simple Markov chain. We then test it on a random Markov chain in
Section 4 and a non-stationary environment in Section 5. In Section 6 we derive two new methods
for policy learning based on HL(λ), and compare them to Sarsa(λ) and Watkins’ Q(λ) on a simple
reinforcement learning problem. Section 7 ends the paper with a summary and some thoughts on
future research directions.

2 Derivation

vk :=

γ u−k ru ,

The empirical future discounted reward of a state sk is the sum of actual rewards following from
state sk in time steps k , k + 1, . . ., where the rewards are discounted as they go into the future.
Formally, the empirical value of state sk at time k for k = 1, ..., t is,
∞
Xu=k
where the future rewards ru are geometrically discounted by γ < 1. In practice the exact value of
vk is always unknown to us as it depends not only on rewards that have been already observed, but
also on unknown future rewards. Note that if sm = sn for m 6= n, that is, we have visited the same
state twice at different times m and n, this does not imply that vn = vm as the observed rewards
following the state visit may be different each time.
s should be as close as possible to the true expected
Our goal is that for each state s the estimate V t
future discounted reward V s . Thus, for each state s we would like Vs to be close to vk for all k such
that s = sk . Furthermore, in non-stationary environments we would like to discount old evidence
by some parameter λ ∈ (0, 1]. Formally, we want to minimise the loss function,
t
1
sk (cid:1)2
Xk=1
λt−k (cid:0)vk − V t
2
For stationary environments we may simply set λ = 1 a priori.
As we wish to minimise this loss, we take the partial derivative with respect to the value estimate of
each state and set to zero,
t
Xk=1
sk (cid:1)δsk s = V t
λt−k (cid:0)vk − V t
s

λt−k δsk s vk = 0,

λt−k δsk s −

∂L
∂V t
s

= −

L :=

.

t
Xk=1

(3)

(4)

t
Xk=1

2

s due to the presence of the Kronecker δsk s , deﬁned δxy := 1 if
where we could change V t
sk into V t
s := Pt
k=1 λt−k δsk s we get
x = y , and 0 otherwise. By deﬁning a discounted state visit counte r N t

t
Xk=1
Since vk depends on future rewards rk , Equation (5) can not be used in its current form. Next we
note that vk has a self-consistency property with respect to the rewards. Speci ﬁcally, the tail of the
future discounted reward sum for each state depends on the empirical value at time t in the following
way,

λt−k δsk s vk .

s N t
V t
s =

(5)

t−1
Xu=k
Substituting this into Equation (5) and exchanging the order of the double sum,

γ u−k ru + γ t−k vt .

vk =

=

s N t
V t
s =

λt−k δsk sγ t−k vt

λt−k δsk sγ u−k ru +

u
Xk=1
λt−u

t−1
t
Xk=1
Xu=1
t−1
t
u
Xu=1
Xk=1
Xk=1
(λγ )t−k δsk s vt
(λγ )u−k δsk s ru +
= Rt
s + E t
s vt ,
s := Pt−1
s := Pt
s ru is
k=1 (λγ )t−k δsk s is the eligibility trace of state s, and Rt
where E t
u=1 λt−uE u
the discounted reward with eligibility.
s depend only on quantities known at time t. The only unknown quantity is vt , which we
s and Rt
E t
st . In other words, we
have to replace with our current estimate of this value at time t, which is V t
bootstrap our estimates. This gives us,

(6)
s + E t
s = Rt
s N t
sV t
V t
st .
st ). Substituting this back into Equation (6)
For state s = st , this simpli ﬁes to V t
st − E t
st /(N t
st = Rt
we obtain,

V t
s N t
s = Rt
s + E t
s

Rt
st
st − E t
N t
st
This gives us an explicit expression for our V estimates. However, from an algorithmic perspective
an incremental update rule is more convenient. To derive this we make use of the relations,
N t+1
E t+1
Rt+1
s = λN t
s = λγE t
s = λRt
s + λE t
s + δst+1 s ,
s + δst+1 s ,
s rt ,
s = 0. Inserting these into Equation (7) with t replaced by t + 1,
with N 0
s = E 0
s = R0
Rt+1
st+1
st+1 − E t+1
N t+1
st+1
Rt
st+1 + E t
st+1 rt
st+1 − γE t
N t
st+1

s rt + E t+1
= λRt
s + λE t
s

s + E t+1
= Rt+1
s

s N t+1
V t+1
s

(7)

.

.

s and substituting back in,
By solving Equation (6) for Rt
st + E t
st+1 V t
st+1 − E t
st+1 V t
N t
st+1 rt
s rt + E t+1
V t+1
s N t+1
st (cid:1) + λE t
s = λ(cid:0)V t
s − E t
s N t
sV t
s
N t
st+1 − γE t
st+1
s + δst+1 s (cid:1)V t
= (cid:0)λN t
s − λE t
s − δst+1 sV t
sV t
st + λE t
s rt
N t
st+1 V t
st+1 − E t
st+1 V t
st + E t
st+1 rt
+ E t+1
s
st+1 − γE t
N t
st+1

.

Dividing through by N t+1
s

s + δst+1 s ),
(= λN t
st + λE t
sV t
s − λE t
−δst+1 sV t
s rt
λN t
s + δst+1 s

V t+1
s = V t
s +

3

+

st + E t
st+1 V t
st+1 − E t
st+1 V t
s + δst+1 s )(N t
(λγE t
st+1 rt )
st+1 )(λN t
st+1 − γE t
(N t
s + δst+1 s )

.

+

+

V t+1
s = V t
s +

Making the ﬁrst denominator the same as the second, then expa nding the numerator,
st+1 E t
st+1 − λγE t
s N t
st+1 − δst+1 sV t
st N t
sV t
st+1 − λE t
s rtN t
λE t
s rt
st+1 )(λN t
st+1 − γE t
(N t
s + δst+1 s )
st+1 V t
sE t
st+1 − λγE t
st+1 V t
sN t
s δst+1 s + λγE t
st+1 V t
st + γE t
sV t
st+1 E t
λγE t
st
st+1 )(λN t
st+1 − γE t
(N t
s + δst+1 s )
st + δst+1 sE t
st+1 V t
st+1 − δst+1 sE t
st+1 V t
st+1 rt + δst+1 sN t
sE t
λγE t
st+1 rt
st+1 )(λN t
st+1 − γE t
(N t
s + δst+1 s )
After cancelling equal terms (keeping in mind that in every term with a Kronecker δxy factor we
s we obtain,
may assume that x = y as the term is always zero otherwise), and factoring out E t
st + δst+1 s rt (cid:1)
s (cid:0)λrtN t
st+1 − δst+1 sV t
st+1 V t
s δst+1 s + λγN t
st+1 + γV t
st N t
st+1 − λV t
E t
st+1 )(λN t
st+1 − γE t
(N t
s + δst+1 s )
st+1 + δst+1 s we obtain our update rule,
Finally, by factoring out λN t

V t+1
s = V t
s +

.

V t+1
s βt (s, st+1 ) (cid:0)rt + γV t
s = V t
s + E t
st+1 − V t
st (cid:1),
where the learning rate is given by,
N t
1
st+1
st+1 − γE t
N t
N t
st+1
s

βt (s, st+1 ) :=

.

(8)

(9)

Examining Equation (8), we ﬁnd the usual update equation for
temporal difference learning with eli-
gibility traces (see Equation (2)), however the learning rate α has now been replaced by βt (s, st+1 ).
This learning rate was derived from statistical principles by minimising the squared loss between
the estimated and true state value. In the derivation we have exploited the fact that the latter must be
self-consistent and then bootstrapped to get Equation (6). This gives us an equation for the learning
rate for each state transition at time t, as opposed to the standard temporal difference learning where
the learning rate α is either a ﬁxed free parameter for all transitions, or is dec reased over time by
some monotonically decreasing function. In either case, the learning rate is not automatic and must
be experimentally tuned for good performance. The above derivation appears to theoretically solve
this problem.
The ﬁrst term in βt seems to provide some type of normalisation to the learning rate, though the
intuition behind this is not clear to us. The meaning of second term however can be understood as
s measures how often we have visited state s in the recent past. Therefore, if N t
follows: N t
s ≪
then state s has a value estimate based on relatively few samples, while state st+1 has a
N t
st+1
value estimate based on relatively many samples. In such a situation, the second term in βt boosts
the learning rate so that V t+1
moves more aggressively towards the presumably more accurate
s
. In the opposite situation when st+1 is a less visited state, we see that the reverse occurs
rt + γV t
st+1
and the learning rate is reduced in order to maintain the existing value of Vs .

3 A simple Markov process

For our ﬁrst test we consider a simple Markov process with 51 s tates. In each step the state number
is either incremented or decremented by one with equal probability, unless the system is in state 0
or 50 in which case it always transitions to state 25 in the following step. When the state transitions
from 0 to 25 a reward of 1.0 is generated, and for a transition from 50 to 25 a reward of -1.0 is
generated. All other transitions have a reward of 0. We set the discount value γ = 0.99 and then
computed the true discounted value of each state by running a brute force Monte Carlo simulation.

We ran our algorithm 10 times on the above Markov chain and computed the root mean squared
error in the value estimate across the states at each time step averaged across each run. The optimal

4

0.40

0.35

0.30

0.25

0.20

0.15

0.10

E
S
M
R

HL(1.0)
TD(0.9) a = 0.1
TD(0.9) a = 0.2

0.40

0.35

0.30

0.25

0.20

0.15

0.10

E
S
M
R

HL(1.0)
TD(0.9) a = 8.0/sqrt(t)
TD(0.9) a = 2.0/cbrt(t)

0.05
0.0

0.5

1.0
Time

1.5

2.0
x1e+4

0.05
0.0

0.5

1.0
Time

1.5

2.0
x1e+4

Figure 1: 51 state Markov process averaged over
10 runs. The parameter a is the learning rate α.

Figure 2: 51 state Markov process averaged over
300 runs.

value of λ for HL(λ) was 1.0, which was to be expected given that the environment is stationary and
thus discounting old experience is not helpful.

For TD(λ) we tried various different learning rates and values of λ. We could ﬁnd no settings where
TD(λ) was competitive with HL(λ). If the learning rate α was set too high the system would learn
as fast as HL(λ) brieﬂy before becoming stuck. With a lower learning rate the ﬁnal performance
was improved, however the initial performance was now much worse than HL(λ). The results of
these tests appear in Figure 1.

Similar tests were performed with larger and smaller Markov chains, and with different values of γ .
HL(λ) was consistently superior to TD(λ) across these tests. One wonders whether this may be due
to the fact that the implicit learning rate that HL(λ) uses is not ﬁxed. To test this we explored the
performance of a number of different learning rate functions on the 51 state Markov chain described
above. We found that functions of the form κ
t always performed poorly, however good performance
and κ
. As the results were much
was possible by setting κ correctly for functions of the form κ
√t
3√t
closer, we averaged over 300 runs. These results appear in Figure 2.

With a variable learning rate TD(λ) is performing much better, however we were still unable to ﬁn d
an equation that reduced the learning rate in such a way that TD(λ) would outperform HL(λ). This
is evidence that HL(λ) is adapting the learning rate optimally without the need for manual equation
tuning.

4 Random Markov process

To test on a Markov process with a more complex transition structure, we created a random 50
state Markov process. We did this by creating a 50 by 50 transition matrix where each element was
set to 0 with probability 0.9, and a uniformly random number in the interval [0, 1] otherwise. We
then scaled each row to sum to 1. Then to transition between states we interpreted the ith row as a
probability distribution over which state follows state i. To compute the reward associated with each
transition we created a random matrix as above, but without normalising. We set γ = 0.9 and then
ran a brute force Monte Carlo simulation to compute the true discounted value of each state.

The λ parameter for HL(λ) was simply set to 1.0 as the environment is stationary. For TD we
experimented with a range of parameter settings and learning rate decrease functions. We found that
a ﬁxed learning rate of α = 0.2, and a decreasing rate of 1.5
performed reasonable well, but never
3√t
as well as HL(λ). The results were generated by averaging over 10 runs, and are shown in Figure 3.
Although the structure of this Markov process is quite different to that used in the previous experi-
ment, the results are again similar: HL(λ) preforms as well or better than TD(λ) from the beginning
to the end of the run. Furthermore, stability in the error towards the end of the run is better with
HL(λ) and no manual learning tuning was required for these performance gains.

5

E
S
M
R

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0
0

HL(1.0)
TD(0.9) a = 0.2
TD(0.9) a = 1.5/cbrt(t)

0.30

0.25

0.20

E
S
M
R

0.15

0.10

0.05

HL(0.9995)
TD(0.8) a = 0.05
TD(0.9) a = 0.05

1000

2000

3000

4000

5000

0.00
0.0

0.5

Time

1.0
Time

1.5

2.0
x1e+4

Figure 3: Random 50 state Markov process. The
parameter a is the learning rate α.

Figure 4: 21 state non-stationary Markov pro-
cess.

5 Non-stationary Markov process

The λ parameter in HL(λ), introduced in Equation (4), reduces the importance of old observations
when computing the state value estimates. When the environment is stationary this is not useful and
so we can set λ = 1.0, however in a non-stationary environment we need to reduce this value so that
the state values adapt properly to changes in the environment. The more rapidly the environment is
changing, the lower we need to make λ in order to more rapidly forget old observations.
To test HL(λ) in such a setting, we used the Markov chain from Section 3, but reduced its size to
21 states to speed up convergence. We used this Markov chain for the ﬁrst 5,000 time steps. At that
point, we changed the reward when transitioning from the last state to middle state to from -1.0 to
be 0.5. At time 10,000 we then switched back to the original Markov chain, and so on alternating
between the models of the environment every 5,000 steps. At each switch, we also changed the
target state values that the algorithm was trying to estimate to match the current conﬁguration of the
environment. For this experiment we set γ = 0.9.
As expected, the optimal value of λ for HL(λ) fell from 1 down to about 0.9995. This is about what
we would expect given that each phase is 5,000 steps long. For TD(λ) the optimal value of λ was
around 0.8 and the optimum learning rate was around 0.05. As we would expect, for both algorithms
when we pushed λ above its optimal value this caused poor performance in the periods following
each switch in the environment (these bad parameter settings are not shown in the results). On the
other hand, setting λ too low produced initially fast adaption to each environment switch, but poor
performance after that until the next environment change. To get accurate statistics we averaged
over 200 runs. The results of these tests appear in Figure 4.

For some reason HL(0.9995) learns faster than TD(0.8) in the ﬁrst half of the ﬁrst cycle, but only
equally fast at the start of each following cycle. We are not sure why this is happening. We could
improve the initial speed at which HL(λ) learnt in the last three cycles by reducing λ, however that
comes at a performance cost in terms of the lowest mean squared error attained at the end of each
cycle. In any case, in this non-stationary situation HL(λ) again performed well.

6 Windy Gridworld

Reinforcement learning algorithms such as Watkins’ Q(λ) [8] and Sarsa(λ) [5, 4] are based on
temporal difference updates. This suggests that new reinforcement learning algorithms based on
HL(λ) should be possible.
For our ﬁrst experiment we took the standard Sarsa( λ) algorithm and modi ﬁed it in the obvious way
to use an HL temporal difference update. In the presentation of this algorithm we have changed
notation slightly to make things more consistent with that typical in reinforcement learning. Specif-
ically, we have dropped the t super script as this is implicit in the algorithm speci ﬁcati on, and have

6

Algorithm 1 HLS(λ)
Initialise Q(s, a) = 0, N (s, a) = 1 and E (s, a) = 0 for all s, a
Initialise s and a
repeat
Take action a, observed r , s′
Choose a′ by using ǫ-greedy selection on Q(s′ , ·)
∆ ← r + γQ(s′ , a′ ) − Q(s, a)
E (s, a) ← E (s, a) + 1
N (s, a) ← N (s, a) + 1
for all s, a do
β ((s, a), (s′ , a′ )) ←
end for
for all s, a do
Q(s, a) ← Q(s, a) + β (cid:0)(s, a), (s′ , a′ )(cid:1)E (s, a)∆
E (s, a) ← γλE (s, a)
N (s, a) ← λN (s, a)
end for
s ← s′ ; a ← a′
until end of run

1
N (s′ ,a′ )−γE (s′ ,a′ )

N (s′ ,a′ )
N (s,a)

deﬁned Q(s, a) := V(s,a) , E (s, a) := E(s,a) and N (s, a) := N(s,a) . Our new reinforcement learn-
ing algorithm, which we call HLS(λ) is given in Algorithm 1. Essentially the only changes to the
standard Sarsa(λ) algorithm have been to add code to compute the visit counter N (s, a), add a loop
to compute the β values, and replace α with β in the temporal difference update.
To test HLS(λ) against standard Sarsa(λ) we used the Windy Gridworld environment described on
page 146 of [6]. This world is a grid of 7 by 10 squares that the agent can move through by going
either up, down, left of right. If the agent attempts to move off the grid it simply stays where it is.
The agent starts in the 4th row of the 1st column and receives a reward of 1 when it ﬁnds its way to
the 4th row of the 8th column. To make things more difﬁcult, there is a “wind” blowi
ng the agent
up 1 row in columns 4, 5, 6, and 9, and a strong wind of 2 in columns 7 and 8. This is illustrated in
Figure 5. Unlike in the original version, we have set up this problem to be a continuing discounted
task with an automatic transition from the goal state back to the start state.

We set γ = 0.99 and in each run computed the empirical future discounted reward at each point in
time. As this value oscillated we also ran a moving average through these values with a window
length of 50. Each run lasted for 50,000 time steps as this allowed us to see at what level each
learning algorithm topped out. These results appear in Figure 6 and were averaged over 500 runs to
get accurate statistics.

Despite putting considerable effort into tuning the parameters of Sarsa(λ), we were unable to achieve
a ﬁnal future discounted reward above 5.0. The settings show n on the graph represent the best ﬁnal
value we could achieve. In comparison HLS(λ) easily beat this result at the end of the run, while
being slightly slower than Sarsa(λ) at the start. By setting λ = 0.99 we were able to achieve the
same performance as Sarsa(λ) at the start of the run, however the performance at the end of the run
was then only slightly better than Sarsa(λ). This combination of superior performance and fewer
parameters to tune suggest that the beneﬁts of HL (λ) carry over into the reinforcement learning
setting.

Another popular reinforcement learning algorithm is Watkins’ Q(λ). Similar to Sarsa(λ) above, we
simply inserted the HL(λ) temporal difference update into the usual Q(λ) algorithm in the obvious
way. We call this new algorithm HLQ(λ)(not shown). The test environment was exactly the same as
we used with Sarsa(λ) above.
The results this time were more competitive (these results are not shown). Nevertheless, despite
spending a considerable amount of time ﬁne tuning the parame ters of Q(λ), we were unable to beat
HLQ(λ). As the performance advantage was relatively modest, the main beneﬁt of HLQ (λ) was
that it achieved this level of performance without having to tune a learning rate.

7

6

5

4

3

2

1

d
r
a
w
e
R
 
d
e
t
n
u
o
c
s
i
D
 
e
r
u
t
u
F

0
0

HLS(0.995) e = 0.003
Sarsa(0.5) a = 0.4 e = 0.005

1

2

3

4

Time

5
x1e+4

Figure 5: [Windy Gridworld] S marks the start
state and G the goal state, at which the agent
jumps back to S with a reward of 1.

Figure 6: Sarsa(λ) vs. HLS(λ) in the Windy
Gridworld. Performance averaged over 500 runs.
On the graph, e represents the exploration param-
eter ǫ, and a the learning rate α.

7 Conclusions

We have derived a new equation for setting the learning rate in temporal difference learning with
eligibility traces. The equation replaces the free learning rate parameter α, which is normally ex-
perimentally tuned by hand. In every setting tested, be it stationary Markov chains, non-stationary
Markov chains or reinforcement learning, our new method produced superior results.

To further our theoretical understanding, the next step would be to try to prove that the method
converges to correct estimates. This can be done for TD(λ) under certain assumptions on how the
learning rate decreases over time. Hopefully, something similar can be proven for our new method.
In terms of experimental results, it would be interesting to try different types of reinforcement learn-
ing problems and to more clearly identify where the ability to set the learning rate differently for
different state transition pairs helps performance. It would also be good to generalise the result to
episodic tasks. Finally, just as we have successfully merged HL(λ) with Sarsa(λ) and Watkins’
Q(λ), we would also like to see if the same can be done with Peng’s Q(λ) [3], and perhaps other
reinforcement learning algorithms.

Acknowledgements

This research was funded by the Swiss NSF grant 200020-107616.

References

[1] A. P. George and W. B. Powell. Adaptive stepsizes for recursive estimation with applications in approxi-
mate dynamic programming. Journal of Machine Learning, 65(1):167–198, 2006.
[2] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research,
4:1107–1149, 2003.
[3] J. Peng and R. J. Williams. Increamental multi-step Q-learning. Machine Learning, 22:283–290, 1996.
[4] G. A. Rummery. Problem solving with reinforcement learning. PhD thesis, Cambridge University, 1995.
[5] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technial Report
CUED/F-INFENG/TR 166, Engineering Department, Cambridge University, 1994.
[6] R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge, MA, MIT Press, 1998.
[7] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9–44, 1988.
[8] C.J.C.H Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Oxford, 1989.
[9] I. H. Witten. An adaptive optimal controller for discrete-time markov environments.
Information and
Control, 34:286–295, 1977.

8


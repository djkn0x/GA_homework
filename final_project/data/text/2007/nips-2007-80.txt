Nearest-Neighbor-Based Active Learning for Rare
Category Detection

Jingrui He
School of Computer Science
Carnegie Mellon University
jingruih@cs.cmu.edu

Jaime Carbonell
School of Computer Science
Carnegie Mellon University
jgc@cs.cmu.edu

Abstract

Rare category detection is an open challenge for active learning, especially in
the de-novo case (no labeled examples), but of signiﬁcant practical importance for
data mining - e.g. detecting new ﬁnancial transaction fraud patterns, where normal
legitimate transactions dominate. This paper develops a new method for detecting
an instance of each minority class via an unsupervised local-density-differential
sampling strategy. Essentially a variable-scale nearest neighbor process is used to
optimize the probability of sampling tightly-grouped minority classes, subject to
a local smoothness assumption of the majority class. Results on both synthetic
and real data sets are very positive, detecting each minority class with only a frac-
tion of the actively sampled points required by random sampling and by Pelleg’s
Interleave method, the prior best technique in the sparse literature on this topic.

1 Introduction

In many real world problems, the proportion of data points in different classes is highly skewed:
some classes dominate the data set (majority classes), and the remaining classes may have only a
few examples (minority classes). However, it is very important to detect examples from the minority
classes via active learning. For example, in fraud detection tasks, most of the records correspond to
normal transactions, and yet once we identify a new type of fraud transaction, we are well on our
way to stopping similar future fraud transactions [2]. Another example is in astronomy. Most of
the objects in sky survey images are explainable by current theories and models. Only 0.001% of
the objects are truly beyond the scope of current science and may lead to new discoveries [8]. Rare
category detection is also a bottleneck in reducing the sampling complexity of active learning [1,
5]. The difference between rare category detection and outlier detection is that: in rare category
detection, the examples from one or more minority classes are often self-similar, potentially forming
compact clusters, while in outlier detection, the outliers are typically scattered.
Currently, only a few methods have been proposed to address this challenge. For example, in [8],
the authors assumed a mixture model to ﬁt the data, and selected examples for labeling according
to different criteria; in [6], the authors proposed a generic consistency algorithm, and proved upper
bounds and lower bounds for this algorithm in some speciﬁc situations. Most of the existing methods
require that the majority classes and the minority classes be separable or work best in the separable
case. However, in real applications, the support regions of the majority and minority classes often
overlap, which affects negatively the performance of these methods.
In this paper, we propose a novel method for rare category detection in the context of active learning.
We typically start de-novo, no category labels, though our algorithm makes no such assumption.
Different from existing methods, we aim to solve the hard case, i.e. we do not assume separability or
near-separability of the classes. Intuitively, the method makes use of nearest neighbors to measure
local density around each example. In each iteration, the algorithm selects an example with the

1

maximum change in local density on a certain scale, and asks the oracle for its label. The method
stops once it has found at least one example from each class (given the knowledge of the number
of classes). When the minority classes form compact clusters and the majority class distribution
is locally smooth, the method will select examples both on the boundary and in the interior of the
minority classes, and is proved to be effective theoretically. Experimental results on both synthetic
and real data sets show the superiority of our method over existing methods.
The rest of the paper is organized as follows. In Section 2, we introduce our method and provide
theoretical justiﬁcation, ﬁrst for binary classes and then for multiple classes. Section 3 gives exper-
imental results. Finally, we conclude the paper in Section 4.

2 Rare category detection

2.1 Problem deﬁnition
Given a set of unlabeled examples S = {x1 , . . . , xn}, xi ∈ Rd , which come from m distinct classes,
i.e. yi ∈ {1, . . . , m}, the goal is to ﬁnd at least one example from each class by requesting as few
total labels as possible. For the sake of simplicity, assume that there is only one majority class,
which corresponds to yi = 1, and all the other classes are minority classes.

2.2 Rare category detection for the binary case
First let us focus on the simplest case where m = 2, and Pr[yi = 1] (cid:192) Pr[yi = 2] = p, i.e.
p (cid:191) 1. Here, we assume that we have an estimate of the value of p a priori. Next, we introduce our
method for rare category detection based on nearest neighbors, which is presented in Algorithm 1.
The basic idea is to ﬁnd maximum changes in local density, which might indicate the location of a
rare category.
The algorithm works as follows. Given the unlabeled set S and the prior of the minority class p, we
ﬁrst estimate the number K of minority class examples in S . Then, for each example, we record
its distance from the K th nearest neighbor, which could be realized by kd-trees [7]. The minimum
distance over all the examples is assigned to r (cid:48) . Next, we draw a hyper-ball centered at each example
with radius r (cid:48) , and count the number of examples enclosed by this hyper-ball, which is denoted as
ni . ni is roughly in proportion to the local density. To measure the change of local density around a
certain point xi , in each iteration of Step 3, we subtract nj of neighboring points from ni , and let the
maximum value be the score of xi . The example with the maximum score is selected for labeling
by the oracle. If the example is from the minority class, stop the iteration; otherwise, enlarge the
neighborhood where the scores of the examples are re-calculated and continue.
Before giving theoretical justiﬁcation, here, we give an intuitive explanation of why the algorithm
works. Assume that the minority class is concentrated in a small region and the probability distri-
bution function (pdf) of the majority class is locally smooth. Firstly, since the support region of the
minority class is very small, it is important to ﬁnd its scale. The r (cid:48) value obtained in Step 1 will
be used to calculate the local density ni . Since r (cid:48) is based on the minimum K th nearest neighbor
distance, it is never too large to smooth out changes of local density, and thus it is a good measure of
the scale. Secondly, the score of a certain point, corresponding to the change in local density, is the
maximum of the difference in local density between this point and all of its neighboring points. In
this way, we are not only able to select points on the boundary of the minority class, but also points
in the interior, given that the region is small. Finally, by gradually enlarging the neighborhood where
the scores are calculated, we can further explore the interior of the support region, and increase our
chance of ﬁnding a minority class example.

2.3 Correctness

In this subsection, we prove that if the minority class is concentrated in a small region and the pdf
of the majority class is locally smooth, the proposed algorithm will repeatedly sample in the region
where minority class examples occur with high probability.
Let f1 (x) and f2 (x) denote the pdf of the majority and minority classes respectively, where x ∈ Rd .
To be precise, we make the following assumptions.

2

Algorithm 1 Nearest-Neighbor-Based Rare Category Detection for the Binary Case (NNDB)
Require: S , p
1: Let K = np. For each example, calculate the distance to its K th nearest neighbor. Set r (cid:48) to be
the minimum value among all the examples.
2: ∀xi ∈ S , let N N (xi , r (cid:48) ) = {x|x ∈ S, (cid:107)x − xi(cid:107) ≤ r (cid:48) }, and ni = |N N (xi , r (cid:48) )|.
3: for t = 1 : n do
∀xi ∈ S , if xi has not been selected, then si =
(ni − nj ); otherwise, si = −∞.
max
4:
xj ∈N N (xi ,tr(cid:48) )
Query x = arg maxxi∈S si .
5:
If the label of x is 2, break.
6:
7: end for

Assumptions

1. f2 (x) is uniform within a hyper-ball B of radius r centered at b, i.e. f2 (x) = 1
V (r) , if
x ∈ B ; and 0 otherwise, where V (r) ∝ rd is the volume of B .
2. f1 (x) is bounded and positive in B 1 , i.e. f1 (x) ≥
(1−p)V (r) , ∀x ∈ B and f1 (x) ≤
c1 p
(1−p)V (r) , ∀x ∈ Rd , where c1 , c2 > 0 are two constants.
c2 p
With the above assumptions, we have the following claim and theorem. Note that variants of the
following proof apply if we assume a different minority class distribution, such as a tight Gaussian.
Claim 1. ∀, δ > 0, if n ≥ max{ 1
δ }, where r2 =
1 p2 log 3
2(1−2−d )2 p2 log 3
2 )4 log 3
1
1
δ ,
δ ,
2c2
4 V ( r2
2 , then with probability at least 1 − δ ,
, and V ( r2
2 ) is the volume of a hyper-ball with radius r2
r
1
(1+c2 )
2 ≤ r (cid:48) ≤ r and | ni
n − E ( ni
n )| ≤ V (r (cid:48) ), 1 ≤ i ≤ n, where V (r (cid:48) ) is the volume of a hyper-ball
d
r2
with radius r (cid:48) .
Proof. First, notice that the expected proportion of points falling inside B , E ( |N N (b,r)|
) ≥ (c1 +1)p,
n
and that the maximum expected proportion of points falling inside any hyper-ball of radius r2
2 ,
[E ( |N N (x, r2
2 )|
)] ≤ 2−d p. Then
max
x∈Rd
n
or ∃xi ∈ S s.t. | ni
)| > V (r (cid:48) )]
− E ( ni
Pr[r (cid:48) > r or r (cid:48) <
r2
2
n
n
≤ Pr[r (cid:48) > r ] + Pr[r (cid:48) <
)| > V (r (cid:48) )]
− E ( ni
and ∃xi ∈ S s.t. | ni
] + Pr[r (cid:48) ≥ r2
r2
2
2
n
n
)| > V (r (cid:48) )|r (cid:48) ≥ r2
− E ( ni
)| > K ] + n Pr[| ni
|N N (x,
≤ Pr[|N N (b, r)| < K ] + Pr[max
r2
x∈Rd
2
2
n
n
2 )
| N N (x, r2
= Pr[| N N (b, r)
)| > V (r (cid:48) )|r (cid:48) ≥ r2
− E ( ni
| > p] + n Pr[| ni
| < p] + Pr[max
x∈Rd
2
n
n
n
n
≤ e−2nc2
1 p2 + e−2n(1−2−d )2 p2 + 2ne−2n2 V (r (cid:48) )2
where the last inequality is based on Hoeffding bound.
3 and 2ne−2n2 V (r(cid:48) ) ≤ 2ne−2n2 V ( r2
3 , e−2n(1−2−d )2 p2 ≤ δ
1 p2 ≤ δ
2 )2 ≤ δ
Let e−2nc2
3 , we obtain
δ , n ≥
δ , and n ≥
n ≥ 1
δ . (cid:165)
1 p2 log 3
2(1−2−d )2 p2 log 3
2 )4 log 3
1
1
2c2
4 V ( r2
Based on Claim 1, we get the following theorem, which shows the effectiveness of the proposed
method.
Main Theorem. If

]

]

1. Let B 2 be the hyper-ball centered at b with radius 2r . The minimum distance between
the points inside B and the ones outside B 2 is not too large, i.e. min{(cid:107)xi − xj (cid:107)|xi , xj ∈
S, (cid:107)xi − b(cid:107) ≤ r, (cid:107)xj − b(cid:107) > 2r} ≤ α, where α is a positive parameter.
1Notice that here we are only dealing with the hard case where f1 (x) is positive within B . In the separable
case where the support regions of the two classes do not overlap, we can use other methods to detect the
minority class, such as the one proposed in [8].

3

2. f1 (x) is locally smooth, i.e. ∀x, y ∈ Rd , |f1 (x)− f1 (y)| ≤ β(cid:107)x−y(cid:107)
, where β ≤ p2OV ( r2
2 ,r)
2d+1 V (r)2
α
2 , r) is the volume of the overlapping region of two hyper-balls: one is of radius
and OV ( r2
r , the other one is of radius r2
2 , and its center is on the sphere of the bigger one.
3. The number of examples is sufﬁciently large,
i.e. n ≥ max{ 1
δ }.
2(1−2−d )2 p2 log 3
2 )4 log 3
1 p2 log 3
1
1
δ ,
δ ,
(1−p)4 β 4 V ( r2
2c2
(cid:101) iterations, NNDB will query at least one example
then with probability at least 1 − δ , after (cid:100) 2α
r2
whose probability of coming from the minority class is at least 1
3 , and it will continue querying such
p(1−p) − 2) · α
examples until the (cid:98)(
r (cid:99)th iteration.
2d
Proof. Based on Claim 1, using condition 3, if the number of examples is sufﬁciently large, then with
probability at least 1 − δ , r2
2 ≤ r (cid:48) ≤ r and | ni
n − E ( ni
n )| ≤ (1 − p)βV (r (cid:48) ), 1 ≤ i ≤ n. According to
condition 2, ∀xi , xj ∈ S s.t. (cid:107)xi − b(cid:107) > 2r , (cid:107)xj − b(cid:107) > 2r and (cid:107)xi − xj (cid:107) ≤ α, E ( ni
n ) and E ( nj
n )
will not be affected by the minority class, and |E ( ni
n ) − E ( nj
n )| ≤ (1 − p)βV (r (cid:48) ) ≤ (1 − p)βV (r).
Note that α is always bigger than r . Based on the above inequalities, we have
| ni
− nj
| ≤ | ni
− E ( ni
)| + | nj
− E ( nj
)| + |E ( ni
) − E ( nj
)| ≤ 3(1 − p)βV (r)
(1)
n
n
n
n
n
n
n
n
From inequality (1), it is not hard to see that ∀xi , xj ∈ S , s.t. (cid:107)xi − b(cid:107) > 2r and (cid:107)xi − xj (cid:107) ≤ α,
n ≤ 3(1 − p)βV (r), i.e. when tr (cid:48) = α,
n − nj
ni
≤ 3(1 − p)βV (r)
si
n
This is because if (cid:107)xj − b(cid:107) ≤ 2r , the minority class may also contribute to nj
n , and thus the score
may be even smaller.
On the other hand, based on condition 1, there exist two points xk , xl ∈ S , s.t. (cid:107)xk − b(cid:107) ≤ r ,
(cid:107)xl − b(cid:107) > 2r , and (cid:107)xk − xl (cid:107) ≤ α. Since the contribution of the minority class to E ( nk
n ) is at least
p·OV ( r2
n ) ≥ p·OV ( r2
V (r) − (1 − p)βV (r (cid:48) ) ≥ p·OV ( r2
n ) − E ( nl
V (r) − (1 − p)βV (r). Since
2 ,r)
2 ,r)
2 ,r)
, so E ( nk
V (r)
for any example xi ∈ S , we have | ni
n − E ( ni
n )| ≤ (1 − p)βV (r (cid:48) ) ≤ (1 − p)βV (r), therefore
− 3(1 − p)p2 · OV ( r2
− 3(1 − p)βV (r) ≥ p · OV ( r2
≥ p · OV ( r2
2 , r)
2 , r)
2 , r)
− nl
V (r)
V (r)
2d+1V (r)
n

nk
n

(2)

(3)

Since p is very small, p (cid:192) 3(1−p)p2
2d+1

n − nl
n > 3(1 − p)βV (r), i.e. when tr (cid:48) = α,
; therefore, nk
> 3(1 − p)βV (r)
sk
n
In Step 4 of the proposed method, we gradually enlarge the neighborhood to calculate the change of
local density. When tr (cid:48) = α, based on inequalities (2) and (3), ∀xi ∈ S , (cid:107)xi − b(cid:107) > 2r , we have
sk > si . Therefore, in this round of iteration, we will pick an example from B 2 . In order for tr (cid:48) to
be equal to α, the value of t would be (cid:100) α
r (cid:48) (cid:101) ≤ (cid:100) 2α
(cid:101).
r2
If we further increase t so that tr (cid:48) = cα, where c > 1, we have the following conclusion: ∀xi , xj ∈
S , s.t. (cid:107)xi − b(cid:107) > 2r and (cid:107)xi − xj (cid:107) ≤ cα, ni
n − nj
n ≤ (c + 2)(1 − p)βV (r), i.e. si
n ≤ (c + 2)(1 −
p)βV (r). As long as p ≥ (c+2)(1−p)p2
, i.e. c ≤ 2d
p(1−p) − 2, then ∀xi ∈ S , (cid:107)xi − b(cid:107) > 2r , sk > si ,
2d
and we will pick examples from B 2 . Since r (cid:48) ≤ r , the method will continue querying examples in
B 2 until the (cid:98)(
p(1−p) − 2) · α
r (cid:99)th iteration.
2d
Finally, we show that the probability of picking a minority class example from B 2 is at least 1
3 .
To this end, we need to calculate the maximum probability mass of the majority class within B 2 .
Consider the case where the maximum value of f1 (x) occurs at b, and this pdf decreases by β every
the shape of f1 (x) is a cone
time x moves away from b in the direction of the radius by α, i.e.
) · f1 (b)
in (d + 1) dimensional space. Since f1 (x) must integrate to 1, i.e. V ( αf1 (b)
d+1 = 1, where
β
V (α) ) 1
d
) is the volume of a hyper-ball with radius αf1 (b)
V ( αf1 (b)
, we have f1 (b) = ( d+1
d+1 .
d+1 β
β
β

4

Therefore, the probability mass of the majority class within B 2 is:
V (2r)(f1 (b) − 2r
2r
β
d + 1 V (2r) < V (2r)f1 (b)
β ) +
α
α
d+1 = 2d V (r)
= V (2r)( d + 1
(d + 1) 1
) 1
d
d+1 β
d+1 β
V (α)
V (α) 1
d+1
p2 · OV ( r2
2 , r)
d+1 ≤ (d + 1) 1
< (d + 1) 1
d+1 (2d+1V (r)β ) d
d+1 (
V (r)
where V (2r) is the volume of a hyper-ball with radius 2r . Therefore, if we select a point at random
p+(1−p)·2p ≥ p
p+2p = 1
p
from B 2 , the probability that this point is from the minority class is at least
3 .
(cid:165)

) d
d+1 < 2p

d
d+1

2.4 Rare category detection for multiple classes

In subsection 2.2, we have discussed rare category detection for the binary case. In this subsection,
we focus on the case where m > 2. To be speciﬁc, let p1 , . . . , pm be the priors of the m classes,
and p1 (cid:192) pi , i (cid:54)= 1. Our goal is to use as few label requests as possible to ﬁnd at least one example
from each class.
The method proposed in subsection 2.2 can be easily generalized to multiple classes, which is pre-
sented in Algorithm 2. In this algorithm, we are given the priors of all the minority classes. Using
each pi , we estimate the number Ki of examples from this class, and calculate the corresponding r (cid:48)
i
value in the same manner as NNDB. Then, we calculate the local density at each example based on
i . In the outer loop of Step 9, we calculate the r (cid:48) value which is the minimum of
different scales r (cid:48)
all the r (cid:48)
i whose corresponding classes have not been discovered yet and its index. In the inner loop
of Step 11, we gradually enlarge the neighborhood to calculate the score of each example. This is
the same as NNDB, except that we preclude the examples that are within a certain distance of any
selected example from being selected. This heuristic is to avoid repeatedly selecting examples from
the same discovered class. The inner loop stops when we ﬁnd an example from an undiscovered
class. Then we will update the r (cid:48) value and resume the inner loop. If the minority classes form
compact clusters and are far apart from each other, NNDM is able to detect examples from each
minority class with a small number of label requests.

Algorithm 2 Nearest-Neighbor-Based Rare Category Detection for Multiple Classes (NNDM)
Require: S , p2 , . . . , pm
1: for i = 2 : m do
Let Ki = npi .
2:
For each example, calculate the distance between this example and its K th
i nearest neighbor.
3:
Set r (cid:48)
i to be the minimum value among all the examples.
4: end for
i=2 r (cid:48)
5: Let r (cid:48)
1 = maxm
i .
6: for i = 1 : m do
j = |N N (xj , r (cid:48)
i }, and ni
i ) = {x|x ∈ S, (cid:107)x − xj (cid:107) ≤ r (cid:48)
∀xj ∈ S , let N N (xj , r (cid:48)
i )|.
7:
8: end for
9: while not all the classes have been discovered do
i |1 ≤ i ≤ m, and class i has not been discovered}, and s be the correspond-
Let r (cid:48) = min{r (cid:48)
10:
ing index, i.e. r (cid:48) = r (cid:48)
s .
for t = 1 : n do
for each xi that has been selected and labeled yi , ∀x ∈ S , s.t. (cid:107)x − xi(cid:107) ≤ r (cid:48)
yi , si = −∞;
i − ns
for all the other examples, si =
max
(ns
j ).
xj ∈N N (xi ,tr(cid:48) )
Query x = arg maxxi∈S si .
13:
If x belongs to a class that has not been discovered, break.
14:
end for
15:
16: end while

11:
12:

In NNDB and NNDM, we need the priors of the minority classes as the input. As we will see in the
next section, our algorithms are robust against small perturbations in the priors.

5

3 Experimental results

In this section, we compare our methods (NNDB and NNDM) with the best method proposed in [8]
(Interleave) and random sampling (RS) on both synthetic and real data sets. In Interleave, we use
the number of classes as the number of components in the mixture model. For both Interleave and
RS, we run the experiment multiple times and report the average results.

3.1 Synthetic data sets

Figure 1(a) shows a synthetic data set where the pdf of the majority class is Gaussian and the pdf of
the minority class is uniform within a small hyper-ball. There are 1000 examples from the majority
class and only 10 examples from the minority class. Using Interleave, we need to label 35 examples,
using RS, we need to label 101 examples, and using NNDB, we only need to label 3 examples in
order to sample one from the minority class, which are denoted as ‘x’ in Figure 1(b). Notice that
the ﬁrst 2 examples that NNDB selects are not from the correct region. This is because the number
of examples from the minority class is very small, and the local density may be affected by the
randomness in the data.

(a) Data Set

(b) Examples Selected by NNDB, denoted as ‘x’

Figure 1: Synthetic Data Set 1.

In Figure 2(a), the X-shaped data consisting of 3000 examples correspond to the majority class, and
the four characters ‘NIPS’ correspond to four minority classes, which consist of 138, 79, 118, and
206 examples respectively. Using Interleave, we need to label 1190 examples, using RS, we need
to label 83 examples, and using NNDM, we only need to label 5 examples in order to get one from
each of the minority classes, which are denoted as ‘x’ in Figure 2(b). Notice that in this example,
Interleave is even worse than RS. This might be because some minority classes are located in the
region where the density of the majority class is not negligible, and thus may be ‘explained’ by the
majority-class mixture-model component.

3.2 Real data sets

In this subsection, we compare different methods on two real data sets: Abalone [3] and Shuttle [4].
The ﬁrst data set consists of 4177 examples, described by 7 dimensional features. The examples
come from 20 classes:
the proportion of the largest class is 16.50%, and the proportion of the
smallest class is 0.34%. For the second data set, we sub-sample the original training set to produce
a smaller data set with 4515 examples, described by 9 dimensional features. The examples come
from 7 classes: the proportion of the largest class is 75.53%, and the proportion of the smallest class
is 0.13%.
The comparison results are shown in Figure 3(a) and Figure 3(b) respectively. From these ﬁgures,
we can see that NNDM is signiﬁcantly better than Interleave and RS: with Abalone data set, to ﬁnd

6

−3−2−101234−1012345−3−2−101234−1012345(a) Data Set

(b) Examples Selected by NNDM, denoted as ‘x’

Figure 2: Synthetic Data Set 2.

all the classes, Interleave needs 280 label requests, RS needs 483 label requests, and NNDM only
needs 125 label requests; with Shuttle data set, to ﬁnd all the classes, Interleave needs 140 label
requests, RS needs 512 label requests, and NNDM only needs 87 label requests. This is because
as the number of components becomes larger, the mixture model generated by Interleave is less
reliable due to the lack of labeled examples, thus we need to select more examples. Furthermore,
the majority and minority classes may not be near-separable, which is a disaster for Interleave. On
the other hand, NNDM does not assume a generative model for the data, and only focuses on the
change in local density, which is more effective on the two data sets.

(a) Abalone

(b) Shuttle

Figure 3: Learning Curves for Real Data Sets

3.3

Imprecise priors

The proposed algorithms need the priors of the minority classes as input. In this subsection, we test
the robustness of NNDM against modest mis-estimations of the class priors. The performance of
NNDB is similar to NNDM, so we omit the results here. In the experiments, we use the same data
sets as in subsection 3.2, and add/subtract 5%, 10%, and 20% from the true priors of the minority
classes. The results are shown in Figure 4. From these ﬁgures, we can see that NNDM is very robust
to small perturbations in the priors. For example, with Abalone data set, if we subtract 10% from
the true priors, only one more label request is needed in order to ﬁnd all the classes.

7

050100150200250020406080100120140160180200050100150200250020406080100120140160180200010020030040050005101520Number of Selected ExamplesClasses DiscoveredNNDMInterleaveRS01002003004005006001234567Number of Selected ExamplesClasses DiscoveredNNDMInterleaveRS(a) Abalone

(b) Shuttle

Figure 4: Robustness Study

4 Conclusion

In this paper, we have proposed a novel method for rare category detection, useful for de-novo active
learning in serious applications. Different from existing methods, our method does not rely on the
assumption that the data is near-separable. It works by selecting examples corresponding to regions
with the maximum change in local density, and depending on scaling, it will select class-boundary
or class-internal samples of minority classes. The method could be scaled up using kd-trees [7]. The
effectiveness of the proposed method is guaranteed by theoretical justiﬁcation, and its superiority
over existing methods is demonstrated by extensive experimental results on both synthetic and real
data sets. Moreover, it is very robust to modest perturbations in estimating true class priors.
Acknowledgments

This paper is based on work in part supported by the Defense Advanced Research Projects Agency
(DARPA) under contract number NBCHD030010.

References
[1] M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proc. of the 23rd Int.
Conf. on Machine Learning, pages 65–72, 2006.
[2] S. Bay, K. Kumaraswamy, M. Anderle, R. Kumar, and D. Steier. Large scale detection of
irregularities in accounting data. In Proc. of the 6th Int. Conf. on Data Mining, pages 75–86,
2006.
[3] C. Blake and C. Merz.
repository of machine learning databases.
Uci
http://www.ics.uci.edu/ machine/MLRepository.html, 1998.
[4] P.
repository.
Brazdil
and
J.
Gama.
Statlog
http://www.niaad.liacc.up.pt/old/statlog/datasets/shuttle/shuttle.doc.html, 1991.
[5] S. Dasgupta. Coarse sample complexity bounds for active learning.
In Advances in Neural
Information Processing Systems 19, 2005.
[6] S. Fine and Y. Mansour. Active sampling for multiple output identiﬁcation. In The 19th Annual
Conf. on Learning Theory, pages 620–634, 2006.
[7] A. Moore. A tutorial on kd-trees. Technical report, University of Cambridge Computer Labo-
ratory, 1991.
[8] D. Pelleg and A. Moore. Active learning for anomaly and rare-category detection. In Advances
in Neural Information Processing Systems 18, 2004.

In

In

8

05010015020025005101520Number of Selected ExamplesClasses Discovered−5%−10%−20%0+5%+10%+20%0204060801001234567Number of Selected ExamplesClasses Discovered−5%−10%−20%0+5%+10%+20%
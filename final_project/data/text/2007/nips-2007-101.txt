Statistical Analysis of Semi-Supervised Regression

John Lafferty
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213
lafferty@cs.cmu.edu

Larry Wasserman
Department of Statistics
Carnegie Mellon University
Pittsburgh, PA 15213
larry@stat.cmu.edu

Abstract

Semi-supervised methods use unlabeled data in addition to labeled data to con-
struct predictors. While existing semi-supervised methods have shown some
promising empirical performance, their development has been based largely based
on heuristics. In this paper we study semi-supervised learning from the viewpoint
of minimax theory. Our ﬁrst result shows that some common met hods based on
regularization using graph Laplacians do not lead to faster minimax rates of con-
vergence. Thus, the estimators that use the unlabeled data do not have smaller
risk than the estimators that use only labeled data. We then develop several new
approaches that provably lead to improved performance. The statistical tools of
minimax analysis are thus used to offer some new perspective on the problem of
semi-supervised learning.

1

Introduction

Suppose that we have labeled data L = {( X 1 , Y1 ), . . . ( X n , Yn )} and unlabeled data U =
{X n+1 , . . . X N } where N (cid:29) n and X i ∈ R D . Ordinary regression and classi ﬁcation techniques
use L to predict Y from X . Semi-supervised methods also use the unlabeled data U in an attempt
to improve the predictions. To justify these procedures, it is common to invoke one or both of the
following assumptions:

Manifold Assumption (M): The distribution of X lives on a low dimensional manifold.
Semi-Supervised Smoothness Assumption (SSS): The regression function m (x ) =
EY | X = x is very smooth where the density p(x ) of X is large. In particular, if there
is a path connecting X i and X j on which p(x ) is large, then Yi and Y j should be similar
with high probability.

While these assumptions are somewhat intuitive, and synthetic examples can easily be constructed to
demonstrate good performance of various techniques, there has been very little theoretical analysis
of semi-supervised learning that rigorously shows how the assumptions lead to improved perfor-
mance of the estimators.

In this paper we provide a statistical analysis of semi-supervised methods for regression, and pro-
pose some new techniques that provably lead to better inferences, under appropriate assumptions. In
particular, we explore precise formulations of SSS, which is motivated by the intuition that high den-
sity level sets correspond to clusters of similar objects, but as stated above is quite vague. To the best
of our knowledge, no papers have made the assumption precise and then explored its consequences
in terms of rates of convergence, with the exception of one of the ﬁrst papers on semi-supervised
learning, by Castelli and Cover (1996), which evaluated a simple mixture model, and the recent
paper of Rigollet (2006) in the context of classi ﬁcation. Th is situation is striking, given the level
of activity in this area within the machine learning community; for example, the recent survey of
semi-supervised learning by Zhu (2006) contains 163 references.

1

Among our ﬁndings are:

1. Under the manifold assumption M, the semi-supervised smoothness assumption SSS is
super ﬂuous. This point was made heuristically by Bickel and Li (2006), but we show
that in fact ordinary regression methods are automatically adaptive if the distribution of X
concentrates on a manifold.
2. Without the manifold assumption M, the semi-supervised smoothness assumption SSS as
usually deﬁned is too weak, and current methods don’t lead to improved inferences. In
particular, methods that use regularization based on graph Laplacians do not achieve faster
rates of convergence.
3. Assuming speci ﬁc conditions that relate m and p , we develop new semi-supervised meth-
ods that lead to improved estimation. In particular, we propose estimators that reduce bias
by estimating the Hessian of the regression function, improve the choice of bandwidths
using unlabeled data, and estimate the regression function on level sets.

The focus of the paper is on a theoretical analysis of semi-supervised regression techniques, rather
than the development of practical new algorithms and techniques. While we emphasize regression,
most of our results have analogues for classi ﬁcation. Our in tent is to bring the statistical perspective
of minimax analysis to bear on the problem, in order to study the interplay between the labeled
sample size and the unlabeled sample size, and between the regression function and the data density.
By studying simpli ﬁed versions of the problem, our analysis
suggests how precise formulations of
assumptions M and SSS can be made and exploited to lead to improved estimators.

2 Preliminaries

The data are ( X 1 , Y1 , R1 ), . . . , ( X N , Y N , R N ) where Ri ∈ {0, 1} and we observe Yi only if Ri = 1.
The labeled data are L = {( X i , Yi ) Ri = 1} and the unlabeled data are U = {( X i , Yi ) Ri = 0}.
For convenience, assume that data are labeled so that Ri = 1 for i = 1, . . . , n and Ri = 0 for
i = n + 1, . . . , N . Thus, the labeled sample size is n , and the unlabeled sample size is u = N − n .
Let p(x ) be the density of X and let m (x ) = E(Y | X = x ) denote the regression function. Assume
that R ⊥⊥ Y | X (missing at random) and that Ri | X i ∼ Bernoulli(π (Xi )). Finally, let µ = P( Ri =
1) = R π (x ) p(x )d x . For simplicity we assume that π (x ) = µ for all x . The missing at random
assumption R ⊥⊥ Y | X is crucial, although this point is rarely emphasized in the machine learning
literature.
It is clear that without some further conditions, the unlabeled data are useless. The key assumption
we need is that there is some correspondence between the shape of the regression function m and
the shape of the data density p .

We will use minimax theory to judge the quality of an estimator. Let R denote a class of regression
functions and let F denote a class of density functions. In the classical setting, we observe labeled
data ( X 1 , Y2 ), . . . , ( X n , Yn ). The pointwise minimax risk, or mean squared error (MSE), is deﬁned
by
E(bm n (x ) − m (x ))2
sup
Rn (x ) = inf
bm n
m∈R, p∈F
where the inﬁmum is over all estimators. The global minimax r isk is deﬁned by
E Z (bm n (x ) − m (x ))2 d x .
sup
Rn = inf
bm n
m∈R, p∈F
A typical assumption is that R is the Sobolev space of order two, meaning essentially that m has
smooth second derivatives. In this case we have1 Rn (cid:16) n−4/(4+ D ) . The minimax rate is achieved
by kernel estimators and local polynomial estimators. In particular, for kernel estimators if we use
a product kernel with common bandwidth h n for each variable, choosing h n ∼ n−1/(4+ D ) yields an
1We write an (cid:16) bn to mean that an /bn is bounded away from 0 and inﬁnity for large n . We have suppressed
some technicalities such as moment assumptions on  = Y − m ( X ).

(1)

(2)

2

(3)

estimator with the minimax rate. The difﬁculty is that the ra te Rn (cid:16) n−4/(4+ D ) is extremely slow
when D is large.
In more detail, let C > 0 and let B be a positive deﬁnite matrix, and deﬁne
(x − x0 )T B(x − x0 ) (cid:27)
R = (cid:26)m (cid:12)(cid:12)(cid:12)m (x ) − m (x0 ) − (x − x0 )T ∇m (x0 )(cid:12)(cid:12)(cid:12) ≤
C
2
F = (cid:26) p p(x ) ≥ b > 0, | p(x1 ) − p(x2 )| ≤ ckx1 − x2 kα
2 (cid:27).
Fan (1993) shows that the local linear estimator is asymptotically minimax for this class. This esti-
mator is given by bm n (x ) = a0 where (a0 , a1 ) minimizes Pn
1 ( X i −x ))2 K ( H −1/2 ( X i −
i =1 (Yi −a0−a T
x )), where K is a symmetric kernel and H is a matrix of bandwidths.
The asymptotic MSE of the local linear estimator bm (x ) using the labeled data is
2 (K )tr(Hm (x ) H )(cid:19)2
R ( H ) = (cid:18) 1
ν0σ 2
1
µ2
p(x ) + o( tr( H ))
(5)
+
n | H |1/2
2
where Hm (x ) is the Hessian of m at x , µ2 (K ) = R K 2 (u ) d u and ν0 is a constant. The optimal
bandwidth matrix H∗ is given by
2 (K )n D p(x ) !2/( D+4)
H∗ =   ν0σ 2 |Hm |1/2
(Hm )−1
(6)
µ2
and R ( H∗ ) = O (n−4/(4+ D ) ). This result is important to what follows, because it suggests that if the
Hessian Hm of the regression function is related to the Hessian H p of the data density, one may be
able to estimate the optimal bandwidth matrix from unlabeled data in order to reduce the risk.

(4)

3 The Manifold Assumption

It is common in the literature to invoke both M and SSS. But if M holds, SSS is not needed. This
is argued by Bickel and Li (2006) who say, “We can unwittingly take advantage of low dimensional
structure without knowing it.”
Suppose X ∈ R D has support on a manifold M with dimension d < D . Let bm h be the local linear
estimator with diagonal bandwidth matrix H = h 2 I . Then Bickel and Li show that the bias and
variance are
J2 (x )
b(x ) = h 2 J1 (x )(1 + o P (1)) and v (x ) =
n h d (1 + o P (1))
(7)
for some functions J1 and J2 . Choosing h (cid:16) n−1/(4+d ) yields a risk of order n−4/(4+d ) , which is the
optimal rate for data that to lie on a manifold of dimension d .
To use the above result we would need to know d . Bickel and Li argue heuristically that the following
procedure will lead to a reasonable bandwidth. First, estimate d using the procedure in Levina
and Bickel (2005). Now let B = {λ1 / n 1/(bd+4) , . . . , λ B / n 1/(bd+4) } be a set of bandwidths, scaling
the asymptotic order n−1/(bd+4) by different constants. Finally, choose the bandwidth h ∈ B that
minimizes a local cross-validation score.
We now show that, in fact, one can skip the step of estimating d . Let E 1 , . . . , E n be independent
Bernoulli (θ = 1
2 ) random variables. Split the data into two groups, so that I0 = {i E i = 0} and
E i = 1}. Let H = {n−1/(4+d )
1 ≤ d ≤ D }. Construct bm h for h ∈ H using the data
I1 = {i
in I0 , and estimate the risk from I1 by setting bR (h ) = | I1 |−1 Pi ∈ I1 (Yi − bm h ( X i ))2 . Finally, let bh
minimize bR (h ) and set bm = bmbh . For simplicity, let us assume that both Yi and X i are bounded by a
ﬁnite constant B .
Theorem 1. Suppose that and |Yi | ≤ B and | X i j | ≤ B for all i and j . Assume the conditions in
Bickel and Li (2006). Suppose that the data density p(x ) is supported on amanifold of dimension
d ≥ 4. Thenwehave that
E(bm (x ) − m (x ))2 = eO (cid:18)
n 4/(4+d ) (cid:19) .
1
3

(8)

The notation eO allows for logarithmic factors in n .
Proof. The risk is, up to a constant, R (h ) = E(Y − bm ( X ))2 , where ( X , Y ) is a new pair
and Y = m ( X ) +  . Note that (Y − bm h ( X ))2 = Y 2 − 2Y bm h ( X ) + bm 2
h ( X ), so R (h ) = E(Y 2 ) −
2E(Y bm h ( X )) + bm 2
h ( X ). Let n 1 = | I1 |. Then,
n 1 Xi ∈ I1 bm 2
n 1 Xi ∈ I1
n 1 Xi ∈ I1
1
1
2
Y 2
bR (h ) =
(9)
Yi bm h ( X i ) +
h ( X i ).
i −
By conditioning on the data in I0 and applying Bernstein’s inequality, we have
P (cid:18)max
h∈H | bR (h ) − R (h )| > (cid:19) ≤ Xh∈H
P (cid:0)| bR (h ) − R (h )| >  (cid:1) ≤ De−n c 2
for some c > 0. Setting n = √C log n/ n for suitably large C , we conclude that
P  max
n ! −→ 0.
h∈H | bR (h ) − R (h )| > r C log n
Let h ∗ minimize R (h ) over H. Then, except on a set of probability tending to 0,
≤ bR (h ∗ ) + r C log n
R (bh ) ≤ bR (bh ) + r C log n
n
n
n 4/(4+d ) (cid:19) + 2r C log n
≤ R (h ∗ ) + 2r C log n
n 4/(4+d ) (cid:19)
= eO (cid:18)
= O (cid:18)
1
1
(13)
n
n
If d = 4 then O (√log n/ n ) =
where we used the assumption d ≥ 4 in the last equality.
eO (n−4/(4+d ) ); if d > 4 then O (√log n/ n ) = o (cid:0)n 4/(4+d ) (cid:1). (cid:3)
We conclude that ordinary regression methods are automatically adaptive, and achieve the low-
dimensional minimax rate if the distribution of X concentrates on a manifold; there is no need for
semi-supervised methods in this case. Similar results apply to classi ﬁcation.

(10)

(11)

(12)

4 Kernel Regression with Laplacian Regularization

In practice, it is unlikely that the distribution of X would be supported exactly on a low-dimensional
manifold. Nevertheless, the shape of the data density p(x ) might provide information about the
regression function m (x ), in which case the unlabeled data are informative.

Several recent methods for semi-supervised learning attempt to exploit the smoothness assumption
SSS using regularization operators deﬁned with respect to g raph Laplacians (Zhu et al., 2003; Zhou
et al., 2004; Belkin et al., 2005). The technique of Zhu et al. (2003) is based on Gaussian random
ﬁelds and harmonic functions deﬁned with respect to discret
e Laplace operators. To express this
method in statistical terms, recall that standard kernel regression corresponds to the locally constant
estimator
K h ( X i , x )(Yi − m (x ))2 = Pn
nXi =1
i =1 K h ( X i , x ) Yi
bm n (x ) = arg min
Pn
i =1 K h ( X i , x )
m (x )
where K h is a symmetric kernel depending on bandwidth parameters h . In the semi-supervised
approach of Zhu et al. (2003), the locally constant estimate bm (x ) is formed using not only the
labeled data, but also using the estimates at the unlabeled points. Suppose that the ﬁrst n data points
( X 1 , Y1 ), . . . , ( X n , Yn ) are labeled, and the next u = N − n points are unlabeled, X n+1 , . . . , X n+u .
The semi-supervised regression estimate is then (bm ( X 1 ), bm ( X 2 ), . . . , bm ( X N )) given by
NXi =1
NXj =1
K h ( X i , X j ) (m ( X i ) − m ( X j ))2
bm = arg min
m

(14)

(15)

4

(17)

where the minimization is carried out subject to the constraint m ( X i ) = Yi , i = 1, . . . , n . Thus,
the estimates are coupled, unlike the standard kernel regression estimate (14) where the estimate at
each point x can be formed independently, given the labeled data.
The estimator can be written in closed form as a linear smoother bm = C −1 B Y = G Y where
bm = (bm ( X n+1 ), . . . , m ( X n+u ))T is the vector of estimates over the unlabeled test points, and Y =
(Y1 , . . . , Yn )T is vector of labeled values. The ( N − n )× ( N − n ) matrix C and the ( N − n )× n matrix
B denote blocks of the combinatorial Laplacian on the data graph corresponding to the labeled and
unlabeled data:
C (cid:19)
 = (cid:18) A B T
(16)
B
where the Laplacian  = i j has entries
i j = (cid:26)Pk K h ( X i , X k )
if i = j
otherwise.
−K h ( X i , X j )
This expresses the effective kernel G in terms of geometric objects such as heat kernels for the
discrete diffusion equations (Smola and Kondor, 2003).
This estimator assumes the noise is zero, since bm ( X i ) = Yi for i = 1, . . . , n . To work in the
standard model Y = m ( X ) +  , the natural extension of the harmonic function approach is mani-
fold regularization (Belkin et al., 2005; Sindhwani et al., 2005; Tsang and Kwok, 2006). Here the
estimator is chosen to minimize the regularized empirical risk functional
NXi =1
nXj =1
NXi =1
NXj =1
K H ( X i , X j ) (cid:0)Y j − m ( X i )(cid:1)2
K H ( X i , X j ) (cid:0)m ( X j ) − m ( X i )(cid:1)2
Rγ (m ) =
+ γ
where H is a matrix of bandwidths and K H ( X i , X j ) = K ( H −1/2 ( X i − X j )). When γ = 0 the
standard kernel smoother is obtained. The regularization term is
NXi =1
NXj =1
K H ( X i , X j ) (cid:0)m ( X j ) − m ( X i )(cid:1)2
= 2m T m
J (m ) ≡
where  is the combinatorial Laplacian associated with K H . This regularization term is motivated
by the semi-supervised smoothness assumption—it favors fun ctions m for which m ( X i ) is close to
m ( X j ) when X i and X j are similar, according to the kernel function. The name manifold regulariza-
2 J (m ) → RM k∇m (x )k2 dM x , the energy of m over the manifold.
1
tion is justi ﬁed by the fact that
While this regularizer has primarily been used for SVM classi ﬁers (Belkin et al., 2005), it can be
used much more generally. For an appropriate choice of γ , minimizing the functional (18) can be
expected to give essentially the same results as the harmonic function approach that minimizes (15).
Theorem 2. Suppose that D ≥ 2. Let em H ,γ minimize (18), and let  p, H be the differential
operatordeﬁnedby
trace(H f (x ) H ) + ∇ p(x )T H ∇ f (x )
1
(20)
 p, H f (x ) =
.
2
p(x )
Then the asymptoticMSEof em H ,γ (x ) is
n (µ + γ ) p(x )| H |1/2 +   c2 (µ + γ )
 p, H m (x )!2
(cid:18) I −
 p, H (cid:19)−1
c1µσ 2
eM =
µ
where µ = P( Ri = 1).
Note that the bias of the standard kernel estimator, in the notation of this theorem, is b(x ) =
c2 p, H m (x ), and the variance is V (x ) = c1 / n p(x )| H |1/2 . Thus, this result agrees with the standard
supervised MSE in the special case γ = 0. It follows from this theorem that eM = M + o( tr( H ))
where M is the usual MSE for a kernel estimator. Therefore, the minimum of eM has the same
leading order in H as the minimum of M .
The proof is given in the full version of the paper. The implication of this theorem is that the
estimator that uses Laplacian regularization has the same rate of convergence as the usual kernel
estimator, and thus the unlabeled data have not improved the estimator asymptotically.

+ o( tr( H ))

(21)

(18)

(19)

γ

µ

5

5 Semi-Supervised Methods With Improved Rates

The previous result is negative, in the sense that it shows unlabeled data do not help to improve the
rate of convergence. This is because the bias and variance of a manifold regularized kernel esti-
mator are of the same order in H as the bias and variance of standard kernel regression. We now
demonstrate how improved rates of convergence can be obtained by formulating and exploiting ap-
propriate SSS assumptions. We describe three different approaches: semi-supervised bias reduction,
improved bandwidth selection, and averaging over level sets.

5.1 Semi-Supervised Bias Reduction

(22)

We ﬁrst show a positive result by formulating an SSS assumpti on that links the shape of p to the
shape of m by positing a relationship between the Hessian Hm of m and the Hessian H p of p . Under
this SSS assumption, we can improve the rate of convergence by reducing the bias.
To illustrate the idea, take p(x ) known (i.e., N = ∞) and suppose that Hm (x ) = H p (x ). Deﬁne
1
µ2
2 (K )tr(Hm (x ) H )
em n (x ) = bm n (x ) −
2
where bm n (x ) is the local linear estimator.
Theorem 3. The riskof em n (x ) is O (cid:0)n−8/(8+ D ) (cid:1).
Proof. First note that the variance of the estimator em n , conditional on X 1 , . . . , X n ,
is
Var(em n (x )| X 1 , . . . , X n ) = Var(bm n (x )| X 1 , . . . , X n ). Now, the term 1
2 µ2
2 (K )tr(Hm (x ) H ) is pre-
cisely the bias of the local linear estimator, under the SSS assumption that H p (x ) = Hm (x ). Thus,
the ﬁrst order bias term has been removed. The result now foll ows from the fact that the next term
in the bias of the local linear estimator is of order O (tr( H )4 ). (cid:3)
By assuming 2` derivatives are matched, we get the rate n−(4+4`)/(4+4`+ D ) . When p is estimated
from the data, the risk will be inﬂated by N −4/(4+ D ) assuming standard smoothness assumptions
on p . This term will not dominate the improved rate n−(4+4`)/(4+4`+ D ) as long as N > n ` . The
assumption that Hm = H p can be replaced by the more realistic assumption that Hm = g ( p ; β )
for some parameterized family of functions g (·; β ). Semiparametric methods can then be used to
estimate β . This approach is taken in the following section.

5.2
Improved Bandwidth Selection
Let bH be the estimated bandwidth using the labeled data. We will now show how a bandwidth
bH ∗ can be estimated using the labeled and unlabeled data together, such that, under appropriate
assumptions,
| R ( bH ∗ ) − R ( H ∗ )|
| R ( bH ) − R ( H ∗ )| = 0, where H ∗ = arg min
lim sup
(23)
R ( H ).
H
n→∞
Therefore, the unlabeled data allow us to construct an estimator that gets closer to the oracle risk.
The improvement is weaker than the bias adjustment method. But it has the virtue that the optimal
local linear rate is maintained even if the proposed model linking Hm to p is incorrect.
We begin in one dimension to make the ideas clear. Let bm H denote the local linear estimator with
bandwidth H ∈ R, H > 0. To use the unlabeled data, note that the optimal (global) bandwidth
is H ∗ = (c2 B /(4n c1 A))1/5 where A = R m 00 (x )2 d x and B = R d x / p(x ). Let bp(x ) be the kernel
density estimator of p using X 1 , . . . , X N and bandwidth h = O ( N −1/5 ). We assume
(SSS)
m 00 (x ) = G θ ( p) for some function G depending on ﬁnitely many parameters θ .
Now let \m 00 (x ) = Gbθ (bp), and deﬁne bH ∗ = (cid:16) c2 bB
4n c1 bA (cid:17)1/5
where bA = R (\m 00 (x ))2 d x and bB =
R d x /bp(x ).
6

lim sup
n→∞

Theorem 4. Suppose that \m 00 (x ) − m 00 (x ) = O P ( N −β ) where β > 2
5 . Let N = N (n ) → ∞ as
n → ∞. If N / n 1/4 → ∞, then
| R ( bH ∗ ) − R ( H ∗ )|
| R ( bH ) − R ( H ∗ )| = 0.
Proof. The risk is
n H (cid:19) .
p(x ) + o (cid:18) 1
n H Z
R ( H ) = c1 H 4 Z (m 00 (x ))2 d x +
d x
c2
(25)
The oracle bandwidth is H ∗ = c3 / n 1/5 and then R ( H ∗ ) = O (n−4/5 ). Now let bH be the bandwidth
estimated by cross-validation. Then, since R 0 ( H ∗ ) = 0 and H ∗ = O (n−1/5 ), we have
( bH − H ∗ )2
R 00 ( H ∗ ) + O (| bH − H ∗ |3 )
R ( bH ) =
(26)
2
( bH − H ∗ )2
O (n−2/5 ) + O (| bH − H ∗ |3 ).
(27)
=
2
From Girard (1998), bH − H ∗ = O P (n−3/10 ). Hence, R ( bH ) − R ( H ∗ ) = O P (n−1 ). Also, bp(x ) −
p(x ) = O ( N −2/5 ). Since \m 00 (x ) − m 00 (x ) = O P ( N −β ),
bH ∗ − H ∗ = O P (cid:18) N −2/5
n 1/5 (cid:19) + O P (cid:18) N −β
n 1/5 (cid:19) .
(28)
The ﬁrst term is o P (n−3/10 ) since N > n 1/4 . The second term is o P (n−3/10 ) since β > 2/5. Thus
R ( bH ∗ ) − R ( H ∗ ) = o P (1/ n ) and the result follows. (cid:3)
The proof in the multidimensional case is essentially the same as in the one dimensional case, except
that we use the multivariate version of Girard’s result, namely, H∗ − bH = O P (n−( D+2)/(2( D+4)) ).
This leads to the following result.
Theorem 5. Let N = N (n ). If N / n D/4 → ∞,bθ − θ = O P ( N −β ) for some β > 2
4+ D then
| R ( bH ∗ ) − R ( H ∗ )|
| R ( bH ) − R ( H ∗ )| = 0.
lim sup
n→∞
Recall that SSS is motivated by the intuition that high density level sets should correspond to clusters
of similar objects. Another approach to quantifying SSS is to make this cluster assumption explicit.
Rigollet (2006) shows one way to do this in classi ﬁcation. He re we focus on regression.
Suppose that L = {x
p(x ) > λ} can be decomposed into a ﬁnite number of connected, compact,
convex sets C1 , . . . , C g where λ is chosen so that L c has negligible probability. For N large we can
replace L with L = {x bp(x ) > λ} with small loss in accuracy, where bp is an estimate of p using
the unlabeled data; see Rigollet (2006) for details. Let k j = Pn
i =1 I ( X i ∈ C j ) and for x ∈ C j
deﬁne
bm (x ) = Pn
i =1 Yi I ( X i ∈ C j )
(30)
.
k j
Thus, bm (x ) simply averages the labels of the data that fall in the set to which x belongs. If the
regression function is slowly varying in over this set, the risk should be small. A similar estimator
is considered by Cortes and Mohri (2006), but they do not provide estimates of the risk.
Theorem 6. The riskof bm (x ) for x ∈ L ∩ C j isboundedby
O (cid:18) 1
nπ j (cid:19) + O (cid:16)δ 2
j (cid:17)
j ξ 2
where δ j = supx ∈C j k∇m (x )k, ξ j = diameter(C j ) and π j = P( X ∈ C j ).

5.3 Averaging over Level Sets

(24)

(29)

(31)

7

Proof. Since the k j are Binomial, k j = nπ j + o(1) almost surely. Thus, the variance of bm (x )
is O (1/(nπ j )). The mean, given X 1 , . . . , X n , is
k j Xi X i ∈C j
k j Xi X i ∈C j
1
1
(32)
m ( X i ) = m (x ) +
(m ( X i ) − m (x )).
Now m ( X i ) − m (x ) = ( X j − x )T ∇m (u i ) for some u i between x and X i . Hence, |m ( X i ) − m (x )| ≤
k X j − x k supx ∈C j k∇m (x )k and so the bias is bounded by δ j ξ j . (cid:3)
This result reveals an interesting bias-variance tradeoff. Making λ smaller decreases the variance
and increases the bias. Suppose the two terms are balanced at λ = λ∗ . Then we will beat the usual
rate of convergence if π j (λ∗ ) > n− D/(4+ D ) .
6 Conclusion

Semi-supervised methods have been very successful in many problems. Our results suggest that the
standard explanations for this success are not correct. We have indicated some new approaches to
understanding and exploiting the relationship between the labeled and unlabeled data. Of course, we
make no claim that these are the only ways of incorporating unlabeled data. But our results indicate
that decoupling the manifold assumption and the semi-supervised smoothness assumption is crucial
to clarifying the problem.

7 Acknowlegments

We thank Partha Niyogi for several interesting discussions. This work was supported in part by NSF
grant CCF-0625879.

References
B E LK IN , M . , N IYOG I , P. and S INDHWAN I , V. (2005). On manifold regularization. In Proceedings of the Tenth
International Workshop on Arti ﬁcial Intelligence and Statistics (AISTAT 20 05).
B ICK E L , P. and L I , B . (2006). Local polynomial regression on unknown manifolds. Tech. rep., Department of
Statistics, UC Berkeley.
CA S T E L L I , V. and COV ER , T. (1996). The relative value of labeled and unlabeled samples in pattern recogni-
tion with an unknown mixing parameter. IEEE Trans. on Info. Theory 42 2101–2117.
CORT E S , C . and MOHR I , M . (2006). On transductive regression. In Advances in Neural Information Process-
ing Systems (NIPS), vol. 19.
FAN , J . (1993). Local linear regression smoothers and their minimax efﬁc iencies. The Annals of Statistics 21
196–216.
G IRARD , D . (1998). Asymptotic comparison of (partial) cross-validation, gcv and randomized gcv in nonpara-
metric regression. Ann. Statist. 12 315–334.
L EV INA , E . and B ICK E L , P. (2005). Maximum likelihood estimation of intrinsic dimension. In Advances in
Neural Information Processing Systems (NIPS), vol. 17.
N IYOG I , P. (2007). Manifold regularization and semi-supervised learning: Some theoretical analyses. Tech.
rep., Departments of Computer Science and Statistics, University of Chicago.
R IGO L L E T, P. (2006). Generalization error bounds in semi-supervised classiﬁc ation under the cluster assump-
tion. arxiv.org/math/0604233 .
S INDHWAN I , V. , N IYOG I , P. , B E LK IN , M . and K E ERTH I , S . (2005). Linear manifold regularization for large
scale semi-supervised learning. In Proc. of the 22nd ICML Workshop on Learning with Partially Classiﬁed
Training Data.
SMO LA , A . and KONDOR , R . (2003). Kernels and regularization on graphs.
Theory, COLT/KW.
T SANG , I . and KWOK , J . (2006). Large-scale sparsiﬁed manifold regularization.
Information Processing Systems (NIPS), vol. 19.
ZHOU , D . , BOU SQU E T, O . , LA L , T. , W E S TON , J . and SCH Ö LKO P F , B . (2004). Learning with local and global
consistency. In Advances in Neural Information Processing Systems (NIPS), vol. 16.
ZHU , X . (2006). Semi-supervised learning literature review. Tech. rep., University of Wisconsin.
ZHU , X . , GHAHRAMAN I , Z . and LA FFERTY, J . (2003). Semi-supervised learning using Gaussian ﬁelds and
harmonic functions. In ICML-03, 20th International Conference on Machine Learning.

In Conference on Learning

In Advances in Neural

8


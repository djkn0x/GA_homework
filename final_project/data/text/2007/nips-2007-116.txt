Semi-Supervised Multitask Learning

Qiuhua Liu, Xuejun Liao, and Lawrence Carin
Department of Electrical and Computer Engineering
Duke University
Durham, NC 27708-0291, USA

Abstract

A semi-supervised multitask learning (MTL) framework is presented, in which
M parameterized semi-supervised classiﬁers, each associated with one of M par-
tially labeled data manifolds, are learned jointly under the constraint of a soft-
sharing prior imposed over the parameters of the classiﬁers. The unlabeled data
are utilized by basing classiﬁer learning on neighborhoods, induced by a Markov
random walk over a graph representation of each manifold. Experimental results
on real data sets demonstrate that semi-supervised MTL yields signiﬁcant im-
provements in generalization performance over either semi-supervised single-task
learning (STL) or supervised MTL.

1 Introduction

Supervised learning has proven an effective technique for learning a classiﬁer when the quantity of
labeled data is large enough to represent a sufﬁcient sample from the true labeling function. Un-
fortunately, a generous provision of labeled data is often not available since acquiring the label of
a datum is expensive in many applications. A classiﬁer supervised by a limited amount of labeled
data is known to generalize poorly even if it produces zero training errors. There has been much
recent work on improving the generalization of classiﬁers based on using information sources be-
yond the labeled data. These studies fall into two major categories: (i) semi-supervised learning
[9, 12, 15, 10] and (ii) multitask learning (MTL) [3, 1, 13]. The former employs the information
from the data manifold, in which the manifold information provided by the usually abundant unla-
beled data is exploited, while the latter leverages information from related tasks.
In this paper we attempt to integrate the beneﬁts offered by semi-supervised learning and MTL, by
proposing semi-supervised multitask learning. The semi-supervised MTL framework consists of M
semi-supervised classiﬁers coupled by a joint prior distribution over the parameters of all classiﬁers.
Each classiﬁer provides the solution for a partially labeled data classiﬁcation task. The solutions for
the M tasks are obtained simultaneously under the uniﬁed framework.
Existing semi-supervised algorithms are often not directly amenable to MTL extensions. Transduc-
tive algorithms directly operate on labels. Since the label is a local property of the associated data
point, information sharing must be performed at the level of data locations, instead of at the task
level. The inductive algorithm in [10] employs a data-dependent prior to encode manifold informa-
tion. Since the information transferred from related tasks is also often represented by a prior, the
two priors will compete and need be balanced; moreover, this precludes a Dirichlet process [6] or
its variants to represent the sharing prior across tasks, because the base distribution of a Dirichlet
process cannot be dependent on any particular manifold.
We develop a new semi-supervised formulation, which enjoys several nice properties that make the
formulation immediately amenable to an MTL extension. First, the formulation has a parametric
classiﬁer built for each task, thus multitask learning can be performed efﬁciently at the task level,
using the parameters of the classiﬁers. Second, the formulation encodes the manifold information

of each task inside the associated likelihood function, sparing the prior for exclusive use by the
information from related tasks. Third, the formulation lends itself to a Dirichlet process, allowing
the tasks to share information in a complex manner.
The new semi-supervised formulation is used as a key component of our semi-supervised MTL
framework. In the MTL setting, we have M partially labeled data manifolds, each deﬁning a clas-
siﬁcation task and involving design of a semi-supervised classiﬁer. The M classiﬁers are designed
simultaneously within a uniﬁed sharing structure. The key component of the sharing structure is a
soft variant of the Dirichlet process (DP), which implements a soft-sharing prior over the parameters
of all classiﬁers. The soft-DP retains the clustering property of DP and yet does not require exact
sharing of parameters, which increases ﬂexibility and promotes robustness in information sharing.

2 Parameterized Neighborhood-Based Classiﬁcation

The new semi-supervised formulation, termed parameterized neighborhood-based classiﬁcation
(PNBC), represents the class probability of a data point by mixing over all data points in the neigh-
borhood, which is formed via Markov random walk over a graph representation of the manifold.

2.1 Neighborhoods Induced by Markov Random Walk
Let G = (X , W) be a weighted graph such that X = {x1 , x2 , · · ·, xn} is a set of vertices that
coincide with the data points in a ﬁnite data manifold, and W = [wij ]n×n is the afﬁnity matrix with
the (i, j )-th element wij indicating the immediate afﬁnity between data points xi and xj . We follow
[12, 15] to deﬁne wij = exp(−0.5 (cid:107)xi − xj (cid:107)2/σ2
i ), where (cid:107) · (cid:107) is the Euclidean norm and σij > 0.
A Markov random walk on graph G = (X , W) is characterized by a matrix of one-step transition
wij(cid:80)n
probabilities A = [aij ]n×n , where aij is the probability of transiting from xi to xj via a single step
and is given by aij =
[4]. Let B = [bij ]n×n = At . Then (i, j )-th element bij represents
wik
k=1
the probability of transiting from xi to xj in t steps.
Data point xj is said to be a t-step neighbor of xi if bij > 0. The t-step neighborhood of xi ,
denoted as Nt (xi ), is deﬁned by all t-step neighbors of xi along with the associated t-step transition
probabilities, i.e., Nt (xi ) = {(xj , bij ) : bij > 0, xj ∈ X }. The appropriateness of a t-step
neighborhood depends on the right choice of t. A rule of choosing t is given in [12], based on
maximizing the margin of the associated classiﬁer on both labeled and unlabeled data points.
The σi in specifying wij represents the step-size (distance traversed in a single step) for xi to reach
its immediate neighbor, and we have used a distinct σ for each data point. Location-dependent
step-sizes allow one to account for possible heterogeneities in the data manifold — at locations with
dense data distributions a small step-size is suitable, while at locations with sparse data distributions
a large step-size is appropriate. A simple choice of heterogeneous σ is to let σi be related to the
distance between xi and close-by data points, where closeness is measured by Euclidean distance.
Such a choice ensures each data point is immediately connected to some neighbors.

2.2 Formulation of the PNBC Classiﬁer
Let p∗ (yi |xi , θ) be a base classiﬁer parameterized by θ , which gives the probability of class label
yi of data point xi , given xi alone (which is a zero-step neighborhood of xi ). The base classiﬁer
can be implemented by any parameterized probabilistic classiﬁer. For binary classiﬁcation with
y ∈ {−1, 1}, the base classiﬁer can be chosen as logistic regression with parameters θ , which
expresses the conditional class probability as
p∗ (yi |xi , θ) = [1 + exp(−yiθT xi )]−1
(1)
where a constant element 1 is assumed to be preﬁxed to each x (the preﬁxed x is still denoted as x
for notational simplicity), and thus the ﬁrst element in θ is a bias term.
Let p(yi |Nt (xi ), θ) denote a neighborhood-based classiﬁer parameterized by θ , representing the
(cid:80)n
probability of class label yi for xi , given the neighborhood of xi . The PNBC classiﬁer is deﬁned as
a mixture
p(yi |Nt (xi ), θ) =
j=1 bij p∗ (yi |xj , θ)

(2)

where the j -th component is the base classiﬁer applied to (xj , yi ) and the associated mixing propor-
tion is deﬁned by the probability of transiting from xi to xj in t steps. Since the magnitude of bij
automatically determines the contribution of xj to the mixture, we let index j run over the entire X
for notational simplicity.
The utility of unlabeled data in (2) is conspicuous — in order for xi to be labeled yi , each neighbor
xj must be labeled consistently with yi , with the strength of consistency proportional to bij ; in
such a manner, yi implicitly propagates over the neighborhood of xi . By taking neighborhoods
into account, it is possible to obtain an accurate estimate of θ , based on a small amount of labeled
data. The over-ﬁtting problem associated with limited labeled data is ameliorated in the PNBC
formulation, through enforcing consistent labeling over each neighborhood.
Let L ⊆ {1, 2, · · · , n} denote the index set of labeled data in X . Assuming the labels are condition-
(cid:161){yi , i ∈ L}|{Nt (xi ) : i ∈ L}, θ
(cid:162)
(cid:81)
(cid:81)
(cid:80)n
ally independent, we write the neighborhood-conditioned likelihood function
j=1 bij p∗ (yi |xj , θ)
i∈L p(yi |Nt (xi ), θ) =
=
p
3 The Semi-Supervised MTL Framework

i∈L

(3)

3.1 The sharing prior

(4)

Suppose we are given M tasks, deﬁned by M partially labeled data sets
: i = 1, 2, · · · , nm } ∪ {ym
: i ∈ Lm }
Dm = {xm
i
i
for m = 1, · · · , M , where ym
i and Lm ⊂ {1, 2, · · · , nm } is the index set
is the class label of xm
of labeled data in task m. We consider M PNBC classiﬁers, parameterized by θm , m = 1, · · · , M ,
i
(cid:81)M
with θm responsible for task m. The M classiﬁers are not independent but coupled by a prior joint
distribution over their parameters
p(θ1 , · · · , θM ) =
m=1 p(θm |θ1 , · · · , θm−1 )
(cid:164)
(cid:163)
(cid:80)m−1
with the conditional distributions in the product deﬁned by
p(θm |θ1 , · · · , θm−1 ) =
αp(θm |Υ) +
l=1 N (θm ; θl , η2 I)
(5)
1
α+m−1
where α > 0, p(θm |Υ) is a base distribution parameterized by Υ, N ( · ; θl , η2 I) is a normal dis-
tribution with mean θl and covariance matrix η2 I. As discussed below, the prior in (4) is linked to
Dirichlet processes and thus is more general than a parametric prior, as used, for example, in [5].
Each normal distribution represents the prior transferred from a previous task; it is the meta-
knowledge indicating how the present task should be learned, based on the experience with a previ-
ous task. It is through these normal distributions that information sharing between tasks is enforced.
Taking into account the data likelihood, unrelated tasks cannot share since they have dissimilar solu-
tions and forcing them to share the same solution will decrease their respective likelihood; whereas,
related tasks have close solutions and sharing information helps them to ﬁnd their solutions and
improve their data likelihoods.
The base distribution represents the baseline prior, which is exclusively used when there are no
previous tasks available, as is seen from (5) by setting m = 1. When there are m − 1 previous
α+m−1 , and uses the prior transferred from each
tasks, one uses the baseline prior with probability
α
of the m − 1 previous tasks with probability
α+m−1 . The α balances the baseline prior and the
1
priors imposed by previous tasks. The role of baseline prior decreases as m increases, which is in
agreement with our intuition, since the information from previous tasks increase with m.
The formulation in (5) is suggestive of the polya urn representation of a Dirichlet process (DP) [2].
The difference here is that we have used a normal distribution to replace Dirac delta in Dirichlet
processes. Since N (θm |θl , η2 I) approaches Dirac delta δ(θm − θl ) as η2 → 0, we recover the
Dirichlet process in the limit case when limit case when η2 → 0.
The motivation behind the formulation in (5) is twofold. First, a normal distribution can be regarded
as a soft version of the Dirac delta. While the Dirac delta requires two tasks to have exactly the
same θ when sharing occurs, the soft delta only requires sharing tasks to have similar θ’s. The

soft sharing may therefore be more consistent with situations in practical applications. Second, the
normal distribution is analytically more appealing than the Dirac delta and allows simple maximum
a posteriori (MAP) solutions. This is an attractive property considering that most classiﬁers do not
have conjugate priors for their parameters and Bayesian learning cannot be performed exactly.
Under the sharing prior in (4), the current task is equally inﬂuenced by each previous task but is
inﬂuenced unevenly by future tasks — a distant future task has less inﬂuence than a near future task.
The ordering of the tasks imposed by (4) may in principle affect performance, although we have not
(cid:163)
(cid:164)
(cid:80)
found this to be an issue in the experimental results. Alternatively, one may obtain a sharing prior
that does not depend on task ordering, by modifying (5) as
p(θm |θ−m ) =
αp(θm |Υ) +
l (cid:54)=mN (θm ; θl , η2 I)
(6)
1
α+M −1
where θ−m = {θ1 , · · · , θM } \ {θm}. The prior joint distribution of {θ1 , · · · , θM } associated with
the full conditionals in (6) is not analytically available, nether is the corresponding posterior joint
distribution, which causes technical difﬁculties in performing MAP estimation.

3.2 Maximum A Posteriori (MAP) Estimation
(cid:162)
(cid:161){ym
Assuming that, given {θ1 , · · · , θM }, the class labels of different tasks are conditionally independent,
(cid:81)M
(cid:81)
(cid:80)nm
the joint likelihood function over all tasks can be written as
i ) : i ∈ Lm}M
m=1 |{Nt (xm
i , i ∈ Lm}M
m=1 , {θm}M
p
m=1
i |xm
ij p∗ (ym
=
j , θm )
(7)
j=1 bm
i∈Lm
m=1
where the m-th term in the product is taken from (3), with the superscript m indicating the task
index. Note that the neighborhoods are built for each task independently of other tasks, thus a
random walk is always restricted to the same task (the one where the starting data point belongs)
(cid:161){θm }M
(cid:162)
and can never traverse multiple tasks. From (4), (5), and (7), one can write the logarithm of the joint
posterior of {θ1 , · · · , θM }, up to a constant translation that does not depend on {θ1 , · · · , θM },
(cid:170)
(cid:164)
(cid:169)
(cid:163)
(cid:80)nm
(cid:80)
(cid:80)m−1
(cid:80)M
i ) : i ∈ Lm }M
m=1 , {Nt (xm
i , i ∈ Lm }M
(cid:96)MAP (θ1 , · · · , θM ) = ln p
m=1 |{ym
m=1
i |xm
αp(θm |Υ) +
ij p∗ (ym
j , θm )
ln
l=1 N (θm ; θl , η2 I)
+
ln
=
(8)
j=1 bm
i∈Lm
m=1
We seek the parameters {θ1 , · · · , θM } that maximize the log-posterior, which is equivalent to si-
multaneously maximizing the prior in (4) and the likelihood function in (7). As seen from (5), the
prior tends to have similar θ’s across tasks (similar θ’s increase the prior); however sharing between
unrelated tasks is discouraged, since each task requires a distinct θ to make its likelihood large.
As a result, to make the prior and the likelihood large at the same time, one must let related tasks
have similar θ’s. Although any optimization techniques can be applied to maximize the objective
function (8), expectation maximization (EM) is particularly suitable, since the objective function
involves summations under the logarithmic operation. To conserve space the algorithmic details are
omitted here.
Utilization of the manifold information and the information from related tasks has greatly reduced
the hypothesis space. Therefore, point MAP estimation in semi-supervised MTL will not suffer as
much from overﬁtting as in supervised STL. This argument will be supported by the experimental re-
sults in Section 4.2, where semi-supervised MTL outperforms both supervised MTL and supervised
STL, although the former is based on MAP and the latter two are based on Bayesian learning.
With MAP estimation, one obtains the parameters of the base classiﬁer in (1) for each task, which
can be employed to predict the class label of any data point in the associated task, regardless of
whether the data point has been seen during training. In the special case when predictions are desired
only for the unlabeled data points seen during training (transductive learning), one can alternatively
employ the PNBC classiﬁer in (2) to perform the predictions.

4 Experimental Results

First we consider semi-supervised learning on a single task and establish the competitive perfor-
mance of the PNBC in comparison with existing semi-supervised algorithms. Then we demonstrate
the performance improvements achieved by semi-supervised MTL, relative to semi-supervised STL
and supervised MTL. Throughout this section, the base classiﬁer in (1) is logistic regression.

4.1 Performance of the PNBC on a Single Task

Figure 1: Transductive results of the PNBC. The horizontal axis is the size of XL .

Figure 2: Inductive results of the PNBC on Ionosphere. The horizontal axis is the size of XU .

The PNBC is evaluated on three benchmark data sets – Pima Indians Diabetes Database (PIMA),
Wisconsin Diagnostic Breast Cancer (WDBC) data, and Johns Hopkins University Ionosphere
database (Ionosphere), which are taken from the UCI machine learning repository [11]. The evalu-
ation is performed in comparison to four existing semi-supervised learning algorithms, namely, the
transductive SVM [9], the algorithm of Szummer & Jaakkola [12], GRF [15], and Logistic GRF
[10]. The performance is evaluated in terms of classiﬁcation accuracy, deﬁned as the ratio of the
number of correctly classiﬁed data over the total number of data being tested.
We consider two testing modes: transductive and inductive. In the transductive mode, the test data
are the unlabeled data that are used in training the semi-supervised algorithms; in the inductive
mode, the test data are a set of holdout data unseen during training. We follow the same procedures
as used in [10] to perform the experiments. Denote by X any of the three benchmark data sets and
Y the associated set of class labels. In the transductive mode, we randomly sample XL ⊂ X and
assume the associated class labels YL are available; the semi-supervised algorithms are trained by
X ∪ YL and tested on X \ XL . In the inductive mode, we randomly sample two disjoint data subsets
XL ⊂ X and XU ⊂ X , and assume the class labels YL associated with XL are available; the semi-
supervised algorithms are trained by XL ∪ YL ∪ XU and tested on 200 data randomly sampled from
X \ (XL ∪ XU ).
The comparison results are summarized in Figures 1 and 2, where the results of the PNBC and the
algorithm of Szummer & Jaakkola are calculated by us, and the results of remaining algorithms are
cited from [10]. The algorithm of Szummer & Jaakkola [12] and the PNBC use σi = minj (cid:107)xi −
xj (cid:107)/3 and t = 100; learning of the PNBC is based on MAP estimation. Each curve in the ﬁgures
is a result averaged from T independent trials, with T = 20 for the transductive results and T = 50
for the inductive results. In the inductive case, the comparison is between the proposed algorithm
and the Logistic GRF, as the others are transductive algorithms.
For the PNBC, we can either use the base classiﬁer in (1) or the PNBC classiﬁer in (2) to predict the
labels of unlabeled data seen in training (the transductive mode). In the inductive mode, however,
the {bij } are not available for the test data (unseen in training) since they are not in the graph
representation, therefore we can only employ the base classiﬁer. In the legends of Figures 1 and 2, a
sufﬁx “II” to PNBC indicates that the PNBC classiﬁer in (2) is employed in testing; when no sufﬁx
is attached, the base classiﬁer is employed in testing.
Figures 1 and 2 show that the PNBC outperforms all the competing algorithms in general, regardless
of the number of labeled data points. The improvements are particularly signiﬁcant on PIMA and

204060800.620.640.660.680.70.720.74Number of labeled dataAccuracy on unlabeled dataPIMAPNBCSzummer & JaakkolaLogistic GRFGRFTransductive SVM204060800.860.880.90.920.940.96Number of labeled dataAccuracy on unlabeled dataWDBCPNBCSzummer & JaakkolaLogistic GRFGRFTransductive SVM204060800.650.70.750.80.850.9Number of Labeled DataAccuracy on unlabeled dataIonospherePNBCSzummer & JaakkolaLogistic GRFGRFTransductive SVMPNBC−II0204060801001200.50.60.70.80.9Number of Unlabeled SamplesAccuracy on separated test data10 labeled samplesPNBCLogistic GRF0204060801001200.50.60.70.80.9Number of Unlabeled SamplesAccuracy on separated test data20 labeled samplesPNBCLogistic GRF0204060801001200.50.60.70.80.9Number of Unlabeled SamplesAccuracy on separated test data30 labeled samplesPNBCLogistic GRF0204060801001200.50.60.70.80.9Number of Unlabeled SamplesAccuracy on separated test data40 labeled samplesPNBCLogistic GRFIonosphere. As indicated in Figure 1(c), employing manifold information in testing by using (2)
can improve classiﬁcation accuracy in the transductive learning case. The margin of improvements
achieved by the PNBC in the inductive learning case is striking and encouraging — as indicated
by the error bars in Figure 2, the PNBC signiﬁcantly outperforms Logistic GRF in almost all indi-
vidual trials. Figure 2 also shows that the advantage of the PNBC becomes more conspicuous with
decreasing amount of labeled data considered during training.

4.2 Performance of the Semi-Supervised MTL Algorithm

We compare the proposed semi-supervised MTL against: (a) semi-supervised single-task learning
(STL), (b) supervised MTL, (c) supervised STL, (d) supervised pooling; STL refers to designing
M classiﬁers independently, each for the corresponding task, and pooling refers to designing a sin-
gle classiﬁer based on the data of all tasks. Since we have evaluated the PNBC in Section 4.1 and
established its effectiveness, we will not repeat the evaluation here and employ PNBC as a represen-
tative semi-supervised learning algorithm in semi-supervised STL. To replicate the experiments in
[13], we employ AUC as the performance measure, where AUC stands for area under the receiver
operation characteristic (ROC) curve [7].
The basic setup of the semi-supervised MTL algorithm is as follows. The tasks are ordered as
they are when the data are provided to the experimenter (we have randomly permuted the tasks and
found the performance does not change much). A separate t-neighborhood is employed to represent
the manifold information (consisting of labeled and unlabeled data points) for each task, where the
step-size at each data point is one third of the shortest distance to the remaining points and t is set
to half the number of data points. The base prior p(θm |Υ) = N (θm ; 0, υ2 I) and the soft delta is
N (θm ; θl , η2 I), where υ = η = 1. The α balancing the base prior and the soft delta’s is 0.3. These
settings represent the basic intuition of the experimenter; they have not been tuned in any way and
therefore do not necessarily represent the best settings for the semi-supervised MTL algorithm.

(a)
(b)
Figure 3: (a) Performance of the semi-supervised MTL algorithm on landmine detection, in com-
parison to the remaining ﬁve algorithms. (b) The Hinton diagram of between-task similarity when
there are 140 labeled data in each task.

Landmine Detection First we consider the remote sensing problem considered in [13], based on
data collected from real landmines. In this problem, there are a total of 29 sets of data, collected
from various landmine ﬁelds. Each data point is represented by a 9-dimensional feature vector
extracted from radar images. The class label is binary (mine or false mine). The data are available
at http://www.ee.duke.edu/∼lcarin/LandmineData.zip.
Each of the 29 data sets deﬁnes a task, in which we aim to ﬁnd landmines with a minimum number
of false alarms. To make the results comparable to those in [13], we follow the authors there and take
data sets 1-10 and 16-24 to form 19 tasks. Of the 19 selected data sets, 1-10 are collected at foliated
regions and 11-19 are collected at regions that are bare earth or desert. Therefore we expect two
dominant clusters of tasks, corresponding to the two different types of ground surface conditions.
To replicate the experiments in [13], we perform 100 independent trials, in each of which we ran-
domly select a subset of data for which labels are assumed available, train the semi-supervised MTL

204060801001201400.550.60.650.70.750.8Number of Labeled Data in Each TaskAverage AUC on 19 tasksSupervised STLSupervised PoolingSupervised MTLSemi−Supervised STLSemi−Supervised MTL2468101214161824681012141618Index of Landmine FieldIndex of Landmine Fieldand semi-supervised STL classiﬁers, and test the classiﬁers on the remaining data. The AUC av-
eraged over the 19 tasks is presented in Figure 3(a), as a function of the number of labeled data,
where each curve represents the mean calculated from the 100 independent trials and the error bars
represent the corresponding standard deviations. The results of supervised STL, supervised MTL,
and supervised pooling are cited from [13].
Semi-supervised MTL clearly yields the best results up to 80 labeled data points; after that super-
vised MTL catches up but semi-supervised MTL still outperforms the remaining three algorithms
by signiﬁcant margins. In this example semi-supervised MTL seems relatively insensitive to the
amount of labeled data; this may be attributed to the doubly enhanced information provided by the
data manifold plus the related tasks, which signiﬁcantly augment the information available in the
limited labeled data. The superiority of supervised pooling over supervised STL on this dataset sug-
gests that there are signiﬁcant beneﬁts offered by sharing across the tasks, which partially explains
why supervised MTL eventually catches up with semi-supervised MTL.
We plot in Figure 3(b) the Hinton diagram [8] of the between-task sharing matrix (an average over
the 100 trials) found by the semi-supervised MTL when there are 140 labeled data in each task.
The (m, l)-th element of similarity matrix is equal to exp(− (cid:107)θm−θl (cid:107)2
) (normalized such that the
2
maximum element is one), which is represented by a square in the Hinton diagram, with a larger
square indicating a larger value of the corresponding element. As seen from Figure 3(b), there is a
dominant sharing among tasks 1-10 and another dominant sharing among tasks 11-19. Recall from
the beginning of the section that data sets 1-10 are from foliated regions and data sets 11-19 are
from regions that are bare earth or desert. Therefore, the sharing is in agreement with the similarity
between tasks.

Art Images Retrieval We now consider the problem of art image retrieval [14, 13], in which we
have a library of 642 art images and want to retrieve the images based on a user’s preference. The
preference of each user is available on a subset of images, therefore the objective is to learn the pref-
erence of each user based on a subset of training examples. Each image is represented by a vector
of features and a user’s rating is represented by a binary label (like or dislike). The users’ prefer-
ences are collected in a web-based survey, which can be found at http://honolulu.dbs.informatik.uni-
muenchen.de:8080/paintings/index.jsp.
We consider the same 69 users as considered in [13], who each rated more than 100 images. The
preference prediction for each user is treated as a task, with the associated set of ground truth data
deﬁned by the images rated by the user. These 69 tasks are used in our experiment to evaluate
the performance of semi-supervised MTL. Since two users may give different ratings to exactly the
same image, pooling the tasks together can lead to multiple labels for the same data point. For this
reason, we exclude supervised pooling and semi-supervised pooling in the performance comparison.

Figure 4: Performance of the semi-supervised MTL algorithm on art image retrieval, in comparison
to the remaining three algorithms.

Following [13], we perform 50 independent trials, in each of which we randomly select a subset of
images rated by each user, train the semi-supervised MTL and semi-supervised STL classiﬁers, and
test the classiﬁers on the remaining images. The AUC averaged over the 69 tasks is presented in
Figure 4, as a function of the number of labeled data (rated images), where each curve represents

5101520253035404550550.520.530.540.550.560.570.580.590.60.610.62Number of Labeled Data for Each TaskAverage AUC on 68 tasksSupervised STLSupervised MTLSemi−supervised STLSemi−supervised MTLthe mean calculated from the 50 independent trials and the error bars represent the corresponding
standard deviations. The results of supervised STL and supervised MTL are cited from [13].
Semi-supervised MTL performs very well, improving upon results of the three other algorithms by
signiﬁcant margins in almost all individual trials (as seen from the error bars). It is noteworthy
that the performance improvement achieved by semi-supervised MTL over semi-supervised STL
is larger than corresponding improvement achieved by supervised MTL over supervised STL. The
greater improvement demonstrates that unlabeled data can be more valuable when used along with
multitask learning. The additional utility of unlabeled data can be attributed to its role in helping to
ﬁnd the appropriate sharing between tasks.

5 Conclusions

A framework has been proposed for performing semi-supervised multitask learning (MTL). Recog-
nizing that existing semi-supervised algorithms are not conveniently extended to an MTL setting,
we have introduced a new semi-supervised formulation to allow a direct MTL extension. We have
proposed a soft sharing prior, which allows each task to robustly borrow information from related
tasks and is amenable to simple point estimation based on maximum a posteriori. Experimental
results have demonstrated the superiority of the new semi-supervised formulation as well as the
additional performance improvement offered by semi-supervised MTL. The superior performance
of semi-supervised MTL on art image retrieval and landmine detection show that manifold infor-
mation and the information from related tasks could play positive and complementary roles in real
applications, suggesting that signiﬁcant beneﬁts can be offered in practice by semi-supervised MTL.

References
[1] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal of Machine
Learning Research, pages 83–99, 2003.
[2] D. Blackwell and J. MacQueen. Ferguson distributions via polya urn schemes. Annals of Statistics, 1:
353–355, 1973.
[3] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.
[4] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[5] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proc. 17th SIGKDD Conf. on Knowledge
Discovery and Data Mining, 2004.
[6] T. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1:209–230,
1973.
[7] J. Hanley and B. McNeil. The meaning and use of the area under a receiver operating characteristic (ROC)
curve. Radiology, 143:29–36, 1982.
[8] G. E. Hinton and T. J. Sejnowski. Learning and relearning in boltzmann machines. In J. L. McClelland,
D. E. Rumelhart, and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in
the Microstructure of Cognition, volume 1, pages 282–317. MIT Press, Cambridge, MA, 1986.
[9] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In Proc. 16th
International Conf. on Machine Learning (ICML), pages 200–209. Morgan Kaufmann, San Francisco,
CA, 1999.
[10] B. Krishnapuram, D. Williams, Y. Xue, A. Hartemink, L. Carin, and M. Figueiredo. On semi-supervised
classiﬁcation. In Advances in Neural Information Processing Systems (NIPS), 2005.
[11] D.J. Newman, S. Hettich, C.L. Blake, and C.J. Merz. UCI repository of machine learning databases.
http://www.ics.uci.edu/∼mlearn/MLRepository.html, 1998.
[12] M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with markov random walks. In Advances in
Neural Information Processing Systems (NIPS), 2002.
[13] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multi-task learning for classiﬁcation with dirichlet
process priors. Journal of Machine Learning Research (JMLR), 8:35–63, 2007.
[14] K. Yu, A. Schwaighofer, V. Tresp, W.-Y. Ma, and H.J. Zhang. Collaborative ensemble learning: Com-
bining collaborative and content-based information ﬁltering via hierarchical bayes. In Proceedings of the
19th International Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2003), 2003.
[15] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic
functions. In The Twentieth International Conference on Machine Learning (ICML), 2003.


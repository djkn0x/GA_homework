Adaptive optimization of hyperparameters in L2-regularised
logistic regression

Ahmed Abdel-Gawad
ahmedag@stanford.edu

Simon Ratner
sratner@stanford.edu

December 14, 2007

Abstract

We investigate a gradient-based method for adaptive optimization of hyperparameters in logistic
regression models. Adaptive optimization of hyperparameters reduces the computational cost of select-
ing good hyperparameter values, and allows these optimal values to be pinpointed more precisely, as
compared to an exhaustive search of the hyperparameter space.

1 Introduction

Supervised learning methods often include many additional hyperparameters besides the parameters being
optimized, such as the regularization parameters in regularized methods, and kernel parameters. The eﬀec-
tiveness of a particular learning algorithm is strongly inﬂuenced by the choice of such hyperparameters. A
common method for selecting good hyperparameter values is via an exhaustive grid search over the hyper-
parameter space. While this yields acceptable results, it is often computationally expensive, and outright
infeasible if the number of hyperparameters is large.
Recently, several methods have been proposed for optimizing hyperparameter selection, including cross-
validation optimizations for kernel logistic regression [1], SVM [2] and conditional log-linear models [3]. These
methods rely on re-training the model in the inner loop of the algorithm, after each hyperparameter update.
Further, gradient-based methods have been successfully used in neural networks to optimize regularization
parameters concurrently with the parameters of the model [4, 5]. In this paper, we investigate applying similar
gradient-based techniques for adaptive hyperparameter optimization to L2-regularized logistic regression.

2 Preliminaries

We consider L2-regularized logistic regression, a solution to which maximize the log-likelyhood C over the
|T |(cid:88)
training data set T :
i=1
Additionally, we use a stretched sigmoid approximation to the step function sθ (x) = 1/(1 + exp(−σ1 θT x))
(cid:80)|V |
to deﬁne a validation function, V , as a smooth approximation to the generalization error over the hold-out
validation set V :
(cid:80)|V |
i=1 1{sθ (x(i) ) (cid:54)= y (i) )}
i=1 sθ (x(i) )(1 − 2y (i) ) + y (i)

log p(y (i) |x(i) ; θ) − λ||θ ||2

V (θ) = 1|V |
= 1|V |

(1)

(2)

C (θ) =

1

We use Newton’s method for optimizing the model parameters:
θ(k+1) = θ(k) − H −1
C (θ(k) )∇θ C (θ(k) )
W = diag(hθ(k) (x(i) )(1 − hθ(k) (x(i) )), i ∈ {1 . . . |T |}), and input matrix X = (cid:2)x(1) . . . x(n) (cid:3)T
where θ(k) is the value of θ after k iterations of the algorithm. Let diagonal matrices Λ = diag(0, λ . . .) and
, i ∈ {1 . . . |T |}.
Then the gradient of the ob jective function can be written as
|T |(cid:88)
i=1

(y (i) − hθ(k) (x(i) ))x(i) − 2Λθ(k)

∇θ C (θ(k) ) =

(4)

(3)

and the Hessian matrix is

HC (θ(k) ) = −X T W X − 2Λ
(5)
Training data set T and validation data set V are chosen to be non-overlapping subsets of the available data.

3 Derivation of hyperparameter update rule

We similarly use Newton’s method to update the regularization parameter, λ, with the validation function
used in place of the ob jective. Note that λ(k+1) is a function of the updated model parameters θ(k+1) , while
θ(k) is treated as a constant.
λ(k+1) = λ(k) − H −1
V (θ(k+1) (λ(k) ))∇λV (θ(k+1) (λ(k) ))
Derivation of the gradient and Hessian in equation (6) is based on the method in [4].

(6)

3.1 Gradient

The gradient of the validation function with respect to λ is given by:

(θ(k+1) )T .∇θ V (θ(k+1) (λ(k) ))

(7)

Diﬀerentiating (2) with respect to θ(k+1) :

∇λV (θ(k+1) (λ(k) )) = ∂
∂λ(k)
|V |(cid:88)
i=1

1
|V |

∇θ V (θ(k+1) (λ(k) )) =
−σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
(cid:20) ∂
(cid:20) ∂
(cid:21)
Diﬀerentiating (3) with respect to λ(k) :
∂λ(k)
∂λ(k)
C (θ(k) ), we use the property of matrix inverse HC (θ(k) )H −1
∂λ(k) H −1
C (θ(k) ) = I . Diﬀer-
To ﬁnd the derivative
∂
(cid:21)
(cid:20) ∂
(cid:21)
(cid:20) ∂
entiating with respect to λ(k) :
H −1
C (θ(k) )
HC (θ(k) )
∂λ(k)
∂λ(k)

(cid:21)
∇θ C (θ(k) )

∇θ C (θ(k) ) − H −1
C (θ(k) )

H −1
C (θ(k) ) + HC (θ(k) )

(θ(k+1) ) = −

H −1
C (θ(k) )

(8)

(9)

= 0

(10)

∂
∂λ(k)

2

Diﬀerentiating (5) with respect to λ(k) :

∂
∂λ(k)

(11)

∂
∂λ(k)

From (9), (10) and (11) we now get:

HC (θ(k) ) = 2I
(cid:104)
(cid:105)2
C (θ(k) ) = −2
H −1
H −1
C (θ(k) )
∇θ C (θ(k) ) = 2θ(k)
∂
C (θ(k) ) (cid:2)−H −1
C (θ(k) )∇θ C (θ(k) ) + θ(k) (cid:3)
∂λ(k)
∂λ(k) (θ(k+1) ) = −2H −1
∂
= −2H −1
C (θ(k) )θ(k+1)
Substituting (8) and (14) back into (7), we arrive at the equation for the gradient of the validation function
(cid:104)
C (θ(k) )θ(k+1) (cid:105)T
with respect to λ:
|V |(cid:88)
H −1
i=1

−σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))

∇λV (θ(k+1) (λ(k) )) = − 2
|V |

(15)

(12)

(13)

(14)

3.2 Hessian

(16)

∂
∂λ(k)

(cid:104) ∂
C (θ(k) )θ(k+1) (cid:3)T (cid:105) (cid:80)|V |
(cid:2)H −1
The Hessian of the validation function with respect to λ can be written as:
(cid:104) ∂
(cid:105)
(cid:80)|V |
i=1 −σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
HV (θ(k+1) (λ(k) )) = − 2|V |
∂λ(k)
− 2|V |
i=1 −σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
H −1
C (θ(k) )θ(k+1)
∂λ(k)
C (θ(k) )(cid:3) θ(k+1) + H −1
(cid:2)H −1
∂λ(k) θ(k+1) (cid:3)T
C (θ(k) )θ(k+1) (cid:3)T = (cid:2)(cid:2) ∂
Using (12) and (14):
θ(k+1) (cid:105)T
(cid:104)(cid:2)H −1
C (θ(k) )(cid:3)2
∂λ(k) H −1
C (θ(k) ) ∂
= −4
(cid:80)|V |
Returning to equation (16), the remaining diﬀerentiation can be obtained through the chain rule:
(cid:80)|V |
i=1 −σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
i=1 −σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
(cid:80)|V |
∂λ(k) (θ(k+1) )T
= ∂
∂
∂ θ(k+1)
= (cid:80)|V |
i=1 −σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
∂
∂ θ(k+1)
1 x(i)T x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))(1 − 2sθ (x(i) ))
i=1 −σ2
Substituting (18) along with (17) into (16), we get the ﬁnal form of the Hessian of the validation function
C (θ(k) )2 θ(k+1) (cid:3)T (cid:80)|V |
(cid:2)H −1
with respect to λ:
C (θ(k) )θ(k+1) (cid:3)T
(cid:2)H −1
i=1 −σ1x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))
HV (θ(k+1) (λ(k) )) = 8|V |
(cid:105) (cid:2)H −1
· (cid:104)(cid:80)|V |
C (θ(k) )θ(k+1) (cid:3)
+ 4|V |
1 x(i)T x(i) (1 − 2y (i) )sθ (x(i) )(1 − sθ (x(i) ))(1 − 2sθ (x(i) ))
i=1 σ2
(19)
The quantity H −1
C (θ(k) ) is already available from the parameter update step, so this calculation can be
performed eﬃciently.

∂
∂λ(k)

(17)

(18)

3

(a) 2-d, min error at λ ∈ (0.125, 0.5)

(b) 5-d (2 plotted), min error at λ ∈ (0.25, 1.0)

Figure 1: Grid search over the hyperparameter space in (a) two, and (b) ﬁve-dimensional data set.

4 Results

We compare the accuracy and performance of our algorithm to a grid search over the logarithmic space of
hyperparamer values. Such a grid search over λ is shown in Figure 1, where each graph corresponds to
training using a certain constant value of λ. The data sets we used for these results are generated from
multi-variate Gaussian distributions with two label classes. We added a small amount of noise to the data to
make the use of regularization beneﬁcial. Further, the ﬁve-dimensional data set contains irrelevant features.
In these data sets, there appears to be an optimal value of λ which minimises the generalization error.
Figure 2(a) shows the adaptation of λ during the training phase of our algorithm on a two-dimensional
data set. The minimum value of generalization error reached is 0.00699 at a λ = 0.1575. Figure 2(b) shows
the algorithm running on a ﬁve-dimensional data set, converging on an error of 0.029 at λ = 0.2767. These
results match the approximate values obtained through grid search.
Comparing both methods in terms of speed, the grid search method took 54 and 57 iterations to converge
for the two- and ﬁve-dimensional cases, respectively. On the other hand, the adaptation method took 63
and 111 iterations, respectively. We comment here that the grid search was done for just 8 values of λ
where a larger number might be needed to get more accuracy. Additionally, this comparison is for a single
hyperparameter. We expect that applying the same method to adapt multiple hyperparameters (such as the
kernel parameters) would maintain a comparable level of performance, while the cost of grid search grows
exponentially with the number of hyperparameters searched.

4

−4−202−4−2024λ = 0, εv = 0.0389−4−202−4−2024λ = 0.015625, εv = 0.021−4−202−4−2024λ = 0.03125, εv = 0.013−4−202−4−2024λ = 0.0625, εv = 0.00998−4−202−4−2024λ = 0.125, εv = 0.00699−4−202−4−2024λ = 0.25, εv = 0.00699−4−202−4−2024λ = 0.5, εv = 0.00699−4−202−4−2024λ = 1, εv = 0.00798−4−202−505λ = 0, εv = 0.08−4−202−505λ = 0.015625, εv = 0.052−4−202−505λ = 0.03125, εv = 0.045−4−202−505λ = 0.0625, εv = 0.041−4−202−505λ = 0.125, εv = 0.036−4−202−505λ = 0.25, εv = 0.031−4−202−505λ = 0.5, εv = 0.029−4−202−505λ = 1, εv = 0.031(a) 2-d, optimal λ = 0.1575 after 63 iterations

(b) 5-d, optimal λ = 0.2767 after 111 iterations

Figure 2: Learning λ for (a) two, and (b) ﬁve-dimensional data set.

5 Conclusion and further work

Gradient-based method of adapting hyperparameters during the learning phase was previously used success-
fully in neural networks. This work aimed to study the ability to extend the method to other supervised
learning methods. Since the results of adapting the regularization parameter in L2-regularized logistic re-
gression are promising, we think this method can be further extended to adapt multiple hyperparameters,
such as the kernel parameters in kernel logistic regression.
It is not possible to apply this technique directly to kernel SVM, as SVM is usually solved in its dual
form, where the optimization problem is constrained and not smooth with respect to the hyperparameters.
Recently, however, there has been work on solving SVM in its primal form while maintaining the convenience
of the kernel method [6]; it may be possible to apply the gradient technique in that setting.

References

[1] M. Seeger, “Cross-validation optimization for large scale hierarchical classiﬁcation kernel methods,” in
NIPS, 2006.

[2] S. S. Keerthi, V. Sindhwani, and O. Chapelle, “An eﬃcient method for gradient-based adaptation of
hyperparameters in svm models,” in NIPS, pp. 673–680, 2006.

[3] C. Do, C.-S. Foo, and A. Ng, “Eﬃcient multiple hyperparameter learning for log-linear models,” in NIPS,
2007.

[4] D. Chen and M. T. Hagan, “Optimal use of regularization and cross-validation in neural network model-
ing,” Neural Networks, 1999. IJCNN ’99. International Joint Conference on, vol. 2, pp. 1275–1280, Jul
1999.

[5] R. Eigenmann and J. Nossek, “Gradient based adaptive regularization,” Neural Networks for Signal
Processing IX, 1999. Proceedings of the 1999 IEEE Signal Processing Society Workshop, pp. 87–94, Aug
1999.

[6] O. Chapelle, “Training a support vector machine in the primal,” Neural Computation, vol. 19, pp. 1155–
1178, Mar 2007.

5

010203040506070−0.4−0.3−0.2−0.100.10.20.3 iteration (k)0102030405060700.0060.0070.0080.0090.010.0110.0120.0130.0140.015  εvλ(k+1)∇λV(θ(k+1)(λ(k)))020406080100120−0.4−0.3−0.2−0.100.10.20.3iteration (k)0204060801001200.030.0350.040.0450.05  εvλ(k+1)∇λV(θ(k+1)(λ(k)))
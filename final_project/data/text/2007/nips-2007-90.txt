Density Estimation under Independent Similarly
Distributed Sampling Assumptions

Tony Jebara, Yingbo Song and Kapil Thadani
Department of Computer Science
Columbia University
New York, NY 10027
{ jebara,yingbo,kapil }@cs.columbia.edu

Abstract

A method is proposed for semiparametric estimation where parametric and non-
parametric criteria are exploited in density estimation and unsupervised learning.
This is accomplished by making sampling assumptions on a dataset that smoothly
interpolate between the extreme of independently distributed (or id) sample data
(as in nonparametric kernel density estimators) to the extreme of independent
identically distributed (or iid) sample data. This article makes independent simi-
larly distributed (or isd) sampling assumptions and interpolates between these two
using a scalar parameter. The parameter controls a Bhattacharyya afﬁnity penalty
between pairs of distributions on samples. Surprisingly, the isd method maintains
certain consistency and unimodality properties akin to maximum likelihood esti-
mation. The proposed isd scheme is an alternative for handling nonstationarity in
data without making drastic hidden variable assumptions which often make esti-
mation difﬁcult and laden with local optima. Experiments in density estimation
on a variety of datasets con ﬁrm the value of
isd over iid estimation, id estimation
and mixture modeling.

1 Introduction

Density estimation is a popular unsupervised learning technique for recovering distributions from
data. Most approaches can be split into two categories: parametric methods where the functional
form of the distribution is known a priori (often from the exponential family (Collins et al., 2002;
Efron & Tibshirani, 1996)) and non-parametric approaches which explore a wider range of distri-
butions with less constrained forms (Devroye & Gyor ﬁ, 1985) . Parametric approaches can under ﬁt
or may be mismatched to real-world data if they are built on incorrect a priori assumptions. A
popular non-parametric approach is kernel density estimation or the Parzen windows method (Sil-
verman, 1986). However, these may over- ﬁt thus requiring sm oothing, bandwidth estimation and
adaptation (Wand & Jones, 1995; Devroye & Gyor ﬁ, 1985; Bengi o et al., 2005). Semiparametric
efforts (Olking & Spiegelman, 1987) combine the complementary advantages of both schools. For
instance, mixture models in their in ﬁnite-component setti ng (Rasmussen, 1999) as well as statistical
processes (Teh et al., 2004) make only partial parametric assumptions. Alternatively, one may seed
non-parametric distributions with parametric assumptions (Hjort & Glad, 1995) or augment para-
metric models with nonparametric factors (Naito, 2004). This article instead proposes a continuous
interpolation between iid parametric density estimation and id kernel density estimation. It makes
independent similarly distributed (isd) sampling assumptions on the data. In isd, a scalar parameter
λ trades off parametric and non-parametric properties to produce an overall better density estimate.
The method avoids sampling or approximate inference computations and only recycles well known
parametric update rules for estimation. It remains computationally efﬁcient, unimodal and consistent
for a wide range of models.

This paper is organized as follows. Section 2 shows how id and iid sampling setups can be smoothly
interpolated using a novel isd posterior which maintains log-concavity for many popular models.
Section 3 gives analytic formulae for the exponential family case as well as slight modiﬁcations
to familiar maximum likelihood updates for recovering parameters under isd assumptions. Some
consistency properties of the isd posterior are provided. Section 4 then extends the method to hidden
variable models or mixtures and provides simple update rules. Section 5 provides experiments
comparing isd with id and iid as well as mixture modeling. We conclude with a brief discussion.

2 A Continuum between id and iid

Assume we are given a dataset of N − 1 inputs x1 , . . . , xN −1 from some sample space Ω. Given
a new query input xN also in the same sample space, density estimation aims at recovering a
density function p(x1 , . . . , xN −1 , xN ) or p(xN |x1 , . . . , xN −1 ) using a Bayesian or frequentist ap-
proach. Therefore, a general density estimation task is, given a dataset X = x1 , . . . , xN , recover
p(x1 , . . . , xN ). A common subsequent assumption is that the data points are id or independently
sampled which leads to the following simpliﬁcation:

N
Yn=1
The joint likelihood factorizes into a product of independent singleton marginals pn (xn ) each of
which can be different. A stricter assumption is that all samples share the same singleton marginal:

pid (X ) =

pn (xn ).

p(xn ).

piid (X ) =

N
Yn=1
which is the popular iid sampling situation. In maximum likelihood estimation, either of the above
likelihood scores (pid or piid ) is maximized by exploring different settings of the marginals. The
id setup gives rise to what is commonly referred to as kernel density or Parzen estimation. Mean-
while, the iid setup gives rise to traditional iid parametric maximum likelihood (ML) or maximum
a posteriori (MAP) estimation. Both methods have complementary advantages and disadvantages.
The iid assumption may be too aggressive for many real world problems. For instance, data may
be generated by some slowly time-varying nonstationary distribution or (more distressingly) from
a distribution that does not match our parametric assumptions. Similarly, the id setup may be too
ﬂexible and might over- ﬁt when the marginal
pn (x) is myopically recovered from a single xn .
Consider the parametric ML and MAP setting where parameters Θ = {θ1 , . . . , θN } are used to
de ﬁne the marginals. We will use p(x|θn ) = pn (x) interchangeably. The MAP id parametric
setting involves maximizing the following posterior (likelihood times a prior) over the models:

p(xn |θn )p(θn ).

pid (X , Θ) =

N
Yn=1
To mimic ML, simply set p(θn ) to uniform. For simplicity assume that these singleton priors are
always kept uniform. Parameters Θ are then estimated by maximizing pid . To obtain the iid setup,
we can maximize pid subject to constraints that force all marginals to be equal, in other words
θm = θn for all m, n ∈ {1, . . . , N }.
Instead of applying N (N − 1)/2 hard pairwise constraints in an iid setup, consider imposing
penalty functions across pairs of marginals. These penalty functions reduce the posterior score when
marginals disagree and encourage some stickiness between models (Teh et al., 2004). We measure
the level of agreement between two marginals pm (x) and pn (x) using the following Bhattacharyya
afﬁnity metric (Bhattacharyya, 1943) between two distribu tions:
B (pm , pn ) = B (p(x|θm ), p(x|θn )) = Z pβ (x|θm )pβ (x|θn )dx.
This is a symmetric non-negative quantity in both distributions pm and pn . The natural choice
for the setting of β is 1/2 and in this case, it is easy to verify the afﬁnity is maximal an d equals
one if and only if pm (x) = pn (x). A large family of alternative information divergences exist

to relate pairs of distributions (Topsoe, 1999) and are discussed in the Appendix. In this article,
the Bhattacharyya afﬁnity is preferred since it has some use ful computational, analytic, and log-
concavity properties. In addition, it leads to straightforward variants of the estimation algorithms as
in the id and iid situations for many choices of parametric densities. Furthermore, (unlike Kullback
Leibler divergence) it is possible to compute the Bhattacharyya afﬁnity analytically and efﬁciently
for a wide range of probability models including hidden Markov models (Jebara et al., 2004).

We next de ﬁne (up to a constant scaling) the posterior score f or independent similarly distributed
(isd) data:

(1)

Bλ/N (p(x|θm ), p(x|θn )).

pλ (X , Θ) ∝ Yn
p(xn |θn )p(θn ) Ym 6=n
Here, a scalar power λ/N is applied to each afﬁnity. The parameter λ adjusts the importance of the
similarity between pairs of marginals. Clearly, if λ → 0, then the afﬁnity is always unity and the
marginals are completely unconstrained as in the id setup. Meanwhile, as λ → ∞, the afﬁnity is
zero unless the marginals are exactly identical. This produces the iid setup. We will refer to the isd
posterior as Equation 1 and when p(θn ) is set to uniform, we will call it the isd likelihood. One can
also view the additional term in isd as id estimation with a modiﬁed prior ˜p(Θ) as follows:
˜p(Θ) ∝ Yn
p(θn ) Ym 6=n
This prior is a Markov random ﬁeld tying all parameters in a pa irwise manner in addition to the
standard singleton potentials in the id scenario. However, this perspective is less appealing since it
disguises the fact that the samples are not quite id or iid.

Bλ/N (p(x|θm ), p(x|θn )).

One of the appealing properties of iid and id maximum likelihood estimation is its unimodality for
log-concave distributions. The isd posterior also bene ﬁts from a unique optimum and log-concav ity.
However, the conditional distributions p(x|θn ) are required to be jointly log-concave in both param-
eters θn and data x. This set of distributions includes the Gaussian distribution (with ﬁxed variance)
and many exponential family distributions such as the Poisson, multinomial and exponential distri-
bution. We next show that the isd posterior score for log-concave distributions is log-concave in Θ.
This produces a unique estimate for the parameters as was the case for id and iid setups.

Theorem 1 The isd posterior is log-concave for jointly log-concave density distributions and for
log-concave prior distributions.

log p(θn ) +

Proof 1 The isd log-posterior is the sum of the id log-likelihoods, the singleton log-priors and pair-
wise log-Bhattacharyya afﬁnities:
λ
log pλ (X , Θ) = const + Xn
log p(xn |θn ) + Xn
N Xn Xm 6=n
The id log-likelihood is the sum of the log-probabilities of distributions that are log-concave in the
parameters and is therefore concave. Adding the log-priors maintains concavity since these are log-
concave in the parameters. The Bhattacharyya afﬁnities are log-concave by the following key result
(Prekopa, 1973). The Bhattacharyya afﬁnity for log-concav e distributions is given by the integral
over the sample space of the product of two distributions. Since the term in the integral is a product
of jointly log-concave distributions (by assumption), the integrand is a jointly log-concave function.
Integrating a log-concave function over some of its arguments produces a log-concave function in
the remaining arguments (Prekopa, 1973). Therefore, the Bhattacharyya afﬁnity is log-concave in
the parameters of jointly log-concave distributions. Finally, since the isd log-posterior is the sum of
concave terms and concave log-Bhattacharyya afﬁnities, it must be concave.

log B (pm , pn).

This log-concavity permits iterative and greedy maximization methods to reliably converge in prac-
tice. Furthermore, the isd setup will produce convenient update rules that build upon iid estimation
algorithms. There are additional properties of isd which are detailed in the following sections. We
ﬁrst explore the β = 1/2 setting and subsequently discuss the β = 1 setting.

3 Exponential Family Distributions and β = 1/2

We ﬁrst specialize the above derivations to the case where th e singleton marginals obey the expo-
nential family form as follows:
p(x|θn ) = exp (cid:0)H (x) + θT
n T (x) − A(θn )(cid:1) .
An exponential family distribution is speciﬁed by providin g H , the Lebesgue-Stieltjes integrator, θn
the vector of natural parameters, T , the sufﬁcient statistic, and A the normalization factor (which
is also known as the cumulant-generating function or the log-partition function). Tables of these
values are shown in (Jebara et al., 2004). The function A is obtained by normalization (a Legendre
transform) and is convex by construction. Therefore, exponential family distributions are always
log-concave in the parameters θn . For the exponential family, the Bhattacharyya afﬁnity is c om-
putable in closed form as follows:
B (pm , pn ) = exp (A(θm/2 + θn/2) − A(θm )/2 − A(θn )/2) .
Assuming uniform priors on the exponential family parameters, it is now straightforward to write
an iterative algorithm to maximize the isd posterior. We ﬁnd settings of θ1 , . . . , θN that maximize
the isd posterior or log pλ (X , Θ) using a simple greedy method. Assume a current set of param-
eters is available ˜θ1 , . . . , ˜θN . We then update a single θn to increase the posterior while all other
parameters (denoted ˜Θ/n ) remain ﬁxed at their previous settings. It sufﬁces to consi
der only terms
in log pλ (X , Θ) that are variable with θn :

A(θn ) +

A( ˜θm /2 + θn/2).

N + λ(N − 1)
N

log pλ (X , θn , ˜Θ/n ) = const + θT
n T (xn ) −

2λ
N Xm 6=n
If the exponential family is jointly log-concave in parameters and data (as is the case for Gaussians),
this term is log-concave in θn . Therefore, we can take a partial derivative of it with respect to θn and
set to zero to maximize:
N + λ(N − 1) 
A′ ( ˜θm/2 + θn/2)
λ
N
N Xm 6=n
T (xn ) +
 .
For the Gaussian mean case (i.e. a white Gaussian with covariance locked at identity), we have
A(θ) = θT θ. Then a closed-form formula is easy to recover from the above1. However, a simpler
iterative update rule for θn is also possible as follows. Since A(θ) is a convex function, we can
compute a linear variational lower bound on each A(θm/2 + θn/2) term for the current setting of
θn :

A′ (θn ) =

(2)

+

A(θn )

(θn − ˜θn ).

N + λ(N − 1)
N
T
2A( ˜θm/2 + ˜θn/2) + A′ ( ˜θm /2 + ˜θn/2)

log pλ (X , θn , ˜Θ/n) ≥ const + θT
n T (xn ) −
λ
N Xm 6=n
This gives an iterative update rule of the form of Equation 2 where the θn on the right hand side is
kept ﬁxed at its previous setting (i.e. replace the right han d side θn with ˜θn ) while the equation is
iterated multiple times until the value of θn converges. Since we have a variational lower bound,
each iterative update of θn monotonically increases the isd posterior. We can also work with a robust
(yet not log-concave) version of the isd score which has the form:
log 
B (pm , pn )
λ
N Xn
 Xm 6=n
log ˆpλ (X , Θ) = const + Xn
log p(xn |θn ) + Xn
 .
and leads to the general update rule (where α = 0 reproduces isd and larger α increases robustness):
N + λ(N − 1) 
A′ ( ˜θm/2 + ˜θn /2)
(N − 1)Bα(p(x| ˜θm ), p(x| ˜θn ))
λ
N
N Xm 6=n
T (xn ) +
 .
Pl 6=n Bα(p(x| ˜θl ), p(x| ˜θn ))
We next examine marginal consistency, another important property of the isd posterior.
1The update for the Gaussian mean with covariance=I is: θn =
1
N +λ(N −1)/2 (N xn + λ/2 Pm6=n

A′ (θn ) =

log p(θn ) +

˜θm ).

3.1 Marginal Consistency in the Gaussian Mean Case

For marginal consistency, if a datum and model parameter are hidden and integrated over, this should
not change our estimate. It is possible to show that the isd posterior is marginally consistent at least
in the Gaussian mean case (one element of the exponential family). In other words, marginalizing
over an observation and its associated marginal’s parameter (which can be taken to be xN and θN
without loss of generality) still produces a similar isd posterior on the remaining observations X/N
and parameters Θ/N . Thus, we need:
Z Z pλ (X , Θ)dxN dθN ∝ pλ (X/N , Θ/N ).
We then would recover the posterior formed using the formula in Equation 1 with only N − 1
observations and N − 1 models.

Theorem 2 The isd posterior with β = 1/2 is marginally consistent for Gaussian distributions.

p(xi |θi )

B 2λ/N (pm , pN )dθN

p(xi |θi )

p(θn )

A(θm )
2

−

B 2λ/N (pm , pn )

Proof 2 Start by integrating over xN :
N
N
N −1
Z pλ (X , Θ)dxN ∝
Yn=1
Ym=n+1
Yi=1
Assume the singleton prior p(θN ) is uniform and integrate over θN to obtain:
N −1
N −1
N −1
N −1
B 2λ/N (pm , pn ) Z
Z Z pλ (X , Θ)dxN dθN ∝
Ym=1
Yn=1
Ym=n+1
Yi=1
Consider only the right hand integral and impute the formula for the Bhattacharyya afﬁnity:
N −1
N −1
B 2λ/N (pm , pN )dθN = Z exp   2λ
2 ! dθN
2 (cid:19) −
A (cid:18) θm
A(θN )
θN
Z
Ym=1
Xm=1
2
N
In the (white) Gaussian case A(θ) = θT θ which simpliﬁes the above into:
N −1
B 2λ/N (pm , pN )dθN = Z exp  −
2 (cid:19)! dθN
A (cid:18) θm
θN
Z
Xm=1
2
N −1
N −1
∝ exp  
2 (cid:19) −
A (cid:18) θm
2λ
θn
Xm=n+1
Xn=1
+
N (N − 1)
2
N −1
N −1
2λ
Yn=1
Ym=n+1
N (N −1) (pm , pn)
Reinserting the integral changes the exponent of the pairs of Bhattacharyya afﬁnities between the
(N − 1) models raising it to the appropriate power λ/(N − 1):
N −1
N −1
N −1
Z Z pλ (X , Θ)dxN dθN ∝
Ym=n+1
Yn=1
Yi=1
Therefore, we get the same isd score that we would have obtained had we started with only (N − 1)
data points. We conjecture that it is possible to generalize the marginal consistency argument to
other distributions beyond the Gaussian. The isd estimator thus has useful properties and still agrees
with id when λ = 0 and iid when λ = ∞. Next, the estimator is generalized to handle distributions
beyond the exponential family where latent variables are implicated (as is the case for mixtures of
Gaussians, hidden Markov models, latent graphical models and so on).

B 2λ/(N −1)(pm , pn) = pλ (X/N , Θ/N ).

A(θm )
2

−

∝

B

+

−

p(xi |θi )

N −1
Ym=1

2λ
N

2 !
A(θn )

4 Hidden Variable Models and β = 1

≥ const + log p(xn |θn )p(θn ) +

log pλ (X , θn , ˜Θ/n) = const + log p(xn |θn )p(θn ) +

One important limitation of most divergences between distributions is that they become awkward
when dealing with hidden variables or mixture models. This is because they may involve intractable
integrals. The Bhattacharyya afﬁnity with the setting β = 1, also known as the probability product
kernel, is an exception to this since it only involves integrating the product of two distributions.
In fact, it is known that this afﬁnity is efﬁcient to compute f
or mixtures of Gaussians, multino-
mials and even hidden Markov models (Jebara et al., 2004). This permits the afﬁnity metric to
efﬁciently pull together parameters θm and θn . However, for mixture models, there is the presence
of hidden variables h in addition to observed variables. Therefore, we replace all the marginals
p(x|θn ) = Ph p(x, h|θn ). The afﬁnity is still straightforward to compute for any pai r of latent
variable models (mixture models, hidden Markov models and so on). Thus, evaluating the isd pos-
terior is straightforward for such models when β = 1. We next provide a variational method that
makes it possible to maximize a lower bound on the isd posterior in these cases.
Assume a current set of parameters is available ˜Θ = ˜θ1 , . . . , ˜θN . We will ﬁnd a new setting for θn
that increases the posterior while all other parameters (denoted ˜Θ/n ) remain ﬁxed at their previous
settings. It sufﬁces to consider only terms in log pλ (X , Θ) that depend on θn . This yields:
2λ
log Z p(x| ˜θm )p(x|θn )dx
N Xm 6=n
2λ
N Xm 6=n Z p(x| ˜θm ) log p(x|θn )dx
The application of Jensen’s inequality above produces an auxiliary function Q(θn | ˜Θ/n) which is a
lower-bound on the log-posterior. Note that each density function has hidden variables, p(xn |θn ) =
Ph p(xn , h|θn ). Applying Jensen’s inequality again (as in the Expectation-Maximization or EM
algorithm) replaces the log-incomplete likelihoods over h with expectations over the complete pos-
teriors given the previous parameters ˜θn . This gives isd the following auxiliary function Q(θn | ˜Θ) =
2λ
N Xm 6=n Z p(x| ˜θm ) Xh
p(h|x, ˜θn ) log p(x, h|θn )dx.
p(h|xn , ˜θn ) log p(xn , h|θn ) + log p(θn ) +
Xh
This is a variational lower bound which can be iteratively maximized instead of the original isd
posterior. While it is possible to directly solve for the maximum of Q(θn | ˜Θ) in some mixture
models, in practice, a further simpliﬁcation is to replace t he integral over x with synthesized samples
drawn from p(x| ˜θm ). This leads to the following approximate auxiliary function (based on the law
of large numbers) which is merely the update rule for EM for θn with s = 1, . . . , S virtual samples
xm,s obtained from the m’th model p(x| ˜θm ) for each of the other N − 1 models, ˜Q(θn | ˜Θ) =
2λ
SN Xm 6=n Xs Xh
Xh
We now have an efﬁcient update rule for latent variable model s (mixtures, hidden Markov models,
etc.) which maximizes a lower bound on pλ (X , Θ). Unfortunately, as with most EM implementa-
tions, the arguments for log-concavity no longer hold.

p(h|xn , ˜θn ) log p(xn , h|θn ) + log p(θn ) +

p(h|xm,s , ˜θn ) log p(xm,s , h|θn ).

5 Experiments

A preliminary way to evaluate the usefulness of the isd framework is to explore density estimation
over real-world datasets under varying λ. If we set λ large, we have the standard iid setup and
only ﬁt a single parametric model to the dataset. For small λ, we obtain the kernel density or
Parzen estimator. In between, an iterative algorithm is available to maximize the isd posterior to
obtain potentially superior models θ∗
N . Figure 1 shows the isd estimator with Gaussian
1 , . . . , θ∗
models on a ring-shaped 2D dataset. The new estimator recovers the shape of the distribution more
accurately. To evaluate performance on real data, we aggregate the isd learned models into a single
density estimate as is done with Parzen estimators and compute the iid likelihood of held out test

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2
−1
0
1
λ = 0, α = 0

2

1.5

1

0.5

0

−0.5

−1

−1.5

1.5

1

0.5

0

−0.5

−1

−1.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2
−1.5
−1
−0.5
0
0.5
1
1.5
λ = 1, α = 0

2

−2

−1.5
−1
−0.5
0
0.5
1
1.5
λ = 2, α = 0

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−1.5
−1
−0.5
0
0.5
1
1.5
λ = ∞, α = 0

1.5

1

0.5

0

−0.5

−1

−1.5

1.5

1

0.5

0

−0.5

−1

−1.5

−2
−1
0
1
λ = 0, α = 1
2

2

−2

−1.5
−1
−0.5
0
0.5
1
1.5
λ = 1, α = 1
2

2

−2

−1.5
−1
−0.5
0
0.5
1
1.5
λ = 2, α = 1
2

2

−2
−1.5
−1
−0.5
0
0.5
1
1.5
λ = ∞, α = 1
2

2

2

Figure 1: Estimation with isd for Gaussian models (mean and covariance) on synthetic data.

iid-∞ isd α = 0 isd α = 1
Dataset
iid-5
iid-4
iid-3
iid-2
iid-1
id
2
-1.19e2
-5.61e3 -1.36e3 -1.36e3 -1.19e3 -7.98e2 -6.48e2 -4.86e2
-2.26e2
SPIRAL
-9.79e2
-9.79e2
MIT-CBCL -9.82e2 -1.39e3 -1.19e3 -1.00e3 -1.01e3 -1.10e3 -3.14e3
-1.94e3 -2.02e4 -3.23e4 -2.50e4 -1.68e4 -3.15e4 -4.02e2
-4.47e2
-4.51e2
HEART
-8.09e2
-8.28e2
DIABETES -6.25e3 -2.12e5 -2.85e5 -4.48e5 -2.03e5 -3.40e5 -8.22e2
CANCER -5.80e3 -7.22e6 -2.94e6 -3.92e6 -4.08e6 -3.96e6 -1.22e2
-5.54e2
-5.54e2
-3.41e3 -2.53e4 -1.88e4 -2.79e4 -2.62e4 -3.23e4 -4.56e2
-4.69e2
-4.74e2
LIVER

Table 1: Gaussian test log-likelihoods using id, iid, EM, ∞ GMM and isd estimation.

data via Pτ log (cid:0) 1
n )(cid:1) . A larger score implies a better p(x) density estimate. Table 1
N Pn p(xτ |θ∗
summarizes experiments with the Gaussian (mean and covariance) models. On 6 standard datasets,
we show the average test log-likelihood of Gaussian estimation while varying the settings of λ
compared to a single iid Gaussian, an id Parzen RBF estimator and a mixture of 2 to 5 Gaussians
using EM. Comparisons with (Rasmussen, 1999) are also shown. Cross-validation was used to
choose the σ , λ or EM local minimum (from ten initializations), for the id, isd and EM algorithms
respectively. Train, cross-validation and test split sizes where 80%, 10% and 10% respectively. The
test log-likelihoods show that isd outperformed iid, id and EM estimation and was comparable to
in ﬁnite Gaussian mixture ( iid−∞) models (Rasmussen, 1999) (which is a far more computationally
demanding method).
In another synthetic experiment with hidden Markov models, 40 sequences
of 8 binary symbols were generated using 2 state HMMs with 2 discrete emissions. However, the
parameters generating the HMMs were allowed to slowly drift during sampling (i.e. not iid). The
data was split into 20 training and 20 testing examples. Table 2 shows that the isd estimator for
certain values of λ produced higher test log-likelihoods than id and iid.

6 Discussion

This article has provided an isd scheme to smoothly interpolate between id and iid assumptions in
density estimation. This is done by penalizing divergence between pairs of models using a Bhat-
tacharyya afﬁnity. The method maintains simple update rule s for recovering parameters for exponen-
tial families as well as mixture models. In addition, the isd posterior maintains useful log-concavity
and marginal consistency properties. Experiments show its advantages in real-world datasets where
id or iid assumptions may be too extreme. Future work involves extending the approach into other
aspects of unsupervised learning such as clustering. We are also considering computing the isd pos-

λ = 0
λ = 5 λ = 10 λ = 20 λ = 30 λ = ∞
λ = 4
λ = 3
λ = 2
λ = 1
-5.7153 -5.5875 -5.5692 -5.5648 -5.5757 -5.5825 -5.5849 -5.5856 -5.6152 -5.5721

Table 2: HMM test log-likelihoods using id, iid and isd estimation.

terior with a normalizing constant which depends on λ and thus permits a direct estimate of λ by
maximization instead of cross-validation2.

7 Appendix: Alternative Information Divergences

There is a large family of information divergences (Topsoe, 1999) between pairs of distributions
(Renyi measure, variational distance, χ2 divergence, etc.) that can be used to pull models pm and pn
towards each other. The Bhattacharya, though, is computationally easier to evaluate and minimize
over a wide range of probability models (exponential families, mixtures and hidden Markov models).
An alternative is the Kullback-Leibler divergence D(pmkpn) = R pm (x)(log pm(x)− log pn (x))dx
and its symmetrized variant D(pmkpn)/2 + D(pnkpm)/2. The Bhattacharyya afﬁnity is related to
the symmetrized variant of KL. Consider a variational distribution q that lies between the input pm
and pn . The log Bhattacharyya afﬁnity with β = 1/2 can be written as follows:
log B (pm , pn ) = log Z q(x) ppm (x)pn (x)
q(x)
Thus, B (pm , pn ) ≥ exp(−D(qkpm )/2 − D(qkpn )/2). The choice of q that maximizes the lower
bound on the Bhattacharyya is q(x) = 1
Z ppm (x)pn (x). Here, Z = B (pm , pn ) normalizes q(x)
and is therefore equal to the Bhattacharyya afﬁnity. Thus we have the following property:
−2 log B (pm , pn ) = min
D(qkpm) + D(qkpn ).
q
It is interesting to note that the Jensen-Shannon divergence (another symmetrized variant of KL)
emerges by placing the variational q distribution as the second argument in the divergences:
2J S (pm , pn ) = D(pmkpm/2 + pn/2) + D(pnkpm/2 + pn/2) = min
D(pmkq) + D(pnkq).
q
Simple manipulations then show 2J S (pm , pn ) ≤ min(D(pmkpn ), D(pnkpm )). Thus, there are
close ties between Bhattacharyya, Jensen-Shannon and symmetrized KL divergences.

dx ≥ −D(qkpm)/2 − D(qkpn )/2.

References
Bengio, Y., Larochelle, H., & Vincent, P. (2005). Non-local manifold Parzen windows. Neural Information
Processing Systems.
Bhattacharyya, A. (1943). On a measure of divergence between two statistical populations deﬁned by their
probability distributions. Bull. Calcutta Math Soc.
Collins, M., Dasgupta, S., & Schapire, R. (2002). A generalization of principal components analysis to the
exponential family. NIPS.
Devroye, L., & Gyor ﬁ, L. (1985). Nonparametric density estimation: The l1 view. John Wiley.
Efron, B., & Tibshirani, R. (1996). Using specially designed exponential families for density estimation. The
Annals of Statistics, 24, 2431–2461.
Hjort, N., & Glad, I. (1995). Nonparametric density estimation with a parametric start. The Annals of Statistics,
23, 882–904.
Jebara, T., Kondor, R., & Howard, A. (2004). Probability product kernels. Journal of Machine Learning
Research, 5, 819–844.
Naito, K. (2004). Semiparametric density estimation by local l2 - ﬁtting.
1192.
Olking, I., & Spiegelman, C. (1987). A semiparametric approach to density estimation. Journal of the American
Statistcal Association, 82, 858–865.
Prekopa, A. (1973). On logarithmic concave measures and functions. Acta. Sci. Math., 34, 335–343.
Rasmussen, C. (1999). The inﬁnite Gaussian mixture model. NIPS.
Silverman, B. (1986). Density estimation for statistics and data analysis. Chapman and Hall: London.
Teh, Y., Jordan, M., Beal, M., & Blei, D. (2004). Hierarchical Dirichlet processes. NIPS.
Topsoe, F. (1999). Some inequalities for information divergence and related measures of discrimination. Jour-
nal of Inequalities in Pure and Applied Mathematics, 2.
Wand, M., & Jones, M. (1995). Kernel smoothing. CRC Press.

The Annals of Statistics, 32, 1162–

2Work supported in part by NSF Award IIS-0347499 and ONR Award N000140710507.


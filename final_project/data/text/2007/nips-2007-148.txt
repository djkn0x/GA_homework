Random Features for Large-Scale Kernel Machines

Ali Rahimi
Intel Research Seattle
Seattle, WA 98105
ali.rahimi@intel.com

Benjamin Recht
Caltech IST
Pasadena, CA 91125
brecht@ist.caltech.edu

Abstract

To accelerate the training of kernel machines, we propose to map the input data
to a randomized low-dimensional feature space and then apply existing fast linear
methods. The features are designed so that the inner products of the transformed
data are approximately equal to those in the feature space of a user speciﬁed shift-
invariant kernel. We explore two sets of random features, provide convergence
bounds on their ability to approximate various radial basis kernels, and show
that in large-scale classiﬁcation and regression tasks linear machine learning al-
gorithms applied to these features outperform state-of-the-art large-scale kernel
machines.

1

Introduction

Kernel machines such as the Support Vector Machine are attractive because they can approximate
any function or decision boundary arbitrarily well with enough training data. Unfortunately, meth-
ods that operate on the kernel matrix (Gram matrix) of the data scale poorly with the size of the
training dataset. For example, even with the most powerful workstation, it might take days to
train a nonlinear SVM on a dataset with half a million training examples. On the other hand, lin-
ear machines can be trained very quickly on large datasets when the dimensionality of the data is
small [1, 2, 3]. One way to take advantage of these linear training algorithms for training nonlinear
machines is to approximately factor the kernel matrix and to treat the columns of the factor matrix
as features in a linear machine (see for example [4]). Instead, we propose to factor the kernel func-
tion itself. This factorization does not depend on the data, and allows us to convert the training and
evaluation of a kernel machine into the corresponding operations of a linear machine by mapping
data into a relatively low-dimensional randomized feature space. Our experiments show that these
random features, combined with very simple linear learning techniques, compete favorably in speed
and accuracy with state-of-the-art kernel-based classiﬁcation and regression algorithms, including
those that factor the kernel matrix.
The kernel trick is a simple way to generate features for algorithms that depend only on the inner
product between pairs of input points. It relies on the observation that any positive deﬁnite function
k(x, y) with x, y ∈ Rd deﬁnes an inner product and a lifting φ so that the inner product between
lifted datapoints can be quickly computed as hφ(x), φ(y)i = k(x, y). The cost of this convenience
is that the algorithm accesses the data only through evaluations of k(x, y), or through the kernel
matrix consisting of k applied to all pairs of datapoints. As a result, large training sets incur large
computational and storage costs.
Instead of relying on the implicit lifting provided by the kernel trick, we propose explicitly mapping
the data to a low-dimensional Euclidean inner product space using a randomized feature map z :
Rd → RD so that the inner product between a pair of transformed points approximates their kernel
evaluation:
k(x, y) = hφ(x), φ(y)i ≈ z(x)0z(y).

(1)

1

Unlike the kernel’s lifting φ, z is low-dimensional. Thus, we can simply transform the input with
z, and then apply fast linear learning methods to approximate the answer of the corresponding
nonlinear kernel machine. In what follows, we show how to construct feature spaces that uniformly
approximate popular shift-invariant kernels k(x − y) to within  with only D = O(d−2 log 1
2 )
dimensions, and empirically show that excellent regression and classiﬁcation performance can be
obtained for even smaller D .
at a test point x requires computing f (x) = PN
In addition to giving us access to extremely fast learning algorithms, these randomized feature maps
also provide a way to quickly evaluate the machine. With the kernel trick, evaluating the machine
i=1 cik(xi , x), which requires O(N d) operations to
compute and requires retaining much of the dataset unless the machine is very sparse. This is often
unacceptable for large datasets. On the other hand, after learning a hyperplane w, a linear machine
can be evaluated by simply computing f (x) = w0z(x), which, with the randomized feature maps
presented here, requires only O(D + d) operations and storage.
We demonstrate two randomized feature maps for approximating shift invariant kernels. Our ﬁrst
randomized map, presented in Section 3, consists of sinusoids randomly drawn from the Fourier
transform of the kernel function we seek to approximate. Because this map is smooth, it is well-
suited for interpolation tasks. Our second randomized map, presented in Section 4, partitions the
input space using randomly shifted grids at randomly chosen resolutions. This mapping is not
smooth, but leverages the proximity between input points, and is well-suited for approximating ker-
nels that depend on the L1 distance between datapoints. Our experiments in Section 5 demonstrate
that combining these randomized maps with simple linear learning algorithms competes favorably
with state-of-the-art training algorithms in a variety of regression and classiﬁcation scenarios.

2 Related Work

The most popular methods for large-scale kernel machines are decomposition methods for solving
Support Vector Machines (SVM). These methods iteratively update a subset of the kernel machine’s
coefﬁcients using coordinate ascent until KKT conditions are satisﬁed to within a tolerance [5,
6]. While such approaches are versatile workhorses, they do not always scale to datasets with
more than hundreds of thousands of datapoints for non-linear problems. To extend learning with
kernel machines to these scales, several approximation schemes have been proposed for speeding
up operations involving the kernel matrix.
The evaluation of the kernel function can be sped up using linear random projections [7]. Throwing
away individual entries [7] or entire rows [8, 9, 10] of the kernel matrix lowers the storage and
computational cost of operating on the kernel matrix. These approximations either preserve the
separability of the data [8], or produce good low-rank or sparse approximations of the true kernel
matrix [7, 9]. Fast multipole and multigrid methods have also been proposed for this purpose,
but, while they appear to be effective on small and low-dimensional problems, they have not been
demonstrated on large datasets. Further, the quality of the Hermite or Taylor approximation that
these methods rely on degrades exponentially with the dimensionality of the dataset [11]. Fast
nearest neighbor lookup with KD-Trees has been used to approximate multiplication with the kernel
matrix, and in turn, a variety of other operations [12]. The feature map we present in Section 4 is
reminiscent of KD-trees in that it partitions the input space using multi-resolution axis-aligned grids
similar to those developed in [13] for embedding linear assignment problems.

3 Random Fourier Features

Our ﬁrst set of random features project data points onto a randomly chosen line, and then pass the
resulting scalar through a sinusoid (see Figure 1 and Algorithm 1). The random lines are drawn
from a distribution so as to guarantee that the inner product of two transformed points approximates
the desired shift-invariant kernel.
The following classical theorem from harmonic analysis provides the key insight behind this trans-
formation:
Theorem 1 (Bochner [15]). A continuous kernel k(x, y) = k(x − y) on Rd is positive deﬁnite if
and only if k(δ) is the Fourier transform of a non-negative measure.

2

x

ω

RD

Kernel Name
Gaussian
Laplacian
Cauchy

R2

k(∆)
e− k∆k2
Q
2
2
e−k∆k1
2
1+∆2
d

d

p(ω)
Q
2 e− kωk2
(2π)− D
2
2
1
π(1+ω2
d
d )
e−k∆k1

Figure 1: Random Fourier Features. Each component of the feature map z(x) projects x onto a random
direction ω drawn from the Fourier transform p(ω) of k(∆), and wraps this line onto the unit circle in R2 .
After transforming two points x and y in this way, their inner product is an unbiased estimator of k(x, y). The
table lists some popular shift-invariant kernels and their Fourier transforms. To deal with non-isotropic kernels,
the data may be whitened before applying one of these kernels.

(2)

k(x − y) =

p(ω)ejω 0 (x−y) dω = Eω [ζω (x)ζω (y)∗ ],

If the kernel k(δ) is properly scaled, Bochner’s theorem guarantees that its Fourier transform p(ω)
Z
is a proper probability distribution. Deﬁning ζω (x) = ejω 0 x , we have
Rd
so ζω (x)ζω (y)∗ is an unbiased estimate of k(x, y) when ω is drawn from p.
To obtain a real-valued random feature for k , note that both the probability distribution p(ω) and
the kernel k(∆) are real, so the integrand ejω 0 (x−y) may be replaced with cos ω 0 (x − y). Deﬁning
zω (x) = [ cos(x) sin(x) ]0 gives a real-valued mapping that satisﬁes the condition E [zω (x)0 zω (y)] =
√
k(x, y), since zω (x)0 zω (y) = cos ω 0 (x − y). Other mappings such as zω (x) =
2 cos(ω 0x +
b), where ω is drawn from p(ω) and b is drawn uniformly from [0, 2π ], also satisfy the condition
E [zω (x)0 zω (y)] = k(x, y).
We can lower the variance of zω (x)0 zω (y) by concatenating D randomly chosen zω into a column
PD
√
vector z and normalizing each component by
D . The inner product of points featureized by the
2D-dimensional random feature z, z(x)0z(y) = 1
j=1 zωj (x)zωj (y) is a sample average of
D
zωj (x)zωj (y) and is therefore a lower variance approximation to the expectation (2).
Since zω (x)0 zω (y) is bounded between -1 and 1, for a ﬁxed pair of points x and y, Hoeffd-
ing’s inequality guarantees exponentially fast convergence in D between z(x)0z(y) and k(x, y):
Pr [|z(x)0z(y) − k(x, y)| ≥ ] ≤ 2 exp(−D2 /2). Building on this observation, a much stronger
assertion can be proven for every pair of points in the input space simultaneously:
Claim 1 (Uniform convergence of Fourier features). Let M be a compact subset of Rd with diam-
eter diam(M). Then, for the mapping z deﬁned in Algorithm 1, we have
(cid:19)
(cid:18)
(cid:21)
(cid:20)
(cid:19)2
(cid:18) σp diam(M)
|z(x)0z(y) − k(x, y)| ≥ 
sup
x,y∈M

p ≡ Ep [ω 0ω ]
(cid:16) d
(cid:17)
where σ2
Fur-
the Fourier transform of k .
is the second moment of
supx,y∈M |z(x)0z(y) − k(y, x)| ≤  with any constant probability when D =
ther,
2 log σp diam(M)
Ω
.

The proof of this assertion ﬁrst guarantees that z(x)0z(y) is close to k(x − y) for the centers of an
-net over M × M. This result is then extended to the entire space using the fact that the feature
map is smooth with high probability. See the Appendix for details.
exp (cid:0)−γ kx − yk2 (cid:1), we have σ2
p is equal to the trace of the Hessian of k at 0.
By a standard Fourier identity, the scalar σ2
It
quantiﬁes the curvature of the kernel at the origin. For the spherical Gaussian kernel, k(x, y) =
p = 2dγ .

≤ 28

Pr

exp

− D2
4(d + 2)

,

3

Algorithm 1 Random Fourier Features.
R e−jω 0∆k(∆) d∆.
Require: A positive deﬁnite shift-invariant kernel k(x, y) = k(x − y).
Ensure: A randomized feature map z(x) : Rd → R2D so that z(x)0z(y) ≈ k(x − y).
Let z(x) ≡ q 1
Compute the Fourier transform p of the kernel k : p(ω) = 1
Draw D iid samples ω1 , · · · , ωD ∈ Rd from p.
2π
D x) ]0 .
1 x) ··· cos(ω 0
1 x) ··· sin(ω 0
D [ cos(ω 0
D x) sin(ω 0

4 Random Binning Features

Our second random map partitions the input space using randomly shifted grids at randomly chosen
resolutions and assigns to an input point a binary bit string that corresponds to the bin in which it
falls (see Figure 2 and Algorithm 2). The grids are constructed so that the probability that two points
x and y are assigned to the same bin is proportional to k(x, y). The inner product between a pair of
transformed points is proportional to the number of times the two points are binned together, and is
therefore an unbiased estimate of k(x, y).

≈

+

+

+ · · · =

k(xi , xj )

z1 (xi )0 z1 (xj )

z2 (xi )0 z2 (xj )

z3 (xi )0 z3 (xj )

z(xi )0z(xj )

Figure 2: Random Binning Features. (left) The algorithm repeatedly partitions the input space using a ran-
domly shifted grid at a randomly chosen resolution and assigns to each point x the bit string z (x) associated
with the bin to which it is assigned. (right) The binary adjacency matrix that describes this partitioning has
z (xi )0 z (xj ) in its ij th entry and is an unbiased estimate of kernel matrix.
(cid:16)
(cid:17)
We ﬁrst describe a randomized mapping to approximate the “hat” kernel khat (x, y ; δ) =
0, 1 − |x−y |
on a compact subset of R × R, then show how to construct mappings for
max
δ
more general separable multi-dimensional kernels. Partition the real number line with a grid of
(cid:16)
(cid:17)
pitch δ , and shift this grid randomly by an amount u drawn uniformly at random from [0, δ ]. This
grid partitions the real number line into intervals [u + nδ, u + (n + 1)δ ] for all integers n. The
0, 1 − |x−y |
probability that two points x and y fall in the same bin in this grid is max
[13].
δ
In other words, if we number the bins of the grid so that a point x falls in bin ˆx = b x−u
δ c and y
falls in bin ˆy = b y−u
δ c, then Pru [ ˆx = ˆy |δ ] = khat (x, y ; δ). If we encode ˆx as a binary indicator
vector z (x) over the bins, z (x)0 z (y) = 1 if x and y fall in the same bin and zero otherwise, so
Pru [z (x)0 z (y) = 1|δ ] = Eu [z (x)0 z (y)|δ ] = khat (x, y ; δ). Therefore z is a random map for khat .
compact subset of R × R: k(x, y) = R ∞
Now consider shift-invariant kernels that can be written as convex combinations of hat kernels on a
0 khat (x, y ; δ)p(δ) dδ . If the pitch δ of the grid is sampled
from p, z again gives a random map for k because Eδ,u [z (x)0 z (y)] = Eδ [Eu [z (x)0 z (y)|δ ]] =
Eδ [khat (x, y ; δ)] = k(x, y). That is, if the pitch δ of the grid is sampled from p, and the shift u is
drawn uniformly from [0, δ ] the probability that x and y are binned together is k(x, y). Lemma 1 in
the appendix shows that p can be easily recovered from k by setting p(δ) = δ ¨k(δ). For example, in
the case of the Laplacian kernel, kLaplacian (x, y) = exp(−|x − y |), p(δ) is the Gamma distribution
δ exp(−δ). For the Gaussian kernel, ¨k is not everywhere positive, so this procedure does not yield a
random map.
Qd
Random maps for separable multivariate shift-invariant kernels of the form k(x − y) =
m=1 km (|xm−ym |) (such as the multivariate Laplacian kernel) can be constructed in a similar way
if each km can be written as a convex combination of hat kernels. We apply the above binning pro-
cess over each dimension of Rd independently. The probability that xm and ym are binned together
in dimension m is km (|xm − ym |). Since the binning process is independent across dimensions, the

4

1000000001000000001000000001000000001000000001000000001000000001probability that x and y are binned together in every dimension is Qd
m=1 km (|xm−ym |) = k(x−y).
In this multivariate case, z (x) encodes the integer vector [ ˆx1 ,··· , ˆxd ] corresponding to each bin of the
d-dimensional grid as a binary indicator vector. In practice, to prevent overﬂows when computing
z (x) when d is large, our implementation eliminates unoccupied bins from the representation. Since
functions z into a larger list of features z and scaling by p1/P . The inner product z(x)0z(y) =
there are never more bins than training points, this ensures no overﬂow is possible.
We can again reduce the variance of the estimator z (x)0 z (y) by concatenating P random binning
PP
p=1 zp (x)0 zp (y) is the average of P independent z (x)0 z (y) and has therefore lower variance.
1
P
Since z (x)0 z (y) is binary, Hoeffding’s inequality guarantees that for a ﬁxed pair of points x and y,
z(x)0z(y) converges exponentially quickly to k(x, y) as a function of P . Again, a much stronger
claim is that this convergence holds simultaneously for all points:
Claim 2. Let M be a compact subset of Rd with diameter diam(M). Let α = E [1/δ ] and let Lk
(cid:17)
 − (cid:16) P 2
 ,
(cid:21)
(cid:20)
denote the Lipschitz constant of k with respect to the L1 norm. With z as above, we have
8 + ln 
|z(x)0z(y) − k(x, y)| ≤ 
Lk
sup
Pr
d + 1
x,y∈M
Note that α = R ∞
δ p(δ) dδ = R ∞
¨k(δ) dδ is 1, and Lk = 1 for the Laplacian kernel. The proof
1
of the claim (see the appendix) partitions M × M into a few small rectangular cells over which
0
0
k(x, y) does not change much and z(x) and z(y) are constant. With high probability, at the centers
of these cells z(x)0z(y) is close to k(x, y), which guarantees that k(x, y) and z(x)0z(y) are close
throughout M × M.
Require: A point x ∈ Rd . A kernel function k(x, y) = Qd
Algorithm 2 Random Binning Features.
m=1 km (|xm − ym |), so that pm (∆) ≡
∆¨km (∆) is a probability distribution on ∆ ≥ 0.
Ensure: A randomized feature map z(x) so that z(x)0z(y) ≈ k(x − y).
for p = 1 . . . P do
Draw grid parameters δ, u ∈ Rd with the pitch δm ∼ pm , and shift um from the uniform
distribution on [0, δm ].
Let z return the coordinate of the bin containing x as a binary indicator vector zp (x) ≡
z(x) ≡ q 1
e).
e, · · · , d xd−ud
hash(d x1−u1
δ1
δd
end for
P [ z1 (x)···zP (x) ]0 .

≥ 1 − 36dP α diam(M) exp

5 Experiments

The experiments summarized in Table 1 show that ridge regression with our random features is a fast
way to approximate the training of supervised kernel machines. We focus our comparisons against
the Core Vector Machine [14] because it was shown in [14] to be both faster and more accurate than
other known approaches for training kernel machines, including, in most cases, random sampling of
datapoints [8]. The experiments were conducted on the ﬁve standard large-scale datasets evaluated
in [14], excluding the synthetic datasets. We replicated the results in the literature pertaining to the
CVM, SVMlight , and libSVM using binaries provided by the respective authors.1 For the random
feature experiments, we trained regressors and classiﬁers by solving the ridge regression problem

1We include KDDCUP99 results for completeness, but note this dataset is inherently oversampled: training
an SVM (or least squares with random features) on a random sampling of 50 training examples (0.001% of the
training dataset) is sufﬁcient to consistently yield a test-error on the order of 8%. Also, while we were able
to replicate the CVM’s 6.2% error rate with the parameters supplied by the authors, retraining after randomly
shufﬂing the training set results in 18% error and increases the computation time by an order of magnitude.
Even on the original ordering, perturbing the CVM’s regularization parameter by a mere 15% yields 49% error
rate on the test set [16].

5

Dataset
CPU
regression
6500 instances 21 dims
Census
regression
18,000 instances 119 dims
Adult
classiﬁcation
32,000 instances 123 dims
Forest Cover
classiﬁcation
522,000 instances 54 dims
KDDCUP99 (see footnote)
classiﬁcation
4,900,000 instances 127 dims

Fourier+LS
3.6%
20 secs
D = 300
5%
36 secs
D = 500
14.9%
9 secs
D = 500
11.6%
71 mins
D = 5000
7.3%
1.5 min
D = 50

Binning+LS
5.3%
3 mins
P = 350
7.5%
19 mins
P = 30
15.3%
1.5 mins
P = 30
2.2%
25 mins
P = 50
7.3%
35 mins
P = 10

CVM
5.5%
51 secs

8.8%
7.5 mins

14.8%
73 mins

2.3%
7.5 hrs

6.2% (18%)
1.4 secs (20 secs)

Exact SVM
11%
31 secs
ASVM
9%
13 mins
SVMTorch
15.1%
7 mins
SVMlight
2.2%
44 hrs
libSVM
8.3%
< 1 s
SVM+sampling

Table 1: Comparison of testing error and training time between ridge regression with random features, Core
Vector Machine, and various state-of-the-art exact methods reported in the literature. For classiﬁcation tasks,
the percent of testing points incorrectly predicted is reported, and for regression tasks, the RMS error normal-
ized by the norm of the ground truth.

Figure 3: Accuracy on test data continues to improve as the training set grows. On the Forest dataset, using
random binning, doubling the dataset size reduces testing error by up to 40% (left). Error decays quickly as P
grows (middle). Training time grows slowly as P grows (right).

2 + λkwk2
minw kZ0w − yk2
2 , where y denotes the vector of desired outputs and Z denotes the
matrix of random features. To evaluate the resulting machine on a datapoint x, we can simply
compute w0z(x). Despite its simplicity, ridge regression with random features is faster than, and
provides competitive accuracy with, alternative methods. It also produces very compact functions
because only w and a set of O(D) random vectors or a hash-table of partitions need to be retained.
Random Fourier features perform better on the tasks that largely rely on interpolation. On the other
hand, random binning features perform better on memorization tasks (those for which the standard
SVM requires many support vectors), because they explicitly preserve locality in the input space.
This difference is most dramatic in the Forest dataset.
Figure 3(left) illustrates the beneﬁt of training classiﬁers on larger datasets, where accuracy con-
tinues to improve as more data are used in training. Figure 3(middle) and (right) show that good
performance can be obtained even from a modest number of features.

6 Conclusion

We have presented randomized features whose inner products uniformly approximate many popular
kernels. We showed empirically that providing these features as input to a standard linear learning
algorithm produces results that are competitive with state-of-the-art large-scale kernel machines in
accuracy, training time, and evaluation time.
It is worth noting that hybrids of Fourier features and Binning features can be constructed by con-
catenating these features. While we have focused on regression and classiﬁcation, our features can
be applied to accelerate other kernel methods, including semi-supervised and unsupervised learn-
ing algorithms. In all of these cases, a signiﬁcant computational speed-up can be achieved by ﬁrst
computing random features and then applying the associated linear technique.

6

1001021041060.10.20.30.40.5Training set sizeTesting error102030405023456P% error10203040504008001200Ptraining+testing time (sec)7 Acknowledgements

We thank Eric Garcia for help on early versions of these features, Sameer Agarwal and James R.
Lee for helpful discussions, and Erik Learned-Miller and Andres Corrada-Emmanuel for helpful
corrections.

References
[1] T. Joachims. Training linear SVMs in linear time. In ACM Conference on Knowledge Discovery and
Data Mining (KDD), 2006.
Interior-point methods for massive Support Vector Machines. SIAM
[2] M. C. Ferris and T. S. Munson.
Journal of Optimization, 13(3):783–804, 2003.
[3] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.
In IEEE International Conference on Machine Learning (ICML), 2007.
[4] D. DeCoste and D. Mazzoni. Fast query-optimized kernel machine classiﬁcation via incremental approx-
imate nearest support vectors. In IEEE International Conference on Machine Learning (ICML), 2003.
[5] J. Platt. Using sparseness and analytic QP to speed training of Support Vector Machines. In Advances in
Neural Information Processing Systems (NIPS), 1999.
[6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
[7] D. Achlioptas, F. McSherry, and B. Sch ¨olkopf. Sampling techniques for kernel methods. In Advances in
Neural Information Processing Systems (NIPS), 2001.
[8] A. Blum. Random projection, margins, kernels, and feature-selection. LNCS, 3940:52–68, 2006.
[9] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approximations.
In Foundations of Computer Science (FOCS), pages 378–390, 1998.
[10] P. Drineas and M. W. Mahoney. On the nystrom method for approximating a Gram matrix for improved
kernel-based learning. In COLT, pages 323–337, 2005.
[11] C. Yang, R. Duraiswami, and L. Davis. Efﬁcient kernel machines using the improved fast gauss transform.
In Advances in Neural Information Processing Systems (NIPS), 2004.
[12] Y. Shen, A. Y. Ng, and M. Seeger. Fast gaussian process regression using KD-Trees. In Advances in
Neural Information Processing Systems (NIPS), 2005.
[13] P. Indyk and N. Thaper. Fast image retrieval via embeddings. In International Workshop on Statistical
and Computational Theories of Vision, 2003.
[14] I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core Vector Machines: Fast SVM training on very large data
sets. Journal of Machine Learning Research (JMLR), 6:363–392, 2005.
[15] W. Rudin. Fourier Analysis on Groups. Wiley Classics Library. Wiley-Interscience, New York, reprint
edition edition, 1994.
[16] G. Loosli and S. Canu. Comments on the ‘Core Vector Machines: Fast SVM training on very large data
sets’. Journal of Machine Learning Research (JMLR), 8:291–301, February 2007.
[17] F. Cucker and S. Smale. On the mathematical foundations of learning. Bull. Amer. Soc., 39:1–49, 2001.

A Proofs
R ∞
Lemma 1. Suppose a function k(∆) : R → R is twice differentiable and has the form
0 p(δ) max(0, 1 − ∆
δ ) dδ . Then p(δ) = δ ¨k(δ).
Z ∞
Proof. We want p so that
Z ∆
Z ∞
Z ∞
Z ∞
p(δ) max(0, 1 − ∆/δ) dδ
k(∆) =
0
p(δ) dδ − ∆
p(δ)(1 − ∆/δ) dδ =
p(δ) · 0 dδ +
To solve for p, differentiate twice w.r.t. to ∆ to ﬁnd that ˙k(∆) = − R ∞
=
∆
∆
0
∆
∆ p(δ)/δ dδ and ¨k(∆) =
p(∆)/∆.

p(δ)/δ dδ.

(3)

(4)

7

(5)

(6)

(7)

(cid:19)2

(cid:18) 2rσp


.

≥ 1 − 2

Proof of Claim 1. Deﬁne s(x, y) ≡ z(x)0z(y), and f (x, y) ≡ s(x, y) − k(y, x). Since f , and s
are shift invariant, as their arguments we use ∆ ≡ x − y ∈ M∆ for notational simplicity.
M∆ is compact and has diameter at most twice diam(M), so we can ﬁnd an -net that covers M∆
using at most T = (4 diam M/r)d balls of radius r [17]. Let {∆i }T
i=1 denote the centers of these
balls, and let Lf denote the Lipschitz constant of f . We have |f (∆)| <  for all ∆ ∈ M∆ if
|f (∆i )| < /2 and Lf < 
2r for all i. We bound the probability of these two events.
Since f is differentiable, Lf = k∇f (∆∗ )k, where ∆∗ = arg max∆∈M∆ k∇f (∆)k. We have
f ] = E k∇f (∆∗ )k2 = E k∇s(∆∗ )k2 − E k∇k(∆∗ )k2 ≤ E k∇s(∆∗ )k2 ≤ Epkωk2 = σ2
E [L2
p , so
(cid:19)2
(cid:18) 2rσp
f ≥ t] ≤ E [L2
i ≤
h
by Markov’s inequality, Pr[L2
f ]/t, or
Lf ≥ 
Pr
.
2r

i=1 |f (∆i )| ≥ /8(cid:3) ≤ 2T exp (cid:0)−D2 /2(cid:1) .
Pr (cid:2)∪T
The union bound followed by Hoeffding’s inequality applied to the anchors in the -net gives
(cid:18) 4 diam(M)
(cid:21)
(cid:20)
(cid:19)d
exp (cid:0)−D2 /8(cid:1) −
Combining (5) and (6) gives a bound in terms of the free variable r :
|f (∆)| ≤ 
sup
(cid:16) κ1
(cid:17) 1
∆∈M∆
r
This has the form 1 − κ1 r−d − k2 r2 . Setting r =
d+2 turns this to 1 − 2κ
2
d
, and
d+2
d+2
2 κ
1
κ2
assuming that σp diam(M)
≥ 1 and diam(M) ≥ 1, proves the ﬁrst part of the claim. To prove the

second part of the claim, pick any probability for the RHS and solve for D .
g=1 ddiam(M)/δgm e ≤ (cid:16)
(cid:17)
Proof of Claim 2. M can be covered by rectangles over each of which z is constant. Let δpm be the
P + diam(M) PP
overlapping grids produce at most Nm = PP
pitch of the pth grid along the mth dimension. Each grid has at most ddiam(M)/δpm e bins, and P
m=1Nm ≤ t(P + P diam(M)α)(cid:3) ≥ 1 − d/t.
By Markov’s inequality and the union bound, Pr (cid:2)∀d
1
p=1
δpm
partitions along the mth dimension. The expected value of the right hand side is P + P diam(M)α.
That is, with probability 1 − d/t, along every dimension, we have at most t(P + P diam(M)α)
that PNm
one-dimensional cells. Denote by dmi the width of the ith cell along the mth dimension and observe
i=1 dmi ≤ diam(M). We further subdivide these cells into smaller rectangles of some small
PNm
width r to ensure that the kernel k varies very little over each of these cells. This results in at most
r e ≤ Nm+diam(M)
i=1 d dmi
(cid:17)d
can be covered with T ≤ (cid:16) 3tP α diam(M)
small one-dimensional cells over each dimension. Plugging in the
upper bound for Nm , setting t ≥ 1
αP and assuming α diam(M) ≥ 1, with probability 1 − d/t, M
r
rectangles of side r centered at {xi }T
i=1 .
r
The condition |z (x, y) − k(x, y)| ≤  on M × M holds if |z (xi , yi ) − k(xi , yi )| ≤  − Lk rd
and z(x) is constant throughout each rectangle. With rd = 
, the union bound followed by
Pr [∪ij |z (xi , yj ) − k(xi , yj )| ≥ /2] ≤ 2T 2 exp (cid:0)−P 2 /8(cid:1)
2Lk
Hoeffding’s inequality gives
(cid:19)
(cid:18)
(cid:21)
(cid:20)
Combining this with the probability that z(x) is constant in each cell gives a bound in terms of t:
− 2(3tP α diam(M))d 2Lk
− P 2
≥1 − d
|z (x, y) − k(x, y)| ≤ 
sup
(cid:16) κ1
(cid:17) 1
8
x,y∈M×M

t
d+1 , which results in an
2κ2

This has the form 1 − κ1 t−1 − κ2 td . To prove the claim, set t =
1
upper bound of 1 − 3κ1κ
d+1
2

exp

(8)

Pr

.

Pr

.

8


Compressed Regression

Shuheng Zhou∗ John Lafferty∗ † Larry Wasserman‡

∗Computer Science Department
‡ Department of Statistics
† Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

Recent research has studied the role of sparsity in high dimensional regression and
signal reconstruction, establishing theoretical limits for recovering sparse models
from sparse data.
In this paper we study a variant of this problem where the
original n input variables are compressed by a random linear transformation to
m (cid:28) n examples in p dimensions, and establish conditions under which a sparse
linear model can be successfully recovered from the compressed data. A primary
motivation for this compression procedure is to anonymize the data and preserve
privacy by revealing little information about the original data. We characterize
the number of random projections that are required for `1 -regularized compressed
regression to identify the nonzero coefﬁcients in the true m odel with probabil-
ity approaching one, a property called “sparsistence.” In a ddition, we show that
`1 -regularized compressed regression asymptotically predicts as well as an or-
acle linear model, a property called “persistence.” Finall y, we characterize the
privacy properties of the compression procedure in information-theoretic terms,
establishing upper bounds on the rate of information communicated between the
compressed and uncompressed data that decay to zero.

1

Introduction

Two issues facing the use of statistical learning methods in applications are scale and privacy. Scale
is an issue in storing, manipulating and analyzing extremely large, high dimensional data. Privacy
is, increasingly, a concern whenever large amounts of conﬁd ential data are manipulated within an
organization. It is often important to allow researchers to analyze data without compromising the
privacy of customers or leaking conﬁdential information ou tside the organization. In this paper we
show that sparse regression for high dimensional data can be carried out directly on a compressed
form of the data, in a manner that can be shown to guard privacy in an information theoretic sense.

The approach we develop here compresses the data by a random linear or afﬁne transformation,
reducing the number of data records exponentially, while preserving the number of original input
variables. These compressed data can then be made available for statistical analyses; we focus on
the problem of sparse linear regression for high dimensional data. Informally, our theory ensures
that the relevant predictors can be learned from the compressed data as well as they could be from
the original uncompressed data. Moreover, the actual predictions based on new examples are as
accurate as they would be had the original data been made available. However, the original data
are not recoverable from the compressed data, and the compressed data effectively reveal no more
information than would be revealed by a completely new sample. At the same time, the inference
algorithms run faster and require fewer resources than the much larger uncompressed data would
require. The original data need not be stored; they can be transformed “on the ﬂy” as they come in.

1

†
In more detail, the data are represented as a n × p matrix X . Each of the p columns is an attribute,
and each of the n rows is the vector of attributes for an individual record. The data are compressed
by a random linear transformation X 7→ eX ≡  X , where  is a random m × n matrix with
m (cid:28) n . It is also natural to consider a random afﬁne transformatio n X 7→ eX ≡  X + , where 
is a random m × p matrix. Such transformations have been called “matrix mask ing” in the privacy
literature [6]. The entries of  and  are taken to be independent Gaussian random variables, but
other distributions are possible. We think of eX as “public,” while  and  are private and only
needed at the time of compression. However, even with  = 0 and  known, recovering X from
eX requires solving a highly under-determined linear system and comes with information theoretic
privacy guarantees, as we demonstrate.
In standard regression, a response variable Y = Xβ +  ∈ Rn is associated with the input variables,
where i are independent, mean zero additive noise variables. In compressed regression, we assume
that the response is also compressed, resulting in the transformed response eY ∈ Rm given by Y 7→
eY ≡ Y =  Xβ +  = eX β + e . Note that under compression, ei , i ∈ {1, . . . , m }, in the
transformed noise e =  are no longer independent. In the sparse setting, the parameter β ∈ R p
is sparse, with a relatively small number s = kβ k0 of nonzero coefﬁcients in β . The method we
focus on is `1 -regularized least squares, also known as the lasso [17]. We study the ability of the
compressed lasso estimator to identify the correct sparse set of relevant variables and to predict well.
We omit details and technical assumptions in the following theorems for clarity. Our ﬁrst result
shows that the lasso is sparsistent under compression, meaning that the correct sparse set of relevant
variables is identi ﬁed asymptotically.
Sparsistence (Theorem 3.3): If the number of compressed examples m satisﬁes C1 s 2 log n ps ≤
m ≤ √C2n/ log n, and the regularization parameter λm satisﬁes λm → 0 and m λ2
m / log p →
1
2m keY − eX β k2
∞, then the compressed lasso estimator eβm = arg minβ
2 + λm kβ k1 is sparsistent:
P (cid:0)supp(eβm ) = supp(β )(cid:1) → 1 as m → ∞,where supp(β ) = { j :
j 6= 0}.
Our second result shows that the lasso is persistent under compression. Roughly speaking, per-
sistence [10] means that the procedure predicts well, as measured by the predictive risk R (β ) =
E (cid:0)Y − β T X (cid:1)2 , where X ∈ R p is a new input vector and Y is the associated response. Persistence is
a weaker condition than sparsistency, and in particular does not assume that the true model is linear.
Persistence (Theorem 4.1): Given a sequence of sets of estimators Bn ,m ⊂ R p such that Bn ,m =
{β : kβ k1 ≤ L n ,m } with log2 (n p) ≤ m ≤ n, the sequence of compressed lasso estimators eβn ,m =
2 is persistent with the predictive risk R (β ) = E (cid:0)Y − β T X (cid:1)2 over
kβ k1≤L n ,m keY − eX β k2
argmin
P
uncompressed data with respect to Bn ,m , meaning that R (eβn ,m ) − infkβ k1≤L n ,m R (β )
−→ 0, as
n → ∞, in case L n ,m = o (m / log(n p))1/4.
Our third result analyzes the privacy properties of compressed regression. We evaluate privacy in
information theoretic terms by bounding the average mutual information I (eX ; X )/ n p per matrix
entry in the original data matrix X , which can be viewed as a communication rate. Bounding this
mutual information is intimately connected with the problem of computing the channel capacity of
certain multiple-antenna wireless communication systems [13].
Information Resistence (Propositions 5.1 and 5.2): The rate at which information about X is
= O (cid:0) m
n (cid:1) → 0, where the
revealed by the compressed data eX satisﬁes rn ,m = sup I ( X ; eX )
n p
supremum isoverdistributionson theoriginaldata X .
As summarized by these results, compressed regression is a practical procedure for sparse learning
in high dimensional data that has provably good properties. Connections with related literature are
brieﬂy reviewed in Section 2. Analyses of sparsistence, per sistence and privacy properties appear in
Section 3–5. Simulations for sparsistence and persistence of the compressed lasso are presented in
Section 6. The proofs are included in the full version of the paper, available at http://arxiv.
org/abs/0706.0534.

2

2 Background and Related Work

In this section we brieﬂy review related work in high dimensi onal statistical inference, compressed
sensing, and privacy, to place our work in context.
Sparse Regression. An estimator that has received much attention in the recent literature is the
lasso bβn [17], deﬁned as bβn = arg min 1
2n kY − Xβ k2
2 + λn kβ k1 , where λn is a regularization param-
eter. In [14] it was shown that the lasso is consistent in the high dimensional setting under certain
assumptions. Sparsistency proofs for high dimensional problems have appeared recently in [20]
and [19]. The results and method of analysis of Wainwright [19], where X comes from a Gaussian
ensemble and i is i.i.d. Gaussian, are particularly relevant to the current paper. We describe this
Gaussian Ensemble result, and compare our results to it in Sections 3, 6.Given that under com-
pression, the noise e =  is not i.i.d, one cannot simply apply this result to the compressed case.
Persistence for the lasso was ﬁrst deﬁned and studied by Gree
nshtein and Ritov in [10]; we review
their result in Section 4.
Compressed Sensing. Compressed regression has close connections to, and draws motivation from
compressed sensing [4, 2]. However, in a sense, our motivation is the opposite of compressed
sensing. While compressed sensing of X allows a sparse X to be reconstructed from a small number
of random measurements, our goal is to reconstruct a sparse function of X . Indeed, from the point
of view of privacy, approximately reconstructing X , which compressed sensing shows is possible
if X is sparse, should be viewed as undesirable; we return to this point in Section ??. Several
authors have considered variations on compressed sensing for statistical signal processing tasks
[5, 11]. They focus on certain hypothesis testing problems under sparse random measurements, and
a generalization to classi ﬁcation of a signal into two or mor e classes. Here one observes y = x ,
where y ∈ Rm , x ∈ Rn and  is a known random measurement matrix. The problem is to select
between the hypotheses eHi : y = (si +  ). The proofs use concentration properties of random
projection, which underlie the celebrated Johnson-Lindenstrauss lemma. The compressed regression
problem we introduce can be considered as a more challenging statistical inference task, where the
problem is to select from an exponentially large set of linear models, each with a certain set of
relevant variables with unknown parameters, or to predict as well as the best linear model in some
class.
Privacy. Research on privacy in statistical data analysis has a long history, going back at least to [3].
We refer to [6] for discussion and further pointers into this literature; recent work includes [16]. The
work of [12] is closely related to our work at a high level, in that it considers low rank random linear
transformations of either the row space or column space of the data X . The authors note the Johnson-
Lindenstrauss lemma, and argue heuristically that data mining procedures that exploit correlations
or pairwise distances in the data are just as effective under random projection. The privacy analysis
is restricted to observing that recovering X from eX requires solving an under-determined linear
system. We are not aware of previous work that analyzes the asymptotic properties of a statistical
estimator under random projection in the high dimensional setting, giving information-theoretic
guarantees, although an information-theoretic quanti ﬁca tion of privacy was proposed in [1]. We
cast privacy in terms of the rate of information communicated about X through eX , maximizing over
all distributions on X , and identify this with the problem of bounding the Shannon capacity of a
multi-antenna wireless channel, as modeled in [13]. Finally, it is important to mention the active
area of cryptographic approaches to privacy from the theoretical computer science community, for
instance [9, 7]; however, this line of work is quite different from our approach.

3 Compressed Regression is Sparsistent

In the standard setting, X is a n × p matrix, Y = Xβ +  is a vector of noisy observations under
a linear model, and p is considered to be a constant. In the high-dimensional setting we allow p to
grow with n . The lasso refers to the following: ( P1 ) min kY − Xβ k2
2 such that kβ k1 ≤ L . In
Lagrangian form, this becomes: ( P2 ) min 1
2n kY − Xβ k2
2 + λn kβ k1 . For an appropriate choice of
the regularization parameter λ = λ(Y, L ), the solutions of these two problems coincide.
In compressed regression we project each column X j ∈ Rn of X to a subspace of m dimensions,
using an m × n random projection matrix . Let eX =  X be the compressed design matrix, and
3

let eY = Y be the compressed response. Thus, the transformed noise e is no longer i.i.d.. The
compressed lasso is the following optimization problem, for eY =  Xβ +  = eX + e , with em
being the set of optimal solutions:
1
1
2m keY − eX β k2
2m keY − eX β k2
(a ) ( eP2 ) min
2 + λm kβ k1 , (b) em = arg min
(1)
2 + λm kβ k1 .
β ∈R p
Although sparsistency is the primary goal in selecting the correct variables, our analysis establishes
conditions for the stronger property of sign consistency:
Deﬁnition 3.1. (Sign Consistency) A set of estimators n is sign consistent with the true β if
P (cid:0)∃bβn ∈ n s.t.sgn(bβn ) = sgn(β )(cid:1) → 1 as n → ∞, where sgn(·) is given by sgn(x ) = 1, 0, and
−1 for x >, =, or < 0 respectively. As a shorthand,denote the event that a sign consistent solution
existswith E (cid:0)sgn(bβn ) = sgn(β ∗ )(cid:1) := (cid:8)∃bβ ∈ n such that sgn(bβ ) = sgn(β ∗ )(cid:9).
Clearly, if a set of estimators is sign consistent then it is sparsistent.
All recent work establishing results on sparsity recovery assumes some form of incoherence condi-
tion on the data matrix X . To formulate such a condition, it is convenient to introduce an additional
6= 0} be the set of relevant variables and let S c = {1, . . . , p} \ S
piece of notation. Let S = { j : β j
be the set of irrelevant variables. Then X S and X S c denote the corresponding sets of columns of the
matrix X . We will impose the following incoherence condition; related conditions are used by [18]
in a deterministic setting. Let k Ak∞ = maxi P p
j =1 | Ai j | denote the matrix ∞-norm.
Deﬁnition 3.2. ( S -Incoherence) Let X be an n × p matrix and let S ⊂ {1, . . . , p} be nonempty.
We say that X is S -incoherent in case
S X S − I| S |(cid:13)(cid:13)(cid:13)∞ ≤ 1 − η, for some η ∈ (0, 1].
(cid:13)(cid:13)(cid:13) 1
S c X S (cid:13)(cid:13)(cid:13)∞ + (cid:13)(cid:13)(cid:13) 1
n X T
n X T
Although not explicitly required, we only apply this deﬁnit
ion to X such that columns of X satisfy
(cid:13)(cid:13) X j (cid:13)(cid:13)2
2 = (n ), ∀ j ∈ {1, . . . , p}. We can now state our main result on sparsistency.
Theorem 3.3. Suppose that, before compression, Y = Xβ ∗ + , where each column of X is
normalized to have `2-norm n, and ε ∼ N (0, σ 2 In ). Assume that X is S-incoherent, where S =
supp(β ∗ ),anddeﬁne s = | S | and ρm = mini ∈ S |β ∗i |. Weobserve,aftercompression, eY = eX β ∗ +e ,
where eY = Y , eX =  X , ande = ,where i j ∼ N (0, 1
n ). Let eβm ∈ em as in (1b). If
η (cid:19) (ln p + 2 log n + log 2(s + 1)) ≤ m ≤ r n
(cid:18) 16C1 s 2
4C2 s
(3)
η2 +
16 log n
√8e ≈ 7.6885, and λm → 0 satisﬁes
with C1 = 4e
√6π ≈ 2.5044 and C2 =
ρm (r log s
S X S )−1(cid:13)(cid:13)(cid:13)∞) → 0.
m + λm (cid:13)(cid:13)(cid:13)( 1
m η2λ2
1
m
n X T
log( p − s ) → ∞, and (b)
(a )
Then the compressed lasso is sparsistent: P (cid:0)supp(eβm ) = supp(β )(cid:1) → 1 as m → ∞.
4 Compressed Regression is Persistent
Persistence (Greenshtein and Ritov [10]) is a weaker condition than sparsistency. In particular, the
assumption that E(Y | X ) = β T X is dropped. Roughly speaking, persistence implies that a procedure
predicts well. We review the arguments in [10] ﬁrst; we then a dapt it to the compressed case.
Uncompressed Persistence. Consider a new pair ( X , Y ) and suppose we want to predict Y from X .
The predictive risk using predictor β T X is R (β ) = E(Y − β T X )2 . Note that this is a well-deﬁned
quantity even though we do not assume that E(Y | X ) = β T X . It is convenient to rewrite the risk in
the following way: deﬁne Q = (Y, X 1 , . . . , X p ) and γ = (−1, β1 , . . . , β p )T , then
R (β ) = γ T γ , where  = E(Q Q T ).

(4)

(2)

(5)

4

Let Q = (Q †
1 Q †
n )T , where Q †
2 · · · Q †
i = (Yi , X 1i , . . . , X pi )T ∼ Q , ∀i = 1, . . . , n are i.i.d. random
vectors and the training error is
nXi =1
1
1
i β )2 = γ T b n γ , where b n =
QT Q.
(Yi − X T
bRn (β ) =
(6)
n
n
Given Bn = {β : kβ k1 ≤ L n } for L n = o (cid:0)(n/ log n )1/4 (cid:1), we deﬁne the oracle predictor β∗,n =
kβ k1≤L n bRn (β ).
kβ k1≤L n R (β ), and the uncompressed lasso estimator bβn = arg min
arg min
Assumption 1. Suppose that, for each j and k , E (cid:0)| Z |q (cid:1) ≤ q !M q−2 s /2, for every q ≥ 2 and some
constants M and s , where Z = Q j Q k − E(Q j Q k ), where Q j , Q k denote elements of Q .
Following arguments in [10], it can be shown that under Assumption 1 and given a sequence of sets
of estimators Bn = {β : kβ k1 ≤ L n } for L n = o (cid:0)(n/ log n )1/4 (cid:1), the sequence of uncompressed
P
lasso estimators bβn = arg min β ∈ Bn bRn (β ) is persistent, i.e., R (bβn ) − R (β∗,n )
→ 0.
Compressed Persistence.
For the compressed case, again we want to predict ( X , Y ), but
now the estimator bβn ,m is based on the lasso from the compressed data of size m n . Let γ =
(−1, β1 , . . . , β p )T as before and we replace bRn with
1
QT T Q.
bRn ,m (β ) = γ T b n ,m γ , where b n ,m =
(7)
m n
log(n pn ) (cid:17)1/4
Given compressed sample size m n , let Bn ,m = {β : kβ k1 ≤ L n ,m }, where L n ,m = o (cid:16) m n
.
We deﬁne the compressed oracle predictor β∗,n ,m = arg min β : kβ k1≤L n ,m R (β ) and the compressed
lasso estimator bβn ,m = arg min β : kβ k1≤L n ,m bRn ,m (β ).
Theorem 4.1. Under Assumption 1, we further assume that there exists a constant M1 > 0 such
that E(Q 2
j ) < M1 , ∀ j ,where Q j denotes the j t h element of Q. For any sequence Bn ,m ⊂ R p with
log2 (n pn ) ≤ m n ≤ n, where Bn ,m consists of all coefﬁcient vectors β such that kβ k1 ≤ L n ,m =
o (cid:0)(m n / log(n pn ))1/4 (cid:1),thesequenceofcompressedlassoprocedures bβn ,m = argminβ ∈ Bn ,m bRn ,m (β )
→ 0,when pn = O (cid:0)en c (cid:1) for c < 1/2.
P
ispersistent: R (bβn ,m ) − R (β∗,n ,m )
The main difference between the sequence of compressed lasso estimators and the original un-
compressed sequence is that n and m n together deﬁne the sequence of estimators for the com-
pressed data. Here m n is allowed to grow from  (log2 (n p)) to n ; hence for each ﬁxed
n ,
(cid:8)bβn ,m , ∀m n such that log2 (n p) < m n ≤ n(cid:9) deﬁnes a subsequence of estimators. In Section 6 we
illustrate the compressed lasso persistency via simulations to compare the empirical risks with the
oracle risks on such a subsequence for a ﬁxed n .
5
Information Theoretic Analysis of Privacy
Next we derive bounds on the rate at which the compressed data eX reveal information about the
uncompressed data X . Our general approach is to consider the mapping X 7→  X +  as a noisy
communication channel, where the channel is characterized by multiplicative noise  and additive
noise . Since the number of symbols in X is n p we normalize by this effective block length to
I ( X ; eX )
deﬁne the information rate rn ,m per symbol as rn ,m = sup p( X )
. Thus, we seek bounds on
n p
the capacity of this channel. A privacy guarantee is given in terms of bounds on the rate rn ,m → 0
I ( X ; eX ) = H ( X ) − H ( X | eX ) ≈ 0,
decaying to zero. Intuitively, if the mutual information satis ﬁes
then the compressed data eX reveal, on average, no more information about the original data X than
could be obtained from an independent sample.
The underlying channel is equivalent to the multiple antenna model for wireless communication
[13], where there are n transmitter and m receiver antennas in a Raleigh ﬂat-fading environment.
The propagation coefﬁcients between pairs of transmitter a nd receiver antennas are modeled by the
matrix entries i j ; they remain constant for a coherence interval of p time periods. Computing the

5

channel capacity over multiple intervals requires optimization of the joint density of pn transmitted
signals, the problem studied in [13]. Formally, the channel is modeled as Z =  X + γ , where
n Pn
γ > 0, i j ∼ N (0, 1), i j ∼ N (0, 1/ n ) and 1
i =1 E[ X 2
i j ] ≤ P , where the latter is a power
constraint.
Theorem 5.1. Suppose that E [ X 2
j ] ≤ P and the compressed data are formed by Z =  X + γ ,
where ism ×n withindependententriesi j ∼ N (0, 1/ n ) and ism × p withindependententries
n log (cid:16)1 + P
γ 2 (cid:17) .
I ( X ; Z )
n p ≤ m
i j ∼ N (0, 1). Thentheinformationrate rn ,m satisﬁes rn ,m = sup p( X )
This result is implicitly contained in [13]. When  = 0, or equivalently γ = 0, which is the
case assumed in our sparsistence and persistence results, the above analysis yields the trivial bound
rn ,m ≤ ∞. We thus derive a separate bound for this case; however, the resulting asymptotic order
of the information rate is the same.

Theorem 5.2. Suppose that E [ X 2
j ] ≤ P and the compressed data are formed by Z =  X , where
 is m × n with independent entries i j ∼ N (0, 1/ n ). Then the information rate rn ,m satisﬁes
I ( X ; Z )
n p ≤ m
2n log (2π e P ) .
rn ,m = sup p( X )
Under our sparsistency lower bound on m , the above upper bounds are rn ,m = O (log(n p)/ n ). We
note that these bounds may not be the best possible since they are obtained assuming knowledge of
the compression matrix , when in fact the privacy protocol requires that  and  are not public.

6 Experiments

In this section, we report results of simulations designed to validate the theoretical analysis presented
in previous sections. We ﬁrst present results that show the c ompressed lasso is comparable to the
uncompressed lasso in recovering the sparsity pattern of the true linear model. We then show results
on persistence that are in close agreement with the theoretical results of Section 4. We only include
Figures 1–2 here; additional plots are included in the full v ersion.
Sparsistency. Here we run simulations to compare the compressed lasso with the uncompressed
lasso in terms of the probability of success in recovering the sparsity pattern of β ∗ . We use random
matrices for both X and , and reproduce the experimental conditions of [19]. A design parameter
is the compression factor f = n
m , which indicates how much the original data are compressed.
The results show that when the compression factor f is large enough, the thresholding behaviors
as speci ﬁed in (8) and (9) for the uncompressed lasso carry ov er to the compressed lasso, when
X is drawn from a Gaussian ensemble.
In general, the compression factor f
is well below the
requirement that we have in Theorem 3.3 in case X is deterministic. In more detail, we consider the
Gaussian ensemble for the projection matrix , where i , j ∼ N (0, 1/ n ) are independent. The noise
is  ∼ N (0, σ 2 ), where σ 2 = 1. We consider Gaussian ensembles for the design matrix X with both
diagonal and Toeplitz covariance. In the Toeplitz case, the covariance is given by T (ρ )i , j = ρ |i − j | ;
we use ρ = 0.1. [19] shows that when X comes from a Gaussian ensemble under these conditions,
there exist ﬁxed constants θ` and θu such that for any ν > 0 and s = supp (β ), if
(8)
n > 2(θu + ν )s log( p − s ) + s + 1,
then the lasso identi ﬁes true variables with probability ap proaching one. Conversely, if
(9)
n < 2(θ` − ν )s log( p − s ) + s + 1,
then the probability of recovering the true variables using the lasso approaches zero. In the follow-
ing simulations, we carry out the lasso using procedure lars(Y, X ) that implements the LARS
algorithm of [8] to calculate the full regularization path. For the uncompressed case, we run
lars(Y, X ) such that Y = Xβ ∗ + , and for the compressed case we run lars(Y,  X ) such
that Y =  Xβ ∗ +  . The regularization parameter is λm = cp(log( p − s ) log s )/m . The results
show that the behavior under compression is close to the uncompressed case.
Persistence. Here we solve the following `1 -constrained optimization problem eβ =
kβ k1≤L kY − Xβ k2 directly, based on algorithms described by [15]. We constrain the solu-
arg min
tion to lie in the ball Bn = {kβ k1 ≤ L n }, where L n = n 1/4 /√log n . By [10], the uncompressed lasso
6

s
s
e
c
c
u
s
 
f
o
 
b
o
r
P

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

Toeplitz r=0.1; Fractional Power g=0.5, a=0.2

p=128

256

512

1024

Uncompressed
f = 120

0

50

100
200
150
Compressed dimension m

250

300

s
s
e
c
c
u
s
 
f
o
 
b
o
r
P

s
s
e
c
c
u
s
 
f
o
 
b
o
r
P

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

Identity; FP g=0.5, a=0.2; p=1024

Uncomp.
f = 5
f = 10
f = 20
f = 40
f = 80
f = 120

0.0

0.5

1.0
2.0
1.5
Control parameter q

2.5

3.0

Toeplitz r=0.1; FP g=0.5, a=0.2; p=1024

Uncomp.
f = 5
f = 10
f = 20
f = 40
f = 80
f = 120

0.0

0.5

1.0
2.0
1.5
Control parameter q

2.5

3.0

Figure 1: Plots of the number of samples versus the probability of success for recovering sgn(β ∗ ).
Each point on a curve for a particular θ or m , where m = 2θ σ 2 s log( p − s ) + s + 1, is an average
over 200 trials; for each trial, we randomly draw X n× p , m×n , and  ∈ Rn . The covariance  =
n E (cid:0) X T X (cid:1) and model β ∗ are ﬁxed across all curves in the plot. The sparsity level is s ( p) = 0.2 p1/2 .
1
The four sets of curves in the left plot are for p = 128, 256, 512 and 1024, with dashed lines
marking m for θ = 1 and s = 2, 3, 5 and 6 respectively. In the plots on the right, each curve has
a compression factor f ∈ {5, 10, 20, 40, 80, 120} for the compressed lasso, thus n = f m ; dashed
lines mark θ = 1. For  = I , θu = θ` = 1, while for  = T (0.1), θu ≈ 1.84 and θ` ≈ 0.46 [19],
for the uncompressed lasso in (8) and in (9).

n=9000, p=128, s=9

Uncompressed predictive
Compressed predictive
Compressed empirical

k
s
i
R

8
1

6
1

4
1

2
1

0
1

8

0

2000

4000

6000

8000

Compressed dimension m

Figure 2: Risk versus compressed dimension. We ﬁx n = 9000 and p = 128, and set s ( p) = 3 and
L n = 2.6874. The model is β ∗ = (−0.9, −1.7, 1.1, 1.3, −0.5, 2, −1.7, −1.3, −0.9, 0, . . . , 0)T so
that (cid:13)(cid:13)β ∗b (cid:13)(cid:13)1 > L n and β ∗b 6∈ Bn , and the uncompressed oracle predictive risk is R = 9.81. For each
value of m , a data point corresponds to the mean empirical risk, which is deﬁned in (7), over 100
trials, and each vertical bar shows one standard deviation. For each trial, we randomly draw X n× p
with i.i.d. row vectors x i ∼ N (0, T (0.1)), and Y = Xβ ∗ +  .

7

estimator bβn is persistent over Bn . For the compressed lasso, given n and pn , and a varying com-
pressed sample size m , we take the ball Bn ,m = {β : kβ k1 ≤ L n ,m } where L n ,m = m 1/4 /plog(n pn ).
The compressed lasso estimator bβn ,m for log2 (n pn ) ≤ m ≤ n , is persistent over Bn ,m by Theo-
rem 4.1. The simulations conﬁrm this behavior.
7 Acknowlegments

This work was supported in part by NSF grant CCF-0625879.

References

[1] D. Agrawal and C. C. Aggarwal. On the design and quantiﬁcation of p rivacy preserving data mining
algorithms. In In Proceedings of the 20th Symposium on Principles of Database Systems, May 2001.
[2] E. Candès, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements.
Communications in Pure and Applied Mathematics, 59(8):1207–1223, August 2006.
[3] T. Dalenius. Towards a methodology for statistical disclosure control. Statistik Tidskrift, 15:429–444,
1977.
[4] D. Donoho. Compressed sensing. IEEE Trans. Info. Theory, 52(4):1289–1306, April 2006.
[5] M. Duarte, M. Davenport, M. Wakin, and R. Baraniuk. Sparse signal detection from incoherent projections.
In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, 2006.
[6] G. Duncan and R. Pearson. Enhancing access to microdata while protecting conﬁdentiality: Prospects for
the future. Statistical Science, 6(3):219–232, August 1991.
[7] C. Dwork. Differential privacy.
In 33rd International Colloquium on Automata, Languages and
Programming–ICALP 2006 , pages 1–12, 2006.
[8] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–
499, 2004.
[9] J. Feigenbaum, Y. Ishai, T. Malkin, K. Nissim, M. J. Strauss, and R. N. Wright. Secure multiparty compu-
tation of approximations. ACM Trans. Algorithms, 2(3):435–472, 2006.
[10] E. Greenshtein and Y. Ritov. Persistency in high dimensional linear predictor-selection and the virtue of
over-parametrization. Journal of Bernoulli, 10:971–988, 2004.
[11] J. Haupt, R. Castro, R. Nowak, G. Fudge, and A. Yeh. Compressive sampling for signal classiﬁcation. In
Proc. Asilomar Conference on Signals, Systems, and Computers, October 2006.
[12] K. Liu, H. Kargupta, and J. Ryan. Random projection-based multiplicative data perturbation for privacy
preserving distributed data mining. IEEE Trans. on Knowl. and Data Engin., 18(1), Jan. 2006.
[13] T. L. Marzetta and B. M. Hochwald. Capacity of a mobile multiple-antenna communication link in
Rayleigh ﬂat fading.
IEEE Trans. Info. Theory, 45(1):139–157, January 1999.
[14] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.
Technical Report 720, Department of Statistics, UC Berkeley, 2006.
[15] M. Osborne, B. Presnell, and B. Turlach. On the lasso and its dual. J. Comp. and Graph. Stat., 9(2):319–
337, 2000.
[16] A. P. Sanil, A. Karr, X. Lin, and J. P. Reiter. Privacy preserving regression modelling via distributed
computation. In Proceedings of Tenth ACM SIGKDD, 2004.
[17] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267–288,
1996.
[18] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information
Theory, 50(10):2231–2242, 2004.
[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity. Technical Report
709, Department of Statistics, UC Berkeley, May 2006.
[20] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Research, 7:2541–2567,
2007.

8


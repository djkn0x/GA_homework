The Tradeoffs of Large Scale Learning

L ´eon Bottou
NEC laboratories of America
Princeton, NJ 08540, USA
leon@bottou.org

Olivier Bousquet
Google Z ¨urich
8002 Zurich, Switzerland
olivier.bousquet@m4x.org

Abstract

This contribution develops a theoretical framework that takes into account the
effect of approximate optimization on learning algorithms. The analysis shows
distinct tradeoffs for the case of small-scale and large-scale learning problems.
Small-scale learning problems are subject to the usual approximation–estimation
tradeoff. Large-scale learning problems are subject to a qualitatively different
tradeoff involving the computational complexity of the underlying optimization
algorithms in non-trivial ways.

1 Motivation

The computational complexity of learning algorithms has seldom been taken into account by the
learning theory. Valiant [1] states that a problem is “learn able” when there exists a probably approx-
imatively correct learning algorithm with polynomial complexity. Whereas much progress has been
made on the statistical aspect (e.g., [2, 3, 4]), very little has been told about the complexity side of
this proposal (e.g., [5].)

Computational complexity becomes the limiting factor when one envisions large amounts of training
data. Two important examples come to mind:

• Data mining exists because competitive advantages can be achieved by analyzing the
masses of data that describe the life of our computerized society. Since virtually every
computer generates data, the data volume is proportional to the available computing power.
Therefore one needs learning algorithms that scale roughly linearly with the total volume
of data.

• Arti ﬁcial intelligence attempts to emulate the cognitive c apabilities of human beings. Our
biological brains can learn quite efﬁciently from the conti nuous streams of perceptual data
generated by our six senses, using limited amounts of sugar as a source of power. This
observation suggests that there are learning algorithms whose computing time requirements
scale roughly linearly with the total volume of data.

This contribution ﬁnds its source in the idea that approxima te optimization algorithms might be
sufﬁcient for learning purposes. The ﬁrst part proposes new decomposition of the test error where
an additional term represents the impact of approximate optimization. In the case of small-scale
learning problems, this decomposition reduces to the well known tradeoff between approximation
error and estimation error. In the case of large-scale learning problems, the tradeoff is more com-
plex because it involves the computational complexity of the learning algorithm. The second part
explores the asymptotic properties of the large-scale learning tradeoff for various prototypical learn-
ing algorithms under various assumptions regarding the statistical estimation rates associated with
the chosen objective functions. This part clearly shows that the best optimization algorithms are not
necessarily the best learning algorithms. Maybe more surprisingly, certain algorithms perform well
regardless of the assumed rate for the statistical estimation error.

2 Approximate Optimization

2.1 Setup

En (f ) =

1
n

that is,

Following [6, 2], we consider a space of input-output pairs (x, y) ∈ X × Y endowed with a proba-
bility distribution P (x, y). The conditional distribution P (y |x) represents the unknown relationship
between inputs and outputs. The discrepancy between the predicted output ˆy and the real output
y is measured with a loss function ℓ( ˆy , y). Our benchmark is the function f ∗ that minimizes the
expected risk
E (f ) = Z ℓ(f (x), y) dP (x, y) = E [ℓ(f (x), y)],
f ∗ (x) = arg min
E [ ℓ( ˆy , y)| x].
ˆy
Although the distribution P (x, y) is unknown, we are given a sample S of n independently drawn
training examples (xi , yi ), i = 1 . . . n. We deﬁne the empirical risk
n
Xi=1
Our ﬁrst learning principle consists in choosing a family F of candidate prediction functions and
ﬁnding the function fn = arg minf ∈F En (f ) that minimizes the empirical risk. Well known com-
binatorial results (e.g., [2]) support this approach provided that the chosen family F is sufﬁciently
restrictive. Since the optimal function f ∗ is unlikely to belong to the family F , we also deﬁne
F and fn are well deﬁned and unique.
F = arg minf ∈F E (f ). For simplicity, we assume that f ∗ , f ∗
f ∗
We can then decompose the excess error as
(1)
E [E (fn ) − E (f ∗ )] = E [E (f ∗
F ) − E (f ∗ )] + E [E (fn ) − E (f ∗
F )] = Eapp + Eest ,
where the expectation is taken with respect to the random choice of training set. The approximation
error Eapp measures how closely functions in F can approximate the optimal solution f ∗ . The
estimation error Eest measures the effect of minimizing the empirical risk En (f ) instead of the
expected risk E (f ). The estimation error is determined by the number of training examples and by
the capacity of the family of functions [2]. Large families1 of functions have smaller approximation
errors but lead to higher estimation errors. This tradeoff has been extensively discussed in the
literature [2, 3] and lead to excess error that scale between the inverse and the inverse square root of
the number of examples [7, 8].

ℓ(f (xi ), yi ) = En [ℓ(f (x), y)].

2.2 Optimization Error

Finding fn by minimizing the empirical risk En (f ) is often a computationally expensive operation.
Since the empirical risk En (f ) is already an approximation of the expected risk E (f ), it should
not be necessary to carry out this minimization with great accuracy. For instance, we could stop an
iterative optimization algorithm long before its convergence.
Let us assume that our minimization algorithm returns an approximate solution ˜fn such that
En ( ˜fn ) < En (fn ) + ρ
where ρ ≥ 0 is a predeﬁned tolerance. An additional term Eopt = E(cid:2)E ( ˜fn ) − E (fn )(cid:3) then appears
in the decomposition of the excess error E = E(cid:2)E ( ˜fn ) − E (f ∗ )(cid:3):
F )] + E(cid:2)E ( ˜fn ) − E (fn )(cid:3)
E = E [E (f ∗
F ) − E (f ∗ )] + E [E (fn ) − E (f ∗
(2)
= Eapp + Eest + Eopt .
We call this additional term optimization error. It reﬂects the impact of the approximate optimization
on the generalization performance. Its magnitude is comparable to ρ (see section 3.1.)

1We often consider nested families of functions of the form Fc = {f ∈ H, Ω(f ) ≤ c}. Then, for each
value of c, function fn is obtained by minimizing the regularized empirical risk En (f ) + λΩ(f ) for a suitable
choice of the Lagrange coefﬁcient λ. We can then control the estimation-approximation tradeoff by choosing
λ instead of c.

2.3 The Approximation–Estimation–Optimization Tradeoff

This decomposition leads to a more complicated compromise. It involves three variables and two
constraints. The constraints are the maximal number of available training example and the maximal
computation time. The variables are the size of the family of functions F , the optimization accuracy
ρ, and the number of examples n. This is formalized by the following optimization problem.
subject to (cid:26)
The number n of training examples is a variable because we could choose to use only a subset of
the available training examples in order to complete the optimization within the alloted time. This
happens often in practice. Table 1 summarizes the typical evolution of the quantities of interest with
the three variables F , n, and ρ increase.

n ≤ nmax
T (F , ρ, n) ≤ Tmax

E = Eapp + Eest + Eopt

min
F ,ρ,n

(3)

Table 1: Typical variations when F , n, and ρ increase.

n

F
(approximation error) ց
(estimation error)
ր ց
(optimization error)
· · ·
· · · ր
(computation time)
ր ր ց

ρ

Eapp
Eest
Eopt
T

The solution of the optimization program (3) depends critically of which budget constraint is active:
constraint n < nmax on the number of examples, or constraint T < Tmax on the training time.

• We speak of small-scale learning problem when (3) is constrained by the maximal number
of examples nmax . Since the computing time is not limited, we can reduce the optimization
error Eopt to insigni ﬁcant levels by choosing ρ arbitrarily small. The excess error is then
dominated by the approximation and estimation errors, Eapp and Eest . Taking n = nmax ,
we recover the approximation-estimation tradeoff that is the object of abundant literature.
• We speak of large-scale learning problem when (3) is constrained by the maximal com-
puting time Tmax . Approximate optimization, that is choosing ρ > 0, possibly can achieve
better generalization because more training examples can be processed during the allowed
time. The speci ﬁcs depend on the computational properties o f the chosen optimization
algorithm through the expression of the computing time T (F , ρ, n).

3 The Asymptotics of Large-scale Learning

In the previous section, we have extended the classical approximation-estimation tradeoff by taking
into account the optimization error. We have given an objective criterion to distiguish small-scale
and large-scale learning problems. In the small-scale case, we recover the classical tradeoff between
approximation and estimation. The large-scale case is substantially different because it involves
the computational complexity of the learning algorithm. In order to clarify the large-scale learning
tradeoff with sufﬁcient generality, this section makes sev eral simpli ﬁcations:

• We are studying upper bounds of the approximation, estimation, and optimization er-
rors (2).
It is often accepted that these upper bounds give a realistic idea of the actual
convergence rates [9, 10, 11, 12]. Another way to ﬁnd comfort
in this approach is to say
that we study guaranteed convergence rates instead of the possibly pathological special
cases.
• We are studying the asymptotic properties of the tradeoff when the problem size increases.
Instead of carefully balancing the three terms, we write E = O(Eapp ) + O(Eest ) + O(Eopt )
and only need to ensure that the three terms decrease with the same asymptotic rate.
• We are considering a ﬁxed family of functions F and therefore avoid taking into account
the approximation error Eapp . This part of the tradeoff covers a wide spectrum of practical
realities such as choosing models and choosing features. In the context of this work, we do

not believe we can meaningfully address this without discussing, for instance, the thorny
issue of feature selection. Instead we focus on the choice of optimization algorithm.
• Finally, in order to keep this paper short, we consider that the family of functions F is
linearly parametrized by a vector w ∈ Rd . We also assume that x, y and w are bounded,
ensuring that there is a constant B such that 0 ≤ ℓ(fw (x), y) ≤ B and ℓ(·, y) is Lipschitz.

We ﬁrst explain how the uniform convergence bounds provide c onvergence rates that take the op-
timization error into account. Then we discuss and compare the asymptotic learning properties of
several optimization algorithms.

3.1 Convergence of the Estimation and Optimization Errors

The optimization error Eopt depends directly on the optimization accuracy ρ. However, the accuracy
ρ involves the empirical quantity En ( ˜fn ) − En (fn ), whereas the optimization error Eopt involves
its expected counterpart E ( ˜fn ) − E (fn ). This section discusses the impact on the optimization
error Eopt and of the optimization accuracy ρ on generalization bounds that leverage the uniform
convergence concepts pioneered by Vapnik and Chervonenkis (e.g., [2].)
In this discussion, we use the letter c to refer to any positive constant. Multiple occurences of the
letter c do not necessarily imply that the constants have identical values.

3.1.1 Simple Uniform Convergence Bounds

,

Recall that we assume that F is linearly parametrized by w ∈ Rd . Elementary uniform convergence
results then state that
|E (f ) − En (f )|– ≤ cr d
E » sup
n
f ∈F
where the expectation is taken with respect to the random choice of the training set.2 This result
immediately provides a bound on the estimation error:
Eest = E ˆ `E (fn ) − En (fn )´ + `En (fn ) − En (f ∗
F )´ + `En (f ∗
F ) − E (f ∗
F )´ ˜
|E (f ) − En (f )| – ≤ cr d
≤ 2 E » sup
.
n
f ∈F
This same result also provides a combined bound for the estimation and optimization errors:
Eest + Eopt = EˆE ( ˜fn ) − En ( ˜fn )˜ + EˆEn ( ˜fn ) − En (fn )˜
+ E [En (fn ) − En (f ∗
F )] + E [En (f ∗
F ) − E (f ∗
F )]
n ! .
= c  ρ + r d
+ ρ + 0 + cr d
≤ cr d
n
n
Unfortunately, this convergence rate is known to be pessimistic in many important cases. More
sophisticated bounds are required.

3.1.2 Faster Rates in the Realizable Case

When the loss functions ℓ( ˆy , y) is positive, with probability 1 − e−τ for any τ > 0, relative uniform
convergence bounds state that

log

+

τ
n

.

n
d

sup
f ∈F

≤ cr d
E (f ) − En (f )
pE (f )
n
This result is very useful because it provides faster convergence rates O(log n/n) in the realizable
case, that is when ℓ(fn (xi ), yi ) = 0 for all training examples (xi , yi ). We have then En (fn ) = 0,
En ( ˜fn ) ≤ ρ, and we can write
E ( ˜fn ) − ρ ≤ cqE ( ˜fn ) r d
n
τ
log
+
.
n
d
n
2Although the original Vapnik-Chervonenkis bounds have the form cq d
d , the logarithmic term can
n log n
be eliminated using the “chaining” technique (e.g., [10].)

Viewing this as a second degree polynomial inequality in variable qE ( ˜fn ), we obtain
E ( ˜fn ) ≤ c „ρ +
n « .
d
n
τ
log
+
n
d
Integrating this inequality using a standard technique (see, e.g., [13]), we obtain a better convergence
rate of the combined estimation and optimization error:
F )i ≤ E hE ( ˜fn )i = c „ρ +
Eest + Eopt = E hE ( ˜fn ) − E (f ∗
3.1.3 Fast Rate Bounds

d « .
n

d
n

log

Many authors (e.g., [10, 4, 12]) obtain fast statistical estimation rates in more general conditions.
These bounds have the general form
d (cid:19)α (cid:19) for
Eapp + Eest ≤ c (cid:18) Eapp + (cid:18) d
1
n
2
n
This result holds when one can establish the following variance condition:
F ) (cid:19)2− 1
F (X ), Y )(cid:1)2 i ≤ c (cid:18) E (f ) − E (f ∗
α
∀f ∈ F E h(cid:0)ℓ(f (X ), Y ) − ℓ(f ∗
The convergence rate of (4) is described by the exponent α which is determined by the quality of
the variance bound (5). Works on fast statistical estimation identify two main ways to establish such
a variance condition.

≤ α ≤ 1 .

(4)

(5)

log

.

• Exploiting the strict convexity of certain loss functions [12, theorem 12]. For instance, Lee
et al. [14] establish a O(log n/n) rate using the squared loss ℓ( ˆy , y) = ( ˆy − y)2 .
• Making assumptions on the data distribution. In the case of pattern recognition problems,
for instance, the “Tsybakov condition” indicates how clean ly the posterior distributions
P (y |x) cross near the optimal decision boundary [11, 12]. The realizable case discussed in
section 3.1.2 can be viewed as an extreme case of this.

Despite their much greater complexity, fast rate estimation results can accomodate the optimization
accuracy ρ using essentially the methods illustrated in sections 3.1.1 and 3.1.2. We then obtain a
bound of the form
d (cid:19)α
+ ρ (cid:19) .
E = Eapp + Eest + Eopt = E hE ( ˜fn ) − E (f ∗ )i ≤ c (cid:18) Eapp + (cid:18) d
n
n
For instance, a general result with α = 1 is provided by Massart [13, theorem 4.2]. Combining this
result with standard bounds on the complexity of classes of linear functions (e.g., [10]) yields the
following result:
E = Eapp + Eest + Eopt = E hE ( ˜fn ) − E (f ∗ )i ≤ c (cid:18) Eapp +
d
n
d
n
See also [15, 4] for more bounds taking into account the optimization accuracy.

+ ρ (cid:19) .

(7)

(6)

log

log

3.2 Gradient Optimization Algorithms

We now discuss and compare the asymptotic learning properties of four gradient optimization algo-
F and wn
rithms. Recall that the family of function F is linearly parametrized by w ∈ Rd . Let w∗
F and fn deﬁned in section 2.1. In this section, we assume that the
correspond to the functions f ∗
functions w 7→ ℓ(fw (x), y) are convex and twice differentiable with continuous second derivatives.
Convexity ensures that the empirical const function C (w) = En (fw ) has a single minimum.
Two matrices play an important role in the analysis: the Hessian matrix H and the gradient covari-
ance matrix G, both measured at the empirical optimum wn .
∂w2 (wn ) = En(cid:20) ∂ 2 ℓ(fwn (x), y)
∂ 2C
(cid:21) ,
H =
∂w2
G = En"(cid:18) ∂ ℓ(fwn (x), y)
(cid:19) (cid:18) ∂ ℓ(fwn (x), y)
∂w
∂w

(cid:19)′ # .

(9)

(8)

The relation between these two matrices depends on the chosen loss function. In order to summarize
them, we assume that there are constants λmax ≥ λmin > 0 and ν > 0 such that, for any η > 0,
we can choose the number of examples n large enough to ensure that the following assertion is true
with probability greater than 1 − η :

and

tr(G H −1 ) ≤ ν
EigenSpectrum(H ) ⊂ [ λmin , λmax ]
The condition number κ = λmax /λmin is a good indicator of the difﬁculty of the optimization [16] .
The condition λmin > 0 avoids complications with stochastic gradient algorithms. Note that this
condition only implies strict convexity around the optimum. For instance, consider the loss func-
tion ℓ is obtained by smoothing the well known hinge loss ℓ(z , y) = max{0, 1 − yz} in a small
neighborhood of its non-differentiable points. Function C (w) is then piecewise linear with smoothed
edges and vertices. It is not strictly convex. However its minimum is likely to be on a smoothed
vertex with a non singular Hessian. When we have strict convexity, the argument of [12, theorem 12]
yields fast estimation rates α ≈ 1 in (4) and (6). This is not necessarily the case here.
The four algorithm considered in this paper use information about the gradient of the cost function
to iteratively update their current estimate w(t) of the parameter vector.

(10)

• Gradient Descent (GD) iterates

∂C
∂w

(w(t)) = w(t) − η

w(t + 1) = w(t) − η

n
Xi=1
ℓ(cid:0)fw(t) (xi ), yi (cid:1)
where η > 0 is a small enough gain. GD is an algorithm with linear convergence [16].
When η = 1/λmax , this algorithm requires O(κ log(1/ρ)) iterations to reach accuracy ρ.
The exact number of iterations depends on the choice of the initial parameter vector.
• Second Order Gradient Descent (2GD) iterates

∂
∂w

1
n

1
n

H −1

∂
∂w

(w(t)) = w(t) −

w(t + 1) = w(t) −

w(t + 1) = w(t) − H −1 ∂C
∂w

n
Xi=1
ℓ(cid:0)fw(t) (xi ), yi (cid:1)
where matrix H −1 is the inverse of the Hessian matrix (8). This is more favorable than
Newton’s algorithm because we do not evaluate the local Hessian at each iteration but
simply assume that we know in advance the Hessian at the optimum. 2GD is a superlinear
optimization algorithm with quadratic convergence [16]. When the cost is quadratic, a
single iteration is sufﬁcient. In the general case, O(log log(1/ρ)) iterations are required to
reach accuracy ρ.
• Stochastic Gradient Descent (SGD) picks a random training example (xt , yt ) at each
iteration and updates the parameter w on the basis of this example only,
∂
η
ℓ(cid:0)fw(t) (xt ), yt (cid:1).
∂w
t
Murata [17, section 2.2], characterizes the mean ES [w(t)] and variance VarS [w(t)] with
respect to the distribution implied by the random examples drawn from the training set S at
each iteration. Applying this result to the discrete training set distribution for η = 1/λmin ,
we have δw(t)2 = O(1/t) where δw(t) is a shorthand notation for w(t) − wn .
We can then write
ES [ C (w(t)) − inf C ] = ES ˆtr`H δw(t) δw(t)′ ´˜ + o` 1
t ´
= tr ` H ES [δw(t)] ES [δw(t)]′ + H VarS [w(t)] ´ + o` 1
t ´
t ´ ≤ νκ2
≤ tr(GH )
t + o` 1
t + o` 1
t ´ .
Therefore the SGD algorithm reaches accuracy ρ after less than ν κ2/ρ + o(1/ρ) iterations
on average. The SGD convergence is essentially limited by the stochastic noise induced
by the random choice of one example at each iteration. Neither the initial value of the
parameter vector w nor the total number of examples n appear in the dominant term of this
bound! When the training set is large, one could reach the desired accuracy ρ measured on
the whole training set without even visiting all the training examples. This is in fact a kind
of generalization bound.

(11)

Table 2: Asymptotic results for gradient algorithms (with probability 1). Compare the second
last column (time to optimize) with the last column (time to reach the excess test error ǫ).
Legend: n number of examples; d parameter dimension; κ, ν see equation (10).

GD

2GD

SGD

2SGD

Algorithm Cost of one
iteration

Iterations
Time to reach
Time to reach
to reach ρ
accuracy ρ
E ≤ c (Eapp + ε)
O(cid:16)κ log 1
O(cid:16) d2 κ
ε (cid:17)
O(cid:16)ndκ log 1
ρ (cid:17)
ρ (cid:17)
ε1/α log2 1
O(nd)
ρ (cid:17) O(cid:16)(cid:0)d2 + nd(cid:1) log log 1
ρ (cid:17) O(cid:16) d2
ε (cid:17)
O(cid:0)d2 + nd(cid:1) O(cid:16)log log 1
ε1/α log 1
ε log log 1
ρ + o(cid:16) 1
ρ (cid:17)
O(cid:16) dν κ2
ρ (cid:17)
O(cid:16) d ν κ2
ε (cid:17)
ν κ2
O(d)
ρ + o(cid:16) 1
O(cid:16) d2 ν
O(cid:16) d2 ν
ε (cid:17)
ρ (cid:17)
ρ (cid:17)
O(cid:0)d2 (cid:1)
ν
• Second Order Stochastic Gradient Descent (2SGD) replaces the gain η by the inverse of
the Hessian matrix H :

w(t + 1) = w(t) −

H −1 ∂
1
ℓ(cid:0)fw(t) (xt ), yt (cid:1).
∂w
t
Unlike standard gradient algorithms, using the second order information does not change
the inﬂuence of ρ on the convergence rate but improves the constants. Using again [17,
theorem 4], accuracy ρ is reached after ν /ρ + o(1/ρ) iterations.

For each of the four gradient algorithms, the ﬁrst three colu mns of table 2 report the time for a single
iteration, the number of iterations needed to reach a predeﬁ ned accuracy ρ, and their product, the
time needed to reach accuracy ρ. These asymptotic results are valid with probability 1, since the
probability of their complement is smaller than η for any η > 0.
The fourth column bounds the time necessary to reduce the excess error E below c (Eapp + ε) where c
d ´α in (6) achieves
is the constant from (6). This is computed by observing that choosing ρ ∼ ` d
n log n
the fastest rate for ε, with minimal computation time. We can then use the asymptotic equivalences
ε . Setting the fourth column expressions to Tmax and solving for ǫ yields
ρ ∼ ε and n ∼ d
ε1/α log 1
the best excess error achieved by each algorithm within the limited time Tmax . This provides the
asymptotic solution of the Estimation–Optimization trade off (3) for large scale problems satisfying
our assumptions.

These results clearly show that the generalization performance of large-scale learning systems de-
pends on both the statistical properties of the estimation procedure and the computational properties
of the chosen optimization algorithm. Their combination leads to surprising consequences:

• The SGD and 2SGD results do not depend on the estimation rate α. When the estimation
rate is poor, there is less need to optimize accurately. That leaves time to process more
examples. A potentially more useful interpretation leverages the fact that (11) is already a
kind of generalization bound: its fast rate trumps the slower rate assumed for the estimation
error.
• Second order algorithms bring little asymptotical improvements in ε. Although the super-
linear 2GD algorithm improves the logarithmic term, all four algorithms are dominated by
the polynomial term in (1/ε). However, there are important variations in the inﬂuence of
the constants d, κ and ν . These constants are very important in practice.
• Stochastic algorithms (SGD, 2SGD) yield the best generalization performance despite be-
ing the worst optimization algorithms. This had been described before [18] and observed
in experiments.

In contrast, since the optimization error Eopt of small-scale learning systems can be reduced to
insigni ﬁcant levels, their generalization performance is solely determined by the statistical properties
of their estimation procedure.

4 Conclusion
Taking in account budget constraints on both the number of examples and the computation time,
we ﬁnd qualitative differences between the generalization performance of small-scale learning sys-
tems and large-scale learning systems. The generalization properties of large-scale learning systems
depend on both the statistical properties of the estimation procedure and the computational proper-
ties of the optimization algorithm. We illustrate this fact with some asymptotic results on gradient
algorithms.

Considerable reﬁnements of this framework can be expected. Extending the analysis to regular-
ized risk formulations would make results on the complexity of primal and dual optimization algo-
rithms [19, 20] directly exploitable. The choice of surrogate loss function [7, 12] could also have a
non-trivial impact in the large-scale case.

Acknowledgments Part of this work was funded by NSF grant CCR-0325463.

References
[1] Leslie G. Valiant. A theory of learnable. Proc. of the 1984 STOC, pages 436–445, 1984.
[2] Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer Series in Statistics.
Springer-Verlag, Berlin, 1982.
[3] St ´ephane Boucheron, Olivier Bousquet, and G ´abor Lugosi. Theory of classiﬁcation: a survey of recent
advances. ESAIM: Probability and Statistics, 9:323–375, 2005.
[4] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory and Related Fields,
135(3):311–334, 2006.
[5] J. Stephen Judd. On the complexity of loading shallow neural networks. Journal of Complexity, 4(3):177–
192, 1988.
[6] Richard O. Duda and Peter E. Hart. Pattern Classiﬁcation And Scene Analysis . Wiley and Son, 1973.
[7] Tong Zhang. Statistical behavior and consistency of classiﬁcation me thods based on convex risk mini-
mization. The Annals of Statistics, 32:56–85, 2004.
[8] Clint Scovel and Ingo Steinwart. Fast rates for support vector machines. In Peter Auer and Ron Meir,
editors, Proceedings of the 18th Conference on Learning Theory (COLT 2005), volume 3559 of Lecture
Notes in Computer Science, pages 279–294, Bertinoro, Italy, June 2005. Springer-Verlag.
[9] Vladimir N. Vapnik, Esther Levin, and Yann LeCun. Measuring the VC-dimension of a learning machine.
Neural Computation, 6(5):851–876, 1994.
[10] Olivier Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of
Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002.
[11] Alexandre B. Tsybakov. Optimal aggregation of classiﬁers in statis tical learning. Annals of Statististics,
32(1), 2004.
[12] Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classiﬁcation and risk bounds.
Journal of the American Statistical Association, 101(473):138–156, March 2006.
[13] Pascal Massart. Some applications of concentration inequalities to statistics. Annales de la Facult ´e des
Sciences de Toulouse, series 6, 9(2):245–303, 2000.
[14] Wee S. Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning with
squared loss. IEEE Transactions on Information Theory, 44(5):1974–1980, 1998.
[15] Shahar Mendelson. A few notes on statistical learning theory. In Shahar Mendelson and Alexander J.
Smola, editors, Advanced Lectures in Machine Learning, volume 2600 of Lecture Notes in Computer
Science, pages 1–40. Springer-Verlag, Berlin, 2003.
[16] John E. Dennis, Jr. and Robert B. Schnabel. Numerical Methods For Unconstrained Optimization and
Nonlinear Equations. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1983.
[17] Noboru Murata. A statistical study of on-line learning. In David Saad, editor, Online Learning and Neural
Networks. Cambridge University Press, Cambridge, UK, 1998.
[18] L ´eon Bottou and Yann Le Cun. Large scale online learning.
In Sebastian Thrun, Lawrence K. Saul,
and Bernhard Sch ¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press,
Cambridge, MA, 2004.
[19] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of KDD’06, Philadelphia, PA,
USA, August 20-23 2006. ACM.
[20] Don Hush, Patrick Kelly, Clint Scovel, and Ingo Steinwart. QP algorithms with guaranteed accuracy and
run time for support vector machines. Journal of Machine Learning Research, 7:733–769, 2006.


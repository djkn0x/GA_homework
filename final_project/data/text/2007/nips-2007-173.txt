The Value of Labeled and Unlabeled Examples when
the Model is Imperfect

Kaushik Sinha
Dept. of Computer Science and Engineering
Ohio State University
Columbus, OH 43210
sinhak@cse.ohio-state.edu

Mikahil Belkin
Dept. of Computer Science and Engineering
Ohio State University
Columbus, OH 43210
mbelkin@cse.ohio-state.edu

Abstract

Semi-supervised learning, i.e. learning from both labeled and unlabeled data has
received signi(cid:2)cant attention in the machine learning literature in recent years.
Still our understanding of the theoretical foundations of the usefulness of unla-
beled data remains somewhat limited. The simplest and the best understood sit-
uation is when the data is described by an identi(cid:2)able mixture model, and where
each class comes from a pure component. This natural setup and its implications
ware analyzed in [11, 5]. One important result was that in certain regimes, labeled
data becomes exponentially more valuable than unlabeled data.
However, in most realistic situations, one would not expect that the data comes
from a parametric mixture distribution with identi(cid:2)able components. There have
been recent efforts to analyze the non-parametric situation, for example, (cid:147)cluster(cid:148)
and (cid:147)manifold(cid:148) assumptions have been suggested as a basis for analysis. Still,
a satisfactory and fairly complete theoretical understanding of the nonparametric
problem, similar to that in [11, 5] has not yet been developed.
In this paper we investigate an intermediate situation, when the data comes from a
probability distribution, which can be modeled, but not perfectly, by an identi(cid:2)able
mixture distribution. This seems applicable to many situation, when, for example,
a mixture of Gaussians is used to model the data. the contribution of this paper is
an analysis of the role of labeled and unlabeled data depending on the amount of
imperfection in the model.

1 Introduction

In recent years semi-supervised learning, i.e. learning from labeled and unlabeled data, has drawn
signi(cid:2)cant attention. The ubiquity and easy availability of unlabeled data together with the increased
computational power of modern computers, make the paradigm attractive in various applications,
while connections to natural learning make it also conceptually intriguing. See [15] for a survey on
semi-supervised learning.
From the theoretical point of view, semi-supervised learning is simple to describe. Suppose the data
is sampled from the joint distribution p(x; y), where x is a feature and y is the label. The unlabeled
data comes from the marginal distribution p(x). Thus the the usefulness of unlabeled data is tied
to how much information about joint distribution can be extracted from the marginal distribution.
Therefore, in order to make unlabeled data useful, an assumption on the connection between these
distributions needs to be made.

1

In the non-parametric setting several such assumptions have been recently proposed, including the
the cluster assumption and its re(cid:2)nement, the low-density separation assumption [7, 6], and the
manifold assumption [3]. These assumptions relate the shape of the marginal probability distribution
to class labels. The low-density separation assumption states that the class boundary passes through
the low density regions, while the manifold assumption proposes that the proximity of the points
should be measured along the data manifold. However, while these assumptions has motivated
several algorithms and have been shown to hold empirically, few theoretical results on the value of
unlabeled data in the non-parametric setting are available so far. We note the work of Balcan and
Blum ([2]), which attempts to unify several frameworks by introducing a notion of compatibility
between labeled and unlabeled data. In a slightly different setting some theoretical results are also
available for co-training ([4, 8]).
Far more complete results are available in the parametric setting. There one assumes that the dis-
tribution p(x; y) is a mixture of two parametric distribution p1 and p2 , each corresponding to a
different class. Such mixture is called identi(cid:2)able, if parameters of each component can be uniquely
determined from the marginal distribution p(x). The study of usefulness of unlabeled data under this
assumption was undertaken by Castelli and Cover ([5]) and Ratsaby and Venkatesh ([11]). Among
several important conclusions from their study was the fact under a certain range of conditions,
labeled data is exponentially more important for approximating the Bayes optimal classi(cid:2)er than
unlabeled data. Roughly speaking, unlabeled data may be used to identify the parameters of each
mixture component, after which the class attribution can be established exponentially fast using only
few labeled examples.
While explicit mixture modeling is of great theoretical and practical importance, in many applica-
tions there is no reason to believe that the model provides a precise description of the phenomenon.
Often it is more reasonable to think that our models provide a rough approximation to the underlying
probability distribution, but do not necessarily represent it exactly. In this paper we investigate the
limits of usefulness of unlabeled data as a function of how far the best (cid:2)tting model strays from the
underlying probability distribution.
The rest of the paper is structured as follows: we start with an overview of the results available for
identi(cid:2)able mixture models together with some extensions of these results. We then describe how
the relative value of labeled and unlabeled data changes when the true distribution is a perturbation
of a parametric model. Finally we discuss various regimes of usability for labeled and unlabeled
data and represent our (cid:2)ndings in Fig 1.

2 Relative Value of Labeled and Unlabeled Examples
Our analysis is conducted in the standard classi(cid:2)cation framework and studies the behavior P error (cid:0)
PBayes , where Perror is probability of misclassi(cid:2)cation for a given classi(cid:2)er and PBayes is the
classi(cid:2)cation error of the optimal classi(cid:2)er. The quantity Perror (cid:0) PBayes is often referred to as the
excess probability of error and expresses how far our classi(cid:2)er is from the best possible.
In what follows, we review some theoretical results that describe behavior of the excess error prob-
ability as a function of the number of labeled and unlabeled examples. We will denote number of
labeled examples by l and the number of unlabeled examples by u. We omit certain minor techni-
cal details to simplify the exposition. The classi(cid:2)er, for which Perror is computed is based on the
underlying model.
Theorem 2.1. (Ratsaby and Venkatesh [11]) In a two class identi(cid:2)able mixture model, let the
equiprobable class densities p1 (x); p2 (x) be d-dimensional Gaussians with unit covariance matri-
ces. Then for suf(cid:2)ciently small (cid:15) > 0 and arbitrary (cid:14) > 0, given l = O (cid:0)log (cid:14)(cid:0)1 (cid:1) labeled and
u = O (cid:16) d2
(cid:15)3 (cid:14) (d log (cid:15)(cid:0)1 + log (cid:14)(cid:0)1 )(cid:17) unlabeled examples respectively, with con(cid:2)dence at least 1 (cid:0) (cid:14) ,
probability of error Perror (cid:20) PBayes (1 + c(cid:15)) for some positive constant c.
Since the mixture is identi(cid:2)able, parameters can be estimated from unlabeled examples alone. La-
beled examples are not required for this purpose. Therefore, unlabeled examples are used to estimate
the mixture and hence the two decision regions. Once the decision regions are established, labeled
examples are used to label them. An equivalent form of the above result in terms of labeled and
u1=3 (cid:1) + O (exp((cid:0)l)). For a (cid:2)xed dimension d, this
unlabeled examples is Perror (cid:0) PBayes = O (cid:0) d
2

indicates that labeled examples are exponentially more valuable than the unlabeled examples in re-
ducing the excess probability of error, however, when d is not (cid:2)xed, higher dimensions slower these
rates.
Independently, Cover and Castelli provided similar results in a different setting under Bayesian
framework.
Theorem 2.2. (Cover and Castelli [5]) In a two class mixture model, let p1 (x); p2 (x) be the para-
metric class densities and let h((cid:17)) be the prior over the unknown mixing parameter (cid:17) . Then
Perror (cid:0) PBayes = O (cid:18) 1
u (cid:19) + expf(cid:0)Dl + o(l)g
where D = (cid:0) logf2p(cid:17)(1 (cid:0) (cid:17)) R pp1 (x)p2 (x)dxg
In their framework, Cover and Castelli [5] assumed that parameters of individual class densities
are known, however the associated class labels and mixing parameter are unknown. Under such
assumption their result shows that the above rate is obtained when l 3+(cid:15)u(cid:0)1 ! 0 as l + u ! 1. In
particular this implies that, if ue(cid:0)Dl ! 0 and l = o(u) the excess error is essentially determined by
the number of unlabeled examples. On the other hand if u grows faster than eDl , then excess error
is determined by the number of labeled examples. For detailed explanation of the above statements
see pp-2103 [5]. The effect of dimensionality is not captured in their result.
Both results indicate that if the parametric model assumptions are satis(cid:2)ed, labeled examples are
exponentially more valuable than unlabeled examples in reducing the excess probability of error.
In this paper we investigate the situation when the parametric model assumptions are only satis(cid:2)ed
to a certain degree of precision, which seems to be a natural premise in a variety of practical settings.
It is interesting to note that uncertainty can appear for different reasons. One source of uncertainty
is a lack of examples, which we call Type-A. Imperfection of the model is another source of uncer-
tainty, which we will refer to as Type-B.

(cid:15) Type-A uncertainty for perfect model with imperfect information: Individual class
densities follow the assumed parametric model. Uncertainty results from (cid:2)niteness of ex-
amples. Perturbation size speci(cid:2)es how well parameters of the individual class densities
can be estimated from (cid:2)nite data.
(cid:15) Type-B uncertainty for imperfect model: Individual class densities does not follow the
assumed parametric model. Perturbation size speci(cid:2)es how well the best (cid:2)tting model can
approximate the underlying density.

Before proceeding further, we describe our model and notations. We take the instance space X (cid:26) R d
with labels f(cid:0)1; 1g. True class densities are always represented by p1 (x) and p2 (x) respectively. In
case of Type-A uncertainty they are simply p1 (xj(cid:18)1 ) and p2 (xj(cid:18)2 ). In case of Type-B uncertainty
p1 (x); p2 (x) are perturbations of two d-dimensional densities from a parametric family F . We will
denote the mixing parameter by t and the individual parametric class densities by f 1 (xj(cid:18)1 ); f2 (xj(cid:18)2 )
respectively and the resulting mixture density as tf1 (xj(cid:18)1 ) + (1 (cid:0) t)f2 (xj(cid:18)2 ). We will show some
speci(cid:2)c results when F consists of spherical Gaussian distributions with unit covariance matrix and
2 . In such a case (cid:18)1 ; (cid:18)2 2 Rd represent the means of the corresponding densities and the
t = 1
mixture density is indexed by a 2d dimensional vector (cid:18) = [(cid:18)1 ; (cid:18)2 ]. The class of such mixtures is
identi(cid:2)able and hence using unlabeled examples alone, (cid:18) can be estimated by ^(cid:18) 2 R2d . By jj (cid:1) jj we
2 ;2 the Sobolev norm. Note that for some
represent the standard Euclidean norm in Rd and by jj (cid:1) jj d
(cid:15) > 0, jj (cid:1) jj d
2 ;2 < (cid:15) implies jj (cid:1) jj1 < (cid:15) and jj (cid:1) jj1
< (cid:15). We will frequently use the following term
log( a
to represent the optimal number of labeled
(cid:14) )
(t(cid:0)Ae)(1(cid:0)2p(PBayes +Be)(1(cid:0)PBayes(cid:0)Be))
L(a; t; e) =
examples for correctly classifying estimated decision regions with high probability (as will be clear
in the next section) where, t represents mixing parameter, e represents perturbation size and a is an
integer variable and A; B are constants.

3

2.1 Type-A Uncertainty : Perfect Model Imperfect Information
Due to (cid:2)niteness of unlabeled examples, density parameters can not be estimated arbitrarily close to
the true parameters in terms of Euclidean norm. Clearly, how close they can be estimated depends
on the number of unlabeled examples used u, dimension d and con(cid:2)dence probability (cid:14) . Thus,
Type-I uncertainty inherently gives rise to a perturbation size de(cid:2)ned by (cid:15)1 (u; d; (cid:14)) such that, a (cid:2)xed
u de(cid:2)nes a perturbation size (cid:15)1 (d; (cid:14)). Because of this perturbation, estimated decision regions differ
from the true decision regions. From [11] it is clear that only very few labeled examples are good
enough to label these two estimated decision regions reasonably well with high probability. Let
such a number of labeled examples be l (cid:3) . But what happens if the number of labeled examples
available is greater than l(cid:3) ? Since the individual densities follow the parametric model exactly,
these extra labeled examples can be used to estimate the density parameters and hence the decision
regions. However, using a simple union bound it can be shown ([10]) that the asymptotic rate for
convergence of such estimation procedure is O (cid:18)q d
(cid:14) )(cid:19). Thus, provided we have u unlabeled
l log( d
examples if we want to represent the rate at which excess probability of error reduces as a function
of the number of labeled examples, it is clear that initially the error reduces exponentially fast in
number of labeled examples (following [11]) but then it reduces only at a rate O (cid:18)q d
(cid:14) )(cid:19).
l log( d
Provided we use the following strategy, this extends the result of
[11] as given in the Theorem
below.
We adopt the following strategy to utilize labeled and unlabeled examples in order to learn a classi-
(cid:2)cation rule.
Strategy 1:
1. Given u unlabeled examples, and con(cid:2)dence probability (cid:14) > 0 use maximum likelihood
estimation method to learn the parameters of the mixture model such that the estimates
^(cid:18)1 ; ^(cid:18)2 are only (cid:15)1 (u; d; (cid:14)) = O(cid:3) (cid:0) d
u1=3 (cid:1) close to the actual parameters with probability at
4 .
least 1 (cid:0) (cid:14)
2. Use l(cid:3) labeled examples to label the estimated decision regions with probability of incorrect
labeling no greater than (cid:14)
4 .
3. If l > l(cid:3) examples are available use them to estimate the individual density parameters with
probability at least 1 (cid:0) (cid:14)
2 .
Theorem 2.3. Let the model be a mixture of two equiprobable d dimensional spherical Gaussians
p1 (xj(cid:18)1 ); p2 (xj(cid:18)2 ) having unit covariance matrices and means (cid:18)1 ; (cid:18)2 2 Rd . For any arbitrary
1 > (cid:14) > 0, if strategy 1 is used with u unlabeled examples then there exists a perturbation size
(cid:15)1 (u; d; (cid:14)) > 0 and positive constants A; B such that using l (cid:20) l (cid:3) = L(24; 0:5; (cid:15)1) labeled exam-
ples, Perror (cid:0) PBayes reduces exponentially fast in the number of labeled examples with probability
2 ). If more labeled examples l > l(cid:3) are provided then with probability at least (1 (cid:0) (cid:14)
2 ),
at least (1 (cid:0) (cid:14)
(cid:14) )(cid:19) as l ! 1. If we
Perror (cid:0) PBayes asymptotically converges to zero at a rate O (cid:18)q d
l log( d
represent the reduction rate of this excess error(Perror (cid:0) PBayes ) as a function of labeled examples
Ree (l), then this can be compactly represented as,
if l (cid:20) l(cid:3)
Ree (l) = 8<
O (exp((cid:0)l))
(cid:14) )(cid:19) if l > l(cid:3)
O (cid:18)q d
l log( d
:
After using l(cid:3) labeled examples Perror = PBayes + O((cid:15)1 ).
2.2 Type-B Uncertainty: Imperfect Model
In this section we address the main question raised in this paper. Here the individual class densities
do not follow the assumed parametric model exactly but are a perturbed version of the assumed
model. The uncertainty in this case is speci(cid:2)ed by the perturbation size (cid:15)2 which roughly indicates
by what extent the true class densities differ form that of the best (cid:2)tting parametric model densities.

4

For any mixing parameter t 2 (0; 1) let us consider a two class mixture model with individual class
densities p1 (x); p2 (x) respectively. Suppose the best knowledge available about this mixture model
is that individual class densities approximately follow some parametric form from a class F . We
assume that best approximations of p1 ; p2 within F are f1 (xj(cid:18)1 ); f2 (xj(cid:18)2 ) respectively, such that
for i 2 f1; 2g; (fi (cid:0) pi ) are in Sobolev class H
2 and there exists a perturbation size (cid:15)2 > 0 such
d
that jjp1 (cid:0) f1 jj d
2 ;2 (cid:20) (cid:15)2 . Here, the Sobolev norm is used as a smoothness
2 ;2 (cid:20) (cid:15)2 and jjp2 (cid:0) f2 jj d
condition and implies that true densities are smooth and not (cid:147)too different(cid:148) from the best (cid:2)tting
parametric model densities and in particular, if jjfi (cid:0) pi jj d
2 ;2 (cid:20) (cid:15)2 then jjfi (cid:0) pi jj1 (cid:20) (cid:15)2 and
jjfi (cid:0) pi jj1 (cid:20) (cid:15)2 .
We (cid:2)rst show that due to the presence of this perturbation size, even complete knowledge of the
best (cid:2)tting model parameters does not help in learning optimal classi(cid:2)cation rule in the following
sense. In the absence of any perturbation, complete knowledge of model parameters implies that
the decision boundary and hence two decision regions are explicitly known but not their labels.
Thus, using only a very small number of labeled examples Perror reduces exponentially fast in the
number of labeled examples to PBayes as number of labeled examples increases. However, due to
the presence of perturbation size, Perror reduces exponentially fast in number of labeled examples
only up to PBayes + O((cid:15)2 ). Since beyond this, parametric model assumptions do not hold due to the
presence of perturbation size, some non parametric technique must be used to estimate the actual
decision boundary. For any such nonparametric technique Perror now reduces at a much slower rate.
This trend is roughly what the following theorem says. Here f1 ; f2 are general parametric densities
not necessarily Gaussians. In what follows we assume that p1 ; p2 2 C1 and hence convergence rate
for non parametric classi(cid:2)cation (see [14]) is O (cid:16) 1
pl (cid:17). Slower rate results if in(cid:2)nite differentiability
condition is not satis(cid:2)ed.
Theorem 2.4. In a two class mixture model with individual class densities p1 (x); p2 (x) and mixing
parameter t 2 (0; 1), let the mixture density of best (cid:2)tting parametric model be tf1 (xj(cid:18)1 ) + (1 (cid:0)
t)f2 (xj(cid:18)2 ) where f1 ; f2 belongs to some parametric class F and true densities p1 ; p2 are perturbed
version of f1 ; f2 . For a perturbation size (cid:15)2 > 0, if jjf1 (cid:0) p1 jj d
2 ;2 (cid:20) (cid:15)2 ; jjf2 (cid:0) p2 jj d
2 ;2 (cid:20) (cid:15)2
and (cid:18)1 ; (cid:18)2 are known then for any 0 < (cid:14) < 1, there exists positive constants A; B such that for
l (cid:20) l(cid:3) = L(6; t; (cid:15)2) labeled example, Perror (cid:0) PBayes reduces exponentially fast in the number of
labeled examples with probability at least (1 (cid:0) (cid:14)). If more labeled examples l > l (cid:3) are provided
Perror (cid:0) PBayes asymptotically converges to zero at a rate O (cid:16) 1
pl (cid:17) as l ! 1.
After using l(cid:3) labeled examples Perror = PBayes + O ((cid:15)2 ). Thus, from the above theorem it can
be thought that as labeled examples are added, initially the excess error reduces at a very fast rate
(exponentially in the number of labeled examples) until Perror (cid:0) PBayes = O ((cid:15)2 ). After that
the excess error reduces only polynomially fast in the number of labeled examples. In proving of
the above theorem we used (cid:2)rst order Taylor series approximation to get an crude upper bound for
decision boundary movement. However, in case of a speci(cid:2)c class of parametric densities such a
crude approximation may not be necessary. In particular, as we show next, if the best (cid:2)tting model
is a mixture of spherical Gaussians where the boundary is linear hyperplane, explicit upper bound
of boundary movement can be found. In the following, we assume the class F to be a class of d
dimensional spherical Gaussians with identity covariance matrix. However, the true model is an
equiprobable mixture of perturbed versions of these individual class densities. As before, given
u unlabeled examples and l labeled examples we want a strategy to learn a classi(cid:2)cation rule and
analyze the effect of these examples and also of perturbation size (cid:15)2 in reducing excess probability
of error.
One option to achieve this task is to use the unlabeled examples to estimate the true mixture density
2 p2 , however number of unlabeled examples required to estimate mixture density using non
2 p1 + 1
1
parametric kernel density estimation is exponential to the number of dimensions [10]. Thus, for
high dimensional data this is not an attractive option and also such an estimate does not provide any
clue as to where the decision boundary is. A better option will be to use the unlabeled examples
to estimate the best (cid:2)tting Gaussians within F . Number of unlabeled examples needed to estimate
such a mixture of Gaussians is only polynomial in the number of dimensions [10] and it is easy
to show that the distance between the Bayesian decision function and the decision function due to
2 ;2 norm sense.
Gaussian approximation is at most (cid:15)2 away in jj:jj d
5

Now suppose we use the following strategy to use labeled and unlabeled examples.
Strategy 2:
1. Assume the examples are distributed according to a mixture of equiprobable Gaussians
with unit covariance matrices and apply maximum likelihood estimation method to (cid:2)nd the
best Gaussian approximation of the densities.
2. Use small number of labeled examples l (cid:3) to label the two approximate decision regions
correctly with high probability.
3. If more (l > l(cid:3) ) labeled examples are available, use them to learn a better decision function
using some nonparametric technique.
Theorem 2.5. In a two class mixture model with equiprobable class densities p1 (x); p2 (x), let the
2 f2 (xj(cid:18)2 ) where f1 ; f2 are
mixture density of the best (cid:2)tting parametric model be 1
2 f1 (xj(cid:18)1 ) + 1
d dimensional spherical Gaussians with means (cid:18)1 ; (cid:18)2 2 Rd and p1 ; p2 are perturbed version of
f1 ; f2 , such that for a perturbation size (cid:15)2 > 0, jjf1 (cid:0) p1 jj d
2 ;2 (cid:20) (cid:15)2 . For
2 ;2 (cid:20) (cid:15)2 ; jjf2 (cid:0) p2 jj d
any (cid:15) > 0 and 0 < (cid:14) < 1, there exists positive constants A; B such that if strategy 2 is used
(cid:14) )(cid:17) unlabeled and l(cid:3) = L (0:5; 12; ((cid:15) + (cid:15)2 )) labeled examples
with u = O (cid:16) d2
(cid:15)3 (cid:14) (d log 1
(cid:15) + log 1
then for l (cid:20) l(cid:3) , Perror (cid:0) PBayes reduces exponentially fast in the number of labeled examples
with probability at least (1 (cid:0) (cid:14)). If more labeled examples l > l (cid:3) are provided, Perror (cid:0) PBayes
asymptotically converges to zero at most at a rate O (cid:16) 1
pl (cid:17) as l ! 1. If we represent the reduction
rate of this excess error (Perror (cid:0) PBayes ) as a function of labeled examples as Ree (l), then this
can compactly represented as,
if l (cid:20) l(cid:3)
Ree (l) = ( O (exp((cid:0)l))
pl (cid:17)
O (cid:16) 1
if l > l(cid:3)
After using l(cid:3) labeled examples, Perror = PBayes + O((cid:15) + (cid:15)2 ). Note that when number of unla-
beled examples is in(cid:2)nite, parameters of the best (cid:2)tting model can be estimated arbitrarily well, i.e.,
(cid:15) ! 0 and hence Perror (cid:0) PBayes reduces exponentially fast in the number of labeled examples
until Perror (cid:0) PBayes = O((cid:15)2 ). On the other hand if (cid:15) = O((cid:15)2 ), Perror (cid:0) PBayes still reduces
exponentially fast in the number of labeled examples until Perror (cid:0) PBayes = O((cid:15)2 ). This implies
that O((cid:15)2 ) close estimate of parameters of the best (cid:2)tting model is (cid:147)good(cid:148) enough. A more precise
estimate of parameters of the best (cid:2)tting model using more unlabeled examples does not help reduc-
ing Perror (cid:0) PBayes at the same exponential rate beyond Perror (cid:0) PBayes = O((cid:15)2 ). The following
Corollary states this important fact.
Corollary 2.6. For a perturbation size (cid:15)2 > 0, let the best (cid:2)tting model for a mixture of equiprobable
densities be a mixture of equiprobable d dimensional spherical Gaussians with unit covariance
If using u(cid:3) = O (cid:16) d2
(cid:14) )(cid:17) unlabeled examples parameters of the best
matrices.
2 (cid:14) (d log 1
+ log 1
(cid:15)3
(cid:15)2
(cid:2)tting model can be estimated O((cid:15)2 ) close in Euclidean norm sense, then any additional unlabeled
examples u > u(cid:3) does not help in reducing the excess error.

3 Discussion on different rates of convergence
In this section we discuss the effect of perturbation size (cid:15)2 on the behavior of Perror (cid:0) PBayes and
its effect on controlling the value of labeled and unlabeled examples. Different combinations of
number of labeled and unlabeled examples give rise to four different regions where P error (cid:0) PBayes
behaves differently as shown in Figure 1 where the x axis corresponds to the number of unlabeled
examples and the y axis corresponds to the number of labeled examples.
Let u(cid:3) be the number of unlabeled examples required to estimate the parameters of the best (cid:2)tting
model O((cid:15)2 ) close in Euclidean norm sense. Using O (cid:3) notation to hide the log factors, according to
2 (cid:17). When u > u(cid:3) , unlabeled examples have no role to play in reducing
Theorem 2.5, u(cid:3) = O(cid:3) (cid:16) d3
(cid:15)3
Perror (cid:0) PBayes as shown in region II and part of III in Figure 1. For u (cid:20) u(cid:3) , unlabeled examples
becomes useful only in region I and IV. When u(cid:3) unlabeled examples are available to estimate the
parameters of the best (cid:2)tting model O((cid:15)2 ) close, let the number of labeled examples required to
6

label the estimated decision regions so that Perror (cid:0) PBayes = O((cid:15)2 ) be l(cid:3) . The (cid:2)gure is just for
graphical representation of different regions where Perror (cid:0) PBayes reduces at different rates.
l

I V : O (cid:16) 1pl (cid:17)
Non-parametric methods

l(cid:3)1

l(cid:3)

I I I : (cid:18) O(cid:3)(cid:16)p d
u1=3 (cid:17)
l (cid:17)+O(cid:3)(cid:16) d
2

(cid:19)

I : O(exp((cid:0)l)) + O (cid:3) (cid:0) d
u1=3 (cid:1)

I I : O(exp((cid:0)l))
u(cid:3) = O(cid:3) (cid:16) d3
2 (cid:17)
(cid:15)3
Figure 1: The Big Picture. Behavior of Perror (cid:0) PBayes for different labeled and unlabeled exam-
ples

u

3.1 Behavior of Perror (cid:0) PBayes in Region-I
In this region u (cid:20) u(cid:3) unlabeled examples estimate the decision regions and l (cid:3)u labeled examples,
which depends on u, are required to correctly label these estimated regions. P error (cid:0)PBayes reduces
at a rate O (exp((cid:0)l)) + O (cid:16) d
3 (cid:17) for u < u(cid:3) and l < l(cid:3)u . This rate can be interpreted as the rate
1
u
at which unlabeled examples estimate the parameters of the best (cid:2)tting model and rate at which
labeled examples correctly label these estimated decision regions. However, for small u estimation
of the decision regions will be bad and and corresponding l (cid:3)u > l(cid:3) . Instead of using these large
number labeled examples to label poorly estimated decision regions, they can instead be used to
estimate the parameters of the best (cid:2)tting model and as will be seen next, this is precisely what
happens in region III. Thus in region I, l is restricted to l < l (cid:3) and Perror (cid:0) PBayes reduces at a rate
3 (cid:17).
exp ((cid:0)O(l)) + O (cid:16) d
1
u
3.2 Behavior of Perror (cid:0) PBayes in Region-II
In this section l (cid:20) l(cid:3) and u > u(cid:3) . As shown in Corollary 2.6, using u(cid:3) unlabeled examples
parameters of the best (cid:2)tting model can be estimated O((cid:15)2 ) close in Euclidean norm sense and more
precise estimate of the best (cid:2)tting model parameters using more unlabeled examples u > u(cid:3) does
not help reducing Perror (cid:0) PBayes . Thus, unlabeled examples have no role to play in this region
and for small number of labeled examples l (cid:20) l (cid:3) , Perror (cid:0) PBayes reduces at a rate O (exp((cid:0)l)).
3.3 Behavior of Perror (cid:0) PBayes in Region-III
In this region u (cid:20) u(cid:3) and hence model parameters have not been estimated O((cid:15)2 ) close to the
parameters of the best (cid:2)tting model. Thus, in some sense model assumptions are still valid and
there is a scope for better estimation of the parameters. Number of labeled examples available in
this region is greater than what is required for mere labeling the estimated decision regions using
u unlabeled examples and hence these excess labeled examples can be used to estimate the model
parameters. Note that once the parameters have been estimated O((cid:15)2 ) close to the parameters of the
best (cid:2)tting model using labeled examples, parametric model assumptions are no longer valid. If l (cid:3)1
is the number of such labeled examples, then in this region l (cid:3) < l (cid:20) l(cid:3)1 . Also note that depending
on number of unlabeled examples u (cid:20) u(cid:3) , l(cid:3) , and l(cid:3)1 are not (cid:2)xed numbers but will depend on
u. In presence of labeled examples alone, using Theorem 2.3, Perror (cid:0) PBayes reduces at a rate
O(cid:3) (cid:18)q d
l (cid:19). Since parameters are being estimated both using labeled and unlabeled examples, the
7

effective rate at which Perror (cid:0) PBayes reduces at this region can be thought of as the mean of the
two.

3.4 Behavior of Perror (cid:0) PBayes in Region-IV
In this region when u > u(cid:3) ; l > l(cid:3) and when u (cid:20) u(cid:3) ; l > l(cid:3)1 . In either case, since the parameters of
the best (cid:2)tting model have been estimated O((cid:15)2 ) close to the parameters of the best (cid:2)tting model,
parametric model assumptions are also no longer valid and excess labeled examples must be used in
nonparametric way. For nonparametric classi(cid:2)cation technique either one of the two basic families
of classi(cid:2)ers, plug-in classi(cid:2)ers or empirical risk minimization (ERM) classi(cid:2)ers can be used [13,
9]. A nice discussion on the rate and fast rate of convergence of both these types of classi(cid:2)ers
can be found in [1, 12]. The general convergence rate i.e.
the rate at which expected value of
(Perror (cid:0) PBayes ) reduces is of the order O(l(cid:0)(cid:12) ) as l ! 1 where (cid:12) > 0 is some exponent and is
typically (cid:12) (cid:20) 0:5. Also it was shown in [14] that under general conditions this bound can not be
improved in a minimax sense. In particular it was shown that if the true densities belong to C 1 class
then this rate is O( 1
). However, if in(cid:2)nite differentiability condition is not satis(cid:2)ed then this rate
pl
is much slower.
Acknowledgements This work was supported by NSF Grant No 0643916.

References
[1] J. Y. Audibert and A. Tsybakov. Fast convergence rate for plug-in estimators under margin
conditions. In Unpublished manuscript, 2005.
[2] M-F. Balcan and A. Blum. A PAC-style model for learning from labeled and unlabeled data.
In 18th Annual Conference on Learning Theory, 2005.
[3] M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine Learn-
ing, 56, Invited, Special Issue on Clustering:209(cid:150)239, 2004.
[4] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In 11th
Annual Conference on Learning Theory, 1998.
[5] V. Castelli and T. M. Cover. The relative values of labeled and unlabeld samples in pat-
IEEE Trans. Information Theory,
tern recognition with an unknown mixing parameters.
42((6):2102(cid:150)2117, 1996.
[6] O. Chapelle, J. Weston, and B. Scholkopf. Cluster kernels for semi-supervised learning. NIPS,
15, 2002.
[7] O. Chapelle and A. Zien. Semi-supervised classi(cid:2)cation by low density separation. In 10th
International Workshop on Arti(cid:2)cial Intelligence and Statistics, 2005.
[8] S. Dasgupta, M. L. Littman, and D. McAllester. PAC generalization bounds for co-training.
NIPS, 14, 2001.
[9] L. Devroye, L. Gyor(cid:2), and G. Lugosi. A probabilistic theory of pattern recognition. Springer,
New York, Berlin, Heidelberg, 1996.
[10] J. Ratsaby. The complexity of learning from a mixture of labeled and unlabeled examples. In
Phd Thesis, 1994.
[11] J. Ratsaby and S. S. Venkatesh. Learning from a mixture of labeled and unlabeled examples
with parametric side information. In 8th Annual Conference on Learning Theory, 1995.
[12] A. B. Tsybakov. Optimal aggregation of classi(cid:2)ers in statistical learning. Ann. Statist.,
32(1):135(cid:150)166, 1996.
[13] V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.
[14] Y. Yang. Minimax nonparametric classi(cid:2)cation- part I: Rates of convergence, part II: Model
selection for adaptation. IEEE Trans. Inf. Theory, 45:2271(cid:150)2292, 1999.
[15] X. Zhu. Semi-supervised literature survey. Technical Report 1530, Department of Computer
Science, University of Wisconsin Madison, December 2006.

8


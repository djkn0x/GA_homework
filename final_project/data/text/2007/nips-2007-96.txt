Learning with Tree-Averaged Densities and
Distributions

Sergey Kirshner
AICML and Dept of Computing Science
University of Alberta
Edmonton, Alberta, Canada T6G 2E8
sergey@cs.ualberta.ca

Abstract

We utilize the ensemble of trees framework, a tractable mixture over super-
exponential number of tree-structured distributions [1], to develop a new model
for multivariate density estimation. The model is based on a construction of tree-
structured copulas – multivariate distributions with uniform on [0, 1] marginals.
By averaging over all possible tree structures, the new model can approximate
distributions with complex variable dependencies. We propose an EM algorithm
to estimate the parameters for these tree-averaged models for both the real-valued
and the categorical case. Based on the tree-averaged framework, we propose a
new model for joint precipitation amounts data on networks of rain stations.

1 Introduction

Multivariate real-valued data appears in many real-world data sets, and a lot of research is being
focused on the development of multivariate real-valued distributions. One of the challenges in con-
structing such distributions is that univariate continuous distributions commonly do not have a clear
multivariate generalization. The most studied exception is the multivariate Gaussian distribution ow-
ing to properties such as closed form density expression with a convenient generalization to higher
dimensions and closure over the set of linear projections. However, not all problems can be ad-
dressed fairly with Gaussians (e.g., mixtures, multimodal distributions, heavy-tailed distributions),
and new approaches are needed for such problems.
While modeling multivariate distributions is in general difﬁcult due to complicated functional forms
and the curse of dimensionality, learning models for individual variables (univariate marginals) is
often straightforward. Once the univariate marginals are known (or assumed known), the rest can
be modeled using copulas, multivariate distributions with all univariate marginals equal to uniform
distributions on [0, 1] (e.g., [2, 3]). A large portion of copula research concentrated on bivariate
copulas as extensions to higher dimensions are often difﬁcult. Thus if the desired distribution de-
composes into its univariate marginals and only bivariate distributions, the machinery of copulas can
be effectively utilized.
Distributions with undirected tree-structured graphical models (e.g., [4]) have exactly these prop-
erties, as probability density functions over the variables with tree-structured conditional indepen-
dence graphs can be written as a product involving univariate marginals and bivariate marginals
corresponding to the edges of the tree. While tree-structured dependence is perhaps too restrictive,
a richer variable dependence can be obtained by averaging over a small number of different tree
structures [5] or all possible tree structures; the latter can be done analytically for categorical-valued
distributions with an ensemble-of-trees model [1]. In this paper, we extend this tree-averaged model
to continuous variables with the help of copulas and derive a learning algorithm to estimate the
parameters within the maximum likelihood framework with EM [6]. Within this framework, the

1

parameter estimation for tree-structured and tree-averaged models requires optimization over only
univariate and bivariate densities potentially avoiding the curse of dimensionality, a property not
shared by alternative models that relax the dependence restriction of trees (e.g., vines [7]).
The main contributions of the paper are the new tree-averaged model for multivariate copulas, a
parameter estimation algorithm for tree-averaged framework (for both categorical and real-valued
complete data), and a new model for multi-site daily precipitation amounts, an important application
in hydrology. In the process, we introduce previously unexplored tree-structured copula density and
an algorithm for estimation of its structure and parameters. The paper is organized as follows.
First, we describe copulas, their densities, and some of their useful properties (Section 2). We then
construct multivariate copulas with tree-structured dependence from bivariate copulas (Section 3.1)
and show how to estimate the parameters of the bivariate copulas and perform the edge selection.
To allow more complex dependencies between the variables, we describe a tree-averaged copula,
a novel copula object constructed by averaging over all possible spanning trees for tree-structured
copulas, and derive a learning algorithm for the estimation of the parameters from data for the tree-
averaged copulas (Section 4). We apply our new method to a benchmark data set (Section 5.1);
we also develop a new model for multi-site precipitation amounts, a problem involving both binary
(rain/no rain) and continuous (how much rain) variables (Section 5.2).

2 Copulas

Let X = (X1 , . . . , Xd ) be a vector random variable with corresponding probability distribution
F (cdf) deﬁned on Rd . We denote by V the set of d components (variables) of X and refer
to individual variables as Xv for v ∈ V . For simplicity, we will refer to assignments to ran-
dom variables by lower case letters, e.g., Xv = xv will be denoted by xv . Let Fv (xv ) =
F (Xv = xv , Xu = ∞ : u ∈ V \ {v}) denote a univariate marginal of F over the variable Xv .
Let pv (xv ) denote the probability density function (pdf) of Xv . Let av = Fv (xv ), and let
a = (a1 , . . . , ad ), so a is a vector of quantiles of components of x with respect to corresponding
univariate marginals. Next, we deﬁne copula, a multivariate distribution over vectors of quantiles.
Deﬁnition 1. The copula associated with F is a distribution function C : [0, 1]d → [0, 1] that
satisﬁes
F (x) = C (F1 (x1 ) , . . . , Fd (xd )) , x ∈ Rd .
(1)
F (cid:0)F −1
(ad )(cid:1) is the unique choice for (1).
If F is a continuous distribution on Rd with univariate marginals F1 , . . . , Fd , then C (a) =
(a1 ) , . . . , F −1
1
d
Assuming that F has d-th order partial derivatives, the probability density function (pdf) can be
obtained from the distribution function via differentiation and expressed in terms of a derivative of
= c (a) Y
Y
a copula:
p (x) = ∂ dF (x)
= ∂ dC (a)
= ∂ dC (a)
∂ av
pv (xv )
(2)
∂xv
∂ a1 . . . ∂ ad
∂x1 . . . ∂xd
∂x1 . . . ∂xd
v∈V
v∈V
Suppose we are given a complete data set D = (cid:8)x1 , . . . , xN (cid:9) of d-component real-valued vec-
where c (a) = ∂ dC (a)
is referred to as a copula density function.
(cid:1) under i.i.d. assumption. A maximum likelihood (ML) estimate for the
tors xn = (cid:0)xn
∂a1 ...∂ad
1 , . . . , xd
parameters of c (or p) from data can be obtained my maximizing the log-likelihood of D
1
NX
NX
ln p (D) = X
v∈V
n=1
n=1
The ﬁrst term of the log-likelihood corresponds to the total log-likelihood of all univariate marginals
of p, and the second term to the log-likelihood of its d-variate copula. These terms are not inde-
pendent as the second term in the sum is deﬁned in terms of the probability expressions in the ﬁrst
summand; except for a few special cases, a direct optimization of (3) is prohibitively complicated.
However a useful (and asymptotically consistent) heuristic is ﬁrst to maximize the log-likelihood for
the marginals (ﬁrst term only), and then to estimate the parameters for the copula given the solution

1 ) , . . . , Fd (xn
ln c (F1 (xn
d )) .

ln pv (xn
v ) +

(3)

2

for the marginals. The univariate marginals can be accurately estimated by either ﬁtting the parame-
ters for some appropriately chosen univariate distributions or by applying non-parametric methods1
(cid:16) ˆF (xn
(cid:17)
Let A = (cid:8)a1 , . . . , aN (cid:9) where an = (an
as the marginals are estimated independent of each other and do not suffer from the curse of di-
mensionality. Let ˆpv (xv ) be the estimated pdf for component v , and ˆFv be the corresponding cdf.
1 ) , . . . , ˆF (xn
ln c (A) = PN
d ) =
d )
be a set of estimated
1 , . . . , an
quantiles. Under the above heuristic, ML estimate for copula density c is computed by maximizing
n=1 ln c (an ).
3 Exploiting Tree-Structured Dependence

Joint probability distributions are often modeled with probabilistic graphical models where the struc-
ture of the graph captures the conditional independence relations of the variables. The joint distri-
bution is then represented as a product of functions over subsets of variables. We would like to
keep the number of variables for each of the functions small as the number of parameters and the
number of points needed for parameter estimation often grows exponentially with the number of
variables. Thus, we focus on copulas with tree dependence. Trees play an important role in prob-
abilistic graphical models as they allow for efﬁcient exact inference [10] as well as structure and
parameter learning [4]. They can also be placed in a fully Bayesian framework with decomposable
priors allowing to compute expected values (over all possible spanning trees) of product of functions
deﬁned on the edges of the trees [1]. As we will see later in this section, under the tree-structured de-
pendence, a copula density can be computed as products of bivariate copula densities over the edges
of the graph. This property allows us to estimate the parameters for the edge copulas independently.

3.1 Tree-Structured Copulas

cp (a) =

cp (au , av ) .

We consider tree-structured Markov networks, i.e., undirected graphs that do not have loops. For a
distribution F admitting tree-structured Markov networks (referred from now on as tree-structured
"Y
# Y
distributions), assuming that p (x) > 0 and p (x) < ∞ for x ∈ R ⊆ Rd , the density (for x ∈ R)
can be rewritten as
puv (xu , xv )
p (x) =
pv (xv )
(4)
pu (xu ) pv (xv ) .
v∈V
{u,v}∈E
This formulation easily follows from the Hammersley-Clifford theorem [11]. Note that for {u, v} ∈
E , a copula density cuv (au , av ) for F (xu , xv ) can be computed using Equation 2:
cuv (au , av ) = puv (xu , xv )
pu (xu ) pv (xv ) .
= Y
= Y
Using Equations 2, 4, and 5, cp (a) for F (x) can be computed as
Q
p (x)
puv (xu , xv )
pu (xu ) pv (xv )
v∈V pv (xv )
{u,v}∈E
{u,v}∈E
Equation 6 states that a copula density for a tree-structured distribution decomposes as a product
of bivariate copulas over its edges. The converse is true as well; a tree-structured copula can be
constructed by specifying copulas for the edges of the tree.
cE (a) = Y
Theorem 1. Given a tree or a forest G = (V , E ) and copula densities cuv (au , av ) for {u, v} ∈ E ,
cuv (au , av )
{u,v}∈E
ln c (A) = X
NX
For a tree-structured density, the copula log-likelihood can be rewritten as
{u,v}∈E
n=1
1These approaches for copula estimation are referred to as inference for the margins (IFM) [8] and canonical
maximum likelihood (CML) [9] for parametric and non-parametric forms for the marginals, respectively.

is a valid copula density.

ln cuv (an
v ) ,
u , an

(5)

(6)

3

and the parameters can be ﬁtted by maximizing PN
n=1 ln cuv (an
v ) independently for different
u , an
pairs {u, v} ∈ E . The tree structure can be learned from the data as well, as in the Chow-Liu
algorithm [4]. Full algorithm can be found in an extended version of the paper [12].

4 Tree-Averaged Copulas

While the framework from Section 3.1 is computationally efﬁcient and convenient for implementa-
tion, the imposed tree-structured dependence is too restrictive for real-world problems. Vines [7],
for example, deal with this problem by allowing recursive reﬁnements for the bivariate probabilities
over variables not connected by the tree edges. However, vines require estimation of additional char-
acteristics of the distribution (e.g., conditional rank correlations) requiring estimation over large sets
of variables, which is not advisable when the amount of available data is not large. Our proposed
method would only require optimization of parameters of bivariate copulas from the corresponding
two components of weighted data vectors. Using the Bayesian framework for spanning trees from
[1], it is possible to construct an object constituting a convex combination over all possible spanning
trees allowing a much richer set of conditional independencies than a single tree.
Meil ˘a and Jaakkola [1] proposed a decomposable prior over all possible spanning tree structures.
Let β be a symmetric matrix of non-negative weights for all pairs of distinct variables and zeros on
the diagonal. Let E be a set of all possible spanning trees over V . The probability distribution over
Y
βuv where Z = X
Y
all spanning tree structures over V is deﬁned as
1
P (E ∈ E|β) =
Z
E ∈E
{u,v}∈E
{u,v}∈E
Even though the sum is over |E| = dd−2 trees, Z can be efﬁciently computed in closed form using
a weighted generalization of Kirchoff ’s Matrix Tree Theorem (e.g., [1]).
Theorem 2. Let P (E ) be a distribution over spanning tree structures deﬁned by (7). Then the
normalization constant Z is equal to the determinant |L? (β)|, with matrix L? (β) representing the
(cid:26) −βuv
ﬁrst (d − 1) rows and columns of the matrix L (β) given by:
P
u, v ∈ V , u 6= v ;
w∈V βvw u, v ∈ V , u = v .
β is a generalization of an adjacency matrix, and L (β) is a generalization of the Laplacian matrix.
The decomposability property of the tree prior (Equation 7) allows us to compute the average of
the tree-structured distributions over all dd−2 tree structures. In [1], such averaging was applied to
 Y
  Y
 =
tree-structured distributions over categorical variables. Similarly, we deﬁne a tree-averaged copula
X
r (a) = X
density as a convex combination of copula densities of the form (6):
cuv (au , av )
E ∈E
E ∈E
{u,v}∈E
{u,v}∈E
where entry (uv) of matrix βc (a) denotes βuv cuv (au , av ). A ﬁnite convex combination of copulas
is a copula, so r (a) is a copula density.

|L? (βc (a))|
|L? (β)|

Luv (β) = Lvu (β) =

P (E |β) c (a) =

1
Z

βuv .

(7)

βuv

4.1 Parameter Estimation
Given a set of estimated quantile values A, a suitable parameter values β (edge weight matrix) and
θ (parameters for bivariate edge copulas) can be found by maximizing the log-likelihood of A:
NX
NX
ln |L? (βc (an |θ))| − N ln |L? (β)| .
ln r (an |β , θ) =
l (β , θ) = ln r (A|β , θ) =
n=1
n=1
However, the parameter optimization of l (β , θ) cannot be done analytically. Instead, noticing that
we are dealing with a mixture model (granted, one where the number of mixture components is
super-exponential), we propose performing the parameter optimization with the EM algorithm [6].2
2A possibility of EM algorithm for ensemble-of-trees with categorical data was mentioned [1], but the idea
was abandoned due to the concern about the M-step.

(8)

4

Algorithm TR EEAV ERAGEDCO PU LAD EN S I TY(D , c)
Inputs: A complete data set D of d-component real-valued vectors; a set of of bivariate para-
metric copula densities c = {cuv : u, v ∈ V }
1. Estimate univariate margins ˆFv (Xv ) for all components v ∈ V treating all components
(cid:16) ˆF1 (xn
(cid:17)
independently.
2. Replace D with A consisting of vectors an =
1 ) , . . . , ˆFd (xn
d )
xn in D
3. Initialize β and θ
4. Run until convergence (as determined by change in log-likelihood, Equation 8)
• E-step: For all vectors an and pairs {u, v}, compute P ({u, v} ∈ E |an , β , θ)
• M-step:
– Update β with gradient ascent
(cid:21) |L? (βc(a))|
(cid:20) Q
– Update θuv for all pairs by setting partial derivative with respect to parameters
of θuv (Equation 9) to zero and solving corresponding equations
Output: Denoting au = ˆF (xu ) and av = ˆF (xv ), ˆp (x) =
ˆpv (xv )
|L? (β)|
v∈V

for each vector

0

, θ

.

Figure 1: Algorithm for estimation of a pdf with tree-averaged copulas.
0 maximizing the expected joint log-likelihood M (cid:0)β
0 ; β , θ(cid:1) given current
While there are dd−2 possible mixture components (spanning trees), in the E-step, we only need
to compute the posterior probabilities for d (d − 1) /2 edges. Each step of EM consists of ﬁnd-
0
0
ing parameters β
, θ
, θ
X
NX
0 (cid:1)(cid:3)
0 (cid:1) c (cid:0)an |E , θ
P (En |an , β , θ) ln (cid:2)P (cid:0)E |β
0 ; β , θ(cid:1) =
M (cid:0)β
parameter values β , θ where
NX
= X
0 (cid:1)(cid:12)(cid:12) ;
uv )) − N ln (cid:12)(cid:12)L? (cid:0)β
En∈E
n=1
v |θ 0
sn ({u, v}) (ln β 0
Q{u,v}∈E (βuv cuv (an
uv + ln cuv (an
u , an
sn ({u, v}) = X
P (En |an , β , θ) = X
{u,v}
n=1
v |θuv ))
u , an
|L? (βc (an ))|
E∈E
E∈E
{u,v}∈E
{u,v}∈E
The probability distribution P (En |an , β , θ) is of the same form as the tree prior, so to compute
sn ({u, v}) one needs to compute the sum of probabilities of all trees containing edge {u, v}.
Theorem 3. Let P (E |β) be a tree prior deﬁned in Equation 7. Let Q (β) = (L? (β))−1 where L?
( βuv (Quu (β) + Qvv (β) − 2Quv (β))
X
is obtained by removing row and column w from L. Then
: u 6= v , u 6= w, v 6= w,
P (E |β) =
βuwQuu (β)
:
v = w,
u = w.
:
βwvQvv (β)
E ∈E: {u,v}∈E
O (cid:0)d3 (cid:1). Assuming a candidate bivariate copula cuv has one free parameter θuv , θuv can be optimized
As a consequence of Theorem 3, for each an , all d (d − 1) /2 edge probabilities sn ({u, v}) can
be computed simultaneously with time complexity of a single (d − 1) × (d − 1) matrix inversion,
∂M (cid:0)β
0 ; β , θ(cid:1)
NX
by setting
0
v ; θ 0
sn ({u, v}) ∂ ln cuv (an
uv )
u , an
, θ
=
(9)
∂ θ 0
∂ θ 0
 
!
uv
uv
n=1
X
NX
to 0. (See [12] for more details.) The parameters of the tree prior can be updated by maximizing
sn ({u, v})
uv − ln |L? (β)| ,
ln β 0
{u,v}
n=1

1
N

,

5

ln βuv ∀ {u, v}, with time complexity O (cid:0)d3 (cid:1) per iteration. The outline of the EM algorithm is
an expression concave in ln βuv ∀ {u, v}. β
0 can be updated using a gradient ascent algorithm on
complexity of each EM iteration is O (cid:0)N d3 (cid:1).
shown in Figure 1. Assuming the complexity of each bivariate copula update is O (N ), the time
The EM algorithm can be easily transferred to tree averaging for categorical data. The E-step does
not change, and in the M-step, the parameters for the univariate marginals are updated ignoring
bivariate terms. Then, the parameters for the bivariate distributions for each edge are updated con-
strained on the new values of the parameters for the univariate distributions. While the algorithm
does not guarantee a maximization of the expected log-likelihood, it nonetheless worked well in our
experiments.

5 Experiments

5.1 MAGIC Gamma Telescope Data Set

First, we tested our tree-averaged density estimator on a MAGIC Gamma Telescope Data Set
from the UCI Machine Learning Repository [13]. We considered only the examples from
this set consists of 12332 vectors of d = 10 real-valued compo-
class gamma (signal);
nents. The univariate marginals are not Gaussian (some are bounded; some have multiple
modes). Fig. 2 shows an average log-likelihood of models trained on training sets with N =
50, 100, 200, 500, 1000, 2000, 5000, 10000 and evaluated on 2000-example test sets (averaged over
10 training and test sets). The marginals were estimated using Gaussian kernel density estima-
tors (KDE) with Rule-of-Thumb bandwidth selection. All of the models except for full Gaussian
have the same marginals, differ only in the multivariate dependence (copula). As expected from
the curse of dimensionality, product KDE improves logarithmically with the amount of data. Not
only the marginals are not Gaussian (evidenced by a Gaussian copula with KDE marginals outper-
forming a Gaussian distribution), the multivariate dependence is also not Gaussian, evidenced by a
tree-structured Frank copula outperforming a tree-structured and a full Gaussian copula. However,
model averaging even with the wrong dependence model (tree-averaged Gaussian copula) yields
superior performance.

5.2 Multi-Site Precipitation Modeling

We applied the tree-averaged framework to the problem of modeling daily rainfall amounts for a
regional spatial network of stations. The task is to build a generative model capturing the spatial
and temporal properties of the data. This model can be used in at least two ways: ﬁrst, to sample
sequences from it and to use them as inputs for other models, e.g., crop models; and second, as
a descriptive model of the data. Hidden Markov models (possible with non-homogeneous transi-
tions) are being frequently used for this task (e.g., [14]) with the transition distribution responsible
for modeling of temporal dependence, and the emission distributions capturing most of the spatial
dependence. Additionally, HMMs can be viewed as assigning rainfall daily patterns to “weather
states” (or corresponding emission components), and both these states (as described by either their
parameters or the statistics of the patterns associated with it) and their temporal evolution often
offer useful synoptic insight. We will use HMMs as the wrapper model with tree-averaged (and
tree-structured) distributions to model the emission components.
The distribution of daily rainfall amounts for any given station can be viewed as a non-overlapping
mixture with one component corresponding to zero precipitation, and the other component to posi-
tive precipitation. For a station v , let rv be the precipitation amount, πv be a probability of positive
precipitation, and let fv (rv |λv ) be a probability density function for amounts given positive precip-
(cid:26) 1 − πv
itation:
rv = 0,
:
πv fv (rv |λv )
rv > 0.
:
For a pair of stations {u, v}, let πuv denote the probability of simultaneous positive amounts and
cuv (Fu (ru |λu ) , Fv (rv |λv ) |θuv ) denote the copula density for simultaneous positive amounts;

p (rv |πv , λv ) =

6

then

.

p (ru , rv |πu , πv , πuv , λu , λv ) =

 1 − πu − πv + πuv
:
ru = 0, rv = 0,
(πv − πuv ) fv (rv |λv )
:
ru = 0, rv > 0,
(πu − πuv ) fu (ru |λu )
:
ru > 0, rv = 0,
πuv fu (ru ) fv (rv ) c (Fu (ru ) , Fv (rv ))
:
ru > 0, rv > 0.
# Y
"Y
We can now deﬁne a tree-structured and tree-averaged probability distributions, pt (r) and pta (r),
respectively, over the amounts:
ωuv (r) = p (ru , rv |πu , πv , πuv , λu , λv )
p (rv |πv )
p (ru |πu , λu ) p (rv |πv , λv ) , pt (r |π , λ, θ , E ) =
# |L? (βω (r))|
"Y
ωuv (r) ,
pta (r |π , λ, θ , β) = X
v∈V
{u,v}∈E
P (E |β) pt (r |π , λ, θ , E ) =
p (rv |πv )
|L? (β)|
v∈V
E ∈E
We employ univariate exponential distributions fv (rv ) = λv e−λv rv and bivariate Gaussian copulas
uv Φ−1 (av )2−2θuv Φ−1 (au )Φ−1 (av )
uv Φ−1 (au )2+θ2
− θ2
1√
2(1−θ2
cuv (au , av ) =
.
uv )
e
1−θ2
uv
We applied the models to a data set collected from 30 stations from a region in Southeastern Aus-
tralia (Fig. 3) 1986-2005, April-October, (20 sequences 214 30-dimensional vectors each). We
used a 5-state HMM with three different types of emission distributions: tree-averaged (pta ), tree-
structured (pt ), and conditionally independent (ﬁrst term of pt and pta ). We will refer to these
models HMM-TA, HMM-Tree, and HMM-CI, respectively. For HMM-TA, we reduced the number
of free parameters by only allowing edges for stations adjacent to each other as determined by the
the Delaunay triangulation (Fig. 3). We also did not learn the edge weights (β ) setting them to 1 for
selected edges and to 0 for the rest. To make sure that the models do not overﬁt, we computed their
out-of-sample log-likelihood with cross-validation, leaving out one year at a time (not shown). (5
states were chosen because the leave-one-year out log-likelihood starts to ﬂatten out for HMM-TA
at 5 states.) The resulting log-likelihoods divided by the number of days and stations are −0.9392,
−0.9522, and −1.0222 for HMM-TA, HMM-Tree, and HMM-CI, respectively. To see how well
the models capture the properties of the data, we trained each model on the whole data set (with
50 restarts of EM), and then simulated 500 sequences of length 214. We are particularly interested
in how well they measure pairwise dependence; we concentrate on two measures: log-odds ratio
for occurrence and Kendall’s τ measure of concordance for pairs when both stations had positive
amounts. Both are shown in Fig. 4. Both plots suggest that HMM-CI underestimates the pairwise
dependence for strongly dependent pairs (as indicated by its trend to predict lower absolute values
for log-odds and concordance); HMM-Tree estimating the dependence correctly mostly for strongly
dependent pairs (as indicated by good prediction for high values), but underestimating it for mod-
erate dependence; and HMM-TA performing the best for most pairs except for the ones with very
strong dependence.

Acknowledgements

This work has been supported by the Alberta Ingenuity Fund through the AICML. We thank Stephen
Charles (CSIRO, Australia) for providing us with precipitation data.

References
[1] M. Meil ˘a and T. Jaakkola. Tractable Bayesian learning of tree belief networks. Statistics and Computing,
16(1):77–92, 2006.
[2] H. Joe. Multivariate Models and Dependence Concepts, volume 73 of Monographs on Statistics and
Applied Probability. Chapman & Hall/CRC, 1997.
[3] R. B. Nelsen. An Introduction to Copulas. Springer Series in Statistics. Springer, 2nd edition, 2006.
[4] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE
Transactions on Information Theory, IT-14(3):462–467, May 1968.
[5] M. Meil ˘a and M. I. Jordan. Learning with mixtures of trees. Journal of Machine Learning Research,
1(1):1–48, October 2000.

7

Figure 2: Averaged test set per-feature log-
likelihood for MAGIC data:
independent KDE
(black solid (cid:5)), product KDE (blue dashed ◦),
Gaussian (brown solid ♦), Gaussian copula (or-
ange solid +), Gaussian tree-copula (magenta
dashed x), Frank tree-copula (blue dashed (cid:3)),
Gaussian tree-averaged copula (red solid x).

Figure 3: Station map with station locations (red
dots), coastline, and the pairs of stations se-
lected according to Delaunay triangulation (dot-
ted lines)

Figure 4: Scatter-plots of log-odds ratios for occurrence (left) and Kendall’s τ measure of concor-
dance (right) for all pairs of stations for the historical data vs HMM-TA (red o), HMM-Tree (blue
x), and HMM-CI (green ·).

[6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via EM
algorithm. Journal of the Royal Statistical Society Series B-Methodological, 39(1):1–38, 1977.
[7] T. Bedford and R. M. Cooke. Vines – a new graphical model for dependent random variables. The Annals
of Statistics, 30(4):1031–1068, 2002.
[8] H. Joe and J.J. Xu. The estimation method of inference functions for margins for multivariate models.
Technical report, Department of Statistics, University of British Columbia, 1996.
[9] C. Genest, K. Ghoudi, and L.-P. Rivest. A semiparametric estimation procedure of dependence parameters
in multivariate families of distributions. Biometrika, 82:543–552, 1995.
[10] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kauf-
mann Publishers, Inc., San Francisco, California, 1988.
[11] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical
Society Series B-Methodological, 36(2):192–236, 1974.
[12] S. Kirshner. Learning with tree-averaged densities and distributions. Technical Report TR 08-01, Depart-
ment of Computing Science, University of Alberta, 2008.
[13] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[14] E. Bellone. Nonhomogeneous Hidden Markov Models for Downscaling Synoptic Atmospheric Patterns to
Precipitation Amounts. PhD thesis, Department of Statistics, University of Washington, 2000.

8

5010020050010002000500010000−3.2−3.1−3−2.9−2.8−2.7−2.6Training set sizeLog−likelihood per feature  Independent KDEProduct KDEGaussianGaussian CopulaGaussian TCopulaFrank TCopulaGaussian TACopula143144145146147148149150−38−37−36−35−34−33LongitudeLatitude  CoastlineStationsSelected pairs11.522.533.544.5511.522.533.544.55Log−odds from the historical dataLog−odds from the simulated data  HMM−TAHMM−TreeHMM−CIy=x00.10.20.30.40.50.60.700.10.20.30.40.50.60.7Kendall’s τ from the historical dataKendall’s τ from the simulated data  HMM−TAHMM−TreeHMM−CIy=x
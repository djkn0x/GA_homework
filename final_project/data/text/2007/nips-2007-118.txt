People Tracking with the Laplacian Eigenmaps
Latent Variable Model

Zhengdong Lu
CSEE, OGI, OHSU
zhengdon@csee.ogi.edu

Miguel ´A. Carreira-Perpi ˜n ´an
EECS, UC Merced
http://eecs.ucmerced.edu

Cristian Sminchisescu
University of Bonn
sminchisescu.ins.uni-bonn.de

Abstract

Reliably recovering 3D human pose from monocular video requires models that
bias the estimates towards typical human poses and motions. We construct pri-
ors for people tracking using the Laplacian Eigenmaps Latent Variable Model
(LELVM). LELVM is a recently introduced probabilistic dimensionality reduc-
tion model that combines the advantages of latent variable models —a multimodal
probability density for latent and observed variables, and globally differentiable
nonlinear mappings for reconstruction and dimensionality reduction—with those
of spectral manifold learning methods —no local optima, abil
ity to unfold highly
nonlinear manifolds, and good practical scaling to latent spaces of high dimen-
sion. LELVM is computationally efﬁcient, simple to learn fr om sparse training
data, and compatible with standard probabilistic trackers such as particle ﬁlters.
We analyze the performance of a LELVM-based probabilistic sigma point mixture
tracker in several real and synthetic human motion sequences and demonstrate that
LELVM not only provides sufﬁcient constraints for robust op eration in the pres-
ence of missing, noisy and ambiguous image measurements, but also compares
favorably with alternative trackers based on PCA or GPLVM priors.

Recent research in reconstructing articulated human motion has focused on methods that can exploit
available prior knowledge on typical human poses or motions in an attempt to build more reliable
algorithms. The high-dimensionality of human ambient pose space—between 30-60 joint angles
or joint positions depending on the desired accuracy level, makes exhaustive search prohibitively
expensive. This has negative impact on existing trackers, which are often not sufﬁciently reliable at
reconstructing human-like poses, self-initializing or recovering from failure. Such difﬁculties have
stimulated research in algorithms and models that reduce the effective working space, either us-
ing generic search focusing methods (annealing, state space decomposition, covariance scaling) or
by exploiting speci ﬁc problem structure (e.g. kinematic ju mps). Experience with these procedures
has nevertheless shown that any search strategy, no matter how effective, can be made signi ﬁcantly
more reliable if restricted to low-dimensional state spaces. This permits a more thorough explo-
ration of the typical solution space, for a given, comparatively similar computational effort as a
high-dimensional method. The argument correlates well with the belief that the human pose space,
although high-dimensional in its natural ambient parameterization, has a signi ﬁcantly lower percep-
tual (latent or intrinsic) dimensionality, at least in a practical sense—many poses that are possible
are so improbable in many real-world situations that it pays off to encode them with low accuracy.

A perceptual representation has to be powerful enough to capture the diversity of human poses in a
sufﬁciently broad domain of applicability (the task domain ), yet compact and analytically tractable
for search and optimization. This justi ﬁes the use of models
that are nonlinear and low-dimensional
(able to unfold highly nonlinear manifolds with low distortion), yet probabilistically motivated and
globally continuous for efﬁcient optimization. Reducing d imensionality is not the only goal: per-
ceptual representations have to preserve critical properties of the ambient space. Reliable tracking
needs locality: nearby regions in ambient space have to be mapped to nearby regions in latent space.
If this does not hold, the tracker is forced to make unrealistically large, and difﬁcult to predict jumps
in latent space in order to follow smooth trajectories in the joint angle ambient space.

1

In this paper we propose to model priors for articulated motion using a recently introduced proba-
bilistic dimensionality reduction method, the Laplacian Eigenmaps Latent Variable Model (LELVM)
[1]. Section 1 discusses the requirements of priors for articulated motion in the context of proba-
bilistic and spectral methods for manifold learning, and section 2 describes LELVM and shows how
it combines both types of methods in a principled way. Section 3 describes our tracking frame-
work (using a particle ﬁlter) and section 4 shows experiment s with synthetic and real human motion
sequences using LELVM priors learned from motion-capture data.
Related work: There is signi ﬁcant work in human tracking, using both gener ative and discrimina-
tive methods. Due to space limitations, we will focus on the more restricted class of 3D generative
algorithms based on learned state priors, and not aim at a full literature review. Deriving com-
pact prior representations for tracking people or other articulated objects is an active research ﬁeld,
steadily growing with the increased availability of human motion capture data. Howe et al. and
Sidenbladh et al. [2] propose Gaussian mixture representations of short human motion fragments
(snippets) and integrate them in a Bayesian MAP estimation framework that uses 2D human joint
measurements, independently tracked by scaled prismatic models [3]. Brand [4] models the human
pose manifold using a Gaussian mixture and uses an HMM to infer the mixture component index
based on a temporal sequence of human silhouettes. Sidenbladh et al. [5] use similar dynamic priors
and exploit ideas in texture synthesis —efﬁcient nearest-ne
ighbor search for similar motion frag-
ments at runtime—in order to build a particle- ﬁlter tracker w
ith observation model based on contour
and image intensity measurements. Sminchisescu and Jepson [6] propose a low-dimensional proba-
bilistic model based on ﬁtting a parametric reconstruction mapping (sparse radial basis function) and
a parametric latent density (Gaussian mixture) to the embedding produced with a spectral method.
They track humans walking and involved in conversations using a Bayesian multiple hypotheses
framework that fuses contour and intensity measurements. Urtasun et al. [7] use a dynamic MAP
estimation framework based on a GPLVM and 2D human joint correspondences obtained from an
independent image-based tracker. Li et al. [8] use a coordinated mixture of factor analyzers within a
particle ﬁltering framework, in order to reconstruct human motion in multiple views using chamfer
matching to score different conﬁguration. Wang et al. [9] le arn a latent space with associated dy-
namics where both the dynamics and observation mapping are Gaussian processes, and Urtasun et
al. [10] use it for tracking. Taylor et al. [11] also learn a binary latent space with dynamics (using
an energy-based model) but apply it to synthesis, not tracking. Our work learns a static, generative
low-dimensional model of poses and integrates it into a particle ﬁlter for tracking. We show its
ability to work with real or partially missing data and to track multiple activities.

1 Priors for articulated human pose

We consider the problem of learning a probabilistic low-dimensional model of human articulated
motion. Call y ∈ RD the representation in ambient space of the articulated pose of a person. In this
paper, y contains the 3D locations of anywhere between 10 and 60 markers located on the person’s
joints (other representations such as joint angles are also possible). The values of y have been
normalised for translation and rotation in order to remove rigid motion and leave only the articulated
motion (see section 3 for how we track the rigid motion). While y is high-dimensional, the motion
pattern lives in a low-dimensional manifold because most values of y yield poses that violate body
constraints or are simply atypical for the motion type considered. Thus we want to model y in terms
of a small number of latent variables x given a collection of poses {yn }N
n=1 (recorded from a human
with motion-capture technology). The model should satisfy the following: (1) It should deﬁne a
probability density for x and y, to be able to deal with noise (in the image or marker measurements)
and uncertainty (from missing data due to occlusion or markers that drop), and to allow integration
in a sequential Bayesian estimation framework. The density model should also be ﬂexible enough
to represent multimodal densities. (2) It should deﬁne mapp ings for dimensionality reduction F :
y → x and reconstruction f : x → y that apply to any value of x and y (not just those in the
training set); and such mappings should be deﬁned on a global coordinate system, be continuous
(to avoid physically impossible discontinuities) and differentiable (to allow efﬁcient optimisation
when tracking), yet ﬂexible enough to represent the highly n onlinear manifold of articulated poses.
From a statistical machine learning point of view, this is precisely what latent variable models
(LVMs) do; for example, factor analysis deﬁnes linear mappi ngs and Gaussian densities, while the
generative topographic mapping (GTM; [12]) deﬁnes nonline ar mappings and a Gaussian-mixture
density in ambient space. However, factor analysis is too limited to be of practical use, and GTM—

2

while ﬂexible—has two important practical problems: (1) the
latent space must be discretised to
allow tractable learning and inference, which limits it to very low (2–3) latent dimensions; (2) the
parameter estimation is prone to bad local optima that result in highly distorted mappings.

Another dimensionality reduction method recently introduced, GPLVM [13], which uses a Gauss-
ian process mapping f (x), partly improves this situation by deﬁning a tunable parame ter xn for
each data point yn . While still prone to local optima, this allows the use of a better initialisation
for {xn }N
n=1 (obtained from a spectral method, see later). This has prompted the application of
GPLVM for tracking human motion [7]. However, GPLVM has some disadvantages: its training is
very costly (each step of the gradient iteration is cubic on the number of training points N , though
approximations based on using few points exist); unlike true LVMs, it deﬁnes neither a posterior
distribution p(x|y) in latent space nor a dimensionality reduction mapping E {x|y}; and the latent
representation it obtains is not ideal. For example, for periodic motions such as running or walking,
repeated periods (identical up to small noise) can be mapped apart from each other in latent space
because nothing constrains xn and xm to be close even when yn = ym (see ﬁg. 3 and [10]).
There exists a different type of dimensionality reduction methods, spectral methods (such as Isomap,
LLE or Laplacian eigenmaps [14]), that have advantages and disadvantages complementary to those
of LVMs. They deﬁne neither mappings nor densities but just a correspondence (xn , yn ) between
points in latent space xn and ambient space yn . However, the training is efﬁcient (a sparse eigen-
value problem) and has no local optima, and often yields a correspondence that successfully models
highly nonlinear, convoluted manifolds such as the Swiss roll. While these attractive properties have
spurred recent research in spectral methods, their lack of mappings and densities has limited their
applicability in people tracking. However, a new model that combines the advantages of LVMs and
spectral methods in a principled way has been recently proposed [1], which we brieﬂy describe next.

2 The Laplacian Eigenmaps Latent Variable Model (LELVM)

LELVM is based on a natural way of deﬁning an out-of-sample ma pping for Laplacian eigenmaps
(LE) which, in addition, results in a density model. In LE, typically we ﬁrst deﬁne a
k-nearest-
neighbour graph on the sample data {yn }N
n=1 and weigh each edge yn ∼ ym by a Gaussian afﬁnity
2 k(yn − ym )/σk2 ). Then the latent points X result from:
function K (yn , ym ) = wnm = exp (− 1
s.t. X ∈ RL×N , XDX⊤ = I, XD1 = 0
(1)
min tr (cid:0)XLX⊤ (cid:1)
where we deﬁne the matrix XL×N = (x1 , . . . , xN ), the symmetric afﬁnity matrix WN ×N , the de-
gree matrix D = diag (PN
n=1 wnm ), the graph Laplacian matrix L = D−W, and 1 = (1, . . . , 1)⊤ .
The constraints eliminate the two trivial solutions X = 0 (by ﬁxing an arbitrary scale) and
x1 = · · · = xN (by removing 1, which is an eigenvector of L associated with a zero eigenvalue).
The solution is given by the leading u2 , . . . , uL+1 eigenvectors of the normalised afﬁnity matrix
N = D− 1
2 WD− 1
2 , namely X = V⊤D− 1
2 with VN ×L = (v2 , . . . , vL+1 ) (an a posteriori trans-
lated, rotated or uniformly scaled X is equally valid).

Following [1], we now deﬁne an out-of-sample mapping F(y) = x for a new point y as a semi-
supervised learning problem, by recomputing the embedding as in (1) (i.e., augmenting the graph
Laplacian with the new point), but keeping the old embedding ﬁxed:
tr (cid:16)( X x ) (cid:16) L
x⊤ (cid:17)(cid:17)
K(y)⊤ 1⊤K(y) (cid:17) (cid:16) X⊤
K(y)
(2)
min
x∈RL
2 k(y − yn )/σk2 ) for n = 1, . . . , N is the kernel induced by
where Kn (y) = K (y, yn ) = exp (− 1
the Gaussian afﬁnity (applied only to the k nearest neighbours of y, i.e., Kn (y) = 0 if y ≁ yn ).
This is one natural way of adding a new point to the embedding by keeping existing embedded
points ﬁxed. We need not use the constraints from (1) because they would trivially determine x, and
the uninteresting solutions X = 0 and X = constant were already removed in the old embedding
anyway. The solution yields an out-of-sample dimensionality reduction mapping x = F(y):
1⊤K(y) = PN
K (y,yn )
x = F(y) = X K(y)
n=1
PN
n′=1 K (y,yn′ )
applicable to any point y (new or old). This mapping is formally identical to a Nadaraya-Watson
estimator (kernel regression; [15]) using as data {(xn , yn )}N
n=1 and the kernel K . We can take this
a step further by deﬁning a LVM that has as joint distribution a kernel density estimate (KDE):
N PN
N PN
N PN
n=1 Ky (y, yn )Kx (x, xn ) p(y) = 1
n=1 Ky (y, yn ) p(x) = 1
p(x, y) = 1
n=1 Kx (x, xn )

(3)

xn

3

where Ky is proportional to K so it integrates to 1, and Kx is a pdf kernel in x –space. Consequently,
the marginals in observed and latent space are also KDEs, and the dimensionality reduction and
reconstruction mappings are given by kernel regression (the conditional means E {y|x}, E {x|y}):
F(y) = PN
f (x) = PN
yn = PN
Kx (x,xn )
n=1 p(n|y)xn
n=1 p(n|x)yn .
n=1
PN
n′=1 Kx (x,xn′ )
spaces:
latent
bandwidths
We
ambient
and
the
in
different
be
to
allow the
2 k(x − xn )/σx k2 ) and Ky (y, yn ) ∝ exp (− 1
2 k(y − yn )/σy k2 ).
Kx (x, xn ) ∝ exp (− 1
They
may be tuned to control the smoothness of the mappings and densities [1].

(4)

Thus, LELVM naturally extends a LE embedding (efﬁciently ob tained as a sparse eigenvalue prob-
lem with a cost O(N 2 )) to global, continuous, differentiable mappings (NW estimators) and po-
tentially multimodal densities having the form of a Gaussian KDE. This allows easy computation
of posterior probabilities such as p(x|y) (unlike GPLVM). It can use a continuous latent space of
arbitrary dimension L (unlike GTM) by simply choosing L eigenvectors in the LE embedding. It
has no local optima since it is based on the LE embedding. LELVM can learn convoluted mappings
(e.g. the Swiss roll) and deﬁne maps and densities for them [1 ]. The only parameters to set are the
graph parameters (number of neighbours k , afﬁnity width σ ) and the smoothing bandwidths σx , σy .

3 Tracking framework

(5)

We follow the sequential Bayesian estimation framework, where for state variables s and observation
variables z we have the recursive prediction and correction equations:
p(st |z0:t−1 ) = R p(st |st−1 ) p(st−1 |z0:t−1 ) dst−1
p(st |z0:t ) ∝ p(zt |st ) p(st |z0:t−1 ).
We deﬁne the state variables as s = (x, d) where x ∈ RL is the low-dim. latent space (for pose)
and d ∈ R3 is the centre-of-mass location of the body (in the experiments our state also includes
the orientation of the body, but for simplicity here we describe only the translation). The observed
variables z consist of image features or the perspective projection of the markers on the camera
plane. The mapping from state to observations is (for the markers’ case, assuming M markers):
f−−−−→ y ∈ R3M −−→ ⊕ P−−−−−→ z ∈ R2M
x ∈ RL
d ∈ R3
where f is the LELVM reconstruction mapping (learnt from mocap data); ⊕ shifts each 3D marker
by d; and P is the perspective projection (pinhole camera), applied to each 3D point separately. Here
we use a simple observation model p(zt |st ): Gaussian with mean given by the transformation (6)
and isotropic covariance (set by the user to control the inﬂu ence of measurements in the tracking).
We assume known correspondences and observations that are obtained either from the 3D markers
(for tracking synthetic data) or 2D tracks obtained from a 2D tracker. Our dynamics model is

(6)

(7)
p(st |st−1 ) ∝ pd (dt |dt−1 ) px (xt |xt−1 ) p(xt )
where both dynamics models for d and x are random walks: Gaussians centred at the previous
step value dt−1 and xt−1 , respectively, with isotropic covariance (set by the user to control the
inﬂuence of dynamics in the tracking); and p(xt ) is the LELVM prior. Thus the overall dynamics
predicts states that are both near the previous state and yield feasible poses. Of course, more complex
dynamics models could be used if e.g. the speed and direction of movement are known.

As tracker we use the Gaussian mixture Sigma-point particle ﬁlter (GMSPPF) [16]. This is a par-
ticle ﬁlter that uses a Gaussian mixture representation for
the posterior distribution in state space
and updates it with a Sigma-point Kalman ﬁlter. This Gaussia n mixture will be used as proposal
distribution to draw the particles. As in other particle ﬁlt er implementations, the prediction step
is carried out by approximating the integral (5) with particles and updating the particles’ weights.
Then, a new Gaussian mixture is ﬁtted with a weighted EM algor ithm to these particles. This re-
places the resampling stage needed by many particle ﬁlters a nd mitigates the problem of sample
depletion while also preventing the number of components in the Gaussian mixture from growing
over time. The choice of this particular tracker is not critical; we use it to illustrate the fact that
LELVM can be introduced in any probabilistic tracker for nonlinear, nongaussian models. Given the
corrected distribution p(st |z0:t ), we choose its mean as recovered state (pose and location). It is also
possible to choose instead the mode closest to the state at t − 1, which could be found by mean-shift
or Newton algorithms [17] since we are using a Gaussian-mixture representation in state space.

4

4 Experiments

We demonstrate our low-dimensional tracker on image sequences of people walking and running,
both synthetic ( ﬁg. 1) and real ( ﬁg. 2–3). Fig. 1 shows the mod
el copes well with persistent partial
occlusion and severely subsampled training data (A,B), and quantitatively evaluates temporal recon-
struction (C). For all our experiments, the LELVM parameters (number of neighbors k , Gaussian
afﬁnity σ , and bandwidths σx and σy ) were set manually. We mainly considered 2D latent spaces
(for pose, plus 6D for rigid motion), which were expressive enough for our experiments. More
complex, higher-dimensional models are straightforward to construct. The initial state distribution
p(s0 ) was chosen a broad Gaussian, the dynamics and observation covariance were set manually to
control the tracking smoothness, and the GMSPPF tracker used a 5-component Gaussian mixture
in latent space (and in the state space of rigid motion) and a small set of 500 particles. The 3D
representation we use is a 102-D vector obtained by concatenating the 3D markers coordinates of all
the body joints. These would be highly unconstrained if estimated independently, but we only use
them as intermediate representation; tracking actually occurs in the latent space, tightly controlled
using the LELVM prior. For the synthetic experiments and some of the real experiments ( ﬁgs. 2–3)
the camera parameters and the body proportions were known (for the latter, we used the 2D outputs
of [6]). For the CMU mocap video ( ﬁg. 2 B) we roughly guessed. We used mocap data from several
sources (CMU, OSU). As observations we always use 2D marker positions, which, depending on
the analyzed sequence were either known (the synthetic case), or provided by an existing tracker
[6] or speci ﬁed manually ( ﬁg. 2 B). Alternatively 2D point trackers similar to the ones of [7] can be
used. The forward generative model is obtained by combining the latent to ambient space mapping
(this provides the position of the 3D markers) with a perspective projection transformation. The
observation model is a product of Gaussians, each measuring the probability of a particular marker
position given its corresponding image point track.
Experiments with synthetic data: we analyze the performance of our tracker in controlled condi-
tions (noise perturbed synthetically generated image tracks) both under regular circumstances (rea-
sonable sampling of training data) and more severe conditions with subsampled training points and
persistent partial occlusion (the man running behind a fence, with many of the 2D marker tracks
obstructed). Fig. 1B,C shows both the posterior ( ﬁltered) latent space distributi on obtained from
our tracker, and its mean (we do not show the distribution of the global rigid body motion; in all
experiments this is tracked with good accuracy). In the latent space plot shown in ﬁg. 1 B, the onset
of running (two cycles were used) appears as a separate region external to the main loop. It does not
appear in the subsampled training set in ﬁg. 1 B, where only one running cycle was used for training
and the onset of running was removed. In each case, one can see that the model is able to track quite
competently, with a modest decrease in its temporal accuracy, shown in ﬁg. 1 C, where the averages
are computed per 3D joint (normalised wrt body height). Subsampling causes some ambiguity in
the estimate, e.g. see the bimodality in the right plot in ﬁg. 1C. In another set of experiments (not
shown) we also tracked using different subsets of 3D markers. The estimates were accurate even
when about 30% of the markers were dropped.
Experiments with real images: this shows our tracker’s ability to work with real motions of differ-
ent people, with different body proportions, not in its latent variable model training set ( ﬁgs. 2–3).
We study walking, running and turns. In all cases, tracking and 3D reconstruction are reasonably ac-
curate. We have also run comparisons against low-dimensional models based on PCA and GPLVM
( ﬁg. 3). It is important to note that, for LELVM, errors in the pose estimates are primarily caused
by mismatches between the mocap data used to learn the LELVM prior and the body proportions of
the person in the video. For example, the body proportions of the OSU motion captured walker are
quite different from those of the image in ﬁg. 2–3 (e.g. note h
ow the legs of the stick man are shorter
relative to the trunk). Likewise, the style of the runner from the OSU data (e.g. the swinging of the
arms) is quite different from that of the video. Finally, the interest points tracked by the 2D tracker
do not entirely correspond either in number or location to the motion capture markers, and are noisy
and sometimes missing. In future work, we plan to include an optimization step to also estimate the
body proportions. This would be complicated for a general, unconstrained model because the di-
mensions of the body couple with the pose, so either one or the other can be changed to improve the
tracking error (the observation likelihood can also become singular). But for dedicated prior pose
educed. The model simply cannot assume
models like ours these difﬁculties should be signi ﬁcantly r
highly unlikely stances —these are either not representable
at all, or have reduced probability—and
thus avoids compensatory, unrealistic body proportion estimates.

5


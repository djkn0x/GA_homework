Learning with Transformation Invariant Kernels

Christian Walder
Max Planck Institute for Biological Cybernetics
72076 T ¨ubingen, Germany
christian.walder@tuebingen.mpg.de

Olivier Chapelle
Yahoo! Research
Santa Clara, CA
chap@yahoo-inc.com

Abstract

This paper considers kernels invariant to translation, rotation and dilation. We
show that no non-trivial positive deﬁnite (p.d.) kernels exist which are radial and
dilation invariant, only conditionally positive deﬁnite (c.p.d.) ones. Accordingly,
we discuss the c.p.d. case and provide some novel analysis, including an elemen-
tary derivation of a c.p.d. representer theorem. On the practical side, we give a
support vector machine (s.v.m.) algorithm for arbitrary c.p.d. kernels. For the thin-
plate kernel this leads to a classiﬁer with only one parameter (the amount of regu-
larisation), which we demonstrate to be as effective as an s.v.m. with the Gaussian
kernel, even though the Gaussian involves a second parameter (the length scale).

1 Introduction

Recent years have seen widespread application of reproducing kernel Hilbert space (r.k.h.s.) based
methods to machine learning problems (Sch ¨olkopf & Smola, 2002). As a result, kernel methods
have been analysed to considerable depth. In spite of this, the aspects which we presently investigate
seem to have received insufﬁcient attention, at least within the machine learning community.
The ﬁrst is transformation invariance of the kernel, a topic touched on in (Fleuret & Sahbi, 2003).
Note we do not mean by this the local invariance (or insensitivity) of an algorithm to application
speciﬁc transformations which should not affect the class label, such as one pixel image translations
(see e.g. (Chapelle & Sch ¨olkopf, 2001)). Rather we are referring to global invariance to transforma-
tions, in the way that radial kernels (i.e. those of the form k(x, y) = φ(kx − yk)) are invariant to
translations. In Sections 2 and 3 we introduce the more general concept of transformation scaled-
ness, focusing on translation, dilation and orthonormal transformations. An interesting result is that
there exist no non-trivial p.d. kernel functions which are radial and dilation scaled.
There do exist non-trivial c.p.d. kernels with the stated invariances however. Motivated by this,
we analyse the c.p.d. case in Section 4, giving novel elementary derivations of some key results,
most notably a c.p.d. representer theorem. We then give in Section 6.1 an algorithm for applying
the s.v.m. with arbitrary c.p.d. kernel functions. It turns out that this is rather useful in practice,
for the following reason. Due to its invariances, the c.p.d. thin-plate kernel which we discuss in
Section 5, is not only richly non-linear, but enjoys a duality between the length-scale parameter
and the regularisation parameter of Tikhonov regularised solutions such as the s.v.m. In Section
7 we compare the resulting classiﬁer (which has only a regularisation parameter), to that of the
s.v.m. with Gaussian kernel (which has an additional length scale parameter). The results show that
the two algorithms perform roughly as well as one another on a wide range of standard machine
learning problems, notwithstanding the new method’s advantage in having only one free parameter.
In Section 8 we make some concluding remarks.

1

◦ T

f ∈F Θ(f ) =
arg min

2 Transformation Scaled Spaces and Tikhonov Regularisation
Deﬁnition 2.1. Let T be a bijection on X and F a Hilbert space of functions on some non-empty
set X such that f 7→ f ◦ T is a bijection on F . F is T -scaled if
hf , giF = gT (F ) hf ◦ T , g ◦ T iF
(1)
for all f ∈ F , where gT (F ) ∈ R+ is the norm scaling function associated with the operation of T
on F . If gT (F ) = 1 we say that F is T -invariant.
The following clariﬁes the behaviour of Tikhonov regularised solutions in such spaces.
(cid:18)
(cid:19)
Lemma 2.2. For any Θ : F −−−→ R and T such that f 7→ f ◦ T is a bijection of F , if the left hand
side is unique then
fT ∈F Θ(fT ◦ T )
arg min
T = arg minfT ∈F Θ(fT ◦ T ). By deﬁnition we have
Proof. Let f ∗ = arg minf ∈F Θ(f ) and f ∗
that ∀g ∈ F , Θ(f ∗
T ◦ T ) ≤ Θ(g ◦ T ). But since f 7→ f ◦ T is a bijection on F , we also have
∀g ∈ F , Θ(f ∗
T ◦ T ) ≤ Θ(g). Hence, given the uniqueness, this implies f ∗ = f ∗
T ◦ T .
The following Corollary follows immediately from Lemma 2.2 and Deﬁnition 2.1.
(cid:18)
(cid:19)
(cid:18)
(cid:17)(cid:19)
(cid:16) kf k2F /gT (F ) + X
kf k2F + X
Corollary 2.3. Let Li be any loss function. If F is T -scaled and the left hand side is unique then
Li (f (T xi ))
◦ T .
i
i
Corollary 2.3 includes various learning algorithms for various choices of Li — for example the
s.v.m. with linear hinge loss for Li (t) = max (0, 1 − yi t), and kernel ridge regression for Li (t) =
(yi − t)2 . Let us now introduce the speciﬁc transformations we will be considering.
Deﬁnition 2.4. Let Ws , Ta and OA be the dilation, translation and orthonormal transformations
Rd → Rd deﬁned for s ∈ R \ {0}, a ∈ Rd and orthonormal A : Rd → Rd by Wsx = sx,
Tax = x + a and OAx = Ax respectively.
Hence, for an r.k.h.s. which is Ws -scaled for arbitrary s 6= 0, training an s.v.m. and dilating the
resultant decision function by some amount is equivalent training the s.v.m. on similarly dilated
input patterns but with a regularisation parameter adjusted according to Corollary 2.3.
While (Fleuret & Sahbi, 2003) demonstrated this phenomenon for the s.v.m. with a particular kernel,
as we have just seen it is easy to demonstrate for the more general Tikhonov regularisation setting
with any function norm satisfying our deﬁnition of transformation scaledness.

Li (f (xi ))

arg min
f ∈F

=

arg min
f ∈F

3 Transformation Scaled Reproducing Kernel Hilbert Spaces

is T -scaled iff

We now derive the necessary and sufﬁcient conditions for a reproducing kernel (r.k.) to correspond
to an r.k.h.s. which is T -scaled. The relationship between T -scaled r.k.h.s.’s and their r.k.’s is easy
to derive given the uniqueness of the r.k. (Wendland, 2004). It is given by the following novel
Lemma 3.1 (Transformation scaled r.k.h.s.). The r.k.h.s. H with r.k. k : X × X → R, i.e. with k
satisfying
hk(·, x), f (·)iH = f (x),
k(x, y) = gT (H) k(T x, T y).
Which we prove in the accompanying technical report (Walder & Chapelle, 2007) . It is now easy
to see that, for example, the homogeneous polynomial kernel k(x, y) = hx, yip corresponds to a
Ws -scaled r.k.h.s. H with gWs (H) = hx, yip / hsx, syip = s−2p . Hence when the homogeneous
polynomial kernel is used with the hard-margin s.v.m. algorithm, the result is invariant to multiplica-
tive scaling of the training and test data. If the soft-margin s.v.m. is used however, then the invariance

(2)

(3)

2

holds only under appropriate scaling (as per Corollary 2.3) of the margin softness parameter (i.e. λ
of the later equation (14)).
We can now show that there exist no non-trivial r.k.h.s.’s with radial kernels that are also Ws -scaled
for all s 6= 0. First however we need the following standard result on homogeneous functions:
Lemma 3.2. If φ : [0, ∞) → R and g : (0, ∞) → R satisfy φ(r) = g(s)φ(rs) for all r ≥ 0 and
s > 0 then φ(r) = aδ(r) + brp and g(s) = s−p , where a, b, p ∈ R, p 6= 0, and δ is Dirac’s function.
Which we prove in the accompanying technical report (Walder & Chapelle, 2007). Now, suppose
that H is an r.k.h.s. with r.k. k on Rd × Rd . If H is Ta -invariant for all a ∈ Rd then
k(x, y) = k(T−y x, T−y y) = k(x − y , 0) , φT (x − y).
If in addition to this H is OA -invariant for all orthogonal A, then by choosing A such that A(x−y) =
kx − yk ˆe where ˆe is an arbitrary unit vector in Rd we have
k(x, y) = k(OAx, OAy) = φT (OA (x − y)) = φT (kx − yk ˆe) , φOT (kx − yk)
i.e. k is radial. All of this is straightforward, and a similar analysis can be found in (Wendland,
2004). Indeed the widely used Gaussian kernel satisﬁes both of the above invariances. But if we
now also assume that H is Ws -scaled for all s 6= 0 — this time with arbitrary gWs (H) — then
k(x, y) = gWs (H)k(Wsx, Wsy) = gW|s| (H)φOT (|s| kx − yk)
so that letting r = kx − yk we have that φOT (r) = gW|s| (H)φOT (|s| r) and hence by Lemma 3.2
that φOT (r) = aδ(r) + brp where a, b, p ∈ R. This is positive semi-deﬁnite for the trivial case
p = 0, but there are various ways of showing this cannot be non-trivially positive semi-deﬁnite for
(cid:18) a
(cid:19)
p 6= 0. One simple way is to consider two arbitrary vectors x1 and x2 such that kx1 − x2 k = d > 0.
For the corresponding Gram matrix
bdp
K ,
,
bdp
a
to be positive semi deﬁnite we require 0 ≤ det(K ) = a2 − b2d2p , but for arbitrary d > 0 and
a < ∞, this implies b = 0. This may seem disappointing, but fortunately there do exist c.p.d. kernel
functions with the stated properties, such as the thin-plate kernel. We discuss this case in detail in
Section 5, after the following particularly elementary and in part novel introduction to c.p.d. kernels.

4 Conditionally Positive Deﬁnite Kernels
In the last Section we alluded to c.p.d. kernel functions – these are given by the following
α ∈ Rm \ {0} satisfying Pm
Deﬁnition 4.1. A continuous function φ : X × X → R is conditionally positive deﬁnite with
respect to (w.r.t.) the linear space of functions P if, for all m ∈ N, all {xi }i=1...m ⊂ X , and all
Pm
j=1 αj p(xj ) = 0 for all p ∈ P , the following holds
j,k=1 αj αk φ(xj , xk ) > 0.
Due to the positivity condition (4) — as opposed one of non negativity — we are referring to c.p.d.
rather than conditionally positive semi-deﬁnite kernels. The c.p.d. case is more technical than the
p.d. case. We provide a minimalistic discussion here — for more details we recommend e.g. (Wend-
land, 2004). To avoid confusion, let us note in passing that while the above deﬁnition is quite stan-
dard (see e.g. (Wendland, 2004; Wahba, 1990)), many authors in the machine learning community
use a deﬁnition of c.p.d. kernels which corresponds to our deﬁnition when P = {1} (e.g. (Sch ¨olkopf
& Smola, 2002)) or when P is taken to be the space of polynomials of some ﬁxed maximum degree
{α ∈ Rm : Pm
(e.g. (Smola et al., 1998)). Let us now adopt the notation P ⊥ (x1 , . . . , xm ) for the set
i=1 αip(xi ) = 0 for all p ∈ P } .
The c.p.d. kernels of Deﬁnition 4.1 naturally deﬁne a Hilbert space of functions as per
Deﬁnition 4.2. Let φ : X × X → R be a c.p.d. kernel w.r.t. P . We deﬁne Fφ (X ) to be the Hilbert
nPm
o
space of functions which is the completion of the set
j=1 αj φ(·, xj ) : m ∈ N, x1 , .., xm ∈ X , α ∈ P ⊥ (x1 , .., xm )
E
DPm
Pn
= Pm
j=1 αj φ(·, xj ), Pn
which due to the deﬁnition of φ we may endow with the inner product
k=1 βk φ(·, yk )
k=1 αj βk φ(xj , yk ).
j=1

(4)

(5)

,

Fφ (X )

3

Note that φ is not the r.k. of Fφ (X ) — in general φ(x, ·) does not even lie in Fφ (X ). For the
remainder of this Section we develop a c.p.d. analog of the representer theorem. We begin with
Lemma 4.3. Let φ : X × X → R be a c.p.d. kernel w.r.t. P and p1 , . . . pr a basis for P .
j=1 αj φ(·, xj ) ∈ Fφ (X ) and sP = Pr
Pm
For any {(x1 , y1 ), . . . (xm , ym )} ⊂ X × R, there exists an s = sFφ (X ) + sP where sFφ (X ) =
k=1 βk pk ∈ P , such that s(xi ) = yi , i = 1 . . . m.
A simple and elementary proof (which shows (17) is solvable when λ = 0), is given in (Wendland,
2004) and reproduced in the accompanying technical report (Walder & Chapelle, 2007). Note that
although such an interpolating function s always exists, it need not be unique. The distinguishing
property of the interpolating function is that the norm of the part which lies in Fφ (X ) is minimum.
Deﬁnition 4.4. Let φ : X × X → R be a c.p.d. kernel w.r.t. P . We use the notation Pφ (P ) to denote
Note that Fφ (X ) ⊕ Pφ (P ) is a direct sum since p = Pm
the projection Fφ (X ) ⊕ P → Fφ (X ).
j=1 βiβj φ(zi , zj ) = Pm
Fφ (X ) = hp, piFφ (X ) = Pm
Pn
j=1 βiφ(zj , ·) ∈ P ∩ Fφ (X ) implies
kpk2
j=1 βj p(zj ) = 0.
i=1
Hence, returning to the main thread, we have the following lemma — our proof of which seems to
P . Consider an arbitrary function s = sFφ (X ) + sP with sFφ (X ) = Pm
be novel and particularly elementary.
Lemma 4.5. Denote by φ : X × X → R a c.p.d. kernel w.r.t. P and by p1 , . . . pr a basis for
and sP = Pr
j=1 αj φ(·, xj ) ∈ Fφ (X )
k=1 βk pk ∈ P . kPφ (P )skFφ (X ) ≤ kPφ (P )f kFφ (X ) holds for all f ∈ Fφ (X ) ⊕ P
satisfying
f (xi ) = s(xi ), i = 1 . . . m.
(6)
Proof. Let f be an arbitrary element of Fφ (X ) ⊕ P . We can always write f as
rX
nX
mX
j=1
k=1
l=1
If we deﬁne1 [Px ]i,j = pj (xi ), [Pz ]i,j = pj (zi ), [Φxx ]i,j = φ(xi , xj ), [Φxz ]i,j = φ(xi , zj ), and
[Φzx ]i,j = φ(zi , xj ), then the condition (6) can hence be written
Pxβ = Φxxα + Φxz b + Pxc,
(7)
and the deﬁnition of Fφ (X ) requires that e.g. α ∈ P ⊥ (x1 , . . . , xm ), hence implying the constraints
x (α + α) + P >
P >
x α = 0 and P >
z b = 0.
(8)
(cid:18)α + α
(cid:19)
(cid:19)> (cid:18)Φxx Φxz
(cid:18)α + α
(cid:19)
The inequality to be demonstrated is then
|
{z
}
Φzx Φzz
b
b
,Φ
(cid:18)α
(cid:19)>
(cid:19)>
(cid:18)α
(cid:19)
(cid:18)α
(cid:18)α
(cid:19)
{z
}
|
}
{z
|
{z
}
|
R = α>Φxxα
Φ
+ 2
Φ
+
,
0
b
b
b
z β = 0, and since Φ is c.p.d. w.r.t. (cid:0)P >
(cid:1) that ∆1 ≥ 0. But
=L
,∆1
,∆2
it follows from (8) that P >
x α + P >
z }| {
(7) and (8) imply that L ≤ R, since
x
=0
α>Px (β − c) − α>Φxz b + α>Φxz b = 0.
∆2 = α>Φxxα + α>Φxz b =

(αi + αi ) φ(·, xj ) +

L , α>Φxxα ≤

blφ(·, zl ) +

ck pk .

By expanding

f =

, R.

(9)

P >
z

1Square brackets w/ subscripts denote matrix elements, and colons denote entire rows or columns.

4

(10)

Using these results it is now easy to prove an analog of the representer theorem for the p.d. case.
Theorem 4.6 (Representer theorem for the c.p.d. case). Denote by φ : X × X → R a c.p.d. kernel
w.r.t. P , by Ω a strictly monotonic increasing real-valued function on [0, ∞), and by c : Rm →
(cid:17)
(cid:16)kPφ (P )f k2
R ∪ {∞} an arbitrary cost function. There exists a minimiser over Fφ (X ) ⊕ P of
which admits the form Pm
W (f ) , c (f (x1 ), . . . , f (xm )) + Ω
Fφ (X )
i=1 αiφ(·, xi ) + p, where p ∈ P .
Proof. Let f be a minimiser of W. Let s = Pm
i=1 αiφ(·, xi ) + p satisfy s(xi ) = f (xi ), i =
Fφ (X ) ≥
1 . . . m. By Lemma 4.3 we know that such an s exists. But by Lemma 4.5 kPφ (P )sk2
Fφ (X ) . As a result, W (s) ≤ W (f ) and s is a minimizer of W with the correct form.
kPφ (P )f k2
5 Thin-Plate Regulariser
(
Deﬁnition 5.1. The m-th order thin-plate kernel φm : Rd × Rd → R is given by
(−1)m−(d−2)/2 kx − yk2m−d log(kx − yk)
if d ∈ 2N,
φm (x, y) =
(11)
(−1)m−(d−1)/2 kx − yk2m−d
if d ∈ (2N − 1),
(cid:0)Rd (cid:1) of
for x 6= y , and zero otherwise. φm is c.p.d. with respect to πm−1 (Rd ), the set of d-variate polyno-
mials of degree at most m − 1. The kernel induces the following norm on the space Fφm
Deﬁnition 4.2 (this is not obvious — see e.g. (Wendland, 2004; Wahba, 1990))
(cid:18) ∂
(cid:19) (cid:18) ∂
(cid:19)
Z ∞
Z ∞
hf , giFφm (Rd )
, hψf , ψgiL2 (Rd )
dX
dX
· · ·
· · ·
· · ·
· · ·
∂
∂
(cid:0)Rd (cid:1) → L2 (Rd ) is a regularisation operator, implicitly deﬁned above.
dx1 . . . dxd ,
g
f
xd=−∞
x1=−∞
∂xim
∂xi1
∂xim
∂xi1
i1=1
im=1
(cid:0)Rd (cid:1)) = gTa (Fφm
(cid:0)Rd (cid:1)) = 1. Moreover, from the chain rule we have
where ψ : Fφm
(cid:19)
(cid:18) ∂
Clearly gOA (Fφm
· · ·
(f ◦ Ws ) = sm
· · ·
◦ Ws
∂
∂
∂
f
∂xi1
∂xim
∂xi1
∂xim
and therefore since hf , giL2 (Rd ) = sd hf ◦ Ws , g ◦ Ws iL2 (Rd ) ,we can immediately write
(cid:0)Rd (cid:1)) = s−(2m−d) . Note that although it may appear that this can be shown more
hψ (f ◦ Ws ) , ψ (g ◦ Ws )iL2 (Rd ) = s2m h(ψf ) ◦ Ws , (ψg) ◦ Ws iL2 (Rd ) = s2m−d hψf , ψgiL2 (Rd )
(13)
so that gWs (Fφm
easily using (11) and an argument similar to Lemma 3.1, the process is actually more involved due
to the log factor in the ﬁrst case of (11), and it is necessary to use the fact that the kernel is c.p.d.
w.r.t. πm−1 (Rd ). Since this is redundant and not central to the paper we omit the details.

(12)

=

6 Conditionally Positive Deﬁnite s.v.m.

In the Section 3 we showed that non-trivial kernels which are both radial and dilation scaled cannot
be p.d. but rather only c.p.d. It is therefore somewhat surprising that the s.v.m. — one of the most
widely used kernel algorithms — has been applied only with p.d. kernels, or kernels which are
c.p.d. respect only to P = {1} (see e.g. (Boughorbel et al., 2005)). After all, it seems interesting
to construct a classiﬁer independent not only of the absolute positions of the input data, but also of
their absolute multiplicative scale.
Hence we propose using the thin-plate kernel with the s.v.m. by minimising the s.v.m. objective over
the space Fφ (X ) ⊕ P (or in some cases just over Fφ (X ), as we shall see in Section 6.1). For this
we require somewhat non-standard s.v.m. optimisation software. The method we propose seems
simpler and more robust than previously mentioned solutions. For example, (Smola et al., 1998)
mentions the numerical instabilities which may arise with the direct application of standard solvers.

5

Dataset Gaussian
Thin-Plate
dim/n
10.567 (0.547) 10.667 (0.586) 2/3000∗
banana
26.574 (2.259) 28.026 (2.900) 9/263
breast
diabetes 23.578 (0.989) 23.452 (1.215) 8/768
36.143 (0.969) 38.190 (2.317) 9/144
ﬂare
german 24.700 (1.453) 24.800 (1.373) 20/1000
17.407 (2.142) 17.037 (2.290) 13/270
heart

Dataset Gaussian
Thin-Plate
dim/n
3.210 (0.504) 1.867 (0.338) 18/2086
image
ringnm 1.533 (0.229) 1.833 (0.200) 20/3000∗
8.931 (0.640) 8.651 (0.433) 60/2844
splice
thyroid 4.199 (1.087) 3.247 (1.211) 5/215
twonm 1.833 (0.194) 1.867 (0.254) 20/3000∗
wavefm 8.333 (0.378) 8.233 (0.484) 21/3000

Table 1: Comparison of Gaussian and thin-plate kernel with the s.v.m. on the UCI data sets. Results
are reported as “mean % classiﬁcation error (standard error)”. dim is the input dimension and n
the total number of data points. A star in the n column means that more examples were available
but we kept only a maximum of 2000 per class in order to reduce the computational burden of the
extensive number of cross validation and model selection training runs (see Section 7). None of the
data sets were linearly separable so we always used used the normal (β unconstrained) version of
the optimisation described in Section 6.1.

6.1 Optimising an s.v.m. with c.p.d. Kernel

It is simple to implement an s.v.m. with a kernel φ which is c.p.d. w.r.t. an arbitrary ﬁnite dimensional
space of functions P by extending the primal optimisation approach of (Chapelle, 2007) to the c.p.d.
nX
case. The quadratic loss s.v.m. solution can be formulated as arg minf ∈Fφ (X )⊕P of
max(0, 1 − yi f (xi ))2 ,
λ kPφ (P )f k2
Fφ (X ) +
i=1
Note that for the second order thin-plate case we have X = Rd and P = π1 (Rd ) (the space of
constant and ﬁrst order polynomials). Hence dim (P ) = d + 1 and we can take the basis to be
pj (x) = [x]j for j = 1 . . . d along with pd+1 = 1.
is given by fsvm (x) = Pn
i=1 αiφ(xi , x) + Pdim(P )
It follows immediately from Theorem 4.6 that, letting p1 , p2 , . . . pdim(P ) span P , the solution to (14)
βj pj (x). Now, if we consider only the margin
j=1
violators — those vectors which (at a given step of the optimisation process) satisfy yi f (xi ) < 1,
we can replace the max (0, ·) in (14) with (·). This is equivalent to making a local second order
approximation. Hence by repeatedly solving in this way while updating the set of margin violators,
nX
we will have implemented a so-called Newton optimisation. Now, since
i,j=1

kPφ (P )fsvm k2
Fφ (X ) =

αiαj φ(xi , xj ),

(14)

(15)

the local approximation of the problem is, in α and β
minimise λα>Φα + kΦα + P β − yk2 , subject to P >α = 0,
(16)
where [Φ]i,j = φ(xi , xj ), [P ]j,k = pk (xj ), and we assumed for simplicity that all vectors violate
(cid:19)
(cid:19)−1 (cid:18)y
(cid:18)λI + Φ P >
(cid:18)α
(cid:19)
the margin. The solution in this case is given by (Wahba, 1990)
0
0
β
P
In practice it is essential that one makes a change of variable for β in order to avoid the numerical
problems which arise when P is rank deﬁcient or numerically close to it. In particular we make the
QR factorisation (Golub & Van Loan, 1996) P > = QR, where Q>Q = I and R is square. We then
solve for α and β = Rβ . As a ﬁnal step at the end of the optimisation process, we take the minimum
norm solution of the system β = Rβ , β = R#β where R# is the pseudo inverse of R. Note that
although (17) is standard for squared loss regression models with c.p.d. kernels, our use of it in
optimising the s.v.m. is new. The precise algorithm is given in (Walder & Chapelle, 2007), where
we also detail two efﬁcient factorisation techniques, speciﬁc to the new s.v.m. setting. Moreover, the
method we present in Section 6.2 deviates considerably further from the existing literature.

(17)

=

.

6

6.2 Constraining β = 0
Previously, if the data can be separated with only the P part of the function space — i.e. with α = 0
— then the algorithm will always do so regardless of λ. This is correct in that, since P lies in the null
space of the regulariser kPφ (P )·k2
Fφ (X ) , such solutions minimise (14), but may be undesirable for
various reasons. Firstly, the regularisation cannot be controlled via λ. Secondly, for the thin-plate,
P = π1 (Rd ) and the solutions are simple linear separating hyperplanes. Finally, there may exist
inﬁnitely many solutions to (14). It is unclear how to deal with this problem — after all it implies
that the regulariser is simply inappropriate for the problem at hand. Nonetheless we still wish to
apply a (non-linear) algorithm with the previously discussed invariances of the thin-plate.
To achieve this, we minimise (14) as before, but over the space Fφ (X ) rather than Fφ (X ) ⊕ P . It
is important to note that by doing so we can no longer invoke Theorem 4.6, the representer theorem
for the c.p.d. case. This is because the solvability argument of Lemma 4.3 no longer holds. Hence
we do not know the optimal basis for the function, which may involve inﬁnitely many φ(·, x) terms.
The way we deal with this is simple — instead of minimising over Fφ (X ) we consider only the
nPn
o
ﬁnite dimensional subspace given by
j=1 αj φ(·, xj ) : α ∈ P ⊥ (x1 , . . . , xn )
where x1 , . . . xn are those of the original problem (14). The required update equation can be ac-
(cid:1) P⊥ (cid:1)−1
⊥ (cid:0)λΦ + Φ>
α = −P⊥ (cid:0)P >
quired in a similar manner as before. The closed form solution to the constrained quadratic pro-
gramme is in this case given by (see (Walder & Chapelle, 2007))
⊥ Φ>
P >
sxΦsx
(18)
sxys
where Φsx = [Φ]s,: , s is the current set of margin violators and P⊥ the null space of P satisfying
P P⊥ = 0. The precise algorithm we use to optimise in this manner is given in the accompanying
technical report (Walder & Chapelle, 2007), where we also detail efﬁcient factorisation techniques.

,

7 Experiments and Discussion

We now investigate the behaviour of the algorithms which we have just discussed, namely the thin-
plate based s.v.m. with 1) the optimisation over Fφ (X ) ⊕ P as per Section 6.1, and 2) the optimi-
(cid:16)− kx − yk2 /(2σ2 )
(cid:17)
sation over a subspace of Fφ (X ) as per Section 6.2. In particular, we use the second method if the
data is linearly separable, otherwise we use the ﬁrst. For a baseline we take the Gaussian kernel
k(x, y) = exp
, and compare on real world classiﬁcation problems.
Binary classiﬁcation (UCI data sets). Table 1 provides numerical evidence supporting our claim
that the thin-plate method is competitive with the Gaussian, in spite of it’s having one less hyper
parameter. The data sets are standard ones from the UCI machine learning repository. The experi-
ments are extensive — the experiments on binary problems alone includes all of the data sets used in
(Mika et al., 2003) plus two additional ones (twonorm and splice). To compute each error measure,
we used ﬁve splits of the data and tested on each split after training on the remainder. For parameter
selection, we performed ﬁve fold cross validation on the four-ﬁfths of the data available for training
each split, over an exhaustive search of the algorithm parameter(s) (σ and λ for the Gaussian and
happily just λ for the thin-plate). We then take the parameter(s) with lowest mean error and retrain
on the entire four ﬁfths. We ensured that the chosen parameters were well within the searched range
by visually inspecting the cross validation error as a function of the parameters. Happily, for the
thin-plate we needed to cross validate to choose only the regularisation parameter λ, whereas for the
Gaussian we had to choose both λ and the scale parameter σ . The discovery of an equally effec-
tive algorithm which has only one parameter is important, since the Gaussian is probably the most
popular and effective kernel used with the s.v.m. (Hsu et al., 2003).
Multi class classiﬁcation (USPS data set). We also experimented with the 256 dimensional, ten
class USPS digit recognition problem. For each of the ten one vs. the rest models we used ﬁve fold
cross validation on the 7291 training examples to ﬁnd the parameters, retrained on the full training
set, and labeled the 2007 test examples according to the binary classiﬁer with maximum output. The
Gaussian misclassiﬁed 88 digits (4.38%), and the thin-plate 85 (4.25%). Hence the Gaussian did not
perform signiﬁcantly better, in spite of the extra parameter.

7

Computational complexity.
The normal computational complexity of the c.p.d. s.v.m. algo-
rithm is the usual O(nsv
3 ) — cubic in the number of margin violators. For the β = 0 variant
(necessary only on linearly separable problems — presently only the USPS set) however, the cost
is O(nb
2nsv + nb
3 ), where nb is the number of basis functions in the expansion. For our USPS
experiments we expanded on all m training points, but if nsv (cid:28) m this is inefﬁcient and proba-
bly unnecessary. For example the ﬁnal ten models (those with optimal parameters) of the USPS
problem had around 5% margin violators, and so training each Gaussian s.v.m. took only ∼ 40s in
comparison to ∼ 17 minutes (with the use of various efﬁcient factorisation techniques as detailed
in the accompanying (Walder & Chapelle, 2007) ) for the thin-plate. By expanding on only 1500
randomly chosen points however, the training time was reduced to ∼ 4 minutes while incurring only
88 errors — the same as the Gaussian. Given that for the thin-plate cross validation needs to be
performed over one less parameter, even in this most unfavourable scenario of nsv (cid:28) m, the overall
times of the algorithms are comparable. Moreover, during cross validation one typically encoun-
ters larger numbers of violators for some suboptimal parameter conﬁgurations, in which cases the
Gaussian and thin-plate training times are comparable.

8 Conclusion

We have proven that there exist no non-trivial radial p.d. kernels which are dilation invariant (or
more accurately, dilation scaled), but rather only c.p.d. ones. Such kernels have the advantage that,
to take the s.v.m. as an example, varying the absolute multiplicative scale (or length scale) of the data
has the same effect as changing the regularisation parameter — hence one needs model selection to
chose only one of these, in contrast to the widely used Gaussian kernel for example.
Motivated by this advantage we provide a new, efﬁcient and stable algorithm for the s.v.m. with
arbitrary c.p.d. kernels. Importantly, our experiments show that the performance of the algorithm
nonetheless matches that of the Gaussian on real world problems.
The c.p.d. case has received relatively little attention in machine learning. Our results indicate that
it is time to redress the balance. Accordingly we provided a compact introduction to the topic,
including some novel analysis which includes an new, elementary and self contained derivation of
one particularly important result for the machine learning community, the representer theorem.

References
Boughorbel, S., Tarel, J.-P., & Boujemaa, N. (2005). Conditionally positive deﬁnite kernels for svm based
image recognition. Proc. of IEEE ICME’05. Amsterdam.
Chapelle, O. (2007). Training a support vector machine in the primal. Neural Computation, 19, 1155–1178.
Chapelle, O., & Sch ¨olkopf, B. (2001).
In
Incorporating invariances in nonlinear support vector machines.
T. Dietterich, S. Becker and Z. Ghahramani (Eds.), Advances in neural information processing systems 14,
609–616. Cambridge, MA: MIT Press.
Fleuret, F., & Sahbi, H. (2003). Scale-invariance of support vector machines based on the triangular kernel.
Proc. of ICCV SCTV Workshop.
Golub, G. H., & Van Loan, C. F. (1996). Matrix computations. Baltimore MD: The Johns Hopkins University
Press. 2nd edition.
Hsu, C.-W., Chang, C.-C., & Lin, C.-J. (2003). A practical guide to support vector classiﬁcation (Technical
Report). National Taiwan University.
Mika, S., R ¨atsch, G., Weston, J., Sch ¨olkopf, B., Smola, A., & M ¨uller, K.-R. (2003). Constructing descriptive
and discriminative non-linear features: Rayleigh coefﬁcients in feature spaces. IEEE PAMI, 25, 623–628.
Sch ¨olkopf, B., & Smola, A. J. (2002). Learning with kernels: Support vector machines, regularization, opti-
mization, and beyond. Cambridge: MIT Press.
Smola, A., Sch ¨olkopf, B., & M ¨uller, K.-R. (1998). The connection between regularization operators and support
vector kernels. Neural Networks, 11, 637–649.
Wahba, G. (1990). Spline models for observational data. Philadelphia: Series in Applied Math., Vol. 59, SIAM.
Walder, C., & Chapelle, O. (2007). Learning with transformation invariant kernels (Technical Report 165).
Max Planck Institute for Biological Cybernetics, Department of Empirical Inference, T ¨ubingen, Germany.
Wendland, H. (2004). Scattered data approximation. Monographs on Applied and Computational Mathematics.
Cambridge University Press.

8


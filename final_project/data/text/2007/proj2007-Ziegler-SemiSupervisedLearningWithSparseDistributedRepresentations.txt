Semi-Supervised Learning with Sparse Distributed Representations

David Ziegler
dziegler@stanford.edu
CS 229 Final Pro ject

1 Introduction

For many machine learning applications, labeled data
may be very diﬃcult or costly to obtain. For instance
in the case of speech analysis, the average annotation
time for a one hour telephone conversation transcript
is 400 hours.[7] To circumvent this problem, one can
use semi-supervised learning algorithms which utilize
unlabeled data to improve performance on a super-
vised learning task. Since unlabeled data is typically
much easier to obtain, this can be an attractive ap-
proach.

In this paper, we combine semi-supervised learning
with two types of sparse distributed representations:
local and global.
In a local sparse distributed rep-
resentation we attempt to ﬁnd local sparse features,
while in a global sparse distributed representation we
attempt to represent our test set using a sparse com-
bination of the training set. In both cases, sparsity
turns out to have desirable properties which we can
leverage in semi-supervised learning.

2 Local Sparse Distributed Represen-
tations

Sparse coding is very well suited toward the extraction
local sparse features. The goal of sparse coding is
to represent input vectors as a sparse approximate
weighted linear combination of basis vectors. That is,
for input vector y ∈ Rk ,
y ≈ !j
where b1 , · · · , bn ∈ Rk and s ∈ Rn is a sparse vector of
coeﬃcients. Unlike similar methods such as PCA, the
basis set B founded in sparse coding can be overcom-
plete (n > k), and can represent nonlinear features of
x. To ﬁnd the optimal B and s we solve the following

bj sj = B s

(1)

optimization problem as formulated by [3]:
||x(i) − !j
minimizeb,a !i
a(i)
2 + β ||a(i) ||1
j bj ||2
||bj ||2 ≤ 1, ∀j ∈ 1, · · · , s
s.t

(2)

(3)

3 The Structural Self-Taught Learning
Algorithm

Traditionally, semi-supervised algorithms have as-
sumed that the unlabeled data and the labeled data
are drawn from the same distribution. A recent
semi-supervised framework proposed by Raina et al[4]
called self-taught learning abandons this assumption,
and only requires that the unlabeled data be within
the same problem domain, or modality. The approach
taken by the authors in [4] is to learn a set of bases on
the unlabeled data via sparse coding, and then repre-
sent the labeled data as a sparse linear combination
of the learned bases. This allows them to map their
labeled data into a higher level represenation, which
they can then perform supervised learning on. The
algorithm is summarized below. This algorithm falls

Algorithm 1 Self-taught Learning via Sparse Coding
=
training
Labeled
set
1: Input:
T
), · · · , (x(m)
(x(1)
, y (1)
, y (m)
). Unlabeled
data
l
l
l
l
u , · · · , x(k)
x(1)
u .
2: Using unlabeled data x(i)
u , solve the optimization
problem (1) to obtain bases B . Compute fea-
tures for the classiﬁcation task to obtain a new
labeled training set ˆT = {(ˆa(x(i)
i=1 , where
l ), y (i) )}m
ˆa(x(i)
, y (i) ) = argmina(i) ||x(i)
l − sumj a(i)
2 +
j bj ||2
l
β ||a(i) ||1 . Learn a classiﬁer C by applying a su-
pervised learning algorithm to the labeled train-
ing set ˆT .
3: Ouput: Learned classiﬁer for the classiﬁcation
task.

under the more general case of self-taught learning

algorithms which we call structural self-taught learn-
ing. In structural self-taught learning, we attempt to
extract an abstract underlying structure from the un-
labeled data, use this underlying structure to pro ject
our labeled data into a higher level represenation, and
then perform supervised learning on the labeled data.
This approach to self-taught learning opens up two
important questions which we will attempt to address:

1. How does the choice of our structural mapping
function aﬀect performance on our supervised learn-
ing task (in this case, sparse coding)?

2. How does the similarity between the labeled and
unlabeled data aﬀect performance on our supervised
learning task?

While there has been good theoretical work done
recently in semi-supervised learning[1] and transfer
learning[2], none of these approaches are applicable
to the case of self-taught learning, where we do not
assume anything about the distribution of the unla-
beled data.
We deﬁne the compatibility function χ : Du × Dl ×
f → [0, 1] relating the compatability between unla-
beled distribution Du , labeled distribution Dl , and
structural mapping function f , as χ(Du , Dl , f ) =
1 − Exl∈Dl [χ(xu , xl , f )]. In the case of sparse coding,
χ(xu , xl , a) = 1 − E [xl − B ˆa(xl )]
(4)
Essentially, we measure the compatibility between an
unlabeled distribution, a labeled distribution, and a
mapping function, by the reconstruction error with
respect to the the optimal basis found over the unla-
beled data.

4 MNIST Experiments

For our experiments with self-taught learning, we
used the MNIST handwritten digits dataset for la-
beled data. Unlabeled data was taken from three
sources: natural images, computer font characters,
and MNIST. The number of bases was kept ﬁxed at
512 and all images were resized to 14 × 14 pixels. We
used two classiﬁers: a linear and gaussian svm and
compared the performance between the raw pixel in-
tensities and the sparse codes.

The results generally support our hypothesis that a
higher compatibility function yields better results on
the sparse coding classiﬁcation relative to the raw
pixel intensities (ﬁgure 1). The results are more pro-

Figure 2: Sample bases learned from natural images

nounced for the linear classiﬁer. This could be that
the gaussian kernel is better suited towards raw pixel
intensities, and we note that to allow for a fair com-
parison between the raw and sparse coding features,
we did not use the sparse coding kernel derived in [4].

5 Global Sparse Distributed Repre-
sentations

A diﬀerent approach which we call a global sparse dis-
tributed representation is to represent features from
a test example using a sparse combination of features
from the training set. Unlike local sparse distributed
representations, it turns out that within the context of
a global sparse distributed representation, the selec-
tion of features becomes completely irrelevant. Recent
research has shown that for face recognition, if the
representation is sparse enough, random linear pro-
jected features perform just as well or better than
feature selection for via Eigenfaces, Laplacianfaces,
or Fisherfaces [6]. Building oﬀ of the pattern recog-
nition algorithm used in [6] we devise a simple semi-
supervised learning algorithm which utilizes a boot-
strapping approach to expand our training set.

We assume that images within the same category
lie on a low-dimensional linear subspace. The k in-
dividual subspaces can be represented by matrices
A1 , A2 , · · · , Ak where each column in Ai is a train-
ing sample from class i. Let y be an image from the
test set, and A = [A1A2 · · · Ak ]. Then ideally,
y = Ax0 ∈ Rm

(5)

2

!"#$%&#&’()$*$
9$*:-$#(;<$/&=

+$%&#&’(,-$."."/(0&*(0.1&
>??
C??
>???

2$3(4+."&$-5
@ABC?D
I?BGID
I?BEID

06(4+."&$-5 2$3(40785 06(40785
@@BGHD I?BCID
@@BEFD
ICBEID IFBI@D
IHB@FD
IGBCED
IEB@HD IEB?GD

!"#$%&’(’)’&*
+,-./
+,-0-0
+,-0.1

!"#$%&#&’()$*$
89;0,

+$%&#&’(,-$."."/(0&*(0.1&
>??
C??
>???

2$3(4+."&$-5
@ABC?D
I?BGID
I?BEID

06(4+."&$-5 2$3(40785 06(40785
@@BGHD @@BG>D
@@BCAD
ICBEID IFB@ED
IGBC>D
IFBGC
IEB@HD
ICBAI

!"#$%&’(’)’&*
+,112.
+,112.
+,112.

!"#$%&#&’()$*$
JK"*(6L$-$M*&-=

+$%&#&’(,-$."."/(0&*(0.1&
>??
C??
>???

2$3(4+."&$-5
@ABC?D
I?BGID
I?BEID

06(4+."&$-5 2$3(40785 06(40785
@@BGHD @GBCHD
@CB>CD
ICBEID IHBEID
I?BI>D
I>BI?D
IEB@HD IFB?ID

!"#$%&’(’)’&*
+,-23.
+,-233
+,-232

Figure 1: MNIST Results

where x0 = [0, · · · , 0,α i,1 ,α i,2 , · · · ,α i,ni , 0, · · · , 0, ]T ∈
Rn is a coeﬃcient vector whose entries are mostly
zero except those associated with category i. Thus, a
new test image should be predominantly represented
using training images from the same sub ject, and this
representation will be naturally sparse if the number
of categories k is reasonably large.
Let R ∈ Rdxm be a transform matrix with d ( m.
Multiplying both sides of (5) by R yields ˜y = Ry =
RAx0 = ˜Ax0 . Recent developments in compressed
sensing have shown that the desired x0 is the solution
to the l1 minimization problem:

min ||x||1

s.t. ˜y = ˜Ax

Furthermore, if x has p ( n nonzeros, then

d ≥ 2p log(n/d)

(6)

(7)

random measurements are suﬃcient for sparse recov-
ery with high probability[6]. Thus even with ran-
domly selected facial features, if the dimension of the
image is suﬃciently large then we should still be able
to classify the test image.
Let δi (x) ∈ Rn be a vector whose only nonzero entries
are the entries in x associated with category i. The
classiﬁcation algorithm is then given by Algorithm 2
[6]. Note that the minimization problem in step 3 is
slightly diﬀerent than in (6) to model noise and error
in the data.

6 Semi-Supervised Learning in Global
Sparse Distributed Representations

We deﬁne the Sparsity Concentration Index as in [6]:

SCI(x) = k
k − 1 · maxi ||δi (x)||1 /||x||1 − 1 ∈ [0, 1]
(8)
The SCI provides a measure of how concentrated the
coeﬃcients are on a speciﬁc category. When SCI = 1,
the test image is represented using only images from a
single sub ject, and when SCI = 0, the coeﬃcients are
spread evenly over all categories. We also deﬁne ˆr to
be the ratio between the smallest and second smallest
residual.

The residuals calculated in Algorithm 1 measures how
well the the sparse representation approximates the
test image, while the SCI and ˆr give us a measure of
how good the representation is in terms of localiza-
tion.

Algorithm 2 Recognition via Sparse Representation
1: Input: a matrix of training images A ∈ Rm×n for
k sub jects, a linear feature transform R ∈ Rd×m ,
a test image y ∈ Rm , and an error tolerance .
2: Compute features ˜y = Ry and ˜A = RA and nor-
malize ˜y and columns of ˜A to unit length.
s.t. || ˜y − ˜Ax||2 ≤ 
3: Solve: min ||x||1
4: Compute residuals ri (y) = || ˜y − ˜Aδi (x)||2 for i =
1, · · · , k
5: Output: identity(y) = argmini ri (y)

After running Algorithm 3 on our unlabeled data, we
then run Algorithm 2 to perform the supervised learn-
ing task. Alternatively, we can run Algorithm 3 on
the test set, augmenting our training set with images

3

6:
7:

Algorithm 3 Bootstrapping a Global Sparse Repre-
sentation Classiﬁer
1: Input: a matrix of training images A ∈ Rm×n for
k sub jects, a linear feature transform R ∈ Rd×m ,
a matrix of unlabeled images Y ∈ Rm×q , and an
error tolerance .
2: loop
numLabeled = 0
3:
for all y ∈ Y do
4:
Compute features ˜y = Ry and ˜A = RA
5:
and normalize ˜y and columns of ˜A to
unit length
s.t. || ˜y − ˜Ax||2 ≤ 
Solve: min ||x||1
Compute residuals ri (y) = || ˜y− ˜Aδi (x)||2
for i = 1, · · · , k
if SCI(x) ≥ τ and ˆr ≥ σ then
j = identity(y) = argmini ri (y)
Append y to Aj and remove y from
Y
Update: A = [A1A2 · · · Ak ]
11:
numLabeled+ = 1
12:
end if
13:
end for
14:
if numLabeled = 0 then
15:
break
16:
end if
17:
18: end loop
19: Output: Augmented training matrix A ∈ Rm×p ,
p ≥ n

8:
9:
10:

Figure 3: Top:Residuals, Bottom: x obtained from
(6)

4

Original image

Random projection
feature

Figure 4: Random Feature Example

from the test set, and then run Algorithm 2 on the
remaining test images.

This algorithm has two potential drawbacks. The ﬁrst
is that if τ and σ are set improperly, then we might
add misclassiﬁed images to our training set, which
would then cause compounding errors in later classiﬁ-
cations, thus obscuring the categories in our training
set.
In our experiments this problem did not arise,
but this is an inherent weakness in any bootstrapping
algorithm. However, so long as the conditions in (7)
are satisﬁed, the probability of a sparse incorrect re-
construction occurring has very low probability. Thus
by setting τ and σ properly, the probability of a mis-
classiﬁed images being added to the training set is
very low.

The other concern is raised by the theory underly-
ing (7), which is that by increasing our training set
while keeping the dimensionality and number of cat-
egories ﬁxed, we reduce the probability that we can
successfully reconstruct the test image since we are
potentially decreasing the sparsity of x. For category
i with ni examples, and R ∈ Rm×n if
ni + |support()| > m/3
then we should not expect to perfectly recover x and
 [5]. However, this is generally not a problem since
so long as the dimension of R is large enough. If one
suspects that this might be a problem, one could stop
the algorithm once the sparsity started getting close
to the bound in (4).

(9)

7 Experiments with Face Recognition

Experiments were conducted the Extended Yale B
database. The database consists of 2, 414 cropped and
normalized frontal-face images of 38 individuals under
various laboratory controlled lighting conditions. The
feature space dimension chosen was 504.

We ﬁrst randomly split half the data into a training

recent development, with many attractive properties.
In particular, the ability to classify using random fea-
tures. The bootstrapping algorithm shown here is
fairly simplistic but surprisingly robust and helpful
in boosting performance. To explore this algorithm
further, it would be useful to perform experiments
where the parameters were set too low, causing mis-
classiﬁcations to enter our training set, and see how
performance degrades as a result.

9 Acknowledgements

Thanks to Honglak Lee for helping me with my sparse
coding implementation.

References

[1] M. Balcan and A. Blum. A pac-style model for
learning from labeled and unlabeled data, 2005.

[2] J. Baxter. Theoretical models of learning to learn,
1997.

[3] Honglak Lee, Alexis Battle, Ra jat Raina, and An-
drew Y. Ng. Eﬃcient sparse coding algorithms. In
Neural Information Processing Systems, 2006.

[4] Ra jat Raina, Alexis Battle, Honglak Lee, Ben-
jamin Packer, and Andrew Y. Ng. Self-taught
learning: transfer learning from unlabeled data. In
ICML ’07: Proceedings of the 24th international
conference on Machine learning, pages 759–766,
New York, NY, USA, 2007. ACM.

[5] John Wright, Arvind Ganesh, Allen Yang, and
Yi Ma. Robust face recognition via sparse rep-
resentation, 2007.

[6] Allen Y. Yang, John Wright, Yi Ma, and
S. Shankar Sastry. Feature selection in face recog-
nition: A sparse representation perspective. Tech-
nical Report UCB/EECS-2007-99, EECS Depart-
ment, University of California, Berkeley, Aug
2007.

[7] X. Zhu. Semi-supervised learning literature sur-
vey.
Technical Report TR 1530, Computer
Sciences Department, University of Wisconsin -
Madison, June 2007.

Figure 5: Training set augmented with unlabeled set

Figure 6: Training set augmented with test set

set and half into a test set. Then a portion of the
training set was randomly split oﬀ and randomly per-
muted into a third ”unlabeled training set”. We then
ran our bootstrapping algorithm on this unlabeled set
to augment our training set, and then tested our aug-
mented training set with the separate test set (ﬁgure
5). The algorithm does demonstrate signiﬁcant im-
provement on the test set, although the amount it
improves performance seems to decrease asymptoti-
cally as the initial training set size is increased.

We also ran a third experiment where we used the en-
tire training set and ran the bootstrapping algorithm
on the test set, augmenting the training set with the
test set. We again get a performance improvement,
although the eﬀect is small since we start out with a
large training set (ﬁgure 6). In all of the bootstrap-
ping experiments, τ and σ were set conservatively to
ensure that no unlabeled data was added to the train-
ing set and misclassiﬁed. In all the above experiments,
post-experiment analysis showed that the unlabeled
data which was added to the training set was all cor-
rectly classiﬁed.

8 Future Work and Conclusions

We provided a way which allows one to quantitatively
determine how the compatibility between the unla-
beled and labeled data will aﬀect performance on the
supervised learning task for a given structural map-
ping function. To make this result more robust, it
would be nice to show this compatibility with non-
image data, and to experiment with diﬀerent mapping
functions such as PCA. Also, it would be interesting
to see at what point of incompatibility the unlabeled
data stopped helping performance on the supervised
task.

The global sparse distributed representation is a fairly

5


A Spectral Regularization Framework for
Multi-Task Structure Learning

Andreas Argyriou
Department of Computer Science
University College London
Gower Street, London WC1E 6BT, UK
a.argyriou@cs.ucl.ac.uk

Charles A. Micchelli
Department of Mathematics and Statistics
SUNY Albany
1400 Washington Avenue
Albany, NY, 12222, USA

Massimiliano Pontil
Department of Computer Science
University College London
Gower Street, London WC1E 6BT, UK
m.pontil@cs.ucl.ac.uk

Yiming Ying
Department of Engineering Mathematics
University of Bristol
University Walk, Bristol, BS8 1TR, UK
enxyy@bristol.ac.uk

Abstract
Learning the common structure shared by a set of supervised tasks is an important
practical and theoretical problem. Knowledge of this structure may lead to bet-
ter generalization performance on the tasks and may also facilitate learning new
tasks. We propose a framework for solving this problem, which is based on reg-
ularization with spectral functions of matrices. This class of regularization prob-
lems exhibits appealing computational properties and can be optimized ef(cid:2)ciently
by an alternating minimization algorithm. In addition, we provide a necessary
and suf(cid:2)cient condition for convexity of the regularizer. We analyze concrete ex-
amples of the framework, which are equivalent to regularization with Lp matrix
norms. Experiments on two real data sets indicate that the algorithm scales well
with the number of tasks and improves on state of the art statistical performance.

1 Introduction

Recently, there has been renewed interest in the problem of multi-task learning, see [2, 4, 5, 14,
16, 19] and references therein. This problem is important in a variety of applications, ranging from
conjoint analysis [12], to object detection in computer vision [18], to multiple microarray data set
integration in computational biology [8] (cid:150) to mention just a few. A key objective in many multi-
task learning algorithms is to implement mechanisms for learning the possible structure underlying
the tasks. Finding this common structure is important because it allows pooling information across
the tasks, a property which is particularly appealing when there are many tasks but only few data
per task. Moreover, knowledge of the common structure may facilitate learning new tasks (transfer
learning), see [6] and references therein.
In this paper, we extend the formulation of [4], where the structure shared by the tasks is described
by a positive de(cid:2)nite matrix. In Section 2, we propose a framework in which the task parameters and
the structure matrix are jointly computed by minimizing a regularization function. This function has
the following appealing property. When the structure matrix is (cid:2)xed, the function decomposes across
the tasks, which can hence be learned independently with standard methods such as SVMs. When
the task parameters are (cid:2)xed, the optimal structure matrix is a spectral function of the covariance of
the tasks and can often be explicitly computed. As we shall see, spectral functions are of particular
interest in this context because they lead to an ef(cid:2)cient alternating minimization algorithm.

1

The contribution of this paper is threefold. First, in Section 3 we provide a necessary and suf(cid:2)cient
condition for convexity of the optimization problem. Second, in Section 4 we characterize the spec-
tral functions which relate to Schatten Lp regularization and present the alternating minimization
algorithm. Third, in Section 5 we discuss the connection between our framework and the convex
optimization method for learning the kernel [11, 15], which leads to a much simpler proof of the
convexity in the kernel than the one given in [15]. Finally, in Section 6 we present experiments on
two real data sets. The experiments indicate that the alternating algorithm runs signi(cid:2)cantly faster
than gradient descent and that our method improves on state of the art statistical performance on
these data sets. They also highlight that our approach can be used for transfer learning.

2 Modelling Tasks’ Structure

In this section, we introduce our multi-task learning framework. We denote by Sd the set of d (cid:2) d
++ ) the subset of positive semide(cid:2)nite (de(cid:2)nite) ones and by Od the
+ (Sd
symmetric matrices, by Sd
set of d (cid:2) d orthogonal matrices. For every positive integer n, we de(cid:2)ne INn = f1; : : : ; ng. We
let T be the number of tasks which we want to simultaneously learn. We assume for simplicity
that each task t 2 INT is well described by a linear function de(cid:2)ned, for every x 2 IRd , as w>
t x,
where wt is a (cid:2)xed vector of coef(cid:2)cients. For each task t 2 INT , there are m data examples
f(xtj ; ytj ) : j 2 INmg (cid:26) IRd (cid:2) IR available. In practice, the number of examples per task may vary
but we have kept it constant for simplicity of notation.
Our goal is to learn the vectors w1 ; : : : ; wT , as well as the common structure underlying the tasks,
from the data examples. In this paper we follow the formulation in [4], where the tasks’ structure
is summarized by a positive de(cid:2)nite matrix D which is linked to the covariance matrix between
the tasks, W W > . Here, W denotes the d (cid:2) T matrix whose t-th column is given by the vector wt
(we have assumed for simplicity that the mean task is zero). Speci(cid:2)cally, we learn W and D by
minimizing the function

(2.1)
Reg(W; D) := Err(W ) + (cid:13) Penalty(W; D);
where (cid:13) is a positive parameter which balances the importance between the error and the penalty.
The former may be any bounded from below and convex function evaluated at the values w >
t xtj ,
t 2 INT , j 2 INm . Typically, it will be the average error on the tasks, namely, Err(W ) =
t xtj ) and ‘ : IR (cid:2) IR ! [0; 1) is a prescribed
Lt (wt ), where Lt (wt ) = Pj2INm
Pt2INT
‘(ytj ; w>
loss function (e.g. quadratic, SVM, logistic etc.). We shall assume that the loss ‘ is convex in its
second argument, which ensures that the function Err is also convex. The latter term favors the tasks
sharing some common structure and is given by

w>
t F (D)wt ;

(2.2)

Penalty(W; D) = tr(F (D)W W > ) =

T
Xt=1
++ is a prescribed spectral matrix function. This is to say that F is induced
where F : Sd
++ ! Sd
by applying a function f : (0; 1) ! (0; 1) to the eigenvalues of its argument. That is, for every
++ we write D = U (cid:3)U > , where U 2 Od , (cid:3) = Diag((cid:21)1 ; : : : ; (cid:21)d ), and de(cid:2)ne
D 2 Sd
(2.3)
F (D) = U F ((cid:3))U > ; F ((cid:3)) = Diag(f ((cid:21)1 ); : : : ; f ((cid:21)d )):
In the rest of the paper, we will always use F to denote a spectral matrix function and f to denote
the associated real function, as above.
Minimization of the function Reg allows us to learn the tasks and at the same time a good represen-
tation for them which is summarized by the eigenvectors and eigenvalues of the matrix D . Different
choices of the function f re(cid:3)ect different properties which we would like the tasks to share. In the
special case that f is a constant, the tasks are totally independent and the regularizer (2.2) is a sum
of T independent L2 regularizers. In the case f ((cid:21)) = (cid:21)(cid:0)1 , which is considered in [4], the regular-
izer favors a sparse representation in the sense that the tasks share a small common set of features.
More generally, functions of the form f ((cid:21)) = (cid:21)(cid:0)(cid:11) ; (cid:11) (cid:21) 0, allow for combining shared features
and task-speci(cid:2)c features to some degree tuned by the exponent (cid:11). Moreover, the regularizer (2.2)
ensures that the optimal representation (optimal D) is a function of the tasks’ covariance W W > .
Thus, we propose to solve the minimization problem
inf nReg(W; D) : W 2 IRd(cid:2)T ; D 2 Sd
++ ; tr D (cid:20) 1o
2

(2.4)

for functions f belonging to an appropriate class. As we shall see in Section 4, the upper bound
on the trace of D in (2.4) prevents the in(cid:2)mum from being zero, which would lead to over(cid:2)tting.
Moreover, even though the in(cid:2)mum above is not attained in general, the problem in W resulting
after partial minimization over D admits a minimizer.
Since the (cid:2)rst term in (2.1) is independent of D , we can (cid:2)rst optimize the second term with respect
to D . That is, we can compute the in(cid:2)mum
(2.5)
(cid:10)f (W ) := inf (cid:8)tr(F (D)W W > ) : D 2 Sd
++ ; tr D (cid:20) 1(cid:9) :
In this way we could end up with an optimization problem in W only. However, in general this
would be a complex matrix optimization problem. It may require sophisticated optimization tools
such as semide(cid:2)nite programming, which may not scale well with the size of W . Fortunately, as
we shall show, problem (2.4) can be ef(cid:2)ciently solved by alternately minimizing over D and W . In
particular, in Section 4 we shall show that (cid:10)f is a function of the singular values of W only. Hence,
the only matrix operation required by alternate minimization is singular value decomposition and
the rest are merely vector problems.
Finally, we note that the ideas above may be extended naturally to a reproducing kernel Hilbert space
setting [3].

3 Joint Convexity via Matrix Concave Functions
In this section, we address the issue of convexity of the regularization function (2.1). Our main
result characterizes the class of spectral functions F for which the term w >F (D)w is jointly convex
in (w; D), which in turn implies that (2.4) is a convex optimization problem.
To illustrate our result, we require the matrix analytic concept of concavity, see, for example, [7].
We say that the real-valued function g : (0; 1) ! IR is matrix concave of order d if
++ and (cid:21) 2 [0; 1] ;
8A; B 2 Sd
(cid:21)G(A) + (1 (cid:0) (cid:21))G(B ) (cid:22) G((cid:21)A + (1 (cid:0) (cid:21))B )
where G is de(cid:2)ned as in (2.3). The notation (cid:22) denotes the Loewner partial order on Sd : C (cid:22) D
if and only if D (cid:0) C is positive semide(cid:2)nite. If g is a matrix concave function of order d for any
d 2 IN, we simply say that g is matrix concave. We also say that g is matrix convex (of order d)
if (cid:0)g is matrix concave (of order d). Clearly, matrix concavity implies matrix concavity of smaller
orders (and hence standard concavity).
Theorem 3.1. Let F : Sd
++ be a spectral function. Then the function (cid:26) : IRd (cid:2)Sd
++! Sd
++! [0; 1)
deﬁned as (cid:26)(w; D) = w>F (D)w is jointly convex if and only if 1
f is matrix concave of order d.
++ and (cid:21) 2
Proof. By de(cid:2)nition, (cid:26) is convex if and only if, for any w1 ; w2 2 IRd ; D1 ; D2 2 Sd
(0; 1), it holds that
(cid:26)((cid:21)w1 + (1 (cid:0) (cid:21))w2 ; (cid:21)D1 + (1 (cid:0) (cid:21))D2 ) (cid:20) (cid:21)(cid:26)(w1 ; D1 ) + (1 (cid:0) (cid:21))(cid:26)(w2 ; D2 ):
Let C := F ((cid:21)D1 + (1 (cid:0) (cid:21))D2 ); A := F (D1 )=(cid:21); B := F (D2 )=(1 (cid:0) (cid:21)), w := (cid:21)w1 + (1 (cid:0) (cid:21))w2
and z := (cid:21)w1 : Using this notation, the above inequality can be rewritten as
(3.1)
8 w; z 2 IRd :
w>Cw (cid:20) z>Az + (w (cid:0) z )>B (w (cid:0) z )
The right hand side in (3.1) is minimized for z = (A + B )(cid:0)1Bw and hence (3.1) is equivalent to
w>Cw (cid:20) w> (cid:2)B (A + B )(cid:0)1A(A + B )(cid:0)1B + (cid:0)I (cid:0) (A + B )(cid:0)1B (cid:1)>
B (cid:0)I (cid:0) (A + B )(cid:0)1B (cid:1)(cid:3)w ;
8 w 2 IRd , or to
C (cid:22) B (A + B )(cid:0)1A(A + B )(cid:0)1B + (cid:0)I (cid:0) (A + B )(cid:0)1B (cid:1)>
B (cid:0)I (cid:0) (A + B )(cid:0)1B (cid:1)
= B (A + B )(cid:0)1A(A + B )(cid:0)1B + B (cid:0) 2B (A + B )(cid:0)1B + B (A + B )(cid:0)1B (A + B )(cid:0)1B
= B (cid:0) B (A + B )(cid:0)1B = (A(cid:0)1 + B(cid:0)1 )(cid:0)1 ;
where the last equality follows from the matrix inversion lemma [10, Sec. 0.7]. The above inequality
is identical to (see e.g. [10, Sec. 7.7])

A(cid:0)1 + B(cid:0)1 (cid:22) C (cid:0)1 ;

3

or, using the initial notation,
(cid:21)(cid:0)F (D1 )(cid:1)(cid:0)1
+ (1 (cid:0) (cid:21))(cid:0)F (D2 )(cid:1)(cid:0)1
(cid:22) (cid:0)F ((cid:21)D1 + (1 (cid:0) (cid:21))D2 )(cid:1)(cid:0)1
:
f is matrix
++ ; (cid:21) 2 (0; 1) if and only if 1
By de(cid:2)nition, this inequality holds for any D1 ; D2 2 Sd
concave of order d.
Examples of matrix concave functions on (0; 1) are log(x + 1) and the function xs for s 2 [0; 1]
(cid:150) see [7] for other examples and theoretical results. We conclude with the remark that, whenever 1
f
is matrix concave of order d, function (cid:10)f in (2.5) is convex, because it is the partial in(cid:2)mum of a
jointly convex function [9, Sec. IV.2.4].

p Prenorms
4 Regularization with Schatten L
4.1 Partial Minimization of the Penalty Term

In this section, we focus on the family of negative power functions f and obtain that function (cid:10)f
in (2.5) relates to the Schatten Lp prenorms. We start by showing that problem (2.5) reduces to a
minimization problem in IRd , by application of a useful matrix inequality. In the following, we let
B take the place of W W > for brevity.
Lemma 4.1. Let F : Sd ! Sd be a spectral function, B 2 Sd and (cid:12)i ; i 2 INd , the eigenvalues of
B . Then,
(cid:14)i (cid:20) 1) :
++ ; tr D (cid:20) 1g = inf ( Xi2INd
: (cid:14)i > 0; i 2 INd ; Xi2INd
inf ftr(F (D)B ) : D 2 Sd
Moreover, for the inﬁmum on the left to be attained, F (D) has to share a set of eigenvectors with B
so that the corresponding eigenvalues are in the reverse order as the (cid:12)i .

f ((cid:14)i )(cid:12)i

Proof. We use an inequality of Von Neumann [13, Sec. H.1.h] to obtain, for all X; Y 2 Sd , that
tr(X Y ) (cid:21) Xi2INd
(cid:21)i(cid:22)i
where (cid:21)i and (cid:22)i are the eigenvalues of X and Y in nonincreasing and nondecreasing order, re-
spectively. The equality is attained whenever X = U Diag((cid:21))U > ; Y = U Diag((cid:22))U > for some
U 2 Od . Applying this inequality for X = F (D); Y = B and denoting f ((cid:14)i ) = (cid:21)i ; i 2 INd , the
result follows.

(tr B s )

Using this lemma, we can now derive the solution of problem (2.5) in the case that f is a negative
power function.
Proposition 4.2. Let B 2 Sd
+ and s 2 (0; 1]. Then we have that
++ ; tr D (cid:20) 1o :
s = inf ntr(D
1
s(cid:0)1
s B ) : D 2 Sd
++ the inﬁmum is attained and the minimizer is given by D =
Moreover, if B 2 Sd

B s
tr B s :
Proof. By Lemma 4.1, it suf(cid:2)ces to show the analogous statement for vectors, namely that
1
i !
(cid:14)i (cid:20) 1)
= inf ( Xi2INd
  Xi2INd
s
(cid:12)i : (cid:14)i > 0; i 2 INd ; Xi2INd
(cid:12) s
where (cid:12)i (cid:21) 0; i 2 INd . To this end, we apply H ¤older’s inequality with p = 1
:
s and q = 1
1(cid:0)s
(cid:14)i!1(cid:0)s
(cid:12)i!s
(cid:12)i!s   Xi2INd
(cid:20)   Xi2INd
i (cid:20)   Xi2INd
(cid:12)i(cid:17)
i = Xi2INd (cid:16)(cid:14)
Xi2INd
(cid:14)1(cid:0)s
(cid:12) s
(cid:12) s
When (cid:12)i > 0; i 2 INd , the equality is attained for (cid:14)i =
; i 2 INd . To show that the
i
(cid:12) s
Pj2INd
j
inequality is sharp in all other cases, we replace (cid:12)i by (cid:12)i;" := (cid:12)i + ", i 2 INd ; " > 0, de(cid:2)ne
j;" ) and take the limits as " ! 0.
(cid:14)i;" = (cid:12) s
i;" =(Pj (cid:12) s

s(cid:0)1
s

s(cid:0)1
s

s(cid:0)1
s

(cid:14)

i

s

s(cid:0)1
s

i

(cid:14)

i

:

(cid:14)

i

4

The above result implies that the regularization problem (2.4) is conceptually equivalent to regular-
ization with a Schatten Lp prenorm of W , when the coupling function f takes the form f (x) = x1(cid:0)
2
p
with p 2 (0; 2], p = 2s. The Schatten Lp prenorm is the Lp prenorm of the singular values of a
matrix. In particular, trace norm regularization (see [1, 17]) corresponds to the case p = 1. We also
note that generalization error bounds for Schatten Lp norm regularization can be derived along the
lines of [14].

4.2 Learning Algorithm

p
2

:

(4.2)

D" (W ) =

Lemma 4.1 demonstrates that optimization problems such as (2.4) with spectral regularizers of the
form (2.2) are computationally appealing, since they decompose to vector problems in d variables
along with singular value decomposition of the matrix W . In particular, for the Schatten Lp prenorm
with p 2 (0; 2], the proof of Proposition 4.2 suggests a way to solve problem (2.4). We modify the
penalty term (2.2) as
(4.1)
Penalty" (W; D) = tr(cid:0)F (D)(W W > + "I )(cid:1);
where " > 0 and let Reg" (W; D) = Err(W ) + (cid:13) Penalty" (W; D) be the corresponding regulariza-
tion function. By Proposition 4.2, for a (cid:2)xed W 2 IRd(cid:2)T there is a unique minimizer of Penalty"
(under the constraints in (2.5)), given by the formula
(W W > + "I )
p
tr(W W > + "I )
2
Moreover, there exists a minimizer of problem (2.4), which is unique if p 2 (1; 2].
Therefore, we can solve problem (2.4) using an alternating minimization algorithm, which is an
extension of the one presented in [4] for the special case F (D) = D(cid:0)1 . Each iteration of the
algorithm consists of two steps. In the (cid:2)rst step, we keep D (cid:2)xed and minimize over W . This
consists in solving the problem
t F (D)wt : W 2 IRd(cid:2)T ) :
min ( Xt2INT
Lt (wt ) + (cid:13) Xt2INT
w>
This minimization can be carried out independently for each task since the regularizer decouples
when D is (cid:2)xed. Speci(cid:2)cally, introducing new variables for (F (D))
2 wt yields a standard L2 reg-
1
ularization problem for each task with the same kernel K (x; z ) = x> (F (D))(cid:0)1 z , x; z 2 IRd . In
other words, we simply learn the parameters wt (cid:150) the columns of matrix W (cid:150) independently by
a regularization method, for example by an SVM or ridge regression method, for which there are
well developed tool boxes. In the second step, we keep matrix W (cid:2)xed and minimize over D using
equation (4.2).
Space limitations prevent us from providing a convergence proof of the algorithm. We only note
that following the proof detailed in [3] for the case p = 1, one can show that the sequence produced
by the algorithm converges to the unique minimizer of Reg" if p 2 [1; 2], or to a local minimizer
if p 2 (0; 1). Moreover, by [3, Thm. 3] as " goes to zero the algorithm converges to a solution of
problem (2.4), if p 2 [1; 2]. In theory, an algorithm without "-perturbation does not converge to a
minimizer, since the columns of W and D always remain in the initial column space. In practice,
however, we have observed that even such an algorithm converges to an optimal solution, because
of round-off effects.

5 Relation to Learning the Kernel
In this section, we discuss the connection between the multi-task framework (2.1)-(2.4) and the
framework for learning the kernel, see [11, 15] and references therein. To this end, we de(cid:2)ne the
kernel Kf (D)(x; z ) = x> (F (D))(cid:0)1 z , x; z 2 IRd , the set of kernels Kf = fKf (D) : D 2
++ ; tr D (cid:20) 1g and, for every kernel K , the task kernel matrix Kt = (K (xti ; xtj ) : i; j 2 INm ),
Sd
t 2 INT . It is easy to prove, using Weyl’s monotonicity theorem [10, Sec. 4.3] and [7, Thm. V.2.5],
that the set Kf is convex if and only if 1
f is matrix concave. By the well-known representer theorem
(see e.g. [11]), problem (2.4) is equivalent to minimizing the function
t Kt ct!
Xt2INT   Xi2INm
‘(yti ; (Kt ct )i ) + (cid:13) c>

(5.1)

5

over ct 2 IRm (for t 2 INT ) and K 2 Kf . It is apparent that the function (5.1) is not jointly convex
in ct and K . However, minimizing each term over the vector ct gives a convex function of K .
Proposition 5.1. Let K be the set of all reproducing kernels on IRd . If ‘(y ; (cid:1)) is convex for any
y 2 IR then the function Et : K ! [0; 1) deﬁned for every K 2 K as
‘(yti ; (Kt c)i ) + (cid:13) c>Kt c : c 2 IRm)
Et (K ) = min ( Xi2INm

is convex.

Proof. Without loss of generality, we can assume as in [15] that Kt are invertible for all t 2 INT .
t a,
For every a 2 IRm and K 2 K , we de(cid:2)ne the function Gt (a; K ) = Pi2INm
‘(yti ; ai )+(cid:13) a>K (cid:0)1
which is jointly convex by Theorem 3.1. Clearly, Et (K ) = minfGt (a; K ) : a 2 IRm g. Recalling
IV.2.4], we obtain the
that the partial minimum of a jointly convex function is convex [9, Sec.
convexity of Et .
The fact that the function Et is convex has already been proved in [15], using minimax theorems
and Fenchel duality. Here, we were able to simplify the proof of this result by appealing to the joint
convexity property stated in Theorem 3.1.

6 Experiments
In this section, we (cid:2)rst report a comparison of the computational cost between the alternating min-
imization algorithm and the gradient descent algorithm. We then study how performance varies for
different Lp regularizers, compare our approach with other multi-task learning methods and report
experiments on transfer learning.
We used two data sets in our experiments. The (cid:2)rst one is the computer survey data from [12]. It
was taken from a survey of 180 persons who rated the likelihood of purchasing one of 20 different
personal computers. Here the persons correspond to tasks and the computer models to examples.
The input represents 13 different computer characteristics (price, CPU, RAM etc.) while the output
is an integer rating on the scale 0 (cid:0) 10. Following [12], we used the (cid:2)rst 8 examples per task as the
training data and the last 4 examples per task as the test data. We measured the root mean square
error of the predicted from the actual ratings for the test data, averaged across people.
The second data set is the school data set from the Inner London Education Authority (see
http://www.cmm.bristol.ac.uk/learning-training/multilevel-m-support/datasets.shtml). It consists of
examination scores of 15362 students from 139 secondary schools in London. Thus, there are 139
tasks, corresponding to predicting student performance in each school. The input consists of the year
of the examination, 4 school-speci(cid:2)c and 3 student-speci(cid:2)c attributes. Following [5], we replaced
categorical attributes with binary ones, to obtain 27 attributes in total. We generated the training and
test sets by 10 random splits of the data, so that 75% of the examples from each school (task) belong
to the training set and 25% to the test set. Here, in order to compare our results with those in [5], we
used the measure of percentage explained variance, which is de(cid:2)ned as one minus the mean squared
test error over the variance of the test data and indicates the percentage of variance explained by
the prediction model. Finally, we note that in both data sets we used the square loss, tuned the
regularization parameter (cid:13) with 5-fold cross-validation and added an additional input component
accounting for the bias term.
In the (cid:2)rst experiment, we study the computational cost of the alternating minimization algorithm
against the gradient descent algorithm, both implemented in Matlab, for the Schatten L1:5 norm. The
left plot in Figure 1 shows the value of the objective function (2.1) versus the number of iterations,
on the computer survey data. The curves for different learning rates (cid:17) are shown, whereas for rates
greater than 0:05 gradient descent diverges. The alternating algorithm curve for " = 10(cid:0)16 is also
shown. We further note that for both data sets our algorithm typically needed less than 30 iterations
to converge. The right plot depicts the CPU time (in seconds) needed to reach a value of the objective
function which is less than 10(cid:0)5 away from the minimum, versus the number of tasks. It is clear
that our algorithm is at least an order of magnitude faster than gradient descent with the optimal
learning rate and scales better with the number of tasks. We note that the computational cost of our
method is mainly due to the T ridge regressions in the supervised step (learning W ) and the singular

6

value decomposition in the unsupervised step (learning D). A singular value decomposition is also
needed in gradient descent, for computing the gradient of the Schatten Lp norm. We have observed
that the cost per iteration is smaller for gradient descent but the number of iterations is at least an
order of magnitude larger, leading to the large difference in time cost.

28.5

28

27.5

27

Reg
26.5

26

25.5

25

24.5
 
0

 

h = 0.05
h = 0.03
h = 0.01
Alternating

20

40

60

80

100

iterations

6

5

4

seconds
3

2

1

0
 
50

Alternating
h = 0.05

 

100

tasks

150

200

Figure 1: Comparison between the alternating algorithm and the gradient descent algorithm.

4

3.5

3

RMSE

2.5

2

1.5
0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

p

0.27

0.265

0.26

0.255

expl. variance
0.25

0.245

0.24

0.235
0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

p

Figure 2: Performance versus p for the computer survey data (left) and the school data (right).

Table 1: Comparison of different methods on the computer survey data (left) and school data (right).
RMSE
Method
3.88
p = 2
p = 1
1.93
1.86
p = 0.7
Hierarchical Bayes [12]
1.90

Method
p = 2
p = 1
Hierarchical Bayes [5]

Explained variance
23:5 (cid:6) 2:0%
26:7 (cid:6) 2:0%
29:5 (cid:6) 0:4%

In the second experiment we study the statistical performance of our method as the spectral function
changes. Speci(cid:2)cally, we choose functions giving rise to Schatten Lp prenorms, as discussed in
Section 4. The results, shown in Figure 2, indicate that the trace norm is the best norm on these
data sets. However, on the computer survey data a value of p less than one gives the best result
overall. From this we speculate that our method can even approximate well the solutions of certain
non-convex problems. In contrast, on the school data the trace norm gives almost the best result.
Next, in Table 1, we compare our algorithm with the hierarchical Bayes (HB) method described in
[5, 12]. This method also learns a matrix D using Bayesian inference. Our method improves on
the HB method on the computer survey data and is competitive on the school data (even though our
regularizer is simpler than HB and the data splits of [5] are not available).
Finally, we present preliminary results on transfer learning. On the computer survey data, we trained
our method with p = 1 on 150 randomly selected tasks and then used the learned structure matrix D
for training 30 ridge regressions on the remaining tasks. We obtained an RMSE of 1:98 on these 30
(cid:147)new(cid:148) tasks, which is not much worse than an RMSE of 1:88 on the 150 tasks. In comparison, when

7

using the raw data (D = I
d ) on the 30 tasks we obtained an RMSE of 3:83. A similar experiment was
performed on the school data, (cid:2)rst training on a random subset of 110 schools and then transferring
D to the remaining 29 schools. We obtained an explained variance of 19:2% on the new tasks. This
was worse than the explained variance of 24:8% on the 110 tasks but still better than the explained
variance of 13:9% with the raw representation.
7 Conclusion
We have presented a spectral regularization framework for learning the structure shared by many
supervised tasks. This structure is summarized by a positive de(cid:2)nite matrix which is a spectral
function of the tasks’ covariance matrix. The framework is appealing both theoretically and prac-
tically. Theoretically, it brings to bear the rich class of spectral functions which is well-studied in
matrix analysis. Practically, we have argued via the concrete example of negative power spectral
functions, that the tasks’ parameters and the structure matrix can be ef(cid:2)ciently computed using an
alternating minimization algorithm, improving upon state of the art statistical performance on two
real data sets. A natural question is to which extent the framework can be generalized to allow for
more complex task sharing mechanisms, in which the structure parameters depend on higher order
statistical properties of the tasks.
Acknowledgements
This work was supported by EPSRC Grant EP/D052807/1, NSF Grant DMS 0712827 and by the
IST Programme of the European Commission, PASCAL Network of Excellence IST-2002-506778.
References
[1] J. Abernethy, F. Bach, T. Evgeniou, and J-P. Vert. Low-rank matrix factorization with attributes. Technical
Report N24/06/MM, Ecole des Mines de Paris, 2006.
[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unla-
beled data. Journal of Machine Learning Research, 6:1817(cid:150)1853, 2005.
[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 2007.
In press.
[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information
Processing Systems 19, pages 41(cid:150)48. 2007.
[5] B. Bakker and T. Heskes. Task clustering and gating for bayesian multi(cid:150)task learning. Journal of Machine
Learning Research, 4:83(cid:150)99, 2003.
[6] J. Baxter. A model for inductive bias learning. J. of Artiﬁcial Intelligence Research, 12:149(cid:150)198, 2000.
[7] R. Bhatia. Matrix Analysis. Graduate texts in Mathematics. Springer, 1997.
[8] R. Chari, W.W. Lockwood, and B.P. Coe et al. Sigma: a system for integrative genomic microarray
analysis of cancer genomes. BMC Genomics, 7:324, 2006.
[9] J.-B. Hiriart-Urruty and C. Lemar ·echal. Convex Analysis and Minimization Algorithms. Springer, 1996.
[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.
[11] G.R.G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M.I. Jordan. Learning the kernel matrix
with semide(cid:2)nite programming. Journal of Machine Learning Research, 5:27(cid:150)72, 2005.
[12] P. J. Lenk, W. S. DeSarbo, P. E. Green, and M. R. Young. Hierarchical Bayes conjoint analysis: recovery
of partworth heterogeneity from reduced experimental designs. Marketing Science, 15(2):173(cid:150)191, 1996.
[13] A. W. Marshall and I. Olkin. Inequalities: Theory of Majorization and its Applications. Academic Press,
1979.
[14] A. Maurer. Bounds for linear multi-task learning. J. of Machine Learning Research, 7:117(cid:150)139, 2006.
[15] C.A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine
Learning Research, 6:1099(cid:150)1125, 2005.
[16] R. Raina, A. Y. Ng, and D. Koller. Constructing informative priors using transfer learning. In Proceedings
of the 23rd International Conference on Machine Learning, 2006.
[17] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Advances in
Neural Information Processing Systems 17, pages 1329(cid:150)1336. 2005.
[18] A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing features: ef(cid:2)cient boosting procedures for multi-
class object detection. In Proc. of Conf. on Computer Vision and Pattern Recognition. 2:762-769, 2004.
[19] J. Zhang, Z. Ghahramani, and Y. Yang. Learning multiple related tasks using latent independent compo-
nent analysis. In Advances in Neural Information Processing Systems 18, pages 1585(cid:150)1592. 2006.

8


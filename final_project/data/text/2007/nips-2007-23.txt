Unsupervised Feature Selection for Accurate
Recommendation of High-Dimensional Image Data

Sabri Boutemedjet
DI, Universite de Sherbrooke
2500 boulevard de l’Universit ´e
Sherbrooke, QC J1K 2R1, Canada
sabri.boutemedjet@usherbrooke.ca

Djemel Ziou
DI, Universite de Sherbrooke
2500 boulevard de l’Universit ´e
Sherbrooke, QC J1K 2R1, Canada
djemel.ziou@usherbrooke.ca

Nizar Bouguila
CIISE, Concordia University
1515 Ste-Catherine Street West
Montreal, QC H3G 1T7, Canada
bouguila@ciise.concordia.ca

Abstract

Content-based image suggestion (CBIS) targets the recommendation of products
based on user preferences on the visual content of images. In this paper, we mo-
tivate both feature selection and model order identiﬁcation as two key issues for
a successful CBIS. We propose a generative model in which the visual features
and users are clustered into separate classes. We identify the number of both user
and image classes with the simultaneous selection of relevant visual features us-
ing the message length approach. The goal is to ensure an accurate prediction
of ratings for multidimensional non-Gaussian and continuous image descriptors.
Experiments on a collected data have demonstrated the merits of our approach.

1 Introduction

Products in today ’s e-market are described using both visual and textual information. From con-
sumer psychology, the visual information has been recognized as an important factor that inﬂuences
the consumer’s decision making and has an important power of persuasion [4]. Furthermore, it is
well recognized that the consumer choice is also inﬂuenced by the external environment or context
such as the time and location [4]. For example, a consumer could express an information need
during a travel that is different from the situation when she or he is working or even at home.
(CBIS) [4] motivates the modeling of user preferences with
“Content-Based Image Suggestion”
respect to visual information under the in ﬂuence of the context. Therefore, CBIS aims at the sug-
gestion of products whose relevance is inferred from the history of users in different contexts on
images of the previously consumed products. The domains considered by CBIS are a set of users
U = {1, 2, . . . , Nu}, a set of visual documents V = {(cid:2)v 1 , (cid:2)v2 , . . . , (cid:2)vNv }, and a set of possible con-
texts E = {1, 2, . . . , Ne}. Each (cid:2)vk is an arbitrary descriptor (visual, textual, or categorical) used
to represent images or products. In this work, we consider an image as a D-dimensional vector
(cid:2)v = (v1 , v2 , . . . , vD ). The visual features may be local such as interest points or global such as
color, texture, or shape. The relevance is expressed explicitly on an ordered voting (or rating) scale
deﬁned as R = {r1 , r2 , . . . , rNr }. For example, the ﬁve star scale (i.e. N r = 5) used by Amazon al-
lows consumers to give different degrees of appreciation. The history of each user u ∈ U , is deﬁned
as Du = {< u, e(j) , (cid:2)v (j) , r(j) > |e(j) ∈ E , (cid:2)v (j) ∈ V , r(j) ∈ R, j = 1, . . . , |Du |}.

Figure 1: The VCC-FMM identiﬁes like-mindedness from similar appreciations on similar images
represented in 3-dimensional space. Notice the inter-relation between the number of image clusters
and the considered feature subset.

In literature, the modeling of user preferences has been addressed mainly within collaborative ﬁl-
tering (CF) and content-based ﬁltering (CBF) communities. On the one hand, CBF approaches [12]
D u taken
build a separate model of “liked” and “disliked ” discrete data (word features) from each
individually. On the other hand, CF approaches predict the relevance of a given product for a given
user based on the preferences provided by a set of “like-minded ” (similar tastes) users. The data set
used by CF is the user-product matrix (∪ Nu
u=1Du ) which is discrete since each product is represented
by a categorical index. The Aspect model [7] and the ﬂexible mixture model (FMM) [15] are exam-
ples of some model-based CF approaches. Recently, the authors in [4] have proposed a statistical
model for CBIS which uses both visual and contextual information in modeling user preferences
with respect to multidimensional non Gaussian and continuous data. Users with similar preferences
are considered in [4] as those who appreciated with similar degrees similar images. Therefore, in-
stead of considering products as categorical variables (CF), visual documents are represented by
a richer visual information in the form of a vector of visual features (texture, shape, and interest
points). The similarity between images and between user preferences is modeled in [4] through a
single graphical model which clusters users and images separately into homogeneous groups in a
similar way to the ﬂexible mixture model (FMM) [15]. In addition, since image data are generally
non-Gaussian [1], class-conditional distributions of visual features are assumed Dirichlet densities.
By this way, the like-mindedness in user preferences is captured at the level of visual features.

Statistical models for CBIS are useful tools in modeling for many reasons. First, once the model is
learned from training data (union of user histories), it can be used to “suggest” unknown (possibly
unrated) images efﬁciently i.e. few effort is required at the prediction phase. Second, the model can
be updated from new data (images or ratings) in an online fashion in order to handle the changes in
either image clusters and/or user preferences. Third, model selection approaches can be employed
to identify “without supervision ”
both numbers of user preferences and image clusters (i.e. model
order) from the statistical properties of the data. It should be stressed that the unsupervised selection
of the model order was not addressed in CF/CBF literature. Indeed, the model order in many well-

founded statistical models such as the Aspect model [7] or FMM [15] was set “empirically”
as a
compromise between the model’s complexity and the accuracy of prediction, but not from the data.

From an “image collection modeling” point of view, the work in [4] has focused on modeling user
preferences with respect to non-Gaussian image data. However, since CBIS employs generally high-
dimensional image descriptors, then the problem of modeling accurately image collections needs to
be addressed in order to overcome the curse of dimensionality and provide accurate suggestions.
Indeed, the presence of many irrelevant features degrades substantially the performance of the mod-
eling and prediction [6] in addition to the increase of the computational complexity. To achieve a
better modeling, we consider feature selection and extraction as another “key issue”
for CBIS. In
literature [6], the process of feature selection in mixture models have not received as much attention
as in supervised learning. The main reason is the absence of class labels that may guide the selection
process [6]. In this paper, we address the issue of feature selection in CBIS through a new generative
model which we call Visual Content Context-aware Flexible Mixture Model (VCC-FMM). Due to
the problem of the inter-relation between feature subsets and the model order i.e. different feature
subsets correspond to different natural groupings of images, we propose to learn the VCC-FMM
from unlabeled data using the Minimum Message Length (MML) approach [16]. The next Section
details the VCC-FMM model with an integrated feature selection. After that, we discuss the identi ﬁ-
cation of the model order using the MML approach in Section 3. Experimental results are presented
in Section 4. Finally, we conclude this paper by a summary of the work.

2 The Visual Content Context Flexible Mixture Model
The data set D used to learn a CBIS system is the union of all user histories i.e. D = ∪ u∈U Du . From
this data set we model both like-mindedness shared by user groups as well as the visual and semantic
similarity between images [4]. For that end, we introduce two latent variables z and c to label each
observation < u, e, (cid:2)v, r > with information about user classes and image classes, respectively.
(cid:2)
In order to make predictions on unseen images, we need to model the joint event p((cid:2)v , r, u, e) =
z ,c p((cid:2)v , r, u, e, z , c). Then, the rating r for a given user u, context e and a visual document (cid:2)v can be
predicted on the basis of probabilities p(r|u, e, v) that can be derived by conditioning the generative
model p(u, e, v , r). We notice that the full factorization of p((cid:2)v , r, u, e, z , c) using the chain rule
leads to quantities with a huge number of parameters which are difﬁcult to interpret in terms of the
data [4]. To overcome this problem, we make use of some conditional independence assumptions
that constitute our statistical approximation of the joint event p((cid:2)v , r, u, e). These assumptions are
illustrated by the graphical representation of the model in ﬁgure 2. Let K and M be the number of
user classes and images classes respectively, an initial model for CBIS can be derived as [4]:
M(cid:3)
K(cid:3)
p(z )p(c)p(u|z )p(e|z )p((cid:2)v|c)p(r|z , c)
p((cid:2)v , r, u, e) =
(1)
c=1
z=1
The quantities p(z ) and p(c) denote the a priori weights of user and image classes. p(u|z ) and p(e|z )
denote the likelihood of a user and context to belong respectively to the user’s class z . p(r|z , c) is the
probability to sample a rating for a given user class and image class. All these quantities are modeled
from discrete data. On the other hand, image descriptors are high-dimensional, continuous and
generally non Gaussian data [1]. Thus, the distribution of class-conditional densities p((cid:2)v |c) should
be modeled carefully in order to capture efﬁciently the added-value of the visual information. In this
work, we assume that p((cid:2)v |c) is a Generalized Dirichlet distribution (GDD) which is more appropriate
than other distributions such as the Gaussian or Dirichlet distributions in modeling image collections
[1]. This distribution has a more general covariance structure and provides multiple shapes. The
distribution of the c-th component (cid:2)Θ∗
∗
c is given by equation (2). The
superscript is used to denote
the unknown true GDD distribution.
(1 − l(cid:3)
D(cid:4)
cl + β ∗
Γ(α∗
p((cid:2)v | (cid:2)Θ
cl )
Γ(α∗
cl )Γ(β ∗
(cid:2)D
cl )
k=1
l=1
cl+1 for l = 1, . . . , D − 1
cl+1 − β ∗
cl − α∗
cl = β ∗
l=l vl < 1 and 0 < vl < 1 for l = 1, . . . , D . γ ∗
where
D − 1. In equation (2) we have set (cid:2)Θ∗
and γ ∗
D = β ∗
c = (α∗
c1 , β ∗
c1 , . . . , α∗
cD , β ∗
cD ). From the math-
ematical properties of the GDD, we can transform using a geometric transformation the data point

vα∗
cl−1
l

∗
c ) =

γ∗
cl

vk )

(2)

Figure 2: Graphical representation of VCC-FMM.

(cid:2)v into another data point (cid:2)x = (x 1 , . . . , xD ) with independent features without loss of information
(cid:5)D
[1]. In addition, each x l of (cid:2)x generated by the c-th component, follows a Beta distribution p b (.|θ∗
cl )
cl ) which leads to the fact p((cid:2)x| (cid:2)Θ∗
l=1 pb (xl |θ∗
with parameters θ ∗
cl = (α∗
cl , β ∗
c ) =
cl ). The indepen-
dence between x l makes the estimation of a GDD very efﬁcient i.e. D estimations of univariate Beta
distributions without loss of accuracy. However, even with independent features, the unsupervised
identiﬁcation of image clusters based on high-dimensional descriptors remains a hard problem due
to the omnipresence of noisy, redundant and uninformative features [6] that degrade the accuracy of
the modeling and prediction. We consider feature selection and extraction as a “key ” methodology
in order to remove that kind of features in our modeling. Since x l are independent, then we can
features in the representation space X . However, we need some deﬁnition of
extract “relevant”
feature’s relevance. From ﬁgure 1, four well-separated image clusters can be identi ﬁed from only
two relevant features 1 and 2 which are multimodal and in ﬂuenced by class labels. On the other
hand, feature 3 is unimodal (i.e. irrelevant) and can be approximated by a single Beta distribution
pb (.|ξl ) common to all components. This deﬁnition of feature’s relevance has been motivated in
unsupervised learning [2][9]. Let (cid:2)φ = (φ1 , . . . , φD ) be a set of missing binary variables denoting
the relevance of all features. φ l is set to 1 when the l-th feature is relevant and 0 otherwise. The
θ ∗
(cid:6)
cl , φl ) (cid:4) (cid:6)
(cid:7)φl
(cid:7)1−φl
cl can be approximated as [2][9]:
“true” Beta distribution
pb (xl |ξl )
pb (xl |θcl )
p(xl |θ∗
(3)
By considering each φ l as Bernoulli variable with parameters p(φ l = 1) = l1 and p(φl = 0) = l2
(l1 + l2 = 1) then, the distribution p(x l |θ∗
cl ) can be obtained after marginalizing over φ l [9] as:
p(xl |θ∗
cl ) (cid:4) l1 pb (xl |θcl ) + l2 pb (xl |ξl ). The VCC-FMM model is given by equation (4). We notice
that both models [3] [4] are special cases of VCC-FMM.
D(cid:4)
K(cid:3)
M(cid:3)
p(z )p(u|z )p(e|z )p(c)p(r|z , c)
[l1 pb (xl |θcl ) + l2 pb (xl |ξl )]

p((cid:2)x, r, u, e) =

(4)

z=1

c=1

l=1

3 A Uniﬁed Objective for Model and Feature Selection using MML

We denote by (cid:2)θA
π the parameter vector of the multinomial distribution of any discrete variable A
(cid:2)
conditioned on its parent Π of VCC-FMM (see ﬁgure 2). We have A| Π=π ∼ M ulti(1; (cid:2)θA
π ) where
(cid:6)
πa = p(A = a|Π = π) and
θA
a θA
πa = 1. Also, we employ the superscripts θ and ξ to denote the
(cid:7)
i.e. θcl =
parameters of the Beta distribution of relevant and irrelevant components, respectively
z , (cid:2)θR
z , (cid:2)θE
. The set Θ of all VCC-FMM parameters is deﬁned by (cid:2)θU
zc , (cid:2)θφl ,
l , β ξ
cl ) and ξl = (αξ
(αθ
cl , β θ
l )
(cid:2)θZ , (cid:2)θC and θcl , ξl . The log-likelihood of a data set of N independent and identically distributed
observations D = {< u (i) , e(i) , (cid:2)x(i) , r(i) > |i = 1, . . . , N , u(i) ∈ U , e(i) ∈ E , (cid:2)x(i) ∈ X , r(i) ∈ R}
is given by:
D(cid:4)
M(cid:3)
K(cid:3)
N(cid:3)
log p(D|Θ) =

p(z )p(c)p(u(i) |z )p(e(i) |z )p(r(i) |z , c)

|θcl ) + l2 pb (x(i)
l

log

[l1 pb (x(i)
l

|ξl )]

i=1

z=1

c=1

l=1

(5)

The maximum likelihood (ML) approach which optimizes equation (5) w.r.t Θ is not appropriate
for learning VCC-FMM since both K and M are unknown. In addition, the likelihood increases
monotonically with the number of components and favors lower dimensions [5]. To overcome these
problems, we deﬁne a message length objective [16] for both the estimation of Θ and identiﬁcation
of K and M using MML [9][2]. This objective incorporates in addition to the log-likelihood, a
penalty term which encodes the data to penalize complex models as:
M M L(K, M ) = − log p(Θ) +
) − log p(D|Θ)
log |I (Θ)| +
1
1
s
(1 + log
(6)
12
2
2
In equation (6), |I (Θ)|, p(Θ), and s denote the Fisher information, prior distribution and the to-
tal number of parameters, respectively. The Fisher information of a parameter is the expectation
of the second derivatives with respect to the parameter of the minus log-likelihood.
It is com-
mon sense to assume an independence among the different groups of parameters which factor-
izes both |I (Θ)| and p(Θ) over the Fisher and prior distribution of different groups of parame-
ters, respectively. We approximate the Fisher information of the VCC-FMM from the complete
likelihood which assumes the knowledge about the values of hidden variables for each observation
< u(i) , e(i) , (cid:2)x(i) , r(i) >∈ D. The Fisher information of θ cl and ξl can be computed by following a
similar methodology of [1]. Also, we use the result found in [8] in computing the Fisher information
(cid:6)
(cid:7)NA−1 /
π of a discrete variable A with NA different values in a data set of N observations. |I ( (cid:2)θA
π )| is
(cid:5)NA
of (cid:2)θA
given by |I ((cid:2)θA
π )| =
a=1 θA
N p(Π = π)
πa [8], where p(Π = π) is the marginal prob-
ability of the parent Π. The graphical representation of of VCC-FMM does not involve variable
ancestors (parents of parents). Therefore, the marginal probabilities p(Π = π) are simply the pa-
(cid:9)
(cid:8)
(cid:5)Nr
zc )| is computed
rameters of the multinomial distribution of the parent variable. For example, |I ( (cid:2)θR
as: |I ((cid:2)θR
zc )| =
N Nr−1 (θC
z )Nr−1
r=1 θR
c θZ
/
zcr . In case of complete ignorance, it is common to
employ the Jeffrey’s prior for different groups of parameters. Replacing p(Θ) and I (Θ) in (6), and
(cid:2)D
(cid:2)D
(cid:2)K
after discarding the ﬁrst order terms, the MML objective is given by:
(cid:2)M
M M L(K, M ) = Np
l=1 log l2 + 1
z=1 log θZ
2 N Z
l=1 log l1 +
2 log N + M
p
z
c − log p(D|Θ)
2 (Nr − 1)
+ 1
c=1 log θC
(7)
with Np = 2D(M + 1) + K (Nu + Ne − 2) + M K (Nr − 1) and N Z
p = Nr + Nu + Ne − 3. For
ﬁxed values of K , M and D , the minimization of MML objective with respect to Θ is equivalent to
a maximum a posteriori (MAP) estimate with the following improper Dirichlet priors [9]:
p(1 , . . . , D ) ∝ D(cid:4)
p((cid:2)θZ ) ∝ K(cid:4)
p((cid:2)θC ) ∝ M(cid:4)
−1
−M
− N Z
− Nr −1
p
(θZ
(θC
z )
c )
2 ,
2
l1
l2
z=1
c=1
l=1

(8)

,

3.1 Estimation of parameters

(9)

We optimize the MML of the data set using the Expectation-Maximization (EM) algorithm in order
to estimate the parameters. In the E-step, the joint posterior probabilities of the latent variables given
the observations are computed as Q zci = p(z , c|u(i) , e(i) , (cid:2)x(i) , r(i) , ˆΘ):
(cid:5)
| ˆξl ))
| ˆθcl ) + l2 p(x(i)
(cid:2)
(cid:5)
z ˆθC
ze(i) ˆθR
c ˆθU
zu(i) ˆθE
ˆθZ
l (l1 p(x(i)
zcr(i)
l
l
Qzci =
| ˆθcl ) + l2 p(x(i)
| ˆξl ))
ˆθZ
z ˆθC
c ˆθU
zu(i) ˆθE
ze(i) ˆθR
l (l1 p(x(i)
zcr(i)
z ,c
l
l
(cid:11)
(cid:10) (cid:2)
(cid:10) (cid:2)
(cid:11)
In the M-step, the parameters are updated using the following equations:
(cid:2)
(cid:2)
z Qzci − Nr −1
c Qzci − N Z
(cid:10) (cid:2)
(cid:11) ,
(cid:10) (cid:2)
, 0
, 0
p
i
i
2
2
(cid:2)
(cid:2)
z Qzci − Nr −1
c Qzci − N Z
, 0
p
(cid:2)
(cid:2)
(cid:2)
2
2
(cid:2)
i:r(i)=r Qzci
ˆθR
zcr =
(cid:7)
i Qzci
− 1, 0
(cid:7)
− M , 0

i:e(i)=e
N ˆθZ
z
|ξl )
Qzci l2 pb (x(i)
l
|θcl )+l2 pb (x(i)
l1 pb (x(i)
l
l
Qzci l1 pb (Xil |θcl )
|ξl )
|θcl )+l2 pb (x(i)
l1 pb (x(i)
l
l

ˆθE
,
ze =
(cid:6) (cid:2)
(cid:6) (cid:2)
max
z ,c,i

i:u(i)=u
N ˆθZ
z

max
(cid:2)

max
(cid:2)
(cid:2)

c max

c Qzci

(cid:11)
, 0

1
l1

= 1 +

z max
(cid:2)

i

(10)

(11)

(12)

i

|ξl )

c Qzci

max

ˆθZ
z =

ˆθU
zu =

ˆθC
c =

z ,c,i

The parameters of Beta distributions θ cl and ξl are updated using the Fisher scoring method based
on the ﬁrst and second order derivatives of the MML objective [1].

4 Experiments

The beneﬁts of using feature selection and the contextual information are evaluated by considering
two variants: V-FMM and V-GD-FMM in addition the original VCC-FMM given by equation (4).
ze constant for all e ∈ E . On the
V-FMM does not handle the contextual information and assumes θ E
other hand, feature selection is not considered for V-GD-FMM by setting  l1 = 1 and pruning the
uninformative components ξ l for l = 1, . . . , D .

4.1 Data Set

We have collected ratings from 27 subjects who participated in the experiment (i.e. N u = 27) dur-
ing a period of three months. The participating subjects are graduate students in faculty of science.
Subjects received periodically (twice a day) a list of three images on which they assign relevance
degrees expressed on a ﬁve star rating scale (i.e. N r = 5). We deﬁne the context as a combination of
two attributes: location L = {in − campus, out − campus} inferred from the Internet Protocol (IP)
address of the subject, and time as T = (weekday , weekend) i.e N e = 4. A data set D of 13446
ratings is collected (N = 13446). We have used a collection of 4775 (i.e. N v = 4775) images col-
lected from Washington University [10] and collections of free photographs which we categorized
manually into 41 categories. For visual content characterization, we have employed both local and
global descriptors. For local descriptors, we use the 128-dimensional Scale Invariant Feature Trans-
form (SIFT) [11] to represent image patches. We employ vector quantization to SIFT descriptors
and we build a histogram for each image (“bag of visual words ”). The size of the visual vocabulary
is 500. For global descriptors, we used the color correlogram for image texture representation, and
the edge histogram descriptor. Therefore, a visual feature vector is represented in a 540-dimensional
space (D = 540). We measure the accuracy of the prediction by the Mean Absolute Error (MAE)
which is the average of the absolute deviation between the actual and predicted ratings.

4.2 First Experiment: Evaluating the inﬂuence of model order on the prediction accuracy

This experiment tries to investigate the relationship between the assumed model order deﬁned by K
and M on the prediction accuracy of VCC-FMM. It should be noticed that the ground truth number
is not known for our data set D. We run this experiment on a ground truth
of user classes K ∗
(artiﬁcial) data with known K and M . D GT is sampled from the preferences P 1 and P2 of two
most dissimilar subjects according to Pearson correlation coefﬁcients [14]. We sample ratings for
100 simulated users from the preferences P 1 and P2 only on images of four image classes. For each
user, we generate 80 ratings (∼ 20 ratings per context). Therefore, the ground truth model order is
K ∗ = 2 and M ∗ = 4. The choice of M ∗
is purely motivated by convenience of presentation since
similar performance was reported for higher values of M ∗
. We learn the VCC-FMM model using
one half of DGT for different choices of training and validation data. The model order deﬁned by
M = 15 and K = 15 is used to initialize EM algorithm.
Figure 3(a) shows that both K and M have been identiﬁed correctly on D GT since the lowest MML
was reported for the model order deﬁned by M = 4 and K = 2. The selection of the best model
order is important since it in ﬂuences the accuracy of the prediction (MAE) as illustrated by Figure
3(b). It should be noticed that the over-estimation of M (M > M ∗
) leads to more errors than the
over-estimation of K (K > K ∗
).

4.3 Second Experiment: Comparison with state-of-the-art

The aim of this experiment is to measure the contribution of the visual information and the user’s
context in making accurate predictions comparatively with some existing CF approaches. We make
comparisons with the Aspect model [7], Pearson Correlation (PCC)[14], Flexible Mixture Model
(FMM) [15], and User Rating Proﬁle (URP) [13]. For accurate estimators, we learn the URP model
using Gibs sampling. We retained for the previous algorithms, the model order that ensured the
lowest MAE.

(a) MML

(b) MAE

Figure 3: MML and MAE curves for different model orders on D GT .

Table 1: Averaged MAE over 10 runs of the different algorithms on D
V-FMM V-GD-FMM VCC-FMM
URP
FMM
PCC(baseline) Aspect
1.327
1.201
1.145
1.116
0.890
0.754
0.646
0.014
0.027
0.051
0.040
0.034
0.042
0.036
0.00%
9.49% 13.71% 15.90% 32.94%
43.18%
55.84%

Avg MAE
Deviation
Improvement

The ﬁrst ﬁve columns of table 1 show the added value provided by the visual information compara-
tively with pure CF techniques. For example, the improvement in the rating’s prediction reported by
V-FMM is 3.52% and 1.97% comparatively with FMM and URP, respectively. The algorithms (with
context information) shown in the last two columns have also improved the accuracy of the predic-
tion comparatively with the others (at least 15.28%). This explains the importance of the contextual
information on user preferences. Feature selection is also important since VCC-FMM has reported
a better accuracy (14.45%) than V-GD-FMM. Furthermore, it is reported in ﬁgure 4(a) that VCC-
FMM is less sensitive to data sparsity (number of ratings per user) than pure CF techniques. Finally,
the evolution of the average MAE provided VCC-FMM for different proportions of unrated images
remains under < 25% for up to 30% of unrated images as shown in Figure 4(b). We explain the
stability of the accuracy of VCC-FMM for data sparsity and new images by the visual information
since only cluster representatives need to be rated.

(a) Data sparsity

(b) new images

Figure 4: MAE curves with error bars on the data set D.

5 Conclusions

This paper has motivated theoretically and empirically the importance of both feature selection and
model order identiﬁcation from unlabeled data as important issues in content-based image sugges-
tion. Experiments on collected data showed also the importance of the visual information and the
user’s context in making accurate suggestions.

Acknowledgements

The completion of this research was made possible thanks to Natural Sciences and Engineering Re-
search Council of Canada (NSERC), Bell Canada’s support through its Bell University Laboratories
R&D program and a start-up grant from Concordia University.

References

[1] N. Bouguila and D. Ziou. High-Dimensional Unsupervised Selection and Estimation of a Finite Gen-
eralized Dirichlet Mixture Model Based on Minimum Message Length. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 29(10):1716–1731, 2007.
[2] S. Boutemedjet, N. Bouguila, and D. Ziou. Unsupervised Feature and Model Selection for Generalized
Dirichlet Mixture Models.
In Proc. of International Conference on Image Analysis and Recognition
(ICIAR), pages 330–341. LNCS 4633, 2007.
[3] S. Boutemedjet and D. Ziou. Content-based Collaborative Filtering Model for Scalable Visual Document
Recommendation. In Proc. of IJCAI-2007 Workshop on Multimodal Information Retrieval, pages 11–18,
2007.
[4] S. Boutemedjet and D. Ziou. A Graphical Model for Context-Aware Visual Content Recommendation.
IEEE Transactions on Multimedia, 10(1):52–62, 2008.
[5] J. G. Dy and C. E. Brodley. Feature Selection for Unsupervised Learning. Journal of Machine Learning
Research, 5:845–889, 2004.
[6] I. Guyon and A. Elisseeff. An Introduction to Variable and Feature Selection. Journal of Machine
Learning Research, 3:1157–1182, 2003.
[7] T. Hofmann. Latent Semantic Models for Collaborative Filtering. ACM Transactions on Information
Systems, 22(1):89–115, 2004.
[8] P. Kontkanen, P. Myllymki, T. Silander, H. Tirri, and P. Grnwald. On Predictive Distributions and
Bayesian Networks. Statistics and Computing, 10(1):39–54, 2000.
[9] M. H. C. Law, M.A.T. Figueiredo, and A. K. Jain. Simultaneous Feature Selection and Clustering Using
Mixture Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(9), 2004.
[10] J. Li and J. Z. Wang. Automatic Linguistic Indexing of Pictures by a Statistical Modeling Approach.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(9):49–68, 2003.
[11] D.G. Lowe. Distinctive Image Features From Scale-Invariant Keypoints. International Journal of Com-
puter Vision, 60(2):91–110, 2004.
[12] J. Muramastsu M. Pazzani and D. Billsus. Syskill and Webert:Identifying Interesting Web Sites. In In
Proc. of the 13th National Conference on Artiﬁcial Intelligence (AAAI) , 1996.
[13] B. Marlin. Modeling User Rating Proﬁles For Collaborative Filtering. In Proc. of Advances in Neural
Information Processing Systems 16 (NIPS), 2003.
[14] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl. Grouplens: An Open Architecture for
Collaborative Filtering of Netnews. In Proc. of ACM Conference on Computer Supported Cooperative
Work, 1994.
[15] L. Si and R. Jin. Flexible Mixture Model for Collaborative Filtering.
Conference on Machine Learning (ICML), pages 704–711, 2003.
[16] C. Wallace. Statistical and Inductive Inference by Minimum Message Length. Information Science and
Statistics. Springer, 2005.

In Proc. of 20th International


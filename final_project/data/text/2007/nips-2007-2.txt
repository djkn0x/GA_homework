Variational Inference for Diffusion Processes

C ´edric Archambeau
University College London
c.archambeau@cs.ucl.ac.uk

Manfred Opper
Technical University Berlin
opperm@cs.tu-berlin.de

Yuan Shen
Aston University
y.shen2@aston.ac.uk

Dan Cornford
Aston University
d.cornford@aston.ac.uk

John Shawe-Taylor
University College London
jst@cs.ucl.ac.uk

Abstract

Diffusion processes are a family of continuous-time continuous-state stochastic
processes that are in general only partially observed. The joint estimation of the
forcing parameters and the system noise (volatility) in these dynamical systems is
a crucial, but non-trivial task, especially when the system is nonlinear and multi-
modal. We propose a variational treatment of diffusion processes, which allows
us to compute type II maximum likelihood estimates of the parameters by sim-
ple gradient techniques and which is computationally less demanding than most
MCMC approaches. We also show how a cheap estimate of the posterior over the
parameters can be constructed based on the variational free energy.

1

Introduction

Continuous-time diffusion processes, described by stochastic differential equations (SDEs), arise
naturally in a range of applications from environmental modelling to mathematical ﬁnance [13]. In
statistics the problem of Bayesian inference for both the state and parameters, within partially ob-
served, non-linear diffusion processes has been tackled using Markov Chain Monte Carlo (MCMC)
approaches based on data augmentation [17, 11], Monte Carlo exact simulation methods [6], or
Langevin / hybrid Monte Carlo methods [1, 3]. Within the signal processing community solutions
to the so called Zakai equation [12] based on particle ﬁlters [8], a variety of extensions to the Kalman
ﬁlter/smoother [2, 5] and mean ﬁeld analysis of the SDE together with moment closure methods [10]
have also been proposed. In this work we develop a novel variational approach to the problem of
approximate inference in continuous-time diffusion processes, including a marginal likelihood (ev-
idence) based inference technique for the forcing parameters. In general, joint parameter and state
inference using naive methods is complicated due to dependencies between state and system noise
parameters.
We work in continuous time, computing distributions over sample paths1 , and discretise only in
our posterior approximation, which has advantages over methods based on discretising the SDE
directly [3]. The approximate inference approach we describe is more computationally efﬁcient than
competing Monte Carlo algorithms and could be further improved in speed by deﬁning a variety
of sub-optimal approximations. The approximation is also more accurate than existing Kalman
smoothing methods applied to non-linear systems [4]. Ultimately, we are motivated by the critical
requirement to estimate parameters within large environmental models, where at present only a small
number of Kalman ﬁlter/smoother based estimation algorithms have been attempted [2], and there
have been no likelihood based attempts to estimate the system noise forcing parameters.

1A sample path is a continuous-time realisation of a stochatic process in a certain time interval. Hence, a
sample path is an inﬁnite dimensional object.

1

In Section 2 and 3, we introduce the formalism for a variational treatment of partially observed diffu-
sion processes with measurement noise and we provide the tools to estimate the optimal variational
posterior process [4]. Section 4 deals with the estimation of the drift and the system noise parame-
ter, as well as the estimation of the optimal initial conditions. Finally, the approach is validated on a
bi-stable nonlinear system in Section 5. In this context, we also discuss how to construct an estimate
of the posterior distribution over parameters based on the variational free energy.

2 Diffusion processes with measurement error
Consider the continuous-time continuous-state stochastic process X = {Xt , t0 ≤ t ≤ tf }. We
assume this process is a d-dimensional diffusion process. Its time evolution is described by the
following SDE (to be interpreted as an Ito stochastic integral):
dWt ∼ N (0, dtI).
dXt = fθ (t, Xt ) dt + Σ1/2 dWt ,
(1)
The nonlinear vector function fθ deﬁnes the deterministic drift and the positive semi-deﬁnite matrix
Σ ∈ Rd×d is the system noise covariance. The diffusion is modelled by a d-dimensional Wiener
process W = {Wt , t0 ≤ t ≤ tf } (see e.g. [13] for a formal deﬁnition). Eq. (1) deﬁnes a process
with additive system noise. This might seem restrictive at ﬁrst sight. However, it can be shown
[13, 17, 6] that a range of state dependent stochastic forcings can be transformed into this form.
It is further assumed that only a small number of discrete-time latent states are observed and that the
observations are subject to measurement error. We denote the set of observations at the discrete times
{tn}N
n=1 by Y = {yn}N
n=1 and the corresponding latent states by {xn}N
n=1 , with xn = Xt=tn .For
simplicity, the measurement noise is modelled by a zero-mean multivariate Gaussian density,with
covariance matrix R ∈ Rd×d .

3 Approximate inference for diffusion processes
(cid:28)
(cid:29)
Our approximate inference scheme builds on [4] and is based on a variational inference approach
(see for example [7]). The aim is to minimise the variational free energy, which is deﬁned as follows:
ln p(Y , X |θ , Σ)
FΣ (q , θ) = −
, X = {Xt , t0 ≤ t ≤ tf },
(2)
q(X |Σ)
q
where q(X |Σ) is an approximate posterior process over sample paths in the interval [t0 , tf ] and θ
are the parameters, excluding the stochastic forcing covariance matrix Σ. Hence, this quantity is an
upper bound to the negative log-marginal likelihood:
− ln p(Y |θ , Σ) = FΣ (q , θ) − KL [q(X |Σ)(cid:107)p(X |Y , θ , Σ)] ≤ FΣ (q , θ).
(3)
As noted in Appendix A, this bound is ﬁnite if the approximate process is another diffusion process
with a system noise covariance chosen to be identical to that of the prior process induced by (1).
The standard approach for learning the parameters in presence of latent variables is to use an EM
type algorithm [9]. However, since the variational distribution is restricted to have the same system
noise covariance (see Appendix A) as the true posterior, the EM algorithm would leave this covari-
ance completely unchanged in the M step and cannot be used for learning this crucial parameter.
Therefore, we adopt a different approach, which is based on a conjugate gradient method.

3.1 Optimal approximate posterior process

We consider an approximate time-varying linear process with the same diffusion term, that is the
same system noise covariance:
dWt ∼ N (0, dtI),
dXt = g(t, Xt ) dt + Σ1/2 dWt ,
(4)
where g(t, x) = −A(t)x+b(t), with A(t) ∈ Rd×d and b(t) ∈ Rd . In other words, the approximate
posterior process q(X |Σ) is restricted to be a Gaussian process [4]. The Gaussian marginal at time
t is deﬁned as follows:

q(Xt |Σ) = N (Xt |m(t), S(t)),

t0 ≤ t ≤ tf ,

(5)

2

where m(t) ∈ Rd and S(t) ∈ Rd×d are respectively the marginal mean and the marginal covariance
at time t. In the rest of the paper, we denote q(Xt |Σ) by the shorthand notation qt .
For ﬁxed parameters θ and assuming that there is no observation at the initial time t0 , the optimal
approximate posterior process q(X |Σ) is the one minimizing the variational free energy, which is
(cid:90) tf
(cid:90) tf
Eobs (t) (cid:88)
given by (see Appendix A)
FΣ (q , θ) =
δ(t − tn ) dt + KL [q0 (cid:107)p0 ] .
Esde (t) dt +
t0
t0
n
(cid:10)(fθ (t, Xt ) − g(t, Xt ))(cid:62)Σ−1 (fθ (t, Xt ) − g(t, Xt ))(cid:11)
The function δ(t) is Dirac’s delta function. The energy functions Esde (t) and Eobs (t) are deﬁned as
follows:
1
(cid:10)(Yt − Xt )(cid:62)R−1 (Yt − Xt )(cid:11)
2
qt
1
1
ln |R|.
+ d
Eobs (t) =
ln 2π +
2
2
2
qt
where {Yt , t0 ≤ t ≤ tf } is the underlying continuous-time observable process.

Esde (t) =

(6)

,

(7)

(8)

3.2 Smoothing algorithm

The variational parameters to optimise in order to ﬁnd the optimal Gaussian process approximation
are A(t), b(t), m(t) and S(t). For a linear SDE with additive system noise, it can be shown that
the time evolution of the means and the covariances are described by a set of ordinary differential
equations [13, 4]:
˙m(t) = −A(t)m(t) + b(t),
(9)
˙S(t) = −A(t)S(t) − S(t)A(cid:62) (t) + Σ,
(10)
where ˙ denotes the time derivtive. These equations provide us with consistency constraints for the
(cid:90) tf
marginal means and the marginal covariances along sample paths. To enforce these constraints we
(cid:62) (t)(cid:0) ˙m(t) + A(t)m(t) − b(t)(cid:1) dt
formulate the Lagrangian
Lθ ,Σ = FΣ (q , θ) −
(cid:90) tf
(cid:16) ˙S(t) + 2A(t)S(t) − Σ
(cid:110)
(cid:17)(cid:111)
λ
t0
−
tr
Ψ(t)
(11)
dt,
t0
where λ(t) ∈ Rd and Ψ(t) ∈ Rd×d are time dependent Lagrange multipliers, with Ψ(t) symmetric.
First, taking the functional derivatives of Lθ ,Σ with respect to A(t) and b(t) results in the following
gradient functions:
∇ALθ ,Σ (t) = ∇AEsde (t) − λ(t)m(cid:62) (t) − 2Ψ(t)S(t),
∇bLθ ,Σ (t) = ∇bEsde (t) + λ(t).
The gradients ∇AEsde (t) and ∇bEsde (t) are derived in Appendix B.
Secondly, taking the functional derivatives of Lθ ,Σ with respect to m(t) and S(t), setting to zero
and rearranging leads to a set of ordinary differential equations, which describe the time evolution
of the Lagrange multipliers, along with jump conditions when there are observations:
˙λ(t) = −∇mEsde (t) + A(cid:62) (t)λ(t), λ+
n − ∇mEobs (t)|t=tn ,
−
n = λ
˙Ψ(t) = −∇SEsde (t) + 2Ψ(t)A(t), Ψ+
n − ∇SEobs (t)|t=tn .
n = Ψ−
The optimal variational functions can be computed by means of a gradient descent technique, such
as the conjugate gradient (see e.g., [16]). The explicit gradients with respect to A(t) and b(t)
are given by (12) and (13). Since m(t), S(t), λ(t) and Ψ(t) are dependent on these parameters,
one needs also to take the corresponding implicit derivatives into account. However, these implicit
gradients vanish if the consistency constraints for the means (9) and the covariances (10), as well as
the ones for the Lagrange multipliers (14-15), are satisﬁed. One way to achieve this is to perform a
forward propagation of the means and the covariances, followed by a backward propagation of the
Lagrange multipliers, and then to take a gradient step. The resulting algorithm for computing the
optimal posterior q(X |Σ) over sample paths is detailed in Algorithm 1.

(14)
(15)

(12)
(13)

3

Algorithm 1 Compute the optimal q(X |Σ).
1: input(m0 , S0 , θ , Σ, t0 , tf , ∆t, ω)
2: K ← (tf − t0 )/∆t
3: initialise {Ak , bk }k≥0
4: repeat
for k = 0 to K − 1 do
5:
mk+1 ← mk − (Akmk − bk )∆t
6:
Sk+1 ← Sk − (Ak Sk + SkA(cid:62)
k − Σ)∆t
7:
end for{forward propagation}
8:
for k = K to 1 do
9:
λk−1 ← λk + (∇mEsde |t=tk − A(cid:62)
k λk )∆t
10:
Ψk−1 ← Ψk + (∇SEsde |t=tk − 2ΨkAk )∆t
11:
if observation at tk−1 then
12:
λk−1 ← λk−1 + ∇mEobs |t=tk−1
13:
Ψk−1 ← Ψk−1 + ∇SEobs |t=tk−1
14:
end if{jumps}
15:
end for{backward sweep (adjoint operation)}
16:
update {Ak , bk }k≥0 using the gradient functions (12) and (13)
17:
18: until minimum of Lθ ,Σ is attained {optimisation loop}
19: return {Ak , bk , mk , Sk , λk , Ψk }k≥0

4 Parameter estimation

The parameters to optimise include the parameters of the prior over the initial state, the drift func-
tion parameters and the system noise covariance. The estimation of the parameters related to the
observable process are not discussed in this work, although it is a straightforward extension.
The smoothing algorithm described in the previous section computes the optimal posterior process
by providing us with the stationary solution functions A(t) and b(t). Therefore, when subsequently
optimising the parameters we only need to compute their explicit derivatives; the implicit ones
vanish since ∇ALθ ,Σ = 0 and ∇bLθ ,Σ = 0. Before computing the gradients, we integrate (11) by
(cid:90) tf
(cid:111)
(cid:110)
(cid:62) (t)(cid:0)A(t)m(t) − b(t)(cid:1) − ˙λ
parts to make the boundary conditions explicit. This leads to
Lθ ,Σ = FΣ (q , θ) −
(cid:90) tf
(cid:110)
(cid:111)
(t)m(t)
Ψ(t)(cid:0)2A(t)S(t) − Σ(cid:1) − ˙Ψ(t)S(t)
λ
t0
dt − tr {Ψf Sf } + tr {Ψ0S0} ,
t0
At the ﬁnal time tf , there are no consistency constraints, that is λf and Ψf are both equal to zero.

(cid:62)
f mf + λ

dt − λ

−

tr

(cid:62)

(cid:62)
0 m0

(16)

4.1
Initial state
The initial variational posterior q(x0 ) is chosen equal to N (x0 |m0 , S0 ) to ensure that the approxi-
mate process is a Gaussian one. Taking the derivatives of (16) with respect to m0 and S0 results in
(cid:1) ,
(cid:0)τ −1
the following expressions:
1
0 I − S−1
∇m0 Lθ ,Σ = λ0 + τ −1
0 (m0 − µ0 ), ∇S0 Lθ ,Σ = Ψ0 +
2
0
where the prior p(x0 ) is assumed to be an isotropic Gaussian density with mean µ0 . Its variance τ0
is taken sufﬁciently large to give a broad prior.

(17)

4.2 Drift

(cid:90) tf
The gradients for the drift function parameters θf only depend on the total energy associated to the
SDE. Their general expression is given by
∇θf Lθ ,Σ =
t0

∇θf Esde (t) dt,

(18)

4

(cid:68)
(cid:69)
(fθ (t, Xt ) − g(t, Xt ))(cid:62) Σ−1∇θf fθ (t, Xt )
where ∇θf Esde (t) =
. Note that the observations
qt
do play a role in this gradient as they enter through g(t, Xt ) and the expectation w.r.t. q(Xt |Σ).

4.3 System noise

(19)

Σ−1 .

Estimating the system noise covariance (or volatility) is essential as the system noise, together with
the drift function, determines the dynamics. In general, this parameter is difﬁcult to estimate using
an MCMC approach because the efﬁciency is strongly dependent on the discrete approximation of
the SDE and most methods break down when the time step ∆t gets too small [11, 6]. For example
in a Bayesian MCMC approach, which alternates between sampling paths and parameters, the latent
paths imputed between observations must have a system noise parameter which is arbitrarily close
to its previous value in order to be accepted by a Metropolis sampler. Hence, the algorithm becomes
extremely slow. Note, that for the same reason, a naive EM algorithm within our approach breaks
down. However, in our method, we can simply compute approximations to the marginal likelihood
and its gradient directly. In the next section, we will compare our results to a direct MCMC estimate
of the marginal likelihood which is a time consuming method.
(cid:90) tf
(cid:90) tf
The gradient of (16) with respect to Σ is given by
∇ΣLθ ,Σ =
∇ΣEsde (t) dt +
(fθ (t, Xt ) − g(t, Xt )) (fθ (t, Xt ) − g(t, Xt ))(cid:62)(cid:69)
2 Σ−1 (cid:68)
Ψ(t) dt,
t0
t0
where ∇ΣEsde (t) = − 1
qt
5 Experimental validation on a bi-stable system
fθ (t, x) = 4x (cid:0)θ − x2 (cid:1) ,
In order to validate the approach, we consider the 1 dimensional double-well system:
θ > 0,
(20)
where fθ (t, x) is the drift function. This dynamical system is highly nonlinear and its stationary
distribution is multi-modal. It has two stable states, one in x = −θ and one in x = +θ . The system
is driven by the system noise, which makes it occasionally ﬂip from one well to the other.
In the experiments, we set the drift parameter θ to 1, the system noise standard deviation σ to 0.5 and
the measurement error standard deviation r to 0.2. The time step for the variational approximation
is set to ∆t = 0.01, which is identical to the time resolution used to generate the original sample
path. In this setting, the exit time from one of the wells is 4000 time units [15]. In other words, the
transition from one well to the other is highly unlikely in the window of roughly 8 time units that
we consider and where a transition occurs.
Figure 1(a) compares the variational solution to the outcomes of a hybrid MCMC simulation of the
posterior process using the true parameter values. The hybrid MCMC approach was proposed in
[1]. At each step of the sampling process, an entire sample path is generated. In order to keep the
acceptance of new paths sufﬁciently high, the basic MCMC algorithm is combined with ideas from
Molecular Dynamics, such that the MCMC sampler moves towards regions of high probability in the
state space. An important drawback of MCMC approaches is that it might be extremely difﬁcult to
monitor their convergence and that they may require a very large number of samples before actually
converging. In particular, over 100, 000 sample paths were necessary to reach convergence in the
case of the double-well system.
The solution provided by the hybrid MCMC is here considered as the base line solution. One can ob-
serve that the variational solution underestimates the uncertainty (smaller error bars). Nevertheless,
the time of the transition is correctly located. Convergence of the smoothing algorithm was achieved
in approximately 180 conjugate gradient steps, each one involving a forward and backward sweep.
The optimal parameters and the optimal initial conditions for the variational solution are given by
ˆθf = 0.85,
ˆs0 = 0.45.
ˆm0 = 0.88,
ˆσ = 0.72,
(21)
Convergence of the outer optimization loop is typically reached after less then 10 conjugate gradient
steps. While the estimated value for the drift parameter is within 15% percent from its true value,

5

(a)

(b)

Figure 1: (a) Variational solution (solid) compared to the hybrid MCMC solution (dashed), using
the true parameter values. The curves denote the mean paths and the shaded regions are the two-
standard deviation noise tubes. (b) Posterior of the system noise variance (diffusion term). The plain
curve and the dashed curve are respectively the approximations of the posterior shape based on the
variational free energy and MCMC.

the deviation of the system noise is worse. Deviations may be explained by the fact that the number
of observations is relatively small. Furthermore, we have chosen a sample path which contains
a transition between the two wells within a small time interval and is thus highly untypical with
respect to the prior distribution. This fact was experimentally assessed by estimating the parameters
on a sample path without transition, in a time window of the same size. In this case, we obtained
estimate roughly within 5% of the true parameter values: ˆσ = 0.46 and ˆθf = 0.92. Finally, it turns
out that our estimate for ˆσ is close to the one obtained from the MCMC approach as discussed next.

Posterior distribution over the parameters
Interestingly, minimizing the free energy Fσ2 for different values of σ provides us with much more
information than a single point estimate for the parameters [14]. Using a suitable prior over p(σ),
we can approximate the posterior over the system noise variance via
p(σ2 |Y ) ∝ e−Fσ2 p(σ2 ),
(22)
where we take e−Fσ2 (at its minimum) as an approximation to the marginal likelihood of the ob-
servations p(Y |σ2 ). To illustrate this point, we assume a non-informative Gamma prior p(σ2 ) =
G (α, β ), with α = 10−3 and β = 10−3 . A comparison with preliminary MCMC estimates for
p(Y |σ2 ) for θ = 1 and a set of system noise variances indicates that the shape of our approximation
is a reasonable indicator of the shape of the posterior. Figure 1(b) shows that at least the mean and
the variance of the density come out fairly well.

6 Conclusion

We have presented a variational approach to the approximate inference of stochastic differential
equations from a ﬁnite set of noisy observations. So far, we have tested the method on a one dimen-
sional bi-stable system only. Comparison with a Monte Carlo approach suggests that our method
can reproduce the posterior mean fairly well but underestimates the variance in the region of the
transition. Parameter estimates also agree well with the MC predictions.
In the future, we will extend our method in various directions. Although our approach is based on
a Gaussian approximation of the posterior process, we expect that one can improve on it and obtain
non-Gaussian predictions at least for various marginal posterior distributions, including that of the
latent variable Xt at a ﬁxed time t. This should be possible by generalising our method for the
computation of a non-Gaussian shaped probability density for the system noise parameter using the
free energy. An important extension of our method will be to systems with many degrees of freedom.

6

012345678!2!1.5!1!0.500.511.52tx0.30.40.50.60.70.80.900.20.40.60.811.21.41.6!2We hope that the possibility of using simpler suboptimal parametrisations of the approximating
Gaussian process will allow us to obtain a tractable inference method that scales well to higher
dimensions.

Acknowledgments

This work has been funded by the EPSRC as part of the Variational Inference for Stochastic Dynamic
Environmental Models (VISDEM) project (EP/C005848/1).

A The Kullback-Leibler divergence interpreted as a path integral

In this section, we show that the Kullback-Leibler divergence between the posterior process
p(Xt |Y , θ , Σ) and its approximation q(X |Σ) can be interpreted as a path integral over time.
It
is an average over all possible realisations, called sample paths, of the continuous-time (i.e., inﬁnite
dimensional) random variable described by the SDE in the time interval under consideration.
Consider the Euler-Muryama discrete approximation (see for example [13]) of the SDE (1) and its
linear approximation (4):

(25)

(26)

N (xk+1 |xk + gk∆t, Σ∆t),

∆xk = fk∆t + Σ1/2∆wk ,
(23)
∆xk = gk∆t + Σ1/2∆wk ,
(24)
where ∆xk ≡ xk+1 − xk and wk ∼ N (0, ∆tI). The vectors fk and gk are shorthand notations
for fθ (tk , xk ) and g(tk , xk ). Hence, the joint distributions of discrete sample paths {xk }k≥0 for the
p(x0 , . . . , xK |Σ) = p(x0 ) (cid:89)
true process and its approximation follow from the Markov property:
N (xk+1 |xk + fk∆t, Σ∆t),
q(x0 , . . . , xK |Σ) = q(x0 ) (cid:89)
k>0
k>0
where p(x0 ) is the prior on the intial state x0 and q(x0 ) is assumed to be Gaussian. Note thate we
do not restrict the variational posterior to factorise over the latent states.
(cid:90)
KL [q(cid:107)p] = KL [q(x0 )(cid:107)p(x0 )] − (cid:88)
The Kullback-Leibler divergence between the two discretized prior processes is given by
q(xk ) (cid:104)ln p(xk+1 |xk )(cid:105)q(xk+1 |xk ) dxk
(cid:88)
(cid:10)(fk − gk )(cid:62)Σ−1 (fk − gk )(cid:11)
k>0
1
= KL [q(x0 )(cid:107)p(x0 )] +
q(xk ) ∆t,
2
k>0
where we omitted the conditional dependency on Σ for simplicity. The second term on the right
hand side is a sum in ∆t. As a result, taking limits for ∆t → 0 leads to a proper Riemann integral,
(cid:90) tf
(cid:10)(ft − gt )(cid:62)Σ−1 (ft − gt )(cid:11)
which deﬁnes an integral over the average sample path:
1
KL [q(X |Σ)(cid:107)p(X |θ , Σ)] = KL [q0 (cid:107)p0 ] +
(27)
dt,
2
qt
t0
where X = {Xt , t0 ≤ t ≤ tf } denotes the stochastic process in the interval [t0 , tf ]. The distribu-
tion qt = q(Xt |Σ) is the marginal at time t for a given system noise covariance Σ.
It is important to realise that the KL between the induced prior process and its approximation is
ﬁnite because the system noise covariances are chosen to be identical.
If this was not the case,
the normalizing constants of p(xk+1 |xk ) and q(xk+1 |xk ) would not cancel. This would result in
KL → ∞ when ∆t → 0.
FΣ (q , θ) = − (cid:88)
If we assume that the observations are i.i.d., it follows also that
(cid:104)ln p(yn |xn )(cid:105)q(xn ) + KL [q(X |Σ)(cid:107)p(X |θ , Σ)] .
n
Clearly, minimising this expression with respect to the variational parameters for a given system
noise Σ and for a ﬁxed parameter vector θ is equivalent to minimising the KL between the vari-
ational posterior q(X |Σ) and the true posterior p(X |Y , θ , Σ), since the normalizing constant is
independent of sample paths.

7

B The gradient functions
The general expressions for the gradients of Esde (t) with respect to the variational functions are
∇AEsde (t) = Σ−1 (cid:110)(cid:104)∇x fθ (t, Xt )(cid:105)qt
(cid:111)
given by
∇bEsde (t) = Σ−1 (cid:110)− (cid:104)fθ (t, Xt )(cid:105)qt
(cid:111)
S(t) − ∇bEsde (t)m(cid:62) (t),
+ A(t)
fθ (t, Xt ) (Xt − m(t))(cid:62)(cid:69)
(cid:68)
− A(t)m(t) + b(t)
where (cid:104)∇x fθ (t, Xt )(cid:105)qt
S(t) =
qt

is invoked in order to obtain (28).

,

(28)

(29)

References
[1] F. J. Alexander, G. L. Eyink, and J. M. Restrepo. Accelerated Monte Carlo for optimal estimation of time
series. Journal of Statistical Physics, 119:1331–1345, 2005.
[2] J. D. Annan, J. C. Hargreaves, N. R. Edwards, and R. Marsh. Parameter estimation in an intermediate
complexity earth system model using an ensemble Kalman ﬁlter. Ocean Modelling, 8:135–154, 2005.
[3] A. Apte, M. Hairer, A. Stuart, and J. Voss. Sampling the posterior: An approach to non-Gaussian data
assimilation. Physica D, 230:50–64, 2007.
[4] C. Archambeau, D. Cornford, M. Opper, and J. Shawe-Taylor. Gaussian process approximation of
stochastic differential equations. Journal of Machine Learning Research: Workshop and Conference
Proceedings, 1:1–16, 2007.
[5] D. Barber. Expectation correction for smoothed inference in switching linear dynamical systems. Journal
of Machine Learning Research, 7:2515–2540, 2006.
[6] A. Beskos, O. Papaspiliopoulos, G. Roberts, and P. Fearnhead. Exact and computationally efﬁcient
likelihood-based estimation for discretely observed diffusion processes (with discussion). Journal of
the Royal Statistical Society B, 68(3):333–382, 2006.
[7] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, 2006.
[8] D. Crisan and T. Lyons. A particle approximation of the solution of the Kushner-Stratonovitch equation.
Probability Theory and Related Fields, 115(4):549–578, 1999.
[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via EM
algorithm. Journal of the Royal Statistical Society B, 39(1):1–38, 1977.
[10] G. L. Eyink, J. L. Restrepo, and F. J. Alexander. A mean ﬁeld approximation in data assimilation for
nonlinear dynamics. Physica D, 194:347–368, 2004.
[11] A. Golightly and D. J. Wilkinson. Bayesian inference for nonlinear multivariate diffusion models observed
with error. Computational Statistics and Data Analysis, 2007. Accepted.
[12] A. H. Jazwinski. Stochastic Processes and Filtering Theory. Academic Press, New York, 1970.
[13] Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations. Springer,
Berlin, 1999.
[14] H. Lappalainen and J. W. Miskin. Ensemble learning. In M. Girolami, editor, Advances in Independent
Component Analysis, pages 76–92. Springer-Verlag, 2000.
[15] R. N. Miller, M. Ghil, and F. Gauthiez. Advanced data assimilation in strongly nonlinear dynamical
systems. Journal of the Atmospheric Sciences, 51:1037–1056, 1994.
[16] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 2000.
[17] G. Roberts and O. Stramer. On inference for partially observed non-linear diffusion models using the
Metropolis-Hastings algorithm. Biometirka, 88:603–621, 2001.

8


CS229 FINAL PROJECT, DECEMBER 14, 2007

1

HMM Analysis and Synthesis
of Acoustic Drum Signals
Nicholas J. Bryan
Center for Computer Research in Music and Acoustics

Abstract— Hidden-Markov Models (HMMs) have been widely
used for speech processing, understanding, and synthesis with
great success. The purpose of this work is to apply this prior
knowledge and investigate the effectiveness of HMMs on short-
duration percussive musical signals. Three main topics of interest
are investigated: isolated instrument recognition, isolated rhythm
transcription for the purpose of genre recognition, and isolated
instrument synthesis. Overall, satisfactory results were achieved
with clear motivation for improvement.

I . IN TRODUC T ION
P ERCUSSION, drums, and other rhythmic acoustic signals
and patterns are an integral aspect of modern day music.
Entire music genres, careers, and extensive software appli-
cations are based off of rhythm or musical patterns through
time, providing a large motivation to learn and model such
information. More speciﬁcally, large databases of previously
recorded drum sounds are common place throughout the music
industry with little or no method of automatically labeling,
identifying, or searching with respect to musical parameters,
forcing manual searching with real-time auditory assessment.
To attack this issue and provide insight on such problems
as “search-by-rhythm” or “search-by-rhythm-genre”, isolated
instrument recognition, short-duration rhythmic recognition,
and isolated re-synthesis of acoustic drum set signals of typical
performance are investigated. To model the time-series infor-
mation of both the acoustic pressure information of the musical
samples as well as the rhythmic information of a musical
measure, continuous and discrete observation Hidden Markov
models are used respectively. Once the isolated instrument
recognition HMM models are complete, synthesis can be
performed using the learned HMM models. For an overview
of percussion transcription techniques see [1].

I I . F EATUR E V EC TOR S
With respect to the input feature vectors, the input time-
domain audio signals are converted to Mel-frequency cepstral
coefﬁcients (MFCCs) [2]. Introduced in [3], MFCCs attempt
to more closely model the human auditory response, while
exploiting the decorrelating property of the cepstrum [4]. The
cepstrum of an audio signal can informally be deﬁned as the
inverse Fourier transform of the logarithm magnitude of the
Fourier transform. The synthesis step (inverse transform) in
application actually uses the discrete cosine transform and
in [5] was shown to be effective in approximating principal
component analysis. Thirteen MFCC coefﬁcients are generated

out of a 512-point FFT using .025 seconds windows overlap-
ping every .010 seconds. Ideally, the MFCC data effectively
captures a pitch-independent frequency response of the audio
signal over time. Using such feature vectors also allows for a
respectable synthesis of the instrument sounds.

I I I . I SO LATED R ECOGN I T ION
To break down the problem of isolated acoustic drum set
recognition, six basis classes are used to represent each major
sub-instrument of a typical drum set (s = snare drum, b =
bass drum, h = hi-hat, t = toms, c = cymbals, and si =
silence). Two-hundred audio samples for each basis class are
used for learning from a commercially available drum sample
database [6]. The EM (or in this context the Baum-Welch)
algorithm is used for learning the continuous observations
using a single Gaussian mixture model [7], [8]. To effectively
model the remaining combinations of drum sounds such as a
snare drum and bass drum played simultaneously (sb = snare
+ bass), combination data is created via random sampling,
adding together, and amplitude normalization from the basis
class audio ﬁles [9]. Using all physically realizable combina-
tions (no more than four simultaneous sub-instruments at a
time as well as silence exclusion) of the six basis classes, a
total of 28 overall classes were used. Additionally, the level of
combination of each class was identiﬁed with a corresponding
complexity level (basis class = 1,
two added together =
2, etc). For each class, a 5-state left-to-right state-sequence
HMM model was generated. For recognition, a maximum log-
likelihood classiﬁcation is used.

Fig. 1. Continuous Observation HMM State Sequence

IV. RHY THM IC TRAN SCR I P T ION FOR G ENR E
R ECOGN I T ION
Once each isolated recognition model is learned, musical
sequence or rhythmic recognition of audio ﬁles can be applied

ContinuousS1S2S3MFCC parametersCS229 FINAL PROJECT, DECEMBER 14, 2007

2

by using multiple instances of isolated recognition. The se-
quence or rhythmic recognition must decode each event of the
sub-instruments or sub-instrument combinations. Speciﬁcally,
isolated recognition must be processed on each event within
the rhythmic drum pattern (typically one to two measures),
where an event occurs at every smallest division of musical
time or beat. Additionally, each rhythm data example must be
normalized with respect to time. Once time normalized, the
sequence recognition becomes straight forward and indepen-
dent of beat detection errors. Reason 3.0 with Dr. REX loop
player (a commercially available music production software)
was used to set a standard 120 beats per minute for each multi-
measure example of a standard 4/4 time signature with 16th
note quantization (i.e. for a two measure pattern, 32 isolated
recognition classiﬁcations well be made). For training and

As a result, a source-ﬁlter model can be used with a shaped-
noise input signal to excite the synthesized parameters [10].
Pink noise (ﬁltered white noise) is used as the excitation and
convolved with the frequency response parameters (MFCCs)
for the ﬁnal output audio signal. Pink noise can be deﬁned
by a power spectral density proportional to the reciprocal of
the frequency. The overall synthesis model can be illustrated
in ﬁg. 4, similar to [11]. Each time window of MFCC data
essentially acts as a dynamic ﬁlter, shaping the spectrally ﬂat
(or sloped) noise. Ideally, the generated state sequence can
be used to parametrically control the synthesis of the audio
waveform such as controlling the attack, decay, sustain, and
release parameters of the drum signals. The parameters of the
synthesis were unfortunately difﬁcult to control with respect
to auditory evaluation due to the transient behavior or drum
signals (this is not the case for the typical speech application).
Moreover, numerous HMM models of varying state sequence
lengths were used with little noticeable improvement. Overall,
a 5-state sequence was used.

Fig. 2.

Isolated Recognition for Every 16th Note Division

testing purposes, a small collection of ﬁfty rock rhythmic pat-
terns and ﬁfty hip-hop patterns (previously deﬁned by genre)
were decoded into discrete events consisting of the isolated
recognition classes. Once the decoded instrument patterns are
generated, the data can then be used to model the rhythmic
characteristics of the respective genre. A discrete-observation
(28 class) HMM model can then be used to classify using
a maximum log-likelihood approach. The two-class dataset
illustrated signiﬁcant results considering the minimal number
of examples. See Results

Fig. 4. Synthesis Model

V I . R E SU LT S
Overall results proved moderately successful. The following
will be a presentation and discussion of the results for the iso-
lated recognition, rhythmic transcription for genre recognition,
and synthesis. For all confusion matrices, the rows represent
the known correct classiﬁcation, while the columns represent
the predicted classiﬁcation.

A. Isolated Recognition
With respect to isolated recognition, two main training and
testing schemes were implemented. Initially, 10-fold cross-
validation was used on the six basis class (200 examples/class)
HMM models only. Very accurate results were obtained. See
the confusion matrix in ﬁg. 5. After basis class veriﬁcation,
10-fold cross-validation was used to train/test all 28 classes to-
gether and can be seen in ﬁg. 13. Unfortunately, classiﬁcation
accuracy greatly decreases as the complexity level increases,
making musicological analysis of the rhythm transcription less
useful. The misclassiﬁcations, however, are typically educated

Fig. 3. Discrete Observation HMM State Sequence

V. SYNTH E S I S
Independent of the rhythmic transcription, the isolated basis
class HMM models can be used to re-synthesize audio wave
forms. The trained HMMs and an optimal state sequence can
generate the modeled MFCC parameters using the expected
value observations for each state. Unfortunately, while the
MFCC features give an approximate form of the frequency
transfer function of the audio signals, difﬁculty arises when
attempting to invert the MFCC process (all phase information
of the signal is lost by taking the logarithm of the magnitude).

SnareSnare + BassHi-HatHi-Hat + SnareDiscreteS1S2S3Drum Sample DatabaseSpectral ParametersHMM TrainingOptimal State SequenceHMM Spectral Parameter GenerationShaped Noise ExcitationSynthesisAudio OutputAnalysisSynthesisDrum Sample DatabaseSpectral ParametersHMM TrainingOptimal State SequenceHMM Spectral Parameter GenerationShaped Noise ExcitationSynthesisAudio OutputAnalysisSynthesisCS229 FINAL PROJECT, DECEMBER 14, 2007

3

Fig. 5. Basis Class Confusion Matrix (%)

in some manner (i.e. a snare drum gets misclassiﬁed as a
snare + bass drum) and can be interpreted as resonances of the
basis classiﬁcation. A general analysis of the complexity vs.
classiﬁcation accuracy result can be seen in ﬁg. 9. See ﬁg. 13
for a confusion matrix of all classiﬁcation.

Fig. 6. Complexity vs. Classiﬁcation Accuracy

B. Rhythmic Transcription for Genre Recognition
Although the performance of the isolated recognition sig-
niﬁcantly decreases with complex instrument combinations,
accurate genre recognition using sequential isolated recogni-
tion was obtained. Using leave-one-out cross-validation and
the minimal database of ﬁfty hip-hop and ﬁfty rock classiﬁed
two-measure audio ﬁles, genre-classiﬁcation by rhythmic tran-
scription provided around 76-80% classiﬁcation accuracy. See
the confusion matrices for multiple tests using varying state-
sequence lengths. It should be noted that a large improvement
on the classiﬁcation accuracy should result with a signiﬁcantly
larger database, providing promise for rhythm-based genre
recognition.

Fig. 7.

32-State Discrete Observation HMM Results

Fig. 8.

16-State Discrete Observation HMM Results

Fig. 9.

4-State Discrete Observation HMM Results

C. Synthesis
Recognizable synthesis was achieved and can be seen by
comparing the original energy, time-domain, and spectrogram
data to the optimal state-sequence synthesized data. The
synthesis, however, proved quite difﬁcult to control using the
simplistic synthesis model shown in ﬁg. 4. Additionally, the
limited frequency range of the MFCC coefﬁcients severely
limited the quality of the synthesized sounds. Unfortunately,
increasing the feature vectors limits the overall accuracy of
HMM model due to the limited size of data. Moreover, the
MFCC data does little to model
time-domain information
which is critical for modeling the transient behavior of drum
signals. An attempt
to integrate the delta and delta-delta
MFCCs was made with little success due to the limited
database size. Increasing the dataset size and adding signiﬁcant
features such as time-domain volume envelope information
should signiﬁcantly increase the behavior for synthesis. See
ﬁgures 10, 11, 12 below for an example of a synthesized
hi-hat drum sound (see http://ccrma.stanford.edu/∼njb/cs229
for sound examples). The state-sequence transitions can be
visibly seen through the abrupt changes in each plot of data
over time. When comparing sound quality between the isolated
instrument synthesis sounds, the hi-hat and snare instruments
proved the most effective because of the more noise modeled
sound. The tom drum and bass drum proved to be more
difﬁcult and require a reﬁned excitation signal.

V I I . CONC LU S ION S
Overall, satisfactory results were achieved. Isolated recog-
nition and rhythmic transcription or sequential isolated recog-
nition for genre classiﬁcation illustrated signiﬁcant promise
with little future reﬁnement making search-by-rhythm a pos-
sibility for future revision. Unfortunately, the HMM synthesis
proved to be quite difﬁcult and exhibited a large need for
improvement with respect to the signal processing synthesis
model. Moreover, increasing the dataset size will be needed
and should signiﬁcantly improve the synthesis model as well
as the isolated recognition.

ACKNOW LEDGM EN T
Thank you to Professor Andrew Ng, the CS229 course TAs,
and Professor Ge Wang of the Center for Computer Research
in Music and Acoustics (CCRMA) for advice and guidance.

Confusion Matrix (%)sbchtsis10000000b0990010c0097.6002.38h5.50094.500t000099.50.5si00000100   Confusion Matrix (%)RockHi-HopRock7624Hi-Hop2080   Confusion Matrix (%)RockHi-HopRock7426Hi-Hop2278   Confusion Matrix (%)RockHi-HopRock8020Hi-Hop1882CS229 FINAL PROJECT, DECEMBER 14, 2007

4

Fig. 10. Original vs. Synthesized Optimal State Sequence Energy

Fig. 12. Original vs. Synthesized Optimal State Sequence Spectrogram

applications in speech recognition,” Proceedings of the IEEE, vol. 77,
no. 2, pp. 257–286, 1989.
[8] K. Murphy, “Hidden markov model
toolbox for matlab,”
(hmm)
[Online]. Available: http://www.cs.ubc.ca/∼murphyk/Software/
1998.
HMM/hmm.html
[9] J. Paulus and A. Klapuri, “Conventional and periodic ngrams in
the transcription of drum sequences,” 2003.
[Online]. Available:
citeseer.ist.psu.edu/paulus03conventional.html
[10] t. . Yamagishi, J .
[11] A. Tokuda, K.; Heiga Zen; Black, “An hmm-based speech synthesis
system applied to english,” Speech Synthesis, 2002. Proceedings of 2002
IEEE Workshop on, pp. 227–230, 11-13 Sept. 2002.

Fig. 11. Original vs. Synthesized Optimal State Sequence Time Domain
Signal

R E FERENC E S

[1] D. FitzGerald and J. Paulus, “Unpitched percussion transcription,” in
Signal Processing Methods for Music Transcription, A. Klapuri and
M. Davy, Eds. Springer-Verlag, 2006, pp. 131–162.
[2] D. Ellis,
inversion.”
and
“Rasta/plp/mfcc
feature
calculation
http://www.ee.columbia.edu/∼dpwe/resources/
[Online]. Available:
matlab/rastamat/
[3] P. Davis, S.; Mermelstein, “Comparison of parametric representations
for monosyllabic word recognition in continuously spoken sentences,”
Acoustics, Speech, and Signal Processing [see also IEEE Transactions
on Signal Processing], IEEE Transactions on, vol. 28, no. 4, pp. 357–
366, Aug 1980.
[4] T. F. Quatieri, Discrete-Time Speech Signal Processing: Principles and
Practices. New Jersey: Prentice Hall, 2001.
[5] B. Logan, “Mel frequency cepstral coefﬁcients for music modeling,”
in Proceedings of
the First
International Symposium on Music
Information Retrieval (ISMIR), Plymouth, Massachusetts, oct 2000.
[Online]. Available: citeseer.ist.psu.edu/logan00mel.html
[6] “Jason mcgerr
sessions
reﬁll.”
[Online]. Available: http://www.
propellerheads.se/
[7] L. R. Rabiner, “A tutorial on hidden markov models and selected

CS229 FINAL PROJECT, DECEMBER 14, 2007

5

Fig. 13. All Class Confusion Matrix (%)

Confusion Matrix (%)sbchtsisbscshstbcbhbtchcthtsbcsbhsbtbchbctchtschsctshtbhtsbchsbcts860000011.50.51000000000.50000000.5000b080002011.5000005.5000001000000000c0076.500001.5001.50017000.5000002.50.50000h20080.5005010000.500.50001.50000000000t00007600000.50023000000.5000000000si000001000000000000000000000000sb0.519.500007000.50.502000002.53.5000001000sc11.5011.50000.548.54.500.5002.50030.50000.513.51.5001.50sh12007002359.500201.50006.50000102.50.52.50st0.50.5004.5060144005000013400.50002.5000.5bc000000000049.50008.5020.5009.53.510.53002.51.5bh0100007.5000061.50002012.50400001.59.50.50bt030022.500000.50072000002000000000ch00184.500023.5010040002004.50020.50.5003.50ct000000000070002308.500023.53023.5000.511ht0000101.5003.50310039.5021.50010011.533.510sbc000000020025000.52.5039.5003.501.53.59.51083.5sbh100000140.53.50011.500010470.51.5000.508.555.50sbt00.50010700230010.50000353.5000001.5000bch000000001.50126021011.53038.50.511100.5191.5bct000000000070002308.500023.53023.5000.511cht00000000000.500073.53.51.5036.530.50.5114.55.5517.5sch00620009700002100400100.541.50.50.5070sct00000001013.500022018.50008.560.524.50.50311sht00.500004.501.58.503.50001507.540.5020034.51800bht00.50000000.530112002607.51.5101009.5360.50sbch0.50000001304.5302.50018602401.57.51.520250sbct0000000.50001.500012.513.5010.534801600.5120
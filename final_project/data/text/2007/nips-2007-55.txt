Learning Visual Attributes

Vittorio Ferrari ∗
University of Oxford (UK)

Andrew Zisserman
University of Oxford (UK)

Abstract

We present a probabilistic generative model of visual attributes, together with an efﬁcient
learning algorithm. Attributes are visual qualities of obj ects, such as ‘red’, ‘striped’, or
‘spotted’. The model sees attributes as patterns of image segments, repeatedly sharing some
characteristic properties. These can be any combination of appearance, shape, or the layout
of segments within the pattern. Moreover, attributes with general appearance are taken
into account, such as the pattern of alternation of any two colors which is characteristic
for stripes. To enable learning from unsegmented training images, the model is learnt
discriminatively, by optimizing a likelihood ratio.
As demonstrated in the experimental evaluation, our model can learn in a weakly supervised
setting and encompasses a broad range of attributes. We show that attributes can be learnt
starting from a text query to Google image search, and can then be used to recognize the
attribute and determine its spatial extent in novel real-world images.

1 Introduction
In recent years, the recognition of object categories has become a major focus of computer vision and
has shown substantial progress, partly thanks to the adoption of techniques from machine learning
and the development of better probabilistic representations [1, 3]. The goal has been to recognize
object categories, such as a ‘car’, ‘cow’ or ‘shirt’. Howeve r, an object also has many other qualities
apart from its category. A car can be red, a shirt striped, a ball round, and a building tall. These visual
attributes are important for understanding object appearance and for d escribing objects to other
people. Figure 1 shows examples of such attributes. Automatic learning and recognition of attributes
can complement category-level recognition and therefore improve the degree to which machines
perceive visual objects. Attributes also open the door to appealing applications, such as more speciﬁc
queries in image search engines (e.g. a spotted skirt, rathe r than just any skirt). Moreover, as
different object categories often have attributes in common, modeling them explicitly allows part
of the learning task to be shared amongst categories, or allows previously learnt knowledge about
an attribute to be transferred to a novel category. This may r educe the total number of training
images needed and improve robustness. For example, learning the variability of zebra stripes under
non-rigid deformations tells us a lot about the corresponding variability in striped shirts.

In this paper we propose a probabilistic generative model of visual attributes, and a procedure for
learning its parameters from real-world images. When prese nted with a novel image, our method in-
fers whether it contains the learnt attribute and determine s the region it covers. The proposed model
encompasses a broad range of attributes, from simple colors such as ‘red’ or ‘green’ to complex pat-
terns such as ‘striped’ or ‘checked’. Both the appearance and the shape of pattern elements (e.g. a
single stripe) are explicitly modeled, along with their layout within the overall pattern (e.g. adjacent
stripes are parallel). This enables our model to cover attri butes de ﬁned by appearance (‘red’), by
shape (‘round’), or by both (the black-and-white stripes of zebras). Furthermore, the model takes
into account attributes with general appearance, such as stripes which are characterized by a pattern
of alternation ABAB of any two colors A and B, rather than by a speciﬁc combination of colors.

Since appearance, shape, and layout are modeled explictly, the learning algorithm gains an under-
standing of the nature of the attribute. As another attractive feature, our method can learn in a
weakly supervised setting, given images labeled only by the presence or absence of the attribute,

∗This research was supported by the EU project CLASS. The authors thank Dr. Josef Sivic for fruitful
discussions and helpful comments on this paper.

unary

red

binary

black/white stripes

round
generic stripes
Figure 1: Examples of different kinds of attributes. On the left we show two simple attributes, whose charac-
teristic properties are captured by individual image segments (appearance for red, shape for round). On the
right we show more complex attributes, whose basic element is a pair of segments.

without indication of the image region it covers. The presence/absence labels can be noisy, as the
training method can tolerate a considerable number of mislabeled images. This enables attributes to
be learnt directly from a text speciﬁcation by collecting tr aining images using a web image search
engine, such as Google-images, and querying on the name of the attribute.

Our approach is inspired by the ideas of Jojic and Caspi [4], where patterns have constant appearance
within an image, but are free to change to another appearance in other images. We also follow the
generative approach to learning a model from a set of images u sed by many authors, for example
LOCUS [10]. Our parameter learning is discriminative – the b ene ﬁts of this have been shown
before, for example for training the constellation model of [3]. In term of functionality, the closest
works to ours are those on the analysis of regular textures [5 , 6]. However, they work with textures
covering the entire image and focus on ﬁnding distinctive ap pearance descriptors. In constrast, here
textures are attributes of objects, and therefore appear in complex images containing many other
elements. Very few previous works appeared in this setting [7, 11]. The approach of [7] focuses
on colors only, while in [11] attributes are limited to indiv idual regions. Our method encompasses
also patterns de ﬁned by pairs of regions, allowing to captur e more complex attributes. Moreover,
we take up the additional challenge of learning the pattern geometry.

Before describing the generative model in section 3, in the next section we brie ﬂy introduce image
segments, the elementary units of measurements observed in the model.

2 Image segments – basic visual representation

The basic units in our attribute model are image segments extracted using the algorithm of [2]. Each
segment has a uniform appearance, which can be either a color or a simple texture (e.g. sand, grain).
Figure 2a shows a few segments from a typical image.

Inspired by the success of simple patches as a basis for appearance descriptors [8, 9], we randomly
sample a large number of 5 × 5 pixel patches from all training images and cluster them using k-
means [8]. The resulting cluster centers form a codebook of patch types. Every pixel is soft-assigned
to the patch types. A segment is then represented as a normalized histogram over the patch types
of the pixels it contains. By clustering the segment histograms from the training images we obtain
a codebook A of appearances ( ﬁgure 2b). Each entry in the codebook is a prototype segment
descriptor, representing the appearance of a subset of the s egments from the training set.

Each segment s is then assigned the appearance a ∈ A with the smallest Bhattacharya distance to the
histogram of s. In addition to appearance, various geometric properties o f a segment are measured,
summarizing its shape. In our current implementation, these are: curvedness, compactness, elonga-
tion ( ﬁgure 2c), fractal dimension and area relative to the i mage. We also compute two properties of
pairs of segments: relative orientation and relative area ( ﬁgure 2d).

C

C
P

P

A

M

m

A
P2

m
M

A1

A2

(ln

)

A
1
A2

θ 1

θ1 θ2
−

c

θ 2
b
d
a
Figure 2: Image segments as visual features. a) An image with a few segments overlaid, including two pairs
of adjacent segments on a striped region. b) Each row is an entry from the appearance codebook A (i.e.
one appearance; only 4 out of 32 are shown). The three most frequent patch types for each appearance are
displayed. Two segments from the stripes are assigned to the white and black appearance respectively (arrows).
c) Geometric properties of a segment: curvedness, which is the ratio between the number of contour points C
with curvature above a threshold and the total perimeter P ; compactness; and elongation, which is the ratio
between the minor and major moments of inertia. d) Relative geometric properties of a pair of segments:
relative area and relative orientation. Notice how these measures are not symmetric (e.g. relative area is the
area of the ﬁrst segment wrt to the second).

3 Generative models for visual attributes
Figure 1 shows various kinds of attributes. Simple attributes are entirely characterized by properties
of a single segment (unary attributes). Some unary attributes are de ﬁned by their appearance, suc h
as colors (e.g. red, green) and basic textures (e.g. sand, gr ainy). Other unary attributes are de ﬁned by
a segment shape (e.g. round). All red segments have similar appearance, regardless of shape, while
all round segments have similar shape, regardless of appear ance. More complex attributes have a
basic element composed of two segments (binary attributes). One example is the black/white stripes
of a zebra, which are composed of pairs of segments sharing similar appearance and shape across
all images. Moreover, the layout of the two segments is characteristic as well: they are adjacent,
nearly parallel, and have comparable area. Going yet furthe r, a general stripe pattern can have any
appearance (e.g. blue/white stripes, red/yellow stripes) . However, the pairs of segments forming
a stripe pattern in one particular image must have the same appearance. Hence, a characteristic of
general stripes is a pattern of alternation ABABAB. In this case, appearance is common within an
image, but not across images.

The attribute models we present in this section encompass all aspects discussed above. Essentially,
attributes are found as patterns of repeated segments, or pa irs of segments, sharing some properties
(geometric and/or appearance and/or layout).

3.1 Image likelihood.
We start by describing how the model M explains a whole image I . An image I is represented by a
set of segments {s}. A latent variable f is associated with each segment, taking the value f = 1 for
a foreground segment, and f = 0 for a background segment. Foreground segments are those on t he
image area covered by the attribute. We collect f for all segments of I into the vector F. An image
has a foreground appearance a, shared by all the foreground segments it contains. The like lihood of
an image is
p(I |M; F, a) = Yx∈I
where x is a pixel, and M are the model parameters. These include α ⊂ A, the set of appearances
allowed by the model, from which a is taken. The other parameters are used to explain segments and
are dicussed below. The probability of pixels is uniform within a segment, and independent across
segments:

p(x|M; F, a)

(1)

p(x|M; F, a) = p(sx |M; f , a)
(2)
with sx the segment containing x. Hence, the image likelihood can be expressed as a product over
the probability of each segment s, counted by its area Ns (i.e. the number of pixels it contains)
p(sx |M; f , a) = Ys∈I
p(I |M; F, a) = Yx∈I

p(s|M; f , a)Ns

(3)

α

a

s

(a)

λ

f

β

(b)

γ

R

δ

β

α

a

λ
1
λ
2

c

s

f

iS

S i D
D
G
G
Figure 3: a) Graphical model for unary attributes. D is the number of images in the dataset, Si is the number
of segments in image i, and G is the total number of geometric properties considered (both active and inactive).
b) Graphical model for binary attributes. c is a pair of segments. Φ1,2 are the geometric distributions for each
segment a pair. Ψ are relative geometric distributions (i.e. measure properties between two segments in a pair,
such as relative orientation), and there are R of them in total (active and inactive). δ is the adjacency model
parameter. It tells whether only adjacent pairs of segments are considered (so p(c|δ = 1) is one only iff c is a
pair of adjacent segments).

C
i

Note that F and a are latent variables associated with a particular image, so there is a different F
and a for each image. In contrast, a single model M is used to explain all images.

3.2 Unary attributes
g }) is de ﬁned
Segments are the only observed variables in the unary model. A segment s = (sa , {sj
by its appearance sa and shape, captured by a set of geometric measurements {sj
g }, such as elon-
gation and curvedness. The graphical model in ﬁgure 3a illus trates the conditional probability of
image segments
p(s|M; f , a) = (cid:26) p(sa |a) · Qj p(sj
g |Φj )vj
if f = 1
if f = 0
β
The likelihood for a segment depends on the model parameters M = (α, β , {λj }), which specify
a visual attribute. For each geometric property λj = (Φj , vj ), the model de ﬁnes its distribution
Φj over the foreground segments and whether the property is active or not (v j = 1 or 0). Active
properties are relevant for the attribute (e.g. elongation is relevant for stripes, while orientation is
not) and contribute substantially to its likelihood in (4). Inactive properties instead have no impact
on the likelihood (exponentiation by 0).
It is the task of the learning stage to determine which
properties are active and their foreground distribution.
The factor p(sa |a) = [sa = a] is 1 for segments having the foreground appearance a for this image,
and 0 otherwise (thus it acts as a selector). The scalar value β represents a simple background model:
all segments assigned to the background have likelihood β . During inference and learning we want
to maximize the likelihood of an image given the model over F, which is achieved by setting f to
foreground when the f = 1 case of equation (4) is greater than β .
As an example, we give the ideal model parameters for the attr ibute ‘red’. α contains the red
appearance only. β is some low value, corresponding to how likely it is for non-red segments to
be assigned the red appearance. No geometric property {λj } is active (i.e. all v j = 0).

(4)

3.3 Binary attributes
The basic element of binary attributes is a pair of segments. In this section we extend the unary
model to describe pairs of segments. In addition to duplicating the unary appearance and geomet-
ric properties, the extended model includes pairwise prope rties which do not apply to individual
segments. In the graphical model of ﬁgure 3b, these are relat
ive geometric properties γ (area, orien-
tation) and adjacency δ , and together specify the layout of the attribute. For example, the orientation
of a segment with respect to the other can capture the paralle lism of subsequent stripe segments.
Adjacency expresses whether the two segments in the pair are adjacent (like in stripes) or not (like
the maple leaf and the stripes in the canadian ﬂag). We consid er two segments adjacent if they share
part of the boundary. A pattern characterized by adjacent segments is more distinctive, as it is less
likely to occur accidentally in a negative image.

Segment likelihood. An image is represented by a set of segments {s}, and the set of all possible
pairs of segments {c}. The image likelihood p(I |M; F, a) remains as de ﬁned in equation (3), but

now a = (a1 , a2 ) speciﬁes two foreground appearances, one for each segment i n the pair. The
likelihood of a segment s is now de ﬁned as the maximum over all pairs containing it
p(s|M; f , a) = (cid:26) max{c|s∈c} p(c|M, t)
β

if f = 1
if f = 0

(5)

Pair likelihood. The observed variables in our model are segments s and pairs of segments c. A
pair c = (s1 , s2 , {ck
r }) is de ﬁned by two segments s1 , s2 and their relative geometric measurements
r } (relative orientation and relative area in our implementat ion). The likelihood of a pair given
{ck
the model is
· Yj (cid:16)p(sj
2 (cid:17)
· Yk (cid:16)p(ck
r (cid:17) · p(c|δ)
j
j
r |Ψk )vk
1 )v
1,g |Φj
2 )v
2,g |Φj
1 · p(sj
p(c|M, a) = p(s1,a , s2,a |a)
{z
}
|
appearance
{z
}
|
|
{z
}
layout
shape
The binary model parameters M = (α, β , δ, {λj
1 }, {λj
2}, {γ k}) control the behavior of the pair
likelihood. The two sets of λj
i = (Φj
i , vj
i ) are analogous to their counterparts in the unary model,
and de ﬁne the geometric distributions and their associated activation states for each segment in the
pair respectively. The layout part of the model captures the interaction between the two segments in
the pair. For each relative geometric property γ k = (Ψk , vk
r ) the model gives its distribution Ψk over
r . The model parameter δ determines whether
pairs of foreground segments and its activation state vk
the pattern is composed of pairs of adjacent segments (δ = 1) or just any pair of segments (δ = 0).
The factor p(c|δ) is de ﬁned as 0 iff δ = 1 and the segments in c are not adjacent, while it is 1 in all
other cases (so, when δ = 1, p(c|δ) acts as a pair selector). The appearance factor p(s1,a , s2,a |a) =
[s1,a = a1 ∧ s2,a = a2 ] is 1 when the two segments have the foreground appearances a = (a1 , a2 )
for this image.

(6)

As an example, the model for a general stripe pattern is as fol lows. α = (A, A) contains all
are active (vj
pairs of appearances from A. The geometric properties λelong
1 = 1) and their
, λcurv
1
1
distributions Φj
1 peaked at high elongation and low curvedness. The corresponding properties {λj
2}
have similar values. The layout parameters are δ = 1, and γ rel area , γ rel orient are active and
peaked at 0 (expressing that the two segments are parallel and have the same area). Finally, β is a
value very close to 0, as the probability of a random segment under this complex model is very low.

4 Learning the model
Image Likelihood. The image likelihood de ﬁned in (3) depends on the foreground /background
labels F and on the foreground appearance a. Computing the complete likelihood, given only the
model M, involves maximizing a over the appearances α allowed by the model, and over F:

p(I |M) = max
a∈α

max
F

p(I |M; F, a)

(7)

The maximization over F is easily achieved by setting each f to the greater of the two cases in
equation (4) (equation (5) for a binary model). The maximization over a requires trying out all
allowed appearances α. This is computationally inexpensive, as typically there are about 32 entries
in the appearance codebook.

Training data. We learn the model parameters in a weakly supervised setting . The training data
consists of positive I+ = {I i
+} and negative images I− = {I i
−}. While many of the positive
images contain examples of the attribute to be learnt ( ﬁgure 4), a considerable proportion don’t.
Conversely, some of the negative images do contain the attribute. Hence, we must operate under a
weak assumption: the attribute occurs more frequently on po sitive training images than on negative.
Moreover, only the (unreliable) image label is given, not the location of the attribute in the image.
As demonstrated in section 5, our approach is able to learn from this noisy training data.
Although our attribute models are generative, learning them in a discriminative fashion greatly helps
given the challenges posed by the weakly supervised setting. For example, in ﬁgure 4 most of the
overall surface for images labeled ‘red’ is actually white. Hence, a maximum likelihood estimator
over the positive training set alone would learn white, not red. A discriminative approach instead

positive training images

negative training images

Figure 4: Advantages of discriminative training. The task is to learn the attribute ‘red’. Although the most
frequent color in the positive training images is white, white is also common across the negative set.

∈I+

p(I i
− |M)

∈I−

p(I i
+ |M)

p(I+ |M)
p(I− |M)

notices that white occurs frequently also on the negative set, and hence correctly picks up red, as it
is most discriminative for the positive set. Formally, the task of learning is to determine the model
parameters M that maximize the likelihood ratio
= QI i
+
QI i
−
Learning procedure. The parameters of the binary model are M = (α, β , δ, {λj
1}, {λj
2}, {γ k}),
as de ﬁned in the previous sections. Since the binary model is a superset of the unary one, we only
explain here how to learn the binary case. The procedure for t he unary model is derived analogously.
In our implementation, α can contain either a single appearance, or all appearances in the codebook
A. The former case covers attributes such as colors, or patter ns with speciﬁc colors (such as zebra
stripes). The latter case covers generic patterns, as it all ows each image to pick a different appearance
a ∈ α, while at the same time it properly constrains all segments/pairs within an image to share the
same appearance (e.g. subsequent pairs of stripe segments have the same appearance, forming a
pattern of alternation ABABAB). Because of this de ﬁnition, α can take on (1 + |A|)2 /2 different
values (sets of appearances). As typically a codebook of |A| ≤ 32 appearances is sufﬁcient to model
the data, we can afford exhaustive search over all possible values of α. The same goes for δ , which
can only take on two values.

(8)

Given a ﬁxed α and δ , the learning task reduces to estimating the background probability β , and the
geometric properties {λj
1 }, {λj
2}, {γ k }. To achieve this, we need determine the latent variable F for
each training image, as it is necessary for estimating the geometric distributions over the foreground
segments. These are in turn necessary for estimating β . Given β and the geometric properties we
can estimate F (equation (6)). This particular circular dependence in the structure of our model
suggests a relatively simple and computationally cheap approximate optimization algorithm:
1. For each I ∈ {I+ S I− }, estimate an initial F and a via equation (7), using an initial
β = 0.01, and no geometry (i.e. all activation variables set to 0).
2. Estimate all geometric distributions Φj
1 , Φj
2 , Ψk over the foreground segments/pairs from
all images, according to the initial estimates {F}.
3. Estimate β and the geometric activations v iteratively:
(a) Update β as the average probability of segments from I− . This is obtained using the
foreground expression of (5) for all segments of I− .
(b) Activate the geometric property which most increases th e likelihood-ratio (8) (i.e. set
the corresponding v to 1). Stop iterating when no property increases (8).
4. The above steps already yield a reasonable estimate of all model parameters. We use it as
initialization for the following EM-like iteration, which re ﬁnes β and Φj
1 , Φj
2 , Ψk
(a) Update {F} given the current β and geometric properties (set each f to maximize (5))
1 , Φj
(b) Update Φj
2 , Ψk given the current {F}.
1 , Φj
(c) Update β over I− using the current Φj
2 , Ψk .

The algorithm is repeated over all possible α and δ , and the model maximizing (8) is selected. Notice
how β is continuously re-estimated as more geometric properties are added. This implicitly offers to
the selector the probability of an average negative segment under the current model as an up-to-date
baseline for comparison. It prevents the model from overspe cializing as it pushes it to only pick up
properties which distinguish positive segments/pairs from negative ones.

(a)

(b)

(c)

0

0

Segment 1

Segment 2

Layout

1

<.33

>.67

0

1

<.33

>.67

−π/2

0

π/2

−4

0

4

elongation

1

<.33
>.67
curvedness

0

area

1

0

compactness

0.4

0

elongation

1

>.67
<.33
curvedness

relative orientation

−4

0
relative area

4

Figure 5: a) color models learnt for red, green, blue, and yellow. For each, the three most frequent patch
types are displayed. Notice how each model covers different shades of a color. b+c) geometric properties of the
learned models for stripes (b) and dots (c). Both models are binary, have general appearance, i.e. α = (A, A),
and adjacent segments, i.e. δ = 1. The ﬁgure shows the geometric distributions for the activa ted geometric
properties. Lower elongation values indicate more elongated segments. A blank slot means the property is not
active for that attribute. See main text for discussion.

One last, implicit, parameter is the model complexity: is the attribute unary or binary ? This is
tackled through model selection: we learn the best unary and binary models independently, and then
select the one with highest likelihood-ratio. The comparison is meaningful because image likelihood
is measured in the same way in both unary and binary cases (i.e . as the product over the segment
probabilities, equation (3)).

5 Experimental results
Learning. We present results on learning four colors (red, green, blue , and yellow) and three
patterns (stripes, dots, and checkerboard). The positive t raining set for a color consists of the 14
images in the ﬁrst page returned by Google-images when queri ed by the color name. The proportion
of positive images unrelated to the color varies between 21% and 36%, depending on the color (e.g.
ﬁgure 4). The negative training set for a color contains all p ositive images for the other colors. Our
approach delivers an excellent performance. In all cases, the correct model is returned: unary, no
active geometric property, and the correct color as a speciﬁ c appearance ( ﬁgure 5a).

Stripes are learnt from 74 images collected from Google-ima ges using ‘striped’, ‘stripe’, ‘stripes’
as queries. 20% of them don’t contain stripes. The positive training set for dots contains 35 images,
29% of them without dots, collected from textile vendors websites and Google-images (keywords
‘dots’, ‘dot’, ‘polka dots’). For both attributes, the 56 images for colors act as negative training
set. As shown in ﬁgure 5, the learnt models capture well the na ture of these attributes. Both stripes
and dots are learnt as binary and with general appearance, wh ile they differ substantially in their
geometric properties. Stripes are learnt as elongated, rat her straight pairs of segments, with largely
the same properties for the two segments in a pair. Their layo ut is meaningful as well: adjacent,
nearly parallel, and with similar area.
In contrast, dots ar e learnt as small, unelongated, rather
curved segments, embedded within a much larger segment. This can be seen in the distribution of
the area of the ﬁrst segment, the dot, relative to the area of t he second segment, the ‘background’
on which dots lie. The background segments have a very curved , zigzagging outline, because they
circumvent several dots. In contrast to stripes, the two segments that form this dotted pattern are not
symmetric in their properties. This characterisic is model ed well by our approach, con ﬁrming its
ﬂexibility. We also train a model from the ﬁrst
22 Google-images for the query ‘checkerboard’, 68%
of which show a black/white checkerboard. The learnt model is binary, with one segment for a black
square and the other for an adjacent white square, demonstra ting the learning algorithm correctly
infers both models with speciﬁc and generic appearance, ada pting to the training data.

Recognition. Once a model is learnt, it can be used to recognize whether a novel image contains
the attribute, by computing the likelihood (7). Moreover, the area covered by the attribute is local-
ized by the segments with f = 1 ( ﬁgure 6). We report results for red, yellow, stripes, and do ts. All
test images are downloaded from Yahoo-images, Google-images, and Flickr. There are 45 (red), 39
(yellow), 106 (stripes), 50 (dots) positive test images. In general, the object carrying the attribute
stands against a background, and often there are other objec ts in the image, making the localization
task non-trivial. Moreover, the images exhibit extreme var iability: there are paintings as well as pho-
tographs, stripes appear in any orientation, scale, and app earance, and they are often are deformed

Figure 6: Recognition results. Top row: red (left) and yellow (right). Middle rows: stripes. Bottom row:
dots. We give a few example test images and the corresponding localizations produced by the learned models.
Segments are colored according to their foreground likelihood, using matlab’s jet colormap (from dark blue to
green to yellow to red to dark red). Segments deemed not to belong to the attribute are not shown (black). In
the case of dots, notice how the pattern is formed by the dots themselves and by the uniform area on which they
lie. The ROC plots shows the image classi ﬁcation performanc e for each attribute. The two lower curves in
the stripes plot correspond to a model without layout, and without either layout nor any geometry respectively.
Both curves are substantially lower, conﬁrming the usefuln ess of the layout and shape components of the model.

(human body poses, animals, etc.). The same goes for dots, which can vary in thickness, spacing,
and so on. Each positive set is coupled with a negative one, in which the attribute doesn’t appear,
composed of 50 images from the Caltech-101 ‘Things’ set [12]. Because these negative images are
rich in colors, textures and structure, they pose a consider able challenge for the classiﬁcation task.

As can be seen in ﬁgure 6, our method achieves accurate locali zations of the region covered by the
attribute. The behavior on stripe patterns composed of more than two appearances is particularly
interesting (the trousers in the rightmost example). The model explains them as disjoint groups of
binary stripes, with the two appearances which cover the largest image area. In terms of recognizing
whether an image contains the attribute, the method perform s very well for red and yellow, with ROC
equal-error rates above 90%. Performance is convincing also for stripes and dots, espec ially since
these attributes have generic appearance, and hence must be recognized based only on geometry and
layout. In contrast, colors enjoy a very distinctive, speci ﬁc appearance.

References

[1] N. Dalal and B. Triggs, Histograms of Oriented Gradients for Human Detection, CVPR, 2005.
[2] P. Felzenszwalb and D Huttenlocher, Efﬁcient Graph-Based Image Segmentation , IJCV, (50):2, 2004.
[3] R. Fergus, P. Perona, and A. Zisserman, Object Class Recognition by Unsupervised Scale-Invariant
Learning, CVPR, 2003.
[4] N. Jojic and Y. Caspi, Capturing image structure with probabilistic index maps, CVPR, 2004
[5] S. Lazebnik, C. Schmid, and J. Ponce, A Sparse Texture Representation Using Local Afﬁne Regions ,
PAMI, (27):8, 2005
[6] Y. Liu, Y. Tsin, and W. Lin, The Promise and Perils of Near-Regular Texture, IJCV, (62):1, 2005
[7] J. Van de Weijer, C. Schmid, and J. Verbeek, Learning Color Names from Real-World Images, CVPR,
2007.
[8] M. Varma and A. Zisserman, Texture classi ﬁcation: Are ﬁlter banks necessary? , CVPR, 2003.
[9] J. Winn, A. Criminisi, and T. Minka, Object Categorization by Learned Universal Visual Dictionary,
ICCV, 2005.
[10] J. Winn and N. Jojic. LOCUS: Learning Object Classes with Unsupervised Segmentation, ICCV, 2005.
[11] K. Yanai and K. Barnard, Image Region Entropy: A Measure of ”Visualness” of Web Image
s Associated
with One Concept, ACM Multimedia, 2005.
[12] Caltech 101 dataset: www.vision.caltech.edu/Image Datasets/Caltech101/Caltech101.html


Stable Dual Dynamic Programming

Tao Wang(cid:3) Daniel Lizotte Michael Bowling Dale Schuurmans
Department of Computing Science
University of Alberta
ftrysi,dlizotte,bowling,daleg@cs.ualberta.ca

Abstract

Recently, we have introduced a novel approach to dynamic programming and re-
inforcement learning that is based on maintaining explicit representations of sta-
tionary distributions instead of value functions. In this paper, we investigate the
convergence properties of these dual algorithms both theoretically and empirically,
and show how they can be scaled up by incorporating function approximation.

1 Introduction

Value function representations are dominant in algorithms for dynamic programming (DP) and rein-
forcement learning (RL). However, linear programming (LP) methods clearly demonstrate that the
value function is not a necessary concept for solving sequential decision making problems. In LP
methods, value functions only correspond to the primal formulation of the problem, while in the dual
they are replaced by the notion of state (or state-action) visit distributions [1, 2, 3]. Despite the well
known LP duality, dual representations have not been widely explored in DP and RL. Recently, we
have showed that it is entirely possible to solve DP and RL problems in the dual representation [4].
Unfortunately, [4] did not analyze the convergence properties nor implement the proposed ideas. In
this paper, we investigate the convergence properties of these newly proposed dual solution tech-
niques, and show how they can be scaled up by incorporating function approximation. The proof
techniques we use to analyze convergence are simple, but lead to useful conclusions. In particular,
we ﬁnd that the standard convergence results for value based approaches also apply to the dual case,
even in the presence of function approximation and off-policy updating. The dual approach appears
to hold an advantage over the standard primal view of DP/RL in one major sense: since the funda-
mental objects being represented are normalized probability distributions (i.e., belong to a bounded
simplex), dual updates cannot diverge. In particular, we ﬁnd that dual updates converge (i.e. avoid
oscillation) in the very circumstance where primal updates can and often do diverge: gradient-based
off-policy updates with linear function approximation [5, 6].

2 Preliminaries

We consider the problem of computing an optimal behavior strategy in a Markov decision process
(MDP), deﬁned by a set of actions A, a set of states S , a jS jjAj by jS j transition matrix P , a reward
vector r and a discount factor (cid:13) , where we assume the goal is to maximize the in ﬁnite horizon
discounted reward r0 + (cid:13) r1 + (cid:13) 2 r2 + (cid:1) (cid:1) (cid:1) = P1
It is known that an optimal behavior
t=0 (cid:13) t rt .
strategy can always be expressed by a stationary policy, whose entries (cid:25) (sa) specify the probability
of taking action a in state s. Below, we represent a policy (cid:25) by an equivalent representation as
an jS j (cid:2) jS jjAj matrix (cid:5) where (cid:5)(s;s0 a) = (cid:25) (sa) if s0 = s, otherwise 0. One can quickly verify
that the matrix product (cid:5)P gives the state-to-state transition probabilities induced by the policy
(cid:25) in the environment P , and that P (cid:5) gives the state-action to state-action transition probabilities
induced by policy (cid:25) in P . The problem is to compute an optimal policy given either (a) a complete

(cid:3)Current afﬁliation: Computer Sciences Laboratory, Australian National University, tao.wang@anu.edu.au.

speciﬁcation of the environmental variables P and r (the “ planning problem”), or (b) limited access
to the environment through observed states and rewards and the ability to select actions to cause
further state transitions (the “ learning problem”). The ﬁrst problem is normally tackled by LP or DP
methods, and the second by RL methods. In this paper, we will restrict our attention to scenario (a).

3 Dual Representations

Traditionally, DP methods for solving the MDP planning problem are typically expressed in terms of
the primal value function. However, [4] demonstrated that all the classical algorithms have natural
duals expressed in terms of state and state-action probability distributions.

In the primal representation, the policy state-action value function can be speciﬁed by an jS jjAj(cid:2) 1
vector q = P1
i=0 (cid:13) i (P (cid:5))i r which satisﬁes q = r + (cid:13)P (cid:5)q. To develop a dual form of state-
action policy evaluation, one considers the linear system d> = (1 (cid:0) (cid:13) )(cid:23)
> + (cid:13)d>P (cid:5), where (cid:23) is
the initial distribution over state-action pairs. Not only is d a proper probability distribution over
state-action pairs, it also allows one to easily compute the expected discounted return of the policy (cid:25) .
However, recovering the state-action distribution d is inadequate for policy improvement. Therefore,
one considers the following jS jjAj (cid:2) jS jjAj matrix H = (1 (cid:0) (cid:13) )I + (cid:13)P (cid:5)H . The matrix H that
satisﬁes
this linear relation is similar to d> , in that each row is a probability distribution and the
entries H(sa;s0 a0 ) correspond to the probability of discounted state-action visits to (s 0a0 ) for a policy
(cid:25) starting in state-action pair (sa). Unlike d> , however, H drops the dependence on (cid:23) , giving
(1 (cid:0) (cid:13) )q = H r. That is, given H we can easily recover the state-action values of (cid:25) .
For policy improvement, in the primal representation one can derive an improved policy (cid:25)
0 via
the update a(cid:3) (s) = arg maxa q(sa) and (cid:25)
(sa) = 1 if a = a(cid:3) (s), otherwise 0. The dual form
0
of the policy update can be expressed in terms of the state-action matrix H for (cid:25) is a(cid:3) (s) =
arg maxa H(sa;:) r.
In fact, since (1 (cid:0) (cid:13) )q = H r, the two policy updates given in the primal
0 . Further details are given in [4].
and dual respectively, must lead to the same resulting policy (cid:25)

4 DP algorithms and convergence

We ﬁrst
investigate whether dynamic programming operators with the dual representations exhibit
the same (or better) convergence properties to their primal counterparts. These questions will be an-
swered in the afﬁrmati ve. In the tabular case, dynamic programming algorithms can be expressed by
operators that are successively applied to current approximations (vectors in the primal case, matri-
ces in the dual), to bring them closer to a target solution; namely, the ﬁx ed point of a desired Bellman
equation. Consider two standard operators, the on-policy update and the max-policy update.

For a given policy (cid:5), the on-policy operator O is deﬁned as
and
Oq = r + (cid:13)P (cid:5)q
OH = (1 (cid:0) (cid:13) )I + (cid:13)P (cid:5)H;
for the primal and dual cases respectively. The goal of the on-policy update is to bring current
representations closer to satisfying the policy-speciﬁc Bellman equations,
and
q = r + (cid:13)P (cid:5)q
H = (1 (cid:0) (cid:13) )I + (cid:13)P (cid:5)H
The max-policy operator M is different in that it is neither linear nor deﬁned by any reference policy,
but instead applies a greedy max update to the current approximations
and MH = (1 (cid:0) (cid:13) )I + (cid:13)P (cid:5)(cid:3)
Mq = r + (cid:13)P (cid:5)(cid:3) [q]
r [H ];
where (cid:5)(cid:3) [q](s) = maxa q(sa) and (cid:5)(cid:3)
r [H ](s;:) = H(sa0 (s);:) such that a0 (s) = arg maxa [H r](sa) .
The goal of this greedy update is to bring the representations closer to satisfying the optimal-policy
Bellman equations q = r + (cid:13)P (cid:5)(cid:3) [q] and H = (1 (cid:0) (cid:13) )I + (cid:13)P (cid:5)(cid:3)
r [H ].

4.1 On-policy convergence

For the on-policy operator O , convergence to the Bellman ﬁx ed point is easily proved in the primal
case, by establishing a contraction property of O with respect to a speciﬁc norm on q vectors. In
particular, one deﬁnes a weighted 2-norm with weights given by the stationary distribution deter-
mined by the policy (cid:5) and transition model P : Let z (cid:21) 0 be a vector such that z>P (cid:5) = z> ;
that is, z is the stationary state-action visit distribution for P (cid:5). Then the norm is deﬁned as

2 = q>Z q = P(sa) z(sa)q2
(sa) , where Z = diag(z). It can be shown that kP (cid:5)qkz (cid:20) kqkz
kqkz
and kOq1 (cid:0) Oq2kz (cid:20) (cid:13) kq1 (cid:0) q2kz (see [7]). Crucially, for this norm, a state-action transition is
not an expansion [7]. By the contraction map ﬁx ed point theorem [2] there exists a unique ﬁx ed point
of O in the space of vectors q. Therefore, repeated applications of the on-policy operator converge
to a vector q(cid:5) such that q(cid:5) = Oq(cid:5) ; that is, q(cid:5) satisﬁes
the policy based Bellman equation.
Analogously, for the dual representation H , one can establish convergence of the on-policy oper-
ator by ﬁrst deﬁning an approximate weighted norm over matrices and then verifying that O is a
contraction with respect to this norm. Deﬁne
2 = X
2 = kH rkz
z(sa) ( X
kH kz;r
(sa)
(s0 a0 )
It is easily veriﬁed that this deﬁnition satisﬁes
the property of a pseudo-norm, and in particular,
satisﬁes
the triangle inequality. This weighted 2-norm is deﬁned with respect to the stationary
distribution z, but also the reward vector r. Thus, the magnitude of a row normalized matrix is
determined by the magnitude of the weighted reward expectations it induces.

H(sa;s0 a0 ) r(s0 a0 ) )2

(1)

Interestingly, this deﬁnition allows us to establish the same non-expansion and contraction results
as the primal case. We can have kP (cid:5)H kz;r (cid:20) kH kz;r by arguments similar to the primal case.
Moreover, the on-policy operator is a contraction with respect to k(cid:1)kz;r .

Lemma 1 kOH1 (cid:0) OH2 kz;r (cid:20) (cid:13) kH1 (cid:0) H2 kz;r

Proof:
kH kz;r .

kOH1 (cid:0) OH2 kz;r = (cid:13) kP (cid:5)(H1 (cid:0) H2 )kz;r (cid:20) (cid:13) kH1 (cid:0) H2 kz;r since kP (cid:5)H kz;r (cid:20)

Thus, once again by the contraction map ﬁx ed point theorem there exists a ﬁx ed point of O among
row normalized matrices H , and repeated applications of O will converge to a matrix H(cid:5) such that
OH(cid:5) = H(cid:5) ; that is, H(cid:5) satisﬁes
the policy based Bellman equation for dual representations. This
argument shows that on-policy dynamic programming converges in the dual representation, without
making direct reference to the primal case. We will use these results below.

4.2 Max-policy convergence

The strategy for establishing convergence for the nonlinear max operator is similar to the on-policy
case, but involves working with a different norm. Instead of considering a 2-norm weighted by the
visit probabilities induced by a ﬁx ed policy, one simply uses the max-norm in this case: kqk1 =
max(sa) jq(sa) j. The contraction property of the M operator with respect to this norm can then
be easily established in the primal case: kMq1 (cid:0) Mq2 k1 (cid:20) (cid:13) kq1 (cid:0) q2 k1 (see [2]). As in the
on-policy case, contraction sufﬁces
to establish the existence of a unique ﬁx ed point of M among
vectors q, and that repeated application of M converges to this ﬁx ed point q(cid:3) such that Mq(cid:3) = q(cid:3) .
To establish convergence of the off-policy update in the dual representation, ﬁrst deﬁne the max-
norm for state-action visit distribution as
kH k1 = max
(sa)

j X
(s0 a0 )
Then one can simply reduce the dual to the primal case by appealing to the relationship (1(cid:0)(cid:13) )Mq =
MH r to prove convergence of MH .

H(sa;s0 a0 ) r(s0 a0 ) j

(2)

Lemma 2 If (1(cid:0) (cid:13) )q = H r, then (1(cid:0) (cid:13) )Mq = MH r.
Proof: (1(cid:0)(cid:13) )Mq = (1(cid:0)(cid:13) )r+(cid:13)P (cid:5)(cid:3) [(1(cid:0)(cid:13) )q]) = (1(cid:0) (cid:13) )r+ (cid:13)P (cid:5)(cid:3) [H r] = (1(cid:0) (cid:13) )r+ (cid:13)P (cid:5)(cid:3)
r [H ]r =
MH r where the second equality holds since we assumed (1 (cid:0) (cid:13) )q(sa) = [H r](sa) for all (sa).

Thus, given convergence of Mq to a ﬁx ed point Mq(cid:3) = q(cid:3) , the same must also hold for MH .
However, one subtlety here is that the dual ﬁx ed point is not unique. This is not a contradiction
because the norm on dual representations k(cid:1)kz;r is in fact just a pseudo-norm, not a proper norm.
That is, the relationship between H and q is many to one, and several matrices can correspond to
the same q. These matrices form a convex subspace (in fact, a simplex), since if H1 r = (1 (cid:0) (cid:13) )q
and H2 r = (1 (cid:0) (cid:13) )q then ((cid:11)H1 + (1 (cid:0) (cid:11))H2 )r = (1 (cid:0) (cid:13) )q for any (cid:11), where furthermore (cid:11) must be
restricted to 0 (cid:20) (cid:11) (cid:20) 1 to maintain nonnegativity. The simplex of ﬁx ed points fH(cid:3) : MH(cid:3) = H(cid:3) g
is given by matrices H(cid:3) that satisfy H(cid:3) r = (1 (cid:0) (cid:13) )q(cid:3) .

5 DP with function approximation

Primal and dual updates exhibit strong equivalence in the tabular case, as they should. However,
when we begin to consider approximation, differences emerge. We next consider the convergence
properties of the dynamic programming operators in the context of linear basis approximation. We
focus on the on-policy case here, because, famously, the max operator does not always have a ﬁx ed
point when combined with approximation in the primal case [8], and consequently suffers the risk
of divergence [5, 6].

Note that the max operator cannot diverge in the dual case, even with basis approximation, by
boundedness alone; although the question of whether max updates always converge in this case
remains open. Here we establish that a similar bound on approximation error in the primal case can
be proved for the dual approach with respect to the on-policy operator.

In the primal case, linear approximation proceeds by ﬁxing a small set of basis functions, forming
a jS jjAj (cid:2) k matrix (cid:8), where k is the number of bases. The approximation of q can be expressed
by a linear combination of bases ^q = (cid:8)w where w is a k (cid:2) 1 vector of adjustable weights. This is
equivalent to maintaining the constraint that ^q 2 col span((cid:8)).
In the dual, a linear approximation
to H can be expressed as vec( ^H ) = (cid:9)w, where the vec operator creates a column vector from
a matrix by stacking the column vectors of the matrix below one another, w is a k (cid:2) 1 vector of
adjustable weights as it is in the primal case, and (cid:9) is a (jS jjAj)2 (cid:2) k matrix of basis functions.
To ensure that ^H remains a nonnegative, row normalized approximation to H , we simply add the
constraints that ^H 2 simplex((cid:9)) (cid:17) f ^H : vec( ^H ) = (cid:9)w; (cid:9) (cid:21) 0;(1>(cid:10)I )(cid:9) = 11> ;w (cid:21) 0; w>1 = 1g
where the operator (cid:10) is the Kronecker product.
In this section, we ﬁrst
introduce operators (projection and gradient step operators) that ensure the
approximations stay representable in the given basis. Then we consider their composition with the
on-policy and off-policy updates, and analyze their convergence properties. For the composition of
the on-policy update and projection operators, we establish a similar bound on approximation error
in the dual case as in the primal case.

5.1 Projection Operator

Recall that in the primal, the action value function q is approximated by a linear combination of
bases in (cid:8). Unfortunately, there is no reason to expect Oq or Mq to stay in the column span of
(cid:8), so a best approximation is required. The subtlety resolved by Tsitsiklis and Van Roy [7] is to
identify a particular form of best approximation—weighted
least squares—that
ensures convergence
is still achieved when combined with the on-policy operator O . Unfortunately, the ﬁx ed point of this
combined update operator is not guaranteed to be the best representable approximation of O’s ﬁx ed
point, q(cid:5) . Nevertheless, a bound can be proved on how close this altered ﬁx ed point is to the best
representable approximation.

We summarize a few details that will be useful below: First, the best least squares approximation is
computed with respect to the distribution z. The map from a general q vector onto its best approxi-
mation in col span((cid:8)) is deﬁned by another operator, P , which projects q into the column span of
2 = (cid:8)((cid:8)>Z(cid:8))(cid:0)1(cid:8)>Z q, where ^q is an approximation for
(cid:8), P q = argmin ^q2col span((cid:8)) kq (cid:0) ^qkz
value function q. The important property of this weighted projection is that it is a non-expansion op-
erator in k(cid:1)kz , i.e., kP qkz (cid:20) kqkz , which can be easily obtained from the generalized Pythagorean
theorem. Approximate dynamic programming then proceeds by composing the two operators —the
on-policy update O with the subspace projection P —to
compute the best representable approxima-
tion of the one step update. This combined operator is guaranteed to converge, since composing a
non-expansion with a contraction is still a contraction, i.e., kq+ (cid:0) q(cid:5) kz (cid:20) 1
1(cid:0)(cid:13) kq(cid:5) (cid:0) P q(cid:5) kz [7].
Linear function approximation in the dual case is a bit more complicated because matrices are being
represented, not vectors, and moreover the matrices need to satisfy row normalization and nonneg-
ativity constraints. Nevertheless, a very similar approach to the primal case can be successfully
applied. Recall that in the dual, the state-action visit distribution H is approximated by a linear
combination of bases in (cid:9). As in the primal case, there is no reason to expect that an update like
OH should keep the matrix in the simplex. Therefore, a projection operator must be constructed
that determines the best representable approximation to OH . One needs to be careful to deﬁne

this projection with respect to the right norm to ensure convergence. Here, the pseudo-norm k(cid:1)kz;r
deﬁned in Equation 1 suits this purpose. Deﬁne the weighted projection operator P over matrices
kH (cid:0) ^H kz;r
2
(3)
argmin
PH =
^H2simplex((cid:9))
The projection could be obtained by solving the above quadratic program. A key result is that this
projection operator is a non-expansion with respect to the pseudo-norm k(cid:1)kz;r .

Theorem 1 kPH kz;r (cid:20) kH kz;r

Proof: The easiest way to prove the theorem is to observe that the projection operator P is really
a composition of three orthogonal projections: ﬁrst, onto the linear subspace span((cid:9)), then onto
the subspace of row normalized matrices span((cid:9)) \ fH : H 1 = 1g, and ﬁnally onto the space of
nonnegative matrices span((cid:9)) \ fH : H 1 = 1g \ fH : H (cid:21) 0g. Note that the last projection into
the nonnegative halfspace is equivalent to a projection into a linear subspace for some hyperplane
tangent to the simplex. Each one of these projections is a non-expansion in k(cid:1)kz;r in the same way:
a generalized Pythagorean theorem holds. Consider just one of these linear projections P1
2
2 = kP1H r + H r (cid:0) P1H rkz
2 = kP1H + H (cid:0) P1H kz;r
kH kz;r
2 + kH (cid:0) P1H kz;r
2 = kP1H kz;r
2 + kH r (cid:0) P1H rkz
= kP1H rkz
Since the overall projection is just a composition of non-expansions, it must be a non-expansion.

2

As in the primal, approximate dynamic programming can be implemented by composing the on-
policy update O with the projection operator P . Since O is a contraction and P a non-expansion,
P O must also be a contraction, and it then follows that it has a ﬁx ed point. Note that, as in the tabular
case, this ﬁx ed point is only unique up to H r-equivalence, since the pseudo-norm k(cid:1)kz;r does not
distinguish H1 and H2 such that H1 r = H2 r. Here too, the ﬁx ed point is actually a simplex
of equivalent solutions. For simplicity, we denote the simplex of ﬁx ed points for P O by some
representative H+ = P OH+ . Finally, we can recover an approximation bound that is analogous
to the primal bound, which bounds the approximation error between H+ and the best representable
approximation to the on-policy ﬁx ed point H(cid:5) = OH(cid:5) .

Theorem 2 kH+ (cid:0) H(cid:5) kz;r (cid:20) 1
1(cid:0)(cid:13) kPH(cid:5) (cid:0) H(cid:5) kz;r

First note that kH+ (cid:0)H(cid:5) kz;r = kH+ (cid:0)PH(cid:5) +PH(cid:5) (cid:0)H(cid:5) kz;r (cid:20) kH+ (cid:0)PH(cid:5) kz;r +
Proof:
kPH(cid:5) (cid:0)H(cid:5) kz;r by generalized Pythagorean theorem. Then since H+ = P OH+ and P is a
non-expansion operator, we have kH+ (cid:0)PH(cid:5) kz;r = kP OH+ (cid:0)PH(cid:5) kz;r (cid:20) kOH+ (cid:0)H(cid:5) kz;r :
Finally, using H(cid:5) = OH(cid:5) and Lemma 1, we obtain kOH+ (cid:0)H(cid:5) kz;r = kOH+ (cid:0)OH(cid:5) kz;r (cid:20)
(cid:13) kH+ (cid:0)H(cid:5) kz;r : Thus (1(cid:0) (cid:13) )kH+ (cid:0)H(cid:5) kz;r (cid:20) kPH(cid:5) (cid:0)H(cid:5) kz;r .

To compare the primal and dual results, note that despite the similarity of the bounds, the projection
operators do not preserve the tight relationship between primal and dual updates. That is, even if
(1 (cid:0) (cid:13) )q = H r and (1 (cid:0) (cid:13) )(Oq) = (OH )r, it is not true in general that (1 (cid:0) (cid:13) )(P Oq) = (P OH )r.
The most obvious difference comes from the fact that in the dual, the space of H matrices has
bounded diameter, whereas in the primal, the space of q vectors has unbounded diameter in the
natural norms. Automatically, the dual updates cannot diverge with compositions like P O and PM;
yet, in the primal case, the update PM is known to not have ﬁx ed points in some circumstances [8].

5.2 Gradient Operator

In large scale problems one does not normally have the luxury of computing full dynamic pro-
gramming updates that evaluate complete expectations over the entire domain, since this requires
knowing the stationary visit distribution z for P (cid:5) (essentially requiring one to know the model of
the MDP). Moreover, full least squares projections are usually not practical to compute. A key in-
termediate step toward practical DP and RL algorithms is to formulate gradient step operators that
only approximate full projections. Conveniently, the gradient update and projection operators are
independent of the on-policy and off-policy updates and can be applied in either case. However, as
we will see below, the gradient update operator causes signi ﬁcant
instability in the off-policy update,

to the degree that divergence is a common phenomenon (much more so than with full projections).
Composing approximation with an off-policy update (max operator) in the primal case can be very
dangerous. All other operator combinations are better behaved in practice, and even those that are
not known to converge usually behave reasonably. Unfortunately, composing the gradient step with
an off-policy update is a common algorithm attempted in reinforcement learning (Q-learning with
function approximation), despite being the most unstable.

In the dual representation, one can derive a gradient update operator in a similar way to the primal,
except that it is important to maintain the constraints on the parameters w, since the basis functions
are probability distributions. We start by considering the projection objective
1
2 subject to
kH (cid:0) ^H kz;r
vec( ^H ) = (cid:9)w; w (cid:21) 0; w>1 = 1
JH =
2
The unconstrained gradient of the above objective with respect to w is
rw JH = (cid:9)> (r> (cid:10) I )>Z (r> (cid:10) I )((cid:9)w (cid:0) h) = (cid:0)>Z (r> (cid:10) I )( ^h (cid:0) h)
where (cid:0) = (r> (cid:10) I )(cid:9), h = vec(H ), and ^h = vec( ^H ). However, this gradient step cannot be
followed directly because we need to maintain the constraints. The constraint w>1 = 1 can be
maintained by ﬁrst projecting the gradient onto it, obtaining (cid:14)w = (I (cid:0) 1
k 11> )rw JH . Thus, the
weight vector can be updated by

1
11> )(cid:0)>Z (r> (cid:10) I )( ^h (cid:0) h)
wt+1 = wt (cid:0) (cid:11)(cid:14)w = wt (cid:0) (cid:11)(I (cid:0)
k
where (cid:11) is a step-size parameter. Then the gradient operator can then be deﬁned by
1
11> )(cid:0)>Z (r> (cid:10) I )( ^h (cid:0) h)
G ^hh = ^h (cid:0) (cid:11)(cid:9)(cid:14)w = ^h (cid:0) (cid:11)(cid:9)(I (cid:0)
k
(Note that to further respect the box constraints, 0 (cid:20) h (cid:20) 1, the stepsize might need to be reduced
and additional equality constraints might have to be imposed on some of the components of h that
are at the boundary values.)

Similarly as in the primal, since the target vector H (i.e., h) is determined by the underlying dynamic
programming update, this gives the composed updates
1
GO ^h = ^h (cid:0) (cid:11)(cid:9)(I (cid:0)
11> )(cid:0)>Z (r> (cid:10) I )( ^h(cid:0)O ^h) and
k
1
11> )(cid:0)>(r> (cid:10) I )( ^h(cid:0)M ^h)
GM ^h = ^h (cid:0) (cid:11)(cid:9)(I (cid:0)
k
respectively for the on-policy and off-policy cases (ignoring the additional equality constraints).

Thus far, the dual approach appears to hold an advantage over the standard primal approach, since
convergence holds in every circumstance where the primal updates converge, and yet the dual up-
dates are guaranteed never to diverge because the fundamental objects being represented are nor-
malized probability distributions (i.e., belong to a bounded simplex). We now investigate the con-
vergence properties of the various updates empirically.

6 Experimental Results

To investigate the effectiveness of the dual representations, we conducted experiments on various
domains, including randomly synthesized MDPs, Baird’s star problem [5], and on the mountain car
problem. The randomly synthesized MDP domains allow us to test the general properties of the
algorithms. The star problem is perhaps the most-cited example of a problem where Q-learning
with linear function approximation diverges [5], and the mountain car domain has been prone to
divergence with some primal representations [9] although successful results were reported when
bases are selected by sparse tile coding [10].

For each problem domain, twelve algorithms were run over 100 repeats with a horizon of 1000 steps.
The algorithms were: tabular on-policy (O), projection on-policy (P O), gradient on-policy (GO),
tabular off-policy (M), projection off-policy (PM), and gradient off-policy (GM), for both the
primal and the dual. The discount factor was set to (cid:13) = 0:9. For on-policy algorithms, we measure
the difference between the values generated by the algorithms and those generated by the analytically
determined ﬁx ed-point. For off-policy algorithms, we measure the difference between the values
generated by the resulting policy and the values of the optimal policy. The step size for the gradient
updates was 0:1 for primal representations and 100 for dual representations. The initial values of

state-action value functions q are set according to the standard normal distribution, and state-action
visit distributions H are chosen uniformly randomly with row normalization. Since the goal is to
investigate the convergence of the algorithms without carefully crafting features, we also choose
random basis functions according to a standard normal distribution for the primal representations,
and random basis distributions according to a uniform distribution for the dual representations.

Randomly Synthesized MDPs. For the synthesized MDPs, we generated the transition and re-
ward functions of the MDPs randomly—the
transition function is uniformly distributed between 0
and 1 and the reward function is drawn from a standard normal. Here we only reported the results
of random MDPs with 100 states, 5 actions, and 10 bases, observed consistent convergence of the
dual representations on a variety of MDPs, with different numbers of states, actions, and bases. In
Figure 1(right), the curve for the gradient off-policy update (GM) in the primal case (dotted line
with the circle marker) blows up (diverges), while all the other algorithms in Figure 1 converge.
Interestingly, the approximate error of the dual algorithm P OH (4:60(cid:2)10(cid:0)3 ) is much smaller than
the approximate error of the corresponding primal algorithm P Oq (4:23(cid:2) 10(cid:0)2 ), even though their
theoretical bounds are the same (see Figure 1(left)).

On−Policy Update on Random MDPs

Off−Policy Update on Random MDPs

1010

t
n
i
o
P
 
e
c
n
e
r
e
f
e
R
 
m
o
r
f
 
e
c
n
e
r
e
f
f
i
D

105

100

10−5

Oq

POq

G Oq

OH
POH
G OH

1010

t
n
i
o
P
 
e
c
n
e
r
e
f
e
R
 
m
o
r
f
 
e
c
n
e
r
e
f
f
i
D

105

100

10−5

Mq

PMq

G Mq

MH
PMH
G MH

10−10

100

200

300

400
600
500
Number of Steps

700

800

900

1000

10−10

100

200

300

400
600
500
Number of Steps

700

800

900

1000

Figure 1: Updates of state-action value q and visit distribution H on randomly synthesized MDPs

The Star Problem. The star problem has 7 states and 2 actions. The reward function is zero
for each transition. In these experiments, we used the same ﬁx ed policy and linear value function
approximation as in [5]. In the dual, the number of bases is also set to 14 and the initial values of the
state-action visit distribution matrix H are uniformly distributed random numbers between 0 and 1
with row normalization. The gradient off-policy update in the primal case diverges (see the dotted
line with the circle marker in Figure 2(right)). However, all the updates with the dual representation
algorithms converge.

On−Policy Update on Star Problem

Off−Policy Update on Star Problem

1010

t
n
i
o
P
 
e
c
n
e
r
e
f
e
R
 
m
o
r
f
 
e
c
n
e
r
e
f
f
i
D

105

100

10−5

Oq

POq

G Oq

OH
POH
G OH

1010

t
n
i
o
P
 
e
c
n
e
r
e
f
e
R
 
m
o
r
f
 
e
c
n
e
r
e
f
f
i
D

105

100

10−5

Mq

PMq

G Mq

MH
PMH
G MH

10−10

100

200

300

400
500
600
Number of Steps

700

800

900

1000

10−10

100

200

300

400
500
600
Number of Steps

700

800

900

1000

Figure 2: Updates of state-action value q and visit distribution H on the star problem

The Mountain Car Problem The mountain car domain has continuous state and action spaces,
which we discretized with a simple grid, resulting in an MDP with 222 states and 3 actions. The
number of bases was chosen to be 5 for both the primal and dual algorithms. For the same reason
as before, we chose the bases for the algorithms randomly. In the primal representations with linear
function approximation, we randomly generated basis functions according to the standard normal
distribution. In the dual representations, we randomly picked the basis distributions according to
the uniform distribution. In Figure 3(right), we again observed divergence of the gradient off-policy
update on state-action values in the primal, and the convergence of all the dual algorithms (see Figure
3). Again, the approximation error of the projected on-policy update P OH in the dual (1:90(cid:2) 101 )
is also considerably smaller than P Oq (3:26(cid:2) 102 ) in the primal.

On−Policy Update on Mountain Car

Off−Policy Update on Mountain Car

1010

t
n
i
o
P
 
e
c
n
e
r
e
f
e
R
 
m
o
r
f
 
e
c
n
e
r
e
f
f
i
D

105

100

10−5

Oq

POq

G Oq

OH
POH
G OH

1010

t
n
i
o
P
 
e
c
n
e
r
e
f
e
R
 
m
o
r
f
 
e
c
n
e
r
e
f
f
i
D

105

100

10−5

Mq

PMq

G Mq

MH
PMH
G MH

10−10

100

200

300

400
500
600
Number of Steps

700

800

900

1000

10−10

100

200

300

400
500
600
Number of Steps

700

800

900

1000

Figure 3: Updates of state-action value q and visit distribution H on the mountain car problem

7 Conclusion

Dual representations maintain an explicit representation of visit distributions as opposed to value
functions [4]. We extended the dual dynamic programming algorithms with linear function ap-
proximation, and studied the convergence properties of the dual algorithms for planning in MDPs.
We demonstrated that dual algorithms, since they are based on estimating normalized probability
distributions rather than unbounded value functions, avoid divergence even in the presence of ap-
proximation and off-policy updates. Moreover, dual algorithms remain stable in situations where
standard value function estimation diverges.

References

[1] M. Puterman. Markov Decision Processes: Discrete Dynamic Programming. Wiley, 1994.
[2] D. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, 1995.
[3] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.
[4] T. Wang, M. Bowling, and D. Schuurmans. Dual representations for dynamic programming and rein-
forcement learning. In Proceeding of the IEEE International Symposium on ADPRL, pages 44 –51, 2007.
[5] L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In International
Conference on Machine Learning, pages 30 –37, 1995.
[6] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
[7] J. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation.
IEEE Trans. Automat. Control, 42(5):674 –690, 1997.
[8] D. de Farias and B. Van Roy. On the existence of ﬁx ed points for approximate value iteration and
temporal-difference learning. J. Optimization Theory and Applic., 105(3):589–608, 2000.
[9] J. A. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the value
function. In NIPS 7, pages 369–376, 1995.
[10] R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding.
In Advances in Neural Information Processing Systems, pages 1038–1044, 1996.


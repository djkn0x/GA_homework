State Indexed Policy Search by Dynamic Programming 
 

Charles DuHadway 
5435537 

Yi Gu 
5103372 

 
December 14, 2007 
 

Abstract 
We  consider  the  reinforcement  learning  problem  of 
simultaneous 
trajectory-following 
and 
obstacle 
avoidance  by  a  radio-controlled  car. A  space-indexed 
non-stationary controller policy class  is chosen  that  is 
linear in the features set, where the multiplier of each 
feature  in  each  controller  is  learned  using  the  policy 
search  by  dynamic  programming  algorithm.  Under 
the control of the learned policies the radio-controlled 
car  is shown to be capable of reliably following a pre-
defined  trajectory  while  avoiding  point  obstacles 
along its way. 
 

1. Introduction 
We  consider  the  reinforcement  learning  problem  of 
trajectory-following involving discrete decisions. Casting 
the  problem  as  a  Markov  decision  process  (MDP),  we 
have a tuple (S, A, {Psa}, g, R) denoting, respectively, the 
set  of  states,  the  set  of  actions,  the  state  transition 
probabilities, the discount factor and the reward function, 
the  objective  is  to  find  a  policy  set  Popt  that  is  optimal 
with  respect  to  some  choice  of  R.  In  cases  where  the 
system dynamics, R and P are linear or linearizable, Popt 
can  be  readily  solved  for  e.g.  by  linear  quadratic 
regulators (LQR)  [1]. Our current work however  focuses 
on  the  case  where  such  linearity  does  not  hold,  and 
therefore  generalizes  the  class  of  trajectory-following 
problems 
non-trivial 
involving 
scenarios 
to 
environmental constraints. 
 
We  use  a  radio-controlled  (RC)  car  as  a  test  platform, 
and  introduce  non-linearities  to  R  and  P  through  the 
need to make discrete decisions in trying to avoid a point 
obstacle  along  a  pre-defined  trajectory.  We  choose  the 
policy  search  by  dynamic  programming 
(PSDP) 
algorithm  as  the  means  to  finding  Popt.  This  is  because 
given  a  base  distribution  (indicating  roughly  how  often 
we  expect  a  good  policy  to  visit  each  state)  PSDP  can 
efficiently  compute  Popt  in  spite  of  non-linear  R  and  P 
[2], hence our  trajectory-following problem  fits naturally 
into this framework. 
 

In  trajectory-following  problems  actions  customized  for 
local  path  characteristics  are  especially  useful.  Requisite 
to  such  exploitation  however  are:  a)  a  non-stationary 
policy  set  i.e.  p˛P  changes  as  the  car  traverses  the 
trajectory, b) an accurate correspondence between spatial 
position and the policy p used. However,  the accuracy  in 
this  correspondence  is  inevitably  degraded  by  run-time 
uncertainties.  Consequently  we  chose  for  the  policies  p 
to  be  indexed  over  space  instead  of  time  increments,  as 
under  such  a  scheme  the  correspondence  would  be 
immune from the said uncertainties.  
 
The  goal  of  our  work  is  to  investigate  the  feasibility  of 
employing  PSDP  in  trajectory-following  problems  under 
a space-indexed scheme. More concretely, we aim to find 
a  suitable  parameterization  of  the  system S,  action  space 
A,  discount  mechanism,  reward  function  R  and  policy 
class P, such that the RC car would reliably follow a pre-
defined  trajectory  in  the  absence  of  obstacles,  and  avoid 
obstacles with some margin in their presence. 
 
We will first present a space-indexed parameterization of 
the  system,  and  then  define  the  policy  class  and  feature 
set.  This  leads  on  to  the  algorithm’s  implementatio n, 
starting  with  a  brief  definition  of  PSDP,  followed  by  a 
section-by-section  account  of  the  choices  made  for  the 
various  functions,  parameters  and  other  algorithmic 
components.  The  results  and  accompanying  analysis  are 
provided in the final part of the paper. 

2. System parameterization 
Typically,  a  reasonable  system  parameterization  for  a 
vehicle could be 
[
]T
=
q
s
x
y
u
v
u
v
 
-
-
1
1
t
t
t
t
t
t
t
t
where  xt  and  yt  are  the  Cartesian  coordinates  of  the 
vehicle  in  the  world  coordinate  system  (WCS),  qt  the 
orientation  of  the  car  in  the  WCS,  and  ut  and  vt  are  the 
longitudinal  and 
the  vehicle 
in 
lateral  velocities 
coordinate  system  (VCS).  All  values  are  with  respect  to 
the  center  of  the  vehicle,  which  is  assumed  to  be  a 
rectangle.  The  subscripts  t  and  t-1  indicate  the  time 
instants 
the  values  correspond.  This 
to  which 
parameterization is illustrated graphically in Figure 1. 
 

1 

 

4,

T

]
T

All  pd(qd):  SﬁA  come  form  the  same  policy  class,  and 
for  simplicity  we  chose  each  pd(qd)  to  be  linear  in  the 
feature set i.e. 
(
)
(
)
(cid:215)
=
qqp
sg
[
d
d
d
d
=
qqqqq
1,
2,
3,
d
d
d
d
d
[
]T
=
(
)
g
g
g
g
sg
4
3
2
1
d
g1  is  simply  the  cross-track  error  ld,  g2  is  the  yaw  of  the 
car  with  respect  to  the  target  trajectory  tangent  at  d;  for 
example  if  the  orientation  of  the  vehicle  at  d  in WCS  is 
40(cid:176) while  the  trajectory  tangent at d  in WCS  is 37(cid:176),  then 
g2=3(cid:176).  Together  g1  and  g2  keep  the  vehicle  close  and 
parallel to the target trajectory. 
 
g3  is  the  anticipated  clearance  surplus  from  the  obstacle. 
For  instance  if  ld=1.5  cm  (1.5  cm  to  the  right  of  the 
trajectory  looking  in  the  direction  of  travel)  as  the  car  is 
approaching  a  point  obstacle  some  distance  ahead 
situated 5 cm  to  the  left of  the  trajectory, and  the desired 
clearance  is  20  cm  to  the  right  of  the  obstacle,  then 
g3=(1.5+5)-20=-13.5  cm  i.e.  the  car  needs  to  be  13.5  cm 
further  to  the  right  to  avoid  colliding  with  the  obstacle. 
As  a  conservative  measure,  g3 ’s  value  is  offset  by  5  cm 
so  that  g3=-25  cm  at  -20  cm  real  clearance,  increasing  to 
-5  cm  at  0  cm  real  clearance,  beyond  which  its  absolute 
value decays exponentially while remaining negative. 
 
g3  is  intended  to  steer  the  car  off  the  target  trajectory 
away  and  from  an  obstacle.  g3=0  when  no  obstacle  is 
within  the  control  horizon.  We  allow  the  car  to  re-
evaluate  which  side  to  pass  an  obstacle  on  at  each  step, 
the rule being that if the obstacle is currently to the left of 
the car,  then pass  it on  the right and vice versa, and g3  is 
computed accordingly. 
 
g4  is  the  obstacle  conditioned  yaw.  Its  value  is  non-zero 
only  when  the  car  is  approaching  an  obstacle  and  is 
steering in a direction that would reduce the clearance. In 
such  cases,  g4  is  yaw  of  the  trajectory  tangent  as 
measured  in  the  VCS.  This  feature  is  included  to 
preserve  any  clearance achieved  via  g3,  since an  existing 
clearance  would  tend  to  make  g3  small,  and  a  controller 
without g4 would be dominated by g1 and g2.  
 
This  concludes  the  definition  of  the  policy  class,  and we 
note  in  particular  that  the  policy  class  is  clearly  non-
linear and cannot be easily cast into a form suitable for a 
linear solver. 

Note 
is  not 
time 
that 
included  as  a  state  as  the 
mapping  of  controller  to  the 
time  instant  it  is  used  makes 
this 
the 
implicit.  Under 
space-indexed  scheme  for  a 
in  R2 
vehicle 
driving 
however,  each  controller  is 
mapped  to  a  subspace  in  R 
which  we  will 
call 
a 
Figure 1. Time-
“waypoint ”.  We  define  each 
indexed states 
waypoint  to  be  a  subspace 
orthogonal to the trajectory tangent at the point where the 
waypoint exists. This is illustrated in Figure 2. 
 

 

the 
To 
localize 
vehicle 
a 
along 
waypoint,  we  define 
l  as  the  cross-track 
error  between  where 
the  target  trajectory 
intersects 
the 
 
waypoint  and  where 
Figure 2. Space-indexed states 
the 
the  center  of 
vehicle  intersects  with  the  same  waypoint.  This  gives  a 
space-indexed system parameterization 
[
]T
=
q
v
t
s
l
v
u
u
 
-
-
1
1
d
d
d
d
d
d
d
d
where  td denotes  the  time at  space  index d along  the pre-
defined  target  trajectory,  while  qd,  ud  and  vd  are  defined 
similarly as before except they now denote state values at 
a  space  index  d.  Note  that  the  position  of  the  vehicle  is 
implicitly but completely specified. 
 
s =&
),( asf
 is  used  to 
The  simple  bicycle  model 
describe  the  vehicle  dynamics,  where  a˛A  is  an  action. 
State  propagation  from  index  d  to  d+1  is  achieved  by 
first  computing  Dt  such  that  at  td+1=td+Dt  the  vehicle  is 
precisely  at  waypoint  d+1,  then  states  at  d+1  are 
computed  by  propagating  sd  forward  through  f(s,a)  by 
exactly Dt. For brevity, states capturing historic values of 
the  steering  angle  used  to  model  the  steering  dynamics 
are not shown. 

3. Policy class 
The  ultimate  performance  of  the  algorithm  depends 
critically on the policy class, which is defined by both its 
format  and  the  features  that  constitutes  it.  As mentioned 
previously,  the  class  of  policy  employed  is  non-
i.e.  (cid:213)(Q)=(p1(q1), …, pD-1(qD-1)),  where 
stationary 
Q=(q1, …, qD-1),  so  that  we  have  a  one-to-one  mapping 
between each index d and stationary policy pd(qd). 
 

2 

4. Algorithm implementation 

-
1

E

-
1

 

=

s

;

d

”

)

|

i

)




(
sR

V
pp
,...,
t
D

-

1
D
∑


=
di
+
E

4.1 Definition 
We  start  by  considering  a  trajectory  that  spans  D  space 
indices  and  a  policy  class  as  defined  in  3,  we  define  the 
value function V(s) 
( )
1
”
s
D
)(
sR

(
pp
,...,
s
D
d
[
]s
( )
V
p
p
~
,...,
Ps
-
+
p
(
)
1
1
d
sds
D
~
Ps
where 
p indicates  that  the  expectation  is  with 
)(
s d
s
to 
respect 
s  drawn 
from 
the 
state 
transition 
P p . The PSDP algorithm takes the form: 
probability
)( s
s d
for d = D-1, D-2, …,0  
( )]
=
p
max
arg
[
s
 Set
V
ppm
p
,
1 ,...,
d
-
1
d
d
D
p
P˛
where  we  assume  we  are  given  a  sequence  of  base 
distributions  md  over  the  states.  PSDP  is  thus  a  policy 
iteration  algorithm  that  yields  an  optimal  pd  for  every 
space  index.  The  dynamic  programming  part  is  evident 
in  that  the  solution  for  md  starts  from  the  end  of  the 
trajectory towards the beginning. 

E

s

~

+

 

4.2 Initialization 
We now  turn our attention  to md, and note  that we do not 
know  the set of base distributions  (m0, m1, …,  mD-1) over a 
pre-defined  trajectory.  However  if  we  had  a  controller 
that  could  simultaneously  track  a  target  trajectory  and 
avoid  obstacles,  then  by  running  the  RC  car  over  the 
trajectory using that controller a large m number of times, 
we would see a representative distribution of s at d across 
all m trials. In this way we have effectively sampled from 
md without  knowing  the  explicit  form  of md;  the  problem 
is  that  if  we  had  such  a  controller,  we  wouldn ’t  ha ve 
needed to run PSDP  to begin with. One way around    this 
catch  22  is  to  start  PSDP  by  sampling  from  a  poor 
approximation of md i.e. 
 
1.  Run  the  RC  car  over  the  target  trajectory  without 
obstacles m  times  using  a  suboptimal  yet  reasonable 
controller.  Add  obstacles 
the  m 
to  each  of 
trajectories. 
2.  Derive  a  set  of  policies  (cid:213)  using  PSDP  based  on 
these m samples. 
3.  Run  the  RC  car  over  m  trajectories  with  obstacles 
using (cid:213). 
4.  Iterate  over  steps  2  and  3  until  (cid:213)  converges,  at 
which  point  we  deem  that  we  have  been  sampling 
from a good approximation of (m0, m1, …,  mD-1). 

 
In  our  implementation,  the  suboptimal  controller  is 
derived  in  two  steps.  First  we  compute  a  noiseless  state 

3 

trajectory  using  a  heuristically-tuned  PI  controller  over 
the target trajectory with no obstacles. The resulting state 
sequence  is  used  as  the  starting  point  for  finding  a  LQR 
controller  using  differential  dynamic  programming 
(DDP).  We  finally  obtain  m  samples  by  using  the  LQR 
controller in step 1 above. 
 
Graphically,  the  PSDP  algorithm  as  defined  in  4.1  and 
initialized in 4.2 appears as in Figure 3. 
 

Figure 3. PSDP algorithmic flow 

 

4.3 Action space 
The control of an RC car involves steering and throttle. If 
we  choose  to  drive  the  vehicle  at  constant  velocity,  then 
our  action  space  is  simplified  to  comprise  of  only 
steering.  The  steering  angle  range  of  the  RC  car  is  a 
continuous  interval  from  -28(cid:176)  (full  lock  left)  to  28(cid:176)  (full 
lock  right).  In  order  to  keep  the  calculation  of  V(s) 
tractable however this interval is discretized to 10 points: 
A={-28(cid:176), -21.78(cid:176), …
, 21.78(cid:176), 28(cid:176)}. 

4.4 Transition probability 
Two mechanisms  influence  the  transition from state sd  to 
sd+1  viz.  system  dynamics  and  noise.  The  former  is 
captured  by  f(s,a),  while  the  noise  is  modeled  as  zero-
mean  Gaussian  measurement  noise.  Consequently,  our 
transition probability is (
)2
+
D
st
(
,
,
)
~
asf
s
sN
 
+
1
d
d
d
d
where s2 is chosen to be consistent with variance of state 
measurements obtained from the RC car ’s Kalman filt er. 

4.5 Discount factor 
Recall that V(s) is the expected sum of R(s) over distance, 
and  terms  for  which  ghR(sd+h-1)  < e  for  some  small e  are 
ignored  as  they  have  negligible  effect  (those  more  than 
h-1 indices down the length of the trajectory from d). If g 
is  close  to  1  then  the  learned  policies  are  anticipatory  of 
the future and tend to be more globally optimal. However 
the  closer  g  is  to  1  the  larger  p  becomes  for  a  given  e, 
with  a  corresponding  longer  computation  time.  If  g  is 
close  to  0,  the  computation  time  would  be  shorter,  but 

the  policies  would  also  be  less  sensitive  to  future  high-
cost  consequences  to  current  actions,  making  them  only 
locally optimal. 
 
We  found  that  the  geometrically  decaying  weighting 
envelope g imposes could not yield a satisfactory balance 
between  global-optimality  and  computation  time  for  any 
value  between  0  and  1.  Consequently  we  sacrificed 
envelope  continuity  and  instead  employed  a  rectangular 
weighting  envelope.  V(s)  is  thus  the  sum  of  a  fixed 
number  h  of  reward  functions  each  of which  is weighted 
equally. 

2

4

 

b

)

=

b
3

(
sR d

otherwise

4.6 Reward function 
The  desired  behaviors  of  the  car  include:  a)  close 
tracking  of  the  target  trajectory,  b)  reliable  avoidance  of 
obstacles,  and  c)  use  of  steering  actions  that  are 
realizable  i.e.  limited  bandwidth  actuation.  Since  it  is 
more  natural  to  treat  these  attributes  in  the  negative  e.g. 
we do not wish the car to be far from the target trajectory, 
R(s)  will  henceforth  refer  to  a  cost  function  instead  of  a 
reward  function.  The  problem  remains  unchanged,  only 
now we wish  to minimize R(s)  instead of maximizing  it. 
From these a 4-component cost function was conceived: 
+

2
2
qb
lb
collision
 no if
&
2
1

 -

c

+
+

1
qb
&

2
c



1
where  q&  is  derivative  of  steering  angle,  c  is  the  distance 
between  the center of  the car and a point obstacle, and c1 
is  the  desired  minimum  clearance  to  an  obstacle.  If  
c  <  c1,  then  a  collision  is  considered  to  have  occurred  –
 
this effectively  treats  the car as a circular object and was 
a conscious and conservative choice. 
 
The  term  involving  l  penalizes  deviation  from  the  target 
path,  the  term  involving  q&  penalizes  rapid  changes  in 
steering,  the  constant  b4  penalizes  a  collision  and  the 
term  involving c and c1 enhances  the repulsion of  the car 
away from the obstacle during learning. 
 
Figure 4  is a visualization aid  to R(s), which has discrete 
regimes  and  is  clearly  non-linear.  Note  in  particular  the 
expected parabolic appearance of  the b1 contribution,  the 
half  cylinder  and  the  slight  half  cone  atop  of  it  in  b) 
attributable  to  the  b4  and  b3  terms  respectively.  The 
weighting 
parameters  b1 
chosen 
b4  were 
to 
experimentally. 
 

(a)  No obstacle 

(b)  Collision 
Figure 4. Cost function visualization 

 

 

 
 
Note  that  at  this  point,  we  have  also  fully  specified  the 
MDP  in  the  context  of  our  trajectory  following  problem, 
having defined the tuple (S, A, {Psa}, g, R). 

4.7 Optimization 
Referring  to Figure 3,  in each  iteration of PSDP we have 
m  sample  state  trajectories each containing D points. For 
a DP backup at d: 
 
1.  We  start  with  sd  on  the  first  sample  trajectory,  note 
the random number seed, take action a1˛A. 
2.  Compute  sd+1, …,s
d+h-1  (h  as  defined  in  4.5),  the 
( )sVa1
corresponding  costs  R(s)  and 
 (the  subscript 
a1 indicates unique correspondence to action a1). 
3.  Re-seed the random number generator with the value 
noted in 1, take action a2˛A and repeat step 2. 
4.  Repeat steps 2 and 3 for actions a3 to a10. 
5.  Repeat steps 1 to 4 for sample trajectories 2 to m. 
 
At  the  end  of  the  5  steps, we  have  a  record  of  how  V(s) 
varies  with  action  at  index  d  for  each  of  the  m  samples. 
We  then  use  the  coordinate  descend  algorithm  with 
dynamic  resolution  to  find  the  q  that  minimizes  the 
expected value i.e. we solve 
( )]
(
)
qp
=
arg
min
E
s
d
d
)
)
(
(
qp
QP˛
d
d
To  expedite  the  numeric  search,  the  starting  point  is 
chosen  to  be  the  least  squares  estimate  qinit=(BTB)-1BTb 
where  b  is  the  vector  of  value-minimizing  steering  angle 

[
V
ppm
,
d
d

p
1 ,...,
-+
hd

 

~

s

+

1

4 

specific  to  each  trajectory  and  B  is  a  matrix  containing 
the feature values associated with each  trajectory. This  is 
based on the observation that the least squares solution of 
q  to  individual  steering  actions  that  minimize  individual 
V(s)  is  often  close  to  the  solution  that  minimized  the 
aggregate. 

5. Results and discussion 

5.1  Performance 
The 
learned  controllers  performed  well 
in  both 
simulation and  the  physical  system. Example  trajectories 
from  both  are  shown  in  Figure  5.  To  quantitatively 
evaluate  the performance of a  learned controller we used 
both  the  cost  function  and  obstacle  collision  rate.  The 
controller shown in Figure 5 had a collision rate of 3.3%. 
 
The  target  and  actual  trajectories  are  shown  in  blue  and 
black  respectively.  Obstacles  are  shown  as  double  red 
circles.  The  inner  circle  denotes  the  closest  the  physical 
car  could  pass  by  the  obstacle  without  collision  if 
perfectly  aligned  (exactly  tangent  to  the  obstacle).  The 
outer circle denotes  the  farthest  the car could be and  still 
manage  to  hit  the  obstacle  given  worst  case  alignment 
(exactly  orthogonal  to  the  obstacle).  Note  the  controller 
perform wells despite  the obvious difference  in sampling 
rates between the actual and simulated systems. 
 

 
Figure 5. Simulated and physical trajectories 

 

 
Circular  trajectories  were  used  to  test  the  actual  car 
system due to physical limitations in our tracking system. 
Similar performance was observed in simulation for both 
straight and oval trajectories and we expect our system to 
perform well over a large variety of trajectories. 

5.2  Choosing a policy class and cost function 
Choosing  the  appropriate  policy  class  and  cost  function 
was  non-trivial  and  required  a  series  of  tests  to  choose 
between  several  alternatives.  Figure  6  shows  the  results 
of  one  such  test  comparing  two  cost  functions:  one  with 
an  accurate,  bounding  box  collision  test  and  one  with  a 
conservatively  inaccurate  bounding  circle  collision  test. 
In  this  case  the  latter’s  performance  far  exceeded 
the 
former ’s. 
 

 

x
 
 
 
 
 
 
 
 
)
%
(
 
e
t
a
r
 
n
o
i
s
i
l
l
o
C

100

90

80

70

60

50

40

30

20

10

0

1

Bounding Circle
Bounding Box

Bounding Circle
Bounding Box

70000

60000

50000

40000

30000

20000

10000

0

2

3

4

5

6

7

8

9

10

1

2

3

4

5

6

7

8

9

10

Iterations

It erat ions
Figure 6. Bounding box vs. circle collision test 

 
Similar  tests were  used  to choose  the  specific  feature  set 
and  cost  function  described  above.  Among  other  things 
we 
learned 
that 
the  standard 
integral 
term  was 
unnecessary  for  our  learning  task.  We  also  found  it 
important to use a cost function with no flat areas.  

5.3  Conclusions 
We  have  implemented  a  system  that  demonstrates  the 
feasibility  and  effectiveness  of  employing  PSDP  in  a 
state-indexed  scheme  to  solve  a  reinforcement  learning 
problem  with  a  non-linear  policy  class  and  reward 
function.  Our  approach  takes  full  advantage  of  PSDP’s 
strengths  while  eliminating  its  dependence  on  accurate 
space-time correlation. 

5.4  Next Steps 
The  obvious  next  step  is  a  detailed  comparison  of  state-
indexed  PSDP  with 
time-indexed  PSDP.  We  are 
confident 
that 
such  a  comparison  will  clearly 
demonstrate  the  superiority  of  state-based  PSDP.  This 
should  be  especially  clear  over  long  or  non-uniform 
trajectories. 

Acknowledgments 
The  authors  wish  to  acknowledge  valuable  inputs  from 
Andrew  Ng  and  Adam  Coates  during  the  course  of  this 
research project. 

References 
[1]  Abbeel  P.,  Coates  A.,  Quigley  M.,  and  Ng  A. 
Y.. An Application of Reinforcement Learning to 
Aerobatic Helicopter Flight, in NIPS 19, 2007 
[2]  Bagnell  J.  A.,  Kakade  S.,  Ng  Y  A.,  and 
Schneider  J..  Policy  search  by  dynamic 
programming, in NIPS 16, 2004. 

 

5 


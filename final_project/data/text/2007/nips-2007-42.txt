Inferring Neural Firing Rates from Spike Trains
Using Gaussian Processes

John P. Cunningham1 , Byron M. Yu1;2;3 , Krishna V. Shenoy1;2
1Department of Electrical Engineering,
2Neurosciences Program, Stanford University, Stanford, CA 94305
fjcunnin,byronyu,shenoyg@stanford.edu

Maneesh Sahani3
3Gatsby Computational Neuroscience Unit, UCL
Alexandra House, 17 Queen Square, London, WC1N 3AR, UK
maneesh@gatsby.ucl.ac.uk

Abstract

Neural spike trains present challenges to analytical efforts due to their noisy,
spiking nature. Many studies of neuroscienti(cid:2)c and neural prosthetic importance
rely on a smoothed, denoised estimate of the spike trainâ€™s underlying (cid:2)ring rate.
Current techniques to (cid:2)nd time-varying (cid:2)ring rates require ad hoc choices of
parameters, offer no con(cid:2)dence intervals on their estimates, and can obscure
potentially important single trial variability. We present a new method, based
on a Gaussian Process prior, for inferring probabilistically optimal estimates of
(cid:2)ring rate functions underlying single or multiple neural spike trains. We test the
performance of the method on simulated data and experimentally gathered neural
spike trains, and we demonstrate improvements over conventional estimators.

Introduction
1
Neuronal activity, particularly in cerebral cortex, is highly variable. Even when experimental con-
ditions are repeated closely, the same neuron may produce quite different spike trains from trial
to trial. This variability may be due to both randomness in the spiking process and to differences
in cognitive processing on different experimental trials. One common view is that a spike train is
generated from a smooth underlying function of time (the (cid:2)ring rate) and that this function carries
a signi(cid:2)cant portion of the neural information. If this is the case, questions of neuroscienti(cid:2)c and
neural prosthetic importance may require an accurate estimate of the (cid:2)ring rate. Unfortunately, these
estimates are complicated by the fact that spike data gives only a sparse observation of its underlying
rate. Typically, researchers average across many trials to (cid:2)nd a smooth estimate (averaging out spik-
ing noise). However, averaging across many roughly similar trials can obscure important temporal
features [1]. Thus, estimating the underlying rate from only one spike train (or a small number of
spike trains believed to be generated from the same underlying rate) is an important but challenging
problem.
The most common approach to the problem has been to collect spikes from multiple trials in a peri-
stimulus-time histogram (PSTH), which is then sometimes smoothed by convolution or splines [2],
[3]. Bin sizes and smoothness parameters are typically chosen ad hoc (but see [4], [5]) and the result
is fundamentally a multi-trial analysis. An alternative is to convolve a single spike train with a kernel.
Again, the kernel shape and time scale are frequently ad hoc. For multiple trials, researchers may
average over multiple kernel-smoothed estimates. [2] gives a thorough review of classical methods.

1

More recently, point process likelihood methods have been adapted to spike data [6](cid:150)[8]. These
methods optimize (implicitly or explicitly) the conditional intensity function (cid:21)(tjx(t); H (t)) (cid:151)
which gives the probability of a spike in [t; t + dt), given an underlying rate function x(t) and the
history of previous spikes H (t) (cid:151) with respect to x(t). In a regression setting, this rate x(t) may
be learned as a function of an observed covariate, such as a sensory stimulus or limb movement.
In the unsupervised setting of interest here, it is constrained only by prior expectations such as
smoothness. Probabilistic methods enjoy two advantages over kernel smoothing. First, they allow
explicit modelling of interactions between spikes through the history term H (t) (e.g., refractory
periods). Second, as we will see, the probabilistic framework provides a principled way to share
information between trials and to select smoothing parameters.
In neuroscience, most applications of point process methods use maximum likelihood estimation. In
the unsupervised setting, it has been most common to optimize x(t) within the span of an arbitrary
basis (such as a spline basis [3]). In other (cid:2)elds, a theory of generalized Cox processes has been
developed, where the point process is conditionally Poisson, and x(t) is obtained by applying a link
function to a draw from a random process, often a Gaussian process (GP) (e.g. [9]). In this approach,
parameters of the GP, which set the scale and smoothness of x(t) can be learned by optimizing the
(approximate) marginal likelihood or evidence, as in GP classi(cid:2)cation or regression. However, the
link function, which ensures a nonnegative intensity, introduces possibly undesirable artifacts. For
instance, an exponential link leads to a process that grows less smooth as the intensity increases.
Here, we make two advances. First, we adapt the theory of GP-driven point processes to incorpo-
rate a history-dependent conditional likelihood, suitable for spike trains. Second, we formulate the
problem such that nonnegativity in x(t) is achieved without a distorting link function or sacri(cid:2)ce of
tractability. We also demonstrate the power of numerical techniques that makes application of GP
methods to this problem computationally tractable. We show that GP methods employing evidence
optimization outperform both kernel smoothing and maximum-likelihood point process models.

2 Gaussian Process Model For Spike Trains
Spike trains can often be well modelled by gamma-interval point processes [6], [10]. We assume the
underlying nonnegative (cid:2)ring rate x(t) : t 2 [0; T ] is a draw from a GP, and then we assume that
our spike train is a conditionally inhomogeneous gamma-interval process (IGIP), given x(t). The
spike train is represented by a list of spike times y = fy0 ; : : :; yN g. Since we will model this spike
train as an IGIP1 , y j x(t) is by de(cid:2)nition a renewal process, so we can write:

p(y j x(t)) =

N
Yi=1
where p0 ((cid:1)) is the density of the (cid:2)rst spike occuring at y0 , and pT ((cid:1)) is the density of no spikes being
observed on (yN ; T ]; the density for IGIP intervals (of order (cid:13) (cid:21) 1) (see e.g. [6]) can be written as:

p(yi j yi(cid:0)1 ; x(t)) (cid:1) p0 (y0 j x(t)) (cid:1) pT (T j yN ; x(t));

(1)

(2)

p(yi j yi(cid:0)1 ; x(t)) =

x(u)du(cid:19)(cid:13)(cid:0)1
exp(cid:26)(cid:0)(cid:13) Z yi
(cid:0)((cid:13) ) (cid:18)(cid:13) Z yi
x(u)du(cid:27):
(cid:13)x(yi )
yi(cid:0)1
yi(cid:0)1
The true p0 ((cid:1)) and pT ((cid:1)) under this gamma-interval spiking model are not closed form, so we sim-
plify these distributions as intervals of an inhomogeneous Poisson process (IP). This step, which
we (cid:2)nd to sacri(cid:2)ce very little in terms of accuracy, helps to preserve tractability. Note also that
we write the distribution in terms of the inter-spike-interval distribution p(y i jyi(cid:0)1 ; x(t)) and not
(cid:21)(tjx(t); H (t)), but the process could be considered equivalently in terms of conditional intensity.
We now discretize x(t) : t 2 [0; T ] by the time resolution of the experiment ((cid:1), here 1 ms), to
yield a series of n evenly spaced samples x = [x1 ; : : :; xn ]0 (with n = T
(cid:1) ). The events y become
N + 1 time indices into x, with N much smaller than n. The discretized IGIP output process is now
(ignoring terms that scale with (cid:1)):
1The IGIP is one of a class of renewal models that works well for spike data (much better than inhomoge-
neous Poisson; see [6], [10]). Other log-concave renewal models such as the inhomogeneous inverse-Gaussian
interval can be chosen, and the implementation details remain unchanged.

2

p(y j x) =

xk(cid:1)(cid:27)(cid:21)

N
Yi=1
(cid:1) xy0 exp(cid:26)(cid:0)

yi(cid:0)1
xk(cid:1)(cid:19)(cid:13)(cid:0)1
(cid:20) (cid:13)xyi
exp(cid:26)(cid:0)(cid:13)
(cid:0)((cid:13) ) (cid:18)(cid:13)
Xk=yi(cid:0)1
y0(cid:0)1
n(cid:0)1
xk(cid:1)(cid:27) (cid:1) exp(cid:26)(cid:0)
Xk=yN
Xk=0
where the (cid:2)nal two terms are p0 ((cid:1)) and pT ((cid:1)), respectively [11]. Our goal is to estimate a smoothly
varying (cid:2)ring rate function from spike times. Loosely, instead of being restricted to only one family
of functions, GP allows all functions to be possible; the choice of kernel determines which functions
are more likely, and by how much. Here we use the standard squared exponential (SE) kernel. Thus,
x (cid:24) N ((cid:22)1; (cid:6)), where (cid:6) is the positive de(cid:2)nite covariance matrix de(cid:2)ned by

yi(cid:0)1
Xk=yi(cid:0)1
xk(cid:1)(cid:27);

(3)

(cid:20)
2

f exp(cid:26)(cid:0)
(ti (cid:0) tj )2(cid:27) + (cid:27)2
(cid:6) = (cid:8)K (ti ; tj )(cid:9)i;j2f1;:::;ng where K (ti ; tj ) = (cid:27)2
v (cid:14)ij :
For notational convenience, we de(cid:2)ne the hyperparameter set (cid:18) = [(cid:22); (cid:13) ; (cid:20); (cid:27) 2
v ]. Typically, the
f ; (cid:27)2
GP mean (cid:22) is set to 0. Since our intensity function is nonnegative, however, it is sensible to treat (cid:22)
instead as a hyperparameter and let it be optimized to a positive value. We note that other standard
kernels - including the rational quadratic, Matern (cid:23) = 3
2 , and Matern (cid:23) = 5
2 - performed similarly to
the SE; thus we only present the SE here. For an in depth discussion of kernels and of GP, see [12].
As written, the model assumes only one observed spike train; it may be that we have m trials believed
to be generated from the same (cid:2)ring rate pro(cid:2)le. Our method naturally incorporates this case: de(cid:2)ne
1 j x) = Ym
p(y(i) j x), where y(i) denotes the ith spike train observed.2 Otherwise, the
p(fygm
i=1
model is unchanged.

(4)

3 Finding an Optimal Firing Rate Estimate
3.1 Algorithmic Approach
Ideally, we would calculate the posterior on (cid:2)ring rate p(x j y) = R(cid:18) p(x j y; (cid:18))p((cid:18))d(cid:18) (integrating
over the hyperparameters (cid:18)), but this problem is intractable. We consider two approximations:
replacing the integral by evaluation at the modal (cid:18) , and replacing the integral with a sum over a
discrete grid of (cid:18) values. We (cid:2)rst consider choosing a modal hyperparameter set (ML-II model
selection, see [12]), i.e. p(x j y) (cid:25) q(x j y; (cid:18) (cid:3) ) where q((cid:1)) is some approximate posterior, and

(5)

p(y j x; (cid:18))p(x j (cid:18))dx:

(cid:18)(cid:3) = argmax
(cid:18)

p((cid:18) j y) = argmax
(cid:18)

p((cid:18))p(y j (cid:18)) = argmax
(cid:18)

p((cid:18)) Zx
(This and the following equations hold similarly for a single observation y or multiple observations
1 , so we consider only the single observation for notational brevity.) Speci(cid:2)c choices for the
fygm
hyperprior p((cid:18)) are discussed in Results. The integral in Eq. 5 is intractable under the distributions
we are modelling, and thus we must use an approximation technique. Laplace approximation and
Expectation Propagation (EP) are the most widely used techniques (see [13] for a comparison). The
Laplace approximation (cid:2)ts an unnormalized Gaussian distribution to the integrand in Eq. 5. Below
we show this integrand is log concave in x. This fact makes reasonable the Laplace approximation,
since we know that the distribution being approximated is unimodal in x and shares log concavity
with the normal distribution. Further, since we are modelling a non-zero mean GP, most of the
Laplace approximated probability mass lies in the nonnegative orthant (as is the case with the true
posterior). Accordingly, we write:
2Another reasonable approach would consider each trial as having a different rate function x that is a draw
from a GP with a nonstationary mean function (cid:22)(t). Instead of inferring a mean rate function x
(cid:3) , we would
learn a distribution of means. We are considering this choice for future work.

3

p(y j (cid:18)) = Zx

p(y j x; (cid:18))p(x j (cid:18))dx (cid:25) p(y j x(cid:3) ; (cid:18))p(x(cid:3) j (cid:18))

n
2

(2(cid:25))
j(cid:3)(cid:3) + (cid:6)(cid:0)1 j

;

1
2

(6)

x log p(y j x; (cid:18)) jx=x(cid:3) . Note that in general
where x(cid:3) is the mode of the integrand and (cid:3)(cid:3) = (cid:0)r2
both (cid:6) and (cid:3)(cid:3) (and x(cid:3) , implicitly) are functions of the hyperparameters (cid:18) . Thus, Eq. 6 can be
differentiated with respect to the hyperparameter set, and an iterative gradient optimization (we used
conjugate gradients) can be used to (cid:2)nd (locally) optimal hyperparameters. Algorithmic details and
the gradient calculations are typical for GP; see [12]. The Laplace approximation also naturally
provides con(cid:2)dence intervals from the approximated posterior covariance ((cid:6)(cid:0)1 + (cid:3)(cid:3) )(cid:0)1 .
We can also consider approximate integration over (cid:18) using the Laplace approximation above. The
Laplace approximation produces a posterior approximation q(x j y; (cid:18)) = N (cid:0)x(cid:3) ; ((cid:3)(cid:3) + (cid:6)(cid:0)1 )(cid:0)1 (cid:1)
and a model evidence approximation q((cid:18) j y) (Eq. 6). The approximate integrated posterior can be
written as p(x j y) = E(cid:18)jy [p(x j y; (cid:18))] (cid:25) Pj q(x j y; (cid:18)j )q((cid:18)j j y) for some choice of samples
(cid:18)j (which again gives con(cid:2)dence intervals on the estimates). Since the dimensionality of (cid:18) is small,
and since we (cid:2)nd in practice that the posterior on (cid:18) is well behaved (well peaked and unimodal), we
(cid:2)nd that a simple grid of (cid:18)j works very well, thereby obviating MCMC or another sampling scheme.
This approximate integration consistently yields better results than a modal hyperparameter set, so
we will only consider approximate integration for the remainder of this report.
For the Laplace approximation at any value of (cid:18) , we require the modal estimate of (cid:2)ring rate x(cid:3) ,
which is simply the MAP estimator:

x(cid:3) = argmax
x(cid:23)0

p(x j y) = argmax
x(cid:23)0

p(y j x)p(x):

(7)

Solving this problem is equivalent to solving an unconstrained problem where p(x) is a truncated
multivariate normal (but this is not the same as individually truncating each marginal p(x i ); see
[14]). Typically a link or squashing function would be included to enforce nonnegativity in x, but
this can distort the intensity space in unintended ways. We instead impose the constraint x (cid:23) 0,
which reduces the problem to being solved over the (convex) nonnegative orthant. To pose the
problem as a convex program, we de(cid:2)ne f (x) = (cid:0)log p(y j x)p(x):

f (x) =

N
Xi=1

yN (cid:0)1
Xk=y0

yi(cid:0)1
Xk=yi(cid:0)1

xk(cid:1)(cid:1)(cid:19) +
(cid:18)(cid:0)log xyi (cid:0) ((cid:13) (cid:0) 1)log (cid:0)
y0(cid:0)1
n(cid:0)1
1
Xk=yN
Xk=1
(x (cid:0) (cid:22)1)T (cid:6)(cid:0)1 (x (cid:0) (cid:22)1) + C;
(cid:0)log xy0 +
xk(cid:1) +
xk(cid:1) +
2

(cid:13)xk(cid:1)

where C represents constants with respect to x. From this form follows the Hessian

x f (x) = (cid:6)(cid:0)1 + (cid:3) where (cid:3) = (cid:0)r2
r2
x log p(y j x; (cid:18)) = B + D ;

(8)

(9)

(10)

where D = diag(x(cid:0)2
yN ) is positive semide(cid:2)nite and diagonal. B is
yi : : :; 0; : : :; x(cid:0)2
y0 ; : : :; 0; : : :; x(cid:0)2
block diagonal with N blocks. Each block is rank 1 and associates its positive, nonzero eigenvalue
with eigenvector [0; : : :; 0; bT
i ; 0; : : :; 0]T . The remaining n (cid:0) N eigenvalues are zero. Thus, B has
total rank N and is positive semide(cid:2)nite. Since (cid:6) is positive de(cid:2)nite, it follows then that the Hessian
is also positive de(cid:2)nite, proving convexity. Accordingly, we can use a log barrier Newton method to
ef(cid:2)ciently solve for the global MAP estimator of (cid:2)ring rate x(cid:3) [15].
In the case of multiple spike train observations, we need only add extra terms of negative log like-
lihood from the observation model. This (cid:3)ows through to the Hessian, where r2
x f (x) = (cid:6)(cid:0)1 + (cid:3)
and (cid:3) = (cid:3)1 + : : : + (cid:3)m , with (cid:3)i 8 i 2 f1; : : :; mg de(cid:2)ned for each observation as in Eq. 10.

4

3.2 Computational Practicality
This method involves multiple iterative layers which require many Hessian inversions and other ma-
trix operations (matrix-matrix products and determinants) that cost O(n3 ) in run-time complexity
and O(n2 ) in memory, where (x 2 IRn ). For any signi(cid:2)cant data size, a straightforward implemen-
tation is hopelessly slow. With 1 ms time resolution (or similar), this method would be restricted
to spike trains lasting less than a second, and even this problem would be burdensome. Achieving
computational improvements is critical, as a naive implementation is, for all practical purposes, in-
tractable. Techniques to improve computational performance are a subject of study in themselves
and are beyond the scope of this paper. We give a brief outline in the following paragraph.
In the MAP estimation of x(cid:3) , since we have analytical forms of all matrices, we avoid explicit
representation of any matrix, resulting in linear storage. Hessian inversions are avoided using the
matrix inversion lemma and conjugate gradients, leaving matrix vector multiplications as the single
costly operation. Multiplication of any vector by (cid:3) can be done in linear time, since (cid:3) is a (block-
wise) vector outer product matrix. Since we have evenly spaced resolution of our data x in time
indices ti , (cid:6) is Toeplitz; thus multiplication by (cid:6) can be done using Fast Fourier Transform (FFT)
methods [16]. These techniques allow exact MAP estimation with linear storage and nearly linear
run time performance. In practice, for example, this translates to solving MAP estimation problems
of 103 variables in fractions of a second, with minimal memory load. For the modal hyperparameter
scheme (as opposed to approximately integrating over the hyperparameters), gradients of Eq. 6
must also be calculated at each step of the model evidence optimization. In addition to using similar
techniques as in the MAP estimation, log determinants and their derivatives (associated with the
Laplace approximation) can be accurately approximated by exploiting the eigenstructure of (cid:3).
In total, these techniques allow optimal (cid:2)ring rates functions of 103 to 104 variables to be estimated
in seconds or minutes (on a modern workstation). These data sizes translate to seconds of spike
data at 1 ms resolution, long enough for most electrophysiological trials. This algorithm achieves a
reduction from a naive implementation which would require large amounts of memory and would
require many hours or days to complete.

4 Results
We tested the methods developed here using both simulated neural data, where the true (cid:2)ring rate
was known by construction, and in real neural spike trains, where the true (cid:2)ring rate was estimated
by a PSTH that averaged many similar trials. The real data used were recorded from macaque
premotor cortex during a reaching task (see [17] for experimental method). Roughly 200 repeated
trials per neuron were available for the data shown here.
We compared the IGIP-likelihood GP method (hereafter, GP IGIP) to other rate estimators (kernel
smoothers, Bayesian Adaptive Regressions Splines or BARS [3], and variants of the GP method)
using root mean squared difference (RMS) to the true (cid:2)ring rate. PSTH and kernel methods approxi-
mate the mean conditional intensity (cid:21)(t) = EH (t) [(cid:21)(tjx(t); H (t))]. For a renewal process, we know
(by the time rescaling theorem [7], [11]) that (cid:21)(t) = x(t), and thus we can compare the GP IGIP
(which (cid:2)nds x(t)) directly to the kernel methods. To con(cid:2)rm that hyperparameter optimization im-
proves performance, we also compared GP IGIP results to maximum likelihood (ML) estimates of
x(t) using (cid:2)xed hyperparameters (cid:18) . This result is similar in spirit to previously published likelihood
methods with (cid:2)xed bases or smoothness parameters. To evaluate the importance of an observation
model with spike history dependence (the IGIP of Eq. 3), we also compared GP IGIP to an inho-
mogeneous Poisson (GP IP) observation model (again with a GP prior on x(t); simply (cid:13) = 1 in
Eq. 3).
The hyperparameters (cid:18) have prior distributions (p((cid:18)) in Eq. 5). For (cid:27)f , (cid:20), and (cid:13) , we set log-
normal priors to enforce meaningful values (i.e. (cid:2)nite, positive, and greater than 1 in the case of
(cid:13) ). Speci(cid:2)cally, we set log((cid:27) 2
f ) (cid:24) N (5; 2) ; log((cid:20)) (cid:24) N (2; 2), and log((cid:13) (cid:0) 1) (cid:24) N (0; 100).
The variance (cid:27)v can be set arbitrarily small, since the GP IGIP method avoids explicit inversions
of (cid:6) with the matrix inversion lemma (see 3.2). For the approximate integration, we chose a grid
consisting of the empirical mean rate for (cid:22) (that is, total spike count N divided by total time T )
and ((cid:13) ; log((cid:27) 2
f ); log((cid:20))) 2 [1; 2; 4] (cid:2) [4; : : :; 8] (cid:2) [0; : : :; 7]. We found this coarse grid (or similar)
produced similar results to many other very (cid:2)nely sampled grids.

5

60

50

40

30

20

10

)
c
e
s
/
s
e
k
i
p
s
(
 
e
t
a
R
 
g
n
i
r
i
F

70

60

50

40

30

20

10

)
c
e
s
/
s
e
k
i
p
s
(
 
e
t
a
R
 
g
n
i
r
i
F

0

0

0.6

0.4

0.2

0.8
1
Time (sec)
(a) Data Set L20061107.214.1; 1 spike train

1.4

1.2

1.6

0

0

0.6

0.4

0.2

0.8
Time (sec)
(b) Data Set L20061107.14.1; 4 spike trains

1.2

1.4

1.6

1

50

45

40

35

30

25

20

15

10

5

)
c
e
s
/
s
e
k
i
p
s
(
 
e
t
a
R
 
g
n
i
r
i
F

16

14

12

10

8

6

4

2

)
c
e
s
/
s
e
k
i
p
s
(
 
e
t
a
R
 
g
n
i
r
i
F

0

0

0

1.2

0.6

0.4

0.2

0.8
1
0.8
Time (sec)
Time (sec)
(c) Data Set L20061107.151.5; 8 spike trains
(d) Data Set L20061107.46.3; 1 spike train
Figure 1: Sample GP (cid:2)ring rate estimate. See full description in text.

1.4

1.6

0.2

0.4

0.6

1.2

1.4

1.6

0

1

The four examples in Fig. 1 represent experimentally gathered (cid:2)ring rate pro(cid:2)les (according to the
methods in [17]). In each of the plots, the empirical average (cid:2)ring rate of the spike trains is shown
in bold red. For simulated spike trains, the spike trains were generated from each of these empirical
average (cid:2)ring rates using an IGIP ((cid:13) = 4, comparable to (cid:2)ts to real neural data). For real neural
data, the spike train(s) were selected as a subset of the roughly 200 experimentally recorded spike
trains that were used to construct the (cid:2)ring rate pro(cid:2)le. These spike trains are shown as a train of
black dots, each dot indicating a spike event time (the y-axis position is not meaningful). This spike
train or group of spike trains is the only input given to each of the (cid:2)tting models. In thin green and
magenta, we have two kernel smoothed estimates of (cid:2)ring rates; each represents the spike trains
convolved with a normal distribution of a speci(cid:2)ed standard deviation (50 and 100 ms). We also
smoothed these spike trains with adaptive kernel [18], (cid:2)xed ML (as described above), BARS [3],
and 150 ms kernel smoothers. We do not show these latter results in Fig. 1 for clarity of (cid:2)gures.
These standard methods serve as a baseline from which we compare our method. In bold blue, we
see x(cid:3) , the results of the GP IGIP method. The light blue envelopes around the bold blue GP (cid:2)ring
rate estimate represent the 95% con(cid:2)dence intervals. Bold cyan shows the GP IP method. This color
scheme holds for all of Fig. 1.
We then ran all methods 100 times on each (cid:2)ring rate pro(cid:2)le, using (separately) simulated and real
neural spike trains. We are interested in the average performance of GP IGIP vs. other GP methods
(a (cid:2)xed ML or a GP IP) and vs. kernel smoothing and spline (BARS) methods. We show these
results in Fig. 2. The four panels correspond to the same rate pro(cid:2)les shown in Fig. 1. In each
panel, the top, middle, and bottom bar graphs correspond to the method on 1, 4, and 8 spike trains,
respectively. GP IGIP produces an average RMS error, which is an improvement (or, less often,
a deterioration) over a competing method. Fig. 2 shows the percent improvement of the GP IGIP
method vs. the competing method listed. Only signi(cid:2)cant results are shown (paired t-test, p < 0:05).

6

     GP Methods                            Kernel Smoothers
GP IP       Fixed ML      short      medium      long     adaptive     BARS

(b) L20061107.14.1; 1,4,8 spike trains

     GP Methods                            Kernel Smoothers
GP IP       Fixed ML      short      medium      long     adaptive     BARS

     GP Methods                            Kernel Smoothers
GP IP       Fixed ML      short      medium      long     adaptive     BARS

(a) L20061107.214.1; 1,4,8 spike trains

     GP Methods                            Kernel Smoothers
GP IP       Fixed ML      short      medium      long     adaptive     BARS

50

%

0

50

%

0
50

%

0

50

%

0
50

%

0
50

%

0

50

%

0
50

%

0

50

%

0

50

%

0
50

%

0
50

%

0

(c) L20061107.151.5; 1,4,8 spike trains

(d) L20061107.46.3; 1,4,8 spike trains

Figure 2: Average percent RMS improvement of GP IGIP method (with model selection) vs. method
indicated in the column title. See full description in text.

Blue improvement bars are for simulated spike trains; red improvement bars are for real neural spike
trains. The general positive trend indicates improvements, suggesting the utility of this approach.
Note that, in the few cases where a kernel smoother performs better (e.g. the long bandwidth kernel
in panel (b), real spike trains, 4 and 8 spike trains), outperforming the GP IGIP method requires
an optimal kernel choice, which can not be judged from the data alone. In particular, the adaptive
kernel method generally performed more poorly than GP IGIP. The relatively poor performance of
GP IGIP vs. different techniques in panel (d) is considered in the Discussion section. The data
sets here are by no means exhaustive, but they indicate how this method performs under different
conditions.

5 Discussion

We have demonstrated a new method that accurately estimates underlying neural (cid:2)ring rate functions
and provides con(cid:2)dence intervals, given one or a few spike trains as input. This approach is not
without complication, as the technical complexity and computational effort require special care.
Estimating underlying (cid:2)ring rates is especially challenging due to the inherent noise in spike trains.
Having only a few spike trains deprives the method of many trials to reduce spiking noise. It is
important here to remember why we care about single trial or small number of trial estimates, since
we believe that in general the neural processing on repeated trials is not identical. Thus, we expect
this signal to be dif(cid:2)cult to (cid:2)nd with or without trial averaging.
In this study we show both simulated and real neural spike trains. Simulated data provides a good test
environment for this method, since the underlying (cid:2)ring rate is known, but it lacks the experimental
proof of real neural spike trains (where spiking does not exactly follow a gamma-interval process).
For the real neural spike trains, however, we do not know the true underlying (cid:2)ring rate, and thus we
can only make comparisons to a noisy, trial-averaged mean rate, which may or may not accurately
re(cid:3)ect the true underlying rate of an individual spike train (due to different cognitive processing on
different trials). Taken together, however, we believe the real and simulated data give good evidence
of the general improvements offered by this method.
Panels (a), (b), and (c) in Fig. 2 show that GP IGIP offers meaningful improvements in many cases
and a small loss in performance in a few cases. Panel (d) tells a different story. In simulation, GP
IGIP generally outperforms the other smoothers (though, by considerably less than in other panels).
In real neural data, however, GP IGIP performs the same or relatively worse than other methods.
This may indicate that, in the low (cid:2)ring rate regime, the IGIP is a poor model for real neural spiking.

7

It may also be due to our algorithmic approximations (namely, the Laplace approximation, which
allows density outside the nonnegative orthant). We will report on this question in future work.
Furthermore, some neural spike trains may be inherently ill-suited to analysis. A problem with this
and any other method is that of very low (cid:2)ring rates, as only occasional insight is given into the
underlying generative process. With spike trains of only a few spikes/sec, it will be impossible
for any method to (cid:2)nd interesting structure in the (cid:2)ring rate. In these cases, only with many trial
averaging can this structure be seen.
Several studies have investigated the inhomogeneous gamma and other more general models (e.g.
[6], [19]), including the inhomogeneous inverse gaussian (IIG) interval and inhomogeneous Markov
interval (IMI) processes. The methods of this paper apply immediately to any log-concave inhomo-
geneous renewal process in which inhomogeneity is generated by time-rescaling (this includes the
IIG and several others). The IMI (and other more sophisticated models) will require some changes
in implementation details; one possibility is a variational Bayes approach. Another direction for
this work is to consider signi(cid:2)cant nonstationarity in the spike data. The SE kernel is standard, but
it is also stationary; the method will have to compromise between areas of categorically different
covariance. Nonstationary covariance is an important question in modelling and remains an area of
research [20]. Advances in that (cid:2)eld should inform this method as well.

Acknowledgments
This work was supported by NIH-NINDS-CRCNS-R01, the Michael Flynn SGF, NSF, NDSEGF,
Gatsby, CDRF, BWF, ONR, Sloan, and Whitaker. This work was conceived at the UK Spike Train
Workshop, Newcastle, UK, 2006; we thank Stuart Baker for helpful discussions during that time.
We thank Vikash Gilja, Stephen Ryu, and Mackenzie Risch for experimental, surgical, and animal
care assistance. We thank also Araceli Navarro.

References
[1] B. Yu, A. Afshar, G. Santhanam, S. Ryu, K. Shenoy, and M. Sahani. Advances in NIPS, 17,
2005.
[2] R. Kass, V. Ventura, and E. Brown. J. Neurophysiol, 94:8(cid:150)25, 2005.
[3] I. DiMatteo, C. Genovese, and R. Kass. Biometrika, 88:1055(cid:150)1071, 2001.
[4] H. Shimazaki and S. Shinomoto. Neural Computation, 19(6):1503(cid:150)1527, 2007.
[5] D. Endres, M. Oram, J. Schindelin, and P. Foldiak. Advances in NIPS, 20, 2008.
[6] R. Barbieri, M. Quirk, L. Frank, M. Wilson, and E. Brown. J Neurosci Methods, 105:25(cid:150)37,
2001.
[7] E. Brown, R. Barbieri, V. Ventura, R. Kass, and L. Frank. Neural Comp, 2002.
[8] W. Truccolo, U. Eden, M. Fellows, J. Donoghue, and E. Brown. J Neurophysiol., 93:1074(cid:150)
1089, 2004.
[9] J. Moller, A. Syversveen, and R. Waagepetersen. Scandanavian J. of Stats., 1998.
[10] K. Miura, Y. Tsubo, M. Okada, and T. Fukai. J Neurosci., 27:13802(cid:150)13812, 2007.
[11] D. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes. Springer, 2002.
[12] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
[13] M. Kuss and C. Rasmussen. Journal of Machine Learning Res., 6:1679(cid:150)1704, 2005.
[14] W. Horrace. J Multivariate Analysis, 94(1):209(cid:150)221, 2005.
[15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[16] B. Silverman. Journal of Royal Stat. Soc. Series C: Applied Stat., 33, 1982.
[17] C. Chestek, A. Batista, G. Santhanam, B. Yu, A. Afshar, J. Cunningham, V. Gilja, S. Ryu,
M. Churchland, and K. Shenoy. J Neurosci., 27:10742(cid:150)10750, 2007.
[18] B. Richmond, L. Optican, and H. Spitzer. J. Neurophys., 64(2), 1990.
[19] R. Kass and V. Ventura. Neural Comp, 14:5(cid:150)15, 2003.
[20] C. Paciorek and M. Schervish. Advances in NIPS, 15, 2003.

8


Predicting New Search-Query Cluster Volume

Jacob Sisk, Cory Barr

December 14, 2007

1 Problem Statement

Search engines allow people to ﬁnd information important to them, and search engine
companies derive their proﬁt from delivering paid advertis ements in response to user
queries as well as “organic” results. The paid advertisemen ts are matched to the user’s
query, usually by way of shared similar keywords or topics. This poses many prob-
lems for search-engine companies and their advertisers, almost all of which stem from
the long tail of the distribution of user queries. Search-engine companies have spent
tremendous effort monetizing this tail by providing imaginative techologies to match
low-frequency queries ( “Riemannian manifold”) to relevan t advertisements ( “Springer
book sale”).
Novel queries are an under-monetized segment of this long tail. The world changes
rapidly. New products, news, gossip, memes, stories and ideas consistently emerge.
Predicting the volume of queries about these novel topics is the subject of this report.
If we observe a novel query, the likelihood of never seeing that query again is
67.1% (measured one month past the query’s initial appearance). On the other hand,
if we observe that novel query occuring a few times (even better if by a few different
people), it becomes more probable the query is about some new idea or thing, and we
are more likely to see it again in the future.

2 Data

2.1 Query Logs, Dataset Construction

To build a corpus of novel queries, we constructed a Bloom ﬁlt er 30 gigabytes in size
with an estimated false positive rate of less than 0.01% containing over 25 billion
queries issued to the Yahoo! search engine in 2005 and 2006. We then sampled 2.5%
of the search trafﬁc from January 2007, retaining only queri es not issued in 2005 or
2006. There are 16.8 million unique queries in this sample. Since we are looking for
new topics and believe queries about new topics may take many different lexical forms,
we Porter-stemmed the queries. Then, for each novel query q occuring at time t0 , we
built a regular time series beginning at t0 using a period of one minute that recorded
1) the number of times q was re-issued in each subsequent minute (for up to 28 days),
2) the number of new users in each minute t0 + i who issued q but never issued q in

1

, t0 , t0 + 5)

f (q(i)
1
2
3 to 9
≥ 10

D = 5
|qi |
16.5m 0.0033
0.0193
213k
0.0662
12k
155
0.1355

10 min.
0.0049
0.0264
0.0775
0.1355

30 min.
0.0071
0.0347
0.0886
0.1355

60 min.
0.008
0.0373
0.092
0.1419

3 hrs
0.009
0.0392
0.0952
0.1419

1 day
0.0116
0.0429
0.1006
0.1484

7 days
0.0184
0.0506
0.1124
0.1484

28 days
0.0275
0.0595
0.122
0.1548

Table 1: Probability of query repetition given frequency in ﬁrst 5 minutes

, t0 , t0 + 5)

u(q(i)
1
2
≥ 3

D = 5
|qi |
16.8m 0.0036
2472
0.0825
0.22
200

10 min
0.0052
0.1331
0.325

30 min
0.0075
0.1913
0.41

60 min
0.0084
0.2091
0.445

3 hrs
0.0094
0.2229
0.47

1 day
0.012
0.2573
0.495

7 days
0.0189
0.2876
0.5

28 days
0.028
0.3139
0.515

Table 2: Probability of query repetition given user count in ﬁrst 5 minutes

t0 , ..., t0 + i − 1, and 3) the number of repeat users who issued q in minute t0 + i and also
issued q at some point prior to t0 + i.

2.2 Descriptive Statistics

To help design features, we examined how informative tallies of query frequency were
in small time windows early in the history of a novel query. Speci ﬁcally, if a novel
t0 and has frequency f (q(i) ) in [t0 , t0 + d)–which we will
query q(i) is ﬁrst issued at
, t0 , t0 + d)–it is instructive to empirically estimate the likelihood t hat q(i)
denote f (q(i)
is issued in some larger, later time window [t0 + d, t0 + d + D]. It is equally instructive
to do this considering the number of novel users issuing q(i) , which we denote u(q), in
[t0 , t0 + d). This gives us an estimate of
, t0 , t0 + d(cid:17)
, t0 + d, t0 + d + D) > 0k f (q(i)
P (cid:16) f (q(i)
Table 1 shows an estimate for the conditional probability of repeated query issuance
given the frequency we observe for that query in the ﬁrst ﬁve m inutes of its lifespan.
This table demonstates that seeing a novel query more than once in ﬁve minutes greatly
increases the chances we will see that query again, both in the next few minutes as
well as up to a month in the future. Table 2 demonstates we can estimate the same
reoccurrence for u(q, t0 + d, t0 + d + D), and the effect is even stronger.

(1)

2.3 Clustering: From Queries to Topics

After examining the recorded queries, we felt clustering semantically and temporally
related queries would provide aggregate cluster statistics and much more informative
training data for a supervised learning problem than examining individual query behav-
ior. In addition, a substantial portion of novel queries are not useful to search engine
advertisers, including navigational queries (13.7% of queries in our sample), DNS er-
rors, etc. We hypothesised a cluster-inclusion criterion could be designed that many
of these unwanted queries would not pass, and eliminating unclustered queries would
improve our training set.

2

label

coverag execut hussein videotap jazeera

queries

coverag of saddam execut
al jazeera coverag of saddam execut
videotap of saddam hussein execut
saddam hussein execut full coverag
saddam hussein hospit bed execut

carbon collector nikki spe ne
ne for spe carbon nikki
ne spe
free download of ne for spe carbon
ne for spe carbon g
carbon collector nikki spe ne
ne for spe carbon collector locat p

Table 3: Example Query Clusters and Their Labels

We clustered using an agglomerative algorithm based on the Jaccard distance be-
tween two queries. This distance was extended to a Jaccard distance between a query
and a cluster by means of comparing a query to a computer-generated cluster label
of tokens in the cluster selected by a tf.idf criterion. This technique did discard most
non-informative queries in addition to ameliorating data sparsity issues. Unfortunately,
despite implementing an inverted index to eliminate comparing queries to others with
no token intersection, the algorithm was computationally expensive, and we had to sub-
sample down to 0.025% of the queries from January 2007. Table 3 presents examples
of some clusters and their labels.
It should be noted that the scope of this study is to examine the feasibility of pre-
dicting search-query volume through supervised learning. However, focusing on clus-
tering lies outside our present scope. Consequently, our experiments are designed to
conﬁrm the feasibility of predicting search-query cluster volume and persistence given
a reasonably well-clustered training set, which our initial clustering method achieved.

3 Predicting Future Query Volume

3.1 Experimental Framework

For every experiment, our design matrix was constructed by breaking up the ﬁrst T 1
of a cluster’s query volume history into a regular time series of n pieces some e apart,
recording query volume (or l og(1 + vol ume)) for each of the time slices. The dependent
variable was the total query volume observed in the next T 2 units of time for that
cluster.

3.2 Regression

Due to data sparsity, logistic regression performed unpredictably. Linear regression
faired better. Regressions were compared to a baseline model of predicting zero future
query volume. For almost all choices of T1 , T2 and e, regression outperformed the base-
line, sometimes radically. Table 4 shows root mean-squared-error for the regression
with the baseline in parentheses. Statistics are the result of 10-fold cross validation.

3.3 Support Vector Machine Prediction

We had constraints on computing time, but preliminary experiments showed an SVM
with a polynomial kernel could signi ﬁcantly outperform the linear model. Furthermore,

3

T1 = 30 ( 1
2 hour)
T1 = 60 (1 hour)

T1 = 240 (4 hours)

T1 = 1440 (one day)

T1 = 10080 (one week)

T2 = 1440 (1 day)
e = 5 : 0.22(0.23)
e = 5 : 0.21(0.22)
e = 5 : 0.21(0.21)
e = 30 : 0.20(0.21)
e = 60 : 0.20(0.21)
e = 5 : 0.24(0.22)
e = 30 : 0.20(0.22)
e = 60 : 0.20(0.22)
e = 360 : 0.20(0.21)
e = 30 : 0.17(0.18)
e = 60 : 0.17(0.18)
e = 360 : 0.17(0.18)
e = 1440 : 0.17(0.18)

T2 = 10080 (1 week)
e = 5 : 0.40(0.52)
e = 5 : 0.40(0.52)
e = 5 : 0.41(0.52)
e = 30 : 0.40(0.51)
e = 60 : 0.40(0.52)
e = 5 : 0.47(0.51)
e = 30 : 0.40(0.51)
e = 60 : 0.39(0.50)
e = 360 : 0.40(0.51)
e = 30 : 0.40(0.45)
e = 60 : 0.39(0.45)
e = 360 : 0.38(0.45)
e = 1440 : 0.38(0.45)

T2 = 40320 (28 days)
e = 5 : 0.32(0.85)
e = 5 : 0.32(0.84)
e = 5 : 0.34(0.84)
e = 30 : 0.32(0.84)
e = 60 : 0.33(0.84)
e = 5 : 0.72(0.82)
e = 30 : 0.36(0.82)
e = 60 : 0.35(0.82)
e = 360 : 0.34(0.82)
e = 60 : 0.51(0.70)
e = 360 : 0.48(0.70)
e = 1440 : 0.47(0.70)

Table 4: Linear Regression for Future Query Volume Prediction

the SVM did a very good job of predicting outliers. This is important, since outlier
queries are likely the most easily monetizable.
Figure 1 provides evidence of the model’s accuracy. The graph displays a distinctly
modal tendency. Our model either predicts correct search volume extremely well, or
predicts no volume. However, above a certain volume threshold the SVM performs
with exceptional precision. Since our goal is to ultimately predict novel, persistant
large-volume clusters for search-engine monetization, performance below some vol-
ume threshold is likely to be unimportant or perhaps entirely irrelevant. Therefore, the
performance of this model implies ﬁtting a model using an SVM provides satisfactory
application performance.

Figure 1: SVM Regression (T1 =1 day, T2 =1 week) (3rd degree polynomial kernel)

3.4 Markov Model

We felt a Markov-model prediction system could be appropriate for our time-series
data. We deﬁned states as discretized volume levels with the
following ranges: 0,

4

START
0
1i
1d
2i
2d
10i
10d
20i
20d
100i
100d

START 0
0.07
0.13
0.89
0.00
0.92
0.00
0.92
0.00
0.80
0.00
0.86
0.00
0.08
0.04
0.11
0.04
0.03
0.03
0.08
0.03
0.07
0.07
0.06
0.06

1i
0.07
0.03
0.02
0.03
0.05
0.04
0.04
0.04
0.06
0.03
0.07
0.06

1d
0.07
0.07
0.05
0.05
0.08
0.05
0.08
0.04
0.06
0.03
0.07
0.06

2i
0.07
0.00
0.01
0.00
0.05
0.02
0.20
0.04
0.03
0.03
0.07
0.06

2d
0.07
0.00
0.01
0.00
0.02
0.02
0.08
0.36
0.03
0.03
0.07
0.06

10i
0.07
0.00
0.00
0.00
0.00
0.00
0.16
0.04
0.09
0.08
0.07
0.06

10d
0.07
0.00
0.00
0.00
0.00
0.00
0.08
0.14
0.03
0.13
0.07
0.06

20i
0.07
0.00
0.00
0.00
0.00
0.00
0.04
0.04
0.34
0.13
0.13
0.06

20d
0.07
0.00
0.00
0.00
0.00
0.00
0.04
0.04
0.20
0.34
0.07
0.19

100i
0.07
0.00
0.00
0.00
0.00
0.00
0.04
0.04
0.03
0.03
0.07
0.06

100d
0.07
0.00
0.00
0.00
0.00
0.00
0.04
0.04
0.03
0.03
0.07
0.06

Table 5: Markov Model State-Transition Probabilities

1, 2-9, 10-19, 20-99, 100-999, and 1000+. In addition, each volume-bucket state is
partitioned into an “increasing” and “decreasing” state, w
here “increasing” is deﬁned
as having at least 50% of the query volume in the latter half of the time span. In our
experiment, each state covers one day. Table 5 presents state-transition probabilities.
A phenomenon occurs when the volume reaches 20 queries in a day. The state
transition indicates the probability of maintaining at least the current volume increases
30%, which suggests a possible metric for cluster persistence.
Our Markov-model prediction system consistently predicts a daily total volume of
0 if the previous day had volume less than 10. In our experiments, the prediction for
higher-volume clusters was always several states too low (but never 0). We believe a
second-order Markov model could offer greater prediction. The Markov model also
has two advantages over the SVM. First, it can readily model query volume per day of
the week, which differs radically. Second, it offers the distinct advantage of being in-
tuitively understandable for non-quantitive employees. However, we believe the SVM
model remains the superior choice.

4 Future Work

Though tangential to our supervised regression study, it seems clear that increasing per-
formance of query clustering is the most likely means to increase the ultimate utility
of the supervised-learning research. Consequently, current efforts are focused on unsu-
pervised clustering techniques that incorporate cluster-inclusion criteria and automatic
determination of the optimal number of clusters.
It would also be worthwhile to add richer features. Speci ﬁca lly, including standard
ARMA time series features (second order, differenced, etc) and to comparing their per-
formance to that of an SVM on the ﬁrst order features with a non -linear kernel might
prove informative.
The primary motivation for this research is to quickly alert online advertisers about
emerging topics for which people are searching. Advertisers “bid” on phrases they
think will be issued by users who want their products or services. Consequently, a
query (or query in a cluster) becoming bidded by an advertiser is as useful a dependent
variable as future query volume. Behavior encoded in the time-series may help predict
biddedness. However, biddedness might prove best handled as an addition classi ﬁca-
tion problem once high-volume, persistant clusters are identi ﬁed.

5


Topmoumoute online natural gradient algorithm

Nicolas Le Roux
University of Montreal
nicolas.le.roux@umontreal.ca

Pierre-Antoine Manzagol
University of Montreal
manzagop@iro.umontreal.ca

Yoshua Bengio
University of Montreal
yoshua.bengio@umontreal.ca

Abstract

Guided by the goal of obtaining an optimization algorithm that is both fast and
yields good generalization, we study the descent direction maximizing the de-
crease in generalization error or the probability of not increasing generalization
error. The surprising result is that from both the Bayesian and frequentist perspec-
tives this can yield the natural gradient direction. Although that direction can be
very expensive to compute we develop an ef(cid:2)cient, general, online approximation
to the natural gradient descent which is suited to large scale problems. We re-
port experimental results showing much faster convergence in computation time
and in number of iterations with TONGA (Topmoumoute Online natural Gradient
Algorithm) than with stochastic gradient descent, even on very large datasets.

Introduction

An ef(cid:2)cient optimization algorithm is one that quickly (cid:2)nds a good minimum for a given cost func-
tion. An ef(cid:2)cient learning algorithm must do the same, with the additional constraint that the func-
tion is only known through a proxy. This work aims to improve the ability to generalize through
more ef(cid:2)cient learning algorithms.
Consider the optimization of a cost on a training set with access to a validation set. As the end
objective is a good solution with respect to generalization, one often uses early stopping: optimizing
the training error while monitoring the validation error to (cid:2)ght over(cid:2)tting. This approach makes
the underlying assumption that over(cid:2)tting happens at the later stages. A better perspective is that
over(cid:2)tting happens all through the learning, but starts being detrimental only at the point it overtakes
the (cid:147)true(cid:148) learning. In terms of gradients, the gradient of the cost on the training set is never collinear
with the true gradient, and the dot product between the two actually eventually becomes negative.
Early stopping is designed to determine when that happens. One can thus wonder: can one limit
over(cid:2)tting before that point? Would this actually postpone that point?
From this standpoint, we discover new justi(cid:2)cations behind the natural gradient [1]. Depending on
certain assumptions, it corresponds either to the direction minimizing the probability of increasing
generalization error, or to the direction in which the generalization error is expected to decrease
the fastest. Unfortunately, natural gradient algorithms suffer from poor scaling properties, both with
respect to computation time and memory, when the number of parameters becomes large. To address
this issue, we propose a generally applicable online approximation of natural gradient that scales
linearly with the number of parameters (and requires computation time comparable to stochastic
gradient descent). Experiments show that it can bring signi(cid:2)cant faster convergence and improved
generalization.

1

1 Natural gradient
Let eL be a cost de(cid:2)ned as eL((cid:18)) = Z L(x; (cid:18))p(x)dx where L is a loss function over some parameters
(cid:18) and over the random variable x with distribution p(x). The problem of minimizing eL over (cid:18) is often
encountered and can be quite dif(cid:2)cult. There exist various techniques to tackle it, their ef(cid:2)ciency
depending on L and p. In the case of non-convex optimization, gradient descent is a successful
technique. The approach consists in progressively updating (cid:18) using the gradient eg = d eL
d(cid:18) .
[1] showed that the parameter space is a Riemannian space of metric eC (the covariance of the
gradients), and introduced the natural gradient as the direction of steepest descent in this space.
The natural gradient direction is therefore given by eC (cid:0)1 eg. The Riemannian space is known to
correspond to the space of functions represented by the parameters (instead of the space of the
parameters themselves).
The natural gradient somewhat resembles the Newton method. [6] showed that, in the case of a mean
squared cost function, the Hessian is equal to the sum of the covariance matrix of the gradients and
of an additional term that vanishes to 0 as the training error goes down. Indeed, when the data are
generated from the model, the Hessian and the covariance matrix are equal. There are two important
differences: the covariance matrix eC is positive-de(cid:2)nite, which makes the technique more stable,
but contains no explicit second order information. The Hessian allows to account for variations in
the parameters. The covariance matrix accounts for slight variations in the set of training samples. It
also means that, if the gradients highly disagree in one direction, one should not go in that direction,
even if the mean suggests otherwise. In that sense, it is a conservative gradient.

2 A new justi(cid:2)cation for natural gradient

Until now, we supposed we had access to the true distribution p. However, this is usually not the
case and, in general, the distribution p is only known through the samples of the training set. These
samples de(cid:2)ne a cost L (resp. a gradient g ) that, although close to the true cost (resp. gradient), is
not equal to it. We shall refer to L as the training error and to eL as the generalization error. The
danger is then to over(cid:2)t the parameters (cid:18) to the training set, yielding parameters that are not optimal
with respect to the generalization error.
A simple way to (cid:2)ght over(cid:2)tting consists in determining the point when the continuation of the
optimization on L will be detrimental to eL. This can be done by setting aside some samples to
form a validation set that will provide an independent estimate of eL. Once the error starts increasing
on the validation set, the optimization should be stopped. We propose a different perspective on
over(cid:2)tting. Instead of only monitoring the validation error, we consider using as descent direction
an estimate of the direction that maximizes the probability of reducing the generalization error. The
goal is to limit over(cid:2)tting at every stage, with the hope that the optimal point with respect to the
validation should have lower generalization error.
Consider a descent direction v . We know that if v T eg is negative then the generalization error drops
(for a reasonably small step) when stepping in the direction of v . Likewise, if v T g is negative then
the training error drops. Since the learning objective is to minimize generalization error, we would
like vT eg as small as possible, or at least always negative.
nXi=1
1
@L(xi ; (cid:18))
By de(cid:2)nition, the gradient on the training set is g =
and n is the
gi where gi =
@ (cid:18)
n
number of training samples. With a rough approximation, one can consider the g i s as draws from the
true gradient distribution and assume all the gradients are independent and identically distributed.
The central limit theorem then gives
g (cid:24) N  eg ; eC
n !
where eC is the true covariance matrix of @L(x;(cid:18))
@ (cid:18) wrt p(x).
2

(1)

We will now show that, both in the Bayesian setting (with a Gaussian prior) and in the frequentist
setting (with some restrictions over the type of gradient considered), the natural gradient is optimal
in some sense.

(3)

2.1 Bayesian setting
In the Bayesian setting, eg is a random variable. We would thus like to de(cid:2)ne a posterior over eg given
the samples gi in order to have a posterior distribution over v T eg for any given direction v . The prior
over eg will be a Gaussian centered in 0 of variance (cid:27) 2 I . Thus, using eq. 1, the posterior over eg given
the gi s (assuming the only information over eg given by the gi s is through g and C ) is
eg jg ; eC (cid:24) N 0@ I + eC
(cid:27)2 + n eC (cid:0)1(cid:19)(cid:0)11A
n(cid:27)2 !(cid:0)1
g ; (cid:18) I
(2)
n(cid:27)2 , we therefore have
Denoting eC(cid:27) = I +
eC
vT egjg ; eC (cid:24) N  vT eC (cid:0)1
!
(cid:27) eC v
vT eC (cid:0)1
(cid:27) g ;
n
Using this result, one can choose between several strategies, among which two are of particular
interest:
(cid:15) choosing the direction v such that the expected value of v T eg is the lowest possible (to
maximize the immediate gain). In this setting, the direction v to choose is
(4)
v / (cid:0) eC (cid:0)1
(cid:27) g :
If (cid:27) < 1, this is the regularized natural gradient. In the case of (cid:27) = 1, eC(cid:27) = I and this
is the batch gradient descent.
(cid:15) choosing the direction v to minimize the probability of v T eg to be positive. This is equiva-
lent to (cid:2)nding
vT eC (cid:0)1
(cid:27) g
argminv
pvT eC (cid:0)1
(cid:27) eC v
(we dropped n for the sake of clarity, since it does not change the result). If we square this
quantity and take the derivative with respect to v , we (cid:2)nd 2 eC (cid:0)1
(cid:27) g(vT eC (cid:0)1
(cid:27) g)(vT eC (cid:0)1
(cid:27) eC v) (cid:0)
(cid:27) g)2 at the numerator. The (cid:2)rst term is in the span of eC (cid:0)1
(cid:27) g and the second
2 eC (cid:0)1
(cid:27) eC v(vT eC (cid:0)1
one is in the span of eC (cid:0)1
(cid:27) eC v . Hence, for the derivative to be zero, we must have g / eC v
(since eC and eC(cid:27) are invertible), i.e.
(5)
v / (cid:0) eC (cid:0)1 g :
This direction is the natural gradient and does not depend on the value of (cid:27) .

2.2 Frequentist setting
In the frequentist setting, eg is a (cid:2)xed unknown quantity. For the sake of simplicity, we will only
consider (as all second-order methods do) the directions v of the form v = M T g (i.e. we are only
allowed to go in a direction which is a linear function of g ).
n (cid:17), we have
Since g (cid:24) N (cid:16)eg ;
eC
vT eg = gT M g (cid:24) N  egT M eg; egT M T eCM eg
!
n
The matrix M (cid:3) which minimizes the probability of v T eg to be positive satis(cid:2)es
M (cid:3) = argminM egT M eg
egT M T CM eg
3

(6)

(7)

The numerator of the derivative of this quantity is egeg T M T eCM egegT (cid:0) 2 eCM egegT M egegT . The (cid:2)rst
term is in the span of eg and the second one is in the span of eCM eg . Thus, for this derivative to be
0 for all eg , one must have M / eC (cid:0)1 and we obtain the same result as in the Bayesian case: the
natural gradient represents the direction minimizing the probability of increasing the generalization
error.
3 Online natural gradient
The previous sections provided a number of justi(cid:2)cations for using the natural gradient. However,
the technique has a prohibitive computational cost, rendering it impractical for large scale problems.
Indeed, considering p as the number of parameters and n as the number of examples, a direct batch
implementation of the natural gradient is O(p2 ) in space and O(np2 + p3 ) in time, associated re-
spectively with the gradients’ covariance storage, computation and inversion. This section reviews
existing low complexity implementations of the natural gradient, before proposing TONGA, a new
low complexity, online and generally applicable implementation suited to large scale problems. In
the previous sections we assumed the true covariance matrix eC to be known. In a practical algorithm
we of course use an empirical estimate, and here this estimate is furthermore based on a low-rank
approximation denoted C (actually a sequence of estimates Ct ).
3.1 Low complexity natural gradient implementations
[9] proposes a method speci(cid:2)c to the case of multilayer perceptrons. By operating on blocks of
the covariance matrix, this approach attains a lower computational complexity 1. However, the tech-
nique is quite involved, speci(cid:2)c to multilayer perceptrons and requires two assumptions: Gaussian
distributed inputs and a number of hidden units much inferior to that of input units. [2] offers a more
general approach based on the Sherman-Morrison formula used in Kalman (cid:2)lters: the technique
maintains an empirical estimate of the inversed covariance matrix that can be updated in O(p 2 ). Yet
the memory requirement remains O(p2 ). It is however not necessary to compute the inverse of the
gradients’ covariance, since one only needs its product with the gradient. [10] offers two approaches
to exploit this. The (cid:2)rst uses conjugate gradient descent to solve C v = g . The second revisits
[9] thereby achieving a lower complexity. [8] also proposes an iterative technique based on the
minimization of a different cost. This technique is used in the minibatch setting, where C v can be
computed cheaply through two matrix vector products. However, estimating the gradient covariance
only from a small number of examples in one minibatch yields unstable estimation.

3.2 TONGA
Existing techniques fail to provide an implementation of the natural gradient adequate for the large
scale setting. Their main failings are with respect to computational complexity or stability. TONGA
was designed to address these issues, which it does this by maintaining a low rank approximation of
the covariance and by casting both problems of (cid:2)nding the low rank approximation and of computing
the natural gradient in a lower dimensional space, thereby attaining a much lower complexity. What
we exploit here is that although a covariance matrix needs many gradients to be estimated, we can
take advantage of an observed property that it generally varies smoothly as training proceeds and
moves in parameter space.

3.2.1 Computing the natural gradient direction between two eigendecompositions
Even though our motivation for the use of natural gradient implied the covariance matrix of the em-
pirical gradients, we will use the second moment (i.e. the uncentered covariance matrix) throughout
the paper (and so did Amari in his work). The main reason is numerical stability. Indeed, in the
batch setting, we have (assuming C is the centered covariance matrix and g the mean) v = C (cid:0)1 g ,
thus C v = g . But then, (C + gg T )v = g + ggT v = g(1 + gT v) and
v
(C + ggT )(cid:0)1 g =
1 + gT v
1Though the technique allows for a compact representation of the covariance matrix, the working memory
requirement remains the same.

(8)

= (cid:22)v

4

Even though the direction is the same, the scale changes and the norm of the direction is bounded
kgk cos(g ;v) .
by
1
Since TONGA operates using a low rank estimate of the gradients’ non-centered covariance, we
must be able to update cheaply. When presented with a new gradient, we integrate its information
using the following update formula2:

(13)

(10)
(11)

(9)
Ct = (cid:13) ^Ct(cid:0)1 + gt gT
t
where C0 = 0 and ^Ct(cid:0)1 is the low rank approximation at time step t (cid:0) 1. Ct is now likely of
greater rank, and the problem resides in computing its low rank approximation ^Ct . Writing ^Ct(cid:0)1 =
t(cid:0)1 ,
Xt(cid:0)1X T
t with Xt = [p(cid:13)Xt(cid:0)1
Ct = XtX T
gt ]
With such covariance matrices, computing the (regularized) natural direction v t is equal to
vt = (Ct + (cid:21)I )(cid:0)1 gt = (XtX T
t + (cid:21)I )(cid:0)1 gt
t + (cid:21)I )(cid:0)1Xtyt with yt = [0; : : : 0; 1]T :
vt = (XtX T
Using the Woodbury identity with positive de(cid:2)nite matrices [7], we have
(12)
t Xt + (cid:21)I )(cid:0)1 yt
vt = Xt (X T
If Xt is of size p (cid:2) r (with r < p, thus yielding a covariance matrix of rank r), the cost of this
t Xt can be rewritten as
computation is O(pr2 + r3 ). However, since the Gram matrix Gt = X T
(cid:19) ;
(cid:19) = (cid:18)
Gt = (cid:18) (cid:13)X T
p(cid:13)X T
t(cid:0)1Xt(cid:0)1 p(cid:13)X T
(cid:13)Gt(cid:0)1
t(cid:0)1 gt
t(cid:0)1 gt
p(cid:13) gT
p(cid:13) gT
gT
gT
t Xt(cid:0)1
t gt
t Xt(cid:0)1
t gt
the cost of computing Gt using Gt(cid:0)1 reduces to O(pr + r3 ). This stresses the need to keep r small.
3.2.2 Updating the low-rank estimate of Ct
To keep a low-rank estimate of Ct = XtX T
t , we can compute its eigendecomposition and keep only
the (cid:2)rst k eigenvectors. This can be made at low cost using its relation to that of G t :
Gt = V DV T
(14)
Ct = (XtV D(cid:0) 1
2 )D(XtV D(cid:0) 1
2 )T
The cost of such an eigendecomposition is O(kr 2 + pkr) (for the computation of the eigendecom-
position of the Gram matrix and the computation of the eigenvectors, respectively). Since the cost of
computing the natural direction is O(pr + r 3 ), it is computationally more ef(cid:2)cient to let the rank of
Xt grow for several steps (using formula 12 in between) and then compute the eigendecomposition
using
t+b with Xt+b = h(cid:13)Ut ;
2 gt+b ]i
b(cid:0)1
t+b
1
Ct+b = Xt+bX T
(cid:13)
(cid:13)
2 gt+b(cid:0)1 ;
(cid:13)
: : :
2 gt+1 ;
with Ut the unnormalized eigenvectors computed during the previous eigendecomposition.
3.2.3 Computational complexity
The computational complexity of TONGA depends on the complexity of updating the low rank
approximation and on the complexity of computing the natural gradient. The cost of updating the
approximation is in O(k(k + b)2 + p(k + b)k) (as above, using r = k + b). The cost of computing
the natural gradient vt is in O(p(k + b) + (k + b)3 ) (again, as above, using r = k + b). Assuming
k + b (cid:28) p(p) and k (cid:20) b, TONGA’s total computational cost per each natural gradient computation
is then O(pb).
Furthermore, by operating on minibatch gradients of size b 0 , we end up with a cost per example of
b0 ). Choosing b = b0 , yields O(p) per example, the same as stochastic gradient descent. Empiri-
O( bp
cal comparison of cpu time also shows comparable CPU time per example, but faster convergence.
In our experiments, p was in the tens of thousands, k was less than 5 and b was less than 50.
The result is an approximate natural gradient with low complexity, general applicability and (cid:3)exi-
bility over the tradoff between computations and the quality of the estimate.
2The second term is not weighted by 1(cid:0)(cid:13) so that the in(cid:3)uence of gt in Ct is the same for all t, even t = 0.To
keep the magnitude of the matrix constant, one must use a normalization constant equal to 1 + (cid:13) + : : : + (cid:13) t .

5


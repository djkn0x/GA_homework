Modeling Natural Sounds
with Modulation Cascade Processes

Richard E. Turner and Maneesh Sahani
Gatsby Computational Neuroscience Unit
17 Alexandra House, Queen Square, London, WC1N 3AR, London

Abstract

Natural sounds are structured on many time-scales. A typical segment of speech,
for example, contains features that span four orders of magnitude: Sentences
(∼ 1 s); phonemes (∼ 10−1 s); glottal pulses (∼ 10−2 s); and formants (. 10−3 s).
The auditory system uses information from each of these time-scales to solve com-
plicated tasks such as auditory scene analysis [1]. One route toward understand-
ing how auditory processing accomplishes this analysis is to build neuroscience-
inspired algorithms which solve similar tasks and to compare the properties of
these algorithms with properties of auditory processing. There is however a dis-
cord: Current machine-audition algorithms largely concentrate on the shorter
time-scale structures in sounds, and the longer structures are ignored. The rea-
son for this is two-fold. Firstly, it is a difﬁcult technical problem to construct
an algorithm that utilises both sorts of information. Secondly, it is computation-
ally demanding to simultaneously process data both at high resolution (to extract
short temporal information) and for long duration (to extract long temporal infor-
mation). The contribution of this work is to develop a new statistical model for
natural sounds that captures structure across a wide range of time-scales, and to
provide efﬁcient learning and inference algorithms. We demonstrate the success
of this approach on a missing data task.

1 Introduction

Computational models for sensory processing are still in their infancy, but one promising approach
has been to compare aspects of sensory processing with aspects of machine-learning algorithms
crafted to solve the same putative task. A particularly fruitful approach in this vein uses the genera-
tive modeling framework to derive these learning algorithms. For example, Independent Component
Analysis (ICA) and Sparse Coding (SC), Slow Feature Analysis (SFA), and Gaussian Scale Mix-
ture Models (GSMMs) are examples of algorithms corresponding to generative models that show
similarities with visual processing [3]. In contrast, there has been much less success in the auditory
domain, and this is due in part to the paucity of ﬂexible models with an explicit temporal dimension
(although see [2]). The purpose of this paper is to address this imbalance.

This paper has three parts. In the ﬁrst we review models for the short-time structure of sound and
argue that a probabilistic time-frequency model has several distinct beneﬁts over traditional time-
frequency representations for auditory modeling. In the second we review a model for the long-time
structure in sounds, called probabilistic amplitude demodulation.
In the third section these two
models are combined with the notion of auditory features to produce a full generative model for
sounds called the Modulation Cascade Process (MCP). We then show how to carry out learning and
inference in such a complex hierarchical model, and provide results on speech for complete and
missing data tasks.

1

2 Probabilistic Time-Frequency Representations
Most representations of sound focus on the short temporal structures. Short segments (< 10−1 s) are
frequently periodic and can often be efﬁciently represented in a Fourier basis as the weighted sum of
a few sinusoids. Of course, the spectral content of natural sounds changes slowly over time. This is
handled by time-frequency representations, such as the Short-Time Fourier Transform (STFT) and
spectrogram, which indicate the spectral content of a local, windowed section of the sound. More
T 0X
speciﬁcally, the STFT ( xd,t ) and spectrogram (sd,t ) of a discretised sound (yt0 ) are given by,
t0=1
The (possibly frequency dependent) duration of the window (rt−t0 ) must be chosen carefully, as it
controls whether features are represented in the spectrum or in the time-variation of the spectra. For
example, the window for speech is typically chosen to last for several pitch periods, so that both
pitch and formant information is represented spectrally.

rt−t0 yt0 exp (−iωd t0 ) ,

sd,t = log |xd,t |.

xd,t =

(1)

The ﬁrst stage of the auditory pathway derives a time-frequency-like representation mechanically at
the basilar membrane. Subsequent stages extract progressively more complex auditory features, with
structure extending over more time. Thus, computational models of auditory processing often begin
with a time-frequency (or auditory-ﬁlter bank) decomposition, deriving new representations from
the time-frequency coefﬁcients [4]. Machine-learning algorithms also typically operate on the time-
frequency coefﬁcients, and not directly on the waveform. The potential advantage lies in the ease
with which auditory features may be extracted from the STFT representation. There are, however,
associated problems. For example, time-frequency representations tend to be over-complete (e.g.
the number of STFT coefﬁcients tends to be larger than the number of samples of the original sound
T × D > T 0 ). This means that realisable sounds live on a manifold in the time-frequency space (for
the STFT this manifold is a hyper-plane). Algorithms that solve tasks like ﬁlling-in missing data
or denoising must ensure that the new coefﬁcients lie on the manifold. Typically this is achieved in
an ad hoc manner by projecting time-frequency coefﬁcients back onto the manifold according to an
arbitrary metric [5]. For generative models of time-frequency coefﬁcients, it is difﬁcult to force the
model to generate only on the realisable manifold. An alternative is to base a probabilistic model of
the waveform on the same heuristics that led to the original time-frequency representation. Not only
does this side-step the generation problem, but it also allows parameters of the representation, like
the “window”, to be chosen automatically.
The heuristic behind the STFT – that sound comprises sinusoids in slowly-varying linear superpo-
p(xd,t |xd,t−1 ) = Norm (cid:0)λdxd,t−1 , σ2
(cid:1) ,
sition – led Qi et al [6] to propose a probabilistic algorithm called Bayesian Spectrum Estimation
(BSE), in which the sinusoid coefﬁcients ( xd,t ) are latent variables. The forward model is,
 X
!
d
p(yt |xt ) = Norm
xd,t sin (ωd t + φd ) , σ2
y
d
The prior distribution over the coefﬁcients is Gaussian and auto-regressive, evolving at a rate con-
d . Thus, as λd → 1 and σ2
d → 0 the processes become
trolled by the dynamical parameters λd and σ2
very slow, and as λd → 0 and σ2
d → ∞ they become very fast. More precisely, the length-scale of
the coefﬁcients is given by λd = − log(λd ). The observations are generated by a weighted sum of
sinusoids, plus Gaussian noise. This model is essentially a Linear Gaussian State Space System with
time varying weights deﬁned by the sinusoids. Thus, inference is simple, proceeding via the Kalman
Smoother recursions with time-varying weights. In effect, these recursions dynamically adjust the
window used to derive the coefﬁcients, based on the past history of the stimulus. BSE is a model for
the short-time structure of sounds and it will essentially form the bottom level of the MCP. In the
next section we turn our attention to a model of the longer-time structure.

(3)

(2)

.

3 Probabilistic Demodulation Cascade

A salient property of the long-time statistics of sounds is the persistence of strong amplitude mod-
ulation [7]. Speech, for example, contains power in isolated regions corresponding to phonemes.

2

The phonemes themselves are localised into words, and then into sentences. Motivated by these
observations, Anonymous Authors [8] have proposed a model for the long-time structures in sounds
using a demodulation cascade. The basic idea of the demodulation cascade is to represent a sound as
a product of processes drawn from a hierarchy, or cascade, of progressively longer time-scale mod-
ulators. For speech this might involve three processes: representing sentences on top, phonemes in
the middle, and pitch and formants at the bottom (e.g. ﬁg. 1A and B). To construct such a repre-
sentation, one might start with a traditional amplitude demodulation algorithm, which decomposes
a signal into a quickly-varying carrier and more slowly-varying envelope. The cascade could then
be built by applying the same algorithm to the (possibly transformed) envelope, and then to the en-
velope that results from this, and so on. This procedure is only stable, however, if both the carrier
and the envelope found by the demodulation algorithm are well-behaved. Unfortunately, traditional
methods (like the Hilbert Transform, or low-pass ﬁltering a non-linear transformation of the stimu-
lus) return a suitable carrier or envelope, but not both. A new approach to amplitude demodulation
is thus called for.

z(m)
t

= Norm

p

z(m)
0

x(m)
t = fa(m)

z(m)
t

λm z(m)
t−1 , σ2
m

x(1)
t = Norm (0, 1) ,

In a nutshell, the new approach is to view amplitude demodulation as a task of probabilistic in-
ference. This is natural, as demodulation is fundamentally ill-posed — there are inﬁnitely many
decompositions of a signal into a positive envelope and real valued carrier — and so prior infor-
mation must always be leveraged to realise such a decomposition. The generative model approach
makes this information explicit. Furthermore, it not necessary to use the recursive procedure (just
(cid:16)
(cid:17)
(cid:16)
(cid:17)
(cid:16)
(cid:17) ∀t > 0,
described) to derive a modulation cascade: the whole hierarchy can be estimated at once using a
single generative model. The generative model for Probabilistic Amplitude Demodulation (PAD) is
|z(m)
(cid:17) ∀m > 1,
(cid:16)
MY
= Norm (0, 1) ,
t−1
p
x(m)
.
t
m=1
A set of modulators (X2:M ) are drawn in a two stage process: First a set of slowly varying processes
(Z2:M ) are drawn from a one-step linear Gaussian prior (identical to Eq. 2). The effective length-
scales of these processes, inherited by the modulators, are ordered such that λm > λm−1 . Second
the modulators are formed by passing these variables through a point-wise non-linearity to enforce
(cid:1) = log (cid:0) exp (cid:0)z(m)
(cid:0)z(m)
t + a(m) (cid:1) + 1(cid:1),
positivity. A typical choice might be
t
which is logarithmic for large negative values of z(m)
, and linear for large positive values. This
t
transforms the Gaussian distribution over z(m)
into a sparse, non-negative, distribution, which is a
t
good match to the marginal distributions of natural envelopes. The parameter a(m) controls exactly
where the transition from log to linear occurs, and consequently alters the degree of sparsity. These
positive signals modulate a Gaussian white-noise carrier, to yield observations y1:T by a simple
point-wise product. A typical draw from this generative model can be seen in Fig. 1C. This model
is a fairly crude one for natural sounds. For example, as described in the previous section, we
expect that the carrier process will be structured and yet it is modelled as Gaussian white noise. The
surprising observation is that this very simple model is excellent at demodulation.

(4)

(5)

yt =

fa(m)

(6)

Inference in this model typically proceeds by a zero-temperature EM-like procedure. Firstly the
carrier (x(1)
) is integrated out and then the modulators are found by maximum a posteriori (MAP).
t
Slower, more Bayesian algorithms that integrate over the modulators using MCMC indicate that this
approximation is not too severe, and the results are compelling.

4 Modulation Cascade Processes

We have reviewed two contrasting models: The ﬁrst captures the local harmonic structure of sounds,
but has no long-time structure; The second captures long-time amplitude modulations, but models
the short-time structure as white noise. The goal of this section is to synthesise both to form a new
model. We are guided by the observation that the auditory system might implement a similar syn-
thesis. In the well-known psychophysical phenomenon of comodulation masking release (see [9] for
a review), a tone masked by noise with a bandwidth greater than an auditory ﬁlter becomes audible

3

Figure 1: An example of a modulation-cascade representation of speech (A and B) and typical sam-
ples from generative models used to derive that representation (C). A) The spoken-speech waveform
(black) is represented as the product of a carrier (blue), a phoneme modulator (red) and a sentence
modulator (magenta). B) A close up of the ﬁrst sentence ( 2 s) additionally showing the derived enve-
lope (x(2)
t x(3)
) superposed onto the speech (red, bottom panel). C) The generative model (M = 3)
t
with a carrier (blue), a phoneme modulator (red) and a sentence modulator (magenta).

if the noise masker is amplitude modulated. This suggests that long-time envelope information is
processed and analysed across (short-time) frequency channels in the auditory system.

p

(7)

(8)

(9)

p

p

(cid:16)

A simple way to combine the two models would be to express each ﬁlter coefﬁcient of the time-
frequency model as a product of processes (e.g. xd,t = x(1)
d,t x(2)
d,t ). However, power across even
widely seperated channels of natural sounds can be strongly correlated [7]. Furthermore, comodu-
lation masking release suggests that amplitude-modulation is processed across frequency channels
and not independently in each channel. Presumably this reﬂects the collective modulation of wide-
band (or harmonic) sounds, with features that span many frequencies. Thus, a synthesis of BSE and
PAD should incorporate the notion of auditory features.
(cid:16)
(cid:17)
(cid:16)
(cid:17)
The forward model. The Modulation Cascade Process (MCP) is given by
(cid:16)
(cid:17)
km ,t |z(m)
λ(m) z(m)
z(m)
m = 1 : 3, t > 0,
= Norm
km ,t−1 , σ2
km ,t−1 , θ
(m)
(cid:17)
(cid:1) , µyt = X
= Norm (cid:0)µyt , σ2
km ,t , a(m) ) m = 1 : 3, t ≥ 0,
km ,t = f (z(m)
z(m)
x(m)
= Norm (0, 1) ,
km ,0
k2 ,tx(3)
gd,k1 ,k2 x(1)
k1 ,tx(2)
sin (ωd t + φd ) .
t
y
d,k1 ,k2
Once again, latent variables are arranged in a hierarchy according to their time-scales (which de-
pend on m). At the top of the hierarchy is a long-time process which models slow structures, like
the sentences of speech. The next level models more quickly varying structure (like phonemes).
Finally, the bottom level of the hierarchy captures short-time variability (intra-phoneme variability
for instance). Unlike in PAD, the middle and lower levels now contain multiple process. So, for
example if K1 = 4 and K2 = 2, there would be four quickly varying modulators in the lower level,
individual spectral features (given by P
two modulators in the middle level, and one slowly varying modulator at the top (see ﬁg. 2A).
The idea is that the modulators in the ﬁrst level independently control the presence or absence of
d gd,k1 ,k2 sin (ωd t + φd )). For example, in speech a typical
phoneme might be periodic, but this periodicity might change systematically as the speaker alters
their pitch. This change in pitch might be modeled using two spectral features: one for the start of the
phoneme and one for the end, with a region of coactivation in the middle. Indeed it is because speech

yt |x(m)
t

, θ

4

x(1)1:Tx(2)1:Tx(3)1:T246y1:Ttime /s0.511.5time /sA.B.C.                                   Figure 2: A. Schematic representation of the MCP forward model in the simple case when K1 = 4,
K2 = 2 and D = 6. The hierarchy of latent variables moves from the slowest modulator at the
top (magenta) to the fastest (blue) with an intermediate modulator between (red). The outer-product
of the modulators multiplies the generative weights (black and white, only 4 of the 8 shown). In
turn, these modulate sinusoids (top right) which are summed to produce the observations (bottom
right). B. A draw from the forward model using parameters learned from a spoken-sentence (see
the results section for more details of the model). The grey bars on the top four panels indicate the
region depicted in the bottom four panels.

and other natural sounds are not precisely stationary even over short time-scales that we require the
lowest layer of the hierarchy. The role of the modulators in the second level is to simultaneously
turn on groups of similar features. For example, one modulator might control the presence of all the
harmonic features and the other the broad-band features. Finally the top level modulator gates all
the auditory features at once. Fig. 2B shows a draw from the forward model for a more complicated
example. Promisingly the samples share many features of natural sounds.

Relationship to other models. This model has an interesting relationship to previous statistical
models and in particular to the GSMMs. It is well known that when ICA is applied to data from
natural scenes the inferred ﬁlter coefﬁcients tend not to be independent (see [3, 10]), with coefﬁcients
corresponding to similar ﬁlters sharing power. GSMMs model dependencies using a hierarchical
framework, in which the distribution over the coefﬁcients depends on a set of latent variables that
introduce correlations between their powers. The MCP is similar, in that the higher level latent
variables alter the power of similar auditory features. Indeed, we suggest that the correlations in the
power of ICA coefﬁcients are a sign that AM is prevalent in natural scenes. The MCP can be seen
as a generalisation of the GSMMs to include time-varying latent variables, a deeper hierarchy and a
probabilistic time-frequency representation.

Inference and learning algorithms. Any type of learning in the MCP is computationally demand-
ing. Motivated by speed, and encouraged by the results from PAD, the aim will therefore be to ﬁnd
a joint MAP estimate of the latent variables and the weights, that is
log p(X, Y, G|θ).
X, G = arg max
X,G

(10)

5

x(3)tx(2)tx(1)t00.511.52ytx(3)tx(2)tx(1)t0.840.860.880.90.920.94yttime /s··x(3)tx(2)tx(1)t········+++·=ytA.B.log p(X, Y, G|θ) =

Note that we have introduced a prior over the generative tensor. This prevents an undesirable feature
of combined MAP and ML inference in such models: namely that the weights grow without bound,
enabling the modal values of latent variables to shrink towards zero, increasing their density under
(cid:18) TX
TX
X
3X
the prior. The resulting cost function is,
log p(yt |x(1)
km ,t |z(m)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) + log p(z(m)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dz(m)
, x(2)
log p(z(m)
t ) +
km ,t−1 )
(cid:19)
+ X
TX
t
t=1
t=1
m=1
km
km ,t
km ,0 )
dx(m)
t=0
k1 ,k2 ,d
km ,t
We would like to optimise this objective-function with respect to the latent variables (x(m)
km ,t ) and the
generative tensor (gd,k1 ,k2 ). There are, however, two main obstacles. The ﬁrst is that there are a
large number of latent variables to estimate (T × (K1 + K2 )), making inference slow. The second
is that the generative tensor contains a large number of elements D × K1 × K2 , making learning
slow too. The solution is to ﬁnd a good initialisation procedure, and then to ﬁne-tune using a slow
EM-like algorithm that iterates between updating the latents and the weights. First we outline the
initialisation procedure.

log

+

log p(gd,k1 ,k2 )

(11)

γd,k1 x(1)
k1 ,t sin (ωd t + φd ) ,

The key to learning complicated hierarchical models is to initialise well, and so the procedure devel-
oped for the MCP will be explained in some detail. The main idea is to learn the model one layer at
a time. This is achieved by clamping the upper layers of the hierarchy that are not being learned to
unity. In the ﬁrst stage of the initialisation, for example, the top and middle levels of the hierarchy
µyt = X
are clamped and the mean of the emission distribution becomes
where γd,k1 = P
d,k1
k2 gd,k1 ,k2 . Learning and inference then proceed by gradient based optimisation of
the cost-function (log p(X, Y, G|θ)) with respect to the un-clamped latents (x(1)
k1 ,t ) and the contracted
generative weights (γd,k1 ). This is much faster than the full optimisation as there are both fewer
latents and fewer parameters to estimate. When this process is complete, the second layer of latent
variables is un-clamped, and learning of these variables commences. This requires the full generative
tensor, which must be initialised from the contracted generative weights learned at the previous
stage. One choice is to set the individual weights to their averages gd,k1 ,k2 = 1
γd,k1 and this
K2
works well, but empirically slows learning. An alternative is to use small chunks of sounds to
learn the lower level weights. These chunks are chosen to be relatively stationary segments that
have a time-scale similar to the second-level modulators. This allows us to make the simplifying
assumption that just one second-level modulator was active during the chunk. The generative tensor
can be therefore be initialised using gd,k1 ,k2 = γd,k1 δk2 ,j . Typically this method causes the second
stage of learning to converge faster, and to a similar solution.

(12)

In contrast to the initialisation, the ﬁne tuning algorithm is simple. In the E-Step the latent variables
are updated simultaneously using gradient based optimisation of Eq. 11. In the M-Step, the gen-
erative tensor is updated using co-ordinate ascent. That is to say that we sequentially update each
gk1 ,k2 using gradient based optimisation of Eq. 11 and iterate over k1 and k2 . In principle, joint
optimisation of the generative tensor and latent variables is possible, but the memory requirements
are prohibitive. This is also why co-ordinate ascent is used to learn the generative tensor (rather than
using the usual linear regression solution which involves a prohibitive matrix inverse).

5 Results

The MCP was trained on a spoken sentence, lasting 2s and sampled at 8000Hz, using the algorithm
outlined in the previous section. The time-scales of the modulators were chosen to be {20 ms,
200 ms, 2 s}. The time-frequency representation had D/2 = 100 sines and D/2 = 100 cosines
spaced logarithmically from 100 − 4000Hz. The model was given K1 = 18 latent variables in the
ﬁrst level of the hierarchy, and K2 = 6 in the second. Learning took 16hrs to run on a 2.2 GHz
Opteron with 2Gb of memory.

6

Figure 3: Application of the MCP to speech. Left panels: The inferred latent variable hierarchy.
At top is the sentence modulator (magenta). Next are the phoneme modulators, followed by the
The learned spectral features (pg2
intra-phoneme modulators. These are coloured according to which of the phoneme modulators they
interact most strongly with. The speech waveform is shown in the bottom panel. Right panels:
sin + g2
cos ) coloured according to phoneme modulator. For ex-
ample, the top panel show the spectra from gk1=1:18,k2=1 . Spectra corresponding to one phoneme
modulator look similar and offer the features only differ in their phase.

[t]

The results can be seen in Fig. 3. The MCP recovers a sentence modulator, phoneme modulators,
and intra-phoneme modulators. Typically a pair of features are used to model a phoneme, and often
they have similar spectra as expected. The spectra fall into distinct classes: those which are har-
monic (modelling voiced features) and those which are broad-band (modelling unvoiced features).
One way of assessing which features of speech the model captures is to sample from the forward
model using the learned parameters. This can be seen in Fig. 2B. The conclusion is that the model
is capturing structure across a wide range of time-scales: formants and pitch structure, phoneme
structure, and sentence structure.

There are, however, two noticeable differences between the real and generated data. The ﬁrst is that
the generated data contain fewer transients and noise segments than natural speech, and more vowel-
like components. The reason for this is that at the sampling rates used, many of the noisy segments
are indistinguishable from white-noise and are explained using observation noise. These problems
are alleviated by moving to higher sampling rates, but the algorithm is then markedly slower. The
second difference concerns the inferred and generated latent variables in that the former are much
sparser than the latter. The reason is that learned generative tensor contains many gk1 ,k2 which are
nearly zero. In generation, this means that signiﬁcant contributions to the output are only made when
particular pairs of phoneme and intra-phoneme modulators are active. So although many modulators
are active at one time, only one or two make sizeable contributions. Conversely, in inference, we
can only get information about the value of a modulator when it is part of a contributing pair. If this
is not the case, the inference goes to the maximum of the prior which is zero. In effect there are
large error-bars on the non-contributing components’ estimates.

Finally, to indicate the improvement of the MCP over PAD and BSE, we compare the algorithms
abilities to ﬁll in missing sections of a spoken sentence. The average root-mean-squared (RMS)
error per sample is used as a metric to compare the algorithms. In order to use the MCP to ﬁll in the
missing data, it is ﬁrst necessary to learn a set of auditory features. The MCP was therefore trained
on a different spoken sentence from the same speaker, before inference was carried out on the test
data. To make the comparison fair, BSE is given an identical set of sinusoidal basis functions as
MCP, and the associated smoothness priors were learned on the same training data.

Typical results can be seen in ﬁg. 4. On average the RMS errors for MCP, BSE and PAD were:
{0.10, 0.30, 0.41}. As PAD models the carrier as white noise it predicts zeros in the missing regions

7

x(3)tx(2)t,k2x(1)t,k100.511.5time /sy01000200030004000frequency /HZFigure 4: A selection of typical missing data results for three phonemes (columns). The top row
shows the original speech segement with the missing regions shown in red. The middle row shows
the predictions made by the MCP and the bottom row those made by BSE.

and therefore it merely serves as a baseline in these experiments. Both MCP and BSE smoothly
interpolate their latent variables over the missing region. However, whereas BSE smoothly inter-
polates each sinusoidal component independently, MCP interpolates the set of learned auditory fea-
tures in a complex manner determined by the interaction of the modulators. It is for this reason that
it improves over BSE by such a large margin.

6 Conclusion

We have introduced a neuroscience-inspired generative model for natural sounds that is capable of
capturing structure spanning a wide range of temporal scales. The model is a marriage between a
probabilistic time-frequency representation (that captures the short-time structure) and a probabilis-
tic demodulation cascade (that captures the long-time structure). When the model is trained on a
spoken sentence, the ﬁrst level of the hierarchy learns auditory features (weighted sets of sinusoids)
that capture structures like different voiced sections of speech. The upper levels comprise a tem-
porally ordered set of modulators are used to represent sentence structure, phoneme structure and
intra-phoneme variability. The superiority of the new model over its parents was demonstrated in
a missing data experiment where it out-performed the Bayesian time-frequency analysis by a large
margin.

References

[1] Bregman, A.S. (1990) Auditory Scene Analysis. MIT Press.
[2] Smith E. & Lewicki, M.S. (2006) Efﬁcient Auditory Coding. Nature 439 (7079).
[3] Simoncelli, E.P. (2003) Vision and the statistics of the visual environment. Curr Opin Neurobi 13(2):144-9.
[4] Patterson, R.D. (2000) Auditory images: How complex sounds are represented in the auditory system. J
Acoust Soc Japan (E) 21(4):183-190.
[5] Grifﬁn, D. & Lim J. (1984) Signal estimation from modiﬁed short-time Fourier transform.
ASSP 32(2):236-243.
[6] Qi, Y., Minka, T. & Picard, R.W. (2002) Bayesian Spectrum Estimation of Unevenly Sampled Nonstation-
ary Data. MIT Media Lab Technical Report Vismod-TR-556.
[7] Attias, H. & Schreiner, C.E. (1997) Low-Order Temporal Statistics of Natural Sounds. Adv in Neural Info
Processing Sys 9. MIT Press.
[8] Anonymous Authors (2007) Probabilistic Amplitude Demodulation. ICA 2007 Conference Proceedings.
Springer, in press.
[9] Moore, B.C.J. (2003) An Introduction to the Psychology of Hearing. Academic Press.
[10] Karklin, Y. & Lewicki, M.S. (2005) A hierarchical Bayesian model for learning nonlinear statistical reg-
ularities in nonstationary natural signals. Neural Comput 17(2):397-423.

IEEE Trans. on

8

originalMCP0.080.10.120.140.160.18time /sBSE0.080.10.120.140.16time /s0.150.20.250.3time /s
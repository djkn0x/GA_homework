A Uni ﬁed Near-Optimal Estimator For Dimension Reduction in lα
(0 < α ≤ 2) Using Stable Random Projections
Trevor J. Hastie
Ping Li
Department of Statistics
Department of Statistical Science
Department of Health, Research and Policy
Faculty of Computing and Information Science
Stanford University
Cornell University
hastie@stanford.edu
pingli@cornell.edu

Abstract
Many tasks (e.g., clustering) in machine learning only require the lα distances in-
stead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2),
the method of stable random projections can efﬁciently compute the lα distances
in massive datasets (e.g., the Web or massive data streams) in one pass of the data.
The estimation task for stable random projections has been an interesting topic.
We propose a simple estimator based on the fractional power of the samples (pro-
jected data), which is surprisingly near-optimal in terms of the asymptotic vari-
ance. In fact, it achieves the Cram ´er-Rao bound when α = 2 and α = 0+. This
new result will be useful when applying stable random projections to distance-
based clustering, classiﬁcations, kernels, massive data s treams etc.

1 Introduction
Dimension reductions in the lα norm (0 < α ≤ 2) have numerous applications in data mining,
information retrieval, and machine learning. In modern applications, the data can be way too large
for the physical memory or even the disk; and sometimes only one pass of the data can be afforded
for building statistical learning models [1, 2, 5]. We abstract the data as a data matrix A ∈ Rn×D .
In many applications, it is often the case that we only need the lα properties (norms or distances) of
A. The method of stable random projections [9, 18, 22] is a useful tool for efﬁciently computing the
lα (0 < α ≤ 2) properties in massive data using a small (memory) space.
Denote the leading two rows in the data matrix A by u1 , u2 ∈ RD . The lα distance d(α) is
D
Xi=1
|u1,i − u2,i |α .
d(α) =
The choice of α is beyond the scope of this study; but basically, we can treat α as a tuning parameter.
In practice, the most popular choice, i.e., the α = 2 norm, often does not work directly on the original
(unweighted) data, as it is well-known that truly large-scale datasets (especially Internet data) are
ubiquitously “heavy-tailed.” In machine learning, it is of
ten crucial to carefully term-weight the
data (e.g., taking logarithm or tf-idf) before applying subsequent learning algorithms using the l2
norm. As commented in [12, 21], the term-weighting procedure is often far more important than
ﬁne-tuning the learning parameters. Instead of weighting t he original data, an alternative scheme
is to choose an appropriate norm. For example, the l1 norm has become popular recently, e.g.,
LASSO, LARS, 1-norm SVM [23], Laplacian radial basis kernel [4], etc. But other norms are also
possible. For example, [4] proposed a family of non-Gaussian radial basis kernels for SVM in the
form K (x, y ) = exp (−ρ Pi |xi − yi |α ), where x and y are data points in high-dimensions; and [4]
showed that α ≤ 1 (even α = 0) in some cases produced better results in histogram-based image
classiﬁcations. The lα norm with α < 1, which may initially appear strange, is now well-understood
to be a natural measure of sparsity [6]. In the extreme case, when α → 0+, the lα norm approaches
the Hamming norm (i.e., the number of non-zeros in the vector).

(1)

Therefore, there is the natural demand in science and engineering for dimension reductions in the
lα norm other than l2 . While the method of normal random projections for the l2 norm [22] has
become very popular recently, we have to resort to more general methodologies for 0 < α < 2.
The idea of stable random projections is to multiply A with a random projection matrix R ∈ RD×k
(k ≪ D). The matrix B = A × R ∈ Rn×k will be much smaller than A. The entries of R are
(typically) i.i.d. samples from a symmetric α-stable distribution [24], denoted by S (α, 1), where α
is the index and 1 is the scale. We can then discard the original data matrix A because the projected
matrix B now contains enough information to recover the original lα properties approximately.

ˆd(α),me =

A symmetric α-stable random variable is denoted by S (α, d), where d is the scale parameter. If
x ∼ S (α, d), then its characteristic function (Fourier transform of the density function) would be
E (cid:0)exp (cid:0)√−1xt(cid:1)(cid:1) = exp (−d|t|α ) ,
(2)
whose inverse does not have a closed-form except for α = 2 (i.e., normal) or α = 1 (i.e., Cauchy).
Applying stable random projections on u1 ∈ RD , u2 ∈ RD yields, respectively, v1 = RTu1 ∈ Rk
and v2 = RTu2 ∈ Rk . By the properties of Fourier transforms, the projected differences, v1,j − v2,j ,
j = 1, 2, ..., k , are i.i.d. samples of the stable distribution S (α, d(α) ), i.e.,
(3)
xj = v1,j − v2,j ∼ S (α, d(α) ),
j = 1, 2, ..., k .
Thus, the task is to estimate the scale parameter from k i.i.d. samples xj ∼ S (α, d(α) ). Because no
closed-form density functions are available except for α = 1, 2, the estimation task is challenging
when we seek estimators that are both accurate and computationally efﬁcient.
For general 0 < α < 2, a widely used estimator is based on the sample inter-quantiles [7, 20], which
can be simpliﬁed to be the sample median estimator by choosing the 0.75 - 0.25 sample quantiles
and using the symmetry of S (α, d(α) ). That is
median{|xj |α , j = 1, 2, ..., k}
median{S (α, 1)}α
It has been well-known that the sample median estimator is not accurate, especially when the
sample size k is not too large. Recently, [13] proposed various estimators based on the geometric
mean and the harmonic mean of the samples. The harmonic mean estimator only works for small
α. The geometric mean estimator has nice properties including closed-form variances, closed-form
tail bounds in exponential forms, and very importantly, an analog of the Johnson-Lindenstrauss (JL)
Lemma [10] for dimension reduction in lα . The geometric mean estimator, however, can still be
improved for certain α, especially for large samples (e.g., as k → ∞).
1.1 Our Contribution: the Fractional Power Estimator
The fractional power estimator, with a simple uniﬁed format for all 0 < α ≤ 2, is (surprisingly)
near-optimal in the Cram ´er-Rao sense (i.e., when k → ∞, its variance is close to the Cram ´er-Rao
lower bound). In particularly, it achieves the Cram ´er-Rao bound when α = 2 and α → 0+.
(α) , denoted by ˆR(α),λ .
The basic idea is straightforward. We ﬁrst obtain an unbiase d estimator of dλ
We then estimate d(α) by (cid:16) ˆR(α),λ(cid:17)1/λ
, which can be improved by removing the O (cid:0) 1
k (cid:1) bias (this
consequently also reduces the variance) using Taylor expansions. We choose λ = λ∗ (α) to minimize
the theoretical asymptotic variance. We prove that λ∗ (α) is the solution to a simple convex program,
i.e., λ∗ (α) can be pre-computed and treated as a constant for every α. The main computation
j=1 |xj |λ∗ α(cid:17)1/λ∗
involves only (cid:16)Pk
; and hence this estimator is also computationally efﬁcient .
1.2 Applications
The method of stable random projections is useful for efﬁciently computing the lα properties (norms
or distances) in massive data, using a small (memory) space.

.

(4)

Massive data streams are fundamental in many modern
• Data stream computations
data processing application [1, 2, 5, 9]. It is common practice to store only a very small
sketch of the streams to efﬁciently compute the lα norms of the individual streams or the lα
distances between a pair of streams. For example, in some cases, we only need to visually
monitor the time history of the lα distances; and approximate answers often sufﬁce.
One interesting special case is to estimate the Hamming norms (or distances) using the
fact that, when α → 0+, d(α) = PD
i=1 |u1,i − u2,i |α approaches the total number of
non-zeros in {|u1,i − u2,i |}D
i=1 , i.e., the Hamming distance [5]. One may ask why not just
(binary) quantize the data and then apply normal random projections to the binary data. [5]
considered that the data are dynamic (i.e., frequent addition/subtraction) and hence pre-
quantizing the data would not work. With stable random projections, we only need to
update the corresponding sketches whenever the data are updated.

In many applications including distanced-based
• Computing all pairwise distances
clustering, classiﬁcations and kernels (e.g.) for SVM, we o nly need the pairwise distances.
Computing all pairwise distances of A ∈ Rn×D would cost O(n2D), which can be signif-
icantly reduced to O(nDk + n2k) by stable random projections. The cost reduction will
be more considerable when the original datasets are too large for the physical memory.
While it is often infeasible to store the original matrix
• Estimating lα distances online
A in the memory, it is also often infeasible to materialize all pairwise distances in A. Thus,
in applications such as online learning, databases, search engines, online recommendation
systems, and online market-basket analysis, it is often more efﬁcient if we store B ∈ Rn×k
in the memory and estimate any pairwise distance in A on the ﬂy only when it is necessary.

When we treat α as a tuning parameter, i.e., re-computing the lα distances for many different α,
stable random projections will be even more desirable as a cost-saving device.

2 Previous Estimators
We assume k i.i.d. samples xj ∼ S (α, d(α) ), j = 1, 2, ..., k . We list several previous estimators.
• The geometric mean estimator is recommended in [13] for α < 2.
Qk
j=1 |xj |α/k
ˆd(α),gm =
k (cid:1)(cid:3)k .
k (cid:1) Γ (cid:0)1 − 1
α
k (cid:1) sin (cid:0) π
(cid:2) 2
π Γ (cid:0) α
2
k (cid:1)(cid:3)k
(α) ( (cid:2) 2
k (cid:1)(cid:3)2k − 1)
k (cid:1) sin (cid:0)π α
k (cid:1) Γ (cid:0)1 − 2
π Γ (cid:0) 2α
Var (cid:16) ˆd(α),gm (cid:17) = d2
k (cid:1) Γ (cid:0)1 − 1
α
(cid:2) 2
π Γ (cid:0) α
k (cid:1) sin (cid:0) π
2
π 2
12 (cid:0)α2 + 2(cid:1)(cid:27) + O (cid:18) 1
k2 (cid:19) .
(α) (cid:26) 1
= d2
k
• The harmonic mean estimator is recommended in [13] for 0 < α ≤ 0.344.
 k −   −πΓ(−2α) sin (πα)
2 α(cid:1)(cid:3)2 − 1!! ,
− 2
π Γ(−α) sin (cid:0) π
2 α(cid:1)
ˆd(α),hm =
Pk
(cid:2)Γ(−α) sin (cid:0) π
j=1 |xj |−α
k   −πΓ(−2α) sin (πα)
2 α(cid:1)(cid:3)2 − 1! + O (cid:18) 1
k2 (cid:19) .
1
Var (cid:16) ˆd(α),hm (cid:17) = d2
(α)
(cid:2)Γ(−α) sin (cid:0) π
k Pk
• For α = 2, the arithmetic mean estimator, 1
j=1 |xj |2 , is commonly used, which has
variance = 2
(2) . It can be improved by taking advantage of the marginal l2 norms [17].
k d2

(5)

(7)

(6)

(8)

(9)

3 The Fractional Power Estimator

The fractional power estimator takes advantage of the following statistical result in Lemma 1.
Lemma 1 Suppose x ∼ S (cid:0)α, d(α) (cid:1). Then for −1 < λ < α,
α (cid:19) Γ(λ) sin (cid:16) π
Γ (cid:18)1 −
2
λ
E (cid:0)|x|λ (cid:1) = dλ/α
(α)
π
2
If α = 2, i.e., x ∼ S (2, d(2) ) = N (0, 2d(2) ), then for λ > −1,
2 (cid:19) Γ(λ) sin (cid:16) π
Γ (cid:18)1 −
2Γ (λ)
λ
2
λ(cid:17) = dλ/2
E (cid:0)|x|λ (cid:1) = dλ/2
(2)
(2)
Γ (cid:0) λ
2 (cid:1)
π
2
Proof: For 0 < α ≤ 2 and −1 < λ < α, (10) can be inferred directly from [24, Theorem 2.6.3].
For α = 2, the moment E (cid:0)|x|λ (cid:1) exists for any λ > −1. (11) can be shown by directly integrating
the Gaussian density (using the integral formula [8, 3.381.4]). The Euler’s re ﬂection formula
2 (cid:1) = 21−2z√πΓ(2z ) are handy.
Γ(1 − z )Γ(z ) = π
sin(πz) and the duplication formula Γ(z )Γ (cid:0)z + 1

λ(cid:17) .

(10)

(11)

.

The fractional power estimator is de ﬁned in Lemma 2. See the proof in Appendix A.

×

(12)

where

g (λ; α) ,

Lemma 2 Denoted by ˆd(α),f p , the fractional power estimator is de ﬁned as
!1/λ∗
Pk
j=1 |xj |λ∗ α
ˆd(α),f p =   1
π Γ(1 − λ∗ )Γ(λ∗α) sin (cid:0) π
2
2 λ∗α(cid:1)
k
2 λ∗α(cid:1)(cid:3)2 − 1!! ,
 1 −
λ∗ − 1(cid:19)   2
π Γ(1 − 2λ∗ )Γ(2λ∗α) sin (πλ∗α)
2λ∗ (cid:18) 1
1
1
k
(cid:2) 2
π Γ(1 − λ∗ )Γ(λ∗α) sin (cid:0) π
λ2   2
2 λα(cid:1)(cid:3)2 − 1! .
π Γ(1 − 2λ)Γ(2λα) sin (πλα)
1
λ∗ = argmin
g (λ; α) =
(cid:2) 2
π Γ(1 − λ)Γ(λα) sin (cid:0) π
− 1
2α λ< 1
2
Asymptotically (i.e., as k → ∞), the bias and variance of ˆd(α),f p are
k2 (cid:19) ,
E (cid:16) ˆd(α),f p(cid:17) − d(α) = O (cid:18) 1
2 λ∗α(cid:1)(cid:3)2 − 1! + O (cid:18) 1
λ∗2   2
π Γ(1 − 2λ∗ )Γ(2λ∗α) sin (πλ∗α)
k2 (cid:19) .
1
1
Var (cid:16) ˆd(α),f p(cid:17) = d2
(α)
k
π Γ(1 − λ∗ )Γ(λ∗α) sin (cid:0) π
(cid:2) 2
j=1 |xj |λ∗ α(cid:17)1/λ∗
Note that in calculating ˆd(α),f p , the real computation only involves (cid:16)Pk
all other terms are basically constants and can be pre-computed.
Figure 1(a) plots g (λ; α) as a function of λ for many different values of α. Figure 1(b) plots the
optimal λ∗ as a function of α. We can see that g (λ; α) is a convex function of λ and −1 < λ∗ < 1
2
(except for α = 2), which will be proved in Lemma 3.

, because

(15)

(14)

(13)

1

0.8

2

0.5

1.2

1.5

1.999

1.9
1.95

r
o
t
c
a
f
 
e
c
n
a
i
r
a
V

3.2
3
2.8
2.6
2.4
2.2
2
1.8
1.6
1.4
1.2
1
−1   −.8 −.6 −.4 −.2 0    .2  .4  .6  .8  1   
λ
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α
Figure 1: Left panel plots the variance factor g (λ; α) as functions of λ for different α, illustrating
g (λ; α) is a convex function of λ and the optimal solution (lowest points on the curves) are between
-1 and 0.5 (α < 2). Note that there is a discontinuity between α → 2− and α = 2. Right panel plots
the optimal λ∗ as a function of α. Since α = 2 is not included, we only see λ∗ < 0.5 in the ﬁgure.

0.3
2e−16

0.5
0.4
0.3
0.2
0.1
0
−0.1
−0.2
−0.3
−0.4
−0.5
−0.6
−0.7
−0.8
−0.9
−1

t
p
o
λ

3.1 Special cases
The discontinuity, λ∗ (2−) = 0.5 and λ∗ (2) = 1, re ﬂects the fact that, for x ∼ S (α, d), E (cid:0)|x|λ (cid:1)
exists for −1 < λ < α when α < 2 and exists for any λ > −1 when α = 2.
k Pk
When α = 2, since λ∗ (2) = 1, the fractional power estimator becomes 1
j=1 |xj |2 , i.e., the
arithmetic mean estimator. We will from now on only consider 0 < α < 2.
when α → 0+, since λ∗ (0+) = −1, the fractional power estimator approaches the harmonic mean
estimator, which is asymptotically optimal when α = 0+ [13].
When α → 1, since λ∗ (1) = 0 in the limit, the fractional power estimator has the same asymptotic
variance as the geometric mean estimator.

3.2 The Asymptotic (Cram ´er-Rao) Ef ﬁciency

For an estimator ˆd(α) , its variance, under certain regularity condition, is lower-bounded by the Infor-
mation inequality (also known as the Cram ´er-Rao bound) [11 , Chapter 2], i.e., Var (cid:16) ˆd(α) (cid:17) ≥ 1
kI(α) .
The Fisher Information I(α) can be approximated by computationally intensive procedures [19].
When α = 2, it is well-known that the arithmetic mean estimator attains the Cram ´er-Rao bound.
When α = 0+, [13] has shown that the harmonic mean estimator is also asymptotically optimal.
Therefore, our fractional power estimator achieves the Cram ´er-Rao bound, exactly when α = 2,
and asymptotically when α = 0+.
1
The asymptotic (Cram ´er-Rao) efﬁciency is de ﬁned as the rat
io of
kI(α) to the asymptotic variance of
ˆd(α) (d(α) = 1 for simplicity). Figure 2 plots the efﬁciencies for all esti mators we have mentioned,
illustrating that the fractional power estimator is near-optimal in a wide range of α.

y
c
n
e
i
c
i
f
f
E

0.7

0.5

1

0.9

0.8

 

0.6

Fractional
Geometric
Harmonic
Median
0.4
 
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α
Figure 2: The asymptotic Cram ´er-Rao efﬁciencies of variou s estimators for 0 < α < 2, which are
1
kI(α) to the asymptotic variances of the estimators. Here k is the sample size and I(α) is
the ratios of
the Fisher Information (we use the numeric values in [19]). The asymptotic variance of the sample
median estimator ˆd(α),me is computed from known statistical theory for sample quantiles. We can
see that the fractional power estimator ˆd(α),f p is close to be optimal in a wide range of α; and it
always outperforms both the geometric mean and the harmonic mean estimators. Note that since we
only consider α < 2, the efﬁciency of ˆd(α),f p does not achieve 100% when α → 2−.
3.3 Theoretical Properties
We can show that, when computing the fractional power estimator ˆd(α),f p , to ﬁnd the opti-
mal λ∗ only involves searching for the minimum on a convex curve in the narrow range λ∗ ∈
(cid:0)max (cid:8)−1, − 1
2α (cid:9) , 0.5(cid:1). These properties theoretically ensure that the new estimator is well-de ﬁned
and is numerically easy to compute. The proof of Lemma 3 is brie ﬂy sketched in Appendix B.
λ2   2
2 λα(cid:1)(cid:3)2 − 1! ,
π Γ(1 − 2λ)Γ(2λα) sin (πλα)
1
Lemma 3 Part 1:
(16)
g (λ; α) =
(cid:2) 2
π Γ(1 − λ)Γ(λα) sin (cid:0) π
is a convex function of λ.
g (λ; α), satisﬁes −1 < λ∗ < 0.5.
Part 2: For 0 < α < 2, the optimal λ∗ = argmin
2α λ< 1
− 1
2
3.4 Comparing Variances at Finite Samples

It is also important to understand the small sample performance of the estimators. Figure 3 plots
the empirical mean square errors (MSE) from simulations for the fractional power estimator, the
harmonic mean estimator, and the sample median estimator. The MSE for the geometric mean
estimators can be computed exactly without simulations.
Figure 3 indicates that the fractional power estimator ˆd(α),f p also has good small sample perfor-
mance unless α is close to 2. After k ≥ 50, the advantage of ˆd(α),f p becomes noticeable even
when α is very close to 2. It is also clear that the sample median estimator has poor small sample
performance; but even at very large k , its performance is not that good except when α is about 1.

)
E
S
M
(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

0.7

0.6

0.5

0.4

0.3

0.2

0.1

k = 10

 

Fractional
Geometric
Harmonic
Median

0
 
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α

)
E
S
M
(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

0.12

0.1

0.08

0.06

0.04

0.02

k = 50

 

Fractional
Geometric
Harmonic
Median

0
 
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α

 

 

0.06

0.05

0.04

0.03

k = 500

k = 100

0.02

0.01

)
E
S
M
(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

)
E
S
M
(
 
r
o
r
r
e
 
e
r
a
u
q
s
 
n
a
e
M

0.0109
0.01
0.009
0.008
0.007
0.006
0.005
Fractional
0.004
Fractional
Geometric
0.003
Geometric
Harmonic
0.002
Harmonic
Median
0.001
Median
0
 
0
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
 
α
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
α
Figure 3: We simulate the mean square errors (MSE) (106 simulations at every α and k) for the
harmonic mean estimator (0 < α ≤ 0.344 only) and the fractional power estimator. We compute
the MSE exactly for the geometric mean estimator (for 0.344α < 2). The fractional power has good
accuracy (small MSE) at reasonable sample sizes (e.g., k ≥ 50). But even at small samples (e.g.,
k = 10), it is quite accurate except when α approaches 2.

4 Discussion
j=1 |xj |λ∗ α(cid:17)1/λ∗
The fractional power estimator ˆd(α),f p ∝ (cid:16)Pk
can be treated as a linear estimator
in because the power 1/λ∗ is just a constant. However, Pk
j=1 |xj |λ∗ α is not a metric because
λ∗α < 1, as shown in Lemma 3. Thus our result does not con ﬂict the cele brated impossibility result
[3], which proved that there is no hope to recover the original l1 distances using linear projections
and linear estimators without incurring large errors.

Although the fractional power estimator achieves near-optimal asymptotic variance, analyzing its
tail bounds does not appear straightforward.
In fact, when α approaches 2, this estimator does
not have ﬁnite moments much higher than the second order, sug gesting poor tail behavior. Our
additional simulations (not included in this paper) indicate that ˆd(α),f p still has comparable tail
probability behavior as the geometric mean estimator, when α ≤ 1.
Finally, we should mention that the method of stable random projections does not take advantage of
the data sparsity while high-dimensional data (e.g., text data) are often highly sparse. A new method
call Conditional Random Sampling (CRS) [14 –16] may be more preferable in highly sparse data.
5 Conclusion
In massive datasets such as the Web and massive data streams, dimension reductions are often crit-
ical for many applications including clustering, classiﬁc ations, recommendation systems, and Web
search, because the data size may be too large for the physical memory or even for the hard disk and
sometimes only one pass of the data can be afforded for building statistical learning models.
While there are already many papers on dimension reductions in the l2 norm, this paper focuses on
the lα norm for 0 < α ≤ 2 using stable random projections, as it has become increasingly popular in
machine learning to consider the lα norm other than l2 . It is also possible to treat α as an additional
tuning parameter and re-run the learning algorithms many times for better performance.

Our main contribution is the fractional power estimator for stable random projections. This esti-
mator, with a uniﬁed format for all 0 < α ≤ 2, is computationally efﬁcient and (surprisingly) is
also near-optimal in terms of the asymptotic variance. We also prove some important theoretical
properties (variance, convexity, etc.) to show that this estimator is well-behaved. We expect that this
work will help advance the state-of-the-art of dimension reductions in the lα norms.

−

1
k

−

< λ <

ˆR(α),λ =

−1/α < λ < 1

(α) , denoted by ˆR(α),λ ,
By Lemma 1, we ﬁrst seek an unbiased estimator of of dλ
P k
j=1 |xj |λα
π Γ(1 − λ)Γ(λα) sin (cid:0) π
2
2 λα(cid:1)
whose variance is
d2λ
k   2
2 λα(cid:1) (cid:3) 2 − 1! ,
π Γ(1 − 2λ)Γ(2λα) sin (πλα)
1
1
Var (cid:16) ˆR(α),λ (cid:17) =
(α)
(cid:2) 2
π Γ(1 − λ)Γ(λα) sin (cid:0) π
2α
2
A biased estimator of d(α) would be simply (cid:16) ˆR(α),λ(cid:17)1/λ
, which has O (cid:0) 1
k (cid:1) bias. This bias can
be removed to an extent by Taylor expansions [11, Theorem 6.1.1]. While it is well-known that
bias-corrections are not always bene ﬁcial because of the bi as-variance trade-off phenomenon, in our
case, it is a good idea to conduct the bias-correction because the function f (x) = x1/λ is convex for
λ − 1(cid:1) x1/λ−2 > 0, assuming − 1
λ x1/λ−1 and f ′′ (x) = 1
x > 0. Note that f ′ (x) = 1
2α < λ < 1
λ (cid:0) 1
2 .
Because f (x) is convex, removing the O (cid:0) 1
k (cid:1) bias will also lead to a smaller variance.
We call this new estimator the “fractional power ” estimator
:
Var (cid:16) ˆR(α),λ (cid:17)
λ (cid:18) 1
1
− 1(cid:19) (cid:16) dλ
ˆd(α),f p,λ = (cid:16) ˆR(α),λ (cid:17) 1/λ
(α) (cid:17) 1/λ−2
λ
2
! 1/λ   1 −
P k
j=1 |xj |λα
− 1(cid:19)   2
=   1
2 λα(cid:1) (cid:3) 2 − 1! ! ,
π Γ(1 − 2λ)Γ(2λα) sin (πλα)
2λ (cid:18) 1
1
1
2
π Γ(1 − λ)Γ(λα) sin (cid:0) π
2 λα(cid:1)
(cid:2) 2
π Γ(1 − λ)Γ(λα) sin (cid:0) π
k
λ
k
(α) . The asymptotic variance would be
where we plug in the estimated dλ
(α) (cid:17) 1/λ−1(cid:19) 2
Var (cid:16) ˆd(α),f p,λ (cid:17) = Var (cid:16) ˆR(α),λ (cid:17) (cid:18) 1
+ O (cid:18) 1
k2 (cid:19)
λ (cid:16) dλ
λ2 k   2
2 λα(cid:1) (cid:3) 2 − 1! + O (cid:18) 1
π Γ(1 − 2λ)Γ(2λα) sin (πλα)
1
k2 (cid:19) .
= d2
(α)
(cid:2) 2
π Γ(1 − λ)Γ(λα) sin (cid:0) π
The optimal λ, denoted by λ∗ , is then
λ2   2
2 ( 1
2 λα(cid:1)(cid:3)2 − 1!) .
π Γ(1 − 2λ)Γ(2λα) sin (πλα)
λ∗ = argmin
π Γ(1 − λ)Γ(λα) sin (cid:0) π
(cid:2) 2
− 1
2α λ< 1
We sketch the basic steps; and we direct readers to the additional supporting material for more detail.
We use the in ﬁnite-product representations of the Gamma and sine functions [8, 8.322,1.431.1],
Ys=1   1 −
∞

B Proof of Lemma 3

∞
Ys=1 (cid:18) 1 +

exp (−γe z)
z

s (cid:19) −1
z

exp (cid:18) z
s (cid:19) ,

s2 π2 ! ,
z2

Γ(z) =

sin(z) = z

A Proof of Lemma 2

,

to re-write g (λ; α) as

g(λ; α) =

(M (λ; α) − 1) =

fs (λ; α) − 1! ,
λ2   ∞
1
1
Ys=1
λ2
s (cid:19) 3   1 −
s (cid:19) −1 (cid:18) 1 −
s (cid:19) 2 (cid:18) 1 +
λα
λα
2λα
λ
fs (λ; α) = (cid:18) 1 −
s (cid:19) (cid:18) 1 +
With respect to λ, the ﬁrst two derivatives of g (λ; α) are

4s2 ! −2
λ2 α2

(cid:18) 1 −

s (cid:19) −1
2λ

.

∂ g
∂λ

=

∂ 2 g
∂λ2

=

λ2   −
2
1
λ
λ2   6
M
λ2

(M − 1) +

∞
Xs=1
∂ 2 log fs
∂λ2

M ! .
∂ log fs
∂λ
∂λ ! 2
+   ∞
∂ log fs
Xs=1

+

∞
Xs=1

−

4
λ

∞
Xs=1

∂λ ! −
∂ log fs

6
λ4

.

= 2λ

∂ 3 log fs
∂λ3

=

=

+

−

+

+

−

∂ 4 log fs
∂λ4

∂ 2 log fs
∂λ2

=

−

α2
(s − λα)2

−

+ 2α3 (cid:18)

−

8
(s + 2λα)3

−

3α2
(s + λα)2

2
(2s − λα)4

−

1
(s − λα)4

2
4s2 − λ2 α2

2
(2s − λα)3

−

1
(s − λα)3

+

3
(s + λα)3

2α2
(2s + λα)2

2α2
(2s − λα)2

+ α2 (cid:18)
+

3
(s + λα)4

+

16
(s + 2λα)4

1
s2 − 3sλ + 2λ2

4
(s − λ)3

+

16
(s − 2λ)3

∞
Xs=1
∞
Xs=1

−12
(s − λ)4

+

96
(s − 2λ)4

2
(2s + λα)3 (cid:19) ,

−2
(s − λ)2

+

4
(s − 2λ)2

2
(2s + λα)4 (cid:19) .

1
s2 + 3sλα + 2λ2 α2

1
s2 − λ2 α2 (cid:19) ,
4α2
+
(s + 2λα)2

Also,
∞
∂ log fs
Xs=1
∂λ
∞
Xs=1
∞
Xs=1
∞
Xs=1
∞
Xs=1
∞
+ 6α4 (cid:18)
Xs=1
∂λ2 > 0, it sufﬁces to show λ4 ∂ 2 g
To show ∂ 2 g
∂λ2 > 0, which can shown based on its own second deriva-
∂ 4 log fs
tive (and hence we need P∞
). Here we consider λ 6= 0 to avoid triviality. To complete
∂λ4
s=1
the proof, we use some properties of the Riemann’s Zeta function and the in ﬁnite countability.
(cid:12)(cid:12)(cid:12)λ∗
Next, we show that λ∗ < −1 does not satisfy ∂ g(λ;α)
= 0, which is equivalent to h(λ∗ ) = 1,
∂λ
∂λ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) λ∗ ! = 1,
h(λ∗ ) = M (λ∗ )   1 −
∞
λ∗
∂ log fs
Xs=1
2
∂λ > 0, i.e., h(λ) < h(−1). We then show ∂h(−1)
We show that when λ < −1, ∂h
∂α < 0 for
0 < α < 0.5; and hence h(−1; α) < h(−1; 0+) = 1. Therefore, we must have λ∗ > −1.
References
[1] C. Aggarwal, editor. Data Streams: Models and Algorithms. Springer, New York, NY, 2007.
[2] B. Babcock, S. Babu, M. Datar, R. Motwani, and J. Widom. Models and issues in data stream systems. In PODS, 1–16, 2002.
[3] B. Brinkman and M. Charikar. On the impossibility of dimension reduction in l1 . Journal of ACM, 52(2):766–788, 2005.
[4] O. Chapelle, P. Haffner, and V. Vapnik. Support vector machines for histogram-based image classiﬁcation.
IEEE Trans. Neural Networks,
10(5):1055–1064, 1999.
[5] G. Cormode, M. Datar, P. Indyk, and S. Muthukrishnan. Comparing data streams using hamming norms (how to zero in). In VLDB,
335–345, 2002.
[6] D. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289–1306, 2006.
[7] E. Fama and R. Roll. Parameter estimates for symmetric stable distributions. JASA, 66(334):331–338, 1971.
[8]
I. Gradshteyn and I. Ryzhik. Table of Integrals, Series, and Products. Academic Press, New York, ﬁfth edition, 1994.
[9] P. Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. Journal of ACM, 53(3):307–323,
2006.
[10] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mapping into Hilbert space. Contemporary Mathematics, 26:189–206, 1984.
[11] E. Lehmann and G. Casella. Theory of Point Estimation. Springer, New York, NY, second edition, 1998.
[12] E. Leopold and J. Kindermann. Text categorization with support vector machines. how to represent texts in input space? Machine
Learning, 46(1-3):423–444, 2002.
[13] P. Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In SODA, 2008.
[14] P. Li and K. Church. Using sketches to estimate associations. In HLT/EMNLP, 708–715, 2005.
[15] P. Li and K. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, 33(3):305–354,
2007.
[16] P. Li, K. Church, and T. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS, 873–880,
2007.
[17] P. Li, T. Hastie, and K. Church. Improving random projections using marginal information. In COLT, 635–649, 2006.
[18] P. Li, T. Hastie, and K. Church. Nonlinear estimators and tail bounds for dimensional reduction in l1 using cauchy random projections.
Journal of Machine Learning Research (To appear) .
[19] M. Matsui and A. Takemura. Some improvements in numerical evaluation of symmetric stable density and its derivatives. Communica-
tions on Statistics-Theory and Methods, 35(1):149–172, 2006.
[20] J. McCulloch. Simple consistent estimators of stable distribution parameters. Communications on Statistics-Simulation, 15(4):1109–
1136, 1986.
[21] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling the poor assumptions of naive Bayes text classiﬁers. In ICML, 616–623, 2003.
[22] S. Vempala. The Random Projection Method. American Mathematical Society, Providence, RI, 2004.
[23] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. In NIPS, Vancouver, 2003.
[24] V. M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society, Providence, RI, 1986.


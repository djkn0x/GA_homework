Efﬁcient Inference for Distributions on Permutations

Jonathan Huang
Carnegie Mellon University
jch1@cs.cmu.edu

Carlos Guestrin
Carnegie Mellon University
guestrin@cs.cmu.edu

Leonidas Guibas
Stanford University
guibas@cs.stanford.edu

Abstract
Permutations are ubiquitous in many real world problems, such as voting,
rankings and data association. Representing uncertainty over permutations is
challenging, since there are n! possibilities, and typical compact representations
such as graphical models cannot efﬁciently capture the mutu al exclusivity con-
straints associated with permutations. In this paper, we use the “low-frequency”
terms of a Fourier decomposition to represent such distributions compactly. We
present Kronecker conditioning, a general and efﬁcient approach for maintaining
these distributions directly in the Fourier domain. Low order Fourier-based
approximations can lead to functions that do not correspond to valid distributions.
To address this problem, we present an efﬁcient quadratic pr ogram deﬁned
directly in the Fourier domain to project the approximation onto a relaxed form
of the marginal polytope. We demonstrate the effectiveness of our approach on a
real camera-based multi-people tracking setting.

1

Introduction

Permutations arise naturally in a variety of real situations such as card games, data association
problems, ranking analysis, etc. As an example, consider a sensor network that tracks the positions
of n people, but can only gather identity information when they walk near certain sensors. Such
mixed-modality sensor networks are an attractive alternative to exclusively using sensors which can
measure identity because they are potentially cheaper, easier to deploy, and less intrusive. See [1]
for a real deployment. A typical tracking system maintains tracks of n people and the identity of
the person corresponding to each track. What makes the problem difﬁcult is that identities can be
confused when tracks cross in what we call mixing events. Maintaining accurate track-to-identity
assignments in the face of these ambiguities based on identity measurements is known as the
Identity Management Problem [2], and is known to be N P -hard. Permutations pose a challenge for
probabilistic inference, because distributions on the group of permutations on n elements require
storing at least n! − 1 numbers, which quickly becomes infeasible as n increases. Furthermore,
typical compact representations, such as graphical models, cannot capture the mutual exclusivity
constraints associated with permutations.

Diaconis [3] proposes maintaining a small subset of Fourier coefﬁcients of the actual distribution al-
lowing for a principled tradeoff between accuracy and complexity. Schumitsch et al. [4] use similar
ideas to maintain a particular subset of Fourier coefﬁcient s of the log probability distribution. Kon-
dor et al. [5] allow for general sets of coefﬁcients, but assu me a restrictive form of the observation
model in order to exploit an efﬁcient FFT factorization. The main contributions of this paper are:
• A new, simple and general algorithm, Kronecker Conditioning, which performs all proba-
bilistic inference operations completely in the Fourier domain. Our approach is general, in
the sense that it can address any transition model or likelihood function that can be repre-
sented in the Fourier domain, such as those used in previous work, and can represent the
probability distribution with any desired set of Fourier coefﬁcients.
• We show that approximate conditioning can sometimes yield Fourier coefﬁcients which do
not correspond to any valid distribution, and present a method for projecting the result back
onto a relaxation of the marginal polytope.
• We demonstrate the effectiveness of our approach on a real camera-based multi-people
tracking setting.

1

2 Filtering over permutations
In identity management, a permutation σ represents a joint assignment of identities to internal tracks,
with σ(i) being the track belonging to the ith identity. When people walk too closely together, their
identities can be confused, leading to uncertainty over σ . To model this uncertainty, we use a Hidden
Markov Model on permutations, which is a joint distribution over P (σ (1) , . . . , σ (T ) , z (1) , . . . , z (T ) )
which factors as:
P (σ (1) , . . . , σ (T ) , z (1) , . . . , z (T ) ) = P (z (1) |σ (1) ) Yt
where the σ (t) are latent permutations and the z (t) denote observed variables. The conditional
probability distribution P (σ (t) |σ (t−1) ) is called the transition model, and might reﬂect for example,
that the identities belonging to two tracks were swapped with some probability. The distribution
P (z (t) |σ (t) ) is called the observation model, which might capture a distribution over the color of
clothing for each individual.

P (z t |σ (t) ) · P (σ (t) |σ (t−1) ),

We focus on ﬁltering , in which one queries the HMM for the posterior at some timestep, conditioned
on all past observations. Given the distribution P (σ (t) |z (1) , . . . , z (t) ), we recursively compute
P (σ (t+1) |z (1) , . . . , z (t+1) ) in two steps: a prediction/rollup step and a conditioning step. The
ﬁrst updates the distribution by multiplying by the transit
ion model and marginalizing out the
P (σ (t+1) |z (1) , . . . , z (t) ) = Pσ(t) P (σ (t+1) |σ (t) )P (σ (t) |z (1) , . . . , z (t) ).
previous timestep:
The
second conditions
the distribution on an observation z (t+1) using Bayes
rule:
P (σ (t+1) |z (1) , . . . , z (t+1) ) ∝ P (z (t+1) |σ (t+1) )P (σ (t+1) |z (1) , . . . , z (t) ).
Since there are n!
permutations, a single update requires O((n!)2 ) ﬂops and is consequently intractable for all but
very small n. The approach that we advocate is to maintain a compact approximation to the true
distribution based on the Fourier transform. As we discuss later, the Fourier based approximation
is equivalent to maintaining a set of low-order marginals, rather than the full joint, which we regard
as being analagous to an Assumed Density Filter [6].

3 Fourier projections of functions on the Symmetric Group
Over the last 50 years, the Fourier Transform has been ubiquitously applied to everything digital,
particularly with the invention of the Fast Fourier Transform. On the real line, the Fourier Transform
is a well-studied method for decomposing a function into a sum of sine and cosine terms over
a spectrum of frequencies. Perhaps less familiar, is its group theoretic generalization, which we
review in this section with an eye towards approximating functions on the group of permutations, the
Symmetric Group. For permutations on n objects, the Symmetric Group will be abbreviated by Sn .
The formal deﬁnition of the Fourier Transform relies on the t heory of group representations, which
we brieﬂy discuss ﬁrst. Our goal in this section is to motivat
e the idea that the Fourier transform of
a distribution P is related to certain marginals of P . For references on this subject, see [3].
Deﬁnition 1. A representation of a group G is a map ρ from G to a set of invertible dρ × dρ
matrix operators which preserves algebraic structure in the sense that for all σ1 , σ2 ∈ G,
ρ(σ1σ2 ) = ρ(σ1 ) · ρ(σ2 ). The matrices which lie in the image of this map are called the
representation matrices, and we will refer to dρ as the degree of the representation.

Representations play the role of basis functions, similar to that of sinusoids, in Fourier theory. The
simplest basis functions are constant functions — and our ﬁr
st example of a representation is the triv-
ial representation ρ0 : G → R which maps every element of G to 1. As a more pertinent example,
we deﬁne the 1st order permutation representation of Sn to be the degree n representation, τ1 , which
maps a permutation σ to its corresponding permutation matrix given by: [τ1 (σ)]ij = 1 {σ(j ) = i}.
For example, the permutation in S3 which swaps the second and third elements maps to:
τ1 (1 7→ 1, 2 7→ 3, 3 7→ 2) = 0@
0 1A .
0
1
The τ1 representation can be thought of as a collection of n2 functions at once, one for each matrix
entry, [τ1 (σ)]ij . There are other possible permutation representations - for example the 2nd order
unordered permutation representation, τ2 , is deﬁned by the action of a permutation on unordered
pairs of objects, ([ρ(σ)]{i,j},{ℓ,k} = 1 {σ({ℓ, k}) = {i, j }}), and is a degree n(n−1)
representation.
2
And the list goes on to include many more complicated representations.

1
0
0

0
0
1

2

It is useful to think of two representations as being the same if the representation matrices are equal
up to some consistent change of basis. This idea is formalized by declaring two representations ρ
and τ to be equivalent if there exists an invertible matrix C such that C −1 · ρ(σ) · C = τ (σ) for all
σ ∈ G. We write this as ρ ≡ τ .
Most representations can be seen as having been built up by smaller representations. We say that
a representation ρ is reducible if there exist smaller representations ρ1 , ρ2 such that ρ ≡ ρ1 ⊕ ρ2
where ⊕ is deﬁned to be the direct sum representation:
ρ1 ⊕ ρ2 (g) , „ ρ1 (g)
ρ2 (g) « .
0
0
In general, there are inﬁnitely many inequivalent represen tations. However, for any ﬁnite group,
there is always a ﬁnite collection of atomic representation s which can be used to build up any
other representation using direct sums. These representations are referred to as the irreducibles
of a group, and they are simply the collection of representations which are not reducible. We will
refer to the set of irreducibles by R. It can be shown that any representation of a ﬁnite group G
is equivalent to a direct sum of irreducibles [3], and hence, for any representation τ , there exists a
matrices C for which C −1 · τ · C = ⊕ρi∈R ⊕ ρi , where the inner ⊕ refers to some ﬁnite number
of copies of the irreducible ρi .
Describing the irreducibles of Sn up to equivalence is a subject unto itself; We will simply say
that there is a natural way to order the irreducibles of Sn that corresponds to ‘simplicity’ in the
same way that low frequency sinusoids are simpler than higher frequency ones. We will refer to the
irreducibles in this order as ρ0 , ρ1 , . . . . For example, the ﬁrst two irreducibles form the ﬁrst order
permutation representation (τ1 ≡ ρ0 ⊕ ρ1 ), and the second order permutation representation can be
formed by the ﬁrst 3 irreducibles.

(1)

Irreducible representation matrices are not always orthogonal, but they can always be chosen to be
so (up to equivalence). For notational convenience, the irreducible representations in this paper will
always be assumed to be orthogonal.

f (σ) =

3.1 The Fourier transform
On the real line, the Fourier Transform corresponds to computing inner products of a function with
sines and cosines at varying frequencies. The analogous deﬁ nition for ﬁnite groups replaces the
sinusoids by group representations.
Deﬁnition 2. Let f : G → R be any function on a group G and let ρ be any representation on G.
ˆfρ = Pσ f (σ)ρ(σ).
The Fourier Transform of f at the representation ρ is deﬁned to be:
There are two important points which distinguish this Fourier Transform from the familiar version
on the real line — it is matrix-valued, and instead of real num bers, the inputs to ˆf are representations
of G. The collection of Fourier Transforms of f at all irreducibles form the Fourier Transform of f .
As in the familiar case, there is an inverse transform given by:
dρk Tr h ˆf T
ρk · ρk (σ)i ,
|G| Xk
1
where k indexes over the collection of irreducibles of G.
We provide two examples for intuition. For functions on the real line, the Fourier Transform at
zero gives the DC component of a signal. This is also true for functions on a group; If f : G → R
is any function, then the Fourier Transform of f at the trivial representation is constant with
ˆfρ0 = Pσ f (σ). Thus, for any probability distribution P , we have ˆPρ0 = 1. If P were the uniform
distribution, then ˆPρ = 0 at all irreducibles except at the trivial representation.
The Fourier Transform at τ1 also has a simple interpretation:
f (σ)1 {σ(j ) = i} = Xσ :σ(j )=i
f (σ)[τ1 (σ)]ij = Xσ∈Sn
[ ˆfτ1 ]ij = Xσ∈Sn
Thus, if P is a distribution, then ˆPτ1 is a matrix of marginal probabilties, where the ij -th element
is the marginal probability that a random permutation drawn from P maps element j to i. Similarly,
the Fourier transform of P at the second order permutation representation is a matrix of marginal
probabilities of the form P (σ({i, j }) = {k , ℓ}).

(2)

f (σ).

3

In Section 5, we will discuss function approximation by bandlimiting the Fourier coefﬁcients, but
this example should illustrate the fact that maintaining Fourier coefﬁcients at low-order irreducibles
is the same as maintaining low-order marginal probabilities, while higher order irreducibles
correspond to more complicated marginals.
4
Inference in the Fourier domain
Bandlimiting allows for compactly storing a distribution over permutations, but the idea is rather
moot if it becomes necessary to transform back to the primal domain each time an inference
operation is called. Naively, the Fourier Transform on Sn scales as O((n!)2 ), and even the fastest
Fast Fourier Transforms for functions on Sn are no faster than O(n! log(n!)) (see [7] for example).
To resolve this issue, we present a formulation of inference which operates solely in the Fourier
domain, allowing us to avoid a costly transform. We begin by discussing exact inference in the
Fourier domain, which is no more tractable than the original problem because there are n! Fourier
coefﬁcients, but it will allow us to discuss the bandlimitin g approximation in the next section. There
are two operations to consider: prediction/rollup, and conditioning. The assumption for the rest of
this section is that the Fourier Transforms of the transition and observation models are known. We
discuss methods for obtaining the models in Section 7.

4.1 Fourier prediction/rollup
We will consider one particular type of transition model — th at of a random walk over a group.
This model assumes that σ (t+1) is generated from σ (t) by drawing a random permutation τ (t)
from some distribution Q(t) and setting σ (t+1) = τ (t)σ (t) . In our identity management example,
τ (t) represents a random identity permutation that might occur among tracks when they get close
to each other (a mixing event), but the random walk model appears in other applications such as
modeling card shufﬂes [3]. The Fourier domain Prediction/R ollup step is easily formulated using
the convolution theorem (see also [3]):
Proposition 3. Let Q and P be probability distributions on Sn . Deﬁne the convolution of Q and P to
2 )P (σ2 ). Then for any representation ρ, h\Q ∗ P iρ
be the function [Q ∗ P ] (σ1 ) = Pσ2
Q(σ1 · σ−1
=
bQρ · bPρ , where the operation on the right side is matrix multiplication.
The Prediction/Rollup step for the random walk transition model can be written as a convolution:
Q(t) (σ (t+1) ·(σ (t) )−1 )P (σ (t) ) = hQ(t) ∗ P i (σ (t+1) ).
Q(t) (τ (t) )·P (σ (t) ) = Xσ(t)
X
P (σ (t+1) ) =
{(σ(t) ,τ (t) ) : σ(t+1)=τ (t) ·σ(t) }
Then assuming that bP (t)
and bQ(t)
ρ are given, the prediction/rollup update rule is simply:
ρ
ρ · bP (t)
ρ ← bQ(t)
bP (t+1)
ρ .
Note that the update requires only knowledge of ˆP and does not require P . Furthermore, the update
is pointwise in the Fourier domain in the sense that the coefﬁcients at the
representation ρ affect
bP (t+1)
only at ρ.
ρ
4.2 Fourier conditioning
An application of Bayes rule to ﬁnd a posterior distribution P (σ |z ) after observing some evidence z
requires a pointwise product of likelihood L(z |σ) and prior P (σ), followed by a normalization step.
We showed earlier that the normalization constant Pσ L(z |σ) · P (σ) is given by the Fourier trans-
form of \
tion step of conditioning
L(t)P (t) at the trivial representation — and therefore the normaliza
can be implemented by simply dividing each Fourier coefﬁcie nt by the scalar h \
L(t)P (t) iρ0
.
The pointwise product of two functions f and g , however, is trickier to formulate in the Fourier
domain. For functions on the real line, the pointwise product of functions can be implemented
ˆf and ˆg , and so a natural question is: can we apply a
by convolving the Fourier coefﬁcients of
similar operation for functions over other groups? Our answer to this is that there is an analogous
(but more complicated) notion of convolution in the Fourier domain of a general ﬁnite group. We
present a convolution-based conditioning algorithm which we call Kronecker Conditioning, which,
in contrast to the pointwise nature of the Fourier Domain prediction/rollup step, and much like
convolution, smears the information at an irreducible ρk to other irreducibles.

4

(3)

(4)

(5)

f (σ) · g(σ) =

Fourier transforming the pointwise product Our approach to Fourier Transforming the point-
wise product in terms of ˆf and ˆg is to manipulate the function f (σ)g(σ) so that it can be seen as the
result of an inverse Fourier Transform. Hence, the goal will be to ﬁnd matrices Ak (as a function of
ˆf , ˆg ) such that for any σ ∈ G,
dρk Tr “AT
k · ρk (σ)” ,
|G| Xk
1
where Ak = hcf giρk
. For any σ ∈ G we can write the pointwise product in terms ˆf and ˆg using the
inverse Fourier Transform (Equation 2):
f (σ) · g(σ) = " 1
ρi · ρi (σ)”# · " 1
ρj · ρj (σ)”#
dρi Tr “ ˆf T
dρj Tr “ˆgT
|G| Xi
|G| Xj
= „ 1
|G| «2 Xi,j
dρi dρj hTr “ ˆf T
ρi · ρi (σ)” · Tr “ˆgT
ρj · ρj (σ)”i .
Now we want to manipulate this product of traces in the last line to be just one trace (as in
Equation 3), by appealing to some properties of the matrix Kronecker product. The connection
to the pointwise product ( ﬁrst observed in [8]), lies in the p roperty that for any matrices U, V ,
Tr (U ⊗ V ) = (Tr U ) · (Tr V ). Applying this to Equation 4, we have:
Tr “ ˆf T
ρi · ρi (σ)” · Tr “ˆgT
ρj · ρj (σ)” = Tr ““ ˆf T
ρi · ρi (σ)” ⊗ “ˆgT
ρj · ρj (σ)””
= Tr „“ ˆfρi ⊗ ˆgρj ”T
· (ρi (σ) ⊗ ρj (σ))« ,
where the last line follows by standard matrix properties. The term on the right, ρi (σ) ⊗ ρj (σ),
itself happens to be a representation, called the Kronecker Product Representation.
In general,
the Kronecker Product representation is reducible, and so it can decomposed into a direct sum of
irreducibles. This means that if ρi and ρj are any two irreducibles of G, there exists a similarity
transform Cij such that for any σ ∈ G,
zijkMℓ=1
· [ρi ⊗ ρj ] (σ) · Cij = Mk
The ⊕ symbols here refer to a matrix direct sum as in Equation 1, k indexes over all irreducible
representations of Sn , while ℓ indexes over a number of copies of ρk which appear in the de-
composition. We index blocks on the right side of this equation by pairs of indices (k , ℓ). The
number of copies of each ρk is denoted by the integer zijk , the collection of which, taken over
all triples (i, j, k), are commonly referred to as the Clebsch-Gordan series. Note that we allow
the zijk to be zero, in which case ρk does not contribute to the direct sum. The matrices Cij are
known as the Clebsch-Gordan coefﬁcients . The Kronecker Product Decomposition problem is
that of ﬁnding the irreducible components of the Kronecker p roduct representation, and thus to
ﬁnd the Clebsch-Gordan series/coefﬁcients for each pair of
representations (ρi , ρj ). Decomposing
the Kronecker product inside Equation 5 using the Clebsch-Gordan series/coefﬁcients yields the
desired Fourier Transform, which we summarize here:
Proposition 4. Let ˆf , ˆg be the Fourier Transforms of functions f and g respectively, and for each
· (cid:16) ˆfρi ⊗ ˆgρj (cid:17) · Cij . Then the
ordered pair of irreducibles (ρi , ρj ), deﬁne the matrix: Aij , C −1
ij
Fourier tranform of the pointwise product f g is:
zijkXℓ=1
hcf giρk
dρk |G| Xij
1
(6)
ij is the block of Aij corresponding to the (k , ℓ) block in ⊕k ⊕zijk
where Akℓ
ℓ
See the Appendix for a full proof of Proposition 4. The Clebsch-Gordan series, zijk , plays an
important role in Equation 6, which says that the (ρi , ρj ) crossterm contributes to the pointwise
product at ρk only when zijk > 0. For example,

ρk (σ).

C −1
ij

=

dρi dρj

Akℓ
ij ,

ρk .

ρ1 ⊗ ρ1 ≡ ρ0 ⊕ ρ1 ⊕ ρ2 ⊕ ρ3 .
So z1,1,k = 1 for k ≤ 3 and is zero otherwise.

(7)

5

Unfortunately, there are no analytical formulas for ﬁnding the Clebsch-Gordan series or coefﬁcients,
and in practice, these computations can take a long time. We emphasize however, that as fundamen-
tal quantities, like the digits of π , they need only be computed once and stored in a table for future
reference. Due to space limitations, we will not provide complete details on computing these num-
bers. We refer the reader to Murnaghan [9], who provides general formulas for computing Clebsch-
Gordan series for pairs of low-order irreducibles, and to Appendix 1 for details about computing
Clebsch-Gordan coefﬁcients. We will also make precomputed coefﬁcients available on the web.
5 Approximate inference by bandlimiting
We approximate the probability distribution P (σ) by ﬁxing a bandlimit B and maintaining the
Fourier transform of P only at irreducibles ρ0 , . . . ρB . We refer to this set of irreducibles as B . As on
the real line, smooth functions are generally well approximated by only a few Fourier coefﬁcients,
while “wigglier ” functions require more. For example, when B = 3, B is the set ρ0 , ρ1 , ρ2 , and
ρ3 , which corresponds to maintaining marginal probabilities of the form P (σ((i, j )) = (k , ℓ)).
During inference, we follow the procedure outlined in the previous section but ignore the higher
order terms which are not maintained. Pseudocode for bandlimited prediction/rollup and Kronecker
conditioning is given in Figures 1 and 2.

Since the Prediction/Rollup step is pointwise in the Fourier domain, the update is exact for the
maintained irreducibles because higher order irreducibles cannot affect those below the bandlimit.
As in [5], we ﬁnd that the error from bandlimiting creeps in th rough the conditioning step. For
example, Equation 7 shows that if B = 1 (so that we maintain ﬁrst-order marginals), then the
pointwise product spreads information to second-order marginals. Conversely, pairs of higher-order
irreducibles may propagate information to lower-order irreducibles. If a distribution is diffuse, then
most of the energy is stored in low-order Fourier coefﬁcient s anyway, and so this is not a big prob-
lem. However, it is when the distribution is sharply concentrated at a small subset of permutations,
that the low-order Fourier projection is unable to faithfully approximate the distribution, in many
circumstances, resulting in a bandlimited Fourier Transform with negative “marginal probabilities ”!
To combat this problem, we present a method for enforcing nonnnegativity.

Projecting to a relaxed marginal polytope The marginal polytope, M, is the set of marginals
which are consistent with some joint distribution over permutations. We project our approximation
onto a relaxation of the marginal polytope, M′ , deﬁned by linear inequality constraints that
marginals be nonnegative, and linear equality constraints that they correspond to some legal Fourier
transform. Intuitively, our relaxation produces matrices of marginals which are doubly stochastic
(rows and columns sum to one and all entries are nonnegative), and satisfy lower-order marginal
consistency (different high-order marginals are consistent at lower orders).

(f (σ) − g(σ))2 =

After each conditioning step, we apply a ‘correction’ to the approximate posterior P (t) by ﬁnding
the bandlimited function in M′ which is closest to P (t) in an L2 sense. To perform the projection,
we employ the Plancherel Theorem [3] which relates the L2 distance between functions on Sn to a
distance metric in the Fourier domain.
dρk Tr (cid:18)(cid:16) ˆfρk − ˆgρk (cid:17)T
· (cid:16) ˆfρk − ˆgρk (cid:17)(cid:19) .
Proposition 5. Xσ
|G| Xk
1
We formulate the optimization as a quadratic program where the objective is to minimize the right
side of Equation 8 — the sum is taken only over the set of mainta
ined irreducibles, B , and subject
to the linear constraints which deﬁne M′ .
We remark that even though the projection will always produce a Fourier transform corresponding
to nonnegative marginals, there might not necessarily exist a joint probability distribution on Sn
consistent with those marginals.
In the case of ﬁrst-order m arginals, however, the existence of
a consistent joint distribution is guaranteed by the Birkhoff-von Neumann theorem [10], which
states that a matrix is doubly stochastic if and only if it can be written as a convex combination of
permutation matrices. And so for the case of ﬁrst-order marg inals, our relaxation is in fact, exact.
6 Related Work
The Identity Management problem was ﬁrst introduced in [2] w hich maintains a doubly stochastic
ﬁrst order belief matrix to reason over data associations. Schumitsch et al. [4] exploits a similar
idea, but formulated the problem in log-space.

(8)

6

Figure 1: Pseudocode for the Fourier Prediction/Rollup Algorithm.
PR ED IC T IONRO L LU P
ρk · ˆP (t)
ρk ← ˆQ(t)
foreach ρk ∈ B do ˆP (t+1)
ρk ;

← 0 //Initialize Posterior

Figure 2: Pseudocode for the Kronecker Conditioning Algorithm.
KRON ECK ERCOND I T ION ING
foreach ρk ∈ B do h \L(t)P (t) iρk
//Pointwise Product
foreach ρi ∈ B do
foreach ρj ∈ B do
z ← CGseries(ρi , ρj ) ;
ij · “ ˆfρi ⊗ ˆgρj ” · Cij ;
Cij ← CGcoef f icients(ρi , ρj ) ; Aij ← C T
for ρk ∈ B such that zijk 6= 0 do
for ℓ = 1 to zk do
h \L(t)P (t) iρk
← h \L(t)P (t) iρk
dρj
dρi
//Akℓ
n! Akℓ
ij is the (k , ℓ) block of Aij
ij
dρk
Z ← h \L(t)P (t) iρ0
;
foreach ρk ∈ B do h \L(t)P (t) iρk
Z h \L(t)P (t) iρk
← 1
Kondor et al. [5] were the ﬁrst to show that the data associati on problem could be approximately
handled via the Fourier Transform. For conditioning, they exploit a modi ﬁed FFT factorization
which works on certain simpli ﬁed observation models. Our ap proach generalizes the type of
observations that can be handled in [5] and is equivalent in the simpli ﬁed model that they present.
We require O(D3n2 ) time in their setting. Their FFT method saves a factor of D due to the fact that
certain representation matrices can be shown to be sparse. Though we do not prove it, we observe
that the Clebsch-Gordan coefﬁcients, Cij are typically similarly sparse, which yields an equivalent
running time in practice. In addition, Kondor et al. do not address the issue of projecting onto valid
marginals, which, as we show in our experimental results, is fundamental in practice.

+

//Normalization

Willsky [8] was the ﬁrst to formulate a nonabelian version of
the FFT algorithm (for Metacyclic
groups) as well as to note the connection between pointwise products and Kronecker product
decompositions for general ﬁnite groups. In this paper, we a ddress approximate inference, which is
necessary given the n! complexity of inference for the Symmetric group.

7 Experimental results
For small n, we compared our algorithm to exact inference on synthetic datasets in which tracks are
drawn at random to be observed or swapped. For validation we measure the L1 distance between true
and approximate marginal distributions. In (Fig. 3(a)), we call several mixings followed by a single
observation, after which we measured error. As expected, the Fourier approximation is better when
there are either more mixing events, or when more Fourier coefﬁcients are maintained. In (Fig. 3(b))
we allow for consecutive conditioning steps and we see that that the projection step is fundamental,
especially when mixing events are rare, reducing the error dramatically. Comparing running times,
it is clear that our algorithm scales gracefully compared to the exact solution (Fig. 3(c)).

We also evaluated our algorithm on data taken from a real network of 8 cameras (Fig. 3(d)). In the
data, there are n = 11 people walking around a room in fairly close proximity. To handle the fact
that people can freely leave and enter the room, we maintain a list of the tracks which are external
to the room. Each time a new track leaves the room, it is added to the list and a mixing event is
called to allow for m2 pairwise swaps amongst the m external tracks.

The number of mixing events is approximately the same as the number of observations. For each
observation, the network returns a color histogram of the blob associated with one track. The
task after conditioning on each observation is to predict identities for all tracks inside the room,
and the evaluation metric is the fraction of accurate predictions. We compared against a baseline
approach of predicting the identity of a track based on the most recently observed histogram
at that track. This approach is expected to be accurate when there are many observations and
discriminative appearance models, neither of which our problem afforded. As (Fig. 3(e)) shows,

7

0.1

0.08

0.06

0.04

0.02

s
l
a
n
i
g
r
a
m
 
r
e
d
r
o
 
t
s
1
 
t
a
 
r
o
r
r
e
 
1
L

Error of Kronecker Conditioning, n=8

Projection versus No Projection (n=6)

 

b=1, w/o Projection
b=2, w/o Projection
b=3, w/o Projection
b=1, w/Projection
b=2, w/Projection
b=3, w/Projection
b=0 (Uniform distribution)

 

b=1
b=2
b=3

0.6

0.5

0.4

0.3

0.2

0.1

s
s
l
p
a
e
n
t
i
g
s
e
r
a
m
M
i
t
 
 
r
0
e
5
d
2
r
o
 
r
 
e
t
s
v
1
o
 
 
t
d
a
e
 
r
g
o
a
r
r
r
e
e
v
 
1
A
L

Running time of 10 forward algorithm iterations

 

b=1
b=2
b=3
exact

5

4

3

2

1

s
d
n
o
c
e
s
 
n
i
 
e
m
i
t
 
g
n
i
n
n
u
R

0
 
0

5
10
# Mixing Events
(a) Error of Kronecker Con-
ditioning

15

0

 

0.2
0.8
0.6
0.4
Fraction of Observation events
(b) Projection vs. No Projec-
tion

0
 
4

8

5

6
n
(c) n versus Running Time

7

d
e
i
f
i
t
n
e
d
I
 
y
l
t
c
e
r
r
o
c
 
s
k
c
a
r
T
 
%

60

50

40

30

20

10

0

Omniscient
w/Projection

w/o Projection

Baseline

 

(d) Sample Image
(e) Accuracy for Camera Data
Figure 3: Evaluation on synthetic ((a)-(c)) and real camera network ((d),(e)) data.
both the baseline and ﬁrst order model(without projection)
fared poorly, while the projection step
dramatically boosted the accuracy. To illustrate the difﬁc ulty of predicting based on appearance
alone, the rightmost bar reﬂects the performance of an omniscient tracker who knows the result of
each mixing event and is therefore left only with the task of distinguishing between appearances.
8 Conclusions
We presented a formulation of hidden Markov model inference in the Fourier domain. In particular,
we developed the Kronecker Conditioning algorithm which performs a convolution-like operation
on Fourier coefﬁcients to ﬁnd the Fourier transform of the po
sterior distribution. We argued that
bandlimited conditioning can result in Fourier coefﬁcient s which correspond to no distribution, but
that the problem can be remedied by projecting to a relaxation of the marginal polytope. Our eval-
uation on data from a camera network shows that our methods outperform well when compared to
the optimal solution in small problems, or to an omniscient tracker in large problems. Furthermore,
we demonstrated that our projection step is fundamental to obtaining these high-quality results.

We conclude by remarking that the mathematical framework developed in this paper is quite general.
In fact, both the prediction/rollup and conditioning formulations hold over any ﬁnite group, provid-
ing a principled method for approximate inference for problems with underlying group structure.
Acknowledgments
This work is supported in part by the ONR under MURI N000140710747, the ARO under grant
W911NF-06-1-0275, the NSF under grants DGE-0333420, EEEC-540865, Nets-NOSS 0626151
and TF 0634803, and by the Pennsylvania Infrastructure Technology Alliance (PITA). Carlos
Guestrin was also supported in part by an Alfred P. Sloan Fellowship. We thank Kyle Heath for
helping with the camera data and Emre Oto, and Robert Hough for valuable discussions.
References
[1] Y. Ivanov, A. Sorokin, C. Wren, and I. Kaur. Tracking people in mixed modality systems. Technical
Report TR2007-11, MERL, 2007.
[2] J. Shin, L. Guibas, and F. Zhao. A distributed algorithm for managing multi-target identities in wireless
ad-hoc sensor networks. In IPSN, 2003.
[3] P. Diaconis. Group Representations in Probability and Statistics. IMS Lecture Notes, 1988.
[4] B. Schumitsch, S. Thrun, G. Bradski, and K. Olukotun. The information-form data association ﬁlter. In
NIPS. 2006.
[5] R. Kondor, A. Howard, and T. Jebara. Multi-object tracking with representations of the symmetric group.
In AISTATS, 2007.
[6] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In UAI, 1998.
[7] R. Kondor. Snob: a C++ library for fast Fourier transforms on the symmetric group, 2006. Available at
http://www.cs.columbia.edu/ ˜risi/Snob/ .
[8] A. Willsky. On the algebraic structure of certain partially observable ﬁn ite-state markov processes. Infor-
mation and Control, 38:179–212, 1978.
[9] F.D. Murnaghan. The analysis of the kronecker product of irreducible representations of the symmetric
group. American Journal of Mathematics, 60(3):761–784, 1938.
[10] J. van Lint and R.M. Wilson. A Course in Combinatorics. Cambridge University Press, 2001.

8


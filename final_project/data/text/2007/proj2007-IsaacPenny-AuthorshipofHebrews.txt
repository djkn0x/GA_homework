Isaac Penny 
CS 229 
Term Project 
Final Report 
 
1. Introduction 
For  the  term  project,  I  applied  machine  learning  to  text  classification  in  ancient  documents.  In  particular,  I  used  a 
machine  learning  algorithm,  trained  on  the  Pauline  epistles  of  the  Bible ’ s  New  Testament,  to  determine  the 
probability  that  Paul  also  authored  the  Epistle  to  the  Hebrews.  Of  the  twenty  seven  books  in  the  New  Testament, 
only the Epistle to the Hebrews does not contain an explicit claim of authorship. However, tradition and the writings 
of several early church leaders indicate Paul as the book’ s author.  
 
2. Approach 
A Support Vector Machine, was chosen  for  the  logistic classification process. An SVM was chosen  for  its  “ off-the-
shelf”  ease of use and its wide acceptance within the field of text classification1. The support vector machine further 
utilizes  a  simplified  version  of Sequential Minimization  and Optimization  algorithm2. See  the  referenced  papers  for 
more algorithm details. 
 
2.1 The Text 
All books were evaluated in Greek to avoid the affects of translation. The Greek text used is the Stephanus edition of 
the  Textus  Receptus,  compiled  in  1550  A.D.  The  text  itself  is  in  the  public  domain,  however  the  project  utilizes  a 
proprietary  version  obtained  from  Hermeneutika  Software,  under  an  academic  license3.  The  Hermeneutika  version 
of  the  text  also  provides  the  Greek  root  word  and  morphology  (part  of  speech,  number,  person,  tense,  mood,  and 
voice)  for  each  word  in  the  text.  An  excerpt  from  the  text  showing  John  3:16  ( “ For God  so  loved  the  world …
” )  is 
shown below: 
 

John 3:16  (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:3)(cid:4)(cid:5)(cid:7)bo (cid:9)(cid:10)(cid:11)(cid:12)(cid:7)(cid:9)(cid:10)(cid:13)(cid:12)(cid:7)c (cid:14)(cid:15)(cid:9)(cid:10)(cid:13)(cid:16)(cid:14)(cid:17)(cid:18)(cid:19)(cid:7)(cid:10)(cid:15)(cid:9)(cid:10)(cid:16)(cid:10)(cid:13)(cid:5)(cid:7)viaa3s (cid:8)(cid:20)(cid:7)(cid:8)(cid:20)(cid:7)dnms (cid:21)(cid:18)(cid:8)(cid:11)(cid:6)(cid:7)(cid:21)(cid:18)(cid:8)(cid:13)(cid:6)(cid:7)nnms (cid:4)(cid:8)(cid:11)(cid:19)(cid:7)
(cid:8)(cid:20)(cid:7) dams  (cid:22)(cid:8)(cid:13)(cid:17)(cid:23)(cid:8)(cid:19)(cid:7) (cid:22)(cid:8)(cid:13)(cid:17)(cid:23)(cid:8)(cid:6)(cid:7) nams  (cid:5) (cid:3)(cid:17)(cid:4)(cid:18)(cid:7) (cid:5) (cid:3)(cid:17)(cid:4)(cid:18)(cid:7) c  (cid:4)(cid:8)(cid:11)(cid:19)(cid:7) (cid:8)(cid:7) (cid:20)dams  (cid:2)(cid:24)(cid:20)(cid:8)(cid:11)(cid:19)(cid:7) (cid:2)(cid:24)(cid:20)(cid:8)(cid:13)(cid:6)(cid:7) nams  (cid:10)(cid:2)(cid:15)(cid:4)(cid:8)(cid:2)(cid:25)(cid:7) (cid:10)(cid:2)(cid:15)(cid:4)(cid:8)(cid:13)(cid:6)(cid:7) rpgms 
(cid:4)(cid:8)(cid:11)(cid:19)(cid:7) (cid:8)(cid:7)(cid:20)dams (cid:23)(cid:8)(cid:19)(cid:8)(cid:9)(cid:18)(cid:19)(cid:14)(cid:25)(cid:7)(cid:23)(cid:8)(cid:19)(cid:8)(cid:9)(cid:18)(cid:19)(cid:14)(cid:13)(cid:6)(cid:7)aamsn (cid:18)(cid:26)(cid:27)(cid:5)(cid:22)(cid:18)(cid:19)(cid:7)(cid:27)(cid:24)(cid:13)(cid:27)(cid:5)(cid:23)(cid:24)(cid:7)viaa3s (cid:24)(cid:3)(cid:19)(cid:10)(cid:7)(cid:24)(cid:3)(cid:19)(cid:10)(cid:7)c (cid:16)(cid:10)(cid:25)(cid:6)(cid:7)(cid:16)(cid:10)(cid:25)(cid:6)(cid:7)anmsn (cid:8)(cid:20)(cid:7)(cid:8)(cid:20)(cid:7)
dnms (cid:16)(cid:24)(cid:17)(cid:4)(cid:18)(cid:2)(cid:13)(cid:5)(cid:19)(cid:7) (cid:16)(cid:24)(cid:17)(cid:4)(cid:18)(cid:2)(cid:13)(cid:5)(cid:7) vppanms (cid:18)(cid:24)(cid:15)(cid:6)(cid:7)(cid:18)(cid:24)(cid:15)(cid:6)(cid:7)p (cid:10)(cid:2)(cid:15)(cid:4)(cid:8)(cid:11)(cid:19)(cid:7)(cid:10)(cid:2)(cid:15)(cid:4)(cid:8)(cid:13)(cid:6)(cid:7) rpams (cid:23)(cid:14)(cid:11)(cid:7)(cid:23)(cid:14)(cid:13)(cid:7)xo (cid:10)(cid:15)(cid:16)(cid:8)(cid:13)(cid:28)(cid:14)(cid:4)(cid:10)(cid:24)(cid:7)(cid:10)(cid:15)(cid:16)(cid:8)(cid:13)(cid:28)(cid:28)(cid:2)(cid:23)(cid:24)(cid:7)
vsam3s (cid:10)(cid:15)(cid:28)(cid:28)(cid:7)(cid:10)(cid:15)(cid:28)(cid:28)(cid:10)(cid:13)(cid:7)c (cid:18)(cid:26)(cid:29)(cid:14)(cid:30)(cid:7)(cid:18)(cid:26)(cid:29)(cid:5)(cid:7)vspa3s (cid:31)(cid:5)(cid:14)(cid:11)(cid:19)(cid:7)(cid:31)(cid:5)(cid:14)(cid:13)(cid:7)nafs (cid:10)(cid:24)(cid:15)(cid:5)(cid:13)(cid:19)(cid:24)(cid:8)(cid:19)(cid:7)(cid:10)(cid:24)(cid:15)(cid:5)(cid:13)(cid:19)(cid:24)(cid:8)(cid:6)(cid:7)aafsn  

 
Each  word  above  appears  in  triplet.  The  first  word  is  the  original  Greek  word.  The  second  word  is  the  Greek  root 
word.  The  third  word  in  each  triplet  is  the  morphology  of  the  Greek  word  (ex:  vsam3s  means  a  verb  with 
subjunctive mood, aorist tense, middle voice, which is 3rd person and singular in number).  
 
2.2 Training Examples 
Each  book  in  the  New  Testament  is  divided  up  into  chapter  and  verse  divisions,  by  scholars  to  aid  in  easy 
referencing.  Individual  verses  from  each  book  were  used  as  training  examples.  Positive  training  examples  were 
provided by  the 13 Pauline books  in  the New Testament. Negative  training examples were provided by  the 13  non-
Pauline books in the New Testament.  
 

2.3 Parsing the Text 
The  data  needed  to  be  extracted  from  its  plain  text  format  and  stored  in  a  useful  data  structure,  before  it  could  be 
used  for  classification.  The  data  was  extracted  using  a  simple  text  string  search,  where  spaces  were  treated  as 
delimiters  between  words.  The  data  was  then  parsed  into  a  five  dimensional  cell  array  with  the  following 
dimensions: 
 

1. Book number (1 = Matthew, 2 = Mark, etc.) 
2. Chapter number 
3. Verse number 
4. Word number within the verse 
5. String type (1 = inflected word, 2 = uninflected word, 3 = morphology)  

 
This structure maintains all of the original relationships between the data, while making it easy to extract the desired 
from of a particular word from the text.  
 
2.4 Feature Selection 
N-gram  frequencies  were  used  as  input  features  for  each  training  example.  The  density  of  the  data  in  the  feature 
space of n-grams of size two and higher was deemed too sparse to be useful, thus only unigrams were used.  
 
Only root (uninflected) unigrams were used for classification. This approach results in a smaller number of features, 
than  if  all  of  the  inflected  forms  of  a  given word were  used. The  smaller  number  of  features  results  in  a  less  sparse 
set  of  training  data. The more  dense  training  set  helps  the  classifier  generalize  better  to  test  sets where  the  test  data 
set  has  a  predominantly  different  morphology  than  the  training  set.  Ignoring  the  morphology  in  determining 
authorship  assumes  that  the  choice  of  root  word  (ex:  play  versus  compete)  is  a  more  significant  indicator  of 
authorship than is the choice of morphology (ex: played versus have been playing).    
 
2.5 Creating Dictionaries 
A  dictionary  of  all  uninflected  unigrams  was  created  by  scanning  the  five  dimensional  datastructure  mentioned 
above. The frequency of occurance for each unigram was also recorded.  
 
One  of  the  main  goals  of  the  project  was  to  quantify  the  effect  of  feature  space  size  on  classification.  Thus 
dictionaries  of  various  sizes  were  created.  d100  is  a  dictionary  composed  of  the  one  hundred  unigrams  that  occur 
most  frequently  in  the  New  Testament.  Choosing  the  most  frequent  unigrams  has  two  benefits.  First,  the  feature 
space will be less sparse and therefore more useful for classification purposes. Also, frequency of use with common 
unigrams  ((cid:8)(cid:20)(cid:7) the,  (cid:1)(cid:2)(cid:3)(cid:4)  and,  etc.)  is  only  slightly  affected  by  a  work ’ s  content.  As  such  they  are  commonly  used 
indicators of authorship4. Equation 1 shows an example of the aforementioned dictionary: 
(cid:2)
(cid:1)
(cid:8)(cid:20)
(cid:3)
(cid:4)
(cid:22)(cid:10)(cid:24)(cid:13)
(cid:4)
(cid:3)
˛
(cid:3)
(cid:4)
...
(cid:4)
(cid:3)
(cid:21)(cid:2)(cid:23)(cid:8)(cid:13)(cid:5)
(cid:5)
(cid:6)

 !"# 
9267
...
1

·
100 2
(cid:1)

(1)  

 

 

d
100

=

 

2.6 Cross-validation 
K-fold  cross-validation  was  used  to  explore  the  effect  of  feature  space  size  on  classification  error.  In  K-fold  cross-
validation, the original training data set is divided into K  subsets. Of the K subsets, a  single subset  is retained as test 
data,  while  the  remaining  K  −
 1  subsets  are  used  as  training  data. The  cross-validation  process  is  repeated K  times, 
with  each  of  the  K  subsets  being  used  exactly  once  as  the  test  data  set.  Cross-validation  error  is  then  the  mean 
classification error among the K repetitions5.  
 
3. Results 
3.1 Effect of Feature Space Size on Cross-validation 
The  training  data  set  was  divided  into  ten  subsets  for  cross-validation  purposes.  Figure  3.1  shows  the  effect  of 
feature space size on cross-validation error.  

 

100

90

80

70

60

50

40

30

20

10

)
%
(
 
r
o
r
r
E
 
n
o
i
t
a
d
i
l
a
v
-
s
s
o
r
C

0

0

20

40

60

140
120
100
80
Feature S pac e S iz e  (num ber of unigram s )

160

180

200

220

 

Figure 3.1. Effect of feature space size on cross-validation error. 
 
As seen in the  figure, using a dictionary size of more than  fifty unigrams does not significantly reduce classification 
error. 
 
3.2 Percent Verses Classification 
Each  verse  in  the  book  of Hebrews was  individually  classified  as  pauline  or  nonpauline.  This  process was  repeated 
using each dictionary. The process also repeated for several other books believed to be either pauline or nonpauline. 
Figure 3.2 shows the resulting percentage of verses classified as Pauline. 

100

e
n
i
l
u
a
p
%
 
,
e
n
i
l
u
a
P
 
s
a
 
d
e
i
f
i
s
s
a
l
C
 
s
e
s
r
e
V
 
f
o
 
t
n
e
c
r
e
P

90

80

70

60

50

40

30

20

10

0

0

 

E pis t le  to Hebrews
P au line  (Rom ans ,  Ga lat ians ,   Tit us )
Nonpau line  (M ark ,   John,  Reve lat ions )

50

150
100
Feature S pac e S iz e  (num ber of  un igram s )

200

 

Figure 3.2. Percentage of verses in Hebrews and several other NT books classified as Pauline. 

 
As  expected  given  the  cross-validation  study,  a  feature  space  size  (dictionary  size)  of  greater  than  fifty  unigrams 
does  not  further  separate  the  various  book  categories.  Also,  it  should  be  noted  that  the  variance  in  the  nonpauline 
category is significantly larger than that in the Pauline category. This is to be expected, as there is a lurking variable 
of multiple  authors within  the  nonpauline  category. The  resulting  analysis  shows  that  for  dictionary  sizes  over  fifty 
unigrams, the mean percent of verses in Hebrews classified as Pauline is 58%.  
 
3.3 Statistical Significance of Results 
The  classification  of  verses  from  Hebrews  certainly  appears  closer  to  that  of  the  pauline  books  than  that  of 
nonpauline  books.  The  question  becomes  whether  this  difference  is  statistically  significant.  Using  the  central  limit 
theorem,  we  hypothesize  that  the  percentage  of  verses  classified  as  pauline  for  a  book  will  itself  be  distributed 
normally  about  the  mean  value  for  that  book’ s  category.  Thus  we  can  use  a  standard  normal  z-test  to  calculate  the 
probability  that  Hebrews  is  in  the  Pauline  and  non  Pauline  categories.  For  a  feature  space  of  size  fifty,  the  pauline 
and nonpauline z-scores were calculated. A z-score measures the distance of a data point  from a category’ s mean in 
units  of  the  category’ s  standard  deviation. The  standard  normal  distribution  can  then  be  used  to  find  the  probability 
that the variation within each category can explain the datapoint’ s departure from the category mean. This calculated 
probability  is  the  probability  that  the  data  point  belongs  to  the  category.  The  z-scores  and  probabilities  are 
summarized in Table 3.3. 
 

Category 

Pauline 

Nonpauline 

Table 3.3. Results of Z significance test using d50 . 
-
m
% pauline
(
category
s
category

Z-score 

=

 

P

Hebrews in Category

)

 

5.4 

6.7 

 

3.8 10 -·
8
1.1 10-·
11

 

 

4. Conclusions 
The  z-test  indicates  that  Hebrews  as  a  whole  is  more  likely  to  be  in  the  pauline  category  than  it  is  to  be  in  the 
nonpauline category. The low probability that Hebrews belongs to either the pauline or nonpauline categories might 
also  suggest  that  Hebrews  was  written  by  a  mystery  author  whose  writings  are  not  otherwise  included  in  the  New 
Testament.  However,  since  we  do  not  have  training  data  for  the  mystery  author,  one  cannot  evaluate  such  a 
hypothesis using the current approach.  
 
5. Future Work 
This  conclusion  is  premature. Multiple  authors were  lumped  into  the  nonpauline  category. Thus  it  could  be  that  the 
variation  within  the  writings  of  an  individual  nonpauline  author  is  high  enough  to  account  for  Hebrews’   deviation 
from that author ’ s mean classification score. However, training individual classifiers  for each of the New Testament 
authors has the downside of a lack-of-training-data problem. This approach is suggested for future work. 
 
This project only considers the authorship of Hebrews as a complete unit. Future work might statistically analyze the 
distribution of Pauline classified verses within Hebrews to determine if certain  sections of  the book are more or less 
likely to have been written by Paul. 
 
6. Bibliography 
 
1. 
Platt, John. 1998. Fast Training of Support Vector Machines using Sequential Minimal Optimization. 
Advances in Kernel Methods – Support Vector Learning. MIT Press: Boston. 

Boser, Guyon, Vapnik. 1992. A training algorithm for optimal margin classifiers. 5th Annual ACM Workshop 
on COLT. ACM Press: Pittsburgh. pp144-152. 

Erasmus, Desiderius. 1550. Textus Receptus. Public Domain. Made available by Hermeneutika Software. 
www.bibleworks.com Accessed November 20, 2007.  

Various. 2007. Context-free grammar. Wikipedia Online Encyclopedia. Accessed at  
http://en.wikipedia.org/wiki/Context-free_grammar on December 4, 2007. 

Various. 2007. Cross-Validation. Wikipedia Online Encyclopedia. Accessed at 
http://en.wikipedia.org/wiki/Cross_validation on December 5, 2007. 

 

2. 

 
3. 

 
4. 

 
5. 

 
 


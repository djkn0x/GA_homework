The In ﬁnite Markov Model

Daichi Mochihashi ∗
NTT Communication Science Laboratories
Hikaridai 2-4, Keihanna Science City
Kyoto, Japan 619-0237
daichi@cslab.kecl.ntt.co.jp

Eiichiro Sumita
ATR / NICT
Hikaridai 2-2, Keihanna Science City
Kyoto, Japan 619-0288
eiichiro.sumita@atr.jp

Abstract

We present a nonparametric Bayesian method of estimating variable order Markov
processes up to a theoretically inﬁnite order. By extending a stick-breaking prior,
which is usually deﬁned on a unit interval, “vertically” to t
he trees of inﬁnite depth
associated with a hierarchical Chinese restaurant process , our model directly infers
the hidden orders of Markov dependencies from which each symbol originated.
Experiments on character and word sequences in natural lang uage showed that
the model has a comparative performance with an exponential ly large full-order
model, while computationally much efﬁcient in both time and space. We expect
that this basic model will also extend to the variable order h ierarchical clustering
of general data.

1

Introduction

Since the pioneering work of Shannon [1], Markov models have not only been taught in elementary
information theory classes, but also served as indispensab le tools and building blocks for sequence
modeling in many ﬁelds, including natural language process ing, bioinformatics [2], and compres-
sion [3]. In particular, (n−1)th order Markov models over words are called “n-gram” langua ge
models and play a key role in speech recognition and machine translation, as regards choosing the
most natural sentence among candidate transcriptions [4].

Despite its mathematical simplicity, an inherent problem with a Markov model is that we must
determine its order. Because higher-order Markov models have an exponentially large number of
parameters, their orders have been restricted to a small, of ten ﬁxed number. In fact, for “n-gram”
models the assumed word dependency n is usually set at from three to ﬁve due to the high dimen-
sionality of the lexicon. However, word dependencies will often have a span of greater than n for
phrasal expressions or compound proper nouns, or a much shor ter n will sufﬁce for some grammat-
ical relationships. Similarly, DNA or amino acid sequences might have originated from multiple
temporal scales that are unknown to us.

To alleviate this problem, many “variable-order” Markov mo dels have been proposed [2, 5, 6, 7].
However, all stemming from [5] and [7], they are based on prun ing a huge candidate sufﬁx tree by
employing such criteria as KL-divergences. This kind of “po st-hoc” approach suffers from several
important limitations: First, when we want to consider deeper dependences, the candidate tree to be
pruned will be extremely large. This is especially prohibit ive when the lexicon size is large as with
language. Second, the criteria and threshold for pruning th e tree are inherently exogeneous and must
be set carefully so that they match the desired model and current data. Third, pruning by empirical
counts in advance, which is often used to build “arbitrary or der” candidate trees in these approaches,
is shown to behave very badly [8] and has no theoretical standpoints.

In contrast, in this paper we propose a complete generative model of variable-order Markov pro-
cesses up to a theoretically inﬁnite order. By extending a st
ick-breaking prior, which is usually

∗This research was conducted while the ﬁrst author was afﬁliated with ATR/N ICT.

Depth 0

ǫ

Depth 0

ǫ

1

sing like

”will ”

”of ”

”and”

1

sing like

”will ”

”of ”

”and”

”she will ”
2

”he will ”

”states of ”

”bread and”

”she will ”
2

”order of ”

”states of ”

”bread and”

cry

like

america

sing

=customer

butter
=proxy customer

cry
= customer

”united states of ”

butter

= proxy customer

”the united states of ”

america

(a) Sufﬁx Tree representation of the hierarchical Chinese
Restaurant process on a second-order Markov model. Each
count is a customer in this sufﬁx tree.

(b) Inﬁnite sufﬁx tree of the proposed model.
Deploying customers at suitable depths, i.e.
Markov orders, is our inference problem.
Figure 1: Hierarchical Chinese restaurant processes over t he ﬁnite and inﬁnite sufﬁx trees.

deﬁned on a unit interval, “vertically” to the trees of inﬁni
te depth associated with a hierarchical
Chinese restaurant process, our model directly infers the h idden orders of Markov dependencies
from which each symbol originated. We show this is possible with a small change to the inference
of the hiearchical Pitman-Yor process in discrete cases, and actually makes it more efﬁcient in both
computational time and space. Furthermore, we extend the variable model by latent topics to show
that we can induce the variable length “stochastic phrases ”
for topic by topic.

2 Sufﬁx Trees on Hierarchical Chinese Restaurant Processes

The main obstacle that has prevented consistent approaches to variable order Markov models is
the lack of a hierarchical generative model of Markov proces ses that allows estimating increas-
ingly sparse distributions as its order gets larger. Howeve r, now we have the hierarchical (Poisson-)
Dirichlet process that can be used as a ﬁxed order language mo del [9][10], it is natural for us to
extend these models to variable orders also by using a nonparametric approach. While we concen-
trate here on discrete distributions, the same basic approa ch can be applied to a Markov process on
continuous distributions, such as Gaussians that inherit their means from their parent distributions.
For concreteness below we use a language model example, but the same model can be applied to
any discrete sequences, such as characters, DNAs, or even binary streams for compression.

Consider a trigram language model, which is a second-order M arkov model over words often em-
ployed in speech recognition. Following [9], this Markov model can be represented by a sufﬁx tree
of depth two, as shown in Figure 1(a). When we predict a word “si ng” after a context “she will ”,
we descend this sufﬁx tree from the root (which corresponds t o null string context), using the con-
1 Now we arrive at the leaf node that
text backwards to follow a branch “will ” and then “she will ”.
represents the context, and we can predict “sing” by using th e count distribution at this node.

During the learning phase, we begin with a sufﬁx tree that has no counts. For every time a three word
sequence appears in the training data, such as “she will sing ” mentioned above, we add a count of a
ﬁnal word ( “sing”) given the context ( “she will ”) to the cont
ext node in the sufﬁx tree. In fact this
corresponds to a hierarchical Chinese restaurant process, where each context node is a restaurant
and each count is a customer associated with a word. Here each node, i.e. restaurant, might not
have customers for all the words in the lexicon. Therefore, w hen a customer arrives at a node and
stochastically needs a new table to sit down, a copy of him, namely a proxy customer, is sent to its
parent node. When a node has no customer to compute the probabi lity of some word, it uses the
distribution of customers at the parent node and appropriat ely interpolates it to sum to 1.

Assume that the node “she will ” does not have a customer of “li
ke.” We can nevertheless compute
the probability of “like” given “she will ” if its sibling “he
will ” has a customer “like”. Because that
sibling has sent a copy of the customer to the common parent “w ill ”, the probability is computed by
appropriately interpolating the trigram probability give n “she will ”, which is zero, with the bigram
probability given “will ”, which is not zero at the parent nod e.

1This is the leftmost path in Figure 1(a). When there is no corresponding br anch, we will create it.

1 − qi

i

1 − qj

j

1 − qk

k

(1 − qi ) is a “penetration probability” of a
Figure 2: Probabilistic sufﬁx tree of an inﬁnite depth.
descending customer at each node i, deﬁning a stick-breaking process over the inﬁnite tree.

p(s|h) =

Consequently, in the hierarchical Pitman-Yor language model (HPYLM), the predictive probability
of a symbol s = st in context h = st−n · · · st−1 is recursively computed by
c(s|h)− d · ths
θ + d · th·
θ + c(h)
θ + c(h)
where h′ = st−n+1 · · · st−1 is a shortened context with the farthest symbol dropped. c(s|h) is the
count of s at node h, and c(h) = Ps c(s|h) is the total count at node h. ths is the number of times
symbol s is estimated to be generated from its parent distribution p(s|h′ ) rather than p(s|h) in the
training data: th· = Ps ths is its total. θ and d are the parameters of the Pitman-Yor process, and
can be estimated through the distribution of customers on a sufﬁx tree by Gamma and Beta posterior
distributions, respectively. For details, see [9].

p(s|h′ ),

(1)

+

Although this Bayesian Markov model is very principled and attractive, we can see from Figure 1(a)
that all the real customers (i.e., counts) are ﬁxed at the dep th (n−1) in the sufﬁx tree. Because actual
sequences will have heterogeneous Markov dependencies, we want a Markov model that deploys
customers at different levels in the sufﬁx tree according to the true Markov order from which each
customer originated. But how can we model such a heterogeneous property of Markov sequences?

3

In ﬁnite-order Hierarchical Chinese Restaurant Processe s

Intuitively, we know that sufﬁx trees that are too deep are im probable and symbol dependencies
decay largely exponentially with context lengths. However, some customers may reside in a very
deep node (for example, “the united states of america”) and s ome in a shallow node ( “shorter than”).
Our model for deploying customers must be ﬂexible enough to a ccommodate all these possibilities.

3.1

Introducing Suf ﬁx Tree Prior

For this purpose, we assume that each node i in the sufﬁx tree has a hidden probability qi of stopping
at node i when following a path from the root of the tree to add a customer. In other words, (1 − qi )
is the “penetration probability” when descending an inﬁnit
e depth sufﬁx tree from its root (Figure 2).
We assume that each qi is generated from a prior Beta distribution independently a s:
(2)
i.i.d.
qi ∼ Be(α, β )
This choice is mainly for simplicity: however, later we will show that the ﬁnal predictive perfor-
mance does not signi ﬁcantly depend on α or β .
When we want to generate a symbol st given a context h = s−∞ · · · st−2 st−1 , we descend the sufﬁx
tree from the root following a path st−1 → st−2 → · · · , according to the probability of stopping at a
level l given by
l−1
Y
i=0
When we stop at level l, we generate a symbol st using the context st−l · · ·st−2 st−1 . Since qi differs
from node to node, we may reach very deep nodes with high probability if the qi ’s along the path
are equally small (the “penetration” of this branch is high)
; or, we may stop at a very shallow node
if the qi ’s are very high (the “penetration” is low). In general, the p robability to reach a node decays
exponentially with levels according to (3), but the degrees are different to allow for long sequences
of typical phrases.

(l = 0, 1, · · · , ∞)

p(n = l|h) = ql

(1 − qi ) .

(3)

Note that even for the same context h, the context length that was used to generate the next symbol
may differ stochastically for each appearance according to (3).

3.2

Inference

Of course, we do not know the hidden probability qi possessed by each node. Then, how can
we estimate it? Note that the generative model above amounts to introducing a vector of hidden
variables, n = n1n2 · · · nT , that corresponds to each Markov order (n = 0 · · · ∞) from which each
symbol st in s = s1 s2 · · · sT originated. Therefore, we can write the probability of s as follows:

p(s, z, n) .

p(s) = X
X
n
z
Here, z = z1 z2 · · · zT is a vector that represents the hidden seatings of the proxy customers described
in Section 2, where 0 ≤ zt ≤ nt means how recursively the st ’s proxy customers are stochastically
sent to parent nodes. To estimate these hidden variables n and z, we use a Gibbs sampler as in [9].
Since in the hierarchical (Poisson-)Dirichlet process the customers are exchangeable [9] and qi is
i.i.d. as shown in (2), this process is also exchangeable and therefore we can always assume, by a
suitable permutation, that the customer to resample is the ﬁ nal customer.

(4)

In our case, we only explicitly resample nt given n−t (n excluding nt ), as follows:

nt ∼ p(nt |s, z−t , n−t ).

(5)

Notice here that when we sample nt , we already know the other depths n−t that other words have
reached in the sufﬁx tree. Therefore, when computing (5) usi ng (3), the expectation of each qi is
ai +α
ai + bi +α+β
where ai is the number of times node i was stopped at when generating other words, and bi is
the number of times node i was passed by. Using this estimate, we decompose the conditional
probability of (5) as

E [qi ] =

(6)

,

p(nt |s, z−t , n−t ) ∝ p(st |s−t , z−t , n) p(nt |s−t , z−t , n−t ) .

(7)

The ﬁrst term is the probability of st under HPYLM when the Markov order is known to be nt ,
given by (1). The second term is the prior probability of reac hing that node at depth nt . By using
(6) and (3), this probability is given by

p(nt = l|s−t , z−t , n−t ) =

l−1
Y
i=0
Expression (7) is a tradeoff between these two terms: the prediction of st will be increasingly better
when the context length nt becomes long, but we can select it only when the probability of reaching
that level in the sufﬁx tree is supported by the other counts i n the training data.

bi +β
ai + bi +α+β

al +α
al + bl +α+β

(8)

.

Using these probabilities, we can construct a Gibbs sampler, as shown in Figure 3, to iteratively
resample n and z in order to estimate the parameter of the variable order hier archical Pitman-Yor
language model (VPYLM)2 . In this sampler, we ﬁrst remove the t’th customer who resides at a depth
of order[t] in the sufﬁx tree, and decrement ai or bi accordingly along the path. Sampling a new
depth (i.e. Markov order) according to (7), we put the t’th customer back at the new depth recorded
as order[t], and increment ai or bi accordingly along the new path. When we add a customer st , zt
is implicitly sampled because st ’s proxy customer is recursively sent to parent nodes in case a new
table is needed to sit him down.

1: for j = 1 · · · N do
for t = randperm(1 · · · T ) do
2:
if j > 1 then
3:
remove customer (order[t], st , s1:t−1 )
4:
end if
5:
order[t] = add customer (st , s1:t−1 ) .
6:
end for
7:
8: end for

Figure 3: Gibbs Sampler of VPYLM.

/* n-gram node */

struct ngram {
ngram *parent;
splay *children; /* = (ngram **) */
/* = (restaurant **) */
splay *symbols;
int stop;
/* ah */
/* bh */
int through;
/* c(h) */
int ncounts;
/* th · */
int ntables;
int id;
/* symbol id */

};
Figure 4: Data structure of a sufﬁx tree node.
Counts ah and bh are maintained at each node. We
used Splay Trees for efﬁcient insertion/deletion.

2This is a speci ﬁc application of our model to the hierarchical Pitman-Yor pr ocesses for discrete data.

‘how queershaped little children drawling-desks, which would get through that dormouse!’
said alice; ‘let us all for anything the secondly, but it to have and another question, but i
shalled out, ‘you are old,’ said the you’re trying to far out to sea.
(a) Random walk generation from a character model.
Character
t h e s e c o n d l y ,
f o r a n y t h i n g
‘ l e t u s a l l
a l i c e ;
s a i d
· · ·
Markov order 5 6 5 4 7 10 6 5 4 3 7 1 4 8 2 4 4 6 5 5 4 4 5 5 6 4 5 6 7 7 7 5 3 3 4 5 9 11 6 4 8 9 8 9 4 4 4 7 3 4 3 · · ·
(b) Markov orders used to generate each character above.
Figure 5: Character-based inﬁnite Markov model trained on “ Alice in Wonderland.”

This sampler is an extension of that reported in [9] using stochastically different orders n (n =
0 · · · ∞) for each customer. In practice, we can place some maximum order nmax on n and sample
within it 3 , or use a small threshold ǫ to stop the descent when the prior probability (8) of reaching
that level is smaller than ǫ. In this case, we obtain an “inﬁnite” order Markov model: now
we can
eliminate the order from Markov models by integrating it out .

Because each node in the sufﬁx tree may have a huge number of ch ildren, we used Splay Trees [11]
for the efﬁcient search as in [6]. Splay Trees are self-organ izing binary search trees having amortized
O(log n) order, that automatically put frequent items at shallower nodes. This is ideal for sequences
with a power law property like natural languages. Figure 4 sh ows our data structure of a node in a
sufﬁx tree.

3.3 Prediction

Since we do not usually know the Markov order of a context h = s−∞ · · · s−2 s−1 beforehand, when
making predictions we consider n as a latent variable and average over it, as follows:
p(s|h) = P∞
n=0 p(s, n|h)
= P∞
n=0 p(s|h, n)p(n|h) .
Here, p(s|n, h) is a HPYLM prediction of order n through (1), and p(n|h) is the probability distri-
bution of latent Markov order n possessed by the context h, obtained through (8). In practice, we
further average (10) over the conﬁgurations of n and s through N Gibbs iterations on training data
s, as HPYLM does.

(9)
(10)

Since p(n|h) has a product form as (3), we can also write the above expression recursively by
introducing an auxiliary probability p(s|h, n+ ) as follows:
p(s|h, n+ ) = qn · p(s|h, n) + (1 − qn ) · p(s|h, (n+ 1)+ ) ,
(11)
p(s|h) ≡ p(s|h, 0+ ) .
(12)
This formula shows that qn in fact deﬁnes the stick-breaking process on an inﬁnite tree
, where
breaking proportions will differ branch to branch as oppose d to a single proportion on a unit interval
used in ordinary Dirichlet processes. In practice, we can truncate the inﬁnite recursion in (11) and
rescale it to make p(n|h) a proper distribution.

3.4

“Stochastic Phrases ” on Suf ﬁx Tree

In the expression (9) above, p(s, n|h) is the probability that the symbol s is generated by a Markov
process of order n on h, that is, using the last n symbols of h as a Markov state. This means that a
subsequence s−n · · · s−1 s forms a “phrase”: for example, when “Gaussians ” was generat
ed using a
context “mixture of ”, we can consider “mixture of Gaussians
” as a phrase and assign a probability
to this subsequence, which represents its cohesion strength irrespective of its length.

In other words, instead of emitting a single symbol s at the root node of sufﬁx tree, we can ﬁrst
stochastically descend the tree according to the probabili ty to stop by (3). Finally, we emit s given
the context s−n · · · s−1 , which yields a phrase s−n · · · s−1 s and its cohesion probability. Therefore,
by traversing the sufﬁx tree, we can compute p(s, n|h) for all the subsequences efﬁciently. For
concrete examples, see Figure 8 and 10 in Section 4.
3Notice that by setting (α, β ) = (0, ∞), we always obtain qi = 0: with some maximum order nmax , this
is equivalent to always using the maximum depth, and thus to reducing the model to the original HPYLM. In
this regard, VPYLM is a natural superset that includes HPYLM [9].

n

9

8

7

6

5

4

3

2

1

0

g
n
s
s
n
n
o
r
e
t
i
a
e
d
c
m
i
t
t
t
s
e
r
p
c
e
a
e
i
a
g
n
s
u
s
u
p
t
m
p
g
z
l
k
e
e
u
o
n
t
e
y
o
e
s
S
d
o
m
a
e
s
e
t
e
l
l
p
o
t
e
p
o
t
n
c
i
.
c
y
s
e
i
o
r
o
O
e
a
e
t
f
e
r
l
s
l
e
h
o
l
e
p
n
p
a
u
b
a
t
r
i
a
o
e
i
x
i
k
r
r
r
h
om
h
h
h
f
t
o
.
t
w
s
w
w
E
r
d
u
o
p
p
a
ap
o
p
a
u
a
n
e
s
i
c
c
k
i
t
f
|t
f
t
t
t
t
l
i

Figure 6: Estimated Markov order distributions from which e ach word has been generated.

4 Experiments

To investigate the behavior of the inﬁnite Markov model, we c onducted experiments on character
and word sequences in natural language.

4.1

Inﬁnite character Markov model

Character-based Markov model is widely employed in data compression and has important applica-
tion in language processing, such as OCR and unknown word recognition. In this experiment, we
used a 140,931 characters text of “Alice in Wonderland” and b uilt an inﬁnite Markov model using
uniform Beta prior and truncation threshold ǫ = 0.0001 in Section 3.2.
Perplexity
Max. order
Figure 5(a) is a random walk generation from this inﬁnite mod el.
6.048
n = 3
To generate this, we begin with an inﬁnite sequence of ‘begin ning
3.803
n = 5
of sentence’ special symbols, and sample the next character accord-
3.519
n = 10
ing to the generative model given the already sampled sequence as
3.502
n = ∞
the context. Figure 5(b) is the actual Markov orders used for gen-
Table 1: Perplexity results of
eration by (8). Without any notion of “word”, we can see that o ur
Character models.
model correctly captures it and even higher dependencies be tween
“words ”. In fact, the model contained many nodes that corres pond to valid words as well as the
connective fragments between them. Table 1 shows predictive perplexity4 results on separate test
data. Compared with truncations n = 3, 5 and 10, the inﬁnite model performs the best in all the
variable order options.

4.2 Bayesian ∞-gram Language Model

Data For a word-based “n-gram” model of language, we used a random subset of the standard
NAB Wall Street Journal language modeling corpus [12] 5 , totalling 10,007,108 words (409,246
sentences) for training and 10,000 sentences for testing. Symbols that occurred fewer than 10 times
in total and punctuation (commas, quotation marks etc.) are mapped to special characters, and all
sentences are lowercased, yielding a lexicon of 26,497 words. As HPYLM is shown to converge
very fast [9], according to preliminary experiments we used N = 200 Gibbs iterations for burn-in,
and a further 50 iterations to evaluate the perplexity of the test data.
Results Figure 6 shows the Hinton diagram of estimated Markov order d istributions on part of the
training data, computed according to (7). As for the perplex ity, Table 2 shows the results compared
with the ﬁxed-order HPYLM with the number of nodes in each mod el. n means the ﬁxed order for
HPYLM, and the maximum order nmax in VPYLM. For the “inﬁnite” model of
n = ∞, we used a
threshold ǫ = 10−8 in Section 3.2 for descending the sufﬁx tree.
As empirically found by [12], perplexities will saturate when n becomes large, because only a small
portion of words actually exhibit long-range dependencies . However, we can see that the VPYLM
performance is comparable to that of HPYLM with much fewer nodes and restaurants up to n = 7
and 8, where vanilla HPYLM encounters memory overﬂow caused by a r apid increase in the number
of parameters. In fact, the inference of VPYLM is about 20% faster than that of HPYLM of the

4Perplexity is a reciprocal of average predictive probabilities, thus smalle r is better.
5We also conducted experiments on standard corpora of Chinese (char acter-wise) and Japanese, and ob-
tained the same line of results presented in this paper.

n HPYLM VPYLM Nodes(H) Nodes(V)
1,344K
1,417K
113.74
113.60
3
7,466K
12,699K
101.69
101.08
5
27,193K 10,182K
100.68
N/A
7
100.58
34,459K 10,434K
N/A
8
10,629K
100.36
∞

s
e
c
n
e
r
r
u
c
c
O

3.5×106
3.0×106
2.5×106
2.0×106
1.5×106
1.0×106
5.0×105
0.0×100

Table 2: Perplexity Results of VPYLM and
HPYLM on the NAB corpus with the number
of nodes in each model. N/A means a memory
overﬂow caused by the expected number of nodes
shown in italic.

 0  1  2  3  4  5  6  7  8  9  10  11  12
n
Figure 7: Global distribution of sampled
Markov orders on the ∞-gram VPYLM over
the NAB corpus. n = 0 is unigram, n = 1 is
bigram,· · · .

same order despite the additional cost of sampling n-gram orders, because it appropriately avoids
the addition of unnecessarily deep nodes on the sufﬁx tree. T he perplexity at n = ∞ is the lowest
compared to all ﬁxed truncations, and contains only necessa ry number of nodes in the model.

Figure 7 shows a global n-gram order distribution from a sing le posterior sample of Gibbs iteration in
∞-gram VPYLM. Note that since we added an inﬁnite number of dum my symbols to the sentence
heads as usual, every word context has a maximum possible length of ∞. We can see from this
ﬁgure that the context lengths that were actually used decay largely exponentially, as intuitively
expected. Because of the tradeoff between using a longer, more predictive context and the penalty
incurred when reaching a deeper node, interestingly a peak emerges around n = 3 ∼ 4 as a global
phenomenon.

With regard to the hyperparameter that deﬁnes the prior form s of sufﬁx trees, we used a
(4, 1)-
prior in this experiment.
In fact, this hyperparameter can b e optimized by the empirical Bayes
method using each Beta posterior of qi in (6). By using the Newton-Raphson iteration of [13], this
converged to (0.85, 0.57) on a 1 million word subset of the NAB corpus. However, we can see that
the performance does not depend signi ﬁcantly on the prior. F igure 9 shows perplexity results for the
same data, using (α, β ) ∈ (0.1 ∼ 10)× (0.1 ∼ 10). We can see from this ﬁgure that the performance
is almost stable, except when β is signi ﬁcantly greater than α. Finally, we show in Figure 8 some
“stochastic pharases ” in Section 3.4 induced on the NAB corp us.

4.3 Variable Order Topic Model

While previous approaches to latent topic modeling assumed a ﬁxed order such as unigrams or
bigrams, the order is generally not ﬁxed and unknown to us. Th erefore, we used a Gibbs sampler
for the Markov chain LDA [14] and augmented it by sampling Markov orders at the same time.

Because “topic-speci ﬁc” sequences constitute only some pa
rt of the entire data, we assumed that the
“generic” model generated the document according to probab ility λ, and the rest are generated by
the LDA of VPYLM. We endow λ a uniform Beta prior and used the posterior estimate for samp ling
that will differ document to document.

For the experiment, we used the NIPS papers dataset of 1739 documents. Among them, we used
random 1500 documents for training and random 50 documents f rom the rest of 239 documents
for testing, after the same preprocessing for the NAB corpus . We set a symmetric Dirichlet prior

Stochastic phrases in the sufﬁx tree
primary new issues
ˆ at the same time
is a unit of
from # % in # to # %
in a number of
in new york stock exchange composite trading
mechanism of the european monetary
increase as a result of
tiffany & co.

p(s, n)
0.9784
0.9726
0.9512
0.9026
0.8896
0.8831
0.7566
0.7134
0.6617
:
Figure 8:
“Stochastic phrases ” induced by the 8-gram
VPYLM trained on the NAB corpus.

PPL
PPL
 136
 134
 132
 130
 128
 126
 124
0.1

0.5

 134
 132
 130
 128
 126
 124
 122

105

210.5

β

5

0.1

1
α

2

10
Figure 9: Perplexity results using dif-
ferent hyperparameters on the 1M NAB
corpus.

—
—
p(n, s) Phrase
0.9904 in section #
0.9900 the number of
0.9856 in order to
0.9832 in table #
0.9752 dealing with
0.9693 with respect to
(a) Topic 0 ( “generic”)

p(n, s) Phrase
0.9853 et al
0.9840 receptive ﬁeld
0.9630 excitatory and inhibitory
0.9266 in order to
0.8939 primary visual cortex
0.8756 corresponds to
(b) Topic 1
Figure 10: Topic based stochastic pharases.
γ = 0.1 and the number of topics M = 5, nmax = 5 and ran a N = 200 Gibbs iterations to obtain
a single posterior set of models.

p(n, s) Phrase
0.9823 monte carlo
0.9524 associative memory
0.9081 as can be seen
0.8206 parzen windows
0.8044 in the previous section
0.7790 american institute of physics
(c) Topic 4

predictive
Although
(VPYLDA=116.62,
slight
are
improvements
the
perplexity
in
“stochastic pharases ” computed on each top ic VPYLM show interesting
VPYLM=117.28),
characteristics shown in Figure 10. Although we used a small number of latent topics in this
experiment to avoid data sparsenesses, in future research w e need a more ﬂexible topic model where
the number of latent topics will differ from node to node in the sufﬁx tree.

5 Discussion and Conclusion
In this paper, we presented a completely generative approach to estimating variable order Markov
processes. By extending a stick-breaking process “vertica lly” over a sufﬁx tree of hierarchical Chi-
nese restaurant processes, we can make a posterior inferenc e on the Markov orders from which each
data originates.

Although our architecture looks similar to Polya Trees [15] , in Polya Trees their recursive partitions
are independent while our stick-breakings are hierarchica lly organized according to the sufﬁx tree.
In addition to apparent application of our approach to hiera rchical continuous distributions like
Gaussians, we expect that the basic model can be used for the distribution of latent variables. Each
data is assigned to a deeper level just when needed, and resides not only in leaf nodes but also in the
intermediate nodes, by stochastically descending a clustering hierarchy from the root as described
in this paper.
References
[1] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27:379–423,
623–656, 1948.
[2] Alberto Apostolico and Gill Bejerano. Optimal amnesic probabilistic automata, or, how to learn and
classify proteins in linear time and space. Journal of Computational Biology, 7:381–393, 2000.
[3] F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. The Context-Tree Weighting Method: Basic Properties.
IEEE Trans. on Information Theory, 41:653–664, 1995.
[4] Frederick Jelinek. Statistical Methods for Speech Recognition. Language, Speech, and Communication
Series. MIT Press, 1998.
[5] Peter Buhlmann and Abraham J. Wyner. Variable Length Markov Chains. The Annals of Statistics,
27(2):480–513, 1999.
[6] Fernando Pereira, Yoram Singer, and Naftali Tishby. Beyond Wo rd N-grams.
Workshop on Very Large Corpora, pages 95–106, 1995.
[7] Dana Ron, Yoram Singer, and Naftali Tishby. The Power of Amnesia. In Advances in Neural Information
Processing Systems, volume 6, pages 176–183, 1994.
[8] Andreas Stolcke. Entropy-based Pruning of Backoff Language Models. In Proc. of DARPA Broadcast
News Transcription and Understanding Workshop, pages 270–274, 1998.
[9] Yee Whye Teh. A Bayesian Interpretation of Interpolated Kneser-Ney. Technical Report TRA2/06,
School of Computing, NUS, 2006.
[10] Sharon Goldwater, Thomas L. Grifﬁths, and Mark Johnson. Inte rpolating Between Types and Tokens by
Estimating Power-Law Generators. In NIPS 2005, 2005.
[11] Daniel Sleator and Robert Tarjan. Self-Adjusting Binary Search Trees. JACM, 32(3):652–686, 1985.
[12] Joshua T. Goodman. A Bit of Progress in Language Modeling, Extended Version. Technical Report
MSR –TR –2001–72, Microsoft Research, 2001.
[13] Thomas P. Minka. Estimating a Dirichlet distribution, 2000. http://research.microsoft.com/˜minka/papers/
dirichlet/.
[14] Mark Girolami and Ata Kab ´an. Simplicial Mixtures of Markov Chains: Distributed Modelling of Dy-
namic User Proﬁles. In NIPS 2003. 2003.
[15] R. Daniel Mauldin, William D. Sudderth, and S. C. Williams. Polya Trees and Random Distributions.
Annals of Statistics, 20(3):1203–1221, 1992.

In Proc. of the Third


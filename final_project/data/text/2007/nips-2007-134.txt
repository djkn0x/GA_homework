The Generalized FITC Approximation

Andrew Naish-Guzman & Sean Holden
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD. United Kingdom
{agpn2,sbh11}@cl.cam.ac.uk

Abstract

We present an efﬁcient generalization of the sparse pseudo- input Gaussian pro-
cess (SPGP) model developed by Snelson and Ghahramani [1], applying it to
binary classiﬁcation problems. By taking advantage of the S PGP prior covari-
ance structure, we derive a numerically stable algorithm with O(N M 2 ) training
complexity —asymptotically the same as related sparse meth
ods such as the in-
formative vector machine [2], but which more faithfully represents the posterior.
We present experimental results for several benchmark problems showing that
in many cases this allows an exceptional degree of sparsity without compromis-
ing accuracy. Following [1], we locate pseudo-inputs by gradient ascent on the
marginal likelihood, but exhibit occasions when this is likely to fail, for which we
suggest alternative solutions.

1 Introduction

Gaussian processes are a ﬂexible and popular approach to non -parametric modelling. Their con-
ceptually simple architecture is allied with a sound Bayesian foundation, so that not only does their
predictive power rival state-of-the-art discriminative methods such as the support vector machine,
but they also have the additional bene ﬁt of providing an esti mate of variance, giving an error bar for
their prediction. However, there is a computational price to pay for this robust framework: the time
for training scales as N 3 for N data points, and the cost of prediction is O(N 2 ) per test case.
Recently, there has been great interest in ﬁnding sparse app roximations to the full Gaussian process
(GP) in order to accelerate training and prediction times respectively to O(N M 2 ) and O(M 2 ),
where M ≪ N is the size of an auxiliary set, often a subset of the training data, termed variously
the inducing inputs, pseudo-inputs or the active set [3, 4, 5, 2, 6, 7, 1]; in this paper, we use the
terms interchangeably. Qui ˜nonero-Candela and Rasmussen [8] demonstrated how many of these
schemes are related through different approximations to the joint prior over training and test points.
In this paper we consider the “fully independent training co nditional” or FITC approximation, which
appeared originally in Snelson and Ghahramani [1] as the sparse pseudo-input GP (SPGP).

Restricted to a Gaussian noise model, the FITC approximation is entirely tractable; however, for
many problems, the Gaussian assumption is inappropriate. In this paper, we describe an extension
for non-Gaussian likelihoods, considering as an example probit noise for binary classiﬁcation. This
is not only a common problem, but our results bear out the intuition that sparse methods are well-
suited: many data sets enjoy the property that class label does not ﬂuctuate rapidly in the input space,
often allowing large regions to be summarized with very few inducing inputs. Contrast this with
regression problems, where higher frequency components in the latent signal demand the pseudo-
inputs appear in much higher density.

The informative vector machine (IVM) of Lawrence et al. [2] is another sparse GP method that has
been extended to non-Gaussian noise models. It is a subset of data method in which the active set

1

is grown incrementally from the training data using a fast information gain heuristic to ﬁnd at each
stage the optimal inclusion. When a threshold number of points have been added, the algorithm
terminates: only data accumulated into the active set are relevant for prediction; remaining points
in ﬂuence the model only in the weak sense of guiding previous steps of the algorithm. Our method is
an improvement in three regards: ﬁrstly, the FITC approxima tion makes use of all the data, yielding
for the same active set a closer approximation to the posterior distribution. Secondly, unlike the
standard IVM approach, we ﬁt a stable posterior at each itera tion, providing more accurate marginal
likelihood estimates, and derivatives thereof, to allow more reliable model selection. Finally, we
argue with experimental justiﬁcation that the ability to lo cate inducing inputs independently of the
training data, as compared with the greedy approach that drives the IVM, can be a great advantage
in ﬁnding the sparsest solutions. We discuss these points an d other related work in greater detail in
section 6.

The structure of this paper is as follows: in section 2 we describe the FITC approximation; this is
followed in section 3 by a detailed description of its representation for a non-Gaussian noise model;
section 4 provides a brief account of the procedure for model selection; experimental results appear
in section 5, which we discuss in section 6; our concluding remarks are in section 7.

2 The FITC approximation

Given a domain X and covariance function K (·, ·) ∈ X × X → R, a Gaussian process (GP) over
the space of real-valued functions of X speciﬁes the joint distribution at any ﬁnite set X ⊂ X :

p(f |X) = N (f ; 0 , Kﬀ ) ,

n=1 are (latent) values associated with each xn ∈ X, and Kﬀ is the Gram
where the f = {fn}N
matrix, the evaluation of the covariance function at all pairs (xi , xj ). We apply Bayes’ rule to obtain
the posterior distribution over the f , given the observed X and y, which with the assumption of
i.i.d. Gaussian corrupted observations is also normally distributed. Predictions at X⋆ are made by
marginalizing over f in the (Gaussian) joint p(f , f⋆ |X, y, X⋆ ). See [9] for a thorough introduction.
In order to derive the FITC approximation, we follow [8] and introduce a set of M inducing inputs
¯X = { ¯x1 , ¯x2 , . . . , ¯xM } with associated latent values u. By the consistency of GPs, we have
p(f , f⋆ |X, X⋆ , ¯X) = Z p(f , f⋆ |u, X, X⋆ )p(u| ¯X)du ≈ Z q(f |u, X)q(f⋆ |u, ¯X)p(u| ¯X)du,
where p(u| ¯X) = N (u ; 0 , Kuu ). In the ﬁnal expression we make the critical approximation b y
imposing a conditional independence assumption on the joint prior over training and test cases:
communication between them must pass through the bottleneck of the inducing inputs. The FITC
approximation follows by letting
q(f |u, X) = N (cid:0)f ; KfuK−1
uuu , diag (Kﬀ − Qﬀ )(cid:1) ,
q(f⋆ |u, X⋆ ) = N (cid:0)f⋆ ; K⋆uK−1
uuu , diag (K⋆⋆ − Q⋆⋆ )(cid:1) ,
.
uuKub . Of interest for predictions is the posterior distribution over the induc-
where Qab
= KauK−1
ing inputs; this is most efﬁciently obtained via Bayes’ rule after inferring the distribution over f .1
Using (1) and marginalizing over the exact prior on u we obtain the approximate prior on f
q(f |X) = Z N (cid:0)f ; KfuK−1
uuu , diag (Kﬀ − Qﬀ )(cid:1) N (u ; 0 , Kuu ) du
= N (f ; 0 , Qﬀ + diag (Kﬀ − Qﬀ )) .
In the original paper, Snelson and Ghahramani placed the pseudo-inputs randomly and learned their
locations by non-linear optimization of the marginal likelihood. We have adopted the idea in this
paper, but as emphasized in [8], the FITC approximation is applicable regardless of how the inducing

(1)
(2)

(3)

1We could also infer the posterior over u directly, rather than marginalizing over the inducing inputs as here.
Running EP in this setting, each site maintains a belief about the full M ×M covariance, and we obtain a slower
O(N M 3 ) algorithm. Furthermore, calculations to evaluate the derivatives of the log marginal likelihood with
respect to inducing inputs ¯xm are signi ﬁcantly complicated by their presence in both prio r and likelihood.

2

inputs are obtained, and other schemes for their initialization could equally well be married with our
algorithm.
In the case of classiﬁcation, a sigmoidal function assigns c lass labels yn ∈ {±1} with a probability
that increases monotonically with the latent fn . We use the probit with bias β ,
= Z yn (fn+β )
.
−∞
The posterior distribution p(f |X, y) is only tractable for Gaussian likelihoods, hence we must resort
to a further approximation, either by generating Monte Carlo samples from it or ﬁtting deterministi-
cally a Gaussian approximation. Of the latter methods, expectation propagation is possibly the most
accurate (at least for GP classiﬁcation; see [10]), and it is
the approach we follow below.

p(yn |fn , β ) = σ(yn (fn + β ))

N (z ; 0 , 1) dz .

(4)

3 Inference

We begin with a very brief account of expectation propagation (EP); for more details, see [11,
12]. Suppose we have an intractable distribution over f whose unnormalized form factorizes into
a product of terms, such as a dense Gaussian prior t0 (f ) and a series of independent likelihoods
n=1 . EP constructs the approximate posterior as a product of scaled site functions ˜tn .
{tn (yn |fn )}N
For computational tractability, these sites are usually chosen from an exponential family with natural
parameters θ , since in this case their product retains the same functional form as its components.
The Gaussian (µ, Σ) has a natural parameterization (b, Π) = (Σ−1µ, − 1
2 Σ−1 ). If the prior is of
this form, its site function is exact:

p(f |y) =

t0 (f )

(5)

1
Z

zn ˜tn (fn ; θn ),

tn (yn |fn ) ≈ q(f ; θ ) = t0 (f )

N
N
Yn=1
Yn=1
where Z is the marginal likelihood and zn are the scale parameters. Ideally, we would choose θ at
the global minimum of some divergence measure d(pkq), but the necessary optimization is usually
intractable. EP is an iterative procedure that ﬁnds a minimi zer of KL(cid:0)p(f |y)kq(f ; θ )(cid:1) on a pointwise
basis: at each iteration, we select a new site n, and from the product of the cavity distribution formed
by the current marginal with the omission of that site, and the true likelihood term tn , we obtain the
so-called tilted distribution qn (fn ; θ\n ). A simpler optimization minθn KL (cid:0)qn (fn ; θ\n )kq(fn ; θ )(cid:1)
then ﬁts only the parameters θn : this is equivalent to moment matching between the two distributions,
with scale zn chosen to match the zeroth-order moments. After each site update, the moments at the
remaining sites are liable to change, and several iterations may be required before convergence.

In the discussion below we omit the moment calculations for the probit model, since they correspond
to those of traditional GP classiﬁcation (for more details, consult [9]). Of greater interest is how the
mean and covariance structure of the approximate posterior is preserved. Examining the form of the
prior (3), we see the covariance consists of a diagonal component D0 and a rank-M term P0M0PT
0 ,
where P0 = Kfu and M0 = K−1
uu (zero subscripts refer to these initial values; the matrices are
updated during the course of the EP iterations). Since the observations yn are generated i.i.d., we
can expect this decomposition to persist in the posterior.
EP requires efﬁcient operations for marginalization to obt ain p(fn ), and for updating the posterior
distribution after re ﬁning a site, as well as for refreshing the posterior to avoid loss of numerical
precision. Decomposing M = RT R into its Cholesky factor,2 we represent the posterior covariance
A and mean h by

A = D + PRT RPT ,

h = ν + Pγ ,

2Care must be taken that the factors share the correct orientation. When our environment offers only upper
Cholesky factors RT R, the initialization of R0 = chol `K−1
uu ´ can be achieved without computing the explicit
inverse via the following matrix rotations:
R0 := rot180 “chol`rot180 (Kuu ) ´T

\ I” .

3

eeT

and

eeT ,

E−1 = D −

(DED)−1 = D−1 −

n = P(n,·) and dn = Dnn ,
where D is diagonal, ν is N × 1 and γ is M × 1. Writing pT
obtaining marginals in O(M 2 ).
hn = νn + pT
Ann = dn + kRpnk
n γ ,
Now consider a change in the precision at site n by πn . De ﬁne the vector e of length N such that
en = 1 and all other elements are zero. The new covariance Anew is obtained by inverting the sum
of the old precision matrix and the change in precision. If we let E = D−1 + πn eeT , so that
πn d2
πn
n
1 + πn dn
1 + πndn
then from the matrix inversion lemma, A−1 = D−1 −D−1PRT (RPT D−1PRT +I)−1RPT D−1 ,
and incorporating the update to site n,
Anew = E−1 − E−1D−1PRT (cid:16)RPT (DED)−1PRT − I − RPT D−1PRT (cid:17)−1
= Dnew + PnewRT
newRnewPT
new ,
where we expand the inversion to obtain a rank-1 downdate to the Cholesky factor R;3 in summary
πnd2
πndn
n
eeT O(1) update,
epT
n O(M ) update,
Pnew = P −
Dnew = D −
1 + πn dn
1 + πn dn
Rnew = chol↓ (cid:18)RT (cid:18)I − Rpn
n RT (cid:19) R(cid:19) O(M 2 ) update.
πn
pT
1 + πnAnn
If the second site parameter, corresponding to precision times mean, is changed by bn , then
=⇒ hnew = Anew (cid:0)A−1
A−1
newhnew = A−1h + bne
new − πneeT (cid:1) h + Anew bne
= νnew + Pnewγnew ,

RPT D−1E−1

where

νnew = ν +

RT
newRnewpn

e (O(1)), γnew = γ +

bn − πn hn
(bn + πnνn )dn
(cid:0)O(M 2 )(cid:1).
1 + πndn
1 + πn dn
It is necessary to refresh the covariance and mean every complete EP cycle to avoid loss of precision.
Dnew = (I + D0Π)−1 D0
Pnew = (I + D0Π)−1 P0
(O(N M )),
(O(N )),
0 (cid:1)(cid:17)T (cid:19) /R0
Rnew = rot180 (cid:18)chol (cid:16)rot180(cid:0)I + R0PT
0 Π (I + D0Π)−1 P0RT
(cid:0)O(N M 2 )(cid:1),
where Rnew is obtained being careful to ensure the orientations of the factorizations are not mixed.
Finally, the mean is refreshed using

νnew = Dnewb in O(N ),
where we have assumed h0 = 0.
Reviewing the algorithm above, we see that EP costs are dominated by the O(M 2 ) Cholesky down-
date at each site inclusion. After visiting each of the N sites, we are advised to perform a full refresh,
which is O(N M 2 ), together leading to asymptotic complexity of O(N M 2 ).

newRnewPT
γnew = RT
newb in O(N M ),

3.1 Predictions

To make predictions, we marginalize out u from (2). Initially, Bayes’ theorem is used to ﬁnd the
posterior distribution over u from the inferred posterior over f :
0 CR−T
0 c, R−1
p(u|f ) ∝ p(f |u)p(u) = N (u | R−1
),
0
0 D−1
0 D−1
and C−1 = I + R0PT
where c = CR0PT
0 P0RT
0 .
0 f

3 If the factor
is negative, we make a rank-1 update, guaranteed to preserve the positive deﬁnite
πn
1+πnAnn
property. Note that on rare occasions, loss of precision can cause a downdate to result in a non-positive deﬁnite
covariance matrix. If this occurs, we should abort the update and refresh the posterior from scratch. In any
case, to improve conditioning, it is recommended to add a small multiple of the identity to the prior M0 .

4

),

Let our posterior approximation be q(f |y) = N (f ; h , A). Hence
p(u|y) ≈ Z p(u|f )q(f |y)df = N (u | R−1
0 ΣR−T
0 µ, R−1
0
0 AD−1
0 D−1
0 D−1
0 P0RT
where µ = CR0PT
0 h and Σ = C + CR0PT
0 C.
Obtaining these terms is O(N M 2 ) if we take advantage of the structure of A; the most stable
method is via the Cholesky factorization of C−1 , rather than forming the explicit inverse. At x⋆ ,
p(f⋆ |x⋆ , y) = Z p(f⋆ |u)p(u|y)du = N (f⋆ | µ⋆ , σ2
⋆ );
after precomputations, µ⋆ = kT
0 µ is O(M ), and σ2
0 (Σ − I) R0k⋆ is O(M 2 ).
⋆ RT
⋆ = k⋆⋆ + kT
⋆ RT
In the classiﬁcation domain, we will usually be interested i n
p(y⋆ |x⋆ , y) = Z p(y⋆ |f⋆)p(f⋆ |x⋆ , y)df⋆ = σ   y⋆µ⋆
⋆ ! .
p1 + σ2
4 Model selection

EP provides an estimate of the log evidence by matching the 0th-order moments zn at each inclusion.
When our posterior approximation is exponential family, Seeger [12] shows the estimate to be

L =

log Cn = log zn − Φ(θ post ) + Φ(θ\n ),

log Cn + Φ(θ post) − Φ(θ prior ), where

N
Xn=1
where Φ(·) denotes the log partition function and θ are again the natural parameters, with super-
scripts indicating prior, posterior and cavity. Of interest for model selection are derivatives of the
marginal likelihood with respect to hyperparameters {ξ , ¯X, β}, respectively the kernel parameters,
pseudo-input locations, and noise model parameters. When the EP ﬁxed point conditions hold (that
is, the moments of the tilted distributions match the marginals up to second order for all sites),
∇θ prior L = η post − η prior
where η denotes the moment parameters of the exponential family (for the Gaussian, these are
(µ, Σ + µµT )) and βn is a parameter of site n (and does not feature in the prior). Finally, we need
derivatives ∇ξ θ prior and ∇ ¯Xθ prior . The long-winded details are omitted, but by careful consideration
of the covariance structure, it is again possible to limit the complexity to O(N M 2 ).
Since we run EP until convergence, our estimates for the marginal likelihood and its derivatives are
accurate, allowing us reliablty to ﬁt a model that maximizes
the evidence. This is in contrast to the
IVM, in which sites excluded from the active set have parameters clamped to zero, and where those
included are not iterated to convergence, such that the necessary ﬁxed point conditions do not hold.
A particular problem, suffered also by the similar algorithm in [13], is that derivative calculations
must be interleaved with site inclusions, and the latter operation tends to disrupt gradient information
gained from the previous step. These complications are all sidestepped in our SPGP implementation.

and

∇βn L = log zn ,

5 Experiments

We conducted tests on a variety of data, including two small sets from [14]4 and the benchmark
suite of R ¨atsch.5 The dimensionality of these classiﬁcation problems ranges
from two to sixty, and
the size of the training sets is of the order of 400 to 1000. Results are presented in table 1. For
crabs and the R ¨atsch sets, we average over ten folds of the data; for the synth problem, Ripley has
already divided the data into training and test partitions. Comparisons are made with the full GP
classiﬁer, and the SVM, a widely-used discriminative model which in practice is found to yield
relatively sparse solutions; we consider also the IVM, a popular framework for building sparse

4Available from http://www.stats.ox.ac.uk/pub/PRNN/.
5Available from http://ida.first.fhg.de/projects/bench/benchmarks.htm.

5

Table 1: Test errors and predictive accuracy (smaller is better) for the GP classiﬁer, the support
vector machine, the informative vector machine, and the sparse pseudo-input GP classiﬁer.

Data set
train:test dim

GPC
nlp
err

name

250:1000 2
synth
80:120
crabs
5
400:4900 2
banana
breast-cancer 200:77
9
468:300
diabetes
8
666:400
ﬂare-solar
9
700:300
german
20
170:100
heart
13
1300:1010 18
image
400:7000 20
ringnorm
1000:2175 60
splice
140:75
thyroid
5
150:2051 3
titanic
400:7000 20
twonorm
400:4600 21
waveform

0.097 0.227
0.039 0.096
0.105 0.237
0.288 0.558
0.231 0.475
0.346 0.570
0.230 0.482
0.178 0.423
0.027 0.078
0.016 0.071
0.115 0.281
0.043 0.093
0.221 0.514
0.031 0.085
0.100 0.229

SVM
#sv
err

0.098 98
0.168 67
0.106 151
0.277 122
0.226 271
0.331 556
0.247 461
0.166 92
0.040 462
0.016 157
0.102 698
0.056 61
0.223 118
0.027 220
0.107 148

IVM
nlp M

err

0.096 0.235 150
0.066 0.134 60
0.105 0.242 200
0.307 0.691 120
0.230 0.486 400
0.340 0.628 550
0.290 0.658 450
0.203 0.455 120
0.028 0.082 400
0.016 0.101 100
0.225 0.403 700
0.041 0.120 40
0.242 0.578 100
0.031 0.085 300
0.100 0.232 250

SPGPC
err
nlp M
0.087 0.234
4
0.043 0.105 10
0.107 0.261 20
2
0.281 0.557
2
0.230 0.485
3
0.338 0.569
0.236 0.491
4
0.172 0.414
2
0.031 0.087 200
0.014 0.089
2
0.126 0.306 200
0.037 0.128
6
2
0.231 0.520
0.026 0.086
2
0.099 0.228 10

linear models. In all cases, we employed the isotropic squared exponential kernel, avoiding here the
anisotropic version primarily to allow comparison with the SVM: lacking a probabilistic foundation,
its kernel parameters and regularization constant must be set by cross-validation. For the IVM,
hyperparameter optimization is interleaved with active set selection as described in [2], while for the
other GP models, we ﬁt hyperparameters by gradient ascent on the estimated marginal likelihood,
limiting the process to twenty conjugate gradient iterations; we retained for testing that of three
to ﬁve randomly initialized models which the evidence most f avoured. Results on the R ¨atsch data
for the semi-parametric radial basis function network are omitted for lack of space, but available at
the site given in footnote 5. In comparison with that model, SPGP tends to give sparser and more
accurate results (with the bene ﬁt of a sound Bayesian framew ork).

Identical tests were run for a range of active set sizes on the IVM and SPGP classiﬁer, and we have
attempted to present the large body of results in its most comprehensible form: we list only the
sparsest competitive solution obtained. This means that using M smaller than shown tends to cause
a deterioriation in performance, but not that there is no advantage in increasing the value. After all,
as M → N we expect error rates to match those of the full model (at least for the IVM, which
uses a subset of the training data).6 However, we believe that in exploring the behaviour of a sparse
model, the essential question is: what is the greatest sparsity we can achieve without compromising
performance? (since if sparsity were not an issue, we would simply revert to the original GP).
Small values of M for the FITC approximation were found to give remarkably low error rates, and
incremented singly would often give an improved approximation. In contrast, the IVM predictions
were no better than random guesses for even moderate M —it usually failed if the active set was
smaller than a threshold around N/3, where it was simply discarding too much information —and
greater step sizes were required for noticeable improvements in performance. With a few exceptions
then, for FITC we explored small M , while for the IVM we used larger values, more widely spread.
More challenging is the task of discriminating 4s from non-4s in the USPS digit database: the data
are 256-dimensional, and there are 7291 training and 2007 test points. With 200 pseudo-inputs (and
51,200 parameters for optimization), error rates for SPGPC are 1.94%, with an average negative log
probability of 0.051 nats. These ﬁgures improve when the all ocation is raised to 400 pseudo-inputs,
to 1.79% and 0.048 nats. When provided with only 200 points, the IVM ﬁgures are 9.97% and 0.421
nats—this can be regarded as a failure to generalize, since i
t corresponds to labelling all test inputs
s of 1.54% and NLP of 0.085 nats.
—but given an active set of 400 it reaches error rate
as “not 4 ”

6Note that the evidence is a poor metric for choosing M since it tends to increase monotonically as the
explicative power of the full GP is restored.

6

6 Discussion

A sparse approximation closely related to FITC is the “deter ministic training conditional” (DTC),
whose covariance consists solely of the low-rank term LMLT ; it has appeared elsewhere under
the name projected latent variables [13]. In generative terms, DTC ﬁrst obtains a posterior proc ess
by conditioning on the inducing inputs; observations y are then drawn as noisy samples of the
mean of this process. FITC is similar, but the draws are noisy samples from the posterior process
itself —hence, while the noise component for DTC is a constan
t corruption σ2 , for FITC it grows
away from the inducing inputs to Knn + σ2 . In comparing their SPGP model with DTC, Snelson and
Ghahramani [1] suggest that it is for this reason (i.e. due to the diagonal component in the covariance
in FITC) that the optimization of pseudo-inputs by gradient ascent on the marginal likelihood can
succeed: without the noise reduction afforded locally by relocating pseudo-inputs, DTC does not
provide a sufﬁciently large gradient for them to move, and th e optimization gets stuck. We believe
the same mechanism operates in general for non-Gaussian noise.

This difﬁculty would not be signiﬁcant if alternative heuri
stics for building the active set greedily
were effective. We hypothesize however that the most informative vectors in the greedy sense of
the IVM tend to be those which lie close to the decision boundary. Such points will have a rela-
tively strong in ﬂuence on its shape since the effect of the ke rnel falls off exponentially in distance
squared. A preferable solution may be that empirically found to occur with Tipping’s relevance
vector machine (RVM) [15], a degenerate GP where a particular prior on weights means only a few
basis functions survive an evidence maximization procedure to form the model;7 there, the classi-
ﬁer was often parameterized by points distant from the decis ion boundary, suggested to be more
“representative ” of the data.

We illustrate with a simple example that, provided the optimization is feasible, very sparse solutions
may more easily be found if the inducing inputs can be positioned independently of the data. This
allows the size of the active set to grow with the complexity of the problem, rather than with N , the
number of training points. We drew samples from a two-dimensional “xor ” problem, consisting of
four unit-variance Gaussian clusters at (±1.5, ±1.5) with a small overlap, giving an optimal error
rate of around 13% and in loose terms a complexity which requires an active set of size four. By
increasing the size of the training set N in increments from 40 to 400, we obtained the learning
curves of ﬁgure 1 for the IVM and FITC models: plotted against N is the size of active set required
for the error rate to fall below 15%. Whereas the FITC model requires a constant four points to
explain the data, the demands of the IVM appear to increase almost linearly with N .
Evidently, the FITC model is able to capture salient details more readily than the IVM, but we
may object that it is also a richer likelihood. We therefore show learning curves for the FITC
approximation run using the IVM active set and, generously, optimal kernel parameters. With a
relatively simple and low-dimensional problem, the bene ﬁt of the adaptable active set that FITC
offers is clearly less signiﬁcant than that of the improved a pproximation itself —although there is
a factor of 2 difference, and we believe the effects will be more pronounced for more complex
data. However, a sensible compromise where optimization of all pseudo-inputs is computationally
infeasible is to run the IVM to obtain an initial active set, but then switch to the FITC approximation
and optimize only kernel parameters, or just a small selection of the pseudo-inputs. Another option,
explored by Snelson and Ghaharamani [17] for this model in the case of regression, is to learn a
low dimensional projection of the data —advantageous, sinc
e in this setting the pseudo-inputs only
operate under projection and can be treated as low-dimensional, potentially reducing signiﬁcantly
the scale of the optimization problem. We report results of this extension in future work.

7 Conclusions

We have presented an efﬁcient and numerically stable way of i mplementing the sparse FITC model
in Gaussian processes. By way of example we considered binary classiﬁcation in which extra data
points are introduced to form a continuously adaptable active set. We have demonstrated that the
locations of these pseudo-inputs can be ﬁt synchronously wi th parameters of the kernel, and that

7We have not compared our model with the RVM since that approximation suffers from nonsensical variance
estimates away from the data. Rasmussen and Qui ˜nonero-Can dela [16] show how it can be “healed” through
augmentation, but the resulting model is no longer sparse in the sense of providing O(M 2 ) predictions.

7

M
t
e
s

e
v
i
t
c
a

f
o

e
z
i
S

150

100

50

8
4
0

FITC
IVM
IVM/FITC

2

0

-2

40

200
Size of training set N

360

-2

0

2

Figure 1: Left: learning curves for the toy problem described in the text. Right: contours of posterior
probability for FITC in ten CG iterations from a random initialization of pseudo-inputs (black dots).

this procedure allows for very sparse solutions. Certain data sets, particularly those of very high
dimensionality, are not amenable to this approach since the number of hyperparameters is unfeasibly
large for non-linear optimization. In this case, we suggest resorting to a greedy approach, using a
fast heuristic like the IVM to build the active set, but adopting the FITC approximation thereafter.
An alternative which deserves investigation is to attempt an initial round of k-means clustering.

References
[1] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances
in Neural Information Processing Systems 18. MIT Press, 2005.
[2] Neil Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: the informa-
tive vector machine. In Advances in Neural Information Processing Systems 15. MIT Press, 2003.
[3] Manfred Opper and Ole Winther. Gaussian processes for classi ﬁcation: mean ﬁeld methods.
Neural
Computation, 12(11):2655–2684, 2000.
[4] Volker Tresp. A Bayesian committee machine. Neural Computation, 12(11):2719–2741, 2000.
[5] Alex Smola and Peter Bartlett. Sparse greedy Gaussian process regression.
In Advances in Neural
Information Processing Systems 13. MIT Press, 2001.
[6] Lehel Csat ´o. Gaussian processes: iterative sparse approximations. PhD thesis, Aston University, 2002.
[7] Matthias Seeger. Bayesian Gaussian process models: PAC-Bayesian generalisation error bounds and
sparse approximations. PhD thesis, University of Edinburgh, 2003.
[8] Joaquin Qui ˜nonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaus-
sian process regression. Journal of Machine Learning Research, 6(12):1939–1959, 2005.
[9] Carl Rasmussen and Christopher Williams. Gaussian processes for machine learning. MIT Press, 2006.
[10] Malte Kuss and Carl Edward Rasmussen. Assessing approximations for Gaussian process classi ﬁcation.
In Advances in Neural Information Processing Systems 18. MIT Press, 2005.
[11] Thomas Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts
Institute of Technology, 2001.
[12] Matthias Seeger.
families, 2005.
Expectation propagation for exponential
http://www.cs.berkeley.edu/ ˜mseeger/papers/epexpfam .ps.gz.
[13] Matthias Seeger, Christopher Williams, and Neil Lawrence. Fast forward selection to speed up sparse
Gaussian process regression. In Proceedings of the 9th International Workshop on AI Stats. Society for
Arti ﬁcial Intelligence and Statistics, 2003.
[14] Brian Ripley. Pattern recognition and neural networks. Cambridge University Press, 1996.
[15] Michael E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine
Learning Research, 1:211–244, 2001.
[16] Carl Edward Rasmussen and Joaquin Qui ˜nonero-Candela . Healing the relevance vector machine through
augmentation. In Proceedings of 22nd ICML. ACM Press, 2005.
[17] Edward Snelson and Zoubin Ghahramani. Variable noise and dimensionality reduction for sparse Gaus-
sian processes. In Proceedings of the 22nd Annual Conference on Uncertainty in AI. AUAI Press, 2006.

Available from

8


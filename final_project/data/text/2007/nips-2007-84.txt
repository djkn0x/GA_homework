Trans-dimensional MCMC for Bayesian Policy
Learning

Matt Hoffman
Dept. of Computer Science
University of British Columbia
hoffmanm@cs.ubc.ca

Arnaud Doucet
Depts. of Statistics and Computer Science
University of British Columbia
arnaud@cs.ubc.ca

Nando de Freitas
Dept. of Computer Science
University of British Columbia
nando@cs.ubc.ca

Ajay Jasra
Dept. of Mathematics
Imperial College London
ajay.jasra@imperial.ac.uk

Abstract

A recently proposed formulation of the stochastic planning and control problem
as one of parameter estimation for suitable artiﬁcial statistical models has led to
the adoption of inference algorithms for this notoriously hard problem. At the
algorithmic level, the focus has been on developing Expectation-Maximization
(EM) algorithms. In this paper, we begin by making the crucial observation that
the stochastic control problem can be reinterpreted as one of trans-dimensional
inference. With this new interpretation, we are able to propose a novel reversible
jump Markov chain Monte Carlo (MCMC) algorithm that is more efﬁcient than
its EM counterparts. Moreover, it enables us to implement full Bayesian policy
search, without the need for gradients and with one single Markov chain. The
new approach involves sampling directly from a distribution that is proportional
to the reward and, consequently, performs better than classic simulations methods
in situations where the reward is a rare event.

1 Introduction

Continuous state-space Markov Decision Processes (MDPs) are notoriously difﬁcult to solve. Ex-
cept for a few rare cases,
including linear Gaussian models with quadratic cost,
there is no
closed-form solution and approximations are required [4]. A large number of methods have been
proposed in the literature relying on value function approximation and policy search; including
[3, 10, 14, 16, 18]. In this paper, we follow the policy learning approach because of its promise and
remarkable success in complex domains; see for example [13, 15]. Our work is strongly motivated
by a recent formulation of stochastic planning and control problems as inference problems. This line
of work appears to have been initiated in [5], where the authors used EM as an alternative to standard
stochastic gradient algorithms to maximize an expected cost. In [2], a planning problem under un-
certainty was solved using a Viterbi algorithm. This was later extended in [21]. In these works, the
number of time steps to reach the goal was ﬁxed and the plans were not optimal in expected reward.
An important step toward surmounting these limitations was taken in [20, 19]. In these works, the
standard discounted reward control problem was expressed in terms of an inﬁnite mixture of MDPs.
To make the problem tractable, the authors proposed to truncate the inﬁnite horizon time.

Here, we make the observation that, in this probabilistic interpretation of stochastic control, the
objective function can be written as the expectation of a positive function with respect to a trans-
dimensional probability distribution, i.e. a probability distribution deﬁned on a union of subspaces

1

of different dimensions. By reinterpreting this function as a (artiﬁcial) marginal likelihood, it is
easy to see that it can also be maximized using an EM-type algorithm in the spirit of [5]. However,
the observation that we are dealing with a trans-dimensional distribution enables us to go beyond
EM. We believe it creates many opportunities for exploiting a large body of sophisticated inference
algorithms in the decision-making context.

In this paper, we propose a full Bayesian policy search alternative to the EM algorithm. In this
approach, we set a prior distribution on the set of policy parameters and derive an artiﬁcial posterior
distribution which is proportional to the prior times the expected reward. In the simpler context
of myopic Bayesian experimental design, a similar method was developed in [11] and applied suc-
cessfully to high-dimensional problems [12]. Our method can be interpreted as a trans-dimensional
extension of [11]. We sample from the resulting artiﬁcial posterior distribution using a single trans-
dimensional MCMC algorithm, which only involves a simple modiﬁcation of the MCMC algorithm
developed to implement the EM.

Although the Bayesian policy search approach can beneﬁt from gradient information, it does not
require gradients. Moreover, since the target is proportional to the expected reward, the simulation
is guided to areas of high reward automatically. In the ﬁxed policy case, the value function is often
computed using importance sampling. In this context, our algorithm could be reinterpreted as an
MCMC algorithm sampling from the optimal importance distribution.

2 Model formulation

We consider the following class of discrete-time Markov decision processes (MDPs):

X1 ∼ µ(·)
Xn | (Xn−1 = x, An−1 = a) ∼ fa ( ·| x)
Rn | (Xn = x, An = a) ∼ ga ( ·| x)
An | (Xn = x, θ) ∼ πθ ( ·| x) ,

(1)

where n = 1, 2, . . . is a discrete-time index, µ(·) is the initial state distribution, {Xn} is the
X −valued state process, {An } is the A−valued action process, {Rn} is a positive real-valued re-
ward process, fa denotes the transition density, ga the reward density and πθ is a randomized policy.
If we have a deterministic policy then πθ ( a| x) = δϕθ (x) (a). In this case, the transition model
fa ( ·| x) assumes the parametrization fθ ( ·| x). The reward model could also be parameterized as
gθ ( ·| x). It should be noted that for this work we will be working within a model-based framework
and as a result will require knowledge of the transition model (although it could be learned).
We are here interested in maximizing with respect to the parameters of the policy θ the expected
future reward
µ (θ) = E " ∞Xn=1
γ n−1Rn# ,
V π
where 0 < γ < 1 is a discount factor and the expectation is with respect to the probabilistic model
deﬁned in (1). As shown in [20], it is possible to re-write this objective of optimizing an inﬁnite
horizon discounted reward MDP (where the reward happens at each step) as one of optimizing an
inﬁnite mixture of ﬁnite horizon MDPs (where the reward only happens at the last time step).
In particular, we note that by introducing the trans-dimensional probability distribution on ] {k} ×
X k × Ak × R+ given by
kYn=1
kYn=2
pθ (k , x1:k , a1:k , rk ) = (1 − γ ) γ k−1µ (x1 ) gak ( rk | xk )
fan−1 ( xn | xn−1 )
πθ ( an | xn ) ,
(2)
we can easily rewrite V π
µ (θ) as an inﬁnite mixture model of ﬁnite horizon MDPs, with the reward
happening at the last horizon step; namely at k . Speciﬁcally we have:
∞Xk=1 Z rk pθ (k , x1:k , a1:k , rk ) dx1:k da1:k drk
2

µ (θ) = (1 − γ )−1 Epθ [RK ] = (1 − γ )−1
V π

(3)

for a randomized policy. Similarly, for a deterministic policy, the representation (3) also holds for
the trans-dimensional probability distribution deﬁned on ] {k} × X k × R+ given by
kYn=2
pθ (k , x1:k , rk ) = (1 − γ ) γ k−1µ (x1 ) gθ ( rk | xk )
fθ ( xn | xn−1 ) .
The representation (3) was also used in [6] to compute the value function through MCMC for a
ﬁxed θ. In [20], this representation is exploited to maximize V π
µ (θ) using the EM algorithm which,
applied to this problem, proceeds as follows at iteration i

(4)

θi = arg max
θ∈Θ

Q (θi−1 , θ)

where

.

Q (θi−1 , θ) = Eepθi−1
[log (RK .pθ (K, X1:K , A1:K , RK ))] ,
rk pθ (k , x1:k , a1:k , rk )
epθ (k , x1:k , a1:k , rk ) =
Epθ [RK ]
Unlike [20], we are interested in problems with potentially nonlinear and non-Gaussian properties.
In these situations, the Q function cannot be calculated exactly. The standard Monte Carlo EM
approach consists of sampling from epθ (k , x1:k , a1:k , rk ) using MCMC to obtain a Monte Carlo es-
timate of the Q function. As epθ (k , x1:k , a1:k , rk ) is proportional to the reward, the samples will con-
sequently be drawn in regions of high reward. This is a particularly interesting feature in situations
where the reward function is concentrated in a region of low probability mass under pθ (k , x1:k , rk ),
which is often the case in high-dimensional control settings. Note that if we wanted to estimate
V π
µ (θ) using importance sampling, then the distribution epθ (k , x1:k , a1:k , rk ) corresponds to the op-
timal zero-variance importance distribution.
Alternatively, instead of sampling from epθ (k , x1:k , a1:k , rk ) using MCMC, we could proceed as
in [20] to derive forward-backward algorithms to implement the E-step which can be implemented
here using Sequential Monte Carlo (SMC) techniques. We have in fact done this using the smoothing
algorithms proposed in [9]. However, we will focus the discussion on a different MCMC approach
based on trans-dimensional simulation. As shown in the experiments, the latter does considerably
better.

Finally, we remark that for a deterministic policy, we can introduce the trans-dimensional distribu-
tion:
rk pθ (k , x1:k , rk )
epθ (k , x1:k , rk ) =
Epθ [RK ] .
In addition, and for ease of presentation only, we focus the discussion on deterministic policies and
reward functions gθ ( rn | xn ) = δr(xn ) (rn ) ; the extension of our algorithms to the randomized case
is straightforward.

3 Bayesian policy exploration

The EM algorithm is particularly sensitive to initialization and might get trapped in a severe lo-
cal maximum of V π
µ (θ). Moreover, in the general state-space setting that we are considering, the
particle smoothers in the E-step can be very expensive computationally.

To address these concerns, we propose an alternative full Bayesian approach. In the simpler context
of experimental design, this approach was successfully developed in [11], [12]. The idea consists
of introducing a vague prior distribution p (θ) on the parameters of the policy θ. We then deﬁne the
new artiﬁcial probability distribution deﬁned on Θ × ] {k} × X k by
p (θ, k , x1:k ) ∝ r (xk ) pθ (k , x1:k ) p (θ) .
By construction, this target distribution admits the following marginal in θ
p (θ) ∝ V π
µ (θ) p (θ)
and we can select an improper prior distribution p (θ) ∝ 1 if RΘ V π
µ (θ) dθ < ∞.
3

If we could sample from p (θ), then the generated samples (cid:8)θ(i)(cid:9) would concentrate themselves
in regions where V π
µ (θ) is large. We cannot sample from p (θ) directly but we can developed a
trans-dimensional MCMC algorithm which will generate asymptotically samples from p (θ, k , x1:k ),
hence samples from p (θ).

Our algorithm proceeds as follows. Assume the current state of the Markov chain targeting
p (θ, k , x1:k ) is (θ, k , x1:k ). We propose ﬁrst to update the components (k , x1:k ) conditional upon θ
using a combination of birth, death and update moves using the reversible jump MCMC algorithm
[7, 8, 17]. Then we propose to update θ conditional upon the current value of (k , x1:k ). This can
be achieved using a simple Metropolis-Hastings algorithm or a more sophisticated dynamic Monte
Carlo schemes. For example, if gradient information is available, one could adopt Langevin diffu-
sions and the hybrid Monte Carlo algorithm [1]. The overall algorithm is depicted in Figure 1. The
details of the reversible jump algorithm are presented in the following section.

1.

Initialization: set (k(0) , x(0)
1:k(0) , θ(0) ).
2. For i = 0 to N − 1
• Sample u ∼ U[0,1] .
• If (u ≤ bk )
– then carry out a “bir th” move: Increase the horizon length of the MDP, say
k(i) = k(i−1) + 1 and inser t a new state.
– else if (u ≤ bk + dk ) then carry out a “death” move: decrease the horizon
length of the MDP, say k(i) = k(i−1) − 1 and an existing state.
– else let k(i) = k(i−1) and generate samples x(i)
1:k(i) of the MDP states.
End If.
• Sample the policy parameters θ(i) conditional on the samples (x(i)
1:k(i) , k(i) ).

Figure 1: Generic reversible jump MCMC for Bayesian policy learning.

We note that for a given θ the samples of the states and horizon generated by this Markov chain
will also be distributed (asymptotically) according to the trans-dimensional distribution epθ (k , x1:k ).
Hence, they can be easily adapted to generate a Monte Carlo estimate of Q (θi−1 , θ). This allows
us to side-step the need for expensive smoothing algorithms in the E-step. The trans-dimensional
simulation approach has the advantage that the samples will concentrate themselves automatically
in regions where epθ (k) has high probability masses. Moreover, unlike in the EM framework, it is
no longer necessary to truncate the time domain.
4 Trans-Dimensional Markov chain Monte Carlo

We present a simple reversible jump method composed of two reversible moves (birth and death)
and several update moves. Assume the current state of the Markov chain targeting epθ (k , x1:k )
is (k , x1:k ). With probability1 bk , we propose a birth move;
that is we sample a location
uniformly in the interval {1, ..., k + 1}, i.e. J ∼ U {1, ..., k + 1}, and propose the candidate
(k + 1, x1:j−1 , x∗ , xj :k ) where X ∗ ∼ qθ ( ·| xj−1:j ). This candidate is accepted with probability
Abirth = min{1, αbirth} where we have for j ∈ {2, ..., k − 1}
αbirth = epθ (k + 1, x1:j−1 , x∗ , xj :k ) dk+1
epθ (k , x1:k ) bk qθ ( x∗ | xj−1:j )
γ fθ ( x∗ | xj−1 ) fθ ( xj | x∗ ) dk+1
=
fθ ( xj | xj−1 ) bk qθ ( x∗ | xj−1:j )
γ µ (x∗ ) fθ ( x1 | x∗ ) dk+1
µ (x1 ) bk qθ ( x∗ | x1 )

for j = 1

αbirth =

,

,

1 In practice we can set the birth and death probabilities such that bk = dk = uk = 1/3.

4

and j = k + 1

.

αbirth =

γ r (x∗ ) fθ ( x∗ | xk ) dk+1
r (xk ) bk qθ ( x∗ | xk )
With probability dk , we propose a death move; that is J ∼ U {1, ..., k} and we propose the candidate
(k − 1, x1:j−1 , xj+1:k ) which is accepted with probability Adeath = min{1, αdeath} where for
j ∈ {2, ..., k − 1}
αdeath = epθ (k − 1, x1:j−1 , xj+1:k ) bk+1 qθ ( xj | xj−1:j+1 )
epθ (k , x1:k ) dk
fθ ( xj+1 | xj−1 ) bk+1 qθ ( xj | xj−1:j+1 )
=
,
γ fθ ( xj+1 | xj ) fθ ( xj | xj−1 ) dk

for j = 1

and for j = k

αdeath =

µ (x2 ) qθ ( x1 | x2 ) bk+1
γ µ (x1 ) fθ ( x2 | x1 ) dk

,

αdeath =

r (xk−1 ) qθ ( xk | xk−1 ) bk+1
γ r (xk ) fθ ( xk | xk−1 ) dk
The αbirth and αdeath terms derived above can be thought of as ratios between the distribution over
the newly proposed state of the chain (i.e. after the birth/death) and the current state. These terms
must also ensure reversibility and the dimension-matching requirement for reversible jump MCMC.
For more information see [7, 8].

.

Finally with probability uk = 1 − bk − dk , we propose a standard (ﬁxed dimensional) move where
we update all or a subset of the components x1:k using say Metropolis-Hastings or Gibbs moves.
There are many design possibilities for these moves.
In general, one should block some of the
variables so as to improve the mixing time of the Markov chain. If one adopts a simple one-at-a
time Metropolis-Hastings scheme with proposals qθ ( x∗ | xj−1:j+1 ) to update the j -th term, then the
candidate is accepted with probability Aupdate = min{1, αupdate} where for j ∈ {2, ..., k − 1}
αupdate = epθ (k , x1:j−1 , x∗ , xj+1:k ) qθ ( xj | xj−1 , x∗ , xj+1 )
epθ (k , x1:k ) qθ ( x∗ | xj−1:j+1 )
fθ ( x∗ | xj−1 ) fθ ( xj+1 | x∗ ) qθ ( xj | xj−1 , x∗ , xj+1 )
=
fθ ( xj | xj−1 ) fθ ( xj+1 | xj ) qθ ( x∗ | xj−1:j+1 )

,

for j = 1

and for j = k

αupdate =

µ (x∗ ) fθ ( x2 | x∗ ) qθ ( x1 | x∗ , x2 )
µ (x1 ) fθ ( x2 | x1 ) qθ ( x∗ | x1:2 )

,

αupdate =

r (x∗ ) fθ ( x∗ | xk−1 ) qθ ( xk | x∗ , xk−1 )
r (xk ) fθ ( xk | xk−1 ) qθ ( x∗ | xk−1:k )

.

Under weak assumptions on the model, the Markov chain {K (i) , X (i)
1:K } generated by this transition
kernel will be irreducible and aperiodic and hence will generate asymptotically samples from the
target distribution epθ (k , x1:k ).
We emphasize that the structure of the distributions epθ ( x1:k | k) will not in many applications vary
signiﬁcantly with k and we often have epθ ( x1:k | k) ≈ epθ ( x1:k | k + 1). Hence the probability of hav-
ing the reversible moves accepted will be reasonable. Standard Bayesian applications of reversible
jump MCMC usually do not enjoy this property and it makes it more difﬁcult to design fast mixing
algorithms. In this respect, our problem is easier.

5 Experiments

It should be noted from the outset that the results presented in this paper are preliminary, and serve
mainly as an illustration of the Monte Carlo algorithms presented earlier. With that note aside, even
these simple examples will give us some intuition about the algorithms’ performance and behavior.

5

1.4

1.2

1

0.8

0.6

0.4

0.2

0

−0.2

−0.2

0

0.2

0.4

0.6

0.8

1

1.2

1.4

Figure 2: This ﬁgure shows an illustration of the 2d state-space described in section 5. Ten sample points
are shown distributed according to µ, the initial distribution, and the contour plot corresponds to the reward
function r . The red line denotes the policy parameterized by some angle θ , while a path is drawn in blue
sampled from this policy.

We are also very optimistic as to the possible applications of analytic expressions for linear Gaussian
models, but space has not allowed us to present simulations for this class of models here.

We will consider state- and action-spaces X = A = R2 such that each state x ∈ X is a 2d position
and each action a ∈ A is a vector corresponding to a change in position. A new state at time n
is given by Xn = Xn−1 + An−1 + νn−1 where νn−1 denotes zero-mean Gaussian noise. Finally
we will let µ be a normal distribution about the origin, and consider a reward (as in [20]) given by
an unnormalized Gaussian about some point m, i.e. r(x) = exp(− 1
2 (x − m)T Σ−1 (x − m)). An
illustration of this space can be seen in Figure 2 where m = (1, 1).

For these experiments we chose a simple, stochastic policy parameterized by θ ∈ [0, 2π ]. Under
this policy, an action An = (w + δ) · (cos(θ + ω ), sin(θ + ω )) is taken where δ and ω are normally
distributed random variables and w is some (small) constant step-length. Intuitively, this policy cor-
responds to choosing a direction θ in which the agent will walk. While unrealistic from a real-world
perspective, this allows us a method to easily evaluate and plot the convergence of our algorithm.
For a state-space with initial distribution and reward function deﬁned as in Figure 2 the optimal
policy corresponds to θ = π/4.

We ﬁrst implemented a simple SMC-based extension of the EM algorithm described in [20], wherein
a particle ﬁlter was used for the forwards/backwards ﬁlters. The plots in Figure 3 compare the
SMC-based and trans-dimensional approaches performing on this synthetic example. Here the in-
ferred value of θ is shown against CPU time, averaged over 5 runs. The ﬁrst thing of note is the
terrible performance of the SMC-based algorithm—in fact we had to make the reward broader and
closer to the initial position in order to ensure that the algorithm converges in a reasonable amount
of time. This comes as no surprise considering the O(N 2k2
max ) time complexity necessary for com-
puting the importance weights. While there do exist methods [9] for reducing this complexity to
O(N log N k2
max ), the discrepancy between this and the reversible jump MCMC method suggests
that the MCMC approach may be more adapted to this class of problems. In the ﬁnite/discrete case
it is also possible, as shown by Toussaint et al (2006), to reduce the k2
max term to kmax by calculating
updates only using messages from the backwards recursion. The SMC method might further be im-
proved by better choices for the artiﬁcial distribution ηn (xn ) in the backwards ﬁlter. In this problem
we used a vague Gaussian centered on the relevant state-space. It is however possible that any added
beneﬁt from a more informative η distribution is counterbalanced by the time required to calculate
this η , for example by simulating particles forward in order to ﬁnd the invariant distribution, etc.

Also shown in ﬁgure 3 is the performance of a Monte Carlo EM algorithm using reversible jump
MCMC in the E-step. Both this and the fully Bayesian approach perform comparably, although the
fully Bayesian approach shows less in-run variance, as well as less variance between runs. The EM
algorithm was also more sensitive, and we were forced to increase the number of samples N used

6

Convergence of θ as a function of time

Convergence of θ as a function of time

Two−filter EM
Monte Carlo EM
Bayes. policy search
Optimal (baseline)

Monte Carlo EM
Bayes. policy search
Optimal (baseline)

)
s
n
a
i
d
a
r
 
n
i
(
 
θ

0.85

0.8

0.75

1.5

1.4

1.3

1.2

1.1

1

0.9

0.8

)
s
n
a
i
d
a
r
 
n
i
(
 
θ

0.7

0

500

1000
1500
cpu time (in seconds)

2000

2500

0.7

0

200

400
600
cpu time (in seconds)

800

1000

Figure 3: The left ﬁgure shows estimates for the policy parameter θ as a function of the CPU time used to
calculate that value. This data is shown for the three discussed Monte Carlo algorithms as applied to a synthetic
example and has been averaged over ﬁve runs; error bars are shown for the SMC-based EM algorithm. Because
of the poor performance of the SMC-based algorithm it is difﬁcult to compare the performance of the other two
algorithms using only this plot. The right ﬁgure shows a smoothed and “zoomed” version of the right plot in
order to show the reversible-jump EM algorithm and the fully Bayesian algorithm in more detail. In both plots
a red line denotes the known optimal policy parameter of π/4.

by the E-step as the algorithm progressed, as well as controlling the learning rate with a smoothing
parameter. For higher dimensional and/or larger models it is not inconceivable that this could have
an adverse affect on the algorithms performance.

Finally, we also compared the proposed Bayesian policy exploration method to the PEGASUS [14]
approach using a local search method. We initially tried using a policy-gradient approach, but
because of the very highly-peaked rewards the gradients become very poorly scaled and would have
required more tuning. As shown in Figure 4, the Bayesian strategy is more efﬁcient in this rare
event setting. As the dimension of the state-space increases, we expect this difference to become
even more pronounced.

6 Discussion

We believe that formulating stochastic control as a trans-dimensional inference problem is fruitful.
This formulation relies on minimal assumptions and allows us to apply modern inference algorithms
to solve control problems. We have focused here on Monte Carlo methods and have presented—
to the best of our knowledge—the ﬁrst application of reversible jump MCMC to policy search.
Our results, on an illustrative example, showed that this trans-dimensional MCMC algorithm is
more effective that standard policy search methods and alternative Monte Carlo methods relying on
particle ﬁlters. However, this methodology remains to be tested on high-dimensional problems. For
such scenarios, we expect that it will be necessary to develop more efﬁcient MCMC strategies to
explore the policy space efﬁciently.

References

[1] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for machine learning.
Machine Learning, 50:5–43, 2003.

[2] H. Attias. Planning by probabilistic inference. In Uncertainty in Artiﬁcial Intelligence, 2003.

[3] J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence
Research, 15:319–350, 2001.

[4] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 1995.

[5] P. Dayan and G. E. Hinton. Using EM for reinforcement learning. Neural Computation, 9:271–278, 1997.

7

Evolution of policy parameters against transition-model samples

rjmdp
pegasus
optimal

)
a
t
e
h
t
(
 
r
e
t
e
m
a
r
a
p
 
y
c
i
l
o
p

0.8

0.7

0.6

0.5

0.4

0.3

0.2
0

1000

2000
7000
6000
5000
4000
3000
number of samples taken from transition-model

8000

9000

Figure 4: Convergence of PEGASUS and our Bayesian policy search algorithm when started from θ = 0
and converging to the optimum of θ∗ = π/4. The plots are averaged over 10 runs. For our algorithm we
plot samples taken directly from the MCMC algorithm itself: plotting the empirical average would produce an
estimate whose convergence is almost immediate, but we also wanted to show the “burn-in” period. For both
algorithms lines denoting one standard deviation are shown and performance is plotted against the number of
samples taken from the transition model.

[6] A. Doucet and V. B. Tadic. On solving integral equations using Markov chain Monte Carlo methods.
Technical Report CUED-F-INFENG 444, Cambridge University Engineering Department, 2004.

[7] P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.
Biometrika, 82:711–732, 1995.

[8] P. J. Green. Trans-dimensional Markov chain Monte Carlo.
2003.

In Highly Structured Stochastic Systems,

[9] M. Klaas, M. Briers, N. de Freitas, A. Doucet, and S. Maskell. Fast particle smoothing: If i had a million
particles. In International Conference on Machine Learning, 2006.

[10] G. Lawrence, N. Cowan, and S. Russell. Efﬁcient gradient estimation for motor control learning.
Uncertainty in Artiﬁcial Intelligence, pages 354–36, 2003.

In

[11] P. M ¨uller. Simulation based optimal design. Bayesian Statistics, 6, 1999.

[12] P. M ¨uller, B. Sans ´o, and M. De Iorio. Optimal Bayesian design by inhomogeneous Markov chain simu-
lation. J. American Stat. Assoc., 99:788–798, 2004.

[13] A. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang. Inverted autonomous
helicopter ﬂight via reinforcement learning. In International Symposium on Experimental Robotics, 2004.

[14] A. Y. Ng and M. I. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs.
Uncertainty in Artiﬁcial Intelligence, 2000.

In

[15] J. Peters and S. Schaal. Policy gradient methods for robotics.
Intelligent Robotics Systems, 2006.

In IEEE International Conference on

[16] M. Porta, N. Vlassis, M. T. J. Spaan, and P. Poupart. Point-based value iteration for continuous POMDPs.
Journal of Machine Learning Research, 7:2329–2367, 2006.

[17] S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number of components.
Journal of the Royal Statistical Society B, 59(4):731–792, 1997.

[18] S. Thrun. Monte Carlo POMDPs. In S. Solla, T. Leen, and K.-R. M ¨uller, editors, Neural Information
Processing Systems, pages 1064–1070. MIT Press, 2000.

[19] M. Toussaint, S. Harmeling, and A. Storkey. Probabilistic inference for solving (PO)MDPs. Technical
Report EDI-INF-RR-0934, University of Edinburgh, School of Informatics, 2006.

[20] M. Toussaint and A. Storkey. Probabilistic inference for solving discrete and continuous state Markov
decision processes. In International Conference on Machine Learning, 2006.

[21] D. Verma and R. P. N. Rao. Planning and acting in uncertain environments using probabilistic inference.
In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, 2006.

8


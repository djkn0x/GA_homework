Theoretical Analysis of Learning with
Reward-Modulated Spike-Timing-Dependent
Plasticity

Robert Legenstein, Dejan Pecevski, Wolfgang Maass
Institute for Theoretical Computer Science
Graz University of Technology
A-8010 Graz, Austria
{legi,dejan,maass}@igi.tugraz.at

Abstract

Reward-modulated spike-timing-dependent plasticity (STDP) has
recently
emerged as a candidate for a learning rule that could explain how local learning
rules at single synapses support behaviorally relevant adaptive changes in com-
plex networks of spiking neurons. However the potential and limitations of this
learning rule could so far only be tested through computer simulations. This ar-
ticle provides tools for an analytic treatment of reward-mo dulated STDP, which
allow us to predict under which conditions reward-modulated STDP will be able
to achieve a desired learning effect. In particular, we can p roduce in this way
a theoretical explanation and a computer model for a fundamental experimental
ﬁnding on biofeedback in monkeys (reported in [1]).

1 Introduction

A major puzzle for understanding learning in biological org anisms is the relationship between ex-
perimentally well-established learning rules for synapse s (such as STDP) on the microscopic level
and adaptive changes of the behavior of biological organisms on the macroscopic level. Neuromod-
ulatory systems which send diffuse signals related to reinforcements (rewards) and behavioral state
to several large networks of neurons in the brain, have been i dentiﬁed as likely intermediaries that
relate these two levels of learning. It is well-known that the consolidation of changes of synaptic
weights in response to pre- and postsynaptic neuronal activ ity requires the presence of such third
signals [2]. Corresponding spike-based learning rules of the form

(1)

= cj i (t)d(t),

dwj i (t)
dt
have been proposed in [3], where wj i is the weight of a synapse from neuron i to neuron j , cj i (t) is
an eligibility trace of this synapse which collects proposed weight changes resulting from a learning
rule such as STDP, and d(t) = h(t) − ¯h is a neuromodulatory signal with mean ¯h (where h(t) might
for example represent reward prediction errors, encoded th rough the concentration of dopamine in
the extra-cellular ﬂuid). We will consider in this article o nly cases where the reward prediction
error is equal to the current reward. We will refer to d(t) simply as the reward signal. Obviously
such learning scheme (1) faces a large credit-assignment problem, since not only those synapses
for which weight changes would increase the chances of future reward receive the top-down signal
d(t), but billions of other synapses too. Nevertheless the brain is able to solve this credit-assignment
problem, as has been shown in one of the earliest (but still among the most amazing) demonstrations
of biofeedback in monkeys [1]. The spiking activity of single neurons (in area 4 of the precentral
gyrus) was recorded, the current ﬁring rate of this neuron wa s made visible to the monkey in the

form of an illuminated meter, and the monkey received food rewards for increases (or in alternating
trials for decreases) of the ﬁring rate of this neuron from it s average level. The monkeys learnt quite
reliably (on the time scale of 10’s of minutes) to change the ﬁ ring rate of this neuron in the currently
rewarded direction 1. Obviously the existence of learning mechanisms in the brain which are able to
solve this difﬁcult credit assignment problem is fundament al for understanding and modeling many
other learning features of the brain. We present in section 3 and 4 of this abstract a learning theory for
(1), where the eligibility trace cij (t) results from standard forms of STDP, which is able to explain
the success of the experiment in [1]. This theoretical model is con ﬁrmed by computer simulations
(see section 4.1). In section 5 we leave this concrete learning experiment and investigate under what
conditions neurons can learn through trial and error (via reward-modulated STDP) associations of
speciﬁc ﬁring patterns to speciﬁc patterns of input spikes.
The resulting theory leads to predictions
of speciﬁc parameter ranges for STDP that support this gener al form of learning. These were tested
through computer experiments, see 5.1.

Other interesting results of computer simulations of rewar d-modulated STDP in the context of neural
circuits were recently reported in [3] and [4] (we also refer to these articles for reviews of preceding
work by Seung and others).

2 Models for neurons and synaptic plasticity

(t − s − r)

dr W (−r)S post
j

(t − s − r)S pre
i

, t(3)
, t(2)
t(1)
, . . . is formalized
The spike train of a neuron i which ﬁres action potentials at times
i
i
i
δ(t − t(n)
by a sum of Dirac delta functions Si (t) = Pt(n)
). We assume that positive and negative
i
i
weight changes suggested by STDP for all pairs of pre- and postsynaptic spikes (according to the
two integrals in (2)) are collected in an eligibility trace cj i (t), where the impact of a spike pairing
with the second spike at time t − s on the eligibility trace at time t is given by some function fc (s)
for s ≥ 0:
cj i (t) = Z ∞
dsfc (s) (cid:20)Z ∞
0
0

dr W (r)S post
(t − s)S pre
j
i
+ Z ∞
(t − s)(cid:21) .
0
e− s
In our simulations, fc (s) is a function of the form fc (s) = s
τe if s ≥ 0 and 0 otherwise, with
τe
time constant τe = 0.5s. W (r) denotes the standard exponential STDP learning window
if r ≥ 0
W (r) = (cid:26) A+ e−r/τ+
,
if r < 0
−A−er/τ−
,
where the positive constants A+ and A− scale the strength of potentiation and depression, τ+ and
τ− are positive time constants de ﬁning the width of the positiv e and negative learning window, and
S pre
, S post
are the spike trains of the presynaptic and postsynaptic neu ron respectively. The actual
i
j
weight change is the product of the eligibility trace with the reward signal as de ﬁned by equation (1).
We assume that weights are clipped at the lower boundary value 0 and an upper boundary wmax .
We use a linear Poisson neuron model whose output spike train S post
(t) is a realization of a Poisson
j
process with the underlying instantaneous ﬁring rate Rj (t). The effect of a spike of presynaptic
neuron i at time t′ on the membrane potential of neuron j is modeled by an increase in the instan-
taneous ﬁring rate by an amount wj i (t′ )ǫ(t − t′ ), where ǫ is a response kernel which models the
time course of a postsynaptic potential (PSP) elicited by an input spike. Since STDP according to
[3] has been experimentally con ﬁrmed only for excitatory sy napses, we will consider plasticity only
for excitatory connections and assume that wj i ≥ 0 for all i and ǫ(s) ≥ 0 for all s. Because the
synaptic response is scaled by the synaptic weights, we can assume without loss of generality that
the response kernel is normalized to R ∞
0 ds ǫ(s) = 1. In this linear model, the contributions of all
inputs are summed up linearly:
n
Xi=1 Z ∞
Rj (t) =
0
1Adjacent neurons tended to change their ﬁring rate in the sam e direction, but also differential changes of
directions of ﬁring rates of pairs of neurons are reported in [1] (when these differential changes were rewarded).

ds wj i (t − s) ǫ(s) Si (t − s) ,

,

(2)

(3)

(4)

where S1 , . . . , Sn are the n presynaptic spike trains.

3 Theoretical analysis of the resulting weight changes

=

,

(5)

ds fc (s) hDj i (t, s, r) νj i (t − s, r)iT

ds fc (s + r) hDj i (t, s, r) νj i (t − s, r)iT ,(6)

We are interested in the expected weight change over some time interval T (see [5]), where the
expectation is over realizations of the stochastic input- and output spike trains as well as a stochastic
realization of the reward signal, denoted by the ensemble average h·iE
wj i (t′ )dt′+E
T *Z t+T
= (cid:28)(cid:28) d
wj i (t)(cid:29)T (cid:29)E
1
d
hwj i (t + T ) − wj i (t)iE
dt
T
dt
t
where we used the abbreviation hf (t)iT = T −1 R t+T
f (t′ ) dt′ . Using equation (1), this yields
t
dr W (r) Z ∞
= Z ∞
hwj i (t + T ) − wj i (t)iE
T
0
0
dr W (r) Z ∞
+ Z 0
−∞
|r |
where Dj i (t, s, r) = hd(t)| Neuron j spikes at t − s, and neuron i spikes at t − s − riE is the
average reward at time t given a presynaptic spike at time t − s − r and a postsynaptic spike at
time t − s, and νj i (t, r) = hSj (t)Si (t − r)iE describes correlations between pre- and postsynaptic
spike timings (see [6] for the derivation). We see that the expected weight change depends on how
the correlations between the pre- and postsynaptic neurons correlate with the reward signal. If these
correlations are varying slowly with time, we can exploit th e self-averaging property of the weight
vector. Analogously to [5], we can drop the ensemble average on the left hand side and obtain:
hwj i (t)iT = Z ∞
dr W (r) Z ∞
0
0
dr W (r) Z ∞
+ Z 0
−∞
|r |
In the following, we will always use the smooth time-averaged vector hwj i (t)iT , but for brevity, we
will drop the angular brackets. If one assumes for simplicity that the impact of a pre-post spike pair
on the eligibility trace is always triggered by the postsynaptic spike, one gets (see [6] for details):
ds fc (s) Z ∞
= Z ∞
dwj i (t)
dt
−∞
0
This assumption (which is common in STDP analysis) will introduce a small error for post-before
pre spike pairs, since if a reward signal arrives at some time dr after the pairing, the weight update
will be proportional to fc (dr ) instead of fc (dr + r). For the analyses presented in this article, the
simpliﬁed equation (8) is a good approximation for the learn ing dynamics (see [6]). Equation (8)
shows that if the reward signal does not depend on pre- and pos tsynaptic spike statistics, the weight
will change according to standard STDP scaled by a constant proportional to the mean reward.

ds fc (s + r) hDj i (t, s, r) νj i (t − s, r)iT .

dr W (r) hDj i (t, s, r) νj i (t − s, r)iT .

ds fc (s) hDj i (t, s, r) νj i (t − s, r)iT

(7)

(8)

d
dt

4 Application to biofeedback experiments

We now apply our theoretical approach to the biofeedback experiments by Fetz and Baker [1] that
we have sketched in the introduction. The authors showed that it is possible to increase and decrease
the ﬁring rate of a randomly chosen neuron by rewarding the mo nkey for its high (respectively low)
ﬁring rates. We assume in our model that a reward is delivered to all neurons in the simulated
recurrent network with some delay dr every time a speciﬁc neuron k in the network produces an
action potential
d(t) = Z ∞
0
where ǫr (r) is the shape of the reward pulse corresponding to one postsyn aptic spike of the rein-
forced neuron. We assume that the reward kernel ǫr has zero mass, i.e., ¯ǫr = R ∞
0 dr ǫr (r) = 0. In

(t − dr − r)ǫr (r).

dr S post
k

(9)

dr′ ǫr (r′ )

dr′ ǫr (r′ )ǫ(s + r − dr − r′ ) + ǫr (s − dr ) ≈ ǫr (s − dr ).

our simulations, this reward kernel will have a positive bump in the ﬁrst few hundred milliseconds,
and a long tailed negative bump afterwards. With the linear Poisson neuron model (see Section 2),
the correlation of the reward with pre-post spike pairs of th e reinforced neuron is (see [6])
Dki (t, s, r) = wki Z ∞
0
The last approximation holds if the impact of a single input spike on the membrane potential is
small. The correlation of the reward with pre-post spike pai rs of non-reinforced neurons is
Dj i (t, s, r) = Z ∞
νkj (t − dr − r′ , s − dr − r′ ) + wkiwj i ǫ(s + r − dr − r′ )ǫ(r)
νj (t − s) + wj i ǫ(r)
0
(11)
If the contribution of a single postsynaptic potential to the membrane potential is small, we can
neglect the impact of the presynaptic spike and write
Dj i (t, s, r) ≈ Z ∞
νkj (t − dr − r′ , s − dr − r′ )
νj (t − s)
0
Hence, the reward-spike correlation of a non-reinforced ne uron depends on the correlation of this
neuron with the reinforced neuron. The mean weight change fo r weights to the reinforced neuron is
given by
wki (t) = Z ∞
ds fc (s + dr )ǫr (s) Z ∞
d
dt
0
−∞
This equation basically describes STDP with a learning rate that is proportional to the eligibility
function in the time around the reward-delay. The mean weigh t change of neurons j 6= k is given by
dr W (r) Z ∞
ds fc (s) Z ∞
wj i (t) = Z ∞
dr′ ǫr (r′ ) (cid:28) νkj (t − dr − r′ , s − dr − r′ )
νj i (t − s, r)(cid:29)T
νj (t − s)
0
−∞
0
(14)
If the output of neurons j and k are uncorrelated, this evaluates to approximately zero (se e [6]).
The result can be summarized as follows. The reinforced neuron is trained by STDP. Other neurons
are trained by STDP with a learning rate proportional to thei r correlation with the reinforced neuron.
If a neuron is uncorrelated with the reinforced neuron, the l earning rate is approximately zero.

dr W (r) hνki (t − dr − s, r)iT .

dr′ ǫr (r′ )

d
dt

.

(12)

(10)

.

(13)

4.1 Computer simulations

In order to test the theoretical predictions for the experim ent described in the previous section, we
have performed a computer simulation with a generic neural m icrocircuit receiving a global reward
signal. This global reward signal increases its value every time a speciﬁc neuron (the reinforced
neuron) in the circuit ﬁres. The circuit consists of 1000 lea ky integrate-and- ﬁre (LIF) neurons (80%
excitatory and 20% inhibitory), which are interconnected by conductance based synapses. The short
term dynamics of synapses was modeled in accordance with experimental data (see [6]). Neurons
within the recurrent circuit were randomly connected with p robabilities pee = 0.08, pei = 0.08,
pie = 0.096 and pii = 0.064 where the ee, ei, ie, ii indices designate the type of the presynaptic
and postsynaptic neurons (excitatory or inhibitory). To re produce the synaptic background activity
of neocortical neurons in vivo, an Ornstein-Uhlenbeck (OU) conductance noise process modeled
according to ([7]) was injected in the neurons, which also elicited spontaneous ﬁring of the neurons
in the circuit with an average rate of 4Hz. In half of the neurons part of the noise was substituted
with random synaptic connections from the circuit, in order to observe how the learning mecha-
nisms work when most of the input conductance in the neuron comes from a larger number of input
synapses which are plastic, instead of a static noise process. The function fc (t) from equation (2)
e− t
had the form fc (t) = t
τe if t ≥ 0 and 0 otherwise, with time constant τe = 0.5s. The reward
τe
signal during the simulation was computed according to eq. (9), with the following shape for ǫr (t)
t
t
− t
− t
r − A−
τ +
−
r .
τ
r
τ −
τ +
r
r
The parameter values for ǫr (t) were chosen such as to produce a positive reward pulse with a p eak
delayed 0.5s from the spike that caused it, and a long tailed negative bump so that R ∞
0 dt ǫr (t) = 0.

ǫr (t) = A+
r

(15)

e

e

A

]
z
H
[
 
e
t
a
r

11

10

9

8

7

6

5

4

3

B
)
0.70
x
a
m
w
/
w
(

0.65

0.60

s
t
h
g
i
e
w
 
.
g
v
a

0.55

0.50

0.45

0

5

10
time [min]

15

20

C

before
learning

after
learning

5

10
time [min]

15

20

0

1

2

3
5
4
time [sec]

6

7

8

Figure 1: Computer simulation of the experiment by Fetz and Baker [1]. A) The ﬁring rate of the
reinforced neuron (solid line) increases while the average ﬁring rate of 20 other randomly chosen
neurons in the circuit (dashed line) remains unchanged. B) Evolution of the average synaptic weight
of excitatory synapses connecting to the reinforced neuron (solid line) and to other neurons (dashed
line). C) Spike trains of the reinforced neuron at the beginning and at the end of the simulation.

For values of other model parameters see [6]. The learning ru le (1) was applied to all synapses in the
circuit which have excitatory presynaptic and postsynaptic neurons. The simulation was performed
for 20 min simulated biological time with a simulation time step of 0.1ms.
Fig. 1 shows that the ﬁring rate and synaptic weights of the re inforced neuron increase within a few
minutes of simulated biological time, while those of the other neurons remain largely unchanged.
Note that this reinforcement learning task is more difﬁcult
than that of the ﬁrst computer experiment
of [3], where postsynaptic ﬁring within 10 ms after presynap tic ﬁring of a randomly chosen synapse
was rewarded, since the relationship between synaptic activity (and hence with STDP) is less direct
in this setup. Whereas a very low spontaneous ﬁring rate of 1 H z was required in [3], this simulation
shows that reinforcement learning is also feasible at rate levels which correspond to those reported
in [1].

5 Rewarding spike-timings

In order to explore the limits of reward-modulated STDP, we h ave also investigated a substantially
more demanding reinforcement learning scenario. The rewar d signal d(t) was given in dependence
on how well the output spike train S post
of the neuron j matched some rather arbitrary spike train S ∗
j
that was produced by some neuron that received the same n input spike trains as the trained neuron
i ∈ {0, wmax }, but in addition n′ − n further
with arbitrary weights w∗ = (w∗
n )T , w∗
1 , . . . , w∗
spike trains Sn+1 , . . . , Sn′ with weights w∗
i = wmax . This setup provides a generic reinforcement
learning scenario, when a quite arbitrary (and not perfectl y realizable) spike output is reinforced, but
simultaneously the performance of the learner can be evalua ted quite clearly according to how well
its weights w1 , . . . , wn match those of the target neuron for those n input spike trains which both of
them receive. The reward d(t) at time t is given by
d(t) = Z ∞
dr κ(r)S post
j
−∞
where the function κ(r) with ¯κ = R ∞
−∞ ds κ(s) > 0 describes how the reward signal depends
on the time difference between a postsynaptic spike and a target spike and dr > 0 is the delay
of the reward. Our theoretical analysis below suggests that this reinforcement learning task can
in principle be solved by reward-modulated STDP if some constraints are fulﬁlled. The analysis
also reveals which reward kernels κ are suitable for this learning setup. The reward correlatio n for
synapse i is (see [6])
Dj i (t, s, r) = Z ∞
dr′ κ(r′ ) (cid:2)ν post
(t − dr ) + δ(s − dr ) + wj i (s + r − dr )ǫ(s + r − dr )(cid:3)
j
−∞
i ǫ(s + r − dr − r′ )] ,
[ν ∗ (t − dr − r′ ) + w∗
where ν post
(t) = hS post
(t)iE denotes the mean rate of the trained neuron at time t, and ν ∗ (t) =
j
j
hS ∗ (t)iE denotes the mean rate of the target spike train at time t. Since weights are changing very

(t − dr )S ∗ (t − dr − r),

(17)

(16)

(18)

slowly, we have wj i (t − s − r) = wj i (t). In the following, we will drop the dependence of wj i on
t for brevity. For simplicity, we assume that input rates are s tationary and uncorrelated. In this case
(since the weights are changing slowly), also the correlations between inputs and outputs can be
assumed stationary, νj i (t, r) = νj i (r). We assume that the eligibility function fc (dr ) ≈ fc (dr + r)
if |r| is on a time scale of a PSP, the learning window, or the reward kernel, and that dr is large
compared to these time scales. Then, for uncorrelated Poisson input spike trains of rate ν pre
and the
i
linear Poisson neuron model, the weight change at synapse j i is given by
dwj i (t)
≈ ¯κ ¯fc ν ∗ν pre
ν post
(cid:2)ν post
¯W + wj i ¯Wǫ (cid:3)
i
j
j
dt
i ν post
(cid:2)ν post
+¯κfc (dr )ν pre
¯W + wj i ¯Wǫ (cid:3) (cid:2)ν ∗ + ν ∗wj i + w∗
(cid:3)
j
j
i
Z ∞
dr W (r)ǫκ (r) + wj i Z ∞
(cid:20)ν post
dr W (r)ǫ(r)ǫκ (r)(cid:21)
i ν pre
+fc (dr )w∗
j
i
−∞
−∞
¯W + wj i ¯Wǫ (cid:3) Z ∞
(cid:2)ν post
i wj i ν pre
+fc (dr )w∗
dr ǫ(r)ǫκ (r),
i
j
0
−∞ dr W (r), ǫκ(r) = R ∞
0 dr fc (r), ¯W = R ∞
where ¯fc = R ∞
−∞ dr′ κ(r′ )ǫ(r − r′ ) is the con-
volution of the reward kernel with the PSP is the integral ove r the STDP learning window, and
¯Wǫ = R ∞
−∞ dr ǫ(r)W (r).
We will now bound the expected weight change for synapses j i with w∗
i = wmax and for synapses
j k with w∗
jk = 0. In this way we can derive conditions for which the expected weight change for the
former synapses is positive, and that for the latter type is n egative. First, we assume that the integral
over the reward kernel is positive. In this case, the weight change is negative for synapses i with
¯W > wj i ¯Wǫ . In the worst case, wj i is wmax and ν post
i > 0, and −ν post
i = 0 if and only if ν pre
w∗
j
j
is small. We have to guarantee some minimal output rate ν post
min such that even if wj i = wmax , this
inequality is fulﬁlled. This could be guaranteed by some noi se current. For synapses i with w∗
i =
wmax , we obtain two more conditions (see [6] for a derivation). The conditions are summarized in
inequalities (19)-(21). If these inequalities are fulﬁlle d and input rates are positive, then the weight
vector converges on average from any initial weight vector t o w∗ .
−ν post
¯W > wmax ¯Wǫ
min
¯W Z ∞
Z ∞
dr W (r)ǫ(r)ǫκ (r) ≥ −ν post
dr ǫ(r)ǫκ (r)
max
0
−∞
¯fc
Z ∞
dr W (r)ǫκ (r) > − ¯W ¯κ (cid:20) ν ∗ν post
ν ∗
max(cid:21) ,
max
+ ν ∗ + ν post
wmax
fc (dr )
wmax
−∞
max is the maximal output rate. The second condition is less severe, and should be easily
where ν post
fulﬁlled in most setups. If this is the case, the ﬁrst conditi
on (19) ensures that weights with w∗ = 0
are depressed while the third condition (21) ensures that we ights with w∗ = wmax are potentiated.
Optimal reward kernels: From condition (21), we can deduce optimal reward kernels κ. The
kernel should be such that the integral R ∞
−∞ dr W (r)ǫκ (r) is large, while the integral over κ is small
(but positive). Hence, ǫκ (r) should be positive for r > 0 and negative for r < 0. In the following
experiments, we use a simple kernel which satisﬁes the afore mentioned constraints:
− t−tκ
− t−tκ
κ(r) = ( Aκ
if t − tκ ≥ 0
τ κ
τ κ
+ (e
2 )
1 − e
t−tκ
t−tκ
otherwise
−Aκ
τ κ
τ κ
2 )
,
1 − e
− (e
1 and τ κ
where Aκ
+ and Aκ
− are positive scaling constants, τ κ
2 de ﬁne the shape of the two double-
exponential functions the kernel is composed of, and tκ de ﬁnes the offset of the zero-crossing from
the origin. The optimal offset from the origin is negative and in the order of tens of milliseconds
for usual PSP-shapes ǫ. Hence, reward is positive if the neuron spikes around the ta rget spike or
somewhat later, and negative if the neuron spikes much too early.

(21)

(19)

(20)

+

,

5.1 Computer simulations

In the computer simulations we explored the learning rule in a more biologically realistic setting,
where we used a leaky integrate-and- ﬁre (LIF) neuron with in put synaptic connections coming from

1.0

0.8

0.6

0.4

0.2

A
)
x
a
m
w
/
w
(

s
t
h
g
i
e
w
 
e
g
a
r
e
v
a

0.0

0

B
before learning

∗
S
target   
(= rewarded
spike times)

realizable part 
∗
of target        
S

after learning

30

60
time [min]

90

120

0

1

2
time [sec]

3

4

Figure 2: Reinforcement learning of spike times. A) Synaptic weight changes of the trained LIF
neuron, for 5 different runs of the experiment. The curves sh ow the average of the synaptic weights
that should converge to w∗
i = 0 (dashed lines), and the average of the synaptic weights that should
i = wmax (solid lines) with a different shading for each simulation run. B) Com-
converge to w∗
parison of the output of the trained neuron before (upper tra ce) and after learning (lower trace; the
same input spike trains and the same noise inputs were used before and after training for 2 hours).
The second trace from above shows those spike times which are rewarded, the third trace shows the
target spike train without the additional noise inputs.

1.0

0.5

0.0

-0.5

0.5

0.0

-0.5

A
)
x
a
m
w
=
∗
w
(
w
∆
B
)
0
=
∗
w
(
w
∆

-1.0
Exp.No.

1

2

3

4

5

6

Figure 3: Predicted average weight
change (black bars) calculated
from equation (18), and the es-
timated average weight change
(gray bars) from simulations, pre-
sented for 6 different experiments
with different parameter settings
(see Table 1).2 A) Weight change
values for synapses with w∗
i =
wmax . B) Weight change values
for synapses with w∗
i = 0. Cases
where the constraints are not ful-
ﬁlled are shaded with gray color.

a generic neural microcircuit composed of 1000 LIF neurons. The synapses were conductance
based exhibiting short term facilitation and depression. The trained neuron and the arbitrarily given
neuron which produced the target spike train S ∗ ( “target neuron ”) both were connected to the same
randomly chosen, 100 excitatory and 10 inhibitory neurons f rom the circuit. The target neuron
had 10 additional excitatory input connections (these weights were set to wmax ), not accessible to
the trained neuron. Only the synapses of the trained neuron connecting from excitatory neurons
were set to be plastic. The target neuron had a weight vector w ith w∗
i = 0 for 0 ≤ i < 50 and
i = wmax for 50 ≤ i < 110.
The generic neural microcircuit from which the trained and
w∗
the target neurons receive the input had 80% excitatory and 20% inhibitory neurons interconnected
randomly with a probability of 0.1. The neurons received background synaptic noise as modeled in
[7], which caused spontaneous activity of the neurons with an average ﬁring rate of 6.9Hz. During
the simulations, we observed a ﬁring rate of 10.6Hz for the trained, and 19Hz for the target neuron.
The reward was delayed by 0.5s, and we used the same eligibility trace function fc (t) as in the
simulations for the biofeedback experiment (see [6] for details). The simulations were run for two
hours simulated biological time, with a simulation time step of 0.1ms. We performed 5 repetitions
of the experiment, each time with different randomly genera ted circuits and different initial weight
values for the trained neuron. In each of the 5 runs, the avera ge synaptic weights of synapses with
i = wmax and w∗
i = 0 approach their target values, as shown in Fig. 2A. In order to test how
w∗

2The values in the ﬁgure are calculated as ∆w = w(tsim )−w(0)
for the simulations, and with ∆w =
wmax /2
hdw/dtitsim
for the predicted value. w(t) is the average weight over synapses with the same value of w∗ .
wmax /2

min [Hz] A+106 A−
Ex. τǫ [ms] wmax ν post
A+
16.62 1.05
10
0.012
10
1
11.08 1.02
5
0.020
7
2
5.54 1.10
6
0.010
20
3
4
7
0.020
5
11.08 1.07
20.77 1.10
6
0.015
10
5
6
25
0.005
3
13.85 1.01

2 [ms] Aκ
τ+ ,τ κ
+
3.34
20,20
4.58
15,16
1.46
25,40
25,16
4.67
3.75
25,20
25,20
3.34

tsim [h]
5
10
16
13
3
13

Table 1: Parameter values used
for the simulations in Figure
3. Both cases where the con-
straints are satisﬁed and not sat-
isﬁed were covered. PSPs were
modeled as ǫ(s) = e(−s/τǫ )/τǫ .

closely the learning neuron reproduces the target spike tra in S ∗ after learning, we have performed
additional simulations where the same spiking input SI is applied to the learning neuron before and
after we conducted the learning experiment (results are rep orted in Fig. 2B).

The equations in section 5 de ﬁne a parameter space for which t he trained neuron can learn the target
synapse pattern w∗ . We have chosen 6 different parameter values encompassing cases with satisﬁed
and non-satisﬁed constraints, and performed experiments w here we compare the predicted average
weight change from equation (18) with the actual average weight change produced by simulations.
Figure 3 summarizes the results. In all 6 experiments, the su fﬁcient conditions (19)-(21) were cor-
rect. In those cases where these conditions were not met, the weight moved in the opposite direction,
suggesting that the theoretically sufﬁcient conditions (1 9)-(21) might also be necessary.

6 Discussion

We have developed in this paper a theory of reward-modulated STDP. This theory predicts that re-
inforcement learning through reward-modulated STDP is also possible at biologically more realistic
spontaneous ﬁring rates than the average rate of 1 Hz that was used (and argued to be needed) in the
extensive computer experiments of [3]. We have also shown bo th analytically and through computer
experiments that the result of the fundamental biofeedback experiment in monkeys from [1] can be
explained on the basis of reward-modulated STDP. The resulting theory of reward-modulated STDP
makes concrete predictions regarding the shape of various f unctions (e.g. reward functions) that
would optimally support the speed of reward-modulated learning for the generic (but rather difﬁ-
cult) learning tasks where a neuron is supposed to respond to input spikes with speciﬁc patterns of
output spikes, and only spikes at the right times are rewarded. Further work (see [6]) shows that
reward-modulated STDP can in some cases replace supervised training of readout neurons from
generic cortical microcircuit models.
Acknowledgment: We would like to thank Gordon Pipa and Matthias Munk for helpful discussions.
Written under partial support by the Austrian Science Fund FWF, project # P17229, project #
S9102 and project # FP6-015879 (FACETS) of the European Union.

References

[1] E. E. Fetz and M. A. Baker. Operantly conditioned patterns of precentral unit activity and correlated
responses in adjacent cells and contralateral muscles. J Neurophysiol, 36(2):179–204, Mar 1973.
[2] C. H. Bailey, M. Giustetto, Y.-Y. Huang, R. D. Hawkins, and E. R. Kandel. Is heterosynaptic modulation
essential for stabilizing Hebbian plasticity and memory? Nature Reviews Neuroscience, 1:11–20, 2000.
[3] E. M. Izhikevich. Solving the distal reward problem through linkage of STDP and dopamine signaling.
Cerebral Cortex Advance Access, January 13:1–10, 2007.
[4] R. V. Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity.
Neural Computation, 6:1468–1502, 2007.
[5] W. Gerstner and W. M. Kistler. Spiking Neuron Models. Cambridge University Press, Cambridge, 2002.
[6] R. Legenstein, D. Pecevski, and W. Maass. Theory and applications of reward-modulated spike-timing-
dependent plasticity. in preparation, 2007.
[7] J.M. Fellous A. Destexhe, M. Rudolph and T.J. Sejnowski. Fluctuating synaptic conductances recreate in
vivo-like activity in neocortical neurons. Neuroscience, 107(1):13–24, 2001.


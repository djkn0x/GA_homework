Comparison of objective functions for estimating
linear-nonlinear models

Tatyana O. Sharpee
Computational Neurobiology Laboratory,
the Salk Institute for Biological Studies, La Jolla, CA 92037
sharpee@salk.edu

Abstract

This paper compares a family of methods for characterizing neural feature selec-
tivity with natural stimuli in the framework of the linear-nonlinear model. In this
model, the neural ﬁring rate is a nonlinear function of a small number of relevant
stimulus components. The relevant stimulus dimensions can be found by max-
imizing one of the family of objective functions, R ´enyi divergences of different
orders [1, 2]. We show that maximizing one of them, R ´enyi divergence of or-
der 2, is equivalent to least-square ﬁtting of the linear-nonlinear model to neural
data. Next, we derive reconstruction errors in relevant dimensions found by max-
imizing R ´enyi divergences of arbitrary order in the asymptotic limit of large spike
numbers. We ﬁnd that the smallest errors are obtained with R ´enyi divergence of
order 1, also known as Kullback-Leibler divergence. This corresponds to ﬁnding
relevant dimensions by maximizing mutual information [2]. We numerically test
how these optimization schemes perform in the regime of low signal-to-noise ra-
tio (small number of spikes and increasing neural noise) for model visual neurons.
We ﬁnd that optimization schemes based on either least square ﬁtting or informa-
tion maximization perform well even when number of spikes is small. Information
maximization provides slightly, but signiﬁcantly, better reconstructions than least
square ﬁtting. This makes the problem of ﬁnding relevant dimensions, together
with the problem of lossy compression [3], one of examples where information-
theoretic measures are no more data limited than those derived from least squares.

1 Introduction

The application of system identiﬁcation techniques to the study of sensory neural systems has a
long history. One family of approaches employs the dimensionality reduction idea: while inputs
are typically very high-dimensional, not all dimensions are equally important for eliciting a neural
response [4, 5, 6, 7, 8]. The aim is then to ﬁnd a small set of dimensions {ˆe1 , ˆe2 , . . .} in the stimulus
space that are relevant for neural response, without imposing, however, a particular functional de-
pendence between the neural response and the stimulus components {s1 , s2 , . . .} along the relevant
dimensions:
P (spike|s) = P (spike)g(s1 , s2 , ..., sK ),

(1)

If the inputs are Gaussian, the last requirement is not important, because relevant dimensions can be
found without knowing a correct functional form for the nonlinear function g in Eq. (1). However,
for non-Gaussian inputs a wrong assumption for the form of the nonlinearity g will lead to systematic
errors in the estimate of the relevant dimensions themselves [9, 5, 1, 2]. The larger the deviations of
the stimulus distribution from a Gaussian, the larger will be the effect of errors in the presumed form
of the nonlinearity function g on estimating the relevant dimensions. Because inputs derived from a
natural environment, either visual or auditory, have been shown to be strongly non-Gaussian [10], we

will concentrate here on system identiﬁcation methods suitable for either Gaussian or non-Gaussian
stimuli.
To ﬁnd the relevant dimensions for neural responses probed with non-Gaussian inputs, Hunter and
Korenberg proposed an iterative scheme [5] where the relevant dimensions are ﬁrst found by assum-
ing that the input–output function g is linear. Its functional form is then updated given the current
estimate of the relevant dimensions. The inverse of g is then used to improve the estimate of the
relevant dimensions. This procedure can be improved not to rely on inverting the nonlinear function
g by formulating optimization problem exclusively with respect to relevant dimensions [1, 2], where
the nonlinear function g is taken into account in the objective function to be optimized. A family of
objective functions suitable for ﬁnding relevant dimensions with natural stimuli have been proposed
based on R ´enyi divergences [1] between the the probability distributions of stimulus components
along the candidate relevant dimensions computed with respect to all inputs and those associated
with spikes. Here we show that the optimization problem based on the R ´enyi divergence of order 2
corresponds to least square ﬁtting of the linear-nonlinear model to neural spike trains. The Kullback-
Leibler divergence also belongs to this family and is the R ´enyi divergence of order 1. It quantiﬁes
the amount of mutual information between the neural response and the stimulus components along
the relevant dimension [2]. The optimization scheme based on information maximization has been
previously proposed and implemented on model [2] and real cells [11]. Here we derive asymptotic
errors for optimization strategies based on R ´enyi divergences of arbitrary order, and show that rele-
vant dimensions found by maximizing Kullback-Leibler divergence have the smallest errors in the
limit of large spike numbers compared to maximizing other R ´enyi divergences, including the one
which implements least squares. We then show in numerical simulations on model cells that this
trend persists even for very low spike numbers.

χ2 [v] =

dsP (s)

2 Variance as an Objective Function
(cid:184)2
(cid:183)
(cid:90)
One way of selecting a low-dimensional model of neural response is to minimize a χ2 -difference
between spike probabilities measured and predicted by the model after averaging across all inputs s:
− P (spike|s · v)
P (spike|s)
P (spike)
P (spike)
(cid:183)
(cid:184)2
(cid:90)
(cid:90)
(cid:90)
where dimension v is the relevant dimension for a given model described by Eq. (1) [multiple
dimensions could also be used, see below]. Using the Bayes’ rule and rearranging terms, we get:
P (s|spike)
[Pv (x|spike)]2
[P (s|spike)]2
− P (s · v|spike)
−
χ2 [v] =
=
P (s · v)
P (s)
P (s)
Pv (x)
In the last integral averaging has been carried out with respect to all stimulus components except
for those along the trial direction v, so that integration variable x = s · v. Probability distributions
Pv (x) and Pv (x|spike) represent the result of this averaging across all presented stimuli and those
(cid:90)
(cid:90)
that lead to a spike, respectively:
dsP (s)δ(x − s · v), Pv (x|spike) =

dsP (s|spike)δ(x − s · v),

Pv (x) =

,

dx

dsP (s)

ds

(2)

. (3)

(4)

where δ(x) is a delta-function. In practice, both of the averages (4) are calculated by bining the
range of projections values x and computing histograms normalized to unity. Note that if there
multiple spikes are sometimes elicited, the probability distribution P (x|spike) can be constructed
by weighting the contribution from each stimulus according to the number of spikes it elicited.
If neural spikes are indeed based on one relevant dimension, then this dimension will explain all of
the variance, leading to χ2 = 0. For all other dimensions v, χ2 [v] > 0. Based on Eq. (3), in order
(cid:184)2
(cid:183)
(cid:90)
to minimize χ2 we need to maximize
Pv (x|spike)
F [v] =
dxPv (x)
(5)
,
Pv (x)
which is a R ´enyi divergence of order 2 between probability distribution Pv (x|spike) and Pv (x), and
are part of a family of f -divergences measures that are based on a convex function of the ratio of

I [v] =

=

.

(7)

.

.

(6)

(8)

(9)

dt

,

Fmax =

ds

dxPv (x)

F (α) [v] =

the two probability distributions (instead of a power α in a R ´enyi divergence of order α) [12, 13, 1].
(cid:184)α
(cid:183)
(cid:90)
For optimization strategy based on R ´enyi divergences of order α, the relevant dimensions are found
by maximizing:
Pv (x|spike)
1
α − 1
Pv (x)
(cid:90)
(cid:90)
By comparison, when the relevant dimension(s) are found by maximizing information [2], the goal
is to maximize Kullback-Leibler divergence, which can be obtained by taking a formal limit α → 1:
dxPv (x|spike) ln Pv (x|spike)
dxPv (x) Pv (x|spike)
ln Pv (x|spike)
Pv (x)
Pv (x)
Pv (x)
(cid:90)
Returning to the variance optimization, the maximal value for F [v] that can be achieved by any
dimension v is:
[P (s|spike)]2
P (s)
It corresponds to the variance in the ﬁring rate averaged across different inputs (see Eq. (9) below).
Computation of the mutual information carried by the individual spike about the stimulus relies on
(cid:183)
(cid:184)2
similar integrals. Following the procedure outlined for computing mutual information [14], one can
(cid:90)
use the Bayes’ rule and the ergodic assumption to compute Fmax as a time-average:

1
r(t)
Fmax =
¯r
T
where the ﬁring rate r(t) = P (spike|s)/∆t is measured in time bins of width ∆t using multiple
repetitions of the same stimulus sequence . The stimulus ensemble should be diverse enough to
justify the ergodic assumption [this could be checked by computing Fmax for increasing fractions of
the overall dataset size]. The average ﬁring rate ¯r = P (spike)/∆t is obtained by averaging r(t) in
time.
The fact that F [v] < Fmax can be seen either by simply noting that χ2 [v] ≥ 0, or from the data
processing inequality, which applies not only to Kullback-Leibler divergence, but also to R ´enyi
divergences [12, 13, 1]. In other words, the variance in the ﬁring rate explained by a given dimension
F [v] cannot be greater than the overall variance in the ﬁring rate Fmax . This is because we have
averaged over all of the variations in the ﬁring rate that correspond to inputs with the same projection
value on the dimension v and differ only in projections onto other dimensions.
(cid:35)
(cid:34)(cid:181)
Optimization scheme based on R ´enyi divergences of different orders have very similar structure. In
(cid:182)α−1
(cid:90)
particular, gradient could be evaluated in a similar way:
Pv (x|spike)
∇vF (α) = α
dxPv (x|spike) [(cid:104)s|x, spike(cid:105) − (cid:104)s|x(cid:105)] d
(cid:82)
α − 1
Pv (x)
dx
where (cid:104)s|x, spike(cid:105) =
ds sδ(x− s ·v)P (s|spike)/P (x|spike), and similarly for (cid:104)s|x(cid:105). The gradient
is thus given by a weighted sum of spike-triggered averages (cid:104)s|x, spike(cid:105) − (cid:104)s|x(cid:105) conditional upon
projection values of stimuli onto the dimension v for which the gradient of information is being
evaluated. The similarity of the structure of both the objective functions and their gradients for
different R ´enyi divergences means that numeric algorithms can be used for optimization of R ´enyi
divergences of different orders. Examples of possible algorithms have been described [1, 2, 11] and
include a combination of gradient ascent and simulated annealing.
Here are a few facts common to this family of optimization schemes. First, as was proved in the case
of information maximization based on Kullback-Leibler divergence [2], the merit function F (α) [v]
does not change with the length of the vector v. Therefore v · ∇vF = 0, as can also be seen directly
from Eq. (10), because v · (cid:104)s|x, spike(cid:105) = x and v · (cid:104)s|x(cid:105) = x. Second, the gradient is 0 when
evaluated along the true receptive ﬁeld. This is because for the true relevant dimension according
to which spikes were generated, (cid:104)s|s1 , spike(cid:105) = (cid:104)s|s1 (cid:105), a consequence of the fact that relevant
projections completely determine the spike probability. Third, merit functions, including variance
and information, can be computed with respect to multiple dimensions by keeping track of stimulus

,

(10)

Pv1 ,v2 (x1 , x2 ) =

(cid:90)
projections on all the relevant dimensions when forming probability distributions (4). For example,
in the case of two dimensions v1 and v2 , we would use
(cid:90)
Pv1 ,v2 (x1 , x2 |spike) =
ds δ(x1 − s · v1 )δ(x2 − s · v2 )P (s|spike),
ds δ(x1 − s · v1 )δ(x2 − s · v2 )P (s),

(cid:82)
compute
respect
variance with
the
to
dx1dx2 [P (x1 , x2 |spike)]2 /P (x1 , x2 ).
If multiple stimulus dimensions are relevant for eliciting the neural response, they can always be
found (provided sufﬁcient number of responses have been recorded) by optimizing the variance
according to Eq. (11) with the correct number of dimensions.
In practice this involves ﬁnding
a single relevant dimension ﬁrst, and then iteratively increasing the number of relevant dimensions
considered while adjusting the previously found relevant dimensions. The amount by which relevant
dimensions need to be adjusted is proportional to the contribution of subsequent relevant dimensions
to neural spiking (the corresponding expression has the same functional form as that for relevant
dimensions found by maximizing information, cf. Appendix B [2]). If stimuli are either uncorrelated
or correlated but Gaussian, then the previously found dimensions do not need to be adjusted when
additional dimensions are introduced. All of the relevant dimensions can be found one by one, by
always searching only for a single relevant dimension in the subspace orthogonal to the relevant
dimensions already found.

as F [v1 , v2 ]

(11)

to

the

two

dimensions

=

3 Illustration for a model simple cell

Here we illustrate how relevant dimensions can be found by maximizing variance (equivalent to least
square ﬁtting), and compare this scheme with that of ﬁnding relevant dimensions by maximizing
information, as well as with those that are based upon computing the spike-triggered average. Our
goal is to reconstruct relevant dimensions of neurons probed with inputs of arbitrary statistics. We
used stimuli derived from a natural visual environment [11] that are known to strongly deviate from
a Gaussian distribution. All of the studies have been carried out with respect to model neurons.
Advantage of doing so is that the relevant dimensions are known. The example model neuron is
taken to mimic properties of simple cells found in the primary visual cortex. It has a single relevant
dimension, which we will denote as ˆe1 . As can be seen in Fig. 1(a), it is phase and orientation
sensitive. In this model, a given stimulus s leads to a spike if the projection s1 = s · ˆe1 reaches a
threshold value θ in the presence of noise: P (spike|s)/P (spike) ≡ g(s1 ) = (cid:104)H (s1 − θ + ξ )(cid:105), where
a Gaussian random variable ξ with variance σ2 models additive noise, and the function H (x) = 1
for x > 0, and zero otherwise. The parameters θ for threshold and the noise variance σ2 determine
the input–output function. In what follows we will measure these parameters in units of the standard
deviation of stimulus projections along the relevant dimension. In these units, the signal-to-noise
ratio is given by σ .
Figure 1 shows that it is possible to obtain a good estimate of the relevant dimension ˆe1 by maxi-
mizing either information, as shown in panel (b), or variance, as shown in panel(c). The ﬁnal value
of the projection depends on the size of the dataset, as will be discussed below. In the example
shown in Fig. 1 there were ≈ 50, 000 spikes with average probability of spike ≈ 0.05 per frame, and
the reconstructed vector has a projection ˆvmax · ˆe1 = 0.98 when maximizing either information or
variance. Having estimated the relevant dimension, one can proceed to sample the nonlinear input–
output function. This is done by constructing histograms for P (s · ˆvmax ) and P (s · ˆvmax |spike) of
projections onto vector ˆvmax found by maximizing either information or variance, and taking their
ratio. Because of the Bayes’ rule, this yields the nonlinear input–output function g of Eq. (1). In
Fig. 1(d) the spike probability of the reconstructed neuron P (spike|s · ˆvmax ) (crosses) is compared
with the probability P (spike|s1 ) used in the model (solid line). A good match is obtained.
In actuality, reconstructing even just one relevant dimension from neural responses to correlated
non-Gaussian inputs, such as those derived from real-world, is not an easy problem. This fact can
be appreciated by considering the estimates of relevant dimension obtained from the spike-triggered
average (STA) shown in panel (e). Correcting the STA by second-order correlations of the input
ensemble through a multiplication by the inverse covariance matrix results in a very noisy estimate,

Figure 1: Analysis of a model visual neuron with one relevant dimension shown in (a). Panels (b)
and (c) show normalized vectors ˆvmax found by maximizing information and variance, respectively;
(d) The probability of a spike P (spike|s · ˆvmax ) (blue crosses – information maximization, red
crosses – variance maximization) is compared to P (spike|s1 ) used in generating spikes (solid line).
Parameters of the model are σ = 0.5 and θ = 2, both given in units of standard deviation of s1 ,
which is also the units for the x-axis in panels (d and h). The spike–triggered average (STA) is shown
in (e). An attempt to remove correlations according to the reverse correlation method, C −1
a priorivsta
(decorrelated STA), is shown in panel (f) and in panel (g) with regularization (see text). In panel
(h), the spike probabilities as a function of stimulus projections onto the dimensions obtained as
decorrelated STA (blue crosses) and regularized decorrelated STA (red crosses) are compared to a
spike probability used to generate spikes (solid line).
shown in panel (f). It has a projection value of 0.25. Attempt to regularize the inverse of covariance
matrix results in a closer match to the true relevant dimension [15, 16, 17, 18, 19] and has a projection
value of 0.8, as shown in panel (g). While it appears to be less noisy, the regularized decorrelated
STA can have systematic deviations from the true relevant dimensions [9, 20, 2, 11]. Preferred
orientation is less susceptible to distortions than the preferred spatial frequency [19]. In this case
regularization was performed by setting aside 1/4 of the data as a test dataset, and choosing a cutoff
on the eigenvalues of the input covariances matrix that would give the maximal information value
on the test dataset [16, 19].

4 Comparison of Performance with Finite Data

In the limit of inﬁnite data the relevant dimensions can be found by maximizing variance, informa-
tion, or other objective functions [1]. In a real experiment, with a dataset of ﬁnite size, the optimal
vector found by any of the R ´enyi divergences ˆv will deviate from the true relevant dimension ˆe1 .
In this section we compare the robustness of optimization strategies based on R ´enyi divergences of
various orders, including least squares ﬁtting (α = 2) and information maximization (α = 1), as the
dataset size decreases and/or neural noise increases.
The deviation from the true relevant dimension δv = ˆv − ˆe1 arises because the probability distri-
butions (4) are estimated from experimental histograms and differ from the distributions found in
the limit of inﬁnite data size. The effects of noise on the reconstruction can be characterized by
taking the dot product between the relevant dimension and the optimal vector for a particular data
sample: ˆv · ˆe1 = 1 − 1
2 δv2 , where both ˆv and ˆe1 are normalized, and δv is by deﬁnition orthogo-
nal to ˆe1 . Assuming that the deviation δv is small, we can use quadratic approximation to expand
the objective function (obtained with ﬁnite data) near its maximum. This leads to an expression
δv = −[H (α) ]−1∇F (α) , which relates deviation δv to the gradient and Hessian of the objective
function evaluated at the vector ˆe1 . Subscript (α) denotes the order of the R ´enyi divergence used
as an objective function. Similarly to the case of optimizing information [2], the Hessian of R ´enyi

102030102030102030102030102030102030102030102030102030102030102030102030-6-4-202460.00.20.40.60.81.0-6-4-202460.00.20.40.60.81.0STAdecorrelated STAdecorrelated STA(a)(b)(c)(d)(e)(f)(g)(h)truthregularizedfiltered stimulus (sd=1)filtered stimulus (sd=1)spike probabilityspike probabilitymaximally informativedimensiondimension ofmaximal variancemaximizinginformation (x)variance  (x)decorrelatedSTA  (x)regularizeddecorrelatedSTA (x)truthB (α)
ij = α2

.

(13)

dxP (x|spike)Cij (x)

(cid:182)(cid:184)2
(cid:184)α−3 (cid:183)
(cid:183)
(cid:181)
(cid:90)
divergence of arbitrary order when evaluated along the optimal dimension ˆe1 is given by
P (x|spike)
P (x|spike)
dxP (x|spike)Cij (x)
ij = −α
d
H (α)
(12)
,
P (x)
P (x)
dx
where Cij (x) = ((cid:104)si sj |x(cid:105) − (cid:104)si |x(cid:105)(cid:104)sj |x(cid:105)) are covariance matrices of inputs sorted by their projec-
tion x along the optimal dimension.
When averaged over possible outcomes of N trials, the gradient is zero for the optimal direction. In
(cid:104)
(cid:105)
(cid:104)∇F (α)∇F (α)T (cid:105) (cid:163)
(cid:164)−2
other words, there is no speciﬁc direction towards which the deviations δv are biased. Next, in order
to measure the expected spread of optimal dimensions around the true one ˆe1 , we need to evaluate
(cid:104)δv2 (cid:105) = Tr
, and therefore need to know the variance of the gradient
H (α)
of F averaged across different equivalent datasets. Assuming that the probability of generating a
(cid:183)
(cid:184)2α−4 (cid:183)
(cid:184)2
(cid:90)
(cid:105) = B (α)
i ∇F (α)
spike is independent for different bins, we ﬁnd that (cid:104)∇F (α)
ij /Nspike , where
j
P (x|spike)
P (x|spike)
d
P (x)
P (x)
dx
Therefore an expected error in the reconstruction of the optimal ﬁlter by maximizing variance is
inversely proportional to the number of spikes:
(cid:104)δv2 (cid:105) = 1 − Tr(cid:48) [BH −2 ]
ˆv · ˆe1 ≈ 1 − 1
(14)
,
2
2Nspike
where we omitted superscripts (α) for clarity. Tr(cid:48) denotes the trace taken in the subspace orthogo-
nal to the relevant dimension (deviations along the relevant dimension have no meaning [2], which
determined by the trace of the Hessian of information, (cid:104)δv2 (cid:105) ∝ Tr(cid:48) (cid:163)
(cid:164)
mathematically manifests itself in dimension ˆe1 being an eigenvector of matrices H and B with the
zero eigenvalue). Note that when α = 1, which corresponds to Kullback-Leibler divergence and
information maximization, A ≡ H α=1 = Bα=1 . The asymptotic errors in this case are completely
A−1
, reproducing the previ-
ously published result for maximally informative dimensions [2]. Qualitatively, the expected error
∼ D/(2Nspike ) increases in proportion to the dimensionality D of inputs and decreases as more
spikes are collected. This dependence is in common with expected errors of relevant dimensions
found by maximizing information [2], as well as methods based on computing the spike-triggered
average both for white noise [1, 21, 22] and correlated Gaussian inputs [2].
Next we examine which of the R ´enyi divergences provides the smallest asymptotic error (14) for
estimating relevant dimensions. Representing the covariance matrix as Cij (x) = γik (x)γjk (x)
(cid:90)
(cid:90)
(exact expression for matrices γ will not be needed), we can express the Hessian matrix H and
covariance matrix for the gradient B as averages with respect to probability distribution P (x|spike):
dxP (x|spike)a(x)bT (x),
dxP (x|spike)b(x)bT (x), H =
B =
(15)
where the gain function g(x) = P (x|spike)/P (x), and matrices bij (x) = αγij (x)g (cid:48) (x) [g(x)]α−2
and aij (x) = γij (x)g (cid:48) (x)/g(x). Cauchy-Schwarz identity for scalar quantities states that,
(cid:104)b2 (cid:105)/(cid:104)ab(cid:105)2 ≥ 1/(cid:104)a2 (cid:105), where the average is taken with respect to some probability distribution.
(cid:90)
A similar result can also be proven for matrices under a Tr operation as in Eq. (14). Applying the
matrix-version of the Cauchy-Schwarz identity to Eq. (14), we ﬁnd that the smallest error is obtained
when
dxP (x|spike)a(x)aT (x),

Tr(cid:48) [BH −2 ] = Tr(cid:48) [A−1 ], with A =

(16)

Matrix A corresponds to the Hessian of the merit function for α = 1: A = H (α=1) . Thus, among the
various optimization strategies based on R ´enyi divergences, Kullback-Leibler divergence (α = 1)
has the smallest asymptotic errors. The least square ﬁtting corresponds to optimization based on
R ´enyi divergence with α = 2, and is expected to have larger errors than optimization based on
Kullback-Leibler divergence (α = 1) implementing information maximization. This result agrees
with recent ﬁndings that Kullback-Leibler divergence is the best distortion measure for performing
lossy compression [3].
Below we use numerical simulations with model cells to compare the performance of information
(α = 1) and variance (α = 2) maximization strategies in the regime of relatively small numbers

of spikes. We are interested in the range 0.1 ∼< D/Nspike ∼< 1, where the asymptotic results do not
necessarily apply. The results of simulations are shown in Fig. 2 as a function of D/Nspike , as well
as with varying neural noise levels. To estimate sharper (less noisy) input/output functions with σ =
1.5, 1.0, 0.5, 0.25, we used larger number of bins (16, 21, 32, 64), respectively. Identical numerical
algorithms, including the number of bins, were used for maximizing variance and information. The
relevant dimension for each simulated spike train was obtained as an average of 4 jackknife estimates
computed by setting aside 1/4 of the data as a test set. Results are shown after 1000 line optimizations
(D = 900), and performance on the test set was checked after every line optimization. As can be
seen, generally good reconstructions with projection values ∼> 0.7 can be obtained by maximizing
either information or variance, even in the severely undersampled regime D < Nspike . We ﬁnd that
reconstruction errors are comparable for both information and variance maximization strategies, and
are better or equal (at very low spike numbers) than STA-based methods. Information maximization
achieves signiﬁcantly smaller errors than the least-square ﬁtting, when we analyze results for all
simulations for four different models cells and spike numbers (p < 10−4 , paired t-test).

Figure 2: Projection of vector ˆvmax obtained by maximizing information (red ﬁlled symbols) or
variance (blue open symbols) on the true relevant dimension ˆe1 is plotted as a function of ratio be-
tween stimulus dimensionality D and the number of spikes Nspike , with D = 900. Simulations were
carried out for model visual neurons with one relevant dimension from Fig. 1(a) and the input-output
function Eq.(1) described by threshold θ = 2.0 and noise standard deviation σ = 1.5, 1.0, 0.5, 0.25
for groups labeled A ((cid:52)), B ((cid:53)), C ((cid:176)), and D (2), respectively. The left panel also shows results
obtained using spike-triggered average (STA, gray) and decorrelated STA (dSTA, black). In the right
panel, we replot results for information and variance optimization together with those for regularized
decorrelated STA (RdSTA, green open symbols). All error bars show standard deviations.
5 Conclusions

In this paper we compared accuracy of a family of optimization strategies for analyzing neural re-
sponses to natural stimuli based on R ´enyi divergences. Finding relevant dimensions by maximizing
one of the merit functions, R ´enyi divergence of order 2, corresponds to ﬁtting the linear-nonlinear
model in the least-square sense to neural spike trains. Advantage of this approach over standard least
square ﬁtting procedure is that it does not require the nonlinear gain function to be invertible. We
derived errors expected for relevant dimensions computed by maximizing R ´enyi divergences of ar-
bitrary order in the asymptotic regime of large spike numbers. The smallest errors were achieved not
in the case of (nonlinear) least square ﬁtting of the linear-nonlinear model to the neural spike trains
(R ´enyi divergence of order 2), but with information maximization (based on Kullback-Leibler di-
vergence). Numeric simulations on the performance of both information and variance maximization
strategies showed that both algorithms performed well even when the number of spikes is very small.
With small numbers of spikes, reconstructions based on information maximization had also slightly,
but signiﬁcantly, smaller errors those of least-square ﬁtting. This makes the problem of ﬁnding rel-
evant dimensions, together with the problem of lossy compression [23, 3], one of examples where

000.51.01.52.02.50.10.20.30.40.50.60.70.80.91.0STAdecorrelated STAmaximizing informationmaximizing varianceABCDABCDD / Nspikeprojection on true dimension00.51.01.52.02.50.50.60.70.80.91.0maximizing informationmaximizing varianceregularized decorrelated STAD / NspikeABCDinformation-theoretic measures are no more data limited than those derived from least squares. It
remains possible, however, that other merit functions based on non-polynomial divergence measures
could provide even smaller reconstruction errors than information maximization.

References
[1] L. Paninski. Convergence properties of three spike-triggered average techniques. Network: Comput.
Neural Syst., 14:437–464, 2003.
[2] T. Sharpee, N.C. Rust, and W. Bialek. Analyzing neural responses to natural signals: Maximally informa-
tiove dimensions. Neural Computation, 16:223–250, 2004. See also physics/0212110, and a preliminary
account in Advances in Neural Information Processing 15 edited by S. Becker, S. Thrun, and K. Ober-
mayer, pp. 261-268 (MIT Press, Cambridge, 2003).
[3] Peter Harremo ¨es and Naftali Tishby. The Information bottleneck revisited or how to choose a good
distortion measure. Proc. of the IEEE Int. Symp. on Information Theory (ISIT), 2007.
[4] E. de Boer and P. Kuyper. Triggered correlation. IEEE Trans. Biomed. Eng., 15:169–179, 1968.
[5] I. W. Hunter and M. J. Korenberg. The identiﬁcation of nonlinear biological systems: Wiener and Ham-
merstein cascade models. Biol. Cybern., 55:135–144, 1986.
[6] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-sensitive neuron in
the blowﬂy visual system: coding and information transfer in short spike sequences. Proc. R. Soc. Lond.
B, 265:259–265, 1988.
[7] V. Z. Marmarelis. Modeling Methodology for Nonlinear Physiological Systems. Ann. Biomed. Eng.,
25:239–251, 1997.
[8] W. Bialek and R. R. de Ruyter van Steveninck. Features and dimensions: Motion estimation in ﬂy vision.
q-bio/0505003, 2005.
[9] D. L. Ringach, G. Sapiro, and R. Shapley. A subspace reverse-correlation technique for hte study of visual
neurons. Vision Res., 37:2455–2464, 1997.
[10] D. L. Ruderman and W. Bialek. Statistics of natural images: scaling in the woods. Phys. Rev. Lett.,
73:814–817, 1994.
[11] T. O. Sharpee, H. Sugihara, A. V. Kurgansky, S. P. Rebrik, M. P. Stryker, and K. D. Miller. Adaptive
ﬁltering enhances information transmission in visual cortex. Nature, 439:936–942, 2006.
[12] S. M. Ali and S. D. Silvey. A general class of coefﬁceint of divergence of one distribution from another.
J. R. Statist. Soc. B, 28:131–142, 1966.
[13] I. Csisz ´ar. Information-type measures of difference of probability distrbutions and indirect observations.
Studia Sci. Math. Hungar., 2:299–318, 1967.
[14] N. Brenner, S. P. Strong, R. Koberle, W. Bialek, and R. R. de Ruyter van Steveninck. Synergy in a neural
code. Neural Computation, 12:1531–1552, 2000. See also physics/9902067.
[15] F. E. Theunissen, K. Sen, and A. J. Doupe. Spectral-temporal receptive ﬁelds of nonlinear auditory
neurons obtained using natural sounds. J. Neurosci., 20:2315–2331, 2000.
[16] F.E. Theunissen, S.V. David, N.C. Singh, A. Hsu, W.E. Vinje, and J.L. Gallant. Estimating spatio-temporal
receptive ﬁelds of auditory and visual neurons from their responses to natural stimuli. Network, 3:289–
316, 2001.
[17] K. Sen, F. E. Theunissen, and A. J. Doupe. Feature analysis of natural sounds in the songbird auditory
forebrain. J. Neurophysiol., 86:1445–1458, 2001.
[18] D. Smyth, B. Willmore, G. E. Baker, I. D. Thompson, and D. J. Tolhurst. The receptive ﬁelds organization
of simple cells in the primary visual cortex of ferrets under natural scene stimulation. J. Neurosci.,
23:4746–4759, 2003.
[19] G. Felsen, J. Touryan, F. Han, and Y. Dan. Cortical sensitivity to visual features in natural scenes. PLoS
Biol., 3:1819–1828, 2005.
[20] D. L. Ringach, M. J. Hawken, and R. Shapley. Receptive ﬁeld structure of neurons in monkey visual
cortex revealed by stimulation with natural image sequences. Journal of Vision, 2:12–24, 2002.
[21] N. C. Rust, O. Schwartz, J. A. Movshon, and E. P. Simoncelli. Spatiotemporal elements of macaque V1
receptive ﬁelds. Neuron, 46:945–956, 2005.
[22] Schwartz O., J.W. Pillow, N.C. Rust, and E. P. Simoncelli. Spike-triggered neural characterization. Jour-
nal of Vision, 176:484–507, 2006.
[23] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In B. Hajek and R. S. Sreeni-
vas, editors, Proceedings of the 37th Allerton Conference on Communication, Control and Computing, pp
368–377. University of Illinois, 1999. See also physics/0004057.


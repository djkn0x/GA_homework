Adaptive Online Gradient Descent

Peter L. Bartlett
Division of Computer Science
Department of Statistics
UC Berkeley
Berkeley, CA 94709
bartlett@cs.berkeley.edu

Elad Hazan
IBM Almaden Research Center
650 Harry Road
San Jose, CA 95120
hazan@us.ibm.com

Alexander Rakhlin ∗
Division of Computer Science
UC Berkeley
Berkeley, CA 94709
rakhlin@cs.berkeley.edu

Abstract

We study the rates of growth of the regret in online convex optimization. First,
we show that a simple extension of the algorithm of Hazan et al eliminates the
need for a priori knowledge of the lower bound on the second derivatives of the
observed functions. We then provide an algorithm, Adaptive Online Gradient
Descent, which interpolates between the results of Zinkevich for linear functions
√
and of Hazan et al for strongly convex functions, achieving intermediate rates
T and log T . Furthermore, we show strong optimality of the algorithm.
between
Finally, we provide an extension of our results to general norms.

1 Introduction
The problem of online convex optimization can be formulated as a repeated game between a player
t=1 ft (xt ), is not much larger than the smallest total loss PT
loss, PT
and an adversary. At round t, the player chooses an action xt from some convex subset K of Rn ,
and then the adversary chooses a convex loss function ft . The player aims to ensure that the total
t=1 ft (x) of any ﬁxed action x.
The difference between the total loss and its optimal value for a ﬁxed action is known as the regret,
TX
TX
which we denote
t=1
t=1

ft (xt ) − min
x∈K

RT =

ft (x).

Many problems of online prediction of individual sequences can be viewed as special cases of online
convex optimization, including prediction with expert advice, sequential probability assignment,
and sequential investment [1]. A central question in all these cases is how the regret grows with the
number of rounds of the game.
√
Zinkevich [2] considered the following gradient descent algorithm, with step size ηt = Θ(1/
(Here, ΠK (v) denotes the Euclidean projection of v on to the convex set K .)

t).

∗Corresponding author.

1

Algorithm 1 Online Gradient Descent (OGD)
1: Initialize x1 arbitrarily.
2: for t = 1 to T do
Predict xt , observe ft .
3:
Update xt+1 = ΠK (xt − ηt+1∇ft (xt )).
4:
5: end for

RT ≤ c

λt + p(λ1 , . . . , λT ).

√
T , where T is the number of rounds
Zinkevich showed that the regret of this algorithm grows as
of the game. This rate cannot be improved in general for arbitrary convex loss functions. However,
this is not the case if the loss functions are uniformly convex, for instance, if all ft have second
derivative at least H > 0. Recently, Hazan et al [3] showed that in this case it is possible for the
regret to grow only logarithmically with T , using the same algorithm but with the smaller step size
ηt = 1/(H t). Increasing convexity makes online convex optimization easier.
The algorithm that achieves logarithmic regret must know in advance a lower bound on the convexity
of the loss functions, since this bound is used to determine the step size. It is natural to ask if this is
√
essential: is there an algorithm that can adapt to the convexity of the loss functions and achieve the
same regret rates in both cases—O(log T ) for uniformly convex functions and O(
T ) for arbitrary
convex functions? In this paper, we present an adaptive algorithm of this kind.
The key technique is regularization: We consider the online gradient descent (OGD) algorithm,
but we add a uniformly convex function, the quadratic λtkxk2 , to each loss function ft (x). This
TX
corresponds to shrinking the algorithm’s actions xt towards the origin. It leads to a regret bound of
the form
t=1
The ﬁrst term on the right hand side can be viewed as a bias term; it increases with λt because
the presence of the regularization might lead the algorithm away from the optimum. The second
term is a penalty for the ﬂatness of the loss functions that becomes smaller as the regularization
increases. We show that choosing the regularization coefﬁcient λt so as to balance these two terms
√
in the bound on the regret up to round t is nearly optimal in a strong sense. Not only does this choice
T and log T regret rates in the linear and uniformly convex cases, it leads to a kind of
give the
oracle inequality: The regret is no more than a constant factor times the bound on regret that would
have been suffered if an oracle had provided in advance the sequence of regularization coefﬁcients
λ1 , . . . , λT that minimizes the ﬁnal regret bound.
To state this result precisely, we introduce the following deﬁnitions. Let K be a convex subset of
Rn and suppose that supx∈K kxk ≤ D . For simplicity, throughout the paper we assume that K is
centered around 0, and, hence, 2D is the diameter of K . Deﬁne a shorthand ∇t = ∇ft (xt ). Let Ht
be the largest value such that for any x∗ ∈ K ,
kx∗ − xtk2 .
t (x∗ − xt ) + Ht
ft (x∗ ) ≥ ft (xt ) + ∇>
(1)
s=1 λs and H 1:t := Pt
k∇tk ≤ Gt . Deﬁne λ1:t := Pt
2
In particular, if ∇2 ft − Ht · I (cid:23) 0, then the above inequality is satisﬁed. Furthermore, suppose
s=1 Hs . Let H 1:0 = 0. Let us now state the
Adaptive Online Gradient Descent algorithm as well as the theoretical guarantee for its performance.
Algorithm 2 Adaptive Online Gradient Descent
1: Initialize x1 arbitrarily.
(cid:18)q
(cid:19)
2: for t = 1 to T do
Predict xt , observe ft .
3:
t /(3D2 ) − (H 1:t + λ1:t−1 )
(H 1:t + λ1:t−1 )2 + 8G2
Compute λt = 1
2
Compute ηt+1 = (H 1:t + λ1:t )−1 .
5:
Update xt+1 = ΠK (xt − ηt+1 (∇ft (xt ) + λtxt )).
6:
7: end for

4:

.

2

 
TX
Theorem 1.1. The regret of Algorithm 2 is bounded by
∗
1:T +
D2λ
t=1

RT ≤ 3

inf
1 ,...,λ∗
λ∗
T

!

.

(Gt + λ∗
t D)2
∗
H 1:t + λ
1:t

While Algorithm 2 is stated with the squared Euclidean norm as a regularizer, we show that it
is straightforward to generalize our technique to other regularization functions that are uniformly
convex with respect to other norms. This leads to adaptive versions of the mirror descent algorithm
analyzed recently in [4, 5].

2 Preliminary results

.

G2
t
H 1:t

In particular, loosening the bound,
2RT ≤

The following theorem gives a regret bound for the OGD algorithm with a particular choice of step
size. The virtue of the theorem is that the step size can be set without knowledge of the uniform
lower bound on Ht , which is required in the original algorithm of [3]. The proof is provided in
Section 4 (Theorem 4.1), where the result is extended to arbitrary norms.
Theorem 2.1. Suppose we set ηt+1 = 1
. Then the regret of OGD is bounded as
TX
H 1:t
RT ≤ 1
2
t=1
Pt
maxt G2
t
mint
1
s=1 Hs
t
Pt
Note that nothing prevents Ht from being negative or zero, implying that the same algorithm gives
logarithmic regret even when some of the functions are linear or concave, as long as the partial
s=1 Hs are positive and not too small. The above result already provides an important
averages 1
problem with the algorithm. If H1 > 0 and Ht = 0 for all t > 1, then Pt
t
extension to the log-regret algorithm of [3]: no prior knowledge on the uniform convexity of the
functions is needed, and the bound is in terms of the observed sequence {Ht}. Yet, there is still a
√
s=1 Hs = H1 , resulting
√
T ) bound can be obtained. In the
in a linear regret bound. However, we know from [2] that a O(
next section we provide an algorithm which interpolates between O(log T ) and O(
T ) bound on
the regret depending on the curvature of the observed functions.

log T .

3 Adaptive Regularization
Suppose the environment plays a sequence of ft ’s with curvature Ht ≥ 0. Instead of performing
gradient descent on these functions, we step in the direction of the gradient of ˜ft (x) = ft (x) +
2 λtkxk2 , where the regularization parameter λt ≥ 0 is chosen appropriately at each step as a
1
function of the curvature of the previous functions. We remind the reader that K is assumed to be
centered around the origin, for otherwise we would instead use kx − x0 k2 to shrink the actions xt
towards the origin x0 . Applying Theorem 2.1, we obtain the following result.
Theorem 3.1. If the Online Gradient Descent algorithm is performed on the functions ˜ft (x) =
2 λtkxk2 with
ft (x) + 1

ηt+1 =

1
H 1:t + λ1:t
TX
for any sequence of non-negative λ1 , . . . , λT , then
RT ≤ 1
2 D2λ1:T +
t=1

1
2

(Gt + λtD)2
H 1:t + λ1:t

.

3

,

+

ft (xt ) +

ft (x) +

.

!

+

2G2
t
H 1:t + λ1:t

(Gt + λtD)2
H 1:t + λ1:t

2λ2
t D2
H 1:t + λ1:t−1 + λt

  TX
(cid:18)
(cid:19)
TX
TX
Proof. By Theorem 2.1 applied to functions ˜ft ,
(Gt + λtD)2
1
1
1
2 λtkxk2
≤ min
2 λtkxtk2
H 1:t + λ1:t
2
x
t=1
t=1
t=1
PT
Indeed, it is easy to verify that condition (1) for ft implies the corresponding statement with ˜Ht =
Ht +λt for ˜ft . Furthermore, by linearity, the bound on the gradient of ˜ft is Gt +λtkxtk ≤ Gt +λtD .
t=1 ft (x). Then, dropping the kxtk2 terms and bounding kx∗ k2 ≤ D2 ,
TX
TX
ft (xt ) ≤ TX
Deﬁne x∗ = arg minx
1
(Gt + λtD)2
1
ft (x∗ ) +
2 D2λ1:T +
H 1:t + λ1:t
2
t=1
t=1
t=1
which proves the the theorem.
The following inequality is important in the rest of the analysis, as it allows us to remove the de-
 
!
(cid:19)
(cid:18)
pendence on λt from the numerator of the second sum at the expense of increased constants. We
TX
TX
have
1
1
≤ 1
2 D2λ1:T +
D2λ1:T +
TX
2
2
t=1
t=1
≤ 3
G2
2 D2λ1:T +
t
,
H 1:t + λ1:t
t=1
where the ﬁrst inequality holds because (a + b)2 ≤ 2a2 + 2b2 for any a, b ∈ R.
√
It turns out that for appropriate choices of {λt}, the above theorem recovers the O(
T ) bound on the
regret for linear functions [2] and the O(log T ) bound for strongly convex functions [3]. Moreover,
under speciﬁc assumptions on the sequence {Ht}, we can deﬁne a sequence {λt} which produces
√
intermediate rates between log T and
T . These results are exhibited in corollaries at the end of
this section.
Of course, it would be nice to be able to choose {λt} adaptively without any restrictive assump-
cal balancing. Observe that the upper bound of Eq. (2) consists of two sums: D2 PT
tions on {Ht}. Somewhat surprisingly, such a choice can be made near-optimally by simple lo-
PT
t=1 λt and
G2
. The ﬁrst sum increases in any particular λt and the other decreases. While the
t
t=1
H 1:t+λ1:t
inﬂuence of the regularization parameters λt on the ﬁrst sum is trivial, the inﬂuence on the second
sum is more involved as all terms for t ≥ t0 depend on λt0 . Nevertheless, it turns out that a simple
choice of λt is optimal to within a multiplicative factor of 2. This is exhibited by the next lemma.
TX
Lemma 3.1. Deﬁne
HT ({λt}) = HT (λ1 . . . λT ) = λ1:T +
t=1
where Ct ≥ 0 does not depend on λt ’s. If λt satisﬁes λt =
Ct
H 1:t+λ1:t
HT ({λt}) ≤ 2 inf
HT ({λ∗
t }).
{λ∗
t }≥0
t } be the optimal sequence of non-negative regularization
Proof. We prove this by induction. Let {λ∗
coefﬁcients. The base of the induction is proved by considering two possibilities: either λ1 < λ∗
1 or
1 ≤ 2(λ∗
not. In the ﬁrst case, λ1 + C1 /(H1 + λ1 ) = 2λ1 ≤ 2λ∗
1 + C1 /(H1 + λ∗
1 )). The other case
is proved similarly.
Now, suppose

Ct
H 1:t + λ1:t

,

for t = 1, . . . , T , then

(2)

HT −1 ({λt}) ≤ 2HT −1 ({λ∗
t }).
∗
TX
Consider two possibilities. If λ1:T < λ
1:T , then
HT ({λt}) = λ1:T +
t=1

Ct
H 1:t + λ1:t

= 2λ1:T ≤ 2λ
1:T ≤ 2HT ({λ∗
t }).
∗

4

If, on the other hand, λ1:T ≥ λ
∗
1:T , then
Ct
Ct
Ct
λT +
= 2
∗
H 1:T + λ1:T
H 1:T + λ1:T
H 1:T + λ
1:T
Using the inductive assumption, we obtain
HT ({λt}) ≤ 2HT ({λ∗
t }).

≤ 2

(cid:18)

≤ 2

λ∗
T +

Ct
∗
H 1:T + λ
1:T

(cid:19)

.

!

TX
t=1

G2
≤ inf
t
TX
1 ,...,λ∗
λ∗
H 1:t + λ1:t
T
(Gt + λ∗
t D)2
∗
H 1:t + λ
1:t
t=1

The lemma above is the key to the proof of the near-optimal bounds for Algorithm 2 1 .
 
∗
!
3D2λ
1:T + 2

Proof. (of Theorem 1.1)
TX
By Eq. 2 and Lemma 3.1,
RT ≤ 3
 
2 D2λ1:T +
t=1
1
1
∗
1:T +
inf
2 D2λ
1 ,...,λ∗
λ∗
2
T
provided the λt are chosen as solutions to
3
2 D2λt =
(cid:19)
(cid:18)q
It is easy to verify that
1
t /(3D2 ) − (H 1:t + λ1:t−1 )
(H 1:t + λ1:t−1 )2 + 8G2
2
is the non-negative root of the above quadratic equation. We note that division by zero in Algorithm 2
occurs only if λ1 = H1 = G1 = 0. Without loss of generality, G1 6= 0, for otherwise x1 is
minimizing f1 (x) and regret is negative on that round.

G2
t
H 1:t + λ1:t−1 + λt

G2
t
∗
H 1:t + λ
1:t

,

.

≤ 6

λt =

(3)

Hence, the algorithm has a bound on the performance which is 6 times the bound obtained by the
best ofﬂine adaptive choice of regularization coefﬁcients. While the constant 6 might not be optimal,
it can be shown that a constant strictly larger than one is unavoidable (see previous footnote).
We also remark that if the diameter D is unknown, the regularization coefﬁcients λt can still be
chosen by balancing as in Eq. (3), except without the D2 term. This choice of λt , however, increases
the bound on the regret suffered by Algorithm 2 by a factor of O(D2 ).
Let us now consider some special cases and show that Theorem 1.1 not only recovers the rate of
increase of regret of [3] and [2], but also provides intermediate rates. For each of these special cases,
we provide a sequence of {λt} which achieves the desired rates. Since Theorem 1.1 guarantees that
Algorithm 2 is competitive with the best choice of the parameters, we conclude that Algorithm 2
achieves the same rates.
Corollary 3.1. Suppose Gt ≤ G for all 1 ≤ t ≤ T . Then for any sequence of convex functions
√
{ft}, the bound on regret of Algorithm 2 is O(
T ).
√
 
!
TX
TX
T and λt = 0 for 1 < t ≤ T . By Eq. 2,
Proof. Let λ1 =
1
≤ 3
(Gt + λtD)2
2 D2λ1:T +
D2λ1:T +
TX
H 1:t + λ1:t
2
t=1
t=1
√
≤ 3
G2√
2 D2
T
t=1

G2
(cid:18) 3
t
H 1:t + λ1:t
2 D2 + G2

(cid:19) √

T +

T .

=

1Lemma 3.1 effectively describes an algorithm for an online problem with competitive ratio of 2. In the full
version of this paper we give a lower bound strictly larger than one on the competitive ratio achievable by any
online algorithm for this problem.

5

√

G2
t
H 1:t

≤ 1
2

tH ≤ G
2H (log T + 1).
G

Proof. Set λt = 0 for all t. It holds that RT ≤ 1
2

Hence, the regret of Algorithm 2 can never increase faster than
T . We now consider the assump-
tions of [3].
Corollary 3.2. Suppose Ht ≥ H > 0 and G2
t < G for all 1 ≤ t ≤ T . Then the bound on regret of
PT
PT
Algorithm 2 is O(log T ).
t=1
t=1
The above proof also recovers the result of Theorem 2.1. The following Corollary shows a spectrum
of rates under assumptions on the curvature of functions.
Corollary 3.3. Suppose Ht = t−α and Gt ≤ G for all 1 ≤ t ≤ T .
1. If α = 0, then RT = O(log T ).
√
2. If α > 1/2, then RT = O(
T ).
3. If 0 < α ≤ 1/2, then RT = O(T α ).
s=1 Hs ≥ R t−1
λ1 = T α and λt = 0 for 1 < t ≤ T . Note that Pt
Proof. The ﬁrst two cases follow immediately from Corollaries 3.1 and 3.2. For the third case, let
x=0 (x + 1)−αdx = (1 − α)−1 t1−α −
 
!
(1 − α)−1 . Hence,
TX
TX
≤ 3
D2λ1:T +
2 D2λ1:T +
t=1
t=1
≤ 2D2T α + G2 (1 − α)

(Gt + λtD)2
H 1:t + λ1:t

1
2

G2
t
TX
H 1:t + λ1:t
t=1

1
t1−α − 1

≤ 2D2T α + 2G2 1
α

T α + O(1) = O(T α ).

4 Generalization to different norms

The original online gradient descent (OGD) algorithm as analyzed by Zinkevich [2] used the Eu-
clidean distance of the current point from the optimum as a potential function. The logarithmic
regret bounds of [3] for strongly convex functions were also stated for the Euclidean norm, and
such was the presentation above. However, as observed by Shalev-Shwartz and Singer in [5], the
proof technique of [3] extends to arbitrary norms. As such, our results above for adaptive regular-
ization carry on to the general setting, as we state below . Our notation follows that of Gentile and
Warmuth [6].
Deﬁnition 4.1. A function g over a convex set K is called H -strongly convex with respect to a
convex function h if

∀x, y ∈ K . g(x) ≥ g(y) + ∇g(y)> (x − y) + H
2 Bh (x, y).
Here Bh (x, y) is the Bregman divergence with respect to the function h, deﬁned as
Bh (x, y) = h(x) − h(y) − ∇h(y)> (x − y).
the function g(x) = kxk2
2 is
This notion of strong convexity generalizes the Euclidean notion:
strongly convex with respect to h(x) = kxk2
2 (in this case Bh (x, y) = kx − yk2
2 ). More gen-
erally, the Bregman divergence can be thought of as a squared norm, not necessarily Euclidean,
i.e., Bh (x, y) = kx − yk2 . Henceforth we also refer to the dual norm of a given norm, deﬁned
by kyk∗ = supkxk≤1 {y>x}. For the case of ‘p norms, we have kyk∗ = kykq where q satisﬁes
q = 1, and by H ¨older’s inequality y>x ≤ kyk∗ kxk ≤ 1
2 kyk2∗ + 1
2 kxk2 (this holds for norms
p + 1
1
other than ‘p as well).

6

For simplicity, the reader may think of the functions g , h as convex and differentiable2 . The follow-
ing algorithm is a generalization of the OGD algorithm to general strongly convex functions (see
the derivation in [6]). In this extended abstract we state the update rule implicitly, leaving the issues
of efﬁcient computation for the full version (these issues are orthogonal to our discussion, and were
addressed in [6] for a variety of functions h).

Algorithm 3 General-Norm Online Gradient Descent
1: Input: convex function h
2: Initialize x1 arbitrarily.
3: for t = 1 to T do
Predict xt , observe ft .
4:
Compute ηt+1 and let yt+1 be such that ∇h(yt+1 ) = ∇h(xt ) − 2ηt+1∇ft (xt ).
5:
Let xt+1 = arg minx∈K Bh (x, yt+1 ) be the projection of yt+1 onto K .
6:
7: end for

The methods of the previous sections can now be used to derive similar, dynamically optimal, bounds
on the regret. As a ﬁrst step, let us generalize the bound of [3], as well as Theorem 2.1, to general
norms:
Theorem 4.1. Suppose that, for each t, ft is a Ht -strongly convex function with respect to h, and let
h be such that Bh (x, y) ≥ kx − yk2 for some norm k · k. Let k∇ft (xt )k∗ ≤ Gt for all t. Applying
TX
the General-Norm Online Gradient Algorithm with ηt+1 = 1
, we have
H 1:t
RT ≤ 1
2
t=1

G2
t
H 1:t

.

Proof. The proof follows [3], with the Bregman divergence replacing the Euclidean distance as a
potential function. By assumption on the functions ft , for any x∗ ∈ K ,
ft (xt ) − ft (x∗ ) ≤ ∇ft (xt )> (xt − x∗ ) − Ht
2 Bh (x∗ , xt ).
By a well-known property of Bregman divergences (see [6]), it holds that for any vectors x, y , z ,
(x − y)> (∇h(z ) − ∇h(y)) = Bh (x, y) − Bh (x, z ) + Bh (y , z ).
Combining both observations,
2(ft (xt ) − ft (x∗ )) ≤ 2∇ft (xt )> (xt − x∗ ) − HtBh (x∗ , xt )
1
(∇h(yt+1 ) − ∇h(xt ))> (x∗ − xt ) − HtBh (x∗ , xt )
=
ηt+1
1
[Bh (x∗ , xt ) − Bh (x∗ , yt+1 ) + Bh (xt , yt+1 )] − HtBh (x∗ , xt )
=
ηt+1
≤ 1
[Bh (x∗ , xt ) − Bh (x∗ , xt+1 ) + Bh (xt , yt+1 )] − HtBh (x∗ , xt ),
ηt+1
where the last inequality follows from the Pythagorean Theorem for Bregman divergences [6], as
xt+1 is the projection w.r.t the Bregman divergence of yt+1 and x∗ ∈ K is in the convex set.
(cid:19)
(cid:18) 1
(cid:19)
(cid:18) 1
TX
2RT ≤ TX
Summing over all iterations and recalling that ηt+1 = 1
,
H 1:t
− 1
+ Bh (x∗ , x1 )
TX
ηt+1
ηt
η2
t=1
t=2
t=1
2Since the set of points of nondifferentiability of convex functions has measure zero, convexity is the only
property that we require. Indeed, for nondifferentiable functions, the algorithm would choose a point ˜xt , which
is xt with the addition of a small random perturbation. With probability one, the functions would be smooth
at the perturbed point, and the perturbation could be made arbitrarily small so that the regret rate would not be
affected.

Bh (x∗ , xt )

Bh (xt , yt+1 ).

Bh (xt , yt+1 )

− H1

+

=

1
ηt+1

1
ηt+1

(4)

− Ht

7

We proceed to bound Bh (xt , yt+1 ). By deﬁnition of Bregman divergence, and the dual norm in-
equality stated before,
Bh (xt , yt+1 ) + Bh (yt+1 , xt ) = (∇h(xt ) − ∇h(yt+1 ))> (xt − yt+1 )
= 2ηt+1∇ft (xt )> (xt − yt+1 )
≤ η2
t+1 k∇tk2∗ + kxt − yt+1 k2 .
Thus, by our assumption Bh (x, y) ≥ kx − yk2 , we have
t+1 k∇tk2∗ + kxt − yt+1 k2 − Bh (yt+1 , xt ) ≤ η2
Bh (xt , yt+1 ) ≤ η2
t+1 k∇tk2∗ .
TX
TX
Plugging back into Eq. (4) we get
RT ≤ 1
2
t=1
t=1

t =
ηt+1G2

G2
t
H 1:t

1
2

.

The generalization of our technique is now straightforward. Let A2 = supx∈K g(x) and 2B =
supx∈K k∇g(x)k∗ . The following algorithm is an analogue of Algorithm 2 and Theorem 4.2 is the
analogue of Theorem 1.1 for general norms.

Algorithm 4 Adaptive General-Norm Online Gradient Descent
1: Initialize x1 arbitrarily. Let g(x) be 1-strongly convex with respect to the convex function h.
(cid:18)q
(cid:19)
2: for t = 1 to T do
Predict xt , observe ft
3:
t /(A2 + 2B 2 ) − (H 1:t + λ1:t−1 )
(H 1:t + λ1:t−1 )2 + 8G2
Compute λt = 1
2
Compute ηt+1 = (H 1:t + λ1:t )−1 .
5:
2 ∇g(xt ))).
Let yt+1 be such that ∇h(yt+1 ) = ∇h(xt ) − 2ηt+1 (∇ft (xt ) + λt
6:
Let xt+1 = arg minx∈K Bh (x, yt+1 ) be the projection of yt+1 onto K .
7:
8: end for

4:

.

Theorem 4.2. Suppose that each ft is a Ht -strongly convex function with respect to h, and let g be
a 1-strongly convex with respect h. Let h be such that Bh (x, y) ≥ kx − yk2 for some norm k · k. Let
!
 
TX
k∇ft (xt )k∗ ≤ Gt . The regret of Algorithm 4 is bounded by
(Gt + λ∗
t B )2
RT ≤ inf
∗
(A2 + 2B 2 )λ
1:T +
∗
1 ,...,λ∗
λ∗
H 1:t + λ
1:t
T
t=1
If the norm in the above theorem is the Euclidean norm and g(x) = kxk2 , we ﬁnd that D =
supx∈K kxk = A = B and recover the results of Theorem 1.1.

.

References
[1] Nicol `o Cesa-Bianchi and G ´abor Lugosi. Prediction, Learning, and Games. Cambridge University Press,
2006.
[2] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML,
pages 928–936, 2003.
[3] Elad Hazan, Adam Kalai, Satyen Kale, and Amit Agarwal. Logarithmic regret algorithms for online convex
optimization. In COLT, pages 499–513, 2006.
[4] Shai Shalev-Shwartz and Yoram Singer. Convex repeated games and Fenchel duality. In B. Sch ¨olkopf,
J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press,
Cambridge, MA, 2007.
[5] Shai Shalev-Shwartz and Yoram Singer. Logarithmic regret algorithms for strongly convex repeated games.
In Technical Report 2007-42. The Hebrew University, 2007.
[6] C. Gentile and M. K. Warmuth. Proving relative loss bounds for on-line learning algorithms using Bregman
divergences. In COLT. Tutorial, 2000.

8


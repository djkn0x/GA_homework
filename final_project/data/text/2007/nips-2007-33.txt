PSVM: Parallelizing Support Vector Machines
on Distributed Computers

Edward Y. Chang∗, Kaihua Zhu, Hao Wang, Hongjie Bai,
Jian Li, Zhihuan Qiu, & Hang Cui
Google Research, Beijing, China

Abstract

Support Vector Machines (SVMs) suffer from a widely recognized scalability
problem in both memory use and computational time. To improve scalability,
we have developed a parallel SVM algorithm (PSVM), which reduces memory
use through performing a row-based, approximate matrix factorization, and which
loads only essential data to each machine to perform parallel computation. Let n
denote the number of training instances, p the reduced matrix dimension after
factorization (p is signiﬁcantly smaller than n), and m the number of machines.
PSVM reduces the memory requirement from O(n2 ) to O(np/m), and improves
computation time to O(np2 /m). Empirical study shows PSVM to be effective.
PSVM Open Source is available for download at http://code.google.com/p/psvm/.

1 Introduction

Let us examine the resource bottlenecks of SVMs in a binary classiﬁcation setting to explain our
proposed solution. Given a set of training data X = {(xi , yi )|xi ∈ Rd}n
i=1 , where xi is an obser-
vation vector, yi ∈ {−1, 1} is the class label of xi , and n is the size of X , we apply SVMs on X to
train a binary classiﬁer. SVMs aim to search a hyperplane in the Reproducing Kernel Hilbert Space
(RKHS) that maximizes the margin between the two classes of data in X with the smallest train-
n(cid:88)
ing error (Vapnik, 1995). This problem can be formulated as the following quadratic optimization
problem:
1
(cid:107)w(cid:107)2
min P (w, b, ξ) =
2 + C
2
i=1
s.t. 1 − yi (wT φ(xi ) + b) ≤ ξi ,
ξi > 0,
where w is a weighting vector, b is a threshold, C a regularization hyperparameter, and φ(·) a basis
function which maps xi to an RKHS space. The decision function of SVMs is f (x) = wT φ(x) + b,
where the w and b are attained by solving P in (1). The optimization problem in (1) is the primal
formulation of SVMs. It is hard to solve P directly, partly because the explicit mapping via φ(·)
can make the problem intractable and partly because the mapping function φ(·) is often unknown.
The method of Lagrangian multipliers is thus introduced to transform the primal formulation into
the dual one

(1)

ξi

1
min D(α) =
2 αT Qα − αT 1
s.t. 0 ≤ α ≤ C, yT α = 0,
(cid:80)n
where [Q]ij = yi yj φT (xi )φ(xj ), and α ∈ Rn is the Lagrangian multiplier variable (or dual
variable). The weighting vector w is related with α in w =
i=1 αiφ(xi ).
∗This work was initiated in 2005 when the author was a professor at UCSB.

(2)

1

The dual formulation D(α) requires an inner product of φ(xi ) and φ(xj ). SVMs utilize the kernel
trick by specifying a kernel function to deﬁne the inner-product K (xi , xj ) = φT (xi )φ(xj ). We
thus can rewrite [Q]ij as yi yj K (xi , xj ). When the given kernel function K is psd (positive semi-
deﬁnite), the dual problem D(α) is a convex Quadratic Programming (QP) problem with linear
constraints, which can be solved via the Interior-Point method (IPM) (Mehrotra, 1992). Both the
computational and memory bottlenecks of the SVM training are the IPM solver to the dual formu-
lation of SVMs in (2).
Currently, the most effective IPM algorithm is the primal-dual IPM (Mehrotra, 1992). The principal
idea of the primal-dual IPM is to remove inequality constraints using a barrier function and then
resort to the iterative Newton’s method to solve the KKT linear system related to the Hessian matrix
Q in D(α). The computational cost is O(n3 ) and the memory usage O(n2 ).
In this work, we propose a parallel SVM algorithm (PSVM) to reduce memory use and to parallelize
both data loading and computation. Given n training instances each with d dimensions, PSVM ﬁrst
loads the training data in a round-robin fashion onto m machines. The memory requirement per
machine is O(nd/m). Next, PSVM performs a parallel row-based Incomplete Cholesky Factor-
ization (ICF) on the loaded data. At the end of parallel ICF, each machine stores only a fraction
of the factorized matrix, which takes up space of O(np/m), where p is the column dimension of
√
the factorized matrix. (Typically, p can be set to be about
n without noticeably degrading train-
ing accuracy.) PSVM reduces memory use of IPM from O(n2 ) to O(np/m), where p/m is much
smaller than n. PSVM then performs parallel IPM to solve the quadratic optimization problem
in (2). The computation time is improved from about O(n2 ) of a decomposition-based algorithm
(e.g., SVMLight (Joachims, 1998), LIBSVM (Chang & Lin, 2001), SMO (Platt, 1998), and Sim-
pleSVM (Vishwanathan et al., 2003)) to O(np2 /m). This work’s main contributions are: (1) PSVM
achieves memory reduction and computation speedup via a parallel ICF algorithm and parallel IPM.
(2) PSVM handles kernels (in contrast to other algorithmic approaches (Joachims, 2006; Chu et al.,
2006)). (3) We have implemented PSVM on our parallel computing infrastructures. PSVM effec-
tively speeds up training time for large-scale tasks while maintaining high training accuracy.
PSVM is a practical, parallel approximate implementation to speed up SVM training on today’s
distributed computing infrastructures for dealing with Web-scale problems. What we do not claim
are as follows: (1) We make no claim that PSVM is the sole solution to speed up SVMs. Algorithmic
approaches such as (Lee & Mangasarian, 2001; Tsang et al., 2005; Joachims, 2006; Chu et al.,
2006) can be more effective when memory is not a constraint or kernels are not used. (2) We do not
claim that the algorithmic approach is the only avenue to speed up SVM training. Data-processing
approaches such as (Graf et al., 2005) can divide a serial algorithm (e.g., LIBSVM) into subtasks
on subsets of training data to achieve good speedup. (Data-processing and algorithmic approaches
complement each other, and can be used together to handle large-scale training.)

2 PSVM Algorithm

The key step of PSVM is parallel ICF (PICF). Traditional column-based ICF (Fine & Scheinberg,
2001; Bach & Jordan, 2005) can reduce computational cost, but the initial memory requirement
is O(np), and hence not practical for very large data set. PSVM devises parallel row-based ICF
(PICF) as its initial step, which loads training instances onto parallel machines and performs factor-
ization simultaneously on these machines. Once PICF has loaded n training data distributedly on m
machines, and reduced the size of the kernel matrix through factorization, IPM can be solved on par-
allel machines simultaneously. We present PICF ﬁrst, and then describe how IPM takes advantage
of PICF.

2.1 Parallel ICF
ICF can approximate Q (Q ∈ Rn×n ) by a smaller matrix H (H ∈ Rn×p , p (cid:191) n), i.e., Q ≈
HH T . ICF, together with SMW (the Sherman-Morrison-Woodbury formula), can greatly reduce
the computational complexity in solving an n × n linear system. The work of (Fine & Scheinberg,
2001) provides a theoretical analysis of how ICF inﬂuences the optimization problem in Eq.(2). The
authors proved that the error of the optimal objective value introduced by ICF is bounded by C 2 l/2,
where C is the hyperparameter of SVM, l is the number of support vectors, and  is the bound of

2

Algorithm 1 Row-based PICF
Input: n training instances; p: rank of ICF matrix H ; m: number of machines
Output: H distributed on m machines
Variables:
v: fraction of the diagonal vector of Q that resides in local machine
k : iteration number;
xi : the ith training instance
M : machine index set, M = {0, 1, . . . , m − 1}
Ic : row-index set on machine c (c ∈ M ), Ic = {c, c + m, c + 2m, . . .}
1: for i = 0 to n − 1 do
2:
Load xi into machine imodulom.
3: end for
4: k ← 0; H ← 0; v ← the fraction of the diagonal vector of Q that resides in local machine. (v(i)(i ∈ Im )
can be obtained from xi )
5: Initialize master to be machine 0.
6: while k < p do
Each machine c ∈ M selects its local pivot value, which is the largest element in v:
7:

lpvk,c = max
i∈Ic
and records the local pivot index, the row index corresponds to lpvk,c :

v(i).

lpik,c = arg max
i∈Ic
Gather lpvk,c ’s and lpik,c ’s (c ∈ M ) to master.
The master selects the largest local pivot value as global pivot value gpvk and records in ik , row index
corresponding to the global pivot value.

v(i).

8:
9:

gpvk = max
c∈M

lpvk,c .

10:
11:
12:
13:

The master broadcasts gpvk and ik .
Change master to machine ik%m.
Calculate H (ik , k) according to (3) on master.
The master broadcasts the pivot instance xik and the pivot row H (ik , :). (Only the ﬁrst k + 1 values of
the pivot row need to be broadcast, since the remainder are zeros.)
Each machine c ∈ M calculates its part of the kth column of H according to (4).
14:
Each machine c ∈ M updates v according to (5).
15:
k ← k + 1
16:
17: end while

H (ik , k) =

ICF approximation (i.e. tr(Q − HH T ) < ). Experimental results in Section 3 show that when p is
√
set to
n, the error can be negligible.
Our row-based parallel ICF (PICF) works as follows: Let vector v be the diagonal of Q and suppose
(cid:112)
the pivots (the largest diagonal values) are {i1 , i2 , . . . , ik }, the k th iteration of ICF computes three
equations:
H (Jk , k) = (Q(Jk , k) − k−1(cid:88)
v(ik )
H (Jk , j )H (ik , j ))/H (ik , k)
j=1
v(Jk ) = v(Jk ) − H (Jk , k)2 ,
(5)
where Jk denotes the complement of {i1 , i2 , . . . , ik }. The algorithm iterates until the approximation
k (measured by trace(Q − HkH T
k )) is satisfactory, or the predeﬁned maximum
of Q by HkH T
iterations (or say, the desired rank of the ICF matrix) p is reached.
As suggested by G. Golub, a parallelized ICF algorithm can be obtained by constraining the par-
allelized Cholesky Factorization algorithm, iterating at most p times. However, in the proposed
algorithm (Golub & Loan, 1996), matrix H is distributed by columns in a round-robin way on m
machines (hence we call it column-based parallelized ICF). Such column-based approach is opti-
mal for the single-machine setting, but cannot gain full beneﬁt from parallelization for two major
reasons:

(3)

(4)

3

1. Large memory requirement. All training data are needed for each machine to calculate Q(Jk , k).
(cid:80)k−1
Therefore, each machine must be able to store a local copy of the training data.
2.
Limited
calculation
parallelizable
computation.
Only
the
inner
product
j=1 H (Jk , j )H (ik , j )) in (4) can be parallelized. The calculation of pivot selection, the
(
summation of local inner product result, column calculation in (4), and the vector update in (5)
must be performed on one single machine.
To remedy these shortcomings of the column-based approach, we propose a row-based approach to
parallelize ICF, which we summarize in Algorithm 1. Our row-based approach starts by initializing
variables and loading training data onto m machines in a round-robin fashion (Steps 1 to 5). The
algorithm then performs the ICF main loop until the termination criteria are satisﬁed (e.g., the rank
of matrix H reaches p). In the main loop, PICF performs ﬁve tasks in each iteration k :
• Distributedly ﬁnd a pivot, which is the largest value in the diagonal v of matrix Q (steps 7 to 10).
Notice that PICF computes only needed elements in Q from training data, and it does not store Q.
• Set the machine where the pivot resides as the master (step 11).
• On the master, PICF calculates H (ik , k) according to (3) (step 12).
• The master then broadcasts the pivot instance xik and the pivot row H (ik , :) (step 13).
• Distributedly compute (4) and (5) (steps 14 and 15).
At the end of the algorithm, H is stored distributedly on m machines, ready for parallel IPM (pre-
sented in the next section). PICF enjoys three advantages: parallel memory use (O(np/m)), parallel
computation (O(p2n/m)), and low communication overhead (O(p2 log(m))). Particularly on the
communication overhead, its fraction of the entire computation time shrinks as the problem size
grows. We will verify this in the experimental section. This pattern permits a larger problem to be
solved on more machines to take advantage of parallel memory use and computation.

2.2 Parallel IPM

)(cid:52)x

(6)

(7)

(cid:52)ν =

+ diag( λi
C − αi
)(cid:52)x

As mentioned in Section 1, the most effective algorithm to solve a constrained QP problem is the
primal-dual IPM. For detailed description and notations of IPM, please consult (Boyd, 2004; Mehro-
(cid:181)
(cid:182)
tra, 1992). For the purpose of SVM training, IPM boils down to solving the following equations in
the Newton step iteratively.
(cid:182)
(cid:181)
1
(cid:52)λ = −λ + vec
t(C − αi )
1
(cid:52)ξ = −ξ + vec
− diag( ξi
tαi
αi
yT Σ−1z + yT α
yT Σ−1y
+ λi
D = diag( ξi
C − αi
αi
(cid:52)x = Σ−1 (z − y(cid:52)ν ),
where Σ and z depend only on [α, λ, ξ , ν ] from the last iteration as follows:
Σ = Q + diag( ξi
+ λi
C − αi
αi
1
1
1
z = −Qα + 1n − ν y +
vec(
C − αi
αi
t
The computation bottleneck is on matrix inverse, which takes place on Σ for solving (cid:52)ν in (8)
and (cid:52)x in (10). Equation (11) shows that Σ depends on Q, and we have shown that Q can be
approximated through PICF by HH T . Therefore, the bottleneck of the Newton step can be sped up
from O(n3 ) to O(p2n), and be parallelized to O(p2n/m).
Distributed Data Loading
To minimize both storage and communication cost, PIPM stores data distributedly as follows:

(8)

(9)

(10)

(11)

(12)

)

)

−

).

4

• Distribute matrix data. H is distributedly stored at the end of PICF.
• Distribute n × 1 vector data. All n × 1 vectors are distributed in a round-robin fashion on m
machines. These vectors are z, α, ξ , λ, ∆z, ∆α, ∆ξ , and ∆λ.
• Replicate global scalar data. Every machine caches a copy of global data including ν , t, n, and
∆ν . Whenever a scalar is changed, a broadcast is required to maintain global consistency.
Parallel Computation of (cid:52)ν
Rather than walking through all equations, we describe how PIPM solves (8), where Σ−1 appears
twice. An interesting observation is that parallelizing Σ−1 z (or Σ−1 y ) is simpler than parallelizing
Σ−1 . Let us explain how parallelizing Σ−1 z works, and parallelizing Σ−1 y can follow suit.
According to SMW (the Sherman-Morrison-Woodbury formula), we can write Σ−1 z as
Σ−1 z = (D + Q)−1 z ≈ (D + HH T )−1 z
= D−1 z − D−1H (I + H T D−1H )−1H T D−1 z
= D−1 z − D−1H (GGT )−1H T D−1 z .
Σ−1 z can be computed in four steps:
1. Compute D−1 z . D can be derived from locally stored vectors, following (9). D−1 z is a n × 1
vector, and can be computed locally on each of the m machines.
2. Compute t1 = H T D−1 z . Every machine stores some rows of H and their corresponding part
of D−1 z . This step can be computed locally on each machine. The results are sent to the master
(which can be a randomly picked machine for all PIPM iterations) to aggregate into t1 for the next
step.
3. Compute (GGT )−1 t1 . This step is completed on the master, since it has all the required
data. G can be obtained from H in a straightforward manner as shown in SMW. Computing
t2 = (GGT )−1 t1 is equivalent to solving the linear equation system t1 = (GGT )t2 . PIPM ﬁrst
solves t1 = Gy0 , then y0 = GT t2. Once it has obtained y0 , PIPM can solve GT t2 = y0 to obtain
t2 . The master then broadcasts t2 to all machines.
4. Compute D−1H t2 All machines have a copy of t2 , and can compute D−1H t2 locally to solve
for Σ−1 z .
Similarly, Σ−1 y can be computed at the same time. Once we have obtained both, we can solve ∆ν
according to (8).

2.3 Computing b and Writing Back

When the IPM iteration stops, we have the value of α and hence the classiﬁcation function
Ns(cid:88)
i=1

αi yik(si , x) + b

f (x) =

Here Ns is the number of support vectors and si are support vectors. In order to complete this
classiﬁcation function, b must be computed. According to the SVM model, given a support vector s,
if ys = +1, or f (s) = −1,
if ys = −1.
we obtain one of the two results for f (s): f (s) = +1,
In practice, we can select M , say 1, 000, support vectors and compute the average of the bs in
parallel using MapReduce (Dean & Ghemawat, 2004).

3 Experiments

We conducted experiments on PSVM to evaluate its 1) class-prediction accuracy, 2) scalability on
large datasets, and 3) overheads. The experiments were conducted on up to 500 machines in our
data center. Not all machines are identically conﬁgured; however, each machine is conﬁgured with
a CPU faster than 2GHz and memory larger than 4GBytes.

5

Table 1: Class-prediction Accuracy with Different p Settings.

dataset
svmguide1
mushrooms
news20
Image
CoverType
RCV

samples (train/test)
3, 089/4, 000
7, 500/624
18, 000/1, 996
199, 957/84, 507
522, 910/58, 102
781, 265/23, 149

LIBSVM p = n0.1
0.6563
0.9608
1
0.9904
0.6949
0.7835
0.7293
0.849
0.9764
0.9769
0.9575
0.8527

p = n0.2
0.9
0.9920
0.6949
0.7210
0.9762
0.8586

p = n0.3
0.917
1
0.6969
0.8041
0.9766
0.8616

p = n0.4
0.9495
1
0.7806
0.8121
0.9761
0.9065

p = n0.5
0.9593
1
0.7811
0.8258
0.9766
0.9264

3.1 Class-prediction Accuracy
PSVM employs PICF to approximate an n × n kernel matrix Q with an n × p matrix H . This
experiment evaluated how the choice of p affects class-prediction accuracy. We set p of PSVM to nt ,
where t ranges from 0.1 to 0.5 incremented by 0.1, and compared its class-prediction accuracy with
that achieved by LIBSVM. The ﬁrst two columns of Table 1 enumerate the datasets and their sizes
with which we experimented. We use Gaussian kernel, and select the best C and σ for LIBSVM and
PSVM, respectively. For CoverType and RCV, we loosed the terminate condition (set -e 1, default
√
0.001) and used shrink heuristics (set -h 0) to make LIBSVM terminate within several days. The
table shows that when t is set to 0.5 (or p =
n), the class-prediction accuracy of PSVM approaches
that of LIBSVM.
We compared only with LIBSVM because it is arguably the best open-source SVM implementa-
tion in both accuracy and speed. Another possible candidate is CVM (Tsang et al., 2005). Our
experimental result on the CoverType dataset outperforms the result reported by CVM on the same
dataset in both accuracy and speed. Moreover, CVM’s training time has been shown unpredictable
by (Loosli & Canu, 2006), since the training time is sensitive to the selection of stop criteria and
hyper-parameters. For how we position PSVM with respect to other related work, please refer to
our disclaimer in the end of Section 1.

3.2 Scalability

For scalability experiments, we used three large datasets. Table 2 reports the speedup of PSVM
on up to m = 500 machines. Since when a dataset size is large, a single machine cannot store
the factorized matrix H in its local memory, we cannot obtain the running time of PSVM on one
machine. We thus used 10 machines as the baseline to measure the speedup of using more than
10 machines. To quantify speedup, we made an assumption that the speedup of using 10 machines
is 10, compared to using one machine. This assumption is reasonable for our experiments, since
PSVM does enjoy linear speedup when the number of machines is up to 30.
√

n); LIBSVM training time is reported on the last row for reference.

Table 2: Speedup (p is set to

Image (200k)
Time (s)
Speedup
Machines
10∗
(9)
1, 958
10
34.2
(8)
572
30
41.4
(14)
473
50
59.4
(47)
330
100
150
274
(40)
71.4
66.7
(41)
294
200
49.4
(78)
397
250
24.1
500
814
(123)
LIBSVM 4, 334 NA
NA

CoverType (500k)
Time (s)
Speedup
10∗
(442)
16, 818
30.1
(10)
5, 591
46.8
(60)
3, 598
80.8
(29)
2, 082
1, 865
(93)
90.2
118.7
(24)
1, 416
119.7
(115)
1, 405
101.6
1, 655
(34)
28, 149 NA
NA

RCV (800k)
Time (s)
(1373)
45, 135
(98)
12, 289
(92)
7, 695
(34)
4, 992
3, 313
(59)
(69)
3, 163
(203)
2, 719
2, 671
(193)
184, 199 NA

Speedup
10∗
36.7
58.7
90.4
136.3
142.7
166.0
169.0
NA

We trained PSVM three times for each dataset-m combination. The speedup reported in the table
is the average of three runs with standard deviation provided in brackets. The observed variance in
speedup was caused by the variance of machine loads, as all machines were shared with other tasks

6

running on our data centers. We can observe in Table 2 that the larger is the dataset, the better is
the speedup. Figures 1(a), (b) and (c) plot the speedup of Image, CoverType, and RCV, respectively.
All datasets enjoy a linear speedup when the number of machines is moderate. For instance, PSVM
achieves linear speedup on RCV when running on up to around 100 machines. PSVM scales well till
around 250 machines. After that, adding more machines receives diminishing returns. This result
led to our examination on the overheads of PSVM, presented next.

(a) Image (200k) speedup

(b) Covertype (500k) speedup

(c) RCV (800k) speedup

(d) Image (200k) overhead

(e) Covertype (500k) overhead

(f) RCV (800k) overhead

(g) Image (200k) fraction

(h) Covertype (500k) fraction

(i) RCV (800k) fraction

Figure 1: Speedup and Overheads of Three Datasets.

3.3 Overheads

PSVM cannot achieve linear speedup when the number of machines continues to increase beyond
a data-size-dependent threshold. This is expected due to communication and synchronization over-
heads. Communication time is incurred when message passing takes place between machines. Syn-
chronization overhead is incurred when the master machine waits for task completion on the slowest
machine. (The master could wait forever if a child machine fails. We have implemented a check-
point scheme to deal with this issue.)
The running time consists of three parts: computation (Comp), communication (Comm), and syn-
chronization (Sync). Figures 1(d), (e) and (f) show how Comm and Sync overheads inﬂuence the
speedup curves. In the ﬁgures, we draw on the top the computation only line (Comp), which ap-
proaches the linear speedup line. Computation speedup can become sublinear when adding ma-
chines beyond a threshold. This is because the computation bottleneck of the unparallelizable step
12 in Algorithm 1 (which computation time is O(p2 )). When m is small, this bottleneck is insignif-
icant in the total computation time. According to the Amdahl’s law; however, even a small fraction
of unparallelizable computation can cap speedup. Fortunately, the larger the dataset is, the smaller
is this unparallelizable fraction, which is O(m/n). Therefore, more machines (larger m) can be
employed for larger datasets (larger n) to gain speedup.

7

When communication overhead or synchronization overhead is accounted for (the Comp + Comm
line and the Comp + Comm + Sync line), the speedup deteriorates. Between the two overheads, the
synchronization overhead does not impact speedup as much as the communication overhead does.
Figures 1(g), (h), and (i) present the percentage of Comp, Comm, and Sync in total running time.
The synchronization overhead maintains about the same percentage when m increases, whereas the
percentage of communication overhead grows with m. As mentioned in Section 2.1, the communi-
cation overhead is O(p2 log(m)), growing sub-linearly with m. But since the computation time per
node decreases as m increases, the fraction of the communication overhead grows with m. There-
fore, PSVM must select a proper m for a training task to maximize the beneﬁt of parallelization.

4 Conclusion

In this paper, we have shown how SVMs can be parallelized to achieve scalable performance. PSVM
distributedly loads training data on parallel machines, reducing memory requirement through ap-
proximate factorization on the kernel matrix. PSVM solves IPM in parallel by cleverly arranging
computation order. We have made PSVM open source at http://code.google.com/p/psvm/.
Acknowledgement

The ﬁrst author is partially supported by NSF under Grant Number IIS-0535085.
References
Bach, F. R., & Jordan, M. I. (2005). Predictive low-rank decomposition for kernel methods. Pro-
ceedings of the 22nd International Conference on Machine Learning.
Boyd, S. (2004). Convex optimization. Cambridge University Press.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Chu, C.-T., Kim, S. K., Lin, Y.-A., Yu, Y., Bradski, G., Ng, A. Y., & Olukotun, K. (2006). Map
reduce for machine learning on multicore. NIPS.
Dean, J., & Ghemawat, S. (2004). Mapreduce: Simpliﬁed data processing on large clusters.
OSDI’04: Symposium on Operating System Design and Implementation.
Fine, S., & Scheinberg, K. (2001). Efﬁcient svm training using low-rank kernel representations.
Journal of Machine Learning Research, 2, 243–264.
Ghemawat, S., Gobioff, H., & Leung, S.-T. (2003). The google ﬁle system. 19th ACM Symposium
on Operating Systems Principles.
Golub, G. H., & Loan, C. F. V. (1996). Matrix computations. Johns Hopkins University Press.
Graf, H. P., Cosatto, E., Bottou, L., Dourdanovic, I., & Vapnik, V. (2005). Parallel support vector
machines: The cascade svm. In Advances in neural information processing systems 17, 521–528.
Joachims, T. (1998). Making large-scale svm learning practical. Advances in Kernel Methods -
Support Vector Learning.
Joachims, T. (2006). Training linear svms in linear time. ACM KDD, 217–226.
Lee, Y.-J., & Mangasarian, O. L. (2001). Rsvm: Reduced support vector machines. First SIAM
International Conference on Data Mining. Chicago.
Loosli, G., & Canu, S. (2006). Comments on the core vector machines: Fast svm training on very
large data sets (Technical Report).
Mehrotra, S. (1992). On the implementation of a primal-dual interior point method. SIAM J. Opti-
mization, 2.
Platt, J. (1998). Sequential minimal optimization: A fast algorithm for training support vector
machines (Technical Report MSR-TR-98-14). Microsoft Research.
Tsang, I. W., Kwok, J. T., & Cheung, P.-M. (2005). Core vector machines: Fast svm training on
very large data sets. Journal of Machine Learning Research, 6, 363–392.
Vapnik, V. (1995). The nature of statistical learning theory. New York: Springer.
Vishwanathan, S., Smola, A. J., & Murty, M. N. (2003). Simplesvm. ICML.

8


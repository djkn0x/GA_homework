Cluster Stability for Finite Samples

Ohad Shamir† and Naftali Tishby†‡
† School of Computer Science and Engineering
‡ Interdisciplinary Center for Neural Computation
The Hebrew University
Jerusalem 91904, Israel
{ohadsh,tishby}@cs.huji.ac.il

Abstract

Over the past few years, the notion of stability in data clustering has received
growing attention as a cluster validation criterion in a sample-based framework.
However, recent work has shown that as the sample size increases, any clustering
model will usually become asymptotically stable. This led to the conclusion that
stability is lacking as a theoretical and practical tool. The discrepancy between
this conclusion and the success of stability in practice has remained an open ques-
tion, which we attempt to address. Our theoretical approach is that stability, as
used by cluster validation algorithms, is similar in certain respects to measures
of generalization in a model-selection framework. In such cases, the model cho-
sen governs the convergence rate of generalization bounds. By arguing that these
rates are more important than the sample size, we are led to the prediction that
stability-based cluster validation algorithms should not degrade with increasing
sample size, despite the asymptotic universal stability. This prediction is substan-
tiated by a theoretical analysis as well as some empirical results. We conclude that
stability remains a meaningful cluster validation criterion over ﬁnite samples.

1 Introduction

Clustering is one of the most common tools of unsupervised data analysis. Despite its widespread
use and an immense amount of literature, distressingly little is known about its theoretical founda-
tions [14]. In this paper, we focus on sample based clustering, where it is assumed that the data to
be clustered are actually a sample from some underlying distribution.

A major problem in such a setting is assessing cluster validity. In other words, we might wish to
know whether the clustering we have found actually corresponds to a meaningful clustering of the
underlying distribution, and is not just an artifact of the sampling process. This problem relates to the
issue of model selection, such as determining the number of clusters in the data or tuning parameters
of the clustering algorithm. In the past few years, cluster stability has received growing attention
as a criterion for addressing this problem.
Informally, this criterion states that if the clustering
algorithm is repeatedly applied over independent samples, resulting in ’similar’ clusterings, then
these clusterings are statistically signiﬁcant. Based on this idea, several cluster validity methods
have been proposed (see [9] and references therein), and were shown to be relatively successful for
various data sets in practice.

However, in recent work, it was proven that under mild conditions, stability is asymptotically fully
determined by the behavior of the objective function which the clustering algorithm attempts to
optimize. In particular, the existence of a unique optimal solution for some model choice implies
stability as sample size increase to inﬁnity. This will happen regardless of the model ﬁt to the data.
From this, it was concluded that stability is not a well-suited tool for model selection in clustering.
This left open, however, the question of why stability is observed to be useful in practice.

1

In this paper, we attempt to explain why stability measures should have much wider relevance than
what might be concluded from these results. Our underlying approach is to view stability as a
measure of generalization, in a learning-theoretic sense. When we have a ’good’ model, which is
stable over independent samples, then inferring its ﬁt to the underlying distribution should be easy.
In other words, stability should ’work’ because stable models generalize better, and models which
generalize better should ﬁt the underlying distribution better. We emphasize that this idea in itself is
not novel, appearing explicitly and under various guises in many aspects of machine learning. The
novelty in this paper lies mainly in the predictions that are drawn from it for clustering stability.

The viewpoint above places emphasis on the nature of stability for ﬁnite samples. Since generaliza-
tion is meaningless when the sample is inﬁnite, it should come as no surprise that stability displays
similar behavior. On ﬁnite samples, the generalization uncertainty is virtually always strictly posi-
tive, with different model choices leading to different convergence rates towards zero for increasing
sample size. Based on the link between stability and generalization, we predict that on realistic
data, all risk-minimizing models asymptotically become stable, but the rates of convergence to this
ultimate stability differ. In other words, an appropriate scaling of the stability measures will make
them independent of the actual sample size used. Using this intuition, we characterize and prove a
mild set of conditions, applicable in principle to a wide class of clustering settings, which ensure
the relevance of cluster stability for arbitrarily large sample sizes. We then prove that the stability
measure used in previous work to show negative asymptotic results on stability, actually allows us to
discern the ’correct’ model, regardless of how large is the sample, for a certain simple setting. Our
results are further validated by some experiments on synthetic and real world data.

2 Deﬁnitions and notation
We assume that the data sample to be clustered, S = {x1 , .., xm }, is produced by sampling instances
i.i.d from an underlying distribution D , supported on a subset X of Rn . A clustering CD for some
D ⊆ X is a function from D × D to {0, 1}, deﬁning an equivalence relation on D with a ﬁnite
number of equivalence classes (namely, CD (xi , xj ) = 1 if xi and xj belong to the same cluster,
and 0 otherwise). For a clustering CX of the instance space, and a ﬁnite sample S , let CX |S denote
the functional restriction of CX on S × S .
A clustering algorithm A is a function from any ﬁnite sample S ⊆ X , to some clustering CX of
the instance space1 . We assume the algorithm is driven by optimizing an objective function, and
has some user-deﬁned parameters Θ. In particular, Ak denotes the algorithm A with the number of
clusters chosen to be k .

Following [2], we deﬁne the stability of a clustering algorithm A on ﬁnite samples of size m as:
stab(A, D , m) = ES1 ,S2 dD (A(S1 ), A(S2 )),
(1)
where S1 and S2 are samples of size m, drawn i.i.d from D , and dD is some ’dissimilarity’ function
between clusterings of X , to be speciﬁed later.
Let (cid:96) denote a loss function from any clustering CS of a ﬁnite set S ⊆ X to [0, 1]. (cid:96) may or may not
correspond to the objective function the clustering algorithm attempts to optimize, and may involve
a global quality measure rather than some average over individual instances. For a ﬁxed sample size,
we say that (cid:96) obeys the bounded differences property (see [11]), if for any clustering CS it holds that
|(cid:96)(CS ) − (cid:96)(CS (cid:48) )| ≤ a, where a is a constant, and CS (cid:48) is obtained from CS by replacing at most one
instance of S by any other instance from X , and clustering it arbitrarily.
A hypothesis class H is deﬁned as some set of clusterings of X . The empirical risk of a clustering
CX ∈ H on a sample S of size m is (cid:96)(CX |S ). The expected risk of CX , with respect to samples
S of size m, will be deﬁned as ES (cid:96)(CX |S ). The problem of generalization is how to estimate the
expected risk, based on the empirical data.

1Many clustering algorithms, such as spectral clustering, do not induce a natural clustering on X based on
a clustering of a sample. In that case, we view the algorithm as a two-stage process, in which the clustering
of the sample is extended to X through some uniform extension operator (such as assigning instances to the
’nearest’ cluster in some appropriate sense).

2

3 A Bayesian framework for relating stability and generalization

The relationship between generalization and various notions of stability is long known, but has been
dealt with mostly in a supervised learning setting (see [3][5] [8] and references therein). In the
context of unsupervised data clustering, several papers have explored the relevance of statistical
stability and generalization, separately and together (such as [1][4][14][12]). However, there are
not many theoretical results quantitatively characterizing the relationship between the two in this
setting. The aim of this section is to informally motivate our approach, of viewing stability and
generalization in clustering as closely related.

Relating the two is very natural in a Bayesian setting, where clustering stability implies an ’unsur-
prising’ posterior given a prior, which is based on clustering another sample. Under this paradigm,
we might consider ’soft clustering’ algorithms which return a distribution over a measurable hypoth-
esis class H , rather than a speciﬁc clustering. This distribution typically reﬂects the likelihood of a
clustering hypothesis, given the data and prior assumptions. Extending our notation, we have that
for any sample S , A(S ) is now a distribution over H . The empirical risk of such a distribution, with
respect to sample S (cid:48) , is deﬁned as (cid:96)(A(S )|S (cid:48) ) = ECX ∼A(S ) (cid:96)(CX |S (cid:48) ).
In this setting, consider for example the following simple procedure to derive a clustering hypothesis
distribution, as well as a generalization bound: Given a sample of size 2m drawn i.i.d from D , we
randomly split it into two samples S1 ,S2 each of size m, and use A to cluster each of them separately.
Then we have the following:
Theorem 1. For the procedure deﬁned above, assume (cid:96) obeys the bounded differences property
with parameter 1/m. Deﬁne the clustering distance dD (P , Q) in Eq. (1), between two distributions
P ,Q over the hypothesis class H , as the Kullback-Leibler divergence DKL [Q||P ]2 . Then for a ﬁxed
conﬁdence parameter δ ∈ (0, 1), it holds with probability at least 1 − δ over the draw of samples S1
(cid:114)
and S2 of size m, that
ES (cid:96)(A(S2 )|S ) − (cid:96)(A(S2 )|S2 ) ≤

dD (A(S1 ), A(S2 )) + ln(m/δ) + 2
2m − 1

.

≤

=

ES1 ,S2

dD (A(S1 ), A(S2 )) + ln(m/δ) + 2
2m − 1

The theorem is a straightforward variant of the PAC-Bayesian theorem [10]. Since the loss function
is not necessarily an empirical average, we need to utilize McDiarmid’s bound for random variables
with bounded differences, instead of Hoeffding’s bound. Other than that, the proof is identical, and
is therefore ommited.
This theorem implies that the more stable is the Bayesian algorithm, the tighter the expected gen-
(cid:114)
(cid:114)
eralization bounds we can achieve. In fact, the ’expected’ magnitude of the high-probability bound
we will get (over drawing S1 and S2 and performing the procedure described above) is:
(cid:114)
ES1 ,S2 dD (A(S1 ), A(S2 )) + ln(m/δ) + 2
2m − 1
stab(A, D , m) + ln(m/δ) + 2
2m − 1
Note that the only model-dependent quantity in the expression above is stab(A, D , m). Therefore,
carrying out model selection by attempting to minimize these types of generalization bounds is
closely related to minimizing stab(A, D , m). In general, the generalization bound might converge
to 0 as m → ∞, but this is immaterial for the purpose of model selection. The important factor is
the relative values of the measure, over different choices of the algorithm parameters Θ. In other
words, the important quantity is the relative convergence rates of this bound for different choices of
Θ, governed by stab(A, D , m).
R
This informal discussion only exempliﬁes the relationship between generalization and stability, since
the setting and the deﬁnition of dD here differs from the one we will focus on later in the paper.
Although these ideas can be generalized, they go beyond the scope of this paper, and we leave it for
future work.
2Where we deﬁne DKL [Q||P ] =
X Q(X ) ln(Q(X )/P (X )), and DKL [q ||p] for q , p ∈ [0, 1] is deﬁned
as the divergence of Bernoulli distributions with parameters q and p.

.

3

4 Effective model selection for arbitrarily large sample sizes

From now on, following [2], we will deﬁne the clustering distance function dD of Eq. (1) as:
x1 ,x2∼D (A(S1 )(x1 , x2 ) (cid:54)= A(S2 )(x1 , x2 )) .
dD (A(S1 ), A(S2 )) =
Pr
In other words, the clustering distance is the probability that two independently drawn instances
from D will be in the same cluster under one clustering, and in different clusters under another
clustering.

(2)

In [2], it is essentially proven that if there exists a unique optimizer to the clustering algo-
rithm’s objective function, to which the algorithm converges for asymptotically large samples, then
stab(A, D , m) converges to 0 as m → ∞, regardless of the parameters of A. From this, it was
concluded that using stability as a tool for cluster validity is problematic, since for large enough
samples it would always be approximately zero, for any algorithm parameters chosen.

However, using the intuition gleaned from the results of the previous section, the different conver-
gence rates of the stability measure (for different algorithm parameters) should be more important
than their absolute values or the sample size. The key technical result needed to substantiate this
intuition is the following theorem:
(cid:80)m
Theorem 2. Let X, Y be two random variables bounded in [0, 1], and with strictly positive ex-
(cid:80)m
pected values. Assume E[X ]/E[Y ] ≥ 1 + c for some positive constant c. Letting X1 , . . . , Xm and
ˆX = 1
(cid:33)
(cid:195)
(cid:33)
(cid:195)
(cid:182)2
(cid:181)
(cid:182)4
(cid:181)
Y1 , . . . , Ym be m identical independent copies of X and Y respectively, deﬁne
i=1 Xi
m
and ˆY = 1
i=1 Yi . Then it holds that:
m
− 1
Pr( ˆX ≤ ˆY ) ≤ exp
8 mE[X ]

− 1
4 mE[X ]

c
1 + c

+ exp

c
1 + c

.

The importance of this theorem becomes apparent when ˆX , ˆY are taken to be empirical estimators
of stab(A, D , m) for two different algorithm parameter sets Θ, Θ(cid:48) . For example, suppose that ac-
cording to our stability measure (see Eq. (1)), a cluster model with k clusters is more stable than a
model with k (cid:48) clusters, where k (cid:54)= k (cid:48) , for sample size m (e.g., stab(Ak , D , m) < stab(Ak(cid:48) , D , m)).
√
These stability measures might be arbitrarily close to zero. Assume that with high probability over
the choice of samples S1 and S2 of size m, we can show that dD (Ak (S1 ), Ak (S2 )) ≤ 1/
√
m, while
dD (Ak(cid:48) (S1 ), Ak(cid:48) (S2 )) ≥ 1.01/
m. We cannot compute these exactly, since the deﬁnition of dD
involves an expectation over the unknown distribution D (see Eq. (2)). However, we can estimate
them by drawing another sample S3 of m instance pairs, and computing a sample mean to estimate
Eq. (2). According to Thm. 2, since dD (Ak (S1 ), Ak (S2 )) and dD (Ak(cid:48) (S1 ), Ak(cid:48) (S2 )) have slightly
different convergence rates (c ≥ 0.01), which are slower than Θ(1/m), then we can discern which
number of clusters is more stable, with a high probability which actually improves as m increases.

Therefore, we can use Thm. 2 as a guideline for when a stability estimator might be useful for arbi-
trarily large sample sizes. Namely, we need to show it is an expected value of some random variable,
with at least slightly different convergence rates for different model selections, and with at least
some of them dominating Θ(1/m). We would expect these conditions to hold under quite general
settings, since most stability measures are based on empirically estimating the mean of some ran-
√
dom variable. Moreover, a central-limit theorem argument leads us to expect an asymptotic form of
Ω(1/
m), with the exact constants dependent on the model. This convergence rate is slow enough
for the theorem to apply. The difﬁcult step, however, is showing that the differing convergence rates
can be detected empirically, without knowledge of D . In the example above, this reduces to show-
ing that with high probability over S1 and S2 , dD (Ak (S1 ), Ak (S2 )) and dD (Ak(cid:48) (S1 ), Ak(cid:48) (S2 )) will
indeed differ by some constant ratio independent of m.
(cid:179)
(cid:180)
Proof of Thm. 2. Using a relative entropy variant of Hoeffding’s bound [7], we have that for any
1 > b > 0 and 1/E[Y ] > a > 1, it holds that:
(cid:179)
(cid:180)
≤ exp (−m DKL [bE [X ] || E [X ]]) ,
ˆX ≤ bE[X ]
ˆY ≥ aE[Y ]
≤ exp (−m DKL [aE [Y ] || E [Y ]]) .

Pr

Pr

4

≤ exp

ˆY ≥ aE[Y ]

(cid:181)
(cid:182)
(cid:179)
(cid:180)
By substituting the bound DKL [p||q ] ≥ (p − q)2 /2 max{p, q} in the two inequalities, we get:
(cid:181)
(cid:181)
(cid:182)(cid:182)
(cid:179)
(cid:180)
− 1
2 mE [X ] (1 − b)2
ˆX ≤ bE[X ]
≤ exp
− 1
1
− 2
2 mE [Y ]
a +
a
which hold whenever 1 > b > 0 and a > 1. Let b = 1 − (1 − E[Y ]/E[X ])2 /2, and a =
(cid:33)
(cid:195)
(cid:182)4
(cid:181)
bE[X ]/E[Y ]. It is easily veriﬁed that
b < 1 and a > 1. Substituting these values into the r.h.s of
Eq. (3), and to both sides of Eq. (4), and after some algebra, we get:
(cid:33)
(cid:195)
− 1
(cid:182)2
(cid:181)
8 mE[X ]
− 1
c
4 mE[X ]
1 + c
As a result, by the union bound, we have that Pr( ˆX ≤ ˆY ) is at most the sum of the r.h.s of the last
two inequalities, hence proving the theorem.

Pr( ˆX ≤ bE[X ]) ≤ exp

Pr( ˆY ≥ bE[X ]) ≤ exp

c
1 + c

(3)

(4)

Pr

Pr

,

,

.

As a proof of concept, we show that for a certain setting, the stability measure used by [2], as deﬁned
above, is meaningful for arbitrarily large sample sizes, even when this measure converges to zero for
any choice of the required number of clusters. The result is a simple counter-example to the claim
that this phenomenon makes cluster stability a problematic tool.
The setting we analyze is a mixture distribution of three well-separated unequal Gaussians in R,
where an empirical estimate of stability, using a centroid-based clustering algorithm, is utilized
to discern whether the data contain 2, 3 or 4 clusters. We prove that with high probability, this
empirical estimation process will discern k = 3 as much more stable than both k = 2 and k = 4
(by an amount depending on the separation between the Gaussians). The result is robust enough to
hold even if in addition one performs normalization procedures to account for the fact that higher
number of clusters entail more degrees of freedom for the clustering algorithm (see [9]).

We emphasize that the simplicity of this setting is merely for the sake of analytical convenience.
The proof itself relies on a general and intuitive characteristic of what constitutes a ’wrong’ model
(namely, having cluster

boundaries in areas of high density), rather than any speciﬁc feature of this setting. We are currently
working on generalizing this result, using a more involved analysis.
In this setting, by the results of [2], stab(Ak , D , m) will converge to 0 as m → ∞ for k = 2, 3, 4.
The next two lemmas, however, show that the stability measure for k = 3 (the ’correct’ model order)
is smaller than the other two, by a substantial ratio independent of m, and that this will be discerned,
with high probability, based on the empirical estimates of dD (Ak (S1 ), Ak (S2 )). The proofs are
(cid:181)
(cid:181)
(cid:181)
(cid:182)
(cid:182)
(cid:182)
technical, and appear in the supplementary material to this paper.
Lemma 1. For some µ > 0, let D be a Gaussian mixture distribution on R, with density function
− (x − µ)2
− (x + µ)2
1
1
2
− x2
√
√
√
exp
exp
exp
p(x) =
+
+
.
2
2
2
6
6
2π
3
2π
2π
Assume µ (cid:192) 1, so that the Gaussians are well separated. Let Ak be a centroid-based cluster-
ing algorithm, which is given a sample and required number of clusters k , and returns a set of k
centroids, minimizing the k-means objective function (sum of squared Euclidean distances between
(cid:181)
(cid:182)
each instance and its nearest centroid). Then the following holds, with o(1) signifying factors which
converge to 0 as m → ∞:
, stab(A4 , D , m) ≥ 0.4 − o(1)
stab(A2 , D , m) ≥ 1 − o(1)
(cid:182)
(cid:181)
− µ2
√
√
exp
32
7
m
m
stab(A3 , D , m) ≤ 1.1 + o(1)
− µ2
√
8
m

exp

.

5

Lemma 2. For the setting described in Lemma 1,
it holds that over the draw of indepen-
2 ) (each of size m from D),
dent sample pairs (S1 , S2 ), (S (cid:48)
1 , S (cid:48)
2 ), (S (cid:48)(cid:48)
1 , S (cid:48)(cid:48)
the ratio between
dD (A2 (S (cid:48)
1 ), A2 (S (cid:48)
2 )) and dD (A3 (S1 ), A3 (S2 )), as well as the ratio between dD (A4 (S (cid:48)(cid:48)
1 ), A4 (S (cid:48)(cid:48)
(cid:181)
(cid:181)
(cid:182)
(cid:181)
(cid:182)(cid:182)
2 ))
and dD (A3 (S1 ), A3 (S2 )), is larger than 2 with probability of at least:
− µ2
− µ2
1 − (4 + o(1))
exp
16
32

+ exp

.

,

It should be noted that the asymptotic notation is merely to get rid of second-order terms, and is not
an essential feature. Also, the constants are by no means the tightest possible. With these lemmas,
we can prove that a direct estimation of stab(A, D , m), based on a random sample, allows us to
discern the more stable model with high probability, for arbitrarily large sample sizes.
Theorem 3. For the setting described in Lemma 1, deﬁne the following unbiased estimator ˆθk,4m
of stab(Ak , D , m): Given a sample of size 4m, split it randomly into 3 disjoint subsets S1 ,S2 ,S3 of
(cid:179)
(cid:180)
(cid:88)
size m,m and 2m respectively. Estimate dD (Ak (S1 ), Ak (S2 )) by computing
1
Ak (S1 )(xi , xm+i ) (cid:54)= Ak (S2 )(xi , xm+i )
1
m
xi ,xm+i∈S3
where (x1 , .., xm ) is a random permutation of S3 , and return this value as an estimate of
stab(Ak , D , m). If three samples of size 4m each are drawn i.i.d from D , and are used to calculate
(cid:179)
(cid:110)
(cid:111)(cid:180)
(cid:161)−Ω
(cid:161)−Ω(µ2 )
(cid:162)
(cid:161)√
(cid:162)(cid:162)
ˆθ2,4m , ˆθ3,4m , ˆθ4,4m , then
ˆθ3,4m ≥ min
ˆθ2,4m , ˆθ4,4m
m
.
(cid:161)−Ω(µ2 )
(cid:162)
Denoting the event above as B , and assuming it does not occur, we have that the estimators
ˆθ2,4m , ˆθ3,4m , ˆθ4,4m are each an empirical average over an additional sample of size m, and the
expected value of ˆθ3,4m is at least twice smaller than the expected values of the other two. More-
(cid:111) (cid:175)(cid:175) B (cid:123)
(cid:179)
(cid:110)
(cid:180)
√
(cid:161)−Ω
(cid:161)√
(cid:162)(cid:162)
over, by Lemma 1, the expected value of dD (A3 (S1 ), A3 (S2 )) is Ω(1/
m). Invoking Thm. 2, we
have that:
≤ exp
ˆθ3,4m ≥ min
ˆθ2,4m , ˆθ4,4m
m
Combining Eq. (5) and Eq. (6) yield the required result.

Pr
(cid:181)
Proof. Using Lemma 2, we have that:
2 ))}
min {dD (A2 (S (cid:48)
1 ), A4 (S (cid:48)(cid:48)
2 )), dD (A4 (S (cid:48)(cid:48)
1 ), A2 (S (cid:48)
dD (A3 (S1 ), A3 (S2 ))

+ exp
(cid:182)

≤ exp

.

(5)

≤ 2

< exp

Pr

Pr

(6)

5 Experiments

In order to further substantiate our analysis above, some experiments were run on synthetic and
real world data, with the goal of performing model selection over the number of clusters k . Our
3 different
ﬁrst experiment simulated the setting discussed in section 4 (see ﬁgure 1). We tested
Gaussian mixture distributions (with µ = 5, 7, 8), and sample sizes m ranging from 25 to 222 . For
each distribution and sample size, we empirically estimated ˆθ2 , ˆθ3 and ˆθ4 as described in section
4, using the k-means algorithm, and repeated this procedure over 1000 trials. Our results show
√
that although these empirical estimators converge towards zero, their convergence rates differ, with
m results in approximately
approximately constant ratios between them. Scaling the graphs by
constant and differing stability measures for each µ. Moreover, the failure rate does not increase with
sample size, and decreases rapidly to negligible size as the Gaussians become more well separated
- exactly in line with Thm. 3. Notice that although in the previous section we assumed a large
separation between the Gaussians for analytical convenience, good results are obtained even when
this separation is quite small.

For the other experiments, we used the stability-based cluster validation algorithm proposed in [9],
which was found to compare favorably with similar algorithms, and has the desirable property of

6

Figure 1: Empirical validation of results in section 4. In each row, the leftmost sub- ﬁgure is the
actual distribution, the middle sub- ﬁgure is a log-log plot of the estimators ˆθ2 , ˆθ3 , ˆθ4 (averaged over
1000 trials), as a function of the sample size, and on the right is the failure rate as a function of the
sample size (percentage of trials where ˆθ3 was not the smallest of the three).

Figure 2: Performance of stability based algorithm in [9] on 3 data sets. In each row, the leftmost
sub-ﬁgure is a sample representing the distribution, the middle sub-ﬁgure is a log-log plot of the
computed stability indices (averaged over 100 trials), and on the right is the failure rate (in detecting
the most stable model over repeated trials). In the phoneme data set, the algorithm selects 3 clusters
as the most stable models, since the vowels tend to group into a single cluster. The ’failures’ are all
due to trials when k = 4 was deemed more stable.

7

−10−505101000.10.20.3Distributionp(x)10110310510710−810−610−410−2Valuesofˆθ2,ˆθ3,ˆθ4  10110310510700.10.20.30.40.50.5Failure Rate−10−5051000.10.20.3p(x)10110310510710−810−610−410−2  10110310510700.10.20.30.40.5−10−5 0  5  10 00.10.20.3p(x)10110310510710−810−610−410−2m  10110310510700.10.20.30.40.5mk=2k=3k=4k=2k=3k=4k=2k=3k=4−10−505−10−505Random Sample10210310410510610−410−310−210−1k=3k=4k=5k=6k=7Values of stability method index10210310410500.10.20.30.40.5Failure Rate−202−20220004000800010−310−210−1k=2k=3k=4k=520004000800000.10.20.30.40.5−1000100200300−50050  shiydclaaao5001000500010−310−210−1k=3k=4k=5k=6m5001000500000.10.20.30.40.5mproducing a clear quantitative stability measure, bounded in [0, 1]. Lower values match models
with higher stability. The synthetic data sets selected (see ﬁgure 2) were a mixture of 5 Gaussians,
and segmented 2 rings. We also experimented on the Phoneme data set [6], which consists of
4, 500 log-periodograms of 5 phonemes uttered by English speakers, to which we applied PCA
projection on 3 principal components as a pre-processing step. The advantage of this data set is its
clear low-dimensional representation relative to its size, allowing us to get nearer to the asymptotic
convergence rates of the stability measures. All experiments used the k-means algorithm, except for
the ring data set which used the spectral clustering algorithm proposed in [13].

Complementing our theoretical analysis, the experiments clearly demonstrate that regardless of the
actual stability measures per ﬁxed sample size, they seem to eventually follow roughly constant and
differing convergence rates, with no substantial degradation in performance. In other words, when
stability works well for small sample sizes, it should also work at least as well for larger sample
sizes. The universal asymptotic convergence to zero does not seem to be a problem in that regard.

6 Conclusions

In this paper, we propose a principled approach for analyzing the utility of stability for cluster
validation in large ﬁnite samples. This approach stems from viewing stability as a measure of
generalization in a statistical setting. It leads us to predict that in contrast to what might be concluded
from previous work, cluster stability does not necessarily degrade with increasing sample size. This
prediction is substantiated both theoretically and empirically.

The results also provide some guidelines (via Thm. 2) for when a stability measure might be relevant
for arbitrarily large sample size, despite asymptotic universal stability. They also suggest that by
appropriate scaling, stability measures would become insensitive to the actual sample size used.
These guidelines do not presume a speciﬁc clustering framework. However, we have proven their
fulﬁllment rigorously only for a certain stability measure and clustering setting. The proof can be
generalized in principle, but only at the cost of a more involved analysis. We are currently working
on deriving more general theorems on when these guidelines apply.
Acknowledgements: This work has been partially supported by the NATO SfP Programme and the
PASCAL Network of excellence.

References
[1] Shai Ben-David. A framework for statistical clustering with a constant time approximation algorithms for
k-median clustering. In Proceedings of COLT 2004, pages 415–426.
[2] Shai Ben-David, Ulrike von Luxburg, and D ´avid P ´al. A sober look at clustering stability. In Proceedings
of COLT 2006, pages 5–19.
[3] Olivier Bousquet and Andr ´e Elisseeff. Stability and generalization. Journal of Machine Learning Re-
search, 2:499–526, 2002.
[4] Joachim M. Buhmann and Marcus Held. Model selection in clustering by uniform convergence bounds.
In Advances in Neural Information Processing Systems 12, pages 216–222, 1999.
[5] Andrea Caponnetto and Alexander Rakhlin. Stability properties of empirical risk minimization over
donsker classes. Journal of Machine Learning Research, 6:2565–2583, 2006.
[6] Trevor Hastie, Robert Tibshirani, Jerome Friedman. The Elements of Statistical Learning. Springer, 2001.
[7] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the Amer-
ican Statistical Association, 58(301):13–30, March 1963.
[8] Samuel Kutin and Partha Niyogi. Almost-everywhere algorithmic stability and generalization error. In
Proceeding of the 18th confrence on Uncertainty in Artiﬁcial Intelligence (UAI) , pages 275–282, 2002.
[9] Tilman Lange, Volker Roth, Mikio L. Braun, and Joachim M. Buhmann. Stability-based validation of
clustering solutions. Neural Computation, 16(6):1299–1323, June 2004.
[10] D.A. McAllester. Pac-bayesian stochastic model selection. Machine Learning Journal, 51(1):5–21, 2003.
[11] C. McDiarmid. On the method of bounded differences.
In Surveys in Combinatorics, volume 141 of
London Mathematical Society Lecture Note Series, pages 148–188. Cambridge University Press, 1989.
[12] Alexander Rakhlin and Andrea Caponnetto. Stability of k-means clustering.
In Advances in Neural
Information Processing Systems 19. MIT Press, Cambridge, MA, 2007.
[13] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8):888–905, 2000.
[14] Ulrike von Luxburg and Shai Ben-David. Towards a statistical theory of clustering. Technical report,
PASCAL workshop on clustering, London, 2005.

8


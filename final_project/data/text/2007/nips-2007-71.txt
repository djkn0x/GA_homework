Competition adds complexity

Judy Goldsmith
Department of Computer Science
University of Kentucky
Lexington, KY
goldsmit@cs.uky.edu

Martin Mundhenk
Friedrich-Schiller-Universit ¨at Jena
Jena, Germany
mundhenk@cs.uni-jena.de

Abstract

It is known that determinining whether a DEC-POMDP, namely, a cooperative
partially observable stochastic game (POSG), has a cooperative strategy with pos-
itive expected reward is complete for NEXP. It was not known until now how
cooperation affected that complexity. We show that, for competitive POSGs, the
complexity of determining whether one team has a positive-expected-reward strat-
egy is complete for NEXPNP .

1 Introduction

From online auctions to Texas Hold’em, AI is captivated by multi-agent interactions based on com-
petition. The problem of ﬁnding a winning strategy harks back to the ﬁrst days of chess programs.
Now, we are starting to have the capacity to handle issues like stochastic games, partial informa-
tion, and real-time video inputs for human player modeling. This paper looks at the complexity of
computations involving the ﬁrst two factors: partially obs ervable stochastic games (POSGs).

There are many factors that could affect the complexity of different POSG models: Do the players,
collectively, have sufﬁcient information to reconstruct a state? Do they communicate or cooperate?
Is the game zero sum, or do the players’ individual utilities depend on other players’ utilities? Do
the players even have models for other players’ utilities?

The ultimate question is, what is the complexity of ﬁnding a w inning strategy for a particular player,
with no assumptions about joint observations or knowledge of other players’ utilities. Since a special
case of this is the DEC-POMDP, where ﬁnding an optimal (joint , cooperative) policy is known to be
NEXP-hard [1], this problem cannot be any easier than in NEXP.
We show that one variant of this problem is hard for the class NEXPNP .

2 Deﬁnitions and Preliminaries

2.1 Partially observable stochastic games

A partially observable stochastic game (POSG) describes multi-player stochastic game with imper-
fect information by its states and the consequences of the players actions on the system. We follow
the de ﬁnition from [2] and denote it as a tuple M = (I , S, s0 , A, O, t , o, r), where

• I is the ﬁnite set {1, 2, . . . , k} of agents (or players), S is a ﬁnite set of
states, with distin-
guished initial state s0 ∈ S, A is a ﬁnite set of actions, and O is a ﬁnite set of observations
• t : S × Ak × S → [0, 1] is the transition probability function, where t (s, a1 , . . . , ak , s′ ) is the
probability that state s′ is reached from state s when each agent i chooses action ai
• o : S × I → O is the observation function , where o(s, i) is the observation made in state s
by agent i, and

1

• r : S × Ak × I → Z is the reward function, where r(s, a1 , . . . , ak , i) is the reward gained by
agent i in state s, when the agents take actions a1 , . . . , ak . (Z is the set of integers.)

A POSG where all agents have the same reward function is called a decentralized partially-
observable Markov decision process (see [1]).
Let M = (I , S, s0 , A, O, t , o, r) be a POSG. A step of M is a transition from one state to another
according to the transition probability function t . A run of M is a sequence of steps that starts in
the initial state s0 . The outcome of each step is probabilistic and depends on the actions chosen. For
each agent, a policy describes how to choose actions depending on observations made during the
run of the process. A (history-dependent) policy p chooses an action dependent on all observations
made by the agent during the run of the process. This is described as a function p : O∗ → A, mapping
each ﬁnite sequence of observations to an action.
A trajectory q of length |q| = m for M is a sequence of states q = s1 , s2 , . . . , sm (m ≥ 1, si ∈ S)
which starts with the initial state of M , i.e. s1 = s0 . Given policies p1 , . . . , pk , each trajectory q
has a probability prob(q, p1 , . . . , pk ). We will use some abbreviations in the sequel. For p1 , . . . , pk
we will write pk
1 , and for p1 (o(s1 , 1) · · · o(sj , 1)), . . . , pk (o(s1 , k) · · · o(sj , k)) we will write pk
1 (qj
1 )
accordingly. Then prob(q, p1 , . . . , pk ) is de ﬁned by
|q|−1(cid:213)
i=1

1 (qi
t (si , pk
1 ), si+1 ) .

prob(q, pk
1 ) =

We use Tl (s) to denote all length l trajectories which start in the initial state s0 and end in state s. The
1 ) obtained by agent i in state s after exactly l steps under policies pk
expected reward Ri (s, l , pk
1 is
the reward obtained in s by the actions according to pk
1 weighted by the probability that s is reached
after l steps,
(cid:229)
1 ), i) · prob(q, pk
1 (ql
r(s, pk
1 ) .
q∈Tl (s),q=(s1 ,...,sl )

Ri (s, l , pk
1 ) =

A POSG may behave differently under different policies. The quality of a policy is determined by
its performance, i.e. by the sum of expected rewards received on it. We use |M | to denote the size
of the representation of M .1 The short-term performance for policies pk
1 for agent i with POSG M
is the expected sum of rewards received by agent i during the next |M | steps by following the policy
pk
1 , i.e.
1 ) = (cid:229)
perf i (M , pk
s∈S
The performance is also called the expected reward.

Ri(s, |M |, pk
1 ) .

Agents may cooperate or compete in a stochastic game. We want to know whether a stochastic game
can be won by some agents. This is formally expressed in the following decision problems.
The cooperative agents problem for k agents:
a POSG M for k agents
instance:
are there policies p1 , . . . , pk under which every agent has positive performance ?
query:
(I.e. ∃p1 , . . . , pk : Vk
i=1 perf i (M , pk
1 ) > 0 ?)
The competing agents problem for 2k agents:
a POSG M for 2k agents
instance:
are there policies p1 , . . . , pk under which all agents 1, 2, . . . , k have positive per-
query:
formance independent of which policies agents k + 1, k + 2, . . . , 2k choose? (I.e.
∃p1 , . . . , pk ∀pk+1 , . . . , p2k : Vk
i=1 perf i (M , p2k
1 ) > 0 ?)
It was shown by Bernstein et al. [1] that the cooperative agents problem for two or more agents is
complete for NEXP.

1The size of the representation of M is the number of bits to encode the entire model, where the function
t , o, and r are encoded by tables. We do not consider smaller representations. In fact, smaller representations
may increase the complexity.

2

2.2 NEXPNP

A Turing machine M has exponential running time, if there is a polynomial p such that for every
input x, the machine M on input x halts after at most 2 p(|x|) steps. NEXP is the class of sets that
can be decided by a nondeterministic Turing machine within exponential time. NEXPNP is the class
of sets that can be decided by a nondeterministic oracle Turing machine within exponential time,
when a set in NP is used as an oracle. Similar as for the class NPNP , it turns out that a NEXPNP
computation can be performed by an NEXP oracle machine that asks exactly one query to a co NP
oracle and accepts if and only if the oracle accepts.

2.3 Domino tilings

Domino tiling problems are useful for reductions between different kinds of computations. They
have been proposed by Wang [3], and we will use it according to the following de ﬁnition.

De ﬁnition 2.1 We use [m] to denote the set {0, 1, 2, . . . , m − 1}. A tile type T = (V, H ) consists of
two ﬁnite sets V , H ⊆ N × N. A T -tiling of an m-square (m ∈ N) is a mapping t : [m] × [m] → N that
satisﬁes both the following conditions.

1. Every pair of two neighboured tiles in the same row is in H .
I.e. for all r ∈ [m] and c ∈ [m − 1], (t(r, c), t(r, c + 1)) ∈ H .
2. Every pair of two neighboured tiles in the same column is in V .
I.e. for all r ∈ [m − 1] and c ∈ [m], (t(r, c), t(r + 1, c)) ∈ V .

The exponential square tiling problem is the set of all pairs (T , 1k ), where T is a tile type and 1k is
a string consisting of k 1s (k ∈ N), such that there exists a T -tiling of the 2k -square.

It was shown by Savelsbergh and van Emde Boas [4] that the exponential square tiling problem
is complete for NEXP. We will consider the following variant, which we call the exponential S2
square tiling problem: given a pair (T , 1k ), does there exist a row w of tiles and a T -tiling of the
2k -square with ﬁnal row w, such that there exists no T -tiling of the 2k -square with initial row w?
The proof technique of Theorem 2.29 in [4], which translates Turing machine computations into
tilings, is very robust in the sense that simple variants of the square tiling problem can analogously
be shown to be complete for different complexity classes. Together with the above characterization
of NEXPNP it can be used to prove the following.
Theorem 2.2 The exponential S2 square tiling problem is complete for NEXPNP .

3 Results

POSGs can be seen as a generalization of partially-observable Markov decision processes (PO-
MDPs) in that POMDPs have only one agent and POSGs allow for many agents. Papadimitriou
and Tsitsiklis [5] proved that it is PSPACE-complete to decide the cooperative agents problem for
POMDPs. The result of Bernstein et al. [1] shows that in case of history-dependent policies, the
complexity of POSGs is greater than the complexity of POMDPs. We show that this difference
does not appear when stationary policies are considered instead of history-dependent policies. For
POMDPs, the problem appears to be NP-complete [6]. A stationary policy is a mapping O → A
from observations to actions. Whenever the same observation is made, the same action is chosen by
a stationary policy.

Theorem 3.1 For any k ≥ 2, the cooperative agents problem for k agents for stationary policies is
NP-complete.

Proof We start with proving NP-hardness. A POSG with only one agent is a POMDP. The problem
of deciding, for a given POMDP M , whether there exists a stationary policy such that the short-term
performance of M is greater than 0, is NP-complete [6]. Hence, the cooperative agents problem for
stationary policies is NP-hard.

3

It remains to show containment in NP. Let M = (I , S, s0 , A, O, t , o, r) be a POSG. We assume that t
is represented in a straightforward way as a table. Let p1 , . . . , pk be a sequence of stationary policies
for the k agents. This sequence can be straightforwardly represented using not more space than
the representation of t takes. Under a ﬁxed sequence of policies, the performance of
the POSG for
all of the agents can be calculated in polynomial time. Using a guess and check approach (guess
the stationary policies and evaluate the POSG), this shows that the cooperative agents problem for
stationary policies is in NP.
2

In the same way we can characterize the complexity of a problem that we will need in the proof of
Lemma 3.3.

Corollary 3.2 The following problem is coNP-complete.
a POSG M for k agents
instance:
query:
do all agents under every stationary policy have positive performance? (I.e.
i=1 perf i (M , pk
∀stationary p1 . . . pk : Vk
1 ) > 0 ?)
The cooperative agents problem was shown to be NEXP-complete by Bernstein et al. [1]. Not
surprisingly, if the agents compete, the problem becomes harder.

Lemma 3.3 For every k ≥ 1, the competing agents problem for 2k agents is in NEXPNP .
Proof The basic idea is as follows. We guess policies p1 , p2 , . . . , pk for agents 1, 2, . . . , k, and
construct a POSG that “implements” these policies and leave
s open the actions chosen by agents
k + 1, . . . , 2k.
This new POSG has states for all short-term trajectories through the origin POSG. Therefore, its
size is exponential in the size of the origin POSG. Because the history is stored in every state, and
the POSG is loop-free, it turns out that the new POSG can be taken as a POMDP for which a (joint)
policy with positive reward is searched. This problem is known to be NP-complete.
Let M = (I , S, s0 , A, O, t , o, r) be a POSG with 2k agents, and let p1 , . . . , pk be short-term policies for
, r ′ ) as follows2 . In M ′ , we have as agents
M . We de ﬁne a k-agent POSG M ′ = (I ′
, S′
, s′
0 , A, O′
, t ′
, o′
I ′ = {k + 1, . . . , 2k}. The set of states of M ′ is the
those of M , whose policies are not ﬁxed, i.e.
cross product of states from M and all trajectories up to length |M | over S, i.e. S′ = S × S≤|M |+1 .
The meaning of state (s, u) ∈ S′ is, that state s can be reached on a trajectory u (that ends with s)
0 = (s0 , s0 ). The state (s0 , e) is taken as
0 is s′
through M with the ﬁxed policies. The initial state s′
a special sink state. After |M | + 2 steps, the sink state is entered in M ′ and it is not left thereafter.
All rewards gained in the sink state are 0. Now for the transition probabilities. If s is reached on
trajectory u in M and the actions a1 , . . . , ak are according to the ﬁxed policies p1 , . . . , pk , then the
probabiliy of reaching state s′ on trajectory us′ according to t in M is the same as to reach (s′
, us′ )
in M ′ from (s, u). In the formal description, the sink state has to be considered, too.
t ′((s, u), ak , . . . , a2k , ( ˆs, ˆu)) =
0,

t (s, p1 (o(us, 1)), · · · , pk (o(us, k)), ak+1 , . . . , a2k , ˆs),
1,

The observation in M ′ is the sequence of observations made in the trajectory that is contained in
each state, i.e. o′ ((s, w)) = o(w), where o(e) is any element of O. Finally, the rewards. Essentially,
we are interested in the rewards obtained by the agents 1, 2, . . . , k. The rewards obtained by the other
agents have no impact on this, only the actions the other agents choose. Therefore, agent i obtains the
rewards in M ′ that are obtained by agent i − k in M . In this way, the agents k + 1, . . . , 2k obtain in
M ′ the same rewards that are obtained by agents 1, 2, . . . , k in M , and this is what we are interested
in. This results in r ′ ((s, u), ak , . . . , a2k , i) = r(s, p1 (o(u, 1)), · · · , pk (o(u, k)), ak+1 , . . . , a2k , i − k) for
i = k + 1, . . . , 2k.

if u 6= e and u ˆs 6= ˆu
if ˆu = u ˆs, | ˆu| ≤ |M |, u 6= e
if |u| = |M | + 1 or u = e, and ˆu = e

2 S≤|M | denotes the set of sequences up to |M | elements from S. The empty sequence is denoted by
e. For w ∈ S≤|M | we use o(w, i) to describe the sequence of observations made by agent i on trajectory w.
The concatenation of sequences u and w is denoted uw. We do not distinguish between elements of sets and
sequences of one element.

4

Notice that the size of M ′ is exponential in the size of M . The sink state in M ′ is the only state that
lies on a loop. This means, that on all trajectories through M ′ , the sink state is the only state that
may appear more than once. All states other than the sink state contain the full history of how they
are reached. Therefore, there is a one-to-one correspondence between history-dependent policies
for M and stationary policies for M ′ (with regard to horizon |M |). Moreover, the corresponding
policies have the same performances.
Claim 1 Let p1 , . . . , p2k be short-term policies for M , and let ˆpk+1 , . . . , ˆp2k be their corresponding
stationary policies for M ′ .
For |M | steps and i = 1, 2, . . . , k, perf i (M , p2k
1 ) = perf i+k (M ′

, ˆp2k
k+1 ).

Thus, this yields an NEXPNP algorithm to decide the competitive agents problem. The input is a
POSG M for 2k agents. In the ﬁrst step, the policies for the agents 1 , 2, . . . , k are guessed. This takes
nondeterministic exponential time. In the second step, the POSG M ′ is constructed from the input
M and the guessed policies. This takes exponential time (in the length of the input M ). Finally, the
oracle is queried whether M ′ has positive performance for all agents under all stationary policies.
This problem belongs to coNP (Corollary 3.2). Henceforth, the algorithm shows the competing
agents problem to be in NEXPNP .
2

Lemma 3.4 For every k ≥ 2, the competing agents problem for 2k agents is hard for NEXPNP .
Proof We give a reduction from the exponential S2 square tiling problem to the competing agents
problem.
Let T = (T , 1k ) be an instance of the exponential S2 square tiling problem, where T = (V, H ) is a
tile type. We will show how to construct a POSG M with 4 agents from it, such that T is a positive
instance of the exponential S2 square tiling problem if and only if (1) agents 1 and 2 have a tiling
for the 2k square with ﬁnal row w such that (2) agents 3 and 4 have no tiling for the 2k square with
initial row w.

The basic idea for checking of tilings with POSGs for two agents stems from Bernstein et al. [1],
but we give a slight simpliﬁcation of their proof technique,
and in fact have to extend it for four
agents later on. The POSG is constructed so that on every trajectory each agent sees a position in
the square. This position is chosen by the process. The only action of the agent that has impact
on the process is putting a tile on the given position. In fact, the same position is observed by the
agents in different states of the POSG. From a global point of view, the process splits into two parts.
The ﬁrst part checks whether both agents know the same tiling , without checking that it is a correct
tiling. In the state where the agents are asked to put their tiles on the given position, a high negative
reward is obtained if the agents put different tiles on that position.
”High negative ” means that,
if there is at least one trajectory on which such a reward is obtained, then the performance of the
whole process will be negative. The second part checks whether the tiling is correct. The idea is to
give both the agents neighboured positions in the square and to ask each which tile she puts on that
position. Notice that the agents do not know in which part of the process they are. This means, that
they do not know whether the other agent is asked for the same position, or for its upper or right
neighbour. This is why the agents cannot cheat the process. A high negative reward will be obtained
if the agents’ tiles do not ﬁt together.
For the ﬁrst part, we need to construct is a POSG Pk for two agents, that allows both agents to
make the same sequence of observations consisting of 2k bits. This sequence is randomly chosen,
and encodes a position in a 2k × 2k grid. At the end, state same is reached, at which no observation is
made. At this state, it will be checked whether both agents put the same tile at this position (see later
on). The task of Pk is to provide both agents with the same position. Figure 1 shows an example
for a 24 × 24 -square. The initial state is s4 . Dashed arrows indicate transitions with probability 1
2
independent of the actions. The observation of agent 1 is written on the left hand side of the states,
and the observations of agent 2 at the right hand side. In s4 , the agents make no observation. In Pk
both agents always make the same observations.

The second part is more involved. The goal is to provide both agents with neighboured positions
in the square. Eventually, it is checked whether the tiles they put on the neighboured positions
are according to the tile type T . Because the positions are encoded in binary, we can make use

5

s4

s

s

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

0

0

0

0

0

0

0

0

0

0

0

0

0

1

1

1

1

1

1

1

1

0

1

1

1

1

1

1

0

1

0

0

0

0

0

0

1

0

row

0

0

column

same

hori

same

check tiles

Figure 1: P4

Figure 2: C3,4

Figure 3: L3,4

of the following fact of subsequent binary numbers. Let u = u1 . . . uk and w = w1 . . . wk be bitwise
representation of strings.
if nw = nu + 1, then for some index l it holds that (1) ui = wi for i =
1, 2, . . . , l − 1, (2) wl = 1 and ul = 0, and (3) w j = 0 and u j = 1 for j = l + 1, . . . , k.
The POSG Cl ,k is intended to provide the agents with two neighboured positions in the same row,
where the index of the leftmost bit of the column encoding where both positions distinguish is l .
(The C stands for column.) Figure 2 shows an example for the 24 -square. The “
ﬁnal state ” of
Cl ,k
is the state hori, from which it is checked whether the agents put horizontically ﬁtting tiles together.
In the same way, a POSG Rl ,k can be constructed (R stands for row), whose task is, to check
whether two tiles in neighboured rows correspond to a correct tiling. This POSG has the ﬁnal state
ver t, from which on it is checked whether two tiles ﬁt vertically.

Finally, we have to construct the last part of the POSG. It consists of the states same, hori, ver t (as
mentioned above), good, bad, and sink. All transitions between these states are deterministic (i.e.
with probability 1). From state same the state good is reached, if both agents take the same action
– otherwise bad is reached. From state hori the state good is reached, if action a1 by agent 1 and a2
by agent 2 make a pair (a1 , a2 ) in H , i.e. in the set of horizontically correct pairs of tiles – ot herwise
bad is reached. Similarly, from state ver t the state good is reached, if action a1 by agent 1 and a2 by
agent 2 make a pair (a1 , a2 ) in V . All these transitions are with reward 0. From state good the state
sink is reached on every action with reward 1, and from state bad the state sink is reached on every
action with reward −(22k+2 ). When the state sink is reached, the process stays there on any action,
and all agents obtain reward 0. All rewards are the same for both agents. (This part can be seen in
the overall picture in Figure 4).
From these POSGs we construct a POSG T2,k that checks whether two agents know the same correct
tiling for a 2k × 2k square, as described above. There are 2k + 1 parts of T2,k . The initial state of each
part can be reached with one step from the initial state s0 of T2,k . The parts of T2,k are as follows.

• P2k with initial state s (checks whether two agents have the same tiling)
• For each l = 1, 2, . . . , k, we take Cl ,k . Let cl be the initial state of Cl ,k .

6

s0

ck

r1

Ck,k

R1,k

rk

Rk,k

sk

Pk

c1

C1,k

same

hori

ver t

good

bad

sink

Figure 4: T2,k

• For each l = 1, 2, . . . , k, we take Rl ,k . Let rl be the initial state of Rl ,k .
There are 22k + 2 · (cid:229)k
l=1 2k · 2l−1 =: t r(k) trajectories with probability > 0 through T2,k . Notice
that t r(k) < 22k+2 . From the initial state s0 of T2,k , each of the initial states of the parts is reachable
independent on the action chosen by the agents. We will give transition probabilities to the transition
from s0 to each of the initial states of the parts in a way, that eventually each trajectory has the same
probability.

if s′ = s, i.e. the initial state of Pk
if s ∈ {rl , cl | l = 1, 2, . . . , k}

t (s0 , a1 , a2 , s′ ) = ( 22k
t r(k) ,
2k+l−1
t r(k)
In the initial state s0 and in the initial states of all parts, the observation e is made. When a state
same, hori, ver t is reached, each agent has made 2k + 3 observations, where the ﬁrst and last are e
and the remaining 2k are each in {0, 1}. Such a state is the only one where the actions of the agents
have impact on the process. Because of the partial observability, they cannot know in which part
of T2,k they are. The agents can win, if they both know the same correct tiling and interpret the
sequence of observations as the position in the grid they are asked to put a tile on. On the other
hand, if both agents know different tilings or the tiling they share is not correct, then at least one
trajectory will end in a bad state and has reward −(22k+2 ). The structure of the POSG is given in
Figure 4.

Claim 2 Let (T , 1k ) be an instance of the exponential square tiling problem.

(1) There exists a polynomial time algorithm, that on input (T , 1k ) outputs T2,k .

(2) There exists a T -tiling of the 2k square if and only if there exist policies for the agents under
which T2,k has performance > 0.

Part (1) is straightforward. Part (2) is not much harder. If there exists a T -tiling of the 2k square,
both agents use the same policy according to this tiling. Under these policies, state bad will not be
reached. This guarantess performance > 0 for both agents. For the other direction: if there exist
policies for the agents under which T2,k has performance > 0, then state bad is not reached. Hence,
T -tiling of the 2k
both agents use the same policy. It can be shown inductively that this policy “is” a
square.

7

The POSG for the competing agents problem with 4 agents consists of three parts. The ﬁrst part is
a copy of T2,k . It is used to check whether the ﬁrst square can be tiled corre ctly (by agents 1 and
2). In this part, the negative rewards are increased in a way that guarantees the performance of the
POSG to be negative whenever agents 1 and 2 do not correctly tile their square. The second part
is a modiﬁed copy of T2,k . It is used to check whether the second square can be tiled correctly (by
agents 3 and 4). Whenever state bad is left in this copy, reward 0 is obtained, and whenever state
good is left, reward −1 is obtained. The third part checks whether agent 1 puts the same tiles into
the last row of its square as agent 3 puts into the ﬁrst row of it s square. (See L3,4 in Figure 3 as an
example.) If this succeeds, the performance of the third part equals 0, otherwise it has performance
1. These three parts run in parallel.

If agents 1 and 2 have a tiling for the ﬁrst square, the perform ance of the ﬁrst part equals 1.

• If agents 3 and 4 are able to continue this tiling through their square, the performance
of the second part equals −1 and the performance of the third part equals 0. At all, the
performance of the POSG under these policies equals 0.
• If agents 3 and 4 are not able to continue this tiling through their square, then the perfor-
mance of part 2 and part 3 is strictly greater −1. At all, the performance of the POSG under
these policies is > 0.

Lemmas 3.3 and 3.4 together yield completeness of the competing agents problem.

Theorem 3.5 For every k ≥ 2, the competing agents problem for 2k agents is complete for NEXPNP .

2

4 Conclusion

We have shown that competition makes life —and computation —
more complex. However, in order
to do so, we needed teamwork. It is not yet clear what the complexity is of determining the existence
of a good strategy for Player I in a 2-person POSG, or a 1-against-many POSG.
There are other variations that can be shown to be complete for NEXPNP , a complexity class that,
shockingly, has not been well explored. We look forward to further results about the complexity of
POSGs, and to additional NEXPNP -completeness results for familiar AI and ML problems.

References

[1] Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity
of decentralized control of Markov decision processes. Math. Oper. Res., 27(4):819 –840, 2002.
[2] E. Hansen, D. Bernstein, and S. Zilberstein. Dynamic programming for partially observable
stochastic games. In Proceedings of the Nineteenth National Conference on Artiﬁ cial Intelli-
gence (AAAI-04), pages 709 –715, 2004.
[3] Hao Wang. Proving theorems by pattern recognition II. Bell Systems Technical Journal, 40:1 –
42, 1961.
[4] M. Savelsbergh and P. van Emde Boas. Bounded tiling, an alternative to satisﬁability. In Gerd
Wechsung, editor, 2nd Frege Conference, volume 20 of Mathematische Forschung, pages 354 –
363. Akademie Verlag, Berlin, 1984.
[5] C.H. Papadimitriou and J.N. Tsitsiklis. The complexity of Markov decision processes. Mathe-
matics of Operations Research, 12(3):441 –450, 1987.
[6] Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. Complexity results
for ﬁnite-horizon Markov decision process problems. Journal of the ACM, 47(4):681 –720, 2000.

8


Classiﬁcation via Minimum Incremental Coding
Length (MICL)

John Wright∗, Yi Ma
Coordinated Science Laboratory
University of Illinois at Urbana-Champaign
{jnwright,yima}@uiuc.edu

Yangyu Tao, Zhouchen Lin, Heung-Yeung Shum
Visual Computing Group
Microsoft Research Asia
{v-yatao,zhoulin,hshum}@microsoft.com

Abstract

We present a simple new criterion for classiﬁcation, based on principles from lossy
data compression. The criterion assigns a test sample to the class that uses the min-
imum number of additional bits to code the test sample, subject to an allowable
distortion. We prove asymptotic optimality of this criterion for Gaussian data and
analyze its relationships to classical classiﬁers. Theoretical results provide new
insights into relationships among popular classiﬁers such as MAP and RDA, as
well as unsupervised clustering methods based on lossy compression [13]. Mini-
mizing the lossy coding length induces a regularization effect which stabilizes the
(implicit) density estimate in a small-sample setting. Compression also provides
a uniform means of handling classes of varying dimension. This simple classi-
ﬁcation criterion and its kernel and local versions perform competitively against
existing classiﬁers on both synthetic examples and real imagery data such as hand-
written digits and human faces, without requiring domain-speciﬁc information.

1

Introduction

One quintessential problem in statistical learning [9, 20] is to construct a classiﬁer from labeled
training data (xi , yi ) ∼iid pX,Y (x, y). Here, xi ∈ Rn is the observation, and yi ∈ {1, . . . , K } its
associated class label. The goal is to construct a classiﬁer g : Rn → {1, . . . , K } which minimizes
the expected risk (or probability of error): g∗ = arg min E[Ig(X ) (cid:54)=Y ], where the expectation is taken
with respect to pX,Y . When the conditional class distributions pX |Y (x|y) and the class priors pY (y)
are given, the maximum a posterior (MAP) assignment
ˆy(x) = arg miny∈{1,...,K } − ln pX |Y (x|y) − ln pY (y)
(1)
gives the optimal classiﬁer. This amounts to a minimum coding length principle: the optimal clas-
siﬁer minimizes the Shannon optimal (lossless) coding length of the test data x with respect to
the distribution of the true class. The ﬁrst term is the number of bits needed to code x w.r.t. the
distribution of class y , and the second term is the number of bits needed to code the label y for x.

Issues with Learning the Distributions from Training Samples.
In the typical classiﬁcation
setting, the distributions pX |Y (x|y) and pY (y) need to be learned from a set of labeled training
∗The authors gratefully acknowledge support from grants NSF Career IIS-0347456, NSF CRS-EHS-
0509151, NSF CCF-TF-0514955, and ONR YIP N00014-05-1-0633.

1

data. Conventional approaches to model estimation (implicitly) assume that the distributions are
nondegenerate and the samples are sufﬁciently dense. However, these assumptions fail in many
classiﬁcation problems which are vital for applications in computer vision [10, 11]. For instance, the
set of images of a human face taken from different angles and under different lighting conditions
often lie in a low-dimensional subspace or submanifold of the ambient space [2]. As a result, the as-
sociated distributions are degenerate or nearly degenerate. Moreover, due to the high dimensionality
of imagery data, the set of training images is typically sparse.
Inferring the generating probability distribution pX,Y from a sparse set of samples is an inherently
ill-conditioned problem [20]. Furthermore, in the case of degenerate distributions, the classical
likelihood function (1) does not have a well-deﬁned maximum [20]. Thus, to infer the distribution
from the training data or to use it to classify new observations, the distribution or its likelihood
function needs to be properly “regularized.” Typically, this is accomplished either explicitly via
smoothness constraints, or implicitly via parametric assumptions on the distribution [3]. However,
even if the distributions are assumed to be generic Gaussians, explicit regularization is still necessary
to achieve good small-sample performance [6].
In many real problems in computer vision, the distributions associated with different classes of data
have different model complexity. For instance, when detecting a face in an image, features associated
with the face often have a low-dimensional structure which is “embedded” as a submanifold in a
cloud of essentially random features from the background. Model selection criteria such as minimum
description length (MDL) [12, 16] serve as important modiﬁcations to MAP for model estimation
across classes of different complexity. It selects the model that minimizes the overall coding length
of the given (training) data, hence the name “minimum description length” [1]. Notice, however, that
MDL does not specify how the model complexity should be properly accounted for when classifying
new test data among models that have different dimensions.

Solution from Lossy Data Coding. Given the difﬁculty of learning the (potentially degenerate)
distributions pX |Y (x|y) from a few samples in a high-dimensional space, it makes more sense to
seek good “surrogates” for implementing the minimum coding length principle (1). Our idea is to
measure how efﬁciently a new observation can be encoded by each class of the training data subject
to an allowable distortion, and to assign the new observation to the class that requires the minimum
number of additional bits. We dub this criterion “minimum incremental coding length” (MICL) for
classiﬁcation. It provides a counterpart of the MDL principle for model estimation and as a surrogate
for the minimum coding length principle for classiﬁcation.
The proposed MICL criterion naturally addresses the issues of regularization and model complexity.
Regularization is introduced through the use of lossy coding, i.e. coding the test data x upto an
allowable distortion1 (placing our approach along the lines of lossy MDL [15]). This contrasts with
Shannon’s optimal lossless coding length, which requires precise knowledge of the true distributions.
Lossy coding length also accounts for model complexity by directly measuring the difference in the
volume (hence dimension) of the training data with and without the new observation.

Relationships to Existing Classiﬁers. While MICL and MDL both minimize a coding-theoretic
objective, MICL differs strongly from traditional MDL approaches to classiﬁcation such as those
proven inconsistent in [8]. Those methods chose a decision boundary that minimizes the total num-
ber of bits needed to code the boundary and the samples it incorrectly classiﬁes. In contrast, MICL
uses coding length directly as a measure of how well the training data represent the new sample.
The inconsistency result of [8] does not apply in this modiﬁed context. Within the lossy data cod-
ing framework, we establish that the MICL criterion leads to a family of classiﬁers that generalize
the conventional MAP classiﬁer (1). We prove that for Gaussian distributions, the MICL criterion
asymptotically converges to a regularized version of MAP2 (see Theorem 1) and give a precise es-
timate of the convergence rate (see Theorem 2). Thus, lossy coding induces a regularization effect
similar to Regularized Discriminant Analysis (RDA) [6], with similar gains in ﬁnite sample per-
formance with respect to MAP/QDA. The fully Bayesian approach to model estimation, in which
posterior distributions over model parameters are estimated also provides ﬁnite sample gains over

1 Information Bottleneck also uses lossy coding, but in an unsupervised manner, for clustering, feature
selection and dimensionality reduction [19]. We apply lossy coding in the supervised (classiﬁcation) setting.
2MAP subject to a Gaussian assumption is often referred to as Quadratic Discriminant Analysis (QDA) [9].

2

ML/MAP [14]. However, that method is sensitive to the choice of prior when the number of samples
is less than the dimension of the space, a situation that poses no difﬁculty to our proposed classiﬁer.
When the distributions involved are not Gaussian, the MICL criterion can still be applied locally,
similar to the popular k-Nearest Neighbor (k-NN) classiﬁer. However, the local MICL classi-
ﬁer signiﬁcantly improves the k-NN classiﬁer as it accounts for both the number of samples and
the distribution of the samples within the neighborhood. MICL can also be kernelized to handle
nonlinear/non-Gaussian data, an extension similar to the generalization of Support Vector Machines
(SVM) to nonlinear decision boundaries. The kernelized version of MICL provides a simple alter-
native to the SVM approach of constructing a linear decision boundary in the embedded (kernel)
space, and better exploits the covariance structure of the embedded data.

2 Classiﬁcation Criterion and Analysis

2.1 Minimum Incremental Coding Length.
A lossy coding scheme [5] maps vectors X = (x1 , . . . , xm ) ∈ Rn×m to a sequence of binary bits,
from which the original vectors can be recovered upto an allowable distortion E[(cid:107) ˆx − x(cid:107)2 ] ≤ ε2 .
The length of the bit sequence is then a function Lε (X ) : Rn×m → Z+ . If we encode each class
represented by a two-part code using (cid:80)K
of training data Xj
.= {xi : yi = j } separately using Lε (Xj ) bits, the entire training dataset can be
j=1 Lε (Xj ) − |Xj | log2 pY (j ) bits. Here, the second term is
the minimum number of bits needed to (losslessly) code the class labels yi .
Now, suppose we are given a test observation x ∈ Rn , whose associated class label y(x) = j is
unknown. If we code x jointly with the training data Xj of the j th class, the number of additional
bits needed to code the pair (x, y) is δLε (x, j ) = Lε (Xj ∪ {x}) − Lε (Xj ) + L(j ). Here, the ﬁrst two
terms measure the excess bits needed to code (x, Xj ) upto distortion ε2 , while the last term L(j )
is the cost of losslessly coding the label y(x) = j . One may view these as “ﬁnite-sample lossy”
surrogates for the Shannon coding lengths in the ideal classiﬁer (1). This interpretation naturally
leads to the following classiﬁer:
Criterion 1 (Minimum Incremental Coding Length). Assign x to the class which minimizes the
number of additional bits needed to code (x, ˆy), subject to the distortion ε:
ˆy(x) .= arg minj∈{1,...,K } δLε (x, j ).

(2)

The above criterion (2) can be taken as a general principle for classiﬁcation, in the sense that it can be
applied using any lossy coding scheme. Nevertheless, effective classiﬁcation demands that the cho-
sen coding scheme be approximately optimal for the given data. From a ﬁnite sample perspective,
Lε should approximate the Kolmogorov complexity of X , while in an asymptotic, statistical setting
it should approach the lower bound given by the rate-distortion of the generating distribution [5].

Lossy Coding of Gaussian Data. We will ﬁrst consider a coding length function Lε introduced
and rigorously justiﬁed in [13], which is (asymptotically) optimal for Gaussians. The (implicit) use
of a coding scheme which is optimal for Gaussian sources is equivalent to assuming that the condi-
tional class distributions pX |Y can be well-approximated by Gaussians. After rigorously analyzing
this admittedly restrictive scenario, we will extend the MICL classiﬁer (with this same Lε function)
2 log2 det (cid:0)I + n
ε2 Σ(cid:1) (bits/vector). Observations
to arbitrary, multimodal distributions via an effective local Gaussian approximation.
For a multivariate Gaussian source N (µ, Σ), the average number of bits needed to code a vector
(cid:80)
(cid:80)
subject to a distortion ε2 is approximately Rε (Σ) .= 1
X = (x1 , . . . , xm ) with sample mean ˆµ = 1
i xi and covariance ˆΣ(X ) = 1
i (xi −
m−1
m
ˆµ)(xi − ˆµ)T can be represented upto expected distortion ε2 using ≈ mRε ( ˆΣ) bits. The optimal
(cid:1) bits. The total number of bits required to code X is therefore
(cid:0)1 + ˆµT ˆµ
codebook is adaptive to the data, and can be encoded by representing the principal axes of the
covariance using an additional nRε ( ˆΣ) bits. Encoding the mean vector µ requires an additional
(cid:16)
(cid:17)
(cid:1).
(cid:0)1 +
2 log2
n
ε2
ˆµT ˆµ
Lε (X ) .= m + n
ˆΣ(X )
+ n
I + n
log2
log2 det
2
2
ε2
ε2

(3)

3

k-NN
SVM-RBF
Figure 1: MICL harnesses linear structure in the data to interpolate (left) and extrapolate (center) in
sparsely sampled regions. Popular classiﬁers such as k-NN and SVM-RBF do not (right).

MICL

The ﬁrst term gives the number of bits needed to represent the distribution of the xi about their mean,
and the second gives the cost of representing the mean. The above function well-approximates the
optimal coding length for Gaussian data, and has also been shown to give a good upper bound on the
number of bits needed to code ﬁnitely many samples lying on a linear subspace (e.g., a degenerate
Gaussian distribution) [13].

Coding the Class Label. Since the label Y is discrete, it can be coded losslessly. If the test class
labels Y are known to have the marginal distribution P [Y = j ] = πj , then the optimal coding
lengths are (within one bit): L(j ) = − log2 πj . In practice, we may replace πj with the estimate
ˆπj = |Xj |/m. Notice that as in the MAP classiﬁer, the πj essentially form a prior on class labels.
Combining this coding length the class label with the coding length function (3) for the observations,
we summarize the MICL criterion (2) as Algorithm 1 below:

Algorithm 1 (MICL Classiﬁer).
1: Input: m training samples partitioned into K classes X1 , X2 , . . . , XK and a test sample x.
2: Compute prior distribution of class labels ˆπj = |Xj |/m.
3: Compute incremental coding length of x for each class:
(cid:1).
(cid:0)1 + ˆµT ˆµ
ε2 ˆΣ(X )(cid:1) + n
log2 det (cid:0)I + n
δLε (x, j ) = Lε (Xj ∪ {x}) − Lε (Xj ) − log2 ˆπj ,
Lε (X ) .= m+n
2 log2
where
ε2
2
4: Output: ˆy(x) = arg minj=1,...,K δLε (x, j ).
The Lε (Xj ∪ {x}) can be computed in O(min(m, n)2 ) time (see [21]), allowing the MICL classiﬁer
to be directly applied to high-dimensional data. Figure 1 shows the performance of Algorithm 1 on
two toy problems. In both cases, the MICL criterion harnesses the covariance structure of the data
to achieve good classiﬁcation in sparsely sampled regions. In the left example, the criterion inter-
polates the data structure to achieve correct classiﬁcation, even near the origin where the samples
are sparse. In the right example, the criterion extrapolates the horizontal line to the other side of the
plane. Methods such as k-NN and SVM do not achieve the same effect. Notice, however, that these
decision boundaries are similar to what MAP/QDA would give. This raises an important question:
what is the precise relationship between MICL and MAP, and when is MICL superior?

2.2 Asymptotic Behavior and Relationship to MAP

In this section, we analyze the asymptotic behavior of Algorithm 1 as the number of training samples
goes to inﬁnity. The following result, whose proof is given in [21], indicates that MICL converges
to a regularized version of ML/MAP, subject to a reward on the dimension of the classes:
Theorem 1 (Asymptotic MICL [21]). Let the training samples {(xi , yi )}m
i=1 ∼iid pX,Y (x, y), with
.= E[X |Y = j ], Σj
.= C ov(X |Y = j ). Then as m → ∞, the MICL criterion coincides
(cid:17)
(cid:16)
µj
x (cid:12)(cid:12) µj , Σj + ε2
(asymptotically, with probability one) with the decision rule
1
LG
2 Dε (Σj ),
+ ln πj +
ˆy(x) = argmax
I
n
j=1,...,K
where LG (·| µ, Σ) is the log-likelihood function for a N (µ, Σ) distribution , and Dε (Σj )
n I )−1 ) is the effective dimension of the j -th model, relative to the distortion ε2 .
tr(Σj (Σj + ε2

(4)
.=

4

Figure 2: Left: Excess risk incurred by using MAP rather than MICL, as a function of ε and m. (a)
isotropic Gaussians. (b) anisotropic Gaussians. Right: Excess risk for nested classes, as a function
of n and m. (c) MICL vs. MAP. (d) MICL vs. RDA. In all examples, MICL is superior for n (cid:29) m.

This result shows that asymptotically, MICL generates a family of MAP-like classiﬁers parametrized
by the distortion ε2 . If all of the distributions are nondegenerate (i.e. their covariance matrices Σj
are nonsingular), then limε→0 (Σj + ε2
n I ) = Σj and limε→0 Dε (Σj ) = n, a constant across the
various classes. Thus, for nondegenerate data, the family of classiﬁers induced by MICL contains
the conventional MAP classiﬁer (1) at ε = 0. Given a ﬁnite number, m, of samples, any reasonable
rule for choosing the distortion ε2 should therefore ensure that ε → 0 as m → ∞. This guarantees
that for non-degenerate distributions, MICL converges to the asymptotically optimal MAP criterion.
Simulations (e.g., Figure 1) suggest that the limiting behavior provides useful information even
for ﬁnite training data. The following result, proven in [21], veriﬁes that the MICL discriminant
functions δLε (x, j ) converge quickly to their limiting form δL∞
ε (x, j ):
Theorem 2 (MICL Convergence Rate [21]). As the number of samples, m → ∞, the MICL criterion
ε (z , j )(cid:12)(cid:12) ≤ c(α) · m− 1
1 − α, (cid:12)(cid:12)δLε (z , j ) − δL∞
(2) converges to its asymptotic form, (4) at a rate of m− 1
2 . More speciﬁcally, with probability at least
2 for some constant c(α) > 0.
Improvements over MAP
2.3

In the above, we have established the fact that asymptotically, the MICL criterion (4) is just as good
as the MAP criterion. Nevertheless, the MICL criterion makes several important modiﬁcations to
MAP, which signiﬁcantly improve its performance on sparsely sampled or degenerate data.

Regularization and Finite-Sample Behavior. Notice that the ﬁrst two terms of the asymptotic
MICL criterion (4) have the form of a MAP criterion, based on an N (µ, Σ + ε2
n I ) distribution.
This is somewhat equivalent to softening the distribution by ε2
n along each dimension, and has two
important effects. First, it renders the associated MAP decision rule well-deﬁned, even if the true
data distribution is (almost) degenerate. Even for non-degenerate distributions, there is empirical
evidence that for appropriately chosen ε, ˆΣ + ε2
n I gives more stable ﬁnite-sample classiﬁcation [6].
Figure 2 demonstrates this effect on two simple examples. The generating distributions are param-
eterized as (a) µ1 = [− 1
2 , 0], Σ1 = Σ2 = I , and (b) µ1 = [− 3
2 , 0], µ2 = [ 1
4 , 0], µ2 = [ 3
4 , 0],
Σ1 = diag(1, 4), Σ2 = diag(4, 1). In each example, we vary the number of training samples, m,
and the distortion ε. For each (m, ε) combination, we draw m training samples from two Gaus-
sian distributions N (µi , Σi ), i = 1, 2, and estimate the Bayes risk of the resulting MICL and MAP
classiﬁers. This procedure is repeated 500 times, to estimate the overall Bayes risk with respect to
variations in the training data. Figure 2 visualizes the difference in risks, RM AP − RM ICL . Posi-
tive values indicate that MICL is outperforming MAP. The red line approximates the zero level-set,
where the two methods perform equally well. In the isotropic case (a), MICL outperforms MAP for
all sufﬁciently large ε. with a larger performance gain when the number of samples is small. In the
anisotropic case (b), for most ε, MICL dramatically outperforms MAP for small sample sizes. We
will see in the next example that this effect becomes more pronounced as the dimension increases.
be rewritten as Dε (Σj ) = (cid:80)n
Dimension Reward. The effective dimension term Dε (Σj ) in the large-n MICL criterion (4) can
i=1 λi /( ε2
n + λi ), where λi is the ith eigenvalue of Σj . If the data lie
near a d-dimensional subspace (λ1 . . . λd (cid:29) ε2/n and λd+1 . . . λn (cid:28) ε2/n), Dε ≈ d. In general,

5

−2−10123152739516375log εNumber of training samplesRMAP − RMICL  −0.04−0.0200.020.040.06−2−10131527395163log εNumber of training samplesRMAP − RMICL  00.020.040.061022341020304050Ambient dimensionNumber of training samplesRMAP − RMICL  00.20.40.60.81022341020304050Ambient dimensionNumber of training samplesRRDA − RMICL  −0.0500.050.10.150.2Dε can be viewed as “softened” estimate of the dimension3 , relative to the distortion ε2 . MICL
therefore rewards distributions that have relatively higher dimension.4 However, this effect is some-
what countered by the regularization induced by ε, which rewards lower dimensional distributions.
Figure 2(right) empirically compares MICL to the conventional MAP and the regularized MAP (or
RDA [6]). We draw m samples from three nested Gaussian distributions: one of full rank n, one of
rank n/2, and one of rank 1. All samples are corrupted by 4% Gaussian noise. We estimate the Bayes
risk for each (m, n) combination as in the previous example. The regularization parameter in RDA
and the distortion ε for MICL are chosen independently for each trial by cross validation. Plotted
are the (estimated) differences in risk, RM AP − RM ICL (Fig. 2 (c)) and RRDA − RM ICL (Fig. 2
(d)). The red lines again correspond to the zero level-set of the difference. Unsurprisingly, MICL
outperforms MAP for most (m, n), and that the effect is most pronounced when n is large and m is
small. When m is much smaller than n (e.g. the bottom row of Figure 2 right), MICL demonstrates
a signiﬁcant performance gain with respect to RDA. As the number of samples increases, there is a
region where RDA is slightly better. For most (m, n), MICL and RDA are close in performance.

2.4 Extensions to Non-Gaussian Data

In practice, the data distribution(s) of interest may not be Gaussian. If the rate-distortion function is
known, one could, in principle, carry out similar analysis as for the Gaussian case. Nevertheless, in
this subsection, we discuss two practical modiﬁcations to the MICL criterion that are applicable to
arbitrary distributions and preserve the desirable properties discussed in the previous subsections.
log2 det(cid:0)I +α X X T (cid:1) = log2 det(cid:0)I +α X T X (cid:1).
Kernel MICL Criterion. Since X X T and X T X have the same non-zero eigenvalues,
(5)
This identity shows that Lε (X ) can also be computed from the inner products between the xi . If the
data x (of each class) are not Gaussian but there exists a nonlinear map ψ : Rn → H such that the
transformed data ψ(x) are (approximately) Gaussian, we can replace the inner product xT
1 x2 with
a symmetric positive deﬁnite kernel function k(x1 , x2 ) .= ψ(x1 )T ψ(x2 ). Choosing a proper kernel
function will improve classiﬁcation performance for non-Gaussian distributions. In practice, popular
choices include the polynomial kernel k(x1 , x2 ) = (xT
1 x2 + 1)d , the radial basis function (RBF)
kernel k(x1 , x2 ) = exp(−γ (cid:107)x1 − x2(cid:107)2 ) and their variants. Implementation details, including how
to properly account for the mean and dimension of the embedded data, are given in [21].
A similar transformation is used to generate nonlinear decision boundaries with SVM. Notice, how-
ever, that whereas SVM constructs a linear decision boundary in the lifted space H, kernel MICL
exploits the covariance structure of the lifted data, generating decision boundaries that are (asymp-
totically) quadratic. In Section 3 we will see that even for real data whose statistical nature is unclear,
kernel MICL outperforms SVM when applied with the same kernel function.

Local MICL Criterion. For real data whose distribution is unknown, it may be difﬁcult to ﬁnd an
appropriate kernel function. In this case, MICL can still be applied locally, in a neighborhood of the
test sample x. Let N k (x) denote the k nearest neighbors of x in the training set X . Training data in
j (x) .= Xj ∩ N k (x), j = 1, . . . , K. In the MICL
this neighborhood that belong to each class are N k
classiﬁer (Algorithm 1), we replace the incremental coding length δLε (x, j ) by its local version:
j (x) ∪ {x}) − Lε (N k
δLε (x, j ) = Lε (N k
j (x)) + L(j ),
(6)
with L(j ) = − log2 (|N k
j (x)|/|N k (x)|). Theorem 1 implies that this gives a universal classiﬁer:
Corollary 3. Suppose the conditional density pj (x) = p(x|y = j ) of each class is nondegenerate.
Then if k = o(m) and k , m → ∞, the local MICL criterion converges to the MAP criterion (1).
This follows, since as the radius of the neighborhood shrinks, the cost of coding the class label,
− log2 (|N k
j (x)|/|N k (x)|) → − log2 pj (x), dominates the coding length, (6). In this asymptotic
setting the local MICL criterion behaves like k-Nearest Neighbor (k-NN). However, the ﬁnite-
sample behavior of the local MICL criterion can differ drastically from that of k-NN, especially

3This quantity has been dubbed the effective number of parameters in the context of ridge regression [9].
4This contrasts with the dimension penalties typical in model selection/estimation.

6

(a) KMICL-RBF

(b) SVM-RBF

(c) LMICL

(d) 5-NN

Figure 3: Nonlinear extensions to MICL, compared to SVM and k-NN. Local MICL produces a
smoother and more intuitive decision boundary than k-NN. Kernel MICL and SVM produce similar
boundaries, that are smoother and better respect the data structure than those given by local methods.

Method
Error Method
LMICL 1.6% SVM-Poly [20]
3.1% Best [18]
k-NN

Error
1.4%
0.4%

Method
Error Method
LMICL 4.9% KMICL-Poly
5.3% SVM-Poly [4]
k-NN

Error
4.7%
5.3%

Table 1: Results for handwritten digit recognition. Left: MNIST dataset. Right: USPS dataset, with
identical preprocessing and kernel function. Here, kernel-MICL slightly outperforms SVM.

when the samples are sparse and the distributions involved are almost degenerate. In this case, from
(4), local MICL effectively approximates the local shape of the distribution pj (x) by a (regularized)
Gaussian, exploiting structure in the distribution of the nearest neighbors (see ﬁgure 3).

3 Experiments with Real Imagery Data

Using experiments on real data, we demonstrate that MICL and its nonlinear variants approach the
best results from more sophisticated systems, without relying on domain-speciﬁc information.

Handwritten Digit Recognition. We ﬁrst test the MICL classiﬁer on two standard datasets for
handwritten digit recognition (Table 1 top). The MNIST handwritten digit dataset [10] consists of
60,000 training images and 10,000 test images. We achieved better results using the local version
of MICL, due to non-Gaussian distribution of the data. With k = 20 and ε = 150, local MICL
achieves a test error 1.59%, outperforming simple methods such as k-NN as well as many more
complicated neural network approaches (e.g. LeNet-1 [10]). MICL’s error rate approaches the
best result for a generic learning machine (1.4% error for SVM with a degree-4 polynomial kernel).
Problem-speciﬁc approaches have resulted in lower error rates, however, with the best reported result
achieved using a specially engineered neural network [18].
We also test on the challenging USPS digits database (Table 1 bottom). Here, even humans have
considerable difﬁculties (≈ 2.5% error). With k = 35 and ε = 0.03, local MICL achieves an error
rate of 4.88%, again outperforming k-NN. We further compare the performance of kernel MICL
to SVM (using [4]) on this dataset with the same homogeneous, degree 3 polynomial kernel, and
identical preprocessing (normalization and centering), allowing us to compare pure classiﬁcation
performace. Here, SVM achieves a 5.3% error, while kernel-MICL achieves an error rate of 4.7%
with distortion ε = 0.0067 (chosen automatically by cross-validation). Using domain-speciﬁc infor-
mation, one can achieve better results. For instance [17] achieves 2.7% error using tangent distance
to a large number of prototypes. Other preprocessing steps, synthetic training images, or more ad-
vanced skew-correction and normalization techniques have been applied to lower the error rate for
SVM (e.g. 4.1% in [20]). While we have avoided extensive preprocessing here, so as to isolate the
effect of the classiﬁer, such preprocessing can be readily incorporated into our framework.

Face Recognition. We further verify MICL’s effectiveness on sparsely sampled high-dimensional
data using the Yale Face Database B [7], which tests illumination sensitivity of face recognition
algorithms. Following [7, 11], we use subsets 1 and 2 for training, and report the average test error
across the four subsets. We apply Algorithm 1, not the local or kernel version, with ε = 75. MICL
signiﬁcantly outperforms classical subspace techniques on this problem (see Table 2), with error
0.9% near the best reported results in [7, 11] that were obtained using a domain-speciﬁc model of

7

Method
MICL
Subspace [7]

Error Method
0.9% Eigenface [7]
4.6% Best [11]

Error
25.8%
0% Subsets 1,2 (training)

Subsets 1-4 (testing)

Table 2: Face recognition under widely varying illumination. MICL outperforms classical face
recognition methods such as Eigenfaces on Yale Face Database B [7].

illumination for face images. We suggest that the source of this improved performance is precisely
the regularization induced by lossy coding. In this problem the number of training vectors per class,
19, is small compared to the dimension, n = 32, 256 (for raw 168 × 192 images). Simulations (e.g.
Figure 2) show that this is exactly the circumstance in which MICL is superior to MAP and even
RDA. Interestingly, this suggests that directly exploiting degenerate or low-dimensional structures
via MICL renders dimensionality reduction before classifying unnecessary or even undesirable.

4 Conclusion

We have proposed and studied a new information theoretic classiﬁcation criterion, Minimum In-
cremental Coding Length (MICL), establishing its optimality for Gaussian data. MICL generates a
family of classiﬁers that inherit many of the good properties of MAP, RDA, and k-NN, while ex-
tending their working conditions to sparsely sampled or degenerate high-dimensional observations.
MICL and its kernel and local versions approach best reported performance on high-dimensional vi-
sual recognition problems without domain-speciﬁc engineering. Due to its simplicity and ﬂexibility,
we believe MICL can be successfully applied to a wide range of real-world classiﬁcation problems.

References
[1] A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling.
IEEE Transactions on Information Theory, 44(6):2743–2760, 1998.
[2] R. Basri and D. Jacobs. Lambertian reﬂection and linear subspaces. PAMI, 25(2):218– 233, 2003.
[3] P. Bickel and B. Li. Regularization in statistics. TEST, 15(2):271–344, 2006.
[4] C. Chang and C. Lin. LIBSVM: a library for support vector machines, 2001.
[5] T. Cover and J. Thomas. Elements of Information Theory. Wiley Series in Telecommunications, 1991.
[6] J. Friedman. Regularized discriminant analysis. JASA, 84:165–175, 1989.
[7] A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for face
recognition under variable lighting and pose. PAMI, 23(6):643–660, 2001.
[8] P. Grunwald and J. Langford. Suboptimal behaviour of Bayes and MDL in classiﬁcation under misspeci-
ﬁcation. In Proceedings of Conference on Learning Theory, 2004.
[9] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001.
[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278–2324, 1998.
[11] K. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting.
PAMI, 27(5):684–698, 2005.
[12] J. Li. A source coding approach to classiﬁcation by vector quantization and the principle of minimum
description length. In IEEE DCC, pages 382–391, 2002.
[13] Y. Ma, H. Derksen, W. Hong, and J. Wright. Segmentation of multivariate mixed data via lossy data
coding and compression. PAMI, 29(9):1546–1562, 2007.
[14] D. MacKay. Developments in probabilistic modelling with neural networks – ensemble learning. In Proc.
3rd Annual Symposium on Neural Networks, pages 191–198, 1995.
[15] M. Madiman, M. Harrison, and I. Kontoyiannis. Minimum description length vs. maximum likelihood in
lossy data compression. In IEEE International Symposium on Information Theory, 2004.
[16] J. Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978.
[17] P. Simard, Y. LeCun, and J. Denker. Efﬁcient pattern recognition using a new transformation distance. In
Proceedings of NIPS, volume 5, 1993.
[18] P. Simard, D. Steinkraus, and J. Platt. Best practice for convolutional neural networks applied to visual
document analysis. In ICDAR, pages 958–962, 2003.
[19] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Allerton, 1999.
[20] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 2000.
[21] J. Wright, Y. Tao, Z. Lin, Y. Ma, and H. Shum. Classiﬁcation via minimum incremental coding length
(MICL). Technical report, UILU-ENG-07-2201, http://perception.csl.uiuc.edu/coding, 2007.

8


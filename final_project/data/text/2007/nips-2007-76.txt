Convex Relaxations of Latent Variable Training

Yuhong Guo and Dale Schuurmans
Department of Computing Science
University of Alberta
fyuhong, daleg@cs.ualberta.ca

Abstract

We investigate a new, convex relaxation of an expectation-maximization (EM)
variant that approximates a standard objective while eliminating local minima.
First, a cautionary result is presented, showing that any convex relaxation of EM
over hidden variables must give trivial results if any dependence on the missing
values is retained. Although this appears to be a strong negative outcome, we then
demonstrate how the problem can be bypassed by using equivalence relations in-
stead of value assignments over hidden variables. In particular, we develop new
algorithms for estimating exponential conditional models that only require equiv-
alence relation information over the variable values. This reformulation leads to
an exact expression for EM variants in a wide range of problems. We then develop
a semide(cid:2)nite relaxation that yields global training by eliminating local minima.

1 Introduction

Few algorithms are better known in machine learning and statistics than expectation-maximization
(EM) [5]. One reason is that EM solves a common problem(cid:151)learning from incomplete data(cid:151)that
occurs in almost every area of applied statistics. Equally well known to the algorithm itself, how-
ever, is the fact that EM suffers from shortcomings. Here it is important to distinguish between
the EM algorithm (essentially a coordinate descent procedure [10]) and the objective it optimizes
(marginal observed or conditional hidden likelihood). Only one problem is due to the algorithm
itself: since it is a simple coordinate descent, EM suffers from slow (linear) convergence and there-
fore can require a large number of iterations to reach a solution. Standard optimization algorithms
such as quasi-Newton methods can, in principle, require exponentially fewer iterations to achieve the
same accuracy (once close enough to a well behaved solution) [2, 11]. Nevertheless, EM converges
quickly in many circumstances [12, 13]. The main problems attributed to EM are not problems
with the algorithm per se, but instead are properties of the objective it optimizes. In particular, the
standard objectives tackled by EM are not convex in any standard probability model (e.g. the expo-
nential family). Non-convexity immediately creates the risk of local minima, which unfortunately
is not just a theoretical concern: EM often does not produce very good results in practice, and can
sometimes fail to improve signi(cid:2)cantly upon initial parameter settings [9]. For example, the (cid:2)eld of
unsupervised grammar induction [8] has been thwarted in its attempts to use EM for decades and is
still unable to infer useful syntactic models of natural language from raw unlabeled text.
We present a convex relaxation of EM for a standard training criterion and a general class of mod-
els in an attempt to understand whether local minima are really a necessary aspect of unsupervised
learning. Convex relaxations have been a popular topic in machine learning recently [4, 16]. In this
paper, we propose a convex relaxation of EM that can be applied to a general class of directed graph-
ical models, including mixture models and Bayesian networks, in the presence of hidden variables.
There are some technical barriers to overcome in achieving an effective convex relaxation however.
First, as we will show, any convex relaxation of EM must produce trivial results if it maintains any
dependence on the values of hidden variables. Although this result suggests that any convex relax-
ation of EM cannot succeed, we subsequently show that the problem can be overcome by working

with equivalence relations over the values of the hidden variables, rather than the missing values
themselves. Although equivalence relations provide an easy way to solve the symmetry collapsing
problem, they do not immediately yield a convex EM formulation, because the underlying estima-
tion principles for directed graphical models have not been formulated in these terms. Our main
technical contribution therefore is a reformulation of standard estimation principles for exponen-
tial conditional models in terms of equivalence relations on variable values, rather than the variable
values themselves. Given an adequate reformulation of the core estimation principle, developing a
useful convex relaxation of EM becomes possible.

1.1 EM Variants

P (x; y(k+1) jw)

Before proceeding, it is important to (cid:2)rst clarify the precise EM variant we address. In fact, there are
many EM variants that optimize different criteria. Let z = (x; y) denote a complete observation,
where x refers to the observed part of the data and y refers to the unobserved part; and let w
refer to the parameters of the underlying probability model, P (x; yjw). (Here we consider discrete
probability distributions just for simplicity of the discussion.) Joint and conditional EM algorithms
are naive (cid:147)self-supervised(cid:148) training procedures that alternate between inferring the values of the
missing variables and optimizing the parameters of the model
(joint EM update)
P (x; yjw(k) ) w(k+1) = arg max
y(k+1) = arg max
y
w
(conditional EM update)
P (y(k+1) jx; w)
P (yjx; w(k) ) w(k+1) = arg max
y(k+1) = arg max
y
w
These are clearly coordinate descent procedures that make monotonic progress in their objectives,
P (x; yjw) and P (yjx; w). Moreover, the criteria being optimized are in fact well motivated objec-
tives for unsupervised training: joint EM is frequently used in statistical natural language processing
(where it is referred to as (cid:147)Viterbi EM(cid:148) [3, 7]); the conditional form has been used in [16]. The pri-
mary problem with these iterations is not that they optimize approximate or unjusti(cid:2)ed criteria, but
rather that they rapidly get stuck in poor local maxima due to the extreme updates made on y. By far,
the more common form of EM(cid:151)contributing the very name expectation-maximization(cid:151)is given by
(marginal EM update)
(k+1)
= P (yjx; w(k) ) w(k+1) = arg max
q(k+1)
w Py q
log P (x; yjw)
y
y
where qy is a distribution over possible missing values. Although it is not immediately ob-
vious what this iteration optimizes, it has long been known that it monotonically improves the
marginal likelihood P (xjw) [5].
[10] later showed that the E-step could be generalized to
maxqy Py qy log (cid:0)P (x; yjw(k) )=qy (cid:1). Due to the softer qy update, the standard EM update does
not as converge as rapidly to a local maximum as the joint and conditional variants; however, as a
result, it tends to (cid:2)nd better local maxima. Marginal EM has subsequently become the dominant
form of EM algorithm in the literature (although joint EM is still frequently used in statistical NLP
[3, 7]). Nevertheless, none of the training criteria are jointly convex in the optimization variables,
thus these iterations are only guaranteed to (cid:2)nd local maxima.
Independent of the updates, the three training criteria are not equivalent nor equally well moti-
vated. In fact, for most applications we are more interested in acquiring an accurate conditional
P (yjx; w), rather than optimizing the marginal P (xjw) [16]. Of the three training criteria therefore
(joint, conditional and marginal), marginal likelihood appears to be the least relevant to learning
predictive models. Nevertheless, the convex relaxation techniques we propose can be applied to all
three objectives. For simplicity we will focus on maximizing joint likelihood in this paper, since
it incorporates aspects of both marginal and conditional training. Conveniently, joint and marginal
EM pose nearly identical optimization problems:
qy (cid:16) Py qy log P (x; yjw)(cid:17)
(joint EM objective) arg max
max
P (x; yjw) = arg max
max
w
y
w
qy (cid:16) Py qy log P (x; yjw)(cid:17)+H (qy )
(marg. EM objective) arg max
w Py P (x; yjw) = arg max
max
w
where qy is a distribution over possible missing values [10]. Therefore, much of the analysis we
provide for joint EM also applies to marginal EM, leaving only a separate convex relaxation of
the entropy term that can be conducted independently. We will also primarily consider the hidden
variable case and assume a (cid:2)xed set of random variables Y1 ; :::; Y‘ is always unobserved, and a (cid:2)xed
set of variables X‘+1 ; :::; Xn is always observed. The technique remains extendable to the general
missing value case however.

2 A Cautionary Result for Convexity

Our focus in this paper will be to develop a jointly convex relaxation to the minimization problem
posed by joint EM

(cid:0) Pi log P (xi ; yi jw)
min
min
w
y
One obvious issue we must face is to relax the discrete constraints on the assignments y. However,
the challenge is deeper than this. In the hidden variable case(cid:151)when the same variables are miss-
ing in each observation(cid:151)there is a complete symmetry between the missing values. In particular,
for any optimal solution (y; w) there must be other, equivalent solutions (y 0 ; w0 ) corresponding
to a permutation of the hidden variable values. Unfortunately, this form of solution symmetry has
devastating consequences for any convex relaxation: Assume one attempts to use any jointly con-
vex relaxation f (qy ; w) of the standard loglikelihood objective (1), where the the missing variable
assignment y has been relaxed into a continuous probabilistic assignment qy (like standard EM).

(1)

Lemma 1 If f is strictly convex and invariant to permutations of unobserved variable values, then
y ; w(cid:3) ), must satisfy q(cid:3)
the global minimum of f , (q(cid:3)
y = uniform.
Proof: Assume (qy ; w) is a global minimum of f but qy 6= uniform. Then there must be some
y ; w0 ) = ((cid:5)(qy ); (cid:5)(w)) satis(cid:2)es
permutation of the missing values, (cid:5), such that the alternative (q 0
y ; w0 ). By the strict
y 6= qy . But by the permutation invariance of f , this implies f (qy ; w) = f (q0
q0
convexity of f , we then have f (cid:0)(cid:11)(qy ; w) + (1 (cid:0) (cid:11))(q0
y ; w0 )(cid:1) < (cid:11)f (qy ; w) + (1 (cid:0) (cid:11))f (q0
y ; w0 ) =
f (qy ; w), for 0 < (cid:11) < 1, contradicting the global optimality of f (qy ; w).
Therefore, any convex relaxation of (1) that uses a distribution qy over missing values and does
not make arbitrary distinctions can never do anything but produce a uniform distribution over the
hidden variable values. (The same is true for marginal and conditional versions of EM.) Moreover,
any non-strictly convex relaxation must admit the uniform distribution as a possible solution. This
trivialization is perhaps the main reason why standard EM objectives have not been previously con-
vexi(cid:2)ed. (Note that standard coordinate descent algorithms simply break the symmetry arbitrarily
and descend into some local solution.) This negative result seems to imply that no useful convex
relaxation of EM is possible in the hidden variable case. However, our key observation is that a
convex relaxation expressed in terms of an equivalence relation over the missing values avoids this
symmetry breaking problem. In particular, equivalence relations exactly collapse the unresolvable
symmetries in this context, while still representing useful structure over the hidden assignments.
Representations based on equivalence relations are a useful tool for unsupervised learning that has
largely been overlooked (with some exceptions [4, 15]). Our goal in this paper, therefore, will be to
reformulate standard training objectives to use only equivalence relations on hidden variable values.

3 Directed Graphical Models

We will derive a convex relaxation framework for a general class of probability models(cid:151)namely,
directed models(cid:151)that includes mixture models and discrete Bayesian networks as special cases. A
directed model de(cid:2)nes a joint probability distribution over a set of random variables Z1 ; :::; Zn by
exploiting the chain rule of probability to decompose the joint into a product of locally normalized
j=1 P (zj jz(cid:25)(j ) ; wj ). Here, (cid:25)(j ) (cid:18) f1; :::; j (cid:0) 1g, and wj is
conditional distributions P (zjw) = Qn
the set of parameters de(cid:2)ning conditional distribution j . Furthermore, we will assume an exponential
family representation for the conditional distributions
j (cid:30)j (zj ; z(cid:25)(j ) ) (cid:0) A(wj ; z(cid:25)(j ) )(cid:1) ; where
P (zj jz(cid:25)(j ) ; wj ) = exp (cid:0)w>
j (cid:30)j (a; z(cid:25)(j ) )(cid:1) (cid:17)
A(wj ; z(cid:25)(j ) ) = log (cid:16) Pa exp (cid:0)w>
and (cid:30)j (zj ; z(cid:25)(j ) ) denotes a vector of features evaluated on the value of the child and its parents.
For simplicity, we will initially restrict our discussion to discrete Bayesian networks, but then rein-
troduce continuous random variables later. A discrete Bayesian network is just a directed model
where the conditional distributions are represented by a sparse feature vector indicating the identity
of the child-parent con(cid:2)guration (cid:30)j (zj ; z(cid:25)(j ) ) = (:::1(zj =a;z(cid:25)(j )=b) :::)> . That is, there is a single
indicator feature for each local con(cid:2)guration (a; b).

A particularly convenient property of directed models is that the complete data likelihood decom-
poses into an independent sum of local loglikelihoods
(2)
(cid:25)(j ) ) (cid:0) A(wj ; zi
j ; zi
j (cid:30)j (z i
Pi log P (zi jw) = Pj Pi w>
(cid:25)(j ) )
Thus the problem of solving for a maximum likelihood set of parameters, given complete training
data, amounts to solving a set of independent log-linear regression problems, one for each variable
Zj . To simplify notation, consider one of the log-linear regression problems in (2) and drop the
subscript j . Then, using a matrix notation we can rewrite the j th local optimization problem as
W (cid:16) Pi A(W; (cid:8)i: )(cid:17) (cid:0) tr((cid:8)W Y > )
min
where W 2 IRc(cid:2)v , (cid:8) 2 f0; 1gt(cid:2)c , and Y 2 f0; 1gt(cid:2)v , such that t is the number of training
examples, v is the number of possible values for the child variable, c is the number of possible
con(cid:2)gurations for the parent variables, and tr is the matrix trace. To explain this notation, note that
Y and (cid:8) are indicator matrices that have a single 1 in each row, where Y indicates the value of
the child variable, and (cid:8) indicates the speci(cid:2)c con(cid:2)guration of the parent values, respectively; i.e.
Y 1 = 1 and (cid:8)1 = 1, where 1 denotes the vector of all 1s. (This matrix notation greatly streamlines
the presentation below.) We also use the notation (cid:8)i: to denote the ith row vector in (cid:8). Here, the
log normalization factor is given by A(W; (cid:8)i: ) = log Pa exp ((cid:8)i:W 1a ), where 1a denotes a sparse
vector with a single 1 in position a.
Below, we will consider a regularized form of the objective, and thereby work with the maximum a
posteriori (MAP) form of the problem
(cid:11)
W (cid:16) Pi A(W; (cid:8)i: )(cid:17) (cid:0) tr((cid:8)W Y > ) +
min
2
This provides the core estimation principle at the heart of Bayesian network parameter learning.
However, for our purposes it suffers from a major drawback: (3) is not expressed in terms of equiv-
alence relations between the variable values. Rather it is expressed in terms of direct indicators of
speci(cid:2)c variable values in speci(cid:2)c examples(cid:151)which will lead to a trivial outcome if we attempt
any convex relaxation. Instead, we require a fundamental reformulation of (3) to remove the value
dependence and replace it with a dependence only on equivalence relationships.

tr(W >W )

(3)

4 Log-linear Regression on Equivalence Relations

The (cid:2)rst step in reformulating (3) in terms of equivalence relations is to derive its dual.

Lemma 2 An equivalent optimization problem to (3) is

(cid:0)tr((cid:2) log (cid:2)> ) (cid:0)

max
(cid:2)

1
2(cid:11)

(4)

tr (cid:0)(Y (cid:0) (cid:2))>(cid:8)(cid:8)> (Y (cid:0) (cid:2))(cid:1)
Proof: The proof follows a standard derivation, which we sketch; see e.g. [14]. First, by considering
the Fenchel conjugate of A it can be shown that
tr((cid:2)>
i: (cid:8)i:W ) (cid:0) (cid:2)i: log (cid:2)>
i:

subject to (cid:2)i: (cid:21) 0; (cid:2)i:1 = 1

subject to (cid:2) (cid:21) 0; (cid:2)1 = 1

A(W; (cid:8)i: ) = max
(cid:2)i:
Substituting this in (3) and then invoking the strong minimax property [1] allows one to show that
(3) is equivalent to
(cid:0)tr((cid:2) log (cid:2)> ) (cid:0) tr((Y (cid:0) (cid:2))>(cid:8)W ) +

(cid:11)
max
min
2
(cid:2)
W
(cid:11) (cid:8)> (Y (cid:0) (cid:2)), yielding (4).
Finally, the inner minimization can be solved by setting W = 1
Interestingly, deriving the dual has already achieved part of the desired result: the parent con(cid:2)gura-
tions now only enter the problem through the kernel matrix K = (cid:8)(cid:8)> . For Bayesian networks this
kernel matrix is in fact an equivalence relation between parent con(cid:2)gurations: (cid:8) is a 0-1 indicator
matrix with a single 1 in each row, implying that Kij = 1 iff (cid:8)i: = (cid:8)j : , and Kij = 0 otherwise.
But more importantly, K can be re-expressed as a function of the individual equivalence relations on
each of the parent variables. Let Y p 2 f0; 1gt(cid:2)vp indicate the value of a parent variable Zp for each

subject to (cid:2) (cid:21) 0; (cid:2)1 = 1

tr(W >W )

training example. That is, Y p
i: is a 1 (cid:2) vp sparse row vector with a single 1 indicating the value of
variable Zp in example i. Then M p = Y pY p> de(cid:2)nes an equivalence relation over the assignments
to variable Zp , since M p
ij = 1 if Y p
j : and M p
ij = 0 otherwise. It is not hard to see that the
i: = Y p
equivalence relation over complete parent con(cid:2)gurations, K = (cid:8)(cid:8)> , is equal to the component-
wise (Hadamard) product of the individual equivalence relations for each parent variable. That is,
K = (cid:8)(cid:8)> = M 1 (cid:14) M 2 (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) M p , since Kij = 1 iff M 1
ij = 1 and M 2
ij = 1 and ... M p
ij = 1.
Unfortunately, the dual problem (4) is still expressed in terms of the indicator matrix Y over child
variable values, which is still not acceptable. We still need to reformulate (4) in terms of the equiv-
alence relation matrix M = Y Y > . Consider an alternative dual parameterization (cid:10) 2 IR t(cid:2)t such
that (cid:10) (cid:21) 0, (cid:10)1 = 1, and (cid:10)Y = (cid:2). (Note that (cid:2) 2 IRt(cid:2)v , for v < t, and therefore (cid:10) is larger than
(cid:2). Also note that as long as every child value occurs at least once in the training set, Y has full rank
v . If not, then the child variable effectively has fewer values, and we could simply reduce Y until
it becomes full rank again without affecting the objective (3).) Therefore, since Y is full rank, for
any (cid:2), some (cid:10) must exist that achieves (cid:10)Y = (cid:2). Then we can relate the primal parameters to this
larger set of dual parameters by the relation W = 1
(cid:11) (cid:8)> (I (cid:0) (cid:10))Y . (Even though (cid:10) is larger than (cid:2),
they can only express the same realizable set of parameters W .) To simplify notation, let B = I (cid:0) (cid:10)
(cid:11) (cid:8)>BY . If we reparameterize the original problem using this relation,
and note the relation W = 1
then it is possible to show that an equivalent optimization problem to (3) is given by
1
B (cid:16) Pi A(B ; (cid:8)i:(cid:17) (cid:0) tr(KBM ) +
(5)
subject to B (cid:20) I ; B1 = 0
tr(B>KBM )
min
2(cid:11)
where K = (cid:8)(cid:8)> and M = Y Y > are equivalence relations on the parent con(cid:2)gurations and
child values respectively. The formulation (5) is now almost completely expressed in terms of
equivalence relations over the data, except for one subtle problem:
the log normalization factor
(cid:11) (cid:8)i:(cid:8)>BY 1a (cid:1) still directly depends on the label indicator matrix Y .
A(B ; (cid:8)i: ) = log Pa exp (cid:0) 1
Our key technical lemma is that this log normalization factor can be re-expressed to depend on the
equivalence relation matrix M alone.
Lemma 3 A(B ; (cid:8)i: ) = log Pj exp (cid:0) 1
(cid:11) Ki:BM:j (cid:0) log 1>M:j (cid:1)
Proof: The main observation is that an equivalence relation over value indicators, M = Y Y > ,
consists of columns copied from Y . That is, for all j , M:j = Y:a for some a corresponding to the
child value in example j . Let y(j ) denote the child value in example j and let (cid:12) i: = 1
(cid:11) Ki:B . Then
1
Pa exp (cid:0) 1
(cid:11) (cid:8)i:(cid:8)>BY 1a (cid:1) = Pa exp((cid:12) i:Y:a ) = Pa Pj :y(j )=a
jf‘:y(‘)=agj exp((cid:12) i:M:j )
exp((cid:12) i:M:j ) = Pj exp((cid:12) i:M:j (cid:0) log 1>M:j )
1
1
= Pj
jf‘:y(‘)=y(j )gj exp((cid:12) i:M:j ) = Pj
1>M:j
Using Lemma 3 one can show that the dual problem to (5) is given by the following.

(cid:0)tr((cid:3) log (cid:3)> ) (cid:0) 1>(cid:3) log(M 1) (cid:0)

Theorem 1 An equivalent optimization problem to (3) is
1
max
2(cid:11)
(cid:3)(cid:21)0;(cid:3)1=1
where K = M 1 (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) M p for parent variables Z1 ; :::; Zp .
Proof: This follows the same derivation as Lemma 2, modi(cid:2)ed by taking into account the extra
term introduced by Lemma 3. First, considering the Fenchel conjugate of A, it can be shown that
1
i: (cid:0) (cid:3)i: log (cid:3)>
Ki:BM (cid:3)>
max
A(B ; (cid:8)i: ) =
i: (cid:0) (cid:3)i: log(M 1)
(cid:11)
(cid:3)i:(cid:21)0;(cid:3)i: 1=1
Substituting this in (5) and then invoking the strong minimax property [1] allows one to show that
(5) is equivalent to

tr((I (cid:0) (cid:3))>K (I (cid:0) (cid:3))M )

(6)

(cid:0)tr((cid:3) log (cid:3)> ) (cid:0) 1>(cid:3) log(M 1) (cid:0)

1
1
min
max
2(cid:11)
(cid:11)
B(cid:20)I ;B1=0
(cid:3)(cid:21)0;(cid:3)1=1
Finally, the inner minimization on B can be solved by setting B = I (cid:0) (cid:3), yielding (6).
This gives our key result: the log-linear regression (3) is equivalent to (6), which is now expressed
strictly in terms of equivalence relations over the parent con(cid:2)gurations and child values. That is, the
value indicators, (cid:8) and Y , have been successfully eliminated from the formulation. Given a solution
(cid:3)(cid:3) to (6), the optimal model parameters W (cid:3) for (3) can be recovered via W (cid:3) = !
(cid:11) (cid:8)> (I (cid:0) (cid:3)(cid:3) )Y .

tr((I (cid:0) (cid:3))>KBM ) +

tr(B>KBM )

5 Convex Relaxation of Joint EM

min
wj

j jzi
(cid:0) log P (z i
(cid:25)(j ) ; wj ) +

w>
j wj

(cid:11)
2

(cid:0)tr((cid:3)j log (cid:3)>
j ) (cid:0) 1>(cid:3)j log(M j 1) (cid:0)
max
(cid:3)j (cid:21)0;(cid:3)j 1=1

The equivalence relation form of log-linear regression can be used to derive useful relaxations of
EM variants for directed models. In particular, by exploiting Theorem 1, we can now re-express
the regularized form of the joint EM objective (1) strictly in terms of equivalence relations over the
hidden variable values
fY h g X
min
j
tr (cid:0)(I (cid:0) (cid:3)j )>K j (I (cid:0) (cid:3)j )M j (cid:1) (8)
fM h g X
= min
j
subject to M h = Y hY h>
(9)
; Y h 2 f0; 1gt(cid:2)vh ; Y h1 = 1
where h ranges over the hidden variables, and K j = M j1 (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) M jp for the parent variables
Zj1 ; :::; Zjp of Zj .
Note that (8) is an exact reformulation of the joint EM objective (7); no relaxation has yet been
introduced. Another nice property of the objective in (8) is that is it concave in each (cid:3) j and convex
in each M h individually (a maximum of convex functions is convex [2]). Therefore, (8) appears
as though it might admit an ef(cid:2)cient algorithmic solution. However, one dif(cid:2)culty in solving the
resulting optimization problem is the constraints. Although the constraints imposed in (9) are not
convex, there is a natural convex relaxation suggested by the following.

1
2(cid:11)

(7)

Lemma 4 (9) is equivalent to: M 2 f0; 1gt(cid:2)t ; diag(M ) = 1; M = M > ; M (cid:23) 0; rank(M ) = v .

A natural convex relaxation of (9) can therefore be obtained by relaxing the discreteness constraint
and dropping the nonconvex rank constraint, yielding
M h 2 [0; 1]t(cid:2)t ; diag(M h ) = 1; M h = M h>
(10)
; M h (cid:23) 0
Optimizing the exact objective in (8) subject to the relaxed convex constraints (10) provides the
foundation for our approach to convexifying EM. Note that since (8) and (10) are expressed solely
in terms of equivalence relations, and do not depend on the speci(cid:2)c values of hidden variables in
any way, this formulation is not subject to the triviality result of Lemma 1.
However, there are still some details left to consider. First, if there is only a single hidden variable
then (8) is convex with respect to the single matrix variable M h . This result immediately provides
a convex EM training algorithm for various applications, such as for mixture models for example
(see the note regarding continuous random variables below). Second, if there are multiple hidden
variables that are separated from each other (none are neighbors, nor share a common child) then the
formulation (8) remains convex and can be directly applied. On the other hand, if hidden variables
are connected in any way, either by sharing a parent-child relationship or having a common child,
then (8) is no longer jointly convex because the trace term is no longer linear in the matrix variables
fM h g. In this case, we can restore convexity by further relaxing the problem: To illustrate, if there
are multiple hidden parents Zp1 ; :::; Zp‘ for a given child, then the combined equivalence relation
M p1 (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) M p‘ is a Hadamard product of the individual matrices. A convex formulation can be
recovered by introducing an auxiliary matrix variable ~M to replace M p1 (cid:14) (cid:1) (cid:1) (cid:1) (cid:14)M p‘ in (8) and adding
the set of linear constraints ~Mij (cid:20) M p
ij for p 2 fp1 ; :::; p‘ g, ~Mij (cid:21) M p1
ij + (cid:1) (cid:1) (cid:1) + M p‘
ij (cid:0) ‘ + 1
to approximate the componentwise ’and’. A similar relaxation can also be applied when a child is
hidden concurrently with hidden parent variables.
Continuous Variables The formulation in (8) can be applied to directed models with continuous
random variables, provided that all hidden variables remain discrete. If every continuous random
variable is observed, then the subproblems on these variables can be kept in their natural formula-
tions, and hence still solved. This extension is suf(cid:2)cient to allow the formulation to handle Gaussian
mixture models, for example. Unfortunately, the techniques developed in this paper do not apply to
the situation where there are continuous hidden variables.
Recovering the Model Parameters Once the relaxed equivalence relation matrices fM h g have
been obtained, the parameters of they underlying probability model need to be recovered. At an

Convex EM
Viterbi EM
Fully Supervised
Bayesian
Train
Test
Test
Train
Test
Train
networks
9.16 (cid:6):21
8.96 (cid:6):24
11.73 (cid:6):38
11.29 (cid:6):44
7.90 (cid:6):04
7.23 (cid:6):06
Synth1
5.55 (cid:6):19
5.27 (cid:6):18
6.41 (cid:6):23
6.02 (cid:6):20
4.50 (cid:6):03
4.24 (cid:6):04
Synth2
6.23 (cid:6):18
6.41 (cid:6):14
7.81 (cid:6):35
8.18 (cid:6):33
4.93 (cid:6):02
5.32 (cid:6):05
Synth3
6.50 (cid:6):28
6.51 (cid:6):35
7.07 (cid:6):23
6.70 (cid:6):27
5.53 (cid:6):04
5.23 (cid:6):04
Diabetes
6.03 (cid:6):09
5.81 (cid:6):07
6.93 (cid:6):21
6.74 (cid:6):34
5.32 (cid:6):03
5.07 (cid:6):03
Pima
3.06 (cid:6):16
2.98 (cid:6):19
3.94 (cid:6):29
3.90 (cid:6):31
2.31 (cid:6):02
2.18 (cid:6):05
Cancer
11.74 (cid:6):25
13.62 (cid:6):20
11.94 (cid:6):32
13.75 (cid:6):17
10.23 (cid:6):16
12.30 (cid:6):06
Alarm
Asian
2.78 (cid:6):12
2.70 (cid:6):14
2.36 (cid:6):03
2.21 (cid:6):05
2.33 (cid:6):02
2.17 (cid:6):05
Table 1: Results on synthetic and real-world Bayesian networks: average loss (cid:6) standard deviation

optimal solution to (8), one not only obtains fM h g, but also the associated set of dual parameters
f(cid:3)j g. Therefore, we can recover the primal parameters Wj from the dual parameters (cid:3)j by using
the relationship Wj = 1
j (I (cid:0)(cid:3)j )Y j established above, which only requires availability of a label
(cid:11) (cid:8)>
assignment matrix Y j . For observed variables, Y j is known, and therefore the model parameters
can be immediately recovered. For hidden variables, we (cid:2)rst need to compute a rank vh factorization
of M h . Let V = U (cid:6)1=2 where U and (cid:6) are the top vh eigenvector and eigenvalue matrices of the
centered matrix HM hH , such that H = I (cid:0) 1
t 11> . One simple idea to recover ^Yh from V is to run
k-means on the rows of V and construct the indicator matrix. A more elegant approach would be to
use a randomized rounding scheme [6], which also produces a deterministic ^Yh , but provides some
guarantees about how well ^Yh ^Y >
h approximates M h . Note however that V is an approximation
of Y h where the row vectors have been re-centered on the origin in a rotated coordinate system.
Therefore, a simpler approach is just to map the rows of V back onto the simplex by translating the
mean back to the simplex center and rotation the coordinates back into the positive orthant.

6 Experimental Results

An important question to ask is whether the relaxed, convex objective (8) is in fact over-relaxed, and
whether important structure in the original marginal likelihood objective has been lost as a result. To
investigate this question, we conducted a set of experiments to evaluate our convex approach com-
pared to the standard Viterbi (i.e. joint) EM algorithm, and to supervised training on fully observed
data. Our experiments are conducted using both synthetic Bayesian networks and real networks,
while measuring the trained models by their logloss produced on the fully observed training data
and testing data. All the results reported in this paper are averages over 10 times repeats. The test
size for the experiments is 1000, the training size is 100 without speci(cid:2)cation. For a fair comparison,
we used 10 random restarts for Viterbi EM to help avoid poor local optima.
For the synthetic experiments, we constructed three Bayesian networks: (1) Bayesian network 1
(Synth1) is a three layer network with 9 variables, where the two nodes in the middle layer are
picked as hidden variables; (2) Bayesian network 2 (Synth2) is a network with 6 variables and
6 edges, where a node with 2 parents and 2 children is picked as hidden variable; (3) Bayesian
network 3 (Synth3) is a Naive Bayes model with 7 variables, where the parent node is selected as
the hidden variable. The parameters are generated in a discriminative way to produce models with
apparent causal relations between the connected nodes. We performed experiments on these three
synthetic networks using varying training sizes: 50, 100 and 150. Due to space limits, we only
report the results for training size 100 in Table 1. Besides these three synthetic Bayesian networks,
we also ran experiments using real UCI data, where we used Naive Bayes as the model structure,
and set the class variables to be hidden. The middle two rows of the Table 1 show the results on two
UCI data sets.
Here we can see that the convex relaxation was successful at preserving structure in the EM ob-
jective, and in fact, generally performed much better than the Viterbi EM algorithm, particularly
in the case (Synth1) where there was two hidden variables. Not surprisingly, supervised training
on the complete data performed better than the EM methods, but generally demonstrated a larger
gap between training and test losses than the EM methods. Similar results were obtained for both

larger and smaller training sample sizes. For the UCI experiments, the results are very similar to the
synthetic networks, showing good results again for the convex EM relaxation.
Finally, we conducted additional experiments on three real world Bayesian networks: Alarm, Cancer
and Asian (downloaded from http://www.norsys.com/networklibrary.html). We picked one well
connected node from each model to serve as the hidden variable, and generated data by sampling
from the models. Table 1 shows the experimental results for these three Bayesian networks. Here
we can see that the convex EM relaxation performed well on the Cancer and Alarm networks. Since
we only picked one hidden variable from the 37 variables in Alarm, it is understandable that any
potential advantage for the convex approach might not be large. Nevertheless, a slight advantage is
still detected here. Much weaker results are obtained on the Asian network however. We are still
investigating what aspects of the problem are responsible for the poorer approximation in this case.

7 Conclusion

We have presented a new convex relaxation of EM that obtains generally effective results in simple
experimental comparisons to a standard joint EM algorithm (Viterbi EM), on both synthetic and
real problems. This new approach was facilitated by a novel reformulation of log-linear regression
that refers only to equivalence relation information on the data, and thereby allows us to avoid
the symmetry breaking problem that blocks naive convexi(cid:2)cation strategies from working. One
shortcoming of the proposed technique however is that it cannot handle continuous hidden variables;
this remains a direction for future research. In one experiment, weaker approximation quality was
obtained, and this too is the subject of further investigation.

References
[1] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization. Springer, 2000.
[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge U. Press, 2004.
[3] S. Chen. Models for grapheme-to-phoneme conversion. In Eurospeech, 2003.
[4] T. De Bie and N. Cristianini. Fast SDP relaxations of graph cut clustering, transduction, and
other combinatorial problems. Journal of Machine Learning Research, 7, 2006.
[5] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Series B, 39(1):1(cid:150)38, 1977.
[6] M. Goemans and D. Williamson. Improved approximation algorithms for maximum cut and
satis(cid:2)ability problems using semide(cid:2)nite programming. JACM, 42(6):1115(cid:150)1145, 1995.
[7] S. Goldwater and M. Johnson. Bias in learning syllable structure. In Proc. CONLL, 2005.
[8] D. Klein and C. Manning. Corpus-based induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings ACL, 2004.
[9] B. Merialdo. Tagging text with a probabilistic model. Comput. Ling., 20(2):155(cid:150)171, 1994.
[10] R. Neal and G. Hinton. A view of the em algorithm that justi(cid:2)es incremental, sparse, and other
variants. In M. Jordan, editor, Learning in Graphical Models. Kluwer, 1998.
[11] J. Nocedal and S. Wright. Numerical Optimization. Springer, 1999.
[12] R. Salakhutdinov, S. Roweis, and Z. Ghahramani. Optimization with EM and expectation-
conjugate-gradient. In Proceedings ICML, 2003.
[13] N. Srebro, G. Shakhnarovich, and S. Roweis. An investigation of computational and informa-
tional limits in gaussian mixture clustering. In Proceedings ICML, 2006.
[14] M. Wainwright and M. Jordan. Graphical models, exponential families, and variational infer-
ence. Technical Report TR-649, UC Berkeley, Dept. Statistics, 2003.
[15] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Max margin clustering. In NIPS 17, 2004.
[16] L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans. Discriminative unsupervised learning of
structured predictors. In Proceedings ICML, 2006.


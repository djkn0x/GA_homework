Catching Up Faster in Bayesian
Model Selection and Model Averaging

Peter Gr ¨unwald
Tim van Erven
Steven de Rooij
Centrum voor Wiskunde en Informatica (CWI)
Kruislaan 413, P.O. Box 94079
1090 GB Amsterdam, The Netherlands
{Tim.van.Erven,Peter.Grunwald,Steven.de.Rooij}@cwi.nl

Abstract

Bayesian model averaging, model selection and their approximations such as BIC
are generally statistically consistent, but sometimes achieve slower rates of con-
vergence than other methods such as AIC and leave-one-out cross-validation. On
the other hand, these other methods can be inconsistent. We identify the catch-up
phenomenon as a novel explanation for the slow convergence of Bayesian meth-
ods. Based on this analysis we deﬁne the switch-distributio n, a modi ﬁcation of the
Bayesian model averaging distribution. We prove that in many situations model
selection and prediction based on the switch-distribution is both consistent and
achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The
method is practical; we give an efﬁcient algorithm.

1

Introduction

We consider inference based on a countable set of models (sets of probability distributions), focusing
on two tasks: model selection and model averaging. In model selection tasks, the goal is to select
the model that best explains the given data. In model averaging, the goal is to ﬁnd the weighted
combination of models that leads to the best prediction of future data from the same source.

An attractive property of some criteria for model selection is that they are consistent under weak
conditions, i.e. if the true distribution P ∗ is in one of the models, then the P ∗ -probability that this
model is selected goes to one as the sample size increases. BIC [14], Bayes factor model selection
[8], Minimum Description Length (MDL) model selection [3] and prequential model validation [5]
are examples of widely used model selection criteria that are usually consistent. However, other
model selection criteria such as AIC [1] and leave-one-out cross-validation (LOO) [16], while of-
ten inconsistent, do typically yield better predictions. This is especially the case in nonparametric
settings, where P ∗ can be arbitrarily well-approximated by a sequence of distr ibutions in the (para-
metric) models under consideration, but is not itself conta ined in any of these. In many such cases,
the predictive distribution converges to the true distribu tion at the optimal rate for AIC and LOO
[15, 9], whereas in general BIC, the Bayes factor method and prequential validation only achieve
the optimal rate to within an O(log n) factor [13, 20, 6]. In this paper we reconcile these seemingly
conﬂicting approaches [19] by improving the rate of converg ence achieved in Bayesian model se-
lection without losing its convergence properties. First we provide an example to show why Bayes
sometimes converges too slowly.
Given priors on models M1 , M2 , . . . and parameters therein, Bayesian inference associates each
model Mk with the marginal distribution pk , given in (1), obtained by averaging over the parameters
according to the prior. In model selection the preferred mod el is the one with maximum a posteriori
probability. By Bayes’ rule this is arg maxk pk (xn )w(k), where w(k) denotes the prior probability
of Mk . We can further average over model indices, a process called Bayesian Model Averaging
(BMA). The resulting distribution pbma (xn ) = Pk pk (xn )w(k) can be used for prediction. In a se-
1

quential setting, the probability of a data sequence xn := x1 , . . . , xn under a distribution p typically
decreases exponentially fast in n. It is therefore common to consider − log p(xn ), which we call the
codelength of xn achieved by p. We take all logarithms to base 2, allowing us to measure codelength
in bits. The name codelength refers to the correspondence between codelength functions and prob-
ability distributions based on the Kraft inequality, but one may also think of the codelength as the
accumulated log loss that is incurred if we sequentially predict the xi by conditioning on the past,
i.e. using p(·|xi−1 ) [3, 6, 5, 11]. For BMA, we have − log pbma (xn ) = Pn
i=1 − log pbma (xi |xi−1 ).
Here the ith term represents the loss incurred when predicting xi given xi−1 using pbma (·|xi−1 ),
which turns out to be equal to the posterior average: pbma (xi |xi−1 ) = Pk pk (xi |xi−1 )w(k |xi−1 ).
Prediction using pbma has the advantage that the codelength it achieves on xn is close to the code-
length of pˆk , where ˆk is the index of best of the marginals p1 , p2 , . . . Namely, given a prior w on
model indices, the difference between − log pbma (xn ) = − log(Pk pk (xn )w(k)) and − log pˆk (xn )
must be in the range [0, − log w(ˆk)], whatever data xn are observed. Thus, using BMA for pre-
diction is sensible if we are satis ﬁed with doing essentiall y as well as the best model under con-
sideration. However, it is often possible to combine p1 , p2 , . . . into a distribution that achieves
smaller codelength than pˆk ! This is possible if the index ˆk of the best distribution changes with
the sample size in a predictable way. This is common in model selection, for example with nested
models, say M1 ⊂ M2 . In this case p1 typically predicts better at small sample sizes (roughly,
because M2 has more parameters that need to be learned than M1 ), while p2 predicts better
eventually. Figure 1 illustrates this phenomenon. It shows the accumulated codelength difference
− log p2 (xn ) − (− log p1 (xn )) on “The Picture of Dorian Gray” by Oscar Wilde, where
p1 and p2
are the Bayesian marginal distributions for the ﬁrst-order and second-order Markov chains, respec-
tively, and each character in the book is an outcome. Note that the example models M1 and M2 are
very crude; for this particular application much better mod els are available. In more complicated,
more realistic model selection scenarios, the models may st ill be wrong, but it may not be known
how to improve them. Thus M1 and M2 serve as a simple illustration only. We used uniform priors
on the model parameters, but for other common priors similar behaviour can be expected. Clearly
p1 is better for about the ﬁrst 100 000 outcomes, gaining a head start of approximately 40 000 bits.
Ideally we should predict the initial 100 000 outcomes using p1 and the rest using p2 . However, pbma
only starts to behave like p2 when it catches up with p1 at a sample size of about 310 000, when the
codelength of p2 drops below that of p1 . Thus, in the shaded area pbma behaves like p1 while p2 is
making better predictions of those outcomes: since at n = 100 000, p2 is 40 000 bits behind, and at
n = 310 000, it has caught up, in between it must have outperformed p1 by 40 000 bits!
The general pattern that ﬁrst one model is
better and then another occurs widely, both
on real-world data and in theoretical set-
tings. We argue that failure to take this
effect into account leads to the suboptimal
rate of convergence achieved by Bayes fac-
tor model selection and related methods.
We have developed an alternative method
to combine distributions p1 and p2 into a
single distribution psw , which we call the
switch-distribution, deﬁned in Section 2.
Figure 1 shows that psw behaves like p1
initially, but in contrast to pbma it starts
to mimic p2 almost immediately after p2
starts making better predictions; it essen-
tially does this no matter what sequence xn is actually observed. psw differs from pbma in that it
is based on a prior distribution on sequences of models rather than simply a prior distribution on
models. This allows us to avoid the implicit assumption that there is one model which is best at
all sample sizes. After conditioning on past observations, the posterior we obtain gives a better
indication of which model performs best at the current sample size, thereby achieving a faster rate
of convergence. Indeed, the switch-distribution is related to earlier algorithms for tracking the best
expert developed in the universal prediction literature [7, 18, 17 , 10]; however, the applications we
have in mind and the theorems we prove are completely different. In Sections 3 and 4 we show
that model selection based on the switch-distribution is consistent (Theorem 1), but unlike standard

 50000  100000  150000  200000  250000  300000  350000  400000  450000
Sample size
Figure 1: The Catch-up Phenomenon

Markov order 2
Bayesian Model Averaging
Switch−Distribution

)
s
t
i
b
(
 
1
 
r
e
d
r
o
 
v
o
k
r
a
M
 
h
t
i
w
 
e
c
n
e
r
e
f
f
i
d
 
h
t
g
n
e
l
e
d
o
C

−100000

 0

 60000

 40000

 20000

−60000

−80000

 0

−20000

−40000

2

Bayes factor model selection achieves optimal rates of convergence (Theorem 2). Proofs of the
theorems are in Appendix A. In Section 5 we give a practical algorithm that computes the switch-
distribution for K (rather than 2) predictors in Θ(n · K ) time. In the full paper, we will give further
details of the proof of Theorem 1 and a more detailed discussion of Theorem 2 and the implications
of both theorems.

2 The Switch-Distribution for Model Selection and Prediction

Preliminaries Suppose X∞ = (X1 , X2 , . . .) is a sequence of random variables that take values
in sample space X ⊆ Rd for some d ∈ Z+ = {1, 2, . . .}. For n ∈ N = {0, 1, 2, . . .}, let xn = (x1 ,
. . ., xn ) denote the ﬁrst n outcomes of X∞ , such that xn takes values in the product space X n =
X1 × · · · × Xn . (We let x0 denote the empty sequence.) Let X ∗ = S∞
n=0 X n . For m > n, we write
n+1 for (Xn+1 , . . ., Xm ), where m = ∞ is allowed and we omit the subscript when n = 0.
X m
Any distribution P (X∞ ) may be deﬁned by a sequential prediction strategy p that predicts the
next outcome at any time n ∈ N. To be precise: Given the previous outcomes xn at time n, this
prediction strategy should issue a conditional density p(Xn+1 |xn ) with corresponding distribution
P (Xn+1 |xn ) for the next outcome Xn+1 . Such sequential prediction strategies are sometimes called
prequential forecasting systems [5]. An instance is given in Example 1 below. We assume that the
density p(Xn+1 |xn ) is taken relative to either the usual Lebesgue measure (if X is continuous)
or the counting measure (if X is countable). In the latter case p(Xn+1 |xn ) is a probability mass
function. It is natural to deﬁne the joint density p(xm |xn ) = p(xn+1 |xn ) · · · p(xm |xm−1 ) and let
n+1 |xn ) be the unique distribution such that, for all m > n, p(X m
n+1 |xn ) is the density of its
P (X∞
n+1 |xn ) is well-deﬁned even if X is continuous,
n+1 . To ensure that P (X∞
marginal distribution for X m
we impose the natural requirement that for any k ∈ Z+ and any ﬁxed event Ak+1 ⊆ Xk+1 the
probability P (Ak+1 |xk ) is a measurable function of xk , which holds automatically if X is countable.

Model Selection and Prediction The goal in model selection is to choose an explanation for
observed data xn from a potentially inﬁnite list of candidate models M1 , M2 , . . . We consider
parametric models, which are sets {pθ : θ ∈ Θ} of prediction strategies pθ that are indexed by ele-
ments of Θ ⊆ Rd , for some smallest possible d ∈ N, the number of degrees of freedom. Examples
of model selection are regression based on a set of basis func tions such as polynomials (d is the
number of coefﬁcients of the polynomial), the variable sele ction problem in regression [15, 9, 20]
(d is the number of variables), and histogram density estimation [13] (d is the number of bins). A
model selection criterion is a function δ : X ∗ → Z+ that, given any data sequence xn ∈ X ∗ , selects
the model Mk with index k = δ(xn ).
We associate each model Mk with a single prediction strategy ¯pk . The bar emphasizes that ¯pk is a
meta-strategy based on the prediction strategies in Mk . In many approaches to model selection, for
example AIC and LOO, ¯pk is deﬁned using some estimator ˆθk for each model Mk , which maps a
sequence xn of previous observations to an estimated parameter value that represents a “best guess ”
of the true/best distribution in the model. Prediction is then based on this estimator: ¯pk (Xn+1 |
xn ) = p ˆθk (xn ) (Xn+1 | xn ), which also deﬁnes a joint density ¯pk (xn ) = ¯pk (x1 ) · · · ¯pk (xn |xn−1 ).
The Bayesian approach to model selection or model averaging goes the other way around. We start
out with a prior w on Θk , and deﬁne the Bayesian marginal density
¯pk (xn ) = Zθ∈Θk
When ¯pk (xn ) is non-zero this joint density induces a unique conditional density ¯pk (Xn+1 | xn ) =
¯pk (Xn+1 , xn )/ ¯pk (xn ), which is equal to the mixture of pθ ∈ Mk according to the posterior,
w(θ |xn ) = pθ (xn )w(θ)/ R pθ (xn )w(θ) dθ , based on xn . Thus the Bayesian approach also de-
ﬁnes a prediction strategy
¯pk (Xn+1 |xn ), whose corresponding distribution may be thought of as
an estimator. From now on we sometimes call the distributions induced by ¯p1 , ¯p2 , . . . “estimators ”,
even if they are Bayesian. This uni ﬁed view is known as prequential or predictive MDL [11, 5].
Example 1. Suppose X = {0, 1}. Then a prediction strategy ¯p may be based on the Bernoulli
model M = {pθ | θ ∈ [0, 1]} that regards X∞ as a sequence of independent, identically distributed
Bernoulli random variables with Pθ (Xn+1 = 1) = θ . We may predict Xn+1 using the maximum
likelihood (ML) estimator based on the past, i.e. using ˆθ(xn ) = n−1 Pn
i=1 xi . The prediction for
x1 is then undeﬁned. If we use a smoothed ML estimator such as the Laplace estimator, ˆθ ′ (xn ) =

pθ (xn )w(θ) dθ .

(1)

3

(n + 2)−1 (Pn
i=1 xi + 1), then all predictions are well-deﬁned. Perhaps surprising ly, the predictor
¯p′ deﬁned by ¯p′ (Xn+1 | xn ) = p ˆθ ′ (xn ) (Xn+1 ) equals the Bayesian predictive distribution based on
a uniform prior. Thus in this case a Bayesian predictor and an estimation-based predictor coincide!

The Switch-Distribution Suppose p1 , p2 , . . . is a list of prediction strategies for X∞ . (Although
here the list is inﬁnitely long, the developments below can w ith little modi ﬁcation be adjusted to the
case where the list is ﬁnite.) We ﬁrst deﬁne a family
Q = {qs : s ∈ S} of combinator prediction
strategies that switch between the original prediction strategies. Here the parameter space S is
deﬁned as

qs (Xn+1 |xn ) =

S = {(t1 , k1 ), . . . , (tm , km ) ∈ (N × Z+ )m | m ∈ Z+ , 0 = t1 < . . . < tm}.
(2)
The parameter s ∈ S speci ﬁes the identities of m constituent prediction strategies and the sample
sizes, called switch-points, at which to switch between them. For s = ((t′
m′ )), we
1 , k ′
1 ), . . . , (t′
m′ , k ′
i and m(s) = m′ . We omit the argument when the parameter s is clear
i , ki (s) = k ′
deﬁne ti (s) = t′
from context, e.g. we write t3 for t3 (s). For each s ∈ S the corresponding qs ∈ Q is deﬁned as:

if n < t2 ,
pk1 (Xn+1 |xn )
if t2 ≤ n < t3 ,
pk2 (Xn+1 |xn )
...
...

if tm−1 ≤ n < tm ,
pkm−1 (Xn+1 |xn )
if tm ≤ n.
pkm (Xn+1 |xn )
Switching to the same predictor multiple times is allowed. The extra switch-point t1 is included
to simplify notation; we always take t1 = 0. Now the switch-distribution is deﬁned as a Bayesian
mixture of the elements of Q according to a prior π on S:
Deﬁnition 1 (Switch-Distribution). Let π be a probability mass function on S. Then the switch-
distribution Psw with prior π is the distribution for X∞ such that, for any n ∈ Z+ , the density of its
marginal distribution for X n is given by
psw (xn ) = Xs∈S
Although the switch-distribution provides a general way to combine prediction strategies, in this
paper it will only be applied to combine prediction strategies ¯p1 , ¯p2 , . . . that correspond to models.
In this case we may deﬁne a corresponding model selection cri
terion δsw . To this end, let Kn+1 :
S → Z+ be a random variable that denotes the strategy/model that is used to predict Xn+1 given
past observations xn . Formally, Kn+1 (s) = ki (s) iff ti (s) ≤ n and i = m(s) ∨ n < ti+1 (s).
Algorithm 1, given in Section 5, efﬁciently computes the pos terior distribution on Kn+1 given xn :
π(Kn+1 = k | xn ) = P{s:Kn+1 (s)=k} π(cid:0)s(cid:1)qs (xn )
psw (xn )
which is deﬁned whenever psw (xn ) is non-zero. We turn this into a model selection criterion
δsw (xn ) = arg maxk π(Kn+1 = k |xn ) that selects the model with maximum posterior probability.

qs (xn ) · π(s).

,

(5)

(3)

(4)

3 Consistency

If one of the models, say with index k∗ , is actually true, then it is natural to ask whether δsw is
consistent, in the sense that it asymptotically selects k∗ with probability 1. Theorem 1 below states
that this is the case under certain conditions which are only slightly stronger than those required for
the consistency of standard Bayes factor model selection.
Bayes factor model selection is consistent if for all k , k ′ 6= k , ¯Pk (X∞ ) and ¯Pk′ (X∞ ) are mutually
singular, that is, if there exists a measurable set A ⊆ X ∞ such that ¯Pk (A) = 1 and ¯Pk′ (A) = 0 [3].
For example, this can usually be shown to hold if the models are nested and for each k , Θk is a subset
of Θk+1 of wk+1 -measure 0 [6]. For consistency of δsw , we need to strengthen this to the requirement
n+1 | xn ) and ¯Pk′ (X∞
6= k and all xn ∈ X ∗ , the distributions ¯Pk (X∞
n+1 | xn ) are
that, for all k ′
mutually singular. For example, if X1 , X2 , . . . are i.i.d. according to each Pθ in all models, but also
if X is countable and ¯pk (xn+1 | xn ) > 0 for all k , all xn+1 ∈ X n+1 , then this conditional mutual
singularity is automatically implied by ordinary mutual singularity of ¯Pk (X∞ ) and ¯Pk′ (X∞ ).

4

Let Es = {s′ ∈ S | m(s′ ) > m(s), (ti (s′ ), ki (s′ )) = (ti (s), ki (s)) for i = 1, . . . , m(s)} denote
the set of all possible extensions of s to more switch-points. Let ¯p1 , ¯p2 , . . . be Bayesian prediction
strategies with respective parameter spaces Θ1 , Θ2 , . . . and priors w1 , w2 , . . ., and let π be the prior
of the corresponding switch-distribution.
Theorem 1 (Consistency of the Switch-Distribution). Suppose π is positive everywhere on {s ∈
S | m(s) = 1} and is such that there exists a positive constant c such that, for every s ∈ S,
n+1 | xn ) and ¯Pk′ (X∞
c · π(s) ≥ π(Es ). Suppose further that ¯Pk (X∞
n+1 | xn ) are mutually singular
for all k , k ′ ∈ Z+ , k 6= k ′ , xn ∈ X ∗ . Then, for all k∗ ∈ Z+ , for all θ∗ ∈ Θk∗ except for a subset of
Θk∗ of wk∗ -measure 0, the posterior distribution on Kn+1 satis ﬁes
π(Kn+1 = k∗ | X n ) n→∞−→ 1
with Pθ∗ -probability 1.

(6)

The requirement that c · π(s) ≥ π(Es ) is automatically satis ﬁed if π is of the form:
m
Yi=2
where πM , πK and πT are priors on Z+ with full support, and πM is geometric: πM (m) = θm−1 (1 − θ)
for some 0 ≤ θ < 1. In this case c = θ/(1 − θ).

πT (ti |ti > ti−1 )πK (ki ),

π(s) = πM (m)πK (k1 )

(7)

4 Optimal Risk Convergence Rates

Suppose X1 , X2 , . . . are distributed according to P ∗ . We deﬁne the risk at sample size n ≥ 1 of the
estimator ¯P relative to P ∗ as
Rn (P ∗ , ¯P ) = EX n−1∼P ∗ [D(P ∗ (Xn = · | X n−1 )k ¯P (Xn = · | X n−1 ))],
where D(·k·) is the Kullback-Leibler (KL) divergence [4]. This is the standard deﬁnition of risk
relative to KL divergence. The risk is always well-deﬁned, a nd equal to 0 if ¯P (Xn+1 | X n ) is
equal to P ∗ (Xn+1 | X n ). The following identity connects information-theoretic redundancy and
accumulated statistical risk (see [4] or [6, Chapter 15]): I f P ∗ admits a density p∗ , then for all
prediction strategies ¯p,

(8)

Ri (P ∗ , ¯P ).

EX n∼P ∗ [− log ¯p(X n ) + log p∗ (X n )] =

n
Xi=1
For a union of parametric models M = Sk≥1 Mk , we deﬁne the
information closure hMi =
{P ∗ | inf P ∈M D(P ∗ kP ) = 0}, i.e. the set of distributions for X∞ that can be arbitrarily well
approximated by elements of M. Theorem 2 below shows that, for a very large class of P ∗ ∈ hMi,
¯P1 , ¯P2 , . . . achieves the same risk as any other
the switch-distribution deﬁned relative to estimators
model selection criterion deﬁned with respect to the same es timators, up to lower order terms; in
other words, model averaging based on the switch-distribut ion achieves at least the same rate of
convergence as model selection based on any model selection criterion whatsoever (the issue of
averaging vs selection will be discussed at length in the full paper). The theorem requires that the
prior π in (4) is of the form (7), and satis ﬁes
(9)
− log πM (m) = O(m) ; − log πK (k) = O(log k) ; − log πT (t) = O(log t).
Thus, πM , the prior on the total number of switch points, is allowed to decrease either polynomially
or exponentially (as required for Theorem 1); πT and πK must decrease polynomially. For example,
we could set πT (t) = πK (t) = 1/(t(t + 1)), or we could take the universal prior on the integers [12].
Let M∗ ⊂ hMi be some subset of interest of the information closure of mode l M. M∗ may consist
of just a single, arbitrary distribution P ∗ in hMi \M – in that case Theorem 2 shows that the switch-
distribution converges as fast as any other model selection criterion on any distribution in hMi that
cannot be expressed parametrically relative to M – or it may be a large, nonparametric family. In
that case, Theorem 2 shows that the switch-distribution achieves the minimax convergence rate. For
example, if the models Mk are k-bin histograms [13], then hMi contains every distribution on
[0, 1] with bounded continuous densities, and we may, for example, take M∗ to be the set of all
distributions on [0, 1] which have a differentiable density p∗ such that p∗ (x) and (d/dx)p∗ (x) are
bounded from below and above by some positive constants.
We restrict ourselves to model selection criteria which, at sample size n, never select a model Mk
with k > nτ for some arbitrarily large but ﬁxed τ > 0; note that this condition will be met for most

5

(10)

Rn′ (P ∗ , ¯Pδ ),

practical model selection criteria. Let h : Z+ → R+ denote the minimax optimal achievable risk as
a function of the sample size, i.e.
h(n) =

sup
inf
sup
δ :X n→{1,2,...,⌈nτ ⌉}
P ∗∈M∗
n′≥n
where the inﬁmum is over all model selection criteria restri cted to sample size n, and ⌈·⌉ denotes
rounding up to the nearest integer. ¯pδ is the prediction strategy satisfying, for all n′ ≥ n, all
xn′
∈ X n′ , ¯pδ (Xn′+1 | xn′
) := ¯pδ(xn ) (Xn′+1 | xn′
), i.e. at sample size n it predicts xn+1 using
¯pk for the k = δ(X n ) chosen by δ , and it keeps predicting future xn′+1 by this k . We call h(n)
the minimax optimal rate of convergence for model selection relative to data from M∗ , model list
M1 , M2 , . . ., and estimators ¯P1 , ¯P2 , . . . The deﬁnition is slightly nonstandard, in that we require a
second supremum over n′ ≥ n. This is needed because, as will be discussed in the full paper, it can
sometimes happen that, for some P ∗ , some k , some n′ > n, Rn′ (P ∗ , ¯Pk ) > Rn (P ∗ , ¯Pk ) (see also
[4, Section 7.1]). In cases where this cannot happen, such as regression with standard ML estimators,
and in cases where, uniformly for all k , supn′≥n Rn′ (P ∗ , ¯Pk )−Rn (P ∗ , ¯Pk ) = o(Pn
i=1 h(i)) (in the
full paper we show that this holds for, for example, histogram density estimation), our Theorem 2
also implies minimax convergence in terms of the standard de ﬁnition, without the
supn′≥n . We
expect that the supn′≥n can be safely ignored for most “reasonable” models and estim ators.
Theorem 2. Deﬁne Psw for some model class M = ∪k≥1Mk as in (4), where the prior π sat-
is ﬁes (9). Let M∗ be a subset of hMi with minimax rate h such that nh(n) is increasing, and
nh(n)/(log n)2 → ∞. Then

≤ 1.

lim sup
n→∞

supP ∗∈M∗ Pn
i=1 Ri (P ∗ , Psw )
Pn
i=1 h(i)
The requirement that nh(n)/(log n)2 → ∞ will typically be satis ﬁed whenever M∗ \ M is
nonempty. Then M∗ contains P ∗ that are “nonparametric” relative to the chosen sequence of mod-
els M1 , M2 , . . . Thus, the problem should not be “too simple”: we do not know wh ether (11) holds
in the parametric setting where P ∗ ∈ Mk for some k on the list. Theorem 2 expresses that the
accumulated risk of the switch-distribution, as n increases, is not signi ﬁcantly larger than the ac-
cumulated risk of any other procedure. This “convergence in sum” has been co nsidered before by,
for example, [13, 4], and is compared to ordinary convergenc e in the full paper, where we will also
give example applications of the theorem and further discuss (10). The proof works by bounding
the redundancy of the switch-distribution, which, by (8), is identical to the accumulated risk. It is
not clear whether similar techniques can be used to bound the individual risk.

(11)

5 Computing the Switch-Distribution

Algorithm 1 sequentially computes the posterior probabili ty on predictors p1 , p2 , . . .. It requires that
π is a prior of the form in (7), and πM is geometric, as is also required for Theorem 1 and permitted
in Theorem 2. The algorithm resembles F IX ED -SHAR E [7], but whereas F IX ED -SHAR E implicitly
imposes a geometric distribution for πT , we allow general priors by varying the shared weight with
n. We do require slightly more space to cope with πM .

Algorithm 1 SW I TCH(xN )
⊲ K is the number of experts; θ is as in the deﬁnition of πM .
k ← (1 − θ) · πK (k) od
for k = 1, . . . , K do initialise wa
k ← θ · πK (k); wb
(a K -sized array)
Report prior π(K1 ) = wa
K1
for n = 1, . . . , N do
k · pk (xn |xn−1 ) od (loss update)
for k = 1, . . . , K do wa
k · pk (xn |xn−1 ); wb
k ← wb
k ← wa
(share update)
pool ← πT (Z = n | Z ≥ n) · Pk wa
k
for k = 1, . . . , K do
wa
k ← wa
θ · pool · πK (k)
k · πT (Z 6= n | Z ≥ n) +
k ← wb
wb
+ (1 − θ) · pool · πK (k)
k
k + wb
)/ Pk (wa
k )
od
This algorithm can be used to obtain fast convergence in the sense of Theorem 2, which can be
extended to cope with a restriction to only the ﬁrst K experts. Theorem 1 can be extended to show

od
Report posterior π(Kn+1 | xn ) = (wa
+ wb
Kn+1
Kn+1

(a K -sized array)

6

consistency in this case as well. If πT (Z = n | Z ≥ n) and πK (k) can be computed in constant time,
then the running time is Θ(N · K ), which is of the same order as that of fast model selection cri teria
like AIC and BIC. We will explain this algorithm in more detail in a forthcoming publication.

Acknowledgements We thank Y. Mansour, whose remark over lunch at COLT 2005 sparked off
all this research. We thank P. Harremo ¨es and W. Koolen for mathematical support. This work was
supported in part by the IST Programme of the European Commun ity, under the PASCAL Network
of Excellence, IST-2002-506778. This publication only reﬂ ects the authors’ views.

A Proofs

= 0

(12)

with ¯Pk∗ -probability 1.

Proof of Theorem 1. Let Un = {s ∈ S | Kn+1 (s) 6= k∗ } denote the set of ‘bad’ parameters s that
select an incorrect model. It is sufﬁcient to show that
π(cid:0)s(cid:1)qs (X n )
n→∞ Ps∈Un
lim
Ps∈S π(cid:0)s(cid:1)qs (X n )
To see this, suppose the theorem is false. Then there exists a Φ ⊆ Θk∗ with wk∗ (Φ) > 0 such that
¯Pk∗ we have a contradiction with (12).
(6) does not hold for any θ∗ ∈ Φ. But then by deﬁnition of
Now let A = {s ∈ S : km (s) 6= k∗ } denote the set of parameters that are bad for sufﬁciently lar ge n.
We observe that for each s′ ∈ Un there exists at least one element s ∈ A that uses the same sequence
of switch-points and predictors on the ﬁrst n + 1 outcomes (this implies that Ki (s) = Ki (s′ ) for
i = 1, . . . , n + 1) and has no switch-points beyond n (i.e. tm (s) ≤ n). Consequently, either s′ = s
or s′ ∈ Es . Therefore
′ (xn ) ≤ Xs∈A
(π(s) + π(Es )) qs (xn ) ≤ (1 + c) Xs∈A
Xs
′ )qs
π(s
′∈Un
Deﬁning the mixture r(xn ) = Ps∈A π(s)qs (xn ), we will show that
r(X n )
with ¯Pk∗ -probability 1.
(14)
lim
= 0
π(s = (0, k∗ )) · ¯pk∗ (X n )
n→∞
Using (13) and the fact that Ps∈S π(s)qs (xn ) ≥ π(s = (0, k∗ )) · ¯pk∗ (xn ), this implies (12). For
tm+1 |xtm ) equals ¯Pkm (X∞
all s ∈ A and xtm (s) ∈ X tm (s) , by deﬁnition Qs (X∞
tm+1 |xtm ), which is
mutually singular with ¯Pk∗ (X∞
tm+1 |xtm ) by assumption. If X is a separable metric space, which
holds because X ⊆ Rd for some d ∈ Z+ , it can be shown that this conditional mutual singularity
implies mutual singularity of Qs (X∞ ) and ¯Pk∗ (X∞ ). To see this for countable X , let Bxtm be any
event such that Qs (Bxtm |xtm ) = 1 and ¯Pk∗ (Bxtm |xtm ) = 0. Then, for B = {y∞ ∈ X ∞ | y∞
tm+1 ∈
Bytm }, we have that Qs (B ) = 1 and ¯Pk∗ (B ) = 0. In the uncountable case, however, B may not be
measurable. We omit the full proof, which was shown to us by P. Harremo ¨es. Any countable mixture
of distributions that are mutually singular with Pk∗ , in particular R, is mutually singular with Pk∗ .
This implies (14) by Lemma 3.1 of [2], which says that for any two mutually singular distributions
R and P , the density ratio r(X n )/p(X n ) goes to 0 as n → ∞ with P -probability 1.

π(s)qs (xn ).

(13)

h(i),

h(i) + ǫα,n

Ri (P ∗ , Psw ) ≤ α

Proof of Theorem 2. We will show that for every α > 1,
n
n
n
Xi=1
Xi=1
Xi=1
sup
P ∗∈M∗
n→∞−→ 0, and ǫα,1 , ǫα,2 , . . . are ﬁxed constants that only depend on α, but not on the
where ǫα,n
chosen subset M∗ of hMi. Theorem 2 is a consequence of (15), which we will proceed to prove.
Let δn : X n → {1, . . . , ⌈nτ ⌉} be a model selection criterion, restricted to samples of size n, that
is minimax optimal, i.e. it achieves the inﬁmum in (10). If su ch a δn does not exist, we take a δn
that is almost minimax optimal in the sense that it achieves the inﬁmum to within h(n)/n. For
j ≥ 1, let tj = ⌈αj−1 ⌉ − 1. Fix an arbitrary n > 0 and let m be the unique integer such that
tm < n ≤ tm+1 . We will ﬁrst show that for arbitrary xn , psw achieves redundancy not much worse
than qs with s = (t1 , k1 ), . . . , (tm , km ), where ki = δti (xti ). Then we show that the redundancy of
this qs is small enough for (15) to hold. Thus, to achieve this redundancy, it is sufﬁcient to take only
a logarithmic number m − 1 of switch-points: m − 1 < logα (n + 1). Formally, we have, for some
c > 0, uniformly for all n, xn ∈ X n ,

(15)

7

qs

′ ) ≤ − log qs (xn ) − log πM (m) −
′ (xn )π(s

m
Xj=1
− log psw (xn ) = − log Xs
′∈S
≤ − log qs (xn ) + c log(n + 1) + cm(τ + 1) log n = − log qs (xn ) + O((log n)2 ).
(16)
Here the second inequality follows because of (9), and the ﬁn al equality follows because m ≤
logα (n + 1) + 1. Now ﬁx any P ∗ ∈ hMi. Since P ∗ ∈ hMi, it must have some density p∗ . Thus,
applying (8), and then (16), and then (8) again, we ﬁnd that
n
Xi=1

Ri (P ∗ , Psw ) = EX n∼P ∗ [− log psw (X n ) + log p∗ (X n )]

log πT (tj )πK (kj )

=

Ri (P ∗ , Qs ) + O((log n)2 ) =

Ri (P ∗ , ¯Pkj ) + O((log n)2 ). (17)

≤ EX n∼P ∗ [− log qs (X n ) + log p∗ (X n )] + O((log n)2 )
min{tj+1 ,n}
m
n
Xi=tj +1
Xj=1
Xi=1
For i appearing in the second sum, with tj < i ≤ tj+1 , we have Ri (P ∗ , ¯Pkj ) ≤
supi′≥tj+1 Ri′ (P ∗ , ¯Pkj ) = supi′≥tj+1 Ri′ (P ∗ , ¯Pδtj (xtj ) ) ≤ h(tj + 1), so that
1
tj+1
1
Ri (P ∗ , ¯Pkj ) ≤
tj + 1
tj + 1
tj + 1
where the middle inequality follows because nh(n) is increasing (condition (b) of the theorem).
j=1 Pmin{tj+1 ,n}
Ri (P ∗ , ¯Pkj ) ≤ α Pn
Summing over i, we get Pm
i=1 h(i). Combining this with
i=tj +1
(17), it follows that Pn
i=1 Ri (P ∗ , Psw ) ≤ α Pn
i=1 h(i) + O((log n)2 ). Because this holds for arbi-
trary P ∗ ∈ M∗ (with the constant in the O notation not depending on P ∗ ), (15) now follows by the
requirement of Theorem 2 that nh(n)/(log n)2 → ∞.

· (tj + 1)h(tj + 1) ≤

· ih(i) ≤

h(i) ≤ αh(i),

References
[1] H. Akaike. A new look at statistical model identiﬁcation.
IEEE T. Automat. Contr., 19(6):716–723, 1974.
[2] A. Barron. Logically Smooth Density Estimation. PhD thesis, Stanford University, Stanford, CA, 1985.
[3] A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling.
IEEE T. Inform. Theory, 44(6):2743–2760, 1998.
[4] A. R. Barron. Information-theoretic characterization of Bayes pe rformance and the choice of priors in
parametric and nonparametric problems. In Bayesian Statistics 6, pages 27–52, 1998.
[5] A. P. Dawid. Statistical theory: The prequential approach. J. Roy. Stat. Soc. A, 147, Part 2:278–292, 1984.
[6] P. D. Gr ¨unwald. The Minimum Description Length Principle. The MIT Press, 2007.
[7] M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32:151–178, 1998.
[8] R. E. Kass and A. E. Raftery. Bayes factors. J. Am. Stat. Assoc., 90(430):773–795, 1995.
[9] K. Li. Asymptotic optimality of cp , cl , cross-validation and generalized cross-validation: Discrete index
set. Ann. Stat., 15:958–975, 1987.
[10] C. Monteleoni and T. Jaakkola. Online learning of non-stationary sequences.
Information Processing Systems, volume 16, Cambridge, MA, 2004. MIT Press.
[11] J. Rissanen. Universal coding, information, prediction, and es timation. IEEE T. Inform. Theory, IT-30(4):
629–636, 1984.
[12] J. Rissanen. Stochastic Complexity in Statistical Inquiry. World Scientiﬁc, 1989.
[13] J. Rissanen, T. P. Speed, and B. Yu. Density estimation by stochastic complexity. IEEE T. Inform. Theory,
38(2):315–323, 1992.
[14] G. Schwarz. Estimating the dimension of a model. Ann. Stat., 6(2):461–464, 1978.
[15] R. Shibata. Asymptotic mean efﬁciency of a selection of regression variables. Ann. I. Stat. Math., 35:
415–423, 1983.
[16] M. Stone. An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. J.
Roy. Stat. Soc. B, 39:44–47, 1977.
[17] P. Volf and F. Willems. Switching between two universal source coding algorithms. In Proceedings of the
Data Compression Conference, Snowbird, Utah, pages 491–500, 1998.
[18] V. Vovk. Derandomizing stochastic prediction strategies. Machine Learning, 35:247–282, 1999.
[19] Y. Yang. Can the strengths of AIC and BIC be shared? Biometrica, 92(4):937–950, 2005.
[20] Y. Yang. Model selection for nonparametric regression. Statistica Sinica, 9:475–499, 1999.

In Advances in Neural

8


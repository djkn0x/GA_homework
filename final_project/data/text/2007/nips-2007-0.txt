Inferring Elapsed Time
from Stochastic Neural Processes

Misha B. Ahrens and Maneesh Sahani
Gatsby Computational Neuroscience Unit, UCL
Alexandra House, 17 Queen Square, London, WC1N 3AR
{ahrens, maneesh}@gatsby.ucl.ac.uk

Abstract

Many perceptual processes and neural computations, such as speech recognition,
motor control and learning, depend on the ability to measure and mark the passage
of time. However, the processes that make such temporal judgements possible are
unknown. A number of different hypothetical mechanisms have been advanced,
all of which depend on the known, temporally predictable evolution of a neu-
ral or psychological state, possibly through oscillations or the gradual decay of a
memory trace. Alternatively, judgements of elapsed time might be based on ob-
servations of temporally structured, but stochastic processes. Such processes need
not be speci ﬁc to the sense of time; typical neural and sensor y processes contain at
least some statistical structure across a range of time scales. Here, we investigate
the statistical properties of an estimator of elapsed time which is based on a simple
family of stochastic process.

1

Introduction

The experience of the passage of time, as well as the timing of events and intervals, has long been
of interest in psychology, and has more recently attracted attention in neuroscience as well. Timing
information is crucial for the correct functioning of a large number of processes, such as accurate
limb movement, speech and the perception of speech (for example, the difference between “ba” and
“pa” lies only in the relative timing of voice onsets), and ca usal learning.

Neuroscienti ﬁc evidence that points to a specialized neura l substrate for timing is very sparse, par-
ticularly when compared to the divergent set of speci ﬁc mech anisms which have been theorized.
One of the most inﬂuential proposals, the scalar expectancy theory (SET) of timing [1], suggests
that interval timing is based on the accumulation of activity from an internal oscillatory process.
Other proposals have included banks of oscillators which, when ﬁne-tuned, produce an alignment
of phases at a speci ﬁed point in time that can be used to genera te a neuronal spike [2]; models in
which timing occurs via the characteristic and monotonic decay of memory traces [3] or reverberant
activity [4]; and randomly-connected deterministic networks, which, given neuronal processes of
appropriate timescales, can be shown to encode elapsed time implicitly [5].

Although this multitude of theories shows that there is little consensus on the mechanisms respon-
sible for timing, it does point out an important fact: that timing information is present in a range
of different processes, from oscillations to decaying memories and the dynamics of randomly con-
nected neural networks. All of the theories above choose one speci ﬁc such process, and suggest that
observers rely on that one alone to judge time. An alternative, which we explore here, is to phrase
time estimation as a statistical problem, in which the elapsed time ∆t is extracted from a collec-
tion of stochastic processes whose statistics are known. This is loosely analagous to accounts have
appeared in the psychological literature in the form of number-of-events models [6], which suggest
that the number of events in an interval inﬂuence the percept
ion of its duration. Such models have

1

been related to recent psychological ﬁndings the show that t he nature of the stimulus being timed
affects judgments of duration [7].

Here, by contrast, we consider the properties of duration estimators that are based on more general
stochastic processes. The particular stochastic processes we analyze are abstract. However, they
may be seen as models both for internally-generated neural processes, such as (spontaneous) net-
work activity and local ﬁeld potentials, and for sensory pro cesses, in the form of externally-driven
neural activity, or (taking a functional view) in the form of the stimuli themselves. Both neural activ-
ity and sensory input from the environment follow well-deﬁn ed temporal statistical patterns, but the
exploitation of these statistics has thus far not been studied as a potential substrate for timing judge-
ments, despite being potentially attractive. Such a basis for timing is consistent with recent studies
that show that the statistics of external stimuli affect timing estimates [8, 7], a behavior not captured
by the existing mechanistic models. In addition, there is evidence that timing mechanisms are dis-
tributed [9] but subject to local (e.g. retinotopic or spatiotopic) biases [10]. Using the distributed
time-varying processes which are already present in the brain is implementationally efﬁcient, and
lends itself straightforwardly to a distributed implementation. At the same time, it suggests a possi-
ble origin for the modality-speci ﬁcity and locality of the b ias effects, as different sets of processes
may be exploited for different timing purposes. Here, we show primarily that interval estimates
based on such processes obey a Weber-like scaling law for accuracy under a wide range of assump-
tions, as well as scaling with process number that is consistent with experimental observation; and
we use estimation theoretic analysis to ﬁnd the reasons behi nd the robustness of these scaling laws.

Neuronal spike trains exhibit internal dependencies on many time scales, ranging from milliseconds
to tens of seconds [11, 12], so these — or, more likely, proces
ses derived from spike trains, such
as average network activity — are plausible candidates for t
he types of processes assumed in this
paper. Likewise, sensory information too varies over a large range of temporal scales [13]. The
particular stochastic processes we use here are Gaussian Processes, whose power spectra are chosen
to be broad and roughly similar to those seen in natural stimuli.

2 The framework

To illustrate how random processes contain timing information, consider a random walk starting at
the origin, and suppose that we see a snapshot of the random walk at another, unknown, point in
time. If the walk were to end up very far from the origin, and if some statistics of the random walk
were known, we would expect that the time difference between the two observations, ∆t, must be
reasonably long in comparison to the diffusion time of the process. If, however, the second point
were still very close to the origin, we might assign a high probability to ∆t ≈ 0, but also some
probability (associated with delayed return to the orgin) to |∆t| > 0. Access to more than one such
random walk would lead to more accurate estimates (e.g. if two random walks had both moved very
little between the two instances in time, our conﬁdence that ∆t ≈ 0 would be greater). From such
considerations it is evident that, on the basis of multiple stochastic processes, one can build up a
probabilistic model for ∆t.
To formalize these ideas, we model the random processes as a family of independent stationary
Gaussian Processes (GPs, [14]). A GP is a stochastic process y(t) in which any subset of observa-
tions {y(t), y(t′ ), y(t′′ ), ...} is jointly Gaussian distributed, so that the probability distribution over
observations is completely speci ﬁed by a mean value (here se t to zero) and a covariance structure
(here assumed to remain constant in time). We denote the set of processes by {yi (t)}. Although
this is not a necessity, we let each process evolve independently according to the same stochastic
dynamics; thus the process values differ only due to the random effects. Mimicking the tempo-
ral statistics of natural scenes [15], we choose the dynamics to simultaneously contain multiple
time scales — speci ﬁcally, the power spectrum approximatel
y follows a 1/f 2 power law, were
f = frequency = 1/(time scale). Some instances of such processes are shown in Figure 1.
Stationary Gaussian processes are fully described by the covariance function K (∆t):

hyi (t)yi (t + ∆t)i = K (∆t)
so that the probability of observing a sequence of values [yi (t1 ), yi (t2 ), ..., yi (tn )] is Gaussian dis-
tributed, with zero mean and covariance matrix Σn,n′ = K (tn′ − tn ).

2

y

r
e
w
o
p
 
g
o
l

0

−5

−10

time

−4

−2
0
log frequency

2

4

Figure 1: Left: Two examples of the GPs used for inference of ∆t. Right: Their power spectrum.
This is approximately a 1/f 2 spectrum, similar to the temporal power spectrum of visual scenes.

To generate processes with multiple time scales, we approximate a 1/f 2 spectrum with a sum over
Q squared exponential covariance functions:

K (∆t) =

q ) + σ2
q exp(−∆t2 /2l2
α2
y I (∆t)

QXq=1
Here σ2
y I (∆t) describes the instantaneous noise around the underlying covariance structure (I is
the indicator function, which equals 1 when its argument is zero), and lq are the time scales of the
component squared exponential functions. We take these to be linearly spaced, so that lq ∝ q . To
q = 1/Q. Figure
mimic a 1/f 2 spectrum, we choose the power of each component to be constant: α2
1 shows that this choice does indeed quite accurately reproduce a 1/f 2 power spectrum.
To illustrate how elapsed time is implicitly encoded in such stochastic processes, we infer the dura-
tion of an interval [t, t + ∆t] from two instantaneous observations of the processes, namely {yi (t)}
and {yi (t+∆t)}. For convenience, yi is used to denote the vector [yi (t), yi (t+∆t)]. The covariance
matrix Σ(∆t) of yi , which is of size 2x2, gives rise to a likelihood of these observations,

yi(cid:19)
|Σ|−1/2 exp (cid:18)−
P ({yi (t)}, {yi (t + ∆t)}|∆t) ∝ Yi
With the assumption of a weak prior1 , this yields a posterior distribution over ∆t:

T
i Σ−1

1
2

y

Φ(∆t) = P (∆t|{yi }) ∝ P (∆t) · Yi
P (yi |∆t)
∝ P (∆t) · exp  −
yi (cid:3)!
2 Xi (cid:2)log |Σ| + y
1
This distribution gives a probabilistic description of the time difference between two snapshots of
the random processes. As we will see below (see Figure 2), this distribution tends to be centred on
the true value of ∆t, showing that such random processes may indeed be exploited to obtain timing
information. In the following section, we explore the statistical properties of timing estimates based
on Φ, and show that they correspond to several experimental ﬁndi ngs.

T
i Σ−1

1 such as P (∆t) = β exp(−β∆t)Θ(∆t) with β ≪ 1 and Θ the Heaviside function, or P (∆t) = U [0, tmax ];
the details of the weak prior do not affect the results.

3

t
 
∆
 
d
e
t
a
m
i
t
s
e

25

20

15

10

5

0

0

5

10

∆ t

15

n
o
i
t
a
i
v
e
d
 
d
r
a
d
n
a
t
s

4

3

2

1

0

0

5

10

∆ t

15

Figure 2: Statistics of the inference of ∆t from snapshots of a group of GPs. The GPs have time
scales in the interval [0.05, 50]. Left: The mean estimated times (blue) are clustered around the true
times (dashed). Right: The Weber law of timing, σ ∝ ∆t, approximately holds true for this model.
The error bars are standard errors derived via a Laplace approximation to the posterior. A straight
line ﬁt is shown with a dashed line. The Cramer-Rao bound (blue), which will be derived later in
the text, predicts the empirical data well.

3 Scaling laws and behaviour

3.1 Empirical demonstration of Weber’s law

Many behavioral studies have shown that the standard deviation of interval estimates is proportional
to the interval being judged, σ ∝ ∆t, across a wide range of timescales and tasks (e.g. [1]). Here,
we show that GP-based estimates share this property under broad conditions.

To compare the behaviour of the model to experimental data, we must choose a mapping from the
function Φ to a single scalar value, which will model the observer’s report. A simple choice is
to assume that the reported ∆t is the maximum a-posteriori (MAP) estimator based on Φ, that is,
c∆tMAP = argmax∆t Φ(∆t). To compare the statistics of this estimator to the experimental obser-
vation, we took samples {yi (t)} and {yi (t + ∆t)} from 50 GPs with identical 1/f 2 -like statistics
containing time scales from 1 to 40 time units. 100 samples were generated for each ∆t (ranging
from 1 to 16 time unis), leading to 100 estimates, c∆tMAP . These estimates are plotted in Figure 2A.
They are seen to follow the true ∆t. Their spread around the true value increases with increasing
∆t. The standard deviation of this spread is plotted in Figure 2B, and is a roughly linear function of
∆t. Thus, time estimation is possible using the stochastic process framework, and the Weber law of
timing holds fairly accurately.

3.2 Fisher Information and Weber’s law

A number of questions about this Weber-like result naturally arise: Does it still hold if one changes
the power spectrum of the processes? What if one changes the scale of the instantaneous noise? We
y , and found that the Weber law was still approximately satis ﬁ ed. When
increased the noise scale σ2
changing the power spectrum of the processes from a 1/f 2 -type spectrum to a 1/f 3 -type spectrum
i ∝ 1), the Weber law was still approximately satis ﬁed (Figure
i ∝ li instead of α2
(by letting α2
3). This result may appear somewhat counter-intuitive, as one might expect that the accuracy of the
estimator for ∆t would increase as the power in frequencies around 1/∆t increased; thus, changing
the power spectrum to 1/f 3 might be expected to result in more accurate estimates of large ∆t
(lower frequencies) as compared to estimates of small ∆t, but this was not the case.
To ﬁnd reasons for this behaviour, it would useful to have an a nalytical expression for the relation-
ship between the variability of the estimated duration and the true duration. This is complex, but a
simpler analytical approximation to this relation can be constructed through the Cramer-Rao bound.
This is a lower bound on the asymptotic variance of an unbiased Maximum Likelihood estimator of
∆t and is given by the inverse Fisher Information:

4

y

time

2.5

2

1.5

1

0.5

n
o
i
t
a
i
v
e
d
 
d
r
a
d
n
a
t
s

0

0

5

10

∆ t

15

Figure 3: Left: Two examples of GPs with a different power spectrum (α2
i ∝ li , for li ∝ i, which
approximates a 1/f 3 power spectrum, resulting in much smoother dynamics). Right: Inference of
∆t based on these altered processes. Note that the estimator c∆tMAP is based on the true likelihood,
i.e., the new 1/f 3 statistics. The Weber law still approximately holds, even though the dynamics
is different from the initial case. The empirical standard deviation is again well predicted by the
analytical Cramer-Rao bound (blue).

Var(c∆t) ≥ 1/IF (∆t)
The Fisher Information, assuming that the elapsed time is estimated on the basis of N processes,
each evolving according to covariance matrix Σ(∆t), is given by the expression
∂∆t (cid:21)
Tr (cid:20)Σ−1 ∂Σ
Ey
IF (∆t) = −N D ∂ 2 log P ({yi }|∆t)
Σ−1 ∂Σ
∂∆t2
∂∆t
This bound is plotted in blue in Figure 2, and again in Figure 3, and can be seen to be a good
approximation to the empirical behaviour of the model.

N
2

(1)

=

What is the reason for the robust Weber-like behaviour? To answer this question, consider a different
but related model, in which there are N Gaussian processes, again labeled i, but each now evolv-
ing according to different covariance matrix Ci (∆t). Previously, each process reﬂected structure at
many timescales. In this new model, each process evolves with a single squared-exponential covari-
ance kernel, and thus a single time-constant. This will allow us to see how each process contributes
to the accuracy of the estimator.
Thus, in this model, [Ci (∆t)]n,n′ = α2
y I (tn′ −tn ). (The power spectrum
i )+σ2
i exp(−(tn′ −tn )2 /2l2
is then shaped as exp(−f 2 l2
i /2).) The likelihood of observing the processes at two instances is now
|Ci |−1/2 exp (cid:18)−
i yi(cid:19)
P ({yi (t)}, {yi (t + ∆t)}|∆t) ∝ Yi
T
i C−1
This model shows very similar behaviour to the original model, but is somewhat less natural. Its
advantage lies in the fact that the Fisher Information can now be decomposed as a sum over different
time scales,

(2)

1
2

y

IF,i =

∂∆t (cid:21)
Tr (cid:20)C−1
IF (∆t) = Xi
2 Xi
∂Ci
1
i
Using the Fisher Information to plot Cramer-Rao bounds for different types of processes {yi (t)}
(Figure 4, dashed lines), we ﬁrst note that the bounds are all
relatively close to linear, even though
the parameters governing the processes are very different. In particular, we tested both linear spacing
of time scales (li ∝ i) and quadratic spacing (li ∝ i2 ), and we tested a constant power distribution

∂Ci
∂∆t

C−1
i

5

power ~ time scale
lengthscales spaced linearly

power ~ time scale
lengthscales spaced quadratically

l=6
l=11,...
l=46
Cr.−Rao bound

l=0.7
l=2.4,...
l=42.3
Cr.−Rao bound

2
/
1
−
)
F
I
(
 
d
n
a
 
F
I

0

10

20

∆ t

30

0

10

20

∆ t

30

power = constant
lengthscales spaced linearly

power = constant
lengthscales spaced quadratically

l=6
l=11,...
l=46
Cr.−Rao bound

l=0.7
l=2.4,...
l=42.3
Cr.−Rao bound

2
/
1
−
)
F
I
(
 
d
n
a
 
F
I

2
/
1
−
)
F
I
(
 
d
n
a
 
F
I

2
/
1
−
)
F
I
(
 
d
n
a
 
F
I

0

10

20

∆ t

30

0

10

20

∆ t

30

Figure 4: Fisher Information and Cramer-Rao bounds for the model of equation 2. The Cramer-Rao
bound is the square root of the inverse of the sum of all the Fisher Information curves (note that
only a few Fisher Information curves are shown). The noise scale σ2
y = 0.1, and the time scales are
either li = i, i = 1, 2, . . . , 50 (linear) or li = i2 /50, i = 1, 2, . . . , 50 (quadratic). The power of each
i = li . The graphs show that each time scale contributes to
i = 1 (constant) or α2
process is either α2
the estimation of a wide range of ∆t, and that the Cramer-Rao bounds are all fairly linear, leading
to a robust Weber-like behaviour of the estimator of elapsed time.

i ∝ li ). None of
(αi = 1) and a power distribution where slower processes have more power (α2
these manipulations caused the Cramer-Rao bound to deviate much from linearity.

Next, we can evaluate the contribution of each time scale to the accuracy of estimates of ∆t, by
inspecting the Fisher Information IF,i of a given process yi . Figure 4 shows that (contrary to the
intuition that time scales close to ∆t contribute most to the estimation of ∆t) a process evolving at
a certain time scale lj contributes to the estimation of elapsed time ∆t even if ∆t is much smaller
than lj (indeed, the peak of IF,j does not lie at lj , but below it). This lies at the heart of the robust
Weber-like behaviour: the details of the distribution of time scales do not matter much, because each
time scale contributes to the estimation of a wide range of ∆t. For similar reasons, the distribution
of power does not drastically affect the Cramer-Rao bound. From the graphs of IF,i , it is evident
that the Weber law arises from an accumulation of high values of Fisher Information at low values
of ∆t.
Very small values of ∆t may be an exception, if the instantaneous noise dominates the subtle changes
that the processes undergo during very short periods; for these ∆t, the standard deviation may rise.
This is reﬂected by a subtle rise in some of the Cramer-Rao bou nds at very low values of ∆t.
However, it may be assumed that the shortest times that neural systems can evaluate are no shorter
than the scale of the fastest process within the system, making these small ∆t’s irrelevant.

3.3 Dependence of timing variability on the number of processes

Increasing the number of processes, say Nprocesses , will add more terms to the likelihood and make
the estimated ∆t more accurate. The Fisher Information (equation 1) scales with Nprocesses , which
suggests that the standard deviation of c∆tMAP is proportional to 1/pNprocesses ; this was conﬁrmed
empirically (data not shown).
6

Psychologically and neurally, increasing the number of processes would correspond to adding more
perceptual processes, or expanding the size of the network that is being monitored for timing es-
timation. Although experimental data on this issue is sparse, in [9], it is shown that unimanual
rhythm tapping results in a higher variability of tapping times than bimanual rhythm tapping, and
that tapping with two hands and a foot results in even lower variability.
This correlates well with the theoretical scaling behaviour of the estimator c∆tMAP . Note that a
similar scaling law is obtained from the Multiple Timer Model [16]. This is not a model for timing
itself, but for the combination of timing estimates of multiple timers; the Multiple Timer Model
combines these estimates by averaging, which is the ML estimate arising from independent draws
of equal variance Gaussian random variables, also resulting in a 1/√N scaling law.
Experimentally, a slower decrease in variability than a 1/√N law was observed. This can be ac-
counted for by assuming that the processes governing the right and left hands are dependent, so that
the number of effectively independent processes grows more slowly than the number of effectors.

4 Conclusion

We have shown that timing information is present in random processes, and can be extracted prob-
abilistically if certain statistics of the processes are known. A neural implementation of such a
framework of time estimation could use both internally generated population activity as well as
external stimuli to drive its processes.

The timing estimators considered were based on the full probability distribution of the process val-
ues at times t and t′ , but simpler estimators could also be constructed. There are two reasons for
considering simpler estimators: First, simpler estimators might be more easily implemented in neu-
ral systems. Second, to calculate Φ(∆t), one needs all of {yi (t), yi (t′ )}, so that (at least) {yi (t)}
has to be stored in memory. One way to construct a simpler estimator might be to select a particular
class (say, a linear function of {yi }) and optimize over its parameters. Alternatively, an estimator
may be based on the posterior distribution over ∆t conditioned on a reduced set of parameters, with
the neglected parameters integrated out. Another route might be to consider different stochatic pro-
cesses, which have more compact sufﬁcient statistics (e.g. Brownian motion, being translationally
invariant, would require only {yi (t′ ) − yi (t)} instead of {yi (t), yi (t′ )}; we have not considered such
processes because they are unbounded and therefore hard to associate with sensory or neural pro-
cesses). We have not addressed how a memory mechanism might be combined with the stochastic
process framework; this will be explored in the future.

The intention of this paper is not to offer a complete theory of neural and psychological timing, but to
examine the statistical properties of a hitherto neglected substrate for timing — stochastic processes
that take place in the brain or in the sensory world. It was demonstrated that estimators based on
such processes replicate several important behaviors of humans and animals. Full models might be
based on the same substrate, thereby naturally incorporating the same behaviors, but contain more
completely speci ﬁed relations to external input, memory me chanisms, adaptive mechanisms, neural
implementation, and importantly, (supervised) learning of the estimator.

The neural and sensory processes that we assume to form the basis of time estimation are, of course,
not fully random. But when the deterministic structure behind a process is unknown, they can still be
treated as stochastic under certain statistical rules, and thus lead to a valid timing estimator. Would
the GP likelihood still apply to real neural processes or would the correct likelihood be completely
different? This is unknown; however, the Multivariate Central Limit Theorem implies that sums
of i.i.d. stochastic processes tend to Gaussian Processes —
so that, when e.g. monitoring average
neuronal activity, the correct estimator may well be based on a GP likelihood.

An issue that deserves consideration is the mixing of internal (neural) and external (sensory) pro-
cesses. Since timing information is present in both sensory processes (such as sound and movement
of the natural world, and the motion of one’s body) and internal processes (such as ﬂuctuations in
network activity), and because stimulus statistics inﬂuen ce timing estimates, we propose that psy-
chological and neural timing may make use of both types of processes. However, ﬂuctuations in the
external world do not always translate into neural ﬂuctuati ons (e.g. there is evidence for a spatial

7

code for temporal frequency in V2 [17]), so that neural and stimulus ﬂuctuations cannot always be
treated on the same footing. We will address this issue in the future.

The framework presented here has some similarities with the very interesting and more explicitly
physiological model proposed by Buonomano and colleagues [5, 18], in which time is implicitly
encoded in deterministic2 neural networks through slow neuronal time constants. However, temporal
information in the network model is lost when there are stimulus-independent ﬂuctuations in the
network activity, and the network can only be used as a reliable timer when it starts from a ﬁxed
resting state, and if the stimulus is identical on every trial. The difference in our scheme is that here
timing estimates are based on statistics, rather than deterministic structure, so that it is fundamentally
robust to noise, internal ﬂuctuations, and stimulus change s. The stochastic process framework is,
however, more abstract and farther removed from physiology, and a neural implementation may well
share some features of the network model of timing.

Acknowledgements: We thank Jeff Beck for useful suggestions, and Peter Dayan and Carlos Brody
for interesting discussions.

References
[1] J Gibbon. Scalar expectancy theory and Weber’s law in animal timing. Psychol Rev, 84:279–325, 1977.
[2] R C Miall. The storage of time intervals using oscillating neurons. Neural Comp, 1:359–371, 1989.
[3] J E R Staddon and J J Higa. Time and memory: towards a pacemaker-free theory of interval timing. J
Exp Anal Behav, 71:215–251, 1999.
[4] G Bugmann. Towards a neural model of timing. Biosystems, 48:11–19, 1998.
[5] D V Buonomano and M M Merzenich. Temporal information transformed into a spatial code by a neural
network with realistic properties. Science, 267:1028–1030, 1995.
[6] D Poynter. Judging the duration of time intervals: A process of remembering segments of experience.
In I Levin and D Zakay, editors, Time and human cognition: A life-span perspective, pages 305–331.
Elsevier, 1989.
[7] R Kanai, C L E Paffen, H Hogendoorn, and F A J Verstraten. Time dilation in dynamic visual display. J
Vision, 6:1421–1430, 2006.
[8] D M Eagleman, P U Tse, D V Buonomano, P Janssen, A C Nobre, and A O Holcombe. Time and the
brain: How subjective time relates to neural time. J Neurosci, pages 10369–10371, 2005.
[9] R B Ivry, T C Richardson, and L L Helmuth. Improved temporal stability in multi-effector movements. J
Exp Psychol, 28:72–92, 2002.
[10] D Burr, A Tozzi, and M C Morrone. Neural mechanisms for timing visual events are spatially selective in
real-world coordinates. Nat Neurosci, 10:423–425, 2007.
[11] M C Teich, C Heneghan, and S B Lowen. Fractal characted of the neural spike train in the visual system
of the cat. J Opt Soc Am A, 14:529–546, 1997.
[12] L C Osborne, W Bialek, and S G Lisberger. Time course of information about motion direction in visual
area MT of macaque monkeys. J Neurosci, 24:3210–3222, 2004.
[13] H Attias and C E Schreiner. Temporal low-order statistics of natural sounds.
Information Processing Systems 9, pages 27–33, 1996.
[14] C E Rasmussen and C K I Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge,
MA, 2006.
[15] D W Dong and J J Atick. Statistics of natural time-varying images. Network: Computation in Neural
Systems, 6:345–358, 1995.
[16] R B Ivry and T C Richardson. Temporal control and coordination: the multiple timer model. Brain and
Cognition, 48:117–132, 2002.
[17] K H Foster, J P Gaska, M Nagler, and D A Pollen. Spatial and temporal frequency selectivity of neurones
in visual cortical areas v1 and v2 of the macaque monkey. J Physiol, 365:331–363, 1985.
[18] U R Karmarkar and D V Buonomano. Timing in the absence of clocks: encoding time in neural network
states. Neuron, 53:427–438, 2007.

In Advances in Neural

2While this model and some other previous models might also contain neuronal noise, it is the deterministic
(and known) element of their behaviour which encodes time.

8


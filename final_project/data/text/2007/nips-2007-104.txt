Convex Clustering with Exemplar-Based Models

Polina Golland
Danial Lashkari
Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139
{danial, polina}@csail.mit.edu

Abstract
Clustering is often formulated as the maximum likelihood estimation of a mixture
model that explains the data. The EM algorithm widely used to solve the resulting
optimization problem is inherently a gradient-descent method and is sensitive to
initialization. The resulting solution is a local optimum in the neighborhood of
the initial guess. This sensitivity to initialization presents a signiﬁcant challenge
in clustering large data sets into many clusters. In this paper, we present a dif-
ferent approach to approximate mixture ﬁtting for clustering. We introduce an
exemplar-based likelihood function that approximates the exact likelihood. This
formulation leads to a convex minimization problem and an efﬁcient algorithm
with guaranteed convergence to the globally optimal solution. The resulting clus-
tering can be thought of as a probabilistic mapping of the data points to the set of
exemplars that minimizes the average distance and the information-theoretic cost
of mapping. We present experimental results illustrating the performance of our
algorithm and its comparison with the conventional approach to mixture model
clustering.

1 Introduction
Clustering is one of the most basic problems of unsupervised learning with applications in a wide
variety of ﬁelds. The input is either vectorial data, that is, vectors of data points in the feature
space, or proximity data, the pairwise similarity or dissimilarity values between the data points. The
choice of the clustering cost function and the optimization algorithm employed to solve the problem
Intuitively, most methods seek compact clusters of data
determines the resulting clustering [1].
points, namely, clusters with relatively small intra-cluster and high inter-cluster distances. Other
approaches, such as Spectral Clustering [2], look for clusters of more complex shapes lying on some
low dimensional manifolds in the feature space. These methods typically transform the data such
that the manifold structures get mapped to compact point clouds in a different space. Hence, they
do not remove the need for efﬁcient compact-cluster-ﬁnding techniques such as k-means.
The widely used Soft k-means method is an instance of maximum likelihood ﬁtting of a mixture
model through the EM algorithm. Although this approach yields satisfactory results for problems
with a small number of clusters and is relatively fast, its use of a gradient-descent algorithm for
minimization of a cost function with many local optima makes it sensitive to initialization. As
the search space grows, that is, the number of data points or clusters increases, it becomes harder
to ﬁnd a good initialization. This problem often arises in emerging applications of clustering for
large biological data sets such as gene-expression. Typically, one runs the algorithm many times
with different random initializations and selects the best solution. More sophisticated initialization
methods have been proposed to improve the results but the challenge of ﬁnding good initialization
for EM algorithm remains [4].
We aim to circumvent the initialization procedure by designing a convex problem whose global
optimum can be found with a simple algorithm.
It has been shown that mixture modeling can

1

be formulated as an instance of iterative distance minimization between two sets of probability
distributions [3]. This formulation shows that the non-convexity of mixture modeling cost function
comes from the parametrization of the model components . More precisely, any mixture model is,
by deﬁnition, a convex combination of some set of distributions. However, for a ﬁxed number of
mixture components, the set of all such mixture models is usually not convex when the distributions
have, say, free mean parameters in the case of normal distributions.
Inspired by combinatorial,
non-parametric methods such as k-medoids [5] and afﬁnity propagation [6], our main idea is to
employ the notion of exemplar ﬁnding, namely, ﬁnding the data points which could best describe
the data set. We assume that the clusters are dense enough such that there is always a data point
very close to the real cluster centroid and, thus, restrict the set of possible cluster means to the set
of data points. Further, by taking all data points as exemplar candidates, the modeling cost function
becomes convex. A variant of EM algorithm ﬁnds the globally optimal solution.
Convexity of the cost function means that the algorithm will unconditionally converge to the global
minimum. Moreover, since the number of clusters is not speciﬁed a priori, the algorithm automat-
ically ﬁnds the number of clusters depending only on one temperature-like parameter. This param-
eter, which is equivalent to a common ﬁxed variance in case of Gaussian models, deﬁnes the width
scale of the desired clusters in the feature space. Our method works exactly in the same way with
both proximity and vectorial data, unifying their treatment and providing insights into the modeling
assumptions underlying the conversion of feature vectors into pairwise proximity data.
In the next section, we introduce our maximum likelihood function and the algorithm that maximizes
it. In Section 3, we make a connection to the Rate-Distortion theory as a way to build intuition about
our objective function. Section 4 presents implementation details of our algorithm. Experimental
results comparing our method with a similar mixture model ﬁtting method are presented in Section
5, followed by a discussion of the algorithm and the related work in Section 6.

2 Convex Cost Function
Given a set of data points X = {x1 , · · · , xn } ⊂ IRd , mixture model clustering seeks to maximize
(cid:21)
(cid:20) kX
nX
the scaled log-likelihood function
qj f (xi ; mj )
,
j=1
i=1

j=1 , {mj }k
l({qj }k
j=1 ; X ) =

log

1
n

(1)

where f (x; m) is an exponential family distribution on random variable X.
It has been shown
that there is a bijection between regular exponential families and a broad family of divergences
called Bregman divergence [7]. Most of the well-known distance measures, such as Euclidean
distance or Kullback-Leibler divergence (KL-divergence) are included in this family. We em-
ploy this relationship and let our model be an exponential family distribution on X of the form
f (x; m) = C (x) exp(−dφ (x, m)) where dφ is some Bregman divergence and C (x) is indepen-
dent of m. Note that with this representation, m is the expected value of X under the distribution
f (x; m). For instance, taking Euclidean distance as the divergence, we obtain normal distribution
as our model f .
In this work, we take models of the above form whose parameters m lie in the same space as data
vectors. Thus, we can restrict the set of mixture components to the distributions centered at the data
points, i.e., mj ∈ X . Yet, for a speciﬁed number of clusters k , the problem still has a combinatorial
nature of choosing the right k cluster centers among n data points. To avoid this problem, we
increase the number of possible components to n and represent all data points as cluster-center
(cid:21)
(cid:20) nX
nX
nX
nX
candidates. The new log-likelihood function is
j=1
i=1
j=1
i=1
where fj (x) is an exponential family member with its expectation parameter equal to the j th data
n
o
vector and the constant denotes a term that does not depend on the unknown variables {qj }n
Q|Q(·) = Pn
j=1 .
The constant scaling factor β in the exponent controls the sharpness of mixture components. We
maximize l(·; X ) over the set of all mixture distributions Q =
j=1 qj fj (·)
.

l({qj }n
j=1 ; X ) =

qj e−βdφ (xi ,xj )

qj fj (xi ) =

log

+ const. ,

(2)

1
n

1
n

log

2

(3)

The log-likelihood function (2) can be expressed in terms of the KL-divergence by deﬁning ˆP (x) =
D( ˆP kQ) = − X
1/n, x ∈ X , to be the empirical distribution of the data on IRd and by noting that
ˆP (x) log Q(x) − H( ˆP ) = −l({qj }n
j=1 ; X ) + const.
x∈X
where H( ˆP ) is the entropy of the empirical distribution and does not depend on the unknown mixture
coefﬁcients {qj }n
j=1 . Consequently, the maximum likelihood problem can be equivalently stated as
the minimization of the KL-divergence between ˆP and the set of mixture distributions Q.
It is easy to see that unlike the unconstrained set of mixture densities considered by the likelihood
function (1), set Q is convex. Our formulation therefore leads to a convex minimization problem.
Furthermore, it is proved in [3] that for such a problem, the sequence of distributions Q(t) with
corresponding weights {q (t)
j }n
X
j=1 deﬁned iteratively via
Pn
ˆP (x)fj (x)
j 0=1 q (t)
j 0 fj 0 (x)
x∈X
is guaranteed to converge to the global optimum solution Q∗ if the support of the initial distribution
is the entire index set, i.e., q (0)
j > 0 for all j .

= q (t)
j

q (t+1)
j

(4)

3 Connection to Rate-Distortion Problems
Now, we present an equivalent statement of our problem on the product set of exemplars and data
points. This alternative formulation views our method as an instance of lossy data compression and
directly implies the optimality of the algorithm (4).
The following proposition is introduced and proved in [3]:
Proposition 1. Let Q0 be the set of distributions of the complete data random variable (J, X) ∈
{1, · · · , n} × IRd with elements Q0 (j, x) = qj fj (x). Let P 0 be the set of all distributions on the
same random variable (J, X) which have ˆP as their marginal on X. Then,
P 0∈P 0 ,Q0∈Q0D(P 0 kQ0 )
Q∈Q D( ˆP kQ) =
min
min
(5)
where Q is the set of all marginal distributions of elements of Q0 on X. Furthermore, if Q∗ and
(P 0∗ , Q0∗ ) are the corresponding optimal arguments, Q∗ is the marginal of Q0∗ .
(cid:26) 1
This proposition implies that we can express our problem of minimizing (3) as minimization of
D(P 0 kQ0 ) where P 0 and Q0 are distributions of the random variable (J, X). Speciﬁcally, we deﬁne:
n rij , x = xi ∈ X ;
P 0 (j, x) = ˆP (x)P 0 (j |x) =
Q0 (j, x) = qj C (x)e−βdφ (x,xj )
0,
otherwise
where qj and rij = P 0 (j |x = xi ) are probability distributions over the set {j }n
j=1 . This formulation
ensures that P 0 ∈ P 0 , Q0 ∈ Q0 and the objective function is expressed only in terms of variables
qj and P 0 (j |x) for x ∈ X . Our goal is then to solve the minimization problem in the space of
j=1 × {j }n
distributions of random variable (J, I ) ∈ {j }n
j=1 , namely, in the product space of exemplar
× data point indices. Substituting expressions (6) into the KL-divergence D(P 0 kQ0 ), we obtain the
(cid:21)
(cid:20)
nX
equivalent cost function:
1
D(P 0 kQ0 ) =
+ const.
+ βdφ (xi , xj )
P
rij
n
i,j=1
It is straightforward to show that for any set of values rij , setting qj = 1
i rij minimizes (7).
(cid:20)
(cid:21)
nX
n
Substituting this expression into the cost function, we obtain the ﬁnal expression
P
1
D(P 0 kQ0∗ (P 0 )) =
rij
+ βdφ (xi , xj )
1
n
i0 ri0 j
n
i,j=1
= I(I ; J ) + βEI ,J dφ (xi , xj ) + const.

log rij
qj

+ const. ,

log

(6)

rij

(7)

(8)

3

where the ﬁrst term is the mutual information between the random variables I (data points) and
J (exemplars) under the distribution P 0 and the second term is the expected value of the pairwise
distances with the same distribution on indices. The n2 unknown values of rij lie on n separate
n-dimensional simplices. These parameters have the same role as cluster responsibilities in soft
k-means: they stand for the probability of data point xi choosing data point xj as its cluster-center.
The algorithm described in (4) is in fact the same as the standard Arimoto-Blahut algorithm [10]
commonly used for solving problems of the form (8).
We established that the problem of maximizing log-likelihood function (2) is equivalent to the min-
imization of objective function (8). This helps us to interpret this problem in the framework of
Rate-Distortion theory. The data set can be thought of as an information source with a uniform
distribution on the alphabet X . Such a source has entropy log n, which means that any scheme for
encoding an inﬁnitely long i.i.d. sequence generated by this source requires on average this number
of bits per symbol, i.e., has a rate of at least log n. We cannot compress the information source
beyond this rate without tolerating some distortion, when the original data points are encoded into
other points with nonzero distances between them. We can then consider rij ’s as a probabilistic
encoding of our data set onto itself with the corresponding average distortion D = EI ,J dφ (xi , xj )
and the rate I(I ; J ). A solution r∗
ij that minimizes (8) for some β yields the least rate that can be
achieved having no more than the corresponding average distortion D . This rate is usually denoted
by R(D), a function of average distortion, and is called the rate-distortion function [8]. Note that
we have ∂R/∂D = −β , 0 < β < ∞ at any point on the rate-distortion function graph. The weight
qj for the data point xj is a measure of how likely this point is to appear in the compressed repre-
sentation of the data set, i.e., to be an exemplar. Here, we can rigorously quantify our intuitive idea
that higher number of clusters (corresponding to higher rates) is the inherent cost of attaining lower
average distortion. We will see an instance of this rate-distortion trade-off in Section 5.

(9)

4 Implementation
The implementation of our algorithm costs two matrix-vector multiplications per iteration, that
is, has a complexity of order n2 per iteration, if solved with no approximations. Letting sij =
exp(−βdφ (xi , xj )) and using two auxiliary vectors z and η , we obtain the simple update rules
nX
nX
1
sij
z (t)
sij q (t)
η (t)
q (t+1)
= η (t)
j q (t)
i =
j =
j
j
j
z (t)
n
j=1
i=1
i
less than 1 otherwise [10]. In practice, we compute the gap between maxj (log ηj ) and P
where the initialization q (0)
is nonzero for all the data points we want to consider as possible exem-
j
plars. At the ﬁxed point, the values of ηj are equal to 1 for all data points in the support of qj and are
j qj log ηj
in each iteration and stop the algorithm when this gap becomes less than a small threshold. Note
that the soft assignments r(t)
ij = q (t)
j sij /nz (t)
i need to be computed only once after the algorithm has
converged.
Any value of β ∈ [0, ∞) yields a different solution to (8) with different number of nonzero qj values.
Smaller values of β correspond to having wider clusters and greater values correspond to narrower
clusters. Neither extreme, one assigning all data points to the central exemplar and the other taking
all data points as exemplars, is interesting. For reasonable ranges of β , the solution is sparse and the
resulting number of nonzero components of qj determines the ﬁnal number of clusters.
Similar to other interior-point methods, the convergence of our algorithm becomes slow as we move
close to the vertices of the probability simplex where some qj ’s are very small. In order to improve
the convergence rate, after each iteration, we identify all qj ’s that are below a certain threshold
(10−3 /n in our experiments,) set them to zero and re-normalize the entire distribution over the
remaining indices. This effectively excludes the corresponding points as possible exemplars and
reduces the cost of the following iterations.
In order to further speed up the algorithm for very large data sets, we can search over values of
sij for any i and keep only the largest no values in any row turning the proximity matrix into a
sparse one. The reasoning is simply that we expect any point to be represented in the ﬁnal solution
with exemplars relatively close to it. We observed that as long as no values are a few times greater
than the expected number of data points in each cluster, the ﬁnal results remain almost the same

4

Figure 1: Left: rate-distortion function for the example described in the text. The line with slope −βo is also
illustrated for comparison (dotted line) as well as the point corresponding to β = βo (cross) and the line tangent
to the graph at that point. Right: the exponential of rate (dotted line) and number of hard clusters for different
values of beta (solid line.) The rate is bounded above by logarithm of number of clusters.

with or without this preprocessing. However, this approximation decreases the running time of the
algorithm by a factor n/no .

5 Experimental Results
To illustrate some general properties of our method, we apply it to the set of 400 random data points
in IR2 shown in Figure 2. We use Euclidean distance and run the algorithm for different values of
β . Figure 1 (left) shows the resulting rate-distortion function for this example. As we expect, the
estimated rate-distortion function is smooth, monotonically decreasing and convex. To visualize the
clustering results, we turn the soft responsibilities into hard assignments. Here, we ﬁrst choose the
set of exemplars to be the set of all indices j that are MAP estimate exemplars for some data point
the case of Gaussian models, we chose an empirical value βo = n2 log n/ P
i under P 0 (j |xi ). Then, any point is assigned to its closest exemplar. Figure 2 illustrates the shapes
of the resulting hard clusters for different values of β . Since β has dimensions of inverse variance in
i,j kxi − xj k2 so that
values β around βo give reasonable results. We can see how clusters split when we increase β . Such
cluster splitting behavior also occurs in the case of a Gaussian mixture model with unconstrained
cluster centers and has been studied as the phase transitions of a corresponding statistical system
[9]. The nature of this connection remains to be further investigated.
The resulting number of hard clusters for different values of β are shown in Figure 1 (right). The
ﬁgure indicates two regions of β with relatively stable number of clusters, namely 4 and 10, while
other cluster numbers have a more transitory nature with varying β . The distribution of data points
in Figure 2 shows that this is a reasonable choice of number of clusters for this data set. However,
we also observe some ﬂuctuations in the number of clusters even in the more stable regime of values
of β . Comparing this behavior with the monotonicity of our rate shows how, by turning the soft
assignments into the hard ones, we lose the strong optimality guarantees we have for the original soft
solution. Nevertheless, since our global optimum is minimum to a well justiﬁed cost function, we
expect to obtain relatively good hard assignments. We further discuss this aspect of the formulation
in Section 6.
The main motivation for developing a convex formulation of clustering is to avoid the well-known
problem of local optima and sensitivity to initialization. We compare our method with a regular
mixture model of the form (1) where f (x; m) is a Gaussian distribution and the problem is solved
using the EM algorithm. We will refer to this regular mixture model as the soft k-means. The k-
means algorithm is a limiting case of this mixture-model problem when β → ∞, hence the name
soft k-means. The comparison will illustrate how employing convexity helps us better explore the
search space as the problem grows in complexity. We use synthetic data sets by drawing points from
unit variance Gaussian distributions centered around a set of vectors.
There is an important distinction between the soft k-means and our algorithm: although the results
of both algorithms depend on the choice of β , only the soft k-means needs the number of clusters k
as an input. We run the two algorithms for ﬁve different values of β which were empirically found

5

010020030040050060070080090010000123456Average DistortionRate (bits)00.511.522.5024681012β/βoFigure 2: The clusters found for different values of β , (a) 0.1βo (b) 0.5βo (c) βo (d) 1.2βo (e) 1.6βo (f)
1.7βo . The exemplar data point of each cluster is denoted by a cross. The range of normal distributions for any
mixture model is illustrated here by circles around these exemplar points with radius equal to the square root of
the variance corresponding to the value of β used by the algorithm (σ = (2β )−1/2 ). Shapes and colors denote
cluster labels.

to yield reasonable results for the problems presented here. As a measure of clustering quality, we
use micro-averaged precision. We form the contingency tables for the cluster assignments found by
the algorithm and the true cluster labels. The percentage of the total number of data points assigned
to the right cluster is taken as the precision value of the clustering result. Out of the ﬁve runs with
different values of β , we take the result with the best precision value for any of the two algorithms.
In the ﬁrst experiment, we look at the performance of the two algorithms as the number of clusters
increases. Different data sets are generated by drawing 3000 data points around some number of
cluster centers in IR20 with all clusters having the same number of data points. Each component of
any data-point vector comes from an independent Gaussian distribution with unit variance around
the value of the corresponding component of its cluster center. Further, we randomly generate
components of the cluster-center vectors from a Gaussian distribution with variance 25 around zero.
In this experiment, for any value of β , we repeat soft k-means 1000 times with random initialization
and pick the solution with the highest likelihood value. Figure 3 (left) presents the precision values as
a function of the number of clusters in the mixture distribution that generates the 3000 data points.
The error bars summarize the standard deviation of precision over 200 independently generated
data sets. We can see that performance of soft k-means drops as the number of clusters increases
while our performance remains relatively stable. Consequently, as illustrated in Figure 3 (right),

6

−40−30−20−10010203040−40−30−20−10010203040(a)−40−30−20−10010203040−40−30−20−10010203040(b)−40−30−20−10010203040−40−30−20−10010203040(c)−40−30−20−10010203040−40−30−20−10010203040(d)−40−30−20−10010203040−40−30−20−10010203040(e)−40−30−20−10010203040−40−30−20−10010203040(f)Figure 3: Left: average precision values of Convex Clustering and Soft k-means for different numbers of
clusters in 200 data sets of 3000 data points. Right: precision gain of using Convex Clustering in the same
experiment.
the average precision difference of the two algorithms increases with increasing number of clusters.
Since the total number of data points remains the same, increasing the number of clusters results in
increasing complexity of the problem with presumably more local minima to the cost function. This
trend agrees with our expectation that the results of the convex algorithm improves relative to the
original one with a larger search space.
As another way of exploring the complexity of the problem, in our second experiment, we generate
data sets with different dimensionality. We draw 100 random vectors, with unit variance Gaussian
√
distribution in each component, around any of the 40 cluster centers to make data sets of total 4000
data points. The cluster centers are chosen to be of the form (0, · · · , 0,
50, 0, · · · , 0) where we
change the position of the nonzero component to make different cluster centers. In this way, the
pairwise distance between all cluster centers is 50 by formation.
Figure 4 (left) presents the precision values found for the two algorithms when 4000 points lie in
spaces with different dimensionality. Soft k-means was repeated 100 times with random initializa-
tion for any value of β . Again, the relative performance of Convex Clustering when compared to
soft k-means improves with the increasing problem complexity. This is another evidence that for
larger data sets the less precise nature of our constrained search, as compared to the full mixture
models, is well compensated by its ability to always ﬁnd its global optimum. In general the value
of β should be tuned to ﬁnd the desired solution. We plan to develop a more systematic way for
choosing β .

6 Discussion and Related Work
Since only the distances take part in our formulation and the values of data point vectors are not
required, we can extend this method to any proximity data. Given a matrix Dn×n = [dij ] that
describes the pairwise symmetric or asymmetric dissimilarities between data points, we can replace
dφ (xi , xj )’s in (8) with dij ’s and solve the same minimization problem whose convexity can be
directly veriﬁed. The algorithm works in exactly the same way and all the aforementioned properties
carry over to this case as well.
A previous application of rate-distortion theoretic ideas in clustering led to the deterministic anneal-
ing (DA). In order to avoid local optima, DA gradually decreases an annealing parameter, tightening
the bound on the average distortion [9]. However, at each temperature the same standard EM updates
are used. Consequently, the method does not provide strong guarantees on the global optimality of
the resulting solution.
Afﬁnity propagation is another recent exemplar-based clustering algorithm. It ﬁnds the exemplars
by forming a factor graph and running a message passing algorithm on the graph as a way to mini-
agation can be stated as P
mize the clustering cost function [6]. If the data point i is represented by the data point ci , assuming
a common preference parameter value λ for all data points, the objective function of afﬁnity prop-
i dici + λk where k is the number of found clusters. The second term
is needed to put some cost on picking any point as an exemplar to prevent the trivial case of send-
ing any point to itself. Outstanding results have been reported for the afﬁnity propagation [6] but
theoretical guarantees on its convergence or optimality are yet to be established.

7

 5681012152025307580859095100105Number of ClustersAverage Precision Convex Clustering Soft k‐means  568101215202530-50510152025Number of ClustersAverage Precision GainFigure 4: Left: average precision values of Convex Clustering and Soft k-means for different data dimension-
ality in 100 data sets of 4000 data points with 40 clusters. Right: precision gain of using Convex Clustering in
the same experiment.
P
We can interpret our algorithm as a relaxation of this combinatorial problem to the soft assignment
case by introducing probabilities P(ci = j ) = rij of associating point i with an exemplar j . The
marginal distribution qj = 1
i rij is the probability that point j is an exemplar. In order to use
n
analytical tools for solving this problem, we have to turn the regularization term k into a continuous
function of assignments. A possible choice might be H(q), entropy of distribution qj , which is
bounded above by log k . However, the entropy function is concave and any local or global minimum
of a concave minimization problem over a simplex occurs in an extreme point of the feasible domain
which in our case corresponds to the original combinatorial hard assignments [11]. In contrast, using
mutual information I(I , J ) induced by rij as the regularizing term turns the problem into a convex
problem. Mutual information is convex and serves as a lower bound on H(q) since it is always less
than the entropy of both of its random variables. Now, by letting λ = 1/β we arrive to our cost
function in (8). We can therefore see that our formulation is a convex relaxation of the original
combinatorial problem.
In conclusion, we proposed a framework for constraining the search space of general mixture models
to achieve global optimality of the solution.
In particular, our method promises to be useful in
problems with large data sets where regular mixture models fail to yield consistent results due to
their sensitivity to initialization. We also plan to further investigate generalization of this idea to the
models with more elaborate parameterizations.
Acknowledgements. This research was supported in part by the NIH NIBIB NAMIC U54-
EB005149, NCRR NAC P41-RR13218 grants and by the NSF CAREER grant 0642971.
References
[1] J. Puzicha, T. Hofmann, and J. M. Buhmann, “Theory of proximity based clustering: Structure detection
by optimization,” Pattern Recognition, Vol. 33, No. 4, pp. 617–634, 2000.
[2] A. Y. Ng, M. I. Jordan, and Y. Weiss, “On Spectral Clustering: Analysis and an Algorithml,” Advances in
Neural Information Processing Systems, Vol. 14, pp. 849–856, 2001.
[3] I. Csisz ´ar and P. Shields, “Information Theory and Statistics: A Tutorial,” Foundations and Trends in
Communications and Information Theory, Vol. 1, No. 4, pp. 417–528, 2004.
[4] M. Meil ˘a, and D. Heckerman, “An Experimental Comparison of Model-Based Clustering Methods,” Ma-
chine Learning, Vol. 42, No. 1-2, pp. 9–29, 2001.
[5] J. Han, and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, 2001.
[6] B. J. Frey, and D. Dueck, “Clustering by Passing Messages Between Data Points,” Science, Vol. 315, No.
5814, pp. 972–976, 2007.
[7] A. Banerjee, S. Merugu, I. S.Dhillon, and J. Ghosh, “Clustering with Bregman Divergences,” Journal of
Machine Learning Research, Vol. 6, No. 6, pp. 1705-1749, 2005.
[8] T. M. Cover, and J. A. Thomas, Elements of information theory, New York, Wiley, 1991.
[9] K. Rose, “Deterministic Annealing for Clustering, Compression, Classiﬁcation, Regression, and Related
Optimization Problems,” Proceedings of the IEEE, Vol. 86, No. 11, pp. 2210–2239, 1998.
[10] R. E. .Blahut, “Computation of Channel Capacity and Rate-Distortion Functions,” IEEE Transactions on
Information Theory, Vol. IT-18, No. 4, pp. 460–473, 1974.
[11] M. Pardalos, and J. B. Rosen, “Methods for Global Concave Minimization: A Bibliographic Survey,”
SIAM Review, Vol. 28, No. 3., pp. 367–379, 1986.

8

5075100125150859095100Number of DimensionsAverage Precision Convex Clustering Soft k‐means  507510012515081012141618Number of DimensionsAverage Precision Gain
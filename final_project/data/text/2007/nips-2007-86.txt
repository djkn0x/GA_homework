Learning Monotonic Transformations for
Classi(cid:12)cation

Andrew G. Howard
Department of Computer Science
Columbia University
New York, NY 10027
ahoward@cs.columbia.edu

Tony Jebara
Department of Computer Science
Columbia University
New York, NY 10027
jebara@cs.columbia.edu

Abstract

A discriminative method is proposed for learning monotonic transforma-
tions of the training data while jointly estimating a large-margin classi(cid:12)er.
In many domains such as document classi(cid:12)cation, image histogram classi(cid:12)-
cation and gene microarray experiments, (cid:12)xed monotonic transformations
can be useful as a preprocessing step. However, most classi(cid:12)ers only explore
these transformations through manual trial and error or via prior domain
knowledge. The proposed method learns monotonic transformations auto-
matically while training a large-margin classi(cid:12)er without any prior knowl-
edge of the domain. A monotonic piecewise linear function is learned which
transforms data for subsequent processing by a linear hyperplane classi(cid:12)er.
Two algorithmic implementations of the method are formalized. The (cid:12)rst
solves a convergent alternating sequence of quadratic and linear programs
until it obtains a locally optimal solution. An improved algorithm is then
derived using a convex semide(cid:12)nite relaxation that overcomes initializa-
tion issues in the greedy optimization problem. The e(cid:11)ectiveness of these
learned transformations on synthetic problems, text data and image data
is demonstrated.

1

Introduction

Many (cid:12)elds have developed heuristic methods for preprocessing data to improve perfor-
mance. This often takes the form of applying a monotonic transformation prior to using
a classi(cid:12)cation algorithm. For example, when the bag of words representation is used in
document classi(cid:12)cation, it is common to take the square root of the term frequency [6, 5].
Monotonic transforms are also used when classifying image histograms. In [3], transforma-
tions of the form xa where 0 (cid:20) a (cid:20) 1 are demonstrated to improve performance. When
classifying genes from various microarray experiments it is common to take the logarithm of
the gene expression ratio [2]. Monotonic transformations can also capture crucial properties
of the data such as threshold and saturation e(cid:11)ects.

In this paper, we propose to simultaneously learn a hyperplane classi(cid:12)er and a monotonic
transformation. The solution produced by our algorithm is a piecewise linear monotonic
function and a maximum margin hyperplane classi(cid:12)er similar to a support vector machine
(SVM) [4]. By allowing for a richer class of transforms learned at training time (as opposed
to a rule of thumb applied during preprocessing), we improve classi(cid:12)cation accuracy. The
learned transform is speci(cid:12)cally tuned to the classi(cid:12)cation task. The main contributions
of this paper include, a novel framework for estimating a monotonic transformation and
a hyperplane classi(cid:12)er simultaneously at training time, an e(cid:14)cient method for (cid:12)nding a

,1nx

,2nx

,n Dx

1w

2w

Dw

ny

b

Figure 1: Monotonic transform applied to each dimension followed by a hyperplane classi(cid:12)er.

locally optimal solution to the problem, and a convex relaxation to (cid:12)nd a globally optimal
approximate solution.

The paper is organized as follows. In section 2, we present our formulation for learning a
piecewise linear monotonic function and a hyperplane. We show how to learn this combined
model through an iterative coordinate ascent optimization using interleaved quadratic and
linear programs to (cid:12)nd a local minimum. In section 3, we derive a convex relaxation based
on Lasserreâ€™s method [8]. In section 4 synthetic experiments as well as document and image
classi(cid:12)cation problems demonstrate the diverse utility of our method. We conclude with a
discussion and future work.

2 Learning Monotonic Transformations

For an unknown distribution P (~x; y) over inputs ~x 2 <d and labels y 2 f(cid:0)1; 1g, we assume
that there is an unknown nuisance monotonic transformation (cid:8)(x) and unknown hyperplane
parameterized by ~w and b such that predicting with f (x) = sign( ~w T (cid:8)(~x) + b) yields a low
expected test error R = R 1
2 jy (cid:0) f (x)jdP (~x; y). We would like to recover (cid:8)(~x); ~w ; b from a
labeled training set S = f(~x1 ; y1 ); : : : ; (~xN ; yN )g which is sampled i.i.d. from P (~x; y). The
transformation acts elementwise as can be seen in Figure 1.
We propose to learn both a maximum margin hyperplane and the unknown transform (cid:8)(x)
simultaneously. In our formulation, (cid:8)(x) is a piecewise linear function that we parameterize
with a set of K knots fz1 ; : : : ; zK g and associated positive weights fm1 ; : : : ; mK g where
zj 2 < and mj 2 <+ . The transformation can be written as (cid:8)(x) = PK
j=1 mj (cid:30)j (x) where
(cid:30)j (x) are truncated ramp functions acting on vectors and matrices elementwise as follows:
(cid:30)j (x) = 8<
x (cid:20) zj
0
zj < x < zj+1
zj+1 (cid:20) x
:
This is a less common way to parameterize piecewise linear functions. The positivity con-
straints enforce monotonicity on (cid:8)(x) for all x. A more common method is to parameterize
the function value (cid:8)(z ) at each knot z and apply order constraints between subsequent knots
to enforce monotonicity. Values in between knots are found through linear interpolation.
This is the method used in isotonic regression [10], but in practice, these are equivalent
formulations. Using truncated ramp functions is preferable for numerous reasons. They can
be easily precomputed and are sparse. Once precomputed, most calculations can be done
via sparse matrix multiplications. The positivity constraints on the weights ~m will also yield
a simpler formulation than order constraints and interpolation which becomes important in
subsequent relaxation steps.

x(cid:0)zj
zj+1(cid:0)zj
1

(1)

Figure 2a shows the truncated ramp function associated with knot z1 . Figure 2b shows
a conic combination of truncated ramps that builds a piecewise linear monotonic function.
Combining this with the support vector machine formulation leads us to the following learn-
ing problem:

1

0.8

0.6

0.4

0.2

0

m1+m2+m3+m4+m5

m1+m2+m3+m4

m1+m2+m3

m1+m2

z2

z5

a) Truncated ramp function (cid:30)1 (x).

z1

m1
z1
z2
z3
b) (cid:8)(x) = P5
j=1 mj (cid:30)j (x).
Figure 2: Building blocks for piecewise linear functions.

z4

sub ject to

min
~w ;~(cid:24) ;b; ~m

N
Xi=1
k ~wk2
2 + C
(cid:24)i
yi 0
mj (cid:30)j ( ~xi )+ + b1
* ~w ;
K
Xj=1
A (cid:21) 1 (cid:0) (cid:24)i 8i
@
(cid:24)i (cid:21) 0; mj (cid:21) 0; Xj
mj (cid:20) 1 8i; j
where ~(cid:24) are the standard SVM slack variables, ~w and b are the maximum margin solution
for the training set that has been transformed via (cid:8)(x) with learned weights ~m. Before
training, the knot locations are chosen at the empirical quantiles so that they are evenly
spaced in the data.

(2)

This problem is nonconvex due to the quadratic term involving ~w and ~m in the classi(cid:12)cation
constraints. Although it is di(cid:14)cult to (cid:12)nd a globally optimal solution, the structure of the
problem suggests a simple method for (cid:12)nding a locally optimal solution. We can divide the
problem into two convex subproblems. This amounts to solving a support vector machine
for ~w and b with a (cid:12)xed (cid:8)(x) and alternatively solving for (cid:8)(x) as a linear program with
the SVM solution (cid:12)xed. In both subproblems, we optimize over ~(cid:24) as it is part of the hinge
loss. This yields an e(cid:14)cient convergent optimization method. However, this method can
get stuck in local minima. In practice, we initialize it with a linear (cid:8)(x) and iterate from
there. Alternative initializations do not yield much help. This leads us to look for a method
to e(cid:14)ciently (cid:12)nd global solutions.

3 Convex Relaxation

When faced with a nonconvex quadratic problem, an increasingly popular technique is to
relax it into a convex one. Lasserre [8] proposed a sequence of convex relaxations for
these types of nonconvex quadratic programs. This method replaces all quadratic terms
in the original optimization problem with entries in a matrix.
In its simplest form this
matrix corresponds to the outer product of the the original variables with rank one and
semide(cid:12)nite constraints. The relaxation comes from dropping the rank one constraint on
the outer product matrix. Lasserre proposed more elaborate relaxations using higher order
moments of the variables. However, we mainly use the (cid:12)rst moment relaxation along with
a few of the second order moment constraints that do not require any additional variables
beyond the outer product matrix.

A convex relaxation could be derived directly from the primal formulation of our problem.
Both ~w and ~m would be relaxed as they interact in the nonconvex quadratic terms. Un-

fortunately, this yields a semide(cid:12)nite constraint that scales with both the number of knots
and the dimensionality of the data. This is troublesome because we wish to work with high
dimensional data such as a bag of words representation for text. However, if we (cid:12)rst (cid:12)nd
the dual formulation for ~w , b, and ~(cid:24) , we only have to relax ~m which yields both a tighter
relaxation and a less computationally intensive problem. Finding the dual leaves us with the
following min max saddle point problem that will be subsequently relaxed and transformed
into a semide(cid:12)nite program:

min
~m

(3)

max
~(cid:11)

2~(cid:11)T ~1 (cid:0) ~(cid:11)T 0
@Y 0
mimj (cid:30)i (X )T (cid:30)j (X )1
A Y 1
@Xi;j
A ~(cid:11)
0 (cid:20) (cid:11)i (cid:20) C; ~(cid:11)T ~y = 0; mj (cid:21) 0; Xj
mj (cid:20) 1 8i; j
where ~1 is a vector of ones, ~y is a vector of the labels, Y = diag(~y) is a matrix with the
labels on its diagonal with zeros elsewhere, and X is a matrix with ~xi in the ith column.
We introduce the relaxation via the substitution M = (cid:22)m (cid:22)mT and constraint M (cid:23) 0 where
(cid:22)m is constructed by concatenating 1 with ~m. We can then transform the relaxed min max
problem into a semide(cid:12)nite program similar to the multiple kernel learning framework [7]
by (cid:12)nding the dual with respect to ~(cid:11) and using the Schur complement lemma to generate
a linear matrix inequality [1]:

min
M ;t;(cid:21);~(cid:23) ;~(cid:14)

sub ject to

t
  Y Pi;j Mi;j (cid:30)i (X )T (cid:30)j (X )Y ~1 + ~(cid:23) (cid:0) ~(cid:14) + (cid:21)~y
(~1 + ~(cid:23) (cid:0) ~(cid:14) + (cid:21)~y )T
t (cid:0) 2C~(cid:14)T ~1
M (cid:23) 0; M (cid:21) 0; M (cid:22)1 (cid:20) ~0; M0;0 = 1; ~(cid:23) (cid:21) ~0; ~(cid:14) (cid:21) ~0

! (cid:23) 0

(4)

where ~0 is a vector of zeros and (cid:22)1 is a vector with (cid:0)1 in the (cid:12)rst dimension and ones in the
rest. The variables (cid:21), ~(cid:23) , ~(cid:14) arise from the dual transformation. This relaxation is exact if M
is a rank one matrix.

The above can be seen as a generalization of the multiple kernel learning framework. Instead
of learning a kernel from a combination of kernels, we are learning a combination of inner
products of di(cid:11)erent functions applied to our data. In our case, these are truncated ramp
functions. The terms (cid:30)i (X )T (cid:30)j (X ) are not Mercer kernels except when i = j . This more
general combination requires the stricter constraints that the mixing weights M form a
positive semide(cid:12)nite matrix, a constraint which is introduced via the relaxation. This is
a su(cid:14)cient condition for the resulting matrix Pi;j Mi;j (cid:30)i (X )T (cid:30)j (X ) to also be positive
semide(cid:12)nite.
When using this relaxation, we can recover the monotonic transform by using the (cid:12)rst
column (row) as the mixing weights, ~m, of the truncated ramp functions.
In practice,
however, we use the learned kernel in our predictions k(~x; ~x 0 ) = Pi;j Mi;j (cid:30)i (~x)T (cid:30)j (~x0 ).
4 Experiments

4.1 Synthetic Experiment

In this experiment we will demonstrate our methodâ€™s ability to recover a monotonic trans-
formation from data. We sampled data near a linear decision boundary and generated labels
based on this boundary. We then applied a strictly monotonic function to this sampled data.
The training set is made up of the transformed points and the original labels. A linear al-
gorithm will have di(cid:14)culty because the mapped data is not linearly separable. However,

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

a)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

d)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

b)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

e)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

c)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

f )

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

g)

h)

i)

Figure 3: a) Original data. b) Data transformed by a logarithm. c) Data transformed
by a quadratic function. d-f ) The transformation functions learned using the nonconvex
algorithm. g-i) The transformation functions learned using the convex algorithm.

if we could recover the inverse monotonic function, then a linear decision boundary would
perform well.

Figure 3a shows the original data and decision boundary. Figure 3b shows the data and
hyperplane transformed with a normalized logarithm. Figure 3c depicts a quadratic trans-
form. 600 data points were sampled, and then transformed. 200 were used for training, 200
for cross validation and 200 for testing. We compared our locally optimal method (L mono),
our convex relaxation (C mono) and a linear SVM (linear). The linear SVM struggled on
all of the transformed data while the other methods performed well as reported in Figure 4.
The learned transforms for L mono are plotted in Figure 3(d-f ). The solid blue line is the
mean over 10 experiments, and the dashed blue is the standard deviation. The black line
is the true target function. The learned functions for C mono are in Figure 3(g-i). Both
algorithms performed quite well on the task of classi(cid:12)cation and recover nearly the exact
monotonic transform. The local method outperformed the relaxation slightly because this
was an easy problem with few local minima.

4.2 Document Classi(cid:12)cation

In this experiment we used the four universities WebKB dataset. The data is made up of
web pages from four universities plus an additional larger set from miscellaneous universities.

Linear
L Mono
C Mono

linear
0.0005
0.0020
0.0025

exponential
0.0375
0.0005
0.0075

square root
0.0685
0.0020
0.0025

total
0.0355
0.0015
0.0042

Figure 4: Testing error rates for the synthetic experiments.

Linear
TFIDF
Sqrt
Poly
RBF
L Mono
C Mono

1 vs 2
0.0509
0.0428
0.0363
0.0499
0.0514
0.0338
0.0322

1 vs 3
0.0879
0.0891
0.0667
0.0861
0.0836
0.0739
0.0776

1 vs 4
0.1381
0.1623
0.0996
0.1389
0.1356
0.0854
0.0812

2 vs 3
0.0653
0.0486
0.0456
0.0599
0.0641
0.0511
0.0501

2 vs 4
0.1755
0.1910
0.1153
0.1750
0.1755
0.1060
0.0973

3 vs 4
0.0941
0.1096
0.0674
0.0950
0.0981
0.0602
0.0584

total
0.1025
0.1059
0.0711
0.1009
0.1024
0.0683
0.0657

Figure 5: Testing error rates for WebKB.

These web pages are then categorized. We will be working with the largest four categories:
student, faculty, course, and pro ject. The task is to solve all six pairwise classi(cid:12)cation
problems. In [6, 5] preprocessing the data with a square root was demonstrated to yield
good results. We will compare our nonconvex method (L mono), and our convex relaxation
(C mono) to a linear SVM with and without the square root, with TFIDF features and also
a kernelized SVM with both the polynomial kernel and the RBF kernel. We will follow the
setup of [6] by training on three universities and the miscellaneous university set and testing
on web pages from the fourth university. We repeated this four fold experiment (cid:12)ve times.
For each fold, we use a subset of 200 points for training, 200 to cross validate the parameter
settings, and all of the fourth universityâ€™s points for testing.

Our two methods outperform the competition on average as reported in Figure 5. The
convex relaxation chooses a step function nearly every time. This outputs a 1 if a word is
in the training vector and 0 if it is absent. The nonconvex greedy algorithm does not end
up recovering this solution as reliably and seems to get stuck in local minima. This leads to
slightly worse performance than the convex version.

4.3 Image Histogram Classi(cid:12)cation

In this experiment, we used the Corel image dataset. In [3], it was shown that monotonic
transforms of the form xa for 0 (cid:20) a (cid:20) 1 worked well. The Corel image dataset is made up
of various categories, each containing 100 images. We chose four categories of animals: 1)
eagles, 2) elephants, 3) horses, and 4) tigers. Images were transformed into RGB histograms
following the binning strategy of [3, 5]. We ran a series of six pairwise experiments where the
data was randomly split into 80 percent training, 10 percent cross validation, and 10 percent
testing. These six experiments were repeated 10 times. We compared our two methods to
a linear support vector machine, as well as an SVM with RBF and polynomial kernels. We
also compared to the set of transforms xa for 0 (cid:20) a (cid:20) 1 where we cross validated over
a = f0; :125; :25; :5; :625; :75; :875; 1g. This set includes linear a = 1 at one end, a binary
threshold a = 0 at the other (choosing 00 = 0), and the square root transform in the middle.

The convex relaxation performed best or tied for best on 4 out 6 of the experiments and
was the best overall as reported in Figure 6. The nonconvex version also performed well
but ended up with a lower accuracy than the cross validated family of xa transforms. The
key to this dataset is that most of the data is very close to zero due to few pixels being in a
given bin. Cross validation over xa most often chose low nonzero a values. Our method had
many knots in these extremely low values because that was where the data support was.
Plots of our learned functions on these small values can be found in Figure 7(a-f ). Solid
blue is the mean for the nonconvex algorithm and dashed blue is the standard deviation.
Similarly, the convex relaxation is in red.

Linear
Sqrt
Poly
RBF
xa
L Mono
C Mono

1 vs 2
0.08
0.03
0.07
0.06
0.08
0.05
0.04

1 vs 3
0.10
0.05
0.10
0.08
0.04
0.06
0.03

1 vs 4
0.28
0.09
0.28
0.22
0.03
0.04
0.03

2 vs 3
0.11
0.12
0.11
0.10
0.03
0.05
0.04

2 vs 4
0.14
0.08
0.15
0.13
0.09
0.13
0.06

3 vs 4
0.26
0.20
0.23
0.23
0.06
0.05
0.05

total
0.1617
0.0950
0.1567
0.1367
0.0550
0.0633
0.0417

Figure 6: Testing error rates on Corel dataset.

1

0.8

0.6

0.4

0.2

0

âˆ’0.2

0

1

0.8

0.6

0.4

0.2

0

âˆ’0.2

0

0.5

1

1.5

2
x 10âˆ’3

0.5

1

1.5

2
x 10âˆ’3

1

0.8

0.6

0.4

0.2

0

âˆ’0.2

0

1

0.8

0.6

0.4

0.2

0

âˆ’0.2

0

0.5

1

1.5

2
x 10âˆ’3

0.5

1

1.5

2
x 10âˆ’3

1

0.8

0.6

0.4

0.2

0

âˆ’0.2

0

1

0.8

0.6

0.4

0.2

0

âˆ’0.2

0

0.5

1

1.5

2
x 10âˆ’3

0.5

1

1.5

2
x 10âˆ’3

Figure 7: The learned transformation functions for 6 Corel problems.

4.4 Gender classi(cid:12)cation

In this experiment we try to di(cid:11)erentiate between images of males and females. We have
1755 labelled images from the FERET dataset processed as in [9]. Each processed image
is a 21 by 12 pixel 256 color gray scale image that is rastorized to form training vectors.
There are 1044 male images and 711 female images. We randomly split the data into 80
percent training, 10 percent cross validation, and and 10 percent testing. We then compare
a linear SVM to our two methods on 5 random splits of the data. The learned monotonic
function from L Mono and C Mono are similar to a sigmoid function which indicates that
useful saturation and threshold e(cid:11)ects where uncovered by our methods. Figure 8a shows
examples of training images before and after they have been transformed by our learned
function. Figure 8b summarizes the results. Our learned transformation outperforms the
linear SVM with the convex relaxation performing best.

5 Discussion

A data driven framework was presented for jointly learning monotonic transformations of
input data and a discriminative linear classi(cid:12)er. The joint optimization improves classi(cid:12)-
cation accuracy and produces interesting transformations that otherwise would require a
priori domain knowledge. Two implementations were discussed. The (cid:12)rst is a fast greedy
algorithm for (cid:12)nding a locally optimal solution. Subsequently, a semide(cid:12)nite relaxation of
the original problem was presented which does not su(cid:11)er from local minima. The greedy
algorithm has similar scaling properties as a support vector machine yet has local minima
to contend with. The semide(cid:12)nite relaxation is more computationally intensive yet ensures
a reliable global solution. Nevertheless, both implementations were helpful in synthetic and
real experiments including text and image classi(cid:12)cation and improved over standard support
vector machine tools.

Algorithm Error
.0909
Linear
.0818
L Mono
C Mono
.0648

a)

b)

Figure 8: a) Original and transformed gender images. b) Error rates for gender classi(cid:12)cation.

A natural next step is to explore faster (convex) algorithms that take advantage of the
speci(cid:12)c structure of the problem. These faster algorithms will help us explore extensions
such as learning transformations across multiple tasks. We also hope to explore applications
to other domains such as gene expression data to re(cid:12)ne the current logarithmic transforms
necessary to compensate for well-known saturation e(cid:11)ects in expression level measurements.
We are also interested in looking at fMRI and audio data where monotonic transformations
are useful.

6 Acknowledgements

This work was supported in part by NSF Award IIS-0347499 and ONR Award
N000140710507.

References

[1] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,
2004.
[2] M. Brown, W. Grundy, D. Lin, N. Christianini, C. Sugnet, M. Jr, and D. Haussler.
Support vector machine classi(cid:12)cation of microarray gene expression data, 1999.
[3] O. Chapelle, P. Hafner, and V.N. Vapnik. Support vector machines for histogram-based
classi(cid:12)cation. Neural Networks, IEEE Transactions on, 10:1055{1064, 1999.
[4] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273{297,
1995.
[5] M. Hein and O. Bousquet. Hilbertian metrics and positive de(cid:12)nite kernels on probability
measures. In Proceedings of Arti(cid:12)cial Intel ligence and Statistics, 2005.
[6] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. Journal of Machine
Learning Research, 5:819{844, 2004.
[7] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the
kernel matrix with semide(cid:12)nite programming. Journal of Machine Learning Research,
5:27{72, 2004.
[8] J.B. Lasserre. Convergent LMI relaxations for nonconvex quadratic programs.
Proceedings of 39th IEEE Conference on Decision and Control, 2000.
[9] B. Moghaddam and M.H. Yang. Sex with support vector machines. In Todd K. Leen,
Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Pro-
cessing 13, pages 960{966. MIT Press, 2000.
[10] T. Robertson, F.T. Wright, and R.L. Dykstra. Order Restricted Statistical Inference.
Wiley, 1988.

In


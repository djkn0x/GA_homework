Regulator Discovery from Gene Expression Time
Series of Malaria Parasites: a Hierarchical Approach

Jos ´e Miguel Hern ´andez-Lobato
Escuela Polit ´ecnica Superior
Universidad Aut ´onoma de Madrid, Madrid, Spain
Josemiguel.hernandez@uam.es

Tjeerd Dijkstra
Leiden Malaria Research Group
LUMC, Leiden, The Netherlands
t.dijkstra@lumc.nl

Tom Heskes
Institute for Computing and Information Sciences
Radboud University Nijmegen, Nijmegen, The Netherlands
t.heskes@science.ru.nl

Abstract

We introduce a hierarchical Bayesian model for the discovery of putative regula-
tors from gene expression data only. The hierarchy incorporates the knowledge
that there are just a few regulators that by themselves only regulate a handful
of genes. This is implemented through a so-called spike-and-slab prior, a mix-
ture of Gaussians with different widths, with mixing weights from a hierarchical
Bernoulli model. For efﬁcient inference we implemented expectation propaga-
tion. Running the model on a malaria parasite data set, we found four genes with
signiﬁcant homology to transcription factors in an amoebe, one RNA regulator
and three genes of unknown function (out of the top ten genes considered).

1 Introduction

Bioinformatics provides a rich source for the application of techniques from machine learning. Es-
pecially the elucidation of regulatory networks underlying gene expression has lead to a cornucopia
of approaches: see [1] for review. Here we focus on one aspect of network elucidation, the identi-
ﬁcation of the regulators of the causative agent of severe malaria, Plasmodium falciparum. Several
properties of the parasite necessitate a tailored algorithm for regulator identiﬁcation:
• In most species gene regulation takes place at the ﬁrst stage of gene expression when a
DNA template is transcribed into mRNA. This transcriptional control is mediated by spe-
ciﬁc transcription factors. Few speciﬁc transcription factors have been identiﬁed in Plas-
modium based on sequence homology with other species [2, 3]. This could be due to
Plasmodium possessing a unique set of transcription factors or due to other mechanisms of
gene regulation, e.g. at the level of mRNA stability or post-transcritional regulation.
• Compared with yeast, gene expression in Plasmodium is hardly changed by perturbations
e.g. by adding chemicals or changing temperature [4]. The biological interpretation of this
ﬁnding is that the parasite is so narrowly adapted to its environment inside a red blood cell
that it follows a stereotyped gene expression program. From a machine learning point of
view, this ﬁnding means that network elucidation techniques relying on perturbations of
gene expression cannot be used.
• Similar to yeast [5], data for three different strains of the parasite with time series of gene
expression are publicly available [6]. These assay all of Plasmodium’s 5,600 genes for
about 50 time points. In contrast to yeast, there are no ChIP-chip data available and fewer
then ten transcription factor binding motifs are known.

1

Together, these properties point to a vector autoregressive model making use of the gene expression
time series. The model should not rely on sequence homology information but it should be ﬂexible
enough to integrate sequence information in the future. This points to a Bayesian model as favored
approach.

2 The model

(1)

z (t),

p(t) − 1
τz

We start with a semi-realistic model of transcription based on Michaelis-Menten kinetics [1] and
subsequently simplify to obtain a linear model. Denoting the concentration of a certain mRNA
transcript at time t by z (t) we write:
V1 a1 (t)M1
· · · VN aN (t)MN
dz (t)
dt =
K1 + a1 (t)M1
KN + aN (t)MN
with aj (t) the concentration of the j-th activator (positive regulator), p(t) the concentration of RNA
polymerase and Vj , Kj , Mj and τz reaction constants. N denotes the number of potential activators.
The activator is thought to bind to DNA motifs upstream of the transcription start site and binds RNA
polymerase which reads the DNA template to produce an mRNA transcript. Mj can be thought of
as the multiplicity of the motif, τz captures the characteristic life time of the transcript. While
reasonably realistic, this equation harbors too many unknowns for reliable inference: 3N + 1 with
N ≈ 1000. We proceed with several simpliﬁcations:
• aj (t) (cid:3) Kj : activator concentration is low;
• p(t) = p0 is constant;
dt ≈ z(t+Δ)−z(t)
• dz(t)
with Δ the sampling period;
Δ
• Δ ≈ τz : sampling period roughly equal to transcript life time.
Counting time in units of Δ and taking logarithms on both sides, Equation (1) then simpliﬁes to
log z (t + 1) = C + M1 log a1 (t) + · · · + MN log aN (t),
with C = log(T V1 · · · VN p0/(K1 · · · KN )). This is a linear model for gene expression level given
the expression levels of a set of activators. With a similar derivation one can include repressors [1].

2.1 A Bayesian model for sparse linear regression
Let y be a vector with the log expression of the target gene and X = (x1 , . . . , xN ) a matrix whose
columns contain the log expression of the candidate regulators. Assuming that the measurements
are corrupted with additive Gaussian noise, we get y ∼ N (Xβ , σ2 I) where β = (β1 , . . . , βN )T
is a vector of regression coefﬁcients and σ2 is the variance of the noise. Such a linear model is
commonly used [7, 8, 9]. Both y and x1 , . . . , xN are mean-centered vectors with T measurements.
We specify an inverse gamma (IG) prior for σ2 so that P (σ2 ) = IG(σ2 , ν /2, ν λ/2), where λ is a
prior estimate of σ2 and ν is the sample size associated with that estimate. We assume that a priori
all components βi are independent and take a so-called “spike and slab prior” [10] for each of them.
That is, we introduce binary latent variables γi , with γi = 1 if xi takes part in the regression of y
N(cid:2)
N(cid:2)
and γi = 0 otherwise. Given γ , the prior on β then reads
P (β|γ ) =
P (βi |γi ) =
N (βi , 0, v1)γi N (βi , 0, v0 )1−γi ,
i=1
i=1
where N (x, μ, σ2 ) denotes a Gaussian density with mean μ and variance σ2 evaluated at x. In order
to enforce sparsity, the variance v1 of the slab should be larger than the variance v0 of the spike.
Instead of picking the hyperparameters v1 and v0 directly, it is convenient to pick a threshold of
practical signiﬁcance δ so that P (γi = 1) gets more weight when |βi | > δ and P (γi = 0) gets more
weight when |βi | < δ [10]. In this way, given δ and one of v1 or v0 , we pick the other one such that
log(v1/v0 )
0 − v−1
v−1
1

δ2 =

(2)

.

2

P (γ ) =

Bern(γi , w) =

Finally, we assign independent Bernoulli priors to the components of the latent vector γ :
N(cid:2)
N(cid:2)
i=1
i=1
so that each of the x1 , . . . , xN can independently take part in the regression with probability w. We
can identify the candidate genes whose expression is more likely to be correlated with the target
(cid:3)
(cid:3)
gene by means of the posterior distribution of γ :
P (γ |y, X) =
P (γ , β, σ2 |y, X) dβ dσ2 ∝
β ,σ2
β ,σ2

P (γ , β , σ2 , y|X) dβ dσ2 ,

wγi (1 − w)1−γi ,

where
(cid:6) (cid:4)
(cid:4)
P (γ , β , σ2 , y|X) = N (y, Xβ , σ2 I)P (β |γ )P (γ )P (σ2 )
T(cid:2)
N(cid:5)
N(cid:2)
(cid:7)N (βi , 0, v1)γi N (βi , 0, v0 )1−γi
N (yt ,
(cid:4)
(cid:6)
xi,t βi , σ2 )
N(cid:2)
t=1
i=1
i=1
Bern(γi , w)
i=1

IG(σ2 , ν /2, ν λ/2) .

=

(cid:8)(cid:6)

(3)

Unfortunately, this posterior distribution cannot be computed exactly if the number N of candidate
genes is larger than 25. An approximation based on Markov Chain Monte Carlo (MCMC) methods
has been proposed in [11].

2.2 A hierarchical model for gene regulation

In the section above we made use of the prior information that a target gene is typically regulated
by a small number of regulators. We have not yet made use of the prior information that a regulator
typically regulates more than one gene. We incorporate this information by a hierarchical extension
of our previous model. We introduce a vector τ of binary latent variables where τi = 1 if gene i is
⎡
⎤
a regulator and τi = 0 otherwise. The following joint distribution captures this idea:
⎣ N(cid:2)
T −1(cid:2)
N(cid:5)
⎦
xi,t βj,i , σ2
j )
⎡
⎤
i=1, i(cid:2)=j
⎣ N(cid:2)
t=1
j=1
⎦
(cid:4)
N(cid:2)
j=1
i=1

⎤
⎦
IG(σ2
j , νj /2, νj λj /2)
(cid:6)
Bern(τi , w)

P (τ , γ , β, σ2 |X) =
⎡
⎣ N(cid:2)
N(cid:2)
⎡
i=1,i(cid:2)=j
⎣ N(cid:2)
N(cid:2)
j=1
i=1,i(cid:2)=j
j=1

N (βj,i , 0, v1 )γj,i N (βj,i , 0, v0 )1−γj,i
⎤
⎦

Bern(γj,i , w1 )τi Bern(γj,i , w0 )1−τi

N (xj,t+1 ,

(4)

.

In this hierarchical model, γ is a matrix of binary latent variables where γj,i = 1 if gene i takes
part in the regression of gene j and γj,i = 0 otherwise. The relationship between regulators and
regulatees suggests that P (γj,i = 1|τi = 1) should be bigger than P (γj,i = 1|τi = 0) and thus
w1 > w0 . Matrix β contains regression coefﬁcients where βj,i is the regression coefﬁcient between
the expression of gene i and the delayed expression of gene j . Hyperparameter w represents the prior
j of the vector σ2 contain the variance
probability of any gene being a regulator and the elements σ2
of the noise in each of the N regressions. Hyperparameters λj and νj have the same meaning as in
the model for sparse linear regression. The corresponding plate model is illustrated in Figure 1.
We can identify the genes more likely to be regulators by means of the posterior distribution P (τ |X).
Compared with the sparse linear regression model we expanded the number of latent variables from
O(N ) to O(N 2 ). In order to keep inference feasible we turn to an approximate inference technique.

3

M

L

L

τE

NEJ

γE

βE



M

M

σ

NJ

6

λ

ν



Figure 1: The hierarchical
model for gene regulation.

3 Expectation propagation

The Expectation Propagation (EP) algorithm [12] allows to perform approximate Bayesian infer-
In all Bayesian problems, the joint distribution of the model parameters θ and a data set
ence.
D = {(xi , yi ) : i = 1, . . . , n} with i.i.d. elements can be expressed as a product of terms
n(cid:2)
n+1(cid:2)
P (θ , D) =
P (yi |xi , θ )P (θ ) =
ti (θ ) ,
(5)
i=1
i=1
where tn+1 (θ ) = P (θ ) is the prior distribution for θ and ti (θ ) = P (yi |xi , θ ) for i = 1, . . . , n.
ti (θ ) ≈ n+1(cid:2)
n+1(cid:2)
Expectation propagation proceeds to approximate (5) with a product of simpler terms
˜ti (θ ) = Q(θ ) ,
(6)
i=1
i=1
where all the term approximations ˜ti are restricted to belong to the same family F of exponential
distributions, but they do not have to integrate 1. Note that Q will also be in F because F is closed
(cid:2)
under multiplication. Each term approximation ˜ti is chosen so that
˜tj (θ ) = ˜ti (θ )Q\i (θ )
Q(θ ) = ˜ti (θ )
j (cid:2)=i
(cid:2)
j (cid:2)=i
in terms of the direct Kullback-Leibler (K-L) divergence. The pseudocode of the EP algorithm is:
1. Initialize the term approximations ˜ti and Q to be uniform.
2. Repeat until all ˜ti converge:
(a) Choose a ˜ti to reﬁne and remove it from Q to get Q\i (e.g. dividing Q by ˜ti ).
(b) Update the term ˜ti so that it minimizes the K-L divergence between tiQ\i and ˜tiQ\i .
(c) Re-compute Q so that Q = ˜tiQ\i .
The optimization problem in step (b) is solved by matching sufﬁcient statistics between a distribu-
tion Q(cid:3)
within the F family and tiQ\i , the new ˜ti is then equal to Q(cid:3)/Q\i . Because Q belongs to the
exponential family it is generally trivial to calculate its normalization constant. Once Q is normal-
ized it can approximate P (θ |D). Finally, EP is not guaranteed to converge, although convergence
can be improved by means of damped updates or double-loop algorithms [13].

˜tj (θ ) = ti (θ )Q\i (θ ) ,

is as close as possible to

ti (θ )

3.1 EP for sparse linear regression

The application of EP to the models of Section 2 introduces some nontrivial technicalities.
(cid:4)
(cid:6)
Furthermore, we describe several techniques to speed up the EP algorithm. We approximate
P (γ , β , σ2 , y|X) for sparse linear regression by means of a factorized exponential distribution:
N(cid:2)
P (γ , β , σ2 , y|X) ≈
Bern(γi , qi )N (βi , μi , si )
IG(σ2 , a, b) ≡ Q(γ , β , σ2 ) ,
i=1

(7)

4

where {qi , μi , si : i = 1, . . . , N }, a and b are free parameters. Note that in the approximation
(cid:13)n
Q(γ , β , σ2 ) all the components of the vectors γ and β and the variable σ2 are considered to be
independent; this allows the approximation of P (γ |y, X) by
i=1 Bern(γi , qi ). We tune the pa-
rameters of Q(γ , β , σ2 ) by means of EP over the unnormalized density P (γ , β , σ2 , y|X). Such
density appears in (3) as a product of T + N terms (not counting the priors) which correspond to the
ti terms in (5). This way, we have T + N term approximations with the same form as (7) and which
correspond to the term approximations ˜ti in (6). The complexity is O(T N ) per iteration, because
updating any of the ﬁrst T term approximations requires N operations. However, some of the EP
update operations require to compute integrals which do not have a closed form expression. To avoid
that, we employ the following simpliﬁcations when we update the ﬁrst T term approximations:
1. When updating the parameters {μi , si : i = 1, . . . , N } of the Gaussians in the term ap-
proximations, we approximate a Student’s t-distribution by means of a Gaussian distribu-
tion with the same mean and variance. This approximation becomes more accurate as the
degrees of freedom of the t-distribution increase.
2. When updating the parameters {a, b} of the IG in the term approximations, instead of
propagating the sufﬁcient statistics of an IG distribution we propagate the expectations of
1/σ2 and 1/σ4 . To achieve this, we have to perform two approximations like the one stated
above. Note that in this case we are not minimizing the direct K-L divergence. However,
at convergence, we expect the resulting IG in (7) to be sufﬁciently accurate.

In order to improve convergence, we re-update all the N last term approximations each time one
of the ﬁrst T term approximations is updated. Computational complexity does not get worse than
O(T N ) and the resulting algorithm turns out to be faster. By comparison, the MCMC method
in [11] takes O(N 2 ) steps to generate a single sample from P (γ |y, X). On problems of much
smaller size than we will consider in our experiments, one typically requires on the order of 10000
samples to obtain reasonably accurate estimates [10].

Q(τ , γ , β , σ2 ) =

⎤
⎦ ,
IG(σ2
j , aj , bj )

3.2 EP for gene regulation
We approximate P (τ , γ , β, σ2 |X) by the factorized exponential distribution
⎡
⎤
⎦ (cid:4)
(cid:6)
⎣ N(cid:2)
N(cid:2)
N(cid:2)
Bern(τi , ti )
Bern(γj,i , wj,i )
⎡
⎡
⎤
i=1,i(cid:2)=j
N(cid:2)
⎣ N(cid:2)
⎣ N(cid:2)
i=1
j=1
⎦
N (βj,i , μj,i , sj,i )
i=1,i(cid:2)=j
j=1
j=1
where {aj , bj , ti , wj,i , μj,i , sj,i : i = 1, . . . , N ; j = 1, . . . , N ; i (cid:7)= j } are free parameters. The
(cid:13)N
posterior probability P (τ |X) that indicates which genes are more likely to be regulators can then
i=1 Bern(τi , ti ). Again, we ﬁx the parameters in Q(τ , γ , β, σ2 ) by means of
be approximated by
EP over the joint density P (τ , γ , β , σ2 |X). It is trivial to adapt the EP algorithm used in the sparse
linear regression model to this new case: the terms to be approximated are the same as before except
for the new N (N − 1) terms for the prior on γ . As in the previous section and in order to improve
convergence, we re-update all the N (N − 1) term approximations corresponding to the prior on β
each time N of the N (T − 1) term approximations corresponding to regressions are updated. In
order to reduce memory requirements, we associate all the N (N − 1) terms for the prior on β into
a single term, which we can do because they are independent so that we only store in memory one
term approximation instead of N (N − 1). We also group the N (N − 1) terms for the prior on γ
into N independent terms and the N (T − 1) terms for the regressions into T − 1 independent terms.
Assuming a constant number of iterations (in our experiments, we need at most 20 iterations for EP
to converge), the computational complexity and the memory requirements of the resulting algorithm
are O(T N 2). This indicates that it is feasible to analyze data sets which contain the expression
pattern of thousands of genes. An MCMC algorithm would require O(N 3 ) to generate just a single
sample.

5

4 Experiments with artiﬁcial data

We carried out experiments with artiﬁcially generated data in order to validate the EP algorithms.
In the experiments for sparse linear regression we ﬁxed the hyperparameters in (3) so that ν = 3,
λ is the sample variance of the target vector y, v1 = 1, δ = N −1 , v0 is chosen according to
(2) and w = N −1 . In the experiment for gene regulation we ﬁxed the hyperparameters in (4) so
that w = (N − 1)−1 , νi = 3 and λi is the sample variance of the vector xi , w1 = 10−1(N −
1)−1 , w0 = 10−2 (N − 1)−1 , v1 = 1, δ = 0.2 and v0 is chosen according to (2). Although the
posterior probabilities are sensitive to some of the choices, the orderings of these probabilities, e.g.,
to determine the most likely regulators, are robust to even large changes.

4.1 Sparse linear regression
In the ﬁrst experiment we set T = 50 and generated x1 , . . . , x6000 ∼ N (0, 32 I) candidate vectors
and a target vector y = x1 − x2 + 0.5 x3 − 0.5 x4 + ε, where ε ∼ N (0, I). The EP algorithm
assigned values close to 1 to w1 and w2 , the parameters w3 and w4 obtained values 5.2 · 10−3 and
0.5 respectively and w5 , . . . , w6000 were smaller than 3 · 10−4 . We repeated the experiment several
times (each time using new data) and obtained similar results on each run.
In the second experiment we set T = 50 and generated a target vector y ∼ N (0, 32 I) and
x1 , . . . , x500 candidate vectors so that xi = y + εi for i = 2, . . . , 500, where εi ∼ N (0, I).
The candidate vector x1 is generated as x1 = y + 0.5 ε1 where ε1 ∼ N (0, I). This way, the noise
in x1 is twice as small as the noise in the other candidate vectors. Note that all the candidate vec-
tors are highly correlated with each other and with the target vector. This is what happens in gene
expression data sets where many genes show similar expression patterns. We ran the EP algorithm
100 times (each time using new data) and it always assigned to all the w1 , . . . , w500 more or less the
same value of 6 · 10−4 . However, w1 obtained the highest value on 54 of the runs and it was among
the three ws with highest value on 87 of the runs.
Finally, we repeated these experiments setting N = 100, using the MCMC method of [11] and the
EP algorithm for sparse linear regression. Both techniques produced results that are statistically
indistinguishable (the approximations obtained through EP fall within the variation of the MCMC
method), for EP within a fraction of the time of MCMC.

4.2 Gene regulation
In this experiment we set T = 50 and generated a vector z with T + 1 values from a sinusoid. We
then generated 49 more vectors x2 , ..., x50 where xi,t = zt + εi,t for i = 2, . . . , 50 and t = 1, . . . , T ,
where εi,t ∼ N (0, σ2 ) and σ is one fourth of the sample standard deviation of z. We also generated
a vector x1 so that x1,t = zt+1 + εt where t = 1, . . . , T and εt ∼ N (0, σ2 ). In this way, x1 acts as
a regulator for x2 , ..., x50 . A single realization of the vectors x1 , . . . , x50 is displayed on the left of
Figure 2. We ran the EP algorithm for gene regulation over 100 different realizations of x1 , . . . , x50 .
The algorithm assigned t1 the highest value on 33 of the runs and x1 was ranked among the top ﬁve
on 74 of the runs. This indicates that the EP algorithm can successfully detect small differences in
correlations and should be able to ﬁnd new regulators in real microarray data.

5 Experiments with real microarray data

We applied our algorithm to four data sets. The ﬁrst is a yeast cell-cycle data set from [5] which is
commonly used as a benchmark for regulator discovery. Data sets two through four are from three
different Plasmodium strains [6]. Missing values were imputed by nearest neighbors [14] and the
hyperparameters were ﬁxed at the same values as in Section 4. The yeast cdc15 data set contains
23 measurements of 6178 genes. We singled out 751 genes which met a minimum criterion for cell
cycle regulation [5]. The top ten genes with the highest values for τ along with their annotation from
the Saccharomyces Genome database are listed in table 5: the top two genes are speciﬁc transcription
factors and IOC2 is associated with transcription regulation. As 4% of the yeast genome is associated
with transcription the probability of this occurring by chance is 0.0062. However, although the result
is statistically signiﬁcant, we were disappointed to ﬁnd none of the known cell-cycle regulators (like
ACE2, FKH* or SWI*) among the top ten.

6

n
o
i
s
s
e
r
p
x
E

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
1
−

Regulator
Regulatees

Gene PF11_321
Genes positively regulated
Genes negatively regulated

3

2

1

0

1
−

2
−

n
o
i
s
s
e
r
p
x
E

0

10

20

30

40

50

0

10

20

30

40

50

Measurement
Measurement
Figure 2: Left: Plot of the vectors x2 , ..., x50 in grey and the vector x1 in black. The vector x1
contains the expression of a regulator which would determine the expressions in x2 , ..., x50 . Right:
Expressions of gene PF11 321 (black) and the 100 genes which are more likely to be regulated by it
(light and dark grey). Two clusters of positively and negatively regulated genes can be appreciated.

rank

standard
name

common
name

annotation

CHA4
1 YLR098c
2 YOR315w SFG1
3 YJL073w
JEM1
4 YOR023c
AHC1
5 YOR105w -
6 YLR095w IOC2
7 YOR321w PMT3
8 YLR231c
BNA5
9 YOR248w -
10 YOR247w SRL1

DNA binding transcriptional activator
putative transcription factor for growth of superﬁcial pseudohyphae
DNAJ-like chaperone
subunit of the ADA histone acetyl transferase complex
dubious open reading frame
transcription elongation
protein O-mannosyl transferase
kynureninase
dubious open reading frame
mannoprotein

The three data sets for the malaria parasite [6] contain 53 measurements (3D7), 50 measurements
(Dd2) and 48 measurements (HB3). We focus on 3D7 as this is the sequenced reference strain. We
singled out 751 genes who showed the highest variation as quantiﬁed by the interquartile range of the
expression measurements. The top ten genes with the highest values for τ along with their annotation
from PlasmoDB are listed in table 5. Recalling the motivation for our approach, the paucity of known
transcription factors, we cannot expect to ﬁnd many annotated regulators in PlasmoDB version 5.4.
Thus, we list the BLASTP hits provided by PlasmoDB instead of the absent annotation. These
hits were the highest scoring ones outside of the genus Plasmodium. We ﬁnd four genes with a
large identity to transcription factors in Dictyostelium (a recently sequenced social amoebe) and one
annotated helicase which typically functions in post-transcriptional regulation. Interestingly three
genes have no known function and could be regulators.

rank

standard name

annotation or selected BLASTP hits

PFC0950c
1
2
PF11 0321
3
PFI1210w
4 MAL6P1.233
5
PFD0175c
6 MAL7P1.34
7 MAL6P1.182
8
PF13 0140
9
PF13 0138
10 MAL13P1.14

25% identity to GATA binding TF in Dictyostelium
25% identity to putative WRKY TF in Dictyostelium
no BLASTP matches outside Plasmodium genus
no BLASTP matches outside Plasmodium genus
32% identity to GATA binding TF in Dictyostelium
35% identity to GATA binding TF in Dictyostelium
N-acetylglucosaminyl-phosphatidylinositol de-n-acetylase
dihydrofolate synthase/folylpolyglutamate synthase
no BLASTP matches outside Plasmodium genus
DEAD box helicase

Results for the HB3 strain were similar in that ﬁve putative regulators were found. Somewhat
disappointing, we found only one putative regulator (a helicase) among the top ten genes for Dd2.

7

6 Conclusion and discussion

Our approach enters a ﬁeld full of methods enforcing sparsity ([15, 8, 7, 16, 9]). Our main contri-
butions are: a hierarchical model to discover regulators, a tractable algorithm for fast approximate
inference in models with many interacting variables, and the application to malaria.

Arguably most related is the hierarchical model in [15]. The covariates in this model are a dozen
external variables, coding experimental conditions, instead of the hundreds of expression levels of
other genes as in our model. Furthermore, the prior in [15] enforces sparsity on the “columns” of
β to implement the idea that some genes are not inﬂuenced by any of the experimental conditions.
Our prior, on the other hand, enforces sparsity on the “rows” in order to ﬁnd regulators.

Future work could include more involved priors, e.g., enforcing sparsity on both “rows” and
“columns” or incorporating information from DNA sequence data. The approximate inference tech-
niques described in this paper make it feasible to evaluate such extensions in a fraction of the time
required by MCMC methods.

References
[1] T.S. Gardner and J.J. Faith. Reverse-engineering transcription control networks. Physics of
Life Reviews, 2:65–88, 2005.
[2] R. Coulson, N. Hall, and C. Ouzounis. Comparative genomics of transcriptional control in the
human malaria parasite Plasmodium falciparum. Genome Res., 14:1548–1554, 2004.
[3] S. Balaji, M.M. Babu, L.M. Iyer, and L. Aravind. Discovery of the principal speciﬁc transcrip-
tion factors of apicomplexa and their implication for the evolution of the ap2-integrase dna
binding domains. Nucleic Acids Research, 33(13):3994–4006, 2005.
[4] T. Sakata and E.A. Winzeler. Genomics, systems biology and drug development for infectuous
diseases. Molecular BioSystems, 3:841–848, 2007.
[5] P.T. Spellman, G. Sherlock, V.R. Iyer, K. Anders, M.B. Eisen, P.O. Brown, and D. Botstein.
Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast Saccharomyces cere-
visiae by microarray hybridization. Molecular Biology of the Cell, 9(12):3273–3297, 1998.
[6] M. LLinas, Z. Bozdech, E. D. Wong, A.T. Adai, and J. L. DeRisi. Comparative whole
genome transcriptome analysis of three Plasmodium falciparum strains. Nucleic Acids Re-
search, 34(4):1166–1173, 2006.
[7] M. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, UCL, 2003.
[8] C. Sabatti and G.M. James. Bayesian sparse hidden components analysis for transcription
regulation networks. Bioinformatics, 22(6):739–746, 2006.
[9] S.T. Jensen, G. Chen, and C.J. Stoeckert. Bayesian variable selection and data integration for
biological regulatory networks. The Annals of Applied Statistics, 1:612–633, 2007.
[10] E.I. George and R.E. McCulloch. Approaches for Bayesian variable selection. Statistica
Sinica, 7:339–374, 1997.
[11] E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. Journal of the Amer-
ican Statistical Association, 88(423):881–889, 1993.
[12] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001.
[13] T. Heskes and O. Zoeter. Expectation propagation for approximate inference in dynamic
Bayesian networks. In UAI-2002, pages 216–223, 2002.
[14] O. Troyanskaya, M. Cantor, P. Brown, T. Hastie, R. Tibshirani, and D. Botstein. Missing value
estimation methods for dna microarrays. Bioinformatics, 17(6):520–525, 2001.
[15] J. Lucas, C. Carvalho, Q. Wang, A. Bild, J. Nevins, and M. West. Sparse statistical modelling
in gene expression genomics.
In K.A. Do, P. M ¨uller, and M. Vannucci, editors, Bayesian
inference for gene expression and proteomics. Springer, 2006.
[16] M.Y. Park, T. Hastie, and R. Tibshirani. Averaged gene expressions for regression. Biostatis-
tics, 8:212–227, 2007.

8


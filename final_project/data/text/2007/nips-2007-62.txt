Kernel Measures of Conditional Dependence

Kenji Fukumizu
Institute of Statistical Mathematics
4-6-7 Minami-Azabu, Minato-ku
Tokyo 106-8569 Japan
fukumizu@ism.ac.jp

Arthur Gretton
Max-Planck Institute for Biological Cybernetics
Spemannstraße 38, 72076 T ¨ubingen, Germany
arthur.gretton@tuebingen.mpg.de

Xiaohai Sun
Max-Planck Institute for Biological Cybernetics
Spemannstraße 38, 72076 T ¨ubingen, Germany
xiaohi@tuebingen.mpg.de

Bernhard Sch ¨olkopf
Max-Planck Institute for Biological Cybernetics
Spemannstraße 38, 72076 T ¨ubingen, Germany
bernhard.schoelkopf@tuebingen.mpg.de

Abstract

We propose a new measure of conditional dependence of random variables, based
on normalized cross-covariance operators on reproducing kernel Hilbert spaces.
Unlike previous kernel dependence measures, the proposed criterion does not de-
pend on the choice of kernel in the limit of in ﬁnite data, for a wide class of ker-
nels. At the same time, it has a straightforward empirical estimate with good
convergence behaviour. We discuss the theoretical properties of the measure, and
demonstrate its application in experiments.

1 Introduction

Measuring dependence of random variables is one of the main concerns of statistical inference. A
typical example is the inference of a graphical model, which expresses the relations among variables
in terms of independence and conditional independence. Independent component analysis employs
a measure of independence as the objective function, and feature selection in supervised learning
looks for a set of features on which the response variable most depends.

Kernel methods have been successfully used for capturing (conditional) dependence of variables
[1, 5, 8, 9, 16]. With the ability to represent high order moments, mapping of variables into re-
producing kernel Hilbert spaces (RKHSs) allows us to infer properties of the distributions, such as
independence and homogeneity [7]. A drawback of previous kernel dependence measures, however,
is that their value depends not only on the distribution of the variables, but also on the kernel, in
contrast to measures such as mutual information.

In this paper, we propose to use the Hilbert-Schmidt norm of the normalized conditional cross-
covariance operator, and show that this operator encodes the dependence structure of random vari-
ables. Our criterion includes a measure of unconditional dependence as a special case. We prove in
the limit of inﬁnite data, under assumptions on the richness of the RKHS, that this measure has an
explicit integral expression which depends only on the probability densities of the variables, despite
being deﬁned in terms of kernels. We also prove that its empirical estimate converges to the kernel-
independent value as the sample size increases. Furthermore, we provide a general formulation for

1

the “richness”
of an RKHS, and a theoretically motivated kernel selection method. We successfully
apply our measure in experiments on synthetic and real data.

2 Measuring conditional dependence with kernels

The probability law of a random variable X is denoted by PX , and the space of the square integrable
functions with probability P by L2 (P ). The symbol X ??Y j Z indicates the conditional indepen-
dence of X and Y given Z . The null space and the range of an operator T are written N (T ) and
R(T ), respectively.
2.1 Dependence measures with normalized cross-covariance operators

Covariance operators on RKHSs have been successfully used for capturing dependence and condi-
tional dependence of random variables, by incorporating high order moments [5, 8, 16]. We give
a brief review here; see [5, 6, 2] for further detail. Suppose we have a random variable (X; Y ) on
X (cid:2) Y , and RKHSs HX and HY on X and Y , respectively, with measurable positive deﬁnite kernels
kX and kY . Throughout this paper, we assume the integrability
(A-1)
E [kX (X; X )] < 1; E [kY (Y ; Y )] < 1.
This assumption ensures HX (cid:26) L2 (PX ) and HY (cid:26) L2 (PY ). The cross-covariance operator (cid:6)Y X :
HX ! HY is deﬁned by the unique bounded operator that satisﬁes
(1)
( = E [f (X )g(Y )] (cid:0) E [f (X )]E [g(Y )])
hg ; (cid:6)Y X f iHY = Cov[f (X ); g(Y )]
for all f 2 HX and g 2 HY . If Y = X , (cid:6)XX is called the covariance operator, which is self-adjoint
and positive. The operator (cid:6)Y X naturally extends the covariance matrix CY X on Euclidean spaces,
and represents higher order correlations of X and Y through f (X ) and g(Y ) with nonlinear kernels.
It is known [2] that the cross-covariance operator can be decomposed into the covariance of the
marginals and the correlation; that is, there exists a unique bounded operator VY X such that
Y Y VY X (cid:6)1=2
(cid:6)Y X = (cid:6)1=2
(2)
XX ;
R(VY X ) (cid:26) R((cid:6)Y Y ), and N (VY X )? (cid:26) R((cid:6)XX ). The operator norm of VY X is less than or equal
to 1. We call VY X the normalized cross-covariance operator (NOCCO, see also [4]).
While the operator VY X encodes the same information regarding the dependence of X and Y as
(cid:6)Y X , the former rather expresses the information more directly than (cid:6)Y X , with less in ﬂuence of the
marginals. This relation can be understood as an analogue to the difference between the covariance
Cov[X; Y ] and the correlation Cov[X; Y ]=(Var(X )Var(Y ))1=2 . Note also that kernel canonical
correlation analysis [1] uses the largest eigenvalue of VY X and its corresponding eigenfunctions [4].
Suppose we have another random variable Z on Z and RKHS (HZ ; kZ ), which satisfy the analog
to (A-1). We then deﬁne the normalized conditional cross-covariance operator,
(3)
VY X jZ = VY X (cid:0) VY Z VZX ;
for measuring the conditional dependence of X and Y given Z , where VY Z and VZX are deﬁned
similarly to Eq. (2). The operator VY X jZ may be better understood by expressing it as
Y Y (cid:0)(cid:6)Y X (cid:0) (cid:6)Y Z (cid:6)(cid:0)1
ZZ (cid:6)ZX (cid:1)(cid:6)(cid:0)1=2
VY X jZ = (cid:6)(cid:0)1=2
XX ;
where (cid:6)Y X jZ = (cid:6)Y X (cid:0) (cid:6)Y Z (cid:6)(cid:0)1
ZZ (cid:6)ZX can be interpreted as a nonlinear extension of the condi-
tional covariance matrix CY X (cid:0) CY Z C (cid:0)1
ZZ CZX of Gaussian random variables.
The operator (cid:6)Y X can be used to determine the independence of X and Y : roughly speaking,
(cid:6)Y X = O if and only if X ??Y . Similarly, a relation between (cid:6)Y X jZ and conditional independence,
X ??Y j Z , has been established in [5]: if the extended variables (cid:127)X = (X; Z ) and (cid:127)Y = (Y ; Z ) are
used, X ??Y j Z is equivalent to (cid:6) (cid:127)X (cid:127)Y jZ = O . We will give a rigorous treatment in Section 2.2
Noting that the conditions (cid:6)Y X = O and (cid:6)Y X jZ = O are equivalent to VY X = O and VY X jZ = O ,
respectively, we propose to use the Hilbert-Schmidt norms of the latter operators as dependence

2

(5)

measures. Recall that an operator A : H1 ! H2 is called Hilbert-Schmidt if for complete or-
thonormal systems (CONSs) f(cid:30)i g of H1 and f j g of H2 , the sum Pi;j h j ; A(cid:30)i i2
is ﬁnite (see
H2
[13]). For a Hilbert-Schmidt operator A, the Hilbert-Schmidt (HS) norm kAkH S is deﬁned by
H S = Pi;j h j ; A(cid:30)i i2
. It is easy to see that this sum is independent of the choice of CONSs.
kAk2
H2
Provided that VY X and VY X jZ are Hilbert-Schmidt, we propose the following measures:
I CON D (X; Y jZ ) = kV (cid:127)Y (cid:127)X jZ k2
H S ;
I N OCCO (X; Y ) = kVY X k2
H S :
A sufﬁcient condition that these operators are Hilbert-Schmidt will be discussed in Section 2.3.
It is easy to provide empirical estimates of the measures. Let (X1 ; Y1 ; Z1 ); : : : ; (Xn ; Yn ; Zn )
be an i.i.d. sample from the joint distribution. Using the empirical mean elements bm (n)
X =
n Pn
n Pn
i=1 kX ( (cid:1) ; Xi ) and bm(n)
1
Y = 1
i=1 kY ( (cid:1) ; Yi ), an estimator of (cid:6)Y X is
Y )(cid:10)kX ( (cid:1) ; Xi ) (cid:0) bm(n)
X ; (cid:1) (cid:11)HX
n Pn
i=1 (kY ( (cid:1) ; Yi ) (cid:0) bm(n)
b(cid:6)(n)
Y X = 1
:
b(cid:6)(n)
XX and b(cid:6)(n)
Y Y are deﬁned similarly. The estimators of VY X and VY X jZ are respectively
Y X (cid:0) b(cid:6)(n)
Y Y + "n I (cid:1)(cid:0)1=2 b(cid:6)(n)
Y X =(cid:0) b(cid:6)(n)
XX + "n I (cid:1)(cid:0)1=2
bV (n)
;
where "n > 0 is a regularization constant used in the same way as [1, 5], and
Y X (cid:0) bV (n)
Y X jZ = bV (n)
bV (n)
Y Z bV (n)
(6)
ZX ;
from Eq. (3). The HS norm of the ﬁnite rank operator bV (n)
Y X jZ is easy to calculate. Let GX , GY , and
GZ be the centered Gram matrices, such that GX;ij = hkX ( (cid:1) ; Xi ) (cid:0) bm(n)
X ; kX ( (cid:1) ; Xj ) (cid:0) bm(n)
X iHX
and so on, and deﬁne RX , RY , and RZ as RX = GX (GX +n"n In )(cid:0)1 ; RY = GY (GY +n"n In )(cid:0)1 ;
and RZ = GZ (GZ + n"n In )(cid:0)1 . The empirical dependence measures are then
(cid:17) (cid:13)(cid:13) bV (n)
(cid:127)Y (cid:127)X jZ (cid:13)(cid:13)2
H S = Tr(cid:2)R (cid:127)Y R (cid:127)X (cid:0) 2R (cid:127)Y R (cid:127)X RZ + R (cid:127)Y RZ R (cid:127)X RZ (cid:3);
^I CON D
n
Y X (cid:13)(cid:13)2
(X; Y ) (cid:17) (cid:13)(cid:13) bV (n)
H S = Tr(cid:2)RY RX (cid:3);
^I N OCCO
(8)
n
where the extended variables are used for ^I CON D
. These empirical estimators, and use of "n , will be
n
justiﬁed in Section 2.4 by showing the convergence to I N OCCO and I CON D . With the incomplete
Cholesky decomposition [17] of rank r , the complexity to compute ^I CON D
is O(r2n).
n

(4)

(7)

2.2 Inference on probabilities by characteristic kernels

To relate I N OCCO and I CON D with independence and conditional independence, respectively, the
RKHS should contain a sufﬁciently rich class of functions to represent all higher order moments.
Similar notions have already appeared in the literature: universal kernel on compact domains [15]
and Gaussian kernels on the entire Rm characterize independence via the cross-covariance operator
[8, 1]. We now discuss a uniﬁed class of kernels for inference on probabilities.
Let (X ; B) be a measurable space, X a random variable on X , and (H; k) an RKHS on X satisfying
assumption (A-1). The mean element of X on H is deﬁned by the unique element mX 2 H such
that hmX ; f iH = E [f (X )] for all f 2 H (see [7]). If the distribution of X is P , we also use mP to
denote mX . Letting P be the family of all probabilities on (X ; B), we deﬁne the map Mk by
Mk : P ! H;
P 7! mP :
The kernel k is said to be characteristic1 if the map Mk is injective, or equivalently, if the condition
EX(cid:24)P [f (X )] = EX(cid:24)Q [f (X )] (8f 2 H) implies P = Q.
The notion of a characteristic kernel is an analogy to the characteristic function EP [ep(cid:0)1uT X ],
which is the expectation of the Fourier kernel kF (x; u) = ep(cid:0)1uT x . Noting that mP = mQ iff
EP [k(u; X )] = EQ [k(u; X )] for all u 2 X , the deﬁnition of a characteristic kernel generalizes the
well-known property of the characteristic function that EP [kF (u; X )] uniquely determines a Borel
probability P on Rm . The next lemma is useful to show that a kernel is characteristic.
1Although the same notion was called probability-determining in [5], we call it ”characteristic ”
with the characteristic function.

by analogy

3

Lemma 1. Let q (cid:21) 1. Suppose that (H; k) is an RKHS on a measurable space (X ; B) with k
measurable and bounded. If H + R (the direct sum of the two RKHSs) is dense in Lq (X ; P ) for any
probability P on (X ; B), the kernel k is characteristic.
Proof. Assume mP = mQ . By the assumption, for any " > 0 and a measurable set A, there is a
function f 2 H and c 2 R such that jEP [f (X )] + c (cid:0) P (A)j < " and jEQ [f (Y )] + c (cid:0) Q(A)j < ",
from which we have jP (A) (cid:0) Q(A)j < 2". Since " > 0 is arbitrary, this means P (A) = Q(A).
Many popular kernels are characteristic. For a compact metric space, it is easy to see that the RKHS
given by a universal kernel [15] is dense in L2 (P ) for any P , and thus characteristic (see also [7]
Theorem 3). It is also important to consider kernels on non-compact spaces, since many standard
random variables, such as Gaussian variables, are deﬁned on non-compact spaces. The next theorem
implies that many kernels on the entire Rm , including Gaussian and Laplacian, are characteristic.
The proof is an extension of Theorem 2 in [1], and is given in the supplementary material.
Theorem 2. Let (cid:30)(z ) be a continuous positive function on Rm with the Fourier transform ~(cid:30)(u),
and k be a kernel of the form k(x; y) = (cid:30)(x (cid:0) y). If for any (cid:24) 2 Rm there exists (cid:28)0 such that
R ~(cid:30)((cid:28) (u+(cid:24)))2
du < 1 for all (cid:28) > (cid:28)0 , then the RKHS associated with k is dense in L2 (P ) for any
~(cid:30)(u)
Borel probability P on Rm . Hence k is characteristic with respect to the Borel (cid:27) -ﬁeld.
The assumptions to relate the operators with independence are well described by using characteristic
kernels and denseness. The next result generalizes Corollary 9 in [5] (we omit the proof: see [5, 6]).
Theorem 3. (i) Assume (A-1) for the kernels. If the product kX kY is characteristic, then we have
()
X ??Y :
VY X = O
(ii) Denote (cid:127)X = (X; Z ) and k (cid:127)X
= kX kZ . In addition to (A-1), assume that the product k (cid:127)X
characteristic kernel on (X (cid:2) Z ) (cid:2) Y , and HZ + R is dense in L2 (PZ ). Then,
()
X ??Y j Z:
VY (cid:127)X jZ = O
From the above results, we can guarantee that VY X and VY (cid:127)X jZ will detect independence and condi-
tional independence, if we use a Gaussian or Laplacian kernel either on a compact set or the whole
of Rm . Note also that we can substitute V (cid:127)Y (cid:127)X jZ for VY (cid:127)X jZ in Theorem 3 (ii).
2.3 Kernel-free integral expression of the measures

kY is a

A remarkable property of I N OCCO and I CON D is that they do not depend on the kernels under some
assumptions, having integral expressions containing only the probability density functions. The
probability EZ [PX jZ (cid:10) PY jZ ] on X (cid:2) Y is deﬁned by EZ [PY jZ (cid:10) PX jZ ](B (cid:2) A) R E [(cid:31)B (Y )jZ =
z ]E [(cid:31)A (X )jZ = z ]dPZ (z ) for A 2 BX and B 2 BY .
Theorem 4. Let (cid:22)X and (cid:22)Y be measures on X and Y , respectively, and assume that the probabilities
PX Y and EZ [PX jZ (cid:10) PY jZ ] are absolutely continuous with respect to (cid:22)X (cid:2) (cid:22)Y with probability
density functions pX Y and pX??Y jZ , respectively. If HZ + R and (HX (cid:10) HY ) + R are dense in
L2 (PZ ) and L2 (PX (cid:10) PY ), respectively, and VY X and VY Z VZX are Hilbert-Schmidt, then we have
pX (x)pY (y) (cid:19)2
(cid:18) pX Y (x; y)
H S = Z ZX (cid:2)Y
pX??Y jZ (x; y)
I CON D = kVY X jZ k2
pX (x)pY (y) (cid:0)
pX (x)pY (y)d(cid:22)X d(cid:22)Y ;
where pX and pY are the density functions of the marginal distributions PX and PY , respectively.
As a special case of Z = ;, we have
(cid:18) pX Y (x; y)
pX (x)pY (y) (cid:0) 1(cid:19)2
H S = Z ZX (cid:2)Y
I N OCCO = kVY X k2
Sketch of the proof (see the supplement for the complete proof). Since it is known [8] that (cid:6)ZZ is
Hilbert-Schmidt under (A-1), there exist CONSs f(cid:30)i g1i=1 (cid:26) HX and f j g1j=1 (cid:26) HY consisting of
the eigenfunctions of (cid:6)XX and (cid:6)Y Y , respectively, with (cid:6)XX (cid:30)i = (cid:21)i(cid:30)i ((cid:21)i (cid:21) 0) and (cid:6)Y Y  j =

pX (x)pY (y)d(cid:22)X d(cid:22)Y :

(9)

4

(cid:23)j  j ((cid:23)j (cid:21) 0). Then, kVY X jZ k2
H S admits the expansion
HY o:
P1i;j=1nh j ; VY X (cid:30)i i2
HY (cid:0) 2h j ; VY X (cid:30)i iHY h j ; VY Z VZX (cid:30)i iHY + h j ; VY Z VZX (cid:30)i i2
+ = fi 2 N j (cid:23)i > 0g, and deﬁne ~(cid:30)i = ((cid:30)i (cid:0) E [(cid:30)i (X )])=p(cid:21)i
Let I X
+ = fi 2 N j (cid:21)i > 0g and I Y
and ~ j = ( j (cid:0) E [ j (Y )])=p(cid:23)j for i 2 I X
+ . For simplicity, L2 denotes L2 (PX (cid:10) PY ).
+ and j 2 I Y
With the notations ~(cid:30)0 = 1 and ~ 0 = 1, it is easy to see that the class f ~(cid:30)i ~ j gi2I X
is a
+ [f0g;j2I Y
+ [f0g
CONS of L2 . From Parseval’s equality, the ﬁrst
term of the above expansion is rewritten as
pX pY (cid:1)L2
+ (cid:0) ~(cid:30)i ~ j ; pXY
EY X (cid:2) ~ j (Y ) ~(cid:30)i (X )(cid:3)2
= Pi2I X
HY = Pi2I X
Pi2I X
+ h ~ j ; (cid:6)Y X ~(cid:30)i i2
+ ;j2I Y
+ ;j2I Y
+ ;j2I Y
+
= (cid:13)(cid:13) pXY (x;y)
pX (x)pY (y) (cid:13)(cid:13)2
pX (x)pY (y) (cid:13)(cid:13)2
E (cid:2) ~ j (Y )] (cid:0) 1 = (cid:13)(cid:13) pXY (x;y)
E (cid:2) ~(cid:30)i (X )] (cid:0) Pj2I Y
L2 (cid:0) Pi2I X
L2 (cid:0) 1:
+
+
By a similar argument,
the second and third term of
the expansion are rewritten as
pX pY (cid:13)(cid:13)2
+ 2 and (cid:13)(cid:13) pX??Y jZ
pX pY (cid:1)L2
(cid:0)2(cid:0) pXY
; pX??Y jZ
L2 (cid:0) 1; respectively. This completes the proof.
pX pY
Many practical kernels, such as the Gaussian and Laplacian, satisfy the assumptions in the above
theorem, as we saw in Theorems 2 and the remark after Lemma 1. While the empirical estimate
from ﬁnite
samples depends on the choice of kernels, it is a desirable property for the empirical
dependence measure to converge to a value that depends only on the distributions of the variables.

Eq. (9) shows that, under the assumptions, I N OCCO is equal to the mean square contingency, a
well-known dependence measure[14] commonly used for discrete variables. As we show in Section
2.4, ^I N OCCO
works as a consistent kernel estimator of the mean square contingency.
n
The expression of Eq. (9) can be compared with the mutual information,
M I (X; Y ) = Z ZX (cid:2)Y
d(cid:22)X d(cid:22)Y :
Both the mutual information and the mean square contingency are nonnegative, and equal to zero
if and only if X and Y are independent. Note also that from log z (cid:20) z (cid:0) 1, the inequality
M I (X; Y ) (cid:20) I N OCCO (X; Y ) holds under the assumptions of Theorem 4. While the mutual infor-
mation is the best known dependence measure, its ﬁnite sample empirical estimate is not straight-
forward, especially for continuous variables. The direct estimation of a probability density function
is infeasible if the joint space has even a moderate number of dimensions.

pX Y (x; y)
pX (x)pY (y)

pX Y (x; y) log

2.4 Consistency of the measures

It is important to ask whether the empirical measures converge to the population value I CON D and
I N OCCO , since this provides a theoretical justi ﬁcation for the empirical measures. It is known [4]
that bV (n)
Y X converges in probability to VY X in operator norm. The next theorem asserts convergence
in HS norm, provided that VY X is Hilbert-Schmidt. Although the proof is analogous to the case of
operator norm, it is more involved to discuss the HS norm. We give it in the supplementary material.
Theorem 5. Assume that VY X , VY Z , and VZX are Hilbert-Schmidt, and that the regularization
constant "n satisﬁes "n ! 0 and "3
nn ! 1. Then, we have the convergence in probability
k bV (n)
k bV (n)
and
(n ! 1):
Y X jZ (cid:0) VY X jZ kH S ! 0
Y X (cid:0) VY X kH S ! 0
! I N OCCO and ^I CON D
In particular, ^I N OCCO
! I CON D (n ! 1) in probability.
n
n
2.5 Choice of kernels

(10)

As with all empirical measures, the sample estimates ^I N OCCO
and ^I CON D
are dependent on the
n
n
kernel, and the problem of choosing a kernel has yet to be solved. Unlike supervised learning, there
are no easy criteria to choose a kernel for dependence measures. We propose a method of choosing
a kernel by considering the large sample behavior. We explain the method only brieﬂy in this paper.

The basic idea is that a kernel should be chosen so that the covariance operator detects independence
of variables as effectively as possible. It has been recently shown [10], under the independence of

5

4

2

0

−2

−4
−4

−2

0

2

4

4

2

0

−2

−4
−4

−2

0

2

4

O
C
C
O
N
I

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2
0

0.2

0.4
Angle

0.6

0.8

Figure 1: Left and Middle: Examples of data ((cid:18) = 0 and (cid:18) = (cid:25)=4). Right: The marks ”o”
show ^I N OCCO
for each angle and the 95th percentile of the permutation test, respectively.
n

and ”+”

X and Y , that the measure H S I C = k b(cid:6)(n)
H S ([8]) multiplied by n converges to an in ﬁnite
Y X k2
mixture of (cid:31)2 distributions with variance Varlim [nH S I C ] = 2k(cid:6)XX k2
H S . We choose a
H S k(cid:6)Y Y k2
kernel so that the bootstrapped variance VarB [nH S I C ] of nH S I C is close to this theoretical limit
variance. More precisely, we compare the ratio T = VarB [nH S I C ]=Varlim [nH S I C ] for various
candidate kernels. In preliminary experiments for choosing the variance parameter (cid:27) of Gaussian
kernels, we often observed the ratio decays and saturates below 1, as (cid:27) increases. Therefore, we use
(cid:27) starting the saturation by choosing the minimum of (cid:27) among all candidates that satisfy jT(cid:27) (cid:0) (cid:11)j (cid:20)
(1 + (cid:14)) min(cid:27) jT(cid:27) (cid:0) (cid:11)j for (cid:14) > 0; (cid:11) 2 (0; 1]. We always use (cid:14) = 0:1 and (cid:11) = 0:5. We can expect
that the chosen kernel uses the data effectively. While there is no rigorous theoretical guarantee, in
and ^I CON D
the next section we see that the method gives a reasonable result for ^I N OCCO
.
n
n

3 Experiments

To evaluate the dependence measures, we use a permutation test of independence for data sets with
various degrees of dependence. The test randomly permutes the order of Y1 ; : : : ; Yn to make many
samples independent of (X1 ; : : : ; Xn ), thus simulating the null distribution under independence.
For the evaluation of ^I CON D
, the range of Z is partitioned into Z1 ; : : : ; ZL with the same number
n
of data, and the sample f(Xi ; Yi ) j Zi 2 Z‘ g within the ‘-th bin is randomly permuted. The
signiﬁcance level is always set to 5%. In the following experiments, we always use Gaussian kernels
2(cid:27)2 kx1(cid:0)x2 k2 and choose (cid:27) by the method proposed in Section 2.5.
e(cid:0) 1
Synthetic data for dependence. The random variables X (0) and Y (0) are independent and uni-
formly distributed on [(cid:0)2; 2] and [a; b] [ [(cid:0)b; (cid:0)a], respectively, so that (X (0) ; Y (0) ) has a scalar
covariance matrix. (X ((cid:18)) ; Y ((cid:18)) ) is the rotation of (X (0) ; Y (0) ) by (cid:18) 2 [0; (cid:25)=4] (see Figure 1). X ((cid:18))
and Y ((cid:18)) are always uncorrelated, but dependent for (cid:18) 6= 0. We generate 100 sets of 200 data.
, H S I C = k b(cid:6)(n)
We perform permutation tests with ^I N OCCO
H S , and the mutual information
Y X k2
n
(MI). For the empirical estimates of MI, we use the advanced method from [11], with no need for
explicit estimation of the densities. Since ^I N OCCO
is an estimate of the mean square contingency,
n
we also apply a relevant contingency-table-based independence test ([12]), partitioning the variables
into bins. Figure 1 shows the values of ^I N OCCO
for a sample. In Table 1, we see that the results
n
of ^I N OCCO
are stable w.r.t. the choice of "n , provided it is sufﬁciently small. We ﬁx "n = 10(cid:0)6
n
for all remaining experiments. While all the methods are able to detect the dependence, ^I N OCCO
n
with the asymptotic choice of (cid:27) is the most sensitive to very small dependence. We also observe
the chosen parameters (cid:27)Y for Y increase from 0.58 to 2.0 as (cid:18) increases. The small (cid:27)Y for small (cid:18)
seems reasonable, because the range of Y is split into two small regions.
Chaotic time series. We evaluate a chaotic time series derived from the coupled H ´enon map. The
variables X and Y are four dimensional: the components X1 ; X2 ; Y1 , and Y2 follow the dynamics
(X1 (t + 1); X2 (t + 1)) = (1:4 (cid:0) X1 (t)2 + 0:3X2 (t); X1 (t)), (Y1 (t + 1); Y2 (t + 1)) = (1:4 (cid:0)
f(cid:13)X1 (t)Y1 (t) + (1 (cid:0) (cid:13) )Y2 (t)2 g + 0:1Y2 (t); Y1 (t)), and X3 ; X4 ; Y3 ; Y4 are independent noise with
N (0; (0:5)2 ). X and Y are independent for (cid:13) = 0, while they are synchronized chaos for (cid:13) > 0
(see Figure 2 for examples). A sample consists of 100 data generated from this system. Table 2

6

Angle (degree)
^I N OCCO
(" = 10(cid:0)4 , Median)
n
^I N OCCO
(" = 10(cid:0)6 , Median)
n
^I N OCCO
(" = 10(cid:0)8 , Median)
n
^I N OCCO
(Asymp. Var.)
n
HSIC (Median)
HSIC (Asymp. Var.)
MI (#Nearest Neighbors = 1)
MI (#Nearest Neighbors = 3)
MI (#Nearest Neighbors = 5)
Conting. Table (#Bins= 3)
Conting. Table (#Bins= 4)
Conting. Table (#Bins= 5)

0
94
92
93
94
93
93
93
96
97
100
98
98

4.5
23
20
15
11
92
44
62
43
49
96
29
82

9
0
1
0
0
63
1
11
0
0
46
0
5

13.5
0
0
0
0
5
0
0
0
0
9
0
0

18
0
0
0
0
0
0
0
0
0
1
0
0

22.5
0
0
0
0
0
0
0
0
0
0
0
0

27
0
0
0
0
0
0
0
0
0
0
0
0

31.5
0
0
0
0
0
0
0
0
0
0
0
0

36
0
0
0
0
0
0
0
0
0
0
0
0

40.5
0
0
0
0
0
0
0
0
0
0
0
0

45
0
0
0
0
0
0
0
0
0
0
0
0

Table 1: Comparison of dependence measures. The number of times independence is accepted out
of 100 permutation tests is shown.
”Asymp. Var.”
is the method in Section 2.5.
”Median ”
is a
heuristic method [8] which chooses (cid:27) as the median of pairwise distances of the data.

2

1

0

−1

)
t
(
2
X

2

1

0

−1

)
t
(
1
Y

 

I(X
|X
,Y
)
t
t
t+1
Thresh (a=0.05)

1

0.8

0.6

0.4

0.2

5

4

3

2

1

 

I(Y
|Y
,X
)
t
t
t+1
Thresh (a=0.05)

−1

1

2

−2

−1

1

2

−2
−2

0
0
X
(t)
X
(t)
1
1
(a) Plot of H ´enon map (b) Xt;1 -Yt;1 ((cid:13) = 0:25)

0
0
 
 
0
0.2
0.4
0.6
0
0.2
0.4
(d) I (Yt+1 ; Xt jYt )
(c) I (Xt+1 ; Yt jXt )
Figure 2: Chaotic time series. (a,b): examples of data. (c,d) examples of ^I CON D
(colored ”o”)
n
the threshholds of the permutation test with signi ﬁcance level 5% (black ”+”).

0.6

and

t=1 .
shows the results of permutation tests of independence for the instantaneous pairs (X (t); Y (t))100
The proposed ^I N OCCO
outperforms the other methods in capturing small dependence.
n
Next, we apply ^I CON D
to detect the causal structure of the same time series. Note that the series X
n
is a cause of Y for (cid:13) > 0, but there is no opposite causality, i.e., Xt+1??Yt j Xt and Yt+1 6??Xt j Yt .
In Table 3, it is remarkable that ^I CON D
detects the small causal in ﬂuence
from Xt to Yt+1 for
n
(cid:13) (cid:21) 0:1, while for (cid:13) = 0 the result is close to the theoretical value of 95%.
Graphical modeling from medical data. This is the inference of a graphical model from data with
no time structure. The data consist of three variables, creatinine clearance (C), digoxin clearance
(D), urine ﬂo w (U). These were taken from 35 patients, and analyzed with graphical models in [3,
Section 3.1.4.]. From medical knowledge, D should be independent of U when controlling C. Table
4 shows the results of the permutation tests and a comparison with the linear method. The relation
D??U j C is strongly afﬁrmed by ^I CON D
, while the partial correlation does not ﬁnd it.
n

(cid:13) (strength of coupling)
^I N OCCO
n
HSIC
MI (k = 3)
MI (k = 5)
MI (k = 7)

0.0
97
75
87
87
87

0.1
66
70
91
88
86

0.2
21
58
83
75
75

0.3
1
52
73
67
64

0.4
0
13
23
23
21

0.5
1
1
6
5
5

0.6
0
0
0
0
0

Table 2: Results for the independence tests for the chaotic time series. The number of times inde-
pendence was accepted out of 100 permutation tests is shown. (cid:13) = 0 implies independence.

7

(cid:13) (coupling)
^I N OCCO
n
HSIC

0.0
97
94

H0 : Yt is not a cause of Xt+1
0.5
0.4
0.3
0.2
0.1
68
81
85
93
96
94
92
81
60
73

0.6
75
66

0.0
96
93

H0 : Xt is not a cause of Yt+1
0.5
0.4
0.3
0.2
0.1
0
0
0
0
0
95
85
56
1
1

0.6
0
1

Table 3: Results of the permutation test of non-causality for the chaotic time series. The number of
times non-causality was accepted out of 100 tests is shown.

Kernel measure
^I CON D
n
1.458
0.776
0.194
0.343

D??U j C
C??D
C??U
D??U

P -value
0.924
<0.001
0.117
0.023

Linear method
(partial) correl.
0.4847
0.7754
0.3092
0.5309

Parcorr(D; U jC )
Corr(C; D)
Corr(C; U )
Corr(D; U )

P -value
0.0037
0.0000
0.0707
0.0010

Table 4: Graphical modeling from the medical data. Higher P -values indicate (conditional) inde-
pendence more strongly.

4 Concluding remarks

There are many dependence measures, and further theoretical and experimental comparison is im-
portant. That said, one unambiguous strength of the kernel measure we propose is its kernel-free
population expression. It is interesting to ask if other classical dependence measures, such as the
mutual information, can be estimated by kernels (in a broader sense than the expansion about inde-
pendence of [9]). A relevant measure is the kernel generalized variance (KGV [1]), which is based
on a sum of the logarithm of the eigenvalues of VY X , while I N OCCO is their squared sum. It is also
interesting to investigate whether the KGV has a kernel-free expression. Another topic for further
study is causal inference with the proposed measure, both with and without time information ([16]).

References
[1] F. Bach and M. Jordan. Kernel independent component analysis. J. Machine Learning Res., 3:1–48, 2002.
[2] C. Baker. Joint measures and cross-covariance operators. Trans. Amer. Math. Soc., 186:273–289, 1973.
[3] D. Edwards. Introduction to graphical modelling. Springer verlag, New York, 2000.
[4] K. Fukumizu, F. Bach, and A. Gretton. Statistical consistency of kernel canonical correlation analysis. J.
Machine Learning Res., 8:361–383, 2007.
[5] K. Fukumizu, F. Bach, and M. Jordan. Dimensionality reduction for supervised learning with reproducing
kernel Hilbert spaces. J. Machine Learning Res., 5:73 –99, 2004.
[6] K. Fukumizu, F. Bach, and M. Jordan. Kernel dimension reduction in regression. Tech Report 715, Dept.
Statistics, University of California, Berkeley, 2006.
[7] A. Gretton, K. Borgwardt, M. Rasch, B. Sch ¨olkopf, and A. Smola. A kernel method for the two-sample-
problem. Advances in NIPS 19. MIT Press, 2007.
[8] A. Gretton, O. Bousquet, A. Smola, and B. Sch ¨olkopf. Measuring statistical dependence with Hilbert-
Schmidt norms. 16th Intern. Conf. Algorithmic Learning Theory, pp.63–77. Springer, 2005.
[9] A. Gretton, R. Herbrich, A. Smola, O. Bousquet and B. Sch ¨olkopf. Kernel Methods for Measuring
Independence. J. Machine Learning Res., 6:2075 –2129, 2005.
[10] A. Gretton, K. Fukumizu, C. Teo, L. Song, B. Sch ¨olkopf, A. Smola. A Kernel Statistical Test of Indepen-
dence. Advances in NIPS 21. 2008, to appear.
[11] A. Kraskov, H. St ¨ogbauer, and P. Grassberger. Estimating mutual information. Physical Review E, 69,
066138-1–16, 2004.
[12] T. Read and N. Cressie. Goodness-of-Fit Statistics for Discrete Multivariate Data. Springer-Verlag, 1988.
[13] M. Reed and B. Simon. Functional Analysis. Academic Press, 1980.
[14] A. R ´enyi. Probability Theory. Horth-Holland, 1970.
[15] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. J. Machine
Learning Res., 2:67–93, 2001.
[16] X. Sun, D. Janzing, B. Sch ¨olkopf, and K. Fukumizu. A kernel-based causal learning algorithm. Proc.
24th Intern. Conf. Machine Learning, 2007 to appear.
[17] S. Fine and K. Scheinberg Efﬁcient SVM Training using Low-Rank Kernel Representations J. Machine
Learning Res., 2:243–264, 2001.

8


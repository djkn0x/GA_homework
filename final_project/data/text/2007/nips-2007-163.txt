Message Passing for Max-weight Independent Set

Sujay Sanghavi
LIDS, MIT
sanghavi@mit.edu

Devavrat Shah
Dept. of EECS, MIT
devavrat@mit.edu

Alan Willsky
Dept. of EECS, MIT
willsky@mit.edu

Abstract

We investigate the use of message-passing algorithms for the problem of ﬁnding
the max-weight independent set (MWIS) in a graph. First, we study the perfor-
mance of loopy max-product belief propagation. We show that, if it converges,
the quality of the estimate is closely related to the tightness of an LP relaxation
of the MWIS problem. We use this relationship to obtain sufﬁci ent conditions for
correctness of the estimate. We then develop a modi ﬁcation o f max-product – one
that converges to an optimal solution of the dual of the MWIS problem. We also
develop a simple iterative algorithm for estimating the max-weight independent
set from this dual solution. We show that the MWIS estimate obtained using these
two algorithms in conjunction is correct when the graph is bipartite and the MWIS
is unique. Finally, we show that any problem of MAP estimation for probability
distributions over ﬁnite domains can be reduced to an MWIS pro blem. We believe
this reduction will yield new insights and algorithms for MAP estimation.

1

Introduction

The max-weight independent set (MWIS) problem is the following: given a graph with positive
weights on the nodes, ﬁnd the heaviest set of mutually non-ad jacent nodes. MWIS is a well studied
combinatorial optimization problem that naturally arises in many applications. It is known to be
NP-hard, and hard to approximate [6].
In this paper we investigate the use of message-passing
algorithms, like loopy max-product belief propagation, as practical solutions for the MWIS problem.
We now summarize our motivations for doing so, and then outline our contribution.

Our primary motivation comes from applications. The MWIS problem arises naturally in many
scenarios involving resource allocation in the presence of interference.
It is often the case that
large instances of the weighted independent set problem need to be (at least approximately) solved
in a distributed manner using lightweight data structures.
In Section 2.1 we describe one such
application: scheduling channel access and transmissions in wireless networks. Message passing
algorithms provide a promising alternative to current scheduling algorithms.

Another, equally important, motivation is the potential for obtaining new insights into the perfor-
mance of existing message-passing algorithms, especially on loopy graphs. Tantalizing connections
have been established between such algorithms and more traditional approaches like linear pro-
gramming (see [9] and references). The MWIS problem provides a rich, yet relatively tractable, ﬁrst
framework in which to investigate such connections.

1.1 Our contributions

In Section 4 we construct a probability distribution whose MAP estimate corresponds to the MWIS
of a given graph, and investigate the application of the loopy Max-product algorithm to this distrit-
buion. We demonstrate that there is an intimate relationship between the max-product ﬁxed-points
and the natural LP relaxation of the original independent set problem. We use this relationship to
provide a certi ﬁcate of correctness for the max-product ﬁxed point in certain problem instances .

1

In Section 5 we develop two iterative message-passing algorithms. The ﬁrst, obtained by a minor
modi ﬁcation of max-product, calculates the optimal soluti on to the dual of the LP relaxation of the
MWIS problem. The second algorithm uses this optimal dual to produce an estimate of the MWIS.
This estimate is correct when the original graph is bipartite.

In Section 3 we show that any problem of MAP estimation in which all the random variables can
take a ﬁnite number of values (and the probability distribut
ion is positive over the entire domain) can
be reduced to a max-weight independent set problem. This implies that any algorithm for solving
the independent set problem immediately yields an algorithm for MAP estimation. We believe this
reduction will prove useful from both practical and analytical perspectives.

2 Max-weight Independent Set, and its LP Relaxation

Consider a graph G = (V , E ), with a set V of nodes and a set E of edges. Let N (i) = {j ∈ V :
(i, j ) ∈ E } be the neighbors of i ∈ V . Positive weights wi , i ∈ V are associated with each node.
A subset of V will be represented by vector x = (xi ) ∈ {0, 1}|V | , where xi = 1 means i is in the
subset xi = 0 means i is not in the subset. A subset x is called an independent set if no two nodes
in the subset are connected by an edge: (xi , xj ) 6= (1, 1) for all (i, j ) ∈ E . We are interested in
ﬁnding a maximum weight independent set (MWIS) x∗ . This can be naturally posed as an integer
program, denoted below by IP. The linear programing relaxation of IP is obtained by replacing the
integrality constraints xi ∈ {0, 1} with the constraints xi ≥ 0. We will denote the corresponding
linear program by LP. The dual of LP is denoted below by DUAL.
min X(i,j )∈E
DUAL :
s.t. Xj∈N (i)
λij ≥ wi , for all i ∈ V ,
λij ≥ 0, for all (i, j ) ∈ E .

nXi=1
xi + xj ≤ 1 for all (i, j ) ∈ E ,
xi ∈ {0, 1}.

wixi ,

λij ,

max

IP :

s.t.

It is well-known that LP can be solved efﬁciently, and if it has an integral optimal so lution then this
solution is an MWIS of G. If this is the case, we say that there is no integrality gap between LP and
IP or equivalently that the LP relaxation is tight. usIt is well known [3] that the LP relaxation is
tight for bipartite graphs. More generally, for non-bipartite graphs, tightness will depend on the
node weights. We will use the performance of LP as a benchmark with which to compare the
performance of our message passing algorithms.

The next lemma states the standard complimentary slackness conditions of linear programming,
specialized for LP above, and for the case when there is no integrality gap.

Lemma 2.1 When there is no integrality gap between IP and LP, there exists a pair of optimal
solutions x = (xi ), λ = (λij ) of LP and DUAL respectively, such that: (a) x ∈ {0, 1}n , (b)
xi (cid:16)Pj∈N (i) λij − wi(cid:17) = 0 for all i ∈ V , (c) (xi + xj − 1) λij = 0, for all (i, j ) ∈ E .
2.1 Sample Application: Scheduling in Wireless Networks
We now brieﬂy describe an important application that requir es an efﬁcient, distributed solution to the
MWIS problem: transmision scheduling in wireless networks that lack a centralized infrastructure,
and where nodes can only communicate with local neighbors (e.g. see [4]). Such networks are
ubiquitous in the modern world: examples range from sensor networks that lack wired connections
to the fusion center, and ad-hoc networks that can be quickly deployed in areas without coverage,
to the 802.11 wi- ﬁ networks that currently represent the mos t widely used method for wireless data
access.

Fundamentally, any two wireless nodes that transmit at the same time and over the same frequencies
will interfere with each other, if they are located close by. Interference means that the intended
receivers will not be able to decode the transmissions. Typically in a network only certain pairs

2

of nodes interfere. The scheduling problem is to decide which nodes should transmit at a given
time over a given frequency, so that (a) there is no interference, and (b) nodes which have a large
amount of data to send are given priority. In particular, it is well known that if each node is given a
weight equal to the data it has to transmit, optimal network operation demands scheduling the set of
nodes with highest total weight. If a “ conﬂict graph” is made
, with an edge between every pair of
interfering nodes, the scheduling problem is exactly the problem of ﬁnding the MWIS of the conﬂict
graph. The lack of an infrastructure, the fact that nodes often have limited capabilities, and the local
nature of communication, all necessitate a lightweight distributed algorithm for solving the MWIS
problem.

3 MAP Estimation as an MWIS Problem

1
Z

q(y) =

exp (φα (yα )) =

In this section we show that any MAP estimation problem is equivalent to an MWIS problem on
a suitably constructed graph with node weights. This construction is related to the “overcomplete
basis ” representation [7]. Consider the following canonic al MAP estimation problem: suppose we
are given a distribution q(y) over vectors y = (y1 , . . . , yM ) of variables ym , each of which can take
a ﬁnite value. Suppose also that q factors into a product of strictly positive functions, which we ﬁnd
convenient to denote in exponential form:
exp  Xα∈A
φα (yα )!
Z Yα∈A
1
Here α speci ﬁes the domain of the function φα , and yα is the vector of those variables that are in
the domain of φα . The α’s also serve as an index for the functions. A is the set of functions. The
MAP estimation problem is to ﬁnd a maximizing assignment y∗ ∈ arg maxy q(y).
We now build an auxillary graph eG, and assign weights to its nodes, such that the MAP estimation
problem above is equivalent to ﬁnding the MWIS of eG. There is one node in eG for each pair (α, yα ),
where yα is an assignment (i.e. a set of values for the variables) of domain α. We will denote this
node of eG by δ(α, yα ).
There is an edge in eG between any two nodes δ(α1 , y1
α2 ) if and only if there exists
α1 ) and δ(α2 , y2
a variable index m such that
1. m is in both domains, i.e. m ∈ α1 and m ∈ α2 , and
m .
2. the corresponding variable assignments are different, i.e. y1
m 6= y2

In other words, we put an edge between all pairs of nodes that correspond to inconsistent assign-
ments. Given this graph eG, we now assign weights to the nodes. Let c > 0 be any number such that
c + φα (yα ) > 0 for all α and yα . The existence of such a c follows from the fact that the set of
assignments and domains is ﬁnite. Assign to each node δ(α, yα ) a weight of c + φα (yα ).
Lemma 3.1 Suppose q and eG are as above.
let δ∗ =
(a) If y∗ is a MAP estimate of q ,
α ) | α ∈ A} be the set of nodes in eG that correspond to each domain being consistent
{δ(α, y∗
with y∗ . Then, δ∗ is an MWIS of eG. (b) Conversely, suppose δ∗ is an MWIS of eG. Then, for every
domain α, there is exactly one node δ(α, y∗
α ) included in δ∗ . Further, the corresponding domain
α | α ∈ A} are consistent, and the resulting overall vector y∗ is a MAP estimate of q .
assignments{y∗
Example. Let y1 and y2 be binary variables with joint distribution
1
q(y1 , y2 ) =
exp(θ1 y1 + θ2 y2 + θ12 y1 y2 )
Z
where the θ are any real numbers. The corresponding eG is shown
to the right. Let c be any number such that c + θ1 , c + θ2 and c + θ12
are all greater than 0. The weights on the nodes in eG are: θ1 + c on
node “1” on the left,
θ2 + c for node “1” on the right,
θ12 + c for
the node “11”, and
c for all the other nodes.
3

10

11

01

00

0

1

0

1

4 Max-product for MWIS

p (x) =

The classical max-product algorithm is a heuristic that can be used to ﬁnd the MAP assignment of a
probability distribution. Now, given an MWIS problem on G = (V , E ), associate a binary random
variable Xi with each i ∈ V and consider the following joint distribution: for x ∈ {0, 1}n ,
1{xi+xj ≤1} Yi∈V
Z Y(i,j )∈E
1
where Z is the normalization constant. In the above, 1 is the standard indicator function: 1true = 1
Z exp (Pi wixi ) if x is an independent set, and
It is easy to see that p(x) = 1
and 1false = 0.
p(x) = 0 otherwise. Thus, any MAP estimate arg maxx p(x) corresponds to a maximum weight
independent set of G.
The update equations for max-product can be derived in a standard and straightforward fashion from
the probability distribution. We now describe the max-product algorithm as derived from p. At every
iteration t each node i sends a message {mt
i→j (1)} to each neighbor j ∈ N (i). Each node
i→j (0), mt
also maintains a belief {bt
i (1)} vector. The message and belief updates, as well as the ﬁnal
i (0), bt
output, are computed as follows.

exp(wixi ),

(1)

Max-product for MWIS

j→i (1) = 1 for all (i, j ) ∈ E .
(o) Initially, m0
i→j (0) = m0
(i) The messages are updated as follows:
i→j (0) = max  Yk 6=j,k∈N (i)
k→i (0) , ewi Yk 6=j,k∈N (i)
mt+1
mt
i→j (1) = Yk 6=j,k∈N (i)
mt+1
mt
k→i (0).
(ii) Nodes i ∈ V , compute their beliefs as follows:
(0) = Yk∈N (i)
(1) = ewi Yk∈N (i)
(iii) Estimate max. wt. independent set x(bt+1 ) as follows: xi (bt+1
) = 1{bt+1
(1)>bt+1
i
i
i
(iv) Update t = t + 1; repeat from (i) till x(bt ) converges and output the converged estimate.

k→i (1)
mt

mt+1
k→i (1).

mt+1
k→i (0),

bt+1
i

bt+1
i

,

(0)} .

For the purpose of analysis, we ﬁnd it convenient to transfor m the messages be deﬁning 1 γ t
i→j =
i→j (1) (cid:17). Step (i) of max-product now becomes
log (cid:16) mt
i→j (0)
mt
k→i
i→j = max 
0, wi − Xk 6=j,k∈N (i)
γ t+1
γ t
where we use the notation (x)+ = max{x, 0}. The estimation of step (iii) of max-product becomes:
xi (γ t+1 ) = 1{wi−Pk∈N (i) γk→i>0} . This modi ﬁcation of max-product is often known as the “min-
sum” algorithm, and is just a reformulation of the max-produ ct. In the rest of the paper we refer to
this as simply the max-product algorithm.

(2)

,

1 If the algorithm starts with all messages being strictly positive, the messages will remain strictly positive
over any ﬁnite number of iterations. Thus taking logs is a valid operation.

4

4.1 Fixed Points of Max-Product

When applied to general graphs, max product may either (a) not converge, (b) converge, and yield
the correct answer, or (c) converge but yield an incorrect answer. Characterizing when each of the
three situations can occur is a challenging and important task. One approach to this task has been to
look directly at the ﬁxed points, if any, of the iterative pro cedure [8].

Proposition 4.1 Let γ represent a ﬁxed point of the algorithm, and let x(γ ) = (xi (γ )) be the
corresponding estimate for the independent set. Then, the following properties hold:

(a) Let i be a node with estimate xi (γ ) = 1, and let j ∈ N (i) be any neighbor of i. Then,
the messages on edge (i, j ) satisfy γi→j > γj→i . Further, from this it can be deduced that x(γ )
represents an independent set in G.
(b) Let j be a node with xj (γ ) = 0, which by deﬁnition means that wj − Pk∈N (j ) γk→j ≤ 0.
Suppose now there exists a neighbor i ∈ N (j ) whose estimate is xi (γ ) = 1. Then it has to be that
wj − Pk∈N (j ) γk→j < 0, i.e. the inequality is strict.
(c) For any edge (j1 , j2 ) ∈ E , if the estimates of the endpoints are xj1 (γ ) = xj2 (γ ) = 0, then it has
to be that γj1→j2 = γj2→j1 . In addition, if there exists a neighbor i1 ∈ N (j1 ) of j1 whose estimate
is xi1 (γ ) = 1, then it has to be that γj1→j2 = γj2→j1 = 0 (and similarly for a neighbor i2 of j2 ).

The properties shown in Proposition 4.1 reveal striking similarities between the messages γ of ﬁxed
points of max-product, and the optimal λ that solves the dual linear program DUAL. In particular,
suppose that γ is a ﬁxed point at which the corresponding estimate x(γ ) is a maximal independent
set: for every j whose estimate xj (γ ) = 0 there exists a neighbor i ∈ N (j ) whose estimate is
xi (γ ) = 1. The MWIS, for example, is also maximal (if not, one could add a node to the MWIS and
obtain a higher weight). For a maximal estimate, it is easy to see that
• (xi (γ ) + xj (γ ) − 1) γi→j = 0 for all edges (i, j ) ∈ E .
• xi (γ ) (cid:16)γi→j + Pk∈N (i)−j γk→i − wi(cid:17) = 0 for all i, j ∈ V
At least semantically, these relations share a close resemblance to the complimentary slackness
conditions of Lemma 2.1. In the following lemma we leverage this resemblance to derive a certi ﬁcate
of optimality of the max-product ﬁxed point estimate for cer tain problems.

Lemma 4.1 Let γ be a ﬁxed point of max-product and x(γ ) the corresponding estimate of the
independent set. Deﬁne G′ = (V , E ′ ) where E ′ = E \{(i, j ) ∈ E : γi→j = γj→i = 0} is the
set of edges with at least one non-zero message. Then, if G′ is acyclic, we have that : (a) x(γ ) is
a solution to the MWIS for G, and (b) there is no integrality gap between LP and IP, i.e. x(γ ) is
an optimal solution to LP. Thus the lack of cycles in G′ provides a certi ﬁcate of optimality for the
estimate x(γ ).

Max-product vs. LP relaxation. The following general question has been of great recent interest:
which of the two, max-product and LP relaxation, is more powerful ? We now brieﬂy investigate
this question for MWIS. As presented below, we ﬁnd that there a re examples where one technique
is better than the other. That is, neither technique clearly dominates the other.

To understand whether correctness of max-product (e.g. Lemma 4.1) provides information about
LP relaxation, we consider the simplest loopy graph: a cycle. For bipartite graph, we know that
LP relaxation is tight, i.e. provides answer to MWIS. Hence, we consider odd cycle. The following
result suggests that if max-product works then it must be that LP relaxation is tight (i.e. LP is no
weaker than max-product for cycles).

Corollary 4.1 Let G be an odd cycle, and γ a ﬁxed point of Max-product. Then, if there exists at
least one node i whose estimate xi (γ ) = 1, then there is no integrality gap between LP and IP.

Next, we present two examples which help us conclude that neither max-product nor LP relaxation
dominate the other. The following ﬁgures present graphs and the corresponding ﬁxed points of
max-product. In each graph, numbers represent node weights, and an arrow from i to j represents

5

a message value of γi→j = 2. All other messages have γ are equal to 0. The boxed nodes indicate
the ones for which the estimate xi (γ ) = 1. It is easy to verify that both represent max-product ﬁxed
points.

2

2

2

2

2

2

3

3

3

3

3

3

For the graph on the left, the max-product ﬁxed point results
in an incorrect estimate. However,
the graph is bipartite, and hence LP will get the correct answer. In the graph on the right, there is
an integrality gap between LP and IP: setting each xi = 1
2 yields an optimal value of 7.5, while
the optimal solution to IP has value 6. However, the estimate at the ﬁxed point of max-pr oduct
is the correct MWIS. In both of these examples, the ﬁxed points
lie in the strict interiors of non-
trivial regions of attraction: starting the iterative procedure from within these regions will result in
convergence to the ﬁxed point.

These examples indicate that it may not be possible to resolve the question of relative strength of the
two procedures based solely on an analysis of the ﬁxed points of max-product.

5 A Convergent Message-passing Algorithm

In this section we present our algorithm for ﬁnding the MWIS of a graph. It is based on modifying
max-product by drawing upon a dual co-ordinate descent and barrier method. Speci ﬁcally, the
algorithm is as follows: (1) For small enough parameters ε, δ , run subroutine DESCENT(ǫ, δ ) (close
to) convergence. This will produce output λε,δ = (λε,δ
ij )(i,j )∈E . (2) For small enough parameter δ1 ,
use subroutine EST(λε,δ , δ1 ), to produce an estimate for the MWIS as the output of algorithm.
Both of the subroutines, DESCENT, EST are iterative message-passing procedures. Before going
into details of the subroutines, we state the main result about correctness and convergence of this
algorithm.

Theorem 5.1 The following properties hold for arbitrary graph G and weights: (a) For any choice
of ε, δ, δ1 > 0, the algorithm always converges. (b) As ε, δ → 0, λε,δ → λ∗ where λ∗ is an optimal
solution of DUAL . Further, if G is bipartite and the MWIS is unique, then the following holds: (c)
For small enough ε, δ, δ1 , the algorithm produces the MWIS as output.

5.1 Subroutine: DESCENT

Consider the standard coordinate descent algorithm for DUAL: the variables are {λij , (i, j ) ∈
E }(with notation λij = λj i ) and at each iteration t one edge (i, j ) ∈ E is picked2 and update
jk
ij = max 
ik , wj − Xk∈N (j ),k 6=i
0, wi − Xk∈N (i),k 6=j
λt+1
λt
λt
The λ on all the other edges remain unchanged from t to t + 1. Notice the similarity (at least
syntactic) between (3) and update of max-product (min-sum) (2): essentially, the dual coordinate
descent is a sequential bidirectional version of the max-product algorithm !
It is well known that the coordinate descent always coverges, in terms of cost for linear programs.
Further, it converges to an optimal solution if the constraints are of the product set type (see [2] for
details). However, due to constraints of type Pj∈N (i) λij ≥ wi in DUAL, the algorithm may not
2A good policy for picking edges is round-robin or uniformly at random

(3)

6

converge to an optimal of DUAL. Therefore, a direct adaptation of max-product to mimic dual co-
ordinate descent is not good enough. We use barrier (penalty) function based approach to overcome
this difﬁculty. Consider the following convex optimizatio n problem obtained from DUAL by adding
a logarithmic barrier for constraint violations with ε ≥ 0 controlling penalty due to violation.
λij − wi
log  Xj∈N (i)
λij  − ε Xi∈V
min  X(i,j )∈E
λij ≥ 0, for all (i, j ) ∈ E .
subject to
The following is coordinate descent algorithm for CP(ε).

CP(ε) :

DESCENT(ε, δ )

(o) The parameters are variables λij , one for each edge (i, j ) ∈ E . We will use notation that
j i . The vector λ is iteratively updated, with t denoting the iteration number.
ij = λt
λt
◦ Initially, set t = 0 and λ0
ij = max{wi , wj } for all (i, j ) ∈ E .
(i) In iteration t + 1, update parameters as follows:
◦ Pick an edge (i, j ) ∈ E . This edge selection is done so that each edge is chosen
inﬁnitely often as t → ∞ (for example, at each t choose an edge uniformly at random.)
◦ For all (i′ , j ′ ) ∈ E , (i′ , j ′ ) 6= (i, j ) do nothing, i.e. λt+1
i′ j ′ = λt
i′ j ′ .
◦ For edge (i, j ), nodes i and j exchange messages as follows:
i→j = wi − Xk 6=j,k∈N (i)
j→i = wj − Xk′ 6=i,k′∈N (j )
ki+
γ t+1
, γ t+1
λt
i→j and b = γ t+1
as follows: with a = γ t+1
◦ Update λt+1
j→i ,
ij
ij =   a + b + 2ε + p(a − b)2 + 4ε2
!+
λt+1
2
(ii) Update t = t + 1 and repeat till algorithm converges within δ for each component.
(iii) Output λ, the vector of paramters at convergence,
Remark. The iterative step (4) can be rewritten as follows: for some β ∈ [1, 2],
kj 
ij = β ε + max 
ik ,  wj − Xk∈N (j )\i
−β ε, wi − Xk∈N (i)\j
λt+1
λt
λt
i→j , γ t+1
where β depends on values of γ t+1
j→i . Thus the updates in DESCENT are obtained by small
but important perturbation of dual coordinate descent for DUAL, and making it convergent. The
output of DESCENT(ε, δ ), say λε,δ → λ∗ as ε, δ → 0 where λ∗ is an optimal solution of DUAL.

k′ j +
λt

,

.

(4)

5.2 Subroutine: EST

DESCENT yields a good estimate of the optimal solution to DUAL, for small values of ǫ and
δ . However, we are interested in the (integral) optimum of LP. In general, it is not possible to
recover the solution of a linear program from a dual optimal solution. However, we show that such
a recovery is possible through EST algorithm described below for the MWIS problem when G is
bipartite with unique MWIS. This procedure is likely to extend for general G when LP relaxation is
tight and LP has unique solution.

EST(λ, δ1 ).

7

(o) The algorithm iteratively estimates x = (xi ) given λ.
(i) Initially, color a node i gray and set xi = 0 if Pj∈N (i) λij > wi . Color all other nodes
with green and leave their values unspeci ﬁed. The condition Pj∈N (i) λij > wi is checked
as whether Pj∈N (i) λij ≥ wi + δ1 or not.
(ii) Repeat the following steps (in any order) till no more changes can happen:
◦ if i is green and there exists a gray node j ∈ N (i) with λij > 0, then set xi = 1 and
color it orange. The condition λij > 0 is checked as whether λij ≥ δ1 or not.
◦ if i is green and some orange node j ∈ N (i), then set xi = 0 and color it gray.
(iii) If any node is green, say i, set xi = 1 and color it red.
(iv) Produce the output x as an estimation.

6 Discussion

We believe this paper opens several interesting directions for investigation. In general, the exact rela-
tionship between max-product and linear programming is not well understood. Their close similarity
for the MWIS problem, along with the reduction of MAP estimation to an MWIS problem, suggests
that the MWIS problem may provide a good ﬁrst step in an investi gation of this relationship.

Also, our novel message-passing algorithm and the reduction of MAP estimation to an MWIS prob-
lem immediately yields a new message-passing algorithm for MAP estimation. It would be interest-
ing to investigate the power of this algorithm on more general discrete estimation problems.

References

IEEE

[1] M. Bayati, D. Shah and M. Sharma, “Max Weight Matching via Max Pro duct Belief Propagation,”
ISIT, 2005.
[2] D. Bertsekas, “Non Linear Programming”, Athena Scientiﬁc .
[3] M. Grtschel, L. Lovsz, and A. Schrijver, “Polynomial algorithms fo r perfect graphs,” in C. Berge and V.
Chvatal (eds.) Topics on Perfect Graphs Ann. Disc. Math. 21, North-Holland, Amsterdam(1984) 325-356.
[4] K. Jung and D. Shah, “Low Delay Scheduing in Wireless Networks,”
IEEE ISIT, 2007.
[5] C. Moallemi and B. Van Roy, “Convergence of the Min-Sum Messag e Passing Algorithm for Quadratic
Optimization,” Preprint, 2006 available at arXiv:cs/0603058
[6] Luca Trevisan, “Inapproximability of combinatorial optimization prob lems,” Technical Report TR04-065,
Electronic Colloquium on Computational Complexity, 2004.
[7] M. Wainwright and M. Jordan, “Graphical models, exponential fa milies, and variational inference,” UC
Berkeley, Dept. of Statistics, Technical Report 649. September, 2003.
[8] J. Yedidia, W. Freeman and Y. Weiss, “Generalized Belief Propaga tion,” Mitsubishi Elect. Res. Lab., TR-
2000-26, 2000.
[9] Y. Weiss, C. Yanover, T. Meltzer “MAP Estimation, Linear Programm ing and Belief Propagation with
Convex Free Energies ” UAI 2007

8


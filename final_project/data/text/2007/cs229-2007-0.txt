Emotion Detection from Speech 

1. Introduction 
Although  emotion  detection  from  speech  is  a  relatively  new  field  of  research,  it  has  many  potential 
applications.  In  human-computer  or  human-human  interaction  systems,  emotion  recognition  systems 
could  provide  users  with  improved  services  by  being  adaptive  to  their  emotions.  In  virtual  worlds, 
emotion recognition could help simulate more realistic avatar interaction.  
 
The  body  of  work  on  detecting  emotion  in  speech  is  quite  limited.  Currently,  researchers  are  still 
debating  what  features  influence  the  recognition  of  emotion  in  speech.  There  is  also  considerable 
uncertainty as to the best algorithm for classifying emotion, and which emotions to class together. 
 
In  this  project,  we  attempt  to  address  these  issues.  We  use  K-Means  and  Support  Vector  Machines 
(SVMs)  to  classify  opposing  emotions.  We  separate  the  speech  by  speaker  gender  to  investigate  the 
relationship between gender and emotional content of speech.  
 
There  are  a  variety  of  temporal  and  spectral  features  that  can  be  extracted  from  human  speech. We  use 
statistics relating to the pitch, Mel Frequency Cepstral Coefficients (MFCCs) and Formants of speech as 
inputs  to  classification  algorithms.  The  emotion  recognition  accuracy  of  these  experiments  allow  us  to 
explain which features carry the most emotional information and why.  
 
It  also  allows  us  to  develop  criteria  to  class  emotions  together.  Using  these  techniques  we  are  able  to 
achieve high emotion recognition accuracy.   
 
2. Corpus of Emotional Speech Data 
The  data  used  for  this  project  comes  from  the  Linguistic  Data  Consortium’s  study  on  Emotional 
Prosody and Speech Transcripts  [1]. The audio  recordings and  corresponding  transcripts were collected 
over an eight month period in 2000-2001 and are designed to support research in emotional prosody.  
 
The  recordings  consist  of  professional  actors  reading  a  series  of  semantically  neutral  utterances  (dates 
and numbers) spanning fourteen distinct emotional categories, selected after Banse & Scherer's study of 
vocal  emotional  expression  in  German  [2].  There  were  5  female  speakers  and  3  male  speakers,  all  in 
their mid-20s. The number of utterances  that belong  to each emotion category  is  shown  in Table 1. The 
recordings were  recorded with a sampling  rate of 22050Hz and encoded in  two-channel  interleaved 16-
bit  PCM,  high-byte-first  ("big-endian")  format.  They were  then  converted  to  single  channel  recordings 
by taking the average of both channels and removing the DC-offset.  
 

Anxiety 
Panic 
Disgust 
Neutral 
170 
141 
171 
82 
Sadness 
Despair 
Cold Anger 
Hot Anger 
149 
171 
155 
138 
Boredom 
Interest 
Happy 
Elation 
154 
176 
177 
159 
 
Contempt 
Pride 
Shame 
148 
 
181 
150 
Table 1: Number of utterances belonging to each Emotion Category 

3. Feature Extraction 

Pitch and related features 
Bäzinger  et  al.  argued  that  statistics  related  to  pitch  conveys  considerable  information  about  emotional 
status  [3].  Yu  et  al.  have  shown  that  some  statistics  of  the  pitch  carries  information  about  emotion  in 
Mandarin speech [4].  

400

350

E lation

Despair

For  this  project,  pitch  is  extracted  from  the  speech  waveform  using  a  modified  version  of  the  RAPT 
algorithm  for  pitch  tracking  [5]  implemented  in  the  VOICEBOX  toolbox  [6].  Using  a  frame  length  of 
50ms,  the pitch  for each  frame was  calculated  and placed  in  a vector  to correspond  to  that  frame.  If  the 
speech is unvoiced the corresponding marker in the pitch vector was set to zero.  
Figure  1  shows  the  variation  in  pitch  for  a  female 
P itch vs  Frame Number
speaker  uttering  “Seventy  one”  in  emotional  states  of 
despair  and  elation.  It  is  evident  from  this  figure  that 
the  mean  and  variance  of  the  pitch  is  higher  when 
“Seventy one”  is uttered  in elation  rather  than despair. 
In  order  to  capture  these  and  other  characteristics,  the 
following  statistics  are  calculated  from  the  pitch  and 
used in the pitch feature vector: 
•  Mean, Median, Variance, Maximum, Minimum (for 
the pitch vector and its derivative) 
•  Average energies of voiced and unvoiced speech 
•  Speaking  rate  (inverse  of  the  average  length  of  the 
voiced part of the utterance) 
Hence, the pitch feature vector is 13-dimensional.  
 

30
40
Frame Number

150

200

250

300

60

70

20

50

10

0

)
z
H
(
h
c
t
i
P

       Figure 1: Variation in Pitch for 2 emotional states   

50

0

0

10

100

t
n
e
i
c
i
f
f
e
o
C
 
C
C
F
M
 
t
s
1
t
n
e
i
c
i
f
f
e
o
C
 
C
C
F
M
 
d
n
2

MFCC and related features 
MFCCs  are  the  most  widely  used  spectral  representation  of  speech  in  many  applications,  including 
speech and speaker  recognition. Kim et al. argued that statistics relating to MFCCs also carry emotional 
information [7]. 
 
For  each  25ms  frame  of  speech,  thirteen  standard 
MFCC  parameters  are  calculated  by  taking  the 
absolute  value  of  the  STFT,  warping  it  to  a  Mel 
frequency  scale,  taking  the  DCT  of  the  log-Mel-
spectrum and  returning  the first 13 components [8].  
Figure  2  shows  the  variation  in  three MFCCs  for  a 
female 
speaker  uttering  “Seventy  one” 
in 
emotional  states  of  despair  and  elation.  It  is 
evident  from  this  figure  that  the  mean  of  the  first 
coefficient is higher when “Seventy one”  is uttered 
in  elation  rather  than  despair,  but  is  lower  for  the 
second  and  third  coefficients.  In  order  to  capture 
these  and  other  characteristics,  we  extracted 
statistics  based  on 
the  MFCCs.  For  each 
coefficient  and  its  derivative  we  calculated  the 
mean,  variance, maximum  and minimum  across  all 
frames. We  also  calculate  the mean,  variance, maximum  and minimum  of  the mean  of  each  coefficient 
and its derivative. Each MFCC feature vector is 112-dimensional.  

    Figure 2: Variation in MFCCs for 2 emotional states 

50
60
Frame Number

50
60
Frame Number

60
50
Frame Number

t
n
e
i
c
i
f
f
e
o
C
 
C
C
F
M
 
d
r
3

100

100

100

-20

-20

90

90

20

20

30

80

90

20

30

30

40

70

80

70

80

10

40

0

10

20

0

20

0

40

70

0

E lat ion

Des pair

E lat ion

Des pair

Des pair

E lat ion

Formants and related features 
Tracking  formants  over  time  is  used  to  model  the  change  in  the  vocal  tract  shape.  The  use  of  Linear 
Predictive Coding  (LPC)  to model  formants  is widely used  in  speech  synthesis  [9]. Prior work done by 
Petrushin  suggests  that  formants  carry  information  about  emotional  content  [10].  The  first  three 
formants  and  their  bandwidths  were  estimated  using  LPC  on  15ms  frames  of  speech.  For  each  of  the 
three  formants,  their  derivatives  and  bandwidths,  we  calculated  the  mean,  variance,  maximum  and 
minimum across all frames. We also calculate the mean, variance, maximum and minimum of the mean 
of each formant frequency, its derivative and bandwidth. The formant feature vector is 48-dimensional.  

4. Classification 
We  tried  to  differentiate  between  “opposing”  emotional  states.  Six  different  “opposing”  emotion  pairs 
were chosen: despair and elation, happy and  sadness,  interest  and boredom,  shame  and pride, hot anger 
and elation, and cold anger and sadness.  
 
For  each  emotion  pair,  we  formed  data  sets  comprising  of  emotional  speech  from  all  speakers,  only 
male speakers, and only female speakers because the  features are affected by  the gender of  the  speaker. 
For  example,  the  pitch  of  males  ranges  from  80Hz  to  200Hz  while  the  pitch  of  females  ranges  from 
150Hz to 350Hz [11]. This corresponds to a total of eighteen unique data sets.  
 
For  each  data  set, we  formed  inputs  to  our  classification  algorithm  comprising  of  feature  vectors  from: 
Pitch only, MFCCs only, Formants only, Pitch & MFCCs, Pitch & Formants, MFCCs & Formants, and 
Pitch,  MFCCs  &  Formants.  Hence,  for  each  emotion  pair,  the  classification  algorithm  was  run  on 
twenty one different sets of inputs.  

K-Means Clustering 
For  each  emotion  pair,  all  input  sets  were  clustered  using  K-Means  clustering  (k  =  2)  for  all  twelve 
combinations of the parameters listed below: 
Distance Measure Minimized:  Squared Euclidean,  L1  norm, Correlation,  and Cosine  (the Correlation 
and Cosine distance measures used here are as defined in the MATLAB ‘kmeans’ function).  
Initial  Cluster  Centroids:  Random  Centroids  and  User  Defined  Centroids  (UDC).  A  UDC  is  the 
centroid that minimizes the distance measure for the input features of one emotion in the emotion pair.  
Maximum Number of  Iterations: 1 (only when the initial cluster centroid is a UDC) and 100 (for both 
Random and UDC centroids).  
 
The  error  used  to  obtain  the  recognition  accuracy  is  the  average  of  the  training  errors  obtained  by  10-
fold  cross  validation  and  is  an  estimate  of  the  generalization  error.  The  variance  of  the  recognition 
accuracy  is  the  variance  of  these  training  errors. For  each  experiment,  the  highest  recognition  accuracy 
achieved, its variance, the inputs features and clustering parameters used, is listed in Table 2. 
 

 

 
 
 
 
 
All Speakers 
Iterations  Recognition Accuracy  Variance 
Centroid 
Distance Measure 
Features 
Experiment 
1.74% 
75.76% 
100 
UDC 
L1 norm 
MFCC 
despair-elation 
14.34% 
77.91% 
1 
UDC 
L1 norm  
MFCC 
happy-sadness 
100 
71.21% 
2.48% 
UDC 
L1 norm 
Pitch 
interest-boredom 
3.23% 
73.15% 
1 
UDC 
L1 norm 
MFCC 
shame-pride 
10.75% 
69.70% 
1 
UDC 
L1 norm 
MFCC 
hot anger-elation 
3.35% 
1 
75.66% 
UDC 
L1 norm 
MFCC 
cold anger-sadness 
 
 
 
 
 
 
Male Speakers 
Iterations  Recognition Accuracy  Variance 
Centroid 
Distance Measure 
Features 
Experiment 
0.14% 
87.80% 
1 
UDC 
Correlation 
MFCC & Pitch 
despair-elation 
3.66% 
88.80% 
1 
UDC 
L1 norm 
happy-sadness 
MFCC 
100 
81.20% 
6.36% 
Random 
Cosine 
interest-boredom  MFCC & Pitch 
15.53% 
74.24% 
1 
UDC 
Correlation 
MFCC & Pitch 
shame-pride 
14.95% 
65.89% 
1 
UDC 
L1 norm 
MFCC 
hot anger-elation 
9.78% 
1 
88.43% 
UDC 
L1 norm 
MFCC 
cold anger-sadness 
 
 
 
 
 
 
Female Speakers 
Iterations  Recognition Accuracy  Variance 
Centroid 
Distance Measure 
Features 
Experiment 
9.66% 
80.42% 
1 
UDC 
L1 norm 
MFCC 
despair-elation 
15.24% 
72.80% 
1 
UDC 
L1 norm 
MFCC 
happy-sadness 
1 
70.62% 
18.06% 
UDC 
L1 norm 
interest-boredom 
MFCC 
19.79% 
81.18% 
1 
UDC 
L1 norm 
MFCC 
shame-pride 
4.37% 
77.16% 
1 
UDC 
L1 norm 
MFCC 
hot anger-elation 
MFCC 
1 
72.04% 
15.00% 
UDC 
Correlation 
cold anger-sadness 
Table 2: Highest Recognition Accuracies using K-means Clustering  

Support Vector Machines (SVMs) 
A  modified  version  of  the  2-class  SVM  classifier  in  Schwaighofer’s  SVM  Toolbox  [12]  was  used  to 
classify all input sets of each emotion pair. The two kernels used and their parameters are: 
1. Linear Kernel  (with parameter C,  corresponding  to  the upper  bound  for  the  coefficients αi’s,  ranges 
from 0.1-100, with multiplicative step 10). 
2.  Radial  Basis  Function  (RBF)  Kernel  (parameter  C,  corresponding  to  the  upper  bound  for  the 

coefficients αi’s, ranges from 0.1-10, with multiplicative step 2 )  
 
The  recognition  accuracy  and  variance  was  calculated  using  the  same  technique  as  for  K-means.  For 
each  experiment,  the  highest  recognition  accuracy  achieved,  its  variance,  the  inputs  features  and 
clustering parameters used is listed in Table 3. 
 

All Speakers 
Experiment 
despair-elation 
happy-sadness 
interest-boredom 
shame-pride 
hot anger-elation 
cold anger-sadness 

Male Speakers 
Experiment 
despair-elation 
happy-sadness 
interest-boredom 
shame-pride 
hot anger-elation 
cold anger-sadness 

Female Speakers 
Experiment 
despair-elation 
happy-sadness 
interest-boredom 
shame-pride 
hot anger-elation 
cold anger-sadness 

 
 
 
 
 
C 
Recognition Accuracy  Variance 
Kernel 
Features 
6.4 
5.52% 
83.44% 
RBF Kernel 
MFCC 
1 
19.65% 
72.50% 
Linear Kernel 
MFCC 
10 
17.68% 
65.15% 
Linear Kernel 
MFCC + Pitch 
1.6 
7.93% 
76.55% 
RBF Kernel 
MFCC 
1 
19.79% 
60.00% 
Linear Kernel 
MFCC + Pitch 
6.4 
4.10% 
86.00% 
RBF Kernel 
MFCC 
 
 
 
 
 
C 
Recognition Accuracy  Variance 
Kernel 
Features 
1 
6.57% 
96.67% 
Linear Kernel 
MFCC + Pitch 
1 
99.17% 
24.55% 
Linear Kernel 
MFCC 
10 
22.78% 
96.15% 
Linear Kernel 
MFCC + Pitch 
100 
24.59% 
91.54% 
Linear Kernel 
MFCC + Pitch 
10 
16.22% 
90.00% 
Linear Kernel 
MFCC 
100 
96.67% 
21.25% 
Linear Kernel 
MFCC 
 
 
 
 
 
C 
Recognition Accuracy  Variance 
Kernel 
Features 
1 
14.03% 
79.50% 
Linear Kernel 
MFCC + Pitch 
1 
62.00% 
11.11% 
Linear Kernel 
Pitch 
100 
11.85% 
80.53% 
Linear Kernel 
Pitch 
1 
23.61% 
76.88% 
Linear Kernel 
Pitch 
10 
13.52% 
88.75% 
Linear Kernel 
MFCC + Pitch 
10 
96.11% 
7.15% 
Linear Kernel 
MFCC 
Table 3: Highest Recognition Accuracies using 2-Class SVMs  

6. Discussion 
The results obtained by the experiments performed allow us to make the following observations. 
 
Using  the  formant  feature  vector  as  an  input  to  our  classification  algorithms,  always  results  in  sub-
optimal  recognition  accuracy.  We  can  infer  that  formant  features  do  not  carry  much  emotional 
information. Since formants are used  to model  the  resonance  frequencies  (and shape) of  the vocal  tract, 
we can postulate that different emotions do not significantly affect the vocal tract shape. 
 
Using  Squared  Euclidean  as  a  distance measure  for K-means  always  results  in  sub-optimal  recognition 
accuracy. Using this distance metric effectively places a lot of weight on the magnitude of an element in 
the feature vector. Hence, an input feature that might vary a lot between the two opposing emotions may 
be discounted by this distance measure, if the mean of this feature is smaller than that of other features.  
 
Tables  2  and  3  indicate  that  the  recognition  accuracy  is  higher  when  the  emotion  pairs  of  male  and 
female  speakers  were  classified  separately.  We  postulate  two  reasons  for  this  behavior.  First,  using  a 
larger  number  of  speakers  (as  in  the  all  speaker  case)  increases  the  variability  associated  with  the 

features, thereby hindering correct classification. Second, since MFCCs are used for speaker recognition, 
we hypothesize that the features also carry information relating to the identity of the speaker. In addition 
to  emotional  content  MFCCs  and  Pitch  also  carry  information  about  the  gender  of  the  speaker.  This 
additional information is unrelated to emotion and increases misclassification. 
 
Tables  2  and  3  also  suggest  that  the  recognition  rate  for  female  speakers  is  lower  than  male  speakers 
when  classifying  emotional  states  of  elation,  happiness  and  interest.  The  higher  number  of  female 
speakers  than  male  speakers  in  our  data  set  may  contribute  to  this  lower  recognition  accuracy.  Further 
investigation  suggested  that  in  excitable  emotional  states  such  as  interest,  elation  and  happiness,  the 
variance of the Pitch and MFCCs increases significantly. However, variance of  the Pitch and MFCCs is 
higher  for  female  voices  than  male  voices.  Hence,  this  increase  in  variance  is  masked  by  the  natural 
variance  in  female voices, which  could make  the  features  less  effective at correctly  classifying  agitated 
emotions in female speakers. 
 
Of  all  the  methods  implemented,  SVMs  with  a  linear  kernel  give  us  the  best  results  for  single-gender 
classification,  especially  in  male  speakers.  This  indicates  that  this  feature  space  is  almost  linearly 
separable. The best  results using K-Means classification are usually obtained when the cluster centroids 
are UDCs which we think indicates that unsupervised  learning algorithms such as K-Means cannot pick 
up on all the information contained in the feature sets, unless we add some bias to the features.  

7. Conclusion & Future Work 
Although  it  is  impossible  to  accurately  compare  recognition  accuracies  from  this  study  to  other  studies 
because  of  the  different  data  sets  used,  the  methods  implemented  here  are  extremely  promising.  The 
recognition  accuracies  obtained  using  SVMs  with  linear  kernels  for  male  speakers  are  higher  than  any 
other  study.  Previous  studies  have  neglected  to  separate  out  male  and  female  speakers.  This  project 
shows  that  there  is  significant  benefit  in  doing  so. Our methods  are  reasonably  accurate  at  recognizing 
emotions  in  female  and  all  speakers.  Our  project  shows  that  features  derived  from  agitated  emotions 
such as happiness, elation and interest have similar properties, as do those from more subdued emotions 
such  as  despair  and  sadness.  Hence,  ‘agitated’  and  ‘subdued’  emotion  class  can  encompass  these 
narrower emotions. This is especially useful for animating gestures of avatars in virtual worlds.   
 
This project focused on 2-way classification. The performance of these methods should be evaluated for 
multi-class classification (using multi-class SVMs and K-Means). In addition the features could be fit to 
Gaussians  and  classified  using Gaussian Mixture Models.  The  speakers  used  here  uttered  numbers  and 
dates  in  various  emotions  –  the  words  themselves  carried  to  emotional  information.  In  reality,  word 
choice  can  indicate  emotion.  MFCCs  are  widely  used  in  speech  recognition  systems  and  also  carry 
emotional  information.  Existing  speech  recognition  systems  could  be  modified  to  detect  emotions  as 
well.  To  help  improve  emotion  recognition  we  could  combine  methods  in  this  project  and  methods 
similar to the Naïve Bayes in order to take advantage of the emotional content of the words. 

8. References 
[1] http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002S28 
[2]  R.Banse,  K.R.Scherer,  “Acoustic  profiles  in  vocal  emotion  expression”,  Journal  of  Personality  and  Social  Psychology,  Vol.70,  614-
636, 1996 
[3] T.Bänziger, K.R.Scherer, “The role of intonation in emotional expression”, Speech Communication, Vol.46, 252-267, 2005 
[4]  F.Yu,  E.Chang, Y.Xu,  H.Shum,  “Emotion  detection  from  speech  to  enrich multimedia  content”,  Lecture Notes  In Computer  Science, 
Vol.2195, 550-557, 2001 
[5] D.Talkin, “A Robust Algorithm for Pitch Tracking (RAPT)”, Speech Coding & Synthesis, 1995 
[6] http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html 
[7] S.Kim, P.Georgiou, S.Lee, S.Narayanan. “Real-time emotion detection system using speech: Multi-modal fusion of different  timescale 
features”, Proceedings of IEEE Multimedia Signal Processing Workshop, Chania, Greece, 2007 
[8] http://www.ee.columbia.edu/~dpwe/resources/matlab/rastamat/ 
[9] L.R.Rabiner and B.H.Juang. “Fundamentals of Speech Recognition”, Upper Saddle River; NJ: Prentice-Hall, 1993 
[10] V.A Petrushin, “Emotional Recognition  in Speech  Signal: Experimental Study, Development,  and Application”, ICSLP-2000, Vol.2, 
222-225, 2000 
[11] L.R.Rabiner and R.W.Schafer. “Digital processing of speech signals”, Englewood Cliffs; London: Prentice-Hall, 1978 
[12] http://ida.first.fraunhofer.de/~anton/software.html 


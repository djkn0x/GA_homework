Sample complexity of testing the manifold hypothesis

Hariharan Narayanan∗
Laboratory for Information and Decision Systems
EECS, MIT
Cambridge, MA 02139
har@mit.edu
Sanjoy Mitter
Laboratory for Information and Decision Systems
EECS, MIT
Cambridge, MA 02139
mitter@mit.edu

Abstract

The hypothesis that high dimensional data tends to lie in the vicinity of a low di-
mensional manifold is the basis of a collection of methodologies termed Manifold
Learning. In this paper, we study statistical aspects of the question of ﬁtting a
manifold with a nearly optimal least squared error. Given upper bounds on the
dimension, volume, and curvature, we show that Empirical Risk Minimization
can produce a nearly optimal manifold using a number of random samples that is
independent of the ambient dimension of the space in which data lie. We obtain
an upper bound on the required number of samples that depends polynomially on
the curvature, exponentially on the intrinsic dimension, and linearly on the intrin-
sic volume. For constant error, we prove a matching minimax lower bound on the
sample complexity that shows that this dependence on intrinsic dimension, volume
2 + log 1
and curvature is unavoidable. Whether the known lower bound of O( k
2 )
δ
for the sample complexity of Empirical Risk minimization on k−means applied
to data in a unit ball of arbitrary dimension is tight, has been an open question
(cid:16) k
(cid:16)
(cid:16)
(cid:17)(cid:17)
(cid:17)
since 1997 [3]. Here  is the desired bound on the error and δ is a bound on the
probability of failure. We improve the best currently known upper bound [14] of
k , log4 k
+ log 1
2 + log 1
O( k2
2 ) to O
. Based on these results, we
min
δ

δ
2
2
2
devise a simple algorithm for k−means and another that uses a family of convex
programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional
data, where the sample complexity is independent of the ambient dimension.

1

Introduction

We are increasingly confronted with very high dimensional data in speech signals, images, gene-
expression data, and other sources. Manifold Learning can be loosely deﬁned to be a collection of
methodologies that are motivated by the belief that this hypothesis (henceforth called the manifold
hypothesis) is true. It includes a number of extensively used algorithms such as Locally Linear
Embedding [17], ISOMAP [19], Laplacian Eigenmaps [4] and Hessian Eigenmaps [8]. The sample
complexity of classiﬁcation is known to be independent of the ambient dimension [15] under the
manifold hypothesis, (assuming the decision boundary is a manifold as well,) thus obviating the
curse of dimensionality. A recent empirical study [6] of a large number of 3 × 3 images, represented
as points in R9 revealed that they approximately lie on a two dimensional manifold known as the
∗Research supported by grant CCF-0836720

1

Figure 1: Fitting a torus to data.

Klein bottle. On the other hand, knowledge that the manifold hypothesis is false with regard to
certain data would give us reason to exercise caution in applying algorithms from manifold learning
and would provide an incentive for further study.
It is thus of considerable interest to know whether given data lie in the vicinity of a low dimensional
manifold. Our primary technical results are the following.

1. We obtain uniform bounds relating the empirical squared loss and the true squared loss
over a class F consisting of manifolds whose dimensions, volumes and curvatures are
bounded in Theorems 1 and 2. These bounds imply upper bounds on the sample complexity
of Empirical Risk Minimization (ERM) that are independent of the ambient dimension,
exponential in the intrinsic dimension, polynomial in the curvature and almost linear in the
volume.
2. We obtain a minimax lower bound on the sample complexity of any rule for learning a
manifold from F in Theorem 6 showing that for a ﬁxed error, the the dependence of the
sample complexity on intrinsic dimension, curvature and volume must be at least exponen-
tial, polynomial, and linear, respectively.
(cid:16) k
(cid:16)
(cid:16)
(cid:17)(cid:17)
(cid:17)
3. We improve the best currently known upper bound [14] on the sample complexity of Em-
pirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimen-
k , log4 k
+ log 1
2 + log 1
sion from O( k2
2 ) to O
. Whether the known lower
min
δ

δ
2
2
2
2 + log 1
bound of O( k
2 ) is tight, has been an open question since 1997 [3]. Here  is the
δ
desired bound on the error and δ is a bound on the probability of failure.

One technical contribution of this paper is the use of dimensionality reduction via random projec-
tions in the proof of Theorem 5 to bound the Fat-Shattering dimension of a function class, elements
of which roughly correspond to the squared distance to a low dimensional manifold. The application
of the probabilistic method involves a projection onto a low dimensional random subspace. This is
then followed by arguments of a combinatorial nature involving the VC dimension of halfspaces,
and the Sauer-Shelah Lemma applied with respect to the low dimensional subspace. While random
projections have frequently been used in machine learning algorithms, for example in [2, 7], to our
knowledge, they have not been used as a tool to bound the complexity of a function class. We il-
lustrate the algorithmic utility of our uniform bound by devising an algorithm for k−means and a
convex programming algorithm for ﬁtting a piecewise linear curve of bounded length. For a ﬁxed
error threshold and length, the dependence on the ambient dimension is linear, which is optimal
since this is the complexity of reading the input.

2 Connections and Related work

In the context of curves, [10] proposed “Principal Curves”, where it was suggested that a natural
curve that may be ﬁt to a probability distribution is one where every point on the curve is the center
of mass of all those points to which it is the nearest point. A different deﬁnition of a principal curve
was proposed by [12], where they attempted to ﬁnd piecewise linear curves of bounded length which
minimize the expected squared distance to a random point from a distribution. This paper studies
the decay of the error rate as the number of samples tends to inﬁnity, but does not analyze the
dependence of the error rate on the ambient dimension and the bound on the length. We address this
in a more general setup in Theorem 4, and obtain sample complexity bounds that are independent of

2

the ambient dimension, and depend linearly on the bound on the length. There is a signiﬁcant amount
of recent research aimed at understanding topological aspects of data, such its homology [20, 16].
2 + log 1
It has been an open question since 1997 [3], whether the known lower bound of O( k
2 ) for
δ
the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball
(cid:16) k
(cid:17)
(cid:16)
(cid:16)
(cid:17)(cid:17)
of arbitrary dimension is tight. Here  is the desired bound on the error and δ is a bound on the
2 + log 1
probability of failure. The best currently known upper bound is O( k2
2 ) and is based on
δ
k , log4 k
+ log 1
Rademacher complexities. We improve this bound to O
, using an
min

δ
2
2
2
argument that bounds the Fat-Shattering dimension of the appropriate function class using random
projections and the Sauer-Shelah Lemma. Generalizations of principal curves to parameterized
principal manifolds in certain regularized settings have been studied in [18]. There, the sample
complexity was related to the decay of eigenvalues of a Mercer kernel associated with the regularizer.
When the manifold to be ﬁt is a set of k points (k−means), we obtain a bound on the sample
complexity s that is independent of m and depends at most linearly on k , which also leads to an
approximation algorithm with additive error, based on sub-sampling. If one allows a multiplicative
error of 4 in addition to an additive error of , a statement of this nature has been proven by Ben-
David (Theorem 7, [5]).

3 Upper bounds on the sample complexity of Empirical Risk Minimization
unit ball B in Rm , let L(M, P ) := (cid:82) d(M, x)2dP (x). Given a set of i.i.d points x = {x1 , . . . , xs }
In the remainder of the paper, C will always denote a universal constant which may differ across
the paper. For any submanifold M contained in, and probability distribution P supported on the
manifold in Merm (x) ∈ F such that (cid:80)s
from P , a tolerance  and a class of manifolds F , Empirical Risk Minimization (ERM) outputs a
i=1 d(xi , Merm )2 ≤ /2 + inf N ∈F d(xi , N )2 . Given error
parameters , δ , and a rule A that outputs a manifold in F when provided with a set of samples, we
deﬁne the sample complexity s = s(, δ, A) to be the least number such that for any probability
distribution P in the unit ball, if the result of A applied to a set of at least s i.i.d random samples
from P is N , then P [L(N , P ) < infM∈F L(M, P ) + ] > 1 − δ.

3.1 Bounded intrinsic curvature
Let M be a Riemannian manifold and let p ∈ M. Let ζ be a geodesic starting at p.
Deﬁnition 1. The ﬁrst point on ζ where ζ ceases to minimize distance is called the cut point of p
along M. The cut locus of p is the set of cut points of M. The injectivity radius is the minimum
taken over all points of the distance between the point and its cut locus. M is complete if it is
complete as a metric space.
Let Gi = Gi (d, V , λ, ι) be the family of all isometrically embedded, complete Riemannian sub-
manifolds of B having dimension less or equal to d, induced d−dimensional volume less or
(cid:16)
(cid:16)
(cid:17)(cid:17)d
equal to V , sectional curvature less or equal to λ and injectivity radius greater or equal to ι. Let
, which for brevity, we denote Uint .
Uint ( 1
d
C
 , d, V , λ, ι) := V
(cid:18) Uint
(cid:19)
(cid:19) Uint
(cid:19)
(cid:19)
(cid:18)(cid:18) 1
(cid:18)
min(,ι,λ−1/2 )
Theorem 1. Let  and δ be error parameters. If
s ≥ C
log4
, Uint
2 +
min
2

(cid:20)
(cid:21)
and x = {x1 , . . . , xs} is a set of i.i.d points from P then,
L(M, P ) < 
L(Merm (x), P ) − infM∈Gi
P
The proof of this theorem is deferred to Section 4.

> 1 − δ.

,

1
2 log

1
δ

3.2 Bounded extrinsic curvature

We will consider submanifolds of B that have the property that around each of them, for any radius
r < τ , the boundary of the set of all points within a distance r is smooth. This class of submanifolds

3

has appeared in the context of manifold learning [16, 15]. The condition number is deﬁned as
follows.
Deﬁnition 2 (Condition Number). Let M be a smooth d−dimensional submanifold of Rm . We
deﬁne the condition number c(M) to be 1
τ , where τ is the largest number to have the property that
for any r < τ no two normals of length r that are incident on M have a common point unless it is
on M.
Let Ge = Ge (d, V , τ ) be the family of Riemannian submanifolds of B having dimension ≤ d,
(cid:17)(cid:17)d
(cid:16)
(cid:16)
volume ≤ V and condition number ≤ 1
τ . Let  and δ be error parameters. Let Uext ( 1
 , d, τ ) :=
, which for brevity, we denote by Uext .
d
V
C
(cid:18)
(cid:18)(cid:18) 1
(cid:19)
(cid:18) Uext
(cid:19)
(cid:19)
(cid:19) Uext
min(,τ )
Theorem 2. If
s ≥ C
log4
2 +
min
, Uext
2

P (cid:104)L(Merm (x), P ) − infM L(M, P ) < 
(cid:105)
and x = {x1 , . . . , xs} is a set of i.i.d points from P then,
4 Relating bounded curvature to covering number

> 1 − δ.

1
2 log

(1)

1
δ

,

In this subsection, we note that that bounds on the dimension, volume, sectional curvature and
injectivity radius sufﬁce to ensure that they can be covered by relatively few −balls. Let V M
be
the volume of a ball of radius M centered around a point p. See ([9], page 51) for a proof of the
p
following theorem.
Theorem 3 (Bishop-G ¨unther Inequality). Let M be a complete Riemannian manifold and assume
(cid:16) sin(t
(cid:17)n−1
(cid:82) r
that r is not greater than the injectivity radius ι. Let KM denote the sectional curvature of M and
√
p (r) ≥ 2π
let λ > 0 be a constant. Then, KM ≤ λ implies V M
n
λ)√
2
dt.
(cid:1)d .
p () > (cid:0) 
Γ( n
2 )
0
λ
− 1
), then, V M
Proof of Theorem 1. As a consequence of Theorem 3, we obtain an upper bound of V (cid:0) C d
(cid:1)d on
Thus, if  < min(ι, πλ
2
2
C d
If {M ∩
the number of disjoint sets of the form M ∩ B/32 (p) that can be packed in M.

B/32 (p1 ), . . . , M ∩ B/32 (pk )} is a maximal family of disjoint sets of the form M ∩ B/32 (p), then
(cid:18)
(cid:19)d
there is no point p ∈ M such that min
(cid:107)p − pi (cid:107) > /16. Therefore, M is contained in the union of
B/16 (pi ). Therefore, we may apply Theorem 4 with U (cid:0) 1
(cid:1) ≤ V
balls, (cid:83)
i

i
The proof of Theorem 2 is along the lines of that of Theorem 1, so it has been deferred to the journal
version.

C d
− 1
2 ,ι)

min(,λ

.

5 Class of manifolds with a bounded covering number

In this section, we show that uniform bounds relating the empirical squares loss and the expected
squared loss can be obtained for a class of manifolds whose covering number at a different scale 
has a speciﬁed upper bound. Let U : R+ → Z+ be any integer valued function. Let G be any family
of subsets of B such that for all r > 0 every element M ∈ G can be covered using open Euclidean
balls of radius r centered around U ( 1
r ) points; let this set be ΛM (r). Note that if the subsets consist
of k−tuples of points, U (1/r) can be taken to be the constant function equal to k and we recover
(cid:12)(cid:12)(cid:12)(cid:12),
(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
the k−means question. A priori, it is unclear if
i=1 d(xi , M)2
s

− EP d(x, M)2

sup
M∈G

(2)

4

P

(3)

(4)

log4

Then,

+

,

1
2 log

1
δ

U (16/),

> 1 − δ.

− EP d(x, M)2

is a random variable, since the supremum of a set of random variables is not always a random
(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
(cid:12)(cid:12)(cid:12)(cid:12),
variable (although if the set is countable this is true). However (2) is equal to
i=1 d(xi , ΛM (1/n))2
− EP d(x, ΛM (1/n))2
n→∞ sup
lim
M∈G
s
and for each n, the supremum in the limits is over a set parameterized by U (n) points, which without
loss of generality we may take to be countable (due to the density and countability of rational points).
Thus, for a ﬁxed n, the quantity in the limits is a random variable. Since the limit as n → ∞ of a
sequence of bounded random variables is a random variable as well, (2) is a random variable too.
(cid:18) U (16/)
(cid:18) U (16/)
(cid:19)
(cid:18) 1
(cid:18)
(cid:19)
(cid:19)(cid:19)
Theorem 4. Let  and δ be error parameters. If
s ≥ C
2 min
2

(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
(cid:12)(cid:12)(cid:12)(cid:12) <
(cid:20)
(cid:21)
i=1 d(xi , M)2

sup
M∈G
s
2
Proof. For every g ∈ G , let c(g , ) = {c1 , . . . , ck } be a set of k := U (16/) points in g ⊆ B , such
that g is covered by the union of balls of radius /16 centered at these points. Thus, for any point
(cid:17)2
d2 (x, g) ≤ (cid:16) 
x ∈ B ,
+ d(x, c(g , ))
16
 mini (cid:107)x − ci (cid:107)
≤ 2
+ d(x, c(g , ))2 .
+
256
8
Since mini (cid:107)x − ci (cid:107) is less or equal to 2, the last expression is less than 
2 + d(x, c(g , ))2 .
Our proof uses the “kernel trick” in conjunction with Theorem 5. Let Φ : (x1 , . . . , xm )T (cid:55)→
2−1/2 (x1 , . . . , xm , 1)T map a point x ∈ Rm to one in Rm+1 . For each i, let ci := (ci1 , . . . , cim )T ,
(cid:107)ci (cid:107)2
and ˜ci := 2−1/2 (−ci1 , . . . , −cim ,
)T . The factor of 2−1/2 is necessitated by the fact that we
2
wish the image of a point in the unit ball to also belong to the unit ball. Given a collection of points
c := {c1 , . . . , ck } and a point x ∈ B , let fc (x) := d(x, c(g , ))2 . Then,
fc (x) = (cid:107)x(cid:107)2 + 4 min(Φ(x) · ˜c1 , . . . , Φ(x) · ˜ck ).
(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
(cid:12)(cid:12)(cid:12)(cid:12) ≤
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
For any set of s samples x1 , . . . , xs ,
i=1 (cid:107)xi (cid:107)2
(cid:80)s
− EP (cid:107)x(cid:107)2
− EP fc (x)
i=1 fc (xi )
(cid:12)(cid:12)(cid:12)(cid:12)
s
s
Φ(xi ) · ˜ci
i=1 min
i
+ 4 sup
fc∈G
s
(cid:12)(cid:12)(cid:12)(cid:12) >
(cid:21)

− EP min
i

− EP (cid:107)x(cid:107)2

Φ(x) · ˜ci

(5)

(6)

(7)

(8)

(cid:12)(cid:12)(cid:12)(cid:12).

sup
fc∈G

(9)

P

(cid:20)(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
By Hoeffding’s inequality,
i=1 (cid:107)xi (cid:107)2
s
(cid:34)
(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
which is less than δ
2 .
i=1 min
By Theorem 5, P
(cid:34)
(cid:12)(cid:12)(cid:12)(cid:12) (cid:80)s
i
sup
fc∈G
s
i=1 fc (xi )
s

Therefore, P

sup
fc∈G


4

,

< 2e−( 1
8 )s2
(cid:35)
(cid:12)(cid:12)(cid:12)(cid:12) > 
16

< δ
2 .

Φ(xi )·˜ci

− EP min
(cid:35)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ 
i
− EP fc (x)
2

Φ(x) · ˜ci

≥ 1 − δ.

5

Figure 2: Random projections are likely to preserve linear separations.

6 Bounding the Fat-Shattering dimension using random projections

,

C

1
δ

1
2 log

1
δ

Independent of m, if

The core of the uniform bounds in Theorems 1 and 2 is the following uniform bound on the minimum
of k linear functions on a ball in Rm .
Theorem 5. Let F be the set of all functions f from B := {x ∈ Rm : (cid:107)x(cid:107) ≤ 1} to R, such that for
some k vectors v1 , . . . , vk ∈ B ,
(vi · x).
f (x) := min
(cid:19)
(cid:19)
(cid:19)
(cid:18) k
(cid:18) k
(cid:18) 1
i
s ≥ C
2 log4
+
, k
2 min
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) < 
(cid:34)
(cid:35)

(cid:80)s
then
> 1 − δ.
− EP F (x)
i=1 F (xi )
P
(10)
sup
It has been open since 1997 [3], whether the known lower bound of C (cid:0) k
(cid:1) on the sample
F ∈F
s
2 log 1
2 + 1
(cid:19)
(cid:18) k2
δ
complexity s is tight. Theorem 5 in [14], uses Rademacher complexities to obtain an upper bound
of
1
1
(11)
.
2 +
2 log
C
δ
(The scenarios in [3, 14] are that of k−means, but the argument in Theorem 4 reduces k−means to
(cid:18) 1
(cid:18) k
(cid:19)
(cid:19)
(cid:18) k
(cid:19)
our setting.) Theorem 5 improves this to
2 log4
2 min
(cid:19)
(cid:18) k
(cid:18) k

by putting together (11) with a bound of
1
1
4 log4
2 log
δ

obtained using the Fat-Shattering dimension. Due to constraints on space, the details of the proof of
Theorem 5 will appear in the journal version, but the essential ideas are summarized here.
24 ) and x1 , . . . , xu be a set of vectors that is γ−shattered by F . We would like to use
Let u := fatF ( 
VC theory to bound u, but doing so directly leads to a linear dependence on the ambient dimension
, we consider a g−dimensional random
m. In order to circumvent this difﬁculty, for g := C log(u+k)
2
linear subspace and the image under an appropriately scaled orthogonal projection R of the points
2 −shatter coefﬁcient of {Rx1 , . . . , Rxu }
x1 , . . . , xu onto it. We show that the expected value of the γ
is at least 2u−1 using the Johnson-Lindenstrauss Lemma [11] and the fact that {x1 , . . . , xu} is
γ−shattered. Using Vapnik-Chervonenkis theory and the Sauer-Shelah Lemma, we then show that
2 log2 (cid:0) k
2 log2 (cid:0) k
(cid:1) . By a well-known theorem of [1], a bound of C k
(cid:1)
2 −shatter coefﬁcient cannot be more than uk(g+2) . This implies that 2u−1 ≤ uk(g+2) , allowing us
γ
24 ) ≤ C k
to conclude that fatF ( 


on fatF ( 
24 ) implies the bound in (13) on the sample complexity, which implies Theorem 5.

1
2 log
(cid:19)

, k

+

(12)

(13)

C

+

6

γx1x2x3x4Rx1Rx2Rx3Rx4γ2RandommapR7 Minimax lower bounds on the sample complexity

Let K be a universal constant whose value will be ﬁxed throughout this section. In this section, we
will state lower bounds on the number of samples needed for the minimax decision rule for learning
from high dimensional data, with high probability, a manifold with a squared loss that is within 
of the optimal. We will construct a carefully chosen prior on the space of probability distributions
and use an argument that can either be viewed as an application of the probabilistic method or of the
fact that the Minimax risk is at least the risk of a Bayes optimal manifold computed with respect to
this prior. Let U be a K 2dk−dimensional vector space containing the origin, spanned by the basis
{e1 , . . . , eK 2d k } and S be the surface of the ball of radius 1 in Rm . We assume that m be greater or
equal to K 2dk + d. Let W be the d−dimensional vector space spanned by {eK 2d k+1 , . . . , eK 2d k+d }.
is the translation of W by x. Note that each Si has radius τ . Let (cid:96) = (cid:0)K 2d k
(cid:1) and {M1 , . . . , M(cid:96)}
√
Let S1 , . . . , SK 2d k denote spheres, such that for each i, Si := S ∩ (
1 − τ 2 ei + W ), where x + W
K d k
consist of all K dk−element subsets of {S1 , . . . , SK 2d k }. Let ωd be the volume of the unit ball in
Rd . The following theorem shows that no algorithm can produce a nearly optimal manifold with
high probability unless it uses a number of samples that depends linearly on volume, exponentially
on intrinsic dimension and polynomially on the curvature.
Theorem 6. Let F be equal to either Ge (d, V , τ ) or Gi (d, V , 1
(cid:99). Let A
τ 2 , πτ ). Let k = (cid:98)
V
(cid:16) 1
(cid:17)2
5
4 τ )d
dωd (K
be an arbitrary algorithm that takes as input a set of data points x = {x1 , . . . , xk } and outputs a
manifold MA (x) in F . If  + 2δ < 1
− τ
(cid:20)
(cid:21)
√
then,
3
2
2
< 1 − δ,
L(MA (x), P ) − infM∈F L(M, P ) < 
infP
where P ranges over all distributions supported on B and x1 , . . . , xk are i.i.d draws from P .
Proof. Observe from Lemma ?? and Theorem 3 that F is a class of a manifolds such that
each manifold in F is contained in the union of K 3d
2 k m−dimensional balls of radius τ , and
{M1 , . . . , M(cid:96)} ⊆ F .
(The reason why we have K 3d
rather than K 5d
4 as in the statement of
2
the theorem is that the parameters of Gi (d, V , τ ) are intrinsic, and to transfer to the extrinsic setting
of the last sentence, one needs some leeway.) Let P1 , . . . , P(cid:96) be probability distributions that are
uniform on {M1 , . . . , M(cid:96)} with respect to the induced Riemannian measure. Suppose A is an al-
gorithm that takes as input a set of data points x = {x1 , . . . , xt} and outputs a manifold MA (x).
(cid:20)(cid:12)(cid:12)L(MA (x), P ) − infM∈F L(M, P )(cid:12)(cid:12) < 
(cid:21)
(cid:20)(cid:12)(cid:12)L(MA (x), Pr ) − infM∈F L(M, Pr )(cid:12)(cid:12) < 
(cid:21)
Let r be chosen uniformly at random from {1, . . . , (cid:96)}. Then,
(cid:20)(cid:12)(cid:12)L(MA (x), Pr ) − infM∈F L(M, Pr )(cid:12)(cid:12) < (cid:12)(cid:12)x
≤ EPr
(cid:2)L(MA (x), Pr ) < (cid:12)(cid:12)x(cid:3) .
= ExPPr
= ExPPr
We ﬁrst prove a lower bound on inf x Er [L(MA (x), Pr )|x].
(cid:2)L(MA (x), Pr )(cid:12)(cid:12)x(cid:3) = Er,xk+1
(cid:2)d(MA (x), xk+1 )2 (cid:12)(cid:12)x(cid:3) .
We see that
Er
Conditioned on x, the probability of the event (say Edif ) that xk+1 does not belong to the same
2 .
sphere as one of the x1 , . . . , xk is at least 1
Conditioned on Edif and x1 , . . . , xk , the probability that xk+1 lies on a given sphere Sj is equal to
K 2 k−k(cid:48) otherwise, where k (cid:48) ≤ k is the number of spheres in
0 if one of x1 , . . . , xk lies on Sj and
1
{Si } which contain at least one point among x1 , . . . , xk .
By construction, A(x1 , . . . , xk ) can be covered by K 3d
2 k balls of radius τ ; let their centers be
.
y1 , . . . , y
3d
2 k

(14)

Px

P

infP

K

7

P

(cid:21)

However, it is easy to check that for any dimension m, the cardinality of the set Sy of all Si that
2 k . Therefore, P (cid:104)
(cid:105)
− τ (cid:12)(cid:12)x
√
have a nonempty intersection with the balls of radius
centered around y1 , . . . , y
, is at most
1
3d
(cid:21)
(cid:20)
2
2
2 k
(cid:12)(cid:12)x
d(MA (x), xk+1 ) ≥ 1
√
K 3d
is at least
2
2
√
P [Edif ] P [xk+1 (cid:54)∈ Sy |Edif ]
≥
}, xk+1 ) ≥ 1
d({y1 , . . . , y
P
2
2
K 2dk − k (cid:48) − K 3d
≥
1
2 k
K 2dk − k (cid:48)
(cid:16) 1
(cid:17)2
2
(cid:2)d(MA (x), xk+1 )2 (cid:12)(cid:12)x(cid:3) ≥ 1
≥ 1
3 .
(cid:2)L(MA (x), Pr ) < (cid:12)(cid:12)x(cid:3) to be more than 1 − δ if inf x PPr
(cid:2)L(MA (x), Pr )(cid:12)(cid:12)x(cid:3) >
− τ
Therefore, Er,xk+1
√
. Finally, we observe that it is not pos-
3
2
2
sible for ExPPr
 + 2δ , because L(MA (x), Pr ) is bounded above by 2.

3d
2 k

K

K

8 Algorithmic implications
8.1 k−means
Applying Theorem 4 to the case when P is a distribution supported equally on n speciﬁc points (that
(cid:33)
(cid:33)
(cid:32)
(cid:32)
are part of an input) in a unit ball of Rm , we see that in order to obtain an additive  approximation
log4 (cid:0) k
(cid:1)
for the k−means problem with probability 1 − δ , it sufﬁces to sample
s ≥ C
1
1
k

2 log
2
2
δ
points uniformly at random (which would have a cost of O(s log n) if the cost of one random bit
is O(1)) and exhaustively solve k−means on the resulting subset. Supposing that a dot product
between two vectors xi , xj can be computed using ˜m operations, the total cost of sampling and
then exhaustively solving k−means on the sample is O( ˜msks log n). In contrast, if one asks for a
multiplicative (1 + ) approximation, the best running time known depends linearly on n [13]. If
P is an unknown probability distribution, the above algorithm improves upon the best results in a
natural statistical framework for clustering [5].

, k

+

8.2 Fitting piecewise linear curves

In this subsection, we illustrate the algorithmic utility of the uniform bound in Theorem 4 by ob-
taining an algorithm for ﬁtting a curve of length no more than L, to data drawn from an unknown
probability distribution P supported in B , whose sample complexity is independent of the ambient
dimension. This curve, with probability 1 − δ , achieves a mean squared error of less than  more
than the optimum. The proof of its correctness and analysis of its run-time have been deferred to the
(cid:19)
(cid:19)
(cid:18) log4 ( k
(cid:18)
journal version. The algorithm is as follows:
 (cid:101) and s ≥ C
1. Let k := (cid:100) L
 )
. Sample points x1 , . . . , xs i.i.d
2 log 1
+ 1
k
, k
2. For every permutation σ of [s], minimize the convex objective function (cid:80)n
2
2
δ
from P for s =, and set J := span({xi }s
i=1 ).
over the convex set of all s−tuples of points (y1 , . . . , ys ) in J , such that (cid:80)s−1
i=1 d(xσ(i) , yi )2
i=1 (cid:107)yi+1 −
yi (cid:107) ≤ L.
3. If the minimum over all (y1 , . . . , ys ) (and σ ) is achieved for (z1 , . . . , zs ), output the curve
obtained by joining zi to zi+1 for each i by a straight line segment.

9 Acknowledgements

We are grateful to Stephen Boyd for several helpful conversations.

8

References
[1] Noga Alon, Shai Ben-David, Nicol `o Cesa-Bianchi, and David Haussler. Scale-sensitive di-
mensions, uniform convergence, and learnability. J. ACM, 44(4):615–631, 1997.
[2] Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and
random projection. In FOCS, pages 616–623, 1999.
[3] Peter Bartlett. The minimax distortion redundancy in empirical quantizer design. IEEE Trans-
actions on Information Theory, 44:1802–1813, 1997.
[4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data
representation. Neural Comput., 15(6):1373–1396, 2003.
[5] Shai Ben-David. A framework for statistical clustering with constant time approximation al-
gorithms for k-median and k-means clustering. Mach. Learn., 66(2-3):243–257, 2007.
[6] Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46:255–
308, January 2009.
[7] Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS, pages 634–644, 1999.
[8] David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding
techniques for high-dimensional data. Proceedings of the National Academy of Sciences,
100(10):5591–5596, May 2003.
[9] A. Gray. Tubes. Addison-Wesley, 1990.
[10] Trevor J. Hastie and Werner Stuetzle. Principal curves. Journal of the American Statistical
Association, 84:502–516, 1989.
[11] William Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert
space. Contemporary Mathematics, 26:419–441, 1984.
[12] Bal ´azs K ´egl, Adam Krzyzak, Tam ´as Linder, and Kenneth Zeger. Learning and design of prin-
cipal curves. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22:281–297,
2000.
[13] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A simple linear time (1+)−approximation
algorithm for k-means clustering in any dimensions. In FOCS, pages 454–462, 2004.
[14] Andreas Maurer and Massimiliano Pontil. Generalization bounds for k-dimensional coding
schemes in hilbert spaces. In ALT, pages 79–91, 2008.
[15] H. Narayanan and P. Niyogi. On the sample complexity of learning smooth cuts on a manifold.
In Proc. of the 22nd Annual Conference on Learning Theory (COLT), June 2009.
[16] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submani-
folds with high conﬁdence from random samples. Discrete & Computational Geometry, 39(1-
3):419–441, 2008.
[17] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear
embedding. SCIENCE, 290:2323–2326, 2000.
[18] Alexander J. Smola, Sebastian Mika, Bernhard Sch ¨olkopf, and Robert C. Williamson. Regu-
larized principal manifolds. J. Mach. Learn. Res., 1:179–209, 2001.
[19] J. B. Tenenbaum, V. Silva, and J. C. Langford. A Global Geometric Framework for Nonlinear
Dimensionality Reduction. Science, 290(5500):2319–2323, 2000.
[20] Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Compu-
tational Geometry, 33(2):249–274, 2005.

9


Using the Deformable Part Model with Autoencoded Feature Descriptors
for Ob ject Detection

Hyunghoon Cho and David Wu

December 10, 2010

1

Introduction

Given its performance in recent years' PASCAL Visual Ob ject Classes VOC Challenge [1], the Deformable Part Model (DPM)
is widely regarded to be one of the state-of-the-art ob ject detection and localization algorithms. The DPM as described by
Felzenszwalb, et al. [2] uses Histogram of Oriented Gradients (HOG) descriptors [3] as the underlying feature representation
for an ob ject. In this paper, we consider using features learned by a single-layered, sparse autoencoder as a substitute for
HOG descriptors in the DPM. The rationale for this is that these learned feature descriptors may capture additional details
present in the image that are not present in a human-engineered set of features such as HOG. Using this more descriptive set
of features may in turn yield a better ob ject detector. Additionally, it is noteworthy that while deep learning algorithms such
as the sparse autoencoder alone generally do not perform as well compared to other vision algorithms, it may be possible to
integrate learned features into an existing image classication system such as the DPM. To further evaluate the performance
of such an integration, we consider the eect of factors such as block normalization and colored features on the performance
of the autoencoder-backed DPM. Finally, we also examine the performance of the autoencoder-backed DPM across several
dierent ob ject classes from the PASCAL VOC Challenge.

2 Methodology

Our training images are sampled from the PASCAL 2010 training image sets and our test images are sampled from the
PASCAL 2007 test image sets [1]. For this paper, we consider two dierent training set sizes: small (20 positives : 200
negatives) and medium (50:500).

2.1 Autoencoder Training

We train a sparse, single layered, autoencoder with a sigmoid activation function1 to obtain a set of features. We iterate
over each positive training image in the training set and crop the image to the ground truth bounding box for the image.
We convert the image to grayscale and whiten it. From this set of whitened, positive examples, we sample 400,000 8x8
image patches to train an autoencoder with 64, 100, or 128 hidden nodes. We center and standardize each patch x âˆˆ R64
according to xâˆ’Âµx
prior to training the autoencoder. We also consider colored image patches in which each 8x8 image patch
Ïƒx
now corresponds to a vector x âˆˆ R192 (64 components per color channel) and train an autoencoder with 256 hidden nodes.
In the DPM, Felzenszwalb, et al.
[2] consider the mirror image for each training example to eectively double the size of
the training set. With HOG descriptors, it is sucient to mirror the orientation of each gradient to determine the HOG
representation of the mirror image. To allow for similar functionality with the autoencoder, we compute the mirror image of
each feature learned by the original autoencoder to obtain a mirrored autoencoder with the same number of hidden units.
We concatenate the original and mirrored autoencoders to obtain a set of 128, 200, or 256 features. Finally, Felzenszwalb,
et al. [2] also incorporate one additional occlusion feature that denotes whether the full ob ject is visible in the image or not.
Likewise, we introduce one additional occlusion feature.

2.2 Model Training and Visualization

We replace the HOG feature descriptors in the DPM. At the low level, the DPM divides the image into a rigid grid of 8x8
pixel non-overlapping cells and computes a histogram for each cell. In addition, the model captures ner details (part lters)
in the ob ject by using smaller 4x4 pixel patches. We adapt this approach to work with the autoencoder features.

1Autoencoder training toolkit provided by Adam Coates

1

2.2.1 Histograms of Autoencoder Features

To determine the features that comprise a particular image patch, we determine the distribution of features in a particular
image patch x by computing g(W x + b) where g(x) = 1
1+eâˆ’x is the sigmoid activation function for the hidden layer, and W
and b are the corresponding weight and intercept terms for the inputs to the autoencoder. The resultant activation vector
expresses the degree to which each hidden node is activated by the given image patch.
In addition to using the sigmoid activation function, we consider another method of computing a distribution of features
for each image patch. In particular, we substitute a nonlinear activation function H (x) that is 0 below some threshold c1 ,
(cid:40)
and linear with slope a above the threshold. We cap the activation at a value c2 .
0
x < c1
min {a(x âˆ’ c1 ), c2} x â‰¥ c1

H (x) =

We determine our histogram of features by computing H (W x + b). In particular, we choose c1 ,c2 , and a so that the range of
values in the histogram of features is similar to that computed by HOG.
To train the DPM, we compute histograms of features for 4x4 image patches which are used by the part lters. We cannot
train a completely separate autoencoder on 4x4 patches since the features learned from 4x4 image patches do not necessarily
coincide with features learned from 8x8 image patches. We consider two dierent approaches.
In the rst approach, we
construct a 4x4 autoencoder from the 8x8 autoencoder where for each feature in the 8x8 autoencoder, we remove every other
row and column to produce a scaled down 4x4 feature. We use this autoencoder to construct the histogram of features for
4x4 image patches. In the second approach, we use only the 8x8 autoencoder. To construct the histogram for 4x4 image
patches, we scale the 4x4 image patch up to obtain an 8x8 image patch, which we then run through the autoencoder to
obtain the histogram.
We consider a simple method of incorporating color into the histograms using the existing autoencoder. When training
the model, instead of working with grayscale images, we consider each color channel separately. Now, for each feature, we
simply take the maximum activation for each feature over the three color channels and use that as the activation.
For the nal test, we consider the result of combining HOG descriptors with autoencoder features. In this scheme, we
rst compute a histogram of autoencoder features using an autoencoder with 100 hidden nodes. We then compute the HOG
descriptors and adjoin these two vectors to obtain a 232 element vector (100 autoencoder + 100 mirror + 32 HOG) that
serves as the feature descriptor for the image patch.

2.2.2 Normalization

In Dalal and Triggs' HOG implementation [3], they consider block normalization where the gradients for a patch are averaged
over neighboring blocks. Felzenszwalb, et al. has experimentally demonstrated noticeable improvements using a block
normalization procedure [2]. We consider a similar normalization method as [3]. First we calculate the energy Ei,j (the
square of the norm) of the feature histogram for each image patch (i, j ). Then we take the histogram at each patch (x, y)
(cid:33)
(cid:32)
(cid:88)
and multiply the elements by the normalization constant Cx,y where
(cid:112)Ex,y + Ex+i,y + Ex,y+j + Ex+i,y+j + 
1
i,jâˆˆ{1,âˆ’1}
where  is a small constant so we do not divide by zero. This normalization has the eect of mitigating situations where
there is a high local variation in the energy of a block.

Cx,y =

1
4

2.2.3 Model Visualization

To visualize the model, we take the image patch that maximally activates each hidden node in the autoencoder. These image
patches are given by the rows in the weight matrix for the input layer. For each patch in the model, we alpha-blend the
feature image patches using Î±i = a2
E as the opacity of the ith feature, where ai is the activation value and E is the energy of
i
the feature histogram.

2.2.4 Tuning the Latent SVM

Finally, we consider the eect of changing the regularization parameter used to train the latent SVM (LSVM) in the DPM.
The value used by Felzenszwalb, et al.
[2] works well in the case of HOG descriptors, but may not be well tuned for
autoencoder features. Hence, we consider both higher and lower values of the regularization parameter C in the LSVM
ob jective function. We evaluate by testing on both a subset of the training set as well as the test set.

2

2.3 Model Evaluation

To evaluate an ob ject model, we test on a sample of test images from the PASCAL VOC 2007 [1] test set that are similar to the
training images. We use the same scoring criterion as PASCAL: a detection is correct if and only if area[Predictedâˆ©True]
area[PredictedâˆªTrue] > 0.5
where Predicted denotes the bounding box predicted by the model and True denotes the true bounding box for the image.
If one ob ject is detected multiple times, only one is taken to be a true positive and all the other ones are taken to be
false positives. Using this information, we can compute a precision-recall curve for the detector, as well as compute a value
describing the average precision for the detector. This is the metric used by PASCAL.

3 Results and Discussion

Figure 1: Top left: sample training image of bicycle. Bottom left, middle: The same picture represented using HOG
descriptors, grayscale autoencoder features, and colored autoencoder features. Right: Grayscale (top) and colored (bottom)
autoencoder features learned from 8x8 grayscale and colored image patches of bicycles.

Figure 2: From left to right: model of bicycle using HOG descriptors, autoencoder with 100 hidden nodes, colored autoencoder
with 256 hidden nodes, and both HOG and autoencoder with 100 hidden nodes (HOG features in yellow). Top row shows
root lter and bottom row shows ner part lters. To the right, we show the precision-recall curve for the four models.

Table 1 shows the eect of various parameters on average precision of the model. With the exception of the specied
parameter we are evaluating, each model described in the table is for a bicycle model trained without block normalization
and using only the 8x8 autoencoder with 100 hidden nodes on a medium training set. Noting that the autoencoder with the
nonlinear H (x) activation function slightly outperformed models trained with the sigmoid g(x) activation function, for the
ma jority of tests, we only train and evaluate models using that particular activation function. First, we see that while there
is no sizable benet from using a larger training set in the case of HOG, there is an improvement in the autoencoder models
by using a larger training set. Therefore, one possible way of achieving better performance with the autoencoder may be

3

HOG Descriptors

Small Training Set

Medium Training Set

Block Normalization
Without Patch Standardization
Scaled Down 4x4 Autoencoder
Features over Max Color Channel
Higher Threshold for H (x) (c1 = âˆ’4)
Highest Threshold for H (x) (c1 = âˆ’3)

0.9054

0.8915

-
-
-
-
-
-

-

0.8563

Activation Function
Nonlinear H (x)
Sigmoid g(x)
0.8154 (64 Hidden Nodes)
0.8228 (100 Hidden Nodes)
0.8596 (64 Hidden Nodes)
0.8607 (100 Hidden Nodes)
0.7850
0.7499
0.7139
0.8466
0.8489
0.8235

0.8406
-
-
-
-
-

Table 1: Average precision for dierent training set sizes and methods of computing histogram of autoencoder features

to further increase the size of the training set. Interestingly, increasing the number of features did not appear to have as
substantial an impact as increasing the training set size.
Next, we consider four variations (described above) on the way we compute our feature histogram: adding block normal-
ization, removing patch standardization when training the DPM, using a scaled-down 4x4 autoencoder for computing 4x4
part features, and evaluating on each color channel separately. Contrary to expectations, the block normalization scheme
does not increase the eectiveness of the model, and in the case where we use the H (x) activation function, results in
signicantly worse performance. Similarly, removing patch standardization also decreases the performance of the model.
However, removing this additional preprocessing step signicantly increases the speed of the training by almost a factor of
two. Hence, in cases where run-time performance may be more important than the accuracy of the model, this may be a
viable trade-o. Finally, we consider the approach of using a separate 4x4 autoencoder (eectively a scaled down version
of the 8x8 autoencoder). This leads to a signicant drop in detector precision compared to the model trained using only the
8x8 autoencoder (where 4x4 image patches are scaled up to 8x8 and represented using features from the 8x8 autoencoder).
This may have been due to the fact that simply removing every other pixel from each feature in the 8x8 autoencoder is not
a reasonable way of constructing a 4x4 autoencoder and may have ended up distorting the features. Finally, we consider a
way of incorporating color similar to what was done by Felzenszwalb, et al. [2]. We use the maximum hidden node activation
value over the three color channels as the activation for a particular image patch. This appears to have a fairly insignicant,
even detrimental, eect on the performance of the model. A potentially better method of incorporating color may be to use
a separate autoencoder that incorporates colored image patches. We discuss this result later.
The nal set of tests we consider is to change the threshold used in computing the H (x) activations. Higher thresholds
would imply fewer hidden node activations for each image patch, possibly resulting in less noise in the histogram of features,
but at the same time, reduce the number of features that are expressed in the histogram. The data indicate that performance
generally deteriorated for higher values of the threshold (admitting fewer node activations).

HOG Descriptors
Autoencoder Features

Bicycle
0.8915
0.8607

Car
0.5688
0.4928

Bottle Horse
0.2467
0.6546
0.4674
0.0870

Sofa
0.5250
0.2421

Airplane
0.6263
0.5478

Table 2: Average precision values for dierent ob ject classes

Table 2 shows the average precision values for dierent ob ject models from the VOC challenge [1]. For each ob ject class,
we train the autoencoder-backed DPM with a medium training set and using H (x) as the activation function. We test both
the autoencoder-backed model and a HOG-backed model on a sample test set taken from the VOC challenge. Though the
autoencoder-backed DPM did not achieve superior performance, these results indicate that our system has the potential to
work across many dierent ob ject categories.
Table 3 above shows the average precision values for models trained with a variable number of features. Note that
for an autoencoder with n nodes, the number of features is 2n + 1 (original autoencoder features + mirrored features +
occlusion feature). In the case where we integrate the autoencoder features with the HOG descriptors, the occlusion feature
is already incorporated in the HOG features. Each of the above models are trained on the bicycle medium training set
using activation function H (x). It is interesting to note that additional features (hidden nodes) did not necessarily correlate
to better performance; there was a slight gain going from 64 hidden nodes to 100 hidden nodes, but not much thereafter.
Interestingly, incorporating color also did not yield an observable benet. It is possible that we need a larger training set
in order to further improve performance. Finally, it is noteworthy that incorporating both HOG descriptors as well as
In particular, its performance is
autoencoder features lead to an observable improvement in the detector's performance.

4

32 Features (32 HOG)
129 Features (64 Hidden Nodes)
201 Features (100 Hidden Nodes)
257 Features (128 Hidden Nodes)
232 Features (100 Hidden Nodes, 32 HOG)
513 Features (256 Hidden Nodes with Color)

Average Precision
0.8915
0.8596
0.8607
0.8388
0.8993
0.8402

Table 3: Average precision values using dierent number of features (bicycle, medium training set)

slightly higher than the HOG model trained on the same training set. This indicates that there might be features that are
captured by HOG, but are not completely captured by the autoencoder.

Regularization Parameter Average Precision (Training) Average Precision (Test)
0.8915
0.9145
0.002 (HOG)
0.001
0.8908
0.8440
0.8607
0.9076
0.002
0.8270
0.9083
0.004
0.8374
0.9107
0.006
0.02
0.9111
0.8390

Table 4: Average precision values from dierent regularization parameters (bicycle, medium training set)

In Table 4, we consider using dierent values of the regularization parameter C used to train the latent SVM desribed
in [2] to try and make the average precision on the training set converge to the average precision achieved on the test set.
In general, while increasing the value of C increases performance on the training set, performance on the test set appear
relatively unaected. The value Felzenszwalb, et. al [2] used to train the HOG-backed DPM (0.002) appears to work well in
practice for the autoencoder models. Overall, changing the regularization parameter did not appear to be very signicant as
far as performance on the test set is concerned.

4 Conclusion and Future Work

Overall, the results indicate that the autencoder-backed DPM is a viable ob ject detector. While using only autoencoder
features do not outperform the model trained using HOG features, the average precision values tend to be comparable.
Furthermore, integrating autoencoder features with HOG descriptors produces a detector whose results are actually slightly
better than those obtained by HOG alone. Results also indicate that variations such as using more hidden nodes, adding
block normalization, and varying the threshold for H (x) do not lead to noticeable gains in detector performance. Rather, the
single more important factor appears to be training set size. Current results show that we may be able to achieve substantial
improvements if we further increase the size of the training set and that is one avenue of future consideration. Another
interesting possibility for further research would be to use multiple, stacked autoencoders to learn more complex low-level
feature representations for a given ob ject. Finally, we are currently looking into optimizing the run-time performance of
the training algorithm in order to support substantially larger training sets, which should in turn, lead to better overall
performance.

5 Acknowledgments

We would like to thank Professor Andrew Ng and Adam Coates for the help and advice they provided for this pro ject.

6 References

1 M. Everingham and L. van Gool. The PASCAL Visual Ob jects Classes Challenge 2010.

http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2010/index.html.

2 P. Felzenszwalb, D. McAllester, D. Ramanan. A Discriminatively Trained, Multiscale, Deformable Part Model. Proceedings
of the IEEE CVPR 2008.

3 N. Dalal and B.Triggs. Histograms of oriented gradients for human detection. In CVPR, pages I: 886-893, 2005.

5


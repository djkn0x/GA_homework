Layer-wise analysis of deep networks with Gaussian
kernels

Gr ´egoire Montavon
Machine Learning Group
TU Berlin
gmontavon@cs.tu-berlin.de

Mikio L. Braun
Machine Learning Group
TU Berlin
mikio@cs.tu-berlin.de

Klaus-Robert M ¨uller
Machine Learning Group
TU Berlin
krm@cs.tu-berlin.de

Abstract

Deep networks can potentially express a learning problem more efﬁciently than lo-
cal learning machines. While deep networks outperform local learning machines
on some problems, it is still unclear how their nice representation emerges from
their complex structure. We present an analysis based on Gaussian kernels that
measures how the representation of the learning problem evolves layer after layer
as the deep network builds higher-level abstract representations of the input. We
use this analysis to show empirically that deep networks build progressively bet-
ter representations of the learning problem and that the best representations are
obtained when the deep network discriminates only in the last layers.

1

Introduction

Local learning machines such as nearest neighbors classi ﬁe rs, radial basis function (RBF) kernel
machines or linear classi ﬁers predict the class of new data p oints from their neighbors in the input
space. A limitation of local learning machines is that they cannot generalize beyond the notion
of continuity in the input space. This limitation becomes detrimental when the Bayes classi ﬁer
has more variations (ups and downs) than the number of labeled samples available. This situation
typically occurs on problems where an instance — let’s say, a
handwritten digit — can take various
forms due to irrelevant variation factors such as its position, its size, its thickness and more complex
deformations. These multiple factors of variation can greatly increase the complexity of the learning
problem (Bengio, 2009).

This limitation motivates the creation of learning machines that can map the input space into a
higher-level representation where regularities of higher order than simple continuity in the input
space can be expressed. Engineered feature extractors, nonlocal kernel machines (Zien et al., 2000)
or deep networks (Rumelhart et al., 1986; LeCun et al., 1998; Hinton et al., 2006; Bengio et al., 2007)
can implement these more complex regularities. Deep networks implement them by distorting the
input space so that initially distant points in the input space appear closer. Also, their multilayered
nature acts as a regularizer, allowing them to share at a given layer features computed at the previous
layer (Bengio, 2009). Understanding how the representation is built in a deep network and how to
train it efﬁciently received a lot of attention (Goodfellow et al., 2009; Larochelle et al., 2009; Erhan
et al., 2010). However, it is still unclear how their nice representation emerges from their complex
structure, in particular, how the representation evolves from layer to layer.

The main contribution of this paper is to introduce an analysis based on RBF kernels and on the
kernel principal component analysis (kPCA, Sch ¨olkopf et al., 1998) that can capture and quantify the
layer-wise evolution of the representation in a deep network. In practice, for each layer 1 ≤ l ≤ L
of the deep network, we take a small labeled dataset D , compute its image D(l) at the layer l of the
deep network and measure what dimensionality the local model built on top of D(l) must have in
order to solve the learning problem with a certain accuracy.

1

input

output

f1

f2

f3

l = 0

l = 1

l = 2

l = 3

y

y

y

y

x

f1 (x)

f2 (f1 (x))

f3 (f2 (f1 (x)))

)
d
(
e

r
o
r
r
e

)
o
d
(
e

r
o
r
r
e

l = 0
l = 1
l = 2
l = 3

dimensionality d

layer l

Figure 1: As we move from the input to the output of the deep network, better representations of
the learning problem are built. We measure this improvement with the layer-wise RBF analysis
presented in Section 2 and Section 3.2. This analysis relates the prediction error e(d) to the di-
mensionality d of a local model built at each layer of the deep network. As the data is propagated
through the deep network, lower errors are obtained with lower-dimensional local models. The plots
on the right illustrate this dynamic where the thick gray arrows indicate the forward path of the deep
network and where do is a ﬁxed number of dimensions.

We apply this novel analysis to a multilayer perceptron (MLP), a pretrained multilayer perceptron
(PMLP) and a convolutional neural network (CNN). We observe in each case that the error and the
dimensionality of the local model decrease as we propagate the dataset through the deep network.
This reveals that the deep network improves the representation of the learning problem layer after
layer. This progressive layer-wise simpli ﬁcation is illus trated in Figure 1. In addition, we observe
that the CNN and the PMLP tend to postpone the discrimination to the last layers, leading to more
transferable features and better-generalizing representations than for the simple MLP. This result
suggests that the structure of a deep network, by enforcing a separation of concerns between low-
level generic features and high-level task-speci ﬁc featur es, has an important role to play in order to
build good representations.

2 RBF analysis of a learning problem

We would like to quantify the complexity of a learning problem p(y | x) where samples are drawn
independently from a probability distribution p(x, y). A simple way to do it is to measure how many
degrees of freedom (or dimensionality d) a local model must have in order to solve the learning
problem with a certain error e. This analysis relates the dimensionality d of the local model to its
prediction error e(d).
In practice, there are many ways to deﬁne the dimensionality of a model, for example, (1) the
number of samples given to the learning machine, (2) the number of required hidden nodes of a
neural network (Murata et al., 1994), (3) the number of support vectors of a SVM or (4) the number
of leading kPCA components of the input distribution p(x) used in the model. The last option is
chosen for the following two reasons:

First, the kPCA components are added cumulatively to the prediction model as the dimensionality of
the model increases, thus offering stability, while in the case of support vector machines, previously
chosen support vectors might be dropped in favor of other support vectors in higher-dimensional
models.

Second, the leading kPCA components obtained with a ﬁnite an d typically small number of samples
n are similar to those that would be obtained in the asymptotic case where p(x, y) is fully observed
(n → ∞). This property is shown by Braun (2006) and Braun et al. (2008) in the case of a single
kernel, and by extension, in the case of a ﬁnite set of kernels .

This last property is particularly useful since p(x, y) is unknown and only a ﬁnite number of observa-
tions are available. The analysis presented here is strongly inspired from the relevant dimensionality
estimation (RDE) method of Braun et al. (2008) and is illustrated in Figure 2 for a small two-

2

d = 1
e(d) = 0.5

d = 2
e(d) = 0.25

d = 3
e(d) = 0.25

d = 4
e(d) = 0

d = 5
e(d) = 0

d = 6
e(d) = 0

Figure 2: Illustration of the RBF analysis on a toy dataset of 12 samples. As we add more and more
leading kPCA components, the model becomes more ﬂexible, cr eating a better decision boundary.
Note that with four leading kPCA components out of the 12 kPCA components, all the samples are
already classi ﬁed perfectly.

dimensional toy example. In the next lines, we present the computation steps required to estimate
the error as a function of the dimensionality.
Let {(x1 , y1 ), . . . , (xn , yn )} be a dataset of n points drawn independently from p(x, y) where yi is
an indicator vector having value 1 at the index corresponding to the class of xi and 0 elsewhere. Let
X = (x1 , . . . , xn ) and Y = (y1 , . . . , yn ) be the matrices associated to the inputs and labels of the
dataset. We compute the kernel matrix K associated to the dataset:
kx − x′ k2
2σ2 (cid:19) .
where k(x, x′ ) = exp (cid:18)−
The kPCA components u1 , . . . , un are obtained by performing an eigendecomposition of K where
eigenvectors u1 , . . . , un have unit length and eigenvalues λ1 , . . . , λn are sorted by decreasing mag-
nitude:

[K ]ij = k(xi , xj )

K = (u1 | . . . |un ) · diag(λ1 , . . . , λn ) · (u1 | . . . |un )⊤

Let ˆU = (u1 | . . . |ud ) and ˆΛ = diag(λ1 , . . . , λd ) be a d-dimensional approximation of the eigende-
composition. We ﬁt a linear model β ⋆ that maps the projection on the d leading components of the
training data to the log-likelihood of the classes

2
β ⋆ = argminβ || exp( ˆU ˆU ⊤β ) − Y ||
F
where β is a matrix of same size as Y and where the exponential function is applied element-wise.
The predicted class log-probability log( ˆy) of a test point (x, y) is computed as

log( ˆy) = k(x, X ) ˆU ˆΛ−1 ˆU ⊤β ⋆ + C

where k(x, X ) is a matrix of size 1 × n computing the similarities between the new point and each
training point and where C is a normalization constant. The test error is deﬁned as:

e(d) = Pr(argmax ˆy 6= argmax y)

The training and test error can be used as an approximation bound for the asymptotic case n → ∞
where the data would be projected on the real eigenvectors of the input distribution. In the next
sections, the training and test error are depicted respectively as dotted and solid lines in Figure 3 and
as the bottom and the top of error bars in Figure 4. For each dimension, the kernel scale parameter σ
that minimizes e(d) is retained, leading to a different kernel for each dimensionality. The rationale
for taking a different kernel for each model is that the optimal scale parameter typically shrinks as
more leading components of the input distribution are observed.

3 Methodology

In order to test our two hypotheses (the progressive emergence of good representations in deep
networks and the role of the structure for postponing discrimination), we consider three deep net-
works of interest, namely a convolutional neural network (CNN), a multilayer perceptron (MLP)
and a variant of the multilayer perceptron pretrained in an unsupervised fashion with a deep belief

3

network (PMLP). These three deep networks are chosen in order to evaluate how the two types of
regularizers implemented respectively by the CNN and the PMLP impact on the evolution of the
representation layer after layer. We describe how they are built, how they are trained and how they
are analyzed layer-wise with the RBF analysis described in Section 2.

The multilayer perceptron (MLP) is a deep network obtained by alternating linear transformations
and element-wise nonlinearities. Each layer maps an input vector of size m into an output vector
of size n and consists of (1) a linear transformation linearm→n (x) = w · x + b where w is a
weight matrix of size n × m learned from the data and (2) a non-linearity applied element-wise
to the output of the linear transformation. Our implementation of the MLP maps two-dimensional
images of 28 × 28 pixels into a vector of size 10 (the 10 possible digits) by applying successively
the following functions:

f1 (x) = tanh(linear28×28→784 (x))
f2 (x) = tanh(linear784→784 (x))
f3 (x) = tanh(linear784→784 (x))
f4 (x) = softmax(linear784→10 (x))

The pretrained multilayer perceptron (Hinton et al., 2006) that we abbreviate PMLP in this paper
is a variant of the MLP where weights are initialized with a deep belief network (DBN, Hinton
et al., 2006) using an unsupervised greedy layer-wise pretraining procedure. This particular weight
initialization acts as a regularizer, allowing to learn better-generalizing representation of the learning
problem than the simple MLP.

The convolutional neural network (CNN, LeCun et al., 1998) is a deep network obtained by al-
ternating convolution ﬁlters y = convolvea×b
m→n (x) transforming a set of m input features maps
{x1 , . . . , xm } into a set of n output features maps {yi = Pm
j=1 wij ⋆ xj + bi , i = 1 . . . , n} where
the convolution ﬁlters wij of size a × b are learned from data, and pooling units subsampling each
feature map by a factor two. Our implementation maps images of 32 × 32 pixels into a vector of
size 10 (the 10 possible digits) by applying successively the following functions:
f1 (x) = tanh(pool(convolve5×5
1→36 (x)))
f2 (x) = tanh(pool(convolve5×5
36→36 (x)))
f3 (x) = tanh(linear5×5×36→400 (x))
f4 (x) = softmax(linear400→10 (x))
The CNN is inspired by the structure of biological visual systems (Hubel and Wiesel, 1962). It
combines three ideas into a single architecture: (1) only local connections between neighboring
pixels are allowed, (2) the convolution operator applies the same ﬁlter over the whole feature map
and (3) a pooling mechanism at the top of each convolution ﬁlt er adds robustness to input distortion.
These mechanisms act as a regularizer on images and other types of sequential data, and learn well-
generalizing models from few data points.

3.1 Training the deep networks

Each deep network is trained on the MNIST handwriting digit recognition dataset (LeCun et al.,
1998). The MNIST dataset consists of predicting the digit 0 – 9 from scanned handwritten digits of
28 × 28 pixels. We partition randomly the MNIST training set in three subsets of 45000, 5000 and
10000 samples that are respectively used for training the deep network, selecting the parameters of
the deep network and performing the RBF analysis.

We consider three training procedures:

1. No training: the weights of the deep network are left at their initial value. If the deep
network hasn’t received unsupervised pretraining, the weights are set randomly according
to a normal distribution N (0, γ−1 ) where γ denotes for a given layer the number of input
nodes that are connected to a single output node.
2. Training on an alternate task: the deep network is trained on a binary classi ﬁcation task t hat
consists of determining whether the digit is original (positive example) or whether it has

4

been transformed by one of the 11 possible rotation/ ﬂip comb inations that differs from the
original (negative example). This problem has therefore 540000 labeled samples (45000
positives and 495000 negatives). The goal of training a deep network on an alternate task
is to learn features on a problem where the number of labeled samples is abundant and then
reuse these features to learn the target task that has typically few labels. In the alternate task
described earlier, negative examples form a cloud around the manifold of positive examples
and learning this manifold potentially allows the deep network to learn features that can be
transfered to the digit recognition task.

3. Training on the target task: the deep network is trained on the digit recognition task using
the 45000 labeled training samples.

These procedures are chosen in order to assess the forming of good representations in deep networks
and to test the role of the structure of deep networks on different aspects of learning, such as the
effectiveness of random projections, the transferability of features from one task to another and the
generalization to new samples of the same distribution.

3.2 Applying the RBF analysis to deep networks

In this section, we explain how the RBF analysis described in Section 2 is applied to analyze layer-
wise the deep networks presented in Section 3.
Let f = fL ◦ · · · ◦ f1 be the trained deep network of depth L. Let D be the analysis dataset containing
the 10000 samples of the MNIST dataset on which the deep network hasn’t been trained. For each
layer, we build a new dataset D(l) corresponding to the mapping of the original dataset D to the l
ﬁrst layers of the deep network. Note that by deﬁnition, the i
ndex zero corresponds to the raw input
data (mapped through zero layers):
D(l) = (cid:26) D
{(fl ◦ · · · ◦ f1 (x), t) | (x, t) ∈ D)}
Then, for each dataset D(0) , . . . , D(L) we perform the RBF analysis described in Section 2. We use
n = 2500 samples for computing the eigenvectors and the remaining 7500 samples to estimate the
prediction error of the model. This analysis yields for each dataset D(l) the error as a function of the
dimensionality of the model e(d). A typical evolution of e(d) is depicted in Figure 1.
The goal of this analysis is to observe the evolution of e(d) layer after layer for the deep networks
and training procedures presented in Section 3 and to test the two hypotheses formulated in Section 1
(the progressive emergence of good representations in deep networks and the role of the structure
for postponing discrimination). The interest of using a local model to solve the learning problem
is that the local models are blind with respect to possibly better representations that could be ob-
tained in previous or subsequent layers. This local scoping property allows for ﬁne isolation of the
representations in the deep network. The need for local scoping also arises when “debugging” deep
architectures. Sometimes, deep architectures perform reasonably well even when the ﬁrst layers do
something wrong. This analysis is therefore able to detect these “bugs ”.

l = 0 ,
1 ≤ l ≤ L .

The size n of the dataset is selected so that it is large enough to approximate well the asymptotic
case (n → ∞) but also be small enough so that computing the eigendecomposition of the kernel
matrix of size n × n is fast. We choose a set of scale parameters for the RBF kernel corresponding
to the 0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95 and 0.99 quantiles of the distribution of distances
between pairs of data points.

4 Results

Layer-wise evolution of the error e(d) is plotted in Figure 3 in the supervised training case. The
layer-wise evolution of the error when d is ﬁxed to 16 dimensions is plotted in Figure 4. Both ﬁgures
capture the simultaneous reduction of error and dimensionality performed by the deep network when
trained on the target task.
In particular, they illustrate that in the last layers, a few number of
dimensions is sufﬁcient to build a good model of the target ta sk.

5

Figure 3: Layer-wise evolution of the error e(d) when the deep network has been trained on the
target task. The solid line and the dotted line represent respectively the test error and the training
error. As the data distribution is mapped through more and more layers, more accurate and lower-
dimensional models of the learning problem can be obtained.

From these results, we ﬁrst demonstrate some properties of d eep networks trained on an “asymp-
totically” large number of samples. Then, we demonstrate th e important role of structure in deep
networks.

4.1 Asymptotic properties of deep networks

When the deep network is trained on the target task with an “asy mptotically” large number of sam-
ples (45000 samples) compared to the number of dimensions of the local model, the deep network
builds representations layer after layer in which a low number of dimensions can create more accu-
rate models of the learning problem.

This asymptotic property of deep networks should not be thought of as a statistical superiority of
deep networks over local models. Indeed, it is still possible that a higher-dimensional local model
applied directly on the raw data performs as well as a local model applied at the output of the deep
network. Instead, this asymptotic property has the following consequence:

Despite the internal complexity of deep networks a local interpretation of the representation is pos-
sible at each stage of the processing. This means that deep networks do not explode the original data
distribution into a statistically intractable distribution before recombining everything at the output,
but instead, apply controlled distortions and reductions of the input space that preserve the statistical
tractability of the data distribution at every layer.

4.2 Role of the structure of deep networks

We can observe in Figure 4 (left) that even when the convolutional neural network (CNN) and the
pretrained MLP (PMLP) have not received supervised training, the ﬁrst layers slightly improve the
representation with respect to the target task. On the other hand, the representation built by a simple
MLP with random weights degrades layer after layer. This observation highlights the structural
prior encoded by the CNN: by convolving the input with several random convolution ﬁlters and
subsampling subsequent feature maps by a factor two, we obtain a random projection of the input
data that outperforms the implicit projection performed by an RBF kernel in terms of task relevance.
This observation closely relates to results obtained in (Ranzato et al., 2007; Jarrett et al., 2009) where
it is observed that training the deep network while keeping random weights in the ﬁrst layers still
allows for good predictions by the subsequent layers. In the case of the PMLP, the successive layers
progressively disentangle the factors of variation (Hinton and Salakhutdinov, 2006; Bengio, 2009)
and simplify the learning problem.

We can observe in Figure 4 (middle) that the phenomenon is even clearer when the CNN and the
PMLP are trained on an alternate task: they are able to create generic features in the ﬁrst layers
that transfer well to the target task. This observation suggests that the structure embedded in the
CNN and the PMLP enforces a separation of concerns between the ﬁrst layers that encode low-
level features, for example, edge detectors, and the last layers that encode high-level task-speci ﬁc

6

Figure 4: Evolution of the error e(do ) as a function of the layer l when do has been ﬁxed to 16
dimensions. The top and the bottom of the error bars represent respectively the test error and the
training error of the local model.

MLP, alternate task
MLP, target task
PMLP, alternate task
PMLP, target task
CNN, alternate task
CNN, target task

Figure 5: Leading components of the weights (receptive ﬁeld s) obtained in the ﬁrst layer of each
architecture. The ﬁlters learned by the CNN and the pretrain ed MLP are richer than the ﬁlters
learned by the MLP. The ﬁrst component of the MLP trained on th e alternate task dominates all
other components and prevents good transfer on the target task.

features. On the other hand, the standard MLP trained on the alternate task leads to a degradation of
representations. This degradation is even higher than in the case of random weights, despite all the
prior knowledge on pixel neighborhood contained implicitly in the alternate task.

Figure 5 shows that the MLP builds receptive ﬁelds that are sp atially informative but dissimilar
between the two tasks. The fact that receptive ﬁelds are diff erent for each task indicates that the
MLP tries to discriminate already in the ﬁrst layers. The abs ence of a built-in separation of concerns
between low-level and high-level feature extractors seems to be a reason for the inability to learn
transferable features.
It indicates that end-to-end transfer learning on unstructured learning ma-
chines is in general not appropriate and supports the recent success of transfer learning on restricted
portions of the deep network (Collobert and Weston, 2008; Weston et al., 2008) or on structured
deep networks (Mobahi et al., 2009).

When the deep networks are trained on the target task, the CNN and the PMLP solve the problem
differently as the MLP. In Figure 4 (right), we can observe that the CNN and the PMLP tend to
postpone the discrimination to the last layers while the MLP starts to discriminate already in the ﬁrst
layers. This result suggests that again, the structure contained in the CNN and the PMLP enforces
a separation of concerns between the ﬁrst layers encoding lo w-level generic features and the last
layers encoding high-level task-speci ﬁc features. This se paration of concerns might explain the
better generalization of the CNN and PMLP observed respectively in (LeCun et al., 1998; Hinton
et al., 2006). It also rejoins the ﬁndings of Larochelle et al
. (2009) showing that the pretraining of the
PMLP must be unsupervised and not supervised in order to build well-generalizing representations.

5 Conclusion

We present a layer-wise analysis of deep networks based on RBF kernels. This analysis estimates
for each layer of the deep network the number of dimensions that is necessary in order to model well
a learning problem based on the representation obtained at the output of this layer.

7

We observe that a properly trained deep network creates representations layer after layer in which a
more accurate and lower-dimensional local model of the learning problem can be built.

We also observe that despite a steady improvement of representations for each architecture of interest
(the CNN, the MLP and the pretrained MLP), they do not solve the problem in the same way: the
CNN and the pretrained MLP seem to separate concerns by building low-level generic features in
the ﬁrst layers and high-level task-speci ﬁc features in the
last layers while the MLP does not enforce
this separation. This observation emphasizes the limitations of black box transfer learning and, more
generally, of black box training of deep architectures.

References
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks.
In Advances in Neural Information Processing Systems 19, pages 153–160. MIT Press, 2007.
Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning,
2(1):1–127, 2009.
Mikio L. Braun. Accurate bounds for the eigenvalues of the kernel matrix. Journal of Machine
Learning Research, 7:2303–2328, Nov 2006.
Mikio L. Braun, Joachim Buhmann, and Klaus-Robert M ¨uller. On relevant dimensions in kernel
feature spaces. Journal of Machine Learning Research, 9:1875–1908, Aug 2008.
R. Collobert and J. Weston. A uni ﬁed architecture for natura l language processing: Deep neural
networks with multitask learning.
In International Conference on Machine Learning, ICML,
2008.
Dumitru Erhan, Yoshua Bengio, Aaron C. Courville, Pierre-Antoine Manzagol, Pascal Vincent, and
Samy Bengio. Why does unsupervised pre-training help deep learning?
Journal of Machine
Learning Research, 11:625–660, 2010.
Ian Goodfellow, Quoc Le, Andrew Saxe, and Andrew Y. Ng. Measuring invariances in deep net-
works. In Advances in Neural Information Processing Systems 22, pages 646–654, 2009.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507, July 2006.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527–1554, 2006.
D. H. Hubel and T. N. Wiesel. Receptive ﬁelds, binocular inte raction and functional architecture in
the cat’s visual cortex. The Journal of physiology, 160:106–154, January 1962.
Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best multi-
stage architecture for object recognition? In Proc. International Conference on Computer Vision
(ICCV’09). IEEE, 2009.
Hugo Larochelle, Yoshua Bengio, J ´er ˆome Louradour, and Pascal Lamblin. Exploring strategies for
training deep neural networks. J. Mach. Learn. Res., 10:1–40, 2009. ISSN 1532-4435.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(1):2278–2324, November 1998.
Hossein Mobahi, Ronan Collobert, and Jason Weston. Deep learning from temporal coherence
in video. In L ´eon Bottou and Michael Littman, editors, Proceedings of the 26th International
Conference on Machine Learning, pages 737–744, Montreal, June 2009. Omnipress.
Noboru Murata, Shuji Yoshizawa, and Shun ichi Amari. Network information criterion - determin-
ing the number of hidden units for an arti ﬁcial neural networ k model.
IEEE Transactions on
Neural Networks, 5:865–872, 1994.
Genevieve B. Orr and Klaus-Robert M ¨uller, editors. Neural Networks: Tricks of the Trade, this book
is an outgrowth of a 1996 NIPS workshop, volume 1524 of Lecture Notes in Computer Science,
1998. Springer.
M. A. Ranzato, Fu J. Huang, Y. L. Boureau, and Y. LeCun. Unsupervised learning of invariant fea-
ture hierarchies with applications to object recognition. In Computer Vision and Pattern Recog-
nition, 2007. CVPR ’07. IEEE Conference on, pages 1–8, 2007.

8

D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. Nature, 323(6088):533–536, 1986.
Bernhard Sch ¨olkopf, Alexander Smola, and Klaus-Robert M ¨uller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural Comput., 10(5):1299–1319, 1998.
Jason Weston, Fr ´ed ´eric Ratle, and Ronan Collobert. Deep learning via semi-supervised embedding.
In ICML ’08: Proceedings of the 25th international conference on Machine learning, pages 1168–
1175, 2008.
Alexander Zien, Gunnar R ¨atsch, Sebastian Mika, Bernhard Sch ¨olkopf, Thomas Lengauer, and
Klaus-Robert M ¨uller. Engineering support vector machine kernels that recognize translation ini-
tiation sites. Bioinformatics, 16(9):799–807, 2000.

9


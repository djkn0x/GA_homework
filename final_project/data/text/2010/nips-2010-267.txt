Multiple Kernel Learning and the SMO Algorithm

S. V. N. Vishwanathan, Zhaonan Sun, Nawanol Theera-Ampornpunt
Purdue University
vishy@stat.purdue.edu, sunz@stat.purdue.edu, ntheeraa@cs.purdue.edu

Manik Varma
Microsoft Research India
manik@microsoft.com

Abstract

Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more gen-
erally, linear MKL regularised by the Bregman divergence, using the Sequential
Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to
implement and adapt, and efﬁciently scales to large problems. As a result, it has
gained widespread acceptance and SVMs are routinely trained using SMO in di-
verse real world applications. Training using SMO has been a long standing goal
in MKL for the very same reasons. Unfortunately, the standard MKL dual is not
differentiable, and therefore can not be optimised using SMO style co-ordinate as-
cent. In this paper, we demonstrate that linear MKL regularised with the p-norm
squared, or with certain Bregman divergences, can indeed be trained using SMO.
The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly
faster than state-of-the-art specialised p-norm MKL solvers. We show that we can
train on a hundred thousand kernels in approximately seven minutes and on ﬁfty
thousand points in less than half an hour on a single core.

1

Introduction

Research on Multiple Kernel Learning (MKL) needs to follow a two pronged approach. It is im-
portant to explore formulations which lead to improvements in prediction accuracy. Recent trends
indicate that performance gains can be achieved by non-linear kernel combinations [7, 18, 21], learn-
ing over large kernel spaces [2] and by using general, or non-sparse, regularisation [6, 7, 12, 18].
Simultaneously, efﬁcient optimisation techniques need to be developed to scale MKL out of the lab
and into the real world. Such algorithms can help in investigating new application areas and different
facets of the MKL problem including dealing with a very large number of kernels and data points.

Optimisation using decompositional algorithms such as Sequential Minimal Optimization
(SMO) [15] has been a long standing goal in MKL [3] as the algorithms are simple, easy to im-
plement and efﬁciently scale to large problems. The hope is that they might do for MKL what SMO
did for SVMs – allow people to play with MKL on their laptops, modify and adapt it for diverse real
world applications and explore large scale settings in terms of number of kernels and data points.

Unfortunately, the standard MKL formulation, which learns a linear combination of base kernels
subject to l1 regularisation, leads to a dual which is not differentiable. SMO can not be applied as a
result and [3] had to resort to expensive Moreau-Yosida regularisation to smooth the dual. State-of-
the-art algorithms today overcome this limitation by solving an intermediate saddle point problem
rather than the dual itself [12, 16].

Our focus, in this paper, is on training p-norm MKL, with p > 1, using the SMO algorithm. More
generally, we prove that linear MKL regularised by certain Bregman divergences, can also be trained

1

using SMO. We shift the emphasis ﬁrmly back towards solving the dual in such cases. The lp -
MKL dual is shown to be differentiable and thereby amenable to co-ordinate ascent. Placing the
p-norm squared regulariser in the objective lets us efﬁciently solve the core reduced two variable
optimisation problem analytically in some cases and algorithmically in others. Using results from [4,
9], we can compute the lp -MKL Hessian, which brings into play second order variable selection
methods which tremendously speed up the rate of convergence [8]. The standard decompositional
method proof of convergence [14] to the global optimum holds with minor modiﬁcations.

The resulting optimisation algorithm, which we call SMO-MKL, is straight forward to implement
and efﬁcient. We demonstrate that SMO-MKL can be signiﬁcantly faster than the state-of-the-art
specialised p-norm solvers [12]. We empirically show that the SMO-MKL algorithm is robust with
the desirable property that it is not greatly affected within large operating ranges of p. This implies
that our algorithm is well suited for learning both sparse, and non-sparse, kernel combinations.
Furthermore, SMO-MKL scales well to large problems. We show that we can efﬁciently combine
a hundred thousand kernels in approximately seven minutes or train on ﬁfty thousand points in less
than half an hour using a single core on standard hardware where other solvers fail to produce results.
The SMO-MKL code can be downloaded from [20].

2 Related Work

Recent trends indicate that there are three promising directions of research for obtaining performance
improvements using MKL. The ﬁrst involves learning non-linear kernel combinations. A framework
for learning general non-linear kernel combinations subject to general regularisation was presented
in [18]. It was demonstrated that, for feature selection, the non-linear GMKL formulation could
perform signiﬁcantly better not only as compared to linear MKL but also state-of-the-art wrapper
methods and ﬁlter methods with averaging. Very signiﬁcant performance gains in terms of pure
classiﬁcation accuracy were reported in [21] by learning a different kernel combination per data
point or cluster. Again, the results were better not only as compared to linear MKL but also baselines
such as averaging. Similar trends were observed for regression while learning polynomial kernel
combinations [7]. Other promising directions which have resulted in performance gains are sticking
to standard MKL but combining an exponentially large number of kernels [2] and linear MKL with
p-norm regularisers [6, 12]. Thus MKL based methods are beginning to deﬁne the state-of-the-art
for very competitive applications, such as object recognition on the Caltech 101 database [21] and
object detection on the PASCAL VOC 2009 challenge [19].

In terms of optimisation,
initial work on MKL leveraged general purpose SDP and QCQP
solvers [13]. The SMO+M.-Y. regularisation method of [3] was one of the ﬁrst techniques that
could efﬁciently tackle medium scale problems. This was superseded by the SILP technique of [17]
which could, very impressively, train on a million point problem with twenty kernels using paral-
lelism. Unfortunately, the method did not scale well with the number of kernels. In response, many
two-stage wrapper techniques came up [2, 10, 12, 16, 18] which could be signiﬁcantly faster when
the number of training points was reasonable but the number of kernels large. SMO could indirectly
be used in some of these cases to solve the inner SVM optimisation. The primary disadvantage of
these techniques was that they solved the inner SVM to optimality. In fact, the solution needed to
be of high enough precision so that the kernel weight gradient computation was accurate and the
algorithm converged. In addition, Armijo rule based step size selection was also very expensive and
could involve tens of inner SVM evaluations in a single line search. This was particularly expensive
since the kernel cache would be invalidated from one SVM evaluation to the next. The one big
advantage of such two-stage methods for l1 -MKL was that they could quickly identify, and discard,
the kernels with zero weights and thus scaled well with the number of kernels. Most recently, [12]
have come up with specialised p-norm solvers which make substantial gains by not solving the inner
SVM to optimality and working with a small active set to better utilise the kernel cache.

3 The l

p -MKL Formulation

The objective in MKL is to jointly learn kernel and SVM parameters from training data {(xi , yi )}.
Given a set of base kernels {Kk } and corresponding feature maps {φk }, linear MKL aims to learn
a linear combination of the base kernels as K = Pk dkKk . If the kernel weights are restricted to
2

wt
k

(4)

λ
2

dp
k )

dkαtHkα +

min
w,b,ξ≥0,d≥0

min
w,b,ξ≥0,d≥0

be non-negative, then the MKL task corresponds to learning a standard SVM in the feature space
formed by concatenating the vectors √dkφk . The primal can therefore be formulated as
λ
2
s. t. yi (Xk pdkwt
2 Xk
wk +C Xi
(Xk
dp
wt
1
kφk (xi )+ b) ≥ 1− ξi (1)
ξi +
k )
p
k
2
The regularisation on the kernel weights is necessary to prevent them from shooting off to inﬁnity.
Which regulariser one uses depends on the task at hand. In this Section, we limit ourselves to the
p-norm squared regulariser with p > 1. If it is felt that certain kernels are noisy and should be
discarded then a sparse solution can be obtained by letting p tend to unity from above. Alternatively,
if the application demands dense solutions, then larger values of p should be selected. Note that the
primal above can be made convex by substituting wk for √dkwk to get
λ
2
s. t. yi (Xk
2 Xk
wk /dk + C Xi
(Xk
dp
wt
wt
1
kφk (xi ) + b) ≥ 1 − ξi (2)
ξi +
k )
p
k
2
We ﬁrst derive an intermediate saddle point optimisation problem obtained by minimising only w,
b and ξ . The Lagrangian is
2
p −Xi
αi [yi (Xk
2 Xk
wk /dk +Xi
(Xk
wt
L = 1
kφk (xi ) + b) − 1 + ξi ] (3)
(C − βi )ξi +
Differentiating with respect to w, b and ξ to get the optimality conditions and substituting back
results in the following intermediate saddle point problem
λ
2 Xk
(Xk
1tα − 1
min
max
2
d≥0
α∈A
where A = {α|0 ≤ α ≤ C 1, 1tY α = 0}, Hk = Y Kk Y and Y is a diagonal matrix with the labels
on the diagonal. Note that most MKL methods end up optimising either this, or a very similar, saddle
point problem. To now eliminate d we again form the Lagrangian
λ
2
2 Xk
(Xk
p − Xk
L = 1tα − 1
2
2
p −1dp−1
= 0 ⇒ λ(Xk
dp
2 αtHkα
k = γk + 1
k )
2
p = Xk
dp
2 αtHkα)
dk (γk + 1
k )
λ
1
2
(Xk
(Xk
⇒ L = 1tα −
p = 1tα −
2
2λ
where 1
p + 1
q = 1. Since Hk is positive semi-deﬁnite, αtHkα ≥ 0 and since γk ≥ 0 it is clear that
the optimal value of γk is zero. Our lp -MKL dual therefore becomes
1
(Xk
1tα −
D ≡ max
8λ
α∈A
and the kernel weights can be recovered from the dual variables as
q − 1
1
2λ  Xk
(αtHkα)q!
p
1
Note that our dual objective, unlike the objective in [3], is differentiable with respect to α. The
SMO algorithm can therefore be brought to bear where two variables are selected and optimised
using gradient or Newton methods and the process repeated until convergence.

∂L
∂ dk
⇒ λ(Xk

2 αtHkα)q )
(γk + 1

(αtHkα)q )

2
q

(αtHkα)

q
p

(5)

(6)

(7)

(8)

2
p

dp
k )

dkαtHkα +

dp
k )

γk dk

(9)

(10)

dp
k )

2
q

dk =

Also note that it has sometimes been observed that l2 regularisation can provide better results than
l1 [6, 7, 12, 18]. For this special case, when p = q = 2, the reduced two variable problem can
be solved analytically. This was one of the primary motivations for choosing the p-norm squared
regulariser and placing it in the primal objective (the other was to be consistent with other p-norm
formulations [9, 11]). Had we included the regulariser as a primal constraint then the dual would
have the q -norm rather than the q -norm squared. Our dual would then be near identical to Eq. (9)
in [12]. However, it would then no longer have been possible to solve the two variable reduced
problem analytically for the 2-norm special case.

3

4 SMO-MKL Optimisation

We now develop the SMO-MKL algorithm for optimising the lp MKL dual. The algorithm has three
main components: (a) reduced variable optimisation; (b) working set selection and (c) stopping
criterion and kernel caching. We build the SMO-MKL algorithm around the LibSVM code base [5].

4.1 The Reduced Variable Optimisation

The SMO algorithm works by repeatedly choosing two variables (assumed to be α1 and α2 without
loss of generality in this Subsection) and optimising them while holding all other variables constant.
If α1 ← α1 + ∆ and α2 ← α2 + s∆, the dual simpliﬁes to
1
(Xk
∆∗ = argmax
(ak∆2 + 2bk∆ + ck )q )
(1 + s)∆ −
8λ
L≤∆≤U
where s = −y1 y2 , L = (s == +1) ? max(−α1 , −α2 )
: max(−α1 , α2 − C ), U =
: min(C − α1 , α2 ), ak = H11k + H22k + 2sH12k ,
(s == +1) ? min(C − α1 , C − α2 )
bk = αt (H:1k + sH:2k ) and ck = αtHkα. Unlike as in SMO, ∆∗ can not be found analyti-
cally for arbitrary p. Nevertheless, since this is a simple one dimensional concave optimisation
problem, we can efﬁciently ﬁnd the global optimum using a variety of methods. We tried bisection
search and Brent’s algorithm but the Newton-Raphson method worked best – partly because the one
dimensional Hessian was already available from the working set selection step.

(11)

2
q

4.2 Working Set Selection

The choice of which two variables to select for optimisation can have a big impact on training time.
Very simple strategies, such as random sampling, can have very little cost per iteration but need many
iterations to converge. First and second order working set selection techniques are more expensive
per iteration but converge in far fewer iterations.

We implement the greedy second order working set selection strategy of [8]. We do not give the
variable selection equations due to lack of space but refer the interested reader to the WSS2 method
of [8] and our source code [20]. The critical thing is that the selection of the ﬁrst (second) variable
involves computing the gradient (Hessian) of the dual. These are readily derived to be
∇αD = 1 − Xk
dkHkα = 1 − H α
1
λ Xk
∇2
αD = −H −
where ∇θk f −1 (θ) = (2 − q)θ2−2q
+ (q − 1)θ2−q
q
q
where D has been overloaded to now refer to the dual objective. Rather than compute the gradient
∇αD repeatedly, we speed up variable selection by caching, separately for each kernel, Hkα. The
cache needs to be updated every time we change α in the reduced variable optimisation. However,
since only two variables are changed, Hkα can be updated by summing along just two columns of
the kernel matrix. This involves only O(M ) work in all, where M is the number of kernels, since
the column sums can be pre-computed for each kernel. The Hessian is too expensive to cache and is
recomputed on demand.

∇θk f −1 (θ)(Hkα)(Hkα)t

αtHkα (14)

θ2q−2
k

θq−2
k

and θk =

(12)

(13)

1
2λ

4.3 Stopping Criterion and Kernel Caching

We terminate the SMO-MKL algorithm when the duality gap falls below a pre-speciﬁed threshold.
Kernel caching strategies can have a big impact on performance since kernel computations can
dominate everything else in some cases. While a few different kernel caching techniques have been
explored for SVMs, we stick to the standard one used in LibSVM [5]. A Least Recently Used
(LRU) cache is implemented as a circular queue. Each element in the queue is a pointer to a recently
accessed (common) row of each of the individual kernel matrices.

4

5 Special Cases and Extensions

We brieﬂy discuss a few special cases and extensions which impact our SMO-MKL optimisation.

5.1

2-Norm MKL

As we noted earlier, 2-norm MKL has sometimes been found to outperform MKL trained with l1
regularisation [6, 7, 12, 18]. For this special case, when p = q = 2, our dual and reduced variable
optimisation problems simplify to polynomials of degree four
1
8λ Xk
1
8λ Xk
(1 + s)∆ −
Just as in standard SMO, ∆∗ can now be found analytically by using the expressions for the roots of
a cubic. This makes our SMO-MKL algorithm particularly efﬁcient for p = 2 and our code defaults
to the analytic solver for this special case.

∆∗ = argmax
L≤∆≤U

(ak∆2 + 2bk∆ + ck )2

D2 ≡ max
α∈A

(αtHkα)2

1tα −

(15)

(16)

5.2 The Bregman Divergence as a Regulariser

The Bregman divergence generalises the squared p-norm. It is not a metric as it is not symmetric and
does not obey the triangle inequality. In this Subsection, we demonstrate that our MKL formulation
can also incorporate the Bregman divergence as a regulariser.
Let F be any differentiable, strictly convex function and f = ∇F represent its gradient. The
Bregman divergence generated by F is given by rF (d) = F (d) − F (d0 ) − (d − d0 )t f (d0 ). Note
that ∇rF (d) = f (d) − f (d0 ). Incorporating the Bregman divergence as a regulariser in our primal
objective leads to the following intermediate saddle point problem and Lagrangian
2 Xk
1tα − 1
IB ≡ min
max
d≥0
α∈A
LB = 1tα − Xk
2 αtHkα) + λrF (d)
dk (γk + 1
∇dLB = 0 ⇒ f (d) − f (d0 ) = g(α, γ )/λ
⇒ d = f −1 (f (d0 ) + g(α, γ )/λ) = f −1 (θ(α, γ ))
where g is a vector with entries gk (α, γ ) = γk + 1
2 αtHkα and θ(α, γ ) = f (d0 ) + g(α, γ )/λ.
Substituting back in the Lagrangian and discarding terms dependent on just d0 results in the dual
1tα + λ(F (f −1 (θ)) − θ t f −1 (θ))
DR ≡ max
α∈A,γ≥0
In many cases the optimal value of γ will turn out to be zero and the optimisation can efﬁciently be
carried out over α using our SMO-MKL algorithm.

dkαtHkα + λrF (d)

(18)

(17)

(21)

(19)

(20)

Generalised KL Divergence To take a concrete example, different from the p-norm squared used
thus far, we investigate the use of the generalised KL divergence as a regulariser. Choosing F (d) =
Pk dk (log(dk ) − 1) leads to the generalised KL divergence between d and d0
rKL (d) = Xk
k ) − Xk
dk + Xk
dk log(dk /d0
d0
k
Plugging in rKL in IB and following the steps above leads to the following dual problem
1tα − λ Xk
which can be optimised straight forwardly using our SMO-MKL algorithm once we plug in the
gradient and hessian information. However, discussing this further would take us too far out of the
scope of this paper. We therefore stay focused on lp -MKL for the remainder of this paper.

2λ αtHkα
1

max
α∈A

d0
k e

(22)

(23)

5

5.3 Regression and Other Loss Functions

While we have discussed MKL based classiﬁcation so far we can easily adapt our formulation to
handle other convex loss functions such as regression, novelty detection, etc. We demonstrate this
for the ǫ-insensitive loss function for regression. The primal, intermediate saddle point and ﬁnal
dual problems are given by

PR ≡

wt
k

2
p

dp
k )

λ
2

(Xk

i + ξ−
(ξ+
i ) +

2 Xk
wk /dk + C Xi
1
min
w,b,ξ±≥0,d≥0
such that ± (Xk
kφk (xi ) + b − yi ) ≤ ǫ + ξ±
wt
i
λ
2 Xk
1t (Y α − ǫ|α|) − 1
max
2
≤|α|≤C 1, 1tα=0
1
(Xk
8λ
SMO has a slightly harder time optimising DR due to the |α| term which, though in itself not
differentiable, can be gotten around by substituting α = α+ − α− at the cost of doubling the
number of dual variables.

1t (Y α − ǫ|α|) −

max
0≤|α|≤C 1, 1tα=0

IR ≡ min
d≥0

(αtKkα)q )

2
q

dkαtKkα +

(24)

(25)

(26)

(27)

2
p

dp
k )

(Xk

DR ≡

6 Experiments

In this Section, we empirically compare the performance of our proposed SMO-MKL algorithm
against the specialised lp -MKL solver of [12] which is referred to as Shogun. Code, scripts and
parameter settings were helpfully provided by the authors and we ensure that our stopping criteria
are compatible. All experiments are carried out on a single core of an AMD 2380 2.5 GHz processor
with 32 Gb RAM. Our focus in these experiments is purely on training time and speed of optimisa-
tion as the prediction accuracy improvements of lp -MKL have already been documented [12].

We carry out two sets of experiments. The ﬁrst, on small scale UCI data sets, are carried out using
pre-computed kernels. This performs a direct comparison of the algorithmic components of SMO-
MKL and Shogun. We also carry out a few large scale experiments with kernels computed on the
ﬂy. This experiment compares the two methods in totality. In this case, kernel caching can have an
effect, but not a signiﬁcant one as the two methods have very similar caching strategies.

For each UCI data set we generated kernels as recommended in [16]. We generated RBF kernels
with ten bandwidths for each individual dimension of the feature vector as well as the full feature
vector itself. Similarly, we also generated polynomial kernels of degrees 1, 2 and 3. All kernels
matrices were pre-computed and normalised to have unit trace. We set C = 100 as it gives us a
reasonable accuracy on the test set. Note that for some value of λ, SMO-MKL and Shogun will
converge to exactly the same solution [12]. Since this value is not known a priori we arbitrarily set
λ = 1.

Training times on the UCI data sets are presented in Table 1. Means and standard deviations are
reported for ﬁve fold cross-validation. As can be seen, SMO-MKL is signiﬁcantly faster than Shogun
at converging to similar solutions and obtaining similar test accuracies. In many cases, SMO-MKL
is more than four times as fast and in some case more than ten or twenty times as fast. Note that our
test classiﬁcation accuracy on Liver is a lot lower than Shogun’s. This is due to the arbitrary choice
of λ. We can vary our λ on Liver to recover the same accuracy and solution as Shogun with a further
decrease in our training time.

Another very positive thing is that SMO-MKL appears to be relatively stable across a large operating
range of p. The code is, in most of the cases as expected, fastest when p = 2 and gets slower as
one increases or decreases p. Interestingly though, the algorithm doesn’t appear to be signiﬁcantly
slower for other values of p. Therefore, it is hoped that SMO-MKL can be used to learn sparse
kernel combinations as well as non-sparse ones.

Moving on to the large scale experiments with kernels computed on the ﬂy, we ﬁrst tried combining
a hundred thousand RBF kernels on the Sonar data set with 208 points and 59 dimensional features.

6

p

1.10
1.33
1.66
2.00
2.33
2.66
3.00

p

1.10
1.33
1.66
2.00
2.33
2.66
3.00

# Kernels Selected
Shogun
SMO-MKL
137.2 ± 53.8
26.4 ± 0.8
40.8 ± 1.3
62.4 ± 4.7
72.2 ± 4.8
100.2 ± 3.7
134.4 ± 5.6
126.4 ± 4.3
177.8 ± 8.3
162.8 ± 3.6
188.2 ± 4.7
188.8 ± 5.1
192.0 ± 2.6
194.4 ± 1.2

Table 1: Training times on UCI data sets with N training points, D dimensional features, M kernels
and T test points. Mean and standard deviations are reported for 5-fold cross validation.
(a) Australian: N =552, T =138, D=13, M =195.
Training Time (s)
Test Accuracy (%)
Shogun
SMO-MKL
Shogun
SMO-MKL
85.22 ± 2.81
85.22 ± 2.96
58.52 ± 16.49
4.89 ± 0.31
4.16 ± 0.16
33.58 ± 2.58
85.36 ± 3.79
85.07 ± 2.85
4.31 ± 0.19
31.89 ± 1.25
85.65 ± 3.73
85.07 ± 2.85
85.22 ± 2.99
85.80 ± 3.74
27.08 ± 7.18
4.27 ± 0.10
24.92 ± 6.46
4.88 ± 0.18
85.80 ± 3.74
85.07 ± 2.85
5.19 ± 0.05
26.90 ± 2.05
85.80 ± 3.68
85.22 ± 2.85
5.48 ± 0.21
27.06 ± 2.20
85.51 ± 3.69
85.22 ± 2.85
(b) Ionosphere: N =280, T =71, D=33, M =442.
Test Accuracy (%)
Training Time (s)
SMO-MKL
Shogun
SMO-MKL
Shogun
92.03 ± 1.68
92.60 ± 1.35
19.82 ± 4.02
2.85 ± 0.16
92.60 ± 1.86
92.03 ± 1.42
8.49 ± 0.61
2.78 ± 1.18
2.42 ± 0.28
10.49 ± 2.27
91.74 ± 2.08
91.74 ± 1.37
2.16 ± 0.16
13.99 ± 4.68
92.03 ± 1.68
91.17 ± 2.45
2.35 ± 0.25
24.90 ± 9.43
92.03 ± 1.68
91.74 ± 2.08
92.03 ± 1.68
92.03 ± 1.68
33.05 ± 3.66
2.50 ± 0.32
3.03 ± 0.99
36.23 ± 3.62
92.31 ± 1.41
91.75 ± 2.05
(c) Liver: N =276, T =69, D=5, M =91.
Training Time (s)
Test Accuracy (%)
Shogun
SMO-MKL
Shogun
SMO-MKL
66.67 ± 9.91
62.90 ± 9.81
2.15 ± 0.12
0.53 ± 0.03
0.54 ± 0.03
0.92 ± 0.05
66.09 ± 8.48
71.59 ± 8.92
0.56 ± 0.04
1.14 ± 0.23
66.96 ± 7.53
70.72 ± 9.28
72.17 ± 6.94
66.96 ± 7.06
1.72 ± 0.57
0.54 ± 0.04
73.33 ± 6.71
66.38 ± 7.36
2.35 ± 0.36
0.63 ± 0.03
0.65 ± 0.02
2.53 ± 0.44
65.22 ± 6.80
72.75 ± 7.96
0.67 ± 0.03
3.40 ± 0.55
65.22 ± 6.74
73.91 ± 7.28
(d) Sonar: N =166, T =42, D=59, M =793.
Test Accuracy (%)
Training Time (s)
Shogun
SMO-MKL
Shogun
SMO-MKL
4.95 ± 0.29
47.19 ± 3.85
85.15 ± 7.99
81.25 ± 8.71
87.03 ± 6.85
84.65 ± 9.37
18.28 ± 1.63
4.00 ± 0.76
87.51 ± 6.28
88.47 ± 6.68
20.27 ± 8.84
4.48 ± 1.63
3.31 ± 0.31
31.52 ± 5.07
88.94 ± 6.00
88.95 ± 6.33
3.54 ± 0.35
51.83 ± 17.96
88.94 ± 4.97
88.94 ± 5.41
88.94 ± 4.97
88.94 ± 4.97
64.59 ± 9.19
3.83 ± 0.38
3.96 ± 0.45
70.08 ± 9.18
88.94 ± 4.97
89.92 ± 5.13

# Kernels Selected
Shogun
SMO-MKL
39.40 ± 1.50
9.40 ± 1.02
24.40 ± 2.06
43.60 ± 2.42
44.20 ± 2.23
57.00 ± 3.29
78.00 ± 2.28
71.00 ± 5.29
88.20 ± 1.72
82.40 ± 2.42
83.20 ± 2.32
90.80 ± 0.40
85.20 ± 3.37
91.00 ± 0.00

# Kernels Selected
SMO-MKL
Shogun
125.2 ± 7.3
50.0 ± 2.7
217.0 ± 23.4
120.8 ± 6.0
200.8 ± 4.4
291.4 ± 33.0
328.0 ± 6.6
364.2 ± 15.4
413.6 ± 5.6
412.2 ± 6.6
436.6 ± 4.3
430.6 ± 4.6
434.4 ± 4.8
442.0 ± 0.0

# Kernels Selected
Shogun
SMO-MKL
91.2 ± 6.9
258.0 ± 24.8
374.2 ± 20.9
247.8 ± 7.7
451.6 ± 12.0
383.0 ± 5.7
661.2 ± 10.2
664.8 ± 35.2
770.8 ± 4.4
763.0 ± 7.0
789.4 ± 2.8
782.0 ± 3.4
786.0 ± 4.1
792.2 ± 1.1

p

1.10
1.33
1.66
2.00
2.33
2.66
3.00

p

1.10
1.33
1.66
2.00
2.33
2.66
3.00

Note that these kernels do not form any special hierarchy so approaches such as [2] are not applica-
ble. Timing results on a log-log scale are given in Figure (1a). As can be seen, SMO-MKL appears
to be scaling linearly with the number of kernels and we converge in less than half an hour on all
hundred thousand kernels for both p = 2 and p = 1.33. If we were to run the same experiment using
pre-computed kernels then we converge in approximately seven minutes (see Fig (1b)). On the other
hand, Shogun took six hundred seconds to combine just ten thousand kernels computed on the ﬂy.

The trend was the same when we increased the number of training points. Figure (1c) and (1d) plot
timing results on a log-log scale as the number of training points is varied on the Adult and Web
data sets (please see [1] for data set details and downloads). We used 50 kernels computed on the

7

Sonar

SMO−MKL p=1.33
SMO−MKL p=2.00

Sonar

SMO−MKL p=1.33
SMO−MKL p=2.00

7.5

7

6.5

6

5.5

5

)
s
(
 
)
e
m
i
T
(
g
o
l

 

7

6

5

4

3

2

1

)
s
(
 
)
e
m
i
T
(
g
o
l

Adult

SMO−MKL p=1.33
SMO−MKL p=2.00
Shogun p=1.33
Shogun p=2.00

 

10

)
s
(
 
)
e
m
i
T
(
g
o
l

9

8

7

6

5

4

3

2

 

8

7

6

5

4

3

2

)
s
(
 
)
e
m
i
T
(
g
o
l

Web

SMO−MKL p=1.33
SMO−MKL p=2.00

 

 
4.5
9

9.5

10
11
10.5
log(# Kernels)

(a) Sonar

11.5

12

 
0
6

7

8

10
9
log(# Kernels)

11

12

 
1
7

7.5

8
9.5
9
8.5
log(# Training Points)

10

10.5

 
1
7.5

8

8.5
10
9.5
9
log(# Training Points)

10.5

11

(b) Sonar Pre-computed

(c) Adult

(d) Web

Figure 1: Large scale experiments varying the number of kernels and points. See text for details.

ﬂy for these experiments. On Adult, till about six thousand points, SMO-MKL is roughly 1.5 times
faster than Shogun for p = 1.33 and 5 times faster for p = 2. However, on reaching eleven thousand
points, Shogun starts taking more and more time to converge and we could not get results for sixteen
thousand points or more. SMO-MKL was unaffected and converged on the full data set with 32,561
points in 9245.80 seconds for p = 1.33 and 8511.12 seconds for p = 2. We tried the Web data set
to see whether the SMO-MKL algorithm would scale beyond 32K points. Training on all 49,749
points and 50 kernels took 1574.73 seconds (i.e. less than half an hour) with p = 1.33 and 2023.35
seconds with p = 2.

7 Conclusions

We developed the SMO-MKL algorithm for efﬁciently optimising the lp -MKL formulation. We
placed the emphasis ﬁrmly back on optimising the MKL dual rather than the intermediate saddle
point problem on which all state-of-the-art MKL solvers are based. We showed that the lp -MKL
dual is differentiable and that placing the p-norm squared regulariser in the primal objective lets us
analytically solve the reduced variable problem for p = 2. We could also solve the convex, one-
dimensional reduced variable problem when p 6= 2 by the Newton-Raphson method. A second-order
working set selection algorithm was implemented to speed up convergence. The resulting algorithm
is simple, easy to implement and efﬁciently scales to large problems. We also showed how to
generalise the algorithm to handle not just p-norms squared but also certain Bregman divergences.

In terms of empirical performance, we compared the SMO-MKL algorithm to the specialised lp -
MKL solver of [12] referred to as Shogun. It was demonstrated that SMO-MKL was signiﬁcantly
faster than Shogun on both small and large scale data sets – sometimes by an order of magnitude.
SMO-MKL was also found to be relatively stable for various values of p and could therefore be
used to learn both sparse, and non-sparse, kernel combinations. We demonstrated that the algorithm
could combine a hundred thousand kernels on Sonar in approximately seven minutes using pre-
computed kernels and in less than half an hour using kernels computed on the ﬂy. This is signiﬁcant
as many non-linear kernel combination forms, which lead to performance improvements but are
non-convex, can be recast as convex linear MKL with a much larger set of base kernels. The SMO-
MKL algorithm can now be used to tackle such problems as long as an appropriate regulariser can
be found. We were also able to train on the entire Web data set with nearly ﬁfty thousand points
and ﬁfty kernels computed on the ﬂy in less than half an hour. Other solvers were not able to
return results on these problems. All experiments were carried out on a single core and therefore,
we believe, redeﬁne the state-of-the-art in terms of MKL optimisation. The SMO-MKL code is
available for download from [20].

Acknowledgements

We are grateful to Saurabh Gupta, Marius Kloft and Soren SSonnenburg for helpful discussions,
feedback and help with Shogun.

References

[1] http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html.

8

[2] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In NIPS, pages
105–112, 2008.

[3] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO
algorithm. In ICML, pages 6–13, 2004.

[4] A. Ben-Tal, T. Margalit, and A. Nemirovski. The ordered subsets mirror descent optimization method
with applications to tomography. SIAM Journal of Opimization, 12(1):79–108, 2001.

[5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/˜cjlin/libsvm.
[6] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In UAI, 2009.

[7] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels. In NIPS, 2009.

[8] R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information for training
SVM. JMLR, 6:1889–1918, 2005.

[9] C. Gentile. Robustness of the p-norm algorithms. ML, 53(3):265–299, 2003.

[10] M. Gonen and E. Alpaydin. Localized multiple kernel learning. In ICML, 2008.

[11] J. Kivinen, M. K. Warmuth, and B. Hassibi. The p-norm generaliziation of the LMS algorithm for adaptive
ﬁltering. IEEE Trans. Signal Processing, 54(5):1782–1793, 2006.

[12] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. Muller, and A. Zien. Efﬁcient and accurate lp -norm
Multiple Kernel Learning. In NIPS, 2009.

[13] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semideﬁnite programming. JMLR, 5:27–72, 2004.

[14] C. J. Lin, S. Lucidi, L. Palagi, A. Risi, and M. Sciandrone. Decomposition algorithm model for singly
linearly-constrained problems subject to lower and upper bounds. JOTA, 141(1):107–126, 2009.

[15] J. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in
Kernel Methods – Support Vector Learning, pages 185–208, 1999.

[16] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. SimpleMKL. JMLR, 9:2491–2521, 2008.

[17] S. Sonnenburg, G. Raetsch, C. Schaefer, and B. Schoelkopf. Large scale multiple kernel learning. JMLR,
7:1531–1565, 2006.

[18] M. Varma and B. R. Babu. More generality in efﬁcient multiple kernel learning. In ICML, 2009.

[19] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV,
2009.

[20] S. V. N. Vishwanathan, Z. Sun, N. Theera-Ampornpunt, and M. Varma, 2010. The SMO-MKL code
http://research.microsoft.com/˜manik/code/SMO-MKL/download.html.

[21] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao. Group-sensitive multiple kernel learning for object catego-
rization. In ICCV, 2009.

9


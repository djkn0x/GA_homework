Travel Time Estimation Using Floating Car Data

Ra Sevlian

This pro ject explores the use of machine learning techniques to accurately predict travel times
in city streets and highways using oating car data (location information of user vehicles on a road
network). The aim of this report is twofold, rst we present a general architecture of solving this
problem, then present and evaluate few techniques on real oating car data gathered over a month
on a 5 Km highway in New Delhi.

1 Floating Car Data Based Trac Estimation

Data used to estimate the travel times on road networks come in two varieties, one being xed
sensors on the side of the road such as magnetometer detectors or highway cameras [5, 6]. The
second method is oating car data (FCD). Floating car data are position xes of vehicles traversing
city streets throughout the day. The most common type of FCD comes from taxi's or delivery
vehicles which are on main arterial roads and highways throughout most of the day.
This second approach has many positive and negative attributes that must be dealth with to
provide accurate travel time inference. First, FCD is perhaps the most inexpensive data to attain,
since many taxi services, and delivery companies automatically gather this data on their vehicles
for logistic purposes. Second, position xes are generally very accurate, since GPS is used and this
has high accuracy. There are however many disadvantages as well. First, FCD is usually sampled
infrequenty, on the order of 2-3 minutes. The reason for this is that taxi or delivery companies do
not need such ne time granularity of their vehicles position. Therefore quiet a bit of preprocessing
needs to take place in order to snap sets of points onto the proper streets with the possibility
that multiple paths might have lead to the same pair of traverse points. Another disadvantage of
this method is that a high density of data is required to get meaningfull travel time predictions
for a given road network. Keeping all this in mind, constructing more and more accurate travel
time predictions can be a fruitful Algorithms/Machine Learning/Statistical Modelling problem with
various problems to tackle.

2 Prediction Architecture

Building a trac estimation, system using millions of incoming FCD streams is a computational
and algorithmic challenge. Various subcomponents requiring signicant research work need to be
developed and integrated. Here we give a breif overview of the various processing steps required as
well as the issues explored in this report. Figure (2) presents a high level view of such a system.

1

2 Prediction Architecture

2

Fig. 1: Travel Time Prediction Architecture:
{t1x1 y1} ... {tN xN yN }.

For

single user,

input

is

sequence

of

3 Preprocessing

3

3 Preprocessing

3.1 Motion Detection

Since the incoming streams of trac data come from taxi cabs, delivery vehicles, or other comercial
eet vehicles, there will always be a certain amount of ambiguity between a slowdown in trac
and a commercial stop by the vehicle. (i.e. for a taxi customer, or a delivery vehicle dropping o
packages). Therefore, any further processing must clean out all unwanted stops that are in the GPS
logs. The most common and inuitive technique, and that which is used in this pro ject is to track
the the number of consequtive GPS points that are less than a specied distance Dmax from each
other.
At each iteration a running tally Nt is kept for the number of GPS points that are within Dmax
of each other. If Nt is greater than some threshold Nmax then the previous Nmax points are labeled
as invalid. To compute the threshold parameters (NmaxD), a supervised learning algorithm is can
technique. In live taxi data, the taxi driver will activate a counter to track the distance crossed
by the vehicle and the time spent farying the customer accross town, this is used to construct the
proper labels for motion and stopping of the taxi. If this secondary informaiton is not available,
another option is to visually inpect a large data set of GPS data, and manually identify the regions
where a stop has taken place.

3.2 Map Matching

Map Matching is a widely studied problem in transportation research is perhaps the most com-
putaionally dicult and important subcomponent. The input is a time indexed sequence of GPS
coordinates {t1x1 y1} ... {tN xN yN }, where ti is a standard unix timestamp data structure, and pairs
(cid:110)
(cid:111)
(cid:110)
(cid:111)
(cid:8)eN
(cid:8)e1
(cid:9)
(cid:9)
(xi yi ) represent latitude and longitude coordinate. Map matching attempts to eciently and accu-
(cid:9)
here(cid:8)ei
rately match consecutive points to sequence of links representing road segments in a stardard city
t1,S tart t1,End
. . .
tN ,S tart tN ,End
map. Standard map matching outputs,
,
nâˆˆE1
nâˆˆE1
n
n
represents a set road segment links for the ith path inthegral. Also, Ei is the set of
nâˆˆE1
n
all indices representing such path edges. Figure (4.1) shows a simple example of map matching.
Map matching is done in a variety of ways depending on a tradeo of accuracy and eciency.
For the task of realtime high capacity map matching, several techniques involving hueristic graph
search are shown in [1, 2, 3]. These techniques follow a general strategy of greedily adding road
segments to a solution set as points are processed, each candidate road segment receives a score by
a distance function dened on road segments and GPS probes. A standard distance function is is
given by nding the shortest distance between the GPS point and point on the road segment.
Given a road segment dened by all convex combinations of two GPS coordinates, A and B .
eA,B = {(x, y) : Î± âˆˆ [0, 1] , (x, y) = Î±A + (1 âˆ’ Î±) B } points inside the road segment are dened as,
(cid:110) min
eA,B (Î±) = Î±A + (1 âˆ’ Î±) B . With the standard distance function used map matching algorithms is.
2 and (cid:93)({xy},B )< Ï€
(cid:93)({xy},A)< Ï€
dGP S (eA,B (Î±),{xy})
d ({xy} , e) =
2
Î±
min{dGP S (A,{xy}),dGP S (B ,{xy})}
else
Where dGP S (p1 p2 )and (cid:93) (p1 , p2 ) represent standard geographic distance and angle between
two GPS coordinates. After the candidate pool grows to a specied size, pruning via a hueristic
shrinks the candidate pool.

4 Supervised learning for Trac Estimation

4

Fig. 2: Example of map matching of set of GPS points. Input is set of points with outputted path
integrals.

4 Supervised learning for Trac Estimation

This report explores the use of supervised learning for trac inference on links on a road network.
Each technique relies on dierent modelling assumptions and produces dierent estimation results
(cid:17)
(cid:16)(cid:8)ei
(cid:9)
and has particular advantages and disadvantages that dictates use in individual situations.
(cid:9)
learned parameters, and output an estimate of the travel times through a new set of links (cid:8)ei
The most general form of learning and prediction is that after learning takes place, some model
; Î˜
parameterized by Î˜ will be available. Some prediction function, F
will use the
nâˆˆEi
n
n
4.1 Regression Technique:

nâˆˆEi

.

Regression techniques rely on an additive model for travel times. That is the travel time for a
commuter traversing a set of links will be the sum of the travel times for the set of links traversed.
(cid:16)(cid:8)ei
(cid:17)
(cid:9)
= (cid:80)
Therefore if the current link travel times are already known, then the estimated travel time for
some path through the network in gure (4.1) will the sum of the link travel times.
F
tÎ˜ (en )
; Î˜
nâˆˆEi
nâˆˆEi
n
tÎ˜ (en ) âˆ¼ N (cid:0)(cid:80)
tÎ˜ (en ) , Ïƒ2 (cid:1)
y i | (cid:80)
Under a regression model, each tÎ˜ (ei ) = Î¸i therefore given link travel times, any observed travel
time will follow
nâˆˆEi
Y = X Î˜ + E
Throughout this pro ject, we make dierent assumptions on the observed travel times depending
on whether long term averages, or short term deviations are being computed.

nâˆˆEi

4 Supervised learning for Trac Estimation

5

4.1.1 Long Run Historical Travel Time
A goal of trac ow analysis is having long term historic data on ow volume through various
t (en ) âˆ’ t (enâˆ’1 ) âˆ¼ N (cid:0)0, Ï„ 2 (cid:1) n = {1, . . . , N âˆ’ 1}
links. To construct a regression model taking into account spatial variation of historic TT values,
we can assume that adjacent links are distributed normally:
This enforces a penalty on sudden changes to the historic means, as well as makes estimation
i log P (cid:0)y i | (cid:80)
t (en )(cid:1) + (cid:80)
(cid:80)
of parameters tractable when the maximum likelihood estimator is underdetermined. With this
assumption we construct the following maximum a posteriori estimator.
t (en )(cid:13)(cid:13)2
(cid:13)(cid:13)y i âˆ’ (cid:80)
Ï„ 2 (cid:107)(cid:80)
(cid:80)
m log P (t (em ) âˆ’ t (emâˆ’1 ))
âˆ† = arg max
nâˆˆEi
t(e)
m t (em ) âˆ’ t (emâˆ’1 )(cid:107)2
+ 1
1
= arg min
nâˆˆEi
i
Ïƒ2
2
t(e)
(cid:107)Y âˆ’ X Î˜(cid:107) + Î»1 (cid:107)DÎ˜(cid:107)2
= arg min
Î˜
Which is simply a Ridge Regression or L2 Regression with smoothness penalty on the variation
of historic mean along dierent links [6].

4.1.2 Short Term Incidence Detection on Live Data

Given a short time window (on order of 1 hour) we assume the distribution on any given day for link
t (ei ) = Î¸i + âˆ†iwhere Î¸i is a historical average and âˆ†i is random variable representing the deviation
that day from the historical average. The important assumption here is that this deviation is
distributed with a heavy tail. (i.e. no deviation with high probability and large deviation with low
probability). To model this, we use a laplacian prior distrbution: p(âˆ†i ; Ïƒ) = e| âˆ’âˆ†
Ïƒ | .
i log P (cid:0)y i | (cid:80)
tÎ˜ (en )(cid:1) + (cid:80)
(cid:80)
Given a set of historic data, and a small number of measured daily measured paths, we can use
this model to computer short term deviations from the historic values.
tÎ˜ (en )(cid:13)(cid:13)2
(cid:13)(cid:13)y i âˆ’ (cid:80)
(cid:80)
(cid:80)
Ë†âˆ† = arg max
m log P (tÎ˜ (em ))
nâˆˆEi
tÎ˜ (e)
m (cid:107)tÎ˜ (em )(cid:107)2
+ 1
1
= arg min
nâˆˆEi
2
i
Ïƒ2
Ï„ 2
2
tÎ˜ (e)
(cid:107)Y âˆ’ X (Î˜ + âˆ†)(cid:107)2
2 + Î»2 (cid:107)Î˜ + âˆ†(cid:107)1
= arg min
âˆ†
Therefore in every small time window that live probe data arrives, an L1 regression is performed
estimate deviations from the historic travel times. Since probe volume is small compared to the
regression, the parameters {âˆ†}i are computed and predictions are given by (cid:80)
number of links, this technique makes intuitive sense. When performing live prediction we assume
that the parameters {Î˜, Î»1 , Î»2 } are already computed. Therefore in each time window used for
(Î¸n + âˆ†n ).
nâˆˆEi
4.1.3 Computing Model Parameters

In modelling travel time distributions, we explicitley separate long term and short term behaviour to
easily estimate the model parameters. With this, parameter estimation is performed in two stages.
First, a subset of the training data is aggregated and crossvalidation is performed to determine Î˜
and appropriate Î»1 . In the second stage, the short term model parameter Î»2 is computed with
similar cross validation with training data (See note (1)). This data processing model is shown in
gure (4.1.3). This is a visual interpretation of how data processing takes place. Each block is a
tuple of paths and travel times (i.e. X, and Y from gure (4.1)).

5 Experimental Results

6

Fig. 3: Data Processing model for estimating historic means for link travel times, as well as L1
regression parameters. Test data used daily to compute short term deviations from historic
average.

Tab. 1: Input Data Statistics: 4/5/2008 - 4/26/2008 (22 days, 8-9 AM)
std
min max mean
Raw Data in Sector
34
110
66.3
27.5
13.6 Processed Path Integrals
26.8
65
11

1

4.2 Travel Time Median Backprojection:

The second general technique used in travel time estimation is to merely backpro ject the travel
time distances across each traversed link weighted proportionally to the lenght of the links. After
many paths are processed, each link will map to a list of backpro jected travel time values. The
travel time estimate of the link is simply the median over the distribution. We use this algorithm
as a naive baseline to compare the more sophisticated model.

5 Experimental Results

5.1 Preprocessing Results

Components of the system architecture were implemented according to gure (2) to process oating
car data from approximately 40 Taxi's over the course of about one month in urban New Delhi.
The total dataset of GPS logs span 22 days: dates 4/5/2008 to 4/26/2008. The data was sampled
infrequently at about 2-3 minutes. The reason for this is that some GPS devices sampled every 2
minutes while some sampled every 3 minutes. Since the focus of this report is the core inference
techniques, a single 5 Km highway (gure (5.1)) was chosen for analysis instead of a region of the
city, or the entire city.
instead of an entire city. Another reason why a long highway was chosen
is that the fundemental assumption that link travel times can be added to estimate the combined
travel time is a more accurate model for highways than for city arterial roads [4].

1 Recall that this separation between long term historic data and short term uctuations is an assumption we
explicitly make. A more thorough model would merely include daily laplacian deviations into the rst model.
However comstructing an estimator for this latent variable model, requires an EM algorithm. This is a denately a
future extension of our work.

5 Experimental Results

7

Fig. 4: Single road learning experiment.

Fig. 5: Track Creation process - Primary reprocessing step.

The data included all FCD logs that were contained in the geospatial coordinates shown in gure
(5.1). Therefore much of the data belonged to taxi's that might have traversed nearby streets. To
isolate the data required for inference of the highlighted street, full map matching on the data was
implement and used to seperate FCD data traversing nearby streets. Additionally, stop detection
as described earlier was used to further remove artifacts that would cause false positives in incident
detection. Statistics on the raw input data and nal post processed data are given in table (5.1).
For this pro ject, additional processing was done to remove any artifacts caused by missed detections
of taxi stops. From table (5.1) it is apparent that a signicant proportion of the data contained
stops or belonged to paths on nearby streets.
Figure (5.1) shows a sample of the processing done. The each green point was determined an
acceptable path integral end point, while red points were rejected. The dashed line indicates the
start and end of each path integral. All logs for this example were sampled uniformly. By visual
inspection, one can infere that the upper left quadrant will have much lower travel times, than the
lower right portion of the street.

5 Experimental Results

8

Tab. 2: Main Results comparing 3 algorithms.
Error Rate (% of True)
Technique
26.82 %
Historical Average
Historical Average And Incidence Detection
22.1 %
32.4 %
Median Backpro ject

Std Dev.
.10
.12
.14

N
234
234
234

95 % Condence Interval
[25.53, 28.1]
[20.56, 23.6]
[30.60, 34.19]

5.2

Inference Results

Supervised learning of post processed data follows the model presented in gure (4.1.3). Here,
N 1
train blocks (each representing one day's paths) were chosen to learn the historical travel time
model. From the entire N 1
train 5 fold cross validation was used with varying model parameter Î»1
to estimate the optimal historical travel time Î˜. That is the Î»1 yielding the lowest cross validation
error for the data was chosen as the optimal parameter. Figure (5.2) shows the results from cross
validation; the historical travel times, optimal Î»1and minimum training error. Cross validation
error for training historic travel times yielded an optimal Î»1 = 24.45 and M SEmin = 22%.
Fitting the parameters for the daily incidence model follows an identical methodology as for the
long term historic mean estimation. N 2
train days of data are used, wher for each day, leave one out
cross validation (since the number of path integrals is on the order of 10 - 15) is performed where
parameter âˆ† is estimated with the training data and used to predict the travel time of the one out
path integral/TT. As before, Î»2 is varied and an optimal value can be determined by looking at
the lowest test error.

5.3 Test Prediction

Finally with the model parameters computed, the incident detection technique is tested as follows:
All data not used in the rst two phases (Ntest ) is tested for each individual day, whereby all but
one of the path integrals is assumed to be the day's probe data (i.e. leave one out model validation)
. This data is used to to nd the sparse deviations from the historic mean {âˆ†} by solving the
LASSO problem presented earlier. The deviations are then incorporated with the historic mean to
provide the predicted travel time.

6 Conclusions and Future Directions

9

In this report we tested 3 separate algorithms to compare their results. Multiple random assign-
ments were made between the 22 les and the subsets used for training and testing (N 1
train , N 2
train , Ntest ).
First we merely use the historic means as a default estimate, without updating the link travel times
by any live data. Second we apply the incidence detection technique introduced in the paper. Fi-
nally we apply the naive median backpro ject technique. Table (5.3) summarizes the nal computed
test errors for the dierent techniques discussed. The results show an improvement of incidence
detection over merely using the historical average via regularized least square. The worst tech-
nique was the median backpro ject. The error values are in percentage of true link travel time. A
simple statistical analysis of the incidence detection results shows that the improvement is statisti-
cally signicant. Under 95% condence interval there is a statistically signicant dierence in each
algorithms results.

6 Conclusions and Future Directions

This report presents a unied architecture for Floating Car Data based travel time inference as well
as proposes an incidence based inference technique that is evaluated and shown to be quiet eective
at estimating travel times on various links in transportation networks. The largest impediment in
achieving more accuracy appears to be the amount of data required. The road network was chosen
since from the data available, it had the highest density of FCD. Regardless, at the rush hour time
of 8-9 AM an average of 26 or so path integrals were constructed. Therefore future tests require a
higher density of FCD. Also, a unied EM algorithm (mentioned in the footnotes) for training the
historical data, while incorporating the sparse deviations model should be investigated.

References

[1] Dongdong Wu, Tongyu Zhu, Weifeng Lv, Xin Gao; , " A Heuristic Map-Matching Algorithm
by Using Vector-Based Recognition," Computing in the Global Information Technology, 2007.
ICCGI 2007. International Multi-Conference on , vol., no., pp.18-18, 4-9 March 2007

[2] Dongdong Wu, Tongyu Zhu, Weifeng Lv, Xin Gao; , "A Quick Map-Matching Algorithm by
Using Grid Based Selecting," Computing in the Global Information Technology, 2007. ICCGI
2007. International Multi-Conference on , vol., no., pp.18-18, 4-9 March 2007

[3] Marchal F., J.K. Hackney, K.W. Axhausen, Ecient Map-Matching of Large GPS Data Sets-
Tests on a speed monitoring experiment in Zurich , STRC 2005

[4] Ra jagopal R.  Large Monitoring Systems: Data Analysis, Design and Deployment . 2009.

[5] Kwong, R. Kavaler, R. Ra jagopal, and P. Varaiya. Arterial travel time estimation based on
vehicle re-identication using wireless magnetic sensors. Transportation Research Part C. 2009

[6] Hastie, T. Tibshirani R, and Friedman J. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction , 2008


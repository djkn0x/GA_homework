Network Flow Algorithms for Structured Sparsity

Julien Mairal∗
INRIA - Willow Project-Team†
julien.mairal@inria.fr

Rodolphe Jenatton∗
INRIA - Willow Project-Team†
rodolphe.jenatton@inria.fr

Guillaume Obozinski
INRIA - Willow Project-Team†
guillaume.obozinski@inria.fr

Francis Bach
INRIA - Willow Project-Team†
francis.bach@inria.fr

Abstract

We consider a class of learning problems that involve a structured sparsity-
inducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas
a lot of effort has been put in developing fast optimization methods when the
groups are disjoint or embedded in a speciﬁc hierarchical structure, we address
here the case of general overlapping groups. To this end, we show that the cor-
responding optimization problem is related to network ﬂow optimization. More
precisely, the proximal problem associated with the norm we consider is dual to a
quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which com-
putes its solution exactly in polynomial time. Our algorithm scales up to millions
of variables, and opens up a whole new range of applications for structured sparse
models. We present several experiments on image and video data, demonstrating
the applicability and scalability of our approach for various problems.

1

Introduction

Sparse linear models have become a popular framework for dealing with various unsupervised and
supervised tasks in machine learning and signal processing. In such models, linear combinations of
small sets of variables are selected to describe the data. Regularization by the ℓ1 -norm has emerged
as a powerful tool for addressing this combinatorial variable selection problem, relying on both a
well-developed theory (see [1] and references therein) and efﬁcient algorithms [2, 3, 4].

The ℓ1 -norm primarily encourages sparse solutions, regardless of the potential structural relation-
ships (e.g., spatial, temporal or hierarchical) existing between the variables. Much effort has recently
been devoted to designing sparsity-inducing regularizations capable of encoding higher-order infor-
mation about allowed patterns of non-zero coefﬁcients [5, 6, 7, 8, 9, 10], with successful applications
in bioinformatics [6, 11], topic modeling [12] and computer vision [9, 10].

By considering sums of norms of appropriate subsets, or groups, of variables, these regulariza-
tions control the sparsity patterns of the solutions. The underlying optimization problem is usually
difﬁcult, in part because it involves nonsmooth components. Proximal methods have proven to be
effective in this context, essentially because of their fast convergence rates and their scalability [3, 4].
While the settings where the penalized groups of variables do not overlap or are embedded in a tree-
shaped hierarchy [12] have already been studied, regularizations with general overlapping groups
have, to the best of our knowledge, never been addressed with proximal methods.

This paper makes the following contributions:
− It shows that the proximal operator associated with the structured norm we consider can be

∗Contributed equally.
†Laboratoire d’Informatique de l’Ecole Normale Sup ´erieure (INRIA/ENS/CNRS UMR 8548)

1

computed with a fast and scalable procedure by solving a quadratic min-cost ﬂow problem.
− It shows that the dual norm of the sparsity-inducing norm we consider can also be evaluated
efﬁciently, which enables us to compute duality gaps for the corresponding optimization problems.
− It demonstrates that our method is relevant for various applications, from video background
subtraction to estimation of hierarchical structures for dictionary learning of natural image patches.

2 Structured Sparse Models

We consider in this paper convex optimization problems of the form

f (w) + λΩ(w),

min
w∈Rp
where f : Rp → R is a convex differentiable function and Ω : Rp → R is a convex, nonsmooth,
sparsity-inducing regularization function. When one knows a priori that the solutions of this learn-
ing problem have only a few non-zero coefﬁcients, Ω is often chosen to be the ℓ1 -norm (see [1, 2]).
When these coefﬁcients are organized in groups, a penalty encoding explicitly this prior knowl-
edge can improve the prediction performance and/or interpretability of the learned models [13, 14].
Denoting by G a set of groups of indices, such a penalty might for example take the form:
Ω(w) , X
|wj | = X
g∈G
g∈G

ηg kwg k∞ ,

ηg max
j∈g

(1)

(2)

where wj is the j -th entry of w for j in [1; p] , {1, . . . , p}, the vector wg in R|g | records the
coefﬁcients of w indexed by g in G , and the scalars ηg are positive weights. A sum of ℓ2 -norms is
also used in the literature [7], but the ℓ∞ -norm is piecewise linear, a property that we take advantage
of in this paper. Note that when G is the set of singletons of [1; p], we get back the ℓ1 -norm.

If G is a more general partition of [1; p], variables are selected in groups rather than individually.
When the groups overlap, Ω is still a norm and sets groups of variables to zero together [5]. The
latter setting has ﬁrst been considered for hierarchies [7, 11, 15], and then extended to general group
structures [5].1 Solving Eq. (1) in this context becomes challenging and is the topic of this paper.
Following Jenatton et al. [12] who tackled the case of hierarchical groups, we propose to approach
this problem with proximal methods, which we now introduce.

2.1 Proximal Methods

min
w∈Rp

f (w0 ) + (w − w0 )⊤∇f (w0 ) + λΩ(w) +

In a nutshell, proximal methods can be seen as a natural extension of gradient-based techniques,
and they are well suited to minimizing the sum f + λΩ of two convex terms, a smooth function f
—continuously differentiable with Lipschitz-continuous gradient— and a potentially non-smooth
function λΩ (see [16] and references therein). At each iteration, the function f is linearized at the
current estimate w0 and the so-called proximal problem has to be solved:
L
2
The quadratic term keeps the solution in a neighborhood where the current linear approximation
holds, and L > 0 is an upper bound on the Lipschitz constant of ∇f . This problem can be rewritten as
1
2
with λ′ , λ/L, and u , w0 − 1
L ∇f (w0 ). We call proximal operator associated with the regulariza-
tion λ′Ω the function that maps a vector u in Rp onto the (unique, by strong convexity) solution w⋆
of Eq. (3). Simple proximal methods use w⋆ as the next iterate, but accelerated variants [3, 4] are
also based on the proximal operator and require to solve problem (3) exactly and efﬁciently to enjoy
their fast convergence rates. Note that when Ω is the ℓ1 -norm, the solution of Eq. (3) is obtained by
soft-thresholding [16]. The approach we develop in the rest of this paper extends [12] to the case of
general overlapping groups when Ω is a weighted sum of ℓ∞ -norms, broadening the application of
these regularizations to a wider spectrum of problems.2

ku − wk2
2 + λ′Ω(w),

kw − w0 k2
2 .

min
w∈Rp

(3)

1Note that other types of structured sparse models have also been introduced, either through a different
norm [6], or through non-convex criteria [8, 9, 10].
2For hierarchies, the approach of [12] applies also to the case of where Ω is a weighted sum of ℓ2 -norms.

2

3 A Quadratic Min-Cost Flow Formulation

In this section, we show that a convex dual of problem (3) for general overlapping groups G can
be reformulated as a quadratic min-cost ﬂow problem. We present an efﬁcient algorithm to solve it
exactly, as well as a related algorithm to compute the dual norm of Ω. We start by considering the
dual formulation to problem (3) introduced in [12], for the case where Ω is a sum of ℓ∞ -norms:

min
ξ∈Rp×|G |

Lemma 1 (Dual of the proximal problem [12])
Given u in Rp , consider the problem
1
2 (cid:13)(cid:13)(cid:13)
ξg (cid:13)(cid:13)(cid:13)
u − X
g∈G
where ξ = (ξg )g∈G is in Rp×|G | , and ξg
j denotes the j -th coordinate of the vector ξg . Then, every
solution ξ⋆ = (ξ⋆g )g∈G of Eq. (4) satisﬁes w⋆ = u−Pg∈G ξ⋆g , where w⋆ is the solution of Eq. (3).
Without loss of generality,3 we assume from now on that the scalars uj are all non-negative, and we
constrain the entries of ξ to be non-negative. We now introduce a graph modeling of problem (4).

s.t. ∀g ∈ G , kξg k1 ≤ ληg

ξg
j = 0 if j /∈ g ,

2

2

and

(4)

3.1 Graph Model

Let G be a directed graph G = (V , E , s, t), where V is a set of vertices, E ⊆ V × V a set of arcs, s
a source, and t a sink. Let c and c′ be two functions on the arcs, c : E → R and c′ : E → R+ , where
c is a cost function and c′ is a non-negative capacity function. A ﬂow is a non-negative function
on arcs that satisﬁes capacity constraints on all arcs (the value of the ﬂow on an arc is less than or
equal to the arc capacity) and conservation constraints on all vertices (the sum of incoming ﬂows at
a vertex is equal to the sum of outgoing ﬂows) except for the source and the sink.

We introduce a canonical graph G associated with our optimization problem, and uniquely charac-
terized by the following construction:
(i) V is the union of two sets of vertices Vu and Vgr , where Vu contains exactly one vertex for
each index j in [1; p], and Vgr contains exactly one vertex for each group g in G . We thus have
|V | = |G | + p. For simplicity, we identify groups and indices with the vertices of the graph.
(ii) For every group g in G , E contains an arc (s, g). These arcs have capacity ληg and zero cost.
(iii) For every group g in G , and every index j in g , E contains an arc (g , j ) with zero cost and
inﬁnite capacity. We denote by ξg
j the ﬂow on this arc.
(iv) For every index j in [1; p], E contains an arc (j, t) with inﬁnite capacity and a
2 (uj − ¯ξj )2 , where ¯ξj is the ﬂow on (j, t). Note that by ﬂow conservation, we neces-
cost cj , 1
sarily have ¯ξj = Pg∈G ξg
j .
Examples of canonical graphs are given in Figures 1(a)-(c). The ﬂows ξg
j associated with G can now
be identiﬁed with the variables of problem (4): indeed, the sum of the costs on the edges leading
to the sink is equal to the objective function of (4), while the capacities of the arcs (s, g) match the
constraints on each group. This shows that ﬁnding a ﬂow minimizing the sum of the costs on such a
graph is equivalent to solving problem (4).

When some groups are included in others, the canonical graph can be simpliﬁed to yield a graph
with a smaller number of edges. Speciﬁcally, if h and g are groups with h ⊂ g , the edges (g , j ) for
j ∈ h carrying a ﬂow ξg
j can be removed and replaced by a single edge (g , h) of inﬁnite capacity and
zero cost, carrying the ﬂow Pj∈h ξg
j . This simpliﬁcation is illustrated in Figure 1(d), with a graph
⋆
equivalent to the one of Figure 1(c). This does not change the optimal value of ¯ξ
, which is the
quantity of interest for computing the optimal primal variable w⋆ (a proof and a formal deﬁnition
of these equivalent graphs are available in a longer technical report [17]). These simpliﬁcations are
useful in practice, since they reduce the number of edges in the graph and improve the speed of the
algorithms we are now going to present.

3Let ξ⋆ denote a solution of Eq. (4). Optimality conditions of Eq. (4) derived in [12] show that for all j in
[1; p], the signs of the non-zero coefﬁcients ξ⋆g
for g in G are the same as the signs of the entries uj . To solve
j
Eq. (4), one can therefore ﬂip the signs of the negative variables uj , then solve the modiﬁed dual formulation
(with non-negative variables), which gives the magnitude of the entries ξ⋆g
(the signs of these being known).
j

3

s

ξg
1 + ξg
2 + ξg
3 ≤ ληg

ξg
1

u1

g

ξg
2

u2

ξg
3

s

ξg
1 + ξg
2 ≤ ληg

2 + ξh
ξh
3 ≤ ληh

g

ξg
2

ξg
1

h

ξh
2

u3

u1

u2

ξh
3

u3

¯ξ1 , c1

¯ξ2 , c2

¯ξ3 , c3

¯ξ1 , c1

¯ξ2 , c2

¯ξ3 , c3

t

t

(a) G = {g = {1, 2, 3}}.

(b) G = {g = {1, 2}, h = {2, 3}}.

2 + ξh
ξh
3 ≤ ληh

2 + ξg
1 + ξg
ξg
3 ≤ ληg

2 + ξh
ξh
3 ≤ ληh

s

g

2 + ξg
1 + ξg
ξg
3 ≤ ληg

s

g

ξg
1

u1

ξg
2

u2

ξh
2

ξg
3

h

ξh
3

u3

ξg
1

ξg
2 + ξg
3
ξg
2 + ξh
2

u1

u2

h

ξg
3 + ξh
3

u3

¯ξ1 , c1

¯ξ2 , c2

¯ξ3 , c3

¯ξ1 , c1

¯ξ2 , c2

¯ξ3 , c3

t

t

(c) G = {g = {1, 2, 3}, h = {2, 3}}.

(d) G = {g = {1} ∪ h, h = {2, 3}}.

Figure 1: Graph representation of simple proximal problems with different group structures G . The
three indices 1, 2, 3 are represented as grey squares, and the groups g , h in G as red discs. The
source is linked to every group g , h with respective maximum capacity ληg , ληh and zero cost. Each
2 (uj − ¯ξj )2 . All
variable uj is linked to the sink t, with an inﬁnite capacity, and with a cost cj , 1
other arcs in the graph have zero cost and inﬁnite capacity. They represent inclusion relationships
in-between groups, and between groups and variables. The graphs (c) and (d) correspond to a special
case of tree-structured hierarchy in the sense of [12]. Their min-cost ﬂow problems are equivalent.

3.2 Computation of the Proximal Operator

Quadratic min-cost ﬂow problems have been well studied in the operations research literature [18].
One of the simplest cases, where G contains a single group g (Ω is the ℓ∞ -norm) as in Figure 1(a),
can be solved by an orthogonal projection on the ℓ1 -ball of radius ληg .
It has been shown that
such a projection can be done in O(p) operations [18, 19]. When the group structure is a tree as in
Figure 1(d), the problem can be solved in O(pd) operations, where d is the depth of the tree [12, 18].4

The general case of overlapping groups is more difﬁcult. Hochbaum and Hong have shown in [18]
that quadratic min-cost ﬂow problems can be reduced to a speciﬁc parametric max-ﬂow problem,
for which an efﬁcient algorithm exists [20].5 While this generic approach could be used to solve
Eq. (4), we propose to use Algorithm 1 that also exploits the fact that our graphs have non-zero costs
only on edges leading to the sink. As shown in the technical report [17], it has a signiﬁcantly better
performance in practice. This algorithm clearly shares some similarities with existing approaches
in network ﬂow optimization such as the simpliﬁed version of [20] presented in [21] that uses a
divide and conquer strategy. Moreover, we have discovered after that this paper was accepted for
publication that an equivalent algorithm exists for minimizing convex functions over polymatroid

4When restricted to the case where Ω is a sum of ℓ∞ -norms, the approach of [12] is in fact similar to [18].
5By deﬁnition, a parametric max-ﬂow problem consists in solving, for every value of a parameter, a max-
ﬂow problem on a graph whose arc capacities depend on this parameter.

4

sets [22]. This equivalence, however, requires a non-trivial representation of structured sparsity-
inducing norms with submodular functions, as recently pointed out by [23].

Algorithm 1 Computation of the proximal operator for overlapping groups.
1: Inputs: u ∈ Rp , a set of groups G , positive weights (ηg )g∈G , and λ (regularization parameter).
2: Build the initial graph G0 = (V0 , E0 , s, t) as explained in Section 3.2.
3: Compute the optimal ﬂow: ¯ξ ← computeFlow(V0 , E0 ).
4: Return: w = u − ¯ξ (optimal solution of the proximal problem).
Function computeFlow(V = Vu ∪ Vgr , E )
1
2 (uj − γ j )2 s.t. Pj∈Vu
γ j ≤ λ Pg∈Vgr
1: Projection step: γ ← arg minγ Pj∈Vu
2: For all nodes j in Vu , set γ j to be the capacity of the arc (j, t).
3: Max-ﬂow step: Update ( ¯ξj )j∈Vu by computing a max-ﬂow on the graph (V , E , s, t).
4: if ∃ j ∈ Vu s.t. ¯ξj 6= γ j then
Denote by (s, V + ) and (V − , t) the two disjoint subsets of (V , s, t) separated by the minimum
5:
(s, t)-cut of the graph, and remove the arcs between V + and V − . Call E+ and E− the two
remaining disjoint subsets of E corresponding to V + and V − .
( ¯ξj )j∈V +
← computeFlow(V + , E+ ).
u
( ¯ξj )j∈V −
← computeFlow(V − , E− ).
7:
u
8: end if
9: Return: ( ¯ξj )j∈Vu .

ηg .

6:

The intuition behind this algorithm is the following: The ﬁrst step looks for a candidate value for
¯ξ = Pg∈G ξg by solving a relaxed version of problem Eq. (4), where the constraints kξg k1 ≤ ληg are
dropped and replaced by a single one k ¯ξk1 ≤ λ Pg∈G ηg . The relaxed problem only depends on ¯ξ
and can be solved in linear time. By calling its solution γ , it provides a lower bound ku − γ k2
2 /2
on the optimal cost. Then, the second step tries to ﬁnd a feasible ﬂow of the original problem (4)
such that the resulting vector ¯ξ matches γ , which is in fact a max-ﬂow problem [24]. If ¯ξ = γ ,
then the cost of the ﬂow reaches the lower bound, and the ﬂow is optimal. If ¯ξ 6= γ , the lower
bound is not achievable, and we construct a minimum (s, t)-cut of the graph [25] that deﬁnes two
disjoints sets of nodes V + and V − ; V + is the part of the graph that could potentially have received
more ﬂow from the source (the arcs between s and V + are not saturated), whereas all arcs linking s
to V − are saturated. At this point, it is possible to show that the value of the optimal min-cost
ﬂow on all arcs between V + and V − is necessary zero. Thus, removing them yields an equivalent
optimization problem, which can be decomposed into two independent problems of smaller sizes
and solved recursively by the calls to computeFlow(V + , E+ ) and computeFlow(V − , E− ). A
formal proof of correctness of Algorithm 1 and further details are relegated to [17].

The approach of [18, 20] is guaranteed to have the same worst-case complexity as a single max-ﬂow
algorithm. However, we have experimentally observed a signiﬁcant discrepancy between the worst
case and empirical complexities for these ﬂow problems, essentially because the empirical cost of
each max-ﬂow is signiﬁcantly smaller than its theoretical cost. Despite the fact that the worst-case
guarantee of our algorithm is weaker than their (up to a factor |V |), it is more adapted to the structure
of our graphs and has proven to be much faster in our experiments (see technical report [17]).

Some implementation details are crucial to the efﬁciency of the algorithm:
• Exploiting connected components: When there exists no arc between two subsets of V , it is
possible to process them independently in order to solve the global min-cost ﬂow problem.
• Efﬁcient max-ﬂow algorithm: We have implemented the “push-relabel” algorithm of [24]
for solving our max-ﬂow problems, using classical heuristics that signiﬁcantly speed it up in
practice (see [24, 26]). This algorithm leverages the concept of pre-ﬂow that relaxes the deﬁni-
tion of ﬂow and allows vertices to have a positive excess.
It can be initialized with any valid
pre-ﬂow, enabling warm-restarts when the max-ﬂow is called several times as in our algorithm.
• Improved projection step: The ﬁrst line of the function computeFlow can be replaced by
1
2 (uj − γ j )2 s.t. Pj∈Vu
γ ← arg minγ Pj∈Vu
ηg and |γ j | ≤ λ Pg∋j ηg . The
γ j ≤ λ Pg∈Vgr
idea is that the structure of the graph will not allow ¯ξj to be greater than λ Pg∋j ηg after the max-
ﬂow step. Adding these additional constraints leads to better performance when the graph is not
well balanced. This modiﬁed projection step can still be computed in linear time [19].

5

3.3 Computation of the Dual Norm

The dual norm Ω∗ of Ω, deﬁned for any vector κ in Rp by Ω∗ (κ) , maxΩ(z)≤1 z⊤κ, is a key quan-
tity to study sparsity-inducing regularizations [5, 15, 27]. We use it here to monitor the convergence
of the proximal method through a duality gap, and deﬁne a proper optimality criterion for prob-
lem (1). We denote by f ∗ the Fenchel conjugate of f [28], deﬁned by f ∗ (κ) , supz [z⊤κ − f (z)].
The duality gap for problem (1) can be derived from standard Fenchel duality arguments [28] and
it is equal to f (w) + λΩ(w) + f ∗ (−κ) for w, κ in Rp with Ω∗ (κ) ≤ λ. Therefore, evaluating
the duality gap requires to compute efﬁciently Ω∗ in order to ﬁnd a feasible dual variable κ. This is
equivalent to solving another network ﬂow problem, based on the following variational formulation:
s.t. X
g∈G
In the network problem associated with (5), the capacities on the arcs (s, g), g ∈ G , are set to τ ηg ,
and the capacities on the arcs (j, t), j in [1; p], are ﬁxed to κj . Solving problem (5) amounts to
ﬁnding the smallest value of τ , such that there exists a ﬂow saturating the capacities κj on the arcs
leading to the sink t (i.e., ¯ξ = κ). The algorithm below is proven to be correct in [17].

ξg = κ, and ∀g ∈ G , kξg k1 ≤ τ ηg with ξg
j = 0 if j /∈ g .

Ω∗ (κ) = min
τ
ξ∈Rp×|G |

(5)

Algorithm 2 Computation of the dual norm.
1: Inputs: κ ∈ Rp , a set of groups G , positive weights (ηg )g∈G .
2: Build the initial graph G0 = (V0 , E0 , s, t) as explained in Section 3.3.
3: τ ← dualNorm(V0 , E0 ).
4: Return: τ (value of the dual norm).

Function dualNorm(V = Vu ∪ Vgr , E )
κj )/(Pg∈Vgr
1: τ ← (Pj∈Vu
ηg ) and set the capacities of arcs (s, g) to τ ηg for all g in Vgr .
2: Max-ﬂow step: Update ( ¯ξj )j∈Vu by computing a max-ﬂow on the graph (V , E , s, t).
3: if ∃ j ∈ Vu s.t. ¯ξj 6= κj then
Deﬁne (V + , E+ ) and (V − , E− ) as in Algorithm 1, and set τ ← dualNorm(V − , E− ).
4:
5: end if
6: Return: τ .

4 Applications and Experiments

Our experiments use the algorithm of [4] based on our proximal operator, with weights ηg set to 1.

4.1 Speed Comparison

We compare our method (ProxFlow) and two generic optimization techniques, namely a subgradient
descent (SG) and an interior point method,6 on a regularized linear regression problem. Both SG and
ProxFlow are implemented in C++. Experiments are run on a single-core 2.8 GHz CPU. We con-
sider a design matrix X in Rn×p built from overcomplete dictionaries of discrete cosine transforms
(DCT), which are naturally organized on one- or two-dimensional grids and display local corre-
lations. The following families of groups G using this spatial information are thus considered: (1)
every contiguous sequence of length 3 for the one-dimensional case, and (2) every 3×3-square in the
two-dimensional setting. We generate vectors y in Rn according to the linear model y = Xw0 + ε,
where ε ∼ N (0, 0.01kXw0 k2
2 ). The vector w0 has about 20% percent nonzero components, ran-
domly selected, while respecting the structure of G , and uniformly generated between [−1, 1].

In our experiments, the regularization parameter λ is chosen to achieve the same sparsity as w0 . For
SG, we take the step size to be equal to a/(k + b), where k is the iteration number, and (a, b) are the
best parameters selected in {10−3 , . . . , 10}× {102 , 103 , 104 }. For the interior point methods, since
problem (1) can be cast either as a quadratic (QP) or as a conic program (CP), we show in Figure 2
the results for both formulations. Our approach compares favorably with the other methods, on
three problems of different sizes, (n, p) ∈ {(100, 103 ), (1024, 104 ), (1024, 105 )}, see Figure 2. In
addition, note that QP, CP and SG do not obtain sparse solutions, whereas ProxFlow does. We have
also run ProxFlow and SG on a larger dataset with (n, p) = (100, 106 ): after 12 hours, ProxFlow
and SG have reached a relative duality gap of 0.0006 and 0.02 respectively.7
6 In our simulations, we use the commercial software Mosek, http://www.mosek.com/.
7Due to the computational burden, QP and CP could not be run on every problem.

6

)
m
u
m
i
t
p
o
 
o
t
 
e
c
n
a
t
s
i
d
 
e
v
i
t
a
l
e
r
(
g
o
l

2

0

−2

−4

−6

−8

−10
 
−2

n=100, p=1000, one−dimensional DCT

 

CP
QP
ProxFlow
SG

−1
1
0
log(CPU time) in seconds

2

)
m
u
m
i
t
p
o
 
o
t
 
e
c
n
a
t
s
i
d
 
e
v
i
t
a
l
e
r
(
g
o
l

n=1024, p=10000, two−dimensional DCT

 

2

0

−2

−4

−6

−8

−10
 
−2

CP
ProxFlow
SG

0
2
log(CPU time) in seconds

4

)
m
u
m
i
t
p
o
 
o
t
 
e
c
n
a
t
s
i
d
 
e
v
i
t
a
l
e
r
(
g
o
l

n=1024, p=100000, one−dimensional DCT
 

2

0

−2

−4

−6

−8

−10
 
−2

ProxFlow
SG

0
2
log(CPU time) in seconds

4

Figure 2: Speed comparisons: distance to the optimal primal value versus CPU time (log-log scale).6

Figure 3: From left to right: original image y; estimated background Xw; foreground (the sparsity
pattern of e used as mask on y) estimated with ℓ1 ; foreground estimated with ℓ1 + Ω; another
foreground obtained with Ω, on a different image, with the same values of λ1 , λ2 as for the previous
image. For the top row, the percentage of pixels matching the ground truth is 98.8% with Ω, 87.0%
without. As for the bottom row, the result is 93.8% with Ω, 90.4% without (best seen in color).

4.2 Background Subtraction

Following [9, 10], we consider a background subtraction task. Given a sequence of frames from a
ﬁxed camera, we try to segment out foreground objects in a new image. If we denote by y ∈ Rn a
test image, we model y as a sparse linear combination of p other images X ∈ Rn×p , plus an error
term e in Rn , i.e., y ≈ Xw + e for some sparse vector w in Rp . This approach is reminiscent
of [29] in the context of face recognition, where e is further made sparse to deal with occlusions.
The term Xw accounts for background parts present in both y and X, while e contains speciﬁc,
1
2 ky − Xw − ek2
2 +
or foreground, objects in y. The resulting optimization problem is minw,e
λ1kwk1 + λ2kek1 , with λ1 , λ2 ≥ 0. In this formulation, the ℓ1 -norm penalty on e does not take
into account the fact that neighboring pixels in y are likely to share the same label (background or
foreground), which may lead to scattered pieces of foreground and background regions (Figure 3).
We therefore put an additional structured regularization term Ω on e, where the groups in G are
all the overlapping 3 × 3-squares on the image. A dataset with hand-segmented evaluation images
is used to illustrate the effect of Ω.8 For simplicity, we use a single regularization parameter, i.e.,
λ1 = λ2 , chosen to maximize the number of pixels matching the ground truth. We consider p = 200
images with n = 57600 pixels (i.e., a resolution of 120 × 160, times 3 for the RGB channels). As
shown in Figure 3, adding Ω improves the background subtraction results for the two tested videos,
by encoding, unlike the ℓ1 -norm, both spatial and color consistency.

4.3 Multi-Task Learning of Hierarchical Structures

In [12], Jenatton et al. have recently proposed to use a hierarchical structured norm to learn dictio-
naries of natural image patches. Following this work, we seek to represent n signals {y1 , . . . , yn }
of dimension m as sparse linear combinations of elements from a dictionary X = [x1 , . . . , xp ]
in Rm×p . This can be expressed for all i in [1; n] as yi ≈ Xwi , for some sparse vector wi in Rp .
In [12], the dictionary elements are embedded in a predeﬁned tree T , via a particular instance of the
structured norm Ω; we refer to it as Ωtree , and call G the underlying set of groups. In this case, each
signal yi admits a sparse decomposition in the form of a subtree of dictionary elements.

8

http://research.microsoft.com/en-us/um/people/jckrumm/wallflower/testimages.htm

7

Inspired by ideas from multi-task learning [14], we propose to learn the tree structure T by pruning
irrelevant parts of a larger initial tree T0 . We achieve this by using an additional regularization
term Ωjoint across the different decompositions, so that subtrees of T0 will simultaneously be removed
for all signals yi . In other words, the approach of [12] is extended by the following formulation:
n
1
h 1
2 + λ1Ωtree (wi )i+λ2Ωjoint (W), s.t. kxj k2 ≤ 1, for all j in [1; p], (6)
X
kyi − Xwi k2
n
2
i=1
where W , [w1 , . . . , wn ] is the matrix of decomposition coefﬁcients in Rp×n . The new regular-
ization term operates on the rows of W and is deﬁned as Ωjoint (W) , Pg∈G maxi∈[1;n] |wi
g |.9 The
overall penalty on W, which results from the combination of Ωtree and Ωjoint , is itself an instance
of Ω with general overlapping groups, as deﬁned in Eq (2).

min
X,W

To address problem (6), we use the same optimization scheme as [12], i.e., alternating between X
and W, ﬁxing one variable while optimizing with respect to the other. The task we consider is the
denoising of natural image patches, with the same dataset and protocol as [12]. We study whether
learning the hierarchy of the dictionary elements improves the denoising performance, compared to
standard sparse coding (i.e., when Ωtree is the ℓ1 -norm and λ2 = 0) and the hierarchical dictionary
learning of [12] based on predeﬁned trees (i.e., λ2 = 0). The dimensions of the training set —
50 000 patches of size 8× 8 for dictionaries with up to p = 400 elements — impose to handle large
graphs, with |E | ≈ |V | ≈ 4.107 . Since problem (6) is too large to be solved many times to select the
regularization parameters (λ1 , λ2 ) rigorously, we use the following heuristics: we optimize mostly
with the currently pruned tree held ﬁxed (i.e., λ2 = 0), and only prune the tree (i.e., λ2 > 0)
every few steps on a random subset of 10 000 patches. We consider the same hierarchies as in [12],
involving between 30 and 400 dictionary elements. The regularization parameter λ1 is selected on
the validation set of 25 000 patches, for both sparse coding (Flat) and hierarchical dictionary learning
(Tree). Starting from the tree giving the best performance (in this case the largest one, see Figure 4),
we solve problem (6) following our heuristics, for increasing values of λ2 . As shown in Figure 4,
there is a regime where our approach performs signiﬁcantly better than the two other compared
methods. The standard deviation of the noise is 0.2 (the pixels have values in [0, 1]); no signiﬁcant
improvements were observed for lower levels of noise.

r
o
r
r
E
 
e
r
a
u
q
S
 
n
a
e
M

Denoising Experiment: Mean Square Error
0.21
 

Flat
Tree
Multi−task Tree

100
300
200
Dictionary Size

400

0.2

0.19

 
0

Figure 4: Left: Hierarchy obtained by pruning a larger tree of 76 elements. Right: Mean square
error versus dictionary size. The error bars represent two standard deviations, based on three runs.

5 Conclusion

We have presented a new optimization framework for solving sparse structured problems involving
sums of ℓ∞ -norms of any (overlapping) groups of variables. Interestingly, this sheds new light on
connections between sparse methods and the literature of network ﬂow optimization. In particular,
the proximal operator for the formulation we consider can be cast as a quadratic min-cost ﬂow
problem, for which we propose an efﬁcient and simple algorithm. This allows the use of accelerated
gradient methods. Several experiments demonstrate that our algorithm can be applied to a wide class
of learning problems, which have not been addressed before within sparse methods.

Acknowledgments

This paper was partially supported by the European Research Council (SIERRA Project). The au-
thors would like to thank Jean Ponce for interesting discussions and suggestions.

9The simpliﬁed case where Ωtree and Ωjoint are the ℓ1 - and mixed ℓ1 /ℓ2 -norms [13] corresponds to [30].

8

References

[1] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Ann. Stat.,
37(4):1705–1732, 2009.

[2] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann. Stat., 32(2):407–499,
2004.

[3] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical report, Center for
Operations Research and Econometrics (CORE), Catholic University of Louvain, 2007.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM J. Imag. Sci., 2(1):183–202, 2009.

[5] R. Jenatton, J-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms.
Technical report, 2009. Preprint arXiv:0904.3523v1.

[6] L. Jacob, G. Obozinski, and J.-P. Vert. Group Lasso with overlap and graph Lasso. In Proc. ICML, 2009.

[7] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical
variable selection. Ann. Stat., 37(6A):3468–3497, 2009.

[8] R. G. Baraniuk, V. Cevher, M. Duarte, and C. Hegde. Model-based compressive sensing. IEEE T. Inform.
Theory, 2010. to appear.

[9] V. Cehver, M.F. Duarte, C. Hedge, and R.G. Baraniuk. Sparse signal recovery using markov random
ﬁelds. In Adv. NIPS, 2008.

[10] J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. In Proc. ICML, 2009.

[11] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In
Proc. ICML, 2010.

[12] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary
learning. In Proc. ICML, 2010.

[13] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Roy. Stat.
Soc. B, 68:49–67, 2006.

[14] G. Obozinski, B. Taskar, and M. I. Jordan. Joint covariate selection and joint subspace selection for
multiple classiﬁcation problems. Stat. Comput., 20(2):231–252, 2010.

[15] F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Adv. NIPS, 2008.

[16] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing.
Algorithms for Inverse Problems in Science and Engineering. Springer, 2010.

In Fixed-Point

[17] J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. Network ﬂow algorithms for structured sparsity.
Technical report, 2010. Preprint arXiv:1008.5209v1.

[18] D. S. Hochbaum and S. P. Hong. About strongly polynomial time algorithms for quadratic optimization
over submodular constraints. Math. Program., 69(1):269–309, 1995.

[19] P. Brucker. An O(n) algorithm for quadratic knapsack problems. Oper. Res. Lett., 3:163–166, 1984.

[20] G. Gallo, M. E. Grigoriadis, and R. E. Tarjan. A fast parametric maximum ﬂow algorithm and applica-
tions. SIAM J. Comput., 18:30–55, 1989.

[21] M. Babenko and A.V. Goldberg. Experimental evaluation of a parametric ﬂow algorithm. Technical
report, Microsoft Research, 2006. MSR-TR-2006-77.

[22] H. Groenevelt. Two algorithms for maximizing a separable concave function over a polymatroid feasible
region. Eur. J. Oper. Res., pages 227–236, 1991.

[23] F. Bach. Structured sparsity-inducing norms through submodular functions. In Adv. NIPS, 2010.

[24] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum ﬂow problem.
Symposium on Theory of Computing, 1986.

In Proc. of ACM

[25] L. R. Ford and D. R. Fulkerson. Maximal ﬂow through a network. Canadian J. Math., 8(3), 1956.

[26] B. V. Cherkassky and A. V. Goldberg. On implementing the pushrelabel method for the maximum ﬂow
problem. Algorithmica, 19(4):390–410, 1997.

[27] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional
analysis of M-estimators with decomposable regularizers. In Adv. NIPS, 2009.

[28] J. M. Borwein and A. S. Lewis. Convex analysis and nonlinear optimization. Springer, 2006.

[29] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representa-
tion. IEEE T. Pattern. Anal., pages 210–227, 2008.

[30] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar. Collaborative hierarchical sparse modeling.
Technical report, 2010. Preprint arXiv:1003.0400v1.

9


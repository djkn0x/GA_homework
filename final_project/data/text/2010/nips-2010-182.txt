A Family of Penalty Functions for Structured
Sparsity

Charles A. Micchelli∗
Department of Mathematics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon Tong
Hong Kong
charles micchelli@hotmail.com

Jean M. Morales
Department of Computer Science
University College London
Gower Street, London WC1E
England, UK
j.morales@cs.ucl.ac.uk

Massimiliano Pontil
Department of Computer Science
University College London
Gower Street, London WC1E
England, UK
m.pontil@cs.ucl.ac.uk

Abstract

We study the problem of learning a sparse linear regression v ector under addi-
tional conditions on the structure of its sparsity pattern. We present a family of
convex penalty functions, which encode this prior knowledge by means of a set of
constraints on the absolute values of the regression coefﬁc ients. This family sub-
sumes the ℓ1 norm and is ﬂexible enough to include different models of spa rsity
patterns, which are of practical and theoretical importanc e. We establish some im-
portant properties of these functions and discuss some examples where they can be
computed explicitly. Moreover, we present a convergent opt imization algorithm
for solving regularized least squares with these penalty functions. Numerical sim-
ulations highlight the bene ﬁt of structured sparsity and th e advantage offered by
our approach over the Lasso and other related methods.

1 Introduction

The problem of sparse estimation is becoming increasingly important in machine learning and statis-
tics. In its simplest form, this problem consists in estimating a regression vector β ∗ ∈ Rn from a
data vector y ∈ Rm , obtained from the model y = X β ∗ + ξ , where X is an m × n matrix, which
may be ﬁxed or randomly chosen and ξ ∈ Rm is a vector resulting from the presence of noise. An
important rationale for sparse estimation comes from the ob servation that in many practical applica-
tions the number of parameters n is much larger than the data size m, but the vector β ∗ is known to
be sparse, that is, most of its components are equal to zero. Under these circumstances, it has been
shown that regularization with the ℓ1 norm, commonly referred to as the Lasso method, provides an
effective means to estimate the underlying regression vector as well as its sparsity pattern, see for
example [4, 12, 15] and references therein.

In this paper, we are interested in sparse estimation under a dditional conditions on the sparsity pat-
tern of β ∗ . In other words, not only do we expect that β ∗ is sparse but also that it is structured sparse,
namely certain con ﬁgurations of its nonzero components are to be preferred to others. This problem

∗C.A. Micchelli is also with the Dept. of Mathematics and Statistics, State University of New York, Albany,
USA. We are grateful to A. Argyriou and Y. Ying for valuable di scussions. This work was supported by NSF
Grant ITR-0312113, Air Force Grant AFOSR-FA9550, and EPSRC Grant EP/D071542/1.

1

arises is several applications, see [10] for a discussion. The prior knowledge that we consider in
this paper is that the vector |β ∗ |, whose components are the absolute value of the corresponding
components of β ∗ , should belong to some prescribed convex set Λ. For certain choices of Λ this
implies a constraint on the sparsity pattern as well. For example, the set Λ may include vectors with
some desired monotonicity constraints, or other constrain ts on the “shape ” of the regression vector.
Unfortunately, the constraint that |β ∗ | ∈ Λ is nonconvex and its implementation is computational
challenging. To overcome this difﬁculty, we propose a novel
family of penalty functions. It is based
on an extension of the ℓ1 norm used by the Lasso method and involves the solution of a smooth
convex optimization problem, which incorporates the struc tured sparsity constraints. As we shall
see, a key property of our approach is that the penalty function equals the ℓ1 norm of a vector β
when |β | ∈ Λ and it is strictly greater than the ℓ1 norm otherwise. This observation suggests that
the penalty function encourages the desired structured spa rsity property.

There has been some recent research interest on structured s parsity, see [1, 2, 7, 9, 10, 11, 13, 16]
and references therein. Closest to our approach are penalty methods built around the idea of mixed
ℓ1 − ℓ2 norms. In particular, the group Lasso method [16] assumes th at the components of the
underlying regression vector β ∗ can be partitioned into prescribed groups, such that the res triction
of β ∗ to a group is equal to zero for most of the groups. This idea has been extended in [10, 17]
by considering the possibility that the groups overlap acco rding to certain hierarchical or spatially
related structures. A limitation of these methods is that they can only handle sparsity patterns form-
ing a single connected region. Our point of view is different from theirs and provides a means to
designing more general and ﬂexible penalty functions which maintain convexity whilst modeling
richer model structures. For example, we will demonstrate t hat our family of penalty functions can
model sparsity pattern forming multiple connected regions of coefﬁcients.

The paper is organized as follows. In Section 2 we de ﬁne the le arning method. In particular, we
describe the associated penalty function and establish some of its important properties. In Section
3 we provide examples of penalty functions, deriving the exp licit analytical form in some important
cases, namely the case that the set Λ is a box or the wedge with nonincreasing coordinates. In
Section 4 we address the issue of solving the learning method numerically by means of an alternating
minimization algorithm. Finally, in Section 5 we provide numerical simulations with this method,
showing the advantage offered by our approach.

2 Learning method
In this section, we introduce the learning method and establish some important properties of the
associated penalty function. We let R++ be the positive real line and let Nn be the set of positive
++ and estimate β ∗ by a
integers up to n. We prescribe a convex subset Λ of the positive orthant Rn
solution of the convex optimization problem
2 + 2ρΩ(β |Λ) : β ∈ Rn (cid:9) ,
min (cid:8)kX β − yk2
where k · k2 denotes the Euclidean norm. The penalty function takes the form
(2.2)
Ω(β |Λ) = inf {Γ(β , λ) : λ ∈ Λ}
2 Pi∈Nn (cid:16) β 2
+ λi(cid:17) .
++ → R is given by the formula Γ(β , λ) = 1
and the function Γ : Rn × Rn
i
λi
Note that Γ is convex on its domain because each of its summands are likewise convex functions.
Hence, when the set Λ is convex it follows that Ω(·|Λ) is a convex function and (2.1) is a convex
optimization problem. An essential idea behind our construction of this function, is that, for every
λ ∈ R++ , the quadratic function Γ(·, λ) provides a smooth approximation to |β | from above, which
is exact at β = ±λ. We indicate this graphically in Figure 1-a. This fact follows immediately
√ab. Using the same inequal-
by the arithmetic-geometric mean inequality, namely (a + b)/2 ≥
ity it also follows that the Lasso problem corresponds to (2.1) when Λ = Rn
++ , that is it holds that
++ ) = kβk1 := Pi∈Nn |βi |. This important special case motivated us to consider the general
Ω(β |Rn
method described above. The utility of (2.2) is that upon inserting it into (2.1) results in an optimiza-
tion problem over λ and β with a continuously differentiable objective function. Hence, we have
succeeded in expressing a nondifferentiable convex objective function by one which is continuously
differentiable on its domain.

(2.1)

The next proposition provides a justiﬁcation of the penalty function as a means to incorporate struc-
tured sparsity and establish circumstances for which the penalty function is a norm.

2

 

β=0.20
β=1.00
β=2.00

 

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

abs
λ=0.75
λ=1.50
2

0

 
0

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0
 
−2.5

2

3

3.5

4

4.5

5

1

−1

−2

2.5

0.5

1

1.5

1.5

0.5

−0.5

−1.5

2.5
0
(b)
(a)
Figure 1: (a): Function Γ(·, λ) for some values of λ; (b): Function Γ(β , ·) for some values of β .
Proposition 2.1. For every β ∈ Rn , it holds that kβk1 ≤ Ω(β |Λ) and the equality holds if and
only if |β | := (|βi | : i ∈ Nn ) ∈ Λ. Moreover, if Λ is a nonempty convex cone then the function
Ω(·|Λ) is a norm and we have that Ω(β |Λ) ≤ ωkβk1 , where ω := max{Ω(ek |Λ) : k ∈ Nn} and
{ek : k ∈ Nn} is the canonical basis of Rn .
Proof. By the arithmetic-geometric inequality we have that kβk1 ≤ Γ(β , λ), proving the ﬁrst as-
sertion. If |β | ∈ Λ, there exists a sequence {λk : k ∈ N} in Λ, such that limk→∞ λk = |β |.
Since Ω(β |Λ) ≤ Γ(β , λk ) it readily follows that Ω(β |Λ) ≤ kβk1 . Conversely, if |β | ∈ Λ, then
there is a sequence {λk : k ∈ N} in Λ, such γ (β , λk ) ≤ kβ1k + 1/k . This inequality implies
that some subsequence of this sequence converges to a λ ∈ Λ. Using the arithmetic-geometric
we conclude that λ = |β | and the result follows. To prove the second part, observe tha t if Λ
is a nonempty convex cone, namely, for any λ ∈ Λ and t ≥ 0 it holds that tλ ∈ Λ, we have
that Ω is positive homogeneous. Indeed, making the change of variable λ′ = λ/|t| we see that
Ω(tβ |Λ) = |t|Ω(β |Λ). Moreover, the above inequality, Ω(β |Λ) ≥ kβk1 , implies that if Ω(β |Λ) = 0
then β = 0. The proof of the triangle inequality follows from the homogeneity and convexity of Ω,
namely Ω(α + β |Λ) = 2Ω ((α + β )/2|Λ) ≤ Ω(α|Λ) + Ω(β |Λ). Finally, note that Ω(β |Λ) ≤ ωkβk1
if and only if ω = max{Ω(β |Λ) : kβk1 = 1}. Since Ω is convex the maximum above is achieved at
an extreme point of the ℓ1 unit ball.

This proposition indicates that the function Ω(·|Λ) penalizes less vectors β which have the property
that |β | ∈ Λ, hence encouraging structured sparsity. Indeed, any permu tation of the coordinates
of a vector β with the above property will incur in the same or a larger valu e of the penalty term.
Moreover, for certain choices of the set Λ, some of which we describe below, the penalty function
will encourage vectors which not only are sparse but also have sparsity patterns (1{|βi |>0} : i ∈
Nn ) ∈ Λ, where 1{·} denotes the indicator function.
We end this section by noting that a normalized version of the group Lasso penalty [16] is included
in our setting as a special case. If {Jℓ : ℓ ∈ Nk }, k ∈ Nn form a partition of the index set Nn , the
corresponding group Lasso penalty is de ﬁned as ΩGL (β ) = Pℓ∈Nk p|Jℓ | kβJℓ k2 , where, for every
J ⊆ Nn , we use the notation βJ = (βj : j ∈ J ). It is a easy matter to verify that ΩGL (·) = Ω(·|Λ)
for Λ = {λ : λ ∈ Rn
++ , λj = θℓ , j ∈ Jℓ , ℓ ∈ Nk , θℓ > 0}.

3 Examples of the penalty function
We proceed to discuss some examples of the set Λ ⊆ Rn
++ which may be used in the design of the
penalty function Ω(·|Λ). All but the ﬁrst example fall into the category that Λ is a polyhedral cone,
that is Λ = {λ : λ ∈ Rn
++ , Aλ ≥ 0}, where A is an m × n matrix. Thus, in view of Proposition 2.1
the function Ω(·|Λ) is a norm.
The ﬁrst example corresponds to the prior knowledge that the magnitude of the components of the
regression vector should be in some prescribed intervals.
Example 3.1. We choose a, b ∈ Rn , 0 < a ≤ b and de ﬁne the corresponding box as B [a, b] :=
[ai , bi ].
Ni∈Nn
The theorem below establishes the form of the box penalty; see also [8, 14] for related penalty
functions. To state our result, we de ﬁne, for every t ∈ R, the function (t)+ = max(0, t).

3

1
2bi

(ai − |βi |)2
+ +

Theorem 3.1. We have that
+(cid:19) .
Ω(β |B [a, b]) = kβk1 + Xi∈Nn (cid:18) 1
(|βi | − bi )2
2ai
Moreover, the components of the vector λ(β ) := argmin{Γ(β , λ) : λ ∈ B [a, b]} are given by the
equations λi (β ) = |βi | + (ai − |βi |)+ − (|βi | − b)+ , i ∈ Nn .
Proof. Since Ω(β |B [a, b]) = Pi∈Nn
Ω(βi |[ai , bi ]) it sufﬁces to establish the result in the case
n = 1. We shall show that if a, b, β ∈ R, a ≤ b then
1
1
(|β | − b)2
(a − |β |)2
Ω(β |[a, b]) = |β | +
+ .
+ +
2b
2a
Since both sides of the above equation are continuous functions of β it sufﬁces to prove this equation
for β ∈ R\{0}. In this case, the function Γ(β , ·) is strictly convex in the second argument, and so,
has a unique minimum in R++ at λ = |β |, see also Figure 1-b. Moreover, if |β | ≤ a the constrained
minimum occurs at λ = a, whereas if |β | ≥ b, it occurs at λ = b. This establishes the formula for
λ(β ). Consequently, we have that
2 (cid:18) β 2
2 (cid:18) β 2
+ b(cid:19) 1{|β |>b}.
+ a(cid:19) 1{|β |<a} +
1
1
Ω(β |[a, b]) = |β |1{a≤|β |≤b} +
b
a
Equation (3.1) now follows by a direct computation.

(3.1)

Note that the function in equation (3.1) is a concatenation of two quadratic functions, connected
together with a linear function. Thus, the box penalty will favor sparsity only for a = 0, case that is
de ﬁned by a limiting argument.

The second example implements the prior knowledge that the coordinates of the vector λ are ordered
in a non increasing fashion.
Example 3.2. We de ﬁne the wedge as W = {λ : λ ∈ Rn
++ , λj ≥ λj+1 , j ∈ Nn−1 }.
We say that a partition J = {Jℓ : ℓ ∈ Nk } of Nn is contiguous if for all i ∈ Jℓ , j ∈ Jℓ+1 ,
ℓ ∈ Nk−1 , it holds that i < j . For example, if n = 3, partitions {{1, 2}, {3}} and {{1}, {2}, {3}}
are contiguous but {{1, 3}, {2}} is not.
Theorem 3.2. For every β ∈ (R\{0})n there is a unique contiguous partition J = {Jℓ : ℓ ∈ Nk }
of Nn , k ∈ Nn , such that
Ω(β |W ) = Xℓ∈Nk p|Jℓ | kβJℓ k2 .
(3.2)
Moreover, the components of the vector λ(β ) = argmin{Γ(β , λ) : λ ∈ W } are given by
λj (β ) = kβJℓ k2
, j ∈ Jℓ , ℓ ∈ Nk
p|Jℓ |
and, for every ℓ ∈ Nk and subset K ⊂ Jℓ formed by the ﬁrst k < |Jℓ | elements of Jℓ , it holds that
> kβJℓ\K k2
kβK k2√k
(3.4)
p|Jℓ | − k
The partition J appearing in the theorem is determined by the set of inequali ties λj ≥ λj+1 which
are an equality at the minimum. This set is identiﬁed by exami ning the Karush-Kuhn-Tucker opti-
mality conditions [3] of the optimization problem (2.2) for Λ = W . The detailed proof is reported
in the supplementary material. Equations (3.3) and (3.4) indicate a strategy to compute the partition
associated with a vector β . We explain how to do this in Section 4.
An interesting property of the Wedge penalty is that it has the form of a group Lasso penalty (see
the discussion at the end of Section 2) with groups not ﬁxed a-priori but depending on the location
of the vector β . The groups are the elements of the partition J and are identiﬁed by certain convex

(3.3)

.

4

Ω(β |W ) =

constraints on the vector β . For example, for n = 2 we obtain that Ω(β |W ) = kβk1 if |β1 | > |β2 |
and Ω(β |W ) = √2kβk2 otherwise. For n = 3, we have that

kβk1 ,
if |β1 | > |β2 | > |β3 |
p2(β 2
if |β1 | ≤ |β2 | and β 2
2 > 2β 2
1 + β 2
1 + β 2
2 ) + |β3 |,
3
|β1 | + p2(β 2
if |β2 | ≤ |β3 | and 2β 2
2 + β 2
1 > β 2
2 + β 2
3 ),

3
p3(β 2
2 + β 2
1 + β 2
otherwise
3 ),
where we have also reported the partition involved in each ca se.
The next example is an extension of the wedge set which is inspired by previous work on the group
Lasso estimator with hierarchically overlapping groups [1 7]. It models vectors whose magnitude is
ordered according to a graphical structure. Within this con text, the wedge corresponds to the set
associated with a line graph.
Example 3.3. We let A be the incidence matrix of a directed graph and choose Λ = {λ : λ ∈
++ , Aλ ≥ 0}.
Rn
We have con ﬁrmed that Theorem 3.2 extends to the case that the graph is a tree but the general case
is yet to be understood. We postpone this discussion to a future occasion.

J = {{1}, {2}, {3}}
J = {{1, 2}, {3}}
J = {{1}, {2, 3}}
J = {{1, 2, 3}}

Next, we note that the wedge may equivalently be expressed as the constraint that the difference
vector D1 (λ) := (λj+1 −λj : j ∈ Nn−1 ) is less than or equal to zero. Our next example extends this
observation by using the higher order difference operator, which is given by the formula Dk (λ) =
ℓ (cid:1)λj+k−ℓ : j ∈ Nn−k (cid:17).
(cid:16)λj+k + Pℓ∈Nk
(−1)ℓ(cid:0)k
Example 3.4. For every k ∈ Nn we de ﬁne the set W k := {λ : λ ∈ Rn
++ , Dk (λ) ≥ 0}.
The corresponding penalty Ω(·|W k ) encourages vectors whose sparsity pattern is concentrated on
at most k different contiguous regions. The case k = 1 essentially corresponds to the wedge,
while the case k = 2 includes vectors which have a convex “pro ﬁle ” and whose spar
sity pattern is
concentrated either on the ﬁrst elements of the vector, on th e last, or on both.

We end this section by discussing a useful construction which may be applied to generate new
penalty functions from available ones. It is obtained by composing a set Θ ⊆ Rk
++ with a linear
transformation, modeling the sum of the components of a vector, across the elements of a prescribed
partition {Pℓ : ℓ ∈ Nk } of Nn . That is, we let Λ = {λ : λ ∈ Rn
λj : ℓ ∈ Nk ) ∈ Θ}. We
++ , (Pj∈Pℓ
use this construction in the composite wedge experiments in Section 5.

4 Optimization method

In this section, we address the issue of implementing the learning method (2.1) numerically. Since
the penalty function Ω(·|Λ) is constructed as the in ﬁmum of a family of quadratic regular izers,
the optimization problem (2.1) reduces to a simultaneous minimization over the vectors β and λ.
For a ﬁxed λ ∈ Λ, the minimum over β ∈ Rn is a standard Tikhonov regularization and can
be solved directly in terms of a matrix inversion. For a ﬁxed β , the minimization over λ ∈ Λ
requires computing the penalty function (2.2). These observations naturally suggests an alternating
minimization algorithm, which has already been considered in special cases in [1]. To describe our
algorithm we choose ǫ > 0 and introduce the mapping φǫ : Rn → Rn
++ , whose i-th coordinate at
β ∈ Rn is given by φǫ
i + ǫ. For β ∈ (R\{0})n , we also let λ(β ) = argmin{Γ(β , λ) :
i (β ) = pβ 2
λ ∈ Λ}. The alternating minimization algorithm is de ﬁned as follo ws: choose, λ0 ∈ Λ and, for
k ∈ N, de ﬁne the iterates
β k = diag(λk−1 )(diag(λk−1 )X ⊤X + ρI )−1y
λk = λ(φǫ (β k )).
The following theorem establishes convergence of this algorithm. Its proof is presented in the sup-
plementary material.
Theorem 4.1. If the set Λ is convex and, for all a, b ∈ R with 0 < a < b, the set Λa,b := [a, b]n ∩Λ is
a nonempty, compact subset of the interior of Λ then the iterations (4.1) –(4.2) converges to the vector

(4.1)
(4.2)

5

Initialization: k ← 0
Input: β ∈ Rn ; Output: J1 , . . . , Jk
for t = 1 to n do
Jk+1 ← {t}; k ← k + 1
while k > 1 and kβJk−1
k2
k2√|Jk |
kβJk
√|Jk−1 | ≤
Jk−1 ← Jk−1 ∪ Jk ; k ← k − 1
end
end
Figure 2: Iterative algorithm to compute the wedge penalty
2 + 2ρΩ(φǫ (β )|Λ) : β ∈ Rn (cid:9). Moreover, any convergent subsequence
γ (ǫ) := argmin (cid:8)ky − X βk2
ℓ (cid:1) : ℓ ∈ N} converges to a solution of the optimization problem (2.1).
of the sequence {γ (cid:0) 1
The most challenging step in the alternating algorithm is the computation of the vector λk . For-
tunately, if Λ is a second order cone, problem (2.2) de ﬁning the penalty fun ction Ω(·|Λ) may be
reformulated as a second order cone program (SOCP), see e.g. [5]. To see this, we introduce an
additional variable t ∈ Rn and note that
Ω(β |Λ) = min ( Xi∈Nn
ti + λi : k(2βi , ti − λi )k2 ≤ ti + λi , ti ≥ 0, i ∈ Nn , λ ∈ Λ) .
In particular, in all examples in Section 3, the set Λ is formed by linear constraints and, so, problem
(2.2) is an SOCP. We may then use available tool-boxes to compute the solution of this problem.
However, in special cases the computation of the penalty function may be signiﬁcantly facilitated by
using the analytical formulas derived in Section 3. Here, for simplicity we describe how to do this
in the case of the wedge penalty. For this purpose we say that a vector β ∈ Rn is admissible if, for
every k ∈ Nn , it holds that kβNk k2/√k ≤ kβk2/√n.
The proof of the next lemma is straightforward and we do not elaborate on the details.
Lemma 4.1. If β ∈ Rn and δ ∈ Rp are admissible and kβk2/√n ≤ kδk2/√p then (β , δ) is
admissible.

The iterative algorithm presented in Figure 2 can be used to ﬁ nd the partition J = {Jℓ : ℓ ∈ Nk }
and, so, the vector λ(β ) described in Theorem 3.2. The algorithm processes the components of
vector β in a sequential manner. Initially, the ﬁrst component forms
the only set in the partition.
After the generic iteration t − 1, where the partition is composed of k sets, the index of the next
components, t, is put in a new set Jk+1 . Two cases can occur: the means of the squares of the sets
are in strict descending order, or this order is violated by t he last set. The latter is the only case
that requires further action, so the algorithm merges the la st two sets and repeats until the sets in
the partition are fully ordered. Note that, since the only op eration performed by the algorithm is
the merge of admissible sets, Lemma 4.1 ensures that after each step t the current partition satisﬁes
the conditions (3.4). Moreover, the while loop ensures that after each step the current partition
satisﬁes, for every ℓ ∈ Nk−1 , the constraints kβJℓ k2p|Jℓ | > kβJℓ+1 k2p|Jℓ+1 |. Thus, the output
of the algorithm is the partition J de ﬁned in Theorem 3.2. In the actual implementation of the
algorithm, the means of squares of each set can be saved. This allows us to compute the mean of
squares of a merged set as a weighted mean, which is a constant time operation. Since there are
n − 1 consecutive terms in total, this is also the maximum number of merges that the algorithm can
perform. Each merge requires exactly one additional test, s o we can conclude that the running time
of the algorithm is linear.

5 Numerical simulations

In this section we present some numerical simulations with the proposed method. For simplicity,
we consider data generated noiselessly from y = X β ∗ , where β ∗ ∈ R100 is the true underlying
regression vector, and X is an m × 100 input matrix, m being the sample size. The elements of X
are generated i.i.d. from the standard normal distribution , and the columns of X are then normalized
such that their ℓ2 norm is 1. Since we consider the noiseless case, we solve the interpolation problem
min{Ω(β ) : y = X β}, for different choices of the penalty function Ω. In practice, we solve problem
(2.1) for a tiny value of the parameter ρ = 10−8 , which we found to be sufﬁcient to ensure that the

6

350

300

250

200

150

100

50

r
o
r
r
e
 
l
e
d
o
M

 

Lasso
Box−A
Box−B
Box−C

400

350

300

250

200

150

100

50

r
o
r
r
e
 
l
e
d
o
M

 

Lasso
Wedge
GL−lin

700

600

500

400

300

200

100

r
o
r
r
e
 
l
e
d
o
M

0
 
12

15

18

20
25
Sample size
(a)

50

75

100

0
 
12

15

18

50

75

100

0
 
12

15

18

25
20
Sample size
(b)

5000

4000

r
o
r
r
e
 
l
e
d
o
M

3000

2000

1000

 

Lasso
C−Wedge
GL−ind
GL−hie
GL−con

2500

2000

r
o
r
r
e
 
l
e
d
o
M

1500

1000

500

 

Lasso
W−2
Wedge
GL−lin

80

70

60

50

40

30

20

10

r
o
r
r
e
 
l
e
d
o
M

 

Lasso
Wedge
GL−lin

50

75

100

20
25
Sample size
(c)

 

Lasso
W−3
Wedge
GL−lin

0
 
12

15

18

50

75

30
35
20
25
20
25
Sample size
Sample size
Sample size
(f)
(e)
(d)
Figure 3: Comparison between different penalty methods: (a ) Box vs. Lasso; (b,c) Wedge vs. Hier-
archical group Lasso; (d) Composite wedge; (e) Convex; (f) Cubic. See text for more information

100

100

75

100

15

18

25

28

50

75

0
 
12

50

0
 
22

error term in (2.1) is negligible at the minimum. All experim ents were repeated 50 times, generating
each time a new matrix X . In the ﬁgures we report the average of the model error E[k ˆβ − β ∗ k2
2 ] of
the vector ˆβ learned by each method, as a function of the sample size m. In the following, we discuss
a series of experiments, corresponding to different choice s for the model vector β ∗ and its sparsity
pattern. In all experiments, we solved the optimization problem (2.1) with the algorithm presented
in Section 4. Whenever possible we solved step (4.2) using the formulas derived in Section 3 and
resorted to the solver CVX (http://cvxr.com/cvx/) in the other cases.
Box. In the ﬁrst experiment the model is 10-sparse, where each nonzero component, in a random
position, is an integer uniformly sampled in the interval [−10, 10]. We wish to show that the more
accurate the prior information about the model is, the more p recise the estimate will be. We use
a box penalty (see Theorem 3.1) constructed “around ” the mod el, imagining that an oracle tells us
that each component |β ∗
i | is bounded within an interval. We consider three boxes B [a, b] of different
i | − r)+ and radii r = 5, 1 and 0.1, which we denote as
i |)+ and bi = (|β ∗
sizes, namely ai = (r − |β ∗
Box-A, Box-B and Box-C, respectively. We compare these methods with the Lasso – see Figure 3-a.
As expected, the three box penalties perform better. Moreover, as the radius of a box diminishes,
the amount of information about the true model increases, and the performance improves.
Wedge. In the second experiment, we consider a regression vector, w hose components are nonin-
creasing in absolute value and only a few are nonzero. Speciﬁ cally, we choose a 10-sparse vector:
j = 11 − j , if j ∈ N10 and zero otherwise. We compare the Lasso, which makes no use o f such
β ∗
ordering information, with the wedge penalty Ω(β |W ) (see Example 3.2 and Theorem 3.2) and the
hierarchical group Lasso in [17], which both make use of such information. For the group Lasso
we choose Ω(β ) = Pℓ∈N100 ||βJℓ ||, with Jℓ = {ℓ, ℓ + 1, . . . , 100}, ℓ ∈ N100 . These two methods
are referred to as “Wedge ” and “GL-lin ” in Figure 3-b, respec
tively. As expected both methods
improve over the Lasso, with “GL-lin ” being the best of the tw o. We further tested the robustness
of the methods, by adding two additional nonzero components with value of 10 to the vector β ∗ in a
random position between 20 and 100. This result, reported in Figure 3-c, indicates that “GL-li n ” is
more sensitive to such a perturbation.
Composite wedge. Next we consider a more complex experiment, where the regres sion vector is
sparse within different contiguous regions P1 , . . . , P10 , and the ℓ1 norm on one region is larger than
the ℓ1 norm on the next region. We choose sets Pi = {10(i − 1) + 1, . . . , 10i}, i ∈ N10 and
generate a 6-sparse vector β ∗ whose i-th nonzero element has value 31 − i (decreasing) and is in
a random position in Pi , for i ∈ N6 . We encode this prior knowledge by choosing Ω(β |Λ) with
Λ = (cid:8)λ ∈ R100 : ||λPi ||1 ≥ kλPi+1 ||1 , i ∈ N9(cid:9). This method constraints the sum of the sets to be
nonincreasing and may be interpreted as the composition of the wedge set with an average operation
across the sets Pi , see the discussion at the end of Section 3. This method, which is referred to as “C-
Wedge ” in Figure 3-d, is compared to the Lasso and to three oth er versions of the group Lasso. The

7

25

20

15

10

5

0

25

20

15

10

5

10

8

6

4

2

0

10

8

6

4

2

0
0
Figure 4: Lasso vs. penalty Ω(·|W 2 ) (left) and Ω(·|W 3 ) (Right); see text for more information.

ﬁrst is a standard group Lasso with the nonoverlapping group s Ji = Pi , i ∈ N10 , thus encouraging
the presence of sets of zero elements, which is useful because there are 4 such sets. The second is a
j=i Pj , i ∈ N10 . A problem
variation of the hierarchical group Lasso discussed above w ith Ji = ∪10
with these approaches is that the ℓ2 norm is applied at the level of the individual sets Pi , which does
not promote sparsity within these sets. To counter this effect we can enforce contiguous nonzero
patterns within each of the Pi , as proposed by [10]. That is, we consider as the groups the se ts
formed by all sequences of q ∈ N9 consecutive elements at the beginning or at the end of each of the
sets Pi , for a total of 180 groups. These three groupings will be referred to as “GL-ind ”, “GL-hie’‘,
“GL-con ” in Figure 3-d, respectively. This result indicate
s the advantage of “C-Wedge ” over the
other methods considered. In particular, the group Lasso me thods fall behind our method and the
Lasso, with “GL-con ” being slight better than “GL-ind ” and “
GL-hie ”. Notice also that all group
Lasso methods gradually diminish the model error until they have a point for each dimension, while
our method and the Lasso have a steeper descent, reaching zero at a number of points which is less
than half the number of dimensions.
Convex and Cubic. To show the ﬂexibility of our framework, we consider two furt her examples
of sparse regression vectors with additional structured pr operties. In the ﬁrst example, most of the
components of this vector are zero, but the ﬁrst and the last f ew elements follow a discrete convex
trend. Speciﬁcally, we choose β ∗ = (52 , 42 , 32 , 22 , 1, 0, . . . , 0, 1, 22, 32 , 42 , 52 ) ∈ R100 . In this
case, we expect the penalty function Ω(β |W 2 ) to outperform the Lasso, because it favors vectors
with convex shape. Results are shown in Figure3-e, where this penalty is named “W-2 ”. In lack
of other speciﬁc methods to impose this convex shape constra int, and motivating by the fact that
the ﬁrst few components decrease, we compare it with two meth ods that favors a learned vector
that is decreasing: the Wedge and the group Lasso with Jk = {k , . . . , 100} for k ∈ N100 . These
methods and the Lasso fail to use the prior knowledge of convexity, and are outperformed by using
the constraint set W 2 . The second example considers the case where |β ∗ | ∈ W 3 , namely the
differences of the second order are decreasing. This vector is constructed from the cubic polynomial
p(t) = −t(t − 1.5)(t + 6.5). The polynomial is evaluated at 100 equally spaced (0.1) points, starting
from −7. The resulting vector starts with 5 nonzero components and has then a bump of another
15 elements. We use our method with the penalty Ω(β |W 3 ), which is referred to as “W-3 ” in the
Figure. The model error, compared again with “W-1 ” and group
Lasso linear, is shown in Figure
3-f. Finally, Figure 4 displays the regression vector found by the Lasso and the vector learned by
“W-2 ” (left) and by the Lasso and “W-3 ” (right), in a single ru
n with sample size of 15 and 35,
respectively. The estimated vectors (green) are superpose d to the true vector (black). Our method
provides a better estimate than the Lasso in both cases.

Conclusion

We proposed a family of penalty functions that can be used to model structured sparsity in linear
regression. We provided theoretical, algorithmic and comp utational information about this new
class of penalty functions. Our theoretical observations h ighlight the generality of this framework
to model structured sparsity. An important feature of our ap proach is that it can deal with richer
model structures than current approaches while maintainin g convexity of the penalty function. Our
practical experience indicates that these penalties perfo rm well numerically, improving over state
of the art penalty methods for structure sparsity, suggesti ng that our framework is promising for
applications.
In the future, it would be valuable to extend the ideas presented here to learning
nonlinear sparse regression models. There is also a need to c larify the rate of convergence of the
algorithm presented here.

8

References
[1] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learn-
ing, 73(3):243 –272, 2008.
[2] R.G. Baraniuk, V. Cevher, M.F. Duarte, and C. Hegde. Model-based compressive sensing.
Information Theory, IEEE Transactions on, 56(4):1982 –2001, 2010.
[3] D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.
[4] P.J. Bickel, Y. Ritov, and A.B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.
Annals of Statistics, 37:1705 –1732, 2009.
[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[6] J.M. Danskin. The theory of max-min, with applications. SIAM Journal on Applied Mathe-
matics, 14(4):641 –664, 1966.
[7] J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. In Proceedings of the
26th Annual International Conference on Machine Learning, pages 417 –424. ACM, 2009.
[8] L. Jacob. Structured priors for supervised learning in c omputational biology. 2009. Ph.D.
Thesis.
[9] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In Interna-
tional Conference on Machine Learning (ICML 26), 2009.
[10] R. Jenatton, J.-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing
norms. arXiv:0904.3523v2, 2009.
[11] S. Kim and E.P. Xing. Tree-guided group lasso for multi-task regression with structured spar-
sity. Technical report, 2009. arXiv:0909.1373.
[12] K. Lounici. Sup-norm convergence rate and sign concentration property of Lasso and Dantzig
estimators. Electronic Journal of Statistics, 2:90 –102, 2008.
[13] K. Lounici, M. Pontil, A.B Tsybakov, and S. van de Geer. Taking advantage of sparsity in
multi-task learning.
In Proc. of the 22nd Annual Conference on Learning Theory (COLT),
2009.
[14] A.B. Owen. A robust hybrid of lasso and ridge regression . In Prediction and discovery: AMS-
IMS-SIAM Joint Summer Research Conference, Machine and Statistical Learning: Prediction
and Discovery, volume 443, page 59, 2007.
[15] S.A. van de Geer. High-dimensional generalized linear models and the Lasso. Annals of
Statistics, 36(2):614, 2008.
[16] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society, Series B (Statistical Methodology), 68(1):49 –67, 2006.
[17] P. Zhao, G. Rocha, and B. Yu. Grouped and hierarchical model selection through composite
absolute penalties. Annals of Statistics, 37(6A):3468 –3497, 2009.

9

A Appendix

In this appendix we provide the proof of Theorems 3.2 and 4.1.

A.1 Proof of Theorem 3.2

Before proving the theorem we require some additional notat ion. Given any two disjoint subsets
J, K ⊆ Nn we de ﬁne the region
> kβK k2
QJ,K = (cid:26)β : β ∈ Rn , kβJ k2
|K | (cid:27) .
2
2
|J |
Note that the boundary of this region is determined by the zero set of a homogeneous polynomial of
degree two. We also need the following construction.
De ﬁnition A.1. For every subset S ⊆ Nn−1 we set k = |S | + 1 and label the elements of S in
increasing order as S = {jℓ : ℓ ∈ Nk−1 }. We associate with the subset S a contiguous partition
of Nn , given by J (S ) = {Jℓ : ℓ ∈ Nk }, where we de ﬁne Jℓ := {jℓ−1 + 1, jℓ} and set j0 = 0 and
jk = n.

A subset S of Nn−1 also induces two regions in Rn which play a central role in the identiﬁcation of
the wedge penalty. First, we describe the region which “cros ses over” the induced partition J (S ).
This is de ﬁned to be the set
OS := \ (cid:8)QJℓ ,Jℓ+1 : ℓ ∈ Nk−1 (cid:9) .
In other words, β ∈ OS if the average of the square of its components within each reg ion Jℓ strictly
decreases with ℓ. The next region which is essential in our analysis is the “st ays within ” region,
induced by the partition J (S ). This region requires the notation Jℓ,q := {j : j ∈ Jℓ , j ≤ q} and is
de ﬁned by the equation
IS := \ nQJℓ ,Jℓ,q : q ∈ Jℓ , ℓ ∈ Nko ,
(A.2)
where Q denotes the closure of the set Q. In other words, all vectors β within this region have the
property that, for every set Jℓ ∈ J (S ), the average of the square of a ﬁrst segment of components
of β within this set is not greater than the average over Jℓ . We note that if S is the empty set the
above notation should be interpreted as OS = Rn and
IS = \{QNn ,Nq : q ∈ Nn}.
We also introduce, for every S ∈ Nn−1 the sets
US := OS ∩ IS ∩ (R\{0})n .
We shall prove the following slightly more general version the Theorem 3.2
Theorem A.1. The collection of sets U := {US : S ⊆ Nn−1} forms a partition of (R\{0})n . For
each β ∈ (R\{0})n there is a unique S ∈ Nn−1 such that β ∈ US , and
Ω(β |W ) = Xℓ∈Nk p|Jℓ | kβJℓ k2 ,
where k = |S | + 1. Moreover, the components of the vector λ(β ) := argmin{Γ(β , λ) : λ ∈ W } is
given by the equations λj (β ) = µℓ , j ∈ Jℓ , ℓ ∈ Nk , where
µℓ = kβJℓ k2
.
p|Jℓ |
Proof. First, let us observe that there are n − 1 inequality constraints de ﬁning W . It readily follows
that all vectors in this constraint set are regular, in the sense of optimization theory, see [3, p. 279].
Hence, we can appeal to [3, Prop. 3.3.4, p. 316 and Prop. 3.3.6, p. 322], which state that λ ∈ Rn
++

(A.3)

(A.1)

(A.4)

10

is a solution to the minimum problem determined by the wedge penalty, if and only if there exists a
vector α = (αi : i ∈ Nn−1 ) with nonnegative components such that
β 2
j
+ 1 + αj−1 − αj = 0,
j ∈ Nn ,
−
λ2
j
where we set α0 = αn = 0. Furthermore, the following complementary slackness condi tions hold
true

(A.5)

(A.6)
αj (λj+1 − λj ) = 0, j ∈ Nn−1 .
To unravel these equations, we let S := {j : λj > λj+1 , j ∈ Nn−1}, which is the subset of indexes
corresponding to the constraints that are not tight. When k ≥ 2, we express this set in the form
{jℓ : ℓ ∈ Nk−1 } where k = |S | + 1.
As explained in De ﬁnition A.1, the set S induces the partition J (S ) = {Jℓ : ℓ ∈ Nk } of Nn . When
k = 1 our notation should be interpreted to mean that S is empty and the partition J (S ) consists
only of Nn . In this case, it is easy to solve the equations (A.5) and (A.6). In fact, all components of
the vector λ have a common value, say µ > 0, and by summing both sides of equation (A.5) over
j ∈ Nn we obtain that µ2 = kβk2
2/n. Moreover, summing both sides of the same equation over
j /µ2 + q and, since αq ≥ 0 we conclude that β ∈ IS = US .
j ∈ Nq we obtain that αq = − Pj∈Nq
β 2
We now consider the case that k ≥ 2. Hence, the vector λ has equal components on each subset
Jℓ , which we denote by µℓ , ℓ ∈ Nk−1 . The de ﬁnition of the set S implies that the µℓ are strictly
decreasing and equation (A.6) implies that αj = 0, for every j ∈ S . Summing both sides of equation
(A.5) over j ∈ Jℓ we obtain that
1
ℓ Xj∈Jℓ
β 2
−
j + |Jℓ | = 0
µ2
from which equation (A.4) follows. Since the µℓ are strictly decreasing, we conclude that β ∈ OS .
Moreover, choosing q ∈ Jℓ and summing both sides of equations (A.5) over j ∈ Jℓ,q we obtain that
0 ≤ αq = − kβJℓ,q k2
2
+ |Jℓ,q |
µ2
ℓ
which implies that β ∈ QJℓ ,Jℓ,q
. Since this holds for every q ∈ Jℓ and ℓ ∈ Nk we conclude that
β ∈ IS and therefore, it follows that β ∈ US .
In summary, we have shown that β ∈ US . In particular, this implies that the collection of sets U
covers (R\{0})n . Next, we show that the elements of U are disjoint. To this end, we observe that,
the computation described above can be reversed. That is to say, conversely for any S ⊆ Nn−1 and
β ∈ US we conclude that the vectors α and λ de ﬁne above solve the equations (A.5) and (A.6).
Since the wedge penalty function is strictly convex we know that equations (A.5) and (A.6) have a
unique solution. Now, if β ∈ US ∩ US ′ then it must follow that λ = λ′ . Consequently, since the
vectors λ and λ′ are a constant on any element of their respective partitions J (S ) and J (S ′ ), strictly
decreasing from one element to the next in those partition, i t must be the case that S1 = S2 .

We note that if some components of β are zero we may compute Ω(β |Λ) as a limiting process, since
the function Ω(·|Λ) is continuous.
Proof of Theorem 4.1 We divide the proof into several steps. To this end, we de ﬁne
2 + 2ρΓ(φǫ (β ), λ)
Eǫ (β , λ) := ky − X βk2
and let β (λ) := argmin{Eǫ (α, λ) : α ∈ Rn }.
Step 1. We de ﬁne two sequences, θk = Eǫ (β k , λk−1 ) and νk = Eǫ (β k , λk ) and observe, for any
k ≥ 2, that
(A.7)
θk+1 ≤ νk ≤ θk ≤ νk−1 .
These inequalities follow directly from the de ﬁnition of th e alternating algorithm, see equations (4.1)
and (4.2).
Step 2. We de ﬁne the compact set B = {β : β ∈ Rn , kβk1 ≤ θ1}. From the ﬁrst inequality in
Proposition 2.1, kβk1 ≤ Ω(β |Λ), and inequality (A.7) we conclude, for every k ∈ N, that β k ∈ B .

11

Step 3. We de ﬁne a function g : Rn → R at β ∈ Rn as
g (β ) = min {Eǫ (α, λ(φǫ (β ))) : α ∈ Rn} .
We claim that g is continuous on B . In fact, there exists a constant κ > 0 such that, for every
γ 1 , γ 2 ∈ B , it holds that

(A.9)

(A.8)

|g (γ 1 ) − g (γ 2 )| ≤ κkλ(φǫ (γ 1 )) − λ(φǫ (γ 2 ))k∞ .
The essential ingredient in the proof of this inequality is the fact that by our hypothesis on the set
Λ there exists constant a and b such that, for all β ∈ B , λ(φǫ (β )) ∈ [a, b]n . This fact follows by
Danskin’s Theorem [6].
Step 4. By step 2, there exists a subsequence {β kℓ : ℓ ∈ N} which converges to ˜β ∈ B and, for all
β ∈ Rn and λ ∈ Λ, it holds that
Eǫ ( ˜β , λ(φǫ ( ˜β ))) ≤ Eǫ (β , λ(φǫ ( ˜β ))), Eǫ ( ˜β , λ(φǫ ( ˜β ))) ≤ Eǫ ( ˜β , λ).
Indeed, from step 1 we conclude that there exists ψ ∈ R++ such that
θk = lim
lim
νk = ψ .
k→∞
k→∞
Under our hypothesis the mapping β 7→ λ(β ) is continuous for β ∈ (R\{0})n , we conclude that
λkℓ = λ(φǫ ( ˜β )).
lim
ℓ→∞
By the de ﬁnition of the alternating algorithm, we have, for a ll β ∈ Rn and λ ∈ Λ, that
θk+1 = Eǫ (β k+1 , λk ) ≤ Eǫ (β , λk ),
νk = Eǫ (β k , λk ) ≤ Eǫ (β k , λ).
From this inequality we obtain, passing to limit, inequalities (A.9).
Step 5. The vector ( ˜β , λ(φǫ ( ˜β )) is a stationary point. Indeed, since Λ is admissible, by step 3,
λ(φǫ ( ˜β )) ∈ int(Λ). Therefore, since Eǫ is continuously differentiable this claim follows from step
4.
Step 6. The alternating algorithm converges. This claim follows from the fact that Eǫ is strictly
convex. Hence, Eǫ has a unique global minimum in Rn × Λ, which in virtue of inequalities (A.9) is
attained at ( ˜β , λ(φǫ ( ˜β ))).
The last claim in the theorem follows from the fact that the set {γ (ǫ) : ǫ > 0} is bounded and the
function λ(β ) is continuous.

12


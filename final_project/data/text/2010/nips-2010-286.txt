Relaxed Clipping: A Global Training Method
for Robust Regression and Classi ﬁcation

Yaoliang Yu, Min Yang, Linli Xu, Martha White, Dale Schuurmans
University of Alberta, Dept. Computing Science, Edmonton AB T6G 2E8, Canada
{yaoliang,myang2,linli,whitem,dale}@cs.ualberta.ca

Abstract

Robust regression and classi ﬁcation are often thought to re quire non-convex loss
functions that prevent scalable, global training. However, such a view neglects
the possibility of reformulated training methods that can yield practically solvable
alternatives. A natural way to make a loss function more robust to outliers is
to truncate loss values that exceed a maximum threshold. We demonstrate that
a relaxation of this form of “loss clipping” can be made globa lly solvable and
applicable to any standard loss while guaranteeing robustness against outliers. We
present a generic procedure that can be applied to standard loss functions and
demonstrate improved robustness in regression and classi ﬁ cation problems.

1

Introduction

Robust statistics is a well established ﬁeld that analyzes t he sensitivity of common estimators to out-
liers and provides alternative estimators that achieve improved robustness [11, 13, 17, 23]. Outliers
are understood to be observations that have been corrupted, incorrectly measured, mis-recorded,
drawn under different conditions than those intended, or so atypical as to require separate model-
ing. The main goal of classical robust statistics is to make estimators invariant, or nearly invariant,
to arbitrary changes made to a non-trivial fraction of the sample data—a go al that is equally rele-
vant to machine learning research given that data sets are often collected with limited or no quality
control, making outliers ubiquitous. Unfortunately, the state-of-the-art in robust statistics relies on
non-convex training criteria that have yet to yield efﬁcien t global solution methods [13, 17, 23].

Although many robust regression methods have been proposed in the classical literature, M-
estimators continue to be a dominant approach [13, 17]. These correspond to the standard machine
learning approach of minimizing a sum of prediction errors under a given loss function (assuming
a ﬁxed scaling). M-estimation is reasonably well understoo d, analytically tractable, and provides
a simple framework for trading off between robustness against outliers and data efﬁciency on in-
liers [13, 17]. Unfortunately, robustness in this context comes with a cost: when minimizing a
convex loss, even a single data point can dominate the result. That is, any (non-constant) convex
loss function exhibits necessarily unbounded sensitivity to even a single outlier [17, §5.4.1]. Al-
though unbounded sensitivity can obviously be mitigated by imposing prior bounds on the domain
and range of the data [5, 6], such is not always possible in practice. Instead, the classical literature
achieves bounded outlier sensitivity by considering redescending loss functions (see [17, §2.2] for a
deﬁnition), or more restrictively, bounded loss functions, both of which are inherently non-convex.
Robust regression has also been extensively investigated in computer vision [2, 26], where a similar
conclusion has been reached that bounded loss functions are necessary to counteract the types of
outliers created by edge discontinuities, multiple motions, and specularities in image data.

For classi ﬁcation the story is similar. The attempt to avoid outlier sensitivity has led many to propose
bounded loss functions [8, 15, 18, 19, 25] to replace the standard convex, unbounded losses deployed
in support vector machines and boosting [9] respectively. In fact, [16] has shown that minimizing

1

any convex margin loss cannot achieve robustness to random misclassi ﬁcation noise. The conclusion
reached in the classi ﬁcation literature, as in the regressi on literature, is therefore that non-convexity
is necessary to ensure robustness against outliers —creating an apparent
dilemma: one can achieve
global training via convexity or outlier robustness via boundedness, but not both.

In this paper we present a counterpoint to these pessimistic conclusions. In particular, we present a
general model for bounding any convex loss function, via a process of “loss clipping”, that ensures
bounded sensitivity to outliers. Although the resulting optimization problem is not, by itself, con-
vex, we demonstrate an efﬁcient convex relaxation and round ing procedure that guarantees bounded
response to data—a guarantee that cannot be established for a ny convex loss minimization on its
own. The approach we propose is generic and can be applied to any standard loss function, be it
for regression or classi ﬁcation. Our work is inspired by a nu mber of studies that have investigated
robust estimators in computer vision and machine learning [2, 26, 27, 30]. However, these previ-
ous attempts were either hampered by local optimization or restricted to special cases; none had
guarantees of global training and outlier insensitivity.

Before proceeding it is important to realize that there are many alternative conceptions of “robust-
ness ” in the literature that do not correspond to the notion w e are investigating. For example, work on
“robust optimization” [28, 29] considers minimizing the wo rst case loss achieved given prespeci ﬁed
bounds on the maximum data deviation that will be considered. Although interesting, these results
do not directly bear on the question at hand since we explicitly do not bound the magnitude of the
outliers (i.e. the degree of leverage [23, §1.1], nor the size of response deviations). Another notion
of robustness is algorithmic stability under leave-one-out perturbation [3]. Although loosely related,
algorithmic stability addresses the analysis of given learning procedures rather than describing how
a stable algorithm might be generally achieved in the presence of arbitrary outliers. We also do not
focus on asymptotic or inﬁnitesimal notions from robust sta tistics, such as inﬂuence functions [11],
nor impose boundedness assumptions on the domain and range of the data or the predictor [5, 6].

2 Background

We consider the standard supervised setting where one is given an input matrix X and output targets
y, with the goal of learning a predictor h : ℜm → ℜ. Each row of X gives the feature representation
for one training example, denoted Xi: , with corresponding target yi . We will assume the predictor
can be written as a generalized linear model; that is, the predictions are given by ˆyi = f (Xi:θ) for
a ﬁxed transfer function f (possibly identity) and a vector of parameters θ . For training, we will
consider the standard L2 regularized loss minimization problem

min
θ

L(yi , ˆyi ) = min
θ

n
n
γ
γ
Xi=1
Xi=1
2 kθk2
2 kθk2
2 +
2 +
where L denotes the loss function, γ is the regularization constant, and n denotes the number of
training examples. Normally the loss function L is chosen to be convex in θ so that the minimization
problem can be solved efﬁciently. Although convexity is imp ortant for computational tractability, it
has the undesired side-effect of causing unbounded outlier sensitivity, as mentioned. Obviously, the
severity of the problem will range from minimal to extreme depending on the nature of the distribu-
tion over (x, y). Nevertheless, our goal in this paper will be to eliminate unbounded sensitivity for
convex loss functions while retaining a scalable computational approach.1

L(yi , f (Xi:θ))

(1)

Standard Convex Loss Functions: Our general construction applies to arbitrary convex losses,
but we will demonstrate our methods on standard loss functions employed in regression and clas-
si ﬁcation. A standard example is Bregman divergences, which are deﬁned by taking a strongly
convex differentiable potential Φ then taking the difference between the potential and its ﬁrs t
order Taylor approximation, obtaining a loss LΦ ( ˆyky) = Φ( ˆy) − Φ(y) − φ(y)( ˆy − y), where
φ(y) = Φ′ (y) [1, 14]. Several natural loss functions can be deﬁned this wa y, including least squares
LΦ ( ˆyky) = ( ˆy − y)2/2, using the potential Φ(y) = y2/2, and forward KL-divergence LΦ ( ˆyky) =
ˆy ln ˆy
y + (1 − ˆy) ln 1− ˆy
1−y , using the potential Φ(y) = y ln y + (1 − y) ln(1 − y) for 0 ≤ y ≤ 1.
1All results in this paper extend to reproducing kernel Hilbert spaces via the representer theorem [24], but
for clarity of presentation we will use an explicit feature representation X even though it is not a requirement.

2

A related construction is matching losses [14], which are determined by taking a strictly increasing
differentiable transfer function f to be used in prediction via ˆy = f (z ) where z = x⊤θ . Then, given
a transfer f , a loss can be deﬁned by LF ( ˆzkz ) = R ˆz
z f (ζ ) − f (z ) dζ = F ( ˆz ) − F (z ) − f (z )( ˆz − z )
such that F satis ﬁes F ′ (z ) = f (z ). By deﬁnition, matching losses are also Bregman divergence s,
since F is differentiable and the assumptions on f imply that F is strongly convex. These two loss
constructions are related by the equality LΦ (yk ˆy) = LF ( ˆzkz ) where F is the Legendre-Fenchel
conjugate of Φ [4, §3.3], z = f −1 (y) = φ(y) and ˆz = f −1 ( ˆy) = φ( ˆy) [1, 14]. For example, the
ˆy + (1 − y) ln 1−y
post-prediction KL-divergence y ln y
1− ˆy is equal to the convex pre-prediction loss
LF ( ˆzkz ) = ln(e ˆz + 1) − ln(ez + 1) − σ(z )( ˆz − z ) via the transfer ˆy = σ( ˆz ) = (1 + e− ˆz )−1 . Such
losses are prevalent in regression and probabilistic classi ﬁcation settings.
For discrete classi ﬁcation it is also natural to work with a c ontinuous pre-prediction space ˆz = x⊤θ ,
recovering discrete post-predictions ˆy ∈ {−1, 1} via a step transfer ˆy = sign(z ). Although a step
transfer does not admit the matching loss construction, a surrogate margin loss can be obtained by
taking a nonincreasing function l such that limm→∞ l(m) = 0, then deﬁning Ll ( ˆy , y) = l(y ˆy).
Here y ˆy is known as the classi ﬁcation margin . Standard examples include misclassi ﬁcation loss,
Ll ( ˆy , y) = 1(y ˆy<0) , support vector machine (hinge) loss, Ll ( ˆy , y) = max(0, 1 − y ˆy), binomial
deviance loss, Ll ( ˆy , y) = ln(1 + e−y ˆy ) [12], and Adaboost loss, Ll ( ˆy , y) = e−y ˆy [9]. If the margin
loss is furthermore chosen to be convex, efﬁcient minimizat
ion can be attained.
To unify our presentation below we will simply denote all loss functions by ℓ(y , x⊤θ), with the
understanding that ℓ(y , x⊤θ) = LΦ (x⊤θky) if the loss is Bregman divergence on potential Φ;
ℓ(y , x⊤θ) = LF (x⊤θkf −1 (y)) if the loss is a matching loss with transfer f ; and ℓ(y , x⊤θ) =
l(yx⊤θ) if the loss is a margin loss with margin function l. In each case, the loss is convex in the
parameters θ . Note that by their very convexity these losses cannot be robust: all admit unbounded
sensitivity to a single outlier (the same is also true for L1 loss when applied to regression).

Bounded loss functions: As observed, non-convex loss functions are necessary to bound the ef-
fects of outliers [17]. Black and Rangarajan [2] provide a useful catalog of bounded and redescend-
ing loss functions for robust regression, of which a representative example is the Geman and Mc-
Clure loss L(y , ˆy) = ( ˆy − y)2/(τ + ( ˆy − y)2 ) for τ > 0; see Figure 1. Unfortunately, as Figure 1
makes plain, boundedness implies non-convexity (for any non-constant function). It therefore ap-
pears that bounded loss functions achieve robustness at the cost of losing global training guarantees.
Our goal is to show that robustness and efﬁcient global train ing are not mutually exclusive. Despite
extensive research on regression and classi ﬁcation, almos t no work we are aware of (save perhaps
[30] in a limited way) attempts to reconcile robustness to outliers with global training algorithms.

3 Loss Clipping

Adapting the ideas of [2, 27, 30], given any convex loss ℓ(y , x⊤θ) deﬁne the clipped loss as
ℓc (y , x⊤θ) = min(1, ℓ(y , x⊤θ)).
Figure 1 demonstrates loss clipping for some standard loss functions. Given a clipped loss, a robust
form of training problem (1) can be written as

(2)

min
θ

(3)

ℓc (yi , Xi:θ).

n
γ
Xi=1
2 kθk2
2 +
Clearly such a training objective bounds the inﬂuence of any one training example on the ﬁnal re-
sult. Unfortunately, the formulation (3) is not computationally convenient because the optimization
problem it poses is neither convex nor smooth. To make progress on the computational question
we exploit a key observation: for any loss function, its corresponding clipped loss can be indirectly
expressed by an auxiliary optimization of a smooth objective (if the original loss function itself was
smooth). That is, given a loss ℓ(y , x⊤θ) deﬁne the corresponding ρ-relaxed loss to be
ℓρ (y , x⊤θ) = ρℓ(y , x⊤θ) + 1 − ρ
(4)
for 0 ≤ ρ ≤ 1; see Figure 1. This construction is an instance of an outlier process as described
in [2] and is motivated by a special case hinge-loss construction originally proposed in [30]. The

3

s
s
o
l

0
y − ˆy
Figure 1: Comparing standard losses (dashed) with corresponding “clipped” los
ses (solid), ρ-relaxed losses
(dotted), and non-convex robust losses (dash-dotted). Left: squared loss (dashed), clipped (solid), 1/3-relaxed
(dotted), robust Geman and McClure loss [2] (dash-dotted). Center: SVM hinge loss (dashed), clipped [27, 30]
(solid), 1/2-relaxed (upper dotted), robust 1 − tanh(y ˆy) loss [19] (dash-dotted). Right: Adaboost exponential
loss (dashed), clipped (solid), 1/2-relaxed (upper dotted), robust 1 − tanh(y ˆy) loss [19] (dash-dotted).

0
y ˆy

0
y ˆy

ρ-relaxation provides a convenient characterization of any clipped loss, since it can be shown in
general that minimizing a corresponding ρ-relaxed loss is equivalent to minimizing the clipped loss.
Proposition 1 For any loss function ℓ(y , x⊤θ), we have ℓc (y , x⊤θ) = min0≤ρ≤1 ℓρ (y , x⊤θ).
(The proof is straightforward, but it is given in the supplement for completeness.) Proposition 1
now allows us to reformulate (3) as a smooth optimization using the fact that the optimization is
completely separable between the ρi variables:

min
0≤ρ≤1

(3) = min
θ

γ
2 kθk2
2 +

n
Xi=1
Unfortunately, the resulting problem is not jointly convex in ρ and θ even though it is convex in each
given the other. Such marginal convexity might suggest that an alternating minimization strategy,
however the proof of Proposition 1 shows that each minimization over ρ will result in ρi = 0 for
losses greater than 1, or ρi = 1 for losses less than 1. Such discrete assignments immediately causes
the search to get trapped in local minima, requiring a more sophisticated approach to be considered.

ρi ℓ(yi , Xi:θ) + 1 − ρi .

(5)

4 A Convex Relaxation

One contribution of this paper is to derive an exact reformulation of (5) that admits a convex re-
laxation and rounding scheme that retain bounded sensitivity to outliers. We ﬁrst show how the
relaxation can be efﬁciently solved by a scalable algorithm that eliminates any need for semideﬁnite
programming, then provide a guarantee of bounded outlier sensitivity in Section 5.

(6)

Reformulation: To ease the notational burden, let us rewrite (5) in matrix-vector form
min
(5) = min
R(ρ, θ)
0≤ρ≤1
θ
γ
2 kθk2 + ρ⊤ ℓ(y, X θ) + 1⊤ (1 − ρ).
where
(7)
R(ρ, θ) =
Here 1 denotes the vector of all 1s, and it is understood that ℓ(y, X θ) refers to the n × 1 vector
of individual training losses. Given that ℓ(·, ·) is convex in its second argument we will be able to
exploit Fenchel duality to re-express the min-min form (6) into a min-max form that will serve as
the basis for the subsequent relaxation. In particular, consider the deﬁnition
αx⊤θ − ℓ(y , x⊤θ).
ℓ∗ (y , α) = sup
θ
By construction, ℓ∗ (y , α) is guaranteed to be convex in α since it is a pointwise maximum over
linear functions [4, §3.2].
Lemma 1 For any convex differentiable loss function ℓ(y , x⊤θ) such that the level sets of ℓα (v) =
αx⊤ (θ − v) + ℓ(y , x⊤v) are bounded, we have
ℓ(y , x⊤θ) = sup
α

αx⊤θ − ℓ∗ (y , α).

(8)

(9)

4

(This is a standard result, but a proof is given in the supplement for completeness.) For standard
losses ℓ∗ (y , α) can be computed explicitly [1, 7]. For example, if ℓ(y , x⊤θ) = (y − x⊤θ)2 /2 then
ℓ∗ (y , α) = α2/2 + αy . Now let ∆(α) denote putting α in the main diagonal of a square matrix and
let ℓ∗ (y, α) refer to the n × 1 vector of dual values over training examples. We can then express the
main reformulation as follows.

(6) =

Theorem 1 Let K = XX ⊤ denote the kernel matrix over input data. Then
α −(n + 1) ν ⊤T (α)ν
min
sup
− 1
1, ν1= 1
1≤ν≤ 1
, kν k=1
√n+1
√n+1
√n+1
where ν is an (n + 1) × 1 variable, α is an n × 1 variable, and the matrix T (α) is given by
(ℓ∗ (y, α) + 1)⊤
4 (cid:20) 2(1⊤ ℓ∗ (y, α) − n)
(cid:21) . (11)
8γ (cid:20) 1⊤
I (cid:21) ∆(α)K∆(α) [ 1 I ] +
1
1
T (α) =
ℓ∗ (y, α) + 1
0

(10)

The proof consists in ﬁrst dualizing θ in (6) via Lemma 1, which establishes the key relationship
θ = − 1
(12)
γ X ⊤∆(ρ)α.
The remainder of the proof is merely algebra: given a solution ν to (10), the corresponding solution
2 (1 + ν 2:n+1√n + 1). See the supplement for full details.
ρ to (6) can be recovered via ρ = 1
Note that the formulation (10) given in Theorem 1 is exact. No approximation to the problem (6)
has been introduced to this point. Unfortunately, as in (6), the formulation (10) is still not directly
amenable to an efﬁcient algorithm: the objective is concave in α, conveniently, but it is not convex
in ν . The advantage attained by (10) however is that we can now derive an effective relaxation.

1

Relaxation: Let δ (M ) denote the main diagonal vector of the square matrix M and let tr(M )
denote the trace. Consider the following relaxation
α −(n + 1) tr(M T (α))
min
(10) ≥
sup
M (cid:23)0, δ(M )= 1
n+1
1 −(n + 1) tr(M T (α))
= sup
min
M (cid:23)0, δ(M )= 1
α
n+1
where we used strong minimax duality to obtain (14) from (13): since the constraint region on M
is compact and the inner objective is concave and convex in α and M respectively, Sion’s mini-
max theorem is applicable [22, §37]. Next enforce the constraint δ (M ) = 1
n+1 1 with a Lagrange
multiplier λ:
M (cid:23)0, tr(M )=1 −(n + 1) tr(M T (α)) + λ⊤ (1 − (n + 1)δ (M ))
min
trhM (T (α) + ∆(λ)) i.
λ⊤1 − (n + 1)
max
= sup
M (cid:23)0, tr(M )=1
α, λ
This relaxed formulation (16) is now amenable to efﬁcient gl obal optimization: The outer problem
is jointly concave in α and λ, since it is a pointwise minimum of concave functions. The inner
optimization with respect to M can now be simpli ﬁed by exploiting the well known result [21] :
ν ⊤ hT (α) + ∆(λ)iν .
trhM (T (α) + ∆(λ)) i = max
max
M (cid:23)0, tr(M )=1
kν k=1
Therefore, given α and λ, the inner problem is solved by the maximum eigenvector of T (α)+∆(λ).

(14) = sup
α, λ

(17)

(15)

(13)

(14)

(16)

Optimization Procedure: Given training data, an outer maximization can be executed jointly over
α and λ to maximize (16). This outer problem is concave in α and λ hence no local maxima exist.
Although the outer problem is not smooth, many effective methods exist for nonsmooth convex
optimization [20, 31]. Each outer function evaluation (and subgradient calculation) requires the
inner problem (17) to be solved. Fortunately, a simple power method [10] can be used to efﬁciently
compute a maximum eigenvector solution to the inner problem by only performing matrix-vector
multiplications on the individual factors of the two low rank matrices making up T (α), meaning the
inner problem can be solved without ever forming a large n × n matrix T (α). That is, if X is n × m
each inner iteration requires at most O(nm) computation.

5

Solution Recovery: At a solution, the values of (13) –(16) are equal, and all prov ide a lower bound
on the original objective (6). Ideally, given a maximizer α∗ for (14) one would recover a prediction
model θ via (12). However, (12) requires ρ to be acquired ﬁrst, which could be obtained from a
ν that solves (10). Unfortunately, the relaxation step taken in (13) means that the solution to (14)
(recovered from the ν that solves (17)) does not necessarily solve (10): the inner solution ν in (17)
might not be unique. If it is unique, we immediately have the optimal solution to (10) hence an
exact solution to the original problem (6). More typically, however, the maximum eigenvector is
not unique at (α∗ , λ∗ ), meaning that a gap has been introduced—this occurs if and onl
y if the inner
solution M ∗ to (14) is not rank 1. In such cases we need to use a rounding procedure to recover an
effective rank 1 approximation.

Rounding Method: Given the inner maximizer (α∗ , λ∗ ) of (16) we do not need to explicitly
construct the outer minimizer M ∗ . Instead, it sufﬁces to construct a basis for M ∗ by collecting the
set of maximum eigenvectors ˜V = { ˜ν 1 , ..., ˜ν k } of T (α∗ ) + ∆(λ∗ ) in (17) (note that k is usually
much smaller than n + 1). A solution can then be indirectly obtained by solving a small semideﬁnite
program to recover a k × k matrix C ∗ that satis ﬁes C ∗ (cid:23) 0 and δ ( ˜V C ∗ ˜V ⊤ ) = 1
n+1 1. Note that
C ∗ = Q∗Σ∗Q∗⊤ for some orthonormal Q∗ and diagonal Σ∗ where σ∗j ≥ 0 and Pk
j=1 σ∗j = 1, hence
M ∗ = V ∗Σ∗V ∗⊤ such that V ∗ = {ν ∗1 , ..., ν ∗k } = ˜V Q∗ . Given V ∗ and Σ∗ a rounded solution for ˆρ
2 (cid:0)1 + ¯ν ∗2:n+1√n + 1(cid:1).
can be recovered simply by computing ¯ν ∗ = Pk
j=1 σ∗j ν ∗j then setting ˆρ = 1
From the constraints on C ∗ it follows that −1√n+1 ≤ ¯ν ∗j ≤ 1√n+1
hence 0 ≤ ˆρj ≤ 1 ∀j (details in
the supplement). Finally, instead of relying on (12) to recover the model parameters ˆθ from ˆρ, we
explicitly minimize the ˆρ-relaxed loss (7) given ˆρ to recover ˆθ via ˆθ = arg minθ R( ˆρ, θ).
Although the rounding step has introduced an approximation, we establish that bounded outlier sen-
sitivity can still be retained, even after the above relaxation and rounding processes, and demonstrate
experimentally that the gap from optimality is generally not too large.

5 Bounding Outlier Sensitivity

Thus far we have proposed a robust training objective, provided an efﬁcient convex relaxation that
establishes a lower bound, and proposed a simple rounding method for recovering an approximate
solution. The question remains as to whether the approximate solution retains bounded sensitivity
to outliers (or to leverage points [23, §1.1]). Let (ρ∗ , θ∗ ) denote the joint minimizer of (6) and let
( ˆρ, ˆθ) denote the approximate solution obtained from the procedure above.
First, observe that an upper bound on the approximation error can be easily computed by subtract-
ing the lower bound value obtained in (14) –(16) from R( ˆρ, ˆθ). Our experiments below show that
reasonable gaps are obtained in this way. Nevertheless one would still like to guarantee that the gap
stays bounded in the presence of arbitrary outliers and leverage points.

Theorem 2 ˆR( ˆρ, α∗ ) ≤ 2R(ρ∗ , θ∗ ) ≤ 2n, where ˆR( ˆρ, α∗ ) is the value of (10) at the rounded
solution ˆρ. Furthermore, if the unclipped loss ℓ(y , ˆy) is b-Lipschitz in ˆy for b < ∞ and either y or
K remains bounded, then there exists a c < ∞ such that R( ˆρ, ˆθ) ≤ c.
That is, the ρ-relaxed loss obtained by the rounded solution stays bounded in this case, even when
accounting for the proposed relaxation and rounding procedure and data perturbation. The complete
proof takes some space, however the key steps are to show that −(n+ 1)tr(M ∗T (α∗ )) ≤ R(ρ∗ , θ∗ ),
and then use this to establish that ˆR( ˆρ, α∗ ) ≤ 2R(ρ∗ , θ∗ ) and R( ˆρ, ˆθ) ≤ c, respectively (full details
in the supplement). Thus, ( ˆρ, ˆθ) will not chase outliers or leverage points arbitrarily in this situation.
Note that the proposed method cannot be characterized by minimizing a ﬁxed convex loss. That is,
the tightest convex upper bound for any convex loss function is simply given by the function itself,
which corresponds to setting ρi = 1 for every training example. By contrast, our approximation
method does not choose a constant ρi = 1 for every training example, but instead adaptively chooses
ρi values, closer to 1 for inliers and closer to 0 for outliers. The resulting upper bound on the clipped
loss (hence on the misclassi ﬁcation error in the margin loss case) is much tighter than that achieved
by simply minimizing a convex loss. This outcome is demonstrated clearly in our experiments.

6

60

40

20

y

0

−20

−40

−60
−6
 

 

Data
ClipAlt (local)
L1
L2
ClipRelax

−4

−2

0
x
(a)

2

4

6

60

40

20

y

0

−20

−40

−60
 
−6

 

Data
ClipAlt (local)
L1
L2
ClipRelax

−4

−2

0
x
(b)

2

4

6

60

40

20

y

0

−20

−40

−60
 
−6

 

Data
ClipAlt (local)
L1
L2
ClipRelax

−4

−2

0
x
(c)

2

4

6

Figure 2: Comparison on three demonstration data sets.

Loss
L2
L1
HuberM
GM (local)
ClipAlt (local)
ClipRelax
OptimGap

p = 0.0
2.53 ± 0.0015
2.53 ± 0.0015
2.52 ± 0.0015
2.53 ± 0.0015
2.53 ± 0.0019
2.53 ± 0.0016
1.65% ± 0.31%

outlier probability
p = 0.2
25.11 ± 13.78
26.52 ± 16.09
12.02 ± 5.33
2.60 ± 0.10
2.75 ± 0.27
2.68 ± 0.12
0.10% ± 0.22%

p = 0.4
19.04 ± 15.62
27.14 ± 22.40
12.30 ± 5.87
2.62 ± 0.09
2.81 ± 0.27
2.53 ± 0.87
0.70% ± 1.31%

Table 1: Synthetic experiment with n = 200, m = 5, and t = 500. Test error rates (RMSE) on clean
data (average ± standard deviations) at different outlier probabilities p, 20 repeats. The bottom row shows the
relative gap obtained between the ρ-relaxed loss of the rounded solution and the computed lower bound (16).

6 Experimental Results

In this section, we experimentally evaluate the preceding technical developments on synthetic and
real data for both regression and classi ﬁcation.
Regression: We ﬁrst illustrate the behavior of the various regression te chniques by a simple demon-
stration. In Figure 2 (a) and (b), we generate a cluster of linearly related data y = x in a small interval
about the origin, then add outliers. In Figure 2 (c) the target linear model is mixed with another more
dispersed model. We compare the behaviours of standard regression losses: least-squares (L2), L1
(L1), the Huber minimax loss (HuberM) [13, 17], and the robust Geman and McClure loss (GM) [2].
To these we compare the proposed relaxed method (ClipRelax), along with an alternating minimizer
of the clipped loss (ClipAlt). (In this problem the value of γ has little effect, and is simply set to 0.1.)
Figure 2 demonstrates that the three convex losses, L2, L1 and HuberM, are dominated by outliers.
By contrast, ClipRelax successfully found the correct linear model in each case. Note that the robust
GM loss ﬁnds two different minima, corresponding to that of L 2 and ClipRelax respectively, hence
it was not depicted in the plot. ClipAlt also gets trapped in local minima as expected: it ﬁnds the
correct model in Figure 2 (a) but incorrect models in Figure 2 (b) and (c).

In our second synthetic regression experiment we consider larger problems. Here a target weight
vector θ is drawn from N (0, I ), with inputs Xi: sampled uniformly from [0, 1]m , m = 5. The
outputs yi are computed as yi = Xi:θ + ǫi , ǫi ∼ N (0, 1
4 ). We then seed the data set with outliers
by randomly re-sampling each yi (and Xi: ) from N (0, 105 ) and N (0, 102 ) respectively, governed
by an outlier probability p. Here 200 of the 700 examples are randomly chosen as the training set
and the rest used for testing. We compare the same six methods: L2, L1, HuberM, GM, ClipAlt and
ClipRelax. The regularization parameter γ was set on a separate validation set. These experiments
are repeated 20 times and average (Huber loss) test errors on clean data are reported (with standard
deviations) in Table 1. Clearly, the outliers signi ﬁcantly affect the performance of least squares. In
this case the proposed relaxation performs comparably to the the non-convex GM loss. Interestingly,
this experiment shows that the relative gap between the ρ-robust loss obtained by the proposed
method and the lower bound on the optimal ρ-robust loss (16) remains remarkably small, indicating
our robust relaxation (almost) optimally minimizes the original non-convex clipped loss.

7

Loss
L2
L1
HuberM
GM (local)
ClipAlt (local)
ClipRelax

Astronomy
(1, 46, 46)
2.484
0.170
0.149
0.166
0.176
0.131

Cal-housing
(8, 100, 1000)
804.5 ± 892.5
0.325 ± 0.046
0.306 ± 0.050
0.329 ± 0.048
0.329 ± 0.048
0.136 ± 0.155

Pumadyn
(32, 500, 1000)
1.300 × 105 ± 68.29
5.133 ± 0.056
5.377 ± 0.007
4.399 ± 0.003
4.075 ± 1 × 10−6
4.075 ± 1 × 10−6

Table 2: Error rates (average root mean squared error, ± standard deviations) for the different regression
estimators on various data sets. The values of (m, n, tt) are indicated for each data set, where m is the number
of features, n is the number of training examples, and tt the number of testing samples.

Loss
Logit
1-tanh (local)
ClipAlt (local)
ClipRelax

p = 0.02
4.88 ± 6.17
0.91 ± 1.93
0.46 ± 0.64
0.26 ± 0.34

p = 0.05
1.61 ± 7.23
2.30 ± 2.85
1.51 ± 1.45
0.78 ± 0.78

p = 0.1
17.67 ± 4.00
6.49 ± 4.32
4.27 ± 2.57
2.49 ± 3.38

p = 0.2
19.53 ± 2.91
13.96 ± 3.38
11.32 ± 3.48
10.10 ± 8.21

Table 3: Misclassi ﬁcation error rates on clean data (average error, ± standard deviations) on the Long-Servedio
problem [16] with increasing noise levels p.

Finally, we investigated the behavior of the regression methods on a few real data sets. We chose
three data sets: astronomy data containing outliers from [23], and two UCI data sets, seeding the
the UCI data sets with outliers. Test results are reported on clean data to avoid skewing the reported
results. For UCI data, outliers were added by resampling Xi: and yi from N (0, 1000), with 5%
outliers. The regularization parameter γ was chosen through 10-fold cross validation on the training
set. Note that in real regression problems one needs to obtain an estimate for the the scale, given
by the true standard deviation of the noise in the data. Here we estimated the scale using the mean
absolute deviation, a robust approach commonly used in the robust statistics literature [17].
In
Table 2, one can see that on both data sets, ClipRelax clearly outperformed the other methods. L2 is
clearly skewed by the outliers. Unsurprisingly, the classical robust loss functions, L1 and HuberM,
perform better than L2 in the presence of outliers, but not as well as ClipRelax.
Classi ﬁcation: We investigated the well known case study from [16] and compared the proposed
method to logistic regression (i.e. the logit, or binomial deviance loss [12]) and the robust 1 − tanh
loss [19] in a classi ﬁcation context. Here 200 examples were drawn from the target distribution with
label noise applied at various levels. The experiment was repeated 50 times to obtain average results
and standard deviations. Table 3 shows the test error performance in clean data of the different
methods. From these results one can conclude that ClipRelax is more robust than standard logit
training. Training with logit loss is slightly better than the tanh loss algorithm in terms of training
loss, but not very signi ﬁcantly. It is interesting to see tha t when the prediction error is measured on
clean labels ClipRelax generalizes signi ﬁcantly better th an the robust 1− tanh loss. This implies that
the classi ﬁcation model produced by ClipRelax is closer to t he true model despite of the presence of
outliers, demonstrating that the proposed method can be robust in a simple classi ﬁcation context.

7 Conclusion

We have proposed a robust estimation method for regression and classi ﬁcation based on a notion
of “loss-clipping”. Although the method is not as fast as sta ndard convex training, it is scalable to
problems of moderate size. The key beneﬁt is competitive (or better) estimation quality than the
state-of-the-art in robust estimation, while ensuring provable robustness to outliers and computable
bounds on the optimality gap. To the best of our knowledge these two properties have never been
previously achieved simultaneously. It would be interesting to investigate whether the techniques
developed can also be applied to other forms of robust estimators from the classical literature, includ-
ing GM, MM, L, R and S estimators [11, 13, 17, 23]. Connections with algorithmic stability [3] and
inﬂuence function based analysis [5, 6, 11] merit further in vestigation. Obtaining tighter bounds on
approximation quality that would enable a proof of consistency also remains an important challenge.

8

References

[1] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with Bregman divergences. Journal of
Machine Learning Research, 6:1705–1749, 2005.
[2] M. Black and A. Rangarajan. On the uniﬁcation of line processes, ou tlier rejection, and robust statistics
with applications in early vision. International Journal of Computer Vision, 19(1):57–91, 1996.
[3] O. Bousquet and A. Elisseeff. Stability and generalization. J. of Machine Learning Research, 2, 2002.
[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge U. Press, 2004.
[5] A. Christmann and I. Steinwart. On robustness properties of convex risk minimization methods for pattern
recognition. Journal of Machine Learning Research, 5:1007–1034, 2004.
[6] A. Christmann and I. Steinwart. Consistency and robustness of kernel-based regression in convex risk
minimization. Bernoulli, 13(3):799–819, 2007.
[7] M. Collins, R. Schapire, and Y. Singer. Logistic regression, AdaBoost and Bregman distances. Machine
Learning, 48, 2002.
[8] Y. Freund. A more robust boosting algorithm, 2009. arXiv.org:0905.2138.
[9] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to
boosting. Journal of Computer and Systems Sciences, 55(1):119–139, 1997.
[10] G. Golub and C. Van Loan. Matrix Computations. Johns Hopkins U. Press, 1996.
[11] F. Hampel, E. Ronchetti, P. Rousseeuw, and W. Stahel. Robust Statistics: The Approach Based on Inﬂu-
ence Functions. Wiley, 1986.
[12] T. Hastie, R. Tibshirani, and J. Friedman. Elements of Statistical Learning. Springer, 2nd edition, 2009.
[13] P. Huber and E. Ronchetti. Robust Statistics. Wiley, 2nd edition, 2009.
[14] J. Kivinen and M. Warmuth. Relative loss bounds for multidimensional regression problems. Machine
Learning, 45:301–329, 2001.
[15] N. Krause and Y. Singer. Leveraging the margin more carefully.
Conference on Machine Learning (ICML), 2004.
[16] P. Long and R. Servedio. Random classiﬁcation noise defeats all c onvex potential boosters. Machine
Learning, 78:287–304, 2010.
[17] R. Maronna, R.D. Martin, and V. Yohai. Robust Statistics: Theory and Methods. Wiley, 2006.
[18] H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classiﬁcation: theory, ro-
bustness to outliers, and SavageBoost. In Advances in Neural Information Processing Systems (NIPS),
volume 21, pages 1049–1056, 2008.
[19] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses.
In Advances in Large Margin Classiﬁers . MIT Press, 2000.
[20] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer, 1994.
[21] M. Overton and R. Womersley. Optimality conditions and duality theory for minimizing sums of the
largest eigenvalues of symmetric matrices. Mathematical Programming, 62(2):321–357, 1993.
[22] R. Rockafellar. Convex Analysis. Princeton U. Press, 1970.
[23] P. Rousseeuw and A. Leroy. Robust Regression and Outlier Detection. Wiley, 1987.
[24] B. Schoelkopf and A. Smola. Learning with Kernels. MIT Press, 2002.
[25] X. Shen, G. Tseng, X. Zhang, and W.-H. Wong. On ψ -learning. Journal of the American Statistical
Association, 98(463):724–734, 2003.
[26] C. Stewart. Robust parameter estimation in computer vision. SIAM Review, 41(3), 1999.
[27] Y. Wu and Y. Liu. Robust truncated hinge loss support vector machines. Journal of the American Statis-
tical Association, 102(479):974–983, 2007.
[28] H. Xu, C. Caramanis, and S. Mannor. Robust regression and Lasso. In Advances in Neural Information
Processing Systems (NIPS), volume 21, pages 1801–1808, 2008.
[29] H. Xu, C. Caramanis, and S. Mannor. Robustness and regularization of support vector machines. Journal
of Machine Learning Research, 10:1485–1510, 2009.
[30] L. Xu, K. Crammer, and D. Schuurmans. Robust support vector machine training via convex outlier
ablation. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI) , 2006.
[31] J. Yu, S. Vishwanathan, S. G ¨unter, and N. Schraudolph. A quasi-Newton approach to nonsmooth convex
optimization problems in machine learning. J. of Machine Learning Research, 11:1145–1200, 2010.

In Proceedings of the International

9


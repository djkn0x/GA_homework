Short-term memory in neuronal networks through
dynamical compressed sensing

Surya Ganguli
Sloan-Swartz Center for Theoretical Neurobiology, UCSF, San Francisco, CA 94143
surya@phy.ucsf.edu

Haim Sompolinsky
Interdisciplinary Center for Neural Computation, Hebrew University, Jerusalem 91904, Israel
and Center for Brain Science, Harvard University, Cambridge, Massachusetts 02138, USA
haim@fiz.huji.ac.il

Abstract

Recent proposals suggest that large, generic neuronal networks could store mem-
ory traces of past input sequences in their instantaneous state. Such a proposal
raises important theoretical questions about the duration of these memory traces
and their dependence on network size, connectivity and signal statistics. Prior
work, in the case of gaussian input sequences and linear neuronal networks, shows
that the duration of memory traces in a network cannot exceed the number of neu-
rons (in units of the neuronal time constant), and that no network can out-perform
an equivalent feedforward network. However a more ethologically relevant sce-
nario is that of sparse input sequences. In this scenario, we show how linear neural
networks can essentially perform compressed sensing (CS) of past inputs, thereby
attaining a memory capacity that exceeds the number of neurons. This enhanced
capacity is achieved by a class of “orthogonal” recurrent networks and not by
feedforward networks or generic recurrent networks. We exploit techniques from
the statistical physics of disordered systems to analytically compute the decay of
memory traces in such networks as a function of network size, signal sparsity and
integration time. Alternately, viewed purely from the perspective of CS, this work
introduces a new ensemble of measurement matrices derived from dynamical sys-
tems, and provides a theoretical analysis of their asymptotic performance.

1

Introduction

How neuronal networks can store a memory trace for recent sequences of stimuli is a central question
in theoretical neuroscience. The inﬂuential idea of attractor dynamics [1], suggests how single
stimuli can be stored as stable patterns of activity, or ﬁxed point attractors, in the dynamics of
recurrent networks. But, such simple ﬁxed points are incapable of storing sequences. More recent
proposals [2, 3, 4] suggest that recurrent networks could store temporal sequences of inputs in their
ongoing, transient activity, even if they do not have nontrivial ﬁxed points. In principle, past inputs
could be read out from the instantaneous activity of the network. However, the theoretical principles
underlying the ability of recurrent networks to store temporal sequences in their transient dynamics
are poorly understood. For example, how long can memory traces last in such networks, and how
does memory capacity depend on parameters like network size, connectivity, or input statistics?
Several recent theoretical studies have made progress on these issues in the case of linear neuronal
networks and gaussian input statistics. Even in this simple setting, the relationship between the
memory properties of a neural network and its connectivity is nonlinear, and so understanding this

1

relationship poses an interesting challenge. Jaeger [4] proved a rigorous sum-rule (reviewed in more
detail below) which showed that even in the absence of noise, no recurrent network can remember
inputs for an amount of time that exceeds the number of neurons (in units of the neuronal time
constant) in the network. White et al. [5] showed that in the presence of noise, a special class
of “orthogonal” networks, but not generic recurrent networks, could have memory that scales with
network size. And ﬁnally, Ganguli et. al. [6] used the theory of Fisher information to show that the
memory of a recurrent network cannot exceed that of an equivalent feedforward network, at least for
times up to the network size, in units of the neuronal time constant.
A key reason theoretical progress was possible in these works was that even though the optimal
estimate of past inputs was a nonlinear function of the network connectivity, it was still a linear
function of the current network state, due to the gaussianity of the signal (and possible noise) and
the linearity of the dynamics. It is not clear for example, how these results would generalize to
nongaussian signals, whose reconstruction from the current network state would require nonlinear
operations. Here we report theoretical progress on understanding the memory capacity of linear
recurrent networks for an important class of nongaussian signals, namely sparse signals. Indeed a
wide variety of temporal signals of interest are sparse in some basis, for example human speech
in a wavelet basis. We use ideas from compressed sensing (CS) to deﬁne memory curves which
capture the decay of memory traces in neural networks for sparse signals, and provide methods to
compute these curves analytically. We ﬁnd strikingly different properties of memory curves in the
sparse setting compared to the gaussian setting. Although motivated by the problem of memory, we
also contribute new results to the ﬁeld of CS itself, by introducing and analyzing new classes of CS
measurement matrices derived from dynamical systems. Our main results are summarized in the
discussion section. In the next section, we begin by reviewing more quantitatively the problem of
short-term memory in neuronal networks, compressed sensing, and the relation between the two.

2 Short-term memory as dynamical compressed sensing.

Consider a discrete time network dynamics given by
x(n) = Wx(n − 1) + vs0 (n).
(1)
Here a scalar, time dependent signal s0 (n) drives a recurrent network of N neurons. x(n) ∈ RN
is the network state at time n, W is an N × N recurrent connectivity matrix, and v is a vector of
feedforward connections from the signal into the network. We choose v to have norm 1, and we
demand that the dynamics be stable so that if ρ is the squared magnitude of the largest eigenvalue
of W, then ρ < 1. If we think of the signal history {s0 (n − k)|k ≥ 0} as an inﬁnite dimensional
k is s(n − k), then the current network state x is linearly
temporal vector s0 whose k’th component s0
related to s through the effective N by ∞ measurement matrix A, i.e. x = As0 , where the matrix
elements
Aµk = (Wk v)µ , µ = 1, . . . , N , k = 0, . . . , ∞
reﬂect the effect of an input k timesteps in the past on the activity of neuron µ. The extent to
which the dynamical system in (1) can remember the past can then be quantiﬁed by how well one
can recover s0 from x [4, 5, 6].
In the case where the signal has zero mean gaussian statistics
with covariance (cid:104)s0
l (cid:105) = δk,l , the optimal, minimum mean squared error estimate ˆs of the signal
k s0
history is given by ˆs = AT (AAT )−1x. The correlation between the estimate ˆsk and the true signal
important sum-rule for M (k): (cid:80)∞
k , averaged over the gaussian statistics of s0 , then deﬁnes a memory curve M (k) = (cid:104)ˆsk s0
k (cid:105)s0 ,
s0
whose decay as k increases quantiﬁes the decay of memory for past inputs in (1). Jaeger proved an
k=0 M (k) = N for any recurrent connectivity W and feedforward
connectivity v. Given that M (k) cannot exceed 1 for any k , an important consequence of this sum-
rule is that it is not possible to recover an input signal k timesteps into the past when k is much
larger than N in the sense that ˆsk will be at most weakly correlated with s0
k .
Generically, one may not hope to remember sequences lasting longer than N timesteps with only N
neurons, but in the case of temporally sparse inputs, the ﬁeld of compressed sensing (CS) suggests
this may be possible. CS [7, 8] shows how to recover a sparse T dimensional signal s0 , in which
only a fraction f of the elements are nonzero, from a set of N linear measurements x = As0 where
A is an N by T measurement matrix with N < T . One approach to recovering an estimate ˆs of s0

(2)

2

|si |

subject to x = As,

(3)

ˆs = arg mins

from x involves L1 minimization,

T(cid:88)
i=1
which ﬁnds the sparsest signal, as measured by smallest L1 norm, consistent with the measurement
constraints. Much of the seminal work in CS [9, 10, 11] has focused on sufﬁcient conditions on A
such that (3) is guaranteed to perfectly recover the true signal, so that ˆs = s0 . However, many large
random measurement matrices A which violate sufﬁcient conditions proven in the literature still
nevertheless typically yield perfect signal recovery. Alternate work [12, 13, 14, 15] which analyzes
the asymptotic performance of large random measurement matrices in which each matrix element
is drawn i.i.d. from a gaussian distribution, has revealed a phase transition in performance as a
function the signal sparsity f and the degree of subsampling α = N/T . In the α-f plane, there is
a critical phase boundary αc (f ) such that if α > αc (f ) then CS will typically yield perfect signal
reconstruction, whereas if α < αc (f ), CS will yield errors.
Motivated by the above work in CS, we propose here that a neural network, or more generally any
dynamical system as in (1), could in principle perform compressed sensing of its past inputs, and that
a long but sparse signal history s0 could potentially be recovered from the instantaneous network
state x. We quantify the memory capabilities of a neural network for sparse signals, by assessing
our ability to reconstruct the past signal using L1 minimization. Given a network state x arising
from a signal history s0 through (1), we can obtain an estimate ˆs of the past using (3), where the
measurement matrix A is given by (2). We then deﬁne a memory curve
E (k) = (cid:104)(ˆsk − s0
k )2 (cid:105)s0 ,

(4)

namely the average reconstruction error of a signal k timesteps in the past averaged over the statistics
of s0 . The rise of this error as k increases captures the decay of memory traces in (1). The central
goal of this paper is to obtain a deeper understanding of the memory properties of neural networks
for sparse signals by studying the memory curve E (k) and especially its dependence on W. In
particular, we are interested in classes of network connectivities W and input statistics for which
E (k) can remain small even for k (cid:29) N . Such networks can essentially perform compressed sensing
of their past inputs.
From the perspective of CS, measurement matrices A of the form in (2), henceforth referred to as
dynamical CS matrices, possess several new features not considered in the existing CS literature,
features which could pose severe challenges for a recurrent network W to achieve good CS per-
formance. First, A is an N by ∞ matrix, and so from the perspective of the phase diagram for
CS reviewed above, it is likely that A is in the error phase; thus perfect reconstruction of the true
signal, even for recent inputs will not be possible. Second, because we demand stable dynamics
in (1), the columns of A decay as k increases: ||Wk v||2 < ρk where again ρ < 1 is the squared
magnitude of the largest eigenvalue of W. Such decay can compound errors. Third, the different
columns of A can be correlated; if one thinks of Wk v as the state of the network k timesteps after a
single unit input pulse, it is clear that temporal correlations in the evolving network response to this
pulse are equivalent to correlations in the columns of A in (2). Such correlations could potentially
adversely affect the performance of CS based on A, as well as complicate the theoretical analysis of
CS performance. Nevertheless, despite all these seeming difﬁculties, in the following we show that
a special class of network connectivities can indeed achieve good CS performance in which errors
are controlled and memory traces can last longer than the number of neurons.

3 Memory in an Annealed Approximation to a Dynamical System

In this section, we work towards an analytic understanding of the memory curve E (k) deﬁned in
(4). This curve depends on W, v and the statistics of s0 . We would like to understand its prop-
erties for ensembles of large random networks W, just as the asymptotic performance of CS was
analyzed for large random measurement matrices A [12, 13, 14, 15]. However, in the dynamical
setting, even if W is drawn from a simple random matrix ensemble, A in (2) will have correlations
across its columns, making an analytical treatment of the memory curve difﬁcult. Here we consider
an ensemble of measurement matrices A which approximate dynamical CS matrices and can be

3

|si |,

(5)

E (s) =

uT AT Au +

treated analytically. We consider matrices in which each element Aµk is drawn i.i.d from a zero
mean gaussian distribution with variance ρk . Since we are interested in memory that lasts O(N )
timesteps, we choose ρ = e−1/τ N , with τ O(1). This so called annealed approximation (AA) to a
dynamical CS matrix captures two of the salient properties of dynamical CS matrices, their inﬁnite
temporal extent and the decay of successive columns, but neglects the analytically intractable corre-
lations across columns. Such annealed CS matrices can be thought of as arising from “imaginary”
dynamical systems in which network activity patterns over time in response to a pulse decay, but
are somehow temporally uncorrelated. τ can be thought of as the effective integration time of this
dynamical system, in units of the number of neurons. Finally, to fully specify E (k), we must choose
the statistics of s0 . We assume s0 has a probability f of being nonzero at any given time, and if
nonzero, this nonzero value is drawn from a distribution P (s) which for now we take to be arbitrary.
T(cid:88)
To theoretically compute the memory curve E (k), we deﬁne an energy function
λ
2
i=1
where u ≡ s − s0 is the residual, and we consider the Gibbs distribution PG (s) = 1
Z e−βE (s) .
We will later take λ → ∞ so that the quadratic part of the energy function enforces the constraint
As = As0 , and then take the low temperature β → ∞ limit so that PG (s) concentrates onto
the global minimum of (3). In this limit, we can extract the memory curve E (k) as the average of
(sk − s0
k )2 over PG and the statistics of s0 . Although PG depends on A, for large N , the properties of
PG , including the memory curve E (k), do not depend on the detailed realization of A, but only on its
statistics. Indeed we can compute all properties of PG for any typical realization of A by averaging
over both A and s0 . This is done using the replica method [16] in our supplementary material.
The replica method has been used recently in several works to analyze CS for the traditional case
of uniform random gaussian measurement matrices [14, 17, 15]. We ﬁnd that the statistics of each
(cid:19)2
(cid:18)
(cid:113)
component sk in PG (s), conditioned on the true value s0
k is well described by a mean ﬁeld effective
Hamiltonian
s − s0
k − z
βλ
2(1 + βλ∆Q)
(cid:90)
where z is a random variable with a standard normal distribution. Thus the mean ﬁeld approximation
to the marginal distribution of a reconstruction component sk is
exp(−H M F
Dz
1
P M F
(sk = s) =
k
k
ZM F
k
where Dz = dz e− 1
2 z2 is a Gaussian measure. The order parameters Q0 and ∆Q ≡ Q1 − Q0 obey
∞(cid:88)
ρk (cid:104)(cid:104) (cid:104)u(cid:105)2
(cid:105)(cid:105)
1
∞(cid:88)
HM F
N
k
k=0
ρk (cid:104)(cid:104) (cid:104)δu2 (cid:105)HM F
1
N
z
k
k=0
Here (cid:104)u(cid:105)HM F
and (cid:104)δu2 (cid:105)HM F
are the mean and variance of the residual uk = sk − s0
k with re-
(cid:80)∞
spect to a Gibbs distribution with Hamiltonian given by (6), and the double angular average (cid:104)(cid:104) · (cid:105)(cid:105)z
k
k
(cid:80)∞
refers to integrating over the Gaussian distribution of z . Q1 and Q0 have simple interpretations
k=1 ρk (cid:104)u2
k (cid:105)PG
and
in terms of the original Gibbs distribution PG deﬁned above: Q1 = 1
N
k=1 ρk (cid:104)uk (cid:105)2
, for typical realizations of A. Thus the order parameter equations (8)-(9)
Q0 = 1
PG
N
can be understood as self-consistency conditions for the deﬁnition of Q0 and ∆Q in the mean ﬁeld
approximation to PG . In this approximation, the complicated constraints coupling sk for various k
are replaced with a random gaussian force z in (6) which tends to prevent the marginal sk from as-
suming the true value s0
k . This force is what remains of the measurement constraints after averaging
over A, and its statistics are in turn a function of Q0 and Q1 , as determined by the replica method.
Now to compute the memory curve E (k), we must take the limits λ, β , N → ∞ and complete the
k . The λ → ∞ limit can be taken immediately in (6) and λ disappears from the
average over s0
problem. Now as β → ∞, self consistent solutions to (8) and (9) can be found when Q0 ≡ q0 and

+ β |s|,

H M F
k

(s) = ρk

(8)

(9)

z

(cid:105)(cid:105)

.

(6)

(7)

Q0 =

∆Q =

Q0/ρk

(s)),

4

(10)

(11)

(12)

(13)

=

1
β

H M F
k

(s) = β

where

η(x, σ) = arg mins

∆Q ≡ ∆q/β , where q0 and ∆q are O(1). This limit is similar to that taken in a replica analysis of
(cid:20)
(cid:21)
(cid:16)
(cid:17)2
(cid:112)
CS for random gaussian matrices in the error regime [15]. Taking this limit, (6) becomes
s − s0
k − z
+ |s|
1
ρ−k q0
.
2ρ−k∆q
Since the entire Hamiltonian is proportional to β , in the large β limit, the statistics of sk are domi-
(cid:112)
= η(cid:0)s0
ρ−k q0 , ρ−k∆q(cid:1),
nated by the global minimum of (10). In particular, we have
(cid:104)s(cid:105)HM F
(cid:19)
(cid:18) 1
k + z
k
(s − x)2
= sgn(x)(|x| − σ)+ ,
+ |s|
2
σ
is a soft thresholding function which also arises in message passing approaches [18] to solving the
CS problem in (3), and (y)+ = y if y > 0 and is otherwise 0. The optimization in (12) can be
understood intuitively as follows: suppose one measures a scalar value x which is a true signal
s0 corrupted by additive gaussian noise with variance σ . Under a Laplace prior e−|s0 | on the true
signal, η(x, σ) is simply the MAP estimate of s0 given the data x, which basically chooses the
estimate s = 0 unless the data exceeds the noise level σ . Thus we see that in (10), ρ−k∆q plays the
(cid:112)
χ(cid:0)s0
ρ−k q0 , ρ−k∆q(cid:1),
role of an effective noise level which increases with time k . Also, the variance of s at large β is
(cid:104)(δs)2 (cid:105)HM F
k + z
k
where
χ(x, σ) = σ Θ(|x| − σ),
(14)
and Θ(x) is a step function at 0. Inserting (11) and (13) and the ansatz ∆Q ≡ ∆q/β into (8) and
(9) then removes β from the problem. But before making these substitutions, we ﬁrst take N → ∞
k=0 → (cid:82) ∞
(cid:80)∞
at ﬁxed τ and f of O(1) by taking a continuum approximation for time, t = k/N , ρk → e−t/τ ,
(cid:90) ∞
0 dt. Moreover, we average over the true signal history s0
k , so that (8) and (9)
1
(cid:112)
et/τ q0 , et/τ ∆q) − s0 (cid:1)2 (cid:11)(cid:11)
dt e−t/τ (cid:10)(cid:10) (cid:0)η(s0 + z
N
become,
(cid:90) ∞
(cid:112)
et/τ q0 , et/τ ∆q) (cid:11)(cid:11)
dt e−t/τ (cid:10)(cid:10) χ(s0 + z
(15)
q0 =
z ,s0
0
(16)
z ,s0 ,
∆q =
distribution of s0 , i.e. (cid:10)(cid:10) F (z , s0 ) (cid:11)(cid:11)
z ,s0 ≡ (1 − f ) (cid:82) Dz F (z , 0) + f (cid:82) Dz ds0 P (s0 )F (z , s0 ).
0
where the double angular average reﬂects an integral over the gaussian distribution of z and the full
Finally the memory curve E (t) is simply the continuum limit of the averaged squared residual
(cid:105)(cid:105)z ,s0 , and is given by
(cid:104)(cid:104) (cid:104)u(cid:105)2
(cid:112)
et/τ q0 , et/τ ∆q) − s0 (cid:1)2 (cid:11)(cid:11)
E (t) = (cid:10)(cid:10) (cid:0)η(s0 + z
HM F
k
Equations (15),(16), and (17) now depend only on τ , f and P (s0 ), and their theoretical predictions
can now be compared with numerical experiments. In this work we focus on a simple class of plus-
minus (PM) signals in which P (s0 ) = 1/2 δ(s0 − 1) + 1/2 δ(s0 + 1). Fig. 1A shows an example
of a PM signal s0 with f = 0.01, while Fig. 1B shows an example of a reconstruction of ˆs using
L1 minimization in (3) where the data x used in (3) was obtained from s0 using a random annealed
measurement matrix with τ = 1. Clearly there are errors in the reconstruction, but remarkably,
despite the decay in the columns of A, the reconstruction is well correlated with the true signal for
a time up to 4 times the number of measurements. We can derive theoretical memory curves for any
given f and τ by numerically solving for q0 and ∆q in (15),(16), and inserting the results into (17).
Examples of the agreement between theory and simulations are shown in Fig. 1C-E.
As t → ∞, L1 minimization always yields a zero signal estimate, so the memory curve asymptoti-
cally approaches f for large t. A convenient measure of memory capacity is the time T1/2 at which
the memory curve reaches half its asymptotic error value, i.e. E (T1/2 ) = f /2. A principle feature

z ,s0 .

(17)

5

Figure 1: Memory in the annealed approximation. (A) A PM signal s0 with f = 0.01 that lasts
T = 10N timesteps where N = 500. (B) A reconstruction of s0 from the output of an annealed
measurement matrix with N = 500, τ = 1. (C,D,E) Example memory curves for f = 0.01, and
τ = 1 (C), 2 (D), 3 (E). (F) T1/2 as a function of τ . The 4 curves from top to bottom are for
f = 0.01, 0.02, 0.03, 0.04. (G) T1/2 optimized over τ for each f . (H) The initial error as a function
of f . The 3 curves from bottom to top are for τ = 1, 2, 3. For (C-H), red curves are theoretical
predictions while blue curves and points are from numerical simulations of L1 minimization with
N = 100 averaged over 300 trials. The width of the blue curves reﬂects standard error.

of this family of memory curves is that for any given f there is an optimal τ which maximizes T1/2
(Fig. 1F) . The presence of this optimum arises due to a competition between decay and interference.
If τ is too small, signal measurements decay too quickly, thereby preventing large memory capacity.
However, if τ is too large, signals from the distant past do not decay away, thereby interfering with
the measurements of more recent signals, and again degrading memory. As f decreases, long time
signal interference is reduced, thereby allowing larger values of τ to be chosen without degrading
memory for more recent signals. For any given f , we can compute T1/2 (f ) optimized over τ (Fig.
1G). This memory capacity, again measured in units of the number of neurons, already exceeds 1 at
modest values of f = 0.1, and diverges as f → 0, as does the optimal value of τ . By analyzing (15)
and (16) in the limit f → 0 and τ → ∞, we ﬁnd that ∆q is O(1) while q0 → 0. Furthermore, as
f → 0, the optimal T1/2 is O(
f log 1/f ).
1
The smallest error occurs at t = 0 and it is natural to ask how this error E (0) behaves as a function
of f for small f to see how well the most recent input can be reconstructed in the limit of sparse
signals. We analyze (15) and (16) in the limit f → 0 and τ of O(1), and ﬁnd that E (0) is O(f 2 ) as
conﬁrmed in Fig. 1F. Furthermore, E (0) monotonically increases with τ for ﬁxed f as more signals
from the past interfere with the most recent input.

4 Orthogonal Dynamical Systems

We have seen in the previous section that annealed CS matrices have remarkable memory properties,
but our main interest was to exhibit a dynamical CS matrix as in (2) capable of good compressed
√
sensing, and therefore short-term memory, performance. Here we show that a special class of net-
ρO where O is any orthogonal matrix, and v is any random unit
work connectivity in which W =
norm vector possesses memory properties remarkably close to that of the annealed matrix ensemble.
Fig. 2A-F presents results identical to that of Fig. 1C-H except for the crucial change that all simu-
lation results in Fig. 2 were obtained using dynamical CS matrices of the form Aµk = (ρk/2Ok v)µ ,
rather than annealed CS matrices. All red curves in Fig. 2A-F are identical to those in Fig. 1 and
reﬂect the theory of annealed CS matrices derived in the previous section.
For small τ , we see small discrepancies between memory curves for orthogonal neural networks
and the annealed theory (Fig. 2A-B), but as τ increases, this discrepancy decreases (Fig. 2C).
In particular, from the perspective of the optimal T1/2 for which larger τ is relevant, we see a
remarkable match between the optimal memory capacity of orthogonal neural networks and that
predicted by the annealed theory (see Fig. 2E). And there is good match in the initial error even at
small τ (Fig. 2F).

6

0246810−101ts0A0246810−101tEstimateBCDEFHG24681000.51tE(t) / f24681000.51tE(t) / f24681000.51tE(t) / f0123450246810τT1/20       0.05    0.1 0246810T1/2f00.0250.0500.51fE(0) / fFigure 2: Memory in orthogonal neuronal networks. Panels (A-F) are identical to panels (C-H) in
Fig. 1 except now the blue curves and points are obtained from simulations of L1 minimization using
measurement matrices derived from an orthogonal neuronal network. (G) The mean and standard
deviation of σf for 5 annealed (red) and 5 orthogonal matrices (blue) with N=200 and T=3000.

The key difference between the annealed and the dynamical CS matrices is that the former neglects
correlations across columns that can arise in the latter. How strong are these correlations for the
case of orthogonal matrices? Motivated by the restricted isometry property [11], we consider the
following probe of the strength of correlations across columns of A. Consider an N by f T matrix
B obtained by randomly subsampling the columns of an N by T measurement matrix A. Let σf
be the maximal eigenvalue of the matrix BT B of inner products of columns of B. σf is a measure
of the strength of correlations across the f T sampled columns of A. We can estimate the mean
and standard deviation of σf due to the random choice of f T columns of A and plot the results
as function of f . To separate the issue of correlations from decay, we do this analysis for ρ = 1
and ﬁnite T (similar results are obtained for large T and ρ < 1). Results are shown in Fig 2 for 5
instances of annealed (red) and dynamical (blue) CS matrices. We see strikingly different behavior
in the two ensembles. Correlations are much stronger in the dynamical ensemble, and ﬂuctuate from
instance to instance, while they are weaker in the annealed ensemble, and do not ﬂuctuate (the 5 red
curves are on top of each other). Given the very different statistical properties of the two ensembles,
the level of agreement between the simulated memory properties of orthogonal neural networks, and
the theory of annealed CS matrices is remarkable.
Why do orthogonal neural networks perform so well, and can more generic networks have similar
√
performance? The key to understanding the memory, and CS, capabilities of orthogonal neural
√
networks lies in the eigenvalue spectrum of an orthogonal matrix. The eigenvalues of W =
ρO,
when O is a large random orthogonal matrix, are uniformly distributed on a circle of radius
ρ
in the complex plane. Thus when ρ = e−1/τ N , the sequence of vectors Wk v explore the full
N dimensional space of network activity patterns for O(τ N ) time steps before decaying away. In
√
contrast, a generic random Gaussian matrix W with elements drawn i.i.d from a zero mean gaussian
ρ in the complex
with variance ρ/N has eigenvalues uniformly distributed on a solid disk of radius
plane. Thus the sequence of vectors Wk v no longer explore a high dimensional space of activity
patterns; components of v in the direction of eigenmodes of W with small eigenvalues will rapidly
decay away, and so the sequence will rapidly become conﬁned to a low dimensional space. Good
compressed sensing matrices often have columns that are random and uncorrelated. From the above
considerations, it is clear that dynamical CS matrices derived from orthogonal neural networks can
come close to this ideal, while those derived from generic gaussian networks cannot.

5 Discussion

In this work we have made progress on the theory of short-term memory for nongaussian, sparse,
temporal sequences stored in the transient dynamics of neuronal networks. We used the framework
of compressed sensing, speciﬁcally L1 minimization, to reconstruct the history of the past input sig-
nal from the current network activity state. The reconstruction error as a function of time into the past
then yields a well-deﬁned memory curve that reﬂects the memory capabilities of the network. We
studied the properties of this memory curve and its dependence on network connectivity, and found

7

ABCDFE24681000.51tE(t) / f24681000.51tE(t) / f24681000.51tE(t) / f0123450246810τT1/20       0.05    0.1 0246810T1/2f00.0250.0500.51fE(0) / f00.050.102468101214fMax CorrGresults that were qualitatively different from prior theoretical studies devoted to short-term memory
in the setting of gaussian input statistics. In particular we found that orthogonal neural networks,
but importantly, not generic random gaussian networks, are capable of remembering inputs for a
time that exceeds the number of neurons in the network, thereby circumventing a theorem proven in
[4], which limits the memory capacity of any network to be less than the number of neurons in the
gaussian signal setting. Also, recurrent connectivity plays an essential role in allowing a network to
have a memory capacity that exceeds the number of neurons. Thus purely feedforward networks,
which always outperform recurrent networks (for times less than the network size) in the scenario of
gaussian signals and noise [6] are no longer optimal for sparse input statistics. Finally, we exploited
powerful tools from statistical mechanics to analytically compute memory curves as a function of
signal sparsity and network integration time. Our theoretically computed curves matched reasonably
well simulations of orthogonal neural networks. To our knowledge, these results represent the ﬁrst
theoretical calculations of short-term memory curves for sparse signals in neuronal networks.
We emphasize that we are not suggesting that biological neural systems use L1 minimization to
reconstruct past inputs. Instead we use L1 minimization in this work simply as a theoretical tool to
probe the memory capabilities of neural networks. However, neural implementations of L1 mini-
mization exist [19, 20], so if stimulus reconstruction were the goal of a neural system, reconstruction
performance similar to what is reported here could be obtained in a neurally plausible manner. Also,
we found that orthogonal neural networks, because of their eigenvalue spectrum, display remark-
able memory properties, similar to that of an annealed approximation. Such special connectivity
is essential for memory performance, as random gaussian networks cannot have memory similar
to the annealed approximation. Orthogonal connectivity could be implemented in a biologically
plausible manner using antisymmetric networks with inhibition operating in continuous time. When
exponentiated, such connectivities yield the orthogonal networks considered here in discrete time.
Our results are relevant not only to the ﬁeld of short-term memory, but also to the ﬁeld of compressed
sensing (CS). We have introduced two new ensembles of random CS measurement matrices. The
ﬁrst of these, dynamical CS matrices, are the effective measurements a dynamical system makes on
a continuous temporal stream of input. Dynamical CS matrices have three properties not considered
in the existing CS literature: they are inﬁnite in temporal extent, have columns that decay over time
and exhibit correlations between columns. We also introduce annealed CS matrices, that are also
inﬁnite in extent and have decaying columns, but no correlations across columns. We show how to
analytically calculate the time course of reconstruction error in the annealed ensemble and compare
it to the dynamical ensemble for orthogonal dynamical systems. Our results show that orthogonal
dynamical systems can perform CS even while operating with errors.
This work suggests several extensions. Given the importance of signal statistics in determining
memory capacity, it would be interesting to study memory for sparse nonnegative signals. The
inequality constraints on the space of allowed signals arising from nonnegativity can have important
effects in CS; they shift the phase boundary between perfect and error-prone reconstruction [12, 13,
15], and they allow the existence of a new phase in which signal reconstruction is possible even
without L1 minimization [15]. We have found, through simulations, dramatic improvements in
memory capacity in this case, and are extending the theory to explain these effects. Also, we have
used a simple model for sparseness, in which a fraction of signal elements are nonzero. But our
theory is general for any signal distribution, and could be used to analyze other models of sparsity,
i.e. signals drawn from Lp priors. Also, we have worked in the high SNR limit. However our
theory can be extended to analyze memory in the presence of noise by working at ﬁnite λ. But most
importantly, a deeper understanding of the relationship between dynamical CS matrices and their
annealed counterparts would desirable. The effects of temporal correlations in the network activity
patterns of orthogonal dynamical systems is central to this problem. For example, we have seen that
these temporal correlations introduce strong correlations between the columns of the corresponding
dynamical CS matrix (Fig. 2G), yet the memory properties of these matrices agree well with our
annealed theory (Fig. 2E-F), which neglects these correlations. We leave this observation as an
intriguing puzzle for the ﬁelds of short-term memory, dynamical systems, and compressed sensing.

Acknowledgments

S. G. and H. S. thank the Swartz Foundation, Burroughs Wellcome Fund, and the Israeli Science
Foundation for support, and Daniel Lee for useful discussions.

8

References
[1] J.J. Hopﬁeld. Neural networks and physical systems with emergent collective computational
abilities. PNAS, 79(8):2554, 1982.
[2] W. Maass, T. Natschlager, and H. Markram. Real-time computing without stable states: A new
framework for neural computation based on perturbations. Neural computation, 14(11):2531–
2560, 2002.
[3] H. Jaeger and H. Haas. Harnessing nonlinearity: Predicting chaotic systems and saving energy
in wireless communication. Science, 304(5667):78, 2004.
[4] H. Jaeger. Short term memory in echo state networks. GMD Report 152 German National
Research Center for Information Technology, 2001.
[5] O.L. White, D.D. Lee, and H. Sompolinsky. Short-term memory in orthogonal neural net-
works. Phys. Rev. Lett., 92(14):148102, 2004.
[6] S. Ganguli, D. Huh, and H. Sompolinsky. Memory traces in dynamical systems. Proc. Natl.
Acad. Sci., 105(48):18970, 2008.
[7] A.M. Bruckstein, D.L. Donoho, and M. Elad. From sparse solutions of systems of equations
to sparse modeling of signals and images. Siam Review, 51(1):34–81, 2009.
[8] E. Candes and M. Wakin. An introduction to compressive sampling. IEEE Sig. Proc. Mag.,
25(2):21–30, 2008.
[9] D.L. Donoho and M. Elad. Optimally sparse representation in general (non-orthogonal) dic-
tionaries via l1 minimization. PNAS, 100:2197–2202, 2003.
[10] E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction
from highly incomplete frequency information. IEEE Trans. Inf. Theory, 52(2):489–509, 2006.
[11] E. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theory, 51:4203–
4215, 2005.
[12] D.L. Donoho and J. Tanner. Sparse nonnegative solution of underdetermined linear equations
by linear programming. PNAS, 102:9446–51, 2005.
[13] D.L. Donoho and J. Tanner. Neighborliness of randomly projected simplices in high dimen-
sions. PNAS, 102:9452–7, 2005.
[14] Y. Kabashima, T. Wadayama, and T. Tanaka. A typical reconstruction limit for compressed
sensing based on l p-norm minimization. J. Stat. Mech., page L09003, 2009.
[15] S. Ganguli and H. Sompolinsky. Statistical mechanics of compressed sensing. Phys. Rev. Lett.,
104(18):188701, 2010.
[16] M. Mezard, G. Parisi, and M.A. Virasoro. Spin glass theory and beyond. World scientiﬁc
Singapore, 1987.
[17] S. Rangan, A.K. Fletcher, and Goyal V.K. Asymptotic analysis of map estimation via the
replica method and applications to compressed sensing. CoRR, abs/0906.3234, 2009.
[18] D.L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed
sensing. Proc. Natl. Acad. Sci., 106(45):18914, 2009.
[19] Y. Xia and M.S. Kamel. A cooperative recurrent neural network for solving l 1 estimation
problems with general linear constraints. Neural computation, 20(3):844–872, 2008.
[20] C.J. Rozell, D.H. Johnson, R.G. Baraniuk, and B.A. Olshausen. Sparse coding via thresholding
and local competition in neural circuits. Neural computation, 20(10):2526–2563, 2008.

9


Random Projections for k -means Clustering

Christos Boutsidis
Department of Computer Science
RPI

Anastasios Zouzias
Department of Computer Science
University of Toronto

Petros Drineas
Department of Computer Science
RPI

Abstract

This paper discusses the topic of dimensionality reduction for k -means clustering. We prove that
any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 )
dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability
the optimal k -partition of the point set is preserved within a factor of 2 + ε. The projection is done
by post-multiplying A with a d × t random matrix R having entries +1/√t or −1/√t with equal
probability. A numerical implementation of our technique and experiments on a large face images
dataset verify the speed and the accuracy of our theoretical results.

1 Introduction

The k -means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last ﬁfty
years [20]. In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became
popular and found applications in both theoretical computer science [2] and data analytics [4]. This paper focuses on
the application of the random projection method (see Section 2.3) to the k -means clustering problem (see De ﬁnition
1). Formally, assuming as input a set of n points in d dimensions, our goal is to randomly project the points into ˜d
dimensions, with ˜d ≪ d, and then apply a k -means clustering algorithm (see De ﬁnition 2) on the projec ted points. Of
course, one should be able to compute the projection fast without distorting signiﬁcantly the “clusters” of the origina
l
point set. Our algorithm (see Algorithm 1) satisﬁes both con ditions by computing the embedding in time linear in the
size of the input and by distorting the “clusters” of the data
set by a factor of at most 2 + ε, for some ε ∈ (0, 1/3) (see
Theorem 1). We believe that the high dimensionality of modern data will render our algorithm useful and attractive in
many practical applications [9].

Dimensionality reduction encompasses the union of two different approaches: feature selection, which embeds the
points into a low-dimensional space by selecting actual dimensions of the data, and feature extraction, which ﬁnds an
embedding by constructing new artiﬁcial features that are,
for example, linear combinations of the original features.
Let A be an n × d matrix containing n d-dimensional points (A(i) denotes the i-th point of the set), and let k be
the number of clusters (see also Section 2.2 for more notation). We slightly abuse notation by also denoting by A
˜d with f (A(i) ) = ˜A(i) for all
the n-point set formed by the rows of A. We say that an embedding f : A → R
i ∈ [n] and some ˜d < d, preserves the clustering structure of A within a factor φ, for some φ ≥ 1, if ﬁnding an
optimal clustering in ˜A and plugging it back to A is only a factor of φ worse than ﬁnding the optimal clustering
directly in A. Clustering optimality and approximability are formally presented in De ﬁnitions 1 and 2, respectively.
Prior efforts on designing provably accurate dimensionality reduction methods for k -means clustering include: (i) the
Singular Value Decomposition (SVD), where one ﬁnds an embed ding with image ˜A = UkΣk ∈ Rn×k such that the
clustering structure is preserved within a factor of two; (ii) random projections, where one projects the input points into
t = Ω(log(n)/ε2 ) dimensions such that with constant probability the clustering structure is preserved within a factor
of 1 + ε (see Section 2.3); (iii) SVD-based feature selection, where one can use the SVD to ﬁnd c = Ω(k log(k/ε)/ε2)
actual features, i.e. an embedding with image ˜A ∈ Rn×c containing (rescaled) columns from A, such that with constant
probability the clustering structure is preserved within a factor of 2 + ε. These results are summarized in Table 1. A
head-to-head comparison of our algorithm with existing results allows us to claim the following improvements: (i)

1

Ref.
Year
[6]
1999
Folklore
-
2009
[5]
2010 This paper

Accuracy
2
1 + ε
2 + ε
2 + ε

Dimensions
Description
SVD - feature extraction
k
RP - feature extraction
Ω(log(n)/ε2 )
SVD - feature selection Ω(k log(k/ε)/ε2)
RP - feature extraction
Ω(k/ε2 )

Time
O(nd min{n, d})
O(nd⌈ε−2 log(n)/ log(d)⌉)
O(nd min{n, d})
O(nd⌈ε−2 k/ log(d)⌉)
Table 1: Dimension reduction methods for k-means. In the RP methods the construction is done with random sign matrices and
the mailman algorithm (see Sections 2.3 and 3.1, respectively).

reduce the running time by a factor of min{n, d}⌈ε2 log(d)/k⌉, while losing only a factor of ε in the approximation
accuracy and a factor of 1/ε2 in the dimension of the embedding; (ii) reduce the dimension of the embedding and
the running time by a factor of log(n)/k while losing a factor of one in the approximation accuracy; (iii) reduce the
dimension of the embedding by a factor of log(k/ε) and the running time by a factor of min{n, d}⌈ε2 log(d)/k⌉,
respectively. Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher
scores [7], are very popular in applications (see also surveys on the topic [8, 13]). However, they lack a theoretical
worst case analysis of the form we describe in this work.

2 Preliminaries

We start by formally de ﬁning the k -means clustering problem using matrix notation. Later in this section, we precisely
describe the approximability framework adopted in the k -means clustering literature and ﬁx the notation.
De ﬁnition 1.
[THE K -MEAN S CLU S T ER ING PROBL EM ]
Given a set of n points in d dimensions (rows in an n × d matrix A) and a positive integer k denoting the number of
clusters, ﬁnd the n × k indicator matrix Xopt such that
X ∈X (cid:13)(cid:13)A − X X ⊤A(cid:13)(cid:13)
2
F .
Xopt = arg min
Here X denotes the set of all n × k indicator matrices X . The functional F (A, X ) = (cid:13)(cid:13)A − X X ⊤A(cid:13)(cid:13)
2
F is the so-called
k -means objective function. An n × k indicator matrix has exactly one non-zero element per row, which denotes
cluster membership. Equivalently, for all i = 1, . . . , n and j = 1, . . . , k , the i-th point belongs to the j -th cluster if
and only if Xij = 1/√zj , where zj denotes the number of points in the corresponding cluster. Note that X ⊤X = Ik ,
where Ik is the k × k identity matrix.
2.1 Approximation Algorithms for k -means clustering

(1)

Finding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation
algorithms for k -means clustering. The following de ﬁnition captures the fr amework of such efforts.
De ﬁnition 2.
[K -MEAN S A P PROX IMAT ION ALGOR ITHM ]
An algorithm is a “ γ -approximation ” for the k -means clustering problem (γ ≥ 1) if it takes inputs A and k , and
returns an indicator matrix Xγ that satisﬁes with probability at least 1 − δγ ,
X ∈X (cid:13)(cid:13)A − X X ⊤A(cid:13)(cid:13)
γ A(cid:13)(cid:13)
(cid:13)(cid:13)A − Xγ X ⊤
2
2
F .
F ≤ γ min
In the above, δγ ∈ [0, 1) is the failure probability of the γ -approximation k -means algorithm.
For our discussion, we ﬁx the γ -approximation algorithm to be the one presented in [14], which guarantees γ = 1 + ε′
for any ε′ ∈ (0, 1] with running time O(2(k/ε′ )O(1)
dn).
2.2 Notation

(2)

Given an n × d matrix A and an integer k with k < min{n, d}, let Uk ∈ Rn×k (resp. Vk ∈ Rd×k ) be the matrix
of the top k left (resp. right) singular vectors of A, and let Σk ∈ Rk×k be a diagonal matrix containing the top

2

k singular values of A in non-increasing order. If we let ρ be the rank of A, then Aρ−k is equal to A − Ak , with
k . By A(i) we denote the i-th row of A. For an index i taking values in the set {1, . . . , n} we write
Ak = UkΣkV ⊤
i ∈ [n]. We denote, in non-increasing order, the non-negative singular values of A by σi (A) with i ∈ [ρ]. kAkF and
kAk2 denote the Frobenius and the spectral norm of a matrix A, respectively. A† denotes the pseudo-inverse of A, i.e.
the unique d × n matrix satisfying A = AA†A, A†AA† = A† , (AA† )⊤ = AA† , and (A†A)⊤ = A†A. Note also
that (cid:13)(cid:13)A†(cid:13)(cid:13)2 = σ1 (A† ) = 1/σρ (A) and kAk2 = σ1 (A) = 1/σρ (A† ). A useful property of matrix norms is that for
any two matrices C and T of appropriate dimensions, kC T kF ≤ kC kF kT k2 ; this is a stronger version of the standard
submultiplicavity property. We call P a projector matrix if it is square and P 2 = P . We use E [Y ] and Var [Y ] to take
the expectation and the variance of a random variable Y and P (e) to take the probability of an event e. We abbreviate
“independent identically distributed ” to “i.i.d.” and “wi
th probability ” to “w.p.”. Finally, all logarithms are base
two.

2.3 Random Projections

A classical result of Johnson and Lindenstrauss states that any n-point set in d dimensions - rows in a matrix A ∈ Rn×d
- can be linearly projected into t = Ω(log(n)/ε2 ) dimensions while preserving pairwise distances within a factor of
1 ± ε using a random orthonormal matrix [12]. Subsequent research simpliﬁed the proof of the above result by showing
that such a projection can be generated using a d × t random Gaussian matrix R, i.e., a matrix whose entries are i.i.d.
Gaussian random variables with zero mean and variance 1/√t [11]. More precisely, the following inequality holds
with high probability over the randomness of R,
(1 − ε) (cid:13)(cid:13)A(i) − A(j) (cid:13)(cid:13)2 ≤ (cid:13)(cid:13)A(i)R − A(j)R(cid:13)(cid:13)2 ≤ (1 + ε) (cid:13)(cid:13)A(i) − A(j) (cid:13)(cid:13)2 .
Notice that such an embedding ˜A = AR preserves the metric structure of the point-set, so it also preserves, within a
factor of 1 + ε, the optimal value of the k -means objective function of A. Achlioptas proved that even a (rescaled)
random sign matrix sufﬁces in order to get the same guarantee s as above [1], an approach that we adopt here (see step
two in Algorithm 1). Moreover, in this paper we will heavily exploit the structure of such a random matrix, and obtain,
as an added bonus, savings on the computation of the projection.

(3)

3 A random-projection-type k-means algorithm
Algorithm 1 takes as inputs the matrix A ∈ Rn×d , the number of clusters k , an error parameter ε ∈ (0, 1/3), and some
γ -approximation k -means algorithm. It returns an indicator matrix X˜γ determining a k -partition of the rows of A.

Input: n × d matrix A (n points, d features), number of clusters k , error parameter ε ∈ (0, 1/3), and
γ -approximation k -means algorithm.
Output: Indicator matrix X˜γ determining a k -partition on the rows of A.
1. Set t = Ω(k/ε2 ), i.e. set t = to ≥ ck/ε2 for a sufﬁciently large constant c.
2. Compute a random d × t matrix R as follows. For all i ∈ [d], j ∈ [t]
Rij = (cid:26)+1/√t, w.p. 1/2,
−1/√t, w.p. 1/2.
3. Compute the product ˜A = AR.
4. Run the γ -approximation algorithm on ˜A to obtain X˜γ ; Return the indicator matrix X˜γ

Algorithm 1: A random projection algorithm for k -means clustering.

3.1 Running time analysis

Algorithm 1 reduces the dimensions of A by post-multiplying it with a random sign matrix R. Interestingly, any
“random projection matrix ” R that respects the properties of Lemma 2 with t = Ω(k/ε2 ) can be used in this step. If R
is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and

3

compute the product AR in O(nd⌈ε−2 k/ log(d)⌉) time. Indeed, the mailman algorithm computes (after preprocessing
1 ) a matrix-vector product of any d-dimensional vector (row of A) with an d × log(d) sign matrix in O(d) time.
By partitioning the columns of our d × t matrix R into ⌈t/ log(d)⌉ blocks, the claim follows. Notice that when
k = O(log(d)), then we get an - almost - linear time complexity O(nd/ε2 ). The latter assumption is reasonable in our
setting since the need for dimension reduction in k -means clustering arises usually in high-dimensional data (large d).
Other choices of R would give the same approximation results; the time complexity to compute the embedding would
be different though. A matrix where each entry is a random Gaussian variable with zero mean and variance 1/√t
would imply an O(knd/ε2 ) time complexity (naive multiplication). In our experiments in Section 5 we experiment
with the matrix R described in Algorithm 1 and employ MatLab’s matrix-matrix BLAS implementation to proceed
in the third step of the algorithm. We also experimented with a novel MatLab/C implementation of the mailman
algorithm but, in the general case, we were not able to outperform MatLab’s built-in routines (see section 5.2).

Finally, note that any γ -approximation algorithm may be used in the last step of Algorithm 1. Using, for example,
the algorithm of [14] with γ = 1 + ε would result in an algorithm that preserves the clustering within a factor of
2 + ε, for any ε ∈ (0, 1/3), running in time O(nd⌈ε−2 k/ log(d)⌉ + 2(k/ε)O(1)
kn/ε2 ). In practice though, the Lloyd
algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically
does well. We thus employ the Lloyd algorithm for our experimental evaluation of our algorithm in Section 5. Note
that, after using the proposed dimensionality reduction method, the cost of the Lloyd heuristic is only O(nk2 /ε2) per
iteration. This should be compared to the cost of O(knd) per iteration if applied on the original high dimensional data.

4 Main Theorem

if the k -means
Theorem 1 is our main quality-of-approximation result for Algorithm 1. Notice that if γ = 1, i.e.
problem with inputs ˜A and k is solved exactly, Algorithm 1 guarantees a distortion of at most 2 + ε, as advertised.
Theorem 1. Let the n × d matrix A and the positive integer k < min{n, d} be the inputs of the k -means clustering
problem. Let ε ∈ (0, 1/3) and assume access to a γ -approximation k -means algorithm. Run Algorithm 1 with inputs
A, k , ε, and the γ -approximation algorithm in order to construct an indicator matrix X˜γ . Then with probability at
least 0.97 − δγ ,
optA(cid:13)(cid:13)
F ≤ (1 + (1 + ε)γ ) (cid:13)(cid:13)A − XoptX ⊤
(cid:13)(cid:13)A − X˜γ X ⊤
˜γ A(cid:13)(cid:13)
2
2
F .

(4)

Proof of Theorem 1

The proof of Theorem 1 employs several results from [19] including Lemma 6, 8 and Corollary 11. We summarize
these results in Lemma 2 below. Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make
sure that the matrix R constructed in Algorithm 1 is consistent with De ﬁnition 1 and Lemma 5 in [19]. Theorem 1.1
of [1] immediately shows that the random sign matrix R of Algorithm 1 satisﬁes De ﬁnition
1 and Lemma 5 in [19].
Lemma 2. Assume that the matrix R is constructed by using Algorithm 1 with inputs A, k and ε.
1. Singular Values Preservation: For all i ∈ [k ] and w.p. at least 0.99,
|1 − σi (V ⊤
k R)| ≤ ε.
2. Matrix Multiplication: For any two matrices S ∈ Rn×d and T ∈ Rd×k ,
F i ≤
E h(cid:13)(cid:13)S T − SRR⊤T (cid:13)(cid:13)
2
2
F kT k2
t kS k2
F .
3. Moments: For any C ∈ Rn×d : E hkCRk2
F i = kC k2
F and Var [kCRkF ] ≤ 2 kC k4
F /t.
The ﬁrst statement above assumes
c being sufﬁciently large (see step 1 of Algorithm 1). We continue with several
novel results of general interest.

1Reading the input d × log d sign matrix requires O(d log d) time. However, in our case we only consider multiplication with
a random sign matrix, therefore we can avoid the preprocessing step by directly computing a random correspondence matrix as
discussed in [15, Preprocessing Section].

4

Lemma 3. Under the same assumptions as in Lemma 2 and w.p. at least 0.99,
(cid:13)(cid:13)(cid:13)(V ⊤
k R)⊤(cid:13)(cid:13)(cid:13)2 ≤ 3ε.
†
− (V ⊤
k R)
Proof. Let Φ = V ⊤
k R; note that Φ is a k × t matrix and the S V D of Φ is Φ = UΦΣΦV ⊤
Φ , where UΦ and ΣΦ are k × k
† and (V ⊤
matrices, and VΦ is a t × k matrix. By taking the SVD of (V ⊤
k R)⊤ we get
k R)
(cid:13)(cid:13)(cid:13)(V ⊤
k R)⊤(cid:13)(cid:13)(cid:13)2
= (cid:13)(cid:13)VΦΣ−1
Φ (cid:13)(cid:13)2 = (cid:13)(cid:13)VΦ (Σ−1
Φ − ΣΦ (cid:13)(cid:13)2 ,
Φ (cid:13)(cid:13)2 = (cid:13)(cid:13)Σ−1
†
− (V ⊤
Φ U ⊤
Φ − VΦΣΦU ⊤
Φ − ΣΦ )U ⊤
k R)
Φ can be dropped without changing any unitarily invariant norm. Let Ψ = Σ−1
Φ − ΣΦ ; Ψ is a k × k
since VΦ and U ⊤
diagonal matrix. Assuming that, for all i ∈ [k ], σi (Φ) and τi (Ψ) denote the i-th largest singular value of Φ and the
i-th diagonal element of Ψ, respectively, it is

(5)

Since Ψ is a diagonal matrix,

τi (Ψ) =

1 − σi (Φ)σk+1−i (Φ)
σk+1−i

.

.

1 − σi (Φ)σk+1−i (Φ)
τi (Ψ) = max
σk+1−i (Φ)
1≤i≤k
ε ∈ (0, 1/3) and elementary calculations sufﬁce to conclude the

kΨk2 = max
1≤i≤k
The ﬁrst statement of Lemma 2, our choice of
proof.
Lemma 4. Under the same assumptions as in Lemma 2 and for any n × d matrix C w.p. at least 0.99,
kCRkF ≤ p(1 + ε) kC kF .
Proof. Notice that there exists a sufﬁciently large constant c such that t ≥ ck/ε2 . Then, setting Z = kCRk2
F , using
the third statement of Lemma 2, the fact that k ≥ 1, and Chebyshev’s inequality we get
2 kC k4
Var [Z ]
P (cid:16)|Z − E [Z ] | ≥ ε kC k2
F(cid:17) ≤
2
F
ck ≤ 0.01.
F ≤
F ≤
ε2 kC k4
tε2 kC k4
The last inequality follows assuming c sufﬁciently large. Finally, taking square root on both side s concludes the
proof.

(6)

(7)

.

V ⊤
k + E ,

k . Then, setting A =
V ⊤

Lemma 5. Under the same assumptions as in Lemma 2 and w.p. at least 0.97,
†
Ak = (AR)(V ⊤
k R)
where E is an n × d matrix with kE kF ≤ 4ε kA − Ak kF .
†
†
k is an n × d matrix, let us write E = Ak − (AR)(V ⊤
Proof. Since (AR)(V ⊤
V ⊤
k R)
k R)
Ak + Aρ−k , and using the triangle inequality we get
kE kF ≤ (cid:13)(cid:13)(cid:13)Ak − AkR(V ⊤
+ (cid:13)(cid:13)(cid:13)Aρ−k R(V ⊤
k (cid:13)(cid:13)(cid:13)F
k (cid:13)(cid:13)(cid:13)F
†
†
V ⊤
V ⊤
k R)
k R)
†
k R) = k thus (V ⊤
The ﬁrst statement of Lemma 2 implies that rank (V ⊤
= Ik , where Ik is the k × k identity
k R)(V ⊤
k R)
†
matrix. Replacing Ak = UkΣk V ⊤
= Ik we get that
k and setting (V ⊤
k R)(V ⊤
k R)
(cid:13)(cid:13)(cid:13)Ak − AkR(V ⊤
= (cid:13)(cid:13)(cid:13)Ak − UkΣkV ⊤
k (cid:13)(cid:13)(cid:13)F
k (cid:13)(cid:13)(cid:13)F
= (cid:13)(cid:13)Ak − UkΣkV ⊤
k (cid:13)(cid:13)F = 0.
†
†
V ⊤
V ⊤
k R(V ⊤
k R)
k R)
To bound the second term above, we drop V ⊤
k , add and subtract the matrix Aρ−kR(V ⊤
k , and use the triangle
k R)⊤V ⊤
inequality and submultiplicativity:
(cid:13)(cid:13)(cid:13)Aρ−k R(V ⊤
k (cid:13)(cid:13)(cid:13)F ≤ (cid:13)(cid:13)Aρ−k R(V ⊤
k R)⊤(cid:13)(cid:13)F + (cid:13)(cid:13)(cid:13)Aρ−k R((V ⊤
k R)⊤ )(cid:13)(cid:13)(cid:13)F
†
†
V ⊤
− (V ⊤
k R)
k R)
≤ (cid:13)(cid:13)Aρ−k RR⊤Vk (cid:13)(cid:13)F + kAρ−kRkF (cid:13)(cid:13)(cid:13)(V ⊤
k R)⊤(cid:13)(cid:13)(cid:13)2
†
− (V ⊤
k R)
.
5

Now we will bound each term individually. A crucial observation for bounding the ﬁrst term is that Aρ−k Vk =
ρ−k Vk = 0 by orthogonality of the columns of Vk and Vρ−k . This term now can be bounded using the
Uρ−kΣρ−k V ⊤
second statement of Lemma 2 with S = Aρ−k and T = Vk . This statement, assuming c sufﬁciently large, and an
application of Markov’s inequality on the random variable (cid:13)(cid:13)Aρ−kRR⊤Vk − Aρ−k Vk (cid:13)(cid:13)F give that w.p. at least 0.99,
(cid:13)(cid:13)Aρ−k RR⊤Vk (cid:13)(cid:13)F ≤ 0.5ε kAρ−k kF .
(8)
The second two terms can be bounded using Lemma 3 and Lemma 4 on C = Aρ−k . Hence by applying a union bound
on Lemma 3, Lemma 4 and Inq. (8), we get that w.p. at least 0.97,
kE kF ≤ (cid:13)(cid:13)Aρ−kRR⊤Vk (cid:13)(cid:13)F + kAρ−kRkF (cid:13)(cid:13)(cid:13)(V ⊤
k R)⊤(cid:13)(cid:13)(cid:13)2
†
− (V ⊤
k R)
≤ 0.5ε kAρ−k kF + p(1 + ε) kAρ−k kF · 3ε
≤ 0.5ε kAρ−k kF + 3.5ε kAρ−k kF
= 4ε · kAρ−k kF .
The last inequality holds thanks to our choice of ε ∈ (0, 1/3).
Proposition 6. A well-known property connects the SVD of a matrix and k -means clustering. Recall De ﬁnition 1, and
notice that XoptX ⊤
optA is a matrix of rank at most k . From the SVD optimality we immediately get that
F ≤ (cid:13)(cid:13)A − XoptX ⊤
optA(cid:13)(cid:13)
2
F = kA − Ak k2
kAρ−k k2
F .
4.1 The proof of Eqn. (4) of Theorem 1
We start by manipulating the term (cid:13)(cid:13)A − X˜γ X ⊤
˜γ A(cid:13)(cid:13)
2
in Eqn. (4). Replacing A by Ak + Aρ−k , and using the
F
Pythagorean theorem (the subspaces spanned by the components Ak − X˜γ X ⊤
˜γ Ak and Aρ−k − X˜γ X ⊤
˜γ Aρ−k are
perpendicular) we get
˜γ )Aρ−k (cid:13)(cid:13)
+ (cid:13)(cid:13)(I − X˜γ X ⊤
˜γ )Ak (cid:13)(cid:13)
F = (cid:13)(cid:13)(I − X˜γ X ⊤
(cid:13)(cid:13)A − X˜γ X ⊤
˜γ A(cid:13)(cid:13)
2
2
2
F
F
|
{z
}
{z
}
|
θ2
θ2
1
2
We ﬁrst bound the second term of Eqn. (10). Since
˜γ is a projector matrix, it can be dropped without
I − X˜γ X ⊤
increasing a unitarily invariant norm. Now Proposition 6 implies that
F ≤ (cid:13)(cid:13)A − XoptX ⊤
optA(cid:13)(cid:13)
2
2 ≤ kAρ−k k2
θ2
F .
We now bound the ﬁrst term of Eqn. (10):
θ1 ≤ (cid:13)(cid:13)(cid:13)(I − X˜γ X ⊤
k (cid:13)(cid:13)(cid:13)F
˜γ )AR(Vk R)†V ⊤
+ kE kF
˜γ )AR(cid:13)(cid:13)F (cid:13)(cid:13)(cid:13)(VkR)†(cid:13)(cid:13)(cid:13)2
≤ (cid:13)(cid:13)(I − X˜γ X ⊤
+ kE kF
opt )AR(cid:13)(cid:13)F (cid:13)(cid:13)(cid:13)(VkR)†(cid:13)(cid:13)(cid:13)2
≤ √γ (cid:13)(cid:13)(I − XoptX ⊤
+ kE kF
≤ √γp(1 + ε) (cid:13)(cid:13)(I − XoptX ⊤
1
opt )A(cid:13)(cid:13)F
opt )A(cid:13)(cid:13)F
+ 4ε (cid:13)(cid:13)(I − XoptX ⊤
1 − ε
≤ √γ (1 + 2.5ε) (cid:13)(cid:13)(I − XoptX ⊤
opt )A(cid:13)(cid:13)F + √γ 4ε (cid:13)(cid:13)(I − XoptX ⊤
opt )A(cid:13)(cid:13)F
≤ √γ (1 + 6.5ε) (cid:13)(cid:13)(I − XoptX ⊤
opt )A(cid:13)(cid:13)F
In Eqn. (12) we used Lemma 5, the triangle inequality, and the fact that I − ˜Xγ ˜X ⊤
γ is a projector matrix and can be
dropped without increasing a unitarily invariant norm. In Eqn. (13) we used submultiplicativity (see Section 2.2) and
the fact that V ⊤
k can be dropped without changing the spectral norm. In Eqn. (14) we replaced X˜γ by Xopt and the
factor √γ appeared in the ﬁrst term. To better understand this step, no tice that X˜γ gives a γ -approximation to the
optimal k -means clustering of the matrix AR, and any other n × k indicator matrix (for example, the matrix Xopt )
satisﬁes
opt (cid:1) AR(cid:13)(cid:13)
F ≤ γ (cid:13)(cid:13)(cid:0)I − XoptX ⊤
X ∈X (cid:13)(cid:13)(I − X X ⊤)AR(cid:13)(cid:13)
˜γ (cid:1) AR(cid:13)(cid:13)
(cid:13)(cid:13)(cid:0)I − X˜γ X ⊤
2
2
2
F .
F ≤ γ min
6

(13)

(14)

(11)

(10)

.

(9)

(12)

(15)

(16)

(17)

F vs. t

P vs. t

T vs. t

0.036

0.034

0.032

0.03

0.028

0.026

0.024

0.022

F
 
e
u
l
a
v
 
n
o
i
t
c
n
u
f
 
e
v
i
t
c
e
j
b
o
 
d
e
z
i
l
a
m
r
o
n

0.7

0.65

0.6

0.55

0.5

0.45

0.4

P
 
e
t
a
r
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c
−
s
i
M

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

T
 
s
d
n
o
c
e
s
 
n
i
 
e
r
u
d
e
c
o
r
p
 
s
n
a
e
m
−
k
 
f
o
 
e
m
i
T

0.02

0

50

100
200
150
number of dimensions t

250

300

0.35

0

50

100
200
150
number of dimensions t

250

300

0

0

50

100
200
150
number of dimensions t

250

300

Figure 1: The results of our experiments after running Algorithm 1 with k = 40 on the face images collection.

In Eqn. (15) we used Lemma 4 with C = (I − XoptX ⊤
opt )A, Lemma 3 and Proposition 6. In Eqn. (16) we used the
fact that γ ≥ 1 and that for any ε ∈ (0, 1/3) it is (√1 + ε)/(1 − ε) ≤ 1 + 2.5ε. Taking squares in Eqn. (17) we get
1 ≤ γ (1 + 28ε) (cid:13)(cid:13)(I − XoptX ⊤
opt )A(cid:13)(cid:13)
2
θ2
F .
Finally, rescaling ε accordingly and applying the union bound on Lemma 5 and De ﬁni tion 2 concludes the proof.

5 Experiments

This section describes an empirical evaluation of Algorithm 1 on a face images collection. We implemented our
algorithm in MatLab and compared it against other prominent dimensionality reduction techniques such as the Local
Linear Embedding (LLE) algorithm and the Laplacian scores for feature selection. We ran all the experiments on a
Mac machine with a dual core 2.26 Ghz processor and 4 GB of RAM. Our empirical ﬁndings are very promising
indicating that our algorithm and implementation could be very useful in real applications involving clustering of
large-scale data.

5.1 An application of Algorithm 1 on a face images collection

We experiment with a face images collection. We downloaded the images corresponding to the ORL database from
[21]. This collection contains 400 face images of dimensions 64 × 64 corresponding to 40 different people. These
images form 40 groups each one containing exactly 10 different images of the same person. After vectorizing each
2-D image and putting it as a row vector in an appropriate matrix, one can construct a 400 × 4096 image-by-pixel
matrix A. In this matrix, objects are the face images of the ORL collection while features are the pixel values of the
images. To apply the Lloyd’s heuristic on A, we employ MatLab’s function kmeans with the parameter determining
the maximum number of repetitions setting to 30. We also chose a deterministic initialization of the Lloyd’s iterative
E-M procedure, i.e. whenever we call kmeans with inputs a matrix ˜A ∈ R400× ˜d , with ˜d ≥ 1, and the integer k = 40,
we initialize the cluster centers with the 1-st, 11-th,..., 391-th rows of ˜A, respectively. Note that this initialization
corresponds to picking images from the forty different groups of the available collection, since the images of every
group are stored sequentially in A. We evaluate the clustering outcome from two different perspectives. First, we
measure and report the objective function F of the k -means clustering problem. In particular, we report a normalized
version of F , i.e. ˜F = F /||A||2
F . Second, we report the mis-classiﬁcation accuracy of the cl ustering result. We
denote this number by P (0 ≤ P ≤ 1), where P = 0.9, for example, implies that 90% of the objects were assigned
to the correct cluster after the application of the clustering algorithm. In the sequel, we ﬁrst perform experiments by
running Algorithm 1 with everything ﬁxed but
t, which denotes the dimensionality of the projected data. Then, for
four representative values of t, we compare Algorithm 1 with three other dimensionality reduction methods as well
with the approach of running the Lloyd’s heuristic on the original high dimensional data.

We run Algorithm 1 with t = 5, 10, ..., 300 and k = 40 on the matrix A described above. Figure 1 depicts the results
of our experiments. A few interesting observations are immediate. First, the normalized objective function ˜F is a
piece-wise non-increasing function of the number of dimensions t. The decrease in ˜F is large in the ﬁrst few choices

7

t = 10

t = 20

t = 50

t = 100

P
0.5900
0.6500
0.3400
0.6255
0.4225

F
0.0262
0.0245
0.0380
0.0220
0.0283

P
0.6750
0.7125
0.3875
0.6255
0.4800

F
0.0268
0.0247
0.0362
0.0220
0.0255

P
0.7650
0.7725
0.4575
0.6255
0.6425

F
0.0269
0.0258
0.0319
0.0220
0.0234

P
0.6500
0.6150
0.4850
0.6255
0.6575

F
0.0324
0.0337
0.0278
0.0220
0.0219

SVD
LLE
LS
HD
RP

Table 2: Numerics from our experiments with ﬁve different me thods.

of t; then, increasing the number of dimensions t of the projected data decreases ˜F by a smaller value. The increase
of t seems to become irrelevant after around t = 90 dimensions. Second, the mis-classiﬁcation rate P is a piece-wise
non-decreasing function of t. The increase of t seems to become irrelevant again after around t = 90 dimensions.
Another interesting observation of these two plots is that the mis-classiﬁcation rate is not directly relevant to the
objective function F . Notice, for example, that the two have different behavior from t = 20 to t = 25 dimensions.
Finally, we report the running time T of the algorithm which includes only the clustering step. Notice that the increase
in the running time is - almost - linear with the increase of t. The non-linearities in the plot are due to the fact that
the number of iterations that are necessary to guarantee convergence of the Lloyd’s method are different for different
values of t. This observation indicates that small values of t result to signiﬁcant computational savings, especially
when n is large. Compare, for example, the one second running time that is needed to solve the k -means problem
when t = 275 against the 10 seconds that are necessary to solve the problem on the high dimensional data. To our
bene ﬁt, in this case, the multiplication AR takes only 0.1 seconds resulting to a total running time of 1.1 seconds
which corresponds to an almost 90% speedup of the overall procedure.
We now compare our algorithm against other dimensionality reduction techniques. In particular, in this paragraph
we present head-to-head comparisons for the following ﬁve m ethods: (i) SVD: the Singular Value Decomposition
(or Principal Components Analysis) dimensionality reduction approach - we use MatLab’s svds function; (ii) LLE:
the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K
determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10]
- we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k -means algorithm on the High
Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab
implementation. The results of our experiments on A, k = 40 and t = 10, 20, 50, 100 are shown in Table 2. In terms of
computational complexity, for example t = 50, the time (in seconds) needed for all ﬁve methods (only the di mension
reduction step) are TSV D = 5.9, TLLE = 4.4, TLS = 0.32, THD = 0, and TRP = 0.03. Notice that our algorithm
is much faster than the other approaches while achieving worse (t = 10, 20), slightly worse (t = 50) or slightly better
(t = 100) approximation accuracy results.

5.2 A note on the mailman algorithm for matrix-matrix and matrix-vector multiplication
In this section, we compare three different implementations of the third step of Algorithm 1. As we already discussed
in Section 3.1, the mailman algorithm is asymptotically faster than naively multiplying the two matrices A and R.
In this section we want to understand whether this asymptotic behavior of the mailman algorithm is indeed achieved
in a practical implementation. We compare three different approaches for the implementation of the third step of
our algorithm: the ﬁrst is MatLab’s function times(A, R) (MM1); the second exploits the fact that we do not need to
explicitly store the whole matrix R, and that the computation can be performed on the ﬂy (column- by-column) (MM2);
the last is the mailman algorithm [15] (see Section 3.1 for more details). We implemented the last two algorithms in
C using MatLab’s MEX technology. We observed that when A is a vector (n = 1), then the mailman algorithm
is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15]. Moreover, it’s
worth-noting that (MM2) is also superior compared to (MM1). On the other hand, our best implementation of the
mailman algorithm for matrix-matrix operations is inferior to both (MM1) and (MM2) for any 10 ≤ n ≤ 10, 000.
Based on these ﬁndings, we chose to use (MM1) for our experime ntal evaluations.
Acknowledgments: Christos Boutsidis was supported by NSF CCF 0916415 and a Gerondelis Foundation Fellow-
ship; Petros Drineas was partially supported by an NSF CAREER Award and NSF CCF 0916415.

2 In particular, we run W = constructW (A); S cores = LaplacianS core(A, W );

8

References

[1] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of
Computer and System Science, 66(4):671 –687, 2003.
[2] N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In
ACM Symposium on Theory of Computing (STOC), pages 557 –563, 2006.
[3] D. Aloise, A. Deshpande, P. Hansen, and P. Popat. NP-hardness of Euclidean sum-of-squares clustering. Machine
Learning, 75(2):245 –248, 2009.
[4] E. Bingham and H. Mannila. Random projection in dimensionality reduction: applications to image and text
data. In ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), pages 245 –
250, 2001.
[5] C. Boutsidis, M. W. Mahoney, and P. Drineas. Unsupervised feature selection for the k -means clustering problem.
In Advances in Neural Information Processing Systems (NIPS), 2009.
[6] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering in large graphs and matrices. In ACM-
SIAM Symposium on Discrete Algorithms (SODA), pages 291 –299, 1999.
[7] D. Foley and J. Sammon. An optimal set of discriminant vectors. IEEE Transactions on Computers, C-24(3):281 –
289, March 1975.
[8] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning
Research, 3:1157 –1182, 2003.
[9] I. Guyon, S. Gunn, A. Ben-Hur, and G. Dror. Result analysis of the NIPS 2003 feature selection challenge. In
Advances in Neural Information Processing Systems (NIPS), pages 545 –552. 2005.
[10] X. He, D. Cai, and P. Niyogi. Laplacian score for feature selection. In Advances in Neural Information Processing
Systems (NIPS) 18, pages 507 –514. 2006.
[11] P. Indyk and R. Motwani Approximate nearest neighbors: towards removing the curse of dimensionality. In
ACM Symposium on Theory of Computing (STOC), pages 604 –613, 1998.
[12] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary mathe-
matics, 26(189-206):1 –1, 1984.
[13] E. Kokiopoulou, J. Chen and Y. Saad. Trace optimization and eigenproblems in dimension reduction methods.
Numerical Linear Algebra with Applications, to appear.
[14] A. Kumar, Y. Sabharwal, and S. Sen. A simple linear time (1+ε)-approximation algorithm for k-means clustering
in any dimensions. In IEEE Symposium on Foundations of Computer Science (FOCS), pages 454 –462, 2004.
[15] E. Liberty and S. Zucker. The Mailman algorithm: A note on matrix-vector multiplication. Information Process-
ing Letters, 109(3):179 –182, 2009.
[16] S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129 –137, 1982.
[17] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy. The effectiveness of Lloyd-type methods for the
k -means problem. In IEEE Symposium on Foundations of Computer Science (FOCS), pages 165 –176, 2006.
[18] S. Roweis, and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:5500,
pages 2323-2326, 2000.
[19] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In IEEE Symposium
on Foundations of Computer Science (FOCS), pages 329 –337, 2006.
[20] X. Wu et al. Top 10 algorithms in data mining. Knowledge and Information Systems, 14(1):1 –37, 2008.
[21] http://www.cs.uiuc.edu/ ˜ dengcai2/Data/FaceData.html
[22] http://www.cs.uiuc.edu/ ˜ dengcai2/Data/data.html
[23] http://www.cs.nyu.edu/ ˜ roweis/lle/

9


Spectral Regularization for Support Estimation

Ernesto De Vito
DSA, Univ. di Genova, and
INFN, Sezione di Genova, Italy
devito@dima.ungie.it

Lorenzo Rosasco
CBCL - MIT, - USA, and
IIT, Italy
lrosasco@mit.edu

Alessandro Toigo
Politec. di Milano, Dept. of Math., and
INFN, Sezione di Milano, Italy
toigo@ge.infn.it

Abstract

In this paper we consider the problem of learning from data the support of a prob-
ability distribution when the distribution does not have a density (with respect to
some reference measure). We propose a new class of regularized spectral esti-
mators based on a new notion of reproducing kernel Hilbert space, which we call
. Completely regular kernels allow to capture the relevant
“completely regular”
geometric and topological properties of an arbitrary probability space. In partic-
ular, they are the key ingredient to prove the universal consistency of the spectral
estimators and in this respect they are the analogue of universal kernels for su-
pervised problems. Numerical experiments show that spectral estimators compare
favorably to state of the art machine learning algorithms for density support esti-
mation.

1 Introduction

In this paper we consider the problem of estimating the support of an arbitrary probability distribu-
tion and we are more broadly motivated by the problem of learning from complex high dimensional
data. The general intuition that allows to tackle these problems is that, though the initial repre-
sentation of the data is often very high dimensional, in most situations the data are not uniformly
distributed, but are in fact con ﬁned to a small (possibly low dimensional) region. Making such an
intuition rigorous is the key towards designing effective algorithms for high dimensional learning.
The problem of estimating the support of a probability distribution is of interest in a variety of ap-
plications such as anomaly/novelty detection [8], or surface modeling [16]. From a theoretical point
of view the problem has been usually considered in the setting where the probability distribution has
a density with respect to a known measure (for example the Lebesgue measure in Rd or the volume
measure on a manifold). Among others we mention [22, 5] and references therein. Algorithms in-
spired by Support Vector Machine (SVM), often called one-class SVM are have been proposed see
[17, 20] and references therein. Another kernel method, related to the one we discuss in this paper, is
presented in [11]. More generally one of the main approaches to learning from high dimensional is
the one considered in manifold learning. In this context the data are assumed to lie on a low dimen-
sional Riemannian sub-manifold embedded (that is represented) in a high dimensional Euclidean
space. This framework inspired algorithms to solve a variety of problems such as: semisupervised
learning [3], clustering [23], data parameterization/dimensionality reduction [15, 21], to name a few.
The basic assumption underlying manifold learning is often too restrictive to describe real data and
this motivates considering other models, such as the setting where the data are assumed to be es-
sentially concentrated around a low dimensional manifold as in [12], or can be modeled as samples
from a metric space as in [10].

1

In this paper we consider a general scenario (see [18]) where the underlying model is a probability
space (X, ρ) and we are given a (similarity) function K which is a reproducing kernel. The available
training set is an i.i.d sample x1 , . . . , xn ∼ ρ. The geometry (and topology) in (X, ρ) is de ﬁned by
the kernel K . While this framework is abstract and poses new challenges, by assuming the similarity
function to be a reproducing kernel we can make full use of the good computational properties of
kernel methods and the powerful theory of reproducing kernel Hilbert spaces (RKHS) [2]. Interest-
ingly, the idea of using a reproducing kernel K to construct a metric on a set X is originally due to
Schoenberg (see for example [4]).
Broadly speaking, in this setting we consider the problem of ﬁnding a model of the smallest region
Xρ containing all the data. A rigorous formalization of this problem requires: 1) de ﬁn ing the region
Xρ , 2) specifying the sense in which we model Xρ . This can be easily done if the probability distri-
bution has density p with respect to a known measure, in fact Xρ = {x ∈ X : p(x) > 0}, but is
otherwise a challenging question for a general distribution. Intuitively, Xρ can be thought of as the
region where the distribution is concentrated, that is ρ(Xρ ) = 1. However, there are many different
sets having this property. If X is Rd (in fact any topological space), a natural candidate to de ﬁn e the
region of interest, is the notion of support of a probability distribution – de ﬁned as the intersection
of the closed subsets C of X , such that ρ(C ) = 1. In an arbitrary probability space the support of
the measure is not well de ﬁned since no topology is given.
The reproducing kernel K provides a way to solve this problem and also suggests a possible ap-
proach to model Xρ . The ﬁrst idea is to use the fact that under mild assumptions t he kernel de ﬁnes
a metric on X [18], so that the concept of closed set, hence that of support, is well de ﬁned. The
second idea is to use the kernel to construct a function Fρ such that the level set corresponding to
one is exactly the support Xρ – in this case we say that the RKHS associated to K separates the
support Xρ . By doing this we are in fact imposing an assumption on Xρ : given a kernel K , we can
only separate certain sets. More precisely, our contribution is two-fold.

• We prove that Fρ is uniquely de ﬁned by the null space of the integral operator associated
to K . Given that the integral operator (and its spectral properties) can be approximated
studying the kernel matrix on a sample, this result suggests a way to estimate the support
empirically. However, a further complication arises from the fact that in general zero is
not an isolated point of the spectrum, so that the estimation of a null space is an ill-posed
problem (see for example [9]). Then, a regularization approach is needed in order to ﬁnd a
stable (hence generalizing) estimator. In this paper, we consider a spectral estimator based
on a spectral regularization strategy, replacing the kernel matrix with its regularized version
(Tikhonov regularization being one example).
• We introduce the notion of completely regular RKHS, that answer positively to the ques-
tion whether there exist kernels that can separate the support of any distribution. Examples
of completely regular kernels are presented and results suggesting how they can be con-
structed are given. The concept of completely regular RKHS plays a role similar to the
concept of universal kernels in supervised learning, for example see [19].

Finally, given the above results, we show that the regularized spectral estimator enjoys a universal
consistency property: the correct support can be asymptotically recovered for any problem (that is
any probability distribution).
The plan of the paper is as follows. In Section 2 we introduce the notion of completely regular
kernels and their basic properties. In Section 3 we present the proposed regularized algorithms. In
Section 4 and 5 we provide a theoretical and empirical analysis, respectively. Proofs and further
development can be found in the supplementary material.

2 Completely regular reproducing kernel Hilbert spaces

In this section we introduce the notion of a completely regular reproducing kernel Hilbert space.
Such a space de ﬁnes a geometry on a measurable space X which is compatible with the measurable
structure. Furthermore it shows how to de ﬁne a function F such that the one level set is the support
of the probability distribution. The function is determined by the spectral projection associated with
the null eigenvalue of the integral operator de ﬁned by the re producing kernel. All the proofs of this
section are reported in the supplementary material.

2

We assume X to be a measurable space with a probability measure ρ. We ﬁx a complex 1 reproducing
kernel Hilbert space H on X with a reproducing kernel K : X × X → C [2]. The scalar product
and the norm are denoted by h·, ·i, linear in the ﬁrst argument, and k·k, respectively. For all x ∈ X ,
Kx ∈ H denotes the function K (·, x). For each function f ∈ H, the reproducing property f (x) =
hf , Kx i holds for all x ∈ X . When different reproducing kernel Hilbert spaces are considered, we
denote by HK the reproducing kernel Hilbert space with reproducing kernel K . Before giving the
de ﬁnition of completely regular RKHS, which is the key conce pt presented in this section, we need
some preliminary de ﬁnitions and results.
De ﬁnition 1. A subset C ⊂ X is separated by H, if, for any x0 6∈ C , there exists f ∈ H such that
(1)
and
f (x0 ) 6= 0
f (x) = 0
∀x ∈ C.
For example, if X = Rd and H is the reproducing kernel Hilbert space with linear kernel K (x, t) =
x · t, the sets separated by H are precisely the hyperplanes containing the origin. In Eq. (1) the
function f depends on x0 and C , but Proposition 1 below will show that there is a function, possibly
not in H, whose one level set is precisely C ( if K (x, x) = 1 ). Note that in [19] a different notion
of separating property is given.
We need some further notation. For any set C , let PC : H → H be the orthogonal projection onto
the closure of the linear space generated by {Kx | x ∈ C }, so that P 2
C = PC , P ∗
C = PC and
ker PC = {Kx | x ∈ C }⊥ = {f ∈ H | f (x) = 0, ∀x ∈ C }.
Moreover let FC : X → C be de ﬁned by FC (x) = hPC Kx , Kx i .
Proposition 1. For any subset C ⊂ X , the following facts are equivalent
(i) the set C is separated by H;
(ii) for all x 6∈ C , Kx /∈ Ran PC ;
(iii) C = {x ∈ X | FC (x) = K (x, x)}.
If one of the above conditions is satisﬁed, then K (x, x) 6= 0
∀x /∈ C.
A natural and minimal requirement on H is to be able to separates any pairs of distinct points and
this implies that Kx 6= Kt if x 6= t and K (x, x) 6= 0. The ﬁrst condition ensures the metric given
by
(2)
dK (x, y ) = kKx − Ktk
x, t ∈ X.
to be well de ﬁned. Then (X, dK ) is a metric space and the sets separated by H are always dK -
closed, see Prop. 2 below. This last property is not enough to ensure that we can evaluate ρ on the
set separated by RKHS H. In fact the σ-algebra generated by the metric d might not be contained in
the σ-algebra on X . The next result shows that assuming the kernel to be measurable is enough to
solve this problem.
Proposition 2. Assume that Kx 6= Kt if x 6= t, then the sets separated by H are closed with respect
to dK . Moreover, if H is separable and the kernel is measurable, then the sets separated by H are
measurable.

Given the above premises, the following is the key de ﬁnition that characterizes the reproducing
kernel Hilbert spaces which are able to separate the largest family of subsets of X .
De ﬁnition 2 (Completely Regular RKHS). A reproducing kernel Hilbert space H with reproducing
kernel K such that Kx 6= Kt if x 6= t is called completely regular if H separates all the subsets
C ⊂ X which are closed with respect to the metric (2).
The term completely regular is borrowed from topology, where a topological space is called com-
pletely regular if, for any closed subset C and any point x0 /∈ C , there exists a continuous function f
such that f (x0 ) 6= 0 and f (x) = 0 for all x ∈ C . In the supplementary material, several examples of
completely regular reproducing kernel Hilbert spaces are given, as well as a discussion on how such
spaces can be constructed. A particular case is when X is already a metric space with a distance

1Considering complex valued RKHS allows to use the theory of Fourier transform and for practical prob-
lems we can simply consider real valued kernels.

3

function dX . If K is continuous with respect to dX , the assumption of complete regularity forces
the metrics dK and dX to have the same closed subsets. Then, the supports de ﬁned by dK and dX
are the same. Furthermore, since the closed sets of X are independent of H, the complete regularity
of H can be proved by showing that a suitable family of bump2 functions is contained in H.
Corollary 1. Let X be a separable metric space with respect to a metric dX . Assume that the kernel
K is a continuous function with respect to dX and that the space H separates every subset C which
is closed with respect to dX . Then
(i) The space H is separable and K is measurable with respect to the Borel σ-algebra gener-
ated by dX .

(ii) The metric dK de ﬁned by (2) is equivalent to dX , that is, a set is closed with respect to dK
if and only if it is closed with respect to dX .
(iii) The space H is completely regular.
As a consequence of the above result, many classical reproducing kernel Hilbert spaces are com-
pletely regular. For example, if X = Rd and H is the Sobolev space of order s with s > d/2, then H
is completely regular. This is due to the fact that the space of smooth compactly supported functions
is contained in H. In fact, a standard result of analysis ensures that, for any closed set C and any
x0 /∈ C there exists a smooth bump function such that f (x0 ) = 1 and its support is contained in
the complement of C . Interestingly enough, if H is the reproducing kernel Hilbert space with the
Gaussian kernel, it is known that the elements of H are analytic functions, see Cor. 4.44 in [19].
Clearly H can not be completely regular. Indeed, if C is a closed subset of Rd with not empty inte-
rior and f ∈ H is such that f (x) = 0 for all x ∈ C , a standard result of complex analysis implies
that f (x) = 0 for every x ∈ Rd . Finally, the next result shows that the reproducing kernel can be
normalized to one on the diagonal under the mild assumption that K (x, x) 6= 0 for all x ∈ X .
Lemma 1. Assume that K (x, x) > 0 for all x ∈ X . Then the reproducing kernel Hilbert space
K (x, t)
with the normalized kernel K ′(x, t) =
separates the same sets as H.
pK (x, x)K (t, t)
Finally we brie ﬂy mention some examples and refer to the supp lementary material for further de-
velopments. In particular, we prove that both the Laplacian kernel K (x, y ) = e− kx−yk2√2σ
and ℓ1 -
exponential kernel K (x, y ) = e− kx−yk1√2σ
de ﬁned on Rd are completely regular for any σ > 0 and
d ∈ N.
3 Spectral Algorithms for Learning the Support

In this section, we ﬁrst discuss our framework and our main as sumptions. Then we present the
proposed regularized spectral algorithms.
Motivated by the results in the previous section, we describe our framework which is given by a triple
(X, ρ, K ). We consider a probability space (X, ρ) and a training set x = (x1 . . . , xn ) sampled
i.i.d. with respect to ρ. Moreover we consider a reproducing kernel K satisfying the following
assumption.
Assumption 1. The reproducing kernel K is measurable and K (x, x) = 1, for all x ∈ X . Moreover
K de ﬁnes a completely regular and separable RKHS H.
We endow X with the metric dK de ﬁned in (2), so that X becomes a separable metric space. The
assumption of complete regularity ensures that any closed subset is separated by H and, hence, is
measurable by Prop. 2. Then we can de ﬁne the support Xρ of the measure ρ, as the intersection of
all the closed sets C ⊂ X , such that ρ(C ) = 1. Clearly Xρ is closed and ρ(Xρ ) = 1 (note that this
last property depends on the separability of X , hence of H).
Summarizing the key result in the previous section, under the above assumptions, Xρ is the one level
set of the function Fρ : X → [0, 1]

Fρ (x) = hPρKx , Kx i ,
2Given an open subset U and a compact subset C ⊂ U , a bump function is a continuous compactly sup-
ported function which is one on C and its support is contained in U .

4

where Pρ is a short notation for PXρ . Since Fρ depends on the unknown measure ρ, in practice
it cannot be explicitly calculated. To design an effective empirical estimator we develop a novel
characterization of the support of an arbitrary distribution that we describe in the next section.

3.1 A New Characterization of the Support

The key observation towards de ﬁning a learning algorithm to estimate Xρ it is that the projection Pρ
can be expressed in terms of the integral operator de ﬁned by t he kernel K .
To see this, for all x ∈ X , let Kx ⊗ Kx denote the rank one positive operator on H, given by
f ∈ H.
(Kx ⊗ Kx )(f ) = hf , Kx i Kx = f (x)Kx
Moreover, let T : H → H be the linear operator de ﬁned as
T = ZX
Kx ⊗ Kxdρ(x),
where the integral converges in the Hilbert space of Hilbert-Schmidt operators on H (see for example
[7] for the proof). Using the reproducing property in H [2], it is straightforward to see that T is
simply the integral operator with kernel K with domain and range in H.
Then, one can easily see that the null space of T is precisely (I − Pρ )H, so that
Pρ = T †T ,

(3)

where T † is the pseudo-inverse of T (see for example [9]). Hence
Fρ (x) = (cid:10)T †T Kx , Kx(cid:11) .
Observe that in general Kx does not belong to the domain of T † and, if θ denotes the Heaviside
function with θ(0) = 0, then spectral theory gives that Pρ = T †T = θ(T ). The above observation
is crucial as it gives a new characterization of the support of ρ in terms of the null space of T and
the latter can be estimated from data.

3.2 Spectral Regularization Algorithms

Finally, in this section, we describe how to construct an estimator Fn of Fρ . As we mentioned above,
Eq. (3) suggests a possible way to learn the projection from ﬁ nite data. In fact, we can consider the
empirical version of the integral operator associated to K which is simply de ﬁned by

Tn =

almost surely,

n
1
Xi=1
Kxi ⊗ Kxi .
n
The latter operator is an unbiased estimator of T . Indeed, since Kx ⊗ Kx is a bounded random
variable into the separable Hilbert space of Hilbert-Schmidt operators, one can use concentration
inequalities for random variables in Hilbert spaces to prove that
√n
lim
log n kT − TnkHS = 0
n→+∞
where k·kHS is the Hilbert-Schmidt norm (see for example [14] for a short proof). However, in
general T †
nTn does non converge to T †T since 0 is an accumulation point of the spectrum of T or,
equivalently, since T † is not a bounded operator. Hence, a regularization approach is needed.
In this paper we study a spectral ﬁltering approach which rep laces T †
n with an approximation gλ (Tn )
obtained ﬁltering out
the components corresponding to the small eigenvalues of Tn . The function gλ
is de ﬁned by spectral calculus. More precisely if Tn = Pj σj vj ⊗ vj is a spectral decomposition of
Tn , then gλ (Tn ) = Pj gλ (σj )vj ⊗ vj . Spectral regularization de ﬁned by linear ﬁlters is classi
cal in
the theory of inverse problems [9]. Intuitively, gλ (Tn ) is an approximation of the generalized inverse
n and it is such that the approximation gets better, but the condition number of gλ (Tn ) gets worse
T †
as λ decreases. More formally these properties are captured by the following set of conditions.
Assumption 2. For σ ∈ [0, 1], let rλ (σ) := σgλ (σ), then
• rλ (σ) ∈ [0, 1], ∀λ > 0,

(4)

5

• limλ→0 rλ (σ) = 1, , ∀σ > 0
• |rλ (σ) − rλ (σ ′ )| ≤ Lλ |σ − σ ′ |, ∀λ > 0, where Lλ is a positive constant depending on λ.
Examples of algorithms that fall into the above class include iterative methods– akin to boosting
gλ (σ) = Pmλ
k=0 (1 − σ)k , spectral cut-off gλ (σ) = 1
1σ≤λ (σ), and Tikhonov regular-
1σ>λ (σ) + 1
σ
λ
σ+λ . We refer the reader to [9] for more details and examples, and, given the space
ization gλ (σ) = 1
constraints, will focus mostly on Tikhonov regularization in the following.
For a chosen ﬁlter, the regularized empirical estimator of Fρ can be de ﬁned by
Fn (x) = hgλ (Tn )TnKx , Kx i .
One can see that that the computation of Fn reduces to solving a simple ﬁnite dimensional problem
involving the empirical kernel matrix de ﬁned by the trainin g data. Towards this end, it is useful to
introduce the sampling operator Sn : H → Cn de ﬁned by Snf = (f (x1 ), . . . , f (xn )), f ∈ H,
which can be interpreted as the restriction operator which evaluates functions in H on the training set
nα = Pn
points. The adjoint S ∗
n : Cn → H of Sn is given by S ∗
i=1 αiKxi , α = (α1 , . . . , αn ) ∈ Cn ,
and can be interpreted as the out-of-sample extension operator. A simple computation shows that
nSn and SnS ∗
n = Kn is the n by n kernel matrix, where the (i, j )-entry is K (xi , xj ).
Tn = 1
n S ∗
Then it is easy to see that gλ (Tn )Tn = gλ (S ∗
n gλ (Kn/n)Sn , so that
nSn/n = 1
nSn /n)S ∗
n S ∗
1
n
where kx is the n-dimensional column vector kx = SnKx = (K (x1 , x), . . . , K (xn , x)) . Note that
Equation (6) plays the role of a representer theorem for the spectral estimator, in the sense that it
reduces the problem of ﬁnding an estimator in an in ﬁnite dime
nsional space to a ﬁnite dimensional
problem.

T gλ (Kn/n)kx ,

(5)

Fn (x) =

kx

(6)

4 Theoretical Analysis: Universal Consistency

In this section we study the consistency property of spectral estimators. All the proofs of this section
are reported in the supplementary material. We prove the results only for the ﬁlter corresponding to
the classical Tikhonov regularization though the same results hold for the class of spectral ﬁlters de-
scribed by Assumption 2. To study the consistency of the methods we need to choose an appropriate
performance measure to compare Fn and Fρ . Note that there is no natural notion of risk, since we
have to compute the function on and off the support. Also note that standard metric used for support
estimation (see for example [22, 5]) cannot be used in our analsys since they rely on the existence
of a reference measure µ (usually the Lebesgue measure) and the assumption that ρ is absolutely
continuous with respect to µ.
The following preliminary result shows that we can control the convergence of the Tikhonov esti-
mator Fn , de ﬁned by gλ (T ) = (Tn + λn I )−1 , to Fρ uniformly on any compact set of X , provided
a suitable sequence λn .
Theorem 1. Let Fn be the estimator de ﬁned by Tikhonov regularization and choo se a sequence λn
so that

(7)

(8)

lim
n→∞

λn = 0 and

limsup
n→∞

log n
λn√n

< +∞,

then

sup
x∈C |Fn (x) − Fρ (x)| = 0,
lim
n→+∞
for every compact subset C of X

almost surely,

We add three comments. First, we note that, as we mentioned before, Tikhonov regularization
can be replaced by a large class of ﬁlters. Second, we observe that a natural choice would be the
regularization de ﬁned by kernel PCA [11], which correspond s to truncating the generalized inverse
of the kernel matrix at some cutoff parameter λ. However, one can show that, in general, in this case
it is not possible to choose λ so that the sample error goes to zero. In fact, for KPCA the sample
error depends on the gap between the M -th and the M + 1-th eigenvalue of T [1], where M -th
and M + 1-th are the eigenvalues around the cutoff parameter. Such a gap can go to zero with an

6

arbitrary rate so that there exists no choice of the cut-off parameter ensuring convergence to zero
of the sample error. Third, we note that the uniform convergence of Fn to Fρ on compact subsets
does not imply the convergence of the level sets of Fn to the corresponding level sets of Fρ , for
example with respect to the standard Hausdorff distance among closed subsets. In practice to have
an effective decision rule, an off-set parameter τn can be introduced and the level set is replaced by
Xn = {x ∈ X | Fn (x) ≥ 1 − τn} – recall that Fn takes values in [0, 1]. The following result will
show that for a suitable choice of τn the Hausdorff distance between Xn ∩ C and Xρ ∩ C goes to
zero for all compact sets C . We recall that the Hausdorff distance between two subsets A, B ⊂ X is
dK (b, A)}
dH (A, B ) = max{ sup
dK (a, B ), sup
a∈A
b∈B
Theorem 2. If the sequence (τn )n∈N converges to zero in such a way that
supx∈C |Fn (x) − Fρ (x)|
τn

almost surely

(9)

lim sup
n→∞

≤ 1,

then,

lim
n→+∞
for any compact subset C .

dH (Xn ∩ C, Xρ ∩ C ) = 0

almost surely,

We add two comments. First, it is possible to show that, if the (normalized) kernel K is such that
limx′→∞ Kx (x′ ) = 0 for any x ∈ X – as it happens for the Laplacian kernel, then Theorems 1
and 2 also hold by choosing C = X . Second, note that the choice of τn depends on the rate of
convergence of Fn to Fρ which will itself depend on some a-priori assumption on ρ. Developing
learning rates and ﬁnite sample bound is a key question that w e will tackle in future work.

5 Empirical Analysis

In this section we describe some preliminary experiments aimed at testing the properties and the
performances of the proposed methods both on simlauted and real data. Again for space constraints
we will only discuss spectral algorithms induced by Tikhonov regularization. Note that while com-
putations can be made efﬁcient in several ways, we consider a simple algorithmic protocol and leave
a more re ﬁned computational study for future work. Followin g the discussion in the last section,
T (Kn + nλI )−1kx and a point is labeled
Tikhonov regularization de ﬁnes an estimator Fn (x) = kx
as belonging to the support if Fn (x) ≥ 1 − τ . The computational cost for the algorithm is, in the
worst case, of order n3 , like standard regularized least squares, for training and order N n2 if we
have to predict the value of Fn at N test points. In practice, one has to choose a good value for the
regularization parameter λ and this requires computing multiple solutions, a so called regularization
path. As noted in [13], if we form the inverse using the eigendecomposition of the kernel matrix the
price of computing the full regularization path is essentially the same as that of computing a single
solution (note that the cost of the eigen-decomposition of Kn is also of order n3 though the constant
is worse). This is the strategy that we consider in the following. In our experiments we consid-
ered two data-sets the MNIST data-set and the CBCL face database. For the digits we considered
a reduced set consisting of a training set of 5000 images and a test set of 1000 images. In the ﬁrst
experiment we trained on 500 images for the digit 3 and tested on 200 images of digits 3 and 8. Each
experiment consists of training on one class and testing on two different classes and was repeated
for 20 trials over different training set choices. The performance is evaluated computing ROC curve
(and the corresponding AUC value) for varying τ , τ ′ , τ ′′ . For all our experiments we considered the
Laplacian kernel. Note that, in this case the algorithm requires to choose 3 parameters: the regular-
ization parameter λ, the kernel width σ and the threshold τ . In supervised learning cross validation
is typically used for parameter tuning, but cannot be used in our setting since support estimation is
an unsupervised problem. Then, we considered the following heuristics. The kernel width is cho-
sen as the median of the distribution of distances of the K -th nearest neighbor of each training set
point for K = 10. Fixed the kernel width, we choose regularization parameter in correspondence
of the maximum curvature in the eigenvalue behavior– see Fig ure 1, the rational being that after this
value the eigenvalues are relatively small. For comparison we considered a Parzen window density
estimator and one-class SVM (1CSVM )as implemented by [6]. For the Parzen window estimator
we used the same kernel used in the spectral algorithm, that is the Laplacian kernel and use the

7

160

140

120

100

80

60

40

20

e
d
u
t
i
n
i
g
a
M
 
s
e
u
l
a
v
n
e
g
i
E

Eigenvalues Decay
Regularization Parameter

0

 
0

50

100

150
250
200
Eigenvalues Index

300

350

400

Eigenvalues Decay

 

Eigenvalues Decay

 

e
d
u
t
i
n
i
g
a
M
 
s
e
u
l
a
v
n
e
g
i
E

18

16

14

12

10

8

6

4

2

0

 

Eigenvalues Decay
Regularization Parameter

5

10

15

20
30
25
Eigenvalues Index

35

40

45

50

Figure 1: Decay of the eigenvalues of the kernel matrix ordered in decreasing magnitude and corre-
sponding regularization parameter (Left) and a detail of the ﬁrst 50 eigenvalues (Right).

same width used in our estimator. Given a kernel width an estimate of the probability distribution
is computed and can be used to estimate the support by ﬁxing a t hreshold τ ′ . For the one-class
SVM we considered the Gaussian kernel, so that we have to ﬁx th e kernel width and a regularization
parameter ν . We ﬁx the kernel width to be the same used by our estimator and ﬁxed ν = 0.9. For
the sake of comparison, also for one-class SVM we considered a varying offset τ ′′ . The ROC curves
on the different tasks are reported (for one of the trial) in Figure 2, Left. The mean and standard
deviation of the AUC for the 3 methods is reported in Table 5. Similar experiments were repeated
considering other pairs of digits, see Table 5. Also in the case of the CBCL data sets we considered
a reduced data-set consisting of 472 images for training and other 472 for test. On the different test
performed on the Mnist data the spectral algorithm always achieves results which are better- and
often substantially better - than those of the other methods. On the CBCL dataset SVM provides the
best result, but spectral algorithm still provides a competitive performance.

6 Conclusions

In this paper we presented a new approach to estimate the support of an arbitrary probability distri-
bution. Unlike previous work we drop the assumption that the distribution has a density with respect
to a (known) reference measure and consider a general probability space. To overcome this prob-
lem we introduce a new notion of RKHS, that we call completely regular, that captures the relevant
geometric properties of the probability distribution. Then, the support of the distribution can be
characterized as the null space of the integral operator de ﬁ ned by the kernel and can be estimated
using a spectral ﬁltering approach. The proposed estimator s are proven to be universally consistent
and have good empirical performances on some benchmark data-sets. Future work will be devoted

MNIST 9vs4

MNIST 1vs7

CBCL

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
o
P
e
u
r
T

 

Spectral
Parzen
OneClassSVM

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
o
P
e
u
r
T

 

Spectral
Parzen
OneClassSVM

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
o
P
e
u
r
T

 

Spectral
Parzen
OneClassSVM

0

 
0

0.1

0.2

0.3

0.4

0.5
FalsePos

0.6

0.7

0.8

0.9

1

0

 
0

0.1

0.2

0.3

0.4

0.5
FalsePos

0.6

0.7

0.8

0.9

1

0

 
0

0.1

0.2

0.3

0.4

0.5
FalsePos

0.6

0.7

0.8

0.9

1

Figure 2: ROC curves for the different estimator in three different tasks: digit 9vs 4 Left, digit 1vs 7
Center, CBCL Right.

Spectral
Parzen
1CSVM

3vs 8
0.8371 ± 0.0056

8vs 3
0.7830 ± 0.0026

1vs 7
0.9921 ± 4.7283e − 04

9vs 4
0.8651 ± 0.0024

CBCL
0.8682 ± 0.0023

0.7841 ± 0.0069

0.7656 ± 0.0029

0.9811 ± 3.4158e − 04

0.0.7244 ± 0.0030

0.8778 ± 0.0023

0.7896 ± 0.0061

0.7642 ± 0.0032

0.9889 ± 1.8479e − 04

0.7535 ± 0.0041

0.8824 ± 0.0020

Table 1: Average and standard deviation of the AUC for the different estimators on the considered
tasks.

8

to derive ﬁnite sample bounds, to develop strategies to scal e-up the algorithms to massive data-sets
and to a more extensive experimental analysis.

References

[1] P. M. Anselone. Collectively compact operator approximation theory and applications to in-
tegral equations. Prentice-Hall Inc., Englewood Cliffs, N. J., 1971.
[2] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337 –404, 1950.
[3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. J. Mach. Learn. Res., 7:2399 –2434, 2006.
[4] C. Berg, J. Christensen, and P. Ressel. Harmonic analysis on semigroups, volume 100 of
Graduate Texts in Mathematics. Springer-Verlag, New York, 1984.
[5] G. Biau, B. Cadre, D. Mason, and Bruno Pelletier. Asymptotic normality in density support
estimation. Electron. J. Probab., 14:no. 91, 2617 –2635, 2009.
[6] S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotomamonjy. Svm and kernel methods matlab
toolbox. Perception Systmes et Information, INSA de Rouen, Rouen, France, 2005.
[7] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces of
integrable functions and Mercer theorem. Anal. Appl. (Singap.), 4(4):377 –408, 2006.
[8] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Comput. Surv.,
41(3):1 –58, 2009.
[9] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems, volume 375 of
Mathematics and its Applications. Kluwer Academic Publishers Group, Dordrecht, 1996.
[10] M. Hein, O. Bousquet, and B. Schlkopf. Maximal margin classiﬁcation for metric spaces.
Journal of Computer and System Sciences, 71(3):333 –359, 10 2005.
[11] H. Hoffmann. Kernel pca for novelty detection. Pattern Recogn., 40(3):863 –874, 2007.
[12] P Niyogi, S Smale, and S Weinberger. A topological view of unsupervised learning from noisy
data. preprint, Jan 2008.
[13] R. Rifkin and R. Lippert. Notes on regularized least squares. Technical report, Massachusetts
Institute of Technology, 2007.
[14] L. Rosasco, M. Belkin, and E. De Vito. On learning with integral operators. J. Mach. Learn.
Res., 11:905 –934, 2010.
[15] S Roweis and L Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-
ence, Jan 2000.
[16] B. Sch ¨olkopf, J. Giesen, and S. Spalinger. Kernel methods for implicit surface modeling. In
Advances in Neural Information Processing Systems 17, pages 1193 –1200, Cambridge, MA,
2005. MIT Press.
[17] B. Sch ¨olkopf, J. Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. Estimating the support
of a high-dimensional distribution. Neural Comput., 13(7):1443 –1471, 2001.
[18] S. Smale and D.X. Zhou. Geometry of probability spaces. Constr. Approx., 30(3):311 –323,
2009.
[19] I. Steinwart and A. Christmann. Support vector machines. Information Science and Statistics.
Springer, New York, 2008.
[20] I. Steinwart, D. Hush, and C. Scovel. A classiﬁcation fr amework for anomaly detection. J.
Mach. Learn. Res., 6:211 –232 (electronic), 2005.
[21] J. Tenenbaum, V. Silva, and J. Langford. A global geometric framework for nonlinear dimen-
sionality reduction. Science, Jan 2000.
[22] A. B. Tsybakov. On nonparametric estimation of density level sets. Ann. Statist., 25(3):948 –
969, 1997.
[23] U. Von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4), 2007.

9


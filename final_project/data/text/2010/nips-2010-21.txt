Online Classi ﬁcation with Speci ﬁcity Constraints

Andrey Bernstein
Department of Electrical Engineering
Technion - Israel Institute of Technology
Haifa, 32000, Israel
andreyb@tx.technion.ac.il

Shie Mannor
Department of Electrical Engineering
Technion - Israel Institute of Technology
Haifa, 32000, Israel
shie@ee.technion.ac.il

Nahum Shimkin
Department of Electrical Engineering
Technion - Israel Institute of Technology
Haifa, 32000, Israel
shimkin@ee.technion.ac.il

Abstract

We consider the online binary classi ﬁcation problem, where we are given m clas-
si ﬁers. At each stage, the classi ﬁers map the input to the pro
bability that the input
belongs to the positive class. An online classi ﬁcation meta -algorithm is an algo-
rithm that combines the outputs of the classi ﬁers in order to attain a certain goal,
without having prior knowledge on the form and statistics of the input, and with-
out prior knowledge on the performance of the given classi ﬁe rs. In this paper, we
use sensitivity and speci ﬁcity as the performance metrics of the meta-algorithm. In
particular, our goal is to design an algorithm that satis ﬁes
the following two prop-
erties (asymptotically): (i) its average false positive rate (fp-rate) is under some
given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the
tp-rate of the best convex combination of the m given classi ﬁers that satis ﬁes fp-
rate constraint, in hindsight. We show that this problem is in fact a special case of
the regret minimization problem with constraints, and therefore the above goal is
not attainable. Hence, we pose a relaxed goal and propose a corresponding practi-
cal online learning meta-algorithm that attains it. In the case of two classi ﬁers, we
show that this algorithm takes a very simple form. To our best knowledge, this is
the ﬁrst algorithm that addresses the problem of the average tp-rate maximization
under average fp-rate constraints in the online setting.

1

Introduction

Consider the binary classi ﬁcation problem, where each inpu t is classi ﬁed into +1 or −1. A classi ﬁer
is an algorithm which, for every input, classi ﬁes that input
. In general, classi ﬁers may produce the
probability of the input to belong to class 1. There are several metrics for the performance of the
classi ﬁer in the ofﬂine setting, where a training set is give
n in advance. These include error (or
mistake) count, true positive rate, and false positive rate; see [6] for a discussion. In particular,
the true positive rate (tp-rate) is given by the fraction of the number of positive instances correctly
classi ﬁed out of the total number of the positive instances, while false positive rate (fp-rate) is given
by the fraction of the number of negative instances incorrectly classi ﬁed out of the total number
of the negative instances. A receiver operating characteristics (ROC) graph then depicts different
classi ﬁers using their tp-rate on the Y axis, while fp-rate on the X axis (see [6]). We note that
there are alternative names for these metrics in the literature. In particular, the tp-rate is also called
sensitivity, while one minus the fp-rate is usually called speci ﬁcity . In what follows, we prefer to
use the terms tp-rate and fp-rate, as we think that they are self-explaining.

1

In this paper we focus on the online classi ﬁcation problem, where no training set is given in ad-
vance. We are given m classi ﬁers, which at each stage n = 1, 2, ... map the input instance to the
probability of the instance to belong to the positive class. An online classi ﬁcation meta-algorithm
(or a selection algorithm) is an algorithm that combines the outputs of the given classi ﬁers in order
to attain a certain goal, without prior knowledge on the form and statistics of the input, and without
prior knowledge on the performance of the given classi ﬁers. The assumption is that the observed
sequence of classi ﬁcation probabilities and labels comes f rom some unknown source and, thus, can
be arbitrary. Therefore, it is convenient to formulate the online classi ﬁcation problem as a repeated
game between an agent and some abstract opponent that stands for the collective behavior of the
classi ﬁers and the realized labels. We note that, in this for mulation, we can identify the agent with a
corresponding online classi ﬁcation meta-algorithm.

There is a rich literature that deals with the online classi ﬁ cation problem, in the competitive ra-
tio framework, such as [5, 1]. In these works, the performance guarantees are usually expressed
in terms of the mistake bound of the algorithm. In this paper, we take a different approach. Our
performance metrics will be the average tp-rate and fp-rate of the meta-algorithm, while the per-
formance guarantees will be expressed in the regret minimization framework. In a seminal paper,
Hannan [8] introduced the optimal reward-in-hindsight r∗
n with respect to the empirical distribu-
n is in fact
tion of opponent’s actions, as a performance goal of an online algorithm. In our case, r∗
the maximal tp-rate the agent could get at time n by knowing the classi ﬁcation probabilities and
actual labels beforehand, using the best convex combination of the classi ﬁers . The regret is then
n and the actual average tp-rate obtained by the agent. Hannan
deﬁned as the difference between r∗
showed in [8] that there exist online algorithms whose regret converges to zero (or below) as time
progresses, regardless of the opponent’s actions, at 1/√n rate. Such algorithms are often called
no-regret, Hannan-consistent, or universally consistent algorithms. Additional no-regret algorithms
were proposed in the literature over the years, such as Blackwell’s approachability-based algorithm
[2] and weighted majority schemes [10, 7] (see [4] for an overview of these and other related algo-
rithms). These algorithms can be directly applied to the problem of online classi ﬁcation when the
goal is only to obtain no-regret with respect to the optimal tp-rate in hindsight.

However, in addition to tp-rate maximization, some performance guarantees in terms of the fp-
rate are usually required. In particular, it is reasonable to require (following the Neyman-Pearson
approach) that, in the long term, the average fp-rate of the agent will be below some given threshold
0 < γ < 1.
In this case the tp-rate can be considered as the average reward obtained by the
agent, while fp-rate – as the average cost. This is in fact a sp ecial case of the regret minimization
problem with constraints whose study was initiated by Mannor et al. in [11]. They deﬁne d the
constrained reward-in-hindsight with respect to the empirical distribution of opponent’s actions,
as a performance goal of an online algorithm. This quantity is the maximal average reward the
agent could get in hindsight, had he known the opponent’s actions beforehand, by using any ﬁxed
(mixed) action, while satisfying the average cost constraints. The desired online algorithm then has
to satisfy two requirements: (i) it should have a vanishing regret (with respect to the constrained
reward-in-hindsight); and (ii) it should asymptotically satisfy the average cost constraints.
It is
shown in [11] that such algorithms do not exist in general. The positive result is that a relaxed
goal, which is deﬁned in terms of the
convex hull of the constrained reward-in-hindsight over an
appropriate space, is attainable. The two no-regret algorithms proposed in [11] explicitly involve
either the convex hull or a calibrated forecast of the opponent’s actions. Both of these algorithms
may not be computationally feasible, since there are no efﬁc ient (polynomial time) procedures for
the computation of both the convex hull and a calibrated forecast.

In this paper, we take an alternative approach to that of [11]. Instead of examining the constrained
tp-rate in hindsight (or its convex hull), our starting point is the “standard” regret with respect to
the optimal (unconstrained) tp-rate, and we consider a certain relaxation thereof. In particular, we
deﬁne a simple relaxed form of the optimal tp-rate in-hindsi ght, by subtracting a positive constant
from the latter. We then ﬁnd the minimal constant needed in or der to have a vanishing regret (with
respect to this relaxed goal) while asymptotically satisfying the average fp-rate constraint. The mo-
tivation for this approach is as follows. We know that if the constraints are always satis ﬁed, then the
optimal tp-rate in-hindsight is attainable (using relatively simple no-regret algorithms). On the other
hand, when the constraints need to be actively satis ﬁed, we s hould “pay” some penalty in terms of
the attainability of the tp-rate in-hindsight. In our case, we express this penalty in terms of the re-
laxation constant mentioned above. One of the main contributions of this paper is a computationally

2

feasible online algorithm, the Constrained Regret Matching (CRM) algorithm, that attains the posed
performance goal. We note that although we focus in this paper on the online classi ﬁcation problem,
our algorithm can be easily extended to the general case of regret minimization under average cost
constraints.

The paper is structured as follows. In Section 2 we formally deﬁne the online classi ﬁcation problem
and the goal of the meta-algorithm. In Section 3 we present the general problem of constrained
regret minimization, and show that the online classi ﬁcatio n problem is its special case. In Section
4 we deﬁne our relaxed goal in terms of the unconstrained opti mal tp-rate in-hindsight, propose the
CRM algorithm, and show that it can be implemented efﬁcientl y. Section 5 discusses the special
case of two classi ﬁers and corresponding experimental resu lts. We conclude in Section 6 with some
ﬁnal remarks.

2 Online Classiﬁcation

We consider the online binary classi ﬁcation problem from an abstract space to {1, −1}. We are given
m classi ﬁers that map an input instance to the probability tha t the instance belongs to the positive
class. We denote by A = {1, ...m} the set of indices of the classi ﬁers. An online classi ﬁcation meta-
algorithm is an algorithm that combines the outputs of the given classi ﬁers in order to attain a certain
goal, without prior knowledge on the form and statistics of the input, and without prior knowledge
on the performance of the given classi ﬁers. In what follows, we identify the meta-algorithm with an
agent, and use both these notions interchangeably. The time axis is discrete, with index n = 1, 2, ....
At stage n, the following events occur: (i) the input instance is presented to the classi ﬁers (but not
to the agent); (ii) each classi ﬁer a ∈ A outputs fn (a) ∈ [0, 1], which is the probability of the input
to belong to class 1, and simultaneously the agent chooses a classi ﬁer an ; and (iii) the correct label
of the instance, bn ∈ {1, −1}, is revealed.
There are several standard performance metrics of classi ﬁe rs. These include error count, true-
positive rate (which is also termed recall or sensitivity), and false-positive rate (one minus the fp-rate
is usually termed speci ﬁcity ). As discussed in [6], tp-rate and fp-rate metrics have some attractive
properties, such as that they are insensitive to changes in class distribution, and thus we focus on
these metrics in this paper. In the online setting, no training set is given in advance, and therefore
these rates have to be updated online, using the obtained data at each stage. Observe that this
data is expressed in terms of the vector zn , (cid:0){fn (a)}a∈A , bn (cid:1) ∈ [0, 1]m × {−1, 1}. We let
rn = r(an , zn ) , fn (an ) I {bn = 1} and cn = c(an , zn ) , fn (an ) I {bn = 0} denote the reward
and the cost of the agent at time n. Note that rn is the probability that the instance with positive
label at time n will be classi ﬁed correctly by the agent, while cn is the probability that the instance
with negative label will be classi ﬁed incorrectly. Then, ¯βtp (n) , Pn
k=1 rk / Pn
I {bn = 1} and
k=1
¯βf p (n) , Pn
k=1 ck / Pn
I {bn = −1} are the average tp-rate and fp-rate of the agent at time n,
k=1
respectively.
Our aim is to design a meta-algorithm that will have ¯βtp (n) not worse than the tp-rate of the best
convex combination of the m given classi ﬁers (in hindsight), while satisfying ¯βf p (n) ≤ γ , for some
0 < γ < 1 (asymptotically, almost surely, for any possible sequence z1 , z2 , ...). In fact, this problem
is a special case of the regret minimization problem with constraints. In the next section we thus
present the general constrained regret minimization framework, and discuss its applicability to the
case of online classi ﬁcation.

3 Constrained Regret Minimization

3.1 Model Deﬁnition

We consider the problem of an agent facing an arbitrary varying environment. We identify the en-
vironment with some abstract opponent, and therefore obtain a repeated game formulation between
the agent and the opponent. The constrained game is deﬁned by a tuple
(A, Z , r, c, Γ) where A
denotes the ﬁnite set of possible actions of the agent; Z denotes the compact set of possible out-
comes (or actions) of the environment; r : A × Z → R is the reward function; c : A × Z → R`
is the vector-valued cost function; and Γ ⊆ R` is a convex and closed set within which the average

3

cost vector should lie in order to satisfy the constraints. An important special case is that of linear
constraints, that is Γ = (cid:8)c ∈ R` : ci ≤ γi , i = 1, ..., `(cid:9) for some vector γ ∈ R` .
The time axis is discrete, with index n = 1, 2, .... At time step n, the following events occur: (i) The
agent chooses an action an , and the opponent chooses an action zn , simultaneously; (ii) the agent
observes zn ; and (iii) the agent receives a reward rn = r(an , zn ) ∈ R and a cost cn = c(an , zn ) ∈
n Pn
n Pn
k=1 rk and ¯cn , 1
R` . We let ¯rn , 1
k=1 ck denote the average reward and cost of the agent
at time n, respectively. Let Hn , Z n−1 × An−1 denote the set of all possible histories of actions till
time n. At time n, the agent chooses an action an according to the decision rule πn : Hn → ∆(A),
where ∆(A) is the set of probability distributions over the set A. The collection π = {πn }∞
n=1 is the
strategy of the agent. That is, at each time step, a strategy prescribes some mixed action p ∈ ∆(A),
based on the observed history. A strategy for the opponent is deﬁned similarly. We denote the mixed
action of the opponent by q ∈ ∆(Z ), which is the probability density over Z .
In what follows, we will use the shorthand notation r(p, q) , Pa∈A p(a) Rz∈Z q(z )r(a, z )
for the expected reward under mixed actions p ∈ ∆(A) and q ∈ ∆(Z ). The notation
r(a, q), c(p, q), c(p, z ), c(a, q) will be interpreted similarly. We make the following assumption that
the agent can satisfy the constraints in expectation against any mixed action of the opponent.
Assumption 3.1 (Satis ﬁability of Constraints) . For every q ∈ ∆(Z ), there exists p ∈ ∆(A), such
that c(p, q) ∈ Γ.
Assumption 3.1 is essential, since otherwise the opponent can violate the average-cost constraints
simply by playing the corresponding stationary strategy q .
Let ¯qn (z ) , Pn
k=1 δ {z − zk } /n denote the empirical density of the opponent’s actions at time n,
so that ¯qn ∈ ∆(Z ). The optimal reward-in-hindsight is then given by
n
n
n (z1 , ..., zn ) , 1
1
a∈A Zz∈Z
Xk=1
Xk=1
r∗
max
δ {z − zk } = max
r(a, zk ) = max
r(a, z )
n
n
a∈A
a∈A
implying that r∗
n = r∗ ( ¯qn ).
In what follows, we will use the term “reward envelope” in or der
to refer to functions ρ : ∆(Z ) → R. The simplest reward envelope is the (unconstrained) best-
response envelope (BE) ρ = r∗ . The n-stage regret of the algorithm (with respect to the BE) is then
r∗ ( ¯qn ) − ¯rn . The no-regret algorithm must ensure that the regret vanishes as n → ∞ regardless
of the opponent’s actions. However, in our case, in addition to vanishing regret, we need to satisfy
the cost constraints. Obviously, the BE need not be attainable in the presence of constraints, and
therefore other reward envelopes should be considered. Hence, we use the following deﬁnition
(introduced in [11]) in order to assess the online performance of the agent.
Deﬁnition 3.1 (Attainability and No-Regret). A reward envelope ρ : ∆(Z ) → R is Γ-attainable if
there exists a strategy π for the agent such that, almost surely, (i) lim supn→∞ (ρ( ¯qn ) − ¯rn ) ≤ 0 ,
and (ii) limn→∞ d(¯cn , Γ) = 0, for every strategy of the opponent. Here, d(·, Γ) is Euclidean set-to-
point distance. Such a strategy π is called constrainedno-regret strategy with respect to ρ.

r(a, ¯qn ),

A natural extension of the BE to the constrained setting was deﬁned in [11], by noting that if the
agent knew in advance that the empirical distribution of the opponents actions is ¯qn = q , he could
choose the constrained best response mixed action p, which is a solution of the corresponding opti-
mization problem:

r∗
so that c(p, q) ∈ Γ} .
Γ (q) , max
p∈∆(A) {r(p, q) :
We refer to r∗
Γ as the constrained best-response envelope (CBE).
The ﬁrst positive result that appeared in the literature was
that of Shimkin [12], which showed that
the value vΓ , minq∈∆(Z ) r∗
Γ (q) of the constrained game is attainable by the agent. The algorithm
which attains the value is based on Blackwell’s approachability theory [3], and is computationally
efﬁcient provided that vΓ can be computed ofﬂine. Unfortunately, it was shown in [11] t hat r∗
Γ (q)
Γ ), is attainable1 .
itself is not attainable in general. However, the (lower) convex hull of r∗
Γ (q), conv (r∗
Two no-regret algorithms with respect to conv (r∗
Γ ) are suggested in [11]. To our best knowledge,
1The (lower) convex hull of a function f : X → R is the largest convex function which is nowhere larger
than f .

(1)

4

these algorithms are inefﬁcient (i.e., not polynomial); th ese are the only existing constrained no-
regret algorithms in the literature.

It should be noted that the problem that is considered here can not be formulated as an instance of
online convex optimization [13, 9] – see [11] for a discussion on this issue.

3.2 Application to the Online Classi ﬁcation Problem

(2)

For the model described in Section 2, A = {1, ..., m} denotes the set of possible classi ﬁers and Z de-
notes the set of possible outputs of the classi ﬁers and the tr ue labels, that is: z = (cid:0){f (a)}a∈A , b(cid:1) ∈
[0, 1]m × {−1, 1} , Z . The reward at time n is rn = r(an , zn ) = fn (an ) I {bn = 1} and the cost
is cn = c(an , zn ) = fn (an ) I {bn = −1}. Note that in this case, the mixed action of the opponent
q ∈ ∆(Z ) is q(f , b) = q(f |b)q(b), where q(f |b) is the conditional density of the predictions of the
classi ﬁers and q(b) is the probability of the label b. It is easy to check that
r(p, q) = q(1) Xa∈A
p(a)βtp (q ; a),
where βtp (q ; a) , Rf f (a)q(f |1) is the tp-rate of classi ﬁer a under distribution q . Regarding the
cost, the goal is to keep it under a given threshold 0 < γ < 1. Since the regret minimization
framework requires additive rewards and costs, we deﬁne the following modi ﬁed cost funct
ion:
cγ (a, z ) , c(a, z ) − γ I {b = −1} , and similarly to the reward above, we have that
cγ (p, q) = q(−1)  Xa∈A
p(a)βf p (q ; a) − γ! ,
where βf p (q ; a) , Rf q(f | − 1)f (a) is the fp-rate of classi ﬁer a under distribution q . We
note that keeping the average fp-rate of the agent ¯βf p (n) ≤ γ is equivalent
to keeping
(1/n) Pn
k=1 cγ (ak , zk ) ≤ 0.
Since our goal is to keep the fp-rate below γ , some assumption on classi ﬁers should be im-
posed in order to satisfy Assumption 3.1. We assume here that the classi ﬁers’ single-stage false-
In particular, we redeﬁne 2
positive probability is such that it allows satisfying the constraint.
Z , {z = (f , b) ∈ [0, 1]m × {−1, 1} : if b = −1, f (a) ≤ γa } , where 0 ≤ γa ≤ 1, and there
exists a∗ such that γa∗ < γ . Under this assumption, it is clear that for every q ∈ ∆(Z ), there exists
p ∈ ∆(A), such that cγ (p, q) ≤ 0; in fact this p is the probability mass concentrated on a∗ . If
additional prior information is available on the single-stage performance of the given classi ﬁers, this
may be usefully used to further restrict the set Z . For example, we can also restrict z = (f , 1) by
f (a) ≥ λa for some 0 < λa < 1. Such additional restrictions will generally contribute to reduc-
ing the value of the optimal relaxation parameter α∗ (see (7) below). This effect will be explicitly
demonstrated in Section 5.

(3)

(4)

We proceed to compute the BE and CBE. Using (2), the BE is
a∈{1,...,m} {βtp (q ; a)} , q(1)β ∗ (q),
r∗ (q) , max
r(a, q) = q(1) max
a∈A
where β ∗ (q) is the optimal (unconstrained) tp-rate in hindsight under distribution q . Now, using (1),
γ (q), where
(2), and (3) we have that r∗
γ (q) = q(1)β ∗
p∈∆(A) (Xa∈A
p(a)βf p (q ; a) ≤ γ) ,
so that Xa∈A
β ∗
γ (q) , max
is the optimal constrained tp-rate in hindsight under distribution q . Finally, note that the value of the
γ (q) = 0 in this case.
constrained game vγ , minq∈∆(Z ) r∗
As a consequence of this formulation, the algorithms proposed in [11] can be in principle used in
γ . However, given the implementation difﬁculties associate d with
order to attain the convex hull of r∗
these algorithms, we are motivated to examine more carefully the problem of regret minimization
with constraints and provide more practical no-regret algorithms with formal guarantees.

p(a)βtp (q ; a) :

(5)

2This assumption can always be satis ﬁed by adding a
ﬁctitious classiﬁer a0 that always outputs a ﬁxed
f (a0 ) < γ , irrespectively of the data. However, such an addition might adversely affect the value of the
optimal relaxation parameter α∗ (see (7) below), and should be avoided if possible.

5

4 Constrained Regret Matching

We next deﬁne a relaxed reward envelope for the online classi
ﬁcation problem. The proposed is in
fact applicable to the problem of constrained regret minimization in general. However, due to space
limitation, we present it directly for our classi ﬁcation pr oblem.

(6)

(7)

Our starting point here in deﬁning an attainable reward enve lope will be the BE r∗ (q) = q(1)β ∗ (q).
Clearly, r∗ is in general not attainable in the presence of fp-constraints, and we thus consider a
α is a convex
α (q) , q(1)(β ∗ (q) − α). Obviously, r∗
relaxed version thereof. For α ≥ 0, set r∗
function, and we can always pick α ≥ 0 large enough, such that r∗
α is attainable. Furthermore, recall
that the value vγ of the constrained game is attainable by the agent. Observe that, generally, r∗
α (q)
can be smaller than vγ = 0. We thus introduce the following modi ﬁcation:
rSR
α (q) , q(1) max {0, β ∗ (q) − α} .
α as the scalar-relaxed best-response envelope (SR-BE). Now, let3
We refer to rSR
q∈∆(Z ) (cid:0)β ∗ (q) − β ∗
α∗ , max
γ (q)(cid:1) .
We note that rSR
α∗ (q) is strictly above 0 at some point, unless the game is in some sense trivial (see
the supplementary material for a proof). According to Deﬁni
tion 3.1, we are seeking for a strategy
π that is: (i) an α-relaxed no-regret strategy for the average reward, and (ii) ensures that the cost
constraints are asymptotically satis ﬁed. Thus, at each tim e step, we need to balance between the
need of maximizing the average tp-rate and satisfying the average fp-rate constraint. Below we
propose an algorithm which solves this trade-off for α ≥ α∗ .
We introduce some further notation. Let
Rα
(8)
k (a) , [fk (a) − fk (ak ) − α] I {bk = 1} , a ∈ A, Lk , cγ (ak , zk ),
denote the instantaneous α-regret and the instantaneous constraint violation (respectively) at time
k . We have that the average α-regret and constraints violation at time n are
α
n (a) = ¯qn (1) (cid:2)βtp ( ¯qn ; a) − ¯βtp (n) − α(cid:3) , a ∈ A; Ln = ¯qn (0)[ ¯βf p (n) − γ ].
(9)
R
Using this notation, the Constrained Regret Matching (CRM) algorithm is given in Algorithm 1. We
then have the following result.
Theorem 4.1. Suppose that the CRM algorithm is applied with parameter α ≥ α∗ , where α∗ is
given in (7). Then, under Assumption 3.1, it attains rSR
α (6) in the sense of Deﬁnition 3.1. That is, (i)
¯βf p (n) ≤ 0,
lim inf n→∞ (cid:0) ¯βtp (n) − max {0, maxa∈A βtp ( ¯qn ; a) − α}(cid:1) ≥ 0 , and (ii) lim supn→∞
for every strategy of the opponent, almost surely.
The proof of this Theorem is based on Blackwell’s approachability theory [3], and is given in the
supplementary material. We note that the mixed action required by the CRM algorithm always
exists provided that α ≥ α∗ . It can be easily shown (see the supplementary material) that whenever
Pa∈A hR
n−1 (a)i+
α
> 0, this action can be computed by solving the following linear program:
p∈Bn Xa∈A:pα
(pα
n (a) − p(a)) ,
min
n (a)>p(a)
where Bn , np ∈ ∆(A) : (cid:2)Ln−1 (cid:3)+ (cid:0)Pa0∈A p(a0 )f (a0 ) − γ (cid:1) ≤ 0, ∀z = (f , −1) ∈ Z o and
/ Pa0∈A hR
n (a) = hR
n−1 (a)i+
n−1 (a0 )i+
α
α
is the α-regret matching strategy. Note also that
pα
when the average constraints violation Ln−1 is non-positive, the minimum in (10) is obtained by
n−1 (a)i+
n . Finally, when Pa∈A hR
α
= 0, any action p ∈ Bn can be chosen. It is worth men-
p = pα
tioning that our algorithm, and in particular the program (10), can not be formulated in the Online
Convex Programming (OCP) framework [13, 9], since the equivalent reward functions in our case
are trajectory-dependent, while in the OCP it is assumed that these functions are arbitrary, but ﬁxed
(i.e., they should not depend on the agent’s actions).

(10)

3 In general, the parameter α∗ may be difﬁcult to compute analytically. See the supplementary material for
a discussion on computational aspects. Also, in the supplementary material we propose an adaptive algorithm
which avoids this computation (see a remark at the end of Section 4). Finally, in Section 5 we show that in the
case of two classi ﬁers this computation is trivial.

6

Algorithm 1 CRM Algorithm
Parameter: α ≥ 0.
Initialization: At time n = 0 use arbitrary action a0 .
At times n = 1, 2, ... ﬁnd a mixed action p ∈ ∆(A) such that

Pa∈A hR
n−1 (a)i+ (cid:0)f (a) − Pa0∈A p(a0 )f (a0 ) − α(cid:1) ≤ 0,
α
(cid:2)Ln−1 (cid:3)+ (cid:0)Pa0∈A p(a0 )f (a0 ) − γ (cid:1) ≤ 0,

α
n (a) and Ln,i are given in (9). Draw classi ﬁer an from p.
where R

∀z = (f , 1) ∈ Z ,
∀z = (f , −1) ∈ Z ,

(11)

Remark. In practice, it may be possible to attain rSR
α with α < α∗ if the opponent is not entirely
adversarial. In order to capitalize on this possibility, an adaptive algorithm can be used that adjusts
the value of α online. The idea is to start from some small initial value α0 ≥ 0 (possibly α0 = 0).
At each time step n, we would like to use a parameter α = αn for which inequality (11) can be
satis ﬁed. This inequality is always satis ﬁed when
α ≥ α∗ . If however α < α∗ , the inequality may
or may not be satis ﬁed. In the latter case, α can be increased so that the condition is satis ﬁed. In
addition, once in a while, α can be reset to α0 , in order to obtain better results. In the supplementary
material we further discuss the adaptive scheme, and prove a convergence rate for it. We note that
the adaptive scheme does not require the computation of the optimal α∗ , as it discovers it online.

5 The Special Case of Two Classiﬁers

α =

.

βtp (q ; 1),

βtp (q ; 2),

γ−βf p (q ;2)
βf p (q ;1)−βf p (q ;2) βtp (q ; 1) +

βf p (q ;1)−γ
βf p (q ;1)−βf p (q ;2) βtp (q ; 2),

r∗
γ (q) = q(1)

if βtp (q ; 1) > βtp (q ; 2)
and βf p (q ; 1) > γ ,
if βtp (q ; 1) > βtp (q ; 2)
and βf p (q ; 1) ≤ γ ,
otherwise.

If m = 2, we can obtain explicit expressions for the reward envelopes and for the algorithm. In
particular, we have two classi ﬁers, and we assume that the ou tputs of these classi ﬁers lie in the set
Z , (cid:8)z ∈ (f , b) ∈ [0, 1]2 × {−1, 1} : if b = −1, f (1) ≤ γ1 , f (2) ≤ γ2 ; if b = 1, f (2) ≥ λ(cid:9) such
that γ1 > γ , γ2 < γ , and λ ≥ 0. Observe that under this assumption, classi ﬁer 2 has one-stage per-
formance guarantees that will allow to obtain better guarantees of the meta-algorithm. By computing
explicitly the CBE, we obtain


Therefore, the relaxation parameter is
(βf p (q ; 1) − γ )(cid:27) =
q : βtp (q ;1)>βtp (q ;2),βf p (q ;1)>γ (cid:26) βtp (q ; 1) − βtp (q ; 2)
(1 − λ)(γ1 − γ )
max
γ1 − γ2
βf p (q ; 1) − βf p (q ; 2)
Finally, it is easy to check using (10) that Algorithm 1 reduces in this case to the following sim-
ple rule: (i) if Pa∈A hR
> 0, choose p(1) = min npα
n−1 (a)i+
γ1−γ2 o, where pα
α
n (1), γ−γ2
n (1) =
/ Pa∈A hR
n−1 (a)i+
n−1 (1)i+
hR
α
α
denotes the α-regret matching strategy; (ii) otherwise, choose
arbitrary action with p(1) ≤ γ−γ2
.
γ1−γ2
We simulated the CRM algorithm with the following parameters: γ = 0.3, γ1 = 0.4, γ2 = 0.2, λ =
0.7. This gives the relaxation parameter of α = 0.15. Half of the input instances were positives and
the other half were negatives (on average). The time was divided into episodes with exponentially
growing lengths. At each odd episode, both classi ﬁers had similar tp-rate and both of the m satis ﬁed
the constraints, while in each even episode, classi ﬁer 1 was perfect in positives’ classi ﬁcation, but did
not satisfy the constraints. The results are shown in Figure 1. We compared the performance of the
CRM algorithm to a simple unconstrained no-regret algorithm that treats both the true-positive and
false positive probabilities similarly, but with different weight. In particular, the reward at stage n of
this algorithm is gn (w) = fn (an ) I {bn = 1} − wfn (an ) I {bn = −1} for some weight parameter

7

tp-rate

fp-rate

w = 1.1

w = 1.3

γ =

w = 1.33

w = 1.4

n

w = 1.1

w = 1.3

w = 1.33

w = 1.4

n

β ∗ ( ¯qn )

γ ( ¯qn )
β ∗

CRM

NR(w)

Figure 1: Experimental results for the case of two classi ﬁer s.

w ≥ 0. Given w , this is simply a no-regret algorithm with respect to gn (w). When w = 0, the
algorithm performs tp-rate maximization, while if w is large, it performs fp-rate minimization. We
call this algorithm NR(w). As can be seen from Figure 1, the CRM algorithm outperforms NR(w)
for any ﬁxed parameter w . For w = 1.1, NR(w) has a better tp-rate, but the fp-rate constraint is
violated most of the time. For w = 1.4, the constraints are always satis ﬁed, but the tp-rate is alw ays
dominated by that of the CRM algorithm. For w = 1.3, 1.33 it can be seen that the constraints are
satis ﬁed (or almost satis ﬁed), but the tp-rate is usually do minated by that of the CRM algorithm.

6 Conclusion

We studied regret minimization with average-cost constraints, with the focus on computationally
feasible algorithm for the special case of online classi ﬁca tion problem with speci ﬁcity constraints.
We deﬁned a relaxed version of the best-response reward enve lope and showed that it can be attained
by the agent while satisfying the constraints, provided that the relaxation parameter is above a certain
threshold. A polynomial no-regret algorithm was provided. This algorithm generally solves a linear
program at each time step, while in some special case the algorithm’s mixed action reduces to the
simple α-regret matching strategy. To the best of our knowledge, this is the ﬁrst algorithm that
addresses the problem of the average tp-rate maximization under average fp-rate constraints in the
online setting.
In addition, an adaptive scheme that adapts the relaxation parameter online was
brieﬂy discussed. Finally, the special case of two classi ﬁe
rs was discussed, and the experimental
results for this case show that our algorithm outperforms a simple no-regret algorithm which takes
as the reward function a weighted sum of the tp-rate and fp-rate.

Some remarks about our algorithm and results follow. First, the guaranteed convergence rate of
the algorithm is of O(1/√n) since it is based on Blackwell’s approachability theorem4 . Second,
additional constraints can be easily incorporated in the presented framework, since the general regret
minimization framework assumes a vector of constraints. Third, it seems that there is an inherent
trade-off between complexity and performance in the studied problem. In particular, in case of a
single constraint, the maximal attainable relaxed goal is the convex hull of the CBE (see [11]) but no
polynomial algorithms are known that attain this goal. Our results show that, by further relaxing the
goal, it is possible to devise attaining polynomial algorithms. Finally, we note that the assumption
on the single-stage fp-rates of the classi ﬁers can be weaken ed by assuming that, in each sufﬁciently
large period of time, the average fp-rate of each classi ﬁer a is bounded by γa . Our approach and
results can be then extended to this case, by treating each such period as a single stage.

4A straightforward application of this theorem also gives √m dependence of the rate on the number of clas-
siﬁers. We note that it is possible to improve the dependence to log(m) by using a potential based Blackwell’s
approachability strategy (see for example [4], Chapter 7.8)

8

References

[1] Y. Amit, S. Shalev-Shwartz, and Y. Singer. Online classi ﬁcation for complex problems using
simultaneous projections. In NIPS 2006.
[2] D. Blackwell. Controlled random walks.
In Proceedings of the International Congress of
Mathematicians, volume III, pages 335–338, 1954.
[3] D. Blackwell. An analog of the minimax theorem for vector payoffs. Paci ﬁc Journal of Math-
ematics, 6:1–8, 1956.
[4] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University
Press, New York, NY, USA, 2006.
[5] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive
algorithms. Journal of Machine Learning Research, 7:551–585, 2006.
[6] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861–874,
2006.
[7] Y. Freund and R.E. Schapire. Adaptive game playing using multiplicative weights. Games and
Economic Behavior, 29(12):79–103, 1999.
[8] J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of
Games, 3:97–139, 1957.
[9] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimiza-
tion. Machine Learning, 69(2-3):169–192, 2007.
[10] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Com-
putation, 108(2):212–261, 1994.
[11] S. Mannor, J. N. Tsitsiklis, and J. Y. Yu. Online learning with sample path constraints. Journal
of Machine Learning Research, 10:569–590, 2009.
[12] N. Shimkin. Stochastic games with average cost constraints. Annals of the International
Society of Dynamic Games, Vol. 1: Advances in Dynamic Games and Applications, 1994.
[13] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML ’03), pages
928–936, 2003.

9


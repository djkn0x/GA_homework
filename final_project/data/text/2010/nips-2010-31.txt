Variational Inference over Combinatorial Spaces

Alexandre Bouchard-C ˆot ´e∗ Michael I. Jordan∗,†
∗Computer Science Division
†Department of Statistics
University of California at Berkeley

Abstract

Since the discovery of sophisticated fully polynomial randomized algorithms for a range of
#P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces
has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guar-
antees, the slow running time of many of these randomized algorithms and the restrictive
assumptions on the potentials have hindered the applicability of these algorithms to ma-
chine learning. Because of this, in applications to combinatorial spaces simple exact mod-
els are often preferred to more complex models that require approximate inference [4].
Variational inference would appear to provide an appealing alternative, given the success
of variational methods for graphical models [5]; unfortunately, however, it is not obvious
how to develop variational approximations for combinatorial objects such as matchings,
partial orders, plane partitions and sequence alignments. We propose a new framework that
extends variational inference to a wide range of combinatorial spaces. Our method is based
on a simple assumption: the existence of a tractable measure factorization, which we show
holds in many examples. Simulations on a range of matching models show that the algo-
rithm is more general and empirically faster than a popular fully polynomial randomized
algorithm. We also apply the framework to the problem of multiple alignment of protein
sequences, obtaining state-of-the-art results on the BAliBASE dataset [6].

Introduction
1
not, and suppose that the goal is to compute P
The framework we propose is applicable in the following setup: let C denote a combinatorial space,
by which we mean a ﬁnite but large set, where testing membership is tractable, but enumeration is
x∈C f (x), where f is a positive function. This setup
subsumes many probabilistic inference and classical combinatorics problems. It is often intractable
to compute this sum, so approximations are used.
P
We approach this problem by exploiting a ﬁnite collection of sets {Ci } such that C = ∩iCi . Each Ci
is larger than C , but paradoxically it is often possible to ﬁnd such a decomposition where for each i,
f (x) is tractable. We give many examples of this in Section 3 and Appendix B.1 This paper
x∈Ci
describes an effective way of using this type of decomposition to approximate the original sum.
Another way of viewing this setup is in terms of exponential families. In this view, described in
detail in Section 2, the decomposition becomes a factorization of the base measure. As we will
show, the exponential family view gives a principled way of deﬁning variational approximations.
In order to make variational approximations tractable in the combinatorial setup, we use what we
call an implicit message representation. The canonical parameter space of the exponential family
enables such representation. We also show how additional approximations can be introduced in
cases where the factorization has a large number of factors. These further approximations rely on
an outer bound of the partition function, and therefore preserve the guarantees of convex variational
objective functions.
While previous authors have proposed mean ﬁeld or loopy belief propagation algorithms to approx-
imate the partition function of a few speciﬁc combinatorial models—for example [7, 8] for parsing,

1The appendices can be found in the supplementary material.

1

and [9, 10] for computing the permanent of a matrix—we are not aware of a general treatment of
variational inference in combinatorial spaces.
There has been work on applying variational algorithms to the problem of maximization over combi-
natorial spaces [11, 12, 13, 14], but maximization over combinatorial spaces is rather different than
summation. For example, in the bipartite matching example considered in both [13] and this paper,
there is a known polynomial algorithm for maximization, but not for summation. Our approach
is also related to agreement-based learning [15, 16], although agreement-based learning is deﬁned
within the context of unsupervised learning using EM, while our framework is agnostic with respect
to parameter estimation.
The paper is organized as follows: in Section 2 we present the measure factorization framework; in
Section 3 we show examples of this framework applied to various combinatorial inference problems;
and in Section 4 we present empirical results.

2 Variational measure factorization

In this section, we present the variational measure factorization framework. At a high level, the ﬁrst
step is to construct an equivalent but more convenient exponential family. This exponential family
will allow us to transform variational algorithms over graphical models into approximation algo-
rithms over combinatorial spaces. We ﬁrst describe the techniques needed to do this transformation
in the case of a speciﬁc variational inference algorithm—loopy belief propagation—and then discuss
mean-ﬁeld and tree-reweighted approximations.
To make the exposition more concrete, we use the running example of approximating the value and
gradient of the log-partition function of a Bipartite Matching model (BM) over KN ,N , a well-known
#P problem [17]. Unless we mention otherwise, we will consider bipartite perfect matchings; non-
bipartite and non-perfect matchings are discussed in Section 3.1. The reader should keep in mind,
however, that our framework is applicable to a much broader class of combinatorial objects. We
develop several other examples in Section 3 and in Appendix B.

2.1 Setup

Since we are dealing with discrete-valued random variables X , we can assume without loss of
generality that the probability distribution for which we want to compute the partition function and
X
X
moments is a member of a regular exponential family with canonical parameters θ ∈ RJ :
P(X ∈ B ) =
exp{hφ(x), θi − A(θ)}ν (x), A(θ) = log
exp{hφ(x), θi}ν (x),
x∈B
x∈X
for a J -dimensional sufﬁcient statistic φ and base measure ν over F = 2X , both of which are
of computing P
assumed (again, without loss of generality) to be indicator functions : φj , ν : X → {0, 1}. Here
X is a superset of both C and all of the Ci s. The link between this setup and the general problem
x∈C f (x) is the base measure ν , which is set to the indicator function over C :
ν (x) = 1[x ∈ C ], where 1[·] is equal to one if its argument holds true, and zero otherwise.
The goal is to approximate A(θ) and ∇A(θ) (recall that the j -th coordinate of the gradient, ∇j A, is
measures ν (x) = QI
equal to the expectation of the sufﬁcient statistic φj under the exponential family with base measure
function assumed to be tractable: Ai (θ) = log P
ν [5]). We want to exploit situations where the base measure can be written as a product of I
i=1 νi (x) such that each factor νi : X → {0, 1} induces a super-partition
x∈X exp{hφ(x), θi}νi (x). This computation is
typically done using dynamic programming (DP). We also assume that the gradient of the super-
partition functions is tractable, which is typical for DP formulations.
the space X is a product of N 2 binary alignment variables, x =
In the case of BM,
x1,1 , x1,2 , . . . , xN ,N . In the Standard Bipartite Matching formulation (which we denote by SBM),
the sufﬁcient statistic takes the form φj (x) = xm,n . The measure factorization we use to enforce
NX
NY
NX
NY
the matching property is ν = ν1 ν2 , where:
m=1
n=1
n=1
m=1

xm,n ≤ 1],

xm,n ≤ 1].

ν2 (x) =

1[

ν1 (x) =

1[

(1)

(2)

2

BPMF(θ , A1 , . . . , AI )
i = θ + P
1: ζ (1)
i,j = 0
(cid:16)¯ξ(t)
(cid:16)∇Ai
(cid:17)(cid:17) − ¯ξ(t)
2: for t = 1, 2, . . . , T do
i0 :i0 6=i ζ (t−1)
¯ξ(t)
3:
i0
(cid:16)
(cid:17)
θ + P
ζ (t)
i = logit
4:
i
i
5: end for
6: return ˆµ = logistic
i ζ (T )
i
Figure 1: Left: the bipartite graphical model used for the MRF construction described in Section 2.2. Right:
pseudocode for the BPMF algorithm. See Section 2 and Appendix A.2 for the derivation.

We show in Appendix A.3 that A1 and A2 can be computed in time O(N 2 ) for the SBM.
The last assumption we make is that given a vector s ∈ RJ , there is at most one possible conﬁgu-
ration x with φ(x) = s. We call this the rich sufﬁcient statistic condition. Since we are concerned
in this framework with computing expectations, not with parameter estimation, this can be done
without loss of generality. For example, if the original exponential family is curved (e.g., by param-
eter tying), for the purpose of computing expectations one can always work in the over-complete
parameterization, and then project back to the coarse sufﬁcient statistic for parameter estimation.

2.2 Markov random ﬁeld reformulation

We start by constructing an equivalent but more convenient exponential family. This general con-
struction has an associated bipartite Markov Random Field (MRF) with structure KI ,J , shown in
Figure 1. This new bipartite structure should not be confused with the bipartite graph from the
KN ,N bipartite graph speciﬁc to the BM example: the former is part of the general theory, the latter
is speciﬁc to the bipartite matching example.
The bipartite MRF has I random variables in the ﬁrst graph component, B1 , . . . , BI , each having a
copy of X as its domain. In the second component, the graph has J random variables, S1 , . . . , SJ ,
where Sj has a binary domain {0, 1}. The pairwise potential between an event {Bi = x} in the ﬁrst
component and one {Sj = s} in the second is given by Ψi,j (x, s) = 1[φj (x) = s]. The following
one-node potentials are also included: Ψi (x) = νi (x) and Ψj (s) = eθj s .
The equivalence between the two formulations follows from the rich sufﬁcient statistic condition,
(cid:26) 1 if x1 = x2 = · · · = xI
IY
· · · X
X
X
JY
which implies (for a full proof of the equivalence, see Appendix A.1):
0 otherwise.
s1∈{0,1}
s2∈{0,1}
sJ ∈{0,1}
i=1
j=1

1[φj (xi ) = sj ] =

(3)

This transformation into an equivalent MRF reveals several possible variational approximations.
We show in the next section how loopy belief propagation [18] can be modiﬁed to tractably accom-
modate this transformed exponential family, even though some nodes in the graphical model—the
Bi s—have a domain of exponential size. We then describe similar updates for mean ﬁeld [19] and
tree-reweighted [20] variational algorithms. We will refer to these algorithms as BPMF (Belief Prop-
agation on Measure Factorizations), MFMF (Mean Field on Measure Factorizations) and TRWMF
(Tree-Reweighted updates on Measure Factorizations). In contrast to BPMF, MFMF is guaranteed
to converge2 , and TRWBF is guaranteed to provide an upper bound on the partition function.3

2.3

Implicit message representation

The variables Bi have a domain of exponential size, hence if we applied belief propagation updates
naively, the messages going from Bi to Sj would require summing over an exponential number of
terms, and messages going from Sj to Bi would require an exponential amount of storage. To avoid
summing explicitly over exponentially many terms, we adapt an idea from [7] and exploit the fact

2Although we did not have convergence issues with BPMF in our experiments.
3Surprisingly, MFMF does not provide a lower bound (see Appendix A.6).

3

B1BIS1S2SJ.........familyiscurvedbyparametertying,forthepurposeofcomputingexpectationsonecanalwaysworkintheover-completeparameterization,andthenprojectbacktothecoarsesufﬁcientstatisticforparameterestimation.2.2MarkovrandomﬁeldreformulationWestartbyconstructinganequivalentbutmoreconvenientexponentialfamily.Thisgeneralcon-structionhasanassociatedbipartiteMarkovRandomField(MRF)withstructureKI,J.ThisnewbipartitestructureshouldnotbeconfusedwiththebipartitegraphfromtheKM,NbipartitegraphspeciﬁctotheBMexample:theformerispartofthegeneraltheory,andisnotspeciﬁctothebipartitematchingexample.ThebipartiteMRFhasIrandomvariablesintheﬁrstgraphcomponent,B1,...,BI,eachhavingacopyofXasitsdomain.Inthesecondcomponent,thegraphhasJrandomvariables,S1,...,SJ,whereSjhasabinarydomain{0,1}.Thepairwisepotentialsbetweenanevent(Bi=x)intheﬁrstcomponentandone(Sj=s)inthesecondisgivenbyΨi,j(x,s)=1[φj(x)=s].Thefollowingonenodepotentialsarealsoincluded:Ψi(x)=νi(x)andΨj(s)=eθjs.Theequivalencebetweenthetwoformulationsfollowsfromtherichsufﬁcientstatisticcondition,whichimplies:Xs1∈{0,1}Xs2∈{0,1}···XsJ∈{0,1}IYi=1JYj=11[φj(xi)=sj]=1ifx1=x2=···=xI0otherwise.ThistransformationintoanequivalentMRFrevealsseveralpossiblevariationalapproximations.WeshowinthenextsectionhowloopyBPupdates[14]canbedeﬁnedoverthisgraph,eventhoughsomenodesinthisgraph—theBis—havedomainsofexponentialsize.Wethendescribetheupdatesformeanﬁeld[15]andTRW[16].IncontrasttoBP,thesealgorithmscanprovidedboundsonthepartitionfunction.2.3ImplicitmessagerepresentationThevariablesBihaveexponentialsizedomains,henceifweappliedBPupdatesnaively,themes-sagesgoingfromBitoSjwouldrequiresummingoveranexponentialnumberofterms,andmes-sagesgoingfromSjtoBiwouldrequireanexponentialamountofstorage.Toavoidsummingexplicitlyoverexponentiallymanyterms,weuseatechniqueinspiredby[7]andexploitthefactthatanefﬁcientalgorithmisassumedforcomputingthesuper-partitionfunctionAianditsderiva-tives.ToavoidtheexponentialstorageofmessagesgoingtoBi,weuseanimplicitrepresentationofthesemessagesinthecanonicalparameterspace.LetusdenotethemessagesgoingfromSjtoBibyMj→i(s),s∈{0,1}andthereversemessages,mi→j(x),x∈X.UsingthedeﬁnitionsofΨi,j,Ψi,Ψj,thestandardBPupdatesbecome:mi→j(s)∝Xx∈X1[φj(x)=s]νi(x)Yj!:j!#=jMj!→i(x)Mj→i(x)∝Xs∈{0,1}eθjs1[φj(x)=s]Yi!:i!#=imi!→j(s).(3)ThetaskistogetanupdateequationthatdoesnotrepresentMj→i(x)explicitly,byexploitingthefactthatthesuper-partitionfunctionsAiandtheirderivativescanbecomputedefﬁciently.Todoso,itisconvenienttousethefollowingequivalentrepresentationforthemessagesmi→j(s):ζi,j=logmi→j(1)mi→j(0)∈[−∞,+∞].Ifwealsoletfi,j(x)denoteanyfunctionproportionalto!j!:j!"=jMj!→i(x),wecanwrite:ζi,j=log„Px∈Xφj(x)fi,j(x)νi(x)Px∈X(1−φj(x))fi,j(x)νi(x)«=logit„Px∈Xφj(x)fi,j(x)νi(x)Px∈Xfi,j(x)νi(x)«,(4)3familyiscurvedbyparametertying,forthepurposeofcomputingexpectationsonecanalwaysworkintheover-completeparameterization,andthenprojectbacktothecoarsesufﬁcientstatisticforparameterestimation.2.2MarkovrandomﬁeldreformulationWestartbyconstructinganequivalentbutmoreconvenientexponentialfamily.Thisgeneralcon-structionhasanassociatedbipartiteMarkovRandomField(MRF)withstructureKI,J.ThisnewbipartitestructureshouldnotbeconfusedwiththebipartitegraphfromtheKM,NbipartitegraphspeciﬁctotheBMexample:theformerispartofthegeneraltheory,andisnotspeciﬁctothebipartitematchingexample.ThebipartiteMRFhasIrandomvariablesintheﬁrstgraphcomponent,B1,...,BI,eachhavingacopyofXasitsdomain.Inthesecondcomponent,thegraphhasJrandomvariables,S1,...,SJ,whereSjhasabinarydomain{0,1}.Thepairwisepotentialsbetweenanevent(Bi=x)intheﬁrstcomponentandone(Sj=s)inthesecondisgivenbyΨi,j(x,s)=1[φj(x)=s].Thefollowingonenodepotentialsarealsoincluded:Ψi(x)=νi(x)andΨj(s)=eθjs.Theequivalencebetweenthetwoformulationsfollowsfromtherichsufﬁcientstatisticcondition,whichimplies:Xs1∈{0,1}Xs2∈{0,1}···XsJ∈{0,1}IYi=1JYj=11[φj(xi)=sj]=1ifx1=x2=···=xI0otherwise.ThistransformationintoanequivalentMRFrevealsseveralpossiblevariationalapproximations.WeshowinthenextsectionhowloopyBPupdates[14]canbedeﬁnedoverthisgraph,eventhoughsomenodesinthisgraph—theBis—havedomainsofexponentialsize.Wethendescribetheupdatesformeanﬁeld[15]andTRW[16].IncontrasttoBP,thesealgorithmscanprovidedboundsonthepartitionfunction.2.3ImplicitmessagerepresentationThevariablesBihaveexponentialsizedomains,henceifweappliedBPupdatesnaively,themes-sagesgoingfromBitoSjwouldrequiresummingoveranexponentialnumberofterms,andmes-sagesgoingfromSjtoBiwouldrequireanexponentialamountofstorage.Toavoidsummingexplicitlyoverexponentiallymanyterms,weuseatechniqueinspiredby[7]andexploitthefactthatanefﬁcientalgorithmisassumedforcomputingthesuper-partitionfunctionAianditsderiva-tives.ToavoidtheexponentialstorageofmessagesgoingtoBi,weuseanimplicitrepresentationofthesemessagesinthecanonicalparameterspace.LetusdenotethemessagesgoingfromSjtoBibyMj→i(s),s∈{0,1}andthereversemessages,mi→j(x),x∈X.UsingthedeﬁnitionsofΨi,j,Ψi,Ψj,thestandardBPupdatesbecome:mi→j(s)∝Xx∈X1[φj(x)=s]νi(x)Yj!:j!#=jMj!→i(x)Mj→i(x)∝Xs∈{0,1}eθjs1[φj(x)=s]Yi!:i!#=imi!→j(s).(3)ThetaskistogetanupdateequationthatdoesnotrepresentMj→i(x)explicitly,byexploitingthefactthatthesuper-partitionfunctionsAiandtheirderivativescanbecomputedefﬁciently.Todoso,itisconvenienttousethefollowingequivalentrepresentationforthemessagesmi→j(s):ζi,j=logmi→j(1)mi→j(0)∈[−∞,+∞].Ifwealsoletfi,j(x)denoteanyfunctionproportionalto!j!:j!"=jMj!→i(x),wecanwrite:ζi,j=log„Px∈Xφj(x)fi,j(x)νi(x)Px∈X(1−φj(x))fi,j(x)νi(x)«=logit„Px∈Xφj(x)fi,j(x)νi(x)Px∈Xfi,j(x)νi(x)«,(4)3familyiscurvedbyparametertying,forthepurposeofcomputingexpectationsonecanalwaysworkintheover-completeparameterization,andthenprojectbacktothecoarsesufﬁcientstatisticforparameterestimation.2.2MarkovrandomﬁeldreformulationWestartbyconstructinganequivalentbutmoreconvenientexponentialfamily.Thisgeneralcon-structionhasanassociatedbipartiteMarkovRandomField(MRF)withstructureKI,J.ThisnewbipartitestructureshouldnotbeconfusedwiththebipartitegraphfromtheKM,NbipartitegraphspeciﬁctotheBMexample:theformerispartofthegeneraltheory,andisnotspeciﬁctothebipartitematchingexample.ThebipartiteMRFhasIrandomvariablesintheﬁrstgraphcomponent,B1,...,BI,eachhavingacopyofXasitsdomain.Inthesecondcomponent,thegraphhasJrandomvariables,S1,...,SJ,whereSjhasabinarydomain{0,1}.Thepairwisepotentialsbetweenanevent(Bi=x)intheﬁrstcomponentandone(Sj=s)inthesecondisgivenbyΨi,j(x,s)=1[φj(x)=s].Thefollowingonenodepotentialsarealsoincluded:Ψi(x)=νi(x)andΨj(s)=eθjs.Theequivalencebetweenthetwoformulationsfollowsfromtherichsufﬁcientstatisticcondition,whichimplies:Xs1∈{0,1}Xs2∈{0,1}···XsJ∈{0,1}IYi=1JYj=11[φj(xi)=sj]=1ifx1=x2=···=xI0otherwise.ThistransformationintoanequivalentMRFrevealsseveralpossiblevariationalapproximations.WeshowinthenextsectionhowloopyBPupdates[14]canbedeﬁnedoverthisgraph,eventhoughsomenodesinthisgraph—theBis—havedomainsofexponentialsize.Wethendescribetheupdatesformeanﬁeld[15]andTRW[16].IncontrasttoBP,thesealgorithmscanprovidedboundsonthepartitionfunction.2.3ImplicitmessagerepresentationThevariablesBihaveexponentialsizedomains,henceifweappliedBPupdatesnaively,themes-sagesgoingfromBitoSjwouldrequiresummingoveranexponentialnumberofterms,andmes-sagesgoingfromSjtoBiwouldrequireanexponentialamountofstorage.Toavoidsummingexplicitlyoverexponentiallymanyterms,weuseatechniqueinspiredby[7]andexploitthefactthatanefﬁcientalgorithmisassumedforcomputingthesuper-partitionfunctionAianditsderiva-tives.ToavoidtheexponentialstorageofmessagesgoingtoBi,weuseanimplicitrepresentationofthesemessagesinthecanonicalparameterspace.LetusdenotethemessagesgoingfromSjtoBibyMj→i(s),s∈{0,1}andthereversemessages,mi→j(x),x∈X.UsingthedeﬁnitionsofΨi,j,Ψi,Ψj,thestandardBPupdatesbecome:mi→j(s)∝Xx∈X1[φj(x)=s]νi(x)Yj!:j!#=jMj!→i(x)Mj→i(x)∝Xs∈{0,1}eθjs1[φj(x)=s]Yi!:i!#=imi!→j(s).(3)ThetaskistogetanupdateequationthatdoesnotrepresentMj→i(x)explicitly,byexploitingthefactthatthesuper-partitionfunctionsAiandtheirderivativescanbecomputedefﬁciently.Todoso,itisconvenienttousethefollowingequivalentrepresentationforthemessagesmi→j(s):ζi,j=logmi→j(1)mi→j(0)∈[−∞,+∞].Ifwealsoletfi,j(x)denoteanyfunctionproportionalto!j!:j!"=jMj!→i(x),wecanwrite:ζi,j=log„Px∈Xφj(x)fi,j(x)νi(x)Px∈X(1−φj(x))fi,j(x)νi(x)«=logit„Px∈Xφj(x)fi,j(x)νi(x)Px∈Xfi,j(x)νi(x)«,(4)3wherelogit(x)=logx−log(1−x).Thismeansthatifwecanﬁndaparametervectorξi,j∈Rdsuchthatfi,j(x)=exp!φ(x),ξi,j"∝Yj!:j!!=jMj!→i(x),thenwecouldwrite1ζi,j=logit!∇jAi(ξi,j)".Wederivesuchavectorξi,jasfollows:Yj!:j!!=jMj!→i(x)=Yj!:j!!=jXsj!∈{0,1}eθj!sj!1[φj!(x)=sj!]Yi!:i!!=imi!→j!(sj!)=Yj!:j!!=jeθj!φj!(x)Yi!:i!!=imi!→j!(φj!(x))∝exp8<:Xj!:j!!=jφj!(x)0@θj!+Xi!:i!!=iζi!,j!1A9=;.Therequiredparametersaretherefore:!ξi,j"j!=1[j$=j!]#θj!+$i!:i!"=iζi!,j!%.2.4ReuseofpartitionfunctioncomputationsNaively,theupdatesderivedsofarwouldrequirecomputingJtimeseachsuper-partitionfunctionAiateachmessagepassingiteration.WeshowthatthiscanbereducedtocomputingeachAionlyonceperiteration,aconsiderablegain.Weﬁrstdeﬁnethevectors:¯ξi=θ+Xi!:i!!=iζi!,andthenrewritethenumeratorinsidethelogitfunctioninEquation(4)asfollows:Xx∈Xφj(x)fi,j(x)νi(x)=Xs∈{0,1}Xx:φj(x)=sexp!φ(x),¯ξi"e−¯ξi,jssνi(x)=e−¯ξi,j∇jAi(¯ξi),andsimilarityforthedenominator:Xx∈Xfi,j(x)νi(x)=e−¯ξi,j∇jAi(¯ξi)+(1−∇jAi(¯ξi))=1+(e−¯ξi,j−1)∇jAi(¯ξi)Thisargumentholdsforξijﬁnite.Addingconditionshandlingtheothercases,wegetthefollowingmessageupdates:ζi,j=logit`∇Ai(¯ξi,j)´−¯ξi,jif¯ξi,jisﬁnite¯ξi,jotherwise.2.5OthervariationalalgorithmsTheideasusedtoderivetheBPupdatescanbeextendedtoothervariationalalgorithmswithminormodiﬁcations.Weshowheretwoexamples:anaivemeanﬁeldalgorithm,andaTRWapproxima-tion.1Notethatinordertohandlethecaseswhereacanonicalparametercoordinateis+∞,weneedtoslightlyredeﬁnethesuper-partitionfunctionsasfollows:Ai(θ)=Xx∈Cexp(JXj=11[θj<+∞]θjφj(x))νi(x)JYj=11[θj=+∞⇒φj(x)=1]4wherelogit(x)=logx−log(1−x).Thismeansthatifwecanﬁndaparametervectorξi,j∈Rdsuchthatfi,j(x)=exp!φ(x),ξi,j"∝Yj!:j!!=jMj!→i(x),thenwecouldwrite1ζi,j=logit!∇jAi(ξi,j)".Wederivesuchavectorξi,jasfollows:Yj!:j!!=jMj!→i(x)=Yj!:j!!=jXsj!∈{0,1}eθj!sj!1[φj!(x)=sj!]Yi!:i!!=imi!→j!(sj!)=Yj!:j!!=jeθj!φj!(x)Yi!:i!!=imi!→j!(φj!(x))∝exp8<:Xj!:j!!=jφj!(x)0@θj!+Xi!:i!!=iζi!,j!1A9=;.Therequiredparametersaretherefore:!ξi,j"j!=1[j$=j!]#θj!+$i!:i!"=iζi!,j!%.2.4ReuseofpartitionfunctioncomputationsNaively,theupdatesderivedsofarwouldrequirecomputingJtimeseachsuper-partitionfunctionAiateachmessagepassingiteration.WeshowthatthiscanbereducedtocomputingeachAionlyonceperiteration,aconsiderablegain.Weﬁrstdeﬁnethevectors:¯ξi=θ+Xi!:i!!=iζi!,andthenrewritethenumeratorinsidethelogitfunctioninEquation(4)asfollows:Xx∈Xφj(x)fi,j(x)νi(x)=Xs∈{0,1}Xx:φj(x)=sexp!φ(x),¯ξi"e−¯ξi,jssνi(x)=e−¯ξi,j∇jAi(¯ξi),andsimilarityforthedenominator:Xx∈Xfi,j(x)νi(x)=e−¯ξi,j∇jAi(¯ξi)+(1−∇jAi(¯ξi))=1+(e−¯ξi,j−1)∇jAi(¯ξi)Thisargumentholdsforξijﬁnite.Addingconditionshandlingtheothercases,wegetthefollowingmessageupdates:ζi,j=logit`∇Ai(¯ξi,j)´−¯ξi,jif¯ξi,jisﬁnite¯ξi,jotherwise.2.5OthervariationalalgorithmsTheideasusedtoderivetheBPupdatescanbeextendedtoothervariationalalgorithmswithminormodiﬁcations.Weshowheretwoexamples:anaivemeanﬁeldalgorithm,andaTRWapproxima-tion.1Notethatinordertohandlethecaseswhereacanonicalparametercoordinateis+∞,weneedtoslightlyredeﬁnethesuper-partitionfunctionsasfollows:Ai(θ)=Xx∈Cexp(JXj=11[θj<+∞]θjφj(x))νi(x)JYj=11[θj=+∞⇒φj(x)=1]4............(4)

Mj 0→i (x)

mi0→j (s).

that an efﬁcient algorithm is assumed for computing the super-partition function Ai and its deriva-
tives. To avoid the exponential storage of messages going to Bi , we use an implicit representation
of these messages in the canonical parameter space.
Let us denote the messages going from Sj to Bi by Mj→i (s), s ∈ {0, 1} and the reverse messages
by mi→j (x), x ∈ X . From the deﬁnitions of Ψi,j , Ψi , Ψj , the explicit belief propagation updates
Y
mi→j (s) ∝ X
are:
Mj→i (x) ∝ X
Y
1[φj (x) = s]νi (x)
x∈X
j 0 :j 0 6=j
eθj s1[φj (x) = s]
i0 :i0 6=i
s∈{0,1}
The task is to get an update equation that does not represent Mj→i (x) explicitly, by exploiting
the fact that the super-partition functions Ai and their derivatives can be computed efﬁciently. To
do so, it is convenient to use the following equivalent representation for the messages mi→j (s):
If we also let fi,j (x) denote any function proportional to Q
ζi,j = log mi→j (1) − log mi→j (0) ∈ [−∞, +∞].4
(cid:18) P
(cid:18) P
(cid:19)
(cid:19)
j 0 :j 0 6=j Mj 0→i (x), we can write:
P
P
x∈X φj (x)fi,j (x)νi (x)
x∈X φj (x)fi,j (x)νi (x)
(5)
x∈X (1 − φj (x))fi,j (x)νi (x)
ζi,j = log
= logit
,
x∈X fi,j (x)νi (x)
where logit(x) = log x − log(1 − x). This means that if we can ﬁnd a parameter vector ξ i,j ∈ RJ
fi,j (x) = exphφ(x), ξi,j i ∝ Y
such that
then we could write ζi,j = logit (cid:0)∇j Ai (ξ i,j )(cid:1) . We derive such a vector ξ i,j as follows:
Mj 0→i (x),
j 0 :j 0 6=j
X
Y
Y
Y
eθj 0 sj 0 1[φj 0 (x) = sj 0 ]
eθj 0 φj 0 (x) Y
Y
Mj 0→i (x) =
mi0→j 0 (sj 0 )
j 0 :j 0 6=j
j 0 :j 0 6=j
i0 :i0 6=i
sj 0 ∈{0,1}
 X
 ,
θj 0 +
mi0→j 0 (φj 0 (x))
X
i0 :i0 6=i
j 0 :j 0 6=j
∝ exp
ζi0 ,j 0
j 0 :j 0 6=j
i0 :i0 6=i
(cid:16)
(cid:17)
where in the last step we have used the assumption that φj has domain {0, 1}, which implies that
(cid:1)
required parameters are therefore: (cid:0)ξ i,j
θj 0 + P
mi→j (φj (x)) = exp{φj (x) log mi→j (1) + (1 − φj (x)) log mi→j (0)} ∝ exp{φj (x)ζi,j }. The
j 0 = 1[j 6= j 0 ]
i0 :i0 6=i ζi0 ,j 0
.
2.4 Reuse of partition function computations

=

φj 0 (x)

Naively, the updates derived so far would require computing each super-partition function J times at
each message passing iteration. We show that this can be reduced to computing each super-partition
function only once per iteration, a considerable gain.
X
We ﬁrst deﬁne the vectors:
¯ξi = θ +
i0 :i0 6=i
X
X
X
and then rewrite the numerator inside the logit function in Equation (5) as follows:
exp{hφ(x), ¯ξi i} · e
− ¯ξi,j s · s · νi (x)
x∈X
s∈{0,1}
x:φj (x)=s
= eAi (¯ξi )− ¯ξi,j ∇j Ai (¯ξi ),
4 In what follows, we will assume that ζi,j ∈ (−∞, +∞). The extended real line is treated in Appendix C.1.

φj (x)fi,j (x)νi (x) =

ζ i0 ,

4

X
and similarly for the denominator:
fi,j (x)νi (x) = eAi (¯ξi )− ¯ξi,j ∇j Ai (¯ξi ) + eAi (¯ξi ) (1 − ∇j Ai (¯ξi ))
= eAi (¯ξi ) (cid:16)
(cid:17)
x∈X
− ¯ξi,j − 1)∇j Ai (¯ξi )
.
1 + (e
logit (cid:0)∇Ai ( ¯ξi,j )(cid:1) − ¯ξi,j , where the logit function of a vector, logit v , is deﬁned as the vector of
After plugging in the reparameterization of the numerator and denominator back into the logit
function in Equation (5) and doing some algebra, we obtain the more efﬁcient update ζi,j =
the logit function applied to each entry of the vector v . See Figure 1 for a summary of the BPMF
algorithm.

2.5 Other variational algorithms

The ideas used to derive the BPMF updates can be extended to other variational algorithms with
minor modiﬁcations. We sketch here two examples: a naive mean ﬁeld algorithm, and a TRW
approximation. See Appendix A.2 for details.
In the case of naive mean ﬁeld applied the graphical model described in Section 2.2, the updates
take a form similar to Equations (4), except that the reverse incoming message is not omitted when
computing an outgoing message. As a consequence, the updates are not directional and can be
Mj (s) ∝ X
Y
associated to nodes in the graphical model rather than edges:
Y
mi (x) ∝ X
mi (x)
1[φj (x) = s]νi (x)
x∈X
j
s∈{0,1}
i
This yields the following implicit updates:5
ξ(t) = θ +

eθj s1[φj (x) = s]
X
ζ (t−1)
(cid:16)∇Ai
(cid:16)
ξ(t)(cid:17)(cid:17)
i
i
ζ (t)
i = logit
and the moment approximation ˆµ = logistic(ξ).
 ·
θj 0 − ρi→j 0 ζi,j 0 +
In the case of TRW, lines 3 and 6 in the pseudocode of Figure 1 stay the same, while the update in
(cid:26) ρj 0→i
X
line 4 becomes:(cid:0)ξi,j
(cid:1)
if j 0 6= j
(1 − ρi→j ) otherwise,
i0 :i0 6=i
where ρi→j are marginals of a spanning tree distribution over KI ,J . We show in Appendix A.2 how
the idea in Section 2.4 can be exploited to reuse computations of super-partition functions in the
case of TRW as well.

ρi0→j 0 ζi0 ,j 0

(6)

(7)

Mj (s).

,

j 0 =

2.6 Large factorizations

In some cases, it might not be possible to write the base measure as a succinct product of factors.
Fortunately, there is a simple and elegant workaround to this problem that retains good theoretical
guarantees. The basic idea is that dropping measures with domain {0, 1} in a factorization can only
increase the value of the partition function. This solution is especially attractive in the context of
outer approximations such as the TRW algorithm, because it preserves the upper bound property of
the approximation. We show an example of this in Section 3.2.

3 Examples of factorizations

In this section, we show three examples of measure factorizations. See Appendix B for two more
examples (partitions of the plane, and traveling salesman problems).

5Assuming that naive mean ﬁeld is optimized coordinate-wise, with an ordering that optimizes all of the
mi ’s, then all of the Mj ’s.

5

(a)

(b)

(c)

Figure 2: (a) An example of a valid multiple alignment between three sequences. (b) Examples of invalid
multiple sequence alignments illustrating what is left out by the factors in the decomposition of Section 3.2.
(c) The DAG representation of a partial order. An example of linearization is A,C,D,B,E,F,G,H,I. The ﬁne red
dashed lines and blue lines demonstrate an example of two forests covering the set of edges, forming a measure
decomposition with two factors. The linearization A,D,B,E,F,G,H,I,C is an example of a state allowed by one
factor but not the other.

3.1 More matchings

Our approach extends naturally to matchings with higher-order (augmented) sufﬁcient statistic,
and to non-bipartite/non-perfect matchings. Let us ﬁrst consider an Higher-order Bipartite Model
(HBM), which has all the basic sufﬁcient statistic coordinates found in SBM, plus those of the form
φj (x) = xm,n · xm+1,n+1 . We claim that with the factorization of Equation (2), the super-partition
functions A1 and A2 are still tractable in HBM. To see why, note that computing A1 can be done
by building an auxiliary exponential family with associated graphical model given by a chain of
length N , and where the state space of each node in this chain is {1, 2, . . . , N }. The basic sufﬁ-
cient statistic coordinates φj (x) = xm,n are encoded as node potentials, and the augmented ones as
edge potentials in the chain. This yields a running time of O(N 3 ) for computing one super-partition
function and its gradient (see Appendix A.3 for details). The auxiliary exponential family technique
used here is reminiscent of [21].
space is the set of (cid:0)N
(cid:1) alignment indicators, we propose a decomposition into N measures. Each
Extension to non-perfect and non-bipartite matchings can also be done easily. In the ﬁrst case, a
one checks that a single node is connected to at most one other node: νn (x) = 1[PN
dummy “null” node is added to each bipartite component. In the second case, where the original
2
n0=1 xn,n0 ≤ 1].
3.2 Multiple sequence alignment

We start by describing the space of pairwise alignments (which is tractable), and then discuss the
extension to multiple sequences (which quickly becomes infeasible as the number of sequences
increases). Consider two sequences of length M and N respectively. A pairwise sequence alignment
is a bipartite graph on the characters of the two sequences (where each bipartite component has
the characters of one of the sequences) constrained to be monotonic: if a character at index m ∈
{1, . . . , M } is aligned to a character at index n ∈ {1, . . . , N } and another character at index m0 >
m is aligned to index n0 , then we must have n0 > n. A multiple alignment between K sequences of
lengths N1 , N2 , . . . , NK is a K -partite graph, where the k-th components’ vertices are the characters
of the k-th sequence, and such that the following three properties hold: (1) each pair of components
forms a pairwise alignment as described above; (2) the alignments are transitive, i.e., if character
c1 is aligned to c2 and c2 is aligned to c3 then c1 must be aligned to c3 ; (3) the alignments satisfy
a partial order property: there exists a partial order p on the connected components of the graph
with the property that if C1 <p C2 are two distinct connected components and c1 ∈ C1 , c2 ∈ C2
are in the same sequence, then the index of c1 in the sequence is smaller than the index of c2 . See
Figure 2(a,b) for an illustration.
there are (cid:0)K
(cid:1) pairwise alignment measures, and T =
We use the technique of Section 2.6, and include only the pairwise alignment and transitiv-
ity constraints, creating a variational objective function that is an outer bound of the origi-
nal objective.
In this factorization,
2
6

ACTACACGTACCACACACTACTACTCTMonotonicity violationTransitivity violationPartial order violationACDFGBEFI(a)

(b)

(c)

Figure 3: Experiments discussed in Section 4.1 on two of the matching models discussed. (a) and (b) on SBM,
(c), on HBM.
P
k,k0 ,k00 :k 6=k0 6=k00 6=k NkNk0 Nk00 transitivity measures. We show in Appendix A.4 that all the mes-
sages for one iteration can be computed in time O(T ).

3.3 Linearization of partial orders

A linearization of a partial order p over N objects is a total order t over the same objects such
that x ≤p y ⇒ x ≤t y . Counting the number of linearizations is a well-known #P problem [22].
Equivalently, the problem can be view as a matching between a DAG G = (V , E ) and the integers
{1, 2, . . . , N } with the order constraints speciﬁed on the edges of the DAG.
To factorize the base measure, consider a collection of I directed forests on V , Gi = (V , Ei ), i ∈ I
such that their union covers G: ∪iEi = E . See Figure 2(c) for an example. For a single forest Gi , a
straightforward generalization of the algorithm used to compute HBM’s super-partition can be used.
This generalization is simply to use sum-product with graphical model Gi instead of sum-product
on a chain as in HBM (see Appendix A.5 for details). Again, the state space of the node of the
graphical model is {1, 2, . . . , N }, but this time the edge potentials enforce the ordering constraints
of the current forest.

4 Experiments

4.1 Matchings

As a ﬁrst experiment, we compared the approximation of SBM described in Section 2 to the Fully
Polynomial Randomized Approximation Scheme (FPRAS) described in [23]. We performed all our
experiments on 100 iid random bipartite graphs of size N , where each edge has iid appearance prob-
ability p, a random graph model that we denote by RB(N , p). In the ﬁrst and second experiments, we
used RB(10, 0.9). In this case, exact computation is still possible, and we compared the mean Root
Mean Squared (RMS) of the estimated moments to the truth. In Figure 3(a), we plot this quantity as
a function of the time spent to compute the 100 approximations. In the variational approximation,
we measured performance at each iteration of BPMF, and in the sampling approach, we measured
performance after powers of two sampling rounds. The conclusion is that the variational approxi-
mation attains similar levels of error in at least one order of magnitude less time in the RB(10, 0.9)
regime.
Next, we show in Figure 3(b) the behavior of the algorithms as a function of p, where we also
added the mean ﬁeld algorithm to the comparison. In each data point in the graph, the FPRAS was
run no less than one order of magnitude more time than the variational algorithms. Both variational
strategies outperform the FPRAS in low-density regimes, where mean ﬁeld also slightly outperforms
BPMF. On the other hand, for high-density regimes, only BPMF outperforms the FPRAS, and mean
ﬁeld has a bias compared to the other two methods.
The third experiment concerns the augmented matching model, HBM. Here we compare two types
of factorization and investigate the scalability of the approaches to larger graphs. Factorization F1
is a simpler factorization of the form described in Section 3.1 for non-bipartite graphs. This ignores
the higher-order sufﬁcient statistic coordinates, creating an outer approximation. Factorization F2,

7

0.11101001000Time (s)00.10.20.30.4Mean RMSFPRASBPMF0.50.60.70.80.91Bipartite Graph Density00.10.20.30.40.50.60.7Mean RMSFPRASMean FieldLoopy BP102030405060Graph size00.020.040.060.080.10.120.14Mean Normalized LossHBM-F1HBM-F2BAliBASE protein group
short, < 25% identity
short, 20% — 40% identity
short, > 35% identity
All

Sum of Pairs score (SP)
BPMF-1 BPMF-2 BPMF-3 Clustal [24]
0.76
0.71
0.74
0.68
0.95
0.95
0.89
0.94
0.98
0.98
0.97
0.97
0.91
0.91
0.88
0.88

ProbCons [25]
0.72
0.92
0.98
0.89

Table 1: Average SP scores in the ref1/test1 directory of BAliBASE. BPMF-i denotes the average SP of the
BPMF algorithm after i iterations of (parallel) message passing.

described in Section 3.1 speciﬁcally for HBM, is tighter. The experimental setup is based on a gen-
erative model over noisy observations of bipartite perfect matchings described in Appendix C.2. We
show in Figure 3(c) the results of a sequence of these experiments for different bipartite component
sizes N/2. This experiments demonstrates the scalability of sophisticated factorizations, and their
superiority over simpler ones.

4.2 Multiple sequence alignment

To assess the practical signiﬁcance of this framework, we also apply it to BAliBASE [6], a standard
protein multiple sequence alignment benchmark. We compared our system to Clustal 2.0.12 [24],
the most popular multiple alignment tool, and ProbCons 1.12, a state-of-the-art system [25] that also
relies on enforcing transitivity constraints, but which is not derived via the optimization of an objec-
tive function. Our system uses a basic pair HMM [26] to score pairwise alignments. This scoring
function captures a proper subset of the biological knowledge exploited by Clustal and ProbCons.6
The advantage of our system over the other systems is the better optimization technique, based on
the measure factorization described in Section 3.2. We used a standard technique to transform the
pairwise alignment marginals into a single valid multiple sequence alignment (see Appendix C.3).
Our system outperformed both baselines after three BPMF parallel message passing iterations. The
algorithm converged in all protein groups, and performance was identical after more than three itera-
tions. Although the overall performance gain is not statistically signiﬁcant according to a Wilcoxon
signed-rank test, the larger gains were obtained in the small identity subset, the “twilight zone”
where research on multiple sequence alignment has focused.
One caveat of this multiple alignment approach is its running time, which is cubic in the length of
the longest sequence, while most multiple sequence alignment approaches are quadratic. For exam-
ple, the running time for one iteration of BPMF in this experiment was 364.67s, but only 0.98s for
Clustal—this is why we have restricted the experiments to the short sequences section of BAliBASE.
Fortunately, several techniques are available to decrease the computational complexity of this algo-
rithm: the transitivity factors can be subsampled using a coarse pass, or along a phylogenetic tree;
and computation of the factors can be entirely parallelized. These improvements are orthogonal to
the main point of this paper, so we leave them for future work.

5 Conclusion

Computing the moments of discrete exponential families can be difﬁcult for two reasons: the struc-
ture of the sufﬁcient statistic that can create junction trees of high tree-width, and the structure of
the base measures that can induce an intractable combinatorial space. Most previous work on vari-
ational approximations has focused on the ﬁrst difﬁculty; however, the second challenge also arises
frequently in machine learning. In this work, we have presented a framework that ﬁlls this gap.
It is based on an intuitive notion of measure factorization, which, as we have shown, applies to
a variety of combinatorial spaces. This notion enables variational algorithms to be adapted to the
combinatorial setting. Our experiments both on synthetic and naturally-occurring data demonstrate
the viability of the method compared to competing state-of-the-art algorithms.

6More precisely it captures long gap and hydrophobic core modeling.

8

References
[1] Alexander Karzanov and Leonid Khachiyan. On the conductance of order Markov chains. Order,
V8(1):7–15, March 1991.
[2] Mark Jerrum, Alistair Sinclair, and Eric Vigoda. A polynomial-time approximation algorithm for the
In Proceedings of the Annual ACM Symposium on
permanent of a matrix with non-negative entries.
Theory of Computing, pages 712–721, 2001.
[3] David Wilson. Mixing times of lozenge tiling and card shufﬂing Markov chains. The Annals of Applied
Probability, 14:274–325, 2004.
[4] Adam Siepel and David Haussler. Phylogenetic estimation of context-dependent substitution rates by
maximum likelihood. Mol Biol Evol, 21(3):468–488, 2004.
[5] Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning, 1:1–305, 2008.
[6] Julie Thompson, Fr ´ed ´eric Plewniak, and Olivier Poch. BAliBASE: A benchmark alignments database for
the evaluation of multiple sequence alignment programs. Bioinformatics, 15:87–88, 1999.
In Proceedings of the
[7] David A. Smith and Jason Eisner. Dependency parsing by belief propagation.
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 145–156, Honolulu,
October 2008.
[8] David Burkett, John Blitzer, and Dan Klein.
Joint parsing and alignment with weakly synchronized
grammars. In North American Association for Computational Linguistics, Los Angeles, 2010.
[9] Bert Huang and Tony Jebara. Approximating the permanent with belief propagation. ArXiv e-prints,
2009.
[10] Yusuke Watanabe and Michael Chertkov. Belief propagation and loop calculus for the permanent of a
non-negative matrix. J. Phys. A: Math. Theor., 2010.
[11] Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. Max-margin parsing.
In EMNLP, 2004.
[12] Ben Taskar, Simon Lacoste-Julien, and Dan Klein. A discriminative matching approach to word align-
ment. In EMNLP 2005, 2005.
[13] John Duchi, Daniel Tarlow, Gal Elidan, and Daphne Koller. Using combinatorial optimization within
max-product belief propagation. In Advances in Neural Information Processing Systems, 2007.
[14] Aron Culotta, Andrew McCallum, Bart Selman, and Ashish Sabharwal. Sparse message passing algo-
rithms for weighted maximum satisﬁability. In New England Student Symposium on Artiﬁcial Intelligence,
2007.
[15] Percy Liang, Ben Taskar, and Dan Klein. Alignment by agreement. In North American Association for
Computational Linguistics (NAACL), pages 104–111, 2006.
[16] Percy Liang, Dan Klein, and Michael I. Jordan. Agreement-based learning.
Information Processing Systems (NIPS), 2008.
[17] Leslie G. Valiant. The complexity of computing the permanent. Theoret. Comput. Sci., 1979.
[18] Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Generalized belief propagation. In Advances
in Neural Information Processing Systems, pages 689–695, Cambridge, MA, 2001. MIT Press.
[19] Carsten Peterson and James R. Anderson. A mean ﬁeld theory learning algorithm for neural networks.
Complex Systems, 1:995–1019, 1987.
[20] Martin J. Wainwright, Tommi S. Jaakkola, and Alan S. Willsky. Tree-reweighted belief propagation algo-
rithms and approximate ML estimation by pseudomoment matching. In Proceedings of the International
Conference on Articial Intelligence and Statistics, 2003.
[21] Alexandre Bouchard-C ˆot ´e and Michael I. Jordan. Optimization of structured mean ﬁeld objectives. In
Proceedings of Uncertainty in Artiﬁcal Intelligence, 2009.
[22] Graham Brightwell and Peter Winkler. Counting linear extensions. Order, 1991.
[23] Lars Eilstrup Rasmussen. Approximating the permanent: A simple approach. Random Structures and
Algorithms, 1992.
[24] Des G. Higgins and Paul M. Sharp. CLUSTAL: a package for performing multiple sequence alignment
on a microcomputer. Gene, 73:237–244, 1988.
[25] Chuong B. Do, Mahathi S. P. Mahabhashyam, Michael Brudno, and Seraﬁm Batzoglou. PROBCONS:
Probabilistic consistency-based multiple sequence alignment. Genome Research, 15:330–340, 2005.
[26] David B. Searls and Kevin P. Murphy. Automata-theoretic models of mutation and alignment. In Proc Int
Conf Intell Syst Mol Biol., 1995.

In Advances in Neural

9


Multi-label Multiple Kernel Learning by Stochastic
Approximation: Application to Visual Object Recognition

Serhat S. Bucak∗
bucakser@cse.msu.edu

Rong Jin∗
rongjin@cse.msu.edu

Anil K. Jain∗†
jain@cse.msu.edu

Dept. of Comp. Sci. & Eng.∗
Michigan State University
East Lansing, MI 48824,U.S.A.

Dept. of Brain & Cognitive Eng.†
Korea University, Anam-dong,
Seoul, 136-713, Korea

Abstract

Recent studies have shown that multiple kernel learning is very effective for object recognition,
leading to the popularity of kernel learning in computer vision problems. In this work, we develop
an efﬁcient algorithm for multi-label multiple kernel lear ning (ML-MKL). We assume that all the
classes under consideration share the same combination of kernel functions, and the objective is
to ﬁnd the optimal kernel combination that beneﬁts all the cl
asses. Although several algorithms
have been developed for ML-MKL, their computational cost is linear in the number of classes,
making them unscalable when the number of classes is large, a challenge frequently encountered
in visual object recognition. We address this computational challenge by developing a framework
for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis
shows that the complexity of our algorithm is O(m1/3√lnm), where m is the number of classes.
Empirical studies with object recognition show that while achieving similar classi ﬁcation accuracy,
the proposed method is signi ﬁcantly more efﬁcient than the s
tate-of-the-art algorithms for ML-MKL.

1

Introduction

Recent studies have shown promising performance of kernel methods for object classi ﬁcation, recognition and local-
ization [1]. Since the choice of kernel functions can signi ﬁ cantly affect the performance of kernel methods, kernel
learning, or more speci ﬁcally Multiple Kernel Learning (MK L) [2, 3, 4, 5, 6, 7], has attracted considerable amount
of interest in computer vision community. In this work, we focuss on kernel learning for object recognition because
the visual content of an image can be represented in many ways, depending on the methods used for keypoint detec-
tion, descriptor/feature extraction, and keypoint quantization. Since each representation leads to a different similarity
measure between images (i.e., kernel function), the related fusion problem can be cast into a MKL problem.

A number of algorithms have been developed for MKL. In [2], MKL is formulated as a quadratically constraint
quadratic program (QCQP). [8] suggests an algorithm based on sequential minimization optimization (SMO) to im-
prove the efﬁciency of [2]. [9] shows that MKL can be formulat ed as a semi-inﬁnite linear program (SILP) and can
be solved efﬁciently by using off-the-shelf SVM implementa tions. In order to improve the scalability of MKL, several
ﬁrst order optimization methods have been proposed, includ ing the subgradient method [10], the level method [11], the
method based on equivalence between group lasso and MKL [12, 13, 14]. Besides L1-norm [15] and L2-norm [16],
Lp-norm [17] has also been proposed to regularize the weights for kernel combination. Other then the framework
based on maximum margin classi ﬁcation, MKL can also be formu lated by using kernel alignment [18] and Fisher
discriminative analysis frameworks [19].

1

Although most efforts in MKL focus on binary classi ﬁcation p roblems, several recent studies have attempted to extend
MKL to multi-class and multi-label learning [3, 20, 21, 22, 23]. Most of these studies assume that either the same or
similar kernel functions are used by different but related classi ﬁcation tasks. Even though studies show that MKL for
multi-class and multi-label learning can result in signi ﬁc ant improvement in classi ﬁcation accuracy, the computatio nal
cost is often linear in the number of classes, making it computationally expensive when dealing with a large number of
classes. Since most object recognition problems involve many object classes, whose number might go up to hundreds
or sometimes even to thousands, it is important to develop an efﬁcient learning algorithm for multi-class and multi-
label MKL that is sublinear in the number of classes.

In this work, we develop an efﬁcient algorithm for Multi-Lab el MKL (ML-MKL) that assumes all the classi ﬁers
share the same combination of kernels. We note that although this assumption signi ﬁcantly constrains the choice of
kernel functions for different classes, our empirical studies with object recognition show that it does not affect the
classi ﬁcation performance. A similar phenomenon was also o bserved in [21]. A naive implementation of ML-MKL
with shared kernel combination will lead to a computational cost linear in the number of classes. We alleviate this
computational challenge by exploring the idea of combining worst case analysis with stochastic approximation. Our
analysis reveals that the convergence rate of the proposed algorithm is O(m1/3√ln m), which is signi ﬁcantly better
than a linear dependence on m, where m is the number of classes. Our empirical studies show that the proposed MKL
algorithm yields similar performance as the state-of-the-art algorithms for ML-MKL, but with a signi ﬁcantly shorter
running time, making it suitable for multi-label learning with a large number of classes.

The rest of this paper is organized as follows. Section 2 presents the proposed algorithm for Multi-Label MKL,
along with its convergence analysis. Section 3 summarizes the experimental results for object recognition. Section 4
concludes this work.

2 Multi-label Multiple Kernel Learning (ML-MKL)

We denote by D = {x1 , . . . , xn } the collection of n training instances, and by m the number of classes. We introduce
n )> ∈ {−1, +1}n , the assignment of the k th class to all the training instances: yk
i = +1 if xi is
yk = (yk
1 , . . . , yk
assigned to the k-th class and yk
i = −1 otherwise. We introduce κa (x, x0 ) : Rd × Rd 7→ R, a = 1, . . . , s, the s kernel
functions to be combined. We denote by {Ka ∈ Rn×n , a = 1, . . . , s} the collection of s kernel matrices for the data
points in D , i.e., K a
i,j = κa (xi , xj ).
We introduce p = (p1 , . . . , ps ), a probability distribution, for combining kernels. We denote by K(p) = Ps
a=1 paKa
the combined kernel matrices. We introduce the domain P for the probability distribution p, i.e., P = {p ∈ Rs
+ :
p>1 = 1}. Our goal is to learn from the training examples the optimal kernel combination p for all the m classes.
The simplest approach for multi-label multiple kernel learning with shared kernel combination is to ﬁnd the optimal
kernel combination p by minimizing the sum of regularized loss functions of all m classes, leading to the following
optimization problem:
i fk (xi )(cid:1))) ,
k=1 ( mXk=1
mXk=1 ( 1
nXi=1
` (cid:0)yk
2 |fk |2
H(p) +
min
{fk ∈H(p)}m
where `(z ) = max(0, 1 − z ) and H(p) is a Reproducing Kernel Hilbert Space endowed with kernel κ(x, x0 ; p) =
Ps
a=1 paκa (x, x0 ). Hk is the regularized loss function for the k th class. It is straightforward to verify the following
dual problem of (1):
α∈Q1 (L(p, α) =
(αk ◦ yk )>K(p)(αk ◦ yk )(cid:27)) ,
mXk=1 (cid:26)[αk ]>1 −
1
(2)
max
min
2
p∈P
where Q1 = (cid:8)α = (α1 , . . . , αm ) : αk ∈ [0, C ]n , k = 1, . . . , m(cid:9). To solve the optimization problem in Eq. (2), we
can view it as a minimization problem, i.e., minp∈P A(p), where A(p) = maxα∈Q1 L(p, α). We then follow the
subgradient descent approach in [10] and compute the gradient of A(p) as
mXk=1
1
2

(αk (p) ◦ yk )>Ki (αk (p) ◦ yk ),

∂pi A(p) = −

min
p∈P

Hk =

(1)

2

where αk (p) = arg maxα∈[0,C ]n [αk ]>1 − (αk ◦ yk )>K(p)(αk ◦ yk ). We refer to this approach as Multi-label
Multiple Kernel Learning by Sum, or ML-MKL-Sum. Note that this approach is similar to the one proposed
in [21]. The main computational problem with ML-MKL-Sum is that by treating every class equally, in each iteration
of subgradient descent, it requires solving m kernel SVMs, making it unscalable to a very large number of classes.
Below we present a formulation for multi-label MKL whose computational cost is sublinear in the number of classes.

2.1 A Minimax Framework for Multi-label MKL

In order to alleviate the computational difﬁculty arising f rom a large number of classes, we search for the combined
kernel matrix K (p) that minimizes the worst classi ﬁcation error among m classes, i.e.,

min
min
max
{fk ∈H(p)}m
p∈P
1≤k≤m
k=1
Eq. (3) differs from Eq. (1) in that it replaces Pm
k=1 Hk with max1≤k≤m Hk . The main computational advantage
of using maxk Hk instead of Pk Hk is that by using an appropriately designed method, we may be able to ﬁgure
out the most difﬁcult class in a few iterations, and spend mos t of the computational cycles on learning the optimal
kernel combination for the most difﬁcult class. In this way, we are able to achieve a running time that is sublinear
in the number of classes. Below, we present an optimization strategy for Eq. (3) based on the idea of stochastic
approximation.

Hk

(3)

where

.

(4)

A direct approach is to solve the optimization problem in Eq. (3) by its dual form. It is straightforward to derive the
dual problem of Eq. (3) as follows (more details can be found in the supplementary documents)
β∈B L(p, β) = ( mXk=1 (cid:26)[βk ]>1 −
2 )2
(βk ◦ yk )>K(p)(βk ◦ yk )(cid:27) 1
max
min
p∈P
λk = 1) .
B = ((β1 , . . . , βm ) : βk ∈ Rn
mXk=1
+ , k = 1, . . . , m, βk ∈ [0, C λk ]n s.t.
The challenge in solving Eq. (4) is that the solutions {β1 , . . . , βm } in domain B are correlated with each other, making
it impossible to solve each βk independently by an off-the-shelf SVM solver. Although a gradient descent approach
can be developed for optimizing Eq. (4), it is unable to explore the sparse structure in βk making it less efﬁcient than
state-of-the-art SVM solvers. In order to effectively explore the power of off-the-shelf SVM solvers, we rewrite (3) as
follows
γ∈Γ (L(p, γ ) = max
(αk ◦ yk )>K(p)(αk ◦ yk )(cid:27)) ,
γ k (cid:26)αk >
mXk=1
max
α∈Q1
where Γ = {(γ 1 , . . . , γm ) ∈ Rm
+ : γ >1 = 1}. In Eq. (5), we replace max1≤k≤m with maxγ∈Γ . The advantage of
using Eq. (5) is that we can resort to a SVM solver to efﬁcientl y ﬁnd αk for a given combination of kernels K(p).
Given Eq. (5), we develop a subgradient descent approach for solving the optimization problem. In particular, in each
iteration of subgradient descent, we compute the gradient L(p, γ ) with respect to p and γ as follows
mXk=1
1
1
(αk ◦ yk )>K(p)(αk ◦ yk ),
γ k (αk ◦ yk )>Ka (αk ◦ yk ), ∇γ k L(p, γ ) = [αk ]>1 −
2
2
where αk = arg maxα∈[0,C ]n α>1 − (α ◦ yk )>K(p)(α ◦ yk )/2, i.e., a SVM solution to the combined kernel K(p).
ηγ Ps
Following the mirror prox descent method [24], we deﬁne pote ntial functions Φp = ηp
a=1 pa ln pa for p and
Φγ = Pm
i=1 γ i ln γ i for γ , and have the following equations for updating pt and γt
γ k
pa
t
t
exp(−ηγ ∇γ k L(pt , γt )),
Z γ
Z p
t
t

exp(−ηp∇pa L(pt , γt )), γ k
t+1 =

∇pa L(p, γ ) = −

pa
t+1 =

1
2

1 −

(5)

(6)

(7)

min
p∈P

1
2

3

where Z p
t and Z γ
t are normalization factors that ensure p>
t 1 = 1. ηp > 0 and ηγ > 0 are the step sizes for
t 1 = γ >
optimizing p and γ , respectively.

(8)

Unfortunately, the algorithm described above shares the same shortcoming as the other approaches for multiple la-
bel multiple kernel learning, i.e., it requires solving m SVM problems in each iteration, and therefore its compu-
tational complexity is linear in the number of classes. To alleviate this problem, we modify the above algorithm
by introducing the stochastic approximation method. In particular, in each iteration t, instead of computing the full
gradients that requirs solving m SVMs, we sample one classi ﬁcation task according to the mult
inomial distribution
t ). Let jt be the index of the sampled classi ﬁcation task. Using the sam pled task jt , we estimate the
t , . . . , γm
M ulti(γ 1
a (pt , γt ) and bgγ
gradient of L(p, γ ) with respect to pa and γ k , denoted by bgp
k (pt , γt ), as follows
1
(αjt ◦ yjt )>Ka (αjt ◦ yjt ),
bgp
a (pt , γt ) = −
2
k (pt , γt ) = (cid:26)
k 6= jt
0
γk (cid:0)α>
2 (αk ◦ yk )>K(p)(αk ◦ yk )(cid:1) k = jt
bgγ
k 1 − 1
1
a (pt , γt ) and bgγ
i (pt , γt ) only requires αjt and therefore only needs to solve one SVM problem,
The computation of bgp
instead of m SVMs. The key property of the estimated gradients in Eqs. (8) and (9) is that their expectations equal to
the true gradients, as summarized by Proposition 1. This property is the key to the correctness of this algorithm.
Proposition 1. We have
a (pt , γt )] = ∇pa L(pt , γt ), Et [bgγ
Et [bgp
i (pt , γt )] = ∇γi L(pt , γt ),
where Et [·] stands for the expectation over the randomly sampled task jt .
Given the estimated gradients, we will follow Eq. (7) for updating p and γ in each iteration. Since bgγ
i (pt , γt ) is
proportional to 1/γt , to ensure the norm of bgγ
i (pt , γt ) to be bounded, we need to smooth γt+1 . In order to have the
t+1 ,
smoothing effect, without modifying γt+1 , we will sample directly from γ 0
δ
t+1 ← γ k
∀γ ∈ Γ, ∃γ 0 ∈ Γ0 , s.t. γ 0k
t+1 (1 − δ) +
, k = 1, . . . , m,
m
where δ > 0 is a small probability mass used for smoothing and
, k = 1, . . . , m(cid:27) .
Γ0 = (cid:26)γ 0>1 = 1, γ 0
δ
k ≥
m
We refer to this algorithm as Multi-label Multiple Kernel Learning by Stochastic Approximation, or ML-MKL-
SA for short. Algorithm 1 gives the detailed description.

.

(9)

2.2 Convergence Analysis

Since Eq. (5) is a convex-concave optimization problem, we introduce the following citation for measuring the quality
of a solution (p, γ )

γ 0∈Γ L(p, γ 0 ) − min
0∈P L(p0 , γ ).
∆(p, γ ) = max
p
We denote by (p∗ , γ∗ ) the optimal solution to Eq. (5).
Proposition 2. We have the following properties for ∆(p, γ )
1. ∆(p, γ ) ≥ 0 for any solution p ∈ P and γ ∈ Γ
2. ∆ (p∗ , γ ∗ ) = 0
3. ∆(p, γ ) is jointly convex in both p and γ

(11)

We have the following theorem for the convergence rate for Algorithm 1. The detailed proof can be found in the
supplementary document.
Theorem 1. After running Algorithm 1 over T iterations, we have the following inequality for the solution bp and bγ
obtained by Algorithm 1
(ln m + ln s) + ηγ (cid:18)d
0n2C 4 + n2C 2(cid:19) ,
m2
1
2δ2 λ2
E [∆ (bp, bγ )] ≤
ηγ T
where d is a constant term, E[·] stands for the expectation over the sampled task indices of all iterations, and λ0 =
λmax (Ka ), where λmax (Z) stands for the maximum eigenvalue of matrix Z.
max
1≤a≤s

4

Algorithm 1 Multi-label Multiple Kernel Learning: ML-MKL-SA
1: Input

[γt+1 ]k =

• ηp , ηγ : step sizes
• K 1 , . . . , K s : s kernel matrices
• y1 , . . . , ym : the assignments of m different classes to n training instances
• T : number of iterations
• δ : smoothing parameter
2: Initialization
• γ1 = 1/m and p1 = 1/s
3: for t = 1, . . . , T do
t ).
Sample a classi ﬁcation task jt according to the distribution M ulti(γ 1
t , . . . , γm
4:
Compute αjt = arg maxα∈[0,C ]n α>1 − (α ◦ yjt )>K(p)(α ◦ yjt )/2 using an off shelf SVM solver.
5:
a (pt , γt ) and bgγ
Compute the estimated gradients bgp
i (pt , γt ) using Eq. (8) and (9).
6:
Update pt+1 , γt+1 and γ 0
t+1 as follows
7:
pa
t
pa
exp(−ηγ bgp
t+1 =
a (pt , γt )), a = 1, . . . , s.
Z p
t
γ k
exp(ηγ bgγ
t
k (pt , γt )), k = 1, . . . , m; γ 0
t+1 = (1 − δ)γt+1 +
Z γ
t
8: end for
9: Compute the ﬁnal solution bp and bα as
1
bp =
bγ =
T
3 p(ln m)/T , after running Algorithm 1 (on the original paper) over T
2
n m− 1
Corollary 1. With δ = m
3 and ηγ = 1
iterations, we have E[∆(bp, bγ )] ≤ O(nm1/3p(ln m)/T ) in terms of m,n and T .
Since we only need to solve one kernel SVM at each iteration, we have the computational complexity for the proposed
algorithm on the order of O(m1/3p(ln m)/T ), sublinear in the number of classes m.
3 Experiments

TXt=1

δ
m

1.

(10)

γt ,

1
T

TXt=1

pt .

In this section, we empirically evaluate the proposed multiple kernel learning algorithm2 by demonstrating its efﬁ-
ciency and effectiveness on the visual object recognition task.

3.1 Data sets

We use three benchmark data sets for visual object recognition: Caltech-101, Pascal VOC 2006 and Pascal VOC 2007.
Caltech-101 contains 101 different object classes in addition to a “background” clas
s. We use the same settings as [25]
in which 30 instances of each class are used for training and 15 instances for testing. Pascal VOC 2006 data set [26]
consists of 5, 303 images distributed over 10 classes, of which 2, 618 are used for training. Pascal VOC 2007 [27]
consists of 5, 011 training images and 4, 932 test images that are distributed over 20 classes. For both data sets, we
used the default train-test partition provided by VOC Challenge. Unlike Caltech-101 data set, where each image is
assigned to one class, images in VOC data sets can be assigned to multiple classes simultaneously, making it more
suitable for multi-label learning.

2Codes can be downloaded from http://www.cse.msu.edu/˜ bucakser/ML-MKL-SA.rar

5

Table 1: Classi ﬁcation accuracy (AUC) and running times (se cond) of all ML-MKL algorithms on three data sets.
Abbreviations SA, GMKL, Sum, Simple, VSKL, AVG stand for ML-MKL-SA, Generalized MKL, ML-MKL-Sum,
SimpleMKL, variable sparsity kernel learning and average kernel, respectively
Training Time (sec)
Accuracy (AUC)
Simple
Sum
Sum Simple VSKL AVG
9869.40
1814.50
0.77
0.77
0.78
0.80
11549.00
890.65
0.72
0.74
0.74
0.74
0.47
0.42
0.46
0.45
1372.60
18536.37

VSKL
AVG
21266.05 N/A
7368.27
N/A
11370.48 N/A

dataset
CALTECH-101
VOC2006
VOC2007

GMKL
0.79
0.75
0.49

SA
0.80
0.75
0.50

SA
191.17
245.10
1329.40

GMKL
18292.00
2586.90
30333.14

1

0.8

0.6

0.4

0.2

s
t
n
e
i
c
i
f
f
e
o
c
 
l
e
n
r
e
k

0

0

1

0.8

0.6

0.4

0.2

s
t
n
e
i
c
i
f
f
e
o
c
 
l
e
n
r
e
k

800

0

0

0.5

1
time(sec)
GMKL

1.5

2
x 104

1

0.8

0.6

0.4

0.2

s
t
n
e
i
c
i
f
f
e
o
c
 
l
e
n
r
e
k

0

0

500

1000
time(sec)
ML-MKL-Sum

1500

1

0.8

0.6

0.4

0.2

s
t
n
e
i
c
i
f
f
e
o
c
 
l
e
n
r
e
k

0

0

0.5

1
time(sec)
VSKL

1.5

2
x 104

600

200

400
time(sec)
ML-MKL-SA

Figure 1: The evolution of kernel weights over time for CALTECH-101 data set. For GMKL and VSKL, the curves
display the kernel weights that are averaged over all the classes since a different kernel combination is learnt for each
class.

3.2 Kernels

We extracted 9 kernels for Caltech-101 data set by using the software provided in [28]. Three different feature
extraction methods are used for kernel construction: (i) GB: geometric blur descriptors are applied to the detected
keypoints [29]; RBF kernel is used in which the distance between two images is computed by averaging the distance
of the nearest descriptor pairs for the image pair. (ii) PHOW gray/color: keypoints based on dense sampling; SIFT
descriptors are quantized to 300 words and spatial histograms with 2x2 and 4x4 subdivisions are built to generate
chi-squared kernels [30]. (iii) SSIM: self-similarity features taken from [31] are used and spatial histograms based on
300 visual words are used to form the chi-squared kernel.
For VOC data sets, a different procedure, based on the reports of VOC challenges [1], is used to construct multiple
visual dictionaries, and each dictionary results in a different kernel. To obtain multiple visual dictionaries, we deploy
(i) three keypoint detectors, i.e., dense sampling, HARHES [32] and HESLAP [33], (ii) two keypoint descriptors,
i.e., SIFT [33] and SPIN [34]), (iii) two different numbers of visual words, i.e., 500 and 1, 000 visual words, (iv)
two different kernel functions, i.e., linear kernel and chi-squared kernel. The bandwidth of the chi-squared kernels
is calculated using the procedure in [25]. Using the above variants in visual dictionary construction, we constructed
22 kernels for both VOC2007 and VOC2006 data sets. In addition to the K-means implementation in [28], we also
applied a hierarchical clustering algorithm [35] to descriptor quantization for VOC 2007 data set, leading to four more
kernels for VOC2007 data set.

3.3 Baseline Methods

We ﬁrst compare the proposed algorithm ML-MKL-SA to the foll owing MKL algorithms that learn a different kernel
combination for each class: (i) Generalized multiple kernel learning method (GMKL) [25], which reports promising
results for object recognition, (ii) SimpleMKL [10], which learns the kernel combination by a subgradient approach
and (iii) Variable Sparsity Kernel Learning (VSKL), a miror-prox descent based algorithm for MKL [36]. We also
compare ML-MKL-SA to ML-MKL-Sum, which learns a kernel combination shared by all classes as described in
Section 2 using the optimization method in [21]. In all implementations of ML multiple kernel learning algorithms,we
use LIBSVM implementation of one-versus-all SVM where needed.

3.4 Experimental Results

To evaluate the effectiveness of different algorithms for multi-label multiple kernel learning, we ﬁrst compute the ar ea
under precision-recall curve (AUC) for each class, and report the value of AUC averaged over all the classes. We

6

C
U
A

0.8

0.78

0.76

0.74

0.72

 
0

 

 

C
U
A

0.75

0.74

0.73

0.72

0.71

0.7

 
0

ML−MKL−SA
ML−MKL−SUM

800

1000

200

400
600
time(sec)
VOC-2006

ML−MKL−SA
ML−MKL−SUM

1500

2000

500

1000
time(sec)
CALTECH-101

0.5

0.48

0.46

0.44

0.42

C
U
A

 

200

 

ML−MKL−SA
ML−MKL−SUM

1200

1400

1000

400

600
800
time(sec)
VOC-2007

Figure 2: The evolution of classi ﬁcation accuracy over time for ML-MKL-SA and ML-MKL-Sum on three data sets

C
U
A

0.81

0.805

0.8

0.795

0.79

0.785

0.78

0.775

 

 

δ=0
δ=0.2
δ=0.6
δ=1

50

100

150
250
200
number of iterations

300

350

400

0.84

0.82

C
U
A

0.8

0.78

0.76

 

 

η=0.01
η=0.001
η=0.0001

200

400
800
600
number of iterations

1000

1200

Figure 3: Classi ﬁcation accuracy (AUC) of the proposed
algorithm Ml-MKL-SA on CALTECH-101 using differ-
ent values of δ (for ηp = ηγ = 0.01).

Figure 4: Classi ﬁcation accuracy (AUC) of the proposed
algorithm Ml-MKL-SA on CALTECH-101 using differ-
ent values of ηp = ηγ = η for (δ = 0).

evaluate the efﬁciency of algorithms by their running times
for training. All methods are coded in MATLAB and
are implemented on machines with 2 dual-core AMD Opterons running at 2.2GHz, 8GB RAM and linux operating
system.

For the proposed method, itarations stop when bpt−bpt−1
is smaller than 0.01. Unless stated, the smoothing parameter
bpt
δ is set to be 0.2. For simplicity we take η = ηp = ηγ in all the following experiments. Step size η is chosen as 0.0001
for CALTECH-101 data set and 0.001 for VOC data sets in order to achieve the best computational efﬁciency.

Table 1 summarizes the classi ﬁcation accuracies (AUC) and t he running times of all the algorithms over the three
data sets. We ﬁrst note that the proposed MKL method for multi -labeled data, i.e., ML-MKL-SA, yields the best
performance among the methods in comparison, which justi ﬁe s the assumption of using the same kernel combination
for all the classes. Note that a simple approach that uses the average of all kernels yields reasonable performance,
although its classi ﬁcation accuracy is signi ﬁcantly worse
than the proposed approach ML-MKL-SA. Second, we
observe that except for the average kernel method that does not require learning the kernel combination weights, ML-
MKL-SA and ML-MKL-Sum are signi ﬁcantly more efﬁcient than t
he other baseline approaches. This is not surprising
as ML-MKL-SA and ML-MKL-Sum compute a single kernel combination for all classes. Third, compared to ML-
MKL-Sum, we observe that ML-MKL-SA is overall more efﬁcient
, and signi ﬁcantly more efﬁcient for CALTECH-
101 data set. This is because the number of classes in CALTECH-101 is signi ﬁcantly larger than that of the two VOC
challenge data sets. This result further conﬁrms that the pr oposed algorithm is scalable to the data sets with a large
number of classes.

Fig. 1 shows the change in the kernel weights over time for the proposed method and the three baseline methods (i.e.,
ML-MKL-Sum, GMKL, and VSKL) on CALTECH-101 data set. We observe that, overall, ML-MKL-SA shares a
similar pattern as GMKL and VSKL in the evolution curves of kernel weights, but is ten times faster than the two
baseline methods. Although ML-MKL-Sum is signi ﬁcantly mor e efﬁcient than GMKL and VSKL, the kernel weights
learned by ML-MKL-Sum vary signi ﬁcantly, particularly at t he beginning of the learning process, making it a less
stable algorithm than the proposed algorithm ML-MKL-SA. To further compare ML-MKL-SA with ML-MKL-Sum,
in Fig. 2, we show how the classi ﬁcation accuracy is changed o ver time for both methods for all three data sets.
We again observe the unstable behavior of ML-MKL-Sum: the classi ﬁcation accuracy of ML-MKL-Sum could vary
signi ﬁcantly over a relatively short period of time, making it less desirable method for MKL.

7

To evaluate the sensitivity of the proposed method to parameters δ and η , we conducted experiments with varied
values for the two parameters. Fig. 3 shows how the classi ﬁca tion accuracy (AUC) of the proposed algorithm changes
over iterations on CALTECH-101 using four different values of δ . We observe that the ﬁnal classi ﬁcation accuracy
is comparable for different values of δ , demonstrating the robustness of the proposed method to the choice of δ . We
also note that the two extreme cases, i.e, δ = 0 and δ = 1, give the worst performance, indicating the importance of
choosing an optimal value for δ . Fig. 4 shows the classi ﬁcation accuracy for three differen t values of η on CALTECH-
101 data set. We observe that the proposed algorithm achieves similar classi ﬁcation accuracy when η is set to be a
relatively small value (i.e., η = 0.001 and η = 0.0001). This result demonstrates that the proposed algorithm is in
general insensitive to the choice of step size (η ).

4 Conclusion and Future Work

In this paper, we present an efﬁcient optimization framewor k for multi-label multiple kernel learning that combines
worst-case analysis with stochastic approximation. Compared to the other algorithms for ML-MKL, the key advantage
of the proposed algorithm is that its computational cost is sublinear in the number of classes, making it suitable for
handling a large number of classes. We verify the effectiveness of the proposed algorithm by experiments in object
recognition on several benchmark data sets. There are two directions that we plan to explore in the future. First, we
aim to further improve the efﬁciency of ML-MKL by reducing it s dependence on the number of training examples and
speeding up the convergence rate. Second, we plan to improve the effectiveness and efﬁciency of multi-label learning
by exploring the correlation and structure among the classes.

5 Acknowledgements

This work was supported in part by National Science Foundation (IIS-0643494), US Army Research (ARO Award
W911NF-08-010403) and Ofﬁce of Naval Research (ONR N00014-0 9-1-0663). Any opinions, ﬁndings and conclu-
sions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views
of NFS, ARO, and ONR. Part of Anil Jain’s research was supported by WCU (World Class University) program
through the National Research Foundation of Korea funded by the Ministry of Education, Science and Technology
(R31-2008-000-10008-0).

References

[1] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge
2009 (VOC2009) Results.” http://www.pascal-network.org/challenges/V OC/voc2009/workshop/index.html.
[2] G. Lanckriet, T. De Bie, N. Cristianini, M. Jordan, and W. Noble, “A s tatistical framework for genomic data fusion,” Bioin-
formatics, vol. 20, pp. 2626–2635, 2004.
[3] S. Ji, L. Sun, R. Jin, and J. Ye, “Multi-label multiple kernel learning, ” in Proceedings of Neural Information Processings
Systems, 2008.
[4] G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, and M. Jordan, “L earning the kernel matrix with semideﬁnite program-
ming,” Journal of Machine Learning Research, vol. 5, pp. 27–72, 2004.
[5] O. Chapelle and A. Rakotomamonjy, “Second order optimization of ke rnel parameters,” in NIPS Workshop on Kernel Learn-
ing: Automatic Selection of Optimal Kernels, 2008.
[6] P. Gehler and S. Nowozin, “On feature combination for multiclass obje ct classiﬁcation,” in
tional Conference on Computer Vision, 2009.
arning of pre-processing for kernel classiﬁers,” in
[7] P. Gehler and S. Nowozin, “Let the kernel ﬁgure it out: Principled le
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009.
[8] F. Bach, G. Lanckriet, and M. Jordan, “Multiple kernel learning, c onic duality, and the smo algorithm,” in Proceedings of the
21st International Conference on Machine Learning, 2004.
[9] S. Sonnenburg, G. Ratsch, and C. Schafer, “A general and ef ﬁcient multiple kernel learning algorithm,” in
Neural Information Processings Systems, pp. 1273–1280, 2006.
[10] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu, “Simple MKL,” Journal of Machine Learning Research, vol. 9,
pp. 2491–2521, 2008.

Proceedings of the IEEE Interna-

Proceedings of

8

Proceedings of

Proceedings of the

Proceedings of the Conference on

[11] Z. Xu, R. Jin, I. King, and M. R. Lyu, “An extended level method f or efﬁcient multiple kernel learning,” in
Neural Information Processings Systems, pp. 1825–1832, 2008.
[12] Z. Xu, R. Jin, H. Yang, I. King, and M. R. Lyu, “Simple and efﬁcie nt multiple kernel learning by group lasso,” in Proceedings
of the 27th International Conference on Machine Learning, 2010.
[13] F. Bach, “Consistency of the group lasso and multiple kernel learn ing,” Journal of Machine Learning Research, vol. 9,
pp. 1179–1225, 2008.
[14] Z. Xu, R. Jin, S. Zhu, M. R. Lyu, and I. King, “Smooth optimization f or effective multiple kernel learning,” in Proceedings of
the AAAI Conference on Arti ﬁcial Intelligence , 2010.
[15] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet, “More efﬁciency in multiple kernel learning,” in
24th International Conference on Machine Learning, 2007.
[16] M. Kloft, U. Brefeld, A. Sonnenburg, and A. Zien, “Comparing s parse and non-sparse multiple kernel learning,” in NIPS
Workshop on Understanding Multiple Kernel Learning Methods, 2009.
[17] M. Kloft, U. Brefeld, A. Sonnenburg, P. Laskov, K.-R. Muller, and A. Zien, “Efﬁcient and accurate lp-norm multiple kernel
learning,” in Proceedings of Neural Information Processings Systems, 2009.
[18] S. Hoi, M. Lyu, and E. Chang, “Learning the uniﬁed kernel mach
ines for classiﬁcation,” in
Knowledge Discovery and Data Mining, p. 187196, 2006.
[19] J. Ye, J. Chen, and J. S., “Discriminant kernel and regulariza tion parameter learning via semideﬁnite programming,” in
Proceedings of the International Conference on Machine Learning, p. 10951102, 2007.
[20] A. Zien and S. Cheng, “Multiclass multiple kernel learning,” in Proceedings of the 24th International Conference on Machine
Learning, 2007.
[21] L. Tang, J. Chen, and J. Ye, “On multiple kernel learning with multiple labels,” in Proceedings of the 21st International Jont
Conference on Artiﬁcal Intelligence , 2009.
[22] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao, “Group-sensitive multiple kernel learning for object categorization,” in Pro-
ceedings of the IEEE International Conference on Computer Vision, 2009.
[23] F. Orabona, L. Jie, and B. Caputo, “Online-batch strongly conve x multi kernel learning,” in Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2010.
[24] A. Nemirovski, “Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone
operators and smooth convex-concave saddle point problems,” SIAM Journal on Optimization, vol. 15, pp. 229–251, 2004.
[25] M. Varma and D. Ray, “Learning the discriminative power-invaria nce trade-off,” in Proceedings of the IEEE International
Conference on Computer Vision, October 2007.
[26] M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool, “ The PASCAL Visual Object Classes Challenge 2006
(VOC2006) Results.” http://www.pascal-network.org/challenges/VOC/vo c2006/results.pdf.
[27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge
2007 (VOC2007) Results.” http://www.pascal-network.org/challenges/V OC/voc2007/workshop/index.html.
[28] A. Vedaldi and B. Fulkerson, “VLFeat: An open and portable libra ry of computer vision algorithms.” http://www.
vlfeat.org/, 2008.
[29] A. Berg, T. Berg, and J. Malik, “Shape matching and object reco gnition using low distortion correspondences,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, 2005.
[30] S. Lazebnik, C. Schmid, and P. Ponce, “Beyond bag of feature s: Spatial pyramid matching for recognizing natural scene
categories,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2006.
[31] E. Shechtman and I. M., “Matching local self-similarities across ima ges and videos,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2007.
[32] K. Mikolajczyk and C. Schmid, “Distinctive image features from sca le-invariant keypoints,”
Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1615–1630, 2005.
[33] D. Lowe, “Distinctive image features from scale-invariant keypo ints,” International Journal of Computer Vision, vol. 2, no. 60,
pp. 91–110, 2004.
[34] S. Lazebnik, C. Schmid, and P. Ponce, “Sparse texture repres entation using afﬁne-invariant neighborhoods,” in
of the IEEE Conference on Computer Vision and Pattern Recognition, 2003.
[35] M. Muja and D. G. Lowe, “Fast approximate nearest neighbors w ith automatic algorithm conﬁguration,” in
Proceedings of
the International Conference on Computer Vision Theory and Application, pp. 331–340, INSTICC Press, 2009.
[36] J. Saketha Nath, G. Dinesh, S. Raman, C. Bhattacharyya, A. Ben-Tal, and K. Ramakrishan, “On the algorithmics and ap-
plications of a mixed-norm based kernel learning formulation,” in Proceedings of Neural Information Processings Systems,
2009.

IEEE Transactions on Pattern

Proceedings

9


Ef ﬁcient algorithms for learning kernels from
multiple similarity matrices with general convex loss
functions

Achintya Kundu
Dept. of Computer Science & Automation,
Indian Institute of Science, Bangalore.
achintya@csa.iisc.ernet.in

Vikram Tankasali
Dept. of Computer Science & Automation,
Indian Institute of Science, Bangalore.
vikram@csa.iisc.ernet.in

Chiranjib Bhattacharyya
Dept. of Computer Science & Automation,
Indian Institute of Science, Bangalore.
chiru@csa.iisc.ernet.in

Aharon Ben-Tal
Faculty of Industrial Engg. & Management,
Technion - Israel Institute of Technology, Haifa.
abental@ie.technion.ac.il
Visiting Professor, CWI, Amsterdam

Abstract

In this paper we consider the problem of learning an n × n kernel matrix from
m(≥ 1) similarity matrices under general convex loss. Past research have exten-
sively studied the m = 1 case and have derived several algorithms which require
sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not
apply if one uses arbitrary losses and often can not handle m > 1 case. We
present several provably convergent iterative algorithms, where each iteration re-
quires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1
case. One of the major contributions of the paper is to extend the well known Mir-
ror Descent(MD) framework to handle Cartesian product of psd matrices. This
novel extension leads to an algorithm, called EMKL, which solves the problem in
O( m2 log n
) iterations; in each iteration one solves an MKL involving m kernels
ǫ2
and m eigen-decomposition of n × n matrices. By suitably de ﬁning a restriction
on the objective function, a faster version of EMKL is proposed, called REKL,
which avoids the eigen-decomposition. An alternative to both EMKL and REKL
is also suggested which requires only an SVM solver. Experimental results on real
world protein data set involving several similarity matrices illustrate the efﬁcacy
of the proposed algorithms.

1 Introduction

Learning procedures based on positive semide ﬁnite (psd) ke rnel functions, like Support vector ma-
chines (SVMs), have emerged as powerful tools for several learning tasks with wide applicability
[13]. In many applications it is relatively straightforward to de ﬁne measures of similarity between
any pair of examples but extremely difﬁcult to design a kerne l function for accurate classiﬁcation.
For instance, similarity score between two protein sequences given by various measures like BLAST
[1], Smith-Waterman [14], etc are not psd, whence cannot be substituted as kernel. In this paper, we
consider the problem of learning an optimal kernel matrix, from multiple similarity matrices, that
yields accurate classiﬁcation.

Let the set of n × n real symmetric matrices be denoted as Sn and the set of psd matrices as Sn
+ .
Consider a binary classiﬁcation problem with n training examples. Let y ∈ {+1 , −1}n be the

1

vector of class labels and K ∈ Sn
+ be a kernel matrix. The SVM formulation [13] computes a
performance measure ω (K ) by solving
α∈A h α⊤1 − 0.5 α⊤Y K Y α i,
ω (K ) = max
where A = {α ∈ Rn | α⊤y = 0, 0 ≤ α ≤ C 1}, Y = diag(y), 1 = [ 1 . . . 1 ]⊤ ∈ Rn and C user
de ﬁned positive constant.

(1)

1.1 Background and Related work

To the best of our knowledge the problem of handling multiple similarity matrices and arbitrary
convex losses has not been studied before. Existing literature has focussed on only one similarity
matrix and speciﬁc choices of loss function. In this section we brie ﬂy review the related literature.

The problem was ﬁrst studied in [8] for a single similarity ma trix. They introduced the following
optimization problem

,

(2)

ω (K ) + ρ kK − S k2
F ,

min
K∈Sn
+
where S ∈ Sn , is a similarity matrix, whose (i, j )-th element S (i, j ) represents the similarity be-
tween example pair i, j and ω (K ) is de ﬁned in (1). By interchanging the maximization over α and
minimization over K the authors note that the inner minimization admits a closed form solution:
K ∗ = (cid:0) S + (4ρ)−1 (Y α)(Y α)⊤ (cid:1)+
(3)
where (X )+ denotes the psd matrix obtained by clipping the negative eigen values of X to zero, i.e.,
if X = Pn
is the eigen decomposition of X ∈ Sn then (X )+ = Pn
i .
i=1 λi vi v⊤
i=1 max(λi , 0)vi v⊤
i
After plugging in the value of K ∗ authors suggest using sophisticated techniques like Analytic center
cutting plane (ACCP) method to solve the outer maximization in α.
The formulation (2) was studied further in [5] where an iterative algorithm based on solving a
quadratically constrained linear program (QCLP) was proposed. In both the above approaches the
order of maximization and minimization has been interchanged which lead to optimization problems
that can be posed as semi-in ﬁnite quadratically constraine d linear Programs (SIQCLP) [5]. In [6] an
alternate loss function kK − S kF was studied which led to a Second Order Cone Program(SOCP)
formulation. The choice of kK − S k2
F , as a measure of loss, is arbitrary. In this paper we generalize
the setting in (2) by providing an algorithm which works for any convex loss function. We note that
the method used in solving (2) utilizing (3) is speciﬁc to the loss function kK − S k2
F and do not
apply generally. Apart from non-applicability of the existing methods to other loss functions it is
not clear how these procedures could be used to handle multiple similarity matrices.

Contributions: The key contribution of the paper is to design efﬁcient proce dures which can learn
+ , from m(≥ 1) similarity matrices, possibly inde ﬁnite, under general co n-
a kernel matrix, K ∈ Sn
vex sub-differentiable loss function by using either SVM or MKL solvers. We study the problem in
two different settings. In the ﬁrst setting we consider lear ning a single kernel matrix from multiple
similarity matrices under a general loss function. A novel algorithm, referred in the paper as ESKL,
based on the Mirror Descent (MD) [3] procedure is proposed. It is a provably convergent algorithm
which requires O( log n
ǫ2 ) calls to an SVM solver. In the second setting we consider learning separate
kernel matrix for each of the given similarity matrices and then aggregating them using a Multiple
Kernel Learning (MKL) setup. The resultant formulation is non-trivial to solve. We present EMKL
which is based on generalizing the existing MD setup to deal with Cartesian product of psd matri-
ces. Like the previous case it requires O( m2 log n
) calls to an MKL solver. At every iteration the
ǫ2
algorithm also requires eigen-decomposition which is expensive. We present a related algorithm,
REKL, which does not require the expensive eigen-decomposition step but yields similar classiﬁca-
tion performance as EMKL. Apart from allowing general loss functions the procedures also opens
up new avenues for learning multiple kernels, which could be viable alternatives to the framework
proposed in [7].

The remainder of the paper is organized as follows: in section 2 we discuss problem formulation.
Our main contribution is in section 3, where we develop mirror descent algorithms for learning
kernel from multiple inde ﬁnite similarity matrices. We als o analyze the complexity and convergence
properties of the proposed algorithms. Finally we present our experimental results in section 4.

2

2 Problem formulation

(4)

Given multiple similarity matrices {Si : i = 1, . . . , m} we consider the following formulation
m
Li (K − Si ) i,
f (K ) ≡ h ω (K ) + ρ
Xi=1
min
K∈Sn
+ , tr(K )=τ
where ρ ≥ 0 is a trade-off parameter, Li : Sn → R is a convex sub-differentiable loss function
operating on K and Si . We impose the trace constraint on K to ensure good generalization as in [7].
A more naturally suited formulation for handling multiple similarity matrices is to consider learning
individual kernel matrix Ki from similarity matrix Si , ∀i and invoke a Multiple Kernel Learning
+ , βi ≥ 0, ∀i(cid:9). In [7] the
(MKL) setup to obtain a kernel matrix K ∈ K , (cid:8)Pi βi Ki | Ki ∈ Sn
MKL problem is proposed as
(5)
Ω(K1 , . . . , Km ) =
min
ω (K ) ,
K∈K, tr(K )=τ
where the kernels Ki ’s are ﬁxed and βi ’s are variable. Based on MKL we consider the following
kernel learning formulation
m
Li (Ki − Si ) i ,
F (K1 , . . . , Km ) ≡ h Ω(K1 , . . . , Km ) + ρ
Xi=1
min
K1 , ..., Km
s.t. Ki ∈ Sn
+ , tr(Ki ) = τ , i = 1, . . . , m.
Note that Ω(K1 , . . . , Km) can be obtained by solving any standard MKL formulation.
sets Nm
The restriction of Ω(K1 , . . . , Km ) on the Cartesian product of
=
i=1 { Ki
ij | µij ≥ 0, vij is j -th eigen vector of Si }, yields a very interesting alternative to
Pj µij vij v⊤
(6). Based on this restriction we formulate the restricted kernel learning problem as
m
ℓi (µi , λi ) i ,
g (µ1 , . . . , µm ) ≡ h Ω(K1 , . . . Km ) + ρ
Xi=1
min
µ1 , ..., µm∈Rn
+ , Pm
s.t. Ki = Pj µij vij v⊤
ij , Ki ∈ Sn
j=1 µij = τ , i = 1, . . . , m,
where λi = [λi1 . . . λin ]⊤ denotes the eigen values of Si and ℓi : Rn × Rn → R is a convex loss
function on µi = [µi1 . . . µin ]⊤ .
We mention that the formulation (4) generalizes the existing single similarity matrix based formula-
tions. For m = 1 with L(X ) = kX k2
F , L(X ) = kX kF we recover the formulations in [8] and [6]
respectively (albeit with a trace constraint). Also the SOCP based spectrum modiﬁcation learning
formulation [6] proposed in the context of single similarity matrix (m = 1) is a special case of (7).

(6)

(7)

3 Kernel Learning using Mirror Descent

In this section we derive general methods for solving (4) and (6) based on the following assumptions:
loss function Li is convex; a sub-gradient L′
i can be computed efﬁciently and the computed sub-
gradients are bounded. We also assume the availability of an efﬁcient SVM / MKL solver.

3.1 Entropic single kernel learning (ESKL) algorithm

We denote the feasible set of kernels as K = {K ∈ Sn
+ | tr(K ) = τ } and its relative interior as
int(K) = {K ∈ Sn | tr(K ) = τ , K is positive de ﬁnite }. Note that K is convex and compact.
De ﬁne inner-product on Sn as hK, K ′ i = tr(KK ′). From Eqn. (1) we note that ω (K ) is a convex
function of K . Therefore the objective function f in (4) is convex and Lipschitz continuous on K.
Let α∗ denote a maximizer of the SVM dual (1). Then we can compute a sub-gradient of f as
f ′ (K ) = −0.5 Y α∗ α∗⊤ Y + ρ Pm
(8)
i=1 L′
i (K − Si ) .
Thus the convex programming problem (4) satisﬁes the condit ions for applying Mirror Descent
(MD) [2] scheme. To apply MD procedure we require a strongly convex and continuously differen-
tiable function ψ : K → R. Following [2] we choose negative of matrix entropy as the candidate for
ψ :
ψ(K ) = Pn
j=1 λj log λj ,

(9)

3

where (λ1 , . . . , λn ) are the eigen values of K ∈ K. With the above setup we derive an MD
algorithm, named entropic single kernel learning (ESKL) algorithm, similar to the entropic mirror
descent algorithm proposed in [2].

Algorithm 1 Entropic single kernel learning (ESKL) algorithm
Initialization: K (1) ∈ int(K). Set t = 0.
repeat
t := t + 1.
• Obtain α∗ i.e. a maximizer of the SVM dual problem (1) for kernel K = K (t) .
i (K (t) − Si ).
• Compute sub-gradient f ′ (K (t) ) := −0.5 Y α∗α∗⊤Y + ρ Pj L′
• Choose suitable step-size ηt .
• Compute eigen decomposition f ′(K (t) ) = V (t) diag([d(t)
. . . d(t)
n ]) V (t) ⊤ .
1
exp(−ηt d(t)
τ λ(t)
j )
• λ(t+1)
j
, ∀j = 1, . . . , n.
:=
j
exp(−ηt d(t)
l=1 λ(t)
Pn
l )
l
i (cid:17) V (t) ⊤ .
• K (t+1) := V (t) diag (cid:16) hλ(t+1)
. . . λ(t+1)
n
1
until Convergence
Proposition 3.1. Let f (t) denote the objective function value at t-th iteration and f ∗ be the optimal
objective value. If the ESKL algorithm is initialized with K (1) = τ
n I and the step-sizes are chosen
Lip(f ) r 2 log n
f (t) − f ∗ ≤ τ Lip(f ) r 2 log n
1
then min
t
T
1≤t≤T
Lipschitz constant of f such that k f ′(K (t) ) kF ≤ Lip(F ) , t = 1, . . . , T .

, where Lip(f ) is a

as ηt =

Proof. The strong convexity constant of ψ w.r.t. k · kF norm is σ = 1
τ . Let Bψ denote the Bregman
distance function [2] generated by ψ . Then we have Bψ (K, K (1)) ≤ τ log n , ∀ K ∈ K (assuming
n ≥ 3). We complete the proof by applying Theorem 4.2 of [2] to the ESKL algorithm.

3.2 Entropic multiple kernel learning (EMKL) algorithm

Consider the kernel learning formulation (6) which minimizes the distances of kernels {Ki
: i =
1, . . . , m} from the corresponding similarity matrices {Si
: i = 1, . . . , m} and simultaneously
learns an SVM classiﬁer by performing multiple kernel learn ing (MKL) on those kernels. To learn
a non-sparse combination of kernels the following MKL formulation has been proposed in [10]:
m
1
1
α∈A h α⊤1 −
Ki(cid:1)Y α i,
Xi=1
α⊤Y (cid:0)
(10)
Ω(K1 , . . . , Km ) ≡ max
max
2
γ ∈△m
γi
where △m = (cid:8)γ = [γ1 . . . γm ]⊤ : Pi γi ≤ 1, γi ≥ 0, ∀i(cid:9) . With Ω(K1 , . . . , Km) as de ﬁned
above, the objective function F in (6) can be expressed as
α∈A Xi
Fi (Ki ; α, γ ) ,
F (K1 , . . . , Km) = max
max
γ ∈△m
(11)
m 1⊤α − 1
Fi (Ki ; α, γ ) = 1
tr (cid:0)KiY αα⊤Y (cid:1) + ρ Li (Ki − Si ) , i = 1, . . . , m.
2γi
+ : tr(K ) = τ } and Km := Nm
Let V := Nm
i=1 K ⊂ V. Denote
Sn , K = {K ∈ Sn
i=1
K = (K1 , . . . , Km ) ∈ Km . De ﬁne inner product on V as hK, K′ iV := Pm
i i, where
i=1 hKi , K ′
i ). Also de ﬁne a norm on V as kKk = Pm
i=1 kKikF . From (10) we note
hKi , K ′
i i = tr(KiK ′
that Ω(K1 , . . . , Km ) is a convex function of (K1 , . . . , Km) over the compact space Km . Thus the
objective function F in (6) is convex and Lipschitz continuous on Km .
Lemma 3.1. Let (α∗ , γ ∗ ) be a solution of (10) and L′
i be a sub-gradient of Li . Then a sub-gradient
of F is given by
F ′ (K1 , . . . , Km ) = (cid:16) ∂K1 F1 (K1 ; α∗ , γ ∗ ) · · · ∂Km Fm (Km ; α∗ , γ ∗ ) (cid:17),
1
Y α∗ α∗⊤Y + L′
∂Ki Fi (Ki ; α∗ , γ ∗ ) = −
i (Ki − Si ) , i = 1, . . . , m.
2 γ ∗
i

(12)

4

•
Proof. First, we observe that Fi (Ki ; α, γ ) is a convex function of Ki ∈ K and the expression of
∂Ki Fi (Ki ; α, γ ) given in Eqn. (12) is precisely a sub-gradient of Fi . Therefore, we can write

i − Ki , ∂Ki Fi (Ki ; α, γ ) i , ∀K ′
i ; α, γ ) ≥ Fi (Ki ; α, γ ) + h K ′
Fi (K ′
i ∈ K.
By optimality of (α∗ , γ ∗ ) we have F (K1 , . . . , Km) = Pm
i=1 Fi (Ki ; α∗ , γ ∗ ). Because of the
m) ≥ Pm
i ; α∗ , γ ∗ ) for any K′ =
max operation over α, γ , we have F (K ′
i=1 Fi (K ′
1 , . . . , K ′
m) ∈ Km . Applying (13) we arrive at
(K ′
1 , . . . , K ′
m ) ≥ F (K1 , . . . , Km ) + D K′ − K , F ′ (K1 , . . . , Km) EV
1 , . . . , K ′
F (K ′
Hence, F ′ (K1 , . . . , Km ) given in (12) is a sub-gradient of F .

(13)

,

We develop a novel Mirror Descent procedure for problem (6) by de ﬁning a strongly convex and
continuously differentiable function Ψ on the product space Km as
Ψ(K) = Pm
i=1 Pn
j=1 λi,j log λi,j , K ∈ Km ,
where (λi,1 , . . . , λi,n ) denote eigen values of Ki . The resulting algorithm, named entropic multiple
kernel learning (EMKL), is given below.

(14)

Algorithm 2 Entropic multiple kernel learning (EMKL) algorithm
Initialization: K (1)
i ∈ int(K), i = 1, . . . , m. Set t = 0.
repeat
t := t + 1.
Obtain α∗ , γ ∗ by solving the MKL problem (10) with Ki = K (t)
i
for i = 1 to m do
• Compute sub-gradient ∂Ki Fi (K (t)
:= − 1
; α∗ , γ ∗ )
i
2 γ ∗
i
; α∗ , γ ∗ ) = V (t)
• Find eigen decomposition ∂Ki Fi (K (t)
i
i
i,j exp(−ηt d(t)
τ λ(t)
i,j )
i,l exp(−ηt d(t)
l=1 λ(t)
Pn
i,l )
i,n ]) V (t) ⊤
. . . λ(t+1)
diag([λ(t+1)
:= V (t)
i,1
i
i

, j = 1, . . . , n.

• λ(t+1)
i,j

:=

.

, i = 1, . . . , m.

i (K (t)
i − Si ).
Y α∗ α∗⊤Y + L′
i,n ]) V (t) ⊤
i,1 . . . d(t)
diag([d(t)
.
i

• K (t+1)
i
end for
until Convergence

Theorem 3.2. Let F (t) denote the objective function value at t-th iteration and F ∗ be the optimal
objective value. If the EMKL algorithm is initialized with K (1)
i = τ
n I , ∀ i and the step-sizes are
Lip(F ) r 2 log n
F (t) − F ∗ ≤ τ m Lip(F ) r 2 log n
1
then min
m t
T
1≤t≤T
Lip(F ) is a Lipschitz constant of F such that k ∂Ki F (K (t)
; α, γ ) kF ≤ Lip(F ) , ∀i, t.
i

chosen as ηt =

, where

ηt =

m) be an optimal solution of (6). Denote K(t) = (cid:16)K (t)
m (cid:17).
1 , . . . , K (t)
Proof. Let K∗ = (K ∗
1 , . . . , K ∗
We apply the convergence result presented as Theorem 4.2 in [2]. This leads to the following:
F (t) − F ∗ ≤ Lip(F ) s 2 BΨ (K∗ , K(t) )
Lip(F ) s 2 σ BΨ (K∗ , K(t) )
1
t
σ T
where σ > 0 is the strong convexity constant of Ψ and BΨ is the Bregman distance function gener-
ated by Ψ. For the Ψ function de ﬁned in Eqn. (14), we have σ = 1
m τ . Assuming n ≥ 3, we also
have BΨ (K, K(1) ) ≤ m τ log n , ∀ K ∈ Km . Substituting values for BΨ and σ in (15) we obtain
the desired result.

⇒ min
1≤t≤T

(15)

,

5

3.3 Restricted entropic kernel learning (REKL) algorithm

The proposed EMKL algorithm is computationally expensive as it computes eign decomposition
of m matrices of dimension n × n at every iteration. Here we propose an efﬁcient algorithm
by considering the restricted kernel learning formulation (7) where Ω(K1 , . . . , Km) is given in
Eqn. (10). We denote the feasible set for µi as X := {µi ∈ Rn | µij ≥ 0, ∀j, Pn
j=1 µij = τ },
which is a convex compact subset of Rn . We note that Ω in (10) when viewed as a function of
µi ’s, is a convex function on the Cartesian product space X m := Nm
i=1 X . The loss function
ℓi is assumed to be a convex function of µi with bounded sub-gradients. Hence, the objective
function g in (7) is convex and Lipschitz continuous over the compact space X m . Denote a sub-
gradient of ℓi as [ ∂µi1 ℓi (µi , λi ) , . . . , ∂µin ℓi (µi , λi ) ]⊤ . We can compute a sub-gradient of Ω as
Ω′ = ( ∂µ11 Ω , ∂µ12 Ω , . . . , ∂µnn Ω ), where ∂µij Ω = − 1
ij Y α∗ . We derive an MD
α∗⊤Y vij v⊤
2 γ ∗
i
algorithm, named restricted entropic kernel learning (REKL), by extending the entropic mirror de-
scent scheme [2] to deal with Cartesian product of simplices. This is achieved by de ﬁning a strongly
convex and continuously differentiable function ψe : X m → R as
ψe (µ1 , . . . , µm ) = Pm
i=1 Pn
j=1 µij log µij , µi ∈ X , ∀i.
Algorithm 3 Restricted entropic kernel learning (REKL) algorithm
i = 1, . . . , m.
Find eigen decomposition: Si = Pj λij vij v⊤
ij ,
Initialization: µ(1)
i ∈ int(X ) , i = 1, . . . , m. Set t = 0.
repeat
t = t + 1
Obtain α∗ , γ ∗ by solving the MKL problem (10) with Ki = Pj µ(t)
ij , i = 1, . . . , m.
ij vij v⊤
for i = 1 to m do
• Compute sub-gradient g ′ (t)
ij Y α∗ + ∂µij ℓi (µ(t)
:= − 1
α∗⊤Y vij v⊤
ij
i
2 γ ∗
i
ij (cid:17)
ij exp (cid:16)−ηt g ′ (t)
τ µ(t)
il exp (cid:16)−ηt g ′ (t)
il (cid:17)
l=1 µ(t)
Pn

, j = 1, . . . , n.

, λi ).

(16)

• µ(t+1)
ij

:=

end for
until Convergence

Proposition 3.2. Let g (t) denote the objective function value at t-th iteration and g ∗ be the optimal
objective value. If the REKL algorithm is initialized with µ(1)
i = τ
n 1 , ∀ i and the step-sizes are
g (t) − g ∗ ≤ τ m Lip(g ) r 2 log n
Lip(g ) r 2 log n
1
then min
m t
T
1≤t≤T
is a Lipschitz constant of g such that |g ′ (t)
ij | ≤ Lip(g ) , i, = 1, . . . , m, j = 1, . . . , n, t = 1, . . . , T .

chosen as ηt =

, where Lip(g )

Proof. The proof is similar to that of Theorem 3.2.

3.4 Discussion
ǫ2 (cid:17) iterations (see Proposition 3.1), where in each iteration
The ESKL formulation requires O (cid:16) log n
one solves an SVM and eigen-decomposition of n × n matrix. Both EMKL and REKL formulations
require O (cid:16) m2 log n
(cid:17) iterations (see Theorem 3.2 and Proposition 3.2), and in each iteration one
ǫ2
solves an MKL problem. However EMKL is more computationally expensive than REKL.
4 Experiments and Results

In this section we experimentally compare the proposed kernel learning formulations against
IndSVM [8] and the eigen transformation methods: Denoise, Flip, Shift [15]. Given an inde ﬁnite
j , eigen transformation methods gen-
similarity matrix S with eigen-decomposition S = Pj λj vj v⊤
j , where choice µj ’s are: (a) Denoise: µj = max(λj , 0),
erate kernel matrix as K := Pj µj vj v⊤
6

(b) Flip: µj = |λj |, (c) Shift: µj = λj − δ , where δ = min{λ1 , . . . , λn , 0}. We consider the fol-
lowing choices for the loss functions in ESKL / EMKL: [L1 ] L(K −S ) = Pi,j |K (i, j )−S (i, j )|,
[L2 ] L(K − S ) = kK − S kF ,
[L3 ] L(K − S ) = Pi,j |K (i, j ) − S (i, j )|2 . For REKL we
choose ℓ(µj , λj ) = |µj − λj k2 , i.e., the Euclidean distance. Algorithm parameters are tuned using
standard 5 fold cross validation procedure. LibSVM [4] is used as the SVM solver. For each data set
we have considered equal number of positive and negative training / test samples. We report classi-
ﬁcation performance in terms of accuracy and F-score (expre ssed as % ) averaged over 5 different
train / test splits.

4.1 Data sets

We experimented on 10 different data sets including the data sets covered in [16, 6]. We have
generated the inde ﬁnite similarity matrices as prescribed in [16] for each of the following data sets:
Sonar, Liver disorder, Ionosphere, Diabetes and Heart. We have used the same similarity matrices
as in [6]:1 for the data sets Amazon, AuralSonar, Yeast-SW-5-7 and Yeast-SW-5-12 .
To test the proposed multiple similarity based formulations we experimented on a subset of the
SCOP database [9] taken from Protein Classiﬁcation Benchma rk Collection 2 . Considering pro-
teins having < 40% sequence identity, we randomly select 8 super-families which have at least
45 proteins. We compute 3 different pairwise similarity measure for proteins: Psi-BLAST [1],
Smith-Waterman [14] and Needleman-Wunsch [11]. The similarity matrices obtained from these 3
similarity measures are inde ﬁnite in general.

4.2 Effect of various loss functions

We experimentally demonstrate the ability of the proposed ESKL algorithm in handling general
convex loss function. Classiﬁcation performance is presen ted in Table 1. We observe that on Liver
disorder data set L2 loss performs better than L1 , L3 . Again, on Diabetes and Heart data sets
both L1 , L2 provides much better performance better than L3 . From Table 2 we observe that on
AuralSonar data set ESKL formulation works best with L3 loss function. But on Yeast-SW-5-7 data
set L1 loss function works best. Therefore we can say that the choice of loss function has an effect
on classiﬁcation accuracy. This suggest the need for a gener al algorithm which provides ﬂexibility
to choose loss function based on the data set. Hence in this paper we have developed the algorithms
keeping the choice of loss function almost open.

Table 1: Comparison of classiﬁcation accuracy (odd rows) an d F-score (even rows) on UCI data sets

Dataset

Sonar

Liver disorder

Ionosphere

Diabetes

Heart

Eigen Transformation
Shift
Flip
Denoise
71.5
72.5
76.5
75.0
70.6
70.0
55.5
54.5
57.6
52.9
53.8
55.4
91.2
89.6
87.3
87.6
89.9
91.4
64.4
58.7
63.9
65.1
58.8
65.0
75.8
63.8
73.3
73.1
65.1
76.9

IndSVM
[8]
76.5
74.8
59.7
55.8
91.5
91.8
70.2
71.4
76.3
76.5

ESKL
[L2 ]
73.0
71.3
62.8
59.1
91.2
91.4
73.5
74.6
78.8
79.0

[L1 ]
75.5
73.9
61.0
58.9
88.5
88.5
73.3
74.3
78.8
79.5

[L3 ]
75.5
74.1
60.7
57.5
91.5
91.8
69.8
71.0
76.3
76.5

4.3 Combining multiple sequence similarity matrices for Proteins

Consider the task of classifying proteins into super-families when multiple sequence similarity mea-
sures are available. We perform 1 vs rest classiﬁcation experiments on each of the 8 protein su per-
families and report performance averaged over 5 train / test splits. One can extend IndSVM [8]

1http://idl.ee.washington.edu/SimilarityLearning/
2http://net.icgeb.org/benchmark/

7

Table 2: Comparison of classiﬁcation accuracy (odd rows) an d F-score (even rows) on real data sets

Dataset

Amazon

AuralSonar

Yeast-SW-5-7

Yeast-SW-5-12

Eigen Transformation
Denoise
Flip
Shift
85.0
83.8
83.8
85.9
84.8
84.8
87.0
87.0
87.0
86.3
86.3
86.5
75.5
70.0
74.0
74.1
72.4
77.1
86.0
85.5
86.0
87.1
85.8
87.6

IndSVM
[8]
87.5
86.9
88.0
87.3
77.0
77.7
90.0
90.9

ESKL
[L2 ]
85.0
84.3
87.0
86.3
75.5
76.1
90.5
91.35

[L1 ]
88.8
88.0
88.0
87.3
79.0
79.9
90.0
90.9

[L3 ]
88.8
88.0
90.0
89.1
76.5
77.2
90.0
90.9

Table 3: Comparison of classiﬁcation accuracy (odd rows) an d F-score (even rows) on Proteins

Super
family
a.4.1

b.1.18

b.29.1

b.40.4

c.1.8

c.3.1

c.47.1

c.67.1

Linear
SVM
51.9
67.5
63.8
73.9
70.6
55.6
66.9
74.8
58.8
29.5
91.9
90.9
88.1
85.7
88.8
87.2

Eigen
Denoise
53.1
68.1
62.5
73.1
80.6
76.3
68.1
75.4
75.0
65.1
97.5
97.4
86.2
87.8
90.6
89.6

IndSVM simple MKL
[8]
[12]
56.9
54.4
69.9
68.7
65.6
58.1
70.7
74.7
77.5
75.6
70.2
67.0
70.0
59.4
76.7
71.1
66.9
73.7
62.7
50.1
95.0
95.6
94.7
95.4
76.2
90.0
88.8
81.5
91.2
90.6
89.6
90.3

ESKL
[L1 ]
70.0
77.1
71.9
78.6
85.0
83.5
71.9
77.3
80.6
74.6
95.6
95.2
84.4
85.7
81.9
76.8

EMKL
[L1 ]
73.1
78.9
75.6
80.6
83.8
82.1
68.8
76.3
85.0
80.8
95.6
95.4
90.6
90.3
91.2
90.3

REKL
k · k2
84.4
86.5
74.4
78.6
75.0
71.0
76.2
78.5
80.0
77.9
96.2
96.0
84.4
86.4
93.1
92.6

originally proposed to handle single similarity matrix, to multiple similarity matrices by averaging
over the similarity matrices. We implement the linear SVM by considering similarities as feature
and computing a linear kernel. We also compare with a multiple kernel learning formulation, simple
MKL [12]. Denoised version of the similarity matrices are given as input to simple MKL. In Ta-
ble 3 the proposed multiple similarity based kernel learning algorithms ESKL / EMKL / REKL are
compared with the other methods mentioned above. We observe signiﬁcant performance improve-
ment in most cases. We also note that REKL is computationally cheaper than EMKL but provides
reasonably good performance.

5 Conclusion

We have proposed three formulations, (4), (6), (7) for learning kernels from multiple similarity ma-
trices. The key advantages of the proposed algorithms over the state of the art are: (i) require only
SVM / MKL solvers and does not require any other sophisticated tools; (ii) the algorithms are appli-
cable for a wide choice of loss functions and multiple similarity functions. Proposed methods can
also be seen as an alternative to Multiple Kernel learning,which will be explored in future research.

Acknowledgments

Prof. Chiranjib Bhattacharyya was partly supported by Yahoo! faculty award grant.

8

References

[1] Stephen F. Altschul, Thomas L. Madden, Alejandro A. Schffer, Ro A. Schffer, Jinghui Zhang,
Zheng Zhang, Webb Miller, and David J. Lipman. Gapped blast and psiblast: a new generation
of protein database search programs. NUCLEIC ACIDS RES, 25:3389 –3402, 1997.
[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31:167 –175, 2003.
[3] A. Ben-Tal, T. Margalit, and A. Nemirovski. The ordered subsets mirror descent optimization
method with applications to tomography. SIAM J. Optim., 12:79 –108, 2001.
[4] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001.
Software available at http://www.csie.ntu.edu.tw/ ˜ cjlin/libsvm.
[5] J. Chen and J. Ye. Training svm with inde ﬁnite kernels.
In International Conference on
Machine Learning. 2008.
[6] Y. Chen, M. R. Gupta, and B. Recht. Learning kernels from inde ﬁnite similarities. In Interna-
tional Conference on Machine Learning. 2009.
[7] G. R. Gert Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and Michael I. Jordan. Learning
the kernel matrix with semide ﬁnite programming.
Journal of Machine Learning Research,
5:27 –72, 2004.
[8] R. Luss and A. d’Aspremont. Support vector machine classiﬁcation with inde ﬁnite kernels. In
Advances in Neural Information processing Systems. 2007.
[9] A. G. Murzin, S. E. Brenner, T. Hubbard, and C. Chothia. Scop: a structural classiﬁcation
of proteins database for the investigation of sequences and structures. Journal of Molecular
Biology, 247:536 –540, 1995.
[10] J. Saketha Nath, G. Dinesh, S. Raman, C. Bhattacharyya, A. Ben-Tal, and K.R. Ramakrishnan.
On the algorithmics and applications of a mixed-norm based kernel learning formulation. In
Advances in Neural Information Processing Systems, pages 844 –852. 2009.
[11] Saul B. Needleman and Christian D. Wunsch. A general method applicable to the search
for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology,
48(3):443 –453, 1970.
[12] A. Rakotomamonjy, Francis R. Bach, S. Canu, and Y. Grandvalet. Simplemkl. Journal of
Machine Learning Research, 9:2491 –2521, 2008.
[13] Bernhard Sch ¨olkopf and Alexander J. Smola. Learning with Kernels: Support Vector Ma-
chines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learn-
ing). The MIT Press, 2001.
[14] T. F. Smith and M. S. Waterman. Identiﬁcation of common m olecular subsequences. Journal
of Molecular Biology, 147(1):195 – 197, 1981.
[15] G. Wu, Z. Zhang, and E. Y. Chang. An analysis of transformation on non-positive semide ﬁ-
nite similarity matrix for kernel machines. Technical Report, University of California, Santa
Barbara, 2005.
[16] Y. Ying, C. Campbell, and M. Girolami. Analysis of SVM with inde ﬁnite kernels. In Advances
in Neural Information processing Systems, 2009.

9


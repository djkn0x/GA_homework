Variable margin losses for classi ﬁer design

Hamed Masnadi-Shirazi
Statistical Visual Computing Laboratory,
University of California, San Diego
La Jolla, CA 92039
hmasnadi@ucsd.edu

Nuno Vasconcelos
Statistical Visual Computing Laboratory,
University of California, San Diego
La Jolla, CA 92039
nuno@ucsd.edu

Abstract

The problem of controlling the margin of a classi ﬁer is studi ed. A detailed an-
alytical study is presented on how properties of the classi ﬁ cation risk, such as
its optimal link and minimum risk functions, are related to the shape of the loss,
and its margin enforcing properties. It is shown that for a class of risks, denoted
canonical risks, asymptotic Bayes consistency is compatible with simple analyti-
cal relationships between these functions. These enable a precise characterization
of the loss for a popular class of link functions. It is shown that, when the risk is
in canonical form and the link is inverse sigmoidal, the margin properties of the
loss are determined by a single parameter. Novel families of Bayes consistent loss
functions, of variable margin, are derived. These families are then used to design
boosting style algorithms with explicit control of the classi ﬁcation margin. The
new algorithms generalize well established approaches, such as LogitBoost. Ex-
perimental results show that the proposed variable margin losses outperform the
ﬁxed margin counterparts used by existing algorithms. Fina lly, it is shown that
best performance can be achieved by cross-validating the margin parameter.

1

Introduction

Optimal classi ﬁers minimize the expected value of a loss fun ction, or risk. Losses commonly used
in machine learning are upper-bounds on the zero-one classi ﬁcation loss of classical Bayes decision
theory. When the resulting classi ﬁer converges asymptotica lly to the Bayes decision rule, as training
samples increase, the loss is said to be Bayes consistent. Examples of such losses include the hinge
loss, used in SVM design, the exponential loss, used by boosting algorithms such as AdaBoost,
or the logistic loss, used in both classical logistic regression and more recent methods, such as
LogitBoost. Unlike the zero-one loss, these losses assign a penalty to examples correctly classi ﬁed
but close to the boundary. This guarantees a classi ﬁcation m argin, and improved generalization
when learning from ﬁnite datasets [1]. Although the connect
ions between large-margin classi ﬁcation
and classical decision theory have been known since [2], the set of Bayes consistent large-margin
losses has remained small. Most recently, the design of such losses has been studied in [3]. By
establishing connections to the classical literature in probability elicitation [4], this work introduced
a generic framework for the derivation of Bayes consistent losses. The main idea is that there are
three quantities that matter in risk minimization: the loss function φ, a corresponding optimal link
function f ∗
φ , which maps posterior class probabilities to classi ﬁer pre dictions, and a minimum risk
φ , associated with the optimal link.
C ∗
While the standard approach to classi ﬁer design is to deﬁne a l
oss φ, and then optimize it to obtain
φ and C ∗
φ , [3] showed that there is an alternative: to specify f ∗
φ and C ∗
φ , and analytically derive the
f ∗
loss φ. The advantage is that this makes it possible to manipulate the properties of the loss, while
guaranteeing that it is Bayes consistent. The practical relevance of this approach is illustrated in [3],
where a Bayes consistent robust loss is derived, for application in problems involving outliers. This

1

loss is then used to design a robust boosting algorithm, denoted SavageBoost. SavageBoost has been,
more recently, shown to outperform most other boosting algorithms in computer vision problems,
where outliers are prevalent [5]. The main limitation of the framework of [3] is that it is not totally
constructive. It turns out that many pairs (C ∗
φ ,f ∗
φ ) are compatible with any Bayes consistent loss φ.
Furthermore, while there is a closed form relationship between φ and (C ∗
φ ,f ∗
φ ), this relationship is
far from simple. This makes it difﬁcult to understand how the properties of the loss are inﬂuenced
by the properties of either C ∗
φ or f ∗
φ . In practice, the design has to resort to trial and error, by 1)
testing combinations of the latter and, 2) verifying whether the loss has the desired properties. This
is feasible when the goal is to enforce a broad loss property, e.g. that a robust loss should be bounded
for negative margins [3], but impractical when the goal is to exercise a ﬁner degree of control.

In this work, we consider one such problem: how to control the size of the margin enforced by the
loss. We start by showing that, while many pairs (C ∗
φ ,f ∗
φ ) are compatible with a given φ, one of these
pairs establishes a very tight connection between the optimal link and the minimum risk: that f ∗
φ is
φ . We refer to the risk function associated with such a pair as a canonical risk,
the derivative of C ∗
and show that it leads to an equally tight connection between the pair (C ∗
φ ,f ∗
φ ) and the loss φ. For
a canonical risk, all three functions can be obtained from each other with one-to-one mappings of
trivial analytical tractability. This enables a detailed analytical study of how C ∗
φ or f ∗
φ affect φ. We
consider the case where the inverse of f ∗
φ is a sigmoidal function, i.e. f ∗
φ is inverse-sigmoidal, and
show that this strongly constrains the loss. Namely, the latter becomes 1) convex, 2) monotonically
decreasing, 3) linear for large negative margins, and 4) constant for large positive margins. This
implies that, for a canonical risk, the choice of a particular link in the inverse-sigmoidal family
only impacts the behavior of φ around the origin, i.e. the size of the margin enforced by the loss.
This quantity is then shown to depend only on the slope of the sigmoidal inverse-link at the origin.
Since this property can be controlled by a single parameter, the latter becomes a margin-tunning
parameter, i.e. a parameter that determines the margin of the optimal classi ﬁer. This is exploited to
design parametric families of loss functions that allow explicit control of the classi ﬁcation margin.
These losses are applied to the design of novel boosting algorithms of tunable margin. Finally,
it is shown that the requirements of 1) a canonical risk, and 2) an inverse-sigmoidal link are not
unduly restrictive for classi ﬁer design. In fact, approach es like logistic regression or LogitBoost
are special cases of the proposed framework. A number of experiments are conducted to study the
effect of margin-control on the classi ﬁcation accuracy. It
is shown that the proposed variable-margin
losses outperform the ﬁxed-margin counterparts used by exi sting algorithms. Finally, it is shown that
cross-validation of the margin parameter leads to classi ﬁe rs with the best performance on all datasets
tested.

2 Loss functions for classiﬁcation

We start by brieﬂy reviewing the theory of Bayes consistent c lassi ﬁer design. See [2, 6, 7, 3] for
further details. A classi ﬁer h maps a feature vector x ∈ X to a class label y ∈ {−1, 1}. This
mapping can be written as h(x) = sign[p(x)] for some function p : X → R, which is denoted
as the classi ﬁer predictor. Feature vectors and class label s are drawn from probability distributions
PX (x) and PY (y) respectively. Given a non-negative loss function L(x, y), the classi ﬁer is optimal
if it minimizes the risk R(f ) = EX,Y [L(h(x), y)]. This is equivalent to minimizing the conditional
risk EY |X [L(h(x), y)|X = x] for all x ∈ X .
It is useful to express p(x) as a composition of
two functions, p(x) = f (η(x)), where η(x) = PY |X (1|x), and f : [0, 1] → R is a link function.
Classi ﬁers are frequently designed to be optimal with respe ct to the zero-one loss
if y = sign(f );
= (cid:26) 0,
1 − sign(yf )
if y 6= sign(f ),
1,
2
where we omit the dependence on x for notational simplicity. The associated conditional risk is
if f ≥ 0;
= (cid:26) 1 − η ,
if f < 0.
η ,

1 − sign(f )
2

1 + sign(f )
2

(1)

(2)

L0/1 (f , y) =

C0/1 (η , f ) = η

+ (1 − η)

The risk is minimized if

f (x) > 0
f (x) = 0
f (x) < 0

if η(x) > 1
2
if η(x) = 1
2
if η(x) < 1
2




2

(3)

Table 1: Loss φ, optimal link f ∗
φ ]−1 (v) , and minimum conditional risk C ∗
φ (η), optimal inverse link [f ∗
φ (η)
for popular learning algorithms.
Algorithm
SVM
Boosting
Logistic Regression

C ∗
φ (η)
1 − |2η − 1|
2pη(1 − η)
-η log η − (1 − η) log(1 − η)

φ(v)
max(1 − v , 0)
exp(−v)
log(1 + e−v )

f ∗
φ (η)
sign(2η − 1)
2 log η
1
1−η
log η
1−η

φ ]−1 (v)
[f ∗
NA
e2v
1+e2v
ev
1+ev

Examples of optimal link functions include f ∗ = 2η − 1 and f ∗ = log η
1−η . The associated optimal
classi ﬁer h∗ = sign[f ∗ ] is the well known Bayes decision rule (BDR), and the associated minimum
conditional (zero-one) risk is
0/1 (η) = η (cid:18) 1
C ∗
2

sign(2η − 1)(cid:19) + (1 − η) (cid:18) 1
2

sign(2η − 1)(cid:19) .

(4)

1
2

−

1
2

+

A loss which is minimized by the BDR is Bayes consistent. A number of Bayes consistent alter-
natives to the 0-1 loss are commonly used. These include the exponential loss of boosting, the log
loss of logistic regression, and the hinge loss of SVMs. They have the form Lφ (f , y) = φ(yf ), for
different functions φ. These functions assign a non-zero penalty to small positive yf , encouraging
the creation of a margin, a property not shared by the 0-1 loss. The resulting large-margin classi ﬁers
have better generalization than those produced by the latter [1]. The associated conditional risk

Cφ (η , f ) = ηφ(f ) + (1 − η)φ(−f ).

(5)

is minimized by the link

f ∗
φ (η) = arg min
f
leading to the minimum conditional risk function C ∗
φ ). Table 1 lists the loss, optimal
φ (η) = Cφ (η , f ∗
link, and minimum risk of some of the most popular classi ﬁer d esign methods.

Cφ (η , f )

(6)

Conditional risk minimization is closely related to classical probability elicitation in statistics [4].
Here, the goal is to ﬁnd the probability estimator ˆη that maximizes the expected reward

I (η , ˆη) = ηI1 ( ˆη) + (1 − η)I−1 ( ˆη),

(7)

where I1 ( ˆη) is the reward for prediction ˆη when event y = 1 holds and I−1 ( ˆη) the corresponding
reward when y = −1. The functions I1 (·), I−1 (·) should be such that the expected reward is
maximal when ˆη = η , i.e.

I (η , ˆη) ≤ I (η , η) = J (η), ∀η
with equality if and only if ˆη = η . The conditions under which this holds are as follows.
Theorem 1.
[4] Let I (η , ˆη) and J (η) be as deﬁned in (7) and (8). Then 1) J (η) is convex and
2) (8) holds if and only if

(8)

I1 (η) = J (η) + (1 − η)J ′ (η)
I−1 (η) = J (η) − ηJ ′ (η).

(9)
(10)

Hence, starting from any convex J (η), it is possible to derive I1 (·), I−1 (·) so that (8) holds. This
enables the following connection to risk minimization.
Theorem 2. [3] Let J (η) be as deﬁned in (8) and f a continuous function. If the following proper-
ties hold

1. J (η) = J (1 − η),

2. f is invertible with symmetry

f −1 (−v) = 1 − f −1 (v),

(11)

3

then the functions I1 (·) and I−1 (·) derived with (9) and (10) satisfy the following equalities

I1 (η) = −φ(f (η))
I−1 (η) = −φ(−f (η)),

with

φ(v) = −J [f −1 (v)] − (1 − f −1 (v))J ′ [f −1 (v)].

(12)
(13)

(14)

Under the conditions of the theorem, I (η , ˆη) = −Cφ (η , f ). This establishes a new path for classi ﬁer
design [3]. Rather than specifying a loss φ and minimizing Cφ (η , f ), so as to obtain whatever
optimal link f ∗
φ and minimum expected risk C ∗
φ (η) results, it is possible to specify f ∗
φ and C ∗
φ (η)
and derive, from (14) with J (η) = −C ∗
φ (η), the underlying loss φ. The main advantage is the ability
to control directly the quantities that matter for classi ﬁc ation, namely the predictor and risk of the
φ (1 − η) and (11) holds for f ∗
optimal classi ﬁer.The only conditions are that C ∗
φ .
φ (η) = C ∗

3 Canonical risk minimization

In general, given J (η) = −C ∗
φ (η), there are multiple pairs (φ, f ∗
φ ) that satisfy (14). Hence, speci-
ﬁcation of either the minimum risk or optimal link does not co mpletely characterize the loss. This
makes it difﬁcult to control some important properties of th e latter, such as the margin. In this work,
we consider an important special case, where such control is possible. We start with a lemma that
relates the symmetry conditions, on J (η) and f ∗
φ (η), of Theorem 2.
Lemma 3. Let J (η) be a strictly convex and differentiable function such that J (η) = J (1 − η).
Then J ′ (η) is invertible and

[J ′ ]−1 (−v) = 1 − [J ′ ]−1 (v).

(15)

Hence, under the conditions of Theorem 2, the derivative of J (η) has the same symmetry as f ∗
φ (η).
Since this symmetry is the only constraint on f ∗
φ , the former can be used as the latter. Whenever this
holds, the risk is said to be in canonical form, and (f ∗ , J ) are denoted a canonical pair [6] .
Deﬁnition 1. Let J (η) be as deﬁned in (8), and C ∗
φ (η) = −J (η) a minimum risk. If the optimal link
associated with C ∗
φ (η) is

f ∗
φ (η) = J ′ (η)
(16)
the risk Cφ (η , f ) is said to be in canonical form. f ∗
φ (η) is denoted a canonical link and φ(v), the
loss given by (14), a canonical loss.

Note that (16) does not hold for all risks. For example, the risk of boosting is derived from the
convex, differentiable, and symmetric J (η) = −2pη(1 − η). Since this has derivative
1
2η − 1
η
J ′ (η) =
= f ∗
6=
log
φ (η),
pη(1 − η)
1 − η
2
the risk is not in canonical form. What follows from (16) is that it is possible to derive a canonical
risk for any maximal reward J (η), including that of boosting (J (η) = −2pη(1 − η)). This is
discussed in detail in Section 5.
φ (η), and then using (14)
While canonical risks can be easily designed by specifying either J (η) or f ∗
and (16), it is much less clear how to directly specify a loss φ(v) for which (14) holds with a
canonical pair (f ∗ , J ). The following result solves this problem.
Theorem 4. Let Cφ (η , f ) be the canonical risk derived from a convex and symmetric J (η). Then

(17)

φ′ (v) = −[J ′ ]−1 (−v) = [f ∗
φ ]−1 (v) − 1.

(18)

4

= (cid:882) 1(cid:73)’
(cid:73)’
’
=
0

)

(cid:73)(
v
0.5

1

Canonical Boosting

 

16

15

14

13

12

11

10

9

8

7

k
n
a
R
 
.
g
v
A

6

 

9

7

5

3
0.8
1
margin parameter

0.6

0.4

0.2

 

8

6

10

18

16

14

12

k
n
a
R
 
.
g
v
A

Canonical Log

0(cid:73)’
(cid:73)’
=
(cid:73)’
= (cid:882) 0
.
5
0
’
=
v
(cid:882)1
f (cid:73)*
[
(
v
)
]
v
Figure 1: Left: canonical losses compatible with an IS optimal link. Right: Average classiﬁcation rank as a
function of margin parameter, on the UCI data.
This theorem has various interesting consequences. First, it establishes an easy-to-verify necessary
1
condition for the canonical form. For example, logistic regression has [f ∗
1+e−v and
φ ]−1 (v) =
φ′ (v) = − e−v
1
1+e−2v and φ′ (v) = −e−v 6=
φ ]−1 (v) − 1, while boosting has [f ∗
1+e−v = [f ∗
φ ]−1 (v) =
φ ) shows that the former is in canonical form
φ ]−1 (v) − 1. This (plus the symmetry of J and f ∗
[f ∗
but the latter is not. Second, it makes it clear that, up to additive constants, the three components
φ ) of a canonical risk are related by one-to-one relationships. Hence, it is possible to
(φ, C ∗
φ , and f ∗
control the properties of the three components of the risk by manipulating a single function (which
can be any of the three). Finally, it enables a very detailed characterization of the losses compatible
with most optimal links of Table 1.

3
1
0.8
margin parameter

0.6

0.4

0.2

4

 

5

9

7

4

Inverse-sigmoidal links

Inspection of Table 1 suggests that the classi ﬁers produced by boosting, logistic regression, and vari-
ants have sigmoidal inverse links [f ∗
φ ]−1 . Due to this, we refer to the links f ∗
φ as inverse-sigmoidal
(IS). When this is the case, (18) provides a very detailed characterization of the loss φ. In particular,
it can be trivially shown that, letting f (n) be the nth order derivative of f , that the following hold

(19)

lim
v→−∞

φ(1) (v) = −1

[f ∗
φ ]−1 (v) = 0 ⇔ lim
v→−∞
[f ∗
φ(1) (v) = 0
φ ]−1 (v) = 1 ⇔ lim
lim
v→∞
v→∞
([f ∗
φ ]−1 )(n) (v) = 0, n ≥ 1 ⇔ lim
lim
v→±∞
v→±∞
φ ]−1 (v) ∈ (0, 1) ⇔ φ(v) monotonically decreasing
[f ∗
φ ]−1 (v) monotonically increasing ⇔ φ(v) convex
[f ∗
φ ]−1 (0) = .5 ⇔ φ(1) (0) = −.5.
[f ∗
It follows that, as illustrated in Figure 1, the loss φ(v) is convex, monotonically decreasing, linear
(with slope −1) for large negative v , constant for large positive v , and has slope −.5 at the origin.
The set of losses compatible with an IS link is, thus, strongly constrained. The only degrees of
freedom are in the behavior of the function around the origin. This is not surprising, since the only
degrees of freedom of the sigmoid itself are in its behavior within this region.

φ(n+1) (v) = 0, n ≥ 1

(22)

(23)

(21)

(24)

(20)

5

Canonical Log, a=0.4
Canonical Log, a=1
Canonical Log, a=10

)
v
(
1
−
]
*φ
f
[

1

0.8

0.6

0.4

0.2

0

 

−0.6

−0.4

−0.2

0
v

0.2

0.4

0.6

)
v
(
φ

10

9

8

7

6

5

4

3

2

1

0

 
−10

−8

−6

−4

−2

 

 

)
v
(
1
−
]
*φ
f
[

1

0.8

0.6

0.4

0.2

0

 

Canonical Boosting, a=0.4
Canonical Boosting, a=1
Canonical Boosting, a=10

)
v
(
φ

20

18

16

14

12

10

8

6

4

2

0

−1

−0.5

0
v

0.5

1

 
−20

−15

−10

−5

 

Canonical Log, a=0.4
Canonical Log, a=1
Canonical Log, a=10

2

4

6

8

10

 

Canonical Boosting, a=0.4
Canonical Boosting, a=1
Canonical Boosting, a=10

5

10

15

20

0
v

0
v

Figure 2: canonical link (left) and loss (right) for various values of a. (Top) logistic, (bottom) boosting.

What is interesting is that these are the degrees of freedom that control the margin characteristics
of the loss φ. Hence, by controlling the behavior of the IS link around the origin, it is possible to
control the margin of the optimal classi ﬁer. In particular,
the margin is a decreasing function of the
curvature of the loss at the origin, φ(2) (0). Since, from (18), φ(2) (0) = ([f ∗
φ ]−1 )(1) (0), the margin
can be controlled by varying the slope of [f ∗
φ ]−1 at the origin.

5 Variable margin loss functions

The results above enable the derivation of families of canonical losses with controllable margin. In
Section 3, we have seen that the boosting loss is not canonical, but there is a canonical loss for the
minimum risk of boosting. We consider a parametric extension of this risk,

J (η ; a) =

From (16), the canonical optimal link is

−2
a pη(1 − η),

a > 0.

f ∗
φ (η ; a) =

and it can be shown that

2η − 1
apη(1 − η)
av
2p4 + (av)2
is an IS link, i.e. satis ﬁes (19)-(24). Using (18), the corre sponding canonical loss is

φ ]−1 (v ; a) =
[f ∗

1
2

+

(25)

(26)

(27)

1
(p4 + (av)2 − av).
2a
Because it shares the minimum risk of boosting, we refer to this loss as the canonical boosting loss.
It is plotted in Figure 2, along with the inverse link, for various values of a. Note that the inverse

φ(v ; a) =

(28)

6

Table 2: Margin parameter value a of rank 1 for each of the ten UCI datasets.

UCI dataset#
Canonical Log
Canonical Boost

#1
0.4
0.9

#2
0.5
6

#3
0.6
2

#4
0.3
2

#5
0.1
0.4

#6
2
3

#7
0.5
0.2

#8
0.1
4

#9
0.2
0.2

#10
0.2
0.9

link is indeed sigmoidal, and that the margin is determined by a. Since φ(2) (0; a) = a
4 , the margin
increases with decreasing a.

It is also possible to derive variable margin extensions of existing canonical losses. For example,
consider the parametric extension of the minimum risk of logistic regression

J (η ; a) =

1
a

η log(η) +

1
a

(1 − η) log(1 − η).

From (16),

1
η
1 − η
a
This is again a sigmoidal inverse link and, from (18),

[f ∗
φ ](v ; a) =

log

φ ]−1 (v ; a) =
[f ∗

eav
1 + eav

.

(29)

(30)

φ(v ; a) =

[log(1 + eav ) − av ] .

1
a
We denote this loss the canonical logistic loss. It is plotted in Figure 2, along with the corresponding
inverse link for various a. Since φ(2) (0; a) = a
4 , the margin again increases with decreasing a.
Note that, in (28) and (31), margin control is not achieved by simply rescaling the domain of the loss
function. e.g. just replacing log(1 + e−v ) by log(1 + e−av ) in the case of logistic regression. This
would have no impact in classi ﬁcation accuracy, since it wou ld just amount to a change of scale of the
original feature space. While this type of re-scaling occurs in both families of loss functions above
(which are both functions of av ), it is localized around the origin, and only inﬂuences the m argin
properties of the loss. As can be seen seen in Figure 2 all loss functions are identical away from the
origin. Hence, varying a is conceptually similar to varying the bandwidth of an SVM kernel. This
suggests that the margin parameter a could be cross-validated to achieve best performance.

(31)

6 Experiments

A number of easily reproducible experiments were conducted to study the effect of variable mar-
gin losses on the accuracy of the resulting classi ﬁers. Ten b inary UCI data sets were considered:
(#1)sonar, (#2)breast cancer prognostic, (#3)breast cancer diagnostic, (#4)original Wisconsin breast
cancer, (#5)Cleveland heart disease, (#6)tic-tac-toe, (#7)echo-cardiogram, (#8)Haberman’s survival
(#9)Pima-diabetes and (#10)liver disorder. The data was split into ﬁve folds, four used for train-
ing and one for testing. This produced ﬁve training-test pai rs per dataset. The GradientBoost
algorithm [8], with histogram-based weak learners, was then used to design boosted classi ﬁers
which minimize the canonical logistic and boosting losses, for various margin parameters. Gra-
dientBoost was adopted because it can be easily combined with the different losses, guaranteeing
that, other than the loss, every aspect of classi ﬁer design i s constant. This makes the compari-
son as fair as possible. 50 boosting iterations were applied to each training set, for 19 values of
a ∈ {0.1, 0.2, ..., 0.9, 1, 2, ..., 10}. The classi ﬁcation accuracy was then computed per dataset, by
averaging over its ﬁve train/test pairs.

Since existing algorithms can be seen as derived from special cases of the proposed losses, with a =
1, it is natural to inquire whether other values of the margin parameter will achieve best performance.
To explore this question we show, in Figure-1, the average rank of the classi ﬁer designed with each
loss and margin parameter a. To produce the plot, a classi ﬁer was trained on each dataset
, for all
19 values of a. The results were then ranked, with rank 1 (19) being assigned to the a parameter of
smallest (largest) error. The ranks achieved with each a were then averaged over the ten datasets, as
suggested in [9]. For the canonical logistic loss, the best values of a is in the range 0.2 ≤ a ≤ 0.3.
Note that the average rank for this range (between 5 and 6), is better than that (close to 7) achieved
with the logistic loss of LogitBoost [2] (a = 1). In fact, as can be seen from Table 2, the canonical

7

Table 3: Classiﬁcation error for each loss function and UCI dataset.

UCI dataset#
Canonical Log
LogitBoost (a = 1)
Canonical Boost
Canonical Boost, a = 1
AdaBoost

#1
11.2
11.6
12.6
13.2
11.4

#2
11.4
12.4
11.6
12.4
11.4

#3
8
10
21
21
9.4

#4
5.6
6.6
18.6
18.6
6.4

#5
12.4
13.4
17.6
18.6
14

#6
11.8
48.6
7.2
50.8
28

#7
7
6.8
6
7.2
6.6

#8
18.8
21.2
21.8
21.2
21.8

#9
38.2
39.6
37.6
39.4
41.2

#10
27
28.4
28.6
28.2
28.2

Table 4: Classiﬁcation error for each loss function and UCI dataset.

UCI dataset#
Canonical Log, a = 0.2
Canonical Boost, a = 0.2
LogitBoost (a = 1)
AdaBoost

#1
13.2
12.6
12.4
11.4

#2
15
14.8
15.4
15.2

#3
8.4
17.2
8.6
9.2

#4
5
18.6
5.6
6

#5
11.2
12
11.4
11.4

#6
56.2
56.8
46
21.6

#7
6.8
6.8
7.2
7.4

#8
24
23.2
25
23.2

#9
39.8
38.4
40.4
42.8

#10
25.8
26.4
26.4
26.6

logistic loss with a = 1 did not achieve rank 1 on any dataset, whereas canonical logistic losses with
0.2 ≤ a ≤ 0.3 were top ranked on 3 datasets (and with 0.1 ≤ a ≤ 0.4 on 6). For the canonical
boosting loss, there is also a range (0.8 ≤ a ≤ 2) that produces best results. We note that the a
values of the two losses are not directly comparable. This can be seen from Figure-2 where a = 0.4
produces a loss of much larger margin for canonical boosting. Furthermore, the canonical boosting
loss has a heavier tail and approaches zero more slowly than the canonical logistic loss.

Although certain ranges of margin parameters seem to produce best results for both canonical loss
functions, the optimal parameter value is likely to be dataset dependent. This is conﬁrmed by Table 2
which presents the parameter value of rank 1 for each of the ten datasets. Improved performance
should thus be possible by cross-validating the margin parameter a. Table 3 presents the 5-fold
cross validation test error (# of misclassi ﬁed points) obta ined for each UCI dataset and canonical
loss. The table also shows the results of AdaBoost, LogitBoost (canonical logistic, a = 1), and
canonical boosting loss with a = 1. Cross validating the margin results in better performance for
9 out of 10 (8 out 10) datasets for the canonical logistic (boosting) loss, when compared to the
ﬁxed margin ( a = 1) counterparts. When compared to the existing algorithms, at least one of the
margin-tunned classi ﬁers is better than both Logit and AdaB oost for each dataset.

Under certain experimental conditions, cross validation might not be possible or computationally
feasible. Even in this case, it may be better to use a value of a other than the standard a = 1. Table-4
presents results for the case where the margin parameter is ﬁ xed at a = 0.2 for both canonical loss
functions. In this case, canonical logistic and canonical boosting outperform both LogitBoost and
AdaBoost in 7 and 5 of the ten datasets, respectively. The converse, i.e. LogitBoost and AdaBoost
outperforming both canonical losses only happens in 2 and 3 datasets, respectively.

7 Conclusion

The probability elicitation approach to loss function design, introduced in [3], enables the derivation
of new Bayes consistent loss functions. Yet, because the procedure is not fully constructive, this
requires trial and error. In general, it is difﬁcult to antic ipate the properties, and shape, of a loss
function that results from combining a certain minimal risk with a certain link function. In this
work, we have addressed this problem for the class of canonical risks. We have shown that the
associated canonical loss functions lend themselves to analysis, due to a simple connection between
the associated minimum conditional risk and optimal link functions. This analysis was shown to
enable a precise characterization of 1) the relationships between loss, optimal link, and minimum
risk, and 2) the properties of the loss whenever the optimal link is in the family of inverse sigmoid
functions. These properties were then exploited to design parametric families of loss functions
with explicit margin control. Experiments with boosting algorithms derived from these variable
margin losses have shown better performance than those of classical algorithms, such as AdaBoost
or LogitBoost.

8

References

[1] V. N. Vapnik, Statistical Learning Theory.
John Wiley Sons Inc, 1998.
[2] J. Friedman, T. Hastie, and R. Tibshirani, “Additive logistic regress ion: A statistical view of boosting,”
Annals of Statistics, 2000.
[3] H. Masnadi-Shirazi and N. Vasconcelos, “On the design of loss fu nctions for classiﬁcation: theory, robust-
ness to outliers, and savageboost,” in NIPS, 2008, pp. 1049–1056.
[4] L. J. Savage, “The elicitation of personal probabilities and expectatio ns,” Journal of the American Statisti-
cal Association, vol. 66, pp. 783–801, 1971.
[5] C. Leistner, A. Saffari, P. M. Roth, and H. Bischof, “On robustne ss of on-line boosting - a competitive
study,” in IEEE ICCV Workshop on On-line Computer Vision, 2009.
[6] A. Buja, W. Stuetzle, and Y. Shen, “Loss functions for binary class probability estimation and classiﬁcation:
Structure and applications,” 2006.
[7] T. Zhang, “Statistical behavior and consistency of classiﬁcation meth
tion,” Annals of Statistics, 2004.
[8] J. H. Friedman, “Greedy function approximation: A gradient boos ting machine,” The Annals of Statistics,
vol. 29, no. 5, pp. 1189–1232, 2001.
[9] J. Dem ˇsar, “Statistical comparisons of classiﬁers over multiple data sets,”
Research, vol. 7, pp. 1–30, 2006.

ods based on convex risk minimiza-

The Journal of Machine Learning

9


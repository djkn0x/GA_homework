A Novel Kernel for Learning a Neuron Model from
Spike Train Data

Nicholas Fisher, Arunava Banerjee
Department of Computer and Information Science and Engineering
University of Florida
Gainesville, FL 32611
{nfisher,arunava}@cise.ufl.edu

Abstract

From a functional viewpoint, a spiking neuron is a device that transforms input
spike trains on its various synapses into an output spike train on its axon. We
demonstrate in this paper that the function mapping underlying the device can be
tractably learned based on input and output spike train data alone. We begin by
posing the problem in a classiﬁcation based framework. We then derive a novel
kernel for an SRM0 model that is based on PSP and AHP like functions. With
the kernel we demonstrate how the learning problem can be posed as a Quadratic
Program. Experimental results demonstrate the strength of our approach.

1

Introduction

Neurons are the predominant component of the nervous system and understanding them is a ma-
jor challenge in modern neuroscience research [1]. Many neuron models have been proposed to
understand the dynamics of individual and populations of neurons. Although these models vary
in complexity, at a fundamental level they are mechanisms which transform input spike trains into
an output spike train. This view has found expression in the Quantitative Single-Neuron Modeling
competition where submitted models compete on how accurately they can predict the output spike
train of a biological neuron given an input current [2]. Since the vast majority of neurons receive
input from chemical synapses [3], a stricter stipulation would be to predict output spikes based on
input spike trains at the various synapses of the neuron. There are advantages to this variation of
the problem: complicated subthreshold ﬂuctuations in the membrane potential need not be modeled,
since models are now judged strictly on the basis of their performance at predicting the timing of
output spikes. Models now have the liberty to focus on threshold crossings at the expense of be-
ing inaccurate in the subthreshold regime. Not only does the model better represent the functional
complexity of the input/output transformation of a neuron, comparisons to the real neuron can be
conducted in a non-invasive manner.
In this paper we learn a Spike Response Model 0 (SRM0 )[4] approximation of a neuron by only
considering the timing of all afferent (incoming) and efferent (outgoing) spikes of the neuron over
a bounded past. We begin by formulating the problem in a classiﬁcation based supervised learning
framework where spike train data is labeled according to whether the neuron is about to spike, or
has recently spiked. We demonstrate that optimizing the model to properly classify this labeled data
naturally leads to a quadratic programming problem when combined with an appropriate represen-
tation of the model via a dictionary of functions. We then derive a novel kernel on spike trains which
is computed from a dictionary of post-synaptic potential (PSP) and after-hyperpolarizing potential
(AHP) like functions. Finally, experimental results are presented to demonstrate the efﬁcacy of the
approach. For a complementary approach to learning a neuron model from spike train data, see [5].

1

An SRM0 model was chosen for several reasons. First, SRM0 has been shown to be fairly versatile
and accurate at modeling biological neurons [6]. Second, SRM0 is a relatively simple neuron model,
and therefore is likely to display better generalizability on unseen input. Finally, the disparity be-
tween the learned neuron model and the actual neuron could shed light on the various operational
modes of biological neurons. It is conceivable that the learned SRM0 model accurately predicts the
behavior of the neuron a majority of the time. However, there could be states, bursting for example,
where the prediction diverges. In such a case, the neuron can be seen as operating in two differ-
ent modes, one SRM0 like, and the other not. Multiple models could then be learned to model the
neuron in its various operational modes.

2 General model of the neuron

It has been shown, that if one assumes a neuron to be a ﬁnite precision device with fading memory
and a refractory period, then the membrane potential of the neuron, P , can be modeled as a function
of the timing of the neuron’s afferent and efferent spikes which have occurred within a bounded
past [7]. Spikes that have aged past this bound, denoted by Υ, are considered to have a negligible
effect on the present value of P . We denote the arrival times of spikes at synapse j using the vector
tj = (cid:104)tj
(cid:105), where Nj is bounded from above by the number of spikes that can be present
1 , tj
2 . . . tj
Nj
in an Υ window of time. t0 represents the output spike train of the neuron and vectors t1 . . . tm
time. We can then formalize the membrane potential function P : RN → R, where N = (cid:80)m
represent spike trains on the input synapses. tj
i represents the time that has elapsed since that spike
was generated or received by the neuron. Spikes are only considered if they occurred within Υ
j=0 Nj .
P (t0 , . . . , tm ) is deﬁned over the space of all spike trains and reports the present membrane potential
of the neuron. The neuron generates a spike when P (t0 , . . . , tm ) = Θ and dP /dt ≥ 0, where Θ is
the threshold of the neuron. For notational simplicity, we deﬁne the spike conﬁguration, s ∈ RN ,
which represents the timing of all afferent and efferent spikes within the window of length Υ. s is
the vector of vectors, s = (cid:104)t0 , . . . , tm (cid:105). The neuron generates a spike when P (s) = Θ, dP /dt ≥ 0.
As discussed in Section 1, we shall learn an SRM0 approximation of the neuron. The SRM0 model
uses a bounded past history as described above to calculate the present membrane potential of the
neuron. The present membrane potential ˆP is calculated as shown in Equation 1. η models the effect
of a past generated spike, the AHP. j represents the response of the neuron to a presynaptic spike at
synapse j , the PSP. urest is the resting membrane potential. At any given time, the neuron generates
a spike if the membrane potential crosses the threshold from below (i.e., ˆP (s) = Θ, d ˆP /dt ≥ 0).
Nj(cid:88)
m(cid:88)
N0(cid:88)
i=1
j=1
i=1

j (tj
i ) + urest

η(t0
i ) +

ˆP (s) =

(1)

3 Classiﬁcation Problem

In order to learn an SRM0 approximation of a neuron in a non-invasive manner, we pose a super-
vised learning classiﬁcation problem which labels the given spike train data according to whether
the neuron is about to spike or has recently spiked. We denote the former S − and the latter S + . This
problem is equivalent to classifying subthreshold spike conﬁgurations ( ˆP (s) < Θ) from suprathresh-
old spike conﬁgurations ( ˆP (s) ≥ Θ), which leads to the classiﬁcation problem shown in Equation 2.
It should be noted that the true membrane potential function, P , is a feasible solution to this problem
(cid:13)(cid:13)(cid:13) ˆP (s)
(cid:13)(cid:13)(cid:13)2
since P (s) < Θ ∀s ∈ S − and P (s) ≥ Θ ∀s ∈ S + .
s.t. ˆP (s) − Θ ≥ 1 ∀s ∈ S + AND ˆP (s) − Θ ≤ −1 ∀s ∈ S −
To generate training data which belong to S + and S − , we provide the spike conﬁgurations which
occur at a ﬁxed inﬁnitesimal time differential before and after the neuron generates a spike, as
illustrated in Figure 1(a). The spike train at the instant the neuron generated a spike is shown by
the solid lines. We shift the spike window inﬁnitesimally into the past (future) to produce a spike
conﬁguration s ∈ S − (S + ), shown by the up (down) arrows. Notice that the spike which is currently

Min.

(2)

2

generated in the output spike train, t0 , emphasized by the dashed circle, is not included in either
spike conﬁguration s. The reason it is not included in s ∈ S − is that it simply has not been generated
at that point in time. The reason it is not included in s ∈ S + is twofold. First, the spike would induce
an AHP effect which would cause the membrane potential to fall below the threshold. Second, if it
were included, this would cause the classiﬁer to only consider whether or not that particular spike
existed when classifying a given spike conﬁguration as a member of S + or S − . If it did exist, it
would belong to S + , and if it did not exist it would belong to S − . Although this method would work
well for the training data, it would not generalize to unseen live spike train data.

Figure 1: Figure (a) depicts the spike conﬁgurations used in the classiﬁcation problem. Figure (b)
shows the REEF for a ﬁxed value of t = 1s and variable β and τ values. Figure (c) portrays the
form of cross sections of the REEF as a function of t for different values of β and τ .

Producing a hypersurface which can separate the supra-threshold spike conﬁgurations from the sub-
threshold spike conﬁgurations within the spike time feature space, would be extremely difﬁcult.
As discussed above, if we could map a given spike conﬁguration s to its corresponding membrane
potential P (s), then the classiﬁcation problem is trivial. Although we do not have access to the
membrane potential function, we can use a linear combination of functions from a dictionary to
reproduce an approximation to the membrane potential function P . The choice of the dictionary is
crucial. By choosing a dictionary which is tailored to the form of typical PSP and AHP functions,
we increase the likelihood of successfully modeling the given neuron.
of functions of the individual spikes of the spike conﬁguration ( ˆP (s) = (cid:80)m
(cid:80)Nj
The SRM0 model is an additively separable model [8], that is, the membrane potential is a sum
ˆPij (tj
i )). This
i=1
j=0
feature lends itself well to modeling the membrane potential using a linear combination of dictionary
elements. The dictionary used here was one derived from a function used by MacGregor and Lewis
for neuron modeling [9]. It consists of functions (parametrized by β and τ ) of the form
· exp(−β /t) · exp(−t/τ )

1
τ
We call this the reciprocal exponential – exponential function (REEF) dictionary. Figures1(b) and
(c) present the dictionary for various cross sections of t, β and τ .

fβ ,τ (t) =

(3)

4 Approximation of the membrane potential function

We would like to combine members of the chosen dictionary of functions to construct an approxima-
tion of the membrane potential function, P , which will yield a solution to the classiﬁcation problem
posed in Equation 2. We shall ﬁrst discuss how this can be achieved in a discrete setting, where we
combine a ﬁnite number of β and τ parametrized dictionary functions to model P . Following this
we will discuss a continuous formulation, in which we combine elements drawn from an inﬁnite
continuous range of β and τ parametrized dictionary functions to model P . In the context of the
continuous formulation, we will prove a speciﬁc instance of the Representer theorem which was ﬁrst
shown by Kimeldorf and Wahba [10]. The Representer theorem shows that the optimal solution to
the posed classiﬁcation problem must lie in the span of the data points which were used to train the
classiﬁer. In the discrete and continuous formulation, we will ﬁrst model the effect of a single spike
for simplicity. We will conclude this section by extending the continuous formulation to the case of
multiple spikes on a single synapse, and the case of multiple spikes on multiple synapses.

3

135051000.25τREEF as a function of β and τβ0336610000.51Vary τ (β=0.5)Time (ms)  τ = 10τ = 20τ = 30τ = 400336610000.51Vary β (τ=20)Time (ms)  β=0β=5β=10β=15Pasttttt0123S+S−a)b)c)4.1 Discrete Formulation

In the discrete formulation, we wish to approximate the membrane potential function using a linear
combination of a ﬁnite, predeﬁned set of functions from the REEF dictionary. Focusing on the
single spike case, our goal is to model the effect of a single spike on the membrane potential. We
denote this effect on the membrane potential by ˆP and it is deﬁned as a linear combination of
τ · exp(−β /t) · exp(−t/τ ) is
parametrized REEF functions as shown in Equation 4. ft (β , τ ) = 1
now a univariate function over t for ﬁxed values of β and τ . A speciﬁc set of parameter settings
{(β1 , τ1 ), . . . , (βM , τ1 ), (β1 , τ2 ), . . . , (βM , τN )} are used to construct a ˆP that can best reproduce
the effect of the spike on the membrane potential. Inserting Equation 4 into Equation 2 yields a
M(cid:88)
N(cid:88)
quadratic optimization problem on the mixing coefﬁcients αi,j ’s.
i=1
j=1
The major disadvantage of the discrete formulation is that for any given neuron, the optimal value set
of the β ’s and τ ’s is unlikely to be known beforehand. While one can argue that the approximation
ˆP can be improved by increasing M and N , as the number of functions increases, so does the
dimensionality of the feature space. Since M and N can be increased independent of the size of
the training dataset, the procedure is susceptible to over-ﬁtting. To resolve this issue, we shift to a
continuous formulation of the problem, which by virtue of the Representer theorem does not suffer
from the rising feature space dimensionality issue. The dimensionality of the feature space is now
controlled by the span of the training dataset.

αi,j ft (βi , τj )

ˆP (t) =

(4)

4.2.1 Proof

4.2 Continuous formulation
In the continuous formulation, we consider L2 , the Hilbert space of square integrable functions on
the domain {β , τ } ∈ [0, ∞)2 . We are concerned with ﬁnding a threshold dependent classiﬁcation
function ˆP , such that ˆP (t) ≥ Θ + 1 when the spike t ∈ S + and ˆP (t) ≤ Θ − 1 when t ∈ S − . This
(cid:90) ∞
(cid:90) ∞
function is deﬁned in Equation 5.
ˆP (t) = (cid:104)α(β , τ ), ft (β , τ )(cid:105) =
(5)
α(β , τ )ft (β , τ ) dβ dτ
0
0
In this formulation, the mixing function, α(β , τ ), is by deﬁnition a member of L2 . Therefore, if
ft (β , τ ) ∈ L2 , then ˆP (t) is ﬁnite by the Cauchy-Schwartz inequality since (cid:104)α(β , τ ), ft (β , τ )(cid:105) ≤
(cid:107)α(β , τ )(cid:107) · (cid:107)ft (β , τ )(cid:107) < ∞ if both (cid:107)α(β , τ )(cid:107) < ∞ and (cid:107)ft (β , τ )(cid:107) < ∞. To show that ft (β , τ ) ∈
L2 we must show (cid:104)ft (β , τ ), ft (β , τ )(cid:105) < ∞. For ease of readability we shall henceforth suppress
the domain variables in ft (β , τ ) and α(β , τ ) and refer to them as ft and α.
(cid:19)
(cid:18)
(cid:19)
(cid:18)
(cid:90) ∞
(cid:90) ∞
(cid:16)− y
(cid:17) 1
(cid:16)− x
τ
τ
τ
0
0
xy
=
(x + y)2
Therefore (cid:104)ft , ft (cid:105) = t·t
4 < ∞ ∀t ∈ [, ∞) for some  > 0.
(t+t)2 = 1
We must note here that by deﬁning the membrane potential function in this manner, we have formu-
lated a problem which yields a solution which is different from the solution to the discrete problem.
Since the delta function centered at any arbitrary point (β ∗ , τ ∗ ) does not belong to L2 , the mixing
function α cannot be made up of a linear combination of these delta functions, as is the case in the
discrete formulation. In addition, we are not working with a reproducing kernel Hilbert space since
we are considering L2 . However, our deﬁnition in Equation 5 deﬁnes the “point evaluation” of our
membrane potential function.
Since ˆP (t) is deﬁned using the standard inner product in L2 with respect to particular members of
L2 , we can reformulate the classiﬁcation problem in Equation 2 as shown in Equation 8. Here M is

(cid:104)fx , fy (cid:105) =

− β
x

− β
y

(6)

(7)

1
τ

exp

(cid:17)

exp

exp

exp

dβ dτ

4

the number of data points, m = 1 . . . M , and ym is the corresponding classiﬁcation for spike time
tm (that is, ym = +1 if tm ∈ S + and ym = −1 if tm ∈ S − ).
Min. (cid:107)α(cid:107)2
s.t. ym ((cid:104)α, ftm (cid:105) − Θ) ≥ 1 m = {1 . . . M }
for α to the optimization problem speciﬁed in Equation 8 can be expressed as α = (cid:80)M
(8)
We can now use a speciﬁc instance of the Representer theorem [10] to show that the optimal solution
k=1 νk ftk .
We can then substitute this equality back into Equation 8 to produce a dual formulation of the
optimization problem, which is a standard quadratic programming problem.

(9)

α =

νk ftk

4.2.2 Representer Theorem
M(cid:88)
For some ν1 , ν2 , . . . νM ∈ R, the solution to Equation 8 can be written in the form
k=1
Proof We consider the subspace of L2 spanned by the REEF functions evaluated at the times of
the given training data points (span{ ftk : 1 ≤ k ≤ M }). We then consider the projection α(cid:107) of α
on this subspace. By noting α = α(cid:107) + α⊥ and rewriting Equation 8 in its Lagrangian form, we are
left with Equation 10. However, by the deﬁnition of α⊥ , (cid:104)α⊥ , ftk (cid:105) = 0, which then simpliﬁes the
M(cid:88)
(cid:2)1 − yk
(cid:0)(cid:104)α(cid:107) , ftk (cid:105) + (cid:104)α⊥ , ftk (cid:105) − Θ(cid:1)(cid:3)
summation term of Equation 10 to only depend upon α(cid:107) as shown in Equation 11.
Min. (cid:107)α(cid:107)2 +
M(cid:88)
(cid:2)1 − yk
(cid:0)(cid:104)α(cid:107) , ftk (cid:105) − Θ(cid:1)(cid:3)
k=1
Min. (cid:107)α(cid:107)2 +
k=1
In addition, by considering the relation shown in Equation 12, we ﬁnd that the ﬁrst term is minimized
when α = α(cid:107) . Hence, the optimal solution to Equation 8 will lie in the aforementioned subspace
and therefore have the form of Equation 9.
(cid:107)α(cid:107)2 = (cid:107)α(cid:107) (cid:107)2 + (cid:107)α⊥(cid:107)2 ≥ (cid:107)α(cid:107) (cid:107)2

(10)

(11)

(12)

λk

λk

4.2.3 Dual Representation

We can now substitute the form of the optimal solution shown in Equation 9 back into the original
optimization problem shown in Equation 8. This leads to the problem in Equation 13 which is
equivalent to Equation 14. The resultant quadratic programming problem is solvable given that we
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) M(cid:88)
(cid:32)(cid:42) M(cid:88)
(cid:43)
(cid:33)
have access to the positive deﬁnite matrix K , which was derived in Section 4.2.1 and is shown in
Equation 15.
≥ 1 m = {1 . . . M }
(cid:33)
(cid:32) M(cid:88)
Min.
νk ftk , ftm
M(cid:88)
M(cid:88)
k=1
k=1
νkK (tk , tm ) − Θ
(cid:90) ∞
(cid:90) ∞
νi νj K (ti , tj ) s.t. ym
j=1
i=1
k=1
K (ti , tj ) = (cid:104)fti , ftj (cid:105) =
0
0

≥ 1 m = {1 . . . M }

ti tj
(ti + tj )2

fti ftj dβ dτ =

s.t. ym

− Θ

νk ftk

Min.

(14)

(13)

(15)

4.3 Single Synapse

We are now in a position to extend the framework to multiple spikes on a single synapse. Since
we are learning an SRM0 approximation of a neuron, we assume that the effects of spikes are
additively separable [8] and that each spike’s effect on the membrane potential for the given synapse
is identical. Introducing the latter assumption is the core contribution of this section. We ﬁrst deﬁne
the threshold dependent classiﬁcation function for a single spike in a manner identical to that of
the single spike formulation shown in Equation 5. This will be the “stereotyped” effect that a spike
arriving at this synapse has on the membrane potential. Note that the AHP effect of the output spike
train can be modeled seamlessly (as a virtual synapse) in this framework.

5

4.3.1 Primal Problem

We now consider the additive effects of multiple spikes arriving at a synapse. We deﬁne the vector
tm = (cid:104)tm
(cid:105) to be the mth data point, which consists of Nm spikes, represented by
2 , . . . , tm
1 , tm
Nm
their spike times. Note that we have abused notation. Instead of the superscript repeatedly referring
to the synapse in question, it now refers to the data point. The primal optimization problem, deﬁned
(cid:32) Nm(cid:88)
(cid:33)
(cid:10)α, ftm
(cid:11) − Θ
in Equation 16, is equivalent to Equation 17.
Min. (cid:107)α(cid:107)2 s.t. ym
≥ 1 m = {1 . . . M }
(cid:33)
(cid:43)
(cid:32)(cid:42)
Nm(cid:88)
h
h=1
Min. (cid:107)α(cid:107)2 s.t. ym
≥ 1 m = {1 . . . M }
− Θ
The Representer theorem states that the optimal α must lie in span{(cid:80)Nk
ftm
α,
h
h=1
: 1 ≤ k ≤ M }. We
i=1 ftk
i
omit the formal proof since it follows along the lines of the previous case. Therefore, the optimal α
Nk(cid:88)
M(cid:88)
to Equation 17 will be of the form
i=1
k=1

(16)

(17)

(18)

α =

ftk
i

νk

4.3.2 Dual Problem
Substituting back Equation 18 yields the dual problem Equation 19, which can be solved given the
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) M(cid:88)
(cid:43)
(cid:33)
(cid:32)(cid:42) M(cid:88)
Nm(cid:88)
Nk(cid:88)
Nk(cid:88)
positive deﬁnite kernel in Equation 20.
≥ 1 m = {1 . . . M } (19)
s.t. ym
ftk
,
(cid:42) Np(cid:88)
(cid:68)
Nq(cid:88)
Np(cid:88)
Nq(cid:88)
Np(cid:88)
Nq(cid:88)
i
i=1
i=1
h=1
k=1
k=1
K (tp , tq ) =
i=1
i=1
i=1
k=1
k=1
k=1

i · tq
tp
k
k )2
(tp
i + tq

− Θ
(cid:69)

νk
(cid:43)

Min.

, ftq
k

(20)

ftm
h

ftk
i

ftq
k

ftp
i

νk

ftp
i

,

=

=

4.4 Multiple Synapses

In the multiple synapse case, the principles are identical to that of the single synapse, with the
exception that spikes arriving at different synapses could have different effects on the membrane
potential, depending on the strength/type of the synaptic junction. Therefore, we keep the effects of
each synapse on the membrane potential separate by assigning each synapse its own α function.

4.4.1 Primal Problem

Since each synapse and the output has its own α function, this simply adds another summation term
over the S synapses and the output (indexed by 0). The primal optimization problem is deﬁned in
Equation 21 which is equivalent to Equation 22. S is the number of synapses, Nm,s is the number
of spikes on the sth synapse of the mth data point, and tm,s
is the timing of the hth spike on the sth
 S(cid:88)
 ≥ 1 m = {1 . . . M }
h
(cid:69) − Θ
(cid:68)
S(cid:88)
Nm,s(cid:88)
synapse of the mth data point.
αs , ftm,s
 S(cid:88)
 ≥ 1 m = {1 . . . M }
(cid:42)
(cid:43)
h
Nm,s(cid:88)
S(cid:88)
s=0
s=0
h=1
− Θ
s=0
s=0
h=1
theorem states that
lie in
the sth synapses must
for
the optimal αs
: 1 ≤ k ≤ M }. This is identical to the single synapse case for each synapse,

Min.
span{(cid:80)Nk,s
The Representer
i=1 ftk,s
i

(cid:107)αs(cid:107)2 s.t. ym

(cid:107)αs (cid:107)2 s.t. ym

ftm,s
h

Min.

(21)

(22)

αs ,

6

Min.

νk

,

(23)

αs =

ftk,s
i

M(cid:88)
Nk,s(cid:88)
and therefore, the optimal αs to Equation 22 will be of the form
i=1
k=1
4.4.2 Dual Problem
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) M(cid:88)
Substituting Equation 23 into Equation 22 yields the dual problem shown in Equation 24 which can
Nk,s(cid:88)
S(cid:88)
be solved given access to the positive deﬁnite kernel deﬁned in Equation 25.
 S(cid:88)
 ≥ 1 m = {1 . . . M }
(cid:42) M(cid:88)
νk
ftk,s
Nk,s(cid:88)
i
s=0
i=1
k=1
− Θ
νk
ftk,s
(cid:42)Np,s(cid:88)
Nq,s(cid:88)
Np,s(cid:88)
Nq,s(cid:88)
S(cid:88)
i
s=0
i=1
k=1
· tq ,s
tp,s
i
K (tp , tq ) =
k
k )2
i + tq ,s
(tp,s
s=0
i=1
i=1
k=1
k=1

(cid:43)
ftm,s
S(cid:88)
h
s=0

Nm,s(cid:88)
(cid:43)
h=1

ftp,s
i

,

ftq,s
k

=

s.t.

ym

(24)

(25)

4.5 Summary

With the above kernels we are able to formulate quadratic programming problems which can be
solved with SVMlight [11]. The choice of the dictionary used to derive the kernel is critical to the
success of this technique. A dictionary of functions tailored to the forms of PSPs and AHPs will
perform better than a more general class of functions. The properties of the REEF dictionary which
make it suitable for this problem are its exponential decay as well as its additive separability [8]. This
explains why a Gaussian radial basis function (GRBF) does not work well for this problem. The
GRBF kernel is not additive. A slight variation of the GRBF which takes the sum of Gaussian
functions, rather than their product, was also explored. This performed better than the GRBF;
however it did not perform well when applied to more complicated neurons.

5 Results

To test the kernel we learned SRM0 neurons with increasing levels of complexity. We ﬁrst con-
sidered a simplistic neuron which only received spikes on a single synapse. We then increased the
complexity of the neuron, by introducing AHP effects as well as different types (excitatory and in-
hibitory) of afferent synapses with varying synaptic weights. The PSP effect was modeled via the
classical alpha function [PSP(t) = C · t · exp(−t/τ )] while the AHP effect was modeled by an
exponential function[AHP(t) = K · exp(−t/τ )]. Although we learned neurons with varying com-
plexity, for want of space, we discuss here the case of a single neuron that received input spike trains
from 4 excitatory synapses and 1 inhibitory synapse to mimic the ratio of connections observed in
the cortex [12]. The stereotyped PSP for the excitatory and inhibitory synapses differed in their rise
and fall times. The parameters for the stereotyped PSP were set as follows. For the excitatory PSP,
C = 0.1 and τ = 10, where t is in units of milliseconds. For the inhibitory PSP, C = −0.39 and
τ = 5. For the AHP, K = −16.667 and τ = 2.
We ﬁrst trained the classiﬁer using 100,000 seconds of spike train data. Only the spike conﬁgurations
occurring at ﬁxed differentials before and after the neuron emitted a spike were considered. The in-
put spike trains were generated using an inhomogeneous Poisson process, where the rate was varied
sinusoidally around the intended mean spike rate in order to produce a more general set of training
data. This resulted in 1,647,249 training data points, however only 10,681 of them were used in
(cid:17)
(cid:16) correct classiﬁcations
the solution as support vectors. After training, we tested our model using 100 seconds of unseen
data. All spike conﬁgurations were considered when testing, regardless of temporal proximity to
(cid:16) correct negative classiﬁcations
(cid:16) correct positive classiﬁcations
(cid:17)
(cid:17)
spike generation. To quantify our results, we ﬁrst calculated the accuracy
,
total data points
total negative data points
total positive data points

, and speciﬁcity

the sensitivity

.

7

Figure 2: Figure (a) shows histograms of the difference in time between the actual and predicted
spike time by the learned model. Figure (b) shows the various PSP approximations (gray) in com-
parison to the PSP functions used by the neuron (black). Figure (c) depicts the AHP approximation
(gray) and the AHP function used by the neuron (black).

They were 0.9947, 0.9532 and 0.9948 respectively. We also calculated a histogram of how close
the spike predictions were. For every spike produced by the neuron, we determined the temporal
proximity of the closest spike time predicted by the model. We then histogrammed this data. Figure
2(a) shows two histograms depicting these calculations. The larger histogram contains predictions
with time differences varying between 0 and 70 ms, with a bin size of 1 ms while the inlaid his-
togram ranges from 0 to 10 ms and has a bin size of 0.1 ms. Both use a logarithmic scale on the
y-axis. From the histograms, we see that the vast majority of spikes were predicted correctly (with
a temporal proximity of 0 ms) and that out of the mispredicted spike times, the temporal proximity
of all predicted spikes fell within 70 ms of the actual spike time.
In Figures 2(b) and 2(c) we display a comparison of the approximated PSP and AHP versus the true
PSP and AHP. To calculate the classiﬁcation model’s approximated PSP we artiﬁcially send a single
spike across each input synapse. We artiﬁcially generate a spike to produce the AHP approximation.
By considering the distance of the single spike data point from the classiﬁer’s margin as the spike
ages, we can get a scaled and translated version of the PSP and AHP. The ﬁgures show these ap-
proximations scaled and translated back appropriately. In Figure 2(b) we show the approximations
of the PSPs for the input synapses. The approximations are shown in gray; the true PSPs are shown
in black. The different line styles are representative of the different synapses and therefore have
varying synaptic weights. A similar image for the AHP is shown in Figure 2(c). We note that there
are small differences between the approximated and the true functions. If the PSP and AHP ap-
proximations were exact, we would have seen perfect classiﬁcation results. However, as with most
machine learning techniques, the quality of the solution is limited by the training data given.

6 Conclusion

In this paper we have developed a classiﬁcation framework which uses a novel kernel derived from
a REEF dictionary to produce an SRM0 approximation of a neuron. The technique used is non-
invasive in the sense that it only requires the timing of afferent and efferent spikes within a certain
bounded past. The REEF dictionary was chosen due to its similarity to PSP and AHP functions used
in a neuron model proposed by MacGregor and Lewis [9].
By producing an SRM0 approximation, which is additively separable [8], we produce a model which
is both versatile and accurate [6].
In addition, it is a relatively simple model, which allows for
increased generalizability to unseen input. The simplicity of the SRM0 model has the potential to
allow us to observe deviations between the model and the neuron, which can lead to insights on the
various behavioral modes of neurons.

Acknowledgments

This work was supported by a National Science Foundation grant (NSF IIS-0902230) to A.B.

8

0204060100101102103FrequencySpike Time Difference (ms)020406080100−1.5−1−0.500.51VoltageTime (ms)020406080100−20−15−10−505VoltageTime (ms)00.20.40.60.81100101102103FrequencySpike Time Difference (ms)b)c)a)References
[1] R. Jolivet, A. Roth, F. Sch ¨urmann, W. Gerstner, and W. Senn. Special issue on quantitative
neuron modeling. Biological Cybernetics, 99(4):237–239, 2008.
[2] W. Gerstner and R. Naud. How Good Are Neuron Models? Science, 326(5951):379–380,
2009.
[3] W. Gerstner and W. Kistler. Spiking Neuron Models: An Introduction. Cambridge University
Press New York, NY, USA, 2002.
[4] R. Jolivet, T.J. Lewis, and W. Gerstner. The spike response model: a framework to pre-
dict neuronal spike trains. Artiﬁcial Neural Networks and Neural Information Processing–
ICANN/ICONIP 2003, pages 173–173, 2003.
[5] L. Paninski, J.W. Pillow, and E.P. Simoncelli. Maximum likelihood estimation of a stochastic
integrate-and-ﬁre neural encoding model. Neural Computation, 16(12):2533–2561, 2004.
[6] R. Jolivet, T.J. Lewis, and W. Gerstner. Generalized integrate-and-ﬁre models of neuronal
activity approximate spike trains of a detailed model to a high degree of accuracy. Journal of
Neurophysiology, 92(2):959–976, 2004.
[7] A. Banerjee. On the phase-space dynamics of systems of spiking neurons. I: Model and exper-
iments. Neural Computation, 13(1):161–193, 2001.
[8] Tadeusz Stanisz. Functions with separated variables. Master’s thesis, Zeszyty Naukowe Uni-
werstyetu Jagiellonskiego, 1969.
[9] R.J. MacGregor and E.R. Lewis. Neural Modeling. Plenum Press, New York, 1977.
[10] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of
Mathematical Analysis and Applications, 33(1):82–95, 1971.
[11] T. Joachims. Making large-scale support vector machine learning practical. In Advances in
Kernel Methods, pages 169–184. MIT Press, 1999.
[12] E.M. Izhikevich. Simple model of spiking neurons. IEEE Transactions on Neural Networks,
14(6):1569–1572, 2003.

9


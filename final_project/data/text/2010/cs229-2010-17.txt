High Frequency Trading Strategy Based on Prex Trees

Yijia Zhou, 05592862, Financial Mathematics, Stanford University

December 11, 2010

1

Introduction

1.1 Goal

I am an M.S. Finanical Mathematics student pursuing a career in professionl trading; therefore I wish to leverage
what I learned in the class to the development of a high frequency trading strategy based on pattern recognition.
Specically, I wish to achieve two goals:

1. The model should successfully predict future returns given historical data

2. The model should lead to a consistently protable trading strategy with low risk

1.2 Challenge

High-frequency nancial data (min-by-min) is known to be able to pass most martingale tests.
E [Xt+s |Ft ] = Xt
In plain English, it means given historical information, the best prediction of the future is the current state.
That is, the data usually has no predictive value at all in conventional methods. A special technique must be
developed to extract information from nancial time series data.
Another special challenge in high-frequency trading is that the model must be able to make trading decisions
and update itself quickly to keep pace with the high-frequency streaming data. Therefore the algorithm complexity
is constrained.

2 Unsuccessful Conventional Regression/Classication Methods

Conventional regression/classication methods work poorly. This is not surprising because of the nature of nancial
data.

2.1 Regression: Time Series Model

q(cid:88)
p(cid:88)
Apply ARMA(p,q) to historical data. Use AIC to determine optimal p, q :
i=1
i=1

Xt = c + t +

Ï†pXtâˆ’p +

Î¸q tâˆ’q

Failure: Model diagnostics are poor; typical signicance level (out of dierent training set/testing set combina-
tions)=0.6210

Reason: The high-frequency data is known to pass martingale tests. It is improbable that we can extract infor-
mation in time-series methodology.

1

Discretize return into alphabet {-1,0,1}:

2.2 Classication: Multinomial Event Model
ï£±ï£´ï£²ï£´ï£³âˆ’1 ri > a
a â‰¥ ri â‰¥ âˆ’b
0
ri < âˆ’b
1
ï£¹ï£ºï£ºï£ºï£ºï£»
ï£®ï£¯ï£¯ï£¯ï£¯ï£°
(a, b are decision variables) Use a sliding window of m minutes,
1
0
0
...âˆ’1

zi =

X =

The (m + 1)th element is seen as the class label.

I tried 10/20/30-min as sliding window length, but the average probability of prediction being right is
Failure:
only 31%. There is no dierence to rolling a die.

Reason: The conditional independence assumption

P (x1 , ..., xn |y) =

n(cid:89)
i=1

P (xi |y)

is not valid beucase in terms of nancial data, because the precedence of events happening matters a lot in
nancial data (that is exactly the time series pattern I am looking for):

though both having 4 1's and 2 0's, are very dierent from each other in time series.

z1 = [111001] and z2 = [101011]

3 Methodology: Theoretical Framework

3.1 Leaning: Tree Construction

Discretize returns into {-1,0,1} rst. For the purpose of demonstration, I use a binary tree here. Suppose I observe
a sequence

z = {100011110011101100101110010101101}
It is parsed sequentially into a series of patterns that have not been observed before. For example, z1 = {1}, z2 =
{0}, z3 = {00} but not {0} because it has been observed at z2 = {0}. Similarly z4 = {11} but not {1}. Thus z is
parsed to

z = {1}; {0}; {00}; {11}; {110}; {01}; {1101}; {10}; {010}; {111}; {001}; {0101}; {101};
These are the basis patterns. Then I encode the data in a binary tree; the left child being event 0, the right
child being event 1; the node value being the number of occurence. Each time we observe a new pattern, the values
of all nodes on the path grow by 1. For example, after updating the tree by z1 = {1}, I get

2

update after

z2 = {0}

z3 = {00}

tree
Do this recursively and I get the fully-grown tree for the sequence z ,

Note that the value of each nodes equals the sum of its children plus 1. In implementation parsing (which can
take advantage of the tree to decide whether a pattern has appeared already) and updating are interweaved. The
tree is developed and used in a Bayesian mindset, and that is why I call it tree of conditional frequencies.

3.2 Prediction

After the model has nished learning from historical data, we then need to use it in prediction and trading. With no
real-time streaming market data yet, our best guess of what happens next is from the perspective of the root looking
at its children. Now if I receive the streaming data z (cid:48) = {1}, the best estimation is then from the perspective of the
right node. The square block in the tree graph above indicates the scenario of z (cid:48) = {11}. By Laplace Smoothing
the estimation of the return in the next period is
3 = 0|{11}) =
P (z (cid:48)

3 = 1|{11}) =
, P (z (cid:48)

3
5

2
5

3.3 Trading

Prediction alone does not suce; trading style matters, too. I need to optimize the trading policy Ï€ that works
best with the prediction model. Think this way: a more aggressive trader may trade as long as one outcome is
more probable than the other, to fully take advantage of the Law of Large Numbers. However, this may lead to
increased trading costs, and possible losses in a streak (thus larger risk).

3

A more defensive trader initiates trades only when she is condent enough of the probability of one outcome
happening; therefore with each trade protability is more probable.
In this study I specify two trading styles
following the rules below:
â€¢ Aggressive Trading

 If I already have a long position, sell only if the most probable outcome is {-1}. Vice versa.

 If I have no position at hand, initiate trade if the most probable outcome is in {1,-1}.
â€¢ Defensive Trading
 Change the criterion for initiating/holding long positions to P (1|Ft ) âˆ’ P (0 or âˆ’ 1|Ft ) > 

I estimate  and discretizatio parameters a, b using cross-validation. In a base case I assume a = b. My historical
data are split evenly to 8 sections for cross-validation.

3.4 Model Evaluation

Two criteria are useful in evaluating how the model summarizes the data. Dene

Compression Ratio =

Internal Node Ratio =

length(source)
length(encoding)

#(internal nodes)
#(leaves)

Compression ratio measures the randomness of the historical nancial data. The larger compression ratio, the
less random the data are. Internal node ratio measures the diversity of source patternsThe model is more predictive
if there are fewer frequent patterns in the data.

4 Empirical Results

4.1 Data and Trading Assumptions

To make the study as representitative as possible, I retrived the 2-min/5-min/10-min market data of JPY/USD
(foreign exchange, or FX), S&P 500 (equity index) and IBM (equity).
I assume a trading environment for institutional traders. That is, 400:1 leverge (which magnies the prot and
loss by 400 times) is available in FX trading and 20:1 leverage is available in equities. The in-and-out trading cost,
including commissions and spreads, is 2 bps (1 bps=0.01%) for FX and 0.2 cents for equities.

4.2 Results and Interpretation

Due to the limit of pages I show the graphs for aggressive trading style only. The defensive style results will be
summarized in a table later.
First, the compression ratio is steady against the training data size. This suggests the entropy/predictibility of
nancial data does not vary.
The internal node ratio grows steadily in a log n fashion, due to the nature of trees.

4

The model works much better with foreign exchange assets. For equities, unfortunately, it is prone to overtting.
Before trading return dropping dramatically, the daily return is insensitive to discretization parameters a, b.
This is an excellent property.

Using optimal model parameters, the returns (after trading costs) of aggressive/defensive trading are
Daily Return Aggressive Defensive
JPY/USD
0.21%
0.23%
0.10%
0.01%
S&P 500
IBM
-0.10%
0.01%
It is obvious that FX still works the best, while defensive trading can largely enhance the performance of equities
trading.

5 Conclusions
â€¢ Kraft Inequality says the expected codeword length must be greater than or equal to the entropy of the
encoded source. That is, the (lossless) compression ratio cannot be arbitrarily large. An asymptotically
optimal encoding technique is Human Code, which movitates this method.
â€¢ Market price data are clearly compressible, which implies that there are some  preferred paths , that is, the
ternary tree is not uniformly even  some parts are  bushier than others. This suggests that the chain of
event state is not completely  memoryless  the development of the next event state is  path-dependent .
â€¢ Currency data has the nice  high frequency feature.
â€¢ Properly chosen trading strategies might earn decent return.
â€¢ The training phase complexity is O(n log n). The trading phase complexity is O(1) for each new streaming
data point. This is excellent because the algorithm can run very fast so as not to delay making trading
decisions in a high frequency setting.

5


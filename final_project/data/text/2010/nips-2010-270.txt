Multi-View Active Learning in
the Non-Realizable Case

Wei Wang and Zhi-Hua Zhou
National Key Laboratory for Novel Software Technology
Nanjing University, Nanjing 210093, China
{wangw,zhouzh}@lamda.nju.edu.cn

Abstract

The sample complexity of active learning under the realizability assumption has
been well-studied. The realizability assumption, however, rarely holds in prac-
tice. In this paper, we theoretically characterize the sample complexity of active
learning in the non-realizable case under multi-view setting. We prove that, with
unbounded Tsybakov noise, the sample complexity of multi-view active learning
can be eO(log 1
ǫ ), contrasting to single-view setting where the polynomial improve-
ment is the best possible achievement. We also prove that in general multi-view
setting the sample complexity of active learning with unbounded Tsybakov noise
is eO( 1
ǫ ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise,
contrasting to previous polynomial bounds where the order of 1/ǫ is related to the
parameter in Tsybakov noise.

1

Introduction

In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution
deﬁned on the learning task and actively queries some labels from an oracle. In this way, the active
learner can achieve good performance with much fewer labels than passive learning. The number
of these queried labels, which is necessary and sufﬁcient for obtaining a good leaner, is well-known
as the sample complexity of active learning.

Many theoretical bounds on the sample complexity of active learning have been derived based on the
realizability assumption (i.e., there exists a hypothesis perfectly separating the data in the hypothesis
class) [4, 5, 11, 12, 14, 16]. The realizability assumption, however, rarely holds in practice. Recently,
the sample complexity of active learning in the non-realizable case (i.e., the data cannot be perfectly
separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].
It is worth noting that these bounds obtained in the non-realizable case match the lower bound Ω( η2
ǫ2 )
[19], in the same order as the upper bound O( 1
ǫ2 ) of passive learning (η denotes the generalization
error rate of the optimal classiﬁer in the hypothesis class and ǫ bounds how close to the optimal
classiﬁer in the hypothesis class the active learner has to get). This suggests that perhaps active
learning in the non-realizable case is not as efﬁcient as that in the realizable case. To improve the
sample complexity of active learning in the non-realizable case remarkably, the model of the noise
or some assumptions on the hypothesis class and the data distribution must be considered. Tsybakov
noise model [21] is more and more popular in theoretical analysis on the sample complexity of active
learning. However, existing result [8] shows that obtaining exponential improvement in the sample
complexity of active learning with unbounded Tsybakov noise is hard.

Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity
of active learning in the realizable case remarkably, we have an insight that multi-view setting will
also help active learning in the non-realizable case. In this paper, we present the ﬁrst analysis on the

1

sample complexity of active learning in the non-realizable case under multi-view setting, where the
non-realizability is caused by Tsybakov noise. Speciﬁcally:

-We deﬁne α-expansion, which extends the deﬁnition in [3] and [23] to the non-realizable case,
and β -condition for multi-view setting.

-We prove that the sample complexity of active learning with Tsybakov noise under multi-view
setting can be improved to eO(log 1
ǫ ) when the learner satisﬁes non-degradation condition.1 This
exponential improvement holds no matter whether Tsybakov noise is bounded or not, contrasting to
single-view setting where the polynomial improvement is the best possible achievement for active
learning with unbounded Tsybakov noise.

-We also prove that, when non-degradation condition does not hold, the sample complexity of ac-
tive learning with unbounded Tsybakov noise under multi-view setting is eO( 1
ǫ ), where the order of
1/ǫ is independent of the parameter in Tsybakov noise, i.e., the sample complexity is always eO( 1
ǫ )
no matter how large the unbounded Tsybakov noise is. While in previous polynomial bounds, the
order of 1/ǫ is related to the parameter in Tsybakov noise and is larger than 1 when unbounded Tsy-
bakov noise is larger than some degree (see Section 2). This discloses that, when non-degradation
condition does not hold, multi-view setting is still able to lead to a faster convergence rate and our
polynomial improvement in the sample complexity is better than previous polynomial bounds when
unbounded Tsybakov noise is large.

The rest of this paper is organized as follows. After introducing related work in Section 2 and
preliminaries in Section 3, we deﬁne α-expansion in the non-realizable case in Section 4. We analyze
the sample complexity of active learning with Tsybakov noise under multi-view setting with and
without the non-degradation condition in Section 5 and Section 6, respectively. Finally we conclude
the paper in Section 7.

2 Related Work

Generally, the non-realizability of learning task is caused by the presence of noise. For learning the
task with arbitrary forms of noise, Balcan et al. [2] proposed the agnostic active learning algorithm
A2 and proved that its sample complexity is bO( η2
ǫ2 ).2 Hoping to get tighter bound on the sample
complexity of the algorithm A2 , Hanneke [17] deﬁned the disagreement coefﬁcient θ , which depends
on the hypothesis class and the data distribution, and proved that the sample complexity of the
algorithm A2 is bO(θ2 η2
ǫ2 ). Later, Dasgupta et al. [13] developed a general agnostic active learning
algorithm which extends the scheme in [10] and proved that its sample complexity is bO(θ η2
ǫ2 ).
Recently, the popular Tsybakov noise model [21] was considered in theoretical analysis on ac-
tive learning and there have been some bounds on the sample complexity. For some simple cases,
where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sam-
ple complexity is possible [4, 7, 18]. As for the situation where Tsybakov noise is unbounded,
only polynomial improvement in the sample complexity has been obtained. Balcan et al. [4] as-
sumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample
complexity of active learning with unbounded Tsybakov noise is O(cid:0)ǫ− 2
1+λ (cid:1) (λ > 0 depends on
Tsybakov noise). This uniform distribution assumption, however, rarely holds in practice. Castro
and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov
noise is bO(cid:0)ǫ− 2µω+d−2ω−1
(cid:1) (µ > 1 depends on another form of Tsybakov noise, ω ≥ 1 depends
µω
on the H ¨older smoothness and d is the dimension of the data). This result is also based on the
strong uniform distribution assumption. Cavallanti et al. [9] assumed that the labels of examples
are generated according to a simple linear noise model and indicated that the sample complexity
of active learning with unbounded Tsybakov noise is O(cid:0)ǫ−
(1+λ)(2+λ) (cid:1). Hanneke [18] proved that
2(3+λ)
the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity
1+λ (cid:1) for active learning with unbounded Tsybakov noise. For active learning with unbounded
bO(cid:0)ǫ− 2
Tsybakov noise, Castro and Nowak [8] also proved that at least Ω(ǫ−ρ ) labels are requested to learn
1The eO notation is used to hide the factor log log( 1
ǫ ).
2The bO notation is used to hide the factor poly log( 1
ǫ ).
2

an ǫ-approximation of the optimal classiﬁer (ρ ∈ (0, 2) depends on Tsybakov noise). This result
shows that the polynomial improvement is the best possible achievement for active learning with un-
bounded Tsybakov noise in single-view setting. Wang [22] introduced smooth assumption to active
learning with approximate Tsybakov noise and proved that if the classiﬁcation boundary and the
underlying distribution are smooth to ξ -th order and ξ > d, the sample complexity of active learning
is bO(cid:0)ǫ− 2d
ξ+d (cid:1); if the boundary and the distribution are inﬁnitely smooth, the sample complexity of
ǫ )(cid:1). Nevertheless, this result is for approximate Tsybakov noise and
active learning is O(cid:0)poly log( 1
the assumption on large smoothness order (or inﬁnite smoothness order) rarely holds for data with
high dimension d in practice.

3 Preliminaries

In multi-view setting, the instances are described with several different disjoint sets of features. For
the sake of simplicity, we only consider two-view setting in this paper. Suppose that X = X1 × X2
is the instance space, X1 and X2 are the two views, Y = {0, 1} is the label space and D is the
distribution over X ×Y . Suppose that c = (c1 , c2 ) is the optimal Bayes classiﬁer, where c1 and c2 are
the optimal Bayes classiﬁers in the two views, respectively. Let H1 and H2 be the hypothesis class
in each view and suppose that c1 ∈ H1 and c2 ∈ H2 . For any instance x = (x1 , x2 ), the hypothesis
hv ∈ Hv (v = 1, 2) makes that hv (xv ) = 1 if xv ∈ Sv and hv (xv ) = 0 otherwise, where Sv is a
subset of Xv . In this way, any hypothesis hv ∈ Hv corresponds to a subset Sv of Xv (as for how to
combine the hypotheses in the two views, see Section 5). Considering that x1 and x2 denote the same
instance x in different views, we overload Sv to denote the instance set {x = (x1 , x2 ) : xv ∈ Sv }
without confusion. Let S ∗
v correspond to the optimal Bayes classiﬁer cv . It is well-known [15] that
v = {xv : ϕv (xv ) ≥ 1
2 }, where ϕv (xv ) = P (y = 1|xv ). Here, we also overload S ∗
S ∗
v to denote
the instances set {x = (x1 , x2 ) : xv ∈ S ∗
v }. The error rate of a hypothesis Sv under the distribution
D is R(hv ) = R(Sv ) = P r(x1 ,x2 ,y)∈D (cid:0)y 6= I(xv ∈ Sv )(cid:1). In general, R(S ∗
v ) 6= 0 and the excess
v − Sv ) and d(Sv , S ∗
error of Sv can be denoted as follows, where Sv∆S ∗
v = (Sv − S ∗
v ) ∪ (S ∗
v ) is a
pseudo-distance between the sets Sv and S ∗
v .
v ) = ZSv ∆S ∗
R(Sv ) − R(S ∗
v
Let ηv denote the error rate of the optimal Bayes classiﬁer cv which is also called as the noise rate
in the non-realizable case. In general, ηv is less than 1
2 . In order to model the noise, we assume that
the data distribution and the Bayes decision boundary in each view satisﬁes the popular Tsybakov
noise condition [21] that P rxv ∈Xv (|ϕv (xv ) − 1/2| ≤ t) ≤ C0 tλ for some ﬁnite C0 > 0, λ > 0
and all 0 < t ≤ 1/2, where λ = ∞ corresponds to the best learning situation and the noise is called
bounded [8]; while λ = 0 corresponds to the worst situation. When λ < ∞, the noise is called
unbounded [8]. According to Proposition 1 in [21], it is easy to know that (2) holds.
v ) ≥ C1dk
∆ (Sv , S ∗
d(Sv , S ∗
v )

|2ϕv (xv ) − 1|pxv dxv

(2)

, d(Sv , S ∗
v )

(1)

λ , C1 = 2C −1/λ
Here k = 1+λ
λ(λ + 1)−1−1/λ , d∆ (Sv , S ∗
v ) + P r(S ∗
v ) = P r(Sv − S ∗
v − Sv ) is also
0
a pseudo-distance between the sets Sv and S ∗
v , and d(Sv , S ∗
v ) ≤ d∆ (Sv , S ∗
v ) ≤ 1. We will use the
following lamma [1] which gives the standard sample complexity for non-realizable learning task.

Lemma 1 Suppose that H is a set of functions from X to Y = {0, 1} with ﬁnite VC-dimension
V ≥ 1 and D is the ﬁxed but unknown distribution over X × Y . For any ǫ, δ > 0, there is a
positive constant C , such that if the size of sample {(x1 , y1 ), . . . , (xN , yN )} from D is N (ǫ, δ) =
ǫ2 (cid:0)V + log( 1
δ )(cid:1), then with probability at least 1 − δ , for all h ∈ H, the following holds.
C
N XN
I(cid:0)h(xi ) 6= y i (cid:1) − E(x,y)∈D I(cid:0)h(x) 6= y(cid:1)| ≤ ǫ
1
|
i=1
4 α-Expansion in the Non-realizable Case

Multi-view active learning ﬁrst described in [20] focuses on the contention points (i.e., unlabeled
instances on which different views predict different labels) and queries some labels of them. It is
motivated by that querying the labels of contention points may help at least one of the two views
to learn the optimal classiﬁer. Let S1 ⊕ S2 = (S1 − S2 ) ∪ (S2 − S1 ) denote the contention points

3

Table 1: Multi-view active learning with the non-degradation condition
Input: Unlabeled data set U = {x1 , x2 , · · · , } where each example xj is given as a pair (xj
1 , xj
2 )
Process:
Query the labels of m0 instances drawn randomly from U to compose the labeled data set L
iterate: i = 0, 1, · · · , s
Train the classiﬁer hi
v (v = 1, 2) by minimizing the empirical risk with L in each view:
v = arg minh∈Hv P(x1 ,x2 ,y)∈L I(h(xv ) 6= y);
hi
Apply hi
1 and hi
2 to the unlabeled data set U and ﬁnd out the contention point set Qi ;
Query the labels of mi+1 instances drawn randomly from Qi , then add them into L and delete them
from U .
end iterate
+ and hs
Output: hs
−

between S1 and S2 , then P r(S1 ⊕ S2 ) denotes the probability mass on the contentions points. “∆”
and “⊕” mean the same operation rule. In this paper, we use “∆” when referring the excess error
between Sv and S ∗
v and use “⊕” when referring the difference between the two views S1 and S2 . In
order to study multi-view active learning, the properties of contention points should be considered.
One basic property is that P r(S1 ⊕ S2 ) should not be too small, otherwise the two views could be
exactly the same and two-view setting would degenerate into single-view setting.

In multi-view learning, the two views represent the same learning task and generally are consistent
with each other, i.e., for any instance x = (x1 , x2 ) the labels of x in the two views are the same.
Hence we ﬁrst assume that S ∗
1 = S ∗
2 = S ∗ . As for the situation where S ∗
1 6= S ∗
2 , we will discuss on it
further in Section 5.2. The instances agreed by the two views can be denoted as (S1 ∩S2 )∪ (S1 ∩S2 ).
However, some of these agreed instances may be predicted different label by the optimal classiﬁer
S ∗ , i.e., the instances in (S1 ∩ S2 − S ∗ ) ∪ (S1 ∩ S2 − S ∗ ). Intuitively, if the contention points
can convey some information about (S1 ∩ S2 − S ∗ ) ∪ (S1 ∩ S2 − S ∗ ), then querying the labels of
contention points could help to improve S1 and S2 . Based on this intuition and that P r(S1 ⊕ S2 )
should not be too small, we give our deﬁnition on α-expansion in the non-realizable case.
Deﬁnition 1 D is α-expanding if for some α > 0 and any S1 ⊆ X1 , S2 ⊆ X2 , (3) holds.
P r(cid:0)S1 ⊕ S2 (cid:1) ≥ α(cid:16)P r(cid:0)S1 ∩ S2 − S ∗ (cid:1) + P r(cid:0)S1 ∩ S2 − S ∗ (cid:1)(cid:17)
We say that D is α-expanding with respect to hypothesis class H1 × H2 if the above holds for all
S1 ∈ H1 ∩ X1 , S2 ∈ H2 ∩ X2 (here we denote by Hv ∩ Xv the set {h ∩ Xv : h ∈ Hv } for v = 1, 2).
Balcan et al. [3] also gave a deﬁnition of expansion, P r(T1 ⊕ T2 ) ≥ α min (cid:2)P r(T1 ∩ T2 ), P r(T1 ∩
T2 )(cid:3), for realizable learning task under the assumptions that the learner in each view is never “conﬁ-
dent but wrong” and the learning algorithm is able to learn from positive data only. Here Tv denotes
the instances which are classiﬁed as positive conﬁdently in each view. Generally, in realizable learn-
ing tasks, we aim at studying the asymptotic performance and assume that the performance of initial
classiﬁer is better than guessing randomly, i.e., P r(Tv ) > 1/2. This ensures that P r(T1 ∩ T2 ) is
larger than P r(T1 ∩ T2 ). In addition, in [3] the instances which are agreed by the two views but are
predicted different label by the optimal classiﬁer can be denoted as T1 ∩ T2 . So, it can be found that
Deﬁnition 1 and the deﬁnition of expansion in [3] are based on the same intuition that the amount of
contention points is no less than a fraction of the amount of instances which are agreed by the two
views but are predicted different label by the optimal classiﬁers.

(3)

5 Multi-view Active Learning with Non-degradation Condition

In this section, we ﬁrst consider the multi-view learning in Table 1 and analyze whether multi-
view setting can help improve the sample complexity of active learning in the non-realizable case
remarkably. In multi-view setting, the classiﬁers are often combined to make predictions and many
strategies can be used to combine them. In this paper, we consider the following two combination
schemes, h+ and h− , for binary classiﬁcation:
+ (x) = (cid:26) 1 if hi
− (x) = (cid:26) 0 if hi
1 (x1 ) = hi
1 (x1 ) = hi
2 (x2 ) = 1
2 (x2 ) = 0
hi
hi
1 otherwise
0 otherwise

(4)

4

1 = S ∗
5.1 The Situation Where S ∗
2

(5)

(6)

+ and hi
With (4), the error rate of the combined classiﬁers hi
− satisfy (5) and (6), respectively.
R(hi
+ ) − R(S ∗ ) = R(S i
1 ∩ S i
2 ) − R(S ∗ ) ≤ d∆ (S i
1 ∩ S i
2 , S ∗ )
R(hi
− ) − R(S ∗ ) = R(S i
1 ∪ S i
2 ) − R(S ∗ ) ≤ d∆ (S i
1 ∪ S i
2 , S ∗ )
Here S i
v ⊂ Xv (v = 1, 2) corresponds to the classiﬁer hi
v ∈ Hv in the i-th round. In each round of
multi-view active learning, labels of some contention points are queried to augment the training data
set L and the classiﬁer in each view is then reﬁned. As discussed in [23], we also assume that the
learner in Table 1 satisﬁes the non-degradation condition as the amount of labeled training examples
increases, i.e., (7) holds, which implies that the excess error of S i+1
is no larger than that of S i
v in
v
the region of S i
1 ⊕ S i
2 .
v ∆S ∗ (cid:12)(cid:12)S i
v∆S ∗ (cid:12)(cid:12)S i
P r(cid:0)S i+1
2 (cid:1) ≤ P r(S i
1 ⊕ S i
1 ⊕ S i
2 )
To illustrate the non-degradation condition, we give the following example: Suppose the data in
Xv (v = 1, 2) fall into n different clusters, denoted by πv
1 , . . . , πv
n , and every cluster has the same
probability mass for simplicity. The positive class is the union of some clusters while the negative
class is the union of the others. Each positive (negative) cluster πv
ξ in Xv is associated with only
(ξ , ς ∈ {1, . . . , n}) in X3−v (i.e., given an instance xv in πv
3 positive (negative) clusters π3−v
ξ ,
ς
x3−v will only be in one of these π3−v
). Suppose the learning algorithm will predict all instances
ς
in each cluster with the same label, i.e., the hypothesis class Hv consists of the hypotheses which
do not split any cluster. Thus, the cluster πv
ξ can be classiﬁed according to the posterior probability
P (y = 1|πv
ξ ) and querying the labels of instances in cluster πv
ξ will not inﬂuence the estimation of
the posterior probability for cluster πv
ς (ς 6= ξ ). It is evident that the non-degradation condition holds
in this task. Note that the non-degradation assumption may not always hold, and we will discuss on
this in Section 6. Now we give Theorem 1.

(7)

Theorem 1 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to Deﬁnition 1, when the non-degradation condition holds, if s = ⌈ 2 log 1
⌉ and mi =
8ǫ
log 1
C2
1 (cid:0)V + log( 16(s+1)
)(cid:1), the multi-view active learning in Table 1 will generate two classiﬁers hs
256k C
+
C 2
δ
and hs
− , at least one of which is with error rate no larger than R(S ∗ ) + ǫ with probability at least
1 − δ .
Here, V = max[V C (H1 ), V C (H2 )] where V C (H) denotes the VC-dimension of the hypothesis
λ , C1 = 2C −1/λ
class H, k = 1+λ
λ(λ + 1)−1−1/λ and C2 = 5α+8
6α+8 .
0
2 , ﬁrst with Lemma 1 and (2) we have d∆ (S i+1
1 ∩ S i+1
1 ⊕ S i
Proof sketch. Let Qi = S i
| Qi , S ∗ |
2
v ∩ Qi and τi+1 = P r(T i+1
⊕T i+1
−S ∗ )
− 1
Qi ) ≤ 1/8. Let T i+1
v = S i+1
2 . Considering (7) and
1
2
P r(T i+1
⊕T i+1
)
1
2
d∆ (S i
1 ∩ S i
2 |Qi , S ∗ |Qi )P r(Qi ) = P r(S i
1 ∩ S i
1 ∩ S i
2 − S ∗ ) + P r(S i
2 − S ∗ ), then we calculate that
1 ∩ S i+1
d∆ (S i+1
, S ∗ )
2
≤ P r(S i
1 ∩ S i
2 − S ∗ ) + P r(S i
1 ∩ S i
2 − S ∗ ) +
1 ∪ S i+1
d∆ (S i+1
2
≤ P r(S i
1 ∩ S i
2 − S ∗ ) + P r(S i
1 ∩ S i
2 − S ∗ ) +

) ∩ Qi (cid:1)
2 ) − τi+1P r(cid:0)(S i+1
1 ⊕ S i+1
P r(S i
1 ⊕ S i
2
) ∩ Qi (cid:1).
2 ) + τi+1P r(cid:0)(S i+1
1
1 ⊕ S i+1
P r(S i
1 ⊕ S i
2
8
As in each round some contention points are queried and added into the training set, the difference
between the two views is decreasing, i.e., P r(S i+1
1 ⊕ S i+1
) is no larger than P r(S i
1 ⊕ S i
2 ). Let
2
γi = P r(S i
1⊕S i
2−S ∗ )
2 ) − 1
2 , with Deﬁnition 1 and different combinations of τi+1 and γi , we can
P r(S i
⊕S i
1
d∆ (S i+1
∪S i+1
d∆ (S i+1
∩S i+1
6α+8 . When s = ⌈ 2 log 1
,S ∗ )
,S ∗ )
2 ,S ∗ ) ≤ 5α+8
2 ,S ∗ ) ≤ 5α+8
6α+8 or
have either
8ǫ
1
2
1
2
log 1
∪S i
d∆ (S i
∩S i
d∆ (S i
1
1
C2
C2 = 5α+8
2 , S ∗ ) ≤ ǫ or d∆ (S s
6α+8 is a constant less than 1, we have either d∆ (S s
1 ∩ S s
1 ∪ S s
2 , S ∗ ) ≤ ǫ.
Thus, with (5) and (6) we have either R(hs
+ ) ≤ R(S ∗ ) + ǫ or R(hs
(cid:3)
− ) ≤ R(S ∗ ) + ǫ.

⌉, where

, S ∗ )

1
8

5

−

(8)

From Theorem 1 we know that we only need to request Ps
i=0 mi = eO(log 1
ǫ ) labels to learn hs
+
and hs
− , at least one of which is with error rate no larger than R(S ∗ ) + ǫ with probability at least
+ and it happens to satisfy R(hs
1 − δ . If we choose hs
+ ) ≤ R(S ∗ ) + ǫ, we can get a classiﬁer whose
error rate is no larger than R(S ∗ ) + ǫ. Fortunately, there are only two classiﬁers and the probability
of getting the right classiﬁer is no less than 1
+ and hs
2 . To study how to choose between hs
− , we give
Deﬁnition 2 at ﬁrst.
Deﬁnition 2 The multi-view classiﬁers S1 and S2 satisfy β -condition if (8) holds for some β > 0.
P r(cid:0){x : x ∈ S1 ⊕ S2 ∧ y(x) = 0}(cid:1)
P r(cid:0){x : x ∈ S1 ⊕ S2 ∧ y(x) = 1}(cid:1)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) ≥ β
P r(S1 ⊕ S2 )
P r(S1 ⊕ S2 )
(8) implies the difference between the examples belonging to positive class and that belonging to
negative class in the contention region of S1 ⊕ S2 . Based on Deﬁnition 2, we give Lemma 2 which
provides information for deciding how to choose between h+ and h− . This helps to get Theorem 2.
2 log( 4
δ )
Lemma 2 If the multi-view classiﬁers S s
1 and S s
2 satisfy β -condition, with the number of
β 2
2 ∧ y(x) = 1}(cid:1) or P r(cid:0){x : x ∈
labels we can decide correctly whether P r(cid:0){x : x ∈ S s
1 ⊕ S s
2 ∧ y(x) = 0}(cid:1)) is smaller with probability at least 1 − δ .
S s
1 ⊕ S s
Theorem 2 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to Deﬁnition 1, when the non-degradation condition holds, if the multi-view classiﬁers satisfy
β -condition, by requesting eO(log 1
ǫ ) labels the multi-view active learning in Table 1 will generate a
classiﬁer whose error rate is no larger than R(S ∗ ) + ǫ with probability at least 1 − δ .
From Theorem 2 we know that we only need to request eO(log 1
ǫ ) labels to learn a classiﬁer with
error rate no larger than R(S ∗ ) + ǫ with probability at least 1 − δ . Thus, we achieve an exponential
improvement in sample complexity of active learning in the non-realizable case under multi-view
setting. Sometimes, the difference between the examples belonging to positive class and that be-
longing to negative class in S s
1 ⊕ S s
2 may be very small, i.e., (9) holds.
2 ∧ y(x) = 0}(cid:1)
P r(cid:0){x : x ∈ S s
2 ∧ y(x) = 1}(cid:1)
P r(cid:0){x : x ∈ S s
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) = O(ǫ)
1 ⊕ S s
1 ⊕ S s
1 ⊕ S s
1 ⊕ S s
P r(S s
P r(S s
2 )
2 )
If so, we need not to estimate whether R(hs
+ ) or R(hs
− ) is smaller and Theorem 3 indicates that
both hs
+ and hs
− are good approximations of the optimal classiﬁer.

−

(9)

Theorem 3 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to Deﬁnition 1, when the non-degradation condition holds, if (9) is satisﬁed, by request-
ing eO(log 1
ǫ ) labels the multi-view active learning in Table 1 will generate two classiﬁers hs
+ and
− which satisfy either (a) or (b) with probability at least 1 − δ . (a) R(hs
hs
+ ) ≤ R(S ∗ ) + ǫ and
R(hs
− ) ≤ R(S ∗ ) + O(ǫ); (b) R(hs
+ ) ≤ R(S ∗ ) + O(ǫ) and R(hs
− ) ≤ R(S ∗ ) + ǫ.
The complete proof of Theorem 1, and the proofs of Lemma 2, Theorem 2 and Theorem 3 are given
in the supplementary ﬁle.

1 6= S ∗
5.2 The Situation Where S ∗
2

Although the two views represent the same learning task and generally are consistent with each
other, sometimes S ∗
1 may be not equal to S ∗
2 . Therefore, the α-expansion assumption in Deﬁnition
1 should be adjusted to the situation where S ∗
1 6= S ∗
2 . To analyze this theoretically, we replace S ∗
by S ∗
1 ∩ S ∗
2 in Deﬁnition 1 and get (10). Similarly to Theorem 1, we get Theorem 4.
P r(cid:0)S1 ⊕ S2 (cid:1) ≥ α(cid:16)P r(cid:0)S1 ∩ S2 − S ∗
2 (cid:1)(cid:17)
2 (cid:1) + P r(cid:0)S1 ∩ S2 − S ∗
1 ∩ S ∗
1 ∩ S ∗
Theorem 4 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
1 (cid:0)V +
ing to (10), when the non-degradation condition holds, if s = ⌈ 2 log 1
⌉ and mi = 256k C
8ǫ
C 2
log 1
C2
)(cid:1), the multi-view active learning in Table 1 will generate two classiﬁers hs
log( 16(s+1)
+ and hs
− , at
δ
least one of which is with error rate no larger than R(S ∗
1 ∩ S ∗
2 ) + ǫ with probability at least 1 − δ .
(V , k , C1 and C2 are given in Theorem 1.)

(10)

6

Table 2: Multi-view active learning without the non-degradation condition
Input: Unlabeled data set U = {x1 , x2 , · · · , } where each example xj is given as a pair (xj
1 , xj
2 )
Process:
Query the labels of m0 instances drawn randomly from U to compose the labeled data set L;
Train the classiﬁer h0
v (v = 1, 2) by minimizing the empirical risk with L in each view:
v = arg minh∈Hv P(x1 ,x2 ,y)∈L I(h(xv ) 6= y);
h0
iterate: i = 1, · · · , s
Apply hi−1
and hi−1
to the unlabeled data set U and ﬁnd out the contention point set Qi ;
1
2
Query the labels of mi instances drawn randomly from Qi , then add them into L and delete them
from U ;
Query the labels of (2i − 1)mi instances drawn randomly from U − Qi , then add them into L and
delete them from U ;
Train the classiﬁer hi
v by minimizing the empirical risk with L in each view:
v = arg minh∈Hv P(x1 ,x2 ,y)∈L I(h(xv ) 6= y).
hi
end iterate
+ and hs
Output: hs
−

1 ∩ S ∗
v is the optimal Bayes classiﬁer in the v -th view, obviously, R(S ∗
Proof. Since S ∗
2 ) is no less
v ), (v = 1, 2). So, learning a classiﬁer with error rate no larger than R(S ∗
1 ∩ S ∗
than R(S ∗
2 ) + ǫ is not
harder than learning a classiﬁer with error rate no larger than R(S ∗
v ) + ǫ. Now we aim at learning
1 ∩ S ∗
a classiﬁer with error rate no larger than R(S ∗
2 ) + ǫ. Without loss of generality, we assume
2 ) for i = 0, 1, . . . , s. If R(S i
R(S i
v ) ≤ R(S ∗
1 ∩ S ∗
v ) > R(S ∗
1 ∩ S ∗
2 ), we get a classiﬁer with error rate
1 ∩ S ∗
no larger than R(S ∗
2 ) + ǫ. Thus, we can neglect the probability mass on the hypothesis whose
error rate is less than R(S ∗
1 ∩ S ∗
2 ) and regard S ∗
1 ∩ S ∗
2 as the optimal. Replacing S ∗ by S ∗
1 ∩ S ∗
2 in
(cid:3)
the discussion of Section 5.1, with the proof of Theorem 1 we get Theorem 4 proved.
2 , by requesting eO(log 1
1 6= S ∗
Theorem 4 shows that for the situation where S ∗
ǫ ) labels we can learn
+ and hs
two classiﬁers hs
− , at least one of which is with error rate no larger than R(S ∗
1 ∩ S ∗
2 ) + ǫ
with probability at least 1 − δ . With Lemma 2, we get Theorem 5 from Theorem 4.
Theorem 5 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to (10), when the non-degradation condition holds, if the multi-view classiﬁers satisfy β -
condition, by requesting eO(log 1
ǫ ) labels the multi-view active learning in Table 1 will generate a
1 ∩ S ∗
classiﬁer whose error rate is no larger than R(S ∗
2 ) + ǫ with probability at least 1 − δ .
2 ). When S ∗
1 ) and R(S ∗
2 ) is larger than R(S ∗
1 ∩ S ∗
Generally, R(S ∗
1 is not too much different from
1 ⊕ S ∗
2 , i.e., P r(S ∗
S ∗
2 ) ≤ ǫ/2, we have Corollary 1 which indicates that the exponential improvement
in the sample complexity of active learning with Tsybakov noise is still possible.

Corollary 1 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to (10), when the non-degradation condition holds, if the multi-view classiﬁers satisfy β -
2 ) ≤ ǫ/2, by requesting eO(log 1
condition and P r(S ∗
1 ⊕ S ∗
ǫ ) labels the multi-view active learning in
Table 1 will generate a classiﬁer with error rate no larger than R(S ∗
v ) + ǫ (v = 1, 2) with probability
at least 1 − δ .
The proofs of Theorem 5 and Corollary 1 are given in the supplemental ﬁle.

6 Multi-view Active Learning without Non-degradation Condition

Section 5 considers situations when the non-degradation condition holds, there are cases, however,
the non-degradation condition (7) does not hold. In this section we focus on the multi-view active
learning in Table 2 and give an analysis with the non-degradation condition waived. Firstly, we give
Theorem 6 for the sample complexity of multi-view active learning in Table 2 when S ∗
1 = S ∗
2 = S ∗ .
Theorem 6 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
)(cid:1), the multi-view active
1 (cid:0)V + log( 16(s+1)
ing to Deﬁnition 1, if s = ⌈ 2 log 1
⌉ and mi = 256k C
8ǫ
log 1
C 2
δ
C2
learning in Table 2 will generate two classiﬁers hs
+ and hs
− , at least one of which is with error rate
no larger than R(S ∗ ) + ǫ with probability at least 1 − δ . (V , k , C1 and C2 are given in Theorem 1.)

7

In the (i + 1)-th round, we randomly query (2i+1 − 1)mi labels from Qi and add
Proof sketch.
them into L. So the number of training examples for S i+1
(v = 1, 2) is larger than the number of
v
whole training examples for S i
v . Thus we know that d(S i+1
|Qi , S ∗ |Qi ) ≤ d(S i
v |Qi , S ∗ |Qi ) holds
v
for any ϕv . Setting ϕv ∈ {0, 1}, the non-degradation condition (7) stands. Thus, with the proof of
(cid:3)
Theorem 1 we get Theorem 6 proved.
Theorem 6 shows that we can request Ps
i=0 2imi = eO( 1
+ and hs
ǫ ) labels to learn two classiﬁers hs
− ,
at least one of which is with error rate no larger than R(S ∗ ) + ǫ with probability at least 1 − δ . To
guarantee the non-degradation condition (7), we only need to query (2i − 1)mi more labels in the
i-th round. With Lemma 2, we get Theorem 7.

Theorem 7 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to Deﬁnition 1, if the multi-view classiﬁers satisfy β -condition, by requesting eO( 1
ǫ ) labels the
multi-view active learning in Table 2 will generate a classiﬁer whose error rate is no larger than
R(S ∗ ) + ǫ with probability at least 1 − δ .
Theorem 7 shows that, without the non-degradation condition, we need to request eO( 1
ǫ ) labels to
learn a classiﬁer with error rate no larger than R(S ∗ ) + ǫ with probability at least 1 − δ . The order of
1/ǫ is independent of the parameter in Tsybakov noise. Similarly to Theorem 3, we get Theorem 8
+ and hs
which indicates that both hs
− are good approximations of the optimal classiﬁer.
Theorem 8 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to Deﬁnition 1, if (9) holds, by requesting eO( 1
ǫ ) labels the multi-view active learning in Table
+ and hs
2 will generate two classiﬁers hs
− which satisfy either (a) or (b) with probability at least
− ) ≤ R(S ∗ ) + O(ǫ); (b) R(hs
+ ) ≤ R(S ∗ ) + ǫ and R(hs
1 − δ . (a) R(hs
+ ) ≤ R(S ∗ ) + O(ǫ) and
R(hs
− ) ≤ R(S ∗ ) + ǫ.

1 6= S ∗
As for the situation where S ∗
2 , similarly to Theorem 5 and Corollary 1, we have Theorem 9
and Corollary 2.

Theorem 9 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to (10), if the multi-view classiﬁers satisfy β -condition, by requesting eO( 1
ǫ ) labels the multi-view
1 ∩ S ∗
active learning in Table 2 will generate a classiﬁer whose error rate is no larger than R(S ∗
2 ) + ǫ
with probability at least 1 − δ .
Corollary 2 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to (10), if the multi-view classiﬁers satisfy β -condition and P r(S ∗
1 ⊕ S ∗
2 ) ≤ ǫ/2, by requesting
eO( 1
ǫ ) labels the multi-view active learning in Table 2 will generate a classiﬁer with error rate no
larger than R(S ∗
v ) + ǫ (v = 1, 2) with probability at least 1 − δ .
The complete proof of Theorem 6, the proofs of Theorem 7 to 9 and Corollary 2 are given in the
supplementary ﬁle.

7 Conclusion

We present the ﬁrst study on active learning in the non-realizable case under multi-view setting in
this paper. We prove that the sample complexity of multi-view active learning with unbounded Tsy-
bakov noise can be improved to eO(log 1
ǫ ), contrasting to single-view setting where only polynomial
improvement is proved possible with the same noise condition. In general multi-view setting, we
prove that the sample complexity of active learning with unbounded Tsybakov noise is eO( 1
ǫ ), where
the order of 1/ǫ is independent of the parameter in Tsybakov noise, contrasting to previous polyno-
mial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. Generally, the
non-realizability of learning task can be caused by many kinds of noise, e.g., misclassiﬁcation noise
and malicious noise. It would be interesting to extend our work to more general noise model.

Acknowledgments

This work was supported by the NSFC (60635030, 60721002), 973 Program (2010CB327903) and
JiangsuSF (BK2008018).

8

References

[1] M. Anthony and P. L. Bartlett, editors. Neural Network Learning: Theoretical Foundations.
Cambridge University Press, Cambridge, UK, 1999.

[2] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages
65–72, 2006.

[3] M.-F. Balcan, A. Blum, and K. Yang. Co-training and expansion: Towards bridging theory and
practice. In NIPS 17, pages 89–96. 2005.

[4] M.-F. Balcan, A. Z. Broder, and T. Zhang. Margin based active learning.
35–50, 2007.

In COLT, pages

[5] M.-F. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning. In
COLT, pages 45–56, 2008.

[6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT,
pages 92–100, 1998.

[7] R. M. Castro and R. D. Nowak. Upper and lower error bounds for active learning. In Allerton
Conference, pages 225–234, 2006.

[8] R. M. Castro and R. D. Nowak. Minimax bounds for active learning. IEEE Transactions on
Information Theory, 54(5):2339–2353, 2008.

[9] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear classiﬁcation and selective sampling
under low noise conditions. In NIPS 21, pages 249–256. 2009.

[10] D. A. Cohn, L. E. Atlas, and R. E. Ladner.
Machine Learning, 15(2):201–221, 1994.

Improving generalization with active learning.

[11] S. Dasgupta. Analysis of a greedy active learning strategy. In NIPS 17, pages 337–344. 2005.

[12] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS 18, pages 235–
242. 2006.

[13] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In
NIPS 20, pages 353–360. 2008.

[14] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. In
COLT, pages 249–263, 2005.

[15] L. Devroye, L. Gy ¨orﬁ, and G. Lugosi, editors. A Probabilistic Theory of Pattern Recognition.
Springer, New York, 1996.

[16] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by
committee algorithm. Machine Learning, 28(2-3):133–168, 1997.

[17] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, pages
353–360, 2007.

[18] S. Hanneke. Adaptive rates of convergence in active learning. In COLT, 2009.

[19] M. K ¨a ¨ari ¨ainen. Active learning in the non-realizable case. In ACL, pages 63–77, 2006.

[20] I. Muslea, S. Minton, and C. A. Knoblock. Active + semi-supervised learning = robust multi-
view learning. In ICML, pages 435–442, 2002.

[21] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics,
32(1):135–166, 2004.

[22] L. Wang. Sufﬁcient conditions for agnostic active learnable. In NIPS 22, pages 1999–2007.
2009.

[23] W. Wang and Z.-H. Zhou. On multi-view active learning and the combination with semi-
supervised learning. In ICML, pages 1152–1159, 2008.

9


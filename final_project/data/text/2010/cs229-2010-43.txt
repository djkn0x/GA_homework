EPiC: Earthquake Prediction in California

Caroline Suen

David Lo

Frank Li

December 10, 2010

1 Introduction

Earthquake prediction has long been the holy grail for
seismologists around the world. The various factors
aﬀecting earthquakes are far from being understood,
and there exist no known correlations between large-
scale earthquake activity and periodic geological events.
Nonetheless, we hope that machine learning will give us
greater insight into the patterns underlying earthquake
activity, even if we cannot predict the time, location,
and strength of the earthquakes accurately.

2 Background

With the exception of a brief period in the 1970s, earth-
quake prediction was generally considered to be infea-
sible by seismologists. Then, in 1975, Chinese oﬃcials
ordered the evacuation of Haicheng one day before a
magnitude 7.3 earthquake struck [15]. This led to a
ﬂurry of optimism toward earthquake prediction [9, 11],
which was subsequently checked by the failed predic-
tion of the magnitude 7.8 Tangshan earthquake of 1976.
Another failure occurred in Parkﬁeld, California in the
early 1980s. Up to then, magnitude 6.0 earthquakes had
occurred at fairly regular 22-year intervals. This led re-
searchers to predict that an earthquake would strike by
1993; no such earthquake arrived until 2004. To this
day, the Haicheng earthquake remains the only success-
ful earthquake prediction in history. Critically review-
ing earlier reports of successful earthquake predictions,
an international panel of geologists famously concluded
in 1997 that “earthquakes cannot be predicted” [4].
More recently, researchers in China have suggested
that neural networks ensembles and support vector ma-
chines could be used to predict the magnitudes of strong
earthquakes [5, 16], but more research needs to be done
to corroborate their ﬁndings. For now the best earth-
quake forecast is vague at best: the USGS predicts a 63%
probability of one or more magnitude 6.7 earthquakes in
the San Francisco Bay Area between 2007 and 2036 [1].
Rather than attempt to issue earthquake predictions, we
hope to analyze past data for periodic patterns that may
advance our understanding of earthquake dynamics.

3 Methodology

3.1 Data

For data quality purposes, we decided to focus on earth-
quakes in California, which is heavily monitored for
earthquake activity. We scraped, parsed, and removed
duplicates from the following data sources:
• National Geophysical Data Center [7, 8]
• Northern California Earthquake Data Center [6]
• Southern California Earthquake Data Center [10]

Each of these data sources is freely available and con-
tains magnitude, epicenter location, and time of occur-
rence information for earthquakes dating back to 1930.
The data set is not well populated before 1970, and the
number of earthquakes records increases rapidly there-
after. This is partly due to advances in measurement
technology and roughly corresponds to the Northern
California Seismic Network being brought online.

3.2 Poisson Model

3.2.1 Overview

Our initial model sought to estimate the number of
earthquakes at a particular location in the next year. In
developing the model we had several relevant features to
consider, including magnitude, latitude and longitude of
the epicenter, depth of the epicenter, and starting time
of each earthquake. For our initial model, we made two
simpliﬁcations:
• time invariance:
that the frequency of earth-
quakes at a particular location did not change over
time
• magnitude insensitivity:
that all earthquakes
above a speciﬁed magnitude threshold C were con-
sidered equally

By not diﬀerentiating time and magnitude, we could
model the number of earthquakes per year at a speci-
ﬁed location as a Poisson distribution. That is,
f (loc, t) = f (loc) ∼ P oisson(λ(loc)),

1

where f is the observed frequency of earthquakes above
magnitude C at location loc in year t, and λ is the true
(time-invariant) frequency of earthquakes above magni-
tude C at location loc.
However, earthquakes are extremely unlikely to have
occurred at exactly the location of interest, so we decided
to weight all earthquakes based on their distance from
loc. We chose Poisson-like scaling factors similar to the
standard used in locally weighted linear regression.

3.2.2 Deﬁnitions

let t(i) ∈ R,
Given a training set of m earthquakes,
loc(i) ∈ R2 , mag (i) ≥ C denote the time (in years AD),
location (in degrees latitude and longitude), and mag-
nitude (in Richter units) of the ith earthquake. For
notational convenience, we will rewrite location loc =
(lat, lon) in terms of its latitude and longitude compo-
nents where appropriate.
We deﬁne the weighting function between two loca-
(cid:18) −d(loca , locb )2
(cid:19)
tions as
2σ2

w(loca , locb ) = exp

,

2r arcsin

where d is the geodesic (great circle) distance between
the two input locations and σ is a bandwidth parameter
controlling the rate of dropoﬀ of the weights.
It is easy to show that a numerically stable expression
(cid:115)
(cid:19)
(cid:19)
(cid:18) ∆lon
(cid:18) ∆lat
for d(loca , locb ) is
sin2
,
2
2
where ∆lat = lata − latb , ∆lon = lona − lonb , r =
radius of the earth, and all latitudes and longitudes are
in radians.
m(cid:88)
We now deﬁne
i=1

1{t(i) = y}w(loc, loc(i) )

+ cos (lata ) cos (latb ) sin2

k(loc, y) =

to be the weighted count of earthquakes at location loc
in year y .

3.2.3 Mathematical Derivation
For each loc of interest, we wish to ﬁnd the ˆλ(loc) that
maximizes the likelihood of our training set. Time in-
variance implies that each year’s earthquakes are inde-
pendent samples. For a training set spanning the years
yend(cid:89)
ystart and yend , this yields
y=ystart

λk(loc,y)e−λ
k(loc, y)!

ˆλ(loc) = arg max
λ

,

with an extension of the Poisson distribution to allow
(cid:33)
(cid:32) −d(loc, loc(i) )2
for nonintegral values of k(loc, y).
m(cid:88)
Taking derivatives and maximizing yields
2σ2
i=1

1
yend − ystart + 1

ˆλ(loc) =

exp

3.3 Fourier Model

3.3.1 Overview

Our second model was motivated by periodic patterns
observed in the data obtained using the Poisson model
(see section 4 for more details). This model entails per-
forming a Discrete Fourier Transform (DFT) [2] on the
input data and then examining the frequency spectrum
for dominant frequencies. If the input data is strongly
periodic, we would expect to see several frequency peaks;
conversely, if the input data is truly random, we would
expect to see no discernible pattern. Therefore, to ﬁt
the input data to a Fourier model, we simply compute
the DFT for the input data and pick proper coeﬃcients
for the frequencies that have the highest power. The
Fourier model can be parametrized by the number of
frequencies that are used.

3.3.2 Deﬁnitions

We used the Fourier model to analyze which region in
California would have more earthquakes year to year.
We deﬁne the north region as bounded by latitudes 42◦N
and 36◦N and longitudes 128◦W and 114◦W. The south
region is deﬁned as bounded by latitudes 36◦N and 30◦N
with the same longitude range.
Recall that ˆλ(loc) estimates number of earthquakes at
loc. Therefore, to count the number of earthquakes in a
region, we simply sum up the ˆλ values for all locations
in the region, discretized to a grid of 0.1◦ by 0.1◦ .

3.3.3 Mathematical Derivation

We deﬁne the dominant frequencies chosen by the ﬁtting
process as ω1 , . . . , ωn and the DFT coeﬃcient of each
value as Y (jω1 ), . . . , Y (jωn ). Since we will be using a
single-sided DFT, we deﬁne the amplitude component
A(ω) for every frequency ωi as 2|Y (jωi )| for i = 1...n.
Similarly, we deﬁne the phase P (ω) for every frequency
ωi as ∠Y (jωi ). For simplicity, we assume that the input
data has been normalized so that the mean is 0 (e.g.
n(cid:88)
Y (0) = 0). Putting this together yields
i=1

A(ωi )Re(exp(j (ωi t + P (ωi )))

y(t) =

where Re denotes the real component.

2

4 Results and Analysis

4.1 Poisson Model

4.1.1 Results

We implemented our Poisson model ﬁtting in Matlab to
both train and predict earthquake frequency. For our
initial run, we trained the Poisson model on all earth-
quakes after 1970, with magnitude greater than 2.0, and
after some experimentation, with σ = 10km. We evalu-
ated our model at each location on a grid of 0.01 degrees
latitude and longitude, plotted the predicted earthquake
frequency, and overlaid it onto Google Earth (see Table
1) [3]. Red areas are predicted to have relatively frequent
earthquakes, whereas blue areas are predicted to have
little earthquake activity. Our data resulted in more red
areas in regions known to be earthquake prone, such as
the San Francisco Bay Area and Los Angeles, providing
a good ﬁrst validation of our model, as the predictions
appeared reasonable.
We also performed further quantiﬁcation of the gen-
eral accuracy of our model. We overlaid the predicted
earthquake density on a map of known fault lines and a
map of earthquakes recorded in the past week (see Ta-
ble 1). As expected, earthquake-dense areas correspond
with known fault locations, while areas that don’t ex-
perience much earthquake activity do not have many
nearby faults. Turning to real recorded earthquakes, we
similarly observe that earthquakes tend to occur more
frequently in areas marked as hot. However, we do see
that the hot region near Los Angeles did not seem to
have many quakes, while a not-so-hot region near San
Diego is extremely earthquake dense. Because the recent
data only includes one week’s worth of earthquakes, we
concluded that the lack of earthquakes near Los Ange-
les could be coincidence, or it could also point at ﬂaws
in our model. We investigate this further in the next
section.

4.1.2 Validation

We performed validation on the predictive power of our
Poisson model. To do so, we used cross validation where
we trained the predictor on earthquakes after 1970 and
with magnitude larger than 4.0, with 5 years of data
withheld to use as a test set. We then compared the
number of earthquakes predicted by the model versus
the number of earthquakes that actually occurred, for
each location. For this particular example, our test set
is on the years 1980 to 1985. Table 2 shows earthquake
density predicted by the model, earthquake density for
the test set, and the error between the test density and
the predicted density. If our model has high predictive

power, then we expect the magnitude of the error to be
small all around.
Certain regions have a signiﬁcantly higher earthquake
density in the test set (red regions), while other regions
have a signiﬁcantly lower earthquake density (dark blue
regions). From this, we immediately see that the Poisson
model has extremely high error rates for some seismi-
cally active regions. Indeed, we see that regions that are
seismically active in 1980-1985 are not what the model
predicted. Thankfully, regions that are not seismically
active tend to have low error rates, as expected. How-
ever, when we compare two diﬀerent test sets (1975-1980
vs 1980-1985), we see that the distribution is quite dif-
ferent.
As seen above, our analysis regrettably suggests that
one of the central assumptions to our Poisson model,
namely time invariance, is likely not valid. On further
inspection, we concluded that time invariance on the fre-
quency of earthquakes may not always be a reasonable
assumption. For example, a large earthquake frequently
causes numerous aftershocks of considerable magnitude
afterwards, all localized in a small region in a short pe-
riod of time. When a Poisson model is ﬁtted to this
type of data, it erroneously predicts an extremely high
frequency of earthquakes for that region. However, even
though our Poisson model may not be the optimal model
for predicting the number of earthquakes for seismically
active regions, it still provides a fairly accurate indica-
tion of what regions are seismically active. We expand
on this idea in our Fourier model.

4.2 Fourier Model

4.2.1 Results

Because of the diﬀering seismically active “hot spots”
present between 1980-1985 and 1975-1980, we hypothe-
sized that the location of the hot spots might be peri-
odic over time. To test our hypothesis we split Califor-
nia into northern and southern regions, and performed
Fourier analysis with diﬀering numbers of coeﬃcients
in order to predict, for each year, whether the north-
ern or southern region would have more earthquakes.
We used a two-way split because Fourier analysis works
best on continuous-valued functions, and mapping from
two regions to two values that give an approximately
continuous function can be done. Our dataset spanned
from 1950 to 2010. For each year n years after 1950, we
computed the Fourier coeﬃcients based on the previous
n years, then computed which region was expected to
have a higher weight of earthquakes based on the ap-
proximation at year n.

3

Density of earthquakes

Density with fault lines

Density with recent quakes

Table 1: Densities

Density of earthquakes
on training set
(1970-
2010,
1980-
excluding
1985)

4.2.2 Validation

Density of earthquakes
on testing set (1980-1985)

Error between testing set
and training set

Density of earthquakes
on testing set (1975-1980)

Table 2: Model and test densities

To perform testing, we compared the predicted region
with the actual region from the data. We were unable to
use leave-one-out-cross-validation, as performing Fourier
analysis requires the data to not contain any gaps from
year to year. The graphs below show the accuracies
of the predictions for every coeﬃcient choice between
one and eight and a graph of the predictions using the
optimal number of coeﬃcients, three, overlaid with the
test data results (see Table 3). Test error is deﬁned as
the number of mispredicted regions divided by the total
number of predictions. We slightly redeﬁne training er-
ror, due to the sequential training process of training on
year n, n + 1, etc. Instead, we deﬁne training error as
the total number of training errors over all years divided
by the total number of training points over all years.

As seen by the graph, three coeﬃcients is the optimal
number of coeﬃcients to use, as it yields the highest test
accuracy. Beyond three coeﬃcients we see that overﬁt-
ting becomes an issue, as training accuracy increases to
nearly 100% while testing accuracy drops signiﬁcantly.
At three coeﬃcients our model has approximately 75%
test accuracy. When we ﬁlter our data to consider only
earthquakes of magnitude 4.0 or greater, using three

4

coeﬃcients yields a test accuracy of 66%. These facts
suggest that the top three Fourier frequencies achieve
better-than-random predictive performance, as the dis-
tribution of frequent earthquake regions is split evenly
between north and south. These top three frequences
(8 years, 12.8 years, and 16 years) may be of geological
signiﬁcance.

5 Conclusions and Next Steps

In our pro ject we mathematically derived a weighted
Poisson model that estimates the frequency of earth-
quakes by year for a given location. Our Fourier model
extended the Poisson model to predict whether north or
south California will have more earthquakes each year
(in 2011, it’s pro jected to be the northern region).
While the Poisson model has very little predictive
power between years, it provides a good indicator of seis-
mically active regions. Based on this, our Fourier model
detects three ma jor frequencies that should be investi-
gated for geological signiﬁcance. Our model’s accurate
and immediate predictions suggest that high-level earth-
quake prediction may be feasible.
It should also be possible to extend the Fourier model
to be able to predict more than a binary result. For ex-

Comparison of actual and predicted

Learning curve for Fourier model

Table 3: Fourier model results

ample, California can be split into N non-overlapping
bands. To perform a prediction, we would need to
ﬁrst do a pairwise Fourier prediction (using our existing
model) between regions 1 and 2. The winner of this com-
parison would then be compared with region 3, 4, . . . , N .
This approach would give a more speciﬁc prediction and
could also lead to more interesting results.
As mentioned earlier, earthquake prediction has long
been scoﬀed at as an infeasible hope. We hope that our
insights into earthquake dynamics show that while today
earthquake prediction might be a lofty and far-fetched
goal, perhaps one day, there will be data, resources, and
perhaps even machine learning algorithms available for
us to understand and predict patterns of earthquakes
activity.

References

[1] 2007 Working Group on California Earthquake
Probabilities (2008), The Uniform California Earth-
quake Rupture Forecast, Version
2. Available:
http://pubs.usgs.gov/of/2007/1437/
of2007-1437 text.pdf

[2] Discrete Fourier Transform. Wolfram MathWorld. Avail-
able: http://mathworld.wolfram.com/
DiscreteFourierTransform.html

[3] Google, KML Documentation. Available:
https://code.google.com/apis/kml/documentation/

[4] Geller, Robert J. and Jackson, David D. and Ka-
gan, Yan Y. and Mulargia, Francesco (1997), Earth-
quakes Cannot Be Predicted. Science, 5306:1616. doi:
10.1126/science.275.5306.1616

[5] Liu, Yue and Wang, Yuan and Li, Yuan and Zhang,
Bofeng and Wu, Gengfeng (2004), Earthquake Predic-

tion by RBF Neural Network Ensemble. Advances in
Neural Networks - ISNN 2004, 3174:13-17.

[6] NCEDC, NCSN earthquake catalog. Available:
http://www.ncedc.org/ncedc/catalog-search.html

[7] NOAA, The Seismicity Catalog CD-ROM Collection,
1996. Available:
http://www.ngdc.noaa.gov/hazard/ﬂiers/se-0208.shtml

[8] NOAA, Signiﬁcant Earthquake Database. Available:
http://ngdc.noaa.gov/nndc/struts/
form?t=101650&s=1&d=1

[9] Press, Frank (1975), Earthquake Prediction. Scientiﬁc
American, 232.5:14.

[10] SCEC, Southern California earthquake catalog. Avail-
able: http://data.scec.org/ftp/catalogs/SCEC DC/

[11] Scholz, Christopher H., Sykes, Lynn R., Aggarwal, Yash
P. (1973), Earthquake Prediction: A Physical Bases. Sci-
ence, 4102:308-310.

[12] Stark, P. B. (1997), Earthquake prediction:
the null
hypothesis. Geophysical Journal
International, 131:
495499. doi: 10.1111/j.1365-246X.1997.tb06593.x

Files.
Earth/KML
Google
[13] USGS,
http://earthquake.usgs.gov/learn/kml.php

Available:

[14] USGS, Quaternary Faults in Google Earth. Available:
http://earthquake.usgs.gov/hazards/qfaults/google.php

[15] Wang, Kelin and Chen, Qi-Fu and Sun, Shihong and
Wang, Andong (2006), Predicting the 1975 Haicheng
earthquake. Bul letin of
the Seismological Society of
America, 96.3:757-795. doi:10.1785/0120050191

[16] Wang, Wei and Liu, Yue and Li, Guo-zheng and Wu,
Geng-feng and Ma, Qin-zhong and Zhao, Li-fei and Lin,
Ming-zhou (2006), Support vector machine method for
forecasting future strong earthquakes in Chinese main-
land. Acta Seismologica Sinica, 19: 30-38.

5


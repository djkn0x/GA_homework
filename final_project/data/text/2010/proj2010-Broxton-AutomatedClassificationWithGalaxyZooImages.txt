Automated Classiﬁcation of Galaxy Zoo Images
CS229 Final Report
Michael J. Broxton - broxton@stanford.edu

1. Introduction
The Sloan Digital Sky Survey (SDSS) in an ongoing effort to collect an extensive image and spectral 
catalogue of deep sky objects.  Billed as “the most ambitious and inﬂuential survey in the history of 
astronomy,” the SDSS contains rich information that has had a signiﬁcant impact on our understanding of 
the history, structure, and evolution of our universe [York 2000].  Unfortunately the enormous size of the 
SDSS data catalogue makes it unwieldy and difﬁcult to analyze by hand.  One viable approach for 
processing SDSS data on a large scale was demonstrated in 2008 by the Galaxy Zoo 1 project [Lintott 
2008], in which Internet users were asked to categorize millions of images of galaxies into three 
morphological classes: elliptical, spiral, or ʻother. 1ʼ However, after one year of collecting user input, this 
effort ultimately classiﬁed only 1 million out of 230 million objects in the survey catalog.  With more 
observations being added on a daily basis, there is an ever growing need for robust, automated analysis 
& classiﬁcation techniques.
The goal of this project is to demonstrate that machine learning algorithms can be trained to produce 
results that consistently agree with galaxy classiﬁcations produced by human click-workers.  Using 
classiﬁcation statistics released by the Galaxy Zoo project for nearly 900,000 objects [Lintott 2010] as a 
training & validation data set, we show that modern machine learning architectures can be trained to 
perform well on the SDSS galaxy classiﬁcation task.  We directly compare galaxy morphology 
classiﬁcations produced by our algorithms to those produced by Galaxy Zoo users.  With this as our error 
metric, we test various machine learning approaches using two different feature modalities that are readily 
available from the Sloan Digital Sky Survey: (1) hand-engineered features that are good proxies for 
galaxy morphology (e.g. color, luminosity, spectral 
features, or structural parameters); and (2) 423x423 
pixel, 3-channel color images of the galaxies.  Note 
that human classiﬁers on the Galaxy Zoo website 
based their classiﬁcations solely on color imagery (i.e.  
feature set #2).  It is therefore a primary objective of 
this project to determine whether a machine classiﬁer 
can produce comparable results using the image 
feature set alone.  In Section 2 we describe these 
feature modalities in more detail, then in Section 3 we 
present two machine learning architecture for 
processing color galaxy images.  Results are 
presented in Section 4, and we discuss our 
conclusions in Section 5.
2. Feature Modalities
It has been shown in various studies [Odewhan 
1994 , Lahav 1995, Ball 2006, Elting 2008, Banerji 
2010] that promising automated galaxy classiﬁcation 
rates can be achieved by leveraging the wealth of 
morphological and photometric statistics available in 

liklihood of being well modeled as a galaxy 
with a disk-like feature
liklihood of being well modeled as a galaxy 
with a central bulge feature
liklihood of being well modeled as a point 
source
ratios of radii containing 90 and 50 percent of 
the Petrosian ﬂux
second moment of object intensity in the 
CCD (robust to noise)
adaptive ellipticity - (based on mE1 and mE2)
adaptive fourth moment
ratio of range of ﬂuctuations in surface 
texture_i
brightness to full dynamic range of object 
Table 1: These 12 hand-tuned features capture the color, 
morphology, and photometric properties of galaxies.  They are 
available for download for all SDSS observations, and were 
chosen here because they were used in previous galaxy 
classiﬁcation studies (speciﬁcally [Banerji 2010]).

green - red
red - infrared
deVAB_i   
(de Vaucouleurs ﬁt axial ratio)
expAB_i   
(Exponential ﬁt axial ratio)
lnLexp_i  
(Disk ﬁt log liklihood)

green minus red color (red shift removed)
red minus infrared color (red shift removed)

axial ratio for a ﬁt to a galaxy bulge model

axial ratio for a ﬁt to a galaxy disk model

 lnLdeV_i 
(Star log likelihood)

 lnLstar_i 
(Star log likelihood)

petroR90 i/petroR50 i
(concentration)

mRrCc_i

aE_i
mCr4_i

1 The ʻotherʼ category in the Galaxy Zoo data set is used for hybrid objects like galaxy mergers or partially occluded galaxies; or objects that were 
incorrectly classiﬁed as galaxies by SDSS automated data pipeline, such as Stars and Quasars.

Michael Broxton – CS229 Final Report

Galaxy Zoo 
Images

Normalization, cropping

Whiten (PCA)

ICA

Compute Magnitude of FFT; 
crop to discard high freq.

Spectral Signatures

the SDSS database.  For consistency with past 
studies, we chose the same twelve features 
used in [Banerji 2010] (listed in Table 1).  These 
capture a range of properties including the 
galaxyʼs color, degree of ellipticity, and surface 
brightness proﬁle.  Classiﬁcation using hand-
tuned features serves as a baseline case for the 
purposes of this study.
Our novel contribution is to show that modern 
machine learning architectures can classify 
galaxies using exactly the same ʻinputʼ as was 
available to human classiﬁers; namely color 
galaxy images alone.  In fact, we will show that 
image features consistently outperform hand-
engineered morphological and photometric 
features.  In order to achieve this result we found  
it necessary to ﬁrst compute the Fourier 
transform of the imagery, take the magnitude of 
the result (i.e. discarding phase information), and 
then ʻcroppingʼ this power spectrum image to 
retain only the low frequency components.  This 
spectral signature still contains most of the 
salient information from the original image, but is 
invariant to phase and robust to the high 
frequency image noise that is prevalent in 
Galaxy Zoo images.  This approach was inspired 
to a certain extent by models from neuroscience 
of complex visual receptive cells that respond to 
the frequencies and orientations of imagery, but 
exhibit a high degree of phase invariance.  Such 
complex cells can be well modeled as acting on 
the power spectra of visual stimuli [David 2005].  
In our case, the intuitive justiﬁcation for this pre-
processing step is that the frequency content 
and orientation of galaxy images contain salient 
features that are useful for detecting spiral 
structure and elliptical eccentricity respectively; 
whereas the phase is less important since the 
Figure 1: Our two machine learning architecture for galaxy 
classiﬁcation learn sparse features on the Fourier power spectrum 
positions in an image of disks, bulges, spiral 
of galaxy images. In this study we consider two architectures that 
arms, and other features do not encode much 
ﬁrst learn useful features from the imagery in an unsupervised 
useful information for classiﬁcation purposes.  
setting, and then we conduct supervised training using class labels 
from the Galaxy Zoo data set.
Figure 1 shows these pre-processing steps in 
more detail.  Images are ﬁrst normalized and then cropped to a 200x200 pixel window centered on the 
galaxy.  The square root of the Fourier power spectrum is then computed by taking the magnitude of the 
FFT for each galaxy image.  The resulting 200x200 pixel frequency space image is further cropped to the 
center 32x32 pixel region; in essence throwing out high frequency components of the FFT that are 
dominated by image noise.  Finally, the data is whitened using PCA (retaining 99% of variance).  We refer 
to the ﬁnal whitened spectral image as the spectral signature of the galaxy. 

Support Vector Machine
ICA + SVM

Classiﬁcations 
[ spiral, elliptical, other ]

Independent Components

Input Layer

Multiply IC's & spectral signatures

Sparse Layer (greedily trained)

Activations

Softmax Layer

Sparse Layer (greedily trained)

Sparse Network

Michael Broxton – CS229 Final Report

(a)
(b)
(c)
Figure 2: Three sparse feature dictionaries learned from the Fourier power spectrum of galaxy zoo images.  The features produced 
using ICA (a) and the Sparse Autoencoder (b) are similar; both contain ﬁlters that select for an assortment of frequencies (round, 
ring-like features) and orientations (asymmetric, oriented features).  Some sparse autoencoder features change after ﬁne-tuning 
(c), while others remain largely the same.  Note that color seems to play an important role in feature morphologies. Different color 
channels often select for different frequency characteristics within the same ﬁlter.

3. Learning Architectures
We have developed two separate systems that learn sparse features on the Fourier power spectrum of 
Galaxy Zoo images.  The ICA+SVM system (left half of Figure 1) uses Independent Component Analysis 
(ICA) to learn M independent components that exhibit a high degree of selectivity for frequencies (via 
round, ring-like features) and orientations (via asymmetric, oriented features).  Figure 2(a) shows typical 
independent components learned from spectral signatures.  We then compute activations for a given 
galaxy image by multiplying spectral signatures by each independent component.  This produces M 
activations per galaxy (we selected M = 64 in this study).  Finally, we train a SVM using a linear kernel on 
these activations to match known classiﬁcations in our training data set.   
Our second approach employs a neural network with hidden layers that are greedily trained as Sparse 
Autoencoders [Ng 2010].  During pre-training, this Sparse Network system learns a set of features like 
those depicted in Figure 2(b).  Features are subsequently reﬁned during the supervised ﬁne-tuning phase  
(see Figure 2(c)) at the same time as the network trains its Softmax output layer to identify galaxies.  
Sparse weights are held constant during the ﬁrst 50 iterations of ﬁne-tuning, and then allowed to change 
during the remaining 200-300 iterations or until the algorithm converges.  In this study we tested networks 
with one or two sparse hidden layers; each containing 100 sigmoidal units trained with a lifetime sparsity 
constraint as well as a regularization term that helped to prevent over-ﬁtting.
Note that the ICA+SVM approach produces ʻhardʼ classiﬁcations with only one label per galaxy, whereas 
the Sparse Network (by virtue of its softmax output layer) produces ʻsoftʼ classiﬁcations with three 
separate probabilities that indicate the likelihood that a galaxy is a member of each class.  We will show in 
Section 4 that ʻsoftʼ labels are useful for rejecting low-conﬁdence matches that would otherwise appear as 
false positives in the ﬁnal results.
4. Results
We consider three subsets of the Galaxy Zoo data for testing and validation of these algorithms.  The 
clean subset contains galaxies for which 80% or more of human classiﬁers agree on a given galaxyʼs 
class.  We also consider an uncertain subset for which only 50% or more of human classiﬁers agree.  
Finally, we test performance on the full galaxy zoo data set (rejecting only those entries with incomplete 
metadata from the SDSS) in which there is sometimes substantial human disagreement.  The uncertain 
and full subsets contains images that are far more challenging for humans to categorize consistently, and 
it shows how the performance of our algorithm degrades as data becomes increasingly ambiguous.  All 

Michael Broxton – CS229 Final Report

“Clean” Subset
“Clean” Subset
“Clean” Subset
“Clean” Subset
“Clean” Subset
(80% or better Human Agreement)
(80% or better Human Agreement)
(80% or better Human Agreement)
(80% or better Human Agreement)
(80% or better Human Agreement)

“Uncertain” Subset
“Uncertain” Subset
“Uncertain” Subset
“Uncertain” Subset
“Uncertain” Subset
(50% or better Human Agreement)
(50% or better Human Agreement)
(50% or better Human Agreement)
(50% or better Human Agreement)
(50% or better Human Agreement)

Full Dataset
Full Dataset
Full Dataset
Full Dataset
Full Dataset

--

--

--

--

%
Reject
--

%
Reject
--

%
Reject
--

Elliptical

Spiral

Other

Overall

91% 92% 95%

--

97% 97% 86%

--

97.3% 97.9% 0

97.0% --

Elliptical

Spiral

Other

Overall

Elliptical

Spiral

Other

Overall

97.3% 98.7% 5.6% 97.6% --

88.9% 90.1% 0% 86.6% --

97.3% 95.3% 37.5% 94.8% 3.2% 90.5% 85.1% 45.2% 85.6% 6.6% 90.2% 83.8% 43.2% 84.6% 8.4%

Classiﬁer ( Feature Set )
Prior Art [Banerji et. al. 2010]
(Hand-tuned Features)
Softmax Regression 
(Hand-tuned Features)
Support Vector Machine 
(Hand-tuned Features)
ICA + Support Vector Machine 
(Power Spectral Features)
Sparse Network (1 hidden layer)
(Power Spectral Features)
Sparse Network (2 hidden layers)
(Power Spectral Features)
Sparse Network (2 hidden layers)
(Hand-tuned + Spectral Features)
Table 2: Results from classiﬁcation tests for different feature sets and classiﬁers.  The best performance is obtained using a 
combination of spectral signatures and hand-tuned features, however this only improved slightly over performance with spectral 
signatures only.  Spectral features consistently out perform hand-tuned features in all of our tests.  Note that these are results of 
individual tests; we have not yet had time to conduct thorough cross-validation. Some data was not available, and some tests with 
ʻhardʼ classiﬁcation outputs did not allow for us to reject inconclusive entries.  These entries are marked with ʻ--ʼ.

98.4% 98.4% 64.7% 98.3% 0.6% 92.9% 93.9% 68.6% 91.6% 9.0% 90.6% 93.7% 58.8% 89.4% 13%

98.8% 98.2% 70.6% 98.5% 0.5% 93.7% 93.9% 68.5% 92.0% 9.1% 91.0% 93.5% 59.7% 89.5% 13%

99.1% 98.7% 73.3% 98.8% 0.3% 95.0% 93.2% 59.4% 91.8% 6.5% 92.8% 94.1% 50.0% 91.2% 14%

90.2% 90.2% 6.1% 87.4% --

87.7% 88.4% 0% 84.4% --

86.5% 87.6% 0% 83.4% --

tests were conducted using 10,000 randomly selected galaxies from one of the three data subsets.  
Images were split into 7,500 records for training and 2,500 records for testing. 
Table 2 shows results for various combinations of learning algorithms and feature sets.  Results are 
broken out by class, with an overall classiﬁcation rate shown in bold face.  In the ﬁrst row we have 
included results from the [Banerji 2010] study, which they obtained by training a three layer neural 
network on hand-tuned features.  In order to draw an accurate comparison to this prior work, we have 
adopted a similar strategy to the one employed by Banerji et. al. for interpreting classiﬁcation results.  
This ﬁrst requires rejecting a certain number of ʻinconclusiveʼ classiﬁcations made by the machine 
algorithm for galaxies that did not receive more than a 50% probability of belonging to any one of the 
three classes.  Culling out weak classiﬁcations in this manner decreases the number of false positives in 
all classes, and improves classiﬁcation rates (i.e. the fraction of true positives out of all classiﬁcations 
considered for a given class) by several percent.  With the 50% threshold, we typically rejected between 
1% and 10% of the total galaxies being classiﬁed.  The “% Reject” column in Table 2 shows the exact 
percentage of testing examples rejected in each test. 
Note that the 50% threshold was chosen arbitrarily, and should be considered a parameter that can be 
tuned to minimize false positive rates at the expense of increasing the number of false negatives.  
However, we did not have time to study this tradeoff closely, but in future work one could ﬁnd the optimal 
cutoff that perfectly balances false positives and false negatives for any given machine learning algorithm.   
Following Banerjiʼs example, we did in fact study the trade off between false positives and false negatives  
for each class in isolation.  For example, the ʻotherʼ category rarely had the highest probability of the three 
classes (probably due to there being relatively few training examples of this class in the data set), so our 
algorithms superﬁcially appeared to do poorly at classifying objects in other category.  However, we found  
that other objects could be more reliably identiﬁed if we chose a low threshold for determining whether an 
object might be in the other category. We found that P(other) > 19% was the optimal threshold that evenly 
traded off false positives and false negatives in this class for the training data.  Optimal thresholds for the 
elliptical and spiral probabilities were found to be around 43% and 45% respectively.  It should be noted, 
however, that setting thresholds in this manner allows an object to be placed into more than one category 
(or even into none at all).  This may or may not be desirable depending on the application, but we chose 
to analyze our results in this fashion here so that our comparison to Banerjiʼs study could be made as 
accurately as possible.

Michael Broxton – CS229 Final Report

5. Conclusions & Discussion
Techniques using spectral signatures outperformed hand-tuned features in all tests, although slightly 
better results were obtained by training a sparse network using a concatenated vector containing of both 
of these feature types.  The sparse network techniques outperformed others, with additional hidden layers 
yielding a modest increases in classiﬁcation performance.  Overall, we demonstrate a 2% improvement 
over Banerji et. al.ʼs classiﬁcation rates using our sparse network approach.  This is signiﬁcant, especially 
when considering that we achieved this performance using the same color images that served as ʻinputʼ 
for the human classiﬁers, and demonstrated that one need not rely on hand-tuned features to achieve 
good classiﬁcation performance.
Of the two sparse learning techniques presented here, the Sparse Network seems to be the better 
performing and more ﬂexible of the two.  Additional hidden layers can be added to increase performance 
somewhat, although we did notice that additional layers do increase the likelihood of over-ﬁtting to the 
training data.  We also found that the Sparse Network was better at learning an over-complete basis set 
that the ICA+SVM method.  Finally, the ʻsoftʼ classiﬁcations provided by the softmax output layer of the 
Sparse Network provide an intuitive and easily justiﬁed way to ﬁlter out ʻinconclusiveʼ classiﬁcations.  This 
guarantees better classiﬁcation rates if you can afford to throw out some galaxies that are difﬁcult to 
classify.
In the end, we have demonstrated a modest but meaningful improvement over existing techniques, with 
classiﬁcation rates of 99% for ʻcleanʼ galaxies that are easy for humans to classify.  For the full data set 
we have demonstrated classiﬁcation rates for elliptical and spiral galaxies of 93-94%; and this number 
could be increased even higher by raising the threshold and rejecting even more ʻinconclusiveʼ galaxies.  
Considering that human agreement with the full data set is sometimes less than 50% and often less than 
80%, this may be approaching the best performance achievable using Galaxy Zoo data.
Bibliography

M. Banerji et. al. Galaxy Zoo: reproducing galaxy morphologies via machine learning. Monthly Notices of the Royal Astronomical 
Society. Vol. 406. pp. 342–353 (2010)
N. M. Ball, et. al. Robust Machine Learning Applied to Astronomical Data Sets. I. Star-galaxy Classiﬁciation of the Sloan Digital Sky 
Survey DR3 using Decision Trees. The Astrophysical Journal. Vol. 650. pp. 497-509. (2006)
S. V. David and J. L. Gallant. Predicting Neuronal Responses during Natural Vision. Network: Computations in Neural Systems. Vol. 
16(2/3). pp. 239-260. (2005)
C. Elting, C. A. L. Bailer-Jones, K. W. Smith. Photometric Classiﬁcation of Stars, Galaxies and Quasars in the Sloan Digital Sky 
Survey DR6 Using Support Vector Machines. American Institute of Physics Conference Series. Vol. 1082. pp. 9-14 (2008)
O. Lahav, et. al. Galaxies, Human Eyes and Artiﬁcal Neural Networks. Science. pp. 859-862. (1995)
Y. Lecun, et. al. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE. Vol. 86. No. 11. pp. 
2278-2324 (1998)
C. Lintott, et. al. Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey. Monthly 
Notices of the Royal Astronomical Society. Vol. 389. pp. 1179–1180 (2008)
C. Lintott, et. al. Galaxy Zoo 1 : Data Release of Morphological Classiﬁcations for nearly 900,000 galaxies. Monthly Notices of the 
Royal Astronomical Society. pp. 1–14 (2010)
A. Ng. CS294A Lecture Notes: Sparse Autoencoder. Retrieved Nov. 17th, 2010 from http://www.stanford.edu/class/archive/cs/
cs294a/cs294a.1104/sparseAutoencoder.pdf
S.C. Odewahn, M.L. Nielsen. Star-Galaxy Separation using Neural Networks. Vistas in Astronomy. Vol. 38, pp. 281-286. (1994)
M. A. Nieto-Santisteban, A. S. Szalay, and J. Gray. ImgCutout, an Engine of Instantaneous Astronomical Discovery. ADASS XIII 
ASP Conference Series, Vol. XXX (2004)
Sloan Digital Sky Survey. Skyserver retrieved Nov. 17th, 2010 from http://cas.sdss.org/astro/en/tools/search/sql.asp
D. J. York, et. al The Sloan Digital Sky Survey: Technical Summary. The Astronomical Journal, Volume 120, Issue 3, pp. 1579-1587. 
(2000)

Michael Broxton – CS229 Final Report


A reinforcement learning approach for pricing derivatives

Thomas Grassl
Susquehanna International Group

tgrassl@stanford.edu
grassl.thomas@gmail.com

Abstract
Analytical solutions to the derivatives pric-
ing problem are known for only a small sub-
set of derivatives and are usually based on
strict assumptions. Practitioners will there-
fore frequently resort to numerical approxi-
mation techniques. In this paper, I will for-
mulate a simple Markov decision process for
which the optimal value function will, in a
non-arbitrage world, be equivalent to a given
derivative’s fair price function. This means
that derivatives pricing can be understood as
a reinforcement learning problem.
In order
to solve this problem I will propose a simpli-
ﬁed version of the kernel-based reinforcement
learning algorithm suggested in [4] and [6].

1. Introduction

One of the biggest challenges in ﬁnance is to prop-
erly price derivatives. Analytical fair price models are
only known for a small subset of derivatives and are
generally based on rather strict assumptions. Practi-
tioners thus usually turn to numerical approximation
techniques in order to estimate the fair price; common
approaches rely on the use of Monte Carlo simulations
(see [3] or [5]) or on methods rooted in dynamic pro-
gramming (see [1] or [8]). The latter cleverly relate
the fair price of a derivative to the optimal value func-
tion of a Markov decision process (MDP), but require
explicit knowledge of the MDP’s transition probabil-
ities in order to solve for the optimal value function.
Similarly, also Monte Carlo methods need to know the
probability distribution of the state space in order to
generate random samples from which they can extract
their fair price estimates. In reinforcement learning on
the other hand, state space and transition probabilities
are only used implicitly as the learning is based on tra-
jectory samples from the MDP. We can thus learn our
pricing model directly from data without having to as-

last revision: December 10, 2010.

sume that the underlying’s price follows a speciﬁc price
process. This means that one of the biggest pitfalls of
most derivatives pricing methods can be avoided.

2. Theoretical framework

2.1. The derivatives pricing problem

Let U be a risky asset with a stochastic price process
given by {Ut , t ≥ 0}. We can construct another risky
asset D such that its spot price Dt will be a determin-
istic function of Ut at exactly one point in the future,
i.e., at some time T the prices of D and U will be
related according to DT = g(UT ).
In ﬁnance, such
an asset D is called a derivative1 with underlying U
and maturity T . The function g is called the payoﬀ
function of the derivative D .
The derivatives pricing problem deals with the ques-
tion of how the price of D depends on Ut at times
t < T . In absence of arbitrage and under the assump-
tion that {Ut} is a Markov process2 , the fair price3 DF
t
is simply the discounted, expected payoﬀ of D given
Ut , i.e., if r is the risk-free interest rate then
t = e−r(T −t)E [g(UT )|Ut ].
(1)
DF
The diﬃcult part in (1) is to determine E [g(UT )|Ut ].
The function g might not be analytically tractable, or
even if it is, ﬁnding the expected value of g(UT ) re-
quires detailed knowledge of the structure of the prob-
abilistic price process {Ut , t ≥ 0}. Analytical solu-
tions to this problem depend on speciﬁc assumptions
on {Ut} which may break down when applying the re-
sulting model in practice. The classic Black-Scholes
model for example assumes that the returns of Ut are
sampled from a continuous, log-normal distribution,
and by doing so fails to acknowledge the existence of
jump discontinuities or fat tails that can often be ob-
served in real world price data.

1This deﬁnition was chosen because it simpliﬁes much
of the subsequent work. However, most results should also
be applicable to more complicated derivates.
2Note that this is the only assumption that we will im-
pose on the stochastic process {Ut , t ≥ 0}.
3Note the diﬀerence in notation between spot price Dt
and fair price DF
t .

A reinforcement learning approach for pricing derivatives

2.2. Trading as a Markov decision process

Using (1) we can see that this is nothing else than the
fair price of D :

V πH (st ) = DF
t .

(5)

Suppose now that πH is not optimal, i.e., that there is
a state sτ where it is better to sell than to hold:

QπH (sτ , aS ) > QπH (sτ , aH ).

Using equation (4) this implies that
Dτ > e−rE [V πH (sτ +1 )|sτ ] .
The assumption that {Ut} is Markov implies that
e−rE [V πH (sτ +1 )|sτ ] = e−r(T −τ )E [g(UT )|Uτ ] = DF
τ
and thus that Dτ > DF
τ .
Since in a no-arbitrage world Dt = DF
for all t, it
t
follows that no such state sτ can exist, and therefore
that πH is an optimal strategy. A direct implication
of this is that the optimal value function V ∗ satisﬁes
V ∗ (st ) = DF
t
for all states st . This shows that in the absence of
arbitrage, learning the optimal value function V ∗ is
t . In
equivalent to learning the fair value function DF
other words, by solving the above MDP for V ∗ , we will
be able to solve the derivatives pricing problem.
t .
Note that the above derivation required that Dt = DF
The result however remains valid even if the market
tends to underestimate the fair price (i.e. Dt ≤ DF
t )
since πH would still be an optimal policy. If we allow
Dt to be greater than DF
t , then also the resulting value
t . It would thus pro-
function would be greater than DF
vide a measure for the expected extent of mispricings
that a trader can exploit in this ﬂawed market.

(6)

2.4. Generalizing the fair price model

In the above section I have shown that for a speciﬁc
derivative D its fair price can be determined by learn-
ing the optimal value function of a simple MDP. While
theoretically useful, this approach would likely be un-
practical in real-world trading as traders would be re-
quired to maintain a separate model for each derivative
they are interested in. Pricing models with analytical
solutions, e.g., the classic Black-Scholes model, do not
suﬀer from this shortcoming and can easily be applied
to a whole class of derivatives. Such a generalization is
achieved by ﬁnding a suitable parameterization of the
considered derivative; the classic Black-Scholes pricing
approach for example parameterizes a European op-
tion as a tuple consisting of expiration time T , strike

at =

V π (st ) = E

R(st , at ) =

(2)

Suppose now that a trader holds exactly one unit of a
derivative D at time t. At each time prior to expiry
T , he can either sell D at the current spot price Dt or
decide to hold on to it; if he still holds D at time T ,
both selling and holding will have the same outcome:
the derivative will be executed (or sold) for the price
DT = g(UT ). A trading episode ends as soon as D is
either sold or executed. We will denote the trader’s
(cid:26) aS
possible actions as
if D is being sold
aH otherwise
and assume that neither action can aﬀect the market’s
behaviour, i.e., Dt+1 and Ut+1 or, more generally, the
market state mt+1 are independent of at . Only the
trader’s position qt+1 depends on his actions. Com-
bining mt and qt yields the state of the world at time
t, st = (mt , qt ). The state-action reward in this model
will simply be equal to the trader’s monetary compen-
 Dt
sation, i.e.
if at = aS , t < T
if t = T , qt = 1
DT = g(UT )
otherwise
0
For the sake of simplicity, we will assume that all
episodes end at time T ; if qt = 0 for t < T , then the
trader will simply have to choose aH at each timestep
between t and T (and thus receive zero reward).
Using R(st , at ) as deﬁned above, the sum of expected
(cid:34) T(cid:88)
(cid:35)
future rewards of a given strategy (or policy) π dis-
(cid:12)(cid:12)(cid:12) st
counted to time t can be written as
τ =t
From this is it obvious that the value function V π of
our trading strategy obeys the Bellman equation
V π (st ) = R(st , π(st )) + e−rE [V π (st+1 )|st ] .
Note now that because of the special structure of the
rewards outlined in (2), the state-action value function
(cid:26) e−rE [V π (st+1 )|st ]
Q satisﬁes
Dt

e−r(τ −t)R(sτ , π(sτ ))

Qπ (st , at ) =

if at = aH
if at = aS

(4)

.

(3)

2.3. Equivalence of optimal value and fair
price function
For the policy πH where the trader will hold D un-
til expiry, the corresponding value function V πH can
easily be computed:
V πH (st ) = e−r(T −t)E [g(UT )|Ut ] .

A reinforcement learning approach for pricing derivatives

price K and the volatility of returns σ of the underly-
ing asset. The trading MDP can be generalized with
a similar trick: we simply absorb4 the characteristics
of a derivative D into the state st by using a suitable
parameterization ψD and deﬁne an enhanced state

t = (st , ψD ) .
sD
As long as the mapping D → ψD is bijective, it is im-
t+1 where D (cid:54)= ˜D .
t to s ˜D
possible to move from a state sD
This means that we will learn the optimal value func-
tion V ∗ (sD
t ) using the same tra jectories as before and
thus that in a non-arbitrage world V ∗ (sD
t ) = DF
t . If
on the other hand D → ψD is not a bijective mapping
then either V ∗ (sD
t ) = V ∗ (s ˜D
t ) is the desired outcome
t = s ˜D
for sD
t or ψ needs to be improved.
Note that for a bijective parameterization on a discrete
state space, this trick only provides a uniﬁed notation
for accessing the fair price model of distinct deriva-
tives; learning the fair price still takes place separately
for each of the considered derivatives. This will how-
ever change when we attempt to compute approximate
value functions over continuous state spaces.

3. Implementation considerations

3.1. Exploitation vs. Exploration

A problem inherent to reinforcement learning is the ex-
ploration/exploitation dilemma where a reinforcement
agent faces a trade-oﬀ between maximizing the short-
term reward by exploiting his current knowledge of the
MDP or maximizing the long-term reward by explor-
ing unknown regions of the state-action space (see [7]).
The special nature of the trading MDP eliminates this
dilemma: the state st is described as a tuple (mt , qt )
where the trader’s position qt is directly and determin-
istically aﬀected by his actions. A transition from mt
to mt+1 will thus provide information regarding both
possible action choices aS and aH . This property of
the trading MDP simpliﬁes the collection of data as
only the transitions of the market states mt need to
be observed.

3.2. Kernel-Based Reinforcement Learning

Trading is essentially a ﬁnite MDP: a trader only needs
to act at discrete times (e.g., when new information be-
comes available), prices and sizes can only change in
discrete increments and the action space in the above
MDP consists of only two choices, aH and aS . How-
ever, the dimensions of this MDP can quickly become

4For a practical example of how to absorb the deriva-
tive’s parameters into the state space see section (4.2).

so large that the problem is computationally unman-
ageable. To avoid this, we will assume that the trading
MDP has a continuous state space.
Most canonical reinforcement learning algorithms and
their corresponding convergence guarantuees deal with
the problem of approximating optimal policies for ﬁ-
nite state spaces and can usually not easily be gener-
alized to the continuous case. Instead, reinforcement
learning in continuous state space often attempts to
approximate the optimal value function directly from
a given sample of tra jectories from the MDP.
One approach that is equipped with a guarantueed
convergence to the optimal value function V ∗ is
Kernel-Based Reinforcement Learning (see [6]). A
meaningful modiﬁcation to it is provided in [4]:
it is
shown that ﬁnding the approximate value function of
an exact, continuous MDP can be understood to be
equivalent to ﬁnding the exact value function of an ap-
proximate, ﬁnite MDP while still maintaining the same
convergence guarantuees. Applying this approach to
the trading MDP essentially means that we will inter-
pret samples of a high-dimensional discrete MDP as
samples from a continuous MDP which we will then
solve by approximating it with a simpler discrete MDP.

3.3. Constructing an approximate MDP

˜T (s, a, ˆs(i) ) =

Suppose
sampled n transitions
that we have
(s(i) , ˆs(i) , a(i) ) (where we transitioned from state s(i)
to ˆs(i) as a result of action a(i) ) from an MDP
M = (S, A, T , R) with continuous state space S , dis-
crete action space A, transition probabilities T and
reward function R.
Jong and Stone (see [4]) use
these samples to approximate M with a ﬁnite MDP
˜M = (D , A, ˜T , ˜R) where D = {ˆs(i) } is the set of suc-
cessor states. For some kernel function φ with suitably
chosen bandwidth b and premetric d, ˜T and ˜R are de-
(cid:40) 1
(cid:17)
(cid:16) d(s,s(i) )
ﬁned as
if a(i) = a
Z s,a φ
(cid:18) d(s, s(i) )
(cid:19)
(cid:88)
b
0
otherwise
1
˜R(s, a) =
(cid:19)
(cid:18) d(s, s(i) )
φ
Z s,a = (cid:88)
Z s,a
b
i|a(i)=a
b
i|a(i)=a
The authors argue that the exact solution of ˜M , ˜V ∗ ,
converges to the exact solution of M , V ∗ , as the num-
ber of samples increases.
In our trading MDP, the market transitions indepen-
dently of the chosen action a; this yields the following

R(s(i) , a(i) )

φ

.

.

R(s(i) , a)

A reinforcement learning approach for pricing derivatives
(cid:18) d(m, m(i) )
(cid:19)
simpliﬁed version of the above expressions:
1
(cid:18) d(m, m(i) )
(cid:19)
˜T (s, a, ˆs(i) ) = ˜T (m, ˆm(i) ) =
(cid:88)
Z m φ
b
1
(cid:19)
(cid:18) d(m, m(i) )
˜R(s, a) =
Z m = (cid:88)
φ
Z m
b
i
φ
b
i
˜Q(s, a) = ˜R(s, a) + e−r (cid:88)
For the state-action value function ˜Q, it follows that
(cid:26) ˜R(s, a)
˜T (m, ˆm(i) ) ˜V (ˆs(i) )
e−r (cid:80)
i
if a = aS
=
˜T (m, ˆm(i) ) ˜V (ˆs(i) ) otherwise
i
where ˜V (s) = maxa ˜Q(s, a). Thus, estimating V ∗ boils
down to two steps: generating ˜T by observing enough
market transitions, and then solving the MDP ˜M (for
example by using value iteration). A signiﬁcant com-
putational obstacle is that the size of ˜T is quadratic
in the number of observed samples. In [4], the authors
suggest that very small entries of ˜T should be set to
zero.
In the below example this resulted in a very
sparse transition matrix5 without noticeably aﬀecting
the approximation quality of the model.

K :
strike price 1 and underlying prices given by Ut
Ct = K ˜Ct .
Instead of learning Ct directly, one can thus learn ˜Ct
and recover Ct as required. Using strike-relative prices
K yields a more compact price space and makes it
Ut
feasible to eﬃciently learn our model using data from
a variety of derivatives with potentially vastly diﬀerent
price levels.
Note now that σWt models the randomness in the
Black-Scholes world. Since Wt is a Wiener process,
the future price risk σ(WT − Wt ) is distributed ac-
√
T − t). This suggests that in order
cording to N (0, σ
to capture the random extent of future price move-
ments, it is not necessary to explicitly use t, T and σ
√
as separate state features; instead we can use a single
T − t.
feature equal to σ
These observations indicate that a compact but rea-
(cid:19)
(cid:18) Ut
sonably complete deﬁnition of a market state is
√
K
This deﬁnition should enable us to easily combine
derivatives with diﬀerent underlyings, strike prices, ex-
piration times or volatilities into the same model. Ut ,
K and T −t are all directly observable while the volatil-
ity σ needs to be estimated from historical data.6

T − t

mt =

, σ

.

4. Example: Pricing a European call

4.1. The classic Black-Scholes approach
Suppose that the underlying’s spot price Ut follows a
log-normal random walk with drift µ and volatility σ

dUt = µUt dt + σUt dWt

where Wt is a Wiener process. Using a non-arbitrage
argument it can be shown that µ has to equal the risk-
less interest rate r . In such a setting, the Black-Scholes
model (see [2]) deﬁnes the fair price of a European call
option Ct with strike price K and expiration T as
Ct = UtN (d1 ) − K e−r(T −t)N (d2 )
where N is the standard normal cumulative distribu-
tion function and
ln(Ut/K ) + (r ± σ2 /2)(T − t)
√
T − t
σ

d1|2 =

(7)

.

4.2. Parameterizing the state space

Equation (7) can be normalized by recognizing that
Ct is the product of K and the price of a call ˜Ct with
5A pruning threshold of 0.001 reduced the fraction of
nonzero entries from 67% to 1%.

4.3. Results

I initially implemented the above version of Kernel-
Based Reinforcement Learning using a Euclidian dis-
tance metric d and a Gaussian kernel φ. Intuitively, a
small bandwidth b should result in a very bumpy value
function approximation ˜V ∗ as each sampled transition
(m(i) , ˆm(i) ) can only noticeably aﬀect predictions in
its close proximity. As b increases, the value function
should take on an increasingly smooth shape. Initial
experiments conﬁrmed this inuition, but also unveiled
that, for large values of b, ˜V ∗ was consistently over-
estimating the fair price of out-of-the money options:
for large b, the perceptive regions of φ around sam-
pled transitions will frequently overlap, which means
that high prices from rather far away can be propa-
gated through to regions of the state space where the
considered option is nearly worthless. This problem is
intrinsic to the chosen algorithm.
We can ameliorate this problem by recognizing that

6Note that σ is needed for diﬀerentiating between dif-
ferent underlyings; if we would focus on a single underlying
we could disregard σ as the model should pick up this in-
formation directly from the data.

A reinforcement learning approach for pricing derivatives

the Euclidian distance measure is unbiased with re-
gards to the single components of a state: e.g., points
given by (x1 + c, x2 ) and (x1 , x2 + c) are equally far
way from (x1 , x2 ). Such symmetry is not really de-
sired when pricing options: it seems to be a better idea
to compare options with similar strike-relative prices
but diﬀerent risk than to compare options with similar
√
risk but diﬀerent strike-relative prices. Furthermore,
T − t is decreasing with
notice that the future risk σ
inreasing t. Since the true value function reﬂects the
expected future reward, its approximation should ide-
ally only depend on states that truly lie in the future,
i.e., on states where the future risk is smaller than
in the current state. If we would strictly enforce this
forward-looking perspective, estimates close to expiry
would suﬀer from a lack of data; this suggests that
we need a compromise that is looking forward more
than it is looking backwards. These ideas can be in-
corporated into the model by modifying the distance
√
function d such that its contours are egg-shaped curves
T − t.
tilted towards smaller values of σ
This change greatly improved the quality of the model.
Figures (1) and (2) show the fair price approximation
(using b = 0.13) and its associated error as compared
to the Black-Scholes price for a European call with
strike price 10, annualized volatility of 40% and an an-
nual riskless interest rate of 2%. Learning was based
on data from 1000 randomly constructed European
calls (with diﬀerent underlyings, strike prices, expi-
rations etc.) and roughly 11000 randomly generated
transitions; the returns of the respective underlyings
were sampled from a log-normal distribution and the
spot prices of the derivatives were assumed to be equal
to the exact Black-Scholes price. The resulting root-
mean squared error of this approximation corresponds
to a $-value of less than 0.02 which is a promising ﬁrst
result. The model’s behaviour was consistent across
successive runs with diﬀerent initializations of the ran-
dom number generator.

5. Conclusion

I showed how the derivatives pricing problem can be
written as an MDP to which reinforcement learning
techniques can readily be applied. The proposed algo-
rithm, a version of Kernel-Based Reinforcement Learn-
ing, delivered enouraging results on a simple test prob-
lem, but extensive testing is needed in order to evalu-
ate the quality of this approach in more realistic sce-
narios. Applying the model to American or Asian op-
tions and avoiding the quadratic storage requirements
for ˜T by making better use of its sparseness could be
promising directions for future research.

Figure 1. Approximated value function ˜V ∗ for a European
call with strike K = 10

Figure 2. Approximation error ˜V ∗t − Ct

References

[1] Hatem Ben-Ameur, Mich`ele Breton, Lotﬁ Karoui, and Pierre
L’Ecuyer. A dynamic programming approach for pricing options
Journal of Economic Dynamics and Control,
embedded in bonds.
31(7):2212 – 2233, 2007.

[2] Fischer Black and Myron Scholes. The Pricing of Options and Cor-
porate Liabilities. The Journal of Political Economy, 81(3):637–654,
1973.

[3] Mark Broadie and Paul Glasserman. Pricing American-style securities
using simulation. Journal of Economic Dynamics and Control, 21(8-
9):1323 – 1352, 1997. Computational ﬁnancial modelling.

[4] Nicholas Jong and Peter Stone. Kernel-based models for reinforcement
In ICML workshop on Kernel
learning in continuous state spaces.
Machines and Reinforcement Learning, June 2006.

[5] Francis A. Longstaﬀ and Eduardo S. Schwartz. Valuing American
options by simulation: a simple least-squares approach. Review of
Financial Studies, 14(1):113–147, 2001.

[6] Dirk Ormoneit and ´Saunak Sen. Kernel-based reinforcement learning.
Machine Learning, 49:161–178, November 2002.

[7] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An
Introduction (Adaptive Computation and Machine Learning). The MIT
Press, March 1998.

[8] John N. Tsitsiklis and Benjamin Van Roy. Regression methods for
pricing complex American-style options. IEEE Transactions on Neural
Networks, 12:694–703, 2000.

Ut8.08.59.09.510.010.511.011.512.0T−t0246810˜V∗t0.00.51.01.52.0Ut8.08.59.09.510.010.511.011.512.0T−t0246810˜V∗t0.00.51.01.52.0Ut8.08.59.09.510.010.511.011.512.0T−t0246810˜V∗t−Ct−0.05−0.04−0.03−0.02−0.010.000.010.020.03Ut8.08.59.09.510.010.511.011.512.0T−t0246810˜V∗t−Ct−0.05−0.04−0.03−0.02−0.010.000.010.020.03
Global seismic monitoring as probabilistic inference

Nimar S. Arora
Department of Computer Science
University of California, Berkeley
Berkeley, CA 94720
nimar@cs.berkeley.edu

Paul Kidwell
Lawrence Livermore National Lab
Livermore, CA 94550
kidwell1@llnl.gov

Stuart Russell
Department of Computer Science
University of California, Berkeley
Berkeley, CA 94720
russell@cs.berkeley.edu

Erik Sudderth
Department of Computer Science
Brown University
Providence, RI 02912
sudderth@cs.brown.edu

Abstract

The International Monitoring System (IMS) is a global network of sensors whose
purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban
Treaty (CTBT), primarily through detection and localization of seismic events.
We report on the ﬁrst stage of a project to improve on the current automated
software system with a Bayesian inference system that computes the most likely
global event history given the record of local sensor data. The new system, VISA
(Vertically Integrated Seismological Analysis), is based on empirically calibrated,
generative models of event occurrence, signal propagation, and signal detection.
VISA exhibits signiﬁcantly improved precision and recall compared to the current
operational system and is able to detect events that are missed even by the human
analysts who post-process the IMS output.

1

Introduction

The CTBT aims to prevent the proliferation and the advancement of nuclear weapon technology
by banning all nuclear explosions. A global network of seismic, radionuclide, hydroacoustic, and
infrasound sensors, the IMS, has been established to enforce the treaty. The IMS is the world’s
primary global-scale, continuous, real-time system for seismic event monitoring. Data from the IMS
sensors are transmitted via satellite in real time to the International Data Center (IDC) in Vienna,
where automatic event-bulletins are issued at predeﬁned latency. Perfect performance remains well
beyond the reach of current technology: the IDC’s automated system, a highly complex and well-
tuned piece of software, misses nearly one third of all seismic events in the magnitude range of
interest, and about half of the reported events are spurious. A large team of expert analysts post-
processes the automatic bulletins to improve their accuracy to acceptable levels.
Like most current systems, the IDC operates by detection of arriving signals at each sensor station
(the station processing stage) and then grouping multiple detections together to form events (the
network processing stage).1 The time and location of each event are found by various search methods
including grid search [2], the double-difference algorithm [3], and the intersection method [4]. In
the words of [5], “Seismic event location is—at its core—a minimization of the difference between
observed and predicted arrival times.” Although the mathematics of seismic event detection and

1Network processing is thus a data association problem similar to those arising in multitarget tracking [1].

1

localization has been studied for almost 100 years [6], the IDC results indicate that the problem is
far from trivial.
There are three primary sources of difﬁculty: 1) the travel time between any two points on the earth
and the attenuation of various frequencies and wave types are not known accurately; 2) each detector
is subject to local noise that may mask true signals and cause false detections (as much as 90% of
all detections are false); and 3) there are many thousands of detections per day, so the combinatorial
problem of proposing and comparing possible events (subsets of detections) is daunting. These con-
siderations suggest that an approach based on probabilistic inference and combination of evidence
might be effective, and this paper demonstrates that this is in fact the case. For example, such an
approach automatically takes into account non-detections as negative evidence for a hypothesized
event, something that classical methods cannot do.
In simple terms, let X be a random variable ranging over all possible collections of events, with
each event deﬁned by time, location, magnitude, and type (natural or man-made). Let Y range
over all possible waveform signal recordings at all detection stations. Then Pθ (X ) describes a
parameterized generative prior over events, and Pφ (Y | X ) describes how the signal is propagated
and measured (including travel time, selective absorption and scattering, noise, artifacts, sensor
bias, sensor failures, etc.). Given observed recordings Y = y , we are interested in the posterior
P (X | Y = y), and perhaps in the value of X that maximizes it—i.e., the most likely explanation
for all the sensor readings. We also learn the model parameters θ and φ from historical data.
Our overall project, VISA (Vertically Integrated Seismic Analysis), is divided into two stages. The
ﬁrst stage, NET-VISA, is the subject of the current paper. As the name suggests, NET-VISA deals
only with network processing and relies upon the IDC’s pre-existing signal detection algorithms.
(The second stage, SIG-VISA, will incorporate a signal waveform model and thereby subsume the
detection function.) NET-VISA computes a single most-likely explanation: a set of hypothesized
events with their associated detections, marking all other detections as noise. This input-output
speciﬁcation, while not fully Bayesian in spirit, enables direct comparison to the current automated
system bulletin, SEL3. Using the ﬁnal expert-generated bulletin, LEB, as ground truth, we compared
the two systems on 7 days of held-out data. NET-VISA has 16% more recall at the same precision
as SEL3, and 25% more precision at the same recall as SEL3. Furthermore, taking data from the
more comprehensive NEIC (National Event Information Center) database as ground truth for the
continental United States, we ﬁnd that NET-VISA is able to detect events in the IMS data that are
not in the LEB report produced by IDC’s expert analysts; thus, NET-VISA’s true performance may
be higher than the LEB-based calculation would suggest.
The rest of the paper is structured as follows. Section 2 describes the problem in detail and cov-
ers some elementary seismology. Sections 3 and 4 describe the probability model and inference
algorithm. Section 5 presents the results of our evaluation, and Section 6 concludes.

2 The Seismic Association and Localization Problem

Seismic events are disturbances in the earth’s crust. Our work is concerned primarily with earth-
quakes and explosions (nuclear and conventional), but other types of events—waves breaking, trees
falling, ice falling, etc.—may generate seismic waves too. All such waves occur in a variety of types
[7]—body waves that travel through the earth’s interior and surface waves that travel on the surface.
There are two types of body waves—compression or P waves and shear or S waves. There are also
two types of surface waves—Love and Rayleigh. Further, body waves may be reﬂected off different
layers of the earth’s crust and these are labeled distinctly by seismologists. Each particular wave
type generated by a given event is called a phase. These waves are picked up in seismic stations
as ground vibrations. Typically, seismic stations have either a single 3-axis detector or an array
of vertical-axis detectors spread over a scale of many kilometers. Most detectors are sensitive to
nanometer-scale displacements, and so are quite susceptible to noise.
Raw seismometer measurements are run through standard signal processing software that ﬁlters out
non-seismic frequencies and computes short-term and long-term averages of the signal amplitude.
When the ratio of these averages exceeds a ﬁxed threshold, a detection is announced. Various
parameters of the detection are measured—onset time, azimuth (direction from the station to the
source of the wave), slowness (related to the angle of declination of the signal path), amplitude, etc.

2

Based on these parameters, a phase label may be assigned to the detection based on the standard
IASPEI phase catalog [7]. All of these detection attributes may be erroneous.
The problem that we attempt to solve in this paper is to take a continuous stream of detections
(with onset time, azimuth, slowness, amplitude, and phase label) from the roughly 120 IMS seismic
stations as input and produce a continuous stream of events and associations between events and
detections. The parameters of an event are its longitude, latitude, depth, time, and magnitude (mb
or body-wave magnitude). A 3-month dataset (660 GB) has been made available by the IDC for the
purposes of this research. We have divided the dataset into 7 days of validation, 7 days of test, and
the rest as training data. We compute the accuracy of an event history hypothesis by comparison to a
chosen ground-truth history. A bipartite graph is created between predicted and true events. An edge
is added between a predicted and a true event that are at most 5 degrees in distance2 and 50 seconds
in time apart. The weight of the edge is the distance between the two events. Finally, a min-weight
max-cardinality matching is computed on the graph. We report 3 quantities from this matching—
precision (percentage of predicted events that are matched), recall (percentage of true events that
are matched), and average error (average distance in kilometers between matched events).

3 Generative Probabilistic Model

Our generative model for seismic events and detections follows along the lines of the aircraft de-
tection model in [8, Figure 3]. In our model, there is an unknown number of seismic events with
unknown parameters (location, time, etc.). These events produce 14 different types of seismic waves
or phases. A phase from an event may or may not be detected by a station. If a phase is detected at
a station, a corresponding detection is generated. However, the parameters of the detection may be
imprecise. Additionally, an unknown number of noise detections are generated at each station. For
NET-VISA, the evidence Y = y consists only of each station’s set of detections and their parameters.

3.1 Events

The events are generated by a time-homogeneous Poisson process.
If e is the set of events (of
size |e|), λe is the rate of event generation, and T is the time period under consideration, we have
(λe · T )|e| exp (−λe · T )
(1)
Pθ (|e|) =
.
|e|!
The longitude and latitude of the ith event, ei
l are drawn from an event location density, pl (el )
d is uniformly distributed up to a maximum
on the surface of the earth. The depth of the event, ei
t is uniformly distributed
depth D (700 km in our experiments). Similarly, the time of the event ei
m , is drawn from what seismologists refer to as the
between 0 and T . The magnitude of the event, ei
(cid:1) .
λm exp (cid:0)
Gutenberg-Richter distribution, which is in fact an exponential distribution with rate λm :
−λm ei
m
Since all the events are exchangeable, we have
|e|(cid:89)
i=1

(cid:1) .
λeλm exp (cid:0)
Maximum likelihood estimates of λe and λm may be easily determined from historical event fre-
quencies and magnitudes. To approximate pl (el ), we use a kernel density estimate derived from the
following exponentially decaying kernel:

Pθ (ei ) = exp (−λe · T )

Pθ (e) = Pθ (|e|) · |e|! ·

Pθ (ei ) = pl (ei
l )

|e|(cid:89)
i=1

−λm ei
m

pl (ei
l )

1
D

1
D

1
T

(2)

(3)

1 + 1/b2
2πR2

Kb,x (y) =

exp (−∆xy /b)
1 + exp (−π/b)
2 In this paper, by distance between two points on the surface of the earth we refer to the great-circle distance.
This can be represented in degrees, radians, or kilometers (using the average earth radius of 6371 km).

(4)

.

3

Figure 1: Heat map (large values in red, small in blue) of the prior event location density log pl (el ).

Here b > 0 is the bandwidth, ∆xy is the distance (in radians) between locations x and y on the
surface of the earth, and R is the earth’s radius. The bandwidth was estimated via cross-validation.
In addition, we additively mixed this kernel density with a uniform distribution, with prior proba-
bility 0.001, to allow the possibility of explosions at an arbitrary location. The overall density, as
illustrated in Figure 1, was pre-computed on a one degree grid and interpolated during inference.

(5)

3.2 Correct Detections
The probability that an event’s j th phase, 1 ≤ j ≤ J , is detected by a station k , 1 ≤ k ≤ K ,
depends on the wave type or phase, the station, and the event’s magnitude, depth, and distance to the
station. Let dijk be a binary indicator variable for such a detection of event i, and ∆ik the distance
between event i and station k . Then we have
Pφ (dijk = 1 | ei ) = pjk
m , ei
d (ei
d , ∆ik ).
If an event phase is detected at a station, i.e. dijk = 1, our model speciﬁes probability distribution
for the attributes of that detection, aijk . The arrival time, aijk
, is assigned a Laplacian distribu-
t
tion whose mean consists of two parts. The ﬁrst is the IASPEI travel time prediction for that phase,
which depends only on the event depth and the distance between the event and station. The second is
a learned station-speciﬁc correction which accounts for inhomogeneities in the earth’s crust, which
allow seismic waves to travel faster or slower than the IASPEI prediction. The station-speciﬁc cor-
rection also accounts for any systematic biases in picking seismic onsets from waveforms. Let µjk
t
be the location of this Laplacian (a function of the event time, depth, and distance to the station)
and let bjk
t be its scale. Truncating this Laplacian to the range of possible arrival times produces a
(cid:32)
(cid:33)
normalization constant Z jk
, so that
t
t − µjk
− |aijk
t , ei
t (ei
d , ∆ik )|
bjk
t
of the
Similarly, the arrival azimuth and slowness follow a Laplacian distribution. The location aijk
z
arrival azimuth depends only on the location of the event, while the location aijk
s of the arrival slow-
ness depends only on the event depth and distance to the station. The scales of all these Laplacians
(cid:18)
(cid:19)
are ﬁxed for a given phase and station, so that
(cid:18)

| dijk = 1, ei ) =

Pφ (aijk
t

Pφ (aijk
z

1
Z jk
t

exp

exp

(6)

.

| dijk = 1, ei ) =
| dijk = 1, ei ) =

1
Z jk
z
1
Z jk
s

exp

,

− |aijk
z − µjk
z (ei
l )|
bjk
z
− |aijk
s − µjk
s (ei
d , ∆ik )|
bjk
s

(cid:19)

.

Pφ (aijk
s

(7)

(8)

4

.

(9)

exp

Pφ (aijk
a

is similar to the detection probability in that it depends only on the event
The arrival amplitud aijk
a
(cid:32)
(cid:33)
magnitude, depth, and distance to the station. We model the log of the amplitude via a linear regres-
sion model with Gaussian noise:
−

a (ei
m , ei
a ) − µjk
(log(aijk
d , ∆ik ))2
1
| dijk = 1, ei ) =
√2πσ jk
2
2σ jk
a
a
Finally, the phase label aijk
automatically assigned to the detection follows a multinomial distribu-
h
tion whose parameters depends on the true phase, j :
h (aijk
| dijk = 1, ei ) = pjk
Pφ (aijk
h ).
h
The phase- and station-speciﬁc detection distributions, pjk
d (·), were obtained using logistic re-
gression models estimated via a hierarchical Bayesian procedure [9]. Because phase labels indi-
cate among other things the general physical path taken from an event to a station, a distinct set
of features were learned from the event characteristics for each phase. To estimate the individ-
ual station weights αwjk for each phase j and feature w , a hierarchical model was speciﬁed in
which each station-speciﬁc weight is independently drawn from a feature-dependent global Normal
wj ). Weakly informative diffuse priors µwj ∼ N (0, 1002 ),
distribution, so that αwjk ∼ N (µwj , σ2
σ−2
wj ∼ Gamma(0.01, 0.01), were placed on the parameters of these global distributions, and pos-
terior mean estimates of the station-speciﬁc weights obtained via Gibbs sampling. Figure 2 shows
two of the empirical and modeled distributions for one phase-site.

(10)

Figure 2: Conditional detection probabilities and arrival time distributions (relative to the IASPEI
prediction) for the P phase at Station 6.

3.3 False Detections

.

−λk
f · T

(cid:17)
(cid:16)
Each station, k , also generates a set of false detections f k through a time-homogeneous Poisson
process with rate λk
f :
f · T )|f k | exp
(λk
Pφ (|f k |) =
|f k |!
z , and slowness f kl
The time f kl
t , azimuth f kl
s of these false detections are generated uniformly over
a of the false detection is generated from a mixture of two
their respective ranges. The amplitude f kl
Gaussians, pk
a ). Finally, the phase label f kl
h assigned to the false detection follows a multinomial
a (f kl
distribution, pk
h ). If the azimuth and slowness take values on ranges of length Mz and Ms ,
h (f kl
respectively, then the probability of the lth false detection is given by
1
1
1
h (f kl
a )pk
a (f kl
pk
h ) .
T
Mz
Ms
f · T (cid:1) l=|f k |(cid:89)
l=|f k |(cid:89)
Pφ (f kl ) = exp (cid:0)
Since the false detections at a station are exchangeable, we have
−λk
Pφ (f k ) = Pφ (|f k |) · |f k |!
l=1
l=1

h (f kl
a )pk
a (f kl
pk
h ) .

λk
f
MzMs

Pφ (f kl ) =

(11)

(12)

(13)

5

020406080100120140160180Distance(deg)0.00.20.40.60.81.0ProbabilityDetectionprobabilityatstation6forPphase,surfaceeventmodel3.5mbdata3–4mb−6−4−20246Time0.000.020.040.060.080.10ProbabilityTimeResidualsaroundIASPEIpredictionforPphaseatstation6modeldata4

Inference

Combining the model components developed in the preceding section, the overall probability of any
hypothesized sequence of events e, detected event phases d, arrival attributes a for correctly detected
event phases, and arrival attributes f for falsely detected events is
(14)
P (e, d, a, f ) = Pθ (e)Pφ (d | e)Pφ (a | d, e)Pφ (f ).
We will attempt to ﬁnd the most likely explanation consistent with the observations. This involves
determining e, d, a, and f which maximize P (e, d, a, f ), such that the set of detections implied
by d, a, and f correspond exactly with the observed detections. Since detections from real seismic
sensors are observed incrementally and roughly in time-ascending order, our inference algorithm
also produces an incremental hypothesis which advances with time. Our algorithm can be seen as a
form of greedy search, in which the current hypothesis is improved via a set of local moves.
Let MT denote the maximum travel time for any phase. Initially, we start with an event-window
of size W from t0 = 0 to t1 = W , and a detection-window of size W + MT from t0 = 0 to
t1 = W + MT . Our starting hypothesis is that all detections in our detection-window are false
detections and there are no events. We then repeatedly apply the birth, death, improve-event, and
improve-detection moves (described below) for a ﬁxed number of iterations (N times the number of
detections in that window) before shifting the windows forward by a step size S . Any new detections
added to the detection window are again assumed to be false detections. As the windows move
forward the events older than t0 − MT become stable: none of the moves modify either the event or
detections associated with them. These events are then output. While in theory this algorithm never
needs to terminate, our experiments continue until the test dataset is fully consumed.
In order to simplify the computations needed to compare alternate hypotheses, we decompose the
overall probability of Eq. (14) into the contribution from each event. We deﬁne the score Se of an
event as the probability ratio of two hypotheses: one in which the event exists, and another in which
the event doesn’t exist and all of its associated detections are noise. If an event has score less than 1,
 .
δ(dijk, 0) + δ(dijk, 1)
an alternative hypothesis in which the event is deleted clearly has higher probability. Critically, this
(cid:16)−λm ei
(cid:17) (cid:89)
event score is unaffected by other events in the current hypothesis. From Eqs. (3) and (13) we have
Pφ (aijk | dijk , ei )
m
λk
h (f kl
a )pk
f
a (f kl
pk
h )
j,k
MzMs
Note that the ﬁnal fraction is a likelihood ratio comparing interpretations of the same detection as
either the detection of event i’s j th phase at station k , or the lth false detection at station k . We
can further decompose the score into scores Sd for each detection. The score of dijk , deﬁned when
dijk = 1, is the ratio of the probabilities of the hypothesis where the detection is associated with
phase j of event i at station k , and one in which this detection is false and phase j of event i is
missed by station k :

pl (ei
l )λeλm
D

Pφ (dijk | ei )

Se (ei ) =

exp

Sd (dijk ) =

pjk
m , ei
d (ei
Pφ (aijk | dijk , ei )
d , ∆ik )
1 − pjk
λk
m , ei
d (ei
d , ∆ik )
a )pk
h (f kl
a (f kl
pk
f
h )
MzMs
By deﬁnition, any detection with score less than 1 is more likely to be a false detection. Also, the
score of an individual detection is independent of other detections and unassociated events in the
hypothesis. These scores play a key role in the following local search moves.

(15)

.

Birth Move We randomly pick a detection, invert it into an event location (using the detection’s
time, azimuth, and slowness), and sample an event in a 10 degree by 100 second ball around this
inverted location. The depth of the event is ﬁxed at 0, and the magnitude is uniformly sampled.

Improve Detections Move For each detection in the detection window, we consider all possible
phases j of all events i up to MT seconds earlier. We then associate the best event-phase for this
detection that is not already assigned to a detection with higher score at the same station k . If this
best event-phase has score Sd (dijk ) < 1, the detection is changed to a false detection.

Improve Events Move For each event ei , we consider 10 points chosen uniformly at random in a
small ball around the event (2 degrees in longitude and latitude, 100 km in depth, 5 seconds in time,
and 2 units of magnitude), and choose those attributes with the highest score Se (ei ).

6

Figure 3: Precision-recall performance of the proposed NET-VISA and deployed SEL3 algorithms,
treating the analyst-generated LEB as ground truth.

Death Move Any event ei with score Se (ei ) < 1 is deleted, and all of its currently associated
detections are marked as false alarms.

Final Pruning Before outputting event hypotheses, we perform a ﬁnal round of pruning to remove
some duplicate events. In particular, we delete any event for which there is another higher-scoring
event within 5 degrees distance and 50 seconds time. Such spurious, or shadow, event hypotheses
arise because real seismic events generate many more phases than we currently model. In addition,
a single phase may sometimes generate multiple detections due to waveform processing, or “pick”,
errors. These additional unmodeled detections, when taken together, often suggest an additional
event at about the same location and time as the original event.
Note that the birth move is not a greedy move: the proposed event will almost always have a score
Se (ei ) < 1 until some number of detections are assigned in subsequent moves. The overall structure
of these moves could be easily converted to an MCMC or simulated annealing algorithm. However,
in our experiments this search outperformed simple MCMC methods in terms of speed and accuracy.

5 Experimental Results

As discussed in Section 2, we measure the precision, recall, and average error of our predictions via
an assumed ground truth. We ﬁrst treat the IMS analyst-generated LEB as ground truth, and com-
pare the performance of our NET-VISA algorithm to the currently deployed SEL3 system. Using
the scores for hypothesized events, we have generated a precision-recall curve for NET-VISA, and
marked SEL3 on it as a point (see Figure 3). Also in this ﬁgure, we show a precision-recall curve
for SEL3 using scores from an SVM trained to classify true and false SEL3 events [10] (SEL3 ex-
trapolation). As shown in the ﬁgure, NET-VISA has at least 16% more recall at the same precision
as SEL3, and at least 25% more precision at the same recall as SEL3.
The true precision of NET-VISA is perhaps higher than this comparison suggests. We have evaluated
the recall of LEB and NET-VISA with the NEIC dataset as ground truth. Since the NEIC has many
more sensors in the United States than the IMS, it is considered a more reliable summary of seismic
activity in this region. Out of 33 events in the continental United States, LEB found 4, and NET-
VISA found 8 including the 4 found by LEB.
Figure 4 shows the recall and error divided among different types of LEB events. The table on
the left shows a break-down by LEB event magnitude. For magnitudes up to 4, NET-VISA has
nearly 20% higher recall with similar error. The table on the right shows a break-down by azimuth

7

0.40.50.60.70.80.91.0precision0.40.50.60.70.80.91.0recallPrecision-RecallcurvewithLEBasgroundtruthSEL3SEL3extrapolationNET-VISAmb
0 – 2
2 – 3
3 – 4
> 4
all

Count
74
36
558
164
832

SEL3
Recall
64.9
50.0
66.5
86.6
69.7

NET-VISA
Err
Err Recall
91
85.1
101
171
75.0
186
109
85.1
104
70
93.3
80
103
86.3
99

Azimuth
Gap
0 – 90
90 – 180
180 – 270
270 – 360
all

Count
72
315
302
143
832

SEL3
Recall
100.0
88.9
51.0
51.0
69.7

NET-VISA
Err
Err Recall
38
100.0
28
72
93.7
76
126
82.1
134
176
72.0
187
103
86.3
99

Figure 4: Recall and error (km) broken down by LEB event magnitude and azimuth gap (degrees).

gap, deﬁned as the largest difference in consecutive event-to-station azimuths for stations which
detect an event. Large gaps indicate that the event location is under-constrained. For example, if all
stations are to the southwest of an event, the gap is greater than 270 degrees and the event will be
poorly localized along a line running from southwest to northeast. By using evidence about missed
detections ignored by SEL3, NET-VISA reduces this uncertainty and performs much better.
All of the results in this section were produced using 7 days of data from the validation set. The
inference used a window size, W , of 30 minutes, a step size, S , of 15 minutes, and N = 1000
iterations. There were a total of 832 LEB events during this period, and roughly 120,000 detections.
The inference took about 4.5 days on a single core running at 2.5 GHz. Estimating model parameters
from 2.5 months of training data took about 1 hour.

6 Conclusions and Further Work

Our results demonstrate that a Bayesian approach to seismic monitoring can improve signiﬁcantly
on the performance of classical systems. The NET-VISA system can not only reduce the human
analyst effort required to achieve a given level of accuracy, but can also lower the magnitude thresh-
old for reliable detection. Given that the difﬁculty of seismic monitoring was cited as one of the
principal reasons for non-ratiﬁcation of the CTBT by the United States Senate in 1999, one hopes
that improvements in monitoring may increase the chances of ﬁnal ratiﬁcation and entry into force.
Putting monitoring onto a sound probabilistic footing also facilitates further improvements such as
continuous estimation of local noise conditions, travel time, and attenuation models without the need
for ground-truth calibration experiments (controlled explosions). We also expect to lower the detec-
tion threshold signiﬁcantly by extending the generative model to include waveform characteristics,
so that detection becomes part of a globally integrated inference process—and hence susceptible to
top-down inﬂuences—rather than being a purely local, bottom-up, hard-threshold decision.

Acknowledgments

We would like to thank the many seismologists who patiently explained to us the intricacies of their
ﬁeld, among them Ronan LeBras, Robert Engdahl, David Bowers, Bob Pearce, Stephen Myers,
Dmitry Storchak, Istvan Bondar, and Barbara Romanowicz. We also received assistance from sev-
eral Berkeley undergraduates, including Matthew Cann, Hong Hu, Christopher Lin, and Andrew
Lee. The third author’s work was performed under the auspices of the U.S. Department of Energy
at Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The other au-
thors were partially supported by the Preparatory Commission for the CTBT. Finally, the ﬁrst author
wishes to thank his family for their inﬁnite patience and support.

References
[1] Y. Bar-Shalom and T.E. Fortmann. Tracking and Data Association. Academic Press, 1988.
[2] P. M. Shearer. Improving local earthquake location using the L1 norm and waveform cross
correlation: Application to the Whittier Narrows, California, aftershock sequence. J. Geophys.
Res., 102:8269 – 8283, 1997.
[3] F. Waldhauser and W. L. Ellsworth. A double-difference earthquake location algorithm:
method and application to the Northern Hayward Fault, California. Bulletin of the Seismo-
logical Society of America, 90:1353 – 1368, 2000.

8

[4] J. Pujol. Earthquake location tutorial: graphical approach and approximate epicentral location
techniques. Seis. Res. Letter, 75:63 – 74, 2004.
[5] Stephen C. Myers, Gardar Johannesson, and William Hanley. A Bayesian hierarchical method
for multiple-event seismic location. Geophysical Journal International, 171:1049–1063, 2009.
[6] L. Geiger. Probability method for the determination of earthquake epicenters from the arrival
time only. Bull. St. Louis Univ., 8:60 –71, 1912.
[7] D. A. Storchak, J. Schweitzer, and P. Bormann. The IASPEI standard seismic phase list.
Seismol. Res. Lett., 74(6):761 – 772, 2003.
[8] Brian Milch, Bhaskara Marthi, Stuart J. Russell, David Sontag, Daniel L. Ong, and Andrey
Kolobov. BLOG: Probabilistic models with unknown objects. In IJCAI, pages 1352–1359,
2005.
[9] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman &
Hall, 2004.
[10] Lester Mackey, Ariel Kleiner, and Michael I. Jordan. Improved automated seismic event ex-
traction using machine learning. Eos Trans. AGU, 90(52), 2009. Fall Meet. Suppl., Abstract
S31B-1714.

9


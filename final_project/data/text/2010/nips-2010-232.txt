Boosting Classi ﬁer Cascades

Mohammad J. Saberian
Statistical Visual Computing Laboratory,
University of California, San Diego
La Jolla, CA 92039
saberian@ucsd.edu

Nuno Vasconcelos
Statistical Visual Computing Laboratory,
University of California, San Diego
La Jolla, CA 92039
nuno@ucsd.edu

Abstract

The problem of optimal and automatic design of a detector cascade is considered.
A novel mathematical model is introduced for a cascaded detector. This model is
analytically tractable, leads to recursive computation, and accounts for both clas-
si ﬁcation and complexity. A boosting algorithm, FCBoost, i s proposed for fully
automated cascade design. It exploits the new cascade model, minimizes a La-
grangian cost that accounts for both classi ﬁcation risk and complexity. It searches
the space of cascade conﬁgurations to automatically determ ine the optimal num-
ber of stages and their predictors, and is compatible with bootstrapping of neg-
ative examples and cost sensitive learning. Experiments show that the resulting
cascades have state-of-the-art performance in various computer vision problems.

1

Introduction

There are many applications where a classi ﬁer must be design ed under computational constraints.
One problem where such constraints are extreme is that of object detection in computer vision.
To accomplish tasks such as face detection, the classi ﬁer mu st process thousands of examples per
image, extracted from all possible image locations and scales, at a rate of several images per second.
This problem has been the focus of substantial attention since the introduction of the detector cascade
architecture by Viola and Jones (VJ) in [13]. This architecture was used to design the ﬁrst real time
face detector with state-of-the-art classi ﬁcation accura cy. The detector has, since, been deployed
in many practical applications of broad interest, e.g. face detection on low-complexity platforms
such as cameras or cell phones. The outstanding performance of the VJ detector is the result of 1) a
cascade of simple to complex classi ﬁers that reject most non -faces with a few machine operations,
2) learning with a combination of boosting and Haar features of extremely low complexity, and 3)
use of bootstrapping to efﬁciently deal with the extremely l arge class of non-face examples.

While the resulting detector is fast and accurate, the process of designing a cascade is not.
In
particular, VJ did not address the problem of how to automatically determine the optimal cascade
conﬁguration, e.g. the numbers of cascade stages and weak le arners per stage, or even how to design
individual stages so as to guarantee optimality of the cascade as a whole. In result, extensive manual
supervision is required to design cascades with good speed/accuracy trade off. This includes trial-
and-error tuning of the false positive/detection rate of each stage, and of the cascade conﬁguration.
In practice, the design of a good cascade can take up several weeks. This has motivated a number
of enhancements to the VJ training procedure, which can be organized into three main areas: 1)
enhancement of the boosting algorithms used in cascade design, e.g. cost-sensitive variations of
boosting [12, 4, 8], ﬂoat Boost [5] or KLBoost [6], 2) post pro cessing of a learned cascade, by ad-
justing stage thresholds, to improve performance [7], and 3) specialized cascade architectures which
simplify the learning process, e.g. the embedded cascade (ChainBoost) of [15], where each stage
contains all weak learners of previous stages. These enhancements do not address the fundamental
limitations of the VJ design, namely how to guarantee overall cascade optimality.

1

L
R
 

0.8

0.6

0.4

0.2

0

AdaBoost
ChainBoost

10

20

Iterations

30

40

50

L

0.8

0.7

0.6

0.5

0

AdaBoost
ChainBoost

10

20

Iterations

30

40

50

Figure 1: Plots of RL (left) and L (right) for detectors designed with AdaBoost and ChainBoost.

More recently, various works have attempted to address this problem [9, 8, 1, 14, 10]. However, the
proposed algorithms still rely on sequential learning of cascade stages, which is suboptimal, some-
times require manual supervision, do not search over cascade conﬁgurations, and frequently lack
a precise mathematical model for the cascade. In this work, we address these problems, through
two main contributions. The ﬁrst is a mathematical model for
a detector cascade, which is an-
alytically tractable, accounts for both classi ﬁcation and complexity, and is amenable to recursive
computation. The second is a boosting algorithm, FCBoost, that exploits this model to solve the
cascade learning problem. FCBoost solves a Lagrangian optimization problem, where the classi ﬁ-
cation risk is minimized under complexity constraints. The risk is that of the entire cascade, which
is learned holistically, rather than through sequential stage design, and FCBoost determines the opti-
mal cascade conﬁguration automatically. It is also compati ble with bootstrapping and cost sensitive
boosting extensions, enabling efﬁcient sampling of negati ve examples and explicit control of the
false positive/detection rate trade off. An extensive experimental evaluation, covering the problems
of face, car, and pedestrian detection demonstrates its superiority over previous approaches.

2 Problem De ﬁnition

L[yi , f (xi )]

RL (f ) = EX,Y {L[y , f (x)]} ≃

A binary classi ﬁer h(x) maps an example x into a class label y ∈ {−1, 1} according to h(x) =
sign[f (x)], where f (x) is a continuous-valued predictor. Optimal classi ﬁers mini mize a risk
1
|St | Xi
where St = {(x1 , y1 ), . . . , (xn , yn )} is a set of training examples, yi ∈ {1, −1} the class label of
example xi , and L[y , f (x)] a loss function. Commonly used losses are upper bounds on the zero-one
loss, whose risk is the probability of classi ﬁcation error. Hence, RL is a measure of classi ﬁcation
accuracy. For applications with computational constraints, optimal classi ﬁer design must also take
into consideration the classi ﬁcation complexity. This is a chieved by deﬁning a computational risk
1
|St | Xi
where C (f (x)) is the complexity of evaluating f (x), and LC [y , C (f (x))] a loss function that encodes
the cost of this operation. In most detection problems, targets are rare events and contribute little to
the overall complexity. In this case, which we assume throughout this work, LC [1, C (f (x))] = 0
and LC [−1, C (f (x))] is denoted LC [C (f (x))]. The computational risk is thus
1
t | Xxi∈S−
|S−
t
whereS−
t contains the negative examples of St . Usually, more accurate classi ﬁers are more complex.
For example in boosting, where the decision rule is a combination of weak rules, a ﬁner approxima-
tion of the classi ﬁcation boundary (smaller error) require s more weak learners and computation.

RC (f ) = EX,Y {LC [y , C (f (x))]} ≃

LC [yi , C (f (xi ))]

LC [C (f (xi ))].

RC (f ) ≈

(1)

(2)

(3)

Optimal classi ﬁer design under complexity constraints is a problem of constrained optimization,
which can be solved with Lagrangian methods. These minimize a Lagrangian
1
η
t | Xxi∈S−
|St | Xxi∈St
|S−
t

LC [C (f (xi ))],

L[yi , f (xi )] +

L(f ; St ) =

(4)

2

where η is a Lagrange multiplier, which controls the trade-off between error rate and complexity.
Figure 1 illustrates this trade-off, by plotting the evolution of RL and L as a function of the boosting
iteration, for the AdaBoost algorithm [2]. While the risk always decreases with the addition of weak
learners, this is not true for the Lagrangian. After a small number of iterations, the gain in accuracy
does not justify the increase in classi ﬁer complexity. The d esign of classi ﬁers under complexity
constraints has been addressed through the introduction of detector cascades. A detector cascade
H(x) implements a sequence of binary decisions hi (x), i = 1 . . . m. An example x is declared a
target (y = 1) if and only if it is declared a target by all stages of H, i.e. hi (x) = 1, ∀i. Otherwise,
the example is rejected. For applications where the majority of examples can be rejected after a
small number of cascade stages, the average classi ﬁcation t
ime is very small. However, the problem
of designing an optimal detector cascade is still poorly understood. A popular approach, known as
ChainBoost or embedded cascade [15], is to 1) use standard boosting algorithms to design a detector,
and 2) insert a rejection point after each weak learner. This is simple to implement, and creates
a cascade with as many stages as weak learners. However, the introduction of the intermediate
rejection points, a posteriori of detector design, sacri ﬁces the risk-optimality of the de tector. This
is illustrated in Figure 1, where the evolution of RL and L are also plotted for ChainBoost. In this
example, L is monotonically decreasing, i.e. the addition of weak learners no longer carries a large
complexity penalty. This is due to the fact that most negative examples are rejected in the earliest
cascade stages. On the other hand, the classi ﬁcation risk is more than double that of the original
boosted detector. It is not known how close ChainBoost is to optimal, in the sense of (4).

3 Classiﬁer cascades

In this work, we seek the design of cascades that are provably optimal under (4). We start by
introducing a mathematical model for a detector cascade.

3.1 Cascade predictor

Let H(x) = {h1 (x), . . . , hm (x)} be a cascade of m detectors hi (x) = sgn[fi (x)]. To develop
some intuition, we start with a two-stage cascade, m = 2. The cascade implements the decision rule
(5)

H(F )(x) = sgn[F (x)]

where

F (x) = F (f1 , f2 )(x) = (cid:26) f1 (x)
f2 (x)
= f1u(−f1 ) + u(f1 )f2
is denoted the cascade predictor, u(.) is the step function and we omit the dependence on x for
notational simplicity. This equation can be extended to a cascade of m stages, by replacing the
predictor of the second stage, when m = 2, with the predictor of the remaining cascade, when m is
larger. Letting Fj = F (fj , . . . , fm ) be the cascade predictor for the cascade composed of stages j
to m

if f1 (x) < 0
if f1 (x) ≥ 0

(6)

(7)

F = F1 = f1u(−f1 ) + u(f1 )F2 .
More generally, the following recursion holds

(8)

(9)
Fk = fk u(−fk ) + u(fk )Fk+1
with initial condition Fm = fm . In Appendix A, it is shown that combining (8) and (9) recursively
leads to

F = T1,m + T2,m fm
= T1,k + T2,k fk u(−fk ) + T2,kFk+1u(fk ), k < m.
with initial conditions T1,0 = 0, T2,0 = 1 and
(12)
T1,k+1 = T1,k + fk u(−fk ) T2,k ,
T2,k+1 = T2,k u(fk ).
Since T1,k , T2,k , and Fk+1 do not depend on fk , (10) and (11) make explicit the dependence of the
cascade predictor, F , on the predictor of the k th stage.

(10)
(11)

3

3.2 Differentiable approximation

where

< δF (fk ), g >=

Letting F (fk + ǫg) = F (f1 , .., fk + ǫg , ..fm ), the design of boosting algorithms requires the eval-
uation of both F (fk + ǫg), and the functional derivative of F with respect to each fk , along any
direction g
F (fk + ǫg)(cid:12)(cid:12)(cid:12)(cid:12)ǫ=0
d
dǫ
These are straightforward for the last stage since, from (10),
F (fm + ǫg) = am + ǫbm g , < δF (fm ), g >= bm g ,
bm = T2,m .
am = T1,m + T2,m fm = F (fm ),
(14)
In general, however, the right-hand side of (11) is non-differentiable, due to the u(.) functions. A
differentiable approximation is possible by adopting the classic sigmoidal approximation u(x) ≈
tanh(σx)+1
, where σ is a relaxation parameter. Using this approximation in (11),
2
F = F (fk ) = T1,k + T2,k fk (1 − u(fk )) + T2,kFk+1u(fk )
1
2

T2,k [Fk+1 − fk ][tanh(σfk ) + 1].

≈ T1,k + T2,k fk +

bk =

It follows that
< δF (fk ), g > = bk g
1
T2,k (cid:8)[1 − tanh(σfk )] + σ [Fk+1 − fk ][1 − tanh2 (σfk )](cid:9) .
2
es expansion around fk
F (fk + ǫg) can also be simpli ﬁed by resorting to a ﬁrst order Taylor seri
F (fk + ǫg) ≈ ak + ǫbk g
(19)
ak = F (fk ) = T1,k + T2,k (cid:26)fk +
3.3 Cascade complexity

[Fk+1 − fk ][tanh(σfk ) + 1](cid:27) .

(17)

(18)

(20)

(13)

(15)

(16)

.

1
2

In Appendix B, a similar analysis is performed for the computational complexity. Denoting by C (fk )
the complexity of evaluating fk , it is shown that
C (F ) = P1,k + P2,k C (fk ) + P2,k u(fk )C (Fk+1 ).
with initial conditions C (Fm+1 ) = 0, P1,1 = 0, P2,1 = 1 and
(22)
P1,k+1 = P1,k + C (fk ) P2,k
P2,k+1 = P2,k u(fk ).
This makes explicit the dependence of the cascade complexity on the complexity of the k th stage.
In practice, fk = Pl cl gl for gl ∈ U , where U is a set of functions of approximately identical
complexity. For example, the set of projections into Haar features, in which C (fk ) is proportional to
the number of features gl . In general, fk has three components. The ﬁrst is a predictor that is also
used in a previous cascade stage, e.g. fk (x) = fk−1 (x) + cg(x) for an embedded cascade. In this
case, fk−1 (x) has already been evaluated in stage k − 1 and is available with no computational cost.
The second is the set O(fk ) of features that have been used in some stage j ≤ k . These features are
also available and require minimal computation (multiplication by the weight cl and addition to the
running sum). The third is the set N (fk ) of features that have not been used in any stage j ≤ k . The
overall computation is

(21)

(23)
C (fk ) = |N (fk )| + λ|O(fk )|,
where λ < 1 is the ratio of computation required to evaluate a used vs. new feature. For Haar
wavelets, λ ≈ 1
20 . It follows that updating the predictor of the k th stage increases its complexity to
if g ∈ O(fk )
C (fk + ǫg) = (cid:26) C (fk ) + λ
(24)
if g ∈ N (fk ),
C (fk ) + 1
and the complexity of the cascade to
C (F (fk + ǫg)) = P1,k + P2,k C (fk + ǫg) + P2,k u(fk + ǫg)C (Fk+1 )
= αk + γ k C (fk + ǫg) + β k u(fk + ǫg)

(25)
(26)

with

αk = P1,k

γ k = P2,k β k = P2,k C (Fk+1 ).

(27)

4

3.4 Neutral predictors

The models of (10), (11) and (21) will be used for the design of optimal cascades. Another observa-
tion that we will exploit is that

H[F (f1 , . . . , fm , fm )] = H[F (f1 , . . . , fm )].
This implies that repeating the last stage of a cascade does not change its decision rule. For this
reason n(x) = fm (x) is referred to as the neutral predictor of a cascade of m stages.

4 Boosting classiﬁer cascades

In this section, we introduce a boosting algorithm for cascade design.

4.1 Boosting

Boosting algorithms combine weak learners to produce a complex decision boundary. Boost-
ing iterations are gradient descent steps towards the predictor f (x) of minimum risk for the loss
L[y , f (x)] = e−yf (x) [3]. Given a set U of weak learners, the functional derivative of RL along the
direction of weak leaner g is
e−yi (f (xi )+ǫg(xi ))(cid:21)ǫ=0
|St | Xi (cid:20) d
1
dǫ
where wi = e−yi f (xi ) is the weight of xi . Hence, the best update is
g∗ (x) = arg max
g∈U

1
|St | Xi

< δRL (f ), g > =

< −δRL (f ), g > .

yiwi g(xi ),

= −

(28)

(29)

Letting I (x) be the indicator function, the optimal step size along the selected direction, g∗ (x), is
log Pi wi I (yi = g∗ (xi ))
c∈R Xi
c∗ = arg min
Pi wi I (yi 6= g∗ (xi ))
The predictor is updated into f (x) = f (x) + c∗ g∗ (x) and the procedure iterated.

e−yi (f (xi )+cg∗ (xi )) =

1
2

.

(30)

4.2 Cascade risk minimization

To derive a boosting algorithm for
the design of detector cascades, we adopt
L[y , F (f1 , . . . , fm )(x)] = e−yF (f1 ,...,fm )(x) , and minimize the cascade risk
1
|St | Xi

RL (F ) = EX,Y {e−yF (f1 ,...,fm ) } ≈

e−yiF (f1 ,...,fm )(xi ) .

Using (13) and (19),

the loss

< δRL (F (fk )), g >=

e−yi [ak (xi )+ǫbk (xi )g(xi )](cid:21)ǫ=0
|St | Xi (cid:20) d
1
1
|St | Xi
dǫ
i = e−yi ak (xi ) , bk
where wk
i = bk (xi ) and ak , bk are given by (14), (18), and (20). The optimal
descent direction and step size for the k th stage are then

i bk
yiwk
i g(xi ) (31)

= −

g∗
< −δRL (F (fk )), g >
k = arg max
g∈U
c∈R Xi
c∗
k = arg min
In general, because the bk
i are not constant, there is no closed form for c∗
k , and a line search must
be used. Note that, since ak (xi ) = F (fk )(xi ), the weighting mechanism is identical to that of
boosting, i.e. points are reweighed according to how well they are classi ﬁed by the current cascade.
Given the optimal c∗ , g∗ for all stages, the impact of each update in the overall cascade risk, RL , is
evaluated and the stage of largest impact is updated.

i e−yi bk
i cg∗
k (xi ) .
wk

(33)

(32)

5

4.3 Adding a new stage

Searching for the optimal cascade conﬁguration requires su pport for the addition of new stages,
whenever necessary. This is accomplished by including a neutral predictor as the last stage of the
cascade. If adding a weak learner to the neutral stage reduces the risk further than the corresponding
addition to any other stage, a new stage (containing the neutral predictor plus the weak learner) is
created. Since this new stage includes the last stage of the previous cascade, the process mimics the
design of an embedded cascade. However, there are no restrictions that a new stage should be added
at each boosting iteration, or consist of a single weak learner.

4.4

Incorporating complexity constraints

< δRC (F (fk )), g >=

Joint optimization of speed and accuracy, requires the minimization of the Lagrangian of (4). This
requires the computation of the functional derivatives
i (cid:26) d
LC [C (F (fk + ǫg)(xi )](cid:27)ǫ=0
1
t | Xi
ys
|S−
dǫ
i = I (yi = −1). Similarly to boosting, which upper bounds the zero-one loss u(−yf ) by
where ys
the exponential loss e−yf , we rely on a loss that upper-bounds the true complexity. This upper-bound
is a combination of a boosting-style bound u(f + ǫg) ≤ ef +ǫg , and the bound C (f + ǫg) ≤ C (f ) + 1,
which follows from (24). Using (26),

(34)

LC [C (F (fk + ǫg)(xi )] = LC [αk + γ k C (fk + ǫg) + β k u(fk + ǫg)]
= αk + γ k (C (fk ) + 1) + β k efk+ǫg
dǫ LC [C (F (fk + ǫg))](cid:9)ǫ=0 = β k efk g ,
and, since (cid:8) d
< δRC (F (fk )), g > =

1
t | Xi
|S−
with β k
i = β k (xi ) and ψk
i = efk (xi ) . The derivative of (4) with respect to the k th stage predictor is
then

i β k
i ψk
ys
i g(xi )

(35)
(36)

(37)

(38)

(39)

+ η

< δL(F (fk )), g > = < δRL (F (fk )), g > +η < δRC (F (fk )), g >
yiwk
i bk
ys
i β k
i ψk
t | (cid:19) g(xi )
= Xi (cid:18)−
i
i
|S−
|St |
i = e−yi ak (xi ) and ak and bk given by (14), (18), and (20). Given a set of weak learners U ,
with wk
the optimal descent direction and step size for the k th stage are then
g∗
k = arg max
g∈U
k (xi )) .
c∈R ( 1
η
t | Xi
|St | Xi
i ecg∗
i β k
i ψk
ys
c∗
k = arg min
|S−
A pair (g∗
k,1 ) is found among the set O(fk ) and another among the set U − O(fk ) . The one
k,1 , c∗
that most reduces (4) is selected as the best update for the k th stage and the stage with the largest
impact is updated. This gradient descent procedure is denoted Fast Cascade Boosting (FCBoost).

i e−yi bk
i cg∗
k (xi ) +
wk

< −δL(F (fk )), g >

(41)

(40)

5 Extensions

FCBoost supports a number of extensions that we brieﬂy discu ss in this section.

5.1 Cost Sensitive Boosting

As is the case for AdaBoost, it is possible to use cost sensitive risks in FCBoost. For exam-
ple, the risk of CS-AdaBoost: RL (f ) = EX,Y {y c e−yf (x)} [12] or Asym-AdaBoost: RL (f ) =
EX,Y {e−yc yf (x) } [8], where y c = C I (y = −1) + (1 − C )I (y = 1) and C is a cost factor.

6

Data Set
Face
Car
Pedestrian

Train Set
neg
pos
9,000
9,000
10,000
1,000
1,000
10,000

Test Set
neg
pos
832
832
2,000
100
200
2,000

L
R

0.36

0.32

0.28

0.24

0

10

RC

20

30

Figure 2: Left: data set characteristics. Right: Trade-off between the error (RL ) and complexity (RC ) com-
ponents of the risk as η changes in (4).

Table 1: Performance of various classiﬁers on the face, car, and pedestrian test sets.

Method
AdaBoost
ChainBoost
FCBoost (η = 0.02)

RL
0.20
0.45
0.30

Face
RC
50
2.65
4.93

L
1.20
0.50
0.40

RL
0.22
0.65
0.44

Car
RC
50
2.40
5.38

Pedestrian
RC
50
3.34
4.23

L
1.35
0.59
0.54

RL
0.35
.052
0.46

L
1.22
0.70
0.55

5.2 Bootstrapping

Bootstrapping is a procedure to augment the training set, by using false positives of the current
classi ﬁer as the training set for the following [11]. This im proves performance, but is feasible only
when the bootstrapping procedure does not affect previously rejected examples. Otherwise, the
classi ﬁer will forget the previous negatives while learnin g from the new ones. Since FCBoost learns
all cascade stages simultaneously, and any stage can change after bootstrapping, this condition is
violated. To overcome the problem, rather than replacing all negative examples with false positives,
only a random subset is replaced. The negatives that remain in the training set prevent the classi ﬁer
from forgetting about the previous iterations. This method is used to update the training set whenever
the false positive rate of the cascade being learned reaches 50%.

6 Evaluation

Several experiments were performed to evaluate the performance of FCBoost, using face, car, and
pedestrian recognition data sets, from computer vision. In all cases, Haar wavelet features were used
as weak learners. Figure 2 summarizes the data sets.
Effect of η : We started by measuring the impact of η , see (4), on the accuracy and complexity of
FCBoost cascades. Figure 2 plots the accuracy component of the risk, RL , as a function of the
complexity component, RC , on the face data set, for cascades trained with different η . The leftmost
point corresponds to η = 0.05, and the rightmost to η = 0. As expected, as η decreases the cascade
has lower error and higher complexity. In the remaining experiments we used η = 0.02.
Cascade comparison: Figure 3 (a) repeats the plots of the Lagrangian of the risk shown in Fig-
ure 1, for classi ﬁers trained with 50 boosting iterations, on the face data. In addition to AdaBoost
and ChainBoost, it presents the curves of FCBoost with (η = 0.02) and without (η = 0) com-
plexity constraints. Note that, in the latter case, performance is in between those of AdaBoost and
ChainBoost. This reﬂects the fact that FCBoost ( η = 0) does produce a cascade, but this cascade
has worse accuracy/complexity trade-off than that of ChainBoost. On the other hand, the inclusion
of complexity constraints, FCBoost (η = 0.02), produces a cascade with the best trade-off. These
results are conﬁrmed by Table 1, which compares classi ﬁers t
rained on all data sets. In all cases, Ad-
aBoost detectors have the lowest error, but at a tremendous computational cost. On the other hand,
ChainBoost cascades are always the fastest, at the cost of the highest classi ﬁcation error. Finally,
FCBoost (η = 0.02) achieves the best accuracy/complexity trade-off: its cascade has the lowest risk
Lagrangian L. It is close to ten times faster than the AdaBoost detector, and has half of the increase
in classi ﬁcation error (with respect to AdaBoost) of the Cha inBoost cascade. Based on these results,
FCBoost (η = 0.02) was used in the last experiment.

7

0.8

0.7

L

0.6

0.5

0.4

0

FCBoost h=0
FCBoost h=0.02
AdaBoost
ChainBoost

94

90

85

e
t
a
R
 
n
o
i
t
c
e
t
e
D

Viola & Jones
ChainBoost
FloatBoost
WaldBoost
FCBoost

10

20

Iterations
(a)

30

40

50

80

0

25

50
100
75
Number of False Positives
(b)

125

150

Figure 3: a) Lagrangian of the risk for classiﬁers trained with various boosting algo rithms. b) ROC of various
detector cascades on the MIT-CMU data set.

Table 2: Comparison of the speed of different detectors.

Method VJ [13]
Evals
8

FloatBoost [5] ChainBoost [15] WaldBoost [9]
18.9
18.1
10.84

[8]
15.45

FCBoost
7.2

Face detection: We ﬁnish with a face detector designed with FCBoost ( η = 0.02), bootstrapping,
and 130K Haar features. To make the detector cost-sensitive, we used CS-AdaBoost with C = 0.99.
Figure 3 b) compares the resulting ROC to those of VJ [13], ChainBoost [15], FloatBoost [5] and
WaldBoost [9]. Table 2 presents a similar comparison for the detector speed (average number of
features evaluated per patch). Note the superior performance of the FCBoost cascade in terms of
both accuracy and speed. To the best of our knowledge, this is the fastest face detector reported to
date.

A Recursive form of cascade predictor

Applying (9) recursively to (8)
F = f1u(−f1 ) + u(f1 )F2
= f1u(−f1 ) + u(f1 ) [f2u(−f2 ) + u(f2 )F3 ]
= f1u(−f1 ) + f2u(f1 )u(−f2 ) + u(f1 )u(f2 ) [f3u(−f3 ) + u(f3 )F4 ]
k−1
Xi=1
fiu(−fi ) Yj<i
u(fj ) + Fk Yj<k
(46)
= T1,k + T2,kFk
where T1,k = Pk−1
i=1 fiu(−fi ) Qj<i u(fj ) and T2,k = Qj<k u(fj ) satisfy the recursions of (12).
Combining (46) and (9) then leads to (11). (10) follows from (46) and the initial condition Fm = fm .
B Recursive form of cascade complexity

(42)
(43)
(44)

u(fj )

(45)

=

Let C (fk ) be the complexity of evaluating fk . Then
C (F ) = C (f1 ) + u(f1 )C (F2 )
= C (f1 ) + u(f1 )[C (f2 ) + u(f2 )C (F3 )]
k−1
Xi=1
C (fi ) Yj<i
u(fj ) + C (Fk ) Yj<k
= P1,k + P2,k C (Fk )

=

u(fj )

with

P1,k+1 = P1,k + C (fk ) P2,k
P2,k+1 = P2,k u(fk )
and initial conditions P1,1 = 0, P2,1 = 1. The relationship of (47) is a special case of
(52)
C (Fk ) = C (fk ) + u(fk )C (Fk+1 )
with initial conditions C (Fm ) = C (fm ) and C (Fm+1 ) = 0. Combining (52) with (50) leads to (21).

(47)
(48)

(49)

(50)

(51)

8

References

[1] S. C. Brubaker, M. D. Mullin, and J. M. Rehg. On the design of cascades of boosted ensembles
for face detection. International Journal of Computer Vision, 77:65–86, 2008.
[2] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting, 1997.
[3] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of
boosting. Annals of Statistics, 28:2000, 1998.
[4] X. Hou, C.-L. Liu, and T. Tan. Learning boosted asymmetric classi ﬁers for object detection.
In IEEE Conference on Computer Vision and Pattern Recognition,, pages 330–338, 2006.
[5] S. Z. Li and Z. Zhang. Floatboost learning and statistical face detection.
IEEE Trans. on
Pattern Analysis and Machine Intelligence, 26(9):1112–1123, 2004.
[6] C. Liu and H.-Y. Shum;. Kullback-leibler boosting. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 587–594, 2003.
[7] H. Luo. Optimization design of cascaded classi ﬁers. In IEEE Conference on Computer Vision
and Pattern Recognition,, pages 480–485, 2005.
[8] H. Masnadi-Shirazi and N. Vasconcelos. High detection-rate cascades for real-time object
detection. In IEEE International Conference on Computer Vision, volume 2, pages 1–6, 2007.
[9] J. Sochman and J. Matas. Waldboost - learning for time constrained sequential detection. In
IEEE Conference on Computer Vision and Pattern Recognition, pages 150–157, 2005.
[10] J. Sun, J. M. Rehg, and A. Bobick. Automatic cascade training with perturbation bias. IEEE
Conference on Computer Vision and Pattern Recognition, 2:276–283, 2004.
[11] K. K. Sung and T. Poggio. Example based learning for view-based human face detection. IEEE
Trans. on Pattern Analysis and Machine Intelligence, 20:39–51, 1998.
[12] P. Viola and M. Jones. Fast and robust classi ﬁcation usi ng asymmetric adaboost and a detector
cascade. In Advances in Neural Information Processing System, pages 1311–1318, 2001.
[13] P. Viola and M. Jones. Robust real-time object detection. International Journal of Computer
Vision, 57(2):137–154, 2004.
[14] J. Wu, S. Brubaker, M. D. Mullin, and J. M. Rehg. Fast asymmetric learning for cascade face
detection. IEEE Trans. on Pattern Analysis and Machine Intelligence, 3:369–382, 2008.
[15] R. Xiao, L. Zhu, and H.-J. Zhang. Boosting chain learning for object detection.
In IEEE
International Conference on Computer Vision, pages 709–715, 2003.

9


 

 
 
 
 
 
 
 
 
 
 
 
 
 
 

S p h e r e   Em b e d d i n g :    
A n  A p p l i c a t i o n   t o   P a r t - o f - S p e e c h   I n d u c t i o n  

Yariv  Maron  
Gonda Brain Research Cen ter  
Bar-Ilan University  
Ramat -Gan 52900 ,  Israel  
syarivm@yahoo .com  

M ichael Lamar  
Department o f Mathematics and  Compu ter  Science  
Sain t Louis Univers ity  
St.  Louis,  MO 63103 ,  USA    
mlamar@slu .edu  

 
 
 
 
 
 
 

Elie B ienenstock  
Division of  App lied  Mathematics  
And  Department of Neuro sc ience  
Brown Univer sity  
Providence,  RI  02912 ,  USA  
elie@b rown .edu  
 

 

 
 
 
 

Abstract  

Mo tivated   by  an  app licat ion  to   unsupervised   part -of-speech  tagg ing,   we  
present  an  a lgorithm  for   the  Eucl idean  embedd ing  of   large  sets  o f 
categorical  data   based   on  co -occurrence  s tatistics .  We  use  the CODE  model 
of  Globerson  et  al.   but  constrain  the  embedd ing  to   lie  on  a  hig h-
d imensional  un it  sphere .   This  cons traint  allows  for   e fficient   op timiza tion,  
even  in  the  case   o f  large  datasets  and   high  embedd ing  d imens ional ity .  
Using  k -means  clustering  o f  the  embedded   data,   our   approach  e fficiently 
produces  state -o f-the -art  resu lts.   We  ana lyze  the  reasons  why  the  sphere  
constraint   is beneficial   in  this  app licat ion ,   and   conjecture   tha t  these  reasons  
might app ly quite generally to  o ther  large -scale tasks.    

 

1  

I n t r o d u c t i o n  

The  embedd ing  of  ob jects  in   a  low -d imensional  Eucl idean  space  is  a  form  o f  d imensionality  
reduction  tha t  has  been  used   in  the  pas t  mo stly  to   create  2D  representations  o f  data   for   the 
purpose  of  visualizat ion  and   exp loratory  data  analys is   [10 ,   13] .   Most  methods  work  on  
ob jects  of  a   single  type ,   endowed   wi th  a  measure  of  sim ilarity.   O ther   methods,   such   as  [ 3] ,  
embed   ob jects  of  he terogeneous  types,   based   on  their   co -occurrence  s tatistics.   In  th is  paper  
we  demonstrate   that  the   lat ter   can  be  successfully  app li ed   to   unsupervised   part -of-speech  
(POS)  
language 
in  natural 
induc tion,   an  extens ive ly  stud ied ,   cha lleng ing,   prob lem 
processing  [1 ,  4 ,  5 ,  6 ,  7] . 

The  prob lem  we   address  is   d istribu tiona l   POS  tagg ing ,   in  which  words  are  to   be  tagged  
based   on  the   s tatistics  of  their   immed ia te  le ft  and   right  con tex t  in  a  corpus  (ignoring    
morpho logy  and   o ther   fea tures ) .   The  induct ion  task  is   fully  unsupervised ,   i.e. ,   it   uses  no  
anno tations.   This  task  has  bee n  addressed   in  the  past   using  a  variety  of  methods.   Some  
approaches,   such  as  [ 1] ,   comb ine  a  Markovian  assump tion  with  clustering.   Many  recent  
works  use   HMMs,   perhaps   due  to   the ir   excellent   performance  on  the  supervised   ve rsion  o f 
the task [7 ,  2 ,  5] .  Using a  latent -descrip tor  clustering  approach,  [15]  ob tain t he best resu lts  to  
date  for  d istributional -on ly unsuperv ised  POS tagging of  the wide ly -used  WSJ  corpus .  

Using a heterogeneous -data  embedd ing approach   for  this  task,  we  de fine separate embedd ing  

func tions  for   the  ob jects " left word"  and  " right word"  based  on  their  co -occurrence statist ics,  
i.e . ,   based   on  b igram  frequencies.   We  are  in terested   in  modeling   the  stat ist ical  in te ractions  
between  le ft  words  and   right  words,   as  relevant  to   POS  tagging,   rat her   than  their   jo int 
d istribu tion.   Indeed ,   mode ling  the  jo int  d istribution  d irectly  results  in  models   that   do   no t 
hand le  rare   words  we ll.  We   use  the  CODE   (Co-Occurrence  Data  Embedd ing)   model  of  [3] , 
where   stat ist ical  in teraction  is modeled   as  the  negat ive   exponentia l o f  the Eucl idean  d istance 
between 
the  marginal  
incorporates 
the  embedded   po ints.   This  embedd ing  model 
probab ilities,   or   unigram  frequenc ies,   in  a  way  tha t  results  in  appropriate   hand ling   of  bo th 
frequent and  rare  word s.    

The  size   o f  the   datase t   (number   of   po in ts  to   embed)   and   th e   embedd ing   d imensionali t y  are  
severa l-fo ld   larger   than  in  the   app lica tions  stud ied   in  [3] ,   mak ing  the   op timizat ion   methods  
used   by  these   authors  impractical .   Instead ,   we  use   a  s imp le  and   intui tive  s tochastic -grad ien t 
procedure .   Importantly,   in  order   to   hand le  bo th  the  la rge  dataset  and   the  relative ly  high  
d imensional ity  of   the   embedd ing   needed   for   this   app licat ion ,   we   constrain  the   embed d ing   to  
lie  on  the   unit  sphere .   We  there fore  refer   to   this  me thod   as  Spherica l  CODE ,   or   S-CODE.  
The  spherical  constraint  causes  the  regularization  term—the  parti tion  function—to   be  nearly 
constant   and   also   makes  the   stochastic  grad ient  ascent  smoo ther ;  this  a llows  a  seve ral -fo ld  
computational  improvement ,   and   yield s  exce llent   performance.   After   convergence  of  the  
embedd ing  model,   we  use   a   k-means  a lgorithm  to   cluste r   all  the  word s  of  the  corpus ,   based  
on  the ir   embedd ings.   The   induced   POS  labels  are  eva luated   using  the  standard   set ting  for  
this task,  yield ing s tate -of-the-art tagging performance.  

 
M e t h o d s  
2  
2 . 1   M o d e l  

We  represent  a   b igram,   i.e. ,   an  ordered   pair  of  ad jacent words  in  the  corpus,   as  jo int  random 
variab les  (X ,Y) ,   each  tak ing  values  in  W,   the  set  o f  word   types  occurring  in  the   corpus.    
Since   X   and   Y,   the  first  and   second   words  in  a  b igram,   p lay  d i fferent  ro le s,   we   build   a 
heterogeneous  model,   i.e. ,  use   two  embedd ing  functions,       and      .  Bo th map  W  in to  S , 
the un it sphere  in  the r-d imens ional Eucl idean space.  

We use      for   the word -type   frequenc ies :        is  the number  o f  word   tokens of  type  x  d iv ided  
by  the   to ta l  number   o f  tokens   in  the  corpus.     We  re fer   to       as   the  emp irical   ma rginal  
d istribu tion ,   or   un igram  frequency.   We  use             for   the  emp irical  jo int  d is tribut ion  o f  X 
and   Y,   i.e. ,   the  d istribution  of  b igrams   (X ,Y) .     Because  our   ult imate  goal  is  the  clus tering  of  
word   types  for   POS  tagg ing ,   we   wan t  the  embedd ing  to   be  insensitive  to   the  ma rg inals:  two  
word   types  with  similar   con text  d is tribut ions  should   be  mapped   to   neighboring  po in ts  in  S  
even  i f  their   unigram  frequencies  are  very d i fferent.  We  there fore use  the ma rg ina l -marg ina l 
model of  [3] ,  defined  by:  

 

 

 

 

 

            

 
                 

 

 

 

 

 
 
                       
                   
     
   

 

 

 
                           
    

 

(1) 

(2) 

(3) 

The  log- likelihood ,  ,   of  the  corpus  of  b igrams  is  the  expected   va lue ,   under   the  emp irical  
b igram d istribution ,  of the log of the model b igram probab ility :  

 
                            
      

 
                     
      

                                    
    

 

 

(4) 
 

The model  is parame terized  by 2 × |W | po in ts on  the  un it  sphere  S   in  r  d imensions :             
and             .   These   po ints  are  initia lized   randomly,   i.e. ,   independen tly  and   uni formly  on  
S .  
     
To  maximize   the   likelihood ,   we   use  a  grad ient -ascent  approach.   The  grad ient  of  the  log  
like lihood   is  as  fo llows   (o bserve   tha t  the  last   term  in  (4)   does  no t  depend   on  the   model,  
hence does no t contribute to   the  grad ien t ) :  
 

 

 

  

     
  

     

                           
 

 

                             
 

 

 
 

 

 
                                   
 
 
                                   
 

   

   

(5) 

(6) 

 
For   sufficiently  la rge  prob lems   such  as  POS  tagg ing   of  a  large  corpus,   compu ting   the  
partition  func tion,   Z ,   after   each  grad ient  step   or   even  once  every  fixed   number   of  s teps  can  
be   impract ical.     Instead ,   it  turns out  (see Discussion)   that ,   thanks  to   the sphere constraint,   we 
can  approximate  this  dynamic   variab le,   Z,   using  a  cons tan t,     ,   which  arises  from  a  coarse  
approximat ion  in  wh ich  al l  pairs  of  embedd ed   variab les  are  d istributed   un iformly  and  
independently  on  the   sphere .   Thus,   we  set                            with       and        i.i.d. 
uni formly  on  S ,   and   get  our   es timate      as  the   expec ted   value  o f  the  resu lting  random 
variab le ,              
:      
 

                    

 .    

(7) 

 
 
Numerical  evaluation of  (7)  yield s            for  the 25 -d imens ional sphere .  An even coarser  
approximat ion  can be ob tained  by  no ting   that ,   for   large  r ,   the  random  variab le               
               is  fairly  peaked   around   2   ( the  random  variab le            is  close  to   a  Student 's  t  
wi th  r  degrees  of  freedom ,   compressed   by  a  factor   of    ) .   This  yield s  the  est imate        
             .  
 
For   the  present  app lication ,  we  find   that performance  does no t suffer   from   using a cons tan t    
rather   than  recomputing   Z  often  during  grad ien t -ascent.   It  is  also   fairly  robust  to   the   cho ice  
of   .  We observe only minor  change s in performance  for     rang ing over   [0 .1 ,  0 .5] .   
 
We  use  samp l ing  to   compute  a  stochas tic  approxima tion  o f  the  grad ien t.     To   imp lement   the 
firs t  sum   in  (5)   and   (6 )   −  represent ing  an  attrac tion  force  between   the  embedd ings  of  the  
words  in  a   b igram  −   we   samp le   b igrams  from  the  emp irical  jo int            .     Given  a  samp le 
          ,   only  the           and           parameter   vectors  are  updated .     The  partial  updat es  that 
emerge   from  these two  sums  are:  
 
 
 

(8) 
(9) 
 
where       is  the   step  s ize .   In order   to   speed  up   the conve rgence process,  we  use a   learning  rate  
that  decreases  as  word   types   are  repeated ly  observed .     If          is  the  number   of  times  wo rd 
type w has been previously encountered ,  we use:  
 

                                        
                                     ,  

 

               

  
        

                         .  

(10) 

 
The model  is very  robust  to   the cho ice of  the  function   (C) ,  as  long as  i t decreases  smoo thly.  
This  mod ified   learning  rate   also   reduces  the  variab il ity  of   the   tagging  accuracy,   wh ile  
sligh tly increasing its mean.  
 
The  second   sum  in  (5)   and   in  (6)   −   represent ing  a  repulsion   force  −   invo lve s   no t  the 
emp irical jo in t but the product of the emp irical marginals .    T hus,   the comp lete update   is:  

 

 

                                                  

                                                  

 
          
  

 
         
  

                     

(11) 

                   ,    

(12) 

E v a l u a t i o n   a n d   d a t a  

 
where               is  samp led   from  the  jo int           ,  and   x 2 and  y2 are  samp led   from  the ma rg ina l 
       independent ly  from  each  o ther   and   independen tly  from  x1  and   y1.   After   each  s tep ,   the 
updated  vector s are  pro jected  back onto  the sphere  S .  
 
After   convergence ,   for   any word   w,  we  have   two   embedded  vectors,         and      .    These 
vectors  are   concatenated   to   form  a  single  geometric   descrip tion  o f  word   type   w.   The 
co llection   of  a ll  these   vectors  is  then  c lustered   us ing  a  weighted   k -means   clus tering 
algorithm:  in  each  i teration ,   a   cluster ’s  centro id   is  updated   as  the  we igh ted   mean  of  its 
curren tly  assigned   const ituent  vectors,   wi th  the  weight   of  the  vec tor   for   word   w  equal  to  
     .   The  number   o f  clus ters  chosen  depend s  on  whe ther   evaluation  is  to   be  done  agains t 
the PTB45  or  the PTB17  tagset (see below,  Sect ion 2 .2) . 1 
 
2 . 2  
 
The  resul ting   assignmen t  o f  cluster   labels  to   word   types  is  used   to   label  the  corpus.   The 
standard   practice  for   evaluat ing  the  performance  o f  the  induced   labels  is  to   either   map   them 
to   the  go ld -standard   tags,   or   to   use  an  information - theoretic  measure.   We  use  the  three  
evaluation  criteria   that  are   most  common  in  the  recent   literature .   The  first  criterion  map s 
each  clus ter   to   the  POS  tag  that  it  bes t matches  accord ing  to   the  hand -anno tated   labe ls.    The 
match  is  determined   by  find ing  the  tag  tha t  is   mo st  frequently  assigned   to   any  token  of   any  
word   type   in  the  cluster.     Because   the  criterion  is  free  to   assign  several   clusters  to   the  same 
POS  tag,   th is  evaluat ion  technique  is  ca lled   many-to -one  mapp ing ,   or  MTO .     Once  the  map  
is  constructed ,   the  accuracy  score  is  ob tained   as   the  fract ion o f  all  tokens  who se  infer red   tag  
under  the  map  matches  the  hand -anno tated  tag.  
 
The  second   criterion ,   1-to -1   mapp ing,   is  simi lar   to   the  firs t,   but  the  mapp ing  is  restricted  
from  assign ing mu lt ip le   clus ters  to   a  single  tag;   hence   it  is  cal led   one -to -one  mapp ing ,   or   1 -
to-1 .  Most  au thors  construct   the  1 -to -1   mapp ing   greed ily,   assigning  maximal -score   label-to -
tag matches  firs t ;  some   au thors,   e .g.   [15] ,  use  the op tima l map .  Once  the map   is  cons tructed ,  
the accuracy is compu ted  jus t  as in MTO .  The th ird  criter ion ,  variation o f informat ion ,  or  VI , 
is a map -free informa tion -theoretic metric  [9 ,  2] . 
 
We  no te   that   we   and   o ther   authors  found   the  mos t  reliab le  criterion  for   comparing  
unsupervised   POS  taggers   to   be  MTO.   However,   we  inc lude  a ll  three  criteria  for  
comp leteness.  

We  use   the Wall  S treet  Journal  part  o f  the  Penn  Treebank   [8]   (1 ,173 ,766   tokens) .  We  ignore 
cap italizat ion,   leav ing  43 , 766   word   types ,   to   compare  performance   with  o ther   models  
consistently.    Evaluat ion  is done  against  the  ful l  tag  set  (PTB45) ,  and  against  a  coarse  tag   se t 
(PTB17)   [12] . For  PTB45  evaluation,  we use  either  45  or   50  clusters,   in order   for  our   resu lts  
to   be  comparab le   to   all   recent  works .     For   PTB17   evaluation,   we  use   17   clus ters,   as  do   all  
o ther  au thors.  

 
3  

R e s u l t s  

Figure   1   shows   the   model  performance  when  evaluated   wi th   several  measures.   MTO17   and  
MTO50   refer   to   the   number   of  tokens  tagged   correctly  under   the many - to -1  mapp ing   for   the 
PTB45   and   PTB17   tagse ts   respectively.     The  type -accuracy  curves  use  the  same  mapp ing  

                                                 
1 Source code is available at the author’s website: faculty.biu.ac.il/~marony. 

and   tagse ts,   but  record   the   fraction  o f  word   types  whose   inferred   tag  matches  thei r   "modal" 
anno tated   tag,   i.e. ,   the  anno tated   tag  co -occurring  mo st  frequent ly  wi th  this  word   type.    We 
also   show  the  scaled   log  like lihood ,   to   illustrate  i ts   convergence.     These  resu lts  we re 
produced   using  a   constant ,   pre-computed ,     .     Using  this  constant  value  a llows  the  model  to  
run in a  mat ter  of minutes ra ther  than the hours or  days required  by  HMMs and  MRFs .    

Figure 1 : Scores against number  of  iterations  (b igram  updates) .  Scores are averaged  over  10 
sessions,   and   shown   wi th   1 -std   error   bars.  MTO17   is  the  Many - to -1   tagg ing  accuracy  score  
based   on  17   induced   labels  mapped   to   17   tags.  MTO50   is  the Many - to -1   score  based   on  50  
induced   labels  mapped   to   45   tags.  Type Accuracy  17   (50)   is  the  average  accuracy  per   word  
type ,  where  the  go ld -s tandard  tag o f a word  type  is  the  moda l  anno ta ted  tag o f that  type (see  
text) .    All runs used      =  0 .154 , r=25 .  

 

 

 
Figure 2 : Comparison o f models with d i fferent  d imensionali ties :  r =   2 ,   5 ,   10 ,  25 .  MTO17   is  
the Many- to-1  score  based  on 17  induced  labels mapped  to  PTB17 tags.    
 

 

Figure   2   shows  the  model  performance  for   d ifferent  d imensional ities  r.   As  r  increases,   so  
does  the   per formance.   Unl ike  prev ious  app lications  o f   CODE   [3]   (wh ich  often  emphasize  

 

 

01020304050600.50.550.60.650.70.75bigram updates (times 100,000)  MTO17,r=25MTO17,r=10MTO17,r=5MTO17,r=202040608010012000.10.20.30.40.50.60.70.80.91bigram updates (times 100,000)  log-likelihoodMTO17MTO50Type Accuracy 17Type Accuracy 50visua lization  o f  data   and   thus  require  a   low  d imension) ,   this   unsupervised   POS -tagging  
app lication  bene fi ts  from  high  values  o f  r.   La rger   values  of  r  cause  bo th  the   tagg ing 
accuracy to  improve and  the variab ili ty during convergence to  decrease.  

 

Model  

Many-to-1 

1-to-1 

VI  

PTB17   PTB45
-45 

PTB45
-50 

PTB17   PTB45
-45 

PTB45
-50 

PTB17   PTB45
-45 

PTB45
-50 

S-CODE 
(Z=0.1456) 

73 .8 
(0 .5) 

68.8 
(0.16) 

70 .4 
(0 .5) 

S-CODE 
(Z=0.3) 

74 .5 
(0 .2) 

68.6 
(0.16) 

71 .5 
(0 .6) 

52 .2 

50 .0 

50 .0 

2 .93 

3 .46 

3 .46 

54 .9 

48 .7 

48 .8 

2 .80 

3 .38 

3 .39 

LDC 

75 .1 
(0 .04) 

68 .1 
(0 .2) 

71 .2 
(0 .06) 

59 .3 

 

48 .3 

Brown 

 

67 .8 

70.5 

 

50 .1 

51 .3 

 

 

 

 

3 .47 

3 .45 

HMM-EM   64 .7 

HMM-VB   63 .7 

HMM-GS   67 .4 

 

 

 

62 .1 

43 .1 

60 .5 

51 .4 

66 .0 

44 .6 

 

 

 

40 .5 

3 .86 

46 .1 

3 .44 

49 .9 

3 .46 

HMM-
Sparse(32)  

VEM 
 (10-1,10-1) 

70 .2 

65.4 

68.2 

54.6 

 

 

49.5 

44.5 

52 .8 

46 .0 

 

 

 

 

 

 

 

 

 

4 .48 

4 .28 

4 .04 

 

 

Tab le  1 :  Comparison  to   o the r   models,   under   three  d i fferent  evaluat ion  measures.   S -CODE  
uses  r  =   25   d imens ions.   It  was  run  10   times,   each  with   12 ·106  update  steps .   LDC  is  from 
[15];  Brown  shows  the  bes t   results  from  [14]   and   webs ite  mentioned   there in ;  HMM-EM ,  
HMM-VB  and  HMM -GS show  the  best  results  from  [2] ;  HMM -Sparse(32)  and  VEM  show 
the  bes t  results  from  [5] .   The  numbers  in  parentheses  are  standard   deviations.   For   the  VI  
criterion,   lower   va lues  are   better .   PTB45 -45   map s   45   induced   labels   to   45   tags,   wh ile  
PTB45 -50  maps 50  induced   labels  to  45   tags.    
 
Tab le   1   compares  our   model,   S -CODE ,   to   previous  state -o f-the -art  approaches.   Under   the  
Many- to-1   criter ion,  wh ich we   find   to  be  the most  appropriate of  the  three  for   the  evaluat ion  
of  unsupervised   POS  tagger s,   S -CODE   is  superior   to   HMM   results,   and   scores   comparab ly 
to  [15] ,  the highest -per forming model to  date  on th is task .      

 

We  find   that  the  model  is  very  robust  to   the  cho ice  of      wi th in  the  range   0 .1   to   0 .5 .    This 
robustness  lends  promise   fo r   the   usefulness  o f  this  method   for   o ther   app lications   in  wh ich  
the  partition  function  is  impractical   to   compute.   This  po int  is  d iscussed   further   in   the  next  
section.  

 
4  

D i s c u s s i o n  

The  prob lem  of  embedd ing  heterogeneous   categorical  data  ( X ,Y)   based   on  their   co -
occurrence   sta tis tics  may  be   formu lated   as  the  task  o f  find ing   a  pair   o f maps         and       
such  that ,   for  any pair   (x ,y) ,   the  d istance between  the  images of  x  and  y   reflects  the s tatist ical 
interac tion  between  them.   Such  embedd ings  have  been  used   mo stly  for   the  purpose  of 
visua lization  and   exp loratory  data  analys is.   Here  we  demonstrate   that  emb edd ing   can  be 
successful ly  app lied   to   a  we ll -stud ied   computational -linguist ics  task,   achieving  sta te -of- the -
art performance .  
 
4 . 1  

S - C O D E   v .   C O D E  

The  approach  proposed   here,   S -CODE ,   is  a  varian t  of  the CODE  model  of  [3] .   In  the  task  at 
hand ,   the  sets  X   and   Y  to   be   embedded   are  large   (43K) ,   mak ing  mo st  conventional 

embedd ing  approaches ,   includ ing  CODE   (as  imp lemen ted   in  [3]) ,   imprac tical .  As  exp lained  
below,  S-CODE  overcomes  the  large -datase t chal lenge by cons training  the maps  to   lie on  the 
unit sphere.  It uses stochas tic  grad ien t ascent  to  maximize the like lihood  of the model .  

The grad ien t of   the  log - likel ihood  w.r. t.   a  given           includes  two   components,   each wi th  a 
simp le  in tu itive   mean ing.   The   first  component  embod ies  an  attract ion  force,   pulling         
toward        in proportion  to   the  emp irical  jo in t           .  The second  component,   the grad ien t 
of  the  regulariza tion  term ,       ,   embod ies  a  repu lsion  force;  it  keeps  the  so lut ion  away 
from  the   trivial  s tate   where   all  x 's  and   y 's  are  mapped   to   the  same  po int,   and   more  generally  
attemp ts  to   keep   Z  smal l.  The   repulsion  force  pushes        away  from        in  proportion  to  
 
the  product  of  the  emp irical  marginals         and        ,   and   is  scaled   by         
  .   The 
computational comp lex ity o f   Z,  the parti tion  function,  is               .  

In  the   app licat ion  s tud ied   here,   the  use  o f  the  spherical  constra in t  of  S -CODE   has  two  
important  consequences.   Fi rst,   it  makes  the  compu tation  of  Z  unnecessary.   Indeed ,   when  
us ing  the   spherical  cons traint,   we  observed   tha t   Z,   when  actually  computed   and   updated 
every 10 6  step s,  does no t deviate  much  from  its  ini tial value .  For  examp le,   for   r =  25 ,  Z  rises  
smoo thly  from  0 .145   to   0 .182 .   No te  tha t  the  ab so lute  minimum  o f  Z—ob tained   for   a      that  
maps  al l  of  W  to   a  s ingle   po int  on  S   and   a     that  maps  all  of  W  to   the  opposite  po int— is    
     ; the  ab so lu te  max imum o f Z ,  ob tained  for     and    that map  all of W to  the same po int,  is 
1 .  We   also   observed   that  rep lacing  Z,   in  the  update  algor ithm,  by  any  constant      in  the   range  
[ .1   .5 ] does no t dramat ically   alter   the behavior  of  the model .  We neverthe less no te  that  larger  
values  o f      tend   to   yield   a   s lightly  higher   performance  of  the  POS  tagger   bu ilt  from  the  
model.   No te  that  the   only  effec t  of  changing       in  the  stochastic  grad ient  algorithm  is  to  
change   the re lative  s trength of the attract ion and  repulsion terms.  

We  compared   the   performance  of   S -CODE   with  CODE .  The  original  CODE   imp lementat ion  
[3]   could   no t  support  the   s ize   of  our   data  set.   To   overcome  this  limita tion,   we  used   the  
stochastic -grad ient  me thod   described   above ,   but   wi thout   pro jecting   to   the  sphere.   This 
required  us  to   compute   the partition   funct ion,   wh ich   is  highly  computa tiona lly  intensive .  We  
there fore   computed   the   part i tion  function  on ly  once  every  q   update  s teps  (where  one   update 
step   is  the   samp ling  of  one   b igram) .   We  found   that  for   q   =   105  the  partition  func t ion   and  
like lihood   changed   smoo thly  enough  and   converged ,   and   the  embedd ings  yielded   tagging  
performances   that d id  no t d i ffer   signi ficantly  from  those  ob tained  wi th S -CODE .  The second  
important  consequence   o f  imposing  the  spherical  const raint  is  that  it  makes  the  stochast ic  
grad ien t -ascent  procedure  marked ly  smoo ther.  As  a  resul t,   a  relatively  large  step   size   can  be 
used ,   achieving  convergence  and   exce llent  tagging  performance  in  abou t  10   minutes  of  
computation  time   on  a   desk top   mach ine.   CODE   requires  a  smaller   s tep   size  as  wel l  as  th e  
recomputa tion o f  the  partition  function ,   and ,   as  a  resu lt,   computation  time  in  th is  app lication  
was 6  times  longer  than with  S -CODE .  

When  gaug ing   the  app licab ility  o f  S -CODE   to   d i fferent  large -scale  embedd ing  prob lems,  
one  should   try  to   gain  some   underst and ing   of  why  the   spherica l  constraint   stab i lizes  the  
partition  function,   and   whether   Z   wil l  s tab ilize  around   the  same  value  for   o ther   prob lems.  
The  answer   to   the   first  quest ion   appears  to   be  that  the  regularizat ion  term  is  no t  so   st rong  as  
to   prevent  clus ter s  from  forming—this  is  demonstra ted   by  the  exce llent  performance  of  the  
model  when  used   for   POS   tagging—yet  it  is  strong  enough  to   enforce  a  fairly   un i form 
d istribu tion  o f  these   cluster s   on  the  sphere—resul ting  in   a   fairly  s tab le   value  of  Z.   One  may 
reasonab ly  conjecture  that  this  behavior   wi ll  generalize   to   o ther   prob lems .   To   answer   the  
second  question,  we  no te  tha t   the  order  of magnitude of  Z  is essent ially  set by  the coarsest o f  
the  two   estimates  derived   in  Section  2 ,   namely            0 .135 ,   and   that  this  est imate   is  
prob lem-independent .   As  a  resul t,   S -CODE   is,   in  princ ip le,   app licab le  to   datasets  of  much 
larger   size  than  the   presen t  prob lem.   The  computa tional  comp lexity  o f   the  algo rithm  is 
O(Nr) ,   and   the   memory  requirement  is  O( |W |r)   where  N  is  the  number   o f  word   tokens,   and  
|W |  is  the   number   of  word   types.   In  contrast,   and   as  mentioned   above,   CODE ,   even  in  our  
stochastic -grad ient  ver sion ,   is  cons iderab ly more  compu tationa lly  intensive ;  it  wou ld   clearly 
be  comp letely impractica l fo r  much la rger  datasets.    
 
4 . 2  

C o m p a r i s o n   t o   o t h e r   P O S   i n d u c t i o n   m o d e l s  

Even  though  embedd ing  models  have  been  s tud ied   extens ive ly,   they  are  no t  widely  used   for  

POS  tagg ing  (see  however   [18]) .   For  the   unsupervised   POS  tagg ing  task ,   HMMs   have  unti l 
recently  dominated   the  field .   Here  we   show  that  an  embedd ing  model   subs tant ially  
outperforms  HMMs ,   and   achieves   the  same   level  o f  per formance   as  the   best   d istributional -
only model  to  date  [15] .  Models that use  features,  e.g.  morpho logical,  achieve higher  tagging 
precision  [11 ,   14] .   Incorporating  features  into   S -CODE   can  easi ly  be  done,   either   d irectly   or  
in a two -step  approach as in  [14] ; this is le ft  for  future work.  

One  o f  the   widely -acknowledged   cha llenges  in   app lying  HMMs  to   the  unsupervised   POS    
tagging  prob lem  is   that  these   models  do   no t  a fford   a   convenient   veh icle  to   mode ling  an  
important sparseness property o f natural  languages,  name ly  the  fact  that any g iven  wo rd   type 
admits  o f  on ly  a   small  number   of  POS  tags—o ften  only  one   (see   in  particular   [7 ,   2 ,   4]) .   In 
contrast,   the   approach  presented   here  maps  each  word   type  to   a  single  po int  in     .  Hence ,   i t  
assigns a   sing le  tag  to  each word   type,   like a number  of  o ther   recent approaches  [15 ,  16 ,  17] . 
These   approaches  are  incapab le   of  d isamb iguat ing,   i.e. ,   of  assigning   d i fferen t  tags  to   the  
same word  depend ing on context,  as  in  " I  long to  see  a long  movie."  HMMs are,  in p rincip le,  
capab le   of  do ing  so ,   but  at  the   cost  of  over-parame terization.   In  view  o f  the  superior  
performance   of  S -CODE   and   of  o ther   type - leve l  approaches ,   it  appears  that   under -
parameterizat ion might be the better  cho ice  for  th is task.  

Ano ther   d i fference  between  our  model  and  HMMs  previously  app lied   to   th is  prob lem  is  tha t  
our   model  is  symmetr ic,   thereby  modeling  right  and   left  con tex t  d istribution s.   In  contrast ,  
HMMs are  asymmetric   in  that  they  typ ically mode l a  left -to -right  transition and  would   find  a 
d ifferent  so lut ion  i f  a   righ t-to -le ft  trans ition  were  modeled .   We  argue  tha t  us ing  bo th 
d istribu tion s  in  a   symme tric   way  better   cap tures  the  important  linguistic  information.   In  the 
past,   le ft  and   right  d istribu tions  were   extracted   by  factor ing  the  b igram matr ix  and   using  the 
left   and   righ t  eigenvectors.   Such  a   linear   me thod   does   no t  hand le  rare  words   wel l.   Ins tead ,  
we  choose   to   learn  the   ratio                        
 
.   This  approach  allows   words  with  s imilar  
contexts but d i fferent unigram  frequencies to  be embedded  near   each o ther.  

Like  HMMs,   CODE   provides  a  model   of   the  d istribu t ion  o f  the   data  at   hand .   S -CODE  
departs slightly  from  th is  framework .  Since i t does no t    use the exact partition  func tion in the 
stochastic   grad ient   ascent   procedure—and   was  ac tually  found   to   per form  best  when  
rep lacing Z,  in  the  update  ru le,  by a constant  that  is  substantia lly la rger  than the  true  value o f  
Z—it  only  approximately  converges  to   a  local  maximum  of  a  likelihood   func tion.   In  future  
work,   and   as  a  more  rad ical  deviation  from  the  CODE   model,   one  may  then  give  up  
altogether   modeling  the   d ist ribution  of   X   and   Y,   instead   relying  on   a   heur ist ically  mo tivated  
ob jective   function  o f  sphere -constrained   embedd ings        and       ,   to   be  max imized .  
P relimina ry  stud ies  using  a   number   of  alternative  functional  forms  for   the  regularization  
term yielded  promising resul ts.  

Although  S-CODE   and   LDC  [15]   achieve  essentia lly  the  same  level  o f  performance  on  
taggings   tha t  induce   17 ,   45 ,   or   50   labels  (Tab le  1) ,  S -CODE   proves   superior   for   the  
induction  of   very  fine -grained   taggings.   Thus,   we   compared   the  per formances  of   S -CODE  
and  LDC on  the  task of  inducing 300   labels.  Under   the MTO  criterion,  LDC  achieved  80 .9% 
(PTB45 )   and   87 .9%  (PTB17) .   S -CODE   s igni fican tly  outperformed   it,   with  83 .5%  (PTB45) 
and  89 .8% (PTB17) .  

The  appeal  of  S -CODE   l ies  no t  on ly  in  i ts  s trong  performance  on  the  unsupervised   POS 
tagging  prob lem,   bu t  also   in  its  s imp licity,   i ts  robustness,   and   its  math ematica l  ground ing .    
The  mathematics  underlying   CODE ,   as  developed   in  [3] ,   are  intui tive  and   relatively   simp le .  
Modeling  the  jo int  probab ility  o f  word   type  co -occurrence  through   d istances  between  
Euclidean  embedd ings ,   without   relying  on   d iscre te  categories  or   s tates ,   is  a   novel  and  
promising  approach  for   POS  tagg ing .     The  spherical   cons train t  introduced   here  permits  the  
approximat ion  o f  the   part ition  funct ion  by  a  cons tan t,   wh ich  is  the   key  to   the  e fficiency  o f 
the  algorithm  for   large  datasets .     The  stochastic -grad ient   procedure   produces  two   competing 
forces  wi th  intuit ive   mean ing ,   familiar   from  the  li terature  on  learning   in  generative  models. 
 While  the   accuracy  and   compu tational  efficiency  o f  S -CODE   is matched   by  the  recent  LDC  
algorithm  [15] ,   S-CODE   is  more   robust ,   showing  very  little  change  in  performance   over   a 
wide  range   of  imp lemen tation  cho ices.     We  expect  tha t   this  improved   robus tness  will   allow 
S-CODE   to   be   easi ly  and   successfully  app lied   to   o ther   large -scale  tasks ,   bo th  linguistic  and  
non-l inguistic.    
 

R e f e r e n c e s  

 
[1] Alexander  Clark.  2003.  Combining  distributional  and  morphological  information  for  part  of  speech 
induction.  In  10th  Conference  of  the  European  Chapter  of  the  Association  for  Computational 
Linguistics, pages 59–66.  

[2] Jianfeng  Gao  and  Mark  Johnson.  2008.  A  comparison  of  bayesian  estimators  for  unsupervised  Hidden 
Markov Model POS  taggers.  In Proceedings of  the 2008 Conference on Empirical Methods  in Natural 
Language Processing, pages 344–352. 

[3] Amir   Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby.  2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Research, 8:2265–2295. 

[4] Sharon  Goldwater  and  Tom  Griffiths.  2007.  A  fully  Bayesian  approach  to  unsupervised  part -of-speech 
tagging.  In  Proceedings  of  the  45th  Annual  Meeting  of  the  Association  of  Computational  Linguistics, 
pages 744–751. 

[5] João  V.  Graça,  Kuzman  Ganchev,  Ben  Taskar,  and  Fernando  Pereira.  2009.  Posterior  vs.  Parameter 
Sparsity in Latent Variable Models. In Neural Information Processing Systems Conference (NIPS).  

[6] Aria  Haghighi  and  Dan  Klein.  2006.  Prototype-driven  learning  for  sequence  models.  In  Proceedings  of 
the Human Language Technology Conference of the NAACL, Main Conference , pages 320–327.  

[7] Mark  Johnson.  2007. Why  doesn’t  EM  find  good HMM  POS -taggers?  In  Proceedings  of  the  2007  Joint 
Conference  on  Empirical  Methods  in  Natural  Language  Processing  and  Computational  Natural 
Language Learning (EMNLP-CoNLL), pages 296–305. 

[8] M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: 
The Penn Treebank. Computational linguistics, 19(2):313–330. 

[9] Marina Meilă.  2003.  Comparing  clusterings  by  the  variation  of  information.  In  Bernhard  Schölkopf  and 
Manfred  K.  Warmuth,  editors,  COLT  2003:  The  Sixteenth  Annual  Conference  on  Learn ing  Theory, 
volume 2777 of Lecture Notes in Computer Science, pages 173–187. Springer. 

[10] Sam  T.  Roweis  and  Lawrence  K.  Saul.  2000.  Nonlinear  dimensionality  reduction  by  locally  linear 
embedding. Science, 290:2323–2326. 

[11]  Taylor  Berg-Kirkpatrick,  Alexandre  Bouchard-Côté,  John  DeNero,  and  Dan  Klein.  2010.  Painless 
Unsupervised  Learning  with  Features.  In  Human  Language  Technologies:  The  2010  Annual 
Conference  of  the  North  American  Chapter  of  the  Association  for  Computational  Linguistics ,  pages 
582-590. 

[12] Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training  log-linear models on unlabeled 
data.  In  Proceedings  of  the  43rd  Annual  Meeting  of  the  Association  for  Computational  Linguistics 
(ACL’05), pages 354–362. 

[13] Joshua  B.  Tenenbaum,  Vin  de  Silva,  and   John  C.  Langford.  2000.  A  global  geometric  framework  for 
nonlinear dimensionality reduction. Science, 290:2319–2323. 

[14] Christos  Christodoulopoulos,  Sharon  Goldwater  and   Mark  Steedman.  2010.  Two  Decades  of 
Unsupervised  POS  induction:  How  far  have  we  come?   In  Proceedings  of  the  2010  Conference  on 
Empirical Methods in Natural Language Processing (EMNLP 2010) , pages 575–584. 

[15] Michael  Lamar,  Yariv  Maron  and  Elie  Bienenstock.  2010.  Latent-Descriptor  Clustering  for 
Unsupervised POS Induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural 
Language Processing, pages 799–809. 

[16] Yoong  Keok  Lee,  Aria  Haghighi,  and  Regina  Barzilay.  2010.  Simple  Type-Level  Unsupervised  POS 
Tagging.  In  Proceedings  of  the  2010  Conference  on  Empirical  Methods  in  Natural  Language 
Processing, pages 853-861. 

[17] Michael  Lamar,  Yariv  Maron,  Mark  Johnson,  Elie  Bienenstock.  2010.  SVD  and  clustering  for 
unsupervised POS tagging. In Proceedings of the ACL 2010 Conference Short Papers, pages 215-219. 

[18] Ronan Collobert  and  Jason Weston.  2008. A  unified  architecture  for  natural  language  processing: Deep 
neural  networks  with  multitask  learning.  In  Proceedings  of  the  Twenty-fifth  International  Conference 
on Machine Learning (ICML 2008), pages 160–167. 


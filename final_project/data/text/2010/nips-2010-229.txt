Link Discovery using Graph Feature Tracking

Emile Richard
ENS Cachan - CMLA & MilleMercis, France
r.emile.richard@gmail.com

Nicolas Baskiotis
ENS Cachan - CMLA
nicolas.baskiotis@lip6.com

Theodoros Evgeniou
Technology Management and Decision Sciences,
INSEAD
Bd de Constance, Fontainebleau 77300, France
theodoros.evgeniou@insead.edu

Nicolas Vayatis
ENS Cachan & UniverSud - CMLA UMR CNRS 8536, France
nicolas.vayatis@cmla.ens-cachan.fr

Abstract

We consider the problem of discovering links of an evolving undirected graph
given a series of past snapshots of that graph. The graph is observed through the
time sequence of its adjacency matrix and only the presence of edges is observed.
The absence of an edge on a certain snapshot cannot be distinguished from a
missing entry in the adjacency matrix. Additional information can be provided by
examining the dynamics of the graph through a set of topological features, such as
the degrees of the vertices. We develop a novel methodology by building on both
static matrix completion methods and the estimation of the future state of relevant
graph features. Our procedure relies on the formulation of an optimization prob-
lem which can be approximately solved by a fast alternating linearized algorithm
whose properties are examined. We show experiments with both simulated and
real data which reveal the interest of our methodology.

1

Introduction

The prediction of the future state of an evolving graph is a challenge of interest in many applications
such as predicting hyperlinks of webpages [16], ﬁnding protein-protein interactions [7], studying
social networks [9], as well as collaborative ﬁltering and recommendations [6]. Link prediction can
also be seen as a special case of matrix completion where the goal is to estimate the missing entries
of the adjacency matrix of the graph where the entries can be only ”0s” and ”1s”. Matrix completion
became popular after the Netﬂix Challenge and has been extensively studied on both theoretical and
algorithmic aspects [15]. In this paper we consider a special case of predicting the evolution of a
graph, where we only predict the new edges given a ﬁxed set of vertices of an undirected graph by
using the dynamics of the graph over time.
Most of the existing methods in matrix completion assume that weights over the entries (i.e.
the
edges of the graph, e.g. scores in movie recommendation applications) are observed [3]. These
weights provide a richer information than the binary case (existence or absence of a link). Consider
for instance the issue of link prediction in recommender systems. In that case, we consider a bipartite
graph for which the vertices represent products and users, and the edges connect users with the
products they have purchased in the past. The setup we consider in the present paper corresponds to

1

the binary case where we only observe purchase data, say the presence of a link in the graph, without
any score or feedback on the product for a given user. Hence, we will deal here with the situation
where the components of snapshots of the adjacency matrix only consist of ”1s” and missing values.
Moreover, link prediction methods typically use only one snapshot of the graph’s adjacency matrix -
the most recent one - to predict its missing entries [9], or rely on latent variables providing semantic
information for each vertex [11]. Since these methods do not use any information over time, they can
be called static methods. Static methods are based on the heuristic that some topological features,
such as the degree, the clustering coefﬁcient, or the length of the paths, follow speciﬁc distributions.
However, information about how the links of the graph and its topological features have been evolv-
ing over time may also be useful to predict future links. In the example of recommender systems,
knowing that a particular product has been purchased by increasingly more people in a short time
window provides useful information about the type of the recommendations to be made in the next
period. The main idea underlying our work lies in the observation that a few graph features can
capture the dynamics of the graph evolution and provide information for predicting future links.
The purpose of the paper is to present a procedure which exploits the dynamics of the evolution of
the graph to ﬁnd unrevealed links in the graph. The main idea is to learn over time the evolution
of well-chosen local features (at the level of the vertices) of the graph and then, use the predicted
value of these features on the next time period to discover the missing links. Our approach is related
to two theoretical streams of research: matrix completion and diffusion models. In the latter only
the dynamics over time of the degree of a particular vertex of the graph are modeled - the diffusion
of the product corresponding to that vertex for example [17, 14]. Beyond the large number of
static matrix completion methods, only a few methods have been developed that combine static and
dynamic information mainly using parametric methods – see [4] for a survey. For example, [13]
embeds graph vertices on a latent space and use either a Markov model or a Gaussian one to track
the position of the vertices in this space; [10] uses a probabilistic model of the time interval between
the appearance of two edges or subgraphs to predict future edges or subgraphs. However, to the best
of our knowledge, there has not been any regularization based method for this problem, which we
consider in this paper.
The setup of dynamic feature-based matrix completion is presented in Section 2. In Section 3, we
develop a fast linearized algorithm for efﬁcient link prediction. We then discuss the use and esti-
mation of relevant features within this regularization approach in Section 4. Eventually, numerical
experiments on synthetic and real data sets are depicted in Section 5.

2 Dynamic feature-based matrix completion
Setup. We consider a sequence of T undirected graphs with n vertices and n × n binary adjacency
matrices At , t ∈ {1, 2, ..., T } where for each t the edges of the graph are also contained in the graph
at time t + 1. Given At , t ∈ {1, 2, ..., T } the goal is to predict the edges of the graph that are most
likely to appear at time T + 1, that is, the most likely non-zero elements of the binary adjacency
matrix AT +1 . To this purpose we want to learn an n × n real-valued matrix S whose elements
indicate how likely it is that there is a non-zero value at the corresponding position of matrix AT +1 .
The edges that we predict to be the most likely ones at time T + 1 are the ones corresponding to the
largest values in S .
We assume that certain features of matrices At evolve over time smoothly. Such an assumption is
necessary to allow learnability of the evolution of At over time. For simplicity we consider a linear
feature map f : At (cid:55)→ Ft where Ft is an n × k matrix of the form Ft = AtΦ, with Φ an n × k
matrix of features. Various feature maps, possibly nonlinear, can be used. We discuss an example
of such features Φ and a way to predict FT +1 given past values of the feature map F1 , F2 , ..., FT in
Section 4 – but other features or prediction methods can be used in combination with the main part
of the proposed approach. In the proposed method discussed in Section 3 we assume for now that
we already have an estimate of FT +1 .
An optimization problem. The procedure we propose for link prediction is based on the assumption
last adjacency matrix AT , a set of features Φ, and an estimate (cid:98)F of FT +1 based on the sequence
that the dynamics of graph features also drive the discovery of the location of new links. Given the
2

of adjacency matrices At , t ∈ {1, 2, ..., T }, we want to ﬁnd a matrix S which fulﬁlls the following
requirements:
• S has low rank - this is a standard assumption in matrix completion problems [15].
• S is close to the last adjacency matrix AT - the distance between these two matrices will
provide a proxy for the training error.
For any matrix M , we denote by (cid:107)M (cid:107)F = (cid:112)Tr(M (cid:48)M ) , the Frobenius norm of M , with M (cid:48) being
• The values of the feature map at S and AT +1 are similar.
square matrix N . We also deﬁne (cid:107)M (cid:107)∗ = (cid:80)n
the transpose of M and the trace operator Tr(N ) computes the sum of the diagonal elements of the
k=1 σk (M ) , the nuclear norm of a square matrix M
of size n × n, where σk (M ) denotes the k-th largest singular value of M . We recall that a singular
value of matrix M corresponds to the square root of an eigenvalue of M (cid:48)M ordered decreasingly.
ν (cid:107)SΦ − (cid:98)F (cid:107)2
The proposed optimization problem for feature-based matrix completion is then:
= τ (cid:107)S (cid:107)∗ +
(cid:107)S − AT (cid:107)2
1
1
.
min
F +
F ,
2
2
S
and where τ and ν are positive regularization parameters. Each term of the functional L reﬂects
the aforementioned requirements for the desired matrix S . In the case where ν = 0, we do not use
information about the dynamics of the graph. The minimizer of L corresponds to the singular value
thresholding approach developed in [2], which is therefore a special case of (1). Note that a key
difference between link prediction and matrix completion is that in (1) the training error uses all
entries of the adjacency matrix while in the case of matrix completion only the known entries (in our
case the ”1s”) are used. We now discuss an efﬁcient optimization algorithm for (1), the main part of
this work.

L(S, τ , ν ) , with L(S, τ , ν )

(1)

3 An algorithm for link discovery

Solving (1) is computationally slow. We adapt the fast linearization method developed in [5] to
our problem, which attains an optimal iteration complexity when using only ﬁrst order information.
Here, the functional L(S, τ , ν ) is continuous and convex but not differentiable with respect to S .
We propose to convert the minimization of the target functional L(S, τ , ν ) into a tractable problem
through the following steps:
ν (cid:107)SΦ − (cid:98)F (cid:107)2
1. Variable splitting - Set:
(cid:107)S − AT (cid:107)2
g(S, τ ) = τ (cid:107)S (cid:107)∗
1
1
Denote by S, (cid:101)S two n × n matrices. Then, the optimization problem (1) is equivalent to:
and
F +
F .
h(S, ν ) =
2
2
subject to S − (cid:101)S = 0 .
L(S, (cid:101)S ) ,
S, (cid:101)S
(2)
min
= g(S, τ ) + h( (cid:101)S , ν ).
where L(S, (cid:101)S )
.
2. Smoothing the nuclear norm - We recall the variational formulation of the nuclear norm
(cid:107)S (cid:107)∗ = maxZ {(cid:104)S, Z (cid:105)
: σ1 (Z ) ≤ 1}. Using the technique from [12], we can use a
(cid:110)(cid:104)S, Z (cid:105) − η
(cid:111)
smooth approximation of the nuclear norm and replace g in the functional by a surrogate
function gη with η > 0 being a smoothing parameter:
F : σ1 (Z ) ≤ 1
(cid:107)Z (cid:107)2
gη (S, τ ) = τ · max
2
Z
3. Alternating minimization - We propose to minimize the functional which is continuous,
= gη (S, τ ) + h( (cid:101)S , ν ) ,
Lη (S, (cid:101)S )
differentiable and convex:
under the constraint that S = (cid:101)S . To do this, one has to minimize simultaneously the two
.
(3)
functions gη and h. In order to derive the iterative algorithm based on linearized alternating

3

1
2µ

minimization, we introduce two strictly convex approximations of these functions which
Gη ,µ (S, ˜S ) = gη (S, τ ) + (cid:104)∇h( (cid:101)S ), S − (cid:101)S (cid:105) +
(cid:107)S − (cid:101)S (cid:107)2
involve an additional parameter µ > 0:
F
(cid:107)S − (cid:101)S (cid:107)2
Hµ (S, (cid:101)S ) = h( (cid:101)S , ν ) + (cid:104)∇gη (S ), (cid:101)S − S (cid:105) +
1
F
2µ
with the convergence results at the end of this section. We denote by mG ( (cid:101)S ) the minimizer of
where (cid:104)B , C (cid:105) = Tr(B (cid:48)C ) for two matrices B , C . The tuning of the parameter µ will be discussed
Gη ,µ (S, ˜S ) with respect to S and mH (S ) the minimizer of Hµ (S, (cid:101)S ) with respect to (cid:101)S . We can
now formulate an algorithm for the fast minimization of the functional Lη (S, (cid:101)S ) inspired by the
algorithm FALM in [5] (see Algorithm 1). Note that, in the alternating descent for the simultaneous
linear combination of the updates for S and (cid:101)S . The work by Ma and Goldfarb shows indeed that the
minimization of the two functions Gη ,µ and Hµ , we use an auxiliary matrix Zk . This matrix is a
link prediction algorithm are those of the minimizers mG ( (cid:101)S ) and mH (S ). It turns out that in our
particular choice made here leads to an optimal rate of numerical convergence. Key formulas in the
case, these minimizers have explicit expressions which can be derived when solving the ﬁrst-order
optimality condition as Proposition 1 shows.

Algorithm 1 - Link Discovery Algorithm
Parameters: τ , ν, η
Initialization: W0 = Z1 = AT , α1 = 0
(cid:101)Sk ← mH (Sk )
for k = 1, 2, . . . do
(Sk + (cid:101)Sk )
Sk ← mG (Zk )
and
(cid:113)
Wk ← 1
2
(cid:0) αk ( (cid:101)Sk − Wk−1 ) − (Wk − Wk−1 ) (cid:1)
αk+1 ← 1
1 + 4α2
(1 +
k )
2
Zk+1 ← Wk +
1
αk+1
end for
Proposition 1 Let ˆS = (cid:101)S − µ∇h( (cid:101)S ) and the singular value decomposition (cid:98)S = (cid:98)U Diag((cid:98)σ) (cid:98)V . We
 x

also consider the singular value decomposition of S denoted by S = U Diag(ηλ)V . We set the
notation, for x > 0:
τ µ
1 +
η
mG ( (cid:101)S ) = (cid:98)U Diag{α( ˆσ)} (cid:98)V
We then have:
S + ν (cid:98)F Φ(cid:48)(cid:19) (cid:18)(cid:18)
(cid:18)
AT − τ U Diag(cid:0)min{λ, 1}(cid:1)V +
mH (S ) =
1 +
The proof can be found in the Appendix.

In + νΦΦ(cid:48)(cid:19)−1

, x − τ µ

α(x) = max

(cid:19)

1
µ

.

1
µ

Validity of the approximations and rates of convergence. Our strategy replaces the non-differentiable
term in L by a smooth version of it. The next result offers guarantees that minimizing the surrogate
function (3) provides an approximate solution of the initial problem (1). We will say that an element
x is an -optimal solution of a function Ψ(x) if it is such that Ψ(x ) ≤ inf x Ψ(x) + .

4

• We have, for any (S, (cid:101)S ):
Proposition 2 The following statements hold true:
Lη (S, (cid:101)S ) ≤ L(S, (cid:101)S ) ≤ Lη (S, (cid:101)S ) +
nη
.
• To ﬁnd an -optimal solution of L(S, (cid:101)S ), it sufﬁces to ﬁnd an /2-optimal solution of
2
Lη (S, (cid:101)S ) with η = /n.
The proof of this result can be derived straightforwardly from [5]. Moreover, following the proof
√
of Theorem 4.3 in [5], one can show that the number of iterations in order to reach an -optimal
of the gradients of gη and h. With our notations, we can easily derive here: µ = min(cid:0)η/τ , 1/(1 +
solution of Lη using Algorithm 1 is of the order O(1/
). In that result of [5], an optimal choice
ν σ1 (Φ))(cid:1), where σ1 (Φ) is the largest singular value of Φ.
of the parameter µ is provided as the inverse of the largest value for the Lipschitz constant of each
4 Learning the graph features
As discussed above one can use various features Φ and methods to predict the n × k matrix FT +1
given past values of the feature map F1 , F2 , ..., FT . We consider a particular case here to use in
conjunction with the main algorithm in the previous section. In particular, we use as features Φ the
ﬁrst k eigenvectors of the adjacency matrix AT . Let AT = ΩΛΩ(cid:48) be the orthonormal eigenvalue
(:,1:k) , an n × k matrix. Note that
decomposition of AT which is symmetric. We set Φ = Ω(:,1:k)Λ−1
AT Φ = Ω(:,1:k) and that Ω(:,1:k) is the most informative n × k matrix for the reconstruction of AT .
The suggested method aims to estimate AT +1Φ that is informative for the reconstruction of AT +1 .
We denote by Φj , j ∈ {1, 2, ..., k} the n-dimensional feature vectors which are the columns of Φ.
For each feature j ∈ {1, 2, ..., k}, we consider the n-dimensional time series {AtΦj , t = 1, . . . , T }
which describes the evolution of the j -th feature over the n vertices of the graph. We now describe
the procedure for learning the evolution of this j -th graph feature over time:

1. Fix an integer m < T to learn a map between m past values (At−mΦj , . . . , At−1Φj ) and
the current value of the n-dimensional vector AtΦj .
2. Construct the training data for the learning step by using a sliding window of size m from
time t = 1 to t = T ; we then have T − m + 1 training data of dimension n × m for each
feature j .
3. Use ridge regression to ﬁt the training data.
4. Estimate the j -th column of FT +1 as the predicted value for AT +1Φj using the regression
Collecting the results for each j ∈ {1, 2, ..., k}, we obtain the estimate (cid:98)F for the matrix FT +1 used
model at the “point” (AT −m+1Φj , . . . , AT Φj ).
in (1). We point out that using construction with a time shift means that implicitly the relation
between m consecutive values of (At−mΦj , . . . , At−1Φj ) and the next value AtΦj is stable over
time (stationarity assumption). Clearly methods other than ridge regression or other ways of creating
the training data can be used, which we leave for future work.

5 Experimental Results

We tested the proposed method using both simulated and real data sets. As benchmarks we use the
following methods:

1. Static matrix completion corresponding to ν = 0 in (1).
2. The Katz algorithm [8] considered as one of the best static link prediction methods.

5

3. The Preferential Attachment method [1] for which the score (”likelihood”) of an edge
{u, v} is dudv where du and dv are the degrees of u and v .

5.1 Synthetic Data

Qi,j (t) =

We generate sequences of graphs as follows. We ﬁrst generate a sequence of T matrices Q(t) of

 t − µi,j(cid:113)
1 + erf
size n × r whose entries Qi,j (t) are increasing over time as a sigmoid function :
1
2
2σ2
i,j
where µi,j ∈ [0; T ], σi,j ∈ [0; T /3] are picked uniformly for each (i, j ). These matrices provide a
synthetic model for the evolution of the graph over time. We then add noise to the time dynamics
as follows. For a given noise level ω ∈ [0, 1] we replace each entry of Qi,j (t) with probability ω
with any of the other values Qi,j (s) for s picked uniformly from {1, 2, ..., T }. Having constructed
the matrices Q(t), we then generate matrices S (t) = Q(t)Q(t)(cid:48) which are of rank r . We ﬁnally
generate the adjacency matrix At as

A(t) = 1[θ;∞[ (S (t))
for a threshold θ . We pick θ so that the sparsity (i.e. proportion of non-zero entries) of AT reﬂects
the sparsity of the real data used in the next section (≈ 10−3 ). In the experiments, we simulated
graphs with n = 1000 vertices.

5.2 Real Data

Collaborative Filtering1 We can see the purchase histories of e-commerce websites as graph
sequences where links are established between a user and a product when the user purchases that
product. We use data from 10 months music purchase history of a major e-commerce website to
evaluate our method. For our test we selected a set of 103 users and 103 products that had the
highest degrees (number of sales). We split the 8.5 × 103 edges of the graph (corresponding to
purchases) into two parts following their occurrence time. We used the data of the 8 ﬁrst months to
predict the features at the end of the 10th month and use these features as well as the matrix at the
end of the 8th month to discover the purchases during the 2 last months.

5.3 Results

The results are shown in Figure 1 and Tables 1 and 2. The Area Under an ROC Curve (AUC) is
reported. For the simulation data we report the average AUC over 10 simulation runs.
From the simulation results we observe that for low rank underlying matrices, our method outper-
forms the rivals. The same comparative results were observed for ranks as high as 100. Our method
(as well as the static low rank method based on the low rank hypothesis) however fails when the
rank of S (t) is high. However, even in this case our method outperforms the method of static matrix
completion.
The results with the real data further indicate the advantage of using information about the evolution
of the graph over time. Similarly to the simulation data, the proposed method outperforms the static
matrix completion one.

6 Conclusion

The main contribution of this work is the formulation of a learning problem that can be used to
predict the evolution of the edges of a graph over time. A regularization approach to combine both
static graph information as well as information about the dynamics of the evolution of the graph over
time is proposed and an optimization algorithm is developed. Despite using simple graph features

1Notice that we are looking to discover only unobserved links and not new occurences of past links. Thus
the comparaison with some popular benchmarks (as coauthorship data sets) is inappropriate.

6

Figure 1: AUC performance of the proposed algorithm with respect to the two parameters τ and ν
on simulated data.
(r ,ω ) \ Method
(5,0.000)
(5,0.250)
(5,0.750)
(500,0.000)
(500,0.250)
(500,0.750)

Proposed Method
0.671±0.008
0.675 ± 0.009
0.519 ± 0.007
0.592 ± 0.008
0.607 ± 0.011
0.601 ± 0.010

Static
0.648 ± 0.008
0.642 ± 0.007
0.525 ± 0.005
0.587 ± 0.007
0.588 ± 0.009
0.583 ± 0.007

Pref. A.
0.627 ± 0.015
0.602 ± 0.016
0.497 ± 0.007
0.671 ± 0.010
0.649 ± 0.009
0.645 ± 0.017

Katz
0.616 ± 0.015
0.592 ± 0.016
0.491 ± 0.007
0.667 ± 0.009
0.643 ± 0.009
0.641 ± 0.017

Table 1: Simulation data. The average AUC over 10 simulation runs is reported. For each row the
pair of numbers in the ﬁrst column show the rank r and the noise level ω .

as well as estimation of the evolution of the feature values over time, experiments indicate that the
proposed optimization method improves performance relative to benchmarks. Testing, or learning,
other graph features as well as other ways to model their dynamics over time may further improve
performance and is part of future work.

Appendix - Proof of Proposition 1
We ﬁrst write the optimality condition for Gη ,µ (S, ˜S ) with respect to S :
(S − (cid:101)S ) = 0 .
∇S gη (S ) + ∇h( (cid:101)S ) +

1
µ

τ \ ν
0
1
2
3
4

0
0.568
0.626
0.638
0.569
0.569

0.1
0.584
0.684
0.678
0.646
0.556

0.3
0.585
0.683
0.671
0.635
0.562

0.7
0.585
0.675
0.688
0.645
0.565

1.6
0.562
0.668
0.672
0.643
0.563

Table 2: Collaborative Filtering data; AUC for different values of τ and ν . The AUC of preferential
attachment is 0.6019, and Katz reaches 0.6670

7

0246810051015200.450.50.550.60.65(cid:105)(cid:111)AUCWith the notations for (cid:98)S , the previous condition can be written:
µ∇gη (S ) + S − (cid:98)S = 0 .
We now use the fact that ∇gη (S ) = τ U Diag(min{γ , 1})V where S/η = U Diag(γ )V (see [5]).
U = (cid:98)U ,
V = (cid:98)V ,
This observation leads to the solution where S satisﬁes:
ˆσ = µτ min{γ , 1} + ηγ ,
and
which gives the ﬁrst result, since there is a unique solution due to the strict convexity of the function.
Similarly, the optimality condition of Hµ (S, (cid:101)S ) with respect to (cid:101)S is
∇h( (cid:101)S ) + ∇gη (S ) +
( ˜S − S ) = 0 .
1
µ
∇h( (cid:101)S ) = (cid:101)S − AT + ν ( (cid:101)SΦ − (cid:98)F )Φ(cid:48) ,
Since the function h is differentiable as the sum of two quadratic terms, we have:
and we can derive the optimal value for (cid:101)S :
In + νΦΦ(cid:48)(cid:19)−1
(cid:19)
S + ν (cid:98)F Φ(cid:48)(cid:19) (cid:18)(cid:18)
(cid:18)
AT − ∇gη (S ) +
1
mH (S ) =
1 +
µ

. (cid:3)

1
µ

Acknowledgments
This work was partially supported by D IG I T ´EO (B ´EMOL project), that authors greatly thank.

References
[1] A. L. Barab ´asi, H. Jeong, Z. Nda, A. Schubert, and T. Vicsek. Evolution of the social network
of scientiﬁc collaborations. Physica A: Statistical Mechanics and its Applications, 311(3-
4):590–614, 2002.
[2] Emmanuel J. Cand `es and Terence. Tao. A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20(4):1956–1982, 2008.
[3] Emmanuel J. Cand `es and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5), 2009.
[4] Lise Getoor and Christopher P. Diehl. Link mining: a survey. SIGKDD Explorations Newslet-
ter, 7(2):3–12, 2005.
[5] Donald Goldfarb and Shiqlan Ma. Fast alternating linearization methods for minimizing the
sum of two convex functions. Technical Report, Department of IEOR, Columbia University,
2009.
[6] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative ﬁltering for implicit feedback
datasets. In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM
2008), pages 263–272, 2008.
[7] Hisashi Kashima, Tsuyoshi Kato, Yoshihiro Yamanishi, Masashi Sugiyama, and Koji Tsuda.
Link propagation: A fast semi-supervised learning algorithm for link prediction. In Proceed-
ings of the SIAM International Conference on Data Mining, SDM 2009, pages 1099–1110,
2009.
[8] Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39–43,
1953.
[9] David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks.
Journal of the American Society for Information Science and Technology, 58(7):1019–1031,
2007.
[10] Tanya Y. Berger-Wolf Mayank Lahiri. Structure prediction in temporal networks using frequent
subgraphs. IEEE Symposium on Computational Intelligence and Data Mining (CIDM), 2007.

8

[11] Kurt Miller, Thomas Grifﬁths, and Michael Jordan. Nonparametric latent feature models for
link prediction. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta,
editors, Advances in Neural Information Processing Systems 22, pages 1276–1284. 2009.
[12] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,
103(1):127–152, 2005.
[13] Purnamrita Sarkar, Sajid Siddiqi, and Geoffrey J. Gordon. A latent space approach to dynamic
embedding of cooccurrence data. In In Proceedings of the Eleventh International Conference
on Artiﬁcial Intelligence and Statistics (AI-STATS), 2007.
[14] Ashish Sood, Gareth M. James, and Gerard J. Tellis. Functional regression: A new model for
predicting market penetration of new products. Marketing Science, 28(1):36–51, 2009.
[15] Nathan Srebro, Jason D. M. Rennie, and Tommi S. Jaakkola. Maximum-margin matrix fac-
torization. In Lawrence K. Saul, Yair Weiss, and L ´eon Bottou, editors, Advances in Neural
Information Processing Systems 17, pages 1329–1336. MIT Press, Cambridge, MA, 2005.
[16] Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne Koller. Link prediction in relational
data. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch ¨olkopf, editors, Advances in Neural
Information Processing Systems 16. MIT Press, Cambridge, MA, 2004.
[17] Demetrios Vakratsas, Fred M. Feinberg, Frank M. Bass, and Gurumurthy Kalyanaram. The
Shape of Advertising Response Functions Revisited: A Model of Dynamic Probabilistic
Thresholds. Marketing Science, 23(1):109–119, 2004.

9


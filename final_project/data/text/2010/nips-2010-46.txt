Universal Kernels on Non-Standard Input Spaces

Andreas Christmann
University of Bayreuth
Department of Mathematics
D-95440 Bayreuth
andreas.christmann@uni-bayreuth.de

Ingo Steinwart
University of Stuttgart
Department of Mathematics
D-70569 Stuttgart
ingo.steinwart@mathematik.uni-stuttgart.de

Abstract

During the last years support vector machines (SVMs) have been successfully ap-
plied in situations where the input space X is not necessarily a subset of Rd . Ex-
amples include SVMs for the analysis of histograms or colored images, SVMs for
text classiﬁcation and web mining, and SVMs for applications from computational
biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be
consistent to the Bayes risk, if either the input space is a complete separable metric
space and the reproducing kernel Hilbert space (RKHS) H ⊂ Lp (PX ) is dense,
or if the SVM uses a universal kernel k . So far, however, there are no kernels of
practical interest known that satisfy these assumptions, if X (cid:54)⊂ Rd . We close this
gap by providing a general technique based on Taylor-type kernels to explicitly
construct universal kernels on compact metric spaces which are not subset of Rd .
We apply this technique for the following special cases: universal kernels on the
set of probability measures, universal kernels based on Fourier transforms, and
universal kernels for signal processing.

1

Introduction

For more than a decade, kernel methods such as support vector machines (SVMs) have belonged
to the most successful learning methods. Besides several other nice features, one key argument
for using SVMs has been the so-called “kernel trick” [22], which decouples the SVM optimization
problem from the domain of the samples, thus making it possible to use SVMs on virtually any input
space X . This ﬂexibility is in strong contrast to more classical learning methods from both machine
learning and non-parametric statistics, which almost always require input spaces X ⊂ Rd . As a
result, kernel methods have been successfully used in various application areas that were previously
infeasible for machine learning methods. The following, by no means exhaustive, list illustrates this:
• SVMs processing probability measures, e.g. histograms, as input samples have been used to an-
alyze histogram data such as colored images, see [5, 11, 14, 12, 27, 29], and also [17] for non-
extensive information theoretic kernels on measures.
• SVMs for text classiﬁcation and web mining [15, 12, 16],
• SVMs with kernels from computational biology, e.g. kernels for trees and graphs [23].
In addition, several extensions or generalizations of kernel-methods have been considered, see
e.g. [13, 26, 9, 16, 7, 8, 4]. Besides their practical success, SVMs nowadays also possess a rich

1

statistical theory, which provides various learning guarantees, see [31] for a recent account. In-
terestingly, in this analysis, the kernel and its reproducing kernel Hilbert space (RKHS) make it
possible to completely decouple the statistical analysis of SVMs from the input space X . For ex-
ample, if one uses the hinge loss and a bounded measurable kernel whose RKHS H is separable
and dense in L1 (µ) for all distributions µ on X , then [31, Theorem 7.22] together with [31, Theo-
rem 2.31] and the discussion on [31, page 267ff] shows that the corresponding SVM is universally
classiﬁcation consistent even without an entropy number assumption if one picks a sequence (λn )
of positive regularization parameters that satisfy λn → 0 and nλn/ ln n → ∞. In other words,
independently of the input space X , the universal consistency of SVMs is well-understood modulo
an approximation theoretical question, namely that of the denseness of H in all L1 (µ).
For standard input spaces X ⊂ Rd and various classical kernels, this question of denseness has been
positively answered. For example, for compact X ⊂ Rd , [30] showed that, among a few others,
the RKHSs of the Gaussian RBF kernels are universal, that is, they are dense in the space C (X )
of continuous functions f : X → R. With the help of a standard result from measure theory, see
e.g. [1, Theorem 29.14], it is then easy to conclude that these RKHS are also dense in all L1 (µ) for
which µ has a compact support. This key result has been extended in a couple of different directions:
For example, [18] establishes universality for more classes of kernels on compact X ⊂ Rd , whereas
[32] shows the denseness of the Gaussian RKHSs in L1 (µ) for all distributions µ on Rd . Finally,
[7, 8, 28, 29] show that universal kernels are closely related to so-called characteristic kernels that
can be used to distinguish distributions. In addition, all these papers contain sufﬁcient or necessary
conditions for universality of kernels on arbitrary compact metric spaces X , and [32] further shows
that the compact metric spaces are exactly the compact topological spaces on which there exist
universal spaces.
Unfortunately, however, it appears that neither the sufﬁcient conditions for universality nor the proof
of the existence of universal kernels can be used to construct universal kernels on compact metric
spaces X (cid:54)⊂ Rd . In fact, to the best of our knowledge, no explicit example of such kernels has so far
been presented. As a consequence, it seems fair to say that, beyond the X ⊂ Rd -case, the theory of
SVMs is incomplete, which is in contrast to the obvious practical success of SVMs for such input
spaces X as illustrated above.
The goal of this paper is to close this gap by providing the ﬁrst explicit and constructive examples
of universal kernels that live on compact metric spaces X (cid:54)⊂ Rd . To achieve this, our ﬁrst step is to
extend the deﬁnition of the Gaussian RBF kernels, or more generally, kernels that can be expressed
by a Taylor series, from the Euclidean Rd to its inﬁnite dimensional counter part, that is, the space
(cid:96)2 of square summable sequences. Unfortunately, on the space (cid:96)2 we face new challenges due to
its inﬁnite dimensional nature. Indeed, the closed balls of (cid:96)2 are no longer (norm)-compact subsets
of (cid:96)2 and hence we cannot expect universality on these balls. To address this issue, one may be
tempted to use the weak∗ -topology on (cid:96)2 , since in this topology the closed balls are both compact
and metrizable, thus universal kernels do exist on them. However, the Taylor kernels do not belong
to them, because –basically– the inner product (cid:104) · , · (cid:105)(cid:96)2 fails to be continuous with respect to the
weak∗ -topology as the sequence of the standard orthonormal basis vectors show. To address this
compactness issue we consider (norm)-compact subsets of (cid:96)2 , only. Since the inner product of (cid:96)2 is
continuous with respect to the norm by virtue of the Cauchy-Schwarz inequality, it turns out that the
Taylor kernels are continuous with respect to the norm topology. Moreover, we will see that in this
situation the Stone-Weierstraß-argument of [30] yields a variety of universal kernels including the
inﬁnite dimensional extensions of the Gaussian RBF kernels.
However, unlike the ﬁnite dimensional Euclidean spaces Rd and their compact subsets, the compact
subsets of (cid:96)2 can be hardly viewed as somewhat natural examples of input spaces X . Therefore,
we go one step further by considering compact metric spaces X for which there exist a separable
Hilbert space H and an injective and continuous map ρ : X → H. If, in this case, we ﬁx an analytic
function K : R → R that can be globally expressed by its Taylor series developed at zero and
that has strictly positive Taylor coefﬁcients, then k(x, x(cid:48) ) := K ((cid:104)ρ(x), ρ(x(cid:48) )(cid:105)H ) deﬁnes a universal
kernel on X and the same is true for the analogous deﬁnition of Gaussian kernels. Although this
situation may look at a ﬁrst glance even more artiﬁcial than the (cid:96)2 -case, it turns out that quite a few
interesting explicit examples can be derived from this situation. Indeed, we will use this general
result to present examples of Gaussian kernels deﬁned on the set of distributions over some input
space Ω and on certain sets of functions.

2

The paper has the following structure. Section 2 contains the main results and constructs examples
for universal kernels based on our technique. In particular, we show how to construct universal
kernels on sets of probability measures and on sets of functions, the latter being interesting for
signal processing. Section 3 contains a short discussion and Section 4 gives the proofs of the main
results.

K (t) =

t ∈ [−r, r] .

2 Main result
A kernel k on a set X is a function k : X × X → R for which all matrices of the form
i,j=1 , n ∈ N, x1 , . . . , xn ∈ X , are symmetric and positive semi-deﬁnite. Equiva-
(k(xi , xj ))n
lently, k is a kernel if and only there exists a Hilbert space ˜H and a map ˜Φ : X → ˜H such
that k(x, x(cid:48) ) = (cid:104) ˜Φ(x), ˜Φ(x(cid:48) )(cid:105) ˜H for all x, x(cid:48) ∈ X . While neither ˜H or ˜Φ are uniquely determined,
H := (cid:8)(cid:104)v , Φ( · )(cid:105) ˜H : v ∈ ˜H (cid:9)
the so-called reproducing kernel Hilbert space (RKHS) of k , which is given by
and (cid:107)f (cid:107)H := inf {(cid:107)v(cid:107) ˜H : f = (cid:104)v , Φ( · )(cid:105) ˜H } is uniquely determined, see e.g. [31, Chapter 4.2]. For
more information on kernels, we refer to [31, Chapter 4]. Moreover, for a compact metric space
(X, d), we write C (X ) := {f : X → R | f continuous} for the space of continuous functions on X
and equip this space with the usual supremum norm (cid:107) · (cid:107)∞ . A kernel k on X is called universal, if
k is continuous and its RKHS H is dense in C (X ). As mentioned before, this notion, which goes
back to [30], plays a key role in the analysis of kernel-based learning methods. Let r ∈ (0, ∞].
The kernels we consider in this paper are constructed by functions K : [−r, r] → R that can be
∞(cid:88)
expressed by its Taylor series, that is
an tn ,
n=0
∞(cid:88)
For such functions [31, Lemma 4.8] showed that
x, x(cid:48) ∈ √
k(x, x(cid:48) ) := K ((cid:104)x, x(cid:48) (cid:105)Rd ) =
an (cid:104)x, x(cid:48) (cid:105)n
Rd ,
rBRd ,
rBRd := {x ∈ Rd : (cid:107)x(cid:107)2 ≤ √
√
√
n=0
r} with radius
r , whenever all
deﬁnes a kernel on the closed ball
Taylor coefﬁcients an are non-negative. Following [31], we call such kernels Taylor kernels. [30],
see also [31, Lemma 4.57], showed that Taylor kernels are universal, if an > 0 for all n ≥ 0, while
:= (cid:80)
[21] notes that strict positivity on certain subsets of indices n sufﬁces.
Obviously, the deﬁnition (2) of k is still possible, if one replaces Rd by its inﬁnite dimensional and
separable counterpart (cid:96)2 := {(wj )j≥1 : (cid:107)(wj )(cid:107)2
j < ∞}. Let us denote the closed
j≥1 w2
(cid:96)2
unit ball in (cid:96)2 by B(cid:96)2 , or more generally, the closed unit ball of a Banach space E by BE , that is
BE := {v ∈ E : (cid:107)v(cid:107)E ≤ 1}. Our ﬁrst main result shows that this extension leads to a kernel, whose
restrictions to compact subsets are universal, if an > 0 for all n ∈ N0 := N ∪ {0}.
Theorem 2.1 Let K : [−r, r] → R be a function of the form (1). Then we have:
√
rB(cid:96)2 × √
∞(cid:88)
i) If an ≥ 0 for all n ≥ 0, then k :
rB(cid:96)2 → R is a kernel, where
(cid:1) =
k(w, w (cid:48) ) := K (cid:0)(cid:104)w, w (cid:48) (cid:105)(cid:96)2
w, w (cid:48) ∈ √
an (cid:104)w, w (cid:48) (cid:105)n
(cid:96)2
n=0
ii) If an > 0 for all n ∈ N0 , then the restriction k|W ×W : W × W → R of k to an arbitrary
compact set W ⊂ √
rB(cid:96)2 is universal.
To consider a ﬁrst explicit example, let K := exp : R → R be the exponential function. Then
K clearly satisﬁes the assumptions of Theorem 2.1 for all r > 0, and hence the resulting exponen-
tial kernel is universal on every compact subset W of (cid:96)2 . Moreover, for σ ∈ (0, ∞), the related
(cid:1) =
kσ (w, w (cid:48) ) := exp(cid:0)−σ2(cid:107)w − w (cid:48)(cid:107)2
Gaussian-type RBF kernel kσ : (cid:96)2 × (cid:96)2 → R deﬁned by
exp(2σ2 (cid:104)w, w (cid:48) (cid:105)(cid:96)2 )
) exp(σ2 (cid:107)w (cid:48)(cid:107)2
exp(σ2 (cid:107)w(cid:107)2
(cid:96)2
(cid:96)2
(cid:96)2

(1)

(2)

(4)

,

rB(cid:96)2 .

(3)

)

3

an

(5)

x, x(cid:48) ∈ X.

is also universal on every compact W ⊂ (cid:96)2 , since modulo the scaling by σ it is the normalized
version of the exponential kernel, and thus it is universal by [31, Lemma 4.55].
Although we have achieved our ﬁrst goal, namely explicit, constructive examples of universal ker-
nels on X (cid:54)⊂ Rd , the result is so far not really satisfying. Indeed, unlike the ﬁnite dimensional
Euclidean spaces Rd , the inﬁnite dimensional space (cid:96)2 rarely appears as the input space in real-
world applications. The following second result can be used to address this issue.
Theorem 2.2 Let X be a compact metric space and H be a separable Hilbert space such that there
exists a continuous and injective map ρ : X → H. Furthermore, let K : R → R be a function of
the form (1). Then the following statements hold:
i) If an ≥ 0 for all n ∈ N0 , then k : X × X → R deﬁnes a kernel, where
∞(cid:88)
(cid:1) =
k(x, x(cid:48) ) := K (cid:0)(cid:10)ρ(x), ρ(x(cid:48) )(cid:11)
(cid:10)ρ(x), ρ(x(cid:48) )(cid:11)n
H ,
H
n=0
ii) If an > 0 for all n ∈ N0 , then k is a universal kernel.
kσ (x, x(cid:48) ) := exp(cid:0)−σ2(cid:107)ρ(x) − ρ(x(cid:48) )(cid:107)2H (cid:1) ,
iii) For σ > 0, the Gaussian-type RBF-kernel kσ : X × X → R is a universal kernel, where
x, x(cid:48) ∈ X.
(6)
positive non-constant radial basis function kernels such as kσ (x, x(cid:48) ) := exp(cid:0)−σ2 (cid:107)ρ(x) − ρ(x(cid:48) )(cid:107)H (cid:1)
or the Student-type RBF kernels kσ (x, x(cid:48) ) := (cid:0)1 + σ2(cid:107)ρ(x) − ρ(x(cid:48) )(cid:107)2H
(cid:1)−α for σ2 > 0 and α ≥ 1.
It seems possible that the latter result for the Gaussian-type RBF kernel can be extended to other
Indeed, [25] uses the fact that on Rd such kernels have an integral representation in terms of the
Gaussian RBF kernels to show, see [25, Corollary 4.9], that these kernels inherit approximation
properties such as universality from the Gaussian RBF kernels. We expect that the same arguments
can be made for (cid:96)2 and then, in a second step, for the situation of Theorem 2.2.
Before we provide some examples of situations in which Theorem 2.2 can be used to deﬁne explicit
universal kernels, we point to a technical detail of Theorem 2.2, which may be overseen, thus leading
to wrong conclusions.
To this end, let (X, dX ) be an arbitrary metric space, H be a separable Hilbert space and ρ : X → H
be an injective map. We write V := ρ(X ) and equip this space with the metric deﬁned by H.
Thus, ρ : X → V is bijective by deﬁnition. Moreover, since H is assumed to be separable, it is
isometrically isomorphic to (cid:96)2 , and hence there exists an isometric isomorphism I : H → (cid:96)2 . We
write W := I (V ) and equip this set with the metric deﬁned by the norm of (cid:96)2 . For a function
f : W → R, we can then consider the following diagram
f ◦ I ◦ ρ

(X, dX )
6

ρ

-

(R, | · |)
6
f

?
(V , (cid:107) · (cid:107)H )



-

(W, (cid:107) · (cid:107)(cid:96)2 )
(7)
I
Since both ρ and I are bijective, it is easy to see that f not only deﬁnes a function g : X → R
by g := f ◦ I ◦ ρ, but conversely, every function g : X → R has such a representation and this
representation is unique. In other words, there is a one-to-one relationship between the functions
X → R and the functions W → R. Let us now assume that we have a kernel kW on W with RKHS
HW and canonical feature map ΦW : W → HW . Then kX : X × X → R, given by
x, x(cid:48) ∈ X,
kX (x, x(cid:48) ) := kW (I ◦ ρ(x), I ◦ ρ(x(cid:48) )) ,
deﬁnes a kernel on X , since
kX (x, x(cid:48) ) = kW (I ◦ ρ(x), I ◦ ρ(x(cid:48) )) = (cid:104)ΦW (I (ρ(x(cid:48) ))), ΦW (I (ρ(x)))(cid:105)HW ,

x, x(cid:48) ∈ X,

4

shows that ΦW ◦ I ◦ ρ : X → HW is a feature map of kX . Moreover, [31, Theorem 4.21] shows
HX = (cid:8)(cid:104)f , ΦW ◦ I ◦ ρ( · )(cid:105)HW : f ∈ HW
(cid:9) .
that the RKHS HX of kX is given by
Since, for f ∈ HW , the reproducing property of HW gives f ◦ I ◦ ρ(x) = (cid:104)f , ΦW ◦ I ◦ ρ(x)(cid:105)HW
for all x ∈ X we thus conclude that HX = {f ◦ I ◦ ρ : f ∈ HW } =: HW ◦ I ◦ ρ. Let us
now assume that X is compact and that kW is one of the universal kernels considered in Theorem
2.1 or the Gaussian RBF kernel (4). Then the proof of Theorem 2.2 shows that kX is one of the
universal kernels considered in Theorem 2.2. Moreover, if we consider the kernel kV : V × V → R
deﬁned by kV (v , v (cid:48) ) := kW (I (v), I (v (cid:48) )), then an analogous argument shows that kV is a universal
kernel. This raises the question, whether we need the compactness of X , or whether it sufﬁces
to assume that ρ is injective, continuous and has a compact image V . Surprisingly, the answer is
that it depends on the type of universality one needs. Indeed, if ρ is as in Theorem 2.2, then the
compactness of X ensures that ρ is a homeomorphism, that is, ρ−1 : V → X is continuous, too.
Since I is clearly also a homeomorphism, we can easily conclude that C (X ) = C (W ) ◦ I ◦ ρ,
that is, we have the same relationship as we have for the RKHSs HW and HX . From this, the
universality is easy to establish. Let us now assume the compactness of V instead of the compactness
of X . Then, in general, ρ is not a homeomorphism and the sets of continuous functions on X
and V are in general different, even if we consider the set of bounded continuous functions on X .
To see the latter, consider e.g. the map ρ : [0, 1) → S 1 onto the unit sphere S 1 of R2 deﬁned
by ρ(t) := (sin(2πt), cos(2πt)). Now this difference makes it impossible to conclude from the
universality of kV (or kW ) to the universality of kX . However, if τV denotes the topology of V ,
then ρ−1 (τV ) := {ρ−1 (O) : O ∈ τV } deﬁnes a new topology on X , which satisﬁes ρ−1 (τV ) ⊂ τX .
Consequently, there are, in general, fewer continuous functions with respect to ρ−1 (τV ). Now, it
is easy to check that dρ (x, x(cid:48) ) := (cid:107)ρ(x) − ρ(x(cid:48) )(cid:107)H deﬁnes a metric that generates ρ−1 (τV ) and,
since ρ is isometric with respect to this new metric, we can conclude that (X, dρ ) is a compact
metric space. Consequently, we are back in the situation of Theorem 2.2, and hence kX is universal
with respect to the space C (X, dρ ) of functions X → R that are continuous with respect to dρ .
In other words, while HX may fail to approximate every function that is continuous with respect
to dX , it does approximate every function that is continuous with respect to dρ . Whether the latter
approximation property is enough clearly depends on the speciﬁc application at hand.
Let us now present some universal kernels of practical interest. Please note, that although the func-
tion ρ in our examples is even linear, the Theorem 2.2 only assumes ρ to be continuous and injective.
We start with two examples where X is the set of distributions on some space Ω.
Example 1: universal kernels on the set of probability measures.
Let (Ω, dΩ ) be a compact metric space, B(Ω) be its Borel σ -algebra, and X := M1 (Ω) be the set of
dX (P, P(cid:48) ) := inf (cid:8)ε > 0 : P(A) ≤ P(cid:48) (Aε ) + ε for all A ∈ B(Ω)(cid:9),
all Borel probability measures on Ω. Then the topology describing weak convergence of probability
measures can be metrized, e.g., by the Prohorov metric
P, P(cid:48) ∈ X ,
(8)
where Aε := {ω (cid:48) ∈ Ω : dΩ (ω , ω (cid:48) ) < ε for some ω ∈ A}, see e.g. [2, Theorem 6.8, p. 73].
Moreover, (X, dX ) is a compact metric space if and only if (Ω, dΩ ) is a compact metric space, see
[19, Thm. 6.4]. In order to construct universal kernels on (X, dX ) with the help of Theorem 2.2,
it thus remains to ﬁnd separable Hilbert spaces H and injective, continuous embeddings ρ : X →
H. Let kΩ be a continuous kernel on Ω with RKHS HΩ and canonical feature map ΦΩ (ω) :=
kΩ (ω , ·), ω ∈ Ω. Note that kΩ is bounded because it is continuous and Ω is compact. Then HΩ
is separable and ΦΩ is bounded and continuous, see [31, Lemmata 4.23, 4.29, 4.33]. Assume that
kΩ is additionally characteristic, i.e. the function ρ : X → HΩ deﬁned by the Bochner integral
ρ(P) := EPΦΩ is injective. Then the next lemma, which is taken from [10, Thm. 5.1] and which is
a modiﬁcation of a theorem in [3, p. III. 40], ensures the continuity of ρ.
Lemma 2.3 Let (Ω, dΩ ) be a complete separable metric space, H be a separable Banach space and
Φ : Ω → H be a bounded, continuous function. Then ρ : M1 (Ω) → H deﬁned by ρ(P) := EPΦ is
continuous, i.e., EPn Φ → EPΦ, whenever (Pn )n∈N ⊂ M1 (Ω) converges weakly in M1 (Ω) to P.
Consequently, the map ρ : M1 (Ω) → HΩ satisﬁes the assumptions of Theorem 2.2, and hence the
(cid:1) , P, P(cid:48) ∈ M1 (Ω),
kσ (P, P(cid:48) ) := exp (cid:0)−σ2(cid:107)EPΦΩ − EP(cid:48) ΦΩ (cid:107)2HΩ
Gaussian-type RBF kernel
5

(9)

is universal and obviously bounded. Note that this kernel is conceptionally different to characteristic
kernels on Ω. Indeed, characteristic kernels live on Ω and their RKHS consist of functions Ω →
R, while the new kernel kσ lives on M1 (Ω) and its RKHS consists of functions M1 (Ω) → R.
Consequently, kσ can be used to learn from samples that are individual distributions, e.g. represented
by histograms, densities or data, while characteristic kernels can only be used to check whether two
of such distributions are equal or not.
(or characteristic function), that is ρ(P) := ˆP, where ˆP(t) := (cid:82) ei(cid:104)z ,t(cid:105)dµ(z ) ∈ C, t ∈ Rd . It is
Example 2: universal kernels based on Fourier transforms of probability measures.
Consider, the set X := M1 (Ω), where Ω ⊂ Rd is compact. Moreover, let ρ be the Fourier transform
well-known, see e.g. [6, Chap. 9], that, for all P ∈ M1 (Ω), ˆP is uniformly continuous on Rd and
(cid:107) ˆP(cid:107)∞ ≤ 1. Moreover, ρ : P (cid:55)→ ˆP is injective, and if a sequence (Pn ) converges weakly to some
P, then ( ˆPn ) converges uniformly to ˆP on every compact subset of Rd . Now let µ be a ﬁnite
Borel measure on Rd with support(µ) = Rd , e.g., µ can be any probability distribution on Rd with
Lebesgue density h > 0. Then the previous properties of the Fourier transform can be used to
show that ρ : M1 (Ω) → L2 (µ) is continuous, and hence Theorem 2.2 ensures that the following
kσ (P, P(cid:48) ) := exp(cid:0)−σ2(cid:107) ˆP − ˆP(cid:48)(cid:107)2
(cid:1) ,
Gaussian-type kernel is universal and bounded:
P, P(cid:48) ∈ M1 (Ω).
L2 (µ)
In view of the previous two examples, we mention that the probability measures P and P(cid:48) are often
not directly observable in practice, but only corresponding empirical distributions can be obtained.
In this case, a simple standard technique is to construct histograms to represent these empirical distri-
butions as vectors in a ﬁnite-dimensional Euclidean space, although it is well-known that histograms
can yield bad estimates for probability measures. Our new kernels make it possible to directly plug
the empirical distributions into the kernel kσ , even if these distributions do not have the same length.
Moreover, other techniques to convert empirical distributions to absolutely continuous distributions
such as kernel estimators derived via weighted averaging of rounded points (WAPRing) and (averag-
ing) histograms with different origins, [20, 24] can be used in kσ , too. Clearly, the preferred method
will most likely depend on the speciﬁc application at hand, and one beneﬁt of our construction is
that it allows this ﬂexibility.
Example 3: universal kernels for signal processing.
Let (Ω, A, µ) be an arbitrary measure space and L2 (µ) be the usual space of square µ-integrable
functions on Ω. Let us additionally assume that L2 (µ) is separable, which is typically, but not
always, satisﬁed. In addition, let us assume that our input values xi ∈ X are functions taken from
some compact set X ⊂ L2 (µ). A typical example, where this situation occurs, is signal processing,
where the true signal f ∈ L2 ([0, 1]), which is a function of time, cannot be directly observed, but a
smoothed version g := T ◦ f of the signal is observable. This smoothing can often be described by a
compact linear operator T : L2 ([0, 1]) → L2 ([0, 1]), e.g., a convolution operator, acting on the true
signals. Hence, if we assume that the true signals are contained in the closed unit ball BL2 ([0,1]) ,
then the observed, smoothed signals T ◦ f are contained in a compact subset X of L2 ([0, 1]). Let
us now return to the general case introduced above. Then the identity map ρ := id : X → L2 (µ)
kσ (g , g (cid:48) ) := exp(cid:0)−σ2 (cid:107)g − g (cid:48)(cid:107)2
(cid:1) ,
satisﬁes the assumptions of Theorem 2.2, and hence the Gaussian-type kernel
g , g (cid:48) ∈ X,
L2 (µ)
deﬁnes a universal and bounded kernel on X . As in the previous examples, note that the computation
of kσ does not require the functions g and g (cid:48) to be in a speciﬁc format such as a certain discretization.

(11)

(10)

3 Discussion

The main goal of this paper was to provide an explicit construction of universal kernels that are
deﬁned on arbitrary compact metric spaces, which are not necessarily a subset of Rd . There is a
still increasing interest in kernel methods including support vector machines on such input spaces,
e.g. for classiﬁcation or regression purposes for input values being probability measures, histograms
or colored images. As examples, we gave explicit universal kernels on the set of probability distri-
butions and for signal processing. One direction of further research may be to generalize our results
to the case of non-compact metric spaces or to ﬁnd quantitative approximation results.

6

4 Proofs

˜cj

cj

bji
i

cj

bji
i

.

bi

=

bi

=

bji
i

.

0 for the set of all sequences (ji )i≥1 with values in N0 := N ∪ {0}.
In the following, we write NN
Elements of this set will serve us as multi-indices with countably many components. For j = (ji ) ∈
(cid:88)
NN
0 , we will therefore adopt the multi-index notation
|j | :=
ji .
i≥1
Note that |j | < ∞ implies that j has only ﬁnitely many components ji with ji (cid:54)= 0.
Lemma 4.1 Assume that n ∈ N is ﬁxed and that for all j ∈ NN
0 with |j | = n, we have some
constant cj ∈ (0, ∞). Then for all j ∈ NN
0 with |j | = n + 1, there exists a constant ˜cj ∈ (0, ∞)
(cid:19)(cid:18) ∞(cid:88)
(cid:18) (cid:88)
(cid:19)
∞(cid:89)
(cid:88)
∞(cid:89)
such that for all summable sequences (bi ) ⊂ [0, ∞) we have
j∈NN
0 :|j |=n+1
j∈NN
0 :|j |=n
i=1
i=1
i=1
Proof: This can be shown by induction, where the induction step is similar to the proof for the
Cauchy product of series.
Lemma 4.2 Assume that n ∈ N0 is ﬁxed. Then for all j ∈ NN
0 with |j | = n, there exists a constant
(cid:18) ∞(cid:88)
(cid:19)n
∞(cid:89)
(cid:88)
cj ∈ (0, ∞) such that for all summable sequences (bi ) ⊂ [0, ∞) we have
j∈NN
0 :|j |=n
i=1
i=1
2 := (cid:80)
Proof: This can be shown by induction using Lemma 4.1.
Given a non-empty countable set J and a family w := (wj )j∈J ⊂ R, we write (cid:107)w(cid:107)2
j ,
j∈J w2
and, as usual, we denote the space of all families for which this quantity is ﬁnite by (cid:96)2 (J ). Recall
that (cid:96)2 (J ) together with (cid:107) · (cid:107)2 is a Hilbert space and we denote its inner product by (cid:104) · , · (cid:105)(cid:96)2 (J ) .
Moreover, (cid:96)2 := (cid:96)2 (N) is separable, and by using an orthonormal basis representation, it is further
known that every separable Hilbert space is isometrically isomorphic to (cid:96)2 . In this sense, (cid:96)2 can be
viewed as a generic model for separable Hilbert spaces.
The following result provides a method to construct Taylor kernels on closed balls in (cid:96)2 .
Taylor series given in (1), i.e. K (t) = (cid:80)∞
Proposition 4.3 Let r ∈ (0, ∞] and K : [−r, r] → R be a function that can be expressed by its
n=0 an tn , t ∈ [−r, r]. Deﬁne J := {j ∈ NN
0 : |j | < ∞}.
rB(cid:96)2 × √
√
∞(cid:88)
(cid:1) =
k(w, w (cid:48) ) := K (cid:0)(cid:104)w, w (cid:48) (cid:105)(cid:96)2
rB(cid:96)2 → R deﬁned by (3), i.e.
If an ≥ 0 for all n ≥ 0, then k :
w, w (cid:48) ∈ √
an (cid:104)w, w (cid:48) (cid:105)n
,
rB(cid:96)2 ,
(cid:96)2
n=0
is a kernel. Moreover, for all j ∈ J , there exists a constant cj ∈ (0, ∞) such that Φ :
(cid:17)
(cid:16)
∞(cid:89)
(cid:96)2 (J ) deﬁned by
w ∈ √
j∈J
i=1
is a feature map of k , where we use the convention 00 := 1.
Proof: For w, w (cid:48) ∈ √
rB(cid:96)2 , the Cauchy-Schwarz inequality yields |(cid:104)w, w (cid:48) (cid:105)| ≤ (cid:107)w(cid:107)2(cid:107)w (cid:48)(cid:107)2 ≤ r
and thus k is well-deﬁned. Let wi denote the i-th component of w ∈ (cid:96)2 . Since (1) is absolutely
0 , there exists a constant ˜cj ∈ (0, ∞) such
convergent, Lemma 4.2 then shows that, for all j ∈ NN
(cid:88)
∞(cid:89)
∞(cid:89)
that
k(w, w (cid:48) ) =
(w (cid:48)
Setting cj := (cid:112)a|j | ˜cj , we obtain that Φ deﬁned in (12) is indeed a feature map of k , and hence k is
wji
i )ji
a|j | ˜cj
.
i
j∈NN
i=1
i=1
0
a kernel.

rB(cid:96)2 →

Φ(w) :=

rB(cid:96)2 ,

(12)

√

cj

wji
i

,

7

Before we can state our ﬁrst main result the need to recall the following test of universality from
[31, Theorem 4.56].

Theorem 4.4 Let W be a compact metric space and k be a continuous kernel on W with k(w, w) >
0 for all w ∈ W . Suppose that we have an injective feature map Φ : W → (cid:96)2 (J ) of k , where J
is some countable set. We write Φj : W → R for its j -th component, i.e., Φ(w) = (Φj (w))j∈J ,
w ∈ W . If A := span {Φj : j ∈ J } is an algebra, then k is universal.

k(w, w) =

With the help of Theorem 4.4 and Proposition 4.3 we can now prove our ﬁrst main result.
√
now ﬁx a compact W ⊂ √
Proof of Theorem 2.1: We have already seen in Proposition 4.3 that k is a kernel on
rB(cid:96)2 . Let us
rB(cid:96)2 . For every j ∈ J , where J is deﬁned in Proposition 4.3, there are
only ﬁnitely many components ji with ji (cid:54)= 0. Consequently, there exists a bijection between J and
∞(cid:88)
the set of all ﬁnite subsets of N. Since the latter is countable, J is countable. Furthermore, we have
an(cid:107)w(cid:107)2n
≥ a0 > 0
(cid:96)2
n=0
for all w ∈ W , and it is obvious, that the components of the feature map Φ found in Proposition
4.3 span an algebra. Finally, if we have w, w (cid:48) ∈ W with w (cid:54)= w (cid:48) , there exists an i ≥ 1 such that
wi (cid:54)= w (cid:48)
i . For the multi-index j ∈ J that equals 1 at the i-component and vanishes everywhere else
we then have Φ(w) = cj wi (cid:54)= cj w (cid:48)
i = Φ(w (cid:48) ), and hence Φ is injective.
Proof of Theorem 2.2: Since H is separable Hilbert space there exists an isometric isomorphism
I : H → (cid:96)2 . We deﬁne V := ρ(X ), see also the diagram in (7). Since ρ is continuous, V is
the image of a compact set under a continuous map, and thus V is compact and the inverse of
the bijective map I ◦ ρ : X → W is continuous. Consequently, there is a one-to-one relationship
between the continuous functions fX on X and the continuous functions fW on W , namely C (X ) =
C (W ) ◦ I ◦ ρ, see also the discussion following (7). Moreover, the fact that I : H → (cid:96)2 is an
isometric isomorphism yields (cid:104)I (ρ(x)), I (ρ(x(cid:48) ))(cid:105)(cid:96)2 = (cid:104)ρ(x), ρ(x(cid:48) )(cid:105)H for all x, x(cid:48) ∈ X , and hence
the kernel k considered in Theorem 2.2 is of the form kX = kW (I ◦ ρ( · ), I ◦ ρ( · )), where kW
is the corresponding kernel deﬁned on W ⊂ (cid:96)2 considered in Theorem 2.2. Now, the discussion
following (7) showed HX = HW ◦ I ◦ ρ. Consequently, if we ﬁx a function g ∈ C (X ), then
f := g ◦ ρ−1 ◦ I −1 ∈ C (W ) can be approximated by HW , that is, for all ε > 0, there exists an
h ∈ HW such that (cid:107)h − f (cid:107)∞ ≤ ε. Since I ◦ ρ : X → W is bijective and f ◦ I ◦ ρ = g , we conclude
that (cid:107)h ◦ I ◦ ρ − g(cid:107)∞ ≤ ε. Now the assertion follows from h ◦ I ◦ ρ ∈ HX .

References
[1] H. Bauer. Measure and Integration Theory. De Gruyter, Berlin, 2001.
[2] P. Billingsley. Convergence of probability measures. John Wiley & Sons, New York, 2nd edition, 1999.
[3] N. Bourbaki. Integration I. Chapters 1-6. Springer, Berlin, 2004. Translated from the 1959, 1965, and
1967 French originals by S.K. Berberian.
[4] A. Caponnetto, C.A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. J. Mach. Learn.
Res., 9:1615–1646, 2008.
[5] O. Chapelle, P. Haffner, and V. Vapnik. SVMs for histogram-based image classiﬁcation. IEEE Transac-
tions on Neural Networks, 10:1055–1064, 1999.
[6] R. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.
[7] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with repro-
ducing kernel hilbert spaces. J. Mach. Learn. Res., 5:73–99, 2005.
[8] K. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel Dimension Reduction in Regression. Ann. Statist.,
37:1871–1905, 2009.
[9] K. Fukumizu, B. K. Sriperumbudur, A. Gretton, and B. Sch ¨olkopf. Characteristic kernels on groups
and semigroups. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21, pages 473–480. 2009.

8

[10] R. Hable and A. Christmann. Qualitative robustness of support vector machines. arXiv:0912.0874v1,
2009.
[11] M. Hein and O. Bousquet. Kernels, associated structures and generalizations. Technical report, Max-
Planck-Institute for Biological Cybernetics, 2004.
[12] M. Hein and O. Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability measures. In
Z. Ghahramani and R. Cowell, editors, AISTATS, pages 136–143, 2005.
[13] M. Hein, O. Bousquet, and B. Sch ¨olkopf. Maximal margin classiﬁcation for metric spaces. Journal of
Computer and System Sciences, 71:333–359, 2005.
[14] M. Hein, T. N. Lal, and O. Bousquet. Hilbertian metrics on probability measures and their application in
SVM’s. In C. E. Rasmussen, H. H. B ¨ulthoff, M. Giese, and B. Sch ¨olkopf, editors, Pattern Recognition,
Proceedings of the 26th DAGM Symposium, pages 270–277, Berlin, 2004. Springer.
[15] T. Joachims. Learning to Classify Text Using Support Vector Machines. Kluwer Academic Publishers,
Boston, 2002.
[16] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. J. Mach. Learn. Res., 6:129–163,
2005.
[17] A.F.T. Martins, N.A. Smith, E.P. Xing, P.M.Q. Aguiar, and M.A.T. Figueiredo. Nonextensive information
theoretic kernels on measures. J. Mach. Learn. Res., 10:935–975, 2009.
[18] C. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. J. Mach. Learn. Res., 7:2651–2667, 2006.
[19] K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, New York, 1967.
[20] E. Parzen. On estimating of a probability density and mode. Ann. Math. Statist., 35:1065–1076, 1962.
[21] A. Pinkus. Strictly positive deﬁnite functions on a real inner product space. Adv. Comput. Math., 20:263–
271, 2004.
[22] B. Sch ¨olkopf, A. J. Smola, and K.-R. M ¨uller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Comput., 10:1299–1319, 1998.
[23] B. Sch ¨olkopf, K. Tsuda, and J. P. Vert. Kernel Methods in Computational Biology. MIT Press, Cambridge,
MA, 2004.
[24] D. Scott. Averaged shifted histograms: Effective nonparametric density estimation in several dimensions.
Ann. Statist., 13:1024–1040, 1985.
[25] C. Scovel, D. Hush, I. Steinwart, and J. Theiler. Radial kernels and their reproducing kernel Hilbert
spaces. Journal of Complexity, 2010, to appear.
[26] A.J. Smola, A. Gretton, L. Song, and B. Sch ¨olkopf. A Hilbert Space Embedding for Distributions. In
E. Takimoto, editor, Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer, 2007.
Proceedings of the 10th International Conference on Discovery Science, 40-41.
[27] B. Sriperumbudur, K. Fukumizu, A. Gretton, G. Lanckriet, and B. Sch ¨olkopf. Kernel choice and clas-
siﬁability for RKHS embeddings of probability distributions. In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages
1750–1758. 2009.
[28] B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. On the relation between universality, characteristic
kernels and RKHS embeddings of measures. In Yee Whye Teh and M. Titterington, editors, AISTATS
2010, Proc. of the 13th International Conference on Artiﬁcial Intelligence and Statistics, volume 9, pages
773–780. 2010.
[29] B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, characteristic kernels and RKHS em-
beddings of measures. arXiv:1003.0887v1, 2010.
[30] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. J. Mach.
Learn. Res., 2:67–93, 2001.
[31] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York, 2008.
[32] I. Steinwart, D. Hush, and C. Scovel. Function classes that approximate the Bayes risk. In COLT’06, 19th
Conference on Learning Theory, Pittsburgh, 2006.

9


Universal Kernels on Non-Standard Input Spaces

Andreas Christmann
University of Bayreuth
Department of Mathematics
D-95440 Bayreuth
andreas.christmann@uni-bayreuth.de

Ingo Steinwart
University of Stuttgart
Department of Mathematics
D-70569 Stuttgart
ingo.steinwart@mathematik.uni-stuttgart.de

Abstract

During the last years support vector machines (SVMs) have been successfully ap-
plied in situations where the input space X is not necessarily a subset of Rd . Ex-
amples include SVMs for the analysis of histograms or colored images, SVMs for
text classiï¬cation and web mining, and SVMs for applications from computational
biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be
consistent to the Bayes risk, if either the input space is a complete separable metric
space and the reproducing kernel Hilbert space (RKHS) H âŠ‚ Lp (PX ) is dense,
or if the SVM uses a universal kernel k . So far, however, there are no kernels of
practical interest known that satisfy these assumptions, if X (cid:54)âŠ‚ Rd . We close this
gap by providing a general technique based on Taylor-type kernels to explicitly
construct universal kernels on compact metric spaces which are not subset of Rd .
We apply this technique for the following special cases: universal kernels on the
set of probability measures, universal kernels based on Fourier transforms, and
universal kernels for signal processing.

1

Introduction

For more than a decade, kernel methods such as support vector machines (SVMs) have belonged
to the most successful learning methods. Besides several other nice features, one key argument
for using SVMs has been the so-called â€œkernel trickâ€ [22], which decouples the SVM optimization
problem from the domain of the samples, thus making it possible to use SVMs on virtually any input
space X . This ï¬‚exibility is in strong contrast to more classical learning methods from both machine
learning and non-parametric statistics, which almost always require input spaces X âŠ‚ Rd . As a
result, kernel methods have been successfully used in various application areas that were previously
infeasible for machine learning methods. The following, by no means exhaustive, list illustrates this:
â€¢ SVMs processing probability measures, e.g. histograms, as input samples have been used to an-
alyze histogram data such as colored images, see [5, 11, 14, 12, 27, 29], and also [17] for non-
extensive information theoretic kernels on measures.
â€¢ SVMs for text classiï¬cation and web mining [15, 12, 16],
â€¢ SVMs with kernels from computational biology, e.g. kernels for trees and graphs [23].
In addition, several extensions or generalizations of kernel-methods have been considered, see
e.g. [13, 26, 9, 16, 7, 8, 4]. Besides their practical success, SVMs nowadays also possess a rich

1

statistical theory, which provides various learning guarantees, see [31] for a recent account. In-
terestingly, in this analysis, the kernel and its reproducing kernel Hilbert space (RKHS) make it
possible to completely decouple the statistical analysis of SVMs from the input space X . For ex-
ample, if one uses the hinge loss and a bounded measurable kernel whose RKHS H is separable
and dense in L1 (Âµ) for all distributions Âµ on X , then [31, Theorem 7.22] together with [31, Theo-
rem 2.31] and the discussion on [31, page 267ff] shows that the corresponding SVM is universally
classiï¬cation consistent even without an entropy number assumption if one picks a sequence (Î»n )
of positive regularization parameters that satisfy Î»n â†’ 0 and nÎ»n/ ln n â†’ âˆ. In other words,
independently of the input space X , the universal consistency of SVMs is well-understood modulo
an approximation theoretical question, namely that of the denseness of H in all L1 (Âµ).
For standard input spaces X âŠ‚ Rd and various classical kernels, this question of denseness has been
positively answered. For example, for compact X âŠ‚ Rd , [30] showed that, among a few others,
the RKHSs of the Gaussian RBF kernels are universal, that is, they are dense in the space C (X )
of continuous functions f : X â†’ R. With the help of a standard result from measure theory, see
e.g. [1, Theorem 29.14], it is then easy to conclude that these RKHS are also dense in all L1 (Âµ) for
which Âµ has a compact support. This key result has been extended in a couple of different directions:
For example, [18] establishes universality for more classes of kernels on compact X âŠ‚ Rd , whereas
[32] shows the denseness of the Gaussian RKHSs in L1 (Âµ) for all distributions Âµ on Rd . Finally,
[7, 8, 28, 29] show that universal kernels are closely related to so-called characteristic kernels that
can be used to distinguish distributions. In addition, all these papers contain sufï¬cient or necessary
conditions for universality of kernels on arbitrary compact metric spaces X , and [32] further shows
that the compact metric spaces are exactly the compact topological spaces on which there exist
universal spaces.
Unfortunately, however, it appears that neither the sufï¬cient conditions for universality nor the proof
of the existence of universal kernels can be used to construct universal kernels on compact metric
spaces X (cid:54)âŠ‚ Rd . In fact, to the best of our knowledge, no explicit example of such kernels has so far
been presented. As a consequence, it seems fair to say that, beyond the X âŠ‚ Rd -case, the theory of
SVMs is incomplete, which is in contrast to the obvious practical success of SVMs for such input
spaces X as illustrated above.
The goal of this paper is to close this gap by providing the ï¬rst explicit and constructive examples
of universal kernels that live on compact metric spaces X (cid:54)âŠ‚ Rd . To achieve this, our ï¬rst step is to
extend the deï¬nition of the Gaussian RBF kernels, or more generally, kernels that can be expressed
by a Taylor series, from the Euclidean Rd to its inï¬nite dimensional counter part, that is, the space
(cid:96)2 of square summable sequences. Unfortunately, on the space (cid:96)2 we face new challenges due to
its inï¬nite dimensional nature. Indeed, the closed balls of (cid:96)2 are no longer (norm)-compact subsets
of (cid:96)2 and hence we cannot expect universality on these balls. To address this issue, one may be
tempted to use the weakâˆ— -topology on (cid:96)2 , since in this topology the closed balls are both compact
and metrizable, thus universal kernels do exist on them. However, the Taylor kernels do not belong
to them, because â€“basicallyâ€“ the inner product (cid:104) Â· , Â· (cid:105)(cid:96)2 fails to be continuous with respect to the
weakâˆ— -topology as the sequence of the standard orthonormal basis vectors show. To address this
compactness issue we consider (norm)-compact subsets of (cid:96)2 , only. Since the inner product of (cid:96)2 is
continuous with respect to the norm by virtue of the Cauchy-Schwarz inequality, it turns out that the
Taylor kernels are continuous with respect to the norm topology. Moreover, we will see that in this
situation the Stone-WeierstraÃŸ-argument of [30] yields a variety of universal kernels including the
inï¬nite dimensional extensions of the Gaussian RBF kernels.
However, unlike the ï¬nite dimensional Euclidean spaces Rd and their compact subsets, the compact
subsets of (cid:96)2 can be hardly viewed as somewhat natural examples of input spaces X . Therefore,
we go one step further by considering compact metric spaces X for which there exist a separable
Hilbert space H and an injective and continuous map Ï : X â†’ H. If, in this case, we ï¬x an analytic
function K : R â†’ R that can be globally expressed by its Taylor series developed at zero and
that has strictly positive Taylor coefï¬cients, then k(x, x(cid:48) ) := K ((cid:104)Ï(x), Ï(x(cid:48) )(cid:105)H ) deï¬nes a universal
kernel on X and the same is true for the analogous deï¬nition of Gaussian kernels. Although this
situation may look at a ï¬rst glance even more artiï¬cial than the (cid:96)2 -case, it turns out that quite a few
interesting explicit examples can be derived from this situation. Indeed, we will use this general
result to present examples of Gaussian kernels deï¬ned on the set of distributions over some input
space â„¦ and on certain sets of functions.

2

The paper has the following structure. Section 2 contains the main results and constructs examples
for universal kernels based on our technique. In particular, we show how to construct universal
kernels on sets of probability measures and on sets of functions, the latter being interesting for
signal processing. Section 3 contains a short discussion and Section 4 gives the proofs of the main
results.

K (t) =

t âˆˆ [âˆ’r, r] .

2 Main result
A kernel k on a set X is a function k : X Ã— X â†’ R for which all matrices of the form
i,j=1 , n âˆˆ N, x1 , . . . , xn âˆˆ X , are symmetric and positive semi-deï¬nite. Equiva-
(k(xi , xj ))n
lently, k is a kernel if and only there exists a Hilbert space ËœH and a map ËœÎ¦ : X â†’ ËœH such
that k(x, x(cid:48) ) = (cid:104) ËœÎ¦(x), ËœÎ¦(x(cid:48) )(cid:105) ËœH for all x, x(cid:48) âˆˆ X . While neither ËœH or ËœÎ¦ are uniquely determined,
H := (cid:8)(cid:104)v , Î¦( Â· )(cid:105) ËœH : v âˆˆ ËœH (cid:9)
the so-called reproducing kernel Hilbert space (RKHS) of k , which is given by
and (cid:107)f (cid:107)H := inf {(cid:107)v(cid:107) ËœH : f = (cid:104)v , Î¦( Â· )(cid:105) ËœH } is uniquely determined, see e.g. [31, Chapter 4.2]. For
more information on kernels, we refer to [31, Chapter 4]. Moreover, for a compact metric space
(X, d), we write C (X ) := {f : X â†’ R | f continuous} for the space of continuous functions on X
and equip this space with the usual supremum norm (cid:107) Â· (cid:107)âˆ . A kernel k on X is called universal, if
k is continuous and its RKHS H is dense in C (X ). As mentioned before, this notion, which goes
back to [30], plays a key role in the analysis of kernel-based learning methods. Let r âˆˆ (0, âˆ].
The kernels we consider in this paper are constructed by functions K : [âˆ’r, r] â†’ R that can be
âˆ(cid:88)
expressed by its Taylor series, that is
an tn ,
n=0
âˆ(cid:88)
For such functions [31, Lemma 4.8] showed that
x, x(cid:48) âˆˆ âˆš
k(x, x(cid:48) ) := K ((cid:104)x, x(cid:48) (cid:105)Rd ) =
an (cid:104)x, x(cid:48) (cid:105)n
Rd ,
rBRd ,
rBRd := {x âˆˆ Rd : (cid:107)x(cid:107)2 â‰¤ âˆš
âˆš
âˆš
n=0
r} with radius
r , whenever all
deï¬nes a kernel on the closed ball
Taylor coefï¬cients an are non-negative. Following [31], we call such kernels Taylor kernels. [30],
see also [31, Lemma 4.57], showed that Taylor kernels are universal, if an > 0 for all n â‰¥ 0, while
:= (cid:80)
[21] notes that strict positivity on certain subsets of indices n sufï¬ces.
Obviously, the deï¬nition (2) of k is still possible, if one replaces Rd by its inï¬nite dimensional and
separable counterpart (cid:96)2 := {(wj )jâ‰¥1 : (cid:107)(wj )(cid:107)2
j < âˆ}. Let us denote the closed
jâ‰¥1 w2
(cid:96)2
unit ball in (cid:96)2 by B(cid:96)2 , or more generally, the closed unit ball of a Banach space E by BE , that is
BE := {v âˆˆ E : (cid:107)v(cid:107)E â‰¤ 1}. Our ï¬rst main result shows that this extension leads to a kernel, whose
restrictions to compact subsets are universal, if an > 0 for all n âˆˆ N0 := N âˆª {0}.
Theorem 2.1 Let K : [âˆ’r, r] â†’ R be a function of the form (1). Then we have:
âˆš
rB(cid:96)2 Ã— âˆš
âˆ(cid:88)
i) If an â‰¥ 0 for all n â‰¥ 0, then k :
rB(cid:96)2 â†’ R is a kernel, where
(cid:1) =
k(w, w (cid:48) ) := K (cid:0)(cid:104)w, w (cid:48) (cid:105)(cid:96)2
w, w (cid:48) âˆˆ âˆš
an (cid:104)w, w (cid:48) (cid:105)n
(cid:96)2
n=0
ii) If an > 0 for all n âˆˆ N0 , then the restriction k|W Ã—W : W Ã— W â†’ R of k to an arbitrary
compact set W âŠ‚ âˆš
rB(cid:96)2 is universal.
To consider a ï¬rst explicit example, let K := exp : R â†’ R be the exponential function. Then
K clearly satisï¬es the assumptions of Theorem 2.1 for all r > 0, and hence the resulting exponen-
tial kernel is universal on every compact subset W of (cid:96)2 . Moreover, for Ïƒ âˆˆ (0, âˆ), the related
(cid:1) =
kÏƒ (w, w (cid:48) ) := exp(cid:0)âˆ’Ïƒ2(cid:107)w âˆ’ w (cid:48)(cid:107)2
Gaussian-type RBF kernel kÏƒ : (cid:96)2 Ã— (cid:96)2 â†’ R deï¬ned by
exp(2Ïƒ2 (cid:104)w, w (cid:48) (cid:105)(cid:96)2 )
) exp(Ïƒ2 (cid:107)w (cid:48)(cid:107)2
exp(Ïƒ2 (cid:107)w(cid:107)2
(cid:96)2
(cid:96)2
(cid:96)2

(1)

(2)

(4)

,

rB(cid:96)2 .

(3)

)

3

an

(5)

x, x(cid:48) âˆˆ X.

is also universal on every compact W âŠ‚ (cid:96)2 , since modulo the scaling by Ïƒ it is the normalized
version of the exponential kernel, and thus it is universal by [31, Lemma 4.55].
Although we have achieved our ï¬rst goal, namely explicit, constructive examples of universal ker-
nels on X (cid:54)âŠ‚ Rd , the result is so far not really satisfying. Indeed, unlike the ï¬nite dimensional
Euclidean spaces Rd , the inï¬nite dimensional space (cid:96)2 rarely appears as the input space in real-
world applications. The following second result can be used to address this issue.
Theorem 2.2 Let X be a compact metric space and H be a separable Hilbert space such that there
exists a continuous and injective map Ï : X â†’ H. Furthermore, let K : R â†’ R be a function of
the form (1). Then the following statements hold:
i) If an â‰¥ 0 for all n âˆˆ N0 , then k : X Ã— X â†’ R deï¬nes a kernel, where
âˆ(cid:88)
(cid:1) =
k(x, x(cid:48) ) := K (cid:0)(cid:10)Ï(x), Ï(x(cid:48) )(cid:11)
(cid:10)Ï(x), Ï(x(cid:48) )(cid:11)n
H ,
H
n=0
ii) If an > 0 for all n âˆˆ N0 , then k is a universal kernel.
kÏƒ (x, x(cid:48) ) := exp(cid:0)âˆ’Ïƒ2(cid:107)Ï(x) âˆ’ Ï(x(cid:48) )(cid:107)2H (cid:1) ,
iii) For Ïƒ > 0, the Gaussian-type RBF-kernel kÏƒ : X Ã— X â†’ R is a universal kernel, where
x, x(cid:48) âˆˆ X.
(6)
positive non-constant radial basis function kernels such as kÏƒ (x, x(cid:48) ) := exp(cid:0)âˆ’Ïƒ2 (cid:107)Ï(x) âˆ’ Ï(x(cid:48) )(cid:107)H (cid:1)
or the Student-type RBF kernels kÏƒ (x, x(cid:48) ) := (cid:0)1 + Ïƒ2(cid:107)Ï(x) âˆ’ Ï(x(cid:48) )(cid:107)2H
(cid:1)âˆ’Î± for Ïƒ2 > 0 and Î± â‰¥ 1.
It seems possible that the latter result for the Gaussian-type RBF kernel can be extended to other
Indeed, [25] uses the fact that on Rd such kernels have an integral representation in terms of the
Gaussian RBF kernels to show, see [25, Corollary 4.9], that these kernels inherit approximation
properties such as universality from the Gaussian RBF kernels. We expect that the same arguments
can be made for (cid:96)2 and then, in a second step, for the situation of Theorem 2.2.
Before we provide some examples of situations in which Theorem 2.2 can be used to deï¬ne explicit
universal kernels, we point to a technical detail of Theorem 2.2, which may be overseen, thus leading
to wrong conclusions.
To this end, let (X, dX ) be an arbitrary metric space, H be a separable Hilbert space and Ï : X â†’ H
be an injective map. We write V := Ï(X ) and equip this space with the metric deï¬ned by H.
Thus, Ï : X â†’ V is bijective by deï¬nition. Moreover, since H is assumed to be separable, it is
isometrically isomorphic to (cid:96)2 , and hence there exists an isometric isomorphism I : H â†’ (cid:96)2 . We
write W := I (V ) and equip this set with the metric deï¬ned by the norm of (cid:96)2 . For a function
f : W â†’ R, we can then consider the following diagram
f â—¦ I â—¦ Ï

(X, dX )
6

Ï

-

(R, | Â· |)
6
f

?
(V , (cid:107) Â· (cid:107)H )



-

(W, (cid:107) Â· (cid:107)(cid:96)2 )
(7)
I
Since both Ï and I are bijective, it is easy to see that f not only deï¬nes a function g : X â†’ R
by g := f â—¦ I â—¦ Ï, but conversely, every function g : X â†’ R has such a representation and this
representation is unique. In other words, there is a one-to-one relationship between the functions
X â†’ R and the functions W â†’ R. Let us now assume that we have a kernel kW on W with RKHS
HW and canonical feature map Î¦W : W â†’ HW . Then kX : X Ã— X â†’ R, given by
x, x(cid:48) âˆˆ X,
kX (x, x(cid:48) ) := kW (I â—¦ Ï(x), I â—¦ Ï(x(cid:48) )) ,
deï¬nes a kernel on X , since
kX (x, x(cid:48) ) = kW (I â—¦ Ï(x), I â—¦ Ï(x(cid:48) )) = (cid:104)Î¦W (I (Ï(x(cid:48) ))), Î¦W (I (Ï(x)))(cid:105)HW ,

x, x(cid:48) âˆˆ X,

4

shows that Î¦W â—¦ I â—¦ Ï : X â†’ HW is a feature map of kX . Moreover, [31, Theorem 4.21] shows
HX = (cid:8)(cid:104)f , Î¦W â—¦ I â—¦ Ï( Â· )(cid:105)HW : f âˆˆ HW
(cid:9) .
that the RKHS HX of kX is given by
Since, for f âˆˆ HW , the reproducing property of HW gives f â—¦ I â—¦ Ï(x) = (cid:104)f , Î¦W â—¦ I â—¦ Ï(x)(cid:105)HW
for all x âˆˆ X we thus conclude that HX = {f â—¦ I â—¦ Ï : f âˆˆ HW } =: HW â—¦ I â—¦ Ï. Let us
now assume that X is compact and that kW is one of the universal kernels considered in Theorem
2.1 or the Gaussian RBF kernel (4). Then the proof of Theorem 2.2 shows that kX is one of the
universal kernels considered in Theorem 2.2. Moreover, if we consider the kernel kV : V Ã— V â†’ R
deï¬ned by kV (v , v (cid:48) ) := kW (I (v), I (v (cid:48) )), then an analogous argument shows that kV is a universal
kernel. This raises the question, whether we need the compactness of X , or whether it sufï¬ces
to assume that Ï is injective, continuous and has a compact image V . Surprisingly, the answer is
that it depends on the type of universality one needs. Indeed, if Ï is as in Theorem 2.2, then the
compactness of X ensures that Ï is a homeomorphism, that is, Ïâˆ’1 : V â†’ X is continuous, too.
Since I is clearly also a homeomorphism, we can easily conclude that C (X ) = C (W ) â—¦ I â—¦ Ï,
that is, we have the same relationship as we have for the RKHSs HW and HX . From this, the
universality is easy to establish. Let us now assume the compactness of V instead of the compactness
of X . Then, in general, Ï is not a homeomorphism and the sets of continuous functions on X
and V are in general different, even if we consider the set of bounded continuous functions on X .
To see the latter, consider e.g. the map Ï : [0, 1) â†’ S 1 onto the unit sphere S 1 of R2 deï¬ned
by Ï(t) := (sin(2Ï€t), cos(2Ï€t)). Now this difference makes it impossible to conclude from the
universality of kV (or kW ) to the universality of kX . However, if Ï„V denotes the topology of V ,
then Ïâˆ’1 (Ï„V ) := {Ïâˆ’1 (O) : O âˆˆ Ï„V } deï¬nes a new topology on X , which satisï¬es Ïâˆ’1 (Ï„V ) âŠ‚ Ï„X .
Consequently, there are, in general, fewer continuous functions with respect to Ïâˆ’1 (Ï„V ). Now, it
is easy to check that dÏ (x, x(cid:48) ) := (cid:107)Ï(x) âˆ’ Ï(x(cid:48) )(cid:107)H deï¬nes a metric that generates Ïâˆ’1 (Ï„V ) and,
since Ï is isometric with respect to this new metric, we can conclude that (X, dÏ ) is a compact
metric space. Consequently, we are back in the situation of Theorem 2.2, and hence kX is universal
with respect to the space C (X, dÏ ) of functions X â†’ R that are continuous with respect to dÏ .
In other words, while HX may fail to approximate every function that is continuous with respect
to dX , it does approximate every function that is continuous with respect to dÏ . Whether the latter
approximation property is enough clearly depends on the speciï¬c application at hand.
Let us now present some universal kernels of practical interest. Please note, that although the func-
tion Ï in our examples is even linear, the Theorem 2.2 only assumes Ï to be continuous and injective.
We start with two examples where X is the set of distributions on some space â„¦.
Example 1: universal kernels on the set of probability measures.
Let (â„¦, dâ„¦ ) be a compact metric space, B(â„¦) be its Borel Ïƒ -algebra, and X := M1 (â„¦) be the set of
dX (P, P(cid:48) ) := inf (cid:8)Îµ > 0 : P(A) â‰¤ P(cid:48) (AÎµ ) + Îµ for all A âˆˆ B(â„¦)(cid:9),
all Borel probability measures on â„¦. Then the topology describing weak convergence of probability
measures can be metrized, e.g., by the Prohorov metric
P, P(cid:48) âˆˆ X ,
(8)
where AÎµ := {Ï‰ (cid:48) âˆˆ â„¦ : dâ„¦ (Ï‰ , Ï‰ (cid:48) ) < Îµ for some Ï‰ âˆˆ A}, see e.g. [2, Theorem 6.8, p. 73].
Moreover, (X, dX ) is a compact metric space if and only if (â„¦, dâ„¦ ) is a compact metric space, see
[19, Thm. 6.4]. In order to construct universal kernels on (X, dX ) with the help of Theorem 2.2,
it thus remains to ï¬nd separable Hilbert spaces H and injective, continuous embeddings Ï : X â†’
H. Let kâ„¦ be a continuous kernel on â„¦ with RKHS Hâ„¦ and canonical feature map Î¦â„¦ (Ï‰) :=
kâ„¦ (Ï‰ , Â·), Ï‰ âˆˆ â„¦. Note that kâ„¦ is bounded because it is continuous and â„¦ is compact. Then Hâ„¦
is separable and Î¦â„¦ is bounded and continuous, see [31, Lemmata 4.23, 4.29, 4.33]. Assume that
kâ„¦ is additionally characteristic, i.e. the function Ï : X â†’ Hâ„¦ deï¬ned by the Bochner integral
Ï(P) := EPÎ¦â„¦ is injective. Then the next lemma, which is taken from [10, Thm. 5.1] and which is
a modiï¬cation of a theorem in [3, p. III. 40], ensures the continuity of Ï.
Lemma 2.3 Let (â„¦, dâ„¦ ) be a complete separable metric space, H be a separable Banach space and
Î¦ : â„¦ â†’ H be a bounded, continuous function. Then Ï : M1 (â„¦) â†’ H deï¬ned by Ï(P) := EPÎ¦ is
continuous, i.e., EPn Î¦ â†’ EPÎ¦, whenever (Pn )nâˆˆN âŠ‚ M1 (â„¦) converges weakly in M1 (â„¦) to P.
Consequently, the map Ï : M1 (â„¦) â†’ Hâ„¦ satisï¬es the assumptions of Theorem 2.2, and hence the
(cid:1) , P, P(cid:48) âˆˆ M1 (â„¦),
kÏƒ (P, P(cid:48) ) := exp (cid:0)âˆ’Ïƒ2(cid:107)EPÎ¦â„¦ âˆ’ EP(cid:48) Î¦â„¦ (cid:107)2Hâ„¦
Gaussian-type RBF kernel
5

(9)

is universal and obviously bounded. Note that this kernel is conceptionally different to characteristic
kernels on â„¦. Indeed, characteristic kernels live on â„¦ and their RKHS consist of functions â„¦ â†’
R, while the new kernel kÏƒ lives on M1 (â„¦) and its RKHS consists of functions M1 (â„¦) â†’ R.
Consequently, kÏƒ can be used to learn from samples that are individual distributions, e.g. represented
by histograms, densities or data, while characteristic kernels can only be used to check whether two
of such distributions are equal or not.
(or characteristic function), that is Ï(P) := Ë†P, where Ë†P(t) := (cid:82) ei(cid:104)z ,t(cid:105)dÂµ(z ) âˆˆ C, t âˆˆ Rd . It is
Example 2: universal kernels based on Fourier transforms of probability measures.
Consider, the set X := M1 (â„¦), where â„¦ âŠ‚ Rd is compact. Moreover, let Ï be the Fourier transform
well-known, see e.g. [6, Chap. 9], that, for all P âˆˆ M1 (â„¦), Ë†P is uniformly continuous on Rd and
(cid:107) Ë†P(cid:107)âˆ â‰¤ 1. Moreover, Ï : P (cid:55)â†’ Ë†P is injective, and if a sequence (Pn ) converges weakly to some
P, then ( Ë†Pn ) converges uniformly to Ë†P on every compact subset of Rd . Now let Âµ be a ï¬nite
Borel measure on Rd with support(Âµ) = Rd , e.g., Âµ can be any probability distribution on Rd with
Lebesgue density h > 0. Then the previous properties of the Fourier transform can be used to
show that Ï : M1 (â„¦) â†’ L2 (Âµ) is continuous, and hence Theorem 2.2 ensures that the following
kÏƒ (P, P(cid:48) ) := exp(cid:0)âˆ’Ïƒ2(cid:107) Ë†P âˆ’ Ë†P(cid:48)(cid:107)2
(cid:1) ,
Gaussian-type kernel is universal and bounded:
P, P(cid:48) âˆˆ M1 (â„¦).
L2 (Âµ)
In view of the previous two examples, we mention that the probability measures P and P(cid:48) are often
not directly observable in practice, but only corresponding empirical distributions can be obtained.
In this case, a simple standard technique is to construct histograms to represent these empirical distri-
butions as vectors in a ï¬nite-dimensional Euclidean space, although it is well-known that histograms
can yield bad estimates for probability measures. Our new kernels make it possible to directly plug
the empirical distributions into the kernel kÏƒ , even if these distributions do not have the same length.
Moreover, other techniques to convert empirical distributions to absolutely continuous distributions
such as kernel estimators derived via weighted averaging of rounded points (WAPRing) and (averag-
ing) histograms with different origins, [20, 24] can be used in kÏƒ , too. Clearly, the preferred method
will most likely depend on the speciï¬c application at hand, and one beneï¬t of our construction is
that it allows this ï¬‚exibility.
Example 3: universal kernels for signal processing.
Let (â„¦, A, Âµ) be an arbitrary measure space and L2 (Âµ) be the usual space of square Âµ-integrable
functions on â„¦. Let us additionally assume that L2 (Âµ) is separable, which is typically, but not
always, satisï¬ed. In addition, let us assume that our input values xi âˆˆ X are functions taken from
some compact set X âŠ‚ L2 (Âµ). A typical example, where this situation occurs, is signal processing,
where the true signal f âˆˆ L2 ([0, 1]), which is a function of time, cannot be directly observed, but a
smoothed version g := T â—¦ f of the signal is observable. This smoothing can often be described by a
compact linear operator T : L2 ([0, 1]) â†’ L2 ([0, 1]), e.g., a convolution operator, acting on the true
signals. Hence, if we assume that the true signals are contained in the closed unit ball BL2 ([0,1]) ,
then the observed, smoothed signals T â—¦ f are contained in a compact subset X of L2 ([0, 1]). Let
us now return to the general case introduced above. Then the identity map Ï := id : X â†’ L2 (Âµ)
kÏƒ (g , g (cid:48) ) := exp(cid:0)âˆ’Ïƒ2 (cid:107)g âˆ’ g (cid:48)(cid:107)2
(cid:1) ,
satisï¬es the assumptions of Theorem 2.2, and hence the Gaussian-type kernel
g , g (cid:48) âˆˆ X,
L2 (Âµ)
deï¬nes a universal and bounded kernel on X . As in the previous examples, note that the computation
of kÏƒ does not require the functions g and g (cid:48) to be in a speciï¬c format such as a certain discretization.

(11)

(10)

3 Discussion

The main goal of this paper was to provide an explicit construction of universal kernels that are
deï¬ned on arbitrary compact metric spaces, which are not necessarily a subset of Rd . There is a
still increasing interest in kernel methods including support vector machines on such input spaces,
e.g. for classiï¬cation or regression purposes for input values being probability measures, histograms
or colored images. As examples, we gave explicit universal kernels on the set of probability distri-
butions and for signal processing. One direction of further research may be to generalize our results
to the case of non-compact metric spaces or to ï¬nd quantitative approximation results.

6

4 Proofs

Ëœcj

cj

bji
i

cj

bji
i

.

bi

=

bi

=

bji
i

.

0 for the set of all sequences (ji )iâ‰¥1 with values in N0 := N âˆª {0}.
In the following, we write NN
Elements of this set will serve us as multi-indices with countably many components. For j = (ji ) âˆˆ
(cid:88)
NN
0 , we will therefore adopt the multi-index notation
|j | :=
ji .
iâ‰¥1
Note that |j | < âˆ implies that j has only ï¬nitely many components ji with ji (cid:54)= 0.
Lemma 4.1 Assume that n âˆˆ N is ï¬xed and that for all j âˆˆ NN
0 with |j | = n, we have some
constant cj âˆˆ (0, âˆ). Then for all j âˆˆ NN
0 with |j | = n + 1, there exists a constant Ëœcj âˆˆ (0, âˆ)
(cid:19)(cid:18) âˆ(cid:88)
(cid:18) (cid:88)
(cid:19)
âˆ(cid:89)
(cid:88)
âˆ(cid:89)
such that for all summable sequences (bi ) âŠ‚ [0, âˆ) we have
jâˆˆNN
0 :|j |=n+1
jâˆˆNN
0 :|j |=n
i=1
i=1
i=1
Proof: This can be shown by induction, where the induction step is similar to the proof for the
Cauchy product of series.
Lemma 4.2 Assume that n âˆˆ N0 is ï¬xed. Then for all j âˆˆ NN
0 with |j | = n, there exists a constant
(cid:18) âˆ(cid:88)
(cid:19)n
âˆ(cid:89)
(cid:88)
cj âˆˆ (0, âˆ) such that for all summable sequences (bi ) âŠ‚ [0, âˆ) we have
jâˆˆNN
0 :|j |=n
i=1
i=1
2 := (cid:80)
Proof: This can be shown by induction using Lemma 4.1.
Given a non-empty countable set J and a family w := (wj )jâˆˆJ âŠ‚ R, we write (cid:107)w(cid:107)2
j ,
jâˆˆJ w2
and, as usual, we denote the space of all families for which this quantity is ï¬nite by (cid:96)2 (J ). Recall
that (cid:96)2 (J ) together with (cid:107) Â· (cid:107)2 is a Hilbert space and we denote its inner product by (cid:104) Â· , Â· (cid:105)(cid:96)2 (J ) .
Moreover, (cid:96)2 := (cid:96)2 (N) is separable, and by using an orthonormal basis representation, it is further
known that every separable Hilbert space is isometrically isomorphic to (cid:96)2 . In this sense, (cid:96)2 can be
viewed as a generic model for separable Hilbert spaces.
The following result provides a method to construct Taylor kernels on closed balls in (cid:96)2 .
Taylor series given in (1), i.e. K (t) = (cid:80)âˆ
Proposition 4.3 Let r âˆˆ (0, âˆ] and K : [âˆ’r, r] â†’ R be a function that can be expressed by its
n=0 an tn , t âˆˆ [âˆ’r, r]. Deï¬ne J := {j âˆˆ NN
0 : |j | < âˆ}.
rB(cid:96)2 Ã— âˆš
âˆš
âˆ(cid:88)
(cid:1) =
k(w, w (cid:48) ) := K (cid:0)(cid:104)w, w (cid:48) (cid:105)(cid:96)2
rB(cid:96)2 â†’ R deï¬ned by (3), i.e.
If an â‰¥ 0 for all n â‰¥ 0, then k :
w, w (cid:48) âˆˆ âˆš
an (cid:104)w, w (cid:48) (cid:105)n
,
rB(cid:96)2 ,
(cid:96)2
n=0
is a kernel. Moreover, for all j âˆˆ J , there exists a constant cj âˆˆ (0, âˆ) such that Î¦ :
(cid:17)
(cid:16)
âˆ(cid:89)
(cid:96)2 (J ) deï¬ned by
w âˆˆ âˆš
jâˆˆJ
i=1
is a feature map of k , where we use the convention 00 := 1.
Proof: For w, w (cid:48) âˆˆ âˆš
rB(cid:96)2 , the Cauchy-Schwarz inequality yields |(cid:104)w, w (cid:48) (cid:105)| â‰¤ (cid:107)w(cid:107)2(cid:107)w (cid:48)(cid:107)2 â‰¤ r
and thus k is well-deï¬ned. Let wi denote the i-th component of w âˆˆ (cid:96)2 . Since (1) is absolutely
0 , there exists a constant Ëœcj âˆˆ (0, âˆ) such
convergent, Lemma 4.2 then shows that, for all j âˆˆ NN
(cid:88)
âˆ(cid:89)
âˆ(cid:89)
that
k(w, w (cid:48) ) =
(w (cid:48)
Setting cj := (cid:112)a|j | Ëœcj , we obtain that Î¦ deï¬ned in (12) is indeed a feature map of k , and hence k is
wji
i )ji
a|j | Ëœcj
.
i
jâˆˆNN
i=1
i=1
0
a kernel.

rB(cid:96)2 â†’

Î¦(w) :=

rB(cid:96)2 ,

(12)

âˆš

cj

wji
i

,

7

Before we can state our ï¬rst main result the need to recall the following test of universality from
[31, Theorem 4.56].

Theorem 4.4 Let W be a compact metric space and k be a continuous kernel on W with k(w, w) >
0 for all w âˆˆ W . Suppose that we have an injective feature map Î¦ : W â†’ (cid:96)2 (J ) of k , where J
is some countable set. We write Î¦j : W â†’ R for its j -th component, i.e., Î¦(w) = (Î¦j (w))jâˆˆJ ,
w âˆˆ W . If A := span {Î¦j : j âˆˆ J } is an algebra, then k is universal.

k(w, w) =

With the help of Theorem 4.4 and Proposition 4.3 we can now prove our ï¬rst main result.
âˆš
now ï¬x a compact W âŠ‚ âˆš
Proof of Theorem 2.1: We have already seen in Proposition 4.3 that k is a kernel on
rB(cid:96)2 . Let us
rB(cid:96)2 . For every j âˆˆ J , where J is deï¬ned in Proposition 4.3, there are
only ï¬nitely many components ji with ji (cid:54)= 0. Consequently, there exists a bijection between J and
âˆ(cid:88)
the set of all ï¬nite subsets of N. Since the latter is countable, J is countable. Furthermore, we have
an(cid:107)w(cid:107)2n
â‰¥ a0 > 0
(cid:96)2
n=0
for all w âˆˆ W , and it is obvious, that the components of the feature map Î¦ found in Proposition
4.3 span an algebra. Finally, if we have w, w (cid:48) âˆˆ W with w (cid:54)= w (cid:48) , there exists an i â‰¥ 1 such that
wi (cid:54)= w (cid:48)
i . For the multi-index j âˆˆ J that equals 1 at the i-component and vanishes everywhere else
we then have Î¦(w) = cj wi (cid:54)= cj w (cid:48)
i = Î¦(w (cid:48) ), and hence Î¦ is injective.
Proof of Theorem 2.2: Since H is separable Hilbert space there exists an isometric isomorphism
I : H â†’ (cid:96)2 . We deï¬ne V := Ï(X ), see also the diagram in (7). Since Ï is continuous, V is
the image of a compact set under a continuous map, and thus V is compact and the inverse of
the bijective map I â—¦ Ï : X â†’ W is continuous. Consequently, there is a one-to-one relationship
between the continuous functions fX on X and the continuous functions fW on W , namely C (X ) =
C (W ) â—¦ I â—¦ Ï, see also the discussion following (7). Moreover, the fact that I : H â†’ (cid:96)2 is an
isometric isomorphism yields (cid:104)I (Ï(x)), I (Ï(x(cid:48) ))(cid:105)(cid:96)2 = (cid:104)Ï(x), Ï(x(cid:48) )(cid:105)H for all x, x(cid:48) âˆˆ X , and hence
the kernel k considered in Theorem 2.2 is of the form kX = kW (I â—¦ Ï( Â· ), I â—¦ Ï( Â· )), where kW
is the corresponding kernel deï¬ned on W âŠ‚ (cid:96)2 considered in Theorem 2.2. Now, the discussion
following (7) showed HX = HW â—¦ I â—¦ Ï. Consequently, if we ï¬x a function g âˆˆ C (X ), then
f := g â—¦ Ïâˆ’1 â—¦ I âˆ’1 âˆˆ C (W ) can be approximated by HW , that is, for all Îµ > 0, there exists an
h âˆˆ HW such that (cid:107)h âˆ’ f (cid:107)âˆ â‰¤ Îµ. Since I â—¦ Ï : X â†’ W is bijective and f â—¦ I â—¦ Ï = g , we conclude
that (cid:107)h â—¦ I â—¦ Ï âˆ’ g(cid:107)âˆ â‰¤ Îµ. Now the assertion follows from h â—¦ I â—¦ Ï âˆˆ HX .

References
[1] H. Bauer. Measure and Integration Theory. De Gruyter, Berlin, 2001.
[2] P. Billingsley. Convergence of probability measures. John Wiley & Sons, New York, 2nd edition, 1999.
[3] N. Bourbaki. Integration I. Chapters 1-6. Springer, Berlin, 2004. Translated from the 1959, 1965, and
1967 French originals by S.K. Berberian.
[4] A. Caponnetto, C.A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. J. Mach. Learn.
Res., 9:1615â€“1646, 2008.
[5] O. Chapelle, P. Haffner, and V. Vapnik. SVMs for histogram-based image classiï¬cation. IEEE Transac-
tions on Neural Networks, 10:1055â€“1064, 1999.
[6] R. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, 2002.
[7] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning with repro-
ducing kernel hilbert spaces. J. Mach. Learn. Res., 5:73â€“99, 2005.
[8] K. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel Dimension Reduction in Regression. Ann. Statist.,
37:1871â€“1905, 2009.
[9] K. Fukumizu, B. K. Sriperumbudur, A. Gretton, and B. Sch Â¨olkopf. Characteristic kernels on groups
and semigroups. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21, pages 473â€“480. 2009.

8

[10] R. Hable and A. Christmann. Qualitative robustness of support vector machines. arXiv:0912.0874v1,
2009.
[11] M. Hein and O. Bousquet. Kernels, associated structures and generalizations. Technical report, Max-
Planck-Institute for Biological Cybernetics, 2004.
[12] M. Hein and O. Bousquet. Hilbertian metrics and positive deï¬nite kernels on probability measures. In
Z. Ghahramani and R. Cowell, editors, AISTATS, pages 136â€“143, 2005.
[13] M. Hein, O. Bousquet, and B. Sch Â¨olkopf. Maximal margin classiï¬cation for metric spaces. Journal of
Computer and System Sciences, 71:333â€“359, 2005.
[14] M. Hein, T. N. Lal, and O. Bousquet. Hilbertian metrics on probability measures and their application in
SVMâ€™s. In C. E. Rasmussen, H. H. B Â¨ulthoff, M. Giese, and B. Sch Â¨olkopf, editors, Pattern Recognition,
Proceedings of the 26th DAGM Symposium, pages 270â€“277, Berlin, 2004. Springer.
[15] T. Joachims. Learning to Classify Text Using Support Vector Machines. Kluwer Academic Publishers,
Boston, 2002.
[16] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. J. Mach. Learn. Res., 6:129â€“163,
2005.
[17] A.F.T. Martins, N.A. Smith, E.P. Xing, P.M.Q. Aguiar, and M.A.T. Figueiredo. Nonextensive information
theoretic kernels on measures. J. Mach. Learn. Res., 10:935â€“975, 2009.
[18] C. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. J. Mach. Learn. Res., 7:2651â€“2667, 2006.
[19] K. R. Parthasarathy. Probability Measures on Metric Spaces. Academic Press, New York, 1967.
[20] E. Parzen. On estimating of a probability density and mode. Ann. Math. Statist., 35:1065â€“1076, 1962.
[21] A. Pinkus. Strictly positive deï¬nite functions on a real inner product space. Adv. Comput. Math., 20:263â€“
271, 2004.
[22] B. Sch Â¨olkopf, A. J. Smola, and K.-R. M Â¨uller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Comput., 10:1299â€“1319, 1998.
[23] B. Sch Â¨olkopf, K. Tsuda, and J. P. Vert. Kernel Methods in Computational Biology. MIT Press, Cambridge,
MA, 2004.
[24] D. Scott. Averaged shifted histograms: Effective nonparametric density estimation in several dimensions.
Ann. Statist., 13:1024â€“1040, 1985.
[25] C. Scovel, D. Hush, I. Steinwart, and J. Theiler. Radial kernels and their reproducing kernel Hilbert
spaces. Journal of Complexity, 2010, to appear.
[26] A.J. Smola, A. Gretton, L. Song, and B. Sch Â¨olkopf. A Hilbert Space Embedding for Distributions. In
E. Takimoto, editor, Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer, 2007.
Proceedings of the 10th International Conference on Discovery Science, 40-41.
[27] B. Sriperumbudur, K. Fukumizu, A. Gretton, G. Lanckriet, and B. Sch Â¨olkopf. Kernel choice and clas-
siï¬ability for RKHS embeddings of probability distributions. In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages
1750â€“1758. 2009.
[28] B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. On the relation between universality, characteristic
kernels and RKHS embeddings of measures. In Yee Whye Teh and M. Titterington, editors, AISTATS
2010, Proc. of the 13th International Conference on Artiï¬cial Intelligence and Statistics, volume 9, pages
773â€“780. 2010.
[29] B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, characteristic kernels and RKHS em-
beddings of measures. arXiv:1003.0887v1, 2010.
[30] I. Steinwart. On the inï¬‚uence of the kernel on the consistency of support vector machines. J. Mach.
Learn. Res., 2:67â€“93, 2001.
[31] I. Steinwart and A. Christmann. Support Vector Machines. Springer, New York, 2008.
[32] I. Steinwart, D. Hush, and C. Scovel. Function classes that approximate the Bayes risk. In COLTâ€™06, 19th
Conference on Learning Theory, Pittsburgh, 2006.

9


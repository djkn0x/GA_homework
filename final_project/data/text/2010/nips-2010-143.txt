Cross Species Expression Analysis using a Dirichlet
Process Mixture Model with Latent Matchings

Hai-Son Le
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA, USA
hple@cs.cmu.edu

Ziv Bar-Joseph
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA, USA
zivbj@cs.cmu.edu

Abstract

Recent studies compare gene expression data across species to identify core and
species speci ﬁc genes in biological systems. To perform suc h comparisons re-
searchers need to match genes across species. This is a challenging task since
the correct matches (orthologs) are not known for most genes. Previous work in
this area used deterministic matchings or reduced multidimensional expression
data to binary representation. Here we develop a new method that can utilize soft
matches (given as priors) to infer both, unique and similar expression patterns
across species and a matching for the genes in both species. Our method uses
a Dirichlet process mixture model which includes a latent data matching vari-
able. We present learning and inference algorithms based on variational methods
for this model. Applying our method to immune response data we show that it
can accurately identify common and unique response patterns by improving the
matchings between human and mouse genes.

1

Introduction

Researchers have been increasingly relying on cross species analysis to understand how biological
systems operate. Sequence based methods have been successfully applied to identify and charac-
terize coding and functional non coding regions in multiple species [1]. However, sequence infor-
mation is static and thus provides only partial view of cellular activity. More recent studies attempt
to integrate sequence and gene expression data from multiple species [2, 3, 4]. Unlike sequence,
expression levels are dynamic and differ across time and conditions. By combining expression and
sequence data researchers were able to identify both ”core”
and ”divergent ” genes.
”Core” genes
are similarly expressed across species and are useful for constructing models of conserved systems,
for example the cell cycle [2].
”Divergent ” genes are simila r in sequence but differ in expression
across species. These are useful for identifying species speci ﬁc responses, for example why some
pathogens are resistant to drugs while others are not [3].

While useful, cross species analysis of expression data is challenging. In addition to the regular
issues with expression data (noise, missing values, etc.) when comparing expression levels across
species researchers need to match genes across species. For most genes the correct match in another
species (known as ortholog) is not known. A number of methods have been suggested to solve the
matching problem. The ﬁrst set of methods is based on a one to o ne deterministic assignment by
relying on top sequence matches. Such an assignment can be used to concatenate the expression
vectors for matched genes across species and then cluster the resulting vectors. For example, Stuart
et al. [5] constructed ”metagenes ” consisting of top sequen ce matches from four species. These
were used to cluster the data from multiple species to identify conserved and divergent patterns.
Bergmann et al. [6] deﬁned one of the species (species A) as a r eference and ﬁrst clustered genes
in A. They then used matched genes in the second species (B) as starting points for clustering

1

genes in B. When the clustering algorithm converges in B, genes that remain in the cluster are
considered ”core” whereas genes that are removed are ”diver
gent ”. Quon et al. [4] used a mixture of
Gaussians model, which takes as input the expression data of orthologous genes and a phylogenetic
tree connecting the species, to reconstruct the expression proﬁles as well as detecting divergent
links in the phylogeny. The second set of methods allowed for soft matches but was either limited to
analyzing binary or discrete data with very few labels. For example, Lu et al. combined experiments
from multiple species by using Markov Random Fields [7] and Gaussian Random Fields [8] in which
edges represent sequence similarity and potential functions constrain similar genes across species to
have a similar expression pattern.

While both approaches led to successful applications, they suffer from drawbacks that limit their
use in practice. In many cases the top sequence match is not the correct ortholog and a deterministic
assignment may lead to wrong conclusions about the conservation of genes. Methods that have
used soft assignments were limited to summarization of the data (up or down regulated) and could
not utilize more complex proﬁles. Here we present a new metho d that uses soft assignments to
allow comparison and clustering across species of arbitrary expression data without requiring prior
knowledge on the phylogeny. Our method takes as input expression datasets in two species and a
prior on matches between homologous genes in these species (derived from sequence data). The
method simultaneously clusters the expression values for both species while computing a posterior
for the assignment of orthologs for genes. We use Dirichlet Process model to automatically detect
the number of clusters.

We have tested our method on simulated and immune response data. In both cases the algorithm
was able to ﬁnd correct matches and to improve upon methods th at used a deterministic assignment.
While the method was developed for, and applied to, biological data, it is general and can be used to
address other problems including matchings of captions to images (see Section 5).

2 Problem de ﬁnition

In this section, we ﬁrst describe in details the cross specie s analysis problem for gene expression
data. Next, we formalize this as a general clustering and matching problem for cases in which the
matches are not known in advance.

Using microarrays or new sequencing techniques researchers can monitor the expression levels of
genes under certain conditions or at speci ﬁc time points. Fo r each such measurement we obtain a
vector whose elements are the expression values for all genes (there are usually thousands of entries
in each vector). We assume that the input consists of microarray experiments from two species and
each species has a different set of genes. While the exact matches between genes in both species
are not known for most genes, we have a prior for gene pairs (one from each species) which is
derived from sequence data [9]. Our goal is to simultaneously cluster the genes in both species.
Such clustering can identify coherent and divergent responses between the species. In addition, we
would like to infer for each gene in one species whether there exists a homolog that is similarly
expressed in the other species and if so, who.

The problem can also be formalized more generally in the following way. Denote by x =
[x1 , x2 , . . . , xnx ] and y = [y1 , y2 , . . . , yny ] the datasets of samples from two different experiment
settings, where xi ∈ ℜpx and yj ∈ ℜpy . In addition, let M be a sparse non-negative nx × ny matrix
that encodes prior information regarding the matching of samples in x and y. We deﬁne the match
probability between xi and yj as follows:
M(i, j )
1
p(xi and yj are matched) =
Ni
Ni
where Ni = 1 + Pny
j=1 M(i, j ). πi,0 is the prior probability that xi is not matched to any element
in Y . We use πi to denote the vector (πi,0 , . . . , πi,ny ). Finally, let mi ∈ {0, 1, . . . . , ny } be the
latent matching variable. If mi = 1 we say that xi is matched to ymi . If mi = 0 for we say that xi
has no match in y. Our goal is to infer both, the latent variables mj ’s and cluster membership for
pairs of samples (xi , ymi )’s. The following notations are used in the rest of the paper. Lowercase
normal font, e.g x, is used for a single variable and lowercase bold font, e.g x, is used for vectors.
Uppercase bold roman letters, such as M, denote matrices. Uppercase letters, e.g X , are used to
represent random variables and E[X ] represents the expectation of a random variable X .

p(xi is not matched) =

= πi,j

= πi,0

(1)

2

3 Model

Model selection is an important problem when analyzing real world data. Many clustering algo-
rithms, including Gaussian mixture models, require as an input the number of clusters. In addition to
domain knowledge, this model selection question can be addressed using cross validation. Bayesian
nonparametric methods provide an alternative solution allowing the complexity of the model to grow
based on the amount of available data. Under- ﬁtting is addre ssed by the fact that the model allows
for unbounded complexity while over- ﬁtting is mitigated by the Bayesian assumption. We use this
approach to develop a nonparametric model for clustering and matching cross species expression
data. Our model, termed Dirichlet Process Mixture Model with Latent Matchings (DPMMLM) ex-
tends the popular Dirichlet Process Mixture Model to cases where priors are provided to matchings
between vectors to be clustered.

3.1 Dirichlet Process

Let G0 a probability measure on a measurable space. We write G ∼ DP (α, G0 ) if G is a random
probability measure drawn from a Dirichlet process (DP). The existence of the Dirichlet process was
ﬁrst proven by [10]. Furthermore, measures of G are discrete with probability one. This property
can be seen from the explicit stick-breaking construction due to Sethuraman [11] as follows.
Let (Vi )∞
i=1 and (ηi )∞
i=1 be independent sequences of i.i.d random variables: Vi ∼ Beta(1, α) and
ηi ∼ G0 . Then a random measure G deﬁned as
∞
i−1
Xi=1
Yj=1
where δη is a probability measure concentrated at η , is a random probability measure distributed
according to DP(α, G0 ) as shown in [11] .

(1 − Vj )

θi = Vi

G =

θi δηi

(2)

3.2 Dirichlet Process Mixture Model (DPMM)

Dirichlet process has been used as a nonparametric prior on the parameters of a mixture model. This
model is referred to as Dirichlet Process Mixture Model. Let z be the mixture membership indicator
variables for data variables x. Using the stick-breaking construction in (2), the Dirichlet process
mixture model is given by
G ∼ DP(α, G0 )
xi | zi , ηi ∼ F (ηi )
zi , ηi | G ∼ G
where F (ηi ) denotes the distribution of the observation xi given parameter ηi .

(3)

3.3 Dirichlet Process Mixture Model with Latent Matchings (DPMMLM)

In this section, we describe the new mixture model based on DP with latent variables for data
matching between x and y. We use FX (η), FY (η) to denote the marginal distribution of X and
Y respectively; and FX |Y (y , η) to denote the conditional distribution of X given Y . The parameter
η is a random variable of the prior distribution G0 (η | λ0 ) with hyperparameter λ0 . Also, let zi be
the mixture membership of the sample pair (xi , ymi ). Our model is given by:

G ∼ DP(α, G0 )
zi , ηi | G ∼ G
mi | πi ∼ Discrete(πi )
ymi | mi , zi , ηi ∼ FY (ηi ), if mi > 0
xi | mi , zi , ηi , y ∼ (cid:26)FX |Y (ymi , ηi )
FX (ηi )

if mi > 0
otherwise

(4)

The major difference between our model and a regular DPMM is the dependence of xi on y if

3

mi > 0. In other words the assignment of x to a cluster depends on both, its own expression levels
and the levels of the y component to which it is matched. If x is not matched to any y component
then we resort to the marginal distribution FX of the mixture.

3.4 Mean- ﬁeld variational methods

For probabilistic models, mean- ﬁeld variational methods [ 12, 13] provide a deterministic and
bounded approximation to the intractable joint probability of observed and hidden variables. Brieﬂy,
given a model with observed variables x and hidden variables h, we would like to compute log p(x),
which requires us to marginalize over all hidden variables h. Since p(x, h) is often intractable, we
can ﬁnd a tractable probability q(h) that gives the best lower bound of log p(x) using Jensen ’s
inequality:
log p(x) ≥ Zh
Maximizing this lower bound is equivalent to ﬁnding the dist ribution q(h) that minimizes the KL
divergence between q(h) and p(h | x). Hence, q(h) is the best approximation model within the
chosen parametric family.

q(h) log p(x, h) − q(h) log q(h) dh = Eq [log p(x, h)] − Eq [log q(h)]

(5)

3.5 Variational Inference for DPMMLM

Although the DP mixture model is an ”inﬁnite” mixture model,
it is intractable to solve the optimiza-
tion problem when allowing for inﬁnitely many variables. We thus follow the truncation approach
used in [14], and limit the number of cluster to K . When K is chosen to be large enough, the dis-
tribution is a drawn from the Dirichlet process [14]. To restrict the number of clusters to K , we set
VK = 1 and thus obtain θi>K = 0 in (2). The likelihood of the observed data is
nx
p(x, y | α, λ0 ) = Z
Yi=1
m,z,v,η
ny
K
i ozk
Yj=1 (cid:0)πi,j fX |Y (xi | yj , ηk )fY (yj | ηk )(cid:1)mj
Yk=1n(cid:0)πi,0 fX (xi | ηk )(cid:1)m0
i
i
where p(zi | v) = vzi Qzi−1
k=1 (1 − vk ) and v is the stick breaking variables given in Section 3.1. The
ﬁrst part of (6) p(η | λ0 ) p(v | α) is the likelihood of the model parameters and the second part is
the likelihood of the assignments to clusters and matchings.

p(η | λ0 ) p(v | α)

p(zi | v)

(6)

Following the variational inference framework for conjugate-exponential graphical models [15] we
choose the distribution that factorizes over {mi , zi }i=1,...,nx , {vk }k=1,...,K and {ηk }k=1,...,K−1 as
follows:

q(m, z, v, η) =

ny
K−1
nx
K
qθi,j (zi )mj
Yk=1
Yj=0
Yk=1
Yi=1(cid:8)qφi (mi )
i (cid:9)
where qφi (mi ) and qθi,j (zi ) are multinomial distributions and qγk (vk ) are beta distributions. These
distributions are conjugate distributions for the likelihood of the parameters in (6). qλk (ηk ) requires
special treatment due to the coupling of the marginal and conditional distributions in the likelihood.
These issues are discussed in details in section 3.5.2.

qλk (ηk )

qγk (vk )

(7)

Using this variational distribution we obtain a lower bound for the log likelihood:

+

log p(x, y | α, λ0 ) ≥ E[log p(η | λ0 )] + E[log p(V | α)]
ny
nx
K
i ](log πi,j + ρi,j,k )o − E[log q(M, Z, V, η)]
Xi=1 nE[log p(Zi | V)] +
Xk=1
Xj=0
E[M j
i Z k
where all expectations are with respect to the distribution q(m, z, v, η) and
ρi,j,k = (cid:26)E[log fX |Y (Xi | Yj , ηk )] + E[log fY (Yj | ηk )]
E[log fX (Xi | ηk )]

if j > 0
if j = 0

(8)

4

To compute the terms in (8), we note that

E[log p(Zi | V)] =

E[M j
i Z k
i ] = φi,j θi,j,k = ψi,j,k
K
Xk=1
t=k+1 ψi,j,t and q(zi = k) = Pny
where q(zi > k) = Pny
j=0 PK
j=0 ψi,j,k .
3.5.1 Coordinate ascent inference algorithm

q(zi > k)E[log(1 − Vk )] + q(zi = k)E[log Vk ]

The lower bound above can be optimized by a coordinate ascent algorithm. The update rules for
all terms except for the qλ (η), are presented below. These are direct applications of the variational
inference for conjugate-exponential graphical models [15]. We discuss the update rule for qλ (η) in
section 3.5.2.

• Update for qγk (vk ):

ny
nx
Xi=1
Xj=0
• Update for qθi,j (zi ) and qφi (mi ):

γk,1 = 1 +

ψi,j,k

γk,2 = α +

nx
Xi=1

ny
Xj=0

K
Xt=k+1

ψi,j,t

k−1
Xk=1
E[log(1 − Vk )] + E[log Vk ](cid:1)
θi,j,k ∝ exp (cid:0)ρi,j,k +
k−1
K
φi,j ∝ exp (cid:16) log πi,j +
E[log(1 − Vk )] + E[log Vk ](cid:1)(cid:17)
Xk=1
Xk=1
θi,j,k (cid:0)ρi,j,k +
3.5.2 Application of the model to multivariate Gaussians

The previous sections described the model in a general terms. In the rest of this section, and in
our experiments, we focus on data that is assumed to be distributed as a multivariate Gaussian with
unknown mean and covariance matrix. The prior distribution G0 is then given by the conjugate prior
Gaussian-Wishart distribution. In a classical DP Gaussian Mixture Model with Gaussian-Wishart
prior, the posterior distribution of the parameters could be computed analytically. Unfortunately,
in our model, the coupling of the conditional and marginal distribution in the likelihood makes it
difﬁcult to derive analytical formulas for the posterior di stribution. Note that if (X, Y ) ∼ N (µ, Σ)
ΣY (cid:19) then X ∼ N (µX , ΣX ), Y ∼ N (µY , ΣY ) and
with µ = (µX , µY ) and Σ = (cid:18) ΣX ΣX Y
ΣY X
Y (y − µY ), ΣX − ΣX Y Σ−1
X |Y = y ∼ N (µX + ΣX Y Σ−1
(9)
Y ΣY X ).
Therefore, we introduce an approximation distribution for the datasets which decouples the marginal
and conditional distributions as follows:
fY (y | µY , ΛY ) = N (µY , Σ = Λ−1
fX (x | µX , ΛX ) = N (µX , Σ = Λ−1
X )
Y )
fX |Y (x | y , W, b, µX , ΛX ) = N (µX + b − Wy , Σ = Λ−1
X )
where W is a px × py projection matrix and Λ is the precision matrix. In this approximation, we
assume that the covariance matrices of X and X |Y are the same. In other words, the covariance
of X is independent of Y . The matrix W models the linear correlation of X on Y , similar to
−ΣX Y Σ−1
Y in (9).
The priors for µX , ΛX and µY , ΛY are given by Gaussian-Wishart(GW) distributions. A ﬂat im-
proper prior is given to W and b, p0 (W) = 1, p0 (b) = 1 for all W, b. These assumptions lead
to decoupling of the marginal and conditional distributions. Therefore, the distribution qλk (ηk ) can
now be factorized into two GW distributions and a distribution of W. To avoid over-cluttering
symbols, we omit the subscript k of the speci ﬁc cluster k .
q∗
λk (ηk ) = GW (µX , ΛX ) GW (µY , ΛY ) g(W) g(b)

5

Posterior distribution of µY , ΛY : The update rules follow the standard posterior distribution of
Gaussian-Wishart conjugate priors.
Posterior distribution of µX , ΛX and W, b: Due to the coupling of µX , ΛX with W, we do a
coordinate ascent procedure to ﬁnd the optimal posterior di stribution. The posterior distribution of
W, b is a singleton discrete distribution g such that g(W∗ ) = 1, g(b∗ ) = 1.

• Update for posterior distribution of µX , ΛX :

κX = κX 0 + nX

mX =

1
κX

(κX 0mX 0 + nX x)

VX =

where nX =

ψi,j,k , x =

(x − mX 0 )(x − mX 0 )T

νX = νX 0 + nX

S−1
X = S−1
X 0 + VX +

κX 0nX
κX 0 + nX
ny
ny
nx
nx
1
Xj=0
Xn=1
Xj=1
Xi=1 (cid:0)ψi,0,k xi +
ψi,j,k (xi − b + W∗ yj )(cid:1) and
nX
ny
nx
Xj=1
Xi=1 (cid:8)ψi,0,k (xi − x)(xi − x)T +
ψi,j,k (xi − b+W∗ yj − x)(xi − b+W∗ yj − x)T (cid:9).
• Update for W∗ , b∗ : We ﬁnd W∗ , b∗ that maximizes the log likelihood. Taking the deriva-
tive with respect to W∗ and solving for W∗ , we get
ny
nx
nx
W∗ = (cid:16)
j (cid:17)(cid:16)
Xi=1
Xi=1
Xj=1
ψi,j,k (xi − mX − b)yT
ny
nx
b∗ = −(cid:16)
ψi,j,k (xi − mX + W∗ yj )(cid:17)/
Xi=1
Xj=1

j (cid:17)−1
ψi,j,k yj yT

ny
Xj=1

ψi,j,k

ny
Xj=1
nx
Xi=1

4 Experiments and Results

4.1 Simulated data

We demonstrate the performance of the model in identifying data matchings as well as cluster mem-
bership of datapoints using simulated data. To generate a simulated dataset, we sample 120 data-
points from a mixture of three 5-dimensional Gaussians with separation coefﬁcient = 2 leading to
well separated mixtures1 . The covariance matrix was derived from the autocorrelation matrix for
a ﬁrst-order autoregressive process leading to highly depe ndent components (ρ = 0.9). From these
samples, we use the ﬁrst 3 dimensions to create 120 datapoint s x = [x1 , . . . , x120 ]. The last two
dimensions of the ﬁrst 100 datapoints are used to create y = [y1 , . . . , y100 ] (note that there are no
matches for 20 points in x). Hence, the ground truth M matrix is a diagonal 120 × 100 matrix.
We selected a large value for the diagonal entries (τ = 1000) in order to place a strong prior for
the correct matchings. Next, for t = 0, . . . , 20, we randomly select t entries on each row of M
2 r , where r ∼ χ2
1 . We repeat the process 20 times for each t to compute the
and set them to τ
mean and standard deviation shown in Figure 1(a) and Figure 1(b). We compare the performance
of our model(DPMMLM) with a standard Dirichlet Process Mixture Model where each component
in x is matched based on the highest prior: {(xi , yj ∗ ) | i = 1, . . . , 100 and j ∗ = argmaxjM(i, j )}
(DPMM). For all models, the truncation level (K ) is set to 20 and α is 1. Figure 1(a) presents the
percentage of correct matchings inferred by DPMMLM and the highest prior matching. For DP-
MMLM, a datapoint xi is matched to the datapoint yj with the largest posterior probability φi,j .
With the added noise, DPMMLM can still achieve an accuracy of 50% when the highest prior
matching leads to only 25% accuracy. Figure 1(b) and 1(c) show the Normalized Mutual Informa-
tion (NMI) and Adjusted Rand index [17] for the clusters inferred by the two models compared to
the true clusters. As can be seen, while the percentage of correct matchings decreased with the added
noise, DPMMLM still achieves high NMI of 0.8 and Adjusted Rand index of 0.92. In conclusion,
by relying on matchings of points DPMMLM can still performs very well in terms of its ability to
identify correct clusters even with the high noise levels.

1Following [16], a Gaussian mixture is c-separated if for each pair (i, j ) of components, kmi − mj k2 ≥
) , where λmax denotes the maximum eigenvalue of their covariance.
c2D max(λmax
, λmax
i
j

6

DPMMLM
Top matches

100

90

80

70

60

50

40

30

s
g
n
i
h
c
t
a
m
t
c
e
r
r
o
c
f
o
%

20
0

5
15
10
Number of random entries per row (t)
(a) The % of correct matchings.

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M
d
e
z
i
l
a
m
r
o
N

0.1
0

DPMMLM
DPMM

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

x
e
d
n
i
d
n
a
R
d
e
t
s
u
j
d
A

DPMMLM
DPMM

5
15
10
Number of random entries per row (t)
(b) Normalized MI.

20

0.1
0

5
15
10
Number of random entries per row (t)
(c) Adjusted Rand index.

20

Figure 1: Evaluation of the result on simulated data.

4.2

Immune response dataset

Cluster 1

Cluster 2

Cluster 3

Cluster 1

Cluster 2

2 4 6 8
Cluster 4

2 4 6 8
Cluster 5

2 4 6 8

2 4 6 8
Cluster 3

2 4 6 8

4

2

0

−2

−4

4

2

0

−2

−4

2 4 6 8

(b) DPMM

2 4 6 8

2 4 6 8

(a) DPMMLM

Figure 2: The heatmap for clusters inferred for the immune response dataset.

We compared human and mouse immune response datasets to identify similar and divergent genes.
We selected two experiments that studied immune response to gram negative bacteria. The ﬁrst was
a time series of human response to Salmonella [18]. Cells were infected with Salmonella and were
proﬁled at: 0.5h, 1h, 2h, 3h and 4h. The second looked at mouse response to Yersinia enterocolitica
with and without treatment by IFN-γ [19]. We used BLASTN to compute the sequence similarity
(bit-score) between all human and mouse genes. For each species we selected the most varying 500
genes and expanded the gene list to include all matched genes in the other species with a bit score
greater than 75. This led to a set of 1476 human and 1967 mouse genes which we compared using
our model. The M matrix is the bit scores between human and mouse genes thresholded at 75.
The resulting clusters are presented in Figure 2(a). In that ﬁgure, the ﬁrst ﬁve dimensions are human
expression values and each gene in human is matched to the mouse gene with the highest posterior.
Human genes which are not matched to any mouse gene in the cluster have a blank line on the
mouse side of the ﬁgure. The algorithm identi ﬁed ﬁve differe
nt clusters. Clusters 1, 4 and 5 display
a similar expression pattern in human and mouse with genes either up or down regulated in response
to the infection. Genes in cluster 2 differ between the two species being mostly down regulated in
humans while slightly upregulated in mouse. Human genes in cluster 3 also differ from their mouse
orthologs. While they are strongly upregulated in humans, the corresponding mouse genes do not
change much.

7

Corrected P GO term description
P value
regulation of apoptosis
2.86216e-10 <0.001
regulation of cell death
4.97408e-10 <0.001
protein binding
7.82427e-10 <0.001
regulation of programmed cell death
4.14320e-10 <0.001
positive regulation of cellular process
4.49332e-09 <0.001
4.77653e-09 <0.001
positive regulation of biological process
response to chemical stimulus
8.27313e-09 <0.001
1.17013e-07
0.001
cytoplasm

1.28299e-07
2.20104e-07

0.001
0.001

response to stress
cell proliferation

P value
5.06685e-07
6.15795e-07
7.70651e-07
7.78266e-07
1.09778e-06
1.42704e-06
1.91735e-06
3.23244e-06

3.39901e-06
3.66178e-06

Corrected P GO term description
response to stimulus
0.001
negative regulation of biological process
0.001
cellular process
0.001
regulation of localization
0.002
response to organic substance
0.002
0.002
collagen metabolic process
negative regulation of cellular process
0.003
multicellular organismal macromolecule
0.005
metabolic process
interspecies interaction
negative regulation of apoptosis

0.005
0.005

Table 1: The GO enrichment result for cluster 1 identi ﬁed by D PMMLM.

We used the Gene Ontology (GO, www.geneontology.org) to calculate the enrichment of functional
categories in each cluster based on the hypergeometric distribution. Genes in cluster 1 (Table 1)
are associated with immune and stress responses. Interestingly the most signi ﬁcant category for
this cluster is ”regulation of apoptosis ” (corrected p-val ue <0.001). Indeed, both Salmonella and
Yersinia are known to induce apoptosis in host cells [20]. When clustering the two datasets indepen-
dently the p-value for this category is greatly reduced indicating that accurate matchings can lead to
better identi ﬁcation of core pathways (see Appendix). Clus ter 4 contains the most coherent set of
upregulated genes across the two species. One of top GO categories for this cluster is ’response to
molecule of bacterial origin’ (corrected p-value < 0.001) which is the most accurate description of
the condition tested. See Appendix for complete GO tables of all clusters. In contrast to clusters in
which mouse and human genes are similarly expressed, cluster 3 genes are strongly upregulated in
human cells while not changing in mouse. This cluster is enriched for ribosomal proteins (corrected
p-value <0.001). This may indicate different strategies utilized by the bacteria in the two experi-
ments. There are studies that show that pathogens can upregulate the synthesis of ribosomal genes
(which are required for translation) [21] whereas other studies indicate that ribosomal genes may not
change much, or may even be reduced, following infection [22]. The results of our analysis indicate
that while following Salmonella infection in human cells ribosomal genes are upregulated, they are
not activated following Yarsinia infection in mouse.

We have also analyzed the matchings obtained using sequence data alone (prior) and by combining
sequence and expression data (posterior) using our method. The top posterior gene is the same
as the top prior gene in most cases (905 of the 1476 human genes). However, there are several
cases in which the prior and posterior differ. 293 human genes are not matched to any mouse
gene in the cluster they are assigned to indicating that they are expressed in a species dependent
manner. Additionally, for 278 human genes the top posterior and prior mouse gene differ. To test
whether these differences inferred by the algorithm are biologically meaningful we compared our
Dirichlet method to a method that uses deterministic assignments, as was done in the past. Using
such assignments the algorithm identi ﬁed only three cluste rs as shown in Figure 2(b). Neither of
these clusters looked homogenous across species.

5 Conclusions

We have developed a new model for simultaneously clustering and matching genes across species.
The model uses a Dirichlet Process to infer the number of clusters. We developed an efﬁcient
variational inference method that scales to large datasets with almost 2000 datapoints. We have
also demonstrated the power of our method on simulated data and immune response dataset. While
the method was presented in the context of expression data it is general and can be used for other
matching tasks in which a prior can be obtained. For example, when trying to determine a caption
for images extracted from webpages a prior can be obtained by relying on the distance between the
image and the text on the page. Next, clustering can be employed to utilize the abundance of images
that are extracted and improve the matching outcome.

Acknowledgments

We thank the anonymous reviewers for constructive and insightful comments. This work is sup-
ported in part by NIH grant 1RO1 GM085022 and NSF grants DBI-0965316 and CAREER-0448453
to Z.B.J.

8

References

[1] M. Kellis, N. Patterson, M. Endrizzi, B. Birren, and E. S. Lander. Sequencing and comparison
of yeast species to identify genes and regulatory elements. Nature, 423:241–254, May 2003.
[2] L. J. Jensen, T. S. Jensen, U. de Lichtenberg, S. Brunak, and P. Bork. Co-evolution of tran-
scriptional and post-translational cell-cycle regulation. Nature, 443:594–597, Oct 2006.
[3] G. Lelandais et al. Genome adaptation to chemical stress: clues from comparative transcrip-
tomics in Saccharomyces cerevisiae and Candida glabrata. Genome Biol., 9:R164, 2008.
[4] G. Quon, Y. W. Teh, E. Chan, M. Brudno, T. Hughes, and Q. D. Morris. A mixture model
for the evolution of gene expression in non-homogeneous datasets.
In Advances in Neural
Information Processing Systems, volume 21, 2009.
[5] J. M. Stuart, E. Segal, D. Koller, and S. K. Kim. A gene-coexpression network for global
discovery of conserved genetic modules. Science, 302:249–255, Oct 2003.
[6] Sven Bergmann, Jan Ihmels, and Naama Barkai. Similarities and differences in genome-wide
expression data of six organisms. PLoS Biol, 2(1):e9, 12 2003.
[7] Y. Lu, R. Rosenfeld, and Z. Bar-Joseph.
Identifying cycling genes by combining sequence
homology and expression data. Bioinformatics, 22:e314–322, Jul 2006.
[8] Y. Lu, R. Rosenfeld, G. J. Nau, and Z. Bar-Joseph. Cross species expression analysis of innate
immune response. J. Comput. Biol., 17:253–268, Mar 2010.
[9] R. Sharan et al. Conserved patterns of protein interaction in multiple species. Proc. Natl. Acad.
Sci. U.S.A., 102:1974–1979, Feb 2005.
[10] Thomas S. Ferguson. A bayesian analysis of some nonparametric problems. The Annals of
Statistics, 1(2):209–230, 1973.
[11] J. Sethuraman. A constructive deﬁnition of dirichlet p riors. Statistica Sinica, 4:639–650, 1994.
[12] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An in-
troduction to variational methods for graphical models. Machine Learning, 37(2):183–233,
November 1999.
[13] Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and
variational inference. Found. Trends Mach. Learn., 1(1-2):1–305, 2008.
[14] H. Ishwaran and James. Gibbs sampling methods for stick breaking priors. Journal of the
American Statistical Association, pages 161–173, March 2001.
[15] Zoubin Ghahramani and Matthew J. Beal. Propagation algorithms for variational bayesian
learning. In In Advances in Neural Information Processing Systems 13, pages 507–513. MIT
Press, 2001.
[16] Sanjoy Dasgupta. Learning mixtures of gaussians.
In FOCS ’99: Proceedings of the 40th
Annual Symposium on Foundations of Computer Science, Washington, DC, USA, 1999.
[17] M. Meila. Comparing clusterings by the variation of information.
In Learning theory and
Kernel machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop,
COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003: proceedings, page 173.
Springer Verlag, 2003.
[18] C. S. Detweiler et al. Host microarray analysis reveals a role for the Salmonella response
regulator phoP in human macrophage cell death. Proc. Natl. Acad. Sci. U.S.A., 98:5850–5855,
May 2001.
[19] K. van Erp et al. Role of strain differences on host resistance and the transcriptional response
of macrophages to infection with Yersinia enterocolitica. Physiol. Genomics, 25:75–84, 2006.
[20] D. M. Monack, B. Raupach, et al. Salmonella typhimurium invasion induces apoptosis in
infected macrophages. Proc. Natl. Acad. Sci. U.S.A., 93:9833–9838, Sep 1996.
[21] O. O. Zharskaia et al. [Activation of transcription of ribosome genes following human embryo
ﬁbroblast infection with cytomegalovirus in vitro]. Tsitologiia, 45:690–701, 2003.
[22] J. W. Gow, S. Hagan, P. Herzyk, C. Cannon, P. O. Behan, and A. Chaudhuri. A gene signature
for post-infectious chronic fatigue syndrome. BMC Med Genomics, 2:38, 2009.

9


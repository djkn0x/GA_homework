HUMAN ACCURACY ANAYLSIS ON THE AMAZON MECHANICAL TURK

JASON CHEN, JUSTIN HSU, STEFAN WAGER

Platforms such as the Amazon Mechanical Turk (AMT) make it easy and cheap to gather human
input for machine learning problems. The ﬂipside, however, is that some human contributors can be confused
or inaccurate. The challenge then becomes to extract trustworthy information from the answers of more or
less accurate contributors.
Image-Net is an eﬀort to organize images collected online into hierarchal synonym sets (synsets), such
as dog or jigsaw puzzle. Image-Net then employs AMT turkers to label whether the images it has gathered
are actually typical representatives of the synset they were grouped under.
Since the turker labels are not 100% accurate, Image-Net currently determines whether an image is
correctly labeled by ma jority vote.
In this pro ject, we consider machine learning algorithms that could
enable us to identify the more accurate turkers, and to recognize more typical representatives of synsets
using the available AMT votes. First, we present an unsupervised EM algorithm that assigns accuracies to
individual turkers and assumes that their votes are Bernoulli-distributed. Second, we present a supervised
algorithm that identiﬁes a few dozen key images, which we correctly label, and then uses this information
to identify trustworthy turkers. The supervised approach produced some very promising results, especially
in synsets with ambiguous deﬁnitions, such as bishop (the chess piece); the unsupervised results were a bit
more mixed.
In both our approaches, we make the following assumptions: ﬁrst, we assume that each of our voters has
a consistent accuracy level; second, we assume that the ma jority is more often correct than not. We then
attempt to identify turkers who vote with the ma jority most often, infer that they are our best labelers, and
follow these trusted labellers in ambiguous cases.

Unsupervised Learning Approaches

First Approaches: Naive EM and Skill Scoring. Before settling on our ﬁnal design, we attempted a
Naive EM approach. Brieﬂy, we assume each turker has a weight ci corresponding to their accuracy. We
set the weights to be the log odds of the fraction of the turker’s votes that are correct, then we adjusted
the labels of the pictures based on the weighted ma jority vote. Though this seemed promising at ﬁrst,
we encountered one of the main diﬃculties of this pro ject: too many negative images. Since most of the
images the turkers are presented with are negative examples, a turker who votes ’No’ to all images will score
very well. Thus, these people will have disproportionately high weights, skewing our labels, and leading to
mediocre performance.
To remedy this, we considered using a skill scoring approach, in which we roughly estimate the fraction
of positive images. With this number, we can calculate baseline performances of turkers, if they simply
always vote Yes or always vote No. By taking this baseline performance into account, we can decrease the
bias. The main problem with this approach is that it isn’t clear how to normalize the performances. A
turker’s maximum performance depends not only on how many images she votes on, but also on the votes
she casts, as it is easier to get a No vote correct than a Yes vote. After trying several normalizing schemes,
we found that the results were much more promising than the ﬁrst approach, with poor performance only
in identifying false positives.

Final Approach: Speciﬁcity and Sensitivity EM. We went back to our ﬁrst approach, and looked for
a model in which voters were expected to score correctly at a high rate when voting No, and to correct for

Date : November 2010.

1

2

JASON CHEN, JUSTIN HSU, STEFAN WAGER

i (1 − αj )1−yj
(αj )yj
i

i (1 − β j )yj
(β j )1−yj
i

ai := Pr[y1
i , . . . , yRi
i

|yi = 1, α] =

Pr[y1
i , . . . , yRi
i

|θ ] =

i |yi = 0, β j ] =
Pr[y j

i |yi = 1, αj ] =
Pr[y j

this bias. Our idea was to have diﬀerent weights for when turkers voted Yes, and for when turkers voted No.
This led to the following model (derivation closely follows [1]):
Let y = 0, 1 be the ground truth of a given image, and y j = 0, 1 is voter j labeled the image as. For voter
j , we associate parameters αj = Pr[y j = 1|y = 1], β j = Pr[y j = 0|y = 0], known as the sensitivity and the
speciﬁcity, respectively. We also have one more hidden parameter, p = Pr[y = 1], or the fraction of typical
images amongst all images. Now, if D is the observed votes, yuj
is the vote given to picture i by turker
i
uj , the j th turker to vote on this picture, and θ are the parameters αj , β j , p, we have maximum likelihood
o
n
NY
NY
function:
|yi = 1, α] · p + Pr[y1
|yi = 0, β ] · (1 − p)
Pr[D |θ ] =
Pr[y1
i , . . . , yRi
i , . . . , yRi
i
i
i=1
i=1
RiY
RiY
Independence of votes gives:
RiY
RiY
j=1
j=1
|yi = 0, β ] =
bi := Pr[y1
i , . . . , yRi
Thus we want to maximize Pr[D |θ ] = QN
i
we get that the log-likelihood is PN
j=1
j=1
i=1 aip + bi (1 − p). Denoting the ground truth as y = [y1 , . . . , yN ],
i=1 yi ln piai + (1 − yi ) ln(1 − pi )bi
E-step: We calculate the expected value of this log-likelihood (this will be our lower bound for the true
log-likelihood). Taking the expectation with respect to Pr[y |D , θ ] = Qi (y), we have
NX
i=1
where µi = Pr[yi = 1|y1
, θ ]. By Bayes, we update
i , . . . , yRi
i
µi ∝ Pr[y1
|yi = 1, θ ] · Pr[yi = 1]
aip
i , . . . , yRi
aip + bi (1 − p)
i
M-step: We wish to maximize the lower bound calculated in the E-step with respect to the parameters
PN
PN
NX
αj , β j . By setting the α and β gradients of (1) to 0 and update p, we get:
PN
PN
i=1 (1 − µi )(1 − yk
i )
i=1 µi yk
i
PN
i=1 (1 − µi )
i=1 µi
PRi
i=1
i=1 µi every iteration, we can run this until convergence, initializing µi to be
By reestimating p = 1
N
the ma jority vote µi = Pr[yi = 1|y1
j=1 y j
] = 1
i To calculate our ﬁnal labels, since µi is the
i , . . . , yRi
i
N
probability that image i is typical, we can set a threshold γ and mark images that have µi > γ as typical.

µi ln piai + (1 − µi ) ln(1 − pi )bi

p :=

1
N

µi

αk :=

β k :=

(1)

E[ln Pr[D , y |θ ]] =

⇒ µi :=

Results. The results from our unserpervised approach were somewhat mixed. For some synsets, we managed
to perform better than ma jority vote; on others, though, we generated a large number of false positives.
Below are results for the following three synsets, as well as their deﬁnitions according to Image-Net:
• Caterpil lar, Cat : a large tracked vehicle that is propelled by two endless metal belts; frequently used
for moving earth in construction and farm work.
• Bishop : (chess) a piece that can be moved diagonally over unoccupied squares of the same color.
• Horseshoe : U-shaped plate nailed to underside of horse’s hoof.
For each synset, we record the number of positives the algorithm recognized, the fraction of images that
were unambiguously typical, and the false positive rate. We did not count ambiguous images in either of
these categories.

HUMAN ACCURACY ANAYLSIS ON THE AMAZON MECHANICAL TURK

3

Figure 1. Sensitivity (left) and speciﬁcity (right) of users in the Caterpil lar, Cat synset

Results
Images Recognized Deﬁnitely Typical False Positives
13%
66%
531
Cat-Cat: Unsupervised
579
45%
39%
Cat-Cat: Ma jority Vote
43 %
52%
280
Bishop: Unsupervised
3%
91%
167
Bishop: Ma jority Vote
1422
75%
16%
Horseshoe: Unsupervised
1248
72%
13%
Horseshoe: Ma jority Vote
Our algorithm worked fairly well for Caterpil lar, Cat. However, it performed poorly for Bishop. We
suspect this is because a very small fraction of the images presented to the AMT are in fact typical: under
10%. This means that voting on no-images is very easy, while getting a high score on yes-images is hard.
Thus, most turkers have very high speciﬁcities but low sensitivities.
Such numbers tend to push split votes toward a yes-decision, since our high speciﬁcity and low sensitivity
reﬂect an assumption by the model that false positives are quite unlikely, whereas false negatives are common.
If we try to ﬁx this problem by computing only a single credibility for each turker (as we did in our ﬁrst
model), we run into another related problem: all the easy no-votes give most turkers an artiﬁcially high
credibility.
A challenge for unsupervised learning approaches to the Image-Net dataset is that the interesting images
are swamped by a large mass of easy no-images from which it is diﬃcult to learn anything new. In the next
section, we present an alternative approach that only learns turker accuracies from contentious, or diﬃcult
images.

Supervised Learning Approach

We suspect our work unsupervised learning algorithms ran into diﬃculties for the following reasons:
• Learning contributor accuracies from all images is not very eﬀective, because interesting, diﬃcult
images are swamped by easy images on which everyone votes correctly.
• Learning contributor accuracies from the most diﬃcult images in an unsupervised way can be dan-
gerous, since even a large ma jority can be wrong on diﬃcult images.
One way to overcome these problems is to automatically ﬁnd the most diﬃcult and contentious images,
and then have an accredited expert provide the ground truth on these images. Though this seems to defeat
the purpose of using AMT input, we think that this approach is valuable if a researcher really needs accurate
labels on a set of tens of thousands of images and has time to manually label a few dozen images per synset
to train the supervised learning algorithm. After these labels are set, the full collection of AMT votes can
then be used eﬀectively.

4

JASON CHEN, JUSTIN HSU, STEFAN WAGER

Our idea is an example of a collaborative ﬁltering approach, in that we estimate of how each turker
might have voted on our small labeled training set by comparing them to turkers that actually voted on our
training set. We then compute weights for each turker using a Bernoulli maximum likelihood estimate, and
infer labels for our images using a weighted ma jority vote.
Speciﬁcally, we ﬁrst pick 16 images with a negative ma jority vote and 16 images with a positive ma jority
vote, while maximizing the quantity (#yes-votes × #no-votes) for each image. These are our contentious
images form our key images, which we label manually: for each key image kl we provide a deﬁnite label
L(kl ) = 0, 1.
To compute weights for all turkers, we ﬁrst need to to obtain predictions ˜Aikl for how each turker i would
have voted on key image kl . Clearly, if i voted on kl , then ˜Aikl = Aikl . However, if i did not vote on
kl , we need to somehow guess how i would have voted on kl . To do this, we ﬁrst compute the similarity
coeﬃcient uij for all turkers i and j :
P
P
k∈images AikAjk
k∈images |AikAjk |
i /∈ {lj } on kl : ˜Aikl = P
Assuming that turkers {lj } voted on key image kl , we use these parameters to give a guess for the vote of
j uilj Alj kl . We are now ready to compute weights for each turker. Assuming that
the votes by turker i are Bernoulli-distributed such that they are accurate with probability ci , we get the
following experssion to maximize:Y
i,kl

· (1 − ci )max(−L(kl )Aikl ,0)
P
P
L(kl )Aikl
|L(kl )Aikl | . Taking the log of the likelihood
With a little math (see milestone), we get that ci =
kl
kl
). We do not let additive weights drop below
function, we can also ﬁnd additive weights wi = log( ci
1−ci
zero; instead, we ignore turkers with negative weights. Having computed additive weights for each turker as
described, we proceed to a weighted ma jority vote with which we infer labels for the remaining images.

uij =

max(L(kl )Aikl ,0)
c
i

Results. The results using our supervised algorithm were quite promising. Especially for very confusing
synsets, such as Caterpil lar, Cat, our algorithm did a good job at weeding out turkers working under a wrong
deﬁnition. Below are some detailed results; the Yes and No on the second row indicate the decisions by our
algorithm.

Ma jority Vote Yes
Ma jority Vote No
Yes
No
Yes
No
Caterpillar Cat
282
297
159
12232
Total Number of Images
100
100
100
100
Sample Size
8
78
2
55
Number of Deﬁnite Positive
20
13
38
3
Number of Ambiguous
79
2
95
7
Number of Deﬁnite Negative
Many images, such as vehicles with wheels manufactured by Cat, or vehicles with tracks but not directly
useable for farming, were ambiguous as to whether they should be considered to represent Caterpil lar, Cat
or not; we labeled these ambiguous. The sample size row indicates how many images we looked at in each
category to evaluate our algorithm. Evidently, in all the cases where our algorithm disagreed with the
ma jority vote, it had less than 1 chance in 10 of making a non-ambiguous mistake.
From the point of view of Image-Net, which is trying to accumulate a large and accurate database of
images, the two most important metrics are the number of positive images recognized, and the false positive
rate. We provide such statistics below for the 3 synsets we worked with:

HUMAN ACCURACY ANAYLSIS ON THE AMAZON MECHANICAL TURK

5

Images Recognized Deﬁnitely Typical False Positives
Results
4%
70%
464
Cat-Cat: Supervised
39%
45%
579
Cat-Cat: Ma jority Vote
190
89%
0%
Bishop: Supervised
3%
91%
167
Bishop: Ma jority Vote
12%
73%
1259
Horseshoe: Supervised
1248
72%
13%
Horseshoe: Ma jority Vote
Our algorithm performed quite well on both Caterpil lar, Cat and Bishop. We notice that both of these
synsets have somewhat confusing deﬁnitions: some might mistakenly believe that the ﬁrst applies to larvae
of butterﬂies, and the second to Catholic clergy. Our hypothesis is that our algorithm managed to eliminate
confused voters, and thus clean up the vote dataset.
Our algorithm does not, however, perform nearly as well on Horseshoe. The diﬃculty in picking good
images of horseshoes is not so much due to confusing deﬁnitions, but rather in learning what to do with
limit cases such as horseshoe shaped necklaces. These results suggest that our algorithm is good at picking
out confused turkers, but not so good at picking out turkers without a keen attention to detail.
As support for the hypothesis that our algorithm works essentially by eliminating confused labellers,
consider the following numbers: With Caterpil lar, Cat, we gave 20 out of 80 turkers a weight of 0, and with
Bishop, 10 out of 28 voters got no weight. On the other hand, with Horseshoe, we only gave 4 out of 156
turkers a weight of 0. This suggests that we did poorly on the latter synset because we couldn’t identify the
turkers who were consistently wrong on a ma jority of the key images.

Discussion and Further Works

Our results show that machine learning techinques can successfully be applied to improve the value of
votes gathered on AMT. Our supervised approach, in particular, makes it easy for a researcher who is willing
to spend about a minute labelling images from a synset to eliminate data from turkers who do not share her
understanding deﬁnitions of the synsets.
Our unsupervised approach presents some interesting challenges. The central diﬃculty is that, for many
synsets, the number of bad images swamps the number of interesting images. If we give turkers a simple
Bernoulli accuracy, the high number of trivially bad images makes this accuracy artiﬁcially high. Conversely,
if we try to give each turker a separate sensitivity and speciﬁcity, we end up giving each turker an extremely
high speciﬁcity. This results in unwanted false positives, since having very high speciﬁcities makes it unlikely
that any image having received many yes-votes could be a negative.
The success of the supervised approach suggests that it is most useful to infer turker accuracies from
their performance on the most diﬃcult images. An interesting problem for further study would be to work
on unsupervised learning algorithms that focus more closely on contentious images. One way this could be
implemented is by ignoring all images with unanimous votes while learning accuracies. Another approach
could be to introduce a parameter for the diﬃculty of each image–the probability of a turker making a
mistake could then depend both on her accuracy and on the diﬃculty of the image. See, for example, [2] for
an idea similar to the latter.
Our results show that there is much promise and much room for improvement in automatic AMT vote
analysis. Since AMT is one of the most promising ways of collecting machine learning data on a large scale,
it is critical that we ﬁnd good ways of analyzing AMT data as eﬃciently as possible. We look forward to
seeing what approaches will surface in the literature over the next years.

References

[1] V. Rayker, et al. Learning From Crowds, Journal of Machine Learning Research, 11 (2010), 1297–1322
[2] J. Whitehill, et al. Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise,
Advances in Neural Information Processing Systems (forthcoming), 2009.


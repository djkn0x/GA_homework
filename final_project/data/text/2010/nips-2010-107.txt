Guaranteed Rank Minimization via Singular Value
Projection

Prateek Jain
Microsoft Research Bangalore
Bangalore, India
prajain@microsoft.com

Raghu Meka
UT Austin Dept. of Computer Sciences
Austin, TX, USA
raghu@cs.utexas.edu

Inderjit Dhillon
UT Austin Dept. of Computer Sciences
Austin, TX, USA
inderjit@cs.utexas.edu

Abstract

Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental
problem with many important applications in machine learning and statistics. In
this paper we propose a simple and fast algorithm SVP (Singular Value Projec-
tion) for rank minimization under afﬁne constraints (ARMP) and show that SVP
recovers the minimum rank solution for afﬁne constraints that satisfy a restricted
isometryproperty (RIP). Our method guarantees geometric convergence rate even
in the presence of noise and requires strictly weaker assumptions on the RIP con-
stants than the existing methods. We also introduce a Newton-step for our SVP
framework to speed-up the convergence with substantial empirical gains. Next,
we address a practically important application of ARMP - the problem of low-
rank matrix completion, for which the deﬁning afﬁne constraints do not directly
obey RIP, hence the guarantees of SVP do not hold. However, we provide partial
progress towards a proof of exact recovery for our algorithm by showing a more
restricted isometry property and observe empirically that our algorithm recovers
low-rank incoherent matrices from an almost optimal number of uniformly sam-
pled entries. We also demonstrate empirically that our algorithms outperform ex-
isting methods, such as those of [5, 18, 14], for ARMP and the matrix completion
problem by an order of magnitude and are also more robust to noise and sampling
schemes. In particular, results show that our SVP-Newton method is signiﬁcantly
robust to noise and performs impressively on a more realistic power-law sampling
scheme for the matrix completion problem.

1 Introduction

In this paper we study the general afﬁne rank minimization problem (ARMP),
s.t A(X ) = b, X ∈ Rm×n , b ∈ Rd ,
min rank(X )
where A is an afﬁne transformation from Rm×n to Rd .
The afﬁne rank minimization problem above is of considerable practical interest and many important
machine learning problems such as matrix completion, low-dimensional metric embedding, low-
rank kernel learning can be viewed as instances of the above problem. Unfortunately, ARMP is
NP-hard in general and is also NP-hard to approximate ([22]).
Until recently, most known methods for ARMP were heuristic in nature with few known rigorous
guarantees.
In a recent breakthrough, Recht et al. [24] gave the ﬁrst nontrivial results for the

(ARMP)

1

problem obtaining guaranteed rank minimization for afﬁne transformations A that satisfy a restricted
isometryproperty (RIP). Deﬁne the isometry constant of A, δk to be the smallest number such that
for all X ∈ Rm×n of rank at most k ,
≤ (1 + δk )∥X ∥2
≤ ∥A(X )∥2
(1 − δk )∥X ∥2
(1)
F .
F
2
The above RIP condition is a direct generalization of the RIP condition used in the compressive
sensing context. Moreover, RIP holds for many important practical applications of ARMP such
as image compression, linear time-invariant systems. In particular, Recht et al. show that for most
natural families of random measurements, RIP is satisﬁed even for only O(nk log n) measurements.
Also, Recht et al. show that for ARMP with isometry constant δ5k < 1/10, the minimum rank
solution can be recovered by the minimum trace-norm solution.
In this paper we propose a simple and efﬁcient algorithm SVP (Singular Value Projection) based
on the projected gradient algorithm. We present a simple analysis showing that SVP recovers the
minimum rank solution for noisy afﬁne constraints that satisfy RIP and prove the following guar-
antees. (Independent of our work, Goldfarb and Ma [12] proposed an algorithm similar to SVP.
√
However, their analysis and formulation is different from ours. They also require stronger isometry
30, than our analysis.)
assumptions, δ3k < 1/
Theorem 1.1 Suppose the isometry constant of A satisﬁes δ2k < 1/3 and let b = A(X
∗
⌈
⌉
) for a
∗ . Then, SVP (Algorithm 1) with step-size ηt = 1/(1 + δ2k ) converges to X
∗ .
rank-k matrix X
Furthermore, SVP outputs a matrix X of rank at most k such that ∥A(X ) − b∥2
≤ ϵ and ∥X −
2
∗ ∥2
≤ ϵ/(1 − δ2k ) in at most
∥b∥2
iterations.
1
log((1−δ2k )/2δ2k ) log
X
F
2ϵ
Theorem 1.2 (Main) Suppose the isometry constant of A satisﬁes δ2k < 1/3 and let b = A(X
∗
⌈
⌉
)+e
∗ and an error vector e ∈ Rd . Then, SVP with step-size ηt = 1/(1 + δ2k )
for a rank k matrix X
∗ ∥2
≤ C ∥e∥2 + ϵ and ∥X − X
≤
outputs a matrix X of rank at most k such that ∥A(X ) − b∥2
2
F
, ϵ ≥ 0, in at most
C ∥e∥2+ϵ
∥b∥2
iterations for universal constants C, D .
1
1−δ2k
2(C ∥e∥2+ϵ)
log(1/D) log
As our SVP algorithm is based on projected gradient descent, it behaves as a ﬁrst order methods
and may require a relatively large number of iterations to achieve high accuracy, even after iden-
tifying the correct row and column subspaces. To this end, we introduce a Newton-type step in
our framework (SVP-Newton) rather than using a simple gradient-descent step. Guarantees sim-
ilar to Theorems 1.1, 1.2 follow easily for SVP-Newton using the proofs for SVP.
In practice,
SVP-Newton performs better than SVP in terms of accuracy and number of iterations.
We next consider an important application of ARMP:
the low-rank matrix completion problem
(MCP)— given a small number of entries from an unknown low-rank matrix, the task is to complete
the missing entries. Note that RIP does not hold directly for this problem. Recently, Candes and
Recht [6], Candes and Tao [7] and Keshavan et al. [14] gave the ﬁrst theoretical guarantees for the
problem obtaining exact recovery from an almost optimal number of uniformly sampled entries.
While RIP does not hold for MCP, we show that a similar property holds for incoherent matrices
[6]. Given our reﬁned RIP and a hypothesis bounding the incoherence of the iterates arising in SVP,
an analysis similar to that of Theorem 1.1 immediately implies that SVP optimally solves MCP.
We provide strong empirical evidence for our hypothesis and show that that both of our algorithms
recover a low-rank matrix from an almost optimal number of uniformly sampled entries.
In summary, our main contributions are:
• Motivated by [11], we propose a projected gradient based algorithm, SVP, for ARMP and show
that our method recovers the optimal rank solution when the afﬁne constraints satisfy RIP. To the
√
best of our knowledge, our isometry constant requirements are least stringent: we only require
3 by Lee and Bresler [18] and
δ2k < 1/3 as opposed to δ5k < 1/10 by Recht et al., δ3k < 1/4
δ4k < 0.04 by Lee and Bresler [17].
• We introduce a Newton-type step in the SVP method which is useful if high precision is criti-
cally. SVP-Newton has similar guarantees to that of SVP, is more stable and has better empirical
performance in terms of accuracy. For instance, on the Movie-lens dataset [1] and rank k = 3,
SVP-Newton achieves an RMSE of 0.89, while SVT method [5] achieves an RMSE of 0.98.
• As observed in [23], most trace-norm based methods perform poorly for matrix completion when
entries are sampled from more realistic power-law distributions. Our method SVP-Newton is
relatively robust to sampling techniques and performs signiﬁcantly better than the methods of
[5, 14, 23] even for power-law distributed samples.

2

• We show that the afﬁne constraints in the low-rank matrix completion problem satisfy a weaker
restricted isometry property and as supported by empirical evidence, conjecture that SVP (as
well as SVP-Newton) recovers the underlying matrix from an almost optimal number of uni-
formly random samples.
• We evaluate our method on a variety of synthetic and real-world datasets and show that our
methods consistently outperform, both in accuracy and time, various existing methods [5, 14].
2 Method
In this section, we ﬁrst introduce our Singular Value Projection (SVP) algorithm for ARMP and
present a proof of its optimality for afﬁne constraints satisfying RIP (1). We then specialize our
algorithm for the problem of matrix completion and prove a more restricted isometry property for
the same. Finally, we introduce a Newton-type step in our SVP algorithm and prove its convergence.

2.1 Singular Value Decomposition (SVP)
Consider the following more robust formulation of ARMP (RARMP),
∥A(X ) − b∥2
2 s.t X ∈ C (k) = {X : rank(X ) ≤ k}.
1
(RARMP)
min
ψ(X ) =
2
X
The hardness of the above problem mainly comes from the non-convexity of the set of low-rank
matrices C (k). However, the Euclidean projection onto C (k) can be computed efﬁciently using
singular value decomposition (SVD). Our algorithm uses this observation along with the projected
gradient method for efﬁciently minimizing the objective function speciﬁed in (RARMP).
Let Pk : Rm×n → Rm×n denote the orthogonal projection on to the set C (k). That is, Pk (X ) =
{∥Y − X ∥F : Y ∈ C (k)}. It is well known that Pk (X ) can be computed efﬁciently by
argminY
computing the top k singular values and vectors of X .
In SVP, a candidate solution to ARMP is computed iteratively by starting from the all-zero ma-
)
(
)
(
trix and adapting the classical projected gradient descent update as follows (note that ∇ψ(X ) =
AT (A(X ) − b)):
X t − ηtAT (A(X t ) − b)
= Pk
X t − ηt∇ψ(X t )
X t+1 ← Pk
(1)
.
Figure 1 presents SVP in more detail. Note that the iterates X t are always low-rank, facilitating
faster computation of the SVD. See Section 3 for a more detailed discussion of computational issues.

Algorithm 1 Singular Value Projection (SVP) Algorithm
Require: A, b, tolerance ε, ηt for t = 0, 1, 2, . . .
1: Initialize: X 0 = 0 and t = 0
2: repeat
Y t+1 ← X t − ηtAT (A(X t ) − b)
3:
Compute top k singular vectors of Y t+1 : Uk , Σk , Vk
4:
5: X t+1 ← UkΣk V T
t ← t + 1
k
6:
7: until ∥A(X t+1 ) − b∥2
2

≤ ε

Analysis for Constraints Satisfying RIP
∥b∥2
Theorem 1.1 shows that SVP converges to an ϵ-approximate solution of RARMP in O(log
ϵ )
steps. Theorem 1.2 shows a similar result for the noisy case. The theorems follow from the following
lemma that bounds the objective function after each iteration.
∗ be an optimal solution of (RARMP) and let X t be the iterate obtained by SVP
Lemma 2.1 Let X
at t-th iteration. Then, ψ(X t+1 ) ≤ ψ(X
∥A(X
∗ − X t )∥2
∗
2 , where δ2k is the rank 2k
) + δ2k
(1−δ2k )
isometry constant of A.
The lemma follows from elementary linear algebra, optimality of SVD (Eckart-Young theorem) and
two simple applications of RIP. We refer to the supplementary material (Appendix A) for a detailed
proof. We now prove Theorem 1.1. Theorem 1.2 can also be proved similarly; see supplementary
material (Appendix A) for a detailed proof.
Proof of Theorem 1.1 Using Lemma 2.1 and the fact that ψ(X
) = 0, it follows that
∗ − X t )∥2
ψ(X t+1 ) ≤
∥A(X
δ2k
2δ2k
(1 − δ2k )
(1 − δ2k )
2 =

ψ(X t ).

∗

3

⌈
⌉
Hence, ψ(X τ ) ≤ ϵ where τ =
(1−δ2k ) < 1.
Also, note that for δ2k < 1/3,
2δ2k
⌈
⌉
. Further, using RIP for the rank at most 2k matrix X τ − X
∗ we
log((1−δ2k )/2δ2k ) log ψ(X 0 )
1
∗ ∥ ≤ ψ(X τ )/(1 − δ2k ) ≤ ϵ/(1 − δ2k ). Now, the SVP algorithm is initialized using
get: ∥X τ − X
ϵ
∥b∥2
∥b∥2
2 . Hence, τ =
X 0 = 0, i.e., ψ(X 0 ) =
.
1
log((1−δ2k )/2δ2k ) log
2ϵ
2.2 Matrix Completion
We ﬁrst describe the low-rank matrix completion problem formally. For Ω ⊆ [m] × [n], let PΩ :
Rm×n → Rm×n denote the projection onto the index set Ω. That is, (PΩ (X ))ij = Xij for (i, j ) ∈
Ω and (PΩ (X ))ij = 0 otherwise. Then, the low-rank matrix completion problem (MCP) can be
formulated as follows,
s.t PΩ (X ) = PΩ (X
), X ∈ Rm×n .
∗
(MCP)
rank(X )
min
X
Observe that MCP is a special case of ARMP, so we can apply SVP for matrix completion. We
)
(
use step-size ηt = 1/(1 + δ)p, where p is the density of sampled entries and δ is a parameter which
we will explain later in this section. Using the given step-size and update (1), we get the following
update for matrix-completion:
(PΩ (X t ) − PΩ (X
X t+1 ← Pk
X t −
∗
1
(2)
.
))
(1 + δ)p
Although matrix completion is a special case of ARMP, the afﬁne constraints that deﬁne MCP, PΩ ,
do not satisfy RIP in general. Thus Theorems 1.1, 1.2 above and the results of Recht et al. [24] do
not directly apply to MCP. However, we show that the matrix completion afﬁne constraints satisfy
RIP for low-rank incoherent matrices.
Deﬁnition 2.1 (Incoherence) A matrix X ∈ Rm×n with singular value decomposition X =
U ΣV T is µ-incoherent if maxi,j |Uij | ≤ √
m , maxi,j |Vij | ≤ √
µ√
µ√
n .
The above notion of incoherence is similar to that introduced by Candes and Recht [6] and also used
by [7, 14]. Intuitively, high incoherence (i.e., µ is small) implies that the non-zero entries of X
are not concentrated in a small number of entries. Hence, a random sampling of the matrix should
provide enough global information to satisfy RIP.
Using the above deﬁnition, we prove the following reﬁned restricted isometry property.
Theorem 2.2 There exists a constant C ≥ 0 such that the following holds for all 0 < δ < 1,
µ ≥ 1, n ≥ m ≥ 3: For Ω ⊆ [m] × [n] chosen according to the Bernoulli model with density
p ≥ C µ2k2 log n/δ2m, with probability at least 1−exp(−n log n), the following restricted isometry
property holds for all µ-incoherent matrices X of rank at most k:
≤ (1 + δ)p ∥X ∥2
≤ ∥PΩ (X )∥2
(1 − δ)p ∥X ∥2
F .
F
F
Roughly, our proof combines a Chernoff bound estimate for ∥PΩ (X )∥2
F with a union bound over
low-rank incoherent matrices. A proof sketch is presented in Section 2.2.1.
Given the above reﬁned RIP, if the iterates arising in SVP are shown to be incoherent, the arguments
of Theorem 1.1 can be used to show that SVP achieves exact recovery for low-rank incoherent
matrices from uniformly sampled entries. As supported by empirical evidence, we hypothesize that
∗ is incoherent.
the iterates X t arising in SVP remain incoherent when the underlying matrix X
√
n maxt,i,j |U t
|, where U t are the
Figure 1 (d) plots the maximum incoherence maxt µ(X t ) =
ij
left singular vectors of the intermediate iterates X t computed by SVP. The ﬁgure clearly shows
that the incoherence µ(X t ) of the iterates is bounded by a constant independent of the matrix size
n and density p throughout the execution of SVP. Figure 2 (c) plots the threshold sampling density
p beyond which matrix completion for randomly generated matrices is solved exactly by SVP for
ﬁxed k and varying matrix sizes n. Note that the density threshold matches the optimal information-
theoretic bound [14] of Θ(k log n/n).
Motivated by Theorem 2.2 and supported by empirical evidence (Figures 2 (c), (d)) we hypothesize
that SVP achieves exact recovery from an almost optimal number of samples for incoherent matrices.
Conjecture 2.3 Fix µ, k and δ ≤ 1/3. Then, there exists a constant C such that for a µ-
∗ of rank at most k and Ω sampled from the Bernoulli model with density
)⌉)
(⌈
(
incoherent matrix X
∗ with high probability.
p = Ωµ,k ((log n)/m), SVP with step-size ηt = 1/(1 + δ)p converges to X
Moreover, SVP outputs a matrix X of rank at most k such that ∥PΩ (X ) − PΩ (X
)∥2
≤ ϵ after
∗
F
iterations.
1
Oµ,k
log
ϵ

(3)

4

Pr

.

2.2.1 RIP for Matrix Completion on Incoherent Matrices
We now prove the restricted isometry property of Theorem 2.2 for the afﬁne constraints that result
from the projection operator PΩ . To prove Theorem 2.2 we ﬁrst show the theorem for a discrete
collection of matrices using Chernoff type large-deviation bounds and use standard quantization
arguments to generalize to the continuous case. We ﬁrst introduce some notation and provide useful
lemmas for our main proof1 . First, we introduce the notion of α-regularity.
Deﬁnition 2.2 A matrix X ∈ Rm×n is α-regular if maxi,j |Xij | ≤ α√
· ∥X ∥F .
mn
Lemma 2.4 below relates the notion of regularity to incoherence and Lemma 2.5 proves (3) for a
ﬁxed regular matrix when the samples Ω are selected independently.
√
Lemma 2.4 Let X ∈ Rm×n be a µ-incoherent matrix of rank at most k . Then X is µ
k-regular.
(
)
Lemma 2.5 Fix a α-regular X ∈ Rm×n and 0 < δ < 1. Then, for Ω ⊆ [m] × [n] chosen according
(cid:12)(cid:12) ≥ δp∥X ∥2
[ (cid:12)(cid:12)∥PΩ (X )∥2
] ≤ 2 exp
to the Bernoulli model, with each pair (i, j ) ∈ Ω chosen independently with probability p,
− δ2 pmn
− p∥X ∥2
F
F
F
√
3 α2
k)-regular
While the above lemma shows Equation (3) for a ﬁxed rank k , µ-incoherent X (i.e., (µ
X using Lemma 2.4), we need to show Equation (3) for all such rank k incoherent matrices. To
handle this problem, we discretize the space of low-rank incoherent matrices so as to be able to
use the above lemma and a union bound. We now show the existence of a small set of matrices
S (µ, ϵ) ⊆ Rm×n such that every low-rank µ-incoherent matrix is close to an appropriately regular
matrix from the set S (µ, ϵ).
Lemma 2.6 For all 0 < ϵ < 1/2, µ ≥ 1, m, n ≥ 3 and k ≥ 1, there exists a set S (µ, ϵ) ⊆ Rm×n
with |S (µ, ϵ)| ≤ (mnk/ϵ)3 (m+n)k such that the following holds. For any µ-incoherent X ∈ Rm×n
√
of rank k with ∥X ∥2 = 1, there exists Y ∈ S (µ, ϵ) s.t. ∥Y − X ∥F < ϵ and Y is (4µ
k)-regular.
We now prove Theorem 2.2 by combining Lemmas 2.5, 2.6 and applying a union bound. We present
a sketch of the proof but defer the details to the supplementary material (Appendix B).
√
(µ, ϵ) = {Y : Y ∈ S (µ, ϵ), Y is 4µ
k-regular}, where
′
( −δ2 pmn
)
Proof Sketch of Theorem 2.2 Let S
(cid:12)(cid:12) ≥ δp∥Y ∥2
[ (cid:12)(cid:12)∥PΩ (Y )∥2
S (µ, ϵ) is as in Lemma 2.6 for ϵ = δ/9mnk . Let m ≤ n. Then, by Lemma 2.5 and union bound,
] ≤ 2(mnk/ϵ)3(m+n)k exp
for any Y ∈ S
′
(µ, ϵ),
− p∥Y ∥2
≤ exp(C1nk log n)·exp
F
F
F
16µ2k
where C1 ≥ 0 is a constant independent of m, n, k . Thus, if p > C µ2k2 log n/δ2m, where C =
16(C1 + 1), with probability at least 1 − exp(−n log n), the following holds
− p∥Y ∥2
|∥PΩ (Y )∥2
∀Y ∈ S
| ≤ δp∥Y ∥2
′
(4)
(µ, ϵ),
F .
F
F
As the statement of the theorem is invariant under scaling, it is enough to show the statement for all
µ-incoherent matrices X of rank at most k and ∥X ∥2 = 1. Fix such a X and suppose that (4) holds.
(µ, ϵ) such that ∥Y − X ∥F ≤ ϵ. Moreover,
Now, by Lemma 2.6 there exists Y ∈ S
′
∥Y ∥2
≤ (∥X ∥F + ϵ)2 ≤ ∥X ∥2
F + 2ϵ∥X ∥F + ϵ2 ≤ ∥X ∥2
F + 3ϵk .
F
Proceeding similarly, we can show that
− ∥PΩ (X )∥2
|∥PΩ (Y )∥2
− ∥Y ∥2
|∥X ∥2
| ≤ 3ϵk ,
| ≤ 3ϵk .
(5)
Combining inequalities (4), (5) above, with probability at least 1 − exp(−n log n) we have,
F
F
F
F
|∥PΩ (X )∥2
| ≤ 2δp∥X ∥2
− p∥Y ∥2
| + |∥PΩ (Y )∥2
− ∥Y ∥2
| + p |∥X ∥2
− ∥PΩ (Y )∥2
| ≤ |∥PΩ (X )∥2
− p∥X ∥2
F .
F
F
F
F
F
F
F
F
The theorem follows using the above inequality.

( −δ2 pmn
16µ2k

Pr

)

,

2.3 SVP-Newton
In this section we introduce a Newton-type step in our SVP method to speed up its convergence.
Recall that each iteration of SVP (Equation (1)) takes a step along the gradient of the objective
function and then projects the iterate to the set of low rank matrices using SVD. Now, the top k
singular vectors (Uk , Vk ) of Y t+1 = X t − ηtAT (A(X t )− b) determine the range-space and column-
k (X t − ηtAT (A(X t )− b))Vk ).
space of the next iterate in SVP. Then, Σk is given by Σk = Diag(U T
1Detailed proofs of all the lemmas in this section are provided in Appendix B of the supplementary material.

5

(6)

Hence, Σk can be seen as a product of gradient-descent step for a quadratic objective function, i.e.,
k ). This leads us to the following variant of SVP we call SVP-Newton:2
Σk = argminS ψ(Uk SV T
Compute top k-singular vectors Uk , Vk of Y t+1 = X t − ηtAT (A(X t ) − b)
∥A(UkΣk V T
k ) − b∥2 .
X t+1 = UkΣk Vk , Σk = argmin
Ψ(Uk SV T
k ) = argmin
S
S
Note that as A is an afﬁne transformation, Σk can be computed by solving a least squares problem
on k × k variables. Also, for a single iteration, given the same starting point, SVP-Newton decreases
the objective function more than SVP. This observation along with straightforward modiﬁcations of
the proofs of Theorems 1.1, 1.2 show that similar guarantees hold for SVP-Newton as well3 .
Note that the least squares problem for computing Σk has k2 variables. This makes SVP-Newton
computationally expensive for problems with large rank, particularly for situations with a large
number of constraints as is the case for matrix completion. To overcome this issue, we also consider
the alternative where we restrict Σk to be a diagonal matrix, leading to the update
∥A(Uk SV T
k ) − b∥2
argmin
Σk =
S,s.t.,Sij =0 for i ̸=j
We call the above method SVP-NewtonD (for SVP-Newton Diagonal). As for SVP-Newton, guar-
antees similar to SVP follow for SVP-NewtonD by observing that for each iteration, SVP-NewtonD
decreases the objective function more than SVP.
3 Related Work and Computational Issues
The general rank minimization problem with afﬁne constraints is NP-hard and is also NP-hard to
approximate [22]. Most methods for ARMP either relax the rank constraint to a convex function
such as the trace-norm [8], [9], or assume a factorization and optimize the resulting non-convex
problem by alternating minimization [4, 3, 15].
√
The results of Recht et al. [24] were later extended to noisy measurements and isometry constants
up to δ3k < 1/4
3 by Fazel et al. [10] and Lee and Bresler [18]. However, even the best existing
optimization algorithms for the trace-norm relaxation are relatively inefﬁcient in practice. Recently,
Lee and Bresler [17] proposed an algorithm (ADMiRA) motivated by the orthogonalmatchingpur-
suit line of work in compressed sensing and show that for afﬁne constraints with isometry constant
δ4k ≤ 0.04, their algorithm recovers the optimal solution. However, their method is not very efﬁ-
cient for large datasets and when the rank of the optimal solution is relatively large.
For the matrix-completion problem until the recent works of [6], [7] and [14], there were few meth-
ods with rigorous guarantees. The alternating least squares minimization heuristic and its variants
[3, 15] perform the best in practice, but are notoriously hard to analyze. Candes and Recht [6],
∗ is µ-incoherent and the known entries are sampled uniformly
Candes and Tao [7] show that if X
at random with |Ω| ≥ C (µ) k2n log2 n, ﬁnding the minimum trace-norm solution recovers the min-
imum rank solution. Keshavan et.al obtained similar results independently for exact recovery from
uniformly sampled Ω with |Ω| ≥ C (µ, k) n log n.
Minimizing the trace-norm of a matrix subject to afﬁne constraints can be cast as a semi-deﬁnite
program (SDP). However, algorithms for semi-deﬁnite programming, as used by most methods for
minimizing trace-norm, are prohibitively expensive even for moderately large datasets. Recently,
a variety of methods based mostly on iterative soft-thresholding have been proposed to solve the
trace-norm minimization problem more efﬁciently. For instance, Cai et al. [5] proposed a Singular
Value Thresholding (SVT) algorithm which is based on Uzawa’s algorithm [2]. A related approach
based on linearized Bregman iterations was proposed by Ma et al. [20], Toh and Yun [25], while Ji
and Ye [13] use Nesterov’s gradient descent methods for optimizing the trace-norm.
While the soft-thresholding based methods for trace-norm minimization are signiﬁcantly faster than
SDP based approaches, they suffer from slow convergence (see Figure 2 (d)). Also, noisy measure-
ments pose considerable computational challenges for trace-norm optimization as the rank of the
intermediate iterates can become very large (see Figure 3(b)).

2We call our method SVP-Newton as the Newton method when applied to a quadratic objective function
leads to the exact solution by solving the resulting least squares problem.
3As a side note, we can show a stronger result for SVP-Newton when applied to the special case of
compressed-sensing, i.e., when the matrix X is restricted to be diagonal. Speciﬁcally, we can show that under
certain assumptions SVP-Newton converges to the optimal solution in O(log k), improving upon the result of
Maleki [21]. We give the precise statement of the theorem and proof in the supplementary material.

6

(d)
(c)
(b)
(a)
Figure 1: (a) Time taken by SVP and SVT for random instances of the Afﬁne Rank Minimization
Problem (ARMP) with optimal rank k = 5. (b) Reconstruction error for the MIT logo. (c) Empirical
estimates of the sampling density threshold required for exact matrix completion by SVP (here
C = 1.28). Note that the empirical bounds match the information theoretically optimal bound
Θ(k log n/n). (d) Maximum incoherence maxt µ(X t ) over the iterates of SVP for varying densities
p and sizes n. Note that the incoherence is bounded by a constant, supporting Conjecture 2.3.

(d)
(c)
(b)
(a)
Figure 2: (a), (b) Running time (on log scale) and RMSE of various methods for matrix completion
problem with sampling density p = .1 and optimal rank k = 2. (c) Running time (on log scale) of
various methods for matrix completion with sampling density p = .1 and n = 1000. (d) Number of
iterations needed to get RMSE 0.001.

For the case of matrix completion, SVP has an important property facilitating fast computation
of the main update in equation (2); each iteration of SVP involves computing the singular value
decomposition (SVD) of the matrix Y = X t + PΩ (X t − X
∗
), where X t is a matrix of rank at
most k whose SVD is known and PΩ (X t − X
∗
) is a sparse matrix. Thus, matrix-vector products
of the form Y v can be computed in time O((m + n)k + |Ω|). This facilitates the use of fast SVD
computing packages such as PROPACK [16] and ARPACK [19] that only require subroutines for
computing matrix-vector products.
4 Experimental Results
In this section, we empirically evaluate our methods for the afﬁne rank minimization problem and
low-rank matrix completion. For both problems we present empirical results on synthetic as well
as real-world datasets. For ARMP we compare our method against the trace-norm based singular
value thresholding (SVT) method [5]. Note that although Cai et al. present the SVT algorithm in the
context of MCP, it can be easily adapted for ARMP. For MCP we compare against SVT, ADMiRA
[17], the OptSpace (OPT) method of Keshavan et al. [14], and regularized alternating least squares
minimization (ALS). We use our own implementation of SVT for ARMP and ALS, while for matrix
completion we use the code provided by the respective authors for SVT, ADMiRA and OPT. We
report results averaged over 20 runs. All the methods are implemented in Matlab and use mex ﬁles.

4.1 Afﬁne Rank Minimization
We ﬁrst compare our method against SVT on random instances of ARMP. We generate random
matrices X ∈ Rn×n of different sizes n and ﬁxed rank k = 5. We then generate d = 6kn random
afﬁne constraint matrices Ai and compute b = A(X ). Figure 1(a) compares the computational time
required by SVP and SVT (in log-scale) for achieving a relative error (∥A(X ) − b∥2 /∥b∥2 ) of 10
−3 ,
and shows that our method requires many fewer iterations and is signiﬁcantly faster than SVT.
Next we evaluate our method for the problem of matrix reconstruction from random measurements.
As in Recht et al. [24], we use the MIT logo as the test image for reconstruction. The MIT logo
we use is a 38 × 73 image and has rank four. For reconstruction, we generate random measurement
matrices Ai and measure bi = T r(AiX ). We let both SVP and SVT converge and then compute the
reconstruction error for the original image. Figure 1 (b) shows that our method incurs signiﬁcantly
smaller reconstruction error than SVT for the same number of measurements.
Matrix Completion: Synthetic Datasets (Uniform Sampling)
We now evaluate our method against various matrix completion methods for random low-rank ma-

7

406080100120140160100102104n (Size of Matrix)  ARMP: Random InstancesSVPSVT6008001000120014001600024681012ARMP: MIT LogoNumber of ConstraintsError (Frobenius Norm)  SVPSVT100020003000400050000.020.040.060.080.1n (Size of the matrix)SVP Density Threshold  k = 10, threshold pk=10, Cklog(n)/n100020003000400050003.544.555.5Incoherence (SVP)n (Size of the Matrix)m  p=.05p=.15p=.25p=.3510002000300040005000−10123n (Size of Matrix)  SVP−NewtonDSVPSVTALSADMiRAOPT1000200030004000500001234x 10−3n (Size of Matrix)RMSE  SVP−NewtonDSVPSVTALSADMiRAOPT246810100101102103k (Rank of Matrix)Time Taken (secs)  SVP−NewtonDSVPSVTALSADMiRAOPT10002000300040005000050100150200n (Size of Matrix)Number of Iterations  SVP−NewtonDSVPSVTk
2
3
5
7
10
12

SVP-NewtonD
0.90
0.89
0.89
0.89
0.90
0.91

SVP
1.15
1.14
1.09
1.08
1.07
1.08

ALS
0.88
0.87
0.86
0.86
0.87
0.88

SVT
1.06
0.98
0.95
0.93
0.91
0.90

(a)
(d)
(c)
(b)
Figure 3: (a): RMSE incurred by various methods for matrix completion with different rank (k)
solutions on Movie-Lens Dataset. (b): Time(on log scale) required by various methods for matrix
completion with p = .1, k = 2 and 10% Gaussian noise. Note that all the four methods achieve
similar RMSE. (c): RMSE incurred by various methods for matrix completion with p = 0.1, k = 10
when the sampling distribution follows Power-law distribution (Chung-Lu-Vu Model). (d): RMSE
incurred for the same problem setting as plot (c) but with added Gaussian noise.
trices and uniform samples. We generate a random rank k matrix X ∈ Rn×n and generate random
Bernoulli samples with probability p. Figure 2 (a) compares the time required by various methods
−3 on the sampled entries for ﬁxed
(in log-scale) to obtain a root mean square error (RMSE) of 10
k = 2. Clearly, SVP is substantially faster than the other methods. Next, we evaluate our method
for increasing k . Figure 2 (b) compares the overall RMSE obtained by various methods. Note that
SVP-Newton is signiﬁcantly more accurate than both SVP and SVT. Figure 2 (c) compares the time
−3 on the sampled
required by various methods to obtain a root mean square error (RMSE) of 10
entries for ﬁxed n = 1000 and increasing k . Note that our algorithms scale well with increasing k
and are faster than other methods. Next, we analyze reasons for better performance of our methods.
To this end, we plot the number of iterations required by our methods as compared to SVT (Fig-
ure 2 (d)). Note that even though each iteration of SVT is almost as expensive as our methods’, our
methods converge in signiﬁcantly fewer iterations.
Finally, we study the behavior of our method in presence of noise. For this experiment, we generate
random matrices of different size and add approximately 10% Gaussian noise. Figure 2 (c) plots
time required by various methods as n increases from 1000 to 5000. Note that SVT is particularly
sensitive to noise. One of the reason for this is that due to noise, the rank of the intermediate iterates
arising in SVT can be fairly large.
Matrix Completion: Synthetic Dataset (Power-law Sampling) We now evaluate our methods
against existing matrix-completion methods under more realistic power-law distributed samples.
As before, we generate a random rank-k = 10 matrix X ∈ Rn×n and sample the entries of X
using a graph generated using Chung-Lu-Vu model with power-law distributed degrees (see [23])
for details. Figure 3 (c) plots the RMSE obtained by various methods for varying n and ﬁxed
sampling density p = 0.1. Note that SVP-NewtonD performs signiﬁcantly better than SVT as well
as SVP. Figure 3 (d) plots the RMSE obtained by various methods when each sampled entry is
corrupted with around 1% Gaussian noise. Note that here again SVP-NewtonD performs similar to
ALS and is signiﬁcantly better than the other methods including the ICMC method [23] which is
specially designed for power-law sampling but is quite sensitive to noise.
√
Matrix Completion: Movie-Lens Dataset
Finally, we evaluate our method on the Movie-Lens dataset [1], which contains 1 million ratings for
3900 movies by 6040 users. Figure 3 (a) shows the RMSE obtained by each method with varying k .
For SVP and SVP-Newton, we ﬁx step size to be η = 1/p
(t), where t is the number of iterations.
For SVT, we ﬁx δ = .2p using cross-validation. Since, rank cannot be ﬁxed in SVT, we try various
values for the parameter τ to obtain the desired rank solution. Note that SVP-Newton incurs a
RMSE of 0.89 for k = 3. In contrast, SVT achieves a RMSE of 0.98 for the same rank. We remark
that SVT was able to achieve RMSE up to 0.89 but required rank 17 solution and was signiﬁcantly
slower in convergence because many intermediate iterates had large rank (up to around 150). We
attribute the relatively poor performance of SVP and SVT as compared with ALS and SVP-Newton
to the fact that the ratings matrix is not sampled uniformly, thus violating the crucial assumption of
uniformly distributed samples.
Acknowledgements: This research was supported in part by NSF grant CCF-0728879.

8

10002000300040005000100101102103n (Size of Matrix)Time Taken (secs)  SVP−NewtonDSVPSVTALS50010001500200000.511.522.5n (Size of Matrix)RMSE  ICMCALSSVTSVPSVP NewtonD50010001500200001234n (Size of Matrix)RMSE  ICMCALSSVTSVPSVP NewtonDReferences
[1] Movie lens dataset. Public dataset. URL http://www.grouplens.org/taxonomy/term/14.
[2] K. Arrow, L. Hurwicz, and H. Uzawa. Studies in Linear and Nonlinear Programming. Stanford University
Press, Stanford, 1958.
[3] Robert Bell and Yehuda Koren. Scalable collaborative ﬁltering with jointly derived neighborhood inter-
polation weights. In ICDM, pages 43–52, 2007. doi: 10.1109/ICDM.2007.90.
[4] Matthew Brand. Fast online SVD revisions for lightweight recommender systems. In SIAM International
Conference on Data Mining, 2003.
[5] Jian-Feng Cai, Emmanuel J. Cand `es, and Zuowei Shen. A singular value thresholding algorithm for
matrix completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.
[6] Emmanuel J. Cand `es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations
of Computational Mathematics, 9(6):717–772, December 2009.
[7] Emmanuel J. Cand `es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.
IEEE Trans. Inform. Theory, 56(5):2053–2080, 2009.
[8] M. Fazel, H. Hindi, and S. Boyd. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, Arlington, Virginia, 2001.
[9] M. Fazel, H. Hindi, and S. Boyd. Log-det heuristic for matrix rank minimization with applications to
Hankel and Euclidean distance matrices. In American Control Conference, 2003.
[10] M. Fazel, E. Candes, B. Recht, and P. Parrilo. Compressed sensing and robust recovery of low rank
matrices. In Signals, Systems and Computers, 2008 42nd Asilomar Conference on, pages 1043–1047,
Oct. 2008. doi: 10.1109/ACSSC.2008.5074571.
[11] Rahul Garg and Rohit Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse
recovery with restricted isometry property. In ICML, 2009.
[12] Donald Goldfarb and Shiqian Ma. Convergence of ﬁxed point continuation algorithms for matrix rank
minimization, 2009. Submitted.
[13] Shuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization. In ICML,
2009.
[14] Raghunandan H. Keshavan, Sewoong Oh, and Andrea Montanari. Matrix completion from a few entries.
In ISIT’09: Proceedings of the 2009 IEEE international conference on Symposium on Information Theory,
pages 324–328, Piscataway, NJ, USA, 2009. IEEE Press. ISBN 978-1-4244-4312-3.
[15] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model. In
KDD, pages 426–434, 2008. doi: 10.1145/1401890.1401944.
[16] R.M. Larsen. Propack: a software for large and sparse SVD calculations. Available online. URL http:
//sun.stanford.edu/rmunk/PROPACK/.
[17] Kiryung Lee and Yoram Bresler. Admira: Atomic decomposition for minimum rank approximation, 2009.
[18] Kiryung Lee and Yoram Bresler. Guaranteed minimum rank approximation from linear observations by
nuclear norm minimization with an ellipsoidal constraint, 2009.
[19] Richard B. Lehoucq, Danny C. Sorensen, and Chao Yang. ARPACK Users’ Guide: Solution of Large-
Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods. SIAM, Philadelphia, 1998.
[20] S. Ma, D. Goldfarb, and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-
tion. To appear, Mathematical Programming Series A, 2010.
[21] Arian Maleki. Coherence analysis of iterative thresholding algorithms. CoRR, abs/0904.1193, 2009.
[22] Raghu Meka, Prateek Jain, Constantine Caramanis, and Inderjit S. Dhillon. Rank minimization via online
learning. In ICML, pages 656–663, 2008. doi: 10.1145/1390156.1390239.
[23] Raghu Meka, Prateek Jain, and Inderjit S. Dhillon. Matrix completion from power-law distributed sam-
ples. In NIPS, 2009.
[24] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization, 2007. To appear in SIAM Review.
[25] K.C. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least
squares problems. Preprint, 2009. URL http://www.math.nus.edu.sg/˜matys/apg.pdf.

9


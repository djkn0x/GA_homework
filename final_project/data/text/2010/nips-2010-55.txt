Throttling Poisson Processes

Uwe Dick

Tobias Scheffer

Peter Haider

Thomas Vanck Michael Br ¨uckner
University of Potsdam
Department of Computer Science
August-Bebel-Strasse 89, 14482 Potsdam, Germany
{uwedick,haider,vanck,mibrueck,scheffer}@cs.uni-potsdam.de

Abstract

We study a setting in which Poisson processes generate sequences of decision-
making events. The optimization goal is allowed to depend on the rate of decision
outcomes; the rate may depend on a potentially long backlog of events and de-
cisions. We model the problem as a Poisson process with a throttling policy that
enforces a data-dependent rate limit and reduce the learning problem to a convex
optimization problem that can be solved efﬁciently. This problem setting matches
applications in which damage caused by an attacker grows as a function of the rate
of unsuppressed hostile events. We report on experiments on abuse detection for
an email service.

1 Introduction

This paper studies a family of decision-making problems in which discrete events occur on a contin-
uous time scale. The time intervals between events are governed by a Poisson process. Each event
has to be met by a decision to either suppress or allow it. The optimization criterion is allowed to
depend on the rate of decision outcomes within a time interval; the criterion is not necessarily a sum
of a loss function over individual decisions.
The problems that we study cannot adequately be modeled as Mavkov or semi-Markov decision
problems because the probability of transitioning from any value of decision rates to any other value
depends on the exact points in time at which each event occurred in the past. Encoding the entire
backlog of time stamps in the state of a Markov process would lead to an unwieldy formalism. The
learning formalism which we explore in this paper models the problem directly as a Poisson process
with a throttling policy that depends on an explicit data-dependent rate limit, which allows us to
refer to a result from queuing theory and derive a convex optimization problem that can be solved
efﬁciently.
Consider the following two scenarios as motivating applications.
In order to stage a successful
denial-of-service attack, an assailant has to post requests at a rate that exceeds the capacity of the
service. A prevention system has to meet each request by a decision to suppress it, or allow it
to be processed by the service provider. Suppressing legitimate requests runs up costs. Passing
few abusive requests to be processed runs up virtually no costs. Only when the rate of passed
abusive requests exceeds a certain capacity, the service becomes unavailable and costs incur. The
following second application scenario will serve as a running example throughout this paper. Any
email service provider has to deal with a certain fraction of accounts that are set up to disseminate
phishing messages and email spam. Serving the occasional spam message causes no harm other
than consuming computational ressources. But if the rate of spam messages that an outbound email
server discharges triggers alerting mechanisms of other providers, then that outbound server will
become blacklisted and the service is disrupted. Naturally, suppressing any legitimate message is a
disruption to the service, too.

1

Let x denote a sequence of decision events x1 , . . . , xn ; each event is a point xi ∈ X in an instance
space. Sequence t denotes the time stamps ti ∈ R+ of the decision events with ti < ti+1 . We deﬁne
an episode e by the tuple e = (x, t, y) which includes a label y ∈ {−1, +1}. In our application, an
episode corresponds to the sequence of emails sent within an observation interval from a legitimate
(y = −1) or abusive (y = +1) account e. We write xi and ti to denote the initial sequence of the
ﬁrst i elements of x and t, respectively. Note that the length n of the sequences can be different for
different episodes.
Let A = {−1, +1} be a binary decision set, where +1 corresponds to suppressing an event and −1
corresponds to passing it. The decision model π gets to make a decision π (xi , ti ) ∈ A at each point
in time ti at which an event occurs.
′ |x, t) at time t
′ for episode e and decision model π is a crucial concept.
The outbound rate r(cid:25) (t
′ .
It counts the number of events that were let pass during a time interval of lengh τ ending before t
′ |x, t) = |{i : π(xi , ti ) = −1 ∧ ti ∈ [t
′ − τ , t
)}|. In outbound spam
′
It is therefore deﬁned as r(cid:25) (t
throttling, τ corresponds to the time interval that is used by other providers to estimate the incoming
spam rate.
We deﬁne an immediate loss function ℓ : Y × A → R+ that speciﬁes the immediate loss of deciding
{
a ∈ A for an event with label y ∈ Y as
c+ y = +1 ∧ a = −1
c− y = −1 ∧ a = +1
otherwise,
0
where c+ and c− are positive constants, corresponding to costs of false positive and false negative
decisions. Additionally, the rate-based loss λ : Y × R+ → R+ is the loss that runs up per unit
of time. We require λ to be a convex, monotonically increasing function in the outbound rate for
y = +1 and to be 0 otherwise. The rate-based loss reﬂects the risk of the service getting blacklisted
based on the current sending behaviour. This risk grows in the rate of spam messages discharged
and the duration over which a high sending rate of spam messages is maintained.
∫
n∑
The total loss of a model π for an episode e = (x, t, y) is therefore deﬁned as
′ |x, t)) dt
′
λ (y , r(cid:25) (t
t1
i=1
The ﬁrst term penalizes a high rate of unsuppressed events with label +1—in our example, a high
rate of unsuppressed spam messages—whereas the second term penalizes each decision individually.
For the special case of λ = 0, the optimization criterion resolves to a risk, and the problem becomes
a standard binary classiﬁcation problem.
the overall optimization goal
An unknown target distribution over p(x, t, y)
induces
∗
Ex;t;y [L(π ; x, t, y)]. The learning problem consists in ﬁnding π
Ex;t;y [L(π ; x, t, y)]
= argmin(cid:25)
nm , ym )}.
from a training sample of tuples D = {(x1
nm , tm
n1 , t1
n1 , y1 ), . . . , (xm

ℓ (y , π(xi , ti ))

L(π ; x, t, y) =

ℓ(y , a) =

tn+(cid:28)

(1)

(2)

+

2 Poisson Process Model

We assume the following data generation process for episodes e = (x, t, y) that will allow us to
derive an optimization problem to be solved by the learning procedure. First, a rate parameter ρ ,
label y , and the sequence of instances x, are drawn from a joint distribution p(x, ρ , y). Rate ρ is the
parameter of a Poisson process p(t|ρ) which now generates time sequence t. The expected loss of
∫
∫
∫
∑
decision model π is taken over all input sequences x, rate parameter ρ , label y , and over all possible
sequences of time stamps t that can be generated according to the Poisson process.
L(π ; x, t, y)p(t|ρ)p(x, ρ, y)dρdtdx
Ex;t;y [L(π ; x, t, y)] =
t
y

(3)

x

(cid:26)

2.1 Derivation of Empirical Loss

In deriving the empirical counterpart of the expected loss, we want to exploit our assumption that
time stamps are generated by a Poisson process with unknown but ﬁxed rate parameter. For each

2

input episode (x, t, y), instead of minimizing the expected loss over the single observed sequence of
time stamps, we would therefore like to minimize the expected loss over all sequences of time stamps
generated by a Poisson process with the rate parameter that has most likely generated the observed
′ into
sequence of time stamps. Equation 4 introduces the observed time sequence of time stamps t
′ . Equation
Equation 3 and uses the fact that the rate parameter ρ is independent of x and y given t
5 rearranges the terms, and Equation 6 writes the central integral as a conditional expected value
∫
∫
∫
∫
of the loss given the rate ρ. Finally, Equation 7 approximates the integral over all values of ρ by a
∑
∗ for each episode.
single summand with value ρ
(∫
(∫
∫
∫
∑
Ex;t;y [L(π ; x, t, y)] =
t′
t
x
(cid:26)
y
(∫
∫
∫
L(π ; x, t, y)p(t|ρ)dt
∑
t′
(cid:26)
y
∫
∫
∑
t′
(cid:26)
y
Et [L(π ; x, t, y) | ρ
∗
t′
y

)
L(π ; x, t, y)p(t|ρ)p(ρ|t
)
p(ρ|t
)dρ

t
(Et [L(π ; x, t, y) | ρ] p(ρ|t

)
′
)p(x, t

′
, y)dρdtdxdt

, y)dxdt

′
p(x, t

′
, y)dxdt

] p(x, t

, y)dxdt

)dρ

p(x, t

=

≈

(4)

x

x

x

′

′

=

(5)

(6)

(7)

′

′

′

′

′

ˆEx;t;y [L(π ; x, t, y)] =

′
, y) by 1
We arrive at the regularized risk functional in Equation 8 by replacing p(x, t
m for all ob-
∗
e as parameter that generated time stamps te . The
servations in D and inserting MAP estimate ρ
m∑
inﬂuence of the convex regularizer Ω is determined by regularization parameter η > 0.
Et [L(π ; xe , t, ye ) | ρ
∗
1
e ] + ηΩ(π)
m
e=1
e = argmax(cid:26)p(ρ|te )
∗
ρ
Minimizing this risk functional is the basis of the learning procedure in the next section. As noted
in Section 1, for the special case when the rate-based loss λ is zero, the problem reduces to a
standard weighted binary classiﬁcation problem and would be easy to solve with standard learning
algorithms. However, as we will see in Section 4, the λ-dependent loss makes the task of learning
a decision function hard to solve; attributing individual decisions with their “fair share” of the rate
loss—and thus estimating the cost of the decision—is problematic. The Erlang learning model of
Section 3 employs a decision function that allows to factorize the rate loss naturally.

with

(8)

3 Erlang Learning Model

In the following we derive an optimization problem that is based on modeling the policy as a data-
dependent rate limit. This allows us to apply a result from queuing theory and approximate the
empirical risk functional of Equation (8) with a convex upper bound. We deﬁne decision model π
in terms of the function f(cid:18) (xi , ti ) = exp(θTϕ (xi , ti )) which sets a limit on the admissible rate of
{ −1 (“allow”)
events, where ϕ is some feature mapping of the initial sequence (xi , ti ) and θ is a parameter vector.
The throttling model is deﬁned as
if r(cid:25) (ti |xi , ti ) + 1 ≤ f(cid:18) (xi , ti )
otherwise.
+1 (“suppress”)
The decision model blocks event xi , if the number of instances that were sent within [ti − τ , ti ), plus
the current instance, would exceed rate limit f(cid:18) (xi , ti ). We will now transform the optimization goal
of Equation 8 into an optimization problem that can be solved by standard convex optimization tools.
To this end, we ﬁrst decompose the expected loss of an input sequence given the rate parameter in
Equation 8 into immediate and rate-dependent loss terms. Note that te denotes the observed training
sequence whereas t serves as expectation variable for the expectation Et [·|ρe
∗
] over all sequences

π (xi , ti ) =

(9)

3

(10)

+

+

Et

δ

t1

= Et

Et

t1

tne +(cid:28)
λ (ye , r(cid:25) (t

′ |xe , t)) dt

′ | ρ
∗
e

i , ti ) ̸= ye
π(xe

]
ℓ(ye , −ye ) (11)

∗ as in Equation 8.
conditional on the Poisson process rate parameter ρe
[∫
]
ne∑
Et [L(π ; xe , t, ye ) | ρ
∗
e ]
i , ti )) | ρ
′ | ρ
′ |xe , t)) dt
tne +(cid:28)
∗
∗
[ ∫
Et [ℓ(ye , π(xe
= Et
[
]
ne∑
) | ρ
(
λ (ye , r(cid:25) (t
e ]
e
i=1
∗
e
t1
i=1
Equation 10 uses the deﬁnition of the loss function in Equation 2. Equation 11 exploits that only
i , ti ) ̸= ye , incur a positive loss ℓ(y , π(xe
∫
i , ti )).
decisions against the correct label, π(xe
convex
We will
loss
ﬁrst
derive
a
approximation
of
the
expected
rate-based
′ |xe , t)) dt
′ |ρ
∗
Et [
tne+(cid:28)
e ] (left side of Equation 11). Our deﬁnition of the decision
λ (ye , r(cid:25) (t
t1
model allows us to factorize the expected rate-based loss into contributions of individual rate limit
decisions. The convexity will be addressed by Theorem 1.
Since the outbound rate r(cid:25) increases only at decision points ti , we can upper-bound its value with
the value immediately after the most recent decision in Equation 12. Equation 13 approximates
the actual outbound rate with the rate limit given by f(cid:18) (xe
i ). This is reasonable because the
i , te
[∫
]
outbound rate depends on the policy decisions which are deﬁned in terms of the rate limit. Because
t is generated by a Poisson process, Et [ti+1 − ti | ρ
∗
(Equation 14).
e ] = 1
(cid:26)∗
e
′ | ρ
′ |xe , t)) dt
tne +(cid:28)
∗
≤ ne−1∑
λ (ye , r(cid:25) (t
e
≈ ne−1∑
i=1
ne−1∑
i=1
i=1
We have thus established a convex approximation of the left side of Equation 11.
i , ti ) ̸= ye ) | ρ
∗
We will now derive a closed form approximation of Et [δ (π(xe
e ], the second part of
the loss functional in Equation 11. Queuing theory provides a convex approximation: The Erlang-B
formula [5] gives the probability that a queuing process which maintains a constant rate limit of
f within a time interval of τ will block an event when events are generated by a Poisson process
∫ ∞
with given rate parameter ρ. Fortet’s formula (Equation 15) generalizes the Erlang-B formula for
non-integer rate limits.
1
0 e−z (1 + z
(cid:26)(cid:28) )f dz
The integral can be computed efﬁciently using a rapidly converging series, c.f. [5]. The formula
requires a constant rate limit, so that the process can reach an equilibrium. In our model, the rate
limit f(cid:18) (xi , ti ) is a function of the sequences xi and ti until instance xi , and Fortet’s formula
]−1
[∫ ∞
therefore serves as an approximation.
e ] ≈ B (f(cid:18) (xe
i , ti ) = 1)|ρ
∗
∗
Et [δ(π(xe
i , te
e τ )
i ), ρ
−z (1 +
z
ρ∗
e τ
0
Unfortunately, Equation 17 is not convex in θ . We approximate it with the convex upper bound
− log (1 − B (f(cid:18) (xe
∗
the dashed green line in the left panel of Figure 2(b) for an
e τ )) (cf.
i , te
i ), ρ
illustration). This is an upper bound, because − log p ≥ 1 − p for 0 ≤ p ≤ 1; its convexity
i , ti ) = −1)|ρ
∗
is addressed by Theorem 1. Likewise, Et [δ(π(xe
e ] is approximated by upper bound
i , ti ) ̸= ye ) |ρ
∗
∗
e τ )). We have thus derived a convex upper bound of Et [δ (π(xe
e ].
log (B (f(cid:18) (xe
i , te
i ), ρ

Et [ti+1 − ti | ρ
e ]λ(ye , r(cid:25) (ti |xe , t)) + τ λ(ye , r(cid:25) (tne |xe , t))
∗
(
)
(
Et [ti+1 − ti | ρ
∗
(
+ τ λ
e ]λ
1
∗ λ
ρe

(
)
i , te
ye , f(cid:18) (xe
i )
i , te
ye , f(cid:18) (xe
i )

+ τ λ

)
ne , te
ye , f(cid:18) (xe
ne ))

ne , te
ye , f(cid:18) (xe
ne )

i ;te
)f(cid:18) (xe
i )dz

(16)

(17)

=

e

(12)

(13)

(14)

=

B (f , ρτ ) =

(15)

4

Combining the two components of the optimization goal (Equation 11) and adding convex regular-
izer Ω(θ) and regularization parameter η > 0 (Equation 8), we arrive at an optimization problem for
ﬁnding the optimal policy parameters θ .
{
ne−1∑
m∑
(
)
(
)
Optimization Problem 1 (Erlang Learning Model). Over θ , minimize
1
ne∑
)]
[
(
ne , te
ye , f(cid:18) (xe
i , te
ye , f(cid:18) (xe
∗ λ
ne )
+ τ λ
i )
ρe
e=1
i=1
δ(ye= 1) − yeB
− log
∗
i , te
f(cid:18) (xe
i ), ρ
e τ
i=1

}
ℓ(ye , −ye )

+ ηΩ(θ)

(18)

R(θ) =

1
m

+

Next we show that minimizing risk functional R amounts to solving a convex optimization problem.
∗
Theorem 1 (Convexity of R). R(θ) is a convex risk functional in θ for any ρ
e > 0 and τ > 0.

Proof. The convexity of λ and Ω follows from their deﬁnitions. It remains to be shown that both
− log B (f(cid:18) (·), ρ
e τ )) and − log(1 − B (f(cid:18) (·), ρ
e ) are convex in θ . Component ℓ(ye , −ye ) of Equa-
∗
∗
∗
τ )) is convex, monotically
tion 18 is independent of θ . It is known that Fortet’s formula B (f , ρe
e τ > 0 [5]. Furthermore − log(B (f , ρ
∗
∗
decreasing, and positive in f for ρ
e τ ))) is convex and mono-
tonically increasing. Since f(cid:18) (·) is convex in θ , it follows that − log(B (f(cid:18) (·), ρ
∗
e )) is also convex.
Next, we show that − log(1 − B (f(cid:18) (·), ρ
∗
e τ ))) is convex and monotonically decreasing. From the
above it follows that b(f ) = 1 − B (f , ρ
∗
e τ )) is monotonically increasing, concave and positive.
≥ 0 as both summands are positive. Again, it
df 2 − ln(b(f )) = 1
−1
′
′′
Therefore, d2
b2 (f ) b
(f )
(f ) + b
follows that − log(1 − B (f(cid:18) (·), ρ
b(f )
∗
e τ ))) is convex in θ due to the deﬁnition of f(cid:18) .

4 Prior Work and Reference Methods

∗
We will now discuss how the problem of minimizing the expected loss, π
=
Ex;t;y [L(π ; x, t, y)], from a sample of sequences x of events with labels y and observed
argmin(cid:25)
∗ relates to previously studied methods. Sequential decision-making problems
rate parameters ρ
are commonly solved by reinforcement learning approaches, which have to attribute the loss of an
episode (Equation 2) to individual decisions in order to learn to decide optimally in each state. Thus,
a crucial part of deﬁning an appropriate procedure for learning the optimal policy consists in deﬁn-
ing an appropriate state-action loss function. Q(cid:25) (s, a) estimates the loss of performing action a in
state s when following policy π for the rest of the episode.
Several different state-action loss functions for related problems have been investigated in the litera-
ture. For example, policy gradient methods such as in [4] assign the loss of an episode to individual
decisions proportional to the log-probabilities of the decisions. Other approaches use sampled esti-
mates of the rest of the episode Q(si , ai ) = L(π , s) − L(π , si ) or the expected loss if a distribution
of states of the episode is known [7]. Such general purpose methods, however, are not the optimal
choice for the particular problem instance at hand. Consider the special case λ = 0, where the
problem reduces to a sequence of independent binary decisions. Assigning the cumulative loss of
the episode to all instances leads to a grave distortion of the optimization criterion.
∫
As reference in our experiments we use a state-action loss function that assigns the immediate loss
ℓ(y , ai ) to state si only. Decision ai determines the loss incurred by λ only for τ time units, in
′ |x, t))dt
′ . Thus, the loss of
ti+(cid:28)
the interval [ti , ti + τ ). The corresponding rate loss is
λ(y , r(cid:25) (t
deciding ai = −1 instead of ai = +1 is the difference in the corresponding λ-induced loss. Let
ti
−i , t
−i denote the sequence x, t without instance xi . This leads to a state-action loss function that
∫
x
is the sum of immediate loss and λ-induced loss; it serves as our ﬁrst baseline.
it (si , a) = ℓ(y , a) + δ(a = −1)
′ |x
′ |x
−i ) + 1) − λ(y , r(cid:25) (t
∫
−i , t
−i , t
−i ))dt
(19)
Q(cid:25)
λ(y , r(cid:25) (t
ti
′ |x, t)) with τ λ(y , r(cid:25) (ti |x, t)), we deﬁne the state-action loss
ti+(cid:28)
By approximating
λ(y , r(cid:25) (t
ti
function of a second plausible state-action loss that, instead of using the observed loss to estimate

ti+(cid:28)

′

5

(
[
)]
the loss of an action, approximates it with the loss that would be incurred by the current outbound
rate r(cid:25) (ti |x
−i , t
−i ) for τ time units.
−i ) + 1) − λ(y , r(cid:25) (ti |x
ub (si , a) = ℓ(y , a) + δ(a = −1)
λ(y , r(cid:25) (ti |x
−i ))
−i , t
−i , t
(20)
Q(cid:25)
τ
The state variable s has to encode all information a policy needs to decide. Since the loss crucially
′ |x, t), any throttling model must have access to the current outbound
depends on outbound rate r(cid:25) (t
rate. The transition between a current and a subsequent rate depends on the time at which the next
event occurs, but also on the entire backlog of events, because past events may drop out of the
interval τ at any time. In analogy to the information that is available to the Erlang learning model,
it is natural to encode states si as a vector of features ϕ(xi , ti ) (see Section 5 for details) together
with the current outbound rate r(cid:25) (ti |x, t). Given a representation of the state and a state-action loss
function, different approaches for deﬁning the policy π and optimizing its parameters have been
investigated. For our baselines, we use the following two methods.

Policy gradient. Policy gradient methods model a stochastic policy directly as a parameterized
decision function. They perform a gradient descent that always converges to a local optimum [8].
The gradient of the expected loss with respect to the parameters is estimated in each iteration k for
the distribution over episodes, states, and losses that the current policy πk induces. However, in
order to achieve fast convergence to the optimal polity, one would need to determine the gradient for
the distribution over episodes, states, and losses induced by the optimal policy. We implement two
policy gradient algorithms for experimentation which only differ in using Qit and Qub , respectively.
They are denoted PGit and PGub in the experiments. Both use a logistic regression function as
decision function, the two-class equivalent of the Gibbs distribution which is used in the literature.

Iterative Classiﬁer. The second approach is to represent policies as classiﬁers and to employ
methods for supervised classiﬁcation learning. A variety of papers addresses this approach [6, 3, 7].
We use an algorithm that is inspired by [1, 2] and is adapted to the problem setting at hand. Blatt
and Hero [2] investigate an algorithm that ﬁnds non-stationary policies for two-action T-step MDPs
by solving a sequence of one-step decisions via a binary classiﬁer. Classiﬁers πt for time step t are
learned iteratively on the distribution of states generated by the policy (π0 , . . . , πt−1 ). Our derived
algorithm iteratively learns weighted support vector machine (SVM) classiﬁer πk+1 in iteration k+1
on the set of instances and losses Q(cid:25)k (s, a) that were observed after classiﬁer πk was used as policy
on the training sample. The weight vector of πk is denoted θk . The weight of misclassiﬁcation of s
is given by Q(cid:25)k (s, −y). The SVM weight vector is altered in each iteration as θk+1 = (1 − αk )θk +
ˆθ , where ˆθ is the weight vector of the new classiﬁer that was learned on the observed losses. In
αk
the experiments, two iterative SVM learner were implemented, denoted It-SVMit and It-SVMub ,
corresponding to the used state-action losses Qit and Qub , respectively. Note that for the special
case λ = 0 the iterative SVM algorithm reduces to a standard SVM algorithm.
All four procedures iteratively estimate the loss of a policy decision on the data via a state-action
loss function and learn a new policy π based on this estimated cost of the decisions. Convergence
guarantees typically require the Markov assumption; that is, the process is required to possess a
stationary transition distribution P (si+1 |si , ai ). Since the transition distribution in fact depends
on the entire backlog of time stamps and the duration over which state si has been maintained,
the Markov assumption is violated to some extent in practice.
In addition to that, λ-based loss
estimates are sampled from a Poisson process. In each iteration π is learned to minimize sampled
and inherently random losses of decisions. Thus, convergence to a robust solution becomes unlikely.
In contrast, the Erlang learning model directly minimizes the λ-loss by assigning a rate limit. The
rate limit implies an expectation of decisions. In other words, the λ-based loss is minimized without
explicitely estimating the loss of any decisions that are implied by the rate limit. The convexity of
the risk functional in Optimization Problem 1 guarantees convergence to the global optimum.

5 Application

The goal of our experiments is to study the relative beneﬁts of the Erlang learning model and the
four reference methods over a number of loss functions. The subject of our experimentation is the
problem of suppressing spam and phishing messages sent from abusive accounts registered at a
large email service provider. We sample approximately 1,000,000 emails sent from approximately

6

Figure 1: Average loss on test data depending on the inﬂuence of the rate loss c(cid:21) for different
immediate loss constants c− and c+ .

10,000 randomly selected accounts over two days and label them automatically based on information
passed by other email service providers via feedback loops (in most cases triggered by “report spam”
buttons). Because of this automatic labeling process, the labels contain a certain smount of noise.
Feature mapping ϕ determines a vector of moving average and moving variance estimates of several
attributes of the email stream. These attributes measure the frequency of subject changes and sender
address changes, and the number of recipients. Other attributes indicate whether the subject line
or the sender address have been observed before within a window of time. Additionally, a moving
average estimate of the rate ρ is used as feature. Finally, other attributes quantify the size of the
message and the score returned by a content-based spam ﬁlter employed by the email service.
We implemented the baseline methods that were descibed in Section 4, namely the iterative SVM
methods It-SVMub and It-SVMit and the policy gradient methods PGub and PGit . Additionally,
we used a standard support vector machine classiﬁer SVM with weights of misclassiﬁcation corre-
sponding to the costs deﬁned in Equation 1. The Erlang learning model is denoted ELM in the plots.
Linear decision functions were used for all baselines.
In our experiments, we assume a cost
is,
That
that
is quadratic in the outbound rate.
′ |x, t)2 with c(cid:21) > 0 determining the inﬂuence of the rate loss to the
′ |x, t))) = c(cid:21) · r(cid:25) (t
λ(1, r(cid:25) (t
overall loss. The time interval τ was chosen to be 100 seconds. Regularizer Ω(θ) as in Optimization
problem 1 is the commonly used squared l2 -norm Ω(θ) = ∥θ∥2
2 .
We evaluated our method for different costs of incorrectly classiﬁed non-spam emails (c− ), incor-
rectly classiﬁed spam emails (c+ ) (see the deﬁnition of ℓ in Equation 1), and rate of outbound spam
messages (c(cid:21) ). For each setting, we repeated 100 runs; each run used about 50%, chosen at random,
as training data and the remaining part as test data. Splits where chosen such that there were equally
many spam episodes in training and test set. We tuned the regularization parameter η for the Erlang
learning model as well as the corresponding regularization parameters of the iterative SVM methods
and the standard SVM on a separate tuning set that was split randomly from the training data.

5.1 Results

Figure 1 shows the resulting average loss of the Erlang learning model and reference methods.
Each of the three plots shows loss versus parameter c(cid:21) which determines the inﬂuence of the rate
loss on the overall loss. The left plot shows the loss for c− = 5 and c+ = 1, the center plot for
(c− = 10, c+ = 1), and the right plot for (c− = 20, c+ = 1).
We can see in Figure 1 that the Erlang learning model outperforms all baseline methods for larger
values of c(cid:21)—more inﬂuence of the rate dependent loss on the overall loss—in two of the three
settings. For c− = 20 and c+ = 1 (right panel), the performance is comparable to the best baseline
method It-SVMub ; only for the largest shown c(cid:21) = 5 does the ELM outperform this baseline. The
iterative classiﬁer It-SVMub that uses the approximated state-action loss Qub performs uniformly
better than It-SVMit , the iterative SVM method that uses the sampled loss from the previous it-
eration. It-SVMit itself surprisingly shows very similar performance to that of the standard SVM
method; only for the setting c− = 20 and c+ = 1 in the right panel does this iterative SVM method
show superior performance. Both policy gradient methods perform comparable to the Erlang learn-
ing model for smaller values of c(cid:21) but deteriorate for larger values.

7

 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5Losscλc-=5, c+=1ELMIt-SVMitIt-SVMubPGubPGitSVM 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5Losscλc-=10, c+=1ELMIt-SVMitIt-SVMubPGubPGitSVM 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5Losscλc-=20, c+=1ELMIt-SVMitIt-SVMubPGubPGitSVM(a) Average loss and standard error for
small values of cλ .

(b) Left: Fortet’s formula B (eϕθ ; (cid:26) (cid:28) ) (Equation 17) and its
upper bound (cid:0) log(1 (cid:0) B (eϕθ ; (cid:26) )) for (cid:26) (cid:28) = 10. Right:
1 (cid:0) B (eϕθ ; (cid:26) ) and respective upper bound (cid:0) log(B (eϕθ ; (cid:26) )).

As expected, the iterative SVM and the standard SVM algorithms perform better than the Erlang
learning model and policy gradient models if the inﬂuence of the rate pedendent loss is very small.
This can best be seen in Figure 2(a). It shows a detail of the results for the setting c− = 5 and
c+ = 1, for c(cid:21) ranging only from 0 to 1. This is the expected outcome following the considerations
in Section 4. If c(cid:21) is close to 0, the problem approximately reduces to a standard binary classiﬁ-
cation problem, thus favoring the very good classiﬁcation performance of support vector machines.
However, for larger c(cid:21) the inﬂuence of the rate dependent loss rises and more and more dominates
the immediate classiﬁcation loss ℓ. Consequently, for those cases — which are the important ones in
this real world application — the better rate loss estimation of the Erlang learning model compared
to the baselines leads to better performance.
The average training times for the Erlang learning model and the reference methods are in the same
order of magnitude. The SVM algorithm took 14 minutes in average to converge to a solution. The
Erlang learning model converged after 44 minutes and the policy gradient methods took approxi-
mately 45 minutes. The training times of the iterative classiﬁer methods were about 60 minutes.

6 Conclusion

We devised a model for sequential decision-making problems in which events are generated by a
Poisson process and the loss may depend on the rate of decision outcomes. Using a throttling policy
that enforces a data-dependent rate-limit, we were able to factor the loss over single events. Applying
a result from queuing theory led us to a closed-form approximation of the immediate event-speciﬁc
loss under a rate limit set by a policy. Both parts led to a closed-form convex optimization problem.
Our experiments explored the learning model for the problem of suppressing abuse of an email
service. We observed signiﬁcant improvements over iterative reinforcement learning baselines. The
model is being employed to this end in the email service provided by web hosting ﬁrm STRATO.
It has replaced a procedure of manual deactivation of accounts after inspection triggered by spam
reports.

Acknowledgments

We gratefully acknowledge support from STRATO Rechenzentrum AG and the German Science
Foundation DFG.

References
[1] J.A. Bagnell, S. Kakade, A. Ng, and J. Schneider. Policy search by dynamic programming.
Advances in Neural Information Processing Systems, 16, 2004.
[2] D. Blatt and A.O. Hero. From weighted classiﬁcation to policy search. Advances in Neural
Information Processing Systems, 18, 2006.
[3] C. Dimitrakakis and M.G. Lagoudakis. Rollout sampling approximate policy iteration. Machine
Learning, 72(3):157–171, 2008.

8

 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0.2 0.4 0.6 0.8 1Losscλc-=5, c+=1ELMIt-SVMitIt-SVMubPGubPGitSVM 0 1 2 0 1 2 3φ*θFortet functionwith convex upper boundB(exp(φθ),ρτ)-log(1-B(exp(φθ),ρτ)) 0 1 2-1 0 1 2φ*θComplement of Fortet functionwith convex upper bound1-B(exp(φθ),ρτ)-log(B(exp(φθ),ρτ))[4] M. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. Advances in Neural
Information Processing Systems, 19, 2007.
[5] D.L. Jagerman, B. Melamed, and W. Willinger. Stochastic modeling of trafﬁc processes. Fron-
tiers in queueing: models, methods and problems, pages 271–370, 1996.
[6] M.G. Lagoudakis and R. Parr. Reinforcement learning as classiﬁcation: Leveraging modern
classiﬁers. In Proceedings of the 20th International Conference on Machine Learning, 2003.
[7] J. Langford and B. Zadrozny. Relating reinforcement learning performance to classiﬁcation
performance. In Proceedings of the 22nd International Conference on Machine learning, 2005.
[8] R.S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforce-
ment learning with function approximation. Advances in Neural Information Processing Sys-
tems, 12, 2000.

9


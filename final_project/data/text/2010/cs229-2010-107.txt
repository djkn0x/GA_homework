Semi-Supervised Learning of Named Entity Substructure

Alden Timme
aotimme@stanford.edu
CS229 Final Project

Advisor: Richard Socher
richard@socher.org

Abstract

The goal of this project was two-fold: (1)
to provide an algorithm to correctly ﬁnd
and label named entities in text, and (2) to
uncover substructure in the named entities
(such as a ﬁrst name, last name distinction
among person entities). The underlying
algorithm used is a Class Hidden Markov
Model (CHMM), a Hidden Markov Model
with hidden states that emit observed
words as well as observed classes. This
algorithm is further bolstered by incorpo-
rating features into the model, substituting
the multinomial probability distributions
for transitions and emissions in the model
with the outputs of logistic regressions us-
ing the features.

1

Introduction

Named-Entity Recognition (NER) is a task in Natu-
ral Language Processing (NLP) which aims to iden-
tify entity types of interest in a collection of text.
For example, one might want to ﬁnd entities corre-
sponding to a person (PER), location (LOC), or or-
ganization (ORG). Given a set of entity types, the
task of NER is to ﬁnd and correctly label entities
of these types within text, and correctly label other
words as other. Supervised training of a model to
perform NER gives the model a set of labeled train-
ing data and aims to learn enough to correctly label
entities on the test set. NER can be seen as a very
useful task because such entities are often some of

the most important words for determining the con-
tent of a document. As such, one of the obvious
goals of this project was to maximize the precision
and recall of NE assignments. Precision is the accu-
racy of predicted entities (e.g. the probability that an
entity labeled PER is actually a person), and recall
is the ability to recognize an entity (e.g. the proba-
bility that a person entity is actually labeled PER).
The other goal of this project, and a different ob-
jective than other NLP algorithms that are used for
NER solely, is to see if the model could uncover
a latent substructure to the named entities encoun-
tered. That is, we wanted to see if the model would
be able to, for example, discriminate between the
ﬁrst name and last name of a person while labeling
the entire entity as a person, and maybe even be able
to discriminate a third subset which corresponds to
an abbreviation of the ﬁrst name. For instance, take
the person John Ratcliffe, which could appear in its
full form as John Ratcliffe or in other forms such as
John, Ratcliffe, or J. Ratcliffe. An idea of the project
was to see if the model could identify all occurrences
as PER but also identify the differences in the words
that make up the same entity - i.e. that J. and Rat-
cliffe are two different parts of the same entity.
Evaluation of the system is done on two different
data sets. They are the CoNLL-2003 English NE
data set 1 and the MUC-7 English NE data set 2 . La-
beling and evaluation of the NER task requires that
the system correctly determines the entities’ types as
well as their start and end boundaries.

1www.cnts.ua.ac.be/conll2003/ner/
2www-nlpir.nist.gov/related projects/muc/proceedings/muc 7 toc.html

2 The Model

The basic model used for the project is a Class Hid-
den Markov Model as proposed in Krogh (1994). To
supplement the ability of the CHMM as a NE classi-
ﬁer, we then incorporate features in the model using
the same system as Berg-Kirkpatrick, et al (2010).

2.1 Class Hidden Markov Model

(named entities) are then deterministically given by
the state assignments.

2.2 Feature-Enhanced CHMM

Figure 1: The graphical structure of a Class Hidden
Markov Model for NER, in which classes and words
are observed.

This is a Hidden Markov Model where there are
two observed states for each hidden state. In partic-
ular, these observed states are the words of the text
and the classes of entity to which they belong. In re-
ality, a CHMM can be thought of as a regular HMM
which emits a pair of values. However, thinking of
it as a model with two separate emission types helps
for inference, in which only the words are observed.
One can either allow a probability distribution over
classes for each state or assign a class determinis-
tically to each state.
In this project we make the
simplifying assumption that each state is assigned
to only one class. We also assign the same number
of states to each entity type, including other.
Learning and inference with a CHMM is done
in much the same way as with a normal HMM. In
the learning period we wish to determine the tran-
sition and emission probabilities given the observed
classes and words. This is done via a modiﬁed ver-
sion of the forward-backward equations using the
training data. With the one-class per state assump-
tion, this amounts to ﬁnding looking at only valid
paths through the model, where valid paths are those
where the state labels agree with the observed class
labels.
Inference on the test data is then done simply by
performing the standard viterbi algorithm to ﬁnd the
hidden states given the observed words. The classes

Figure 2: Feature-enhanced Class Hidden Markov
Model for NER, where transitions and word emis-
sions are feature-based, with features f and feature-
weights λ. Superscript (t) denotes transition fea-
tures and weights, and superscript (e) denotes emis-
sion features and weights.

p(y (cid:48) |y , λ(t) , f (t) ) =

The feature-enhanced Class Hidden Markov
Model makes use of word features to enhance the
performance of the CHMM. The idea for feature
incorporation comes from Berg-Kirkpatrick et al.
(2010), in which each component multinomial of the
model become the outputs of local multi-class logis-
tic regressions. That is, the transition probability be-
tween two states y and y (cid:48) and the emission probabil-
(cid:16)(cid:80)
(cid:17)
ity of word x from state y are given by, respectively,
(cid:16)(cid:80)
(cid:17)
(cid:80)
i (y , y (cid:48) )
i f (t)
i λ(t)
exp
(cid:17)
(cid:16)(cid:80)
i (y , y (cid:48)(cid:48) )
i λ(t)
i f (t)
y (cid:48)(cid:48) exp
(cid:17)
(cid:16)(cid:80)
(cid:80)
i λ(e)
i f (e)
(y , x)
exp
i
(y , x(cid:48) )
i f (e)
i λ(e)
x(cid:48) exp
i
where superscripts (e) and (t) correspond to emis-
sion and transition features and weights.
Given a normal prior on the feature weights, we
get the regularized log-likelihood of the observed
data as
L(λ) = log P (X = x, C = c|λ, f ) − κ||λ||2
2

p(x|y , λ(e) , f (e) ) =

where x and c are the word and class sequences.
In order to ﬁnd the optimal feature weights λ, we
optimize the regularized log-likelihood by direct
gradient ascent using any Hessian free optimizer (in
this case, LBFS). In order to climb L, we need a
formula for its gradient. However, it turns out that

its gradient is equal to the gradient of the regularized
expected log likelihood,
(cid:40)
f (y , y (cid:48) ) − (cid:88)
∇(cid:96)(λ, e) = (cid:88)
(cid:40)
f (y , x) − (cid:88)
+ (cid:88)
z (cid:48)(cid:48)
y ,y (cid:48)
by ,x(cid:48) f (y , x(cid:48) )
ey ,x
x(cid:48)
y ,x
− 2κλ

ay ,y (cid:48)(cid:48) f (y , y (cid:48)(cid:48) )
(cid:41)

ey ,y (cid:48)

where

ay ,y (cid:48) = p(y (cid:48) |y , λ(t) , f (t) )
by ,x = p(x|y , λ(e) , f (e) )

as computed above, and ey ,x , ey ,y (cid:48) are the expected
counts of the number of emissions of word x from
state y and the expected number of transitions from
y to y (cid:48) .
These expected counts are calculated
through the modiﬁed viterbi algorithm where only
valid paths are considered. At e = e(λ0 ),
∇λ (cid:96)(λ, e(λ0 )) = ∇λL(λ)

so we can use this gradient for the direct gradient as-
cent optimizations, recalculating e(λ) at every step.

3 Results and Evaluation

We ran the model on two well known NER data
sets,
the CoNLL-2003 English data set and the
MUC-7 English data set. The CoNLL data set has
four different named entities, location (LOC), mis-
cellaneous (MISC), organization (ORG), and per-
son (PER), as well as the other (O) class. The
MUC-7 data set has seven different entities, date
(DATE), location (LOC), money (MONEY), orga-
nization (ORG), percent (PERCENT), person (PER-
SON), location (LOC), and time (TIME), along with
the other (O) class. Results for each entity type (and
all entity types) are measured in terms of precision,
recall, and F1 ,

Precision:

Recall:

F1 :

R =

P =

T P
T P + F P
(cid:18) P · R
T P
T P + F N
F1 = 2
P + R

(cid:19)

(cid:41)

Feature
OneDigitNum
TwoDigitNum
FourDigitNum
YearDecade
ContainsDigitAndDash
ContainsDigitAndOneSlash
ContainsDigitAndComma
ContainsDigitAndPeriod
AllCaps
CapPeriod
CapOtherPeriod
CapPeriods
InitialCap

Explanation
Example
Number
9
Year
90
Year
1990
Decade
1990s
Date
03-90
Date
3/4
Money
10,000
10.00 Money/Percent
Organization
IBM
Name Abbr.
J.
St.
Location Abbr.
Loc or Org
N.Y.
Ratcliffe
Capitalized

Table 1: Features used for the feature-enhanced
model on the MUC-7 data set.

where T P is the number of true positives (correct
classiﬁcations), F P is the number of false positives
(incorrectly classiﬁed the entity as its type), and F N
is the number of false negatives (incorrectly classi-
ﬁed entity as not its type).
For each data set, the model was ﬁrst run with-
out features, where the only “features” are the basic
features. A basic feature is the baseline in which the
only feature for a word is the word itself. This corre-
sponds to what is the original Class Hidden Markov
Model without any features. For all models, the only
features used for transitions are the basic features,
since there do not seem to be any notable features of
states that would help with classiﬁcation.
Runs were then done with features. For the
CoNLL data set, I used two different feature sets.
One feature set only includes two features: (1) all
letters in a word are capitalized, and (2) the ini-
tial letter is capitalized. The second feature set was
the “word shape”, where there are only three word
shapes: (1) all letters are capitalized (XX), (2) only
the ﬁrst letter is capitalized (Xx), and (3) all letters
are lower case (xx). The MUC-7 data set feature-
enhanced runs incorporated many more features (Ta-
ble 1), culled from Zhou (2002). Figure 3 shows the
F1 scores for the basic and feature-enhanced runs on
the CoNLL data set and MUC-7 data set with dif-
ferent values of the regularization parameter κ and
different values for the number of states per class

SPC
1
1
1
2
2
2
5
5
5

Type
Basic
Capitalization
Word Shape
Basic
Capitalization
Word Shape
Basic
Capitalization
Word Shape

LOC MISC
0.6500
0.7451
0.7386
0.8135
0.7301
0.7999
0.7247
0.6039
0.6517
0.7809
0.7383
0.7844
0.7082
0.6364
0.6700
0.7565
0.7537
0.6724

ORG
0.5129
0.6654
0.6443
0.3959
0.5934
0.6662
0.4664
0.6172
0.6440

PER
0.5732
0.8297
0.8191
0.6355
0.7989
0.8124
0.6630
0.7938
0.7987

OVERALL
0.6276
0.7744
0.7607
0.6105
0.7197
0.7584
0.6342
0.7228
0.7277

Table 2: F1 scores for best regularization parameter (κ = 0.25) on CoNLL data set with different feature
sets (SPC is the number of states per NE class).

SPC
1
1
2
2
5
5

Type
Basic
Features
Basic
Features
Basic
Features

DATE
0.5986
0.6876
0.5526
0.6833
0.5662
0.7119

LOC MONEY ORG PERCENT PERSON TIME OVERALL
0.5846
0.4673
0.4041
0.3582
0.3960
0.2297
0.4903
0.4582
0.2299
0.5129
0.8374
0.4518
0.5617
0.6991
0.4706
0.3277
0.2759
0.8911
0.3699
0.5782
0.5193
0.5822
0.2275
0.4937
0.8812
0.5035
0.5899
0.6250
0.5315
0.4593
0.3705
0.8469
0.3983
0.2793
0.5818
0.6274
0.2791
0.5517
0.8641
0.5848
0.5446
0.6669

Table 3: F1 scores for best regularization parameter (κ = 0.25) on MUC-7 data set with and without features
(SPC is the number of states per NE class).

(SPC). Tables 2 and 3 contain the results in terms
of F1 scores for the CoNLL and MUC-7 data sets
respectively.

4 Conclusion

As can be seen fairly obviously from Figure 2 and
Figure 3, incorporating features helps quite a bit.
With both data sets, the F1 scores are considerably
lower when the model considers only the basic fea-
tures. Unfortunately, however, there does not seem
to be anything added by the Class HMM for the
CoNLL data set. Having one state per class almost
always does best, suggesting that a feature-enhanced
regular HMM would do better than a Class HMM
with a number of states per class. The MUC-7 data
set does beneﬁt from including a number of states
per class. However, the observation on the CoNLL
data set also suggests that the CHMM does not ex-
ploit a substructure to the named entities. And,
in fact, from analysis of the inference on both the
CoNLL and MUC-7 data sets, I cannot ﬁnd any evi-
dence of the model ﬁnding a substructure within the

Figure 3: F1 scores on the CoNLL data set (top) and
MUC-7 data set (bottom) with different feature sets
and different numbers of states per NE class.

named entities it classiﬁes.
The model also does not do a very good job on the
NER task itself, falling well short of state-of-the-art
numbers, which have F1 scores around 0.89 for the
CoNLL data set and 0.94 for the MUC-7 data set.
Perhaps including more features would help, but I
do not think that better features would account for a
20 − 30% jump in F1 .

References
Anders Krogh. 1994. Hidden Markov Models for La-
beled Sequences. Proceedings of the 12th IAPR In-
ternational Conference on Pattern Recognition. Los
Alamitos IEEE Computer Society Press, California.

GuoDong Zhou and Jian Su.
2002. Named Entity
Recognition using and HMM-based Chunk Tagger.
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 473-480.
Philadelphia, PA, July 2002.

Taylor Berg-Kirkpatrick, Bouchard-Cote Alexandre,
DeNero John, Klein Dan. 2010. Painless Unsuper-
vised Learning with Features. The 2010 Conference
of the North American Chapter of the ACL, pages 582-
590. Los Angeles, California, June 2010.


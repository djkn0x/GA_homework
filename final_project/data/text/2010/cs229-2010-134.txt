VOCAL-BASED MUSICAL GENRE 
CLASSIFICATION 
Arun Miduthuri 
Bryan Huh 
 
 
 
 

Abstract 

 
 
 
Mu s ical  g enre s   are  label s  as s igne d  to  piec e s   of  music.  F eature s   of  a  g enre  
g enerally  include  lyrical  structure,   rhythmic  structure,  instrumentation,  and 
harmonic  cont ent.  In  rec ent  year s,  efforts  hav e  be en  made  to  clas s ify  genre s   of 
music  u sing  machine  l earning  t echnique s   on  the  abov e   mentioned  aspe ct s  of  a 
song.  Our  project  attempts  to  find  what  audio  feature s   are  more  important  when 
clas s ifying  vocal  track s   into  different  g enre s .  Two  vocal  datas e t s   are  examined: 
one  containing  Indian  vocal  g enre s   with  no  accompaniment,  and  another  with 
accompaniment filtered out with the he lp of a rec ent vocal  s eparation t echnique. 
 
 
Introduction 
The  vast  majority  of  research  on  musical 
 
genre  classification  has  targeted  feature-
As  the  digital  database  for  music  grows,  so 
extraction.  Previous  studies  using  "timbral 
does 
its  organization. 
the  demand  for 
features"  such  as  Mel-Frequency  Cepstral 
Currently,  much  of  music  classification  is 
Coefficients 
(MFCCs) 
and 
rhythmic 
still  done  by  individual  users  who  store  the 
features  such  as  beat  features,  chroma 
artist  name,  song  title,  genre,  and  album  in 
features,  and  other  pitch-related  features 
the  metadata  of  the  music  file.  However, 
have  shown  that  by  far  the  most  useful 
the  vast  quantity  of  music  files  on  the  Web 
feature 
in  genre  classification 
is 
the 
is making  the manual  classification  of music 
MFCCs.  Surprisingly,  those  features  which 
libraries  more  and  more  infeasible.  The 
humans 
seem 
to 
rely  on 
in  genre 
automatic  classification  of  music  has  thus 
classification  such  as  rhythm,  harmony,  and 
become  an  important  problem,  and  is  one  of 
vocal  content  have  not  yet  made  an  impact 
the  goals  of  Music  Information  Retrieval 
in 
automatic  genre 
classification. 
In 
(MIR). 
genre 
Automatic  musical 
particular,  the  role  of  vocals  separately  in 
classification  is  particularly  sought  after,  but 
genre  classification  has  never  been  formally 
it  is  also  a  great  challenge.  The  boundaries 
addressed  before.  Since  vocals  play  a  large 
of  a  genre  are  generally  not  well  defined, 
role  for  humans  in  genre  classification,  it  is 
and  even  humans  will  often  disagree  on  the 
an  important  question  how  much  they  are 
genre 
of 
a 
song. 
Indeed, 
human 
being 
utilized 
in 
automatic 
genre 
performance  on  genre  classification  has 
classification.  Perhaps  features  which  have 
shown  that  automatic  genre  classification  is 
had  little  contribution  in  the  past,  when 
fundamentally  limited  by  the  subjectivity  of 
extracted  from  vocals,  can  make  a  greater 
genre [0]. 
impact in musical genre classification. In this 

R elated Work 

paper, we  determine  the  relative importance 
of 
various 
features 
in 
the 
genre 
classification  of  a  standard  Genre  dataset 
used in [1] and a primarily-vocal dataset. 
 
 
 
Musical  genre  classification  has  been 
explored  in  detail  in  the  seminal  work  by 
Tzanetakis  et  al  [1],[2].  Related  problems 
identification  [3],  finding 
include  singer 
using 
similar 
music 
unsupervised 
methods[4],  and 
locating  singing  voice 
segments  within  musical  pieces[5].  Related 
to  our  task  of  vocal  genre  classification  is 
the  broader  idea  of  separating  vocals  from 
accompaniment  in  monoaural  recordings. 
This  has  been  done  in  several  different 
ways,  for  example,  independent  component 
analysis [6], mixed Gaussian models [7], and 
peak  clustering  [8].  In  our  work  we  make 
use  of 
three  primary 
features:  Mel-
frequency  cepstral  coefficients,  chroma 
features,  and  linear  spectral  pair  features 
(LSPs).  MFCCs  and  LSPs  are  widely  used 
for  speech  discrimination,  but  have  proven 
useful  in music  classifiers,  for  example,  see 
[5].  There  are  additional  spectral  centroid, 
rolloff,  and  flux  features  added  on  the  basis 
of  features  used  in  [1].  In  addition,  linear 
spectral  pair  coefficients  have  been  added 
as  they  are  widely  used  in  speech  coding. 
Chroma  features  are  commonly  used  to 
capture  pitch  content  and  harmony.  Since 
LSPs  are  primarily  used 
for  speech 
modeling,  it  would  not  be  surprising  if  LSPs 
contribute most  to  genre  classification  when 
they are extracted from pure vocals. 
 
 
 

D e s ign 

 
Datasets 
Three  datasets  (two  vocal  datasets)  were 
used  for  genre  classification.  The  first  is  a 
standard  genre  dataset  used  in  [1],  which 
we  will  call  the  original/general  Genre 
dataset.  We  selected  those  genres  which 
had  vocal  content:  Country,  Disco,  HipHop, 
Rock,  Blues,  Reggae,  Pop,  Metal.  The 
second  dataset  was  generated  from  this 
Genre  dataset  with  the  aim  of  making  the 
vocal  component  of 
the  songs  more 
prominent.  A  peak  clustering  algorithm 
(Marsyas)  was  used  to  isolate  the  vocals  of 
the  Genre  dataset.  We  call  this  the  Vocal-
separated  dataset.  Finally,  we  used  a 
dataset  of  traditional  Indian  music  since 
Indian music  has  a  large  databank  of  purely 
vocal  songs.  This  was  divided  into  eight 
genres: 
Female 
Female  Bollywood, 
Carnatic,  Female  Hindustani,  Female 
Mantra,  Male  Carnatic,  Male  Hindustani, 
Male  Qawwali,  and  Male  Rajasthani.  Male 
and female vocals were  separated for better 
training. 
 
Features 
The  genre  classification  was  done  using 
combinations  of 
features, 
the  MFCC 
spectral  features  (spectral  centroid,  spectral 
rolloff,  spectral  flux,  time  domain  zero 
crossings)  [1],  chroma  features  and  LSPs.  
We  adopt  the  name  “Timbral  features”  [1] 
for  the  collection  of  MFCCs  and  spectral 
features  combined.  It  is  standard  to  include 
Timbral  features  as  a  baseline.  We  then 
classified  using  Timbral  features  combined 
with  either  chroma  features  or  LSPs.  
Feature 
extraction  was 
done 
using 
MARSYAS,  an  open-source  software  used 
for  audio  analysis  [9].  For  each  audio  file,  a 

single feature vector was computed. 
 
Training and Classi fication 
Previous  results  [2]  have  shown  that  a 
Gaussian  SVM  classifier  is  a  successful 
classifier  in  genre  classification.  Thus  for 
each  of  our  data  sets  a  Gaussian  SVM 
classifier  was  used,  and  our  results  were 
obtained using 10-fold cross-validation. 
 
 
R e sult s  and Discu s s ion 
 
The  figures  on  the  next  page  (figure  2, 
figure 3)  summarize  the  results  for  the  three 
datasets  and  the  various  combinations  of 
features.  For  the  original  Genre  dataset, 
significantly 
chroma 
adding 
features 
improves 
classification 
the 
accuracy. 
However, linear  spectral pair  features made 
no  contribution.  For 
the  Indian  music 
dataset,  Timbral 
and  Timbral/Chroma 
features  alone  gave  poor  classification 
accuracy,  but  here  linear  spectral  pair 
features make a significant contribution. The 
same is true for the Vocal-Separated set. 
 
The  significant  contribution  of  the  linear 
spectral  pair 
features 
in 
the  genre 
classification  of  the  Indian  music  dataset  is 
not  surprising  considering  that  the  Indian 
music consisted entirely of vocals, and LSPs 
Figure 1(a): Confusion Matrix: w/LSP features 

have  traditionally  been  used  for  speech 
coding.  Examination  of 
the  confusion 
matrices  (Figure  1)  in  classification  runs 
where 
the 
linear  spectral  pairs  were 
included  and  left  out  shows  that  without 
them,  a  number  of  the  male  vocal  genres 
were  misclassified  as  belonging  to  Male 
Qawwali.  Interestingly,  one  of  the  female 
genres,  Female  Carnatic,  which  had 
melodies  in  the  same  pitch  range  as  some 
male  voices,  was  also misclassified  as Male 
Qawwali in  every  test  example.  The  genres 
are  intrinsically  somewhat  similar  to  one 
another  in  terms  of  harmonic  quality,  and 
are  generally  mainly  different  in  tone  of 
voice. Evidently linear  spectral pairs capture 
this  tone  difference.  However,  their  failure 
to  improve  the  classification  accuracy  for 
the original Genre dataset  suggests that their 
contribution  can  only  be  made  when  vocals 
are  at least moderately isolated. This can be 
seen  in  the  case  of  separated  vocals,  where 
LSPs  cause  a  dramatic  improvement  in 
classification  accuracy.  In  contrast,  the 
chroma  features,  which  capture  pitch  and 
harmony,  are  assisted  by  background 
instrumentation 
only  make 
they 
as 
contributions  in  the  original  Genre  dataset 
and  marginally 
in 
the  vocal-separated 
dataset, in which there continued to be some 
residual 
background 
accompaniment. 

Figure 1(b): Confusion Matrix: w/o LSP features 

 

FIGURE 2 

 
 

FIGURE 3 
CLASSIFICATION RESULTS AND FEATURE COMPARISON 
 
General genre clas s ification (Dataset 1) 

 

Fea tures 
Timbral 
Timbral + Chroma 
Timbral + LSP 

Fea tures 
Timbral 
Timbral + Chroma 
Timbral + LSP 

Fea tures 
Timbral 
Timbral + Chroma 
Timbral + Chroma + LSP 
 
 

Indian vocal music (Dataset 2) 

Separated vocals (Dataset 3) 

Accuracy 
47.1% 
54.8% 
47.7% 

Accuracy 
32.6% 
33.7% 
52.3% 

Accuracy 
23.2% 
29.5% 
46.9% 

[2] T .  L i ,  G .   Tzanetakis ,   “ Factors  in  Au tomatic Musical Genre 
C lassification  of Audio  S ignals ,” 2003  IEEE Workshop  on  
Applica tions  o f  S igna l  Processing   to  Aud io  and  Acoustics .  
 
[3] Y .E.  K im ,  Br ian  Wh itman ,   “ S inger Identification   in  Popular 
Music Recordings  Using  Vo ice Cod ing  Features ,” Proceedings  o f  
the 3rd  In terna tiona l  Con ference on  Music Information  
Retrieval,  2002 . 
 
[4] X .  Shao ,  C .  Xu ,  M .S .  Kankanhalli ,   “Unsupervised  
C lassification  of Music Genre Using  H idden  Markov  Model,” 
2004  IEEE Internationa l  Con ference on  Media  and  Expo  
(ICME) . 
 
[5] A  Berenzweig ,  D .  El lis ,   “Locating  S ing ing  Vo ice Segments  
with in  Musical S ignals ,” IEEE Workshop  on  the App lica tion  o f  
Signa l  Processing  to  Aud io  and  Acoustics ,  2002 . 
 
[6] S .  Sofianos ,  A .  Ariyaeein ia,  R .  Po lfreman ,  “ Towards  
Effective S ing ing  Vo ice Ex traction  from  S tereophonic 
Recordings ,” IEEE In t Con f  on  Acoustics ,  Speech  and  S igna l  
Processing  2008 . 
 
[7] T sai  et al . ,   “ B l ind  clustering  of popular  music recordings  
based on  singer  vo ice characteristics ,” 4 th  In terna tiona l  
Con ference on  Music Information  Retrieval,  2003 . 
 
[8] Lagrange et al ,   “Normalized  cuts  for predominant melod ic 
source separation ,” IEEE Transactions  on  Aud io ,  Speech,  and  
Language Processing ,  2008 . 
 
[9] Marsyas  too lk it  for audio  classification  tasks ,  
available h ttp ://marsyas .sourceforge.net 

 

Conclu sion and Future Work 
 
We  found  that  linear  spectral  pair  features 
are  most  useful  in  genre  classification  when 
they  can  be  extracted  from  vocals  without 
the 
background 
of 
interference 
instrumentation.  The  opposite  seems  to  be 
true with Chroma  features. This is consistent 
with  intuition,  and  it  suggests  that  first 
isolating  the  components  of  the  music  file, 
and 
the 
then  extracting  features  from 
isolated components  (in particular  the vocals) 
may be an important preceding procedure for 
improved musical genre classification. 
 
The most  immediate  avenue  for  future  work 
would  include  improving  upon  general  genre 
classification  by  including  linear  spectral 
pairs  from  the  separated  vocals.  It  would 
also  be  interesting  to  see  how  the  relative 
importance  of  features  changes  with  gradual 
attenuation 
of 
the 
background 
accompaniment.  In  particular  we  would  like 
to  see  if  any  other  features  commonly  used 
in  genre  classification  behave  differently 
when  the  vocals  are  more  prominent  (for 
example,  LPCCs  are  also  a  common  voice 
feature  like  LSPs).  We  could  also  test  other 
methods  for  separating  vocals,  such  as  by 
means  of  ICA  or  mixed  Gaussian  models. 
Finally,  we  would  like  to  expand  our  vocal 
dataset to include Western genres. 
 
R eferenc e s  
 
[0] S .  L ippens ,  J .  P .  Martens ,  M .  Leman ,  B .  Baets ,  H .  Meyer, 
and  G .   Tzanetakis .   “A  Comparison  of Human and  Au tomatic 
Musical Genre C lassification ,” in  Proceedings  o f   the   IEEE 
Internationa l Con ference on  Aud io ,  Speech and  S igna l 
Processing ,  2004 .   
 
[1] G .   Tzanetakis ,  P .  Cook ,   “Musical Genre Classification  o f 
Audio  S ignals ,” IEEE Transactions  on  Speech  and  Aud io  
Processing ,  Vo l  10 ,  No  5 ,  Ju ly  2002 . 
 


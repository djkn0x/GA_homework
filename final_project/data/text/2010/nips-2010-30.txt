Predictive State Temporal Difference Learning

Byron Boots
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213
beb@cs.cmu.edu

Geoffrey J. Gordon
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213
ggordon@cs.cmu.edu

Abstract

We propose a new approach to value function approximation which combines lin-
ear temporal difference reinforcement learning with subspace identiﬁcation. In
practical applications, reinforcement learning (RL) is complicated by the fact that
state is either high-dimensional or partially observable. Therefore, RL methods
are designed to work with features of state rather than state itself, and the suc-
cess or failure of learning is often determined by the suitability of the selected
features. By comparison, subspace identiﬁcation (SSID) methods are designed to
select a feature set which preserves as much information as possible about state.
In this paper we connect the two approaches, looking at the problem of reinforce-
ment learning with a large set of features, each of which may only be marginally
useful for value function approximation. We introduce a new algorithm for this
situation, called Predictive State Temporal Difference (PSTD) learning. As in
SSID for predictive state representations, PSTD ﬁnds a linear compression op-
erator that projects a large set of features down to a small set that preserves the
maximum amount of predictive information. As in RL, PSTD then uses a Bellman
recursion to estimate a value function. We discuss the connection between PSTD
and prior approaches in RL and SSID. We prove that PSTD is statistically consis-
tent, perform several experiments that illustrate its properties, and demonstrate its
potential on a difﬁcult optimal stopping problem.

1

Introduction

We wish to estimate the value function of a policy in an unknown decision process in a high dimen-
sional and partially-observable environment. We represent the value function in a linear architec-
ture, as a linear combination of features of (sequences of) observations. A popular family of learning
algorithms called temporal difference (TD) methods [1] are designed for this situation. In particular,
least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to estimate
its parameters from sampled trajectories, i.e., from sequences of feature vectors of visited states, by
solving a set of linear equations.
Recently, Parr et al. looked at the problem of value function estimation from the perspective of
both model-free and model-based reinforcement learning [5]. The model-free approach (which
includes TD methods) estimates a value function directly from sample trajectories. The model-based
approach, by contrast, ﬁrst learns a model of the process and then computes the value function from
the learned model. Parr et al. demonstrated that these two approaches compute exactly the same
value function [5]. In the current paper, we build on this insight, while simultaneously ﬁnding a
compact set of features using powerful methods from system identiﬁcation.
First, we look at the problem of improving LSTD from a model-free predictive-bottleneck perspec-
tive: given a large set of features of history, we devise a new TD method called Predictive State
Temporal Difference (PSTD) learning. PSTD estimates the value function through a bottleneck that

1

preserves only predictive information (Section 3). Second, we look at the problem of value function
estimation from a model-based perspective (Section 4). Instead of learning a linear transition model
in feature space, as in [5], we use subspace identiﬁcation [6, 7] to learn a PSR from our samples.
Since PSRs are at least as compact as POMDPs, our representation can naturally be viewed as a
value-directed compression of a much larger POMDP. Finally, we show that our two improved
methods are equivalent. This result yields some appealing theoretical beneﬁts: for example, PSTD
features can be explicitly interpreted as a statistically consistent estimate of the true underlying sys-
tem state. And, the feasibility of ﬁnding the true value function can be shown to depend on the
linear dimension of the dynamical system, or equivalently, the dimensionality of the predictive state
representation—not on the cardinality of the POMDP state space. Therefore our representation is
naturally “compressed” in the sense of [8], speeding up convergence.
We demonstrate the practical beneﬁts of our method with several experiments: we compare PSTD
to competing algorithms on a synthetic example and a difﬁcult optimal stopping problem. In the
latter problem, a signiﬁcant amount of prior work has gone into hand-tuning features. We show that,
if we add a large number of weakly relevant features to these hand-tuned features, PSTD can ﬁnd a
predictive subspace which performs much better than competing approaches, improving on the best
previously reported result for this problem by a substantial margin. The theoretical and empirical
results reported here suggest that, for many applications where LSTD is used to compute a value
function, PSTD can be simply substituted to produce better results.

2 Value Function Approximation
We start from a discrete time dynamical system with a set of states S , a set of actions A, a distribution
over initial states π0 , a transition function T , a reward function R, and a discount factor γ ∈ [0, 1].
J π (s) = E [(cid:80)∞
We seek a policy π , a mapping from states to actions. For a given policy π , the value of state s is
J π (s) = R(s) + γ (cid:80)
deﬁned as the expected discounted sum of rewards when starting in state s and following policy π ,
t=0 γ tR(st ) | s0 = s, π ]. The value function obeys the Bellman equation
s(cid:48) J π (s(cid:48) ) Pr[s(cid:48) | s, π(s)]
(1)
If we know the transition function T , and if the set of states S is sufﬁciently small, we can ﬁnd an
optimal policy with policy iteration: pick an initial policy π , use (1) to solve for the value function
J π , compute the greedy policy for J π (setting the action at each state to maximize the right-hand
side of (1)), and repeat. However, we consider instead the harder problem of estimating the value
function when s is a partially observable latent variable, and when the transition function T is
unknown. In this situation, we receive information about s through observations from a ﬁnite set
O . We can no longer make decisions or predict reward based on S , but instead must use a history
(an ordered sequence of action-observation pairs h = ah
t that have been executed and
1 oh
1 . . . ah
t oh
observed prior to time t): R(h), J (h), and π(h) instead of R(s), J π (s), and π(s). Let H be the set
of all possible histories. H is often very large or inﬁnite, so instead of ﬁnding a value separately for
each history, we focus on value functions that are linear in features of histories
J π (h) = wTφH (h)
(2)
Here w ∈ Rj is a parameter vector and φH (h) ∈ Rj is a feature vector for a history h. So, we can
wTφH (h) = R(h) + γ (cid:80)
rewrite the Bellman equation as
o∈O wTφH (hπo) Pr[hπo | hπ ]
where hπo is history h extended by taking action π(h) and observing o.

(3)

2.1 Least Squares Temporal Difference Learning
In general we don’t know the transition probabilities Pr[hπo | h], but we do have samples of state
t+1 = φH (ht+1 ), and immediate rewards Rt = R(ht ).
features φH
t = φH (ht ), next-state features φH
We can thus estimate the Bellman equation
1:k ≈ R1:k + γwTφH
wTφH
2:k+1
(Here we have used φH
1:k to mean the matrix whose columns are φH
for t = 1 . . . k .) We can can
t
immediately attempt to estimate the parameter w by solving this linear system in the least squares

(4)

2

(cid:0)φH
(cid:1)†
sense: ˆwT = R1:k
1:k − γφH
, where † indicates the pseudo-inverse. However, this solution
ference E[φH (h) − γ (cid:80)
2:k+1
t − γφH
is biased [3], since the independent variables φH
t+1 are noisy samples of the expected dif-
o∈O φH (hπo) Pr[hπo | h]]. In other words, estimating the value function
parameters w is an error-in-variables problem.
The least squares temporal difference (LSTD) algorithm ﬁnds a consistent estimate of w by right-
T (cid:16) 1
T(cid:17)−1
multiplying the approximate Bellman equation (Equation 4) by φH
(cid:80)k
(cid:80)k
(cid:80)k
T :
t
T − γ
t=1 RtφH
t+1φH
t=1 φH
t φH
t=1 φH
t
t
t
k
k
Here, φH
T can be viewed as an instrumental variable [3], i.e., a measurement that is correlated with
t
the true independent variables but uncorrelated with the noise in our estimates of these variables.
1:k φH
the empirical covariance matrices φH
As the amount of data k increases,
/k and
1:k
φH
2:k+1φH
T
/k converge with probability 1 to their population values, and so our estimate of the
1:k
matrix to be inverted in (5) is consistent. So, as long as this matrix is nonsingular, our estimate of
the inverse is also consistent, and our estimate of w converges to the true value with probability 1.

ˆwT = 1
k

(5)

T

3 Predictive Features
LSTD provides a consistent estimate of the value function parameters w; but in practice, if the
number of features is large relative to the number of training samples, then the LSTD estimate of w
is prone to overﬁtting. This problem can be alleviated by choosing a small set of features that only
contains information that is relevant for value function approximation. However, with the exception
of LARS-TD [9], there has been little work on how to select features automatically for value function
approximation when the system model is unknown; and of course, manual feature selection depends
on not-always-available expert guidance. We approach the problem of ﬁnding a good set of features
from a bottleneck perspective. That is, given a large set of features of history, we would like to ﬁnd
a compression that preserves only relevant information for predicting the value function J π . As we
will see in Section 4, this improvement is directly related to spectral identiﬁcation of PSRs.

3.1 Finding Predictive Features Through a Bottleneck

In order to ﬁnd a predictive feature compression, we ﬁrst need to determine what we would like to
predict. The most relevant prediction is the value function itself; so, we could simply try to predict
total future discounted reward. Unfortunately, total discounted reward has high variance, so unless
we have a lot of data, learning will be difﬁcult. We can reduce variance by including other prediction
tasks as well. For example, predicting individual rewards at future time steps seems highly relevant,
and gives us much more immediate feedback. Similarly, future observations hopefully contain
information about future reward, so trying to predict observations can help us predict reward.
We call these prediction tasks, collectively, features of the future. We write φT
for the vector of
t
all features of the “future at time t,” i.e., events starting at time t + 1 and continuing forward.
Now, instead of remembering a large arbitrary set of features of history, we want to ﬁnd a small
subspace of features of history that is relevant for predicting features of the future. We will call this
subspace a predictive compression, and we will write the value function as a linear function of only
the predictive compression of features. To ﬁnd our predictive compression, we will use reduced-
rank regression [10]. We deﬁne the following empirical covariance matrices between features of the
(cid:80)k
(cid:80)k
(cid:98)ΣT ,H = 1
(cid:98)ΣH,H = 1
future and features of histories:
t φH
t=1 φH
t φH
t=1 φT
T
T
Let LH be the lower triangular Cholesky factor of (cid:98)ΣH,H . Then we can ﬁnd a predictive compression
(6)
t
t
k
k
(cid:98)ΣT ,HL−TH for a truncated SVD [11], where U contains the left singular vectors, V contains the right
of histories by a singular value decomposition (SVD) of the weighted covariance: write U DV T ≈
singular vectors, and D is the diagonal matrix of singular values. (We can tune accuracy by keeping
(cid:98)U from the compressed space up to the space of features of the future, and we deﬁne (cid:98)V to be the
more or fewer singular values, i.e., columns of U , V , or D .) We use the SVD to deﬁne a mapping
3

optimal compression operator given (cid:98)U (in a least-squares sense, see [12] for details):
(cid:98)U = U D1/2
(cid:98)V = (cid:98)U T (cid:98)ΣT ,H ( (cid:98)ΣH,H )−1
(7)
By weighting different features of the future differently, we can change the approximate compression
in interesting ways. For example, as we will see in Section 4.2, scaling up future reward by a
constant factor results in a value-directed compression—but, unlike previous ways to ﬁnd value-
another example, let LT be the Cholesky factor of the empirical covariance of future features (cid:98)ΣT ,T .
directed compressions [8], we do not need to know a model of our system ahead of time. For
Then, if we scale features of the future by L−TT , the SVD will preserve the largest possible amount
of mutual information between history and future, yielding a canonical correlation analysis [13, 14].
Now that we have found a predictive compression operator (cid:98)V via Equation 7, we can replace the
3.2 Predictive State Temporal Difference Learning
t with the compressed features (cid:98)V φH
1:k ≈ R1:k + γwT (cid:98)V φH
wT (cid:98)V φH
features of history φH
in the Bellman recursion, Equation 4:
t
(8)
2:k+1
The least squares solution for w is still prone to an error-in-variables problem. The instrumental
variable φH is still correlated with the true independent variables and uncorrelated with noise, and
(cid:80)k
(cid:80)k
(cid:98)ΣR,H = 1
(cid:98)ΣH+ ,H = 1
so we can again use it to unbias the estimate of w . Deﬁne the additional covariance matrices:
t=1 RtφH
t+1φH
t=1 φH
Then, the corrected Bellman equation is wT (cid:98)V (cid:98)ΣH,H = (cid:98)ΣR,H + γwT (cid:98)V (cid:98)ΣH+ ,H , and solving for w
T
T
(9)
t
t
k
k
(cid:16) (cid:98)V (cid:98)ΣH,H − γ (cid:98)V (cid:98)ΣH+ ,H
(cid:17)†
wT = (cid:98)ΣR,H
gives us the Predictive State Temporal Difference (PSTD) learning algorithm:
So far we have provided some intuition for why predictive features should be better than arbitrary
features for temporal difference learning. Below we will show an additional beneﬁt: the model-free
algorithm in Equation 10 is, under some circumstances, equivalent to a model-based method which
uses subspace identiﬁcation to learn Predictive State Representations [6, 7].
4 Predictive State Representations
A predictive state representation (PSR) [15] is a compact and complete description of a dynami-
cal system. Unlike POMDPs, which represent state as a distribution over a latent variable, PSRs
represent state as a set of predictions of tests. Just as a history is an ordered sequence of action-
observation pairs executed prior to time t, we deﬁne a test of length i to be an ordered sequence of
action-observation pairs τ = a1 o1 . . . ai oi that can be executed and observed after time t [15].
The prediction for a test τ after a history h, written τ (h), is the probability that we will see
the test observations τ O = o1 . . . oi , given that we intervene [16] to execute the test actions
τ A = a1 . . . ai : τ (h) = Pr[τ O | h, do(τ A )].
If Q = {τ1 , . . . , τn} is a set of tests, we write
Q(h) = (τ1 (h), . . . , τn (h))T for the corresponding vector of test predictions.
Formally, a PSR consists of ﬁve elements (cid:104)A, O, Q, s1 , F (cid:105). A is a ﬁnite set of possible actions,
and O is a ﬁnite set of possible observations. Q is a core set of tests, i.e., a set whose vector of
predictions Q(h) is a sufﬁcient statistic for predicting the success probabilities of all tests. F is
the set of functions fτ which embody these predictions: τ (h) = fτ (Q(h)). And, m1 = Q() is
the initial prediction vector. In this work we will restrict ourselves to linear PSRs, in which all
τ Q(h) for some vector rτ ∈ R|Q| . Finally, a core
prediction functions are linear: fτ (Q(h)) = rT
set Q is minimal if the tests in Q are linearly independent [17, 18], i.e., no one test’s prediction is a
linear function of the other tests’ predictions.
Since Q(h) is a sufﬁcient statistic for all tests, it is a state for our PSR: i.e., we can remember just
Q(h) instead of h itself. After action a and observation o, we can update Q(h) recursively: if we
aoτ for τ ∈ Q, then we can use Bayes’ Rule to show:
write Mao for the matrix with rows rT
MaoQ(h)
MaoQ(h)
Pr[o | h, do(a)]
mT∞MaoQ(h)

Q(hao) =

(10)

=

4

(11)

where m∞ is a normalizer, deﬁned by mT∞Q(h) = 1 for all h. In addition to the above PSR param-
eters, for reinforcement learning we need a reward function R(h) = ηTQ(h) mapping predictive
states to immediate rewards, a discount factor γ ∈ [0, 1] which weights the importance of future
rewards vs. present ones, and a policy π(Q(h)) mapping from predictive states to actions.
Instead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [6, 7]. TPSRs are a gener-
alization of regular PSRs: a TPSR maintains a small number of sufﬁcient statistics which are linear
combinations of a (potentially very large) set of test probabilities. That is, a TPSR maintains a small
number of feature predictions instead of test predictions. TPSRs have exactly the same predictive
abilities as regular PSRs, but are invariant under similarity transforms: given an invertible matrix
S, we can transform m1 → Sm1 , mT∞ → mT∞S−1 , and Mao → SMaoS−1 without changing the
corresponding dynamical system, since pairs S−1S cancel in Eq. 11. The main beneﬁt of TPSRs
over regular PSRs is that, given any core set of tests, low dimensional parameters can be found
using spectral matrix decomposition and regression instead of combinatorial search. In this respect,
TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace
identiﬁcation [19, 20, 14, 21].
4.1 Learning Transformed PSRs
Let Q be a minimal core set of tests, so that n = |Q| is the linear dimension of the system. Then, let
T be a larger core set of tests (not necessarily minimal), and let H be the set of all possible histories.
t ∈ R(cid:96) for a vector of features of history at time t, and write φT
t ∈ R(cid:96) for a vector
As before, write φH
of features of the future at time t. Since T is a core set of tests, by deﬁnition we can compute any test
prediction τ (h) as a linear function of T (h). And, since feature predictions are linear combinations
of test predictions, we can also compute any feature prediction φ(h) as a linear function of T (h).
We deﬁne the matrix ΦT ∈ R(cid:96)×|T | to embody our predictions of future features: an entry of ΦT is
the weight of one of the tests in T for calculating the prediction of one of the features in φT . Below
we deﬁne several covariance matrices, Equation 12(a–d), in terms of the observable quantities φT
t ,
φH
t , at , and ot , and show how these matrices relate to the parameters of the underlying PSR. These
relationships then lead to our learning algorithm, Eq. 14 below.
T | ht ∼ ω ]. Given
t φH
First we deﬁne ΣH,H , the covariance matrix of features of histories, as E[φH
(cid:98)ΣH,H = 1
t
k samples, we can approximate this covariance:
As k → ∞, (cid:98)ΣH,H converges to the true covariance ΣH,H with probability 1. Next we deﬁne ΣS ,H ,
1:k φH
k φH
T
(12a)
.
1:k
T (cid:12)(cid:12)(cid:12) ht ∼ ω (∀t)
(cid:105)
state at time t, let ΣS ,H = E (cid:104) 1
the cross covariance of states and features of histories. Writing st = Q(ht ) for the (unobserved)
k s1:k φH
. We cannot directly estimate ΣS ,H from
1:k
data, but this matrix will appear as a factor in several of the matrices that we deﬁne below. Next
we deﬁne ΣT ,H , the cross covariance matrix of the features of tests and histories (see [12] for
derivations):(cid:98)ΣT ,H ≡ 1
ΣT ,H ≡ E[φT
T | ht ∼ ω , do(ζ )] = ΦT RΣS ,H
k φT
1:k φH
t φH
(12b)
t
1:k
where row τ of the matrix R is rτ , the linear function that speciﬁes the prediction of the test τ given
the predictions of tests in the core set Q. By do(ζ ), we mean to approximate the effect of executing
all sequences of actions required by all tests or features of the future at once. This is not difﬁcult
in our experiments (in which all tests use compatible action sequences); but see [12] for further
discussion. Eq. 12b tells us that, because of our assumptions about linear dimension, the matrix
ΣT ,H has factors R ∈ R|T |×n and ΣS ,H ∈ Rn×(cid:96) . Therefore, the rank of ΣT ,H is no more than
number of samples k increases, (cid:98)ΣT ,H → ΣT ,H with probability 1.
n, the linear dimension of the system. We can also see that, since the size of ΣT ,H is ﬁxed, as the
Next we deﬁne ΣH,ao,H , a set of matrices, one for each action-observation pair, that represent the
(cid:80)k
covariance between features of history before and after taking action a and observing o.
In the
(cid:98)ΣH,ao,H ≡ 1
following, It (o) is an indicator variable for whether we see observation o at step t.
ΣH,ao,H ≡ E [ΣH,ao,H | ht ∼ ω (∀t), do(a) (∀t)]
Since the dimensions of each (cid:98)ΣH,ao,H are ﬁxed, as k → ∞ these empirical covariances converge
t=1 φH
It (o)φH
T
(12c)
t
t+1
k
to the true covariances ΣH,ao,H with probability 1. Finally we deﬁne ΣR,H , and approximate the
covariance (in this case a vector) of reward and features of history:

T

5

(12d)

(cid:80)k
(cid:98)ΣR,H ≡ 1
T | ht ∼ ω ] = ηTΣS ,H
ΣR,H ≡ E[RtφH
t=1 RtφH
Again, as k → ∞, (cid:98)ΣR,H converges to ΣR,H with probability 1.
T
t
t
k
We now wish to use the above-deﬁned matrices to learn a TPSR from data. To do so we need to
make a somewhat-restrictive assumption: we assume that our features of history are rich enough to
determine the state of the system, i.e., the regression from φH to s is exact: st = ΣS ,HΣ−1H,HφH
t .
We discuss how to relax this assumption in [12]. We also need a matrix U such that U TΦT R is
invertible; with probability 1 a random matrix satisﬁes this condition, but as we will see below, there
are reasons to choose U via SVD of a scaled version of ΣT ,H as described in Sec. 3.1. Using our
assumptions we can show a useful identity for ΣH,ao,H (for proof details see [12]):
ΣS ,HΣ−1H,HΣH,ao,H = MaoΣS ,H
(13)
This identity is at the heart of our learning algorithm: it states that ΣH,ao,H contains a hidden copy
of Mao , the main TPSR parameter that we need to learn. We would like to recover Mao via Eq. 13,
†
Mao = ΣS ,HΣ−1H,HΣH,ao,HΣ
S ,H ; but of course we do not know ΣS ,H . Fortunately, it turns out that
we can use U TΣT ,H as a stand-in, since this matrix differs only by an invertible transform (Eq. 12b).
We now show how to recover a TPSR from the matrices ΣT ,H , ΣH,H , ΣR,H , ΣH,ao,H , and U .
Since a TPSR’s predictions are invariant to a similarity transform of its parameters, our algorithm
only recovers the TPSR parameters to within a similarity transform [7, 12].
bt ≡ U TΣT ,H (ΣH,H )−1φH
t = (U TΦT R)st
(14a)
Bao ≡ U TΣT ,H (ΣH,H )−1ΣH,ao,H (U TΣT ,H )† = (U TΦT R)Mao (U TΦT R)−1
(14b)
η ≡ ΣR,H (U TΣT ,H )† = ηT (U TΦT R)−1
(14c)
bT
Our PSR learning algorithm is simple: replace each true covariance matrix in Eq. 14 by its empirical
estimate. Since the empirical estimates converge to their true values with probability 1 as the sample
size increases, our learning algorithm is clearly statistically consistent.
4.2 Predictive State Temporal Difference Learning (Revisited)
Finally, we are ready to show that the model-free PSTD learning algorithm introduced in Section 3.2
η s + γ (cid:80)
is equivalent to a model-based algorithm built around PSR learning. For a ﬁxed policy π , a PSR
η + γ (cid:80)
or TPSR’s value function is a linear function of state, V (s) = wT s, and is the solution of the
wT = (cid:98)ΣR,H (U T (cid:98)ΣT ,H )† + γ (cid:80)
PSR Bellman equation [22]: for all s, wT s = bT
o∈O wTBπo s, or equivalently, wT =
o∈O wTU T (cid:98)ΣT ,H ( (cid:98)ΣH,H )−1 (cid:98)ΣH,πo,H (U T (cid:98)ΣT ,H )†
o∈O wTBπo . Substituting in our learned PSR parameters from Equations 14(a–c), we get
bT
wTU T (cid:98)ΣT ,H = (cid:98)ΣR,H + γwTU T (cid:98)ΣT ,H ( (cid:98)ΣH,H )−1 (cid:98)ΣH+ ,H
since, by comparing Eqs. 12c and 9, we can see that (cid:80)
o∈O (cid:98)ΣH,πo,H = (cid:98)ΣH+ ,H . Now, deﬁne (cid:98)U and
(cid:98)V as in Eq. 7, and let U = (cid:98)U as suggested above in Sec. 4.1. Then U T (cid:98)ΣT ,H = (cid:98)V (cid:98)ΣH,H , and
(cid:16) (cid:98)V (cid:98)ΣH,H − γ (cid:98)V (cid:98)ΣH+ ,H
(cid:17)†
wT (cid:98)V (cid:98)ΣH,H = (cid:98)ΣR,H + γwT (cid:98)V (cid:98)ΣH+ ,H =⇒ wT = (cid:98)ΣR,H
Eq. 15 is exactly Eq. 10, the PSTD algorithm. So, we have shown that, if we learn a PSR by the
subspace identiﬁcation algorithm of Sec. 4.1 and then compute its value function via the Bellman
equation, we get the exact same answer as if we had directly learned the value function via the
model-free PSTD method. In addition to adding to our understanding of both methods, an important
corollary of this result is that PSTD is a statistically consistent algorithm for PSR value function
approximation—to our knowledge, the ﬁrst such result for a TD method.

(15)

5 Experimental Results
5.1 Estimating the Value Function of a RR-POMDP
We evaluate the PSTD learning algorithm on a synthetic example derived from [23]. The problem is
to ﬁnd the value function of a policy in a partially observable Markov decision Process (POMDP).
The POMDP has 4 latent states, but the policy’s transition matrix is low rank: the resulting belief
distributions lie in a 3-dimensional subspace of the original belief simplex (see [12] for details).

6

Figure 1: Experimental Results. Error bars indicate standard error. (A) Estimating the value func-
tion with a small number of informative features. All three approaches do well. (B) Estimating the
value function with a small set of informative features and a large set of random features. LARS-
TD is designed for this scenario and dramatically outperforms PSTD and LSTD. (C) Estimating
the value function with a large set of semi-informative features. PSTD is able to determine a small
set of compressed features that retain the maximal amount of information about the value function,
outperforming LSTD and LARS-TD. (D) Pricing a high-dimensional derivative via policy iteration.
The optimal threshold strategy (sell if price is above a threshold [24]) is in black, LSTD (16 canon-
ical features) is in blue, LSTD (on the full 220 features) is cyan, LARS-TD (feature selection from
set of 220) is in green, and PSTD (16 dimensions, compressing 220 features) is in red.

We perform 3 experiments, comparing the performance of LSTD, LARS-TD, and PSTD when dif-
ferent sets of features are used.
In each case we compare the value function estimated by each
algorithm to the true value function computed by J π = R(I − γT π )−1 . In the ﬁrst experiment
we execute the policy π for 1000 time steps. We split the data into overlapping histories and tests
of length 5, and sample 10 of these histories and tests to serve as centers for Gaussian radial ba-
sis functions. We then evaluate each basis function at every remaining sample. Then, using these
features, we learned the value function using LSTD, LARS-TD, and PSTD with linear dimension
3 (Figure 1(A)). Each method estimated a reasonable value function. For the second experiment,
we added 490 random, uninformative features to the 10 good features and then attempted to learn
the value function with each of the 3 algorithms (Figure 1(B)). In this case, LSTD and PSTD both
had difﬁculty ﬁtting the value function due to the large number of irrelevant features. LARS-TD,
designed for precisely this scenario, was able to select the 10 relevant features and estimate the
value function better by a substantial margin. For the third experiment, we increased the number of
sampled features from 10 to 500. In this case, each feature was somewhat relevant, but the number
of features was large compared to the amount of training data. This situation occurs frequently in
practice: it is often easy to ﬁnd a large number of features that are at least somewhat related to state.
PSTD outperforms LSTD and LARS-TD by summarizing these features and efﬁciently estimating
the value function (Figure 1(C)).
5.2 Pricing A High-dimensional Financial Derivative
Derivatives are ﬁnancial contracts with payoffs linked to the future prices of basic assets such as
stocks, bonds and commodities. In some derivatives the contract holder has no choices, but in more
complex cases, the holder must make decisions, and the value of the contract depends on how the
holder acts—e.g., with early exercise the holder can decide to terminate the contract at any time and
receive payments based on prevailing market conditions, so deciding when to exercise is an optimal
stopping problem. Stopping problems provide an ideal testbed for policy evaluation methods, since
we can collect a single data set which lets us evaluate any policy: we just choose the “continue”
action forever. (We can then evaluate the “stop” action easily in any of the resulting states.)
We consider the ﬁnancial derivative introduced by Tsitsiklis and Van Roy [24]. The derivative
generates payoffs that are contingent on the prices of a single stock. At the end of each day, the
holder may opt to exercise. At exercise the holder receives a payoff equal to the current price of the
stock divided by the price 100 days beforehand. We can think of this derivative as a “psychic call”:
the holder gets to decide whether s/he would like to have bought an ordinary 100-day European
call option, at the then-current market price, 100 days ago. In our simulation (and unknown to the
investor), the underlying stock price follows a geometric Brownian motion with volatility σ = 0.02
and continuously compounded short term growth rate ρ = 0.0004. Assuming stock prices ﬂuctuate
only on days when the market is open, these parameters correspond to an annual growth rate of
∼ 10%. In more detail, if wt is a standard Brownian motion, then the stock price pt evolves as
∇pt = ρpt∇t + σpt∇wt , and we can summarize relevant state at the end of each day as a vector

7

1234−10−50510151234−10−50510151234−10−50510150510152025300.951.001.051.101.151.201.251.30ValueStateStateStateExpected RewardPolicy IterationA.B.C.D.LSTDPSTDLARS-TDLSTD (16)LSTDPSTDLARS-TDThresholdJLSTDPSTDLARS-TDJLSTDPSTDLARS-TDJπππxt ∈ R100 , with xt = ( pt−99
, pt−98
pt
)T . This process is Markov and ergodic [24, 25]:
, . . . ,
pt−100
pt−100
pt−100
xt and xt+100 are independent and identically distributed. The immediate reward for exercising the
option is G(x) = x(100), and the immediate reward for continuing to hold the option is 0. The
discount factor γ = e−ρ is determined by the growth rate; this corresponds to assuming that the
risk-free interest rate is equal to the stock’s growth rate, meaning that the investor gains nothing in
expectation by holding the stock itself.
E[γ tG(xt ) | x0 = x].
The value of the derivative, if the current state is x, is given by V ∗ (x) = supt
Our goal is to calculate an approximate value function V (x) = wTφH (x), and then use this value
function to generate a stopping time min{t | G(xt ) ≥ V (xt )}. To do so, we sample a sequence
of 1,000,000 states xt ∈ R100 and calculate features φH of each state. We then perform policy
iteration on this sample, alternately estimating the value function under a given policy and then
using this value function to deﬁne a new greedy policy “stop if G(xt ) ≥ wTφH (xt ).”
Within the above strategy, we have two main choices: which features do we use, and how do we
estimate the value function in terms of these features. For value function estimation, we used LSTD,
LARS-TD, or PSTD. In each case we re-used our 1,000,000-state sample trajectory for all iterations:
we start at the beginning and follow the trajectory as long as the policy chooses the “continue” action,
with reward 0 at each step. When the policy executes the “stop” action, the reward is G(x) and the
next state’s features are all 0; we then restart the policy 100 steps in the future, after the process
has fully mixed. For feature selection, we are fortunate: previous researchers have hand-selected a
“good” set of 16 features for this data set through repeated trial and error (see [12] and [24, 25]). We
greatly expand this set of features, then use PSTD to synthesize a small set of high-quality combined
features. Speciﬁcally, we add the entire 100-step state vector, the squares of the components of the
state vector, and several additional nonlinear features, increasing the total number of features from
16 to 220. We use histories of length 1, tests of length 5, and (for comparison’s sake) we choose a
linear dimension of 16. Tests (but not histories) were value-directed by reducing the variance of all
features except reward by a factor of 100.
Figure 1D shows results. We compared PSTD (reducing 220 to 16 features) to LSTD with either
the 16 hand-selected features or the full 220 features, as well as to LARS-TD (220 features) and to
a simple thresholding strategy [24]. In each case we evaluated the ﬁnal policy on 10,000 new ran-
dom trajectories. PSTD outperformed each of its competitors, improving on the next best approach,
LARS-TD, by 1.75 percentage points. In fact, PSTD performs better than the best previously re-
ported approach [24, 25] by 1.24 percentage points. These improvements correspond to appreciable
fractions of the risk-free interest rate (which is about 4 percentage points over the 100 day window
of the contract), and therefore to signiﬁcant arbitrage opportunities: an investor who doesn’t know
the best strategy will consistently undervalue the security, allowing an informed investor to buy it
for below its expected value.
6 Conclusion
In this paper, we attack the feature selection problem for temporal difference learning. Although
well-known temporal difference algorithms such as LSTD can provide asymptotically unbiased es-
timates of value function parameters in linear architectures, they can have trouble in ﬁnite samples:
if the number of features is large relative to the number of training samples, then they can have
high variance in their value function estimates. For this reason, in real-world problems, a substantial
amount of time is spent selecting a small set of features, often by trial and error [24, 25]. To remedy
this problem, we present the PSTD algorithm, a new approach to feature selection for TD methods,
which demonstrates how insights from system identiﬁcation can beneﬁt reinforcement learning.
PSTD automatically chooses a small set of features that are relevant for prediction and value func-
tion approximation. It approaches feature selection from a bottleneck perspective, by ﬁnding a small
set of features that preserves only predictive information. Because of the focus on predictive infor-
mation, the PSTD approach is closely connected to PSRs: under appropriate assumptions, PSTD’s
compressed set of features is asymptotically equivalent to TPSR state, and PSTD is a consistent
estimator of the PSR value function.
We demonstrate the merits of PSTD compared to two popular alternative algorithms, LARS-TD
and LSTD, on a synthetic example, and argue that PSTD is most effective when approximating a
value function from a large number of features, each of which contains at least a little information
about state. Finally, we apply PSTD to a difﬁcult optimal stopping problem, and demonstrate the
practical utility of the algorithm by outperforming several alternative approaches and topping the
best reported previous results.

8

In Proc. Intl. Conf. Machine Learning,

References
[1] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9–44,
1988.
[2] Justin A. Boyan. Least-squares temporal difference learning.
pages 49–56. Morgan Kaufmann, San Francisco, CA, 1999.
[3] Steven J. Bradtke and Andrew G. Barto. Linear least-squares algorithms for temporal difference learning.
In Machine Learning, pages 22–33, 1996.
[4] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. J. Mach. Learn. Res., 4:1107–
1149, 2003.
[5] Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakeﬁeld, and Michael L. Littman. An anal-
ysis of linear models, linear value-function approximation, and feature selection for reinforcement learn-
ing. In ICML ’08: Proceedings of the 25th international conference on Machine learning, pages 752–759,
New York, NY, USA, 2008. ACM.
[6] Matthew Rosencrantz, Geoffrey J. Gordon, and Sebastian Thrun. Learning low dimensional predictive
representations. In Proc. ICML, 2004.
[7] Byron Boots, Sajid M. Siddiqi, and Geoffrey J. Gordon. Closing the learning-planning loop with predic-
tive state representations. In Proceedings of Robotics: Science and Systems VI, 2010.
[8] Pascal Poupart and Craig Boutilier. Value-directed compression of pomdps. In NIPS, pages 1547–1554,
2002.
[9] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares temporal difference
learning. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning,
pages 521–528, New York, NY, USA, 2009. ACM.
[10] Gregory C. Reinsel and Rajabather Palani Velu. Multivariate Reduced-rank Regression: Theory and
Applications. Springer, 1998.
[11] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University Press,
1996.
[12] Byron Boots and Geoffrey J. Gordon. Predictive state temporal difference learning. Technical report,
arXiv.org.
[13] Harold Hotelling. The most predictable criterion. Journal of Educational Psychology, 26:139–142, 1935.
[14] S. Soatto and A. Chiuso. Dynamic data factorization. Technical report, UCLA, 2001.
[15] Michael Littman, Richard Sutton, and Satinder Singh. Predictive representations of state. In Advances in
Neural Information Processing Systems (NIPS), 2002.
[16] Judea Pearl. Causality: models, reasoning, and inference. Cambridge University Press, 2000.
[17] Herbert Jaeger. Observable operator models for discrete stochastic time series. Neural Computation,
12:1371–1398, 2000.
[18] Satinder Singh, Michael James, and Matthew Rudary. Predictive state representations: A new theory for
modeling dynamical systems. In Proc. UAI, 2004.
[19] P. Van Overschee and B. De Moor. Subspace Identiﬁcation for Linear Systems: Theory, Implementation,
Applications. Kluwer, 1996.
[20] Tohru Katayama. Subspace Methods for System Identiﬁcation. Springer-Verlag, 2005.
[21] Daniel Hsu, Sham Kakade, and Tong Zhang. A spectral algorithm for learning hidden Markov models.
In COLT, 2009.
[22] Michael R. James, Ton Wessling, and Nikos A. Vlassis. Improving approximate value iteration using
memories and predictive state representations. In AAAI, 2006.
[23] Sajid Siddiqi, Byron Boots, and Geoffrey J. Gordon. Reduced-rank hidden Markov models. In Proceed-
ings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS-2010),
2010.
[24] John N. Tsitsiklis and Benjamin Van Roy. Optimal stopping of Markov processes: Hilbert space theory,
IEEE
approximation algorithms, and an application to pricing high-dimensional ﬁnancial derivatives.
Transactions on Automatic Control, 44:1840–1851, 1997.
[25] David Choi and Benjamin Roy. A generalized Kalman ﬁlter for ﬁxed point approximation and efﬁcient
temporal-difference learning. Discrete Event Dynamic Systems, 16(2):207–239, 2006.

9


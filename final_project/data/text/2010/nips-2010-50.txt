Learning Bounds for Importance Weighting

Corinna Cortes
Google Research
New York, NY 10011
corinna@google.com

Yishay Mansour
Tel-Aviv University
Tel-Aviv 69978, Israel
mansour@tau.ac.il

Mehryar Mohri
Courant Institute and Google
New York, NY 10012
mohri@cims.nyu.edu

Abstract
This paper presents an analysis of importance weighting for learning from ﬁnite
samples and gives a series of theoretical and algorithmic results. We point out
simple cases where importance weighting can fail, which suggests the need for an
analysis of the properties of this technique. We then give both upper and lower
bounds for generalization with bounded importance weights and, more signiﬁ-
cantly, give learning guarantees for the more common case of unbounded impor-
tance weights under the weak assumption that the second moment is bounded,
a condition related to the R ´enyi divergence of the training and test distributions.
These results are based on a series of novel and general bounds we derive for un-
bounded loss functions, which are of independent interest. We use these bounds to
guide the deﬁnition of an alternative reweighting algorithm and report the results
of experiments demonstrating its beneﬁts. Finally, we analyze the properties of
normalized importance weights which are also commonly used.

1 Introduction
In real-world applications of machine learning, often the sampling of the training and test instances
may differ, which results in a mismatch between the two distributions. For example, in web search
applications, there may be data regarding users who clicked on some advertisement link but little
or no information about other users. Similarly, in credit default analyses, there is typically some
information available about the credit defaults of customers who were granted credit, but no such
information is at hand about rejected costumers. In other problems such as adaptation, the training
data available is drawn from a source domain different from the target domain. These issues of
biased sampling or adaptation have been long recognized and studied in the statistics literature.
There is also a large body of literature dealing with different techniques for sample bias correction
[11, 29, 16, 8, 25, 6] or domain adaptation [3, 7, 19, 10, 17] in the recent machine learning and
natural language processing literature.
A common technique used in several of these publications for correcting the bias or discrepancy is
based on the so-called importance weighting technique. This consists of weighting the cost of errors
on training instances to emphasize the error on some or de-emphasize it on others, with the objective
of correcting the mismatch between the distributions of training and test points, as in sample bias
correction, adaptation, and other related contexts such as active learning [24, 14, 8, 19, 5]. Different
deﬁnitions have been adopted for these weights. A common deﬁnition of the weight for point x is
w(x) = P (x)/Q(x) where P is the target or test distribution and Q is the distribution according to
which training points are drawn. A favorable property of this deﬁnition, which is not hard to verify,
is that it leads to unbiased estimates of the generalization error [8].
This paper presents an analysis of importance weighting for learning from ﬁnite samples. Our study
was originally motivated by the observation that, while this corrective technique seems natural, in
some cases in practice it does not succeed. An example in dimension two is illustrated by Figure 1.
The target distribution P is the even mixture of two Gaussians centered at (0, 0) and (0, 2) both with

1

4

3

2

1

0

−1

−2

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
+
+
−
−
−

−
−
−
−
−
−

−
−
−
σ P
−
x
−
−
σ P
σQ
x
+
+
−
−

+
+
−
−

−
−
−
−
−
−
−
+
+
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
σQ
x
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

Ratio             =  0.75
σ σQ P

Ratio             =  0.3
σ σQ P

r
o
r
r
E

1.0
0.8
0.6
0.4
0.2
0.0

r
o
r
r
E

1.0
0.8
0.6
0.4
0.2
0.0

20

20

5000

5000

500
100
500
100
Training set size
Training set size
−2
−1
0
1
2
3
4
Figure 1: Example of importance weighting. Left ﬁgure: P (in blue) and Q (in red) are even mixtures
of Gaussians. The labels are positive within the unit sphere centered at the origin (in grey), negative
elsewhere. The hypothesis class is that of hyperplanes tangent to the unit sphere. Right ﬁgures: plots
of test error vs training sample size using importance weighting for two different values of the ratio
σQ /σP . The results indicate mean values of the error over 40 runs ± one standard deviation.
standard deviation σP , while the source distribution Q is the even mixture of two Gaussians centered
at (0, 0) and (2, 0) but with standard deviation σQ . The hypothesis class is that of hyperplanes
tangent to the unit sphere. The best classiﬁer is selected by empirical risk minimization. As shown
in Figure 1, for σP /σQ = .3, the error of the hypothesis learned using importance weighting is close
to 50% even for a training sample of 5,000 points and the standard deviation of the error is quite
high. In contrast, for σP /σQ = .75, convergence occurs relatively rapidly and learning is successful.
In Section 4, we discuss other examples where importance weighting does not succeed.
The problem just described is not limited to isolated examples. Similar observations have been made
in the past in both the statistics and learning literature, more recently in the context of the analysis
of boosting by [9] who suggest that importance weighting must be used with care and highlight the
need for convergence bounds and learning guarantees for this technique.
We study the theoretical properties of importance weighting. We show using standard generaliza-
tion bounds that importance weighting can succeed when the weights are bounded. However, this
condition often does not hold in practice. We also show that, remarkably, convergence guarantees
can be given even for unbounded weights under the weak assumption that the second moment of the
weights is bounded, a condition that relates to the R ´enyi divergence of P and Q. We further extend
these bounds to guarantees for other possible reweightings. These results suggest minimizing a bias-
variance tradeoff that we discuss and that leads to several algorithmic ideas. We explore in detail an
algorithm based on these ideas and report the results of experiments demonstrating its beneﬁts.
Throughout this paper, we consider the case where the weight function w is known. When it is
not, it is typically estimated from ﬁnite samples. The effect of this estimation error is speciﬁcally
analyzed by [8]. This setting is closely related to the problem of importance sampling in statistics
which is that of estimating the expectation of a random variable according to P while using a sample
drawn according to Q, with w given [18]. Here, we are concerned with the effect of the weights on
learning from ﬁnite samples. A different setting is when further full access to Q is assumed, von
Neumann’s rejection sampling technique [28] can then be used. We note however that it requires w
to be bounded by some constant M , which is often not guaranteed and is the simplest case of our
bounds. Even then, the method is wasteful as it requires on average M samples to obtain one point.
The remainder of this paper is structured as follows. Section 2 introduces the deﬁnition of the R ´enyi
divergences and gives some basic properties of the importance weights. In Section 3, we give gen-
eralization bounds for importance weighting in the bounded case. We also present a general lower
bound indicating the key role played by the R ´enyi divergence of P and Q in this context. Section 4
deals with the more frequent case of unbounded w. Standard generalization bounds do not apply
here since the loss function is unbounded. We give novel generalization bounds for unbounded loss
functions under the assumption that the second moment is bounded (see Appendix) and use them to
derive learning guarantees for importance weighting in this more general setting. In Section 5, we
discuss an algorithm inspired by these guarantees for which we report preliminary experimental re-
sults. We also discuss why the commonly used remedy of truncating or capping importance weights
may not always provide the desired effect of improved performance. Finally, in Section 6, we study

2

.

(2)

the properties of an alternative reweighting also commonly used which is based on normalized im-
portance weights, and discuss its relationship with the (unnormalized) weights w.
2 Preliminaries
Let X denote the input space, Y the label set, and let L : Y ×Y → [0, 1] be a loss function. We denote
by P the target distribution and by Q the source distribution according to which training points are
drawn. We also denote by H the hypothesis set used by the learning algorithm and by f : X → Y
the target labeling function.
2.1 R ´enyi divergences
Our analysis makes use of the notion of R ´enyi divergence, an information theoretical measure of
the difference between two distributions directly relevant to the study of importance weighting. For
α ≥ 0, the R ´enyi divergence Dα (P $Q) between distributions P and Q is deﬁned by [23]
Q(x) #α−1
P (x) " P (x)
log2 !x
1
(1)
Dα (P $Q) =
.
α − 1
The R ´enyi divergence is a non-negative quantity and for any α> 0, Dα (P $Q) = 0 iff P = Q. For
α = 1, it coincides with the relative entropy. We denote by dα (P $Q) the exponential in base 2 of
the R ´enyi divergence Dα (P $Q):
dα (P $Q) = 2Dα (P "Q) = $ !x
Qα−1 (x) % 1
P α (x)
α−1
2.2 Importance weights
The importance weight for distributions P and Q is deﬁned by w(x) = P (x)/Q(x). In the follow-
ing, the expectations are taken with respect to Q.
Lemma 1. The following identities hold for the expectation, second moment, and variance of w:
(3)
σ2 (w) = d2 (P $Q) − 1.
E[w2 ] = d2 (P $Q)
E[w] = 1
Proof. The ﬁrst equality is immediate. The second moment of w can be expressed as follows in
terms of the R ´enyi divergence:
w2 (x) Q(x) = !x∈X " P (x)
Q(x) #2
P (x)" P (x)
Q(x) # = d2 (P $Q).
[w2 ] = !x∈X
Q(x) = !x∈X
E
Q
Thus, the variance of w is given by σ2 (w) = EQ [w2 ] − EQ [w]2 = d2 (P $Q) − 1.
For any hypothesis h ∈ H , we denote by R(h) its loss and by &Rw (h) its weighted empirical loss:
m!i=1
1
&Rw (h) =
w(xi ) L(h(xi ), f (xi )).
R(h) = E
[L(h(x), f (x))]
m
x∼P
We shall use the abbreviated notation Lh(x) for L(h(x), f (x)), in the absence of any ambiguity
about the target function f . Note that the unnormalized importance weighting of the loss is unbiased:
[w(x)Lh (x)] = !x
Lh (x) Q(x) = !x
P (x)
E
Q(x)
Q
The following lemma gives a bound on the second moment.
Lemma 2. For all α> 0 and x ∈ X , the second moment of the importance weighted loss can be
bounded as follows:
(4)
h (x)] ≤ dα+1 (P $Q) R(h)1− 1
[w2 (x) L2
E
α .
x∼Q
For α = 1, this becomes R(h)2 ≤ Ex∼Q [w2 (x) L2
h (x)] ≤ d2 (P $Q).
3

P (x)Lh (x) = R(h).

P (x)

α+1
α−1
h

α−1
α L2
h (x)

(H ¨older’s inequality)

Proof. The second moment can be bounded as follows:
α $ P (x)
Q(x)$ P (x)
Q(x) % P (x)
Q(x) %2
h (x)] = !x
h(x) = !x
1
[w2 (x) L2
L2
E
x∼Q
(x)% α−1
α $ !x
Q(x) %α% 1
≤ $ !x
P (x)$ P (x)
α
2α
α−1
P (x) L
h
(x)% α−1
= dα+1 (P $Q)$ !x
α
P (x) Lh (x)L
α = dα+1 (P $Q) R(h)1− 1
≤ dα+1 (P $Q) R(h)1− 1
α B 1+ 1
α .
3 Learning Guarantees - Bounded Case
Note that supx w(x) = supx
Q(x) = d∞ (P $Q). We ﬁrst examine the case d∞ (P $Q) < +∞ and use
P (x)
the notation M = d∞ (P $Q). The following proposition follows then directly Hoeffding’s inequality.
Proposition 1 (single hypothesis). Fix h ∈ H . For any δ> 0, with probability at least 1 − δ ,
|R(h) − &Rw (h)|≤ M ’ log(2/δ )
.
2m
The upper bound M , though ﬁnite, can be quite large. The following theorem provides a more
favorable bound as a function of the ratio M /m when any of the moments of w, dα+1 (P $Q),
is ﬁnite, which is the case when d∞ (P $Q) < ∞ since the R ´enyi divergence is a non-decreasing
function of α [23, 2], in particular:
(5)
dα+1 (P $Q) ≤ d∞ (P $Q).
∀α> 0,
Theorem 1 (single hypothesis). Fix h ∈ H . Then, for any α ≥ 1, for any δ> 0, with probability at
least 1− δ , the following bound holds for the importance weighting method:
+ ( 2)dα+1 (P $Q) R(h)1− 1
α − R(h)2 * log 1
2M log 1
R(h) ≤ &Rw (h) +
δ
δ
.
3m
m
3m + + 2d2 (P "Q) log 1
For α = 1 after further simpliﬁcation, this gives R(h) ≤ &Rw (h) + 2M log 1
.
δ
δ
m
Proof. Let Z denote the random variable w(x) Lh (x) − R(h). Then, |Z |≤ M . By lemma 2, the
variance of the random variable Z can be bounded in terms of the R ´enyi divergence dα+1 (P $Q):
[w2 (x) Lh (x)2 ] − R(h)2 ≤ dα+1 (P $Q) R(h)1− 1
σ2 (Z ) = E
α − R(h)2 .
Q
Thus, by Bernstein’s inequality [4], it follows that:
σ2 (Z ) + M /3 #.
Pr[R(h) − &Rw (h) > ] ≤ exp " −m2/2
Setting δ to match this upper bound shows that with probability at least 1 − δ , the following bound
holds for the importance weighting method:
+ ( M 2 log2 1
M log 1
2σ2 (Z ) log 1
R(h) ≤ &Rw (h) +
δ
δ
δ
9m2
m
3m
Using the sub-additivity of √· leads to the simpler expression
+ ( 2σ2 (Z ) log 1
2M log 1
R(h) ≤ &Rw (h) +
δ
δ
3m
m
These results can be straightforwardly extended to general hypothesis sets. In particular, for a ﬁnite
hypothesis set and for α = 1, the application of the union bound yields the following result.

+

.

(6)

.

4

.

(7)

(8)

Theorem 2 (ﬁnite hypothesis set). Let H be a ﬁnite hypothesis set. Then, for any δ> 0, with
probability at least 1− δ , the following bound holds for the importance weighting method:
+ ( 2d2 (P $Q)(log |H | + log 1
2M (log |H | + log 1
δ )
δ )
R(h) ≤ &Rw (h) +
3m
m
For inﬁnite hypothesis sets, a similar result can be shown straightforwardly using covering numbers
instead of |H | or a related measure based on samples of size m [20].
In the following proposition, we give a lower bound that further emphasizes the role of the R ´enyi
divergence of the second order in the convergence of importance weighting in the bounded case.
Proposition 2 (Lower bound). Assume that M < ∞ and σ2 (w)/M 2 ≥ 1/m. Assume that H
contains a hypothesis h0 such that Lh0 (x) = 1 for all x. Then, there exists an absolute constant c,
c = 2/412, such that
h∈H ,,R(h) − &Rw (h),, ≥ ’ d2 (P $Q) − 1
Pr $ sup
% ≥ c > 0.
4m
Proof. Let σH = suph∈H σ(wLh ). If for all x ∈ X , Lh0 (x) = 1, then σ2 (wLh0 ) = d2 (P $Q) − 1 =
H . The result then follows a general theorem, Theorem 9 proven in the Appendix.
σ2 (w) = σ2
4 Learning Guarantees - Unbounded Case
The condition d∞ (P $Q) < ∞ assumed in the previous section does not always hold, even in some
natural cases, as illustrated by the following examples.
4.1 Examples
Assume that P and Q both follow a Gaussian distribution with the standard deviations σP and σQ
and with means µ and µ& :
exp $ −
exp $ −
Q %.
% Q(x) =
(x − µ& )2
(x − µ)2
1
1
√2πσQ
√2πσP
2σ2
2σ2
P
exp - − σ2
., thus, even for σP = σQ and µ += µ& the
In that case, P (x)
Q (x−µ)2−σ2
P (x−µ" )2
Q(x) = σQ
P σ2
2σ2
σP
Q
Q(x) = +∞, and the bound of Theorem 1
importance weights are unbounded, d∞ (P $Q) = supx
P (x)
is not informative. The R ´enyi divergence of the second order is given by:
%P (x)dx
exp $ −
σP / +∞
Q (x − µ)2 − σ2
σ2
P (x − µ& )2
σQ
2σ2
P σ2
−∞
Q
exp $ −
%dx.
P √2π / +∞
Q (x − µ)2 − σ2
2σ2
P (x − µ& )2
σQ
2σ2
P σ2
σ2
−∞
Q
√2
That is, for σQ >
2 σP the variance of the importance weights is bounded. By the additivity
property of the R ´enyi divergence, a similar situation holds for the product and sums of such Gaussian
distributions. Hence, in the rightmost example of Figure 1, the importance weights are unbounded,
but their second moment is bounded. In the next section we provide learning guarantees even for
this setting in agreement with the results observed. For σQ = 0.3σP , the same favorable guarantees
do not hold, and, as illustrated in Figure 1, learning is signiﬁcantly more difﬁcult.
This example of Gaussians can further illustrate what can go wrong in importance weighting. As-
sume that µ = µ& = 0, σQ = 1 and σP = 10. One could have expected this to be an easy case for
importance weighting since sampling from Q provides useful information about P . The problem
is, however, that a sample from Q will contain a very small number of points far from the mean
(of either negative or positive label) and that these points will be assigned very large weights. For
a sample of size m and σQ = 1, the expected value of an extreme point is √2 log m − o(1) and its
5

d2 (P $Q) =

P (x) =

=

.

weight will be in the order of m−1/σ2
Q = m0.99 . Therefore, a few extreme points will domi-
P +1/σ2
nate all other weights and necessarily have a huge inﬂuence on the selection of a hypothesis by the
learning algorithm.
Another related example is when σQ = σP = 1 and µ& = 0. Let µ , 0 depend on the sample size
m. If µ is large enough compared to log(m), then, with high probability, all the weights will be
negligible. This is especially problematic, since the estimate of the probability of any event would
be negligible (in fact both an event and its complement). If we normalize the weights, the issue
is overcome, but then, with high probability, the maximum weight dominates the sum of all other
weights, reverting the situation back to that of the previous example.
4.2 Importance weighting learning bounds - unbounded case
As in these examples, in practice, the importance weights are typically not bounded. However, we
shall show that, remarkably, under the weak assumption that the second moment of the weights
w, d2 (P $Q), is bounded, generalization bounds can be given for this case as well. The follow-
ing result relies on a general learning bound for unbounded loss functions proven in the Appendix
(Corollary 1). We denote by Pdim(U ) the pseudo-dimension of a real-valued function class U [21].
Theorem 3. Let H be a hypothesis set such that Pdim({Lh (x) : h ∈ H }) = p < ∞. Assume that
d2 (P $Q) < +∞ and w(x) += 0 for all x. Then, for any δ> 0, with probability at least 1 − δ , the
following holds:
8( p log 2me
p + log 4
R(h) ≤ &Rw (h) + 25/40d2 (P $Q)
3
δ
m
Proof. Since d2 (P $Q) < +∞, the second moment of w(x)Lh (x) is ﬁnite and upper bounded by
d2 (P $Q) (Lemma 2). Thus, by Corollary 1, we can write
45/3 #,
Pr $ sup
> % ≤ 4 exp "p log
R(h) − &Rw (h)
m8/3
2em
p −
0d2 (P $Q)
h∈H
where p is the pseudo-dimension of the function class H && = {w(x)Lh (x) : h ∈ H }. We now show
that p = Pdim({Lh (x) : h ∈ H }). Let H & denote {Lh(x) : h ∈ H }. Let A = {x1 , . . . , xk } be a
set shattered by H && . Then, there exist real numbers r1 , . . . , rk such that for any subset B ⊆ A there
exists h ∈ H such that
(9)
∀xi ∈ B , w(xi )Lh (xi ) ≥ ri
∀xi ∈ A − B , w(xi )Lh (xi ) < ri .
Since by assumption w(xi ) > 0 for all i ∈ [1, k ], this implies that
(10)
∀xi ∈ B , Lh(xi ) ≥ ri /w(xi )
∀xi ∈ A − B , Lh (xi ) < ri /w(xi ).
Thus, H & shatters A with the witnesses si = ri /w(xi ), i ∈ [1, k ]. Using the same observations, it is
straightforward to see that conversely, any set shattered by H & is shattered by H && .
The convergence rate of the bound is slightly weaker (O(m−3/8 )) than in the bounded case
(O(m−1/2 )). A faster convergence can be obtained however using the more precise bound of Theo-
rem 8 at the expense of readability. The R ´enyi divergence d2 (P $Q) seems to play a critical role in
the bound and thus in the convergence of importance weighting in the unbounded case.
5 Alternative reweighting algorithms
The previous analysis can be generalized to the case of an arbitrary positive function u : X → R,
m 1m
u > 0. Let &Ru (h) = 1
i=1 u(xi )Lh(xi ) and let &Q denote the empirical distribution.
Theorem 4. Let H be a hypothesis set such that Pdim({Lh (x) : h ∈ H }) = p < ∞. Assume that
0 < EQ [u2 (x)] < +∞ and u(x) += 0 for all x. Then, for any δ> 0, with probability at least 1 − δ ,
the following holds:
|R(h) − &Ru (h)|≤ ,,, E
Q )[w(x) − u(x)]Lh (x)*,,,+
8( p log 2me
25/4 max 20EQ [u2 (x)L2
h (x)]3 3
p + log 4
h (x)], √E bQ [u2 (x)L2
δ
m
6

.

Unweighted, Ratio             =  0.75
σ σP      Q
1.0

Impor tance, Ratio             =  0.75
σ σP       Q
1.0

r
o
r
r
E

0.8

0.6

0.4

0.2

0.0

r
o
r
r
E

0.8

0.6

0.4

0.2

0.0

r
o
r
r
E

1.0

0.8

0.6

0.4

0.2

0.0

Quantile, Ratio             =  0.75
σ σP       Q

Capped 1%, Ratio             =  0.75
σ σP      Q
1.0

r
o
r
r
E

0.8

0.6

0.4

0.2

0.0

20

50

200 500
Training set size

2000

20

50

200 500
Training set size

2000

20

50

200 500
Training set size

2000

20

50

200 500
Training set size

2000

and thus

Figure 2: Comparison of the convergence of 4 different algorithms for the learning task of Figure 1:
learning with equal weights for all examples (Unweighted), Importance weighting, using Quantiles
to parameterize the function u, and Capping the largest weights.
Proof. Since R(h) = E[w(x)Lh (x)], we can write
Q )[w(x) − u(x)]Lh (x)* + E[u(x)Lh (x)] − &Ru (h),
R(h) − &Ru (h) = E
Q )[w(x) − u(x)]Lh (x)*,, + | E[u(x)Lh (x)] − &Ru (h)|.
|R(h) − &Ru (h)|≤ ,, E
| E[u(x)Lh (x)] − &Ru (h)| can be bounded by
By Corollary 2 applied to the function u Lh ,
8+ p log 2me
h (x)], √E bQ [u2 (x)L2
25/4 max(0EQ [u2 (x)L2
with probability 1 − δ , with
p +log 4
3
δ
h (x)])
m
p = Pdim({Lh (x) : h ∈ H }) by a proof similar to that of Theorem 3.
The theorem suggests that other functions u than w can be used to reweight the cost of an error
on each training point by minimizing the upper bound, which is a trade-off between the bias term
h (x)], √E bQ [u2 (x)L2
| EQ [(w(x)−u(x))Lh (x)]| and the second moment max 40EQ [u2 (x)L2
h (x)]5,
where the coefﬁcients are explicitly given. Function u can be selected from different families. Using
an upper bound on these quantities that is independent of h and a multiplicative bound of the form
max 20E
[u2 ]3 ≤ 0E
[u2 ], 0E
[u2 ] 41 + O(1/√m)5 ,
Q
Q
bQ
leads to the following optimization problem:
Q )|w(x) − u(x)|* + γ0E
(11)
[u2 ],
E
min
u∈U
Q
where γ> 0 is a parameter controlling the trade-off between bias and variance minimization and
where U is a family of possible weight functions out of which u is selected.
Here, we consider a family of functions U parameterized by the quantiles q of the weight function
w. A function uq ∈ U is then deﬁned as follows: within each quantile, the value taken by uq is the
average of w over that quantile. For small values of γ , the bias term dominates, and very ﬁne-grained
quantiles minimize the bound of equation (11). For large values of γ the variance term dominates
and the bound is minimized by using just one quantile, corresponding to an even weighting of
the training examples. Hence by varying γ from small to large values, the algorithm interpolates
between standard importance weighting with just one example per quantile, and unweighted learning
where all examples are given the same weight. Figure 2 also shows the results of experiments for
the learning task of Figure 1 using the algorithm deﬁned by (11) with this family of functions. The
optimal q is determined by 10-fold cross-validation. We see that a more rapid convergence can be
obtained by using these weights compared to the standard importance weights w.
Another natural family of functions is that of thresholded versions of the importance weights
{uθ : θ> 0, ∀x ∈ X, uθ (x) = min(w(x),θ )}. In fact, in practice, users often cap importance weights
by choosing an arbitrary value θ. The advantage of this family is that, by deﬁnition, the weights are

7

bounded. However, in some cases, larger weights could be critical to achieve a better performance.
Figure 2 illustrates the performance of this approach. Compared to importance weighting, no change
in performance is observed until the largest 1% of the weights are capped, in which case we only
observe a performance degradation. We expect the thresholding to be less beneﬁcial when the large
weights reﬂect the true w and are not an artifact of estimation uncertainties.
6 Relationship between normalized and unnormalized weights
An alternative approach based on the weight function w = P (x)/Q(x) consists of normalizing the
weights. Thus, while in the unnormalized case the unweighted empirical error is replaced by
m!i=1
m!i=1
1
w(xi )
m
m
in the normalized case it is replaced by

w(xi ) Lh (xi ) =

Lh (xi ),

m!i=1
w(xi )
Lh (xi ),
W
with W = 1m
i=1 w(xi ). We refer to &w(x) = w(x)/W as the normalized importance weight. An
advantage of the normalized weights is that they are by deﬁnition bounded by one. However, the
price to pay for this beneﬁt is the fact that the weights are no more unbiased. In fact, several issues
similar to those we pointed out in the Section 4 affect the normalized weights as well.
Here, we maintain the assumption that the second moment of the importance weights is bounded
and analyze the relationship between normalized and unnormalized weights. We show that, under
this assumption, normalized and unnormalized weights are in fact very close, with high probability.
Observe that for any i ∈ [1, m],
m % =
W $1 −
m % .
= w(xi ) $ 1
w(xi )
w(xi )
W
1
&w(xi ) −
W −
m
W ≤ 1, we can write ,,, &w(xi ) − w(xi )
m ,,, ≤ ,,1 − W
m ,, . Since E[w(x)] = 1, we also have
Thus, since w(xi )
m 1m
k=1 E[w(xk )] = 1. Thus, by Corollary 2, for any δ> 0, with probability at least 1 − δ ,
ES [W ] = 1
the following inequality holds
8( log 2me + log 4
m ,,,, ≤ 25/4 max 6+d2 (P $Q), +d2 (P $ &Q)7 3
,,,,1 −
W
δ
,
m
m ,,,, simultaneously for all i ∈ [1, m].
which implies the same upper bound on ,,, &w(xi ) − w(xi )
7 Conclusion
We presented a series of theoretical results for importance weighting both in the bounded weights
case and in the more general unbounded case under the assumption that the second moment of the
weights is bounded. We also initiated a preliminary exploration of alternative weights and showed its
beneﬁts. A more systematic study of new algorithms based on these learning guarantees could lead
to even more beneﬁcial and practically useful results. Several of the learning guarantees we gave
depend on the R ´enyi divergence of the distributions P and Q. Accurately estimating that quantity
is thus critical and should motivate further studies of the convergence of its estimates from ﬁnite
samples. Finally, our novel unbounded loss learning bounds are of independent interest and could
be useful in a variety of other contexts.
References
[1] M. Anthony and J. Shawe-Taylor. A result of Vapnik with applications. Discrete Applied
Mathematics, 47:207 – 217, 1993.

8

[2] C. Arndt. Information Measures: Information and its Description in Science and Engineering.
Signals and Communication Technology. Springer Verlag, 2004.
[3] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain
adaptation. NIPS, 2007.
[4] S. N. Bernstein. Sur l’extension du th ´eor `eme limite du calcul des probabilit ´es aux sommes de
quantit ´es d ´ependantes. Mathematische Annalen, 97:1–59, 1927.
[5] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In ICML,
pages 49–56, New York, NY, USA, 2009.
[6] S. Bickel, M. Br ¨uckner, and T. Scheffer. Discriminative learning for differing training and test
distributions. In ICML, pages 81–88, 2007.
[7] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman. Learning bounds for domain
adaptation. NIPS 2007, 2008.
[8] C. Cortes, M. Mohri, M. Riley, and A. Rostamizadeh. Sample selection bias correction theory.
In ALT, 2008.
[9] S. Dasgupta and P. M. Long. Boosting with diverse base classiﬁers. In COLT, 2003.
[10] H. Daum ´e III and D. Marcu. Domain adaptation for statistical classiﬁers. Journal of Artiﬁcial
Intelligence Research, 26:101–126, 2006.
[11] M. Dud´ık, R. E. Schapire, and S. J. Phillips. Correcting sample selection bias in maximum
entropy density estimation. In NIPS, 2006.
[12] R. M. Dudley. A course on empirical processes. Lecture Notes in Math., 1097:2 – 142, 1984.
[13] R. M. Dudley. Universal Donsker classes and metric entropy. Annals of Probability, 14(4):1306
– 1326, 1987.
[14] C. Elkan. The foundations of cost-sensitive learning. In IJCAI, pages 973–978, 2001.
[15] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other
learning applications. Inf. Comput., 100(1):78–150, 1992.
[16] J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and B. Sch ¨olkopf. Correcting sample
selection bias by unlabeled data. In NIPS, volume 19, pages 601–608, 2006.
[17] J. Jiang and C. Zhai. Instance Weighting for Domain Adaptation in NLP. In ACL, 2007.
[18] J. S. Liu. Monte Carlo strategies in scientiﬁc computing. Springer, 2001.
[19] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algo-
rithms. In COLT, 2009.
[20] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In
COLT, Montr ´eal, Canada, June 2009. Omnipress.
[21] D. Pollard. Convergence of Stochastic Processess. Springer, New York, 1984.
[22] D. Pollard. Asymptotics via empirical processes. Statistical Science, 4(4):341 – 366, 1989.
[23] A. R ´enyi. On measures of information and entropy. In Proceedings of the 4th Berkeley Sym-
posium on Mathematics, Statistics and Probability, page 547561, 1960.
[24] H. Shimodaira.
Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000.
[25] M. Sugiyama, S. Nakajima, H. Kashima, P. von B ¨unau, and M. Kawanabe. Direct importance
estimation with model selection and its application to covariate shift adaptation. In NIPS, 2008.
[26] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.
[27] V. N. Vapnik. Estimation of Dependences Based on Empirical Data, 2nd ed. Springer, 2006.
[28] J. von Neumann. Various techniques used in connection with random digits. Monte Carlo
methods. Nat. Bureau Standards, 12:36–38, 1951.
[29] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive learning by cost-proportionate example
weighting. In ICDM, 2003.

9


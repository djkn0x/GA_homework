Regularized estimation of image statistics
by Score Matching

Diederik P. Kingma
Department of Information and Computing Sciences
Universiteit Utrecht
d.p.kingma@students.uu.nl

Yann LeCun
Courant Institute of Mathematical Sciences
New York University
yann@cs.nyu.edu

Abstract

Score Matching is a recently-proposed criterion for training high-dimensional
density models for which maximum likelihood training is intractable. It has been
applied to learning natural image statistics but has so-far been limited to simple
models due to the difﬁculty of differentiating the loss with respect to the model
parameters. We show how this differentiation can be automated with an extended
version of the double-backpropagation algorithm. In addition, we introduce a reg-
ularization term for the Score Matching loss that enables its use for a broader
range of problem by suppressing instabilities that occur with ﬁnite training sam-
ple sizes and quantized input values. Results are reported for image denoising and
super-resolution.

1 Introduction

Consider the subject of density estimation for high-dimensional continuous random variables, like
images. Approaches for normalized density estimation, like mixture models, often suffer from the
curse of dimensionality. An alternative approach is Product-of-Experts (PoE) [7], where we model
the density as a product, rather than a sum, of component (expert) densities. The multiplicative
nature of PoE models make them able to form complex densities: in contrast to mixture models, each
expert has the ability to have a strongly negative inﬂuence on the density at any point by assigning
it a very low component density. However, Maximum Likelihood Estimation (MLE) of the model
requires differentiation of a normalizing term, which is infeasible even for low data dimensionality.

A recently introduced estimation method is Score Matching [10], which involves minimizing the
square distance between the model log-density slope (score) and data log-density slope, which is
independent of the normalizing term. Unfortunately, applications of SM estimation have thus far
been limited. Besides ICA models, SM has been applied to Markov Random Fields
[14] and
a multi-layer model [13], but reported results on real-world data have been of qualitative, rather
than quantitative nature. Differentiating the SM loss with respect to the parameters can be very
challenging, which somewhat complicates the use of SM in many situations. Furthermore, the proof
of the SM estimator [10] requires certain conditions that are often violated, like a smooth underlying
density or an inﬁnite number of samples.

Other estimation methods are Constrastive Divergence [8] (CD), Basis Rotation [23] and Noise-
Contrastive Estimation [6] (NCE). CD is an MCMC method that has been succesfully applied
to Restricted Boltzmann Machines (RBM’s) [8], overcomplete Independent Component Analysis

1

(ICA) [9], and convolution variants of ICA and RBM’s [21, 19]. Basis Rotation [23] works by re-
stricting weight updates such that they are probability mass-neutral. SM and NCE are consistent
estimators [10, 6], while CD estimation has been shown to be generally asymptotically biased [4].
No consistency results are known for Basis Rotation, to our knowledge. NCE is a promising method,
but unfortunately too new to be included in experiments. CD and Basis Rotation estimation will be
used as a basis for comparison.

In section 2 a regularizer is proposed that makes Score Matching applicable to a much broader
class of problems. In section 3 we show how computation and differentiation of the SM loss can
be performed in automated fashion. In section 4 we report encouraging quantitative experimental
results.

2 Regularized Score Matching

Consider an energy-based [17] model E (x; w), where “energy” is the unnormalized negative log-
density such that the pdf is: p(x; w) = e−E (x;w)/Z (w), where Z (w) is the normalizing constant.
In other words, low energies correspond to high probability density, and high energies correspond
to low probability density.

Score Matching works by ﬁtting the slope (score) of the model density to the slope of the true,
underlying density at the data points, which is obviously independent of the vertical offset of the log-
density (the normalizing constant). Hyv ¨arinen [10] shows that under some conditions, this objective
is equivalent to minimizing the following expression, which involves only ﬁrst and second partial
derivatives of the model density:

px (x)

N
Xi=1   1
(∂ xi )2 ! dx + const
∂ xi (cid:19)2
∂ 2E (x; w)
2 (cid:18) ∂E (x; w)
J (w) = Zx∈RN
with N -dimensional data vector x, weight vector w and true, underlying pdf px (x). Among the
conditions 1 is (1) that px (x) is differentiable, and (2) that the log-density is ﬁnite everywhere. In
practice, the true pdf is unknown, and we have a ﬁnite sample of T discrete data points. The sample
version of the SM loss function is:

(1)

−

J S (w) =

(cid:19)2
N
T
Xi=1   1
(∂ xi )2 !
∂ 2E (x(t) ; w)
2 (cid:18) ∂E (x(t) ; w)
Xt=1
∂ xi
which is asymptotically equivalent to the equation (1) as T approaches inﬁnity, due to the law of
large numbers. This loss function was used in previous publications on SM [10, 12, 13, 15].

1
T

(2)

−

2.1 Issues

Should these conditions be violated, then (theoretically) the pdf cannot be estimated using equation
(1). Only some speciﬁc special-case solutions exist, e.g. for non-negative data [11]. Unfortunately,
situations where the mentioned conditions are violated are not rare. The distribution for quantized
data (like images) is discontinuous, hence not differentiable, since the data points are concentrated
at a ﬁnite number of discrete positions. Moreover, the fact that equation (2) is only equivalent to
equation (1) as T approaches inﬁnity may cause problems: the distribution of any ﬁnite training
set of discrete data points is discrete, hence not differentiable. For proper estimation with SM, data
can be smoothened by whitening; however, common whitening methods (such as PCA or SVD) are
computational infeasible for large data dimensionality, and generally destroy the local structure of
spatial and temporal data such as image and audio. Some previous publications on Score Matching
apply zero-phase whitening (ZCA) [13] which computes a weighed sum over an input patch which
removes some of the original quantization, and can potentially be applied convolutionally. However,

1 The conditions are:
the true (underlying) pdf px (x)
the expectations
is differentiable,
E (cid:2)k∂ log px (x)/∂xk2 (cid:3) and E (cid:2)k∂E (x; w)/∂xk2 (cid:3) w.r.t. x are ﬁnite for any w, and px (x)∂E (x; w)/∂x
goes to zero for any w when kxk → ∞.

2

the amount of information removed from the input by such whitening is not parameterized and
potentially large.

2.2 Proposed solution

1
2

+

1
2

1
2

1
2

N
Xi=1

E "(cid:18) ∂E (x + ǫ; w)
∂ (xi + ǫi ) (cid:19)2 # =
=

Our proposed solution is the addition of a regularization term to the loss, approximately equivalent
to replacing each data point x with a Gaussian cloud of virtual datapoints (x+ ǫ) with i.i.d. Gaussian
noise ǫ ∼ N (0, σ2 I ). By this replacement, the sample pdf becomes smooth and the conditions for
proper SM estimation become satisﬁed. The expected value of the sample loss is:
Xi=1  E "(cid:18) ∂E (x + ǫ; w)
∂ (xi + ǫi ) (cid:19)2#! −
N
N
Xi=1 (cid:18)E (cid:20) ∂ 2E (x + ǫ; w)
(∂ (xi + ǫi ))2 (cid:21)(cid:19) (3)
E (cid:2)J S (x + ǫ; w)(cid:3) =
We approximate the ﬁrst and second term with a simple ﬁrst-order Taylor expansion. Recall that
i (cid:3) = σ2 .
since the noise is i.i.d. Gaussian, E [ǫi ] = 0, E [ǫi ǫj ] = E [ǫi ] E [ǫj ] = 0 if i 6= j , and E (cid:2)ǫ2
The expected value of the ﬁrst term is:
E 
  ∂E (x; w)
N
Xi=1
∂ xi
Xi=1  (cid:18) ∂E (x; w)
N
∂ xi (cid:19)2
The expected value of the second term is:
Xi=1  E " ∂ 2E (x; w)
N
N
N
Xi=1 (cid:18)E (cid:20) ∂ 2E (x + ǫ; w)
Xi=1 (cid:18) ∂ 3E (x; w)
(∂ (xi + ǫi ))2 (cid:21)(cid:19) =
(∂ xi )2 +
∂ xi∂ xi∂ xj
N
Xi=1 (cid:18) ∂ 2E (x; w)
(∂ xi )2 (cid:19) + O(ǫ2
=
i )
Putting the terms back together, we have:
∂ xi ∂ xj (cid:19)2
N
N
N
N
∂ xi (cid:19)2
Xi=1 (cid:18) ∂ 2E
Xj=1 (cid:18) ∂ 2E
Xi=1 (cid:18) ∂E
(∂ xi )2 (cid:19) +
Xi=1
E (cid:2)J S (x + ǫ; w)(cid:3) =
where E = E (x; w). This is the full regularized Score Matching loss. While minimization of above
loss may be feasible in some situations, in general it requires differentiation of the full Hessian w.r.t.
x which scales like O(W 2 ). However, the off-diagonal elements of the Hessian are often dominated
by the diagonal. Therefore, we will use the diagonal approximation:

i )!2
N
Xj=1 (cid:18) ∂ 2E (x; w)
ǫj (cid:19) + O(ǫ2

∂ xi∂ xj
i )!
N
∂ xi∂ xj (cid:19)2
Xj=1 (cid:18) ∂ 2E (x; w)
+ ˆO(ǫ2
+ σ 2

i )#!
ǫj (cid:19) + O(ǫ2

+ ˆO(ǫ2 ) (6)

1
2

σ2

(4)

(5)

1
2

−

Jreg (x; w; λ) = J S (x; w) + λ

(∂ xi )2 (cid:19)2
N
Xi=1 (cid:18) ∂ 2E
where λ sets regularization strength and is related to (but not exactly equal to) 1
2 σ2 in equation (6).
This regularized loss is computationally convenient: the added complexity is almost negligible since
differentiation of the second derivative terms (∂ 2E /(∂ xi )2 ) w.r.t. the weights is already required
for unregularized Score Matching. The regularizer is related to Tikhonov regularization [22] and
curvature-driven smoothing [2] where the square of the curvature of the energy surface at the data
points are also penalized. However, its application has been limited since (contrary to our case) in
the general case it adds considerable computational cost.

(7)

3

Figure 1: Illustration of local computational ﬂow around some node j . Black lines: computation of
j = ∂ 2E /(∂ gi )2 and the SM loss J (x; w). Red lines indicate computa-
quantities δj = ∂E /∂ gj , δ ′
tional ﬂow for differentiation of the Score Matching loss: computation of e.g. ∂ J/∂ δj and ∂ J/∂ gj .
The inﬂuence of weights are not shown, for which the derivatives are computed in the last step.

3 Automatic differentiation of J (x; w)

In most optimization methods for energy-based models [17], the sample loss is deﬁned in readily
obtainable quantities obtained by forward inference in the model. In such situations, the required
derivatives w.r.t. the weights can be obtained in a straightforward and efﬁcient fashion by standard
application of the backpropagation algorithm.

For Score Matching, the situation is more complex since the (regularized) loss (equations 2,7) is
deﬁned in terms of {∂E /∂ xi} and {∂ 2E /(∂ xi )2}, each term being some function of x and w.
In earlier publications on Score Matching for continuous variables [10, 12, 13, 15], the authors
rewrote {∂E /∂ xi} and {∂ 2E /(∂ xi )2 } to their explicit forms in terms of x and w by manually
differentiating the energy2. Subsequently, derivatives of the loss w.r.t. the weights can be found.
This manual differentiation was repeated for different models, and is arguably a rather inﬂexible
approach. A procedure that could automatically (1) compute and (2) differentiate the loss would
make SM estimation more accessible and ﬂexible in practice.

A large class of models (e.g. ICA, Product-of-Experts and Fields-of-Experts), can be interpreted as
a form of feed-forward neural network. Consequently, the terms {∂E /∂ xi} and {∂ 2E /(∂ xi )2 } can
be efﬁciently computed using a forward and backward pass: the ﬁrst pass performs forward inference
(computation of E (x; w)) and the second pass applies the backpropagation algorithm [3] to obtain
the derivatives of the energy w.r.t. the data point ({∂E /∂ xi} and {∂ 2E /(∂ xi )2 }). However, only
the loss J (x; w) is obtained by these two steps. For differentiation of this loss, one must perform an
additional forward and backward pass.

3.1 Obtaining the loss

Consider a feed-forward neural network with input vector x and weights w and an ordered set of
nodes indexed 1 . . . N , each node j with child nodes i ∈ children(j ) with j < i and parent nodes
k ∈ parents(j ) with k < j . The ﬁrst D < N nodes are input nodes, for which the activation value
is gj = xj . For the other nodes (hidden units and output unit), the activation value is determined by
a differentiable scalar function gj ({gi}i∈parents(j) , w). The network’s “output” (energy) is deter-
mined as the activation of the last node: E (x; w) = gN (.). The values δj = ∂E /∂ gj are efﬁciently
computed by backpropagation. However, backpropagation of the full Hessian scales like O(W 2 ),
where W is the number of model weights. Here, we limit backpropagation to the diagonal approx-
imation which scales like O(W ) [1]. This will still result in the correct gradients ∂ 2E /(∂ xj )2 for
one-layer models and the models considered in this paper. Rewriting the equations for the full Hes-
j = ∂ 2E /(∂ gj )2 . The SM loss is split in
sian is a straightforward exercise. For brevity, we write δ ′
j and L = PD
2 PD
two terms: J (x; w) = K + L with K = 1
j )2 . The equations
j=1 δ2
j + λ(δ ′
j=1 −δ ′
for inference and backpropagation are given as the ﬁrst two f or-loops in Algorithm 1.

2Most previous publications do not express unnormalized neg. log-density as “energy”

4

Input: x, w (data and weight vectors)

for j ← D + 1 to N do
compute gj (.)
for i ∈ parents(j ) do
, ∂ 2 gj
(∂ gi )2 , ∂ 3 gj
compute ∂ gj
(∂ gi )3
∂ gi

// Forward propagation

// Backpropagation

∂ gk
∂ gj
∂ gj (cid:17)2
k (cid:16) ∂ gk
∂ 2 gk
(∂ gj )2 + δ ′
← −1 + 2λδ ′
j

δN ← 1, δ ′
N ← 0
for j ← N − 1 to 1 do
δj ← Pk∈children(j) δk
j ← Pk∈children(j) δk
δ ′
for j ← 1 to D do
∂K
← 0; ∂L
← δj ; ∂L
∂ δ ′
∂ δj
∂ δj
j
for j ← D + 1 to N do
∂K
← Pi∈parents(j)
∂ δj
∂L
← Pi∈parents(j)
∂ δj
∂L
← Pi∈parents(j)
∂ δ ′
j
for j ← N to D + 1 do
∂K
← Pk∈children(j)
∂ gj
∂L
← Pk∈children(j)
∂ gj
for w ∈ w do
∂ gj
∂w ← PN
∂w + ∂L
∂K
∂J
j=D+1
∂ gj
∂ gj
Algorithm 1: Compute ∇w J . See sections 3.1 and 3.2 for context.

∂ gj
∂K
∂ gi
∂ δi
∂ 2 gj
∂L
(∂ gi )2 + ∂L
∂ δ ′
∂ δi
i
∂ gi (cid:17)2
i (cid:16) ∂ gj
∂L
∂ δ ′
∂ gk
∂K
+ ∂K
∂ gj
∂ gk
∂ δj
∂ gk
∂L
+ ∂L
∂ gk
∂ gj
∂ δj

∂ gj
∂w + ∂K
∂ δj

∂ gk
∂ gj

∂ gj
∂ gi

δk

δk

∂ 2 gk
(∂ gj )2
∂ 2 gk
∂ 2 gk
(∂ gj )2 + ∂L
(∂ gj )2 + 2 ∂L
∂ δ ′
∂ δ ′
j
j
// Derivatives wrt weights
∂ δ ′
∂ δj
∂w + ∂L
j
∂ δ ′
∂w
j

∂ δj
∂w + ∂L
∂ δj

∂ 3 gk
(∂ gj )3

δ ′
k

δk

// SM Forward propagation

// SM Backward propagation

3.2 Differentiating the loss

Since the computation of the loss J (x; w) is performed by a deterministic forward-backward mech-
anism, this two-step computation can be interpreted as a combination of two networks: the original
network for computing {gj } and E (x; w), and an appended network for computing {δj }, {δ ′
j } and
eventually J (x; w). See ﬁgure 1. The combined network can be differentiated by an extended
version of the double-backpropagation procedure [5], with the main difference that the appended
network not only computes {δj }, but also {δ ′
j }. Automatic differentiation of the combined network
consists of two phases, corresponding to reverse traversal of the appended and original network
respectively: (1) obtaining ∂K/∂ δj , ∂L/∂ δj and ∂L/∂ δ ′
j for each node j in order 1 to N ; (2) ob-
taining ∂ J/∂ gj for each node j in order N to D + 1. These procedures are given as the last two
f or-loops in Algorithm 1. The complete algorithm scales like O(W ).

4 Experiments

E (x; W, α) =

Consider the following Product-of-Experts (PoE) model:
M
Xi=1
where M is the number of experts, wi is an image ﬁlter and the i-th row of W and αi are scaling
parameters. Like in [10], the ﬁlters are L2 normalized to prevent a large portion from vanishing.We
use a slightly modiﬁed Student’s t-distribution (g (z ) = log ((cz )2/2 + 1)) for latent space, so this is
also a Product of Student’s t-distribution model [24]. The parameter c is a non-learnable horizontal
scaling parameter, set to e1.5 . The vertical scaling parameters αi are restricted to positive, by setting
αi = exp βi where βi is the actual weight.

αi g (wT
i x)

(8)

5

4.1 MNIST

The ﬁrst task is to estimate a density model of the MNIST handwritten digits [16]. Since a large
number of models need to be learned, a 2× downsampled version of MNIST was used. The MNIST
dataset is highly non-smooth: for each pixel, the extreme values (0 and 1) are highly frequent lead-
ing to sharp discontinuities in the data density at these points. It is well known that for models
with square weight matrix W , normalized g (.) (meaning R ∞
−∞ exp(−g (x))dx = 1) and αi = 1, the
normalizing constant can be computed [10]: Z (w) = | det W |. For this special case, models can be
compared by computing the log-likelihood for the training- and test set. Unregularized, and regu-
larized models for different choices of λ were estimated and log-likelihood values were computed.
Subsequently, these models were compared on a classiﬁcation task. For each MNIST digit class, a
small sample of 100 data points was converted to internal features by different models. These fea-
tures, combined with the original class label, were subsequently used to train a logistic regression
classiﬁer for each model. For the PoE model, the “activations” g (wT
i x) were used as features. Clas-
siﬁcation error on the test set was compared against reported results for optimal RBM and SESM
models [20].

Results. As expected, unregularized estimation did not result in an accurate model. Figure 2 shows
how the log-likelihood of the train- and test set is optimal at λ∗ ≈ 0.01, and decreases for smaller
λ. Coincidentally, the classiﬁcation performance is optimal for the same choice of λ.

4.2 Denoising

Consider grayscale natural image data from the Berkeley dataset [18]. The data quantized and
therefore non-smooth, so regularization is potentially beneﬁcial. In order to estimate the correct
regularization magnitude, we again esimated a PoE model as in equation (8) with square W , such
that Z (w) = | det W | and computed the log-likelihood of 10.000 random patches under different
regularization levels. We found that λ∗ ≈ 10−5 for maximum likelihood (see ﬁgure 2d). This
value is lower than for MNIST data since natural image data is “less unsmooth”. Subsequently, a
convolutional PoE model known as Fields-of-Experts [21] (FoE) was estimated using regularized
SM:

M
Xi=1
E (x; W, α) = Xp
where p runs over image positions, and x(p) is a square image patch at p. The ﬁrst model has the
same architecture as the CD-1 trained model in [21]: 5 × 5 receptive ﬁelds, 24 experts (M = 24),
and αi and g (.) as in our PoE model. Note that qualitative results of a similar model estimated
with SM have been reported earlier [15]. We found that for best performance, the model is learned
on images “whitened” with a 5 × 5 Laplacian kernel. This is approximately equivalent to ZCA
whitening used in [15].

αig (wT
i x(p) )

(9)

Models are evaluated by means of Bayesian denoising using maximum a posteriori (MAP) estima-
tion. As in a general Bayesian image restoration framework, the goal is to estimate the original
input x given a noisy image y using the Bayesian proportionality p(x|y) ∝ p(y|x)p(x). The as-
sumption is white Gaussian noise such that the likelihood is p(y|x) ∼ N (0, σ2 I ). The model
E (x; w) = − log p(x; w) − Z (w) is our prior. The gradient of the log-posterior is:

∇x log p(x|y) = −∇xE (x; w) +

N
Xi=1
Denoising is performed by initializing x to a noise image, and 300 subsequent steps of steepest
descent according to x′ ← x + α∇x log p(x|y), with α annealed from 2 · 10−2 to 5 · 10−4. For com-
parison, we ran the same denoising procedure with models estimated by CD-1 and Basis Rotation,
from [21] and [23] respectively. Note that the CD-1 model is trained using PCA whitening. The
CD-1 model has been extensively applied to denoising before [21] and shown to compare favourably
to specialized denoising methods.

1
2σ2 ∇x

(yi − xi )2

(10)

Results. Training of the convolutional model took about 1 hour on a 2Ghz machine. Regularization
turns out to be important for optimal denoising (see ﬁgure 2[e-g]). See table 1 for denoising perfor-
mance of the optimal model for speciﬁc standard images. Our model performed signiﬁcantly better

6

test
training

)
.
g
v
a
(
 
d
o
o
h
i
l
e
k
i
l
-
g
o
l

 250

 200

 150

 100

 50

 0

Reg. SM
RBM
SESM

)
%
(
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
l
c

 12.5
 12
 11.5
 11
 10.5
 10
 9.5
 9
 8.5

(a)

 0.01

 0.02

 0.03

 0.01

 0.02

 0.03

λ

(b)

λ

(c)

Log-l. of image patches

σ

noise=1/256

σ

noise=5/256

σ

noise=15/256

-2800

-2900

-3000

-3100

-3200
-6.5 -6 -5.5 -5 -4.5
log10 λ
(d)

R
N
S
P
 
d
e
s
i
o
n
e
d

 48.8

 48.6

 48.4

 48.2

 48

 36.5

 36
 35.5

 35
 34.5

 34

 30

 29

 28

 27

 26

-7 -6 -5 -4 -3 -2
log10 λ

(e)

-7 -6 -5 -4 -3 -2
log10 λ
(f)

-7 -6 -5 -4 -3 -2
log10 λ
(g)

)
.
g
v
a
(
 
d
o
o
h
i
l
e
k
i
l
-
g
o
l

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

(q)

Figure 2: (a) Top: selection of downsampled MNIST datapoints. Middle and bottom: random
sample of ﬁlters from unregularized and regularized (λ = 0.01) models, respectively. (b) Average
log-likelihood of MNIST digits in training- and test sets for choices of λ. Note that λ∗ ≈ 0.01, both
for maximum likelihood and optimal classiﬁcation. (c) Test set error of a logistic regression classiﬁer
learned on top of features, with only 100 samples per class, for different choices of λ. Optimal error
rates of SESM and RBM (ﬁgure 1a in [20]) are shown for comparison. (d) Log-likelihood of 10.000
random natural image patches for complete model, for different choices of λ. (e-g) PSNR of 500
denoised images, for different levels of noise and choices of λ. Note that λ∗ ≈ 10−5 , both for
maximum likelihood and best denoising performance. (h) Some natural images from the Berkeley
dataset. (i) Filters of model with 5 × 5 × 24 weights learned with CD-1 [21], (j) ﬁlters of our model
with 5 × 5 × 24 weights, (k) random selection of ﬁlters from the Basis Rotation [23] model with
15 × 15 × 25 weights, (l) random selection of ﬁlters from our model with 8 × 8 × 64 weights. (m)
Detail of original Lena image. (n) Detail with noise added (σnoise = 5/256). (o) Denoised with
model learned with CD-1 [21], (p) Basis Rotation [23], (q) and Score Matching with (near) optimal
regularization.

7

than the Basis Rotation model and slightly better than the CD-1 model. As reported earlier in [15],
we can verify that the ﬁlters are completely intuitive (Gabor ﬁlters with different phase, orientation
and scale) unlike the ﬁlters of CD-1 and Basis Rotation models (see ﬁgure 2[i-l]).

Table 1: Peak signal-to-noise ratio (PSNR) of denoised images with σnoise = 5/256. Shown errors
are aggregated over different noisy images.

Image
Weights
Barbara
Peppers
House
Lena
Boat

CD-1
(5 × 5) × 24
37.30±0.01
37.63±0.01
37.85±0.02
38.16±0.02
36.33±0.01

Basis Rotation
(15 × 15) × 25
37.08±0.02
37.09±0.02
37.73±0.03
37.97±0.01
36.21±0.01

Our model
(5 × 5) × 24
37.31±0.01
37.41±0.03
38.03±0.04
38.19±0.01
36.53±0.01

4.3 Super-resolution

In addition, models are compared with respect to their performance on a simple version of super-
resolution as follows. An original image xorig is sampled down to image xsmall by averaging blocks
of 2 × 2 pixels into a single pixel. A ﬁrst approximation x is computed by linearly scaling up xsmall
and subsequent application of a low-pass ﬁlter to remove false high frequency information. The
image is than ﬁne-tuned by 200 repetitions of two subsequent steps: (1) reﬁning the image slightly
using x′ ← x + α∇xE (x; w) with α annealed from 2 · 10−2 to 5 · 10−4 ; (2) updating each k × k
block of pixels such that their average corresponds to the down-sampled value. Note: the simple
block-downsampling results in serious aliasing artifacts in the Barbara image, so the Castle image
is used instead.

Results. PSNR values for standard images are shown in table 2. The considered models made give
slight improvements in terms of PSNR over the initial solution with low pass ﬁlter. Still, our model
did slightly better than the CD-1 and Basis Rotation models.

Table 2: Peak signal-to-noise ratio (PSNR) of super-resolved images for different models.

Image
Weights
Peppers
House
Lena
Boat
Castle

Low pass ﬁlter
-
27.54
33.15
32.39
29.20
24.19

CD-1
(5 × 5) × 24
29.11
33.53
33.31
30.81
24.15

Basis Rotation
(15 × 15) × 25
27.69
33.41
33.07
30.77
24.26

Our model
(5 × 5) × 24
29.76
33.48
33.46
30.82
24.31

5 Conclusion

We have shown how the addition of a principled regularization term to the expression of the Score
Matching loss lifts continuity assumptions on the data density, such that the estimation method
becomes more generally applicable. The effectiveness of the regularizer was veriﬁed with the dis-
continuous MNIST and Berkeley datasets, with respect to likelihood of test data in the model. For
both datasets, the optimal regularization parameter is approximately equal for both likelihood and
subsequent classiﬁcation and denoising tasks. In addition, we showed how computation and differ-
entiation of the Score Matching loss can be automated using an efﬁcient algorithm.

8

References

[1] S. Becker and Y. LeCun.
Improving the convergence of back-propagation learning with second-order
methods. In D. Touretzky, G. Hinton, and T. Sejnowski, editors, Proc. of the 1988 Connectionist Models
Summer School, pages 29–37, San Mateo, 1989. Morgan Kaufman.

[2] C. M. Bishop. Neural networks for pattern recognition. Oxford University Press, Oxford, UK, 1996.

[3] A. E. Bryson and Y. C. Ho. Applied optimal control; optimization, estimation, and control. Blaisdell Pub.
Co. Waltham, Massachusetts, 1969.

[4] M. A. Carreira-Perpinan and G. E. Hinton. On contrastive divergence learning. In Artiﬁcial Intelligence
and Statistics, 2005.

[5] H. Drucker and Y. LeCun. Improving generalization performance using double backpropagation. IEEE
Transactions on Neural Networks, 3(6):991–997, 1992.

[6] M. Gutmann and A. Hyv ¨arinen. Noise-contrastive estimation: A new estimation principle for unnor-
malized statistical models.
In Proc. Int. Conf. on Artiﬁcial Intelligence and Statistics (AISTATS2010),
2010.

[7] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,
14:2002, 2000.

[8] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-
tation, 18(7):1527–1554, 2006.

[9] G. E. Hinton, S. Osindero, M. Welling, and Y. W. Teh. Unsupervised discovery of non-linear structure
using contrastive backpropagation. Cognitive Science, 30(4):725–731, 2006.

[10] A. Hyv ¨arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine
Learning Research, 6:695–709, 2005.

[11] A. Hyv ¨arinen.
Some extensions of score matching. Computational Statistics & Data Analysis,
51(5):2499–2512, 2007.

[12] A. Hyv ¨arinen. Optimal approximation of signal priors. Neural Computation, 20:3087–3110, 2008.

[13] U. K ¨oster and A. Hyv ¨arinen. A two-layer ica-like model estimated by score matching. In J. M. de S ´a,
L. A. Alexandre, W. Duch, and D. P. Mandic, editors, ICANN (2), volume 4669 of Lecture Notes in
Computer Science, pages 798–807. Springer, 2007.

[14] U. Koster, J. T. Lindgren, and A. Hyv ¨arinen. Estimating markov random ﬁeld potentials for natural
images. Proc. Int. Conf. on Independent Component Analysis and Blind Source Separation (ICA2009),
2009.

[15] U. K ¨oster, J. T. Lindgren, and A. Hyv ¨arinen. Estimating markov random ﬁeld potentials for natural
images. In T. Adali, C. Jutten, J. M. T. Romano, and A. K. Barros, editors, ICA, volume 5441 of Lecture
Notes in Computer Science, pages 515–522. Springer, 2009.

[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
In Proceedings of the IEEE, pages 2278–2324, 1998.

[17] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning. In
G. Bakir, T. Hofman, B. Sch ¨olkopf, A. Smola, and B. Taskar, editors, Predicting Structured Data. MIT
Press, 2006.

[18] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l
Conf. Computer Vision, volume 2, pages 416–423, July 2001.

[19] S. Osindero and G. E. Hinton. Modeling image patches with a directed hierarchy of markov random
ﬁelds. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing
Systems 20, pages 1121–1128. MIT Press, Cambridge, MA, 2008.

[20] M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In Advances in
Neural Information Processing Systems (NIPS 2007), 2007.

[21] S. Roth and M. J. Black. Fields of experts. International Journal of Computer Vision, 82(2):205–229,
2009.

[22] A. N. Tikhonov. On the stability of inverse problems. Dokl. Akad. Nauk SSSR, (39):176–179, 1943.

[23] Y. Weiss and W. T. Freeman. What makes a good model of natural images. In CVPR 2007: Proceedings
of the 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE
Computer Society, pages 1–8, 2007.

[24] M. Welling, G. E. Hinton, and S. Osindero. Learning sparse topographic representations with products
of student-t distributions. In S. T. S. Becker and K. Obermayer, editors, Advances in Neural Information
Processing Systems 15, pages 1359–1366. MIT Press, Cambridge, MA, 2003.

9


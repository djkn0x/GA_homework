A Framework for Recognizing Hand Gestures

David Knight
CS229 Student

Matthew Tang
CS229 Student

Hendrik Dahlkamp
Mentor

Christian Plagemann
Mentor

December 10, 2010

1

Introduction

Traditional methods for user input, consisting of ﬁxed key input and point-click devices, are slowly
being supplemented and enhanced by touch technologies and their associated touch gestures. While
these methods have proven eﬀective, they are limited in scope in that they require a physical ob ject
or surface with which a user must interact.
The use of hand gestures in free space is often seen as an intuitive next step in the progression of
user input technologies. A system that takes cues from such gestures would unshackle a user from
physical devices. Using such systems, a user only needs their body motion to signal speciﬁc actions
that can be picked up by a camera and interpreted by appropriate computer vision algorithms.
In this paper, we explore the use of machine learning to process and interpret image sequences in
order to correctly recognize and classify a couple simple hand gestures. Our work is part of a larger
body of research by computer science Ph.D. student Hendrik Dahlkamp and computer science
Postdoctoral researcher Christian Plagemann to create a gesture-based UI that allows users to
manipulate virtual ob jects in 3D space. An active infrared camera is used to capture both infrared
intensity data as well as depth information. Unlike a passive RGB camera, the depth information
captured by the infrared camera greatly simpliﬁes segmentation of foreground ob jects. Existing
components of Dahlkamp’s vision system can track the movement of a hand once it has been
recognized as such, crop out the region around the hand, and perform the necessary background
subtraction to produce the training sets we use. Recognizing a hand gesture, then, requires that we
be able to 1) diﬀerentiate a “hand image” from a “nonhand image”, and 2) correlate a sequence of
hand images with a speciﬁc gesture. This paper comprises the development of these two functional
blocks using machine learning techniques.

2 Hand Classiﬁcation

2.1 Training Data

Given an image, we need to be able to classify it as a “hand image” or “nonhand image”. To do
this using a machine learning algorithm, we need an appropriate set of training data. This data is
obtained by capturing typical scenes of people giving hand gestures in a room with an active infrared
camera. The pre-existing hand tracker provided by Hendrik Dahlkamp then crops out 32x32 pixel,
8-bit grayscale images from these scenes and performs the appropriate background subtraction on
them. Some of these images contain hands, while others contain various other ob jects in the scene.

1

(a) Hand Image

(b) Hand Image

(c) Nonhand Image

Figure 1: Sample “hand” and “nonhand” images.

Labeling these images is a matter of separating the “hand” images (e.g. Figs. 1(a), 1(b)) from the
“nonhand” images (e.g. Fig. 1(c)).

2.2 Feature Selection

The main features for characterizing our various images are normalized bin values from a Pyramid
of Histogram of Oriented Gradients (PHOG). Obtaining a Histogram of Oriented Gradients (HOG)
involves calculating a gradient direction and magnitude for every pixel in the image and binning
these gradients by their direction with a weight based on their magnitude [1]. PHOG extends this
method by calculating a histogram for a region, subdividing the region into sub-regions, calculating
histograms for each sub-region, and repeating this process. We used an implementation of PHOG
created by the Visual Geometry Group at the University of Oxford [2], allowing us to conﬁgure
subdivision level depth, bin resolution, and histogram range.
Our use of HOG is motivated by the similarity of our problem of recognizing hands to the
ob jective of detecting humans in the original HOG paper [1]. Additionally, intuition suggests that
gradient direction information should be able to highlight diﬀerences between a hand versus other
blob shapes. PHOG gives us the added ﬂexibility of specifying sublevels on which to take HOG
measurements, in addition to other HOG related parameters. The exact parameters we used were
chosen by performing 70-30 cross validation on our test data.

2.3 Training and Testing

Recall that our goal is to label and track hand positions. We can therefore aﬀord to occasionally
classify a “hand” as a “nonhand” since we would, over a large number of frames, overwhelmingly
label the image content as a “hand”. Once a “hand” has been conﬁrmed over a few frames, the
ob ject tracker will start providing a steady stream of known “hand” images. However, we can ill
aﬀord to classify a “nonhand” ob ject as a “hand” on more than a few occasions, since the hand
tracker would start tracking this “nonhand” ob ject. The precision of our algorithm, then, is more
important than its recall.
Using a simple Support Vector Machine (SVM) with a linear classiﬁer, and using 70-30 cross
validation to train and test, we ﬁnd that we are able to quickly achieve a precision of 99.79% and
a recall of 86.61% on a training set of size 27,000. These results were obtained by using PHOG
with 8 bins/level, and 2 sublevels, resulting in a feature vector size of 8(1 + 22 + 42 ) = 168.
We next add to our feature vector a histogram of intensities, binning intensity values along with
our original oriented gradient values. This quick addition, eﬀectively doubling the length of our

2

Figure 2: Hidden Markov Model.

feature vector to 336 dimensions, manages to improve our precision to 99.90% and our recall to
90.96%, which is suﬃcient for our purposes.

3 Gesture Classiﬁcation

3.1 Hand Position Discrimination

In the previous section, we were able to diﬀerentiate between “hand” and “nonhand” images with
high accuracy. That classiﬁer, used in conjunction with the hand tracker, will now give us a
sequence of hand images from which we can infer gestures. To do so, we ﬁrst characterize a gesture
with a few deﬁning positions. For example, a grasping motion might be characterized by an open
palm followed by a closed ﬁst (or any number of intermediate positions; for the sake of simplicity
we will consider only a two state model for our gesture). Thus, one way to recognize a gesture
from our set of images is to label any sequence consisting of an “open hand” followed quickly by a
“closed hand” as a “grasp” gesture.
To classify hand positions, we take our image sequence and manually label the images preceding
a grasping motion as “open” and label the images following a grasping motion as “closed”. We
then attempt to train a machine learning algorithm on this set of data in order to give us an
initial classiﬁcation of hand states. We ﬁnd that with PHOG and binned intensity values, we are
unable to achieve an accuracy greater than 85%. Various attempts to improve accuracy using these
features (e.g. using a Mixture of Gaussians model) proved ineﬀective. Our improved approach is to
use the images’ pixel intensity values as a feature vector coupled with a SVM with a second order
polynomial kernel. This method yields a classiﬁer with 95% accuracy (90-10 cross validation on a
training set of 4,500) and is used in the ﬁnal recognition system.

3.2 Modeling Image Sequences with a Hidden Markov Model

So far, we have attempted to label every image in our sequence independently, ignoring the correla-
tion between temporally adjacent images. We can exploit this temporal correlation by modeling the
sequence with a Hidden Markov Model. Suppose that at time t the hand we are tracking has a true
state (“open” or “closed”) given by xt . Given an image of the hand yt , we can calculate p(yt |xt ) by
(cid:80)N −1
ﬁtting a logistic function to the margin output of the SVM classiﬁer. Furthermore, we can estimate
p(xt+1 |xt ) by counting transitions in our training sequences. That is, given a typical sequence of
x0 , x1 , . . . , xN ordered in time, we estimate p(xt+1 = a|xt = b) = 1
n=0 1{xn+1 = a}1{xn = b}.
Using forward recursion [3] we can calculate, on the ﬂy, p(xt |y t ) where y t = {y0 , y1 , . . . , yt}. This
N
allows us to take into account all previous observations rather than just the current observation. To
improve the accuracy of our estimate xt , we can delay estimation until time t + d, allowing us to use
forward-backward recursion to calculate p(xt |y t+d ). This will introduce a delay d in our system, but

3

Figure 3: Predictions on a captured 30 frames/s image sequence (shown as Gesture Sequence 1 in Figure 4).
Images that did not resemble either “open” or “closed” hands were labelled 0.5. Introduced a 3 frame delay
for the forward-backward recursion. All parameters were obtained by training on 9 other gesture sequences
of similar length (90-10 cross validation).

allow us to consider future observations in our estimation of xt . We note here that the larger the
delay d, the larger the number of running estimates we need to store, since p(xt |y t+d ) is calculated
from p(xt |y t ), p(xt )|y t+1 ), . . . , p(xt |y t+d−1 ); however, this extra storage can be considered negligible
because the delay d should only be on the order of a few frames. The fundamental tradeoﬀ in
choosing d is therefore accuracy vs. system response time.
We can now deﬁne “grasp” and “release” gestures as points in the frame classiﬁcation output
where conﬁdence values transition from <0.5 to >0.5 and >0.5 to <0.5, respectively. Without
using forward-backward recursion, the raw frame-by-frame output of classiﬁcation is fairly noisy as
is shown in Figure 3. Results in Figure 4 show that trying to identify gestures from the raw output
results in an overall high number of false identiﬁcations. In particular, some gesture sequences show
more false identiﬁcations than true identiﬁcations when using the raw output. Applying forward-
backward recursion signiﬁcantly reduces the number of false identiﬁcations, retains most of the true
identiﬁcations, and slightly increases the frame classiﬁcation accuracy.
Finally, we note that there is a large amount of freedom in choosing criteria for a grasping
event. For example, we can declare a gesture occurrence when our estimated state is “open” for
more than some ﬁxed number of frames, followed by “closed” for some ﬁxed number of frames.
Such a criterion would ﬁlter out overly quick open-close-open events, requiring the user to hold
his/her hand closed for a ﬁxed duration before a grasping event is recognized. Additionally, the
derivative of the classiﬁcation output at a transition point can provide information about the speed
of a gesture. Diﬀerent gestures can be deﬁned for the same basic motion that occurs at diﬀerent
speeds.

4 Conclusion

While using forward-backward recursion greatly reduces the number of false gesture identiﬁcations,
a few false identiﬁcations remain in many gesture sequence tests. Most of the anomalies come
from interpreting quick bursts of classiﬁcation uncertainty as a full transition and can likely be
addressed by imposing more advanced criteria for registering a gesture as was explained in the

4

Figure 4: Gesture recognition results both with and without forward-backward recursion (FBR). Gestures
were identiﬁed by state transition without consideration for gesture speed. Results for each sequence were
obtained by training on the 9 other gesture sequences of similar length (90-10 cross validation).

previous section. Also, forward-backward recursion slightly decreased the number of true gesture
identiﬁcations in a few of the sequence tests. The exact cause of this eﬀect needs to be further
investigated, but it might actually be desirable behavior if the gestures that are being skipped
over are due to uncertain user input. The next step in this pro ject is to implement the gesture
recognition system in C so that it can be integrated with the rest of the vision system developed by
Dahlkamp and Plagemann. This integration will allow the vision system to be tested as a whole.
Overall, we have provided a basic framework with which to classify hand gestures. Given
appropriate methods for distinguishing between “hand” images and “nonhand” images, and for
tracking a hand once it has been identiﬁed, we can produce a sequence of hand images that can
be further analyzed for the purposes of gesture recognition. Given a speciﬁc gesture we want to
detect, we identify key positions of that gesture and develop classiﬁers that are able to correctly
bin the captured images into one of the identiﬁed key positions. We can then estimate transition
probabilities between these positions by counting and averaging transition occurrences in a typical
sequence of gestures, allowing us to use forward-backward recursion to obtain accurate estimates
of the underlying state at any given time. Various criteria can then be used to recognize a gesture
(e.g. when these key positions have been realized in a certain order), providing us with a robust
hand gesture detector.

References

[1] N. Dalal, B. Triggs. “Histograms of Oriented Gradients for Human Detection”. Proc. IEEE
Conf. Computer Vision and Pattern Recognition, vol. 2, 2005: 886-893.

[2] A. Bosch, A. Zisserman, “Pyramid Histogram of Oriented Gradients (PHOG)”. University of
Oxford Visual Geometry Group, http://www.robots.ox.ac.uk/ vgg/research/caltech/phog.html.

[3] L. E. Baum, “An Inequality and Associated Maximization Technique in Statistical Estimation
for Probabilistic Functions of Markov Processes“. Inequalities, vol. 3, 1972: 1-8.

5


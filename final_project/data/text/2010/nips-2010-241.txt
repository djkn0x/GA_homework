Online Learning in the Manifold of
Low-Rank Matrices

Uri Shalit∗, Daphna Weinshall
Computer Science Dept. and ICNC
The Hebrew University of Jerusalem
uri.shalit@mail.huji.ac.il
daphna@cs.huji.ac.il

Gal Chechik
Google Research and
The Gonda Brain Research Center
Bar Ilan University
gal@google.com

Abstract
When learning models that are represented in matrix forms, enforcing a low-rank
constraint can dramatically improve the memory and run time complexity, while
providing a natural regularization of the model. However, naive approaches for
minimizing functions over the set of low-rank matrices are either prohibitively
time consuming (repeated singular value decomposition of the matrix) or nu-
merically unstable (optimizing a factored representation of the low rank matrix).
We build on recent advances in optimization over manifolds, and describe an it-
erative online learning procedure, consisting of a gradient step, followed by a
second-order retraction back to the manifold. While the ideal retraction is hard to
compute, and so is the projection operator that approximates it, we describe an-
other second-order retraction that can be computed efﬁcien tly, with run time and
memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n,
given rank-one gradients. We use this algorithm, LORETA, to learn a matrix-
form similarity measure over pairs of documents represented as high dimensional
vectors. LORETA improves the mean average precision over a passive- aggres-
sive approach in a factorized model, and also improves over a full model trained
over pre-selected features using the same memory requirements. LORETA also
showed consistent improvement over standard methods in a large (1600 classes)
multi-label image classi ﬁcation task.

1
Introduction
Many learning problems involve models represented in matrix form. These include metric learning,
collaborative ﬁltering, and multi-task learning where all
tasks operate over the same set of features.
In many of these models, a natural way to regularize the model is to limit the rank of the correspond-
ing matrix. In metric learning, a low rank constraint allows to learn a low dimensional representation
of the data in a discriminative way. In multi-task problems, low rank constraints provide a way to
tie together different tasks. In all cases, low-rank matrices can be represented in a factorized form
that dramatically reduces the memory and run-time complexity of learning and inference with that
model. Low-rank matrix models could therefore scale to handle substantially many more features
and classes than with full rank dense matrices.

As with many other problems, the rank constraint is non-convex, and in the general case, minimizing
a convex function subject to a rank constraint is NP-hard [1] 1 . As a result, two main approaches have
been commonly used. Sometimes, a matrix W ∈ Rn×m of rank k is represented as a product of two
low dimension matrices W = AB T , A ∈ Rn×k , B ∈ Rm×k and simple gradient descent techniques
are applied to each of the product terms separately [3]. Second, projected gradient algorithms can
be applied by repeatedly taking a gradient step and projecting back to the manifold of low-rank
matrices. Unfortunately, computing the projection to that manifold becomes prohibitively costly for
large matrices and cannot be computed after every gradient step.

∗ also at the Gonda Brain Research Center, Bar Ilan University
1Some special cases are solvable (notably, PCA), relying mainly on singular value decomposition [2] and
semi-deﬁnite programming techniques. These methods scale poorly to lar ge scale tasks.

1

Figure 1: A two step procedure for com-
puting a retracted gradient. The ﬁrst step
computes the Riemannian gradient ξ (the
projection of the gradient onto the tan-
gent space TxMn,m
), yielding xt+ 1
2 =
k
xt + η t∇L(xt ). The second step com-
putes the retraction onto the manifold
xt+1 = Rx (ξ t ).

In this paper we propose new algorithms for online learning on the manifold of low-rank matrices,
which are based on an operation called retraction. Retractions are operators that map from a vector
space that is tangent to the manifold, into the manifold. They include the projection operator as a
special case, but also include other retractions that can be computed dramatically more efﬁciently.
We use second order retractions to develop LORETA – an online algorithm for learning low rank
matrices. It has a memory and run time complexity of O ((n + m)k) when the gradients have rank
one, a case which is relevant to numerous online learning problems as we show below.

We test Loreta in two different domains and learning tasks. First, we learn a bilinear similarity
measure among pairs of text documents, where the number of features (text terms) representing
each document could become very large. Loreta performed better than other techniques that operate
on a factorized model, and also improves retrieval precision by 33% as compared with training a
full rank model over pre-selected most informative features, using comparable memory footprint.
Second, we applied Loreta to image multi-label ranking, a problem in which the number of classes
could grow to millions. Loreta signi ﬁcantly improved over f ull rank models, using a fraction of the
memory required. These two experiments suggest that low-rank optimization could become very
useful for learning in high-dimensional problems.

This paper is organized as follows. We start with an introduction to optimization on manifolds,
describing the notion of retractions. We then derive our low-rank online learning algorithm, and test
it in two applications: learning similarity of text documents, and multi-label ranking for images.

2 Optimization on Riemannian manifolds

The ﬁeld of numerical optimization on smooth manifolds has a dvanced signi ﬁcantly in the past
few years. We start with a short introduction to embedded manifolds, which are the focus of
this paper. An embedded manifold is a smooth subset of an ambient space Rn . For instance the
set {x : ||x||2 = 1, x ∈ Rn }, the unit sphere, is an n − 1 dimensional manifold embedded in n-
dimensional space Rn . Here we focus on the manifold of low-rank matrices, namely, the set of
n × m matrices of rank k where k < m, n. It is an (n + m)k − k2 dimensional manifold embedded
in Rn×m , which we denote Mn,m
. Embedded manifolds inherit many properties from the ambient
k
space, a fact which simpli ﬁes their analysis. For example, t he Riemannian metric for embedded
manifolds is simply the Euclidean metric restricted to the manifold.

L(x)

.

(1)

Motivated by online learning, we focus here on developing a stochastic gradient descent procedure
to minimize a loss function L over the manifold of low-rank matrices Mn,m
,
k
s.t. x ∈ Mn,m
k

min
x
To illustrate the challenge in this problem, consider a simple stochastic gradient descent algorithm
(Fig. 1). At every step t of the algorithm, a gradient step update takes xt+ 1
2 outside of the manifold
M and has to be mapped back onto the manifold. The most common mapping operation is the
projection operation, which, given a point xt+ 1
2 outside the manifold, would ﬁnd the closest point
in M. Unfortunately, the projection operation is very expensive to compute for the manifold of
low rank matrices, since it basically involves a singular value decomposition. Here we describe a
wider class of operations called retractions, that serve a similar purpose: they ﬁnd a point on the
manifold that is in the direction of the gradient. Importantly, we describe a speci ﬁc retraction that
can be computed efﬁciently.
Its runtime complexity depends on 4 quantities:
the model matrix
dimensions m and n; its rank k ; and the rank of the gradient matrix, r . The overall complexity is
O (cid:0)(n + m)(k + r)2 (cid:1), and O ((n + m)k) for rank-one gradients, which are a very common case.
2

To explain how retractions are computed, we ﬁrst describe th e notion of a tangent space and the
Riemannian gradient of a function on a manifold.

Riemannian gradient and the tangent space
Each point x in an embedded manifold M has a tangent space associated with it, denoted TxM
(see Fig. 1). The tangent space is a vector space of the same dimension as the manifold that can
be identi ﬁed in a natural way with a linear subspace of the amb ient space. It is usually simple to
compute the linear projection Px of any point in the ambient space onto the tangent space TxM.
Given a manifold M and a differentiable function L : M → R, the Riemannian gradient ∇L(x)
of L on M at a point x is a vector in the tangent space TxM. A very useful property of embedded
manifolds is the following: given a differentiable function f deﬁned on the ambient space (and thus
on the manifold), the Riemannian gradient of f at point x is simply the linear projection Px of the
ordinary gradient of f onto the tangent space TxM. An important consequence follows in case
the manifold represents the set of points obeying a certain constraint. In this case the Riemannian
gradient of f is equivalent to the ordinary gradient of the f minus the component which is normal
to the constraint. Indeed this normal component is exactly the component which is irrelevant when
performing constrained optimization.

The Riemannian gradient allows us to compute xt+ 1
2 = xt + η t∇L(x), for a given iterate point xt
and step size η t . We now examine how xt+ 1
2 can be mapped back onto the manifold.

Retractions
Intuitively, retractions capture the notion of ”going along a straight line” on the man ifold. The
mathematically ideal retraction is called the exponential mapping: it maps the tangent vector ξ ∈
TxM to a point along a geodesic curve which goes through x in the direction of ξ . Unfortunately, for
many manifolds (including the low-rank manifold considered here) calculating the geodesic curve is
computationally expensive. A major insight from the ﬁeld of Riemannian manifold optimization is
that using the exponential mapping is unnecessary since computationally cheaper retractions exist.
Formally, for a point x in an embedded manifold M, a retraction is any function Rx : TxM → M
which satis ﬁes the following two conditions [4]: (1) Center ing: Rx (0) = x. (2) Local rigidity: the
curve deﬁned by γξ (τ ) = Rx (τ ξ ) satis ﬁes
˙γξ (0) = ξ . It can be shown that any such retraction
approximates the exponential mapping to a ﬁrst order [4].
Second-order retractions, which ap-
proximate the exponential mapping to second order around x, have to satisfy the following stricter
conditions: Px (cid:16) dRx (τ ξ)
|τ =0(cid:17) = 0, for all ξ ∈ TxM, where Px is the linear projection from the
dτ 2
ambient space onto the tangent space TxM. When viewed intrinsically, the curve Rx (τ ξ ) deﬁned
by a second-order retraction has zero acceleration at point x, namely, its second order derivatives
are all normal to the manifold. The best known example of a second-order retraction onto embed-
ded manifolds is the projection operation [5]. Importantly, projections are viewed here as one type
of a second order approximation to the exponential mapping, which can be replaced by any other
second-order retractions, when computing the projection is too costly.

Given the tangent space and a retraction, we can now deﬁne a Ri emannian gradient descent step for
the loss L at point xt ∈ M:
2 = xt + ξ t , with ξ t = ∇L(xt ) = Pxt ( ˜∇L(xt )), where ˜∇L(xt )
(1) Gradient step: Compute xt+ 1
is the ordinary gradient of L in the ambient space.
(2) Retraction step: Compute xt+1 = Rxt (−η t ξ t ), where η t is the step size.
For a proper step size, this procedure can be proved to have local convergence for any retraction [4].

3 Online learning on the low rank manifold

Based on the retractions described above, we now present an online algorithm for learning low-
rank matrices, by performing stochastic gradient descent on the manifold of low rank matrices.
At every iteration the algorithm suffers a loss, and performs a Riemannian gradient step followed
by a retraction to the manifold Mn,m
. Section 3.1 discusses general online updates. Section 3.2
k
discusses the very common case where the online updates induce a gradient of rank r = 1.
In what follows, a lowercase x denotes an abstract point on the manifold, lowercase Greek letters
like ξ denote an abstract tangent vector, and uppercase Roman letters like A denote concrete matrix

3

representations as kept in memory (taking n × m ﬂoat numbers to store). We intermix the two
notations, as in ξ = AZ , when the meaning is clear from the context. The set of n × k matrices of
rank k is denoted Rn×k
.
∗

3.1 The general LORETA algorithm
We start with a Lemma that gives a representation of the tangent space TxM, extending the con-
structions given in [6] to the general manifold of low-rank matrices. The proof is given in the
supplemental material.
Lemma 1. Let x ∈ Mn,m
have a (non-unique) factorization x = AB T , where A ∈ Rn×k
, B ∈
∗
k
. Let A⊥ ∈ Rn×(n−k) and B⊥ ∈ Rm×(m−k) be the orthogonal complements of A and B
Rm×k
∗
respectively, such that AT
⊥A = 0, B T
⊥B = 0, AT
⊥A⊥ = In−k , B T
⊥B⊥ = Im−k . The tangent space
to Mn,m
at x is:
k
0 (cid:21) (cid:20)B T
⊥ (cid:21) : M ∈ Rk×k , N1 ∈ R(m−k)×k , N2 ∈ R(n−k)×k(cid:27) (2)
TxM = (cid:26)[A A⊥ ] (cid:20)M N T
1
B T
N2
Let ξ ∈ Mn,m
be a tangent vector to x = AB T . From the characterization above it follows that
k
r ,
ξ can be decomposed in a unique manner into three orthogonal components: ξ = ξS + ξP
l + ξP
where ξS = AM B T , ξP
r = A⊥N2B T . In online learning we are repeatedly
⊥ and ξP
1 B T
l = AN T
given a rank-r gradient matrix Z , and want to compute a step on Mn,m
in the direction of Z . As
k
a ﬁrst step we wish to ﬁnd its projection
Px (Z ) onto the tangent space. Speci ﬁcally, we wish to
ﬁnd the three matrices M , N1 and N2 such that Px (Z ) = AM B T + AN T
⊥ + A⊥N2B T . Since
1 B T
we assume A is of full column rank, its pseudo-inverse A† obeys A† = (cid:0)AT A(cid:1)−1
AT . The matrix
projecting onto A’s columns, denoted PA , is exactly equal to AA† . We can similarly deﬁne PA⊥ ,PB
and PB⊥ . A straightforward computation shows that for a given matrix Z , we have M = A†ZB †T ,
r = PA⊥ Z PB .
⊥Z T A†T , N2 = AT
⊥ , yielding ξS = PAZ PB , ξP
l = PAZ PB⊥ , ξP
N1 = B T
⊥ZB T
The following theorem deﬁnes the retraction that we use. The proof is given in the supplemental
material.
Theorem 1. Let x ∈ Mn,m
, x = AB T , and x† = B †T A† = B (B T B )−1 (AT A)−1AT (this holds
k
since we assume A and B are of full column rank). Let ξ ∈ TxMn,m
, ξ = ξS + ξP
l + ξP
r , as
k
described above, and let

w1 = x +

ξS x† ξS −

ξP
r x† ξS

(3)

ξS + ξP
r −

1
1
1
2
8
2
1
1
1
ξS + ξP
ξS x† ξP
l −
l
2
8
2
The mapping Rx (ξ ) = w1x†w2 is a second order retraction from a neighborhood Θx ⊂ TxMn,m
k
to Mn,m
.
k

ξS x† ξS −

w2 = x +

,

.

We now have the ingredients necessary for a Riemannian stochastic gradient descent algorithm.

Algorithm 1 : Naive Riemannian stochastic gradient descent
Input: Matrices A ∈ Rn×k
s.t. x = AB T . Matrices G1 ∈ Rn×r , G2 ∈ Rm×r s.t. G1GT
, B ∈ Rm×k
2 =
∗
∗
−ηξ = −η ˜∇L(x) ∈ Rn×m , where ˜∇L(x) is the gradient in the ambient space and η > 0 is the step size.
Output: Matrices Z1 ∈ Rn×k
, Z2 ∈ Rm×k
such that Z1Z T
2 = Rx (−ηξ ).
∗
∗
Compute:
A† = (AT A)−1AT , B † = (B T B )−1B T
A⊥ , B⊥= orthogonal complements of A, B
2 B †T
M = A†G1GT
1 A†T , N2 = AT
2 B †T
⊥G1GT
N1 = B T
⊥G2GT
Z1 = (cid:0)A (cid:0)Ik + 1
8 M 2 (cid:1) + A⊥N2 (cid:0)Ik − 1
2 M (cid:1)(cid:1)
2 M − 1
8 (M T )2 (cid:1) + B⊥N1 (cid:0)Ik − 1
2 M T (cid:1)(cid:1)
Z2 = (cid:0)B (cid:0)Ik + 1
2 M T − 1
Given a gradient in the ambient space ˜∇L(x), we can calculate the matrices M , N1 and N2 which
allow us to represent its projection onto the tangent space, and furthermore allow us to calculate the
retraction. The procedure is outlined in algorithm 1, with some rearranging and term collection.

matrix dimension
k × n,
k × m
n × (n − k), m × (m − k)
k × k
(n − k) × k
n × k
m × k

(m − k) × k ,

4

Algorithm 1 explicitly computes and stores the orthogonal complement matrices A⊥ and B⊥ , which
in the low rank case k ≪ m, n, have size O(mn) as the original x. To improve the memory
complexity, we use the fact that the matrices A⊥ and B⊥ always operate with their transpose. Since
⊥ is a projection matrix, one which we denoted earlier by PA⊥ ,
they are orthogonal, the matrix A⊥AT
and likewise for B⊥ . Because of the orthogonal complementarity, these projection matrices are
equal to In − PA and Im − PB respectively. We use this identity to reformulate the algorithm such
that only matrices of size at most max(n, m) × k or max(n, m) × r are kept in memory. The runtime
complexity of Algorithm 2 can be easily computed based on matrix multiplications complexity, and
equals O (cid:0)(n + m)(k + r)2 (cid:1).
Algorithm 2 : General Riemannian stochastic gradient descent
Input and Output: As in Algorithm 1
Compute:
A† = (AT A)−1AT , B † = (B T B )−1B T
ˆA = A† · G1 ,
ˆB = B † · G2
P rojAG = A · ˆA
Q = ˆB T · ˆA
A∆ = − 1
8 P rojAG · Q + G1 − 1
2 P rojAG + 3
2 G1 · Q
Z1 = A + A∆ · ˆB T
2 B(cid:1) · B †
GBproj = (cid:0)GT
B∆ = − 1
2 Q · GT
8 Q · GBproj + GT
2 − 1
2 GBproj + 3
2
2 = B T + ˆA · B∆
Z T

matrix dimension
k × n,
k × m
k × r
k × r,
n × r
r × r
n × r
n × k
r × m
r × m
k × m

Algorithm 3 , Loreta-1: Rank-one Riemannian stochastic gradient descent
Input: Matrices A ∈ Rn×k
, B ∈ Rm×k
s.t. x = AB T . Matrices A† and B † , the pseudo-inverses of A and
∗
∗
2 = −ηξ = −η ˜∇L(x) ∈ Rn×m , where ˜∇L(x)
B respectively. Vectors G1 ∈ Rn×1 , G2 ∈ Rm×1 s.t. G1GT
is the gradient in the ambient space and η > 0 is the step size.
Output: Matrices Z1 ∈ Rn×k
, Z2 ∈ Rm×k
1 and Z †
2 = Rx (−ηξ ). Matrices Z †
s.t. Z1Z T
2 , the pseudo-inverses
∗
∗
of Z1 and Z2 respectively.
Compute:
ˆA = A† · G1 , ˆB = B † · G2
P rojAG = A · ˆA
Q = ˆB T · ˆA
8 Q(cid:1) + G1 (1 − 1
A∆ = P rojAG (cid:0)− 1
2 + 3
2 Q)
Z1 = A + A∆ · ˆB T
GBproj = (cid:0)GT
2 B(cid:1) · B †
B∆ = GBproj (cid:0)− 1
8 Q(cid:1) + GT
2 (1 − 1
2 + 3
2 Q)
2 = B T + ˆA · B∆
Z T
1 = rank one pseudoinverse update(A, A† , A∆ , ˆB )
Z †
2 = rank one pseudoinverse update(B , B † , B∆ , ˆA)
Z †

matrix dimension
k × 1
n × 1
1 × 1
n × 1
n × k
1 × m
1 × m
k × m
k × n
k × m

3.2 LORETA with rank-one gradients

In many learning problems, the gradient matrix required for a gradient step update has a rank of one.
This is the case for example, when the matrix model W acts as a bilinear form on two vectors, p and
T W q (as in [7, 8], and Sec. 5.1). In that case, the gradient
q , and the loss is a linear function of p
is the rank-one, outer product matrix pq
T . As another example, consider the case of multitask
learning, where the matrix model W operates on a vector input p, and the loss is the squared loss
between the multiple predictions W p and the true labels q : kW p − qk2 . The gradient of the loss
is (W p − q) p
T , which is again a rank-one matrix. We now show how to reduce the complexity of
each iteration to be linear in the model rank k when the rank of the gradient matrix r is one.

Given rank-one gradients (r = 1), the most computationally demanding step in Algorithm 2 is the
computation of the pseudo-inverse of the matrices A and B , taking O(nk2 ) and O(mk2 ) operations.
All other operations are O(max(n, m)k) at most. For r = 1 the outputs Z1 and Z2 become rank-one
updates of the input matrices A and B . This enables us to keep the pseudo-inverses A† and B † from
the previous round, and perform a rank-one update to them, following a procedure developed by [9].

5

This procedure is similar to the better known Sherman-Morrison formula for the inverse of a rank-
one perturbed matrix, and its computational complexity for an n × k matrix is O(nk) operations.
Using that procedure, we derive our ﬁnal algorithm, Loreta-1, the rank-one Riemannian stochastic
gradient descent. Its overall time and space complexity are both O((n + m)k) per gradient step.
The memory requirement of Loreta-1 is about 4nk (assuming m = n), since it receives four input
matrices of size nk (A, B , A† , B † ) and assuming it can compute the four outputs (Z1 , Z2 , Z †
1 , Z †
2 ),
in-place while destroying previously computed terms.

4 Related work
A recent summary of many advances in the ﬁeld of optimization on manifolds is given in [4]. More
speci ﬁc to the ﬁeld of low rank matrix manifolds, some work ha
s been done on the general problem
of optimization with low rank positive semi-deﬁnite (PSD) m atrices. These include [10] and [6]; the
latter introduced the retraction for PSD matrices which we extended here to general low-rank matri-
ces. The problem of minimizing a convex function over the set of low rank matrices, was addressed
by several authors, including [11], and [12] which also considers additional afﬁne constraints, and
its connection to recent advances in compresses sensing. The main tools used in these works are the
trace norm (sum of singular values) and semi-deﬁnite progra mming. See also [2].

More closely related to the current paper are the works by Kulis et al. [13] and Meka et al. [14]. The
ﬁrst deals with learning low rank PSD matrices, and uses the r ank-preserving log-det divergence and
clever factorization and optimization in order to derive an update rule with runtime complexity of
O(nk2 ) for an n × n matrix of rank k . The second uses online learning in order to ﬁnd a minimal
rank square matrix under approximate afﬁne constraints. Th e algorithm does not directly allow a
factorized representation, and depends crucially on an ”or acle” component, which typically requires
to compute an SVD. Multi-class ranking with a large number of features was studied in [3].

5 Experiments
We tested Loreta-1 in two learning tasks: learning a similarity measure between pairs of text docu-
ments using the 20-newsgroups data collected by [15], and learning to rank image label annotations
based on a multi-label annotated set, using the imagenet dataset [16].2

5.1 Learning similarity on the 20 Newsgroups data set
In our ﬁrst set of experiments, we looked at the problem of lea rning a similarity measure between
pairs of text documents. Similarity learning is a well studied problem, closely related to metric
learning (see [17] for a review). It has numerous applications in information retrieval such as query
by example, and ﬁnding related content on the web.
One approach to learn pairwise relations is to measure the similarity of two documents p, q ∈ Rn
T W q parametrized by a model W ∈ Rn×n . Such models
using a bilinear form SW (p, q) = p
can be learned using standard online methods [8], and were shown to achieve high precision. Un-
fortunately, since the number of parameters grows as n2 , storing the matrix W in memory is only
feasible for limited feature dimensionality. To handle larger vocabularies, like those containing all
textual terms found in a corpus, a common approach is to pre-select a subset of the features and train
a model over the low dimensional data. However, such preprocessing may remove crucial signals in
the data even if features are selected in a discriminative way.

To overcome this difﬁculty, we used Loreta-1 to learn a rank- k parametrization of the model W ,
which can be factorized as W = AB T , where A, B ∈ Rn×k . In each of our experiments, we
selected a subset of n features, and trained a rank k model. We varied the number of features n and
the rank of the matrix k so as to use a ﬁxed amount of memory. For example, we used a rank -10
model with 50K features, and a rank-50 model with 10K features.
Similarity learning with Loreta-1. We use an online procedure similar to that in [7, 8]. At each
round, three instances are sampled: a query document q, and two documents p1 and p2 such that
p1 is known to be more similar to q that p2 . We wish that the model assigns a higher similarity
score to the pair (q, p1 ) than the pair (q, p2 ), hence use the online ranking hinge loss deﬁned as
lW (q, p1 , p2 ) = [1 − SW (q, p1 ) + SW (q, p2 )]+ .

2Matlab code for Loreta-1 can be provided upon request.

6

Data preprocessing and feature selection. We used the 20 newsgroups data set (peo-
ple.csail.mit.edu/jrennie/20Newsgroups), containing 20 classes with approximately 1000 documents
each. We removed stop words but did not apply stemming. We selected features that conveyed high
information about the identity of the class (over the training set) using the infogain criterion [18].
The selected features were normalized using tf-idf, and then represented each document as a bag of
words. Two documents were considered similar if they shared the same class label.
Experimental procedure and evaluation protocol. The 20 newsgroups site proposes a split of
the data into train and test sets. We repeated splitting 5 times based on the sizes of the proposed
splits (a train / test ratio of 65% / 35%). We evaluated the learned similarity measures using a
ranking criterion. We view every document q in the test set as a query, and rank the remaining test
documents p by their similarity scores q
T W p. We then compute the precision (fraction of positives)
at the top r ranked documents. We further compute the mean average precision (mAP), a widely
used measure in the information retrieval community, which averages over all values of r .
Comparisons. We compared Loreta with the following approaches. (1) A direct gradient descent
(GD) similar to [3]. The model is represented as a product of two matrices ˆW = AB T . Stochastic
gradient descent steps are computed over the factors A and B , for the same loss used by Loreta
lW (q, p1 , p2 ). The step size η was selected using cross validation. The GD steps are: Anew =
T A. (2) Iterative Passive-Aggressive (PA). We
A + ηq(p1 − p2 )T B , and Bnew = A + η(p1 − p2 )q
found the above GD procedure to be very unstable, often causing the models to diverge. We therefore
used a related online algorithm from the family of passive-aggressive algorithms [19]. We iteratively
optimize over A given a ﬁxed B and vice versa. The optimization is a tradeoff between minimizing
the loss lW , and limiting how much the models change at each iteration. The steps size for updating
lW (q,p1 ,p2 ))
lW (q,p1 ,p2 ))
A is computed to be ηA = max(
kqk2 kBT (p1−p2 )k2 , C ), and ηB = max(
qk2 , C ). C
k(p1−p2 )k2 kAT
is a predeﬁned parameter controlling the maximum magnitude of the step size. This procedure is
numerically more stable because of the normalization by the norms of the matrices multiplied by
the gradient factors. (3) Naive Passive-Aggressive (PA v2) This method is similar to the iterative
PA above, with the step size computed as with unfactored matrices η = max( lW (q,p1 ,p2 ))
kqk2 k(p1−p2 )k2 , C ).
(4) Full rank similarity learning models. We compared with two online metric learning methods,
LEGO [20] and OASIS [8]. Both algorithms learn a full (non-factorized) model, and were run with
n = 1000, in order to be consistent with the memory constraint of Loreta-1. We have not compared
with batch approaches such as [13]

Figure 2b shows the mean average precision obtained with the three measures. Loreta outperforms
the PA approach across all ranks. More importantly, learning a low rank model of rank 30, using the
best 16660 features, is signi ﬁcantly more precise than lear ning a much fuller model of rank 100 and
5000 features. The intuition is that Loreta can be viewed as adaptively learning a linear projection
of the data into low dimensional space, which is tailored to the pairwise similarity task.

5.2
Image multilabel ranking
Our second set of experiments tackled the problem of learning to rank labels for images taken from
a large number of classes (L = 1661) with multiple labels per image.
In our approach, we learn a linear classi ﬁer over n features per each label c ∈ C = {1, . . . , L}, and
stack all models together to a single matrix W ∈ RL×n . At test time, given an image p ∈ Rn , the
product W p provides scores for every label per that image p. Given a ground truth labeling, a good
model would rank the true labels higher than the false ones. Each row of the matrix model can be
thought of as a sub-model for the corresponding label. Imposing a low rank constraint on the model
implies that these sub-models are linear combinations of a smaller number of latent models.
Online learning of label rankings with Loreta-1. At each iteration, an image p is sampled, and
using the current model W the scores for all its labels were computed, W p. These scores are
compared with the ground truth labeling y = {y1 , . . . , yr } ⊂ C . The learner suffers a multilabel
multiclass hinge loss as follows. Let ¯y = argmaxs /∈y (W p)s , be the negative label which obtained
the highest score, where (W p)s is the sth component of the score vector (W p). The loss is then
L(W, p, y) = Pr
i=1 [(W p) ¯y − (W p)yi + 1]+ . We then used the subgradient G of this loss for
Loreta: for the set of indices i1 , i2 , . . . id ⊂ y which incurred a non zero hinge loss, the ij row of G
is p, and for the row ¯y we set G to be −d · p. The matrix G is rank one, unless no loss was suffered
in which case it is 0. .

7

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
P
A
M
(
 
n
o
i
s
i
c
e
r
p
 
e
g
a
r
e
v
a
 
n
a
e
m

0
 
0

0.72

0.68

0.61

)
P
A
m
(
 
n
o
i
s
i
c
e
r
p
 
e
g
a
r
e
v
a
 
n
a
e
m

0.45

 

rank = 100
rank = 75
rank = 50
rank = 40
rank = 30
rank = 20
rank = 10
400

LORETA
PA
PA v2
OASIS
LEGO
10
30

 

)
P
A
m
(
 
n
o
i
s
i
c
e
r
p
 
e
g
a
r
e
v
a
 
n
a
e
m

0.109

0.074

0.02

(a) 20 Newsgroups

(b) 20 Newsgroups

 

(c) ImageNet

 

Loreta−1
Loreta−1 rand. init.
Iterative PA
Iterative PA rand. init.
Matrix Perceptron
Group MC Perceptron
400
1000

0

 
10 50

150

100

100

1000

50
75
200
300
250
matrix rank k
matrix rank k
iterations (thousands)
Figure 2: (a) Mean average precision (mAP) over 20 newsgroups test set as traced along Loreta learning for
various ranks. Curve values are averages over 5 train-test splits. (b) mAP of different models with varying
rank. For each rank, a different number of features was selected using an information gain criterion, such that
the total memory requirement is kept ﬁxed (number of features × rank is constant). 50000 features were used
for rank = 10. LEGO and OASIS were trained with the same memory (using 1000 features and rank=1000).
Error bars denote the standard error of the mean over 5 train-test splits (s.e.m.). (c) ImageNet data. mAP as a
function of the rank k . Curves are means over three train-test splits. Error bars denote the standard error of the
mean (s.e.m.). All hyper parameters were selected using cross validation. Models were initialized either with
k ones along the diagonal, or as a product of rank-k matrices with random normal entries (denoted rand. init.).

Data set and preprocessing.
the ImageNet 2010 Challenge
We used a subset of
(www.imagenet.org/challenges/LSVRC/2010/) containing images labeled with respect to the Word-
Net hierarchy. Each image was manually labeled with a single class label (for a total of 1000 classes).
We added labels for each image, using classes along the path to the root of the hierarchy (adding 676
classes in total). We discarded ancestor labels covering more than 10% of the images, leaving 1661
labels (5.3 labels per image on average). We used ImageNets bag of words representation, based on
vector quantizing SIFT features with a vocabulary of 1000 words, followed by tf-idf normalization.
Experimental procedure and evaluation protocol. We split the data into 30 training and 20 testing
images per every base level label. The quality of the learned label ranking, was evaluated using the
mean average precision (mAP) criterion mentioned above.
Comparisons. We compared the performance of Loreta on this task with three other approaches:
(1) PA: Iterative Passive-Aggressive as described above.
(2) Matrix Perceptron: a full rank
conservative gradient descent (3) Group Multi-Class Perceptron a mixed (2,1) norm online mirror
descent algorithm [21]. Loreta and PA were run using a range of different model ranks. For all three
methods the step size (or C parameter for the PA) was chosen by 5-fold validation on the test set.

Figure Fig. 2c plots the mAP precision of Loreta and PA for different model ranks, while showing
on the right the mAP of the full rank 1000 gradient descent and (2, 1) norm algorithms. Loreta
signi ﬁcantly improves over all other methods across all ran ks.
6 Discussion
We presented Loreta, an algorithm which learns a low-rank matrix based on stochastic Riemannian
gradient descent and efﬁcient retraction to the manifold of
low-rank matrices. Loreta achieves supe-
rior precision in a task of learning similarity in high dimensional feature spaces, and in multi-label
annotation, where it scales well with the number of classes.

Loreta yields a factorized representation of the low rank matrix. For classi ﬁcation, it can be viewed
as learning two matrix components: one that projects the high dimensional data into a low dimen-
sion, and a second that learns to classify in the low dimension. It may become useful in the future
for exploring high dimensional data, or extract relations between large number of classes.

Acknowledgments

This work was supported by the Israel Science Foundation (ISF) and by the European Union under
the DIRAC integrated project IST-027787.

8

References

[1] B.K. Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing,
24(2):227–234, 1995.
[2] M. Fazel, H. Hindi, and S. Boyd. Rank minimization and applications in system theory. In
Proceedings of the 2004 American Control Conference, pages 3273–3278. IEEE, 2005.
[3] B. Bai, J. Weston, R. Collobert, and D. Grangier. Supervised semantic indexing. Advances in
Information Retrieval, pages 761–765, 2009.
[4] P.A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.
Princeton Univ Press, 2008.
[5] P.-A. Absil and J ´er ˆome Malick. Projection-like retractions on matrix manifolds. Technical Re-
port UCL-INMA-2010.038, Department of Mathematical Engineering, Universit ´e catholique
de Louvain, July 2010.
[6] B. Vandereycken and S. Vandewalle. A Riemannian optimization approach for computing low-
rank solutions of Lyapunov equations. SIAM Journal on Matrix Analysis and Applications,
31:2553, 2010.
[7] D. Grangier D. and S. Bengio. A discriminative kernel-based model to rank images from text
queries.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 30:1371–1384,
2008.
[8] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale online learning of image simi-
larity through ranking. Journal of Machine Learning Research, 11:1109–1135, 2010.
[9] C.D. Meyer. Generalized inversion of modi ﬁed matrices.
SIAM Journal on Applied Mathe-
matics, 24(3):315–323, 1973.
[10] M. Journee, F. Bach, PA Absil, and R. Sepulchre. Low-Rank Optimization on the Cone of
Positive Semideﬁnite Matrices. SIAM Journal on Optimization, 20:2327–2351, 2010.
[11] M. Fazel. Matrix rank minimization with applications. PhD thesis, Electrical Engineering
Department, Stanford University, 2002.
[12] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.
[13] B. Kulis, M.A. Sustik, and I.S. Dhillon. Low-rank kernel learning with bregman matrix diver-
gences. The Journal of Machine Learning Research, 10:341–376, 2009.
[14] R. Meka, P. Jain, C. Caramanis, and I.S. Dhillon. Rank minimization via online learning. In
Proceedings of the 25th International Conference on Machine learning, pages 656–663, 2008.
[15] K. Lang. Learning to ﬁlter netnews.
In Proceeding of the 12th Internation Conference on
Machine Learning, pages 331–339, 1995.
[16] J. Deng, W. Dong, R. Socher, L.J. Li, K. Li, and L. Fei-Fei. ImageNet: a large-scale hierar-
chical image database. In Proceedings of the 22nd IEEE Conference on Computer Vision and
Pattern Recognition, 2009.
[17] L. Yang. An overview of distance metric learning. Technical report, School of Computer
Science, Carnegie Mellon University, 2007.
[18] Y. Yang and J.O. Pedersen. A comparative study on feature selection in text categorization. In
Proceedings of the 14th International Conference on Machine learning, pages 412–420, 1997.
[19] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive
algorithms. Journal of Machine Learning Research, 7:551–585, 2006.
[20] P. Jain, B. Kulis, I.S. Dhillon, and K. Grauman. Online metric learning and fast similarity
search. Advances in Neural Information Processing Systems, pages 761–768, 2008.
[21] Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for
learning with matrices, 2010. preprint.

9


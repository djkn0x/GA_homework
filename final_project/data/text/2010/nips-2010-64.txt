A Computational Decision Theory
for Interactive Assistants

Alan Fern
School of EECS
Oregon State University
Corvallis, OR 97331
afern@eecs.oregonstate.edu

Prasad Tadepalli
School of EECS
Oregon State University
Corvallis, OR 97331
tadepall@eecs.oregonstate.edu

Abstract

We study several classes of interactive assistants from the points of view of deci-
sion theory and computational complexity. We ﬁrst introduce a class of POMDPs
called hidden-goal MDPs (HGMDPs), which formalize the problem of interac-
tively assisting an agent whose goal is hidden and whose actions are observable.
In spite of its restricted nature, we show that optimal action selection in ﬁnite hori-
zon HGMDPs is PSPACE-complete even in domains with deterministic dynamics.
We then introduce a more restricted model called helper action MDPs (HAMDPs),
where the assistant’s action is accepted by the agent when it is helpful, and can be
easily ignored by the agent otherwise. We show classes of HAMDPs that are com-
plete for PSPACE and NP along with a polynomial time class. Furthermore, we
show that for general HAMDPs a simple myopic policy achieves a regret, com-
pared to an omniscient assistant, that is bounded by the entropy of the initial goal
distribution. A variation of this policy is shown to achieve worst-case regret that
is logarithmic in the number of goals for any goal distribution.

Introduction
1
Integrating AI with Human Computer Interaction has received signiﬁcant attention in recent years [8,
11, 13, 3, 2]. In most applications, e.g. travel scheduling, information retrieval, or computer desktop
navigation, the relevant state of the computer is fully observable, but the goal of the user is not, which
poses a difﬁcult problem to the computer assistant. The assistant needs to correctly reason about the
relative merits of taking different actions in the presence of signiﬁcant uncertainty about the goals of
the human agent. It might consider taking actions that directly reveal the goal of the agent, e.g. by
asking questions to the user. However, direct communication is often difﬁcult due to the language
mismatch between the human and the computer. Another strategy is to take actions that help achieve
the most likely goals. Yet another strategy is to take actions that help with a large number of possible
goals. In this paper, we formulate and study several classes of interactive assistant problems from
the points of view of decision theory and computational complexity. Building on the framework
of decision-theoretic assistance (DTA) [5], we analyze the inherent computational complexity of
optimal assistance in a variety of settings and the sources of that complexity. Positively, we analyze
a simple myopic heuristic and show that it performs nearly optimally in a reasonably pervasive
assistance problem, thus explaining some of the positive empirical results of [5].
We formulate the problem of optimal assistance as solving a hidden-goal MDP (HGMDP), which
is a special case of a POMDP [6]. In a HGMDP, a (human) agent and a (computer) assistant take
actions in turns. The agent’s goal is the only unobservable part of the state of the system and does
not change throughout the episode. The objective for the assistant is to ﬁnd a history-dependent
policy that maximizes the expected reward of the agent given the agent’s goal-based policy and its
goal distribution. Despite the restricted nature of HGMDPs, the complexity of determining if an
HGMDP has a ﬁnite-horizon policy of a given value is PSPACE-complete even in deterministic

1

environments. This motivates a more restricted model called Helper Action MDP (HAMDP), where
the assistant executes a helper action at each step. The agent is obliged to accept the helper action
if it is helpful for its goal and receives a reward bonus (or cost reduction) for doing so. Otherwise,
the agent can continue with its own preferred action without any reward or penalty to the assistant.
We show classes of this problem that are complete for PSPACE and NP. We also show that for the
class of HAMDPs with deterministic agents there are polynomial time algorithms for minimizing
the expected and worst-case regret relative to an omniscient assistant. Further, we show that the
optimal worst case regret can be characterized by a graph-theoretic property called the tree rank of
the corresponding all-goals policy tree and can be computed in linear time.
The main positive result of the paper is to give a simple myopic policy for general stochastic
HAMDPs that has a regret which is upper bounded by the entropy of the goal distribution. Fur-
thermore we give a variant of this policy that is able to achieve worst-case and expected regret that
is logarithmic in the number of goals without any prior knowledge of the goal distribution.
To the best of our knowledge, this is the ﬁrst formal study of the computational hardness of the prob-
lem of decision-theoretically optimal assistance and the performance of myopic heuristics. While
the current HAMDP results are conﬁned to unobtrusively assisting a competent agent, they provide
a strong foundation for analyzing more complex classes of assistant problems, possibly including
direct communication, coordination, partial observability, and irrationality of users.

2 Hidden Goal MDPs

Throughout the paper we will refer to the entity that we are attempting to assist as the agent and
the assisting entity as the assistant. Our objective is to select actions for the assistant in order to
help the agent maximize its reward. The key complication is that the agent’s goal is not directly
observable to the assistant, so reasoning about the likelihood of possible goals and how to help
maximize reward given those goals is required. In order to support this type of reasoning we will
model the agent-assistant process via hidden goal MDPs (HGMDPs).
General Model. An HGMDP describes the dynamics and reward structure of the environment
via a ﬁrst-order Markov model, where it is assumed that the state is fully observable to both
the agent and assistant.
In addition, an HGMDP describes the possible goals of the agent and
the behavior of the agent when pursuing those goals. More formally, an HGMDP is a tuple
(cid:104)S, G, A, A(cid:48) , T , R, π , IS , IG (cid:105) where S is a set of states, G is a ﬁnite set of possible agent goals,
A is the set of agent actions, A(cid:48) is the set of assistant actions, T is the transition function such that
T (s, g , a, s(cid:48) ) is the probability of a transition to state s(cid:48) from s after taking action a ∈ A ∪ A(cid:48) when
the agent goal is g , R is the reward function which maps S × G × (A ∪ A(cid:48) ) to real valued rewards, π
is the agent’s policy that maps S × G to distributions over A and need not be optimal in any sense,
and IS (IG ) is an initial state (goal) distribution. The dependence of the reward and policy on the
goal allows the model to capture the agent’s desires and behavior under each goal. The dependence
of T on the goal is less intuitive and in many cases there will be no dependence when T is used only
to model the dynamics of the environment. However, we allow goal dependence of T for generality
of modeling. For example, it can be convenient to model basic communication actions of the agent
as changing aspects of the state, and the result of such actions will often be goal dependent.
We consider a ﬁnite-horizon episodic problem setting where the agent begins each episode in a state
drawn from IS with a goal drawn from IG . The goal, for example, might correspond to a physical
location, a dish that the agent wants to cook, or a destination folder on a computer desktop. The
process then alternates between the agent and assistant executing actions (including noops) in the
environment until the horizon is reached. The agent is assumed to select actions according to π . In
many domains, a terminal goal state will be reached within the horizon, though in general, goals can
have arbitrary impact on the reward function. The reward for the episode is equal to the sum of the
rewards of the actions executed by the agent and assistant during the episode. The objective of the
assistant is to reason about the HGMDP and observed state-action history in order to select actions
that maximize the expected (or worst-case) total reward of an episode.
An example HGMDP from previous work [5] is the doorman domain, where an agent navigates a
grid world in order to arrive at certain goal locations. To move from one location to another the
agent must open a door and then walk through the door. The assistant can reduce the effort for the
agent by opening the relevant doors for the agent. Another example from [1] involves a computer

2

desktop where the agent wishes to navigate to certain folders using a mouse. The assistant can select
actions that offer the agent a small number of shortcuts through the folder structure.
Given knowledge of the agent’s goal g in an HGMDP, the assistant’s problem reduces to solving
an MDP over assistant actions. The MDP transition function captures both the state change due to
the assistant action and also the ensuing state change due to the agent action selected according to
the policy π given g . Likewise the reward function on a transition captures the reward due to the
assistant action and the ensuing agent action conditioned on g . The optimal policy for this MDP
corresponds to an optimal assistant policy for g . However, since the real assistant will often have
uncertainty about the agent’s goal, it is unlikely that this optimal performance will be achieved.
Computational Complexity. We can view an HGMDP as a collection of |G| MDPs that share the
same state space, where the assistant is placed in one of the MDPs at the beginning of each episode,
but cannot observe which one. Each MDP is the result of ﬁxing the goal component of the HG-
MDP deﬁnition to one of the goals. This collection can be easily modeled as a restricted type of
partially observable MDP (POMDP) with a state space S × G. The S component is completely ob-
servable, while the G component is unobservable but only changes at the beginning of each episode
(according to IG ) and remains constant throughout an episode. Furthermore, each POMDP tran-
sition provides observations of the agent action, which gives direct evidence about the unchanging
G component. From this perspective HGMDPs appear to be a signiﬁcant restriction over general
POMDPs. However, our ﬁrst result shows that despite this restriction the worst-case complexity is
not reduced even for deterministic dynamics.
Given an HGMDP M , a horizon m = O(|M |) where |M | is the size of the encoding of M , and a
reward target r∗ , the short-term reward maximization problem asks whether there exists a history-
dependent assistant policy that achieves an expected ﬁnite horizon reward of at least r∗ . For general
POMDPs this problem is PSPACE-complete [12, 10], and for POMDPs with deterministic dynam-
ics, it is NP-complete [9]. However, we have the following result.
Theorem 1. Short-term reward maximization for HGMDPs with deterministic dynamics is
PSPACE-complete.

The proof is in the appendix. This result shows that any POMDP can be encoded as an HGMDP
with deterministic dynamics, where the stochastic dynamics of the POMDP are captured via the
stochastic agent policy in the HGMDP. However, the HGMDPs resulting from the PSPACE-hardness
reduction are quite pathological compared to those that are likely to arise in practice. Most impor-
tantly, the agent’s actions provide practically no information about the agent’s goal until the end of
an episode, when it is too late to exploit this knowledge. This suggests that we search for restricted
classes of HGMDPs that will allow for efﬁcient solutions with performance guarantees.

3 Helper Action MDPs
The motivation for HAMDPs is to place restrictions on the agent and assistant that avoid the fol-
lowing three complexities that arise in general HGMDPs: 1) the agent can behave arbitrarily poorly
if left unassisted and as such the agent actions may not provide signiﬁcant evidence about the goal;
2) the agent is free to effectively “ignore” the assistant’s help and not exploit the results of assistive
action, even when doing so would be beneﬁcial; and 3) the assistant actions have the possibility of
negatively impacting the agent compared to not having an assistant. HAMDPs will address the ﬁrst
issue by assuming that the agent is competent at (approximately) maximizing reward without the
assistant. The last two issues will be addressed by assuming that the agent will always “detect and
exploit” helpful actions and that the assistant actions do not hurt the agent.
Informally, the HAMDP provides the assistant with a helper action for each of the agent’s actions.
Whenever a helper action h is executed directly before the corresponding agent action a, the agent
receives a bonus reward of 1. However, the agent will only accept the helper action h (by taking a)
and hence receive the bonus, if a is an action that the agent considers to be good for achieving the
goal without the assistant. Thus, the primary objective of the assistant in an HAMDP is to maximize
the number of helper actions that get accepted by the agent. While simple, this model captures much
of the essence of assistance domains where assistant actions cause minimal harm and the agent is
able to detect and accept good assistance when it arises.
An HAMDP is an HGMDP (cid:104)S, G, A, A(cid:48) , T , R, π , IS , IG (cid:105) with the following constraints:

3

• The agent and the assistant actions sets are A = {a1 , . . . , an} and A(cid:48) = {h1 , . . . , hn}, so
that for each ai there is a corresponding helper action hi .
• The state space is S = W ∪ (W × A(cid:48) ), where W is a set of world states. States in W × A(cid:48)
encode the current world state and the previous assistant action.
• The reward function R is 0 for all assistant actions. For agent actions the reward is zero
unless the agent selects the action ai in state (s, hi ) which gives a reward of 1. That is, the
agent receives a bonus of 1 whenever its action corresponds to the preceding helper action.
• The assistant always acts from states in W , and T is such that taking hi in s deterministi-
cally transitions to (s, hi ).
• The agent always acts from states in S × A(cid:48) , resulting in states in S according to a transition
function that does not depend on hi , i.e. T ((s, hi ), g , ai , s(cid:48) ) = T (cid:48) (s, g , ai , s(cid:48) ) for some
transition function T (cid:48) .
• Finally, for the agent policy, let Π(s, g) be a function that returns a set of actions and P (s, g)
be a distribution over those actions. We will view Π(s, g) as the set of actions that the agent
considers acceptable (or equally good) in state s for goal g . The agent policy π always
selects ai after its helper action hi whenever ai is acceptable. That is, π((s, hi ), g) = ai
whenever ai ∈ Π(s, g). Otherwise the agent draws an action according to P (s, g).
In a HAMDP, the primary impact of an assistant action is to inﬂuence the reward of the following
agent action. The only rewards in HAMDPS are the bonuses received whenever the agent accepts a
helper action. Any additional environmental reward is assumed to be already captured by the agent
policy via Π(s, g) that contains actions that approximately optimize this reward.
The HAMDP model can be adapted to both the doorman domain in [5] and the folder prediction
domain from [1]. In the doorman domain, the helper actions correspond to opening doors for the
agent, which reduce the cost of navigating from one room to another.
Importantly opening an
incorrect door has a ﬁxed reward loss compared to an optimal assistant, which is a key property
of HAMDPs. In the folder prediction domain, the system proposes multiple folders to save a ﬁle,
potentially saving the user a few clicks every time the proposal is accepted.
Despite the apparent simpliﬁcation of HAMDPs over HGMDPs, somewhat surprisingly the worst
case computational complexity is not reduced.
Theorem 2. Short-term reward maximization for HAMDPs is PSPACE-complete.

The proof is in the appendix. Unlike the case of HGMDPs, we will see that the stochastic dynamics
are essential for PSPACE-hardness. Despite this negative result, the following sections show the
utility of the HAMDP restriction by giving performance guarantees for simple policies and improved
complexity results in special cases. So far, there are no analogous results for HGMDPs.

4 Regret Analysis for HAMDPs
Given an assistant policy π (cid:48) , the regret of a particular episode is the extra reward that an omniscient
assistant with knowledge of the goal would achieve over π (cid:48) . For HAMDPs the omniscient assistant
can always achieve a reward equal to the ﬁnite horizon m, because it can always select a helper action
that will be accepted by the agent. Thus, the regret of an execution of π (cid:48) in a HAMDP is equal to
the number of helper actions that are not accepted by the agent, which we will call mispredictions.
From above we know that optimizing regret is PSPACE-hard and thus here we focus on bounding
the expected and worst-case regret of the assistant. We now show that a simple myopic policy is
able to achieve regret bounds that are logarithmic in the number of goals.
Myopic Policy. Intuitively, our myopic assistant policy ˆπ will select an action that has the highest
probability of being accepted with respect to a “coarsened” version of the posterior distribution over
goals. The myopic policy in state s given history H is based on the consistent goal set C (H ), which
is the set of goals that have non-zero probability with respect to history H . It is straightforward to
maintain C (H ) after each observation. The myopic policy is deﬁned as:
IG (C (H ) ∩ G(s, a))
ˆπ(s, H ) = arg max
a
where G(s, a) = {g | a ∈ Π(s, g)} is the set of goals for which the agent considers a to be an
acceptable action in state s. The expression IG (C (H ) ∩ G(s, a)) can be viewed as the probability

4

mass of G(s, a) under a coarsened goal posterior which assigns goals outside of C (H ) probability
zero and otherwise weighs them proportional to the prior.
Theorem 3. For any HAMDP the expected regret of the myopic policy is bounded above by the
entropy of the goal distribution H(IG ).

Proof. The main idea of the proof is to show that after each misprediction of the myopic policy (i.e.
the selected helper action is not accepted by the agent) the uncertainty about the goal is reduced by
a constant factor, which will allow us to bound the total number of mispredictions on any trajectory.
Consider a misprediction step where the myopic policy selects helper action hi in state s given his-
tory H , but the agent does not accept the action and instead selects a∗ (cid:54)= ai . By the deﬁnition of the
myopic policy we know that IG (C (H ) ∩ G(s, ai )) ≥ IG (C (H ) ∩ G(s, a∗ )), since otherwise the
assistant would not have chosen hi . From this fact we now argue that IG (C (H (cid:48) )) ≤ IG (C (H ))/2
where H (cid:48) is the history after the misprediction. That is, the probability mass under IG of the con-
sistent goal set after the misprediction is less than half that of the consistent goal set before the
misprediction. To show this we will consider two cases: 1) IG (C (H ) ∩ G(s, ai )) < IG (C (H ))/2,
and 2) IG (C (H ) ∩ G(s, ai )) ≥ IG (C (H ))/2.
In the ﬁrst case, we immediately get that
IG (C (H )∩G(s, a∗ )) < IG (C (H ))/2. Combining this with the fact that C (H (cid:48) ) ⊆ C (H )∩G(s, a∗ )
we get the desired result that IG (C (H (cid:48) )) ≤ IG (C (H ))/2. In the second case, note that
C (H (cid:48) ) ⊆ C (H ) ∩ (G(s, a∗ ) − G(s, ai )) ⊆ C (H ) − (C (H ) ∩ G(s, ai ))
Combining this with our assumption for the second case implies that IG (C (H (cid:48) )) ≤ IG (C (H ))/2.
This implies that for any episode, after n mispredictions resulting in a history Hn , IG (C (Hn )) ≤
2−n . Now consider an arbitrary episode where the true goal is g . We know that IG (g) is a lower
bound on IG (C (Hn )), which implies that IG (g) ≤ 2−n or equivalently that n ≤ − log(IG (g)).
IG is bounded above by − (cid:80)
Thus for any episode with goal g the maximum number of mistakes is bounded by − log(IG (g)).
Using this fact we get that the expected number of mispredictions during an episode with respect to
g IG (g) log(IG (g)) = H(IG ), which completes the proof.
Since H(IG ) ≤ log(|G|), this result implies that for HAMDPs the expected regret of the myopic
policy is no more than logarithmic in the number of goals. Furthermore, as the uncertainty about
the goal decreases (decreasing H(IG )) the regret bound improves until we get a regret of 0 when IG
puts all mass on a single goal. This logarithmic bound is asymptotically tight in the worst case.
Theorem 4. There exists a HAMDP such that for any assistant policy the expected regret is at least
log(|G|)/2.

Proof. Consider a deterministic HAMDP such that the environment is structured as a binary tree
of depth log(|G|), where each leaf corresponds to one of the |G| goals. By considering a uniform
goal distribution it is easy to verify that at any node in the tree there is an equal chance that the true
goal is in the left or right sub-tree during any episode. Thus, any policy will have a 0.5 chance of
committing a misprediction at each step of an episode. Since each episode is of length log(|G|), the
expected regret of an episode for any policy is log(|G|)/2.

Resolving the gap between the myopic policy bound and this regret lower bound is an open problem.
Approximate Goal Distributions. Suppose that the assistant uses an approximate goal distribution
I (cid:48)
G instead of the true underlying goal distribution IG when computing the myopic policy. That
G (C (H ) ∩ G(s, a)), which we will refer to as the
is, the assistant selects actions that maximize I (cid:48)
myopic policy relative to I (cid:48)
G . The extra regret for using I (cid:48)
G instead of IG can be bounded in terms
of the KL-divergence between these distributions KL(IG (cid:107) I (cid:48)
G ), which is zero when I (cid:48)
G equals IG .
Theorem 5. For any HAMDP with goal distribution IG , the expected regret of the myopic policy
G is bounded above by H(IG ) + KL(IG (cid:107) I (cid:48)
with respect to distribution I (cid:48)
G ).
The proof is in the appendix. Deriving similar results for other approximations is an open problem.
A consequence of Theorem 5 is that the myopic policy with respect to the uniform goal distribution
has expected regret bounded by log(|G|) for any HAMDP, showing that logarithmic regret can be
achieved without knowledge of IG . This can be strengthened to hold for worst case regret.

5

Theorem 6. For any HAMDP, the worst case and hence expected regret of the myopic policy with
respect to the uniform goal distribution is bounded above by log(|G|).

Proof. The proof of Theorem 5 shows that the number of mispredictions on any episode is bounded
above by − log(I (cid:48)
G = 1/|G| which shows a worst case regret bound of log(|G|),
G ). In our case I (cid:48)
which also bounds the expected regret of the uniform myopic policy.

5 Deterministic and Bounded Choice Policies
We now consider several special cases of HAMDPs. First, we restrict the agent’s policy to be
deterministic for each goal, i.e. Π(s, g) has at most a single action for each state-goal pair (s, g).
Theorem 7. The myopic policy achieves the optimal expected reward for HAMDPs with determin-
istic agent policies.

The proof is given in the appendix. We now consider the case where both the agent policy and
the environment are deterministic, and attempt to minimize the worst possible regret compared to
an omniscient assistant who knows the agent’s goal. As it happens, this “minimax policy” can be
captured by a graph-theoretic notion of tree rank that generalizes the rank of decision trees [4].
Deﬁnition 1. The rank of a rooted tree is the rank of its root node.
If a node is a leaf node
then rank(node) = 0, else if a node has at least two distinct children c1 and c2 with equal
highest ranks among all children, then rank(node) = 1+ rank(c1 ). Otherwise rank(node) =
rank of the highest ranked child.

The optimal trajectory tree (OTT) of a HAMDP in deterministic environments is a tree where the
nodes represent the states of the HAMDP reached by the preﬁxes of optimal action sequences for
different goals starting from the initial state.1 Each node in the tree represents a state and a set of
goals for which it is on the optimal path from the initial state.
Since the agent policy and the environment are both deterministic, there is at most one trajectory per
goal in the tree. Hence the size of the optimal trajectory tree is bounded by the number of goals times
the maximum length of any trajectory, which is at most the size of the state space in deterministic
domains. The following Lemma follows by induction on the depth of the optimal trajectory tree.
Lemma 1. The minimum worst-case regret of any policy for an HAMDP for deterministic envi-
ronments and deterministic agent policies is equal to the tree rank of its optimal trajectory tree.

Theorem 8. If the agent policy is deterministic, the problem of minimizing the maximum regret in
HAMDPs in deterministic environments is in P.

Proof. We ﬁrst construct the optimal trajectory tree. We then compute its rank and the optimal
minimax policy using the recursive deﬁnition of tree rank in linear time.

The assumption of deterministic agent policy may be too restrictive in many domains. We now
consider HAMDPs in which the agent policies have a constant bound on the number of possible
actions in Π(s, g) for each state-goal pair. We call them bounded choice HAMDPs.
Deﬁnition 2. The branching factor of a HAMDP is the largest number of possible actions in Π(s, g)
by the agent in any state for any goal and any assistant’s action.

The doorman domain of [5] has a branching factor of 2 since there are at most two optimal actions
to reach any goal from any state.
Theorem 9. Minimizing the worst-case regret in ﬁnite horizon bounded choice HAMDPS of a con-
stant branching factor k ≥ 2 in deterministic environments is NP-complete.

The proof is in the appendix. We can also show that minimizing the expected regret for a bounded
k is NP-hard. We conjecture that this problem is also in NP, but this question remains open.
1 If there are multiple initial states, we build an OTT for each initial state. Then the rank would be the
maximum of the ranks of all trees.

6

6 Conclusions and Future Work

In this paper, we formulated the problem of optimal assistance and analyzed its complexity in mul-
tiple settings. We showed that the general problem of HGMDP is PSPACE-complete due to the lack
of constraints on the user, who can behave stochastically or even adversarially with respect to the
assistant, which makes the assistant’s task very difﬁcult. By suitably constraining the user’s actions
through HAMDPs, we are able to reduce the complexity to NP-complete, but only in deterministic
environments with bounded choice agents. More encouragingly, we are able to show that HAMDPs
are amenable to a simple myopic heuristic which has a regret bounded by the entropy of the goal
distribution when compared to the omniscient assistant. This is a satisfying result since optimal
communication of the goal requires as much information to pass from the agent to the assistant. Im-
portantly, this result applies to stochastic as well as deterministic environments and with no bound
on the number of agent’s action choices.
Although HAMDPs are somewhat restricted compared to possible assistantship scenarios one could
imagine, they in fact ﬁt naturally to many domains where the user is on-line, knows which helper
actions are acceptable, and accepts help when it is appropriate to the goal. Indeed, in many domains,
it is reasonable to constrain the assistant so that the agent has the ﬁnal say on approving the actions
proposed by the assistant. These scenarios range from the ubiquitous auto-complete functions and
Microsoft’s infamous Paperclip to more sophisticated adaptive programs such as Smar tEdit [7]
and TaskTracer [3] that learn assistant policies from users’ long-term behaviors. By analyzing
the complexity of these tasks in a more general framework than what is usually done, we shed
light on some of the sources of complexity such as the stochasticity of the environment and the
agent’s policy. Many open problems remain including generalization of these and other results to
more general assistant frameworks, including partially observable and adversarial settings, learning
assistants, and multi-agent assistance.

7 Appendix

Proof of Theorem 1. Membership in PSPACE follows from the fact that any HGMDP can be poly-
nomially encoded as a POMDP for which policy existence is in PSPACE. To show PSPACE-
hardness, we reduce the QSAT problem to the problem of the existence of a history-dependent
assistant policy of expected reward ≥ r .
Let φ be a quantiﬁed Boolean formula ∀x1∃x2∀x3 . . . ∃xn {C1 (x1 , . . . , xn ) ∧ . . . ∧
Cm (x1 , . . . , xn )}, where each Ci is a disjunctive clause. For us, each goal gi is a quantiﬁed clause,
∀x1∃x2∀x3 . . . ∃xn {Ci (x1 , . . . , xn )}. The agent chooses a goal uniformly randomly from the set
of goals formed from φ and hides it from the assistant. The states consist of pairs of the form (v , i),
where v ∈ {0, 1} is the current value of the goal clause, and i is the next variable to set. The ac-
tions of the assistant are to set the existentially quantiﬁed variables. The agent simulates setting the
universally quantiﬁed variables by choosing actions from the set {0, 1} with equal probability. The
episode terminates when all the variables are set, and the assistant gets a reward of 1 if the value of
the clause is 1 at the end and a reward of 0 otherwise.
Note that the assistant does not get any useful feedback from the agent until it is too late and it
either makes a mistake or solves the goal. The best the assistant can do is to ﬁnd an optimal history-
dependent policy that maximizes the expected reward over the goals in Φ. If Φ is satisﬁable, then
there is an assistant policy that leads to a reward of 1 over all goals and all agent actions, and hence
has an expected value of 1 over the goal distribution. If not, then at least one of the goals will not be
satisﬁed for some setting of the universal quantiﬁers, leading to an expected value < 1.

Proof of Theorem 2. Membership in PSPACE follows easily since HAMDP is a specialization of
HGMDP. The proof of PSPACE-hardness is identical to that of 1 except that here, instead of the
agent’s actions, the stochastic environment models the universal quantiﬁers. The agent accepts
all actions until the last one and sets the variable as suggested by the assistant. After each of the
assistant’s actions, the environment chooses a value for the universally quantiﬁed variable with equal
probability. The last action is accepted by the agent if the goal clause evaluates to 1, otherwise not.
There is a history-dependent policy whose expected reward ≥ the number of existential variables if
and only if the quantiﬁed Boolean formula is satisﬁable.

7

IG (g)

=

IG (g) log(

IG (g) log(

Proof of Theorem 5. The proof is similar to that of Theorem 3, except that since the myopic policy
is with respect to I (cid:48)
G rather than IG , on any episode, the maximum number of mispredictions n is
(cid:18)
(cid:19)
bounded above by − log(I (cid:48)
(cid:88)
(cid:80)
G (g)). Hence, the average number of mispredictions is given by:
) + log(IG (g)) − log(IG (g))
1
1
) − (cid:88)
) =
log(
I (cid:48)
I (cid:48)
(cid:80)
g
G (g)
G (g)
g
IG (g) log(IG (g)) = H(IG ) + KL(IG (cid:107) I
(cid:48)
IG (g)
G ).
I (cid:48)
g
G (g)
g
Proof of Theorem 7. According to the theory of POMDPs, the optimal action in a POMDP maxi-
mizes the sum of the immediate expected reward and the value of the resulting belief state (of the
assistant) [6]. When the agent policy is deterministic, the initial goal distribution IG and the history
of agent actions and states H fully capture the belief state of the agent. Let V (IG , H ) represent
the optimal value of the current belief state. It satisﬁes the following Bellman equation, where H (cid:48)
stands for the history after the assistant’s action hi and the agent’s action aj .
(cid:48)
)
E (R((s, hi ), g , aj )) + V (IG , H
V (IG , H ) = max
hi
Since there is only one agent’s action a∗ (s, g) in Π(s, g), the subsequent state s(cid:48) in H (cid:48) , and its value
(cid:88)
do not depend on hi . Hence the best helper action h∗ of the assistant is given by:
IG (g)I (ai ∈ Π(s, g))
E (R((s, hi ), g , a∗ (s, g))) = arg max
h∗ (IG , H ) = arg max
hi
hi
g∈C (H )
IG (C (H ) ∩ G(s, ai ))

= arg max
hi

where C (H ) is the set of goals consistent with the current history H , and G(s, ai ) is the set of goals
g for which ai ∈ Π(s, g). I (ai ∈ Π(s, g)) is an indicator function which is = 1 if ai ∈ Π(s, g).
Note that h∗ is exactly the myopic policy.

Proof of Theorem 9. We ﬁrst show that the problem is in NP. We build a tree representation of an
optimal history-dependent policy for each initial state which acts as a polynomial-size certiﬁcate.
Every node in the tree is represented by a pair (si , Gi ), where si is a state and Gi is a set of goals
for which the node is on a good path from the root node. We let hi be the helper action selected in
node i. The children of a node in the tree represent possible successor nodes (sj , Gj ) reached by
the agent’s response to hi . Note that multiple children can result from the same action because the
dynamics is a function of the agent’s goal.
To verify that the optimal policy tree is of polynomial size we note that the number of leaf nodes is
upper bounded by |G| × maxg N (g), where N (g) is the number of leaf nodes generated by the goal
g and G is the set of all goals. To estimate N (g), we note that by our protocol, for any node (si , Gi )
where g ∈ Gi and the assistant’s action is hi , if ai ∈ Π(s, g), it will have a single successor that
contains g . Otherwise, there is a misprediction, which leads to at most k successors for g . Hence, the
number of nodes reached for g grows geometrically with the number of mispredictions. Since there
are at most log |G| mispredictions in any such path, N (g) ≤ k log2 |G| = k logk |G| log2 k = |G|log2 k .
Hence the total number of all leaf nodes of the tree is bounded by |G|1+log k , and the total number of
nodes in the tree is bounded by m|G|1+log k , where m is the number of steps to the horizon. Since
this is polynomial in the problem parameters, the problem is in NP.
To show NP-hardness, we reduce 3-SAT to the given problem. We consider each 3-literal clause
Ci of a propositional formula Φ as a possible goal. The rest of the proof is identical to that of
Theorem 1 except that all variables are set by the assistant. The agent accepts every setting, except
possibly the last one which he reverses if the clause evaluates to 0. Since the assistant does not get
any useful information until it makes the clause true or fails to do so, its optimal policy is to choose
the assignment that maximizes the number of satisﬁed clauses so that the mistakes are minimized.
The assistant makes a single prediction mistake on the last literal of each clause that is not satisﬁed
by the assignment. Hence, the worst regret on any goal is 0 iff the 3-SAT problem is satisﬁable.

Acknowledgments
The authors gratefully acknowledge the support of NSF under grants IIS-0905678 and IIS-0964705.

8

References
[1] Xinlong Bao, Jonathan L. Herlocker, and Thomas G. Dietterich. Fewer clicks and less frustration: reduc-
ing the cost of reaching the right folder. In IUI, pages 178–185, 2006.
[2] J. Boger, P. Poupart, J. Hoey, C. Boutilier, G. Fernie, and A. Mihailidis. A decision-theoretic approach to
task assistance for persons with dementia. In IJCAI, 2005.
[3] Anton N. Dragunov, Thomas G. Dietterich, Kevin Johnsrude, Matt McLaughlin, Lida Li, and Jon L. Her-
locker. Tasktracer: A desktop environment to support multi-tasking knowledge workers. In Proceedings
of IUI, 2005.
[4] Andrzej Ehrenfeucht and David Haussler. Learning decision trees from random examples. Information
and Computation, 82(3):231–246, September 1989.
[5] A. Fern, S. Natarajan, K. Judah, and P. Tadepalli. A decision-theoretic model of assistance. In Proceedings
of the International Joint Conference in AI, 2007.
[6] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially
observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998.
[7] Tessa A. Lau, Steven A. Wolfman, Pedro Domingos, and Daniel S. Weld. Programming by demonstration
using version space algebra. Machine Learning, 53(1-2):111–156, 2003.
[8] H. Lieberman. User interface goals, AI opportunities. AI Magazine, 30(2), 2009.
[9] M. L . Littman. Algorithms for Sequential Decision Making. PhD thesis, Brown University, Providence,
RI, 1996.
[10] Martin Mundhenk. The complexity of planning with partially-observable Markov Decision Processes.
PhD thesis, Friedrich-Schiller-Universitdt, 2001.
[11] K. Myers, P. Berry, J. Blythe, K. Conley, M. Gervasio, D. McGuinness, D. Morley, A. Pfeffer, M. Pollack,
and M. Tambe. An intelligent personal assistant for task and time management. AI Magazine, 28(2):47–
61, 2007.
[12] C. Papadimitriou and J. Tsitsiklis. The complexity of Markov Decision Processes. Mathematics of Oper-
ations Research, 12(3):441–450, 1987.
[13] M. Tambe. Electric Elves: What went wrong and why. AI Magazine, 29(2), 2008.

9


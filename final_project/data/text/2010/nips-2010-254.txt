A Reduction from Apprenticeship Learning to
Classi ﬁcation

Umar Syed∗
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
usyed@cis.upenn.edu

Robert E. Schapire
Department of Computer Science
Princeton University
Princeton, NJ 08540
schapire@cs.princeton.edu

Abstract

We provide new theoretical results for apprenticeship learning, a variant of rein-
forcement learning in which the true reward function is unknown, and the goal
is to perform well relative to an observed expert. We study a common approach
to learning from expert demonstrations: using a classi ﬁcat
ion algorithm to learn
to imitate the expert’s behavior. Although this straightforward learning strategy
is widely-used in practice, it has been subject to very little formal analysis. We
prove that, if the learned classi ﬁer has error rate
ǫ, the difference between the
value of the apprentice’s policy and the expert’s policy is O(√ǫ). Further, we
prove that this difference is only O(ǫ) when the expert’s policy is close to optimal.
This latter result has an important practical consequence: Not only does imitating
a near-optimal expert result in a better policy, but far fewer demonstrations are
required to successfully imitate such an expert. This suggests an opportunity for
substantial savings whenever the expert is known to be good, but demonstrations
are expensive or difﬁcult to obtain.

1

Introduction

Apprenticeship learning is a variant of reinforcement learning, ﬁrst introduced by A bbeel & Ng [1]
(see also [2, 3, 4, 5, 6]), designed to address the difﬁculty o f correctly specifying the reward function
in many reinforcement learning problems. The basic idea underlying apprenticeship learning is that
a learning agent, called the apprentice, is able to observe another agent, called the expert, behaving
in a Markov Decision Process (MDP). The goal of the apprentice is to learn a policy that is at least
as good as the expert’s policy, relative to an unknown reward function. This is a weaker requirement
than the usual goal in reinforcement learning, which is to ﬁn d a policy that maximizes reward.
The development of the apprenticeship learning framework was prompted by the observation that,
although reward functions are often difﬁcult to specify, de monstrations of good behavior by an
expert are usually available. Therefore, by observing such a expert, one can infer information about
the true reward function without needing to specify it.

Existing apprenticeship learning algorithms have a number of limitations. For one, they typically
assume that the true reward function can be expressed as a linear combination of a set of known fea-
tures. However, there may be cases where the apprentice is unwilling or unable to assume that the
rewards have this structure. Additionally, most formulations of apprenticeship learning are actually
harder than reinforcement learning; apprenticeship learning algorithms typically invoke reinforce-
ment learning algorithms as subroutines, and their performance guarantees depend strongly on the
quality of these subroutines. Consequently, these apprenticeship learning algorithms suffer from the
same challenges of large state spaces, exploration vs. exploitation trade-offs, etc., as reinforcement

∗Work done while the author was a student at Princeton University.

1

learning algorithms. This fact is somewhat contrary to the intuition that demonstrations from an
expert — especially a good expert — should make the problem ea
sier, not harder.

Another approach to using expert demonstrations that has received attention primarily in the empir-
ical literature is to passively imitate the expert using a classi ﬁcation algorithm (see [7, Section 4] for
a comprehensive survey). Classi ﬁcation is the most well-st udied machine learning problem, and it
is sensible to leverage our knowledge about this “easier ” pr oblem in order to solve a more “difﬁcult ”
one. However, there has been little formal analysis of this straightforward learning strategy (the
main recent example is Ross & Bagnell [8], discussed below). In this paper, we consider a setting
in which an apprentice uses a classi ﬁcation algorithm to pas sively imitate an observed expert in an
MDP, and we bound the difference between the value of the apprentice’s policy and the value of
the expert’s policy in terms of the accuracy of the learned classi ﬁer. Put differently, we show that
apprenticeship learning can be reduced to classi ﬁcation. The idea of reducing one learning problem
to another was ﬁrst proposed by Zadrozny & Langford [9].
Our main contributions in this paper are a pair of theoretical results. First, we show that the differ-
ence between the value of the apprentice’s policy and the expert’s policy is O(√ǫ),1 where ǫ ∈ (0, 1]
is the error of the learned classi ﬁer. Secondly, and perhaps more interestingly, we extend our ﬁrst
result to prove that the difference in policy values is only O(ǫ) when the expert’s policy is close to
optimal. Of course, if one could perfectly imitate the expert, then naturally a near-optimal expert
policy is preferred. But our result implies something further: that near-optimal experts are actually
easier to imitate, in the sense that fewer demonstration are required to achieve the same performance
guarantee. This has important practical consequences. If one is certain a priori that the expert is
demonstrating good behavior, then our result implies that many fewer demonstrations need to be col-
lected than if this were not the case. This can yield substantial savings when expert demonstrations
are expensive or difﬁcult to obtain.

2 Related Work

Several authors have reduced reinforcement learning to simpler problems. Bagnell et al [10] de-
scribed an algorithm for constructing a good nonstationary policy from a sequence of good “one-
step” policies. These policies are only concerned with maxi mizing reward collected in a single
time step, and are learned with the help of observations from an expert. Langford & Zadrozny
[11] reduced reinforcement learning to a sequence of classi ﬁcation problems (see also Blatt & Hero
[12]), but these problems have an unusual structure, and the authors are only able to provide a small
amount of guidance as to how data for these problems can be collected. Kakade & Langford [13]
reduced reinforcement learning to regression, but required additional assumptions about how easily
a learning algorithm can access the entire state space. Importantly, all this work makes the standard
reinforcement learning assumptions that the true rewards are known, and that a learning algorithm
is able to interact directly with the environment. In this paper we are interested in settings where
the reward function is not known, and where the learning algorithm is limited to passively observing
an expert. Concurrently to this work, Ross & Bagnell [8] have described an approach to reducing
imitation learning to classi ﬁcation, and some of their anal ysis resembles ours. However, their frame-
work requires somewhat more than passive observation of the expert, and is focused on improving
the sensitivity of the reduction to the horizon length, not the classi ﬁcation error. They also assume
that the expert follows a deterministic policy, and assumption we do not make.

3 Preliminaries

We consider a ﬁnite-horizon MDP, with horizon H . We will allow the state space S to be inﬁnite,
but assume that the action space A is ﬁnite. Let α be the initial state distribution, and θ the transition
function, where θ(s, a, ·) speci ﬁes the next-state distribution from state s ∈ S under action a ∈ A.
The only assumption we make about the unknown reward function R is that 0 ≤ R(s) ≤ Rmax for
all states s ∈ S , where Rmax is a ﬁnite upper bound on the reward of any state.
1The big-O notation is concealing a polynomial dependence on other problem parameters. We give exact
bounds in the body of the paper.

2

We introduce some notation and deﬁnitions regarding polici es. A policy π is stationary if it is a
mapping from states to distributions over actions. In this case, π(s, a) denotes the probability of
taking action a in state s. Let Π be the set of all stationary policies. A policy π is nonstationary if it
belongs to the set ΠH = Π × · · · (H times) · · · × Π . In this case, πt (s, a) denotes the probability of
taking action a in state s at time t. Also, if π is nonstationary, then πt refers to the stationary policy
that is equal to the tth component of π . A (stationary or nonstationary) policy π is deterministic if
each one of its action distributions is concentrated on a single action. If a deterministic policy π is
stationary, then π(s) is the action taken in state s, and if π is nonstationary, the πt (s) is the action
taken in state s at time t.

We deﬁne the value function V π
t (s) for a nonstationary policy π at time t as follows in the usual
manner:
t (s) , E " H
st = s, at′ ∼ πt′ (st′ , ·), st′+1 ∼ θ(st′ , at′ , ·)# .
R(st′ ) (cid:12)(cid:12)(cid:12)
Xt′=t
V π
So V π
t (s) is the expected cumulative reward for following policy π when starting at state s and time
step t. Note that there are several value functions per nonstationary policy, one for each time step t.
The value of a policy is deﬁned to be V (π) , E [V π
1 (s) | s ∼ α(·)], and an optimal policy π∗ is one
that satis ﬁes π∗ , arg maxπ V (π).
t (s) as an abbreviation for
We write πE to denote the (possibly nonstationary) expert policy, and V E
V πE
(s). Our goal is to ﬁnd a nonstationary apprentice policy πA such that V (πA ) ≥ V (πE ). Note
t
that the values of these policies are with respect to the unknown reward function.
t be the distribution on state-action pairs at time t under policy π . In other words, a sample
Let Dπ
t by ﬁrst drawing s1 ∼ α(·), then following policy π for time steps 1 through
(s, a) is drawn from Dπ
t, which generates a trajectory (s1 , a1 , . . . , st , at ), and then letting (s, a) = (st , at ). We write DE
t as
an abbreviation for DπE
. In a minor abuse of notation, we write s ∼ Dπ
t to mean: draw state-action
t
pair (s, a) ∼ Dπ
t , and discard a.
4 Details and Justiﬁcation of the Reduction

Our goal is to reduce apprenticeship learning to classi ﬁcat
ion, so let us describe exactly how this
reduction is deﬁned, and also justify the utility of such a re duction.
In a classi ﬁcation problem, a learning algorithm is given a t raining set h(x1 , y1 ), . . . , (xm , ym )i,
where each labeled example (xi , yi ) ∈ X × Y is drawn independently from a distribution D on X ×
Y . Here X is the example space and Y is the ﬁnite set of labels. The learning algorithm is also giv en
the deﬁnition of a hypothesis class H, which is a set of functions mapping X to Y . The objective
of the learning algorithm is to ﬁnd a hypothesis h ∈ H such that the error Pr(x,y)∼D (h(x) 6= y) is
small.
For our purposes, the hypothesis class H is said to be PAC-learnable if there exists a learning
δ , 1
algorithm A such that, whenever A is given a training set of size m = poly( 1
ǫ ), the algorithm
ǫ ) steps and outputs a hypothesis ˆh ∈ H such that, with probability at least 1 − δ ,
runs for poly( 1
δ , 1
we have Pr(x,y)∼D (cid:16)ˆh(x) 6= y(cid:17) ≤ ǫ∗
H,D + ǫ. Here ǫ∗
H,D = inf h∈H Pr(x,y)∼D (h(x) 6= y) is the
error of the best hypothesis in H. The expression poly( 1
δ , 1
ǫ ) will typically also depend on other
quantities, such as the number of labels |Y | and the VC-dimension of H [14], but this dependence is
not germane to our discussion.

The existence of PAC-learnable hypothesis classes is the reason that reducing apprenticeship learn-
ing to classi ﬁcation is a sensible endeavor. Suppose that th e apprentice observes m independent
H (cid:1).
trajectories from the expert’s policy πE , where the ith trajectory is a sequence (cid:0)si
1 , ai
1 , . . . , si
H , ai
The key is to note that each (si
t ) can be viewed as an independent sample from the distribution
t , ai
t . Now consider a PAC-learnable hypothesis class H, where H contains a set of functions map-
DE
H δ , 1
ping the state space S to the ﬁnite action space A. If m = poly( 1
ǫ ), then for each time step
t, the apprentice can use a PAC learning algorithm for H to learn a hypothesis ˆht ∈ H such that,
t (cid:16)ˆht (s) 6= a(cid:17) ≤ ǫ∗
with probability at least 1 − 1
H δ , we have Pr(s,a)∼DE
+ ǫ. And by the union
H,DE
t

3

+ ǫ is small, then a
bound, this inequality holds for all t with probability at least 1 − δ . If each ǫ∗
H,DE
t
t = ˆht for all t. This policy uses the learned
natural choice for the apprentice’s policy πA is to set πA
classi ﬁers to imitate the behavior of the expert.

In light of the preceding discussion, throughout the remainder of this paper we make the following
assumption about the apprentice’s policy.
Assumption 1. The apprentice policy πA is a deterministic policy
t (s) 6= a) ≤ ǫ for some ǫ > 0 and all time steps t.
(πA
Pr(s,a)∼DE
t
As we have shown, an apprentice policy satisfying Assumption 1 with small ǫ can be found with
high probability, provided that expert’s policy is well-approximated by a PAC-learnable hypothesis
class and that the apprentice is given enough trajectories from the expert. A reasonable intuition is
that the value of the policy πA in Assumption 1 is nearly as high as the value of the policy πE ; the
remainder of this paper is devoted to conﬁrming this intuiti on.

satis ﬁes

that

5 Guarantee for Any Expert

If the error rate ǫ in Assumption 1 is small, then the apprentice’s policy πA closely imitates the
expert’s policy πE , and we might hope that this implies that V (πA ) is not much less than V (πE ).
This is indeed the case, as the next theorem shows.
Theorem 1. If Assumption 1 holds, then V (πA ) ≥ V (πE ) − 2√ǫH 2Rmax .
In a typical classi ﬁcation problem, it is assumed that the tr aining and test examples are drawn from
the same distribution. The main challenge in proving Theorem 1 is that this assumption does not hold
for the classi ﬁcation problems to which we have reduced the a pprenticeship learning problem. This
t ) appearing in an expert trajectory is distributed
is because, although each state-action pair (si
t , ai
according to DE
t , a state-action pair (st , at ) visited by the apprentice’s policy may not follow this
distribution, since the behavior of the apprentice prior to time step t may not exactly match the
expert’s behavior. So our strategy for proving Theorem 1 will be to show that these differences do
not cause the value of the apprentice policy to degrade too much relative to the value of the expert’s
policy.

( ˆπt (s) 6= a) ≤ ǫ, then for

Before proceeding, we will show that Assumption 1 implies a condition that is, for our purposes,
more convenient.
Lemma 1. Let ˆπ be a deterministic nonstationary policy. If Pr(s,a)∼DE
t
t (s, ˆπt (s)) ≥ 1 − ǫ1 (cid:1) ≥ 1 − ǫ
all ǫ1 ∈ (0, 1] we have Prs∼DE
t (cid:0)πE
ǫ1
Proof. Fix any ǫ1 ∈ (0, 1], and suppose for contradiction that Prs∼DE
t (cid:0)πE
t (s, ˆπt (s)) ≥ 1 − ǫ1 (cid:1) <
t (s, ˆπt (s)) ≥ 1 − ǫ1 , and that s is bad otherwise. Then
. Say that a state s is good if πE
1 − ǫ
ǫ1
( ˆπt (s) = a | s is good)
(s is good) · Pr(s,a)∼DE
( ˆπt (s) = a) = Prs∼DE
Pr(s,a)∼DE
t
t
t
( ˆπt (s) = a | s is bad)
(s is bad) · Pr(s,a)∼DE
+ Prs∼DE
t
t
(s is good)) · (1 − ǫ1 )
(s is good) · 1 + (1 − Prs∼DE
≤ Prs∼DE
t
t
(s is good))
= 1 − ǫ1 (1 − Prs∼DE
t
< 1 − ǫ
( ˆπt (s) = a | s is bad) ≤ 1 − ǫ1 , and the second
where the ﬁrst inequality holds because Pr(s,a)∼DE
t
inequality holds because Prs∼DE
(s is good) < 1 − ǫ
. This chain of inequalities clearly contradicts
ǫ1
t
the assumption of the lemma.

The next two lemmas are the main tools used to prove Theorem 1. In the proofs of these lemmas, we
write sa to denote a trajectory, where sa = (¯s1 , ¯a1 , . . . , ¯sH , ¯aH ) ∈ (S × A)H . Also, let dPπ denote
the probability measure induced on trajectories by following policy π , and let R(sa) = PH
t=1 R(¯st )
4

R(sa)dPπ .

denote the sum of the rewards of the states in trajectory sa. Importantly, using these deﬁnitions we
have
V (π) = Zsa
The next lemma proves that if a deterministic policy “almost ” agrees with the expert’s policy πE in
every state and time step, then its value is not much worse the value of πE .
Lemma 2. Let ˆπ be a deterministic nonstationary policy. If for all states s and time steps t we have
t (s, ˆπt (s)) ≥ 1 − ǫ then V ( ˆπ) ≥ V (πE ) − ǫH 2Rmax .
πE
Proof. Say a trajectory sa is good if it is “consistent ” with
t — and that
sa is bad otherwise. We have
V (πE ) = Zsa
R(sa)dPπE
= Zsa good
R(sa)dPπE + Zsa bad
≤ Zsa good
R(sa)dPπE + ǫH 2Rmax
≤ Zsa good
= V ( ˆπ) + ǫH 2Rmax

ˆπ(¯st ) = ¯at for all time steps

R(sa)dP ˆπ + ǫH 2Rmax

ˆπ — that is,

R(sa)dPπE

where the ﬁrst inequality holds because, by the union bound, PπE assigns at most an ǫH fraction
of its measure to bad trajectories, and the maximum reward of a trajectory is H Rmax . The second
inequality holds because good trajectories are assigned at least as much measure by P ˆπ as by PπE ,
because ˆπ is deterministic.

t we have

The next lemma proves a slightly different statement than Lemma 2: If a policy exactly agrees with
the expert’s policy πE in “almost ” every state and time step, then its value is not mu ch worse the
value of πE .
Lemma 3. Let ˆπ be a nonstationary policy.
time steps
for all
If
t (s, ·)(cid:1) ≥ 1 − ǫ then V ( ˆπ) ≥ V (πE ) − ǫH 2Rmax .
t (cid:0) ˆπt (s, ·) = πE
Prs∼DE
Proof. Say a trajectory sa is good if πE
t (¯st , ·) = ˆπt (¯st , ·) for all time steps t, and that sa is bad
otherwise. We have
V ( ˆπ) = Zsa
R(sa)dP ˆπ
= Zsa good
R(sa)dP ˆπ + Zsa bad
R(sa)dP ˆπ
= Zsa good
R(sa)dPπE + Zsa bad
R(sa)dP ˆπ
R(sa)dPπE − Zsa bad
R(sa)dPπE + Zsa bad
= Zsa
≥ V (πE ) − ǫH 2Rmax + Zsa bad
R(sa)dP ˆπ
≥ V (πE ) − ǫH 2Rmax .
The ﬁrst inequality holds because, by the union bound, PπE assigns at most an ǫH fraction of
its measure to bad trajectories, and the maximum reward of a trajectory is H Rmax . The second
inequality holds by our assumption that all rewards are nonnegative.

R(sa)dP ˆπ

We are now ready to combine the previous lemmas and prove Theorem 1.

5

Proof of Theorem 1. Since the apprentice’s policy πA satis ﬁes Assumption 1, by Lemma 1 we can
choose any ǫ1 ∈ (0, 1] and have
t (s)) ≥ 1 − ǫ1 (cid:1) ≥ 1 − ǫ
t (cid:0)πE
t (s, πA
Prs∼DE
.
ǫ1
t (s, ·) for any
ˆπ as follows: For all time steps t, let ˆπt (s, ·) = πE
Now construct a “dummy” policy
state s where πE
t (s)) ≥ 1 − ǫ1 . On all other states, let ˆπt (s, πA
t (s)) = 1. By Lemma 2
t (s, πA
V (πA ) ≥ V ( ˆπ) − ǫ1H 2Rmax
ǫ
H 2Rmax .
V ( ˆπ) ≥ V (πE ) −
ǫ1
Combining these inequalities yields
V (πA ) ≥ V (πE ) − (cid:18)ǫ1 +
ǫ1 (cid:19) H 2Rmax .
ǫ
Since ǫ1 was chosen arbitrarily, we set ǫ1 = √ǫ, which maximizes this lower bound.

and by Lemma 3

6 Guarantee for Good Expert

Theorem 1 makes no assumptions about the value of the expert’s policy. However, in many cases it
may be reasonable to assume that the expert is following a near-optimal policy (indeed, if she is not,
then we should question the decision to select her as an expert). The next theorem shows that the
dependence of V (πA ) on the classi ﬁcation error ǫ is signi ﬁcantly better when the expert is following
a near-optimal policy.
Theorem 2. If Assumption 1 holds, then V (πA ) ≥ V (πE ) − (cid:0)4ǫH 3Rmax + ∆πE (cid:1), where ∆πE ,
V (π∗ ) − V (πE ) is the suboptimality of the expert’s policy πE .
Note that the bound in Theorem 2 varies with ǫ and not with √ǫ. We can interpret this bound as
follows: If our goal is to learn an apprentice policy whose value is within ∆πE of the expert policy’s
value, we can double our progress towards that goal by halving the classi ﬁcation error rate. On the
other hand, Theorem 2 suggests that the error rate must be reduced by a factor of four.

To see why a near-optimal expert policy should yield a weaker dependence on ǫ, consider an expert
policy πE that is an optimal policy, but in every state s ∈ S selects one of two actions as
1 and
2 uniformly at random. A deterministic apprentice policy πA that closely imitates the expert will
as
2 , but in either case the classi ﬁcation error will not be less t han
1 or πA (s) = as
either set πA (s) = as
1
2 . However, since πE is optimal, both actions as
1 and as
2 must be optimal actions for state s, and so
the apprentice policy πA will be optimal as well.

Our strategy for proving Theorem 2 is to replace Lemma 2 with a different result — namely, Lemma
on error ǫ when ∆πE is small.
6 below — that has a much weaker dependence on the classi ﬁcati
To help us prove Lemma 6, we will ﬁrst need to deﬁne several use
ful policies. The next several
deﬁnitions will be with respect to an arbitrary nonstationa ry base policy πB ; in the proof of Theorem
2, we will make a particular choice for the base policy.

Fix a deterministic nonstationary policy πB ,ǫ that satis ﬁes
t (s, πB ,ǫ
πB
(s)) ≥ 1 − ǫ
t
for some ǫ ∈ (0, 1] and all states s and time steps t. Such a policy always exists by letting ǫ = 1, but
if ǫ is close to zero, then πB ,ǫ is a deterministic policy that “almost ” agrees with
πB in every state
and time step. Of course, depending on the choice of πB , a policy πB ,ǫ may not exist for small ǫ,
but let us set aside that concern for the moment; in the proof of Theorem 2, the base policy πB will
be chosen so that ǫ can be as small as we like.
Having thus deﬁned πB ,ǫ , we deﬁne πB\ǫ as follows: For all states s ∈ S and time steps t, if
t (s, πB ,ǫ (s)) < 1, then let
πB
if πB ,ǫ
(s, a) = 
t


πB
t (s, a)
(s) πB
t (s, a′ )
Pa′ 6=πB ,ǫ
t

otherwise

πB\ǫ
t

(s) = a

0

6

for all actions a ∈ A, and otherwise let πB\ǫ
(s, a) = 1
|A| for all a ∈ A. In other words, in each
t
state s and time step t, the distribution πB\ǫ
(s, ·) is obtained by proportionally redistributing the
t
probability assigned to action πB ,ǫ
t (s, ·) to all other actions. The case
(s) by the distribution πB
t
t (s, ·) assigns all probability to action πB ,ǫ
(s) is treated specially, but as will be clear from
where πB
t
the proof of Lemma 4, it is actually immaterial how the distribution πB\ǫ
(s, ·) is deﬁned in these
t
cases; we choose the uniform distribution for deﬁniteness.

Let πB+ be a deterministic policy deﬁned by

πB+
t

t+1 (s′ ) (cid:12)(cid:12)(cid:12)
s′ ∼ θ(s, a, ·)i
E hV πB
(s) = arg max
a
for all states s ∈ S and time steps t. In other words, πB+
(s) is the best action in state s at time t,
t
assuming that the policy πB is followed thereafter.
The next deﬁnition requires the use of mixed policies. A mixed policy consists of a ﬁnite set of
deterministic nonstationary policies, along with a distribution over those policies; the mixed policy
is followed by drawing a single policy according to the distribution in the initial time step, and
following that policy exclusively thereafter. More formally, a mixed policy is deﬁned by a set of
ordered pairs {(π i , λ(i))}N
i=1 for some ﬁnite N , where each component policy π i is a deterministic
nonstationary policy, PN
i=1 λ(i) = 1 and λ(i) ≥ 0 for all i ∈ [N ].
We deﬁne a mixed policy ˜πB ,ǫ,+ as follows: For each component policy π i and each time step t,
t = πB ,ǫ
t = πB+
either π i
or π i
. There is one component policy for each possible choice; this yields
t
t
N = 2|H | component policies. And the probability λ(i) assigned to each component policy π i is
t = πB ,ǫ
λ(i) = (1 − ǫ)k(i) ǫH−k(i) , where k(i) is the number of times steps t for which π i
.
t
Having established these deﬁnitions, we are now ready to pro ve several lemmas that will help us
prove Theorem 2.
Lemma 4. V ( ˜πB ,ǫ,+ ) ≥ V (πB ).
Proof. The proof will be by backwards induction on t. Clearly V ˜πB ,ǫ,+
(s) = V πB
H (s) for all states
H
H for any policy π depends only on the reward function R. Now suppose
s, since the value function V π
(s) ≥ V πB
for induction that V ˜πB ,ǫ,+
t+1 (s) for all states s. Then for all states s
t+1
(s′ ) (cid:12)(cid:12)(cid:12)
(s) = R(s) + E hV ˜πB ,ǫ,+
(s, ·), s′ ∼ θ(s, a′ , ·)i
a′ ∼ ˜πB ,ǫ,+
t+1
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
≥ R(s) + E hV πB
(s, ·), s′ ∼ θ(s, a′ , ·)i
a′ ∼ ˜πB ,ǫ,+
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
= R(s) + (1 − ǫ)E hV πB
(s), ·)i + ǫE hV πB
s′ ∼ θ(s, πB ,ǫ
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
(s)) · E hV πB
(s), ·)i
t (s, πB ,ǫ
s′ ∼ θ(s, πB ,ǫ
≥ R(s) + πB
t
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
+ (cid:16)1 − πB
(s))(cid:17) · E hV πB
(s), ·)i
t (s, πB ,ǫ
s′ ∼ θ(s, πB+
t
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
(s)) · E hV πB
(s), ·)i
t (s, πB ,ǫ
s′ ∼ θ(s, πB ,ǫ
≥ R(s) + πB
t
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
+ (cid:16)1 − πB
(s))(cid:17) · E hV πB
(s, ·), s′ ∼ θ(s, a′ , ·)i
a′ ∼ πB\ǫ
t (s, πB ,ǫ
t
t
t+1 (s′ ) (cid:12)(cid:12)(cid:12)
= R(s) + E hV πB
t (s), s′ ∼ θ(s, a′ , ·)i
a′ ∼ πB
= V πB
(s).
t
The ﬁrst equality holds for all policies π , and follows straightforwardly from the deﬁnition of V π
t .
The rest of the derivation uses, in order: the inductive hypothesis; the deﬁnition of ˜πB ,ǫ,+ ; property
(s) is the best action with respect to V πB
t+1 ; the fact that πB+
of πB ,ǫ and the fact that πB+
(s) is the
t
t
best action with respect to V πB
t+1 ; the deﬁnition of πB\ǫ ; the deﬁnition of V πB
(s).
t
Lemma 5. V ( ˜πB ,ǫ,+ ) ≤ (1 − ǫH )V (πB ,ǫ ) + ǫH V (π∗ ).

s′ ∼ θ(s, πB+
t

V ˜πB ,ǫ,+
t

(s), ·)i

7

λ(i)V (π i )

λ(i)V (π i )

V ( ˜πB ,ǫ,+ ) =

Proof. Since ˜πB ,ǫ,+ is a mixed policy, by the linearity of expectation we have
N
Xi=1
where each π i is a component policy of ˜πB ,ǫ,+ and λ(i) is its associated probability. Therefore
V ( ˜πB ,ǫ,+ ) = Xi
≤ (1 − ǫ)H V (πB ,ǫ ) + (1 − (1 − ǫ)H )V (π∗ )
≤ (1 − ǫH )V (πB ,ǫ ) + ǫH V (π∗ ).
Here we used the fact that probability (1 − ǫ)H ≥ 1 − ǫH is assigned to a component policy that is
identical to πB ,ǫ , and the value of any component policy is at most V (π∗ ).
Lemma 6. If ǫ < 1
H , then V (πB ,ǫ ) ≥ V (πB ) − ǫH
1−ǫH ∆πB .
Proof. Combining Lemmas 4 and 5 yields
(1 − ǫH )V (πB ,ǫ ) + ǫH V (π∗ ) ≥ V (πB ).
And via algebraic manipulation we have
(1 − ǫH )V (πB ,ǫ ) + ǫH V (π∗ ) ≥ V (πB )
⇒ (1 − ǫH )V (πB ,ǫ ) ≥ (1 − ǫH )V (πB ) + ǫH V (πB ) − ǫH V (π∗ )
⇒ (1 − ǫH )V (πB ,ǫ ) ≥ (1 − ǫH )V (πB ) − ǫH∆πB
ǫH
⇒ V (πB ,ǫ ) ≥ V (πB ) −
∆πB .
1 − ǫH
In the last line, we were able to divide by (1 − ǫH ) without changing the direction of the inequality
because of our assumption that ǫ < 1
H .

We are now ready to combine the previous lemmas and prove Theorem 2.

.

H 2Rmax .

Proof of Theorem 2. Since the apprentice’s policy πA satis ﬁes Assumption 1, by Lemma 1 we can
choose any ǫ1 ∈ (0, 1
H ) and have
t (s)) ≥ 1 − ǫ1 (cid:1) ≥ 1 − ǫ
t (s, πA
t (cid:0)πE
Prs∼DE
ǫ1
As in the proof of Theorem 1, let us construct a “dummy” policy
ˆπ as follows: For all time steps
t, let ˆπt (s, ·) = πE
t (s, ·) for any state s where πE
t (s)) ≥ 1 − ǫ1 . On all other states, let
t (s, πA
t (s)) = 1. By Lemma 3 we have
ˆπt (s, πA
ǫ
V ( ˆπ) ≥ V (πE ) −
ǫ1
Substituting V (πE ) = V (π∗ ) − ∆πE and V ( ˆπ) = V (π∗ ) − ∆ ˆπ and rearranging yields
ǫ
H 2Rmax .
∆ ˆπ ≤ ∆πE +
ǫ1
Now observe that, if we set the base policy πB = ˆπ , then by deﬁnition πA is a valid choice for
πB ,ǫ1 . And since ǫ1 < 1
H we have
V (πA ) ≥ V ( ˆπ) −

ǫ1H
∆ ˆπ
1 − ǫ1H
1 − ǫ1H (cid:18)∆πE +
H 2Rmax(cid:19)
ǫ
ǫ1H
≥ V ( ˆπ) −
ǫ1
1 − ǫ1H (cid:18)∆πE +
H 2Rmax(cid:19)
ǫ
ǫ
ǫ1H
H 2Rmax −
≥ V (πE ) −
ǫ1
ǫ1
where we used Lemma 6, (2) and (1), in that order. Letting ǫ1 = 1
2H proves the theorem.

(1)

(2)

(3)

8

References
[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the 21st International Conference on Machine Learning, 2004.
[2] P Abbeel and A Y Ng. Exploration and apprenticeship learning in reinforcement learning. In
Proceedings of the 22nd International Conference on Machine Learning, 2005.
[3] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning.
In Proceedings of the 23rd International Conference on Machine Learning, 2006.
[4] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In
Advances in Neural Information Processing Systems 20, 2008.
[5] J. Zico Kolter, Pieter Abbeel, and Andrew Ng. Hierarchical apprenticeship learning with ap-
plication to quadruped locomotion. In Advances in Neural Information Processing Systems 20,
2008.
[6] Umar Syed and Robert E. Schapire. Apprenticeship learning using linear programming. In
Proceedings of the 25th International Conference on Machine Learning, 2008.
[7] Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot
learning from demonstration. Robotics and Autonomous Systems, 57(5):469–483, 2009.
[8] St ´ephane Ross and J. Andrew Bagnell. Efﬁcient reduction for im itation learning. In AISTATS,
2010.
[9] Bianca Zadrozny, John Langford, and Naoki Abe.
Cost-sensitive learning by cost-
proportionate example weighting. In Proceedings of the Third IEEE International Conference
on Data Mining, 2003.
[10] J. Andrew Bagnell, Sham Kakade, Andrew Y. Ng, and Jeff Schneider. Policy search by dy-
namic programming. In Advances in Neural Information Processing Systems 15, 2003.
[11] John Langford and Bianca Zadrozny. Relating reinforcement learning performance to classi ﬁ-
cation performance. In Proceedings of the 22nd International Conference on Machine Learn-
ing, 2005.
[12] Doron Blatt and Alfred Hero. From weighted classi ﬁcati on to policy search. In Advances in
Neural Information Processing Systems 18, pages 139–146, 2006.
[13] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learn-
ing. In Proceedings 19th International Conference on Machine Learning, 2002.
[14] V. N. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and Its Applications, 16:264–280, 1971.

9


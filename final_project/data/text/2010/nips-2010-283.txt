Distributionally Robust Markov Decision Processes

Huan Xu
ECE, University of Texas at Austin
huan.xu@mail.utexas.edu

Shie Mannor
Department of Electrical Engineering, Technion, Israel
shie@ee.technion.ac.il

Abstract

We consider Markov decision processes where the values of the parameters are
uncertain. This uncertainty is described by a sequence of nested sets (that is,
each set contains the previous one), each of which corresponds to a probabilistic
guarantee for a different con ﬁdence level so that a set of adm issible probability
distributions of the unknown parameters is speciﬁed. This f ormulation models the
case where the decision maker is aware of and wants to exploit some (yet impre-
cise) a-priori information of the distribution of parameters, and arises naturally in
practice where methods to estimate the con ﬁdence region of p arameters abound.
We propose a decision criterion based on distributional robustness: the optimal
policy maximizes the expected total reward under the most adversarial probability
distribution over realizations of the uncertain parameters that is admissible (i.e.,
it agrees with the a-priori information). We show that ﬁndin g the optimal dis-
tributionally robust policy can be reduced to a standard robust MDP where the
parameters belong to a single uncertainty set, hence it can be computed in poly-
nomial time under mild technical conditions.

1 Introduction

Sequential decision making in stochastic dynamic environments, also called the “planning prob-
lem,” is often modeled using a Markov Decision Process (MDP, cf [1, 2, 3]). In practice, parameter
uncertainty – the deviation of the model parameters from the true ones (re ward r and transition prob-
ability p) – often causes the performance of “optimal” policies to deg
rade signiﬁcantly [4]. Many
efforts have been made to reduce such performance variation under the robust MDP framework
(e.g., [5, 6, 7, 8, 9, 10]). In this context, it is assumed that the parameters can be any member of a
known set (termed the uncertainty set), and solutions are ranked based on their performance under
the (respective) worst parameter realizations.

In this paper we extend the robust MDP framework to deal with probabilistic information on uncer-
tain parameters. To motivate the problem, let us consider the following example. Suppose that an
agent (car, plane, robot etc) wants to ﬁnd a fastest path from the source location to the destination. If
the passing time to area A is uncertain and can be very large, then the solution to robust MDP would
tend to take a detour and avoid A. However, if it is further known that the passing time can be large
only when some unusual event (whose chance is less than, say, 10%), such as a storm, happens, and
otherwise the passing time is reasonable, then avoiding A may be overly pessimistic. The statement
“the probability of the (uncertain) passing time being larg e is at most 10%” is important, and should
be incorporated into the decision making paradigm. Indeed, it was observed that since the robust
MDP framework ignores probabilistic information, it can provide conservative solutions [11, 12].

A different approach to embeding prior information is by adopting a Bayesian perspective on the
parameters of the problem; see [11] and references therein. However, a complete Bayesian prior
to the model parameters may be difﬁcult to conjure as the deci sion maker may not have a reliable
generative model to the uncertainty. For example, in the path planning problem above, the decision
maker may not know how to assign probabilities to the model dynamics when a storm occurs. Our

1

approach offers a middle ground between the fully Bayesian approach and the robust approach:
we want the decision maker to be able to use prior information but we do not require a complete
Bayesian interpretation.

We adapt the distributionally robust approach to MDPs under parameter uncertainty. The distri-
butionally robust formulation has been extensively studied and broadly applied in single stage op-
timization problems to effectively incorporates a-priori probabilistic information of the unknown
parameters (e.g., [13, 14, 15, 16, 17, 18]). In this framework, the uncertain parameters are regarded
as stochastic, with a distribution µ that is not precisely observed, yet assumed to belong to an a-priori
known set C . The objective is then formulated based on the worst-case analysis over distributions
in C . That is, given a utility function u(x, ξ ) where x ∈ X is the optimizing variable and ξ is the
unknown parameter, distributionally robust optimization solves maxx∈X (cid:2) inf µ∈C Eξ∼µu(x, ξ )(cid:3). In-
deed, such approach has also been developed in the mathematical ﬁnance community, usually in the
static setup [19, 20]. Here the goal is to optimize a so-called coherent risk measure, which is shown
to be equivalent to a distributionally robust formulation.

From a decision theory perspective, the distributionally robust approach coincides with the cele-
brated MaxMin Expected Utility framework [21, 22], which states that if a preference relationship
among actions satisﬁes certain axioms, then the optimal act ion maximizes the minimal expected util-
ity with respect to a class of distributions. This approach addresses the famous neglect of probability
cognitive bias [23], i.e., the tendency to completely disregard probability when making a decision
under uncertainty. Two extreme cases of such biases are the normalcy bias, which roughly speak-
ing, can be states as “since a disaster has never occurred the n it never will occur,” and the zero-risk
bias, which stands for the tendency of individuals to prefer small bene ﬁts that are certain to large
ones that are uncertain, regardless of the size of the “certa in ” bene ﬁt and the expected magnitude of
the uncertain one. It is easy to see that the nominal approach and the robust approach suffers from
normalcy bias and zero-risk bias, respectively.

We formulate and solve the distributionally robust MDP with respect to the nested uncertainty set.
The nesting structure implies that there are n different levels of estimation, that is, C 1
s ,
s ⊆ C 2
s ⊆ · · · C n
representing the possible parameters of the problem. The probability that the parameters of state
s belong to C i
s is at least λi . We also require the parameters to be state-wise independent (i.e.,
the uncertainty set is a product set over states). Policies are then ranked based on their expected
performance under the (respective) most adversarial distribution. The main contribution of this paper
is showing that for both the ﬁnite horizon case and the discou nted reward in ﬁnite horizon case, such
optimal policy satisﬁes a Bellman type equation, and can be s olved via backward iteration.
Motivating example. The nested-set formulation is motivated by the “multi-scen ario ” setup, where
in different scenarios the parameters are subject to different levels of uncertainty. For instance, in
the path planning example, the uncertainty of the passing time of A can be modeled as a nested-set
with two uncertainty sets: the parameters with at least 90% belong to a small uncertainty set corre-
sponding to “no storm,” and guaranteed to belong to a large wo rst-case uncertainty set representing
“storm ” with probability of at most
10%. In fact the multi-layer formulations allows the decision
maker to handle more than two scenarios. For example, a plane can encounter scenarios such as
“normal,”
“storm,”
“big storm,” and even “volcano ashes,” e
ach corresponding to a different level of
parameter uncertainty. One appealing advantage of the nested-set formulation is that it does not re-
quire a precise description of the uncertainty, which leads to considerable ﬂexibility. For example, if
the uncertainty set of a robust MDP is not precisely known, then one can instead solve distribution-
ally robust MDP with a 2-set formulation where the inner and the outer sets represent, respectively,
an “optimistic ” estimation and a “conservative ” estimatio
n. Additionally, the nested-set formula-
tion also results from estimating the distributions of parameters via sampling. Such estimation is
often imprecise especially when only a small number of samples is available. Instead, estimating
uncertainty sets with high con ﬁdence can be made more accura te, and one can easily sharpen the
approximation by incorporating more layers of con ﬁdence se ts (i.e, increase n).

2 Preliminaries and Problem Setup

A ( ﬁnite) MDP is de ﬁned as a 6-tuple < T , γ , S, As , p, r > where: T is the possibly in ﬁnite decision
horizon; γ ∈ (0, 1] is the discount factor; S is the ﬁnite state set; As is the ﬁnite action set of state s;
p is the transition probability; and r is the expected reward. That is, for s ∈ S and a ∈ As , r(s, a)

2

is the expected reward and p(s′ |s, a) is the probability to reach state s′ . Following Puterman [1], we
denote the set of all history-dependent randomized policies by ΠHR , and the set of all Markovian
randomized policies by ΠM R . We use subscript s to denote the value associated with state s, e.g., rs
denotes the vector form of rewards associated with state s, and πs is the (randomized) action chosen
at state s for policy π . The elements in vector ps are listed in the following way: the transition
probabilities of the same action are arranged in the same block, and inside each block they are
listed according to the order of the next state. We use s to denote the (random) state following s,
and ∆(s) to denote the probability simplex on As . We use N to represent cartesian product, e.g.,
p = Ns∈S ps . For a policy π , we denote the expected (discounted) total-reward under parameters
p, r by u(π , p, r), that is,
T
Xi=1
γ i−1 r(si , ai )}.
u(π , p, r) , Ep
π {
In this paper we propose and solve distributionally robust policy under parameter uncertainty, which
incorporates a-prior information of how parameters are distributed. Suppose it is known that p
and r follows some unknown distribution µ that belongs to a set CS . We evaluate each policy
by its expected performance under the (respective) most adversarial distribution of the uncertain
parameters, and a distributionally robust policy is the optimal policy according to this measure.
De ﬁnition 1. A policy π∗ ∈ πHR is distributionally robust with respect to CS if it satisﬁes that for
all π ∈ ΠHR ,
µ∈CS Z u(π , p, r) dµ(p, r) ≤ inf
µ′ ∈CS Z u(π∗ , p, r) dµ′ (p, r).
inf
Next we specify the set of admissible distributions of uncertain parameters CS investigated in this
paper. Let 0 = λ0 ≤ λ1 ≤ λ2 ≤ · · · ≤ λn = 1, and P 1
s for s ∈ S . We use the
s ⊆ P 2
s ⊆ · · · ⊆ P n
following set of distributions CS for our model.
CS , {µ|µ = Os∈S
where: Cs , {µs |µs (P n
s ) = 1; µs (P i
s ) ≥ λi , i = 1, · · · , n − 1}.

µs ; µs ∈ Cs , ∀s ∈ S },

(1)

We brie ﬂy explain this set of distributions. For a state s, the condition µs (P n
s ) = 1 means that
the unknown parameters (ps , rs ) are restricted to the outermost uncertainty set; and the condition
s ) ≥ λi means that with probability at least λi , (ps , rs ) ∈ P i
s . Thus, P 1
s provides
µs (P i
s , · · · , P n
probabilistic guarantees of (ps , rs ) for n different uncertainty sets (or equivalently con ﬁdence lev -
els). Note that Ns∈S µs stands for the product measure generated by µs , which indicates that the
parameters among different states are independent. Throughout this paper we make a standard as-
sumption (cf [5, 6, 8]) that P i
s is nonempty, convex and compact.

3 Distributionally robust MDPs: The ﬁnite-horizon case.

In this section we show how to solve distributionally robust policies to MDPs having ﬁnitely many
decision stages. We assume that when a state is visited multiple times, each time it can take a
different parameter realization (non-stationary model). Equivalently, this means that multiple visits
to a state can be treated as visiting different states, which leads to the Assumption 1 without loss
of generality (by adding dummy states). Thus, we can partition S according to the stage each state
belongs to, and let St be the set of states belong to tth stage. The non-stationary model is proposed
in [5] because the stationary model is generally intractable and a lower-bound on it is given by the
non-stationary model.
Assumption 1. (i) Each state belongs to only one stage; (ii) the terminal reward equals zero; and
(iii) the ﬁrst stage only contains one state sini .

We next de ﬁne sequentially robust policies through a backward induction as a policy that is robust
in every step for a standard robust MDP. We will later shows that sequentially robust policies are
also distributionally robust by choosing the uncertainty set of the robust MDP carefully.
De ﬁnition 2. Let T < ∞ and let Ps be the uncertainty set of state s. De ﬁne the following:

3

1. For s ∈ ST , the sequentially robust value ˜vT (s) , 0.

2. For s ∈ St where t < T , the sequentially robust value ˜vt (s) and sequentially robust action
˜πs are de ﬁned as

πs∈∆(s) n min
πs [r(s, a) + γ ˜vt+1 (s)]o.
˜vt (s) , max
Eps
(ps ,rs )∈Ps
πs [r(s, a) + γ ˜vt+1 (s)]o.
πs∈∆(s) n min
˜πs ∈ arg max
Eps
(ps ,rs )∈Ps
3. A policy ˜π∗ is a sequentially robust policy w.r.t. Ps if ∀s ∈ S , ˜π∗
s is a sequentially robust
action.

A standard game theoretic argument implies that sequentially robust actions, and hence sequentially
robust policies, exist. Indeed, from the literature in robust MDP (cf [5, 7, 8]) it is easy to see that a
sequentially robust policy is the solution to the robust MDP where the uncertainty set is Ns Ps . The
following theorem, which is the main result of this paper, shows that any sequentially robust policy
(w.r.t. a speciﬁc uncertainty set) π∗ is distributionally robust.
Theorem 1. Let T < ∞. Let Assumption 1 hold, and suppose that π∗ is a sequentially robust
ˆPs , where
policy w.r.t. Ns

(λi − λi−1 )(rs (i), ps (i))|(ps (i), rs (i)) ∈ P i
s}.

ˆPs = {

Then

n
Xi=1

1. π∗ is a distributionally robust policy with respect to Cs ; and

2. there exists µ∗ ∈ Cs such that (π∗ , µ∗ ) is a saddle point. That is,
π∈ΠHR Z u(π , p, r) dµ∗ (p, r) = Z u(π∗ , p, r) dµ∗ (p, r) = inf
µ∈CS Z u(π∗ , p, r) dµ(p, r).
sup
Therefore, to ﬁnd the sequentially robust policy, we need on ly to solve the sequentially robust action.
Theorem 2. Denote λ0 = 0. For s ∈ St where t < T , the sequentially robust action is given by
n
s )⊤ ˜Vsq(cid:3)o,
q∈∆(s) n
Xi=1
s )⊤
∗ = arg max
i
i
s (cid:2)(r
q + (p
(λi − λi−1 ) min
q
s )∈P i
i
i
(p
s ,r
where m = |As |, ˜vt+1 is the vector form of ˜vt+1 (s′ ) for all s′ ∈ St+1 , and

˜Vs , 


Theorem 2 implies that the computation of the sequentially robust action at a state s critically de-
pends on the structure of the sets P i
s . In fact, it can be shown that for “good ” uncertainty sets,
computing the sequentially robust action is tractable. This claim is made precise by the following
corollary. We omit the proof that is standard.
Corollary 1. The sequentially robust action for state s can be found in polynomial-time, if for each
s has a polynomial separation oracle. Here, a polynomial separation oracle of a
i = 1, · · · , n, P i
convex set H ⊆ Rn is a subroutine that given x ∈ Rn , reports in polynomial time whether x ∈ H,
and if the answer is negative, it ﬁnds a hyperplane that separ ates x and H.

⊤
˜vt+1 e
1 (m)
:
⊤
˜vt+1 e
m(m)

(2)

.

3.1 Proof of Theorem 1

We prove Theorem 1 in this section. The outline of the proof is as follows: We ﬁrst show that for a
given policy, the expected performance under an admissible µ depends only on the expected value
ˆPs . Thus the
of the parameters. Then we show that the set of expected parameters is indeed Ns∈S
4

ˆPs being the uncertainty set.
distributionally robust MDP reduces to the robust MDP with Ns∈S
Finally, by applying results from robust MDPs we prove the theorem. Some of the intermediate
results are stated with proof omitted due to space constraints.
Let ht denote a history up to stage t and s(ht ) denote the last state of history ht . We use πht (a)
to represent the probability of choosing an action a at state s(ht ), following a policy π and under a
history ht . A t + 1 stage history, with ht followed by action a and state s′ is written as (ht , a, s′ ).
With an abuse of notation, we denote the expected reward-to-go under a history as:

u(π , p, r, ht ) , Ep
π {

γ i−t r(si , ai )|(s1 , a1 · · · , st ) = ht}.

T
Xi=t
For π ∈ ΠHR and µ ∈ CS (λ), de ﬁne w(π , µ, ht ) , E(p,r)∼µus (π , p, r, h(t)) =
R u(π , p, r, h(t))dµ(p, r). Thus, w(π , µ, (sini )) = R u(π , p, r) dµ(p, r) is the minimax objec-
tive. One can show that the following recursion formula for w(·) holds, due to the fact that
µ(p, r) = Ns∈S µs (ps , rs ).
Lemma 1. Fix π ∈ ΠHR , µ ∈ CS and a history ht where t < T , denote r = Eµ (r), p = Eµ (p),
then we have:
w(π , µ, ht ) = Z Xa∈As(ht )
γ p(cid:0)s′ |s(ht ), a(cid:1)w(cid:0)π , µ, (ht , a, s′)(cid:1)(cid:17)dµs(ht ) (ps(ht ) , rs(ht ) )
πht (a)(cid:16)r(cid:0)s(ht ), a(cid:1) + Xs′ ∈S
γ p(cid:0)s′ |s(ht ), a(cid:1)w(cid:0)π , µ, (ht , a, s′ )(cid:1)(cid:17).
πht (a)(cid:16)r(cid:0)s(ht ), a(cid:1) + Xs′ ∈S
= Xa∈As(ht )

From Lemma 1, by backward induction, one can show the following lemma holds, which essentially
means that for any policy, the expected performance under an admissible distribution µ only depends
on the expected value of the parameters under µ. Thus, the distributionally robust MDP reduces to
a robust MDP.
Lemma 2. Fix π ∈ ΠHR and µ ∈ CS , denote p = Eµ (p) and r = Eµ (r). We have:
w(cid:0)π , µ, (sini )(cid:1) = u(π , p, r).
Next we characterize the set of expected value of the parameters.
Lemma 3. Fix s ∈ S , we have {Eµs (ps , rs )|µs ∈ Cs} = ˆPs .
ˆPs . We complete the proof of the
Note that Lemma 3 implies that {Eµ (p, r)|µ ∈ CS } = Ns∈S
Theorem 1 using the equivalence of distributionally robust MDPs and robust MDPs where the un-
ˆPs . Recall that for each s ∈ S , ˆPs is convex and compact. It is well known
certainty set is Ns∈S
that for robust MDPs, a saddle point of the minimax objective exists (cf [5, 8]). More precisely,
ˆPs such that
there exists π∗ ∈ ΠHR , (p
∗ ) ∈ Ns∈S
∗ , r
∗ , r
∗ ) = u(π∗ , p
∗ , r
∗ ) =
u(π , p

inf
ˆPs
(r,p)∈Ns∈S
∗ ) can be constructed state-wise: π∗ = Ns∈S π∗
s and (p
Moreover, π∗ and (p
∗ , r
∗ , r
∗ ) =
s ), and for each s ∈ St , π∗
s ) solves the following zero-sum game
∗
∗
∗
∗
Ns∈S (p
s , r
s , (p
s , r
π (cid:0)r(s, a) + γ ˜vt+1 (s)(cid:1).
min
max
Eps
πs
(ps ,rs )∈ ˆPs
It follows that π∗
s is any sequentially robust action, and hence π∗ can be any sequentially robust
policy. From Lemma 3, there exists µ∗
s ∈ Cs that satisﬁes Eµ∗
s ). Let µ∗ =
∗
∗
s (ps , rs ) = (p
s , r
s . By Lemma 2 we have
Ns∈S µ∗

u(π∗ , p, r).

sup
π∈ΠHR

w(cid:0)π , µ∗ , (sini )(cid:1) = sup
∗ , r
∗ );
u(π , p
sup
π∈ΠHR
π∈ΠHR
∗ (cid:1);
w(cid:0)π∗ , µ∗ , (sini )(cid:1) = u(cid:0)π∗ , p
∗ , r
w(cid:0)π∗ , µ, (sini )(cid:1) =
inf
inf
µ∈CS
ˆPs
(p,r)∈Ns

u(π∗ , p, r).

5

This leads to supπ∈ΠHR w(cid:0)π , µ∗ , (sini )(cid:1) = w(cid:0)π∗ , µ∗ , (sini )(cid:1) = inf µ∈CS w(cid:0)π∗ , µ, (sini )(cid:1). Thus,
part (ii) of Theorem 1 holds. Note that part (ii) immediately implies part (i) of Theorem 1.
Remark: Lemma 1 holds for a broader class of distribution sets than we discussed here. Indeed,
the only requirement of C for Lemma 1 to hold is the state-wise decomposibility. Therefore, the
results presented in this paper may well extend to distributionally robust MDPs whose parameters
belongs to other interesting sets of distributions, such as a set of parametric distribution (Gaussian,
exponential, binomial etc) with the distribution parameter not precisely determined.

4 Distributionally robust MDP: The discounted reward inﬁni te horizon case.

In this section we show how to compute a distributionally robust policy for in ﬁnite horizon MDPs.
Speciﬁcally, we generalize the notion of sequentially robu st policies to discounted-reward in ﬁnite-
horizon MDPs, and show that it is distributionally robust in an appropriate sense.
De ﬁnition 3. Let T = ∞ and γ < 1. Denote the uncertainty set by ˆP = Ns
following:
1. The sequentially robust value ˜v∞ (s) w.r.t. ˆPs is the unique solution to the following set of
equations:

ˆPs . We de ﬁne the

πs ∈∆(s) n min
πs [r(s, a) + γ ˜v∞ (s)]o, ∀s ∈ S.
˜v∞ (s) = max
Eps
(ps ,rs )∈ ˆPs
2. The sequentially robust action w.r.t. ˆPs , ˜πs , is given by
πs [r(s, a) + γ ˜v∞ (s)]o.
πs∈∆(s) n min
˜πs ∈ arg max
Eps
(ps ,rs )∈ ˆPs
3. A policy ˜π∗ is a sequentially robust policy w.r.t. ˆPs if ∀s ∈ S , ˜π∗
s is a sequentially robust
action.

min
(p,r)∈ ˆPs

q(a)p(s′ |s, a)v(s′ )].

The sequentially robust policy is well de ﬁned, since the fol lowing operator L : R|S | → R|S | is a γ
contraction for k · k∞ norm.
{Lv}(s) , max
q∈∆(s)

q(a)r(s, a) + γ Xa∈As Xs′ ∈S
[ Xa∈As
Furthermore, given any v, applying L is equivalent to solving a minimax problem, which by Theo-
rem 2 can be efﬁciently computed. Hence, by applying L on any initial v
0 ∈ R|S | repeatedly, the
resulting value vector will converge to the sequentially robust value ˜v exponentially fast.
Note that in the in ﬁnite horizon case, we cannot model the sys tem as (1) having ﬁnitely many states,
and (2) each visited at most once. In contrast, we have to relax either one of these two assumptions,
leading to two different natural formulations. The ﬁrst for mulation, termed non-stationary model,
is to treat the system as having in ﬁnitely many states, each v isited at most once. Therefore, we
consider an equivalent MDP with an augmented state space, where each augmented state is de ﬁned
by a pair (s, t) where s ∈ S and t, meaning state s in the tth horizon. Thus, each augmented state
will be visited at most once, which leads to the following set of distributions.
¯C∞
, {µ|µ = Os∈S,t=1,2,···
S
The second formulation, termed stationary model, treats the system as having a ﬁnite number of
states, while multiple visits to one state is allowed. That is, if a state s is visited for multiple times,
then each time the distribution (of uncertain parameters) µs is the same. Mathematically, we can
adapt the augmented state space as in the non-stationary model, and requires that µs,t does not
depend on t. Thus, the set of admissible distributions is
¯CS , {µ|µ = Os∈S,t=1,2,···
The next theorem is the main result of this section; it shows that a sequentially robust policy is
distributionally robust to both stationary and non-stationary models.

µs,t ; µs,t = µs ; µs ∈ Cs , ∀s ∈ S, ∀t = 1, 2, · · · }.

µs,t ; µs,t ∈ Cs , ∀s ∈ S, ∀t = 1, 2, · · · }.

6

ˆPs where ˆPs =
Theorem 3. Given T = ∞ and γ < 1, any sequentially robust policy w.r.t. Ns
{Pn
s}, is distributionally robust with respect to ¯C∞
S ,
i=1 (λi − λi−1 )(rs (i), ps (i))|(ps (i), rs (i)) ∈ P i
and with respect to ¯CS .
Due to space constraints, we omit the proof details. The basic idea for proving the ¯C∞
S case is to
consider a ˆT -truncated problem, i.e., a ﬁnite horizon problem that stop s at stage ˆT with a termi-
nation reward ˜v∞ (·), and show that the optimal strategy for this problem, which is a sequential
robust strategy, coincides with that of the in ﬁnite horizon one. Indeed, given any sequential robust
strategy π∗ , one can construct a stationary distribution µ∗ such that (π∗ , µ∗ ) is a saddle point for
S R u(π , p, r) dµ(p, r). The proof to ¯CS follows from ¯CS ⊂ ¯C∞
S and µ∗ ∈ ¯CS .
supπ∈ΠHR inf µ∈ ¯C∞
We remark that the decision maker is allowed to take non-stationary strategies, although the distri-
butionally robust solution is proven to be stationary.

Before concluding this section, we brie ﬂy compare the stati onary model and the non-stationary
model. These two formulations model different setups: if the system, more speciﬁcally the distribu-
tion of uncertain parameters, evolves with time, then the non-stationary model is more appropriate;
while if the system is static, then the stationary model is preferable. For any given policy, the worst
expected performance under the non-stationary model provides a lower bound to that of the station-
ary model since ¯CS ⊆ ¯C∞
S . Thus, one can use the non-stationary model to approximate the stationary
model, when the latter is intractable (e.g., in the ﬁnite hor izon case; see Nilim and El Ghaoui [5]).
When the horizon approaches in ﬁnity, such approximation be comes exact, as we showed in this sec-
tion, the optimal solutions to both formulations coincide, and can be computed by iteratively solving
a minimax problem.

5 Numerical simulations

In this section we illustrate with numerical examples that by incorporating additional probabilistic
information, the distributional robust approach handles uncertainty in a more ﬂexible way, which
often leads to a better performance than the nominal approach and the robust approach.

We consider a path planning problem: an agent wants to exit a 4 × 21 maze (shown in Figure 1)
using the least possible time. Starting from the upper-left corner, the agent can move up, down, left
and right, but can only exit the grid at the lower-right corner. Here, a white box stands for a normal
place where the agent needs one time unit to pass through. A shaded box represents a “shaky ” place.
To be more speciﬁc, we consider two setups. The ﬁrst one is the
uncertain cost case, where the
true (yet unknown to the planning agent) time for the agent to pass through a “shaky ” place equals
x = 1 + ˜e(λ), and ˜e(λ) is an exponential distributed random variable with parameter λ. The three
approaches are formulated as follows: the nominal approach takes the most likely value (i.e., 1) as
the parameter; the robust approach takes [1, 1 + 3/λ] as the uncertainty set; and the distributional
robust approach takes into account the additional information that Pr(x ∈ [1, 1 + log 2/λ]) ≥ 0.5
and Pr(x ∈ [1, 1 + 2 log 2/λ]) ≥ 0.75. We vary 1/λ, and test these approaches using 300 runs for
each parameter set. The results are reported in Figure 2 (a).

Figure 1: The maze for the path planning problem.

The second case is the uncertain transition case:
if an agent reaches a “shaky ” place, then the
transition becomes unpredictable – in the next step with pro bability 20% it will make an (unknown)

7

jump. The three approaches are set as follows: The nominal approach neglects this random jump.
The robust approach takes a worst-case analysis, i.e., it assumes that with 20% the agent will jump
to the spot with the highest cost-to-go. The distributionally robust approach takes into account an
additional information that if a jump happens, the probability that it jumps to a spot that is left to
the current place is no more than γ . Each policy is tested over 300 runs, while the true jump is set
as with probability 0.2γ the agent returns to the starting point ( “reboot”), with
0.2(1 − γ ) the agent
stay in the current position for a time unit ( “stuck ”). The re
sults are reported in Figure 2 (b).

(a) Uncertain cost

(b) Uncertain transition

nominal
robust
dis. rob.

90

80

70

60

50

40

30

20

t
i
x
E
 
o
t
 
e
m
i
T

10

0

1

2

3

5

4

1/λ

6

7

8

9

t
i
x
E
 
o
t
 
e
m
i
T

90

80

70

60

50

40

30

20

10

nominal
robust
dis. rob.

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

γ

Figure 2: Simulation results of the path planning problem.

In both the uncertain cost and the uncertain transition probability setups, the distributionally robust
approach outperforms the other two approach over virtually the whole range of parameters. This
is well expected, since additional probabilistic information is available to and incorporated by the
distributionally robust approach.

6 Concluding remarks

In this paper we proposed a distributionally robust approach to mitigate the conservatism of the
robust MDP framework and incorporate additional a-prior probabilistic information regarding the
unknown parameters. In particular, we considered the nested-set structured parameter uncertainty
to model a-prior probabilistic information of the parameters. We proposed to ﬁnd a policy that
achieves maximum expected utility under the worst admissible distribution of the parameters. Such
formulation leads to a policy that is obtained through a Bellman type backward induction, and can
be solved in polynomial time under mild technical conditions.

A different perspective on our work is that we develop a principled approach to the problem of
uncertainty set design in multi-stage decision problems. It has been observed that shrinking the un-
certainty set in single-stage problems leads to better performance. We provide a principled approach
to the problem of uncertainty set selection: the distributionally robust policy is a robust policy w.r.t. a
carefully designed single uncertainty set that depends on the a-priori knowledge.

A natural question is how can we take advantage of the distributionally robust approach and solve
(exactly) a full-blown Bayesian generative model MDP? The problem with taking an increasingly
re ﬁned nested uncertainty structure (i.e., increasing n) is that of representation: the equivalent ro-
bust MDP uncertainty set may become too complicated to represent efﬁciently. Nevertheless, if it is
possible to offer upper and lower bounds on the probability of each nested sets (based on the gener-
ative model), the corresponding distributionally robust policies provide performance bounds on the
optimal policies in the, often intractable, Bayesian model.

Acknowledgements

We thank an anonymous reviewer for pointing out relevant references in mathematical ﬁnance. H.
Xu would like to acknowledge the support from DTRA grant HDTRA1-08-0029. S. Mannor would
like to acknowledge the support the Israel Science Foundation under contract 890015.

8

References

[1] M. L. Puterman. Markov Decision Processes. John Wiley & Sons, New York, 1994.
[2] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.
[3] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
[4] S. Mannor, D. Simester, P. Sun, and J. Tsitsiklis. Bias and variance in value vunction estima-
tion. In Proceedings of the 21th international conference on Machine learning, 2004.
[5] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780 –798, September 2005.
[6] A. Bagnell, A. Ng, and J. Schneider. Solving uncertain Markov decision problems. Technical
Report CMU-RI-TR-01-25, Carnegie Mellon University, August 2001.
[7] C. C. White III and H. K. El Deib. Markov decision processes with imprecise transition prob-
abilities. Operations Research, 42(4):739 –748, July 1992.
[8] G. N. Iyengar. Robust dynamic programming. Mathematics of Operations Research,
30(2):257 –280, 2005.
[9] L. G. Epstein and M. Schneider. Learning under ambiguity. Review of Economic Studies,
74(4):1275 –1303, 2007.
[10] A. Nilim and L. El Ghaoui. Robustness in Markov decision problems with uncertain transition
matrices. In Advances in Neural Information Processing Systems 16, 2004.
[11] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with param-
eter uncertainty. Operations Research, (1):203 –213, 2010.
[12] H. Xu and S. Mannor. The robustness-performance tradeoff in Markov decision processes. In
B. Sch ¨olkopf, J. C. Platt, and T. Hofmann, editors,Advances in Neural Information Processing
Systems 19, pages 1537 –1544. MIT Press, 2007.
[13] H. Scarf. A min-max solution of an inventory problem. In Studies in Mathematical Theory of
Inventory and Production, pages 201 –209. Stanford University Press, 1958.
[14] J. Dupacov ´a. The minimax approach to stochastic progr amming and an illustrative application.
Stochastics, 20:73 –88, 1987.
[15] P. Kall. Stochastic programming with recourse: Upper bounds and moment problems, a review.
In Advances in Mathematical Optimization. Academie-Verlag, Berlin, 1988.
[16] A. Shapiro. Worst-case distribution analysis of stochastic programs. Mathematical Program-
ming, 107(1):91 –96, 2006.
[17] I. Popescu. Robust mean-covariance solutions for stochastic optimization. Operations Re-
search, 55(1):98 –112, 2007.
[18] E. Delage and Y. Ye. Distributional robust optimization under moment uncertainty with appli-
cations to data-driven problems. To appear in Operations Research, 2010.
[19] A. Ruszczy ´nski. Risk-averse dynamic programming for Markov decision processes. Mathe-
matical Programming, Series B, 125:235 –261, 2010.
[20] H. F ¨ollmer and A. Schied. Stochastic ﬁnance: An introduction in discrete time . Berlin: Walter
de Gruyter, 2002.
[21] I. Gilboa and D. Schmeidler. Maxmin expected utility with a non-unique prior. Journal of
Mathematical Economics, 18:141 –153, 1989.
[22] D. Kelsey. Maxmin expected utility and weight of evidence. Oxford Economic Papers, 46:425 –
444, 1994.
[23] J. Baron. Thinking and Deciding. Cambridge University Press, 2000.

9


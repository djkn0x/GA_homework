Agnostic Active Learning Without Constraints

Alina Beygelzimer
IBM Research
Hawthorne, NY
beygel@us.ibm.com

John Langford
Yahoo! Research
New York, NY
jl@yahoo-inc.com

Daniel Hsu
Rutgers University &
University of Pennsylvania
djhsu@rci.rutgers.edu

Tong Zhang
Rutgers University
Piscataway, NJ
tongz@rci.rutgers.edu

Abstract

We present and analyze an agnostic active learning algorithm that works without
keeping a version space. This is unlike all previous approaches where a restricted
set of candidate hypotheses is maintained throughout learning, and only hypothe-
ses from this set are ever returned. By avoiding this version space approach, our
algorithm sheds the computational burden and brittleness associated with main-
taining version spaces, yet still allows for substantial improvements over super-
vised learning for classi ﬁcation.

1

Introduction

In active learning, a learner is given access to unlabeled data and is allowed to adaptively choose
which ones to label. This learning model is motivated by applications in which the cost of labeling
data is high relative to that of collecting the unlabeled data itself. Therefore, the hope is that the
active learner only needs to query the labels of a small number of the unlabeled data, and otherwise
In this work, we are interested in agnostic active
perform as well as a fully supervised learner.
learning algorithms for binary classi ﬁcation that are prov ably consistent, i.e.
that converge to an
optimal hypothesis in a given hypothesis class.

One technique that has proved theoretically proﬁtable is to maintain a candidate set of hypotheses
(sometimes called a version space), and to query the label of a point only if there is disagreement
within this set about how to label the point. The criteria for membership in this candidate set needs
to be carefully deﬁned so that an optimal hypothesis is alway s included, but otherwise this set can be
quickly whittled down as more labels are queried. This technique is perhaps most readily understood
in the noise-free setting [1, 2], and it can be extended to noisy settings by using empirical conﬁdence
bounds [3, 4, 5, 6, 7].

The version space approach unfortunately has its share of signi ﬁcant drawbacks. The ﬁrst is com-
putational intractability: maintaining a version space and guaranteeing that only hypotheses from
this set are returned is difﬁcult for linear predictors and a ppears intractable for interesting nonlinear
predictors such as neural nets and decision trees [1]. Another drawback of the approach is its brittle-
ness: a single mishap (due to, say, modeling failures or computational approximations) might cause
the learner to exclude the best hypothesis from the version space forever; this is an ungraceful fail-
ure mode that is not easy to correct. A third drawback is related to sample re-usability: if (labeled)
data is collected using a version space-based active learning algorithm, and we later decide to use
a different algorithm or hypothesis class, then the earlier data may not be freely re-used because its
collection process is inherently biased.

1

Here, we develop a new strategy addressing all of the above problems given an oracle that returns an
empirical risk minimizing (ERM) hypothesis. As this oracle matches our abstraction of many super-
vised learning algorithms, we believe active learning algorithms built in this way are immediately
and widely applicable.

Our approach instantiates the importance weighted active learning framework of [5] using a rejection
threshold similar to the algorithm of [4] which only accesses hypotheses via a supervised learning
oracle. However, the oracle we require is simpler and avoids strict adherence to a candidate set
of hypotheses. Moreover, our algorithm creates an importance weighted sample that allows for
unbiased risk estimation, even for hypotheses from a class different from the one employed by the
active learner. This is in sharp contrast to many previous algorithms (e.g., [1, 3, 8, 4, 6, 7]) that create
heavily biased data sets. We prove that our algorithm is always consistent and has an improved label
complexity over passive learning in cases previously studied in the literature. We also describe a
practical instantiation of our algorithm and report on some experimental results.

1.1 Related Work

As already mentioned, our work is closely related to the previous works of [4] and [5], both of
which in turn draw heavily on the work of [1] and [3]. The algorithm from [4] extends the selective
sampling method of [1] to the agnostic setting using generalization bounds in a manner similar
to that ﬁrst suggested in [3]. It accesses hypotheses only th rough a special ERM oracle that can
enforce an arbitrary number of example-based constraints; these constraints deﬁne a version space,
and the algorithm only ever returns hypotheses from this space, which can be undesirable as we
previously argued. Other previous algorithms with comparable performance guarantees also require
similar example-based constraints (e.g., [3, 5, 6, 7]). Our algorithm differs from these in that (i) it
never restricts its attention to a version space when selecting a hypothesis to return, and (ii) it only
requires an ERM oracle that enforces at most one example-based constraint, and this constraint is
only used for selective sampling. Our label complexity bounds are comparable to those proved in [5]
(though somewhat worse that those in [3, 4, 6, 7]).

The use of importance weights to correct for sampling bias is a standard technique for many machine
learning problems (e.g., [9, 10, 11]) including active learning [12, 13, 5]. Our algorithm is based
on the importance weighted active learning (IWAL) framework introduced by [5]. In that work, a
rejection threshold procedure called loss-weighting is rigorously analyzed and shown to yield im-
proved label complexity bounds in certain cases. Loss-weighting is more general than our technique
in that it extends beyond zero-one loss to a certain subclass of loss functions such as logistic loss. On
the other hand, the loss-weighting rejection threshold requires optimizing over a restricted version
space, which is computationally undesirable. Moreover, the label complexity bound given in [5]
only applies to hypotheses selected from this version space, and not when selected from the entire
hypothesis class (as the general IWAL framework suggests). We avoid these deﬁciencies using a
new rejection threshold procedure and a more subtle martingale analysis.

Many of the previously mentioned algorithms are analyzed in the agnostic learning model, where
no assumption is made about the noise distribution (see also [14]). In this setting, the label com-
plexity of active learning algorithms cannot generally improve over supervised learners by more
than a constant factor [15, 5]. However, under a parameterization of the noise distribution related to
Tsybakov’s low-noise condition [16], active learning algorithms have been shown to have improved
label complexity bounds over what is achievable in the purely agnostic setting [17, 8, 18, 6, 7]. We
also consider this parameterization to obtain a tighter label complexity analysis.

2 Preliminaries

2.1 Learning Model

Let D be a distribution over X × Y where X is the input space and Y = {±1} are the labels. Let
(X, Y ) ∈ X × Y be a pair of random variables with joint distribution D . An active learner receives
a sequence (X1 , Y1 ), (X2 , Y2 ), . . . of i.i.d. copies of (X, Y ), with the label Yi hidden unless it is
explicitly queried. We use the shorthand a1:k to denote a sequence (a1 , a2 , . . . , ak ) (so k = 0
correspond to the empty sequence).

2

Let H be a set of hypotheses mapping from X to Y . For simplicity, we assume H is ﬁnite but does
not completely agree on any single x ∈ X (i.e., ∀x ∈ X , ∃h, h′ ∈ H such that h(x) 6= h′ (x)). This
keeps the focus on the relevant aspects of active learning that differ from passive learning. The error
of a hypothesis h : X → Y is err(h) := Pr(h(X ) 6= Y ). Let h∗ := arg min{err(h) : h ∈ H} be
a hypothesis of minimum error in H. The goal of the active learner is to return a hypothesis h ∈ H
with error err(h) not much more than err(h∗ ), using as few label queries as possible.

2.2

Importance Weighted Active Learning

In the importance weighted active learning (IWAL) framework of [5], an active learner looks at
the unlabeled data X1 , X2 , . . . one at a time. After each new point Xi , the learner determines a
probability Pi ∈ [0, 1]. Then a coin with bias Pi is ﬂipped, and the label Yi is queried if and only if
the coin comes up heads. The query probability Pi can depend on all previous unlabeled examples
X1:i−1 , any previously queried labels, any past coin ﬂips, and the c urrent unlabeled point Xi .
Formally, an IWAL algorithm speci ﬁes a rejection threshold function p : (X × Y × {0, 1})∗ × X →
[0, 1] for determining these query probabilities. Let Qi ∈ {0, 1} be a random variable conditionally
independent of the current label Yi ,
Qi ⊥⊥ Yi | X1:i , Y1:i−1 , Q1:i−1

and with conditional expectation
E[Qi |Z1:i−1 , Xi ] = Pi
:= p(Z1:i−1 , Xi ).
where Zj
:= (Xj , Yj , Qj ). That is, Qi indicates if the label Yi is queried (the outcome of
the coin toss). Although the notation does not explicitly suggest this, the query probability
Pi = p(Z1:i−1 , Xi ) is allowed to explicitly depend on a label Yj (j < i) if and only if it has
been queried (Qj = 1).

2.3

Importance Weighted Estimators

err(h, Z1:n ) :=

1
n

We ﬁrst review some standard facts about the importance weig hting technique. For a function f :
importance weighted estimator of E[f (X, Y )] from Z1:n ∈ (X × Y ×
X × Y → R, deﬁne the
{0, 1})n to be
nXi=1
1
Qi
bf (Z1:n ) :=
Pi · f (Xi , Yi ).
n
Note that this quantity depends on a label Yi only if it has been queried (i.e., only if Qi = 1; it also
depends on Xi only if Qi = 1). Our rejection threshold will be based on a specialization of this
estimator, speci ﬁcally the importance weighted empirical error of a hypothesis h
nXi=1
In the notation of Algorithm 1, this is equivalent to
n X(Xi ,Yi ,1/Pi )∈Sn
1
where Sn ⊆ X × Y × R is the importance weighted sample collected by the algorithm.
A basic property of these estimators is unbiasedness: E[ bf (Z1:n )] = (1/n) Pn
E[E[(Qi /Pi ) ·
f (Xi , Yi ) | X1:i , Y1:i , Q1:i−1 ]] = (1/n) Pn
i=1
E[(Pi /Pi ) · f (Xi , Yi )] = E[f (X, Y )]. So, for exam-
i=1
ple, the importance weighted empirical error of a hypothesis h is an unbiased estimator of its true
error err(h). This holds for any choice of the rejection threshold that guarantees Pi > 0.
3 A Deviation Bound for Importance Weighted Estimators

Qi
Pi · 1[h(Xi ) 6= Yi ].

(1/Pi ) · 1[h(Xi ) 6= Yi ]

err(h, Sn ) :=

(1)

As mentioned before, the rejection threshold used by our algorithm is based on importance weighted
error estimates err(h, Z1:n ). Even though these estimates are unbiased, they are only reliable when

3

the variance is not too large. To get a handle on this, we need a deviation bound for importance
weighted estimators. This is complicated by two factors that rules out straightforward applications
of some standard bounds:

1. The importance weighted samples (Xi , Yi , 1/Pi ) (or equivalently, the Zi = (Xi , Yi , Qi ))
are not i.i.d. This is because the query probability Pi (and thus the importance weight 1/Pi )
generally depends on Z1:i−1 and Xi .
2. The effective range and variance of each term in the estimator are, themselves, random
variables.

To address these issues, we develop a deviation bound using a martingale technique from [19].
Let f : X × Y → [−1, 1] be a bounded function. Consider any rejection threshold function p :
(X × Y × {0, 1})∗ × X → (0, 1] for which Pn = p(Z1:n−1 , Xn ) is bounded below by some positive
quantity (which may depend on n). Equivalently, the query probabilities Pn should have inverses
1/Pn bounded above by some deterministic quantity rmax (which, again, may depend on n). The
a priori upper bound rmax on 1/Pn can be pessimistic, as the dependence on rmax in the ﬁnal
deviation bound will be very mild—it enters in as
log log rmax . Our goal is to prove a bound on
| bf (Z1:n ) − E[f (X, Y )]| that holds with high probability over the joint distribution of Z1:n .
To start, we establish bounds on the range and variance of each term Wi := (Qi /Pi ) · f (Xi , Yi ) in
the estimator, conditioned on (X1:i , Y1:i , Q1:i−1 ). Let Ei [ · ] denote E[ · |X1:i , Y1:i , Q1:i−1 ]. Note
that Ei [Wi ] = (Ei [Qi ]/Pi ) · f (Xi , Yi ) = f (Xi , Yi ), so if Ei [Wi ] = 0, then Wi = 0. Therefore,
the (conditional) range and variance are non-zero only if Ei [Wi ] 6= 0. For the range, we have
|Wi | = (Qi /Pi ) · |f (Xi , Yi )| ≤ 1/Pi , and for the variance, Ei [(Wi − Ei [Wi ])2 ] ≤ (Ei [Q2
i ]/P 2
i ) ·
f (Xi , Yi )2 ≤ 1/Pi . These range and variance bounds indicate the form of the deviations we can
expect, similar to that of other classical deviation bounds.
Theorem 1. Pick any t ≥ 0 and n ≥ 1. Assume 1 ≤ 1/Pi ≤ rmax for all 1 ≤ i ≤ n, and let
Rn := 1/ min({Pi : 1 ≤ i ≤ n ∧ f (Xi , Yi ) 6= 0} ∪ {1}). With probability at least 1 − 2(3 +
log2 rmax )e−t/2 ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Pi · f (Xi , Yi ) − E[f (X, Y )](cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ r 2Rn t
+ r 2t
nXi=1
Qi
n
n
We defer all proofs to the appendices.
4 Algorithm

Rn t
3n

1
n

+

.

First, we state a deviation bound for the importance weighted error of hypotheses in a ﬁnite hypoth-
esis class H that holds for all n ≥ 1. It is a simple consequence of Theorem 1 and union bounds;
the form of the bound motivates certain algorithmic choices to be described below.
Lemma 1. Pick any δ ∈ (0, 1). For all n ≥ 1, let
(cid:19) .
= O (cid:18) log(n|H|/δ)
16 log(2(3 + n log2 n)n(n + 1)|H|/δ)
εn :=
n
n
Let (Z1 , Z2 , . . .) ∈ (X × Y × {0, 1})∗ be the sequence of random variables speci ﬁed in Section 2.2
using a rejection threshold p : (X × Y × {0, 1})∗ × X → [0, 1] that satis ﬁes p(z1:n , x) ≥ 1/nn for
all (z1:n , x) ∈ (X × Y × {0, 1})n × X and all n ≥ 1.
The following holds with probability at least 1 − δ . For all n ≥ 1 and all h ∈ H,
|(err(h, Z1:n ) − err(h∗ , Z1:n )) − (err(h) − err(h∗ ))| ≤ r εn
+
Pmin,n (h)
where Pmin,n (h) = min{Pi : 1 ≤ i ≤ n ∧ h(Xi ) 6= h∗ (Xi )} ∪ {1} .
We let C0 = O(log(|H|/δ)) ≥ 2 be a quantity such that εn (as deﬁned in Eq. (3)) is bounded as
εn ≤ C0 · log(n + 1)/n. The following absolute constants are used in the description of the rejection

εn
Pmin,n (h)

(4)

(3)

4

Algorithm 1
Notes: see Eq. (1) for the deﬁnition of err (importance weighted error), and Section 4 for the
deﬁnitions of C0 , c1 , and c2 .
Initialize: S0 := ∅.
For k = 1, 2, . . . , n:
1. Obtain unlabeled data point Xk .
2. Let
hk := arg min{err(h, Sk−1 ) : h ∈ H}, and
k := arg min{err(h, Sk−1 ) : h ∈ H ∧ h(Xk ) 6= hk (Xk )}.
h′
k , Sk−1 ) − err(hk , Sk−1 ), and
Let Gk := err(h′
Pk := ( 1 if Gk ≤ q C0 log k
(cid:18)= min (cid:26)1, O (cid:18) 1
k−1 + C0 log k
k−1
G2
otherwise
s
k
where s ∈ (0, 1) is the positive solution to the equation
Gk = (cid:18) c1√s − c1 + 1(cid:19) · r C0 log k
+ (cid:16) c2
s − c2 + 1(cid:17) ·
k − 1
3. Toss a biased coin with Pr(heads) = Pk .
If heads, then query Yk , and let Sk := Sk−1 ∪ {(Xk , Yk , 1/Pk )}.
Else, let Sk := Sk−1 .
Return: hn+1 := arg min{err(h, Sn ) : h ∈ H}.
Figure 1: Algorithm for importance weighted active learning with an error minimization oracle.

k − 1 (cid:27)(cid:19)
C0 log k

Gk (cid:19) ·
1

+

C0 log k
k − 1

.

(2)

threshold and the subsequent analysis: c1 := 5 + 2√2, c2 := 5, c3 := ((c1 + √2)/(c1 − 2))2 ,
c4 := (c1 + √c3 )2 , c5 := c2 + c3 .
Our proposed algorithm is shown in Figure 1. The rejection threshold (Step 2) is based on the
deviation bound from Lemma 1. First, the importance weighted error minimizing hypothesis hk and
the “alternative” hypothesis h′
k are found. Note that both optimizations are over the entire hypothesis
class H (with h′
k only being required to disagree with hk on xk ) —this is a key aspect where our
algorithm differs from previous approaches. The difference in importance weighted errors Gk of
the two hypotheses is then computed. If Gk ≤ p(C0 log k)/(k − 1) + (C0 log k)/(k − 1), then
the query probability Pk is set to 1. Otherwise, Pk is set to the positive solution s to the quadratic
equation in Eq. (2). The functional form of Pk is roughly min{1, (1/G2
k + 1/Gk ) · (C0 log k)/(k −
1)}. It can be checked that Pk ∈ (0, 1] and that Pk is non-increasing with Gk . It is also useful to note
that (log k)/(k − 1) is monotonically decreasing with k ≥ 1 (we use the convention log(1)/0 = ∞).
In order to apply Lemma 1 with our rejection threshold, we need to establish the (very crude) bound
Pk ≥ 1/kk for all k .
Lemma 2. The rejection threshold of Algorithm 1 satis ﬁes p(z1:n−1 , x) ≥ 1/nn for all n ≥ 1 and
all (z1:n−1 , x) ∈ (X × Y × {0, 1})n−1 × X .
Note that this is a worst-case bound; our analysis shows that the probabilities Pk are more like
1/poly(k) in the typical case.

5 Analysis

5.1 Correctness

We ﬁrst prove a consistency guarantee for Algorithm 1 that bo unds the generalization error of the
importance weighted empirical error minimizer. The proof actually establishes a lower bound on

5

the query probabilities Pi ≥ 1/2 for Xi such that hn (Xi ) 6= h∗ (Xi ). This offers an intuitive
characterization of the weighting landscape induced by the importance weights 1/Pi .
Theorem 2. The following holds with probability at least 1 − δ . For any n ≥ 1,
0 ≤ err(hn ) − err(h∗ ) ≤ err(hn , Z1:n−1 ) − err(h∗ , Z1:n−1 ) + r 2C0 log n
n − 1
This implies, for all n ≥ 1,
err(hn ) ≤ err(h∗ ) + r 2C0 log n
2C0 log n
n − 1
n − 1
Therefore, the ﬁnal hypothesis returned by Algorithm 1 afte r seeing n unlabeled data has roughly
the same error bound as a hypothesis returned by a standard passive learner with n labeled data. A
variant of this result under certain noise conditions is given in the appendix.

2C0 log n
n − 1

+

.

+

.

5.2 Label Complexity Analysis

We now bound the number of labels requested by Algorithm 1 after n iterations. The following
lemma bounds the probability of querying the label Yn ; this is subsequently used to establish the
ﬁnal bound on the expected number of labels queried. The key t o the proof is in relating empirical
error differences and their deviations to the probability of querying a label. This is mediated through
the disagreement coefﬁcient , a quantity ﬁrst used by [14] for analyzing the label complex ity of the
A2 algorithm of [3]. The disagreement coefﬁcient θ := θ(h∗ , H, D) is deﬁned as
θ(h∗ , H, D) := sup (cid:26) Pr(X ∈ DIS(h∗ , r))
: r > 0(cid:27)
r
where
DIS(h∗ , r) := {x ∈ X : ∃h′ ∈ H such that Pr(h∗ (X ) 6= h′ (X )) ≤ r and h∗ (x) 6= h′ (x)}
(the disagreement region around h∗ at radius r). This quantity is bounded for many learning prob-
lems studied in the literature; see [14, 6, 20, 21] for more discussion. Note that the supremum can
instead be taken over r > ǫ if the target excess error is ǫ, which allows for a more detailed analysis.
Lemma 3. Assume the bounds from Eq. (4) holds for all h ∈ H and n ≥ 1. For any n ≥ 1,
n − 1 ! .
E[Qn ] ≤ θ · 2 err(h∗ ) + O  θ · r C0 log n
C0 log2 n
+ θ ·
n − 1
Theorem 3. With probability at least 1 − δ , the expected number of labels queried by Algorithm 1
after n iterations is at most
1 + θ · 2 err(h∗ ) · (n − 1) + O (cid:16)θ · pC0n log n + θ · C0 log3 n(cid:17) .
The bound is dominated by a linear term scaled by err(h∗ ), plus a sublinear term. The linear term
err(h∗ ) · n is unavoidable in the worst case, as evident from label complexity lower bounds [15, 5].
When err(h∗ ) is negligible (e.g., the data is separable) and θ is bounded (as is the case for many
problems studied in the literature [14]), then the bound represents a polynomial label complex-
ity improvement over supervised learning, similar to that achieved by the version space algorithm
from [5].

5.3 Analysis under Low Noise Conditions

Some recent work on active learning has focused on improved label complexity under certain noise
there exists constants κ > 0 and 0 < α ≤
conditions [17, 8, 18, 6, 7]. Speci ﬁcally, it is assumed that
1 such that
Pr(h(X ) 6= h∗ (X )) ≤ κ · (err(h) − err(h∗ ))α
(5)
for all h ∈ H. This is related to Tsybakov’s low noise condition [16]. Essentially, this condition
requires that low error hypotheses not be too far from the optimal hypothesis h∗ under the disagree-
ment metric Pr(h∗ (X ) 6= h(X )). Under this condition, Lemma 3 can be improved, which in turn
yields the following theorem.

6

Theorem 4. Assume that for some value of κ > 0 and 0 < α ≤ 1, the condition in Eq. (5) holds
for all h ∈ H. There is a constant cα > 0 depending only on α such that the following holds. With
probability at least 1 − δ , the expected number of labels queried by Algorithm 1 after n iterations is
at most
θ · κ · cα · (C0 log n)α/2 · n1−α/2 .
Note that the bound is sublinear in n for all 0 < α ≤ 1, which implies label complexity improve-
ments whenever θ is bounded (an improved analogue of Theorem 2 under these conditions can be
established using similar techniques). The previous algorithms of [6, 7] obtain even better rates
under these noise conditions using specialized data dependent generalization bounds, but these al-
gorithms also required optimizations over restricted version spaces, even for the bound computation.

6 Experiments

Although agnostic learning is typically intractable in the worst case, empirical risk minimization can
serve as a useful abstraction for many practical supervised learning algorithms in non-worst case
scenarios. With this in mind, we conducted a preliminary experimental evaluation of Algorithm 1,
implemented using a popular algorithm for learning decision trees in place of the required ERM
oracle. Speci ﬁcally, we use the J48 algorithm from Weka v3.6.2 (with default parameters) to select
the hypothesis hk in each round k ; to produce the “alternative” hypothesis
k , we just modify
h′
the decision tree hk by changing the label of the node used for predicting on xk . Both of these
procedures are clearly heuristic, but they are similar in spirit to the required optimizations. We
set C0 = 8 and c1 = c2 = 1—these can be regarded as tuning parameters, with C0 controlling
the aggressiveness of the rejection threshold. We did not perform parameter tuning with active
learning although the importance weighting approach developed here could potentially be used for
that. Rather, the goal of these experiments is to assess the compatibility of Algorithm 1 with an
existing, practical supervised learning procedure.

6.1 Data Sets

We constructed two binary classi ﬁcation tasks using MNIST a nd KDDCUP99 data sets. For MNIST,
we randomly chose 4000 training 3s and 5s for training (using the 3s as the positive class), and used
all of the 1902 testing 3s and 5s for testing. For KDDCUP99, we randomly chose 5000 examples
for training, and another 5000 for testing. In both cases, we reduced the dimension of the data to 25
using PCA.

To demonstrate the versatility of our algorithm, we also conducted a multi-class classi ﬁcation exper-
iment using the entire MNIST data set (all ten digits, so 60000 training data and 10000 testing data).
This required modifying how h′
k is selected: we force h′
k (xk ) 6= hk (xk ) by changing the label of
the prediction node for xk to the next best label. We used PCA to reduce the dimension to 40.

6.2 Results

We examined the test error as a function of (i) the number of unlabeled data seen, and (ii) the number
of labels queried. We compared the performance of the active learner described above to a passive
learner (one that queries every label, so (i) and (ii) are the same) using J48 with default parameters.

In all three cases, the test errors as a function of the number of unlabeled data were roughly the same
for both the active and passive learners. This agrees with the consistency guarantee from Theorem 2.
We note that this is a basic property not satis ﬁed by many active learning algorithms (this issue is
discussed further in [22]).

In terms of test error as a function of the number of labels queried (Figure 2), the active learner
had minimal improvement over the passive learner on the binary MNIST task, but a substantial
improvement over the passive learner on the KDDCUP99 task (even at small numbers of label
queries). For the multi-class MNIST task, the active learner had a moderate improvement over the
passive learner. Note that KDDCUP99 is far less noisy (more separable) than MNIST 3s vs 5s task,
so the results are in line with the label complexity behavior suggested by Theorem 3, which states
that the label complexity improvement may scale with the error of the optimal hypothesis. Also,

7

r
o
r
r
e
 
t
s
e
t

0.25

0.2

0.15

0.1

0.05

 
0

0.1

0.08

r
o
r
r
e
 
t
s
e
t

0.06

0.04

0.02

0

 
0

 

Passive
Active

0.05

0.04

0.03

0.02

0.01

r
o
r
r
e
 
t
s
e
t

1000
3000
2000
number of labels queried
MNIST 3s vs 5s

4000

0

 
0

 

Passive
Active

1000
4000
3000
2000
number of labels queried
KDDCUP99

5000

 

Passive
Active

 

Passive
Active

r
o
r
r
e
 
t
s
e
t

0.24

0.22

0.2

0.18

0.16

0.14

100

200
500
400
300
number of labels queried
KDDCUP99 (close-up)

600

 
0

1
3
2
number of labels queried
MNIST multi-class (close-up)

4

4
x 10

Figure 2: Test errors as a function of the number of labels queried.

the results from MNIST tasks suggest that the active learner may require an initial random sampling
phase during which it is equivalent to the passive learner, and the advantage manifests itself after
this phase. This again is consistent with the analysis (also see [14]), as the disagreement coefﬁcient
can be large at initial scales, yet much smaller as the number of (unlabeled) data increases and the
scale becomes ﬁner.

7 Conclusion

This paper provides a new active learning algorithm based on error minimization oracles, a depar-
ture from the version space approach adopted by previous works. The algorithm we introduce here
motivates computationally tractable and effective methods for active learning with many classi ﬁer
training algorithms. The overall algorithmic template applies to any training algorithm that (i) op-
erates by approximate error minimization and (ii) for which the cost of switching a class prediction
(as measured by example errors) can be estimated. Furthermore, although these properties might
only hold in an approximate or heuristic sense, the created active learning algorithm will be “safe”
in the sense that it will eventually converge to the same solution as a passive supervised learning
algorithm. Consequently, we believe this approach can be widely used to reduce the cost of labeling
in situations where labeling is expensive.

Recent theoretical work on active learning has focused on improving rates of convergence. However,
in some applications, it may be desirable to improve performance at much smaller sample sizes, per-
haps even at the cost of improved rates as long as consistency is ensured. Importance sampling and
weighting techniques like those analyzed in this work may be useful for developing more aggressive
strategies with such properties.

Acknowledgments

This work was completed while DH was at Yahoo! Research and UC San Diego.

8

References
[1] D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning,
15(2):201–221, 1994.
[2] S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Information
Processing Systems 18, 2005.
[3] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Twenty-Third International
Conference on Machine Learning, 2006.
[4] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Advances in
Neural Information Processing Systems 20, 2007.
[5] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In Twenty-Sixth
International Conference on Machine Learning, 2009.
[6] S. Hanneke. Adaptive rates of convergence in active learning. In Twenty-Second Annual Conference on
Learning Theory, 2009.
[7] V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. Manuscript,
2009.
[8] M.-F. Balcan, A. Broder, and T. Zhang. Margin based active learning. In Twentieth Annual Conference
on Learning Theory, 2007.
[9] R. .S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
[10] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem.
SIAM Journal of Computing, 32:48–77, 2002.
[11] M. Sugiyama, M. Krauledat, and K.-R. M ¨uller. Covariate shift adaptation by importance weighted cross
validation. Journal of Machine Learning Research, 8:985–1005, 2007.
[12] M. Sugiyama. Active learning for misspeciﬁed models. In Advances in Neural Information Processing
Systems 18, 2005.
[13] F. Bach. Active learning for misspeciﬁed generalized linear mode ls. In Advances in Neural Information
Processing Systems 19, 2006.
[14] S. Hanneke. A bound on the label complexity of agnostic active learning. In Twenty-Fourth International
Conference on Machine Learning, 2007.
[15] M. K ¨a ¨ari ¨ainen. Active learning in the non-realizable case. In Seventeenth International Conference on
Algorithmic Learning Theory, 2006.
[16] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical lear ning. Annals of Statistics, 32(1):135–
166, 2004.
[17] R. Castro and R. Nowak. Upper and lower bounds for active learning. In Allerton Conference on Com-
munication, Control and Computing, 2006.
[18] R. Castro and R. Nowak. Minimax bounds for active learning.
Learning Theory, 2007.
[19] T. Zhang. Data dependent concentration bounds for sequential prediction algorithms.
Annual Conference on Learning Theory, 2005.
[20] E. Friedman. Active learning for smooth problems. In Twenty-Second Annual Conference on Learning
Theory, 2009.
[21] L. Wang. Sufﬁcient conditions for agnostic active learnable. In Advances in Neural Information Process-
ing Systems 22, 2009.
[22] S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In Twenty-Fifth International Confer-
ence on Machine Learning, 2008.

In Twentieth Annual Conference on

In Eighteenth

9


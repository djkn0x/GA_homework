Deep Coding Network

Shenghuo Zhu† Kai Yu†
Yuanqing Lin† Tong Zhang‡
†NEC Laboratories America, Cupertino, CA 95129
‡Rutgers University, Piscataway, NJ 08854

Abstract

This paper proposes a principled extension of the traditional single-layer ﬂat
sparse coding scheme, where a two-layer coding scheme is derived based on the-
oretical analysis of nonlinear functional approximation that extends recent results
for local coordinate coding. The two-layer approach can be easily generalized
to deeper structures in a hierarchical multiple-layer manner. Empirically, it is
shown that the deep coding approach yields improved performance in benchmark
datasets.

1 Introduction

Sparse coding has attracted signiﬁcant attention in recent years because it has been shown to be
effective for some classiﬁcation problems [12, 10, 9, 13, 11 , 14, 2, 5]. In particular, it has been em-
pirically observed that high-dimensional sparse coding plus linear classiﬁer is successful for image
classiﬁcation tasks such as PASCAL 2009 [7, 15].

The empirical success of sparse coding can be justiﬁed by the oretical analysis [17], which showed
that a modiﬁcation of sparse coding with added locality cons traint, called local coordinate cod-
ing (LCC), represents a new class of effective high dimensional non-linear function approximation
methods with sound theoretical guarantees. Speciﬁcally, L CC learns a nonlinear function in high
dimension by forming an adaptive set of basis functions on the data manifold, and it has nonlinear
approximation power. A recent extension of LCC with added local tangent directions [16] demon-
strated the possibility to achieve locally quadratic approximation power when the underlying data
manifold is relatively ﬂat. This also indicates that the non linear function approximation view of
sparse coding not only yields deeper theoretical understanding of its success, but also leads to im-
proved algorithms based on re ﬁned analysis. This paper foll ows the same idea, where we propose a
principled extension of single-layer sparse coding based on theoretical analysis of a two level coding
scheme.

The algorithm derived from this approach has some advantages over the single-layer approach, and
can also be extended into multi-layer hierarchical systems. Such extension draws connection to
deep belief networks (DBN) [8], and hence we call this approach deep coding network. Hierarchi-
cal sparse coding has two main advantages over its single-layer counter-part. First, at the intuitive
level, the ﬁrst layer (traditional single-layer basis) yie lds a crude description of the data at each ba-
sis function, and multi-layer basis functions provide a natural way to zoom into each single basis
for ﬁner local details — this intuition can be re ﬂected more r
igorously in our nonlinear function
approximation result. Due to the more localized zoom-in effect, it also alleviates the problem of
over ﬁtting when many basis functions are needed. Second, it
is computationally more efﬁcient than
ﬂat coding because we only need to look at locations in the sec ond (or higher) layer corresponding
to basis functions with nonzero coefﬁcients in the ﬁrst (or p
revious) layer. Since sparse coding pro-
duces many zero coefﬁcients, the hierarchical structure si gniﬁcantly eliminates many of the coding
computation. Moreover, instead of ﬁtting a single model wit h many variables as in a ﬂat single layer
approach, our proposal of multi-layer coding requires ﬁtti ng many small models separately, each

1

with a small number of parameters. In particular, ﬁtting the small models can be done in parallel,
e.g. using Hadoop, so that learning a fairly big number of codebooks can still be fast.

2 Sparse Coding and Nonlinear Function Approximation

This section reviews the nonlinear function approximation results of single-layer coding scheme in
[17], and then presents our multi-layer extension. Since the result of [17] requires a modiﬁcation of
the traditional sparse coding scheme called local coordinate coding (LCC), our analysis will rely on
a similar modiﬁcation.

Consider the problem of learning a nonlinear function f (x) in high dimension: x ∈ Rd with large
d. While there are many algorithms in traditional statistics that can learn such a function in low
dimension, when the dimensionality d is large compared to n, the traditional statistical methods will
suffer the so called “curse of dimensionality ”. The recentl y popularized coding approach addresses
this issue. Speciﬁcally, it was theoretically shown in [17]
that a speciﬁc coding scheme called Local
Coordinate Coding can take advantage of the underlying data manifold geometric structure in order
to learn a nonlinear function in high dimension and alleviate the curse of dimensionality problem.

The main idea of LCC, described in [17], is to locally embed points on the underlying data manifold
into a lower dimensional space, expressed as coordinates with respect to a set of anchor points. The
main theoretical observation was relatively simple: it was shown in [17] that on the data manifold, a
nonlinear function can be effectively approximated by a globally linear function with respect to the
local coordinate coding. Therefore the LCC approach turns a very difﬁcult high dimensional nonlin-
ear learning problem into a much simpler linear learning problem, which can be effectively solved
using standard machine learning techniques such as regularized linear classiﬁers. This linearization
is effective because the method naturally takes advantage of the geometric information.

In order to describe the results more formally, we introduce a number of notations. First we denote
by k · k the Euclidean norm (2-norm) on Rd :
kxk = kxk2 = qx2
1 + · · · + x2
d .

De ﬁnition 2.1 (Smoothness Conditions) A function f (x) on Rd is (α, β , ν ) Lipschitz smooth with
respect to a norm k · k if

k∇f (x)k ≤ α,

and

and

(cid:12)(cid:12)f (x′ ) − f (x) − ∇f (x)⊤ (x′ − x)(cid:12)(cid:12) ≤ βkx′ − xk2 ,
(cid:12)(cid:12)f (x′ ) − f (x) − 0.5(∇f (x′ ) + ∇f (x))⊤ (x′ − x)(cid:12)(cid:12)
≤ν kx − x′ k3 ,
where we assume α, β , ν ≥ 0.

These conditions have been used in [16], and they characterize the smoothness of f under zero-th,
ﬁrst, and second order approximations. The parameter α is the Lipschitz constant of f (x), which
is ﬁnite if f (x) is Lipschitz; in particular, if f (x) is constant, then α = 0. The parameter β is
the Lipschitz derivative constant of f (x), which is ﬁnite if the derivative ∇f (x) is Lipschitz; in
particular, if ∇f (x) is constant (that is, f (x) is a linear function of x), then β = 0. The parameter
ν is the Lipschitz Hessian constant of f (x), which is ﬁnite if the Hessian of f (x) is Lipschitz;
in particular, if the Hessian ∇2 f (x) is constant (that is, f (x) is a quadratic function of x), then
ν = 0. In other words, these parameters measure different levels of smoothness of f (x): locally
when kx − x′k is small, α measures how well f (x) can be approximated by a constant function, β
measures how well f (x) can be approximated by a linear function in x, and ν measures how well
f (x) can be approximated by a quadratic function in x. For local constant approximation, the error
term αkx− x′ k is the ﬁrst order in kx− x′ k; for local linear approximation, the error term βkx− x′ k2
is the second order in kx − x′ k; for local quadratic approximation, the error term ν kx − x′k3 is the
third order in kx − x′ k. That is, if f (x) is smooth with relatively small α, β , ν , the error term
becomes smaller (locally when kx − x′ k is small) if we use a higher order approximation.

2

Similar to the single-layer coordinate coding in [17], here we de ﬁne a two-layer coordinate coding
as the following.

De ﬁnition 2.2 (Coordinate Coding) A single-layer coordinate coding is a pair (γ 1 , C 1 ), where
C 1 ⊂ Rd is a set of anchor points (aka basis functions), and γ is a map of x ∈ Rd to [γ 1
v (x)]v∈C 1 ∈
R|C 1 | such that Pv∈C 1 γ 1
v (x) = 1. It induces the following physical approximation of x in Rd :
hγ 1 ,C 1 (x) = X
v∈C 1

γ 1
v (x)v .

A two-layer coordinate coding (γ , C ) consists of coordinate coding systems {(γ 1 , C 1 )} ∪
{(γ 2,v , C 2,v ) : v ∈ C 1 }. The pair (γ 1 , C 1 ) is the ﬁrst layer coordinate coding,
(γ 2,v , C 2,v ) are
second layer coordinate-coding pairs that re ﬁne the ﬁrst la
yer coding for every ﬁrst-layer anchor
point v ∈ C 1 .

The performance of LCC is characterized in [17] using the following nonlinear function approxima-
tion result.

≤ α (cid:13)(cid:13)x − hγ 1 ,C 1 (x)(cid:13)(cid:13) + β X
v∈C 1

Lemma 2.1 (Single-layer LCC Nonlinear Function Approximation) Let (γ 1 , C 1 ) be an arbi-
trary single-layer coordinate coding scheme on Rd . Let f be an (α, β , ν )-Lipschitz smooth function.
We have for all x ∈ Rd :
v (x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f (x) − X
wv γ 1
v∈C 1
where wv = f (v) for v ∈ C 1 .
This result shows that a high dimensional nonlinear function can be globally approximated by a
linear function with respect to the single-layer coding [γ 1
v (x)], with unknown linear coefﬁcients
[wv ]v∈C 1 = [f (v)]v∈C 1 , where the approximation on the right hand size is second order. This
bounds directly suggests the following learning method: for each x, we use its coding [γ 1
v (x)] ∈
R|C 1 | as features. We then learn a linear function of the form Pv wv γ 1
v (x) using a standard linear
learning method such as SVM, where [wv ] is the unknown coefﬁcient vector to be learned. The
optimal coding can be learned using unlabeled data by optimizing the right hand side of (1) over
unlabeled data.

|γ 1
v (x)|kv − xk2 ,

(1)

In the same spirit, we can extend the above result on LCC by including additional layers. This leads
to the following bound.

Lemma 2.2 (Two-layer LCC Nonlinear Function Approximation) Let (γ , C ) = {(γ 1 , C 1 )} ∪
{(γ 2,v , C 2,v ) : v ∈ C 1 } be an arbitrary two-layer coordinate coding on Rd . Let f be an (α, β , ν )-
Lipschitz smooth function. We have for all x ∈ Rd :
kf (x) − X
v (x) − X
v (x) X
wv γ 1
γ 1
u∈C 2,v
v∈C 1
v∈C 1
v (x)|kx − hγ 2,v ,C 2,v (x)k + ν X
≤0.5αkx − hγ 1 ,C 1 (x)k + 0.5α X
|γ 1
v∈C 1
v∈C 1

v (x)|kx − vk3 ,
|γ 1

wv,u γ 2,v
u (x)k

(2)

where wv = f (v) for v ∈ C 1 and wv,u = 0.5∇f (v)⊤(u − v) for u ∈ C 2,v , and
v (x) X
kf (x) − X
γ 1
u∈C 2,v
v∈C 1
v (x)|kx − hγ 2,v ,C 2,v (x)k + β X
|γ 1
v∈C 1
|γ 2,v
u (x)|ku − hγ 2,v ,C 2,v (x)k2 ,

wv,u γ 2,v
u (x)k

|γ 1
v (x)|kx − hγ 2,v ,C 2,v (x)k2

≤α X
v∈C 1
+ β X
v∈C 1

v (x)| X
|γ 1
u∈C 2,v

(3)

where wv,u = f (u) for u ∈ C 2,v .

3

Similar to the interpretation of Lemma 2.1, bounds in Lemma 2.2 implies that we can approximate
a nonlinear function f (x) with linear function of the form
v (x) + X
X
X
wv γ 1
u∈C 2,v
v∈C 1
v∈C 1
where [wv ] and [wv,u ] are the unknown linear coefﬁcients to be learned, and [γ 1
v (x)]v∈C 1 and
u (x)]v∈C 1 ,u∈C 2,v form the feature vector. The coding can be learned from unlabeled data by
[γ 2,v
minimizing the right hand side of (2) or (3).

v (x)γ 2,v
wv,u γ 1
u (x),

Compare with the single-layer coding, we note that the second term on the right hand side of (1)
is replaced by the third term on the right hand side of (2). That is, the linear approximation power
of the single-layer coding scheme (with a quadratic error term) becomes quadratic approximation
power of the two-layer coding scheme (with a cubic error term). The ﬁrst term on the right hand
side of (1) is replaced by the ﬁrst two terms on the right hand o f (2). If the manifold is relatively ﬂat,
then the error terms kx − hγ 1 ,C 1 (x)k and kx − hγ 2,v ,C 2,v (x)k will be relatively small in comparison
to the second term on the right hand side of (1). In such case the two-layer coding scheme can
potentially improve the single-layer system signiﬁcantly . This result is similar to that of [16], where
the second layer uses local PCA instead of another layer of nonlinear coding. However, the bound in
Lemma 2.2 is more re ﬁned and speciﬁcally applicable to nonli
near coding. The bound in (2) shows
the potential of the two-layer coding scheme in achieving higher order approximation power than
single layer coding. Higher order approximation gives meaningful improvement when each |C 2,v |
is relatively small compared to |C 1 |. On the other hand, if |C 1 | is small but each |C 2,v | is relatively
large, then achieving higher order approximation does not lead to meaningful improvement. In such
case, the bound in (3) shows that the performance of the two-level coding is still comparable to that
of one-level coding scheme in (1). This is the situation where the 1st layer is mainly used to par-
tition the space (while its approximation accuracy is not important), while the main approximation
power is achieved with the second layer. The main advantage of two-layer coding in this case is
to save computation. This is because instead of solving a single layer coding system with many
parameters, we can solve many smaller coding systems, each with a small number of parameters.
This is the situation when including nonlinearity in the second layer becomes useful, which means
that the deep-coding network approach in this paper has some advantage over [16] which can only
approximate linear function with local PCA in the second layer.

3 Deep Coding Network

We shall discuss the computational algorithm motivated by Lemma 2.2. While the two bounds (2)
and (3) consider different scenarios depending on the relative size of the ﬁrst layer versus the second
layer, in reality it is difﬁcult to differentiate and usuall y both bounds play a role at the same time.
Therefore we have to consider a mixed effect. Instead of minimizing one bound versus another, we
shall use them to motivate our algorithm, and design a method that accommodate the underlying
intuition re ﬂected by the two bounds.

3.1 Two Layer Formulation

In the following, we let C 1 = {v1 , . . . , vL1 }, γ 1
j , C 2,vj = {vj,1 , . . . , vj,L2 }, and
vj (Xi ) = γ i
2,vj
j,k , where L1 is the size of the ﬁrst-layer codebook, and L2 is the size of each
vj,k (Xi ) = γ i
γ
individual codebook at the second layer. We take a layer-by-layer approach for training, where the
second layer is regarded as a re ﬁnement of the ﬁrst layer, whi
ch is consistent with Lemma 2.2. In
the ﬁrst layer, we learn a simple sparse coding model with all data:




2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
n
X
Xi −




i=1
2
j ≥ 0, X
γ i
subject to γ i
j = 1, kvj k ≤ κ,
j
where κ is some constant, e.g., if all Xi are normalized to have unit length, κ can be set to be
1. For convenience, we not only enforce sum-to-one-constraint on the sparse coefﬁcients, but also

[γ 1 , C 1 ] = arg min
γ ,v

(4)

1
2

L1
X
j=1

γ i
j vj

4

impose nonnegative constraints so that Pj |γ i
j = 1 for all i. This presents a probability
j | = Pj γ i
interpretation of the data, and allow us to approximate the following term on the right hand sides of
(2) and (3):

γ i
j

γ i
j

.

X
j

L2
X
k=1

L2
X
k=1

1/2
2
≤ 
j,k vj,k (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
j,k vj,k (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
X
γ i
γ i
Xi −
Xi −

j
Note that neither sum to one or 1-norm regularization of coefﬁcients is needed in the derivation of
(2), while such constraints are needed in (3). This means additional constraints may hurt perfor-
mance in the case of (2) although it may help in the case of (3). Since we don’t know which case
is the dominant effect, as a compromise we remove the sum-to-one constraint but put in 1-norm
regularization which is tunable. We still keep the positivity constraint for interpretability. This leads
to the following formulation for the second layer:


2
j,k vj,k (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
n
X
γ i
Xi −


i=1
2
subject to γ i
j,k ≥ 0, kvj,k k ≤ 1,
where λ2 is a l1 -norm sparsity regularization parameter controlling the sparseness of solutions. With
the codings on both layers, the sparse representation of Xi is (cid:2)sγ i
j , γ i
j [γ i
j,1 , γ i
j,2 , ..., γ i
j,L2 ](cid:3)j=1,...L1
where s is a scaling factor balances the coding from the two different layers.

[γ 2,vj , C 2,vj ] = arg min
γ ,v

L2
X
k=1

L2
X
k=1







γ i
j,k

+ λ2

γ i
j

1
2

(5)

3.2 Multi-layer Extension

The two-level coding scheme can be easily extended to the third and higher layers. For example,
at the third layer, for each base vj,k , the third-layer coding is to solve the following weighted opti-
mization:


2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
j,k,l vj,k,l (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
n
X
γ i
Xi −


i=1
2
subject to γ i
j,k,l ≥ 0, kvj,k,lk ≤ 1.

3 , C j,k
[γ j,k
3 ] = arg min
γ ,v

+ λ3 X
l

L3
X
l=1




γ i
j,k,l




γ i
j,k

(6)

1
2

3.3 Optimization

The optimization problems in Equations (4) to (6) can be generally solved by alternating the follow-
ing two steps: 1) given current cookbook estimation v , compute the optimal sparse coefﬁcients γ ;
2) given the new estimates of the sparse coefﬁcients, optimi ze the cookbooks.

Step 1 requires solving an independent optimization problem for each data sample, and it can be
computationally very expensive when there are many training examples. In such case, computa-
tional efﬁciency becomes an important issue. We developed s ome efﬁcient algorithms for solving
the optimizations problem in Step 1 by exploiting the fact that the solutions of the optimization
problems are sparse. The optimization problem in Step 1 of (4) can be posed as a nonnegative
quadratic programming problem with a single sum-to-one equality constraint. We employ an ac-
tive set method for this problem that easily handles the constraints [4]. Most importantly, since the
optimal solutions are very sparse, the active set method often gives the exact solution after a few
dozen of iterations. The optimization problem in (5) contains only nonnegative constraints (but not
the sum-to-one constraint), for which we employ a pathwise projected Newton (PPN) method [3]
that optimizes a block of coordinates per iteration instead of one coordinate at a time in the active
set method. As a result, in typical sparse coding settings (for example, in the experiments that we
will present shortly in Section 4), the PPN method is able to give the exact solution of a median size
(e.g. 2048 dimension) nonnegative quadratic programming problem in milliseconds.

Step 2 can be solved in its dual form, which is convex optimization with nonnegative constraints [9].
Since the dual problem contains only nonnegative constraints, we can still employ projected Newton
method.
It is known that the projected Newton method has superlinear convergence rate under

5

fairly mild conditions [3]. The computational cost in Step 2 is often negligible compared to the
computational cost in Step 1 when the cookbook size is no more than a few thousand.

A signiﬁcant advantage of the second layer optimization in o ur proposal is parallelization. As shown
in (5), the second-layer sparse coding is decomposed into L1 independent coding problems, and thus
can be naturally parallelized. In our implementation, this is done through Hadoop.

4 Experiments

4.1 MNIST dataset

We ﬁrst demonstrate the effectiveness of the proposed deep c oding scheme on the popular MNIST
benchmark data [1]. MNIST dataset consists of 60,000 training digits and 10,000 testing digits. In
our experiments of deep coding network, the entire training set is used to learn the ﬁrst-layer coding,
with codebook of size 64. For each of the 64 bases in the ﬁrst la yer, a second-layer codebook was
learned – the deep coding scheme presented in the paper ensur es that the codebook learning can
be done independently. We implemented a Hadoop parallel program that solved the 64 codebook
learning tasks in about an hour – which would have taken 64 hou rs on single machine. This shows
that easy parallelization is a very attractive aspect of the proposed deep coding scheme, especially
for large scale problems.

Table 1 shows the performance of deep coding network on MNIST compared to some previous
coding schemes. There are a number of interesting observations in these results. First, adding
an extra layer yields signiﬁcant improvement on classiﬁcat
ion; e.g. for L1 = 512, the classiﬁcation
error rate for single layer LCC is 2.60% [17] while extended LCC achieves 1.98% [16] (the extended
LCC method in [16] may also be regarded as a two layer method but the second layer is linear); the
two-layer coding scheme here signiﬁcantly improves the per formance with classiﬁcation error rate
of 1.51% . Second, the two-layer coding is less prone to over ﬁtting th an its single-layer counterpart.
In fact, for the single-layer coding, our experiment shows that further increasing the codebook size
will cause over ﬁtting (e.g., with L1 = 8192, the classiﬁcation error deteriorates to 1.78%).
In
contrast, the performance of two-layer coding still improves when the second-layer codebook is as
large as 512 (and the total codebook size is 64 × 512 = 32768, which is very high-dimensional
considering the total number of training data is only 60,000). This property is desirable especially
when high-dimensional representation is preferred in the case of using sparse coding plus linear
classiﬁer for classiﬁcations.

Figure 1 shows some ﬁrst-layer bases and their associated se cond-layer bases. We can see that the
second-layer bases provide deeper details that helps to further explain their ﬁrst layer parent basis;
on the other hand, the parent ﬁrst-layer basis provides an in formative context for its child second-
layer bases. For example, in the seventh row in Fig. 1 where the ﬁrst-layer basis is like Digit 7, this
basis can come from Digit 7, Digit 9 or even Digit 4. Then, its second-layer bases help to further
explain the meaning of the ﬁrst-layer basis: in its associat ed second-layer bases, the ﬁrst two bases
in that row are parts of Digit 9 while the last basis in that row is a part of Digit ’4’. Meanwhile, the
ﬁrst-layer 7-like basis provides important context for its second-layer part-like bases – without the
ﬁrst-layer basis, the fragmented parts (like the ﬁrst two se
cond-layer bases in that row) may not be
very informative. The zoomed-in details contained in deeper bases signiﬁcantly help a classiﬁer to
resolve difﬁcult examples, and interestingly, coarser det ails provide useful context for ﬁner details.

Single layer sparse coding
Number of bases (L1 )
512
1024
2.17
2.60
Local coordinate coding
Extended LCC
1.95
1.82
Two-layer sparse coding
Number of bases (L2 )
64
128
1.69
1.85
L1 = 64

2048
1.79
1.78

4096
1.75
1.64

256
1.53

512
1.51

Table 1: The classiﬁcation error rate (in %) on MNIST dataset with different sparse coding schemes.

6

First−layer bases

Second−layer bases

 

 

Figure 1: Example of bases from a two-layer coding network on MNIST data. For each row, the
ﬁrst image is a ﬁrst-layer basis, and the remaining images ar
e its associated second-layer bases.
The colorbar is the same for all images, but the range it represents differs from image to image –
generally, the color of the background of a image represent zero value, and the colors above and
below that color respectively represent positive and negative values.

4.2 PASCAL 2007

The PASCAL 2007 dataset [6] consists of 20 categories of images such as airplanes, persons, cats,
tables, and so on. It consists of 2501 training images and 2510 validation images, and the task is to
classify an image into one or more of the 20 categories. Therefore, this task can be casted as training
20 binary classiﬁers. The critical issue is how to extract ef fective visual features from the images.
Among different methods, one particularly effective approach is to use sparse coding to derive a
codebook of low-level features (such as SIFT) and represent an image as a bag of visual words [15].
Here, we intend to learn two-layer hierarchical codebooks instead of single ﬂat codebook for the
bag-of-word image representation.

In our experiments, we ﬁrst sampled dense SIFT descriptors ( each is represented by a 128× 1 vector)
on each image using four scales, 7 × 7, 16 × 16, 25 × 25 and 31 × 31 with stepsize of 4. Then,
the SIFT descriptors from all images (both training and validation images) were utilized to learn
ﬁrst-layer codebooks with different dimensions, L1 = 512, 1024 and 2048. Then, given a ﬁrst-
layer codebook, for each basis in the codebook, we learned its second-layer codebook of size 64
by solving the weighted optimization in (5). Again, the second-layer codebook learning was done
in parallel using Hadoop. With the ﬁrst-layer and second-la yer codebooks, each SIFT feature was
coded into a very high dimensional space: using L1 = 1024 as an example, the coding dimension

7

Dimension of the ﬁrst layer ( L1 )
Single-layer sparse coding
Two-layer sparse coding (L2=64)

512
42.7
51.1

1024
45.3
52.8

2048
48.4
53.3

Table 2: Average precision (in %) of classiﬁcation on PASCAL07 dataset using different spar se
coding schemes.

in total is 1024 + 1024 × 64 = 66, 560. For each image, we employed 1 × 1, 2 × 2 and 1 × 3
spatial pyramid matching with max-pooling. Therefore in the end, each image is represented by a
532, 480(= 66, 560×8)×1 high-dimensional vector for L1 = 1024. Table 2 shows the classiﬁcation
results. It is clear that the two-layer sparse coding performs signiﬁcantly better than its single-layer
counterpart.

We would like to point out that, although we simply employed max-pooling in the experiments, it
may not be the best pooling strategy for the hierarchical coding scheme presented in this paper. We
believe a better pooling scheme needs to take the hierarchical structure into account, but this remains
as an open problem and is one of our future work.

5 Conclusion

This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme,
where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional
approximation that extends recent results for local coordinate coding. The two-layer approach can be
easily generalized to deeper structures in a hierarchical multiple-layer manner. There are two main
advantages of multi-layer coding: it can potentially achieve better performance because the deeper
layers provide more details and structures; it is computationally more efﬁcient because coding are
decomposed into smaller problems. Experiment showed that the performance of two-layer coding
can signiﬁcantly improve that of single-layer coding.

For the future directions, it will be interesting to explore the deep coding network with more than
two layers. The formulation proposed in this paper grants a straightforward extension from two
layers to multiple layers. For small datasets like MNIST, the two-layer scheme seems to be already
very powerful. However, for more complicated data, deeper coding with multiple layers may be an
effective way for gaining ﬁner and ﬁner features. For exampl
e, the ﬁrst layer coding picks up some
large categories such as human, bikes, cups, and so on; then for the human category, the second-
layer coding may ﬁnd difference among adult, teenager, and s enior person; and then the third layer
ges.
may ﬁnd even ﬁner features such as race feature at different a

References

[1] http://yann.lecun.com/exdb/mnist/.
[2] Samy Bengio, Fernando Pereira, Yoram Singer, and Dennis Strelow. Group sparse coding. In
NIPS’ 09, 2009.
[3] D P. Bertsekas. Projected newton methods for optimization problems with simple constraints.
SIAM J. Control Optim., 20(2):221 –246, 1982.
[4] Dimitri P. Bertsekas. Nonlinear programming. Athena Scientiﬁc, 2003.
[5] David Bradley and J. Andrew (Drew) Bagnell. Differentiable sparse coding. In Proceedings
of Neural Information Processing Systems 22, December 2008.
[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.
The
PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.
[7] Mark Everingham. Overview and results of the classiﬁcat ion challenge. The PASCAL Visual
Object Classes Challenge Workshop at ICCV, 2009.
[8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504 – 507, July 2006.

8

[9] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efﬁcient sparse coding algo-
rithms. In Proceedings of the Neural Information Processing Systems (NIPS) 19, 2007.
[10] Michael S. Lewicki and Terrence J. Sejnowski. Learning overcomplete representations. Neural
Computation, 12:337 –365, 2000.
[11] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. In
NIPS’ 08, 2008.
[12] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning
a sparse code for nature images. Nature, 381:607 –609, 1996.
[13] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. Self-taught
learning: Transfer learning from unlabeled data. International Conference on Machine Learn-
ing, 2007.
[14] Marc Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep
belief networks. In NIPS’ 07, 2007.
[15] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching
using sparse coding for image classiﬁcation.
In IEEE Conference on Computer Vision and
Pattern Recognition, 2009.
[16] Kai Yu and Tong Zhang. Improved local coordinate coding using local tangents. In ICML’ 09,
2010.
[17] Kai Yu, Tong Zhang, and Yihong Gong. Nonlinear learning using local coordinate coding. In
NIPS’ 09, 2009.

9


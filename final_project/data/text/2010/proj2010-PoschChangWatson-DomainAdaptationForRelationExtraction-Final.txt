Domain Adaptation for Relation Extraction

Daniel Posch, Sheldon Chang, Matthew Watson
{dcposch, sheldonc, mdwatson}@stanford.edu
Mentored by Mihai Surdeanu and David McClosky

2010.12.10

Abstract

We describe our experiments with domain adaptation strategies in order to boost the performance
of the JavaNLP relation extraction system built by Stanford’s natural language processing group. We
implemented a domain adaptation preprocessing step known as EasyAdapt as well as a conﬁdence-
weighted linear classiﬁer with domain adaptation and compared them to several baseline strategies[6].
EasyAdapt outperformed all baselines.

1 Introduction

2 Prior Work

“Relation extraction” means identifying instances of
relations (such as ‘created-by’ or ‘part-of ’), in our
case from a body of text and a set of initial exam-
ples. It is part of the larger ﬁeld of knowledge base
population. Our goal was to make an existing re-
lation extractor perform well on text from multiple
domains, such as Wikipedia and newsgroup articles.
The system begins with positive examples of each re-
lation supplied–in other words, the knowledge base
does not start out empty.

The main focus of our pro ject is applying do-
main adaptation strategies to the KBP system in
the hope of improved performance. Traditionally, do-
main adaption has been about making learning mod-
els perform well on domains other than the training,
or “source” domain. In our case, both our training
and testing data is heterogenous (ie, from multiple
domains). We apply domain adaptation with the in-
tuition that diﬀerent sources of training data will pro-
vide diﬀerent feature weights, so it is a mistake to
treat them all equally.

To our knowledge, and as far as our mentors in
the NLP group are aware, we are the ﬁrst to explore
domain adaptation on a complex information extrac-
tion system.

1

2.1 Relation Extraction

Several diﬀerent approaches have been tried for al-
gorithmically extracting relations from textual data.
Such algorithms might discover, for example, that
Ronald Reagan succeeded Jimmy Carter in oﬃce, or
that wheels are part of a car. They generally fall into
supervised approaches, which take a corpus of text
labeled with known entities and relations, and un-
supervised approaches, which discover new relations
directly from unlabeled text. The Stanford KBP
algorithm[2] focuses on an in-between approach de-
scribed in [3, Mintz et al] – relation extraction via
distant supervision. The algorithm parses input text
into a collection of sentences. An example for a re-
lation is simply a pair of entities, and each sentence
containing that pair is assumed to express that re-
lation.
If one of the examples for person:child
is <Darth Vader, Luke Skywalker>, then the sen-
tence “In Episode VI, Luke Skywalker is shocked to
learn that Darth Vader is his father” would match.
Each tuple of <entity1, entity2, sentence> is
then turned into a large vector of features–for ex-
ample, the words that occur between the two entities
and the relative positions of the entities in a syntactic
parse tree. Each feature is encoded as a string.
Finally, the algorithm creates a set of training ex-
amples {xi , yi } where xi are feature vectors and yi are
the corresponding relation labels, known in Knowl-
edge Base Population as ”slots”. It trains a classiﬁer

on this data. During testing, the system is queried
with an entity and asked to populate the predeﬁned
relations using other named entities that appear in
sentences with the query entity. These sentences,
like the training examples, are drawn from newsgroup
text, web snippets, and Wikipedia. A candidate fea-
ture vector is then constructed for the original entity
and the extracted named entity, and this feature vec-
tor is then classiﬁed as one of the relations or a “no
relation” category.

knowledge base (20% of the entire data available),
with half of the data used as training and the other
half as testing. The train and test sets each con-
tained roughly 300MB of text. Sentences were ex-
tracted with a known slot type in the same manner
as in training. We then scored the classiﬁers’ ability
to predict the correct slot type for a given sentence
by measuring precision (p), recall (r), and F-score
(F1 = p+r
2pr ).

2.2 Domain Adaptation

Domain adaptation in NLP is the challenge of en-
abling algorithms to perform well on text written in
the same language but in diﬀerent formats or styles,
and to be able to train algorithms on similarly dis-
parate input.
We have experimented with several domain adap-
tation strategies.

3 Methods

For this pro ject, we sought to improve the perfor-
mance of the existing KBP model by implement-
ing the EasyAdapt feature augmentation strategy
(Daume 2007), and the conﬁdence weighted multi-
domain linear classiﬁer described in [6, Dredze and
Crammer].
We evaluated these systems against several base-
lines:

ALL using the union of all three domains as the
source domain for the classiﬁer (this is the be-
havior of the original KBP system implemented
by Surdeanu et al)

WEBONLY, NEWSONLY, WIKIONLY
using each domain individually as a source do-
main (e.g. training on just web snippets, then
testing on all three domains)

LININT training three separate classiﬁers on each
domain, then linearly interpolating the result-
ing classiﬁcation probabilities

As Daume noted, these baselines are surprisingly
diﬃcult to beat. His EasyAdapt strategy managed
to surpass all of these baselines (and several others)
on a large variety of datasets, making it one of the
few models for domain adaption to have done so.
We evaluated the baselines and the domain-
adapted systems on a smaller version of the KBP

3.1 EasyAdapt

The ﬁrst and simplest strategy we implemented is
described in [4, Daume 2007]. In that approach, we
simply expand the feature space by adding one copy
of each feature per domain, in addition to the generic,
domain-independent original. For example, if a sen-
tence comes from a web snippet and has feature x,
then we will augment its feature vector to also contain
web-x. It will not contain news-x or wiki-x. If the
original feature space was n-dimensional and we are
adapting our algorithm to d domains, then the new
feature space is n(d + 1)-dimensional – each feature
now has a general version as well as d domain-speciﬁc
ones. When we train a classiﬁer, we are determining
the relative importance of each feature for predicting
instances of each relation, as well as the importance
of each feature in each speciﬁc domain.

3.2 Dredze and Crammer

We implemented a more involved domain adaptation
strategy based on a generalization of the perceptron
algorithm. Conﬁdence-weighted online linear clas-
siﬁers work by assigning a variance to each feature
weight [5]. Whenever a new example xi arrives, the
classiﬁer updates its weights as little as possible to ac-
comodate it. Following the authors’ convention, the
weights after step i are µi and the variances Σi , rep-
resented as a diagonal covariance matrix. The update
rules are:

µi+1 = µi + αyiΣixi
i+1 = Σ−1
Σ−1
i + 2αφxixT
i
As Dredze notes, the uncertainties associated with
each weight are strictly decreasing as training pro-
gresses.
For our setting, we needed the classiﬁer to handle
multiple labels; the classiﬁer described in the paper,
however, is binary. We used another technique, by
Crammer, to do multiclass classiﬁcation [7]. Unlike
its binary counterpart, the multiclass CW classiﬁer

2

has an update rule that cannot be expressed as con-
vex optimization, nor is it solvable in closed form. As
a result, we had to choose among the several heuristic
update functions the paper described. We chose the
simplest function to start with–the single-constraint
update. This takes a form very similar to the update
rule of the binary classiﬁer. In our case, it simpliﬁes
to

µi+1,yi = µi + αiΣi,yi xi
(cid:16)
(cid:17)−1
µi+1,r = µi − αiΣi,r xi
i,l 1 + 2αiφ1{l = yi ∪ l = r}xixT
Σ−
Σi + 1, l =
i
−(1 − 2φmi ) + (cid:112)(1 + 2φmi )2 − 8φ(mi − φvi )
...where ai is given by
4φvi
mi = µi,yi · xi − µi,r · xi and φ is a learning aggres-
sivness parameter. Here, yi is the correct label for

xi , while r is the highest-scoring label accoring to
the weights after step i. Much like a perceptron, no
change is made to the parameters when the system
guesses correctly, ie when yi = r .
We implemented and tested this classiﬁer by itself
and using Daume’s feature augmentation as a prepro-
cessing step.
Finally, we implemented domain adaptation by
leveraging the measures of conﬁdence the classiﬁer is
able to assign to each feature weight. We trained
a conﬁdence-weighted classiﬁer separately for each
source domain, then linearly combined the resulting
weight vectors according to each weight’s conﬁdence,
as described by [6, Dredze and Crammer]. That pa-
per described four diﬀerent ways to combine weights.
We picked one that was both simple, and, in their
tests, eﬀective.
It simply takes the weighted aver-
age of the classiﬁer µ, weighting each element by a
constant minus the corresponding sigma–so that ele-
ments with low sigma (high certainty) are weighted
heavily.

4 Results

We had a few surprises come out of our results. EasyAdapt was the only domain adaptation strategy
that produced an improvement in F-score over the original algorithm (ALL, which simply lumps text from
multiple domains together as one input corpus).
Interestingly, text from web snippets turned out to be
almost as good as the entire input corpus; we had initially suspected that this would be the case for the
more consistent WIKI sentences.
LININT, which interpolates between WEB, WIKI, and NEWS, achieved its highest score by only using
WEB, as shown below:

3

Finally, we evaluated the original, unmodiﬁed JavaNLP relation extractor on the same labels (slots)
shown in [3, Mintz et al]:

Using EasyAdapt, we achieved improvement over almost all relation slots. Considering that JavaNLP is
fairly mature software, we are pleased with this result.

5 Future Work

We may explore additional domain adaptation strate-
gies as the NLP group prepares for the 2010 Text
Analysis Conference. We are hoping to enter submit
our results to a workshop at this conference. Stan-
ford’s relation extraction system may also compete
in the conference’s Knowledge Base Population com-
petition. With Mihai’s guidance, we are considering
several future directions for the pro ject:
• Switching our classiﬁer from mention-level to
relation-level.
In other words, collapse all in-
stances of a given relation, e.g., (Bill Gates,
org:founder, Microsoft) into a single vector.
Based on [8, Riedel et al., ECML 2010].
• Testing the remaining three classiﬁer weight
combination methods described in [Dredze,
Crammer].
• Implementing a more sophisticated update

heuristic for the online multiclass conﬁdence-
weighted classiﬁer described in [Dredze et al].
The paper notes that the cost function each up-
date tries to minimize is not convex, nor does
it have a closed-form solution; as such, updates
are necessarily heuristic.

6 Discussion and conclusion

As our results indicate,
for most relations the
EasyAdapt classiﬁer outperformed the baselines. We
suspect that the reason why we saw these improve-
ments is because augmenting the feature space allows
the system to account for the fact that domains that
see a lot of a certain type of relation will perform
better on that relation. Contrast this with LININT,
where even if a certain relation was predicted with
high conﬁdence in one domain, it was often negated
by the fact that the other domains had not seen the
relation and predicted a NR label.

4

The Dredze and Crammer model is theoretically
immune to this problem, since it takes conﬁdence into
account. However, it did not perform quite as well as
EasyAdapt. This may be because the Drezde’s mul-
ticlass conﬁdence weighted linear classiﬁer has an im-
precise (heuristic) update step. We may experiment
with diﬀerent update steps and diﬀerent weighting
algorithms in future work.
LININT tended to push every predicted label to-
wards the NR (no relation) label. It would only pre-
dict a positive label when there was a fair amount of
conﬁdence across all domains, which did not happen
often. This lead to high accuracy as it was making
obvious choices, but poor overall results.
In our examination of
the feature weights
across all models,
it was clear that some fea-

tures were highly predictive for certain categories.
feature span word:chairman
the
example,
For
a
had
very
high weight
for
the
label-
organization:top membersSLASHemployees.
ing
Other features that consistently demonstrated high
feature weights include the “trigger words” used by
Surdeanu et al in their sentence retrieval. However,
other features had very little predictive power, and
from initial observations, the parse tree features did
not carry a lot of weight. Experimenting with feature
categories and seeing which ones the classiﬁer deems
important is one area we can investigate in the fu-
ture, as using fewer but more relevant features would
improve training speed. Preliminary experiments in
feature tweaking were able to further boost our F1
score by roughly 0.8.

References

[1] Task
Population
Base
Know ledge
for
Description
http://nlp.cs.qc.cuny.edu/kbp/2010/KBP2010 TaskDefinition.pdf

at

TAC

2010

[2] Mihai Surdeanu, et al. A Simple Distant Supervision Approach for the TAC-KBP Slot Fil ling Task
Proceedings of the TAC-KBP Workshop, 2010.

[3] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2008. Distant Supervision for Relation Extraction
without labeled data http://www.stanford.edu/ jurafsky/mintz.pdf

[4] Hal Daume III. 2007. Frustratingly Easy Domain Adaptation http://www.umiacs.umd.edu/ hal/docs/daume07easyadapt.pdf

Conﬁdence-Weighted
2007.
al.
et
Dredze
[5] Mark
http://www.cs.jhu.edu/ mdredze/publications/icml variance.pdf

Linear

Classiﬁcation

[6] Mark Dredze and Koby Crammer. 2008. Online Methods for Multi-Domain Learning and Adaptation
http://www.cs.jhu.edu/ mdredze/publications/multi domain emnlp08.pdf

[7] Koby
Multi-Class
2009.
al.
et
Crammer
http://www.aclweb.org/anthology/D/D09/D09-1052.pdf

Conﬁdence Weighted

Algorithms

[8] Sebastian Riedel et al. ECML 2010. Modeling Relations and Their Mentions without Labeled Text.
http://www.springerlink.com/content/3457744035qm6rw5/fulltext.pdf

5


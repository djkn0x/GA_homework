Permutation Complexity Bound on Out-Sample Error

Malik Magdon-Ismail
Computer Science Department
Rensselaer Ploytechnic Institute
110 8th Street, Troy, NY 12180, USA
magdon@cs.rpi.edu

Abstract

We de ﬁne a data dependent permutation complexity for a hypot hesis set H, which
is similar to a Rademacher complexity or maximum discrepancy. The permutation
complexity is based (like the maximum discrepancy) on dependent sampling. We
prove a uniform bound on the generalization error, as well as a concentration result
which means that the permutation estimate can be efﬁciently estimated.

1 Introduction

Assume a standard setting with data D = {(xi , yi )}n
i=1 , where (xi , yi ) are sampled iid from the joint
distribution p(x, y ) on Rd × {±1}. Let H = {h : Rd 7→ {±1}} be a learning model which produces
a hypothesis g ∈ H when given D (we use g for the hypothesis returned by the learning algorithm
and h for a generic hypothesis in H). We assume the 0-1 loss, so the in-sample error is ein (h) =
2n Pn
1
i=1 (1 − yih(xi )). The out-sample error eout (h) = 1
E [(1 − yh(x))]; the expectation is over
2
the joint distribution p(x, y ). We wish to bound eout (g ). To do so, we will bound |eout (h) − ein (h)|
uniformly over H for all distributions p(x, y ); however, the bound itself will depend on the data,
and hence the distribution. The classic distribution independent bound is the VC-bound (Vapnik and
Chervonenkis, 1971); the hope is that by taking into account the data one can get a tighter bound.
The data dependent permutation complexity1 for H is de ﬁned by:
n
PH (n, D) = Eπ "max
yπi h(xi )# .
1
Xi=1
n
h∈H
Here, π is a uniformly random permutation on {1, . . . , n}. PH (n, D) is an intuitively plausible
measure of the complexity of a model, measuring its ability to correlate with a random permutation
of the target values. The difﬁculty in analyzing PH is that {yπi } is an ordered random sample
from y = [y1 , . . . , yn ], sampled without replacement; as such it is a dependent sampling from a
data driven distribution. Analogously, we may de ﬁne the bootstrap complexity, using the bootstrap
distribution B on y, where each sample yB
is independent and uniformly random over y1 , . . . , yn :
i
n
BH (n, D) = EB "max
i h(xi )# .
Xi=1
yB
h∈H
When the average y -value ¯y = 0, the bootstrap complexity is exactly the Rademacher complexity
(Bartlett and Mendelson, 2002; Fromont, 2007; K ¨a ¨ari ¨ainen and Elomaa, 2003; Koltchinskii, 2001;
Koltchinskii and Panchenko, 2000; Lozano, 2000; Lugosi and Nobel, 1999; Massart, 2000):
n
r "max
rih(xi )# ,
Xi=1
RH (n, D) = E
h∈H
1For simplicity, we assume that H is closed under negation; generally, all the results hold with the complex-
n Pn
ities deﬁned using absolute values, so for example PH (n, D) = Eπ ˆmaxh∈H ˛
˛˜.
i=1 yπi h(xi )˛
1
˛

1
n

1
n

1

ln

.

6
δ

where r is a random vector of i.i.d. fair ±1’s. The maximum discrepancy complexity measure
∆H (n, D) is similar to the Rademacher complexity, with the expectation over r being restricted to
those r satisfying Pn
i=1 ri = 0,
n
ri yih(xi )# .
r "max
1
Xi=1
∆H (n, D) = E
n
h∈H
When ¯y = 0, the permutation complexity is the maximum discrepancy; the permutation complexity
is to maximum discrepancy as the bootstrap complexity is to the Rademacher complexity. The per-
mutation complexity maintains a little more information regarding the distribution. Indeed we prove
a uniform bound very similar to the uniform bound obtained using the Rademacher complexity:
Theorem 1 With probability at least 1 − δ , for every h ∈ H,
eout (h) ≤ ein (h) + PH (n, D) + 13r 1
2n
The probability in this theorem is with respect to the data distribution. The challenge in proving this
theorem is to accomodate samples (yπi ) constructed according to the data, and in a dependent way.
Using our same proof technique, one can also obtain a similar uniform bound with the bootstrap
complexity, where the samples are independent, but according to the data. The proof starts with the
standard ghost sample and symmetrization argument. We then need to handle the data dependent
sampling in the complexity measure, and this is done by introducing a second ghost data set to
govern the sampling. The crucial aspect about sampling according to a second ghost data set is
that the samples are now independent of the data; this is acceptable, provided the two methods of
sampling are close enough; this is what constitutes the meat of the proof given in Section 2.2.
n Pn
1
For a given permutation π , one can compute maxh∈H
i=1 yπi h(xi ) using an empirical risk
minimization; however, the computation of the expectation over permutations is an exponential task,
which needless to say is not feasible. Fortunately, we can establish that the permutation complexity
is concentrated around its expectation, which means that in principle a single permutation sufﬁces
to compute the permutation complexity. Let π be a single random permutation.
Theorem 2 For an absolute constant c ≤ 6 + p2/ ln 2, with probability at least 1 − δ ,
n
yπi h(xi ) + cr 1
3
1
Xi=1
ln
.
PH(n, D) ≤ sup
n
2n
δ
h∈H
The probability here is with respect to random permutations (i.e., it holds for any data set). It is easy
to show concentration for the bootstrap complexity about its expectation – this follows from Mc-
Diarmid’s inequality because the samples are independent. The complication with the permutation
complexity is that the samples are not independent. Nevertheless, we can show the concentration
indirectly by ﬁrst relating the two complexities for any dat a set, and then using the concentration of
the bootstrap complexity (see Section 2.3).

Empirical Results. For a single random permutation, with probability at least 1 − δ ,
n
yπi h(xi ) + O  r 1
δ ! .
1
1
Xi=1
eout (h) ≤ ein (h) + sup
ln
n
n
h∈H
Asymptotically, one random permutation sufﬁces; in practi ce, one should average over a few. In-
deed, a permutation based validation estimate for model selection has been extensively tested (see
Magdon-Ismail and Mertsalov (2010) for details); for classiﬁcation, this permutation estimate is the
permutation complexity after removing a bias term. It outperformed LOO-cross validation and the
Rademacher complexity on real data. We restate those results here, comparing model selection us-
ing the permutation estimate versus using the Rademacher complexity (using real data sets from the
UCI Machine Learning repository (Asuncion and Newman, 2007)). The performance metric is the
regret when compared to oracle model selection on a held out set (lower regret is better). We con-
sidered two model selection tasks: choosing the number of leafs in a decision tree; and, selecting k
in the k-nearest neighbor method. The results reported here are averaged over several (10,000 or
more) random splits of the data into a training set and held out set. We de ﬁne a learning episode as
an empirical risk minimization on O(n) data points.

2

Data

Abalone
Ionosphere
M.Mass
Parkinsons
Pima Ind.
Spambase
Transfusion
WDBC
Diffusion

n

3,132
263
667
144
576
3,450
561
426
2,665

10 Learning Episodes
Decision Trees
k-NN
Perm. Rad.
Rad.
Perm.
0.09
0.12
0.02
0.02
0.18
0.75
0.19
0.84
0.11
0.12
0.06
0.06
0.32
0.34
0.44
0.40
0.12
0.15
0.07
0.07
0.43
0.54
0.07
0.07
0.08
0.12
0.09
0.19
0.33
0.24
0.50
0.37
0.02
0.04
0.03
0.06

100 Learning Episodes
k-NN
Decision Trees
Perm. Rad.
Rad.
Perm.
0.04
0.04
0.02
0.02
0.16
0.70
0.17
0.83
0.11
0.11
0.05
0.05
0.33
0.34
0.43
0.41
0.11
0.14
0.07
0.07
0.43
0.06
0.55
0.07
0.08
0.12
0.09
0.19
0.34
0.23
0.51
0.34
0.02
0.03
0.03
0.06

The permutation complexity appears to dominate most of the time (especially when n is small);
and, when it fails to dominate, it is as good or only slightly worse than the Rademacher estimate.
It is not surprising that as n increases, the performances of the various complexities converges.
Asymptotically, one can deduce several relationships between them, for example the maximum
discrepancy can be asymptotically bounded from above and below by the Rademacher complexity.
Similarly, (see Lemma 5), the bootstrap and permutation complexities are equal, asymptotically. The
small sample performance of the complexities as bounding tools is not easy to discern theoretically,
which is where the empirics comes in. An intuition for why the permutation complexity performs
relatively well is because it maintains more of the true data distribution. Indeed, the permutation
method for validation was found to work well empirically, even in regression (Magdon-Ismail and
Mertsalov, 2010); however, our permutation complexity bound only applies to classiﬁcation.

Open Questions. Can the permutation complexity bound be extended beyond classiﬁcation to (for
example) regression with bounded loss? The permutation complexity displays a bias for severely
unbalanced data; can this bias be removed. We conjecture that it should be possible to get a better
n Pn
1
uniform bound in terms of Eπ [maxh∈H
i=1 (yπi − ¯y )h(xi )].

1.1 Related Work

Out-sample error estimation has extensive coverage, both in the statistics and learning commuities.

(i) Statistical methods try to estimate the out-sample error asymptotically in n, and give consis-
tent estimates under certain model assumptions, for example: ﬁnal prediction error (FPE) (Akaike,
1974); Generalized Cross Validation (GCV) (Craven and Wahba, 1979); or, covariance-type penal-
ties (Efron, 2004; Wang and Shen, 2006). Statistical methods tend to work well when the model has
been well speciﬁed. Such methods are not our primary focus.

(ii) Sampling methods, such as leave-one-out cross validation (LOO-CV), try to estimate the out-
sample error directly. Cross validation is perhaps the most used validation method, dating as far
back as 1931 (Larson, 1931; Wherry, 1931, 1951; Katzell, 1951; Cureton, 1951; Mosier, 1951;
Stone, 1974). The permutation complexity uses a “sampled ” d ata set on which to compute the
complexity; other than this super ﬁcial similarity, the est imates are inherently different.

(iii) Bounds. The most celebrated uniform bound on generalization error is the distribution inde-
pendent bound of Vapnik-Chervonenkis (VC-bound) (Vapnik and Chervonenkis, 1971). Since the
VC-dimension may be hard to compute, empirical estimates have been suggested, (Vapnik et al.,
1994). The VC-bound is optimal among distribution independent bounds; however, for a particular
distribution, it could be sub-optimal. Several data dependent bounds have already been mentioned,
which can typically be estimated in-sample via optimization: maximum discrepancy (Bartlett et al.,
2002); Rademacher-style penalties (Bartlett and Mendelson, 2002; Fromont, 2007; K ¨a ¨ari ¨ainen and
Elomaa, 2003; Koltchinskii, 2001; Koltchinskii and Panchenko, 2000; Lozano, 2000; Lugosi and
Nobel, 1999; Massart, 2000); margin based bounds, for example (Shawe-Taylor et al., 1998). Gen-
eralizations to Gaussian and symmetric, bounded variance r have also been suggested, (Bartlett and
Mendelson, 2002; Fromont, 2007) . One main application of such bounds is that any such approx-
imate estimate of the out-sample error (which satisﬁes some bound of the form of the permutation
complexity bound) can be used for model selection, after adding a (small) penalty for the “complex-

3

ity of model selection ” (see Bartlett et al. (2002)). In practice, this penalty for the complexity of
model selection is ignored (as in Bartlett et al. (2002)).

(iv) Permutation Methods are not new to statistics (Good, 2005; Golland et al., 2005; Wiklund et al.,
2007). Golland et al. (2005) show concentration for a permutation based test of signiﬁcance for the
improved performance of a more complex model, using the Rademacher complexity. We directly
give a uniform bound for the out-sample error in terms of a permutation complexity, answering a
question posed in (Golland et al., 2005) which asks whether there is a direct link between permu-
tation statistics and generalization errors. Indeed, Magdon-Ismail and Mertsalov (2010) construct a
permutation estimate for validation which they empirically test in both classiﬁcation and regression
problems. For classiﬁcation, their estimate is related to t he permutation complexity.

Most relevant to this work are Rademacher penalties and the corresponding (sampling without re-
placement) maximum discrepancy. Bartlett et al. (2002) give a uniform bound using the maximum
discrepancy which is in some sense a uniform bound based on a sampling without replacement
(dependent sampling); however, the sampling distribution is ﬁxed, independent of the data. It is
illustrative to brie ﬂy sketch the derivation of the maximum discrepancy bound. Adapting the proof
in Bartlett et al. (2002) and ignoring terms which are O (cid:0)( 1
n ln 1
δ )1/2 (cid:1), with probability at least 1 − δ :
(a)
≤ ein (h) + ED sup
h∈H {eout (h) − ein (h)}
eout (h) ≤ ein (h) + sup
h∈H {eout (h) − ein (h)} ,
n
i )) ,
h∈H (ED′
1
(b)
Xi=1
yih(xi ) − y ′
′
ih(x
= ein (h) + ED sup
2n
n
i )) ,
h∈H ( 1
(c)
Xi=1
yih(xi ) − y ′
′
ih(x
≤ ein (h) + ED,D′ max
2n
i )
h∈H 
n/2
(d)
1
Xi=1
yih(xi ) − y ′
′
ih(x
≤ ein (h) + ED,D′ max
n


(e)
≤ ein (h) + ∆H (n, D),
= ein (h) + ED∆H (n, D)
(a) follows from McDiarmid’s inequality because eout (h) − ein (h) is stable to a single point pertur-
bation for every h, hence the supremum is also stable; in (b) appears a ghost data set and (c) follows
by convexity of the supremum; in (d), we break the sum into two equal parts, which adds the factor
of two; ﬁnally, (e) follows again by McDiarmid’s inequality because ∆H is stable to single point
perturbations. The discrepancy automatically drops out from using the ghost sample; this does not
happen with data dependent permutation sampling, which is where the difﬁculty lies.

,

2 Permutation Complexity Uniform Bound

We now give the proof of Theorem 1. We will adapt the standard ghost sample approach in VC-type
proofs and the symmetrization trick in (Gin ´e and Zinn, 1984 ) which has greatly simpliﬁed VC-style
proofs. In general, high probability results are with respect to the distribution over data sets. Our
main bounding tool will be McDiarmid’s inequality:
Lemma 1 (McDiarmid (1989)) Let Xi ∈ Ai be independent; suppose f : Qi
Ai 7→ R satisﬁes
|f (x) − f (x1 , . . . , xj−1 , z , xj+1 , . . . , xn )| ≤ cj ,
sup
(x1 ,...,xn )∈Qi Ai
z∈Aj
for j = 1, . . . , n. Then, with probability at least 1 − δ ,
f (X1 , . . . , Xn ) ≤ Ef (X1 , . . . , Xn ) + vuut
n
Xi=1
We also obtain Ef ≤ f + q 1
2 Pn
i ln 1
δ by using −f in McDiarmid’s inequality.
i=1 c2

c2
i ln

1
2

1
δ

.

4

2.1 Permutation Complexity

1
n

The out-sample permutation complexity of a model is:
n
PH (n) = ED PH(n, D) = ED,π "max
yπi h(xi )# ,
Xi=1
h∈H
where the expectation is over the data D = (x1 , y1 ), . . . , (xn , yn ) and a random permutation π . Let
D ′ differ from D only in one example, (xj , yj ) → (x
j ).
′
j , y ′
Lemma 2 |PH (n, D) − PH (n, D ′ )| ≤ 4
n .
Proof: For any permutation π and every h ∈ H, the sum Pn
i=1 yπi h(xi ) changes by at most 4 in
going from D to D ′ ; thus, the maximum over h ∈ H changes by at most 4.
Lemma 2 together with McDiarmid’s inequality implies a concentration of PH (n, D) about PH (n),
which means we can work with PH (n, D) instead of the unknown PH(n).
Corollary 1 With probability at least 1 − δ , PH(n) ≤ PH(n, D) + 4r 1
2n
n Pn
Since ein (h) = 1
2 (1 − 1
i=1 yih(xi )), the empirical risk minimizer gπ on the permuted targets
π can be used to compute PH (n, D) for a particular permutation π .
y
2.2 Bounding the Out-Sample Error

1
δ

ln

.

To bound suph∈H{eout (h) − ein (h)}, we ﬁrst use the standard ghost sample and symmetrization
arguments typical of modern generalization error proofs (see for example Bartlett and Mendelson
(2002); Shawe-Taylor and Cristianini (2004)). Let r
n ] be a ±1 sequence.
′′ = [r′′
1 , . . . , r′′
Lemma 3 With probability at least 1 − δ :
n
i )))# + r 1
h∈H ( 1
h∈H{eout(h) − ein (h)} ≤ ED,D′ " sup
Xi=1
i (yih(xi ) − y ′
r′′
′
ih(x
sup
2n
2n
Proof: We proceed as in the proof of the maximum discrepancy bound in Section 1.1:
n
h∈H ( 1
≤ ED,D′ " sup
i ))# + r 1
(a)
1
Xi=1
yih(xi ) − y ′
′
ih(x
ln
2n
2n
δ
n
i )))# + r 1
h∈H ( 1
= ED,D′ " sup
(b)
Xi=1
i (yih(xi ) − y ′
r′′
′
ih(x
2n
2n
n ln 1
In (a), the O(( 1
δ )1/2 ) term is from applying McDiarmid’s inequality because ein (h) changes by
at most 1
n if one data point changes, and so the supremum changes by at most that much; (b) follows
because r′′
i = −1 corresponds to exchanging xi , x
i in the expectation which does not change the
′
expectation (it amounts to relabeling of random variables).

sup
h∈H{eout(h) − ein (h)}

1
δ

1
δ

ln

.

ln

.

,

Lemma 3 holds for an arbitrary sequence r
′′ which is independent of D, D ′ ; we can take the expec-
′′ , as long as r
′′ , for arbitrarily distributed r
tation with respect to r
′′ is independent of D, D ′ .

2.2.1 Generating Permutations with ±1 Sequences
i = yπi yi ; then,
Fix y; for a given permutation π , de ﬁne a corresponding ±1 sequence r
π by rπ
i yi . Thus, given y, for each of the n! permutations π1 , . . . , πn! , we have a correspond-
yπi = rπ
ing ±1 sequence r
πi ; we thus obtain a multiset of sequences Sy = {r
πn! } (there may be
π1 , . . . , r
repetitions as two different permutations may result in the same sequence of ±1 values); we thus
have a mapping from permutations to the ±1 sequences in Sy . If r, a random vector of ±1s, is

5

uniform on Sy , then r.y (componentwise product) is uniform over the permutations of y. We say
that Sy generates the permutations on y. Similarly, we can de ﬁne Sy′ , the generator of permutations
′ . Unfortunately, Sy , Sy′ depend on D, D ′ , and so we can’t take the expectation uniformly over
on y
(for example) r ∈ Sy . We can overcome this by introducing a second ghost sample D ′′ to “approx-
imately ” generate the permutations for y, y
′ , ultimately allowing us to prove the main result.
Theorem 3 With probability at least 1 − 5δ ,
h∈H{eout(h) − ein (h)} ≤ PH (n) + 9r 1
sup
2n
We obtain Theorem 1 by combining Theorem 3 with Corollary 1.

1
δ

ln

,

2.2.2 Proof of Theorem 3

(1)

′′ . In

1
2n

ED,D′ ,D′′

Let D ′′ be a second, independent ghost sample, and Sy′′ the generator of permutations for y
′′ uniform on Sy′′ . The ﬁrst term on the RHS becomes
Lemma 3, take the expectation over r
n
i ))# ,
n! Xπ " sup
1
Xi=1
i (π )(yih(xi ) − y ′
r′′
′
ih(x
h∈H
′′ (π ) ∈ Sy′′ (previously we used rπ
where each permutation π induces a particular sequence r
i
which is now ri (π )). Consider the sequences r, r
′ corresponding to the permutations on y and y
′ .
The next lemma will ultimately relate the expectation over permutations in the second ghost data set
to the permutations over D, D ′ .
Lemma 4 With probability at least 1 − 2δ , there is a one-to-one mapping from the sequences in
′′ (π )}π to Sy = {r(π )}π such that
Sy′′ = {r
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
′′ ))yih(xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
≤ r 8
1
1
Xi=1
(r′′
i − ri (r
2n
n
δ
′′ ) to denote the sequence r ∈ Sy to which r
′′ is
′′ ∈ Sy′′ and every h ∈ H (we write r(r
for every r
mapped). Similarly, there exists such a mapping from Sy′′ to Sy′ .
The probability here is with respect to y, y
′′ . This lemma says that the permutation generat-
′ and y
ing sets Sy′′ , Sy′ , and Sy are essentially equivalent.

ln

,

Proof: We can (without loss of generality) reorder the points in D ′′ so that the ﬁrst k ′′ are +1, so
k′′ = +1, and the remaining are −1. Similarily, we can order the points in D so that
1 = · · · = y ′′
y ′′
the ﬁrst k are +1, so y1 = · · · = yk = +1. We now construct the mapping from Sy′′ to Sy as
′′ (π) ∈ Sy′′ to r(π) ∈ Sy . This mapping is clearly
follows. For a given permutation π , we map r
bijective since every permutation corresponds uniquely to a sequence in Sy (and Sy′′ ).
′′ disagree
πi or yi 6= y ′′
i . Since y and y
i , either yπi 6= y ′′
i . If ri 6= r′′
Let ri = yπi yi and r′′
πi y ′′
i = y ′′
on exactly |k − k ′′ | locations (and similarly for yπ and y
π ), the number of locations where r and r
′′
′′
disagree is therefore at most 2|k − k ′′ |. Thus, for any r
′′ and any h ∈ H,
′′ ))yih(xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
n
1
1
Xi=1
Xi=1
′′ )| |yih(xi )|
|r′′
(r′′
i − ri (r
≤
i − ri (r
2n
2n
n
2|k − k ′′ |
1
Xi=1
′′ )| ≤
|r′′
i − ri (r
n
2n
We observe that Pn
i ) = 2(k − k ′′ ) and so,
i=1 (yi − y ′′
′′ ))yih(xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
≤ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
i )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
zi (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
n
n
1
1
Xi=1
Xi=1
Xi=1
(yi − y ′′
(r′′
i − ri (r
n
2n
where zi = yi − y ′′
′′ are identically distributed, zi are independent and zero mean.
i . Since y and y
n Pn
We consider the function f (z1 , . . . , zn) = 1
i=1 zi . Since zi ∈ {0, ±2}, if you change one of the
6

1
n

=

,

.

zi , f changes by at most 4
n , and so the conditions hold to apply McDiarmid’s inequality to f . Thus,
i=1 zi (cid:12)(cid:12) ≤ q 1
n Pn
2n ln 1
8
using the symmetry of zi , with probability at least 1 − 2δ , (cid:12)(cid:12)
δ .
Given D, D ′ , D ′′ , assume the mappings which are known to exist by the previous lemma are r(r
′′ )
′′ ). We can rewrite the internal summand in the expression of Equation (1) using the equality
and r
′ (r
′′ ))y ′
′′ ) + r′
i − r′
i (yih(xi ) − y ′
r′′
′
i )) = (r′′
′′ ) + ri (r
′′ ))yih(xi ) − (r′′
′
ih(x
ih(x
i − ri (r
i (r
i (r
i ).
Using Lemma 4, we can, with probability at least 1 − 2δ , bound the term which involves (r′′
i −
′′ )) in Equation (1); and, similarly, with probability at least 1 − 2δ , we bound the term involving
ri (r
′′ )). Thus, with probability at least 1 − 4δ , the expression in Equation (1) is bounded by:
(r′′
i − r′
i (r
n
i ))# + 2r 8
n! Xπ " sup
1
1
1
Xi=1
′′ )y ′
′′ )yih(xi ) − r′
′
ih(x
i (r
(ri (r
,
ln
2n
n
δ
h∈H
′′ ) and r
′′ (π ) cycles through the sequences in Sy′′ . Since the mappings r(r
where r
′′ ) are one-to-
′ (r
′ . Since H is closed
′′ ).y cycles through the permutations of y, and similarly for r
one, r(r
′ (r
′′ ).y
under negation, we ﬁnally obtain the bound
n
n
yπi h(xi )# + ED′
i )# + 2r 8
n! Xπ " sup
n! Xπ " sup
1
1
1
Xi=1
Xi=1
′
y ′
πi h(x
n
2n
h∈H
h∈H
Using this in Lemma 3, with probability at least 1 − 5δ ,
h∈H{eout(h) − ein (h)} ≤ PH (n) + 9r 1
sup
2n

ED,D′ ,D′′

1
2n

ED

1
δ

;

ln

ln

1
δ

.

Commentary.
(i) The permutation complexity bound needs empirical risk minimization, which is
notoriously hard; however, if the same algorithm is used for learning as well as computing P , we can
view it as optimization over a constrained hypothesis set (this is especially so with regularization);
the bounds now hold. (ii) The same proof technique can be used to get a bootstrap complexity
bound; the result is similar. (iii) One could bound PH for VC function classes, showing that this
data dependent bound is asymptotically no worse than a VC-type bound. Bounding permutation
complexity on speciﬁc domains could follow the methods in Ba rtlett and Mendelson (2002).

2.3 Estimating PH (n, D) Using a Single Permutation
We now prove Theorem 2, which states that one can essentially estimate PH(n, D) (an average over
n Pn
1
all permutations) by suph∈H
i=1 yπi h(xi ), using just a single randomly selected permutation π .
Our proof is indirect: we will link PH to the bootstrap complexity BH . The bootstrap complexity
is concentrated via an easy application of McDiarmid’s inequality, which will ultimately allow us to
conclude that the permutation estimate is also concentrated. The bootstrap distribution B constructs
a random sequence y
B of n independent uniform samples from y1 , . . . , yn ; the key requirement is
that yB
i are independent samples. There are nn (not distinct) possible bootstrap sequences.

.

1
Lemma 5 |BH(n, D) − PH (n, D)| ≤
√n
Proof: Let k be the number of yi which are +1; we condition on κ, the number of +1 in the
bootstrap sample. Suppose B |κ samples uniformly among all sequences with κ entries being +1.
i h(xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
BH(n, D) = Eκ EB |κ " sup
κ# ,
1
Xi=1
yB
n
h∈H
The key observation is that we can generate all samples uniformly according to B |κ by ﬁrst gener-
ating a random permutation and then selecting randomly |k − κ| +1’s (or −1’s) to ﬂip, so:
i h(xi )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n
n
Eπ " sup
πi h(xi )# .
κ# = EF|k−κ|
EB |κ " sup
1
1
Xi=1
Xi=1
yF
yB
n
n
h∈H
h∈H

7

(F denotes the ﬂipping random process.) Since yF
πi
n
Xi=1

2|k − κ|
n

yπi h(xi ) −

n
Xi=1

≤ sup
h∈H

sup
h∈H

1
n

differs from yπi in exactly |k − κ| positions,
n
2|k − κ|
1
Xi=1
yF
πi h(xi ) ≤ sup
n
n
h∈H

yπi h(xi ) +

.

1
n

Thus,

2
Eκ [|k − κ|].
|BH(n, D) − PH (n, D)| ≤
n
2 √n (because κ is binomial), the result follows.
Since Eκ [|k − κ|] ≤ pVar[k − κ] ≤ 1
In addition to furthering our cause toward the proof of Theorem 2, Lemma 5 is interesting in its own
right, because it says that permutation and bootstrap sampling are asymptotically similar. The nice
thing about the bootstrap estimate is that the expectation is over independent yB
n . Since the
1 , . . . , yB
bootstrap complexity changes by at most 2
n if you change one sample, by McDiarmid’s inequality,
Lemma 6 For a random bootstrap sample B , with probability at least 1 − δ ,
n
i h(xi ) + 2r 1
1
1
Xi=1
yB
BH(n, D) ≤ sup
.
ln
δ
n
2n
h∈H
We now prove concentration for estimating PH (n, D). As in the proof of Lemma 5, generate y
B
in two steps. First generate κ, the number of +1’s in y
B ; κ is binomial. Now, generate a random
π , and ﬂip (as appropriate) a randomly selected |k − κ| entries, where k is the number
permutation y
of +1’s in y.
If we apply McDiarmid’s inequality to the function which equals the number of
2 n ln 1
+1’s, we immediately get that with probability at least 1 − 2δ , |κ − k | ≤ ( 1
δ )1/2 . Thus, with
π in at most (2n ln 1
probability at least 1 − 2δ , y
B differs from y
δ )1/2 positions. Each ﬂip changes
the complexity by at most 2, hence, with probability at least 1 − 2δ ,
n
n
yπi h(xi ) + 4r 1
1
1
1
Xi=1
Xi=1
yB
sup
i h(xi ) ≤ sup
n
n
2n
δ
h∈H
h∈H
We conclude that for a random permutation π , with probability at least 1 − 3δ ,
n
yπi h(xi ) + 6r 1
1
1
Xi=1
.
ln
BH (n, D) ≤ sup
n
2n
δ
h∈H
Now, combining with Lemma 5, we obtain Theorem 2 after a little algebra, because δ < 1.
We have not only established that PH is concentrated, but we have also established a general con-
nection between the permutation and bootstrap based estimates. In this particular case, we see that
sampling with and without replacement are very closely related. In practice, sampling without re-
placement can be very different, because one is never in the truly asymptotic regime. Along that
vein, even though we have concentration, it pays to take the average over a few permutations.

ln

.

IEEE Trans. Aut. Cont., 19,

References
Akaike, H. (1974). A new look at the statistical model identiﬁcation.
716 –723.
Asuncion, A. and Newman, D. (2007). UCI machine learning repository.
Bartlett, P. L. and Mendelson, S. (2002). Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3, 463 –482.
Bartlett, P. L., Boucheron, S., and Lugosi, G. (2002). Model selection and error estimation. Machine
Learning, 48, 85 –113.
Craven, P. and Wahba, G. (1979). Smoothing noisy data with spline functions. Numerische Mathe-
matik, 31, 377 –403.
Cureton, E. E. (1951). Symposium: The need and means of cross-validation: II approximate linear
restraints and best predictor weights. Education and Psychology Measurement, 11, 12 –15.

8

Efron, B. (2004). The estimation of prediction error: Covariance penalties and cross-validation.
Journal of the American Statistical Association, 99(467), 619 –632.
Fromont, M. (2007). Model selection by bootstrap penalization for classiﬁcation. Machine Learning,
66(2-3), 165 –207.
Gin ´e, E. and Zinn, J. (1984). Some limit theorems for empiri cal processes. Annals of Prob., 12,
929 –989.
Golland, P., Liang, F., Mukherjee, S., and Panchenko, D. (2005). Permutation tests for classiﬁcation.
Learning Theory, pages 501 –515.
Good, P. (2005). Permutation, parametric, and bootstrap tests of hypotheses. Springer.
K ¨a ¨ari ¨ainen, M. and Elomaa, T. (2003). Rademacher penalization over decision tree prunings. In In
Proc. 14th European Conference on Machine Learning, pages 193 –204.
Katzell, R. A. (1951). Symposium: The need and means of cross-validation: III cross validation of
item analyses. Education and Psychology Measurement, 11, 16 –22.
Koltchinskii, V. (2001). Rademacher penalties and structural risk minimization. IEEE Transactions
on Information Theory, 47(5), 1902 –1914.
Koltchinskii, V. and Panchenko, D. (2000). Rademacher processes and bounding the risk of function
learning. In E. Gine, D. Mason, and J. Wellner, editors, High Dimensional Prob. II , volume 47,
pages 443 –459.
Larson, S. C. (1931). The shrinkage of the coefﬁcient of mult iple correlation. Journal of Education
Psychology, 22, 45 –55.
Lozano, F. (2000). Model selection using Rademacher penalization. In Proc. 2nd ICSC Symp. on
Neural Comp.
Lugosi, G. and Nobel, A. (1999). Adaptive model selection using empirical complexities. Annals of
Statistics, 27, 1830 –1864.
Magdon-Ismail, M. and Mertsalov, K. (2010). A permutation approach to validation. In Proc. 10th
SIAM International Conference on Data Mining (SDM).
Massart, P. (2000). Some applications of concentration inequalities to statistics. Annales de la
Facult ´e des Sciencies de Toulouse, X, 245 –303.
McDiarmid, C. (1989). On the method of bounded differences. In Surveys in Combinatorics, pages
148 –188. Cambridge University Press.
Mosier, C. I. (1951). Symposium: The need and means of cross-validation: I problem and designs
of cross validation. Education and Psychology Measurement, 11, 5 –11.
Shawe-Taylor, J. and Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Camb. Univ.
Press.
Shawe-Taylor, J., Bartlett, P. L., Williamson, R. C., and Anthony, M. (1998). Structural risk mini-
mization over data dependent hierarchies. IEEE Transactions on Information Theory, 44, 1926 –
1940.
Stone, M. (1974). Cross validatory choice and assessment of statistical predictions. Journal of the
Royal Statistical Society, 36(2), 111 –147.
Vapnik, V. N. and Chervonenkis, A. (1971). On the uniform convergence of relative frequencies of
events to their pr obabilities. Theory of Probability and its Applications, 16, 264 –280.
Vapnik, V. N., Levin, E., and Le Cun, Y. (1994). Measuring the VC-dimension of a learning machine.
Neural Computation, 6(5), 851 –876.
Wang, J. and Shen, X. (2006). Estimation of generalization error: random and ﬁxed inputs. Statistica
Sinica, 16, 569 –588.
Wherry, R. J. (1931). A new formula for predicting the shrinkage of the multiple correlation coefﬁ-
cient. Annals of Mathematical Statistics, 2, 440 –457.
Wherry, R. J. (1951). Symposium: The need and means of cross-validation: III comparison of cross
validation with statistical inference of betas and multiple r from a single sample. Education and
Psychology Measurement, 11, 23 –28.
Wiklund, S., Nilsson, D., Eriksson, L., Sjostrom, M., Wold, S., and Faber, K. (2007). A randomiza-
tion test for PLS component selection. Journal of Chemometrics, 21(10-11), 427 –439.

9


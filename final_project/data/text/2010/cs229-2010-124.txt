Unsupervised Multi-factor Risk Models for US Equities

CS229 Final Project
Kyle Kelley
December 10, 2010

Abstract
This project compares equity risk models developed using the conventional supervised approach as 
well as a PCA-based unsupervised approach.  A fair-out-of-sample comparison shows that PCA 
based models, which are readily developed without economic expertise, can meet and sometimes 
exceed the performance of industry-standard 1 and 3-factor supervised models.

Motivation
Many successful financial traders will argue that risk management is the most important long-run 
aspect of trading, as it's primary goal is to avoid catastrophic losses.  Note that there is no 
universally agreed-upon measure of  a portfolio's risk, but some commonly-used examples are 
variance, Value-at-Risk (VaR), and worst-case loss.  This project only addresses variance-based 
models.  To be useful (and successful), these risk models must somehow accurately predict future 
variances, which (as we will see) is very difficult.

If a portfolio contains n stocks, then the covariance matrix among the stock returns mathematically 
describes the variance of portfolio returns through the formula
2 = x T  x , where
 p
is the covariance matrix among the stocks
∈ℝ n×n
is the vector of stock holding weights in the portfolio
x ∈ℝn

Hence, we need to have reliable predictions of the covariance matrix to accurately assess portfolio 
risk.  A reasonable approach is to assume the historical covariance equals the future covariance, 
 n1 ⋅n
implying that we would need to accurately estimate 
different parameters, which is 
2
extremely difficult (both from standpoints of computation and accuracy) to do in practice. 

Arbitrage Pricing Theory (APT) 1 postulates that the returns of each stock are driven by a linear 
combination of a small set of k underlying factors: 
r s=  r f   , where
is a matrix of factor exposures for each stock
 ∈ℝn× k
r f ∈ℝ k
is a vector of factor returns
is a vector of idiosyncratic returns (uncorrelated to the other factors)
 ∈ℝn

Assuming APT holds, then we can compute the covariance as 
=  f T  , where
 f ∈ℝ k × k
is a covariance matrix among the factor returns
is the covariance matrix among the idiosyncratic returns
∈ℝn× n

If we assume that the idiosyncratic returns are uncorrelated, the idiosyncratic covariance becomes 
 k 1 ⋅k
n parameters, which is 
diagonal, which means we now only need to estimate  n⋅k 
2

1 For more info see Active Portfolio Management by Grinold & Kahn

much more feasible when k ≪ n .  

The problem with the APT approach is that it doesn't specify what the factors are, and they must 
somehow be determined either empirically or through theory.  In practice, teams of highly-trained 
economic analysts work on determining the factors they think drive equity returns, and then 
determine exposure forecasts that each stock has to these factors.  Some examples of such factor 
models are the 1-factor market model, the 3-factor Fama/French model, and the 68 factor model 
sold by MSCI/Barra.  All of these industry-standard supervised approaches are very time-
consuming to build (and/or expensive to buy), which leads us to wonder how effective an 
unsupervised approach can be in a side-by-side comparison.

Furthermore, the general difficultly of estimating covariance with a factor-based APT approach, 
coupled with the difficulty in dealing with an n-x-n matrix in portfolio optimization, often leads 
portfolio managers to ignore covariance altogether and instead focus on variances (e.g., using a 
diagonal covariance matrix).  This approach is briefly addressed in the results.

Figure 1 shows the magnitudes of the principal components for the daily returns of the S&P500 for 
2006, providing some motivation for a PCA-based approach.  Note that the first principal 
component explains nearly 3 times the variance as the second principal component, suggesting why 
historically 1-factor (market) risk models have been used effectively.  

Figure 1: Magnitudes of Principal 
Components for Daily S&P500 Returns (2006)

One obvious problem with PCA is that the factors produced don't necessarily have any physical 
meaning, and hence we have less confidence they will continue to have any relevance in the future. 
This experiment indirectly measures this.

Methodology

General
The goal of this project is to use the historical returns of n stocks measured at time-intervals of 
length d to predict the covariances of these stocks in future time periods.  We define the return of a 
close
pt
stock at time t to be r t= log 
close 
p t−d

is the closing price at time t.

close
, where  p t

We indicate the time a model is constructed as t model
.  We then form  r train∈ℝ n×mtrain with the 
returns of n stocks in m train prior consecutive time periods (i.e., at times
t model− m train d , t model− mtrain−1  d , ... , t model− d ) to predict the covariance matrix of returns over 
m test consecutive future time periods test ∈ℝ nxn
(i.e., using times 
t model , t model d , ... , t model m train d ).  
Weights
As an additional tuning knob, we use a decaying exponential to weight our historical observations, 
giving more weight to more recent observations.  The weight for a return at time t is
− t model− t
d m hl where  mhl
is the tunable half-life parameter.

w t =2
Idiosyncratic Variance
As mentioned we will assume that idiosyncratic variances are uncorrelated.  Hence, we can account 
for it simply by ensuring that the diagonals of our final covariance matrix equal the predicted 
2 ∈ℝ n
2= 2− diag  test 
variance of each stock.    s
, where   s
is the vector of idiosyncratic 
stock variances,   2 ∈ℝ n
is the vector of overall stock variances.

There are a number of sophisticated autoregressive techniques to forecast variance, but these are 
beyond the scope of this project.  Instead, we use historical exponentially weighted variance as a 
predictor of future variance.  Note that any improvements to this algorithm will improve the results 
of all methods equally.

Error Metric
To evaluate models, we compare the observed covariance matrix over  m test
time periods, actual
, to the trained covariance matrix for the same time periods,  test
.  Our error metric is the 
Frobenius norm of the difference: e test=∥ actual−  test∥F .  
Tuning
The set of tunable parameters are optimized by iterating the following cross-validation procedure on 
each parameter until all the parameters have reasonably converged.  In practice, this never took 
more than 2 iterations.

Cross-validation
We use a rolling cross validation in favor of a typical k-fold cross-validation to avoid any future  
bias in our models (which is crucial to avoid with financial data).  Given a full dataset of returns 
over m total
time periods, we can create  mmodels= mtotal −m test− m train different models with this 
data, and estimate the generalization error as the mean of the test errors:
m models
e gen≈e test = 1
∑
i
etest
mmodels
i=1
By performing this procedure over a sweep of parameter values, we select the optimal parameter as 
the one that results in the lowest measured e test
.

.

Unsupervised (PCA)
T  diag  s
2 
, where 
Using PCA, we estimate the covariance matrix as  test =u diag   u
u ∈ℝ nxk  are the k first principal components of the centered/scaled returns data r train
, and 

∈ℝk are the k corresponding eigenvalues.  Note that this method introduces k as another 
tunable parameter: if the principal components estimated from historical data remain accurate in the 
future, then this method will yield good results.  The exponential weighting is used only for 
estimating stock-specific variances (as already mentioned).
Supervised
The supervised approach uses the same returns matrix r train as well as a returns matrix for k 
factors over the same time intervals, r f ∈ℝ k× mtrain
.  We then use a weighted linear regression (same 
exponential weights as before) to estimate  ∈ℝ nxk  so that  r train≈ f
.  Additionally, we 
compute the weighted covariance among the factors to estimate  f
.  The stock covariance matrix 
can then be estimated as test =  f T diag  s
2
.

Results
The described methodology was used on data for stocks in the S&P500.  All parameters were tuned 
using data from 2006-2009, and then the tuned models were further tested on data from January 1, 
2010 to November 30, 2010.   Table 1 describes the different models being compared:

Table 1: Description of Models Tested
Model Description
IVAR Assumes ideal variance forecasts and 0 covariance between stocks.
SPY 1-Factor supervised model (market)
FF
3-Factor Fama/French supervised model (market, size, value) 
PCA Unsupervised PCA model

This methodology was used to build models on 2 different time horizons: one using daily data to 
predict covariance over 22 intervals (1 month), and one using 30-minute bar data to predict 
covariance over 26 intervals (2 days).  The final tuning results and mean errors are reported in 
Tables 2 and 3.  The time-series of test errors between PCA and FF for the 22 day test are shown in 
Figure 2.

Table 2: Results for d=1 day, mtest=22 (1 month), mtrain = 252 (1 year)
e test 2010
e test 2006-2009
Model mhl k
IVAR NA NA 0.3776
0.1106
0.0742
0.2764
1
SPY 12
0.0735
0.2766
3
FF
15
PCA 8
1
0.2702
0.0719
The most important tuning parameter to determine in all cases was mhl.  An example sweep is 
shown in Figure 3; note the minimum occurs at mhl=8.

The results from tuning k (in PCA models) and tuning mtest (in all models) were less exciting but 
still interesting because they provide good guidelines when working with risk models.  In all cases 
PCA models with k=1 were optimal.  Also, once mhl had been tuned, models with larger mtrain had 
lower test error, suggesting the best method in practice is to use the largest possible window of 
historical data when building risk models.

Table 3: Results for d=30 minutes, mtest=26 (2 days), mtrain = 1638 (6 months)

Model mhl k

e test 2006-2009
IVAR NA NA 0.0290
SPY
60
1
0.0223
NA
NA 3
FF2
PCA 30
1
0.0226

e test 2010
0.0109
0.0087
NA
0.0087

Figure 2: Time-series comparison of PCA and FF test errors

2 This test wasn't performed due to a lack of intraday data for the FF factors 

Figure 3: Tuning m_hl for PCA

Conclusions
There are a number of interesting takeaways from these experiments.  Primarily, it was 
demonstrated that, when tuned properly, unsupervised PCA-based risk modeling can meet and 
sometimes outperform a 1-factor SPY model and a 3-factor Fama-French model (tested over both a 
1-month horizon and a 2-day horizon).  It would be interesting to compare these results to the 68 
factor BARRA model in same framework.

Another interesting result is that PCA always had the smallest error with only 1 factor.  In literature 
(and in some commercial models3) there are often 10-20 factors retained from PCA, but these out-
of-sample tests demonstrated this is suboptimal on average (at least for the horizons tested here).  

Moreover, comparing the tests against the performance of IVAR demonstrates that covariance 
modeling is not a futile effort: in practice it is possible to improve substantially upon a perfect 
variance-only model.

On a final note, the prudent reader should recognize that historical-based risk models clearly have 
their limitations, as they don't generally predict situations that have never occurred.  This is most 
apparent in the plot of test errors above during the period of Fall 2008 (see Figure 2), when all 
models performed far worse than their historical average.  This project merely showed that PCA 
rivaled the alternatives (SPY/FF), not that it always did well on an absolute basis.

3 One such example is Northfield Short-Term Equity Risk Model, which keeps 20 PCA factors 
http://www.northinfo.com/documents/5.pdf


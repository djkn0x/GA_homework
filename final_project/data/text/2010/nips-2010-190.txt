A primal-dual algorithm for group sparse
regularization with overlapping groups

Soﬁa Mosci
DISI- Universit `a di Genova
mosci@disi.unige.it

Silvia Villa
DISI- Universit `a di Genova
villa@dima.unige.it

Alessandro Verri
DISI- Universit `a di Genova
verri@disi.unige.it

Lorenzo Rosasco
IIT - MIT
lrosasco@MIT.EDU

Abstract

We deal with the problem of variable selection when variables must be selected
group-wise, with possibly overlapping groups deﬁned a priori. In particular we
propose a new optimization procedure for solving the regularized algorithm pre-
sented in [12], where the group lasso penalty is generalized to overlapping groups
of variables. While in [12] the proposed implementation requires explicit repli-
cation of the variables belonging to more than one group, our iterative procedure
is based on a combination of proximal methods in the primal space and projected
Newton method in a reduced dual space, corresponding to the active groups. This
procedure provides a scalable alternative with no need for data duplication, and
allows to deal with high dimensional problems without pre-processing for dimen-
sionality reduction. The computational advantages of our scheme with respect
to state-of-the-art algorithms using data duplication are shown empirically with
numerical simulations.

1

Introduction

Sparsity has become a popular way to deal with small samples of high dimensional data and, in a
broad sense, refers to the possibility of writing the solution in terms of a few building blocks. Often,
sparsity based methods are the key towards ﬁnding interpretable models in real-world problems. In
particular, regularization based on (cid:96)1 type penalties is a powerful approach for dealing with the prob-
lem of variable selection, since it provides sparse solutions by minimizing a convex functional. The
success of (cid:96)1 regularization motivated exploring different kinds of sparsity properties for (general-
ized) linear models, exploiting available a priori information, which restricts the admissible sparsity
patterns of the solution. An example of a sparsity pattern is when the input variables are partitioned
into groups (known a priori), and the goal is to estimate a sparse model where variables belonging to
the same group are either jointly selected or discarded. This problem can be solved by regularizing
with the group-(cid:96)1 penalty, also known as group lasso penalty, which is the sum, over the groups, of
the euclidean norms of the coefﬁcients restricted to each group.
A possible generalization of group lasso is to consider groups of variables which can be potentially
overlapping, and the goal is to estimate a model which support is the union of groups. This is a
common situation in bioinformatics (especially in the context of high-throughput data such as gene
expression and mass spectrometry data), where problems are characterized by a very low number of
samples with several thousands of variables. In fact, when the number of samples is not sufﬁcient
to guarantee accurate model estimation, an alternative is to take advantage of the huge amount of
prior knowledge encoded in online databases such as the Gene Ontology. Largely motivated by ap-
plications in bioinformatics, a new type of penalty is proposed in [12], which is shown to give better

1

performances than simple (cid:96)1 regularization.
A straightforward solution to the minimization problem underlying the method proposed in [12] is
to apply state-of-the-art techniques for group lasso (we recall interior-points methods [3, 20], block
coordinate descent [16], and proximal methods [9, 21], also known as forward-backward splitting
algorithms, among others) in an expanded space, built by duplicating variables that belong to more
than one group.
As already mentioned in [12], though very simple, such an implementation does not scale to large
datasets, when the groups have signiﬁcant overlap, and a more scalable algorithm with no data du-
plication is needed. For this reason we propose an alternative optimization approach to solve the
group lasso problem with overlap. Our method does not require explicit replication of the features
and is thus more appropriate to deal with high dimensional problems with large groups overlap.
Our approach is based on a proximal method (see for example [18, 6, 5]), and two ad hoc results
that allow to efﬁciently compute the proximity operator in a much lower dimensional space: with
Lemma 1 we identify the subset of active groups, whereas in Theorem 2 we formulate the reduced
dual problem for computing the proximity operator, where the dual space dimensionality coincides
with the number of active groups. The dual problem can then be solved via Bertsekas’ projected
Newton method [7]. We recall that a particular overlapping structure is the hierarchical structure,
where the overlap between groups is limited to inclusion of a descendant in its ancestors. In this case
the CAP penalty [24] can be used for model selection, as it has been done in [2, 13], but ancestors
are forced to be selected when any of their descendant are selected. Thanks to the nested structure,
the proximity operator of the penalty term can be computed exactly in a ﬁnite number of steps [14].
This is no longer possible in the case of general overlap. Finally it is worth noting that the penalty
analyzed here can be applied also to hierarchical group lasso. Differently from [2, 13] selection of
ancestors is no longer enforced.
The paper is organized as follows. In Section 2 we recall the group lasso functional for overlap-
ping groups and set some notations. In Section 3 we state the main results, present a new iterative
optimization procedure, and discuss computational issues. Finally in Section 4 we present some
numerical experiments comparing running time of our algorithm with state-of-the-art techniques.
The proofs are reported in the Supplementary material.

2 Problem and Notations
notation (cid:107)β (cid:107)G = ((cid:80)
We ﬁrst ﬁx some notations. Given a vector β ∈ Rd , while (cid:107)·(cid:107) denotes the (cid:96)2 -norm, we will use the
j )1/2 to denote the (cid:96)2 -norm of the components of β in G ⊂ {1, . . . , d}.
j∈G β 2
Then, for any differentiable function f : RB → R, we denote by ∂r f its partial derivative with
respect to variables r , and by ∇f = (∂r f )B
r=1 its gradient.
We are now ready to cast group (cid:96)1 regularization with overlapping groups as the following varia-
tional problem. Given a training set {(xi , yi )n
i=1 } ∈ (X × Y )n , a dictionary (ψj )d
generalized linear model f (x) = (cid:80)d
j=1 , and B subsets
of variables G = {Gr }B
r=1 with Gr ⊂ {1, . . . , d}, we assume the estimator to be described by a
(cid:26) 1
(cid:27)
j=1 ψj (x)βj and consider the following regularization scheme
Eτ (β ) = argmin
(cid:107)Ψβ − y(cid:107)2 + 2τ ΩG
β ∗ = argmin
overlap (β )
(1)
,
(cid:80)n
β∈Rd
β∈Rd
n
where Ψ is the n × d matrix given by the features ψj in the dictionary evaluated in the training set
n (cid:107)Ψβ − y(cid:107)2 is the empirical error, 1
i=1 (cid:96) (f (xi ), yi ), when
points, [Ψ]i,j = ψj (xi ). The term 1
the cost function1 (cid:96) : R × Y → R+ is the square loss, (cid:96)(f (x), y) = (y − f (x))2 .
n
The penalty term ΩGoverlap : Rd → R+ is lower semicontinuous, convex, and one-homogeneous,
B(cid:88)
(ΩGoverlap(λβ ) = λΩGoverlap(β ), ∀β ∈ Rd and λ ∈ R+ ), and is deﬁned as
(v1 ,...,vB ),vr ∈Rd ,supp(vr)⊂Gr ,PB
(cid:107)vr (cid:107) .
ΩG
inf
overlap (β ) =
r=1 vr =β
r=1
The functional ΩGoverlap was introduced in [12] as a generalization of the group lasso penalty to
allow overlapping groups, while maintaining the group lasso property of enforcing sparse solutions
which support is a union of groups. When groups do not overlap, ΩGoverlap reduces to the group lasso
1Note our analysis would immediately apply to other loss functions, e.g. the logistic loss.

2

penalty. Note that, as pointed out in [12], using (cid:80)B
r=1 (cid:107)β (cid:107)Gr
as generalization of the group lasso
penalty leads to a solution which support is the complement of the union of groups. For an extensive
study of the properties of ΩGoverlap, its comparison with the (cid:96)1 norm, and its extension to graph lasso,
we therefore refer the interested reader to [12].

3 The GLO-pridu Algorithm

If one needs to solve problem (1) for high dimensional data, the use of standard second-order meth-
ods such as interior-point methods is precluded (see for instance [6]), since they need to solve large
systems of linear equations to compute the Newton steps. On the other hand, ﬁrst order methods
inspired to Nesterov’s seminal paper [19] (see also [18]) and based on proximal methods already
proved to be a computationally efﬁcient alternative in many machine learning applications [9, 21].

3.1 A Proximal algorithm
Given the convex functional Eτ in (1), which is sum of a differentiable term, namely 1
n (cid:107)Ψβ − y(cid:107)2 ,
and a non-differentiable one-homogeneous term 2τ ΩGoverlap, its minimum can be computed with
(cid:19)
(cid:1) (cid:18)
β p = (cid:0)I − πτ /σK
following acceleration of the iterative forward-backward splitting scheme
(cid:113)
hp − 1
(cid:16)−cp +
(cid:17)
ΨT (Ψhp − y)
nσ
cp = (1 − tp )cp−1 ,
tp+1 =
p + 8cp
c2
) + β p−1 (tp − 1) tp+1
hp+1 = β p (1 − tp+1 + tp+1
tp
tp
for a suitable choice of σ . Due to one-homogeneity of ΩGoverlap, the proximity operator associated
σ ΩGoverlap reduces to the identity minus the projection onto the subdifferential of τ
σ ΩGoverlap at
to τ
the origin, which is a closed and convex set. We will denote such a projection as πτ /σK , where
K = ∂ΩGoverlap(0). The above scheme is inspired to [10], and is equivalent to the algorithm named
FISTA [5], which convergence is guaranteed, as recalled in the following theorem
Theorem 1 Given β 0 ∈ Rd , and σ = ||ΨT Ψ||/n, let h1 = β 0 and t1 = 1, c0 = 1, then there exists
a constant C0 such that the iterative update (10) satisﬁes
Eτ (β p ) − Eτ (β ∗ ) ≤ C0
(3)
p2 .
As it happens for other accelerations of the basic forward-backward splitting algorithm such as [19,
6, 4], convergence of the sequence β p is no longer guaranteed unless strong convexity is assumed.
However, sacriﬁcing theoretical convergence for speed may be mandatory in large scale applications.
Furthermore, there is a strong empirical evidence that β p is indeed convergent (see Section 4).

(2)

/4

3.2 The projection
Note that the proximity operator of the penalty ΩGoverlap does not admit a closed form and must be
computed approximatively. In fact the projection on the convex set
overlap(0) = {v ∈ Rd , (cid:107)v(cid:107)Gr ≤ 1 for r = 1, . . . , B }.
K = ∂ΩG
cannot be decomposed group-wise, as in standard group (cid:96)1 regularization, which proximity operator
resolves to a group-wise soft-thresholding operator (see Eq. (9) later). Nonetheless, the following
lemma shows that, when evaluating the projection, πK , we can restrict ourselves to a subset of
ˆB = | ˆG | ≤ B active groups. This equivalence is crucial for speeding up the algorithm, in fact ˆB is
the number of selected groups which is small if one is interested in sparse solutions.
Lemma 1 Given β ∈ Rd , G = {Gr }B
r=1 with Gr ⊂ {1, . . . , d}, and τ > 0, the projection onto the
convex set τ K with K = {v ∈ Rd , (cid:107)v(cid:107)Gr ≤ 1 for r = 1, . . . , B } is given by
(cid:107)v − β (cid:107)2
Minimize
v ∈ Rd , (cid:107)v(cid:107)G ≤ τ for G ∈ ˆG .
subject to
where ˆG := {G ∈ G , (cid:107)β (cid:107)G > τ } .

(4)

3

[πτ K (β )]j =
where λ∗ is the solution of

The proof (given in the supplementary material) is based on the fact that the convex set τ K is the
intersection of cylinders that are all centered on a coordinate subspace. Since ˆB is typically much
smaller than d, it is convenient to solve the dual problem associated to (4).
Theorem 2 Given β ∈ Rd , {Gr }B
r=1 with Gr ⊂ {1, . . . , d}, and τ > 0, the projection onto the
convex set τ K with K = {v ∈ Rd , (cid:107)v(cid:107)Gr
≤ τ for r = 1, . . . , B } is given by
(1 + (cid:80) ˆB
βj
for j = 1, . . . , d
r=1 λ∗
r 1r,j )
ˆB(cid:88)
d(cid:88)
1 + (cid:80) ˆB
−β 2
−
j
argmax
f (λ),
with f (λ) :=
(6)
λr τ 2 ,
λ∈R ˆB
r=1 1r,j λr
r=1
j=1
+
ˆG = {G ∈ G , (cid:107)β (cid:107)G > τ } := { ˆG1 , . . . , ˆG ˆB }, and 1r,j is 1 if j belongs to group ˆGr and 0 otherwise.
Equation (6) is the dual problem associated to (4), and, since strong duality holds, the minimum
of (4) is equal to the maximum of the dual problem, which can be efﬁciently solved via Bertsekas’
projected Newton method described in [7], and here reported as Algorithm 1.

(5)

Algorithm 1 Projection
Given: β ∈ Rd , λinit ∈ R ˆB , η ∈ (0, 1), δ ∈ (0, 1/2),  > 0
Initialize: q = 0, λ0 = λinit
r = 0, or |∂r f (λq )| >  if λq
r > 0, for r = 1, . . . , ˆB ) do
while (∂r f (λq ) > 0 if λq
q := q + 1
q = min{, ||λq − [λq − ∇f (λq )]+ ||}
(cid:26)0
I q
+ = {r such that 0 ≤ λq
r ≤ q , ∂r f (λq ) > 0}
+or s ∈ I q
if r (cid:54)= s, and r ∈ I q
+
∂r ∂s f (λq ) otherwise
λ(α) = [λq − α(H q )−1∇f (λq )]+
(cid:110)
ηm (cid:80)
∂r f (λq ) + (cid:80)
r /∈I q
r∈I q
+
+

(cid:111)
r − λr (ηm )]
∂r f (λq )[λq

Hr,s =

m = 0
while f (λq ) − f (λ(ηm )) ≥ δ
m := m + 1
end while

end while
return λq+1

λq+1 = λ(ηm )

(7)

do

Bertsekas’ iterative scheme combines the basic simplicity of the steepest descent iteration [22] with
the quadratic convergence of the projected Newton’s method [8]. It does not involve the solution of
a quadratic program thereby avoiding the associated computational overhead.

3.3 Computing the regularization path

In Algorithm 2 we report the complete Group Lasso with Overlap primal-dual (GLO-pridu) scheme
for computing the regularization path, i.e. the set of solutions corresponding to different values of
the regularization parameter τ1 > . . . > τT , for problem (1). Note that we employ the continuation
strategy proposed in [11]. A similar warm starting is applied to the inner iteration, where at the p-th
step λinit is determined by the solution of the (p−1)-th projection. Such an initialization empirically
proved to guarantee convergence, despite the local nature of Bertsekas’ scheme.

3.4 The replicates formulation

An alternative way to solve the optimization problem (1) is proposed by [12], where the authors
show that problem (1) is equivalent to the standard group (cid:96)1 regularization (without overlap) in an
expanded space built by replicating variables belonging to more than one group:

4

Algorithm 2 GLO-pridu regularization path
Given: τ1 > τ2 > · · · > τT , G , η ∈ (0, 1), δ ∈ (0, 1/2), 0 > 0, ν > 0
Let: σ = ||ΨT Ψ||/n
Initialize: β (τ0 ) = 0
for t = 1, . . . , T do
Initialize: β 0 = β (τt−1 ), λ∗
0 = 0
while ||β p − β p−1 || > ν ||β p−1 || do
• w = hp − (nσ)−1ΨT (Ψhp − y)
• Find ˆG = {G ∈ G , (cid:107)w(cid:107)G ≥ τ }
j = wj (1 + (cid:80) ˆB
p via Algorithm 1 with groups ˆG , initialization λ∗
• Compute λ∗
p−1 and tolerance 0p−3/2
• Compute β p as β p
r 1r,j )−1 for j = 1, . . . , d, see Equation (5)
r=1 λq+1
• Update cp , tp , and hp as in (10)
end while
β (τt ) = β p
end for
return β (τ1 ), . . . , β (τT )

(cid:40)

(cid:41)

B(cid:88)
1
|| ˜β || ˜Gr
|| ˜Ψ ˜β − y ||2 + 2τ
˜β ∗ ∈ argmin
(8)
,
n
˜β∈R ˜d
r=1
where ˜Ψ is the matrix built by concatenating copies of Ψ restricted each to a certain group, i.e.
|GB |, . . . , ˜d|]}, and ˜d = (cid:80)B
= (Ψj )j∈Gr , where { ˜G1 , . . . , ˜GB } = {[1, . . . , |G1 |], [1+|G1 |, . . . , |G1 |+|G2 |], . . . , [ ˜d −
( ˜Ψj )j∈ ˜Gr
j = (cid:80)B
r=1 |Gr | is the number of total variables obtained after including the
r=1 φGr ( ˜β ∗ ), where φGr : R ˜d → Rd
replicates. One can then reconstruct β ∗ from ˜β ∗ as β ∗
maps ˜β in v ∈ Rd , such that supp(v) ⊂ Gr and (vj )j∈Gr = ( ˜βj )j∈ ˜Gr
, for r = 1, . . . , B . The main
advantage of the above formulation relies on the possibility of using any state-of-the-art optimization
(cid:17)
(cid:16)|| ˜β || ˜Gr
(cid:17)
(cid:16)
procedure for group lasso. In terms of proximal methods, a possible solution is given by Algorithm
3, where Sτ /σ is the proximity operator of the new penalty, and can be computed exactly as
for j ∈ ˜Gr ,
− τ
˜βj ,
Sτ /σ ( ˜β )
σ
Algorithm 3 GL-prox
Given: ˜β 0 ∈ Rd , τ > 0, σ = || ˜ΨT ˜Ψ||/n
(cid:16)˜hp − (nσ)−1 ˜ΨT ( ˜Ψ˜hp − y)
(cid:17)
Initialize: p = 0, ˜h1 = ˜β 0 , t1 = 1
while convergence not reached do
(cid:113)
p := p + 1
˜β p = Sτ /σ
1
cp = (1 − tp )cp−1 ,
(−cp +
tp+1 =
p + 8cp )
c2
4
) + ˜βp−1 (tp − 1) tp+1
˜hp+1 = ˜β p (1 − tp+1 + tp+1
tp
tp

for r = 1, . . . , B .

(10)

(9)

=

j

+

end while
return ˜β p

Note that in principle, by applying Lemma 1, the group-soft-thresholding operator in (9) can be com-
puted only on the active groups. In practice this does not yield any advantage, since the identiﬁcation
of the active groups has the same computational cost of the thresholding itself.

3.5 Computational issues

For both GL-prox and GLO-pridu, the complexity of one iteration is the sum of the complexity of
computing the gradient of the data term and the complexity of computing the proximity operator
of the penalty term. The former has complexity O(dn) and O( ˜dn) for GLO-pridu and GL-prox,

5

respectively, for the case n < d. One should then add at each iteration, the cost of performing the
projection onto K . This can be neglected for the case of replicated variables.On the other hand,
the time complexity of one iteration for Algorithm 1 is driven by the number of active groups ˆB .
This number is typically small when looking for sparse solutions. The complexity is thus given
by the sum of the complexity of evaluating the inverse of the ˆB × ˆB matrix H , O( ˆB 3 ), and the
complexity of performing the product H −1∇g(λ), O( ˆB 2 ). The worst case complexity would then
be O( ˆB 3 ). Nevertheless, in practice the complexity is much lower because matrix H is highly
sparse. In fact, Equation (7) tells us that the part of matrix H corresponding to the active set I+ is
diagonal. As a consequence, if ˆB = ˆB− + ˆB+ , where ˆB− is the number of non active constraints,
and ˆB+ is the number of active constraints, then the complexity of inverting matrix H is at most
O( ˆB+ ) + O( ˆB 3− ). Furthermore the ˆB− × ˆB− non diagonal part of matrix H is highly sparse, since
Hr,s = 0 if ˆGr ∩ ˜Gs = ∅ and the complexity of inverting it is in practice much lower than O( ˆB 3− ).
The worst case complexity for computing the projection onto K is thus O(q · ˆB+ ) + O(q · ˆB 3− ),
where q is the number of iterations necessary to reach convergence. Note that even if, in order to
guarantee convergence, the tolerance for evaluating convergence of the inner iteration must decrease
with the number of external iterations, in practice, thanks to warm starting, we observed that q is
rarely greater than 10 in the experiments presented here.
Concerning the number of iterations required to reach convergence for GL-prox in the replicates
formulation, we empirically observed that it requires a much higher number of iterations than GLO-
pridu (see Table 3). We argue that such behavior is due to the combination of two occurences: 1) the
local condition number of matrix ˜Ψ is 0 even if Ψ is locally well conditioned, 2) the decomposition
of β ∗ as ˜β ∗ is possibly not unique, which is required in order to have a unique solution for (8).
The former is due to the presence of replicated columns in ˜Ψ. In fact, since Eτ is convex but not
necessarily strictly convex – as when n < d –, uniqueness and convergence is not always guaranteed
unless some further assumption is imposed. Most convergence results relative to (cid:96)1 regularization
link uniqueness of the solution as well as the rate of convergence of the Soft Thresholding Iteration
to some measure of local conditioning of the Hessian of the differentiable part of Eτ (see for instance
Proposition 4.1 in [11], where the Hessian restricted to the set of relevant variables is required to
be full rank). In our case the Hessian for GL-prox is simply ˜H = 1/n ˜ΨT ˜Ψ, so that, if the relevant
groups have non null intersection, then ˜H restricted to the set of relevant variables is by no means
full rank. Concerning the latter argument, we must say that in many real world problems, such as
bioinformatics, one cannot easily verify that the solution indeed has a unique decomposition. In
fact, we can think of trivial examples where the replicates formulation has not a unique solution.

4 Numerical Experiments

In this section we present numerical experiments aimed at comparing the running time performance
of GLO-pridu with state-of-the-art algorithms. To ensure a fair comparison, we ﬁrst run some pre-
liminary experiments to identify the fastest codes for group (cid:96)1 regularization with no overlap. We
refer to [6] for an extensive empirical and theoretical comparison of different optimization proce-
dures for solving (cid:96)1 regularization. Further empirical comparisons can be found in [15].

4.1 Comparison of different implementations for standard group lasso

We considered three algorithms which are representative of the optimization techniques used to
solve group lasso: interior-point methods, (group) coordinate descent and its variations, and prox-
imal methods. As an instance of the ﬁrst set of techniques we employed the publicly available
Matlab code at http://www.di.ens.fr/˜fbach/grouplasso/index.htm described
in [1]. For coordinate descent methods, we employed the R-package grlplasso, which imple-
ments block coordinate gradient descent minimization for a set of possible loss functions. In the
following we will refer to these two algorithms as “’GL-IP” and “GL-BCGD”. Finally we use our
Matlab implementation of Algorithm GL-prox as an instance of proximal methods.
We ﬁrst observe that the solutions of the three algorithms coincide up to an error which depends
on each algorithm tolerance. We thus need to tune each tolerance in order to guarantee that all
iterative algorithms are stopped when the level of approximation to the true solution is the same.

6

n = 100

GL-IP
GL-BCGD
GL-prox

n = 500

n = 1000

GL-IP
GL-BCGD
GL-prox

Table 1: Running time (mean and standard deviation) in seconds for computing the entire regular-
ization path of GL-IP, GL-BCGD, and GL-prox for different values of B , and n. For B = 1000,
GL-IP could not be computed due to memory reasons.
B = 10
B = 100 B = 1000
60 ± 90
5.6 ± 0.6
–
2.8 ± 0.6
14.4 ± 1.5
2.1 ± 0.6
183 ± 19
2.9 ± 0.4
0.21 ± 0.04
B = 10
B = 100
B = 1000
370 ± 30
2.30 ± 0.27
–
16.5 ± 1.2
4.7 ± 0.5
2.15 ± 0.16
2.54 ± 0.16
0.1514 ± 0.0025
109 ± 6
B = 10
B = 100 B = 1000
328 ± 22
1.92 ± 0.25
–
GL-IP
18 ± 3
20.6 ± 2.2
2.06 ± 0.26
GL-BCGD
0.182 ± 0.006
4.7 ± 0.5
112 ± 6
GL-prox
Toward this end, we run Algorithm GL-prox with machine precision, ν = 10−16 , in order to have
a good approximation of the asymptotic solution. We observe that for many values of n and d, and
over a large range of values of τ , the approximation of GL-prox when ν = 10−6 is of the same
order of the approximation of GL-IP with optparam.tol = 10−9 , and of GL-BCGD with tol =
10−12 . Note also that with these tolerances the three solutions coincide also in terms of selection,
i.e. their supports are identical for each value of τ . Therefore the following results correspond to
optparam.tol = 10−9 for GL-IP, tol = 10−12 for GL-BCGD, and ν = 10−6 for GL-prox.
For the other parameters of GL-IP we used the values used in the demos supplied with the code.
Concerning the data generation protocol, the input variables x = (x1 , . . . , xd ) are uniformly drawn
from [−1, 1]d . The labels y are computed using a noise-corrupted linear regression function, i.e. y =
β · x + w , where β depends on the ﬁrst 30 variables, βj = 1 if j = 1, . . . , 30, and 0 otherwise, w is an
additive gaussian white noise, and the signal to noise ratio is 5:1. In this case the dictionary coincides
with the variables, Ψj (x) = xj for j = 1, . . . , d. We then evaluate the entire regularization path for
the three algorithms with B sequential groups of 10 variables, (G1=[1, . . . , 10], G2=[11, . . . , 20],
and so on), for different values of n and B . In order to make sure that we are working on the correct
range of values for the parameter τ , we ﬁrst evaluate the set of solutions of GL-prox corresponding
to a large range of 500 values for τ , with ν = 10−4 . We then determine the smallest value of τ
which corresponds to selecting less than n variables, τmin , and the smallest one returning the null
solution, τmax . Finally we build the geometric series of 50 values between τmin and τmax , and use
it to evaluate the regularization path on the three algorithms. In order to obtain robust estimates of
the running times, we repeat 20 times for each pair n, B .
In Table 1 we report the computational times required to evaluate the entire regularization path for
the three algorithms. Algorithms GL-BCGD and GL-prox are always faster than GL-IP which, due
to memory reasons, cannot by applied to problems with more than 5000 variables, since it requires
to store the d × d matrix ΨT × Ψ. It must be said that the code for GP-IL was made available
mainly in order to allow reproducibility of the results presented in [1], and is not optimized in terms
of time and memory occupation. However it is well known that standard second-order methods are
typically precluded on large data sets, since they need to solve large systems of linear equations
to compute the Newton steps. GL-BCGD is the fastest for B = 1000, whereas GL-prox is the
fastest for B = 10, 100. The candidates as benchmark algorithms for comparison with GLO-pridu
are GL-prox and GL-BCGD. Nevertheless we observed that, when the input data matrix contains
a signiﬁcant fraction of replicated columns, this algorithm does not provide sparse solutions. We
therefore compare GLO-pridu with GL-prox only.

4.1.1 Projection vs duplication

The data generation protocol is equal to the one described in the previous experiments, but β depends
on the ﬁrst 12/5b variables (which correspond to the ﬁrst three groups)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
, 0, 0, . . . , 0
).
β = ( c, . . . , c
d−b·12/5 times
b·12/5 times

7

We then deﬁne B groups of size b, so that ˜d = B · b > d. The ﬁrst three groups correspond to the
subset of relevant variables, and are deﬁned as G1 = [1, . . . , b], G2 = [4/5b + 1, . . . , 9/5b], and
G3 = [1, . . . , b/5, 8/5b + 1, . . . , 12/5b], so that they have a 20% pair-wise overlap. The remaining
B − 3 groups are built by randomly drawing sets of b indexes from [1, d]. In the following we
will let n = 10|G1 ∪ G2 ∪ G3 |, i.e. n is ten times the number of relevant variables, and vary d, b.
We also vary the number of groups B , so that the dimension of the expanded space is α times the
input dimension, ˜d = αd, with α = 1.2, 2, 5. Clearly this amounts to taking B = α · d/b. The
parameter α can be thought of as the average number of groups a single variable belongs to. We
identify the correct range of values for τ as in the previous experiments, using GLO-pridu with loose
tolerance, and then evaluate the running time and the number of iterations necessary to compute the
entire regularization path for GL-prox on the expanded space and GLO-pridu, both with ν = 10−6 .
Finally we repeat 20 times for each combination of the three parameters d, b, and α.

d =1000
d =5000
d =10000

Table 2: Running time (mean ± standard deviation) in seconds for b = 10 (top), and b = 100 (below).
For each d and α, the left and right side correspond to GLO-pridu, and GL-prox, respectively.
α = 1.2
α = 2
α = 5
0.15 ± 0.04
0.20 ± 0.09
1.6 ± 0.9
5.1 ± 2.0
12.4 ± 1.3
68 ± 8
103 ± 12
2.4 ± 0.7
1.55 ± 0.29
1.0 ± 0.6
1.1 ± 0.4
790 ± 57
460 ± 110
3.0 ± 0.6
2.1 ± 0.7
2900 ± 400
2.1 ± 1.4
4.5 ± 1.4
α = 1.2
α = 2
α = 5
13.5 ± 0.7
11.6 ± 0.4
11.7 ± 0.4
24.1 ± 2.5
1467 ± 13
42 ± 4
1110 ± 80
85 ± 3
335 ± 21
90 ± 5
38 ± 15
31 ± 13
296 ± 16
90 ± 30
16.6 ± 2.1
13 ± 3
270 ± 120
–

d =1000
d =5000
d =10000

Table 3: Number of iterations (mean ± standard deviation) for b = 10 (top) and b = 100 (below).
For each d and α, the left and right side correspond to GLO-pridu, and GL-prox, respectively.

d =1000
d =5000
d =10000

d =1000
d =5000
d =10000

α = 1.2
80 ± 30
100 ± 30
100 ± 40
70 ± 30
100 ± 30
70 ± 40
α = 1.2
2160 ± 210
913 ± 12
600 ± 400
600 ± 300
81 ± 11
63 ± 11

α = 2
1900 ± 800
1200 ± 500
148 ± 25
139 ± 24
137 ± 26
160 ± 30
α = 2
2700 ± 300
894 ± 11
1860 ± 110
4590 ± 290
1000 ± 500
1800 ± 900

α = 5
11000 ± 1300
2150 ± 160
6600 ± 500
27000 ± 2000
49000 ± 6000
13300 ± 1900
α = 5
895 ± 10
4200 ± 400
6800 ± 500
1320 ± 30
2100 ± 60
–

Running times and number of iterations are reported in Table 2 and 3, respectively. When the degree
of overlap α is low the computational times of GL-prox and GLO-pridu are comparable. As α
increases, there is a clear advantage in using GLO-pridu instead of GL-prox. The same behavior
occurs for the number of iterations.

5 Discussion

We have presented an efﬁcient optimization procedure for computing the solution of group lasso
with overlapping groups of variables, which allows dealing with high dimensional problems with
large groups overlap. We have empirically shown that our procedure has a great computational
advantage with respect to state-of-the-art algorithms for group lasso applied on the expanded space
built by replicating variables belonging to more than one group. We also mention that computational
performance may improve if our scheme is used as core for the optimization step of active set
methods, such as [23]. Finally, as shown in [17], the improved computational performance enables
to use group (cid:96)1 regularization with overlap for pathway analysis of high-throughput biomedical data,
since it can be applied to the entire data set and using all the information present in online databases,
without pre-processing for dimensionality reduction.

8

References
[1] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine
Learning Research, 9:1179–1225, 2008.
[2] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning.
Technical Report HAL 00413473, INRIA, 2009.
[3] F. R. Bach, G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo
algorithm. In ICML, volume 69 of ACM International Conference Proceeding Series, 2004.
[4] A. Beck and Teboulle. M. Fast gradient-based algorithms for constrained total variation image
denoising and deblurring problems. IEEE Transactions on Image Processing, 18(11):2419–
2434, 2009.
[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sci., 2(1):183–202, 2009.
[6] S. Becker, J. Bobin, and E. Candes. Nesta: A fast and accurate ﬁrst-order method for sparse
recovery, 2009.
[7] D. Bertsekas. Projected newton methods for optimization problems with simple constraints.
SIAM Journal on Control and Optimization, 20(2):221–246, 1982.
[8] R. Brayton and J. Cullum. An algorithm for minimizing a differentiable function subject to. J.
Opt. Th. Appl., 29:521–558, 1979.
[9] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:28992934, December 2009.
[10] O. Guler. New proximal point algorithm for convex minimization. SIAM J. on Optimization,
2(4):649–664, 1992.
[11] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: Methodology
and convergence. SIOPT, 19(3):1107–1130, 2008.
[12] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML,
page 55, 2009.
[13] R. Jenatton, J.-Y . Audibert, and F. Bach. Structured variable selection with sparsity-inducing
norms. Technical report, INRIA, 2009.
[14] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical
dictionary learning. In Proceeding of ICML 2010, 2010.
[15] I. Loris. On the performance of algorithms for the minimization of l1 -penalized functionals.
Inverse Problems, 25(3):035008, 16, 2009.
[16] L. Meier, S. van de Geer, and P. Buhlmann. The group lasso for logistic regression. J. R.
Statist. Soc, B(70):53–71, 2008.
[17] S. Mosci, S. Villa, Verri A., and L. Rosasco. A fast algorithm for structured gene selection.
presented at MLSB 2010, Edinburgh.
[18] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of con-
vergence o(1/k2 ). Doklady AN SSSR, 269(3):543–547, 1983.
Smooth minimization of non-smooth functions. Math. Prog. Series A,
[19] Y. Nesterov.
103(1):127–152, 2005.
[20] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. J. R.
Statist. Soc. B, 69:659–677, 2007.
[21] L. Rosasco, M. Mosci, S. Santoro, A. Verri, and S. Villa.
Iterative projection methods for
structured sparsity regularization. Technical Report MIT-CSAIL-TR-2009-050, MIT, 2009.
[22] J. Rosen. The gradient projection method for nonlinear programming, part i: linear constraints.
J. Soc. Ind. Appl. Math., 8:181–217, 1960.
[23] V. Roth and B. Fischer. The group-lasso for generalized linear models: uniqueness of solutions
and efﬁcient algorithms. In Proceedings of 25th ICML, 2008.
[24] P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and
hierarchical variable selection. Annals of Statistics, 37(6A):3468–3497, 2009.

9


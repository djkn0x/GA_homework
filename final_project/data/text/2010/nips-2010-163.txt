Graph-Valued Regression

Han Liu Xi Chen John Lafferty Larry Wasserman

Carnegie Mellon University
Pittsburgh, PA 15213

Abstract

Undirected graphical models encode in a graph G the dependency structure of a
random vector Y . In many applications, it is of interest to model Y given an-
other random vector X as input. We refer to the problem of estimating the graph
G(x) of Y conditioned on X = x as “graph-valued regression”. In this paper,
we propose a semiparametric method for estimating G(x) that builds a tree on the
X space just as in CART (classiﬁ cation and regression trees), but at each leaf of
the tree estimates a graph. We call the method “Graph-optimized CART ”, or Go-
CART. We study the theoretical properties of Go-CART using dyadic partitioning
trees, establishing oracle inequalities on risk minimization and tree partition con-
sistency. We also demonstrate the application of Go-CART to a meteorological
dataset, showing how graph-valued regression can provide a useful tool for ana-
lyzing complex data.

1

Introduction

Let Y be a p-dimensional random vector with distribution P . A common way to study the structure
of P is to construct the undirected graph G = (V , E ), where the vertex set V corresponds to the
p components of the vector Y . The edge set E is a subset of the pairs of vertices, where an edge
between Yj and Yk is absent if and only if Yj is conditionally independent of Yk given all the
other variables. Suppose now that Y and X are both random vectors, and let P (· | X ) denote the
conditional distribution of Y given X . In a typical regression problem, we are interested in the
conditional mean µ(x) = E (Y | X = x). But if Y is multivariate, we may be also interested in
how the structure of P (· | X ) varies as a function of X . In particular, let G(x) be the undirected
graph corresponding to P (· | X = x). We refer to the problem of estimating G(x) as graph-valued
regression.
Let G = {G(x) : x ∈ X } be a set of graphs indexed by x ∈ X , where X is the domain of X .
Then G induces a partition of X , denoted as X1 , . . . , Xm , where x1 and x2 lie in the same partition
element if and only if G(x1 ) = G(x2 ). Graph-valued regression is thus the problem of estimating
the partition and estimating the graph within each partition element.

We present three different partition-based graph estimators; two that use global optimization, and
one based on a greedy splitting procedure. One of the optimization based schemes uses penalized
empirical risk minimization, the other uses held-out risk minimization. As we show, both methods
enjoy strong theoretical properties under relatively weak assumptions; in particular, we establish
oracle inequalities on the excess risk of the estimators, and tree partition consistency (under stronger
assumptions) in Section 4. While the optimization based estimates are attractive, they do not scale
well computationally when the input dimension is large. An alternative is to adapt the greedy algo-
rithms of classical CART, as we describe in Section 3. In Section 5 we present experimental results
on both synthetic data and a meteorological dataset, demonstrating how graph-valued regression can
be an effective tool for analyzing high dimensional data with covariates.

1

2 Graph-Valued Regression
Let y1 , . . . , yn be a random sample of vectors from P , where each yi ∈ R
p . We are interested in
the case where p is large and, in fact, may diverge with n asymptotically. One way to estimate G
from the sample is the graphical lasso or glasso [13, 5, 1], where one assumes that P is Gaussian
(cid:2)
(cid:3)
(cid:1)Ω = arg min
with mean µ and covariance matrix Σ. Missing edges in the graph correspond to zero elements in
the precision matrix Ω = Σ−1 [12, 4, 7]. A sparse estimate of Ω is obtained by solving
(cid:4)
tr(SΩ) − log |Ω| + λ(cid:2)Ω(cid:2)1
(1)
elementwise (cid:2)1 -norm of Ω. A fast algorithm for ﬁ nding (cid:1)Ω was given by Friedman et al. [5], which
Ω(cid:1)0
where Ω is positive deﬁ nite, S is the sample covariance matrix, and (cid:2)Ω(cid:2)1 =
j,k |Ωjk | is the
The theoretical properties of (cid:1)Ω have been studied by Rothman et al. [10] and Ravikumar et al. [9].
involves estimating a single row (and column) of Ω in each iteration by solving a lasso regression.
In practice, it seems that the glasso yields reasonable graph estimators even if Y is not Gaussian;
however, proving conditions under which this happens is an open problem.
We brieﬂ y mention three different strategies for estimating G(x), the graph of Y conditioned on
X = x, each of which builds upon the glasso.
 
!
Parametric Estimators. Assume that Z = (X, Y ) is jointly multivariate Gaussian with covariance
quantities (cid:1)ΣX , (cid:1)ΣY , and (cid:1)ΣX Y , and the marginal precision matrix of X , denoted as ΩX , can be
ΣX ΣX Y
matrix Σ =
. We can estimate ΣX , ΣY , and ΣX Y by their corresponding sample
ΣY X
ΣY
Gaussian formulas. In particular, the conditional covariance matrix of Y | X is (cid:1)ΣY |X = (cid:1)ΣY −
(cid:1)ΣX Y and a sparse estimate of (cid:1)ΩY |X can be obtained by directly plugging (cid:1)ΣY |X into glasso.
(cid:1)ΣY X
(cid:1)ΩX
estimated using the glasso. The conditional distribution of Y given X = x is obtained by standard
However, the estimated graph does not vary with different values of X .
Kernel Smoothing Estimators. We assume that Y given X is Gaussian, but without making any
assumption about the marginal distribution of X . Thus Y | X = x ∼ N (µ(x), Σ(x)). Under
(cid:6) (cid:2)x − xi(cid:2)
(cid:6) (cid:2)x − xi(cid:2)
(cid:7)
(cid:7)
(cid:8) n(cid:5)
n(cid:5)
the assumption that both µ(x) and Σ(x) are smooth functions of x, we estimate Σ(x) via kernel
smoothing:(cid:1)Σ(x) =
(yi − (cid:1)µ(x)) (yi − (cid:1)µ(x))T
K
K
h
h
(cid:6) (cid:2)x − xi(cid:2)
(cid:6) (cid:2)x − xi(cid:2)
(cid:7)
(cid:7)
i=1
i=1
where K is a kernel (e.g. the probability density function of the standard Gaussian distribution), (cid:2) · (cid:2)
(cid:8) n(cid:5)
n(cid:5)
(cid:1)µ(x) =
is the Euclidean norm, h > 0 is a bandwidth and
Now we apply glasso in (1) with S = (cid:1)Σ(x) to obtain an estimate of G(x). This method is appealing
K
K
.
yi
h
h
i=1
i=1
because it is simple and very similar to nonparametric regression smoothing; the method was ana-
lyzed for one-dimensional X in [14]. However, while it is easy to estimate G(x) at any given x, it
X1 , . . . , Xm . Within each Xj , we apply the glasso to get an estimated graph (cid:1)Gj . We then take
requires global smoothness of the mean and covariance functions.
In this approach, we partition X into ﬁ nitely many connected regions
(cid:1)G(x) = (cid:1)Gj for all x ∈ Xj . To ﬁ nd the partition, we appeal to the idea used in CART (classiﬁ cation
Partition Estimators.
and regression trees) [3]. We take the partition elements to be recursively deﬁ ned hyperrectangles.
As is well-known, we can then represent the partition by a tree, where each leaf node corresponds to
a single partition element. In CART, the leaves are associated with the means within each partition
element; while in our case, there will be an estimated undirected graph for each leaf node. We refer
to this method as Graph-optimized CART, or Go-CART. The remainder of this paper is devoted to
the details of this method.

3 Graph-Optimized CART
Let X ∈ R
d and Y ∈ R
p be two random vectors, and let {(x1 , y1 ), . . . , (xn , yn )} be n i.i.d. samples
from the joint distribution of (X, Y ). The domains of X and Y are denoted by X and Y respectively;

2

and for simplicity we take X = [0, 1]d . We assume that
Y | X = x ∼ Np (µ(x), Σ(x))
d → R
d → R
p×p is a matrix-valued
p is a vector-valued mean function and Σ : R
where µ : R
covariance function. We also assume that for each x, Ω(x) = Σ(x)−1 is a sparse matrix, i.e., many
a sparse inverse covariance (cid:1)Ω(x) to estimate Ω(x) for any x ∈ X ; in some situations the graph of
elements of Ω(x) are zero. In addition, Ω(x) may also be a sparse function of x, i.e., Ω(x) = Ω(xR )
for some R ⊂ {1, . . . , d} with cardinality |R| (cid:6) d. The task of graph-valued regression is to ﬁ nd
Ω(x) is of greater interest than the entries of Ω(x) themselves.
nected regions X1 , . . . , Xm , and within each Xj we apply the glasso to estimate a graph (cid:1)Gj . We
Go-CART is a partition based conditional graph estimator. We partition X into ﬁ nitely many con-
then take (cid:1)G(x) = (cid:1)Gj for all x ∈ Xj . To ﬁ nd the partition, we restrict ourselves to dyadic splits,
as studied by [11, 2]. The primary reason for such a choice is the computational and theoretical
tractability of dyadic partition based estimators.
Let T denote the set of dyadic partitioning trees (DPTs) deﬁ ned over X = [0, 1]d , where each
(cid:9)d
DPT T ∈ T is constructed by recursively dividing X by means of axis-orthogonal dyadic splits.
(cid:9)
] × (cid:9)
Each node of a DPT corresponds to a hyperrectangle in [0, 1]d . If a node is associated to the hyper-
rectangle A =
l=1 [al , bl ], then after being dyadically split along dimension k , the two children
l<k [al , bl ] × [ak , ak+bk
are associated with the sub-hyperrectangles A(k)
L =
l>k [al , bl ] and
A(k)
R = A\A(k)
L . Given a DPT T , we denote by Π(T ) = {X1 , . . . , XmT
} the partition of X induced
2
by the leaf nodes of T . For a dyadic integer N = 2K , we deﬁ ne TN to be the collection of all DPTs
such that no partition has a side length smaller than 2−K . Let I (·) denote the indicator function. We
mT(cid:5)
mT(cid:5)
denote µT (x) and ΩT (x) as the piecewise constant mean and precision functions associated with T :
· I (x ∈ Xj ) and ΩT (x) =
· I (x ∈ Xj ) ,
µXj
ΩXj
µT (x) =
j=1
j=1
∈ R
p×p are the mean vector and precision matrix for Xj .
∈ R
p and ΩXj
where µXj
Before formally deﬁ ning our graph-valued regression estimators, we require some further deﬁ ni-
tions. Given a DPT T with an induced partition Π(T ) = {Xj }mT
and its sample version (cid:1)R(T , µT , ΩT ) are deﬁ ned as follows:
j=1 and corresponding mean and
(cid:17)
(cid:10)(cid:11)
(cid:12)
(cid:14)(cid:15)
(cid:16)
mT(cid:5)
precision functions µT (x) and ΩT (x), the negative conditional log-likelihood risk R(T , µT , ΩT )
(cid:13)
(Y − µXj )(Y − µXj )T
− log |ΩXj
|
(cid:17)
(cid:10)(cid:11)
(cid:12)
(cid:14)(cid:15)
(cid:16)
n(cid:5)
mT(cid:5)
ΩXj
(cid:13)
tr
R(T , µT , ΩT ) =
,
E
(cid:1)R(T , µT , ΩT ) =
j=1
(cid:4)
− log |ΩXj
· I (xi ∈ Xj )
|
(yi − µXj )(yi − µXj )T
1
ΩXj
tr
.
(3)
n
j=1
i=1
Let [[T ]] > 0 denote a preﬁ x code over all DPTs T ∈ TN satisfying
T ∈TN 2−[[T ]] ≤ 1. One
such preﬁ x code [[T ]] is proposed in [11], and takes the form [[T ]] = 3|Π(T )| − 1 + (|Π(T )| −
1) log d/ log 2. A simple upper bound for [[T ]] is
[[T ]] ≤ (3 + log d/ log 2)|Π(T )|.
(4)
Our analysis will assume that the conditional means and precision matrices are bounded in the
(cid:2) · (cid:2)∞ and (cid:2) · (cid:2)1 norms; speciﬁ cally we suppose there is a positive constant B and a sequence
L1,n , . . . , LmT ,n , where each Lj,n ∈ R+ is a function of the sample size n, and we deﬁ ne the
(cid:3)
(cid:2)
domains of each µXj and ΩXj as
Mj = {µ ∈ R
p : (cid:2)µ(cid:2)∞ ≤ B } ,
Ω ∈ R
p×p : Ω is positive deﬁ nite, symmetric, and (cid:2)Ω(cid:2)1 ≤ Lj,n
Λj =
(cid:21)
(cid:20) (cid:1)R(T , µT , ΩT ) + pen(T )
(cid:18)(cid:1)µ bXj
(cid:19)m bT
With this notation in place, we can now deﬁ ne two estimators.
(cid:1)T ,
, (cid:1)Ω bXj
Deﬁ nition 1. The penalized empirical risk minimization Go-CART estimator is deﬁ ned as
(cid:22)
where (cid:1)R is deﬁ ned in (3) and pen(T ) = γn · mT
= argminT ∈TN ,µXj
∈Mj ,ΩXj
j=1
[[T ]] log 2+2 log(np)
.
n

· I (X ∈ Xj )

∈Λj

(2)

.

(5)

3

Empirically, we may always set the dyadic integer N to be a reasonably large value; the regulariza-
tion parameter γn is responsible for selecting a suitable DPT T ∈ TN .
We also formulate an estimator that minimizes held-out risk. Practically, we could split the data into
two partitions: D1 = {(x1 , y1 ), . . . , (xn1 , yn1 )} for training and D2 = {((x(cid:4)
n2 ))}
1 ), . . . , (x(cid:4)
, y (cid:4)
, y (cid:4)
(cid:1)Rout (T , µT , ΩT ) =
n2
1
for validation with n1 + n2 = n. The held-out negative log-likelihood risk is then given by
(cid:19)
(cid:16)
(cid:12)
(cid:18)(cid:11)
(cid:14)(cid:15)
n2(cid:5)
mT(cid:5)
(cid:13)
· I (x(cid:4)
i ∈ Xj )
|
i − µXj )(y (cid:4)
i − µXj )T
− log |ΩXj
1
(y (cid:4)
ΩXj
tr
.
(6)
n2
(cid:1)R(T , µT , ΩT )
(cid:1)µT , (cid:1)ΩT = argminµXj
j=1
i=1
Deﬁ nition 2. For each DPT T deﬁ ne
where (cid:1)R is deﬁ ned in (3) but only evaluated on D1 = {(x1 , y1 ), . . . , (xn1 , yn1 )}. The held-out risk
∈Λj
∈Mj ,ΩXj
(cid:1)T = argminT ∈TN
(cid:1)Rout (T , (cid:1)µT , (cid:1)ΩT ).
where (cid:1)Rout is deﬁ ned in (6) but only evaluated on D2 .
minimization Go-CART estimator is
The above procedures require us to ﬁ nd an optimal dyadic partitioning tree within TN . Although
( (cid:1)T , (cid:1)µT , (cid:1)ΩT ). We focus on the held-out risk minimization form as in Deﬁ nition 2, due to its superior
dynamic programming can be applied, as in [2], the computation does not scale to large input dimen-
sions d. We now propose a simple yet effective greedy algorithm to ﬁ nd an approximate solution
empirical performance. But note that our greedy approach is generic and can easily be adapted to
the penalized empirical risk minimization form.
n1(cid:5)
First, consider the simple case that we are given a dyadic tree structure T which induces a partition
Π(T )={X1 , . . . , XmT
} on X . For any partition element Xj , we estimate the sample mean using D1 :
(cid:4)n1
(cid:1)µXj =
yi · I (xi ∈ Xj ) .
1
The glasso is then used to estimate a sparse precision matrix (cid:1)ΩXj . More precisely, let (cid:1)ΣXj be the
I (xi ∈ Xj )
i=1
i=1
n1(cid:5)
(cid:14)T · I (xi ∈ Xj ) .
(cid:13)
(cid:14) (cid:13)
sample covariance matrix for the partition element Xj , given by
(cid:1)ΣXj =
(cid:4)n1
yi − (cid:1)µXj
yi − (cid:1)µXj
1
The estimator (cid:1)ΩXj is obtained by optimizing (cid:1)ΩXj = arg minΩ(cid:1)0
{tr( (cid:1)ΣXj Ω) − log |Ω| + λj (cid:2)Ω(cid:2)1 },
I (xi ∈ Xj )
i=1
i=1
where λj is in one-to-one correspondence with Lj,n in (5). In practice, we run the full regularization
path of the glasso, from large λj , which yields very sparse graph, to small λj , and select the graph
that minimizes the held-out negative log-likelihood risk. To further improve the model selection per-
formance, we reﬁ
t the parameters of the precision matrix after the graph has been selected. That is,
to reduce the bias of the glasso, we ﬁ rst estimate the sparse precision matrix using (cid:2)1 -regularization,
t the Gaussian model without (cid:2)1 -regularization, but enforcing the sparsity pattern
and then we reﬁ
obtained in the ﬁ rst step.
The natural, standard greedy procedure starts from the coarsest partition X = [0, 1]d and then
computes the decrease in the held-out risk by dyadically splitting each hyperrectangle A along
dimension k ∈ {1, . . . d}. The dimension k∗
∆ (cid:1)R(k)
, (cid:1)ΩA(k)
, (cid:1)ΩA(k)
out (A, (cid:1)µA , (cid:1)ΩA ) = (cid:1)Rout (A, (cid:1)µA , (cid:1)ΩA ) − (cid:1)Rout (A(k)
) − (cid:1)Rout (A(k)
R , (cid:1)µA(k)
L , (cid:1)µA(k)
that results in the largest decrease in held-out risk is
selected, where the change in risk is given by
).
If splitting any dimension k of A leads to an increase in the held-out risk, the element A should no
L
L
R
R
longer be split and hence becomes a partition element of Π(T ). The details and pseudo code are
provided in the supplementary materials.

This greedy partitioning method parallels the classical algorithms for classiﬁ cation and regression
that have been used in statistical learning for decades. However, the strength of the procedures given
in Deﬁ nitions 1 and 2 is that they lend themselves to a theoretical analysis under relatively weak
assumptions, as we show in the following section. The theoretical properties of greedy Go-CART
are left to future work.

4

R(T , µT , ΩT ).

4 Theoretical Properties
over TN as
We deﬁ ne the oracle risk R∗
R∗
= R(T ∗ , µ∗
∗
T , Ω
T ) =
inf
∈Λj
∈Mj ,ΩXj
T ∈TN ,µXj
Note that T ∗
T ∗ , and Ω∗
, µ∗
T ∗ might not be unique, since the ﬁ nest partition always achieves the oracle
risk. To obtain oracle inequalities, we make the following two technical assumptions.
Assumption 1. Let T ∈ TN be an arbitrary DPT which induces a partition Π(T ) =
{X1 , . . . , XmT
} on X , we assume that there exists a constant B , such that
log |Ω| ≤ Ln
(cid:2)∞ ≤ B and max
(cid:2)µXj
sup
max
1≤j≤mT
1≤j≤mT
Ω∈Λj
where Λj is deﬁ ned in (5) and Ln = max1≤j≤mT
Lj,n , where Lj,n is the same as in (5). We also
√
assume that
n).
Ln = o(
Assumption 2. Let Y = (Y1 , . . . , Yp )T ∈ R
p . For any A ⊂ X , we deﬁ ne
Zk(cid:1) (A) = Yk Y(cid:1) · I (X ∈ A) − E(Yk Y(cid:1) · I (X ∈ A))
Zj (A) = Yj · I (X ∈ A) − E(Yj · I (X ∈ A)).
We assume there exist constants M1 , M2 , v1 , and v2 , such that
j,A E|Zj (A)|m ≤ m!M m−2
k,(cid:1),A E|Zk(cid:1) (A)|m ≤ m!M m−2
v2
v1
sup
and sup
2
1
2
2
for all m ≥ 2.
δ ∈ (0, 1/4), let (cid:1)T , (cid:1)µ bT , (cid:1)Ω bT be the estimator obtained using the penalized empirical risk minimiza-
Theorem 1. Let T ∈ TN be a DPT that induces a partition Π(T ) = {X1 , . . . , XmT
} on X . For any
(cid:23)
tion Go-CART in Deﬁ nition 1, with a penalty term pen(T ) of the form
[[T ]] log 2 + 2 log p + log(48/δ)
(cid:25)
(cid:24)
pen(T ) = (C1 + 1)LnmT
√
√
n
R( (cid:1)T , (cid:1)µ bT , (cid:1)Ω bT ) − R∗ ≤ inf
v1 + B 2 . Then for sufﬁ ciently large n, the excess risk inequality
where C1 = 8
v2 + 8B
(R(T , µT , ΩT ) − R∗
)
2pen(T ) +
T ∈TN
holds with probability at least 1 − δ .

µXj

inf
∈Mj ,ΩXj

∈Λj

} on X . We

A similar oracle inequality holds when using the held-out risk minimization Go-CART.
(cid:23)
Theorem 2. Let T ∈ TN be a DPT which induces a partition Π(T ) = {X1 , . . . , XmT
deﬁ ne φn (T ) to be a function of n and T such that
√
[[T ]] log 2 + 2 log p + log(384/δ)
2)LnmT
φn (T ) = (C2 +
√
√
√
n
n/2. Let (cid:1)T , (cid:1)µ bT , (cid:1)Ω bT be the estimator constructed using the held-out risk minimization criterion of
2B 2 and Ln = max1≤j≤mT
where C2 = 8
2v2 + 8B
2v1 +
Lj,n . Partition the data into
D1 = {(x1 , y1 ), . . . , (xn1 , yn1 )} and D2 = {(x(cid:4)
n2 )} with sizes n1 = n2 =
, y (cid:4)
, y (cid:4)
1 ), . . . , (x(cid:4)
(cid:25)
(cid:24)
n2
1
R( (cid:1)T , (cid:1)µ bT , (cid:1)Ω bT ) − R∗ ≤ inf
+ φn ( (cid:1)T )
Deﬁ nition 2. Then, for sufﬁ ciently large n, the excess risk inequality
(R(T , µT , ΩT ) − R∗
)
inf
3φn (T ) +
∈Mj ,ΩXj
T ∈TN
∈Λj
µXj
with probability at least 1 − δ .
due to the extra φn ( (cid:1)T ) term, which depends on the complexity of the ﬁ nal estimate (cid:1)T . Due to space
Note that in contrast to the statement in Theorem 1, Theorem 2 results in a stochastic upper bound
limitations, the proofs of both theorems are detailed in the supplementary materials.
We now temporarily make the strong assumption that the model is correct, so that Y given X is
conditionally Gaussian, with a partition structure that is given by a dyadic tree. We show that with
high probability, the true dyadic partition structure can be correctly recovered.

5

Assumption 3. The true model is
Y | X = x ∼ Np (µ∗
∗
T ∗ (x))
T ∗ (x), Ω
mT ∗(cid:5)
mT ∗(cid:5)
where T ∗ ∈ TN is a DPT with induced partition Π(T ∗ ) = {X ∗
j }mT ∗
j=1 and
j I (x ∈ X ∗
j I (x ∈ X ∗
µ∗
µ∗
∗
∗
Ω
j ), Ω
T ∗ (x) =
T ∗ (x) =
j ).
j=1
j=1

(7)

R(T , µT , ΩT ),

(cid:19)
.

∈ Λj

Under this assumption, clearly
R(T ∗ , µ∗
∗
T ∗ , Ω
T ∗ ) =
inf
(cid:18)
mT(cid:5)
mT(cid:5)
T ∈TN ,µT ,ΩT ∈MT
where MT is given by
MT =
I (x ∈ Xj ), Ω(x) =
I (x ∈ Xj ) : µXj
∈ Mj , ΩXj
µXj
ΩXj
µ(x) =
j=1
j=1
Deﬁ nition 3. A tree estimation procedure (cid:1)T is tree partition consistent in case
Let T1 and T2 be two DPTs, if Π(T1 ) can be obtained by further split the hyperrectangles within
Π(T2 ), we say Π(T2 ) ⊂ Π(T1 ). We then have the following deﬁ nitions:
(cid:11)
(cid:16)
) ⊂ Π( (cid:1)T )
→ 1 as n → ∞.
Π(T ∗
P
Note that the estimated partition may be ﬁ ner than the true partition. Establishing a tree parti-
tion consistency result requires further technical assumptions. The following assumption speciﬁ es
that for arbitrary adjacent subregions of the true dyadic partition, either the means or the variances
should be sufﬁ ciently different. Without such an assumption, of course, it is impossible to detect the
boundaries of the true partition.
Assumption 4. Let X ∗
i and X ∗
j be adjacent partition elements of T ∗
, so that they have a common
(cid:26)(cid:26)(cid:26)(cid:26)(cid:26) − log |Σ
(cid:26)(cid:26)(cid:26)(cid:26)(cid:26) Σ∗
= (Ω∗
. Let Σ∗
parent node within T ∗
)−1 . We assume there exist positive constants c1 , c2 , c3 , c4 ,
X ∗
X ∗
i
i
such that either
+ Σ∗
X ∗
X ∗
∗
2 log
i
j
X ∗
2
i
≥ c3 . We also assume
∀j = 1, . . . , mT ∗ ,
) ≥ c1 ,
∗
ρmin (Ω
X ∗
where ρmin (·) denotes the smallest eigenvalue. Furthermore, for any T ∈ TN and any A ∈ Π(T ),
j
we have P (X ∈ A) ≥ c2 .

| − log |Σ
∗
X ∗
j

or (cid:2)µ∗
X ∗
i

− µ∗
X ∗
j

(cid:2)2
2

| ≥ c4

Theorem 3. Under the above assumptions, we have
, c2 c4 }
T ∗ ) > min{ c1 c2 c3
R(T , µT , ΩT ) − R(T ∗ , µ∗
∗
inf
T ∗ , Ω
inf
µT , ΩT ∈MT
T ∈TN , Π(T ∗ )(cid:1)Π(T )
2
where c1 , c2 , c3 , c4 are deﬁ ned in Assumption 4. Moreover, the Go-CART estimator in both the
penalized risk minimization and held-out risk minimization form is tree partition consistent.
This result shows that, with high probability, we obtain a ﬁ ner partition than T ∗
; the assumptions
do not, however, control the size of the resulting partition. The proof of this result appears in the
supplementary material.

5 Experiments

We now present the performance of the greedy partitioning algorithm of Section 3 on both synthetic
data and a real meteorological dataset. In the experiment, we always set the dyadic integer N = 210
to ensure that we can obtain ﬁ ne-tuned partitions of the input space X .
5.1 Synthetic Data
We generate n data points x1 , . . . , xn ∈ R
d with n = 10, 000 and d = 10 uniformly distributed on
the unit hypercube [0, 1]d . We split the square [0, 1]2 deﬁ ned by the ﬁ rst two dimension of the unit

6

20

1

2

3

19

18

17

16

15

14

13

12

11

9

10

4

8

5

6

7

 1

20

1

2

19

18

3

4

5

6

7

8

 X1<
 0.5

17

16

15

1

20

2

14

3

4

13

12

11

9

10

5

6

7

8

12

11

9

10

 2
 X2<
 0.5

 4

17

16

15

19

18

14

13

 X1>
 0.5

20

1

2

3

5

6

7

4

8

12

11

9

10

19

18

17

16

15

14

13

 3

 X2>
 0.5

 7

 X2<
 0.25

 X2>
 0.25

 X1<
 0.75

 X1>
 0.75

 8
 X1<
 0.25

 9

 X1>
 0.25

 10
 X2<
 0.75

 11
 X2>
 0.75

 X2>
 0.5

 X2<
 0.5

20

1

2

3

9

5

6

7

4

8

12

10

11

1

20

2

19

18

3

4

17

16

15

5

6

7

14

13

8

9

12

11

10

20

1

2

 12

 X2<
 0.125

 X2>
 0.125

 X1>
 0.25

 X1<
 0.25

 15

 X1<
 0.375

 X1>
 0.375

 16

 X2<
 0.625

 X2>
 0.625

 X2>
 0.75

 X2<
 0.75

 19

 X1<
 0.875

 X1>
 0.875

 20
 X1<
 X1>
 0.125
 0.125

 21
 X1<
 X1>
 0.125
 0.125

 22
 X2<
 X2>
 0.375
 0.375

 23
 X2<
 X2>
 0.375
 0.375

 24
 X1<
 X1>
 0.625
 0.625

 25
 X1<
 X1>
 0.625
 0.625

 26
 X2<
 X2>
 0.875
 0.875

 27
 X2<
 X2>
 0.875
 0.875

 28

 29

 30

 31

 13

 14

 32

 33

 34

 35

 5

 6

 36

 37

 38

 39

 17

 18

 40

 41

 42

 43

14

13

9

12

11

10

19

18

17

16

15

14

3

13

4

5

6

7

8

9

1

20

2

19

13

2

12

11

10

17

16

15

18

14

1

20

19

3

9

4

8

5

6

7

5

6

7

4

8

18

17

16

15

14

13

19

18

17

16

15

12

11

10

20

1

2

3

14

13

12

11

9

10

17

16

15

19

18

14

13

12

10

11

1

20

19

18

17

16

15

3

9

2

5

6

7

4

8

3

4

5

6

7

8

21.3

21.2

21.1

21

20.9

k
s
i
R
 
t
u
o
−
d
l
e
H

20.8
0

5

41

40

43

42

18

17

38 39

36 37

33

32

35

34

13

14

30 31

28 29

6

(b)

20

1

2

1

20

2

20

1

2

17

16

15

19

18

14

13

3

4

19

18

5

6

7

8

17

16

15

14

13

12

11

9

10

3

9

5

6

7

4

8

17

16

15

19

18

14

13

3

4

8

9

5

6

7

12

11

10

12

11

10

20

1

2

3

19

18

17

16

15

14

13

12

11

9

10

4

8

5

6

7

20

19

1

2

3

18

17

16

15

14

13

9

12

10

11

4

8

5

6

7

17

16

15

19

18

14

13

20

1

2

3

4

5

6

7

8

9

12

10

11

25

20
15
10
5
Splitting Sequence No.
(c)

(a)
Figure 1: Analysis of synthetic data. (a) Estimated dyadic tree structure; (b) Ground true partition. The hori-
zontal axis corresponds to the ﬁ rst dimension denoted as X1 while the vertical axis corresponds to the second
dimension denoted by X2 . The bottom left point corresponds to [0, 0] and the upper right point corresponds to
[1, 1]. It is also the induced partition on [0, 1]2 . The number labeled on each subregion corresponds to each leaf
node ID of the tree in (a); (c) The held-out negative log-likelihood risk for each split. The order of the splits
corresponds the ID of the tree node (from small to large).
hypercube into 22 subregions as shown in Figure 1 (b). For the t-th subregion where 1 ≤ t ≤ 22,
we generate an Erd ¨os-R ´enyi random graph Gt = (V t , E t ) with the number of vertices p = 20,
the number of edges |E | = 10 and the maximum node degree is four. Based on Gt , we generate
i,j = I (i = j ) + 0.245 · I ((i, j ) ∈ E t ), where
the inverse covariance matrix Ωt according to Ωt
(cid:13)
(cid:13)
(cid:14)−1
(cid:14)
0.245 guarantees the positive deﬁ niteness of Ωt when the maximum node degree is 4. For each data
point xi in the t-th subregion, we sample a 20-dimensional response vector yi from a multivariate
Ωt
0,
Gaussian distribution N20
. We also create an equally-sized held-out dataset in the same
manner based on {Ωt }22
t=1 .
The learned dyadic tree structure and its induced partition are presented in Figure 1. We also provide
the estimated graphs for some nodes. We conduct 100 monte-carlo simulations and ﬁ nd that 82 times
out of 100 runs our algorithm perfectly recover the ground true partitions on the X1 -X2 plane and
never wrongly split any irrelevant dimensions ranging from X3 to X10 . Moreover, the estimated
graphs have interesting patterns. Even though the graphs within each subregion are sparse, the
estimated graph obtained by pooling all the data together is highly dense. As the greedy algorithm
proceeds, the estimated graphs become sparser and sparser. However, for the immediate parent
of the leaf nodes, the graphs become denser again. Out of the 82 simulations where we correctly
identify the tree structure, we list the graph estimation performance for subregions 28, 29, 13, 14, 5,
6 in terms of precision, recall, and F1-score in Table 1.

Table 1: The graph estimation performance over different subregions
Mean values over 100 runs (Standard deviation)

subregion

region 28

region 29

region 13

region 14

region 5

region 6

Precision
Recall
F1 − score

0.8327 (0.15)
0.7890 (0.16)
0.7880 (0.11)

0.8429 (0.15)
0.7990 (0.18)
0.7923 (0.12)

0.9853 (0.04)
1.0000 (0.00)
0.9921 (0.02)

0.9821 (0.05)
1.0000 (0.00)
0.9904 (0.03)

0.9906 (0.04)
1.0000 (0.00)
0.9949 (0.02)

0.9899 (0.05)
1.0000 (0.00)
0.9913 (0.02)

We see that for a larger subregion (e.g. 13, 14, 5, 6), it is easier to obtain better recovery perfor-
mance; while good recovery for a very small region (e.g. 28, 29) becomes more challenging. We
also plot the held-out risk in the subplot (c). As can be seen, the ﬁ rst few splits lead to the most
signiﬁ cant decreases of the held-out risk. The whole risk curve illustrates a diminishing return be-
havior. Correctly splitting the large rectangle leads to a signiﬁ cant decrease in the risk; in contrast,
splitting the middle rectangles does not reduce the risk as much. We also conducted simulations
where the true conditional covariance matrix is a continuous function of x; these are presented in
the supplementary materials.

7

17

16

42

44

41

43

18 19

21

20

60

62

9

8

59

61

6

39 40

46

48

45

47

51 52

56

58

49 50

55

57

24

23

26

25

66

28

65

64

27

63

15

5

13

34

36

14

38

31

32

33

35

37

4

3

10

12

29

30

11

53

54

22

1

2

7

CO2
DIR

CH4

CO2
DIR

CH4

CO2

DIR

CH4

GLO

CO

GLO

CO

GLO

CO

TMX

TMP

TMN

DTR

TMX

H2

WET

TMP

CLD

TMN

H2

WET

CLD

TMX

TMP

TMN

H2

WET

CLD

VAP

DTR

VAP

FRS

PRE

PRE
FRS

DTR

VAP

FRS

PRE

CO2

DIR

CH4

GLO

CO

TMX

TMP

TMN

DTR

H2

WET

CLD

VAP

PRE
FRS
(b)

42

44

16

17

18

19

41

43

21

20

8

60

62

59

61

6

46

48

51

52

56

58

24

15

39

40

45

47

49

50

55

57

23

66

65

27

28

64

63

9

26

25

5

34

36

13

31

32

33

35

14

38

37

12

29

30

1

2

4

3

10

11

53

54

22

7

(c)
(a)
Figure 2: Analysis of climate data. (a) Learned partitions for the 100 locations and projected to the US map,
with the estimated graphs for subregions 3, 10, and 33; (b) Estimated graph with data pooled from all 100
locations; (c) the re-scaled partition pattern induced by the learned dyadic tree structure.

5.2 Climate Data Analysis
In this section, we apply Go-CART on a meteorology dataset collected in a similar approach as in
[8]. The data contains monthly observations of 15 different meteorological factors from 1990 to
2002. We use the data from 1990 to 1995 as the training data and data from 1996 to 2002 as the
held-out validation data. The observations span 100 locations in the US between latitudes 30.475 to
47.975 and longitudes -119.75 to -82.25. The 15 meteorological factors measured for each month
include levels of CO2 , CH4 , H2 , CO, average temperature (TMP) and diurnal temperature range
(DTR), minimum temperate (TMN), maximum temperature (TMX), precipitation (PRE), vapor (VAP),
cloud cover (CLD), wet days (WET), frost days (FRS), global solar radiation (GLO), and direct solar
radiation (DIR).

As a baseline, we estimate a sparse graph on the data pooled from all 100 locations, using the glasso
algorithm; the estimated graph is shown in Figure 2 (b). It is seen that the greenhouse gas factor
CO2 is isolated from all the other factors. This apparently contradicts the basic domain knowledge
that CO2 should be correlated with the solar radiation factors (including GLO, DIR), according to
the IPCC report [6] which is one of the most authoritative reports in the ﬁ eld of meteorology. The
reason for the missing edges in the pooled data may be that positive correlations at one location are
canceled by negative correlations at other locations.
Treating the longitude and latitude of each site as two-dimensional covariate X , and the meteorology
data of the p = 15 factors as the response Y , we estimate a dyadic tree structure using the greedy
algorithm. The result is a partition with 66 subregions, shown in Figure 2. The graphs for subregions
3 and 10 (corresponding to the coast of California and Arizona states) are shown in subplot (a)
of Figure 2. The graphs for these two adjacent subregions are quite similar, suggesting spatial
smoothness of the learned graphs. Moreover, for both graphs, CO2 is connected to the solar radiation
factor GLO through CH4 .
In contrast, for subregion 33, which corresponds to the north part of
Arizona, the estimated graph is quite different. In general, it is found that the graphs corresponding
to the locations along the coasts are sparser than those corresponding to the locations in the mainland.

Such observations, which require validation and interpretation by domain experts, are examples of
the capability of graph-valued regression to provide a useful tool for high dimensional data analysis.

8

References

[1] O. Banerjee, L. E. Ghaoui, and A. d’Aspremont. Model selection through sparse maximum
likelihood estimation. Journal of Machine Learning Research, 9:485 –516, March 2008.
[2] G. Blanchard, C. Sch ¨afer, Y. Rozenholc, and K.-R. M ¨uller. Optimal dyadic decision trees.
Mach. Learn., 66(2-3):209–241, 2007.
[3] L. Breiman, J. Friedman, C. J. Stone, and R. Olshen. Classiﬁ cation and regression trees.
Wadsworth Publishing Co Inc, 1984.
[4] D. Edwards. Introduction to graphical modelling. Springer-Verlag Inc, 1995.
[5] J. H. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the
graphical lasso. Biostatistics, 9(3):432–441, 2007.
[6] IPCC. Climate Change 2007–The Physical Science Basis IPCC Fourth Assessment Report.
[7] S. L. Lauritzen. Graphical Models. Oxford University Press, 1996.
[8] A. C. Lozano, H. Li, A. Niculescu-Mizil, Y. Liu, C. Perlich, J. Hosking, and N. Abe. Spatial-
temporal causal modeling for climate change attribution. In ACM SIGKDD, 2009.
[9] P. Ravikumar, M. Wainwright, G. Raskutti, and B. Yu. Model selection in Gaussian graph-
ical models: High-dimensional consistency of (cid:2)1 -regularized MLE.
In Advances in Neural
Information Processing Systems 22, Cambridge, MA, 2009. MIT Press.
[10] A. J. Rothman, P. J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance
estimation. Electronic Journal of Statistics, 2:494 –515, 2008.
[11] C. Scott and R. Nowak. Minimax-optimal classiﬁ cation with dyadic decision trees. Information
Theory, IEEE Transactions on, 52(4):1335–1353, 2006.
[12] J. Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley, 1990.
[13] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model.
Biometrika, 94(1):19–35, 2007.
[14] S. Zhou, J. Lafferty, and L. Wasserman. Time varying undirected graphs. Machine Learning,
78(4), 2010.

9


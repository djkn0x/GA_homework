Learning via Gaussian Herding

Koby Crammer
Department of Electrical Enginering
The Technion
Haifa, 32000 Israel
koby@ee.technion.ac.il

Daniel D. Lee
Dept. of Electrical and Systems Engineering
University of Pennsylvania
Philadelphia, PA 19104
ddlee@seas.upenn.edu

Abstract

We introduce a new family of online learning algorithms based upon constraining
the velocity ﬂow over a distribution of weight vectors. In particular, we show how
to effectively herd a Gaussian weight vector distribution by trading off velocity
constraints with a loss function. By uniformly bounding this loss function, we
demonstrate how to solve the resulting optimization analytically. We compare the
resulting algorithms on a variety of real world datasets, and demonstrate how these
algorithms achieve state-of-the-art robust performance, especially with high label
noise in the training data.

1

Introduction

Online learning algorithms are simple, fast, and require less memory compared to batch learning
algorithms. Recent work has shown that they can also perform nearly as well as batch algorithms in
many settings, making them quite attractive for a number of large scale learning problems [3]. The
success of an online learning algorithm depends critically upon a tradeoff between ﬁtting the current
data example and regularizing the solution based upon some memory of prior hypotheses. In this
work, we show how to incorporate regularization in an online learning algorithm by constraining the
motion of weight vectors in the hypothesis space. In particular, we demonstrate how to use simple
constraints on the velocity ﬂow ﬁeld of Gaussian-distributed weight vectors to regularize online
learning algorithms. This process results in herding the motion of the Gaussian weight vectors to
yield algorithms that are particularly robust to noisy input data.
Recent work has demonstrated how parametric information about the weight vector distribution can
be used to guide online learning [1]. For example, conﬁdence weighted (CW) learning maintains a
Gaussian distribution over linear classiﬁer hypotheses and uses it to control the direction and scale
of parameter updates [9]. CW learning has formal guarantees in the mistake-bound model [7];
however, it can over-ﬁt in certain situations due to its aggressive update rules based upon a separable
data assumption. A newer online algorithm, Adaptive Regularization of Weights (AROW) relaxes
this separable assumption, resulting in an adaptive regularization for each training example based
upon its current conﬁdence [8]. This regularization comes in the form of minimizing a bound on the
Kullback-Leibler divergence between Gaussian distributed weight vectors.
Here we take a different microscopic view of the online learning process. Instead of reweighting and
diffusing the weight vectors in hypothesis space, we model them as ﬂowing under a velocity ﬁeld
given by each data observation. We show that for linear velocity ﬁelds, a Gaussian weight vector
distribution will maintain its Gaussianity, with corresponding updates for its mean and covariance.
The advantage of this approach is that we can incorporate different constraints and regularization
on the resulting velocity ﬁelds to yield more robust online learning algorithms. In the remainder
of this paper, we elucidate the details of our approach and compare its performance on a variety of
experimental data.

1

These algorithms maintain a Gaussian distribution over possible weight vectors in hypothesis space.
In traditional stochastic ﬁltering, weight vectors are ﬁrst reweighted according to how accurately
they describe the current data observation. The remaining distribution is then subjected to random
diffusion, resulting in a new distribution. When the reweighting factor depends linearly upon the
weight vector in combination with a Gaussian diffusion model, a weight vector distribution will
maintain its Gaussianity under such a transformation. The Kalman ﬁlter equations then yield the
resulting change in the mean and covariance of the new distribution. Our approach, on the other
hand, updates the weight vector distribution with each observation by herding the weight vectors
using a velocity ﬁeld. The differences between these two processes are shown in Fig. 1.
2 Background

wi+1 = arg min
w

Consider the following online binary classiﬁcation
problem, that proceeds in rounds. On the ith round
the online algorithm receives an input xi ∈ Rd and
applies its current prediction rule to make a predic-
tion ˆyi ∈ Y , for the binary set Y = {−1, +1}. It
then receives the correct label yi ∈ Y and suffers a
loss (cid:96)(yi , ˆyi ). At this point, the algorithm updates its
prediction rule with the pair (xi , yi ) and proceeds to
the next round. A summary of online algorithms can
be found in [2].
An initial description for possible online algorithms
is provided by the family of passive-aggressive (PA)
Figure 1:
algorithms for linear classiﬁers [5]. The weight vec-
(a) Traditional stochastic ﬁlter-
ing: weight vectors in the hypothesis space are
tor wi at each round is updated with the current in-
reweighted according to the new observation and
put xi and label yi , by optimizing:
undergo diffusion resulting in a new weight vec-
(cid:107)w − wi(cid:107)2 + C (cid:96) ((xi , yi ), w) ,
1
tor distribution. (b) Herding via a velocity ﬁeld:
weights vectors ﬂow in hypothesis space accord-
2
(1)
ing to a constrained velocity ﬁeld, resulting in a
where (cid:96) ((xi , yi ), w) is the squared- or hinge-loss
new weight vector distribution.
function and C > 0 controls the tradeoff between
, αi = (cid:0)max{0, 1 − yi (w(cid:62)
i xi )}(cid:1) / (cid:0)(cid:107)xi(cid:107)2 + 1/C (cid:1) .
optimizing the current loss and being close to the
old weight vector. Eq. (1) can also be expressed in dual form, yielding the PA-II update equation:
(2)
wi+1 = wi + αi yixi
The theoretical properties of this algorithm was analyzed by [5], and it was demonstrated on a variety
of tasks (e.g. [3]).
Online conﬁdence-weighted (CW) learning [9, 7], generalized the PA update principle to multivari-
ate Gaussian distributions over the weight vectors N (µ, Σ) for binary classiﬁcation. The mean
µ ∈ Rd contains the current estimate for the best weight vector, whereas the Gaussian covariance
matrix Σ ∈ Rd×d captures the conﬁdence in this estimate.
CW classiﬁers are trained according to a PA rule that is modiﬁed to track differences in Gaus-
sian distributions. At each round, the new mean and covariance of the weight vector distribu-
tion is chosen by optimizing: (µi+1 , Σi+1 ) = arg minµ,Σ DKL (N (µ, Σ) (cid:107) N (µi , Σi )) such that
Prw∼N (µ,Σ) [yi (w · xi ) ≥ 0] ≥ η .
This particular CW rule may over-ﬁt since it guarantees a correct prediction with likelihood η > 0.5
(cid:0)µi+1 , Σi+1
(cid:1) = arg minµ,Σ DKL (N (µ, Σ) (cid:107) N (µi , Σi )) + λ1 (cid:96)h2 (yi , µ · xi ) + λ2x(cid:62)
at every round. A more recent alternative scheme called AROW (adaptive regularization of weight-
vectors) [8] replaces the guaranteed prediction at each round with the following loss function:
i Σxi ,where
(cid:96)h2 (yi , µ · xi ) = (max{0, 1 − yi (µ · xi )})2 is the squared-hinge loss suffered using the weight
vector µ and λ1 , λ2 ≥ 0 are two tradeoff hyperparameters. AROW [8] has been shown to perform
well in practice, especially for noisy data where CW severely overﬁts.
In this work, we take the view that the Gaussian distribution over weight vectors is modiﬁed by
herding according to a velocity ﬂow ﬁeld. First we show that any change in a Gaussian distributed
random variable can be related to a linear velocity ﬁeld:

2

(a)(b)Theorem 1 Assume that the random variable (r.v.) W is distributed according to a Gaussian dis-
1. The r.v. U = AW + b also has a Gaussian distribution, U ∼ N (cid:0)b + Aµ, AΣA(cid:62) (cid:1) .
tribution, W ∼ N (µ, Σ) ,
2. Assume that a r.v. U is distributed according to a Gaussian distribution, U ∼ N (cid:16)
(cid:17)
˜µ, ˜Σ
Then there exists A and b such that the following linear relation holds, U = AW + b .
2 (W − µ) + µ,
3. Let Υ be any orthogonal matrix Υ(cid:62) = Υ−1 and deﬁne U = Σ 1
2 ΥΣ− 1
then both U and W have the same distribution.
(cid:104)
(cid:62) (cid:105)
Proof: The ﬁrst property follows easily from linear systems theory. The second property is easily
2 and b = ˜µ − ˜Σ 1
2 Σ− 1
2 Σ− 1
shown by taking: A = ˜Σ 1
2 µ . Similarly, for the third property, it sufﬁces
(cid:104)
(cid:62) (cid:105)
2 (E [W] − µ)+µ = µ , and Cov (U) = E
(U − µ) (U − µ)
2 ΥΣ− 1
to show that E [U] = Σ 1
=
(W − µ) (W − µ)
2 ΥΥ(cid:62)Σ 1
2 Υ(cid:62)Σ 1
2 ΣΣ− 1
2 ΥΣ− 1
2 Υ(cid:62)Σ 1
Σ− 1
2 ΥΣ− 1
Σ 1
2 = Σ 1
2 = Σ 1
2 =
2 E
Σ 1
2 Σ 1
2 = Σ .
Thus, the transformation U = AW + b can be viewed as a velocity ﬂow resulting in a change of the
underlying Gaussian distribution of weight vectors. On the other hand, this microscopic view of the
underlying velocity ﬁeld contains more information than merely tracking the mean and covariance
of the Gaussian. This can be seen since many different velocity ﬁelds result in the same overall mean
and covariance. In the next section, we show how we can deﬁne new online learning algorithms by
considering various constraints on the overall velocity ﬁeld. These new algorithms optimize a loss
function by constraining the parameters of this velocity ﬁeld.
3 Algorithms
Our algorithms maintain a distribution, or inﬁnite collection of weight vectors {Wi} for each round
i. Given an instance xi it outputs a prediction based upon the majority of these weight vectors. Each
weight vector Wi is then individually updated to Wi+1 according to a generalized PA rule,
(cid:62)
(W−Wi )
i (W−Wi )+C (cid:96) ((xi , yi ) ,W) ,
Ci (W) where Ci (W)=
Σ−1
1
Wi+1=arg min
2
W
and Σi is a PSD matrix that will be deﬁned shortly. In fact, we assume that Σi is invertible and thus
PD.
Clearly, it is impossible to maintain and update an inﬁnite set of vectors, and thus we employ a
parametric density fi (Wi ; θi ) to weight each vector. In general, updating each individual weight-
vector using some rule (such as the PA update) will modify the parametric family. We thus employ
a Gaussian parametric density with W ∼ N (µi , Σi ), and update the distribution collectively,
Wi+1 = AiWi + bi ,
where Ai ∈ Rd×d represents stretching and rotating the distribution, and the bi ∈ Rd is an overall
translation. Incorporating this linear transformation, we minimize the average of Eq. (3) with respect
to the current distribution,
EWi∼N (µi ,Σi ) [Ci (AWi + b)] .
(Ai , bi ) = arg min
A,b

(4)

.

(3)

We derive the algorithm by computing the expectation Eq. (4) starting with the ﬁrst regularization
term of Eq. (3). After some algebraic manipulations and using the ﬁrst property of Theorem 1 to
(cid:1) .
Tr (cid:0)(A − I )(cid:62)Σ−1
write µ = Aµi + bi we get the expected value for the ﬁrst term of Eq. (3) in terms of µ and A,
(cid:62)
(A − I )Σi
(µ − µi ) +
(µ − µi )
Σ−1
1
1
i
i
2
2
Next, we focus on the expectation of the loss function in their second term of Eq. (3).

(5)

3.1 Expectation of the Loss Function

We consider the expectation,

EWi∼N (µi ,Σi ) [(cid:96) ((xi , yi ) , AWi + b)]

(6)

3

In general, there is no closed form solution for this expectation, and instead we seek for an appro-
priate approximation or bound. For simplicity we consider binary classiﬁcation, denote the signed
margin by M = yi (W(cid:62)x) and write (cid:96) ((x, y), W) = (cid:96)(M ) .
If the loss is relatively concentrated about its mean, then the loss of the expected weight-vector µ is
a good proxy for Eq. (6). Formally, we can deﬁne
(cid:104)
(M − E [M ])2 (cid:105)
Deﬁnition 1 Let F = {f (M ; θ) : θ ∈ Θ} be a family of density functions. A loss function is
uniformly λ-bounded in expectation with respect to F if there exists λ > 0 such that for all θ ∈ Θ
we have that, E [(cid:96) (M )] ≤ (cid:96) (E [M ]) + λ
, where all expectations are with
2 E
respect M ∼ f (M ; θ).

We note in passing that if the loss function (cid:96) is convex with respect to W we always have that,
E [(cid:96) (M )] ≥ (cid:96) (E [M ]). For Gaussian distributions we have that Θ = {µ, Σ} and a loss function
(cid:96) is uniformly λ-bounded in expectation if there exists a λ such that, EN (µ,Σ) [(cid:96) ((x, y), W)] ≤
2 x(cid:62)Σx . We now enumerate some particular cases where losses are uniformly
(cid:96) ((x, y), E [W]) + λ
λ-bounded.
Proposition 2 Assume that the loss function (cid:96)(M ) has a bounded second derivative, (cid:96)(cid:48)(cid:48) (M ) ≤ λ
then (cid:96) is uniformly λ-bounded in expectation.
Proof:
Applying the Taylor expansion about M = E [M ] we get, (cid:96) (M ) = (cid:96) (E [M ]) +
(M − E [M ]) (cid:96)(cid:48) (E [M ]) + 1
2 (M − E [M ])2 (cid:96)(cid:48)(cid:48) (ξ ) ,for some ξ ∈ [M , E [M ]]. Taking the expecta-
(cid:0)y − M (cid:62)x(cid:1)2 is uniformly (λ =)1-bounded in expectation since
tion of both sides and bounding (cid:96)(cid:48)(cid:48) (ξ ) ≤ λ concludes the proof.
For example, the squared loss 1
its second derivative is bounded by unity (1). Another example is the log-loss, log(1 + exp(−M )),
2
being uniformly 1/4-bounded in expectation. Note that the popular hinge and squared-hinge loss
are not even differentiable at M = 1. Nevertheless, we can show explicitly that indeed both are
uniformly λ-bounded, though the proof is omitted here due to space considerations. To conclude,
2 x(cid:62)
i AΣiA(cid:62)xi .
for uniformly λ-bounded loss functions, we bound Eq. (6) with (cid:96) ((xi , yi ), µ) + λ
Thus, our online algorithm minimizes the following bound on Eq. (4), with a change of variables
from the pair (A, b) to the pair (A, µ), where µ is the mean of the new distribution,
(cid:62)
(µ − µi )
(µ − µi ) + C (cid:96) ((xi , yi ), µ) +
Σ−1
1
(cid:1) +
Tr (cid:0)(A − I )(cid:62)Σ−1
i
2
(A − I )Σi
x(cid:62)
i AΣiA(cid:62)xi
C λ
1
i
2
2
In the next section we derive an analytic solution for the last problem. We note that, similar to
AROW, it is decomposed into two additive terms: Eq. (7) which depends only on µ and Eq. (8)
which depends only on A.
4 Solving the Optimization Problem
We consider here the squared-hinge loss, (cid:96) ((x, y), µ) = (cid:0)max{0, 1 − y(µ(cid:62)x)}(cid:1)2 , reducing Eq. (7)
µi+1 = µi + αi yixi , αi = (cid:0)max{0, 1 − yi (µ(cid:62)
i xi )}(cid:1) / (cid:0)x(cid:62)
i Σixi + 1/C (cid:1) ,
to a generalization of PA-II in Mahalanobis distances (see Eq. (2)),
(9)
We now focus on minimizing the second term (Eq. (8)) which depends solely on Ai . For simplicity
we assume λ = 1 and consider two cases.

(Ai , µi+1 ) = arg min
A,µ

(7)

(8)

4.1 Diagonal Covariance Matrix

2 Tr (cid:0)(A − I )(cid:62) (A − I )(cid:1)+ C
We ﬁrst assume that both Σi and A are diagonal, and thus also Σi+1 is diagonal, and thus Σi , Σi+1
2 x(cid:62)
i AΣiA(cid:62)xi .
and A commute with each other. Eq. (8) then becomes, 1
Denote the rth diagonal element of Σi by (Σi )r,r and the rth diagonal element of A by (A)r,r . The

4

(10)

last equation becomes, (cid:80)
(cid:80)
2 ((A)r,r − 1)2 + C
i,r (A)2
r,r (Σi )r,r Taking the derivative with
1
r x2
(cid:16)
(cid:17) ⇒ (Σi+1 )r,r = (Σi )r,r /
(cid:16)
(cid:17)2
r
2
respect to (A)r,r we get,
1 + C x2
1 + C x2
i,r (Σi )r,r
i,r (Σi )r,r
(Ai )r,r = 1/
The last equation is well-deﬁned since the denominator is always greater than or equal to 1.
(cid:0)Tr (cid:0)A(cid:62)Σ−1
(cid:1) − Tr (cid:0)Σ−1
(cid:1)
4.2 Full Covariance Matrix
+Tr (cid:0)Σ−1
(cid:1) − Tr (cid:0)A(cid:62)Σ−1
(cid:1)(cid:1) + C
Expanding Eq. (8) we get 1
i AΣi
i AΣi
2
i AΣiA(cid:62)xi . Setting the
2 x(cid:62)
i Σi
i Σi
i AΣi −
derivative of the last equation with respect to A we get, Σ−1
(cid:1) A = Σ−1
combine terms, (cid:0)Σ−1
i AΣi = 0 . We multiply both terms by Σ−1
I + C xix(cid:62)
(right) and
(cid:1)−1
Ai = (cid:0)Σ−1
i
i + C xix(cid:62)
, Yielding,
i
i
Σ−1
i + C xix(cid:62)
.
(cid:0)AΣiA(cid:62) (cid:1)−1
i
i
inverse, Σ−1
To get Σi+1 we ﬁrst
its
compute
=
i+1
. Substituting Eq. (11) in the last equation we
i+1 = (cid:0)AΣiA(cid:62) (cid:1)−1
(cid:1) xix(cid:62)
i + (cid:0)2C +C 2x(cid:62)
get,
Σ−1
= Σ−1
(12)
i Σixi
i
(cid:0)C 2xiΣix(cid:62)
i + 2C (cid:1) / (cid:0)(1 + C x(cid:62)
i Σixi )2 (cid:1) .
Finally, using the Woodbury identity [12] to compute to updated
covariance matrix,
Σi+1 = Σi − Σixix(cid:62)
i Σi
(13)
We call the above algorithms NHERD for Normal (Gaussian)
Herd. A pseudocode of the algorithm appears in Alg. 3.

.

(11)

4.3 Discussion

Both our update of Σi+1 in Eq. (12) and the update of AROW (see
eq. (8) of [8] ) have the same structure of adding γixix(cid:62)
to Σi .
i
AROW sets γi = C while our update sets γi = 2C +C 2xiΣix(cid:62)
i .
In this aspect, the NHERD update is more aggressive as it in-
creases the eigenvalues of Σ−1
at a faster rate. Furthermore, its
i
update rate is not constant and depends linearly on the current vari-
ance of the margin x(cid:62)
i Σixi ; the higher the variance, the faster the
eigenvalues of Σi decrease. Lastly, we note that the update ma-
Figure 2: Top and center panels:
trix Ai can be written as a product of two terms, one depends on
an illustration of the algorithm’s
the covariance matrix before the update and the other on the co-
(cid:1) (see eq. (8) of [8] ). From Eq. (11) we
˜Σi+1 = (cid:0)Σ−1
update (see text). Bottom panel:
variance matrix after an AROW update. Formally, let ˜Σi+1 be
an illustration of a single update
the covariance matrix after updated using the AROW rule, that is,
for the ﬁve algorithms. The cyan
i + C xix(cid:62)
ellipse represents the weight vec-
i
observe that Ai = ˜Σ−1
tor distribution before the example
i+1Σi , which means that NHERD modiﬁes
is observed. The red-square rep-
Σi if and only if AROW modiﬁes Σi .
resents the mean of the updated
The diagonal updates of AROW and NHERD share similar
distribution and the ﬁve ellipses
(cid:16) ˜Σi+1
(cid:17)
[8] did not specify the speciﬁc update for this
properties.
represents the covariance of each
of the algorithm after given the
that
case, yet using a similar derivation of Sec. 4.1 we get
(cid:17)
(cid:16)
data example ((1, 2), +1). The
the AROW update for diagonal matrices ˜Σi+1 is
=
ordering of the area of the ﬁve el-
(cid:17)
(cid:16) ˜Σi+1
r,r
lipses correlates well with the per-
. Taking the ratio between the rth
1 + C x2
i,r (Σi )r,r
(Σi )r,r /
formance of the algorithms.
i,r (Σi )r,r ≥ 1 .
element of Eq. (10) and the last equation we get,
/(Σi+1 )r,r = 1 + C x2
5

r,r

−2−1012−1.5−1−0.500.511.52−2−1012−1.5−1−0.500.511.52−1−0.500.511.5−0.500.511.5beforeNHERD_PNHERD_ENHERD_DAROW_PAROW_D(Eq. (13))

Σixi

(Eq. (9))

Parameter: C > 0
Initialize: µ1 = 0 , Σ1 = I
for i = 1, . . . , m do
Get input example xi ∈ Rd
Predict ˆyi = sign(µ(cid:62)
i xi )
Get true label yi and suffer loss 1 if ˆyi (cid:54)= yi
i xi ) ≤ 1 then
if yi (µ(cid:62)
max{0,1−yi (µ(cid:62)
i xi )}
Set µi+1=µi + yi
x(cid:62)
i Σixi+ 1
C
Full Covariance:
C 2xiΣix(cid:62)
Set Σi+1=Σi −Σixix(cid:62)
i +2C
i Σi
(1+CxiΣix(cid:62)
i )2
Diagonal Covariance:
Set (Σi+1 )r,r for r = 1 . . . d using Eq. (14)
end if
end for
Return: µm+1 , Σm+1

To conclude, the update of NHERD for diagonal covariance matrices is also more aggressive than
AROW as it increases the (diagonal) elements of its inverse faster than AROW.
An illustration of the two updates appears in Fig. 2 for a problem in a planar 2-dimensional space.
The Gaussian distribution before the update is isotropic with mean µ = (0, 0) and Σ = I2 . Given
the input example x = (1, 2), y = 1 we computed both A and b for both the full (top panel)
and diagonal (center panel) update. The plot illustrates the update of the mean vector (red square),
weight vectors with unit norm (cid:107)w(cid:107) = 1 (blue), and weight vectors with norm of 2, (cid:107)w(cid:107) = 2 (green).
The ellipses with dashed lines il-
lustrate the weights before the
update, and ellipses with solid
lines illustrate the weight-vectors
after the update. All the weight
vectors above the black dotted
line classify the example cor-
rectly and the ones above the
dashed lines classify the exam-
ple with margin of at least unit 1.
The arrows connecting weight-
vectors from the dashed ellipses
to solid ellipses illustrate the up-
date of individual weight-vectors
with the linear transformation
w ← Ai (w − µi ) + µi+1 .
In both updates the current mean
µi is mapped to the next mean
µi+1 . The full update “shrinks”
the covariance in the direction
orthogonal to the example yixi ; vectors close to the margin of unit 1 are modiﬁed less than vec-
tors far from this margin; vectors with smaller margin are updated more aggressively then vectors
with higher margin; even vectors that classify the example correctly with large margin of at least one
are updated, such that their margin is shrunk. This is a consequence of the linear transformation that
ties the update between all weight-vectors. The diagonal update, as designed, maintains a diagonal
matrix, yet shrinks the matrix more in the directions that are more “orthogonal” to the example.
We note in passing that for all previous CW algorithms [7] and AROW [8], a closed form solution
for diagonal matrices was not provided. Instead these papers proposed to diagonalize either Σi+1
(called drop) or Σ−1
(cid:17)2(cid:19)
(cid:18)(cid:16)

i+1 (called project) which was then inverted. Together with the exact solution
of Eq. (10) we get the following three alternative solutions for diagonal matrices,
(cid:17)
(cid:16)
(cid:1) x2
(1/ (Σi )r,r ) + (cid:0)2C +C 2x(cid:62)
1 + C x2
i,r (Σi )r,r
(Σi )r,r /
(cid:17)2 (C 2xiΣix(cid:62)
(Σi )r,r − (cid:16)
i Σix(cid:62)
1/
i,r
i
i +2C )
(Σi )r,r xi,r
(1+CxiΣix(cid:62)
i )2
We investigate these formulations in the next section. Finally, we note that similarly to CW and
AROW, algorithms that employ full matrices can be incorporated with Mercer kernels [11, 14],
while to the best of our knowledge, the diagonal versions can not.
5 Empirical Evaluation

Figure 3: Normal Herd (NHERD)

project

(14)

(Σi+1 )r,r =

exact

drop

We evaluate NHERD on several popular datasets for document classiﬁcation, optical character
recognition (OCR), phoneme recognition, as well as on action recognition in video. We compare our
new algorithm NHERD with the AROW [8] algorithm, which was found to outperform other base-
lines [8]: the perceptron algorithm [13], Passive-Aggressive (PA) [5], conﬁdence weighted learning
(CW) [9, 7] and second order perceptron [1] on these datasets. For both NHERD and AROW
we used the three diagonalization schemes, as mentioned in Eq. (14) in Sec. 4.3. Since AROW
Project and AROW Exact are equivalent we omit the latter, yielding a total of ﬁve algorithms:
NHERD {P , D , E } for Project,Drop,Exact and similarly AROW {P , D}.

6

Figure 4: Performance comparison between algorithms. Each algorithm is represented by a vertex. The weight
of an edge between two algorithms is the fraction of datasets in which the top algorithm achieves lower test
error than the bottom algorithm. An edge with no head indicates a fraction lower than 60% and a bold edge
indicates a fraction greater than 80%. Graphs (left to right) are for noise levels of 0%, 10%, and 30%.

Although NHERD and AROW are designed primarily for binary classiﬁcation, we can modify them
for use on multi-class problems as follows. Following [4], we generalize binary classiﬁcation and
assume a feature function f (x, y) ∈ Rd mapping instances x ∈ X and labels y ∈ Y into a common
space. Given a new example, the algorithm predicts ˆy = arg maxz µ · f (x, z ), and suffers a loss if
y (cid:54)= ˆy . It then computes the difference vector ∆ = f (x, y)−f (x, y (cid:48) ) for y (cid:48) = arg maxz (cid:54)=y f (x, y (cid:48) )
which replaces yx in NHERD (Alg. 3).
We conducted an empirical study using the following datasets. First are datasets from [8]: 36 binary
document classiﬁcation data, and 100 binary OCR data (45 all-pairs of both USPS and MNIST and
1-vs-rest of MNIST). Secondly, we used the nine multi-category document classiﬁcation datasets
used by [6]. Third, we conducted experiments on a TIMIT phoneme classiﬁcation task. Here
we used an experimental setup similar to [10] and mapped the 61 phonetic labels into 48 classes.
We then picked 10 pairs of classes to construct binary classiﬁcation tasks. We focused mainly on
unvoiced phonemes where there is no underlying harmonic source and whose instantiations are
noisy. The ten binary classiﬁcation problems are identiﬁed by a pair of phoneme symbols (one or
two Roman letters). For each of the ten pairs we picked 1, 000 random examples from both classes
for training and 4, 000 random examples for a test set. These signals were then preprocessed by
computing mel-frequency cepstral coefﬁcients (MFCCs) together with ﬁrst and second derivatives
and second order interactions, yielding a feature vector of 902 dimensions. Lastly, we also evaluated
our algorithm on an action recognition problem in video under four different conditions. There are
about 100 samples for each of 6 actions. Each sample is represented using a set of 575 positive real
localized spectral content ﬁlters from the videos. This yields a total of 156 datasets.
Each result for the text datasets was averaged over 10-fold cross-validation, otherwise a ﬁxed split
into training and test sets was used. Hyperparameters (C for NHERD and r for ARROW) and the
number of online iterations (up to 20) were optimized using a single randomized run. In order to
observe each algorithm’s ability to handle non-separable data, we performed each experiment using
various levels of artiﬁcial label noise, generated by independently ﬂipping binary labels.

Results: We ﬁrst summarize the results on all datasets excluding the video recognition dataset in
Fig. 4, where we computed the number of datasets for which one algorithm achieved a lower test
error than another algorithm. The results of this tournament between algorithms is presented as
a winning percentage. An edge between two algorithms shows the fraction of the 155 datasets for
which the algorithm on top had lower test error than the other algorithm. The three panels correspond
to three varying noise levels, from 0%,10% and 30%.
We observe from the ﬁgure that Project generally outperforms Exact which in turn outper-
forms Drop. Furthermore, NHERD outperforms AROW, in particular NHERD P outperforms
AROW P and NHERD D outperforms AROW D. These relations become more prominent when
labeling noise is increased in the training data. The right panel of Fig. 2 illustrates a single update of
each of the ﬁve algorithms: AROW D, AROW D, NHERD D, NHERD E, NHERD P. Each of the
ﬁve ellipses represents the Gaussian weight vector distribution after a single update on an example

7

NHERD_PNHERD_E 56%NHERD_D 66%AROW_P 54%AROW_D 70% 71% 52% 72% 72% 67% 72%NHERD_PNHERD_E 50%NHERD_D 88%AROW_P 76%AROW_D 91% 85% 78% 89% 72% 75% 84%NHERD_PNHERD_E 65%NHERD_D 75%AROW_P 73%AROW_D 80% 70% 76% 80% 74% 59% 74%by each of the ﬁve algorithms. Interestingly, the resulting volume (area) of different ellipses roughly
correspond to the overall performance of the algorithms. The best update – NHERD P – has the
smallest ellipse (with lowest-entropy), and the update with the worst performance – AROW D – has
the largest, highest-entropy ellipse.

More detailed results for NHERD P and
AROW P,
the overall best performing
algorithms, are compared in Fig. 5.
NHERD P and AROW P are compara-
ble when there is no added noise, with
NHERD P winning a majority of the
time. As label noise increases (moving
top-to-bottom in Fig. 5) NHERD P holds
up remarkably well. In almost every high
noise evaluation, NHERD P improves
over AROW P (as well as all other base-
lines, not shown). The bottom-left panel
of Fig. 5 shows the relative improvment
in accuracy of NHERD P over AROW P
on the ten phoneme recognition tasks
with additional 30% label noise. The ten
tasks are ordered according to their sta-
tistical signiﬁcance according to McNe-
mar’s test. The results for the seven right
tasks are statistically signiﬁcant with a p-
value less then 0.001. NHERD P out-
performs AROW P ﬁve times and un-
derperforms twice on these seven signiﬁ-
cant tests. Finally, the bottom-right panel
shows the 10-fold accuracy of the ﬁve
algorithms over the video data, where
clearly NHERD P outperforms all other
algorithms by a wide margin.
Conclusions: We have seen how to in-
corporate velocity constraints in an on-
line learning algorithm.
In addition to
tracking the mean and covariance of a
Gaussian weight vector distribution, reg-
ularization of the linear velocity terms
are used to herd the normal distribution
in the learning process. By bounding the
loss function with a quadratic term, the
resulting optimization can be solved an-
alytically, resulting in the NHERD algo-
rithm. We empirically evaluated the per-
formance of NHERD on a variety of ex-
Figure 5: Three top rows: Accuracy on OCR (left) and text
perimental datasets, and show that the
and phoneme (right) classiﬁcation. Plots compare performance
projected NHERD algorithm generally
between NHERD P and AROW P. Markers above the line in-
outperforms all other online learning al-
dicate superior NHERD P performance and below the line su-
perior AROW P performance. Label noise increases from top
gorithms on these datasets. In particular,
to bottom: 0%, 10% and 30%. NHERD P improves relative
NHERD is very robust when random la-
to AROW P as noise increases. Bottom left: relative accuracy
beling noise is present during training.
improvment of NHERD P over AROW P on the ten phoneme
Acknowledgments:
KC is a Horev
classiﬁcation tasks. Bottom right: accuracy of ﬁve algorithms
Fellow, supported by the Taub Founda-
on the video data. In both cases NHERD P is superior
tions. This work was also supported by German-Israeli Foundation grant GIF-2209-1912.

8

949596979899100949596979899100AROW_PNHERD_P  uspsmnist607080906065707580859095AROW_PNHERD_P  binary textmc textphoneme8688909294969886889092949698AROW_PNHERD_P  uspsmnist60708090556065707580859095AROW_PNHERD_P  binary textmc textphoneme506070809050556065707580859095AROW_PNHERD_P  uspsmnist607080905560657075808590AROW_PNHERD_P  binary textmc textphonemeb−pd−tf−thg−kjh−chm−nm−ngn−ngs−shv−dh−8−6−4−202468Relative Increase in AccuracyAROW_PAROW_DNHERD_ENHERD_PNHERD_D7778798081828384858687AccuracyReferences
[1] Nicol ´o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order perceptron algo-
rithm. Siam Journal of Commutation, 34(3):640–668, 2005.
[2] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge Uni-
versity Press, New York, NY, USA, 2006.
[3] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. An online algorithm for large scale image
similarity learning. In NIPS, 2009.
[4] Michael Collins. Discriminative training methods for hidden markov models: Theory and
experiments with perceptron algorithms. In EMNLP, 2002.
[5] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-aggressive
algorithms. JMLR, 7:551–585, 2006.
[6] K. Crammer, M. Dredze, and A. Kulesza. Multi-class conﬁdence weighted algorithms.
EMNLP, 2009.
[7] K. Crammer, M. Dredze, and F. Pereira. Exact conﬁdence-weighted learning. In NIPS 22,
2008.
[8] K. Crammer, A. Kulesza, and M. Dredze. Adaptive regularization of weighted vectors.
Advances in Neural Information Processing Systems 23, 2009.
[9] M. Dredze, K. Crammer, and F. Pereira. Conﬁdence-weighted linear classiﬁcation. In ICML,
2008.
[10] A. Gunawardana, M. Mahajan, A Acero, and Pl att J. C. Hidden conditional random ﬁelds for
phone classiﬁ cation. In Proceedings of ICSCT, 2005.
[11] J. Mercer. Functions of positive and negative type and their connection with the theory of
integral equations. Philos. Trans. Roy. Soc. London A, 209:415–446, 1909.
[12] K. B. Petersen and M. S. Pedersen. The matrix cookbook, 2007.
[13] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization
in the brain. Psychological Review, 65:386–407, 1958.
[14] Bernhard Sch ¨olkopf and Alexander J. Smola. Learning with Kernels: Support Vector Ma-
chines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2001.

In

In

9


AUDIO SOURCE SEPARATION BY PROBABILISTIC LATENT COMPONENT ANALYSIS
Yinyi Guo, Mofei Zhu
Center for Computer Research in Music and Acoustics
Stanford University

ABSTRACT

The problem of audio source separation from a monophonic
sound mixture having known instrument types but unknown
timbres is presented. An improvement to the Probabilistic La-
tent Component Analysis (PLCA) source separation method
is proposed. The technique uses a basis function dictionary
to produce a ﬁrst round PLCA source separation. The PLCA
weights are then reﬁned by incorporating note onset informa-
tion. The source separation is then performed using a sec-
ond round PLCA in which the reﬁned weights are held ﬁxed,
and the basis functions are updated. Preliminary experimen-
tal results on mixtures of two instruments are quite promising,
showing a 6 dB improvement in SIR over standard PLCA.
Index Terms— Audio Source Separation, Probabilistic
Latent Component Analysis, Basis Function Adaptation, On-
set Detection

1. INTRODUCTION

Source separation of audio has been an active research topic
in audio signal processing. In recent years, a great deal of the
work in this area is based on spectral decomposition meth-
ods. Probabilistic latent component analysis (PLCA) [1] is
one such method. To achieve good separation with few ar-
tifacts, it has a high demand of the prior knowledge of the
target sound in the mixture. Not only do the sound types need
to be known, but precise basis functions of the target timbres
should be pre-trained as well. [2]
In this paper, we extend the PLCA based supervised sep-
aration method described in [2]. We use pre-trained spectral
basis of general instruments as the spectral prior knowledge
to lead the algorithm to give preliminary temporal results,
which can be optimized further. We then perform onset de-
tection based post-processing to adjust the temporal informa-
tion given by the previous stage decomposition. Eventually,
the source separation is performed and the basis functions
of target sources in the mixture are updated with the reﬁned
temporal weights held ﬁxed using a second round PLCA. An
overview of the proposed approach is shown in Fig. 1.




































Fig. 1. System overview ﬂowchart

2. PROPOSED METHOD
2.1. PLCA Model
The Probabilistic Latent Component Analysis (PLCA) algo-
rithm is the most important element of our system, which per-
forms the source separation. PLCA is a probabilistic model,
whose components are all non-negative. It interprets the spec-
trogram as a histogram and the spectral and temporal compo-
nents as distributions along time and frequency. Numerically,
this probabilistic latent spectrogram factorization method is
similar to the non-negative matrix factorization (NMF) ap-
proach [3][4]. The magnitude spectrogram of audio signals
can be treated as a 2D distribution of “sound quanta” over
time and frequency. In this respect, the non-negative factor-
ization is used to model the spectrogram as a linear combina-
tion of spectral basis vectors, which are regarded as the latent
variables. The model is deﬁned as follows,
1. P (f | z ) is deﬁned as a multinomial probability distri-
bution of frequency f given certain latent variable z .
Each distribution is actually a spectral basis vector of

• E Step - Estimate the posterior given spectral basis vec-
tors and weights vectors

Pt (z | f ) =

Pt (f | z ) P (z )
"z Pt (f | z ) P (z )
• M Step - Estimate spectral basis vectors and weights
vectors given the posterior

(3)

(5)

(4)

Pt (z ) = "f Pt (f ) Pt (z | f )
"z "f Pt (f ) Pt (z | f )
P (f | z ) = "t Pt (f ) Pt (z | f )
"t "f Pt (f ) Pt (z | f )
The source separation applying the standard PLCA model is
conducted as follows: [2][5]
• Calculate the spectrogram St (f ) of a sound mixture.
• Learn the basis function P (f | zm ) of the sources Zm
in the mixture using PLCA.
• Initialize the prior-based PLCA algorithm with the
learned basis functions of the active components, and
estimate the model parameters P (f | zm ) and Pt (z ).
• Reconstruct each instrument in the mixture by only us-
ing their corresponding optimized P (zm ), P (f | zm )
The complete process of our proposed approach is de-
scribed in the following sections.

2.2. Fixed-Basis PLCA
We start with a ﬁxed-basis PLCA which is ﬁrst performed to
get weights of individual sources. Given that the types of in-
strument in the mixture are known, we choose accordingly
the basis function of target sources from the pre-trained typ-
ical spectral basis dictionary and ﬁx them as spectral prior
information for the PLCA algorithm. Since the timbre char-
acteristics are very similar among the same type of instru-
ment, this ﬁxed-basis PLCA gives us more robust weights es-
timation comparing with using the traditional PLCA, in which
the spectral basis and weights are not ﬁxed. In order to sep-
arate the target sources in the mixture precisely, the spec-
tral basis should be updated for reconstruction. To achieve
this, more reliable temporal prior knowledge has to be pre-
pared for the next stage PLCA: ﬁrst, we need to further re-
ﬁne the current weight coefﬁcients that may contain unwanted
crossover between notes or instruments. Second, the right
timing when should update the spectral basis function has to
be found based on the reﬁned weights. We therefore perform
an onset detection based post-processing on the weights as a
preparation for the second stage PLCA.

Fig. 2. An illustration of applying PLCA to a spectrogram of
eight notes that are by a piano.

sound component.
2. Pt (z ) is deﬁned as a multinomial probability distribu-
tion of weights for each latent component z at time t
.
3. P (t) is a distribution of the relative magnitudes at dif-
ferent time frame t.
4. Pt (f ) is deﬁned as normalized spectrogram vector at
time frame t
5. St (f ) is deﬁned the original spectrogram vector at time
frame t

(1)
(2)

P (f | z ) Pt (z )

St (f ) = P (t) Pt (f )
Pt (f ) = !
z
Equation 1 is to normalize St into Pt (f ), Equation 2 is
the magnitude decomposition of the spectrogram into spectral
basis vectors and weights vectors. An example of applying
this decomposition is shown in Fig 2.
We can see ﬁve distinct notes in Fig 2. Each of the notes is
represented by a latent variable z . The spectral basis P (f | z )
of each component z can be seen as the harmonic series of
each note.
The expectation-maximization (EM) algorithm can be
used to decompose the spectrogram Pt (f ) into spectral basis
vectors P (f | z ) and corresponding weights Pt (z ). Given
the mixture spectrogram, we randomly initialize all the un-
known parameters, and estimate the model parameters by
iterating between E step and M step until convergence.

2.3. Onset Detection
An amplitude envelope function is calculated from the Short
Time Fourier Transformation of the input mixture signal.
Based on algorithms from previous literature [6, 7], the well-
known ﬁrst order difference function of the spectral energy
envelopes is computed, which gives prominent peaks from
which onsets can be located. A threshold was generated in
order to determine if a peak is an onset candidate [8]. Ev-
ery value above this threshold in the ﬁrst order difference
function of the energy envelope should be a potential onset
peak.

2.4. Post-processing on Weight Coefﬁcients
An onset implies something new happened in a signal. In the
case of a musical signal, this could be a change in pitch or in-
strument, i.e. the precise time when a new note is produced by
an instrument. Knowing the location of the onsets allows us
to easily determine which instruments are playing. This can
be accomplished by comparing the intensities of their weights
envelopes in the frames following the onset frame, since the
weights envelopes could be considered as activation indica-
tors of sources. Thus it is more reliable to make decisions
to remove residual cross-talk components right after onset
frames. After we detect each onset, the weight coefﬁcients
of different notes are compared against their sums of magni-
tude during the following 0.15 seconds. For each instrument,
the note that has the maximum weight1 coefﬁcient among all
the weights is taken as an active note, while the weights of
the other notes are removed. This reﬁnement on the weights
is operated after each onset. Eventually, the weights coefﬁ-
cients of active notes and their corresponding basis functions
are restored for the further processing.

2.5. Update Basis Function
We record those appropriate time frames in which the weights
coefﬁcients after post-processing are relatively reliable to up-
date corresponding basis functions of certain source. If the
sum of the weights of active notes are dominant in some
time frames between two successive onsets, another modiﬁed
PLCA algorithm based on EM learning rule is performed to
update the basis functions of the active notes, as follows:
• For the initialization of the EM algorithm, the basis
functions P (f | z ) are constructed as the basis func-
tions of active notes P (f | za) cascaded with a set
of randomly distributed basis functions P (f | zr ) for
modeling residual sources other than the dominant ac-
tive notes. The new weight vector Pt (z ) consists of
weights of active notes Pt (za ) as well as the weights of
random basis Pt (zr ) that share the rest of the weights.
1 In this project we work on cases of single note concurrently for each
instrument

• The expectation step remains the same as Equation 3.
• The maximization step equations are given by the fol-
lowing update equations:

Pt (zr ) = "f Pt (f ) Pt (zr | f )
"z "f Pt (f ) Pt (zr | f )
P (f | z ) = "t Pt (f ) Pt (z | f )
"t "f Pt (f ) Pt (z | f )

(6)

(7)

The above equations are iterated until convergence. We
only update the weights for random sources Pt (zr ) while the
weights for active latent variables Pt (za) are ﬁxed in the M-
step in that they are ensured to be reliable temporal prior in-
formation given by the ﬁrst round PLCA and post-processing.
The chosen of the number of random basis depends mainly on
the number of sources in the mixture, since those extra ran-
domly initialized basis functions P (f | zr ) are supposed to
explain the artifacts introduced by the algorithm as well as
the sustain or release potions of any source before. The basis
functions of dominant components P (f | za ) will therefore
be biased to update towards the target source timbres eventu-
ally based on the ﬁxed weight coefﬁcients.

3. EXPERIMENTS AND RESULTS
Our algorithm has been tested on synthetic examples of 20
distinct data sets (mixture of two instruments per set), which
include 5 distinct instruments (piano, ﬂute, strings, saxophone
and guitar). All the examples are generated from Logic Pro.
The source timbres of the mixture are different from those in
the pre-trained basis functions in the inventory. For exam-
ple, in the basis functions inventory an octave of piano basis
functions from C4 to C5 are trained from piano sound source
of Steinway; An octave of ﬂute basis functions from C6 to C7
are trained from Super Air Flute . The corresponding test data
is mixture sound sources of Yamaha piano and Thin ﬂute. In
all these experiments we measured the Signal to Distortion
Ratio (SDR), the Signal to Interference Ratio (SIR), and the
Signal to Artifacts Ratio (SAR) as deﬁned in [9]. The re-
sults are shown in Table 1,2. We can see the proposed algo-
rithm enjoys around 6dB improvement in SIR over traditional
PLCA.
In these experiments, we used 5 basis functions per note,
ran 10 times of 20 distinct experiments, each spectral basis
and spectrogram of 2048-point FFT with 75% overlap, 80
iterations of the PLCA algorithm. The approach works on
magnitude spectrograms; for the phase we copy the phase of
the original mixture to each of the separated sources. The re-
sulting separated spectrograms of the piano-ﬂute mixture can
be seen in Fig 3.

SDR(dB)
SAR(dB)
SIR(dB)
Piano
4.85
4.90
25.86
Traditional PLCA
8.45
8.46
32.96
Proposed Algorithm
SDR(dB)
SAR(dB)
SIR(dB)
Flute
8.33
11.79
11.21
Traditional PLCA
Proposed Algorithm
12.16
15.52
15.60
Table 1. Performance metrics for synthetic mixture example
of piano-ﬂute

SDR(dB)
SAR(dB)
SIR(dB)
1.86
1.71
6.05
Improvement
Table 2. Averaged metric improvement over traditional
PLCA for all the 20 experimental datasets

Original Mixture

200

400

600

1000
800
Separated piano

1200

1400

1600

200

400

600

1000
800
Separated flute

1200

1400

1600

500

450

400

350

300

250

200

150

100

50

0

500

450

400

350

300

250

200

150

100

50

0

500

450

400

350

300

250

200

150

100

50

0

200
400
600
800
1000
1200
1400
1600
Fig. 3. Example result of separated piano and ﬂute spectro-
grams from a mixture. The top plot shows the input mixture
spectrogram, the middle one shows the separated piano spec-
trogram, the bottom one shows the separated ﬂute spectro-
gram. x-axis is the time frames. y-axis is the frequency bins
using 2048-point FFT with 75% overlap

4. CONCLUSION
We have presented a learning scheme based on PLCA algo-
rithm that is used for the source separation of multiple concur-
rent instruments knowing the type of instruments in the mix-
ture but without a prior knowledge of the precise basis func-
tions of target sources beforehand. The proposed algorithm
has been demonstrated on mixtures of two instruments. The
use of prior generic basis functions inventory, post-processing
on weight coefﬁcients as well as the basis function adaptation
have been shown to improve the performance of the original
PLCA algorithm. This method shows promising results and
in future work, we plan to improve the performance by study-
ing perceptual auditory model.

5. REFERENCES
[1] M. Shashanka P. Smaragdis, B. Raj, “A probabilistic la-
tent variable model for acoustic modeling,” Advances in
models for acoustic processing, NIPS, 2006.
“Supervised
[2] M. Shashanka P. Smaragdis, B. Raj,
and semi-supervised separation of sounds from single-
channel mixtures,” International Conference on Indepen-
dent Component Analysis and Signal Separation, 2007.
[3] H. Seung D. Lee, “Algorithms for non-negative matrix
factorization,” NIPS, 2001.
[4] M. Shashanka, B. Raj, and P. Smaragdis, “Probabilis-
tic latent variable models as non-negative factorizations,”
Computational Intelligence and Neuoscience Journal,
special issue on Advances in Non-negative Matrix and
Tensor Factorization, 2008.
[5] P. Smaragdis and G. J. Mysore, “Separation by hum-
ming: User-guided sound extraction from monophonic
mixtures,” Proc. of IEEE Workshop on Applications Sig-
nal Processing to Audio and Acoustics, 2009.
[6] E. D. Scheirer, “Tempo and beat analysis of acoustic mu-
sical signals,” J. Acoust. Soc. Am., vol. 104, pp. 588–601,
1998.
“Beat tracking based on
[7] M. Goto and Y. Muraoka,
multiple-agent architecture - a real-time beat tracking
system for audio signals,” Proceedings of The Second In-
ternational Conference on Multiagent Systems, pp.103–
110,, 1996.
[8] C. Duxbury, M. Sandler, and M. Davies, “A hybrid ap-
proach to musical note onset detection,” Proceedings of
the Digital Audio Effects Conference, Hamburg, 2002.
[9] C. Fevotte, R. Gribonval, and E. Vincent., “Bss eval tool-
box user guide,” IRISA Technical Report 1706, Rennes,
France, 2005.


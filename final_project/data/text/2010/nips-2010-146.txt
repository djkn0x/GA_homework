Adaptive Multi-Task Lasso: with Application to
eQTL Detection

Seunghak Lee, Jun Zhu and Eric P. Xing
School of Computer Science, Carnegie Mellon University
{seunghak,junzhu,epxing}@cs.cmu.edu

Abstract

To understand the relationship between genomic variations among population and
complex diseases, it is essential to detect eQTLs which are associated with phe-
notypic effects. However, detecting eQTLs remains a challenge due to complex
underlying mechanisms and the very large number of genetic loci involved com-
pared to the number of samples. Thus, to address the problem, it is desirable to
take advantage of the structure of the data and prior information about genomic
locations such as conservation scores and transcription factor binding sites.
In this paper, we propose a novel regularized regression approach for detecting
eQTLs which takes into account related traits simultaneously while incorporating
many regulatory features. We ﬁrst present a Bayesian networ k for a multi-task
learning problem that includes priors on SNPs, making it possible to estimate the
signiﬁcance of each covariate adaptively. Then we ﬁnd the ma
ximum a posteriori
(MAP) estimation of regression coefﬁcients and estimate we ights of covariates
jointly. This optimization procedure is efﬁcient since it c an be achieved by us-
ing a projected gradient descent and a coordinate descent procedure iteratively.
Experimental results on simulated and real yeast datasets con ﬁrm that our model
outperforms previous methods for ﬁnding eQTLs.

1 Introduction

One of the fundamental problems in computational biology is to understand associations between
genomic variations and phenotypic effects. The most common genetic variations are single nu-
cleotide polymorphisms (SNPs), and many association studies have been conducted to ﬁnd SNPs
that cause phenotypic variations such as diseases or gene-expression traits [1]. However, association
mapping of causal QTLs or eQTLs remains challenging as the variation of complex traits is a result
of contributions of many genomic variations. In this paper, we focus on two important problems to
detect eQTLs. First, we need to ﬁnd methods to take advantage of the structure of data for ﬁnding
association SNPs from high dimensional eQTL datasets when p ≫ N , where p is the number of
SNPs and N is the sample size. Second, we need techniques to take advantage of prior biological
knowledge to improve the performance of detecting eQTLs.

for high-dimensional association
To address the ﬁrst problem, Lasso is a widely used technique
mapping problems, which can yield a sparse and easily interpretable solution via an ℓ1 regularization
[2]. However, despite the success of Lasso, it is limited to considering each trait separately. If we
have multiple related traits it would be bene ﬁcial to estima te eQTLs jointly since we can share
information among related traits. For the second problem, Fig. 1 shows some prior knowledge on
SNPs in a genome including transcription factor binding sites (TFBS), 5’ UTR and exon, which play
important roles for the regulation of genes. For example, TFBS controls the transcription of DNA
sequences to mRNAs. Intuitively, if SNPs are located on these regions, they are more likely to be
true eQTLs compared to those on regions without such annotations since they are related to genes or
gene regulations. Thus, it would be desirable to penalize regression coefﬁcients less corresponding

1

SNPs

Chromosome

Exon 
Transcription factor binding site
5’ UTR
Annotation
Figure 1: Examples of prior knowledge on SNPs including transcription factor binding sites, 5’ UTR and
exon. Arrows represent SNPs and we indicate three genomic annotations on the chromosome. Here association
SNPs are denoted by red arrows (best viewed in color), showing that SNPs on regions with regulatory features
are more likely to be associated with traits.

to SNPs having signiﬁcant annotations such as TFBS in a regul arized regression model. Again, the
widely used Lasso is limited to treating all SNPs equally.

This paper presents a novel regularized regression approach, called adaptive multi-task Lasso, to
effectively incorporate both the relatedness among multiple gene-expression traits and useful prior
knowledge for challenging eQTL detection. Although some methods have been developed for either
adaptive or multi-task learning, to the best of our knowledge, adaptive multi-task Lasso is the ﬁrst
method that can consider prior information on SNPs and multi-task learning simultaneously in one
single framework. For example, Lirnet uses prior knowledge on SNPs such as conservation scores,
non-synonymous coding and UTR regions for a better search of association mappings [3]. However,
Lirnet considers the average effects of SNPs on gene modules by assuming that association SNPs are
shared in a module. This approach is different from multi-task learning where association SNPs are
found for each trait while considering group effects over multiple traits. To ﬁnd genetic markers that
affect correlated traits jointly, the graph-guided fused Lasso [4] was proposed to consider networks
over multiple traits within an association analysis. However, graph-guided fused Lasso does not
incorporate prior knowledge of genomic locations.

Unlike other methods, we de ﬁne the adaptive multi-task Lass o as ﬁnding a MAP estimate of a
Bayesian network, which provides an elegant Bayesian interpretation of our approach; the resultant
optimization problem is efﬁciently solved with an alternat ing minimization procedure. Finally, we
present empirical results on both simulated and real yeast eQTL datasets, which demonstrates the
advantages of adaptive multi-task Lasso over many other competitors.

2 Problem Deﬁnition: Adaptive Multi-task Lasso

Let Xij ∈ {0, 1, 2} denote the number of minor alleles at the j -th SNP of i-th individual for
i = 1, . . . , N and j = 1, . . . , p. We have K related gene traits and Y k
represents the gene
i
expression level of k -th gene of i-th individual for k = 1, . . . , K .
In our setting, we assume
that the K traits are related to each other and we explore the relatedness in a multi-task learning
framework. To achieve the relatedness among tasks via grouping effects [5], we can use any
clustering algorithms such as spectral clustering or hierarchical clustering. In association mapping
problems, these clusters can be viewed as clusters of genes which consist of regulatory networks or
pathways [4]. We treat the problem of detecting eQTLs as a linear regression problem. The general
setting includes one design matrix X and multiple tasks (genes) for k = 1, . . . , K ,
Y k = X β k + ǫ
(1)
where ǫ is a standard Gaussian noise. We further assume that Xij ’s are standardized such that
ij /N = 1, and consider a model without an intercept.
Pi Xij /N = 0 and Pi X 2
Now, the open question is how we can devise an appropriate objective function over β that could ef-
fectively consider the desirable group effects over multiple traits and incorporate useful prior knowl-
edge, as we have stated. To explain the motivation of our work and provide a useful baseline that
ndard Lasso and multi-task Lasso.
grounds the proposed approach, we ﬁrst brie ﬂy review the sta

2.1 Lasso and Multi-task Lasso

Lasso [2] is a technique for estimating the regression coefﬁ cients β and has been widely used
for association mapping problems. Mathematically, it solves the ℓ1 -regularized least square problem,
p
Xj=1

ˆβ = argmin
β

(2)

1
2

kY − X βk2
2 + λ

δj |βj |

2

where λ determines the degree of regularization of nonzero βj . The scaling parameters δj ∈ [0, 1]
are usually ﬁxed (e.g., unit ones) or set by cross-validatio n, which can be very difﬁcult when p is
large. Due to the singularity at the origin, the ℓ1 regularization (Lasso penalty) can yield a stable and
sparse solution, which is desirable for association mapping problems because in most cases we have
p ≫ N and there exists only a small number of eQTLs. It is worth mentioning that Lasso estimates
are posterior mode estimates under a multivariate independent Laplace prior for β [2].
As we can see from problem (2), the standard Lasso does not distinguish the inputs and regression
coefﬁcients from different tasks.
In order to capture some d esirable properties (e.g., shared
structures or sparse patterns) among multiple related tasks, the multi-task Lasso was proposed [5],
which solves the problem,

p
K
1
Xk=1
Xj=1
2
where kβj k2 = qPk (β k
2 is the ℓ2 -norm. This model encourages group-wise sparsity across
j )
related tasks via the ℓ1/ℓ2 regularization. Again, the solution of Eq. (3) can be interpreted as a MAP
estimate under appropriate priors with ﬁxed scaling parameters.

kY k − X β k k2
2 + λ

δj kβj k2

min
β

(3)

Multi-task Lasso has been applied (with some extensions) to perform association analysis [4]. How-
ever, as we have stated, the limitation of current approaches is that they do not incorporate the useful
prior knowledge. The proposed adaptive multi-task Lasso, as to be presented, is an extension of the
multi-task Lasso to perform joint group-wise and within-group feature selection and incorporate the
useful prior knowledge for effective association analysis.

2.2 Adaptive Multi-task Lasso

θj

(4)

min
β

ρj kβj k2

|β k
j | + λ2

kY k − X β k k2
2 + λ1

Now, we formally introduce the adaptive multi-task Lasso. For clarity, we ﬁrst de ﬁne the
sparse
multi-task Lasso with ﬁxed scaling parameters, which will be a sub-problem of
the adaptive
multi-task Lasso, as we shall see. Speciﬁcally, sparse mult i-task Lasso solves the problem,
p
p
K
K
1
Xj=1
Xk=1
Xj=1
Xk=1
2
where θ and ρ are the scaling parameters for the ℓ1 and ℓ1/ℓ2 -norm, respectively. The regularization
parameters λ1 and λ2 can be determined by cross or holdout validation. Obviously, this model sub-
sumes the standard Lasso and multi-task Lasso, and it has three advantages over previous models.
First, unlike the multi-task Lasso, which contains the ℓl/ℓ2 -norm only to achieve group-wise spar-
sity, the ℓ1 -norm in Eq. (4) can achieve sparsity among SNPs within a group. This property is useful
when K tasks are not perfectly related and we need additional sparsity in each block of kβj k2 . In
section 4, we demonstrate the usefulness of the blended regularization. The hierarchical penaliza-
tion [6] can achieve a smooth shrinkage effect for variables within a group, but it cannot achieve
within-group sparsity. Second, unlike Lasso we induce group sparsity across multiple related traits.
Finally, as to be extended, unlike Lasso and multi-task Lasso which treat βj equally or with a ﬁxed
scaling parameter, we can adaptively penalize each βj according to prior knowledge on covariates
in such a way that SNPs having desirable features are less penalized (see Fig. 1 for details of prior
knowledge on SNPs).

To incorporate the prior knowledge as we have stated, we propose to automatically learn the scaling
parameters (θ, ρ) from data. To that end, we de ﬁne θ and ρ as mixtures of features on j -th SNP, i.e.
t and ρj = Xt
θj = Xt
νt f j
ωtf j
(5)
t ,
t is t-th feature for j -th SNP. For example f j
where f j
t can be a conservation score of j -th SNP or one
if the SNP is located on TFBS, zero otherwise. To avoid scaling issues, we assume each feature is
standardized, i.e., Pj f j
t = 1, ∀t. Since we are interested in the relative contributions from different
features, we further add the constraints that Pt ωt = 1 and Pt νt = 1. These constraints can be
interpreted as a regularization on the feature weights ω ≥ 0 and ν ≥ 0.
Although using the de ﬁnitions (5) in problem (4) and jointly estimating β and feature weights (ω , ν )
can give a solution of adaptive multi-task learning, the resultant method would be lack of an el-
egant Bayesian interpretation, which is a desirable property that can make the framework more

3

ﬂexible and easily extensible. Recall that the Lasso
estimates can be interpreted as MAP estimates under
Laplace priors. Similarly, to achieve a framework
that enjoys an elegant Bayesian interpretation, we
de ﬁne a Bayesian network and treat the adaptive
multi-task learning problem as ﬁnding its MAP
estimate. Speciﬁcally, we build a Bayesian network
as shown in Fig. 2 in order to compute the MAP
estimate of β under adaptive scaling parameters,
{θ, ρ}. We de ﬁne the conditional probability of β
given scaling parameters as,

f1

fT

ω

θ

ρ

ν

X

Y

β

Figure 2: Graphical model representation of
adaptive multi-task Lasso.

P (β |θ, ρ) =

1
Z (θ, ρ)

p
p
K
Yk=1
Yj=1
Yj=1
where Z (θ, ρ) is a normalization factor, and P (Y |X, β ) ∼ N (X β , Σ), where Σ is the identity
matrix. Although in principle we can treat θ and ρ as random variables and de ﬁne a fully Bayesian
approach, for simplicity, we de ﬁne θ and ρ as deterministic functions of ω and ν as in Eq. (5).
Extension to a fully Bayesian approach is our future work.

exp (−θj |β k
j |) ×

exp (−ρj kβj k2 )

Now we de ﬁne the adaptive multi-task Lasso as ﬁnding the MAP estimation of β and simultane-
ously estimating the feature weights (ω , ν ), which is equivalent to solving the optimization problem,

p
p
K
K
Xj=1
Xj=1
Xk=1
Xk=1
where ω and ν are related to θ and ρ through Eq. (5) and subject to the constraints as de ﬁned abov e.

kY k − X β k k2
2 + λ1

ρj kβj k2 + log Z (θ, ρ),

|β k
j | + λ2

min
β ,ω ,ν

(6)

1
2

θj

Remark 1 Although we can interpret problem (4) as a MAP estimate of β under appropriate priors when
scaling parameters (θ, ρ) are ﬁxed, it does not enjoy an elegant Bayesian interpretati on if we perform joint
estimation of β and the scaling parameters (ω , ν ) because it ignores normalization factors of the appropriate
priors. Lee et al. [3] used this approach where a regularized regression model is optimized over scaling
parameters and β jointly. Therefore, their method does not have an elegant Bayesian interpretation. Moreover,
as we have stated, Lee et al. [3] did not consider grouping effects over multiple traits.

Remark 2 Our method also differs from the adaptive Lasso [7] , transfer learning with meta-priors [8] and
the Bayesian Lasso [9]. First, although both adaptive Lasso and our method use adaptive parameters for
penalizing regression coefﬁcients, we learn adaptive para meters from prior knowledge on covariates in a multi-
task setting while adaptive Lasso uses ordinary least square solutions for adaptive parameters in a single task
setting. Second, the method of transfer learning with meta-priors [8] is similar to our method in a sense that
both use prior knowledge with multiple related tasks. However, we couple related tasks via ℓ1/ℓ2 penalty while
they couple tasks via transferring hyper-parameters among them. Thus we have group sparsity across tasks
as well as sparsity in each group but they cannot induce group sparsity across different tasks. Finally, the
Bayesian Lasso [9] does not have the grouping effects in multiple traits and the priors used usually do not
consider domain knowledge.

3 Optimization: an Alternating Minimization Approach

Z ≤

Now, we solve the adaptive multi-task Lasso problem (6). First, since the normalization factor Z is
hard to compute, we use its upper bound, as given by,
K−1
p
p
θj (cid:19)K
θj (cid:19)K
2 )2K
2 Γ( K+1
(cid:18) 2
(cid:18) 2
Yj=1 ZRK
exp (−kρj k2 )dρ Yj
(ρj K )K Yj
Yj=1
This integral result is due to normalization constant of K dimensional multivariate Laplace distri-
bution [10, 11]. Using this upper bound, the learning problem is to minimize an upper bound of the
objective function in problem (6), which will be denoted by L(β , ω , ν ) henceforth. Although L is
not joint convex over β , ω and ν , it is convex over β given {ω , ν } and convex over {ω , ν } given β .
We use an alternating optimization procedure which (1) minimizes the upper bound L of problem (6)
over {ω , ν } by ﬁxing β ; and (2) minimizes L over β by ﬁxing {ω , ν } iteratively until convergence
[12]. Both sub-problems are convex and can be solved efﬁcien tly via a projected gradient descent
method and a coordinate descent method, respectively.

(7)

=

π

.

4

(−K log ρj + ρj kβj k2 ) ,

For the ﬁrst step of optimizing L over ω and ν , the sub-problem is to solve
ω∈Pω ,ν∈Pν Xj Xk (cid:16)− log θj + θj |β k
j |(cid:17) + Xj
min
where Pω , {ω : Pt ωt = 1, ωt ≥ 0, ∀t} is a simplex over ω , likewise for Pν . θ and ρ are
functions of ω and ν as de ﬁned in Eq. (5). This constrained problem is convex and c an be solved by
using a gradient descent algorithm combined with a projection onto a simplex sub-space, which can
be efﬁciently done [13]. Since ω and ν are not coupled, we can learn each of them separately.
For the second sub-problem that optimizes L over β given ﬁxed feature weights
(ω , ν ), it is exactly
the optimization problem (4). We can solve it using a coordinate descent procedure, which has been
used to optimize the sparse group Lasso [14]. Our problem is different from the sparse group Lasso
in the sense that the sparse group Lasso includes group penalty over multiple covariates for a single
trait, while adaptive multi-task Lasso considers group effects over multiple traits. Here we solve
problem (4) using a modiﬁed version of the algorithm propose d for the sparse group Lasso.

As summarized in Algorithm 1, the general optimization procedure is as follows: for each j , we
check the group sparsity condition that βj = 0. If it is true, no update is needed for βj . Otherwise,
we check whether β k
j = 0 for each k . If it is true that β k
j = 0, no update is needed for β k
j ; otherwise,
we optimize problem (4) over β k
j with all other coefﬁcients ﬁxed. This one-dimensional opti miza-
tion problem can be efﬁciently solved by using a standard opt imization method. This procedure is
continued until a convergence condition is met.
problem (4) by computing the subgra-
More speciﬁcally, we ﬁrst obtain the optimal conditions for
j and set it to zero:
dient of its objective function with respect to β k
j + λ1θj hk
j (Y k − X β k ) + λ2 ρj g k
−X T
j = 0,

(8)

where g and h are sub-gradients of the ℓ1/ℓ2 -norm and the ℓ1 -norm, respectively. Note that gk
j =
β k
j ∈ [−1, 1].
j 6= 0, otherwise hk
j ) if β k
if βj 6= 0, otherwise kgj k2 ≤ 1; and hk
j = sign(β k
j
kβj k2
Then, we check the group sparsity that βj = 0. To do that, we set βj = 0 in Eq. (8), and we have,

r − λ1θj hk
Xr β k
j )2 .

1
2ρ2
λ2
j

j +λ1 θj hk
r = λ2ρj g k
Xr β k
j , and ||gj ||2
2 =

K
j Xr 6=j
j Xr 6=j
Xk=1
j Y k−X T
X T
j Y k − X T
(X T
According to subgradient conditions, we need to have a gj that satisﬁes the
less than inequality
2 < 1; otherwise, βj will be non-zero. Since gj is a function of hj , it sufﬁces to check whether
kgj k2
the minimal square ℓ2 -norm of gj is less than 1. Therefore, we solve the minimization problem of
2 w.r.t hj , which gives the optimal hj as,
kgj k2
j = 
hk

where ck
2 is less than 1, then βj is zero and no
r . If the minimal kgj k2
j Y k − X T
j =X T
j Pr 6=jXr β k
j =0, ∀k , as follows.
update is needed; otherwise, we continue to the next step of checking whether β k

if |

ck
j
λ1 θj
otherwise

| ≤ 1

ck
j
λ1 θj

sign(

ck
j
λ1 θj

)

(9)

j = 0 in Eq. (8), we have,
Again, we start by assuming β k
j is zero. By setting β k
1
j Xr 6=j
j Xr 6=j
(X T
j Y k − X T
j , and hk
r = λ1θj hk
Xr β k
j Y k − X T
X T
j =
λ1θj
According to the de ﬁnition of the subgradient hk
j , it needs to satisfy the condition that |hk
j | < 1;
j will be non-zero. This checking step can be easily done. After the check, if we have
otherwise, β k
j 6= 0, the problem (4) becomes an one-dimensional optimization problem with respect to β k
j , and
β k
the solution can be obtained using existing optimization algorithms (e.g. optimize function in the
R). We used majorize-minimize algorithm with gradient descent [15].

Xr β k
r ).

With the above two steps, we iteratively optimize (ω , ν ) by ﬁxing β and optimize β by ﬁxing feature
weights until convergence. Note that the parameters λ1 and λ2 in Eq. (4), which determine sparsity
levels, are determined by cross or hold-out validation.

5

Input : X ∈ RN ×p ; Y ∈ RN ×K ; θ ∈ Rp ; ρ ∈ Rp ; and β init ∈ Rp×K
Output: β ∈ Rp×K
β ← β init ;
Iterate this procedure until convergence;
for j ← 1 to p do
j and hk
j )2 where ck
j − λ1 θj hk
k=1 (ck
PK
j are computed as in Eq. (9);
m ← 1
λ2
2 ρ2
j
if m < 1 then β k
j = 0, for all k = 1, . . . K ;
else for k ← 1 to K do
j Xj β k
j (Y k − X β k ) + X T
|X T
j |;
q ← 1
λ1 θj
if q < 1 then β k
j = 0;
else Solve the following one-dimensional optimization problem:
2 kY k − X β k k2
2 + λ1 θj |β k
β k
j | + λ2 ρj kβj k2 ;
1
j ← argmin
βk
j

end

end
Algorithm 1: Optimization algorithm for Equation (4) with ﬁxed scaling p arameters.

4 Simulation Study

To con ﬁrm the behavior of our model, we run the adaptive multi -task Lasso and other methods on
our simulated dataset (p=100, K=10). We ﬁrst randomly selec t 100 SNPs from 114 yeast genotypes
from the yeast eQTL dataset [16]. Following the simulation study in Kim et al. [4], we assume that
some SNPs affect biological networks including multiple traits, and true causal SNPs are selected
by the following procedure. Three sets of randomly selected four SNPs are associated with three
trait clusters (1 − 3), (4 − 6), (7 − 10), respectively. One SNP is associated with two clusters
(1 − 3) and (4 − 6), and one causal SNP is for all traits (1 − 10). For all association SNPs we
set identical association strength from 0.3 to 1. Traits are generated by Y k = X β k + ǫ, for all
k = 1, . . . , 10 where ǫ follows the standard normal distribution. We make 10 features (f1 − f10 ),
of which six are continuous and four are discrete. For the ﬁrs t three continuous features (f1 − f3 ),
the feature value is drawn from s(N (2, 1)) if a SNP is associated with any traits; otherwise from
1
s(N (1, 1)), where s(x) =
1+exp(x) is the sigmoid function. For the other three continuous features
(f4 − f6 ), the value is drawn from s(N (2, 0.5)) if a SNP is associated with any traits; otherwise from
s(N (1, 0.5)). Finally, for the discrete features (f7 − f10 ), the value is set to s(2) with probability
0.8 if a SNP is associated with any traits; otherwise to s(1). We standardize all the features.
l
/l∞
1

True β

Single SNP

A + l

/l
1
2

AML

SML

Lasso

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

 

10

20

30

40

50

60

70

80

90

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

 

2 4 6 8 10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Figure 3: Results of the β matrix estimated by different methods. For visualization, we present normalized
absolute values of regression coefﬁcients and darker color s imply stronger association with traits. For each
matrix, X-axis represents traits (1-10) and Y-axis represents SNPs (1-100). True β is shown in the left.

Fig. 3 shows the estimated β matrix by various methods including AML (adaptive multi-task Lasso),
SML (sparse multi-task Lasso which is AML without adaptive weights), A+ℓ1/ℓ2 (AML without
Lasso penalty), Single SNP [17], Lasso and ℓ1/ℓ∞ (multi-task learning with ℓ1/ℓ∞ norm). In this
ﬁgure, X-axis represents traits (1-10) and Y-axis represen ts SNPs (1-100). Note that regression
parameters (e.g. λ1 and λ2 for AML) were determined by holdout validation, and we set association
strength to 0.3. We also used hierarchical clustering with cutoff criterion 0.8 prior to run AML,
SML, A+ℓ1/ℓ2 and ℓ1/ℓ∞ , and Single SNP and Lasso were analyzed for each trait separately.
We investigate the effect of Lasso penalty in our model by comparing the results of AML and
A+ℓ1/ℓ2 . While AML is slightly more efﬁcient than A+ ℓ1/ℓ2 in ﬁnding association SNPs, both

6

work very well for this task. It is not surprising since hierarchical clustering reproduced true trait
clusters and true β could be detected without considering single SNP level sparsity in each group.
To further validate the effectiveness of Lasso penalty, we run AML and A+ℓ1/ℓ2 without a priori
clustering step. Interestingly, AML could pick correct SNP-traits associations due to Lasso penalty,
however, A+ℓ1/ℓ2 failed to do so (see Fig. 5c,d for the comparison of performance). While Lasso
penalty did not show signiﬁcant contribution for this task w hen we generated a priori clusters, it is
good to include it when the quality of a clustering is not guaranteed. Comparing the results of AML
and SML in Fig. 3, we could observe that adaptive weights improve the performance signiﬁcantly.
Adaptive weights help not only reduce false positives but also increase true positives.

Fig. 4 shows the learned feature weights of ω (ν is al-
most identical to ω and not shown here). The results are
based on 100 simulations for each association strength
0.3, 0.5, 0.8 and 1, and half of error bar represents one
standard deviation from the mean. We could observe that
discrete features f7 − f10 have highest weights while low-
est weights are assigned to f1 − f3 . These weights are
reasonable because f1 − f3 are drawn from Gaussian with
large standard deviation (STD: 1) compared to that of fea-
tures f4 − f6 (STD: 0.5). Also, discrete features are the
most important since they discriminate true association
SNPs with a high probability 0.8.

0.16

0.14

0.12

t
ω

0.1

0.08

0.06

0.04

0.02

f
1

f
2

f
3

f
4

f
5

f
6

f
7

f
8

f
9

f
10

Features
Figure 4: Learned feature weights of ω .

a
1

0.8

0.6

0.4

0.2

y
t
i
v
i
t
i
s
n
e
S

b
1

0.8

0.6

0.4

0.2

y
t
i
v
i
t
i
s
n
e
S

c
1

0.8

0.6

0.4

0.2

y
t
i
v
i
t
i
s
n
e
S

 

d
1

0.8

0.6

0.4

0.2

y
t
i
v
i
t
i
s
n
e
S

 

AML
SML
A+l1/l2
l1/l∞
Lasso
Single SNP

0

0

0

0

1

0.5
0.5
0.5
0.5
1 − Specificity
1 − Specificity
1 − Specificity
1 − Specificity
Figure 5: ROC curves of various methods as association strength varies (a) 0.3, (b) 0.5 on clustered data, (c)
0.3, and (d) 0.5 on input dataset. (a,b) Results on clustered data, where correct groups of gene traits are found
using hierarchical clustering (cutoff = 0.8). (c,d) Results on input dataset without using clustering algorithm.

1

1

1

0

 
0

0

 
0

We compare the sensitivity and speciﬁcity of our model with o ther methods. In Fig. 5, we generated
ROC curves for association strength of 0.3 and 0.5. Fig. 5a,b show the results with a priori hierar-
chical clustering and Fig. 5c,d is with no such preprocessing steps. Using hierarchical clustering we
could correctly ﬁnd three clusters of gene traits at cutoff 0.8. In Fig. 5, when association strength
is small (i.e., 0.3), AML and A+ℓ1/ℓ2 signiﬁcantly outperformed other methods. As association
strength increased, the performance of multi-task learning methods improved quickly while meth-
ods based on a single trait such as Lasso and Single SNP showed gradual increase of performance.

We computed test errors on 100 simulated dataset using 30 samples for test and 84 samples for
training. On average, AML achieved the best test error rate of 0.9427, and the order of other methods
in terms of test errors is: A + ℓ1/ℓ2 (0.9506), SML (1.0436), ℓ1/ℓ∞ (1.0578) and Lasso (1.1080).

5 Yeast eQTL dataset

We analyze the yeast eQTL dataset [16] that contains expression levels of 5,637 genes and 2,956
SNPs. The genotype data include genetic variants of 114 yeast strains that are progenies of the
standard laboratory strain (BY) and a wild strain (RM). We used 141 modules given by Lee et al.
[3] as groups of gene traits, and extracted unique 1,260 SNPs from 2,956 SNPs for our analysis. For
prior biological knowledge on SNPs used for adaptive multi-task Lasso, we downloaded 12 features
from Saccharomyces Genome Database (http://www.yeastgenome.org) including 11 discrete and 1
continuous feature (conservation score). For a discrete feature, we set its value as f j
t = s(2) if the
feature is found on the j -th SNP, f j
t = s(1) otherwise. For conservation score, we set f j
t = s(score).
All the features are then standardized.

7

0.2

0.18

0.12

0.16

0.14

Fig. 6 represents ω learned from the yeast eQTL dataset
(ν is almost identical to ω ). The features are ncRNA (f1 ),
noncoding exon (f2 ), snRNA (f3 ), tRNA (f4 ), intron (f5 ),
binding site (f6 ), 5’ UTR intron (f7 ), LTR retrotranspo-
son (f8 ), ARS (f9 ), snoRNA (f10 ), transposable element
gene (f11 ) and conservation score (f12 ). Five discrete fea-
tures turn out to be important including ncRNA, snRNA,
binding site, 5’ UTR intron and snoRNA as well as one
continuous feature, i.e., conservation score. These re-
sults agree with biological insights. For example, ncRNA,
snRNA and snoRNA are potentially important for gene
regulation since they are functional RNA molecules hav-
ing a variety of roles such as transcriptional regulation [18]. Also, conservation score would be
signiﬁcant since mutation in conserved region is more likel y to result in phenotypic effects.

f5 f6 f7 f8 f9
Features
Figure 6: Learned weights of ω on the yeast
eQTL dataset.

f1 f2 f3 f4

f10 f11

f12

t
ω

0.1

0.08

0.06

0.04

0.02

0

2
0
2
 
*
 
s
t
i
a
r
t
 
d
e
t
a
i
c
o
s
s
a
 
f
o
 
r
e
b
m
u
N

3.5

3

2.5

2

1.5

1

0.5

0

 
0

 

β
ncRNA
snRNA
binding sites
five prime UTR intron
conservation scores

10

20

30

40

50

60

70

80

90

100

110

120

SNPs
Figure 7: Plot of 121 SNPs on chromosome 1 and 2 vs the number of genes affected by the SNPs from the
yeast eQTL analysis (blue bar). Five signi ﬁcant prior knowl edge on SNPs are overlapped with the plot. For
the four discrete priors (ncRNA, snRNA, binding site, 5’ UTR intron) we set the value to 1 if annotated, 0
otherwise. Binding sites and regions with no associated traits are denoted by long green and short blue arrows.

Fig. 7 shows the number of associated genes for SNPs on chromosome 1 and 2, superimposed on 5
signiﬁcant features. We see that association mapping resul ts were affected by both priors and data.
For example, genomic region indicated by blue arrow shows weak association with traits, where
conservation score is low and no other annotations exist. Also we can see that three SNPs located on
binding sites affect a larger number of gene traits (see green arrows). As an example of biological
analysis, we investigate these three association SNPs. The three SNPs are located on telomeres
(chr1:483, chr1:229090, chr2:9425 (chromosome:coordinate)), and these genomic locations are in
cis to Abf1p (autonomously replicating sequence binding factor-1) binding sites. In biology, it is
known that Abf1p acts as a global transcriptional regulator in yeast [19]. Thus, the genomic regions
in telomeres would be good candidates for novel putative eQTL hotspots that regulate the expression
levels of many genes. They were not reported as eQTL hotspots in Yvert et al. [20].

6 Conclusions

In this paper, we proposed a novel regularized regression model, referred to as adaptive multi-task
Lasso, which takes into account multiple traits simultaneously while weights of different covariates
are learned adaptively from prior knowledge and data. Our simulation results support that our model
outperforms other methods via ℓ1 and ℓ1/ℓ2 penalty over multiple related genes, and especially
adaptively learned regularization signiﬁcantly improved the performance. In our experiments on the
yeast eQTL dataset, we could identify putative three eQTL hotspots with biological supports where
SNPs are associated with a large number of genes.

Acknowledgments

This work was done under a support from NIH 1 R01 GM087694-01, NIH 1RC2HL101487-01
(ARRA), AFOSR FA9550010247, ONR N0001140910758, NSF Career DBI-0546594, NSF IIS-
0713379 and Alfred P. Sloan Fellowship awarded to E.X.

8

References
[1] R. Sladek, G. Rocheleau, J. Rung, C. Dina, L. Shen, D. Serre, P. Boutin, D. Vincent, A. Belisle,
S. Hadjadj, et al. A genome-wide association study identiﬁe s novel risk loci for type 2 diabetes.
Nature, 445(7130):881 –885, 2007.
[2] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267 –288, 1996.
[3] S.I. Lee, A.M. Dudley, D. Drubin, P.A. Silver, N.J. Krogan, D. Pe’er, and D. Koller. Learning
a prior on regulatory potential from eQTL data. PLoS Genetics, 5(1):e1000358, 2009.
[4] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative
trait network. PLoS Genetics, 5(8):e1000587, 2009.
[5] G. Obozinski, B. Taskar, and M. Jordan. Multi-task feature selection. In Technical Report,
Department of Statistics, University of California, Berkeley, 2006.
[6] M. Szafranski, Y. Grandvalet, and P. Morizet-Mahoudeaux. Hierarchical penalization. Ad-
vances in Neural Information Processing Systems, 20:1457 –1464, 2007.
[7] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical
Association, 101(476):1418 –1429, 2006.
[8] S.I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller. Learning a meta-level prior for feature
relevance from multiple related tasks. In Proceedings of the 24th International Conference on
Machine Learning, pages 489 –496, 2007.
[9] T. Park and G. Casella. The bayesian Lasso. Journal of the American Statistical Association,
103(482):681 –686, 2008.
[10] B. M. Marlin, M. Schmidt, and K. P. Murphy. Group sparse priors for covariance estimation. In
Proceedings of the 25th Conference on Uncertainty in Artiﬁc ial Intelligence, pages 383 –392,
2009.
[11] E. G ´omez, M. A. Gomez-Viilegas, and J. M. Marin. A multi variate generalization of the
power exponential family of distributions. Communications in Statistics-Theory and Methods,
27(3):589 –600, 1998.
[12] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse coding algorithms. Advances in
Neural Information Processing Systems, 19:801 –808, 2007.
[13] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the
ℓ1 -
ball for learning in high dimensions. In Proceedings of the 25th International Conference on
Machine Learning, pages 272 –279, 2008.
[14] J. Friedman, T. Hastie, and R. Tibshirani. A note on the group Lasso and a sparse group Lasso.
arXiv:1001.0736v1 [math.ST], 2010.
[15] T. T. Wu and K. Lange. Coordinate descent algorithms for Lasso penalized regression. Ann.
Appl. Stat, 2(1):224 –244, 2008.
[16] R. B. Brem and L. Kruglyak. The landscape of genetic complexity across 5,700 gene expres-
sion traits in yeast. Proceedings of the National Academy of Sciences of the United States of
America, 102(5):1572 –1577, 2005.
[17] S. Purcell, B. Neale, K. Todd-Brown, L. Thomas, M. A. R. Ferreira, D. Bender, J. Maller,
P. Sklar, P. I. W. De Bakker, M. J. Daly, et al. PLINK: a tool set for whole-genome association
and population-based linkage analyses. The American Journal of Human Genetics, 81(3):559 –
575, 2007.
[18] G. Storz. An expanding universe of noncoding RNAs. Science, 296(5571):1260 –1263, 2002.
[19] T. Miyake, J. Reese, C. M. Loch, D. T. Auble, and R. Li. Genome-wide analysis of ARS (au-
tonomously replicating sequence) binding factor 1 (Abf1p)-mediated transcriptional regulation
in Saccharomyces cerevisiae. Journal of Biological Chemistry, 279(33):34865 –34872, 2004.
[20] G. Yvert, R. B. Brem, J. Whittle, J. M. Akey, E. Foss, E. N. Smith, R. Mackelprang,
L. Kruglyak, et al. Trans-acting regulatory variation in Saccharomyces cerevisiae and the
role of transcription factors. Nature Genetics, 35(1):57 –64, 2003.

9


Multiclass Clustering using a Semideﬁnite Relaxation

Jason Lee
Institute of Computational and Mathematical Engineering
Stanford University
email: jdl17@stanford.edu

Abstract

We propose a semideﬁnite relaxation for graph clustering known as Max-cut clus-
tering. The clustering problem is formulated in terms of a discrete optimization
problem and then relaxed to a SDP. The SDP is solved using a low-rank factoriza-
tion trick that reduces the number of variables, and then using a simple projected
gradient method. This is joint work with Nathan Srebro at the Toyota Technology
Institute-Chicago and part of research was performed at the Toyota Technology
Institute-Chicago.

1

Introduction

Graph clustering is often formulated as a discrete optimization problem with the goal of balancing
two criteria: cluster coherence and cluster size balance. We ﬁrst discuss the speciﬁc case of binary
clustering to highlight the tradeoffs between cluster coherence and cluster size. We then formulate
this tradeoff as a discrete quadratic integer program and discuss the connection with the max-cut
problem. As with max-cut, the discrete problem can be relaxed to a semideﬁnite program (SDP)
that can be solved with standard solvers. To make the algorithm scalable, a low rank factorization
approach similar to [1] is used to solve the SDP.

1.1 Graph Clustering

There are many possible clustering objectives that have been proposed in the literature and it is sim-
ple to construct new objectives that achieve the two desired properties of cluster size and quality.
The two most common formulations are known as the Ratio Cut [2] and Normalized Cut [3, 4].
Both objectives are discrete and known to be NP-hard, however there is a continuous eigenvalue
relaxation of the problems which leads to spectral clustering on a graph Laplacian. Due to the ease
of computing eigenvectors and the intricate connection with spectral graph theory, graph Laplacian
relaxation methods are commonly used. See [5] for further discussion and comparison of graph
Laplacian-based clustering methods. A problem with the eigenvalue approach is the eigenvalue re-
laxation is a very loose approximation to the original discrete optimization problem; several authors
have proposed tighter relaxations using semideﬁnite programming [6, 7], but these methods only
scale to small-size problems.
Instead of trying to design better clustering objectives, which has
been extensively studied, we propose a simple discrete objective, that admits a solvable, yet tight
relaxation through the max-cut SDP.

2 Binary Clustering

2.1 Max-Cut Problem

The well-studied max-cut problem (partition the vertices into C and C such that the number of
edges between C and C is maximized) can be formulated as the following binary quadratic integer

1

program:

Qij (1 − xixj ) subject to x2
i = 1, for all i.

(cid:88)
(cid:88)
maximize 1
4
j
i
The max-cut problem attempts to maximize the number of edges cut, so a ﬁrst attempt at using it
for clustering is to let Q = −W . However, this leads to the trivial solution of C = V . A simple ﬁx
is to deﬁne Q = δJ − W = δeeT − W for some δ > 0, where J is the matrix of all ones and e is
maximize (cid:88)
the vector of all ones. This choice of Q forces balanced clusters while minimizing the inter-cluster
edges. Thus we solve the following problem, which is equivalent to max-cut with Q = δJ − W :
(Wij − δ)xixj subject to x2
The objective function can be rewritten as (cid:80)
l xl )2 .The term δ((cid:80)
i,j Wij xixj − δ((cid:80)
(2)
i = 1.
i,j
(cid:80)
l xl )2 can
(cid:88)
(cid:88)
be viewed as a penalty function for unbalanced clusters since perfectly balanced clusters satisfy
l xl = 0. Using this observation, Equation 2 is equivalent to
xi )2 ≤ B
subject to x2
i = 1 and
(
Wij xixj
i
i,j
to penalize the difference in volume by replacing ((cid:80)
i xi )2 ≤ B with ((cid:80)
Thus Equation 2 is an example of a bi-criterion objective; maximizing the objective leads to maxi-
mizing W (C, C )+W (C , C ) while minimizing |C |− |C |. We can easily modify the penalty function
i dixi )2 ≤ B .
2.2 Semideﬁnite Relaxation
maximize (cid:88)
The discrete optimization problem proposed in Equation 2 is known to be NP-hard to solve, so we
reforumlate it as a continuous problem:
(Wij − δ)Xij
ij
maximize (cid:88)
This reformulation is equivalent to the discrete problem. However, it is non-convex due to the rank
1 constraint on X . By dropping this constraint, we arrive at the semideﬁnite relaxation of max-cut.
(Wij − δ)Xij
ii = 1 and X (cid:23) 0
subject to X 2
(5)
ij
The semideﬁnite relaxation is a convex problem that can be efﬁciently solved using standard interior
point solvers. Unfortunately, these solvers do not scale to problems with more than a few hundred
maximize (cid:88)
variables. To motivate our solution method, we ﬁrst study an equivalent vector formulation of the
semideﬁnite relaxation.
vi ∈ Rn
(Wij − δ) < vi , vj > subject to ||vi ||2 = 1 and
(6)
ij
In the vector formulation, each binary variable xi is replaced with a vector vi ∈ Rn . The variables
in the vector formulation is V ∈ Rnxn where vi are the rows of V . This semideﬁnite program has
n2 variables; a key idea from Burer and Monteiro [1] is that the number of variables can be reduced
if we constrain each vi ∈ Rr for r < n. For r = 1, the rank-constrained formulation is recovered.
Burer and Monteiro show that if r is large enough the solution to the non-convex problem with
vi ∈ Rr is equivalent to the global optimum of the sdp (Equation 6). The ﬁnal reformulation that
we solve is the rank r constrained relaxation to the discrete problem.
maximize Tr((W − δJ )V V T )
subject to ||vi ||2 ≤ 1
vi ∈ Rr

and

subject to X = xxT

and X 2
ii = 1

(4)

(1)

(3)

(7)

2.3 Projected Gradient Method

To solve Equation 7, we use a simple projected gradient algorithm. The projected gradient algorithm
updates with the rule
V ← P (V + τ (W − δJ )V )
(8)
where P can be computed by normalizing the rows of V . This algorithm is extremely simple and
efﬁcient; each iteration involves matrix-multiplication and normalizing the rows of V . The storage
required is nr variables and in the experiments we choose r ≤ 20. Furthermore, we are guaranteed
the global optimum of the sdp formulation if rk(V ∗ ) < r [1].

2

Wij,abxiaxj b

2.4 Max-Cut Clustering
We ﬁrst formulate the Max-Cut clustering as a discrete problem of the form 2 and then employ the
(cid:80)
same relaxation as described in the binary case. Let k denote the number of clusters and xia = −1, 1
be cluster indicator variables where xia = 1 means node i belongs to cluster a. The xia satisfy
a xia = 2 − k to ensure each node belongs only to one cluster. A binary quadratic program for
maximize (cid:88)
multiclass clustering can be posed as:
(cid:88)
(cid:88)
ij,ab
subject to
xia = (2 − k)
xia = 2 − k ,
n
k
a
i
function on the constraints (cid:80)
a xia = 2 − k and (cid:80)
Wij,ab = 1[a = b]Wij where Wij is the weighted adjacency matrix. The second constraint is
a cluster size constraint that forces each cluster to be of similar size. Using a quadratic penalty
i xia = (2 − k) n
k , this can be converted to a
maximize (cid:88)
max-cut type problem similar to Equation 2.1 Similarly, a low-rank factorization type relaxation can
be used to solve this. The ﬁnal problem in vector form is:
(Wij,ab−δ1[a = b]−λ1[i = j ]) < via , vj b > subject to ||vi ||2 = 1 and
ij,ab
(12)
This equation can be rewritten in the form of Equation 7 by deﬁning Dij,aa = 1 and Qii,ab = 1. D
is rank k and Q is sparse. The objective is Tr (W − δD − λQ)V V T .

and x2
ia = 1

vi ∈ Rr

(9)

(10)

(11)

2.5 Computational Considerations

In the binary case, the algorithm requires a matrix-matrix multiplication at each iteration. The
required work per iteration is O(n2 r) and the storage required is the matrix V which is O(nr),
where n = |V |.
In the multiclass case, the algorithm also requires a matrix-matrix multiplication at each iteration.
The required work per iteration is O(n2k2 r) and the storage required is O(nkr). The adjacency
matrix W is frequently sparse and the two penalty terms are sparse and rank k , respectively.

2.6 Recovering the discrete solution

2.6.1 Rounding Scheme

After solving the optimization problem given in Equation 7, we have a vector vi for each vertex.
The Goemans-Williamson randomized hyperplane rounding assigns xi = sgn(< r, vi >) where
r ∼ N (0, I ). We repeat randomized hyperplane rounding R times and choose the assignment with
largest objective as the ﬁnal clustering. In the multiclass setting, the cluster labels are chosen as
ci = argmaxa < r, via >. This is repeated R times and choose the assignment with largest
objective. We also post-process the best label selected by hyperplane rounding using the relaxation
labeling method developed for inference in markov random ﬁelds. See [9] for details.

3 Experimental Comparison

For all the experiments, we ﬁrst build a K -nearest neighbor graph with K = 10 and weights Wij
deﬁned as Wij = max(si (j ), sj (i)), with si (j ) = exp(− ||xi−xj ||2
) and σi equal to the distance
1 In the conversion to a max-cut problem, we omit terms of the type (cid:80)
2σ2
i
from xi to its K nearest neighbor.
x0 and replacing all linear terms with (cid:80)
ia βiaxia i.e. linear terms in xia . The
linear terms can be handled within our low-rank factorization framework by introducing the dummy variable
ia βiaxiax0 . This observation allows us to do MAP estimation in
Markov Random Fields using the same low-rank relaxation technique. In the multi-class clustering case, the
linear terms correspond to uniform priors over the label set and thus do not change the solution. See [8] for
details. Experimental results for the MRF are not reported here due to lack of space.

3

Description
Dataset Nc
100
Pdigit (1,7)
100
Pdigit (0,1)
Pdigit(0,1,2,3,4)
100
100
Pdigit(1,3,5,7,9)
100
Pdigit(0,2,4,6,8)
100
Pdigit(0,1,2,4,5,6,8)
Pdigits(0,1,2,3,4,5,6,7)
100
100
MNIST(0,1,2,3,4)
100
MNIST(0,1,2,3,4,6,7)
100
MNIST(1,3,5,7,9)
100
MNIST(0,2,4,6,8)
MNIST(0,1,2,3,4,5,6,7,8,9)
100
100
MNIST(1,3,4,6,8)
100
MNIST(0,1,2,4,5,7,8)
100
MNIST(5,6,7,8,9)
MNIST(0,1,2,3,4,5,6,7,8)
800
300
MNIST(2,3,4)
300
MNIST(6,7,8)
300
MNIST(6,7,8,9)
100
NewsgroupsA(7,16)
NewsgroupsA(7,16)
200
300
NewsGroups(11,12)
200
NewsGroups(7,15,17)
200
NewsGroups(7,10,15,17)
NewGroups(2,10,15,18)
200

Misclassiﬁcation Rate
SM(k) NJW (k+1)
k Max-Cut NJW(k)
0.075
0.075
0.075
0.08
2
0
0
0
0
2
0.062
0.074
0.298
5
0.298
0.138
0.16
0.174
5
0.15
0.018
0.056
0.252
0.056
5
0.123
0.126
0.237
0.151
7
0.11875
0.1188
8
0.185
0.2
0.09
0.122
0.302
0.216
5
0.12
0.3671
0.3571
7
0.18
0.186
0.35
0.342
0.356
5
0.148
0.172
0.452
0.404
5
0.421
10
0.479
0.543
0.463
0.13
0.28
0.31
0.302
5
0.4257
0.52
0.6329
0.5229
7
0.354
0.37
0.43
0.426
5
0.1106
9
0.3058
0.2415
0.2569
0.0222
0.0244
0.0544
0.0456
3
0.0067
0.0133
0.012
0.011
3
0.1567
0.2067
0.1975
0.2652
4
0.195
0.275
0.25
2
0.17
0.145
0.1475
2
0.245
0.34
0.08
0.0883
0.1083
0.0967
2
0.2567
0.33
0.425
0.375
3
0.445
0.545
0.5525
0.4888
4
0.44
4
0.4513
0.4513
0.53

SM(k+1)
0.075
0
0.08
0.19
0.184
0.236
0.1225
0.27
0.13
0.476
0.46
0.515
0.38
0.5214
0.484
0.2564
0.0367
0.0133
0.1667
0.04
0.4025
0.1067
0.43
0.6062
0.5325

Table 1: Nc is number of points per cluster and k is the number of clusters. 5 different methods
are compared: our low rank method, Shi-Malik spectral clustering with k eigenvectors, Ng-Jordan-
Weiss spectral clustering with k eigenvectors, Shi-Malik spectral clustering with k + 1 eigenvectors,
and Ng-Jordan-Weiss spectral clustering with k + 1 eigenvectors.

We test on the following datasets:

1. PenDigits (0-9). Handwritten digit dataset. Each data is 8 x-y plane measurements of the
pen position.

2. MNIST (0-9). Handwritten digit dataset. Each data point is a 28 x 28 image of a single
handwritten digit.

3. 20 Newsgroups. Each data point is a vector of term frequency. Data is collected from 20
Newsgroups on different topics.

4. Synthetic Two-Moons dataset. This experiment is reproduced from [10].

For all the experiments, λ = .05 and δ = Wtot
4|V |2 . The algorithm is run for 4000 iterations with
r = 30.
We compare against 2 different variants of the spectral clustering algorithm by Shi-Malik and Ng-
Jordan-Weiss with 50 replications of k-means. The spectral clustering is run with k and k + 1
eigenvectors2 . The clustering accuracy is evaluated by the number of mis-clustered points each
algorithm makes (the lowest among all k ! permutations is reported).

2Spectral clustering is generally done with k eigenvectors where k is the number of classes. We also test
with k + 1 because many of the errors in spectral clustering are due to the k and k + 1 eigenvalue being very
close. In fact, spectral clustering with k + 1 eigenvectors does better in our experiments.

4

(a) Spectral Clustering

(b) Max-cut clustering

Figure 1: Comparison of spectral clustering (left) with MAX -CUT clustering (right).

References
[1] Samuel Burer and R. D. C. Monteiro. A nonlinear programming algorithm for solving semideﬁnite
programs via low-rank factorization. Mathematical Programming (Series B), 95:329–357, 2003.
[2] L. Hagen and A.B. Kahng. New spectral methods for ratio cut partitioning and clustering. Computer-
Aided Design of Integrated Circuits and Systems, IEEE Transactions on, 11(9):1074 –1085, September
1992.
[3] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8):888–905, 2000.
[4] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In
ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS, pages 849–856. MIT Press, 2001.
[5] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17:395–416, 2007.
10.1007/s11222-007-9033-z.
[6] Eric Xing, Eric P. Xing, Michael Jordan, and Michael I. Jordan. On semideﬁnite relaxations for normal-
ized k-cut and connections to spectral clustering, 2003.
[7] Tijl De Bie and Nello Cristianini. Fast sdp relaxations of graph cut clustering, transduction, and other
combinatorial problems. J. Mach. Learn. Res., 7:1409–1436, December 2006.
[8] M. Pawan, Kumar V. Kolmogorov, and P. H. S. Torr. An analysis of convex relaxations for map estimation,
2008.
[9] Timothee Cour and Jianbo Shi. Solving markov random ﬁelds with spectral relaxation. In Proceedings of
the Eleventh International Conference on Artiﬁcial Intelligence and Statistics, volume 11, 2007.
[10] Jason D. Lee, Benjamin Recht, Ruslan Salakhutdinov, Nathan Srebro, and Joel A. Tropp. Practical large-
scale optimization for max-norm regularization. In ADVANCES IN NEURAL INFORMATION PROCESS-
ING SYSTEMS. MIT Press, 2010.

5


A Discriminative Latent Model of Image Region and
Object Tag Correspondence

Yang Wang∗
Department of Computer Science
University of Illinois at Urbana-Champaign
yangwang@uiuc.edu

Greg Mori
School of Computing Science
Simon Fraser University
mori@cs.sfu.ca

Abstract

We propose a discriminative latent model for annotating images with unaligned
object-level textual annotations. Instead of using the bag-of-words image repre-
sentation currently popular in the computer vision communi ty, our model explic-
itly captures more intricate relationships underlying vis ual and textual informa-
tion. In particular, we model the mapping that translates image regions to anno-
tations. This mapping allows us to relate image regions to their corresponding
annotation terms. We also model the overall scene label as latent information.
This allows us to cluster test images. Our training data cons ist of images and their
associated annotations. But we do not have access to the ground-truth region-
to-annotation mapping or the overall scene label. We develop a novel variant of
the latent SVM framework to model them as latent variables. Our experimental
results demonstrate the effectiveness of the proposed mode l compared with other
baseline methods.

1 Introduction

Image understanding is a central problem in computer vision that has been extensively studied in
the forms of various types of tasks. Some previous work focuses on classifying an image with
a single label [6]. Others go beyond single labels and assign a list of annotations to an image
[1, 10, 21]. Recently, efforts have been made to combine various tasks (i.e. classiﬁcation, annotation,
segmentation, etc) together to achieve a more complete understanding of an image [11, 12]. In this
paper, we consider the problem of image understanding with unaligned textual annotations.
In
particular, we focus on the scenario where the annotations r epresent the names of the objects present
in an image. The input to our learning algorithm is a set of images with unaligned textual annotations
(object names). Our goal is to learn a model to predict the annotation (i.e. object names) for a new
image. As a by-product, our model also roughly localizes the image regions corresponding to the
annotation, see Fig. 1. The main contribution of this paper is the development of a model that
incorporates this object annotation to image region corres pondence in a discriminative framework.

In the computer vision literature, there has been a lot of work on exploiting images and their associ-
ated textual information. Barnard et al. [1] predict words a ssociated with whole images or regions
by learning a joint distribution of image regions and words. Berg et al. [3] learn to name faces ap-
pearing in news pictures by learning a probabilistic model o f face appearances, names, and textual
contexts. Wang et al. [21] use a learned bag-of-words topic model to simultaneously classify and
annotate images. Loeff et al. [13] discover scenes by exploiting the correlation between images
and their annotations. Some recent work towards total scene understanding [11, 12] tries to build
sophisticated generative models that jointly perform seve ral tasks, e.g. scene classiﬁcation, object
recognition, image annotation, and image segmentation.

∗Work done while the author was with Simon Fraser University.

1

mallet
athlete
horse
ground
tree

(a)

(b)

(c)

Figure 1: Our goal is to learn a model using images and their associated unaligned textual object annotations
(a) as the training data. Given a new image (b), we can use the model to predict its textual annotations and
roughly localize image regions corresponding to each of the annotation terms (c).

Most of the previous work uses fairly crude “bag-of-words” m odels, treating image features (ex-
tracted from either segmented regions or local interest poi nts) and textual annotations as unordered
entities and looking at their co-occurrence statistics. Ve ry little work explicitly models more detailed
relationships between image regions and annotations that are obvious to humans. For example, if
an image is over-segmented into a large number of segments, each segment typically only corre-
sponds to at most one object. However, most of the previous work ignores this constraint and allows
an image region being used as evidence to explain different objects mentioned by the annotations.
In this paper, we present a discriminative latent model that captures image regions, textual annota-
tions, mappings between visual and textual information, and overall scene labels in a more explicit
manner. Some work [1, 3] tries to incorporate the mapping inf ormation into a generative model.
However due to the limitation of the machine learning tools used in those work, they did not prop-
erly enforce the aforementioned constraint on how image reg ions are mapped to annotations. There
is also work [2] on augmenting training data with this mapping information, but it is unclear how it
can be generalized on test data. With the recent advancement in learning with complex structured
data [7, 18, 21, 25], we believe now it is the time for us to revisit this line of ideas and examine other
modeling tools.

The work by Socher et al. [17] is the most relevant to ours. In that work, they learn to annotate
and segment images by mapping image regions and textual words to a latent meaning space using
context and adjective features. There are important distin ctions between our work and [17]. First of
all, the input to [17] is a set of images (a handful of which are manually labeled) of a single sport
category, and a collection of news articles for that sport. The news ar ticles are generic for that sport,
and the images are not the news photographs directly associa ted with those news articles. Although
they have experimented on applying their model on image collections with mixed sport categories,
their method seems to work better with single sport category training. In contrast, the input to our
learning problem is a set of images from several sport catego ries, together with their associated
textual annotations. We treat the sport category as a latent variable (we call it the scene label) and
implicitly infer it during learning.

2 Model

We propose a discriminative latent model that jointly captures the relationships between image seg-
ments, textual annotations, region-text correspondence, and overall image visual scene labels. Of
course, only the image segments and textual annotations are observed on training data. All the other
information (e.g. scene labels, the mapping between regions and annotations) are treated as latent
variables in the model. A graphical illustration of our mode l is shown in Fig. 2.
The input to our learning module is a set of hx, yi pairs where x denotes an image, and y denotes the
annotation associated with this image. We partition the image into R regions using the segmentation
algorithm in [8], i.e. x = [x1 , x2 , ..., xR ]. For each image region xi , we extract four types of visual
features (see [14]): shape, texture, color, and location. Each of these feature types is vector quantized
to obtain codewords for this feature type. Following [17], we use 20, 25, 40, 8 codewords for each
of the four feature types, respectively. In the end, each reg ion xi is represented as a 4-dimensional
vector xi = (xi1 , xi2 , xi3 , xi4 ), where each xic is the corresponding codeword of the c-th feature
type for this region.
The annotation y of an image is represented as a binary vector y = (y1 , y2 , ..., yV ), where V is the
total number of possible annotation terms. As a terminological convention, we use “annotation ” to
denote the vector y and “annotation term ” to denote each component
yj of the vector. An annotation

2

image regions

......

h
o
r
s
e

1

a
t
h
l
e
t
e
1

c
a
r

0

d
o
g

0

annotation

Figure 2: Graphical illustration of our model. An input image is segmented into several regions. The annotation
of the image is represented as a 0-1 vector indicating the pre sence/absence of each possible annotation term.
Our model captures the unobserved mapping that translate image regions to annotation terms associated with
the image (e.g. horse, athlete). For annotation terms not associated with the image (e.g. car, dog), there are no
mapped image regions. Our model also captures relationship between the unobserved scene label (e.g. polo)
and image regions/annotations.
term yj is “active ” ( yj = 1) if it is associated with this image, and is “inactive ” ( yj = 0) otherwise.
We further assume the number of regions of an image is larger t han or equal to the number of active
annotation terms for an image, i.e. R ≥ PV
j=1 yj . In this work, we assume there are no visually
annotation terms (e.g.
“people ” and
irrelevant annotation terms (e.g.
“wind ”), and there are no
“athlete ”) of an image that refer to the same concept. These c
an be achieved by pre-processing the
annotation terms with Wordnet (see [17]).

Given an image x and its annotation y, we assume there is an underlying unobserved many-to-one
mapping which translates R image regions to each of the active annotation terms. We restrict the
mapping to have the following conditions: (i) each image region is mapped to at most one anno-
tation term. This condition will ensure that an image region is not used to explain two different
annotations; (ii) an active annotation term has one or more image regions mapped to it. This con-
dition will make sure that if an annotation term (say “buildi ng ”) is assigned to an image, there is
at least one image region supporting this annotation term; ( iii) an inactive annotation term has no
image regions mapped to it. This condition will guarantee there are no image regions supporting an
inactive annotation term.
More formally, we introduce a matrix z = {zij : 1 ≤ i ≤ R, 1 ≤ j ≤ V } de ﬁned in the following
to represent this mapping for an image with R regions:
if the i-th image region is mapped to the j -th annotation term
zij = (cid:26) 1
otherwise
0
We use Y to denote the domain of all possible assignments of y. For a ﬁxed annotation y, we use
Z (y) to denote the set of all possible many-to-one mappings that satisfy the conditions (i,ii,iii). It is
easy to verify that any z ∈ Z (y) can be represented using the following three sets of constra ints:
Xj
For a given image, we also assume a discrete unobserved “scen e ” label s which takes its value
between 1 and S . We introduce the scene label to capture the fact that the annotations of images are
typically well clustered according to their underlying scenes. For example, an image of a “sailing ”
scene tends to have annotation terms like “athlete ”, “sailb
oat”, “water”, etc. However, it is not quite
simple to de ﬁne the vocabulary to label scenes [13]. In our wo rk, we treat the scene label as a latent
variable (hence we do not need its ground-truth label or even a vocabulary for de ﬁning it) and let
tutes a scene. As we will demonstrate
the learning algorithm automatically ﬁgure out what consti
in the experiment, the “scenes” learned by our model on a coll
ection of sport images do match our
intuitions, e.g. they roughly correspond to different spor t categories in the data.

zij ∈ {0, 1}, ∀i, ∀j

zij = yj , ∀j ;

zij ≤ 1, ∀i;

max
i

(1)

(2)

Inspired by the latent SVM [7, 25], we measure the compatibility between an image x and an
annotation y using the following scoring function:

fθ (x, y) = max
max
s∈S
z∈Z (y)
where θ are the model parameters and Φ(x, y, z, s) is a feature vector de ﬁned on x, y, z and s. The
model parameters have three parts θ = {α, β , γ }, and θ⊤ · Φ(x, y, z, s) is de ﬁned as:
θ⊤ · Φ(x, y, z, s) = α⊤φ(x, z) + β⊤ψ(x, s) + γ⊤ϕ(y, s)

θ⊤ · Φ(x, y, z, s)

(3)

(4)

3

The details of each of the terms in (4) are described in the fol lowing.
Region-Annotation Matching Potential α⊤φ(x, z): This potential function measures the com-
patibility of mapping image regions to their corresponding annotation terms. Recall an image re-
gion xi consists of codewords from four different feature types xi = (xi1 , xi2 , xi3 , xi4 ). Let Nc
(c = 1, 2, 3, 4) denotes the number of codewords of feature type c. The parameters α consist of four
c=1 corresponding to each of the four feature types. Each αc is a matrix of
components α = {αc }4
w,j can be interpreted as the compatibility between the codeword w
size Nc × V , where an entry αc
(1 ≤ w ≤ Nc ) of feature type c and the annotation term j (1 ≤ j ≤ V ). The potential function is
written as:

α⊤ φ(x, z) =

αc
xic ,j · zij =

Nc
R
4
V
V
R
4
Xi=1
Xc=1
Xj=1
Xw=1
Xj=1
Xi=1
Xc=1
where 1(·) is the indicator function. Note that the de ﬁnition of this po tential function does not
involve y since y is implicitly determined by z, i.e. yj = maxi zij .
Image-Scene Potential β⊤ψ(x, s): This potential function measures the compatibility between an
image x and a scene label s. Similarly, the parameters β consist of four parts β = {β c }4
c=1 corre-
sponding to the four feature types, where an entry β c
w,s is the compatibility between the codeword
w of type c and the scene label s. This potential function is written as:

αc
w,j · 1(xic = w) · zij

(5)

β c
xic ,s =

β⊤ψ(x, s) =

Nc
R
4
R
4
S
Xi=1
Xc=1
Xt=1
Xw=1
Xi=1
Xc=1
Annotation-Scene Potential γ⊤ϕ(y, s): This potential function measures the compatibility be-
tween an annotation y and a scene label s. The parameters γ consist of S components γ = {γ t}S
t=1
j,1 is the
corresponding to each of the scene label. Each component γ t is a V × 2 matrix, where γ t
j,0 is the compatibility of setting yj = 0
compatibility of setting yj = 1 for the scene label t, and γ t
for the scene label t. This potential function is written as:

β c
w,t · 1(xic = w) · 1(s = t)

(6)

γ s
j,yj =

V
Xj=1

γ⊤ϕ(y, s) =

S
Xt=1 (cid:16)γ t
j,1 · 1(yj = 1) · 1(s = t)(cid:17)(7a)
j,0 · 1(yj = 0) · 1(s = t) + γ t

V
Xj=1
V
S
Xt=1 (cid:16)γ t
j,1 · yj · 1(s = t)(cid:17)
Xj=1
j,0 · (1 − yj ) · 1(s = t) + γ t
The equivalence of (7a) and (7b) is due to 1(yj = 0) ≡ 1 − yj and 1(yj = 1) ≡ yj for yj ∈ {0, 1},
which are easy to verify.

=

(7b)

3 Inference

Given the model parameters θ = {α, β , γ }, the inference problem is to ﬁnd the best annotation y
∗
for a new image x, i.e. y
∗ = arg maxy fθ (x, y). The inference requires solving the following
optimization problem:

max
y∈Y

fθ (x, y) = max
s∈S

max
y∈Y

max
z∈Z (y)

θ⊤Φ(x, y, z, s)

(8)

Since we can enumerate all the possible values of the scene label s, the main difﬁculty of solving
(8) is the inner maximization over y and z for a ﬁxed s, i.e.:

max
y∈Y

max
z∈Z (y)

θ⊤Φ(x, y, z, s)

(9)

In the following, we develop a method for solving (9) based on linear program (LP) relaxation. To
formulate the problem as an LP, we ﬁrst de ﬁne the following:

aij =

4
Xc=1

Nc
Xw=1

αc
w,j

1(xic = w), ∀i, ∀j

j,1 − rs
bj = rs
j,0 , ∀j

(10)

4

(11)

bj yj

zij ≤ 1, max
i

zij = yj , zij ∈ {0, 1}, ∀i ∀j

Then it is easy to verify that the optimization problem in (9) can be equivalently written as (the
constant in the objective not involving y or z is omitted):
s.t. Xj
aij zij + Xj
y,z Xi,j
max
The optimization problem (11) is not convex. But we can relax its constraints to make it an LP. First
we reformulate (11) as an integer linear program (ILP):
zij ≤ 1, zij ≤ yj ≤ Xi
bj yj s.t. Xj
aij zij + Xj
y,z Xi,j
max
It is easy to verify that (11) and (12) are equivalent. Of course, (12) still has the integral constraint
zij ∈ {0, 1}, which makes the optimization problem NP-hard. So we further relax the value of zij
to a real value in the range of [0, 1].
Putting everything together, the LP relaxation of (11) can be written as:
zij ≤ 1, zij ≤ yj ≤ Xi
bj yj s.t. Xj
aij zij + Xj
y,z Xi,j
max
After solving (13) with any LP solver, we round zij to the closest integer and obtain yj as yj =
maxi zij .

zij , 0 ≤ zij ≤ 1, 0 ≤ yj ≤ 1, ∀i ∀j (13)

zij , zij ∈ {0, 1}, yj ∈ {0, 1}, ∀i ∀j (12)

4 Learning

We now describe how to learn the model parameters θ from a set of N training examples hx
n i
n , y
(n = 1, 2, ..., N ). Note that the training data only contain images and their annotations. We do not
have the ground-truth scene label s or the mapping z for any of the training images, so we have to
treat them as latent variables during learning.

We adopt the latent SVM (LSVM) framework [7, 25] for learning . LSVMs extend the popular
structural SVMs [18, 19] to handle latent variables during t raining. LSVMs and their variants have
been successfully applied in several computer vision applications, e.g. object detection [7, 20],
human action recognition [22, 16], human-object interaction [4], objects and attributes [23], human
poses and actions [24], group activity recognition [9], etc.
The latent SVM learns the model parameters θ by solving the following optimization problem:

ξn

1
2

min
θ

||θ||2 + C

n , y
s.t. fθ (x

n ) − fθ (x
n , y) ≥ ∆(y, y

N
Xn=1
n ) is a loss function measuring the cost incurred by predicting y when the ground-
where ∆(y, y
truth annotation is y
n . We use a simple Hamming loss which decomposes as ∆(y, y
n ) =
PV
j ) is 1 if yj 6= yn
j ), where ℓ(yj , yn
j and 0 otherwise. Note that our loss function
j=1 ℓ(yj , yn
only involves the annotation y, because this is the only ground-truth label we have access to.
The problem in (14) can be equivalently written as an unconstrained problem:

n ) − ξn , ∀n, ∀y

(14)

1
2

min
θ

||θ||2 + C

N
n , y)(cid:17), Rn = fθ (x
y (cid:16)∆(y, y
Xn=1
n ) + fθ (x
(Ln − Rn ), where Ln = max
n , y
We use the non-convex bundle optimization in [5] to solve (15). In a nutshell, the algorithm itera-
tively builds an increasingly accurate piecewise quadratic approximation to the objective function.
During each iteration, a new linear cutting plane is found via a subgradient of the objective function
and added to the piecewise quadratic approximation. The key of applying this algorithm to solve
(15) is computing the two subgradients ∂θ Ln and ∂θRn for a particular θ, which we describe in
detail below.
First we describe how to compute ∂θ L. Let (y
∗ , s∗ ) be the solution to the following optimization
∗ , z
problem (called loss-augmented inference in the structura l SVM literature):
n ) + fθ (x
n , y)

∆(y, y

(16)

n ) (15)

max
s

max
y

max
z∈Z (y)

5

Then it is easy to show that a subgradient ∂θ Ln can be calculated as ∂θLn = Φ(x
∗ , s∗ ).
n , y
∗ , z
The loss-augmented inference problem in (16) is similar to the inference problem in (8), except for
n ). We can modify the LP relaxation method in Sec. 3 to solve (16) for a
an additional term ∆(y, y
ﬁxed s (and enumerate s to get the ﬁnal solution). First of all, it is easy to verify th at ℓ(yj , yn
j ) can
be re-formulated as:

(17)

(18)

if yn
j ) ≡ (cid:26) 1 − yj
j = 1
ℓ(yj , yn
if yn
j = 0
yj
Using (17), it is easy to show that if we re-de ﬁne bj as below, the ILP in (12) will solve the loss-
augmented inference (16) for a ﬁxed s:
j,0 − 1 if yn
j,1 − γ s
bj = (cid:26) γ s
j = 1
j,0 + 1 if yn
γ s
j,1 − γ s
j = 0
Similarly, we can relax the problem to an LP using the same method in Sec. 3.
Now we describe how to compute ∂θR. Let (z
⋆ , s⋆ ) be the solution to the following optimization
problem: maxs maxz∈Z (yn ) fθ (x
n ). Then it can be shown that a subgradient ∂θRn can be
n , y
calculated as ∂θRn = Φ(x
⋆ , s⋆ ). For a ﬁxed s, it is easy to show that the maximization over
n , y
n , z
z can be solved by the following ILP:
aij zij , s.t. Xj
z Xi,j
max
Similarly, we can solve (19) via LP relaxation by replacing the integral constraint zij ∈ {0, 1} with
a linear constraint 0 ≤ zij ≤ 1.

zij = yn
j , ∀i; zij ∈ {0, 1}, ∀i ∀j

(19)

5 Experiments

We test our model on the UIUC sport dataset [11]. It contains images collected from eight sport
classes: badminton, bocce, croquet, polo, rock climbing, rowing, sailing and snowboarding. Each
image is annotated with a set of tags denoting the objects in it. We remove annotation terms occur-
ring fewer than three times. We randomly choose half of the da ta as the test set. From the other half,
we randomly select 50 images from each class to form the validation set. The remaining data are
used as the training set.

We feed the training images and associated annotations (but not the ground-truth sport category
labels) to our learning algorithm and set the number of laten t scene labels to be eight (i.e.
the
number of sport classes). We initialize the parameters of ou r model as follows. First we cluster the
training images into eight cluster using the following method. For each training image, we construct
a feature vector from the visual information of the image itself and the textual information of its
annotation. The visual information is simply the concatenation of visual word counts from all the
regions in the image (normalized between 0 and 1), i.e. the dimensionality of the visual feature is
PC
c=1 Nc . The textual information is the 0-1 vector of the annotation , i.e. the dimensionality is V .
We then run k-means clustering based on the combined visual and textual features to cluster training
images into eight clusters. We use the cluster membership of each training image as the initial
guess of the scene label s (which we call pseudo-scene label). We then initialize the parameters
β by examining the co-occurrence counts of visual words and pseudo-scene labels on the training
data. Similarly, we initialize the parameters γ by the co-occurrence counts of annotation terms and
pseudo-scene labels. The parameters α are initialized by the co-occurrence counts of visual words
and annotation terms with the mapping constraints ignored.

We compare our model with a baseline method which is a set of linear SVMs separately trained
for predicting the 0/1 output of each annotation term based on the feature vector from the visual
information. Following [21], we use the F-measure to measure the annotation performance. The
comparison is shown in Table 1(a). Our model outperforms the baseline SVM method. We also list
the published result of [22] in the table. However, it is important to remember that it is not directly
comparable to other numbers in Table 1(a), since [22] uses different image features and different
subsets of the dataset unspeciﬁed in the paper. We visualize some results on the test data in Fig. 5.

The scene labels s produced by our model for the test images can be considered as a clustering of
the scenes in those images. We can measure the quality of the scene clustering by comparing with

6

Figure 3: Visualization of γ parameters. Each plot corresponds to a scene label s, we show the weights of top
ﬁve components of γ s
j,1 of all j ∈ {1..V } (y-axis) and the corresponding annotation terms (x-axis).

athlete

ceiling

ﬂoor

grass

rowboat sailboat

sky

sun

tree

wat er

Figure 4: Visualization of the “position” components of the α parameters for some annotation terms. Bright
areas correspond to high values.

the ground-truth scene labels (i.e. sport categories) of th e test images. For comparison, we consider
three baseline algorithms. The ﬁrst baseline algorithm is t o run k-means clustering on the test data
based on the visual features. However the comparison to this baseline algorithm is not completely
fair, since the baseline does not exploit any information from the annotations on the training data.
So we de ﬁne other two baseline algorithms that use this extra information.

For the second baseline algorithm (which we call pseudo-label+SVM ), we run k-means clustering
on both training and validation data. We use both visual features and textual features for the clus-
tering. After running k-means clustering, we assign a pseudo-label to each image in the training
or validation set by its cluster membership. Then we train a multi-class SVM based on the visual
features of the training images and their pseudo-labels. The parameters of the SVM classiﬁer are
chosen by validating on the validation images (visual features only) with their pseudo-labels. For a
test image, we use the trained SVM classiﬁer to assign a pseud o-label based on the visual feature of
this image. The predicted pseudo-labels of test images serve as a clustering of those images.

For the third baseline algorithm (which we call pseudo-annotation+K-means), we ﬁrst train separate
SVM classiﬁers to predict the annotation from the visual fea ture, using the ground-truth annotations
of the validation set to choose the free parameters in SVM classiﬁers. For a set of test images,
we use the trained SVM classiﬁers to predict their associate d annotations (which we call pseudo-
annotations). Then we run k-means to cluster those test images based on bo th visual features and
textual features. The textual features are obtained from th e pseudo-annotations.

We use the normalized mutual information (NMI) [15] to quantitatively measure the clustering re-
sults. Let Ω = {ω1 , ω2 , ..., ωK } be a set of clusters, and D = {d1 , d2 , ..., dK } be the set of ground-
I (Ω;D)
[H (Ω)+H (D)]/2 , where I (·) and H (·) are the
truth categories. The NMI is de ﬁned as NMI(Ω, D) =
mutual information and the entropy, respectively. The minimum of NMI is 0 if the cluster is random
with respect to the ground-truth. Higher NMIs means better c lustering results. The comparison is
shown in Table 1(b). Our model outperforms other baseline me thods on the scene clustering task.

We can visualize some of the parameters to get insights about the learned model. For a particular
j,1 measures the compatibility of setting the j -th annotation term
scene label s, the parameter γ s
active for the scene label s. We sort the annotation terms according to γ s
j,1 . In Fig 3, we visualize
the top ﬁve annotation terms for each of the eight possible va lues of s. Intuitively, these eight scene
clusters obtained from our model seem to match well to the eight different sport categories of this
dataset. We also visualize the “position ” (i.e.
c = 4) components of the α parameters (Fig. 4) for
several annotation terms as follows. For a particular annot ation term j , we ﬁnd the most preferred
“position ” visual word w∗ for this annotation term by w∗ = arg maxw α4
w,j . The cluster center of
the visual word w∗ de ﬁnes an 8 × 8 position mask of image locations (see [14]), which is visualized
in Fig. 4. We can see that the learned α parameters make intuitive sense, e.g. “water” is preferred
at
the bottom of the image, while “sky ” is preferred at the top of
the image.

7

method
our approach
SVM
[21]

F-measure
0.4552
0.4112
0.3500

(a)

method
our approach
pseudo-label + SVM
pseudo-annotation + K-means
K-means
(b)

NMI
0.5295
0.4134
0.3267
0.2227

Table 1: Comparison of image annotation (a) and scene clustering (b). The number of clusters is set to be eight
for all methods. See the text for more descriptions.

Figure 5: (Best viewed in color) Results of annotation and segmentation on the UIUC sport dataset. Different
annotation terms are shown in different colors. Image regions mapped to an annotation term are overlayed with
the color corresponding to that annotation term.
6 Conclusion
We have presented a discriminatively trained latent model for capturing the relationships among
image regions, textual annotations, and overall scenes. Our ultimate goal is to achieve total scene
understanding from cheaply available Internet data. Although most previous work in scene under-
standing focuses on generative probabilistic models (e.g. [1, 3, 11, 12, 21]), this paper offers an
alternative path towards this goal via a discriminative framework. We believe discriminative meth-
ods offer a complementary advantage over generative ones. Certain relationships (e.g. the mapping
between images regions and annotation terms) are hard to mod el, hence largely ignored in the gen-
erative approaches. But those relationships are easy to incorporate in a max-margin discriminative
approach like ours.
In this work we have provided evidence that modeling these re lationships can improve image an-
notation. Our work provides a general solution that can be broadly applied in other applications
involving mapping relationships, e.g. Youtube videos with annotations, movie clips with captions,
face detection with person names, etc. There are many open issues to address in future research:
(1) extending our model to handle a richer set of annotation terms (nouns, verbs, adjectives, etc) by
modifying the many-to-one correspondence assumption. (2) exploring the use of this model with
noisier annotation data (e.g. raw Flickr or YouTube tags); (3) exploiting the linguistic structure of
tags.

8

References

International

[1] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. M. Blei, and M. I. Jordan. Matching words and
pictures. Journal of Machine Learning Research, 3:1107–1135, 2003.
[2] K. Barnard and Q. Fan. Reducing correspondence ambibuity in loosely labeled training data. In IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, 2007.
[3] T. L. Berg, A. C. Berg, J. Edwards, and D. Forsyth. Who’s in the picture.
In Advances in Neural
Information Processing Systems, volume 17, pages 137–144. MIT Press, 2004.
[4] C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for static human-object interactions. In
Workshop on Structured Models in Computer Vision, 2010.
[5] T.-M.-T. Do and T. Artieres. Large margin training for hi dden markov models with partially observed
states. In International Conference on Machine Learning, 2009.
[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL visual object
classes (VOC) challenge. International Journal of Computer Vision, 88(2):303–338, 2010.
[7] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discrimi-
natively trained part based models.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
2009.
[8] P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient grap h-based image segmentation.
Journal of Computer Vision, 2004.
[9] T. Lan, Y. Wang, W. Yang, and G. Mori. Beyond actions: Discriminative models for contextual group
activities. In Advances in Neural Information Processing Systems. MIT Press, 2010.
[10] J. Li and J. Z. Wang. Automatic linguistic indexing of pictures by a statistical modeling approach. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 25(9):1075–1088, September 2003.
[11] L.-J. Li and L. Fei-Fei. What, where and who? classifying events by scene and object recognition. In
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2007.
[12] L.-J. Li, R. Socher, and L. Fei-Fei. Towards total scene understanding: Classi ﬁcation, annotation and
segmentation in an automatic framework.
In IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, 2009.
[13] N. Loeff and A. Farhadi. Scene discovery by matrix factorization. In European Conference on Computer
Vision, 2008.
[14] T. Malisiewicz and A. A. Efros. Recognition by association via learning per-exemplar distances. In IEEE
Computer Society Conference on Computer Vision and Pattern Recongition, 2008.
[15] C. D. Manning. Introduction to Information Retrieval. Cambridge University Press, 2008.
[16] J. C. Niebles, C.-W. Chen, and L. Fei-Fei. Modeling temporal structure of decomposable motion segments
for activity classi ﬁcation. In European Conference on Computer Vision, 2010.
[17] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised segmentation and annotation of images
using unaligned text corpora. In IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, 2010.
[18] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Advances in Neural Information
Processing Systems, volume 16. MIT Press, 2004.
[19] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and
interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, 2005.
[20] A. Vedaldi and A. Zisserman. Structured output regress ion for detection with partial truncation.
Advances in Neural Information Processing Systems. MIT Press, 2009.
[21] C. Wang, D. Blei, and L. Fei-Fei. Simultaneous image classi ﬁcation and annotation. In IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, 2009.
[22] Y. Wang and G. Mori. Max-margin hidden conditional random ﬁelds for human action recognition. In
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2009.
[23] Y. Wang and G. Mori. A discriminative latent model of object classes and attributes.
Conference on Computer Vision, 2010.
[24] W. Yang, Y. Wang, and G. Mori. Recognizing human actions from still images with latent poses. In IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, 2010.
[25] C.-N. Yu and T. Joachims. Learning structural SVMs with latent variables. In International Conference
on Machine Learning, 2009.

In

In European

9


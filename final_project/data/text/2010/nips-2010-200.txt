Random Walk Approach to Regret Minimization

Hariharan Narayanan
MIT
Cambridge, MA 02139
har@mit.edu

Alexander Rakhlin
University of Pennsylvania
Philadelphia, PA 19104
rakhlin@wharton.upenn.edu

Abstract
We propose a computationally efﬁcient random walk on a convex body which
rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex
optimization and repeated games, the algorithm yields low regret and presents a
novel efﬁcient method for implementing mixture forecasting strategies.

1

Introduction

This paper brings together two topics: online convex optimization and sampling from logconcave
distributions over convex bodies.
Online convex optimization has been a recent focus of research [30, 25], for it presents an abstrac-
tion that uniﬁes and generalizes a number of existing results in online learning. Techniques from
the theory of optimization (in particular, Fenchel and minimax duality) have proven to be key for
understanding the rates of growth of regret [25, 1]. Deterministic regularization methods [3, 25]
have emerged as natural black-box algorithms for regret minimization, and the choice of the regu-
larization function turned out to play a pivotal role in limited-feedback problems [3]. In particular,
the authors of [3] demonstrated the role of self-concordant regularization functions and the Dikin
ellipsoid for minimizing regret. The latter gives a handle on the local geometry of the convex set,
crucial for linear optimization with limited feedback.
Random walks in a convex body gained much attention following the breakthrough paper of Dyer,
Frieze and Kannan [9], who exhibited a polynomial time randomized algorithm for estimating the
volume of a convex body. It is known that the problem of computing this volume by a deterministic
algorithm is #P-hard. Over the two decades following [9], the polynomial dependence of volume
computation on the dimension n has been drastically decreased from O∗ (n23 ) to O∗ (n4 ) [17]. The
development was accomplished through the study of several geometric random walks: the Ball Walk
and Hit-and-Run (see [26] for a survey). The driving force behind such results are the isoperimetric
inequalities which can be extended from uniform to general logconcave distributions. In particular,
computing the volume of a convex body can be seen as a special case of integration of a logconcave
function, and there has been a number of major results on mixing time for sampling from logconcave
distributions [17, 18]. Connections to optimization have been established in [12, 18], among others.
More recently, a novel random walk, called the Dikin Walk has been proposed in [19, 13]. By
exploiting the local geometry of the set, this random walk is shown to mix rapidly, and offers a
number of advantages over the other random walks.
While the aim of online convex optimization is different from that of sampling from logconcave
distributions, the fact that the two communities recognized the importance of the Dikin ellipsoid is
remarkable. In this paper we build a bridge between the two topics. We show that the problem of
online convex optimization can be solved by sampling from logconcave distributions, and that the
Dikin Walk can be adapted to mix rapidly to a certain time-varying distribution. In fact, it mixes fast
enough that for linear cost functions only one step of the guided Dikin Walk is necessary per round
of the repeated game. This is surprisingly similar to the sufﬁciency of one Damped Newton step of
Algorithm 2 in [3], due to locally quadratic convergence ensured by the self-concordant regularizer.

1

The time-varying Gibbs distributions from which we sample are closely related to Mixture Forecast-
ers and Bayesian Model Averaging methods (see [7, Section 11.10] as well as [29, 28, 4, 10]). To
the best of our knowledge, the method presented in this paper is the ﬁrst provably computationally-
efﬁcient approach to solving a class of problems which involves integrating over continuous sets of
decisions. From the Bayesian point of view, our algorithm is an efﬁcient procedure for sampling
from posterior distributions, and can be used for settings outside of regret minimization.

Prior work: The closest to our work is the result of [11] for Universal Portfolios. Unlike our one-
step Markov chain, the algorithm of [11] works with a discretization of the probability simplex and
requires a number of steps which has adverse dependence on the time horizon and accuracy. This
seems unavoidable with the Grid Walk. In [2], it was shown that the Weighted Average Forecaster
[15, 27] on a prohibitively large class of experts is optimal in terms of regret for a certain multitask
problem, yet computationally inefﬁcient. A Markov chain has been proposed with the required
stationary distribution, but no mixing time bounds have been derived. In [8], the authors faced a
similar problem whereby a near-optimal regret can be achieved by the Weighted Average Forecaster
on a prohibitively large discretization of the set of decisions. Sampling from time-varying Markov
chains has been investigated in the context of network dynamics [24], and has been examined from
the point of view of linear stochastic approximation in reinforcement learning [14]. Beyond [11],
we are not aware of any results to date where a provably rapidly mixing walk is used to solve regret
minimization problems.
It is worth emphasizing that without the Dikin Walk [19], the one-step mixing results of this paper
seem out of reach. In particular, when sampling from exponential distributions, the known bounds
for the conductance of the Ball Walk and Hit-and-Run are not scale-independent. In order to obtain
O(√T ) regret, one has to be able to sample the target distribution with an error that is O(1/√T ).
As a consequence of the deterioration of the bounds on the conductance as the scale tends to zero,
the number of steps necessary per round would tend to inﬁnity as T tends to inﬁnity.

2 Main Results

Let K⊂ Rn be a convex compact set and let F be a set of convex functions from K to R. Online
convex optimization is deﬁned as a repeated T -round game between the player (the algorithm) and
Nature (adversary) [30, 25]. From the outset we assume that Nature is oblivious (see [7]), i.e. the
individual sequence of decisions !1 , . . . , !T ∈F can be ﬁxed before the game. We are interested in
randomized algorithms, and hence we consider the following online learning model: on round t, the
player chooses a distribution (or, a mixed strategy) µt−1 supported on K and “plays” a random Xt ∼
µt−1 . Nature then reveals the cost function !t ∈F . The goal of the player is to control expected
regret (see Lemma 1) with respect to a randomized strategy deﬁned by a ﬁxed distribution pU ∈P
for some collection of distributions P . If P contains Dirac delta distributions, the comparator term is
indeed the best ﬁxed decision x∗ ∈K chosen in hindsight. A procedure which guarantees sublinear
growth of regret for any distribution pU ∈P will be called Hannan consistent with respect to P . We
now state a natural procedure for updating distributions µt which guarantees Hannan consistency
for a wide range of problems. This procedure is similar to the Mixture Forecaster used in the
prediction context [29, 28, 4, 10]. Denote the cumulative cost functions by Lt (x) = !t
s=1 !s (x),
with L0 (x) ≡ 0, and let η> 0 be a learning rate. Let q0 (x) be some prior probability distribution
supported on K. Deﬁne the following sequence of functions
qt (x) = q0 (x) exp {−ηL t (x)} ,
∀t ∈{ 1, . . . , T }
(1)
for every x ∈K . Deﬁne the probability distribution µt over K at time t to have density
where Zt = "x∈K
= q0 (x)e−ηLt (x)
dµt (x)
qt (x)dx.
dx
Zt
Let D(p||q) stand for the Kullback-Leibler divergence between distributions p and q . The following
lemma1 gives an equality for expected regret with respect to a ﬁxed randomized strategy. It bears
1Due to its simplicity, the lemma has likely appeared in the literature, yet we could not locate a reference for
this form with equality and in the context of online convex optimization. The closest results appear in [28, 10],
[7, p. 326] in the context of prediction, and in [4] in the context of density estimation with exponential families.

(2)

2

D(µt−1 ||µt ).

striking similarity to upper bounds on regret in terms of Bregman divergences for the Follow the
Regularized Leader and Mirror Descent methods [23, 5], [7, Therem 11.1].
Lemma 1. Let Xt be a random variable distributed according to µt−1 , for all t ∈{ 1, . . . , T }, as
deﬁned in (2). Let U be a random variable with distribution pU . The expected regret is
E # T$t=1
!t (U )% = η−1 (D(pU ||µ0 ) − D(pU ||µT )) + η−1
T$t=1
T$t=1
!t (Xt ) −
Specializing to the case !(x) ∈ [0, 1] over K,
!t (U )% ≤ η−1D(pU ||µ0 ) + T η/8.
E # T$t=1
T$t=1
!t (Xt ) −
Before proceeding, let us make a few remarks. First, if the divergence between the comparator
distribution pU and the prior µ0 is bounded, the result yields O(√T ) rates of regret growth for
bounded losses by choosing η appropriately. To bound the divergence between a continuous initial
µ0 and a point comparator at some x∗ , the analysis can be carried out in two stages: comparison
to a “small-covariance” Gaussian centered at x∗ , followed by an observation that the loss of the
“small-covariance” Gaussian strategy is not very different from the loss of the deterministic strategy
x∗ . This analysis can be found in [7, p. 326] and gives a near-optimal O(√T log T ) regret bound.
We also note that for linear cost functions, the notion of expected regret coincides with regret for
deterministic strategies. Third, we note that if the prior is of the form q0 (x) ∝ exp{−R(x)} for
some convex function R, then qt (x) ∝ exp {− (ηL t (x) + R(x))}, bearing similarity to the objec-
tive function of the Follow the Regularized Leader algorithm [23, 3]. In general, we can encode
prior knowledge in q0 . For instance, if the cost functions are linear and the set K is a convex hull
of N vertices (e.g. probability simplex), then the minimum loss is attained at one of the vertices,
and a uniform prior on the vertices yields the Weighted Average Forecaster with the usual log N
dependence [7]. Finally, we note that in online convex optimization, one of the difﬁculties is the
issue of projections back to the set K. This issue does not arise when dealing with distributions,
but instead translates into the difﬁculty of sampling. We ﬁnd these parallels between sampling and
optimization intriguing.
We defer the easy proof of Lemma 1 to p. 8. Having a bound on regret, a natural question is whether
there exists a computationally efﬁcient algorithm for playing Xt according to the mixed strategy
given in (2). The main result of this paper is that for linear Lipschitz cost functions the guided
random walk (Algorithm 1 below) produces a sequence of points X1 , . . . , XT ∈K with respective
distributions σ0 , . . . , σT −1 such that σi is close to µi for all 0 ≤ i ≤ T − 1. Moreover, Xi is obtained
from Xi−1 with only one random step. The step requires sampling from a Gaussian distribution with
covariance given by the Hessian of the self-concordant barrier and can be implemented efﬁciently
whenever the Hessian can be computed. The computation time exactly matches [3, Algorithm 2]: it
is the same as time spent inverting a Hessian matrix, which is O(n3 ) or less.
Let us now discuss our assumptions. First, the analysis of the random walk is carried out only for
linear cost functions with a bounded Lipschitz constant. An analysis for general convex functions
might be possible, but for the sake of brevity we restrict ourselves to the linear case. Note that
convex cost functions can be linearized and a standard argument shows that regret for linearized
functions can only be larger than that for the convex functions [30]. The second assumption is that
q0 does not depend on T and has a bounded L2 norm with respect to the uniform distribution on K.
This means that q0 can be not only uniform, but, for instance, of the form q0 (x) ∝ exp{−R(x)}.
Theorem 2. Suppose !t : K *→ [0, 1] are linear functions with Lipschitz constant 1 and the prior q0
is of bounded L2 norm with respect to uniform distribution on K. Then the one-step random walk
(Algorithm 1) produces a sequence X1 , . . . , XT with distributions σ0 , . . . , σT −1 such that for all i,
"x∈K |dσi (x) − dµi (x)|≤ C ηn3 ν 2 ,
where µi are deﬁned in (2), ν is the parameter of self-concordance, and C is an absolute con-
stant. Therefore, regret of Algorithm 1 is within O(√T ) from the ideal procedure of Lemma 1. In

3

!t (Xt ) −

particular, by choosing η appropriately, for an absolute constant C $ ,
E # T$t=1
!t (U )% ≤ C $n3/2 ν&T D(pU ||µ0 ).
T$t=1
Proof. The statement follows directly from Lemma 1, Theorem 9, and an observation that for
bounded losses
’’Eµt−1 !t (Xt ) − Eσt−1 !t (Xt )’’ ≤ "x∈K |!t (x)| · |dµt−1 (x) − dσt−1 (x)|≤ C ηn3 ν 2 .
3 Sampling from a time-varying Gibbs distribution

(3)

Sketch of the Analysis The sufﬁciency of only one step of the random walk is made possible
by the fact that the distributions µt−1 and µt are close, and thus µt−1 is a (very) warm start for
µt . The reduction in distance between the distributions after a single step is due to a general fact
(Lov ´asz-Simonovits [16]) which we state in Theorem 6. The majority of the work goes into lower
bounding the conductance of the random walk by a quantity independent of T (Lemma 5). Since
the random walk of Algorithm 1 takes advantage of the local geometry of the set, the conductance
is lower bounded by (a) proving an isoperimetric inequality (Theorem 3) for the Riemannian metric
(which states that the measure of the gap between two well-separated sets is large) and (b) by proving
that for close-by (in the Riemannian metric) points, their transition functions are not too different
(Lemma 4). Section 3 is organized as follows. In Section 3.1, the main building blocks for proving
mixing time are stated, and their proofs appear later in Section 4. In Section 3.2, we use the mixing
result of Section 3.1 to show that Algorithm 1 indeed closely tracks the distributions µt (Theorem 9).

3.1 Bounding Mixing Time

In the remainder of this paper, C will denote a universal constant that may change from line to
line. For any function F on the interior int(K) having continuous derivatives of order k , for vectors
h1 , . . . , hk ∈ Rn and x ∈ int(K), for k ≥ 1, we recursively deﬁne
Dk−1 (x + hk )[h1 , . . . , hk−1 ] − Dk−1 (x)[h1 , . . . , hk−1 ]
Dk F (x)[h1 , . . . , hk ] := lim
,

→0
where D0F (x) := F (x). Let F be a self-concordant barrier of K with a parameter ν (see [20]).
For x, y ∈K , ρ(x, y) is the distance in the Riemannian metric whose metric tensor is the Hessian
of F . Thus, the metric tensor on the tangent space at x assigns to a vector v the length -v-x :=
D2F (x)[v , v ], and to a pair of vectors v , w , the inner product .v , w/x := D2F (x)[v , w]. We have
ρ(x, y) = inf Γ (z -dΓ-z where the inﬁmum is taken over all rectiﬁable paths Γ from x to y . Let M
be the metric space whose point set is K and metric is ρ. We assume !i are linear and 1−Lipschitz
with respect to ρ. For x ∈ int(K), let Gx denote the unique Gaussian probability density function
on Rn such that
Gx (y) ∝ exp )−
+ V (x)* , where V (x) =
1
n-x − y-2
x
2
r2
Further, deﬁne the scaled cumulative cost as st (y) := r2 ηL t (y). Note that shape of Gx is precisely
given by the Dikin ellipsoid, which is deﬁned as a unit ball in -·- x around a point x [20, 3].
The Markov chain Mt considered in this paper is such that for x, y ∈K , one step x → y is given
by Algorithm 1. A simple calculation shows that the detailed balance conditions are satisﬁed with
respect to a stationary distribution µt (deﬁned in Eq. (2)). Therefore the Markov chain is reversible
and has this stationary measure. The next results imply that this Markov chain is rapidly mixing.
The ﬁrst main ingredient is an isoperimetric inequality necessary for lower bounding conductance.
Theorem 3. Let S1 and S2 be measurable subsets of K and µ a probability measure supported on
K that possesses a density whose logarithm is concave. Then,
1
µ((K \ S1 ) \ S2 ) ≥
2(1 + 3ν ) ρ(S1 , S2 )µ(S1 )µ(S2 ).

ln det D2F (x) and r = 1/(C n)

4

Algorithm 1 One Step Random Walk (Xt , st )
Input: current point Xt ∈K and scaled cumulative cost st .
Output: next point Xt+1 ∈K
Toss a fair coin. If Heads, set Xt+1 := Xt .
Else,
Sample Z from GXt . If Z /∈K , let Xt+1 := Xt .
If Z ∈K , let
with prob. min ,1, GZ (Xt ) exp(st (Xt ))
GXt (Z ) exp(st (Z )) -
Xt+1 := +Z
otherwise.
Xt

K

Xt+1

Xt

Figure 1: The new point is sam-
pled from a Gaussian distribution
whose shape is deﬁned by the lo-
cal metric. Dotted lines are the unit
Dikin ellipsoids.

The next Lemma relates the Riemannian metric ρ to the Markov Chain. Intuitively, it says that for
close-by points, their transition distributions cannot be far apart.
Lemma 4. If x, y ∈K and ρ(x, y) ≤ r
C√n , then dT V (Px , Py ) ≤ 1 − 1
C .
Theorem 3 and Lemma 4 together give a lower bound on conductance of the Markov Chain.
Lemma 5 (Bound on Conductance). Let µ be any exponential distribution on K. The conductance
2 (S1
Px (K \ S1 )dµ(x)
Φ := inf
µ(S1 )
µ(S1 )≤ 1
C νn√n .
of the Markov Chain in Algorithm 1 is bounded below by
1
The lower bound on conductance of Lemma 5 can now be used with the following general result on
the reduction of distance between distributions.
Theorem 6 (Lov ´asz-Simonovits [16]). Let γ0 be the initial distribution for a lazy reversible ergodic
Markov chain whose conductance is Φ and stationary measure is γ , and γk be the distribution of
γ0 (S )
the k th step. Let M := supS
γ (S ) where the supremum is over all measurable subsets S of K. For
every bounded f , let -f -2,γ denote .(K
f (x)2dγ (x). For any ﬁxed f , let E f be the map that takes
f (y)dPx (y). Then if (K
x to (K
f (x)dγ (x) = 0,
2 *k
-E k f -2,γ ≤ )1 −
Φ2
-f -2,γ .
In summary, Lemma 5 provides a lower bound on conductance, while Theorem 6 ensures reduction
of the norm whenever conductance is large enough. In the next section, these two are put together.
We will show that reduction in the norm guarantees that the distribution after one step of the random
walk (k = 1 in Theorem 6) is close to the desired distribution µt .

3.2 Tracking the distributions
Let {σi }∞i=1 be the probability measures with bounded density, supported on K, corresponding to
the distribution of a point during different steps of the evolution of the algorithm. For i ∈ N, let
- · -µi denote the L2 norm with respect to the measure µi . We shall write - · -i for brevity. Hence,
f 2dµi 01/2 . Furthermore,
for a measurable function f : K→ R, -f -i = /(K
dµi (x)
q0 (x)e−ηLi (x)dx
Zt+1
sup
= sup
Zt ≤ e2η ≤ 1 + ¯η
dµi+1 (x)
q0 (x)e−ηLi+1 (x)dx
x∈K
x∈K
where we used the fact that !i+1 (x) ≤ 1 and ¯η is an appropriate multiple of η , e.g. ¯η = (e2 − 1)η
does the job. Analogously, dµi+1 /dµi ≤ 1 + ¯η over K. It then follows that the norms at time i and
i + 1 are comparable:
-f -i (1 + ¯η)−1 ≤ -f -i+1 ≤ -f -i (1 + ¯η)
(5)

(4)

5

(6)

(7)

.

Proof.

+ ¯η(1 + ¯η).

The mixing results of Lemma 5 together with Theorem 6 imply
Corollary 7. For any i,
dµi − 11111i )1 − ) 1
dµi − 11111i ≤ 1111
1111
C n3 ν 2 **
dσi+1
dσi
Corollary 7 says that σi+1 is “closer” than σi to µi by a multiplicative constant. We now show that
the distance of σi+1 to µi+1 is (additively) not much worse than its distance to µi . The multiplicative
reduction in distance is shown to be dominating the additive increase, concluding the proof that σi
is close to µi for all i (Theorem 9).
Lemma 8. For any i, it holds that
dµi − 11111i
1111
dµi+1 − 11111i+1 ≤ (1 + ¯η)2 1111
σi+1
dσi+1
dµi − 11111i+1
dµi+1 − 11111i+1 − 1111
dµi+1 − 11111i+1 − 1111
dµi − 11111i
= 1111
1111
dσi+1
dσi+1
dσi+1
dσi+1
dµi − 11111i+1 − 1111
dµi − 11111i
+ 1111
dσi+1
dσi+1
We ﬁrst establish a bound of Cη on (6). For any function f : K→ R, let f + (x) = max(0, f (x))
and f − (x) = min(0, f (x)). By the triangle inequality,
dµi − 11111i+1 ≤ 1111
dµi+1 − 11111i+1 − 1111
dµi 1111i+1
1111
dσi+1
dσi+1
dσi+1
dσi+1
dµi+1 −
.
Now, using (4) and (5),
dµi *−11111
+ 11111
dµi *+11111
= 11111
dµi 1111
1111
) dσi+1
) dσi+1
2
2
2
dσi+1
dσi+1
dσi+1
dµi+1 −
dµi+1 −
i+1
i+1
i+1
+ 1111
dµi+1 31111
dµi+1 31111
≤ 1111
¯η1 21 ≥
¯η1 21 <
2
2
dσi+1
dσi+1
dµi
dµi
dµi
dµi
i+1
i+1
i+1 ≤ ¯η2 (1 + ¯η)2 1111
= ¯η2 1111
dµi 1111
dµi 1111
2
2
dσi+1
dσi+1
.
i
Thus, (6) is bounded as
1111
dµi − 11111i*
= ¯η(1 + ¯η) )1 + 1111
dµi+1 − 11111i+1 − 1111
dµi − 11111i+1 ≤ ¯η(1 + ¯η) 1111
dµi 1111i
dσi+1
σi+1
dσi+1
dσi+1
Next, a bound on (7) follows simply by the norm comparison inequality (5):
1111
dµi − 11111i
dµi − 11111i ≤ ¯η 1111
dµi − 11111i+1 − 1111
dσi+1
dσi+1
dσi+1
.
The statement follows by rearranging the terms.
Theorem 9. If 111 dσ0
dµ0 − 11110
< ¯η(1 + ¯η), where ¯η = (e2 − 1)η , then for all i,
dµi − 11111i ≤ C ηn3 ν 2 .
1111
dσi
Consequently, for all i
"x∈K |dσi (x) − dµi (x)|≤ C ηn3 ν 2 .
6

dσi+1
dµi+1 −

+ ¯η(1 + ¯η).

Proof. By Corollary 7 and Lemma 8, we see that
C n3 ν 2 ** 1111
dµi+1 − 11111i+1 ≤ (1 + ¯η)2 )1 − ) 1
1111
dµi − 11111i
dσi
dσi+1
n3 ν 2 ),
Since ¯η = o( 1
dµi − 11111i
dµi+1 − 11111i+1 ≤ )1 − ) 1
1111
C n3 ν 2 ** 1111
dσi
dσi+1
+ C η .
Let 0 ≤ a < 1 and b > 0, and x0 , x1 , . . . , be any sequence of non-negative numbers such that,
x0 ≤ b and for each i, xi+1 ≤ axi + b. We see, by unfolding the recurrence, that xi+1 ≤ b
1−a .
From this and (8), the ﬁrst statement of the theorem follows. The second statement follows from
dµi51/2
dµi − 1’’’’ dµi ≤ 4" ) dσi
dµi − 11111i
= 1111
" |dσi − dµi | = " ’’’’
dµi − 1*2
dσi
dσi
4 Proof Sketch

(8)

.

In this section, we prove the main building blocks stated in Section 3.1. Consider a time step t. Let
dT V represent total variation distance. Without loss of generality, assume x is the origin and assume
st (x) = 0. For x ∈K and a vector v , |v |x is deﬁned to be
sup
α. The following relation holds:
x±αv∈K
Theorem 10 (Theorem 2.3.2 (iii) [21]). Let F be a self-concordant barrier whose self-concordance
parameter is ν . Then |h|x ≤ -h-x ≤ 2(1 + 3ν )|h|x for all h ∈ Rn and x ∈ int(K).
We term (S1 , (M \ S1 ) \ S2 , S2 ) a δ -partition of M, if δ ≤ dM (S1 , S2 ) :=
dM (x, y),
inf
x∈S1 ,y∈S2
where S1 , S2 are measurable subsets of M. Let Pδ be the set of all δ -partitions of M. If µ is a
measure on M, the isoperimetric constant is deﬁned as
, M, µt* .
and Ct := C ) r
µ((M \ S1 ) \ S2 )
C (δ, M, µ) := inf
√n
µ(S1 )µ(S2 )
Pδ
Given interior points x, y in int(K), suppose p, q are the ends of the chord in K containing x, y
and p, x, y , q lie in that order. Denote by σ(x, y) the cross ratio |x−y ||p−q |
. Let dH denote the
|p−x||q−y |
Hilbert (projective) metric deﬁned by dH (x, y) := ln (1 + σ(x, y)) . For two sets S1 and S2 , let
σ(S1 , S2 ) := inf x∈S1 ,y∈S2 σ(x, y).
Proof of Theorem 3. For any z on the segment xy an easy computation shows that dH (x, z ) +
dH (z , y) = dH (x, y). Therefore it sufﬁces to prove the result inﬁnitesimally. By a result due to
Nesterov and Todd [22, Lemma 3.1],
x ≤ ρ(x, y) ≤ − ln(1 − -x − y-x ).
-x − y-x − -x − y-2
ρ(x,y)
= 1, and a direct computation shows that
whenever -x − y-x < 1. From (9) limy→x
)x−y)x
dH (x, y)
σ(x, y)
= lim
lim
|x − y |x ≥ 1.
|x − y |x
y→x
y→x
Hence, using Theorem 10, the Hilbert metric and the Riemannian metric satisfy
ρ(x, y) ≤ 2(1 + 3ν )dH (x, y).
The statement of the theorem is now an immediate consequence of the following result due to Lov ´asz
and Vempala [18]: If S1 and S2 are measurable subsets of K and µ a probability measure supported
on K that possesses a density whose logarithm is concave, then
µ((K \ S1 ) \ S2 ) ≥ σ(S1 , S2 )µ(S1 )µ(S2 ).

(9)

7

Proof of Lemma 5. Let S1 be a measurable subset of K such that µ(S1 ) ≤ 1
2 and S2 := K \ S1 be
its complement. Let S $1 = S1 ∩{ x’’Px (S2 ) ≤ 1/C } and S $2 = S2 ∩{ y ’’Py (S1 ) ≤ 1/C }. By the
reversibility of the chain, which is easily checked,
Px (S2 )dµ(x) = "S2
"S1
Py (S1 )dµ(y).
If x ∈ S $1 and y ∈ S $2 then,
(w)* dµ(w) = 1 −
min ) dPx
dT V (Px , Py ) := 1 − "K
dPy
(w),
dµ
dµ
C√n , then dT V (Px , Py ) ≤ 1 − 1
Lemma 4 implies that if ρ(x, y) ≤ r
C . Therefore
r
inf
ρ(S $1 , S $2 ) :=
ρ(x, y) ≥
C√n
.
x∈S "1 ,y∈S "2

(10)

1
C

.

Therefore Theorem 3 implies that
ρ(S $1 , S $2 )
r
min(µ(S $1 ), µ(S $2 )) ≥
µ((K \ S $1 ) \ S $2 ) ≥
Cν √n
2(1 + 3ν )
C )µ(S2 ). Then,
First suppose µ(S $1 ) ≥ (1 − 1
C )µ(S1 ) and µ(S $2 ) ≥ (1 − 1
"S1
≥ C min(µ(S $1 ), µ(S $2 ))
Px (S2 )dµ(x) ≥ µ((K \ S $1 ) \ S $2 ) ≥ Cµ(S $1 )
C
C
and we are done. Otherwise, without loss of generality, suppose µ(S $1 ) ≤ (1 − 1
C )µ(S1 ). Then
"S1
µ(S1 )
Px (S2 )dµ(x) ≥
C

min(µ(S $1 ), µ(S $2 )).

and we are done.

ηE

−

!t (Xt ) =

+ ηE!t (Xt ).
(11)

η!t (x)dµt−1 (x) = log Zt
Zt−1

Proof of Lemma 1. We have that
D(µt−1 ||µt ) = "K
+ "K
= log Zt
dµt−1 log qt−1Zt
Zt−1
Zt−1 qt
Rearranging, canceling the telescoping terms, and using the fact that Z0 = 1
T$t=1
T$t=1
D(µt−1 ||µt ) − log ZT .
Let U be a random variable with a probability distribution pU . Then
E!t (U ) = η−1 "K −ηLT (u)dpU (u) = η−1 "K
T$t=1
dpU (u) log qT (u)
q0 (u)
!t (U )% = η−1 "K
T$t=1
T$t=1
dpU (u) log qT (u)/ZT
D(µt−1 ||µt )
!t (Xt ) −
q0 (u)
T$t=1
= η−1 (D(pU ||µ0 ) − D(pU ||µT )) + η−1
Now, from Eq. (11), the KL divergence can be also written as
D(µt−1 ||µt ) = log (K
e−η)t (x) qt−1 (x)dx
+ ηE!t (Xt ) = log Ee−η()t (Xt )−E)t (Xt ))
(K
qt−1 (x)dx
By representing the divergence in this form, one can obtain upper bounds via known methods, such
as Log-Sobolev inequalities (e.g. [6]). In the simplest case of bounded loss, it is easy to show that
D(µt−1 ||µt ) ≤ O(η2 ), and the particular constant 1/8 can be obtained by, for instance, applying
Lemma A.1 in [7]. This proves the second part of the lemma.

Combining,
E # T$t=1

D(µt−1 ||µt ).

+ η−1

8

References
[1] J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through
minimax duality. In COLT ’09, 2009.
[2] J. Abernethy, P. L. Bartlett, and A. Rakhlin. Multitask learning with expert advice. In Proceedings of The
Twentieth Annual Conference on Learning Theory, pages 484–498, 2007.
[3] J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efﬁcient algorithm for bandit linear
optimization. In Proceedings of The Twenty First Annual Conference on Learning Theory, 2008.
[4] K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential
family of distributions. Machine Learning, 43(3):211–246, June 2001.
[5] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex opti-
mization. Oper. Res. Lett., 31(3):167–175, 2003.
[6] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities using the entropy method. 31:1583–
1614, 2003.
[7] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.
[8] V. Dani, T. P. Hayes, and S. Kakade. The price of bandit information for online optimization. In Advances
in Neural Information Processing Systems 20. Cambridge, MA, 2008.
[9] M. Dyer, A. Frieze, and R. Kannan. A random polynomial-time algorithm for approximating the volume
of convex bodies. Journal of the ACM (JACM), 38(1):1–17, 1991.
[10] S. Kakade and A. Ng. Online bounds for Bayesian algorithms. In Proceedings of Neural Information
Processing Systems (NIPS 17), 2005.
[11] A. Kalai and S. Vempala. Efﬁcient algorithms for universal portfolios. The Journal of Machine Learning
Research, 3:440, 2003.
[12] A.T. Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of Operations
Research, 31(2):253–266, 2006.
[13] R. Kannan and H. Narayanan. Random walks on polytopes and an afﬁne interior point method for linear
programming. In STOC, 2009. Accepted (pending revisions), Mathematics of Operations Research.
[14] V. R. Konda and J. N. Tsitsiklis. Linear stochastic approximation driven by slowly varying markov chains.
Systems and Control Letters, 50:95–102, 2003.
[15] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation,
108(2):212–261, 1994.
[16] L. Lov ´asz and M. Simonovits. Random walks in a convex body and an improved volume algorithm.
Random Structures and Algorithms, 4(4):359–412, 1993.
[17] L. Lov ´asz and S. Vempala. Simulated annealing in convex bodies and an o∗ (n4 ) volume algorithm. J.
Comput. Syst. Sci., 72(2):392–417, 2006.
[18] L. Lov ´asz and S. Vempala. The geometry of logconcave functions and sampling algorithms. Random
Struct. Algorithms, 30(3):307–358, 2007.
[19] H. Narayanan. Randomized interior point methods for sampling and optimization. CoRR, abs/0911.3950,
2009.
[20] A.S. Nemirovskii. Interior point polynomial time methods in convex programming, 2004.
[21] Y. E. Nesterov and A. S. Nemirovskii. Interior Point Polynomial Algorithms in Convex Programming.
SIAM, Philadelphia, 1994.
[22] Y.E. Nesterov and MJ Todd. On the Riemannian geometry deﬁned by self-concordant barriers and
interior-point methods. Foundations of Computational Mathematics, 2(4):333–361, 2008.
[23] A. Rakhlin. Lecture notes on online learning, 2008. http://stat.wharton.upenn.edu/˜rakhlin/papers/online learning.pdf.
[24] D. Shah and J. Shin. Dynamics in congestion games. In Proceedings of ACM Sigmetrics, 2010.
[25] S. Shalev-Shwartz and Y. Singer. Convex repeated games and fenchel duality. In NIPS. 2007.
[26] S. Vempala. Geometric random walks: A survey. In Combinatorial and computational geometry. Math.
Sci. Res. Inst. Publ, 52:577–616, 2005.
[27] V. Vovk. Aggregating strategies. In Proceedings of the Third Annual Workshop on Computational Learn-
ing Theory, pages 372–383. Morgan Kaufmann, 1990.
[28] V. Vovk. Competitive on-line statistics. International Statistical Review, 69:213–248, 2001.
[29] K. Yamanishi. Minimax relative loss analysis for sequential prediction algorithms using parametric hy-
potheses. In COLT’ 98, pages 32–43, New York, NY, USA, 1998. ACM.
[30] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.

9


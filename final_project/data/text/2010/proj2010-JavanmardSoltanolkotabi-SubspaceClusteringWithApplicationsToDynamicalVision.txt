Subspace Clustering with Applications to Dynamical Vision
(CS 229 Final Project)

Adel Javanmard

Mahdi Soltanolkotabi

December 10, 2010

1.

INTRODUCTION
Data that arises from engineering applications often con-
tains some type of low dimensional structure that enables
intelligent representation and processing. This leads to a
very challenging problem: discovering compact representa-
tions of high-dimensional data. A very common approach
to address this problem is modeling data as a mixture of
multiple linear (or aﬃne) subspaces. Given a set of data
points drawn from a union of subspaces, subspace clustering
refers to the problem of ﬁnding the number of subspaces,
their dimension and the segmentation of data. In this sense,
subspace clustering can be thought of as a generalization
of the Principal Component Analysis (PCA) method.
In
PCA, the underlying assumption is that the data is drawn
from a single subspace while in subspace segmentation, data
is assumed to be concentrated around multiple low rank sub-
spaces.
A number of methods have been developed to address the
subspace clustering problem, including algebraic methods,
spectral clustering-based methods and statistical methods.
On the heels of compressed sensing and low rank matrix re-
covery[1, 2, 3], a new class of algorithms have very recently
emerged. These algorithms try to represent each data point,
in a union of subspaces, as a linear combination of all other
points. By enforcing a low dimensional structure on this
representation (e.g.
l1-norm for sparsity or nuclear norm
for low-rankness), an aﬃnity matrix is built which is subse-
quently used for subspace segmentation. This new approach
resolves the exponential complexity issues of some of the
previous methods and is more robust to noisy data. Moti-
vated by this approach, we propose a new algorithm, called
Nuclear- ℓ1
for the segmentation problem. Moreover, we
present an iterative scheme as an alternative method for
handling large scale problems.
Subspace segmentation has many applications in com-
puter vision, image processing, and system theory. In this
pro ject we will mainly focus on motion segmentation, and
use the Nuclear-ℓ1 algorithm for this problem. Based on the
simulation results on the Hopkins155 motion database, the
performance of Nuclear-ℓ1 is quite competitive.

2.
SUBSPACE SEPARATION
2.1 Noiseless Scenario
Let X be a matrix in Rn×N with columns drawn from a
union of k subspaces {Si}k
i=1 , of unknown dimensions, em-
bedded in a larger space with dimension n (the data has an
ambient dimension n ). More speciﬁcally X = [X1 , · · · , Xk ]P ,

Figure 1: A set of points concentrated around the union
of subspaces S1 , S2 , and S3 .

where Xi ∈ Rn×Ni
is the set of Ni samples drawn from
the ith subspace Si and P is a permutation marix. Also
N = Pk
i=1 Ni denotes the total number of samples. Assum-
ing that the subspaces are of low dimension and independent
1 , the goal is to segment all data points into their respective
subspaces (see Fig.1. for an illustration).
There are two algorithms that use low dimensional repre-
sentation for subspace clustering. Both of these algorithms
have provable recovery gaurantees when the subspaces are
independent [4, 6]. We will brieﬂy describe them in the next
two subsections. Based on these two algorithms we will pro-
pose a new algorithm (Nuclear-ℓ1 ) which is subsequently
described.

2.1.1
Low Rank Representation (LRR) [4]
In this setting the assumption is that each data vector
can be represented by a linear combination of a small num-
ber of vectors in a dictionary A = [a1 , a2 , · · · , am ],
i.e.,
X = AZ , where Z ∈ RN ×N is the coeﬃcient matrix. A
natural choice for the dictionary is A = X , i.e. one tries
to write each point as a linear combination of all the data
points. This is a reasonable assumption if the sample points
are suﬃciently dense in their respective subspaces. In this
sense each Xi is self-expressive : Xi = XiZi for some Zi ∈
RNi ×Ni . Hence, the permuted block-diagonal matrix Z =
diag (Z1 , Z2 , · · · , Zk )P satisﬁes X = X Z .
1The subspaces are independent if and only if Pk
i=1 Si =
⊕k
i=1Si

The idea of LRR method is to enforce a low-rank struc-
ture on Z as a surrogate for “block-diagaonal-ness” of Z by
solving the problem:

rank(Z )

min
Z
s.t., X = AZ

(2.1)

After ﬁnding Z one can perform subspace segmentation by
spectral clustering algorithms such as normalized cut [7]. It
is well known that (2.1) is NP-hard. A common relaxation
to rank minimization problem is to replace the rank function
with the nuclear norm, resulting in the following problem:

||Z ||∗

min
Z
s.t., X = AZ

(2.2)

It has been shown in [4] that this problem eﬀectively re-
covers the block diagonal structure when the subspaces are
independent. The details of LRR is shown in Algorithm 1.

Algorithm 1: Subspace Segmentation by LRR
Input: data matrix X , number of subspaces k.
1. Solve the optimization problem to get Z ∗ .
2. Construct an undirected graph by using |Z ∗ | + |(Z ∗ )T |
as the aﬃnity matrix of the graph.
3. Use Normalized Cut to segment the vertices of the
graph into k clusters.

2.1.2 Sparse Reconstruction Method (SR) [6]
Sparse Reconstruction Method (SR) enforces sparsity on
the coeﬃcient matrix Z by minimizing its ℓ1 -norm. For-
mally, it solves the problem:

||Z ||ℓ1

min
Z
s.t., X = AZ,
[Zii ] = 0.

(2.3)

Note that without the constraint [Zii ] = 0, the method is
prone to return the trivial solution Z = I . After ﬁnding
the aﬃnity matrix Z, similar to LRR, the algorithm uses
spectral clustering algorithms for subspace clustering. It has
been shown in [6] that this problem also recovers the block
diagonal structure when the subspaces are independent.
2.1.3 Proposed Method (Nuclear - ℓ1 Algorithm)
The LRR algorithm is pretty robust to (gross) noise; how-
ever, it shows poor performance if the independence assump-
tion about the subspaces is not valid. On the other hand,
SR looks for the sparsest coeﬃcient matrix Z and is more
likely to work when the subspaces are not independent.
Motivated by this observation, we propose the nuclear-ℓ1
algorithm as a unifying approach that incorporates LRR and
SR methods. This algorithm obtains the coeﬃcient matrix
Z by solving

kZ kℓ1 + θkZ k∗

min
Z
s.t., X = AZ

(2.4)

Note that by setting θ = 0, the nuclear-ℓ1 algorithm re-
duces to SR. Also, in the limit θ → ∞, nuclear-ℓ1 reduces
to LRR. Therefore, it can be thought of a generalization
of both LRR and SR methods and is likely to inherit the
beneﬁts of each of them simultaneously. Some of the use-
ful properties of Nuclear-ℓ1 norm and its applications/proof

guarantees in data modeling in general (and more specif-
ically subspace extraction) is discussed in a paper by the
second author [11].
2.2 Noisy Scenario
According to [5], the main challenge of subspace segmen-
tation is to handle noisy in data, i.e., to handle the data
that may not strictly follow subspace structures. The au-
thors in [4] claim that the LRR method has a better per-
formance in the presence of noise compared with previous
algorithms. As the experimental reports in [5] demonstrate,
the SR based segmentation method are quite competitive in
real-world applications. However, since SR ﬁnds the spars-
est representation of each data vector individually, it is not
guaranteed to capture the global structure of the data. This
drawback can adversely aﬀect the performance when the
data is grossly corrupted.
Assume that a percentage of the data vectors are grossly
corrupted and the others are contaminated by small noise.
Therefore, X = X 0 +E , where X is the observed data and E
is the noisy part. In order to recover the low rank matrix Z
from the observed data, LRR solves the following problem:

min
Z,E

||Z ||∗ + λ||E ||2,1

s.t., X = AZ + E

(2.5)

where ||.||2,1 denotes the ℓ2/ℓ1 -norm2 .
Similarly, the nuclear-ℓ1 algorithm is modiﬁed in noisy
scenarios by introducing a regularization term as follows.

min
Z,E

||Z ||ℓ1 + θ||Z ||∗ + λ||E ||2,1

s.t., X = AZ + E

(2.6)

2.2.1 Iterative Method for solving Large Scale Prob-
lems using Nuclear-ℓ1
The problem (2.6) is a convex optimization problem and
can be solved using semideﬁnite programming solvers. These
solvers are based on interior-point methods, and are prob-
lematic when the size of the matrix is large because they
need to solve huge systems of linear equations to compute
the Newton direction. In fact, they can only handle n × n
matrices with n ≤ 100. Therefore, we need to ﬁnd another
way to solve the above problem which is scalable to large
matrices. To this end, we convert (2.6) to the following
equivalent problem:

min
Z,E ,J1 ,J2

s.t.,

||J1 ||∗ + θ||J2 ||ℓ1 + λ||E ||2,1

X = AZ + E ,
Z = J1
J1 = J2

(2.7)

Using the Augmented Lagrange Multiplier (ALM) method,
an iterative scheme can be proposed to solve this problem.
Consider the following augmented Lagrange function:

||J1 ||∗ + θ||J2 ||ℓ1 + λ||E ||2,1
3 (J1 − J2 )”
2 (Z − J1 )” + tr “Y T
1 (X − AZ − E )” + tr “Y T
+tr “Y T
µ
(||X − AZ − E ||2
F + ||Z − J1 ||2
F + ||J1 − J2 ||2
+
F ).
2
2For matrix A = [A1 , A2 , · · · , An ], where Ai denotes the ith
column, ||A||2,1 is given by Pn
i=1 ||Ai ||2

The above problem is unconstrained and can be minimized
with respect to J1 , J2 , Z, and E , respectively, by ﬁxing the
other variables. This results in the following update rules:

(i) Fix the others and update J1 by
J1 = arg min||J1 ||∗ + tr “Y T
2 (Z − J1 )” + tr “Y T
3 (J1 − J2 )”
µ
||Z − J1 ||2
+
F
2
µ
= arg min||J1 ||∗ +
2

||J1 − Z + (Y3 − Y2 )/µ||2
F

= arg min

1
µ

||J1 ||∗ +

1
2

||J1 − Z + (Y3 − Y2 )/µ||2
F .

where o is the Hadamard product and M 0 is a mask matrix
that has 1 on the blocks and zeros everywhere else.

Algorithm 2: Solving Problem (2.7) by ALM
Input: data matrix X , regularization parameters λ, θ.
Initialize: Z = J1 = J2 = 0, E = 0, Y1 = Y2 = Y3 = 0,
µ = 10−6 , U = 106 , ρ = 1.1, and ǫ = 10−8 .
While ||X − AZ − E ||∞ ≥ ǫ or ||Z − J 1||∞ ≥ ǫ
or ||J1 − J 2||∞ ≥ ǫ do
1. Fix the others and update J1 by
J1 = D1/µ (Z + (Y2 − Y3 )/µ).

2. Fix the others and update J2 by
J2 = Sθ/µ (J1 + Y3/µ).

||J1 − J2 ||2
F

3. Fix the others and update Z by
Z = (I + AT A)−1 (AT (X − E ) + J1 + (AT Y1 − Y2 )/µ)

µ
2

4. Fix the others and update E .
Deﬁne Q := X − AZ + Y1/µ and let qi be the ith column
of Q. Then, the ith column of E , denoted by Ei updates
qi
Ei = max(||qi ||2 − λ
µ , 0)
||qi ||2

5. Update the multipliers
Y1 = Y1 + µ(X − AZ − E ),
Y2 = Y2 + µ(Z − J1 ),
Y3 = Y3 + µ(J1 − J2 ).
6. Update µ by µ = min(ρµ, U ).
end While.

3.1 Experiment on Noiseless data
We construct 5 independent subspaces {Si }5
i=1 whose ba-
sis {Ui }5
i=1 are 40 × 3 matrices with entries i.i.d. N (0, 1).
Thus each subspace has rank 3 and the ambient dimension is
40. We construct a 40× 50 data matrix X = [X1 , · · · , X5 ] by
sampling 10 data vectors from each subspace by Xi = UiCi ,
1 ≤ i ≤ 5 with Ci being a 3 × 10 i.i.d. N (0, 1) matrix.
The aﬃnity matrix Z obtained by performing noiseless
SR, LRR, and Nuclear-ℓ1 on this data is shown in Fig. 2.
As can be seen the oﬀ block diagonal terms are very small.

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

(a) recall = 0.46, precision = 100%

(b) recall = 0.87, precision = 100%

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

(c) recall = 0.91, precision = 100

Figure 2: Comparison of the aﬃnity matrices produced
by (a) SR, (b) LRR, (c) Nuclear-ℓ1 on noiseless synthe-
sized data.

(ii) Fix the others and update J2 by
J2 = arg min θ||J2 ||ℓ1 + tr “Y T
3 (J1 − J2 )” +
µ
||J1 − J2 + Y3/µ||2
= arg min θ||J2 ||ℓ1 +
F
2
1
2

||J1 − J2 + Y3/µ||2
F .

||J2 ||ℓ1 +

= arg min

θ
µ

(iii) Fix the others and update Z by

Z = (I + AT A)−1 (AT (X − E ) + J1 + (AT Y1 − Y2 )/µ).

(iv) Fix the others and update E by
E = arg min λ||E ||2,1 + tr “Y T
1 (X − AZ − E )”
µ
||X − AZ − E ||2
+
F
2

= arg min

λ
µ

||E ||2,1 +

1
2

||E − (X − AZ + Y1/µ)||2
F .

Here, µ > 0 is a penalty parameter. Note that although the
steps (i), (ii), and (iv) are convex problems, they all have
simple closed form solutions.
Let Sτ : R → R denote the shrinkage operator Sτ [x] =
sgn(x) · max(|x| − τ , 0), and extend it to matrices by ap-
plying it to each element. Furthermore, let Dτ (X ) denote
the singular value thresholding operator for matrices given
by Dτ (X ) = U Sτ (Σ)V ∗ , where X = U ΣV ∗ is any singular
value decomposition. It is not hard to see that the solutions
to step (i) and (ii) are respectively given by

J1 = D1/µ (Z + (Y2 − Y3 )/µ),
J2 = Sθ/µ (J1 + Y3/µ).

The solution to step (iv) is given by virtue of lemma 3.3
in [8]; Deﬁne Q := X−AZ+Y1 /µ and let qi be the ith column
of Q. Then, the ith column of the solution E , denoted by
Ei is given by

Ei = max(||qi ||2 −

λ
µ

, 0)

qi
||qi ||2

.

The inexact ALM method is outlined in algorithm 2.

3. EXPERIMENT ON SYNTHESIZED DATA
In this section we present the result of performing Nuclear-
ℓ1 LRR and SR on synthesized data. We perform experi-
ments in both the noisy and noiseless case. Our criteria for
evaluating the performance are:
kM 0 oZ ∗ k0
kZ ∗ k0

kM 0 oZ ∗ k0
kM 0 k0

precision =

recall =

,

The value of recall and precision using each algorithm is
presented in the caption of each subﬁgure. Notice that all
algorithms have precision = 100%, this is in line with the-
ory because with high probability the subspaces Ui will be
independent in this case and thus we expect all algorithms
to perform well.

3.2 Experiment on Noisy data
We sample 100 data vectors from 5 subspaces constructed
in a similar way as in Example 3.0.1. This time, a percentage
of the data are grossly corrupted by a multiplicative Gaus-
sian noise N (0, 0.2) and the rest are contaminated by small
additive Gaussian noise N (0, 0.01). We use the modiﬁed ver-
sions of LRR and Nuclear-ℓ1 for noisy case to segment data.
The parameter λ in LRR was set to 0.2. The parameters
λ and θ in Nuclear-ℓ1 were set to 0.2 and 0.1, respectively.
The results are summarized in table 1. These results verify
that Nuclear-ℓ1 is more robust to gross noise.

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

(a) SR

(b) LRR

5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

30

35

40

45

50

(c) Nuclear-ℓ1

Table 1: Experiment results on noisy synthesized data
LRR
Nuclear-ℓ1
recall
precision
precision
64 % 99.84 %
92 %
61 % 98.61 %
81 %
70 %
55 % 97.35 %

Percentage of large noise
10 %
20 %
30 %

recall
62%
60%
51%

3.3 Comparison of robustness to the indepen-
dence assumption
We construct 5 independent subspaces {Si}5
i=1 whose ba-
sis {Ui }5
i=1 are 10 × 3 matrices with entries i.i.d. N (0, 1).
Thus each subspace has rank 3 and the ambient dimension is
10. We construct a 10× 50 data matrix X = [X1 , · · · , X5 ] by
sampling 10 data vectors from each subspace by Xi = UiCi ,
1 ≤ i ≤ 5 with Ci being a 3 × 10 i.i.d. N (0, 1) matrix. The
aﬃnity matrix Z obtained by performing noiseless SR, LRR,
and Nuclear-ℓ1 on this data is shown in Fig. 3. Note that the
subspaces are not independent in this case (5 subspaces of
dimension 3 in R10 ). As can be seen in Fig. 3, LRR has a lot
of nonzero coeﬃcients in the oﬀ block-diagonal part, were
as the non-zero coeﬃcients in Z using SR and Nuclear-ℓ1
is almost concentrated in the block diagonals, verifying our
previous intuition that SR and Nuclear-ℓ1 should perform
better in this setting.

4. MOTION SEGMENTATION
One of the main problems in dynamic vision is the anal-
ysis of dynamic scenes. In these scenes, in addition to the
motion of the camera, there are multiple moving ob ject in
the scene (usually independent of each other). Thus in or-
der to analyze dynamic video scenes an initial step is motion
segmentation. i.e. given multiple image frames of a dynamic
scene taken by a (possibly moving) camera, the goal is to
cluster the tra jectory of feature points ( on the moving ob-
jects) according to the diﬀerent motions these tra jectories
belong to. In literature many diﬀerent camera models have
been proposed. In this pro ject we will focus on the aﬃne
camera model.

Figure 3: Comparison of the aﬃnity matrices produced
by SR, LRR, Nuclear-ℓ1 without the independence as-
sumption

4.1 Formulation of Motion Segmentation as
Subspace Separation
Consider a feature point y , in 3-D space (y ∈ R3 ). and
it’s pro jection on the 2-D image plane (x ∈ R2 ). Under the
aﬃne camera model, these two quantities are related by a
linear transformation:

(4.1)

x = Af »y
1–
where Af ∈ R2×4 is known as the aﬃne motion matrix.
More speciﬁcally as described in [5],
Af = K 2
3
1 0 0 0
0 1 0 0
4
5
0 0 0 1
Here K ∈ R2×3 is the camera calibration matrix and (R, t) ∈
SE (3) is the relative orientation of the image plane with re-
spect to the world coordinates. Suppose we have access to
the tra jectories of P feature points of a rigid ob ject, ob-
tained from F 2-D image frames taken by a moving camera
(denoted by {yf p }p=1...P
f =1...F , where yf p is the pro jection of the
p-th point onto the image plane in the f -th frame ). The
linear constrains of (4.1) can be lumped together in the form:

1–
» R t
0T

where

X = Af Y

· · ·
. . .
· · ·

x1P
...
xF P

x12
x11
X = 2
...
...
64
xF 1 xF 2
3
Af = 2
A1
...
75
64
AF
where Af is the aﬃne motion matrix at frame f. These
linear constraints relates the 3-D coordinates to the tracked
feature points. Notice that

3
75
1 –
yP

; Y = »y1
1

· · ·
· · ·

rank(X ) = rank(Af Y ) ≤ min(rank(Af ), rank(Y )) ≤ 4

This latter fact shows that the tra jectories of feature points
from a single rigid motion will all lie in a linear subspace
of R2F of dimension at most four. In case there are k mov-
ing ob jects the tra jectory of the feature points will lie in
a union of k linear subspaces in R2F . Thus the problem
of motion segmentation (separating feature points based on
their movement) becomes equivalent to the separation of
data drawn from multiple subspaces, based on the subspace
they belong to.

5. EXPERIMENTS ON HOPKINS 155
In this section we test Nuclear-ℓ1 on real motion segmen-
tation tasks. Some previous state of-the-art methods are
also included for comparison.
We evaluate Nuclear-ℓ1 , LRR and SR on the Hopkins 155
motion database [9]. The database consists of 155 sequences
of two and three motions which can be divided into three
main categories: checkerboard, traﬃc, and articulated se-
quences. The tra jectories are extracted automatically with
a tracker, and outliers are manually removed. Therefore, the
tra jectories are corrupted by noise, but do not have missing
entries or outliers.
We consider two settings on this database. The ﬁrst one is
to compare all algorithms under the same circumstances. i.e.
all algorithms use the raw data without any special prepro-
cessing and we use the same spectral partitioning technique
on all of them. In the second setting, diﬀerent algorithms use
speciﬁc preprocessing and postprocessing techniques to en-
hance their performance. Table 2 and 3 shows that Nuclear-
ℓ1 is competitive in both settings. Another interesting result
is that the new class of algorithms ( SR, LRR, Nuclear-ℓ1 )
outperform more classical approaches in motion segmenta-
tion.
The basic algorithm of Nuclear-ℓ1 was able to achieve an
error rate of 2.87%. This performance can be enhanced by
some additional pre-post processing techniques as follows
(these are the same “tricks” used in the LRR method);
First, we notice that the data base has low noise level,
therefore to avoid overﬁtting, we randomly chose 10% of the
entries and corrupt them by adding small Gaussian noise.
Second, since the aﬃnity matrix Z is asymmetric, we convert
it into a Positive Semi-Deﬁnite (PSD) matrix Z1 by solving

min
Z1

kZ1 k∗ + αkE k1

s.t. Z = Z1 + E ,
Z1 (cid:23) 0

with α set to be 0.8. Third, inspired by [10], we decompose
2
Z1 into Z1 = QQT and deﬁne L = ( ˜Q ˜QT )
, where ˜Q is Q
with normalized rows. Fourth, we use Lβ (β = 2.26) as the
aﬃnity matrix for spectral clustering. Finally, in the interest
of fair comparison since there is a degenerated sequence in
Hopkins 155 and previous results disregard the degenerated
sequence so do we.

Table 2: Classiﬁcation error on raw data
RANSAC
LSA
Algorithm GPCA
8.22%
8.99%
NA
mean
Nuclear-ℓ1
SR
Algorithm
LRR
mean
3.89% 3.16%
2.87%

Table 3: Classiﬁcation error with algorithm speciﬁc pre-
post processing
Algorithm GPCA
LSA
SR
LRR
mean
10.34% 4.94% 1.24% 0.87%

Nuclear-ℓ1
0.69%

To give a visual veriﬁcation of our algorithm Fig. 4 shows
the result of applying Nuclear-ℓ1 to one video sequences in
the Hopkins 155 data base.

Figure 4: Visual performance veriﬁcation of nuclear-ℓ1
algorithm on an example frame from Hopkins 155.

6. REFERENCES
[1] E. Candes, J. Romberg, and T. Tao. Stable signal recovery
from incomplete and inaccurate measurements.
Communications on Pure and Applied Mathematics,
59(8):1207-1223, 2006
[2] M. Fazel, Matrix rank minimization with applications,
PhD thesis,2002
[3] E. J. Candes and B. Recht, Exact matrix completion via
convex optimization, Foundations of Computational
Mathematics, vol. 9, no. 6, pp. 717-772, 2009
[4] G. Liu et al., Robust Recovery of Subspaces Structures by
Low-Rank Representation, arXiv 1010.2955v2
[5] S. Rao et al., Motion Segmentation in the Presence of
Outlying, Incomplete, or Corrupted Trajectories, IEEE
Trans. on Pattern Analysis and Machine Intelligence, vol.
32, no. 10, pp. 1832-1845, 2010.
[6] E. Elhamifar et al., Sparse Subspace Clustering, in IEEE
Conference on Computer Vision and Pattern Recognition,
vol. 2, 2009, pp. 2790-2797.
[7] J. Shi et al., Normalized Cuts and Image Segmentation,
IEEE Trans. on Pattern Analysis and Machine Intelligence,
pp. 888-905, 2000.
[8] J. Yang et al., A fast algorithm for edge-preserving
variational multichannel image restoration, SIAM Journal
on Imaging Sciences, vol. 2, no. 2, pp. 569-592, 2009.
[9] R. Tron and R. Vidal, A benchmark for the comparison of
3-d motion segmentation algorithms, in IEEE Conference
on Computer Vision and Pattern Recognition, 2007, pp.
1-8.
[10] F. Lauer and o. Christoph Schn Spectral clustering of
linear subspaces for motion segmentation, in IEEE
International Conference on Computer Vision, 2009.
[11] M. Soltanolkotabi, The power of Nuclear-ℓ1 relaxations in
Modeling Data, Stanford University Technical Report.
under preperation.


Transduction with Matrix Completion:
Three Birds with One Stone

Andrew B. Goldberg1 , Xiaojin Zhu1 , Benjamin Recht1 , Jun-Ming Xu1 , Robert Nowak2
Department of {1Computer Sciences, 2Electrical and Computer Engineering}
University of Wisconsin-Madison, Madison, WI 53706
{goldberg, jerryzhu, brecht, xujm}@cs.wisc.edu, nowak@ece.wisc.edu

Abstract

We pose transductive classi ﬁcation as a matrix completion problem. By assuming
the underlying matrix has a low rank, our formulation is able to handle three prob-
lems simultaneously: i) multi-label learning, where each item has more than one
label, ii) transduction, where most of these labels are unspeciﬁed, and iii) miss-
ing data, where a large number of features are missing. We obtained satisfactory
results on several real-world tasks, suggesting that the low rank assumption may
not be as restrictive as it seems. Our method allows for different loss functions to
apply on the feature and label entries of the matrix. The resulting nuclear norm
minimization problem is solved with a modi ﬁed ﬁxed-point continuation method
that is guaranteed to ﬁnd the global optimum.

1

Introduction

Semi-supervised learning methods make assumptions about how unlabeled data can help in the
learning process, such as the manifold assumption (data lies on a low-dimensional manifold) and
the cluster assumption (classes are separated by low density regions) [4, 16].
In this work, we
present two transductive learning methods under the novel assumption that the feature-by-item and
label-by-item matrices are jointly low rank. This assumption effectively couples different label pre-
diction tasks, allowing us to implicitly use observed labels in one task to recover unobserved labels
in others. The same is true for imputing missing features. In fact, our methods learn in the difﬁ-
cult regime of multi-label transductive learning with missing data that one sometimes encounters in
practice. That is, each item is associated with many class labels, many of the items’ labels may be
unobserved (some items may be completely unlabeled across all labels), and many features may also
be unobserved. Our methods build upon recent advances in matrix completion, with efﬁcient algo-
rithms to handle matrices with mixed real-valued features and discrete labels. We obtain promising
experimental results on a range of synthetic and real-world data.

2 Problem Formulation
Let x1 . . . xn ∈ Rd be feature vectors associated with n items. Let X = [x1 . . . xn ] be the d × n
feature matrix whose columns are the items. Let there be t binary classi ﬁcation tasks, y1 . . . yn ∈
{−1, 1}t be the label vectors, and Y = [y1 . . . yn ] be the t × n label matrix. Entries in X or Y can
be missing at random. Let ΩX be the index set of observed features in X, such that (i, j ) ∈ ΩX if
and only if xij is observed. Similarly, let ΩY be the index set of observed labels in Y . Our main goal
is to predict the missing labels yij for (i, j ) /∈ ΩY . Of course, this reduces to standard transductive
learning when t = 1, |ΩX | = nd (no missing features), and 1 < |ΩY | < n (some missing labels).
In our more general setting, as a side product we are also interested in imputing the missing features,
and de-noising the observed features, in X.

1

2.1 Model Assumptions

The above problem is in general ill-posed. We now describe our assumptions to make it a well-
deﬁned problem. In a nutshell, we assume that X and Y are jointly produced by an underlying
low rank matrix. We then take advantage of the sparsity to ﬁll in the missing labels and features
using a modiﬁed method of matrix completion. Speci ﬁcally, we assume the following generative
story. It starts from a d × n low rank “pre”-feature matrix X0 , with rank(X0 ) (cid:28) min(d, n). The
(cid:0)y0
(cid:1)> ≡ y0
actual feature matrix X is obtained by adding iid Gaussian noise to the entries of X0 : X = X0 + ,
where ij ∼ N (0, σ2
j ∈ Rt of item j are
Y0 = (cid:2)y0
(cid:3) be the soft label matrix. Note the combined (t + d) × n matrix (cid:2)Y0 ; X0 (cid:3) is low
 ). Meanwhile, the t “soft” labels
1j . . . y0
tj
j + b, where W is a t × d weight matrix, and b ∈ Rt is a bias vector. Let
rank too: rank((cid:2)Y0 ; X0 (cid:3)) ≤ rank(X0 ) + 1. The actual label yij ∈ {−1, 1} is generated randomly
j = Wx0
produced by y0
ij ) = 1/ (cid:0)1 + exp(−yij y0
ij )(cid:1). Finally, two random masks ΩX , ΩY
1 . . . y0
n
via a sigmoid function: P (yij |y0
are applied to expose only some of the entries in X and Y , and we use ω to denote the percentage of
observed entries. This generative story may seem restrictive, but our approaches based on it perform
well on synthetic and real datasets, outperforming several baselines with linear classiﬁers.

2.2 Matrix Completion for Heterogeneous Matrix Entries
diate low rank matrix (cid:2)Y0 ; X0 (cid:3). Then, X0 will contain the denoised and completed features, and
With the above data generation model, our task can be deﬁned as follows. Given the partially
observed features and labels as speci ﬁed by X, Y , ΩX , ΩY , we would like to recover the interme-
The key assumption is that the (t + d) × n stacked matrix (cid:2)Y0 ; X0 (cid:3) is of low rank. We will start
sign(Y0 ) will contain the completed and correct labels.
from a “hard” formulation that is illustrative but impractical, then relax it.
argmin
rank(Z)
(1)
Z∈R(t+d)×n
Here, Z is meant to recover (cid:2)Y0 ; X0 (cid:3) by directly minimizing the rank while obeying the observed
z(i+t)j = xij , ∀(i, j ) ∈ ΩX
sign(zij ) = yij , ∀(i, j ) ∈ ΩY ;
s.t.
features and labels. Note the indices (i, j ) ∈ ΩX are with respect to X, such that i ∈ {1, . . . , d}. To
index the corresponding element in the larger stacked matrix Z, we need to shift the row index by t
to skip the part for Y0 , and hence the constraints z(i+t)j = xij . The above formulation assumes that
there is no noise in the generation processes X0 → X and Y0 → Y . Of course, there are several
issues with formulation (1), and we handle them as follows:
• rank() is a non-convex function and difﬁcult to optimize. Following recent work in
Pmin(t+d,n)
matrix completion [3, 2], we relax rank() with the convex nuclear norm kZk∗ =
σk (Z), where σk ’s are the singular values of Z. The relationship between
rank(Z) and kZk∗ is analogous to that of ‘0 -norm and ‘1 -norm for vectors.
k=1
• There is feature noise from X0 to X. Instead of the equality constraints in (1), we minimize
2 (u − v)2 in this
a loss function cx (z(i+t)j , xij ). We choose the squared loss cx (u, v) = 1
work, but other convex loss functions are possible too.
• Similarly, there is label noise from Y0 to Y . The observed labels are of a different type
than the observed features. We therefore introduce another loss function cy (zij , yij ) to
In this work, we use the logistic loss cy (u, v) =
account for the heterogeneous data.
log(1 + exp(−uv)).
In addition to these changes, we will model the bias b either explicitly or implicitly, leading to two
Z ∈ R(t+d)×n , hence the name. Here, Z corresponds to the stacked matrix (cid:2)WX0 ; X0 (cid:3) instead of
alternative matrix completion formulations below.
Formulation 1 (MC-b). In this formulation, we explicitly optimize the bias b ∈ Rt in addition to
(cid:2)Y0 ; X0 (cid:3), making it potentially lower rank. The optimization problem is
X
X
1
µkZk∗ + λ
argmin
cy (zij + bi , yij ) +
|ΩY |
|ΩX |
(i,j )∈ΩY
(i,j )∈ΩX
Z,b

cx (z(i+t)j , xij ),

(2)

2

where µ, λ are positive trade-off weights. Notice the bias b is not regularized. This is a convex
problem, whose optimization procedure will be discussed in section 3. Once the optimal Z, b are
found, we recover the task-i label of item j by sign(zij + bi ), and feature k of item j by z(k+t)j .
one to each item. The corresponding pre-feature matrix is augmented into (cid:2)X0 ; 1> (cid:3), where 1 is the
Formulation 2 (MC-1). In this formulation, the bias is modeled implicitly within Z. Similar to how
bias is commonly handled in linear classi ﬁers, we append an additional feature with constant value
Y0 are linear combinations of rows in (cid:2)X0 ; 1> (cid:3), i.e., rank((cid:2)Y0 ; X0 ; 1> (cid:3)) = rank((cid:2)X0 ; 1> (cid:3)). We
then let Z correspond to the (t + d + 1) × n stacked matrix (cid:2)Y0 ; X0 ; 1> (cid:3), by forcing its last row to
j = Wx0
j + b, the rows of the soft label matrix
all-1 vector. Under the same label assumption y0
X
X
be 1> (hence the name):
1
µkZk∗ + λ
argmin
|ΩY |
|ΩX |
Z∈R(t+d+1)×n
(i,j )∈ΩY
(i,j )∈ΩX
z(t+d+1)· = 1> .
s.t.
This is a constrained convex optimization problem. Once the optimal Z is found, we recover the
task-i label of item j by sign(zij ), and feature k of item j by z(k+t)j .
Other formulations are certainly possible. One way is to let Z correspond to (cid:2)Y0 ; X0 (cid:3) directly,
MC-b and MC-1 differ mainly in what is in Z, which leads to different behaviors of the nuclear norm.
Despite the generative story, we do not explicitly recover the weight matrix W in these formulations.
without introducing bias b or the all-1 row, and hope nuclear norm minimization will prevail. This
is inferior in our preliminary experiments, and we do not explore it further in this paper.

cx (z(i+t)j , xij )

cy (zij , yij ) +

(3)

3 Optimization Techniques

We solve MC-b and MC-1 using modiﬁcations of the Fixed Point Continuation (FPC) method of Ma,
Goldfarb, and Chen [10].1 While nuclear norm minimization can be converted into a semideﬁnite
programming (SDP) problem [2], current SDP solvers are severely limited in the size of problems
they can solve. Instead, the basic ﬁxed point approach is a computationally efﬁcient alternative,
which provably converges to the globally optimal solution and has been shown to outperform SDP
solvers in terms of matrix recoverability.

3.1 Fixed Point Continuation for MC-b

We ﬁrst describe our modiﬁed FPC method for MC-b. It differs from [10] in the extra bias variables
and multiple loss functions. Our ﬁxed point iterative algorithm to solve the unconstrained problem
of (2) consists of two alternating steps for each iteration k :
1. (gradient step) bk+1 = bk − τb g(bk ), Ak = Zk − τZ g(Zk )
2. (shrinkage step) Zk+1 = SτZ µ (Ak ).
In the gradient step, τb and τZ are step sizes whose choice will be discussed next. Overloading
notation a bit, g(bk ) is the vector gradient, and g(Zk ) is the matrix gradient, respectively, of the two
X
loss terms in (2) (i.e., excluding the nuclear norm term):
−yij
λ
 λ|ΩY |
|ΩY |
1 + exp(yij (zij + bi ))
j :(i,j )∈ΩY
−yij
i ≤ t and (i, j ) ∈ ΩY
1+exp(yij (zij +bi )) ,
i > t and (i − t, j ) ∈ ΩX
1|ΩX | (zij − x(i−t)j ),
0,
otherwise
Note for g(zij ), i > t, we need to shift down (un-stack) the row index by t in order to map the
element in Z back to the item x(i−t)j .

g(zij ) =

g(bi ) =

(4)

(5)

1While the primary method of [10] is Fixed Point Continuation with Approximate Singular Value Decom-
position (FPCA), where the approximate SVD is used to speed up the algorithm, we opt to use an exact SVD
for simplicity and will refer to the method simply as FPC.

3

Input: Initial matrix Z0 , bias b0 ,
parameters µ, λ, Step sizes τb , τZ
Determine µ1 > µ2 > · · · > µL = µ > 0.
Set Z = Z0 , b = b0 .
foreach µ = µ1 , µ2 , . . . , µL do
while Not converged do
Compute b = b − τb g(b), A = Z − τZ g(Z)
Compute SVD of A = UΛV>
Compute Z = U max(Λ − τZµ, 0)V>

end

end
Output: Recovered matrix Z, bias b

Algorithm 1: FPC algorithm for MC-b.

Input: Initial matrix Z0 ,
parameters µ, λ, Step sizes τZ
Determine µ1 > µ2 > · · · > µL = µ > 0.
Set Z = Z0 .
foreach µ = µ1 , µ2 , . . . , µL do
while Not converged do
Compute A = Z − τZ g(Z)
Compute SVD of A = UΛV>
Compute Z = U max(Λ − τZµ, 0)V>
Project Z to feasible region z(t+d+1)· = 1>

end

end
Output: Recovered matrix Z

Algorithm 2: FPC algorithm for MC-1.

In the shrinkage step, SτZ µ (·) is a matrix shrinkage operator. Let Ak = UΛV> be the SVD of
Ak . Then SτZ µ (Ak ) = U max(Λ − τZµ, 0)V> , where max is elementwise. That is, the shrinkage
operator shifts the singular values down, and truncates any negative values to zero. This step reduces
the nuclear norm.

Even though the problem is convex, convergence can be slow. We follow [10] and use a con-
tinuation or homotopy method to improve the speed. This involves beginning with a large value
µ1 > µ and solving a sequence of subproblems, each with a decreasing value and using the pre-
vious solution as its initial point. The sequence of values is determined by a decay parameter ηµ :
µk+1 = max{µk ηµ , µ},
k = 1, . . . , L − 1, where µ is the ﬁnal value to use, and L is the number
of rounds of continuation. The complete FPC algorithm for MC-b is listed in Algorithm 1.

A minor modi ﬁcation of the argument in [10] reveals that as long as we choose non-negative step
sizes satisfying τb < 4|ΩY |/(λn) and τZ < min {4|ΩY |/λ, |ΩX |}, the algorithms MC-b will be
guaranteed to converge to a global optimum. Indeed, to guarantee convergence, we only need that
the gradient step is non-expansive in the sense that
kb1 − τb g(b1 ) − b2 + τb g(b2 )k2 + kZ1 − τZ g(Z1 ) − Z2 + τZ g(Z2 )k2
F ≤ kb1 − b2 k2 + kZ1 − Z2 k2
F
for all b1 , b2 , Z1 , and Z2 . Our choice of τb and τZ guarantee such non-expansiveness. Once this
non-expansiveness is satisﬁed, the remainder of the convergence analysis is the same as in [10].

3.2 Fixed Point Continuation for MC-1

Our modiﬁed FPC method for MC-1 is similar except for two differences. First, there is no bias
variable b. Second, the shrinkage step will in general not satisfy the all-1-row constraints in (3).
Thus, we add a third projection step at the end of each iteration to project Zk+1 back to the feasible
region, by simply setting its last row to all 1’s. The complete algorithm for MC-1 is given in Algo-
rithm 2. We were unable to prove convergence for this gradient + shrinkage + projection algorithm.
Nonetheless, in our empirical experiments, Algorithm 2 always converges and tends to outperform
MC-b. The two algorithms have about the same convergence speed.

4 Experiments

We now empirically study the ability of matrix completion to perform multi-class transductive clas-
siﬁcation when there is missing data. We ﬁrst present a family of 24 experiments on a synthetic
task by systematically varying different aspects of the task, including the rank of the problem, noise
level, number of items, and observed label and feature percentage. We then present experiments on
two real-world datasets: music emotions and yeast microarray. In each experiments, we compare
MC-b and MC-1 against four other baseline algorithms. Our results show that MC-1 consistently
outperforms other methods, and MC-b follows closely.
Parameter Tuning and Other Settings for MC-b and MC-1: To tune the parameters µ and λ,
we use 5-fold cross validation (CV) separately for each experiment. Speciﬁcally, we randomly

4

divide ΩX and ΩY into ﬁve disjoint subsets each. We then run our matrix completion algorithms
using 4
5 of the observed entries, measure its performance on the remaining 1
5 , and average over
the ﬁve folds. Since our main goal is to predict unobserved labels, we use label error as the CV
performance criterion to select parameters. Note that tuning µ is quite efﬁcient since all values
under consideration can be evaluated in one run of the continuation method. We set ηµ = 0.25 and,
as in [10], consider µ values starting at σ1 ηµ , where σ1 is the largest singular value of the matrix
of observed entries in [Y ; X] (with the unobserved entries set to 0), and decrease µ until 10−5 .
The range of λ values considered was {10−3 , 10−2 , 10−1 , 1}. We initialized b0 to be all zero and
Z0 to be the rank-1 approximation of the matrix of observed entries in [Y ; X] (with unobserved
entries set to 0) obtained by performing an SVD and reconstructing the matrix using only the largest
singular value and corresponding left and right singular vectors. The step sizes were set as follows:
τZ = min( 3.8|ΩY |
, |ΩX |), τb = 3.8|ΩY |
λn . Convergence was deﬁned as relative change in objective
λ
functions (2)(3) smaller than 10−5 .
Baselines: We compare to the following baselines, each consisting of some missing feature impu-
tation step on X ﬁrst, then using a standard SVM to predict the labels:
[FPC+SVM] Matrix com-
pletion on X alone using FPC [10]. [EM(k)+SVM] Expectation Maximization algorithm to impute
missing X entries using a mixture of k Gaussian components. As in [9], missing features, mixing
component parameters, and the assignments of items to components are treated as hidden variables,
which are estimated in an iterative manner to maximize the likelihood of the data. [Mean+SVM]
Impute each missing feature by the mean of the observed entries for that feature. [Zero+SVM]
Impute missing features by ﬁlling in zeros.
After imputation, an SVM is trained using the available (noisy) labels in ΩY for that task, and
predictions are made for the rest of the labels. All SVMs are linear, trained using SVMlin2 , and the
regularization parameter is tuned using 5-fold cross validation separately for each task. The range
of parameter values considered was {10−8 , 10−7 , . . . , 107 , 108 }.
(xij − ˆxij )2(cid:17)
(cid:16)P
/P
Evaluation Method: To evaluate performance, we consider two measures: transductive label error,
i.e., the percentage of unobserved labels predicted incorrectly; and relative feature imputation error
ij , where ˆx is the predicted feature value. In the tables below,
x2
ij /∈ΩX
ij /∈ΩX
for each parameter setting, we report the mean performance (and standard deviation in parenthesis)
of different algorithms over 10 random trials. The best algorithm within each parameter setting,
as well as any statistically indistinguishable algorithms via a two-tailed paired t-test at signi ﬁcance
level α = 0.05, are marked in bold.

4.1 Synthetic Data Experiments

Synthetic Data Generation: We generate a family of synthetic datasets to systematically explore
the performance of the algorithms. We ﬁrst create a rank- r matrix X0 = LR> , where L ∈ Rd×r
and R ∈ Rn×r with entries drawn iid from N (0, 1). We then normalize X0 such that its entries
have variance 1. Next, we create a weight matrix W ∈ Rt×d and bias vector b ∈ Rt , with all entries
drawn iid from N (0, 10). We then produce X, Y0 , Y according to section 2.1. Finally, we produce
the random ΩX , ΩY masks with ω percent observed entries.
Using the above procedure, we vary ω = 10%, 20%, 40%, n = 100, 400, r = 2, 4, and σ2
 =
0.01, 0.1, while ﬁxing t = 10, d = 20, to produce 24 different parameter settings. For each setting,
we generate 10 trials, where the randomness is in the data and mask.
Synthetic experiment results: Table 1 shows the transductive label errors, and Table 2 shows the
relative feature imputation errors, on the synthetic datasets. We make several observations.

Observation 1: MC-b and MC-1 are the best for feature imputation, as Table 2 shows. However,
the imputations are not perfect, because in these particular parameter settings the ratio between the
number of observed entries over the degrees of freedom needed to describe the feature matrix (i.e.,
r(d + n − r)) is below the necessary condition for perfect matrix completion [2], and because there
is some feature noise. Furthermore, our CV tuning procedure selects parameters µ, λ to optimize
label error, which often leads to suboptimal imputation performance. In a separate experiment (not
reported here) when we made the ratio sufﬁciently large and without noise, and speciﬁcally tuned for

2 http://vikas.sindhwani.org/svmlin.html

5

4

100

400

Table 1: Transductive label error of six algorithms on the 24 synthetic datasets. The varying pa-
 , rank(X0 ) = r , number of items n, and observed label and feature
rameters are feature noise σ2
percentage ω . Each row is for a unique parameter combination. Each cell shows the mean(standard
deviation) of transductive label error (in percentage) over 10 random trials. The “meta-average” row
is the simple average over all parameter settings and all trials.
σ2
n
r
ω
FPC+SVM EM1+SVM Mean+SVM
MC-1
MC-b

34.8(7.0)
31.8(4.3)
40.5(5.7)
34.6(3.9)
10% 37.8(4.0)
0.01
2
100
17.6(2.1)
17.0(2.2)
28.7(4.1)
19.7(2.4)
20% 23.5(2.9)
9.6(1.5)
40% 15.1(3.1)
10.8(1.8)
10.4(1.0)
16.5(2.5)
19.9(1.7)
32.4(2.9)
24.2(1.9)
23.7(1.7)
10% 26.5(2.0)
12.0(1.9)
11.7(1.9)
20.0(1.9)
12.6(2.2)
20% 15.9(2.5)
7.3(1.4)
7.2(1.8)
12.2(1.8)
8.0(1.6)
40% 11.7(2.0)
43.2(2.2)
41.5(2.6)
40.8(4.4)
43.5(2.9)
10% 42.5(4.0)
26.2(2.8)
26.7(1.7)
20% 33.2(2.3)
30.8(2.7)
35.5(1.4)
14.1(2.4)
13.6(2.6)
14.3(2.7)
22.5(2.0)
40% 19.6(3.1)
33.4(1.6)
32.1(1.6)
37.7(1.2)
34.2(1.8)
10% 35.3(3.1)
19.8(1.1)
19.1(1.3)
26.9(1.5)
20.5(1.4)
20% 24.4(2.3)
8.6(1.1)
16.4(1.2)
9.2(0.9)
9.5(0.5)
40% 14.6(1.8)
34.6(3.5)
37.3(6.4)
10% 39.6(5.5)
40.2(5.3)
41.5(6.0)
21.6(2.6)
20.1(1.7)
31.8(4.7)
26.8(3.7)
20% 25.2(2.6)
13.2(2.0)
12.6(1.4)
18.5(2.7)
15.1(2.4)
40% 15.7(3.1)
22.6(1.9)
34.5(3.3)
28.8(2.6)
27.6(2.4)
10% 27.6(2.1)
15.2(1.7)
22.6(2.4)
18.4(2.5)
16.8(2.3)
20% 18.0(2.2)
10.1(1.3)
10.4(2.1)
40% 12.0(2.1)
11.1(1.9)
14.1(2.0)
42.3(2.0)
41.5(2.5)
10% 42.5(4.3)
44.6(2.9)
45.6(1.9)
30.9(3.1)
29.0(2.2)
36.2(2.3)
34.9(3.0)
20% 33.3(1.9)
18.7(2.4)
18.4(3.1)
23.9(2.0)
21.6(2.4)
40% 21.4(2.7)
35.1(1.2)
34.0(1.7)
38.7(1.3)
36.3(1.4)
10% 36.3(2.7)
21.8(1.0)
20% 25.5(2.0)
23.8(1.5)
25.1(1.4)
28.4(1.7)
12.8(0.8)
18.3(1.2)
14.7(1.3)
13.9(1.2)
40% 16.0(1.8)
meta-average
25.6
21.4
22.6
24.1
28.6

Zero+SVM
40.5(5.1)
27.4(4.4)
15.4(2.3)
31.5(2.7)
19.7(1.7)
12.1(2.0)
42.9(2.9)
33.9(1.5)
21.7(2.3)
38.2(1.4)
26.9(1.3)
16.5(1.3)
41.0(5.7)
29.9(4.0)
17.2(2.4)
33.6(2.8)
21.8(2.5)
14.0(2.4)
43.6(2.3)
35.4(1.6)
23.3(2.5)
39.1(1.2)
28.4(1.8)
18.2(1.2)
28.0

2

100

4

100

0.1

400

400

400

imputation error, both MC-b and MC-1 did achieve perfect feature imputation. Also, FPC+SVM is
slightly worse in feature imputation. This may seem curious as FPC focuses exclusively on imputing
X. We believe the fact that MC-b and MC-1 can use information in Y to enhance feature imputation
in X made them better than FPC+SVM.
Observation 2: MC-1 is the best for multi-label transductive classiﬁcation, as suggested by Table 1.
Surprisingly, the feature imputation advantage of MC-b did not translate into classiﬁcation, and
FPC+SVM took second place.

Observation 3: The same factors that affect standard matrix completion also affect classiﬁcation
performance of MC-b and MC-1. As the tables show, everything else being equal, less feature noise
(smaller σ2
 ), lower rank r , more items, or more observed features and labels, reduce label error.
Beneﬁcial combination of these factors (the 6th row) produces the lowest label errors.
Matrix completion beneﬁts from more tasks. We performed one additional synthetic data exper-
iment examining the effect of t (the number of tasks) on MC-b and MC-1, with the remaining data
parameters ﬁxed at ω = 10%, n = 400, r = 2, d = 20, and σ2
 = 0.01. Table 3 reveals that both
MC methods achieve statistically signi ﬁcantly better label prediction and imputation performance
with t = 10 than with only t = 2 (as determined by two-sample t-tests at signi ﬁcance level 0.05).

4.2 Music Emotions Data Experiments

In this task introduced by Trohidis et al. [14], the goal is to predict which of several types of emotion
are present in a piece of music. The data3 consists of n = 593 songs of a variety of musical genres,
each labeled with one or more of t = 6 emotions (i.e., amazed-surprised, happy-pleased, relaxing-
calm, quiet-still, sad-lonely, and angry-fearful). Each song is represented by d = 72 features (8
rhythmic, 64 timbre-based) automatically extracted from a 30-second sound clip.

3Available at http://mulan.sourceforge.net/datasets.html

6

4

100

400

Table 2: Relative feature imputation error on the synthetic datasets. The algorithm Zero+SVM is
not shown because it by deﬁnition has relative feature imputation error 1.
σ2
ω
n
r
EM1+SVM
FPC+SVM
MC-b
MC-1

0.87(0.06)
10% 0.84(0.04)
0.88(0.06)
0.01
2
100
1.01(0.12)
0.57(0.07)
0.57(0.06)
20% 0.54(0.08)
0.67(0.13)
0.27(0.06)
0.27(0.06)
40% 0.29(0.06)
0.34(0.03)
0.72(0.04)
10% 0.73(0.03)
0.79(0.07)
0.76(0.03)
0.45(0.04)
20% 0.43(0.04)
0.50(0.04)
0.46(0.05)
0.22(0.04)
0.21(0.04)
40% 0.30(0.10)
0.24(0.05)
0.96(0.03)
0.96(0.03)
10% 0.99(0.04)
1.22(0.11)
0.77(0.04)
0.78(0.05)
20% 0.77(0.05)
0.92(0.07)
0.40(0.03)
40% 0.42(0.07)
0.49(0.04)
0.42(0.04)
0.88(0.03)
10% 0.87(0.04)
1.00(0.08)
0.89(0.01)
20% 0.69(0.07)
0.67(0.04)
0.66(0.03)
0.69(0.03)
0.29(0.02)
0.38(0.03)
0.34(0.03)
40% 0.34(0.05)
0.93(0.05)
0.93(0.04)
10% 0.92(0.05)
1.18(0.10)
20% 0.69(0.07)
0.94(0.07)
0.74(0.06)
0.72(0.06)
0.52(0.05)
40% 0.51(0.05)
0.67(0.08)
0.53(0.05)
10% 0.79(0.03)
0.80(0.03)
0.84(0.03)
0.96(0.07)
0.64(0.06)
20% 0.64(0.06)
0.73(0.07)
0.67(0.04)
0.45(0.05)
0.57(0.07)
0.49(0.05)
40% 0.48(0.04)
0.97(0.03)
0.97(0.03)
10% 1.01(0.04)
1.25(0.05)
0.85(0.03)
0.85(0.03)
20% 0.84(0.03)
1.07(0.06)
40% 0.59(0.03)
0.61(0.04)
0.63(0.04)
0.80(0.09)
0.92(0.02)
10% 0.90(0.02)
1.08(0.07)
0.92(0.01)
0.77(0.02)
20% 0.75(0.04)
0.86(0.05)
0.79(0.03)
0.55(0.04)
0.66(0.06)
0.59(0.04)
40% 0.56(0.03)
meta-average
0.66
0.66
0.68
0.78

Mean+SVM
1.06(0.02)
1.03(0.02)
1.01(0.01)
1.02(0.01)
1.01(0.00)
1.00(0.00)
1.05(0.01)
1.02(0.01)
1.01(0.01)
1.01(0.00)
1.01(0.00)
1.00(0.00)
1.06(0.02)
1.03(0.02)
1.02(0.01)
1.02(0.01)
1.01(0.00)
1.00(0.00)
1.05(0.02)
1.02(0.01)
1.01(0.01)
1.01(0.01)
1.01(0.00)
1.00(0.00)
1.02

4

100

400

0.1

2

100

400

400

Table 3: More tasks help matrix completion (ω = 10%, n = 400, r = 2, d = 20, σ2
 = 0.01).
t
FPC+SVM
MC-1
MC-b
FPC+SVM
MC-1
MC-b
0.78(0.07)
30.1(2.8)
2
22.9(2.2)
20.5(2.5)
0.78(0.04)
0.76(0.03)
26.5(2.0)
19.9(1.7)
0.73(0.03)
0.72(0.04)
10
23.7(1.7)
0.76(0.03)
relative feature imputation error
transductive label error

Table 4: Performance on the music emotions data.
80%
60%
60%
80%
Algorithm
ω =40%
ω =40%
0.41(0.02)
0.54(0.10)
0.69(0.05)
MC-b
22.2(1.6)
25.2(1.0)
28.0(1.2)
19.8(2.4)
23.7(1.6)
0.25(0.03)
0.46(0.12)
0.60(0.05)
MC-1
27.4(0.8)
0.31(0.03)
0.46(0.02)
0.64(0.01)
FPC+SVM
24.4(2.0)
25.2(1.6)
26.9(0.7)
0.13(0.01)
23.6(1.1)
26.0(1.1)
0.23(0.04)
0.46(0.09)
EM1+SVM
21.2(2.3)
23.1(1.2)
26.2(0.9)
0.15(0.02)
0.27(0.04)
21.6(1.6)
EM4+SVM
0.49(0.10)
0.19(0.00)
0.18(0.00)
26.3(0.8)
0.20(0.01)
Mean+SVM
22.6(1.3)
24.2(1.0)
1.00(0.00)
30.3(0.6)
1.00(0.00)
1.00(0.00)
28.9(1.1)
25.7(1.4)
Zero+SVM
relative feature imputation error
transductive label error

We vary the percentage of observed entries ω = 40%, 60%, 80%. For each ω , we run 10 random
trials with different masks ΩX , ΩY . For this dataset, we tuned only µ with CV, and set λ = 1.
The results are in Table 4. Most importantly, these results show that MC-1 is useful for this real-
world multi-label classiﬁcation problem, leading to the best (or statistically indistinguishable from
the best) transductive error performance with 60% and 80% of the data available, and close to the
best with only 40%.

We also compared these algorithms against an “oracle baseline” (not shown in the table). In this
baseline, we give 100% features (i.e., no indices are missing from ΩX ) and the training labels
in ΩY to a standard SVM, and let it predict the unspeciﬁed labels. On the same random tri-
als, for observed percentage ω = 40%, 60%, 80%, the oracle baseline achieved label error rate
22.1(0.8), 21.3(0.8), 20.5(1.8) respectively. Interestingly, MC-1 with ω = 80% (19.8) is statisti-
cally indistinguishable from the oracle baseline.

7

4.3 Yeast Microarray Data Experiments

This dataset comes from a biological domain and involves the problem of Yeast gene functional
classiﬁcation. We use the data studied by Elisseeff and Weston [5], which contains n = 2417
examples (Yeast genes) with d = 103 input features (results from microarray experiments).4 We
follow the approach of [5] and predict each gene’s membership in t = 14 functional classes. For
this larger dataset, we omitted the computationally expensive EM4+SVM methods, and tuned only
µ for matrix completion while ﬁxing λ = 1.
Table 5 reveals that MC-b leads to statistically signiﬁcantly lower transductive label error for this bi-
ological dataset. Although not highlighted in the table, MC-1 is also statistically better than the SVM
methods in label error. In terms of feature imputation performance, the MC methods are weaker than
FPC+SVM. However, it seems simultaneously predicting the missing labels and features appears to
provide a large advantage to the MC methods. It should be pointed out that all algorithms except
Zero+SVM in fact have small but non-zero standard deviation on imputation error, despite what the
ﬁxed-point formatting in the table suggests. For instance, with ω = 40%, the standard deviation is
0.0009 for MC-1, 0.0011 for FPC+SVM, and 0.0001 for Mean+SVM.
Again, we compared these algorithms to an oracle SVM baseline with 100% observed entries in ΩX .
The oracle SVM approach achieves label error of 20.9(0.1), 20.4(0.2), and 20.1(0.3) for ω =40%,
60%, and 80% observed labels, respectively. Both MC-b and MC-1 signiﬁcantly outperform this
oracle under paired t-tests at signiﬁcance level 0.05. We attribute this advantage to a combination
of multi-label learning and transduction that is intrinsic to our matrix completion methods.

Table 5: Performance on the yeast data.
80%
60%
ω =40%
Algorithm
80%
60%
ω =40%
8.7(0.4)
12.2(0.3)
16.1(0.3)
0.73(0.02)
0.76(0.00)
0.83(0.02)
MC-b
8.5(0.4)
0.74(0.00)
0.92(0.00)
MC-1
0.86(0.00)
13.0(0.2)
16.7(0.3)
0.72(0.00)
0.76(0.00)
0.81(0.00)
FPC+SVM
20.3(0.3)
20.8(0.3)
21.5(0.3)
0.77(0.01)
1.04(0.02)
1.15(0.02)
EM1+SVM
20.4(0.2)
21.2(0.2)
22.0(0.2)
1.00(0.00)
1.00(0.00)
1.00(0.00)
Mean+SVM
20.5(0.4)
21.1(0.2)
21.7(0.2)
1.00(0.00)
21.6(0.2)
1.00(0.00)
1.00(0.00)
20.5(0.4)
Zero+SVM
21.1(0.2)
relative feature imputation error
transductive label error

5 Discussions and Future Work

We have introduced two matrix completion methods for multi-label transductive learning with miss-
ing features, which outperformed several baselines. In terms of problem formulation, our methods
differ considerably from sparse multi-task learning [11, 1, 13] in that we regularize the feature and
label matrix directly, without ever learning explicit weight vectors. Our methods also differ from
multi-label prediction via reduction to binary classi ﬁcation or ranking [15], and via compressed
sensing [7], which assumes sparsity in that each item has a small number of positive labels, rather
than the low-rank nature of feature matrices. These methods do not naturally allow for missing fea-
tures. Yet other multi-label methods identify a subspace of highly predictive features across tasks in
a ﬁrst stage, and learn in this subspace in a second stage [8, 12]. Our methods do not require separate
stages. Learning in the presence of missing data typically involves imputation followed by learning
with completed data [9]. Our methods perform imputation plus learning in one step, similar to EM
on missing labels and features [6], but the underlying model assumption is quite different.

A drawback of our methods is their restriction to linear classiﬁers only. One future extension is to
explicitly map the partial feature matrix to a partially observed polynomial (or other) kernel Gram
matrix, and apply our methods there. Though such mapping proliferates the missing entries, we
hope that the low-rank structure in the kernel matrix will allow us to recover labels that are nonlinear
functions of the original features.

Acknowledgements: This work is supported in part by NSF IIS-0916038, NSF IIS-0953219, AFOSR FA9550-
09-1-0313, and AFOSR A9550-09-1-0423. We also wish to thank Brian Eriksson for useful discussions and
source code implementing EM-based imputation.

4Available at http://mulan.sourceforge.net/datasets.html

8

References

[1] Andreas Argyriou, Charles A. Micchelli, and Massimiliano Pontil. On spectral learning. Jour-
nal of Machine Learning Research, 11:935–953, 2010.
[2] Emmanuel J. Cand `es and Benjamin Recht. Exact matrix completion via convex optimization.
Foundations of Computational Mathematics, 9:717 –772, 2009.
[3] Emmanuel J. Cand `es and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56:2053–2080, 2010.
[4] Olivier Chapelle, Alexander Zien, and Bernhard Sch ¨olkopf, editors. Semi-supervised learning.
MIT Press, 2006.
[5] Andr ´e Elisseeff and Jason Weston. A kernel method for multi-labelled classi ﬁcation.
In
Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, NIPS, pages 681–
687. MIT Press, 2001.
[6] Zoubin Ghahramani and Michael I. Jordan. Supervised learning from incomplete data via
an EM approach. In Advances in Neural Information Processing Systems 6, pages 120 –127.
Morgan Kaufmann, 1994.
[7] Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang. Multi-label prediction via com-
pressed sensing. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta,
editors, Advances in Neural Information Processing Systems 22, pages 772–780. 2009.
[8] Shuiwang Ji, Lei Tang, Shipeng Yu, and Jieping Ye. Extracting shared subspace for multi-label
classiﬁcation. In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 381–389, New York, NY, USA, 2008. ACM.
[9] Roderick J. A. Little and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley-
Interscience, 2nd edition, September 2002.
[10] Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and Bregman iterative methods
for matrix rank minimization. Mathematical Programming Series A, to appear (published
online September 23, 2009).
[11] Guillaume Obozinski, Ben Taskar, and Michael I. Jordan. Joint covariate selection and joint
subspace selection for multiple classi ﬁcation problems. Statistics and Computing, 20(2):231–
252, 2010.
[12] Piyush Rai and Hal Daume. Multi-label prediction via sparse inﬁnite CCA.
In Y. Bengio,
D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural
Information Processing Systems 22, pages 1518 –1526. 2009.
[13] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the
18th Annual Conference on Learning Theory, pages 545–560. Springer-Verlag, 2005.
[14] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vlahavas. Multilabel classiﬁcation of music
into emotions. In Proc. 9th International Conference on Music Information Retrieval (ISMIR
2008), Philadelphia, PA, USA, 2008, 2008.
[15] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data.
Knowledge Discovery Handbook. Springer, 2nd edition, 2010.
[16] Xiaojin Zhu and Andrew B. Goldberg. Introduction to Semi-Supervised Learning. Morgan &
Claypool, 2009.

In Data Mining and

9


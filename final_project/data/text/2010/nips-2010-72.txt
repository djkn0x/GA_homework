Learning Kernels with Radiuses of Minimum
Enclosing Balls

Changshui Zhang
Guangyun Chen
Kun Gai
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology (TNList)
Department of Automation, Tsinghua University, Beijing 100084, China
{gaik02, cgy08}@mails.thu.edu.cn, zcs@mail.thu.edu.cn

Abstract

In this paper, we point out that there exist scaling and initialization problems in
most existing multiple kernel learning (MKL) approaches, which employ the large
margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason
is that the margin itself can not well describe how good a kernel is due to the
negligence of the scaling. We use the ratio between the margin and the radius
of the minimum enclosing ball to measure the goodness of a kernel, and present a
new minimization formulation for kernel learning. This formulation is invariant to
scalings of learned kernels, and when learning linear combination of basis kernels
it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of
norm constraints on combination coefﬁcients. We establish the differentiability of
our formulation, and propose a gradient projection algorithm for kernel learning.
Experiments show that our method signiﬁcantly outperforms both SVM with the
uniform combination of basis kernels and other state-of-art MKL approaches.

1 Introduction

In the past years, kernel methods, like support vector machines (SVM), have achieved great success
in many learning problems, such as classiﬁcation and regression. For such tasks, the performance
strongly depends on the choice of the kernels used. A good kernel function, which implicitly char-
acterizes a suitable transformation of input data, can greatly beneﬁt the accuracy of the predictor.
However, when there are many available kernels, it is difﬁcult for the user to pick out a suitable one.
Kernel learning has been developed to jointly learn both a kernel function and an SVM classiﬁer.
Chapelle et al. [1] present several principles to tune parameters in kernel functions. In particular,
when the learned kernel is restricted to be a linear combination of multiple basis kernels, the prob-
lem of learning the combination coefﬁcients as well as an SVM classiﬁer is usually called multiple
kernel learning (MKL). Lanckriet et al. [2] formulate the MKL problem as a quadratically con-
strained quadratic programming problem, which implicitly uses an L1 norm constraint to promote
sparse combinations. To enhance the computational efﬁciency, different approaches for solving this
MKL problem have been proposed using SMO-like strategies [3], semi-inﬁnite linear program [4],
gradient-based methods [5], and second-order optimization [6]. Some other subsequent work ex-
plores more generality of multiple kernel learning by promoting non-sparse [7, 8] or group-sparse
[9] combinations of basis kernels, or using other forms of learned kernels, e.g., a combination of an
exponential number of kernels [10] or nonlinear combinations [11, 12, 13].
Most existing MKL approaches employ the objective function used in SVM. With an acceptable
empirical loss, they aim to ﬁnd the kernel which leads to the largest margin of the SVM classi-
ﬁer. However, despite the substantial progress in both the algorithmic design and the theoretical
understanding for the MKL problem, none of the approaches seems to reliably outperform baseline

1

methods, like SVM with the uniform combination of basis kernels [13]. As will be shown in this
paper, the large margin principle used in these methods causes the scaling problem and the initializa-
tion problem, which can strongly affect ﬁnal solutions of learned kernels as well as performances.
It implicates that the large margin preference can not reliably result in a good kernel, and thus the
margin itself is not a suitable measure of the goodness of a kernel.
Motivated by the generalization bounds for SVM and kernel learning, we use the ratio between
the margin of the SVM classiﬁer and the radius of the minimum enclosing ball (MEB) of data in
the feature space endowed with the learned kernel as a measure of the goodness of the kernel, and
propose a new kernel learning formulation. Our formulation differs from the radius-based principle
by Chapelle et al. [1]. Their principle is sensitive to kernel scalings when a nonzero empirical loss
is allowed, also causing the same problems as the margin-based formulations. We prove that our
formulation is invariant to scalings of learned kernels, and also invariant to initial scalings of basis
kernels and to the types (e.g., L1 or L2 ) of norm constraints on kernel parameters for the MKL
problem. Therefore our formulation completely addresses the scaling and initialization problems.
Experiments show that our approach gives signiﬁcant performance improvements both over SVM
with the uniform combination of basis kernels and over other state-of-art kernel learning methods.
Our proposed kernel learning problem can be reformulated to a tri-level optimization problem. We
establish the differentiability of a general family of multilevel optimization problems. This enables
us to generally tackle the radius of the minimal enclosing ball, or other complicated optimal value
functions, in the kernel learning framework by simple gradient-based methods. We hope that our
results will also beneﬁt other learning problems.
The paper is structured as follows. Section 2 shows problems in previous MKL formulations. In
Section 3 we present a new kernel learning formulation and give discussions. Then, we study the
differentiability of multilevel optimization problems and give an efﬁcient algorithm in Section 4 and
Section 5, respectively. Experiments are shown in Section 6. Finally, we close with a conclusion.

2 Measuring how good a kernel is
Let D = {(x1 , y1 ), ..., (xn , yn )} denote a training set of n pairs of input points xi ∈ X and target
labels yi ∈ {±1}. Suppose we have a kernel family K = {k : X × X → R}, in which any kernel
function k implicitly deﬁnes a transformation φ(·; k) from the input space X to a feature space by
k(xc , xd ) = hφ(xc ; k), φ(xd ; k)i. Let a classiﬁer be linear in the feature space endowed with k , as
f (x; w, b, k) = hφ(x; k), wi + b,
(1)
the sign of which is used to classify data. The task of kernel learning (for binary classiﬁcation) is to
learn both a kernel function k ∈ K and a classiﬁer w and b.
To make the problem trackable, the learned kernel is usually restricted to a parametric form k(θ) (·, ·),
where θ = [θi ]i is the kernel parameter. Then the problem of learning a kernel transfers to the prob-
k(θ) (·, ·) = Pm
lem of learning a kernel parameter θ . The most common used kernel form is a linear combination
of multiple basis kernels, as
j=1 θj kj (·, ·), θj ≥ 0.
2.1 Problems in multiple kernel learning
2 kwk2 + C P
Most existing MKL approaches, e.g., [2, 4, 5], employ the equivalent objective function as in SVM:
s.t. yi f (xi ; w, b, k) + ξi ≥ 1, ξi ≥ 0,
mink,w,b,ξi
(3)
1
i ξi ,
where ξi is the hinge loss. This problem can be reformulated to
2 kwk2 + C P
˜G(k),
mink :
(4)
s.t. yi f (xi ; w, b, k) + ξi ≥ 1, ξi ≥ 0.
˜G(k) = minw,b,ξi
where
(5)
1
i ξi ,
For any kernel k , the optimal classiﬁer w and b is actually the SVM classiﬁer with the kernel k . Let γ
denote the margin of the SVM classiﬁer in the feature space endowed with k . We have γ−2 = kwk2 .
Thus the term kwk2 makes formulation (3) prefer the kernel that results in an SVM classiﬁer with a
larger margin (as well as an acceptable empirical loss). Here, a natural question is that for different
kernels whether the margins of SVM classiﬁers can well measure the goodness of the kernels.

(2)

2

√
To answer this question, we consider what happens when a kernel k is enlarged by a scalar a:
knew = ak , where a > 1. The corresponding transformations satisfy φ(·; knew ) =
aφ(·; k). For
√
k , let {w∗ , b∗ } denote the optimal solution of (5). For knew , we set w2 = w∗
2 kw2 k2 + C P
a and b2 = b∗
1 ,
1 /
then we have kw2 k2 = kw∗
1 k2 /a, and f (x; w2 , b2 , knew ) and f (x; w∗
1 , b∗
C P
1 , k) are the same classiﬁer,
resulting in the same ξi . Then we obtain: ˜G(ak) = ˜G(knew ) ≤ 1
2 kw∗
1 k2 +
i ξi < 1
i ξi = ˜G(k), which means the enlarged kernel gives a larger margin and a smaller objective
value. As a consequence, on one hand, the large margin preference guides the scaling of the learned
kernel to be as large as possible. On the other hand, any kernel, even the one resulting in a bad
performance, can give an arbitrarily large margin by enlarging its scaling. This problem is called the
scaling problem. It shows that the margin is not a suitable measure of the goodness of a kernel.
In the linear combination case, the scaling problem causes that the kernel parameter θ does not
converge in the optimization. A remedy is to use a norm constraint on θ . However, it has been
shown in recent literature [7, 9] that different types of norm constraints ﬁt different data sets. So
users face the difﬁculty of choosing a suitable norm constraint. Even after a norm constraint is
selected, the scaling problem also causes another problem about the initialization. Consider an L1
norm constraint and a learned kernel which is a combination of two basis kernels, as
k(θ) (·, ·) = θ1k1 (·, ·) + θ2k2 (·, ·),
θ1 , θ2 ≥ 0,
θ1 + θ2 = 1.
(6)
To leave the empirical loss out of consideration, assume: (a) both k1 and k2 can lead to zero em-
pirical loss, (b) k1 results in a larger margin than k2 . For simplicity, we further restrict θ1 and θ2
to be equal to 0 or 1, to enable kernel selection. The MKL formulation (3), of course, will choose
k1 from {k1 , k2 } due to the large margin preference. Then we set knew
(·, ·) = ak1 (·, ·), where a
1
is a small scalar to make that knew
has a smaller margin than k2 . After knew
substitutes for k1 , the
MKL formulation (3) will select k2 from {knew
, k2 }. The example shows that the ﬁnal solution can
1
1
1
be greatly affected by the initial scalings of basis kernels, although a norm constraint is used. This
problem is called the initialization problem. When the MKL framework is extended from the linear
combination cases to the nonlinear cases, the scaling problem becomes more serious, as even a ﬁnite
scaling of the learned kernel may not be generally guaranteed by a simple norm constraint on kernel
parameters for some kernel forms. These problems implicate that the margin itself is not enough to
measure the goodness of kernels.

2.2 Measuring the goodness of kernels with the radiuses of MEB

Now we need to ﬁnd a more reasonable way to measure the goodness of kernels. Below we in-
troduce the generalization error bounds for SVM and kernel learning, which inspire us to con-
pirical error, is bounded by pO(R2γ−2 )/n, where R is the radius of the minimum enclosing
sider the minimum enclosing ball to learn a kernel. For SVM with a ﬁxed kernel, it is well
known that the estimation error, which denotes the gap between the expected error and the em-
ball (MEB) of data in the feature space endowed with the kernel used. For SVM with a ker-
nel learned from a kernel family K, if we restrict that the radius of the minimum enclosing ball
in the feature space endowed with the learned kernel to be no larger than R, then the theoret-
q 8
ical results of Srebro and Ben-David [14] say: for any ﬁxed margin γ > 0 and any ﬁxed ra-
dius R > 0, with probability at least 1 − δ over a training set of size n, the estimation error is
γ 2 − log δ). Scalar dφ denotes
n (2 + dφ log 128en3R2
+ 256 R2
8R log 128nR2
γ 2 log enγ
no larger than
γ 2 dφ
the pseudodimension [14] of the kernel family K. For example, dφ of linear combination ker-
nels is no larger than the number of basis kernels, and dφ of the Gaussian kernels with a form of
k(θ) (xa , xb ) = e−θkxa−xb k2 is no larger than 1 (See [14] for more details). The above results clearly
state that the generalization error bounds for SVM with both ﬁxed kernels and learned kernels de-
pend on the ratio between the margin γ and the radius R of the minimum enclosing ball of data.
Although some new results of the generalization bounds for kernel learning, like [15], give different
types of dependencies on dφ , they also rely on the margin-and-radius ratio.
In SVM with a ﬁxed kernel, the radius R is a constant and we can safely minimize kwk2 (as well as
kernels by kunif = Pp
the empirical loss). However, in kernel learning, the radius R changes drastically from one kernel
to another (An example is given in the supplemental materials: when we uniformly combine p basis
p kj , the squared radius becomes only 1
p of the squared radius of each basis
1
j=1
kernel.). Thus we should also take the radius into account. As a result, we use the ratio between the
margin γ and the radius R to measure how good a kernel is for kernel learning.

3

Given any kernel k , the radius of the minimum enclosing ball, denoted by R(k), can be obtained by:
s.t. y ≥ kφ(xi ; k) − ck2 .
R2 (k) = miny ,c y ,
(7)
P
iβik(xi , xi ) − P
s.t. P
This problem is a convex minimization problem, being equivalent to its dual problem, as
i βi = 1, βi ≥ 0,
R2 (k) = maxβi
i,j βik(xi , xj )βj ,
(8)
which shows a property of R2 (k): for any kernel k and any scalar a > 0, we have R2 (ak) = aR2 (k).

3 Learning kernels with the radiuses
2 R2 (k)kwk2 + C P
Considering the ratio between the margin and the radius of MEB, we propose a new formulation, as
s.t. yi (hφ(xi ; k), wi + b) + ξi ≥ 1, ξi ≥ 0,
radius, and P
mink,w,b,ξi
(9)
1
i ξi ,
where R2 (k)kwk2 is a radius-based regularizer that prefers a large ratio between the margin and the
i ξi is the hinge loss which is an upper bound of empirical misclassiﬁed error. This
optimization problem is called radius based kernel learning problem, referred to as RKL.
Chapelle et al. [1] also utilize the radius of MEB to tune kernel parameters for hard margin SVM.
2 R2 (k(θ) )kwk2 + CR2 (k(θ) ) P
Our formulation (9) is equivalent to theirs if ξi is restricted to be zero. To give a soft margin version,
they modify the kernel matrix K (θ) = K (θ) + 1
C I , resulting in a formulation equivalent to:
i , s.t. yi (hφ(xi ; k(θ) ), wi + b) + ξi ≥ 1, ξi ≥ 0. (10)
min
1
i ξ 2
θ,w,b,ξi
The function R2 (k(θ) ) in the second term, which may become small, makes that minimizing the
√
objective function can not reliably give a small empirical loss, even when C is large. Besides, when
we reduce the scaling of a kernel by multiplying it with a small scalar a and substitute ˜w = w/
a
for w to keep the same ξi , the objective function always decreases (due to the decrease of R2 in
the empirical loss term), still leading to scaling problems. Do et al. [16] recently propose to learn a
P
i , s.t. yi (P
P
linear kernel combination, as deﬁned in (2), through
CP
kwj k2
j hwj , φ(xi ; kj )i + b) + ξi ≥ 1, ξi ≥ 0. (11)
min
i ξ 2
j θj R2 (kj )
j
θj
θ,wj ,b,ξi
Their objective function also can be always decreased by multiplying θ with a large scalar. Thus
their method does not address the scaling problem, also resulting in the initialization problem. If we
initially adjust the scalings of basis kernels to make each R(kj ) be equal to each other, then their for-
mulation is equivalent to the margin-based formulation (3). Different from the above formulations,
our formulation (9) is invariant to scalings of kernels.

+

1
2

3.1

(12)
(13)

Invariance to scalings of kernels
Now we discuss the properties of formulation (9). The RKL problem can be reformulated to
2 R2 (k)kwk2 + C P
mink G(k),
i ξi , s.t. yi (hφ(xi ; k), wi + b) + ξi ≥ 1, ξi ≥ 0.
where G(k) = min
1
w,b,ξi
Functional G(k) deﬁnes a measure of the goodness of kernel functions, which consider a trade-off
between the margin-and-radius ratio and the empirical loss. This functional is invariant to the scaling
of k , as stated by the following proposition.
Proposition 1. For any kernel k and any scalar a > 0, equation G(ak) = G(k) holds.
2 R2 (k)kwk2 + C P
Proof. For the scaled kernel ak , equation R2 (ak) = aR2 (k) holds. Thereby, we get
i ξi , s.t. yi (h√
aφ(xi ; k), wi + b) + ξi ≥ 1, ξi ≥ 0. (14)
G(ak) = minw,b,ξi
a
Let ˜w√
a = w replace w in (14), and then (14) becomes equivalent to (13). Thus G(ak) = G(k).
For a parametric kernel form k(θ) , the RKL problem transfers to minimizing a function g(θ) .=
G(k(θ) ). Here we temporarily focus on the linear combination case deﬁned by (2), and use glinear (θ)
to denote g(θ) in such case. Due to the scaling invariance, for any θ and any a > 0, we have
glinear (aθ) = glinear (θ). It makes the problem of minimizing glinear (θ) be invariant to the types of
norm constraints on θ , as stated in the following.

4

Proposition 2. Given any norm deﬁnition N (·) and any set S ⊆ R, suppose there exists c > 0
that satisﬁes c ∈ S . Let (a) denote the problem of minimizing glinear (θ) s.t. θi ≥ 0, and (b) denote
the problem of minimizing glinear (θ) s.t. θi ≥ 0 and N (θ) ∈ S . Then we have: (1) For any local
(global) optimal solution of (a), denoted by θa ,
cN (θa ) θa is also the local (global) optimal solution
of (b). (2) For any local (global) optimal solution of (b), denoted by θb , θb is also the local (global)
optimal solution of (a).

Proof. The complete proof is given in the the supplemental materials. Here we only prove the
equivalence of global optimal solutions of (a) and (b). On one hand, if θa is the global optimal
solution of (a), then for any θ that satisﬁes θi ≥ 0 and N (θ) ∈ S , we have glinear (
cN (θ) θa ) =
cN (θa ) θa ) = c ∈ S ,
glinear (θa ) ≤ g(θ). Due to N (
cN (θ) θa also satisﬁes the constraint of (b),
cN (θ) θa is the global optimal solution of (b). On the other hand, for any θ (θi ≥ 0),
and thus
glinear (
cN (θ) θ) = glinear (θ) due to the scaling invariance. If θb is the global optimal solution of (b),
cN (θ)θ satisﬁes the constraint of (b), we have glinear (θb ) ≤ glinear ( cN (θ) θ),
then for any θ (θi ≥ 0), as
giving glinear (θb ) ≤ glinear (θ). Thus θb is the global optimal solution of (a).
As the problems of minimizing glinear (θ) under different types of norm constraints on θ are all
equivalent to the same problem without any norm constraint, they are equivalent to each other. Based
on the above proposition, we can also get the another conclusion: in the linear combination case the
minimization problem (12) is also invariant to the initial scalings of basis kernels (see below).
the problem of minimizing G(P
Proposition 3. Let kj denote basis kernels, and aj > 0 be initial scaling coefﬁcients of basis kernels.
Give a norm constraint N (θ) ∈ S , which is by the same deﬁnition as in Proposition 2. Let (a) denote
with different initial scalings: minimizing G(P
j θj kj ) w.r.t. θ s.t. θi ≥ 0 and N (θ) ∈ S , and (b) denote the problem
j θj aj kj ) w.r.t. θ s.t. θi ≥ 0 and N (θ) ∈ S . Then:
(1) Problem (a) and problem (b) have the same local and global optimums. (2) For any local (global)
caj θb
optimal solution of (b), denoted by θb , [
t ]t ) ]j is also the local (global) optimal solution of (a).
j
N ([at θb
mizing G(P
problem (c) is equivalent to the problem of minimizing G(P
Proof. By proposition 2, problems (b) is equivalent to the one without any norm constraint: mini-
j θj aj kj ) w.r.t. θ s.t. θi ≥ 0, which is denoted by problem (c). Let ˜θj = aj θj , and then
˜θj kj ) w.r.t. ˜θ s.t. ˜θi ≥ 0, which is
j
denoted by problem (d) (local and global optimal solutions of problems (c) and (d) have one-to-one
correspondences due to the simple transform ˜θj = aj θj ). Again, by Proposition 2, problem (d) is
equivalent to the one with N (θ) ∈ S , which is indeed problem (a). So we have conclusion (1). By
proper transformations of optimal solutions of these equivalent problems, we get conclusion (2).

Note that in Proposition 3, optimal solutions of problems (a) and (b), which are with different initial
scalings of basis kernels, actually result in the same kernel combinations up to the scalings.
As shown in the above three propositions, our proposed formulation not only completely addresses
scaling and initialization problems, but also is not sensitive to the types of norm constraints used.

3.2 Reformulation to a tri-level optimization problem

The remaining task is to optimize the RKL problem (12). Given a parametric kernel form k(θ) , for
any parameter θ , to obtain the value of the objective function g(θ) = G(k(θ) ) in (12), we need to
solve the SVM-like problem in (13), which is a convex minimization problem and can be solved by
its dual problem. Indeed, the whole RKL problem is transformed to a tri-level optimization problem:
n
o
i,j αiαj yi yj Ki,j (θ), s.t. P
P
P
minθ g(θ),
(15)
n
o
i,j βiKi,j (θ)βj , s.t. P
P
iβiKi,j (θ) − P
iαi − 1
iαi yi = 0, 0 ≤ αi ≤ C
maxαi
,(16)
2r2 (θ)
iβi = 1, βi ≥ 0
maxβi
where r2 (θ) =
.
Notation K (θ) denotes the kernel matrix [k(θ) (xi , xj )]i,j . The above formulations show that given
any θ the calculation of a value of g(θ) requires solving a bi-level optimization problem. First, solve
the MEB dual problem (17), and obtain the optimal value r2 (θ) and the optimal solution, denoted by

where g(θ) =

(17)

5

β ∗
i . Then, take r2 (θ) into the objective function of the SVM dual problem (16), solve it, and obtain
the value of g(θ), as well as the optimal solution of (16), denoted by α∗
i . Unlike in other kernel
learning approaches, here the optimization of the SVM dual problem relies on another optimal value
function r2 (θ), making the RKL problem more challenging.
If g(θ), which is the objective function in the top-level optimization, is differentiable and we can get
its derivatives, then we can use a variety of gradient-based methods to solve the RKL problem. So in
next section, we study the differentiability of a general family of multilevel optimization problems.

4 Differentiability of the multilevel optimization problem
The Danskin’s theorem [17] states the differentiability of the optimal value of a single-level op-
timization problem, and has been applied in many MKL algorithms, e.g., [5, 12]. Unfortunately,
it is not directly applicable to the optimal value of a multilevel optimization problem. Below we
generalize the Danskin’s theorem and give new results about the multilevel optimization problem.
Let Y be a metric space, and X , U and Z be normed spaces. Suppose: (1) The function g1 (x, u, z ),
is continuous on X × U × Z . (2) For all x ∈ X the function g1 (x, ·, ·) is continuously differentiable.
(3) The function g2 (y , x, u) (g2 : Y × X × U → Z ) is continuous on Y × X × U . (4) For all y ∈ Y
the function g2 (y , ·, ·) is continuously differentiable. (5) Sets ΦX ⊆ X and ΦY ⊆ Y are compact.
By these notations, we propose the following theorem about bi-level optimal value functions.
Theorem 1. Let us deﬁne a bi-level optimal value function as
v1 (u) = inf x∈ΦX g1 (x, u, v2 (x, u)),
where v2 (x, u) is another optimal value function as
v2 (x, u) = inf y∈ΦY g2 (y , x, u).
(19)
If for any x and u, g2 (·, x, u) has a unique minimizer y∗ (x, u) over ΦY , then y∗ (x, u) are continuous
on X × U , and v1 (u) is directionally differentiable. Furthermore, if for any u, the g1 (·, u, v2 (·, u))
has also a unique minimizer x∗ (u) over ΦX , then
1. the minimizer x∗ (u) are continuous on U ,
(cid:17)(cid:12)(cid:12)(cid:12)v2=v2 (x∗,u)
(cid:16)∂ g1(x∗,u,v2 )
2. v1 (u) is continuously differentiable, and its derivative is equal to
, where ∂ v2 (x∗ ,u)
∂ g1(x∗,u,v2 )
+ ∂ v2(x∗ ,u)
dv1 (u)
du =
∂u
∂u
∂ v2
∂u
The proof is given in supplemental materials. To apply Theorem 1 to the objective function g(θ) in
the RKL problem (15), we shall make sure the following two conditions are satisﬁed. First, both
the MEB dual problem (17) and the SVM dual problem (16) must have unique optimal solutions.
This can be guaranteed by that the kernel matrix K (θ) is strictly positive deﬁnite. Second, the
kernel matrix K (θ) shall be continuously differentiable to θ . Both conditions can be met in the
linear combination case when each basis kernel matrix is strictly positive deﬁnite, and can also be
P
P
easily satisﬁed in nonlinear cases, like in [11, 12]. If these two conditions are met, then g(θ) is
continuously differentiable and
dθ = − 1
i α∗
i,j α∗
i α∗
i,j α∗
j yi yj Ki,j (θ) dr2 (θ)
dKi,j (θ)
dg(θ)
dθ + 1
j yi yj
2r2 (θ)
2r4 (θ)
dθ − P
dθ = P
dθ
where α∗
i is the optimal solution of the SVM dual problem (16), and
β ∗
i,j β ∗
iβ ∗
dr2 (θ)
dKi,j (θ)
dKi,i (θ)
(22)
j ,
i
i
dθ
where β ∗
is easy. For example, for the linear combination kernel Ki,j (θ) = P
i is the optimal solution of the MEB dual problem (17). In above equations, the value of
dKi,j (θ)
is needed. It depends on the speciﬁc form of the parametric kernels, and the deriving of it
dθ
i,j , we have ∂Ki,j (θ)
=
m θmK m
∂ θm
dθ = −Ki,j (θ)kxi − xj k2 .
i,j . For the Gaussian kernel Ki,j (θ) = e−θkxi−xj k2 , we have dKi,j (θ)
K m
5 Algorithm
With the derivative of g(θ), we use the standard gradient projection approach with the Armijo
rule [18] for selecting step sizes to address the RKL problem. To compare with the most popu-
lar kernel learning algorithm, simpleMKL [5], in experiments we employ the linear combination

= ∂ g2 (y∗ ,x∗ ,u)
∂u

. (20)

(18)

,

(21)

6

no norm constraint. The L1 and L2 norm constraints are as P
j θj = 1 and P
kernel form with nonnegative combination coefﬁcients, as deﬁned in (2). In addition, we also con-
sider three types of norm constraints on kernel parameters (combination coefﬁcients): L1 , L2 and
j = 1, respectively.
j θ2
The projection for the L1 norm and nonnegative constraints can be efﬁciently done by the method
of Duchi et al. [19]. The projection for only nonnegative constraints can be accomplished by set-
ting negative elements to be zero. The projection for the L2 norm and nonnegative constraints need
another step after eliminating negative values: normalize θ by multiplying it with kθk−1
2 .
In our gradient projection algorithm, each calculation of the objective functions g(θ) needs solving
an MEB problem (17) and an SVM problem (16), whereas the gradient calculation and projec-
tion steps have ignorable time complexity compared to MEB and SVM solvers. The MEB and
SVM problems have similar forms of objective functions and constraints, and both of them can be
efﬁciently solved by SMO algorithms. Moreover, previous solutions α∗
i and β ∗
i can be used as “hot-
start” to accelerate the solvers. It is because optimal solutions of two problems are continuous to
kernel parameter θ according to Theorem 1. Thus when θ moves a small step, the optimal solu-
tions also will only change a little. In real experiments our approach usually achieves approximate
convergence within one or two dozens of invocations of SVM and MEB solvers (For lack of space,
examples of the convergence speed of our algorithm are shown in the supplemental materials).
In linear combination cases, the RKL problem, as the radius-based formulation by Chapelle et al. [1],
is not convex. Gradient-based methods only guarantee local optimums. The following states the
nontrivial quality of local optimal solutions and their connections to related convex problems.
Proposition 4. In linear combination cases, for any local optimal solution of the RKL problem,
denoted by θ∗ , there exist C1 > 0 and C2 > 0 that θ∗ is the global optimal solution of the following
P
P
i , s.t. yi (P
convex problem:
j kwj k2 + C1 r2 (θ) + C2
j hwj , φ(xi ; θj kj )i+ b)+ ξi ≥ 1, ξi ≥ 0. (23)
i ξ 2
The proof can be found in the supplemental materials. The proposition also gives another possible
way to address the RKL problem: iteratively solve the convex problem (23) with a search for C1
and C2 . However, it is difﬁcult to ﬁnd exact values of C1 and C2 by a grid search, and even a rough
search will result in too high computational load. Besides, such method is also lack of extension
ability to nonlinear parametric kernel forms. Then, in the experiments, we demonstrate that the
gradient-based approach can give satisfactory performances, which are signiﬁcantly better than ones
of SVM with the uniform combination of basis kernels and of other kernel learning approaches.

min
θ,wj ,b,ξi

1
2

6 Experiments
In this section, we illustrate the performances of our presented RKL approach, in comparison with
SVM with the uniform combination of basis kernels (Unif), the margin-based MKL method using
formulation (3) (MKL), and the kernel learning principle by Chapelle et al. [1] using formulation
(10) (KL-C). The evaluation is made on eleven public available data sets from UCI repository [20]
and LIBSVM Data [21] (see Table 1). All data sets have been normalized to be zero-means and
unit-variances on every feature. The used basis kernels are the same as in SimpleMKL [5]: 10
Gaussian kernels with bandwidths γG ∈ {0.5, 1, 2, 5, 7, 10, 12, 15, 17, 20} and 10 polynomial ker-
nels of degree 1 to 10. All kernel matrices have been normalized to unit trace, as in [5, 7]. Note that
although our RKL formulation is theoretically invariant to the initial scalings, the normalization is
still applied in RKL to avoid numerical problems caused by large value kernel matrices in SVM and
MEB solvers. To show impacts of different norm constraints, we use three types of them: L1 , L2
and no norm constraint. With no norm constraint, only RKL can converge, and so only its results are
reported. The SVM toolbox used is LIBSVM [21]. MKL with the L1 norm constraint is solved by
the code from SimpleMKL [5]. Other problems are solved by standard gradient-projection methods,
where the calculation of gradients of the MKL formulation (3) and Chapelle’s formulation (10) is
20 e, where e is an all-ones vector.
the same as in [5] and [1], respectively. The initial θ is set to be 1
The trade-off coefﬁcients C in SVM, MKL, KL-C and RKL are automatically determined by
.=
3-fold cross-validations on training sets.
In all methods, C is selected from the set Scoef
{0.01, 0.1, 1, 10, 100}. For each data set, we split it to ﬁve parts, and each time we use four parts as
the training set and the remaining one as the test set. The average accuracies with standard deviations
and average numbers of selected basis kernels are reported in Table 1.

7

Table 1: The testing accuracies (Acc.) with standard deviations (in parentheses), and the average
numbers of selected basis kernels (Nk). We set the numbers of our method to be bold if our method
outperforms both Unif and other two kernel learning approaches under the same norm constraint.

Index

1
Unif

8
7
6
5
4
3
2
Ours
Ours
KL-C
MKL
Ours
KL-C
MKL
No
Constraint
L2
L2
L2
L1
L1
L1
Data set
Acc. Nk Acc. Nk Acc. Nk Acc. Nk Acc. Nk Acc. Nk Acc. Nk Acc. Nk
Ionosphere 94.0 (1.4) 20 92.9 (1.6) 3.8 86.0 (1.9) 4.0 95.7 (0.9) 2.8 94.3 (1.5) 20 84.4 (1.6) 18 95.7 (0.9) 3.0 95.7 (0.9) 3.0
51.7 (0.1) 20 79.5 (1.9) 1.0 80.5 (1.9) 2.8 86.5 (2.4) 3.2 82.0 (2.2) 20 74.0 (2.6) 14 86.5 (2.4) 2.2 86.3 (2.5) 3.2
Splice
58.0 (0.0) 20 59.1 (1.4) 4.2 62.9 (3.5) 4.0 64.1 (4.2) 3.6 67.0 (3.8) 20 64.1 (3.9) 11 64.1 (4.2) 8.0 64.3 (4.3) 6.6
Liver
81.2 (1.9) 20 97.7 (1.2) 7.0 94.0 (1.2) 2.0 100 (0.0) 1.0 97.3 (1.6) 20 94.0 (1.3) 17 100 (0.0) 1.0 100 (0.0) 1.6
Fourclass
83.7 (6.1) 20 84.1 (5.7) 7.4 83.3 (5.9) 1.8 84.1 (5.7) 5.2 83.7 (5.8) 20 83.3 (5.1) 19 84.4 (5.9) 5.4 84.8 (5.0) 5.8
Heart
Germannum 70.0 (0.0) 20 70.0 (0.0) 7.2 71.9 (1.8) 9.8 73.7 (1.6) 4.8 71.5 (0.8) 20 71.6 (2.1) 13 73.9 (1.2) 6.0 73.9 (1.8) 5.8
61.4 (2.9) 20 85.5 (2.9) 1.6 73.9 (2.9) 2.0 93.3 (2.3) 4.0 87.4 (3.0) 20 61.9 (3.1) 19 93.5 (2.2) 3.8 93.3 (2.3) 3.8
Musk1
94.4 (1.8) 20 97.0 (1.8) 1.2 97.4 (2.3) 4.6 97.4 (1.6) 6.2 96.8 (1.6) 20 97.4 (2.0) 11 97.6 (1.9) 5.8 97.6 (1.9) 5.8
Wdbc
76.5 (2.9) 20 76.5 (2.9) 7.2 52.2 (5.9) 9.6 76.5 (2.9) 17 75.9 (1.8) 20 51.0 (6.6) 17 76.5 (2.9) 15 76.5 (2.9) 15
Wpbc
76.5 (1.8) 20 82.3 (5.6) 2.6 80.8 (5.8) 7.4 86.0 (2.6) 2.6 85.2 (2.9) 20 80.2 (5.9) 11 86.0 (2.6) 2.6 86.0 (3.3) 3.0
Sonar
Coloncancer 67.2 (11) 20 82.6 (8.5) 13 74.5 (4.4) 11 84.2 (4.2) 7.2 76.5 (9.0) 20 76.0 (3.6) 15 84.2 (4.2) 5.6 84.2 (4.2) 7.6

The results in Table 1 can be summarized as follows. (a) RKL gives the best results on most sets.
Under L1 norm constraints, RKL (Index 4) outperforms all other methods (Index 1, 2, 3) on 8 out
of 11 sets, and also gives results equal to the best ones of other methods on the remaining 3 sets.
In particular, RKL gains 5 or more percents of accuracies on Splice, Liver and Musk1 over MKL,
and gains more than 9 percents on four sets over KL-C. Under L2 norm constraints, the results are
similar: RKL (Index 7) outperforms other methods (Index 5, 6) on 10 out of 11 sets, with only 1
inverse result. (b) Both MKL and KL-C are sensitive to the types of norm constraints (Compare
Index 2 and 5, as well as 3 and 6). As shown in recent literature [7, 9], for the MKL formulation,
different types of norm constraints ﬁt different data sets. However, RKL outperforms MKL (as well
as KL-C) under both L1 and L2 norm constraints on most sets. (c) RKL is invariant to the types
of norm constraints. See Index 4, 7 and 8. Most accuracy numbers of them are the same. Several
exceptions with slight differences are possibly due to precisions of numerical computation. (d) For
MKL, the L1 norm constraint always results in sparse combinations, whereas the L2 norm constraint
always gives non-sparse results (see Index 2 and 5). (e) An interesting thing is that, our presented
RKL gives sparse solutions on most sets, whatever types of norm constraints are used. As there usu-
ally exist redundancies in the basis kernels, the searching for good kernels and small empirical loss
often directly leads to sparse solutions. We notice that KL-C under L2 norm constraints also slightly
promotes sparsity (Index 6). Compared to KL-C under L2 norm constraints, RKL provides not only
higher performances but also more sparsity, which beneﬁts both interpretability and computational
efﬁciency in prediction.

7 Conclusion

In this paper, we show that the margin term used in previous MKL formulations is not a suitable
measure of the goodness of kernels, resulting in scaling and initialization problems. We propose
a new formulation, called RKL, which uses the ratio between the margin and the radius of MEB
to learn kernels. We prove that our formulation is invariant to kernel scalings, and also invariant
to scalings of basis kernels and to the types of norm constraints for the MKL problem. Then,
by establishing the differentiability of a general family of multilevel optimal value functions, we
propose a gradient-based algorithm to address the RKL problem. We also provide the property of
solutions of our algorithm. The experiments validate that our approach outperforms both SVM with
the uniform combination of basis kernels and other state-of-art kernel learning methods.

Acknowledgments

The work is supported by the National Natural Science Foundation of China (NSFC) (Grant
Nos. 60835002 and 61075004) and the National Basic Research Program (973 Program) (No.
2009CB320602).

8

References
[1] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector
machines. Machine Learning, 46(1):131–159, 2002.
[2] G.R.G. Lanckriet, N. Cristianini, P. Bartlett, L.E. Ghaoui, and M.I. Jordan. Learning the kernel matrix
with semideﬁnite programming. The Journal of Machine Learning Research, 5:27–72, 2004.
[3] F.R. Bach, G.R.G. Lanckriet, and M.I. Jordan. Multiple kernel learning, conic duality, and the smo
algorithm. In Proceedings of the twenty-ﬁrst international conference on Machine learning (ICML 2004),
2004.
[4] S. Sonnenburg, G. R ¨atsch, and C. Sch ¨afer. A general and efﬁcient multiple kernel learning algorithm. In
Adv. Neural. Inform. Process Syst. (NIPS 2005), 2006.
[5] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. Journal of Machine Learning
Research, 9:2491–2521, 2008.
[6] O. Chapelle and A. Rakotomamonjy. Second order optimization of kernel parameters. In Proc. of the
NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, 2008.
[7] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K. M ¨uller, and A. Zien. Efﬁcient and Accurate lp-Norm
Multiple Kernel Learning. In Adv. Neural. Inform. Process Syst. (NIPS 2009), 2009.
[8] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In Uncertainty in
Artiﬁcial Intelligence, 2009.
[9] J. Saketha Nath, G. Dinesh, S. Raman, Chiranjib Bhattacharyya, Aharon Ben-Tal, and K. R. Ramakrish-
nan. On the algorithmics and applications of a mixed-norm based kernel learning formulation. In Adv.
Neural. Inform. Process Syst. (NIPS 2009), 2009.
[10] F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Adv. Neural. Inform.
Process Syst. (NIPS 2008), 2008.
[11] M. G ¨onen and E. Alpaydin. Localized multiple kernel learning. In Proceedings of the 25th international
conference on Machine learning (ICML 2008), 2008.
[12] M. Varma and B.R. Babu. More generality in efﬁcient multiple kernel learning. In Proceedings of the
26th International Conference on Machine Learning (ICML 2009), 2009.
[13] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning Non-Linear Combinations of Kernels.
Neural. Inform. Process Syst. (NIPS 2009), 2009.
[14] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned kernels.
In
Proceedings of the International Conference on Learning Theory (COLT 2006), pages 169–183. Springer,
2006.
[15] Yiming Ying and Colin Campbell. Generalization bounds for learning the kernel. In Proceedings of the
International Conference on Learning Theory (COLT 2009), 2009.
[16] H. Do, A. Kalousis, A. Woznica, and M. Hilario. Margin and Radius Based Multiple Kernel Learning. In
Proceedings of the European Conference on Machine Learning (ECML 2009), 2009.
[17] J.M. Danskin. The theory of max-min, with applications. SIAM Journal on Applied Mathematics, pages
641–664, 1966.
[18] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, September 1999.
[19] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the l1-
ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine
learning (ICML 2008), 2008.
[20] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
http://www.ics.uci.edu/∼mlearn/MLRepository.html.
[21] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

Software available at

In Adv.

9


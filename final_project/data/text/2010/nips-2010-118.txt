Non-Stochastic Bandit Slate Problems

Satyen Kale
Yahoo! Research
Santa Clara, CA
skale@yahoo-inc.com

Lev Reyzin∗
Georgia Inst. of Technology
Atlanta, GA
lreyzin@cc.gatech.edu

Robert E. Schapire†
Princeton University
Princeton, NJ
schapire@cs.princeton.edu

Abstract

We consider bandit problems, motivated by applications in online advertising and
news story selection, in which the learner must repeatedly select a slate, that is,
a subset of size s from K possible actions, and then receives rewards for just the
selected actions. The goal is to minimize the regret with respect to total reward of
√
the best slate computed in hindsight. We consider unordered and ordered versions
of the problem, and give efﬁcient algorithms which have regret O(
T ), where the
constant depends on the speciﬁc nature of the problem. We also consider versions
√
of the problem where we have access to a number of policies which make recom-
mendations for slates in every round, and give algorithms with O(
T ) regret for
competing with the best such policy as well. We make use of the technique of
relative entropy projections combined with the usual multiplicative weight update
algorithm to obtain our algorithms.

1

Introduction

In traditional bandit models, the learner is presented with a set of K actions. On each of T rounds,
an adversary (or the world) ﬁrst chooses rewards for each action, and afterwards the learner decides
which action it wants to take. The learner then receives the reward of its chosen action, but does
In the standard bandit setting, the learner’s goal is to
not see the rewards of the other actions.
compete with the best ﬁxed arm in hindsight.
In the more general “experts setting,” each of N
experts recommends an arm on each round, and the goal of the learner is to perform as well as the
best expert in hindsight.
The bandit setting tackles many problems where a learner’s decisions reﬂect not only how well
it performs but also the data it learns from — a good algorithm will balance exploiting actions
it already knows to be good and exploring actions for which its estimates are less certain. One
such real-world problem appears in computational advertising, where publishers try to present their
customers with relevant advertisements. In this setting, the actions correspond to advertisements,
and choosing an action means displaying the corresponding ad. The rewards correspond to the
payments from the advertiser to the publisher, and these rewards depend on the probability of users
clicking on the ads.
Unfortunately, many real-world problems, including the computational advertising problem, do not
ﬁt so nicely into the traditional bandit framework. Most of the time, advertisers have the ability to
display more than one ad to users, and users can click on more than one of the ads displayed to
them. To capture this reality, in this paper we deﬁne the slate problem. This setting is similar to the
traditional bandit setting, except that here the advertiser selects a slate, or subset, of S actions.
In this paper we ﬁrst consider the unordered slate problem, where the reward to the learning algo-
rithm is the sum of the rewards of the chosen actions in the slate. This setting is applicable when all
∗This work was done while Lev Reyzin was at Yahoo! Research, New York. This material is based upon
work supported by the National Science Foundation under Grant #0937060 to the Computing Research Asso-
ciation for the Computing Innovation Fellowship program.
†This work was done while R. Schapire was visiting Yahoo! Research, New York.

1

actions in a slate are treated equally. While this is a realistic assumption in certain settings, we also
deal with the case when different positions in a slate have different importance. Going back to our
computational advertising example, we can see not all ads are given the same treatment (i.e. an ad
displayed higher in a list is more likely to be clicked on). One may plausibly assume that for every
ad and every position that it can be shown in, there is a click-through-rate associated with the (ad,
position) pair, which speciﬁes the probability that a user will click on the ad if it is displayed in that
position. This is a very general user model used widely in practice in web search engines. To ab-
stract this, we turn to the ordered slate problem, where for each action and position in the ordering,
the adversary speciﬁes a reward for using the action in that position. The reward to the learner then
is the sum of the rewards of the (actions, position) pairs in the chosen ordered slate.1 This setting
is similar to that of Gy ¨orgy, Linder, Lugosi and Ottucs ´ak [10] in that the cost of all actions in the
chosen slate are revealed, rather than just the total cost of the slate.
Finally, we show how to tackle these problems in the experts setting, where instead of competing
with the best slate in hindsight, the algorithm competes with the best expert, recommending different
slates on different rounds.
One key idea appearing in our algorithms is to use a variant of the multiplicative weights expert
algorithm for a restricted convex set of distributions. In our case, the restricted set of distributions
over actions corresponds to the one deﬁned by the stipulation that the learner choose a slate instead
of individual actions. Our variant ﬁrst ﬁnds the distribution generated by multiplicative weights and
then chooses the closest distribution in the restricted subset using relative entropy as the distance
metric — this is a type of Bregman projection, which has certain nice properties for our analysis.
Previous Work. The multi-armed bandit problem, ﬁrst studied by Lai and Robbins [15], is a
O(K ln(T )). In the non-stochastic setting, Auer et al. [3] gave regret bounds of O((cid:112)K ln(K )T ).2
classic problem which has had wide application. In the stochastic setting, where the rewards of the
arms are i.i.d., Lai and Robbins [15] and Auer, Cesa-Bianchi and Fischer [2] gave regret bounds of
This non-stochastic setting of the multi-armed bandit problem is exactly the speciﬁc case of our
problem when the slate size is 1, and hence our results generalize those of Auer et al. which can be
recovered by setting s = 1.
bounds of O((cid:112)T log(T )). The constant in the O(·) notation is also worse than our bounds. For a
Our problem is a special case of the more general online linear optimization with bandit feedback
problem [1, 4, 5, 11]. Specializing the best result in this series to our setting, we get worse regret
more speciﬁc comparison of regret bounds, see Section 2. Our algorithms, being specialized for the
slates problem, are simpler to implement as well, avoiding the sophisticated self-concordant barrier
techniques of [1].
This work also builds upon the algorithm in [18] to learn subsets of experts and the algorithm in [12]
for learning permutations, both in the full information setting. Our work is also a special case of
the Combinatorial Bandits setting of Cesa-Bianchi and Lugosi [9]; however, our algorithms obtain
better regret bounds and are computationally more efﬁcient.
Our multiplicative weights algorithm also appears under the name Component Hedge in the inde-
pendent work of Koolen, Warmuth and Kivinen [14]. Furthermore, the expertless, unordered slate
problem is studied by Uchiya, Nakamura and Kudo [17] who obtain the same asymptotic bounds as
appear in this paper, though using different techniques.
For vectors x, y ∈ RK , x · y denotes their inner product, viz. (cid:80)
2 Statement of the problem and main results
Notation.
i xi yi . For ma-
trices X, Y ∈ Rs×K , X • Y denotes their inner product considering them vectors in RsK , viz.

1The unordered slate problem is a special case of the ordered slate problem for which all positional fac-
√
tors are equal. However, the bound on the regret that we get when we consider the unordered slate problem
separately is a factor of ˜O(
s) better than when we treat it as a special case of the ordered slate problem.
2The difference in the regret bounds can be attributed to the deﬁnition of regret in the stochastic and non-
stochastic settings. In the stochastic setting, we compare the algorithm’s expected reward to that of the arm
with the largest expected reward, with the expectation taken over the reward distribution.

2

RegretT =

(cid:96)j (t).

(cid:96)j (t) − min
S

(cid:80)
p and q, let RE(p (cid:107) q) denote their relative entropy, i.e. RE(p (cid:107) q) = (cid:80)
ij Xij Yij . For a set S of actions, let 1S be the indicator vector for that set. For two distributions
i pi ln( pi
).
qi
Problem Statement. In a sequence of rounds, for t = 1, 2, . . . , T , we are required to choose a slate
from a base set A of K actions. An unordered slate is a subset S ⊆ A of s out of the K actions. An
ordered slate is a slate together with an ordering over its s actions; thus, it is a one-to-one mapping
π : {1, 2, . . . , s} → A. Prior to the selection of the slate, the adversary chooses losses3 for the
actions in the slates. Once the slate is chosen, the cost of only the actions in the chosen slate is
revealed. This cost is deﬁned in the following manner:
j ∈ S are revealed, and the cost incurred for choosing S is (cid:80)
• Unordered slate. The adversary chooses a loss vector (cid:96)(t) ∈ RK which speciﬁes a loss
(cid:96)j (t) ∈ [−1, 1] for every action j ∈ A. For a chosen slate S , only the coordinates (cid:96)j (t) for
j∈S (cid:96)j (t).
• Ordered slate. The adversary chooses a loss matrix L(t) ∈ Rs×K which speciﬁes a loss
Lij (t) ∈ [−1, 1] for every action j ∈ A and every position i, 1 ≤ i ≤ s, in the ordering on
the cost incurred for choosing π is (cid:80)s
the slate. For a chosen slate π , the entries Li,π(i) (t) for every position i are revealed, and
i=1 Li,π(i) (t).
In the unordered slate problem, if slate S (t) is chosen in round t, for t = 1, 2, . . . , T , then the regret
T(cid:88)
(cid:88)
T(cid:88)
(cid:88)
of the algorithm is deﬁned to be
j∈S
j∈S (t)
t=1
t=1
Here, the subscript S is used as a shorthand for ranging over all slates S . The regret for the ordered
slate problem is deﬁned analogously.
Our goal is to design a randomized algorithm for online slate selection such that E[RegretT ] = o(T ),
where the expectation is taken over the internal randomization of the algorithm.
Competing with policies. Frequently in applications we have access to N policies which are algo-
rithms that recommend slates to use in every round. These policies might leverage extra information
that we have about the losses in the next round. It is therefore beneﬁcial to devise algorithms that
(cid:88)
T(cid:88)
T(cid:88)
(cid:88)
have low regret with respect to the best policy in the pool in hindsight, where regret is deﬁned as:
j∈Sρ (t)
j∈S (t)
t=1
t=1
Here, ρ ranges over all policies, Sρ (t) is the recommendation of policy ρ at time t, and S (t) is the
algorithm’s chosen slate. The regret is deﬁned analogously for ordered slates. More generally, we
may allow policies to recommend distributions over slates, and our goal is to minimize the expected
regret with respect to the best policy in hindsight, where the expectation is taken over the distribution
recommended by the policy as well as the internal randomization of the algorithm.
Our results. We are now able to formally state our main results:
Theorem 2.1. There are efﬁcient (running in poly(s, K ) time in the no-policies case, and in
4(cid:112)sK ln(K/s)T
4s(cid:112)K ln(K )T
poly(s, K, N ) time with N policies) randomized algorithms achieving the following regret bounds:
4(cid:112)sK ln(N )T
4s(cid:112)K ln(N )T
Ordered slates
Unordered slates
(Sec. 3.3)
(Sec. 3.2)
No policies
N policies
(Sec. 4.1)
(Sec. 4.2)
and [9] are O((cid:112)s3K ln(K/s)T ) in the unordered slates problem, and O(s2(cid:112)K ln(K )T ) in the
To compare, the best bounds obtained for the no-policies case using the more general algorithms [1]
√
ordered slates problem. It is also possible, in the no-policies setting, to devise algorithms that have
regret bounded by O(
T ) with high probability, using the upper conﬁdence bounds technique of [3].
We omit these algorithms in this paper for the sake of brevity.

RegretT =

(cid:96)j (t) − min
ρ

(cid:96)j (t).

3Note that we switch to losses rather than rewards to be consistent with most recent literature on online
learning. Since we allow negative losses, we can easily deal with rewards as well.

3

Algorithm MW(P )
Initialization: An arbitrary probability distribution p(1) ∈ P on the experts, and some η > 0.
For t = 1, 2, . . . , T :
1. Choose distribution p(t) over experts, and observe the cost vector (cid:96)(t).
2. Compute the probability vector ˆp(t + 1) using the following multiplicative update rule:
where Z (t) = (cid:80)
for every expert i,
ˆpi (t + 1) = pi (t) exp(−η(cid:96)i (t))/Z (t)
i pi (t) exp(−η(cid:96)i (t)) is the normalization factor.
3. Set p(t + 1) to be the projection of ˆp(t + 1) on the set P using the RE as a distance
function, i.e. p(t + 1) = arg minp∈P RE(p (cid:107) ˆp(t + 1)).
Figure 1: The Multiplicative Weights Algorithm with Restricted Distributions

(1)

3 Algorithms for the slate problems with no policies

3.1 Main algorithmic ideas

Our starting point is the Hedge algorithm for learning online with expert advice. In this setting, on
each round t, the learner chooses a probability distribution p(t) over experts, each of which then
suffers a (fully observable) loss represented by the vector (cid:96)(t). The learner’s loss is then p(t) · (cid:96)(t).
The main idea of our approach is to apply Hedge (and ideas from bandit variants of it, especially
Exp3 [3]) by associating the probability distributions that it selects with mixtures of (ordered or
unordered) slates, and thus with the randomized choice of a slate. However, this requires that the
selected probability distributions have a particular form, which we describe shortly. We therefore
need a special variant of Hedge which uses only distributions p(t) from some ﬁxed convex subset
P of the simplex of all distributions. The goal then is to minimize regret relative to an arbitrary
distribution p ∈ P . Such a version of Hedge is given in Figure 1, and a statement of its performance
below. This algorithm is implicit in the work of [13, 18].
Theorem 3.1. Assume that η > 0 is chosen so that for all t and i, η(cid:96)i (t) ≥ −1. Then algorithm
MW(P ) generates distributions p(1), . . . , p(T ) ∈ P , such that for any p ∈ P ,
T(cid:88)
T(cid:88)
RE(p (cid:107) p(1))
η
t=1
t=1

(cid:96)(t) · p(t) − (cid:96)(t) · p ≤ η

((cid:96)(t))2 · p(t) +

.

Here, ((cid:96)(t))2 is the vector that is the coordinate-wise square of (cid:96)(t).

3.2 Unordered slates with no policies

To apply the approach described above, we need a way to compactly represent the set of distributions
over slates. We do this by embedding slates as points in some high-dimensional Euclidean space,
and then giving a compact representation of the convex hull of the embedded points. Speciﬁcally, we
represent an unordered slate S by its indicator vector 1S ∈ RK , which is 1 for all coordinates j ∈ S ,
convex polytope deﬁned by the linear constraints (cid:80)K
and 0 for all others. The convex hull X of all such 1S vectors can be succinctly described [18] as the
j=1 xj = s and xj ≥ 0 for j = 1, . . . , K . An
algorithm is given in [18] (Algorithm 2) to decompose any vector x ∈ X into a convex combination
of at most K indicator vectors 1S . We embed the convex hull X of all the 1S vectors in the simplex
of distributions over the K actions simply by scaling down all coordinates by s so that they sum to
1. Let P be this scaled down version of X . Our algorithm is given in Figure 2.
Step 3 of MW(P ) requires us to compute the arg minp∈P RE(p (cid:107) ˆp(t + 1)), which can be solved by
convex programming. A linear time algorithm is given in [13], and a simple algorithm (from [18])
is the following: ﬁnd the least index k such that clipping the largest k coordinates of p to 1
s and
rescaling the rest of the coordinates to sum up to 1 − k
s ensures that all coordinates are at most 1
s ,
and output the probability vector thus obtained. This can be implemented by sorting the coordinates,
and so it takes O(K log(K )) time.

4

Bandit Algorithm for Unordered Slates
(cid:113) (K/s) ln(K/s)
(cid:113) (1−γ )s ln(K/s)
Initialization: Start an instance of MW(P ) with the uniform initial distribution p(1) = 1
K 1. Set
, and γ =
η =
.
K T
T
For t = 1, 2, . . . , T :
1. Obtain the distribution p(t) from MW(P ).
2. Set p(cid:48) (t) = (1 − γ )p(t) + γ
corresponding to slates S as sp(cid:48) (t) = (cid:80)
S qS 1S , where qS > 0 and (cid:80)
K 1A .
3. Note that p(cid:48) (t) ∈ P . Decompose sp(cid:48) (t) as a convex combination of slate vectors 1S
S qS = 1.
4. Choose a slate S to display with probability qS , and obtain the loss (cid:96)j (t) for all j ∈ S .
j (t)) if j ∈ S , and 0 otherwise.
5. Set ˆ(cid:96)j (t) = (cid:96)j (t)/(sp(cid:48)
6. Send ˆ(cid:96)(t) as the loss vector to MW(P ).
Figure 2: The Bandit Algorithm with Unordered Slates

1S (cid:63) ≤ η

We now prove the regret bound of Theorem 2.1. We use the notation Et [X ] to denote the expectation
Et [ ˆ(cid:96)j (t)] = (cid:80)
j (t) = (cid:80)
of a random variable X conditioned on all the randomness chosen by the algorithm up to round
t, assuming that X is measurable with respect to this randomness. We note the following facts:
S(cid:51)j qS · (cid:96)j (t)
S(cid:51)j qS · 1
j (t) = (cid:96)j (t), since p(cid:48)
s . This immediately implies that
sp(cid:48)
Et [ˆ(cid:96)(t) · p(t)] = (cid:96)(t) · p(t) and E[ˆ(cid:96)(t) · p] = (cid:96)(t) · p, for any ﬁxed distribution p.
Note that if we decompose a distribution p ∈ P as a convex combination of 1
s 1S vectors and ran-
domly choose a slate S according to its weight in the combination, then the expected loss, averaged
over the s actions chosen, is (cid:96)(t) · p. We can bound the difference between the expected loss (aver-
j (t) − pj (t)) ≤ (cid:88)
(cid:96)(t) · p(cid:48) (t) − (cid:96)(t) · p(t) = (cid:88)
aged over the s actions) in round t suffered by the algorithm, (cid:96)(t) · p(cid:48) (t), and (cid:96)(t) · p(t) as follows:
(cid:96)j (t) · γ
≤ γ .
(cid:96)j (t)(p(cid:48)
(cid:80)
K
j
j
t (cid:96)(t) · 1
= (cid:88)
(cid:88)
Using this bound and Theorem 3.1, if S (cid:63) = arg minS
s 1S , we have
s 1S (cid:63) (cid:107) p(1))
E[RegretT ]
RE( 1
(cid:96)(t) · p(cid:48) (t) − (cid:96)(t) · 1
E[(ˆ(cid:96)(t))2 · p(t)] +
s
η
s
t
t
We note that the leading factor of 1
s on the expected regret is due to the averaging over the s positions.

(cid:88)
We now bound the terms on the RHS. First, we have
[(ˆ(cid:96)(t))2 · p(t)] = (cid:88)
((cid:96)j (t))2pj (t)
E
(cid:35)
(sp(cid:48)
= (cid:88)
qS = (cid:88)
· (cid:88)
j (t))2
j∈S
t
S
((cid:96)j (t))2pj (t)
((cid:96)j (t))2pj (t)
(sp(cid:48)
(sp(cid:48)
j (t))2
j (t))2
S(cid:51)j
j
j
1−γ , and all |(cid:96)j (t)| ≤ 1.
j (t) ≤ 1
because pj (t)
+ sγT ≤ 4(cid:112)sK ln(K/s)T ,
p(cid:48)
+ s ln(K/s)
E[RegretT ] ≤ η
(cid:113) (K/s) ln(K/s)
(cid:113) (1−γ )s ln(K/s)
η
and γ =
by setting η =
.
(cid:113) (1−γ )s ln(K/s)
T
K T
It remains to verify that η ˆ(cid:96)j (t) ≥ −1 for all i and t. We know that ˆ(cid:96)j (t) ≥ − K
j (t) ≥ γ
sγ , because p(cid:48)
K ,
≤ sγ
K , which is true for our choice of γ .
so all we need to check is that
K T

K
s(1 − γ ) ,

qS ·
(cid:34)

(cid:34)

(cid:35)

· sp(cid:48)
j (t) ≤

+ γT .

K T
1 − γ

5

Bandit Algorithm for Ordered Slates
(cid:113) K ln(K )
(cid:113) (1−γ ) ln(K )
Initialization: Start an instance of MW(P ) with the uniform initial distribution p(1) = 1
sK 1. Set
. For t = 1, 2, . . . , T :
and γ =
η =
K T
T
1. Obtain the distribution p(t) from MW(P ).
2. Set p(cid:48) (t) = (1 − γ )p(t) + γ
of Mπ matrices corresponding to ordered slates π as sp(cid:48) (t) = (cid:80)
sK 1A .
and (cid:80)
3. Note that p(cid:48) (t) ∈ P , and so sp(cid:48) (t) ∈ M. Decompose sp(cid:48) (t) as a convex combination
π qπMπ , where qπ > 0
π qπ = 1.
4. Choose a slate π to display w.p. qπ , and obtain the loss Li,π(i) (t) for all 1 ≤ i ≤ s.
5. Construct the loss matrix ˆL(t) as follows: for 1 ≤ i ≤ s, set ˆLi,π(i) (t) = Li,π(i) (t)
i,π(i) (t) , and
sp(cid:48)
all other entries are 0.
6. Send ˆL(t) as the loss vector to MW(P ).
Figure 3: Bandit Algorithm for Ordered Slates

3.3 Ordered slates with no policies
A similar approach can be used for ordered slates. Here, we represent an ordered slate π by the
subpermutation matrix Mπ ∈ Rs×K which is deﬁned as follows: for i = 1, 2, . . . , s, we have
matrices is the convex polytope deﬁned by the linear constraints: (cid:80)K
i,π(i) = 1, and all other entries are 0. In [7, 16], it is shown that the convex hull M of all the Mπ
(cid:80)s
M π
j=1 Mij = 1 for i = 1, . . . , s;
i=1 Mij ≤ 1 for j = 1, . . . , K ; and Mij ≥ 0 for i = 1, . . . , s and j = 1, . . . , K . Clearly, all
subpermutation matrices Mπ ∈ M. To complete the characterization of the convex hull, we can
show (details omitted) that given any matrix M ∈ M, we can efﬁciently decompose it into a convex
combination of at most K 2 subpermutation matrices.
We identify matrices in Rs×K with vectors in RsK in the obvious way. We embed M in the simplex
of distributions in RsK simply by scaling all the entries down by s so that their sum equals one. Let
P be this scaled down version of M. Our algorithm is given in Figure 3.
The projection in step 3 of MW(P ) can be computed simply by solving the convex program.
In
practice, however, noticing that the relative entropy projection is a Bregman projection, the cyclic
projections method of Bregman [6, 8] is likely to work faster. Adapted to the speciﬁc problem at
hand, this method works as follows (see [8] for details): ﬁrst, for every column j , initialize a dual
variable λj = 1. Then, alternate between row phases and column phases. In a row phase, iterate over
s . The column phase is a little more complicated:
all rows, and rescale them to make them sum to 1
s . Set α(cid:48) = min{λj , α},
ﬁrst, for every column j , compute the scaling factor α to make it sum to 1
and scale the column by α(cid:48) , and update λj ← λj /α(cid:48) . Repeat these alternating row and column
The regret bound analysis is similar to that of Section 3.2. We have Et [ ˆLij (t)] = (cid:80)
phases until convergence to within the desired tolerance.
π :π(i)=j qπ ·
= Lij (t), and hence Et [ ˆL(t) • p(t)] = L(t) • p(t) and E[ ˆL(t) • p] = L(t) • p. We can show
Lij (t)
sp(cid:48)
(cid:80)
also that L(t) • p(cid:48) (t) − L(t) • p(t) ≤ γ .
ij
t L(t) • 1
Using this bound and Theorem 3.1, if π(cid:63) = arg minπ
s Mπ , we have
(cid:88)
= (cid:88)
s Mπ(cid:63) (cid:107)p(1))
E[RegretT ]
RE( 1
L(t) •p(cid:48) (t)−L(t) • 1
E[( ˆL(t))2 •p(t)]+
s
η
s
t
t
(cid:34) s(cid:88)
(cid:34)
(cid:35)
[( ˆL(t))2 • p(t)] = (cid:88)
We now bound the terms on the RHS. First, we have
E
t
π
i=1

(Li,π(i) (t))2pi,π(i) (t)
(sp(cid:48)
i,π(i) (t))2

· (cid:88)
qπ
π :π(i)=j

(Lij (t))2pij (t)
(sp(cid:48)
ij (t))2

s(cid:88)
i=1

K(cid:88)
j=1

=

+ γT .

(cid:35)

Mπ(cid:63) ≤ η

6

qπ ·

=

(Lij (t))2pij (t)
(sp(cid:48)
ij (t))2

ij (t) ≤ K
· sp(cid:48)
1 − γ

,

(cid:113) (K/s) ln(N )
(cid:113) (1−γ )s ln(N )
Bandit Algorithm for Unordered Slates With Policies
Initialization: Start an instance of MW with no restrictions over the set of distributions over the N
N 1. Set η =
, and γ =
policies, with the initial distribution r(1) = 1
.
K T
T
For t = 1, 2, . . . , T :
2. Compute the distribution p(t) = (cid:80)N
1. Obtain the distribution over policies r(t) from MW, and the recommended distribution
over slates φρ (t) ∈ P for each policy ρ.
ρ=1 rρ (t)φρ (t).
3. Set p(cid:48) (t) = (1 − γ )p(t) + γ
corresponding to slates S as sp(cid:48) (t) = (cid:80)
S qS 1S , where qS > 0 and (cid:80)
K 1.
4. Note that p(cid:48) (t) ∈ P . Decompose sp(cid:48) (t) as a convex combination of slate vectors 1S
S qS = 1.
5. Choose a slate S to display with probability qS , and obtain the loss (cid:96)j (t) for all j ∈ S .
j (t) if j ∈ S , and 0 otherwise.
6. Set ˆ(cid:96)j (t) = (cid:96)j (t)/sp(cid:48)
7. Set the loss of policy ρ to be λρ (t) = ˆ(cid:96)(t) · φρ (t) in the MW algorithm.
Figure 4: Bandit Algorithm for Unordered Slates With Policies
(cid:35)
(cid:34)
K(cid:88)
s(cid:88)
j=1
i=1
1−γ , all |Lij (t)| ≤ 1.
ij (t) ≤ 1
because pij (t)
p(cid:48)
s Mπ(cid:63) (cid:107) p(1)) = ln(K ). Plugging these bounds into the bound of Theo-
Finally, we have RE( 1
+ sγT ≤ 4s(cid:112)K ln(K )T ,
rem 3.1, we get the stated regret bound from Theorem 2.1:
+ s ln(K )
E[RegretT ] ≤ η
sK T
(cid:113) (1−γ ) ln(K )
(cid:113) K ln(K )
1 − γ
η
, which satisfy the necessary technical conditions.
T
K T
4 Competing with a set of policies
4.1 Unordered Slates with N Policies
In each round, every policy ρ recommends a distribution over slates φρ (t) ∈ P , where P is the X
scaled down by s as in Section 3.2. Our algorithm is given in Figure 4.
(cid:80)
j (t) = (cid:96)j (t). Thus, Et [λρ (t)] = (cid:96)(t) · φρ (t), and hence Et [λ(t) · r(t)] = (cid:80)
Again the regret bound analysis is along the lines of Section 3.2. We have for any j , Et [ ˆ(cid:96)j (t)] =
S(cid:51)j qS · (cid:96)j (t)
ρ ((cid:96)(t) ·
sp(cid:48)
(cid:80)
φρ (t))rρ (t) = (cid:96)(t) · p(t). We can also show as before that (cid:96)(t) · p(cid:48) (t) − (cid:96)(t) · p(t) ≤ γ .
t (cid:96)(t) · φρ (t), we have
(cid:88)
= (cid:88)
Using this bound and Theorem 3.1, if ρ(cid:63) = arg minρ
RE(eρ(cid:63) (cid:107)r(1))
E[RegretT ]
E[(λ(t))2 · r(t)] +
(cid:96)(t) · p(cid:48) (t) − (cid:96)(t) · φρ(cid:63) (t) ≤ η
s
η
t
t
where eρ(cid:63) is the distribution (vector) that is concentrated entirely on policy ρ(cid:63) .
(cid:35)
(cid:34)(cid:88)
(cid:34)(cid:88)
(cid:35)
We now bound the terms on the RHS. First, we have
( ˆ(cid:96)(t) · φρ (t))2 rρ (t)
(cid:34)(cid:88)
(cid:35)
ρ
ρ
((ˆ(cid:96)(t))2 · φρ (t))rρ (t)
ρ

[(ˆ(cid:96)(t))2 · p(t)] ≤
= E
t

[(λ(t))2 · r(t)] = E
E
t
t

by setting η =

and γ =

K
s(1 − γ ) .

+ γT ,

λρ (t)2 rρ (t)

= E
t

≤ E
t

7

(cid:113) K ln(N )
(cid:113) (1−γ ) ln(N )
Bandit Algorithm for Ordered Slates with Policies
Initialization: Start an instance of MW with no restrictions, over the set of distributions over the N
N 1. Set η =
and γ =
policies, starting with r(1) = 1
.
K T
T
For t = 1, 2, . . . , T :
2. Compute the distribution p(t) = (cid:80)N
1. Obtain the distribution over policies r(t) from MW, and the recommended distribution
over ordered slates φρ (t) ∈ P for each policy ρ.
ρ=1 rρ (t)φρ (t).
3. Set p(cid:48) (t) = (1 − γ )p(t) + γ
Mπ matrices corresponding to ordered slates π as sp(cid:48) (t) = (cid:80)
sK 1A .
and (cid:80)
4. Note that p(cid:48) (t) ∈ P , and so sp(cid:48) (t) ∈ X . Decompose sp(cid:48) (t) as a convex combination of
π qπMπ , where qπ > 0
π qπ = 1.
5. Choose a slate π to display w.p. qπ , and obtain the loss Li,π(i) (t) for all 1 ≤ i ≤ s.
6. Construct the loss matrix ˆL(t) as follows: for 1 ≤ i ≤ s, set ˆLi,π(i) (t) = Li,π(i) (t)
i,π(i) (t) , and
sp(cid:48)
all other entries are 0.
7. Set the loss of policy ρ to be λρ (t) = ˆL(t) • φρ (t) in the MW algorithm.
Figure 5: Bandit Algorithm for Ordered Slates with Policies

The ﬁrst inequality above follows from Jensen’s inequality, and the second one is proved exactly as
in Section 3.2. Finally, we have RE(eρ(cid:63) (cid:107) p(1)) = ln(N ). Plugging these bounds into the bound
+ sγT ≤ 4(cid:112)sK ln(N )T ,
above, we get the stated regret bound from Theorem 2.1:
(cid:113) (1−γ )s ln(N )
(cid:113) (K/s) ln(N )
+ s ln(N )
E[RegretT ] ≤ η
K T
1 − γ
η
and γ =
, which satisfy the necessary technical condi-
K T
T

by setting η =
tions.

4.2 Ordered Slates with N Policies
In each round, every policy ρ recommends a distribution over ordered slates φρ (t) ∈ P , where P is
M scaled down by s as in Section 3.3. Our algorithm is given in Figure 5.
The regret bound analysis is exactly along the lines of that in Section 4.1, with L(t) and ˆL(t) playing
E[RegretT ] ≤ 4s(cid:112)K ln(N )T .
the roles of (cid:96)(t) and ˆ(cid:96)(t) respectively, with the inequalities from Section 3.3. We omit the details
for brevity. We get the stated regret bound from Theorem 2.1:
5 Conclusions and Future Work
√
In this paper, we presented efﬁcient algorithms for the unordered and ordered slate problems with
regret bounds of O(
T ), in the presence and and absence of policies, employing the technique of
Bregman projections on a convex set representing the convex hull of slate vectors.
Possible future work on this problem is in two directions. The ﬁrst direction is to handle other user
models for the loss matrices, such as models incorporating the following sort of interaction between
the chosen actions: if two very similar ads are shown, and the user clicks on one, then the user is
less likely to click on the other. Our current model essentially assumes no interaction.
√
T ) regret bounds for the slate problems in
The second direction is to derive high probability O(
the presence of policies. The techniques of [3] only give such algorithms in the no-policies setting.

References
[1] AB ERN ETHY, J . , HA ZAN , E . , AND RAKH L IN , A . Competing in the dark: An efﬁcient algo-
rithm for bandit linear optimization. In COLT (2008), pp. 263–274.

8

[2] AU ER , P. , C E SA -B IANCH I , N . , AND F I SCH ER , P. Finite-time analysis of the multiarmed
bandit problem. Machine Learning 47, 2-3 (2002), 235–256.
[3] AU ER , P. , C E SA -B IANCH I , N . , FR EUND , Y. , AND SCHA P IRE , R . E . The nonstochastic
multiarmed bandit problem. SIAM J. Comput. 32, 1 (2002), 48–77.
[4] AW ERBUCH , B . , AND K L E INBERG , R . Online linear optimization and adaptive routing. J.
Comput. Syst. Sci. 74, 1 (2008), 97–114.
[5] BARTL ETT, P. L . , DAN I , V. , HAY E S , T. P. , KAKAD E , S . , RAKHL IN , A . , AND T EWAR I ,
In COLT (2008),
A . High-probability regret bounds for bandit online linear optimization.
pp. 335–342.
[6] BR EGMAN , L . The relaxation method of ﬁnding the common point of convex sets and its
application to the solution of problems in convex programming. USSR Comp. Mathematics
and Mathematical Physics 7 (1967), 200–217.
[7] BRUALD I , R . A . , AND L EE , G . M . On the truncated assignment polytope. Linear Algebra
and its Applications 19 (1978), 33–62.
[8] C EN SOR , Y. , AND Z EN IO S , S . Parallel optimization. Oxford University Press, 1997.
[9] C E SA -B IANCH I , N . , AND LUGO S I , G . Combinatorial bandits. In COLT (2009).
[10] GY ¨ORGY, A . , L INDER , T. , LUGO S I , G . , AND OTTUC S ´AK , G . The on-line shortest path
problem under partial monitoring. Journal of Machine Learning Research 8 (2007), 2369–
2403.
[11] HA ZAN , E . , AND KA LE , S . Better algorithms for benign bandits. In SODA (2009), pp. 38–47.
[12] H E LMBO LD , D . P. , AND WARMUTH , M . K . Learning permutations with exponential weights.
In COLT (2007), pp. 469–483.
[13] H ERB ST ER , M . , AND WARMUTH , M . K . Tracking the best linear predictor. Journal of
Machine Learning Research 1 (2001), 281–309.
[14] KOO LEN , W. M . , WARMU TH , M . K . , AND K IV INEN , J . Hedging structured concepts. In
COLT (2010).
[15] LA I , T. , AND ROBB IN S , H . Asymptotically efﬁcient adaptive allocation rules. Advances in
Applied Mathematics 6 (1985), 4–22.
[16] M END EL SOHN , N . S . , AND DU LMAG E , A . L . The convex hull of sub-permutation matrices.
Proceedings of the American Mathematical Society 9, 2 (Apr 1958), 253–254.
[17] UCH IYA , T. , NAKAMURA , A . , AND KUDO , M . Algorithms for adversarial bandit problems
with multiple plays. In ALT (2010), pp. 375–389.
[18] WARMUTH , M . K . , AND KU ZM IN , D . Randomized PCA algorithms with regret bounds that
are logarithmic in the dimension. In In Proc. of NIPS (2006).

9


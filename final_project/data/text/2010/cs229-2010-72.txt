Multiple Experts with Binary Features

Ye Jin & Lingren Zhang

December 9, 2010

1 Introduction

Our intuition for the pro ject comes from the paper ”Supervised Learning from Multi-
ple Experts: Whom to trust when everyone lies a bit” by Raykar, Yu, etc. The paper
analyzed a classiﬁcation problem where instead of observing a ”true” classiﬁcation of
each data point, we observe some classiﬁcation from several experts. The pro ject will
attempt to solve a variation of the problem in which the features are binary instead
of real-valued. In addition, we generalized the problem to do a N-class classiﬁcation
instead of a binary classiﬁcation.

2 Model Description

2.1 Training Data
The data set contains m training examples: {((cid:126)x(i) , (cid:126)y (i) ); i = 1, ..., m}, where (cid:126)x(i) =
j ∈ {0, 1} and (cid:126)y (i) ∈ AR with A = {1, 2, ..., N } representing
(x(i)
1 , x(i)
2 , ...x(i)
k ) with x(i)
the N diﬀerent classes, i.e. the feature space is k -dimensional and there are R experts
providing estimates of the true y (i) .

2.2 Model Assumptions

2.2.1 Naive Bayes Assumption

Similar to the spam classiﬁcation example given in class, we make the Naive Bayes
assumption. Assume that for the ith training example, the x(i)
j ’s are conditionally
independent given the true y (i) . Then we have the following property which is con-
venient:
k(cid:89)
j=1

k |y (i) ) =
2 , ..., x(i)
1 , x(i)
p(x(i)

j |y (i) ).
p(x(i)

(1)

2.2.2 Characteristic Matrix for Each Customer

For the rth expert, we deﬁne his/her characteristic matrix to be M (r) , where M (r)
p,q =
P (yr = q |y = p) for p, q ∈ {1, 2, ..., N } i.e.
the entry on the pth row and qth
column is the probability that the rth expert gives classiﬁcation q given that the true
classiﬁcation is p. Notice that each row of this matrix has to add up to one, hence
the degree of freedom is N (N − 1) instead of N 2 , i.e. we should really describe each
customer using a N × (N − 1) matrix instead of a N × N matrix, but for symmetry
and simplicity we keep it that way for now.

2

3 Single Expert Case: A Generalization to Spam
Classiﬁcation

We use two sets of parameters to model this problem:

φy = P (y (i) = y)
j = 1|y (i) = y).
φj |y = P (x(i)

φN = 1 − (cid:80)N −1
Note that we only consider φ1 , φ2 , ..., φN −1 as parameters, φN can be calculated as
p=1 φp .
The joint likelihood is:

m(cid:89)
m(cid:89)
i=1
m(cid:89)
i=1
i=1

L(φy , φj |y ) =

=

=

2 , ..., x(i)
1 , x(i)
p(x(i)
k , y (i) )
k(cid:89)
p(y (i) )
k(cid:89)
j=1
j |y(i) (1 − φj |y(i) )1−x(i)
x(i)
j
φ
j .
j=1

p(x(i)
j )

φy(i)

Set the partial derivatives of L to 0 and we derive the maximum likelihood estimators:
(cid:80)m
i=1 1{y (i) = y}
(cid:80)m
(cid:80)m
m
i=1 1{y (i) = y}x(i)
j
i=1 1{y (i) = y} .

φj |y =

φy =

4 Multiple Expert Case

4.1 Likelihood Function

We need to use the characteristic matrices M (r) as well as the parameters used in the
single expert case (φy , φj |y ). Let Θ = (M (r) , φy , φj |y ). We can calculate the likelihood
function:

3

m(cid:89)
m(cid:89)
i=1
m(cid:89)
i=1
i=1

L(Θ) =

=

=

P (y (i)
1 , ..., y (i)
R , x(i) ; Θ)
N(cid:88)
R |y (i) = n, x(i) ; Θ)P (x(i) |y (i) = n; Θ)P (y (i) = n; Θ)
(cid:33)
(cid:32) R(cid:89)
(cid:33) (cid:32) k(cid:89)
1 , ..., y (i)
P (y (i)
N(cid:88)
n=1
r=1
n=1
j=1

j |n (1 − φj |n )1−x(i)
x(i)
j
φ
j

M (r)
n,y(i)
r

φn .

However, L(Θ) is quite diﬃcult to maximize because of the summation in the formula
(and hence taking the log-likelihood does not simplify the problem very much). The
solution is to use the EM algorithm with (cid:126)y = (y (1) , ..., y (m) ) as latent variables. Now
consider the new likelihood function:
m(cid:89)
m(cid:89)
i=1
m(cid:89)
i=1
i=1

P (y (i)
1 , ..., y (i)
R , x(i) , y (i) ; Θ)
R |y (i) , x(i) ; Θ)p(x(i) |y (i) ; Θ)p(y (i) ; Θ)
(cid:32) R(cid:89)
(cid:33)
(cid:33) (cid:32) k(cid:89)
1 , ..., y (i)
p(y (i)
r=1
j=1

j |y(i) (1 − φj |y(i) )1−x(i)
x(i)
j
φ
j

M (r)
y(i) ,y(i)
r

L((cid:126)y , Θ) =

φy(i) .

=

=

4.2 The EM Algorithm

Qi (y (i) ) =

4.2.1 E-step
We need Qi (y (i) ) ∝ p(y (i)
R |y (i) , x(i) ; Θ)p(x(i) |y (i) ; Θ)p(y (i) ; Θ), and thus
1 , ..., y (i)
(cid:80)N
R |y (i) , x(i) ; Θ)p(x(i) |y (i) ; Θ)p(y (i) ; Θ)
1 , ..., y (i)
p(y (i)
(cid:19)
(cid:17) (cid:18)(cid:81)k
(cid:16)(cid:81)R
R |y (i) = n, x(i) ; Θ)P (x(i) |y (i) = n; Θ)P (y (i) = n; Θ)
n=1 P (y (i)
1 , ..., y (i)
(cid:17) (cid:18)(cid:81)k
(cid:19)
(cid:16)(cid:81)R
j |y(i) (1 − φj |y(i) )1−x(i)
x(i)
(cid:80)N
r=1 M (r)
j
φy(i)
j=1 φ
j
y(i) ,y(i)
r
j |n (1 − φj |n )1−x(i)
x(i)
r=1 M (r)
j
j=1 φ
j
n=1
n,y(i)
r
(cid:80)R
To initialize (because during the ﬁrst E-step, we do not have Θ to let us calculate
r=1 1{y (i)
r = y (i)}.
Qi (y (i) )), we can set Qi (y (i) ) = 1
R

φn

=

4

4.2.2 M-step

l(Θ) =

(cid:33)

(cid:33)

φn

M (r)
n,y(i)
r

Given Qi (y (i) ) calculated in the E-step, we want to maximize
(cid:33) (cid:32) k(cid:89)
(cid:32)(cid:32) R(cid:89)
m(cid:88)
N(cid:88)
j |n (1 − φj |n )1−x(i)
x(i)
(cid:32) R(cid:88)
(cid:33)
j
Qi (n) log
φ
j
N(cid:88)
m(cid:88)
k(cid:88)
r=1
n=1
i=1
j=1
j ) log(1 − φj |n ))
log φj |n + (1 − x(i)
log M (r)
(x(i)
Qi (n)
=
+ log φn +
= 0 for p = 1, 2, ..., N − 1 (bear in mind that Mn,N = 1 − (cid:80)N −1
j
n,y(i)
r
r=1
n=1
i=1
j=1
is not a free parameter), we get Mn,p ∝ (cid:80)m
∂ l
Setting
p=1 Mn,p
∂M (r)
n,p
i=1 Qi (n)1{y (i)
r = p}, hence
(cid:80)m
(cid:80)m
i=1 Qi (n)1{y (i)
r = p}
= 0 for n = 1, 2, ..., N − 1 (again, φN = 1 − (cid:80)N −1
M (r)
n,p =
.
i=1 Qi (n)
parameter), we get φn ∝ (cid:80)m
Setting ∂ l
n=1 φn is not a free
∂φn
(cid:80)m
(cid:80)m
i=1 Qi (n), hence
(cid:80)N
(cid:80)m
i=1 Qi (n)
i=1 Qi (n)
(cid:18) x(i)
m
i=1 Qi (p)
p=1
− 1−x(i)
(cid:80)m
j
j
1−φj |n
φj |n
(cid:80)m
i=1 Qi (n)x(i)
j
i=1 Qi (n)
It is worth noting that φn , φj |n derived here in the M-step is the same as MLE in the
single expert case, except all the 1{y (i) = n} terms are substituted with Qi (n).

φn =
= (cid:80)m
i=1 Qi (n)

equal to zero. And we get

Lastly, we set

(cid:19)
=

φj |n =

(2)

(3)

(4)

∂ l
∂φj |n

.

.

4.3 Missing Labels

One of the technical details that we need to deal with is the missing labels, meaning
that not all experts give classiﬁcations to all training examples, i.e. y (i)
r does not
necessarily exist for all i and r. It turns out that we can make a small change to
our algorithm to take care of this issue. Let Ri be the set of experts who classiﬁed
(cid:33)
(cid:32) (cid:89)
(cid:33) (cid:32) k(cid:89)
training example i. Then the likelihood function becomes
m(cid:89)
N(cid:88)
r∈Ri
n=1
i=1
j=1

j |n (1 − φj |n )1−x(i)
x(i)
j
φ
j

M (r)
n,y(i)
r

L(Θ) =

φn .

5

Qi (y (i) ) =

(cid:19)
(cid:19)
φy(i)

(cid:17) (cid:18)(cid:81)k
(cid:16)(cid:81)
Consequently, the E-step can be modiﬁed to
(cid:17) (cid:18)(cid:81)k
(cid:16)(cid:81)
j |y(i) (1 − φj |y(i) )1−x(i)
x(i)
(cid:80)N
M (r)
j
j=1 φ
r∈Ri
j
y(i) ,y(i)
r
j |n (1 − φj |n )1−x(i)
x(i)
M (r)
j
(cid:80)
j=1 φ
j
n=1
n,y(i)
r
1{y (i)
r = y (i)}.
with the initial step to be Qi (y (i) ) = 1|Ri |
r∈Ri
For the M-step, the update formulae for φn and φj |n does not change, the formula for
(cid:80)
M (r)
n,p can be rewritten as
(cid:80)
Qi (n)1{y (i)
r = p}
i:r∈Ri
Qi (n)
i:r∈Ri

M (r)
n,p =

r∈Ri

.

.

φn

M (r)
n,p =

4.4 Laplace Smoothing
i=1 Qi (n) and (cid:80)
In the M-step, since (cid:80)m
Another technical detail that we may encounter is that the denominators in the E-step
and M-step formulae may be zero. As a result, we need to apply Laplace smoothing.
(cid:80)
Qi (n) might be zero (consider the case
i:r∈Ri
where nobody ever gave a classiﬁcation of n in the training set, and the ﬁrst M-step
1{y (i)
r = y (i)}), the
right after the initial E-step which is to set Qi (y (i) ) = 1|Ri |
r∈Ri
formulae can be replaced with
(cid:80)
(cid:80)
Qi (n)1{y (i)
r = p} + 1
(cid:80)m
i:r∈Ri
Qi (n) + N
i:r∈Ri
(cid:80)m
i=1 Qi (n) + 1
(cid:80)m
m + N
i=1 Qi (n)x(i)
j + 1
φj |n =
.
n,p = 1 and (cid:80)N
One can also sanity-check that (cid:80)N
i=1 Qi (n) + 2
p=1 M (r)
n=1 φn = 1 using the smoothed
formulae.
(cid:33)
(cid:33) (cid:32) k(cid:89)
(cid:32) (cid:89)
In the E-step, applying Laplace smoothing might be diﬃcult since each
r∈Ri
j=1
term can be very small and it is hard the estimate the order of magnitude of these
terms. As a result, adding 1 to the numerator and N to the denominator, or generally

j |y(i) (1 − φj |y(i) )1−x(i)
x(i)
j
φ
j

M (r)
y(i) ,y(i)
r

φn =

φy(i)

6

adding some pre-determined constant c to the numerator and N c to the denominator,
may destroy most of the meaningful information in the Qi (y (i) ) distribution, making
them all equal to 1
N . However, the good news is that because of the smoothing applied
n,p , φn , φj |n ∈ (0, 1) and the denominator will
in the M-step, one is guaranteed that M (r)
be non-zero, hence there is no need to apply smoothing in the E-step.

7


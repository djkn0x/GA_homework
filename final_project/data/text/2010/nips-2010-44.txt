Movement extraction by detecting
dynamics switches and repetitions

Silvia Chiappa
Statistical Laboratory
Wilberforce Road, Cambridge, UK
silvia@statslab.cam.ac.uk

Jan Peters
Max Planck Institute for Biological Cybernetics
Spemannstrasse 38, Tuebingen, Germany
jan.peters@tuebingen.mpg.de

Abstract

Many time-series such as human movement data consist of a sequence of basic
actions, e.g., forehands and backhands in tennis. Automatically extracting and
characterizing such actions is an important problem for a variety of different appli-
cations. In this paper, we present a probabilistic segmentation approach in which
an observed time-series is modeled as a concatenation of segments corresponding
to different basic actions. Each segment is generated through a noisy transforma-
tion of one of a few hidden trajectories representing different types of movement,
with possible time re-scaling. We analyze three different approximation methods
for dealing with model intractability, and demonstrate how the proposed approach
can successfully segment table tennis movements recorded using a robot arm as
haptic input device.

1

Introduction

Motion capture systems have become widespread in many application areas such as robotics [18],
physical therapy, sports sciences [10], virtual reality [15], artiﬁcial movie generation [13], computer
games [1], etc. These systems are used for extracting the movement templates characterizing basic
actions contained in their recordings. In physical therapy and sports sciences, these templates are
employed to analyze patient’s progress or sports professional’s movements; in robotics, virtual real-
ity, movie generation or computer games, they become the basic elements for composing complex
actions.
In order to obtain the movement templates, boundaries between actions need to be detected. Further-
more, fundamental similarities and differences in the dynamics underlying different actions need to
be captured. For example, in a recording from a game of table tennis, observations corresponding to
different actions can differ, due to different goals for hitting the ball, racket speeds, desired ball in-
teraction, etc. The system needs to determine whether this dissimilarity corresponds to substantially
diverse types of underlying movements (such as in the case of a forehand and a backhand), or not
(such as in the case of two forehands that differ only in speed).
To date, most approaches addressed the problem by using considerable manual interaction [16]; an
important advancement would be to develop an automatic method that requires little human inter-
vention. In this paper, we present a probabilistic model in which actions are assumed to arise from
noisy transformations of a small set of hidden trajectories, each representing a different movement
template, with non-linear time re-scaling accounting for differences in action durations. Action
boundaries are explicitly modeled through a set of discrete random variables. Segmentation is ob-
tained by inferring, at each time-step, the position of the observations in the current action and the
underlying movement template. To guide segmentation, we impose constraints on the minimum and
maximum duration that each action can have.

1

σt+1

· · ·

zt+1

vt+1

· · ·

σt−1

zt−1

vt−1

σt

zt

vt

h1:S
1:M

(b)

(a)

Figure 1: (a) The hidden dynamics shown on the top layer are assumed to generate the time-series
at the bottom. (b) Belief network representation of the proposed segmentation model. Rectangular
nodes indicate discrete variables, while (ﬁlled) oval nodes indicate (observed) continuous variables.

We apply the model to a human game of table tennis recorded with a Barrett WAM used as a haptic
input device, and show that we can obtain a meaningful segmentation of the time-series.

2 The Segmentation Model

In the proposed segmentation approach, the observations originate from a set of continuous-valued
hidden trajectories, each representing a different movement template. Speciﬁcally, we assume that
the observed time-series consists of a concatenation of segments (basic actions), each generated
through a noisy transformation of one of the hidden trajectories, with possible time re-scaling. This
generation process is illustrated in Figure 1 (a), where the observations on the lower graph are
generated from the three underlying hidden trajectories on the upper graph. Time re-scaling happens
during the generation process, e.g., the ﬁrst hidden trajectory of length 97 gives rise to three segments
of length 75, 68 and 94 respectively.
The observed time-series and the S hidden trajectories are represented by the continuous random
m ∈ (cid:60)H ), respectively.
1:M ≡ h1
variables1 v1:T ≡ v1 , . . . , vT (vt ∈ (cid:60)V ) and h1:S
1:M (hi
1:M , . . . , hS
Furthermore, we introduce two sets of discrete random variables σ1:T and z1:T . The ﬁrst set is
used to infer which movement template generated the observations at each time-step, to detect ac-
tion boundaries, and to deﬁne hard constraints on the minimum and maximum duration of each
observed action. The second set is used to model time re-scaling from the hidden trajectories to the
(cid:89)
observations. We assume that the joint distribution of these variables factorizes as follows
p(vt |h1:S
1:M , zt , σt )p(zt |zt−1 , σt−1:t )p(σt |σt−1 ).
p(h1:S
1:M)
t

These independence relations are graphically represented by the belief network of Figure 1 (b).
The variable σt is a triple σt = {st , dt , ct} with a similar role as in regime-switching models with
explicit regime-duration distribution (ERDMs) [4]. The variable st ∈ {1, . . . , S } indicates which
of the S hidden trajectories underlies the observations at time t. The duration variables dt speciﬁes
the time interval spanned by the observations forming the current action, and takes a value between
dmin and dmax . The count variable ct indicates the time distance to the beginning of the next action,
taking value ct = dt and ct = 1 respectively at the beginning and end of an action. More speciﬁcally,
we deﬁne p(σt |σt−1 ) = p(ct |dt , ct−1 )p(dt |dt−1 , ct−1 )p(st |st−1 , ct−1 ) with2

1For the sake of notational simplicity, we describe the model for the case of a single observed time-series
and hidden trajectories of the same length M .
2We assume c0 = 1, cT = 1. For t = 1, p(s1 ) = ˜πs1 , p(d1 ) = ρd1 , p(c1 |d1 ) = δ(c1 = d1 ).

2

756768619456535351Observations647397Hidden dynamics(cid:26)δ(ct = dt )
if ct−1 = 1,
δ(ct = ct−1 − 1) if ct−1 > 1,

p(ct |dt , ct−1 ) =

p(st |st−1 , ct−1 ) =

(cid:26)πst ,st−1
if ct−1 = 1,
(cid:26)ρdt
δ(st = st−1 ) if ct−1 > 1,
if ct−1 = 1,
p(dt |dt−1 , ct−1 ) =
δ(dt = dt−1 ) if ct−1 > 1,
where δ(x = y) = 1 if x = y and δ(x = y) = 0 if x (cid:54)= y , π is a matrix specifying the time-invariant
dynamics-switching distribution, and ρ is a vector deﬁning the action-duration distribution.
(cid:40) ˜ψdt ,ct
The variable zt indicates which of the M elements in the hidden trajectory generated the observa-
tions vt . We deﬁne p(zt |zt−1 , σt−1:t ) = p(zt |zt−1 , dt , ct−1:t ) with
if ct−1 = 1,
p(zt |zt−1 , dt , ct−1:t ) =
zt
zt ,zt−1 if ct−1 > 1.
ψdt ,ct
The vector ˜ψdt ,ct and the matrix ψdt ,ct encode two constraints3 . First, zt − zt−1 ∈ {1, . . . , wmax}
ensures that subsequent observations are generated by subsequent elements of the hidden trajectory
and imposes a limit on the magnitude of time-warping. Second, zt ∈ {dt − ct + 1, . . . , M − ct + 1}
accounts for the dt − ct and ct − 1 observations preceding and following vt in the action.
The hidden trajectories follow independent linear
Table 1: Model’s Generation Mechanism
Markovian dynamics with Gaussian noise, that is
m |hi
m−1 ) = N (F ihi
1 ∼ N (µi , Σi ).
p(hi
m−1 , Σi
H ), hi
Finally, the observations are generated from a linear
transformation of the hidden variables with Gaussian
noise
1:M , zt , σt ) = N (Gst hst
p(vt |h1:S
+ λdt ,t+ct−1 , Σst
V ),
zt
where the term λdt ,t+ct−1 is common to all obser-
vations belonging to the same action and allows for
spatial translation.
The generative process underlying the model is de-
scribed in detail in4 Table 1.
The set Θ of unknown model parameters is given by
V , µ1:S , Σ1:S , π , ˜π , ρ, ψ , ˜ψ , λ}.
{F 1:S , G1:S , Σ1:S
H , Σ1:S
After learning Θ, we can sample a segmentation
from p(σ1:T |v1:T ) or compute the most
likely
1:T = arg maxσ1:T p(σ1:T |v1:T )5 .
segmentation σ∗
Relation to previous models. From a modeling point of view, the presented method builds on
previous approaches that consider the observed time-series as time-warped transformations of one
or several continuous-valued hidden trajectories. In [11], the authors introduced a model in which
different time-series are assumed to be generated by a single continuous-valued latent trace, with
spatial and time re-scaling. This model was used to align speech sequences. In [6], a modiﬁed
version of such a model was employed in the domain of helicopter ﬂight to learn a desired trajectory
from demonstrations. In [12] and [14], the authors considered the case in which each time-series is
generated by one of a set of different hidden trajectories. None of these models can deal with the
situation in which possibly different dynamics underlie different segments of the same time-series.
From an application point of view, previous segmentation systems for extracting basic movements
employed considerable human intervention [16]. On the other hand, automatic probabilistic methods
for modeling movement templates assumed that the time-series data was pre-segmented into basic
movements [5, 17].
3 In the experiments we added the additional constraint that nearly complete movements are observed, that
is zt−dt+ct ∈ {1, . . . , ι}, zt+ct−1 ∈ {, . . . , M } (see the Appendix for more details).
4With π:,st−1 we indicate the vector of transition probabilities from dynamics type st−1 to any dynamics.
5Due to space limitations, we describe only how to sample a segmentation, which is required in the Gibbs
sampling method.

for i = 1, . . . , S do
generate hidden trajectory i
1 ∼ N (µi , Σi )
hi
m ∼ N (0, Σi
m , ηh
m−1 + ηh
hi
m = F ihi
H )
set t = 1
for action a = 1, . . . , A do
sample a dynamics type st ∼ π:,st−1
sample a duration dt ∼ ρ
mark the beginning of the action ct = dt
while ct ≥ 1 do
sample time-warping zt ∼ ψdt ,ct
:,zt−1
generate the observations
zt + λdt ,t+ct−1 + ηv
vt = Gst hst
t
t ∼ N (0, Σst
ηv
V )
t = t + 1
if ct−1 > 1 do
st = st−1 , dt = dt−1 , ct = ct−1 − 1

3

3

Inference and Learning

The interaction between the continuous and discrete hidden variables renders the computation of the
posterior distributions required for learning and sampling a segmentation intractable. In this section,
we present and analyze three different approximation methods for dealing with this problem. In the
1:M , z1:T , σ1:T |v1:T , Θ) is approximated with a simpler distribution
ﬁrst (variational) method, p(h1:S
q , and the optimal q and Θ are found by maximizing a tractable lower bound on the log-likelihood
using an Expectation-Maximization (EM) approach. In the second (maximum a posteriori) method,
1:M , v1:T |Θ) using an
we estimate the most likely set of hidden trajectories and Θ by maximizing p(h1:S
EM approach. In the third (Gibbs sampling) method, we use stochastic EM [3] with Gibbs sampling.

3.1 Variational Method

In the variational approximation, we introduce a distribution q in which the problematic dependence
between the hidden dynamics and the segmentation and time-warping variables is relaxed, that is6
1:M )q(z1:T |σ1:T )q(σ1:T ) .
q(h1:S
1:M , z1:T , σ1:T ) = q(h1:S
From the Kullback-Leibler divergence between this distribution and the original posterior distribu-
tion we obtain a tractable lower bound on the log-likelihood log p(v1:T |Θ), given by
B(q , Θ) = Hq(h1:S
1:M ) + (cid:104)Hq(z1:T |σ1:T ) (cid:105)q(σ1:T ) + Hq(σ1:T )
1:M |Θ)(cid:11)
+ (cid:104)log p(z1:T |σ1:T , Θ)(cid:105)q(z1:T |σ1:T )q(σ1:T ) + (cid:104)log p(σ1:T |Θ)(cid:105)q(σ1:T ) + (cid:10)log p(h1:S
+ (cid:104)log p(v1:T |h1:S
1:M , z1:T , σ1:T , Θ)(cid:105)q(h1:S
1:M )q(z1:T |σ1:T )q(σ1:T )
1:M ) ,
q(h1:S
where (cid:104)·(cid:105)q denotes expectation with respect to q , and Hq denotes the entropy of q . We then use a
variational EM algorithm in which B(q , Θ) is iteratively maximized with respect to q and the model
parameters Θ until convergence7 .
Maximization with respect to q leads to the following updates
(cid:104)log p(v1:T |h1:S
1:M ,z1:T ,σ1:T )(cid:105)q(z1:T |σ1:T )q(σ1:T ) ,
1:M ) ∝ p(h1:S
q(h1:S
1:M )e
(cid:104)log p(v1:T ,z1:T |h1:S
1:M ,σ1:T )(cid:105)q(h1:S
q(σ1:T ) ∝ p(σ1:T )eHq(z1:T |σ1:T ) e
1:M
(cid:104)log p(v1:T |h1:S
1:M ,z1:T ,σ1:T )(cid:105)q(h1:S
q(z1:T |σ1:T ) ∝ p(z1:T |σ1:T )e
1:M

)q(z1:T |σ1:T ) ,

(3)

(1)

(2)

) .

q(z1:T ,σ1:T ) =

Before describing how to perform inference on these distributions, we observe that all quantities
1:M ) can be formulated such
required for learning Θ, sampling a segmentation, and updating q(h1:S
that only partial inference on q(σ1:T ) and q(z1:T |σ1:T ) is required. For example, we can write
(cid:88)
(cid:88)
(cid:10)log p(v1:T |h1:S
1:M , z1:T , σ1:T )(cid:11)
τ ,m
t,i,k
= {st = i, dt = k , ct = 1}. Thus, only
= q(zτ = m|σ i,k,1
with γ i,k,1
), σ i,k,1
= q(σ i,k,1
), ξ i,k,t,m
1:M ) = (cid:81)
t
t
t
t
τ
posteriors for which the count variables take value 1 are required8 .
Inference on q(h1:S
1:M ). We ﬁrst notice that, by using (4) in (1), we obtain q(h1:S
1:M ).
i q(hi
We then observe that we can rewrite the update for q(hi
1:M ) as proportional to the joint distribution
of the following linear gaussian state-space model (LGSSM)
m ∼ N (0, ˆΣi
1 ∼ N (µi , Σi ), ˆv i
m ∼ N (0, Σi
m , ηv
m + ηv
m = Gihi
m , ηh
m−1 + ηh
m = F ihi
hi
H ), hi
V ,m ),

log p(vτ |hi
m , zτ = m, σ i,k,1
t

ξ i,k,t,m
τ

γ i,k,1
t

), (4)

6Conditioning on v1:T in q is omitted for notational simplicity.
7Maximization with respect to Θ is omitted due to space limitations.
8This is common to ERDMs using separate duration and count variables [4].

4

where
m ≡ 1/ai
ˆv i
m

(cid:88)
m ≡ (cid:88)
t(cid:88)
t(cid:88)
γ i,k,1
ξ i,k,t,m
ai
t
τ
τ =t−k+1
τ =t−k+1
t,k
t,k
Therefore, inference on q(h1:S
1:M ) can be accomplished with LGSSM smoothing routines [7].

V ,m ≡ 1/ai
ˆΣi
mΣi
V ,

γ i,k,1
t

vτ ,

ξ i,k,t,m
τ

.

)

αj,l,1
t−k ,

β j,1
t =

q(vt+1:t+k |σ i,k,1
t+k )πi,j β i,1
t+k ρk .

Inference on q(σ1:T ). By substituting update (3) (including the normalization constant) into up-
date (2), we obtain q(σ1:T ) ∝ q(v1:T |σ1:T )p(σ1:T ). This update has the form of the joint distri-
bution of an ERDM using separate duration and count variables [4]. Therefore, we can employ
) = q(vt+1:T |st = j, ct =
= q(σ i,k,1
similar forward-backward recursions. More speciﬁcally γ i,k,1
(cid:88)
(cid:88)
t
t
1)q(σ i,k,1
t αi,k,1
, v1:t )/q(v1:T ) = β i,1
/q(v1:T ), where
t
t
(cid:123)(cid:122)
(cid:125)
(cid:124)
= q(vt−k+1:t |σ i,k,1
|σ j,l,1
p(σ i,k,1
αi,k,1
t−k )
t
t
t
i,k
j l
ρk πi,j
implies q(v1:T ) = (cid:80)
Since we have imposed the constraints c0 = 1, cT = 1, we need to replace terms such as p(dt =
k , ct = 1|ct−k = 1) = ρk with p(dt = k , ct = 1|ct−k = 1, c0 = 1, cT = 1). The constraint cT = 1
j,l αj,l,1
.
T
Required terms such as q(vt−k+1:t |σ i,k,1
t
inference on q(zt−k+1:t |σ i,k,1
).
t
Inference on q(z1:T |σ1:T ). The form of update (3) implies that inference on distributions of the
type q(zt−k+1:t |σ i,k,1
) can be accomplished with forward-backward routines similar to the ones
t
used in hidden Markov models (HMMs).
q(σ1:T |v1:T ) = q(σT |v1:T ) (cid:81)T −1
Sampling a segmentation. A segmentation can be sampled by using the factorization
t=1 q(σt |σt+1 , v1:T ), with
p(σt+1 |σt )q(vt+1 |σt+1 )ασt
t 
β σt+1
q(σt |σt+1 , v1:T ) =
t+1
t+1 
ασt+1
β σt+1
t+1
Suppose that, at time t, ct = 1 and we have sampled dynamics type st = i and duration dt = k .
Then, σt−k+1:t−1 and ct−k are determined by the model assumptions9 , so that we effectively need
to sample st−k , dt−k from the distribution q(st−k , dt−k , ct−k = 1|σt−k+1 , v1:T ), which is given by
ρk πi,: q(vt−k+1:t |σt )α:,:,1
t−k /αi,k,1
,
t
since q(vt−k+2:t |σt−k+2:t )αi,k,k
t−k+1 = αi,k,1
.
t

) can be computed as likelihood terms when performing

.

3.2 Maximum a Posteriori (MAP) Method

Instead of approximating the posterior distribution of all hidden variables, we can approximate only
1:M |v1:T ) with a deterministic distribution, by using the variational method described above in
p(h1:S
m ) is a Dirac delta around its mean. Notice that this is equivalent to compute the most
which q(hi
1:M |Θ)
likely set of hidden trajectories and parameters by maximizing the joint distribution p(v1:T , h1:S
with respect to h1:S
1:M and Θ using an EM algorithm.
3.3 Gibbs Sampling Method
likelihood L(Θ) is approximated by L(Θ) ≈ (cid:80)N
In our stochastic EM approach with Gibbs sampling, the expectation of the complete-data log-
1:M |Θ), where
1:T , ˆh1:S,n
n=1 log p(v1:T , ˆzn
1:T , ˆσn
1:M , z1:T , σ1:T |v1:T ). Such samples can be obtained
1:T , ˆh1:S,n
1:M are samples drawn from p(h1:S
ˆzn
1:T , ˆσn
by iterative drawing from the tractable conditionals
1:M |z1:T , σ1:T , v1:T ).
p(z1:T , σ1:T |h1:S
1:M , v1:T ) and p(h1:S
9The values of c1:T −1 are automatically determined if cT and d1:T are given.

5

Correct Seg.

Variational
Approx.

MAP
Approx.

Gibbs Sampling
Approx.

Time-series 1
1 24 42 66 89
1 17 39 62 82
1 17 39 63 82
1 17 38 62 81
1 14 39 63 79
1 20 40 64 85
1 19 40 64 84
1 17 39 63 82
1 20 40 64 85
1 17 39 63 82
1 22 41 64 88
1 17 40 65 81
1 17 40 64 82

Time-series 2
1 23 46 63
1 23 46 64
1 18 42 63
1 18 42 63
1 22 45 63
1 23 46 62
1 23 46 62
1 23 46 62
1 23 46 63
1 18 36 56
1 20 42 60
1 9 23 47 63 71
1 21 47 62

Time-series 3
1 23 40 63
1 21 39 62
1 22 38 62
1 17 38 60 87
1 23 38 62
1 21 40 64
1 21 40 63
1 22 40 65
1 22 40 63
1 23 38 62
1 16 35 61 82
1 17 40 64 84
1 17 37 60 86

Time-series 4
1 22 47 68
1 23 47 68
1 22 47 66
1 9 23 31 60 66
1 9 23 31 46 67
1 22 47 69
1 22 47 68
1 22 47 68
1 15 20 45 67
1 14 24 38 63
1 14 24 38 63
1 9 22 32 47 68
1 9 23 31 52 74

Time-series 5
1 24 42 65 88 105
1 18 42 65 82 100
1 18 42 65 82 99
1 6 12 42 58 76 83 100
1 11 18 42 60 85 102
1 18 40 55 65 82 97
1 18 42 64 82 98
1 18 42 65 82 96
1 11 19 40 60 85 102
1 16 44 71 97
1 16 40 63 80 102
1 22 44 63 89 104
1 7 13 21 31 58 71 101 114

Table 2: Segmentations given by the variational, MAP and Gibbs sampling methods on 5 artiﬁcial
time-series.

In order to sample from p(z1:T , σ1:T |h1:S
1:M , v1:T ), we can ﬁrst sample a segmentation from
1:M , v1:T ) employing the method described above (with q(·) replaced by p(·|h1:S
p(σ1:T |h1:S
1:M , v1:T )),
and then use a HMM forward-ﬁltering backward-sampling method for
sampling from
p(z1:T |σ1:T , h1:S
1:M |z1:T , σ1:T , v1:T ) may be carried out us-
1:M , v1:T ). Finally, sampling from p(h1:S
ing the forward-ﬁltering backward-sampling procedure described in [8].

3.4 Comparison of the Approximation Methods

In this section, we compare the performance of the approximation methods presented above on 5
artiﬁcially generated time-series. Each time-series (with V=2 or V=3) contains repeated occurrences
of actions arising from the noisy transformation of up to three hidden trajectories with time-warping.
In the second row of Table 2, we give the correct segmentation for each time-series. Each number
indicates the time-step at which a new action starts, whilst the colors indicate the types of dynamics
underlying the actions. In the rows below, we give the segmentations obtained by each approxima-
tion method with 4 different initial random conditions (with minimum and maximum action duration
between 5 and 30).
From the results, we can deduce that Gibbs sampling performs considerably worse than the deter-
ministic approaches. Between the variational and MAP methods, the latter is preferable and gives
a good solution in most cases. The poor performance of Gibbs sampling can be explained by the
1:M and σ1:T , z1:T . The
fact that this method cannot deal well with high correlation between h1:S
continuous hidden variables are sampled given a single set of segmentation and time-warping vari-
ables (unlike update (1) in which we average over segmentation and time-warping variables), which
may result in poor mixing. The inferior performance of the variational method in comparison to
the MAP method would seem to suggest that the posterior covariances of the continuous hidden
variables cannot accurately be estimated.

4 Table Tennis Recordings using a Robot Arm

In this section, we show how the proposed model performs in segmenting time-series obtained
from table tennis recordings using a robot arm moved by a human. The generic goal is to extract
movement templates to be used for robot imitation learning [2, 9]. Here, kinesthetic teach-in can be
advantageous in order to avoid the correspondence problem.
We used the Barrett WAM robot arm shown in Figure 2 as a haptic input device for recording
and replaying movements. We recorded a game of table tennis where a human moved the robot
arm making the typical moves occurring in this speciﬁc setup. These naturally include forehands,
going into an awaiting posture for a forehand, backhands, and going into an awaiting posture for
a backhand. They also include smashes, however, due to the inertia of the robot, they are hard to
perform and only occur using the forehand.

6

Figure 3: This ﬁgure shows the ﬁrst three degrees of freedom (known as ﬂexion-extension,
adduction-abduction and humerus rotation) of a robot arm when used by a human as a haptic in-
put device playing table tennis. The upper graph shows the joint positions while the lower one
shows the joint velocities. The dashed vertical lines indicate the obtained action boundaries and the
numbers the underlying movement templates. This sequence includes moves to the right awaiting
posture (1), moves to the left awaiting posture (2), forehands (3, 5), two incomplete moves towards
the awaiting posture merged with a backhand (4), moves to the left awaiting posture with humerus
rotation (6) and backhands (7).

The recorded time-series contains the joint positions
and velocities of all seven degrees of freedom (DoF)
of an anthropomorphic arm. However, only the
shoulder and upper arm DoF, which are the most sig-
niﬁcant in such movements, were considered for the
analysis. The 1.5 minutes long recording was sub-
sampled at 5 samples per seconds. The minimum
and maximum durations dmin and dmax were set to
4 and 15 respectively, as prior knowledge about ta-
ble tennis would suggest that basic-action durations
are within this range. We also imposed the con-
straint that nearly complete movements are observed
(ι = 2,  = M − 1). The length of the hidden dy-
namics M was set to dmax , the variable wmax was
Figure 2: The Barrett WAM used for record-
set to10 4, and the number of movement templates
ing the table tennis sequences. During the
S was set to 8, as this should be a reasonable upper
experiment the robot is in a gravity compen-
bound on the number of different underlying move-
sation and sequences can be replayed on the
ments. Given the results obtained in the previous
real system.
section, we used the MAP approximation method.
We assumed no prior knowledge on the dynamics of the hidden trajectories. However, in a real ap-
plication of the model we could simplify the problem by incorporating knowledge about previously
identiﬁed movement templates.
As shown in Figure 3, the model segments the time-series into 59 basic movements of forehands
(numbers 3, 5), backhands (7), and going into a right (1) and left (2, 6) awaiting posture. In some
cases, a more ﬂuid game results in incomplete moves towards an awaiting posture and hence into
a composite movement that can no longer be segmented (4). Also, there appear to be two types
of moving back to the left awaiting posture: one which needs untwisting of the humerus rotation
degree of freedom (6), and another which purely employs shoulder degrees of freedom (2).
The action boundaries estimated by the model are in strong agreement with manual visual segmen-
tation, with the exception of movements 4 that should be segmented into two separate movements.
At the web-page http://silviac.yolasite.com we provide a visual interpretation of the segmentation
from which the model accuracy can be appreciated.

10This is the smallest value that ensures that complete actions can be observed.

7

−2.502.5Positions−4.704.0Velocities327272715151472715156715156715156727151567271515427131315675 Conclusions

In this paper we have introduced a probabilistic model for detecting repeated occurrences of ba-
sic movements in time-series data. This model may potentially be applicable in domains such
as robotics, sports sciences, physical therapy, virtual reality, artiﬁcial movie generation, computer
games, etc., for automatic extraction of the movement templates contained in a recording. We have
presented an evaluation on table tennis movements that we have recorded using a robot arm as hap-
tic input device, showing that the model is able to accurately segment the time-series into basic
movements that could be used for robot imitation learning.

Appendix

Constraints on z1:T
Consider an action starting at time 1 and ﬁnishing at time t with the constraints z1 ∈ {1, . . . , ι} and
zt ∈ {, . . . , M }. Suppose that zτ = m for τ ∈ {1, . . . , t − 1}. Then it must be
1. m ∈ {max[τ ,  − (t − τ )wmax ], . . . , min[ι + (τ − 1)wmax , M − (t − τ )]}.
2. zτ +1 ∈ {max[m + 1,  − (t − τ − 1)wmax ], . . . , min[m + wmax , M − (t − τ − 1)]}.
Therefore, we need to modify the original priors ˜ψ , ψ with time-dependent priors with zero values
outside the appropriate range.

References
[1] R. Boulic, B. Ulicny, and D. Thalmann. Versatile walk engine. Journal of Game Development,
1(1):29–52, 2004.
[2] S. Calinon, F. Guenter, and A. Billard. On learning, representing and generalizing a task in a
humanoid robot. IEEE Transactions on Systems, Man and Cybernetics, Part B, 37(2):286–298,
2007.
[3] G. Celeux and J. Diebolt. The SEM algorithm: A probabilistic teacher algorithm derived from
the EM algorithm for the mixture problem. Computational Statistics Quarterly, 2:73–82, 1985.
[4] S. Chiappa. Hidden Markov switching models with explicit regime-duration distribution. Un-
der submission.
[5] S. Chiappa, J. Kober, and J. Peters. Using Bayesian dynamical systems for motion template
libraries. In Advances in NIPS 21, pages 297–304, 2009.
[6] A. Coates, P. Abbeel, and A. Y. Ng. Learning for control from multiple demonstrations. In
Proceedings of ICML, pages 144–151, 2008.
[7] J. Durbin and S. J. Koopman. Time Series Analysis by State Space Methods. Oxford Univ.
Press, 2001.
[8] S. Fr ¨uhwirth-Schnatter. Data augmentation and dynamic linear models. Journal of Time-Series
Analysis, 15:183–202, 1994.
[9] A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor
primitives. In Advances in NIPS 15, pages 1547–1554, 2003.
[10] U. Kersting, P. McAlpine, B. Rosenhahn, H. Seidel, and R. Klette. Marker-less human motion
tracking opportunities for ﬁeld testing in sports. Journal of Biomechanics, 39:S191–S191,
2006.
[11] J. Listgarten, R. M. Neal, S. T. Roweis, and A. Emili. Multiple alignment of continuous time
series. In Advances in NIPS 17, pages 817–824, 2005.
[12] J. Listgarten, R. M. Neal, S. T. Roweis, R. Puckrin, and S. Cutler. Bayesian detection of
infrequent differences in sets of time series with shared structure. In Advances in NIPS 19,
pages 905–912, 2007.
[13] R. McDonnell, S. J˙.org, J. K. Hodgins, F. N. Newell, and C. O’Sullivan. Evaluating the effect
of motion and body shape on the perceived sex of virtual characters. ACM Transactions on
Applied Perception, 5(4), 2009.

8

[14] W. Pan and L. Torresani. Unsupervised hierarchical modeling of locomotion styles. In Pro-
ceedings of ICML, 2009.
[15] M. Peinado, D. Maupu, D. Raunhardt, D. Meziat, D. Thalmann, and R. Boulic. Full-body
avatar control with environment awareness. IEEE Computer Graphics and Applications, 29(3),
2009.
[16] W. Takano, K. Yamane, and Y. Nakamura. Capture database through symbolization, recogni-
tion and generation of motion patterns. In Proceedings of ICRA, pages 3092–3097, 2007.
[17] B. Williams, M. Toussaint, and A. Storkey. Modelling motion primitives and their timing in
biologically executed movements. In Advances in NIPS 20, pages 1609–1616, 2008.
[18] K. Yamane and J. K. Hodgins. Simultaneous tracking and balancing of humanoid robots for
imitating human motion capture data. In Proceedings of IROS, pages 2510–2517, 2009.

9


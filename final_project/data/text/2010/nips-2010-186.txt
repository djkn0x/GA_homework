Natural Policy Gradient Methods with
Parameter-based Exploration for Control Tasks

Atsushi Miyamae†‡ , Yuichi Nagata† , Isao Ono† , Shigenobu Kobayashi†
†: Department of Computational Intelligence and Systems Science
Tokyo Institute of Technology, Kanagawa, Japan
‡: Research Fellow of the Japan Society for the Promotion of Science
{miyamae@fe., nagata@fe., isao@, kobayasi@}dis.titech.ac.jp

Abstract

In this paper, we propose an efﬁcient algorithm for estimating the natural policy
gradient using parameter-based exploration; this algorithm samples directly in the
parameter space. Unlike previous methods based on natural gradients, our algo-
rithm calculates the natural policy gradient using the inverse of the exact Fisher
information matrix. The computational cost of this algorithm is equal to that of
conventional policy gradients whereas previous natural policy gradient methods
have a prohibitive computational cost. Experimental results show that the pro-
posed method outperforms several policy gradient methods.

1 Introduction

Reinforcement learning can be used to handle policy search problems in unknown environments.
Policy gradient methods [22, 20, 5] train parameterized stochastic policies by climbing the gradient
of the average reward. The advantage of such methods is that one can easily deal with continuous
state-action and continuing (not episodic) tasks. Policy gradient methods have thus been successfully
applied to several practical tasks [11, 21, 16].

In the domain of control, a policy is often constructed with a controller and an exploration strat-
egy. The controller is represented by a domain-appropriate pre-structured parametric function. The
exploration strategy is required to seek the parameters of the controller. Instead of directly perturb-
ing the parameters of the controller, conventional exploration strategies perturb the resulting control
signal. However, a signiﬁcant problem with the sampling strategy is that the high variance in their
gradient estimates leads to slow convergence. Recently, parameter-based exploration [18] strategies
that search the controller parameter space by direct parameter perturbation have been proposed, and
these have been demonstrated to work more efﬁciently than conventional strategies [17, 18, 13].
Another approach to speeding up policy gradient methods is to replace the gradient with the natural
gradient [2], the so-called natural policy gradient [9, 4, 15]; this is motivated by the intuition that
a change in the policy parameterization should not inﬂuence the result of the policy update. The
combination of parameter-based exploration strategies and the natural policy gradient is expected
to result in improvements in the convergence rate; however, such an algorithm has not yet been
proposed.

However, natural policy gradients with parameter-based exploration strategies have a disadvantage
in that the computational cost is high. The natural policy gradient requires the computation of
the inverse of the Fisher information matrix (FIM) of the policy distribution; this is prohibitively
expensive, especially for a high-dimensional policy. Unfortunately, parameter-based exploration
strategies tend to have higher dimensions than control-based ones. Therefore, the expected method
is difﬁcult to apply for realistic control tasks.

1

In this paper, we propose a new reinforcement learning method that combines the natural policy
gradient and parameter-based exploration. We derive an efﬁcient algorithm for estimating the natu-
ral policy gradient with a particular exploration strategy implementation. Our algorithm calculates
the natural policy gradient using the inverse of the exact FIM and the Monte Carlo-estimated gra-
dient. The resulting algorithm, called natural policy gradients with parameter-based exploration
(NPGPE), has a computational cost similar to that of conventional policy gradient algorithms. Nu-
merical experiments show that the proposed method outperforms several policy gradient methods,
including the current state-of-the-art NAC [15] with control-based exploration.

2 Policy Search Framework

We consider the standard reinforcement learning framework in which an agent interacts with a
Markov decision process. In this section, we review the estimation of policy gradients and describe
the difference between control- and parameter-based exploration.

2.1 Markov Decision Process Notation
At each discrete time t, the agent observes state st ∈ S , selects action at ∈ A, and then receives an
instantaneous reward rt ∈ (cid:60) resulting from a state transition in the environment. The state S and the
action A are both deﬁned as continuous spaces in this paper. The next state st+1 is chosen according
to the transition probability pT (st+1 |st , at ), and the reward rt is given randomly according to the
expectation R(st , at ). The agent does not know pT (st+1 |st , at ) and R(st , at ) in advance.
The objective of the reinforcement learning agent is to construct a policy that maximizes the agent’s
performance. A parameterized policy π(a|s, θ) is deﬁned as a probability distribution over an action
space under a given state with parameters θ . We assume that each θ ∈ (cid:60)d has a unique well-deﬁned
stationary distribution pD (s|θ). Under this assumption, a natural performance measure for inﬁnite
(cid:90)
(cid:90)
horizon tasks is the average reward
A
S

π(a|s, θ)R(s, a)dads.

pD (s|θ)

η(θ) =

2.2 Policy Gradients

(cid:80)∞
Policy gradient methods update policies by estimating the gradient of the average reward w.r.t. the
t=1 rt − η(θ)|s1 = s, a1 = a, θ], and
policy parameters. The state-action value is Qθ (s, a) = E [
it is assumed that π(a|s, θ) is differentiable w.r.t. θ . The exact gradient of the average reward (see
(cid:90)
(cid:90)
[20]) is given by
A

π(a|s, θ)∇θ log π(a|s, θ)Qθ (s, a)dads.

∇θ η(θ) =

pD (s|θ)

S

(1)

The natural gradient [2] has a basis in information geometry, which studies the Riemannian geomet-
ric structure of the manifold of probability distributions. A result in information geometry states that
the FIM deﬁnes a Riemannian metric tensor on the space of probability distributions [3] and that the
direction of the steepest descent on a Riemannian manifold is given by the natural gradient, given by
the conventional gradient premultiplied by the inverse matrix of the Riemannian metric tensor [2].
Thus, the natural gradient can be computed from the gradient and the FIM, and it tends to converge
faster than the conventional gradient.

Kakade [9] applied the natural gradient to policy search; this was called as the natural policy gra-
dient. If the FIM is invertible, the natural policy gradient ˜∇θ η(θ) ≡ F−1
θ ∇θ η(θ) is given by the
(cid:90)
(cid:90)
policy gradient premultiplied by the inverse matrix of the FIM Fθ . In this paper, we employ the FIM
proposed by Kakade [9], deﬁned as
pD (s|θ)
A

π(a|s, θ)∇θ log π(a|s, θ)∇θ log π(a|s, θ)Tdads.

Fθ =

S

2

Figure 1: Illustration of the main difference between control-based exploration and parameter-based
exploration. The controller ψ(u|s, w) is represented by a single-layer perceptron. While the control-
based exploration strategy (left) perturbs the resulting control signal, the parameter-based explo-
ration strategy (right) perturbs the parameters of the controller.

2.3 Learning from Samples
The calculation of (1) requires knowledge of the underlying transition probabilities pD (s|θ).
The GPOMDP algorithm [5] instead computes a Monte Carlo approximation of (1):
the
agent interacts with the environment, producing an observation, action, and reward sequence
{s1 , a1 , r1 , s2 , ..., sT , aT , rT }. Under mild technical assumptions, the policy gradient approxima-
T(cid:88)
tion is
∇θ η(θ) ≈ 1
rtzt ,
T
t=1
where zt = β zt−1 + ∇θ log π(at |st , θ) is called the eligibility trace [12], ∇θ log π(at |st , θ) is
called the characteristic eligibility [22], and β denotes the discount factor (0 ≤ β < 1). As β → 1,
the estimation approaches the true gradient 1 , but the variance increases (β is set to 0.9 in all
θ ∇θ log π(at |st , θ). Therefore, the natural policy
experiments). We deﬁne ˜∇θ log π(at |st , θ) ≡ F−1
T(cid:88)
T(cid:88)
gradient approximation is
˜∇θ η(θ) ≈ 1
1
F−1
rt ˜zt ,
θ rtzt =
T
T
t=1
t=1
where ˜zt = β ˜zt−1 + ˜∇θ log π(at |st , θ). To estimate the natural policy gradient, the heuristic sug-
gested by Kakade [9] used
Fθ,t = (1 − 1
1
(∇θ log π(at |st , θ)∇θ log π(at |st , θ)T + λI),
t
t
the online estimate of the FIM, where λ is a small positive constant.

)Fθ,t−1 +

(2)

(3)

2.4 Parameter-based Exploration
In most control tasks, we attempt to have a (deterministic or stochastic) controller ψ(u|s, w) and
an exploration strategy, where u ∈ U ⊆ (cid:60)m denotes control and w ∈ W ⊆ (cid:60)n , the parameters
of the controller. The objective of learning is to seek suitable values of the parameters w, and
the exploration strategy is required to carry out stochastic sampling near the current parameters. A
typical exploration strategy model, we call control-based exploration, would be a normal distribution
(cid:181)
(cid:182)
for the control space (Figure1 (left)). In this case, the action of the agent is control, and the policy is
represented by
− 1
1
(u − ψ(s, w))TΣ−1 (u − ψ(s, w))
: S → U ,
πU (u|s, θ) =
exp
(2π)m/2 |Σ|1/2
2
where Σ is the m × m covariance matrix and the agent seeks θ = (cid:104)w, Σ(cid:105). The control at time t is
generated by

˜ut = ψ(st , w),
ut ∼ N ( ˜ut , Σ).
1 [5] showed that the approximation error is proportional to (1− β )/(1− |κ2 |), where κ2 is the sub-dominant
eigenvalue of the Markov chain

3

One useful feature of such a Gaussian unit [22] is that the agent can potentially control its degree of
exploratory behavior.

The control-based exploration strategy samples near the output of the controller. However, the
structures of the parameter space and the control space are not always identical. Therefore, the
sampling strategy generates controls that are not likely to be generated from the current controller,
even if the exploration variances decrease. This property leads to large variance gradient estimates.
This might be one reason why the policy improvement gets stuck.
To address this issue, Sehnke et al. [18] introduced a different exploration strategy for policy gradient
(cid:182)
(cid:181)
methods called policy gradients with parameter-based exploration (PGPE). In this approach, the
action of the agent is the parameters of the controller, and the policy is represented by
1
− 1
πW ( ˜w|s, θ) =
( ˜w − w)T ˜Σ−1 ( ˜w − w)
: S → W ,
(2π)n/2 | ˜Σ|1/2
2
where ˜Σ is the n × n covariance matrix and the agent seeks θ = (cid:104)w, ˜Σ(cid:105). The controller is included
in the dynamics of the environment, and the control at time t is generated by
˜wt ∼ N (w, ˜Σ),
ut = ψ(st , ˜wt ).
GPOMDP-based methods can estimate policy gradients such as partially observable settings, i.e., the
policy πW ( ˜w|s, θ) excludes the observation of the current state. Because this exploration strategy
directly perturbs the parameters (Figure1 (right)), the samples are generated near the current param-
eters under small exploration variances. Note that the advantage of this framework is that because
the gradient is estimated directly by sampling the parameters of the controller, the implementation
of the policy gradient algorithms does not require ∂
∂ θ ψ , which is difﬁcult to derive from complex
controllers.

exp

Sehnke et al. [18] demonstrated that PGPE can yield faster convergence than the control-based ex-
ploration strategy in several challenging episodic tasks. However, the parameter-based exploration
tends to have a higher dimension than the control-based one. Therefore, because of the computa-
tional cost of the inverse of Fθ calculated by (3), natural policy gradients ﬁnd limited applications.

3 Natural Policy Gradients with Parameter-based Exploration

In this section, we propose a new algorithm called natural policy gradients with parameter-based
exploration (NPGPE) for the efﬁcient estimation of the natural policy gradient.

Implementation of Gaussian-based Exploration Strategy
3.1
We employ the policy representation model µ( ˜w|θ), a multivariate normal distribution with parame-
ters θ = (cid:104)w, C(cid:105), where w represents the mean and C, the Cholesky decomposition of the covariance
matrix ˜Σ such that C is an n × n upper triangular matrix and ˜Σ = CTC. Sun et al. [19] noted
two advantages of this implementation: C makes explicit the n(n + 1)/2 independent parameters
determining the covariance matrix ˜Σ; in addition, the diagonal elements of C are the square roots of
the eigenvalues of ˜Σ, and therefore, CTC is always positive semideﬁnite. In the remainder of the
text, we consider θ to be an [n(n + 3)/2]-dimensional column vector consisting of the elements of
w and the upper-right elements of C, i.e.,

θ = [wT , (C1:n,1 )T , (C2:n,2 )T , ..., (Cn:n,n )T ]T .
Here, Ck:n,k is the sub-matrix in C at row k to n and column k .

3.2

Inverse of Fisher Information Matrix

Previous natural policy gradient methods [9] use the empirical FIM, which is estimated from a
sample path. Such methods are highly inefﬁcient for µ( ˜w|θ) to invert the empirical FIM, a matrix
with O(n4 ) elements. We avoid this problem by directly computing the exact FIM.

4

Algorithm 1 Natural Policy Gradient Method with Parameter-based Exploration
Require: θ = (cid:104)w, C(cid:105): policy parameters, ψ(u|s, w): controller, α: step size, β : discount rate, b:
baseline.
1: Initialize ˜z0 = 0, observe s1 .
2: for t = 1, ... do
Draw ξt ∼ N (0, I), compute action ˜wt = CT ξt + w.
3:
Execute ut ∼ ψ(ut |st , ˜wt ), obtain observation st+1 and reward rt .
4:
t ) − 1
˜∇w log µ( ˜wt |θ) = ˜wt − w, ˜∇C log µ( ˜wt |θ) = {triu(ξt ξT
t ) − 1
2 I}C
2 diag(ξt ξT
5:
˜zt = β ˜zt−1 + ˜∇θ log µ( ˜wt |θ)
6:
θ ← θ + α(rt − b)˜zt
7:
8: end for

Fθ =

S

(cid:90)
(cid:90)
Substituting π = µ( ˜w|θ) into (1), we can rewrite the policy gradient to obtain
∇θ η(θ) =
pD (s|θ)
µ( ˜w|θ)∇θ log µ( ˜w|θ)Qθ (s, ˜w)d ˜wds.
(cid:90)
(cid:90)
W
S
Furthermore, the FIM of this distribution is
(cid:90)
pD (s|θ)
µ( ˜w|θ)∇θ log µ( ˜w|θ)∇θ log µ( ˜w|θ)Td ˜wds
W
µ( ˜w|θ)∇θ log µ( ˜w|θ)∇θ log µ( ˜w|θ)Td ˜w.
=
W
Because Fθ is independent of pD (s|θ), we can use the real FIM.
Sun et al. [19] proved that the precise FIM of the Gaussian distribution N (w, CTC) becomes a
(cid:184)
(cid:183)
block-diagonal matrix diag(F0 , ..., Fn ) whose ﬁrst block F0 is identical to ˜Σ−1 and whose k-th
(1 ≤ k ≤ n) block Fk is given by
(cid:184)
(cid:183)
c−2
k,k 0
= [0 I¯k ] C−1 (cid:161)
(cid:162)
+ ˜Σ−1
Fk =
0
0
k:n,k:n
0
I¯k
where vk denotes an n-dimensional column vector of which the only nonzero element is the k-th
element that is one, and I¯k is the [n − k + 1]-dimensional identity matrix.
Further, Akimoto et al. [1] derived the inverse matrix of the k-th diagonal block Fk of the FIM.
(cid:181)
(cid:184)
(cid:184)(cid:182)
(cid:183)
(cid:183)
Because Fθ is a block-diagonal matrix and C is upper triangular, it is easy to verify that the inverse
matrix of the FIM is
− 1
0
0
(cid:183)
(cid:184)
0 I¯k
2

F−1
k = [0 I¯k ] CT
(cid:184)

where we use

k + I
vk vT

k +
vk vT

C

0
I¯k

,

C−T

,

(cid:183)
0
0
0 I¯k

vT
k C

C−1 = vT
k and [0 I¯k ] C

0
0
0 I¯k

C−1 = [0 I¯k ] .

(4)

3.3 Natural Policy Gradient
Now, we derive the eligibility premultiplied by the inverse matrix of the FIM ˜∇θ log µ( ˜wt |θ) =
θ ∇θ log µ( ˜wt |θ) in the same manner as [1]. The characteristic eligibility w.r.t. w is given by
F−1
∇w log µ( ˜wt |θ) = ˜Σ−1 ( ˜wt − w).
0 = ˜Σ and ˜∇w log µ( ˜wt |θ) = F−1
0 ∇w log µ( ˜wt |θ) = ˜wt − w. The characteristic
Obviously, F−1
(cid:162)
(cid:161)
eligibility w.r.t. C is given by
log µ( ˜wt |θ) = vT
∂
i
∂ ci,j

triu(YtC−T ) − diag(C−1 )

vj ,

5

Figure 2: Performance of NPG(w) as compared to that of NPG(u), VPG(w), and VPG(u) in the
linear quadratic regulation task averaged over 100 trials. Left: The empirical optimum denotes the
mean return under the optimum gain. Center and Right: Illustration of the main difference between
control- and parameter-based exploration. The sampling area of 1σ in the state-control space (center)
and the state-parameter space (right) is plotted.

where triu(YtC−T ) denotes the upper triangular matrix whose (i, j ) element is identical to the
(i, j ) element of YtC−T if i ≤ j and zero otherwise, and Yt = C−T ( ˜wt − w)( ˜wt − w)TC−1 is
a symmetric matrix.
(cid:161)
(cid:162)
Let ck = (ck,k , ..., ck,n )T (of dimension n + 1 − k); then, the characteristic eligibility w.r.t. ck is
expressed as
C−1Yt − diag(C−1 )
∇ck log µ( ˜wt |θ) = [0 I¯k ]
(cid:184)
(cid:183)
According to (4), diag(C−1 )vk = c−1
k,k vk and

vk .

(cid:162)
C−1Yt − diag(C−1 )

0
0
Cvk = ck,k vk ,
0 I¯k
(cid:184) (cid:161)
(cid:183)
(cid:184)(cid:182)
(cid:184)(cid:182)
0
0
0 I¯k
(Yt − I)vk .

k Cvk = ck,k and
vT
θ ∇θ log µ( ˜wt |θ) is therefore
the k-th block of F−1
(cid:181)
(cid:183)
k ∇ck log µ( ˜wt |θ)
˜∇ck log µ( ˜wt |θ) = F−1
(cid:181)
(cid:183)
− 1
0
0
= [0 I¯k ] CT
k +
vk vT
0 I¯k
2
− 1
0
0
(cid:180)
(cid:179)
= [0 I¯k ] CT
k +
vk vT
0 I¯k
2
(cid:181)
˜∇C log µ( ˜wt |θ)
Because ˜∇ck log µ( ˜wt |θ)T =
, we obtain
k,k:n
triu(Yt ) − 1
diag(Yt ) − 1
˜∇C log µ( ˜wt |θ) =
2
2
Therefore, the time complexity of computing
˜∇θ log µ( ˜wt |θ) = [ ˜∇w log µ( ˜wt |θ)T , ˜∇c1 log µ( ˜wt |θ)T , ..., ˜∇cn log µ( ˜wt |θ)T ]T
is O(n3 ), which is of the same order as the computation of ∇θ log µ( ˜wt |θ). This is a signi ﬁcant im-
provement over the current natural policy gradient estimation using (2) and (3) with parameter-based
exploration, whose complexity is O(n6 ). Note that more simple forms for exploration distribution
could be used. When we use the exploration strategy that is represented as an independent normal
distribution for each parameter wi in w, the natural policy gradient is estimated in O(n) time. This
limited form ignores the relationship between parameters, but it is practical for high-dimensional
controllers.

(cid:182)
I

C.

C

vk

(5)

3.4 An Algorithm
For a parameterized class of controllers ψ(u|s, w), we can use the exploration strategy µ( ˜w|θ). An
online version based on the GPOMDP algorithm of this implementation is shown in Algorithm 1. In
practice, the parameters of the controller ˜wt are generated by ˜wt = CT ξt + w, where ξt ∼ N (0, I)
are normal random numbers. Now, we can instead use Yt = C−T ( ˜wt −w)( ˜wt −w)TC−1 = ξt ξT
t .
To reduce the variance of the gradient estimation, we employ variance reduction techniques [6] to
adapt the reinforcement baseline b.

6

-2-1.5-1-0.5 0103104105106mean returnstepempirical optimumVPG(w)VPG(u)NPG(w)NPG(u)-4-3-2-1 0 1 2 3 4-4-3-2-1 0 1 2 3 4controlstatemeanparamter-basedcontrol-based-2-1.5-1-0.5 0 0.5 1-4-3-2-1 0 1 2 3 4paramterstatemeanparamter-basedcontrol-basedFigure 3: Simulator of a two-link arm robot.

4 Experiments

In this section, we evaluate the performance of our proposed NPGPE method. The efﬁciency of
parameter-based exploration has been reported for episodic tasks [18]. We compare parameter- and
control-based exploration strategies with natural gradient and conventional ”vanilla” gradients using
a simple continuing task as an example of a linear control problem. We also demonstrate NPGPE’s
usefulness for a physically realistic locomotion task using a two-link arm robot simulator.

4.1

Implementation

We compare two different exploration strategies. The ﬁrst is the parameter-based exploration strat-
egy µ( ˜w|θ) presented in Section 3.1. The second is the control-based exploration strategy (u| ˜u, D)
represented by a normal distribution for a control space, where ˜u is the mean vector of the control
generated by controller ψ and D represents the Cholesky decomposition of the covariance matrix
Σ such that D is an m × m upper triangular matrix and Σ = DTD. The parameters of the policy
πU (u|s, θ) are θ = (cid:104)w, D(cid:105) to be an [n + m(m + 1)/2]-dimensional column vector consisting of the
elements of w and the upper-right elements of D.

4.2 Linear Quadratic Regulator

The following linear control problem can serve as a benchmark of delayed reinforcement tasks [10].
The dynamics of the environment is

st+1 = st + ut + δ,
where s ∈ (cid:60)1 , u ∈ (cid:60)1 , and δ ∼ N (0, 0.52 ). The immediate reward is given by rt = −s2
t − u2
t . In
this experiment, the set of possible states is constrained to lie in the range [-4, 4], and st is truncated.
(cid:112)
When the agent chooses an action that does not lie in the range [−4, 4], the action executed in the
environment is also truncated. The controller is represented by ψ(u|s, w) = s · w, where w ∈ (cid:60)1 .
4β 2 + 1) − 1 from the Riccati equation.
The optimal parameter is given by w∗ = 2/(1 + 2β +
For clariﬁcation, we now write an NPG that employs the natural policy gradient and a VPG that em-
ploys the ”vanilla” policy gradient. Therefore, NPG(w) and VPG(w) denote the use of the parameter-
based exploration strategy, and NPG(u) and VPG(u) denote the use of the control-based exploration
strategy. Our proposed NPGPE method is NPG(w).

Figure2 (left) shows the performance of all compared methods. We can see that the algorithm using
parameter-based exploration had better performance than that using control-based exploration in the
continuing task. The natural policy gradient also improved the convergence speed, and a combina-
tion with parameter-based exploration outperformed all other methods. The reason for the accel-
eration in learning in this case may be the fact that the samples generated by the parameter-based
exploration strategy allow effective search. Figure2 (center and right) show plots of the sampling
area in the state-control space and the state-parameter space, respectively. Because control-based
exploration maintains the sampling area in the control space, the sampling is almost uniform in the
parameter space at around s = 0, where the agent visits frequently. Therefore, the parameter-based
exploration may realize more efﬁcient sampling than the control-based exploration.

4.3 Locomotion Task on a Two-link Arm Robot

We applied the algorithm to the robot shown in Figure3 of Kimura et al. [11]. The objective of
learning is to ﬁnd control rules to move forward. The joints are controlled by servo motors that react

7

(cid:113)(cid:80)
Figure 4: Performance of NPG(w) as compared to that of NPG(u) and NAC(u) in the locomotion
task averaged over 100 trials. Left: Mean performance of all compared methods. Center: Parameters
of controller for NPG(w). Right: Parameters of controller for NPG(u). The parameters of the
controller are normalized by gain i =
j wi,j and weight i,j = wi,j /gain i , where wi,j denotes
the j -th parameter of the i-th joint. Arrows in the center and right denote the changing points of the
relation between two important parameters.

to angular-position commands. At each time step, the agent observes the angular position of two
The control for motor i is generated by ui = 1/(1 + exp(− (cid:80)
motors, where each observation o1 , o2 is normalized to [0, 1], and selects an action. The immediate
reward is the distance of the body movement caused by the previous action. When the robot moves
backward, the agent receives a negative reward. The state vector is expressed as s = [o1 , o2 , 1]T .
j sj wi,j )). The dimension of the
parameters of the policies is dW = n(n + 3)/2 = 27 and dU = n + m(m + 1)/2 = 9 for the
parameter- and control-based exploration strategy, respectively.

We compared NPG(w), i.e., NPGPE, with NPG(u) and NAC(u). NAC is the state-of-the-art policy
gradient algorithm [15] that combines natural policy gradients, actor-critic framework, and least-
squares temporal-difference Q-learning. NAC computes the inverse of a d × d matrix to estimate the
natural steepest ascent direction. Because NAC(w) has O(d3
W ) time complexity for each iteration,
which is prohibitively expensive, we apply NAC to only control-based exploration.

Figure4 (left) shows our results. Initially, NPG(w) is outperformed by NAC(u); however, it then
reaches good solutions with fewer steps. Furthermore, at a later stage, NAC(u) matches NPG(u).
Figure4 (center and right) show the path of the relation between the parameters of the controller.
NPG(w) is much slower than NPG(u) to adapt the relation at an early stage; however, it can seek the
relations of important parameters (indicated by arrows in the ﬁgures) faster, whereas NPG(u) gets
stuck because of inefﬁcient sampling.

5 Conclusions

This paper proposed a novel natural policy gradient method combined with parameter-based ex-
ploration to cope with high-dimensional reinforcement learning domains. The proposed algorithm,
NPGPE, is very simple and quickly calculates the estimation of the natural policy gradient. More-
over, the experimental results demonstrate a signiﬁcant improvement in the control domain.

Future works will focus on developing actor-critic versions of NPGPE that might encourage perfor-
mance improvements at an early stage, and on combining other gradient methods such as natural
conjugate gradient methods [8].

In addition, a comparison with other direct parameter perturbation methods such as ﬁnite difference
gradient methods [14], CMA-ES [7], and NES [19] will be necessary to gain a better understanding
of the properties and efﬁcacy of the combination of parameter-based exploration strategies and the
natural policy gradient. Furthermore, the application of the algorithm to real-world problems is
required to assess its utility.

Acknowledgments

This work was suported by the Japan Society for the Promotion of Science (22 9031).

8

 0 1 2 3 4 5 6104105106107mean returnstepNPG(w)NPG(u)NAC(u)100101102102103104105106107gainstep-1-0.500.51weight100101102102103104105106107gainstep-1-0.500.51weightReferences

[1] Youhei Akimoto, Yuichi Nagata, Isao Ono, and Shigenobu Kobayashi. Bidirectional Relation
between CMA Evolution Strategies and Natural Evolution Strategies. Parallel Problem Solving
from Nature XI, pages 154 –163, 2010.
[2] S. Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):251–
276, 1998.
[3] S. Amari and H. Nagaoka. Methods of Information Geometry. American Mathematical Society,
2007.
[4] J. Andrew Bagnell and Jeff Schneider. Covariant policy search. In IJCAI’03: Proceedings of
the 18th international joint conference on Artiﬁcial intelligence , pages 1019–1024, 2003.
[5] Jonathan Baxter and Peter L. Bartlett. Inﬁnite-horizon policy-gradient estimation.
Journal of
Artiﬁcial Intelligence Research , 15:319–350, 2001.
[6] Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for
gradient estimates in reinforcement learning. The Journal of Machine Learning Research,
5:1471–1530, 2004.
[7] V. Heidrich-Meisner and C. Igel. Variable metric reinforcement learning methods applied to
the noisy mountain car problem. In EWRL 2008, pages 136–150, 2008.
[8] Antti Honkela, Matti Tornio, Tapani Raiko, and Juha Karhunen. Natural conjugate gradient in
variational inference. In ICONIP 2007, pages 305–314, 2008.
[9] S. A. Kakade. A natural policy gradient. In In Advances in Neural Information Processing
Systems, pages 1531–1538, 2001.
[10] H. Kimura and S. Kobayashi. Reinforcement learning for continuous action using stochastic
gradient ascent. In Intelligent Autonomous Systems (IAS-5), pages 288–295, 1998.
[11] Hajime Kimura, Kazuteru Miyazaki, and Shigenobu Kobayashi. Reinforcement learning in
pomdps with function approximation. In ICML ’97: Proceedings of the Fourteenth Interna-
tional Conference on Machine Learning, pages 152–160, 1997.
[12] Hajime Kimura, Masayuki Yamamura, and Shigenobu Kobayashi. Reinforcement learning by
stochastic hill climbing on discounted reward. In ICML, pages 295–303, 1995.
[13] Jens Kober and Jan Peters. Policy search for motor primitives in robotics.
Neural Information Processing Systems 21, pages 849–856, 2009.
[14] Jan Peters and Stefan Schaal. Policy Gradient Methods for Robotics.
In 2006 IEEE/RSJ
International Conference on Intelligent Robots and Systems, pages 2219–2225, 2006.
[15] Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7–9):1180–1190, 2008.
[16] Silvia Richter, Douglas Aberdeen, and Jin Yu. Natural actor-critic for road trafﬁc optimisa-
tion. In Advances in Neural Information Processing Systems 19, pages 1169–1176. MIT Press,
Cambridge, MA, 2007.
[17] Thomas R ¨uckstieß, Martin Felder, and J ¨urgen Schmidhuber. State-dependent exploration for
policy gradient methods. In ECML PKDD ’08: Proceedings of the European conference on
Machine Learning and Knowledge Discovery in Databases - Part II, pages 234–249, 2008.
[18] Frank Sehnke, C Osendorfer, T Rueckstiess, A. Graves, J. Peters, and J. Schmidhuber. Policy
gradients with parameter-based exploration for control. In Proceedings of the International
Conference on Artiﬁcial Neural Networks (ICANN) , pages 387–396, 2008.
[19] Yi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Efﬁcient natural evolution
strategies. In GECCO ’09: Proceedings of the 11th Annual conference on Genetic and evolu-
tionary computation, pages 539–546, 2009.
[20] R. S. Sutton. Policy gradient method for reinforcement learning with function approximation.
In Advances in Neural Information Processing Systems, volume 12, pages 1057–1063, 2000.
[21] Daan Wierstra, Er Foerster, Jan Peters, and Juergen Schmidhuber. Solving deep memory
pomdps with recurrent policy gradients. In In International Conference on Artiﬁcial Neural
Networks, 2007.
[22] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist rein-
forcement learning. In Machine Learning, pages 229–256, 1992.

In Advances in

9


Autonomous Generation of Bounding Boxes for Image Sets of the Same Ob ject

Shui Hu, Jean Feng, Marc Rasi

December 10, 2010

1

Introduction

One of the challenges of computer vision research is to achieve accurate autonomous ob ject recognition. This pro ject focuses
on the speciﬁc problem of ﬁnding bounding boxes of a given ob ject in a set of pictures, each of which is guaranteed to contain
this ob ject. We run our experiments on the image database ImageNet, developed by Stanford’s Vision Lab.
In ImageNet,
each "synset" is a group of about 1000 images of a certain ob ject where the bounding boxes are labeled manually via Amazon
Mechanical Turk. Since this method is not scalable, we propose an algorithm that generates bounding shapes and can partially
eliminate the need for human labor. In this paper, we compare this new algorithm DetectRec, which relies on high-level image
cues such as ob ject shape, to our initial algorithm that relies on low-level cues like HOG.

2 An Initial Algorithm using Low-Level Cues

The initial algorithm we implemented relied on ﬁnding potential bounding boxes based on its “ob jectness” score and then learning
HOG features to identify boxes that contained an ob ject of the desired class. The Ob jectness measure, as described in the paper
“What is an Ob ject?” by Alexe, Desealers, Ferrari, [1] tries to quantify how likely an image window contains an ob ject of any
class based on multi-scale saliency, color contrast, edge density, and closed boundary characteristics.
Our initial algorithm is as follows:

1. Use Ob jectness measure to automatically collect bounding boxes for simple training images (single ob ject against monochrome
background) and generate HOG features for each box

2. Fit a Gaussian distribution for the training data

3. Repeat step 1 on test images, getting 50 candidate bounding boxes per image

4. Use the gaussian distribution to calculate the probability of each of the bounding boxes being correct bounds for the class

As we will show in our results and discussions, this algorithm did not perform very well, which suggests that the gaussian
distribution did not ﬁt the HOG data well, either because there was not enough data or because it is the wrong model to use
in general. So we developed a new algorithm that follows the same general framework of ﬁnding general ob jects and then using
high-level features to identify ob jects belonging to the speciﬁc class.

3 DetectRec Algorithm

DetectRec takes a high-level approach to the ob ject localization problem. in essence, the algorithm only takes into account the
ob ject shape once given the ob ject segmentations. An outline for the algorithm is as follows:

1. Learn generic shape models of an ob ject class by running k-means on the ob ject segmentations from the class.

2. For each image:
a. Use hierarchical ob ject detection (explained below) to ﬁnd potential ob ject.
b. Select the ob ject that is most similar to one of the generic shapes, and draw a tight bounding box around it.

1

Autonomous Generation of Bounding Boxes for Image Sets of the Same Ob ject

Shui Hu, Jean Feng, Marc Rasi

3.1 Learning Shape Models

Given an ob ject segmentation S , the algorithm ﬁrst approximates S with a 20-gon P , which is further simpliﬁed to the vector
(N , C, R) where
• N is the number of segments that are convex, concave, or straight relative to the ob ject. A segment is a set of consecutive
angles of P that have the same convexity (greater than, close to, or less than π) relative to P .
• C is a vector denoting which segments belonging to which class of convexity
• R is vector denoting the regularized version of each angle. Speciﬁcally, the regularized angle is the average of the angles
belonging to that continuous section of the same convexity/concavity. If the segment is straight, the regularized version is
π .

The intuition behind using an angle of vectors is that our shape model can then be made invariant to rescaling, reﬂections, and
rotations. The vector (N , C, R) is useful since it captures the characteristics at diﬀerent levels of generality, where N is the
highest level and R is the lowest.

4 Shape Similarity

From this shape model, there are two ways we can measure shape similarity between two diﬀerent shapes (N (i) , C (i) , R(i) ) and
(N (j ) , C (j ) , R(j ) ). First is simply the diﬀerence in number of segments, which we denote N D(i,j ) .
N D(i,j ) = |N (i) − N (j ) |
The second measure of shape similarity combines the diﬀerence between convexity and angles in the two shape vectors.
In
order to make this metric invariant to rotations and reﬂections, we take the minimum distance over all possible rotations and

(cid:33)−1  (cid:88)
(cid:32) 20(cid:88)
reﬂections of one of the shape vectors. We denote this measure by SC (i,j ) and deﬁne it as
|R(∗)
1{C (∗)
k |
k − R(j )
k }
k = C (j )
C (∗)
k=1
k =C (j )
k
where (C (∗) , R(∗) ) ranges over all cyclic permutations and reversals of (C (i) , R(i) )
We put these two measures together by multiplying them. Smaller N D(i,j ) × SC i,j ) denotes greater similarity. If two comparisons
yield the same N D(i,j ) × SC (i,j ) (for example, many comparisons may have N D = 0), the one with smaller SC (i,j ) is more
similar. N D(i,j ) × SC (i,j ) takes into account both the diﬀerence in the matching segments of and the overall diﬀerence between
two shapes. The value N D accounts for the overall similarity between the two shapes while SC measures the degree of diﬀerence
in matching parts of the two shapes. The similarity score is also capable of identifying the similarities between a complete ob ject
and a partially occluded one.

SC = min
(C (∗) ,R(∗) )

5 Learning Generic Shapes

Given the set of training vectors containing the angles of 20-gon approximations, we cluster the vectors using k-means and return
the centroids as shapes to use in ﬁnding bounding boxes. We use N D × SC as the distance metric for k-means.
To ﬁnd the number of k-means clusters, we use the binary search algorithm. If there are m training examples, we start with
m clusters. For each pair of centroids, if the inter-cluster N D × SC is below a certain threshold, we decrease the number of
clusters. If the intra-cluster N D × SC between two vectors in one cluster is greater than a certain threshold, we increase the
number of clusters.
The centroids correspond to generic shapes for the ob ject class reﬂecting the intra-class variation due to angle of perception and
variation within the ob ject class itself.

2

Autonomous Generation of Bounding Boxes for Image Sets of the Same Ob ject

Shui Hu, Jean Feng, Marc Rasi

Figure 1: Demonstration of the ob ject segmentation algorithm at diﬀerent grid resolutions

6 General Ob ject Segmentation

Given an image, IM , this step produces a list, D , of candidate ob ject segmentations.
First, use k-means to cluster the colors of IM to just two colors, producing a binary image, B IM . This step reduces noise
because the background tends to blend into one color. Then run the following hierarchical ob ject detector on the binary image.

1. Initialize an empty list, D , of ob jects.

2. Divide the black and white image B IM into an 8x8 grid.

3. Set the color of each grid cell to the most common color in the image in the grid cell.

4. Reﬁne the shapes of ob jects in D by absorbing adjacent cells with the same color.

5. Add each contiguous set of cells of the same color to the list D of ob jects.

6. Repeat this process for ﬁner grids (16x16, 32x32, 64x64, 128x128).

At the end, D gives us a list of potential ob jects in IM . These ob jects tend to be the more prominent ob jects in the image
because they tend to have large regions of coherent color which corresponds to contiguous sets of cells of the same color.

7 Results

7.1 Analysis for First Algorithm

We ran the ﬁrst algorithm on pictures from 5 diﬀerent synsets (mangos, shoes, laptops, backpacks, and cameras). For each
synset, our algorithm outputted a trained gaussian model for HOG features of bounding boxes containing ob jects. From the
Ob jectness step of the algorithm, we get 50 candidate bounding boxes and then we classify bounding boxes as positive if they
had an overlap of at least 0.75 with a true (manually labeled) bounding box and as negative if they had no overlap of 0.75 with
a true bounding box. For both algorithms, overlap between two shapes A and B as
area(A ∩ B )
area(A ∪ B )

We then classiﬁed all the bounding boxes using our gaussian model and plotted ROC curves. Figure 2 shows the ROC curves
for the 5 diﬀerent synsets.
We also tried a few other things to get a better understanding of how our algorithm was working. We ﬁrst tried training on
manually labeled data (as opposed to the automatic labeling we normally do in step 2 of our algorithm), which is illustrated by
the green line in the mango ROC plot in ﬁgure 2.
To test our assumption that training on simple images produces better results than training on complicated images, we trained
our gaussian model with image sets of both types. Figure 2 also shows the ROC curves from these tests.
We also tried training on positive examples from one synset and then testing on testing images from a diﬀerent synset, to test if
our algorithm was learning what one speciﬁc type of ob ject looked like or if it was learning what ob jects look like “in general.”
Figure 3 shows the ROC curves from these tests.
Finally, we also plotted 30 ROC curves for testing on cameras after training on 10 through 300 training examples, to see how
our algorithm performance changed with number of training examples.

3

Autonomous Generation of Bounding Boxes for Image Sets of the Same Ob ject

Shui Hu, Jean Feng, Marc Rasi

Figure 2: ROC curves on 5 diﬀerent synsets. Also, training on complicated images.

Figure 3: Left, center: Training on one synset and testing on a diﬀerent synset. Right: ROC curves for varying number of
training examples

7.2 Discussion for First Algorithm

We can see from our plain ROC curves on the individual synsets that our algorithm does do better than chance, which is good.
So our algorithm is slightly helpful in picking out the positives and the negatives from the candidate boxes. But since there are
so few positives in the candidate boxes to begin with (we found that about 2% of the candidate boxes were positives), we are
still left with far more false positives than true positives. So we would like to know why the algorithm does not perform so well
and what would be required to improve it.
Figure 3 show that performance does not increase very much as the number of training examples increases. The lowest green
ROC curve is the ROC curve for 10 training examples, so our algorithm does improve a bit with number of training examples
in the range of 10 to 20 training examples. But additional training examples do not signiﬁcantly improve performance. This
suggests three reasons why our algorithm is not performing so well.

1. The automatically labeled positive training examples might be bad because ob jectness does not ﬁnd ob jects very well, even
in simple pictures.

2. The positive examples might all be so similar that our algorithm does not learn very much when it sees new examples.

3. Our model might not ﬁt the data very well.

Reason (1) is not the reason, as shown by the green line in ﬁgure 2. The green line shows how well our algorithm performs if
we manually label positive examples, and it is not signiﬁcantly diﬀerent from the red line which corresponds to automatically
labeled positive examples. So a combination of reasons (2) and (3) are probably making our algorithm perform poorly. Reason
(2) is reasonable that there is not much variation in the “simple” images of ob jects that we are using for our training examples.
Reason (3) is also likely since a single gaussian distribution probably does not model all the diﬀerent possible ways an ob ject
can appear. Therefore, our algorithm performs more poorly on laptops and backpacks, which are deformable ob jects and look
diﬀerent from diﬀerent angles, than on shoes, mangos, and cameras, which all look reasonably similar across all the images in
the test sets (either because Amazon Turk users collect similar images or the ob ject just looks the same from diﬀerent angles).
Although reason (2) suggests that training a model only on simple images does not generate a robust model, our data does
support the idea that, if we want to automatically generate bounding boxes without human intervention, we should start by
training on the simple images. Speciﬁcally, if we run our automatic algorithm on complicated images instead of on simple images,
we get the ROC curves shown in ﬁgure 2.

4

Autonomous Generation of Bounding Boxes for Image Sets of the Same Ob ject

Shui Hu, Jean Feng, Marc Rasi

Figure 4: Cluster centroids for the shapes used in the DetectRec algorithm on the shoe synset. Diﬀerent shoe shapes are distinctly
visible.

The last thing we tried was to train on examples of one set and test on examples from a diﬀerent synset, to see if our algorithm
was learning the appearance of speciﬁc ob jects versus general ob jects. The ROC curves from this are shown in 3. By training on
one synset and testing on a diﬀerent one, this algorithm performed slightly worse than when we trained and tested on the same
synsets. Yet, it was still better than chance. This suggests that this ﬁrst algorithm is using image cues from the speciﬁc ob jects
and characteristics of general ob jects. It is true that by training on shoes and testing on laptops, actually performed better than
when we trained on laptops and tested on laptops. This just suggests that the algorithm wasn’t able to learn anything speciﬁc
to laptops, which supports reason (3): our model can’t model all important features for complicated ob jects like laptops.

8 DetectRec Results and Analysis

We tested DetectRec on the shoe image synset. With an initial training set of 14 images and a test set of 100 images, DetectRec
achieved 55% accuracy overall. Since classiﬁcation is done in two steps, ﬁrst ﬁnding ob jects and then classifying the ob ject, we
further broke down these results:

1. Given the image test set, the ob ject of the desired class (shoe) was segmented in 71% of images.

2. Given a correct general-ob ject segmentation, the ob ject classiﬁer then correctly identiﬁed the desired ob ject in 77.46% of
the images.

Ob jects detected by the ob ject detector tended to be rather large and diﬀered signiﬁcantly in color from their surroundings.
Over half of the images where the ob ject detector failed to detect the correct ob ject were cases where the correct ob ject was
adjacent to or occluded by another similarly colored ob ject, so the ob ject segmentation algorithm was unable to separate them.
Therefore, DetectRec does suﬀer to some extent by forcing every image to be black and white.
We also tested the diﬀerence from adding more images. Using 30 training images, DetectRec found three representative shapes
and achieved 70% accuracy overall for 50 test images. By breaking down these results again, we found that the general-ob ject
segmentation algorithm achieved 78% accuracy while the ob ject-classiﬁer accuracy rose to 89.74% (once again, this is given that
the ob ject was segmented correctly in the ﬁrst place). This shows that even with a limited amount of data, the ob ject-classiﬁer
will achieve a decent enough accuracy to at least warrant a collaboration between humans and computers to ﬁnd bounding boxes
for images for large databases such as ImageNet.

9 Conclusion

The absolute accuracy of our approach does not compare very well to the state of the art algorithms (Desealers, Alexe, and Ferrari
achieve 70% accuracy by a similar metric to ours on a much more diﬃcult test set in [2]). Our paper show the advantages and
disadvantages to diﬀerent approaches for ob ject localization and can aid in generating bounding boxes for large image databases
such as ImageNet.

References

[1] Alexe, Bogdan. Desealers, Thomas. Ferrari, Vittorio. “What is an Ob ject?” Computer Vision Laboratory, ETH Zurich. IEEE
Computer Vision and Pattern Recognition (CVPR), San Francisco, June 2010.

[2] Alexe, Bogdan. Desealers, Thomas. Ferrari, Vittorio. “Localizing Ob jects While Learning Their Appearance. Computer Vision
Laboratory, ETH Zurich. European Conference on Computer Vision (ECCV), Crete, Greece, September 2010.

5


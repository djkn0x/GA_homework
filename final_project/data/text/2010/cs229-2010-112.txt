Soft Co-Clucstering Via Extension of PLSA

Dakan Wang

1

introduction

In recent years, co-clustering has emerged to be a powerful data mining tool for
two-dimensional co-occurrence and dyadic real data. Typical examples include
co-clustering word-document matrix, user-query matrix, etc. Co-clustering al-
gorithms always outperform the traditional one way clustering in problems with
sparse and high-dimensional co-occurrence data, since they simultaneously clus-
ter rows and columns of a matrix. Co-clustering has been applied to recommen-
dation systems, microarray analysis, etc.
There have been already many works on algorithms for co-clustering [1, 2, 3].
[1] proposed an information theoretical co-clustering algorithm. However, their
method is under the hard setting, i.e., it just allows one row or column to belong
to only one cluster. Such restrictions are not suitable in real world applications.
In real life, one ob ject can belong to more than one categories, and a soft co-
clustering framework allowing mixed membership may make more sense. It is
worth pointing out that [3] also proposed a soft co-clustering model. However,
their model is a bit complex. We will propose a very simple and intuitive model.
We would like to extend the PLSA to a soft co-clustering model which allows
that both columns and rows can belong to diﬀerent clusters. The idea is that
we add two latent sets of nodes to the model. The two latent sets correspond to
the clusters for rows and columns correspondingly. The training of the model
is similar to PLSA and is very eﬃcient and easy to implement.

2 Model

In this section, we will describe the 2-layer Probabilistic Latent Semantic Anal-
ysis(2PLSA) and a corresponding EM algorithm.
Following the idea of PLSA, we assume that the probability of a word w
(cid:88)
(cid:88)
given a document d is deﬁned as follows
p(w|z )p(z |s)p(s|d)
p(w|d) =
z∈Z
s∈S
Here z is the word cluster, and s is the document cluster. This equation can
be interpreted as follows
• Pick a document topic s from d with probability p(s|d)

(1)

1

,

2

• Pick a word topic z from s with probability p(z |s)
• Generate a word w from word topic z with probability p(w|z )
(cid:88)
From the above formulation, we can write the likelihood function to be
n(di , wj ) log p(wj |di )p(di )
(cid:88)
(cid:88)
i,j
(cid:88)
(cid:88)
i,j
k,l
i,j
k,l

p(wj |zk )p(zk |sl )p(sl |di )p(di )
p(wj |zk )p(zk |sl )p(sl |di )p(di )
q(zk , sl |di , wj )

q(zk , sl |di , wj )

n(di , wj ) log

n(di , wj ) log

L =

=

=

3

inference

We brieﬂy summarize the EM algorithm here. Detailed derivations are similar
to those instructed in the class.

3.1 E-Step

q(sl , zk |di , wj ) =

(cid:80)

p(wj |zk )p(zk |sl )p(sl |di )
k,l p(wj |zk )p(zk |sl )p(sl |di )

3.2 M-step

(cid:80)
p(sl , zk , di , wj ) = q(sl , zk |di , wj )p(di , wj )
(cid:80)
i,l p(sl , zk , di , wj )
(cid:80)
p(wj |zk ) =
i,j,l p(sl , zk , di , wj )
(cid:80)
i,j p(sl , zk , di , wj )
(cid:80)
i,j,k p(sl , zk , di , wj )
(cid:80)
j,k p(sl , zk , di , wj )
j,k,l p(sl , zk , di , wj )

p(zk |sl ) =

p(sl |di ) =

(2)

(3)

4 Equivalence with information theoretical co-clustering
and y are respectively clustered into (cid:98)x and (cid:98)y , the loss in mutual information is
[1] deﬁned a co-clustering algorithm from information theory perspective. If x
I (X ; Y ) − I ( (cid:98)X ; (cid:98)Y ) = D(p(X, Y )||q(X, Y ))
(4)
Here D(·||·) is the KL divergence, and q(X, Y ) is deﬁned to be
q(x, y) = p((cid:98)x.(cid:98)y)p(x|(cid:98)x)p(y |(cid:98)y)

(5)

,

3

where

(cid:88)
(cid:88)
p((cid:98)x, (cid:98)y) =
y∈(cid:98)y
x∈(cid:98)x
p(x, y)
p(x|(cid:98)x) =
p(y |(cid:98)y) =
p((cid:98)y)
p((cid:98)x)
p(x)
p(y)
Now we prove the equivalence of our model and that in [1] under the hard
(cid:88)
(cid:88)
setting. We rewrite the likelihood function of our model to be
log p(wj |zk )p(zk |sl )p(sl |di )p(di )
(cid:88)
(cid:88)
i,j
k,l
p(di |sl )p(sl )
p(zk , zl )
(cid:88)
(cid:88)
p(di )
p(sl )
i,j
k,l
p(wj |zk )p(zk , sl )p(di |sl )
n(di , wj ) log
i,j
k,l
(cid:88)
(cid:88)
(cid:88)
If we restrict the above equation to hard setting, we get
n(di , wj ) log p(wj |zk )p(zk , sl )p(di |zl )
di∈sl ,wj ∈zk
l
k

p(wj |zk )

n(di , wj ) log

n(di , wj )

=

=

(6)

(7)

(8)

L =

L =

p(di )

On the other side, in information theoretical co-clustering [1], we want to
minimize
(cid:88)
(cid:88)
(cid:88)
I (D , W ) − I (S, Z )
− (cid:88)
(cid:88)
(cid:88)
=
p(di , wj ) log p(di , wj )
di∈sl ,wj ∈zk
l
k
di∈sl ,wj ∈zk
k
l
Notice that tbe ﬁrst term is constant given a collection of documents and words,
(cid:88)
(cid:88)
(cid:88)
thus we only need to maximize
p(di , wj ) log p(wj |zk )p(zk , sl )p(di |sl )
di∈sl ,wj ∈zk
l
k

p(di , wj ) log p(wj |zk )p(zk , sl )p(di |sl )

(10)

(9)

which diﬀers from our maximum likelihood formulation by only a constant
scalar.

5 Experiments

5.1 Datasets

In order to evaluate our algorithm performance more extensively, we conducted
experiments using two datasets from diﬀerent domains. The ﬁrst domain is

,

4

Tab. 1: Extracted Topics
4
3
2
1
israel
god
drive
image
scsi
think
jew
graphical
arab
believe
ﬁle
mb
war
christ
program disk
fotmat
control
athetist
kill

5
space
nassa
orbit
launch
earth

the 20 Newsgroups dataset, which is a text collection of approximately 20,000
newsgroup documents across 20 diﬀerent newsgroups . Another dataset comes
from the Wikipedia XML dataset, from which we had sampled a subset of 9
document topics composing a total of 1,236 articles as what was done in [2].
Some preprocessing has been applied to on the raw text data. We had ﬁrstly
performed the Porter stemmer on terms and removed all the stop words from a
stop word list.

5.2 Word and Document Clusters

Our algorithm could return document and word clusters easily by doing a sorting
on the corresponding p(w|z ) and p(d|s). In this section, we would list the 5 most
possible words in 5 topics from the 20 Newsgroup dataset, such a table can be
directly constructed by sorting p(w|z ). The results are shown in Table 1. From
Table 1 and the ground truth labeling, we could see that topic 1 to topic 5
will correspond to comp.graphics, comp.sys.ibm.pc.hardware, talk.religion.misc,
talk.politics.mideast and sci.space. We could observe that the word clusters we
got are meaningful and representative of diﬀerent topics.

5.3 Document Clustering Performance

We compared our proposed 2PLSA in terms of clustering accuracy with two
baselines, ITCC and LDCC. We conducted our experiment on the Wikipedia
XML corpus to directly compare our model with the results in [2]. The metrics
they belong to the same class by calculating (cid:80)
we used are Precision, Recall and F1-Measure calculated over pairs of points.
In our co-clustering model, for two documents d1 and d2 , we determine whether
s p(s|d1 )p(s|d2 ) and if the value
is larger than 0.5, we consider d1 and d2 to belong to the same category. As
[2], we also tested the performance of our algorithm with the number of topics
set to be 15 and 20.
Table 2 shows the result of comparisons. It shows that our algorithm could
signiﬁcantly outperform the baseline methods, in terms of precision and F1-
Score. We also would like to add a side note that since our algorithm is based on
the more eﬃcient PLSA model, the computational complexity of our algorithm
is signiﬁcantly lower than that of LDCC.

,

5

Tab. 2: Performance Comparison
Precision Recall F1-Score
Algorithm S
48.83
70.38
38.56
15
2PLSA
LDCC
15
30.88
79.21
44.43
12.00
7.44
31.06
15
ITCC
48.75
70.17
37.32
20
2PLSA
44.09
75.50
31.10
20
LDCC
ITCC
20
31.06
6.16
10.28

Finally, we show the rearranged co-occurrence matrix after co-clustering.
We sampled a subset of 10 topics from 20 Newsgroup dataset with 4,980 doc-
uments and 24,864 words. The left subﬁgure of Figure 1 shows the original
word-document matrix and the right subﬁgure shows the re-ordered matrix by
arranging rows and columns according to the document cluster order and the
word cluster order, respectively. Note that in the right subﬁgure, the clusters are
not falling around the main diagonal since that requires doing topic alignment
between word topics and document topics.

Fig. 1: co-occurrence matrix before and after co-clustering

References

[1] Inderjit S. Dhillon, Subramanyam Mallela, and Dharmendra S. Modha.
Information-theoretic co-clustering.
In Proceedings of the ninth ACM SIGKDD
international conference on Know ledge discovery and data mining, KDD ’03, pages
89–98, New York, NY, USA, 2003. ACM.

[2] M.M. Shaﬁei and E.E. Milios. Latent dirichlet co-clustering. In Data Mining, 2006.
ICDM ’06. Sixth International Conference on, pages 542 –551, 2006.

[3] Hanhuai Shan and Arindam Banerjee. Bayesian co-clustering. In Proceedings of
the 2008 Eighth IEEE International Conference on Data Mining, pages 530–539,
Washington, DC, USA, 2008. IEEE Computer Society.


CS229 Final Project: Enhancing Automated Question Classiﬁcation

William Mee wmee@stanford.edu
Seung-Yeoul Yang syyang@stanford.edu

10 December 2010

Abstract

2 Question Datasets

This class project investigated improving classiﬁcation of
short questions. We ﬁrst established a baseline for fur-
ther work by comparing a number of diﬀerent oﬀ-the-shelf
classiﬁers. We gathered a new corpus to add to ones al-
ready available. We then implemented and applied semi-
supervised classiﬁcation, adding large amounts of unlabelled
data in an attempt to boost classiﬁcation accuracy. Fi-
nally we investigated feature expansion, both via semantic
knowledge and with augmentation by automatically learned
topics.

1

Introduction

This work investigated classiﬁcation of short, closed ques-
tions. Examples of these are

What was the United States’ ﬁrst national monu-
ment?
How do you say red in Spanish?

Apart from being short, such ’factoid’ questions are well-
structured: they generally have a question word, one verb
and one or two nouns. This makes question classiﬁcation
a domain distinct from document classiﬁcation. The ques-
tion classiﬁcation task is important as a part of a larger
automated question and answer system.
We began by analyzing the eﬀectiveness of several clas-
siﬁers, including Naive Bayes and Support Vector Machines
on labelled training and test questions. We then used this
as a baseline to explore the following series of experiments
to improve the accuracy of the classiﬁcation

1. A semi-supervised approach in which we applied Expec-
tation Maximization (EM) on a combination of labelled
and unlabelled questions

2. Feature expansion using lexical information

3. Feature expansion using automatically learned topics

Details of these experiments follow.

We made use of two corpora for this project. Firstly, tests
were done on the TREC-10 QA dataset [7]. This data has
5500 short questions which are labelled with 6 coarse cate-
gories (Abbreviation, Description, Entity, Location, Human
and Number) and 50 ﬁne subcategories. An example ques-
tion from this dataset is

When did the Berlin Wall go up ?

which is assigned to the coarse:ﬁne category NUM:date.
The second corpus is one which was gathered for this
work from Yahoo! Answers [9], the only online question sys-
tem we are aware of which has a public API. Answers data
is structured into a hierarchy of themes, for example ”En-
vironment/Global Warming”. We selected questions from
the History theme, since many of these are in closed form,
and manually labelled the data using the same categories
as the TREC-10 questions. The english questions down-
loaded were those marked as ’resolved’, reverse sorted by
number of responses. Although we ﬁltered out questions
which were not in closed form and de-duplicated, we did
not do spelling correction or make grammatical changes; as
such this Yahoo! History data is signiﬁcantly less regular
than the TREC-10 dataset.

3 Comparison of Supervised Classi-
ﬁers

An important initial step was to establish a baseline com-
parison for diﬀerent supervised classiﬁers on short ques-
tions. For algorithm implementations, we made use of
the Weka Machine Learning Project[8] . This allowed us
to quickly compare three diﬀerent classiﬁers (Naive Bayes,
Naive Bayes Multinomial and SVM). It also allowed us to
experiment with the eﬀects of feature selection, stopwords
and stemming.
In initial experiments, we ran the candi-
date classiﬁers against training subsets of various sizes and
measured the error rate of the resultant classiﬁer by using
10-fold cross-validation against the same training data.

1

Figure 1: Baseline Classiﬁer Comparison, Fine Categories

purposes in many other experiments.

4 Semi-Supervised
Using EM

Classiﬁcation

Semi-supervised classiﬁcation combines labelled and unla-
belled data to improve classiﬁcation accuracy. While this
is an active ﬁeld of research in machine learning in general
[10], there is little application of this to the question-answer
domain. However, the intuition that additional structure
implicitly provided by unlabelled data which could help clas-
siﬁcation seemed applicable. A key motivation is that with
question classiﬁcation, or text classiﬁcation in general, there
is an abundance of unlabelled input data, which we would
like to take advantage of for improving our classiﬁer. Our
own data collection eﬀort was proof of how diﬃcult and
tedious data labelling can be.
In this part of the project, we implemented the combina-
tion of a Naive Bayes Multinomial classiﬁer with Expecta-
tion Maximization in two variations suggested by Nigam et.
al[6]. The basic approach here is a generative one, which
supposes every document is generated from a probabilty
distribution parameterized by θ . The probabilty distribution
consists of a mixture of components cj ∈ C = {c1 , ..., c|C |}.
In addition, the training data D consists of labelled data,
D l , and unlabelled data, Du , such that D = D l ∪ Du .
Parameter estimation is done with the labelled data and
then repeatedly reﬁned by applying to the unlabelled data
and retraining in a form of Expectation Maximization:

1. Build an initial classiﬁer from D l only and calculate
parameters ˆθ

2. Repeat until convergence:
E-Step: Use the current ˆθ to classify Du
M-Step: Re-estimate ˆθ given estimated component
membership of all data

This approach is mathematically correct only for a Naive
Bayes Multinomial classiﬁer. This base algorithm is then
reﬁned in two variants detailed below.

4.1 EM-λ

Assuming there is one-to-one correspondence between a
class and a mixture, i.e., each class is generated by a sin-
gle mixture, the EM-λ algorithm suggested by Nigam et al
allows the contribution of unlabelled data to be weighted
to avoid dominance when the ratio of labelled to unlabelled
data is low. In this scenario the log likelihood equation can

3.1 Feature Selection, Stemming and Stop
Words

We experimented with using Mutual Information measures
to reduce the numbers of features. In all experiments, this
increased the error rate of the categorizing, and the fewer
features used the more the error rate increased. For ex-
ample, selecting the top 300 features for the Naive Bayes
Multinomial on a training set of 5500 questions reduced ac-
curacy from 76.4% to 75.6%; using only 100 features further
reduced this to 69.5%. One explanation of this is that the
questions being categorized are very short, so almost all the
limited number of features have some discriminating use.
We used all features for the rest of this project.
We also experimented with using a set of stop words
obtained from the Apache Lucene project. Similarly to the
feature reduction, in all experiments leaving out the stop
words slightly degraded the performance of the classiﬁers.
For example, the accuracy of Naive Bayes Multinomial on
5500 questions decreased from 76.4% to 75.4%
While some experiments saw a decrease in accuracy when
using stemming, most had an improvement. We therefore
consistently used stemming for the rest of this project.

3.2 Comparison Results

The diﬀerent classiﬁers displayed diﬀerent performance
characteristics with training set size. A summary of our
baseline is given in Figure 1; the data had stemming ap-
plied to the ﬁne categories in the TREC-10 dataset.
It is
unsuprising that that the SVM with linear kernel provides
the best accuracy for any input dataset size, and that accu-
racy increases with dataset size. However since one of the
core experiments we performed (semi-supervised learning
using Expectation Maximization) is based on a Naive Bayes
Multinomial classiﬁer, we used this classiﬁer for comparison

2

be stated as:

+ λ

|C |(cid:88)
j=1

(cid:88)
zij log(P (cj |θ)P (di |cj ; θ))
(cid:96)(θ |D; z ) = logP (θ) +
 (cid:88)

|C |(cid:88)
di∈Dl
zij log(P (cj |θ)P (di |cj ; θ))
di∈Du
j=1
where zij the probability of di belonging to cj ; and λ, 0 ≤
λ ≤ 1, determines the contribution of the unlabelled data
to the overall log likelihood.

4.2 Multiple Mixture Component per Class

The multiple mixture component per class approach relaxes
the assumption in EM-λ, allowing for a many-to-one corre-
spondence between mixture components and classes. This
is motivated by an eﬀort to model documents as close to
reality as possible, since the number of latent variables may
be greater than the number of classes. To ﬁnd the posterior
probability P (cj |di ), once we obtain the probability for each
mixture component, we then need to sum the probabilities
over the classes to which each mixture component belongs.
Note in our implementation, the number of mixture com-
ponents is set to be identical for each class. Furthermore,
we heuristically pick a small number for the number of mix-
ture components since having a large number of mixture
components drastically increases the training time.

4.3 Test Results

We tested both the EM-λ and multiple mixture model
against UIUC dataset and questions collected from Yahoo!
History data. However, neither of them managed to achieve
a signiﬁcant improvement over oﬀ-the-shelf classiﬁers.
In
fact, our experiments showed that in the context of ques-
tion classiﬁcation, incorporating unlabelled data can actu-
ally deteriorate classiﬁcation performance.
As can be seen in Figure 2, the result of EM-λ was worse
than the baseline performance of Naive Bayes Multinomial.
The result was obtained in 10-fold cross validation.
Figure 3 compares the performance of baseline Naive
Bayes Multinomial classiﬁer with multiple mixture compo-
nent classiﬁers of varying mixture component size, denoted
as k . Unfortunately, the algorithm did not do too well on
our data sets. We tried running the classiﬁer with a higher
k such as 6, 8 and so on, but all of them gave the same
result as in the case when k = 2. One possible explanation
for the poor performance may be that questions violate the
model assumption enforced by multiple mixture component
model.

Figure 2: EM-λ Test Results

Figure 3: Multiple Mixture Component Test Results

5 Semantic Expansion

In this section of our project, we investigated augmenting
features of the dataset with ones obtained from semantic
structure. The intuition here is that, because the factoid
questions are almost all single, short sentences, the prob-
ability of a feature being present in both training and test
questions selected from same category is much lower than
in longer documents, and augmenting features with more
generalized version would improve classiﬁcation accuracy.
Our approach was to to identify nouns and verbs in the
questions, and then expand these with hypernyms which
provide generalizations. For example, the hypernyms of
’car’ include both ’motor vehicle’ and ’compartment’. Fol-
lowing similar experiments [3] [2] we used a part-of-speech
tagger in an initial step, and then used a sequence of
WordNet[4] lookups to do the expansion. WordNet maps
a word to one or more senses, each of which is associated

3

Figure 5: Single-Noun Hypernym Expansion Test Results

with a ’symset’ of words which share the same set; each
symset is in turn linked via hypernym (and other) pointers
to other symsets.
The tokens we chose to expand, as well as the expansion
policy within WordNet, were investigated in a series of ex-
periments. In terms of selecting candidate token, we looked
at generalizing all nouns, all verbs and then just the ﬁrst
noun as a substitute for the ’lead’ noun, as suggested by [3].
We then experimented with aggressive expansion, in which
all senses of the word were generalized, versus conservative
expansion, in which only the ﬁrst sense of the word was
generalized (see Figure 4).

Figure 4: Hypernym Expansion, with conservative approach
highlighted

Figure 6: Token Frequencies with Hypernym Expansion of
Nouns

5.1 Test Results

Classiﬁcation accuracy was consistently improved by hyper-
nym expansion of nouns, with the best results obtained in
aggressively expanding a single noun only, as shown in Fig-
ure 5. The improvement was measured across classiﬁers.
The impact of expanding verbs in a similar way had an
insigniﬁcant impact on accuracy; possible explanations in-
clude that the hypernym network is less dense in WordNet
for verbs than for nouns and that the verbs are less useful for
descrimintation; indeed in a large number of the questions,
the only verb is ’to be’.
The positive impact that the hypernym expansion has on
frequency of token occurrence is demonstrated in Figure 6.
The heuristic of using the ﬁrst noun as the lead noun
is possible with the well-structured question dataset, but
would not be applicable in a normal text with longer sen-
tences.
In a second set of experiments, we applied the same ex-
pansions to the Yahoo! History 1000-question dataset. The
baseline classiﬁcation accuracy using Naive Bayes Multino-
mial on this data was 43.5%, which is comparable to the
44% accuracy on the TREC-10 data although the number
of categories dropped to 37 from 50. Details of the im-
provement given by hypernym expansion is given in Table
1.

6 Latent Dirichlet Allocation

Another classiﬁcation method we considered was latent
Dirichlet allocation (LDA). LDA represents a document us-
ing a generative model where given a class, the document
is generated from multiple mixture components . Note that
LDA relaxes the many-to-one correspondence assumption
in the multiple mixture component model, discussed in the
previous section, by assuming a many-to-many correspon-
dence between mixtures components and classes. Experi-
ments have shown that the weakened assumption improves
classiﬁcation for text documents [5].
For our purpose, we used topics obtained from LDA to
augment the features in each training example to improve
classiﬁcation performance. Using the Mallet software pack-
age, we ﬁrst ran LDA on the training examples to extract
topics from the training examples (the number of the top-
ics was empirically set to 300) [1]. We then augmented the

4

analysis. While the latter did not prove eﬀective, noun hy-
pernym expansion based on a simple heuristics was shown
to consistently improve classiﬁcation accuracy across clas-
siﬁers and granularity of categories. Use of more sophisti-
cated word-sense disambiguation and better identiﬁcation
of the lead noun of the sentence is likely to improve this
more, but was beyond the scope of this project.
Our work demonstrated how some aspects of machine
learning are robust across diﬀerent datasets, while others
are restricted, sometimes in subtle ways, which limits their
applicability. Both the failure of semi-supervised classiﬁca-
tion and the eﬀectiveness of semantic feature augmentation
were unanticipated results of this work.

References

[1] Andrew Kachites McCallum.
chine
learning
for
language
http://mallet.cs.umass.edu, 2002.

Mallet:
toolkit.

A ma-
URL

[2] Xin Li and Dan Roth. Learning question classiﬁers:
The role of semantic information. In In Proc. Interna-
tional Conference on Computational Linguistics (COL-
ING, pages 556–562, 2004.

[3] Donald Metzler and W. Bruce Croft. Analysis of sta-
tistical question classiﬁcation for fact-based questions.
Journal of Information Retrieval, 8:481–504, 2004.

[4] George Miller. Wordnet A lexical database for English.
Communications of ACM, 38(11):39–41, 1995.

[5] David M. Blei Andrew Y. Ng and Michael I. Jordan.
Latent dirichlet allocation. Journal of Machine Learn-
ing Research, 3:993–1022, 2003.

[6] Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. Text classiﬁcation from la-
beled and unlabeled documents using em. In Machine
Learning, pages 103–134, 1999.

[7] NIST TREC-10 QA Track. http://trec.nist.gov/
data/qa/.

[8] Weka Machine Learning Project. Weka.
http://www.cs.waikato.ac.nz/˜ml/weka.

URL

[9] Yahoo! Answers. http://answers.yahoo.com/.

[10] Xiaojin Zhu. Semi-supervised learning literature sur-
vey. Technical report, Computer Sciences, University
of Wisconsin-Madison, 2005.

Table 1: Hypernym Expansion of Yahoo! History Data

Description
Baseline NB Multi.
Conservative Hyp. Exp.

Accuracy Precision Recall
0.44
0.38
43.5
45.5
0.41
0.46

original features with the topics, and trained an SVM with
polynomial kernel on the new data. However, as shown in
Figure 7, adding the topics to the original features actu-
ally degraded the performance slightly. In hindsight,this is
somewhat expected because we were not adding any new
information to the features.

Figure 7: LDA Topic Augmentation Test Results

7 Discussion and Conclusion

In this project we broadly examined approaches to improving
classiﬁcation of closed-form ’factoid’ questions. An initial
examination of the impact of stemming, stop words and
feature reduction and comparison of several well-known su-
pervised classiﬁcation techniques allowed us to establish a
baseline to measure later work on.
One of the core aims, the application of semi-supervised
classiﬁcation by adding a large amount of available non-
labelled data to a limited set of labelled data, was exam-
ined in the form of Expectation Maximization which has
successfully been applied to datasets of documents. We
both implemented multiple versions of this technique, but
failed to use it successfully, presumably because the genera-
tive model on which this approach is based, while it may still
apply to questions, is not measurably manifested because
of question length.
This lead to investigations of how the limited features
available could be augmented. We did this both through
application of external semantic knowledge and with topics
automatically learned from the dataset using latent dirichlet

5


A Better BCS
Rahul Agrawal, Sonia Bhaskar, Mark Stefanski

1

I . IN TRODUC T ION
Most NCAA football teams never play each other in a given season, which has made ranking them a long-running
and much-debated problem. The ofﬁcial Bowl Championship Series (BCS) standings are determined in equal parts
by input from coaches, sports writers, and an average of algorithmic rankings. These standing determine which
teams play for the national championship, and, in part, which teams appear in each bowl (playoff) game.
We seek an algorithm that ranks teams better than the BCS standings do. In particular, since the BCS standings
serve to match teams in bowl games, we want better bowl game prediction accuracy. This is a uniquely difﬁcult
prediction problem because, by design, teams squaring off in a bowl game are generally evenly-matched and come
from different conferences, which means they are unlikely to have many common opponents, if any.

I I . TH E MODE L
We model the outcome of a game between two teams as the difference between their (noisy) competitiveness
levels. We assume a team has a certain ﬁxed average ability, µi , and that for each game a multitude of independent
random factors collectively determines to what extent that team’s performance falls short of or exceeds its average
ability. The Central Limit Theorem suggests that we model these random factors as  ∼ N (0, σ2 ). Therefore, team
i’s competitiveness in any given game is distributed as xi = µi +  ∼ N (µi , σ2 ). So when team i plays team j , each
team independently samples from its competitiveness distribution, and the resulting score difference is distributed
as xj − xi ∼ N (µj − µi , 2σ2 ).
We denote by y (k)
the actual score difference of the k th contest between team i and team j . And to avoid
i,j
redundancy, we adopt the convention that the score difference between team i and team j where j > i is team j ’s
score less team i’s score. So our model says that for the k th meeting between teams i and j ,
i,j = xj − xi = µj − µi + δ (k)
y (k)
i,j ,
√
2 ∼ N (0, 2σ2 ) are mutually independent.
where the δ (k)
i,j =








This model easily extends to m games played among a common pool of n teams. For instance, if each of the n
teams plays every other exactly once:


x2 − x1
−1
y1,2
δ1,2
0 . . .
0
0
1
...
...
...
...
...
...
...
...
...
µ1
xn − x1
−1
µ2
y1,n
δ1,n
0 . . .
0
0
1
x3 − x2
0 −1
µ3
y2,3
δ2,3
1
. . .
0
0
...
...
...
...
...
...
...
...
...
...
xn − x2
0 −1
(cid:124) (cid:123)(cid:122) (cid:125)
µn−1
y2,n
δ2,n
0
. . .
0
1
...
...
...
...
...
...
...
...
...
µn
(cid:124)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:123)(cid:122)
0 . . . −1
xn − xn−1
δn−1,n
yn−1,n
1
0
0
(µx )n×1
ym×1
∆m×1
Am×n
If teams i and j do play each other multiple (K > 1) times, then there will be K (duplicate) rows of A corresponding
to the y (1)
i,j , . . . , y (K )
entries of y. If teams i and j do not play at all, then yi,j will be absent from y, as will the
i,j
corresponding row of A. In reality, many teams might not have played each other in the current season – or, in
some cases, ever – so many of the yi,j could be absent from y. In any case, our model reduces to y = Aµx + ∆.

+

=

=

I I I . UNW E IGH TED L IN EAR R EGR E S S ION
A. Finding The Maximum Likelihood Abilities
Since we can rank teams by their abilities, our goal here is to ﬁnd the maximum likelihood estimate of ability
vector µx . To do so, we must set a reference value for the µi because otherwise there is no way of distinguishing
between µx and µ(cid:48)
x that differ by a constant offset k1n :
Aµ(cid:48)
x = A(µx + k1n ) = Aµx + kA1n = Aµx .

2

That is, k1n ∈ N (A) by construction of A. So we introduce the condition 1T
n µx = µ1 + . . . + µn = 0, which sets
the average ability of the pool of teams to zero.
(cid:0)AT A(cid:1)−1
We would like to ﬁnd the maximum likelihood estimate of µx by the least-squares approximation
AT y
n µx = 0. But A is never full rank because its nullspace is always nontrivial, and
and then adjust the result so that 1T
(cid:21)
(cid:20)1T
therefore AT A cannot be full rank (or, consequently, invertible). However, under certain conditions on the games
played (teams are “competitively linked”),
n
(Ac )(m+1)×n =
A
(cid:21)
(cid:20) 0
(cid:20)1T
(cid:20) 0
(cid:21)
(cid:21)
is “skinny” (m + 1 ≥ n) and full rank (see Proof 1 in the Appendix). Note that
(cid:124)(cid:123)(cid:122)(cid:125)
(cid:124)(cid:123)(cid:122)(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
n
µx +
=
∆
y
A
x = (cid:0)AT
(cid:1)−1
(yc )(m+1)×1
∆c
Ac
µ∗
AT
c yc .
c Ac
x is the maximum likelihood estimate of µx , not just the minimizer of ||Acx − yc ||2 because µ∗
Importantly, µ∗
x must
also minimize ||Ax − y||2 (see Proof 2 in the Appendix), the minimizer of which we know is the least maximum
likelihood estimate of µx .

So we can compute

.

= max
σ

= max
σ

B. Finding The Maximum Likelihood Noise Parameter
With the maximum likelihood abilities µ∗
x , we can predict whether team i will beat team j on average by
determining if µ∗
i > µ∗
j . But to determine the probability with which team i beats team j , we need to estimate σ .
To this end, note that y is an m-dimensional Gaussian vector distributed as y ∼ N (µx , 2σ2 Im×m ). Then
(cid:19)(cid:19)
(cid:18)
(cid:18)
log p(y|µ∗
log p(y|µx , σ) = max
max
x , σ)
µx ,σ
σ
(2π)m/22σm(cid:17) − 1
(cid:16)
x )T (2σ2 Im×m )−1 (y − Aµ∗
(y − Aµ∗
− 1
1
(2π)m/2 |2σ2 Im×m |1/2
x )
log
exp
2
− log
x )T (y − Aµ∗
4σ2 (y − Aµ∗
x )
4σ2 ||y − Aµ∗
x ||2 .
1
m log(σ) +
= min
σ
||y − A (cid:0)AT
(cid:1)−1
Taking the derivative of the above with respect to σ , setting it equal to 0, and solving gives the maximum likelihood
estimate
c yc ||
x ||
||y − Aµ∗
AT
√
√
c Ac
2m
2m
which is precisely the root mean squared error of the observed score differences (each of whose variance is 2σ2 ).
Since Gaussians are completely determined by their mean and variance, the maximum likelihood predicted
j − µ∗
outcome of a game between team i and team j is distributed as N (µ∗
i , 2σ∗ 2 ). It follows that we would expect
(cid:18) µ∗
(cid:19)
j − µ∗
the score difference between team i and team j to be, on average, µ∗
i , and that team j defeats team i with
probability
j − µ∗
i√
2σ∗
where Φ is the standard normal gaussian CDF function. More generally, we have a complete description of the
predicted outcome between any two teams.

P {j defeats i} = Φ

,

σ∗ =

=

,

3

IV. LOCA L LY W E IGH TED L IN EAR R EGR E S S ION
We look to improve our predictions by giving increased importance to the most relevant observed outcomes.
Consider a motivating example: Team A defeats team B by 10 points; team B defeats team C by 10 points; and
team C defeats team A by 10 points. We are then asked to predict the outcome of a second meeting between team
A and team B. Unweighted linear regression would assign each team an equal ability and thus would predict a draw
in team A and team B’s second meeting. This is indeed the most likely explanation of the data given the model.
But one would think that team A’s defeat of team B matters more than team B’s transitive defeat of team A.
In predicting the outcome between team i and team j , we consider the most relevant observed outcomes to be the
ones “closest” to the match-up between teams i and j , (i, j ). To capture the notion of distance between games, we
construct a graph in which each vertex represents a team and edges join teams that have played. Then to quantify
the distance between match-ups (i, j ) and (k , l), we compute the length of the smallest cycle containing i, j , k , and
l (adding an edge between i and j ’s vertices and between k and l’s vertices, if necessary). We denote this measure
of distance dG ((i, j ), (k , l)).
From these graph distances we can specify any number of weight schemes. In trying to predict the outcome of
team i and team j , we choose to weight each observed outcome (k , l) as
wi,j (k , l) = dG ((i, j ), (k , l))−α ,
where we set α = 1 because this value minimized win-loss bowl game prediction error over the past three seasons.
To predict yi,j , we construct a diagonal matrix H(i,j )
m×m where the diagonal element corresponding to yk,l is set
(cid:21)
(cid:20) c
n µx = 0 constraint, we construct
to 1/wi,j (k , l). To enforce the 1T
0T
(H(i,j )
m
)(m+1)×(m+1) =
c
0m H(i,j )
where c ∈ R is a very large constant. Then the maximum likelihood estimate of µ(i,j)
(cid:16)
(cid:17)−1
x
weighted case is
AT
c H(i,j )
c H(i,j )
AT
c Ac
c yc .
Yet again, we must solve for the maximum likelihood parameter using Ac and yc instead of A and y to ensure
the matrix to be inverted is full rank. But this does not result in a worse-ﬁtting µ(i,j)
(see Proof 2 in Appendix).
x
Instead of having a single µ∗
x that gives an immediate ranking of the teams, we have distinct µ(i,j)
for each outcome
x
predicted. Returning to the motivating example, it could (and indeed should) be the case that µ(A,B )
> µ(A,B )
A
B
> µ(A,C )
and µ(A,C )
> µ(B ,C )
but µ(B ,C )
. So we have traded consistency of predictions for relevance of predictions.
B
C
C
A

for predicting yi,j in the

µ(i,j)
x =

V. IM P LEM EN TAT ION AND R E SU LT S

A. Data Processing
We trained on the regular season data, and tested on the post-season bowl data. We processed this data by
assigning each team an index, and removing any duplicate games (if team i plays team j , then team j also plays
team i). Since data collection was time-intensive, we collected data for four seasons only.

B. Results and Analysis
The errors in table I show our unweighted and weighted linear regression algorithms have similar performance.
Both found the 2009-2010 bowl games particularly difﬁcult to predict.

Season
2007-2008
2008-2009
2009-2010

Unweighted Linear Regression
Avg. Test W/L Train W/L Test
Avg. Train
0.32
0.23
13.08
10.75
0.32
0.22
11.01
10.96
9.42
14.09
0.20
0.53

Weighted Linear Regression
W/L Test
Avg. Test
0.32
13.33
0.26
10.82
14.22
0.50

TABLE I
SUMMARY O F ERROR S

4

Win-loss error (W/L): The number of games whose outcome we predicted incorrectly, divided by the total number
of games predicted.
Average absolute error (Avg.): The sum of the absolute differences between the actual score margin and our predicted
score margin, divided by the total number of games predicted.

For each of the past three seasons, we trained our algorithm on the regular season data and tested on the bowl
game data. We compared our algorithms’ prediction success rate to the ofﬁcial BCS rankings, the ofﬁcial BCS
computer rankings, and ESPN’s power rankings. These rankings only has predictions for about half of the bowl
games each season (which is why our algorithms’ win-loss test error here differs from that of Table I).

Season
2007-2008
2008-2009
2009-2010

BCS Overall
0.41
0.27
0.40

BCS Comp. Avg.
0.47
0.33
0.40

ESPN Power Ranking
N/A
0.27
0.40

Us (Unweighted)
0.53
0.53
0.40

Us (Weighted)
0.53
0.67
0.47

TABLE II
COM PAR I SON O F RANK ING SY ST EM W IN -LO S S PR ED IC T ION SUCC E S S RATE

Surprisingly, over the past three seasons, the experts’ bowl predictions have been wrong more often than not.
Also, our algorithms performed best – in terms of average training and test error, as well as win-loss training and
test error – on the 2008-2009 season data, but this was the expert rankings’ wost year. Both of our algorithms
equalled or outperformed the expert rankings each of the past three seasons.

C. Predictions
Table III shows our predictions for this current season’s upcoming major bowl games. For each game, we
predicted the winner, the margin of victory, and also computed the probability of the win, derived from σ∗ . Our
two algorithms have made near-identical predictions, and both predicted a comfortable win (with probability 0.76)
for second-ranked Oregon over ﬁrst-ranked Auburn in the national title game.

Major Bowlgame Matchups
Auburn (1)
Oregon (2)
TCU (3)
Wiconsin (5)
Ohio State (6)
Arkansas (8)
Connecticut
Oklahoma (7)
Virginia Tech (13)
Stanford (4)

Winner
Oregon
TCU
Ohio State
Oklahoma
Stanford

Point Margin (Unweighted)
9
7
4
21
11

Point Margin (Weighted)
9
7
4
21
10

Probability of Win
0.7609
0.6928
0.6245
0.9462
0.7940

TABLE III
OUR PR ED IC T ION S FOR U PCOM ING 2010 -2011 MA JOR BOW L GAM E S

V I . CONC LU S ION
The structure of the NCAA football season poses some challenges to the formation of a “fair” ranking system.
Since the vast majority of teams do not play each other, ranking them in a way that is fair and completely assesses
their relative abilities is not an easy problem and has resulted in a lot of criticism of the current BCS rankings.
Our algorithms predicted bowl game results for the past three seasons with much better accuracy than both chance
and expert predictions. We expect our algorithms’ predictions for the this season’s upcoming bowl games to be
similarly successful.
There are plenty of opportunities to extend our work. For one, we could explore different weight schemes for
weighted linear regression. More broadly, since there nothing NCAA football-speciﬁc about our model or our
algorithms, we can apply them to other sports leagues – especially those with more teams than games played per
season – and even competitive activities beyond sports.

V I I . A P P END IX

A. Proof 1
Given a vector of results y, we can construct a graph G in which each vertex represents a team. An edge joins
two vertices if and only if the two teams represented by those two vertices have played each other (and the outcome
is contained in the dataset). We say the pool of teams is “competitively linked” if G is connected.

5

We claim that Ac is full rank if and only if G is connected.
If G is connected, then for any two teams i and j we have
xi − xk1 = ±yi,k1
xk1 − xk2 = ±yk1 ,k2
...
xkK − xj = ±ykK ,j .

Summing these equations,

yi,j = xi − xj = ±yi,k1 + ±yk1 ,k2 + . . . + ±ykK ,j .
Now we can show N (Ac ) = 0. So suppose y = 0. Then for every i and every j
0 = yi,j = xi − xj .
n x = 0 implies x1 = x2 = . . . = xn = 0. So y = 0 implies
Hence x1 = x2 = . . . = xn . Then the constraint 1T
x = 0. Hence N (Ac ) = 0 and Ac is full rank whenever G is connected.
If G is not connected, then we can partition the xi into I and J where no team in I has played a team in J
(and, of course, vice versa). Then deﬁne x(cid:48) ∈ Rn where x(cid:48)
i = 1/|I | if i ∈ I and x(cid:48)
i = −1/|J | if j ∈ J . Clearly,
x(cid:48) (cid:54)= 0 and Acx(cid:48) = 0, so in this case N (Ac ) (cid:54)= 0. Hence if G is not connected, then Ac is not full rank. (This is
what intuition would tell us: If the pool of teams is not “competitively linked,” then there is no way of comparing
connected component of teams to another.)

B. Proof 2
We claim that

(yc − Acx)T Hc (yc − Acx) = min
(y − Ax)T H(y − Ax) = min
J (x) = min
min
x∈Rn
x∈Rn
x∈Rn
x∈Rn
In the special case H = In×n and Hc = I(n+1)×(n+1) , this claim becomes
||yc − Acx||2 .
||y − Ax||2 = min
min
x∈Rn
x∈Rn

Jc (x)

Note that

n x)2 ≥ min
Jc (x) = (yc − Acx)T Hc (yc − Acx) = (y − Ax)T H(y − Ax) + (Hc )1,1 (0 − 1T
J (x)
min
x∈Rn
x∈Rn
for all x ∈ Rn since (Hc )1,1 > 0. So it sufﬁces to show minx∈Rn J (x) ≥ minx∈Rn Jc (x). To this end, let
x(cid:48) = arg minx∈Rn J (x). Then deﬁne a = (1T
n x(cid:48) )/n to be the average of the entries of x(cid:48) . Using a and the fact
that 1n ∈ N (A):
J (x) = J (x(cid:48) )
min
x∈Rn
= (yc − Acx(cid:48) )T Hc (yc − Acx(cid:48) )
= (yc − Ac (x(cid:48) + a1n ))T Hc (yc − Ac (x(cid:48) + a1n ))
= (y − A(x(cid:48) + a1n ))T H(y − A(x(cid:48) + a1n )) + (Hc )1,1 (0 − 1T
n (x(cid:48) + a1n ))2
= (y − Ax(cid:48) + aA1n ))T H(y − Ax(cid:48) + aA1n ) + (Hc )1,1 (1T
n x(cid:48) − a1T
n 1n )2
= (y − Ax(cid:48) )T H(y − Ax(cid:48) ) + (Hc )1,1 (an − an)2
= (y − Ax(cid:48) )T H(y − Ax(cid:48) )
= Jc (x(cid:48) )
≥ min
Jc (x)
x∈Rn


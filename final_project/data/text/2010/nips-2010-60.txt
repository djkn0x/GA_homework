Distributed Dual Averaging in Networks

Martin J. Wainwright1,2
Alekh Agarwal1
John C. Duchi1
Department of Electrical Engineering and Computer Science1 and Department of Statistics2
University of California, Berkeley
Berkeley, CA 94720-1776
{jduchi,alekh,wainwrig}@eecs.berkeley.edu

Abstract

The goal of decentralized optimization over a network is to optimize a global ob-
jective formed by a sum of local (possibly nonsmooth) convex functions using
only local computation and communication. We develop and analyze distributed
algorithms based on dual averaging of subgradients, and provide sharp bounds on
their convergence rates as a function of the network size and topology. Our anal-
ysis clearly separates the convergence of the optimization algorithm itself from
the effects of communication constraints arising from the network structure. We
show that the number of iterations required by our algorithm scales inversely in
the spectral gap of the network. The sharpness of this prediction is conﬁrmed both
by theoretical lower bounds and simulations for various networks.

1

Introduction

Network-structured optimization problems arise in a variety of application domains within the in-
formation sciences and engineering. A canonical example that arises in machine learning is the
problem of minimizing a loss function averaged over a large dataset (e.g. [16, 17]). With terabytes
of data, it is desirable (even necessary) to assign smaller subsets of the data to different proces-
sors, and the processors must communicate to ﬁnd parameters
that minimize the loss over the entire
dataset. Problems such as multi-agent coordination, estimation problems in sensor networks, and
packet routing also are all naturally cast as distributed convex minimization [1, 13, 24]. The seminal
work of Tsitsiklis and colleagues [22, 1] analyzed algorithms for minimization of a smooth func-
tion f known to several agents while distributing processing of components of the parameter vector
x ∈ Rn . More recently, a few researchers have shifted focus to problems in which each processor
locally has its own convex (potentially non-differentiable) objective function [18, 15, 21, 11].

In this paper, we provide a simple new subgradient algorithm for distributed constrained optimiza-
tion of a convex function. We refer to it as a dual averaging subgradient method, since it is based on
maintaining and forming weighted averages of subgradients throughout the network. This approach
is essentially different from previously developed distributed subgradient methods [18, 15, 21, 11],
and these differences facilitate our analysis of network scaling issues —how convergence rates de-
pend on network size and topology. Indeed, the second main contribution of this paper is a careful
analysis that demonstrates a close link between convergence of the algorithm and the underlying
spectral properties of the network. The convergence rates for a different algorithm given by the
papers [18, 15] grow exponentially in the number of nodes n in the network. Ram et al. [21] pro-
vide tighter analysis that yields convergence rates that scale cubically in the network size, but are
independent of the network topology. Consequently, their analysis does not capture the intuition
that distributed algorithms should converge faster on “wel l-connected” networks —expander graphs
being a prime example—than on poorly connected networks (e.g ., chains or cycles). Johansson et
al. [11] analyze a low communication peer-to-peer protocol that attains rates dependent on network
structure. However, in their algorithm only one node has a current parameter value, while all nodes
in our algorithm maintain good estimates of the optimum at all times. This is important in online

1

or streaming problems where nodes are expected to act or answer queries in real-time. In additional
comparison to previous work, our analysis yields network scaling terms that are often substantially
sharper. Our development yields an algorithm with convergence rate that scales inversely in the
spectral gap of the network. By exploiting known results on spectral gaps for graphs with n nodes,
we show that our algorithm obtains an ǫ-optimal solution in O(n2 /ǫ2 ) iterations for a single cycle
or path, O(n/ǫ2 ) iterations for a two-dimensional grid, and O(1/ǫ2 ) iterations for a bounded degree
expander graph. Simulation results show excellent agreement with these theoretical predictions.

2 Problem set-up and algorithm

In this section, we provide a formal statement of the distributed minimization problem and a de-
scription of the distributed dual averaging algorithm.

Distributed minimization: We consider an optimization problem based on functions that are dis-
tributed over a network. More speci ﬁcally, let G = (V , E ) be an undirected graph over the vertex
set V = {1, 2, . . . , n} with edge set E ⊂ V × V . Associated with each i ∈ V is convex func-
tion fi
: Rd → R, and our overarching goal is to solve the constrained optimization problem
n Pn
1
i=1 fi (x), where X is a closed convex set. Each function fi is convex and hence sub-
minx∈X
differentiable, but need not be smooth. We assume without loss of generality that 0 ∈ X , since we
can simply translate X . Each node i ∈ V is associated with a separate agent, and each agent i main-
tains its own parameter vector xi ∈ Rd . The graph G imposes communication constraints on the
agents: in particular, agent i has local access to only the objective function fi and can communicate
directly only with its immediate neighbors j ∈ N (i) := {j ∈ V | (i, j ) ∈ E }.
A concrete motivating example for these types of problems is the machine learning scenario de-
scribed in Section 1. In this case, the set X is the parameter space of the learner. Each function fi is
the empirical loss over the subset of data assigned to processor i, and the average f is the empirical
loss over the entire dataset. We use cluster computing as our model, so each processor is a node in
the cluster and the graph G contains edges between processors connected with small latencies; this
setup avoids communication bottlenecks of architectures with a centralized master node.

Dual averaging: Our algorithm is based on a dual averaging algorithm [20] for minimization of
a (potentially nonsmooth) convex function f subject to the constraint that x ∈ X . We begin by
describing the standard version of the algorithm. The dual averaging scheme is based on a proximal
function ψ : Rd → R assumed to be strongly convex with respect to a norm k·k, more precisely,
2 kx − yk2 for all x, y ∈ X . We assume w.l.o.g. that ψ ≥ 0 on X
ψ(y) ≥ ψ(x) + h∇ψ(x), y − xi + 1
2 kxk2
and that ψ(0) = 0. Such proximal functions include the canonical quadratic ψ(x) = 1
2 , which
is strongly convex with respect to the ℓ2 -norm, and the negative entropy ψ(x) = Pd
j=1 xi log xi−xi ,
which is strongly convex with respect to the ℓ1 -norm for x in the probability simplex.
We assume that each function fi is L-Lipschitz with respect to the same norm k·k —that is,
(1)
for x, y ∈ X .
|fi (x) − fi (y)| ≤ L kx − yk
Many cost functions fi satisfy this type of Lipschitz condition, for instance, convex functions on
a compact domain X or any polyhedral function on an arbitrary domain [8]. The Lipschitz condi-
tion (1) implies that for any x ∈ X and any subgradient gi ∈ ∂ fi (x), we have kgik∗ ≤ L, where
denotes the dual norm to k·k, deﬁned by kvk∗
:= supkuk=1 hv , ui.
k·k∗
The dual averaging algorithm generates a sequence of iterates {x(t), z (t)}∞t=0 contained within X ×
Rd . At time step t, the algorithm receives a subgradient g(t) ∈ ∂ f (x(t)), and updates
x(t + 1) = Πψ
and
(−z (t + 1), α(t)).
z (t + 1) = z (t) − g(t)
X
Here {α(t)}∞t=0 is a non-increasing sequence of positive stepsizes and
ψ(x)(cid:27)
x∈X (cid:26) hz , xi +
1
Πψ
(z , α) := argmin
X
α
is a type of projection. Intuitively, given the current iterate (x(t), z (t)), the next iterate x(t + 1)
to chosen to minimize an averaged ﬁrst-order approximation to the function f , while the proximal

(2)

(3)

2

function ψ and stepsize α(t) > 0 enforce that the iterates {x(t)}∞t=0 do not oscillate wildly. The al-
gorithm is similar to the follow the perturbed/regularized leader algorithms developed in the context
of online learning [12], though in this form the algorithm seems to be originally due to Nesterov [20].
In Section 4, we relate the above procedure to the distributed algorithm we now describe.

Distributed dual averaging: Here we consider a novel extension of dual averaging to the dis-
tributed setting. For all times t, each node i ∈ V maintains a pair of vectors (xi (t), zi (t)) ∈ X × Rd .
At iteration t, node i computes a subgradient gi (t) ∈ ∂ fi (xi (t)) of the local function fi and receives
{zj (t), j ∈ N (i)} from its neighbors. Its update of the current estimate xi (t) is based on a weighted
average of these parameters. To model the process, let P ∈ Rn×n be a doubly stochastic symmetric
matrix with Pij > 0 only if (i, j ) ∈ E when i 6= j . Thus Pn
j=1 Pij = Pj∈N (i) Pij = 1 for all
i ∈ V and Pn
i=1 Pij = Pi∈N (j ) Pij = 1 for all j ∈ V . Given a non-increasing sequence {α(t)}∞t=0
of positive stepsizes, each node i ∈ V updates
zi (t + 1) = Xj∈N (i)
and xi (t + 1) = Πψ
(4)
Pj i zj (t) − gi (t),
(−zi (t + 1), α(t)),
X
where the projection Πψ
i computes the new dual parameter
was deﬁned in (3). In words, node
X
zi (t + 1) from a weighted average of its own subgradient gi (t) and the parameters {zj (t), j ∈ N (i)}
in its neighborhood; it then computes the local iterate xi (t + 1) by a proximal projection. We show
convergence of the local sequence {xi (t)}∞t=1 to an optimum of the global objective via the local
T PT
average bxi (T ) = 1
t=1 xi (t), which can evidently be computed in a decentralized manner.
3 Main results and consequences
We will now state the main results of this paper and illustrate some of their consequences. We give
the proofs and a deeper investigation of related corollaries at length in the sections that follow.

Convergence of distributed dual averaging: We start with a result on the convergence of the
distributed dual averaging algorithm that provides a decomposition of the error into an optimization
term and the cost associated with network communication. In order to state this theorem, we deﬁne
n Pn
the averaged dual variable ¯z (t) := 1
i=1 zi (t), and we recall the local time-average bxi (T ).
Theorem 1 (Basic convergence result). Given a sequence {xi (t)}∞t=0 and {zi (t)}∞t=0 generated by
the updates (4) with step size sequence {α(t)}∞t=0 , for each node i ∈ V and any x∗ ∈ X , we have
TXt=1
TXt=1
L2
1
3L
f (bxi (T )) − f (x∗ ) ≤
ψ(x∗ ) +
max
.
α(t) k ¯z (t) − zj (t)k∗
α(t − 1) +
T α(T )
2T
T
j=1,...,n
Theorem 1 guarantees that after T steps of the algorithm, every node i ∈ V has access to a locally
deﬁned quantity bxi (T ) such that the difference f (bxi (T )) − f (x∗ ) is upper bounded by a sum of
three terms. The ﬁrst two terms in the upper bound in the theor em are optimization error terms that
are common to subgradient algorithms. The third term is the penalty incurred due to having different
estimates at different nodes in the network, and it measures the deviation of each node’s estimate
of the average gradient from the true average gradient. Thus, roughly, Theorem 1 ensures that as
long the bound on the deviation k ¯z (t) − zi (t)k∗
is tight enough, for appropriately chosen α(t) (say
α(t) ≈ 1/√t), the error of bxi (T ) is small uniformly across all nodes i ∈ V .
Convergence rates and network topology: We now turn to investigation of the effects of network
topology on convergence rates. In this section,1 we assume that the network topology is static and
that communication occurs via a ﬁxed doubly stochastic weig ht matrix P at every round. Since P
is symmetric and stochastic, it has largest singular value σ1 (P ) = 1. As the following result shows,
the convergence of our algorithm is controlled by the spectral gap γ (P ) := 1 − σ2 (P ) of P .
Theorem 2 (Rates based on spectral gap). Under the conditions and notation of Theorem 1, suppose
R√1−σ2 (P )
moreover that ψ(x∗ ) ≤ R2 . With step size choice α(t) =
, we have
4L√t
log(T √n)
RL
f (bxi (T )) − f (x∗ ) ≤ 8
for all i ∈ V .
√T ·
p1 − σ2 (P )
1We can weaken these conditions; see the long version of this paper for extensions to random P [4].

3

(a)
(b)
(c)
(d)
Figure 1. (a) A 3-connected cycle. (b) 1-connected two-dimensional grid with non-toroidal boundary
conditions. (c) A random geometric graph. (d) A random 3-regular expander graph.

This theorem establishes a tight connection between the convergence rate of distributed subgradient
methods and the spectral properties of the underlying network. The inverse dependence on the
spectral gap 1 − σ2 (P ) is quite natural, since it is well-known to determine the rates of mixing in
random walks on graphs [14], and the propagation of information in our algorithm is integrally tied
to the random walk on the underlying graph with transition probabilities speci ﬁed by P . Johansson
et al. [11] establish rates for their Markov incremental gradient method (MIGD) of pnΓii /T , where
Γ = (I − P + 1111⊤ /n)−1 ; performing an eigen-decomposition of the Γ matrix shows that √nΓii is
always lower bounded by 1/p1 − σ2 (P ), our bound in Theorem 2.
Using Theorem 2, one can derive explicit convergence rates for several classes of interesting net-
works, and Figure 1 illustrates four graph topologies of interest. As a ﬁrst example, the k-connected
cycle in panel (a) is formed by placing n nodes on a circle and connecting each node to its k neigh-
bors on the right and left. The grid (panel (b)) is obtained by connecting nodes to their k nearest
neighbors in axis-aligned directions. In panel (c), we show a random geometric graph, constructed
by placing nodes uniformly at random in [0, 1]2 and connecting any two nodes separated by a dis-
tance less than some radius r > 0. These graphs are often used to model the connectivity patterns
of distibruted devices such as wireless sensor motes [7]. Finally, panel (d) shows an instance of a
bounded degree expander, which belongs to a special class of sparse graphs that have very good
mixing properties [3]. For many random graph models, a typical sample is an expander with high
probability (e.g. random degree regular graphs [5]). In addition, there are several deterministic con-
structions of expanders that are degree regular (see Section 6.3 of Chung [3] for further details).

(5)

In order to state explicit convergence rates, we need to specify a particular choice of the matrix
P that respects the graph structure. Let A ∈ Rn×n be the symmetric adjacency matrix of the
undirected graph G, satisfying Aij = 1 when (i, j ) ∈ E and Aij = 0 otherwise. For each node
i ∈ V , let δi = |N (i)| = Pn
j=1 Aij denote the degree of node i and deﬁne the diagonal matrix
D = diag{δ1 , . . . , δn }. Letting δmax = maxi∈V δi denote the maximum degree, we deﬁne
δmax + 1 (cid:0)D − A(cid:1),
1
Pn (G) := I −
which is symmetric and doubly stochastic by construction. The following result summarizes our
conclusions for the choice (5) of stochastic matrix for different network topologies. We state the re-
sults in terms of optimization error achieved after T iterations and the number of iterations TG (ǫ; n)
required to achieve error ǫ for network type G with n nodes. (These are equivalent statements.)
Corollary 1. Under the conditions of Theorem 2, using P = Pn (G) gives the following rates.
(a) k-connected paths and cycles: f (bxi (T )) − f (x∗ ) = O(cid:0) RL√T
(cid:1), T (ǫ; n) = ˜O(n2 /ǫ2 ).
n log(T n)
k
(cid:1), T (ǫ; n) = ˜O(n/ǫ2 ).
(b) k-connected √n × √n grids: f (bxi (T )) − f (x∗ ) = O(cid:0) RL√T
√n log(T n)
k
(c) Random geometric graphs with connectivity radius r = Ω(qlog1+ǫ n/n) for any ǫ > 0:
f (bxi (T )) − f (x∗ ) = O(cid:0) RL√T q n
log n log(T n)(cid:1) with high-probability, T (ǫ; n) = ˜O(n/ǫ2 ).
(d) Expanders
with
bounded
ratio
of minimum to maximum node
degree:
log(T n)(cid:1), T (ǫ; n) = ˜O(1/ǫ2 ).
f (bxi (T )) − f (x∗ ) = O(cid:0) RL√T
4

By comparison, the results in the paper [11] give similar bounds for grids and cycles, but for
d-dimensional grids we have T (ǫ; n) = O(n2/d /ǫ2 ) while MIGD achieves T (ǫ; n = O(n/ǫ2 );
for expanders and the complete graph MIGD achieves T (ǫ; n) = O(n/ǫ2 ). We provide the proof of
Corollary 1 in Appendix A. Up to logarithmic factors, the optimization term in the convergence rate
is always of the order RL/√T , while the remaining terms vary depending on the network topology.
1−σ2 (Pn (G)) (cid:1) iterations are required
In general, Theorem 2 implies that at most TG (ǫ; n) = O(cid:0) 1
1
ǫ2 ·
to achieve an ǫ-accurate solution when using the matrix Pn (G) deﬁned in (5). It is interesting to
ask whether this upper bound is actually tight. On one hand, it is known that even for central-
ǫ2 (cid:1) iterations to achieve
ized optimization algorithms, any subgradient method requires at least Ω (cid:0) 1
ǫ-accuracy [19], so that the 1/ǫ2 term is unavoidable. The next proposition addresses the comple-
mentary issue, namely whether the inverse spectral gap term is unavoidable for the dual averaging
2 kxk2
algorithm. For the quadratic proximal function ψ(x) = 1
2 , the following result establishes a
lower bound on the number of iterations in terms of graph topology and network structure:
Proposition 1. Consider the dual averaging algorithm (4) with quadratic proximal function and
communication matrix Pn (G). For any graph G with n nodes, the number of iterations TG (c; n)
required to achieve a ﬁxed accuracy c > 0 is lower bounded as TG (c; n) = Ω(cid:0)
1−σ2 (Pn (G)) (cid:1).
1
The proof of this result, given in Appendix B, involves constructing a “hard” optimization problem
and lower bounding the number of iterations required for our algorithm to solve it. In conjunction
with Corollary 1, Proposition 1 implies that our predicted network scaling is sharp.
Indeed, in
Section 5, we show that the theoretical scalings from Corollary 1—namely, quadratic, linear, and
constant in network size n—are well-matched in simulations of our algorithm.

1
n

¯z (t + 1) =

4 Proof sketches
Setting up the analysis: Using techniques similar to some past work [18], we establish conver-
n Pn
i=1 zi (t) and y(t) := Πψ
gence via the two sequences ¯z (t) := 1
(− ¯z (t), α). The average sum of
X
gradients ¯z (t) evolves in a very simple way: in particular, we have
nXj=1 (cid:0)Pj i (zj (t) − ¯z (t))(cid:1) + ¯z (t) −
nXj=1
nXi=1
nXj=1
1
1
gj (t) = ¯z (t) −
n
n
where the second equality follows from the double-stochasticity of P . The simple evolution (6) of
the averaged dual sequence allows us to avoid difﬁculties wi
th the non-linearity of projection that
have been challenging in earlier work. Before proceeding with the proof of Theorem 1, we state a
few useful results regarding the convergence of the standard dual averaging algorithm [20].
Lemma 2 (Nesterov). Let {g(t)}∞t=1 ⊂ Rd be an arbitrary sequence and {x(t)}∞t=1 deﬁned by the
updates (2). For a non-increasing sequence {α(t)}∞t=0 of positive stepsizes and any x∗ ∈ X ,
TXt=1
TXt=1
1
1
α(t − 1) kg(t)k2
ψ(x∗ ).
hg(t), x(t) − x∗ i ≤
+
∗
2
α(T )
Our second lemma allows us to restrict our analysis to the sequence {y(t)}∞t=0 deﬁned previously.
Lemma 3. Consider sequences {xi (t)}∞t=1 , {zi (t)}∞t=0 , and {y(t)}∞t=0 that evolve according to (4).
Then for each i ∈ V and any x∗ ∈ X , we have
TXt=1
TXt=1
TXt=1
f (y(t)) − f (x∗ ) + L
f (xi (t)) − f (x∗ ) ≤
Now we give the proof of the ﬁrst theorem.

α(t) k ¯z (t) − zi (t)k∗

gj (t),

(6)

.

Proof of Theorem 1: Our proof is based on analyzing the sequence {y(t)}∞t=0 . For any x∗ ∈ X ,
TXt=1
TXt=1
TXt=1
nXi=1
nXi=1
1
1
f (y(t)) − f (x∗ ) =
fi (xi (t)) − f (x∗ ) +
[fi (y(t)) − fi (xi (t))]
n
n
nXi=1
TXt=1
nXi=1
TXt=1
L
1
n ky(t) − xi (t)k ,
n

fi (xi (t)) − f (x∗ ) +

≤

(7)

5

(8)

1
n

2L
n

1
n

L2
2

ψ(x∗ ).

(9)

1
α(T )

α(t − 1) +

fi (xi (t)) − fi (x∗ ) ≤

by the L-Lipschitz continuity of the fi . Letting gi (t) ∈ ∂ fi (xi (t)) be a subgradient of fi at xi (t),
nXi=1
nXi=1
nXi=1
TXt=1
1
hgi (t), y(t) − x∗ i +
hgi (t), xi (t) − y(t)i .
n
s=1 Pn
n Pt−1
i=1 hgi (s), xi + 1
By deﬁnition of ¯z (t) and y(t), we have y(t) = argminx∈X { 1
α(t) ψ(x)}.
Thus, we see that the ﬁrst term in the decomposition (8) can be written in the same way as the bound
in Lemma 2, and as a consequence, we have the bound
(cid:28) nXi=1
gi (t), y(t) − x∗(cid:29) ≤
TXt=1
TXt=1
It remains to control the ﬁnal two terms in the bounds (7) and ( 8). Since kgi (t)k∗ ≤ L by assump-
tion, we use the α-Lipschitz continuity of the projection Πψ
(·, α) [9, Theorem X.4.2.1] to see
X
nXi=1
TXt=1
nXi=1
nXi=1
TXt=1
TXt=1
L
2L
n ky(t) − xi (t)k +
hgi (t), xi (t) − y(t)i ≤
n
nXi=1 (cid:13)(cid:13)(cid:13)Πψ
(−zi (t), α(t))(cid:13)(cid:13)(cid:13) ≤
nXi=1
TXt=1
TXt=1
2L
(− ¯z (t), α(t)) − Πψ
α(t) k ¯z (t) − zi (t)k∗
=
X
X
n
Combining this bound with (7) and (9) yields the running sum bound
TXt=1 (cid:2)f (y(t)) − f (x∗ )(cid:3) ≤
nXj=1
TXt=1
TXt=1
1
α(T )
Applying Lemma 3 to (10) gives that PT
t=1 [f (xi (t)) − f (x∗ )] is upper bounded by
TXt=1
nXj=1
TXt=1
TXt=1
L2
1
2L
ψ(x∗ ) +
α(t − 1) +
n
2
α(T )
Dividing both sides by T and using convexity of f yields the bound in Theorem 1.

α(t) k ¯z (t) − zj (t)k∗

α(t) k ¯z (t) − zj (t)k∗

α(t) k ¯z (t) − zi (t)k∗

ky(t) − xi (t)k

α(t − 1) +

2L
n

ψ(x∗ ) +

L2
2

+ L

.

.

. (10)

Proof of Theorem 2: For this proof sketch, we adopt the following notational conventions. For
an n × n matrix B , we call its singular values σ1 (B ) ≥ σ2 (B ) ≥ · · · ≥ σn (B ) ≥ 0. For a real
symmetric B , we use λ1 (B ) ≥ λ2 (B ) ≥ . . . ≥ λn (B ) to denote the n real eigenvalues of B . We let
∆n = {x ∈ Rn | x (cid:23) 0, Pn
i=1 xi = 1} denote the n-dimensional probability simplex. We make
frequent use of the following inequality [10]: for any positive integer t = 1, 2, . . . and any x ∈ ∆n ,
2 (cid:13)(cid:13)P tx − 11/n(cid:13)(cid:13)1 ≤
(cid:13)(cid:13)P tx − 11/n(cid:13)(cid:13)TV =
√n (cid:13)(cid:13)P tx − 11/n(cid:13)(cid:13)2 ≤
σ2 (P )t√n.
1
1
1
(11)
2
2
n PT
t=1 Pn
We focus on controlling the network error term in Theorem 1, L
i=1 α(t) k ¯z (t) − zi (t)k∗
Deﬁne the matrix Φ(t, s) = P t−s+1 . Let [Φ(t, s)]j i be entry j of column i of Φ(t, s). Then
(cid:18) nXj=1
[Φ(t, r)]j i gj (r − 1)(cid:19) − gi (t).
tXr=s+1
nXj=1
Clearly the above reduces to the standard update (4) when s = t. Since ¯z (t) evolves simply as in
(6), we assume w.l.o.g. that zi (0) = 0 and use (12) to see
(1/n − [Φ(t − 1, s)]j i )gj (s − 1) + (cid:18) 1
nXj=1
t−1Xs=1
n

(gj (t − 1) − gi (t − 1))(cid:19). (13)

[Φ(t, s)]j i zj (s) −

zi (t) − ¯z (t) =

zi (t + 1) =

nXj=1

(12)

.

6

≤

(14)

(15)

k ¯z (t) − zi (t)k∗

gj (t − 1) − gi (t − 1)(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)∗
nXj=1
nXi=1
1
kgj (t − 1) − gi (t − 1)k∗
n

We use the fact that kgi (t)k∗ ≤ L for all i and t and (13) to see that
= (cid:13)(cid:13)(cid:13)(cid:13)
(1/n − [Φ(t − 1, s)]j i )gj (s − 1) + (cid:18) 1
nXj=1
t−1Xs=1
n
nXj=1
t−1Xs=1
kgj (s − 1)k∗ |(1/n) − [Φ(t − 1, s)]j i | +
≤
t−1Xs=1
L k[Φ(t − 1, s)]i − 11/nk1 + 2L.
Now we break the sum in (14) into two terms separated by a cutoff point bt. The ﬁrst term consists
of “throwaway” terms, that is, timesteps
s for which the Markov chain with transition matrix P
has not mixed, while the second consists of steps s for which k[Φ(t − 1, s)]i − 11/nk1 is small.
Note that the indexing on Φ(t − 1, s) = P t−s+1 implies that for small s, Φ(t − 1, s) is close to
uniform. From the inequality (11), we have k[Φ(t, s)]j − 11/nk1 ≤ √nσ2 (P )t−s+1 . Hence, if
log σ2 (P )−1 − 1, then we are guaranteed k[Φ(t, s)]j − 11/nk1 ≤ √nǫ. Thus, by setting
t − s ≥ log ǫ−1
ǫ−1 = T √n, for t − s + 1 ≥ log(T √n)
log σ2 (P )−1 , we have k[Φ(t, s)]j − 11/nk1 ≤ 1
T . For larger s, we
simply have k[Φ(t, s)]j − 11/nk1 ≤ 2. The above suggests that we split the sum at bt = log T √n
log σ2 (P )−1 .
Since t − 1 − (t − bt) = bt and there are at most T steps in the summation,
t−1−btXs=1
t−1Xs=t−bt
kΦ(t − 1, s)ei − 11/nk1 + 2L
kΦ(t − 1, s)ei − 11/nk1 + L
k ¯z (t) − zi (t)k∗ ≤ L
log(T √n)
log(T √n)
≤ 2L
log σ2 (P )−1 + 3L ≤ 2L
+ 3L.
1 − σ2 (P )
The last inequality follows from the concavity of log(·), since log σ2 (P )−1 ≥ 1 − σ2 (P ).
Combining (15) with the running sum bound in (10) of the proof of the basic theorem, Theorem 1,
we ﬁnd that for x∗ ∈ X ,
α(t) + 4L2 log(T √n)
TXt=1
TXt=1
TXt=1
TXt=1
f (y(t)) − f (x∗ ) ≤
α(t − 1) + 6L2
1 − σ2 (P )
Appealing to Lemma 3 allows us to obtain the same result on the sequence xi (t) with slightly
t=1 t−1/2 ≤ 2√T − 1, using the assumption that ψ(x∗ ) ≤ R2 , bounding
worse constants. Since PT
T PT
f (bxi (T )) ≤ 1
t=1 f (xi (t)), and setting α(t) as in the theorem statement completes the proof.
5 Simulations

α(t).

1
α(T )

ψ(x∗ ) +

L2
2

In this section, we report experimental results on the network scaling behavior of the distributed
dual averaging algorithm as a function of the graph structure and number of processors n. These
results illustrate the excellent agreement of the empirical behavior with our theoretical predictions.
For all experiments reported here, we consider distributed minimization of a sum of hinge losses.
We solve a synthetic classi ﬁcation problem, in which we are g iven n pairs of the form (ai , yi ) ∈
Rd × {−1, +1}, where ai ∈ Rd corresponds to a feature vector and yi ∈ {−1, +1} is the associated
label. Given the shorthand notation [c]+ := max{0, c}, the hinge loss associated with a linear
classi ﬁer based on x is given by fi (x) = [1 − yi hai , xi]+ . The global objective is given by the sum
n Pn
f (x) := 1
i=1 [1 − yi hai , xi]+ . Setting L = maxi kai k2 , we note that f is L-Lipschitz and
non-smooth at any point with hai , xi = yi . As is common, we impose a quadratic regularization,
choosing X = {x ∈ Rd | kxk2 ≤ 5}. Then for a given graph size n, we form a random instance
of this SVM classi ﬁcation problem. Although this is a speci ﬁ
c ensemble of problems, we have
observed qualitatively similar behavior for other problem classes. In all cases, we use the optimal
setting of the step size α speci ﬁed in Theorem 2 and Corollary 1.

7

100

10−1

)
∗
x
(
f

-

)
)
t
(
x
(
f

10−2

1400

1200

1000

800

600

400

ε

o
t

s
p
e
t
S

 

n = 225
n = 400
n = 625

T (ǫ; 400)

T (ǫ; 625)

T (ǫ; 225)

 
0

200

400

600
Iterations

800

1000

1200

Figure 2. Plot of the function error ver-
sus the number of iterations for a grid
graph. Each curve corresponds to a grid
with a different number of nodes (n ∈
{225, 400, 600}). As expected, larger
graphs require more iterations to reach
a pre-speciﬁed tolerance ǫ > 0, as de-
ﬁned by the iteration number T (ǫ; n).
The network scaling problem is to de-
termine how T (ǫ; n) scales as a func-
tion of n.

500

450

400

ε

o
t

350

s
p
e
t
S

300

250

200

120

110

100

90

80

ε

o
t

s
p
e
t
S

200
0

200

150
0

200

70
0

200

800

800

1000

1000

400
600
400
600
400
600
Nodes n
Nodes n
Nodes n
(c)
(b)
(a)
Figure 3. Each plot shows the number of iterations required to reach a ﬁxed accur acy ǫ (vertical axis)
versus the network size n (horizontal axis). Panels show the same plot for different graph topologies:
(a) single cycle; (b) two-dimensional grid; and (c) bounded degree expander.
Figure 2 provides plots of the function error maxi [f (bxi (T ) − f (x∗ )] versus the number of iterations
for grid graphs with a varying number of nodes n ∈ {225, 400, 625}. In addition to demonstrating
convergence, these plots also show how the convergence time scales as a function of the graph size.
We also experimented with the algorithm and stepsize suggested by previous analyses [21]; the
resulting stepsize is so small that the method effectively jams and makes no progress.

1000

800

In Figure 3, we compare the theoretical predictions of Corollary 1 with the actual behavior of dual
subgradient averaging. Each panel shows the function TG (ǫ; n) versus the graph size n for the ﬁxed
value ǫ = 0.1; the three different panels correspond to different graph types: cycles (a), grids (b) and
expanders (c). In the panels, each point on the solid blue curve is the average of 20 trials, and the
bars show standard errors. For comparison, the dotted black line shows the theoretical prediction.
Note that the agreement between the empirical behavior and theoretical predictions is excellent in
all cases. In particular, panel (a) exhibits the quadratic scaling predicted for the cycle, panel (b)
exhibits the the linear scaling expected for the grid, and panel (c) shows that expander graphs have
the desirable property of having constant network scaling.

6 Conclusions

In this paper, we have developed and analyzed an efﬁcient alg orithm for distributed optimization
based on dual averaging of subgradients.
In addition to establishing convergence, we provided
a careful analysis of the algorithm’s network scaling. Our results show an inverse scaling in the
spectral gap of the graph, and we showed that this prediction is tight in general via a matching
lower bound. We have implemented our method, and our simulations show that these theoretical
predictions provide a very accurate characterization of its behavior. In the extended version of this
paper [4], we also show that it is possible to extend our algorithm and analysis to the cases in which
communication is random and not ﬁxed, the algorithm receive s stochastic subgradient information,
and for minimization of composite regularized objectives of the form f (x) + ϕ(x).

Acknowledgements:
JCD was supported by an NDSEG fellowship and Google. AA was sup-
ported by a Microsoft Research Fellowship. In addition, AA was partially supported by NSF grants
DMS-0707060 and DMS-0830410. MJW and AA were partially supported by AFOSR-09NL184.

8

References

[1] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: numerical methods.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1989.
[2] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms. IEEE Trans-
actions on Information Theory, 52(6):2508–2530, 2006.
[3] F.R.K. Chung. Spectral Graph Theory. AMS, 1998.
[4] J. Duchi, A. Agarwal, and M. Wainwright. Dual averaging for distributed optimization: con-
vergence analysis and network scaling. URL http://arxiv.org/abs/1005.2012, 2010.
[5] J. Friedman, J. Kahn, and E. Szemer ´edi. On the second eigenvalue of random regular graphs.
In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, pages
587–598, New York, NY, USA, 1989. ACM.
[6] R. Gray. Toeplitz and circulant matrices: A review. Foundations and Trends in Communica-
tions and Information Theory, 2(3):155–239, 2006.
[7] P. Gupta and P. R. Kumar. The capacity of wireless networks. IEEE Transactions on Informa-
tion Theory, 46(2):388–404, 2000.
[8] J. Hiriart-Urruty and C. Lemar ´echal. Convex Analysis and Minimization Algorithms I.
Springer, 1996.
[9] J. Hiriart-Urruty and C. Lemar ´echal. Convex Analysis and Minimization Algorithms II.
Springer, 1996.
[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.
[11] B. Johansson, M. Rabi, and M. Johansson. A randomized incremental subgradient method for
distributed optimization in networked systems. SIAM Journal on Optimization, 20(3):1157–
1170, 2009.
[12] A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. Journal of Com-
puter and System Sciences, 71(3):291–307, 2005.
[13] V. Lesser, C. Ortiz, and M. Tambe, editors. Distributed Sensor Networks: A Multiagent Per-
spective, volume 9. Kluwer Academic Publishers, May 2003.
[14] D. Levin, Y. Peres, and E. Wilmer. Markov Chains and Mixing Times. American Mathematical
Society, 2008.
[15] I. Lobel and A. Ozdaglar. Distributed subgradient methods over random networks. Technical
Report 2800, MIT LIDS, 2008.
[16] R. McDonald, K. Hall, and G. Mann. Distributed training strategies for the structured percep-
tron. In North American Chapter of the Association for Computational Linguistics (NAACL),
2010.
[17] A. Nedic and D. P. Bertsekas.
Incremental subgradient methods for nondifferentiable opti-
mization. SIAM Journal on Optimization, 12(1):109–138, 2001.
[18] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 54:48–61, 2009.
[19] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization .
Wiley, New York, 1983.
[20] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Program-
ming A, 120(1):261–283, 2009.
[21] S. Sundhar Ram, A. Nedic, and V. V. Veeravalli. Distributed subgradient projection algorithm
for convex optimization. In IEEE International Conference on Acoustics, Speech, and Signal
Processing, pages 3653–3656, 2009.
[22] J. Tsitsiklis. Problems in decentralized decision making and computation. PhD thesis, Mas-
sachusetts Institute of Technology, 1984.
[23] U. von Luxburg, A. Radl, and M. Hein. Hitting times, commute distances, and the spectral gap
for large random geometric graphs. URL http://arxiv.org/abs/1003.1266, 2010.
[24] L. Xiao, S. Boyd, and S. J. Kim. Distributed average consensus with least-mean-square devia-
tion. Journal of Parallel and Distributed Computing, 67(1):33–46, 2007.

9


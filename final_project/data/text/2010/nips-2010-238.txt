Sparse Inverse Covariance Selection via
Alternating Linearization Methods

Katya Scheinberg
Department of ISE
Lehigh University
katyas@lehigh.edu

Shiqian Ma, Donald Goldfarb
Department of IEOR
Columbia University
{sm2756,goldfarb}@columbia.edu

Abstract

Gaussian graphical models are of great interest in statistical learning. Because the
conditional independencies between different nodes correspond to zero entries in
the inverse covariance matrix of the Gaussian distribution, one can learn the struc-
ture of the graph by estimating a sparse inverse covariance matrix from sample
data, by solving a convex maximum likelihood problem with an ℓ1 -regularization
term. In this paper, we propose a ﬁrst-order method based on an alternating lin-
earization technique that exploits the problem’s special structure; in particular, the
subproblems solved in each iteration have closed-form solutions. Moreover, our
algorithm obtains an ϵ-optimal solution in O(1/ϵ) iterations. Numerical experi-
ments on both synthetic and real data from gene association networks show that a
practical version of this algorithm outperforms other competitive algorithms.

1 Introduction

In multivariate data analysis, graphical models such as Gaussian Markov Random Fields pro-
vide a way to discover meaningful interactions among variables. Let Y = {y (1) , . . . , y (n) } be
an n-dimensional random vector following an n-variate Gaussian distribution N (µ, (cid:6)), and let
G = (V , E ) be a Markov network representing the conditional independence structure of N (µ, (cid:6)).
Speciﬁcally, the set of vertices V = {1, . . . , n} corresponds to the set of variables in Y , and the
edge set E contains an edge (i, j ) if and only if y (i) is conditionally dependent on y (j ) given all
remaining variables; i.e., the lack of an edge between i and j denotes the conditional indepen-
−1
dence of y (i) and y (j ) , which corresponds to a zero entry in the inverse covariance matrix (cid:6)
([1]). Thus learning the structure of this graphical model is equivalent to the problem of learning the
−1 . To estimate this sparse inverse covariance matrix, one can solve the following
zero-pattern of (cid:6)
∑
∑
log det(X ) − ⟨ ^(cid:6), X ⟩ − ρ∥X ∥0 ,
sparse inverse covariance selection (SICS) problem: maxX∈Sn
++ denotes the set of n × n positive deﬁnite matrices, ∥X ∥0 is the number of nonzeros in
++
where S n
i=1 (Yi − ^β )(Yi − ^β )
⊤ is the sample covariance matrix, ^β = 1
p
p
X , ^(cid:6) = 1
∑
i=1 Yi is the sample
p
p
mean and Yi is the i-th random sample of Y . This problem is NP-hard in general due to the com-
binatorial nature of the cardinality term ρ∥X ∥0 ([2]). To get a numerically tractable problem, one
can replace the cardinality term ∥X ∥0 by ∥X ∥1 :=
|Xij |, the envelope of ∥X ∥0 over the set
{X ∈ Rn×n : ∥X ∥∞ ≤ 1} (see [3]). This results in the convex optimization problem (see e.g.,
i;j
[4, 5, 6, 7]):

− log det(X ) + ⟨ ^(cid:6), X ⟩ + ρ∥X ∥1 .
(1)
max∥U ∥1≤(cid:26) − log det X + ⟨ ^(cid:6) + U, X ⟩, where ∥U ∥∞
Note that (1) can be rewritten as minX ∈Sn
++
is the largest absolute value of the entries of U . By exchanging the order of max and min, we obtain

min
X∈Sn
++

1

− log det X + ⟨ ^(cid:6) + U, X ⟩, which is equivalent to
the dual problem max∥U ∥1≤(cid:26) minX∈Sn
++
{log det W + n : ∥W − ^(cid:6)∥∞ ≤ ρ}.

max
W ∈Sn
++

(2)

Both the primal and dual problems have strictly convex objectives; hence, their optimal solutions
−1 is primal feasible resulting in the duality gap
are unique. Given a dual solution W , X = W
gap := ⟨ ^(cid:6), W
−1 ⟩ + ρ∥W
−1 ∥1 − n.

(3)

The primal and the dual SICS problems (1) and (2) are semideﬁnite programming problems and can
be solved via interior point methods (IPMs) in polynomial time. However, the per-iteration com-
putational cost and memory requirements of an IPM are prohibitively high for the SICS problem.
Although an approximate IPM has recently been proposed for the SICS problem [8], most of the
methods developed for it are ﬁrst-order methods. Banerjee et al. [7] proposed a block coordinate
descent (BCD) method to solve the dual problem (2). Their method updates one row and one column
of W in each iteration by solving a convex quadratic programming problem by an IPM. The g lasso
method of Friedman et al. [5] is based on the same BCD approach as in [7], but it solves each sub-
problem as a LASSO problem by yet another coordinate descent (CD) method [9]. Sun et al. [10]
proposed solving the primal problem (1) by using a BCD method. They formulate the subproblem
as a min-max problem and solve it using a prox method proposed by Nemirovski [11]. The SINCO
method proposed by Scheinberg and Rish [12] is a greedy CD method applied to the primal problem.
All of these BCD and CD approaches lack iteration complexity bounds. They also have been shown
to be inferior in practice to gradient based approaches. A projected gradient method for solving
the dual problem (2) that is considered to be state-of-the-art for SICS was proposed by Duchi et al.
[13]. However, there are no iteration complexity results for it either. Variants of Nesterov’s method
[14, 15] have been applied to solve the SICS problem. d’Aspremont et al. [16] applied Nesterov’s
optimal ﬁrst-order method to solve the primal problem (1) after smoothing the nonsmooth ℓ1 term,
obtaining an iteration complexity bound of O(1/ϵ) for an ϵ-optimal solution, but the implementation
√
in [16] was very slow and did not produce good results. Lu [17] solved the dual problem (2), which
is a smooth problem, by Nesterov’s algorithm, and improved the iteration complexity to O(1/
ϵ).
However, since the practical performance of this algorithm was not attractive, Lu gave a variant
(VSM) of it that exhibited better performance. The iteration complexity of VSM is unknown. Yuan
[18] proposed an alternating direction method based on an augmented Lagrangian framework (see
the ADAL method (8) below). This method also lacks complexity results. The proximal point algo-
rithm proposed by Wang et al. in [19] requires a reformulation of the problem that increases the size
of the problem making it impractical for solving large-scale problems. Also, there is no iteration
complexity bound for this algorithm. The IPM in [8] also requires such a reformulation.
Our contribution. In this paper, we propose an alternating linearization method (ALM) for solving
the primal SICS problem. An advantage of solving the primal problem is that the ℓ1 penalty term in
the objective function directly promotes sparsity in the optimal inverse covariance matrix.
Although developed independently, our method is closely related to Yuan’s method [18]. Both
methods exploit the special form of the primal problem (1) by alternatingly minimizing one of
the terms of the objective function plus an approximation to the other term. The main difference
between the two methods is in the construction of these approximations. As we will show, our
method has a theoretically justiﬁed interpretation and is based on an algorithmic framework with
complexity bounds, while no complexity bound is available for Yuan’s method. Also our method
has an intuitive interpretation from a learning perspective. Extensive numerical test results on both
synthetic data and real problems have shown that our ALM algorithm signiﬁcantly outperforms
other existing algorithms, such as the PSM algorithm proposed by Duchi et al. [13] and the VSM
algorithm proposed by Lu [17]. Note that it is shown in [13] and [17] that PSM and VSM outperform
the BCD method in [7] and g lasso in [5].
Organization of the paper. In Section 2 we brieﬂy review alternating linearization methods for
minimizing the sum of two convex functions and establish convergence and iteration complexity
results. We show how to use ALM to solve SICS problems and give intuition from a learning
perspective in Section 3. Finally, we present some numerical results on both synthetic and real data
in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17].

2

2 Alternating Linearization Methods

We consider here the alternating linearization method (ALM) for solving the following problem:
min F (x) ≡ f (x) + g(x),
(4)
where f and g are both convex functions. An effective way to solve (4) is to “split” f and g by
introducing a new variable, i.e., to rewrite (4) as
{f (x) + g(y) : x − y = 0},
min
x;y
and apply an alternating direction augmented Lagrangian method to it. Given a penalty parameter
1/µ, at the k-th iteration, the augmented Lagrangian method minimizes the augmented Lagrangian
function
∥x − y∥2
L(x, y ; λ) := f (x) + g(y) − ⟨λ, x − y⟩ +
2 ,
with respect to x and y , i.e., it solves the subproblem
L(x, y ; λk ),

1
2µ

(5)

(xk , yk ) := arg min
x;y

(6)

(8)

and updates the Lagrange multiplier λ via:
λk+1 := λk − (xk − yk )/µ.
(7)
Since minimizing L(x, y ; λ) with respect to x and y jointly is usually difﬁcult, while doing so with
 xk+1
respect to x and y alternatingly can often be done efﬁciently, the following alternating direction
version of the augmented Lagrangian method (ADAL) is often advocated (see, e.g., [20, 21]):
:= arg minx L(x, yk ; λk )
:= arg miny L(xk+1 , y ; λk )
yk+1
:= λk − (xk+1 − yk+1 )/µ.
λk+1
version of the ADAL method. xk+1
If we also update λ after we solve the subproblem with respect to x, we get the following symmetric
:= arg minx L(x, yk ; λk
y )
− (xk+1 − yk )/µ
:= λk
λk+1
:= arg miny L(xk+1 , y ; λk+1
y
x
yk+1
)
− (xk+1 − yk+1 )/µ.
x
:= λk+1
λk+1
x
y
Algorithm (9) has certain theoretical advantages when f and g are smooth. In this case, from the
ﬁrst-order optimality conditions for the two subproblems in (9), we have that:
x = ∇f (xk+1 )
y = −∇g(yk+1 ).
and λk+1
(10)
λk+1
Substituting these relations into (9), we obtain the following equivalent algorithm for solving (4),
which we refer to as the alternating linearization minimization (ALM) algorithm.
Algorithm 1 Alternating linearization method (ALM) for smooth problem
⟩
⟨∇g(yk ), x − yk
Input: x0 = y0
⟨∇f (xk+1 ), y − xk+1
⟩
for k = 0, 1, · · · do
∥x − yk ∥2
1. Solve xk+1 := arg minx Qg (x, yk ) ≡ f (x) + g(yk ) +
2 ;
+ 1
2. Solve yk+1 := arg miny Qf (xk+1 , y) ≡ f (xk+1 ) +
∥y −
2(cid:22)
+ 1
xk+1 ∥2
2(cid:22)
2 + g(y);
end for

(9)

Algorithm 1 can be viewed in the following way: at each iteration we construct a quadratic approxi-
mation of the function g(x) at the current iterate yk and minimize the sum of this approximation and
f (x). The approximation is based on linearizing g(x) (hence the name ALM) and adding a “prox”
∥x − yk ∥2
2 . When µ is small enough (µ ≤ 1/L(g), where L(g) is the Lipschitz constant for
term 1
2(cid:22)

3

λk , x − yk

+ 1
2(cid:22)

∥x − yk ∥2
2 ;

⟨∇g(yk ), x − yk
⟩
∇g ) this quadratic function, g(yk ) +
∥x − yk ∥2
2 is an upper approximation to
+ 1
⟨∇f (xk+1 ), y − xk+1
⟩
2(cid:22)
g(x), which means that the reduction in the value of F (x) achieved by minimizing Qg (x, yk ) in Step
1 is not smaller than the reduction achieved in the value of Qg (x, yk ) itself. Similarly, in Step 2 we
∥y − xk+1∥2
build an upper approximation to f (x) at xk+1 , f (xk+1 )+
+ 1
2 ,
2(cid:22)
and minimize the sum Qf (xk+1 , y) of it and g(y).
Let us now assume that f (x) is in the class C 1;1 with Lipschitz constant L(f ), while g(x) is simply
convex. Then from the ﬁrst-order optimality conditions for the second minimization in (9), we have
−λk+1
∈ ∂ g(yk+1 ), the subdifferential of g(y) at y = yk+1 . Hence, replacing ∇g(yk ) in the
deﬁnition of Qg (x, yk ) by −λk+1
y
in (9), we obtain the following modiﬁed version of (9).
y
Algorithm 2 Alternating linearization method with skipping step
1. Solve xk+1 := arg minx Q(x, yk ) ≡ f (x) + g(yk ) − ⟨
⟩
Input: x0 = y0
for k = 0, 1, · · · do
2. If F (xk+1 ) > Q(xk+1 , yk ) then xk+1 := yk .
3. Solve yk+1 := arg miny Qf (xk+1 , y);
4. λk+1 = ∇f (xk+1 ) − (xk+1 − yk+1 )/µ.
end for
Algorithm 2 is identical to the symmetric ADAL algorithm (9) as long as F (xk+1 ) ≤ Q(xk+1 , yk )
at each iteration (and to Algorithm 1 if g(x) is in C 1;1 and µ ≤ 1/ max{L(f ), L(g)}). If this con-
dition fails, then the algorithm simply sets xk+1 ← yk . Algorithm 2 has the following convergence
property and iteration complexity bound. For a proof see the Appendix.
Theorem 2.1. Assume ∇f is Lipschitz continuous with constant L(f ). For β /L(f ) ≤ µ ≤ 1/L(f )
where 0 < β ≤ 1, Algorithm 2 satisﬁes
) ≤ ∥x0 − x
∗ ∥2
F (yk ) − F (x
, ∀k ,
∗
(11)
2µ(k + kn )
∗ is an optimal solution of (4) and kn is the number of iterations until the k − th for which
where x
F (xk+1 ) ≤ Q(xk+1 , yk ). Thus Algorithm 2 produces a sequence which converges to the optimal
solution in function value, and the number of iterations needed is O(1/ϵ) for an ϵ-optimal solution.
If g(x) is also a smooth function in the class C 1;1 with Lipschitz constant L(g) ≤ 1/µ, then Theorem
2.1 also applies to Algorithm 1 since in this case kn = k (i.e., no “skipping” occurs). Note that the
√
iteration complexity bound in Theorem 2.1 can be improved. Nesterov [15, 22] proved that one can
obtain an optimal iteration complexity bound of O(1/
ϵ), using only ﬁrst-order information. His
acceleration technique is based on using a linear combination of previous iterates to obtain a point
where the approximation is built. This technique has been exploited and extended by Tseng [23],
√
Beck and Teboulle [24], Goldfarb et al. [25] and many others. A similar technique can be adopted
to derive a fast version of Algorithm 2 that has an improved complexity bound of O(1/
ϵ), while
keeping the computational effort in each iteration almost unchanged. However, we do not present
this method here, since when applied to the SICS problem, it did not work as well as Algorithm 2.

3 ALM for SICS

The SICS problem

F (X ) ≡ f (X ) + g(X ),
(12)
min
X ∈Sn
++
where f (X ) = − log det(X ) + ⟨ ^(cid:6), X ⟩ and g(X ) = ρ∥X ∥1 , is of the same form as (4). However,
in this case neither f (X ) nor g(X ) have Lipschitz continuous gradients. Moreover, f (X ) is only
deﬁned for positive deﬁnite matrices while g(X ) is deﬁned everywhere. These properties of the
objective function make the SICS problem especially challenging for optimization methods. Nev-
ertheless, we can still apply (9) to solve the problem directly. Moreover, we can apply Algorithm 2
and obtain the complexity bound in Theorem 2.1 as follows.

4

The log det(X ) term in f (X ) implicitly requires that X ∈ S n
++ and the gradient of f (X ), which
is given by −X
−1 + ^(cid:6), is not Lipschitz continuous in S n
++ . Fortunately, as proved in Proposition
∗ ≽ αI , where α =
3.1 in [17], the optimal solution of (12) X
. Therefore, if we deﬁne
1
∥ ^(cid:6)∥+n(cid:26)
C := {X ∈ S n : X ≽ (cid:11)
2 I }, the SICS problem (12) can be formulated as:
{f (X ) + g(Y ) : X − Y = 0, X ∈ C , Y ∈ C }.
(13)
min
X;Y
We can include constraints X ∈ C in Step 1 and Y ∈ C in Step 3 of Algorithm 2. Theorem 2.1
can then be applied as discussed in [25]. However, a difﬁculty now arises when performing the
minimization in Y . Without the constraint Y ∈ C , only a matrix shrinkage operation is needed,
but with this additional constraint the problem becomes harder to solve. Minimization in X with or
without the constraint X ∈ C is accomplished by performing an SVD. Hence the constraint can be
easily imposed.
Instead of imposing constraint Y ∈ C we can obtain feasible solutions by a line search on µ. We
know that the constraint X ≽ (cid:11)
2 I is not tight at the solution. Hence if we start the algorithm with
X ≽ αI and restrict the step size µ to be sufﬁciently small then the iterates of the method will
remain in C .
Note however, that the bound on the Lipschitz constant of the gradient of f (X ) is 1/α2 and hence
can be very large. It is not practical to restrict µ in the algorithm to be smaller than α2 , since µ
determines the step size at each iteration. Hence, for a practical approach we can only claim that the
theoretical convergence rate bound holds in only a small neighborhood of the optimal solution. We
now present a practical version of our algorithm applied to the SICS problem.

Algorithm 3 Alternating linearization method (ALM) for SICS
Input: X 0 = Y 0 , µ0 .
for k = 0, 1, · · · do
0. Pick µk+1 ≤ µk .
1. Solve X k+1 := arg minX ∈C f (X ) + g(Y k ) − ⟨(cid:3)k , X − Y k ⟩ + 1
∥X − Y k ∥2
F ;
2. If g(X k+1 ) > g(Y k ) − ⟨(cid:3)k , X k+1 − Y k ⟩ + 1
∥X k+1 − Y k ∥2
2(cid:22)k+1
F , then X k+1 := Y k .
3. Solve Y k+1 := arg minY f (X k+1 ) + ⟨∇f (X k+1 ), Y − X k+1 ⟩ + 1
∥Y − X k+1 ∥2
2(cid:22)k+1
F +
2(cid:22)k+1
g(Y );
4. (cid:3)k+1 = ∇f (X k+1 ) − (X k+1 − Y k+1 )/µk+1 .
end for

Consider V Diag(d)V

We now show how to solve the two optimization problems in Algorithm 3. The ﬁrst-order optimality
conditions for Step 1 in Algorithm 3, ignoring the constraint X ∈ C are:
∇f (X ) − (cid:3)k + (X − Y k )/µk+1 = 0.
(
)
√
⊤ - the spectral decomposition of Y k + µk+1 ((cid:3)k − ^(cid:6)) and let
(15)
d2
i + 4µk+1
γi =
di +
/2, i = 1, . . . , n.
)
}
{
(
√
Since ∇f (X ) = −X
−1 + ^(cid:6), it is easy to verify that X k+1 := V Diag(γ )V
⊤ satisﬁes (14). When
the constraint X ∈ C is imposed, the optimal solution changes to X k+1 := V Diag(γ )V
⊤ with
, i = 1, . . . , n. We observe that solving (14) requires
d2
i + 4µk+1
/2
γi = max
α/2,
di +
approximately the same effort (O(n3 )) as is required to compute ∇f (X k+1 ). Moreover, from the
solution to (14), ∇f (X k+1 ) is obtained with only a negligible amount of additional effort, since
−1 := V Diag(γ )
−1V
⊤ .
(X k+1 )
The ﬁrst-order optimality conditions for Step 2 in Algorithm 3 are:
0 ∈ ∇f (X k+1 ) + (Y − X k+1 )/µk+1 + ∂ g(Y ).
Since g(Y ) = ρ∥Y ∥1 , it is well known that the solution to (16) is given by
Y k+1 = shrink(X k+1 − µk+1 ( ^(cid:6) − (X k+1 )
−1 ), µk+1ρ),

(14)

(16)

5

where the “shrinkage operator” shrink(Z, ρ) updates each element Zij of the matrix Z by the for-
mula shrink(Z, ρ)ij = sgn(Zij ) · max{|Zij | − ρ, 0}.
The O(n3 ) complexity of Step 1, which requires a spectral decomposition, dominates the O(n2 )
complexity of Step 2 which requires a simple shrinkage. There is no closed-form solution for the
subproblem corresponding to Y when the constraint Y ∈ C is imposed. Hence, we neither impose
this constraint explicitly nor do so by a line search on µk , since in practice this degrades the perfor-
mance of the algorithm substantially. Thus, the resulting iterates Y k may not be positive deﬁnite,
while the iterates X k remain so. Eventually due to the convergence of Y k and X k , the Y k iterates
become positive deﬁnite and the constraint Y ∈ C is satisﬁed.
Let us now remark on the learning based intuition behind Algorithm 3. We recall that −(cid:3)k ∈
∂ g(Y k ). The two steps of the algorithm can be written as
∥X − (Y k + µk+1(cid:3)k )∥2
{f (X ) +
1
F
2µk+1

X k+1 := arg min
X ∈C

(17)

}

and

(18)

}.
−1 ))∥2
∥Y − (X k+1 − µk+1 ( ^(cid:6) − (X k+1 )
{g(Y ) +
1
Y k+1 := arg min
F
2µk+1
Y
The SICS problem is trying to optimize two conﬂicting objectives: on the one hand it tries to ﬁnd a
−1 that best ﬁts the observed data, i.e., is as close to ^(cid:6) as possible, and on the
covariance matrix X
other hand it tries to obtain a sparse matrix X . The proposed algorithm address these two objectives
in an alternating manner. Given an initial “guess” of the sparse matrix Y k we update this guess
by a subgradient descent step of length µk+1 : Y k + µk+1(cid:3)k . Recall that −(cid:3)k ∈ ∂ g(Y k ). Then
problem (17) seeks a solution X that optimizes the ﬁrst objective (best ﬁt of the data) while adding
a regularization term which imposes a Gaussian prior on X whose mean is the current guess for the
sparse matrix: Y k + µk+1(cid:3)k . The solution to (17) gives us a guess for the inverse covariance X k+1 .
We again update it by taking a gradient descent step: X k+1 − µk+1 ( ^(cid:6) − (X k+1 )
−1 ). Then problem
(18) seeks a sparse solution Y while also imposing a Gaussian prior on Y whose mean is the guess
for the inverse covariance matrix X k+1 − µk+1 ( ^(cid:6) − (X k+1 )
−1 ). Hence the sequence of X k ’s is
a sequence of positive deﬁnite inverse covariance matrices that converge to a sparse matrix, while
the sequence of Y k ’s is a sequence of sparse matrices that converges to a positive deﬁnite inverse
covariance matrix.
An important question is how to pick µk+1 . Theory tells us that if we pick a small enough value,
then we can obtain the complexity bounds. However, in practice this value is too small. We discuss
the simple strategy that we use in the next section.

4 Numerical Experiments

In this section, we present numerical results on both synthetic and real data to demonstrate the
efﬁciency of our SICS ALM algorithm. Our codes for ALM were written in MATLAB. All nu-
merical experiments were run in MATLAB 7.3.0 on a Dell Precision 670 workstation with an Intel
Xeon(TM) 3.4GHZ CPU and 6GB of RAM.
Since −(cid:3)k ∈ ∂ g(Y k ), ∥(cid:3)k ∥∞ ≤ ρ; hence ^(cid:6) − (cid:3)k is a feasible solution to the dual problem (2) as
long as it is positive deﬁnite. Thus the duality gap at the k-th iteration is given by:
Dgap := − log det(X k ) + ⟨ ^(cid:6), X k ⟩ + ρ∥X k ∥1 − log det( ^(cid:6) − (cid:3)k ) − n.
(19)
We deﬁne the relative duality gap as: Rel.gap := Dgap/(1 + |pobj | + |dobj |), where pobj and dobj
are respectively the objective function values of the primal problem (12) at point X k , and the dual
problem (2) at ^(cid:6) − (cid:3)k . Deﬁning dk (ϕ(x)) ≡ max{1, ϕ(xk ), ϕ(xk−1 )}, we measure the relative
changes of objective function value F (X ) and the iterates X and Y as follows:
∥X k − X k−1 ∥F
∥Y k − Y k−1 ∥F
|F (X k ) − F (X k−1 )|
d(∥Y ∥F )
dk (∥X ∥F )
dk (|F (X )|)
We terminate ALM when either
(i) Dgap ≤ ϵgap

(ii) max{F rel, X rel, Y rel} ≤ ϵrel .

, X rel :=

, Y rel :=

F rel :=

or

.

(20)

6

Note that in (19), computing log det(X k ) is easy since the spectral decomposition of X k is already
available (see (14) and (15)), but computing log det( ^(cid:6) − (cid:3)k ) requires another expensive spectral
decomposition. Thus, in practice, we only check (20)(i) every Ngap iterations. We check (20)(ii) at
every iteration since this is inexpensive.
A continuation strategy for updating µ is also crucial to ALM. In our experiments, we adopted the
following update rule. After every N(cid:22) iterations, we set µ := max{µ · η(cid:22) , (cid:22)µ}; i.e., we simply reduce
µ by a constant factor η(cid:22) every N(cid:22) iterations until a desired lower bound on µ is achieved.
We compare ALM (i.e., Algorithm 3 with the above stopping criteria and µ updates), with the
projected subgradient method (PSM) proposed by Duchi et al. in [13] and implemented by Mark
Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be
the state-of-the-art algorithms for solving SICS problems. The per-iteration complexity of all three
algorithms is roughly the same; hence a comparison of the number of iterations is meaningful. The
parameters used in PSM and VSM are set at their default values. We used the following parameter
−8 , Ngap = 20, N(cid:22) = 20, (cid:22)µ = max{µ0 η8
−6 }, η(cid:22) =
−3 , ϵrel = 10
values in ALM: ϵgap = 10
(cid:22) , 10
1/3, where µ0 is the initial µ which is set according to ρ; speciﬁcally, in our experiments, µ0 =
100/ρ, if ρ < 0.5, µ0 = ρ if 0.5 ≤ ρ ≤ 10, and µ0 = ρ/100 if ρ > 10.

4.1 Experiments on synthetic data

We randomly created test problems using a procedure proposed by Scheinberg and Rish in [12].
Similar procedures were used by Wang et al. in [19] and Li and Toh in [8]. For a given dimension n,
we ﬁrst created a sparse matrix U ∈ Rn×n with nonzero entries equal to -1 or 1 with equal proba-
∑
bility. Then we computed S := (U ∗ U
−1 was sparse.
⊤
−1 as the true covariance matrix. Hence, S
)
We then drew p = 5n iid vectors, Y1 , . . . , Yp , from the Gaussian distribution N (0, S ) by using the
⊤
p
mvnrnd function in MATLAB, and computed a sample covariance matrix ^(cid:6) := 1
i=1 YiY
.
i
p
We compared ALM with PSM [13] and VSM [17] on these randomly created data with different
ρ. The PSM code was terminated using its default stopping criteria, which included (20)(i) with
−3 . VSM was also terminated when Dgap ≤ 10
−3 . Since PSM and VSM solve the
ϵgap = 10
dual problem (2), the duality gap which is given by (3) is available without any additional spectral
decompositions. The results are shown in Table 1. All CPU times reported are in seconds.

Table 1: Comparison of ALM, PSM and VSM on synthetic data
VSM
PSM
ALM
Rel.gap
Rel.gap
Rel.gap

Dgap

CPU

iter

n

200
500
1000
1500
2000

200
500
1000
1500
2000

200
500
1000
1500
2000

iter

300
220
180
199
200

140
100
100
140
160

180
140
160
180
240

Dgap

CPU

iter

8.70e-4
5.55e-4
9.92e-4
1.73e-3
6.13e-5

9.80e-4
1.69e-4
9.28e-4
2.17e-4
4.70e-4

4.63e-4
4.14e-4
3.19e-4
8.28e-4
9.58e-4

1.51e-6
4.10e-7
3.91e-7
4.86e-7
1.35e-8

1.15e-6
7.59e-8
2.12e-7
3.39e-8
5.60e-8

4.63e-7
1.56e-7
6.07e-8
1.07e-7
9.37e-8

13
84
433
1405
3110

6
39
247
1014
2529

8
55
394
1304
3794

1682
861
292
419
349

6106
903
489
746
613

7536
2099
774
1088
1158

Dgap
(cid:26) = 0:1
9.99e-4
9.98e-4
9.91e-4
9.76e-4
1.12e-3
(cid:26) = 0:5
1.00e-3
9.90e-4
9.80e-4
9.96e-4
9.96e-4
(cid:26) = 1:0
1.00e-3
9.96e-4
9.83e-4
9.88e-4
9.35e-4

1.74e-6
7.38e-7
3.91e-7
2.74e-7
2.46e-7

1.18e-6
4.46e-7
2.24e-7
1.55e-7
1.18e-7

1.00e-6
3.76e-7
1.87e-7
1.27e-7
9.15e-8

38
205
446
1975
3759

137
212
749
3514
6519

171
495
1172
5100
12310

857
946
741
802
915

1000
1067
1039
1191
1640

1296
1015
1310
1484
2132

9.97e-4
9.98e-4
9.97e-4
9.98e-4
1.00e-3

9.99e-4
9.99e-4
9.95e-4
9.96e-4
9.99e-4

9.96e-4
9.97e-4
9.97e-4
9.96e-4
9.99e-4

1.73e-6
7.38e-7
3.94e-7
2.80e-7
2.20e-7

1.18e-6
4.50e-7
2.27e-7
1.55e-7
1.19e-7

9.96e-7
3.76e-7
1.90e-7
1.28e-7
9.77e-8

CPU

37
377
1928
6340
16085

43
425
2709
9405
28779

57
406
3426
11749
37406

From Table 1 we see that on these randomly created SICS problems, ALM outperforms PSM and
VSM in both accuracy and CPU time with the performance gap increasing as ρ increases. For
example, for ρ = 1.0 and n = 2000, ALM achieves Dgap = 9.58e − 4 in about 1 hour and 15
minutes, while PSM and VSM need about 3 hours and 25 minutes and 10 hours and 23 minutes,
respectively, to achieve similar accuracy.
1The MATLAB can be downloaded from http://www.cs.ubc.ca/(cid:24)schmidtm/Software/PQN.html
2The MATLAB code can be downloaded from http://www.math.sfu.ca/(cid:24)zhaosong

7

4.2 Experiments on real data

We tested ALM on real data from gene expression networks using the ﬁve data sets from [8] provided
to us by Kim-Chuan Toh: (1) Lymph node status; (2) Estrogen receptor; (3) Arabidopsis thaliana;
(4) Leukemia; (5) Hereditary breast cancer. See [8] and references therein for the descriptions of
these data sets. Table 2 presents our test results. As suggested in [8], we set ρ = 0.5. From Table 2
we see that ALM is much faster and provided more accurate solutions than PSM and VSM.

Table 2: Comparison of ALM, PSM and VSM on real data

prob.
(1)
(2)
(3)
(4)
(5)

n
587
692
834
1255
1869

iter
60
80
100
120
160

ALM
Rel.gap
5.78e-9
3.32e-8
3.27e-8
1.97e-7
1.18e-7

Dgap
9.41e-6
6.13e-5
7.26e-5
6.69e-4
5.59e-4

CPU
35
73
150
549
2158

iter
178
969
723
1405
1639

PSM
Rel.gap
5.67e-7
5.38e-7
4.50e-7
2.91e-7
2.10e-7

Dgap
9.22e-4
9.94e-4
1.00e-3
9.89e-4
9.96e-4

CPU
64
531
662
4041
14505

iter
467
953
1097
1740
3587

VSM
Rel.gap
6.01e-7
5.16e-7
3.30e-7
2.76e-7
2.09e-7

Dgap
9.78e-4
9.52e-4
7.31e-4
9.36e-4
9.93e-4

CPU
273
884
1668
8568
52978

4.3 Solution Sparsity

In this section, we compare the sparsity patterns of the solutions produced by ALM, PSM and VSM.
For ALM, the sparsity of the solution is given by the sparsity of Y . Since PSM and VSM solve
the dual problem, the primal solution X , obtained by inverting the dual solution W , is never sparse
due to ﬂoating point errors. Thus it is not fair to measure the sparsity of X or a truncated version
of X .
Instead, we measure the sparsity of solutions produced by PSM and VSM by appealing
to complementary slackness. Speciﬁcally, the (i, j )-th element of the inverse covariance matrix
is deemed to be nonzero if and only if |Wij − ^(cid:6)ij | = ρ. We give results for a random problem
(n = 500) and the ﬁrst real data set in Table 3. For each value of ρ, the ﬁrst three rows show
the number of nonzeros in the solution and the last three rows show the number of entries that are
nonzero in the solution produced by one of the methods but are zero in the solution produced by
the other method. The sparsity of the ground truth inverse covariance matrix of the synthetic data
is 6.76%. From Table 3 we can see that when ρ is relatively large (ρ ≥ 0.5), all three algorithms

ALM
PSM
VSM
ALM vs PSM
PSM vs VSM
VSM vs ALM

Table 3: Comparison of sparsity of solutions produced by ALM, PSM and VSM
5
100
50
10
1
0.5
0.1
0.05
0.01
(cid:26)
synthetic problem data
28758
15324
11844
28758
15324
11844
28758
15324
11844
0
0
0
0
0
0
0
0
0
real problem data
587
587
587
587
587
587
0
0
0
0
0
0

ALM
PSM
VSM
ALM vs PSM
PSM vs VSM
VSM vs ALM

700
700
700
0
0
0

587
587
587
0
0
0

75566
75566
75568
2
0
2

65959
65957
65959
2
0
0

63000
63000
63000
0
0
0

37613
37615
37613
0
2
0

106882
106870
106876
14
8
2

142053
142051
142051
2
0
0

2810
2810
2810
0
0
0

587
587
587
0
0
0

37510
37510
37510
0
0
0

4617
4617
4617
0
0
0

587
587
587
0
0
0

produce solutions with exactly the same sparsity patterns. Only when ρ is very small, are there slight
differences. We note that the ROC curves depicting the trade-off between the number of true positive
elements recovered versus the number of false positive elements as a function of the regularization
parameter ρ are also almost identical for the three methods.

Acknowledgements

We would like to thank Professor Kim-Chuan Toh for providing the data set used in Section 4.2. The
research reported here was supported in part by NSF Grants DMS 06-06712 and DMS 10-16571,
ONR Grant N00014-08-1-1118 and DOE Grant DE-FG02-08ER25856.

8

References
[1] S. Lauritzen. Graphical Models. Oxford University Press, 1996.
[2] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24:227–
234, 1995.
[3] J.-B. Hiriart-Urruty and C. Lemar ´echal. Convex Analysis and Minimization Algorithms II: Advanced
Theory and Bundle Methods. Springer-Verlag, New York, 1993.
[4] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika,
94(1):19–35, 2007.
[5] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 2007.
[6] M. Wainwright, P. Ravikumar, and J. Lafferty. High-dimensional graphical model selection using ℓ1 -
regularized logistic regression. NIPS, 19:1465–1472, 2007.
[7] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation for multivariate gaussian for binary data. Journal of Machine Learning Research, 9:485–516,
2008.
[8] L. Li and K.-C. Toh. An inexact interior point method for l1 -regularized sparse covariance selection.
preprint, 2010.
[9] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 58(1):267–288,
1996.
[10] L. Sun, R. Patel, J. Liu, K. Chen, T. Wu, J. Li, E. Reiman, and J. Ye. Mining brain region connectivity for
alzheimer’s disease study via sparse inverse covariance estimation. KDD’09, 2009.
[11] A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on
Optimization, 15(1):229–251, 2005.
Sinco - a greedy coordinate ascent method for sparse in-
[12] K. Scheinberg and I. Rish.
verse covariance selection problem.
Preprint available at http://www.optimization-
2009.
online.org/DB HTML/2009/07/2359.html.
[13] J. Duchi, S. Gould, and D. Koller. Projected subgradient methods for learning sparse Gaussian. Confer-
ence on Uncertainty in Artiﬁcial Intelligence (UAI 2008), 2008.
[14] Y. E. Nesterov. Smooth minimization for non-smooth functions. Math. Program. Ser. A, 103:127–152,
2005.
[15] Y. E. Nesterov. Introductory lectures on convex optimization. 87:xviii+236, 2004. A basic course.
[16] A. D’Aspremont, O. Banerjee, and L. El Ghaoui. First-order methods for sparse covariance selection.
SIAM Journal on Matrix Analysis and its Applications, 30(1):56–66, 2008.
[17] Z. Lu. Smooth optimization approach for sparse covariance selection. SIAM J. Optim., 19(4):1807–1827,
2009.
[18] X. Yuan. Alternating direction methods for sparse covariance selection. 2009. Preprint available at
http://www.optimization-online.org/DB HTML/2009/09/2390.html.
[19] C. Wang, D. Sun, and K.-C. Toh. Solving log-determinant optimization problems by a Newton-CG primal
proximal point algorithm. preprint, 2009.
[20] M. Fortin and R. Glowinski. Augmented Lagrangian methods: applications to the numerical solution of
boundary-value problems. North-Holland Pub. Co., 1983.
[21] R. Glowinski and P. Le Tallec. Augmented Lagrangian and Operator-Splitting Methods in Nonlinear
Mechanics. SIAM, Philadelphia, Pennsylvania, 1989.
[22] Y. E. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence
O(1/k2 ). Dokl. Akad. Nauk SSSR, 269:543–547, 1983.
[23] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. submitted to SIAM
J. Optim., 2008.
[24] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM J. Imaging Sciences, 2(1):183–202, 2009.
[25] D. Goldfarb, S. Ma, and K. Scheinberg. Fast alternating linearization methods for minimizing the sum of
two convex functions. Technical report, Department of IEOR, Columbia University, 2010.

9


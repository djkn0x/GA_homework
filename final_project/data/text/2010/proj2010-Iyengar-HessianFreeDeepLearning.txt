Hessian Free Deep Learning

Subodh Iyengar

December 10, 2010

1 Introduction

Optimization techniques used in Machine Learning play an important role in the
training of the Neural Network in regression and classi(cid:12)cation tasks. Predom-
inantly, (cid:12)rst order optimization methods such as Gradient Descent have been
used in the training of Neural Networks, since second order methods, such as
Newton’s method, are computationally infeasible. However, second order meth-
ods show much better convergence characteristics than (cid:12)rst order methods, be-
cause they also take into account the curvature of the error space. Additionally,
(cid:12)rst order methods require a lot of tuning of the decrease parameter, which is
application speci(cid:12)c. They also have a tendency to get trapped in local optimum
and exhibit slow convergence. Thus Newton’s method is absolutely essential to
train networks with deep architectures.
The reason for in-feasibility of Newton’s method is the computation of the
Hessian matrix, which takes prohibitively long. In(cid:13)uential work by Pearlmutter
[2] led to development of a method of using the Hessian without actually com-
puting it. Recent work [1] has involved training of a deep network consisting
of a number of Restricted Boltzmann Machine using Newton’s method without
directly computing the Hessian matrix, in a form of \Hessian free" learning.
The method had exhibited success on the MNIST handwriting recognition data
set when used to train an Restricted Boltzmann Machine using Hinton’s [3]
method, with a better quality solution for classi(cid:12)cation tasks.
The proposed work for the CS229 pro ject aims to improve upon the method
of \Hessian Free" (HF) learning and apply it to di(cid:11)erent classi(cid:12)cation tasks. To
do this, the Hessian free learning method will be implemented and results for
the experiments using MNIST will be replicated. Through analysis, it is aimed
to propose further modi(cid:12)cations that will improve the method and also run it
on di(cid:11)erent classi(cid:12)cation tasks.

2 The Hessian Free Method

Unlike Gradient Descent, in Newton’s method the update equation uses a second
derivative Hessian to compute the next step:

1

(cid:0)1 (cid:3) rf
X = X (cid:0) H
However, computing the inverse of the Hessian is an O(n3 ) operation. For
Neural Networks with deep architectures, the number of weights is large, thus
the Hessian is a very big matrix. An inversion would take a lot of time and
is simply unacceptable for practical applications. Thus, Newton’s method has
almost been neglected in work on training Neural Networks.
For practical purposes though, it is not necessary to invert the Hessian Ma-
trix. A procedure to calculate it could be formulated as:
(cid:0)1 (cid:3) rf

p = H

(1)

(2)

(3)

H p = rf
The resulting equation is of the form Ax = b and can be solved using some
method like Conjugate gradient, provided that H is positive de(cid:12)nite. However
computing the Hessian itself is an O(n2 ) operation, involving perturbing each
individual weight and propagating it forward.
The Hessian Free method has existed in literature [2] for a long time. The
idea behind the Hessian Free method derives from the equation (3). Instead
of physically computing the Hessian which is time consuming, we only need to
compute the product, H p, a matrix-vector product. We can then solve the same
equation using Conjugate Gradient for p. This can be done by taking the (cid:12)nite
di(cid:11)erence of gradient computed in the direction of p as follows:
rf ((cid:18) + (cid:15)d) (cid:0) rf ((cid:18))
H p = lim
(cid:15)!0
(cid:15)
However it is found that the Hessian matrix is usually very unstable. Schrau-
dolph adapted Pearlmutter’s R-propagation [2] method to use the Gauss-Newton
approximation to the Hessian matrix. The Gauss-Newton Matrix G is guaran-
teed to be positive de(cid:12)nite and is quite a good approximation to the Hessian,
as well as being cheaper to compute.
In a recent paper, Martens [1] uses this technique e(cid:11)ectively to train a deep
network without pre-training. Modi(cid:12)cations were proposed for example using
mini-batches and modifying the stopping conditions of CG to the Hessian free
algorithm to make it more suitable for Machine Learning applications.
His (cid:12)ndings indicate that the method performs well without pre-training
and beats the simple back-propagation learning procedure in various data sets
making it a very interesting method to study.

(4)

3 The problem with the Hessian

In the optimization framework proposed above, the conjugate gradient method
converges to a solution, provided that the Hessian matrix is positive de(cid:12)nite.
However for practical neural networks that represent non-convex functions, the

2

Hessian is not guaranteed to be positive de(cid:12)nite. This fact had a profound e(cid:11)ect
on this pro ject on which a lot of time was expended experimenting with the
Hessian only to discover that it was indeed negative de(cid:12)nite. When optimizing
the weights of the deep Network using the Hessian free method, the loss function
is found to increase rather than decrease. To ascertain whether the Hessian
matrix is indeed positive de(cid:12)nite would involve computing its eigenvalues, and
examining whether each on is positive, which is a computationally intensive
task. Instead, the graphs below that show the behavior of the residue of the
Conjugate gradient step(CG) vs. number of iterations of CG con(cid:12)rm that the
Hessian is negative de(cid:12)nite. The residue (given by the green line) increases
instead of decreasing.

(a) Residue using Gauss Newton

(b) Residue using Hessian(Green)

To subvert this dilemma the Hessian matrix is approximated by the Gauss-
Newton approximation of the Hessian H = J T (cid:3) J where J is the Jacobian
matrix of the neural network. So the CG system that must be solved becomes
[5]

(J T (cid:3) J + (cid:21)I )p = J T (cid:3) E
Where E is the loss function output, i.e. the error term, and J T (cid:3)E represents
the gradient value at that point. The Gauss Newton matrix is guaranteed to be
positive de(cid:12)nite as shown by the CG minimization step in Fig. 3(a).

4 Implementation and Experiments

Initially, the Hessian free method was implemented over the code provided by
Hinton for his Science paper [4]. However it was found that the stacked RBMs
and the other complexities of Hinton’s code resulted in a lot of memory leaks and
the program used to run out of memory easily. To put things into perspective,
the network used in Hinton’s case is a 784-1000-250-30-250-1000-784 network.
Thus the Jacobian matrix has 2.8 mil lion x size of minibatch entries of doubles
and the multiplication of J T by J thus incurs a huge memory overhead.

3

Thus Hinton’s code was reimplemented from scratch and the code was sys-
tematically tested on various sizes of neural networks to prove the correctness if
the method. In the smaller networks the solutions converged to zero very fast,
within 4 epochs. It was tested against plain gradient descent backpropagation
and it did signi(cid:12)cantly better than the latter method. In bigger networks, the
method took time to converge. Hessian Free was tested mainly on 2 neural
network architectures, 784-400-784 and 784-250-30-250-784.
To circumvent the issue of memory, and due to pro ject time constraints, the
networks were trained on a limited number of minibatches from the MNIST
dataset of size 100 each and tested on 10000 patterns on MNIST. The method
was run for a particular number of epochs until it was believed that it had
converged to a low enough value of reconstruction error. The convergence char-
acteristics of each experiment are shown in (cid:12)gures.

(c) Test Error vs. Epochs (Small Network)

(d) Error vs. Epochs (Hinton)

Figure (c) show the convergence of Hessian Free on the 784-400-784 net-
work with 20 minibatches. Figure (e) and (f ) shows the convergence of HF on

(e) Training(R) Testing(B) Error vs. Epochs

(f ) Training(R) Testing(B) Error vs. Epochs

4

01020304050607005101520253035404502040608010012014034567891011121305010015020025030051015202530354045010203040506005101520253035the 784-1000-250-30-250-1000-784 network with 20 minibatches and 100 mini-
batches respectively for training. Figure (d) shows the results of Hinton on
his deep architecture. The Table shows the resulting reconstruction errors for
various experimental parameters. The graphs are compared with the results ob-
tained from Hinton’s experiments and are found to be not quite competitive with
the pretraining methods, however the achievement of this pro ject was in getting
the reconstruction error to fall consistently, since, in numerous initial experi-
ments the reconstruction error used to decrease initially and then increase later.

Network size

Minibatches

784-400-784
784-250-30-250-784
784-250-30-250-784

20
100
20

5 Discussion

Epochs Training Er-
ror
4.4
4.15
6.2

60
50
200

Test
Error
5.1
15.48
10.15

The HF method is found to perform reasonably well for small data set size and
for minibatches of smaller size. The method as coded in this report is still not as
competitive as it should be against CG back-propagation, however with future
work on the improving the method, it should become more competitve. There
could be a lot of re(cid:12)nements the algorithm as coded in this report to make
it more memory e(cid:14)cient like using Schraudolph’s method for calculating the
Gauss Newton approximation as proposed by Martens.

6 Conclusion

In the course of the pro ject, Martens’ algorithm has been replicated to some
extent. The overarching ob jective of the pro ject, i.e to apply the methods in ap-
plications was not met, however encouraging results were obtained by applying
the method to MNIST. Instead this shall be kept in the scope of future work.
Further e(cid:11)ort must be expended to understand the HF algorithm better and
implement some of the more (cid:12)ner aspects of it.

7 Acknowledgements

I would like to thank Quoc Le for his help during this pro ject without whom
the pro ject would not have reached even this stage.

References

[1] Martens, J. Deep learning via Hessian-free optimization, ICML, 2010.

5

[2] Pearlmutter, B. A. Fast exact multiplication by the hessian. Neural
Computation, 1994.

[3] Hinton, G. E. and Salakhutdinov, R. R Reducing the dimensionality of
data with neural networks. Science, Vol. 313. no. 5786, pp. 504 - 507,
28 July 2006.

[4] http://www.cs.toronto.edu/ hinton/MatlabForSciencePaper.html

[5] Numerical Optimization, Nocedal and Wright

6


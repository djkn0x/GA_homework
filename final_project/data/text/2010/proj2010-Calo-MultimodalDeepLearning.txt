U n s u p e r v i s e d   L e a r n i n g   o f  M u l t im o d a l  
F e a t u r e s :   Im a g e s   a n d  Te x t  
 
 
Maurizio  Ca lo  Ca ligaris  
Adv ised by  Andrew  L.  Maas and  Andrew  Y.  Ng  
CS229  Fina l Project  
{mauriz io ,amaas,ang}@cs.s tan ford .edu  
 

I n t r o d u c t i o n  

1  
 
Mult imodal   learning   invo lves  relat ing   information  from  d i sparate   sources.   For   examp le,  
Wikiped ia   conta ins   text,   aud io   and   images;  YouTube  contains  aud io ,   video   and   text;  and  
Flickr  con tains  images and   text.   Our  goal  is  to   find  meaningful   represen tations o f mutimodal  
data so  as to  cap ture  as much informat ion as possib le.  

Hand -eng ineering  task -spec ific   features  for   sing le  modalities   (e.g.   aud io   or   vision)   is  by 
itsel f  a  d i fficult  task   and   is  often  very  time -consuming .   The  cha llenge  get s  signi ficantly  
pronounced  when the  data comes  from  various d i fferent  modalities (e.g.  images and  text) .    

Thus,  we  propose  an unsupe rvised   learning model wh ich  use s  images and   tags  from Flickr   to  
learn  jo int  features  tha t  model  image  and   tex t  correlations.     Furthermore,   we  demonstrate  
cross -modali ty  feature   learn ing ,   in  wh ich  better   features   for   one  modali ty  (e.g.   images)   can  
be   learned   if  mu lt ip le  moda lities   (e.g.   images  and   text)   are  present  during  feature  learning  
time.  

In  the  fo llowing  sections ,  we  presen t  the  network  architecture s we   use   to   learn  b i -modal  and  
cross -modal  features.   We  describe   an  experimental  set t ing  wh ich  demonstrat es  that   we   are 
indeed   ab le   to   learn  features  that  e ffect ive ly  cap ture  informa tion  from  d i fferent  modalities  
and   that we can  further   improve on compu ter  v ision  features  i f we have o ther  modali ties  (e.g  
text)   availab le   during  fea ture   learning  time.   We  then  c onclude  and   o ffer   suggest ions  for  
further  work.    

 
2  

D a t a s e t    

The  NUS -WIDE   dataset  p rovided   by  the  Universi ty  of  Singapore [3]   contains   links  to  
269 ,648   images  from  image   sharing  site Flickr.com,   toge ther  wi th  their  correspond ing  tags.   1 
The  NUS-WIDE  dataset  also  provides a  list of 5018 unique  tags  which appear  more  than 100 
times  in  the  dataset  and   also   appear   in  the WordNet.  By  considering on ly  tags presen t  in  th is  
list,   we   essential ly  get  rid   of   the  prob lem  that  many  tags   in  Flickr   are  no isy  (e.g.   misspelled  
tags  or   in  tags  in  ano ther   language)   or   are   irrelevan t  for   the  task  o f  feature  learning  (e.g.  
proper  names,  model of the camera  used  to  take  p icture) .    
 
   
3  
 
A  key  challenge   in   this   work  is  to   figure  out   a  way  to   comb ine  bo th   visual   and   ling uistic 
aspects  in   a  way  wh ich  allows   for   an   autoencoder   to   lea rn meaningful  represen tations   of  the  
data.   Since   the   correla tions  between  image  and   text  data   are  highly  non -l inear,   it  is  hard   for  
an  autoencoder   or   a   Restricted   Bo ltzmann  Mach ine  (RBM)  
to   form  mu l timodal  
representat ions of the  data  when fed  in  unprocessed  text and  images  as input.    
 

M e t h o d o l o g y  

                                                 
1 These  images were ob tained  by randomly crawling more  than 300 ,000  images from F lick r 's 
pub lically availab le  API ,  and  after  removing dup licates as well as those images that contain 
inappropriate  length -wid th ratios or  who se sizes are too  sma ll.      

Subsequently,   we  describe  the   deep   learning  approach  we  used   to   learn  features  that  jo intly 
model  image  and   text  correlations,   taking  as  input  images  of  variab le  size  along  wi th  their  
correspond ing tags.  
 
 
3 . 1  

V i s u a l  A s p e c t  

To  learn  the  v isual  features,   we   have  used   the  "visua l  words"   model  often  used   in  compute r  
vision.   In   this  model,   we dense -samp le each  image  to  ex tract  low - level  SIFT descrip tors.  We 
then  app ly  the   k-means  clus tering  a lgorithm  to   find   cluster   centro ids,   wh ich  forms  a  
"codebook"   or   "visual  word s"   of  canonical  descrip tors.   Fina lly,   we  use  the   codebook  to  map  
inpu t  patches  in to   1 -of-K  code   vector   ("hard   assignment" ) ,   and   ultimately  represent  each 
image   as  a  "histogram"   of  v isual  word s -  a  1000 -d imensional  vec tor   in  wh ich  the  k -th  entry  
ind icates  how  many  times  the   k -th  canonica l  descrip tor   appears  in  a  given  image.   That 
histogram  is  length -normalized   to   take  into   accoun t  the  variab ility  in  s ize  of  the  d ifferent  
image s.  

 
3 . 2  

L i n g u i s t i c  A s p e c t  

We  use   a   bag -o f-words  model  to   represent  the   tags   associated   wi th  an  image.   Us ing  the  
d ictionary  provided   by  the   NUS -WIDE   dataset,   we  rep resent  tex t  data  correspond ing  to   a 
each  image   as  a   5018 -d imensiona l  b inary  vector   who se  i -th  entry  is  either   0   or   1  depend ing  
on whether  the   i- th word   from the  d ictionary  be longs to  the list of  tags  for  the given image .    

Since   the   tags  are   so   spa rse   ( ~8-9   tags  per   image   on  average)   it  is  d i fficult   for   an  
autoencoder   or   a   Restricted   Bo ltzmann  Machin e(RBM)   to   learn  mean ingful  representat ions 
of  the   da ta .   Thus,   we   map   the   b inary  valued   vector   o f   word s  into   a   more   compac t  vector  
space   as  in  [2] .   More   specifically,   we  form  a  vector   space  model  wh ich  learns  semantica lly 
sensit ive  word   representa tions via  a  probab ilistic model of  tag co -occurrences  and   represents 
each  word   in  the   d ictionary   as  a  20 -d imensional  vector.   For   a  given  document,   we   use  as 
features  the  mean  representa tion vector,   an  average of  the word   representa tions  for   al l words 
appearing in the  document.    
3 . 3  
J o i n t   F e a t u r e   L e a r n i n g  

 

Having  learned   a   vector   space  
model  for   the   linguistic  data,  
we  concatenate   the  mean  tag  
vectors 
representing 
each  
image  
the   correspond ing 
to  
visua l  h istogram ,   and   feed   that  
as  inpu t  to   an   au toencoder, 
wh ich  a ttemp ts   to   reconstruct  
bo th 
modal ities.  
The 
autoencoder   consists  of  one  
inpu t  layer,   one   over -comp lete  
hidden   layer   wh ich  cap tures 
cross -modal 
correlations  
between image  and  text (this    

 

Figure 1  : Jo int Feature Learning  Architec ture  

layer   is  what  cons titutes  our   "jo int"   features ) ,   and   a  linear   output  layer   wh ich  we  set  to   be 
equal  to   the  inpu t.   Given  that  the   text  fea tures  are  very  low-d imens ional   compared   to   the  
visua l  features,   we  mod i fy  the   ob jective   funct ion  o f  the   autoencoder   to   account   for   d ifferent 
we ight ing  between  image   and   text   features .   The  final  ob jective  function  for   the   model  is 
given by   

 

Where   h   is  the   hidden  layer   representat ion,   W (1 )  and  W (2 )   are  the weigh ts  go ing  from  inpu t  to  
hidden  layer,   and   from  hidden  to   the  (linear)   output  layer,   υ  and   τ   are  the  visual  and   textual  
feature   vectors   respe ctively   and   μ   is  a  free  parameter   of  the  model  wh ich  contro ls  the  
relative  importance   o f  the   visua l   and   linguistic   features.   The  firs t  two   terms   thus  cor respond  
to   the   reconstruc tion   errors   for   the  d i fferent   modal ities,   whereas   the  last  two   terms   are 
regu larization  terms  wh ich  tend   to   decrease  the  Frobenius  norm  of  the  we ight  matrices  and  
prevent  overfitting.   We   run  stochastic  grad ient  descen t  to   find   the  op timal   weight  
parameters ,   wh ich  are   used   to   compu te  the  hidden  act iva tions   for   each  examp le   (our   desired  
“jo int”   fea ture  representa tion ) .      

 
3 . 4  
 

C r o s s - M o d a l i t y   L e a r n i n g  
 

The  jo int  fea ture  learning  model  is  no t  very  robust  to   missing  modali ties ,   so   it  can 't  be  used  
in  settings  in   wh ich  mu ltip le   modalit ies  are  avai lab le  during  training  time  bu t  no t  in  test ing 
time.   Thus,   we   propose   two   a lternate  models   wh ich   improve  exis ting   compute r   vision  
features  i f  textual  information  
is  avai lab le   during 
feature  
learning time .    

In  the   firs t  model  ( "Cross -
Modal  I" ,  Figure  2a ) ,   we   train  
a   network  wh ich 
learns  
to  
reconstruct  the   text  features  
given  only  visual  features   as  
inpu t .   Therefore ,   if  we   only  
have   visual  inpu t  avai lab le   at 
test  time ,   we   use   the  lea rnt  
we ights 
to  
compute 
the  
correspond ing 
unit  
hidden 
activa tions  and  hence  ob tain  a    

           Figure 2a)  

 

 

       Figure 2b)  

mu lti -modal representat ion of the data .  We also  propose a                     

E x p e r i m e n t s  

similar   network  ("Cross -Modal  II" ,  Figure  2b)   which  is  trained   to   recons truc t  bo th  
modalities when g iven on ly  visua l data.  We hypo thes ize  that  in  the process o f  recons tructing 
bo th modali ties,   the network  wi ll learn a  better  hidden representa tion o f th e data .  
 
4  
 
We evaluate   the  performance   of  the  ind ividual components of  the sys tem  on a  tag-suggest ion 
task.   The  NUS -WIDE   dataset  provides  ground -tru th  for  
81   "concep ts"   or   " tags" ,   which  mainly  correspond   to   the  
most  frequen tly  occurring  
tags  on  Fl ickr   and   are 
consistent  with  concep ts  commonly  used   in  image  
categorizat ion   (see   Tab le  1) .     For   each  of  the  269 ,648  
images ,  
the  datase t  provides   a   b inary-valued   81 
d imensional   vector,   in  wh ich  i ts  i -th  entry  ind icates 
whether   the   i- th  concep t   corresponds  to   the  image  
(which  does  no t  necessari ly  mean  that   the  i- th  concep t  
appears   in   Fl ickr   as  a   tag  fo r   the   given  imag e) .   The  fact  
that  the  ground -truths  for   the  81   concep ts were manually  
anno tated  circumven ts the  p rob lem that tags in Flickr  are 
generally  incomp le te   and   thus  al lows  us  to   execute  
supervised  
learning  experiments .  
  To   evaluate  our  
system  in  reasonab le  t ime,   we  on ly  consider   the  10  most  
frequent ly occurring concep ts for  our  experiments.    
 

Occurrence  ( %)  

2 .7 

2 .7 

7 .5 

7 .1 

2 .6 

2 .6 

5 .8 

4 .3 

3 .2 

2 .9 

Wa ter   

C louds 

St reet  

Sun 

Reflec t ion  

Concept  

Sky 

Sunset    

Beach  

Tree  

An ima l  

For   each  of  the   10   mo st  frequent ly  occurring  concep ts  in  the  dataset ,   we   remove  the  
correspond ing  tag  from  the   d ictionary  (o therwise  the  task  is  trivia l  for   components  which  
take  inputs  rela ting  to   tex t)   and   train  our   mode ls  (jo int  and   cross -modal)   in  an  unsupervised  
fashion  to   learn  an  appropriate  fea ture  representa tion  fo r   the  data.   We  use  the  ground -tru ths  
for   these   labe ls  to   separate ly  train   L2 -regular ized   logistic  regression  classi fier s.   Each  o f 
these  tag detectors   learns  whether   a  speci fic   concep t  cor responds  to   a g iven  image .  We use  a 
training  set  size   of   50000   examp les   and   test  on  ano ther   50000   examp les,   us ing   the  Area 
Under   the  (ROC)   Curve   (AUC)   metric  to   evaluate   performance  o f  each  componen t  of  the  
system on each ca tegory.   

We  PCA  wh iten  the   v isual   inpu t  (his togram)   to   200   d imensions   (wh ich  we   have   found   is 
enough)   and   norma lize  the   tag  vector   represen tations  so   as  to   have  unit  variance  and   zero 
mean.  We   train  the  networks  using 1 .4x over -comp lete h idden  layer   representation  and  using  
a  weigh ting   parameter   μ=10   to   contro l  the  re lative  importance  given  to   the  textua l   features 
(for   jo int  network  and   for   cross -modal  II) .   The  parameter   μ   was  cho sen  over   a  small  grid  
search o f parame ters to   find  the one with  best  per formance on the training set.    

 
4  

 

 

R e s u l t s  

 

F e a t u r e   R e p r e s e n t a t i o n  
R a w   T a g s   ( B a g   o f   Wo r d s )  
S I F T  
R a w   T a g s + S I F T  
S e m a n t i c   Wo r d   Ve c t o r s  
L S A  
J o i n t   F e a t u r e s  
C r o s s - M o d a l   I  
C r o s s - M o d a l   I I  

M e a n  A U C  
0 . 8 5 0  
0 . 7 5 5  
0 . 8 2 4  
0 . 8 2 1  
0 . 8 0 3  
0 . 8 6 1  
0 . 7 6 2  
0 . 7 7 3  

 
Tab le  2 : Performance of ind ividual compone nts and  comb ination of components of our  
system on  the  tag -suggestion task.  

Tab le  2   shows  the  AUC  scores of  the d i fferent  componen ts  averaged  over   the 10   concep ts.   It 
turns  out  that  l inguistic  information  is  general ly  more   use ful  for   this  task  than  the  visual 
features  (although  SIFT  does  better   than  text   on  suggesting   'c louds '  for   ins tance) .   When  
simp ly  concatenating  SIFT  histograms  to   the  tex tual  features ,   the  performance  decreases  
because o f the  poorer   visual   features.    

The  non -lineari ty  in troduced   by  the  transforma tion  o f   the  tags   into   a  more  compac t  fea ture  
space  makes   it  harder   for   a   linear   class ifier   to   per form  well   in  the  task  o f  tag   sugges tion .  
However,   our   semantic   word   vector   representat ion  sti ll  does  better   than  o ther   methods  such  
as  Latent  Semant ic  Analysis  (LSA)   and   is  wha t  ultima tely  allows  us  to   train  autoencoder  
models   that  outper form  any  o ther   part  o f  the   sys tem.   In  particular,   the  jo in t  feature  learning  
model  achieves  the  bes t  results  on  this   task   by  tak ing  into   account   a ll  o f  the  info rma tion  
availab le -visual and  semant ic -and  comb ining  them  to   form mean ingful  represen tations of  the 
data .   Moreover,   bo th  cross -moda l  networks  ou tperform  SIFT,   which  shows   tha t  better 
features  for   computer   vis ion  can  be  learned   i f  seman tic   informa tion  is  avai la b le   during  
training.      

 
 
4 . 3   V i s u a l i z a t i o n s   o f   L e a r n e d   F e a t u r e s  
To  get  an   idea o f  how we ll  the  cross -modal   sys tem  is do ing  at   reconstructing  the  input  when 
given only  image   features,  we  looked   at  a  few  examp les  in  the  tes t  set  ( for  wh ich  the  sys tem  
was  only  given   access  to   the   SIFT  features  but  no t   to   its  correspond ing  tags)   and   computed  
the  mean  tag  vector   recons tructions   by  the  cross-modal  I I   sys tem.   By  the  way  the  system  is 
built,   it  is  no t  possib le   to   recover   the  actua l  tags,   but  by  looking  a t  the  tags  closest  in  the  

vector   space   model   to   the   reconstruct ion  vector   we   can   quali tatively  see  that  the  sys tem  is  
indeed   do ing  a  reasonab le   job   at  reconstructing  a  semantic  representation  of  the  original  
inpu t.    
 
 
 
 
 
 
 
 
 
 
 
Flickr Tags:  January,  sailboats,  surrey.  
Reconstructed:  Wa ter,  rocks,  sailboat,  
sea ,  agua ,  p ier,  fishing,  canoe,  creek,  
seascape .  
 
 
 
 
 

Flickr Tags:  Chi ld ,  baby,  infant,  newborn  
Reconstructed:  Adorab le,  swea ter,  hair,  
expression,  smiling,  finger s,  look,    
p layfu l,  fluffy,  cu te .  

 
 
 
 

Figure 3 : Visual izations o f Cross -Modal  II  reconstructions.  

C o n c l u s i o n s   a n d   F u r t h e r  Wo r k  

 
4  
 
We  have  shown   that  our   system  e ffect ive ly  rela tes  info rmatio n  from  d isparate  data   sources  
by  learn ing  meaningful  represen tations  that  cap ture  correlations  across  d i fferent  modalities .  
We  env ision  th is  work  may  have  practical  app lications ,   such  as   in  image  retrieval  o r   in  the 
organizat ion  o f  large   personal  pho to   albums ,   given   tha t  the   sys tem  can  be  used   to  
automatica lly  suggest  tags,   categorize  images  and   find   visually  and   semantically   similar  
images  (and  as a  ma tter  of  fact we already have ) .    
 
Moreover,   this  work  can  have   a  signi ficant  impac t  in  the  area  o f  computer   vis ion   research.  
As  a  nex t  step ,   we   would   like   to   evaluate  per formance  on  a  standard ized   dataset.   In  
particular,   we  may  use  the   same  approach  used   to   imp rove  SIFT  features  to   improve  the  
curren tly best  performing   image  features on  a dataset  such as  ImageNet,  and  use  those  learn t 
features  to  beat the current s tate -o f-the -art in ImageNet.  

A c k n o w l e d g m e n t s  

I   would   like   to   thank  the   Univers ity  of   Singapore  fo r   provid ing  the  NUS -WIDE   Flickr  
dataset.   I  would   further   like  to   thank Andrew Maas  and  Prof.  Andrew Ng  for   their   invaluab le  
guidance  on this pro ject,  as we ll as J iquan Ng iam  for   his  very help ful co llaboration.    

R e f e r e n c e s  
[1] K. E. A. van de Sande, T. Gevers & C. G. M. Snoek.(2010). Evaluating color descriptors for object and 
scenerecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , (in press). 
 
[2]A. L. Maas & A. Y. Ng. (2010). A Probabilistic Model for Semantic Word Vectors. NIPS 2010 Workshop on Deep 
Learning and Unsupervised Feature Learning. 
 
[3] T.S Chua, J. Tang, R. Hong, H. Li, Z. Luo, & Yan-Tao Zheng.(2009). NUS-WIDE: A Real-World Web Image 
Database from National University of Singapore. ACM International Conference on Image and Video Retrieval. 
 
[4]J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee & A. Y. Ng. (2010). Multimodal Deep Learning. NIPS 2010 Workshop 
on Deep Learning and Unsupervised Feature Learning. 
 
[5] G. Hinton & R. Salakhutdinov. (2006). Reducing the dimensionality of data with neural networks. Science, 
313(5786):504.  


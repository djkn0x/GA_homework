Global Analytic Solution
for Variational Bayesian Matrix Factorization

Shinichi Nakajima
Nikon Corporation
Tokyo, 140-8601, Japan
nakajima.s@nikon.co.jp

Masashi Sugiyama
Tokyo Institute of Technology
Tokyo 152-8552, Japan
sugi@cs.titech.ac.jp

Ryota Tomioka
The University of Tokyo
Tokyo 113-8685, Japan
tomioka@mist.i.u-tokyo.ac.jp

Abstract

Bayesian methods of matrix factorization (MF) have been actively explored re-
cently as promising alternatives to classical singular value decomposition. In this
paper, we show that, despite the fact that the optimization problem is non-convex,
the global optimal solution of variational Bayesian (VB) MF can be computed
analytically by solving a quartic equation. This is highly advantageous over a
popular VBMF algorithm based on iterated conditional modes since it can only
ﬁnd a local optimal solution after iterations. We further show that the global opti-
mal solution of empirical VBMF (hyperparameters are also learned from data) can
also be analytically computed. We illustrate the usefulness of our results through
experiments.

1 Introduction

The problem of ﬁnding a low-rank approximation of a target matrix through matrix factorization
(MF) attracted considerable attention recently since it can be used for various purposes such as
reduced rank regression [19], canonical correlation analysis [8], partial least-squares [27, 21],
multi-class classiﬁcation [1], and multi-task learning [7, 29].
Singular value decomposition (SVD) is a classical method for MF, which gives the optimal low-
rank approximation to the target matrix in terms of the squared error. Regularized variants of SVD
have been studied for the Frobenius-norm penalty (i.e., singular values are regularized by the ℓ2 -
penalty) [17] or the trace-norm penalty (i.e., singular values are regularized by the ℓ1 -penalty) [23].
Since the Frobenius-norm penalty does not automatically produce a low-rank solution, it should be
combined with an explicit low-rank constraint, which is non-convex. In contrast, the trace-norm
penalty tends to produce sparse solutions, so a low-rank solution can be obtained without explicit
rank constraints. This implies that the optimization problem of trace-norm MF is still convex, and
thus the global optimal solution can be obtained. Recently, optimization techniques for trace-norm
MF have been extensively studied [20, 6, 12, 25].
Bayesian approaches to MF have also been actively explored. A maximum a posteriori (MAP)
estimation, which computes the mode of the posterior distributions, was shown [23] to correspond to
the ℓ1 -MF when Gaussian priors are imposed on factorized matrices [22]. The variational Bayesian
(VB) method [3, 5], which approximates the posterior distributions by factorized distributions, has
also been applied to MF [13, 18]. The VB-based MF method (VBMF) was shown to perform well
in experiments, and its theoretical properties have been investigated [15].

1

Figure 1: Matrix factorization model. H ≤ L ≤ M . A = (a1 , . . . , aH ) and B = (b1 , . . . , bH ).

However, the optimization problem of VBMF is non-convex. In practice, the VBMF solution is
computed by the iterated conditional modes (ICM) [4, 5], where the mean and the covariance of the
posterior distributions are iteratively updated until convergence [13, 18]. One may obtain a local
optimal solution by the ICM algorithm, but many restarts would be necessary to ﬁnd a good local
optimum.
In this paper, we ﬁrst show that, although the optimization problem is non-convex, the global opti-
mal solution of VBMF can be computed analytically by solving a quartic equation. This is highly
advantageous over the standard ICM algorithm since the global optimum can be found without any
iterations and restarts. We next consider an empirical VB (EVB) scenario where the hyperparam-
eters (prior variances) are also learned from data. Again, the optimization problem of EVBMF is
non-convex, but we still show that the global optimal solution of EVBMF can be computed analyti-
cally. The usefulness of our results is demonstrated through experiments.
Recently, the global optimal solution of VBMF when the target matrix is square has been obtained
in [15]. Thus, our contribution to VBMF can be regarded as an extension of the previous result to
general rectangular matrices. On the other hand, for EVBMF, this is the ﬁrst paper that gives the
analytic global solution, to the best of our knowledge. The global analytic solution for EVBMF is
shown to be highly useful in experiments.

2 Bayesian Matrix Factorization

In this section, we formulate the MF problem and review a variational Bayesian MF algorithm.

2.1 Formulation
The goal of MF is to approximate an unknown target matrix U (∈ RL×M ) from its n observations
V n = {V (i) ∈ RL×M }n
i=1 .
We assume that L ≤ M . If L > M , we may simply re-deﬁne the transpose U
⊤ as U so that L ≤ M
holds. Thus this does not impose any restriction.
A key assumption of MF is that U is a low-rank matrix. Let H (≤ L) be the rank of U . Then
the matrix U can be decomposed into the product of A ∈ RM ×H and B ∈ RL×H as follows (see
Figure 1):
⊤
U = BA

.

Assume that the observed matrix V is subject to the following additive-noise model:
V = U + E ,
where E (∈ RL×M ) is a noise matrix. Each entry of E is assumed to independently follow the
ˆ
!
Gaussian distribution with mean zero and variance σ2 . Then, the likelihood p(V n |A, B ) is given by
nX
− 1
2σ2
i=1
where ∥ · ∥Fro denotes the Frobenius norm of a matrix.

p(V n |A, B ) ∝ exp

,

∥V (i) − BA

⊤∥2
Fro

2

U=A⊤LMBHLMH!

.

(2)

r(A,B |V n )

2.2 Variational Bayesian Matrix Factorization
ˆ
!
ˆ
− HX
− HX
We use the Gaussian priors on the parameters A = (a1 , . . . , aH ) and B = (b1 , . . . , bH ):
∥bh∥2
∥ah∥2
φ(U ) = φA (A)φB (B ), where φA (A) ∝ exp
and φB (B ) ∝ exp
2c2
2c2
ah
bh
h=1
h=1
are hyperparameters corresponding to the prior variance. Without loss of generality, we
ah and c2
c2
bh
assume that the product cah cbh is non-increasing with respect to h.
(cid:192)
¿
Let r(A, B |V n ) be a trial distribution for A and B , and let FVB be the variational Bayes (VB) free
energy with respect to r(A, B |V n ):
log r(A, B |V n )
FVB (r|V n ) =
p(V n , A, B )
where 〈·〉p denotes the expectation over p.
The VB approach minimizes the VB free energy FVB (r|V n ) with respect to the trial distribution
r(A, B |V n ), by restricting the search space of r(A, B |V n ) so that the minimization is computation-
HY
ally tractable. Typically, dissolution of probabilistic dependency between entangled parameters (A
and B in the case of MF) makes the calculation feasible:1
r(A, B |V n ) =
rah (ah |V n )rbh (bh |V n ).
The resulting distribution is called the VB posterior. The VB solution bU VB is given by the VB
(1)
h=1
bU VB = 〈BA
posterior mean:
⊤ 〉r(A,B |V n ) .
HY
By applying the variational method to the VB free energy, we see that the VB posterior can be
expressed as follows:
, Σah )NL (bh ; µbh
NM (ah ; µah
r(A, B |V n ) =
, Σbh ),
h=1
where Nd (·; µ, Σ ) denotes the d-dimensional Gaussian density with mean µ and covariance matrix
‡
·−1
·−1
‡
Σ . µah , µbh , Σah , and Σbh satisfy
= Σah Ξ⊤
−2
nβh
, Σah=
= Σbh Ξhµah
σ2 + c
IM , Σbh=
, µbh
h µbh
µah
IL ,
ah
·
‡
where Id denotes the d-dimensional identity matrix, and
nX
X
∥2 + tr(Σbh ),
∥2 + tr(Σah ), βh = ∥µbh
αh = ∥µah
V −
1
⊤
Ξh = n
, V =
µbh′ µ
ah′
σ2
n
h′ ̸=h
i=1
The iterated conditional modes (ICM) algorithm [4, 5] for VBMF (VB-ICM) iteratively updates
HX
µah , µbh , Σah , and Σbh by Eq.(2) from some initial values until convergence [13, 18], allowing one
bU VB−ICM =
to obtain a local optimal solution. Finally, an estimator of U is computed as
⊤
µ
.
ah
0@ 1
·1A ,
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)V (i) − HX
h=1
‡
nX
HX
When the noise variance σ2 is unknown, it may be estimated by the following re-estimation formula:
∥2
∥2∥µbh
αhβh − ∥µah
n
i=1
h=1
h=1
Fro
which corresponds to the derivative of the VB free energy with respect to σ2 set to zero (see Eq.(4)
in Section 3). This can be incorporated in the ICM algorithm by updating σ2 from some initial value
by the above formula in every iteration of the ICM algorithm.
1Although a weaker constraint, r(A, B |V n ) = rA (A|V n )rB (B |V n ), is sufﬁcient to derive a tractable itera-
tive algorithm [13], we assume the stronger one (1) used in [18], which makes our theoretical analysis tractable.

−2
nαh
σ2 + c
bh

1
σ2LM

⊤
ah

µbh

µ

σ2 =

µbh

+

,

V (i) .

3

2.3 Empirical Variational Bayesian Matrix Factorization

In the VB framework, hyperparameters (c2
in the current setup) can also be learned from
ah and c2
bh
data by minimizing the VB free energy, which is called the empirical VB (EVB) method [5].
By setting the derivatives of the VB free energy with respect to c2
to zero, the following
ah and c2
bh
optimality condition can be obtained (see also Eq.(4) in Section 3):
= αh /M and c2
= βh /L.
(3)
c2
ah
bh
The ICM algorithm for EVBMF (EVB-ICM) is to iteratively update c2
ah and c2
by Eq.(3), in addi-
bh
tion to µah , µbh , Σah , and Σbh by Eq.(2). Again, one may obtain a local optimal solution by this
algorithm.

∥2

,

(4)

|+ βh
2c2
bh

+ L
2

− 1
log c2
2
bh

log |Σbh
·

3 Analytic-form Expression of Global Optimal Solution of VBMF
ˆ
In this section, we derive an analytic-form expression of the VBMF global solution.
HX
The VB free energy can be explicitly expressed as follows.
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2
(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)V (i) − HX
|+ αh
log |Σah
− 1
FVB (r|V n ) = nLM
M
‡
log σ2 +
log c2
HX
nX
2c2
2
2
2
ah
ah
h=1
∥2 ∥µbh
αhβh − ∥µah
1
⊤
+ n
+
µbh
µ
2σ2
2σ2
ah
i=1
h=1
h=1
Fro
where | · | denotes the determinant of a matrix. We solve the following problem:
) ∈ R2
++ (∀h = 1, . . . , H ), σ2 ∈ R++ ,
(c2
Given
, c2
ah
bh
, Σah , Σbh ; h = 1, . . . , H })
min FVB ({µah
, µbh
∈ SL
∈ RM , µbh
∈ RL , Σah
∈ SM
++ (∀h = 1, . . . , H ),
s.t. µah
++ , Σbh
++ denotes the set of d × d symmetric positive-deﬁnite matrices. This is a non-convex
where Sd
optimization problem, but still we show that the global optimal solution can be analytically obtained.
Let γh (≥ 0) be the h-th largest singular value of V , and let ωah and ω bh be the associated right
LX
and left singular vectors:2
⊤
Let bγh be the second largest real solution of the following quartic equation with respect to t:
V =
.
γhω bh ω
ah
h=1
!
ˆ
fh (t) := t4 + ξ3 t3 + ξ2 t2 + ξ1 t + ξ0 = 0,
p
(L2 + M 2 )bη2
where the coefﬁcients are deﬁned by
(L − M )2γh
!2
ˆbη2
ξ2 = −
¶ (cid:181)
(cid:181)
ξ3γh +
h
ξ0 ,
,
bη2
LM
LM
1 − σ2L
−
σ4
h =
ξ0 =
vuuut (L + M )σ2
h
c2
nγ 2
n2 c2
vuutˆ
!2 − LM σ4
ah
bh
h
eγh =
(L + M )σ2
σ4
σ4
+
+
+
2n2 c2
2n
2n2 c2
2n
Then we can analytically express the VBMF solution bU VB as in the following theorem.
c2
c2
n2
ah
ah
bh
bh
2 In our analysis, we assume that V has no missing entry, and its singular value decomposition (SVD) is
easily obtained. Therefore, our results cannot be directly applied to missing entry prediction.

2σ4
¶
c2
n2 c2
ah
bh

1 − σ2M
nγ 2
h

,

ξ1 = ξ3

+

γ 2
h .

ξ3 =

Let

,

!

(5)

.

(6)

4

‰bγh
HX
if γh > eγh ,
bU VB =
Theorem 1 The global VB solution can be expressed as
, where bγVB
bγVB
⊤
h =
h ω bh ω
0
ah
otherwise.
h=1
respect to bγh as a necessary and sufﬁcient condition to be a stationary point (note that its quadratic
Sketch of proof: We ﬁrst show that minimizing (4) amounts to a reweighed SVD and any minimizer
is a stationary point. Then, by analyzing the stationary condition (2), we obtain an equation with
approximation gives bounds of the solution [15]). Its rigorous evaluation results in the quartic equa-
tion (5). Finally, we show that only the second largest solution of the quartic equation (5) lies within
The coefﬁcients of the quartic equation (5) are analytic, so bγh can also be obtained analytically3 ,
the bounds, which completes the proof.
e.g., by Ferrari’s method [9] (we omit the details due to lack of space). Therefore, the global VB
solution can be analytically computed. This is a strong advantage over the standard ICM algorithm
since many iterations and restarts would be necessary to ﬁnd a good solution by ICM.
Based on the above result, the complete VB posterior can also be obtained analytically as follows.
HY
HY
Corollary 2 The VB posteriors are given by
rA (A|V n ) =
NM (ah ; µah
rB (B |V n ) =
, Σah ),
, Σbh ),
where, for bγVB
qbγVB
qbγVB
h=1
h=1
bδ
bδh · ωah , µbh
being the solution given by Theorem 1,
ˆ
!
p
− ¡
¢
h
− σ2 (M − L))2 + 4M nσ2 bη2
nbη2
(nbη2
· ω bh ,
= ±
= ±
−1
µah
bδ
h
h
h
2nM (bγVB
− σ2 (M − L)
!
ˆ
p
− ¡
¢
+
h + σ2 (M − L))2 + 4Lnσ2 bη2
(nbη2
nbη2
h
h
h
−1
−2
h + n−1σ2 c
ah )
bδh + n−1σ2 c
2nL(bγVB
h
h + σ2 (M − L)
q
+
h
n2 (M − L)2 (γh − bγVB
n(M − L)(γh − bγVB
−2
)
bh
h
(
h )2 + 4σ4LM
h ) +
c2
c2
if γh > eγh ,
ah
bh
−2
2σ2M c
ah
η2
h
otherwise.

Σbh =
bδh =
bη2
h =

NM (bh ; µbh

Σah =

IM ,

IL ,

,

σ2
ncah cbh

When the noise variance σ2 is unknown, one may use the minimizer of the VB free energy with
respect to σ2 as its estimate. In practice, this single-parameter minimization may be carried out
numerically based on Eq.(4) and Corollary 2.

4 Analytic-form Expression of Global Optimal Solution of Empirical VBMF

In this section, we solve the following problem to obtain the EVBMF global solution:
Given σ2 ∈ R++ ,
min FVB ({µah
; h = 1, . . . , H })
, Σah , Σbh , c2
, c2
, µbh
bh
ah
) ∈ R2
∈ SL
∈ SM
++ (∀h = 1, . . . , H ),
∈ RL , Σah
∈ RM , µbh
++ , (c2
s.t. µah
, c2
++ , Σbh
ah
bh
where Rd
++ denotes the set of the d-dimensional vectors with positive elements. We show that, al-
“
'
“ → '
though this is again a non-convex optimization problem, the global optimal solution can be obtained
analytically. We can observe the invariance of the VB free energy (4) under the transform
−1
−2
−2
(µah
)
)
(shµah
h c2
h c2
, s2
, c2
, Σah , Σbh , c2
h Σbh , s2
, s
, s
, µbh
h µbh
hΣah , s
ah
bh
ah
bh
3 In practice, one may solve the quartic equation numerically, e.g., by the ‘roots’ function in MATLAB R⃝

.

5

(a) V = 1.5

(b) V = 2.1

(c) V = 2.7

+

(7)

˘c2
h =

1
2LM
√

− (L + M )σ2
n

− (L + M )σ2
n
√

Figure 2: Proﬁles of the VB free energy (4) when L = M = H = 1, n = 1, and σ2 = 1 for
observations V = 1.5, 2.1, and 2.7. (a) When V = 1.5 < 2 = γ
, the VB free energy is monotone
increasing and thus the global solution is given by ch → 0. (b) When V = 2.1 > 2 = γ
h
, a local
minimum exists at ch = ˘ch ≈ 1.37, but ∆h ≈ 0.12 > 0 so ch → 0 is still the global solution. (c)
h
, ∆h ≈ −0.74 ≤ 0 and thus the minimizer at ch = ˘ch ≈ 2.26 is the global
When V = 2.7 > 2 = γ
h
solution.
for any {sh ̸= 0; h = 1, . . . , H }. Accordingly, we ﬁx the ratios to cah /cbh = S > 0, and refer to
1A ,
0@γ 2
s(cid:181)
ch := cah cbh also as a hyperparameter.
¶2 − 4LM σ4
Let
γ 2
h
h
n2
√
M )σ/
L +
= (
γ
n.
h
Then, we have the following lemma:
Lemma 3 If γh ≥ γ
, the VB free energy function (4) can have two local minima, namely, ch → 0
and ch = ˘ch . Otherwise, ch → 0 is the only local minimum of the VB free energy.
Sketch of proof: Analyzing the region where ch is so small that the VB solution given ch is bγh = 0,
h
we ﬁnd a local minimum ch → 0. Combining the stationary conditions (2) and (3), we derive a
h whose larger solution is given by Eq.(7). Showing that the
quadratic equation with respect to c2
smaller solution corresponds to saddle points completes the proof.
Figure 2 shows the proﬁles of the VB free energy (4) when L = M = H = 1, n = 1, and σ2 = 1
for observations V = 1.5, 2.1, and 2.7. As illustrated, depending on the value of V , either ch → 0
·
‡
·
‡
¡−2γh ˘γVB
¢
or ch = ˘ch is the global solution.
Let
+ n
nγh
nγh
M σ2 ˘γVB
h + LM ˘c2
h + 1
Lσ2 ˘γVB
h + 1
h
σ2
where ˘γVB
is the VB solution for ch = ˘ch . We can show that the sign of ∆h corresponds to that of
the difference of the VB free energy at ch = ˘ch and ch → 0. Then, we have the following theorem
h
Theorem 4 The hyperparameter bch that globally minimizes the VB free energy function (4) is given
and corollary.
by bch = ˘ch if γh > γ
and ∆h ≤ 0. Otherwise bch → 0.
(
h
HX
bU EVB =
Corollary 5 The global EVB solution can be expressed as
, where bγ EVB
bγ EVB
and ∆h ≤ 0,
˘γVB
if γh > γ
⊤
:=
h
h ω bh ω
h
Since the optimal hyperparameter value bch can be expressed in a closed-form, the global EVB
0
ah
h
otherwise.
h=1
solution can also be computed analytically using the result given in Section 3. This is again a strong
advantage over the standard ICM algorithm since ICM would require many iterations and restarts to
ﬁnd a good solution.

∆h := M log

+ L log

,

(8)

6

012322.53Global solutionhc01233.253.5hcGlobal solution012344.55hcGlobal solution5 Experiments

5.1 Artiﬁcial Dataset

In this section, we experimentally evaluate the usefulness of our analytic-form solutions using arti-
ﬁcial and benchmark datasets. The MATLAB R⃝ code will be available at [14].
P
∗
∗
∗ = 10,
∗ =
∗⊤
H
h with L = 30, M = 100, and H
We randomly created a true matrix V
ha
h=1 b
where every element of {ah , bh } was drawn independently from the standard Gaussian distribution.
We set n = 1, and an observation matrix V was created by adding independent Gaussian noise with
variance σ2 = 1 to each element. We used the full-rank model, i.e., H = L = 30. The noise
variance σ2 was assumed to be unknown, and estimated from data (see Section 2.2 and Section 3).
We ﬁrst investigate the learning curve of the VB free energy over EVB-ICM iterations. We created
the initial values of the EVB-ICM algorithm as follows: µah and µbh were set to randomly created
orthonormal vectors, Σah and Σbh were set to identity matrices multiplied by scalars σ2
ah and σ2
,
bh
respectively. σ2
ah and σ2
as well as the noise variance σ2 were drawn from the χ2 -distribution with
bh
degree-of-freedom one. 10 learning curves of the VB free energy were plotted in Figures 3(a). The
value of the VB free energy of the global solution computed by our analytic-form solution was also
plotted in the graph by the dashed line. The graph shows that the EVB-ICM algorithm reduces the
VB free energy reasonably well over iterations. However, for this artiﬁcial dataset, the convergence
speed was quite slow once in 10 runs, which was actually trapped in a local minimum.
Next, we compare the computation time. Figure 3(b) shows the computation time of EVB-ICM over
iterations and our analytic form-solution. The computation time of EVB-ICM grows almost linearly
with respect to the number of iterations, and it took 86.6 [sec] for 100 iterations on average. On the
other hand, the computation of our analytic-form solution took only 0.055 [sec] on average, includ-
ing the single-parameter search for σ2 . Thus, our method provides the reduction of computation
time in 4 orders of magnitude, with better accuracy as a minimizer of the VB free energy.
sured by G = ∥ bU − V
Next, we investigate the generalization error of the global analytic solutions of VB and EVB, mea-
∗ ∥2
Fro /(LM ). Figure 3(c) shows the mean and error bars (min and max)
over 10 runs for VB with various hyperparameter values and EVB. A single hyperparameter value
was commonly used (i.e., c1 = · · · = cH ) in VB, while each hyperparameter ch was separately
optimized in EVB. The result shows that EVB gives slightly lower generalization errors than VB
with the best common hyperparameter. Thus, automatic hyperparameter selection of EVB works
quite well.
Figure 3(d) shows the hyperparameter values chosen in EVB sorted in the decreasing order. This
shows that, for all 10 runs, ch is positive for h ≤ H
∗ (= 10) and zero for h > H
∗ . This implies that
the effect of automatic relevance determination [16, 5] works excellently for this artiﬁcial dataset.

5.2 Benchmark Dataset

MF can be used for canonical correlation analysis (CCA) [8] and reduced rank regression (RRR)
[19] with appropriately pre-whitened data. Here, we solve these tasks by VBMF and evaluate the
performance using the concrete slump test dataset [28] available from the UCI repository [2].
The experimental results are depicted in Figure 4, which is in the same format as Figure 3. The
results showed that similar trends to the artiﬁcial dataset can still be observed for the CCA task with
the benchmark dataset (the RRR results are similar and thus omitted from the ﬁgure). Overall, the
proposed global analytic solution is shown to be a useful alternative to the popular ICM algorithm.

6 Discussion and Conclusion

Overcoming the non-convexity of VB methods has been one of the important challenges in the
Bayesian machine learning community, since it sometimes prevented us from applying the VB meth-
ods to highly complex real-world problems. In this paper, we focused on the MF problem with no
missing entry, and showed that this weakness could be overcome by computing the global optimal
solution analytically. We further derived the global optimal solution analytically for the EVBMF

7

(a) VB free energy

(b) Computation time
(c) Generalization error
Figure 3: Experimental results for artiﬁcial dataset.

(d) Hyperparameter value

(a) VB free energy
(d) Hyperparameter value
(c) Generalization error
(b) Computation time
Figure 4: Experimental results of CCA for the concrete slump test dataset.

0,

method, where hyperparameters are also optimized based on data samples. Since no hand-tuning
parameter remains in EVBMF, our analytic-form solution is practically useful and computationally
highly efﬁcient. Numerical experiments showed that the proposed approach is promising.
→ ∞, the priors get (almost) ﬂat and the quartic equation (5) is factorized as
“1− σ2L
”γh”“t − M
”γh”“t −“1− σ2M
”γh”“t +“1− σ2M
“1− σ2L
→∞ fh (t) = “t + M
”γh” = 0.
When cah cbh
→∞ eγh =
p
lim
nγ 2
nγ 2
nγ 2
nγ 2
L
L
cah
cbh
h
h
h
h
¶¶
(cid:181)
(cid:181)
Theorem 1 states that its second largest solution gives the VB estimator for γh > limcah cbh
→∞ bγVB
M σ2 /n. Thus we have
1 − M σ2
lim
h = max
nγ 2
cah cbh
h
This is the positive-part James-Stein (PJS) shrinkage estimator [10], operated on each singular com-
ponent separately, and this coincides with the upper-bound derived in [15] for arbitrary cah cbh > 0.
The counter-intuitive fact—a shrinkage is observed even in the limit of ﬂat priors—can be explained
by strong non-uniformity of the volume element of the Fisher metric, i.e., the Jeffreys prior [11], in
the parameter space. We call this effect model-induced regularization (MIR), because it is induced
not by priors but by structure of model likelihood functions. MIR was shown to generally appear in
Bayesian estimation when the model is non-identiﬁable (i.e., the mapping between parameters and
distribution functions is not one-to-one) and the parameters are integrated out at least partially [26].
Thus, it never appears in MAP estimation [15]. The probabilistic PCA can be seen as an example
of MF, where A and B correspond to latent variables and principal axes, respectively [24]. The
MIR effect is observed in its analytic solution when A is integrated out and B is estimated to be the
maximizer of the marginal likelihood.
Our results fully made use of the assumptions that the likelihood and priors are both spherical Gaus-
sian, the VB posterior is column-wise independent, and there exists no missing entry. They were
necessary to solve the free energy minimization problem as a reweighted SVD. An important fu-
ture work is to obtain the analytic global solution under milder assumptions. This will enable us to
handle more challenging problems such as missing entry prediction [23, 20, 6, 13, 18, 22, 12, 25].

γh .

Acknowledgments

The authors appreciate comments by anonymous reviewers, which helped improve our earlier
manuscript and suggested promising directions for future work. MS thanks the support from the
FIRST program. RT was partially supported by MEXT Kakenhi 22700138.

8

0501001.891.91.911.921.931.941.95FVB/(LM)  EVB-AnalyticEVB-ICMIteration050100020406080100120Time(sec)  EVB-AnalyticEVB-ICMIteration1001010.180.20.220.240.260.280.3√chG  EVB-AnalyticVB-Analytic010203000.20.40.60.811.21.41.6hhc^1.8EVB-Analytic050100150200250−63.56−63.55−63.54−63.53−63.52EVB-AnalyticEVB-ICMIteration05010015020025000.20.40.60.811.21.4  Time(sec)EVB-AnalyticEVB-ICMIteration10150100150200G  √ch1010−3−1EVB-AnalyticVB-Analytic012300.050.10.150.20.25hhc^EVB-AnalyticReferences
[1] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classiﬁcation. In
Proceedings of International Conference on Machine Learning, pages 17–24, 2007.
[2] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[3] H. Attias. Inferring parameters and structure of latent variable models by variational Bayes. In Proceed-
ings of the Fifteenth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-99),
pages 21–30, San Francisco, CA, 1999. Morgan Kaufmann.
[4] J. Besag. On the Statistical Analysis of Dirty Pictures. J. Royal Stat. Soc. B, 48:259–302, 1986.
[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, NY, USA, 2006.
[6] J.-F. Cai, E. J. Candes, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM
Journal on Optimization, 20(4):1956–1982, 2008.
[7] O. Chapelle and Z. Harchaoui. A Machine Learning Approach to Conjoint Analysis. In Advances in
neural information processing systems, volume 17, pages 257–264, 2005.
[8] D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-Taylor. Canonical correlation analysis: An overview
with application to learning methods. Neural Computation, 16(12):2639–2664, 2004.
[9] M. Hazewinkel, editor. Encyclopaedia of Mathematics. Springer, 2002.
[10] W. James and C. Stein. Estimation with quadratic loss. In Proceedings of the 4th Berkeley Symposium on
Mathematical Statistics and Probability, volume 1, pages 361–379. University of California Press, 1961.
[11] H. Jeffreys. An Invariant Form for the Prior Probability in Estimation Problems. In Proceedings of the
Royal Society of London. Series A, volume 186, pages 453–461, 1946.
[12] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In Proceedings of Interna-
tional Conference on Machine Learning, pages 457–464, 2009.
[13] Y. J. Lim and T. W. Teh. Variational Bayesian Approach to Movie Rating Prediction. In Proceedings of
KDD Cup and Workshop, 2007.
[14] S. Nakajima. Matlab Code for VBMF, http://sites.google.com/site/shinnkj23/, 2010.
[15] S. Nakajima and M. Sugiyama. Implicit regularization in variational Bayesian matrix factorization. In
Proceedings of 27th International Conference on Machine Learning (ICML2010), 2010.
[16] R. M. Neal. Bayesian Learning for Neural Networks. Springer, 1996.
[17] A. Paterek. Improving Regularized Singular Value Decomposition for Collaborative Filtering. In Pro-
ceedings of KDD Cup and Workshop, 2007.
[18] T. Raiko, A. Ilin, and J. Karhunen. Principal Component Analysis for Large Sale Problems with Lots of
Missing Values. In Proc. of ECML, volume 4701, pages 691–698, 2007.
[19] G. R. Reinsel and R. P. Velu. Multivariate reduced-rank Regression: Theory and Applications. Springer,
New York, 1998.
[20] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.
In Proceedings of the 22nd International Conference on Machine learning, pages 713–719, 2005.
[21] R. Rosipal and N. Kr ¨amer. Overview and recent advances in partial least squares. In Subspace, Latent
Structure and Feature Selection Techniques, volume 3940, pages 34–51. Springer, 2006.
[22] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1257–1264, 2008.
[23] N. Srebro, J. Rennie, and T. Jaakkola. Maximum Margin Matrix Factorization. In Advances in NIPS,
volume 17, 2005.
[24] M.E. Tipping and C.M. Bishop. Probabilistic Principal Component Analysis. Journal of the Royal Sta-
tistical Society: Series B, 61(3):611–622, 1999.
[25] R. Tomioka, T. Suzuki, M. Sugiyama, and H. Kashima. An efﬁcient and general augmented Lagrangian
algorithm for learning low-rank matrices. In Proceedings of International Conference on Machine Learn-
ing, 2010.
[26] S. Watanabe. Algebraic Geometry and Statistical Learning. Cambridge University Press, Cambridge,
UK, 2009.
[27] K. J. Worsley, J-B. Poline, K. J. Friston, and A. C. Evanss. Characterizing the Response of PET and fMRI
Data Using Multivariate Linear Models. NeuroImage, 6(4):305–319, 1997.
[28] I-Cheng Yeh. Modeling slump ﬂow of concrete using second-order regressions and artiﬁcial neural net-
works. Cement and Concrete Composites, 29(6):474–480, 2007.
[29] K. Yu, V. Tresp, and A. Schwaighofer. Learning Gaussian Processes from Multiple Tasks. In Proc. of
ICML, page 1019, 2005.

9


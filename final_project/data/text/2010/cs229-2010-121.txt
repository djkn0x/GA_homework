CS 229    Final Project Report                                                                              Tianxi Li & Yu Wu 

Twitter Hash Tag Prediction Algorithm 

Twitter  is  one  popular  web  application  nowadays.  Twitter  allows  users  to  use 
“Hash  tags”  to classify  their  tweets.  In  this  research project, we propose an algorithm 
to predict tags, by utilizing machine learning and network relatedness methods. 

1. Problem Definition 
Hash  tag  prediction  is  different  from  normal  texts  classification.  Here  we  don’t 
know  how  many  clusters  we  need  to  find.  In  addition,  the  tag  set  changes  so 
frequently  that  it  is  almost  impossible  to  effectively  carry  out  classification  or 
clustering,  since  a  new  tag  would  force  us  to  establish  a  new  class  and  a  new 
classification rule. Our intuition is: if we can measure the correlation between various 
tweets as the mathematical metric we can treat the collected tweets as points in a high 
dimensional space, and construct a network by the latent space model. 
2. Method 
2.1 Theory 
An  intuitive  way  to  solve  this  problem  is  to  use  Euclidean  distance  between 
points  as  the measurement  of  their  similarity. We  developed  our  theory  based  on  this 
distance. Since in a Euclidean Space, the distance is equivalent to the norm of a vector, 
we will focus our discussion on norms. 
u
u u
be  the  standard  bases  (with  unit  norm)  of  a  p-dimensional 
Let 
,
1

p

2

Euclidean  Space.  Then  for  any  vector v with  coordinates



x
1

x
2

x 
1pp

x



,  we 

have

p
v

i
1


x
i

u

i

. Then the Euclidean norm of vector

v

is given by 

2

v

v v
  

p
p
p



i
i
i j
1
1
,
1



  represents  the  inner  product  operation  defined  in  the  Euclidean  Space. 
where 

iu
i  j
u u
ju
Clearly, if we assume
, that is, 
and
are orthogonal, whenever 
, the 
0


i

u u                 (1) 

i
j

x x
i

x
i

u



x
i

u

i



j

i

j

 v
2
ix
Euclidean norm  equals  to
.  In our problem,  the bases  are  the words  in  the 
2
dictionary.  The  preliminary  assumption  for  Euclidean  distance  is  that  the  bases  are 
orthogonal  to  each  other,  that  is,  the  words  in  dictionary  are  uncorrelated,  which  is 
against common sense. Therefore, we need to perform some transformation to capture 
this correlation. 
In  formula  (1),  as 

  are  unit  vectors,  their  inner  product  is  actually  the 

and

iu

ju

CS 229    Final Project Report                                                                              Tianxi Li & Yu Wu 

2





x

p

v



x
1



cosine of the angle between them. Thus we can rewrite (1) in a matrix form as 
x
cos
cos

 



p
1
11
1

 


 



 

cos


 

p
1


1i
p
x
x
  and 
ii  ,
where cos
1

1p
p
1
2

    Now we  try  to  find  the angle between each pair of  terms  in  the dictionary and  then 
calculate  the  matrix M .  Notice  that M is  clearly  a  symmetric  and  non-negative 
definite matrix. If we decompose M in the way 

X
MX       (2) 
T


cos

pp






x

X





. 

p

x

x

M CC

T

                              (3), 

then (2) becomes 

  where X
XC
v
XX
XCC X
T
T


the  Euclidean  norm  of  the  transformed  coordinates.  Here  we  take  (3)  as  the  Eigen 

. So the norm can be seen as 

T

2

value  decomposition  of  M ,  so X could  be  the  coordinates  of  vector
v
  in  a  new 
coordinate  system  where  axes  are  orthogonal  to  each  other.  Please  note  that  we  can 
use  any other decomposition  in  the  form of  (3)  to get  the  same norm  in  computation, 
even  when C is  not  a  square  matrix.  With  this  property,  the  computation  becomes 
applicable. 
 
2.2 Estimate the Cosine Matrix 
 

First,  we  construct  the  preliminary  weighted matrix,  say  ,  by  using  the WordNet 
to  initialize  the  semantic  correlation  among  words  from  the  dictionary.  If  two  words 
t
t
are  similar  to  each  other,  and  they  both  appear  in  one  Tweet,  we  add  positive 
,i
j
weights for both words. This process can be expressed as 
p
 
i
j


x
ij

ˆ
x
i

x
i



 

j

where 

ij 

(0,1)

,  equals  to  one  when 

t

,i

t

j

  are  similar  words  and  zero  otherwise. 

(0,1)

, and if 

ij  , so is 
0

ji . 

for all 

Here we take the same positive number

ij 
Then we can construct the symmetric matrix  as 
1



p
1


  





X
In  the  second  step,  we  get m tweets,  say 1


  
1


p
1

               

ˆX



X        (4) 

get 

ˆ
X

1

ˆ
X .  Then  by  these  data,  we  use  cosine  similarity  in  variable  analysis  to 
m

X ,  and  transform  them  by  (4)  to 
m

CS 229    Final Project Report                                                                              Tianxi Li & Yu Wu 

construct matrix M . Set the text matrix as the  m p matrix   

T
1

ˆ
x
11

ˆ
x
1

p

. 





j





j

ˆ
X



i

ˆ
X

ˆ
X



T
m

)
T

ˆ
X

(

ˆ
x

2
ki

ˆ
x

2
kj

 

ˆ ˆ
x x
ki
kj

cos

ij

ˆ
ˆ
X X
T
i


                (5) 






 






ˆ
ˆ
x
x



m
p
m
1
We would estimate the cosine between the i’th and j’th terms as 
m

k
1

m
m


k
k
1
1


The distance estimate obtained  from  formula  (5)  is equivalent  to what proposed by 
L.  Jing,  L.  Zhou,  K.  Ng  and  J.  Huang  (2006),  but  with  a  better  mathematical 
explanation.  Note  that  since  our  data  is  represented  as  frequency,  all  the  elements  of 
the matrix    would be non-negative. So  the cosine estimated  in  this way can only be 
non-negative. Therefore, all angles between words are cute or right angles. In this way, 
all words  tend  to  be  similar  to  each  other  in  some  degree.  This may well  incorporate 
the  similarity  elements,  but  might  also  be  vulnerable  to  noise.  In  the  following,  we 
give a modified estimate which also  includes  the possibility of obtuse angle and  takes 
dissimilarity into consideration, which is also the sample correlation in statistics, 
m

k
1

m
m


k
k
1
1


Since  the  distance  from  (5)  was  named  as  Ontology  Based  Distance  (OBD)  in  the 
original  paper,  here  we  call  the  distance  in  (6)  centralized  Ontology  Based  Distance 
(COBD). We  will  discuss  the  pros  and  cons  of  the  two  methods  in  Section  3.  In  the 
following sub section, we will make another adjustment to the method. 
 
2.3 Normalization 
 

                            (6) 

cos
ij

ˆ
x


i

ˆ
x


i

(

ˆ
x
ki

(

ˆ
x
ki

(

ˆ
x
kj

ˆ
x
kj

ˆ
x


ˆ
x


j

2

)



)(



2

)



)

j





Note that the various scales of vectors may still cause us some problem. Consider 
X 
X 
X
a  special  case  where
, 
,
.  Obviously, 
(10, 0,
(1, 0, 0)
0)
(0, 0,1)
3
1



2

1X and 2X here  should  have  high  similarity  value  between  them.  But  in  this  case,  the 

distance between 1X and 3X is much smaller. 
To  make  our  method  more  reasonable,  before  we  compute  the  distance  between 
transformed  points, we  need  to  rescale  their  distances  to  the  original  point  as  1. And 
then we measure the Euclidean distance between normalized points. 
 

CS 229    Final Project Report                                                                              Tianxi Li & Yu Wu 

2.4 Prediction of Tags 
 

Finally, we try to predict tags based on the distance. An intuitive way is to simply 
select  the  tag  of  the  closest  tweet.  In  this  case,  it  may  be  unwise  to  simply  pick  the 
closest  tweet’s  tag,  since  that  relies  on  the  acurracy  of  distance  too  heavily.  To 
increase  the  accuracy, we  collect  a  few  closest  tweets,  and make  the prediction based 
on  tag  ratios.  Specifically,  we  will  collect  n  initial  closest  tweets  at  first.  Then  from 
this point, we will keep adding tweets while check a certain tag has become dominate. 
If  there  is  a  tag with  a  ratio  higher  than  50%, we will  choose  this  tag  as  our  primary 
predicted  tag.  Since  in  some  cases  tags  have  very  similar  meanings  (such  as 
#government vs. #election), sometimes we will also pick a secondary tag to predict. 
 
3. Results and Discussion 
    To  compare  the  performances  of  various  distances  discussed  above,  we  use  a  test 
dataset  consist  of  400  tweets  that  are  not  included  in  the  sample  set  we  used  to 
estimate matrix M . There  are  4  different  tags. We  first  process  the OBD  on  a  dataset 
with 665 tweets that are not in our test set, choose the best performance  (=0.2) and 

use  it  for  both OBD  and COBD.  The  table  below  shows  the  test  result  for  Euclidean 
Distance (EucD), OBD and COBD. 
Type II Error 
Test Error Rate 
 
5.1% 
EucD 
16.25% 
4.6% 
13.5% 
COBD 
OBD 
4.2% 
12.75% 
Table1: The test error rate and type II error for three distances. Type II error is the rate 
we assign a wrong tag to a particular tweet. 
     

Both OBD  and COBD outperform EucD,  and OBD  is  the best one.  If we  see  the 
data  for  different  tags  (not  provided  here  for  concise),  we  would  find  COBD  is  the 
most stable one, while EucD is far more unstable. But the disadvantage of COBD lies 
in  computation.  We  need  to  estimate  the  cosine  matrix M to  construct  the  distance, 
which  involves computation for matrices with tens of  thousands rows and columns. It 
won’t  be  a  big  problem  for  OBD  since  the  matrices  are  sparse.  But  in  COBD,  the 
matrix becomes non-sparse,  so we need many decompositions and  transformations of 
matrices to make the computation applicable. Given their close performances, OBD is 
more practical in application, while the COBD is a better model theoretically. 
The  left  picture  in  Figure  1  shows  the  COBD  from  other  tweets  to  a  random 
selected tweet. Different colors represent tweets with different tags. It can be seen that 
most of the tweets are very close to the 1.4142 distance boundary, and the majority of 
points  falling  in  the  circle  are  from  the  correct  tag  group.  This  indicates  that  tweets 
with  different  topics  are  projected  onto  orthogonal  axes.  The  right  plot  illustrates  the 
distance distribution. The lighter the color is, the shorter the corresponding distance is. 
Since the tweets are sorted by tags, we see that the distance within each group appears 

CS 229    Final Project Report                                                                              Tianxi Li & Yu Wu 

to be shorter, as shown by the light rectangles along the diagonal. 

 
           
Figure 1: the distance to one point and the distribution of sample distance matrix 

 

Figure 2: Prediction Visualization 
In Figure2, different colors represent what tag cluster the tweets belong to. A link   
will  be  added  between  a  pair  of  nodes  when  they  are  near  enough.  In  addition,  the 
deeper  color  the  line  is,  the  higher  the  similarity  value  is.  As  we  can  see,  the  lines 
appear  to  be  very  dense  among  each  tag  cluster,  and  sparse  between  tweets  with 
different tags. It indicates that tweets with the same tag cluster are near on average. 
 
Due  to  the  vagueness  of  many  tweets,  the  correct  rate  of  more  than  86%  is 
actually very high. Apart from the accuracy, our method has other advantages:   
(1) The whole system is easy to store (we only need to store the C matrix in (3)). 
(2)  It  is  easy  to  update  when  dictionary  changes  (only  needs  to  compute  an  extra 
column and add it back to original matrix) 
(3)  It won’t lose power when the topics trend changes with time, and it can work with 
personal elements and settings, which makes it more flexible (since we can set the 
algorithm  to  only  consider  the  distance  of  the  objective  tweet  to  certain  subset  of 
other tweets, so elements like location, time, etc can be incorporated.)   
(4)  In addition, the distance provides us the possibility to transform the twitter system 
and  even other  text  systems  into  social networks by  latent  space  approach. So we 
can  use  traditional  social  network  methods  to  discuss  the  properties  of  such 
systems. 


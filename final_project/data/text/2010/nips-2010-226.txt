An Approximate Inference Approach to Temporal
Optimization in Optimal Control

Konrad C. Rawlik
School of Informatics
University of Edinburgh
Edinburgh, UK

Marc Toussaint
TU Berlin
Berlin, Germany

Sethu Vijayakumar
School of Informatics
University of Edinburgh
Edinburgh, UK

Abstract

Algorithms based on iterative local approximations presen t a practical approach
to optimal control in robotic systems. However, they genera lly require the tem-
poral parameters (for e.g. the movement duration or the time point of reaching
an intermediate goal) to be speciﬁed a priori. Here, we present a methodology
that is capable of jointly optimizing the temporal parameters in addition to the
control command pro ﬁles. The presented approach is based on a Bayesian canon-
ical time formulation of the optimal control problem, with the temporal mapping
from canonical to real time parametrised by an additional control variable. An ap-
proximate EM algorithm is derived that efﬁciently optimize s both the movement
duration and control commands offering, for the ﬁrst time, a practical approach to
tackling generic via point problems in a systematic way under the optimal control
framework. The proposed approach, which is applicable to plants with non-linear
dynamics as well as arbitrary state dependent and quadratic control costs, is eval-
uated on realistic simulations of a redundant robotic plant .

1 Introduction

Control of sensorimotor systems, artiﬁcial or biological,
is inherently both a spatial and temporal
process. Not only do we have to specify where the plant has to move to but also when it reaches
that position. In some control schemes, the temporal component is implicit; for example, with a
PID controller, movement duration results from the applica tion of the feedback loop, while in other
cases it is explicit, like for example in ﬁnite or receding ho rizon optimal control approaches where
the time horizon is set explicitly as a parameter of the problem [8, 13].

Although control based on an optimality criterion is certainly attractive, practical approaches for
stochastic systems are currently limited to the ﬁnite horiz on [9, 16] or ﬁrst exit time formulation [14,
17]. The former does not optimize temporal aspects of the movement, i.e., duration or the time when
costs for speciﬁc sub goals of the problem are incurred, assu ming them as given a priori. However,
how should one choose these temporal parameters? This quest ion is non trivial and important even
while considering a simple reaching problem. The solution generally employed in practice is to use
a apriori ﬁxed duration, chosen experimentally. This can re sult in not reaching the goal, having to
use unrealistic range of control commands or excessive (was teful) durations for short distance tasks.
The alternative ﬁrst exit time formulation, on the other han d, either assumes speciﬁc exit states in the
cost function and computes the shortest duration trajectory which fulﬁls the task or assumes a time
stationary task cost function and computes the control which minimizes the joint cost of movement
duration and task cost [17, 1, 14]. This formalism is thus only directly applicable to tasks which do
not require sequential achievement of multiple goals. Although this limitation could be overcome
by chaining together individual time optimal single goal controllers, such a sequential approach has
several drawbacks. First, if we are interested in placing a c ost on overall movement duration, we are
restricted to linear costs if we wish to remain time optimal. A second more important ﬂaw is that

1

future goals should in ﬂuence our control even before we have achieved the previous goal, a problem
which we highlight during our comparative simulation studies.

A wide variety of successful approaches to address stochast ic optimal control problems have been
described in the literature [6, 2, 7]. The approach we presen t here builds on a class of approximate
stochastic optimal control methods which have been successfully used in the domain of robotic ma-
nipulators and in particular, the iLQG [9] algorithm used by [10], and the Approximate Inference
Control (AICO) algorithm [16]. These approaches, as alluded to earl ier, are ﬁnite horizon formu-
lations and consequently require the temporal structure of the problem to be ﬁxed a priori. This
requirement is a direct consequence of a ﬁxed length discret
ization of the continuous problem and
the structure of the temporally non-stationary cost function used, which binds incurrence of goal
related costs to speciﬁc (discretised) time points. The fun damental idea proposed here is to refor-
mulate the problem in canonical time and alternately optimize the temporal and spatial trajectories.
We implement this general approach in the context of the approximate inference formulation of
AICO, leading to an Expectation Maximisation (EM) algorithm where the E-Step reduces to the
standard inference control problem. It is worth noting that due to the similarities between AICO,
iLQG and other algorithms, e.g., DDP [6], the same principle and approach should be applicable
more generally. The proposed approach provides an extensio n to the time scaling approach [12, 3]
by considering the scaling for a complete controlled system , rather then a single trajectory. Addi-
tionally, it also extends previous applications of Expectation Maximisation algorithms for system
identiﬁcation of dynamical systems, e.g. [4, 5], which did n ot consider the temporal aspects.

2 Preliminaries

dx = (F (x) + Bu)dt + dξ

Let us consider a process with state x ∈ RDx and controls u ∈ RDu which is of the form
(cid:10)dξdξ⊤(cid:11) = Q
with non-linear state dependent dynamics F , control matrix B and Brownian motion ξ , and de ﬁne
a cost of the form
L(x(·), u(·)) = Z T
(cid:2)C (x(t), t) + u(t)⊤Hu(t)(cid:3) dt ,
0
with arbitrary state dependent cost C and quadratic control cost. Note in particular that T , the
trajectory length, is assumed to be known. The closed loop stochastic optimal control problem is to
ﬁnd the policy π : x(t) → u(t) given by

(1)

(2)

π∗ = argmin
π

x,u|π ,x(0) {L(x(·), u(·))} .
E

(3)

In practice, the continuous time problem is discretized into a ﬁxed number of K steps of length ∆t ,
leading to the discreet problem with dynamics

P(xk+1 |xk , uk ) = N (xk+1 |xk + (F (x) + Bu)∆t , Q∆t ) ,

(4)

L(x1:K , u1:K ) = CK (xK ) +

where we use N (·|a, A) to denote a Gaussian distribution with mean a and covariance A, and cost
K−1Xk=0 (cid:2)∆tCk (xk ) + u⊤
k (H∆t )uk (cid:3) .
Note that here we used the Euler Forward Method as the discret ization scheme, which will prove
advantageous if a linear cost on the movement duration is chosen, leading to closed form solution
for certain optimization problems. However, in other cases , alternative discretisation methods could
be used and indeed, be preferable.

(5)

2.1 Approximate Inference Control

Recently, it has been suggested to consider a Bayesian inference approach [16] to (discreet) optimal
control problems formalised in Section 2. With the probabil istic trajectory model in (4) as a prior,
an auxiliary (binary) dynamic random task variable rk , with the associated likelihood
P(rk = 1|xk , uk ) = exp (cid:8)−(∆t Ck (xk ) + u⊤
k (H∆t )uk )(cid:9) ,
2

(6)

u0

u1

u2

x0

x1

x2

. . .

xK

r0

r1

r2

rK

θ0

u0

x0

r0

θ1

u1

x1

r1

θ2

u2

x2

. . .

xK

r2

rK

(a)

(b)

Figure 1: The graphical models for (a) standard inference control and (b) the AICO-T model
with canonical time. Circle and square nodes indicate continous and discreet variables respectively.
Shaded nodes are observed.

is introduced, i.e., we interpret the cost as a negative log l ikelihood of task fulﬁlment. Inference
control consists of computing the posterior conditioned on the observation r0:K = 1 within the
resulting model (illustrated as a graphical model in Fig. 1 (a)), and from it obtaining the maximum
a posteriori (MAP) controls. For cases, where the process and cost are lin ear and quadratic in u
respectively, the controls can be marginalised in closed fo rm and one is left with the problem of
computing the posterior
P (x0:K |r0:K = 1) = Yk
with W := Q + BH−1B⊤.
As this posterior is in general not tractable, the AICO [16] a lgorithm computes a Gaussian approxi-
mation to the true posterior using an approximate message pa ssing approach similar in nature to EP
(details are given in supplementary material). The algorithm has been shown to have competitive
performance when compared to iLQG [16].

N (xk+1 |xk + F (xk )∆t , W∆t ) exp(−∆tCk (xk )) ,

(7)

3 Temporal Optimization for Optimal Control

C (x, t) = J (x) +

Often the state dependent cost term C (x, t) in (2) can be split into a set of costs which are incurred
only at speciﬁc times: also referred to as goals, and others w hich are independent of time, that is
NXn=1
Classically, ˆtn refer to real time and are ﬁxed. For instance, in a reaching movement, generall y a cost
that is a function of the distance to the target is incurred on ly at the ﬁnal time T while collision costs
are independent of time and incurred throughout the movemen t. In order to allow the time point at
which the goals are achieved to be in ﬂuenced by the optimizat
ion, we will re-frame the goal driven
part of the problem in a canonical time and in addition to optimizing the controls, also optimize the
mapping from canonical to real time.

δt=ˆtn Vn (x) .

(8)

) the canonical time variable τ with

ds ,

Speciﬁcally, we introduce into the problem de ﬁned by (1) & (2
the associated mapping
τ = β (t) = Z t
1
θ(s)
0
with θ as an additional control. We also reformulate the cost in terms of the time τ as1
Vn (x(β−1 (ˆτn ))) + Z ˆτN
NXn=1
T (θ(s))ds
0
+ Z β−1 ( ˆτN )
(cid:2)J (x(t)) + u(t)⊤Hu(t)(cid:3) dt ,
0
1Note that as β is strictly monotonic and increasing, the inverse function β−1 exists

L(x(·), u(·), θ(·)) =

θ(·) > 0 ,

(9)

(10)

3

with T an additional cost term over the controls θ and the ˆτ1:N ∈ R assumed as given. Based on the
last assumption, we are still required to choose the time point at which individual goals are achieved
and how long the movement lasts; however, this is now done in terms of the canonical time and since
by controlling θ, we can change the real time point at which the cost is incurred, the exact choices
for ˆτ1:N are relatively unimportant. The real time behaviour is main ly speciﬁed by the additional
cost term T over the new controls θ which we have introduced. Note that in the special case where
T is linear, we have R ˆτN
0 T (θs )ds = T (T ), i.e., T is equivalent to a cost on the total movement
duration. Although here we will stick to the linear case, the proposed approach is also applicable
to non-linear duration costs. We brie ﬂy note the similarity of the formulation to the canonical time
formulation of [11] used in an imitation learning setting.

We now discretize the augmented system in canonical time with a ﬁxed number of steps K . Making
the arbitrary choice of a step length of 1 in τ induces, by (9), a sequence of steps in t with length2
∆k = θk . Using this time step sequence and (4) we can now obtain a discreet process in terms of
the canonical time with an explicit dependence on θ0:K−1 . Discretization of the cost in (10) gives
K−1Xk=0 (cid:2)T (θk ) + J (xk )θk + u⊤
NXn=1
kHθk uk (cid:3) ,
for some given ˆk1:N . We now have a new formulation of the optimal control problem that no longer
of the form of equations (4) & (5), e.g. (11) is no longer quadratic in the controls as θ is a control.

L(x1:K , u1:K , θ0:K−1) =

Vn (xˆkn

) +

(11)

Proceeding as for standard inference control and treating t he cost (11) as a neg-log likelihood of
an auxiliary binary dynamic random variable, we obtain the inference problem illustrated by the
Bayesian network in Figure 1(b). With controls u marginalised, our aim is now to ﬁnd the posterior
P(x0:K , θ0:K−1 |r0:K = 1). Unfortunately, this problem is intractable even for the simplest case, e.g.
LQG with linear duration cost. However, observing that for g iven θk ’s, the problem reduces to the
standard case of Section 2.1 suggest restricting ourselves to ﬁnding the MAP estimate for θ0:K−1 and
the associated posterior P(x0:K |θMAP
0:K−1 , r0:K = 1) using an EM algorithm. The solution is obtained
by iterating the E- & M-Steps (see below) until the θ’s have converged; we call this algorithm AICO-
T to re ﬂect the temporal aspect of the optimization.

3.1 E-Step

In general, the aim of the E-Step is to calculate the posterior over the unobserved variables, i.e. the
trajectories, given the current parameter values, i.e. the θi ’s.

q i (x0:K ) = P(x0:K |r0:K = 1, θi
(12)
0:K−1) .
However, as will be shown below we actually only require the expectations (cid:10)xk x⊤
k (cid:11) and (cid:10)xk x⊤
k+1 (cid:11)
during the M-Step. As these are in general not tractable, we compute a Gaussian approximation to
the posterior, following an approximate message passing approach with linear and quadratic approx-
imations to the dynamics and cost respectively [16] (for details, refer to supplementary material).

3.2 M-Step

In the M-Step, we solve

θi+1
0:K−1 = argmax
θ0:K−1

Q(θ0:K−1 |θi
0:K−1 ) ,

(13)

with
Q(θ0:K−1 |θi
0:K−1 ) = hlog P(x0:K , r0:K = 1|θ0:K−1)i
K−1Xk=0
K−1Xk=1
(14)
where h·i denotes the expectation with respect to the distribution calculated in the E-Step, i.e., the
posterior q i (x0:K ) over trajectories given the previous parameter values. The required expectations,

[T (θk ) + θk hJ (xk )i] + constant ,

hlog P(xk+1 |xk , θk )i −

=

2 under the assumption of constant θ(·) during each step

4

hJ (xk )i and

2 D(xk+1 − eF (xk ))⊤ fW−1
k (xk+1 − eF (xk ))E ,
1
Dx
log | fWk | −
(15)
hlog P(xk+1 |xk , θk )i = −
2
with eF (xk ) = xk + F (xk )θk and fWk = θkW, are in general not tractable. Therefore, we take
approximations
1
x⊤
k Jk xk − j⊤
and J (xk ) ≈
(16)
F (xk ) ≈ ak + Ak xk
k xk ,
2
choosing the mean of q i (xk ) as the point of approximation, consistent with the equivalent approxi-
mations made in the E-Step. Under these approximations, it can be shown that, up to additive terms
independent of θ,

Q(θ0:K−1 |θi
0:K−1 ) = −

(cid:20) Dx
K−1Xk=0
k+1 (cid:11))
k (cid:10)xk+1x′
1
log | fWk | + T (θk ) +
Tr( fW−1
2
2
1
k fW−1
− Tr( eA′
Tr( eAk fW−1
k fW−1
k eA′
k eAk hxk i
k hxk+1x′
k hxk x′
k i) + ˜a⊤
k i) +
2
k ˜ak + θk (cid:20) 1
k (cid:11) − jk hxk i(cid:21) (cid:21) ,
Tr(Jk (cid:10)xk x⊤
1
k fW−1
˜a⊤
+
2
2
k = θk ak , eAk = I + θkAk and taking partial derivatives leads to
with ˜a⊤
k Tr (cid:0)W−1 ((cid:10)xk+1x⊤
k+1 (cid:11) − 2 (cid:10)xk+1x⊤
k (cid:11) + (cid:10)xk x⊤
k (cid:11))(cid:1) −
∂Q
1
θ−2
=
2
∂ θk
dθ (cid:12)(cid:12)(cid:12)(cid:12)θk
2 (cid:20) Tr(AW−1A⊤ (cid:10)xk x⊤
k (cid:11)) + 2
dT
1
+ a⊤
kW−1ak + 2a⊤
kW−1Ak hxk i
−
k (cid:11) ) − 2jk hxk i (cid:21) .
+ Tr(Jk (cid:10)xk x⊤
In the general case, we can now use gradient ascent to improve the θ’s. However, in the speciﬁc
is a quadratic in θ−1
case where T is a linear function of θ, we note that 0 = ∂Q
and the unique
k
∂ θk
extremum under the constraint θk > 0 can be found analytically.

(17)

D2
x
2

θ−1
k

3.3 Practical Remarks

The performance of the algorithm can be greatly enhanced by u sing the result of the previous E-
Step as initialisation for the next one. As this is likely to be near the optimum with the new temporal
trajectory, AICO converges within only a few iterations. Additionally, in practise it is often sufﬁcient
to restrict the θk ’s between goals to be constant, which is easily achieved as Q is a sum over the θ’s.
The proposed algorithms leads to a variation of discretizat ion step length which can be a problem.
For one, the approximation error increases with the step len gth which may lead to wrong results. On
the other hand, the algorithm may lead to control frequencie s which are not achievable in practice.
In general, a ﬁxed control signal frequency may be prescribe d by the hardware system. In practice,
θ’s can be kept in a prescribed range by adjusting the number of discretization steps K after an
M-Step.
Finally, although we have chosen to express the time cost in terms of a function of the θ’s, often it
may be desirable to consider a cost directly over the duratio n T . Noting that T = P θk , all that is
dθ with ∂T (P θ)
required is to replace dT
in (17).
∂ θk

4 Experiments

The proposed algorithm was evaluated in simulation. As a basic plant, we used a kinematic simula-
tion of a 2 degrees of freedom (DOF) planar arm, consisting of two links of equal length. The state
of the plant is given by x = (q, ˙q), with q ∈ R2 the joint angles and ˙q ∈ R2 associated angular

5

AICO-T(α = α0 )
AICO-T(α = 2α)
AICO-T(α = 0.5α)

0.6

n
o
i
t
a
r
0.4
u
D
t
n
e
m
e
v
o
0.2
M

300

t
s
o
C
g
n
i
h
c
a
e
R

200

100

AICO-T(α = α0 )
AICO-T(α = 2α0 )
AICO-T(α = 0.5α0 )

600

t
s
o
C
g
n
i
h
c
a
e
R

400

200

AICO (T = 0.07)
AICO (T = 0.24)
AICO (T = 0.41)
AICO-T(α = α0 )

0

0.2

0.8
0.6
0.4
Task Space Movement Distance
(a)

0

0.2

0.4
0.8
0.6
Task Space Movement Distance
(b)

0

0.2

0.4
0.8
0.6
Task Space Movement Distance
(c)

Figure 2: Temporal scaling behaviour using AICO-T. (a & b) Effect of changing time-cost weight
α, (effectively the ratio between reaching cost and duration cost) on (a) duration and (b) reaching
cost (control + state cost). (c) Comparison of reaching costs (control + error cost) for AICO -T and
a ﬁxed duration approach, i.e. AICO.
velocities. The controls u ∈ R2 are the joint space accelerations. We also added some iid noise with
small diagonal covariance.
For all experiments, we used a quadratic control cost and the state dependent cost term:
V (xk ) = Xi
i )⊤Λi (φi (xk ) − y∗
(φi (xk ) − y∗
i ) ,
for some given ˆki and employed a diagonal weight matrix Λi while y∗
i represented the desired state
in task space. For point targets, the task space mapping is φ(x) = (x, y , ˙x, ˙y )⊤, i.e., the map from
x to the vector of end point positions and velocities in task space coordinates. The time cost was
linear, that is, T (θ) = αθ.

δk=ˆki

(18)

4.1 Variable Distance Reaching Task

In order to evaluate the behaviour of AICO-T we applied it to a reaching task with varying start-
target distance. Speciﬁcally, for a ﬁxed start point we cons
idered a series of targets lying equally
spaced along a line in task space. It should be noted that although the targets are equally spaced
in task space and results are shown with respect to movement d istance in task space, the distances
in joint space scale non linearly. The state cost (18) contained a single term incurred at the ﬁnal
discrete step with Λ = 106 · I and the control cost were given by H = 104 · I. Fig. 2(a & b) shows
the movement duration (= P θk ) and standard reaching cost3 for different temporal-cost parameters
α (we used α0 = 2 · 107), demonstrating that AICO-T successfully trades-off the movement duration
and standard reaching cost for varying movement distances. In Fig. 2(c), we compare the reaching
costs of AICO-T with those obtained with a ﬁxed duration appr oach, in this case AICO. Note that
although with a ﬁxed, long duration (e.g., AICO with duratio n T=0.41) the control and error costs
are reduced for short movements, these movements necessari ly have up to 4× longer durations than
those obtained with AICO-T. For example for a movement distance of 0.2 application of AICO-T
results in a optimised movement duration of 0.07 (cf. Fig. 2(a)), making the ﬁxed time approach
impractical when temporal costs are considered. Choosing a short duration on the other hand (AICO
(T=0.07)) leads to signiﬁcantly worse costs for long moveme nts. We further emphasis that the
ﬁxed durations used in this comparison were chosen post hoc b y exploiting the durations suggested
by AICO-T in absence of this, there would have been no practical way of choosing them apart
from experimentation. Furthermore, we would like to highli ght that, although the results suggests a
simple scaling of duration with movement distance, in cluttered environments and plants with more
complex forward kinematics, an efﬁcient decision on the mov ement duration cannot be based only
on task space distance.

4.2 Via Point Reaching Tasks

We also evaluated the proposed algorithm in a more complex via point task. The task requires the
end-effector to reach to a target, having passed at some poin t through a given second target, the

3 n.b. the standard reaching cost is the sum of control costs and cost on the endpoint error, without taking
duration into account, i.e., (11) without the T (θ) term.

6

3.4

3.2

3

2.8

2.6

−0.2

]
d
−0.4
a
r
[
1
t
n
i
o
J
−0.6
e
l
g
n
A

−0.8

−1.5

−2

]
d
a
r
[
2
t
n
i
o
J

e
l
g
n
A

−2.5

2.5

2

n
o
i
t
a
r
u
1.5
D
 
t
n
e
m
e
v
o
0.5
M

1

−0.4−0.2 0

0.2

(a)

0

1
Time

2

0

(b)

1
Time

2

0

Near

Far

25

20
t
s
o
C
15
 
g
n
i
h
10
c
a
e
R

5

0

(c)

Near

Far

Figure 3: Comparision of AICO-T (solid) to the common modelling approach, using AICO,
(a) End point task space trajectories for two dif-
(dashed) with ﬁxed times on a via point task.
ferent via points (circles) obtained for a ﬁxed start point ( triangle). (b) The corresponding joint
space trajectories. (c) Movement durations and reaching costs (control + error cost s) from 10 ran-
dom start points. The proportion of the movement duration sp end before the via point is shown in
light gray (mean in the AICO-T case).

via point. This task is of interest as it can be seen as an abstraction of a diverse range of complex
sequential tasks that requires one to achieve a series of sub -tasks in order to reach a ﬁnal goal. This
task has also seen some interest in the literature on modeling of human movement using the optimal
control framework, e.g., [15]. Here the common approach is to choose the time point at which
one passes the via point such as to divide the movement duration in the same ratio as the distances
between the start point, via point and end target. This requires on the one hand prior knowledge of
these movement distances and on the other, makes the implicit assumption that the two movements
are in some sense independent.

In a ﬁrst experiment, we demonstrate the ability of our appro ach to solve such sequential problems,
adjusting movement durations between sub goals in a princip led manner, and show that it improves
upon the standard modelling approach. Speciﬁcally, we appl y AICO-T to the two via point problems
illustrated in Fig. 3(a) with randomised start states4 . For comparison, we follow the standard mod-
eling approach and apply AICO to compute the controller. We again choose the movement duration
for the standard case post hoc to coincide with the mean movement duration obtained with AICO-T
for each of the individual via point tasks. Each task is expressed using a cost function consisting of
two point target cost terms. Speciﬁcally, (18) takes the for m

V (xk ) = δk= K
2

e )⊤Λe (φ(xk ) − y∗
v ) + δk=K (φ(xk ) − y∗
v )⊤Λv (φ(xk ) − y∗
(φ(xk ) − y∗
e ) ,

(19)

with K the number of discrete steps and diagonal matrices Λv = diag(λpos , λpos , 0, 0), Λe =
v = (·, ·, 0, 0)⊤, y∗
diag(λpos , λpos , λvel , λvel ), where λpos = 105 & λvel = 107 and vectors y∗
e =
(·, ·, 0, 0)⊤ desired states for individual via point and target, respect ively. Note that the cost function
does not penalise velocity at the via point but encourages the stopping at the target. While admittedly
2 ) is likely to be a sub-
the choice of incurring the via point cost at the middle of the movement ( K
optimal choice for the standard approach, one has to conside r that in more complex task spaces, the
relative ratio of movement distances may not be easily accessible and one may have to resort to the
most intuitive choice for the uninformed case as we have done here. Note that although for AICO-T
this cost is incurred at the same discrete step, we allow θ before and after the via point to differ, but
constrain them to be constant throughout each part of the movement, hence, allowing the cost to be
incurred at an arbitrary point in real time. We sampled the in itial position of each joint independently
from a Gaussian distribution with a variance of 3◦ . In Fig. 3(a&b), we show maximum a posteriori
(MAP) trajectories in task space and joint space for control lers computed for the mean initial state.
Interestingly, although the end point trajectory for the near via point produced by AICO-T may
look sub-optimal than that produced by the standard AICO algorithm, closer examination of the
joint space trajectories reveal that our approach results i n more efﬁcient actuation trajectories. In
Fig. 3(c), we illustrate the resulting average movement durations and costs of the mean trajectories.
As can be seen, AICO-T results in the expected passing times for the two via points, i.e. early
vs. late in the movement for the near and far via point, respec tively. This directly leads to a lower
incurred cost compared to un-optimised movement durations .

4For the sake of clarity, Fig. 3(a&b) show MAP trajectories of controllers computed for the mean start state.

7

]
d
a
r
[
1
t
n
i
o
J

e
l
g
n
A

−0.2

−0.4

−0.6

−0.8

−1

−1.2

3.4

3.2

3

2.8

2.6

0.4

0.6

0

0.2
(a)

−1.5

]
d
a
r
[
2
−2
t
n
i
o
J

e
l
g
n
A

−2.5

0

1
Time

2

0

1
Time

2

n
o
i
t
a
r
u
D
 
t
n
e
m
e
v
o
M

2.5

2

1.5

1

0.5

0

Joint Seq.

t
s
o
C
 
g
n
i
h
c
a
e
R

60

50

40

30

20

10

0

Joint Seq.

(b)

(c)

Figure 4:
Joint (solid) vs. sequential (dashed) optimisation using AICO-T for a sequential (via
point) task. (a) Task space trajectories for a ﬁxed start point (triangle). V iapoint and target are
indicated by the circle and square, respectively. (b) The corresponding joint space trajectories. (c)
The movement durations and reaching costs (control + error c ost) for 10 random start points. The
mean proportion of the movement duration spend before the via point is shown in light gray.

In order to highlight the shortcomings of sequential time op timal control, next we compare plan-
ning a complete movement over sequential goals to planning a sequence of individual movements.
Speciﬁcally, using AICO-T, we compare planning the whole vi a point movement (joint planner) to
planning a movement from the start to the via point followed by a second movement from the end
point of the ﬁrst movement (n.b. not from the via point) to the end target (sequential planner). The
joint planner used the same cost function as the previous experiment. For the sequential planner,
each of the two sub-trajectories had half the number of discrete time steps of the joint planner and
the cost functions were given by appropriately splitting (19), i.e.,

V 1 (xk ) = δk= K
2

v )⊤Λv (φ(xk )−y∗
(φ(xk )−y∗
v )

and V 2 (xk ) = δk= K
2

e )⊤Λe (φ(xk )−y∗
(φ(xk )−y∗
e ) ,

with Λv , Λe , y∗
e as for (19). The start states were sampled according to the di stribution used in
v , y∗
the last experiment and in Fig. 4(a&b), we plot the MAP trajectories for the mean start state, in task
as well as joint space. The results illustrate that sequential planning leads to sub-optimal results as
it does not take future goals into consideration. This leads directly to a higher cost (c.f. Fig. 4(c)),
calculated from trials with randomised start state. One should however note that this effect would
be less pronounced if the cost required stopping at the via po int, as it is the velocity away from the
end target which is the main problem for the sequential planner.

5 Conclusion

The contribution of this paper is a novel method for jointly optimizing a movement trajectory and
its time evolution (temporal scale and duration) in the stochastic optimal control framework. As a
special case, this solves the problem of an unknown goal horizon and the problem of trajectory op-
timization through via points when the timing of intermediate constraints is unknown and subject to
optimization. Both cases are of high relevance in practical robotic applications where pre-specifying
a goal horizon by hand is common practice but typically lacks justiﬁcation.

The method was derived in the form of an Expectation-Maximization algorithm where the E-step ad-
dresses the stochastic optimal control problem reformulat ed as an inference problem and the M-step
re-adapts the time evolution of the trajectory. In principle, the proposed framework can be applied
to extend any algorithm that – directly or indirectly – provi
des us with an approximate trajectory
posterior in each iteration. AICO [16] does so directly in te rms of a Gaussian approximation; simi-
larly, the local LQG solution implicit in iLQG [9] can, with little extra computational cost, be used
to compute a Gaussian posterior over trajectories. For algo rithms like DDP [6], which do not lead to
an LQG approximation, we can employ the Laplace method to obtain Gaussian posteriors or adjust
the M-Step for the non-Gaussian posterior. We demonstrated the algorithm on a standard reaching
task with and without via points. In particular, in the via point case, it becomes obvious that ﬁxed
horizon methods and sequenced ﬁrst exit time methods cannot
ﬁnd equally efﬁcient motions as the
proposed method.

8

References

[1] David Barber and Tom Furmston. Solving deterministic po licy (PO)MDPs using expectation-
maximisation and antifreeze. In European Conference on Machine Learning (LEMIR work-
shop), 2009.
[2] Marc Peter Deisenroth, Carl Edward Rasmussen, and Jan Pe ters. Gaussian process dynamic
programming. Neurocomputing, 72(7-9):1508 – 1524, 2009.
[3] Yu-Yi Fu, Chia-Ju Wu, Kuo-Lan Su, and Chia-Nan Ko. A time-scaling method for near-time-
optimal control of an omni-directional robot along speciﬁe d paths. Artiﬁcial Life and Robotics ,
13(1):350 –354, 2008.
[4] Z Ghahramani and G Hinton. Parameter estimation for linear dynamical systems. Technical
Report CRG-TR-96-2, University of Toronto, 1996.
[5] Z Ghahramani and S Roweis. Learning nonlinear dynamical systems using an em algorithm.
In Advances in Neural Information Processing Systems, volume 11, Nov 1999.
[6] D Jacobson and D Mayne. Differential Dynamic Programming. Elsevier, 1970.
[7] Hilbert J. Kappen. A linear theory for control of non-linear stochastic systems. Physical
Review Letters, 95(20):200201, 2005.
[8] Donald E. Kirk. Optimal Control Theory - An Introduction. Prentice-Hall, 1970.
[9] Weiwei Li and Emanuel Todorov. An iterative optimal control and estimation design for non-
linear stochastic system. In Proc. of the 45th IEEE Conference on Decision and Control, 2006.
[10] Djordje Mitrovic, Sho Nagashima, Stefan Klanke, Takamitsu Matsubara, and Sethu Vijayaku-
mar. Optimal feedback control for anthropomorphic manipul ators. In Proc. IEEE International
Conference on Robotics and Automation (ICRA 2010), 2010.
[11] Peter Pastor, Heiko Hoffmann, Tamim Asfour, and Stefan Schaal. Learning and generalization
of motor skills by learning from demonstration. In Proc. IEEE International Conference on
Robotics and Automation (ICRA 2010), Feb 2010.
[12] Gideon Sahar and John M. Hollerbach. Planning of minimum- time trajectories for robot arms.
The International Journal of Robotics Research, 5(3):90 –100, 1986.
[13] Robert F. Stengel. Optimal Control and Estimation. Dover Publications, 1986.
[14] Emanuel Todorov. Compositionality of optimal control laws. In Advances in Neural Informa-
tion Processing Systems, volume 22, 2009.
[15] Emanuel Todorov and Michael Jordan. Optimal feedback control as a theory of motor coordi-
nation. Nature Neuroscience, 5(11):1226 –1235, 2002.
[16] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proc. of the 26
th International Conference on Machine Learning (ICML 2009), 2009.
[17] Marc Toussaint and Amos Storkey. Probabilistic inference for solving discrete and continuous
state Markov Decision Processes. In Proc. of the 23nd Int. Conf. on Machine Learning (ICML
2006), pages 945 –952, 2006.

9


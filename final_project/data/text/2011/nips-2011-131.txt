Generalised Coupled Tensor Factorisation

Y. Kenan Yılmaz
Umut S¸ ims¸ekli
A. Taylan Cemgil
Department of Computer Engineering
Bo ˘gazic¸ i University, Istanbul, Turkey
kenan@sibnet.com.tr, {taylan.cemgil, umut.simsekli}@boun.edu.tr

Abstract

We derive algorithms for generalised tensor factorisation (GTF) by building upon
the well-established theory of Generalised Linear Models. Our algorithms are
general in the sense that we can compute arbitrary factorisations in a message
passing framework, derived for a broad class of exponential family distribu-
tions including special cases such as Tweedie’s distributions corresponding to β -
divergences. By bounding the step size of the Fisher Scoring iteration of the GLM,
we obtain general updates for real data and multiplicative updates for non-negative
data. The GTF framework is, then extended easily to address the problems when
multiple observed tensors are factorised simultaneously. We illustrate our coupled
factorisation approach on synthetic data as well as on a musical audio restoration
problem.

1

Introduction

A fruitful modelling approach for extracting meaningful information from highly structured mul-
tivariate datasets is based on matrix factorisations (MFs). In fact, many standard data processing
methods of machine learning and statistics such as clustering, source separation, independent com-
ponents analysis (ICA), nonnegative matrix factorisation (NMF), latent semantic indexing (LSI)
can be expressed and understood as MF problems. These MF models also have well understood
probabilistic interpretations as probabilistic generative models. Indeed, many standard algorithms
mentioned above can be derived as maximum likelihood or maximum a-posteriori parameter esti-
mation procedures. It is also possible to do a full Bayesian treatment for model selection [1].

2 Z p,r
Z j,r
4

2 Z k,r
1 Z j,r
Z i,r
3

Tensors appear as a natural generalisation of matrix factorisation, when observed data and/or a latent
representation have several semantically meaningful dimensions. Before giving a formal deﬁnition,
consider the following motivating example
X i,j,k
X j,p
X j,q
1 ≈ Xr
2 ≈ Xr
3 ≈ Xr
where X1 is an observed 3-way array and X2 , X3 are 2-way arrays, while Zα for α = 1 . . . 5 are
the latent 2-way arrays. Here, the 2-way arrays are just matrices but this can be easily extended to
objects having arbitrary number of indices. As the term ’N -way array’ is awkward, we prefer using
the more convenient term tensor. Here, Z2 is a shared factor, coupling all models. As the ﬁrst model
is a CP (Parafac) while the second and the third ones are MF’s, we call the combined factorization
as CP/MF/MF model. Such models are of interest when one can obtain different ’views’ of the
same piece of information (here Z2 ) under different experimental conditions. Singh and Gordon
[2] focused on a similar problem called as collective matrix factorisation (CMF) or multi-matrix
factorisation, for relational learning but only for matrix factors and observations. In addition, their
generalised Bregman divergence minimisation procedure assumes matching link and loss functions.
For coupled matrix and tensor factorization (CMTF), recently [3] proposed a gradient-based all-
at-once optimization method as an alternative to alternating least square (ALS) optimization and

2 Z q ,r
Z j,r
5

(1)

1

demonstrated their approach for a CP/MF coupled model. Similar models are used for protein-
protein interactions (PPI) problems in gene regulation [4].

The main motivation of the current paper is to construct a general and practical framework for
computation of tensor factorisations (TF), by extending the well-established theory of Generalised
Linear Models (GLM). Our approach is also partially inspired by probabilistic graphical models:
our computation procedures for a given factorisation have a natural message passing interpretation.
This provides a structured and efﬁcient approach that enabl es very easy development of application
speci ﬁc custom models, priors or error measures as well as al gorithms for joint factorisations where
an arbitrary set of tensors can be factorised simultaneously. Well known models of multiway analysis
(Parafac, Tucker [5]) appear as special cases and novel models and associated inference algorithms
can be automatically be developed. In [6], the authors take a similar approach to tensor factorisations
as ours, but that work is limited to K L and Euclidean costs, generalising MF models of [7] to the
tensor case. It is possible to generalise this line of work to β -divergences [8] but none of these works
address the coupled factorisation case and consider only a restricted class of cost functions.

2 Generalised Linear Models for Matrix/Tensor Factorisation

hxi i = ˆxi =

To set the notation and our approach, we brieﬂy review GLMs fo llowing closely the original notation
of [9, ch 5]. A GLM assumes that a data vector x has conditionally independently drawn components
xi according to an exponential family density
var(xi ) = τ 2 ∂ 2 b(γi )
xi ∼ exp (cid:16) xiγi − b(γi )
∂ b(γi )
− c(xi , τ )(cid:17)
∂ γ 2
τ 2
∂ γi
i
Here, γi are canonical parameters and τ 2 is a known dispersion parameter. hxi i is the expectation of
xi and b(·) is the log partition function, enforcing normalization. The canonical parameters are not
directly estimated, instead one assumes a link function g(·) that ’links’ the mean of the distribution
is the ith row vector of a known model matrix L and
i z where l⊤
ˆxi and assumes that g( ˆxi ) = l⊤
i
z is the parameter vector to be estimated, A⊤ denotes matrix transpose of A. The model is linear
in the sense that a function of the mean is linear in parameters, i.e., g( ˆx) = Lz . A Linear Model
(LM) is a special case of GLM that assumes normality, i.e. xi ∼ N (xi ; ˆxi , σ2 ) as well as linearity
that implies identity link function as g( ˆxi ) = ˆxi = l⊤
i z assuming li are known. Logistic regression
i z ; here log ˆxi and z have a linear relationship [9].
assumes a log link, g( ˆxi ) = log ˆxi = l⊤
The goal in classical GLM is to estimate the parameter vector z . This is typically achieved via
a Gauss-Newton method (Fisher Scoring). The necessary objects for this computation are the log
likelihood, the derivative and the Fisher Information (the expected value of negative of the Fisher
Score). These are easily derived as:
∂L
1
L = Xi
[xiγi − b(γi )]/τ 2 − Xi
τ 2 Xi
(xi − ˆxi )wi g ˆx ( ˆxi )l⊤
=
i
∂ z
(cid:28) ∂ 2L
∂L
∂ z 2 (cid:29) =
1
1
τ 2 L⊤DL
τ 2 L⊤DG(x − ˆx)
=
∂ z
where w is a vector with elements wi , D and G are the diagonal matrices as D = diag(w), G =
diag(g ˆx ( ˆxi )) and
ˆx ( ˆxi )(cid:17)−1
∂ g( ˆxi )
wi = (cid:16)v( ˆxi )g2
∂ ˆxi
with v( ˆxi ) being the variance function related to the observation variance by var(xi ) = τ 2 v( ˆxi ).
Via Fisher Scoring, the general update equation in matrix form is written as
z ← z + (cid:16)L⊤DL(cid:17)−1
Although this formulation is somewhat abstract, it covers a very broad range of model classes that
are used in practice. For example, an important special case appears when the variance functions
are in the form of v( ˆx) = ˆxp . By setting p = {0, 1, 2, 3} these correspond to Gaussian, Poisson,
Exponential/Gamma, and Inverse Gaussian distributions [10, pp.30], which are special cases of the
exponential family of distributions for any p named Tweedie’s family [11]. Those for p = {0, 1, 2},
in turn, correspond to EU, KL and IS cost functions often used for NMF decompositions [12, 7].

L⊤DG(x − ˆx)

(3)

(4)

g ˆx ( ˆxi ) =

(2)

(5)

(6)

c(xi , τ )

2

2.1 Tensor Factorisations (TF) as GLM’s

The key observation for expressing a TF model as a GLM is to identify the multilinear structure
and using an alternating optimization approach. To hide the notational complexity, we will give an
example with a simple matrix factorisation model; extension to tensors will require heavier notation,
but are otherwise conceptually straightforward. Consider a MF model
g( ˆX )i,j = Xr
where Z1 , Z2 and g( ˆX ) are matrices of compatible sizes. Indeed, by applying the vec operator
(vectorization, stacking columns of a matrix to obtain a vector) to both sides of (7) we obtain two
equivalent representation of the same system

g( ˆX ) = Z1Z2

1 Z j,r
Z i,r
2

in scalar

(7)

vec(g( ˆX )) = (I|j | ⊗ Z1 ) vec(Z2 ) =

∂ g( ˆX )
∂ (Z1Z2 )
∂Z2
∂Z2
where I|j | denotes the |j | × |j | identity matrix, ⊗ denotes the Kronecker product [13], and vec Z ≡
~Z . Clearly, this is a GLM where ∇2 plays the role of a model matrix and ~Z2 is the parameter
vector. By alternating between Z1 and Z2 , we can maximise the log-likelihood iteratively; indeed
this alternating maximisation is standard for solving matrix factorisation problems. In the sequel, we
will show that a much broader range of algorithms can be readily derived in the GLM framework.

vec(Z2 ) ≡ ∇2 ~Z2

vec(Z2 ) =

(8)

2.2 Generalised Tensor Factorisation

We deﬁne a tensor Λ as a multiway array with an index set V = {i1 , i2 , . . . , i|α| } where each index
in for n = 1 . . . |α| runs as in = 1 . . . |in |. An element of the tensor Λ is a scalar that we denote
by Λ(i1 , i2 , . . . , i|α| ) or Λi1 ,i2 ,...,i|α| or as a shorthand notation by Λ(v) with v being a particular
|v | denotes number of all distinct conﬁgurations for V , and e.g. if V = {i1 , i2 } then
conﬁguration.
|v | = |i1 ||i2 |. We call the form Λ(v) as element-wise; the notation [ ] yields a tensor by enumerating
all the indices, i.e., Λ = [Λi1 ,i2 ,...,i|α| ] or Λ = [Λ(v)]. For any two tensors X and Y of compatible
order, X ◦ Y is an element-wise multiplication and if not explicitly stressed X/Y is an element-wise
division. 1 is an object of all ones whose order depends on the context where it is used.

A generalised tensor factorisation problem is speci ﬁed by a n observed tensor X (with possibly
missing entries, to be treated later) and a collection of latent tensors to be estimated, Z1:|α| = {Zα }
for α = 1 . . . |α|, and by an exponential family of form (2). The index set of X is denoted by V0 and
the index set of each Zα by Vα . The set of all model indices is V = S|α|
α=1 Vα . We use vα (or v0 )
to denote a particular conﬁguration of the indices for Zα (or X ) while ¯vα denoting a conﬁguration
of the compliment ¯Vα = V /Vα . The goal is to ﬁnd the latent Zα that maximize the likelihood
p(X |Z1:α ) where hX i = ˆX is given via
g( ˆX (v0 )) = X¯v0 Yα
ˆX (i, j, k) =
To clarify our notation with an example, we express the CP (Parafac) model, deﬁned as
Pr Z1 (i, r)Z2 (j, r)Z3 (k , r). In our notation, we take identity link g( ˆX ) = ˆX and the index sets
with V = {i, j, k , r}, V0 = {i, j, k}, ¯V0 = {r}, V1 = {i, r}, V2 = {j, r} and V3 = {k , r}. Our
notation deliberately follows that of graphical models; the reader might ﬁnd it useful to associate
indices with discrete random variables and factors with probability tables [14]. Obviously, while a
TF model does not represent a discrete probability measure, the algebraic structure is nevertheless
analogous.

Zα (vα )

(9)

To extend the discussion in Section 2.1 to the tensor case, we need the equivalent of the model
matrix, when updating Zα . This is obtained by summing over the product of all remaining factors
g( ˆX (v0 )) = X¯v0∩vα
Zα (vα ) X¯v0∩¯vα Yα′ 6=α
Zα′ (vα′ ) = X¯v0∩vα
Lα (oα ) = X¯v0∩¯vα Yα′ 6=α
with oα ≡ (v0 ∪ vα ) ∩ (¯v0 ∪ ¯vα )
Zα′ (vα′ )

Zα (vα )Lα (oα )

3

One related quantity to Lα is the derivative of the tensor g( ˆX ) wrt the latent tensor Zα denoted as
∇α and is deﬁned as (following the convention [13, pp 196])
∂ g( ˆX )
∂Zα

with Lα ∈ R|v0∩¯vα |×|¯v0∩vα |

= I|v0∩vα | ⊗ Lα

∇α =

(10)

(11)

The importance of Lα is that, all the update rules can be formulated by a product and subsequent
contraction of Lα with another tensor Q having exactly the same index set of the observed tensor
X . As a notational abstraction, it is useful to formulate the following function,
Deﬁnition 1. The tensor valued function ∆α (Q) : R|v0 | → R|vα | is deﬁned as
Q(v0 ) Lα (oα )ε i
α (Q) = h Xv0∩¯vα
∆ε
with ∆α (Q) being an object of the same order as Zα and oα ≡ (v0 ∪ vα ) ∩ (¯v0 ∪ ¯vα ). Here, on
the right side, the nonnegative integer ε denotes the element-wise power, not to be confused with an
index. On the left, it should be interpreted as a parameter of the ∆ function. Arguably, ∆ function
abstracts away all the tedious reshape and unfolding operations [5]. This abstraction has also an
important practical facet: the computation of ∆ is algebraically (almost) equivalent to computation
of marginal quantities on a factor graph, for which efﬁcient message passing algorithms exist [14].
ˆX i,j,k = Pp,q ,r Ai,pB j,q C k,rGp,q ,r with V =
Example 1. TUCKER3 is deﬁned as
{i, j, k , p, q , r}, V0 = {i, j, k}, VA = {i, p}, VB = {j, q}, VC = {k , r}, VG = {p, q , r}. Then
for the ﬁrst factor A, the objects LA and ∆ε
A () are computed as follows
LA = "Xq ,r
B j,q C k,rGp,q ,r # = h((C ⊗ B )G⊤ )p
k,j i = h(cid:0)LA (cid:1)p
k,j i
A (Q) = 
k,j 
A (cid:1)p
A (cid:1)p
Qk,j
Xj,k
∆ε
 = (cid:2)(cid:0)QLε
i (cid:0)Lε
i (cid:3)
The index sets marginalised out for LA and ∆A are ¯V0 ∩ ¯VA = {p, q , r} ∩ {j, q , k , r} = {q , r} and
V0 ∩ ¯VA = {i, j, k} ∩ {j, q , k , r} = {j, k}. Also we verify the order of the gradient ∇A (10) as
p
k,j = ∇i,p
I i
i,k,j that conforms the matrix derivation convention [13, pp.196].
i ⊗ LA

(12)

(13)

2.3

Iterative Solution for GTF

(14)

with ∇α =

∂ g( ˆX )
∂Zα

As we have now established a one to one relationship between GLM and GTF objects such as the
observation x ≡ vec X, the mean (and the model estimate) ˆx ≡ vec ˆX , the model matrix L ≡ Lα
and the parameter vector z ≡ vec Zα , we can write directly from (6) as
α D∇α(cid:17)−1
α DG( ~X − ~ˆX )
~Zα ← ~Zα + (cid:16)∇⊤
∇⊤
There are at least two ways that this update can further simpli ﬁed. We may assume an identity
link function, or alternatively we may choose a matching link and lost functions such that they
cancel each other smoothly [2]. In the sequel we consider identity link g( ˆX ) = ˆX that results to
g ˆX ( ˆX ) = 1. This implies G to be identity, i.e. G = I . We deﬁne a tensor W , that plays the same
role as w in (5), which becomes simply the precision (inverse variance function), i.e. W = 1/v( ˆX )
where for the Gaussian, Poisson, Exponential and Inverse Gaussian distributions we have simply
W = ˆX −p with p = {0, 1, 2, 3} [10, pp 30]. Then, the update (14) is reduced to
α D∇α(cid:17)−1
α D( ~X − ~ˆX )
~Zα ← ~Zα + (cid:16)∇⊤
∇⊤
After this simpli ﬁcation we obtain two update rules for GTF f or non-negative and real data.
The update (15) can be used to derive multiplicative update rules (MUR) popularised by [15] for the
nonnegative matrix factorisation (NMF). MUR equations ensure the non-negative parameter updates
as long as starting some non-negative initial values.

(15)

4

Theorem 1. The update equation (15) for nonnegative GTF is reduced to multiplicative form as

Zα ← Zα ◦

∆α (W ◦ X )
∆α (W ◦ ˆX )

s.t. Zα (vα ) > 0

(16)

η =

<

≤

(17)

(Proof sketch) Due to space limitation we leave the full details of the proof, but idea is that inverse
of H = ∇⊤D∇ is identi ﬁed as step size and by use of the results of the Perro n-Frobenious theorem
[16, pp 125] we further bound it as
vα (cid:0)H ~Zα (cid:1)(vα )
~Zα
2 ~Zα
2
since λmax (H ) ≤ max
~ˆX
~ˆX
λmax (∇⊤D∇)
Zα (vα )
∇⊤D
∇⊤D
For the special case of the Tweedie family where the precision is a function of the mean as W =
ˆX −p for p = {0, 1, 2, 3} the update (15) is reduced to
∆α ( ˆX −p ◦ X )
∆α ( ˆX 1−p )
For example, to update Z2 for the NMF model ˆX = Z1Z2 , ∆2 is ∆2 (Q) = Z ⊤
1 Q. Then for the
ˆX ). For the Poisson (p = 1)
Gaussian (p = 0) this reduces to NMF-EU as Z2 ← Z2 ◦ (Z ⊤
1 X )/(Z ⊤
1
1 (X/ ˆX )(cid:1)/(cid:0)Z ⊤
1 1(cid:1) [15].
it reduces to NMF-KL as Z2 ← Z2 ◦ (cid:0)Z ⊤
By dropping the non-negativity requirement we obtain the following update equation:
Theorem 2. The update equation for GTF with real data can be expressed as
∆α (W ◦ (X − ˆX ))
∆2
α (W )

with λα/0 = |vα ∩ ¯v0 |

Zα ← Zα +

Zα ← Zα ◦

2
λα/0

(18)

(19)

(Proof sketch) Again skipping the full details, as part of the proof we set Zα = 1 in (17) speci ﬁcally,
and replacing matrix multiplication of ∇⊤D∇1 by ∇⊤ 2
D1λα/0 completes the proof. Here the
multiplier λα/0 is the cardinality arising from the fact that only λα/0 elements are non-zero in a row
of ∇⊤D∇. Note the example for λα/0 that if Vα ∩ ¯V0 = {p, q} then λα/0 = |p||q | which is number
of all distinct conﬁgurations for the index set {p, q}.
Missing data can be handled easily by dropping the missing data terms from the likelihood [17]. The
net effect of this is the addition of an indicator variable mi to the gradient ∂L/∂ z = τ −2 Pi (xi −
i with mi = 1 if xi is observed otherwise mi = 0. Hence we simply deﬁne a mask
ˆxi )miwi g ˆx ( ˆxi )l⊤
tensor M having the same order as the observation X , where the element M (v0 ) is 1 if X (v0 ) is
observed and zero otherwise. In the update equations, we merely replace W with W ◦ M .

3 Coupled Tensor Factorization

Zα (vα )Rν,α

Here we address the problem when multiple observed tensors Xν for ν = 1 . . . |ν | are factorised
simultaneously. Each observed tensor Xν now has a corresponding index set V0,ν and a particular
conﬁguration will be denoted by v0,ν ≡ uν . Next, we deﬁne a |ν | × |α| coupling matrix R where
Xν and Zα connected
Rν,α = (cid:26) 1
ˆXν (uν ) = X¯uν Yα
otherwise
0
For the coupled factorisation, we get the following expression as the derivative of the log likelihood
∂ ˆXν (uν )
Rν,α Xuν ∩¯vα (cid:16)Xν (uν ) − ˆXν (uν )(cid:17)Wν (uν )
= Xν
∂Zα (vα )
where Wν ≡ W ( ˆXν (uν )) are the precisions. Then proceeding as in section 2.3 (i.e. getting the
Hessian and ﬁnding Fisher Information) we arrive at the upda te rule in vector form as
α,ν Dν ∇α,ν (cid:17)−1(cid:16) Xν
α,ν Dν (cid:0) ~Xν − ~ˆXν (cid:1)(cid:17)
~Zα ← ~Zα + (cid:16) Xν
Rν,α∇⊤
Rν,α∇⊤
5

∂L
∂Zα (vα )

(21)

(22)

(20)

. . .

Zα

. . .

Z|α|

Z1

A

B

C

D

E

. . .

Xν

. . .

X|ν |

X1

X1

X2

X3

Figure 1: (Left) Coupled factorisation structure where the arrow indicates the existence of the inﬂu-
ence of latent tensor Zα onto the observed tensor Xν . (Right) The CP/MF/MF coupled factorisation
problem in 1.

(23)

Q(uν )(cid:16) Yα′ 6=α

where ∇α,ν = ∂ g( ˆXν )/∂Zα . The update equations for the coupled case are quite intuitive; we
calculate the ∆α,ν functions deﬁned as
Zα′ (vα′ )Rν,α (cid:17)ε i
α,ν (Q) = h Xuν ∩¯vα
∆ε
for each submodel and add the results:
Lemma 1. Update for non-negative CTF
Zα ← Zα ◦ Pν Rν,α∆α,ν (Wν ◦ Xν )
Pν Rν,α∆α,ν (cid:16)Wν ◦ ˆXν (cid:17)
In the special case of a Tweedie family, i.e. for the distributions whose precision as Wν = ˆX −p
ν , the
ν (cid:17)(cid:17).
ν ◦ Xν (cid:17)(cid:17) / (cid:16)Pν Rν,α∆α,ν (cid:16) ˆX 1−p
update is Zα ← Zα ◦ (cid:16)Pν Rν,α∆α,ν (cid:16) ˆX −p
Lemma 2. General update for CTF
λα/0 Pν Rν,α∆α,ν (cid:16)Wν ◦ (cid:0)Xν − ˆXν (cid:1)(cid:17)
2
Pν Rν,α∆2
α,ν (Wν )
For the special case of the Tweedie family we plug Wν = ˆX −p
and get the related formula.
ν

Zα ← Zα +

(24)

(25)

4 Experiments

(26)

(27)

with

B j,rE q ,r

B j,rDp,r

Ai,rB j,r C k,r

1 1
1 0
1 0

Here we want to solve the CTF problem introduced (1), which is a coupled CP/MF/MF problem
ˆX i,j,k
ˆX j,p
ˆX j,q
1 = Xr
2 = Xr
3 = Xr
where we employ the symbols A : E for the latent tensors instead of Zα . This factorisation problem
has the following R matrix with |α| = 5, |ν | = 3
ˆX1 = P A1B 1C 1D0E 0
0 0
R = " 1
0 1 #
ˆX2 = P A0B 1C 0D1E 0
1 0
0
0
ˆX3 = P A0B 1C 0D0E 1
α,ν () for ν = 1 (CP)
We want to use the general update equation (25). This requires derivation of ∆ε
and ν = 2 (MF) but not for ν = 3 since that ∆α,3 () has the same shape as ∆α,2 (). Here we show
the computation for B , i.e. for Z2 , which is the common factor
B ,1 (Q) = "Xik
Qi,j,k (cid:16)Ai,r C k,r (cid:17)ε# = Q(1) (C ε ⊙ Aε )
∆ε
B ,2 (Q) = "Xp
Qj,p (cid:0)Dp,r (cid:1)ε# = QDε
∆ε
6

(28)

(29)

with Q(n) being mode-n unfolding operation that turns a tensor into matrix form [5]. In addition,
for ν = 1 the required scalar value λB/0 is |r | here since VB ∩ ¯V0 = {j, r} ∩ {r} = {r} noting that
value λB/0 is the same for ν = 2, 3. The simulated data size for observables is |i| = |j | = |k | =
|p| = |q | = 30 while the latent dimension is |r | = 5. The number of iterations is 1000 with the
Euclidean cost while the experiment produced similar results for KL cost as shown in Figure 2.

A

5
D

5

10

5

0

0

10

5

 
0

10

10

B

5
E

5

5

0

0

10

5

0

0

5

0

0

10

 

10

C

5

Orginal
Initial
Final

10

Figure 2: The ﬁgure compares the original, the initial (star t up) and the ﬁnal (estimate) factors for
Zα = A, B , C, D , E . Only the ﬁrst column, i.e. Zα (1 : 10, 1) is plotted. Note that CP factorisation
is unique up to permutation and scaling [5] while MF factorisation is not unique, but when coupled
with CP it recovers the original data as shown in the ﬁgure. Fo r visualisation, to ﬁnd the correct
permutation, for each of Zα the matching permutation between the original and estimate are found
by solving an orthogonal Procrustes problem [18, pp 601].

4.1 Audio Experiments

In this section, we illustrate a real data application of our approach, where we reconstruct missing
parts of an audio spectrogram X (f , t), that represents the STFT coefﬁcient magnitude at frequenc y
bin f and time frame t of a piano piece, see top left panel of Fig.3. This is a difﬁcul
t matrix
completion problem: as entire time frames (columns of X ) are missing, low rank reconstruction
techniques are likely to be ineffective. Yet such missing data patterns arise often in practice, e.g.,
when packets are dropped during digital communication. We will develop here a novel approach,
expressed as a coupled TF model. In particular, the reconstruction will be aided by an approximate
musical score, not necessarily belonging to the played piece, and spectra of isolated piano sounds.

Pioneering work of [19] has demonstrated that, when a audio spectrogram of music is decomposed
using NMF as X1 (f , t) ≈ ˆX (f , t) = Pi D(f , i)E (i, t), the computed factors D and E tend to be
semantically meaningful and correlate well with the intuitive notion of spectral templates (harmonic
proﬁles of musical notes) and a musical score (reminiscent o f a piano roll representation such as a
MIDI ﬁle). However, as time frames are modeled conditionall y independently, it is impossible to
reconstruct audio with this model when entire time frames are missing.

In order to restore the missing parts in the audio, we form a model that can incorporates musical
information of chords structures and how they evolve in time. In order to achieve this, we hierarchi-
cally decompose the excitation matrix E as a convolution of some basis matrices and their weights:
E (i, t) = Pk,τ B (i, τ , k)C (k , t − τ ). Here the basis tensor B encapsulates both vertical and tem-
poral information of the notes that are likely to be used in a musical piece; the musical piece to
be reconstructed will share B , possibly played at different times or tempi as modelled by G. After
replacing E with the decomposed version, we get the following model (eq 30):
ˆX1 (f , t) = Xi,τ ,k,d
ˆX2 (i, n) = Xτ ,k,m
B (i, τ , k)G(k , m)Y (m, n, τ )
ˆX3 (f , p) = Xi
D(f , i)F (i, p)T (i, p)

D(f , i)B (i, τ , k)C (k , d)Z (d, t, τ )

MIDI ﬁle

Merged training ﬁles

(32)

Test ﬁle

(30)

(31)

7

Here we have introduced new dummy indices d and m, and new ( ﬁxed) factors Z (d, t, τ ) = δ(d −
t + τ ) and Y (m, n, τ ) = δ(m − n + τ ) to express this model in our framework. In eq 32, while
forming X3 we concatenate isolated recordings corresponding to different notes. Besides, T is a
0 − 1 matrix, where T (i, p) = 1(0) if the note i is played (not played) during the time frame p and
F models the time varying amplitudes of the training data. R matrix for this model is deﬁned as
ˆX1 = P D1B 1C 1Z 1G0Y 0F 0T 0
ˆX2 = P D0B 1C 0Z 0G1Y 1F 0T 0
ˆX3 = P D1B 0C 0Z 0G0Y 0F 1T 1
Figure 3 illustrates the performance the model, using K L cost (W = ˆX −1 ) on a 30 second piano
recording where the 70% of the data is missing; we get about 5dB SNR improvement, gracefully
degrading from 10% to 80% missing data: the results are encouraging as quite long portions of audio
are missing, see bottom right panel of Fig.3.

R = " 1 1
0 1
1 0

0 0 0
1 1 0
0 0 1

1 1
0 0
0 0

0
1 #
0

with

(33)

)
z
H
(
 
y
c
n
e
u
q
e
r
F

)
z
H
(
 
y
c
n
e
u
q
e
r
F

2000

1500

1000

500

0

2000

1500

1000

500

0

X1

X2 (Transcription Data)

X3 (Isolated Recordings)

80

60

s
e
t
o
N

40

20

)
z
H
(
 
y
c
n
e
u
q
e
r
F

2000

1500

1000

500

0

50

100
Time (sec)
Ground Truth

150

5

10

15
Time (sec)

20

25

)
z
H
(
 
y
c
n
e
u
q
e
r
F

2000

1500

1000

500

0

)
B
d
(
 
R
N
S

15

10

5

0

 

100

200
Time (sec)
Performance

300

 

Reconst. SNR
Initial SNR

20

40
60
Missing Data Percentage (%)

80

5

10

15
Time (sec)
X1hat (Restored)

20

25

5

10

15
Time (sec)

20

25

Figure 3: Top row, left to right: Observed matrices X1 : spectrum of the piano performance, darker
colors imply higher magnitude (missing data (70%) are shown white), X2 , a piano roll obtained
from a musical score of the piece, X3 , spectra of 88 isolated notes from a piano. Bottom Row:
Reconstructed X1 , the ground truth, and the SNR results with increasing missing data. Here, initial
SNR is computed by substituting 0 as missing values.

5 Discussion

This paper establishes a link between GLMs and TFs and provides a general solution for the compu-
tation of arbitrary coupled TFs, using message passing primitives. The current treatment focused on
ML estimation; as immediate future work, the probabilistic interpretation is to be extended to a full
Bayesian inference with appropriate priors and inference methods. A powerful aspect, which we
have not been able to summarize here is assigning different cost functions, i.e. distributions, to dif-
ferent observation tensors in a coupled factorization model. This requires only minor modi ﬁcations
to the update equations. We believe that, as a whole, the GCTF framework covers a broad range
of models that can be useful in many different application areas beyond audio processing, such as
network analysis, bioinformatics or collaborative ﬁlteri ng.
Acknowledgements: This work is funded by the T ¨UB˙ITAK grant number 110E292, Bayesian
matrix and tensor factorisations (BAYTEN) and Bo ˘gazic¸ i University research fund BAP5723. Umut
S¸ ims¸ekli is also supported by a Ph.D. scholarship from T ¨UB˙ITAK. We also would like to thank to
Evrim Acar for the fruitful discussions.

8

References

[1] A. T. Cemgil, Bayesian inference for nonnegative matrix factorisation models, Computational Intelligence
and Neuroscience 2009 (2009) 1–17.
[2] A. P. Singh, G. J. Gordon, A uni ﬁed view of matrix factorization mode ls, in: ECML PKDD’08, Part II,
no. 5212, Springer, 2008, pp. 358–373.
[3] E. Acar, T. G. Kolda, D. M. Dunlavy, All-at-once optimization for coupled matrix and tensor factoriza-
tions, CoRR abs/1105.3422. arXiv:1105.3422.
[4] Q. Xu, E. W. Xiang, Q. Yang, Protein-protein interaction prediction via collective matrix factorization,
in: In Proc. of the IEEE International Conference on BIBM, 2010, pp. 62–67.
[5] T. G. Kolda, B. W. Bader, Tensor decompositions and applications, SIAM Review 51 (3) (2009) 455–500.
[6] Y. K. Yılmaz, A. T. Cemgil, Probabilistic latent tensor factorization, in: P roceedings of the 9th interna-
tional conference on Latent variable analysis and signal separation, LVA/ICA’10, Springer-Verlag, 2010,
pp. 346–353.
[7] C. Fevotte, A. T. Cemgil, Nonnegative matrix factorisations as probabilistic inference in composite mod-
els, in: Proc. 17th EUSIPCO, 2009.
[8] Y. K. Yılmaz, A. T. Cemgil, Algorithms for probabilistic latent tensor fac torization, Signal Process-
ing(2011),doi:10.1016/j.sigpro.2011.09.033.
[9] C. E. McCulloch, S. R. Searle, Generalized, Linear, and Mixed Models, Wiley, 2001.
[10] C. E. McCulloch, J. A. Nelder, Generalized Linear Models, 2nd Edition, Chapman and Hall, 1989.
[11] R. Kaas, Compound poisson distributions and glm’s, tweedie’s distribution, Tech. rep., Lecture, Royal
Flemish Academy of Belgium for Science and the Arts, (2005).
[12] A. Cichocki, R. Zdunek, A. H. Phan, S. Amari, Nonnegative Matrix and Tensor Factorization, Wiley,
2009.
[13] J. R. Magnus, H. Neudecker, Matrix Differential Calculus with Applications in Statistics and Economet-
rics, 3rd Edition, Wiley, 2007.
[14] M. Wainwright, M. I. Jordan, Graphical models, exponential families, and variational inference, Founda-
tions and Trends in Machine Learning 1 (2008) 1–305.
[15] D. D. Lee, H. S. Seung, Algorithms for non-negative matrix factorization, in: NIPS, Vol. 13, 2001, pp.
556–562.
[16] M. Marcus, H. Minc, A Survey of Matrix Theory and Matrix Inequalities, Dover, 1992.
[17] R. Salakhutdinov, A. Mnih, Probabilistic matrix factorization, in: Advances in Neural Information Pro-
cessing Systems, Vol. 20, 2008.
[18] G. H. Golub, C. F. V. Loan, Matrix computations, 3rd Edition, Johns Hopkins UP, 1996.
[19] P. Smaragdis, J. C. Brown, Non-negative matrix factorization for polyphonic music transcription, in:
WASPAA, 2003, pp. 177–180.

9


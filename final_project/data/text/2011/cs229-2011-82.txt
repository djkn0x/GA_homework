Nonlinear Extensions of Reconstruction ICA

Apaar Sadhwani and Apoorv Gupta
CS229 Project Report, Fall 2011

Abstract— In a recent paper [1] it was observed that un-
supervised feature learning with overcomplete features could
be achieved using linear autoencoders (named Reconstruction
Independent Component Analysis). This algorithm has been
shown to outperform other well-known algorithms by penaliz-
ing the lack of diversity (or orthogonality) amongst features.
In our project, we wish to extend and improve this algorithm
to include other non-linearities.
In this project we have considered three unsupervised learn-
ing algorithms: (a) Sparse Autoencoder (b) Reconstruction ICA
(RICA), a linear autoencoder proposed in [1], and (c) Non-
linear RICA, a proposed extension of RICA for capturing non-
linearities in feature detection.
Our research indicates that exploring non-linear extensions
of RICA holds good promise; preliminary results with hyper-
bolic tangent function on the MNIST dataset showed impressive
accuracy (comparable with sparse autoencoders), robustness,
and required a fraction of the computational effort.

I . MOT IVAT ION
In many problems of interest - text, video, imaging, or
speech - a fundamental problem is that of feature recogni-
tion. Hard coded (or, hand engineered) features have long
been used, and continue to provide us with great accuracy.
However, hard coding features requires much experience and
the performance with unexplored domains remains uncertain.
To overcome these obstacles, we use unsupervised learning,
wherein we try to identify features that occur in a sparse
fashion across our data. Mathematically, this amounts to
trying to re-construct a high dimensional vector by passing
it
through a map with sparsity constraints. The sparsity
constraints force retention of only important details, that is,
the required features.
Given our high level strategy for feature extraction, it is
important to note that while data contains only a sparse set of
features in a particular instance, the number of such features
can be potentially many. In line with this observation, the
restriction that
the number of features be less than the
dimensionality of data seems unnatural; this occurs in many
algorithms, and RICA provides a way of circumventing
this by penalizing degeneracy, instead of imposing it as a
hard constraint. However, with its current formulation, RICA
captures only linear features. Through our study, we explore
the beneﬁts of extending it to non-linear features.

I I . IN TRODUC T ION
In this paper, we compare the performance of three unsu-
pervised learning algorithms on accuracy and time. The ﬁrst
of these algorithms is Sparse Autoencoder, which is a feed-
forward neural network. Here we consider the special case of
a neural network with three layers - an input layer, a hidden

layer, and an output layer. The hidden layer automatically
enforces sparsity constraints since we have a small number
of these. In addition, we enforce penalty on the average
activations of the hidden layer, which is especially useful for
the case of overcomplete features. This is further explained
in Section III. The next algorithm we consider is RICA, as
explained in IV. With its ”soft” orthonormality constraint,
overcompleteness is accommodated with no extra effort.
Sparsity is however enforced as a separate penalty term in
the objective function. The performance obtained depends
considerably on the nature of the penalty term, and we
consider both L1 and L2 penalties. This is further explained
in Section IV, and one may refer to the original paper [1]
for the underlying theory and details.
In the end, we propose a non-linear extension to RICA.
The non-linear extension we test with is only for encoding
the features, while the decoding function remains linear. This
function is hyperbolic tangent, and our choice was based on
its ubiquitous use as a basis function throughout the machine
learning literature. We explain this procedure in greater detail
in Section V.
It is important to note that all three algorithms can be
viewed under the same umbrella - Sparse Autoencoders use
sigmoid function for encoding/decoding, RICA uses linear
function for encoding/decoding, and Non-linear RICA uses
tanh for encoding and linear for decoding. We test
the
three algorithms for different number of hidden features,
both above and below the overcompleteness threshold. The
training and testing is performed on the MNIST dataset of
handwritten digits, which is obtained from IX. The results
and conclusions are discussed in Section VI.

I I I . S PAR SE AU TOENCOD ER S
A. Introduction
set
Suppose we have unlabeled training examples
{x(1) , x(2) , . . .}. An autoencoder neural network is an unsu-
pervised learning algorithm that applies backpropagation,
setting the target values to be equal to the inputs. i.e., it
uses y(i) = x(i) . The autoencoder tries to learn a function
hW ,b (x) ≈ x.
B. Model Formation and Framework
We use the following neural network deﬁned by the fol-
lowing equations and framework. Let nl denote the number
of layers in our network. Our neural network has parameters
(l ) to
(W , b) = (W (1), b(1),W (2), b(2)), where we write Wi j
denote the parameter (or weight) associated with the connec-
tion between unit j in layer l , and unit i in layer l + 1. Also,

(l ) for
1. Perform a feedforward pass, computing the activations (ai
layers L2 , L3 , up to the output layer Lnl , using Equations (1-2).
2. For the output layer (layer nl ), set
δ (nl ) = −(x − a(nl ) ) • f (cid:48) (z(n) )
where • denotes the element-wise product operator.
3. For l = nl − 1, nl − 2, nl − 3, . . . , 2,set
δ (l ) = ((W l )T δ (l+1) ) • f (cid:48) (z(l ) )

(4)

(5)

4. Compute the desired partial derivatives:

∇W (l ) J (W , b; x, y) = δ (l+1) (a(l ) )T
∇b(l ) J (W , b; x, y) = δ (l+1)

(6)

5. Calculate Js parse (W , b) using:

(7)

KL(ρ (cid:107) ˆρ j )

Js parse (W , b) = J (W , b) + β

s2∑
j=1
where ρ , ˆρ j (sparsity constraint) and KL(ρ (cid:107) ˆρ j ) is derived in [3].
∆W (l ) ) + λW (l ) (cid:105)
(cid:104)
Here we update the parameters as follows:
(cid:104) 1
∆b(l ) (cid:105)
1
W (l ) := W (l ) − α
(
m
b(l ) := b(l ) − α
m

(9)

(8)

where α is the learning rate.
6. Train neural network using unconstrained optimizer L-BFGS to ﬁnd
W ∗ = argminW J (W , b).
7. Use σ (W X ), X ={training data, test data} to ﬁnd the training & test
features.
8. Calculate the test and training error using softmax regression.
So, we observe the sparse auto-encoders have the follow-
(cid:104) λ
ing unconstrained optimization problem:
m

(cid:107)σ (W T σ (W x(i) + b)) − x(i)(cid:107)2
2 + β

minW

m
∑
i=1

s2∑
j=1

KL(ρ (cid:107) ˆρ j )
(10)

1
where σ (x) =
1+exp(−x) . As, we will explain later in section
V, this forms our basis for motivation for modifying the
optimization function of RICA.

D. Results

By forming the image formed by the pixel intensity values,
we can understand what feature a hidden unit is looking for.
Each square in the ﬁgure below shows the input image x
that maximally activates one of hidden units. Observation is
that different hidden units have learned to detect edges at
different positions and orientations in the image. We run the
proposed algorithm of Sparse auto-encoder on our dataset
and get the following features.
This demostrates that the sparse auto-encoder algorithm
learns a set of edge detectors (which are like pen strokes
for MNIST dataset). In the ﬁgure below, we show that as
we increase the number of hidden units in the sparse auto-
encoder, the test accuracy starts increasing. At the same
time, the train accuracy reaches 100% which might be an
indication of overﬁtting.

Fig. 1. Autoencoder

(l ) is the bias associated with unit i in layer l + 1. We will
bi
(l ) to denote the activation (or output value) of unit
write ai
i in layer l . Given a ﬁxed setting of the parameters {W ,b},
our neural network deﬁnes a hypothesis hW ,b (x) that tries to
reconstruct the original input. Speciﬁcally, the computation
that this neural network represents is given by:
(1) x3 + b1
(1) x2 + W13
(1) x1 + W12
(2) = f (W11
(2) = f (W21
(1) x1 + W22
(1) x2 + W23
(1) x3 + b2
(1) x3 + b3
(1) x2 + W33
(1) x1 + W32
(2) = f (W31

(1) )
(1) )
(1) )

a1
a2
a3

{hW ,b (x)}i = ai

(2) )

(2) a2

(2) a1

(2) + Wi2

(3) = f (Wi1

(2) + Wi3
(2) a3
(2) + bi
(1) x j + b(1)
(2) = ∑n
In the sequel, we also let zi
j=1 Wi j
denote
i
the total weighted sum of inputs to unit i in layer l , so that
a(l )
i = f (z(l )
i ). Also:
z(l+1) = W (l )a(l ) + b(l )
a(l+1) = f (z(l+1) )

(1)
(2)

+

λ
2

C. Backpropagation algorithm
Given a training set of m examples, we then deﬁne the
overall cost function to be:
(cid:104) 1
(cid:105)
m
∑
J (W , b; x(i) )
J (W , b) =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)hW ,b (x(i) ) − x(i) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:104) 1
(cid:105)
m
i=1
)
m
The second term is a regularization term (also called a
weight decay term) that tends to decrease the magnitude of
the weights, and helps prevent overﬁtting.The weight decay
parameter λ controls the relative importance of the two
terms. The goal is to minimize J (W , b) as a function of W
and b using the following back propagation algorithm:

(W (l )
ji )2
(3)

nl −1
∑
l=1
nl −1
∑
l=1

sl+1∑
j=1
sl+1∑
j=1

sl∑
i=1
sl∑
i=1

(W (l )
ji )2

m
∑
(
i=1

λ
2

1
2

=

+

the orthonormality constraint
ICA. In RICA,
is replaced
like the one we
with a linear reconstruction penalty(jut
deﬁned above for sparse auto-encoders in section III). This
also makes it possible to use unconstrained solver L-BFGS
for the optimization of the cost function. The reconstruction
penalty can also be applied across all receptive ﬁelds and
hence prevents degenerate features. Also, as RICA is based
on foundations similar to auto-encoders, it is less sensitive
to whitening.

(11)

m
∑
i=1

k
∑
j=1

minimizeW

g(W j x(i) ), subject to WW T = I

B. Model Formation
Given the unlabeled data {x(i)}m
i=1 , x(i) ∈ Rn , regular ICA
is deﬁned as the following optimization problem:
k
m
∑
∑
minimizeW
i=1
j=1
RICA on the other hand produces the following uncon-
(cid:105)
(cid:104) λ
strained optimization problem:
m
(cid:107)W T W x(i) − x(i)(cid:107)2
∑
g(W j x(i) )
2 +
m
i=1
(12)
where g is a nonlinear convex function, e.g., smooth L1
penalty: g(.) = log (cosh (.)) as it mimics the L1 norm
(cid:107).(cid:107)1 . We also construct and analyze L2 penalty in later
section. W is the weight matrix W ∈ Rk×n where k is the
number of components (features) and W j is one row (feature)
in W. Observe that RICA is tied-weighted, i.e. W is the
encoding weights matrix and W T is the decoding weights
matrix.(i.e. the encoding step is W x(i) and the decoding step
is W T (W x(i) ).The aim is to minimize the following cost
(cid:104) λ
(cid:105)
function:
g( f (W j x(i) ))
m

(cid:107)W T f (W x(i)) ) − x(i)(cid:107)2
2 +

J (W ) =

m
∑
i=1

m
∑
i=1

k
∑
j=1

where:

J (W ) =

λ
m

H (W ) + G(W )

H (W ) = (W T W X − X )T (W T W X − X )
G(W ) = g(W j X )

(13)

(14)
(15)

The activation function considered in case of RICA is
proposed to be a linear: {W x(i)}. In the next section, we
propose a non-linear extension of RICA which consider a
range of non-linear functions f (W x(i) ) instead of (W x(i) ) and
try to compare its performance with our implementation of
RICA in terms of time and accuracy.

V. NON L IN EAR EX TEN S ION O F R ICA
A. Model Formation
In this section, we propose our formulation of extending
RICA to incorporate non-linear functions. Hereafter, we
refer to our algorithm as NLE-RICA. The main motivation
behind NLE-RICA is to form an intermediate function that

Fig. 2. Output Visualization (200 hidden units)

Fig. 3. Test and Training accuracy for sparse auto-encoder with different
hidden units

IV. R ECON STRUC T ION ICA

A. Motivation
Sparsity has been shown to work well for learning feature
representations that are robust for object recognition. As
discussed in section III, sparse autocoders is one of the many
ways. Other methods include ICA nad ISA (Independent
Subspace Analysis) refer paper. But the standard ICA has
two major drawbacks:
• Overcomplete Features: The ICA is not easy to train in
presence of overcomplete features (when the number of
features (cid:29) dimensionality of input data). Autoencoders
on the other hand work well in case of overcomplete
features.
• Whitening: Standard ICA is sensitive to whitening (a
preprocessing step that decorrelated the input data, and
can not always be computed exactly for high dimen-
sional data.
Both constraints in the standard ICA arise due to the
hard orthonormality constraints requiring the features to
i.e. WW T = I, which is used to prevent
be orthogonal
degenerate solution in the feature matrix W . However,
this condition of orthonormality cannot be met if we have
overcompleteness.

In
proposed Reconstruction
have
authors
the
[1],
ICA(RICA), which is a modiﬁcation to the standard

TABLE I
where:
TH E FO LLOW ING A LGOR I THM I S U S ED FOR L IN EAR R ICA :
H (W ) = (W T f (W X ) − X )T (W T f (W X ) − X )
(18)
1.Initialize λ = {0.1, 0.9}
G(W ) = g( f (W j X ))
2.Using back propagation algorithm, calculate {a(2) , a(3) , δ (1) , δ (1) , δ (1) }
(19)
3.Calculate ∇W H (W ) = 2(WW T W X X T + W X X T W T W − 2W X X T )
and g(.) = log(cosh(.)) and f (.) = γ (.) + (1 − γ )(.) where
4.Calculate ∇W G(W ) = t anh(W X )X T
γ ∈ {0, 0.4, 0.8, 1}. Hence:
m (∇W H (W )) + ∇W G(W )} and J (W )
5.Calculate ∇W J (W ) = λ
6.Use the unconstrained optimizer L-BFGS to ﬁnd W ∗ = argminW J (W )
λ
7.Use f (W X ), X ={training data, test data} to ﬁnd the training & test features.
8.Calculate the test and training error using softmax regression.
m
calculate ∇W H (W )
where we
using
the
back-
algorithm deﬁned
propagation
in
section
III
and
∇W G(W ) = tanh( f (W X ))(γ + (1 − γ )(1 − (tanh(W X ))2 ))X T
λ
m
==

(∇W H (W ) + ∇W T H (W )) + ∇W G(W )
λ
(δ (2)a(1)T + a(2)δ (3)T ) + ∇W G(W )
m

∇W H (W ) + ∇W G(W )

∇W J (W ) =

∇W J (W ) =

(20)

(21)

(22)

mimics linear function in case of RICA and the non-linear
function(sigmoid) in case of sparse auto-encoders. For the
same purpose, we consider the following functional form
for f (W x(i) ):
•
f (x) = x (Linear RICA)
•
f (x) = 0.4x + 0.6 tanh(x) (Mixed NLE-RICA I)
•
f (x) = 0.8x + 0.2 tanh(x) (Mixed NLE-RICA II)
•
f (x) = tanh(x) (Pure NLE-RICA)
The reason for including tanh(x) in the NLE-RICA is its
property of mimicking the shape of sigmoid functions.
Thus, while making a transition from Linear RICA model

TABLE II
TH E FO LLOW ING A LGOR I THM I S U S ED FOR NLE -R ICA :

1.Initialize λ = {0.1, 0.9} and γ ∈ {0, 0.4, 0.8}
2.Using back propagation algorithm, calculate {a(2) , a(3) , δ (1) , δ (2) , δ (3) }
3.Calculate {∇W H (W ), ∇W T H (W ), ∇W G(W )}
m (∇W H (W ) + ∇W T H (W )) + ∇W G(W )} and J (W )
4.Calculate ∇W J (W ) = λ
5.Use the unconstrained optimizer L-BFGS to ﬁnd W ∗ = argminW J (W )
6.Use f (W X ), X ={training data, test data} to ﬁnd the training & test features.
7.Calculate the test and training error using softmax regression.

V I . R E SU LT S

A. Effect of λ
λ seems to play an important role in the calculate of cost
for both linear RICA and NLE-RICA. Very small values of
λ (like those in auto-encoders) provide inadequate weight
to the error term of H (W ). For our study, we chose λ =
{0.1, 0.9}. The results for the two values are summarized in
the graph below.(Figure 5)
It is clear from the graph that we cannot treat a value of
λ ∈ [0.1, 1] to be superior than the other. But a small value
of λ can clearly be ignored.

B. Effect of number of hidden units
Choosing an optimal number of hidden units (k) is speciﬁ-
cally important for the RICA algorithms to work well. A low
number of hidden units make poor prediction giving high
test error. Whereas, high number of units leads to may be
an indication of over-ﬁtting (making 0% training error). We
run our algorithms for k = {200, 400, 700, 800, 900} and get
the following results(Figure 6):

C. Change to L2 norm for reconstruction penalty function
Instead of g(.) = log(cosh(.)) (the usual L1 norm), we now
take g(.) = (cid:107).(cid:107)2 (L2 norm) as the reconstruction penalty. We
apply this reconstruction on the four RICA functions and
obtain the following errors(k=900, λ =0.9) (Table III):
As we can see, the L2 penalty also provides good test and
training accuracy. But, the tradeoff present is slower running
time.

Fig. 4. Graph for tanh(x) and 0.1x + 0.9tanh(x)

to Pure NLE-RICA model we incorporate some linearity
as well. These models are referred to as mixed NLE-RICA
I and Mixed NLE-RICA II models. Hence, NLE-RICA is
deﬁned as the following optimization problem:
(cid:105)
(cid:104) λ
g( f (W j x(i) ))
m
(16)

(cid:107)W T f (W x(i)) ) − x(i)(cid:107)2
2 +

k
∑
j=1

m
∑
i=1

m
∑
i=1

minW

in NLE-RICA,
We observe that
the encoding is done
through f (W x(i) ), but
the decoding is done through
W T f (W x(i) ). This is different from sparse auto-encoder
where you had sigmoid function being used for encoding
(from input layer to hidden layer) and decoding (from hidden
layer to output layer). This also differs from RICA in the
sense that the encoding is carried out by f (W x(i) ) instead of
(W x(i) ). Also, observe the change in the L1 penalty term in
the optimization problem. Let’s deﬁne the conventions before
(cid:105)
(cid:104) λ
discussing the algorithm.
g( f (W j x(i) ))
m

(cid:107)W T f (W x(i)) ) − x(i)(cid:107)2
2 +

J (W ) =

m
∑
i=1

k
∑
j=1

m
∑
i=1

J (W ) =

λ
m

H (W ) + G(W )

(17)

Fig. 5. Training(Top) and Test Accuracy(Bottom) for different values of
λ , k=900, 500 iterations

Fig. 6. Training(Top) and Test Accuracy(Bottom) for different number of
hidden units (λ =0.9, 500 iterations)

TABLE III
T E S T AND TRA IN ING ERROR FOR L2 R ECON S TRUC T ION P ENA LTY

Model
Linear RICA
Mixel NLE-RICA I
Mixed NLE-RICA II
Pure NLE-RICA

Training Accuracy
94.79%
99.92%
99.73%
100%

Test Accuracy
83.13%
83.3%
84.21%
90.95%

D. Effect of γ

We check the test accuracy by varying the parameter
f (.) = γ (.) + (1 − γ ) tanh(.).
γ of our non-linear model
Here we use ∇W G(W ) = 2 ∗ f (W X ). ∗ (γ + (1 − γ )(1 −
(tanh(W X ))2 ))X T in the NLE-RICA algorithm.

As is evident
the test accuracy is
from Figure 7,
decreasing as we move from γ = 0 (Pure NLE-RICA)
to γ = 1 (Linear RICA). Thus, NLE-RICA can be more
accurate than Linear RICA.

Fig. 7. Test Accuracy for different γ (λ =0.9,k=900,50 iterations)

V I I . IM P LEM EN TAT ION

In the project, we run our algorithms on 30,000 train-
ing & 10,000 test samples from the MNIST dataset. The
simulations were effected in the Stanford Corn computing
environment with 8-core 2.7 GHz AMD Opteron (2384)
processor, 32GB RAM, 10GB swap & 75 GB temp Disk,
running Ubuntu GNU/Linux Operating System [4].

V I I I . CONCLU S ION S AND FU TUR E WORK

The class of RICA algorithms, both linear and non-
linear, provide a worthy alternative to Sparse Autoencoders,
especially for scenarios where it becomes important to allow
for overcomplete features. They compete well on accuracy of
classiﬁcation and consume much less computational effort.
They are easy to code, seem quite robust in obtaining a
consistent minima, and give very similar in-sampe and out-
of-sample performance.
The non-linear extension seems a candidate worthy of
exploring further - it beats the linear counterpart consistently.
Future research should be directed towards experimenting
with other functions, and also with deriving a more sound
theoretical basis for their better performance.
Amongst the drawbacks, we found that the values of the
hyperparameters (λ and the number of hidden units) in RICA
play a signiﬁcant role in obtaining a reasonable solution.
Investing time in developing a rule of thumb to guide a ﬁrst
time user would be very useful to more widespread use of
this method.
Time permitting, we would have liked to test our results
on other standard databases for our non-linear extension.
Also, it was hard to compare the performance of Sparse
Autoencoders with that of RICA (and its variants). As yet
another step, we are quite curious and look forward to
developing a theoretical framework to provide rules of thumb
to guide the trade-offs between number of hidden units, linear
or non-linear features, lambda, number of iterations, and
lastly, the number of layers of encoding to obtain a sparse
representation.

IX . ACKNOW L EDG EM EN T S
We would like to acknowledge and thank Mr. Quoc Le,
PhD student at Stanford CS Department, for helping us with
the project and providing us the MNIST dataset.

R E FER ENC E S
[1] Q.V. Le, A. Karpenko, J. Ngiam, A.Y. Ng. (2011)“ICA with Recon-
stuction Cost for Efﬁcient Overcomplete Feature Learning,” NIPS.
[2] Q.V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, A.Y. Ng.(2011)
”On optimization methods for deep learning”, ICML
[3] Ng, A.,”CS294A Lecture Notes - Stanford University” .
[4] Unix Computing Environments, Stanford University.


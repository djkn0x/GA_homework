Algorithms and hardness results
for parallel large-margin learning

Phil Long (Google) and Rocco Servedio (Columbia)

Setting:

• Target: γ -separated halfspace

• Domain: unit ball in Rn

+

+

+

+

_

_

_

_

_

_

_

+

+

+

+

_

• Computational model: PRAM (parallel RAM)

• Question: can output an -accurate hypothesis using
 " time
• poly !log n, log 1
γ , log 1
• poly !n, 1
 " processors?
γ , 1

Poster T068

Positive result

Dependence on 1/ already handled by Boost-by-majority
[Fre95]. Revised goal:

• poly !log n, log 1
γ " time
γ " processors.
• poly !n, 1

Algorithm Number of processors

Perceptron

SmoothBoost

LP

poly(n, 1/γ )

poly(n, 1/γ )

1

This paper

poly(n, 1/γ )

Running time
˜O (1/γ 2 )(log n)
˜O (1/γ 2 )(log n)

poly(n, log(1/γ ))
˜O (1/γ ) + O (log n)

Poster T068

Algorithm

• Parallel boost-by-majority to handle -dependence [Fre95]

• Weak learner :

• Random projection [JL84,AV99]

• Interior point method [Ren88]

• Inver t Hessian using parallel matrix inversion [Rei95]

• Round intermediate solutions (preserve margin)

Poster T068

Negative result

.
.
.

.
.
.

• Some boosters [KM95,MM00,KS02,LS05,LS08] use
decision trees or branching programs.

• Calls to weak learners from the same “layer” parallelizable.

• Q: Can save iterations?

• A: No (so Ω(1/γ 2 ) iterations needed)

• Proof sketch:

• variables conditionally independent given label

• in stage i , give variable i to all weak learners.

Poster T068

Non-Asymptotic Analysis of Stochastic
Approximation Algorithms
for Machine Learning

Francis Bach
INRIA - ENS

Eric Moulines
Telecom ParisTech

Stochastic approximation

• Context: Large-scale learning (“large p, large n, large k”)

• Goal: Minimizing a function f deﬁned on a Hilbert space H

n(θn) of its gradients f !(θn) at
– given only unbiased estimates f !
certain points θn ∈ H

• Stochastic approximation

– Observation of f !
n(θn) = f !(θn) + εn
– εn = additive noise (typically i.i.d.)

• Machine learning - statistics

– fn(θ) = #(θ , zn) where zn is an i.i.d. sequence
– f (θ) = Efn(θ) = generalization error of predictor θ
– Typically fn(θ) = 1
2 ("xn, θ# − yn)2 or log[1 + exp(−yn "xn, θ#)]

Convex stochastic approximation

• Key properties of f and/or fn

– Smoothness: f B -Lipschitz continuous, f ! L-Lipschitz continuous
– Strong convexity: f µ-strongly convex

• Key algorithm: Stochastic gradient descent (a.k.a. Robbins-Monro)

θn = θn−1 − γnf !
n(θn−1)
– Polyak-Ruppert averaging: ¯θn = 1
n !n−1
k=0 θk
– Which learning rate sequence γn? Classical setting: γn = C n−α

• Desirable practical behavior

– Applicable (at least) to least-squares and logistic regression
– Robustness to (potentially unknown) constants (L,B ,µ)
– Adaptivity to diﬃculty of the problem (e.g., strong convexity)

Summary of new results

• Stochastic gradient descent with learning rate γn = C n−α

• Strongly convex smooth objective functions

– Old: O(n−1) rate achieved without averaging for α = 1
– New: O(n−1) rate achieved with averaging for α ∈ [1/2, 1]
– Non-asymptotic analysis with explicit constants

• Non-strongly convex smooth objective functions
– Old: O(n−1/2) rate achieved with averaging for α = 1/2
– New: O(max{n1/2−3α/2 , n−α/2 , nα−1}) rate achieved without
averaging for α ∈ [1/3, 1],

• Take-home message

– Use α = 1/2 with averaging to be adaptive to strong convexity

Online Submodular Set Cover,
Ranking, and Repeated Active Learning
Online Ranking: At each round, the learner produces an ordered list of
items, then suﬀers loss or receives reward. Example: search result ranking

Get  search query 

Display ranked results 

In this paper, loss is:
the number of items needed to achieve some coverage objective
Example: The cost at each round is the number of pages the user needs to
view to deduce the complete information they desire.

Guillory, Bilmes (U. Washington)

1 / 4

Online Submodular Set Cover,
Ranking, and Repeated Active Learning

Repeated Active Learning is an interesting special case where the list
consists of questions to ask or tests to perform. Example: diagnosis

Visited by patient 

Perform series of tests 

Here a reasonable loss is the number of the tests we need to perform
before we can make a accurate diagnosis.

Guillory, Bilmes (U. Washington)

2 / 4

Online Submodular Set Cover,
Ranking, and Repeated Active Learning

For these applications we propose a new online learning problem we call
online submodular set cover.
At round t we pick a sequence S t = v1 , v2 , . . . vn .
A monotone, submodular objective F t is then revealed.
We pay cost equal to the cover time of F t : the minimum value
c ∈ {1, 2, . . . n} such that F t (!c
i =1{vi }) ≥ 1.
Example: F t (S ) is proportional to the number of candidate diseases
eliminated by the set of tests S .

Related but not equivalent to online submodular maximization and online
min-sum submodular set cover (Streeter and Golovin, 2008)

Guillory, Bilmes (U. Washington)

3 / 4

Online Submodular Set Cover,
Ranking, and Repeated Active Learning
Our results:
A low-regret algorithm for online submodular set cover, building on a
recent oﬄine algorithm of Azar and Gamzu.
Extensions to handle multiple objectives, partial information, context.
Experimental results on synthetic data and a movie recommendation
repeated active learning problem.

Guillory, Bilmes (U. Washington)

4 / 4

ABACDEAF
AACDA
AA
ABBCDEC

Sparse estimation problem:
min
min
x

x
x

0

s.t.
s.t.

y
y

= Φ
= Φ

x
x

overcomplete dictionary 
of  basis vectors

 Non-convex, combinatorial problem in general.

 Convex relaxation using the 1 norm produces an 
equivalent solution if Φ is sufficiently unstructured.

AACDCACD
Structured
Unstructured
Φ ΦT
Φ ΦT

Examples:

  iid 
)   
Φ


Φ

unstr

(

N

(0,1) entries

Φ

(

str

)

  

=

  W

⋅ Φ

(

unstr

)

⋅

D

Example:

)   

(

unstr

  random rows of DFT

arbitrary

block
diagonal

E

♦ Apply a Φ-dependent projection that maps x to a new 
space

 
 PΦ=z

( )
x

♦ Use a standard sparsity penalty g in this new space and 
solve:
solve:
= Φ
= Φ
x
min
i

x
( )
PΦ
x

s.t.

=

(
g z

i

)

y

z

The projection operator:
1. Must compensate for dictionary structure. 

2. Preserve sparsity, meaning if z is maximally 
sparse, x is also maximally sparse.

DA

 Convenient optimization via 
reweighted 1 minimization

 Provable performance 
improvement in certain situations

 Toy Example:
 Generate 50-by-100 dictionaries:


N(0,1),   

Φ = Φ
( str )

⋅

D

Φ

( unstr )

( unstr )

e
t
a
r
 
s
s
e
e
c
c
c
u
s

 Generate a sparse x

 Estimate x from observations 

y

( unstr )

= Φ

( unstr )

⋅

  ,      

y

x

= Φ ⋅
( str )

x

( str )

proposed, Φ(unstr)
proposed, Φ(str)
standard, Φ(unstr)
standard, Φ(str)

x

0

Universal low-rank matrix recovery 
from Pauli measurements

Yi-Kai Liu
Applied and Computational Mathematics Division
US National Institute of Standards and Technology

Motivation: experiments with 
complex quantum systems

(cid:132)

(cid:132)

(cid:132)

(cid:132)

(cid:132)

Ion traps
 

Small quantum 
computers
 
Precision metrology
Simulating chemical 
 
dynamics
 
Want to scale up: 
10 to 100 qubits
 

Quantum state tomography

(cid:132)

(cid:132)

(cid:132)

(cid:132)

Characterizing an unknown quantum state: 
want to learn the density matrix
(cid:463)
in Cdxd
 
 
 
For a state of n qubits, d = 2n
=> pretty big!
(cid:132)
 
In many cases, (cid:463)
has rank r << d
 
 
 
, P2
Choose measurement matrices P1
, …
(cid:463)), Tr(P2
Observe Tr(P1
(cid:463)), …
 
 
 
matrix analogue of Fourier basis
Use Pauli matrices –
 
 
 
(cid:132)
Use  com pressed sensing t echn iques!
 
 
 

(cid:132)

(cid:132)

(cid:132)

Our results
There is a un iversa l set of O(rd
log6d) Pauli 
 
measurements, that can be used to reconstruct 
 
in Cdxd
any rank-r state (cid:463)
 
Choose random Pauli matrices, use the matrix Lasso
(cid:132)
Get strong (near-optimal) error bounds [CP’11]
 
 
Random Pauli measurements obey the 
rest r ict ed isom e t ry proper t y ( RI P)
 
Embed the manifold of low-rank matrices into 
(cid:132)
log6d) dimensions
O(rd
 
 
More structured, less random than a Gaussian 
random projection
 

(cid:132)

(cid:132)











 EB EBBEBC
BB EB



 
 ECBE AA
CE FEFFCE FEFFC



  ABCDEC FCABCD EFABEC
 BECFAA BEC

 A F CCE 
EADBBDBDBDADEC
FABC FDBBABEA



F

B

 C CEEF
FCC

EF

EAD AC
























 



































 AE
BB EB



 
  AEA AD BF BABB FA D …B
EBECEAE  BAFEC F
 BCC F
AFE CEC

 BB EBAEBEF EC 
A ABA  E BB  BCA E 

 B  AEEC AB BB AAB A 
E  AAB F BB F A  
EBF  DAAB AD F






























E



CE
B












CCAEE

AEE AE FAEC
 



CE F  A AE  

AEECA F CEC EBF FD BD
A AB AB 

AE

 AE  ADAABEC
 EB AE A AAB ABABBD 
 A DAAB  F  EC

AAE  EC

AE CEC B EAC  A A  F
ABB  AB BED D

E ACE



 










B

  CCAE CE F


 E CE


  C E AE

  C AE  FFECEC

  C EF E ECC AFE

On U-processes and Clustering Performance

St´ephan Cl´emen¸con
LTCI UMR Telecom ParisTech/CNRS No. 5141 - Institut Telecom

Motivation

Pairwise dissimilarity-based clustering techniques are widely used to
segment a dataset into groups, such that data points in the same group
are more similar to each other than to those in other groups. The
empirical criteria these algorithms seek to optimize are of the form of
U -statistics of degree two. We propose to analyze their performance, using
recent advances in the theory of U-processes. The statistical framework
considered permits to establish learning rates for the excess of
clustering risk and to design model selection tools as well.

St´ephan Cl´emen¸con (LTCI)

On U-processes and Clustering Performance

December 2011

1 / 4

Statistical Framework

We observe an i.i.d. sample Dn = {(Xi )i ≤n } of n ≥ 1 observations in a
space X , drawn from a probability distribution µ(dx ). Here, we assume
that the space X is equipped with a dissimilarity measure D : X 2 → R+
that fulﬁlls the properties:
(Symmetry) D (x , x " ) = D (x " , x ),
(Separation) D (x , x " ) = 0 ⇔ x = x " .
The task is to partition the space X in a ﬁnite number of groups, K ≥ 1
say, so as to minimize the quantity (i.e. the within cluster point scatter):
K"
"
2
!Wn (P ) =
n(n − 1)
1≤i <j ≤n
k =1
over all possible partitions P = {C1 , . . . , CK }. The clustering risk is:
K"
E #D (X , X " ) · I{(X , X " ) ∈ C 2
k }$ .
W (P ) =
k =1
Optimal partitions are those that minimize W (P ).
St´ephan Cl´emen¸con (LTCI)
On U-processes and Clustering Performance

D (Xi , Xj ) · I{(Xi , Xj ) ∈ C 2
k },

December 2011

2 / 4

Generalization Ability

Pairwise-based clustering can be cast in terms of minimization of a
U -statistic over a class Π of partition candidates.
Analysis of the Empirical Clustering Risk Minimizers requires to study the
ﬂuctuations of the U-process %!Wn (P ) − W (P ) : P ∈ Π&. Key
ingredients:
Complexity assumption
’’’’’’
#n/2$"
1
&n/2’
i =1
Projection techniques
U -process = Empirical Process + O (1/n)

i D (Xi , Xi +#n/2$ ) · I{(Xi , Xi +#n/2$ ) ∈ C 2}

sup
C , P

,

’’’’’’

St´ephan Cl´emen¸con (LTCI)

On U-processes and Clustering Performance

December 2011

3 / 4

Results and Applications

Learning rates of the order O (1/√n)

Tight probability bounds for the excess of clustering risk

Fast rates

Model selection tools, computing additive complexity penalization
terms:

! Automatic selection of the geometry of the cells
! Choosing the number of cells

St´ephan Cl´emen¸con (LTCI)

On U-processes and Clustering Performance

December 2011

4 / 4


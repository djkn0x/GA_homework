Neuronal Adaptation for Sampling-Based
Probabilistic Inference in Perceptual Bistability

David Reichert
Peggy Seri`es, Amos Storkey

University of Edinburgh
Funding: EPSRC, MRC, BBSRC

NIPS 2011 – T018

David Reichert, University of Edinburgh

NIPS 2011 – T018, Adaptation and sampling in perceptual bistability

Perceptual bistability

Approximative probabilistic inference in the brain?
p(x)

Just neuronal adaptation?

Sampling? Sundareswara and Schrater (2006).

MCMC? Gershman et al. (2009).

David Reichert, University of Edinburgh

NIPS 2011 – T018, Adaptation and sampling in perceptual bistability

Deep Boltzmann machine model

0

20 40 60 80 100 120 140
time

input

David Reichert, University of Edinburgh

NIPS 2011 – T018, Adaptation and sampling in perceptual bistability

Adaptation and sampling

Sampling algorithm: Rates-FPCD (Breuleux et al., 2011)

Biological interpretation: neuronal adaptation!

⇒ Neuronal Adaptation for Sampling-Based
Probabilistic Inference in Perceptual Bistability

We examine:

Temporal dynamics

Necker cube & binocular rivalry

Spatial attention

Bonus: explicit depth modelling

Full story & details: T018!

David Reichert, University of Edinburgh

NIPS 2011 – T018, Adaptation and sampling in perceptual bistability

Sequence learning with hidden units
in spiking neural networks

Johanni Brea, Walter Senn and Jean-Pascal P ﬁster

Sequence Learning with Hidden Units in Spiking Neural Networks (Poster T019)

1 / 4

Task: Learning a distribution of sequences
spikes

neural network
visible
neurons
hidden
neurons

n
o
r
u
e
n

v

h

time

target
P ∗ (v )

learning
w

y
c
n
e
u
q
e
r
f

w : model parameter

model
Pw (v ) = Ph Pw (v , h)

spatio-temporal visible pattern v

Sequence Learning with Hidden Units in Spiking Neural Networks (Poster T019)

2 / 4

Model


s
t
i
n
u
y
r
a
r
t
i
b
r
a

w
D

0

stochastic spike response model

model distribution: Pw (v )

stochastic gradient descent on KL(P ∗ (v )||Pw (v ))

∆w ∝ global factor × pre × post

-40 -20
20
0
40
t p o st -t p re m s
consistent with STDP

EM with impor tance sampling

Sequence Learning with Hidden Units in Spiking Neural Networks (Poster T019)

3 / 4

Results
target

z
simple Hebb

overlay of 20 recalls
}|
only visible

only readout

{
our rule

20
t
i
n
u

10 20 30
time

10 20 30
10 20 30
visible units
p
p

hidden units

10 20 30

10 20 30
p

Sequence Learning with Hidden Units in Spiking Neural Networks (Poster T019)

4 / 4

Information Rates and Optimal

Decoding in Large Neural Populations

Kamiar Rahnama Rad and Liam Paninski

Presented by David Pfau

Stimulus

Spikes

Bayesian Estimate

8

6

4

2

0

−2

−4

−6

0
1
 
=
 
N

8

6

4

2

0

−2

−4

−6

0

0.05

0.1
Time(sec)

0.15

0.2

0

0.05

0.1
Time(sec)

0.15

0.2

0.05

0.1
Time(sec)

0.15

0.2

Biologically-relevant Intermediate signal-to-noise

ratio: in spite of the large number of neurons, the

total amount of information provided by the

population remains ﬁnite.

s
u
l
u
m
i
t
S
 
o
t
 
y
t
i
v
i
t
i
s
n
e
S

High SNR

Intermediate
      SNR

Low SNR

Number of Neurons

• Single neurons convey small amounts of

information about the stimulus.

• Neural populations convey a ﬁnite amount of

information.

• In the limit of large population size, the

suﬃcient statistics have a linearly-weighted

form and a Gaussian distribution, even in

many nonlinear, coupled spiking networks.

• This makes it easy to compute optimal

decoders and information rates.

Stimulus

Spikes

Sufficient Statistics 

5

0

−5

5

0

−5

0

0.05

0.1

0.15

0.2

0

0.05

0.1

0.15

0.2

5

0

−5

0

1
 
=
 
N

5
 
=
 
N

0
2
 
=
 
N

3

2

1

0

−1

1

0.5

0

−0.5

0.6

0.4

0.2

0

Bayesian
Gaussian Decoder
 

5

0

−5

 

5

0

−5

5

0

−5

0.05
0.15
0.1
Time(sec)

0.2

0

0.05
0.15
0.1
Time(sec)

0.2

−0.2

0

0.05
0.15
0.1
Time(sec)

0.2

0.05
0.15
0.1
Time(sec)

0.2

The optimal Bayesian decoder is well-approximated by a linear function of

the suﬃcient statistics when n is even moderately large.

#(cid:8)$(cid:16)(cid:4)(cid:6)B(cid:8)BF(cid:19)(cid:20)(cid:6)%(cid:20)(cid:16)(cid:22)(cid:5)(cid:4)(cid:20)(cid:6)(cid:8)CF(cid:5)(cid:2)(cid:20)B(cid:8)(cid:25)(cid:20)(cid:18)(cid:8)
(cid:6)F(cid:22)(cid:18)(cid:3)(cid:16)(cid:8)(cid:24)(cid:29)(cid:4)AF(cid:8)(cid:4)BF(cid:6)(cid:5)(cid:4)(cid:25)(cid:4)(cid:19)(cid:3)(cid:5)(cid:4)(cid:20)(cid:6)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:3)(cid:6)(cid:7)(cid:3)(cid:8)(cid:9)A(cid:3)(cid:6)(cid:3)B(cid:2)(cid:3)CD(cid:8)E(cid:3)(cid:6)(cid:4)F(cid:16)(cid:8)(cid:17)(cid:18)(cid:3)(cid:6)(cid:19)(cid:2)(cid:4)(cid:6)(cid:3)D(cid:8)(cid:9)F(cid:18)(cid:20)(cid:8)(cid:21)(cid:4)C(cid:20)(cid:6)(cid:19)F(cid:16)(cid:16)(cid:4)
(cid:1)(cid:20)(cid:22)(cid:18)(cid:3)(cid:6)(cid:5)(cid:8)(cid:23)(cid:6)(cid:24)(cid:5)(cid:4)(cid:5)(cid:22)(cid:5)F(cid:8)(cid:20)(cid:25)(cid:8)C(cid:3)(cid:5)(cid:2)FC(cid:3)(cid:5)(cid:4)(cid:19)(cid:3)(cid:16)(cid:8)(cid:24)(cid:19)(cid:4)F(cid:6)(cid:19)F(cid:24)D(cid:8)(cid:26)(cid:27)(cid:28)
(cid:21)(cid:29)(cid:20)(cid:5)(cid:16)(cid:4)(cid:30)(cid:2)(cid:5)(cid:8)(cid:29)(cid:18)F(cid:24)F(cid:6)(cid:5)(cid:3)(cid:5)(cid:4)(cid:20)(cid:6)D(cid:8)(cid:26)(cid:23)(cid:31)(cid:21)(cid:8) !""

(cid:1)

(cid:1)

(cid:20)$(cid:24)F(cid:18)%FB(cid:8)(cid:24)(cid:4)(cid:30)(cid:6)(cid:3)(cid:16)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:5)(cid:8)(cid:6)(cid:7)(cid:8)(cid:5)(cid:3)(cid:4)(cid:8)(cid:8)(cid:19)(cid:20)C$(cid:4)(cid:6)(cid:3)(cid:5)(cid:4)(cid:20)(cid:6)(cid:8)(cid:20)(cid:25)(cid:8)(cid:9)(cid:7)A(cid:5)B(cid:1)C(cid:7)D(cid:9)(cid:5)EF(cid:16)(cid:3)(cid:17)(cid:5)D(cid:18)(cid:4)A(cid:1)

(cid:18)(cid:3)B(cid:3)(cid:18)D(cid:8)(cid:24)F(cid:4)(cid:24)C(cid:4)(cid:19)(cid:8)(cid:24)(cid:4)(cid:30)(cid:6)(cid:3)(cid:16)(cid:24)D(cid:8)(cid:24)(cid:20)(cid:22)(cid:6)B(cid:24)D(cid:8)(cid:6)F(cid:22)(cid:18)(cid:3)(cid:16)(cid:8)(cid:24)(cid:29)(cid:4)AF(cid:24)

F(cid:16)F(cid:19)(cid:5)(cid:18)(cid:20)BF

%(cid:20)(cid:16)(cid:5)(cid:3)(cid:30)F(cid:8)(cid:5)(cid:18)(cid:3)(cid:19)F

(cid:19)F(cid:16)(cid:16)(cid:8)"

(cid:19)F(cid:16)(cid:16)(cid:8)(

(cid:1)

(cid:19)F(cid:16)(cid:16)(cid:8) 

(cid:1)

(cid:18)F(cid:19)(cid:20)%F(cid:18)(cid:8)(cid:5)(cid:4)CF(cid:24)(cid:8)(cid:3)(cid:6)B(cid:8)
&(cid:3)%F(cid:25)(cid:20)(cid:18)C(cid:24)(cid:8)(cid:20)(cid:25)(cid:8)(cid:24)(cid:29)(cid:4)AF(cid:24)(cid:8)
(cid:25)(cid:20)(cid:18)(cid:8)F(cid:3)(cid:19)(cid:2)(cid:8)(cid:19)F(cid:16)(cid:16)’

)$*F(cid:19)(cid:5)(cid:4)%F+(cid:8)C(cid:4)(cid:6)(cid:4)C(cid:4),F(cid:8)-(cid:18)F(cid:19)(cid:20)(cid:6)(cid:24)(cid:5)(cid:18)(cid:22)(cid:19)(cid:5)(cid:4)(cid:20)(cid:6)(cid:8)F(cid:18)(cid:18)(cid:20)(cid:18).(cid:8)/(cid:8)-(cid:24)(cid:29)(cid:3)(cid:18)(cid:24)(cid:4)(cid:5)(cid:7).

(cid:24)(cid:20)(cid:16)%F(cid:8)(cid:24)(cid:29)(cid:4)AF(cid:24)

(cid:24)(cid:20)(cid:16)%F(cid:8)&(cid:3)%F(cid:25)(cid:20)(cid:18)C(cid:24)

C(cid:3)(cid:6)(cid:4)(cid:25)(cid:20)(cid:16)B(cid:8)(cid:20)(cid:25)
(cid:5)(cid:4)CF0(cid:24)(cid:2)(cid:4)(cid:25)(cid:5)FB(cid:8)
&(cid:3)%F(cid:25)(cid:20)(cid:18)C(cid:24)

(cid:28)(cid:24)F(cid:8)$(cid:3)(cid:24)(cid:4)(cid:24)(cid:8)(cid:25)(cid:22)(cid:6)(cid:19)(cid:5)(cid:4)(cid:20)(cid:6)(cid:24)(cid:8)
&(cid:2)(cid:4)(cid:19)(cid:2)(cid:8)(cid:4)(cid:6)(cid:5)F(cid:18)(cid:29)(cid:20)(cid:16)(cid:3)(cid:5)F(cid:8)
(cid:19)(cid:20)(cid:6)(cid:5)(cid:4)(cid:6)(cid:22)(cid:20)(cid:22)(cid:24)(cid:8)(cid:5)(cid:4)CF0(cid:24)(cid:2)(cid:4)(cid:25)(cid:5)(cid:24)

(cid:21)(cid:20)(cid:16)%F(cid:8)$(cid:3)(cid:24)(cid:4)(cid:24)(cid:8)
(cid:19)(cid:20)F(cid:25)(cid:25)(cid:4)(cid:19)(cid:4)F(cid:6)(cid:5)(cid:24)(cid:8)(cid:24)(cid:22)$*F(cid:19)(cid:5)
(cid:5)(cid:20)(cid:8)(cid:19)(cid:20)(cid:6)(cid:24)(cid:5)(cid:18)(cid:3)(cid:4)(cid:6)(cid:5)(cid:24)

(cid:1)

(cid:1)

1(cid:3)%F(cid:25)(cid:20)(cid:18)C(cid:24)(cid:8)(cid:16)F(cid:3)(cid:18)(cid:6)FB(cid:8)(cid:25)(cid:18)(cid:20)C(cid:8)
(cid:18)F(cid:3)(cid:16)(cid:8)B(cid:3)(cid:5)(cid:3)(cid:8)-2(cid:3)(cid:18)(cid:18)(cid:4)(cid:24)(cid:8) !!!.

(cid:19)(cid:16)(cid:22)(cid:24)(cid:5)F(cid:18)(cid:4)(cid:6)(cid:30)

(cid:20)(cid:22)(cid:18)(cid:8)CF(cid:5)(cid:2)(cid:20)B

(cid:1)

(cid:1)

ACCELERATED ADAPTIVE

MARKOV CHAIN FOR PARTITION

FUNCTION COMPUTATION

Stefano Ermon*, Carla Gomes*, Ashish Sabharwal+, 
and Bart Selman*

*Department of Computer Science, Cornell University 

+IBM Watson Research Center

Partition Function Computation
• Normalization constant in factored probabilistic models (e.g., MRFs, MLNs 
with soft probabilistic constraints)

Z = Sum over exponentially many configurations 

Hard to compute. Can we approximate it?

•

Flat Histogram method (Wang-Landau)

E=0

E=2

E=1

Energy = 3

Partition of the set of all possible 
configurations (according to energy)

Adaptive MCMC will eventually visit all subsets 
(= colors = energy levels) equally often (Contrast: 
Metropolis/SA, according to Boltzmann weight)

Step 400

120

100

80

60

40

20

0

Red 
(E=0)

Yellow 
(E=1)

Blue 
(E=2)

Green 
(E=3)

Estimates the size of the subsets (density of states), 
which also gives the partition function Z

Our Contributions

1a. Energy Saturation: single bucket for high energy states
(cid:857)

K K

>K

• Fewer buckets            faster
• (Tight) Upper bound on Z 

1

2

3

Increasing energy

1b. Focused moves: variables occurring in violated constraints are flipped 
more frequently (preserving detailed balance)

Greatly reduced 
number of null 
moves (purple)

2. New application: density of states gives parameterized partition function

e.g. at all temperatures, all weights of the soft constraints            learning

Experimental Results

•

Improved Scaling

Focused Moves + Saturation
outperforms standard flat histogram

• Better Accuracy (vs. Gibbs Sampling, TRW, IJGP)

• Hard Constraints (model counting)
• Soft Constraints
• Hard & Soft constraints

•

Improved Weight Learning 

Close to optimal likelihoods for the trained weights in synthetic Markov Logic 
Networks

The Kernel Beta Process

Lu Ren, Ying jian Wang, David Dunson, and Lawrence Carin

Presented by: David Carlson
Duke University

david.carlson@duke.edu

(ECE Dept. Duke University)

The Kernel Beta Process

1 / 4

The kernel beta process

i , ψ∗
Theorem: Assume parameters {x ∗
i , πi , ωi } are drawn from
measure νX = H (dx ∗ )Q (d ψ∗ )ν (d π , d ω), and that the following
measure is constituted for any covariate x ∈ X :

Bx =

i ; ψ∗
πi K (x , x ∗
i )δωi

∞
Xi =1
For any ﬁnite set of covariates S = {x1 , . . . , x|S |}, deﬁne the random
vector K = (K (x1 , x ∗ ; ψ∗ ), . . . , K (x|S | , x ∗ ; ψ∗ ))T . For ∀A ⊂ F , the
characteristic function for measures at covariates in S satisﬁes
E[e j <u,B(A)> ] = exp{ZX ×Ψ×[0,1]×A
with νX the L´evy measure of the kernel beta process.

(e j <u,Kπ>−1)νX (dx ∗ , d ψ∗ , d π , d ω)}

(ECE Dept. Duke University)

The Kernel Beta Process

2 / 4

Properties of KBP

If B is drawn from KBP, x , x ′ ∈ X , for ∀A ∈ F :

Expectation: E[Bx (A)] = B0 (A)E(Kx )
with E(Kx ) = RX ×Ψ K (x , x ∗ ; ψ∗ )H (dx ∗ )Q (d ψ∗ ).
Covariance: Cov(Bx (A), Bx ′ (A)) =
B0 (d ω)(1−B0 (d ω))
− Cov(Kx , Kx ′ ) RA B 2
E(Kx Kx ′ ) RA
0 (d ω)
c (ω)+1
(If K (x , x ∗ ; ψ∗ ) = 1 for all x ∈ X , E(Kx ) = E(Kx Kx ′ ) = 1, and
Cov(Kx , Kx ′ ) = 0, and the above results reduce to beta process.)
Conditional covariance: With the kernel vectors Kx , Kx ′ ﬁxed, the
conditional covariance is given as:
Corr(Bx (A), Bx ′ (A)) = <Kx ,Kx ′ >
kKx k2 ·kKx ′ k2

(ECE Dept. Duke University)

The Kernel Beta Process

3 / 4

Experiment - music analysis and image denoising

 

 

1

x
e
d
n
i
 
e
c
n
e
u
q
e
S

10

20

30

40

50

60

70

80

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

−0.1

x
e
d
n
i
 
e
c
n
e
u
q
e
S

10

20

30

40

50

60

70

80

0.95

0.9

0.85

0.8

x
e
d
n
i
 
e
c
n
e
u
q
e
s

10

20

30

40

50

60

70

80

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 

10

20

30
40
50
60
Sequence index

70

80

 

10

20

30
40
50
60
Sequence index

70

80

10

20

30

40
50
sequence index

60

70

80

Figure: Music temporal correlation: (a) KBP-FA, (b) BP-FA, (c) dHDP-HMM.

(ECE Dept. Duke University)

The Kernel Beta Process

4 / 4

Figure: Image denoising result

Solving Decision Problems
with Limited Information

Denis D. Mau´a, Cassio P. de Campos
{denis,cassio}@idsia.ch

IDSIA, Switzerland

C1

C2

O1

O2

O8

D1

D2

C8

D8

S1

S2

· · ·

S8

S9

R1

R2

R8

R9

The Language of LIMIDs

◮ Probabilistic Reasoning: Bayesian Networks

◮ Decision Making under Uncertainty:
Limited Memory Inﬂuence Diagrams

◮ Forgetful Agent

◮ Stationary and Non-stationary Policies

◮ Cooperative Agents

◮ Complex Non-Sequential Problems

The Language of LIMIDs

GRID-WORLD

LIMID

G

C1

C2

O1

O2

O8

D1

D2

C8

D8

S1

S2

· · ·

S8

S9

R1

R2

R8

R9

R

tmax ≤ 9

Solving LIMIDs Exactly

e
m
i
t

g
n
i
n
n
u
R

104

102

100

10−2

MPU
MILP

101

1020

1040

1060

Problem size


Robust Multi-Class Gaussian Process Classiﬁcation

Daniel Hern ´andez-Lobato
ICTEAM - Machine Learning Group
Universit ´e catholique de Louvain
Place Sainte Barbe, 2
Louvain-La-Neuve, 1348, Belgium
danielhernandezlobato@gmail.com

Jos ´e Miguel Hern ´andez-Lobato
Department of Engineering
University of Cambridge
Trumpington Street, Cambridge
CB2 1PZ, United Kingdom
jmh233@eng.cam.ac.uk

Pierre Dupont
ICTEAM - Machine Learning Group
Universit ´e catholique de Louvain
Place Sainte Barbe, 2
Louvain-La-Neuve, 1348, Belgium
pierre.dupont@uclouvain.be

Abstract

Multi-class Gaussian Process Classiﬁers (MGPCs) are often affected by over-
ﬁtting problems when labeling errors occur far from the decision boundaries. To
prevent this, we investigate a robust MGPC (RMGPC) which considers labeling
errors independently of their distance to the decision boundaries. Expectation
propagation is used for approximate inference. Experiments with several datasets
in which noise is injected in the labels illustrate the beneﬁts of RMGPC. This
method performs better than other Gaussian process alternatives based on consid-
ering latent Gaussian noise or heavy-tailed processes. When no noise is injected in
the labels, RMGPC still performs equal or better than the other methods. Finally,
we show how RMGPC can be used for successfully identifying data instances
which are difﬁcult to classify correctly in practice.

1

Introduction

Multi-class Gaussian process classiﬁers (MGPCs) are a Bayesian approach to non-parametric multi-
class classiﬁcation with the advantage of producing probabilistic outputs that measure uncertainty
in the predictions [1]. MGPCs assume that there are some latent functions (one per class) whose
value at a certain location is related by some rule to the probability of observing a speciﬁc class
there. The prior for each of these latent functions is speciﬁed to be a Gaussian process. The task of
interest is to make inference about the latent functions using Bayes’ theorem. Nevertheless, exact
Bayesian inference in MGPCs is typically intractable and one has to rely on approximate methods.
Approximate inference can be implemented using Markov-chain Monte Carlo sampling, the Laplace
approximation or expectation propagation [2, 3, 4, 5].
A problem of MGPCs is that, typically, the assumed rule that relates the values of the latent functions
with the different classes does not consider the possibility of observing errors in the labels of the
data, or at most, only considers the possibility of observing errors near the decision boundaries
of the resulting classiﬁer [1]. The consequence is that over-ﬁtting can become a serious problem
when errors far from these boundaries are observed in practice. A notable exception is found in
the binary classiﬁcation case when the labeling rule suggested in [6] is used. Such rule considers
the possibility of observing errors independently of their distance to the decision boundary [7, 8].
However, the generalization of this rule to the multi-class case is difﬁcult. Existing generalizations

1

are in practice simpliﬁed so that the probability of observing errors in the labels is zero [3]. Labeling
errors in the context of MGPCs are often accounted for by considering that the latent functions of the
MGPC are contaminated with additive Gaussian noise [1]. Nevertheless, this approach has again the
disadvantage of considering only errors near the decision boundaries of the resulting classiﬁer and is
expected to lead to over-ﬁtting problems when errors are actually observed far from the boundaries.
Finally, some authors have replaced the underlying Gaussian processes of the MGPC with heavy-
tailed processes [9]. These processes have marginal distributions with heavier tails than those of a
Gaussian distribution and are in consequence expected to be more robust to labeling errors far from
the decision boundaries.
In this paper we investigate a robust MGPC (RMGPC) that addresses labeling errors by introducing
a set of binary latent variables. One latent variable for each data instance. These latent variables
indicate whether the assumed labeling rule is satisﬁed for the associated instances or not. If such
rule is not satisﬁed for a given instance, we consider that the corresponding label has been randomly
selected with uniform probability among the possible classes. This is used as a back-up mechanism
to explain data instances that are highly unlikely to stem from the assumed labeling rule. The
resulting likelihood function depends only on the total number of errors, and not on the distances
of these errors to the decision boundaries. Thus, RMGPC is expected to be fairly robust when
the data contain noise in the labels. In this model, expectation propagation (EP) can be used to
efﬁciently carry out approximate inference [10]. The cost of EP is O(ln3 ), where n is the number
of training instances and l is the number of different classes. RMGPC is evaluated in four datasets
extracted from the UCI repository [11] and from other sources [12]. These experiments show the
beneﬁcial properties of the proposed model in terms of prediction performance. When labeling noise
is introduced in the data, RMGPC outperforms other MGPC approaches based on considering latent
Gaussian noise or heavy-tailed processes. When there is no noise in the data, RMGPC performs
better or equivalent to these alternatives. Extra experiments also illustrate the utility of RMGPC to
identify data instances that are unlikely to stem from the assumed labeling rule.
The organization of the rest of the manuscript is as follows: Section 2 introduces the RMGPC model.
Section 3 describes how expectation propagation can be used for approximate Bayesian inference.
Then, Section 4 evaluates and compares the predictive performance of RMGPC. Finally, Section 5
summarizes the conclusions of the investigation.

2 Robust Multi-Class Gaussian Process Classiﬁcation
Consider n training instances in the form of a collection of feature vectors X = {x1 , . . . , xn} with
associated labels y = {y1 , . . . , yn}, where yi ∈ C = {1, . . . , l} and l is the number of classes. We
follow [3] and assume that, in the noise free scenario, the predictive rule for yi given xi is
yi = arg max
fk (xi ) ,
k
where f1 , . . . , fl are unknown latent functions that have to be estimated. The prediction rule given by
(1) is unlikely to hold always in practice. For this reason, we introduce a set of binary latent variables
z = {z1 , . . . , zn}, one per data instance, to indicate whether (1) is satisﬁed (zi = 0) or not (zi = 1).
In this latter case, the pair (xi , yi ) is considered to be an outlier and, instead of assuming that yi is
generated by (1), we assume that xi is assigned a random class sampled uniformly from C . This is
equivalent to assuming that f1 , . . . , fl have been contaminated with an inﬁnite amount of noise and
serves as a back-up mechanism to explain observations which are highly unlikely to originate from
(1). The likelihood function for f = (f1 (x1 ), f1 (x2 ) . . . , f1 (xn ), f2 (x1 ), f2 (x2 ) . . . , f2 (xn ), . . . ,
1−zi (cid:20) 1
 (cid:89)
(cid:21)zi
fl (x1 ), fl (x2 ), . . . , fl (xn ))T given y, X and z is
n(cid:89)
Θ(fyi (xi ) − fk (xi ))
P (y|X, z, f ) =
l
k (cid:54)=yi
(xi , yi ) is a a mixture of two terms: A ﬁrst term equal to (cid:81)
i=1
where Θ(·) is the Heaviside step function. In (2), the contribution to the likelihood of each instance
Θ(fyi (xi ) − fk (xi )) and a second
k (cid:54)=yi
term equal to 1/l. The mixing coefﬁcient is the prior probability of zi = 1. Note that only the ﬁrst
term actually depends on the accuracy of f . In particular, it takes value 1 when the corresponding
instance is correctly classiﬁed using (1) and 0 otherwise. Thus, the likelihood function described in

(2)

(1)

,

2

(2) considers only the total number of prediction errors made by f and not the distance of these errors
to the decision boundary. The consequence is that (2) is expected to be robust when the observed
data contain labeling errors far from the decision boundaries.
We do not have any preference for a particular instance to be considered an outlier. Thus, z is set to
n(cid:89)
follow a priori a factorizing multivariate Bernoulli distribution:
P (z|ρ) = Bern(z|ρ) =
ρzi (1 − ρ)1−zi ,
i=1
where ρ is the prior fraction of training instances expected to be outliers. The prior for ρ is set to be
a conjugate beta distribution, namely
ρa0−1 (1 − ρ)b0−1
P (ρ) = Beta(ρ|a0 , b0 ) =
(4)
,
B(a0 , b0 )
where B(·, ·) is the beta function and a0 and b0 are free hyper-parameters. The values of a0 and b0
do not have a big impact on the ﬁnal model provided that they are consistent with the prior belief
that most of the observed data are labeled using (1) (b0 > a0 ) and that they are small such that (4) is
not too constraining. We suggest a0 = 1 and b0 = 9.
As in [3], the prior for f1 , . . . , fl is set to be a product of Gaussian processes with means equal to 0
l(cid:89)
and covariance matrices K1 , . . . , Kl , as computed by l covariance functions c1 (·, ·), . . . , cl (·, ·):
P (f ) =
N (fk |0, Kk )
(5)
k=1
where N (·|µ, Σ) denotes a multivariate Gaussian density with mean vector µ and covariance matrix
Σ, f is deﬁned as in (2) and fk = (fk (x1 ), fk (x2 ), . . . , fk (xn ))T , for k = 1, . . . , l.

(3)

2.1

Inference, Prediction and Outlier Identiﬁcation

Given the observed data X and y, we make inference about f , z and ρ using Bayes’ theorem:
P (y|X, z, f )P (z|ρ)P (ρ)P (f )
P (ρ, z, f |y, X) =
(6)
P (y|X)
,
where P (y|X) is the model evidence, a constant useful to perform model comparison under a
(cid:90)
(cid:88)
Bayesian setting [13]. The posterior distribution and the likelihood function can be used to compute
a predictive distribution for the label y(cid:63) ∈ C associated to a new observation x(cid:63) :
P (y(cid:63) |x(cid:63) , y, X) =
P (y(cid:63) |x(cid:63) , z(cid:63) , f(cid:63) )P (z(cid:63) |ρ)P (f(cid:63) |f )P (ρ, z, f |y, X) df df(cid:63) dρ ,
where f(cid:63) = (f1 (x(cid:63) ), . . . , fl (x(cid:63) ))T , P (y(cid:63) |x(cid:63) , z(cid:63) , f(cid:63) ) = (cid:81)
(7)
z ,z(cid:63)
Θ(fk (x(cid:63) ) − fy(cid:63) (x(cid:63) ))1−z(cid:63) (1/l)z(cid:63) ,
k (cid:54)=y(cid:63)
P (z(cid:63) |ρ) = ρz(cid:63) (1 − ρ)1−z(cid:63) and P (f(cid:63) |f ) is a product of l conditional Gaussians with zero mean and
(cid:90)
covariance matrices given by the covariance functions of K1 , . . . , Kl . The posterior for z is
P (ρ, z, f |y, X)df dρ .
P (z|y, X) =
This distribution is useful to compute the posterior probability that the i-th training instance is an
outlier, i.e., P (zi = 1|y, X). For this, we only have to marginalize (8) with respect to all the
components of z except zi . Unfortunately, the exact computation of (6), (7) and P (zi = 1|y, X) is
intractable for typical classiﬁcation problems. Nevertheless, these expressions can be approximated
using expectation propagation [10].

(8)

3 Expectation Propagation
The joint probability of f , z, ρ and y given X can be written as the product of l(n + 1) + 1 factors:
 (cid:34) n(cid:89)
 n(cid:89)
(cid:35)
(cid:34) l(cid:89)
(cid:35)
P (f , z, ρ, y|X) = P (y|X, z, f )P (z|ρ)P (ρ)P (f )
(cid:89)
ψk (f , z, ρ)
ψi (f , z, ρ)
k (cid:54)=yi
i=1
i=1
k=1

ψik (f , z, ρ)

ψρ (f , z, ρ)

=

,

(9)

3

(10)

(11)

˜ψik

ψik

ψi

ψρ

≈

ψk

˜ψk

.

˜ψi

˜ψρ

where each factor has the following form:
ψi (f , z, ρ) = ρzi (1 − ρ)1−zi ,
ψik (f , z, ρ) = Θ(fyi (xi ) − fk (xi ))1−zi (l− 1
l−1 )zi ,
ρa0−1 (1 − ρ)b0−1
ψk (f , z, ρ) = N (fk |0, Kk ) .
ψρ (f , z, ρ) =
,
B(a0 , b0 )
Let Ψ be the set that contains all these exact factors. Expectation propagation (EP) approximates
 (cid:34) n(cid:89)
 n(cid:89)
 n(cid:89)
 (cid:34) n(cid:89)
(cid:35)
(cid:34) l(cid:89)
(cid:35)
(cid:35)
(cid:34) l(cid:89)
(cid:35)
each ψ ∈ Ψ using a corresponding simpler factor ˜ψ such that
(cid:89)
(cid:89)
k (cid:54)=yi
k (cid:54)=yi
i=1
i=1
i=1
i=1
k=1
k=1
In (11) the dependence of the exact and the approximate factors on f , z and ρ has been removed
to improve readability. The approximate factors ˜ψ are constrained to belong to the same family of
exponential distributions, but they do not have to integrate to one. Once normalized with respect to
 (cid:34) n(cid:89)
 n(cid:89)
f , z and ρ, (9) becomes the exact posterior distribution (6). Similarly, the normalized product of the
(cid:35)
(cid:34) l(cid:89)
(cid:35)
(cid:89)
approximate factors becomes an approximation to the posterior distribution:
1
˜ψi (f , z, ρ)
˜ψk (f , z, ρ)
Z
k (cid:54)=yi
i=1
i=1
k=1
where Z is a normalization constant that approximates P (y|X). Exponential distributions are closed
under product and division operations. Therefore, Q has the same form as the approximate factors
and Z can be readily computed. In practice, the form of Q is selected ﬁrst, and the approximate
factors are then constrained to have the same form as Q. For each approximate factor ˜ψ deﬁne
Q\ ˜ψ ∝ Q/ ˜ψ and consider the corresponding exact factor ψ . EP iteratively updates each ˜ψ , one by
one, so that the Kullback-Leibler (KL) divergence between ψQ\ ˜ψ and ˜ψQ\ ˜ψ is minimized. The EP
algorithm involves the following steps:
1. Initialize all the approximate factors ˜ψ and the posterior approximation Q to be uniform.
2. Repeat until Q converges:
(a) Select an approximate factor ˜ψ to reﬁne and compute Q\ ˜ψ ∝ Q/ ˜ψ .
(b) Update the approximate factor ˜ψ so that KL(ψQ\ ˜ψ || ˜ψQ\ ˜ψ ) is minimized.
(c) Update the posterior approximation Q to the normalized version of ˜ψQ\ ˜ψ .
3. Evaluate Z ≈ P (y|X) as the integral of the product of all the approximate factors.

Q(f , z, ρ) =

˜ψik (f , z, ρ)

˜ψρ (f , z, ρ)

,

(12)

The optimization problem in step 2-(b) is convex with a single global optimum. The solution to this
problem is found by matching sufﬁcient statistics between ψQ\ ˜ψ and ˜ψQ\ ˜ψ . EP is not guaranteed
to converge globally but extensive empirical evidence shows that most of the times it converges to
a ﬁxed point [10]. Non-convergence can be prevented by damping the EP updates [14]. Damping
is a standard procedure and consists in setting ˜ψ = [ ˜ψnew ] [ ˜ψold ]1− in step 2-(b), where ˜ψnew is the
updated factor and ˜ψold is the factor before the update.  ∈ [0, 1] is a parameter which controls the
amount of damping. When  = 1, the standard EP update operation is recovered. When  = 0, no
update of the approximate factors occurs. In our experiments  = 0.5 gives good results and EP
seems to always converge to a stationary solution. EP has shown good overall performance when
compared to other methods in the task of classiﬁcation with binary Gaussian processes [15, 16].

3.1 The Posterior Approximation
The posterior distribution (6) is approximated by a distribution Q in the exponential family:
l(cid:89)
N (fk |µk , Σk ) ,
k=1
where N (·|, µ, Σ) is a multivariate Gaussian distribution with mean µ and covariance matrix Σ;
Beta(·|a, b) is a beta distribution with parameters a and b; and Bern(·|p) is a multivariate Bernoulli

Q(f , z, ρ) = Bern(z|p)Beta(ρ|a, b)

(13)

4

˜ψik (f , z, ρ) = ˜sik exp

distribution with parameter vector p. The parameters µk and Σk for k = 1, . . . , l and p, a and b
are estimated by EP. Note that Q factorizes with respect to fk for k = 1, . . . , l. This makes the cost
of the EP algorithm linear in l, the total number of classes. More accurate approximations can be
obtained at a cubic cost in l by considering correlations among the fk . The choice of (13) also makes
all the required computations tractable and provides good results in Section 4.
The approximate factors must have the same functional form as Q but they need not be normalized.
However, the exact factors ψik with i = 1, . . . , n and k (cid:54)= yi , corresponding to the likelihood,
(2), only depend on fk (xi ), fyi (xi ) and zi . Thus, the beta part of the corresponding approximate
factors can be removed and the multivariate Gaussian distributions simplify to univariate Gaussians.
(cid:26)
(cid:20) (fk (xi ) − ˜µik )2
(cid:21)(cid:27)
Speciﬁcally, the approximate factors ˜ψik with i = 1, . . . , n and k (cid:54)= yi are:
(fyi (xi ) − ˜µyi
ik )2
− 1
˜ν yi
2
˜νik
ik
where ˜sik , ˜pik , ˜µik , ˜νik , ˜µyi
ik and ˜ν yi
ik are free parameters to be estimated by EP. Similarly, the exact
factors ψi , with i = 1, . . . , n, corresponding to the prior for the latent variables z, (3), only depend
on ρ and zi . Thus, the Gaussian part of the corresponding approximate factors can be removed and
the multivariate Bernoulli distribution simpliﬁes to a univariate Bernoulli. The resulting factors are:
˜ψi (f , z, ρ) = ˜siρ˜ai−1 (1 − ρ)
i (1 − ˜pi )1−zi ,
˜bi−1 ˜pzi
for i = 1, . . . , n, where ˜si , ˜ai , ˜bi , ˜pi are free parameters to be estimated by EP. The exact factor ψρ
corresponding to the prior for ρ, (4), need not be approximated, i.e., ˜ψρ = ψρ . The same applies to
the exact factors ψk , for k = 1, . . . , l, corresponding to the priors for f1 , . . . , fl , (5). We set ˜ψk = ψk
for k = 1, . . . , l. All these factors ˜ψρ and ˜ψk , for k = 1, . . . , l, need not be reﬁned by EP.

ik (1 − ˜pik )1−zi ,
˜pzi

(14)

(15)

+

3.2 The EP Update Operations
The approximate factors ˜ψik , for i = 1, . . . , n and k (cid:54)= yi , corresponding to the likelihood, are
reﬁned in parallel, as in [17]. This notably simpliﬁes the EP updates. In particular, for each ˜ψik
we compute Q\ ˜ψik as in step 2-(a) of EP. Given each Q\ ˜ψik and the exact factor ψik , we update
each ˜ψik . Then, Qnew is re-computed as the normalized product of all the approximate factors.
Preliminary experiments indicate that parallel and sequential updates converge to the same solution.
The remaining factors, i.e., ˜ψi , for i = 1, . . . , n, are updated sequentially, as in standard EP. Further
details about all these EP updates are found in the supplementary material1 . The cost of EP, assuming
constant iterations until convergence, is O(ln3 ). This is the cost of inverting l matrices of size n × n.

where

3.3 Model Evidence, Prediction and Outlier Identiﬁcation
 ,
 n(cid:88)
 (cid:88)
 + log ˜si
Once EP has converged, we can evaluate the approximation to the model evidence as the integral of
(cid:34) n(cid:88)
(cid:35)
(cid:34) l(cid:88)
(cid:35)
the product of all the approximate terms. This gives the following result:
Ck − log |Mk |
1
+
log Z = B +
log Di
+
log ˜sik
2
k (cid:54)=yi
 , Ck = µT
 + (1 − ˜pi )
 (cid:89)
 (cid:89)
i=1
i=1
k=1
k µk − n(cid:88)
kΣ−1
(cid:40)(cid:80)
Di = ˜pi
k (cid:54)=yi
k (cid:54)=yi
i=1
ik )2/ ˜ν yi
( ˜µyi
if k = yi ,
B = log B(a, b) − log B(a0 , b0 ) ,
k (cid:54)=yi
ii = (cid:80)
ik
otherwise ,
˜µ2
ik / ˜νik
ik )−1 , if yi = k , and
( ˜ν yi
and Mk = ΛkKk + I, with Λk a diagonal matrix deﬁned as Λk
k (cid:54)=yi
ii = ˜ν −1
ik otherwise. It is possible to compute the gradient of log Z with respect to θkj , i.e., the j -th
Λk
1The supplementary material is available online at http://arantxa.ii.uam.es/%7edhernan/RMGPC/.

(1 − ˜pik )

τ k
i =

τ k
i ,

˜pik

(16)

(17)

5

trace

∂ log Z
∂ θkj

hyper-parameter of the k-th covariance function used to compute Kk . Such gradient is useful to ﬁnd
the covariance functions ck (·, ·), with k = 1, . . . , l, that maximize the model evidence. Speciﬁcally,
one can show that, if EP has converged, the gradient of the free parameters of the approximate
(cid:18)
(cid:19)
factors with respect to θkj is zero [18]. Thus, the gradient of log Z with respect to θkj is
= − 1
M−1
(υk )T (M−1
M−1
1
k )T ∂Kk
k Λk ∂Kk
i = (cid:80)
k υk ,
∂ θkj
2
∂ θkj
2
n )T with bk
˜µyi
ik / ˜ν yi
where υk = (bk
ik , if k = yi , and bk
i = ˜µik / ˜νik otherwise.
2 , . . . , bk
1 , bk
k (cid:54)=yi
(cid:18) u − mk√
(cid:19)
(cid:90)
(cid:89)
The predictive distribution (7) can be approximated when the exact posterior is replaced by Q:
+ (1 − ρ)
N (u|my(cid:63) , vy(cid:63) )
P (y(cid:63) |x(cid:63) , y, X) ≈ ρ
Φ
l
vk
k (cid:54)=y(cid:63)
k )T (cid:0)K−1
(cid:1) k(cid:63)
where Φ(·) is the cumulative probability function of a standard Gaussian distribution and
k − (k(cid:63)
k − K−1
k )TK−1
k ΣkK−1
(20)
ρ = a/(a + b) , mk = (k(cid:63)
vk = κ(cid:63)
k Mk υk ,
k ,
k
k equal to the
for k = 1, . . . , l, with k(cid:63)
k equal to the covariances between x(cid:63) and X, and with κ(cid:63)
corresponding variance at x(cid:63) , as computed by ck (·, ·). There is no closed form expression for the
integral in (19). However, it can be easily approximated by a one-dimensional quadrature.
The posterior (8) of z can be similarly approximated by marginalizing Q with respect to ρ and f :
n(cid:89)
(cid:2)pzi
i (1 − pi )1−zi (cid:3) ,
P (z|y, X) ≈ Bern(z|p) =
i=1
where p = (p1 , . . . , pn )T . Each parameter pi of Q, with 1 ≤ i ≤ n, approximates P (zi = 1|y, X),
i.e., the posterior probability that the i-th training instance is an outlier. Thus, these parameters can
be used to identify the data instances that are more likely to be outliers.
The cost of evaluating (16) and (18) is respectively O(ln3 ) and O(n3 ). The cost of evaluating (19)
is O(ln2 ) since K−1
k , with k = 1, . . . , l, needs to be computed only once.

du ,

(19)

+

(18)

(21)

4 Experiments

The proposed Robust Multi-class Gaussian Process Classiﬁer (RMGPC) is compared in several ex-
periments with the Standard Multi-class Gaussian Process Classiﬁer (SMGPC) suggested in [3].
SMGPC is a particular case of RMGPC which is obtained when b0 → ∞. This forces the prior
distribution for ρ, (4), to be a delta centered at the origin, indicating that it is not possible to observe
outliers. SMGPC explains data instances for which (1) is not satisﬁed in practice by considering
Gaussian noise in the estimation of the functions f1 , . . . , fl , which is the typical approach found
in the literature [1]. RMGPC is also compared in these experiments with the Heavy-Tailed Process
Classiﬁer (HTPC) described in [9]. In HTPC, the prior for each latent function fk , with k = 1, . . . , l,
is a Gaussian Process that has been non-linearly transformed to have marginals that follow hyper-
bolic secant distributions with scale parameter bk . The hyperbolic secant distribution has heavier
tails than the Gaussian distribution and is expected to perform better in the presence of outliers.

4.1 Classiﬁcation of Noisy Data

We carry out experiments on four datasets extracted from the UCI repository [11] and from other
sources [12] to evaluate the predictive performance of RMGPC, SMGPC and HTPC when different
fractions of outliers are present in the data2 . These datasets are described in Table 1. All have
multiple classes and a fairly small number n of instances. We have selected problems with small n
because all the methods analyzed scale as O(n3 ). The data for each problem are randomly split 100
times into training and test sets containing respectively 2/3 and 1/3 of the data. Furthermore, the
labels of η ∈ {0%, 5%, 10%, 20%} of the training instances are selected uniformly at random from
C . The data are normalized to have zero mean and unit standard deviation on the training set and
2The R source code of RMGPC is available at http://arantxa.ii.uam.es/%7edhernan/RMGPC/.

6

deﬁned as 1/l (cid:80)l
the average balanced class rate (BCR) of each method on the test set is reported for each value of
η . The BCR of a method with prediction accuracy ak on those instances of class k (k = 1, . . . , l) is
k=1 ak . BCR is preferred to prediction accuracy in datasets with unbalanced class
distributions, which is the case for the datasets displayed in Table 1.

Table 1: Characteristics of the datasets used in the experiments.
# Source
# Classes
# Attributes
# Instances
Dataset
New-thyroid
215
5
3
UCI
UCI
3
13
178
Wine
6
9
214
Glass
UCI
LIBSVM
SVMguide2
319
20
3
(cid:26)
(cid:27)
In our experiments, the different methods analyzed (RMGPC, SMGPC and HTPC) use the same
covariance function for each latent function, i.e., ck (·, ·) = c(·, ·), for k = 1, . . . , l, where
− 1
(xi − xj )T (xi − xj )
2γ
is a standard Gaussian covariance function with length-scale parameter γ . Preliminary experiments
on the datasets analyzed show no signiﬁcant beneﬁt from considering a different covariance function
for each latent function. The diagonal of the covariance matrices Kk , for k = 1, . . . , l, of SMGPC
are also added an extra term equal to ϑ2
k to account for latent Gaussian noise with variance ϑ2
k
around fk [1]. These extra terms are used by SMGPC to explain those instances that are unlikely
to stem from (1). In both RMGPC and SMGPC the parameter γ is found by maximizing (16) using
a standard gradient ascent procedure. The same method is used for tuning the parameters ϑk in
SMGPC. In HTPC an approximation to the model evidence is maximized with respect to γ and the
scale parameters bk , with k = 1, . . . , l, using also gradient ascent [9].

c(xi , xj ) = exp

(22)

New-thyroid
Wine
Glass
SVMguide2

Table 2: Average BCR in % of each method for each problem, as a function of η .
SMGPC
SMGPC
RMGPC
HTPC
RMGPC
HTPC
Dataset
η = 0%
η = 5%
94.2±4.5 93.9±4.4
90.0±5.5 (cid:67) 92.7±4.9 90.7±5.8 (cid:67) 89.7±6.1 (cid:67)
98.0±1.6 98.0±1.6
97.3±2.0 (cid:67) 97.5±1.7 97.3±2.0
96.6±2.2 (cid:67)
65.2±7.7 60.6±8.6 (cid:67) 59.5±8.0 (cid:67) 63.5±8.0 58.9±8.0 (cid:67) 57.9±7.5 (cid:67)
76.3±4.1 74.6±4.2 (cid:67) 72.8±4.1 (cid:67) 75.6±4.3 73.8±4.4 (cid:67) 71.9±4.5 (cid:67)
η = 10%
η = 20%
92.3±5.4 89.0±5.5 (cid:67) 88.3±6.6 (cid:67) 89.5±6.0 85.9±7.4 (cid:67) 85.7±7.7 (cid:67)
97.0±2.2 96.4±2.6
95.6±4.6 (cid:67) 96.6±2.7 95.5±2.6 (cid:67) 95.1±3.0 (cid:67)
63.9±7.9 58.0±7.4 (cid:67) 55.7±7.7 (cid:67) 59.7±8.3 55.5±7.3 (cid:67) 52.8±7.8 (cid:67)
74.9±4.4 72.8±4.7 (cid:67) 71.5±4.7 (cid:67) 72.8±5.1 71.4±5.0 (cid:67) 67.5±5.6 (cid:67)

New-thyroid
Wine
Glass
SVMguide2

Table 2 displays for each problem the average BCR of each method for the different values of η
considered. When the performance of a method is signiﬁcantly different from the performance of
RMGPC, as estimated by a Wilcoxon rank test (p-value < 1%), the corresponding BCR is marked
with the symbol (cid:67). The table shows that, when there is no noise in the labels (i.e., η = 0%), RMGPC
performs similarly to SMGPC in New-Thyroid and Wine, while it outperforms SMGPC in Glass
and SVMguide2. As the level of noise increases, RMGPC is found to outperform SMGPC in all the
problems investigated. HTPC typically performs worse than RMGPC and SMGPC independently of
the value of η . This can be a consequence of HTPC using the Laplace approximation for approximate
inference [9]. In particular, there is evidence indicating that the Laplace approximation performs
worse than EP in the context of Gaussian process classiﬁers [15]. Extra experiments comparing
RMGPC, SMGPC and HTPC under 3 different noise scenarios appear in the supplementary material.
They further support the better performance of RMGPC in the presence of outliers in the data.

4.2 Outlier Identiﬁcation

A second batch of experiments shows the utility of RMGPC to identify data instances that are likely
to be outliers. These experiments use the Glass dataset from the previous section. Recall that for this

7

dataset RMGPC performs signiﬁcantly better than SMGPC for η = 0%, which suggest the presence
of outliers. After normalizing the Glass dataset, we run RMGPC on the whole data and estimate the
posterior probability that each instance is an outlier using (21). The hyper-parameters of RMGPC
are estimated as described in the previous section. Figure 1 shows for each instance (xi , yi ) of the
Glass dataset, with i = 1, . . . , n, the value of P (zi = 1|y, X). Note that most of the instances
are considered to be outliers with very low posterior probability. Nevertheless, there is a small set
of instances that have very high posterior probabilities. These instances are unlikely to stem from
(1) and are expected to be misclassiﬁed when placed on the test set. Consider the set of instances
that are more likely to be outliers than normal instances (i.e., instances 3, 36, 127, 137, 152, 158 and
188). Assume the experimental protocol of the previous section. Table 3 displays the fraction of
times that each of these instances is misclassiﬁed by RMGPC, SMGPC and HTPC when placed on
the test set. The posterior probability that each instance is an outlier, as estimated by RMGPC, is
also reported. The table shows that all the instances are typically misclassiﬁed by all the classiﬁers
investigated, which conﬁrms the difﬁculty of obtaining accurate predictions for them in practice.

Figure 1: Posterior probability that each data instance form the Glass dataset is an outlier.

Table 3: Average test error in % of each method on each data instance that is more likely to be an
outlier. The probability that the instance is an outlier, as estimated by RMGPC, is also displayed.
Glass Data Instances
188-th
158-th
127-th
36-th
3-rd
137-th
152-th
100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0
100.0±0.0 92.0±5.5 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0
100.0±0.0 84.0±7.5 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0 100.0±0.0
0.69
0.96
0.82
0.51
0.86
0.83
1.00

r RMGPC
t
o
SMGPC
s
r
e
r
T
E
HTPC
P (zi = 1|y, X)

5 Conclusions

We have introduced a Robust Multi-class Gaussian Process Classiﬁer (RMGPC). RMGPC considers
only the number of errors made, and not the distance of such errors to the decision boundaries of
the classiﬁer. This is achieved by introducing binary latent variables that indicate when a given
instance is considered to be an outlier (wrongly labeled instance) or not. RMGPC can also identify
the training instances that are more likely to be outliers. Exact Bayesian inference in RMGPC is
intractable for typical learning problems. Nevertheless, approximate inference can be efﬁciently
carried out using expectation propagation (EP). When EP is used, the training cost of RMGPC is
O(ln3 ), where l is the number of classes and n is the number of training instances. Experiments in
four multi-class classiﬁcation problems show the beneﬁts of RMGPC when labeling noise is injected
in the data. In this case, RMGPC performs better than other alternatives based on considering latent
Gaussian noise or noise which follows a distribution with heavy tails. When there is no noise in the
data, RMGPC performs better or equivalent to these alternatives. Our experiments also conﬁrm the
utility of RMGPC to identify data instances that are difﬁcult to classify accurately in practice. These
instances are typically misclassiﬁed by different predictors when included in the test set.

Acknowledgment

All experiments were run on the Center for Intensive Computation and Mass Storage (Louvain). All authors
acknowledge support from the Spanish MCyT (Project TIN2010-21575-C02-02).

8

0501001502000.000.501.00GlassDataInstancesP(z_i=1|y,X)References
[1] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine
Learning (Adaptive Computation and Machine Learning). The MIT Press, 2006.
[2] Christopher K. I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.
[3] Hyun-Chul Kim and Zoubin Ghahramani. Bayesian Gaussian process classiﬁcation with
IEEE Transactions on Pattern Analysis and Machine Intelligence,
the EM-EP algorithm.
28(12):1948–1959, 2006.
[4] R.M Neal. Regression and classiﬁcation using Gaussian process priors. Bayesian Statistics,
6:475–501, 1999.
[5] Matthias Seeger and Michael I. Jordan. Sparse Gaussian process classiﬁcation with multiple
classes. Technical report, University of California, Berkeley, 2004.
[6] M. Opper and O. Winther. Gaussian process classiﬁcation and SVM: Mean ﬁeld results. In
P. Bartlett, B.Schoelkopf, D. Schuurmans, and A. Smola, editors, Advances in large margin
classiﬁers, pages 43–65. MIT Press, 2000.
[7] Daniel Hern ´andez-Lobato and Jos ´e Miguel Hern ´andez-Lobato. Bayes machines for binary
classiﬁcation. Pattern Recognition Letters, 29(10):1466–1473, 2008.
[8] Hyun-Chul Kim and Zoubin Ghahramani. Outlier robust Gaussian process classiﬁcation. In
Structural, Syntactic, and Statistical Pattern Recognition, volume 5342 of Lecture Notes in
Computer Science, pages 896–905. Springer Berlin / Heidelberg, 2008.
[9] Fabian L. Wauthier and Michael I. Jordan. Heavy-Tailed Process Priors for Selective Shrink-
age.
In J. Lafferty, C. K. I. Williams, R. Zemel, J. Shawe-Taylor, and A. Culotta, editors,
Advances in Neural Information Processing Systems 23, pages 2406–2414. 2010.
[10] Thomas Minka. A Family of Algorithms for approximate Bayesian Inference. PhD thesis,
Massachusetts Institute of Technology, 2001.
[11] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[12] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines, 2001.
[13] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and
Statistics). Springer, August 2006.
[14] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Adnan
Darwiche and Nir Friedman, editors, Proceedings of the 18th Conference on Uncertainty in
Artiﬁcial Intelligence, pages 352–359. Morgan Kaufmann, 2002.
[15] Malte Kuss and Carl Edward Rasmussen. Assessing approximate inference for binary Gaussian
process classiﬁcation. Journal of Machine Learning Research, 6:1679–1704, 2005.
[16] H Nickisch and CE Rasmussen. Approximations for binary Gaussian process classiﬁcation.
Journal of Machine Learning Research, 9:2035–2078, 10 2008.
[17] Marcel Van Gerven, Botond Cseke, Robert Oostenveld, and Tom Heskes. Bayesian source
localization with the multivariate Laplace prior.
In Y. Bengio, D. Schuurmans, J. Lafferty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems
22, pages 1901–1909, 2009.
[18] Matthias Seeger. Expectation propagation for exponential families. Technical report, Depart-
ment of EECS, University of California, Berkeley, 2006.

9


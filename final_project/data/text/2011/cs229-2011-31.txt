Detecting Emotion in Human Speech

Alex Mordkovich, Kelly Veit, Daniel Zilber
{amordkov, kjveit, dzilber}@stanford.edu

December 16th, 2011

1

Introduction

Detection of emotion in speech can be applied in
a variety of situations to allocate limited human
resources to clients with the highest levels of dis-
tress or need, such as in automated call centers
or in a nursing home. While a human may not
be available to assist everyone, automated emo-
tion detection can be used to “triage” a customer
and, if they are growing angry or impatient, trans-
fer them to speak with a human.
It may also
be useful for helping autistic children and adults
learn to recognize more subtle social cues, or help
people reﬁne their speaking skills so that they
have better control over the message they send.
Other applications include improving personal as-
sistants, such as Siri, to help convey emotion in
text-based communication. The applications for
emotion detection in speech are varied and prac-
tical; thus, we undertook the task of building a
machine-learning system to detect emotion in ut-
terances of human speech.

2 Features

We used the scripting capabilities of the freely-
available Praat [4] software to process the au-
dio data and extract various statistics,
includ-
ing a standard “Voice Report”. The voice report
includes statistics for pitch, pulses, voicing, jit-
ter, and harmonicity. We use the median, mean,
standard deviation, maximum, and minimum of
the above characteristics as some of our features.
Pulse data includes the number of pulses, number
and mean of the periods, and the standard devi-
ation of the periods. Voicing looks at unvoiced
frames and calculates the number and percentage
of those. Local shimmer is the average absolute
diﬀerence between the amplitudes of consecutive

periods, divided by the average amplitude. There
are ﬁve diﬀerent measurements of jitter taken, all
a variant of local jitter. Local jitter is the average
absolute diﬀerence between consecutive periods,
divided by the average period.
A common set of features used in voice pro-
cessing algorithms is the set of Mel-frequency cep-
stral coeﬃcients (MFCCs). Cepstral features are
those represented on a nonlinear “spectrum of
a spectrum”, i.e. derived by taking the Fourier
transform of the logarithm of a spectrum. These
features are then represented on the Mel scale,
which is designed to approximate the response
of human hearing by emphasizing frequencies to
which humans are sensitive. Using Praat, we ex-
tract 12 commonly-used MFCCs that model the
waveform, and use statistics on these coeﬃcients
as additional feature.

3 Data

For our training, cross-validation, and testing, we
used the Emotional Prosody Speech and Tran-
scripts obtained from the Linguistic Data Consor-
tium [3]. This data consists of recordings of pro-
fessional actors reciting dates and numbers with
various emotional intonations. The semantic con-
tent of the utterances is intended to be emotion-
ally neutral, as a form of psychological control
in the samples. Each of the 15 audio recordings
(∼200MB in size each) is annotated with a tran-
script ﬁle describing the time ranges of the ut-
terances and the corresponding emotion label on
each utterance. A sample transcript line looks
like this:

1325.91 1327.94 A: elation,Seventy-one

This line indicates that the utterance from
96.36 seconds into the recording until 97.44 sec-

1

onds into the recording is an elated utterance of
the phrase “two thousand six”. The audio record-
ings and transcript data is preprocessed in a cus-
tom four-stage pipeline to generate data ﬁle in
which each utterance is a data sample represented
as a single line. This resulting data ﬁle is loaded
into Matlab as a stylized design matrix.
90% of all the samples are used for training
and cross-validation. The remaining 10% are set
aside as test data that is unseen until we are ready
to test our learning algorithms’ hypotheses.

4 PCA

To get a sense of the training data – and to de-
termine whether there are correlations among the
features – we ran Principal Component Analysis
on it. Note that since the various features are on
diﬀerent scales (some are percentages, some are
Hertz, some are counts, etc.), from here on we
work with the zscores (normalized data) rather
than the raw values.

Figure 1: Explaining the Variance

In Figure 1, we see that the ﬁrst two prin-
cipal components explain only about 30% of the
variance. In general, we see that the variance is
distributed thinly across most of the features.
We also noted that the ﬁrst two principal
components gave the largest (absolute) weight to
the features describing the median pitch and the
mean of the 8th MFCC, respectively, indicating
that these are potentially the more “useful” fea-
tures out of the lot.
We can also pro ject the (normalized) data
onto the ﬁrst two principal components. In Fig-
ure 2, we see that almost all the MFCCs are corre-
lated positively with the second principal compo-
nent, whereas some MFCC’s correlate negatively
and some correlate positively with the ﬁrst prin-

2

cipal component. Also we do not see any clear
clustering of data points.

Figure 2: Principal Component Space

5 Supervised K-means

We chose to use K-Means clustering for our ﬁrst
classiﬁcation algorithm due to its simplicity and
applicability to a multilabel problem. However,
because the training data set is labeled, we imple-
mented a supervised version of K-means in which
the starting centroids are calculated by grouping
the data points by label and then calculating the
mean for each label.
Without any further restrictions, this algo-
rithm obtained an error of approximately 85%
when attempting to classify between 14 emotions,
when trained on the complete training data set.
This implies that the data is not clearly separated
into clusters in the given feature space. This is
also a consequence of using 14 input labels in the
training set. To get around this problem, addi-
tional/diﬀerent features need to be carefully se-
lected or a diﬀerent metric should be used to pre-
dict emotions correctly. We explored these op-
tions to improve the K-Means predictions by im-
plementing custom feature selection, label group-
ing, and ﬁtting normal distributions akin to Gaus-
sian descriminant analysis.

5.1 Feature Selection

Feature selection was implemented in two ways.
The ﬁrst way was to ﬁnd the combination of 1-3
features that minimizes training error. The sec-
ond approach was to use a forward or backward
search heuristic. Neither approach improved re-
sults signiﬁcantly. These search algorithms for
new features proved to be slow. Below are some

Test Er-
ror
86%
93%
97%
92%
88%
90%
89%

Train
Error
29%
66%
53%
43%
19%
53%
29%

anxiety
boredom
cold anger
contempt
despair
disgust
elation
happy
hot anger
interest
neutral
panic
pride
sadness
shame
% Error

2
2
2
3
2
3
1
1
1
1
2
3
2
3
2
28.6

Feat. 1

Feat. 2 Feat.3

7
14
13
5
1
13
1

1
2
1
2
2
1
1
2
1
3
3
1
1
2
1
26.2

11
19
18
7
2
18
2

2
3
2
2
3
3
3
3
3
2
2
3
2
3
2
23.8

2
2
3
2
2
3
1
3
1
3
2
2
1
2
3
19.1

-
20
22
18
-
-
-

1
1
1
1
3
1
1
1
1
1
2
2
1
1
1
14.3

of the results of the ﬁrst approach on small sam-
ple sets containing 14 emotions. Small data sets
were used to limit run time.

5.2 Label Grouping

To reduce the generalization error, the emotions
were categorized into several small groups instead
of 14 distinct labels, as shown in the table above.
After running hundreds of randomized labelings,
the best groupings were recorded with error being
signiﬁcantly decreased as expected, in the range
of 25-30% test error. There were some trivial
relabelings in which most of the emotions were
grouped together, recreating the enviroment of
previous research in which one or two emotions
are classiﬁed against all the remaining emotions;
these labelings naturally oﬀered the lowest er-
ror at around 20 to 15%. When checking each
grouping, feature selection was also applied but
set to decrease test error rather than training er-
ror, making the results optmistic.
The grouping corresponding to 19.05% error

3

was reevaluated on diﬀerent samples, resulting in
23.8% and 32.5% testing error. Using two clus-
ters yielded a 15% error, while four yielded a 40%
error.

5.3 Gaussian Fitting

In addition to the label grouping and feature
selection attempts to reduce error, one last ap-
proach we tried was to use gaussian distributions
for each label.
In theory, this would help clas-
sify some of the outliers that were closer to ”in-
correct” centroids when using the L2 norm. The
means were calculated the same way as centroids
but now a covariance matrix was calculated for
the features of samples with a speciﬁed label. To
avoid singular matrices, the data was normalized
using zscore. The results were on par with the
regular K-Means algorithm.

6 SVM

We experimented with the SVM implementations
in Liblinear, LibSVM, and the MATLAB-builtin
SVMClassify for our classiﬁcation tasks. The lin-
ear, polynomial, and RBF kernels were used with
LibSVM. All data was normalized before process-
ing. The results of these methods are shown in
the Results Matrix Tables 1 and 2. Backward
feature search was used to reduce error, as speed
was not a limiting factor. As seen in the result
matrix, this reduced error by approximately 1%
on average.
While others [6][7] have used SVM to classify
emotions to distinguished between two or three
emotions that known a priori, classifying a wider
spectrum of emotions is a more pragmatic en-
deavor. The application of an algorithm that can
choose only between two known emotions is quite
limited. Thus, we decided to train and test our al-
gorithms on utterances ranging the full spectrum
of the 14 available emotions; the largest num-
ber of emotion labels that previous studies have
worked with is only 8. The best generalization
error we achieved for exact emotion classiﬁcation
is 43.67%.

Error
Kernel
Library
64.19%
Linear
LibLinear
Linear
57.21%
LibSVM
Polynomial 55.02%
LibSVM
LibSVM
RBF
43.67%
Table 1: Exact emotion classiﬁcation results

6.1 Exact Emotion Classiﬁcation

Using the multiclass labeling support in LibSVM,
we trained a model to distinguish between 14 emo-
tional states. We used Figure 3.

Kernel
RBF

Error
16.20%

Library
LibSVM - bi-
nary arousal
LibSVM - bi-
nary valence
LibSVM
-
high arousal
only
Table 2: Clustered emotion classiﬁcation results

17.88%

13.10%

RBF

RBF

High Arousal

Neutral
Valence

Neutral
Arousal

High
Valence
elation

Low
Valence
hot anger,
panic,
anxiety
disgust,
sadness,
contempt,
cold
anger,
shame
Low Arousal
boredom
despair
Table 3: Emotion classiﬁcation by arousal/va-
lence

pride,
interest,
happy

Figure 3: Error vs. training set size

As expected, the testing error falls as the
training set size increases, and it appears to be
ﬂattening out. We also note that training error
remains unacceptably high and does not change in
any signiﬁcant way as training set size increases.
However, there is still a large gap between the
testing error and the training error. As such, it
is diﬃcult to say whether our algorithm is suf-
fereing from high variance or high bias. Adding
more data samples will not signiﬁcantly improve
our results. One way to interpret this result is
that set of features we are extracting from the
voice data does not necessarily describe or cor-
relate well with the emotional labels.
If this is
the case, the acoustic and prosodic characterstics
of human utterances need to be studied better to
understand which features are indeed relevant.

arousal (excitement) of the emotion, the error is
reduced. Here, the trade-oﬀ is speciﬁc knowl-
edge of the emotion. For modern applications,
this may be suﬃcient - the valence-arousal space
provides valuable information about the emotion,
allowing a system such as automated call centers
(an application that does not need to distinguish
contempt from sadness, but needs to be sensitive
to low valence) to make an appropriate reaction.
Using a multiclass SVM to group emotions into
discrete groups, Table 3 is the classiﬁcation of
emotions with the lowest error.
It is clear that
this classiﬁcation is useful for applications - it is
similar to classiﬁcations in psychology, such as
the FeelTrace[5] emotion classiﬁcation. Exploring
this concept deeper by distinguishing emotions on
one dimension only, the trade-oﬀ becomes more
apparent. High arousal emotions, for example,
can be distinguished from all other emotions with
a 13.10% generalization error.

6.2 Clustered Emotion Classiﬁcation

7 Conclusion

By grouping emotions according to a 2 dimen-
sional space of level of valence (positivity) and

The purpose of this pro ject was to expand upon
current research in which speech is classiﬁed by

4

emotion. We looked at data that contained sam-
ples spanning 14 emotions and implemented K-
Means Clustering and SVM algorithms to create
predictions. Initially, both algorithms performed
poorly. Feature selection and label grouping im-
proved results at the price of speciﬁc emotion clas-
siﬁcation.

References

[1] Shami, M. & Verhelst, W. (2007). An evalu-
ation of the robustness of existing supervised
machine learning approaches to the classiﬁca-
tion of emotions in speech. Speech Communi-
cation 49:201-212

[2] Casale, S., Russo, A., Scebba, G., Serrano,
S. (2008). Speech Emotion Classiﬁcation Us-
ing Machine Learning Algorithms. Semantic
Computing, IEEE Internation Converence on
26:33

[3] http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002S28

[4] http://www.fon.hum.uva.nl/praat/

[5] http//emotion-research.net

[6] Laukka, Neiberg, Forsell, Karlsson, Elenius
(2011) Expression of aect in spontaneous
speech: Acoustic correlates and automatic de-
tection of irritation and resignation Computer
Speech and Language 25:84-104

[7] Nwe, Foo, De Silva (2003) Speech Emotion
Recognition Using Hidden Markov Models
Speech Communication 41:603-623

5


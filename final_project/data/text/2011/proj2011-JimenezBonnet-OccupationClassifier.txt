dataset to train and test the predictor. All the
attributes were discretized and are listed be-
low with their corresponding number of classes 2 :

1. Occupation (9 levels)
2. Annual income of household (9 levels)
3. Sex (2 levels)
4. Marital status (5 levels)
5. Age (7 levels)
6. Education (6 levels)
7. How long have you lived in the San Fran-
cisco/ Oakland/San Jose area? (5 levels)
8. Dual incomes, if married (3 levels)
9. Persons in your household (9 levels)
10. Persons in household under 18 (9 levels)
11. Householder status (3 levels)
12. Type of home (5 levels)
13. Ethnic classiﬁcation (8 leveles)
14. What language is spoken most often in your
home? (3 levels)

Many of the variables have many levels, in-
cluding the response variable Occupation which
has 9 levels. This, summed to the fact that al-
most 20% (1743 observations) of the data set
have at least one missing value, makes this data
set hard to work with. The treatment of these
missing values played a central role the pro ject.

The dataset was randomly split into a training
set with 7085 entries (80%) and a learning set
with 1772 entries (20%). The former was used
to ﬁt our model and the latter was kept separate
until the very end where it was used to estimate
the prediction error of our ﬁnal model.

Occupation Classiﬁer

Sebasti´an Jim´enez-Bonnet - 05427277

December 15, 2011

1 Introduction

In this pro ject I explored how accurately could
the occupation of a person be guesses based on
other demographic variables. To this end, sev-
eral algorithms were attempted, such as multi-
nomial logistic regression, SVM and Classiﬁca-
tion and Regression Trees (CART). I present
here the results from the most successful algo-
rithm, CART, and discuss some of the reasons
that made it perform better.

2 The Data

The data set used is an extract form a bigger
marketing database1 , originally intended to
classify people by income. The original data
set consists of 9409 questionnaires ﬁlled up by
shopping mall customers in the San Francisco
Bay area. The questionnaires contained 502
questions, and the extract used consists on
14 demographic attributes. This dataset is a
mixed set in which we can ﬁnd many cate-
gorical and numerical variables with a lot of
missing values. After those entries for which the
response variable was missing were removed,
a total of 8857 observations served as our

1Source:
(1987).

Impact Resources,

Inc., Columbus, OH

2For a complete listing of the levels visit http://www-
stat.stanford.edu/ tibs/ElemStatLearn/

1

Figure 1: A plot of the CV error and the training set error for each of the ﬁtted trees, as a fraction of
the error achieved by the single node tree. On the x-axis we have the number of nodes of the corresponding
trees

3 Classiﬁcation and Regression
Trees

ing observations, which is really important for
our dataset.

The CART algorithm, roughly speaking, gener-
ates a sequence of binary splits on one of the in-
put variables at a time, so that the resulting mea-
sure of impurity is minimized each time. The
tree is grown until a stopping criteria is satisﬁed
and ﬁnally it is pruned to avoid overﬁtting. In
order to make a prediction, it assigns the class
with the most votes to each of the ﬁnal nodes.
There are several reasons why trees are spe-
cially appropriate for our particular problem.
First, trees work well with discrete input and
response variables that have many levels. Sec-
ond, they are fairly easy to interpreted which
may lead to a deeper understanding of the re-
lation of the input with the response variable.
Lastly, trees oﬀer several ways to deal with miss-

3.1 Missing values

Binary trees are specially well suited to deal with
missing values. There are several diﬀerent ap-
proaches to this matter, the simplest one is to
forget about them and build a model based only
on observations that have all their attributes
observed. This approach wastes lots of infor-
mation, (we would dismiss almost 20% of our
dataset!) and is not able to oﬀer a prediction for
entries that have missing values themselves.
Another approach is to add a dummy level to
each of our attributes, corresponding to a miss-
ing value. This approach is useful in that it can
help uncover a relation between the missing val-
ues and the response. But if the missing values

2

0501001502002500.50.60.70.80.91.0Classification Relative ErrorNumber of splitsProportion of Root Node ErrorCV estimateTraining SetFigure 2: Our ﬁnal binary tree.

appear randomly or are the product of data pro-
cessing rather that data collection, this approach
is also ineﬃcient.
Both of these approaches can be implemented
by many algorithms, but trees oﬀer an additional
one, surrogate splits. It consists in choosing sev-
eral substitute splits for each of the original splits
elected for the model. In choosing these surro-
gate splits, only variables diﬀerent from the orig-
inal one are considered. The split is then chosen
so that the resulting partition of the data has
the biggest overlap with the partition the origi-
nal split achieved (where only data that has both
variables observed is considered). The second
surrogate excludes the original variables and the
variable for the ﬁrst surrogate, and so on.

3.2 Fitting the predictor

In this implementation of the CART algorithm,
the GINI index is used as the measure of
impurity to grow the tree. Three criteria were

considered to stop branching.

1. The nodes considered for splitting were
required to have at least 5 observations.
2. The end nodes were required to have at least
two observations.
3. The split to be made had to decrease our
measure of impurity by at least Cp = 0.001.

These stopping rules aim to avoid huge trees
that are computationally expensive and which
would be trivially pruned at a later stage.
To ﬁt the model I used R as the software plat-
form, speciﬁcally I used the package rpart. The
algorithm calculates a whole path of nested trees,
from the trivial root node, to the full tree reached
with our stopping criteria. It indexes each tree
by the value of the parameter Cp that would have
taken to stop branching exactly at that tree.
To choose the appropriate tree and avoid
overﬁtting I calculated a 10-fold cross-validation
(CV) error for each of the trees. In Figure 1 we

3

|Age=3,4,5,6,7Age=3,4,5Education=5,6Dual.income=1,2Income=6,7,8,9Sex=2Education=4Sex=1Age=6Dual.income=2Income=2,3,4,5,6,7,8,9Householder=1,2Education=5,6Sex=2Dual.income=1,2Education=1,4Time.bay.area=3,4,5Education=4Income=3,4,6,7,8Age=2Professional  Professional  Clerical      Professional  Factory WorkerProfessional  Homemaker     Professional  Retired       Retired       Professional  Professional  Clerical      Homemaker     Professional  Student       Factory WorkerMilitar       Student       Sales         Student       Figure 3: Accuracy for class of the response variable Occupation.

can see a plot of the training error and the CV er-
ror for each of these trees as a percentage of the
trivial root-node classiﬁcation. The root node
trivial error is 0.679, and is achieved by the clas-
siﬁer which assigns the most frequent class (in
our case Professional/Managerial) to any input.
I used the two standard deviation parsimonious
approach in which the selected model is the sim-
plest model (least number of ﬁnal nodes) which
achieves a CV error within to standard devia-
tions of the minimum CV error.

3.3 Final model

3.4 Prediction

Testing our ﬁnal model on the test set, it has
a resulting test error of 46%. At ﬁrst sight it
may seem a bit high, but considering we have
9 classes for the response variable and a sparse
training set, it is actually a quite remarkable per-
formance. For a frame of reference, it can be
compared to the root node error, which is 68%.
In ﬁgure 3 we can see the classiﬁcation accu-
racy for each class. We can see that, excluding
the Retired (which is easily identiﬁable by age),
there is a clear bias in favor of the more frequent
classes which are Professional and Student. In
fact the least frequent class, Unemployed is never
assigned.

Our ﬁnal model corresponds to a Cp value of
0.00208, which translates into a binary tree with
20 nodes. In ﬁgure 2 we can see the structure
of the model. Note that the model only uses 7
of the 13 possible variables. The variables that
seem to be the most important in our tree are
Age, Annual income and Education.

4 Conclusions

Our tree predictor had a good performance in
classifying our test set. One key feature of our
model was its treatment of the missing values.
For comparison, I ﬁtted another CART classiﬁer
omitting the missing values and got a test error

4

RetiredStudentProfessionalHomemakerClericalFactoryMilitarSalesUnemployedPrediction Accuracy0.00.20.40.60.81.00.9110.8010.7890.3100.2510.1880.1370.0330.0005 References

(i) Hastie T., R. Tibshirani and J. Friedman J
The Elements of Statistical Learning Stan-
ford, California. 2008.

(ii) Bishop, Christofer M. Pattern Recognition
and Machine Learning Cambridge. 2006.

(iii) Michel, Tom M. Machine Learning 1997.

of 53%, which suggests that there is a consider-
able loss of information doing this.
There are some questions that still need to
be answered in order to asses our results. How
general is our model? Can we extend it to the
general population of California or the United
States? One obstacle I can see, which ﬁgure 3
supports, is that our predictions are heavily af-
fected by the frequency of each class. Probably,
in order to have more generally applicable re-
sults, we could weight our observations in order
to balance the frequency of each class with the
frequency of the actual population we want to
predict.
Regarding the importance of the variables, it
is important to keep in mind that binary trees
have a really high variance. Little variation on
the data can mean many diﬀerent combinations
on the selected variables for each split. In order
to better asses the importance of the variables,
at least when there is a lack of context to aid us,
Random Forests has a more robust importance
measure for the predictor variables, and may be
a good continuation of this work.

5


Better Mini-Batch Algorithms
via Accelerated Gradient Methods

Andrew Cotter
Toyota Technological Institute at Chicago
cotter@ttic.edu

Ohad Shamir
Microsoft Research, NE
ohadsh@microsoft.com

Nathan Srebro
Toyota Technological Institute at Chicago
nati@ttic.edu

Karthik Sridharan
Toyota Technological Institute at Chicago
karthik@ttic.edu

Abstract

Mini-batch algorithms have been proposed as a way to speed-up stochastic
convex optimization problems. We study how such algorithms can be im-
proved using accelerated gradient methods. We provide a novel analysis,
which shows how standard gradient methods may sometimes be insuﬃcient
to obtain a signiﬁcant speed-up and propose a novel accelerated gradient
algorithm, which deals with this deﬁciency, enjoys a uniformly superior
guarantee and works well in practice.

1

Introduction

We consider a stochastic convex optimization problem of the form minw∈W L(w), where
L(w) = Ez [￿(w, z )], based on an empirical sample of instances z1 , . . . , zm . We assume that
W is a convex subset of some Hilbert space (which in this paper, we will take to be Euclidean
space), and ￿ is non-negative, convex and smooth in its ﬁrst argument (i.e. has a Lipschitz-
constinuous gradient). The classical learning application is when z = (x, y) and ￿(w, (x, y))
is a prediction loss. In recent years, there has been much interest in developing eﬃcient
ﬁrst-order stochastic optimization methods for these problems, such as mirror descent [2, 6]
and dual averaging [9, 16]. These methods are characterized by incremental updates based
on subgradients ∂ ￿(w, zi ) of individual instances, and enjoy the advantages of being highly
scalable and simple to implement.

An important limitation of these methods is that they are inherently sequential, and so
problematic to parallelize. A popular way to speed-up these algorithms, especially in a
parallel setting, is via mini-batching, where the incremental update is performed on an
average of the subgradients with respect to several instances at a time, rather than a single
b ￿b
instance (i.e., 1
j=1 ∂ ￿(w, zi+j )). The gradient computations for each mini-batch can
be parallelized, allowing these methods to perform faster in a distributed framework (see
for instance [11]). Recently, [10] has shown that a mini-batching distributed framework is
capable of attaining asymptotically optimal speed-up in general (see also [1]).

A parallel development has been the popularization of accelerated gradient descent methods
[7, 8, 15, 5]. In a deterministic optimization setting and for general smooth convex functions,
these methods enjoy a rate of O(1/n2 ) (where n is the number of iterations) as opposed
to O(1/n) using standard methods. However, in a stochastic setting (which is the relevant
one for learning problems), the rate of both approaches have an O(1/√n) dominant term
in general, so the beneﬁt of using accelerated methods for learning problems is not obvious.

1

Algorithm 1 Stochastic Gradient Descent with Mini-Batching (SGD)
Parameters: Step size η , mini-batch size b.
Input: Sample z1 , . . . , zm
w1 = 0
for i = 1 to n = m/b do
b ￿bi
Let ￿i (wi ) = 1
t=b(i−1)+1 ￿(wi , zt )
w￿i+1 := wi − η∇￿i (wi ))
wi+1 := PW (w￿i+1 )
end for
n ￿n
Return ¯w = 1
i=1 wi
Algorithm 2 Accelerated Gradient Method (AG)
Parameters: Step sizes (γi ,β i ), mini-batch size b
Input: Sample z1 , . . . , zm
w = 0
for i = 1 to n = m/b do
b ￿bi
Let ￿i (wi ) := 1
t=b(i−1)+1 ￿(w, zt )
)wag
i wi + (1 − β−1
:= β−1
wmd
i
i
i
w￿i+1 := wmd
i − γi∇￿i (wmd
)
i
wi+1 := PW (w￿i+1 )
wag
i+1 ← β−1
i wi+1 + (1 − β−1
i
end for
Return wag
n

)wag
i

In this paper, we study the application of accelerated methods for mini-batch algorithms, and
provide theoretical results, a novel algorithm, and empirical experiments. The main resulting
message is that by using an appropriate accelerated method, we obtain signiﬁcantly better
stochastic optimization algorithms in terms of convergence speed. Moreover, in certain
regimes acceleration is actually necessary in order to allow a signiﬁcant speedups. The
potential beneﬁt of acceleration to mini-batching has been brieﬂy noted in [4], but here we
study this issue in much more depth. In particular, we make the following contributions:

• We develop novel convergence bounds for the standard gradient method, which
reﬁnes the result of [10, 4] by being dependent on L(w￿ ), the expected loss of
the best predictor in our class. For example, we show that in the regime where the
desired suboptimality is comparable or larger than L(w￿ ), including in the separable
case L(w￿ ) = 0, mini-batching does not lead to signiﬁcant speed-ups with standard
gradient methods.
• We develop a novel variant of the stochastic accelerated gradient method [5], which
is optimized for a mini-batch framework and implicitly adaptive to L(w￿ ).
• We provide an analysis of our accelerated algorithm, reﬁning the analysis of [5] by
being dependent on L(w￿ ), and show how it always allows for signiﬁcant speed-
ups via mini-batching, in contrast to standard gradient methods. Moreover, its
performance is uniformly superior, at least in terms of theoretical upper bounds.
• We provide an empirical study, validating our theoretical observations and the eﬃ-
cacy of our new method.

2 Preliminaries

As discussed in the introduction, we focus on a stochastic convex optimization problem,
where we wish to minimize L(w) = Ez [￿(w, z )] over some convex domain W , using an i.i.d.
sample z1 , . . . , zm . Throughout this paper we assume that the instantaneous loss ￿(·, z ) is
convex, non-negative and H -smooth for each z ∈Z . Also in this paper, we take W to be
the set W = {w : ￿w￿ ≤ D}, although our results can be generalized.

2

We discuss two stochastic optimization approaches to deal with this problem: stochastic
gradient descent (SGD), and accelerated gradient methods (AG). In a mini-batch setting,
both approaches iteratively average sub-gradients with respect to several instances, and use
this average to update the predictor. However, the update is done in diﬀerent ways.

The stochastic gradient descent algorithm (which in more general settings is known as
mirror descent, e.g. [6]) is summarized as Algorithm 1. In the pseudocode, PW refers to the
pro jection on to the ball W , which amounts to rescaling w to have norm at most D .
The accelerated gradient method (e.g., [5]) is summarized as Algorithm 2.
In terms of existing results, for the SGD algorithm we have [4, Section 5.1]
m ￿ ,
E [L( ¯w)] − L(w￿ ) ≤O ￿￿ 1
b
m
whereas for an accelerated gradient algorithm, we have [5]
m2 ￿ .
n )] − L(w￿ ) ≤O ￿￿ 1
b2
E [L(wag
m
Thus, as long as b = o(√m), both methods allow us to use a large mini-batch size b
without signiﬁcantly degrading the performance of either method. This allows the number of
iterations n = m/b to be smaller, potentially resulting in faster convergence speed. However,
these bounds do not show that accelerated methods have a signiﬁcant advantage over the
SGD algorithm, at least when b = o(√m), since both have the same ﬁrst-order term 1/√m.
To understand the diﬀerences between these two methods better, we will need a more reﬁned
analysis, to which we now turn.

+

+

3 Convergence Guarantees

The following theorems provide a reﬁned convergence guarantee for the SGD algorithm and
the AG algorithm, which improves on the analysis of [10, 4, 5] by being explicitly dependent
on L(w￿ ), the expected loss of the best predictor w￿ in W . Due to lack of space, the proofs
are only sketched. The full proofs are deferred to the supplementary material.
=
η
Theorem 1. For
the
Stochastic Gradient Descent
algorithm with
min ￿ 1
L(w￿ )bn ￿, assuming L(0) ≤ HD2 , we get that
2H , ￿ bD2
L(w￿ )Hn
1+￿ HD2
E [L( ¯w)] − L(w￿ ) ≤ ￿ HD2 L(w￿ )
2HD2
9HD2
+
n
2bn
bn
Theorem 2. For the Accelerated Descent algorithm with βi = i+1
2 , γi = γi p where
2p+1 ￿
γ = min ￿ 1
4HD2+√4HD2L(w￿ ) ￿ p
2p+1 ￿
4H , ￿
1044H (n−1)2p ￿ p+1
412HL(w￿ )(n−1)2p+1 , ￿
D2
bD2
b
(1)

+

and

p = min ￿max ￿ log(b)
2 (log(b(n − 1)) − log log(n)) ￿ , 1￿ ,
log log(n)
2 log(n − 1)
as long as n ≥ 904, we have that
n )] − L(w￿ ) ≤ 358￿ HD2L(w￿ )
1428HD2√log n
E [L(wag
b(n − 1)
b(n − 1)

1545HD2
√b(n − 1)

+

+

(2)

4HD2
(n − 1)2

,

+

3

We emphasize that Theorem 2 gives more than a theoretical bound: it actually speciﬁes a
novel accelerated gradient strategy, where the step size γi scales polynomial ly in i, in a way
dependent on the minibatch size b and L(w￿ ). While L(w￿ ) may not be known in advance,
it does have the practical implication that choosing γi ∝ ip for some p < 1, as opposed to
just choosing γi ∝ i as in [5]), might yield superior results.
The key observation used for analyzing the dependence on L(w￿ ) is that for any non-negative
H -smooth convex function f : W ￿→ R, we have [13]:
￿∇f (w)￿ ≤ ￿4H f (w)
This self-bounding property tells us that the norm of the gradient is small at a point if the
loss is itself small at that point. This self-bounding property has been used in [14] in the
online setting and in [13] in the stochastic setting to get better (faster) rates of convergence
for non-negative smooth losses. The implication of this observation are that for any w ∈ W,
￿∇L(w)￿ ≤ ￿4H L(w) and ∀z ∈Z , ￿￿(w, z )￿ ≤ ￿4H￿ (w, z ).
Proof sketch for Theorem 1. The proof for the stochastic gradient descent bound is
mainly based on the proof techniques in [5] and its extension to the mini-batch case in [10].
Following the line of analysis in [5], one can show that
L(wi )￿ − L(w￿ ) ≤ η
E ￿ 1
E ￿￿∇L(wi ) − ∇￿i (wi )￿2 ￿ + D2
n−1￿i=1
n￿i=1
n−1
2η(n−1)
n
In the case of [5], E [￿∇L(wi ) − ∇￿i (wi )￿] is bounded by the variance, and that leads to the
ﬁnal bound provided in [5] (by setting η appropriately). As noticed in [10], in the minibatch
b ￿bi
setting we have ∇￿i (wi ) = 1
t=b(i−1)+1 ￿(wi , zt ) and so one can further show that
E ￿ 1
L(wi )￿ − L(w￿ ) ≤
n−1￿i=1
ib￿t=
n￿i=1
η
b2 (n−1)
n
(i−1)b+1
In [10], each of ￿∇L(wi ) − ∇￿(wi , zt )￿ is bounded by σ0 and so setting η , the mini-batch
bound provided there is obtained. In our analysis we further use the self-bounding property
to (4) and get that
L(wi )￿ − L(w￿ ) ≤ 16Hη
E ￿ 1
n−1￿i=1
n￿i=1
b(n−1)
n
rearranging and setting η appropriately gives the ﬁnal bound.

E ￿∇L(wi ) − ∇￿(wi , zt )￿2 + D2
2η(n−1)

E [L(wi )] + D2
2η(n−1)

(3)

(4)

) − ∇￿i (wmd
i

E [L(wag
n )] − L(w￿ ) ≤

Proof sketch for Theorem 2. The proof of the accelerated method starts in a similar
way as in [5]. For the γi ’a and βi ’s mentioned in the theorem, following similar lines of
analysis as in [5] we get the preliminary bound
i2p E ￿￿￿∇L(wmd
)￿￿2 ￿ +
n−1￿i=1
D2
2γ
i
γ (n − 1)p+1
(n − 1)p+1
In [5] the step size γi = γ (i + 1)/2 and βi = (i + 1)/2 which eﬀectively amounts to
p = 1 and further similar to the stochastic gradient descent analysis. Furthermore, each
E ￿￿￿∇L(wmd
)￿￿2 ￿ is assumed to be bounded by some constant, and thus leads
) − ∇￿i (wmd
i
i
to the ﬁnal bound provided in [5] by setting γ appropriately. On the other hand, we ﬁrst
notice that due to the mini-batch setting, just like in the proof of stochastic gradient descent,
, zt )￿￿2 ￿ +
E ￿￿￿∇L(wmd
n−1￿i=1
ib￿t=
D2
2γ
E [L(wag
) − ∇￿(wmd
i2p
n )] − L(w￿ ) ≤
i
i
b2 (n−1)p+1
γ (n−1)p+1
b(i−1)+1
4

Using smoothness, the self bounding property some manipulations, we can further get the
bound
n−1￿i=1
i )] − L(w￿ )) + 64HγL (w￿ )(n−1)p
(E [L(wag
n )] − L(w￿ ) ≤ 64Hγ
E [L(wag
b(n−1)1−p
b
γ (n−1)p+1 + 32HD2
D2
+
b(n−1)
recursively bounds E [L(wag
n )] − L(w￿ )
of
in terms
above
the
that
Notice
￿n−1
i=1 (E [L(wag
i )] − L(w￿ )). While unrolling the recursion all the way down to 2
does not help, we notice that for any w ∈W , L(w) − L(w￿ ) ≤ 12HD2 + 3L(w￿ ). Hence we
unroll the recursion to M steps and use this inequality for the remaining sum. Optimizing
over number of steps up to which we unroll and also optimizing over the choice of γ , we get
the bound,
n )] − L(w￿ ) ≤ ￿ 1648HD2L(w￿ )
+ 348(6HD2+2L(w￿ ))
E [L(wag
b(n−1)
b(n−1)
(n−1)p+1 + 36HD2
+ 4HD2
log(n)
b(n−1)
(b(n−1))
Using the p as given in the theorem statement, and few simple manipulations, gives the ﬁnal
bound.

p
p+1 + 32HD2
b(n−1)

(b(n − 1))

p
2p+1

4 Optimizing with Mini-Batches

To compare our two theorems and understand their implications, it will be convenient to
treat H and D as constants, and focus on the more interesting parameters of sample size
m, minibatch size b, and optimal expected loss L(w￿ ). Also, we will ignore the logarithmic
factor in Theorem 2, since we will mostly be interested in signiﬁcant (i.e. polynomial)
diﬀerences between the two algorithms, and it is quite possible that this logarithmic factor
is merely an artifact of our analysis. Using m = nb, we get that the bound for the SGD
algorithm is
m ￿ ,
n ￿ = ˜O ￿￿ L(w￿ )
E [L( ¯w)] − L(w￿ ) ≤ ˜O ￿￿ L(w￿ )
b
1
bn
m
and the bound for the accelerated gradient method we propose is
n2 ￿ = ˜O ￿￿ L(w￿ )
n )] − L(w￿ ) ≤ ˜O ￿￿ L(w￿ )
m2 ￿ . (6)
b2
1
E [L(wag
bn
m
To understand the implication these bounds, we follow the approach described in [3, 12] to
analyze large-scale learning algorithms. First, we ﬁx a desired suboptimality parameter ￿,
which measures how close to L(w￿ ) we want to get. Then, we assume that both algorithms
are ran till the suboptimality of their outputs is at most ￿. Our goal would be to understand
the runtime each algorithm needs, till attaining suboptimality ￿, as a function of L(w￿ ),￿, b .

1
√bn

√b
m

(5)

+

+

+

+

+

+

To measure this runtime, we need to discern two settings here: a paral lel setting, where we
assume that the mini-batch gradient computations are performed in parallel, and a serial
setting, where the gradient computations are performed one after the other. In a parallel
setting, we can take the number of iterations n as a rough measure of the runtime (note
that in both algorithms, the runtime of a single iteration is comparable). In a serial setting,
the relevant parameter is m, the number of data accesses.

To analyze the dependence on m and n, we upper bound (5) and (6) by ￿, and invert them
to get the bounds on m and n. Ignoring logarithmic factors, for the SGD algorithm we get
￿ ￿ L(w￿ )
￿ ￿ L(w￿ )
+ 1￿
+ b￿ ,
1
1
￿
￿

m ≤

n ≤

(7)

1
b

·

5

and for the AG algorithm we get
￿ ￿ L(w￿ )
￿ ￿ L(w￿ )
+ √￿￿
+ √b + b√￿￿ .
1
1
1
1
√b
m ≤
·
n ≤
￿
b
￿
First, let us compare the performance of these two algorithms in the parallel setting, where
the relevant parameter to measure runtime is n. Analyzing which of the terms in each
bound dominates, we get that for the SGD algorithm, there are 2 regimes, while for the AG
algorithm, there are 2-3 regimes depending on the relationship between L(w￿ ) and ￿. The
following two tables summarize the situation (again, ignoring constants):

(8)

+

SGD Algorithm
Regime
n
b ≤ ￿L(w￿ )m L(w￿ )
￿2 b
b ≥ ￿L(w￿ )m
1
￿

AG Algorithm
Regime
b ≤ L(w￿ )1/4m3/4
b ≥ L(w￿ )1/4m3/4
b ≤ L(w￿ )m
L(w￿ )m ≤ b ≤ m2/3
b ≥ m2/3
From the tables, we see that for both methods, there is an initial linear speedup as a function
of the minibatch size b. However, in the AG algorithm, this linear speedup regime holds for
much larger minibatch sizes1 . Even beyond the linear speedup regime, the AG algorithm
still maintains a √b speedup, for the reasonable case where ￿ ≥ L(w￿ )2 . Finally, in all
regimes, the runtime bound of the AG algorithm is equal or signiﬁcantly smaller than that
of the SGD algorithm.

n
L(w￿ )
￿2 b
1√￿
L(w￿ )
￿2 b
1
￿√b
1√￿

￿ ≤ L(w￿ )2

￿ ≥ L(w￿ )2

We now turn to discuss the serial setting, where the runtime is measured in terms of m.
Inspecting (7) and (8), we see that a larger size of b actually requires m to increase for both
algorithms. This is to be expected, since mini-batching does not lead to large gains in a
serial setting. However, using mini-batching in a serial setting might still be beneﬁcial for
implementation reasons, resulting in constant-factor improvements in runtime (e.g. saving
overhead and loop control, and via pipelining, concurrent memory accesses etc.). In that
case, we can at least ask what is the largest mini-batch size that won’t degrade the runtime
guarantee by more than a constant. Using our bounds, the mini-batch size b for the SGD
algorithm can scale as much as L/￿, vs. a larger value of L/￿3/2 for the AG algorithm.

Finally, an interesting point is that the AG algorithm is sometimes actually necessary to
obtain signiﬁcant speed-ups via a mini-batch framework (according to our bounds). Based
on the table above, this happens when the desired suboptimality ￿ is not much bigger then
L(w￿ ), i.e. ￿ =Ω( L(w￿ )). This includes the “separable” case, L(w￿ ) = 0, and in general
a regime where the “estimation error” ￿ and “approximation error” L(w￿ ) are roughly the
same—an arguably very relevant one in machine learning. For the SGD algorithm, the
critical mini-batch value ￿L(w￿ )m can be shown to equal L(w￿ )/￿, which is O(1) in our
case. So with SGD we get no non-constant parallel speedup. However, with AG, we still
enjoy a speedup of at least Θ(√b), all the way up to mini-batch size b = m2/3 .

5 Experiments

We implemented both the SGD algorithm (Algorithm 1) and the AG algorithm (Algorithm
2, using step-sizes of the form γi = γi p as suggested by Theorem 2) on two publicly-available
binary classiﬁcation problems, astro-physics and CCAT. We used the smoothed hinge loss
￿(w; x, y), deﬁned as 0.5− yw￿x if yw￿x ≤ 0; 0 if yw￿x > 1, and 0.5(1− yw￿x)2 otherwise.
While both datasets are relatively easy to classify, we also wished to understand the algo-
rithms’ performance in the “separable” case L(w￿ ) = 0, to see if the theory in Section 4
1Since it is easily veriﬁed that ￿L(w￿ )m is generally smaller than both L(w￿ )1/4m3/4 and
L(w￿ )m
6

astro-physics

CCAT

●

0
2
0
0
.
0

8
1
0
0
.
0

6
1
0
0
.
0

4
1
0
0
.
0

s
s
o
L

t
s
e
T

●

●

b=4
b=16
b=64

2
2
0
0
.
0

0
2
0
0
.
0

8
1
0
0
.
0

6
1
0
0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

p

b=16
b=64
b=256

●

●

●

0.2

0.4

0.6

0.8

1.0

p

Figure 1: Left: Test smoothed hinge loss, as a function of p, after training using the AG algorithm
on 6361 examples from astro-physics, for various batch sizes. Right: the same, for 18578 examples
from CCAT. In both datasets, margin violations were removed before training so that L(w￿ ) = 0.
The circled points are the theoretically-derived values p = ln b/(2 ln(n − 1)) (see Theorem 2).

holds in practice. To this end, we created an additional version of each dataset, where
L(w￿ ) = 0, by training a classiﬁer on the entire dataset and removing margin violations.

In all of our experiments, we used up to half of the data for training, and one-quarter
each for validation and testing. The validation set was used to determine the step sizes
η and γi . We justify this by noting that our goal is to compare the performance of the
SGD and AG algorithms, independently of the diﬃculties in choosing their stepsizes. In
the implementation, we neglected the pro jection step, as we found it does not signiﬁcantly
aﬀect performance when the stepsizes are properly selected.

In our ﬁrst set of experiments, we attempted to determine the relationship between the
performance of the AG algorithm and the p parameter, which determines the rate of in-
crease of the step sizes γi . Our experiments are summarized in Figure 5. Perhaps the most
important conclusion to draw from these plots is that neither the “traditional” choice p = 1,
nor the constant-step-size choice p = 0, give the best performance in all circumstances. In-
stead, there is a complicated data-dependent relationship between p, and the ﬁnal classiﬁer’s
performance. Furthermore, there appears to be a weak trend towards higher p performing
better for larger minibatch sizes b, which corresponds neatly with our theoretical predictions.

In our next experiment, we directly compared the performance of the SGD and AG. To do
so, we varied the minibatch size b while holding the total amount of training data (m = nb)
ﬁxed. When L(w￿ ) > 0 (top row of Figure 5), the total sample size m is high and the
suboptimality ￿ is low (red and black plots), we see that for small minibatch size, both
methods do not degrade as we increase b, corresponding to a linear parallel speedup. In
fact, SGD is actually overall better, but as b increases, its performance degrades more
quickly, eventually performing worse than AG. That is, even in the least favorable scenario
for AG (high L(w￿ ) and small ￿, see the tables in Sec. 4), it does give beneﬁts with large
enough minibatch sizes. Further, we see that once the suboptimality ￿ is roughly equal to
L∗ , AG signiﬁcantly outperforms SGD, even with small minibatches, agreeing with theory.
Turning to the case L(w￿ ) = 0 (bottom two rows of Figure 5), which is theoretically more
favorable to AG, we see it is indeed mostly better, in terms of retaining linear parallel
speedups for larger minibatch sizes, even for large data set sizes corresponding to small
suboptimality values, and might even be advantageous with small minibatch sizes.

6 Summary

In this paper, we presented novel contributions to the theory of ﬁrst order stochastic convex
optimization (Theorems 1 and 2, generalizing results of [4] and [5] to be sensitive to L (w￿ )),

7

5
7
0
.
0

0
7
0
.
0

5
6
0
.
0

0
6
0
.
0

5
5
0
.
0

0
5
0
.
0

2
1
0
.
0

8
0
0
.
0

4
0
0
.
0

0
0
0
.
0

8
0
0
.
0

6
0
0
.
0

4
0
0
.
0

2
0
0
.
0

0
0
0
.
0

s
s
o
L

t
s
e
T

s
s
o
L

t
s
e
T

n
o
i
t
a
c
ﬁ
i
s
s
a
l
c
s
i
M
t
s
e
T

astro-physics

CCAT

T=402207
T=25137
T=1571

T=31884
T=7796
T=1949

4
1
.
0

3
1
.
0

2
1
.
0

1
1
.
0

0
1
.
0

9
0
.
0

8
0
.
0

1

5

10

50 100

500

5000

1

10

100

1000

10000

1

5

10

50 100

500

T=402207
T=25137
T=1571

1

10

100

1000

10000

T=402207
T=25137
T=1571

T=31884
T=7796
T=1949

T=31884
T=7796
T=1949

0
3
0
.
0

0
2
0
.
0

0
1
0
.
0

0
0
0
.
0

0
2
0
.
0

5
1
0
.
0

0
1
0
.
0

5
0
0
.
0

0
0
0
.
0

1

5

10

50 100

500

1

10

b

1000

10000

100

b

Figure 2: Test loss on astro-physics and CCAT as a function of mini-batch size b (in log-scale),
where the total amount of training data m = nb is held ﬁxed. Solid lines and dashed lines are for
SGD and AG respectively (for AG, we used p = ln b/(2 ln(n − 1)) as in Theorem 2). The upper row
shows the smoothed hinge loss on the test set, using the original (uncensored) data. The bottom
rows show the smoothed hinge loss and misclassiﬁcation rate on the test set, using the modiﬁed
data where L(w￿ ) = 0. All curves are averaged over three runs.

developed a novel step size strategy for the accelerated method that we used in order to
obtain our results and we saw works well in practice, and provided a more reﬁned analysis
of the eﬀects of minibatching which paints a diﬀerent picture then previous analyses [4, 1]
and highlights the beneﬁt of accelerated methods.

A remaining open practical and theoretical question is whether the bound of Theorem 2
is tight. Following [5], the bound is tight for b = 1 and b → ∞, i.e. the ﬁrst and third
terms are tight, but it is not clear whether the 1/(√bn) dependence is indeed necessary.
It would be interesting to understand whether with a more reﬁned analysis, or perhaps
diﬀerent step-sizes, we can avoid this term, whether an altogether diﬀerent algorithm is
needed, or whether this term does represent the optimal behavior for any method based on
b-aggregated stochastic gradient estimates.

8

References
[1] A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. Technical report,
arXiv, 2011.

[2] A. Beck and M. Teboulle. Mirror descent and nonlinear pro jected subgradient methods
for convex optimization. Operations Research Letters, 31(3):167 – 175, 2003.

[3] L. Bottou and O. Bousquet. The tradeoﬀs of large scale learning. In NIPS, 2007.

[4] O. Dekel, R. Gilad Bachrach, O. Shamir, and L. Xiao. Optimal distributed online
prediction using mini-batches. Technical report, arXiv, 2010.

[5] G. Lan. An optimal method for stochastic convex optimization. Technical report,
Georgia Institute of Technology, 2009.

[6] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609,
2009.

[7] Y. Nesterov. A method for unconstrained convex minimization problem with the rate
of convergence o(1/k2 ). Doklady AN SSSR, 269:543–547, 1983.

[8] Y. Nesterov.
Smooth minimization of non-smooth functions. Math. Program.,
103(1):127–152, 2005.

[9] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical
Programming, 120(1):221–259, August 2009.

[10] O. Shamir O. Dekel, R. Gilad-Bachrach and L. Xiao. Optimal distributed online pre-
diction. In ICML, 2011.

[11] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: primal estimated
sub-gradient solver for SVM. Math. Program., 127(1):3–30, 2011.

[12] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training
set size. In ICML, 2008.

[13] N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In NIPS,
2010.

[14] S.Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis,
Hebrew University of Jerusalem, 2007.

[15] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization.
Submitted to SIAM Journal on Optimization, 2008.

[16] L. Xiao. Dual averaging methods for regularized stochastic learning and online opti-
mization. Journal of Machine Learning Research, 11:2543–2596, 2010.

9

